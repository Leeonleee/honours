{
  "repo": "duckdb/duckdb",
  "pull_number": 3004,
  "instance_id": "duckdb__duckdb-3004",
  "issue_numbers": [
    "2997",
    "2997"
  ],
  "base_commit": "0d3fee81ba83d69adf362d92482101a82a4a5de1",
  "patch": "diff --git a/src/execution/operator/persistent/buffered_csv_reader.cpp b/src/execution/operator/persistent/buffered_csv_reader.cpp\nindex 4b1586d8c024..1eeff65fd4bd 100644\n--- a/src/execution/operator/persistent/buffered_csv_reader.cpp\n+++ b/src/execution/operator/persistent/buffered_csv_reader.cpp\n@@ -1532,8 +1532,8 @@ bool BufferedCSVReader::ReadBuffer(idx_t &start) {\n \twhile (remaining > buffer_read_size) {\n \t\tbuffer_read_size *= 2;\n \t}\n-\tif (remaining + buffer_read_size > MAXIMUM_CSV_LINE_SIZE) {\n-\t\tthrow InvalidInputException(\"Maximum line size of %llu bytes exceeded!\", MAXIMUM_CSV_LINE_SIZE);\n+\tif (remaining + buffer_read_size > options.maximum_line_size) {\n+\t\tthrow InvalidInputException(\"Maximum line size of %llu bytes exceeded!\", options.maximum_line_size);\n \t}\n \tbuffer = unique_ptr<char[]>(new char[buffer_read_size + remaining + 1]);\n \tbuffer_size = remaining + buffer_read_size;\ndiff --git a/src/function/table/copy_csv.cpp b/src/function/table/copy_csv.cpp\nindex 605c0e72b062..ee75c563f1f6 100644\n--- a/src/function/table/copy_csv.cpp\n+++ b/src/function/table/copy_csv.cpp\n@@ -82,6 +82,8 @@ static bool ParseBaseOption(BufferedCSVReaderOptions &options, string &loption,\n \t\toptions.compression = FileCompressionTypeFromString(ParseString(set));\n \t} else if (loption == \"skip\") {\n \t\toptions.skip_rows = ParseInteger(set);\n+\t} else if (loption == \"max_line_size\" || loption == \"maximum_line_size\") {\n+\t\toptions.maximum_line_size = ParseInteger(set);\n \t} else {\n \t\t// unrecognized option in base CSV\n \t\treturn false;\ndiff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp\nindex e70dd6341d92..5b84570b035e 100644\n--- a/src/function/table/read_csv.cpp\n+++ b/src/function/table/read_csv.cpp\n@@ -124,6 +124,8 @@ static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, vector<Value\n \t\t\tresult->include_file_name = BooleanValue::Get(kv.second);\n \t\t} else if (loption == \"skip\") {\n \t\t\toptions.skip_rows = kv.second.GetValue<int64_t>();\n+\t\t} else if (loption == \"max_line_size\" || loption == \"maximum_line_size\") {\n+\t\t\toptions.maximum_line_size = kv.second.GetValue<int64_t>();\n \t\t} else {\n \t\t\tthrow InternalException(\"Unrecognized parameter %s\", kv.first);\n \t\t}\n@@ -230,6 +232,8 @@ static void ReadCSVAddNamedParameters(TableFunction &table_function) {\n \ttable_function.named_parameters[\"compression\"] = LogicalType::VARCHAR;\n \ttable_function.named_parameters[\"filename\"] = LogicalType::BOOLEAN;\n \ttable_function.named_parameters[\"skip\"] = LogicalType::BIGINT;\n+\ttable_function.named_parameters[\"max_line_size\"] = LogicalType::VARCHAR;\n+\ttable_function.named_parameters[\"maximum_line_size\"] = LogicalType::VARCHAR;\n }\n \n double CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p) {\ndiff --git a/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp b/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp\nindex cbaa9f98e573..2c1146b054aa 100644\n--- a/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp\n+++ b/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp\n@@ -95,6 +95,9 @@ struct BufferedCSVReaderOptions {\n \tidx_t buffer_size = STANDARD_VECTOR_SIZE * 100;\n \t//! Consider all columns to be of type varchar\n \tbool all_varchar = false;\n+\t//! Maximum CSV line size: specified because if we reach this amount, we likely have wrong delimiters (default: 2MB)\n+\tidx_t maximum_line_size = 2097152;\n+\n \t//! The date format to use (if any is specified)\n \tstd::map<LogicalTypeId, StrpTimeFormat> date_format = {{LogicalTypeId::DATE, {}}, {LogicalTypeId::TIMESTAMP, {}}};\n \t//! Whether or not a type format is specified\n@@ -111,8 +114,6 @@ enum class ParserMode : uint8_t { PARSING = 0, SNIFFING_DIALECT = 1, SNIFFING_DA\n class BufferedCSVReader {\n \t//! Initial buffer read size; can be extended for long lines\n \tstatic constexpr idx_t INITIAL_BUFFER_SIZE = 16384;\n-\t//! Maximum CSV line size: specified because if we reach this amount, we likely have the wrong delimiters\n-\tstatic constexpr idx_t MAXIMUM_CSV_LINE_SIZE = 1048576;\n \tParserMode mode;\n \n public:\n",
  "test_patch": "diff --git a/test/sql/copy/csv/test_max_line_size.test_slow b/test/sql/copy/csv/test_max_line_size.test_slow\nindex 37cbc31d5267..6ffc8de57354 100644\n--- a/test/sql/copy/csv/test_max_line_size.test_slow\n+++ b/test/sql/copy/csv/test_max_line_size.test_slow\n@@ -2,9 +2,9 @@\n # description: Test lines that exceed the maximum line size\n # group: [csv]\n \n-# generate CSV file with 20 MB string\n+# generate CSV file with 4 MB string\n statement ok\n-COPY (SELECT 10, REPEAT('a', 2048576), 20) TO '__TEST_DIR__/test.csv'\n+COPY (SELECT 10, REPEAT('a', 4048576), 20) TO '__TEST_DIR__/test.csv'\n \n # value is too big for loading\n statement ok\n@@ -13,3 +13,10 @@ CREATE TABLE test (a INTEGER, b VARCHAR, c INTEGER);\n statement error\n COPY test FROM '__TEST_DIR__/test.csv';\n \n+# we can override the max line size\n+statement ok\n+COPY test FROM '__TEST_DIR__/test.csv' (max_line_size 204857600);\n+\n+# also in the read_csv call\n+statement ok\n+INSERT INTO test SELECT * FROM read_csv_auto('__TEST_DIR__/test.csv', max_line_size=204857600);\n\\ No newline at end of file\n",
  "problem_statement": "Make BufferedCSVReader's MAXIMUM_CSV_LINE_SIZE configurable\nThe `MAXIMUM_CSV_LINE_SIZE` setting in `BufferedCSVReader` is [hard coded to 1MB](https://github.com/duckdb/duckdb/blob/477c52aed0075aec926303d4f6b433cd48805a3d/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp#L114-L115). While this is probably sufficient for most, our application allows up to 1MB per column and 2MB per row. As a result, we're unable to load our CSV files using `COPY \u2026 FROM \u2026` due to this hard coded limitation.\r\n\r\nIt would be helpful to have `read_csv_auto` and `COPY \u2026 FROM \u2026` both extended to support a `MAXIMUM_CSV_LINE_SIZE` (or similar, perhaps shorter named) parameter to allow the application to override the 1MB default value.\nMake BufferedCSVReader's MAXIMUM_CSV_LINE_SIZE configurable\nThe `MAXIMUM_CSV_LINE_SIZE` setting in `BufferedCSVReader` is [hard coded to 1MB](https://github.com/duckdb/duckdb/blob/477c52aed0075aec926303d4f6b433cd48805a3d/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp#L114-L115). While this is probably sufficient for most, our application allows up to 1MB per column and 2MB per row. As a result, we're unable to load our CSV files using `COPY \u2026 FROM \u2026` due to this hard coded limitation.\r\n\r\nIt would be helpful to have `read_csv_auto` and `COPY \u2026 FROM \u2026` both extended to support a `MAXIMUM_CSV_LINE_SIZE` (or similar, perhaps shorter named) parameter to allow the application to override the 1MB default value.\n",
  "hints_text": "We could like also just bump the limit to 2 MB for now, looks like this is a heuristic anyway\nWe could like also just bump the limit to 2 MB for now, looks like this is a heuristic anyway",
  "created_at": "2022-01-28T12:54:56Z",
  "modified_files": [
    "src/execution/operator/persistent/buffered_csv_reader.cpp",
    "src/function/table/copy_csv.cpp",
    "src/function/table/read_csv.cpp",
    "src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
  ],
  "modified_test_files": [
    "test/sql/copy/csv/test_max_line_size.test_slow"
  ]
}