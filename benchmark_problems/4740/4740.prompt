You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
[Export] Generated columns aren't properly exported with `EXPORT DATABASE`
### What happens?

When exporting a database containing tables with generated column(s), attempting to reimport the database fails.

### To Reproduce

```sql
create table y(id1 int not null, id2 int generated always as (id1 / 2), id3 text);
insert into y values(10, 'abc');
export database 'test';
drop table y;
import database 'test';
```

### OS:

MacOS

### DuckDB Version:

4.0.1

### DuckDB Client:

CLI

### Full Name:

Thijs Bruineman

### Affiliation:

DuckDB Labs

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree
[Export] Generated columns aren't properly exported with `EXPORT DATABASE`
### What happens?

When exporting a database containing tables with generated column(s), attempting to reimport the database fails.

### To Reproduce

```sql
create table y(id1 int not null, id2 int generated always as (id1 / 2), id3 text);
insert into y values(10, 'abc');
export database 'test';
drop table y;
import database 'test';
```

### OS:

MacOS

### DuckDB Version:

4.0.1

### DuckDB Client:

CLI

### Full Name:

Thijs Bruineman

### Affiliation:

DuckDB Labs

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of src/planner/binder/statement/bind_export.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/statement/export_statement.hpp"
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/planner/operator/logical_export.hpp"
5: #include "duckdb/catalog/catalog_entry/copy_function_catalog_entry.hpp"
6: #include "duckdb/parser/statement/copy_statement.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/common/file_system.hpp"
10: #include "duckdb/planner/operator/logical_set_operation.hpp"
11: #include "duckdb/parser/parsed_data/exported_table_data.hpp"
12: #include "duckdb/parser/constraints/foreign_key_constraint.hpp"
13: 
14: #include "duckdb/common/string_util.hpp"
15: #include <algorithm>
16: 
17: namespace duckdb {
18: 
19: //! Sanitizes a string to have only low case chars and underscores
20: string SanitizeExportIdentifier(const string &str) {
21: 	// Copy the original string to result
22: 	string result(str);
23: 
24: 	for (idx_t i = 0; i < str.length(); ++i) {
25: 		auto c = str[i];
26: 		if (c >= 'a' && c <= 'z') {
27: 			// If it is lower case just continue
28: 			continue;
29: 		}
30: 
31: 		if (c >= 'A' && c <= 'Z') {
32: 			// To lowercase
33: 			result[i] = tolower(c);
34: 		} else {
35: 			// Substitute to underscore
36: 			result[i] = '_';
37: 		}
38: 	}
39: 
40: 	return result;
41: }
42: 
43: bool IsExistMainKeyTable(string &table_name, vector<TableCatalogEntry *> &unordered) {
44: 	for (idx_t i = 0; i < unordered.size(); i++) {
45: 		if (unordered[i]->name == table_name) {
46: 			return true;
47: 		}
48: 	}
49: 	return false;
50: }
51: 
52: void ScanForeignKeyTable(vector<TableCatalogEntry *> &ordered, vector<TableCatalogEntry *> &unordered,
53:                          bool move_only_pk_table) {
54: 	for (auto i = unordered.begin(); i != unordered.end();) {
55: 		auto table_entry = *i;
56: 		bool move_to_ordered = true;
57: 		for (idx_t j = 0; j < table_entry->constraints.size(); j++) {
58: 			auto &cond = table_entry->constraints[j];
59: 			if (cond->type == ConstraintType::FOREIGN_KEY) {
60: 				auto &fk = (ForeignKeyConstraint &)*cond;
61: 				if ((move_only_pk_table && fk.info.type == ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE) ||
62: 				    (!move_only_pk_table && fk.info.type == ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE &&
63: 				     IsExistMainKeyTable(fk.info.table, unordered))) {
64: 					move_to_ordered = false;
65: 					break;
66: 				}
67: 			}
68: 		}
69: 		if (move_to_ordered) {
70: 			ordered.push_back(table_entry);
71: 			i = unordered.erase(i);
72: 		} else {
73: 			i++;
74: 		}
75: 	}
76: }
77: 
78: void ReorderTableEntries(vector<TableCatalogEntry *> &tables) {
79: 	vector<TableCatalogEntry *> ordered;
80: 	vector<TableCatalogEntry *> unordered = tables;
81: 	ScanForeignKeyTable(ordered, unordered, true);
82: 	while (!unordered.empty()) {
83: 		ScanForeignKeyTable(ordered, unordered, false);
84: 	}
85: 	tables = ordered;
86: }
87: 
88: BoundStatement Binder::Bind(ExportStatement &stmt) {
89: 	// COPY TO a file
90: 	auto &config = DBConfig::GetConfig(context);
91: 	if (!config.options.enable_external_access) {
92: 		throw PermissionException("COPY TO is disabled through configuration");
93: 	}
94: 	BoundStatement result;
95: 	result.types = {LogicalType::BOOLEAN};
96: 	result.names = {"Success"};
97: 
98: 	// lookup the format in the catalog
99: 	auto &catalog = Catalog::GetCatalog(context);
100: 	auto copy_function = catalog.GetEntry<CopyFunctionCatalogEntry>(context, DEFAULT_SCHEMA, stmt.info->format);
101: 	if (!copy_function->function.copy_to_bind) {
102: 		throw NotImplementedException("COPY TO is not supported for FORMAT \"%s\"", stmt.info->format);
103: 	}
104: 
105: 	// gather a list of all the tables
106: 	vector<TableCatalogEntry *> tables;
107: 	auto schemas = catalog.schemas->GetEntries<SchemaCatalogEntry>(context);
108: 	for (auto &schema : schemas) {
109: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) {
110: 			if (entry->type == CatalogType::TABLE_ENTRY) {
111: 				tables.push_back((TableCatalogEntry *)entry);
112: 			}
113: 		});
114: 	}
115: 
116: 	// reorder tables because of foreign key constraint
117: 	ReorderTableEntries(tables);
118: 
119: 	// now generate the COPY statements for each of the tables
120: 	auto &fs = FileSystem::GetFileSystem(context);
121: 	unique_ptr<LogicalOperator> child_operator;
122: 
123: 	BoundExportData exported_tables;
124: 
125: 	unordered_set<string> table_name_index;
126: 	for (auto &table : tables) {
127: 		auto info = make_unique<CopyInfo>();
128: 		// we copy the options supplied to the EXPORT
129: 		info->format = stmt.info->format;
130: 		info->options = stmt.info->options;
131: 		// set up the file name for the COPY TO
132: 
133: 		auto exported_data = ExportedTableData();
134: 		idx_t id = 0;
135: 		while (true) {
136: 			string id_suffix = id == 0 ? string() : "_" + to_string(id);
137: 			if (table->schema->name == DEFAULT_SCHEMA) {
138: 				info->file_path = fs.JoinPath(stmt.info->file_path,
139: 				                              StringUtil::Format("%s%s.%s", SanitizeExportIdentifier(table->name),
140: 				                                                 id_suffix, copy_function->function.extension));
141: 			} else {
142: 				info->file_path =
143: 				    fs.JoinPath(stmt.info->file_path,
144: 				                StringUtil::Format("%s_%s%s.%s", SanitizeExportIdentifier(table->schema->name),
145: 				                                   SanitizeExportIdentifier(table->name), id_suffix,
146: 				                                   copy_function->function.extension));
147: 			}
148: 			if (table_name_index.find(info->file_path) == table_name_index.end()) {
149: 				// this name was not yet taken: take it
150: 				table_name_index.insert(info->file_path);
151: 				break;
152: 			}
153: 			id++;
154: 		}
155: 		info->is_from = false;
156: 		info->schema = table->schema->name;
157: 		info->table = table->name;
158: 
159: 		exported_data.table_name = info->table;
160: 		exported_data.schema_name = info->schema;
161: 		exported_data.file_path = info->file_path;
162: 
163: 		ExportedTableInfo table_info;
164: 		table_info.entry = table;
165: 		table_info.table_data = exported_data;
166: 		exported_tables.data.push_back(table_info);
167: 		id++;
168: 
169: 		// generate the copy statement and bind it
170: 		CopyStatement copy_stmt;
171: 		copy_stmt.info = move(info);
172: 
173: 		auto copy_binder = Binder::CreateBinder(context);
174: 		auto bound_statement = copy_binder->Bind(copy_stmt);
175: 		if (child_operator) {
176: 			// use UNION ALL to combine the individual copy statements into a single node
177: 			auto copy_union =
178: 			    make_unique<LogicalSetOperation>(GenerateTableIndex(), 1, move(child_operator),
179: 			                                     move(bound_statement.plan), LogicalOperatorType::LOGICAL_UNION);
180: 			child_operator = move(copy_union);
181: 		} else {
182: 			child_operator = move(bound_statement.plan);
183: 		}
184: 	}
185: 
186: 	// try to create the directory, if it doesn't exist yet
187: 	// a bit hacky to do it here, but we need to create the directory BEFORE the copy statements run
188: 	if (!fs.DirectoryExists(stmt.info->file_path)) {
189: 		fs.CreateDirectory(stmt.info->file_path);
190: 	}
191: 
192: 	// create the export node
193: 	auto export_node = make_unique<LogicalExport>(copy_function->function, move(stmt.info), exported_tables);
194: 
195: 	if (child_operator) {
196: 		export_node->children.push_back(move(child_operator));
197: 	}
198: 
199: 	result.plan = move(export_node);
200: 	properties.allow_stream_result = false;
201: 	properties.return_type = StatementReturnType::NOTHING;
202: 	return result;
203: }
204: 
205: } // namespace duckdb
[end of src/planner/binder/statement/bind_export.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: