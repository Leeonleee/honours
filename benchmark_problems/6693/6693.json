{
  "repo": "duckdb/duckdb",
  "pull_number": 6693,
  "instance_id": "duckdb__duckdb-6693",
  "issue_numbers": [
    "6668",
    "6668"
  ],
  "base_commit": "6525767cf115f7996eaab67641a5eae3a41ab2fd",
  "patch": "diff --git a/src/execution/index/art/art.cpp b/src/execution/index/art/art.cpp\nindex 17ce11723bd6..a618a77bab62 100644\n--- a/src/execution/index/art/art.cpp\n+++ b/src/execution/index/art/art.cpp\n@@ -330,8 +330,7 @@ bool ART::ConstructFromSorted(idx_t count, vector<Key> &keys, Vector &row_identi\n //===--------------------------------------------------------------------===//\n // Insert / Verification / Constraint Checking\n //===--------------------------------------------------------------------===//\n-\n-bool ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {\n+PreservedError ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {\n \n \tD_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);\n \tD_ASSERT(logical_types[0] == input.data[0].GetType());\n@@ -375,12 +374,13 @@ bool ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {\n \n \tIncreaseAndVerifyMemorySize(old_memory_size);\n \tif (failed_index != DConstants::INVALID_INDEX) {\n-\t\treturn false;\n+\t\treturn PreservedError(ConstraintException(\"PRIMARY KEY or UNIQUE constraint violated: duplicate key \\\"%s\\\"\",\n+\t\t                                          AppendRowError(input, failed_index)));\n \t}\n-\treturn true;\n+\treturn PreservedError();\n }\n \n-bool ART::Append(IndexLock &lock, DataChunk &appended_data, Vector &row_identifiers) {\n+PreservedError ART::Append(IndexLock &lock, DataChunk &appended_data, Vector &row_identifiers) {\n \tDataChunk expression_result;\n \texpression_result.Initialize(Allocator::DefaultAllocator(), logical_types);\n \ndiff --git a/src/include/duckdb/execution/index/art/art.hpp b/src/include/duckdb/execution/index/art/art.hpp\nindex 669ea66824f5..50badead626e 100644\n--- a/src/include/duckdb/execution/index/art/art.hpp\n+++ b/src/include/duckdb/execution/index/art/art.hpp\n@@ -72,7 +72,7 @@ class ART : public Index {\n \t          vector<row_t> &result_ids) override;\n \n \t//! Called when data is appended to the index. The lock obtained from InitializeLock must be held\n-\tbool Append(IndexLock &lock, DataChunk &entries, Vector &row_identifiers) override;\n+\tPreservedError Append(IndexLock &lock, DataChunk &entries, Vector &row_identifiers) override;\n \t//! Verify that data can be appended to the index without a constraint violation\n \tvoid VerifyAppend(DataChunk &chunk) override;\n \t//! Verify that data can be appended to the index without a constraint violation using the conflict manager\n@@ -80,7 +80,7 @@ class ART : public Index {\n \t//! Delete a chunk of entries from the index. The lock obtained from InitializeLock must be held\n \tvoid Delete(IndexLock &lock, DataChunk &entries, Vector &row_identifiers) override;\n \t//! Insert a chunk of entries into the index\n-\tbool Insert(IndexLock &lock, DataChunk &data, Vector &row_ids) override;\n+\tPreservedError Insert(IndexLock &lock, DataChunk &data, Vector &row_ids) override;\n \n \t//! Construct an ART from a vector of sorted keys\n \tbool ConstructFromSorted(idx_t count, vector<Key> &keys, Vector &row_identifiers);\ndiff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp\nindex 1cbc881ead01..728e8f2ba637 100644\n--- a/src/include/duckdb/storage/data_table.hpp\n+++ b/src/include/duckdb/storage/data_table.hpp\n@@ -150,8 +150,8 @@ class DataTable {\n \n \t//! Append a chunk with the row ids [row_start, ..., row_start + chunk.size()] to all indexes of the table, returns\n \t//! whether or not the append succeeded\n-\tbool AppendToIndexes(DataChunk &chunk, row_t row_start);\n-\tstatic bool AppendToIndexes(TableIndexList &indexes, DataChunk &chunk, row_t row_start);\n+\tPreservedError AppendToIndexes(DataChunk &chunk, row_t row_start);\n+\tstatic PreservedError AppendToIndexes(TableIndexList &indexes, DataChunk &chunk, row_t row_start);\n \t//! Remove a chunk with the row ids [row_start, ..., row_start + chunk.size()] from all indexes of the table\n \tvoid RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start);\n \t//! Remove the chunk with the specified set of row identifiers from all indexes of the table\ndiff --git a/src/include/duckdb/storage/index.hpp b/src/include/duckdb/storage/index.hpp\nindex 7383654085f2..0fe58c0c24a7 100644\n--- a/src/include/duckdb/storage/index.hpp\n+++ b/src/include/duckdb/storage/index.hpp\n@@ -80,9 +80,9 @@ class Index {\n \t//! Obtain a lock on the index\n \tvirtual void InitializeLock(IndexLock &state);\n \t//! Called when data is appended to the index. The lock obtained from InitializeLock must be held\n-\tvirtual bool Append(IndexLock &state, DataChunk &entries, Vector &row_identifiers) = 0;\n+\tvirtual PreservedError Append(IndexLock &state, DataChunk &entries, Vector &row_identifiers) = 0;\n \t//! Obtains a lock and calls Append while holding that lock\n-\tbool Append(DataChunk &entries, Vector &row_identifiers);\n+\tPreservedError Append(DataChunk &entries, Vector &row_identifiers);\n \t//! Verify that data can be appended to the index without a constraint violation\n \tvirtual void VerifyAppend(DataChunk &chunk) = 0;\n \t//! Verify that data can be appended to the index without a constraint violation using the conflict manager\n@@ -96,7 +96,7 @@ class Index {\n \tvoid Delete(DataChunk &entries, Vector &row_identifiers);\n \n \t//! Insert a chunk of entries into the index\n-\tvirtual bool Insert(IndexLock &lock, DataChunk &input, Vector &row_identifiers) = 0;\n+\tvirtual PreservedError Insert(IndexLock &lock, DataChunk &input, Vector &row_identifiers) = 0;\n \n \t//! Merge another index into this index. The lock obtained from InitializeLock must be held, and the other\n \t//! index must also be locked during the merge\n@@ -147,6 +147,7 @@ class Index {\n \n \t//! Execute the index expressions on an input chunk\n \tvoid ExecuteExpressions(DataChunk &input, DataChunk &result);\n+\tstatic string AppendRowError(DataChunk &input, idx_t index);\n \n protected:\n \t//! Lock used for any changes to the index\ndiff --git a/src/include/duckdb/transaction/local_storage.hpp b/src/include/duckdb/transaction/local_storage.hpp\nindex 61ee639affca..4eeb4c146c92 100644\n--- a/src/include/duckdb/transaction/local_storage.hpp\n+++ b/src/include/duckdb/transaction/local_storage.hpp\n@@ -88,8 +88,8 @@ class LocalTableStorage : public std::enable_shared_from_this<LocalTableStorage>\n \n \tvoid AppendToIndexes(DuckTransaction &transaction, TableAppendState &append_state, idx_t append_count,\n \t                     bool append_to_table);\n-\tbool AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source, TableIndexList &index_list,\n-\t                     const vector<LogicalType> &table_types, row_t &start_row);\n+\tPreservedError AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source, TableIndexList &index_list,\n+\t                               const vector<LogicalType> &table_types, row_t &start_row);\n \n \t//! Creates an optimistic writer for this table\n \tOptimisticDataWriter *CreateOptimisticWriter();\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex 98e99d400b52..a2b05b7a0b4f 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -819,9 +819,10 @@ void DataTable::RevertAppend(idx_t start_row, idx_t count) {\n //===--------------------------------------------------------------------===//\n // Indexes\n //===--------------------------------------------------------------------===//\n-bool DataTable::AppendToIndexes(TableIndexList &indexes, DataChunk &chunk, row_t row_start) {\n+PreservedError DataTable::AppendToIndexes(TableIndexList &indexes, DataChunk &chunk, row_t row_start) {\n+\tPreservedError error;\n \tif (indexes.Empty()) {\n-\t\treturn true;\n+\t\treturn error;\n \t}\n \t// first generate the vector of row identifiers\n \tVector row_identifiers(LogicalType::ROW_TYPE);\n@@ -832,11 +833,13 @@ bool DataTable::AppendToIndexes(TableIndexList &indexes, DataChunk &chunk, row_t\n \t// now append the entries to the indices\n \tindexes.Scan([&](Index &index) {\n \t\ttry {\n-\t\t\tif (!index.Append(chunk, row_identifiers)) {\n-\t\t\t\tappend_failed = true;\n-\t\t\t\treturn true;\n-\t\t\t}\n-\t\t} catch (...) {\n+\t\t\terror = index.Append(chunk, row_identifiers);\n+\t\t} catch (Exception &ex) {\n+\t\t\terror = PreservedError(ex);\n+\t\t} catch (std::exception &ex) {\n+\t\t\terror = PreservedError(ex);\n+\t\t}\n+\t\tif (error) {\n \t\t\tappend_failed = true;\n \t\t\treturn true;\n \t\t}\n@@ -850,12 +853,11 @@ bool DataTable::AppendToIndexes(TableIndexList &indexes, DataChunk &chunk, row_t\n \t\tfor (auto *index : already_appended) {\n \t\t\tindex->Delete(chunk, row_identifiers);\n \t\t}\n-\t\treturn false;\n \t}\n-\treturn true;\n+\treturn error;\n }\n \n-bool DataTable::AppendToIndexes(DataChunk &chunk, row_t row_start) {\n+PreservedError DataTable::AppendToIndexes(DataChunk &chunk, row_t row_start) {\n \tD_ASSERT(is_root);\n \treturn AppendToIndexes(info->indexes, chunk, row_start);\n }\n@@ -1204,9 +1206,9 @@ void DataTable::WALAddIndex(ClientContext &context, unique_ptr<Index> index,\n \t\t\tindex->ExecuteExpressions(intermediate, result);\n \n \t\t\t// insert into the index\n-\t\t\tif (!index->Insert(lock, result, intermediate.data[intermediate.ColumnCount() - 1])) {\n-\t\t\t\tthrow InternalException(\"Error during WAL replay. Can't create unique index, table contains \"\n-\t\t\t\t                        \"duplicate data on indexed column(s).\");\n+\t\t\tauto error = index->Insert(lock, result, intermediate.data[intermediate.ColumnCount() - 1]);\n+\t\t\tif (error) {\n+\t\t\t\tthrow InternalException(\"Error during WAL replay: %s\", error.Message());\n \t\t\t}\n \t\t}\n \t}\ndiff --git a/src/storage/index.cpp b/src/storage/index.cpp\nindex 73e63414b8e2..08b2bc25de5c 100644\n--- a/src/storage/index.cpp\n+++ b/src/storage/index.cpp\n@@ -36,7 +36,7 @@ void Index::InitializeLock(IndexLock &state) {\n \tstate.index_lock = unique_lock<mutex>(lock);\n }\n \n-bool Index::Append(DataChunk &entries, Vector &row_identifiers) {\n+PreservedError Index::Append(DataChunk &entries, Vector &row_identifiers) {\n \tIndexLock state;\n \tInitializeLock(state);\n \treturn Append(state, entries, row_identifiers);\n@@ -90,4 +90,15 @@ BlockPointer Index::Serialize(MetaBlockWriter &writer) {\n \tthrow NotImplementedException(\"The implementation of this index serialization does not exist.\");\n }\n \n+string Index::AppendRowError(DataChunk &input, idx_t index) {\n+\tstring error;\n+\tfor (idx_t c = 0; c < input.ColumnCount(); c++) {\n+\t\tif (c > 0) {\n+\t\t\terror += \", \";\n+\t\t}\n+\t\terror += input.GetValue(c, index).ToString();\n+\t}\n+\treturn error;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp\nindex 69e6d86f9a25..7aec1b489478 100644\n--- a/src/storage/local_storage.cpp\n+++ b/src/storage/local_storage.cpp\n@@ -197,16 +197,16 @@ void LocalTableStorage::FlushToDisk() {\n \toptimistic_writer.FinalFlush();\n }\n \n-bool LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source,\n-                                        TableIndexList &index_list, const vector<LogicalType> &table_types,\n-                                        row_t &start_row) {\n+PreservedError LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source,\n+                                                  TableIndexList &index_list, const vector<LogicalType> &table_types,\n+                                                  row_t &start_row) {\n \t// only need to scan for index append\n \t// figure out which columns we need to scan for the set of indexes\n \tauto columns = index_list.GetRequiredColumns();\n \t// create an empty mock chunk that contains all the correct types for the table\n \tDataChunk mock_chunk;\n \tmock_chunk.InitializeEmpty(table_types);\n-\tbool success = true;\n+\tPreservedError error;\n \tsource.Scan(transaction, columns, [&](DataChunk &chunk) -> bool {\n \t\t// construct the mock chunk by referencing the required columns\n \t\tfor (idx_t i = 0; i < columns.size(); i++) {\n@@ -214,28 +214,28 @@ bool LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGroupCo\n \t\t}\n \t\tmock_chunk.SetCardinality(chunk);\n \t\t// append this chunk to the indexes of the table\n-\t\tif (!DataTable::AppendToIndexes(index_list, mock_chunk, start_row)) {\n-\t\t\tsuccess = false;\n+\t\terror = DataTable::AppendToIndexes(index_list, mock_chunk, start_row);\n+\t\tif (error) {\n \t\t\treturn false;\n \t\t}\n \t\tstart_row += chunk.size();\n \t\treturn true;\n \t});\n-\treturn success;\n+\treturn error;\n }\n \n void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppendState &append_state,\n                                         idx_t append_count, bool append_to_table) {\n-\tbool constraint_violated = false;\n \tif (append_to_table) {\n \t\ttable->InitializeAppend(transaction, append_state, append_count);\n \t}\n+\tPreservedError error;\n \tif (append_to_table) {\n \t\t// appending: need to scan entire\n \t\trow_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {\n \t\t\t// append this chunk to the indexes of the table\n-\t\t\tif (!table->AppendToIndexes(chunk, append_state.current_row)) {\n-\t\t\t\tconstraint_violated = true;\n+\t\t\terror = table->AppendToIndexes(chunk, append_state.current_row);\n+\t\t\tif (error) {\n \t\t\t\treturn false;\n \t\t\t}\n \t\t\t// append to base table\n@@ -243,11 +243,10 @@ void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppen\n \t\t\treturn true;\n \t\t});\n \t} else {\n-\t\tconstraint_violated = !AppendToIndexes(transaction, *row_groups, table->info->indexes, table->GetTypes(),\n-\t\t                                       append_state.current_row);\n+\t\terror = AppendToIndexes(transaction, *row_groups, table->info->indexes, table->GetTypes(),\n+\t\t                        append_state.current_row);\n \t}\n-\tif (constraint_violated) {\n-\t\tPreservedError error;\n+\tif (error) {\n \t\t// need to revert the append\n \t\trow_t current_row = append_state.row_start;\n \t\t// remove the data from the indexes, if there are any indexes\n@@ -273,10 +272,7 @@ void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppen\n \t\tif (append_to_table) {\n \t\t\ttable->RevertAppendInternal(append_state.row_start, append_count);\n \t\t}\n-\t\tif (error) {\n-\t\t\terror.Throw();\n-\t\t}\n-\t\tthrow ConstraintException(\"PRIMARY KEY or UNIQUE constraint violated: duplicated key\");\n+\t\terror.Throw();\n \t}\n }\n \n@@ -412,8 +408,9 @@ void LocalStorage::Append(LocalAppendState &state, DataChunk &chunk) {\n \t// append to unique indices (if any)\n \tauto storage = state.storage;\n \tidx_t base_id = MAX_ROW_ID + storage->row_groups->GetTotalRows() + state.append_state.total_append_count;\n-\tif (!DataTable::AppendToIndexes(storage->indexes, chunk, base_id)) {\n-\t\tthrow ConstraintException(\"PRIMARY KEY or UNIQUE constraint violated: duplicated key\");\n+\tauto error = DataTable::AppendToIndexes(storage->indexes, chunk, base_id);\n+\tif (error) {\n+\t\terror.Throw();\n \t}\n \n \t//! Append the chunk to the local storage\n@@ -434,9 +431,9 @@ void LocalStorage::LocalMerge(DataTable *table, RowGroupCollection &collection)\n \tif (!storage->indexes.Empty()) {\n \t\t// append data to indexes if required\n \t\trow_t base_id = MAX_ROW_ID + storage->row_groups->GetTotalRows();\n-\t\tbool success = storage->AppendToIndexes(transaction, collection, storage->indexes, table->GetTypes(), base_id);\n-\t\tif (!success) {\n-\t\t\tthrow ConstraintException(\"PRIMARY KEY or UNIQUE constraint violated: duplicated key\");\n+\t\tauto error = storage->AppendToIndexes(transaction, collection, storage->indexes, table->GetTypes(), base_id);\n+\t\tif (error) {\n+\t\t\terror.Throw();\n \t\t}\n \t}\n \tstorage->row_groups->MergeStorage(collection);\n",
  "test_patch": "diff --git a/test/fuzzer/pedro/buffer_manager_resize_issue.test b/test/fuzzer/pedro/buffer_manager_resize_issue.test\nindex e60d23fd2107..0ccf6de7120a 100644\n--- a/test/fuzzer/pedro/buffer_manager_resize_issue.test\n+++ b/test/fuzzer/pedro/buffer_manager_resize_issue.test\n@@ -32,4 +32,4 @@ INSERT INTO t2(c1,c0) VALUES (235,36),(43,81),(246,187),(28,149),(206,20),(135,1\n statement error\n INSERT INTO t2(c1,c0) VALUES (86,98),(96,107),(237,190),(253,242),(229,9),(6,147);\n ----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\n\\ No newline at end of file\n+Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n\\ No newline at end of file\ndiff --git a/test/fuzzer/pedro/create_index_error.test b/test/fuzzer/pedro/create_index_error.test\nindex 0bf777f1c5a0..6b77e62b71c5 100644\n--- a/test/fuzzer/pedro/create_index_error.test\n+++ b/test/fuzzer/pedro/create_index_error.test\n@@ -57,7 +57,7 @@ CREATE INDEX i0 ON t1 (c1, (decode('\\x81\\x5C\\xE5'::BLOB)::VARCHAR));\n statement error\n INSERT INTO t1 VALUES (1);\n ----\n-TransactionContext Error: Failed to commit: Conversion Error: Failure in decode: could not convert blob to UTF8 string, the blob contained invalid UTF8 characters\n+Conversion Error: Failure in decode: could not convert blob to UTF8 string, the blob contained invalid UTF8 characters\n \n statement ok\n CREATE INDEX i1 ON t1 USING ART (c1);\ndiff --git a/test/sql/index/art/test_art_fuzzer_issues.test b/test/sql/index/art/test_art_fuzzer_issues.test\nindex 869d0b26f0f1..d5de0e61590c 100644\n--- a/test/sql/index/art/test_art_fuzzer_issues.test\n+++ b/test/sql/index/art/test_art_fuzzer_issues.test\n@@ -30,7 +30,7 @@ CREATE INDEX i2 ON t2 (c1);\n statement error\n INSERT INTO t2 VALUES (decode('g\\x00'::BLOB)::VARCHAR),('g');\n ----\n-TransactionContext Error: Failed to commit: Not implemented Error: Indexes cannot contain BLOBs that contain null-terminated bytes.\n+Not implemented Error: Indexes cannot contain BLOBs that contain null-terminated bytes.\n \n statement ok\n INSERT INTO t2 VALUES ('\\0');\n@@ -92,7 +92,7 @@ CREATE INDEX i21 ON t21 (c1, \"decode\"('\\x00'::BLOB));\n statement error\n INSERT INTO t21 VALUES (1);\n ----\n-TransactionContext Error: Failed to commit: Not implemented Error: Indexes cannot contain BLOBs that contain null-terminated bytes.\n+Not implemented Error: Indexes cannot contain BLOBs that contain null-terminated bytes.\n \n statement error\n CREATE INDEX i21 ON t21 (c1);\n\\ No newline at end of file\ndiff --git a/test/sql/index/art/test_art_import_export.test b/test/sql/index/art/test_art_import_export.test\nindex 298461650071..20476a18be2f 100644\n--- a/test/sql/index/art/test_art_import_export.test\n+++ b/test/sql/index/art/test_art_import_export.test\n@@ -33,4 +33,4 @@ IMPORT DATABASE '__TEST_DIR__/export_index_db'\n statement error\n INSERT INTO raw VALUES (1, 1, 1, 1);\n ----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\n\\ No newline at end of file\n+Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n\\ No newline at end of file\ndiff --git a/test/sql/storage/test_unique_index_checkpoint.test b/test/sql/storage/test_unique_index_checkpoint.test\nindex 27a5beacf404..8a0868a98439 100644\n--- a/test/sql/storage/test_unique_index_checkpoint.test\n+++ b/test/sql/storage/test_unique_index_checkpoint.test\n@@ -20,7 +20,7 @@ restart\n statement error\n INSERT INTO test VALUES (1,101),(2,201);\n ----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\n+Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n \n restart\n \n@@ -38,4 +38,4 @@ restart\n statement error\n INSERT INTO unique_index_test VALUES (1,101),(2,201);\n ----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\n\\ No newline at end of file\n+Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n\\ No newline at end of file\ndiff --git a/test/sql/storage/wal/wal_create_index.test b/test/sql/storage/wal/wal_create_index.test\nindex 9c9a3b684e74..aafbbebddd75 100644\n--- a/test/sql/storage/wal/wal_create_index.test\n+++ b/test/sql/storage/wal/wal_create_index.test\n@@ -45,7 +45,7 @@ logical_opt\t<REGEX>:.*INDEX_SCAN.*\n statement error\n INSERT INTO integers VALUES (1, 1);\n ----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\n+Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n \n restart\n \n@@ -97,7 +97,7 @@ SELECT i FROM integers WHERE i + j = 2\n statement error\n INSERT INTO integers VALUES (1, 1);\n ----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\n+Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n \n statement ok\n DROP INDEX i_index;\n@@ -141,7 +141,7 @@ SELECT i FROM integers WHERE j + i = 2\n statement error\n INSERT INTO integers VALUES (1, 1);\n ----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\n+Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n \n statement ok\n DROP INDEX i_index;\n@@ -164,7 +164,7 @@ restart\n statement error\n INSERT INTO integers VALUES (1, 1);\n ----\n-TransactionContext Error: Failed to commit: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\n+Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n \n statement ok\n DROP INDEX i_index;\n\\ No newline at end of file\ndiff --git a/test/sql/upsert/upsert_conflict_target.test b/test/sql/upsert/upsert_conflict_target.test\nindex 8f5afd5b227b..9a7248a18a7e 100644\n--- a/test/sql/upsert/upsert_conflict_target.test\n+++ b/test/sql/upsert/upsert_conflict_target.test\n@@ -17,7 +17,7 @@ create or replace table tbl (\n statement error\n insert into tbl VALUES (1,2,3), (1,2,3);\n ----\n-Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\n+Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicate key\n \n statement ok\n insert into tbl VALUES (1,2,3), (1,4,5);\n",
  "problem_statement": "Duplicate primary key error with sequences and large tables\n### What happens?\n\nWhen reading in a large CSV file into a table with a primary key generated by a sequence a duplicate key error is encountered.\r\n\r\n>Error: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\r\n\r\nMay be related to https://github.com/duckdb/duckdb/issues/6421.\r\n\r\nMy memory settings during reproduction\r\n```\r\n\u2502 memory_limit                 \u2502 54.9GB\r\n```\n\n### To Reproduce\n\nCreate a large CSV file. I used the following Python script\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nprint(\"A0\")\r\nfor x in range(1618111681):\r\n    print(\"\");\r\n```\r\n\r\nStart Duck and run the following\r\n```sql\r\ncreate sequence T1_sequence start 0 minvalue 0;\r\ncreate table T1 (ID integer not null default (nextval('T1_sequence')), A0 varchar, primary key (ID));\r\ncopy T1(A0) from 'data.csv' (delimiter ',', header);\r\n```\r\n\r\nThis will eventually cause the error\r\n>Error: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\r\n\r\nTrying to run a `SELECT *` on the table after the failure results in an out of memory error.\r\n\r\n```\r\nlibc++abi: terminating with uncaught exception of type duckdb::OutOfMemoryException: Out of Memory Error: failed to allocate data of size 32768\r\nDatabase is launched in in-memory mode and no temporary directory is specified.\r\nUnused blocks cannot be offloaded to disk.\r\n\r\nLaunch the database with a persistent storage back-end\r\nOr set PRAGMA temp_directory='/path/to/tmp.tmp'\r\n[1]    38402 abort      ./build/release/duckdb\r\n```\n\n### OS:\n\nMacOS M1, Linux x86_64\n\n### DuckDB Version:\n\n0.7.1\n\n### DuckDB Client:\n\nCLI, JDBC\n\n### Full Name:\n\nMichael Albers\n\n### Affiliation:\n\nMode Analytics\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nDuplicate primary key error with sequences and large tables\n### What happens?\n\nWhen reading in a large CSV file into a table with a primary key generated by a sequence a duplicate key error is encountered.\r\n\r\n>Error: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\r\n\r\nMay be related to https://github.com/duckdb/duckdb/issues/6421.\r\n\r\nMy memory settings during reproduction\r\n```\r\n\u2502 memory_limit                 \u2502 54.9GB\r\n```\n\n### To Reproduce\n\nCreate a large CSV file. I used the following Python script\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nprint(\"A0\")\r\nfor x in range(1618111681):\r\n    print(\"\");\r\n```\r\n\r\nStart Duck and run the following\r\n```sql\r\ncreate sequence T1_sequence start 0 minvalue 0;\r\ncreate table T1 (ID integer not null default (nextval('T1_sequence')), A0 varchar, primary key (ID));\r\ncopy T1(A0) from 'data.csv' (delimiter ',', header);\r\n```\r\n\r\nThis will eventually cause the error\r\n>Error: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key\r\n\r\nTrying to run a `SELECT *` on the table after the failure results in an out of memory error.\r\n\r\n```\r\nlibc++abi: terminating with uncaught exception of type duckdb::OutOfMemoryException: Out of Memory Error: failed to allocate data of size 32768\r\nDatabase is launched in in-memory mode and no temporary directory is specified.\r\nUnused blocks cannot be offloaded to disk.\r\n\r\nLaunch the database with a persistent storage back-end\r\nOr set PRAGMA temp_directory='/path/to/tmp.tmp'\r\n[1]    38402 abort      ./build/release/duckdb\r\n```\n\n### OS:\n\nMacOS M1, Linux x86_64\n\n### DuckDB Version:\n\n0.7.1\n\n### DuckDB Client:\n\nCLI, JDBC\n\n### Full Name:\n\nMichael Albers\n\n### Affiliation:\n\nMode Analytics\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "\n",
  "created_at": "2023-03-12T22:02:02Z",
  "modified_files": [
    "src/execution/index/art/art.cpp",
    "src/include/duckdb/execution/index/art/art.hpp",
    "src/include/duckdb/storage/data_table.hpp",
    "src/include/duckdb/storage/index.hpp",
    "src/include/duckdb/transaction/local_storage.hpp",
    "src/storage/data_table.cpp",
    "src/storage/index.cpp",
    "src/storage/local_storage.cpp"
  ],
  "modified_test_files": [
    "test/fuzzer/pedro/buffer_manager_resize_issue.test",
    "test/fuzzer/pedro/create_index_error.test",
    "test/sql/index/art/test_art_fuzzer_issues.test",
    "test/sql/index/art/test_art_import_export.test",
    "test/sql/storage/test_unique_index_checkpoint.test",
    "test/sql/storage/wal/wal_create_index.test",
    "test/sql/upsert/upsert_conflict_target.test"
  ]
}