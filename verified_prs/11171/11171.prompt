You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
list_reduce gives wrong results
### What happens?

`list_reduce` sometimes gives wrong results.

### To Reproduce

```sql
CREATE OR REPLACE TABLE df(a int, b int);
INSERT INTO df VALUES (0, 0), (0, 1), (0, 2);
```



```sql
SELECT 
list_reduce(
    list(struct_pack(a, b) ORDER BY b),
    (state, next) -> struct_pack(a:=state.a + (next.b - state.b), b:=next.b)
)
FROM df
```
returns
```
┌──────────────────────────────┐
│           reduced            │
│ struct(a integer, b integer) │
├──────────────────────────────┤
│ {'a': 1, 'b': 2}             │
└──────────────────────────────┘
```
but should return the last entry of 
```sql
SELECT 
list_reduce(
    list(struct_pack(a, b)) OVER (ORDER BY b),
    (state, next) -> struct_pack(a:=state.a + (next.b - state.b), b:=next.b)
)
FROM df
```
```
┌──────────────────────────────┐
│           reduced            │
│ struct(a integer, b integer) │
├──────────────────────────────┤
│ {'a': 0, 'b': 0}             │
│ {'a': 1, 'b': 1}             │
│ {'a': 2, 'b': 2}             │
└──────────────────────────────┘
```

(PS: Check https://github.com/duckdb/duckdb/discussions/10703 for what I think would be a better (and faster) future for many uses of `reduce`)



### OS:

Linux

### DuckDB Version:

'0.10.1-dev717'

### DuckDB Client:

Python

### Full Name:

Soeren Wolfers

### Affiliation:

G-Research

### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?

I have tested with a nightly build

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/logo-dl/DuckDB_Logo-stacked.svg" height="120">
3: </div>
4: <br>
5: 
6: 
7: 
8: 
9: <p align="center">
10:   <a href="https://github.com/duckdb/duckdb/actions">
11:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge">
12:   </a>
13:   <a href="https://discord.gg/tcvwpjfnZx">
14:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
15:   </a>
16:   <a href="https://github.com/duckdb/duckdb/releases/">
17:     <img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release">
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
44: 
45: ## Support
46: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/core_functions/scalar/list/list_reduce.cpp]
1: #include "duckdb/core_functions/scalar/list_functions.hpp"
2: #include "duckdb/core_functions/lambda_functions.hpp"
3: #include "duckdb/planner/expression/bound_cast_expression.hpp"
4: #include "duckdb/planner/expression/bound_function_expression.hpp"
5: 
6: namespace duckdb {
7: 
8: struct ReduceExecuteInfo {
9: 	ReduceExecuteInfo(LambdaFunctions::LambdaInfo &info, ClientContext &context) : left_slice(*info.child_vector) {
10: 		SelectionVector left_vector(info.row_count);
11: 		active_rows.Resize(0, info.row_count);
12: 		active_rows.SetAllValid(info.row_count);
13: 
14: 		right_sel.Initialize(info.row_count);
15: 		left_sel.Initialize(info.row_count);
16: 		active_rows_sel.Initialize(info.row_count);
17: 
18: 		idx_t reduced_row_idx = 0;
19: 
20: 		for (idx_t original_row_idx = 0; original_row_idx < info.row_count; original_row_idx++) {
21: 			auto list_column_format_index = info.list_column_format.sel->get_index(original_row_idx);
22: 			if (info.list_column_format.validity.RowIsValid(list_column_format_index)) {
23: 				if (info.list_entries[list_column_format_index].length == 0) {
24: 					throw ParameterNotAllowedException("Cannot perform list_reduce on an empty input list");
25: 				}
26: 				left_vector.set_index(reduced_row_idx, info.list_entries[list_column_format_index].offset);
27: 				reduced_row_idx++;
28: 			} else {
29: 				// Remove the invalid rows
30: 				info.result_validity->SetInvalid(original_row_idx);
31: 				active_rows.SetInvalid(original_row_idx);
32: 			}
33: 		}
34: 
35: 		left_slice.Slice(left_vector, reduced_row_idx);
36: 
37: 		if (info.has_index) {
38: 			input_types.push_back(LogicalType::BIGINT);
39: 		}
40: 		input_types.push_back(left_slice.GetType());
41: 		input_types.push_back(left_slice.GetType());
42: 		for (auto &entry : info.column_infos) {
43: 			input_types.push_back(entry.vector.get().GetType());
44: 		}
45: 
46: 		expr_executor = make_uniq<ExpressionExecutor>(context, *info.lambda_expr);
47: 	};
48: 	ValidityMask active_rows;
49: 	Vector left_slice;
50: 	unique_ptr<ExpressionExecutor> expr_executor;
51: 	vector<LogicalType> input_types;
52: 
53: 	SelectionVector right_sel;
54: 	SelectionVector left_sel;
55: 	SelectionVector active_rows_sel;
56: };
57: 
58: static bool ExecuteReduce(idx_t loops, ReduceExecuteInfo &execute_info, LambdaFunctions::LambdaInfo &info,
59:                           DataChunk &result_chunk) {
60: 	idx_t original_row_idx = 0;
61: 	idx_t reduced_row_idx = 0;
62: 	idx_t valid_row_idx = 0;
63: 
64: 	// create selection vectors for the left and right slice
65: 	auto data = execute_info.active_rows.GetData();
66: 
67: 	idx_t bits_per_entry = sizeof(idx_t) * 8;
68: 	for (idx_t entry_idx = 0; original_row_idx < info.row_count; entry_idx++) {
69: 		if (data[entry_idx] == 0) {
70: 			original_row_idx += bits_per_entry;
71: 			continue;
72: 		}
73: 
74: 		for (idx_t j = 0; entry_idx * bits_per_entry + j < info.row_count; j++) {
75: 			if (!execute_info.active_rows.RowIsValid(original_row_idx)) {
76: 				original_row_idx++;
77: 				continue;
78: 			}
79: 			auto list_column_format_index = info.list_column_format.sel->get_index(original_row_idx);
80: 			if (info.list_entries[list_column_format_index].length > loops + 1) {
81: 				execute_info.right_sel.set_index(reduced_row_idx,
82: 				                                 info.list_entries[list_column_format_index].offset + loops + 1);
83: 				execute_info.left_sel.set_index(reduced_row_idx, valid_row_idx);
84: 				execute_info.active_rows_sel.set_index(reduced_row_idx, original_row_idx);
85: 
86: 				reduced_row_idx++;
87: 			} else {
88: 				execute_info.active_rows.SetInvalid(original_row_idx);
89: 				info.result.SetValue(original_row_idx, execute_info.left_slice.GetValue(valid_row_idx));
90: 			}
91: 			original_row_idx++;
92: 			valid_row_idx++;
93: 		}
94: 	}
95: 
96: 	if (reduced_row_idx == 0) {
97: 		return true;
98: 	}
99: 
100: 	// create the index vector
101: 	Vector index_vector(Value::BIGINT(loops + 1));
102: 
103: 	// slice the left and right slice
104: 	execute_info.left_slice.Slice(execute_info.left_slice, execute_info.left_sel, reduced_row_idx);
105: 	Vector right_slice(*info.child_vector, execute_info.right_sel, reduced_row_idx);
106: 
107: 	// create the input chunk
108: 	DataChunk input_chunk;
109: 	input_chunk.InitializeEmpty(execute_info.input_types);
110: 	input_chunk.SetCardinality(reduced_row_idx);
111: 
112: 	idx_t slice_offset = info.has_index ? 1 : 0;
113: 	if (info.has_index) {
114: 		input_chunk.data[0].Reference(index_vector);
115: 	}
116: 	input_chunk.data[slice_offset + 1].Reference(execute_info.left_slice);
117: 	input_chunk.data[slice_offset].Reference(right_slice);
118: 
119: 	// add the other columns
120: 	vector<Vector> slices;
121: 	for (idx_t i = 0; i < info.column_infos.size(); i++) {
122: 		if (info.column_infos[i].vector.get().GetVectorType() == VectorType::CONSTANT_VECTOR) {
123: 			// only reference constant vectors
124: 			input_chunk.data[slice_offset + 2 + i].Reference(info.column_infos[i].vector);
125: 		} else {
126: 			// slice the other vectors
127: 			slices.emplace_back(info.column_infos[i].vector, execute_info.active_rows_sel, reduced_row_idx);
128: 			input_chunk.data[slice_offset + 2 + i].Reference(slices.back());
129: 		}
130: 	}
131: 
132: 	result_chunk.Reset();
133: 	result_chunk.SetCardinality(reduced_row_idx);
134: 
135: 	execute_info.expr_executor->Execute(input_chunk, result_chunk);
136: 
137: 	// use the result chunk to update the left slice
138: 	execute_info.left_slice.Reference(result_chunk.data[0]);
139: 	return false;
140: }
141: 
142: void LambdaFunctions::ListReduceFunction(duckdb::DataChunk &args, duckdb::ExpressionState &state,
143:                                          duckdb::Vector &result) {
144: 	// Initializes the left slice from the list entries, active rows, the expression executor and the input types
145: 	bool completed = false;
146: 	LambdaFunctions::LambdaInfo info(args, state, result, completed);
147: 	if (completed) {
148: 		return;
149: 	}
150: 
151: 	ReduceExecuteInfo execute_info(info, state.GetContext());
152: 
153: 	// Since the left slice references the result chunk, we need to create two result chunks.
154: 	// This means there is always an empty result chunk for the next iteration,
155: 	// without the referenced chunk having to be reset until the current iteration is complete.
156: 	DataChunk odd_result_chunk;
157: 	odd_result_chunk.Initialize(Allocator::DefaultAllocator(), {info.lambda_expr->return_type});
158: 
159: 	DataChunk even_result_chunk;
160: 	even_result_chunk.Initialize(Allocator::DefaultAllocator(), {info.lambda_expr->return_type});
161: 
162: 	idx_t loops = 0;
163: 	bool end = false;
164: 	// Execute reduce until all rows are finished
165: 	while (!end) {
166: 		auto &result_chunk = loops % 2 ? odd_result_chunk : even_result_chunk;
167: 		auto &spare_result_chunk = loops % 2 ? even_result_chunk : odd_result_chunk;
168: 
169: 		end = ExecuteReduce(loops, execute_info, info, result_chunk);
170: 		spare_result_chunk.Reset();
171: 
172: 		loops++;
173: 	}
174: 
175: 	if (info.is_all_constant && !info.is_volatile) {
176: 		info.result.SetVectorType(VectorType::CONSTANT_VECTOR);
177: 	}
178: }
179: 
180: static unique_ptr<FunctionData> ListReduceBind(ClientContext &context, ScalarFunction &bound_function,
181:                                                vector<unique_ptr<Expression>> &arguments) {
182: 
183: 	// the list column and the bound lambda expression
184: 	D_ASSERT(arguments.size() == 2);
185: 	if (arguments[1]->expression_class != ExpressionClass::BOUND_LAMBDA) {
186: 		throw BinderException("Invalid lambda expression!");
187: 	}
188: 
189: 	arguments[0] = BoundCastExpression::AddArrayCastToList(context, std::move(arguments[0]));
190: 
191: 	auto &bound_lambda_expr = arguments[1]->Cast<BoundLambdaExpression>();
192: 	if (bound_lambda_expr.parameter_count < 2 || bound_lambda_expr.parameter_count > 3) {
193: 		throw BinderException("list_reduce expects a function with 2 or 3 arguments");
194: 	}
195: 	auto has_index = bound_lambda_expr.parameter_count == 3;
196: 
197: 	unique_ptr<FunctionData> bind_data = LambdaFunctions::ListLambdaPrepareBind(arguments, context, bound_function);
198: 	if (bind_data) {
199: 		return bind_data;
200: 	}
201: 
202: 	auto list_child_type = arguments[0]->return_type;
203: 	list_child_type = ListType::GetChildType(list_child_type);
204: 
205: 	auto cast_lambda_expr =
206: 	    BoundCastExpression::AddCastToType(context, std::move(bound_lambda_expr.lambda_expr), list_child_type, false);
207: 	if (!cast_lambda_expr) {
208: 		throw BinderException("Could not cast lambda expression to list child type");
209: 	}
210: 	bound_function.return_type = cast_lambda_expr->return_type;
211: 	return make_uniq<ListLambdaBindData>(bound_function.return_type, std::move(cast_lambda_expr), has_index);
212: }
213: 
214: static LogicalType ListReduceBindLambda(const idx_t parameter_idx, const LogicalType &list_child_type) {
215: 	return LambdaFunctions::BindTernaryLambda(parameter_idx, list_child_type);
216: }
217: 
218: ScalarFunction ListReduceFun::GetFunction() {
219: 	ScalarFunction fun({LogicalType::LIST(LogicalType::ANY), LogicalType::LAMBDA}, LogicalType::ANY,
220: 	                   LambdaFunctions::ListReduceFunction, ListReduceBind, nullptr, nullptr);
221: 
222: 	fun.null_handling = FunctionNullHandling::SPECIAL_HANDLING;
223: 	fun.serialize = ListLambdaBindData::Serialize;
224: 	fun.deserialize = ListLambdaBindData::Deserialize;
225: 	fun.bind_lambda = ListReduceBindLambda;
226: 
227: 	return fun;
228: }
229: 
230: } // namespace duckdb
[end of src/core_functions/scalar/list/list_reduce.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: