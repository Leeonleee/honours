You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Features request - allow `IGNORE_ERRORS` parameters in `read_csv_auto` in cli mode
# Summary

allow `IGNORE_ERRORS` parameter in function `read_csv_auto`  such as 

```SQL
SELECT * FROM read_csv_auto('some.csv', IGNORE_ERRORS=1);
```

# Justification

#3464 allows `IGNORE_ERRORS` in csv reading functions, this feature could benefits greatly for those would like quick&dirty analysis on data without clean up the csv file first.

Features request - allow `IGNORE_ERRORS` parameters in `read_csv_auto` in cli mode
# Summary

allow `IGNORE_ERRORS` parameter in function `read_csv_auto`  such as 

```SQL
SELECT * FROM read_csv_auto('some.csv', IGNORE_ERRORS=1);
```

# Justification

#3464 allows `IGNORE_ERRORS` in csv reading functions, this feature could benefits greatly for those would like quick&dirty analysis on data without clean up the csv file first.


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of src/function/table/read_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/main/database.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/common/hive_partitioning.hpp"
8: #include "duckdb/main/config.hpp"
9: #include "duckdb/parser/expression/constant_expression.hpp"
10: #include "duckdb/parser/expression/function_expression.hpp"
11: #include "duckdb/parser/tableref/table_function_ref.hpp"
12: #include "duckdb/planner/operator/logical_get.hpp"
13: 
14: #include <limits>
15: 
16: namespace duckdb {
17: 
18: static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, TableFunctionBindInput &input,
19:                                             vector<LogicalType> &return_types, vector<string> &names) {
20: 	auto &config = DBConfig::GetConfig(context);
21: 	if (!config.options.enable_external_access) {
22: 		throw PermissionException("Scanning CSV files is disabled through configuration");
23: 	}
24: 	auto result = make_unique<ReadCSVData>();
25: 	auto &options = result->options;
26: 
27: 	auto &file_pattern = StringValue::Get(input.inputs[0]);
28: 
29: 	auto &fs = FileSystem::GetFileSystem(context);
30: 	result->files = fs.Glob(file_pattern, context);
31: 	if (result->files.empty()) {
32: 		throw IOException("No files found that match the pattern \"%s\"", file_pattern);
33: 	}
34: 
35: 	for (auto &kv : input.named_parameters) {
36: 		auto loption = StringUtil::Lower(kv.first);
37: 		if (loption == "columns") {
38: 			auto &child_type = kv.second.type();
39: 			if (child_type.id() != LogicalTypeId::STRUCT) {
40: 				throw BinderException("read_csv columns requires a struct as input");
41: 			}
42: 			auto &struct_children = StructValue::GetChildren(kv.second);
43: 			D_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());
44: 			for (idx_t i = 0; i < struct_children.size(); i++) {
45: 				auto &name = StructType::GetChildName(child_type, i);
46: 				auto &val = struct_children[i];
47: 				names.push_back(name);
48: 				if (val.type().id() != LogicalTypeId::VARCHAR) {
49: 					throw BinderException("read_csv requires a type specification as string");
50: 				}
51: 				return_types.emplace_back(TransformStringToLogicalType(StringValue::Get(val)));
52: 			}
53: 			if (names.empty()) {
54: 				throw BinderException("read_csv requires at least a single column as input!");
55: 			}
56: 		} else if (loption == "all_varchar") {
57: 			options.all_varchar = BooleanValue::Get(kv.second);
58: 		} else if (loption == "normalize_names") {
59: 			options.normalize_names = BooleanValue::Get(kv.second);
60: 		} else if (loption == "filename") {
61: 			options.include_file_name = BooleanValue::Get(kv.second);
62: 		} else if (loption == "hive_partitioning") {
63: 			options.include_parsed_hive_partitions = BooleanValue::Get(kv.second);
64: 		} else {
65: 			options.SetReadOption(loption, kv.second, names);
66: 		}
67: 	}
68: 	if (!options.auto_detect && return_types.empty()) {
69: 		throw BinderException("read_csv requires columns to be specified. Use read_csv_auto or set read_csv(..., "
70: 		                      "AUTO_DETECT=TRUE) to automatically guess columns.");
71: 	}
72: 	if (options.auto_detect) {
73: 		options.file_path = result->files[0];
74: 		auto initial_reader = make_unique<BufferedCSVReader>(context, options);
75: 
76: 		return_types.assign(initial_reader->sql_types.begin(), initial_reader->sql_types.end());
77: 		if (names.empty()) {
78: 			names.assign(initial_reader->col_names.begin(), initial_reader->col_names.end());
79: 		} else {
80: 			D_ASSERT(return_types.size() == names.size());
81: 		}
82: 		result->initial_reader = move(initial_reader);
83: 	} else {
84: 		result->sql_types = return_types;
85: 		D_ASSERT(return_types.size() == names.size());
86: 	}
87: 	if (result->options.include_file_name) {
88: 		result->filename_col_idx = names.size();
89: 		return_types.emplace_back(LogicalType::VARCHAR);
90: 		names.emplace_back("filename");
91: 	}
92: 
93: 	if (result->options.include_parsed_hive_partitions) {
94: 		auto partitions = HivePartitioning::Parse(result->files[0]);
95: 		result->hive_partition_col_idx = names.size();
96: 		for (auto &part : partitions) {
97: 			return_types.emplace_back(LogicalType::VARCHAR);
98: 			names.emplace_back(part.first);
99: 		}
100: 	}
101: 	result->options.names = names;
102: 	return move(result);
103: }
104: 
105: struct ReadCSVOperatorData : public GlobalTableFunctionState {
106: 	//! The CSV reader
107: 	unique_ptr<BufferedCSVReader> csv_reader;
108: 	//! The index of the next file to read (i.e. current file + 1)
109: 	idx_t file_index;
110: 	//! Total File Size
111: 	idx_t file_size;
112: 	//! How many bytes were read up to this point
113: 	atomic<idx_t> bytes_read;
114: };
115: 
116: static unique_ptr<GlobalTableFunctionState> ReadCSVInit(ClientContext &context, TableFunctionInitInput &input) {
117: 	auto &bind_data = (ReadCSVData &)*input.bind_data;
118: 	auto result = make_unique<ReadCSVOperatorData>();
119: 	if (bind_data.initial_reader) {
120: 		result->csv_reader = move(bind_data.initial_reader);
121: 	} else if (bind_data.files.empty()) {
122: 		// This can happen when a filename based filter pushdown has eliminated all possible files for this scan.
123: 		return move(result);
124: 	} else {
125: 		bind_data.options.file_path = bind_data.files[0];
126: 		result->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);
127: 	}
128: 	result->file_size = result->csv_reader->GetFileSize();
129: 	result->file_index = 1;
130: 	return move(result);
131: }
132: 
133: static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, TableFunctionBindInput &input,
134:                                                 vector<LogicalType> &return_types, vector<string> &names) {
135: 	input.named_parameters["auto_detect"] = Value::BOOLEAN(true);
136: 	return ReadCSVBind(context, input, return_types, names);
137: }
138: 
139: static void ReadCSVFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {
140: 	auto &bind_data = (ReadCSVData &)*data_p.bind_data;
141: 	auto &data = (ReadCSVOperatorData &)*data_p.global_state;
142: 
143: 	if (!data.csv_reader) {
144: 		// no csv_reader was set, this can happen when a filename-based filter has filtered out all possible files
145: 		return;
146: 	}
147: 
148: 	do {
149: 		data.csv_reader->ParseCSV(output);
150: 		data.bytes_read = data.csv_reader->bytes_in_chunk;
151: 		if (output.size() == 0 && data.file_index < bind_data.files.size()) {
152: 			// exhausted this file, but we have more files we can read
153: 			// open the next file and increment the counter
154: 			bind_data.options.file_path = bind_data.files[data.file_index];
155: 			data.csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, data.csv_reader->sql_types);
156: 			data.file_index++;
157: 		} else {
158: 			break;
159: 		}
160: 	} while (true);
161: 
162: 	if (bind_data.options.include_file_name) {
163: 		auto &col = output.data[bind_data.filename_col_idx];
164: 		col.SetValue(0, Value(data.csv_reader->options.file_path));
165: 		col.SetVectorType(VectorType::CONSTANT_VECTOR);
166: 	}
167: 	if (bind_data.options.include_parsed_hive_partitions) {
168: 		auto partitions = HivePartitioning::Parse(data.csv_reader->options.file_path);
169: 
170: 		idx_t i = bind_data.hive_partition_col_idx;
171: 
172: 		if (partitions.size() != (bind_data.options.names.size() - bind_data.hive_partition_col_idx)) {
173: 			throw IOException("Hive partition count mismatch, expected " +
174: 			                  std::to_string(bind_data.options.names.size() - bind_data.hive_partition_col_idx) +
175: 			                  " hive partitions, got " + std::to_string(partitions.size()) + "\n");
176: 		}
177: 
178: 		for (auto &part : partitions) {
179: 			if (bind_data.options.names[i] != part.first) {
180: 				throw IOException("Hive partition names mismatch, expected '" + bind_data.options.names[i] +
181: 				                  "' but found '" + part.first + "' for file '" + data.csv_reader->options.file_path +
182: 				                  "'");
183: 			}
184: 			auto &col = output.data[i++];
185: 			col.SetValue(0, Value(part.second));
186: 			col.SetVectorType(VectorType::CONSTANT_VECTOR);
187: 		}
188: 	}
189: }
190: 
191: static void ReadCSVAddNamedParameters(TableFunction &table_function) {
192: 	table_function.named_parameters["sep"] = LogicalType::VARCHAR;
193: 	table_function.named_parameters["delim"] = LogicalType::VARCHAR;
194: 	table_function.named_parameters["quote"] = LogicalType::VARCHAR;
195: 	table_function.named_parameters["escape"] = LogicalType::VARCHAR;
196: 	table_function.named_parameters["nullstr"] = LogicalType::VARCHAR;
197: 	table_function.named_parameters["columns"] = LogicalType::ANY;
198: 	table_function.named_parameters["header"] = LogicalType::BOOLEAN;
199: 	table_function.named_parameters["auto_detect"] = LogicalType::BOOLEAN;
200: 	table_function.named_parameters["sample_size"] = LogicalType::BIGINT;
201: 	table_function.named_parameters["sample_chunk_size"] = LogicalType::BIGINT;
202: 	table_function.named_parameters["sample_chunks"] = LogicalType::BIGINT;
203: 	table_function.named_parameters["all_varchar"] = LogicalType::BOOLEAN;
204: 	table_function.named_parameters["dateformat"] = LogicalType::VARCHAR;
205: 	table_function.named_parameters["timestampformat"] = LogicalType::VARCHAR;
206: 	table_function.named_parameters["normalize_names"] = LogicalType::BOOLEAN;
207: 	table_function.named_parameters["compression"] = LogicalType::VARCHAR;
208: 	table_function.named_parameters["filename"] = LogicalType::BOOLEAN;
209: 	table_function.named_parameters["hive_partitioning"] = LogicalType::BOOLEAN;
210: 	table_function.named_parameters["skip"] = LogicalType::BIGINT;
211: 	table_function.named_parameters["max_line_size"] = LogicalType::VARCHAR;
212: 	table_function.named_parameters["maximum_line_size"] = LogicalType::VARCHAR;
213: }
214: 
215: double CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p,
216:                          const GlobalTableFunctionState *global_state) {
217: 	auto &data = (const ReadCSVOperatorData &)*global_state;
218: 	if (data.file_size == 0) {
219: 		return 100;
220: 	}
221: 	auto percentage = (data.bytes_read * 100.0) / data.file_size;
222: 	return percentage;
223: }
224: 
225: void CSVComplexFilterPushdown(ClientContext &context, LogicalGet &get, FunctionData *bind_data_p,
226:                               vector<unique_ptr<Expression>> &filters) {
227: 	auto data = (ReadCSVData *)bind_data_p;
228: 
229: 	if (data->options.include_parsed_hive_partitions || data->options.include_file_name) {
230: 		string first_file = data->files[0];
231: 
232: 		unordered_map<string, column_t> column_map;
233: 		for (idx_t i = 0; i < get.column_ids.size(); i++) {
234: 			column_map.insert({get.names[get.column_ids[i]], i});
235: 		}
236: 
237: 		HivePartitioning::ApplyFiltersToFileList(data->files, filters, column_map, get.table_index,
238: 		                                         data->options.include_parsed_hive_partitions,
239: 		                                         data->options.include_file_name);
240: 
241: 		if (data->files.empty() || data->files[0] != first_file) {
242: 			data->initial_reader.reset();
243: 		}
244: 	}
245: }
246: 
247: void BufferedCSVReaderOptions::Serialize(FieldWriter &writer) const {
248: 	// common options
249: 	writer.WriteField<bool>(has_delimiter);
250: 	writer.WriteString(delimiter);
251: 	writer.WriteField<bool>(has_quote);
252: 	writer.WriteString(quote);
253: 	writer.WriteField<bool>(has_escape);
254: 	writer.WriteString(escape);
255: 	writer.WriteField<bool>(has_header);
256: 	writer.WriteField<bool>(header);
257: 	writer.WriteField<bool>(ignore_errors);
258: 	writer.WriteField<idx_t>(num_cols);
259: 	writer.WriteField<idx_t>(buffer_size);
260: 	writer.WriteString(null_str);
261: 	writer.WriteField<FileCompressionType>(compression);
262: 	// read options
263: 	writer.WriteList<string>(names);
264: 	writer.WriteField<idx_t>(skip_rows);
265: 	writer.WriteField<idx_t>(maximum_line_size);
266: 	writer.WriteField<bool>(normalize_names);
267: 	writer.WriteListNoReference<bool>(force_not_null);
268: 	writer.WriteField<bool>(all_varchar);
269: 	writer.WriteField<idx_t>(sample_chunk_size);
270: 	writer.WriteField<idx_t>(sample_chunks);
271: 	writer.WriteField<bool>(auto_detect);
272: 	writer.WriteString(file_path);
273: 	writer.WriteField<bool>(include_file_name);
274: 	writer.WriteField<bool>(include_parsed_hive_partitions);
275: 	// write options
276: 	writer.WriteListNoReference<bool>(force_quote);
277: }
278: 
279: void BufferedCSVReaderOptions::Deserialize(FieldReader &reader) {
280: 	// common options
281: 	has_delimiter = reader.ReadRequired<bool>();
282: 	delimiter = reader.ReadRequired<string>();
283: 	has_quote = reader.ReadRequired<bool>();
284: 	quote = reader.ReadRequired<string>();
285: 	has_escape = reader.ReadRequired<bool>();
286: 	escape = reader.ReadRequired<string>();
287: 	has_header = reader.ReadRequired<bool>();
288: 	header = reader.ReadRequired<bool>();
289: 	ignore_errors = reader.ReadRequired<bool>();
290: 	num_cols = reader.ReadRequired<idx_t>();
291: 	buffer_size = reader.ReadRequired<idx_t>();
292: 	null_str = reader.ReadRequired<string>();
293: 	compression = reader.ReadRequired<FileCompressionType>();
294: 	// read options
295: 	names = reader.ReadRequiredList<string>();
296: 	skip_rows = reader.ReadRequired<idx_t>();
297: 	maximum_line_size = reader.ReadRequired<idx_t>();
298: 	normalize_names = reader.ReadRequired<bool>();
299: 	force_not_null = reader.ReadRequiredList<bool>();
300: 	all_varchar = reader.ReadRequired<bool>();
301: 	sample_chunk_size = reader.ReadRequired<idx_t>();
302: 	sample_chunks = reader.ReadRequired<idx_t>();
303: 	auto_detect = reader.ReadRequired<bool>();
304: 	file_path = reader.ReadRequired<string>();
305: 	include_file_name = reader.ReadRequired<bool>();
306: 	include_parsed_hive_partitions = reader.ReadRequired<bool>();
307: 	// write options
308: 	force_quote = reader.ReadRequiredList<bool>();
309: }
310: 
311: static void CSVReaderSerialize(FieldWriter &writer, const FunctionData *bind_data_p, const TableFunction &function) {
312: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
313: 	writer.WriteList<string>(bind_data.files);
314: 	writer.WriteRegularSerializableList<LogicalType>(bind_data.sql_types);
315: 	writer.WriteField<idx_t>(bind_data.filename_col_idx);
316: 	writer.WriteField<idx_t>(bind_data.hive_partition_col_idx);
317: 	bind_data.options.Serialize(writer);
318: }
319: 
320: static unique_ptr<FunctionData> CSVReaderDeserialize(ClientContext &context, FieldReader &reader,
321:                                                      TableFunction &function) {
322: 	auto result_data = make_unique<ReadCSVData>();
323: 	result_data->files = reader.ReadRequiredList<string>();
324: 	result_data->sql_types = reader.ReadRequiredSerializableList<LogicalType, LogicalType>();
325: 	result_data->filename_col_idx = reader.ReadRequired<idx_t>();
326: 	result_data->hive_partition_col_idx = reader.ReadRequired<idx_t>();
327: 	result_data->options.Deserialize(reader);
328: 	return move(result_data);
329: }
330: 
331: TableFunction ReadCSVTableFunction::GetFunction() {
332: 	TableFunction read_csv("read_csv", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVBind, ReadCSVInit);
333: 	read_csv.table_scan_progress = CSVReaderProgress;
334: 	read_csv.pushdown_complex_filter = CSVComplexFilterPushdown;
335: 	read_csv.serialize = CSVReaderSerialize;
336: 	read_csv.deserialize = CSVReaderDeserialize;
337: 	ReadCSVAddNamedParameters(read_csv);
338: 	return read_csv;
339: }
340: 
341: void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {
342: 	set.AddFunction(ReadCSVTableFunction::GetFunction());
343: 
344: 	TableFunction read_csv_auto("read_csv_auto", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVAutoBind, ReadCSVInit);
345: 	read_csv_auto.table_scan_progress = CSVReaderProgress;
346: 	read_csv_auto.pushdown_complex_filter = CSVComplexFilterPushdown;
347: 	read_csv_auto.serialize = CSVReaderSerialize;
348: 	read_csv_auto.deserialize = CSVReaderDeserialize;
349: 	ReadCSVAddNamedParameters(read_csv_auto);
350: 	set.AddFunction(read_csv_auto);
351: }
352: 
353: unique_ptr<TableFunctionRef> ReadCSVReplacement(ClientContext &context, const string &table_name,
354:                                                 ReplacementScanData *data) {
355: 	auto lower_name = StringUtil::Lower(table_name);
356: 	// remove any compression
357: 	if (StringUtil::EndsWith(lower_name, ".gz")) {
358: 		lower_name = lower_name.substr(0, lower_name.size() - 3);
359: 	} else if (StringUtil::EndsWith(lower_name, ".zst")) {
360: 		lower_name = lower_name.substr(0, lower_name.size() - 4);
361: 	}
362: 	if (!StringUtil::EndsWith(lower_name, ".csv") && !StringUtil::EndsWith(lower_name, ".tsv")) {
363: 		return nullptr;
364: 	}
365: 	auto table_function = make_unique<TableFunctionRef>();
366: 	vector<unique_ptr<ParsedExpression>> children;
367: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
368: 	table_function->function = make_unique<FunctionExpression>("read_csv_auto", move(children));
369: 	return table_function;
370: }
371: 
372: void BuiltinFunctions::RegisterReadFunctions() {
373: 	CSVCopyFunction::RegisterFunction(*this);
374: 	ReadCSVTableFunction::RegisterFunction(*this);
375: 	auto &config = DBConfig::GetConfig(context);
376: 	config.replacement_scans.emplace_back(ReadCSVReplacement);
377: }
378: 
379: } // namespace duckdb
[end of src/function/table/read_csv.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: