You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Duplicate primary key error with sequences and large tables
### What happens?

When reading in a large CSV file into a table with a primary key generated by a sequence a duplicate key error is encountered.

>Error: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key

May be related to https://github.com/duckdb/duckdb/issues/6421.

My memory settings during reproduction
```
│ memory_limit                 │ 54.9GB
```

### To Reproduce

Create a large CSV file. I used the following Python script
```python
#!/usr/bin/env python3

print("A0")
for x in range(1618111681):
    print("");
```

Start Duck and run the following
```sql
create sequence T1_sequence start 0 minvalue 0;
create table T1 (ID integer not null default (nextval('T1_sequence')), A0 varchar, primary key (ID));
copy T1(A0) from 'data.csv' (delimiter ',', header);
```

This will eventually cause the error
>Error: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key

Trying to run a `SELECT *` on the table after the failure results in an out of memory error.

```
libc++abi: terminating with uncaught exception of type duckdb::OutOfMemoryException: Out of Memory Error: failed to allocate data of size 32768
Database is launched in in-memory mode and no temporary directory is specified.
Unused blocks cannot be offloaded to disk.

Launch the database with a persistent storage back-end
Or set PRAGMA temp_directory='/path/to/tmp.tmp'
[1]    38402 abort      ./build/release/duckdb
```

### OS:

MacOS M1, Linux x86_64

### DuckDB Version:

0.7.1

### DuckDB Client:

CLI, JDBC

### Full Name:

Michael Albers

### Affiliation:

Mode Analytics

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree
Duplicate primary key error with sequences and large tables
### What happens?

When reading in a large CSV file into a table with a primary key generated by a sequence a duplicate key error is encountered.

>Error: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key

May be related to https://github.com/duckdb/duckdb/issues/6421.

My memory settings during reproduction
```
│ memory_limit                 │ 54.9GB
```

### To Reproduce

Create a large CSV file. I used the following Python script
```python
#!/usr/bin/env python3

print("A0")
for x in range(1618111681):
    print("");
```

Start Duck and run the following
```sql
create sequence T1_sequence start 0 minvalue 0;
create table T1 (ID integer not null default (nextval('T1_sequence')), A0 varchar, primary key (ID));
copy T1(A0) from 'data.csv' (delimiter ',', header);
```

This will eventually cause the error
>Error: Constraint Error: PRIMARY KEY or UNIQUE constraint violated: duplicated key

Trying to run a `SELECT *` on the table after the failure results in an out of memory error.

```
libc++abi: terminating with uncaught exception of type duckdb::OutOfMemoryException: Out of Memory Error: failed to allocate data of size 32768
Database is launched in in-memory mode and no temporary directory is specified.
Unused blocks cannot be offloaded to disk.

Launch the database with a persistent storage back-end
Or set PRAGMA temp_directory='/path/to/tmp.tmp'
[1]    38402 abort      ./build/release/duckdb
```

### OS:

MacOS M1, Linux x86_64

### DuckDB Version:

0.7.1

### DuckDB Client:

CLI, JDBC

### Full Name:

Michael Albers

### Affiliation:

Mode Analytics

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
11:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
12:   </a>
13:   <a href="https://discord.gg/tcvwpjfnZx">
14:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
15:   </a>
16:   <a href="https://github.com/duckdb/duckdb/releases/">
17:     <img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release">
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of src/execution/index/art/art.cpp]
1: #include "duckdb/execution/index/art/art.hpp"
2: 
3: #include "duckdb/common/radix.hpp"
4: #include "duckdb/common/vector_operations/vector_operations.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: #include "duckdb/storage/arena_allocator.hpp"
7: #include "duckdb/execution/index/art/art_key.hpp"
8: #include "duckdb/common/types/conflict_manager.hpp"
9: 
10: #include <algorithm>
11: #include <cstring>
12: #include <ctgmath>
13: 
14: namespace duckdb {
15: 
16: ART::ART(const vector<column_t> &column_ids, TableIOManager &table_io_manager,
17:          const vector<unique_ptr<Expression>> &unbound_expressions, IndexConstraintType constraint_type,
18:          AttachedDatabase &db, bool track_memory, idx_t block_id, idx_t block_offset)
19: 
20:     : Index(db, IndexType::ART, table_io_manager, column_ids, unbound_expressions, constraint_type, track_memory) {
21: 
22: 	if (!Radix::IsLittleEndian()) {
23: 		throw NotImplementedException("ART indexes are not supported on big endian architectures");
24: 	}
25: 
26: 	// set the root node of the tree
27: 	tree = nullptr;
28: 	if (block_id != DConstants::INVALID_INDEX) {
29: 		tree = Node::Deserialize(*this, block_id, block_offset);
30: 		Verify();
31: 		if (track_memory) {
32: 			buffer_manager.IncreaseUsedMemory(memory_size);
33: 		}
34: 	}
35: 	serialized_data_pointer = BlockPointer(block_id, block_offset);
36: 
37: 	// validate the types of the key columns
38: 	for (idx_t i = 0; i < types.size(); i++) {
39: 		switch (types[i]) {
40: 		case PhysicalType::BOOL:
41: 		case PhysicalType::INT8:
42: 		case PhysicalType::INT16:
43: 		case PhysicalType::INT32:
44: 		case PhysicalType::INT64:
45: 		case PhysicalType::INT128:
46: 		case PhysicalType::UINT8:
47: 		case PhysicalType::UINT16:
48: 		case PhysicalType::UINT32:
49: 		case PhysicalType::UINT64:
50: 		case PhysicalType::FLOAT:
51: 		case PhysicalType::DOUBLE:
52: 		case PhysicalType::VARCHAR:
53: 			break;
54: 		default:
55: 			throw InvalidTypeException(logical_types[i], "Invalid type for index key.");
56: 		}
57: 	}
58: }
59: 
60: ART::~ART() {
61: 	if (!tree) {
62: 		return;
63: 	}
64: 	Verify();
65: 	if (track_memory) {
66: 		buffer_manager.DecreaseUsedMemory(memory_size);
67: 	}
68: 	Node::Delete(tree);
69: 	tree = nullptr;
70: }
71: 
72: //===--------------------------------------------------------------------===//
73: // Initialize Predicate Scans
74: //===--------------------------------------------------------------------===//
75: 
76: unique_ptr<IndexScanState> ART::InitializeScanSinglePredicate(const Transaction &transaction, const Value &value,
77:                                                               ExpressionType expression_type) {
78: 	// initialize point lookup
79: 	auto result = make_unique<ARTIndexScanState>();
80: 	result->values[0] = value;
81: 	result->expressions[0] = expression_type;
82: 	return std::move(result);
83: }
84: 
85: unique_ptr<IndexScanState> ART::InitializeScanTwoPredicates(Transaction &transaction, const Value &low_value,
86:                                                             ExpressionType low_expression_type, const Value &high_value,
87:                                                             ExpressionType high_expression_type) {
88: 	// initialize range lookup
89: 	auto result = make_unique<ARTIndexScanState>();
90: 	result->values[0] = low_value;
91: 	result->expressions[0] = low_expression_type;
92: 	result->values[1] = high_value;
93: 	result->expressions[1] = high_expression_type;
94: 	return std::move(result);
95: }
96: 
97: //===--------------------------------------------------------------------===//
98: // Keys
99: //===--------------------------------------------------------------------===//
100: 
101: template <class T>
102: static void TemplatedGenerateKeys(ArenaAllocator &allocator, Vector &input, idx_t count, vector<Key> &keys) {
103: 	UnifiedVectorFormat idata;
104: 	input.ToUnifiedFormat(count, idata);
105: 
106: 	D_ASSERT(keys.size() >= count);
107: 	auto input_data = (T *)idata.data;
108: 	for (idx_t i = 0; i < count; i++) {
109: 		auto idx = idata.sel->get_index(i);
110: 		if (idata.validity.RowIsValid(idx)) {
111: 			Key::CreateKey<T>(allocator, input.GetType(), keys[i], input_data[idx]);
112: 		}
113: 	}
114: }
115: 
116: template <class T>
117: static void ConcatenateKeys(ArenaAllocator &allocator, Vector &input, idx_t count, vector<Key> &keys) {
118: 	UnifiedVectorFormat idata;
119: 	input.ToUnifiedFormat(count, idata);
120: 
121: 	auto input_data = (T *)idata.data;
122: 	for (idx_t i = 0; i < count; i++) {
123: 		auto idx = idata.sel->get_index(i);
124: 
125: 		// key is not NULL (no previous column entry was NULL)
126: 		if (!keys[i].Empty()) {
127: 			if (!idata.validity.RowIsValid(idx)) {
128: 				// this column entry is NULL, set whole key to NULL
129: 				keys[i] = Key();
130: 			} else {
131: 				auto other_key = Key::CreateKey<T>(allocator, input.GetType(), input_data[idx]);
132: 				keys[i].ConcatenateKey(allocator, other_key);
133: 			}
134: 		}
135: 	}
136: }
137: 
138: void ART::GenerateKeys(ArenaAllocator &allocator, DataChunk &input, vector<Key> &keys) {
139: 	// generate keys for the first input column
140: 	switch (input.data[0].GetType().InternalType()) {
141: 	case PhysicalType::BOOL:
142: 		TemplatedGenerateKeys<bool>(allocator, input.data[0], input.size(), keys);
143: 		break;
144: 	case PhysicalType::INT8:
145: 		TemplatedGenerateKeys<int8_t>(allocator, input.data[0], input.size(), keys);
146: 		break;
147: 	case PhysicalType::INT16:
148: 		TemplatedGenerateKeys<int16_t>(allocator, input.data[0], input.size(), keys);
149: 		break;
150: 	case PhysicalType::INT32:
151: 		TemplatedGenerateKeys<int32_t>(allocator, input.data[0], input.size(), keys);
152: 		break;
153: 	case PhysicalType::INT64:
154: 		TemplatedGenerateKeys<int64_t>(allocator, input.data[0], input.size(), keys);
155: 		break;
156: 	case PhysicalType::INT128:
157: 		TemplatedGenerateKeys<hugeint_t>(allocator, input.data[0], input.size(), keys);
158: 		break;
159: 	case PhysicalType::UINT8:
160: 		TemplatedGenerateKeys<uint8_t>(allocator, input.data[0], input.size(), keys);
161: 		break;
162: 	case PhysicalType::UINT16:
163: 		TemplatedGenerateKeys<uint16_t>(allocator, input.data[0], input.size(), keys);
164: 		break;
165: 	case PhysicalType::UINT32:
166: 		TemplatedGenerateKeys<uint32_t>(allocator, input.data[0], input.size(), keys);
167: 		break;
168: 	case PhysicalType::UINT64:
169: 		TemplatedGenerateKeys<uint64_t>(allocator, input.data[0], input.size(), keys);
170: 		break;
171: 	case PhysicalType::FLOAT:
172: 		TemplatedGenerateKeys<float>(allocator, input.data[0], input.size(), keys);
173: 		break;
174: 	case PhysicalType::DOUBLE:
175: 		TemplatedGenerateKeys<double>(allocator, input.data[0], input.size(), keys);
176: 		break;
177: 	case PhysicalType::VARCHAR:
178: 		TemplatedGenerateKeys<string_t>(allocator, input.data[0], input.size(), keys);
179: 		break;
180: 	default:
181: 		throw InternalException("Invalid type for index");
182: 	}
183: 
184: 	for (idx_t i = 1; i < input.ColumnCount(); i++) {
185: 		// for each of the remaining columns, concatenate
186: 		switch (input.data[i].GetType().InternalType()) {
187: 		case PhysicalType::BOOL:
188: 			ConcatenateKeys<bool>(allocator, input.data[i], input.size(), keys);
189: 			break;
190: 		case PhysicalType::INT8:
191: 			ConcatenateKeys<int8_t>(allocator, input.data[i], input.size(), keys);
192: 			break;
193: 		case PhysicalType::INT16:
194: 			ConcatenateKeys<int16_t>(allocator, input.data[i], input.size(), keys);
195: 			break;
196: 		case PhysicalType::INT32:
197: 			ConcatenateKeys<int32_t>(allocator, input.data[i], input.size(), keys);
198: 			break;
199: 		case PhysicalType::INT64:
200: 			ConcatenateKeys<int64_t>(allocator, input.data[i], input.size(), keys);
201: 			break;
202: 		case PhysicalType::INT128:
203: 			ConcatenateKeys<hugeint_t>(allocator, input.data[i], input.size(), keys);
204: 			break;
205: 		case PhysicalType::UINT8:
206: 			ConcatenateKeys<uint8_t>(allocator, input.data[i], input.size(), keys);
207: 			break;
208: 		case PhysicalType::UINT16:
209: 			ConcatenateKeys<uint16_t>(allocator, input.data[i], input.size(), keys);
210: 			break;
211: 		case PhysicalType::UINT32:
212: 			ConcatenateKeys<uint32_t>(allocator, input.data[i], input.size(), keys);
213: 			break;
214: 		case PhysicalType::UINT64:
215: 			ConcatenateKeys<uint64_t>(allocator, input.data[i], input.size(), keys);
216: 			break;
217: 		case PhysicalType::FLOAT:
218: 			ConcatenateKeys<float>(allocator, input.data[i], input.size(), keys);
219: 			break;
220: 		case PhysicalType::DOUBLE:
221: 			ConcatenateKeys<double>(allocator, input.data[i], input.size(), keys);
222: 			break;
223: 		case PhysicalType::VARCHAR:
224: 			ConcatenateKeys<string_t>(allocator, input.data[i], input.size(), keys);
225: 			break;
226: 		default:
227: 			throw InternalException("Invalid type for index");
228: 		}
229: 	}
230: }
231: 
232: //===--------------------------------------------------------------------===//
233: // Construct from sorted data (only during CREATE (UNIQUE) INDEX statements)
234: //===--------------------------------------------------------------------===//
235: 
236: struct KeySection {
237: 	KeySection(idx_t start_p, idx_t end_p, idx_t depth_p, data_t key_byte_p)
238: 	    : start(start_p), end(end_p), depth(depth_p), key_byte(key_byte_p) {};
239: 	KeySection(idx_t start_p, idx_t end_p, vector<Key> &keys, KeySection &key_section)
240: 	    : start(start_p), end(end_p), depth(key_section.depth + 1), key_byte(keys[end_p].data[key_section.depth]) {};
241: 	idx_t start;
242: 	idx_t end;
243: 	idx_t depth;
244: 	data_t key_byte;
245: };
246: 
247: void GetChildSections(vector<KeySection> &child_sections, vector<Key> &keys, KeySection &key_section) {
248: 
249: 	idx_t child_start_idx = key_section.start;
250: 	for (idx_t i = key_section.start + 1; i <= key_section.end; i++) {
251: 		if (keys[i - 1].data[key_section.depth] != keys[i].data[key_section.depth]) {
252: 			child_sections.emplace_back(child_start_idx, i - 1, keys, key_section);
253: 			child_start_idx = i;
254: 		}
255: 	}
256: 	child_sections.emplace_back(child_start_idx, key_section.end, keys, key_section);
257: }
258: 
259: bool Construct(ART &art, vector<Key> &keys, row_t *row_ids, Node *&node, KeySection &key_section,
260:                bool &has_constraint) {
261: 
262: 	D_ASSERT(key_section.start < keys.size());
263: 	D_ASSERT(key_section.end < keys.size());
264: 	D_ASSERT(key_section.start <= key_section.end);
265: 
266: 	auto &start_key = keys[key_section.start];
267: 	auto &end_key = keys[key_section.end];
268: 
269: 	// increment the depth until we reach a leaf or find a mismatching byte
270: 	auto prefix_start = key_section.depth;
271: 	while (start_key.len != key_section.depth && start_key.ByteMatches(end_key, key_section.depth)) {
272: 		key_section.depth++;
273: 	}
274: 
275: 	// we reached a leaf, i.e. all the bytes of start_key and end_key match
276: 	if (start_key.len == key_section.depth) {
277: 		// end_idx is inclusive
278: 		auto num_row_ids = key_section.end - key_section.start + 1;
279: 
280: 		// check for possible constraint violation
281: 		auto single_row_id = num_row_ids == 1;
282: 		if (has_constraint && !single_row_id) {
283: 			return false;
284: 		}
285: 
286: 		if (single_row_id) {
287: 			node = Leaf::New(start_key, prefix_start, row_ids[key_section.start]);
288: 		} else {
289: 			node = Leaf::New(start_key, prefix_start, row_ids + key_section.start, num_row_ids);
290: 		}
291: 		art.IncreaseMemorySize(node->MemorySize(art, false));
292: 		return true;
293: 	}
294: 	// create a new node and recurse
295: 
296: 	// we will find at least two child entries of this node, otherwise we'd have reached a leaf
297: 	vector<KeySection> child_sections;
298: 	GetChildSections(child_sections, keys, key_section);
299: 
300: 	auto node_type = Node::GetTypeBySize(child_sections.size());
301: 	Node::New(node_type, node);
302: 
303: 	auto prefix_length = key_section.depth - prefix_start;
304: 	node->prefix = Prefix(start_key, prefix_start, prefix_length);
305: 	art.IncreaseMemorySize(node->MemorySize(art, false));
306: 
307: 	// recurse on each child section
308: 	for (auto &child_section : child_sections) {
309: 		Node *new_child = nullptr;
310: 		auto no_violation = Construct(art, keys, row_ids, new_child, child_section, has_constraint);
311: 		Node::InsertChild(art, node, child_section.key_byte, new_child);
312: 		if (!no_violation) {
313: 			return false;
314: 		}
315: 	}
316: 	return true;
317: }
318: 
319: bool ART::ConstructFromSorted(idx_t count, vector<Key> &keys, Vector &row_identifiers) {
320: 
321: 	// prepare the row_identifiers
322: 	row_identifiers.Flatten(count);
323: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
324: 
325: 	auto key_section = KeySection(0, count - 1, 0, 0);
326: 	auto has_constraint = IsUnique();
327: 	return Construct(*this, keys, row_ids, this->tree, key_section, has_constraint);
328: }
329: 
330: //===--------------------------------------------------------------------===//
331: // Insert / Verification / Constraint Checking
332: //===--------------------------------------------------------------------===//
333: 
334: bool ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {
335: 
336: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
337: 	D_ASSERT(logical_types[0] == input.data[0].GetType());
338: 
339: 	auto old_memory_size = memory_size;
340: 
341: 	// generate the keys for the given input
342: 	ArenaAllocator arena_allocator(BufferAllocator::Get(db));
343: 	vector<Key> keys(input.size());
344: 	GenerateKeys(arena_allocator, input, keys);
345: 
346: 	// get the corresponding row IDs
347: 	row_ids.Flatten(input.size());
348: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
349: 
350: 	// now insert the elements into the index
351: 	idx_t failed_index = DConstants::INVALID_INDEX;
352: 	for (idx_t i = 0; i < input.size(); i++) {
353: 		if (keys[i].Empty()) {
354: 			continue;
355: 		}
356: 
357: 		row_t row_id = row_identifiers[i];
358: 		if (!Insert(tree, keys[i], 0, row_id)) {
359: 			// failed to insert because of constraint violation
360: 			failed_index = i;
361: 			break;
362: 		}
363: 	}
364: 
365: 	// failed to insert because of constraint violation: remove previously inserted entries
366: 	if (failed_index != DConstants::INVALID_INDEX) {
367: 		for (idx_t i = 0; i < failed_index; i++) {
368: 			if (keys[i].Empty()) {
369: 				continue;
370: 			}
371: 			row_t row_id = row_identifiers[i];
372: 			Erase(tree, keys[i], 0, row_id);
373: 		}
374: 	}
375: 
376: 	IncreaseAndVerifyMemorySize(old_memory_size);
377: 	if (failed_index != DConstants::INVALID_INDEX) {
378: 		return false;
379: 	}
380: 	return true;
381: }
382: 
383: bool ART::Append(IndexLock &lock, DataChunk &appended_data, Vector &row_identifiers) {
384: 	DataChunk expression_result;
385: 	expression_result.Initialize(Allocator::DefaultAllocator(), logical_types);
386: 
387: 	// first resolve the expressions for the index
388: 	ExecuteExpressions(appended_data, expression_result);
389: 
390: 	// now insert into the index
391: 	return Insert(lock, expression_result, row_identifiers);
392: }
393: 
394: void ART::VerifyAppend(DataChunk &chunk) {
395: 	ConflictManager conflict_manager(VerifyExistenceType::APPEND, chunk.size());
396: 	CheckConstraintsForChunk(chunk, conflict_manager);
397: }
398: 
399: void ART::VerifyAppend(DataChunk &chunk, ConflictManager &conflict_manager) {
400: 	D_ASSERT(conflict_manager.LookupType() == VerifyExistenceType::APPEND);
401: 	CheckConstraintsForChunk(chunk, conflict_manager);
402: }
403: 
404: bool ART::InsertToLeaf(Leaf &leaf, row_t row_id) {
405: #ifdef DEBUG
406: 	for (idx_t k = 0; k < leaf.count; k++) {
407: 		D_ASSERT(leaf.GetRowId(k) != row_id);
408: 	}
409: #endif
410: 	if (IsUnique() && leaf.count != 0) {
411: 		return false;
412: 	}
413: 	leaf.Insert(*this, row_id);
414: 	return true;
415: }
416: 
417: bool ART::Insert(Node *&node, Key &key, idx_t depth, row_t row_id) {
418: 
419: 	if (!node) {
420: 		// node is currently empty, create a leaf here with the key
421: 		node = Leaf::New(key, depth, row_id);
422: 		IncreaseMemorySize(node->MemorySize(*this, false));
423: 		return true;
424: 	}
425: 
426: 	if (node->type == NodeType::NLeaf) {
427: 		// replace leaf with Node4 and store both leaves in it
428: 		// or add a row ID to a leaf, if they have the same key
429: 		auto leaf = (Leaf *)node;
430: 		uint32_t new_prefix_length = 0;
431: 
432: 		// FIXME: this code (if and while) can be optimized, less branching, see Construct
433: 		// leaf node is already there (its key matches the current key), update row_id vector
434: 		if (new_prefix_length == leaf->prefix.Size() && depth + leaf->prefix.Size() == key.len) {
435: 			return InsertToLeaf(*leaf, row_id);
436: 		}
437: 		while (leaf->prefix[new_prefix_length] == key[depth + new_prefix_length]) {
438: 			new_prefix_length++;
439: 			// leaf node is already there (its key matches the current key), update row_id vector
440: 			if (new_prefix_length == leaf->prefix.Size() && depth + leaf->prefix.Size() == key.len) {
441: 				return InsertToLeaf(*leaf, row_id);
442: 			}
443: 		}
444: 
445: 		Node *new_node = Node4::New();
446: 		new_node->prefix = Prefix(key, depth, new_prefix_length);
447: 		IncreaseMemorySize(new_node->MemorySize(*this, false));
448: 
449: 		auto key_byte = node->prefix.Reduce(*this, new_prefix_length);
450: 		Node4::InsertChild(*this, new_node, key_byte, node);
451: 
452: 		Node *leaf_node = Leaf::New(key, depth + new_prefix_length + 1, row_id);
453: 		Node4::InsertChild(*this, new_node, key[depth + new_prefix_length], leaf_node);
454: 		IncreaseMemorySize(leaf_node->MemorySize(*this, false));
455: 
456: 		node = new_node;
457: 		return true;
458: 	}
459: 
460: 	// handle prefix of inner node
461: 	if (node->prefix.Size()) {
462: 
463: 		uint32_t mismatch_pos = node->prefix.KeyMismatchPosition(key, depth);
464: 		if (mismatch_pos != node->prefix.Size()) {
465: 			// prefix differs, create new node
466: 			Node *new_node = Node4::New();
467: 			new_node->prefix = Prefix(key, depth, mismatch_pos);
468: 			IncreaseMemorySize(new_node->MemorySize(*this, false));
469: 
470: 			// break up prefix
471: 			auto key_byte = node->prefix.Reduce(*this, mismatch_pos);
472: 			Node4::InsertChild(*this, new_node, key_byte, node);
473: 
474: 			Node *leaf_node = Leaf::New(key, depth + mismatch_pos + 1, row_id);
475: 			Node4::InsertChild(*this, new_node, key[depth + mismatch_pos], leaf_node);
476: 			IncreaseMemorySize(leaf_node->MemorySize(*this, false));
477: 
478: 			node = new_node;
479: 			return true;
480: 		}
481: 		depth += node->prefix.Size();
482: 	}
483: 
484: 	// recurse
485: 	D_ASSERT(depth < key.len);
486: 	idx_t pos = node->GetChildPos(key[depth]);
487: 	if (pos != DConstants::INVALID_INDEX) {
488: 		auto child = node->GetChild(*this, pos);
489: 		bool insertion_result = Insert(child, key, depth + 1, row_id);
490: 		node->ReplaceChildPointer(pos, child);
491: 		return insertion_result;
492: 	}
493: 
494: 	Node *leaf_node = Leaf::New(key, depth + 1, row_id);
495: 	Node::InsertChild(*this, node, key[depth], leaf_node);
496: 	IncreaseMemorySize(leaf_node->MemorySize(*this, false));
497: 	return true;
498: }
499: 
500: //===--------------------------------------------------------------------===//
501: // Delete
502: //===--------------------------------------------------------------------===//
503: 
504: void ART::Delete(IndexLock &state, DataChunk &input, Vector &row_ids) {
505: 
506: 	DataChunk expression;
507: 	expression.Initialize(Allocator::DefaultAllocator(), logical_types);
508: 
509: 	// first resolve the expressions
510: 	ExecuteExpressions(input, expression);
511: 
512: 	// then generate the keys for the given input
513: 	ArenaAllocator arena_allocator(BufferAllocator::Get(db));
514: 	vector<Key> keys(expression.size());
515: 	GenerateKeys(arena_allocator, expression, keys);
516: 
517: 	auto old_memory_size = memory_size;
518: 
519: 	// now erase the elements from the database
520: 	row_ids.Flatten(input.size());
521: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
522: 
523: 	for (idx_t i = 0; i < input.size(); i++) {
524: 		if (keys[i].Empty()) {
525: 			continue;
526: 		}
527: 		Erase(tree, keys[i], 0, row_identifiers[i]);
528: #ifdef DEBUG
529: 		auto node = Lookup(tree, keys[i], 0);
530: 		if (node) {
531: 			auto leaf = (Leaf *)node;
532: 			for (idx_t k = 0; k < leaf->count; k++) {
533: 				D_ASSERT(leaf->GetRowId(k) != row_identifiers[i]);
534: 			}
535: 		}
536: #endif
537: 	}
538: 
539: 	// if we deserialize nodes while erasing, then we might end up with more
540: 	// memory afterwards, so we have to either increase or decrease the used memory
541: 	Verify();
542: 	if (track_memory && old_memory_size >= memory_size) {
543: 		buffer_manager.DecreaseUsedMemory(old_memory_size - memory_size);
544: 	} else if (track_memory) {
545: 		buffer_manager.IncreaseUsedMemory(memory_size - old_memory_size);
546: 	}
547: }
548: 
549: void ART::Erase(Node *&node, Key &key, idx_t depth, row_t row_id) {
550: 
551: 	if (!node) {
552: 		return;
553: 	}
554: 
555: 	// delete a leaf from a tree
556: 	if (node->type == NodeType::NLeaf) {
557: 		auto leaf = (Leaf *)node;
558: 		leaf->Remove(*this, row_id);
559: 
560: 		if (leaf->count == 0) {
561: 			DecreaseMemorySize(leaf->MemorySize(*this, false));
562: 			Node::Delete(node);
563: 			node = nullptr;
564: 		}
565: 		return;
566: 	}
567: 
568: 	// handle prefix
569: 	if (node->prefix.Size()) {
570: 		if (node->prefix.KeyMismatchPosition(key, depth) != node->prefix.Size()) {
571: 			return;
572: 		}
573: 		depth += node->prefix.Size();
574: 	}
575: 
576: 	idx_t pos = node->GetChildPos(key[depth]);
577: 	if (pos != DConstants::INVALID_INDEX) {
578: 		auto child = node->GetChild(*this, pos);
579: 		D_ASSERT(child);
580: 
581: 		if (child->type == NodeType::NLeaf) {
582: 			// leaf found, remove entry
583: 			auto leaf = (Leaf *)child;
584: 			leaf->Remove(*this, row_id);
585: 
586: 			if (leaf->count == 0) {
587: 				// leaf is empty, delete leaf, decrement node counter and maybe shrink node
588: 				Node::EraseChild(*this, node, pos);
589: 			}
590: 
591: 		} else {
592: 			// recurse
593: 			Erase(child, key, depth + 1, row_id);
594: 			node->ReplaceChildPointer(pos, child);
595: 		}
596: 	}
597: }
598: 
599: //===--------------------------------------------------------------------===//
600: // Point Query (Equal)
601: //===--------------------------------------------------------------------===//
602: 
603: static Key CreateKey(ArenaAllocator &allocator, PhysicalType type, Value &value) {
604: 	D_ASSERT(type == value.type().InternalType());
605: 	switch (type) {
606: 	case PhysicalType::BOOL:
607: 		return Key::CreateKey<bool>(allocator, value.type(), value);
608: 	case PhysicalType::INT8:
609: 		return Key::CreateKey<int8_t>(allocator, value.type(), value);
610: 	case PhysicalType::INT16:
611: 		return Key::CreateKey<int16_t>(allocator, value.type(), value);
612: 	case PhysicalType::INT32:
613: 		return Key::CreateKey<int32_t>(allocator, value.type(), value);
614: 	case PhysicalType::INT64:
615: 		return Key::CreateKey<int64_t>(allocator, value.type(), value);
616: 	case PhysicalType::UINT8:
617: 		return Key::CreateKey<uint8_t>(allocator, value.type(), value);
618: 	case PhysicalType::UINT16:
619: 		return Key::CreateKey<uint16_t>(allocator, value.type(), value);
620: 	case PhysicalType::UINT32:
621: 		return Key::CreateKey<uint32_t>(allocator, value.type(), value);
622: 	case PhysicalType::UINT64:
623: 		return Key::CreateKey<uint64_t>(allocator, value.type(), value);
624: 	case PhysicalType::INT128:
625: 		return Key::CreateKey<hugeint_t>(allocator, value.type(), value);
626: 	case PhysicalType::FLOAT:
627: 		return Key::CreateKey<float>(allocator, value.type(), value);
628: 	case PhysicalType::DOUBLE:
629: 		return Key::CreateKey<double>(allocator, value.type(), value);
630: 	case PhysicalType::VARCHAR:
631: 		return Key::CreateKey<string_t>(allocator, value.type(), value);
632: 	default:
633: 		throw InternalException("Invalid type for index");
634: 	}
635: }
636: 
637: bool ART::SearchEqual(Key &key, idx_t max_count, vector<row_t> &result_ids) {
638: 
639: 	auto old_memory_size = memory_size;
640: 	auto leaf = (Leaf *)(Lookup(tree, key, 0));
641: 	IncreaseAndVerifyMemorySize(old_memory_size);
642: 
643: 	if (!leaf) {
644: 		return true;
645: 	}
646: 	if (leaf->count > max_count) {
647: 		return false;
648: 	}
649: 	for (idx_t i = 0; i < leaf->count; i++) {
650: 		row_t row_id = leaf->GetRowId(i);
651: 		result_ids.push_back(row_id);
652: 	}
653: 	return true;
654: }
655: 
656: void ART::SearchEqualJoinNoFetch(Key &key, idx_t &result_size) {
657: 
658: 	// we need to look for a leaf
659: 	auto old_memory_size = memory_size;
660: 	auto leaf = Lookup(tree, key, 0);
661: 	IncreaseAndVerifyMemorySize(old_memory_size);
662: 
663: 	if (!leaf) {
664: 		return;
665: 	}
666: 	result_size = leaf->count;
667: }
668: 
669: //===--------------------------------------------------------------------===//
670: // Lookup
671: //===--------------------------------------------------------------------===//
672: 
673: Leaf *ART::Lookup(Node *node, Key &key, idx_t depth) {
674: 
675: 	while (node) {
676: 		if (node->type == NodeType::NLeaf) {
677: 			auto leaf = (Leaf *)node;
678: 			auto &leaf_prefix = leaf->prefix;
679: 
680: 			// check if leaf contains key
681: 			for (idx_t i = 0; i < leaf->prefix.Size(); i++) {
682: 				if (leaf_prefix[i] != key[i + depth]) {
683: 					return nullptr;
684: 				}
685: 			}
686: 			return (Leaf *)node;
687: 		}
688: 
689: 		if (node->prefix.Size()) {
690: 			for (idx_t pos = 0; pos < node->prefix.Size(); pos++) {
691: 				if (key[depth + pos] != node->prefix[pos]) {
692: 					// prefix mismatch, does not contain key
693: 					return nullptr;
694: 				}
695: 			}
696: 			depth += node->prefix.Size();
697: 		}
698: 
699: 		// prefix matches key, but no child at byte, does not contain key
700: 		idx_t pos = node->GetChildPos(key[depth]);
701: 		if (pos == DConstants::INVALID_INDEX) {
702: 			return nullptr;
703: 		}
704: 
705: 		// recurse into child
706: 		node = node->GetChild(*this, pos);
707: 		D_ASSERT(node);
708: 		depth++;
709: 	}
710: 
711: 	return nullptr;
712: }
713: 
714: //===--------------------------------------------------------------------===//
715: // Greater Than
716: // Returns: True (If found leaf >= key)
717: //          False (Otherwise)
718: //===--------------------------------------------------------------------===//
719: 
720: bool ART::SearchGreater(ARTIndexScanState *state, Key &key, bool inclusive, idx_t max_count,
721:                         vector<row_t> &result_ids) {
722: 
723: 	auto old_memory_size = memory_size;
724: 	Iterator *it = &state->iterator;
725: 
726: 	// greater than scan: first set the iterator to the node at which we will start our scan by finding the lowest node
727: 	// that satisfies our requirement
728: 	if (!it->art) {
729: 		it->art = this;
730: 		bool found = it->LowerBound(tree, key, inclusive);
731: 		if (!found) {
732: 			IncreaseAndVerifyMemorySize(old_memory_size);
733: 			return true;
734: 		}
735: 	}
736: 	// after that we continue the scan; we don't need to check the bounds as any value following this value is
737: 	// automatically bigger and hence satisfies our predicate
738: 	Key empty_key = Key();
739: 	auto success = it->Scan(empty_key, max_count, result_ids, false);
740: 	IncreaseAndVerifyMemorySize(old_memory_size);
741: 	return success;
742: }
743: 
744: //===--------------------------------------------------------------------===//
745: // Less Than
746: //===--------------------------------------------------------------------===//
747: 
748: bool ART::SearchLess(ARTIndexScanState *state, Key &upper_bound, bool inclusive, idx_t max_count,
749:                      vector<row_t> &result_ids) {
750: 
751: 	if (!tree) {
752: 		return true;
753: 	}
754: 
755: 	auto old_memory_size = memory_size;
756: 	Iterator *it = &state->iterator;
757: 
758: 	if (!it->art) {
759: 		it->art = this;
760: 		// first find the minimum value in the ART: we start scanning from this value
761: 		it->FindMinimum(*tree);
762: 		// early out min value higher than upper bound query
763: 		if (it->cur_key > upper_bound) {
764: 			IncreaseAndVerifyMemorySize(old_memory_size);
765: 			return true;
766: 		}
767: 	}
768: 	// now continue the scan until we reach the upper bound
769: 	auto success = it->Scan(upper_bound, max_count, result_ids, inclusive);
770: 	IncreaseAndVerifyMemorySize(old_memory_size);
771: 	return success;
772: }
773: 
774: //===--------------------------------------------------------------------===//
775: // Closed Range Query
776: //===--------------------------------------------------------------------===//
777: 
778: bool ART::SearchCloseRange(ARTIndexScanState *state, Key &lower_bound, Key &upper_bound, bool left_inclusive,
779:                            bool right_inclusive, idx_t max_count, vector<row_t> &result_ids) {
780: 
781: 	auto old_memory_size = memory_size;
782: 	Iterator *it = &state->iterator;
783: 
784: 	// first find the first node that satisfies the left predicate
785: 	if (!it->art) {
786: 		it->art = this;
787: 		bool found = it->LowerBound(tree, lower_bound, left_inclusive);
788: 		if (!found) {
789: 			IncreaseAndVerifyMemorySize(old_memory_size);
790: 			return true;
791: 		}
792: 	}
793: 	// now continue the scan until we reach the upper bound
794: 	auto success = it->Scan(upper_bound, max_count, result_ids, right_inclusive);
795: 	IncreaseAndVerifyMemorySize(old_memory_size);
796: 	return success;
797: }
798: 
799: bool ART::Scan(Transaction &transaction, DataTable &table, IndexScanState &table_state, idx_t max_count,
800:                vector<row_t> &result_ids) {
801: 
802: 	auto state = (ARTIndexScanState *)&table_state;
803: 	vector<row_t> row_ids;
804: 	bool success;
805: 
806: 	// FIXME: the key directly owning the data for a single key might be more efficient
807: 	D_ASSERT(state->values[0].type().InternalType() == types[0]);
808: 	ArenaAllocator arena_allocator(Allocator::Get(db));
809: 	auto key = CreateKey(arena_allocator, types[0], state->values[0]);
810: 
811: 	if (state->values[1].IsNull()) {
812: 
813: 		// single predicate
814: 		lock_guard<mutex> l(lock);
815: 		switch (state->expressions[0]) {
816: 		case ExpressionType::COMPARE_EQUAL:
817: 			success = SearchEqual(key, max_count, row_ids);
818: 			break;
819: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
820: 			success = SearchGreater(state, key, true, max_count, row_ids);
821: 			break;
822: 		case ExpressionType::COMPARE_GREATERTHAN:
823: 			success = SearchGreater(state, key, false, max_count, row_ids);
824: 			break;
825: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
826: 			success = SearchLess(state, key, true, max_count, row_ids);
827: 			break;
828: 		case ExpressionType::COMPARE_LESSTHAN:
829: 			success = SearchLess(state, key, false, max_count, row_ids);
830: 			break;
831: 		default:
832: 			throw InternalException("Operation not implemented");
833: 		}
834: 
835: 	} else {
836: 
837: 		// two predicates
838: 		lock_guard<mutex> l(lock);
839: 
840: 		D_ASSERT(state->values[1].type().InternalType() == types[0]);
841: 		auto upper_bound = CreateKey(arena_allocator, types[0], state->values[1]);
842: 
843: 		bool left_inclusive = state->expressions[0] == ExpressionType ::COMPARE_GREATERTHANOREQUALTO;
844: 		bool right_inclusive = state->expressions[1] == ExpressionType ::COMPARE_LESSTHANOREQUALTO;
845: 		success = SearchCloseRange(state, key, upper_bound, left_inclusive, right_inclusive, max_count, row_ids);
846: 	}
847: 
848: 	if (!success) {
849: 		return false;
850: 	}
851: 	if (row_ids.empty()) {
852: 		return true;
853: 	}
854: 
855: 	// sort the row ids
856: 	sort(row_ids.begin(), row_ids.end());
857: 	// duplicate eliminate the row ids and append them to the row ids of the state
858: 	result_ids.reserve(row_ids.size());
859: 
860: 	result_ids.push_back(row_ids[0]);
861: 	for (idx_t i = 1; i < row_ids.size(); i++) {
862: 		if (row_ids[i] != row_ids[i - 1]) {
863: 			result_ids.push_back(row_ids[i]);
864: 		}
865: 	}
866: 	return true;
867: }
868: 
869: //===--------------------------------------------------------------------===//
870: // More Verification / Constraint Checking
871: //===--------------------------------------------------------------------===//
872: 
873: string ART::GenerateErrorKeyName(DataChunk &input, idx_t row) {
874: 
875: 	// FIXME: why exactly can we not pass the expression_chunk as an argument to this
876: 	// FIXME: function instead of re-executing?
877: 	// re-executing the expressions is not very fast, but we're going to throw, so we don't care
878: 	DataChunk expression_chunk;
879: 	expression_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);
880: 	ExecuteExpressions(input, expression_chunk);
881: 
882: 	string key_name;
883: 	for (idx_t k = 0; k < expression_chunk.ColumnCount(); k++) {
884: 		if (k > 0) {
885: 			key_name += ", ";
886: 		}
887: 		key_name += unbound_expressions[k]->GetName() + ": " + expression_chunk.data[k].GetValue(row).ToString();
888: 	}
889: 	return key_name;
890: }
891: 
892: string ART::GenerateConstraintErrorMessage(VerifyExistenceType verify_type, const string &key_name) {
893: 	switch (verify_type) {
894: 	case VerifyExistenceType::APPEND: {
895: 		// APPEND to PK/UNIQUE table, but node/key already exists in PK/UNIQUE table
896: 		string type = IsPrimary() ? "primary key" : "unique";
897: 		return StringUtil::Format("Duplicate key \"%s\" violates %s constraint", key_name, type);
898: 	}
899: 	case VerifyExistenceType::APPEND_FK: {
900: 		// APPEND_FK to FK table, node/key does not exist in PK/UNIQUE table
901: 		return StringUtil::Format(
902: 		    "Violates foreign key constraint because key \"%s\" does not exist in the referenced table", key_name);
903: 	}
904: 	case VerifyExistenceType::DELETE_FK: {
905: 		// DELETE_FK that still exists in a FK table, i.e., not a valid delete
906: 		return StringUtil::Format("Violates foreign key constraint because key \"%s\" is still referenced by a foreign "
907: 		                          "key in a different table",
908: 		                          key_name);
909: 	}
910: 	default:
911: 		throw NotImplementedException("Type not implemented for VerifyExistenceType");
912: 	}
913: }
914: 
915: void ART::CheckConstraintsForChunk(DataChunk &input, ConflictManager &conflict_manager) {
916: 
917: 	// don't alter the index during constraint checking
918: 	lock_guard<mutex> l(lock);
919: 
920: 	auto old_memory_size = memory_size;
921: 
922: 	// first resolve the expressions for the index
923: 	DataChunk expression_chunk;
924: 	expression_chunk.Initialize(Allocator::DefaultAllocator(), logical_types);
925: 	ExecuteExpressions(input, expression_chunk);
926: 
927: 	// generate the keys for the given input
928: 	ArenaAllocator arena_allocator(BufferAllocator::Get(db));
929: 	vector<Key> keys(expression_chunk.size());
930: 	GenerateKeys(arena_allocator, expression_chunk, keys);
931: 
932: 	idx_t found_conflict = DConstants::INVALID_INDEX;
933: 	for (idx_t i = 0; found_conflict == DConstants::INVALID_INDEX && i < input.size(); i++) {
934: 
935: 		if (keys[i].Empty()) {
936: 			if (conflict_manager.AddNull(i)) {
937: 				found_conflict = i;
938: 			}
939: 			continue;
940: 		}
941: 
942: 		Leaf *leaf_ptr = Lookup(tree, keys[i], 0);
943: 		if (leaf_ptr == nullptr) {
944: 			if (conflict_manager.AddMiss(i)) {
945: 				found_conflict = i;
946: 			}
947: 			continue;
948: 		}
949: 
950: 		// When we find a node, we need to update the 'matches' and 'row_ids'
951: 		// NOTE: Leafs can have more than one row_id, but for UNIQUE/PRIMARY KEY they will only have one
952: 		D_ASSERT(leaf_ptr->count == 1);
953: 		auto row_id = leaf_ptr->GetRowId(0);
954: 		if (conflict_manager.AddHit(i, row_id)) {
955: 			found_conflict = i;
956: 		}
957: 	}
958: 
959: 	conflict_manager.FinishLookup();
960: 	IncreaseAndVerifyMemorySize(old_memory_size);
961: 
962: 	if (found_conflict == DConstants::INVALID_INDEX) {
963: 		// No conflicts detected
964: 		return;
965: 	}
966: 
967: 	auto key_name = GenerateErrorKeyName(input, found_conflict);
968: 	auto exception_msg = GenerateConstraintErrorMessage(conflict_manager.LookupType(), key_name);
969: 	throw ConstraintException(exception_msg);
970: }
971: 
972: //===--------------------------------------------------------------------===//
973: // Serialization
974: //===--------------------------------------------------------------------===//
975: 
976: BlockPointer ART::Serialize(MetaBlockWriter &writer) {
977: 	lock_guard<mutex> l(lock);
978: 	auto old_memory_size = memory_size;
979: 	if (tree) {
980: 		serialized_data_pointer = tree->Serialize(*this, writer);
981: 	} else {
982: 		serialized_data_pointer = {(block_id_t)DConstants::INVALID_INDEX, (uint32_t)DConstants::INVALID_INDEX};
983: 	}
984: 	IncreaseAndVerifyMemorySize(old_memory_size);
985: 	return serialized_data_pointer;
986: }
987: 
988: //===--------------------------------------------------------------------===//
989: // Merging
990: //===--------------------------------------------------------------------===//
991: 
992: bool ART::MergeIndexes(IndexLock &state, Index *other_index) {
993: 
994: 	auto other_art = (ART *)other_index;
995: 
996: 	if (!this->tree) {
997: 		IncreaseMemorySize(other_art->memory_size);
998: 		tree = other_art->tree;
999: 		other_art->tree = nullptr;
1000: 		return true;
1001: 	}
1002: 
1003: 	return Node::MergeARTs(this, other_art);
1004: }
1005: 
1006: //===--------------------------------------------------------------------===//
1007: // Utility
1008: //===--------------------------------------------------------------------===//
1009: 
1010: string ART::ToString() {
1011: 	if (tree) {
1012: 		return tree->ToString(*this);
1013: 	}
1014: 	return "[empty]";
1015: }
1016: 
1017: void ART::Verify() {
1018: #ifdef DEBUG
1019: 	idx_t current_mem_size = 0;
1020: 	if (tree) {
1021: 		current_mem_size = tree->MemorySize(*this, true);
1022: 	}
1023: 	if (memory_size != current_mem_size) {
1024: 		throw InternalException("Memory_size value (%d) does not match actual memory size (%d).", memory_size,
1025: 		                        current_mem_size);
1026: 	}
1027: #endif
1028: }
1029: 
1030: void ART::IncreaseAndVerifyMemorySize(idx_t old_memory_size) {
1031: 	// since we lazily deserialize ART nodes, it is possible that its in-memory size
1032: 	// increased during lookups
1033: 	Verify();
1034: 	D_ASSERT(memory_size >= old_memory_size);
1035: 	if (track_memory) {
1036: 		buffer_manager.IncreaseUsedMemory(memory_size - old_memory_size);
1037: 	}
1038: }
1039: 
1040: } // namespace duckdb
[end of src/execution/index/art/art.cpp]
[start of src/include/duckdb/execution/index/art/art.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/index/art/art.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/types/data_chunk.hpp"
13: #include "duckdb/common/types/vector.hpp"
14: #include "duckdb/execution/index/art/art_key.hpp"
15: #include "duckdb/execution/index/art/iterator.hpp"
16: #include "duckdb/execution/index/art/leaf.hpp"
17: #include "duckdb/execution/index/art/node.hpp"
18: #include "duckdb/execution/index/art/node16.hpp"
19: #include "duckdb/execution/index/art/node256.hpp"
20: #include "duckdb/execution/index/art/node4.hpp"
21: #include "duckdb/execution/index/art/node48.hpp"
22: #include "duckdb/parser/parsed_expression.hpp"
23: #include "duckdb/storage/data_table.hpp"
24: #include "duckdb/storage/index.hpp"
25: #include "duckdb/storage/meta_block_writer.hpp"
26: 
27: namespace duckdb {
28: 
29: class ConflictManager;
30: 
31: struct ARTIndexScanState : public IndexScanState {
32: 
33: 	//! Scan predicates (single predicate scan or range scan)
34: 	Value values[2];
35: 	//! Expressions of the scan predicates
36: 	ExpressionType expressions[2];
37: 	bool checked = false;
38: 	//! All scanned row IDs
39: 	vector<row_t> result_ids;
40: 	Iterator iterator;
41: };
42: 
43: enum class VerifyExistenceType : uint8_t {
44: 	APPEND = 0,    // appends to a table
45: 	APPEND_FK = 1, // appends to a table that has a foreign key
46: 	DELETE_FK = 2  // delete from a table that has a foreign key
47: };
48: 
49: class ART : public Index {
50: public:
51: 	//! Constructs an ART containing the bound expressions, which are resolved during index construction
52: 	ART(const vector<column_t> &column_ids, TableIOManager &table_io_manager,
53: 	    const vector<unique_ptr<Expression>> &unbound_expressions, IndexConstraintType constraint_type,
54: 	    AttachedDatabase &db, bool track_memory, idx_t block_id = DConstants::INVALID_INDEX,
55: 	    idx_t block_offset = DConstants::INVALID_INDEX);
56: 	~ART() override;
57: 
58: 	//! Root of the tree
59: 	Node *tree;
60: 
61: public:
62: 	//! Initialize a single predicate scan on the index with the given expression and column IDs
63: 	unique_ptr<IndexScanState> InitializeScanSinglePredicate(const Transaction &transaction, const Value &value,
64: 	                                                         ExpressionType expression_type) override;
65: 	//! Initialize a two predicate scan on the index with the given expression and column IDs
66: 	unique_ptr<IndexScanState> InitializeScanTwoPredicates(Transaction &transaction, const Value &low_value,
67: 	                                                       ExpressionType low_expression_type, const Value &high_value,
68: 	                                                       ExpressionType high_expression_type) override;
69: 	//! Performs a lookup on the index, fetching up to max_count result IDs. Returns true if all row IDs were fetched,
70: 	//! and false otherwise
71: 	bool Scan(Transaction &transaction, DataTable &table, IndexScanState &state, idx_t max_count,
72: 	          vector<row_t> &result_ids) override;
73: 
74: 	//! Called when data is appended to the index. The lock obtained from InitializeLock must be held
75: 	bool Append(IndexLock &lock, DataChunk &entries, Vector &row_identifiers) override;
76: 	//! Verify that data can be appended to the index without a constraint violation
77: 	void VerifyAppend(DataChunk &chunk) override;
78: 	//! Verify that data can be appended to the index without a constraint violation using the conflict manager
79: 	void VerifyAppend(DataChunk &chunk, ConflictManager &conflict_manager) override;
80: 	//! Delete a chunk of entries from the index. The lock obtained from InitializeLock must be held
81: 	void Delete(IndexLock &lock, DataChunk &entries, Vector &row_identifiers) override;
82: 	//! Insert a chunk of entries into the index
83: 	bool Insert(IndexLock &lock, DataChunk &data, Vector &row_ids) override;
84: 
85: 	//! Construct an ART from a vector of sorted keys
86: 	bool ConstructFromSorted(idx_t count, vector<Key> &keys, Vector &row_identifiers);
87: 
88: 	//! Search equal values and fetches the row IDs
89: 	bool SearchEqual(Key &key, idx_t max_count, vector<row_t> &result_ids);
90: 	//! Search equal values used for joins that do not need to fetch data
91: 	void SearchEqualJoinNoFetch(Key &key, idx_t &result_size);
92: 
93: 	//! Serializes the index and returns the pair of block_id offset positions
94: 	BlockPointer Serialize(duckdb::MetaBlockWriter &writer) override;
95: 
96: 	//! Merge another index into this index. The lock obtained from InitializeLock must be held, and the other
97: 	//! index must also be locked during the merge
98: 	bool MergeIndexes(IndexLock &state, Index *other_index) override;
99: 
100: 	//! Generate ART keys for an input chunk
101: 	static void GenerateKeys(ArenaAllocator &allocator, DataChunk &input, vector<Key> &keys);
102: 
103: 	//! Generate a string containing all the expressions and their respective values that violate a constraint
104: 	string GenerateErrorKeyName(DataChunk &input, idx_t row);
105: 	//! Generate the matching error message for a constraint violation
106: 	string GenerateConstraintErrorMessage(VerifyExistenceType verify_type, const string &key_name);
107: 	//! Performs constraint checking for a chunk of input data
108: 	void CheckConstraintsForChunk(DataChunk &input, ConflictManager &conflict_manager) override;
109: 
110: 	//! Returns the string representation of an ART
111: 	string ToString() override;
112: 	//! Verifies that the in-memory size value of the index matches its actual size
113: 	void Verify() override;
114: 	//! Increases the memory size by the difference between the old size and the current size
115: 	//! and performs verifications
116: 	void IncreaseAndVerifyMemorySize(idx_t old_memory_size) override;
117: 
118: private:
119: 	//! Insert a row ID into a leaf
120: 	bool InsertToLeaf(Leaf &leaf, row_t row_id);
121: 	//! Insert a key into the tree
122: 	bool Insert(Node *&node, Key &key, idx_t depth, row_t row_id);
123: 	//! Erase a key from the tree (if a leaf has more than one value) or erase the leaf itself
124: 	void Erase(Node *&node, Key &key, idx_t depth, row_t row_id);
125: 	//! Find the node with a matching key, or return nullptr if not found
126: 	Leaf *Lookup(Node *node, Key &key, idx_t depth);
127: 	//! Returns all row IDs belonging to a key greater (or equal) than the search key
128: 	bool SearchGreater(ARTIndexScanState *state, Key &key, bool inclusive, idx_t max_count, vector<row_t> &result_ids);
129: 	//! Returns all row IDs belonging to a key less (or equal) than the upper_bound
130: 	bool SearchLess(ARTIndexScanState *state, Key &upper_bound, bool inclusive, idx_t max_count,
131: 	                vector<row_t> &result_ids);
132: 	//! Returns all row IDs belonging to a key within the range of lower_bound and upper_bound
133: 	bool SearchCloseRange(ARTIndexScanState *state, Key &lower_bound, Key &upper_bound, bool left_inclusive,
134: 	                      bool right_inclusive, idx_t max_count, vector<row_t> &result_ids);
135: };
136: 
137: } // namespace duckdb
[end of src/include/duckdb/execution/index/art/art.hpp]
[start of src/include/duckdb/storage/data_table.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/data_table.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/enums/index_type.hpp"
12: #include "duckdb/common/enums/scan_options.hpp"
13: #include "duckdb/common/mutex.hpp"
14: #include "duckdb/common/types/data_chunk.hpp"
15: #include "duckdb/storage/index.hpp"
16: #include "duckdb/storage/table/table_statistics.hpp"
17: #include "duckdb/storage/block.hpp"
18: #include "duckdb/storage/statistics/column_statistics.hpp"
19: #include "duckdb/storage/table/column_segment.hpp"
20: #include "duckdb/storage/table/persistent_table_data.hpp"
21: #include "duckdb/storage/table/row_group_collection.hpp"
22: #include "duckdb/storage/table/row_group.hpp"
23: #include "duckdb/transaction/local_storage.hpp"
24: #include "duckdb/storage/table/data_table_info.hpp"
25: 
26: namespace duckdb {
27: class BoundForeignKeyConstraint;
28: class ClientContext;
29: class ColumnDataCollection;
30: class ColumnDefinition;
31: class DataTable;
32: class DuckTransaction;
33: class OptimisticDataWriter;
34: class RowGroup;
35: class StorageManager;
36: class TableCatalogEntry;
37: class TableIOManager;
38: class Transaction;
39: class WriteAheadLog;
40: class TableDataWriter;
41: class ConflictManager;
42: enum class VerifyExistenceType : uint8_t;
43: 
44: //! DataTable represents a physical table on disk
45: class DataTable {
46: public:
47: 	//! Constructs a new data table from an (optional) set of persistent segments
48: 	DataTable(AttachedDatabase &db, shared_ptr<TableIOManager> table_io_manager, const string &schema,
49: 	          const string &table, vector<ColumnDefinition> column_definitions_p,
50: 	          unique_ptr<PersistentTableData> data = nullptr);
51: 	//! Constructs a DataTable as a delta on an existing data table with a newly added column
52: 	DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value);
53: 	//! Constructs a DataTable as a delta on an existing data table but with one column removed
54: 	DataTable(ClientContext &context, DataTable &parent, idx_t removed_column);
55: 	//! Constructs a DataTable as a delta on an existing data table but with one column changed type
56: 	DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, const LogicalType &target_type,
57: 	          const vector<column_t> &bound_columns, Expression &cast_expr);
58: 	//! Constructs a DataTable as a delta on an existing data table but with one column added new constraint
59: 	DataTable(ClientContext &context, DataTable &parent, unique_ptr<BoundConstraint> constraint);
60: 
61: 	//! The table info
62: 	shared_ptr<DataTableInfo> info;
63: 	//! The set of physical columns stored by this DataTable
64: 	vector<ColumnDefinition> column_definitions;
65: 	//! A reference to the database instance
66: 	AttachedDatabase &db;
67: 
68: public:
69: 	//! Returns a list of types of the table
70: 	vector<LogicalType> GetTypes();
71: 
72: 	void InitializeScan(TableScanState &state, const vector<column_t> &column_ids,
73: 	                    TableFilterSet *table_filter = nullptr);
74: 	void InitializeScan(DuckTransaction &transaction, TableScanState &state, const vector<column_t> &column_ids,
75: 	                    TableFilterSet *table_filters = nullptr);
76: 
77: 	//! Returns the maximum amount of threads that should be assigned to scan this data table
78: 	idx_t MaxThreads(ClientContext &context);
79: 	void InitializeParallelScan(ClientContext &context, ParallelTableScanState &state);
80: 	bool NextParallelScan(ClientContext &context, ParallelTableScanState &state, TableScanState &scan_state);
81: 
82: 	//! Scans up to STANDARD_VECTOR_SIZE elements from the table starting
83: 	//! from offset and store them in result. Offset is incremented with how many
84: 	//! elements were returned.
85: 	//! Returns true if all pushed down filters were executed during data fetching
86: 	void Scan(DuckTransaction &transaction, DataChunk &result, TableScanState &state);
87: 
88: 	//! Fetch data from the specific row identifiers from the base table
89: 	void Fetch(DuckTransaction &transaction, DataChunk &result, const vector<column_t> &column_ids,
90: 	           const Vector &row_ids, idx_t fetch_count, ColumnFetchState &state);
91: 
92: 	//! Initializes an append to transaction-local storage
93: 	void InitializeLocalAppend(LocalAppendState &state, ClientContext &context);
94: 	//! Append a DataChunk to the transaction-local storage of the table.
95: 	void LocalAppend(LocalAppendState &state, TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
96: 	                 bool unsafe = false);
97: 	//! Finalizes a transaction-local append
98: 	void FinalizeLocalAppend(LocalAppendState &state);
99: 	//! Append a chunk to the transaction-local storage of this table
100: 	void LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk);
101: 	//! Append a column data collection to the transaction-local storage of this table
102: 	void LocalAppend(TableCatalogEntry &table, ClientContext &context, ColumnDataCollection &collection);
103: 	//! Merge a row group collection into the transaction-local storage
104: 	void LocalMerge(ClientContext &context, RowGroupCollection &collection);
105: 	//! Creates an optimistic writer for this table - used for optimistically writing parallel appends
106: 	OptimisticDataWriter *CreateOptimisticWriter(ClientContext &context);
107: 
108: 	//! Delete the entries with the specified row identifier from the table
109: 	idx_t Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_ids, idx_t count);
110: 	//! Update the entries with the specified row identifier from the table
111: 	void Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
112: 	            const vector<PhysicalIndex> &column_ids, DataChunk &data);
113: 	//! Update a single (sub-)column along a column path
114: 	//! The column_path vector is a *path* towards a column within the table
115: 	//! i.e. if we have a table with a single column S STRUCT(A INT, B INT)
116: 	//! and we update the validity mask of "S.B"
117: 	//! the column path is:
118: 	//! 0 (first column of table)
119: 	//! -> 1 (second subcolumn of struct)
120: 	//! -> 0 (first subcolumn of INT)
121: 	//! This method should only be used from the WAL replay. It does not verify update constraints.
122: 	void UpdateColumn(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
123: 	                  const vector<column_t> &column_path, DataChunk &updates);
124: 
125: 	//! Add an index to the DataTable. NOTE: for CREATE (UNIQUE) INDEX statements, we use the PhysicalCreateIndex
126: 	//! operator. This function is only used during the WAL replay, and is a much less performant index creation
127: 	//! approach.
128: 	void WALAddIndex(ClientContext &context, unique_ptr<Index> index,
129: 	                 const vector<unique_ptr<Expression>> &expressions);
130: 
131: 	//! Fetches an append lock
132: 	void AppendLock(TableAppendState &state);
133: 	//! Begin appending structs to this table, obtaining necessary locks, etc
134: 	void InitializeAppend(DuckTransaction &transaction, TableAppendState &state, idx_t append_count);
135: 	//! Append a chunk to the table using the AppendState obtained from InitializeAppend
136: 	void Append(DataChunk &chunk, TableAppendState &state);
137: 	//! Commit the append
138: 	void CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count);
139: 	//! Write a segment of the table to the WAL
140: 	void WriteToLog(WriteAheadLog &log, idx_t row_start, idx_t count);
141: 	//! Revert a set of appends made by the given AppendState, used to revert appends in the event of an error during
142: 	//! commit (e.g. because of an I/O exception)
143: 	void RevertAppend(idx_t start_row, idx_t count);
144: 	void RevertAppendInternal(idx_t start_row, idx_t count);
145: 
146: 	void ScanTableSegment(idx_t start_row, idx_t count, const std::function<void(DataChunk &chunk)> &function);
147: 
148: 	//! Merge a row group collection directly into this table - appending it to the end of the table without copying
149: 	void MergeStorage(RowGroupCollection &data, TableIndexList &indexes);
150: 
151: 	//! Append a chunk with the row ids [row_start, ..., row_start + chunk.size()] to all indexes of the table, returns
152: 	//! whether or not the append succeeded
153: 	bool AppendToIndexes(DataChunk &chunk, row_t row_start);
154: 	static bool AppendToIndexes(TableIndexList &indexes, DataChunk &chunk, row_t row_start);
155: 	//! Remove a chunk with the row ids [row_start, ..., row_start + chunk.size()] from all indexes of the table
156: 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start);
157: 	//! Remove the chunk with the specified set of row identifiers from all indexes of the table
158: 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers);
159: 	//! Remove the row identifiers from all the indexes of the table
160: 	void RemoveFromIndexes(Vector &row_identifiers, idx_t count);
161: 
162: 	void SetAsRoot() {
163: 		this->is_root = true;
164: 	}
165: 	bool IsRoot() {
166: 		return this->is_root;
167: 	}
168: 
169: 	//! Get statistics of a physical column within the table
170: 	unique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id);
171: 	//! Sets statistics of a physical column within the table
172: 	void SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats);
173: 
174: 	//! Checkpoint the table to the specified table data writer
175: 	void Checkpoint(TableDataWriter &writer);
176: 	void CommitDropTable();
177: 	void CommitDropColumn(idx_t index);
178: 
179: 	idx_t GetTotalRows();
180: 
181: 	void GetStorageInfo(TableStorageInfo &result);
182: 	static bool IsForeignKeyIndex(const vector<PhysicalIndex> &fk_keys, Index &index, ForeignKeyType fk_type);
183: 
184: 	//! Initializes a special scan that is used to create an index on the table, it keeps locks on the table
185: 	void InitializeWALCreateIndexScan(CreateIndexScanState &state, const vector<column_t> &column_ids);
186: 	//! Scans the next chunk for the CREATE INDEX operator
187: 	bool CreateIndexScan(TableScanState &state, DataChunk &result, TableScanType type);
188: 
189: 	//! Verify constraints with a chunk from the Append containing all columns of the table
190: 	void VerifyAppendConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
191: 	                             ConflictManager *conflict_manager = nullptr);
192: 
193: private:
194: 	//! Verify the new added constraints against current persistent&local data
195: 	void VerifyNewConstraint(ClientContext &context, DataTable &parent, const BoundConstraint *constraint);
196: 	//! Verify constraints with a chunk from the Update containing only the specified column_ids
197: 	void VerifyUpdateConstraints(ClientContext &context, TableCatalogEntry &table, DataChunk &chunk,
198: 	                             const vector<PhysicalIndex> &column_ids);
199: 	//! Verify constraints with a chunk from the Delete containing all columns of the table
200: 	void VerifyDeleteConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk);
201: 
202: 	void InitializeScanWithOffset(TableScanState &state, const vector<column_t> &column_ids, idx_t start_row,
203: 	                              idx_t end_row);
204: 
205: 	void VerifyForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context, DataChunk &chunk,
206: 	                                VerifyExistenceType verify_type);
207: 	void VerifyAppendForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
208: 	                                      DataChunk &chunk);
209: 	void VerifyDeleteForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
210: 	                                      DataChunk &chunk);
211: 
212: private:
213: 	//! Lock for appending entries to the table
214: 	mutex append_lock;
215: 	//! The row groups of the table
216: 	shared_ptr<RowGroupCollection> row_groups;
217: 	//! Whether or not the data table is the root DataTable for this table; the root DataTable is the newest version
218: 	//! that can be appended to
219: 	atomic<bool> is_root;
220: };
221: } // namespace duckdb
[end of src/include/duckdb/storage/data_table.hpp]
[start of src/include/duckdb/storage/index.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/index.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_set.hpp"
12: #include "duckdb/common/enums/index_type.hpp"
13: #include "duckdb/common/types/data_chunk.hpp"
14: #include "duckdb/common/sort/sort.hpp"
15: #include "duckdb/parser/parsed_expression.hpp"
16: #include "duckdb/planner/expression.hpp"
17: #include "duckdb/storage/table/scan_state.hpp"
18: #include "duckdb/storage/meta_block_writer.hpp"
19: #include "duckdb/execution/expression_executor.hpp"
20: #include "duckdb/common/types/constraint_conflict_info.hpp"
21: 
22: namespace duckdb {
23: 
24: class ClientContext;
25: class TableIOManager;
26: class Transaction;
27: class ConflictManager;
28: 
29: struct IndexLock;
30: 
31: //! The index is an abstract base class that serves as the basis for indexes
32: class Index {
33: public:
34: 	Index(AttachedDatabase &db, IndexType type, TableIOManager &table_io_manager, const vector<column_t> &column_ids,
35: 	      const vector<unique_ptr<Expression>> &unbound_expressions, IndexConstraintType constraint_type,
36: 	      bool track_memory);
37: 	virtual ~Index() = default;
38: 
39: 	//! The type of the index
40: 	IndexType type;
41: 	//! Associated table io manager
42: 	TableIOManager &table_io_manager;
43: 	//! Column identifiers to extract key columns from the base table
44: 	vector<column_t> column_ids;
45: 	//! Unordered set of column_ids used by the index
46: 	unordered_set<column_t> column_id_set;
47: 	//! Unbound expressions used by the index during optimizations
48: 	vector<unique_ptr<Expression>> unbound_expressions;
49: 	//! The physical types stored in the index
50: 	vector<PhysicalType> types;
51: 	//! The logical types of the expressions
52: 	vector<LogicalType> logical_types;
53: 	//! Index constraint type (primary key, foreign key, ...)
54: 	IndexConstraintType constraint_type;
55: 
56: 	//! Attached database instance
57: 	AttachedDatabase &db;
58: 	//! Buffer manager of the database instance
59: 	BufferManager &buffer_manager;
60: 	//! The size of the index in memory
61: 	//! This does not track the size of the index meta information, but only allocated nodes and leaves
62: 	idx_t memory_size;
63: 	//! Flag determining if this index's size is tracked by the buffer manager
64: 	bool track_memory;
65: 
66: public:
67: 	//! Initialize a single predicate scan on the index with the given expression and column IDs
68: 	virtual unique_ptr<IndexScanState> InitializeScanSinglePredicate(const Transaction &transaction, const Value &value,
69: 	                                                                 ExpressionType expressionType) = 0;
70: 	//! Initialize a two predicate scan on the index with the given expression and column IDs
71: 	virtual unique_ptr<IndexScanState> InitializeScanTwoPredicates(Transaction &transaction, const Value &low_value,
72: 	                                                               ExpressionType low_expression_type,
73: 	                                                               const Value &high_value,
74: 	                                                               ExpressionType high_expression_type) = 0;
75: 	//! Performs a lookup on the index, fetching up to max_count result IDs. Returns true if all row IDs were fetched,
76: 	//! and false otherwise
77: 	virtual bool Scan(Transaction &transaction, DataTable &table, IndexScanState &state, idx_t max_count,
78: 	                  vector<row_t> &result_ids) = 0;
79: 
80: 	//! Obtain a lock on the index
81: 	virtual void InitializeLock(IndexLock &state);
82: 	//! Called when data is appended to the index. The lock obtained from InitializeLock must be held
83: 	virtual bool Append(IndexLock &state, DataChunk &entries, Vector &row_identifiers) = 0;
84: 	//! Obtains a lock and calls Append while holding that lock
85: 	bool Append(DataChunk &entries, Vector &row_identifiers);
86: 	//! Verify that data can be appended to the index without a constraint violation
87: 	virtual void VerifyAppend(DataChunk &chunk) = 0;
88: 	//! Verify that data can be appended to the index without a constraint violation using the conflict manager
89: 	virtual void VerifyAppend(DataChunk &chunk, ConflictManager &conflict_manager) = 0;
90: 	//! Performs constraint checking for a chunk of input data
91: 	virtual void CheckConstraintsForChunk(DataChunk &input, ConflictManager &conflict_manager) = 0;
92: 
93: 	//! Delete a chunk of entries from the index. The lock obtained from InitializeLock must be held
94: 	virtual void Delete(IndexLock &state, DataChunk &entries, Vector &row_identifiers) = 0;
95: 	//! Obtains a lock and calls Delete while holding that lock
96: 	void Delete(DataChunk &entries, Vector &row_identifiers);
97: 
98: 	//! Insert a chunk of entries into the index
99: 	virtual bool Insert(IndexLock &lock, DataChunk &input, Vector &row_identifiers) = 0;
100: 
101: 	//! Merge another index into this index. The lock obtained from InitializeLock must be held, and the other
102: 	//! index must also be locked during the merge
103: 	virtual bool MergeIndexes(IndexLock &state, Index *other_index) = 0;
104: 	//! Obtains a lock and calls MergeIndexes while holding that lock
105: 	bool MergeIndexes(Index *other_index);
106: 
107: 	//! Returns the string representation of an index
108: 	virtual string ToString() = 0;
109: 	//! Verifies that the in-memory size value of the index matches its actual size
110: 	virtual void Verify() = 0;
111: 	//! Increases the memory size by the difference between the old size and the current size
112: 	//! and performs verifications
113: 	virtual void IncreaseAndVerifyMemorySize(idx_t old_memory_size) = 0;
114: 
115: 	//! Increases the in-memory size value
116: 	inline void IncreaseMemorySize(idx_t size) {
117: 		memory_size += size;
118: 	};
119: 	//! Decreases the in-memory size value
120: 	inline void DecreaseMemorySize(idx_t size) {
121: 		D_ASSERT(memory_size >= size);
122: 		memory_size -= size;
123: 	};
124: 
125: 	//! Returns true if the index is affected by updates on the specified column IDs, and false otherwise
126: 	bool IndexIsUpdated(const vector<PhysicalIndex> &column_ids) const;
127: 
128: 	//! Returns unique flag
129: 	bool IsUnique() {
130: 		return (constraint_type == IndexConstraintType::UNIQUE || constraint_type == IndexConstraintType::PRIMARY);
131: 	}
132: 	//! Returns primary key flag
133: 	bool IsPrimary() {
134: 		return (constraint_type == IndexConstraintType::PRIMARY);
135: 	}
136: 	//! Returns foreign key flag
137: 	bool IsForeign() {
138: 		return (constraint_type == IndexConstraintType::FOREIGN);
139: 	}
140: 
141: 	//! Serializes the index and returns the pair of block_id offset positions
142: 	virtual BlockPointer Serialize(MetaBlockWriter &writer);
143: 	//! Returns the serialized data pointer to the block and offset of the serialized index
144: 	BlockPointer GetSerializedDataPointer() const {
145: 		return serialized_data_pointer;
146: 	}
147: 
148: 	//! Execute the index expressions on an input chunk
149: 	void ExecuteExpressions(DataChunk &input, DataChunk &result);
150: 
151: protected:
152: 	//! Lock used for any changes to the index
153: 	mutex lock;
154: 	//! Pointer to serialized index data
155: 	BlockPointer serialized_data_pointer;
156: 
157: private:
158: 	//! Bound expressions used during expression execution
159: 	vector<unique_ptr<Expression>> bound_expressions;
160: 	//! Expression executor to execute the index expressions
161: 	ExpressionExecutor executor;
162: 
163: 	//! Bind the unbound expressions of the index
164: 	unique_ptr<Expression> BindExpression(unique_ptr<Expression> expr);
165: };
166: 
167: } // namespace duckdb
[end of src/include/duckdb/storage/index.hpp]
[start of src/include/duckdb/transaction/local_storage.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/transaction/local_storage.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/table/row_group_collection.hpp"
12: #include "duckdb/storage/table/scan_state.hpp"
13: #include "duckdb/storage/table/table_index_list.hpp"
14: #include "duckdb/storage/table/table_statistics.hpp"
15: 
16: namespace duckdb {
17: class AttachedDatabase;
18: class DataTable;
19: class Transaction;
20: class WriteAheadLog;
21: struct TableAppendState;
22: 
23: class OptimisticDataWriter {
24: public:
25: 	OptimisticDataWriter(DataTable *table);
26: 	OptimisticDataWriter(DataTable *table, OptimisticDataWriter &parent);
27: 	~OptimisticDataWriter();
28: 
29: 	void CheckFlushToDisk(RowGroupCollection &row_groups);
30: 	//! Flushes a specific row group to disk
31: 	void FlushToDisk(RowGroup *row_group);
32: 	//! Flushes the final row group to disk (if any)
33: 	void FlushToDisk(RowGroupCollection &row_groups, bool force = false);
34: 	//! Final flush: flush the partial block manager to disk
35: 	void FinalFlush();
36: 
37: 	void Rollback();
38: 
39: private:
40: 	//! Prepare a write to disk
41: 	bool PrepareWrite();
42: 
43: private:
44: 	//! The table
45: 	DataTable *table;
46: 	//! The partial block manager (if we created one yet)
47: 	unique_ptr<PartialBlockManager> partial_manager;
48: 	//! The set of blocks that have been pre-emptively written to disk
49: 	unordered_set<block_id_t> written_blocks;
50: };
51: 
52: class LocalTableStorage : public std::enable_shared_from_this<LocalTableStorage> {
53: public:
54: 	// Create a new LocalTableStorage
55: 	explicit LocalTableStorage(DataTable &table);
56: 	// Create a LocalTableStorage from an ALTER TYPE
57: 	LocalTableStorage(ClientContext &context, DataTable &table, LocalTableStorage &parent, idx_t changed_idx,
58: 	                  const LogicalType &target_type, const vector<column_t> &bound_columns, Expression &cast_expr);
59: 	// Create a LocalTableStorage from a DROP COLUMN
60: 	LocalTableStorage(DataTable &table, LocalTableStorage &parent, idx_t drop_idx);
61: 	// Create a LocalTableStorage from an ADD COLUMN
62: 	LocalTableStorage(ClientContext &context, DataTable &table, LocalTableStorage &parent, ColumnDefinition &new_column,
63: 	                  Expression *default_value);
64: 	~LocalTableStorage();
65: 
66: 	DataTable *table;
67: 
68: 	Allocator &allocator;
69: 	//! The main chunk collection holding the data
70: 	shared_ptr<RowGroupCollection> row_groups;
71: 	//! The set of unique indexes
72: 	TableIndexList indexes;
73: 	//! The number of deleted rows
74: 	idx_t deleted_rows;
75: 	//! The main optimistic data writer
76: 	OptimisticDataWriter optimistic_writer;
77: 	//! The set of all optimistic data writers associated with this table
78: 	vector<unique_ptr<OptimisticDataWriter>> optimistic_writers;
79: 
80: public:
81: 	void InitializeScan(CollectionScanState &state, TableFilterSet *table_filters = nullptr);
82: 	//! Check if we should flush the previously written row-group to disk
83: 	void CheckFlushToDisk();
84: 	//! Flushes the final row group to disk (if any)
85: 	void FlushToDisk();
86: 	void Rollback();
87: 	idx_t EstimatedSize();
88: 
89: 	void AppendToIndexes(DuckTransaction &transaction, TableAppendState &append_state, idx_t append_count,
90: 	                     bool append_to_table);
91: 	bool AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source, TableIndexList &index_list,
92: 	                     const vector<LogicalType> &table_types, row_t &start_row);
93: 
94: 	//! Creates an optimistic writer for this table
95: 	OptimisticDataWriter *CreateOptimisticWriter();
96: };
97: 
98: class LocalTableManager {
99: public:
100: 	shared_ptr<LocalTableStorage> MoveEntry(DataTable *table);
101: 	unordered_map<DataTable *, shared_ptr<LocalTableStorage>> MoveEntries();
102: 	LocalTableStorage *GetStorage(DataTable *table);
103: 	LocalTableStorage *GetOrCreateStorage(DataTable *table);
104: 	idx_t EstimatedSize();
105: 	bool IsEmpty();
106: 	void InsertEntry(DataTable *table, shared_ptr<LocalTableStorage> entry);
107: 
108: private:
109: 	mutex table_storage_lock;
110: 	unordered_map<DataTable *, shared_ptr<LocalTableStorage>> table_storage;
111: };
112: 
113: //! The LocalStorage class holds appends that have not been committed yet
114: class LocalStorage {
115: public:
116: 	// Threshold to merge row groups instead of appending
117: 	static constexpr const idx_t MERGE_THRESHOLD = RowGroup::ROW_GROUP_SIZE / 2;
118: 
119: public:
120: 	struct CommitState {
121: 		unordered_map<DataTable *, unique_ptr<TableAppendState>> append_states;
122: 	};
123: 
124: public:
125: 	explicit LocalStorage(ClientContext &context, DuckTransaction &transaction);
126: 
127: 	static LocalStorage &Get(DuckTransaction &transaction);
128: 	static LocalStorage &Get(ClientContext &context, AttachedDatabase &db);
129: 	static LocalStorage &Get(ClientContext &context, Catalog &catalog);
130: 
131: 	//! Initialize a scan of the local storage
132: 	void InitializeScan(DataTable *table, CollectionScanState &state, TableFilterSet *table_filters);
133: 	//! Scan
134: 	void Scan(CollectionScanState &state, const vector<column_t> &column_ids, DataChunk &result);
135: 
136: 	void InitializeParallelScan(DataTable *table, ParallelCollectionScanState &state);
137: 	bool NextParallelScan(ClientContext &context, DataTable *table, ParallelCollectionScanState &state,
138: 	                      CollectionScanState &scan_state);
139: 
140: 	//! Begin appending to the local storage
141: 	void InitializeAppend(LocalAppendState &state, DataTable *table);
142: 	//! Append a chunk to the local storage
143: 	static void Append(LocalAppendState &state, DataChunk &chunk);
144: 	//! Finish appending to the local storage
145: 	static void FinalizeAppend(LocalAppendState &state);
146: 	//! Merge a row group collection into the transaction-local storage
147: 	void LocalMerge(DataTable *table, RowGroupCollection &collection);
148: 	//! Create an optimistic writer for the specified table
149: 	OptimisticDataWriter *CreateOptimisticWriter(DataTable *table);
150: 
151: 	//! Delete a set of rows from the local storage
152: 	idx_t Delete(DataTable *table, Vector &row_ids, idx_t count);
153: 	//! Update a set of rows in the local storage
154: 	void Update(DataTable *table, Vector &row_ids, const vector<PhysicalIndex> &column_ids, DataChunk &data);
155: 
156: 	//! Commits the local storage, writing it to the WAL and completing the commit
157: 	void Commit(LocalStorage::CommitState &commit_state, DuckTransaction &transaction);
158: 	//! Rollback the local storage
159: 	void Rollback();
160: 
161: 	bool ChangesMade() noexcept;
162: 	idx_t EstimatedSize();
163: 
164: 	bool Find(DataTable *table);
165: 
166: 	idx_t AddedRows(DataTable *table);
167: 
168: 	void AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column, Expression *default_value);
169: 	void DropColumn(DataTable *old_dt, DataTable *new_dt, idx_t removed_column);
170: 	void ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, const LogicalType &target_type,
171: 	                const vector<column_t> &bound_columns, Expression &cast_expr);
172: 
173: 	void MoveStorage(DataTable *old_dt, DataTable *new_dt);
174: 	void FetchChunk(DataTable *table, Vector &row_ids, idx_t count, const vector<column_t> &col_ids, DataChunk &chunk,
175: 	                ColumnFetchState &fetch_state);
176: 	TableIndexList &GetIndexes(DataTable *table);
177: 
178: 	void VerifyNewConstraint(DataTable &parent, const BoundConstraint &constraint);
179: 
180: private:
181: 	ClientContext &context;
182: 	DuckTransaction &transaction;
183: 	LocalTableManager table_manager;
184: 
185: 	void Flush(DataTable &table, LocalTableStorage &storage);
186: };
187: 
188: } // namespace duckdb
[end of src/include/duckdb/transaction/local_storage.hpp]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/chrono.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/helper.hpp"
7: #include "duckdb/common/vector_operations/vector_operations.hpp"
8: #include "duckdb/common/sort/sort.hpp"
9: #include "duckdb/execution/expression_executor.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/parser/constraints/list.hpp"
12: #include "duckdb/planner/constraints/list.hpp"
13: #include "duckdb/planner/expression_binder/check_binder.hpp"
14: #include "duckdb/planner/table_filter.hpp"
15: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
16: #include "duckdb/storage/storage_manager.hpp"
17: #include "duckdb/storage/table/persistent_table_data.hpp"
18: #include "duckdb/storage/table/row_group.hpp"
19: #include "duckdb/storage/table/standard_column_data.hpp"
20: #include "duckdb/transaction/duck_transaction.hpp"
21: #include "duckdb/transaction/transaction_manager.hpp"
22: #include "duckdb/execution/index/art/art.hpp"
23: #include "duckdb/main/attached_database.hpp"
24: #include "duckdb/common/types/conflict_manager.hpp"
25: #include "duckdb/common/types/constraint_conflict_info.hpp"
26: 
27: namespace duckdb {
28: 
29: DataTableInfo::DataTableInfo(AttachedDatabase &db, shared_ptr<TableIOManager> table_io_manager_p, string schema,
30:                              string table)
31:     : db(db), table_io_manager(std::move(table_io_manager_p)), cardinality(0), schema(std::move(schema)),
32:       table(std::move(table)) {
33: }
34: 
35: bool DataTableInfo::IsTemporary() const {
36: 	return db.IsTemporary();
37: }
38: 
39: DataTable::DataTable(AttachedDatabase &db, shared_ptr<TableIOManager> table_io_manager_p, const string &schema,
40:                      const string &table, vector<ColumnDefinition> column_definitions_p,
41:                      unique_ptr<PersistentTableData> data)
42:     : info(make_shared<DataTableInfo>(db, std::move(table_io_manager_p), schema, table)),
43:       column_definitions(std::move(column_definitions_p)), db(db), is_root(true) {
44: 	// initialize the table with the existing data from disk, if any
45: 	auto types = GetTypes();
46: 	this->row_groups =
47: 	    make_shared<RowGroupCollection>(info, TableIOManager::Get(*this).GetBlockManagerForRowData(), types, 0);
48: 	if (data && !data->row_groups.empty()) {
49: 		this->row_groups->Initialize(*data);
50: 	} else {
51: 		this->row_groups->InitializeEmpty();
52: 		D_ASSERT(row_groups->GetTotalRows() == 0);
53: 	}
54: 	row_groups->Verify();
55: }
56: 
57: DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value)
58:     : info(parent.info), db(parent.db), is_root(true) {
59: 	// add the column definitions from this DataTable
60: 	for (auto &column_def : parent.column_definitions) {
61: 		column_definitions.emplace_back(column_def.Copy());
62: 	}
63: 	column_definitions.emplace_back(new_column.Copy());
64: 	// prevent any new tuples from being added to the parent
65: 	lock_guard<mutex> parent_lock(parent.append_lock);
66: 
67: 	this->row_groups = parent.row_groups->AddColumn(context, new_column, default_value);
68: 
69: 	// also add this column to client local storage
70: 	auto &local_storage = LocalStorage::Get(context, db);
71: 	local_storage.AddColumn(&parent, this, new_column, default_value);
72: 
73: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
74: 	parent.is_root = false;
75: }
76: 
77: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
78:     : info(parent.info), db(parent.db), is_root(true) {
79: 	// prevent any new tuples from being added to the parent
80: 	lock_guard<mutex> parent_lock(parent.append_lock);
81: 
82: 	for (auto &column_def : parent.column_definitions) {
83: 		column_definitions.emplace_back(column_def.Copy());
84: 	}
85: 	// first check if there are any indexes that exist that point to the removed column
86: 	info->indexes.Scan([&](Index &index) {
87: 		for (auto &column_id : index.column_ids) {
88: 			if (column_id == removed_column) {
89: 				throw CatalogException("Cannot drop this column: an index depends on it!");
90: 			} else if (column_id > removed_column) {
91: 				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
92: 			}
93: 		}
94: 		return false;
95: 	});
96: 
97: 	// erase the column definitions from this DataTable
98: 	D_ASSERT(removed_column < column_definitions.size());
99: 	column_definitions.erase(column_definitions.begin() + removed_column);
100: 
101: 	storage_t storage_idx = 0;
102: 	for (idx_t i = 0; i < column_definitions.size(); i++) {
103: 		auto &col = column_definitions[i];
104: 		col.SetOid(i);
105: 		if (col.Generated()) {
106: 			continue;
107: 		}
108: 		col.SetStorageOid(storage_idx++);
109: 	}
110: 
111: 	// alter the row_groups and remove the column from each of them
112: 	this->row_groups = parent.row_groups->RemoveColumn(removed_column);
113: 
114: 	// scan the original table, and fill the new column with the transformed value
115: 	auto &local_storage = LocalStorage::Get(context, db);
116: 	local_storage.DropColumn(&parent, this, removed_column);
117: 
118: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
119: 	parent.is_root = false;
120: }
121: 
122: // Alter column to add new constraint
123: DataTable::DataTable(ClientContext &context, DataTable &parent, unique_ptr<BoundConstraint> constraint)
124:     : info(parent.info), db(parent.db), row_groups(parent.row_groups), is_root(true) {
125: 
126: 	lock_guard<mutex> parent_lock(parent.append_lock);
127: 	for (auto &column_def : parent.column_definitions) {
128: 		column_definitions.emplace_back(column_def.Copy());
129: 	}
130: 
131: 	// Verify the new constraint against current persistent/local data
132: 	VerifyNewConstraint(context, parent, constraint.get());
133: 
134: 	// Get the local data ownership from old dt
135: 	auto &local_storage = LocalStorage::Get(context, db);
136: 	local_storage.MoveStorage(&parent, this);
137: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
138: 	parent.is_root = false;
139: }
140: 
141: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, const LogicalType &target_type,
142:                      const vector<column_t> &bound_columns, Expression &cast_expr)
143:     : info(parent.info), db(parent.db), is_root(true) {
144: 	// prevent any tuples from being added to the parent
145: 	lock_guard<mutex> lock(append_lock);
146: 	for (auto &column_def : parent.column_definitions) {
147: 		column_definitions.emplace_back(column_def.Copy());
148: 	}
149: 	// first check if there are any indexes that exist that point to the changed column
150: 	info->indexes.Scan([&](Index &index) {
151: 		for (auto &column_id : index.column_ids) {
152: 			if (column_id == changed_idx) {
153: 				throw CatalogException("Cannot change the type of this column: an index depends on it!");
154: 			}
155: 		}
156: 		return false;
157: 	});
158: 
159: 	// change the type in this DataTable
160: 	column_definitions[changed_idx].SetType(target_type);
161: 
162: 	// set up the statistics for the table
163: 	// the column that had its type changed will have the new statistics computed during conversion
164: 	this->row_groups = parent.row_groups->AlterType(context, changed_idx, target_type, bound_columns, cast_expr);
165: 
166: 	// scan the original table, and fill the new column with the transformed value
167: 	auto &local_storage = LocalStorage::Get(context, db);
168: 	local_storage.ChangeType(&parent, this, changed_idx, target_type, bound_columns, cast_expr);
169: 
170: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
171: 	parent.is_root = false;
172: }
173: 
174: vector<LogicalType> DataTable::GetTypes() {
175: 	vector<LogicalType> types;
176: 	for (auto &it : column_definitions) {
177: 		types.push_back(it.Type());
178: 	}
179: 	return types;
180: }
181: 
182: TableIOManager &TableIOManager::Get(DataTable &table) {
183: 	return *table.info->table_io_manager;
184: }
185: 
186: //===--------------------------------------------------------------------===//
187: // Scan
188: //===--------------------------------------------------------------------===//
189: void DataTable::InitializeScan(TableScanState &state, const vector<column_t> &column_ids,
190:                                TableFilterSet *table_filters) {
191: 	state.Initialize(column_ids, table_filters);
192: 	row_groups->InitializeScan(state.table_state, column_ids, table_filters);
193: }
194: 
195: void DataTable::InitializeScan(DuckTransaction &transaction, TableScanState &state, const vector<column_t> &column_ids,
196:                                TableFilterSet *table_filters) {
197: 	InitializeScan(state, column_ids, table_filters);
198: 	auto &local_storage = LocalStorage::Get(transaction);
199: 	local_storage.InitializeScan(this, state.local_state, table_filters);
200: }
201: 
202: void DataTable::InitializeScanWithOffset(TableScanState &state, const vector<column_t> &column_ids, idx_t start_row,
203:                                          idx_t end_row) {
204: 	state.Initialize(column_ids);
205: 	row_groups->InitializeScanWithOffset(state.table_state, column_ids, start_row, end_row);
206: }
207: 
208: idx_t DataTable::MaxThreads(ClientContext &context) {
209: 	idx_t parallel_scan_vector_count = RowGroup::ROW_GROUP_VECTOR_COUNT;
210: 	if (ClientConfig::GetConfig(context).verify_parallelism) {
211: 		parallel_scan_vector_count = 1;
212: 	}
213: 	idx_t parallel_scan_tuple_count = STANDARD_VECTOR_SIZE * parallel_scan_vector_count;
214: 	return GetTotalRows() / parallel_scan_tuple_count + 1;
215: }
216: 
217: void DataTable::InitializeParallelScan(ClientContext &context, ParallelTableScanState &state) {
218: 	row_groups->InitializeParallelScan(state.scan_state);
219: 
220: 	auto &local_storage = LocalStorage::Get(context, db);
221: 	local_storage.InitializeParallelScan(this, state.local_state);
222: }
223: 
224: bool DataTable::NextParallelScan(ClientContext &context, ParallelTableScanState &state, TableScanState &scan_state) {
225: 	if (row_groups->NextParallelScan(context, state.scan_state, scan_state.table_state)) {
226: 		return true;
227: 	}
228: 	scan_state.table_state.batch_index = state.scan_state.batch_index;
229: 	auto &local_storage = LocalStorage::Get(context, db);
230: 	if (local_storage.NextParallelScan(context, this, state.local_state, scan_state.local_state)) {
231: 		return true;
232: 	} else {
233: 		// finished all scans: no more scans remaining
234: 		return false;
235: 	}
236: }
237: 
238: void DataTable::Scan(DuckTransaction &transaction, DataChunk &result, TableScanState &state) {
239: 	// scan the persistent segments
240: 	if (state.table_state.Scan(transaction, result)) {
241: 		D_ASSERT(result.size() > 0);
242: 		return;
243: 	}
244: 
245: 	// scan the transaction-local segments
246: 	auto &local_storage = LocalStorage::Get(transaction);
247: 	local_storage.Scan(state.local_state, state.GetColumnIds(), result);
248: }
249: 
250: bool DataTable::CreateIndexScan(TableScanState &state, DataChunk &result, TableScanType type) {
251: 	return state.table_state.ScanCommitted(result, type);
252: }
253: 
254: //===--------------------------------------------------------------------===//
255: // Fetch
256: //===--------------------------------------------------------------------===//
257: void DataTable::Fetch(DuckTransaction &transaction, DataChunk &result, const vector<column_t> &column_ids,
258:                       const Vector &row_identifiers, idx_t fetch_count, ColumnFetchState &state) {
259: 	row_groups->Fetch(transaction, result, column_ids, row_identifiers, fetch_count, state);
260: }
261: 
262: //===--------------------------------------------------------------------===//
263: // Append
264: //===--------------------------------------------------------------------===//
265: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, const string &col_name) {
266: 	if (!VectorOperations::HasNull(vector, count)) {
267: 		return;
268: 	}
269: 
270: 	throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name, col_name);
271: }
272: 
273: // To avoid throwing an error at SELECT, instead this moves the error detection to INSERT
274: static void VerifyGeneratedExpressionSuccess(ClientContext &context, TableCatalogEntry &table, DataChunk &chunk,
275:                                              Expression &expr, column_t index) {
276: 	auto &col = table.GetColumn(LogicalIndex(index));
277: 	D_ASSERT(col.Generated());
278: 	ExpressionExecutor executor(context, expr);
279: 	Vector result(col.Type());
280: 	try {
281: 		executor.ExecuteExpression(chunk, result);
282: 	} catch (InternalException &ex) {
283: 		throw;
284: 	} catch (std::exception &ex) {
285: 		throw ConstraintException("Incorrect value for generated column \"%s %s AS (%s)\" : %s", col.Name(),
286: 		                          col.Type().ToString(), col.GeneratedExpression().ToString(), ex.what());
287: 	}
288: }
289: 
290: static void VerifyCheckConstraint(ClientContext &context, TableCatalogEntry &table, Expression &expr,
291:                                   DataChunk &chunk) {
292: 	ExpressionExecutor executor(context, expr);
293: 	Vector result(LogicalType::INTEGER);
294: 	try {
295: 		executor.ExecuteExpression(chunk, result);
296: 	} catch (std::exception &ex) {
297: 		throw ConstraintException("CHECK constraint failed: %s (Error: %s)", table.name, ex.what());
298: 	} catch (...) { // LCOV_EXCL_START
299: 		throw ConstraintException("CHECK constraint failed: %s (Unknown Error)", table.name);
300: 	} // LCOV_EXCL_STOP
301: 	UnifiedVectorFormat vdata;
302: 	result.ToUnifiedFormat(chunk.size(), vdata);
303: 
304: 	auto dataptr = (int32_t *)vdata.data;
305: 	for (idx_t i = 0; i < chunk.size(); i++) {
306: 		auto idx = vdata.sel->get_index(i);
307: 		if (vdata.validity.RowIsValid(idx) && dataptr[idx] == 0) {
308: 			throw ConstraintException("CHECK constraint failed: %s", table.name);
309: 		}
310: 	}
311: }
312: 
313: bool DataTable::IsForeignKeyIndex(const vector<PhysicalIndex> &fk_keys, Index &index, ForeignKeyType fk_type) {
314: 	if (fk_type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE ? !index.IsUnique() : !index.IsForeign()) {
315: 		return false;
316: 	}
317: 	if (fk_keys.size() != index.column_ids.size()) {
318: 		return false;
319: 	}
320: 	for (auto &fk_key : fk_keys) {
321: 		bool is_found = false;
322: 		for (auto &index_key : index.column_ids) {
323: 			if (fk_key.index == index_key) {
324: 				is_found = true;
325: 				break;
326: 			}
327: 		}
328: 		if (!is_found) {
329: 			return false;
330: 		}
331: 	}
332: 	return true;
333: }
334: 
335: // Find the first index that is not null, and did not find a match
336: static idx_t FirstMissingMatch(const ManagedSelection &matches) {
337: 	idx_t match_idx = 0;
338: 
339: 	for (idx_t i = 0; i < matches.Size(); i++) {
340: 		auto match = matches.IndexMapsToLocation(match_idx, i);
341: 		match_idx += match;
342: 		if (!match) {
343: 			// This index is missing in the matches vector
344: 			return i;
345: 		}
346: 	}
347: 	return DConstants::INVALID_INDEX;
348: }
349: 
350: idx_t LocateErrorIndex(bool is_append, const ManagedSelection &matches) {
351: 	idx_t failed_index = DConstants::INVALID_INDEX;
352: 	if (!is_append) {
353: 		// We expected to find nothing, so the first error is the first match
354: 		failed_index = matches[0];
355: 	} else {
356: 		// We expected to find matches for all of them, so the first missing match is the first error
357: 		return FirstMissingMatch(matches);
358: 	}
359: 	return failed_index;
360: }
361: 
362: [[noreturn]] static void ThrowForeignKeyConstraintError(idx_t failed_index, bool is_append, Index *index,
363:                                                         DataChunk &input) {
364: 	auto verify_type = is_append ? VerifyExistenceType::APPEND_FK : VerifyExistenceType::DELETE_FK;
365: 
366: 	D_ASSERT(failed_index != DConstants::INVALID_INDEX);
367: 	D_ASSERT(index->type == IndexType::ART);
368: 	auto &art_index = (ART &)*index;
369: 	auto key_name = art_index.GenerateErrorKeyName(input, failed_index);
370: 	auto exception_msg = art_index.GenerateConstraintErrorMessage(verify_type, key_name);
371: 	throw ConstraintException(exception_msg);
372: }
373: 
374: bool IsForeignKeyConstraintError(bool is_append, idx_t input_count, const ManagedSelection &matches) {
375: 	if (is_append) {
376: 		// We need to find a match for all of the values
377: 		return matches.Count() != input_count;
378: 	} else {
379: 		// We should not find any matches
380: 		return matches.Count() != 0;
381: 	}
382: }
383: 
384: static bool IsAppend(VerifyExistenceType verify_type) {
385: 	return verify_type == VerifyExistenceType::APPEND_FK;
386: }
387: 
388: void DataTable::VerifyForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
389:                                            DataChunk &chunk, VerifyExistenceType verify_type) {
390: 	const vector<PhysicalIndex> *src_keys_ptr = &bfk.info.fk_keys;
391: 	const vector<PhysicalIndex> *dst_keys_ptr = &bfk.info.pk_keys;
392: 
393: 	bool is_append = IsAppend(verify_type);
394: 	if (!is_append) {
395: 		src_keys_ptr = &bfk.info.pk_keys;
396: 		dst_keys_ptr = &bfk.info.fk_keys;
397: 	}
398: 
399: 	auto table_entry_ptr =
400: 	    Catalog::GetEntry<TableCatalogEntry>(context, INVALID_CATALOG, bfk.info.schema, bfk.info.table);
401: 	if (table_entry_ptr == nullptr) {
402: 		throw InternalException("Can't find table \"%s\" in foreign key constraint", bfk.info.table);
403: 	}
404: 	// make the data chunk to check
405: 	vector<LogicalType> types;
406: 	for (auto &col : table_entry_ptr->GetColumns().Physical()) {
407: 		types.emplace_back(col.Type());
408: 	}
409: 	DataChunk dst_chunk;
410: 	dst_chunk.InitializeEmpty(types);
411: 	for (idx_t i = 0; i < src_keys_ptr->size(); i++) {
412: 		dst_chunk.data[(*dst_keys_ptr)[i].index].Reference(chunk.data[(*src_keys_ptr)[i].index]);
413: 	}
414: 	dst_chunk.SetCardinality(chunk.size());
415: 	auto data_table = table_entry_ptr->GetStoragePtr();
416: 
417: 	idx_t count = dst_chunk.size();
418: 	if (count <= 0) {
419: 		return;
420: 	}
421: 
422: 	// Set up a way to record conflicts, rather than directly throw on them
423: 	unordered_set<column_t> empty_column_list;
424: 	ConflictInfo empty_conflict_info(empty_column_list, false);
425: 	ConflictManager regular_conflicts(verify_type, count, &empty_conflict_info);
426: 	ConflictManager transaction_conflicts(verify_type, count, &empty_conflict_info);
427: 	regular_conflicts.SetMode(ConflictManagerMode::SCAN);
428: 	transaction_conflicts.SetMode(ConflictManagerMode::SCAN);
429: 
430: 	data_table->info->indexes.VerifyForeignKey(*dst_keys_ptr, dst_chunk, regular_conflicts);
431: 	regular_conflicts.Finalize();
432: 	auto &regular_matches = regular_conflicts.Conflicts();
433: 	// check whether or not the chunk can be inserted or deleted into the referenced table' transaction local storage
434: 	auto &local_storage = LocalStorage::Get(context, db);
435: 
436: 	bool error = IsForeignKeyConstraintError(is_append, count, regular_matches);
437: 	bool transaction_error = false;
438: 
439: 	bool transaction_check = local_storage.Find(data_table);
440: 	if (transaction_check) {
441: 		auto &transact_index = local_storage.GetIndexes(data_table);
442: 		transact_index.VerifyForeignKey(*dst_keys_ptr, dst_chunk, transaction_conflicts);
443: 		transaction_conflicts.Finalize();
444: 		auto &transaction_matches = transaction_conflicts.Conflicts();
445: 		transaction_error = IsForeignKeyConstraintError(is_append, count, transaction_matches);
446: 	}
447: 
448: 	if (!transaction_error && !error) {
449: 		// No error occurred;
450: 		return;
451: 	}
452: 
453: 	// Some error occurred, and we likely want to throw
454: 	Index *index;
455: 	Index *transaction_index;
456: 
457: 	auto fk_type = is_append ? ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE : ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE;
458: 	// check whether or not the chunk can be inserted or deleted into the referenced table' storage
459: 	index = data_table->info->indexes.FindForeignKeyIndex(*dst_keys_ptr, fk_type);
460: 	if (transaction_check) {
461: 		auto &transact_index = local_storage.GetIndexes(data_table);
462: 		// check whether or not the chunk can be inserted or deleted into the referenced table' storage
463: 		transaction_index = transact_index.FindForeignKeyIndex(*dst_keys_ptr, fk_type);
464: 	}
465: 
466: 	if (!transaction_check) {
467: 		// Only local state is checked, throw the error
468: 		D_ASSERT(error);
469: 		auto failed_index = LocateErrorIndex(is_append, regular_matches);
470: 		D_ASSERT(failed_index != DConstants::INVALID_INDEX);
471: 		ThrowForeignKeyConstraintError(failed_index, is_append, index, dst_chunk);
472: 	}
473: 	if (transaction_error && error && is_append) {
474: 		// When we want to do an append, we only throw if the foreign key does not exist in both transaction and local
475: 		// storage
476: 		auto &transaction_matches = transaction_conflicts.Conflicts();
477: 		idx_t failed_index = DConstants::INVALID_INDEX;
478: 		idx_t regular_idx = 0;
479: 		idx_t transaction_idx = 0;
480: 		for (idx_t i = 0; i < count; i++) {
481: 			bool in_regular = regular_matches.IndexMapsToLocation(regular_idx, i);
482: 			regular_idx += in_regular;
483: 			bool in_transaction = transaction_matches.IndexMapsToLocation(transaction_idx, i);
484: 			transaction_idx += in_transaction;
485: 
486: 			if (!in_regular && !in_transaction) {
487: 				// We need to find a match for all of the input values
488: 				// The failed index is i, it does not show up in either regular or transaction storage
489: 				failed_index = i;
490: 				break;
491: 			}
492: 		}
493: 		if (failed_index == DConstants::INVALID_INDEX) {
494: 			// We don't throw, every value was present in either regular or transaction storage
495: 			return;
496: 		}
497: 		ThrowForeignKeyConstraintError(failed_index, true, index, dst_chunk);
498: 	}
499: 	if (!is_append && transaction_check) {
500: 		auto &transaction_matches = transaction_conflicts.Conflicts();
501: 		if (error) {
502: 			auto failed_index = LocateErrorIndex(false, regular_matches);
503: 			D_ASSERT(failed_index != DConstants::INVALID_INDEX);
504: 			ThrowForeignKeyConstraintError(failed_index, false, index, dst_chunk);
505: 		} else {
506: 			D_ASSERT(transaction_error);
507: 			D_ASSERT(transaction_matches.Count() != DConstants::INVALID_INDEX);
508: 			auto failed_index = LocateErrorIndex(false, transaction_matches);
509: 			D_ASSERT(failed_index != DConstants::INVALID_INDEX);
510: 			ThrowForeignKeyConstraintError(failed_index, false, transaction_index, dst_chunk);
511: 		}
512: 	}
513: }
514: 
515: void DataTable::VerifyAppendForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
516:                                                  DataChunk &chunk) {
517: 	VerifyForeignKeyConstraint(bfk, context, chunk, VerifyExistenceType::APPEND_FK);
518: }
519: 
520: void DataTable::VerifyDeleteForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
521:                                                  DataChunk &chunk) {
522: 	VerifyForeignKeyConstraint(bfk, context, chunk, VerifyExistenceType::DELETE_FK);
523: }
524: 
525: void DataTable::VerifyNewConstraint(ClientContext &context, DataTable &parent, const BoundConstraint *constraint) {
526: 	if (constraint->type != ConstraintType::NOT_NULL) {
527: 		throw NotImplementedException("FIXME: ALTER COLUMN with such constraint is not supported yet");
528: 	}
529: 
530: 	parent.row_groups->VerifyNewConstraint(parent, *constraint);
531: 	auto &local_storage = LocalStorage::Get(context, db);
532: 	local_storage.VerifyNewConstraint(parent, *constraint);
533: }
534: 
535: void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
536:                                         ConflictManager *conflict_manager) {
537: 	if (table.HasGeneratedColumns()) {
538: 		// Verify that the generated columns expression work with the inserted values
539: 		auto binder = Binder::CreateBinder(context);
540: 		physical_index_set_t bound_columns;
541: 		CheckBinder generated_check_binder(*binder, context, table.name, table.GetColumns(), bound_columns);
542: 		for (auto &col : table.GetColumns().Logical()) {
543: 			if (!col.Generated()) {
544: 				continue;
545: 			}
546: 			D_ASSERT(col.Type().id() != LogicalTypeId::ANY);
547: 			generated_check_binder.target_type = col.Type();
548: 			auto to_be_bound_expression = col.GeneratedExpression().Copy();
549: 			auto bound_expression = generated_check_binder.Bind(to_be_bound_expression);
550: 			VerifyGeneratedExpressionSuccess(context, table, chunk, *bound_expression, col.Oid());
551: 		}
552: 	}
553: 	auto &constraints = table.GetConstraints();
554: 	auto &bound_constraints = table.GetBoundConstraints();
555: 	for (idx_t i = 0; i < bound_constraints.size(); i++) {
556: 		auto &base_constraint = constraints[i];
557: 		auto &constraint = bound_constraints[i];
558: 		switch (base_constraint->type) {
559: 		case ConstraintType::NOT_NULL: {
560: 			auto &bound_not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
561: 			auto &not_null = *reinterpret_cast<NotNullConstraint *>(base_constraint.get());
562: 			auto &col = table.GetColumns().GetColumn(LogicalIndex(not_null.index));
563: 			VerifyNotNullConstraint(table, chunk.data[bound_not_null.index.index], chunk.size(), col.Name());
564: 			break;
565: 		}
566: 		case ConstraintType::CHECK: {
567: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
568: 			VerifyCheckConstraint(context, table, *check.expression, chunk);
569: 			break;
570: 		}
571: 		case ConstraintType::UNIQUE: {
572: 			//! check whether or not the chunk can be inserted into the indexes
573: 			if (conflict_manager) {
574: 				// This is only provided when a ON CONFLICT clause was provided
575: 				idx_t matching_indexes = 0;
576: 				auto &conflict_info = conflict_manager->GetConflictInfo();
577: 				// First we figure out how many indexes match our conflict target
578: 				// So we can optimize accordingly
579: 				info->indexes.Scan([&](Index &index) {
580: 					matching_indexes += conflict_info.ConflictTargetMatches(index);
581: 					return false;
582: 				});
583: 				conflict_manager->SetMode(ConflictManagerMode::SCAN);
584: 				conflict_manager->SetIndexCount(matching_indexes);
585: 				// First we verify only the indexes that match our conflict target
586: 				info->indexes.Scan([&](Index &index) {
587: 					if (!index.IsUnique()) {
588: 						return false;
589: 					}
590: 					if (conflict_info.ConflictTargetMatches(index)) {
591: 						index.VerifyAppend(chunk, *conflict_manager);
592: 					}
593: 					return false;
594: 				});
595: 
596: 				conflict_manager->SetMode(ConflictManagerMode::THROW);
597: 				// Then we scan the other indexes, throwing if they cause conflicts on tuples that were not found during
598: 				// the scan
599: 				info->indexes.Scan([&](Index &index) {
600: 					if (!index.IsUnique()) {
601: 						return false;
602: 					}
603: 					index.VerifyAppend(chunk, *conflict_manager);
604: 					return false;
605: 				});
606: 			} else {
607: 				// Only need to verify that no unique constraints are violated
608: 				info->indexes.Scan([&](Index &index) {
609: 					if (!index.IsUnique()) {
610: 						return false;
611: 					}
612: 					index.VerifyAppend(chunk);
613: 					return false;
614: 				});
615: 			}
616: 			break;
617: 		}
618: 		case ConstraintType::FOREIGN_KEY: {
619: 			auto &bfk = *reinterpret_cast<BoundForeignKeyConstraint *>(constraint.get());
620: 			if (bfk.info.type == ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE ||
621: 			    bfk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE) {
622: 				VerifyAppendForeignKeyConstraint(bfk, context, chunk);
623: 			}
624: 			break;
625: 		}
626: 		default:
627: 			throw NotImplementedException("Constraint type not implemented!");
628: 		}
629: 	}
630: }
631: 
632: void DataTable::InitializeLocalAppend(LocalAppendState &state, ClientContext &context) {
633: 	if (!is_root) {
634: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
635: 	}
636: 	auto &local_storage = LocalStorage::Get(context, db);
637: 	local_storage.InitializeAppend(state, this);
638: }
639: 
640: void DataTable::LocalAppend(LocalAppendState &state, TableCatalogEntry &table, ClientContext &context, DataChunk &chunk,
641:                             bool unsafe) {
642: 	if (chunk.size() == 0) {
643: 		return;
644: 	}
645: 	D_ASSERT(chunk.ColumnCount() == table.GetColumns().PhysicalColumnCount());
646: 	if (!is_root) {
647: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
648: 	}
649: 
650: 	chunk.Verify();
651: 
652: 	// verify any constraints on the new chunk
653: 	if (!unsafe) {
654: 		VerifyAppendConstraints(table, context, chunk);
655: 	}
656: 
657: 	// append to the transaction local data
658: 	LocalStorage::Append(state, chunk);
659: }
660: 
661: void DataTable::FinalizeLocalAppend(LocalAppendState &state) {
662: 	LocalStorage::FinalizeAppend(state);
663: }
664: 
665: OptimisticDataWriter *DataTable::CreateOptimisticWriter(ClientContext &context) {
666: 	auto &local_storage = LocalStorage::Get(context, db);
667: 	return local_storage.CreateOptimisticWriter(this);
668: }
669: 
670: void DataTable::LocalMerge(ClientContext &context, RowGroupCollection &collection) {
671: 	auto &local_storage = LocalStorage::Get(context, db);
672: 	local_storage.LocalMerge(this, collection);
673: }
674: 
675: void DataTable::LocalAppend(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
676: 	LocalAppendState append_state;
677: 	auto &storage = table.GetStorage();
678: 	storage.InitializeLocalAppend(append_state, context);
679: 	storage.LocalAppend(append_state, table, context, chunk);
680: 	storage.FinalizeLocalAppend(append_state);
681: }
682: 
683: void DataTable::LocalAppend(TableCatalogEntry &table, ClientContext &context, ColumnDataCollection &collection) {
684: 	LocalAppendState append_state;
685: 	auto &storage = table.GetStorage();
686: 	storage.InitializeLocalAppend(append_state, context);
687: 	for (auto &chunk : collection.Chunks()) {
688: 		storage.LocalAppend(append_state, table, context, chunk);
689: 	}
690: 	storage.FinalizeLocalAppend(append_state);
691: }
692: 
693: void DataTable::AppendLock(TableAppendState &state) {
694: 	state.append_lock = unique_lock<mutex>(append_lock);
695: 	if (!is_root) {
696: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
697: 	}
698: 	state.row_start = row_groups->GetTotalRows();
699: 	state.current_row = state.row_start;
700: }
701: 
702: void DataTable::InitializeAppend(DuckTransaction &transaction, TableAppendState &state, idx_t append_count) {
703: 	// obtain the append lock for this table
704: 	if (!state.append_lock) {
705: 		throw InternalException("DataTable::AppendLock should be called before DataTable::InitializeAppend");
706: 	}
707: 	row_groups->InitializeAppend(transaction, state, append_count);
708: }
709: 
710: void DataTable::Append(DataChunk &chunk, TableAppendState &state) {
711: 	D_ASSERT(is_root);
712: 	row_groups->Append(chunk, state);
713: }
714: 
715: void DataTable::ScanTableSegment(idx_t row_start, idx_t count, const std::function<void(DataChunk &chunk)> &function) {
716: 	idx_t end = row_start + count;
717: 
718: 	vector<column_t> column_ids;
719: 	vector<LogicalType> types;
720: 	for (idx_t i = 0; i < this->column_definitions.size(); i++) {
721: 		auto &col = this->column_definitions[i];
722: 		column_ids.push_back(i);
723: 		types.push_back(col.Type());
724: 	}
725: 	DataChunk chunk;
726: 	chunk.Initialize(Allocator::Get(db), types);
727: 
728: 	CreateIndexScanState state;
729: 
730: 	InitializeScanWithOffset(state, column_ids, row_start, row_start + count);
731: 	auto row_start_aligned = state.table_state.row_group_state.row_group->start +
732: 	                         state.table_state.row_group_state.vector_index * STANDARD_VECTOR_SIZE;
733: 
734: 	idx_t current_row = row_start_aligned;
735: 	while (current_row < end) {
736: 		state.table_state.ScanCommitted(chunk, TableScanType::TABLE_SCAN_COMMITTED_ROWS);
737: 		if (chunk.size() == 0) {
738: 			break;
739: 		}
740: 		idx_t end_row = current_row + chunk.size();
741: 		// start of chunk is current_row
742: 		// end of chunk is end_row
743: 		// figure out if we need to write the entire chunk or just part of it
744: 		idx_t chunk_start = MaxValue<idx_t>(current_row, row_start);
745: 		idx_t chunk_end = MinValue<idx_t>(end_row, end);
746: 		D_ASSERT(chunk_start < chunk_end);
747: 		idx_t chunk_count = chunk_end - chunk_start;
748: 		if (chunk_count != chunk.size()) {
749: 			D_ASSERT(chunk_count <= chunk.size());
750: 			// need to slice the chunk before insert
751: 			idx_t start_in_chunk;
752: 			if (current_row >= row_start) {
753: 				start_in_chunk = 0;
754: 			} else {
755: 				start_in_chunk = row_start - current_row;
756: 			}
757: 			SelectionVector sel(start_in_chunk, chunk_count);
758: 			chunk.Slice(sel, chunk_count);
759: 			chunk.Verify();
760: 		}
761: 		function(chunk);
762: 		chunk.Reset();
763: 		current_row = end_row;
764: 	}
765: }
766: 
767: void DataTable::MergeStorage(RowGroupCollection &data, TableIndexList &indexes) {
768: 	row_groups->MergeStorage(data);
769: 	row_groups->Verify();
770: }
771: 
772: void DataTable::WriteToLog(WriteAheadLog &log, idx_t row_start, idx_t count) {
773: 	if (log.skip_writing) {
774: 		return;
775: 	}
776: 	log.WriteSetTable(info->schema, info->table);
777: 	ScanTableSegment(row_start, count, [&](DataChunk &chunk) { log.WriteInsert(chunk); });
778: }
779: 
780: void DataTable::CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count) {
781: 	lock_guard<mutex> lock(append_lock);
782: 	row_groups->CommitAppend(commit_id, row_start, count);
783: 	info->cardinality += count;
784: }
785: 
786: void DataTable::RevertAppendInternal(idx_t start_row, idx_t count) {
787: 	if (count == 0) {
788: 		// nothing to revert!
789: 		return;
790: 	}
791: 	// adjust the cardinality
792: 	info->cardinality = start_row;
793: 	D_ASSERT(is_root);
794: 	// revert appends made to row_groups
795: 	row_groups->RevertAppendInternal(start_row, count);
796: }
797: 
798: void DataTable::RevertAppend(idx_t start_row, idx_t count) {
799: 	lock_guard<mutex> lock(append_lock);
800: 
801: 	if (!info->indexes.Empty()) {
802: 		idx_t current_row_base = start_row;
803: 		row_t row_data[STANDARD_VECTOR_SIZE];
804: 		Vector row_identifiers(LogicalType::ROW_TYPE, (data_ptr_t)row_data);
805: 		ScanTableSegment(start_row, count, [&](DataChunk &chunk) {
806: 			for (idx_t i = 0; i < chunk.size(); i++) {
807: 				row_data[i] = current_row_base + i;
808: 			}
809: 			info->indexes.Scan([&](Index &index) {
810: 				index.Delete(chunk, row_identifiers);
811: 				return false;
812: 			});
813: 			current_row_base += chunk.size();
814: 		});
815: 	}
816: 	RevertAppendInternal(start_row, count);
817: }
818: 
819: //===--------------------------------------------------------------------===//
820: // Indexes
821: //===--------------------------------------------------------------------===//
822: bool DataTable::AppendToIndexes(TableIndexList &indexes, DataChunk &chunk, row_t row_start) {
823: 	if (indexes.Empty()) {
824: 		return true;
825: 	}
826: 	// first generate the vector of row identifiers
827: 	Vector row_identifiers(LogicalType::ROW_TYPE);
828: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
829: 
830: 	vector<Index *> already_appended;
831: 	bool append_failed = false;
832: 	// now append the entries to the indices
833: 	indexes.Scan([&](Index &index) {
834: 		try {
835: 			if (!index.Append(chunk, row_identifiers)) {
836: 				append_failed = true;
837: 				return true;
838: 			}
839: 		} catch (...) {
840: 			append_failed = true;
841: 			return true;
842: 		}
843: 		already_appended.push_back(&index);
844: 		return false;
845: 	});
846: 
847: 	if (append_failed) {
848: 		// constraint violation!
849: 		// remove any appended entries from previous indexes (if any)
850: 		for (auto *index : already_appended) {
851: 			index->Delete(chunk, row_identifiers);
852: 		}
853: 		return false;
854: 	}
855: 	return true;
856: }
857: 
858: bool DataTable::AppendToIndexes(DataChunk &chunk, row_t row_start) {
859: 	D_ASSERT(is_root);
860: 	return AppendToIndexes(info->indexes, chunk, row_start);
861: }
862: 
863: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
864: 	D_ASSERT(is_root);
865: 	if (info->indexes.Empty()) {
866: 		return;
867: 	}
868: 	// first generate the vector of row identifiers
869: 	Vector row_identifiers(LogicalType::ROW_TYPE);
870: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
871: 
872: 	// now remove the entries from the indices
873: 	RemoveFromIndexes(state, chunk, row_identifiers);
874: }
875: 
876: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
877: 	D_ASSERT(is_root);
878: 	info->indexes.Scan([&](Index &index) {
879: 		index.Delete(chunk, row_identifiers);
880: 		return false;
881: 	});
882: }
883: 
884: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
885: 	D_ASSERT(is_root);
886: 	row_groups->RemoveFromIndexes(info->indexes, row_identifiers, count);
887: }
888: 
889: //===--------------------------------------------------------------------===//
890: // Delete
891: //===--------------------------------------------------------------------===//
892: static bool TableHasDeleteConstraints(TableCatalogEntry &table) {
893: 	auto &bound_constraints = table.GetBoundConstraints();
894: 	for (auto &constraint : bound_constraints) {
895: 		switch (constraint->type) {
896: 		case ConstraintType::NOT_NULL:
897: 		case ConstraintType::CHECK:
898: 		case ConstraintType::UNIQUE:
899: 			break;
900: 		case ConstraintType::FOREIGN_KEY: {
901: 			auto &bfk = *reinterpret_cast<BoundForeignKeyConstraint *>(constraint.get());
902: 			if (bfk.info.type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE ||
903: 			    bfk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE) {
904: 				return true;
905: 			}
906: 			break;
907: 		}
908: 		default:
909: 			throw NotImplementedException("Constraint type not implemented!");
910: 		}
911: 	}
912: 	return false;
913: }
914: 
915: void DataTable::VerifyDeleteConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
916: 	auto &bound_constraints = table.GetBoundConstraints();
917: 	for (auto &constraint : bound_constraints) {
918: 		switch (constraint->type) {
919: 		case ConstraintType::NOT_NULL:
920: 		case ConstraintType::CHECK:
921: 		case ConstraintType::UNIQUE:
922: 			break;
923: 		case ConstraintType::FOREIGN_KEY: {
924: 			auto &bfk = *reinterpret_cast<BoundForeignKeyConstraint *>(constraint.get());
925: 			if (bfk.info.type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE ||
926: 			    bfk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE) {
927: 				VerifyDeleteForeignKeyConstraint(bfk, context, chunk);
928: 			}
929: 			break;
930: 		}
931: 		default:
932: 			throw NotImplementedException("Constraint type not implemented!");
933: 		}
934: 	}
935: }
936: 
937: idx_t DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_identifiers, idx_t count) {
938: 	D_ASSERT(row_identifiers.GetType().InternalType() == ROW_TYPE);
939: 	if (count == 0) {
940: 		return 0;
941: 	}
942: 
943: 	auto &transaction = DuckTransaction::Get(context, db);
944: 	auto &local_storage = LocalStorage::Get(transaction);
945: 	bool has_delete_constraints = TableHasDeleteConstraints(table);
946: 
947: 	row_identifiers.Flatten(count);
948: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
949: 
950: 	DataChunk verify_chunk;
951: 	vector<column_t> col_ids;
952: 	vector<LogicalType> types;
953: 	ColumnFetchState fetch_state;
954: 	if (has_delete_constraints) {
955: 		// initialize the chunk if there are any constraints to verify
956: 		for (idx_t i = 0; i < column_definitions.size(); i++) {
957: 			col_ids.push_back(column_definitions[i].StorageOid());
958: 			types.emplace_back(column_definitions[i].Type());
959: 		}
960: 		verify_chunk.Initialize(Allocator::Get(context), types);
961: 	}
962: 	idx_t pos = 0;
963: 	idx_t delete_count = 0;
964: 	while (pos < count) {
965: 		idx_t start = pos;
966: 		bool is_transaction_delete = ids[pos] >= MAX_ROW_ID;
967: 		// figure out which batch of rows to delete now
968: 		for (pos++; pos < count; pos++) {
969: 			bool row_is_transaction_delete = ids[pos] >= MAX_ROW_ID;
970: 			if (row_is_transaction_delete != is_transaction_delete) {
971: 				break;
972: 			}
973: 		}
974: 		idx_t current_offset = start;
975: 		idx_t current_count = pos - start;
976: 
977: 		Vector offset_ids(row_identifiers, current_offset, pos);
978: 		if (is_transaction_delete) {
979: 			// transaction-local delete
980: 			if (has_delete_constraints) {
981: 				// perform the constraint verification
982: 				local_storage.FetchChunk(this, offset_ids, current_count, col_ids, verify_chunk, fetch_state);
983: 				VerifyDeleteConstraints(table, context, verify_chunk);
984: 			}
985: 			delete_count += local_storage.Delete(this, offset_ids, current_count);
986: 		} else {
987: 			// regular table delete
988: 			if (has_delete_constraints) {
989: 				// perform the constraint verification
990: 				Fetch(transaction, verify_chunk, col_ids, offset_ids, current_count, fetch_state);
991: 				VerifyDeleteConstraints(table, context, verify_chunk);
992: 			}
993: 			delete_count += row_groups->Delete(transaction, this, ids + current_offset, current_count);
994: 		}
995: 	}
996: 	return delete_count;
997: }
998: 
999: //===--------------------------------------------------------------------===//
1000: // Update
1001: //===--------------------------------------------------------------------===//
1002: static void CreateMockChunk(vector<LogicalType> &types, const vector<PhysicalIndex> &column_ids, DataChunk &chunk,
1003:                             DataChunk &mock_chunk) {
1004: 	// construct a mock DataChunk
1005: 	mock_chunk.InitializeEmpty(types);
1006: 	for (column_t i = 0; i < column_ids.size(); i++) {
1007: 		mock_chunk.data[column_ids[i].index].Reference(chunk.data[i]);
1008: 	}
1009: 	mock_chunk.SetCardinality(chunk.size());
1010: }
1011: 
1012: static bool CreateMockChunk(TableCatalogEntry &table, const vector<PhysicalIndex> &column_ids,
1013:                             physical_index_set_t &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
1014: 	idx_t found_columns = 0;
1015: 	// check whether the desired columns are present in the UPDATE clause
1016: 	for (column_t i = 0; i < column_ids.size(); i++) {
1017: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
1018: 			found_columns++;
1019: 		}
1020: 	}
1021: 	if (found_columns == 0) {
1022: 		// no columns were found: no need to check the constraint again
1023: 		return false;
1024: 	}
1025: 	if (found_columns != desired_column_ids.size()) {
1026: 		// not all columns in UPDATE clause are present!
1027: 		// this should not be triggered at all as the binder should add these columns
1028: 		throw InternalException("Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
1029: 	}
1030: 	// construct a mock DataChunk
1031: 	auto types = table.GetTypes();
1032: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
1033: 	return true;
1034: }
1035: 
1036: void DataTable::VerifyUpdateConstraints(ClientContext &context, TableCatalogEntry &table, DataChunk &chunk,
1037:                                         const vector<PhysicalIndex> &column_ids) {
1038: 	auto &constraints = table.GetConstraints();
1039: 	auto &bound_constraints = table.GetBoundConstraints();
1040: 	for (idx_t i = 0; i < bound_constraints.size(); i++) {
1041: 		auto &base_constraint = constraints[i];
1042: 		auto &constraint = bound_constraints[i];
1043: 		switch (constraint->type) {
1044: 		case ConstraintType::NOT_NULL: {
1045: 			auto &bound_not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
1046: 			auto &not_null = *reinterpret_cast<NotNullConstraint *>(base_constraint.get());
1047: 			// check if the constraint is in the list of column_ids
1048: 			// FIXME: double usage of 'i'?
1049: 			for (idx_t i = 0; i < column_ids.size(); i++) {
1050: 				if (column_ids[i] == bound_not_null.index) {
1051: 					// found the column id: check the data in
1052: 					auto &col = table.GetColumn(LogicalIndex(not_null.index));
1053: 					VerifyNotNullConstraint(table, chunk.data[i], chunk.size(), col.Name());
1054: 					break;
1055: 				}
1056: 			}
1057: 			break;
1058: 		}
1059: 		case ConstraintType::CHECK: {
1060: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
1061: 
1062: 			DataChunk mock_chunk;
1063: 			if (CreateMockChunk(table, column_ids, check.bound_columns, chunk, mock_chunk)) {
1064: 				VerifyCheckConstraint(context, table, *check.expression, mock_chunk);
1065: 			}
1066: 			break;
1067: 		}
1068: 		case ConstraintType::UNIQUE:
1069: 		case ConstraintType::FOREIGN_KEY:
1070: 			break;
1071: 		default:
1072: 			throw NotImplementedException("Constraint type not implemented!");
1073: 		}
1074: 	}
1075: 	// update should not be called for indexed columns!
1076: 	// instead update should have been rewritten to delete + update on higher layer
1077: #ifdef DEBUG
1078: 	info->indexes.Scan([&](Index &index) {
1079: 		D_ASSERT(!index.IndexIsUpdated(column_ids));
1080: 		return false;
1081: 	});
1082: 
1083: #endif
1084: }
1085: 
1086: void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
1087:                        const vector<PhysicalIndex> &column_ids, DataChunk &updates) {
1088: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
1089: 
1090: 	D_ASSERT(column_ids.size() == updates.ColumnCount());
1091: 	auto count = updates.size();
1092: 	updates.Verify();
1093: 	if (count == 0) {
1094: 		return;
1095: 	}
1096: 
1097: 	if (!is_root) {
1098: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
1099: 	}
1100: 
1101: 	// first verify that no constraints are violated
1102: 	VerifyUpdateConstraints(context, table, updates, column_ids);
1103: 
1104: 	// now perform the actual update
1105: 	auto &transaction = DuckTransaction::Get(context, db);
1106: 
1107: 	updates.Flatten();
1108: 	row_ids.Flatten(count);
1109: 	auto ids = FlatVector::GetData<row_t>(row_ids);
1110: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
1111: 	if (first_id >= MAX_ROW_ID) {
1112: 		// update is in transaction-local storage: push update into local storage
1113: 		auto &local_storage = LocalStorage::Get(context, db);
1114: 		local_storage.Update(this, row_ids, column_ids, updates);
1115: 		return;
1116: 	}
1117: 
1118: 	// update is in the row groups
1119: 	// we need to figure out for each id to which row group it belongs
1120: 	// usually all (or many) ids belong to the same row group
1121: 	// we iterate over the ids and check for every id if it belongs to the same row group as their predecessor
1122: 	row_groups->Update(transaction, ids, column_ids, updates);
1123: }
1124: 
1125: void DataTable::UpdateColumn(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
1126:                              const vector<column_t> &column_path, DataChunk &updates) {
1127: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
1128: 	D_ASSERT(updates.ColumnCount() == 1);
1129: 	updates.Verify();
1130: 	if (updates.size() == 0) {
1131: 		return;
1132: 	}
1133: 
1134: 	if (!is_root) {
1135: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
1136: 	}
1137: 
1138: 	// now perform the actual update
1139: 	auto &transaction = DuckTransaction::Get(context, db);
1140: 
1141: 	updates.Flatten();
1142: 	row_ids.Flatten(updates.size());
1143: 	row_groups->UpdateColumn(transaction, row_ids, column_path, updates);
1144: }
1145: 
1146: //===--------------------------------------------------------------------===//
1147: // Index Scan
1148: //===--------------------------------------------------------------------===//
1149: void DataTable::InitializeWALCreateIndexScan(CreateIndexScanState &state, const vector<column_t> &column_ids) {
1150: 	// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan
1151: 	state.append_lock = std::unique_lock<mutex>(append_lock);
1152: 	InitializeScan(state, column_ids);
1153: }
1154: 
1155: void DataTable::WALAddIndex(ClientContext &context, unique_ptr<Index> index,
1156:                             const vector<unique_ptr<Expression>> &expressions) {
1157: 
1158: 	// if the data table is empty
1159: 	if (row_groups->IsEmpty()) {
1160: 		info->indexes.AddIndex(std::move(index));
1161: 		return;
1162: 	}
1163: 
1164: 	auto &allocator = Allocator::Get(db);
1165: 
1166: 	DataChunk result;
1167: 	result.Initialize(allocator, index->logical_types);
1168: 
1169: 	DataChunk intermediate;
1170: 	vector<LogicalType> intermediate_types;
1171: 	auto column_ids = index->column_ids;
1172: 	column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
1173: 	for (auto &id : index->column_ids) {
1174: 		auto &col = column_definitions[id];
1175: 		intermediate_types.push_back(col.Type());
1176: 	}
1177: 	intermediate_types.emplace_back(LogicalType::ROW_TYPE);
1178: 	intermediate.Initialize(allocator, intermediate_types);
1179: 
1180: 	// initialize an index scan
1181: 	CreateIndexScanState state;
1182: 	InitializeWALCreateIndexScan(state, column_ids);
1183: 
1184: 	if (!is_root) {
1185: 		throw InternalException("Error during WAL replay. Cannot add an index to a table that has been altered.");
1186: 	}
1187: 
1188: 	// now start incrementally building the index
1189: 	{
1190: 		IndexLock lock;
1191: 		index->InitializeLock(lock);
1192: 
1193: 		while (true) {
1194: 			intermediate.Reset();
1195: 			result.Reset();
1196: 			// scan a new chunk from the table to index
1197: 			CreateIndexScan(state, intermediate, TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED);
1198: 			if (intermediate.size() == 0) {
1199: 				// finished scanning for index creation
1200: 				// release all locks
1201: 				break;
1202: 			}
1203: 			// resolve the expressions for this chunk
1204: 			index->ExecuteExpressions(intermediate, result);
1205: 
1206: 			// insert into the index
1207: 			if (!index->Insert(lock, result, intermediate.data[intermediate.ColumnCount() - 1])) {
1208: 				throw InternalException("Error during WAL replay. Can't create unique index, table contains "
1209: 				                        "duplicate data on indexed column(s).");
1210: 			}
1211: 		}
1212: 	}
1213: 	info->indexes.AddIndex(std::move(index));
1214: }
1215: 
1216: //===--------------------------------------------------------------------===//
1217: // Statistics
1218: //===--------------------------------------------------------------------===//
1219: unique_ptr<BaseStatistics> DataTable::GetStatistics(ClientContext &context, column_t column_id) {
1220: 	if (column_id == COLUMN_IDENTIFIER_ROW_ID) {
1221: 		return nullptr;
1222: 	}
1223: 	return row_groups->CopyStats(column_id);
1224: }
1225: 
1226: void DataTable::SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats) {
1227: 	D_ASSERT(column_id != COLUMN_IDENTIFIER_ROW_ID);
1228: 	row_groups->SetDistinct(column_id, std::move(distinct_stats));
1229: }
1230: 
1231: //===--------------------------------------------------------------------===//
1232: // Checkpoint
1233: //===--------------------------------------------------------------------===//
1234: void DataTable::Checkpoint(TableDataWriter &writer) {
1235: 	// checkpoint each individual row group
1236: 	// FIXME: we might want to combine adjacent row groups in case they have had deletions...
1237: 	TableStatistics global_stats;
1238: 	row_groups->CopyStats(global_stats);
1239: 
1240: 	row_groups->Checkpoint(writer, global_stats);
1241: 
1242: 	// The rowgroup payload data has been written. Now write:
1243: 	//   column stats
1244: 	//   row-group pointers
1245: 	//   table pointer
1246: 	//   index data
1247: 	writer.FinalizeTable(std::move(global_stats), info.get());
1248: }
1249: 
1250: void DataTable::CommitDropColumn(idx_t index) {
1251: 	row_groups->CommitDropColumn(index);
1252: }
1253: 
1254: idx_t DataTable::GetTotalRows() {
1255: 	return row_groups->GetTotalRows();
1256: }
1257: 
1258: void DataTable::CommitDropTable() {
1259: 	// commit a drop of this table: mark all blocks as modified so they can be reclaimed later on
1260: 	row_groups->CommitDropTable();
1261: }
1262: 
1263: //===--------------------------------------------------------------------===//
1264: // GetStorageInfo
1265: //===--------------------------------------------------------------------===//
1266: void DataTable::GetStorageInfo(TableStorageInfo &result) {
1267: 	row_groups->GetStorageInfo(result);
1268: }
1269: 
1270: } // namespace duckdb
[end of src/storage/data_table.cpp]
[start of src/storage/index.cpp]
1: #include "duckdb/storage/index.hpp"
2: #include "duckdb/execution/expression_executor.hpp"
3: #include "duckdb/planner/expression_iterator.hpp"
4: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
5: #include "duckdb/planner/expression/bound_reference_expression.hpp"
6: #include "duckdb/storage/table/append_state.hpp"
7: #include "duckdb/execution/index/art/art.hpp"
8: 
9: namespace duckdb {
10: 
11: Index::Index(AttachedDatabase &db, IndexType type, TableIOManager &table_io_manager,
12:              const vector<column_t> &column_ids_p, const vector<unique_ptr<Expression>> &unbound_expressions,
13:              IndexConstraintType constraint_type_p, bool track_memory)
14: 
15:     : type(type), table_io_manager(table_io_manager), column_ids(column_ids_p), constraint_type(constraint_type_p),
16:       db(db), buffer_manager(BufferManager::GetBufferManager(db)), memory_size(0), track_memory(track_memory) {
17: 
18: 	for (auto &expr : unbound_expressions) {
19: 		types.push_back(expr->return_type.InternalType());
20: 		logical_types.push_back(expr->return_type);
21: 		auto unbound_expression = expr->Copy();
22: 		bound_expressions.push_back(BindExpression(unbound_expression->Copy()));
23: 		this->unbound_expressions.emplace_back(std::move(unbound_expression));
24: 	}
25: 	for (auto &bound_expr : bound_expressions) {
26: 		executor.AddExpression(*bound_expr);
27: 	}
28: 
29: 	// create the column id set
30: 	for (auto column_id : column_ids) {
31: 		column_id_set.insert(column_id);
32: 	}
33: }
34: 
35: void Index::InitializeLock(IndexLock &state) {
36: 	state.index_lock = unique_lock<mutex>(lock);
37: }
38: 
39: bool Index::Append(DataChunk &entries, Vector &row_identifiers) {
40: 	IndexLock state;
41: 	InitializeLock(state);
42: 	return Append(state, entries, row_identifiers);
43: }
44: 
45: void Index::Delete(DataChunk &entries, Vector &row_identifiers) {
46: 	IndexLock state;
47: 	InitializeLock(state);
48: 	Delete(state, entries, row_identifiers);
49: }
50: 
51: bool Index::MergeIndexes(Index *other_index) {
52: 
53: 	IndexLock state;
54: 	InitializeLock(state);
55: 
56: 	switch (this->type) {
57: 	case IndexType::ART: {
58: 		auto art = (ART *)this;
59: 		return art->MergeIndexes(state, other_index);
60: 	}
61: 	default:
62: 		throw InternalException("Unimplemented index type for merge");
63: 	}
64: }
65: 
66: void Index::ExecuteExpressions(DataChunk &input, DataChunk &result) {
67: 	executor.Execute(input, result);
68: }
69: 
70: unique_ptr<Expression> Index::BindExpression(unique_ptr<Expression> expr) {
71: 	if (expr->type == ExpressionType::BOUND_COLUMN_REF) {
72: 		auto &bound_colref = (BoundColumnRefExpression &)*expr;
73: 		return make_unique<BoundReferenceExpression>(expr->return_type, column_ids[bound_colref.binding.column_index]);
74: 	}
75: 	ExpressionIterator::EnumerateChildren(
76: 	    *expr, [this](unique_ptr<Expression> &expr) { expr = BindExpression(std::move(expr)); });
77: 	return expr;
78: }
79: 
80: bool Index::IndexIsUpdated(const vector<PhysicalIndex> &column_ids) const {
81: 	for (auto &column : column_ids) {
82: 		if (column_id_set.find(column.index) != column_id_set.end()) {
83: 			return true;
84: 		}
85: 	}
86: 	return false;
87: }
88: 
89: BlockPointer Index::Serialize(MetaBlockWriter &writer) {
90: 	throw NotImplementedException("The implementation of this index serialization does not exist.");
91: }
92: 
93: } // namespace duckdb
[end of src/storage/index.cpp]
[start of src/storage/local_storage.cpp]
1: #include "duckdb/transaction/local_storage.hpp"
2: #include "duckdb/execution/index/art/art.hpp"
3: #include "duckdb/storage/table/append_state.hpp"
4: #include "duckdb/storage/write_ahead_log.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/storage/table/row_group.hpp"
7: #include "duckdb/transaction/duck_transaction.hpp"
8: #include "duckdb/planner/table_filter.hpp"
9: #include "duckdb/storage/partial_block_manager.hpp"
10: 
11: #include "duckdb/storage/table/column_checkpoint_state.hpp"
12: #include "duckdb/storage/table/column_segment.hpp"
13: #include "duckdb/storage/table_io_manager.hpp"
14: 
15: namespace duckdb {
16: 
17: //===--------------------------------------------------------------------===//
18: // OptimisticDataWriter
19: //===--------------------------------------------------------------------===//
20: OptimisticDataWriter::OptimisticDataWriter(DataTable *table) : table(table) {
21: }
22: 
23: OptimisticDataWriter::OptimisticDataWriter(DataTable *table, OptimisticDataWriter &parent)
24:     : table(table), partial_manager(std::move(parent.partial_manager)),
25:       written_blocks(std::move(parent.written_blocks)) {
26: 	if (partial_manager) {
27: 		partial_manager->FlushPartialBlocks();
28: 	}
29: }
30: 
31: OptimisticDataWriter::~OptimisticDataWriter() {
32: }
33: 
34: bool OptimisticDataWriter::PrepareWrite() {
35: 	// check if we should pre-emptively write the table to disk
36: 	if (table->info->IsTemporary() || StorageManager::Get(table->info->db).InMemory()) {
37: 		return false;
38: 	}
39: 	// we should! write the second-to-last row group to disk
40: 	// allocate the partial block-manager if none is allocated yet
41: 	if (!partial_manager) {
42: 		auto &block_manager = table->info->table_io_manager->GetBlockManagerForRowData();
43: 		partial_manager = make_unique<PartialBlockManager>(block_manager);
44: 	}
45: 	return true;
46: }
47: 
48: void OptimisticDataWriter::CheckFlushToDisk(RowGroupCollection &row_groups) {
49: 	// we finished writing a complete row group
50: 	if (!PrepareWrite()) {
51: 		return;
52: 	}
53: 	// flush second-to-last row group
54: 	auto row_group = row_groups.GetRowGroup(-2);
55: 	FlushToDisk(row_group);
56: }
57: 
58: void OptimisticDataWriter::FlushToDisk(RowGroup *row_group) {
59: 	// flush the specified row group
60: 	D_ASSERT(row_group);
61: 	//! The set of column compression types (if any)
62: 	vector<CompressionType> compression_types;
63: 	D_ASSERT(compression_types.empty());
64: 	for (auto &column : table->column_definitions) {
65: 		compression_types.push_back(column.CompressionType());
66: 	}
67: 	auto row_group_pointer = row_group->WriteToDisk(*partial_manager, compression_types);
68: 
69: 	// update the set of written blocks
70: 	for (idx_t col_idx = 0; col_idx < row_group_pointer.statistics.size(); col_idx++) {
71: 		row_group_pointer.states[col_idx]->GetBlockIds(written_blocks);
72: 	}
73: }
74: 
75: void OptimisticDataWriter::FlushToDisk(RowGroupCollection &row_groups, bool force) {
76: 	if (!partial_manager) {
77: 		if (!force) {
78: 			// no partial manager - nothing to flush
79: 			return;
80: 		}
81: 		if (!PrepareWrite()) {
82: 			return;
83: 		}
84: 	}
85: 	// flush the last row group
86: 	FlushToDisk(row_groups.GetRowGroup(-1));
87: }
88: 
89: void OptimisticDataWriter::FinalFlush() {
90: 	if (!partial_manager) {
91: 		return;
92: 	}
93: 	// then flush the partial manager
94: 	partial_manager->FlushPartialBlocks();
95: 	partial_manager.reset();
96: }
97: 
98: void OptimisticDataWriter::Rollback() {
99: 	if (partial_manager) {
100: 		partial_manager->Clear();
101: 		partial_manager.reset();
102: 	}
103: 	if (!written_blocks.empty()) {
104: 		auto &block_manager = table->info->table_io_manager->GetBlockManagerForRowData();
105: 		for (auto block_id : written_blocks) {
106: 			block_manager.MarkBlockAsFree(block_id);
107: 		}
108: 	}
109: }
110: 
111: //===--------------------------------------------------------------------===//
112: // Local Table Storage
113: //===--------------------------------------------------------------------===//
114: LocalTableStorage::LocalTableStorage(DataTable &table)
115:     : table(&table), allocator(Allocator::Get(table.db)), deleted_rows(0), optimistic_writer(&table) {
116: 	auto types = table.GetTypes();
117: 	row_groups = make_shared<RowGroupCollection>(table.info, TableIOManager::Get(table).GetBlockManagerForRowData(),
118: 	                                             types, MAX_ROW_ID, 0);
119: 	row_groups->InitializeEmpty();
120: 	table.info->indexes.Scan([&](Index &index) {
121: 		D_ASSERT(index.type == IndexType::ART);
122: 		auto &art = (ART &)index;
123: 		if (art.constraint_type != IndexConstraintType::NONE) {
124: 			// unique index: create a local ART index that maintains the same unique constraint
125: 			vector<unique_ptr<Expression>> unbound_expressions;
126: 			for (auto &expr : art.unbound_expressions) {
127: 				unbound_expressions.push_back(expr->Copy());
128: 			}
129: 			indexes.AddIndex(make_unique<ART>(art.column_ids, art.table_io_manager, std::move(unbound_expressions),
130: 			                                  art.constraint_type, art.db, true));
131: 		}
132: 		return false;
133: 	});
134: }
135: 
136: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &new_dt, LocalTableStorage &parent,
137:                                      idx_t changed_idx, const LogicalType &target_type,
138:                                      const vector<column_t> &bound_columns, Expression &cast_expr)
139:     : table(&new_dt), allocator(Allocator::Get(table->db)), deleted_rows(parent.deleted_rows),
140:       optimistic_writer(table, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)) {
141: 	row_groups = parent.row_groups->AlterType(context, changed_idx, target_type, bound_columns, cast_expr);
142: 	parent.row_groups.reset();
143: 	indexes.Move(parent.indexes);
144: }
145: 
146: LocalTableStorage::LocalTableStorage(DataTable &new_dt, LocalTableStorage &parent, idx_t drop_idx)
147:     : table(&new_dt), allocator(Allocator::Get(table->db)), deleted_rows(parent.deleted_rows),
148:       optimistic_writer(table, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)) {
149: 	row_groups = parent.row_groups->RemoveColumn(drop_idx);
150: 	parent.row_groups.reset();
151: 	indexes.Move(parent.indexes);
152: }
153: 
154: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &new_dt, LocalTableStorage &parent,
155:                                      ColumnDefinition &new_column, Expression *default_value)
156:     : table(&new_dt), allocator(Allocator::Get(table->db)), deleted_rows(parent.deleted_rows),
157:       optimistic_writer(table, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)) {
158: 	row_groups = parent.row_groups->AddColumn(context, new_column, default_value);
159: 	parent.row_groups.reset();
160: 	indexes.Move(parent.indexes);
161: }
162: 
163: LocalTableStorage::~LocalTableStorage() {
164: }
165: 
166: void LocalTableStorage::InitializeScan(CollectionScanState &state, TableFilterSet *table_filters) {
167: 	if (row_groups->GetTotalRows() == 0) {
168: 		// nothing to scan
169: 		return;
170: 	}
171: 	row_groups->InitializeScan(state, state.GetColumnIds(), table_filters);
172: }
173: 
174: idx_t LocalTableStorage::EstimatedSize() {
175: 	idx_t appended_rows = row_groups->GetTotalRows() - deleted_rows;
176: 	if (appended_rows == 0) {
177: 		return 0;
178: 	}
179: 	idx_t row_size = 0;
180: 	auto &types = row_groups->GetTypes();
181: 	for (auto &type : types) {
182: 		row_size += GetTypeIdSize(type.InternalType());
183: 	}
184: 	return appended_rows * row_size;
185: }
186: 
187: void LocalTableStorage::CheckFlushToDisk() {
188: 	if (deleted_rows != 0) {
189: 		// we have deletes - we cannot merge row groups
190: 		return;
191: 	}
192: 	optimistic_writer.CheckFlushToDisk(*row_groups);
193: }
194: 
195: void LocalTableStorage::FlushToDisk() {
196: 	optimistic_writer.FlushToDisk(*row_groups);
197: 	optimistic_writer.FinalFlush();
198: }
199: 
200: bool LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source,
201:                                         TableIndexList &index_list, const vector<LogicalType> &table_types,
202:                                         row_t &start_row) {
203: 	// only need to scan for index append
204: 	// figure out which columns we need to scan for the set of indexes
205: 	auto columns = index_list.GetRequiredColumns();
206: 	// create an empty mock chunk that contains all the correct types for the table
207: 	DataChunk mock_chunk;
208: 	mock_chunk.InitializeEmpty(table_types);
209: 	bool success = true;
210: 	source.Scan(transaction, columns, [&](DataChunk &chunk) -> bool {
211: 		// construct the mock chunk by referencing the required columns
212: 		for (idx_t i = 0; i < columns.size(); i++) {
213: 			mock_chunk.data[columns[i]].Reference(chunk.data[i]);
214: 		}
215: 		mock_chunk.SetCardinality(chunk);
216: 		// append this chunk to the indexes of the table
217: 		if (!DataTable::AppendToIndexes(index_list, mock_chunk, start_row)) {
218: 			success = false;
219: 			return false;
220: 		}
221: 		start_row += chunk.size();
222: 		return true;
223: 	});
224: 	return success;
225: }
226: 
227: void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppendState &append_state,
228:                                         idx_t append_count, bool append_to_table) {
229: 	bool constraint_violated = false;
230: 	if (append_to_table) {
231: 		table->InitializeAppend(transaction, append_state, append_count);
232: 	}
233: 	if (append_to_table) {
234: 		// appending: need to scan entire
235: 		row_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {
236: 			// append this chunk to the indexes of the table
237: 			if (!table->AppendToIndexes(chunk, append_state.current_row)) {
238: 				constraint_violated = true;
239: 				return false;
240: 			}
241: 			// append to base table
242: 			table->Append(chunk, append_state);
243: 			return true;
244: 		});
245: 	} else {
246: 		constraint_violated = !AppendToIndexes(transaction, *row_groups, table->info->indexes, table->GetTypes(),
247: 		                                       append_state.current_row);
248: 	}
249: 	if (constraint_violated) {
250: 		PreservedError error;
251: 		// need to revert the append
252: 		row_t current_row = append_state.row_start;
253: 		// remove the data from the indexes, if there are any indexes
254: 		row_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {
255: 			// append this chunk to the indexes of the table
256: 			try {
257: 				table->RemoveFromIndexes(append_state, chunk, current_row);
258: 			} catch (Exception &ex) {
259: 				error = PreservedError(ex);
260: 				return false;
261: 			} catch (std::exception &ex) {
262: 				error = PreservedError(ex);
263: 				return false;
264: 			}
265: 
266: 			current_row += chunk.size();
267: 			if (current_row >= append_state.current_row) {
268: 				// finished deleting all rows from the index: abort now
269: 				return false;
270: 			}
271: 			return true;
272: 		});
273: 		if (append_to_table) {
274: 			table->RevertAppendInternal(append_state.row_start, append_count);
275: 		}
276: 		if (error) {
277: 			error.Throw();
278: 		}
279: 		throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
280: 	}
281: }
282: 
283: OptimisticDataWriter *LocalTableStorage::CreateOptimisticWriter() {
284: 	auto writer = make_unique<OptimisticDataWriter>(table);
285: 	optimistic_writers.push_back(std::move(writer));
286: 	return optimistic_writers.back().get();
287: }
288: 
289: void LocalTableStorage::Rollback() {
290: 	optimistic_writer.Rollback();
291: 	for (auto &writer : optimistic_writers) {
292: 		writer->Rollback();
293: 	}
294: 	optimistic_writers.clear();
295: }
296: 
297: //===--------------------------------------------------------------------===//
298: // LocalTableManager
299: //===--------------------------------------------------------------------===//
300: LocalTableStorage *LocalTableManager::GetStorage(DataTable *table) {
301: 	lock_guard<mutex> l(table_storage_lock);
302: 	auto entry = table_storage.find(table);
303: 	return entry == table_storage.end() ? nullptr : entry->second.get();
304: }
305: 
306: LocalTableStorage *LocalTableManager::GetOrCreateStorage(DataTable *table) {
307: 	lock_guard<mutex> l(table_storage_lock);
308: 	auto entry = table_storage.find(table);
309: 	if (entry == table_storage.end()) {
310: 		auto new_storage = make_shared<LocalTableStorage>(*table);
311: 		auto storage = new_storage.get();
312: 		table_storage.insert(make_pair(table, std::move(new_storage)));
313: 		return storage;
314: 	} else {
315: 		return entry->second.get();
316: 	}
317: }
318: 
319: bool LocalTableManager::IsEmpty() {
320: 	lock_guard<mutex> l(table_storage_lock);
321: 	return table_storage.empty();
322: }
323: 
324: shared_ptr<LocalTableStorage> LocalTableManager::MoveEntry(DataTable *table) {
325: 	lock_guard<mutex> l(table_storage_lock);
326: 	auto entry = table_storage.find(table);
327: 	if (entry == table_storage.end()) {
328: 		return nullptr;
329: 	}
330: 	auto storage_entry = std::move(entry->second);
331: 	table_storage.erase(table);
332: 	return storage_entry;
333: }
334: 
335: unordered_map<DataTable *, shared_ptr<LocalTableStorage>> LocalTableManager::MoveEntries() {
336: 	lock_guard<mutex> l(table_storage_lock);
337: 	return std::move(table_storage);
338: }
339: 
340: idx_t LocalTableManager::EstimatedSize() {
341: 	lock_guard<mutex> l(table_storage_lock);
342: 	idx_t estimated_size = 0;
343: 	for (auto &storage : table_storage) {
344: 		estimated_size += storage.second->EstimatedSize();
345: 	}
346: 	return estimated_size;
347: }
348: 
349: void LocalTableManager::InsertEntry(DataTable *table, shared_ptr<LocalTableStorage> entry) {
350: 	lock_guard<mutex> l(table_storage_lock);
351: 	D_ASSERT(table_storage.find(table) == table_storage.end());
352: 	table_storage[table] = std::move(entry);
353: }
354: 
355: //===--------------------------------------------------------------------===//
356: // LocalStorage
357: //===--------------------------------------------------------------------===//
358: LocalStorage::LocalStorage(ClientContext &context, DuckTransaction &transaction)
359:     : context(context), transaction(transaction) {
360: }
361: 
362: LocalStorage &LocalStorage::Get(DuckTransaction &transaction) {
363: 	return transaction.GetLocalStorage();
364: }
365: 
366: LocalStorage &LocalStorage::Get(ClientContext &context, AttachedDatabase &db) {
367: 	return DuckTransaction::Get(context, db).GetLocalStorage();
368: }
369: 
370: LocalStorage &LocalStorage::Get(ClientContext &context, Catalog &catalog) {
371: 	return LocalStorage::Get(context, catalog.GetAttached());
372: }
373: 
374: void LocalStorage::InitializeScan(DataTable *table, CollectionScanState &state, TableFilterSet *table_filters) {
375: 	auto storage = table_manager.GetStorage(table);
376: 	if (storage == nullptr) {
377: 		return;
378: 	}
379: 	storage->InitializeScan(state, table_filters);
380: }
381: 
382: void LocalStorage::Scan(CollectionScanState &state, const vector<column_t> &column_ids, DataChunk &result) {
383: 	state.Scan(transaction, result);
384: }
385: 
386: void LocalStorage::InitializeParallelScan(DataTable *table, ParallelCollectionScanState &state) {
387: 	auto storage = table_manager.GetStorage(table);
388: 	if (!storage) {
389: 		state.max_row = 0;
390: 		state.vector_index = 0;
391: 		state.current_row_group = nullptr;
392: 	} else {
393: 		storage->row_groups->InitializeParallelScan(state);
394: 	}
395: }
396: 
397: bool LocalStorage::NextParallelScan(ClientContext &context, DataTable *table, ParallelCollectionScanState &state,
398:                                     CollectionScanState &scan_state) {
399: 	auto storage = table_manager.GetStorage(table);
400: 	if (!storage) {
401: 		return false;
402: 	}
403: 	return storage->row_groups->NextParallelScan(context, state, scan_state);
404: }
405: 
406: void LocalStorage::InitializeAppend(LocalAppendState &state, DataTable *table) {
407: 	state.storage = table_manager.GetOrCreateStorage(table);
408: 	state.storage->row_groups->InitializeAppend(TransactionData(transaction), state.append_state, 0);
409: }
410: 
411: void LocalStorage::Append(LocalAppendState &state, DataChunk &chunk) {
412: 	// append to unique indices (if any)
413: 	auto storage = state.storage;
414: 	idx_t base_id = MAX_ROW_ID + storage->row_groups->GetTotalRows() + state.append_state.total_append_count;
415: 	if (!DataTable::AppendToIndexes(storage->indexes, chunk, base_id)) {
416: 		throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
417: 	}
418: 
419: 	//! Append the chunk to the local storage
420: 	auto new_row_group = storage->row_groups->Append(chunk, state.append_state);
421: 
422: 	//! Check if we should pre-emptively flush blocks to disk
423: 	if (new_row_group) {
424: 		storage->CheckFlushToDisk();
425: 	}
426: }
427: 
428: void LocalStorage::FinalizeAppend(LocalAppendState &state) {
429: 	state.storage->row_groups->FinalizeAppend(state.append_state.transaction, state.append_state);
430: }
431: 
432: void LocalStorage::LocalMerge(DataTable *table, RowGroupCollection &collection) {
433: 	auto storage = table_manager.GetOrCreateStorage(table);
434: 	if (!storage->indexes.Empty()) {
435: 		// append data to indexes if required
436: 		row_t base_id = MAX_ROW_ID + storage->row_groups->GetTotalRows();
437: 		bool success = storage->AppendToIndexes(transaction, collection, storage->indexes, table->GetTypes(), base_id);
438: 		if (!success) {
439: 			throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
440: 		}
441: 	}
442: 	storage->row_groups->MergeStorage(collection);
443: }
444: 
445: OptimisticDataWriter *LocalStorage::CreateOptimisticWriter(DataTable *table) {
446: 	auto storage = table_manager.GetOrCreateStorage(table);
447: 	return storage->CreateOptimisticWriter();
448: }
449: 
450: bool LocalStorage::ChangesMade() noexcept {
451: 	return !table_manager.IsEmpty();
452: }
453: 
454: bool LocalStorage::Find(DataTable *table) {
455: 	return table_manager.GetStorage(table) != nullptr;
456: }
457: 
458: idx_t LocalStorage::EstimatedSize() {
459: 	return table_manager.EstimatedSize();
460: }
461: 
462: idx_t LocalStorage::Delete(DataTable *table, Vector &row_ids, idx_t count) {
463: 	auto storage = table_manager.GetStorage(table);
464: 	D_ASSERT(storage);
465: 
466: 	// delete from unique indices (if any)
467: 	if (!storage->indexes.Empty()) {
468: 		storage->row_groups->RemoveFromIndexes(storage->indexes, row_ids, count);
469: 	}
470: 
471: 	auto ids = FlatVector::GetData<row_t>(row_ids);
472: 	idx_t delete_count = storage->row_groups->Delete(TransactionData(0, 0), table, ids, count);
473: 	storage->deleted_rows += delete_count;
474: 	return delete_count;
475: }
476: 
477: void LocalStorage::Update(DataTable *table, Vector &row_ids, const vector<PhysicalIndex> &column_ids,
478:                           DataChunk &updates) {
479: 	auto storage = table_manager.GetStorage(table);
480: 	D_ASSERT(storage);
481: 
482: 	auto ids = FlatVector::GetData<row_t>(row_ids);
483: 	storage->row_groups->Update(TransactionData(0, 0), ids, column_ids, updates);
484: }
485: 
486: void LocalStorage::Flush(DataTable &table, LocalTableStorage &storage) {
487: 	if (storage.row_groups->GetTotalRows() <= storage.deleted_rows) {
488: 		return;
489: 	}
490: 	idx_t append_count = storage.row_groups->GetTotalRows() - storage.deleted_rows;
491: 
492: 	TableAppendState append_state;
493: 	table.AppendLock(append_state);
494: 	if ((append_state.row_start == 0 || storage.row_groups->GetTotalRows() >= MERGE_THRESHOLD) &&
495: 	    storage.deleted_rows == 0) {
496: 		// table is currently empty OR we are bulk appending: move over the storage directly
497: 		// first flush any out-standing storage nodes
498: 		storage.FlushToDisk();
499: 		// now append to the indexes (if there are any)
500: 		// FIXME: we should be able to merge the transaction-local index directly into the main table index
501: 		// as long we just rewrite some row-ids
502: 		if (!table.info->indexes.Empty()) {
503: 			storage.AppendToIndexes(transaction, append_state, append_count, false);
504: 		}
505: 		// finally move over the row groups
506: 		table.MergeStorage(*storage.row_groups, storage.indexes);
507: 	} else {
508: 		// check if we have written data
509: 		// if we have, we cannot merge to disk after all
510: 		// so we need to revert the data we have already written
511: 		storage.Rollback();
512: 		// append to the indexes and append to the base table
513: 		storage.AppendToIndexes(transaction, append_state, append_count, true);
514: 	}
515: 	transaction.PushAppend(&table, append_state.row_start, append_count);
516: }
517: 
518: void LocalStorage::Commit(LocalStorage::CommitState &commit_state, DuckTransaction &transaction) {
519: 	// commit local storage
520: 	// iterate over all entries in the table storage map and commit them
521: 	// after this, the local storage is no longer required and can be cleared
522: 	auto table_storage = table_manager.MoveEntries();
523: 	for (auto &entry : table_storage) {
524: 		auto table = entry.first;
525: 		auto storage = entry.second.get();
526: 		Flush(*table, *storage);
527: 
528: 		entry.second.reset();
529: 	}
530: }
531: 
532: void LocalStorage::Rollback() {
533: 	// rollback local storage
534: 	// after this, the local storage is no longer required and can be cleared
535: 	auto table_storage = table_manager.MoveEntries();
536: 	for (auto &entry : table_storage) {
537: 		auto storage = entry.second.get();
538: 		if (!storage) {
539: 			continue;
540: 		}
541: 		storage->Rollback();
542: 
543: 		entry.second.reset();
544: 	}
545: }
546: 
547: idx_t LocalStorage::AddedRows(DataTable *table) {
548: 	auto storage = table_manager.GetStorage(table);
549: 	if (!storage) {
550: 		return 0;
551: 	}
552: 	return storage->row_groups->GetTotalRows() - storage->deleted_rows;
553: }
554: 
555: void LocalStorage::MoveStorage(DataTable *old_dt, DataTable *new_dt) {
556: 	// check if there are any pending appends for the old version of the table
557: 	auto new_storage = table_manager.MoveEntry(old_dt);
558: 	if (!new_storage) {
559: 		return;
560: 	}
561: 	// take over the storage from the old entry
562: 	new_storage->table = new_dt;
563: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
564: }
565: 
566: void LocalStorage::AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column,
567:                              Expression *default_value) {
568: 	// check if there are any pending appends for the old version of the table
569: 	auto storage = table_manager.MoveEntry(old_dt);
570: 	if (!storage) {
571: 		return;
572: 	}
573: 	auto new_storage = make_unique<LocalTableStorage>(context, *new_dt, *storage, new_column, default_value);
574: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
575: }
576: 
577: void LocalStorage::DropColumn(DataTable *old_dt, DataTable *new_dt, idx_t removed_column) {
578: 	// check if there are any pending appends for the old version of the table
579: 	auto storage = table_manager.MoveEntry(old_dt);
580: 	if (!storage) {
581: 		return;
582: 	}
583: 	auto new_storage = make_unique<LocalTableStorage>(*new_dt, *storage, removed_column);
584: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
585: }
586: 
587: void LocalStorage::ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, const LogicalType &target_type,
588:                               const vector<column_t> &bound_columns, Expression &cast_expr) {
589: 	// check if there are any pending appends for the old version of the table
590: 	auto storage = table_manager.MoveEntry(old_dt);
591: 	if (!storage) {
592: 		return;
593: 	}
594: 	auto new_storage =
595: 	    make_unique<LocalTableStorage>(context, *new_dt, *storage, changed_idx, target_type, bound_columns, cast_expr);
596: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
597: }
598: 
599: void LocalStorage::FetchChunk(DataTable *table, Vector &row_ids, idx_t count, const vector<column_t> &col_ids,
600:                               DataChunk &chunk, ColumnFetchState &fetch_state) {
601: 	auto storage = table_manager.GetStorage(table);
602: 	if (!storage) {
603: 		throw InternalException("LocalStorage::FetchChunk - local storage not found");
604: 	}
605: 
606: 	storage->row_groups->Fetch(transaction, chunk, col_ids, row_ids, count, fetch_state);
607: }
608: 
609: TableIndexList &LocalStorage::GetIndexes(DataTable *table) {
610: 	auto storage = table_manager.GetStorage(table);
611: 	if (!storage) {
612: 		throw InternalException("LocalStorage::GetIndexes - local storage not found");
613: 	}
614: 	return storage->indexes;
615: }
616: 
617: void LocalStorage::VerifyNewConstraint(DataTable &parent, const BoundConstraint &constraint) {
618: 	auto storage = table_manager.GetStorage(&parent);
619: 	if (!storage) {
620: 		return;
621: 	}
622: 	storage->row_groups->VerifyNewConstraint(parent, constraint);
623: }
624: 
625: } // namespace duckdb
[end of src/storage/local_storage.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: