You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
LEFT JOIN UNNEST() requires an ON clause
### What happens?

Sometimes you want to UNNEST with a LEFT JOIN.  Currently you have to provide an ON clause in order to do this.  I have a hacky workaround that I would like to remove.



### To Reproduce

```
with t as (
  select 
    1 as r, 
    [
      {n:1},
      {n:2}
    ] as s
  union   select 
    2 as r, 
    [
      {n:3},
      {n:4}
    ] as s
     
)
select r, s1.s.n from t
LEFT JOIN UNNEST(s) as s1(s)
```

### OS:

linux

### DuckDB Version:

current

### DuckDB Client:

web shell

### Full Name:

lloyd tabb

### Affiliation:

google

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
11:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
12:   </a>
13:   <a href="https://discord.gg/tcvwpjfnZx">
14:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
15:   </a>
16:   <a href="https://github.com/duckdb/duckdb/releases/">
17:     <img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release">
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of src/execution/operator/projection/physical_tableinout_function.cpp]
1: #include "duckdb/execution/operator/projection/physical_tableinout_function.hpp"
2: 
3: namespace duckdb {
4: 
5: class TableInOutLocalState : public OperatorState {
6: public:
7: 	TableInOutLocalState() : row_index(0), new_row(true) {
8: 	}
9: 
10: 	unique_ptr<LocalTableFunctionState> local_state;
11: 	idx_t row_index;
12: 	bool new_row;
13: 	DataChunk input_chunk;
14: };
15: 
16: class TableInOutGlobalState : public GlobalOperatorState {
17: public:
18: 	TableInOutGlobalState() {
19: 	}
20: 
21: 	unique_ptr<GlobalTableFunctionState> global_state;
22: };
23: 
24: PhysicalTableInOutFunction::PhysicalTableInOutFunction(vector<LogicalType> types, TableFunction function_p,
25:                                                        unique_ptr<FunctionData> bind_data_p,
26:                                                        vector<column_t> column_ids_p, idx_t estimated_cardinality,
27:                                                        vector<column_t> project_input_p)
28:     : PhysicalOperator(PhysicalOperatorType::INOUT_FUNCTION, std::move(types), estimated_cardinality),
29:       function(std::move(function_p)), bind_data(std::move(bind_data_p)), column_ids(std::move(column_ids_p)),
30:       projected_input(std::move(project_input_p)) {
31: }
32: 
33: unique_ptr<OperatorState> PhysicalTableInOutFunction::GetOperatorState(ExecutionContext &context) const {
34: 	auto &gstate = op_state->Cast<TableInOutGlobalState>();
35: 	auto result = make_uniq<TableInOutLocalState>();
36: 	if (function.init_local) {
37: 		TableFunctionInitInput input(bind_data.get(), column_ids, vector<idx_t>(), nullptr);
38: 		result->local_state = function.init_local(context, input, gstate.global_state.get());
39: 	}
40: 	if (!projected_input.empty()) {
41: 		result->input_chunk.Initialize(context.client, children[0]->types);
42: 	}
43: 	return std::move(result);
44: }
45: 
46: unique_ptr<GlobalOperatorState> PhysicalTableInOutFunction::GetGlobalOperatorState(ClientContext &context) const {
47: 	auto result = make_uniq<TableInOutGlobalState>();
48: 	if (function.init_global) {
49: 		TableFunctionInitInput input(bind_data.get(), column_ids, vector<idx_t>(), nullptr);
50: 		result->global_state = function.init_global(context, input);
51: 	}
52: 	return std::move(result);
53: }
54: 
55: OperatorResultType PhysicalTableInOutFunction::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
56:                                                        GlobalOperatorState &gstate_p, OperatorState &state_p) const {
57: 	auto &gstate = gstate_p.Cast<TableInOutGlobalState>();
58: 	auto &state = state_p.Cast<TableInOutLocalState>();
59: 	TableFunctionInput data(bind_data.get(), state.local_state.get(), gstate.global_state.get());
60: 	if (projected_input.empty()) {
61: 		// straightforward case - no need to project input
62: 		return function.in_out_function(context, data, input, chunk);
63: 	}
64: 	// when project_input is set we execute the input function row-by-row
65: 	if (state.new_row) {
66: 		if (state.row_index >= input.size()) {
67: 			// finished processing this chunk
68: 			state.new_row = true;
69: 			state.row_index = 0;
70: 			return OperatorResultType::NEED_MORE_INPUT;
71: 		}
72: 		// we are processing a new row: fetch the data for the current row
73: 		D_ASSERT(input.ColumnCount() == state.input_chunk.ColumnCount());
74: 		// set up the input data to the table in-out function
75: 		for (idx_t col_idx = 0; col_idx < input.ColumnCount(); col_idx++) {
76: 			ConstantVector::Reference(state.input_chunk.data[col_idx], input.data[col_idx], state.row_index, 1);
77: 		}
78: 		state.input_chunk.SetCardinality(1);
79: 		state.row_index++;
80: 		state.new_row = false;
81: 	}
82: 	// set up the output data in "chunk"
83: 	D_ASSERT(chunk.ColumnCount() > projected_input.size());
84: 	D_ASSERT(state.row_index > 0);
85: 	idx_t base_idx = chunk.ColumnCount() - projected_input.size();
86: 	for (idx_t project_idx = 0; project_idx < projected_input.size(); project_idx++) {
87: 		auto source_idx = projected_input[project_idx];
88: 		auto target_idx = base_idx + project_idx;
89: 		ConstantVector::Reference(chunk.data[target_idx], input.data[source_idx], state.row_index - 1, 1);
90: 	}
91: 	auto result = function.in_out_function(context, data, state.input_chunk, chunk);
92: 	if (result == OperatorResultType::FINISHED) {
93: 		return result;
94: 	}
95: 	if (result == OperatorResultType::NEED_MORE_INPUT) {
96: 		// we finished processing this row: move to the next row
97: 		state.new_row = true;
98: 	}
99: 	return OperatorResultType::HAVE_MORE_OUTPUT;
100: }
101: 
102: OperatorFinalizeResultType PhysicalTableInOutFunction::FinalExecute(ExecutionContext &context, DataChunk &chunk,
103:                                                                     GlobalOperatorState &gstate_p,
104:                                                                     OperatorState &state_p) const {
105: 	auto &gstate = gstate_p.Cast<TableInOutGlobalState>();
106: 	auto &state = state_p.Cast<TableInOutLocalState>();
107: 	if (!projected_input.empty()) {
108: 		throw InternalException("FinalExecute not supported for project_input");
109: 	}
110: 	TableFunctionInput data(bind_data.get(), state.local_state.get(), gstate.global_state.get());
111: 	return function.in_out_function_final(context, data, chunk);
112: }
113: 
114: } // namespace duckdb
[end of src/execution/operator/projection/physical_tableinout_function.cpp]
[start of src/execution/operator/projection/physical_unnest.cpp]
1: #include "duckdb/execution/operator/projection/physical_unnest.hpp"
2: 
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/common/algorithm.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: #include "duckdb/planner/expression/bound_reference_expression.hpp"
7: #include "duckdb/planner/expression/bound_unnest_expression.hpp"
8: 
9: namespace duckdb {
10: 
11: class UnnestOperatorState : public OperatorState {
12: public:
13: 	UnnestOperatorState(ClientContext &context, const vector<unique_ptr<Expression>> &select_list)
14: 	    : current_row(0), list_position(0), longest_list_length(DConstants::INVALID_INDEX), first_fetch(true),
15: 	      executor(context) {
16: 
17: 		// for each UNNEST in the select_list, we add the child expression to the expression executor
18: 		// and set the return type in the list_data chunk, which will contain the evaluated expression results
19: 		vector<LogicalType> list_data_types;
20: 		for (auto &exp : select_list) {
21: 			D_ASSERT(exp->type == ExpressionType::BOUND_UNNEST);
22: 			auto &bue = exp->Cast<BoundUnnestExpression>();
23: 			list_data_types.push_back(bue.child->return_type);
24: 			executor.AddExpression(*bue.child.get());
25: 		}
26: 
27: 		auto &allocator = Allocator::Get(context);
28: 		list_data.Initialize(allocator, list_data_types);
29: 
30: 		list_vector_data.resize(list_data.ColumnCount());
31: 		list_child_data.resize(list_data.ColumnCount());
32: 	}
33: 
34: 	idx_t current_row;
35: 	idx_t list_position;
36: 	idx_t longest_list_length;
37: 	bool first_fetch;
38: 
39: 	ExpressionExecutor executor;
40: 	DataChunk list_data;
41: 	vector<UnifiedVectorFormat> list_vector_data;
42: 	vector<UnifiedVectorFormat> list_child_data;
43: 
44: public:
45: 	//! Reset the fields of the unnest operator state
46: 	void Reset();
47: 	//! Set the longest list's length for the current row
48: 	void SetLongestListLength();
49: };
50: 
51: void UnnestOperatorState::Reset() {
52: 	current_row = 0;
53: 	list_position = 0;
54: 	longest_list_length = DConstants::INVALID_INDEX;
55: 	first_fetch = true;
56: }
57: 
58: void UnnestOperatorState::SetLongestListLength() {
59: 	longest_list_length = 0;
60: 	for (idx_t col_idx = 0; col_idx < list_data.ColumnCount(); col_idx++) {
61: 
62: 		auto &vector_data = list_vector_data[col_idx];
63: 		auto current_idx = vector_data.sel->get_index(current_row);
64: 
65: 		if (vector_data.validity.RowIsValid(current_idx)) {
66: 
67: 			// check if this list is longer
68: 			auto list_data = UnifiedVectorFormat::GetData<list_entry_t>(vector_data);
69: 			auto list_entry = list_data[current_idx];
70: 			if (list_entry.length > longest_list_length) {
71: 				longest_list_length = list_entry.length;
72: 			}
73: 		}
74: 	}
75: }
76: 
77: PhysicalUnnest::PhysicalUnnest(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list,
78:                                idx_t estimated_cardinality, PhysicalOperatorType type)
79:     : PhysicalOperator(type, std::move(types), estimated_cardinality), select_list(std::move(select_list)) {
80: 	D_ASSERT(!this->select_list.empty());
81: }
82: 
83: static void UnnestNull(idx_t start, idx_t end, Vector &result) {
84: 
85: 	D_ASSERT(result.GetVectorType() == VectorType::FLAT_VECTOR);
86: 	auto &validity = FlatVector::Validity(result);
87: 	for (idx_t i = start; i < end; i++) {
88: 		validity.SetInvalid(i);
89: 	}
90: 	if (result.GetType().InternalType() == PhysicalType::STRUCT) {
91: 		auto &struct_children = StructVector::GetEntries(result);
92: 		for (auto &child : struct_children) {
93: 			UnnestNull(start, end, *child);
94: 		}
95: 	}
96: }
97: 
98: template <class T>
99: static void TemplatedUnnest(UnifiedVectorFormat &vector_data, idx_t start, idx_t end, Vector &result) {
100: 
101: 	auto source_data = UnifiedVectorFormat::GetData<T>(vector_data);
102: 	auto &source_mask = vector_data.validity;
103: 
104: 	D_ASSERT(result.GetVectorType() == VectorType::FLAT_VECTOR);
105: 	auto result_data = FlatVector::GetData<T>(result);
106: 	auto &result_mask = FlatVector::Validity(result);
107: 
108: 	for (idx_t i = start; i < end; i++) {
109: 		auto source_idx = vector_data.sel->get_index(i);
110: 		auto target_idx = i - start;
111: 		if (source_mask.RowIsValid(source_idx)) {
112: 			result_data[target_idx] = source_data[source_idx];
113: 			result_mask.SetValid(target_idx);
114: 		} else {
115: 			result_mask.SetInvalid(target_idx);
116: 		}
117: 	}
118: }
119: 
120: static void UnnestValidity(UnifiedVectorFormat &vector_data, idx_t start, idx_t end, Vector &result) {
121: 
122: 	auto &source_mask = vector_data.validity;
123: 	D_ASSERT(result.GetVectorType() == VectorType::FLAT_VECTOR);
124: 	auto &result_mask = FlatVector::Validity(result);
125: 
126: 	for (idx_t i = start; i < end; i++) {
127: 		auto source_idx = vector_data.sel->get_index(i);
128: 		auto target_idx = i - start;
129: 		result_mask.Set(target_idx, source_mask.RowIsValid(source_idx));
130: 	}
131: }
132: 
133: static void UnnestVector(UnifiedVectorFormat &child_vector_data, Vector &child_vector, idx_t list_size, idx_t start,
134:                          idx_t end, Vector &result) {
135: 
136: 	D_ASSERT(child_vector.GetType() == result.GetType());
137: 	switch (result.GetType().InternalType()) {
138: 	case PhysicalType::BOOL:
139: 	case PhysicalType::INT8:
140: 		TemplatedUnnest<int8_t>(child_vector_data, start, end, result);
141: 		break;
142: 	case PhysicalType::INT16:
143: 		TemplatedUnnest<int16_t>(child_vector_data, start, end, result);
144: 		break;
145: 	case PhysicalType::INT32:
146: 		TemplatedUnnest<int32_t>(child_vector_data, start, end, result);
147: 		break;
148: 	case PhysicalType::INT64:
149: 		TemplatedUnnest<int64_t>(child_vector_data, start, end, result);
150: 		break;
151: 	case PhysicalType::INT128:
152: 		TemplatedUnnest<hugeint_t>(child_vector_data, start, end, result);
153: 		break;
154: 	case PhysicalType::UINT8:
155: 		TemplatedUnnest<uint8_t>(child_vector_data, start, end, result);
156: 		break;
157: 	case PhysicalType::UINT16:
158: 		TemplatedUnnest<uint16_t>(child_vector_data, start, end, result);
159: 		break;
160: 	case PhysicalType::UINT32:
161: 		TemplatedUnnest<uint32_t>(child_vector_data, start, end, result);
162: 		break;
163: 	case PhysicalType::UINT64:
164: 		TemplatedUnnest<uint64_t>(child_vector_data, start, end, result);
165: 		break;
166: 	case PhysicalType::FLOAT:
167: 		TemplatedUnnest<float>(child_vector_data, start, end, result);
168: 		break;
169: 	case PhysicalType::DOUBLE:
170: 		TemplatedUnnest<double>(child_vector_data, start, end, result);
171: 		break;
172: 	case PhysicalType::INTERVAL:
173: 		TemplatedUnnest<interval_t>(child_vector_data, start, end, result);
174: 		break;
175: 	case PhysicalType::VARCHAR:
176: 		TemplatedUnnest<string_t>(child_vector_data, start, end, result);
177: 		break;
178: 	case PhysicalType::LIST: {
179: 		// the child vector of result now references the child vector source
180: 		// FIXME: only reference relevant children (start - end) instead of all
181: 		auto &target = ListVector::GetEntry(result);
182: 		target.Reference(ListVector::GetEntry(child_vector));
183: 		ListVector::SetListSize(result, ListVector::GetListSize(child_vector));
184: 		// unnest
185: 		TemplatedUnnest<list_entry_t>(child_vector_data, start, end, result);
186: 		break;
187: 	}
188: 	case PhysicalType::STRUCT: {
189: 		auto &child_vector_entries = StructVector::GetEntries(child_vector);
190: 		auto &result_entries = StructVector::GetEntries(result);
191: 
192: 		// set the validity mask for the 'outer' struct vector before unnesting its children
193: 		UnnestValidity(child_vector_data, start, end, result);
194: 
195: 		for (idx_t i = 0; i < child_vector_entries.size(); i++) {
196: 			UnifiedVectorFormat child_vector_entries_data;
197: 			child_vector_entries[i]->ToUnifiedFormat(list_size, child_vector_entries_data);
198: 			UnnestVector(child_vector_entries_data, *child_vector_entries[i], list_size, start, end,
199: 			             *result_entries[i]);
200: 		}
201: 		break;
202: 	}
203: 	default:
204: 		throw InternalException("Unimplemented type for UNNEST.");
205: 	}
206: }
207: 
208: static void PrepareInput(UnnestOperatorState &state, DataChunk &input,
209:                          const vector<unique_ptr<Expression>> &select_list) {
210: 
211: 	state.list_data.Reset();
212: 	// execute the expressions inside each UNNEST in the select_list to get the list data
213: 	// execution results (lists) are kept in state.list_data chunk
214: 	state.executor.Execute(input, state.list_data);
215: 
216: 	// verify incoming lists
217: 	state.list_data.Verify();
218: 	D_ASSERT(input.size() == state.list_data.size());
219: 	D_ASSERT(state.list_data.ColumnCount() == select_list.size());
220: 	D_ASSERT(state.list_vector_data.size() == state.list_data.ColumnCount());
221: 	D_ASSERT(state.list_child_data.size() == state.list_data.ColumnCount());
222: 
223: 	// get the UnifiedVectorFormat of each list_data vector (LIST vectors for the different UNNESTs)
224: 	// both for the vector itself and its child vector
225: 	for (idx_t col_idx = 0; col_idx < state.list_data.ColumnCount(); col_idx++) {
226: 
227: 		auto &list_vector = state.list_data.data[col_idx];
228: 		list_vector.ToUnifiedFormat(state.list_data.size(), state.list_vector_data[col_idx]);
229: 
230: 		if (list_vector.GetType() == LogicalType::SQLNULL) {
231: 			// UNNEST(NULL): SQLNULL vectors don't have child vectors, but we need to point to the child vector of
232: 			// each vector, so we just get the UnifiedVectorFormat of the vector itself
233: 			auto &child_vector = list_vector;
234: 			child_vector.ToUnifiedFormat(0, state.list_child_data[col_idx]);
235: 		} else {
236: 			auto list_size = ListVector::GetListSize(list_vector);
237: 			auto &child_vector = ListVector::GetEntry(list_vector);
238: 			child_vector.ToUnifiedFormat(list_size, state.list_child_data[col_idx]);
239: 		}
240: 	}
241: 
242: 	state.first_fetch = false;
243: }
244: 
245: unique_ptr<OperatorState> PhysicalUnnest::GetOperatorState(ExecutionContext &context) const {
246: 	return PhysicalUnnest::GetState(context, select_list);
247: }
248: 
249: unique_ptr<OperatorState> PhysicalUnnest::GetState(ExecutionContext &context,
250:                                                    const vector<unique_ptr<Expression>> &select_list) {
251: 	return make_uniq<UnnestOperatorState>(context.client, select_list);
252: }
253: 
254: OperatorResultType PhysicalUnnest::ExecuteInternal(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
255:                                                    OperatorState &state_p,
256:                                                    const vector<unique_ptr<Expression>> &select_list,
257:                                                    bool include_input) {
258: 
259: 	auto &state = state_p.Cast<UnnestOperatorState>();
260: 
261: 	do {
262: 		// prepare the input data by executing any expressions and getting the
263: 		// UnifiedVectorFormat of each LIST vector (list_vector_data) and its child vector (list_child_data)
264: 		if (state.first_fetch) {
265: 			PrepareInput(state, input, select_list);
266: 		}
267: 
268: 		// finished with all rows of this input chunk, reset
269: 		if (state.current_row >= input.size()) {
270: 			state.Reset();
271: 			return OperatorResultType::NEED_MORE_INPUT;
272: 		}
273: 
274: 		// each UNNEST in the select_list contains a list (or NULL) for this row, find longest list
275: 		// because this length determines how many times we need to repeat for the current row
276: 		if (state.longest_list_length == DConstants::INVALID_INDEX) {
277: 			state.SetLongestListLength();
278: 		}
279: 		D_ASSERT(state.longest_list_length != DConstants::INVALID_INDEX);
280: 
281: 		// we emit chunks of either STANDARD_VECTOR_SIZE or smaller
282: 		auto this_chunk_len = MinValue<idx_t>(STANDARD_VECTOR_SIZE, state.longest_list_length - state.list_position);
283: 		chunk.SetCardinality(this_chunk_len);
284: 
285: 		// if we include other projection input columns, e.g. SELECT 1, UNNEST([1, 2]);, then
286: 		// we need to add them as a constant vector to the resulting chunk
287: 		// FIXME: emit multiple unnested rows. Currently, we never emit a chunk containing multiple unnested input rows,
288: 		//  so setting a constant vector for the value at state.current_row is fine
289: 		idx_t col_offset = 0;
290: 		if (include_input) {
291: 			for (idx_t col_idx = 0; col_idx < input.ColumnCount(); col_idx++) {
292: 				ConstantVector::Reference(chunk.data[col_idx], input.data[col_idx], state.current_row, input.size());
293: 			}
294: 			col_offset = input.ColumnCount();
295: 		}
296: 
297: 		// unnest the lists
298: 		for (idx_t col_idx = 0; col_idx < state.list_data.ColumnCount(); col_idx++) {
299: 
300: 			auto &result_vector = chunk.data[col_idx + col_offset];
301: 
302: 			if (state.list_data.data[col_idx].GetType() == LogicalType::SQLNULL) {
303: 				// UNNEST(NULL)
304: 				chunk.SetCardinality(0);
305: 				break;
306: 
307: 			} else {
308: 
309: 				auto &vector_data = state.list_vector_data[col_idx];
310: 				auto current_idx = vector_data.sel->get_index(state.current_row);
311: 
312: 				if (!vector_data.validity.RowIsValid(current_idx)) {
313: 					UnnestNull(0, this_chunk_len, result_vector);
314: 
315: 				} else {
316: 
317: 					auto list_data = UnifiedVectorFormat::GetData<list_entry_t>(vector_data);
318: 					auto list_entry = list_data[current_idx];
319: 
320: 					idx_t list_count = 0;
321: 					if (state.list_position < list_entry.length) {
322: 						// there are still list_count elements to unnest
323: 						list_count = MinValue<idx_t>(this_chunk_len, list_entry.length - state.list_position);
324: 
325: 						auto &list_vector = state.list_data.data[col_idx];
326: 						auto &child_vector = ListVector::GetEntry(list_vector);
327: 						auto list_size = ListVector::GetListSize(list_vector);
328: 						auto &child_vector_data = state.list_child_data[col_idx];
329: 
330: 						auto base_offset = list_entry.offset + state.list_position;
331: 						UnnestVector(child_vector_data, child_vector, list_size, base_offset, base_offset + list_count,
332: 						             result_vector);
333: 					}
334: 
335: 					// fill the rest with NULLs
336: 					if (list_count != this_chunk_len) {
337: 						UnnestNull(list_count, this_chunk_len, result_vector);
338: 					}
339: 				}
340: 			}
341: 		}
342: 
343: 		chunk.Verify();
344: 
345: 		state.list_position += this_chunk_len;
346: 		if (state.list_position == state.longest_list_length) {
347: 			state.current_row++;
348: 			state.longest_list_length = DConstants::INVALID_INDEX;
349: 			state.list_position = 0;
350: 		}
351: 
352: 		// we only emit one unnested row (that contains data) at a time
353: 	} while (chunk.size() == 0);
354: 	return OperatorResultType::HAVE_MORE_OUTPUT;
355: }
356: 
357: OperatorResultType PhysicalUnnest::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
358:                                            GlobalOperatorState &, OperatorState &state) const {
359: 	return ExecuteInternal(context, input, chunk, state, select_list);
360: }
361: 
362: } // namespace duckdb
[end of src/execution/operator/projection/physical_unnest.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: