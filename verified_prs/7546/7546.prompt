You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
MODE() WITHIN GROUP Incorrect Output
### What happens?

MODE() WITHIN GROUP when provided with an ORDER BY value DESC doesn't produce the correct output.

### To Reproduce

```
SELECT MODE() WITHIN GROUP (ORDER BY order_occurrences DESC) FROM (
VALUES
	(500, 1),
	(1000, 2),
	(800, 3),
	(1000, 4),
	(500, 5),
	(550, 6),
	(400, 7),
	(200, 8),
	(10, 9)
)items_per_order(order_occurrences, item_count);
```
Should output 1000, but gives 500. 

Tested in the DuckDB shell with v.0.7.2-dev2931 as well as within a Workspace running 0.7.0. 

### OS:

Web

### DuckDB Version:

DuckDB Web Shell Database: v0.7.2-dev2931

### DuckDB Client:

Web

### Full Name:

Dwayne McMurchy

### Affiliation:

None

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
11:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
12:   </a>
13:   <a href="https://discord.gg/tcvwpjfnZx">
14:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
15:   </a>
16:   <a href="https://github.com/duckdb/duckdb/releases/">
17:     <img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release">
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of src/core_functions/aggregate/holistic/mode.cpp]
1: // MODE( <expr1> )
2: // Returns the most frequent value for the values within expr1.
3: // NULL values are ignored. If all the values are NULL, or there are 0 rows, then the function returns NULL.
4: 
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/common/operator/comparison_operators.hpp"
8: #include "duckdb/core_functions/aggregate/holistic_functions.hpp"
9: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
10: #include "duckdb/common/unordered_map.hpp"
11: 
12: #include <functional>
13: 
14: namespace std {
15: 
16: template <>
17: struct hash<duckdb::interval_t> {
18: 	inline size_t operator()(const duckdb::interval_t &val) const {
19: 		return hash<int32_t> {}(val.days) ^ hash<int32_t> {}(val.months) ^ hash<int64_t> {}(val.micros);
20: 	}
21: };
22: 
23: template <>
24: struct hash<duckdb::hugeint_t> {
25: 	inline size_t operator()(const duckdb::hugeint_t &val) const {
26: 		return hash<int64_t> {}(val.upper) ^ hash<int64_t> {}(val.lower);
27: 	}
28: };
29: 
30: } // namespace std
31: 
32: namespace duckdb {
33: 
34: using FrameBounds = std::pair<idx_t, idx_t>;
35: 
36: template <class KEY_TYPE>
37: struct ModeState {
38: 	using Counts = unordered_map<KEY_TYPE, size_t>;
39: 
40: 	Counts *frequency_map;
41: 	KEY_TYPE *mode;
42: 	size_t nonzero;
43: 	bool valid;
44: 	size_t count;
45: 
46: 	void Initialize() {
47: 		frequency_map = nullptr;
48: 		mode = nullptr;
49: 		nonzero = 0;
50: 		valid = false;
51: 		count = 0;
52: 	}
53: 
54: 	void Destroy() {
55: 		if (frequency_map) {
56: 			delete frequency_map;
57: 		}
58: 		if (mode) {
59: 			delete mode;
60: 		}
61: 	}
62: 
63: 	void Reset() {
64: 		Counts empty;
65: 		frequency_map->swap(empty);
66: 		nonzero = 0;
67: 		count = 0;
68: 		valid = false;
69: 	}
70: 
71: 	void ModeAdd(const KEY_TYPE &key) {
72: 		auto new_count = ((*frequency_map)[key] += 1);
73: 		if (new_count == 1) {
74: 			++nonzero;
75: 		}
76: 		if (new_count > count) {
77: 			valid = true;
78: 			count = new_count;
79: 			if (mode) {
80: 				*mode = key;
81: 			} else {
82: 				mode = new KEY_TYPE(key);
83: 			}
84: 		}
85: 	}
86: 
87: 	void ModeRm(const KEY_TYPE &key) {
88: 		auto i = frequency_map->find(key);
89: 		auto old_count = i->second;
90: 		nonzero -= int(old_count == 1);
91: 
92: 		i->second -= 1;
93: 		if (count == old_count && key == *mode) {
94: 			valid = false;
95: 		}
96: 	}
97: 
98: 	typename Counts::const_iterator Scan() const {
99: 		//! Initialize control variables to first variable of the frequency map
100: 		auto highest_frequency = frequency_map->begin();
101: 		for (auto i = highest_frequency; i != frequency_map->end(); ++i) {
102: 			// Tie break with the lowest
103: 			if (i->second > highest_frequency->second ||
104: 			    (i->second == highest_frequency->second && i->first < highest_frequency->first)) {
105: 				highest_frequency = i;
106: 			}
107: 		}
108: 		return highest_frequency;
109: 	}
110: };
111: 
112: struct ModeIncluded {
113: 	inline explicit ModeIncluded(const ValidityMask &fmask_p, const ValidityMask &dmask_p, idx_t bias_p)
114: 	    : fmask(fmask_p), dmask(dmask_p), bias(bias_p) {
115: 	}
116: 
117: 	inline bool operator()(const idx_t &idx) const {
118: 		return fmask.RowIsValid(idx) && dmask.RowIsValid(idx - bias);
119: 	}
120: 	const ValidityMask &fmask;
121: 	const ValidityMask &dmask;
122: 	const idx_t bias;
123: };
124: 
125: struct ModeAssignmentStandard {
126: 	template <class INPUT_TYPE, class RESULT_TYPE>
127: 	static RESULT_TYPE Assign(Vector &result, INPUT_TYPE input) {
128: 		return RESULT_TYPE(input);
129: 	}
130: };
131: 
132: struct ModeAssignmentString {
133: 	template <class INPUT_TYPE, class RESULT_TYPE>
134: 	static RESULT_TYPE Assign(Vector &result, INPUT_TYPE input) {
135: 		return StringVector::AddString(result, input);
136: 	}
137: };
138: 
139: template <typename KEY_TYPE, typename ASSIGN_OP>
140: struct ModeFunction {
141: 	template <class STATE>
142: 	static void Initialize(STATE *state) {
143: 		state->Initialize();
144: 	}
145: 
146: 	template <class INPUT_TYPE, class STATE, class OP>
147: 	static void Operation(STATE *state, AggregateInputData &, INPUT_TYPE *input, ValidityMask &mask, idx_t idx) {
148: 		if (!state->frequency_map) {
149: 			state->frequency_map = new unordered_map<KEY_TYPE, size_t>();
150: 		}
151: 		auto key = KEY_TYPE(input[idx]);
152: 		(*state->frequency_map)[key]++;
153: 	}
154: 
155: 	template <class STATE, class OP>
156: 	static void Combine(const STATE &source, STATE *target, AggregateInputData &) {
157: 		if (!source.frequency_map) {
158: 			return;
159: 		}
160: 		if (!target->frequency_map) {
161: 			// Copy - don't destroy! Otherwise windowing will break.
162: 			target->frequency_map = new unordered_map<KEY_TYPE, size_t>(*source.frequency_map);
163: 			return;
164: 		}
165: 		for (auto &val : *source.frequency_map) {
166: 			(*target->frequency_map)[val.first] += val.second;
167: 		}
168: 	}
169: 
170: 	template <class INPUT_TYPE, class STATE>
171: 	static void Finalize(Vector &result, AggregateInputData &, STATE *state, INPUT_TYPE *target, ValidityMask &mask,
172: 	                     idx_t idx) {
173: 		if (!state->frequency_map) {
174: 			mask.SetInvalid(idx);
175: 			return;
176: 		}
177: 		auto highest_frequency = state->Scan();
178: 		if (highest_frequency != state->frequency_map->end()) {
179: 			target[idx] = ASSIGN_OP::template Assign<INPUT_TYPE, INPUT_TYPE>(result, highest_frequency->first);
180: 		} else {
181: 			mask.SetInvalid(idx);
182: 		}
183: 	}
184: 	template <class INPUT_TYPE, class STATE, class OP>
185: 	static void ConstantOperation(STATE *state, AggregateInputData &, INPUT_TYPE *input, ValidityMask &mask,
186: 	                              idx_t count) {
187: 		if (!state->frequency_map) {
188: 			state->frequency_map = new unordered_map<KEY_TYPE, size_t>();
189: 		}
190: 		auto key = KEY_TYPE(input[0]);
191: 		(*state->frequency_map)[key] += count;
192: 	}
193: 
194: 	template <class STATE, class INPUT_TYPE, class RESULT_TYPE>
195: 	static void Window(const INPUT_TYPE *data, const ValidityMask &fmask, const ValidityMask &dmask,
196: 	                   AggregateInputData &, STATE *state, const FrameBounds &frame, const FrameBounds &prev,
197: 	                   Vector &result, idx_t rid, idx_t bias) {
198: 		auto rdata = FlatVector::GetData<RESULT_TYPE>(result);
199: 		auto &rmask = FlatVector::Validity(result);
200: 
201: 		ModeIncluded included(fmask, dmask, bias);
202: 
203: 		if (!state->frequency_map) {
204: 			state->frequency_map = new unordered_map<KEY_TYPE, size_t>();
205: 		}
206: 		const double tau = .25;
207: 		if (state->nonzero <= tau * state->frequency_map->size()) {
208: 			state->Reset();
209: 			// for f ∈ F do
210: 			for (auto f = frame.first; f < frame.second; ++f) {
211: 				if (included(f)) {
212: 					state->ModeAdd(KEY_TYPE(data[f]));
213: 				}
214: 			}
215: 		} else {
216: 			// for f ∈ P \ F do
217: 			for (auto p = prev.first; p < frame.first; ++p) {
218: 				if (included(p)) {
219: 					state->ModeRm(KEY_TYPE(data[p]));
220: 				}
221: 			}
222: 			for (auto p = frame.second; p < prev.second; ++p) {
223: 				if (included(p)) {
224: 					state->ModeRm(KEY_TYPE(data[p]));
225: 				}
226: 			}
227: 
228: 			// for f ∈ F \ P do
229: 			for (auto f = frame.first; f < prev.first; ++f) {
230: 				if (included(f)) {
231: 					state->ModeAdd(KEY_TYPE(data[f]));
232: 				}
233: 			}
234: 			for (auto f = prev.second; f < frame.second; ++f) {
235: 				if (included(f)) {
236: 					state->ModeAdd(KEY_TYPE(data[f]));
237: 				}
238: 			}
239: 		}
240: 
241: 		if (!state->valid) {
242: 			// Rescan
243: 			auto highest_frequency = state->Scan();
244: 			if (highest_frequency != state->frequency_map->end()) {
245: 				*(state->mode) = highest_frequency->first;
246: 				state->count = highest_frequency->second;
247: 				state->valid = (state->count > 0);
248: 			}
249: 		}
250: 
251: 		if (state->valid) {
252: 			rdata[rid] = ASSIGN_OP::template Assign<INPUT_TYPE, RESULT_TYPE>(result, *state->mode);
253: 		} else {
254: 			rmask.Set(rid, false);
255: 		}
256: 	}
257: 
258: 	static bool IgnoreNull() {
259: 		return true;
260: 	}
261: 
262: 	template <class STATE>
263: 	static void Destroy(AggregateInputData &aggr_input_data, STATE *state) {
264: 		state->Destroy();
265: 	}
266: };
267: 
268: template <typename INPUT_TYPE, typename KEY_TYPE, typename ASSIGN_OP = ModeAssignmentStandard>
269: AggregateFunction GetTypedModeFunction(const LogicalType &type) {
270: 	using STATE = ModeState<KEY_TYPE>;
271: 	using OP = ModeFunction<KEY_TYPE, ASSIGN_OP>;
272: 	auto func = AggregateFunction::UnaryAggregateDestructor<STATE, INPUT_TYPE, INPUT_TYPE, OP>(type, type);
273: 	func.window = AggregateFunction::UnaryWindow<STATE, INPUT_TYPE, INPUT_TYPE, OP>;
274: 	return func;
275: }
276: 
277: AggregateFunction GetModeAggregate(const LogicalType &type) {
278: 	switch (type.InternalType()) {
279: 	case PhysicalType::INT8:
280: 		return GetTypedModeFunction<int8_t, int8_t>(type);
281: 	case PhysicalType::UINT8:
282: 		return GetTypedModeFunction<uint8_t, uint8_t>(type);
283: 	case PhysicalType::INT16:
284: 		return GetTypedModeFunction<int16_t, int16_t>(type);
285: 	case PhysicalType::UINT16:
286: 		return GetTypedModeFunction<uint16_t, uint16_t>(type);
287: 	case PhysicalType::INT32:
288: 		return GetTypedModeFunction<int32_t, int32_t>(type);
289: 	case PhysicalType::UINT32:
290: 		return GetTypedModeFunction<uint32_t, uint32_t>(type);
291: 	case PhysicalType::INT64:
292: 		return GetTypedModeFunction<int64_t, int64_t>(type);
293: 	case PhysicalType::UINT64:
294: 		return GetTypedModeFunction<uint64_t, uint64_t>(type);
295: 	case PhysicalType::INT128:
296: 		return GetTypedModeFunction<hugeint_t, hugeint_t>(type);
297: 
298: 	case PhysicalType::FLOAT:
299: 		return GetTypedModeFunction<float, float>(type);
300: 	case PhysicalType::DOUBLE:
301: 		return GetTypedModeFunction<double, double>(type);
302: 
303: 	case PhysicalType::INTERVAL:
304: 		return GetTypedModeFunction<interval_t, interval_t>(type);
305: 
306: 	case PhysicalType::VARCHAR:
307: 		return GetTypedModeFunction<string_t, string, ModeAssignmentString>(type);
308: 
309: 	default:
310: 		throw NotImplementedException("Unimplemented mode aggregate");
311: 	}
312: }
313: 
314: unique_ptr<FunctionData> BindModeDecimal(ClientContext &context, AggregateFunction &function,
315:                                          vector<unique_ptr<Expression>> &arguments) {
316: 	function = GetModeAggregate(arguments[0]->return_type);
317: 	function.name = "mode";
318: 	return nullptr;
319: }
320: 
321: AggregateFunctionSet ModeFun::GetFunctions() {
322: 	const vector<LogicalType> TEMPORAL = {LogicalType::DATE,         LogicalType::TIMESTAMP, LogicalType::TIME,
323: 	                                      LogicalType::TIMESTAMP_TZ, LogicalType::TIME_TZ,   LogicalType::INTERVAL};
324: 
325: 	AggregateFunctionSet mode;
326: 	mode.AddFunction(AggregateFunction({LogicalTypeId::DECIMAL}, LogicalTypeId::DECIMAL, nullptr, nullptr, nullptr,
327: 	                                   nullptr, nullptr, nullptr, BindModeDecimal));
328: 
329: 	for (const auto &type : LogicalType::Numeric()) {
330: 		if (type.id() != LogicalTypeId::DECIMAL) {
331: 			mode.AddFunction(GetModeAggregate(type));
332: 		}
333: 	}
334: 
335: 	for (const auto &type : TEMPORAL) {
336: 		mode.AddFunction(GetModeAggregate(type));
337: 	}
338: 
339: 	mode.AddFunction(GetModeAggregate(LogicalType::VARCHAR));
340: 	return mode;
341: }
342: } // namespace duckdb
[end of src/core_functions/aggregate/holistic/mode.cpp]
[start of src/planner/binder/expression/bind_aggregate_expression.cpp]
1: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
2: #include "duckdb/common/pair.hpp"
3: #include "duckdb/common/operator/cast_operators.hpp"
4: #include "duckdb/parser/expression/function_expression.hpp"
5: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
6: #include "duckdb/planner/expression/bound_cast_expression.hpp"
7: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
8: #include "duckdb/planner/expression/bound_constant_expression.hpp"
9: #include "duckdb/planner/expression_binder/aggregate_binder.hpp"
10: #include "duckdb/planner/expression_binder/base_select_binder.hpp"
11: #include "duckdb/planner/query_node/bound_select_node.hpp"
12: #include "duckdb/execution/expression_executor.hpp"
13: #include "duckdb/function/scalar/generic_functions.hpp"
14: #include "duckdb/main/config.hpp"
15: #include "duckdb/function/function_binder.hpp"
16: #include "duckdb/planner/binder.hpp"
17: 
18: namespace duckdb {
19: 
20: static Value NegatePercentileValue(const Value &v, const bool desc) {
21: 	if (v.IsNull()) {
22: 		return v;
23: 	}
24: 
25: 	const auto frac = v.GetValue<double>();
26: 	if (frac < 0 || frac > 1) {
27: 		throw BinderException("PERCENTILEs can only take parameters in the range [0, 1]");
28: 	}
29: 
30: 	if (!desc) {
31: 		return v;
32: 	}
33: 
34: 	const auto &type = v.type();
35: 	switch (type.id()) {
36: 	case LogicalTypeId::DECIMAL: {
37: 		// Negate DECIMALs as DECIMAL.
38: 		const auto integral = IntegralValue::Get(v);
39: 		const auto width = DecimalType::GetWidth(type);
40: 		const auto scale = DecimalType::GetScale(type);
41: 		switch (type.InternalType()) {
42: 		case PhysicalType::INT16:
43: 			return Value::DECIMAL(Cast::Operation<hugeint_t, int16_t>(-integral), width, scale);
44: 		case PhysicalType::INT32:
45: 			return Value::DECIMAL(Cast::Operation<hugeint_t, int32_t>(-integral), width, scale);
46: 		case PhysicalType::INT64:
47: 			return Value::DECIMAL(Cast::Operation<hugeint_t, int64_t>(-integral), width, scale);
48: 		case PhysicalType::INT128:
49: 			return Value::DECIMAL(-integral, width, scale);
50: 		default:
51: 			throw InternalException("Unknown DECIMAL type");
52: 		}
53: 	}
54: 	default:
55: 		// Everything else can just be a DOUBLE
56: 		return Value::DOUBLE(-v.GetValue<double>());
57: 	}
58: }
59: 
60: static void NegatePercentileFractions(ClientContext &context, unique_ptr<ParsedExpression> &fractions, bool desc) {
61: 	D_ASSERT(fractions.get());
62: 	D_ASSERT(fractions->expression_class == ExpressionClass::BOUND_EXPRESSION);
63: 	auto &bound = BoundExpression::GetExpression(*fractions);
64: 
65: 	if (!bound->IsFoldable()) {
66: 		return;
67: 	}
68: 
69: 	Value value = ExpressionExecutor::EvaluateScalar(context, *bound);
70: 	if (value.type().id() == LogicalTypeId::LIST) {
71: 		vector<Value> values;
72: 		for (const auto &element_val : ListValue::GetChildren(value)) {
73: 			values.push_back(NegatePercentileValue(element_val, desc));
74: 		}
75: 		if (values.empty()) {
76: 			throw BinderException("Empty list in percentile not allowed");
77: 		}
78: 		bound = make_uniq<BoundConstantExpression>(Value::LIST(values));
79: 	} else {
80: 		bound = make_uniq<BoundConstantExpression>(NegatePercentileValue(value, desc));
81: 	}
82: }
83: 
84: BindResult BaseSelectBinder::BindAggregate(FunctionExpression &aggr, AggregateFunctionCatalogEntry &func, idx_t depth) {
85: 	// first bind the child of the aggregate expression (if any)
86: 	this->bound_aggregate = true;
87: 	unique_ptr<Expression> bound_filter;
88: 	AggregateBinder aggregate_binder(binder, context);
89: 	string error, filter_error;
90: 
91: 	// Now we bind the filter (if any)
92: 	if (aggr.filter) {
93: 		aggregate_binder.BindChild(aggr.filter, 0, error);
94: 	}
95: 
96: 	// Handle ordered-set aggregates by moving the single ORDER BY expression to the front of the children.
97: 	//	https://www.postgresql.org/docs/current/functions-aggregate.html#FUNCTIONS-ORDEREDSET-TABLE
98: 	bool ordered_set_agg = false;
99: 	bool negate_fractions = false;
100: 	if (aggr.order_bys && aggr.order_bys->orders.size() == 1) {
101: 		const auto &func_name = aggr.function_name;
102: 		ordered_set_agg = (func_name == "quantile_cont" || func_name == "quantile_disc" || func_name == "mode");
103: 
104: 		if (ordered_set_agg) {
105: 			auto &config = DBConfig::GetConfig(context);
106: 			const auto &order = aggr.order_bys->orders[0];
107: 			const auto sense =
108: 			    (order.type == OrderType::ORDER_DEFAULT) ? config.options.default_order_type : order.type;
109: 			negate_fractions = (sense == OrderType::DESCENDING);
110: 		}
111: 	}
112: 
113: 	for (auto &child : aggr.children) {
114: 		aggregate_binder.BindChild(child, 0, error);
115: 		// We have to negate the fractions for PERCENTILE_XXXX DESC
116: 		if (error.empty() && ordered_set_agg) {
117: 			NegatePercentileFractions(context, child, negate_fractions);
118: 		}
119: 	}
120: 
121: 	// Bind the ORDER BYs, if any
122: 	if (aggr.order_bys && !aggr.order_bys->orders.empty()) {
123: 		for (auto &order : aggr.order_bys->orders) {
124: 			aggregate_binder.BindChild(order.expression, 0, error);
125: 		}
126: 	}
127: 
128: 	if (!error.empty()) {
129: 		// failed to bind child
130: 		if (aggregate_binder.HasBoundColumns()) {
131: 			for (idx_t i = 0; i < aggr.children.size(); i++) {
132: 				// however, we bound columns!
133: 				// that means this aggregation belongs to this node
134: 				// check if we have to resolve any errors by binding with parent binders
135: 				bool success = aggregate_binder.BindCorrelatedColumns(aggr.children[i]);
136: 				// if there is still an error after this, we could not successfully bind the aggregate
137: 				if (!success) {
138: 					throw BinderException(error);
139: 				}
140: 				auto &bound_expr = BoundExpression::GetExpression(*aggr.children[i]);
141: 				ExtractCorrelatedExpressions(binder, *bound_expr);
142: 			}
143: 			if (aggr.filter) {
144: 				bool success = aggregate_binder.BindCorrelatedColumns(aggr.filter);
145: 				// if there is still an error after this, we could not successfully bind the aggregate
146: 				if (!success) {
147: 					throw BinderException(error);
148: 				}
149: 				auto &bound_expr = BoundExpression::GetExpression(*aggr.filter);
150: 				ExtractCorrelatedExpressions(binder, *bound_expr);
151: 			}
152: 			if (aggr.order_bys && !aggr.order_bys->orders.empty()) {
153: 				for (auto &order : aggr.order_bys->orders) {
154: 					bool success = aggregate_binder.BindCorrelatedColumns(order.expression);
155: 					if (!success) {
156: 						throw BinderException(error);
157: 					}
158: 					auto &bound_expr = BoundExpression::GetExpression(*order.expression);
159: 					ExtractCorrelatedExpressions(binder, *bound_expr);
160: 				}
161: 			}
162: 		} else {
163: 			// we didn't bind columns, try again in children
164: 			return BindResult(error);
165: 		}
166: 	} else if (depth > 0 && !aggregate_binder.HasBoundColumns()) {
167: 		return BindResult("Aggregate with only constant parameters has to be bound in the root subquery");
168: 	}
169: 	if (!filter_error.empty()) {
170: 		return BindResult(filter_error);
171: 	}
172: 
173: 	if (aggr.filter) {
174: 		auto &child = BoundExpression::GetExpression(*aggr.filter);
175: 		bound_filter = BoundCastExpression::AddCastToType(context, std::move(child), LogicalType::BOOLEAN);
176: 	}
177: 
178: 	// all children bound successfully
179: 	// extract the children and types
180: 	vector<LogicalType> types;
181: 	vector<LogicalType> arguments;
182: 	vector<unique_ptr<Expression>> children;
183: 
184: 	if (ordered_set_agg) {
185: 		for (auto &order : aggr.order_bys->orders) {
186: 			auto &child = BoundExpression::GetExpression(*order.expression);
187: 			types.push_back(child->return_type);
188: 			arguments.push_back(child->return_type);
189: 			children.push_back(std::move(child));
190: 		}
191: 		aggr.order_bys->orders.clear();
192: 	}
193: 
194: 	for (idx_t i = 0; i < aggr.children.size(); i++) {
195: 		auto &child = BoundExpression::GetExpression(*aggr.children[i]);
196: 		types.push_back(child->return_type);
197: 		arguments.push_back(child->return_type);
198: 		children.push_back(std::move(child));
199: 	}
200: 
201: 	// bind the aggregate
202: 	FunctionBinder function_binder(context);
203: 	idx_t best_function = function_binder.BindFunction(func.name, func.functions, types, error);
204: 	if (best_function == DConstants::INVALID_INDEX) {
205: 		throw BinderException(binder.FormatError(aggr, error));
206: 	}
207: 	// found a matching function!
208: 	auto bound_function = func.functions.GetFunctionByOffset(best_function);
209: 
210: 	// Bind any sort columns, unless the aggregate is order-insensitive
211: 	unique_ptr<BoundOrderModifier> order_bys;
212: 	if (!aggr.order_bys->orders.empty()) {
213: 		order_bys = make_uniq<BoundOrderModifier>();
214: 		auto &config = DBConfig::GetConfig(context);
215: 		for (auto &order : aggr.order_bys->orders) {
216: 			auto &order_expr = BoundExpression::GetExpression(*order.expression);
217: 			const auto sense = config.ResolveOrder(order.type);
218: 			const auto null_order = config.ResolveNullOrder(sense, order.null_order);
219: 			order_bys->orders.emplace_back(sense, null_order, std::move(order_expr));
220: 		}
221: 	}
222: 
223: 	auto aggregate =
224: 	    function_binder.BindAggregateFunction(bound_function, std::move(children), std::move(bound_filter),
225: 	                                          aggr.distinct ? AggregateType::DISTINCT : AggregateType::NON_DISTINCT);
226: 	if (aggr.export_state) {
227: 		aggregate = ExportAggregateFunction::Bind(std::move(aggregate));
228: 	}
229: 	aggregate->order_bys = std::move(order_bys);
230: 
231: 	// check for all the aggregates if this aggregate already exists
232: 	idx_t aggr_index;
233: 	auto entry = node.aggregate_map.find(*aggregate);
234: 	if (entry == node.aggregate_map.end()) {
235: 		// new aggregate: insert into aggregate list
236: 		aggr_index = node.aggregates.size();
237: 		node.aggregate_map[*aggregate] = aggr_index;
238: 		node.aggregates.push_back(std::move(aggregate));
239: 	} else {
240: 		// duplicate aggregate: simplify refer to this aggregate
241: 		aggr_index = entry->second;
242: 	}
243: 
244: 	// now create a column reference referring to the aggregate
245: 	auto colref = make_uniq<BoundColumnRefExpression>(
246: 	    aggr.alias.empty() ? node.aggregates[aggr_index]->ToString() : aggr.alias,
247: 	    node.aggregates[aggr_index]->return_type, ColumnBinding(node.aggregate_index, aggr_index), depth);
248: 	// move the aggregate expression into the set of bound aggregates
249: 	return BindResult(std::move(colref));
250: }
251: } // namespace duckdb
[end of src/planner/binder/expression/bind_aggregate_expression.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: