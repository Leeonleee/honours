You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Export/Import issue regarding parquet and ROW_GROUP_SIZE
### What happens?

When try to export with the next command:

EXPORT DATABASE 'target_directory' (FORMAT PARQUET, COMPRESSION ZSTD, ROW_GROUP_SIZE 100000);
the 
IMPORT is not working.

This is a very minor issue
I only want to point out this


See reproduce section:


### To Reproduce

> EXPORT DATABASE 'target_directory' (FORMAT PARQUET, COMPRESSION ZSTD, ROW_GROUP_SIZE 100000);

The import is not working

> import database 'target_directory';
Error: Not implemented Error: Unsupported option for COPY FROM parquet: ROW_GROUP_SIZE

A workaround is to edit the 'load.sql'  command and remove the ROW_GROUP_SIZE

### OS:

Ubuntu 22.04

### DuckDB Version:

0.61

### DuckDB Client:

duckdb client

### Full Name:

Bert Tijhuis

### Affiliation:

Private interest in this project

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: 
7: 
8: 
9: <p align="center">
10:   <a href="https://github.com/duckdb/duckdb/actions">
11:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19:   <a href="https://github.com/duckdb/duckdb/releases/">
20:     <img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release">
21:   </a>
22: </p>
23: 
24: ## DuckDB
25: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
26: 
27: ## Installation
28: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
29: 
30: ## Data Import
31: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
32: 
33: ```sql
34: SELECT * FROM 'myfile.csv';
35: SELECT * FROM 'myfile.parquet';
36: ```
37: 
38: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
39: 
40: ## SQL Reference
41: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
42: 
43: ## Development
44: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
45: 
46: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of extension/parquet/parquet_extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include "parquet_extension.hpp"
4: 
5: #include "duckdb.hpp"
6: #include "parquet_metadata.hpp"
7: #include "parquet_reader.hpp"
8: #include "parquet_writer.hpp"
9: #include "zstd_file_system.hpp"
10: 
11: #include <fstream>
12: #include <iostream>
13: #include <numeric>
14: #include <string>
15: #include <vector>
16: #ifndef DUCKDB_AMALGAMATION
17: #include "duckdb/catalog/catalog.hpp"
18: #include "duckdb/catalog/catalog_entry/table_function_catalog_entry.hpp"
19: #include "duckdb/common/constants.hpp"
20: #include "duckdb/common/enums/file_compression_type.hpp"
21: #include "duckdb/common/field_writer.hpp"
22: #include "duckdb/common/file_system.hpp"
23: #include "duckdb/common/multi_file_reader.hpp"
24: #include "duckdb/common/types/chunk_collection.hpp"
25: #include "duckdb/function/copy_function.hpp"
26: #include "duckdb/function/table_function.hpp"
27: #include "duckdb/main/client_context.hpp"
28: #include "duckdb/main/config.hpp"
29: #include "duckdb/main/extension_util.hpp"
30: #include "duckdb/parser/expression/constant_expression.hpp"
31: #include "duckdb/parser/expression/function_expression.hpp"
32: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
33: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
34: #include "duckdb/parser/tableref/table_function_ref.hpp"
35: #include "duckdb/planner/operator/logical_get.hpp"
36: #include "duckdb/storage/statistics/base_statistics.hpp"
37: #include "duckdb/storage/table/row_group.hpp"
38: #include "duckdb/common/serializer/format_serializer.hpp"
39: #include "duckdb/common/serializer/format_deserializer.hpp"
40: #endif
41: 
42: namespace duckdb {
43: 
44: struct ParquetReadBindData : public TableFunctionData {
45: 	shared_ptr<ParquetReader> initial_reader;
46: 	vector<string> files;
47: 	atomic<idx_t> chunk_count;
48: 	atomic<idx_t> cur_file;
49: 	vector<string> names;
50: 	vector<LogicalType> types;
51: 
52: 	// The union readers are created (when parquet union_by_name option is on) during binding
53: 	// Those readers can be re-used during ParquetParallelStateNext
54: 	vector<shared_ptr<ParquetReader>> union_readers;
55: 
56: 	// These come from the initial_reader, but need to be stored in case the initial_reader is removed by a filter
57: 	idx_t initial_file_cardinality;
58: 	idx_t initial_file_row_groups;
59: 	ParquetOptions parquet_options;
60: 	MultiFileReaderBindData reader_bind;
61: 
62: 	void Initialize(shared_ptr<ParquetReader> reader) {
63: 		initial_reader = std::move(reader);
64: 		initial_file_cardinality = initial_reader->NumRows();
65: 		initial_file_row_groups = initial_reader->NumRowGroups();
66: 		parquet_options = initial_reader->parquet_options;
67: 	}
68: };
69: 
70: struct ParquetReadLocalState : public LocalTableFunctionState {
71: 	shared_ptr<ParquetReader> reader;
72: 	ParquetReaderScanState scan_state;
73: 	bool is_parallel;
74: 	idx_t batch_index;
75: 	idx_t file_index;
76: 	//! The DataChunk containing all read columns (even filter columns that are immediately removed)
77: 	DataChunk all_columns;
78: };
79: 
80: struct ParquetReadGlobalState : public GlobalTableFunctionState {
81: 	mutex lock;
82: 
83: 	//! The initial reader from the bind phase
84: 	shared_ptr<ParquetReader> initial_reader;
85: 	//! Currently opened readers
86: 	vector<shared_ptr<ParquetReader>> readers;
87: 	//! Flag to indicate a file is being opened
88: 	vector<bool> file_opening;
89: 	//! Mutexes to wait for a file that is currently being opened
90: 	unique_ptr<mutex[]> file_mutexes;
91: 	//! Signal to other threads that a file failed to open, letting every thread abort.
92: 	bool error_opening_file = false;
93: 
94: 	//! Index of file currently up for scanning
95: 	idx_t file_index;
96: 	//! Index of row group within file currently up for scanning
97: 	idx_t row_group_index;
98: 	//! Batch index of the next row group to be scanned
99: 	idx_t batch_index;
100: 
101: 	idx_t max_threads;
102: 	vector<idx_t> projection_ids;
103: 	vector<LogicalType> scanned_types;
104: 	vector<column_t> column_ids;
105: 	TableFilterSet *filters;
106: 
107: 	idx_t MaxThreads() const override {
108: 		return max_threads;
109: 	}
110: 
111: 	bool CanRemoveFilterColumns() const {
112: 		return !projection_ids.empty();
113: 	}
114: };
115: 
116: struct ParquetWriteBindData : public TableFunctionData {
117: 	vector<LogicalType> sql_types;
118: 	vector<string> column_names;
119: 	duckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
120: 	idx_t row_group_size = RowGroup::ROW_GROUP_SIZE;
121: 
122: 	//! If row_group_size_bytes is not set, we default to row_group_size * BYTES_PER_ROW
123: 	static constexpr const idx_t BYTES_PER_ROW = 1024;
124: 	idx_t row_group_size_bytes;
125: 
126: 	ChildFieldIDs field_ids;
127: };
128: 
129: struct ParquetWriteGlobalState : public GlobalFunctionData {
130: 	unique_ptr<ParquetWriter> writer;
131: };
132: 
133: struct ParquetWriteLocalState : public LocalFunctionData {
134: 	explicit ParquetWriteLocalState(ClientContext &context, const vector<LogicalType> &types)
135: 	    : buffer(context, types, ColumnDataAllocatorType::HYBRID) {
136: 		buffer.InitializeAppend(append_state);
137: 	}
138: 
139: 	ColumnDataCollection buffer;
140: 	ColumnDataAppendState append_state;
141: };
142: 
143: void ParquetOptions::Serialize(FieldWriter &writer) const {
144: 	writer.WriteField<bool>(binary_as_string);
145: 	writer.WriteField<bool>(file_row_number);
146: 	writer.WriteSerializable(file_options);
147: }
148: 
149: void ParquetOptions::Deserialize(FieldReader &reader) {
150: 	binary_as_string = reader.ReadRequired<bool>();
151: 	file_row_number = reader.ReadRequired<bool>();
152: 	file_options = reader.ReadRequiredSerializable<MultiFileReaderOptions, MultiFileReaderOptions>();
153: }
154: 
155: BindInfo ParquetGetBatchInfo(const FunctionData *bind_data) {
156: 	auto bind_info = BindInfo(ScanType::PARQUET);
157: 	auto &parquet_bind = bind_data->Cast<ParquetReadBindData>();
158: 	vector<Value> file_path;
159: 	for (auto &path : parquet_bind.files) {
160: 		file_path.emplace_back(path);
161: 	}
162: 	// LCOV_EXCL_START
163: 	bind_info.InsertOption("file_path", Value::LIST(LogicalType::VARCHAR, file_path));
164: 	bind_info.InsertOption("binary_as_string", Value::BOOLEAN(parquet_bind.parquet_options.binary_as_string));
165: 	bind_info.InsertOption("file_row_number", Value::BOOLEAN(parquet_bind.parquet_options.file_row_number));
166: 	parquet_bind.parquet_options.file_options.AddBatchInfo(bind_info);
167: 	// LCOV_EXCL_STOP
168: 	return bind_info;
169: }
170: 
171: class ParquetScanFunction {
172: public:
173: 	static TableFunctionSet GetFunctionSet() {
174: 		TableFunction table_function("parquet_scan", {LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind,
175: 		                             ParquetScanInitGlobal, ParquetScanInitLocal);
176: 		table_function.statistics = ParquetScanStats;
177: 		table_function.cardinality = ParquetCardinality;
178: 		table_function.table_scan_progress = ParquetProgress;
179: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
180: 		table_function.named_parameters["file_row_number"] = LogicalType::BOOLEAN;
181: 		table_function.named_parameters["compression"] = LogicalType::VARCHAR;
182: 		MultiFileReader::AddParameters(table_function);
183: 		table_function.get_batch_index = ParquetScanGetBatchIndex;
184: 		table_function.serialize = ParquetScanSerialize;
185: 		table_function.deserialize = ParquetScanDeserialize;
186: 		table_function.format_serialize = ParquetScanFormatSerialize;
187: 		table_function.format_deserialize = ParquetScanFormatDeserialize;
188: 		table_function.get_batch_info = ParquetGetBatchInfo;
189: 		table_function.projection_pushdown = true;
190: 		table_function.filter_pushdown = true;
191: 		table_function.filter_prune = true;
192: 		table_function.pushdown_complex_filter = ParquetComplexFilterPushdown;
193: 		return MultiFileReader::CreateFunctionSet(table_function);
194: 	}
195: 
196: 	static unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,
197: 	                                                vector<string> &expected_names,
198: 	                                                vector<LogicalType> &expected_types) {
199: 		D_ASSERT(expected_names.size() == expected_types.size());
200: 		ParquetOptions parquet_options(context);
201: 
202: 		for (auto &option : info.options) {
203: 			auto loption = StringUtil::Lower(option.first);
204: 			if (loption == "compression" || loption == "codec") {
205: 				// CODEC option has no effect on parquet read: we determine codec from the file
206: 				continue;
207: 			} else if (loption == "binary_as_string") {
208: 				parquet_options.binary_as_string = true;
209: 			} else if (loption == "file_row_number") {
210: 				parquet_options.file_row_number = true;
211: 			} else {
212: 				throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
213: 			}
214: 		}
215: 
216: 		auto files = MultiFileReader::GetFileList(context, Value(info.file_path), "Parquet");
217: 		return ParquetScanBindInternal(context, std::move(files), expected_types, expected_names, parquet_options);
218: 	}
219: 
220: 	static unique_ptr<BaseStatistics> ParquetScanStats(ClientContext &context, const FunctionData *bind_data_p,
221: 	                                                   column_t column_index) {
222: 		auto &bind_data = bind_data_p->Cast<ParquetReadBindData>();
223: 
224: 		if (IsRowIdColumnId(column_index)) {
225: 			return nullptr;
226: 		}
227: 
228: 		// NOTE: we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
229: 
230: 		auto &config = DBConfig::GetConfig(context);
231: 		if (bind_data.files.size() < 2) {
232: 			if (bind_data.initial_reader) {
233: 				// most common path, scanning single parquet file
234: 				return bind_data.initial_reader->ReadStatistics(bind_data.names[column_index]);
235: 			} else if (!config.options.object_cache_enable) {
236: 				// our initial reader was reset
237: 				return nullptr;
238: 			}
239: 		} else if (config.options.object_cache_enable) {
240: 			// multiple files, object cache enabled: merge statistics
241: 			unique_ptr<BaseStatistics> overall_stats;
242: 
243: 			auto &cache = ObjectCache::GetObjectCache(context);
244: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
245: 			// enabled at all)
246: 			FileSystem &fs = FileSystem::GetFileSystem(context);
247: 
248: 			for (idx_t file_idx = 0; file_idx < bind_data.files.size(); file_idx++) {
249: 				auto &file_name = bind_data.files[file_idx];
250: 				auto metadata = cache.Get<ParquetFileMetadataCache>(file_name);
251: 				if (!metadata) {
252: 					// missing metadata entry in cache, no usable stats
253: 					return nullptr;
254: 				}
255: 				auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
256: 				// we need to check if the metadata cache entries are current
257: 				if (fs.GetLastModifiedTime(*handle) >= metadata->read_time) {
258: 					// missing or invalid metadata entry in cache, no usable stats overall
259: 					return nullptr;
260: 				}
261: 				ParquetReader reader(context, bind_data.parquet_options, metadata);
262: 				// get and merge stats for file
263: 				auto file_stats = reader.ReadStatistics(bind_data.names[column_index]);
264: 				if (!file_stats) {
265: 					return nullptr;
266: 				}
267: 				if (overall_stats) {
268: 					overall_stats->Merge(*file_stats);
269: 				} else {
270: 					overall_stats = std::move(file_stats);
271: 				}
272: 			}
273: 			// success!
274: 			return overall_stats;
275: 		}
276: 
277: 		// multiple files and no object cache, no luck!
278: 		return nullptr;
279: 	}
280: 
281: 	static unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context, vector<string> files,
282: 	                                                        vector<LogicalType> &return_types, vector<string> &names,
283: 	                                                        ParquetOptions parquet_options) {
284: 		auto result = make_uniq<ParquetReadBindData>();
285: 		result->files = std::move(files);
286: 		result->reader_bind =
287: 		    MultiFileReader::BindReader<ParquetReader>(context, result->types, result->names, *result, parquet_options);
288: 		if (return_types.empty()) {
289: 			// no expected types - just copy the types
290: 			return_types = result->types;
291: 			names = result->names;
292: 		} else {
293: 			if (return_types.size() != result->types.size()) {
294: 				throw std::runtime_error(StringUtil::Format(
295: 				    "Failed to read file \"%s\" - column count mismatch: expected %d columns but found %d",
296: 				    result->files[0], return_types.size(), result->types.size()));
297: 			}
298: 			// expected types - overwrite the types we want to read instead
299: 			result->types = return_types;
300: 		}
301: 		return std::move(result);
302: 	}
303: 
304: 	static unique_ptr<FunctionData> ParquetScanBind(ClientContext &context, TableFunctionBindInput &input,
305: 	                                                vector<LogicalType> &return_types, vector<string> &names) {
306: 		auto files = MultiFileReader::GetFileList(context, input.inputs[0], "Parquet");
307: 		ParquetOptions parquet_options(context);
308: 		for (auto &kv : input.named_parameters) {
309: 			auto loption = StringUtil::Lower(kv.first);
310: 			if (MultiFileReader::ParseOption(kv.first, kv.second, parquet_options.file_options, context)) {
311: 				continue;
312: 			}
313: 			if (loption == "binary_as_string") {
314: 				parquet_options.binary_as_string = BooleanValue::Get(kv.second);
315: 			} else if (loption == "file_row_number") {
316: 				parquet_options.file_row_number = BooleanValue::Get(kv.second);
317: 			}
318: 		}
319: 		parquet_options.file_options.AutoDetectHivePartitioning(files, context);
320: 		return ParquetScanBindInternal(context, std::move(files), return_types, names, parquet_options);
321: 	}
322: 
323: 	static double ParquetProgress(ClientContext &context, const FunctionData *bind_data_p,
324: 	                              const GlobalTableFunctionState *global_state) {
325: 		auto &bind_data = bind_data_p->Cast<ParquetReadBindData>();
326: 		if (bind_data.files.empty()) {
327: 			return 100.0;
328: 		}
329: 		if (bind_data.initial_file_cardinality == 0) {
330: 			return (100.0 * (bind_data.cur_file + 1)) / bind_data.files.size();
331: 		}
332: 		auto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100.0 / bind_data.initial_file_cardinality) /
333: 		                  bind_data.files.size();
334: 		percentage += 100.0 * bind_data.cur_file / bind_data.files.size();
335: 		return percentage;
336: 	}
337: 
338: 	static unique_ptr<LocalTableFunctionState>
339: 	ParquetScanInitLocal(ExecutionContext &context, TableFunctionInitInput &input, GlobalTableFunctionState *gstate_p) {
340: 		auto &bind_data = input.bind_data->Cast<ParquetReadBindData>();
341: 		auto &gstate = gstate_p->Cast<ParquetReadGlobalState>();
342: 
343: 		auto result = make_uniq<ParquetReadLocalState>();
344: 		result->is_parallel = true;
345: 		result->batch_index = 0;
346: 		if (input.CanRemoveFilterColumns()) {
347: 			result->all_columns.Initialize(context.client, gstate.scanned_types);
348: 		}
349: 		if (!ParquetParallelStateNext(context.client, bind_data, *result, gstate)) {
350: 			return nullptr;
351: 		}
352: 		return std::move(result);
353: 	}
354: 
355: 	static unique_ptr<GlobalTableFunctionState> ParquetScanInitGlobal(ClientContext &context,
356: 	                                                                  TableFunctionInitInput &input) {
357: 		auto &bind_data = input.bind_data->CastNoConst<ParquetReadBindData>();
358: 		auto result = make_uniq<ParquetReadGlobalState>();
359: 
360: 		result->file_opening = vector<bool>(bind_data.files.size(), false);
361: 		result->file_mutexes = unique_ptr<mutex[]>(new mutex[bind_data.files.size()]);
362: 		if (bind_data.files.empty()) {
363: 			result->initial_reader = nullptr;
364: 		} else {
365: 			result->readers = std::move(bind_data.union_readers);
366: 			if (result->readers.size() != bind_data.files.size()) {
367: 				result->readers = vector<shared_ptr<ParquetReader>>(bind_data.files.size(), nullptr);
368: 			}
369: 			if (bind_data.initial_reader) {
370: 				result->initial_reader = std::move(bind_data.initial_reader);
371: 				result->readers[0] = result->initial_reader;
372: 			} else if (result->readers[0]) {
373: 				result->initial_reader = result->readers[0];
374: 			} else {
375: 				result->initial_reader =
376: 				    make_shared<ParquetReader>(context, bind_data.files[0], bind_data.parquet_options);
377: 				result->readers[0] = result->initial_reader;
378: 			}
379: 		}
380: 		for (auto &reader : result->readers) {
381: 			if (!reader) {
382: 				continue;
383: 			}
384: 			MultiFileReader::InitializeReader(*reader, bind_data.parquet_options.file_options, bind_data.reader_bind,
385: 			                                  bind_data.types, bind_data.names, input.column_ids, input.filters,
386: 			                                  bind_data.files[0], context);
387: 		}
388: 
389: 		result->column_ids = input.column_ids;
390: 		result->filters = input.filters.get();
391: 		result->row_group_index = 0;
392: 		result->file_index = 0;
393: 		result->batch_index = 0;
394: 		result->max_threads = ParquetScanMaxThreads(context, input.bind_data.get());
395: 		if (input.CanRemoveFilterColumns()) {
396: 			result->projection_ids = input.projection_ids;
397: 			const auto table_types = bind_data.types;
398: 			for (const auto &col_idx : input.column_ids) {
399: 				if (IsRowIdColumnId(col_idx)) {
400: 					result->scanned_types.emplace_back(LogicalType::ROW_TYPE);
401: 				} else {
402: 					result->scanned_types.push_back(table_types[col_idx]);
403: 				}
404: 			}
405: 		}
406: 		return std::move(result);
407: 	}
408: 
409: 	static idx_t ParquetScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,
410: 	                                      LocalTableFunctionState *local_state,
411: 	                                      GlobalTableFunctionState *global_state) {
412: 		auto &data = local_state->Cast<ParquetReadLocalState>();
413: 		return data.batch_index;
414: 	}
415: 
416: 	static void ParquetScanSerialize(FieldWriter &writer, const FunctionData *bind_data_p,
417: 	                                 const TableFunction &function) {
418: 		auto &bind_data = bind_data_p->Cast<ParquetReadBindData>();
419: 		writer.WriteList<string>(bind_data.files);
420: 		writer.WriteRegularSerializableList(bind_data.types);
421: 		writer.WriteList<string>(bind_data.names);
422: 		bind_data.parquet_options.Serialize(writer);
423: 	}
424: 
425: 	static unique_ptr<FunctionData> ParquetScanDeserialize(PlanDeserializationState &state, FieldReader &reader,
426: 	                                                       TableFunction &function) {
427: 		auto &context = state.context;
428: 		auto files = reader.ReadRequiredList<string>();
429: 		auto types = reader.ReadRequiredSerializableList<LogicalType, LogicalType>();
430: 		auto names = reader.ReadRequiredList<string>();
431: 		ParquetOptions options(context);
432: 		options.Deserialize(reader);
433: 
434: 		return ParquetScanBindInternal(context, files, types, names, options);
435: 	}
436: 
437: 	static void ParquetScanFormatSerialize(FormatSerializer &serializer, const optional_ptr<FunctionData> bind_data_p,
438: 	                                       const TableFunction &function) {
439: 		auto &bind_data = bind_data_p->Cast<ParquetReadBindData>();
440: 		serializer.WriteProperty("files", bind_data.files);
441: 		serializer.WriteProperty("types", bind_data.types);
442: 		serializer.WriteProperty("names", bind_data.names);
443: 		serializer.WriteProperty("parquet_options", bind_data.parquet_options);
444: 	}
445: 
446: 	static unique_ptr<FunctionData> ParquetScanFormatDeserialize(FormatDeserializer &deserializer,
447: 	                                                             TableFunction &function) {
448: 		auto &context = deserializer.Get<ClientContext &>();
449: 		auto files = deserializer.ReadProperty<vector<string>>("files");
450: 		auto types = deserializer.ReadProperty<vector<LogicalType>>("types");
451: 		auto names = deserializer.ReadProperty<vector<string>>("names");
452: 		auto parquet_options = deserializer.ReadProperty<ParquetOptions>("parquet_options");
453: 		return ParquetScanBindInternal(context, files, types, names, parquet_options);
454: 	}
455: 
456: 	static void ParquetScanImplementation(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {
457: 		if (!data_p.local_state) {
458: 			return;
459: 		}
460: 		auto &data = data_p.local_state->Cast<ParquetReadLocalState>();
461: 		auto &gstate = data_p.global_state->Cast<ParquetReadGlobalState>();
462: 		auto &bind_data = data_p.bind_data->CastNoConst<ParquetReadBindData>();
463: 
464: 		do {
465: 			if (gstate.CanRemoveFilterColumns()) {
466: 				data.all_columns.Reset();
467: 				data.reader->Scan(data.scan_state, data.all_columns);
468: 				MultiFileReader::FinalizeChunk(bind_data.reader_bind, data.reader->reader_data, data.all_columns);
469: 				output.ReferenceColumns(data.all_columns, gstate.projection_ids);
470: 			} else {
471: 				data.reader->Scan(data.scan_state, output);
472: 				MultiFileReader::FinalizeChunk(bind_data.reader_bind, data.reader->reader_data, output);
473: 			}
474: 
475: 			bind_data.chunk_count++;
476: 			if (output.size() > 0) {
477: 				return;
478: 			}
479: 			if (!ParquetParallelStateNext(context, bind_data, data, gstate)) {
480: 				return;
481: 			}
482: 		} while (true);
483: 	}
484: 
485: 	static unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {
486: 		auto &data = bind_data->Cast<ParquetReadBindData>();
487: 		return make_uniq<NodeStatistics>(data.initial_file_cardinality * data.files.size());
488: 	}
489: 
490: 	static idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {
491: 		auto &data = bind_data->Cast<ParquetReadBindData>();
492: 		return data.initial_file_row_groups * data.files.size();
493: 	}
494: 
495: 	// This function looks for the next available row group. If not available, it will open files from bind_data.files
496: 	// until there is a row group available for scanning or the files runs out
497: 	static bool ParquetParallelStateNext(ClientContext &context, const ParquetReadBindData &bind_data,
498: 	                                     ParquetReadLocalState &scan_data, ParquetReadGlobalState &parallel_state) {
499: 		unique_lock<mutex> parallel_lock(parallel_state.lock);
500: 
501: 		while (true) {
502: 			if (parallel_state.error_opening_file) {
503: 				return false;
504: 			}
505: 
506: 			if (parallel_state.file_index >= parallel_state.readers.size()) {
507: 				return false;
508: 			}
509: 
510: 			D_ASSERT(parallel_state.initial_reader);
511: 
512: 			if (parallel_state.readers[parallel_state.file_index]) {
513: 				if (parallel_state.row_group_index <
514: 				    parallel_state.readers[parallel_state.file_index]->NumRowGroups()) {
515: 					// The current reader has rowgroups left to be scanned
516: 					scan_data.reader = parallel_state.readers[parallel_state.file_index];
517: 					vector<idx_t> group_indexes {parallel_state.row_group_index};
518: 					scan_data.reader->InitializeScan(scan_data.scan_state, group_indexes);
519: 					scan_data.batch_index = parallel_state.batch_index++;
520: 					scan_data.file_index = parallel_state.file_index;
521: 					parallel_state.row_group_index++;
522: 					return true;
523: 				} else {
524: 					// Set state to the next file
525: 					parallel_state.file_index++;
526: 					parallel_state.row_group_index = 0;
527: 
528: 					parallel_state.readers[parallel_state.file_index - 1] = nullptr;
529: 
530: 					if (parallel_state.file_index >= bind_data.files.size()) {
531: 						return false;
532: 					}
533: 					continue;
534: 				}
535: 			}
536: 
537: 			if (TryOpenNextFile(context, bind_data, scan_data, parallel_state, parallel_lock)) {
538: 				continue;
539: 			}
540: 
541: 			// Check if the current file is being opened, in that case we need to wait for it.
542: 			if (!parallel_state.readers[parallel_state.file_index] &&
543: 			    parallel_state.file_opening[parallel_state.file_index]) {
544: 				WaitForFile(parallel_state.file_index, parallel_state, parallel_lock);
545: 			}
546: 		}
547: 	}
548: 
549: 	static void ParquetComplexFilterPushdown(ClientContext &context, LogicalGet &get, FunctionData *bind_data_p,
550: 	                                         vector<unique_ptr<Expression>> &filters) {
551: 		auto &data = bind_data_p->Cast<ParquetReadBindData>();
552: 
553: 		auto reset_reader = MultiFileReader::ComplexFilterPushdown(context, data.files,
554: 		                                                           data.parquet_options.file_options, get, filters);
555: 		if (reset_reader) {
556: 			MultiFileReader::PruneReaders(data);
557: 		}
558: 	}
559: 
560: 	//! Wait for a file to become available. Parallel lock should be locked when calling.
561: 	static void WaitForFile(idx_t file_index, ParquetReadGlobalState &parallel_state,
562: 	                        unique_lock<mutex> &parallel_lock) {
563: 		while (true) {
564: 			// To get the file lock, we first need to release the parallel_lock to prevent deadlocking
565: 			parallel_lock.unlock();
566: 			unique_lock<mutex> current_file_lock(parallel_state.file_mutexes[file_index]);
567: 			parallel_lock.lock();
568: 
569: 			// Here we have both locks which means we can stop waiting if:
570: 			// - the thread opening the file is done and the file is available
571: 			// - the thread opening the file has failed
572: 			// - the file was somehow scanned till the end while we were waiting
573: 			if (parallel_state.file_index >= parallel_state.readers.size() ||
574: 			    parallel_state.readers[parallel_state.file_index] || parallel_state.error_opening_file) {
575: 				return;
576: 			}
577: 		}
578: 	}
579: 
580: 	//! Helper function that try to start opening a next file. Parallel lock should be locked when calling.
581: 	static bool TryOpenNextFile(ClientContext &context, const ParquetReadBindData &bind_data,
582: 	                            ParquetReadLocalState &scan_data, ParquetReadGlobalState &parallel_state,
583: 	                            unique_lock<mutex> &parallel_lock) {
584: 		for (idx_t i = parallel_state.file_index; i < bind_data.files.size(); i++) {
585: 			if (!parallel_state.readers[i] && parallel_state.file_opening[i] == false) {
586: 				string file = bind_data.files[i];
587: 				parallel_state.file_opening[i] = true;
588: 				auto pq_options = parallel_state.initial_reader->parquet_options;
589: 
590: 				// Now we switch which lock we are holding, instead of locking the global state, we grab the lock on
591: 				// the file we are opening. This file lock allows threads to wait for a file to be opened.
592: 				parallel_lock.unlock();
593: 
594: 				unique_lock<mutex> file_lock(parallel_state.file_mutexes[i]);
595: 
596: 				shared_ptr<ParquetReader> reader;
597: 				try {
598: 					reader = make_shared<ParquetReader>(context, file, pq_options);
599: 					MultiFileReader::InitializeReader(*reader, bind_data.parquet_options.file_options,
600: 					                                  bind_data.reader_bind, bind_data.types, bind_data.names,
601: 					                                  parallel_state.column_ids, parallel_state.filters,
602: 					                                  bind_data.files.front(), context);
603: 				} catch (...) {
604: 					parallel_lock.lock();
605: 					parallel_state.error_opening_file = true;
606: 					throw;
607: 				}
608: 
609: 				// Now re-lock the state and add the reader
610: 				parallel_lock.lock();
611: 				parallel_state.readers[i] = reader;
612: 
613: 				return true;
614: 			}
615: 		}
616: 
617: 		return false;
618: 	}
619: };
620: 
621: static case_insensitive_map_t<LogicalType> GetChildNameToTypeMap(const LogicalType &type) {
622: 	case_insensitive_map_t<LogicalType> name_to_type_map;
623: 	switch (type.id()) {
624: 	case LogicalTypeId::LIST:
625: 		name_to_type_map.emplace("element", ListType::GetChildType(type));
626: 		break;
627: 	case LogicalTypeId::MAP:
628: 		name_to_type_map.emplace("key", MapType::KeyType(type));
629: 		name_to_type_map.emplace("value", MapType::ValueType(type));
630: 		break;
631: 	case LogicalTypeId::STRUCT:
632: 		for (auto &child_type : StructType::GetChildTypes(type)) {
633: 			if (child_type.first == FieldID::DUCKDB_FIELD_ID) {
634: 				throw BinderException("Cannot have column named \"%s\" with FIELD_IDS", FieldID::DUCKDB_FIELD_ID);
635: 			}
636: 			name_to_type_map.emplace(child_type);
637: 		}
638: 		break;
639: 	default: // LCOV_EXCL_START
640: 		throw InternalException("Unexpected type in GetChildNameToTypeMap");
641: 	} // LCOV_EXCL_STOP
642: 	return name_to_type_map;
643: }
644: 
645: static void GetChildNamesAndTypes(const LogicalType &type, vector<string> &child_names,
646:                                   vector<LogicalType> &child_types) {
647: 	switch (type.id()) {
648: 	case LogicalTypeId::LIST:
649: 		child_names.emplace_back("element");
650: 		child_types.emplace_back(ListType::GetChildType(type));
651: 		break;
652: 	case LogicalTypeId::MAP:
653: 		child_names.emplace_back("key");
654: 		child_names.emplace_back("value");
655: 		child_types.emplace_back(MapType::KeyType(type));
656: 		child_types.emplace_back(MapType::ValueType(type));
657: 		break;
658: 	case LogicalTypeId::STRUCT:
659: 		for (auto &child_type : StructType::GetChildTypes(type)) {
660: 			child_names.emplace_back(child_type.first);
661: 			child_types.emplace_back(child_type.second);
662: 		}
663: 		break;
664: 	default: // LCOV_EXCL_START
665: 		throw InternalException("Unexpected type in GetChildNamesAndTypes");
666: 	} // LCOV_EXCL_STOP
667: }
668: 
669: static void GenerateFieldIDs(ChildFieldIDs &field_ids, idx_t &field_id, const vector<string> &names,
670:                              const vector<LogicalType> &sql_types) {
671: 	D_ASSERT(names.size() == sql_types.size());
672: 	for (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {
673: 		const auto &col_name = names[col_idx];
674: 		auto inserted = field_ids.ids->insert(make_pair(col_name, FieldID(field_id++)));
675: 		D_ASSERT(inserted.second);
676: 
677: 		const auto &col_type = sql_types[col_idx];
678: 		if (col_type.id() != LogicalTypeId::LIST && col_type.id() != LogicalTypeId::MAP &&
679: 		    col_type.id() != LogicalTypeId::STRUCT) {
680: 			continue;
681: 		}
682: 
683: 		// Cannot use GetChildNameToTypeMap here because we lose order, and we want to generate depth-first
684: 		vector<string> child_names;
685: 		vector<LogicalType> child_types;
686: 		GetChildNamesAndTypes(col_type, child_names, child_types);
687: 
688: 		GenerateFieldIDs(inserted.first->second.child_field_ids, field_id, child_names, child_types);
689: 	}
690: }
691: 
692: static void GetFieldIDs(const Value &field_ids_value, ChildFieldIDs &field_ids,
693:                         unordered_set<uint32_t> &unique_field_ids,
694:                         const case_insensitive_map_t<LogicalType> &name_to_type_map) {
695: 	const auto &struct_type = field_ids_value.type();
696: 	if (struct_type.id() != LogicalTypeId::STRUCT) {
697: 		throw BinderException(
698: 		    "Expected FIELD_IDS to be a STRUCT, e.g., {col1: 42, col2: {%s: 43, nested_col: 44}, col3: 44}",
699: 		    FieldID::DUCKDB_FIELD_ID);
700: 	}
701: 	const auto &struct_children = StructValue::GetChildren(field_ids_value);
702: 	D_ASSERT(StructType::GetChildTypes(struct_type).size() == struct_children.size());
703: 	for (idx_t i = 0; i < struct_children.size(); i++) {
704: 		const auto &col_name = StringUtil::Lower(StructType::GetChildName(struct_type, i));
705: 		if (col_name == FieldID::DUCKDB_FIELD_ID) {
706: 			continue;
707: 		}
708: 
709: 		auto it = name_to_type_map.find(col_name);
710: 		if (it == name_to_type_map.end()) {
711: 			string names;
712: 			for (const auto &name : name_to_type_map) {
713: 				if (!names.empty()) {
714: 					names += ", ";
715: 				}
716: 				names += name.first;
717: 			}
718: 			throw BinderException("Column name \"%s\" specified in FIELD_IDS not found. Available column names: [%s]",
719: 			                      col_name, names);
720: 		}
721: 		D_ASSERT(field_ids.ids->find(col_name) == field_ids.ids->end()); // Caught by STRUCT - deduplicates keys
722: 
723: 		const auto &child_value = struct_children[i];
724: 		const auto &child_type = child_value.type();
725: 		optional_ptr<const Value> field_id_value;
726: 		optional_ptr<const Value> child_field_ids_value;
727: 
728: 		if (child_type.id() == LogicalTypeId::STRUCT) {
729: 			const auto &nested_children = StructValue::GetChildren(child_value);
730: 			D_ASSERT(StructType::GetChildTypes(child_type).size() == nested_children.size());
731: 			for (idx_t nested_i = 0; nested_i < nested_children.size(); nested_i++) {
732: 				const auto &field_id_or_nested_col = StructType::GetChildName(child_type, nested_i);
733: 				if (field_id_or_nested_col == FieldID::DUCKDB_FIELD_ID) {
734: 					field_id_value = &nested_children[nested_i];
735: 				} else {
736: 					child_field_ids_value = &child_value;
737: 				}
738: 			}
739: 		} else {
740: 			field_id_value = &child_value;
741: 		}
742: 
743: 		FieldID field_id;
744: 		if (field_id_value) {
745: 			Value field_id_integer_value = field_id_value->DefaultCastAs(LogicalType::INTEGER);
746: 			const uint32_t field_id_int = IntegerValue::Get(field_id_integer_value);
747: 			if (!unique_field_ids.insert(field_id_int).second) {
748: 				throw BinderException("Duplicate field_id %s found in FIELD_IDS", field_id_integer_value.ToString());
749: 			}
750: 			field_id = FieldID(field_id_int);
751: 		}
752: 		auto inserted = field_ids.ids->insert(make_pair(col_name, std::move(field_id)));
753: 		D_ASSERT(inserted.second);
754: 
755: 		if (child_field_ids_value) {
756: 			const auto &col_type = it->second;
757: 			if (col_type.id() != LogicalTypeId::LIST && col_type.id() != LogicalTypeId::MAP &&
758: 			    col_type.id() != LogicalTypeId::STRUCT) {
759: 				throw BinderException("Column \"%s\" with type \"%s\" cannot have a nested FIELD_IDS specification",
760: 				                      col_name, LogicalTypeIdToString(col_type.id()));
761: 			}
762: 
763: 			GetFieldIDs(*child_field_ids_value, inserted.first->second.child_field_ids, unique_field_ids,
764: 			            GetChildNameToTypeMap(col_type));
765: 		}
766: 	}
767: }
768: 
769: unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info, vector<string> &names,
770:                                           vector<LogicalType> &sql_types) {
771: 	D_ASSERT(names.size() == sql_types.size());
772: 	bool row_group_size_bytes_set = false;
773: 	auto bind_data = make_uniq<ParquetWriteBindData>();
774: 	for (auto &option : info.options) {
775: 		const auto loption = StringUtil::Lower(option.first);
776: 		if (option.second.size() != 1) {
777: 			// All parquet write options require exactly one argument
778: 			throw BinderException("%s requires exactly one argument", StringUtil::Upper(loption));
779: 		}
780: 		if (loption == "row_group_size" || loption == "chunk_size") {
781: 			bind_data->row_group_size = option.second[0].GetValue<uint64_t>();
782: 		} else if (loption == "row_group_size_bytes") {
783: 			auto roption = option.second[0];
784: 			if (roption.GetTypeMutable().id() == LogicalTypeId::VARCHAR) {
785: 				bind_data->row_group_size_bytes = DBConfig::ParseMemoryLimit(roption.ToString());
786: 			} else {
787: 				bind_data->row_group_size_bytes = option.second[0].GetValue<uint64_t>();
788: 			}
789: 			row_group_size_bytes_set = true;
790: 		} else if (loption == "compression" || loption == "codec") {
791: 			const auto roption = StringUtil::Lower(option.second[0].ToString());
792: 			if (roption == "uncompressed") {
793: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::UNCOMPRESSED;
794: 			} else if (roption == "snappy") {
795: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
796: 			} else if (roption == "gzip") {
797: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::GZIP;
798: 			} else if (roption == "zstd") {
799: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::ZSTD;
800: 			} else {
801: 				throw BinderException("Expected %s argument to be either [uncompressed, snappy, gzip or zstd]",
802: 				                      loption);
803: 			}
804: 		} else if (loption == "field_ids") {
805: 			if (option.second[0].type().id() == LogicalTypeId::VARCHAR &&
806: 			    StringUtil::Lower(StringValue::Get(option.second[0])) == "auto") {
807: 				idx_t field_id = 0;
808: 				GenerateFieldIDs(bind_data->field_ids, field_id, names, sql_types);
809: 			} else {
810: 				unordered_set<uint32_t> unique_field_ids;
811: 				case_insensitive_map_t<LogicalType> name_to_type_map;
812: 				for (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {
813: 					if (names[col_idx] == FieldID::DUCKDB_FIELD_ID) {
814: 						throw BinderException("Cannot have a column named \"%s\" when writing FIELD_IDS",
815: 						                      FieldID::DUCKDB_FIELD_ID);
816: 					}
817: 					name_to_type_map.emplace(names[col_idx], sql_types[col_idx]);
818: 				}
819: 				GetFieldIDs(option.second[0], bind_data->field_ids, unique_field_ids, name_to_type_map);
820: 			}
821: 		} else {
822: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
823: 		}
824: 	}
825: 	if (!row_group_size_bytes_set) {
826: 		bind_data->row_group_size_bytes = bind_data->row_group_size * ParquetWriteBindData::BYTES_PER_ROW;
827: 	}
828: 	bind_data->sql_types = sql_types;
829: 	bind_data->column_names = names;
830: 	return std::move(bind_data);
831: }
832: 
833: unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &context, FunctionData &bind_data,
834:                                                             const string &file_path) {
835: 	auto global_state = make_uniq<ParquetWriteGlobalState>();
836: 	auto &parquet_bind = bind_data.Cast<ParquetWriteBindData>();
837: 
838: 	auto &fs = FileSystem::GetFileSystem(context);
839: 	global_state->writer = make_uniq<ParquetWriter>(fs, file_path, parquet_bind.sql_types, parquet_bind.column_names,
840: 	                                                parquet_bind.codec, parquet_bind.field_ids.Copy());
841: 	return std::move(global_state);
842: }
843: 
844: void ParquetWriteSink(ExecutionContext &context, FunctionData &bind_data_p, GlobalFunctionData &gstate,
845:                       LocalFunctionData &lstate, DataChunk &input) {
846: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
847: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
848: 	auto &local_state = lstate.Cast<ParquetWriteLocalState>();
849: 
850: 	// append data to the local (buffered) chunk collection
851: 	local_state.buffer.Append(local_state.append_state, input);
852: 
853: 	if (local_state.buffer.Count() > bind_data.row_group_size ||
854: 	    local_state.buffer.SizeInBytes() > bind_data.row_group_size_bytes) {
855: 		// if the chunk collection exceeds a certain size (rows/bytes) we flush it to the parquet file
856: 		local_state.append_state.current_chunk_state.handles.clear();
857: 		global_state.writer->Flush(local_state.buffer);
858: 		local_state.buffer.InitializeAppend(local_state.append_state);
859: 	}
860: }
861: 
862: void ParquetWriteCombine(ExecutionContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
863:                          LocalFunctionData &lstate) {
864: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
865: 	auto &local_state = lstate.Cast<ParquetWriteLocalState>();
866: 	// flush any data left in the local state to the file
867: 	global_state.writer->Flush(local_state.buffer);
868: }
869: 
870: void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
871: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
872: 	// finalize: write any additional metadata to the file here
873: 	global_state.writer->Finalize();
874: }
875: 
876: unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ExecutionContext &context, FunctionData &bind_data_p) {
877: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
878: 	return make_uniq<ParquetWriteLocalState>(context.client, bind_data.sql_types);
879: }
880: 
881: // LCOV_EXCL_START
882: static void ParquetCopySerialize(FieldWriter &writer, const FunctionData &bind_data_p, const CopyFunction &function) {
883: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
884: 	writer.WriteRegularSerializableList<LogicalType>(bind_data.sql_types);
885: 	writer.WriteList<string>(bind_data.column_names);
886: 	writer.WriteField<duckdb_parquet::format::CompressionCodec::type>(bind_data.codec);
887: 	writer.WriteField<idx_t>(bind_data.row_group_size);
888: }
889: 
890: static unique_ptr<FunctionData> ParquetCopyDeserialize(ClientContext &context, FieldReader &reader,
891:                                                        CopyFunction &function) {
892: 	unique_ptr<ParquetWriteBindData> data = make_uniq<ParquetWriteBindData>();
893: 
894: 	data->sql_types = reader.ReadRequiredSerializableList<LogicalType, LogicalType>();
895: 	data->column_names = reader.ReadRequiredList<string>();
896: 	data->codec = reader.ReadRequired<duckdb_parquet::format::CompressionCodec::type>();
897: 	data->row_group_size = reader.ReadRequired<idx_t>();
898: 
899: 	return std::move(data);
900: }
901: // LCOV_EXCL_STOP
902: 
903: //===--------------------------------------------------------------------===//
904: // Execution Mode
905: //===--------------------------------------------------------------------===//
906: CopyFunctionExecutionMode ParquetWriteExecutionMode(bool preserve_insertion_order, bool supports_batch_index) {
907: 	if (!preserve_insertion_order) {
908: 		return CopyFunctionExecutionMode::PARALLEL_COPY_TO_FILE;
909: 	}
910: 	if (supports_batch_index) {
911: 		return CopyFunctionExecutionMode::BATCH_COPY_TO_FILE;
912: 	}
913: 	return CopyFunctionExecutionMode::REGULAR_COPY_TO_FILE;
914: }
915: //===--------------------------------------------------------------------===//
916: // Prepare Batch
917: //===--------------------------------------------------------------------===//
918: struct ParquetWriteBatchData : public PreparedBatchData {
919: 	PreparedRowGroup prepared_row_group;
920: };
921: 
922: unique_ptr<PreparedBatchData> ParquetWritePrepareBatch(ClientContext &context, FunctionData &bind_data,
923:                                                        GlobalFunctionData &gstate,
924:                                                        unique_ptr<ColumnDataCollection> collection) {
925: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
926: 	auto result = make_uniq<ParquetWriteBatchData>();
927: 	global_state.writer->PrepareRowGroup(*collection, result->prepared_row_group);
928: 	return std::move(result);
929: }
930: 
931: //===--------------------------------------------------------------------===//
932: // Flush Batch
933: //===--------------------------------------------------------------------===//
934: void ParquetWriteFlushBatch(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
935:                             PreparedBatchData &batch_p) {
936: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
937: 	auto &batch = batch_p.Cast<ParquetWriteBatchData>();
938: 	global_state.writer->FlushRowGroup(batch.prepared_row_group);
939: }
940: 
941: //===--------------------------------------------------------------------===//
942: // Desired Batch Size
943: //===--------------------------------------------------------------------===//
944: idx_t ParquetWriteDesiredBatchSize(ClientContext &context, FunctionData &bind_data_p) {
945: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
946: 	return bind_data.row_group_size;
947: }
948: 
949: //===--------------------------------------------------------------------===//
950: // Scan Replacement
951: //===--------------------------------------------------------------------===//
952: unique_ptr<TableRef> ParquetScanReplacement(ClientContext &context, const string &table_name,
953:                                             ReplacementScanData *data) {
954: 	auto lower_name = StringUtil::Lower(table_name);
955: 	if (!StringUtil::EndsWith(lower_name, ".parquet") && !StringUtil::Contains(lower_name, ".parquet?")) {
956: 		return nullptr;
957: 	}
958: 	auto table_function = make_uniq<TableFunctionRef>();
959: 	vector<unique_ptr<ParsedExpression>> children;
960: 	children.push_back(make_uniq<ConstantExpression>(Value(table_name)));
961: 	table_function->function = make_uniq<FunctionExpression>("parquet_scan", std::move(children));
962: 
963: 	if (!FileSystem::HasGlob(table_name)) {
964: 		auto &fs = FileSystem::GetFileSystem(context);
965: 		table_function->alias = fs.ExtractBaseName(table_name);
966: 	}
967: 
968: 	return std::move(table_function);
969: }
970: 
971: void ParquetExtension::Load(DuckDB &db) {
972: 	auto &db_instance = *db.instance;
973: 	auto &fs = db.GetFileSystem();
974: 	fs.RegisterSubSystem(FileCompressionType::ZSTD, make_uniq<ZStdFileSystem>());
975: 
976: 	auto scan_fun = ParquetScanFunction::GetFunctionSet();
977: 	scan_fun.name = "read_parquet";
978: 	ExtensionUtil::RegisterFunction(db_instance, scan_fun);
979: 	scan_fun.name = "parquet_scan";
980: 	ExtensionUtil::RegisterFunction(db_instance, scan_fun);
981: 
982: 	// parquet_metadata
983: 	ParquetMetaDataFunction meta_fun;
984: 	ExtensionUtil::RegisterFunction(db_instance, MultiFileReader::CreateFunctionSet(meta_fun));
985: 
986: 	// parquet_schema
987: 	ParquetSchemaFunction schema_fun;
988: 	ExtensionUtil::RegisterFunction(db_instance, MultiFileReader::CreateFunctionSet(schema_fun));
989: 
990: 	CopyFunction function("parquet");
991: 	function.copy_to_bind = ParquetWriteBind;
992: 	function.copy_to_initialize_global = ParquetWriteInitializeGlobal;
993: 	function.copy_to_initialize_local = ParquetWriteInitializeLocal;
994: 	function.copy_to_sink = ParquetWriteSink;
995: 	function.copy_to_combine = ParquetWriteCombine;
996: 	function.copy_to_finalize = ParquetWriteFinalize;
997: 	function.execution_mode = ParquetWriteExecutionMode;
998: 	function.copy_from_bind = ParquetScanFunction::ParquetReadBind;
999: 	function.copy_from_function = scan_fun.functions[0];
1000: 	function.prepare_batch = ParquetWritePrepareBatch;
1001: 	function.flush_batch = ParquetWriteFlushBatch;
1002: 	function.desired_batch_size = ParquetWriteDesiredBatchSize;
1003: 	function.serialize = ParquetCopySerialize;
1004: 	function.deserialize = ParquetCopyDeserialize;
1005: 
1006: 	function.extension = "parquet";
1007: 	ExtensionUtil::RegisterFunction(db_instance, function);
1008: 
1009: 	auto &config = DBConfig::GetConfig(*db.instance);
1010: 	config.replacement_scans.emplace_back(ParquetScanReplacement);
1011: 	config.AddExtensionOption("binary_as_string", "In Parquet files, interpret binary data as a string.",
1012: 	                          LogicalType::BOOLEAN);
1013: }
1014: 
1015: std::string ParquetExtension::Name() {
1016: 	return "parquet";
1017: }
1018: 
1019: } // namespace duckdb
1020: 
1021: #ifdef DUCKDB_BUILD_LOADABLE_EXTENSION
1022: extern "C" {
1023: 
1024: DUCKDB_EXTENSION_API void parquet_init(duckdb::DatabaseInstance &db) { // NOLINT
1025: 	duckdb::DuckDB db_wrapper(db);
1026: 	db_wrapper.LoadExtension<duckdb::ParquetExtension>();
1027: }
1028: 
1029: DUCKDB_EXTENSION_API const char *parquet_version() { // NOLINT
1030: 	return duckdb::DuckDB::LibraryVersion();
1031: }
1032: }
1033: #endif
1034: 
1035: #ifndef DUCKDB_EXTENSION_MAIN
1036: #error DUCKDB_EXTENSION_MAIN not defined
1037: #endif
[end of extension/parquet/parquet_extension.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: