You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
CSV read, INTERNAL Error: Attempted to access index 1 within vector of size 1
### What happens?

Hi,
I try to read a big CSV file and I have INTERNAL Error.

### To Reproduce

I cannot attach a sample file, because it's big.

The source is a CSV, with delim=";" https://opencoesione.gov.it/media/open_data//progetti_esteso_20231231.zip

Decompress it and get the top 290k lines:

```bash
progetti_esteso_20231231.zip
<progetti_esteso_20231231.csv head -n 290000 >tmp.csv
```

Then I run

```
duckdb --csv -c "SET preserve_insertion_order=false;SELECT count(*) FROM read_csv_auto('tmp.csv',parallel=false,all_varchar=true)"
```

and I have 

```
Error: INTERNAL Error: Attempted to access index 1 within vector of size 1
```

### OS:

Debian 12

### DuckDB Version:

v0.10.0 20b1486d11

### DuckDB Client:

0.10.0

### Full Name:

Andrea Borruso

### Affiliation:

onData

### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?

I have not tested with any build

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/logo-dl/DuckDB_Logo-stacked.svg" height="120">
3: </div>
4: <br>
5: 
6: 
7: 
8: 
9: <p align="center">
10:   <a href="https://github.com/duckdb/duckdb/actions">
11:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge">
12:   </a>
13:   <a href="https://discord.gg/tcvwpjfnZx">
14:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
15:   </a>
16:   <a href="https://github.com/duckdb/duckdb/releases/">
17:     <img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release">
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
44: 
45: ## Support
46: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp]
1: #include "duckdb/execution/operator/csv_scanner/string_value_scanner.hpp"
2: #include "duckdb/execution/operator/csv_scanner/csv_casting.hpp"
3: #include "duckdb/execution/operator/csv_scanner/skip_scanner.hpp"
4: #include "duckdb/execution/operator/csv_scanner/csv_file_scanner.hpp"
5: #include "duckdb/main/client_data.hpp"
6: #include "duckdb/common/operator/integer_cast_operator.hpp"
7: #include "duckdb/common/operator/double_cast_operator.hpp"
8: #include <algorithm>
9: #include "utf8proc_wrapper.hpp"
10: 
11: namespace duckdb {
12: 
13: StringValueResult::StringValueResult(CSVStates &states, CSVStateMachine &state_machine,
14:                                      const shared_ptr<CSVBufferHandle> &buffer_handle, Allocator &buffer_allocator,
15:                                      idx_t result_size_p, idx_t buffer_position, CSVErrorHandler &error_hander_p,
16:                                      CSVIterator &iterator_p, bool store_line_size_p,
17:                                      shared_ptr<CSVFileScan> csv_file_scan_p, idx_t &lines_read_p, bool sniffing_p)
18:     : ScannerResult(states, state_machine),
19:       number_of_columns(NumericCast<uint32_t>(state_machine.dialect_options.num_cols)),
20:       null_padding(state_machine.options.null_padding), ignore_errors(state_machine.options.ignore_errors),
21:       null_str_ptr(state_machine.options.null_str.c_str()), null_str_size(state_machine.options.null_str.size()),
22:       result_size(result_size_p), error_handler(error_hander_p), iterator(iterator_p),
23:       store_line_size(store_line_size_p), csv_file_scan(std::move(csv_file_scan_p)), lines_read(lines_read_p),
24:       sniffing(sniffing_p) {
25: 	// Vector information
26: 	D_ASSERT(number_of_columns > 0);
27: 	buffer_handles.push_back(buffer_handle);
28: 	// Buffer Information
29: 	buffer_ptr = buffer_handle->Ptr();
30: 	buffer_size = buffer_handle->actual_size;
31: 	last_position = buffer_position;
32: 
33: 	// Current Result information
34: 	previous_line_start = {iterator.pos.buffer_idx, iterator.pos.buffer_pos, buffer_handle->actual_size};
35: 	pre_previous_line_start = previous_line_start;
36: 	// Fill out Parse Types
37: 	vector<LogicalType> logical_types;
38: 	parse_types = make_unsafe_uniq_array<pair<LogicalTypeId, bool>>(number_of_columns);
39: 	if (!csv_file_scan) {
40: 		for (idx_t i = 0; i < number_of_columns; i++) {
41: 			parse_types[i] = {LogicalTypeId::VARCHAR, true};
42: 			logical_types.emplace_back(LogicalType::VARCHAR);
43: 			string name = "Column_" + to_string(i);
44: 			names.emplace_back(name);
45: 		}
46: 	} else {
47: 		if (csv_file_scan->file_types.size() > number_of_columns) {
48: 			throw InvalidInputException(
49: 			    "Mismatch between the number of columns (%d) in the CSV file and what is expected in the scanner (%d).",
50: 			    number_of_columns, csv_file_scan->file_types.size());
51: 		}
52: 		for (idx_t i = 0; i < csv_file_scan->file_types.size(); i++) {
53: 			auto &type = csv_file_scan->file_types[i];
54: 			if (StringValueScanner::CanDirectlyCast(type, state_machine.options.dialect_options.date_format)) {
55: 				parse_types[i] = {type.id(), true};
56: 				logical_types.emplace_back(type);
57: 			} else {
58: 				parse_types[i] = {LogicalTypeId::VARCHAR, type.id() == LogicalTypeId::VARCHAR};
59: 				logical_types.emplace_back(LogicalType::VARCHAR);
60: 			}
61: 		}
62: 		names = csv_file_scan->names;
63: 		if (!csv_file_scan->projected_columns.empty()) {
64: 			projecting_columns = false;
65: 			projected_columns = make_unsafe_uniq_array<bool>(number_of_columns);
66: 			for (idx_t col_idx = 0; col_idx < number_of_columns; col_idx++) {
67: 				if (csv_file_scan->projected_columns.find(col_idx) == csv_file_scan->projected_columns.end()) {
68: 					// Column is not projected
69: 					projecting_columns = true;
70: 					projected_columns[col_idx] = false;
71: 				} else {
72: 					projected_columns[col_idx] = true;
73: 				}
74: 			}
75: 		}
76: 		if (!projecting_columns) {
77: 			for (idx_t j = logical_types.size(); j < number_of_columns; j++) {
78: 				// This can happen if we have sneaky null columns at the end that we wish to ignore
79: 				parse_types[j] = {LogicalTypeId::VARCHAR, true};
80: 				logical_types.emplace_back(LogicalType::VARCHAR);
81: 			}
82: 		}
83: 	}
84: 
85: 	// Initialize Parse Chunk
86: 	parse_chunk.Initialize(buffer_allocator, logical_types, result_size);
87: 	for (auto &col : parse_chunk.data) {
88: 		vector_ptr.push_back(FlatVector::GetData<string_t>(col));
89: 		validity_mask.push_back(&FlatVector::Validity(col));
90: 	}
91: }
92: 
93: StringValueResult::~StringValueResult() {
94: 	// We have to insert the lines read by this scanner
95: 	error_handler.Insert(iterator.GetBoundaryIdx(), lines_read);
96: 	if (!iterator.done) {
97: 		// Some operators, like Limit, might cause a future error to incorrectly report the wrong error line
98: 		// Better to print nothing to print something wrong
99: 		error_handler.DontPrintErrorLine();
100: 	}
101: }
102: 
103: inline bool IsValueNull(const char *null_str_ptr, const char *value_ptr, const idx_t size) {
104: 	for (idx_t i = 0; i < size; i++) {
105: 		if (null_str_ptr[i] != value_ptr[i]) {
106: 			return false;
107: 		}
108: 	}
109: 	return true;
110: }
111: 
112: void StringValueResult::AddValueToVector(const char *value_ptr, const idx_t size, bool allocate) {
113: 	if (cur_col_id >= number_of_columns) {
114: 		bool error = true;
115: 		if (cur_col_id == number_of_columns && ((quoted && state_machine.options.allow_quoted_nulls) || !quoted)) {
116: 			// we make an exception if the first over-value is null
117: 			error = !IsValueNull(null_str_ptr, value_ptr, size);
118: 		}
119: 		if (error) {
120: 			HandleOverLimitRows();
121: 		}
122: 	}
123: 	if (ignore_current_row) {
124: 		return;
125: 	}
126: 	if (projecting_columns) {
127: 		if (!projected_columns[cur_col_id]) {
128: 			cur_col_id++;
129: 			return;
130: 		}
131: 	}
132: 	if (size == null_str_size) {
133: 		if (((quoted && state_machine.options.allow_quoted_nulls) || !quoted)) {
134: 			if (IsValueNull(null_str_ptr, value_ptr, size)) {
135: 				bool empty = false;
136: 				if (chunk_col_id < state_machine.options.force_not_null.size()) {
137: 					empty = state_machine.options.force_not_null[chunk_col_id];
138: 				}
139: 				if (empty) {
140: 					if (parse_types[chunk_col_id].first != LogicalTypeId::VARCHAR) {
141: 						// If it is not a varchar, empty values are not accepted, we must error.
142: 						cast_errors[chunk_col_id] = std::string("");
143: 					}
144: 					static_cast<string_t *>(vector_ptr[chunk_col_id])[number_of_rows] = string_t();
145: 				} else {
146: 					if (chunk_col_id == number_of_columns) {
147: 						// We check for a weird case, where we ignore an extra value, if it is a null value
148: 						return;
149: 					}
150: 					validity_mask[chunk_col_id]->SetInvalid(number_of_rows);
151: 				}
152: 				cur_col_id++;
153: 				chunk_col_id++;
154: 				return;
155: 			}
156: 		}
157: 	}
158: 	bool success = true;
159: 	switch (parse_types[chunk_col_id].first) {
160: 	case LogicalTypeId::TINYINT:
161: 		success = TrySimpleIntegerCast(value_ptr, size, static_cast<int8_t *>(vector_ptr[chunk_col_id])[number_of_rows],
162: 		                               false);
163: 		break;
164: 	case LogicalTypeId::SMALLINT:
165: 		success = TrySimpleIntegerCast(value_ptr, size,
166: 		                               static_cast<int16_t *>(vector_ptr[chunk_col_id])[number_of_rows], false);
167: 		break;
168: 	case LogicalTypeId::INTEGER:
169: 		success = TrySimpleIntegerCast(value_ptr, size,
170: 		                               static_cast<int32_t *>(vector_ptr[chunk_col_id])[number_of_rows], false);
171: 		break;
172: 	case LogicalTypeId::BIGINT:
173: 		success = TrySimpleIntegerCast(value_ptr, size,
174: 		                               static_cast<int64_t *>(vector_ptr[chunk_col_id])[number_of_rows], false);
175: 		break;
176: 	case LogicalTypeId::UTINYINT:
177: 		success = TrySimpleIntegerCast<uint8_t, false>(
178: 		    value_ptr, size, static_cast<uint8_t *>(vector_ptr[chunk_col_id])[number_of_rows], false);
179: 		break;
180: 	case LogicalTypeId::USMALLINT:
181: 		success = TrySimpleIntegerCast<uint16_t, false>(
182: 		    value_ptr, size, static_cast<uint16_t *>(vector_ptr[chunk_col_id])[number_of_rows], false);
183: 		break;
184: 	case LogicalTypeId::UINTEGER:
185: 		success = TrySimpleIntegerCast<uint32_t, false>(
186: 		    value_ptr, size, static_cast<uint32_t *>(vector_ptr[chunk_col_id])[number_of_rows], false);
187: 		break;
188: 	case LogicalTypeId::UBIGINT:
189: 		success = TrySimpleIntegerCast<uint64_t, false>(
190: 		    value_ptr, size, static_cast<uint64_t *>(vector_ptr[chunk_col_id])[number_of_rows], false);
191: 		break;
192: 	case LogicalTypeId::DOUBLE:
193: 		success =
194: 		    TryDoubleCast<double>(value_ptr, size, static_cast<double *>(vector_ptr[chunk_col_id])[number_of_rows],
195: 		                          false, state_machine.options.decimal_separator[0]);
196: 		break;
197: 	case LogicalTypeId::FLOAT:
198: 		success = TryDoubleCast<float>(value_ptr, size, static_cast<float *>(vector_ptr[chunk_col_id])[number_of_rows],
199: 		                               false, state_machine.options.decimal_separator[0]);
200: 		break;
201: 	case LogicalTypeId::DATE: {
202: 		idx_t pos;
203: 		bool special;
204: 		success = Date::TryConvertDate(value_ptr, size, pos,
205: 		                               static_cast<date_t *>(vector_ptr[chunk_col_id])[number_of_rows], special, false);
206: 		break;
207: 	}
208: 	case LogicalTypeId::TIMESTAMP: {
209: 		success = Timestamp::TryConvertTimestamp(
210: 		              value_ptr, size, static_cast<timestamp_t *>(vector_ptr[chunk_col_id])[number_of_rows]) ==
211: 		          TimestampCastResult::SUCCESS;
212: 		break;
213: 	}
214: 	default: {
215: 		// By default we add a string
216: 		// We only evaluate if a string is utf8 valid, if it's actually a varchar
217: 		if (parse_types[chunk_col_id].second && !Utf8Proc::IsValid(value_ptr, UnsafeNumericCast<uint32_t>(size))) {
218: 			bool force_error = !state_machine.options.ignore_errors && sniffing;
219: 			// Invalid unicode, we must error
220: 			LinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(), lines_read);
221: 			auto csv_error = CSVError::InvalidUTF8(state_machine.options, lines_per_batch);
222: 			error_handler.Error(csv_error, force_error);
223: 			// If we got here, we are ingoring errors, hence we must ignore this line.
224: 			ignore_current_row = true;
225: 			break;
226: 		}
227: 		if (allocate) {
228: 			// If it's a value produced over multiple buffers, we must allocate
229: 			static_cast<string_t *>(vector_ptr[chunk_col_id])[number_of_rows] = StringVector::AddStringOrBlob(
230: 			    parse_chunk.data[chunk_col_id], string_t(value_ptr, UnsafeNumericCast<uint32_t>(size)));
231: 		} else {
232: 			static_cast<string_t *>(vector_ptr[chunk_col_id])[number_of_rows] =
233: 			    string_t(value_ptr, UnsafeNumericCast<uint32_t>(size));
234: 		}
235: 		break;
236: 	}
237: 	}
238: 	if (!success) {
239: 		// We had a casting error, we push it here because we can only error when finishing the line read.
240: 		cast_errors[cur_col_id] = std::string(value_ptr, size);
241: 	}
242: 	cur_col_id++;
243: 	chunk_col_id++;
244: }
245: 
246: Value StringValueResult::GetValue(idx_t row_idx, idx_t col_idx) {
247: 	if (validity_mask[col_idx]->AllValid()) {
248: 		return Value(static_cast<string_t *>(vector_ptr[col_idx])[row_idx]);
249: 	} else {
250: 		if (validity_mask[col_idx]->RowIsValid(row_idx)) {
251: 			return Value(static_cast<string_t *>(vector_ptr[col_idx])[row_idx]);
252: 		} else {
253: 			return Value();
254: 		}
255: 	}
256: }
257: DataChunk &StringValueResult::ToChunk() {
258: 	parse_chunk.SetCardinality(number_of_rows);
259: 	return parse_chunk;
260: }
261: 
262: void StringValueResult::Reset() {
263: 	if (number_of_rows == 0) {
264: 		return;
265: 	}
266: 	number_of_rows = 0;
267: 	cur_col_id = 0;
268: 	chunk_col_id = 0;
269: 	for (auto &v : validity_mask) {
270: 		v->SetAllValid(result_size);
271: 	}
272: 	buffer_handles.clear();
273: 	ignore_current_row = false;
274: }
275: 
276: void StringValueResult::AddQuotedValue(StringValueResult &result, const idx_t buffer_pos) {
277: 	if (result.escaped) {
278: 		if (result.projecting_columns) {
279: 			if (!result.projected_columns[result.cur_col_id]) {
280: 				result.cur_col_id++;
281: 				result.quoted = false;
282: 				result.escaped = false;
283: 				return;
284: 			}
285: 		}
286: 		// If it's an escaped value we have to remove all the escapes, this is not really great
287: 		auto value = StringValueScanner::RemoveEscape(
288: 		    result.buffer_ptr + result.quoted_position + 1, buffer_pos - result.quoted_position - 2,
289: 		    result.state_machine.dialect_options.state_machine_options.escape.GetValue(),
290: 		    result.parse_chunk.data[result.chunk_col_id]);
291: 		result.AddValueToVector(value.GetData(), value.GetSize());
292: 	} else {
293: 		if (buffer_pos < result.last_position + 2) {
294: 			// empty value
295: 			auto value = string_t();
296: 			result.AddValueToVector(value.GetData(), value.GetSize());
297: 		} else {
298: 			result.AddValueToVector(result.buffer_ptr + result.quoted_position + 1,
299: 			                        buffer_pos - result.quoted_position - 2);
300: 		}
301: 	}
302: 	result.quoted = false;
303: 	result.escaped = false;
304: }
305: 
306: void StringValueResult::AddValue(StringValueResult &result, const idx_t buffer_pos) {
307: 	if (result.last_position > buffer_pos) {
308: 		return;
309: 	}
310: 	if (result.quoted) {
311: 		StringValueResult::AddQuotedValue(result, buffer_pos);
312: 	} else {
313: 		result.AddValueToVector(result.buffer_ptr + result.last_position, buffer_pos - result.last_position);
314: 	}
315: 	result.last_position = buffer_pos + 1;
316: }
317: 
318: void StringValueResult::HandleOverLimitRows() {
319: 	LinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(), number_of_rows + 1);
320: 	auto csv_error = CSVError::IncorrectColumnAmountError(state_machine.options, nullptr, number_of_columns,
321: 	                                                      cur_col_id + 1, lines_per_batch);
322: 	error_handler.Error(csv_error);
323: 	// If we get here we need to remove the last line
324: 	cur_col_id = 0;
325: 	chunk_col_id = 0;
326: 	ignore_current_row = true;
327: }
328: 
329: void StringValueResult::QuotedNewLine(StringValueResult &result) {
330: 	result.quoted_new_line = true;
331: }
332: 
333: void StringValueResult::NullPaddingQuotedNewlineCheck() {
334: 	// We do some checks for null_padding correctness
335: 	if (state_machine.options.null_padding && iterator.IsBoundarySet() && quoted_new_line && iterator.done) {
336: 		// If we have null_padding set, we found a quoted new line, we are scanning the file in parallel and it's the
337: 		// last row of this thread.
338: 		LinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(), number_of_rows + 1);
339: 		auto csv_error = CSVError::NullPaddingFail(state_machine.options, lines_per_batch);
340: 		error_handler.Error(csv_error);
341: 	}
342: }
343: 
344: bool StringValueResult::AddRowInternal() {
345: 	if (ignore_current_row) {
346: 		cur_col_id = 0;
347: 		chunk_col_id = 0;
348: 		// An error occurred on this row, we are ignoring it and resetting our control flag
349: 		ignore_current_row = false;
350: 		return false;
351: 	}
352: 	if (!cast_errors.empty()) {
353: 		// A wild casting error appears
354: 		// Recreate row for rejects-table
355: 		vector<Value> row;
356: 		if (!state_machine.options.rejects_table_name.empty()) {
357: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
358: 				if (cast_errors.find(col) != cast_errors.end()) {
359: 					row.push_back(cast_errors[col]);
360: 				} else {
361: 					row.push_back(parse_chunk.data[col].GetValue(number_of_rows));
362: 				}
363: 			}
364: 		}
365: 		for (auto &cast_error : cast_errors) {
366: 			std::ostringstream error;
367: 			// Casting Error Message
368: 			error << "Could not convert string \"" << cast_error.second << "\" to \'"
369: 			      << LogicalTypeIdToString(parse_types[cast_error.first].first) << "\'";
370: 			auto error_string = error.str();
371: 			LinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(), lines_read);
372: 
373: 			auto csv_error = CSVError::CastError(state_machine.options, names[cast_error.first], error_string,
374: 			                                     cast_error.first, row, lines_per_batch);
375: 			error_handler.Error(csv_error);
376: 		}
377: 		// If we got here it means we are ignoring errors, hence we need to signify to our result scanner to ignore this
378: 		// row
379: 		// Cleanup this line and continue
380: 		cast_errors.clear();
381: 		cur_col_id = 0;
382: 		chunk_col_id = 0;
383: 		return false;
384: 	}
385: 	NullPaddingQuotedNewlineCheck();
386: 	quoted_new_line = false;
387: 	// We need to check if we are getting the correct number of columns here.
388: 	// If columns are correct, we add it, and that's it.
389: 	if (cur_col_id != number_of_columns) {
390: 		// We have too few columns:
391: 		if (null_padding) {
392: 			while (cur_col_id < number_of_columns) {
393: 				bool empty = false;
394: 				if (cur_col_id < state_machine.options.force_not_null.size()) {
395: 					empty = state_machine.options.force_not_null[cur_col_id];
396: 				}
397: 				if (projecting_columns) {
398: 					if (!projected_columns[cur_col_id]) {
399: 						cur_col_id++;
400: 						continue;
401: 					}
402: 				}
403: 				if (empty) {
404: 					static_cast<string_t *>(vector_ptr[chunk_col_id])[number_of_rows] = string_t();
405: 				} else {
406: 					validity_mask[chunk_col_id]->SetInvalid(number_of_rows);
407: 				}
408: 				cur_col_id++;
409: 				chunk_col_id++;
410: 			}
411: 		} else {
412: 			// If we are not null-padding this is an error
413: 			LinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(), number_of_rows + 1);
414: 			auto csv_error = CSVError::IncorrectColumnAmountError(state_machine.options, nullptr, number_of_columns,
415: 			                                                      cur_col_id, lines_per_batch);
416: 			error_handler.Error(csv_error);
417: 			// If we are here we ignore_errors, so we delete this line
418: 			number_of_rows--;
419: 		}
420: 	}
421: 	cur_col_id = 0;
422: 	chunk_col_id = 0;
423: 	number_of_rows++;
424: 	if (number_of_rows >= result_size) {
425: 		// We have a full chunk
426: 		return true;
427: 	}
428: 	return false;
429: }
430: 
431: bool StringValueResult::AddRow(StringValueResult &result, const idx_t buffer_pos) {
432: 	if (result.last_position <= buffer_pos) {
433: 		LinePosition current_line_start = {result.iterator.pos.buffer_idx, result.iterator.pos.buffer_pos,
434: 		                                   result.buffer_size};
435: 		idx_t current_line_size = current_line_start - result.previous_line_start;
436: 		if (result.store_line_size) {
437: 			result.error_handler.NewMaxLineSize(current_line_size);
438: 		}
439: 		if (current_line_size > result.state_machine.options.maximum_line_size) {
440: 			LinesPerBoundary lines_per_batch(result.iterator.GetBoundaryIdx(), result.number_of_rows);
441: 			auto csv_error = CSVError::LineSizeError(result.state_machine.options, current_line_size, lines_per_batch);
442: 			result.error_handler.Error(csv_error);
443: 		}
444: 		result.pre_previous_line_start = result.previous_line_start;
445: 		result.previous_line_start = current_line_start;
446: 		// We add the value
447: 		if (result.quoted) {
448: 			StringValueResult::AddQuotedValue(result, buffer_pos);
449: 		} else {
450: 			result.AddValueToVector(result.buffer_ptr + result.last_position, buffer_pos - result.last_position);
451: 		}
452: 		if (result.state_machine.dialect_options.state_machine_options.new_line == NewLineIdentifier::CARRY_ON) {
453: 			if (result.states.states[1] == CSVState::RECORD_SEPARATOR) {
454: 				// Even though this is marked as a carry on, this is a hippie mixie
455: 				result.last_position = buffer_pos + 1;
456: 			} else {
457: 				result.last_position = buffer_pos + 2;
458: 			}
459: 		} else {
460: 			result.last_position = buffer_pos + 1;
461: 		}
462: 	}
463: 
464: 	// We add the value
465: 	return result.AddRowInternal();
466: }
467: 
468: void StringValueResult::InvalidState(StringValueResult &result) {
469: 	// FIXME: How do we recover from an invalid state? Can we restart the state machine and jump to the next row?
470: 	LinesPerBoundary lines_per_batch(result.iterator.GetBoundaryIdx(), result.number_of_rows);
471: 	auto csv_error = CSVError::UnterminatedQuotesError(result.state_machine.options,
472: 	                                                   static_cast<string_t *>(result.vector_ptr[result.chunk_col_id]),
473: 	                                                   result.number_of_rows, result.cur_col_id, lines_per_batch);
474: 	result.error_handler.Error(csv_error);
475: }
476: 
477: bool StringValueResult::EmptyLine(StringValueResult &result, const idx_t buffer_pos) {
478: 	// We care about empty lines if this is a single column csv file
479: 	result.last_position = buffer_pos + 1;
480: 	if (result.states.IsCarriageReturn() &&
481: 	    result.state_machine.dialect_options.state_machine_options.new_line == NewLineIdentifier::CARRY_ON) {
482: 		result.last_position++;
483: 	}
484: 	if (result.number_of_columns == 1) {
485: 		if (result.null_str_size == 0) {
486: 			bool empty = false;
487: 			if (!result.state_machine.options.force_not_null.empty()) {
488: 				empty = result.state_machine.options.force_not_null[0];
489: 			}
490: 			if (empty) {
491: 				static_cast<string_t *>(result.vector_ptr[0])[result.number_of_rows] = string_t();
492: 			} else {
493: 				result.validity_mask[0]->SetInvalid(result.number_of_rows);
494: 			}
495: 			result.number_of_rows++;
496: 		}
497: 		if (result.number_of_rows >= result.result_size) {
498: 			// We have a full chunk
499: 			return true;
500: 		}
501: 	}
502: 	return false;
503: }
504: 
505: StringValueScanner::StringValueScanner(idx_t scanner_idx_p, const shared_ptr<CSVBufferManager> &buffer_manager,
506:                                        const shared_ptr<CSVStateMachine> &state_machine,
507:                                        const shared_ptr<CSVErrorHandler> &error_handler,
508:                                        const shared_ptr<CSVFileScan> &csv_file_scan, bool sniffing,
509:                                        CSVIterator boundary, idx_t result_size)
510:     : BaseScanner(buffer_manager, state_machine, error_handler, sniffing, csv_file_scan, boundary),
511:       scanner_idx(scanner_idx_p),
512:       result(states, *state_machine, cur_buffer_handle, BufferAllocator::Get(buffer_manager->context), result_size,
513:              iterator.pos.buffer_pos, *error_handler, iterator,
514:              buffer_manager->context.client_data->debug_set_max_line_length, csv_file_scan, lines_read, sniffing) {
515: }
516: 
517: StringValueScanner::StringValueScanner(const shared_ptr<CSVBufferManager> &buffer_manager,
518:                                        const shared_ptr<CSVStateMachine> &state_machine,
519:                                        const shared_ptr<CSVErrorHandler> &error_handler)
520:     : BaseScanner(buffer_manager, state_machine, error_handler, false, nullptr, {}), scanner_idx(0),
521:       result(states, *state_machine, cur_buffer_handle, Allocator::DefaultAllocator(), STANDARD_VECTOR_SIZE,
522:              iterator.pos.buffer_pos, *error_handler, iterator,
523:              buffer_manager->context.client_data->debug_set_max_line_length, csv_file_scan, lines_read, sniffing) {
524: }
525: 
526: unique_ptr<StringValueScanner> StringValueScanner::GetCSVScanner(ClientContext &context, CSVReaderOptions &options) {
527: 	auto state_machine = make_shared<CSVStateMachine>(options, options.dialect_options.state_machine_options,
528: 	                                                  CSVStateMachineCache::Get(context));
529: 
530: 	state_machine->dialect_options.num_cols = options.dialect_options.num_cols;
531: 	state_machine->dialect_options.header = options.dialect_options.header;
532: 	auto buffer_manager = make_shared<CSVBufferManager>(context, options, options.file_path, 0);
533: 	auto scanner = make_uniq<StringValueScanner>(buffer_manager, state_machine, make_shared<CSVErrorHandler>());
534: 	scanner->csv_file_scan = make_shared<CSVFileScan>(context, options.file_path, options);
535: 	scanner->csv_file_scan->InitializeProjection();
536: 	return scanner;
537: }
538: 
539: bool StringValueScanner::FinishedIterator() {
540: 	return iterator.done;
541: }
542: 
543: StringValueResult &StringValueScanner::ParseChunk() {
544: 	result.Reset();
545: 	ParseChunkInternal(result);
546: 	return result;
547: }
548: 
549: void StringValueScanner::Flush(DataChunk &insert_chunk) {
550: 	auto &process_result = ParseChunk();
551: 	// First Get Parsed Chunk
552: 	auto &parse_chunk = process_result.ToChunk();
553: 	// We have to check if we got to error
554: 	error_handler->ErrorIfNeeded();
555: 
556: 	if (parse_chunk.size() == 0) {
557: 		return;
558: 	}
559: 	// convert the columns in the parsed chunk to the types of the table
560: 	insert_chunk.SetCardinality(parse_chunk);
561: 
562: 	// We keep track of the borked lines, in case we are ignoring errors
563: 	unordered_set<idx_t> borked_lines;
564: 	D_ASSERT(csv_file_scan);
565: 
566: 	auto &reader_data = csv_file_scan->reader_data;
567: 	// Now Do the cast-aroo
568: 	for (idx_t c = 0; c < reader_data.column_ids.size(); c++) {
569: 		idx_t col_idx = c;
570: 		idx_t result_idx = reader_data.column_mapping[c];
571: 		if (!csv_file_scan->projection_ids.empty()) {
572: 			result_idx = reader_data.column_mapping[csv_file_scan->projection_ids[c].second];
573: 		}
574: 		if (col_idx >= parse_chunk.ColumnCount()) {
575: 			throw InvalidInputException("Mismatch between the schema of different files");
576: 		}
577: 		auto &parse_vector = parse_chunk.data[col_idx];
578: 		auto &result_vector = insert_chunk.data[result_idx];
579: 		auto &type = result_vector.GetType();
580: 		auto &parse_type = parse_vector.GetType();
581: 		if (type == LogicalType::VARCHAR || (type != LogicalType::VARCHAR && parse_type != LogicalType::VARCHAR)) {
582: 			// reinterpret rather than reference
583: 			result_vector.Reinterpret(parse_vector);
584: 		} else {
585: 			string error_message;
586: 			CastParameters parameters(false, &error_message);
587: 			bool success;
588: 			idx_t line_error = 0;
589: 			bool line_error_set = true;
590: 
591: 			if (!state_machine->options.dialect_options.date_format.at(LogicalTypeId::DATE).GetValue().Empty() &&
592: 			    type.id() == LogicalTypeId::DATE) {
593: 				// use the date format to cast the chunk
594: 				success = CSVCast::TryCastDateVector(state_machine->options.dialect_options.date_format, parse_vector,
595: 				                                     result_vector, parse_chunk.size(), parameters, line_error);
596: 			} else if (!state_machine->options.dialect_options.date_format.at(LogicalTypeId::TIMESTAMP)
597: 			                .GetValue()
598: 			                .Empty() &&
599: 			           type.id() == LogicalTypeId::TIMESTAMP) {
600: 				// use the date format to cast the chunk
601: 				success = CSVCast::TryCastTimestampVector(state_machine->options.dialect_options.date_format,
602: 				                                          parse_vector, result_vector, parse_chunk.size(), parameters);
603: 			} else if (state_machine->options.decimal_separator != "." &&
604: 			           (type.id() == LogicalTypeId::FLOAT || type.id() == LogicalTypeId::DOUBLE)) {
605: 				success =
606: 				    CSVCast::TryCastFloatingVectorCommaSeparated(state_machine->options, parse_vector, result_vector,
607: 				                                                 parse_chunk.size(), parameters, type, line_error);
608: 			} else if (state_machine->options.decimal_separator != "." && type.id() == LogicalTypeId::DECIMAL) {
609: 				success = CSVCast::TryCastDecimalVectorCommaSeparated(
610: 				    state_machine->options, parse_vector, result_vector, parse_chunk.size(), parameters, type);
611: 			} else {
612: 				// target type is not varchar: perform a cast
613: 				success = VectorOperations::TryCast(buffer_manager->context, parse_vector, result_vector,
614: 				                                    parse_chunk.size(), &error_message, false, true);
615: 				line_error_set = false;
616: 			}
617: 			if (success) {
618: 				continue;
619: 			}
620: 			// An error happened, to propagate it we need to figure out the exact line where the casting failed.
621: 			UnifiedVectorFormat inserted_column_data;
622: 			result_vector.ToUnifiedFormat(parse_chunk.size(), inserted_column_data);
623: 			UnifiedVectorFormat parse_column_data;
624: 			parse_vector.ToUnifiedFormat(parse_chunk.size(), parse_column_data);
625: 			if (!line_error_set) {
626: 				for (; line_error < parse_chunk.size(); line_error++) {
627: 					if (!inserted_column_data.validity.RowIsValid(line_error) &&
628: 					    parse_column_data.validity.RowIsValid(line_error)) {
629: 						break;
630: 					}
631: 				}
632: 			}
633: 			{
634: 				vector<Value> row;
635: 
636: 				if (state_machine->options.ignore_errors) {
637: 					for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
638: 						row.push_back(parse_chunk.GetValue(col, line_error));
639: 					}
640: 				}
641: 
642: 				LinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(),
643: 				                                 lines_read - parse_chunk.size() + line_error);
644: 				auto csv_error = CSVError::CastError(state_machine->options, csv_file_scan->names[col_idx],
645: 				                                     error_message, col_idx, row, lines_per_batch);
646: 				error_handler->Error(csv_error);
647: 			}
648: 			borked_lines.insert(line_error++);
649: 			D_ASSERT(state_machine->options.ignore_errors);
650: 			// We are ignoring errors. We must continue but ignoring borked rows
651: 			for (; line_error < parse_chunk.size(); line_error++) {
652: 				if (!inserted_column_data.validity.RowIsValid(line_error) &&
653: 				    parse_column_data.validity.RowIsValid(line_error)) {
654: 					borked_lines.insert(line_error);
655: 					vector<Value> row;
656: 					for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
657: 						row.push_back(parse_chunk.GetValue(col, line_error));
658: 					}
659: 					LinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(),
660: 					                                 lines_read - parse_chunk.size() + line_error);
661: 					auto csv_error = CSVError::CastError(state_machine->options, csv_file_scan->names[col_idx],
662: 					                                     error_message, col_idx, row, lines_per_batch);
663: 
664: 					error_handler->Error(csv_error);
665: 				}
666: 			}
667: 		}
668: 	}
669: 	if (!borked_lines.empty()) {
670: 		// We must remove the borked lines from our chunk
671: 		SelectionVector succesful_rows(parse_chunk.size() - borked_lines.size());
672: 		idx_t sel_idx = 0;
673: 		for (idx_t row_idx = 0; row_idx < parse_chunk.size(); row_idx++) {
674: 			if (borked_lines.find(row_idx) == borked_lines.end()) {
675: 				succesful_rows.set_index(sel_idx++, row_idx);
676: 			}
677: 		}
678: 		// Now we slice the result
679: 		insert_chunk.Slice(succesful_rows, sel_idx);
680: 	}
681: }
682: 
683: void StringValueScanner::Initialize() {
684: 	states.Initialize();
685: 
686: 	if (result.result_size != 1 && !(sniffing && state_machine->options.null_padding &&
687: 	                                 !state_machine->options.dialect_options.skip_rows.IsSetByUser())) {
688: 		SetStart();
689: 	}
690: 	result.last_position = iterator.pos.buffer_pos;
691: 	result.previous_line_start = {iterator.pos.buffer_idx, iterator.pos.buffer_pos, cur_buffer_handle->actual_size};
692: 
693: 	result.pre_previous_line_start = result.previous_line_start;
694: }
695: 
696: void StringValueScanner::ProcessExtraRow() {
697: 	result.NullPaddingQuotedNewlineCheck();
698: 	idx_t to_pos = cur_buffer_handle->actual_size;
699: 	while (iterator.pos.buffer_pos < to_pos) {
700: 		state_machine->Transition(states, buffer_handle_ptr[iterator.pos.buffer_pos]);
701: 		switch (states.states[1]) {
702: 		case CSVState::INVALID:
703: 			result.InvalidState(result);
704: 			iterator.pos.buffer_pos++;
705: 			return;
706: 		case CSVState::RECORD_SEPARATOR:
707: 			if (states.states[0] == CSVState::RECORD_SEPARATOR) {
708: 				lines_read++;
709: 				result.EmptyLine(result, iterator.pos.buffer_pos);
710: 				iterator.pos.buffer_pos++;
711: 				return;
712: 			} else if (states.states[0] != CSVState::CARRIAGE_RETURN) {
713: 				lines_read++;
714: 				result.AddRow(result, iterator.pos.buffer_pos);
715: 				iterator.pos.buffer_pos++;
716: 				return;
717: 			}
718: 			lines_read++;
719: 			iterator.pos.buffer_pos++;
720: 			break;
721: 		case CSVState::CARRIAGE_RETURN:
722: 			if (states.states[0] != CSVState::RECORD_SEPARATOR) {
723: 				result.AddRow(result, iterator.pos.buffer_pos);
724: 				iterator.pos.buffer_pos++;
725: 				lines_read++;
726: 				return;
727: 			} else {
728: 				result.EmptyLine(result, iterator.pos.buffer_pos);
729: 				iterator.pos.buffer_pos++;
730: 				lines_read++;
731: 				return;
732: 			}
733: 		case CSVState::DELIMITER:
734: 			result.AddValue(result, iterator.pos.buffer_pos);
735: 			iterator.pos.buffer_pos++;
736: 			break;
737: 		case CSVState::QUOTED:
738: 			if (states.states[0] == CSVState::UNQUOTED) {
739: 				result.SetEscaped(result);
740: 			}
741: 			result.SetQuoted(result, iterator.pos.buffer_pos);
742: 			iterator.pos.buffer_pos++;
743: 			while (state_machine->transition_array
744: 			           .skip_quoted[static_cast<uint8_t>(buffer_handle_ptr[iterator.pos.buffer_pos])] &&
745: 			       iterator.pos.buffer_pos < to_pos - 1) {
746: 				iterator.pos.buffer_pos++;
747: 			}
748: 			break;
749: 		case CSVState::ESCAPE:
750: 			result.SetEscaped(result);
751: 			iterator.pos.buffer_pos++;
752: 			break;
753: 		case CSVState::STANDARD:
754: 			iterator.pos.buffer_pos++;
755: 			while (state_machine->transition_array
756: 			           .skip_standard[static_cast<uint8_t>(buffer_handle_ptr[iterator.pos.buffer_pos])] &&
757: 			       iterator.pos.buffer_pos < to_pos - 1) {
758: 				iterator.pos.buffer_pos++;
759: 			}
760: 			break;
761: 		case CSVState::QUOTED_NEW_LINE:
762: 			result.quoted_new_line = true;
763: 			result.NullPaddingQuotedNewlineCheck();
764: 			iterator.pos.buffer_pos++;
765: 			break;
766: 		default:
767: 			iterator.pos.buffer_pos++;
768: 			break;
769: 		}
770: 	}
771: }
772: 
773: string_t StringValueScanner::RemoveEscape(const char *str_ptr, idx_t end, char escape, Vector &vector) {
774: 	// Figure out the exact size
775: 	idx_t str_pos = 0;
776: 	bool just_escaped = false;
777: 	for (idx_t cur_pos = 0; cur_pos < end; cur_pos++) {
778: 		if (str_ptr[cur_pos] == escape && !just_escaped) {
779: 			just_escaped = true;
780: 		} else {
781: 			just_escaped = false;
782: 			str_pos++;
783: 		}
784: 	}
785: 
786: 	auto removed_escapes = StringVector::EmptyString(vector, str_pos);
787: 	auto removed_escapes_ptr = removed_escapes.GetDataWriteable();
788: 	// Allocate string and copy it
789: 	str_pos = 0;
790: 	just_escaped = false;
791: 	for (idx_t cur_pos = 0; cur_pos < end; cur_pos++) {
792: 		char c = str_ptr[cur_pos];
793: 		if (c == escape && !just_escaped) {
794: 			just_escaped = true;
795: 		} else {
796: 			just_escaped = false;
797: 			removed_escapes_ptr[str_pos++] = c;
798: 		}
799: 	}
800: 	removed_escapes.Finalize();
801: 	return removed_escapes;
802: }
803: 
804: void StringValueScanner::ProcessOverbufferValue() {
805: 	// Process first string
806: 	states.Initialize();
807: 	string overbuffer_string;
808: 	auto previous_buffer = previous_buffer_handle->Ptr();
809: 	if (result.last_position == previous_buffer_handle->actual_size) {
810: 		state_machine->Transition(states, previous_buffer[result.last_position - 1]);
811: 	}
812: 	idx_t j = 0;
813: 	result.quoted = false;
814: 	for (idx_t i = result.last_position; i < previous_buffer_handle->actual_size; i++) {
815: 		state_machine->Transition(states, previous_buffer[i]);
816: 		if (states.EmptyLine() || states.IsCurrentNewRow()) {
817: 			continue;
818: 		}
819: 		if (states.NewRow() || states.NewValue()) {
820: 			break;
821: 		} else {
822: 			overbuffer_string += previous_buffer[i];
823: 		}
824: 		if (states.IsQuoted()) {
825: 			result.SetQuoted(result, j);
826: 		}
827: 		if (states.IsEscaped()) {
828: 			result.escaped = true;
829: 		}
830: 		j++;
831: 	}
832: 	if (overbuffer_string.empty() &&
833: 	    state_machine->dialect_options.state_machine_options.new_line == NewLineIdentifier::CARRY_ON) {
834: 		if (buffer_handle_ptr[iterator.pos.buffer_pos] == '\n') {
835: 			iterator.pos.buffer_pos++;
836: 		}
837: 	}
838: 	// second buffer
839: 	for (; iterator.pos.buffer_pos < cur_buffer_handle->actual_size; iterator.pos.buffer_pos++) {
840: 		state_machine->Transition(states, buffer_handle_ptr[iterator.pos.buffer_pos]);
841: 		if (states.EmptyLine()) {
842: 			if (state_machine->dialect_options.num_cols == 1) {
843: 				break;
844: 			} else {
845: 				continue;
846: 			}
847: 		}
848: 		if (states.NewRow() || states.NewValue()) {
849: 			break;
850: 		} else {
851: 			overbuffer_string += buffer_handle_ptr[iterator.pos.buffer_pos];
852: 		}
853: 		if (states.IsQuoted()) {
854: 			result.SetQuoted(result, j);
855: 		}
856: 		if (states.IsEscaped()) {
857: 			result.escaped = true;
858: 		}
859: 		j++;
860: 	}
861: 	string_t value;
862: 	if (result.quoted) {
863: 		value = string_t(overbuffer_string.c_str() + result.quoted_position,
864: 		                 UnsafeNumericCast<uint32_t>(overbuffer_string.size() - 1 - result.quoted_position));
865: 		if (result.escaped) {
866: 			const auto str_ptr = static_cast<const char *>(overbuffer_string.c_str() + result.quoted_position);
867: 			value =
868: 			    StringValueScanner::RemoveEscape(str_ptr, overbuffer_string.size() - 2,
869: 			                                     state_machine->dialect_options.state_machine_options.escape.GetValue(),
870: 			                                     result.parse_chunk.data[result.chunk_col_id]);
871: 		}
872: 	} else {
873: 		value = string_t(overbuffer_string.c_str(), UnsafeNumericCast<uint32_t>(overbuffer_string.size()));
874: 	}
875: 
876: 	if (states.EmptyLine() && state_machine->dialect_options.num_cols == 1) {
877: 		result.EmptyLine(result, iterator.pos.buffer_pos);
878: 	} else if (!states.IsNotSet()) {
879: 		result.AddValueToVector(value.GetData(), value.GetSize(), true);
880: 	}
881: 
882: 	if (states.NewRow() && !states.IsNotSet()) {
883: 		result.AddRowInternal();
884: 		lines_read++;
885: 	}
886: 
887: 	if (iterator.pos.buffer_pos >= cur_buffer_handle->actual_size && cur_buffer_handle->is_last_buffer) {
888: 		result.added_last_line = true;
889: 	}
890: 	if (states.IsCarriageReturn() &&
891: 	    state_machine->dialect_options.state_machine_options.new_line == NewLineIdentifier::CARRY_ON) {
892: 		result.last_position = ++iterator.pos.buffer_pos + 1;
893: 	} else {
894: 		result.last_position = ++iterator.pos.buffer_pos;
895: 	}
896: 	// Be sure to reset the quoted and escaped variables
897: 	result.quoted = false;
898: 	result.escaped = false;
899: }
900: 
901: bool StringValueScanner::MoveToNextBuffer() {
902: 	if (iterator.pos.buffer_pos >= cur_buffer_handle->actual_size) {
903: 		previous_buffer_handle = cur_buffer_handle;
904: 		cur_buffer_handle = buffer_manager->GetBuffer(++iterator.pos.buffer_idx);
905: 		result.buffer_handles.push_back(cur_buffer_handle);
906: 		if (!cur_buffer_handle) {
907: 			iterator.pos.buffer_idx--;
908: 			buffer_handle_ptr = nullptr;
909: 			// We do not care if it's a quoted new line on the last row of our file.
910: 			result.quoted_new_line = false;
911: 			// This means we reached the end of the file, we must add a last line if there is any to be added
912: 			if (states.EmptyLine() || states.NewRow() || result.added_last_line || states.IsCurrentNewRow() ||
913: 			    states.IsNotSet()) {
914: 				if (result.cur_col_id == result.number_of_columns) {
915: 					result.number_of_rows++;
916: 				}
917: 				result.cur_col_id = 0;
918: 				result.chunk_col_id = 0;
919: 				return false;
920: 			} else if (states.NewValue()) {
921: 				// we add the value
922: 				result.AddValue(result, previous_buffer_handle->actual_size);
923: 				// And an extra empty value to represent what comes after the delimiter
924: 				result.AddRow(result, previous_buffer_handle->actual_size);
925: 				lines_read++;
926: 			} else if (states.IsQuotedCurrent()) {
927: 				// Unterminated quote
928: 				result.InvalidState(result);
929: 			} else {
930: 				result.AddRow(result, previous_buffer_handle->actual_size);
931: 				lines_read++;
932: 			}
933: 			return false;
934: 		}
935: 		iterator.pos.buffer_pos = 0;
936: 		buffer_handle_ptr = cur_buffer_handle->Ptr();
937: 		// Handle overbuffer value
938: 		ProcessOverbufferValue();
939: 		result.buffer_ptr = buffer_handle_ptr;
940: 		result.buffer_size = cur_buffer_handle->actual_size;
941: 		return true;
942: 	}
943: 	return false;
944: }
945: 
946: void StringValueScanner::SkipBOM() {
947: 	if (cur_buffer_handle->actual_size >= 3 && result.buffer_ptr[0] == '\xEF' && result.buffer_ptr[1] == '\xBB' &&
948: 	    result.buffer_ptr[2] == '\xBF') {
949: 		iterator.pos.buffer_pos = 3;
950: 	}
951: }
952: 
953: void StringValueScanner::SkipCSVRows() {
954: 	idx_t rows_to_skip =
955: 	    state_machine->dialect_options.skip_rows.GetValue() + state_machine->dialect_options.header.GetValue();
956: 	if (rows_to_skip == 0) {
957: 		return;
958: 	}
959: 	SkipScanner row_skipper(buffer_manager, state_machine, error_handler, rows_to_skip);
960: 	row_skipper.ParseChunk();
961: 	iterator.pos.buffer_pos = row_skipper.GetIteratorPosition();
962: 	if (row_skipper.state_machine->options.dialect_options.state_machine_options.new_line ==
963: 	        NewLineIdentifier::CARRY_ON &&
964: 	    row_skipper.states.states[1] == CSVState::CARRIAGE_RETURN) {
965: 		iterator.pos.buffer_pos++;
966: 	}
967: 	if (result.store_line_size) {
968: 		result.error_handler.NewMaxLineSize(iterator.pos.buffer_pos);
969: 	}
970: 	lines_read += row_skipper.GetLinesRead();
971: }
972: 
973: void StringValueScanner::SkipUntilNewLine() {
974: 	// Now skip until next newline
975: 	if (state_machine->options.dialect_options.state_machine_options.new_line.GetValue() ==
976: 	    NewLineIdentifier::CARRY_ON) {
977: 		bool carriage_return = false;
978: 		bool not_carriage_return = false;
979: 		for (; iterator.pos.buffer_pos < cur_buffer_handle->actual_size; iterator.pos.buffer_pos++) {
980: 			if (buffer_handle_ptr[iterator.pos.buffer_pos] == '\r') {
981: 				carriage_return = true;
982: 			} else if (buffer_handle_ptr[iterator.pos.buffer_pos] != '\n') {
983: 				not_carriage_return = true;
984: 			}
985: 			if (buffer_handle_ptr[iterator.pos.buffer_pos] == '\n') {
986: 				if (carriage_return || not_carriage_return) {
987: 					iterator.pos.buffer_pos++;
988: 					return;
989: 				}
990: 			}
991: 		}
992: 	} else {
993: 		for (; iterator.pos.buffer_pos < cur_buffer_handle->actual_size; iterator.pos.buffer_pos++) {
994: 			if (buffer_handle_ptr[iterator.pos.buffer_pos] == '\n' ||
995: 			    buffer_handle_ptr[iterator.pos.buffer_pos] == '\r') {
996: 				iterator.pos.buffer_pos++;
997: 				return;
998: 			}
999: 		}
1000: 	}
1001: }
1002: 
1003: bool StringValueScanner::CanDirectlyCast(const LogicalType &type,
1004:                                          const map<LogicalTypeId, CSVOption<StrpTimeFormat>> &format_options) {
1005: 
1006: 	switch (type.id()) {
1007: 		// All Integers (Except HugeInt)
1008: 	case LogicalTypeId::TINYINT:
1009: 	case LogicalTypeId::SMALLINT:
1010: 	case LogicalTypeId::INTEGER:
1011: 	case LogicalTypeId::BIGINT:
1012: 	case LogicalTypeId::UTINYINT:
1013: 	case LogicalTypeId::USMALLINT:
1014: 	case LogicalTypeId::UINTEGER:
1015: 	case LogicalTypeId::UBIGINT:
1016: 	case LogicalTypeId::DOUBLE:
1017: 	case LogicalTypeId::FLOAT:
1018: 		return true;
1019: 	case LogicalTypeId::DATE:
1020: 		// We can only internally cast YYYY-MM-DD
1021: 		if (format_options.at(LogicalTypeId::DATE).GetValue().format_specifier == "%Y-%m-%d") {
1022: 			return true;
1023: 		} else {
1024: 			return false;
1025: 		}
1026: 	case LogicalTypeId::TIMESTAMP:
1027: 		if (format_options.at(LogicalTypeId::TIMESTAMP).GetValue().format_specifier == "%Y-%m-%d %H:%M:%S") {
1028: 			return true;
1029: 		} else {
1030: 			return false;
1031: 		}
1032: 	case LogicalType::VARCHAR:
1033: 		return true;
1034: 	default:
1035: 		return false;
1036: 	}
1037: }
1038: 
1039: void StringValueScanner::SetStart() {
1040: 	if (iterator.pos.buffer_idx == 0 && iterator.pos.buffer_pos == 0) {
1041: 		// This means this is the very first buffer
1042: 		// This CSV is not from auto-detect, so we don't know where exactly it starts
1043: 		// Hence we potentially have to skip empty lines and headers.
1044: 		SkipBOM();
1045: 		SkipCSVRows();
1046: 		return;
1047: 	}
1048: 	// We have to look for a new line that fits our schema
1049: 	// 1. We walk until the next new line
1050: 	bool line_found;
1051: 	unique_ptr<StringValueScanner> scan_finder;
1052: 	do {
1053: 		SkipUntilNewLine();
1054: 		if (state_machine->options.null_padding) {
1055: 			// When Null Padding, we assume we start from the correct new-line
1056: 			return;
1057: 		}
1058: 
1059: 		scan_finder = make_uniq<StringValueScanner>(
1060: 		    0, buffer_manager, state_machine, make_shared<CSVErrorHandler>(true), csv_file_scan, false, iterator, 1);
1061: 		auto &tuples = scan_finder->ParseChunk();
1062: 		line_found = true;
1063: 		if (tuples.number_of_rows != 1) {
1064: 			line_found = false;
1065: 			// If no tuples were parsed, this is not the correct start, we need to skip until the next new line
1066: 			// Or if columns don't match, this is not the correct start, we need to skip until the next new line
1067: 			if (scan_finder->previous_buffer_handle) {
1068: 				if (scan_finder->iterator.pos.buffer_pos >= scan_finder->previous_buffer_handle->actual_size &&
1069: 				    scan_finder->previous_buffer_handle->is_last_buffer) {
1070: 					iterator.pos.buffer_idx = scan_finder->iterator.pos.buffer_idx;
1071: 					iterator.pos.buffer_pos = scan_finder->iterator.pos.buffer_pos;
1072: 					result.last_position = iterator.pos.buffer_pos;
1073: 					iterator.done = scan_finder->iterator.done;
1074: 					return;
1075: 				}
1076: 			}
1077: 			if (iterator.pos.buffer_pos == cur_buffer_handle->actual_size) {
1078: 				// If things go terribly wrong, we never loop indefinetly.
1079: 				iterator.pos.buffer_idx = scan_finder->iterator.pos.buffer_idx;
1080: 				iterator.pos.buffer_pos = scan_finder->iterator.pos.buffer_pos;
1081: 				result.last_position = iterator.pos.buffer_pos;
1082: 				iterator.done = scan_finder->iterator.done;
1083: 				return;
1084: 			}
1085: 		}
1086: 	} while (!line_found);
1087: 	iterator.pos.buffer_idx = scan_finder->result.pre_previous_line_start.buffer_idx;
1088: 	iterator.pos.buffer_pos = scan_finder->result.pre_previous_line_start.buffer_pos;
1089: 	result.last_position = iterator.pos.buffer_pos;
1090: }
1091: 
1092: void StringValueScanner::FinalizeChunkProcess() {
1093: 	if (result.number_of_rows >= result.result_size || iterator.done) {
1094: 		// We are done
1095: 		if (!sniffing) {
1096: 			if (csv_file_scan) {
1097: 				csv_file_scan->bytes_read += bytes_read;
1098: 				bytes_read = 0;
1099: 			}
1100: 		}
1101: 		return;
1102: 	}
1103: 	// If we are not done we have two options.
1104: 	// 1) If a boundary is set.
1105: 	if (iterator.IsBoundarySet()) {
1106: 		iterator.done = true;
1107: 		// We read until the next line or until we have nothing else to read.
1108: 		// Move to next buffer
1109: 		if (!cur_buffer_handle) {
1110: 			return;
1111: 		}
1112: 		bool moved = MoveToNextBuffer();
1113: 		if (cur_buffer_handle) {
1114: 			if (moved && result.cur_col_id < result.number_of_columns && result.cur_col_id > 0) {
1115: 				ProcessExtraRow();
1116: 			} else if (!moved) {
1117: 				ProcessExtraRow();
1118: 			}
1119: 			if (cur_buffer_handle->is_last_buffer && iterator.pos.buffer_pos >= cur_buffer_handle->actual_size) {
1120: 				MoveToNextBuffer();
1121: 			}
1122: 		}
1123: 	} else {
1124: 		// 2) If a boundary is not set
1125: 		// We read until the chunk is complete, or we have nothing else to read.
1126: 		while (!FinishedFile() && result.number_of_rows < result.result_size) {
1127: 			MoveToNextBuffer();
1128: 			if (result.number_of_rows >= result.result_size) {
1129: 				return;
1130: 			}
1131: 			if (cur_buffer_handle) {
1132: 				Process(result);
1133: 			}
1134: 		}
1135: 		iterator.done = FinishedFile();
1136: 		if (result.null_padding && result.number_of_rows < STANDARD_VECTOR_SIZE) {
1137: 			while (result.chunk_col_id < result.parse_chunk.ColumnCount()) {
1138: 				result.validity_mask[result.chunk_col_id++]->SetInvalid(result.number_of_rows);
1139: 				result.cur_col_id++;
1140: 			}
1141: 			result.number_of_rows++;
1142: 		}
1143: 	}
1144: }
1145: } // namespace duckdb
[end of src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: