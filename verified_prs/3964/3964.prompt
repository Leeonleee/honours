You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Write CSV with timestamp columns in ISO 8601 strings
Right now, if I write a CSV with either `COPY` or `EXPORT DATABASE` statements, the only format I can get for timestamp columns is not ISO-8601 compliant, e.g. `2021-02-01 13:00:00`.  I'd like to be able to tell DuckDB to write the ISO-8601 format `2021-02-01T13:00:00Z`, instead.

Documentation for both `COPY` AND `EXPORT DATABASE` suggest I might be able to set a `TIMESTAMPFORMAT` option. But when I try this, I'm told that it is not yet implemented: `Not implemented Error: Unrecognized option for CSV: TIMESTAMPFORMAT`.

I [chatted with `__Alex__` about this in Discord](https://discord.com/channels/909674491309850675/921073327009853451/989378834828767313), and he suggested I file this issue. Thanks!

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of src/execution/operator/persistent/buffered_csv_reader.cpp]
1: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/file_system.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/types/cast_helpers.hpp"
8: #include "duckdb/common/vector_operations/unary_executor.hpp"
9: #include "duckdb/common/vector_operations/vector_operations.hpp"
10: #include "duckdb/function/scalar/strftime.hpp"
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/parser/column_definition.hpp"
13: #include "duckdb/storage/data_table.hpp"
14: #include "utf8proc_wrapper.hpp"
15: #include "utf8proc.hpp"
16: #include "duckdb/parser/keyword_helper.hpp"
17: 
18: #include <algorithm>
19: #include <cctype>
20: #include <cstring>
21: #include <fstream>
22: 
23: namespace duckdb {
24: 
25: static bool ParseBoolean(const Value &value, const string &loption);
26: 
27: static bool ParseBoolean(const vector<Value> &set, const string &loption) {
28: 	if (set.empty()) {
29: 		// no option specified: default to true
30: 		return true;
31: 	}
32: 	if (set.size() > 1) {
33: 		throw BinderException("\"%s\" expects a single argument as a boolean value (e.g. TRUE or 1)", loption);
34: 	}
35: 	return ParseBoolean(set[0], loption);
36: }
37: 
38: static bool ParseBoolean(const Value &value, const string &loption) {
39: 
40: 	if (value.type().id() == LogicalTypeId::LIST) {
41: 		auto &children = ListValue::GetChildren(value);
42: 		return ParseBoolean(children, loption);
43: 	}
44: 	if (value.type() == LogicalType::FLOAT || value.type() == LogicalType::DOUBLE ||
45: 	    value.type().id() == LogicalTypeId::DECIMAL) {
46: 		throw BinderException("\"%s\" expects a boolean value (e.g. TRUE or 1)", loption);
47: 	}
48: 	return BooleanValue::Get(value.CastAs(LogicalType::BOOLEAN));
49: }
50: 
51: static string ParseString(const Value &value, const string &loption) {
52: 	if (value.type().id() == LogicalTypeId::LIST) {
53: 		auto &children = ListValue::GetChildren(value);
54: 		if (children.size() != 1) {
55: 			throw BinderException("\"%s\" expects a single argument as a string value", loption);
56: 		}
57: 		return ParseString(children[0], loption);
58: 	}
59: 	if (value.type().id() != LogicalTypeId::VARCHAR) {
60: 		throw BinderException("\"%s\" expects a string argument!", loption);
61: 	}
62: 	return value.GetValue<string>();
63: }
64: 
65: static int64_t ParseInteger(const Value &value, const string &loption) {
66: 	if (value.type().id() == LogicalTypeId::LIST) {
67: 		auto &children = ListValue::GetChildren(value);
68: 		if (children.size() != 1) {
69: 			// no option specified or multiple options specified
70: 			throw BinderException("\"%s\" expects a single argument as an integer value", loption);
71: 		}
72: 		return ParseInteger(children[0], loption);
73: 	}
74: 	return value.GetValue<int64_t>();
75: }
76: 
77: static vector<bool> ParseColumnList(const vector<Value> &set, vector<string> &names, const string &loption) {
78: 	vector<bool> result;
79: 
80: 	if (set.empty()) {
81: 		throw BinderException("\"%s\" expects a column list or * as parameter", loption);
82: 	}
83: 	// list of options: parse the list
84: 	unordered_map<string, bool> option_map;
85: 	for (idx_t i = 0; i < set.size(); i++) {
86: 		option_map[set[i].ToString()] = false;
87: 	}
88: 	result.resize(names.size(), false);
89: 	for (idx_t i = 0; i < names.size(); i++) {
90: 		auto entry = option_map.find(names[i]);
91: 		if (entry != option_map.end()) {
92: 			result[i] = true;
93: 			entry->second = true;
94: 		}
95: 	}
96: 	for (auto &entry : option_map) {
97: 		if (!entry.second) {
98: 			throw BinderException("\"%s\" expected to find %s, but it was not found in the table", loption,
99: 			                      entry.first.c_str());
100: 		}
101: 	}
102: 	return result;
103: }
104: 
105: static vector<bool> ParseColumnList(const Value &value, vector<string> &names, const string &loption) {
106: 	vector<bool> result;
107: 
108: 	// Only accept a list of arguments
109: 	if (value.type().id() != LogicalTypeId::LIST) {
110: 		// Support a single argument if it's '*'
111: 		if (value.type().id() == LogicalTypeId::VARCHAR && value.GetValue<string>() == "*") {
112: 			result.resize(names.size(), true);
113: 			return result;
114: 		}
115: 		throw BinderException("\"%s\" expects a column list or * as parameter", loption);
116: 	}
117: 	auto &children = ListValue::GetChildren(value);
118: 	// accept '*' as single argument
119: 	if (children.size() == 1 && children[0].type().id() == LogicalTypeId::VARCHAR &&
120: 	    children[0].GetValue<string>() == "*") {
121: 		result.resize(names.size(), true);
122: 		return result;
123: 	}
124: 	return ParseColumnList(children, names, loption);
125: }
126: 
127: struct CSVFileHandle {
128: public:
129: 	explicit CSVFileHandle(unique_ptr<FileHandle> file_handle_p) : file_handle(move(file_handle_p)) {
130: 		can_seek = file_handle->CanSeek();
131: 		plain_file_source = file_handle->OnDiskFile() && can_seek;
132: 		file_size = file_handle->GetFileSize();
133: 	}
134: 
135: 	bool CanSeek() {
136: 		return can_seek;
137: 	}
138: 	void Seek(idx_t position) {
139: 		if (!can_seek) {
140: 			throw InternalException("Cannot seek in this file");
141: 		}
142: 		file_handle->Seek(position);
143: 	}
144: 	idx_t SeekPosition() {
145: 		if (!can_seek) {
146: 			throw InternalException("Cannot seek in this file");
147: 		}
148: 		return file_handle->SeekPosition();
149: 	}
150: 	void Reset() {
151: 		if (plain_file_source) {
152: 			file_handle->Reset();
153: 		} else {
154: 			if (!reset_enabled) {
155: 				throw InternalException("Reset called but reset is not enabled for this CSV Handle");
156: 			}
157: 			read_position = 0;
158: 		}
159: 	}
160: 	bool PlainFileSource() {
161: 		return plain_file_source;
162: 	}
163: 
164: 	bool OnDiskFile() {
165: 		return file_handle->OnDiskFile();
166: 	}
167: 
168: 	idx_t FileSize() {
169: 		return file_size;
170: 	}
171: 
172: 	idx_t Read(void *buffer, idx_t nr_bytes) {
173: 		if (!plain_file_source) {
174: 			// not a plain file source: we need to do some bookkeeping around the reset functionality
175: 			idx_t result_offset = 0;
176: 			if (read_position < buffer_size) {
177: 				// we need to read from our cached buffer
178: 				auto buffer_read_count = MinValue<idx_t>(nr_bytes, buffer_size - read_position);
179: 				memcpy(buffer, cached_buffer.get() + read_position, buffer_read_count);
180: 				result_offset += buffer_read_count;
181: 				read_position += buffer_read_count;
182: 				if (result_offset == nr_bytes) {
183: 					return nr_bytes;
184: 				}
185: 			} else if (!reset_enabled && cached_buffer) {
186: 				// reset is disabled but we still have cached data
187: 				// we can remove any cached data
188: 				cached_buffer.reset();
189: 				buffer_size = 0;
190: 				buffer_capacity = 0;
191: 				read_position = 0;
192: 			}
193: 			// we have data left to read from the file
194: 			// read directly into the buffer
195: 			auto bytes_read = file_handle->Read((char *)buffer + result_offset, nr_bytes - result_offset);
196: 			read_position += bytes_read;
197: 			if (reset_enabled) {
198: 				// if reset caching is enabled, we need to cache the bytes that we have read
199: 				if (buffer_size + bytes_read >= buffer_capacity) {
200: 					// no space; first enlarge the buffer
201: 					buffer_capacity = MaxValue<idx_t>(NextPowerOfTwo(buffer_size + bytes_read), buffer_capacity * 2);
202: 
203: 					auto new_buffer = unique_ptr<data_t[]>(new data_t[buffer_capacity]);
204: 					if (buffer_size > 0) {
205: 						memcpy(new_buffer.get(), cached_buffer.get(), buffer_size);
206: 					}
207: 					cached_buffer = move(new_buffer);
208: 				}
209: 				memcpy(cached_buffer.get() + buffer_size, (char *)buffer + result_offset, bytes_read);
210: 				buffer_size += bytes_read;
211: 			}
212: 
213: 			return result_offset + bytes_read;
214: 		} else {
215: 			return file_handle->Read(buffer, nr_bytes);
216: 		}
217: 	}
218: 
219: 	string ReadLine() {
220: 		string result;
221: 		char buffer[1];
222: 		while (true) {
223: 			idx_t tuples_read = Read(buffer, 1);
224: 			if (tuples_read == 0 || buffer[0] == '\n') {
225: 				return result;
226: 			}
227: 			if (buffer[0] != '\r') {
228: 				result += buffer[0];
229: 			}
230: 		}
231: 	}
232: 
233: 	void DisableReset() {
234: 		this->reset_enabled = false;
235: 	}
236: 
237: private:
238: 	unique_ptr<FileHandle> file_handle;
239: 	bool reset_enabled = true;
240: 	bool can_seek = false;
241: 	bool plain_file_source = false;
242: 	idx_t file_size = 0;
243: 	// reset support
244: 	unique_ptr<data_t[]> cached_buffer;
245: 	idx_t read_position = 0;
246: 	idx_t buffer_size = 0;
247: 	idx_t buffer_capacity = 0;
248: };
249: 
250: void BufferedCSVReaderOptions::SetDelimiter(const string &input) {
251: 	this->delimiter = StringUtil::Replace(input, "\\t", "\t");
252: 	this->has_delimiter = true;
253: 	if (input.empty()) {
254: 		this->delimiter = string("\0", 1);
255: 	}
256: }
257: 
258: void BufferedCSVReaderOptions::SetReadOption(const string &loption, const Value &value,
259:                                              vector<string> &expected_names) {
260: 	if (SetBaseOption(loption, value)) {
261: 		return;
262: 	}
263: 	if (loption == "auto_detect") {
264: 		auto_detect = ParseBoolean(value, loption);
265: 	} else if (loption == "sample_size") {
266: 		int64_t sample_size = ParseInteger(value, loption);
267: 		if (sample_size < 1 && sample_size != -1) {
268: 			throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
269: 		}
270: 		if (sample_size == -1) {
271: 			sample_chunks = std::numeric_limits<uint64_t>::max();
272: 			sample_chunk_size = STANDARD_VECTOR_SIZE;
273: 		} else if (sample_size <= STANDARD_VECTOR_SIZE) {
274: 			sample_chunk_size = sample_size;
275: 			sample_chunks = 1;
276: 		} else {
277: 			sample_chunk_size = STANDARD_VECTOR_SIZE;
278: 			sample_chunks = sample_size / STANDARD_VECTOR_SIZE;
279: 		}
280: 	} else if (loption == "skip") {
281: 		skip_rows = ParseInteger(value, loption);
282: 	} else if (loption == "max_line_size" || loption == "maximum_line_size") {
283: 		maximum_line_size = ParseInteger(value, loption);
284: 	} else if (loption == "sample_chunk_size") {
285: 		sample_chunk_size = ParseInteger(value, loption);
286: 		if (sample_chunk_size > STANDARD_VECTOR_SIZE) {
287: 			throw BinderException(
288: 			    "Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
289: 			    STANDARD_VECTOR_SIZE);
290: 		} else if (sample_chunk_size < 1) {
291: 			throw BinderException("Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be smaller than 1");
292: 		}
293: 	} else if (loption == "sample_chunks") {
294: 		sample_chunks = ParseInteger(value, loption);
295: 		if (sample_chunks < 1) {
296: 			throw BinderException("Unsupported parameter for SAMPLE_CHUNKS: cannot be smaller than 1");
297: 		}
298: 	} else if (loption == "force_not_null") {
299: 		force_not_null = ParseColumnList(value, expected_names, loption);
300: 	} else if (loption == "date_format" || loption == "dateformat") {
301: 		string format = ParseString(value, loption);
302: 		auto &date_format = this->date_format[LogicalTypeId::DATE];
303: 		string error = StrTimeFormat::ParseFormatSpecifier(format, date_format);
304: 		date_format.format_specifier = format;
305: 		if (!error.empty()) {
306: 			throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
307: 		}
308: 		has_format[LogicalTypeId::DATE] = true;
309: 	} else if (loption == "timestamp_format" || loption == "timestampformat") {
310: 		string format = ParseString(value, loption);
311: 		auto &timestamp_format = date_format[LogicalTypeId::TIMESTAMP];
312: 		string error = StrTimeFormat::ParseFormatSpecifier(format, timestamp_format);
313: 		timestamp_format.format_specifier = format;
314: 		if (!error.empty()) {
315: 			throw InvalidInputException("Could not parse TIMESTAMPFORMAT: %s", error.c_str());
316: 		}
317: 		has_format[LogicalTypeId::TIMESTAMP] = true;
318: 	} else if (loption == "escape") {
319: 		escape = ParseString(value, loption);
320: 		has_escape = true;
321: 	} else if (loption == "ignore_errors") {
322: 		ignore_errors = ParseBoolean(value, loption);
323: 	} else {
324: 		throw BinderException("Unrecognized option for CSV reader \"%s\"", loption);
325: 	}
326: }
327: 
328: void BufferedCSVReaderOptions::SetWriteOption(const string &loption, const Value &value) {
329: 	if (SetBaseOption(loption, value)) {
330: 		return;
331: 	}
332: 
333: 	if (loption == "force_quote") {
334: 		force_quote = ParseColumnList(value, names, loption);
335: 	} else {
336: 		throw BinderException("Unrecognized option CSV writer \"%s\"", loption);
337: 	}
338: }
339: 
340: bool BufferedCSVReaderOptions::SetBaseOption(const string &loption, const Value &value) {
341: 	// Make sure this function was only called after the option was turned into lowercase
342: 	D_ASSERT(!std::any_of(loption.begin(), loption.end(), ::isupper));
343: 
344: 	if (StringUtil::StartsWith(loption, "delim") || StringUtil::StartsWith(loption, "sep")) {
345: 		SetDelimiter(ParseString(value, loption));
346: 	} else if (loption == "quote") {
347: 		quote = ParseString(value, loption);
348: 		has_quote = true;
349: 	} else if (loption == "escape") {
350: 		escape = ParseString(value, loption);
351: 		has_escape = true;
352: 	} else if (loption == "header") {
353: 		header = ParseBoolean(value, loption);
354: 		has_header = true;
355: 	} else if (loption == "null" || loption == "nullstr") {
356: 		null_str = ParseString(value, loption);
357: 	} else if (loption == "encoding") {
358: 		auto encoding = StringUtil::Lower(ParseString(value, loption));
359: 		if (encoding != "utf8" && encoding != "utf-8") {
360: 			throw BinderException("Copy is only supported for UTF-8 encoded files, ENCODING 'UTF-8'");
361: 		}
362: 	} else if (loption == "compression") {
363: 		compression = FileCompressionTypeFromString(ParseString(value, loption));
364: 	} else {
365: 		// unrecognized option in base CSV
366: 		return false;
367: 	}
368: 	return true;
369: }
370: 
371: std::string BufferedCSVReaderOptions::ToString() const {
372: 	return "DELIMITER='" + delimiter + (has_delimiter ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
373: 	       ", QUOTE='" + quote + (has_quote ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
374: 	       ", ESCAPE='" + escape + (has_escape ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
375: 	       ", HEADER=" + std::to_string(header) +
376: 	       (has_header ? "" : (auto_detect ? " (auto detected)" : "' (default)")) +
377: 	       ", SAMPLE_SIZE=" + std::to_string(sample_chunk_size * sample_chunks) +
378: 	       ", IGNORE_ERRORS=" + std::to_string(ignore_errors) + ", ALL_VARCHAR=" + std::to_string(all_varchar);
379: }
380: 
381: static string GetLineNumberStr(idx_t linenr, bool linenr_estimated) {
382: 	string estimated = (linenr_estimated ? string(" (estimated)") : string(""));
383: 	return to_string(linenr + 1) + estimated;
384: }
385: 
386: static bool StartsWithNumericDate(string &separator, const string &value) {
387: 	auto begin = value.c_str();
388: 	auto end = begin + value.size();
389: 
390: 	//	StrpTimeFormat::Parse will skip whitespace, so we can too
391: 	auto field1 = std::find_if_not(begin, end, StringUtil::CharacterIsSpace);
392: 	if (field1 == end) {
393: 		return false;
394: 	}
395: 
396: 	//	first numeric field must start immediately
397: 	if (!StringUtil::CharacterIsDigit(*field1)) {
398: 		return false;
399: 	}
400: 	auto literal1 = std::find_if_not(field1, end, StringUtil::CharacterIsDigit);
401: 	if (literal1 == end) {
402: 		return false;
403: 	}
404: 
405: 	//	second numeric field must exist
406: 	auto field2 = std::find_if(literal1, end, StringUtil::CharacterIsDigit);
407: 	if (field2 == end) {
408: 		return false;
409: 	}
410: 	auto literal2 = std::find_if_not(field2, end, StringUtil::CharacterIsDigit);
411: 	if (literal2 == end) {
412: 		return false;
413: 	}
414: 
415: 	//	third numeric field must exist
416: 	auto field3 = std::find_if(literal2, end, StringUtil::CharacterIsDigit);
417: 	if (field3 == end) {
418: 		return false;
419: 	}
420: 
421: 	//	second literal must match first
422: 	if (((field3 - literal2) != (field2 - literal1)) || strncmp(literal1, literal2, (field2 - literal1)) != 0) {
423: 		return false;
424: 	}
425: 
426: 	//	copy the literal as the separator, escaping percent signs
427: 	separator.clear();
428: 	while (literal1 < field2) {
429: 		const auto literal_char = *literal1++;
430: 		if (literal_char == '%') {
431: 			separator.push_back(literal_char);
432: 		}
433: 		separator.push_back(literal_char);
434: 	}
435: 
436: 	return true;
437: }
438: 
439: string GenerateDateFormat(const string &separator, const char *format_template) {
440: 	string format_specifier = format_template;
441: 
442: 	//	replace all dashes with the separator
443: 	for (auto pos = std::find(format_specifier.begin(), format_specifier.end(), '-'); pos != format_specifier.end();
444: 	     pos = std::find(pos + separator.size(), format_specifier.end(), '-')) {
445: 		format_specifier.replace(pos, pos + 1, separator);
446: 	}
447: 
448: 	return format_specifier;
449: }
450: 
451: TextSearchShiftArray::TextSearchShiftArray() {
452: }
453: 
454: TextSearchShiftArray::TextSearchShiftArray(string search_term) : length(search_term.size()) {
455: 	if (length > 255) {
456: 		throw Exception("Size of delimiter/quote/escape in CSV reader is limited to 255 bytes");
457: 	}
458: 	// initialize the shifts array
459: 	shifts = unique_ptr<uint8_t[]>(new uint8_t[length * 255]);
460: 	memset(shifts.get(), 0, length * 255 * sizeof(uint8_t));
461: 	// iterate over each of the characters in the array
462: 	for (idx_t main_idx = 0; main_idx < length; main_idx++) {
463: 		uint8_t current_char = (uint8_t)search_term[main_idx];
464: 		// now move over all the remaining positions
465: 		for (idx_t i = main_idx; i < length; i++) {
466: 			bool is_match = true;
467: 			// check if the prefix matches at this position
468: 			// if it does, we move to this position after encountering the current character
469: 			for (idx_t j = 0; j < main_idx; j++) {
470: 				if (search_term[i - main_idx + j] != search_term[j]) {
471: 					is_match = false;
472: 				}
473: 			}
474: 			if (!is_match) {
475: 				continue;
476: 			}
477: 			shifts[i * 255 + current_char] = main_idx + 1;
478: 		}
479: 	}
480: }
481: 
482: BufferedCSVReader::BufferedCSVReader(FileSystem &fs_p, FileOpener *opener_p, BufferedCSVReaderOptions options_p,
483:                                      const vector<LogicalType> &requested_types)
484:     : fs(fs_p), opener(opener_p), options(move(options_p)), buffer_size(0), position(0), start(0) {
485: 	file_handle = OpenCSV(options);
486: 	Initialize(requested_types);
487: }
488: 
489: BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options_p,
490:                                      const vector<LogicalType> &requested_types)
491:     : BufferedCSVReader(FileSystem::GetFileSystem(context), FileSystem::GetFileOpener(context), move(options_p),
492:                         requested_types) {
493: }
494: 
495: BufferedCSVReader::~BufferedCSVReader() {
496: }
497: 
498: idx_t BufferedCSVReader::GetFileSize() {
499: 	return file_handle ? file_handle->FileSize() : 0;
500: }
501: 
502: void BufferedCSVReader::Initialize(const vector<LogicalType> &requested_types) {
503: 	PrepareComplexParser();
504: 	if (options.auto_detect) {
505: 		sql_types = SniffCSV(requested_types);
506: 		if (sql_types.empty()) {
507: 			throw Exception("Failed to detect column types from CSV: is the file a valid CSV file?");
508: 		}
509: 		if (cached_chunks.empty()) {
510: 			JumpToBeginning(options.skip_rows, options.header);
511: 		}
512: 	} else {
513: 		sql_types = requested_types;
514: 		ResetBuffer();
515: 		SkipRowsAndReadHeader(options.skip_rows, options.header);
516: 	}
517: 	InitParseChunk(sql_types.size());
518: 	// we only need reset support during the automatic CSV type detection
519: 	// since reset support might require caching (in the case of streams), we disable it for the remainder
520: 	file_handle->DisableReset();
521: }
522: 
523: void BufferedCSVReader::PrepareComplexParser() {
524: 	delimiter_search = TextSearchShiftArray(options.delimiter);
525: 	escape_search = TextSearchShiftArray(options.escape);
526: 	quote_search = TextSearchShiftArray(options.quote);
527: }
528: 
529: unique_ptr<CSVFileHandle> BufferedCSVReader::OpenCSV(const BufferedCSVReaderOptions &options) {
530: 	auto file_handle = fs.OpenFile(options.file_path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK,
531: 	                               options.compression, this->opener);
532: 	return make_unique<CSVFileHandle>(move(file_handle));
533: }
534: 
535: // Helper function to generate column names
536: static string GenerateColumnName(const idx_t total_cols, const idx_t col_number, const string &prefix = "column") {
537: 	int max_digits = NumericHelper::UnsignedLength(total_cols - 1);
538: 	int digits = NumericHelper::UnsignedLength(col_number);
539: 	string leading_zeros = string(max_digits - digits, '0');
540: 	string value = to_string(col_number);
541: 	return string(prefix + leading_zeros + value);
542: }
543: 
544: // Helper function for UTF-8 aware space trimming
545: static string TrimWhitespace(const string &col_name) {
546: 	utf8proc_int32_t codepoint;
547: 	auto str = reinterpret_cast<const utf8proc_uint8_t *>(col_name.c_str());
548: 	idx_t size = col_name.size();
549: 	// Find the first character that is not left trimmed
550: 	idx_t begin = 0;
551: 	while (begin < size) {
552: 		auto bytes = utf8proc_iterate(str + begin, size - begin, &codepoint);
553: 		D_ASSERT(bytes > 0);
554: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
555: 			break;
556: 		}
557: 		begin += bytes;
558: 	}
559: 
560: 	// Find the last character that is not right trimmed
561: 	idx_t end;
562: 	end = begin;
563: 	for (auto next = begin; next < col_name.size();) {
564: 		auto bytes = utf8proc_iterate(str + next, size - next, &codepoint);
565: 		D_ASSERT(bytes > 0);
566: 		next += bytes;
567: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
568: 			end = next;
569: 		}
570: 	}
571: 
572: 	// return the trimmed string
573: 	return col_name.substr(begin, end - begin);
574: }
575: 
576: static string NormalizeColumnName(const string &col_name) {
577: 	// normalize UTF8 characters to NFKD
578: 	auto nfkd = utf8proc_NFKD((const utf8proc_uint8_t *)col_name.c_str(), col_name.size());
579: 	const string col_name_nfkd = string((const char *)nfkd, strlen((const char *)nfkd));
580: 	free(nfkd);
581: 
582: 	// only keep ASCII characters 0-9 a-z A-Z and replace spaces with regular whitespace
583: 	string col_name_ascii = "";
584: 	for (idx_t i = 0; i < col_name_nfkd.size(); i++) {
585: 		if (col_name_nfkd[i] == '_' || (col_name_nfkd[i] >= '0' && col_name_nfkd[i] <= '9') ||
586: 		    (col_name_nfkd[i] >= 'A' && col_name_nfkd[i] <= 'Z') ||
587: 		    (col_name_nfkd[i] >= 'a' && col_name_nfkd[i] <= 'z')) {
588: 			col_name_ascii += col_name_nfkd[i];
589: 		} else if (StringUtil::CharacterIsSpace(col_name_nfkd[i])) {
590: 			col_name_ascii += " ";
591: 		}
592: 	}
593: 
594: 	// trim whitespace and replace remaining whitespace by _
595: 	string col_name_trimmed = TrimWhitespace(col_name_ascii);
596: 	string col_name_cleaned = "";
597: 	bool in_whitespace = false;
598: 	for (idx_t i = 0; i < col_name_trimmed.size(); i++) {
599: 		if (col_name_trimmed[i] == ' ') {
600: 			if (!in_whitespace) {
601: 				col_name_cleaned += "_";
602: 				in_whitespace = true;
603: 			}
604: 		} else {
605: 			col_name_cleaned += col_name_trimmed[i];
606: 			in_whitespace = false;
607: 		}
608: 	}
609: 
610: 	// don't leave string empty; if not empty, make lowercase
611: 	if (col_name_cleaned.empty()) {
612: 		col_name_cleaned = "_";
613: 	} else {
614: 		col_name_cleaned = StringUtil::Lower(col_name_cleaned);
615: 	}
616: 
617: 	// prepend _ if name starts with a digit or is a reserved keyword
618: 	if (KeywordHelper::IsKeyword(col_name_cleaned) || (col_name_cleaned[0] >= '0' && col_name_cleaned[0] <= '9')) {
619: 		col_name_cleaned = "_" + col_name_cleaned;
620: 	}
621: 	return col_name_cleaned;
622: }
623: 
624: void BufferedCSVReader::ResetBuffer() {
625: 	buffer.reset();
626: 	buffer_size = 0;
627: 	position = 0;
628: 	start = 0;
629: 	cached_buffers.clear();
630: }
631: 
632: void BufferedCSVReader::ResetStream() {
633: 	if (!file_handle->CanSeek()) {
634: 		// seeking to the beginning appears to not be supported in all compiler/os-scenarios,
635: 		// so we have to create a new stream source here for now
636: 		file_handle->Reset();
637: 	} else {
638: 		file_handle->Seek(0);
639: 	}
640: 	linenr = 0;
641: 	linenr_estimated = false;
642: 	bytes_per_line_avg = 0;
643: 	sample_chunk_idx = 0;
644: 	jumping_samples = false;
645: }
646: 
647: void BufferedCSVReader::InitParseChunk(idx_t num_cols) {
648: 	// adapt not null info
649: 	if (options.force_not_null.size() != num_cols) {
650: 		options.force_not_null.resize(num_cols, false);
651: 	}
652: 	if (num_cols == parse_chunk.ColumnCount()) {
653: 		parse_chunk.Reset();
654: 	} else {
655: 		parse_chunk.Destroy();
656: 
657: 		// initialize the parse_chunk with a set of VARCHAR types
658: 		vector<LogicalType> varchar_types(num_cols, LogicalType::VARCHAR);
659: 		parse_chunk.Initialize(varchar_types);
660: 	}
661: }
662: 
663: void BufferedCSVReader::JumpToBeginning(idx_t skip_rows = 0, bool skip_header = false) {
664: 	ResetBuffer();
665: 	ResetStream();
666: 	sample_chunk_idx = 0;
667: 	bytes_in_chunk = 0;
668: 	end_of_file_reached = false;
669: 	bom_checked = false;
670: 	SkipRowsAndReadHeader(skip_rows, skip_header);
671: }
672: 
673: void BufferedCSVReader::SkipRowsAndReadHeader(idx_t skip_rows, bool skip_header) {
674: 	for (idx_t i = 0; i < skip_rows; i++) {
675: 		// ignore skip rows
676: 		string read_line = file_handle->ReadLine();
677: 		linenr++;
678: 	}
679: 
680: 	if (skip_header) {
681: 		// ignore the first line as a header line
682: 		InitParseChunk(sql_types.size());
683: 		ParseCSV(ParserMode::PARSING_HEADER);
684: 	}
685: }
686: 
687: bool BufferedCSVReader::JumpToNextSample() {
688: 	// get bytes contained in the previously read chunk
689: 	idx_t remaining_bytes_in_buffer = buffer_size - start;
690: 	bytes_in_chunk -= remaining_bytes_in_buffer;
691: 	if (remaining_bytes_in_buffer == 0) {
692: 		return false;
693: 	}
694: 
695: 	// assess if it makes sense to jump, based on size of the first chunk relative to size of the entire file
696: 	if (sample_chunk_idx == 0) {
697: 		idx_t bytes_first_chunk = bytes_in_chunk;
698: 		double chunks_fit = (file_handle->FileSize() / (double)bytes_first_chunk);
699: 		jumping_samples = chunks_fit >= options.sample_chunks;
700: 
701: 		// jump back to the beginning
702: 		JumpToBeginning(options.skip_rows, options.header);
703: 		sample_chunk_idx++;
704: 		return true;
705: 	}
706: 
707: 	if (end_of_file_reached || sample_chunk_idx >= options.sample_chunks) {
708: 		return false;
709: 	}
710: 
711: 	// if we deal with any other sources than plaintext files, jumping_samples can be tricky. In that case
712: 	// we just read x continuous chunks from the stream TODO: make jumps possible for zipfiles.
713: 	if (!file_handle->PlainFileSource() || !jumping_samples) {
714: 		sample_chunk_idx++;
715: 		return true;
716: 	}
717: 
718: 	// update average bytes per line
719: 	double bytes_per_line = bytes_in_chunk / (double)options.sample_chunk_size;
720: 	bytes_per_line_avg = ((bytes_per_line_avg * (sample_chunk_idx)) + bytes_per_line) / (sample_chunk_idx + 1);
721: 
722: 	// if none of the previous conditions were met, we can jump
723: 	idx_t partition_size = (idx_t)round(file_handle->FileSize() / (double)options.sample_chunks);
724: 
725: 	// calculate offset to end of the current partition
726: 	int64_t offset = partition_size - bytes_in_chunk - remaining_bytes_in_buffer;
727: 	auto current_pos = file_handle->SeekPosition();
728: 
729: 	if (current_pos + offset < file_handle->FileSize()) {
730: 		// set position in stream and clear failure bits
731: 		file_handle->Seek(current_pos + offset);
732: 
733: 		// estimate linenr
734: 		linenr += (idx_t)round((offset + remaining_bytes_in_buffer) / bytes_per_line_avg);
735: 		linenr_estimated = true;
736: 	} else {
737: 		// seek backwards from the end in last chunk and hope to catch the end of the file
738: 		// TODO: actually it would be good to make sure that the end of file is being reached, because
739: 		// messy end-lines are quite common. For this case, however, we first need a skip_end detection anyways.
740: 		file_handle->Seek(file_handle->FileSize() - bytes_in_chunk);
741: 
742: 		// estimate linenr
743: 		linenr = (idx_t)round((file_handle->FileSize() - bytes_in_chunk) / bytes_per_line_avg);
744: 		linenr_estimated = true;
745: 	}
746: 
747: 	// reset buffers and parse chunk
748: 	ResetBuffer();
749: 
750: 	// seek beginning of next line
751: 	// FIXME: if this jump ends up in a quoted linebreak, we will have a problem
752: 	string read_line = file_handle->ReadLine();
753: 	linenr++;
754: 
755: 	sample_chunk_idx++;
756: 
757: 	return true;
758: }
759: 
760: void BufferedCSVReader::SetDateFormat(const string &format_specifier, const LogicalTypeId &sql_type) {
761: 	options.has_format[sql_type] = true;
762: 	auto &date_format = options.date_format[sql_type];
763: 	date_format.format_specifier = format_specifier;
764: 	StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
765: }
766: 
767: bool BufferedCSVReader::TryCastValue(const Value &value, const LogicalType &sql_type) {
768: 	if (options.has_format[LogicalTypeId::DATE] && sql_type.id() == LogicalTypeId::DATE) {
769: 		date_t result;
770: 		string error_message;
771: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(string_t(StringValue::Get(value)), result,
772: 		                                                             error_message);
773: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type.id() == LogicalTypeId::TIMESTAMP) {
774: 		timestamp_t result;
775: 		string error_message;
776: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(string_t(StringValue::Get(value)),
777: 		                                                                       result, error_message);
778: 	} else {
779: 		Value new_value;
780: 		string error_message;
781: 		return value.TryCastAs(sql_type, new_value, &error_message, true);
782: 	}
783: }
784: 
785: struct TryCastDateOperator {
786: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, date_t &result, string &error_message) {
787: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(input, result, error_message);
788: 	}
789: };
790: 
791: struct TryCastTimestampOperator {
792: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, timestamp_t &result,
793: 	                      string &error_message) {
794: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(input, result, error_message);
795: 	}
796: };
797: 
798: template <class OP, class T>
799: static bool TemplatedTryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector,
800:                                        idx_t count, string &error_message) {
801: 	D_ASSERT(input_vector.GetType().id() == LogicalTypeId::VARCHAR);
802: 	bool all_converted = true;
803: 	UnaryExecutor::Execute<string_t, T>(input_vector, result_vector, count, [&](string_t input) {
804: 		T result;
805: 		if (!OP::Operation(options, input, result, error_message)) {
806: 			all_converted = false;
807: 		}
808: 		return result;
809: 	});
810: 	return all_converted;
811: }
812: 
813: bool TryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
814:                        string &error_message) {
815: 	return TemplatedTryCastDateVector<TryCastDateOperator, date_t>(options, input_vector, result_vector, count,
816: 	                                                               error_message);
817: }
818: 
819: bool TryCastTimestampVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
820:                             string &error_message) {
821: 	return TemplatedTryCastDateVector<TryCastTimestampOperator, timestamp_t>(options, input_vector, result_vector,
822: 	                                                                         count, error_message);
823: }
824: 
825: bool BufferedCSVReader::TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type) {
826: 	// try vector-cast from string to sql_type
827: 	Vector dummy_result(sql_type);
828: 	if (options.has_format[LogicalTypeId::DATE] && sql_type == LogicalTypeId::DATE) {
829: 		// use the date format to cast the chunk
830: 		string error_message;
831: 		return TryCastDateVector(options, parse_chunk_col, dummy_result, size, error_message);
832: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type == LogicalTypeId::TIMESTAMP) {
833: 		// use the timestamp format to cast the chunk
834: 		string error_message;
835: 		return TryCastTimestampVector(options, parse_chunk_col, dummy_result, size, error_message);
836: 	} else {
837: 		// target type is not varchar: perform a cast
838: 		string error_message;
839: 		return VectorOperations::TryCast(parse_chunk_col, dummy_result, size, &error_message, true);
840: 	}
841: }
842: 
843: enum class QuoteRule : uint8_t { QUOTES_RFC = 0, QUOTES_OTHER = 1, NO_QUOTES = 2 };
844: 
845: void BufferedCSVReader::DetectDialect(const vector<LogicalType> &requested_types,
846:                                       BufferedCSVReaderOptions &original_options,
847:                                       vector<BufferedCSVReaderOptions> &info_candidates, idx_t &best_num_cols) {
848: 	// set up the candidates we consider for delimiter and quote rules based on user input
849: 	vector<string> delim_candidates;
850: 	vector<QuoteRule> quoterule_candidates;
851: 	vector<vector<string>> quote_candidates_map;
852: 	vector<vector<string>> escape_candidates_map = {{""}, {"\\"}, {""}};
853: 
854: 	if (options.has_delimiter) {
855: 		// user provided a delimiter: use that delimiter
856: 		delim_candidates = {options.delimiter};
857: 	} else {
858: 		// no delimiter provided: try standard/common delimiters
859: 		delim_candidates = {",", "|", ";", "\t"};
860: 	}
861: 	if (options.has_quote) {
862: 		// user provided quote: use that quote rule
863: 		quote_candidates_map = {{options.quote}, {options.quote}, {options.quote}};
864: 	} else {
865: 		// no quote rule provided: use standard/common quotes
866: 		quote_candidates_map = {{"\""}, {"\"", "'"}, {""}};
867: 	}
868: 	if (options.has_escape) {
869: 		// user provided escape: use that escape rule
870: 		if (options.escape.empty()) {
871: 			quoterule_candidates = {QuoteRule::QUOTES_RFC};
872: 		} else {
873: 			quoterule_candidates = {QuoteRule::QUOTES_OTHER};
874: 		}
875: 		escape_candidates_map[static_cast<uint8_t>(quoterule_candidates[0])] = {options.escape};
876: 	} else {
877: 		// no escape provided: try standard/common escapes
878: 		quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER, QuoteRule::NO_QUOTES};
879: 	}
880: 
881: 	idx_t best_consistent_rows = 0;
882: 	for (auto quoterule : quoterule_candidates) {
883: 		const auto &quote_candidates = quote_candidates_map[static_cast<uint8_t>(quoterule)];
884: 		for (const auto &quote : quote_candidates) {
885: 			for (const auto &delim : delim_candidates) {
886: 				const auto &escape_candidates = escape_candidates_map[static_cast<uint8_t>(quoterule)];
887: 				for (const auto &escape : escape_candidates) {
888: 					BufferedCSVReaderOptions sniff_info = original_options;
889: 					sniff_info.delimiter = delim;
890: 					sniff_info.quote = quote;
891: 					sniff_info.escape = escape;
892: 
893: 					options = sniff_info;
894: 					PrepareComplexParser();
895: 
896: 					JumpToBeginning(original_options.skip_rows);
897: 					sniffed_column_counts.clear();
898: 
899: 					if (!TryParseCSV(ParserMode::SNIFFING_DIALECT)) {
900: 						continue;
901: 					}
902: 
903: 					idx_t start_row = original_options.skip_rows;
904: 					idx_t consistent_rows = 0;
905: 					idx_t num_cols = 0;
906: 
907: 					for (idx_t row = 0; row < sniffed_column_counts.size(); row++) {
908: 						if (sniffed_column_counts[row] == num_cols) {
909: 							consistent_rows++;
910: 						} else {
911: 							num_cols = sniffed_column_counts[row];
912: 							start_row = row + original_options.skip_rows;
913: 							consistent_rows = 1;
914: 						}
915: 					}
916: 
917: 					// some logic
918: 					bool more_values = (consistent_rows > best_consistent_rows && num_cols >= best_num_cols);
919: 					bool single_column_before = best_num_cols < 2 && num_cols > best_num_cols;
920: 					bool rows_consistent =
921: 					    start_row + consistent_rows - original_options.skip_rows == sniffed_column_counts.size();
922: 					bool more_than_one_row = (consistent_rows > 1);
923: 					bool more_than_one_column = (num_cols > 1);
924: 					bool start_good = !info_candidates.empty() && (start_row <= info_candidates.front().skip_rows);
925: 
926: 					if (!requested_types.empty() && requested_types.size() != num_cols) {
927: 						continue;
928: 					} else if ((more_values || single_column_before) && rows_consistent) {
929: 						sniff_info.skip_rows = start_row;
930: 						sniff_info.num_cols = num_cols;
931: 						best_consistent_rows = consistent_rows;
932: 						best_num_cols = num_cols;
933: 
934: 						info_candidates.clear();
935: 						info_candidates.push_back(sniff_info);
936: 					} else if (more_than_one_row && more_than_one_column && start_good && rows_consistent) {
937: 						bool same_quote_is_candidate = false;
938: 						for (auto &info_candidate : info_candidates) {
939: 							if (quote.compare(info_candidate.quote) == 0) {
940: 								same_quote_is_candidate = true;
941: 							}
942: 						}
943: 						if (!same_quote_is_candidate) {
944: 							sniff_info.skip_rows = start_row;
945: 							sniff_info.num_cols = num_cols;
946: 							info_candidates.push_back(sniff_info);
947: 						}
948: 					}
949: 				}
950: 			}
951: 		}
952: 	}
953: }
954: 
955: void BufferedCSVReader::DetectCandidateTypes(const vector<LogicalType> &type_candidates,
956:                                              const map<LogicalTypeId, vector<const char *>> &format_template_candidates,
957:                                              const vector<BufferedCSVReaderOptions> &info_candidates,
958:                                              BufferedCSVReaderOptions &original_options, idx_t best_num_cols,
959:                                              vector<vector<LogicalType>> &best_sql_types_candidates,
960:                                              std::map<LogicalTypeId, vector<string>> &best_format_candidates,
961:                                              DataChunk &best_header_row) {
962: 	BufferedCSVReaderOptions best_options;
963: 	idx_t min_varchar_cols = best_num_cols + 1;
964: 
965: 	// check which info candidate leads to minimum amount of non-varchar columns...
966: 	for (const auto &t : format_template_candidates) {
967: 		best_format_candidates[t.first].clear();
968: 	}
969: 	for (auto &info_candidate : info_candidates) {
970: 		options = info_candidate;
971: 		vector<vector<LogicalType>> info_sql_types_candidates(options.num_cols, type_candidates);
972: 		std::map<LogicalTypeId, bool> has_format_candidates;
973: 		std::map<LogicalTypeId, vector<string>> format_candidates;
974: 		for (const auto &t : format_template_candidates) {
975: 			has_format_candidates[t.first] = false;
976: 			format_candidates[t.first].clear();
977: 		}
978: 
979: 		// set all sql_types to VARCHAR so we can do datatype detection based on VARCHAR values
980: 		sql_types.clear();
981: 		sql_types.assign(options.num_cols, LogicalType::VARCHAR);
982: 
983: 		// jump to beginning and skip potential header
984: 		JumpToBeginning(options.skip_rows, true);
985: 		DataChunk header_row;
986: 		header_row.Initialize(sql_types);
987: 		parse_chunk.Copy(header_row);
988: 
989: 		if (header_row.size() == 0) {
990: 			continue;
991: 		}
992: 
993: 		// init parse chunk and read csv with info candidate
994: 		InitParseChunk(sql_types.size());
995: 		ParseCSV(ParserMode::SNIFFING_DATATYPES);
996: 		for (idx_t row_idx = 0; row_idx <= parse_chunk.size(); row_idx++) {
997: 			bool is_header_row = row_idx == 0;
998: 			idx_t row = row_idx - 1;
999: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
1000: 				auto &col_type_candidates = info_sql_types_candidates[col];
1001: 				while (col_type_candidates.size() > 1) {
1002: 					const auto &sql_type = col_type_candidates.back();
1003: 					// try cast from string to sql_type
1004: 					Value dummy_val;
1005: 					if (is_header_row) {
1006: 						dummy_val = header_row.GetValue(col, 0);
1007: 					} else {
1008: 						dummy_val = parse_chunk.GetValue(col, row);
1009: 					}
1010: 					// try formatting for date types if the user did not specify one and it starts with numeric values.
1011: 					string separator;
1012: 					if (has_format_candidates.count(sql_type.id()) && !original_options.has_format[sql_type.id()] &&
1013: 					    StartsWithNumericDate(separator, StringValue::Get(dummy_val))) {
1014: 						// generate date format candidates the first time through
1015: 						auto &type_format_candidates = format_candidates[sql_type.id()];
1016: 						const auto had_format_candidates = has_format_candidates[sql_type.id()];
1017: 						if (!has_format_candidates[sql_type.id()]) {
1018: 							has_format_candidates[sql_type.id()] = true;
1019: 							// order by preference
1020: 							auto entry = format_template_candidates.find(sql_type.id());
1021: 							if (entry != format_template_candidates.end()) {
1022: 								const auto &format_template_list = entry->second;
1023: 								for (const auto &t : format_template_list) {
1024: 									const auto format_string = GenerateDateFormat(separator, t);
1025: 									// don't parse ISO 8601
1026: 									if (format_string.find("%Y-%m-%d") == string::npos) {
1027: 										type_format_candidates.emplace_back(format_string);
1028: 									}
1029: 								}
1030: 							}
1031: 							//	initialise the first candidate
1032: 							options.has_format[sql_type.id()] = true;
1033: 							//	all formats are constructed to be valid
1034: 							SetDateFormat(type_format_candidates.back(), sql_type.id());
1035: 						}
1036: 						// check all formats and keep the first one that works
1037: 						StrpTimeFormat::ParseResult result;
1038: 						auto save_format_candidates = type_format_candidates;
1039: 						while (!type_format_candidates.empty()) {
1040: 							//	avoid using exceptions for flow control...
1041: 							auto &current_format = options.date_format[sql_type.id()];
1042: 							if (current_format.Parse(StringValue::Get(dummy_val), result)) {
1043: 								break;
1044: 							}
1045: 							//	doesn't work - move to the next one
1046: 							type_format_candidates.pop_back();
1047: 							options.has_format[sql_type.id()] = (!type_format_candidates.empty());
1048: 							if (!type_format_candidates.empty()) {
1049: 								SetDateFormat(type_format_candidates.back(), sql_type.id());
1050: 							}
1051: 						}
1052: 						//	if none match, then this is not a value of type sql_type,
1053: 						if (type_format_candidates.empty()) {
1054: 							//	so restore the candidates that did work.
1055: 							//	or throw them out if they were generated by this value.
1056: 							if (had_format_candidates) {
1057: 								type_format_candidates.swap(save_format_candidates);
1058: 								if (!type_format_candidates.empty()) {
1059: 									SetDateFormat(type_format_candidates.back(), sql_type.id());
1060: 								}
1061: 							} else {
1062: 								has_format_candidates[sql_type.id()] = false;
1063: 							}
1064: 						}
1065: 					}
1066: 					// try cast from string to sql_type
1067: 					if (TryCastValue(dummy_val, sql_type)) {
1068: 						break;
1069: 					} else {
1070: 						col_type_candidates.pop_back();
1071: 					}
1072: 				}
1073: 			}
1074: 			// reset type detection, because first row could be header,
1075: 			// but only do it if csv has more than one line (including header)
1076: 			if (parse_chunk.size() > 0 && is_header_row) {
1077: 				info_sql_types_candidates = vector<vector<LogicalType>>(options.num_cols, type_candidates);
1078: 				for (auto &f : format_candidates) {
1079: 					f.second.clear();
1080: 				}
1081: 				for (auto &h : has_format_candidates) {
1082: 					h.second = false;
1083: 				}
1084: 			}
1085: 		}
1086: 
1087: 		idx_t varchar_cols = 0;
1088: 		for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
1089: 			auto &col_type_candidates = info_sql_types_candidates[col];
1090: 			// check number of varchar columns
1091: 			const auto &col_type = col_type_candidates.back();
1092: 			if (col_type == LogicalType::VARCHAR) {
1093: 				varchar_cols++;
1094: 			}
1095: 		}
1096: 
1097: 		// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 30% of best_num_cols.
1098: 		if (varchar_cols < min_varchar_cols && parse_chunk.ColumnCount() > (best_num_cols * 0.7)) {
1099: 			// we have a new best_options candidate
1100: 			best_options = info_candidate;
1101: 			min_varchar_cols = varchar_cols;
1102: 			best_sql_types_candidates = info_sql_types_candidates;
1103: 			best_format_candidates = format_candidates;
1104: 			best_header_row.Destroy();
1105: 			auto header_row_types = header_row.GetTypes();
1106: 			best_header_row.Initialize(header_row_types);
1107: 			header_row.Copy(best_header_row);
1108: 		}
1109: 	}
1110: 
1111: 	options = best_options;
1112: 	for (const auto &best : best_format_candidates) {
1113: 		if (!best.second.empty()) {
1114: 			SetDateFormat(best.second.back(), best.first);
1115: 		}
1116: 	}
1117: }
1118: 
1119: void BufferedCSVReader::DetectHeader(const vector<vector<LogicalType>> &best_sql_types_candidates,
1120:                                      const DataChunk &best_header_row) {
1121: 	// information for header detection
1122: 	bool first_row_consistent = true;
1123: 	bool first_row_nulls = false;
1124: 
1125: 	// check if header row is all null and/or consistent with detected column data types
1126: 	first_row_nulls = true;
1127: 	for (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {
1128: 		auto dummy_val = best_header_row.GetValue(col, 0);
1129: 		if (!dummy_val.IsNull()) {
1130: 			first_row_nulls = false;
1131: 		}
1132: 
1133: 		// try cast to sql_type of column
1134: 		const auto &sql_type = best_sql_types_candidates[col].back();
1135: 		if (!TryCastValue(dummy_val, sql_type)) {
1136: 			first_row_consistent = false;
1137: 		}
1138: 	}
1139: 
1140: 	// update parser info, and read, generate & set col_names based on previous findings
1141: 	if (((!first_row_consistent || first_row_nulls) && !options.has_header) || (options.has_header && options.header)) {
1142: 		options.header = true;
1143: 		case_insensitive_map_t<idx_t> name_collision_count;
1144: 		// get header names from CSV
1145: 		for (idx_t col = 0; col < options.num_cols; col++) {
1146: 			const auto &val = best_header_row.GetValue(col, 0);
1147: 			string col_name = val.ToString();
1148: 
1149: 			// generate name if field is empty
1150: 			if (col_name.empty() || val.IsNull()) {
1151: 				col_name = GenerateColumnName(options.num_cols, col);
1152: 			}
1153: 
1154: 			// normalize names or at least trim whitespace
1155: 			if (options.normalize_names) {
1156: 				col_name = NormalizeColumnName(col_name);
1157: 			} else {
1158: 				col_name = TrimWhitespace(col_name);
1159: 			}
1160: 
1161: 			// avoid duplicate header names
1162: 			const string col_name_raw = col_name;
1163: 			while (name_collision_count.find(col_name) != name_collision_count.end()) {
1164: 				name_collision_count[col_name] += 1;
1165: 				col_name = col_name + "_" + to_string(name_collision_count[col_name]);
1166: 			}
1167: 
1168: 			col_names.push_back(col_name);
1169: 			name_collision_count[col_name] = 0;
1170: 		}
1171: 
1172: 	} else {
1173: 		options.header = false;
1174: 		for (idx_t col = 0; col < options.num_cols; col++) {
1175: 			string column_name = GenerateColumnName(options.num_cols, col);
1176: 			col_names.push_back(column_name);
1177: 		}
1178: 	}
1179: }
1180: 
1181: vector<LogicalType> BufferedCSVReader::RefineTypeDetection(const vector<LogicalType> &type_candidates,
1182:                                                            const vector<LogicalType> &requested_types,
1183:                                                            vector<vector<LogicalType>> &best_sql_types_candidates,
1184:                                                            map<LogicalTypeId, vector<string>> &best_format_candidates) {
1185: 	// for the type refine we set the SQL types to VARCHAR for all columns
1186: 	sql_types.clear();
1187: 	sql_types.assign(options.num_cols, LogicalType::VARCHAR);
1188: 
1189: 	vector<LogicalType> detected_types;
1190: 
1191: 	// if data types were provided, exit here if number of columns does not match
1192: 	if (!requested_types.empty()) {
1193: 		if (requested_types.size() != options.num_cols) {
1194: 			throw InvalidInputException(
1195: 			    "Error while determining column types: found %lld columns but expected %d. (%s)", options.num_cols,
1196: 			    requested_types.size(), options.ToString());
1197: 		} else {
1198: 			detected_types = requested_types;
1199: 		}
1200: 	} else if (options.all_varchar) {
1201: 		// return all types varchar
1202: 		detected_types = sql_types;
1203: 	} else {
1204: 		// jump through the rest of the file and continue to refine the sql type guess
1205: 		while (JumpToNextSample()) {
1206: 			InitParseChunk(sql_types.size());
1207: 			// if jump ends up a bad line, we just skip this chunk
1208: 			if (!TryParseCSV(ParserMode::SNIFFING_DATATYPES)) {
1209: 				continue;
1210: 			}
1211: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
1212: 				vector<LogicalType> &col_type_candidates = best_sql_types_candidates[col];
1213: 				while (col_type_candidates.size() > 1) {
1214: 					const auto &sql_type = col_type_candidates.back();
1215: 					//	narrow down the date formats
1216: 					if (best_format_candidates.count(sql_type.id())) {
1217: 						auto &best_type_format_candidates = best_format_candidates[sql_type.id()];
1218: 						auto save_format_candidates = best_type_format_candidates;
1219: 						while (!best_type_format_candidates.empty()) {
1220: 							if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
1221: 								break;
1222: 							}
1223: 							//	doesn't work - move to the next one
1224: 							best_type_format_candidates.pop_back();
1225: 							options.has_format[sql_type.id()] = (!best_type_format_candidates.empty());
1226: 							if (!best_type_format_candidates.empty()) {
1227: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
1228: 							}
1229: 						}
1230: 						//	if none match, then this is not a column of type sql_type,
1231: 						if (best_type_format_candidates.empty()) {
1232: 							//	so restore the candidates that did work.
1233: 							best_type_format_candidates.swap(save_format_candidates);
1234: 							if (!best_type_format_candidates.empty()) {
1235: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
1236: 							}
1237: 						}
1238: 					}
1239: 
1240: 					if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
1241: 						break;
1242: 					} else {
1243: 						col_type_candidates.pop_back();
1244: 					}
1245: 				}
1246: 			}
1247: 
1248: 			if (!jumping_samples) {
1249: 				if ((sample_chunk_idx)*options.sample_chunk_size <= options.buffer_size) {
1250: 					// cache parse chunk
1251: 					// create a new chunk and fill it with the remainder
1252: 					auto chunk = make_unique<DataChunk>();
1253: 					auto parse_chunk_types = parse_chunk.GetTypes();
1254: 					chunk->Move(parse_chunk);
1255: 					cached_chunks.push(move(chunk));
1256: 				} else {
1257: 					while (!cached_chunks.empty()) {
1258: 						cached_chunks.pop();
1259: 					}
1260: 				}
1261: 			}
1262: 		}
1263: 
1264: 		// set sql types
1265: 		for (auto &best_sql_types_candidate : best_sql_types_candidates) {
1266: 			LogicalType d_type = best_sql_types_candidate.back();
1267: 			if (best_sql_types_candidate.size() == type_candidates.size()) {
1268: 				d_type = LogicalType::VARCHAR;
1269: 			}
1270: 			detected_types.push_back(d_type);
1271: 		}
1272: 	}
1273: 
1274: 	return detected_types;
1275: }
1276: 
1277: vector<LogicalType> BufferedCSVReader::SniffCSV(const vector<LogicalType> &requested_types) {
1278: 	for (auto &type : requested_types) {
1279: 		// auto detect for blobs not supported: there may be invalid UTF-8 in the file
1280: 		if (type.id() == LogicalTypeId::BLOB) {
1281: 			return requested_types;
1282: 		}
1283: 	}
1284: 
1285: 	// #######
1286: 	// ### dialect detection
1287: 	// #######
1288: 	BufferedCSVReaderOptions original_options = options;
1289: 	vector<BufferedCSVReaderOptions> info_candidates;
1290: 	idx_t best_num_cols = 0;
1291: 
1292: 	DetectDialect(requested_types, original_options, info_candidates, best_num_cols);
1293: 
1294: 	// if no dialect candidate was found, then file was most likely empty and we throw an exception
1295: 	if (info_candidates.empty()) {
1296: 		throw InvalidInputException(
1297: 		    "Error in file \"%s\": CSV options could not be auto-detected. Consider setting parser options manually.",
1298: 		    options.file_path);
1299: 	}
1300: 
1301: 	// #######
1302: 	// ### type detection (initial)
1303: 	// #######
1304: 	// type candidates, ordered by descending specificity (~ from high to low)
1305: 	vector<LogicalType> type_candidates = {
1306: 	    LogicalType::VARCHAR, LogicalType::TIMESTAMP,
1307: 	    LogicalType::DATE,    LogicalType::TIME,
1308: 	    LogicalType::DOUBLE,  /* LogicalType::FLOAT,*/ LogicalType::BIGINT,
1309: 	    LogicalType::INTEGER, /*LogicalType::SMALLINT, LogicalType::TINYINT,*/ LogicalType::BOOLEAN,
1310: 	    LogicalType::SQLNULL};
1311: 	// format template candidates, ordered by descending specificity (~ from high to low)
1312: 	std::map<LogicalTypeId, vector<const char *>> format_template_candidates = {
1313: 	    {LogicalTypeId::DATE, {"%m-%d-%Y", "%m-%d-%y", "%d-%m-%Y", "%d-%m-%y", "%Y-%m-%d", "%y-%m-%d"}},
1314: 	    {LogicalTypeId::TIMESTAMP,
1315: 	     {"%Y-%m-%d %H:%M:%S.%f", "%m-%d-%Y %I:%M:%S %p", "%m-%d-%y %I:%M:%S %p", "%d-%m-%Y %H:%M:%S",
1316: 	      "%d-%m-%y %H:%M:%S", "%Y-%m-%d %H:%M:%S", "%y-%m-%d %H:%M:%S"}},
1317: 	};
1318: 	vector<vector<LogicalType>> best_sql_types_candidates;
1319: 	map<LogicalTypeId, vector<string>> best_format_candidates;
1320: 	DataChunk best_header_row;
1321: 	DetectCandidateTypes(type_candidates, format_template_candidates, info_candidates, original_options, best_num_cols,
1322: 	                     best_sql_types_candidates, best_format_candidates, best_header_row);
1323: 
1324: 	// #######
1325: 	// ### header detection
1326: 	// #######
1327: 	options.num_cols = best_num_cols;
1328: 	DetectHeader(best_sql_types_candidates, best_header_row);
1329: 
1330: 	// #######
1331: 	// ### type detection (refining)
1332: 	// #######
1333: 	return RefineTypeDetection(type_candidates, requested_types, best_sql_types_candidates, best_format_candidates);
1334: }
1335: 
1336: bool BufferedCSVReader::TryParseComplexCSV(DataChunk &insert_chunk, string &error_message) {
1337: 	// used for parsing algorithm
1338: 	bool finished_chunk = false;
1339: 	idx_t column = 0;
1340: 	vector<idx_t> escape_positions;
1341: 	uint8_t delimiter_pos = 0, escape_pos = 0, quote_pos = 0;
1342: 	idx_t offset = 0;
1343: 
1344: 	// read values into the buffer (if any)
1345: 	if (position >= buffer_size) {
1346: 		if (!ReadBuffer(start)) {
1347: 			return true;
1348: 		}
1349: 	}
1350: 	// start parsing the first value
1351: 	start = position;
1352: 	goto value_start;
1353: value_start:
1354: 	/* state: value_start */
1355: 	// this state parses the first characters of a value
1356: 	offset = 0;
1357: 	delimiter_pos = 0;
1358: 	quote_pos = 0;
1359: 	do {
1360: 		idx_t count = 0;
1361: 		for (; position < buffer_size; position++) {
1362: 			quote_search.Match(quote_pos, buffer[position]);
1363: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1364: 			count++;
1365: 			if (delimiter_pos == options.delimiter.size()) {
1366: 				// found a delimiter, add the value
1367: 				offset = options.delimiter.size() - 1;
1368: 				goto add_value;
1369: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1370: 				// found a newline, add the row
1371: 				goto add_row;
1372: 			}
1373: 			if (count > quote_pos) {
1374: 				// did not find a quote directly at the start of the value, stop looking for the quote now
1375: 				goto normal;
1376: 			}
1377: 			if (quote_pos == options.quote.size()) {
1378: 				// found a quote, go to quoted loop and skip the initial quote
1379: 				start += options.quote.size();
1380: 				goto in_quotes;
1381: 			}
1382: 		}
1383: 	} while (ReadBuffer(start));
1384: 	// file ends while scanning for quote/delimiter, go to final state
1385: 	goto final_state;
1386: normal:
1387: 	/* state: normal parsing state */
1388: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1389: 	position++;
1390: 	do {
1391: 		for (; position < buffer_size; position++) {
1392: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1393: 			if (delimiter_pos == options.delimiter.size()) {
1394: 				offset = options.delimiter.size() - 1;
1395: 				goto add_value;
1396: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1397: 				goto add_row;
1398: 			}
1399: 		}
1400: 	} while (ReadBuffer(start));
1401: 	goto final_state;
1402: add_value:
1403: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1404: 	// increase position by 1 and move start to the new position
1405: 	offset = 0;
1406: 	start = ++position;
1407: 	if (position >= buffer_size && !ReadBuffer(start)) {
1408: 		// file ends right after delimiter, go to final state
1409: 		goto final_state;
1410: 	}
1411: 	goto value_start;
1412: add_row : {
1413: 	// check type of newline (\r or \n)
1414: 	bool carriage_return = buffer[position] == '\r';
1415: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1416: 	finished_chunk = AddRow(insert_chunk, column);
1417: 	// increase position by 1 and move start to the new position
1418: 	offset = 0;
1419: 	start = ++position;
1420: 	if (position >= buffer_size && !ReadBuffer(start)) {
1421: 		// file ends right after newline, go to final state
1422: 		goto final_state;
1423: 	}
1424: 	if (carriage_return) {
1425: 		// \r newline, go to special state that parses an optional \n afterwards
1426: 		goto carriage_return;
1427: 	} else {
1428: 		// \n newline, move to value start
1429: 		if (finished_chunk) {
1430: 			return true;
1431: 		}
1432: 		goto value_start;
1433: 	}
1434: }
1435: in_quotes:
1436: 	/* state: in_quotes */
1437: 	// this state parses the remainder of a quoted value
1438: 	quote_pos = 0;
1439: 	escape_pos = 0;
1440: 	position++;
1441: 	do {
1442: 		for (; position < buffer_size; position++) {
1443: 			quote_search.Match(quote_pos, buffer[position]);
1444: 			escape_search.Match(escape_pos, buffer[position]);
1445: 			if (quote_pos == options.quote.size()) {
1446: 				goto unquote;
1447: 			} else if (escape_pos == options.escape.size()) {
1448: 				escape_positions.push_back(position - start - (options.escape.size() - 1));
1449: 				goto handle_escape;
1450: 			}
1451: 		}
1452: 	} while (ReadBuffer(start));
1453: 	// still in quoted state at the end of the file, error:
1454: 	error_message = StringUtil::Format("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1455: 	                                   GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1456: 	return false;
1457: unquote:
1458: 	/* state: unquote */
1459: 	// this state handles the state directly after we unquote
1460: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1461: 	// or a delimiter/newline, ending the current value and moving on to the next value
1462: 	delimiter_pos = 0;
1463: 	quote_pos = 0;
1464: 	position++;
1465: 	if (position >= buffer_size && !ReadBuffer(start)) {
1466: 		// file ends right after unquote, go to final state
1467: 		offset = options.quote.size();
1468: 		goto final_state;
1469: 	}
1470: 	if (StringUtil::CharacterIsNewline(buffer[position])) {
1471: 		// quote followed by newline, add row
1472: 		offset = options.quote.size();
1473: 		goto add_row;
1474: 	}
1475: 	do {
1476: 		idx_t count = 0;
1477: 		for (; position < buffer_size; position++) {
1478: 			quote_search.Match(quote_pos, buffer[position]);
1479: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1480: 			count++;
1481: 			if (count > delimiter_pos && count > quote_pos) {
1482: 				error_message = StringUtil::Format(
1483: 				    "Error in file \"%s\" on line %s: quote should be followed by end of value, end "
1484: 				    "of row or another quote. (%s)",
1485: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1486: 				return false;
1487: 			}
1488: 			if (delimiter_pos == options.delimiter.size()) {
1489: 				// quote followed by delimiter, add value
1490: 				offset = options.quote.size() + options.delimiter.size() - 1;
1491: 				goto add_value;
1492: 			} else if (quote_pos == options.quote.size() &&
1493: 			           (options.escape.empty() || options.escape == options.quote)) {
1494: 				// quote followed by quote, go back to quoted state and add to escape
1495: 				escape_positions.push_back(position - start - (options.quote.size() - 1));
1496: 				goto in_quotes;
1497: 			}
1498: 		}
1499: 	} while (ReadBuffer(start));
1500: 	error_message = StringUtil::Format(
1501: 	    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of row or another quote. (%s)",
1502: 	    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1503: 	return false;
1504: handle_escape:
1505: 	escape_pos = 0;
1506: 	quote_pos = 0;
1507: 	position++;
1508: 	do {
1509: 		idx_t count = 0;
1510: 		for (; position < buffer_size; position++) {
1511: 			quote_search.Match(quote_pos, buffer[position]);
1512: 			escape_search.Match(escape_pos, buffer[position]);
1513: 			count++;
1514: 			if (count > escape_pos && count > quote_pos) {
1515: 				error_message = StringUtil::Format(
1516: 				    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1517: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1518: 				return false;
1519: 			}
1520: 			if (quote_pos == options.quote.size() || escape_pos == options.escape.size()) {
1521: 				// found quote or escape: move back to quoted state
1522: 				goto in_quotes;
1523: 			}
1524: 		}
1525: 	} while (ReadBuffer(start));
1526: 	error_message =
1527: 	    StringUtil::Format("Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1528: 	                       options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1529: 	return false;
1530: carriage_return:
1531: 	/* state: carriage_return */
1532: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1533: 	if (buffer[position] == '\n') {
1534: 		// newline after carriage return: skip
1535: 		start = ++position;
1536: 		if (position >= buffer_size && !ReadBuffer(start)) {
1537: 			// file ends right after newline, go to final state
1538: 			goto final_state;
1539: 		}
1540: 	}
1541: 	if (finished_chunk) {
1542: 		return true;
1543: 	}
1544: 	goto value_start;
1545: final_state:
1546: 	if (finished_chunk) {
1547: 		return true;
1548: 	}
1549: 	if (column > 0 || position > start) {
1550: 		// remaining values to be added to the chunk
1551: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1552: 		finished_chunk = AddRow(insert_chunk, column);
1553: 	}
1554: 	// final stage, only reached after parsing the file is finished
1555: 	// flush the parsed chunk and finalize parsing
1556: 	if (mode == ParserMode::PARSING) {
1557: 		Flush(insert_chunk);
1558: 	}
1559: 
1560: 	end_of_file_reached = true;
1561: 	return true;
1562: }
1563: 
1564: bool BufferedCSVReader::TryParseSimpleCSV(DataChunk &insert_chunk, string &error_message) {
1565: 	// used for parsing algorithm
1566: 	bool finished_chunk = false;
1567: 	idx_t column = 0;
1568: 	idx_t offset = 0;
1569: 	vector<idx_t> escape_positions;
1570: 
1571: 	// read values into the buffer (if any)
1572: 	if (position >= buffer_size) {
1573: 		if (!ReadBuffer(start)) {
1574: 			return true;
1575: 		}
1576: 	}
1577: 	// start parsing the first value
1578: 	goto value_start;
1579: value_start:
1580: 	offset = 0;
1581: 	/* state: value_start */
1582: 	// this state parses the first character of a value
1583: 	if (buffer[position] == options.quote[0]) {
1584: 		// quote: actual value starts in the next position
1585: 		// move to in_quotes state
1586: 		start = position + 1;
1587: 		goto in_quotes;
1588: 	} else {
1589: 		// no quote, move to normal parsing state
1590: 		start = position;
1591: 		goto normal;
1592: 	}
1593: normal:
1594: 	/* state: normal parsing state */
1595: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1596: 	do {
1597: 		for (; position < buffer_size; position++) {
1598: 			if (buffer[position] == options.delimiter[0]) {
1599: 				// delimiter: end the value and add it to the chunk
1600: 				goto add_value;
1601: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1602: 				// newline: add row
1603: 				goto add_row;
1604: 			}
1605: 		}
1606: 	} while (ReadBuffer(start));
1607: 	// file ends during normal scan: go to end state
1608: 	goto final_state;
1609: add_value:
1610: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1611: 	// increase position by 1 and move start to the new position
1612: 	offset = 0;
1613: 	start = ++position;
1614: 	if (position >= buffer_size && !ReadBuffer(start)) {
1615: 		// file ends right after delimiter, go to final state
1616: 		goto final_state;
1617: 	}
1618: 	goto value_start;
1619: add_row : {
1620: 	// check type of newline (\r or \n)
1621: 	bool carriage_return = buffer[position] == '\r';
1622: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1623: 	finished_chunk = AddRow(insert_chunk, column);
1624: 	// increase position by 1 and move start to the new position
1625: 	offset = 0;
1626: 	start = ++position;
1627: 	if (position >= buffer_size && !ReadBuffer(start)) {
1628: 		// file ends right after delimiter, go to final state
1629: 		goto final_state;
1630: 	}
1631: 	if (carriage_return) {
1632: 		// \r newline, go to special state that parses an optional \n afterwards
1633: 		goto carriage_return;
1634: 	} else {
1635: 		// \n newline, move to value start
1636: 		if (finished_chunk) {
1637: 			return true;
1638: 		}
1639: 		goto value_start;
1640: 	}
1641: }
1642: in_quotes:
1643: 	/* state: in_quotes */
1644: 	// this state parses the remainder of a quoted value
1645: 	position++;
1646: 	do {
1647: 		for (; position < buffer_size; position++) {
1648: 			if (buffer[position] == options.quote[0]) {
1649: 				// quote: move to unquoted state
1650: 				goto unquote;
1651: 			} else if (buffer[position] == options.escape[0]) {
1652: 				// escape: store the escaped position and move to handle_escape state
1653: 				escape_positions.push_back(position - start);
1654: 				goto handle_escape;
1655: 			}
1656: 		}
1657: 	} while (ReadBuffer(start));
1658: 	// still in quoted state at the end of the file, error:
1659: 	throw InvalidInputException("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1660: 	                            GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1661: unquote:
1662: 	/* state: unquote */
1663: 	// this state handles the state directly after we unquote
1664: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1665: 	// or a delimiter/newline, ending the current value and moving on to the next value
1666: 	position++;
1667: 	if (position >= buffer_size && !ReadBuffer(start)) {
1668: 		// file ends right after unquote, go to final state
1669: 		offset = 1;
1670: 		goto final_state;
1671: 	}
1672: 	if (buffer[position] == options.quote[0] && (options.escape.empty() || options.escape[0] == options.quote[0])) {
1673: 		// escaped quote, return to quoted state and store escape position
1674: 		escape_positions.push_back(position - start);
1675: 		goto in_quotes;
1676: 	} else if (buffer[position] == options.delimiter[0]) {
1677: 		// delimiter, add value
1678: 		offset = 1;
1679: 		goto add_value;
1680: 	} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1681: 		offset = 1;
1682: 		goto add_row;
1683: 	} else {
1684: 		error_message = StringUtil::Format(
1685: 		    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of "
1686: 		    "row or another quote. (%s)",
1687: 		    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1688: 		return false;
1689: 	}
1690: handle_escape:
1691: 	/* state: handle_escape */
1692: 	// escape should be followed by a quote or another escape character
1693: 	position++;
1694: 	if (position >= buffer_size && !ReadBuffer(start)) {
1695: 		error_message = StringUtil::Format(
1696: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1697: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1698: 		return false;
1699: 	}
1700: 	if (buffer[position] != options.quote[0] && buffer[position] != options.escape[0]) {
1701: 		error_message = StringUtil::Format(
1702: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1703: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1704: 		return false;
1705: 	}
1706: 	// escape was followed by quote or escape, go back to quoted state
1707: 	goto in_quotes;
1708: carriage_return:
1709: 	/* state: carriage_return */
1710: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1711: 	if (buffer[position] == '\n') {
1712: 		// newline after carriage return: skip
1713: 		// increase position by 1 and move start to the new position
1714: 		start = ++position;
1715: 		if (position >= buffer_size && !ReadBuffer(start)) {
1716: 			// file ends right after delimiter, go to final state
1717: 			goto final_state;
1718: 		}
1719: 	}
1720: 	if (finished_chunk) {
1721: 		return true;
1722: 	}
1723: 	goto value_start;
1724: final_state:
1725: 	if (finished_chunk) {
1726: 		return true;
1727: 	}
1728: 
1729: 	if (column > 0 || position > start) {
1730: 		// remaining values to be added to the chunk
1731: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1732: 		finished_chunk = AddRow(insert_chunk, column);
1733: 	}
1734: 
1735: 	// final stage, only reached after parsing the file is finished
1736: 	// flush the parsed chunk and finalize parsing
1737: 	if (mode == ParserMode::PARSING) {
1738: 		Flush(insert_chunk);
1739: 	}
1740: 
1741: 	end_of_file_reached = true;
1742: 	return true;
1743: }
1744: 
1745: bool BufferedCSVReader::ReadBuffer(idx_t &start) {
1746: 	auto old_buffer = move(buffer);
1747: 
1748: 	// the remaining part of the last buffer
1749: 	idx_t remaining = buffer_size - start;
1750: 
1751: 	bool large_buffers = mode == ParserMode::PARSING && !file_handle->OnDiskFile() && file_handle->CanSeek();
1752: 	idx_t buffer_read_size = large_buffers ? INITIAL_BUFFER_SIZE_LARGE : INITIAL_BUFFER_SIZE;
1753: 
1754: 	while (remaining > buffer_read_size) {
1755: 		buffer_read_size *= 2;
1756: 	}
1757: 
1758: 	// Check line length
1759: 	if (remaining > options.maximum_line_size) {
1760: 		throw InvalidInputException("Maximum line size of %llu bytes exceeded!", options.maximum_line_size);
1761: 	}
1762: 
1763: 	buffer = unique_ptr<char[]>(new char[buffer_read_size + remaining + 1]);
1764: 	buffer_size = remaining + buffer_read_size;
1765: 	if (remaining > 0) {
1766: 		// remaining from last buffer: copy it here
1767: 		memcpy(buffer.get(), old_buffer.get() + start, remaining);
1768: 	}
1769: 	idx_t read_count = file_handle->Read(buffer.get() + remaining, buffer_read_size);
1770: 
1771: 	bytes_in_chunk += read_count;
1772: 	buffer_size = remaining + read_count;
1773: 	buffer[buffer_size] = '\0';
1774: 	if (old_buffer) {
1775: 		cached_buffers.push_back(move(old_buffer));
1776: 	}
1777: 	start = 0;
1778: 	position = remaining;
1779: 	if (!bom_checked) {
1780: 		bom_checked = true;
1781: 		if (read_count >= 3 && buffer[0] == '\xEF' && buffer[1] == '\xBB' && buffer[2] == '\xBF') {
1782: 			position += 3;
1783: 		}
1784: 	}
1785: 
1786: 	return read_count > 0;
1787: }
1788: 
1789: void BufferedCSVReader::ParseCSV(DataChunk &insert_chunk) {
1790: 	// if no auto-detect or auto-detect with jumping samples, we have nothing cached and start from the beginning
1791: 	if (cached_chunks.empty()) {
1792: 		cached_buffers.clear();
1793: 	} else {
1794: 		auto &chunk = cached_chunks.front();
1795: 		parse_chunk.Move(*chunk);
1796: 		cached_chunks.pop();
1797: 		Flush(insert_chunk);
1798: 		return;
1799: 	}
1800: 
1801: 	string error_message;
1802: 	if (!TryParseCSV(ParserMode::PARSING, insert_chunk, error_message)) {
1803: 		throw InvalidInputException(error_message);
1804: 	}
1805: }
1806: 
1807: bool BufferedCSVReader::TryParseCSV(ParserMode mode) {
1808: 	DataChunk dummy_chunk;
1809: 	string error_message;
1810: 	return TryParseCSV(mode, dummy_chunk, error_message);
1811: }
1812: 
1813: void BufferedCSVReader::ParseCSV(ParserMode mode) {
1814: 	DataChunk dummy_chunk;
1815: 	string error_message;
1816: 	if (!TryParseCSV(mode, dummy_chunk, error_message)) {
1817: 		throw InvalidInputException(error_message);
1818: 	}
1819: }
1820: 
1821: bool BufferedCSVReader::TryParseCSV(ParserMode parser_mode, DataChunk &insert_chunk, string &error_message) {
1822: 	mode = parser_mode;
1823: 
1824: 	if (options.quote.size() <= 1 && options.escape.size() <= 1 && options.delimiter.size() == 1) {
1825: 		return TryParseSimpleCSV(insert_chunk, error_message);
1826: 	} else {
1827: 		return TryParseComplexCSV(insert_chunk, error_message);
1828: 	}
1829: }
1830: 
1831: void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vector<idx_t> &escape_positions) {
1832: 	if (length == 0 && column == 0) {
1833: 		row_empty = true;
1834: 	} else {
1835: 		row_empty = false;
1836: 	}
1837: 
1838: 	if (!sql_types.empty() && column == sql_types.size() && length == 0) {
1839: 		// skip a single trailing delimiter in last column
1840: 		return;
1841: 	}
1842: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1843: 		column++;
1844: 		return;
1845: 	}
1846: 	if (column >= sql_types.size()) {
1847: 		if (options.ignore_errors) {
1848: 			error_column_overflow = true;
1849: 			return;
1850: 		} else {
1851: 			throw InvalidInputException("Error on line %s: expected %lld values per row, but got more. (%s)",
1852: 			                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(),
1853: 			                            options.ToString());
1854: 		}
1855: 	}
1856: 
1857: 	// insert the line number into the chunk
1858: 	idx_t row_entry = parse_chunk.size();
1859: 
1860: 	str_val[length] = '\0';
1861: 
1862: 	// test against null string
1863: 	if (!options.force_not_null[column] && strcmp(options.null_str.c_str(), str_val) == 0) {
1864: 		FlatVector::SetNull(parse_chunk.data[column], row_entry, true);
1865: 	} else {
1866: 		auto &v = parse_chunk.data[column];
1867: 		auto parse_data = FlatVector::GetData<string_t>(v);
1868: 		if (!escape_positions.empty()) {
1869: 			// remove escape characters (if any)
1870: 			string old_val = str_val;
1871: 			string new_val = "";
1872: 			idx_t prev_pos = 0;
1873: 			for (idx_t i = 0; i < escape_positions.size(); i++) {
1874: 				idx_t next_pos = escape_positions[i];
1875: 				new_val += old_val.substr(prev_pos, next_pos - prev_pos);
1876: 
1877: 				if (options.escape.empty() || options.escape == options.quote) {
1878: 					prev_pos = next_pos + options.quote.size();
1879: 				} else {
1880: 					prev_pos = next_pos + options.escape.size();
1881: 				}
1882: 			}
1883: 			new_val += old_val.substr(prev_pos, old_val.size() - prev_pos);
1884: 			escape_positions.clear();
1885: 			parse_data[row_entry] = StringVector::AddStringOrBlob(v, string_t(new_val));
1886: 		} else {
1887: 			parse_data[row_entry] = string_t(str_val, length);
1888: 		}
1889: 	}
1890: 
1891: 	// move to the next column
1892: 	column++;
1893: }
1894: 
1895: bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {
1896: 	linenr++;
1897: 
1898: 	if (row_empty) {
1899: 		row_empty = false;
1900: 		if (sql_types.size() != 1) {
1901: 			column = 0;
1902: 			return false;
1903: 		}
1904: 	}
1905: 
1906: 	// Error forwarded by 'ignore_errors' - originally encountered in 'AddValue'
1907: 	if (error_column_overflow) {
1908: 		D_ASSERT(options.ignore_errors);
1909: 		error_column_overflow = false;
1910: 		column = 0;
1911: 		return false;
1912: 	}
1913: 
1914: 	if (column < sql_types.size() && mode != ParserMode::SNIFFING_DIALECT) {
1915: 		if (options.ignore_errors) {
1916: 			column = 0;
1917: 			return false;
1918: 		} else {
1919: 			throw InvalidInputException("Error on line %s: expected %lld values per row, but got %d. (%s)",
1920: 			                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column,
1921: 			                            options.ToString());
1922: 		}
1923: 	}
1924: 
1925: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1926: 		sniffed_column_counts.push_back(column);
1927: 
1928: 		if (sniffed_column_counts.size() == options.sample_chunk_size) {
1929: 			return true;
1930: 		}
1931: 	} else {
1932: 		parse_chunk.SetCardinality(parse_chunk.size() + 1);
1933: 	}
1934: 
1935: 	if (mode == ParserMode::PARSING_HEADER) {
1936: 		return true;
1937: 	}
1938: 
1939: 	if (mode == ParserMode::SNIFFING_DATATYPES && parse_chunk.size() == options.sample_chunk_size) {
1940: 		return true;
1941: 	}
1942: 
1943: 	if (mode == ParserMode::PARSING && parse_chunk.size() == STANDARD_VECTOR_SIZE) {
1944: 		Flush(insert_chunk);
1945: 		return true;
1946: 	}
1947: 
1948: 	column = 0;
1949: 	return false;
1950: }
1951: 
1952: void BufferedCSVReader::Flush(DataChunk &insert_chunk) {
1953: 	if (parse_chunk.size() == 0) {
1954: 		return;
1955: 	}
1956: 
1957: 	bool conversion_error_ignored = false;
1958: 
1959: 	// convert the columns in the parsed chunk to the types of the table
1960: 	insert_chunk.SetCardinality(parse_chunk);
1961: 	for (idx_t col_idx = 0; col_idx < sql_types.size(); col_idx++) {
1962: 		if (sql_types[col_idx].id() == LogicalTypeId::VARCHAR) {
1963: 			// target type is varchar: no need to convert
1964: 			// just test that all strings are valid utf-8 strings
1965: 			auto parse_data = FlatVector::GetData<string_t>(parse_chunk.data[col_idx]);
1966: 			for (idx_t i = 0; i < parse_chunk.size(); i++) {
1967: 				if (!FlatVector::IsNull(parse_chunk.data[col_idx], i)) {
1968: 					auto s = parse_data[i];
1969: 					auto utf_type = Utf8Proc::Analyze(s.GetDataUnsafe(), s.GetSize());
1970: 					if (utf_type == UnicodeType::INVALID) {
1971: 						string col_name = to_string(col_idx);
1972: 						if (col_idx < col_names.size()) {
1973: 							col_name = "\"" + col_names[col_idx] + "\"";
1974: 						}
1975: 						throw InvalidInputException("Error in file \"%s\" between line %llu and %llu in column \"%s\": "
1976: 						                            "file is not valid UTF8. Parser options: %s",
1977: 						                            options.file_path, linenr - parse_chunk.size(), linenr, col_name,
1978: 						                            options.ToString());
1979: 					}
1980: 				}
1981: 			}
1982: 			insert_chunk.data[col_idx].Reference(parse_chunk.data[col_idx]);
1983: 		} else {
1984: 			string error_message;
1985: 			bool success;
1986: 			if (options.has_format[LogicalTypeId::DATE] && sql_types[col_idx].id() == LogicalTypeId::DATE) {
1987: 				// use the date format to cast the chunk
1988: 				success = TryCastDateVector(options, parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1989: 				                            parse_chunk.size(), error_message);
1990: 			} else if (options.has_format[LogicalTypeId::TIMESTAMP] &&
1991: 			           sql_types[col_idx].id() == LogicalTypeId::TIMESTAMP) {
1992: 				// use the date format to cast the chunk
1993: 				success = TryCastTimestampVector(options, parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1994: 				                                 parse_chunk.size(), error_message);
1995: 			} else {
1996: 				// target type is not varchar: perform a cast
1997: 				success = VectorOperations::TryCast(parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1998: 				                                    parse_chunk.size(), &error_message);
1999: 			}
2000: 			if (success) {
2001: 				continue;
2002: 			}
2003: 			if (options.ignore_errors) {
2004: 				conversion_error_ignored = true;
2005: 				continue;
2006: 			}
2007: 			string col_name = to_string(col_idx);
2008: 			if (col_idx < col_names.size()) {
2009: 				col_name = "\"" + col_names[col_idx] + "\"";
2010: 			}
2011: 
2012: 			if (options.auto_detect) {
2013: 				throw InvalidInputException("%s in column %s, between line %llu and %llu. Parser "
2014: 				                            "options: %s. Consider either increasing the sample size "
2015: 				                            "(SAMPLE_SIZE=X [X rows] or SAMPLE_SIZE=-1 [all rows]), "
2016: 				                            "or skipping column conversion (ALL_VARCHAR=1)",
2017: 				                            error_message, col_name, linenr - parse_chunk.size() + 1, linenr,
2018: 				                            options.ToString());
2019: 			} else {
2020: 				throw InvalidInputException("%s between line %llu and %llu in column %s. Parser options: %s ",
2021: 				                            error_message, linenr - parse_chunk.size(), linenr, col_name,
2022: 				                            options.ToString());
2023: 			}
2024: 		}
2025: 	}
2026: 	if (conversion_error_ignored) {
2027: 		D_ASSERT(options.ignore_errors);
2028: 		SelectionVector succesful_rows;
2029: 		succesful_rows.Initialize(parse_chunk.size());
2030: 		idx_t sel_size = 0;
2031: 
2032: 		for (idx_t row_idx = 0; row_idx < parse_chunk.size(); row_idx++) {
2033: 			bool failed = false;
2034: 			for (idx_t column_idx = 0; column_idx < sql_types.size(); column_idx++) {
2035: 
2036: 				auto &inserted_column = insert_chunk.data[column_idx];
2037: 				auto &parsed_column = parse_chunk.data[column_idx];
2038: 
2039: 				bool was_already_null = FlatVector::IsNull(parsed_column, row_idx);
2040: 				if (!was_already_null && FlatVector::IsNull(inserted_column, row_idx)) {
2041: 					failed = true;
2042: 					break;
2043: 				}
2044: 			}
2045: 			if (!failed) {
2046: 				succesful_rows.set_index(sel_size++, row_idx);
2047: 			}
2048: 		}
2049: 		insert_chunk.Slice(succesful_rows, sel_size);
2050: 	}
2051: 	parse_chunk.Reset();
2052: }
2053: } // namespace duckdb
[end of src/execution/operator/persistent/buffered_csv_reader.cpp]
[start of src/function/scalar/date/strftime.cpp]
1: #include "duckdb/function/scalar/date_functions.hpp"
2: 
3: #include "duckdb/planner/expression/bound_function_expression.hpp"
4: 
5: #include "duckdb/common/types/date.hpp"
6: #include "duckdb/common/types/time.hpp"
7: #include "duckdb/common/types/timestamp.hpp"
8: #include "duckdb/common/types/cast_helpers.hpp"
9: 
10: #include "duckdb/common/string_util.hpp"
11: #include "duckdb/common/to_string.hpp"
12: 
13: #include "duckdb/function/scalar/strftime.hpp"
14: 
15: #include "duckdb/common/vector_operations/unary_executor.hpp"
16: 
17: #include "duckdb/execution/expression_executor.hpp"
18: 
19: #include <cctype>
20: 
21: namespace duckdb {
22: 
23: idx_t StrfTimepecifierSize(StrTimeSpecifier specifier) {
24: 	switch (specifier) {
25: 	case StrTimeSpecifier::ABBREVIATED_WEEKDAY_NAME:
26: 	case StrTimeSpecifier::ABBREVIATED_MONTH_NAME:
27: 		return 3;
28: 	case StrTimeSpecifier::WEEKDAY_DECIMAL:
29: 		return 1;
30: 	case StrTimeSpecifier::DAY_OF_MONTH_PADDED:
31: 	case StrTimeSpecifier::MONTH_DECIMAL_PADDED:
32: 	case StrTimeSpecifier::YEAR_WITHOUT_CENTURY_PADDED:
33: 	case StrTimeSpecifier::HOUR_24_PADDED:
34: 	case StrTimeSpecifier::HOUR_12_PADDED:
35: 	case StrTimeSpecifier::MINUTE_PADDED:
36: 	case StrTimeSpecifier::SECOND_PADDED:
37: 	case StrTimeSpecifier::AM_PM:
38: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_SUN_FIRST:
39: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_MON_FIRST:
40: 		return 2;
41: 	case StrTimeSpecifier::MICROSECOND_PADDED:
42: 		return 6;
43: 	case StrTimeSpecifier::MILLISECOND_PADDED:
44: 		return 3;
45: 	case StrTimeSpecifier::DAY_OF_YEAR_PADDED:
46: 		return 3;
47: 	default:
48: 		return 0;
49: 	}
50: }
51: 
52: void StrTimeFormat::AddLiteral(string literal) {
53: 	constant_size += literal.size();
54: 	literals.push_back(move(literal));
55: }
56: 
57: void StrTimeFormat::AddFormatSpecifier(string preceding_literal, StrTimeSpecifier specifier) {
58: 	AddLiteral(move(preceding_literal));
59: 	specifiers.push_back(specifier);
60: }
61: 
62: void StrfTimeFormat::AddFormatSpecifier(string preceding_literal, StrTimeSpecifier specifier) {
63: 	is_date_specifier.push_back(IsDateSpecifier(specifier));
64: 	idx_t specifier_size = StrfTimepecifierSize(specifier);
65: 	if (specifier_size == 0) {
66: 		// variable length specifier
67: 		var_length_specifiers.push_back(specifier);
68: 	} else {
69: 		// constant size specifier
70: 		constant_size += specifier_size;
71: 	}
72: 	StrTimeFormat::AddFormatSpecifier(move(preceding_literal), specifier);
73: }
74: 
75: idx_t StrfTimeFormat::GetSpecifierLength(StrTimeSpecifier specifier, date_t date, dtime_t time, int32_t utc_offset,
76:                                          const char *tz_name) {
77: 	switch (specifier) {
78: 	case StrTimeSpecifier::FULL_WEEKDAY_NAME:
79: 		return Date::DAY_NAMES[Date::ExtractISODayOfTheWeek(date) % 7].GetSize();
80: 	case StrTimeSpecifier::FULL_MONTH_NAME:
81: 		return Date::MONTH_NAMES[Date::ExtractMonth(date) - 1].GetSize();
82: 	case StrTimeSpecifier::YEAR_DECIMAL: {
83: 		auto year = Date::ExtractYear(date);
84: 		return NumericHelper::SignedLength<int32_t, uint32_t>(year);
85: 	}
86: 	case StrTimeSpecifier::MONTH_DECIMAL: {
87: 		idx_t len = 1;
88: 		auto month = Date::ExtractMonth(date);
89: 		len += month >= 10;
90: 		return len;
91: 	}
92: 	case StrTimeSpecifier::UTC_OFFSET:
93: 		// ±HH or ±HH:MM
94: 		return (utc_offset % 60) ? 6 : 3;
95: 	case StrTimeSpecifier::TZ_NAME:
96: 		if (tz_name) {
97: 			return strlen(tz_name);
98: 		}
99: 		// empty for now
100: 		return 0;
101: 	case StrTimeSpecifier::HOUR_24_DECIMAL:
102: 	case StrTimeSpecifier::HOUR_12_DECIMAL:
103: 	case StrTimeSpecifier::MINUTE_DECIMAL:
104: 	case StrTimeSpecifier::SECOND_DECIMAL: {
105: 		// time specifiers
106: 		idx_t len = 1;
107: 		int32_t hour, min, sec, msec;
108: 		Time::Convert(time, hour, min, sec, msec);
109: 		switch (specifier) {
110: 		case StrTimeSpecifier::HOUR_24_DECIMAL:
111: 			len += hour >= 10;
112: 			break;
113: 		case StrTimeSpecifier::HOUR_12_DECIMAL:
114: 			hour = hour % 12;
115: 			if (hour == 0) {
116: 				hour = 12;
117: 			}
118: 			len += hour >= 10;
119: 			break;
120: 		case StrTimeSpecifier::MINUTE_DECIMAL:
121: 			len += min >= 10;
122: 			break;
123: 		case StrTimeSpecifier::SECOND_DECIMAL:
124: 			len += sec >= 10;
125: 			break;
126: 		default:
127: 			throw InternalException("Time specifier mismatch");
128: 		}
129: 		return len;
130: 	}
131: 	case StrTimeSpecifier::DAY_OF_MONTH:
132: 		return NumericHelper::UnsignedLength<uint32_t>(Date::ExtractDay(date));
133: 	case StrTimeSpecifier::DAY_OF_YEAR_DECIMAL:
134: 		return NumericHelper::UnsignedLength<uint32_t>(Date::ExtractDayOfTheYear(date));
135: 	case StrTimeSpecifier::YEAR_WITHOUT_CENTURY:
136: 		return NumericHelper::UnsignedLength<uint32_t>(Date::ExtractYear(date) % 100);
137: 	default:
138: 		throw InternalException("Unimplemented specifier for GetSpecifierLength");
139: 	}
140: }
141: 
142: //! Returns the total length of the date formatted by this format specifier
143: idx_t StrfTimeFormat::GetLength(date_t date, dtime_t time, int32_t utc_offset, const char *tz_name) {
144: 	idx_t size = constant_size;
145: 	if (!var_length_specifiers.empty()) {
146: 		for (auto &specifier : var_length_specifiers) {
147: 			size += GetSpecifierLength(specifier, date, time, utc_offset, tz_name);
148: 		}
149: 	}
150: 	return size;
151: }
152: 
153: char *StrfTimeFormat::WriteString(char *target, const string_t &str) {
154: 	idx_t size = str.GetSize();
155: 	memcpy(target, str.GetDataUnsafe(), size);
156: 	return target + size;
157: }
158: 
159: // write a value in the range of 0..99 unpadded (e.g. "1", "2", ... "98", "99")
160: char *StrfTimeFormat::Write2(char *target, uint8_t value) {
161: 	D_ASSERT(value < 100);
162: 	if (value >= 10) {
163: 		return WritePadded2(target, value);
164: 	} else {
165: 		*target = char(uint8_t('0') + value);
166: 		return target + 1;
167: 	}
168: }
169: 
170: // write a value in the range of 0..99 padded to 2 digits
171: char *StrfTimeFormat::WritePadded2(char *target, uint32_t value) {
172: 	D_ASSERT(value < 100);
173: 	auto index = static_cast<unsigned>(value * 2);
174: 	*target++ = duckdb_fmt::internal::data::digits[index];
175: 	*target++ = duckdb_fmt::internal::data::digits[index + 1];
176: 	return target;
177: }
178: 
179: // write a value in the range of 0..999 padded
180: char *StrfTimeFormat::WritePadded3(char *target, uint32_t value) {
181: 	D_ASSERT(value < 1000);
182: 	if (value >= 100) {
183: 		WritePadded2(target + 1, value % 100);
184: 		*target = char(uint8_t('0') + value / 100);
185: 		return target + 3;
186: 	} else {
187: 		*target = '0';
188: 		target++;
189: 		return WritePadded2(target, value);
190: 	}
191: }
192: 
193: // write a value in the range of 0..999999 padded to 6 digits
194: char *StrfTimeFormat::WritePadded(char *target, uint32_t value, size_t padding) {
195: 	D_ASSERT(padding % 2 == 0);
196: 	for (size_t i = 0; i < padding / 2; i++) {
197: 		int decimals = value % 100;
198: 		WritePadded2(target + padding - 2 * (i + 1), decimals);
199: 		value /= 100;
200: 	}
201: 	return target + padding;
202: }
203: 
204: bool StrfTimeFormat::IsDateSpecifier(StrTimeSpecifier specifier) {
205: 	switch (specifier) {
206: 	case StrTimeSpecifier::ABBREVIATED_WEEKDAY_NAME:
207: 	case StrTimeSpecifier::FULL_WEEKDAY_NAME:
208: 	case StrTimeSpecifier::DAY_OF_YEAR_PADDED:
209: 	case StrTimeSpecifier::DAY_OF_YEAR_DECIMAL:
210: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_MON_FIRST:
211: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_SUN_FIRST:
212: 	case StrTimeSpecifier::WEEKDAY_DECIMAL:
213: 		return true;
214: 	default:
215: 		return false;
216: 	}
217: }
218: 
219: char *StrfTimeFormat::WriteDateSpecifier(StrTimeSpecifier specifier, date_t date, char *target) {
220: 	switch (specifier) {
221: 	case StrTimeSpecifier::ABBREVIATED_WEEKDAY_NAME: {
222: 		auto dow = Date::ExtractISODayOfTheWeek(date);
223: 		target = WriteString(target, Date::DAY_NAMES_ABBREVIATED[dow % 7]);
224: 		break;
225: 	}
226: 	case StrTimeSpecifier::FULL_WEEKDAY_NAME: {
227: 		auto dow = Date::ExtractISODayOfTheWeek(date);
228: 		target = WriteString(target, Date::DAY_NAMES[dow % 7]);
229: 		break;
230: 	}
231: 	case StrTimeSpecifier::WEEKDAY_DECIMAL: {
232: 		auto dow = Date::ExtractISODayOfTheWeek(date);
233: 		*target = char('0' + uint8_t(dow % 7));
234: 		target++;
235: 		break;
236: 	}
237: 	case StrTimeSpecifier::DAY_OF_YEAR_PADDED: {
238: 		int32_t doy = Date::ExtractDayOfTheYear(date);
239: 		target = WritePadded3(target, doy);
240: 		break;
241: 	}
242: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_MON_FIRST:
243: 		target = WritePadded2(target, Date::ExtractWeekNumberRegular(date, true));
244: 		break;
245: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_SUN_FIRST:
246: 		target = WritePadded2(target, Date::ExtractWeekNumberRegular(date, false));
247: 		break;
248: 	case StrTimeSpecifier::DAY_OF_YEAR_DECIMAL: {
249: 		uint32_t doy = Date::ExtractDayOfTheYear(date);
250: 		target += NumericHelper::UnsignedLength<uint32_t>(doy);
251: 		NumericHelper::FormatUnsigned(doy, target);
252: 		break;
253: 	}
254: 	default:
255: 		throw InternalException("Unimplemented date specifier for strftime");
256: 	}
257: 	return target;
258: }
259: 
260: char *StrfTimeFormat::WriteStandardSpecifier(StrTimeSpecifier specifier, int32_t data[], const char *tz_name,
261:                                              char *target) {
262: 	// data contains [0] year, [1] month, [2] day, [3] hour, [4] minute, [5] second, [6] msec, [7] utc
263: 	switch (specifier) {
264: 	case StrTimeSpecifier::DAY_OF_MONTH_PADDED:
265: 		target = WritePadded2(target, data[2]);
266: 		break;
267: 	case StrTimeSpecifier::ABBREVIATED_MONTH_NAME: {
268: 		auto &month_name = Date::MONTH_NAMES_ABBREVIATED[data[1] - 1];
269: 		return WriteString(target, month_name);
270: 	}
271: 	case StrTimeSpecifier::FULL_MONTH_NAME: {
272: 		auto &month_name = Date::MONTH_NAMES[data[1] - 1];
273: 		return WriteString(target, month_name);
274: 	}
275: 	case StrTimeSpecifier::MONTH_DECIMAL_PADDED:
276: 		target = WritePadded2(target, data[1]);
277: 		break;
278: 	case StrTimeSpecifier::YEAR_WITHOUT_CENTURY_PADDED:
279: 		target = WritePadded2(target, AbsValue(data[0]) % 100);
280: 		break;
281: 	case StrTimeSpecifier::YEAR_DECIMAL:
282: 		if (data[0] >= 0 && data[0] <= 9999) {
283: 			target = WritePadded(target, data[0], 4);
284: 		} else {
285: 			int32_t year = data[0];
286: 			if (data[0] < 0) {
287: 				*target = '-';
288: 				year = -year;
289: 				target++;
290: 			}
291: 			auto len = NumericHelper::UnsignedLength<uint32_t>(year);
292: 			NumericHelper::FormatUnsigned(year, target + len);
293: 			target += len;
294: 		}
295: 		break;
296: 	case StrTimeSpecifier::HOUR_24_PADDED: {
297: 		target = WritePadded2(target, data[3]);
298: 		break;
299: 	}
300: 	case StrTimeSpecifier::HOUR_12_PADDED: {
301: 		int hour = data[3] % 12;
302: 		if (hour == 0) {
303: 			hour = 12;
304: 		}
305: 		target = WritePadded2(target, hour);
306: 		break;
307: 	}
308: 	case StrTimeSpecifier::AM_PM:
309: 		*target++ = data[3] >= 12 ? 'P' : 'A';
310: 		*target++ = 'M';
311: 		break;
312: 	case StrTimeSpecifier::MINUTE_PADDED: {
313: 		target = WritePadded2(target, data[4]);
314: 		break;
315: 	}
316: 	case StrTimeSpecifier::SECOND_PADDED:
317: 		target = WritePadded2(target, data[5]);
318: 		break;
319: 	case StrTimeSpecifier::MICROSECOND_PADDED:
320: 		target = WritePadded(target, data[6], 6);
321: 		break;
322: 	case StrTimeSpecifier::MILLISECOND_PADDED:
323: 		target = WritePadded3(target, data[6] / 1000);
324: 		break;
325: 	case StrTimeSpecifier::UTC_OFFSET: {
326: 		*target++ = (data[7] < 0) ? '-' : '+';
327: 
328: 		auto offset = abs(data[7]);
329: 		auto offset_hours = offset / Interval::MINS_PER_HOUR;
330: 		auto offset_minutes = offset % Interval::MINS_PER_HOUR;
331: 		target = WritePadded2(target, offset_hours);
332: 		if (offset_minutes) {
333: 			*target++ = ':';
334: 			target = WritePadded2(target, offset_minutes);
335: 		}
336: 		break;
337: 	}
338: 	case StrTimeSpecifier::TZ_NAME:
339: 		if (tz_name) {
340: 			strcpy(target, tz_name);
341: 			target += strlen(tz_name);
342: 		}
343: 		break;
344: 	case StrTimeSpecifier::DAY_OF_MONTH: {
345: 		target = Write2(target, data[2] % 100);
346: 		break;
347: 	}
348: 	case StrTimeSpecifier::MONTH_DECIMAL: {
349: 		target = Write2(target, data[1]);
350: 		break;
351: 	}
352: 	case StrTimeSpecifier::YEAR_WITHOUT_CENTURY: {
353: 		target = Write2(target, data[0] % 100);
354: 		break;
355: 	}
356: 	case StrTimeSpecifier::HOUR_24_DECIMAL: {
357: 		target = Write2(target, data[3]);
358: 		break;
359: 	}
360: 	case StrTimeSpecifier::HOUR_12_DECIMAL: {
361: 		int hour = data[3] % 12;
362: 		if (hour == 0) {
363: 			hour = 12;
364: 		}
365: 		target = Write2(target, hour);
366: 		break;
367: 	}
368: 	case StrTimeSpecifier::MINUTE_DECIMAL: {
369: 		target = Write2(target, data[4]);
370: 		break;
371: 	}
372: 	case StrTimeSpecifier::SECOND_DECIMAL: {
373: 		target = Write2(target, data[5]);
374: 		break;
375: 	}
376: 	default:
377: 		throw InternalException("Unimplemented specifier for WriteStandardSpecifier in strftime");
378: 	}
379: 	return target;
380: }
381: 
382: void StrfTimeFormat::FormatString(date_t date, int32_t data[8], const char *tz_name, char *target) {
383: 	D_ASSERT(specifiers.size() + 1 == literals.size());
384: 	idx_t i;
385: 	for (i = 0; i < specifiers.size(); i++) {
386: 		// first copy the current literal
387: 		memcpy(target, literals[i].c_str(), literals[i].size());
388: 		target += literals[i].size();
389: 		// now copy the specifier
390: 		if (is_date_specifier[i]) {
391: 			target = WriteDateSpecifier(specifiers[i], date, target);
392: 		} else {
393: 			target = WriteStandardSpecifier(specifiers[i], data, tz_name, target);
394: 		}
395: 	}
396: 	// copy the final literal into the target
397: 	memcpy(target, literals[i].c_str(), literals[i].size());
398: }
399: 
400: void StrfTimeFormat::FormatString(date_t date, dtime_t time, char *target) {
401: 	int32_t data[8]; // year, month, day, hour, min, sec, µs, offset
402: 	Date::Convert(date, data[0], data[1], data[2]);
403: 	Time::Convert(time, data[3], data[4], data[5], data[6]);
404: 	data[7] = 0;
405: 
406: 	FormatString(date, data, nullptr, target);
407: }
408: 
409: string StrfTimeFormat::Format(timestamp_t timestamp, const string &format_str) {
410: 	StrfTimeFormat format;
411: 	format.ParseFormatSpecifier(format_str, format);
412: 
413: 	auto date = Timestamp::GetDate(timestamp);
414: 	auto time = Timestamp::GetTime(timestamp);
415: 
416: 	auto len = format.GetLength(date, time, 0, nullptr);
417: 	auto result = unique_ptr<char[]>(new char[len]);
418: 	format.FormatString(date, time, result.get());
419: 	return string(result.get(), len);
420: }
421: 
422: string StrTimeFormat::ParseFormatSpecifier(const string &format_string, StrTimeFormat &format) {
423: 	if (format_string.empty()) {
424: 		return "Empty format string";
425: 	}
426: 	format.specifiers.clear();
427: 	format.literals.clear();
428: 	format.numeric_width.clear();
429: 	format.constant_size = 0;
430: 	idx_t pos = 0;
431: 	string current_literal;
432: 	for (idx_t i = 0; i < format_string.size(); i++) {
433: 		if (format_string[i] == '%') {
434: 			if (i + 1 == format_string.size()) {
435: 				return "Trailing format character %";
436: 			}
437: 			if (i > pos) {
438: 				// push the previous string to the current literal
439: 				current_literal += format_string.substr(pos, i - pos);
440: 			}
441: 			char format_char = format_string[++i];
442: 			if (format_char == '%') {
443: 				// special case: %%
444: 				// set the pos for the next literal and continue
445: 				pos = i;
446: 				continue;
447: 			}
448: 			StrTimeSpecifier specifier;
449: 			if (format_char == '-' && i + 1 < format_string.size()) {
450: 				format_char = format_string[++i];
451: 				switch (format_char) {
452: 				case 'd':
453: 					specifier = StrTimeSpecifier::DAY_OF_MONTH;
454: 					break;
455: 				case 'm':
456: 					specifier = StrTimeSpecifier::MONTH_DECIMAL;
457: 					break;
458: 				case 'y':
459: 					specifier = StrTimeSpecifier::YEAR_WITHOUT_CENTURY;
460: 					break;
461: 				case 'H':
462: 					specifier = StrTimeSpecifier::HOUR_24_DECIMAL;
463: 					break;
464: 				case 'I':
465: 					specifier = StrTimeSpecifier::HOUR_12_DECIMAL;
466: 					break;
467: 				case 'M':
468: 					specifier = StrTimeSpecifier::MINUTE_DECIMAL;
469: 					break;
470: 				case 'S':
471: 					specifier = StrTimeSpecifier::SECOND_DECIMAL;
472: 					break;
473: 				case 'j':
474: 					specifier = StrTimeSpecifier::DAY_OF_YEAR_DECIMAL;
475: 					break;
476: 				default:
477: 					return "Unrecognized format for strftime/strptime: %-" + string(1, format_char);
478: 				}
479: 			} else {
480: 				switch (format_char) {
481: 				case 'a':
482: 					specifier = StrTimeSpecifier::ABBREVIATED_WEEKDAY_NAME;
483: 					break;
484: 				case 'A':
485: 					specifier = StrTimeSpecifier::FULL_WEEKDAY_NAME;
486: 					break;
487: 				case 'w':
488: 					specifier = StrTimeSpecifier::WEEKDAY_DECIMAL;
489: 					break;
490: 				case 'd':
491: 					specifier = StrTimeSpecifier::DAY_OF_MONTH_PADDED;
492: 					break;
493: 				case 'h':
494: 				case 'b':
495: 					specifier = StrTimeSpecifier::ABBREVIATED_MONTH_NAME;
496: 					break;
497: 				case 'B':
498: 					specifier = StrTimeSpecifier::FULL_MONTH_NAME;
499: 					break;
500: 				case 'm':
501: 					specifier = StrTimeSpecifier::MONTH_DECIMAL_PADDED;
502: 					break;
503: 				case 'y':
504: 					specifier = StrTimeSpecifier::YEAR_WITHOUT_CENTURY_PADDED;
505: 					break;
506: 				case 'Y':
507: 					specifier = StrTimeSpecifier::YEAR_DECIMAL;
508: 					break;
509: 				case 'H':
510: 					specifier = StrTimeSpecifier::HOUR_24_PADDED;
511: 					break;
512: 				case 'I':
513: 					specifier = StrTimeSpecifier::HOUR_12_PADDED;
514: 					break;
515: 				case 'p':
516: 					specifier = StrTimeSpecifier::AM_PM;
517: 					break;
518: 				case 'M':
519: 					specifier = StrTimeSpecifier::MINUTE_PADDED;
520: 					break;
521: 				case 'S':
522: 					specifier = StrTimeSpecifier::SECOND_PADDED;
523: 					break;
524: 				case 'f':
525: 					specifier = StrTimeSpecifier::MICROSECOND_PADDED;
526: 					break;
527: 				case 'g':
528: 					specifier = StrTimeSpecifier::MILLISECOND_PADDED;
529: 					break;
530: 				case 'z':
531: 					specifier = StrTimeSpecifier::UTC_OFFSET;
532: 					break;
533: 				case 'Z':
534: 					specifier = StrTimeSpecifier::TZ_NAME;
535: 					break;
536: 				case 'j':
537: 					specifier = StrTimeSpecifier::DAY_OF_YEAR_PADDED;
538: 					break;
539: 				case 'U':
540: 					specifier = StrTimeSpecifier::WEEK_NUMBER_PADDED_SUN_FIRST;
541: 					break;
542: 				case 'W':
543: 					specifier = StrTimeSpecifier::WEEK_NUMBER_PADDED_MON_FIRST;
544: 					break;
545: 				case 'c':
546: 				case 'x':
547: 				case 'X':
548: 				case 'T': {
549: 					string subformat;
550: 					if (format_char == 'c') {
551: 						// %c: Locale’s appropriate date and time representation.
552: 						// we push the ISO timestamp representation here
553: 						subformat = "%Y-%m-%d %H:%M:%S";
554: 					} else if (format_char == 'x') {
555: 						// %x - Locale’s appropriate date representation.
556: 						// we push the ISO date format here
557: 						subformat = "%Y-%m-%d";
558: 					} else if (format_char == 'X' || format_char == 'T') {
559: 						// %X - Locale’s appropriate time representation.
560: 						// we push the ISO time format here
561: 						subformat = "%H:%M:%S";
562: 					}
563: 					// parse the subformat in a separate format specifier
564: 					StrfTimeFormat locale_format;
565: 					string error = StrTimeFormat::ParseFormatSpecifier(subformat, locale_format);
566: 					D_ASSERT(error.empty());
567: 					// add the previous literal to the first literal of the subformat
568: 					locale_format.literals[0] = move(current_literal) + locale_format.literals[0];
569: 					current_literal = "";
570: 					// now push the subformat into the current format specifier
571: 					for (idx_t i = 0; i < locale_format.specifiers.size(); i++) {
572: 						format.AddFormatSpecifier(move(locale_format.literals[i]), locale_format.specifiers[i]);
573: 					}
574: 					pos = i + 1;
575: 					continue;
576: 				}
577: 				default:
578: 					return "Unrecognized format for strftime/strptime: %" + string(1, format_char);
579: 				}
580: 			}
581: 			format.AddFormatSpecifier(move(current_literal), specifier);
582: 			current_literal = "";
583: 			pos = i + 1;
584: 		}
585: 	}
586: 	// add the final literal
587: 	if (pos < format_string.size()) {
588: 		current_literal += format_string.substr(pos, format_string.size() - pos);
589: 	}
590: 	format.AddLiteral(move(current_literal));
591: 	return string();
592: }
593: 
594: struct StrfTimeBindData : public FunctionData {
595: 	explicit StrfTimeBindData(StrfTimeFormat format_p, string format_string_p)
596: 	    : format(move(format_p)), format_string(move(format_string_p)) {
597: 	}
598: 
599: 	StrfTimeFormat format;
600: 	string format_string;
601: 
602: 	unique_ptr<FunctionData> Copy() const override {
603: 		return make_unique<StrfTimeBindData>(format, format_string);
604: 	}
605: 
606: 	bool Equals(const FunctionData &other_p) const override {
607: 		auto &other = (const StrfTimeBindData &)other_p;
608: 		return format_string == other.format_string;
609: 	}
610: };
611: 
612: template <bool REVERSED>
613: static unique_ptr<FunctionData> StrfTimeBindFunction(ClientContext &context, ScalarFunction &bound_function,
614:                                                      vector<unique_ptr<Expression>> &arguments) {
615: 	if (!arguments[REVERSED ? 0 : 1]->IsFoldable()) {
616: 		throw InvalidInputException("strftime format must be a constant");
617: 	}
618: 	Value options_str = ExpressionExecutor::EvaluateScalar(*arguments[REVERSED ? 0 : 1]);
619: 	auto format_string = options_str.GetValue<string>();
620: 	StrfTimeFormat format;
621: 	if (!options_str.IsNull()) {
622: 		string error = StrTimeFormat::ParseFormatSpecifier(format_string, format);
623: 		if (!error.empty()) {
624: 			throw InvalidInputException("Failed to parse format specifier %s: %s", format_string, error);
625: 		}
626: 	}
627: 	return make_unique<StrfTimeBindData>(format, format_string);
628: }
629: 
630: template <bool REVERSED>
631: static void StrfTimeFunctionDate(DataChunk &args, ExpressionState &state, Vector &result) {
632: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
633: 	auto &info = (StrfTimeBindData &)*func_expr.bind_info;
634: 
635: 	if (ConstantVector::IsNull(args.data[REVERSED ? 0 : 1])) {
636: 		result.SetVectorType(VectorType::CONSTANT_VECTOR);
637: 		ConstantVector::SetNull(result, true);
638: 		return;
639: 	}
640: 	UnaryExecutor::ExecuteWithNulls<date_t, string_t>(
641: 	    args.data[REVERSED ? 1 : 0], result, args.size(), [&](date_t input, ValidityMask &mask, idx_t idx) {
642: 		    if (Date::IsFinite(input)) {
643: 			    dtime_t time(0);
644: 			    idx_t len = info.format.GetLength(input, time, 0, nullptr);
645: 			    string_t target = StringVector::EmptyString(result, len);
646: 			    info.format.FormatString(input, time, target.GetDataWriteable());
647: 			    target.Finalize();
648: 			    return target;
649: 		    } else {
650: 			    mask.SetInvalid(idx);
651: 			    return string_t();
652: 		    }
653: 	    });
654: }
655: 
656: template <bool REVERSED>
657: static void StrfTimeFunctionTimestamp(DataChunk &args, ExpressionState &state, Vector &result) {
658: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
659: 	auto &info = (StrfTimeBindData &)*func_expr.bind_info;
660: 
661: 	if (ConstantVector::IsNull(args.data[REVERSED ? 0 : 1])) {
662: 		result.SetVectorType(VectorType::CONSTANT_VECTOR);
663: 		ConstantVector::SetNull(result, true);
664: 		return;
665: 	}
666: 
667: 	UnaryExecutor::ExecuteWithNulls<timestamp_t, string_t>(
668: 	    args.data[REVERSED ? 1 : 0], result, args.size(), [&](timestamp_t input, ValidityMask &mask, idx_t idx) {
669: 		    if (Timestamp::IsFinite(input)) {
670: 			    date_t date;
671: 			    dtime_t time;
672: 			    Timestamp::Convert(input, date, time);
673: 			    idx_t len = info.format.GetLength(date, time, 0, nullptr);
674: 			    string_t target = StringVector::EmptyString(result, len);
675: 			    info.format.FormatString(date, time, target.GetDataWriteable());
676: 			    target.Finalize();
677: 			    return target;
678: 		    } else {
679: 			    mask.SetInvalid(idx);
680: 			    return string_t();
681: 		    }
682: 	    });
683: }
684: 
685: void StrfTimeFun::RegisterFunction(BuiltinFunctions &set) {
686: 	ScalarFunctionSet strftime("strftime");
687: 
688: 	strftime.AddFunction(ScalarFunction({LogicalType::DATE, LogicalType::VARCHAR}, LogicalType::VARCHAR,
689: 	                                    StrfTimeFunctionDate<false>, false, false, StrfTimeBindFunction<false>));
690: 
691: 	strftime.AddFunction(ScalarFunction({LogicalType::TIMESTAMP, LogicalType::VARCHAR}, LogicalType::VARCHAR,
692: 	                                    StrfTimeFunctionTimestamp<false>, false, false, StrfTimeBindFunction<false>));
693: 
694: 	strftime.AddFunction(ScalarFunction({LogicalType::VARCHAR, LogicalType::DATE}, LogicalType::VARCHAR,
695: 	                                    StrfTimeFunctionDate<true>, false, false, StrfTimeBindFunction<true>));
696: 
697: 	strftime.AddFunction(ScalarFunction({LogicalType::VARCHAR, LogicalType::TIMESTAMP}, LogicalType::VARCHAR,
698: 	                                    StrfTimeFunctionTimestamp<true>, false, false, StrfTimeBindFunction<true>));
699: 
700: 	set.AddFunction(strftime);
701: }
702: 
703: void StrpTimeFormat::AddFormatSpecifier(string preceding_literal, StrTimeSpecifier specifier) {
704: 	numeric_width.push_back(NumericSpecifierWidth(specifier));
705: 	StrTimeFormat::AddFormatSpecifier(move(preceding_literal), specifier);
706: }
707: 
708: int StrpTimeFormat::NumericSpecifierWidth(StrTimeSpecifier specifier) {
709: 	switch (specifier) {
710: 	case StrTimeSpecifier::WEEKDAY_DECIMAL:
711: 		return 1;
712: 	case StrTimeSpecifier::DAY_OF_MONTH_PADDED:
713: 	case StrTimeSpecifier::DAY_OF_MONTH:
714: 	case StrTimeSpecifier::MONTH_DECIMAL_PADDED:
715: 	case StrTimeSpecifier::MONTH_DECIMAL:
716: 	case StrTimeSpecifier::YEAR_WITHOUT_CENTURY_PADDED:
717: 	case StrTimeSpecifier::YEAR_WITHOUT_CENTURY:
718: 	case StrTimeSpecifier::HOUR_24_PADDED:
719: 	case StrTimeSpecifier::HOUR_24_DECIMAL:
720: 	case StrTimeSpecifier::HOUR_12_PADDED:
721: 	case StrTimeSpecifier::HOUR_12_DECIMAL:
722: 	case StrTimeSpecifier::MINUTE_PADDED:
723: 	case StrTimeSpecifier::MINUTE_DECIMAL:
724: 	case StrTimeSpecifier::SECOND_PADDED:
725: 	case StrTimeSpecifier::SECOND_DECIMAL:
726: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_SUN_FIRST:
727: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_MON_FIRST:
728: 		return 2;
729: 	case StrTimeSpecifier::MILLISECOND_PADDED:
730: 	case StrTimeSpecifier::DAY_OF_YEAR_PADDED:
731: 	case StrTimeSpecifier::DAY_OF_YEAR_DECIMAL:
732: 		return 3;
733: 	case StrTimeSpecifier::YEAR_DECIMAL:
734: 		return 4;
735: 	case StrTimeSpecifier::MICROSECOND_PADDED:
736: 		return 6;
737: 	default:
738: 		return -1;
739: 	}
740: }
741: 
742: enum class TimeSpecifierAMOrPM : uint8_t { TIME_SPECIFIER_NONE = 0, TIME_SPECIFIER_AM = 1, TIME_SPECIFIER_PM = 2 };
743: 
744: int32_t StrpTimeFormat::TryParseCollection(const char *data, idx_t &pos, idx_t size, const string_t collection[],
745:                                            idx_t collection_count) {
746: 	for (idx_t c = 0; c < collection_count; c++) {
747: 		auto &entry = collection[c];
748: 		auto entry_data = entry.GetDataUnsafe();
749: 		auto entry_size = entry.GetSize();
750: 		// check if this entry matches
751: 		if (pos + entry_size > size) {
752: 			// too big: can't match
753: 			continue;
754: 		}
755: 		// compare the characters
756: 		idx_t i;
757: 		for (i = 0; i < entry_size; i++) {
758: 			if (std::tolower(entry_data[i]) != std::tolower(data[pos + i])) {
759: 				break;
760: 			}
761: 		}
762: 		if (i == entry_size) {
763: 			// full match
764: 			pos += entry_size;
765: 			return c;
766: 		}
767: 	}
768: 	return -1;
769: }
770: 
771: //! Parses a timestamp using the given specifier
772: bool StrpTimeFormat::Parse(string_t str, ParseResult &result) {
773: 	auto &result_data = result.data;
774: 	auto &error_message = result.error_message;
775: 	auto &error_position = result.error_position;
776: 
777: 	// initialize the result
778: 	result_data[0] = 1900;
779: 	result_data[1] = 1;
780: 	result_data[2] = 1;
781: 	result_data[3] = 0;
782: 	result_data[4] = 0;
783: 	result_data[5] = 0;
784: 	result_data[6] = 0;
785: 	result_data[7] = 0;
786: 
787: 	auto data = str.GetDataUnsafe();
788: 	idx_t size = str.GetSize();
789: 	// skip leading spaces
790: 	while (StringUtil::CharacterIsSpace(*data)) {
791: 		data++;
792: 		size--;
793: 	}
794: 	idx_t pos = 0;
795: 	TimeSpecifierAMOrPM ampm = TimeSpecifierAMOrPM::TIME_SPECIFIER_NONE;
796: 
797: 	// Year offset state (Year+W/j)
798: 	auto offset_specifier = StrTimeSpecifier::WEEKDAY_DECIMAL;
799: 	uint64_t weekno = 0;
800: 	uint64_t weekday = 0;
801: 	uint64_t yearday = 0;
802: 
803: 	for (idx_t i = 0;; i++) {
804: 		D_ASSERT(i < literals.size());
805: 		// first compare the literal
806: 		const auto &literal = literals[i];
807: 		for (size_t l = 0; l < literal.size();) {
808: 			// Match runs of spaces to runs of spaces.
809: 			if (StringUtil::CharacterIsSpace(literal[l])) {
810: 				if (!StringUtil::CharacterIsSpace(data[pos])) {
811: 					error_message = "Space does not match, expected " + literals[i];
812: 					error_position = pos;
813: 					return false;
814: 				}
815: 				for (++pos; pos < size && StringUtil::CharacterIsSpace(data[pos]); ++pos) {
816: 					continue;
817: 				}
818: 				for (++l; l < literal.size() && StringUtil::CharacterIsSpace(literal[l]); ++l) {
819: 					continue;
820: 				}
821: 				continue;
822: 			}
823: 			// literal does not match
824: 			if (data[pos++] != literal[l++]) {
825: 				error_message = "Literal does not match, expected " + literal;
826: 				error_position = pos;
827: 				return false;
828: 			}
829: 		}
830: 		if (i == specifiers.size()) {
831: 			break;
832: 		}
833: 		// now parse the specifier
834: 		if (numeric_width[i] > 0) {
835: 			// numeric specifier: parse a number
836: 			uint64_t number = 0;
837: 			size_t start_pos = pos;
838: 			size_t end_pos = start_pos + numeric_width[i];
839: 			while (pos < size && pos < end_pos && StringUtil::CharacterIsDigit(data[pos])) {
840: 				number = number * 10 + data[pos] - '0';
841: 				pos++;
842: 			}
843: 			if (pos == start_pos) {
844: 				// expected a number here
845: 				error_message = "Expected a number";
846: 				error_position = start_pos;
847: 				return false;
848: 			}
849: 			switch (specifiers[i]) {
850: 			case StrTimeSpecifier::DAY_OF_MONTH_PADDED:
851: 			case StrTimeSpecifier::DAY_OF_MONTH:
852: 				if (number < 1 || number > 31) {
853: 					error_message = "Day out of range, expected a value between 1 and 31";
854: 					error_position = start_pos;
855: 					return false;
856: 				}
857: 				// day of the month
858: 				result_data[2] = number;
859: 				offset_specifier = specifiers[i];
860: 				break;
861: 			case StrTimeSpecifier::MONTH_DECIMAL_PADDED:
862: 			case StrTimeSpecifier::MONTH_DECIMAL:
863: 				if (number < 1 || number > 12) {
864: 					error_message = "Month out of range, expected a value between 1 and 12";
865: 					error_position = start_pos;
866: 					return false;
867: 				}
868: 				// month number
869: 				result_data[1] = number;
870: 				offset_specifier = specifiers[i];
871: 				break;
872: 			case StrTimeSpecifier::YEAR_WITHOUT_CENTURY_PADDED:
873: 			case StrTimeSpecifier::YEAR_WITHOUT_CENTURY:
874: 				// year without century..
875: 				// Python uses 69 as a crossover point (i.e. >= 69 is 19.., < 69 is 20..)
876: 				if (number >= 100) {
877: 					// %y only supports numbers between [0..99]
878: 					error_message = "Year without century out of range, expected a value between 0 and 99";
879: 					error_position = start_pos;
880: 					return false;
881: 				}
882: 				if (number >= 69) {
883: 					result_data[0] = int32_t(1900 + number);
884: 				} else {
885: 					result_data[0] = int32_t(2000 + number);
886: 				}
887: 				break;
888: 			case StrTimeSpecifier::YEAR_DECIMAL:
889: 				// year as full number
890: 				result_data[0] = number;
891: 				break;
892: 			case StrTimeSpecifier::HOUR_24_PADDED:
893: 			case StrTimeSpecifier::HOUR_24_DECIMAL:
894: 				if (number >= 24) {
895: 					error_message = "Hour out of range, expected a value between 0 and 23";
896: 					error_position = start_pos;
897: 					return false;
898: 				}
899: 				// hour as full number
900: 				result_data[3] = number;
901: 				break;
902: 			case StrTimeSpecifier::HOUR_12_PADDED:
903: 			case StrTimeSpecifier::HOUR_12_DECIMAL:
904: 				if (number < 1 || number > 12) {
905: 					error_message = "Hour12 out of range, expected a value between 1 and 12";
906: 					error_position = start_pos;
907: 					return false;
908: 				}
909: 				// 12-hour number: start off by just storing the number
910: 				result_data[3] = number;
911: 				break;
912: 			case StrTimeSpecifier::MINUTE_PADDED:
913: 			case StrTimeSpecifier::MINUTE_DECIMAL:
914: 				if (number >= 60) {
915: 					error_message = "Minutes out of range, expected a value between 0 and 59";
916: 					error_position = start_pos;
917: 					return false;
918: 				}
919: 				// minutes
920: 				result_data[4] = number;
921: 				break;
922: 			case StrTimeSpecifier::SECOND_PADDED:
923: 			case StrTimeSpecifier::SECOND_DECIMAL:
924: 				if (number >= 60) {
925: 					error_message = "Seconds out of range, expected a value between 0 and 59";
926: 					error_position = start_pos;
927: 					return false;
928: 				}
929: 				// seconds
930: 				result_data[5] = number;
931: 				break;
932: 			case StrTimeSpecifier::MICROSECOND_PADDED:
933: 				D_ASSERT(number < 1000000ULL); // enforced by the length of the number
934: 				// milliseconds
935: 				result_data[6] = number;
936: 				break;
937: 			case StrTimeSpecifier::MILLISECOND_PADDED:
938: 				D_ASSERT(number < 1000ULL); // enforced by the length of the number
939: 				// milliseconds
940: 				result_data[6] = number * 1000;
941: 				break;
942: 			case StrTimeSpecifier::WEEK_NUMBER_PADDED_SUN_FIRST:
943: 			case StrTimeSpecifier::WEEK_NUMBER_PADDED_MON_FIRST:
944: 				// m/d overrides WU/w but does not conflict
945: 				switch (offset_specifier) {
946: 				case StrTimeSpecifier::DAY_OF_MONTH_PADDED:
947: 				case StrTimeSpecifier::DAY_OF_MONTH:
948: 				case StrTimeSpecifier::MONTH_DECIMAL_PADDED:
949: 				case StrTimeSpecifier::MONTH_DECIMAL:
950: 					// Just validate, don't use
951: 					break;
952: 				case StrTimeSpecifier::WEEKDAY_DECIMAL:
953: 					// First offset specifier
954: 					offset_specifier = specifiers[i];
955: 					break;
956: 				default:
957: 					error_message = "Multiple year offsets specified";
958: 					error_position = start_pos;
959: 					return false;
960: 				}
961: 				if (number > 53) {
962: 					error_message = "Week out of range, expected a value between 0 and 53";
963: 					error_position = start_pos;
964: 					return false;
965: 				}
966: 				weekno = number;
967: 				break;
968: 			case StrTimeSpecifier::WEEKDAY_DECIMAL:
969: 				if (number > 6) {
970: 					error_message = "Weekday out of range, expected a value between 0 and 6";
971: 					error_position = start_pos;
972: 					return false;
973: 				}
974: 				weekday = number;
975: 				break;
976: 			case StrTimeSpecifier::DAY_OF_YEAR_PADDED:
977: 			case StrTimeSpecifier::DAY_OF_YEAR_DECIMAL:
978: 				// m/d overrides j but does not conflict
979: 				switch (offset_specifier) {
980: 				case StrTimeSpecifier::DAY_OF_MONTH_PADDED:
981: 				case StrTimeSpecifier::DAY_OF_MONTH:
982: 				case StrTimeSpecifier::MONTH_DECIMAL_PADDED:
983: 				case StrTimeSpecifier::MONTH_DECIMAL:
984: 					// Just validate, don't use
985: 					break;
986: 				case StrTimeSpecifier::WEEKDAY_DECIMAL:
987: 					// First offset specifier
988: 					offset_specifier = specifiers[i];
989: 					break;
990: 				default:
991: 					error_message = "Multiple year offsets specified";
992: 					error_position = start_pos;
993: 					return false;
994: 				}
995: 				if (number < 1 || number > 366) {
996: 					error_message = "Year day out of range, expected a value between 1 and 366";
997: 					error_position = start_pos;
998: 					return false;
999: 				}
1000: 				yearday = number;
1001: 				break;
1002: 			default:
1003: 				throw NotImplementedException("Unsupported specifier for strptime");
1004: 			}
1005: 		} else {
1006: 			switch (specifiers[i]) {
1007: 			case StrTimeSpecifier::AM_PM: {
1008: 				// parse the next 2 characters
1009: 				if (pos + 2 > size) {
1010: 					// no characters left to parse
1011: 					error_message = "Expected AM/PM";
1012: 					error_position = pos;
1013: 					return false;
1014: 				}
1015: 				char pa_char = char(std::tolower(data[pos]));
1016: 				char m_char = char(std::tolower(data[pos + 1]));
1017: 				if (m_char != 'm') {
1018: 					error_message = "Expected AM/PM";
1019: 					error_position = pos;
1020: 					return false;
1021: 				}
1022: 				if (pa_char == 'p') {
1023: 					ampm = TimeSpecifierAMOrPM::TIME_SPECIFIER_PM;
1024: 				} else if (pa_char == 'a') {
1025: 					ampm = TimeSpecifierAMOrPM::TIME_SPECIFIER_AM;
1026: 				} else {
1027: 					error_message = "Expected AM/PM";
1028: 					error_position = pos;
1029: 					return false;
1030: 				}
1031: 				pos += 2;
1032: 				break;
1033: 			}
1034: 			// we parse weekday names, but we don't use them as information
1035: 			case StrTimeSpecifier::ABBREVIATED_WEEKDAY_NAME:
1036: 				if (TryParseCollection(data, pos, size, Date::DAY_NAMES_ABBREVIATED, 7) < 0) {
1037: 					error_message = "Expected an abbreviated day name (Mon, Tue, Wed, Thu, Fri, Sat, Sun)";
1038: 					error_position = pos;
1039: 					return false;
1040: 				}
1041: 				break;
1042: 			case StrTimeSpecifier::FULL_WEEKDAY_NAME:
1043: 				if (TryParseCollection(data, pos, size, Date::DAY_NAMES, 7) < 0) {
1044: 					error_message = "Expected a full day name (Monday, Tuesday, etc...)";
1045: 					error_position = pos;
1046: 					return false;
1047: 				}
1048: 				break;
1049: 			case StrTimeSpecifier::ABBREVIATED_MONTH_NAME: {
1050: 				int32_t month = TryParseCollection(data, pos, size, Date::MONTH_NAMES_ABBREVIATED, 12);
1051: 				if (month < 0) {
1052: 					error_message = "Expected an abbreviated month name (Jan, Feb, Mar, etc..)";
1053: 					error_position = pos;
1054: 					return false;
1055: 				}
1056: 				result_data[1] = month + 1;
1057: 				break;
1058: 			}
1059: 			case StrTimeSpecifier::FULL_MONTH_NAME: {
1060: 				int32_t month = TryParseCollection(data, pos, size, Date::MONTH_NAMES, 12);
1061: 				if (month < 0) {
1062: 					error_message = "Expected a full month name (January, February, etc...)";
1063: 					error_position = pos;
1064: 					return false;
1065: 				}
1066: 				result_data[1] = month + 1;
1067: 				break;
1068: 			}
1069: 			case StrTimeSpecifier::UTC_OFFSET: {
1070: 				int hour_offset, minute_offset;
1071: 				if (!Timestamp::TryParseUTCOffset(data, pos, size, hour_offset, minute_offset)) {
1072: 					error_message = "Expected +HH[MM] or -HH[MM]";
1073: 					error_position = pos;
1074: 					return false;
1075: 				}
1076: 				result_data[7] = hour_offset * Interval::MINS_PER_HOUR + minute_offset;
1077: 				break;
1078: 			}
1079: 			case StrTimeSpecifier::TZ_NAME: {
1080: 				// skip leading spaces
1081: 				while (pos < size && StringUtil::CharacterIsSpace(data[pos])) {
1082: 					pos++;
1083: 				}
1084: 				const auto tz_begin = data + pos;
1085: 				// stop when we encounter a space or the end of the string
1086: 				while (pos < size && !StringUtil::CharacterIsSpace(data[pos])) {
1087: 					pos++;
1088: 				}
1089: 				const auto tz_end = data + pos;
1090: 				// Can't fully validate without a list - caller's responsibility.
1091: 				// But tz must not be empty.
1092: 				if (tz_end == tz_begin) {
1093: 					error_message = "Empty Time Zone name";
1094: 					error_position = tz_begin - data;
1095: 					return false;
1096: 				}
1097: 				result.tz.assign(tz_begin, tz_end);
1098: 				break;
1099: 			}
1100: 			default:
1101: 				throw NotImplementedException("Unsupported specifier for strptime");
1102: 			}
1103: 		}
1104: 	}
1105: 	// skip trailing spaces
1106: 	while (pos < size && StringUtil::CharacterIsSpace(data[pos])) {
1107: 		pos++;
1108: 	}
1109: 	if (pos != size) {
1110: 		error_message = "Full specifier did not match: trailing characters";
1111: 		error_position = pos;
1112: 		return false;
1113: 	}
1114: 	if (ampm != TimeSpecifierAMOrPM::TIME_SPECIFIER_NONE) {
1115: 		if (result_data[3] > 12) {
1116: 			error_message =
1117: 			    "Invalid hour: " + to_string(result_data[3]) + " AM/PM, expected an hour within the range [0..12]";
1118: 			return false;
1119: 		}
1120: 		// adjust the hours based on the AM or PM specifier
1121: 		if (ampm == TimeSpecifierAMOrPM::TIME_SPECIFIER_AM) {
1122: 			// AM: 12AM=0, 1AM=1, 2AM=2, ..., 11AM=11
1123: 			if (result_data[3] == 12) {
1124: 				result_data[3] = 0;
1125: 			}
1126: 		} else {
1127: 			// PM: 12PM=12, 1PM=13, 2PM=14, ..., 11PM=23
1128: 			if (result_data[3] != 12) {
1129: 				result_data[3] += 12;
1130: 			}
1131: 		}
1132: 	}
1133: 	switch (offset_specifier) {
1134: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_SUN_FIRST:
1135: 	case StrTimeSpecifier::WEEK_NUMBER_PADDED_MON_FIRST: {
1136: 		// Adjust weekday to be 0-based for the week type
1137: 		weekday = (weekday + 7 - int(offset_specifier == StrTimeSpecifier::WEEK_NUMBER_PADDED_MON_FIRST)) % 7;
1138: 		// Get the start of week 1, move back 7 days and then weekno * 7 + weekday gives the date
1139: 		const auto jan1 = Date::FromDate(result_data[0], 1, 1);
1140: 		auto yeardate = Date::GetMondayOfCurrentWeek(jan1);
1141: 		yeardate -= int(offset_specifier == StrTimeSpecifier::WEEK_NUMBER_PADDED_SUN_FIRST);
1142: 		// Is there a week 0?
1143: 		yeardate -= 7 * int(yeardate >= jan1);
1144: 		yeardate += weekno * 7 + weekday;
1145: 		Date::Convert(yeardate, result_data[0], result_data[1], result_data[2]);
1146: 		break;
1147: 	}
1148: 	case StrTimeSpecifier::DAY_OF_YEAR_PADDED:
1149: 	case StrTimeSpecifier::DAY_OF_YEAR_DECIMAL: {
1150: 		auto yeardate = Date::FromDate(result_data[0], 1, 1);
1151: 		yeardate += yearday - 1;
1152: 		Date::Convert(yeardate, result_data[0], result_data[1], result_data[2]);
1153: 		break;
1154: 	}
1155: 	case StrTimeSpecifier::DAY_OF_MONTH_PADDED:
1156: 	case StrTimeSpecifier::DAY_OF_MONTH:
1157: 	case StrTimeSpecifier::MONTH_DECIMAL_PADDED:
1158: 	case StrTimeSpecifier::MONTH_DECIMAL:
1159: 		// m/d overrides UWw/j
1160: 		break;
1161: 	default:
1162: 		D_ASSERT(offset_specifier == StrTimeSpecifier::WEEKDAY_DECIMAL);
1163: 		break;
1164: 	}
1165: 
1166: 	return true;
1167: }
1168: 
1169: struct StrpTimeBindData : public FunctionData {
1170: 	explicit StrpTimeBindData(StrpTimeFormat format_p, string format_string_p)
1171: 	    : format(move(format_p)), format_string(move(format_string_p)) {
1172: 	}
1173: 
1174: 	StrpTimeFormat format;
1175: 	string format_string;
1176: 
1177: 	unique_ptr<FunctionData> Copy() const override {
1178: 		return make_unique<StrpTimeBindData>(format, format_string);
1179: 	}
1180: 
1181: 	bool Equals(const FunctionData &other_p) const override {
1182: 		auto &other = (const StrpTimeBindData &)other_p;
1183: 		return format_string == other.format_string;
1184: 	}
1185: };
1186: 
1187: static unique_ptr<FunctionData> StrpTimeBindFunction(ClientContext &context, ScalarFunction &bound_function,
1188:                                                      vector<unique_ptr<Expression>> &arguments) {
1189: 	if (!arguments[1]->IsFoldable()) {
1190: 		throw InvalidInputException("strptime format must be a constant");
1191: 	}
1192: 	Value options_str = ExpressionExecutor::EvaluateScalar(*arguments[1]);
1193: 	string format_string = options_str.ToString();
1194: 	StrpTimeFormat format;
1195: 	if (!options_str.IsNull()) {
1196: 		if (options_str.type().id() != LogicalTypeId::VARCHAR) {
1197: 			throw InvalidInputException("strptime format must be a string");
1198: 		}
1199: 		format.format_specifier = format_string;
1200: 		string error = StrTimeFormat::ParseFormatSpecifier(format_string, format);
1201: 		if (!error.empty()) {
1202: 			throw InvalidInputException("Failed to parse format specifier %s: %s", format_string, error);
1203: 		}
1204: 		if (format.HasFormatSpecifier(StrTimeSpecifier::UTC_OFFSET)) {
1205: 			bound_function.return_type = LogicalType::TIMESTAMP_TZ;
1206: 		}
1207: 	}
1208: 	return make_unique<StrpTimeBindData>(format, format_string);
1209: }
1210: 
1211: StrpTimeFormat::ParseResult StrpTimeFormat::Parse(const string &format_string, const string &text) {
1212: 	StrpTimeFormat format;
1213: 	format.format_specifier = format_string;
1214: 	string error = StrTimeFormat::ParseFormatSpecifier(format_string, format);
1215: 	if (!error.empty()) {
1216: 		throw InvalidInputException("Failed to parse format specifier %s: %s", format_string, error);
1217: 	}
1218: 	StrpTimeFormat::ParseResult result;
1219: 	if (!format.Parse(text, result)) {
1220: 		throw InvalidInputException("Failed to parse string \"%s\" with format specifier \"%s\"", text, format_string);
1221: 	}
1222: 	return result;
1223: }
1224: 
1225: string StrpTimeFormat::FormatStrpTimeError(const string &input, idx_t position) {
1226: 	if (position == DConstants::INVALID_INDEX) {
1227: 		return string();
1228: 	}
1229: 	return input + "\n" + string(position, ' ') + "^";
1230: }
1231: 
1232: date_t StrpTimeFormat::ParseResult::ToDate() {
1233: 	return Date::FromDate(data[0], data[1], data[2]);
1234: }
1235: 
1236: timestamp_t StrpTimeFormat::ParseResult::ToTimestamp() {
1237: 	date_t date = Date::FromDate(data[0], data[1], data[2]);
1238: 	const auto hour_offset = data[7] / Interval::MINS_PER_HOUR;
1239: 	const auto mins_offset = data[7] % Interval::MINS_PER_HOUR;
1240: 	dtime_t time = Time::FromTime(data[3] - hour_offset, data[4] - mins_offset, data[5], data[6]);
1241: 	return Timestamp::FromDatetime(date, time);
1242: }
1243: 
1244: string StrpTimeFormat::ParseResult::FormatError(string_t input, const string &format_specifier) {
1245: 	return StringUtil::Format("Could not parse string \"%s\" according to format specifier \"%s\"\n%s\nError: %s",
1246: 	                          input.GetString(), format_specifier,
1247: 	                          FormatStrpTimeError(input.GetString(), error_position), error_message);
1248: }
1249: 
1250: bool StrpTimeFormat::TryParseDate(string_t input, date_t &result, string &error_message) {
1251: 	ParseResult parse_result;
1252: 	if (!Parse(input, parse_result)) {
1253: 		error_message = parse_result.FormatError(input, format_specifier);
1254: 		return false;
1255: 	}
1256: 	result = parse_result.ToDate();
1257: 	return true;
1258: }
1259: 
1260: bool StrpTimeFormat::TryParseTimestamp(string_t input, timestamp_t &result, string &error_message) {
1261: 	ParseResult parse_result;
1262: 	if (!Parse(input, parse_result)) {
1263: 		error_message = parse_result.FormatError(input, format_specifier);
1264: 		return false;
1265: 	}
1266: 	result = parse_result.ToTimestamp();
1267: 	return true;
1268: }
1269: 
1270: date_t StrpTimeFormat::ParseDate(string_t input) {
1271: 	ParseResult result;
1272: 	if (!Parse(input, result)) {
1273: 		throw InvalidInputException(result.FormatError(input, format_specifier));
1274: 	}
1275: 	return result.ToDate();
1276: }
1277: 
1278: timestamp_t StrpTimeFormat::ParseTimestamp(string_t input) {
1279: 	ParseResult result;
1280: 	if (!Parse(input, result)) {
1281: 		throw InvalidInputException(result.FormatError(input, format_specifier));
1282: 	}
1283: 	return result.ToTimestamp();
1284: }
1285: 
1286: static void StrpTimeFunction(DataChunk &args, ExpressionState &state, Vector &result) {
1287: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
1288: 	auto &info = (StrpTimeBindData &)*func_expr.bind_info;
1289: 
1290: 	if (ConstantVector::IsNull(args.data[1])) {
1291: 		result.SetVectorType(VectorType::CONSTANT_VECTOR);
1292: 		ConstantVector::SetNull(result, true);
1293: 		return;
1294: 	}
1295: 	UnaryExecutor::Execute<string_t, timestamp_t>(args.data[0], result, args.size(),
1296: 	                                              [&](string_t input) { return info.format.ParseTimestamp(input); });
1297: }
1298: 
1299: void StrpTimeFun::RegisterFunction(BuiltinFunctions &set) {
1300: 	ScalarFunctionSet strptime("strptime");
1301: 
1302: 	strptime.AddFunction(ScalarFunction({LogicalType::VARCHAR, LogicalType::VARCHAR}, LogicalType::TIMESTAMP,
1303: 	                                    StrpTimeFunction, false, false, StrpTimeBindFunction));
1304: 
1305: 	set.AddFunction(strptime);
1306: }
1307: 
1308: } // namespace duckdb
[end of src/function/scalar/date/strftime.cpp]
[start of src/function/table/copy_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/common/serializer/buffered_serializer.hpp"
4: #include "duckdb/function/copy_function.hpp"
5: #include "duckdb/parser/parsed_data/copy_info.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/common/file_system.hpp"
8: #include "duckdb/common/types/string_type.hpp"
9: #include "duckdb/common/vector_operations/vector_operations.hpp"
10: #include "duckdb/function/scalar/string_functions.hpp"
11: #include <limits>
12: 
13: namespace duckdb {
14: 
15: void SubstringDetection(string &str_1, string &str_2, const string &name_str_1, const string &name_str_2) {
16: 	if (str_1.empty() || str_2.empty()) {
17: 		return;
18: 	}
19: 	if (str_1.find(str_2) != string::npos || str_2.find(str_1) != std::string::npos) {
20: 		throw BinderException("%s must not appear in the %s specification and vice versa", name_str_1, name_str_2);
21: 	}
22: }
23: 
24: //===--------------------------------------------------------------------===//
25: // Bind
26: //===--------------------------------------------------------------------===//
27: 
28: void BaseCSVData::Finalize() {
29: 	// verify that the options are correct in the final pass
30: 	if (options.escape.empty()) {
31: 		options.escape = options.quote;
32: 	}
33: 	// escape and delimiter must not be substrings of each other
34: 	if (options.has_delimiter && options.has_escape) {
35: 		SubstringDetection(options.delimiter, options.escape, "DELIMITER", "ESCAPE");
36: 	}
37: 	// delimiter and quote must not be substrings of each other
38: 	if (options.has_quote && options.has_delimiter) {
39: 		SubstringDetection(options.quote, options.delimiter, "DELIMITER", "QUOTE");
40: 	}
41: 	// escape and quote must not be substrings of each other (but can be the same)
42: 	if (options.quote != options.escape && options.has_quote && options.has_escape) {
43: 		SubstringDetection(options.quote, options.escape, "QUOTE", "ESCAPE");
44: 	}
45: 	if (!options.null_str.empty()) {
46: 		// null string and delimiter must not be substrings of each other
47: 		if (options.has_delimiter) {
48: 			SubstringDetection(options.delimiter, options.null_str, "DELIMITER", "NULL");
49: 		}
50: 		// quote/escape and nullstr must not be substrings of each other
51: 		if (options.has_quote) {
52: 			SubstringDetection(options.quote, options.null_str, "QUOTE", "NULL");
53: 		}
54: 		if (options.has_escape) {
55: 			SubstringDetection(options.escape, options.null_str, "ESCAPE", "NULL");
56: 		}
57: 	}
58: }
59: 
60: static Value ConvertVectorToValue(vector<Value> set) {
61: 	if (set.empty()) {
62: 		return Value::EMPTYLIST(LogicalType::BOOLEAN);
63: 	}
64: 	return Value::LIST(move(set));
65: }
66: 
67: static unique_ptr<FunctionData> WriteCSVBind(ClientContext &context, CopyInfo &info, vector<string> &names,
68:                                              vector<LogicalType> &sql_types) {
69: 	auto bind_data = make_unique<WriteCSVData>(info.file_path, sql_types, names);
70: 
71: 	// check all the options in the copy info
72: 	for (auto &option : info.options) {
73: 		auto loption = StringUtil::Lower(option.first);
74: 		auto &set = option.second;
75: 		bind_data->options.SetWriteOption(loption, ConvertVectorToValue(move(set)));
76: 	}
77: 	// verify the parsed options
78: 	if (bind_data->options.force_quote.empty()) {
79: 		// no FORCE_QUOTE specified: initialize to false
80: 		bind_data->options.force_quote.resize(names.size(), false);
81: 	}
82: 	bind_data->Finalize();
83: 	bind_data->is_simple = bind_data->options.delimiter.size() == 1 && bind_data->options.escape.size() == 1 &&
84: 	                       bind_data->options.quote.size() == 1;
85: 	return move(bind_data);
86: }
87: 
88: static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, CopyInfo &info, vector<string> &expected_names,
89:                                             vector<LogicalType> &expected_types) {
90: 	auto bind_data = make_unique<ReadCSVData>();
91: 	bind_data->sql_types = expected_types;
92: 
93: 	string file_pattern = info.file_path;
94: 
95: 	auto &fs = FileSystem::GetFileSystem(context);
96: 	bind_data->files = fs.Glob(file_pattern, context);
97: 	if (bind_data->files.empty()) {
98: 		throw IOException("No files found that match the pattern \"%s\"", file_pattern);
99: 	}
100: 
101: 	auto &options = bind_data->options;
102: 
103: 	// check all the options in the copy info
104: 	for (auto &option : info.options) {
105: 		auto loption = StringUtil::Lower(option.first);
106: 		auto &set = option.second;
107: 		options.SetReadOption(loption, ConvertVectorToValue(move(set)), expected_names);
108: 	}
109: 	// verify the parsed options
110: 	if (options.force_not_null.empty()) {
111: 		// no FORCE_QUOTE specified: initialize to false
112: 		options.force_not_null.resize(expected_types.size(), false);
113: 	}
114: 	bind_data->Finalize();
115: 	return move(bind_data);
116: }
117: 
118: //===--------------------------------------------------------------------===//
119: // Helper writing functions
120: //===--------------------------------------------------------------------===//
121: static string AddEscapes(string &to_be_escaped, const string &escape, const string &val) {
122: 	idx_t i = 0;
123: 	string new_val = "";
124: 	idx_t found = val.find(to_be_escaped);
125: 
126: 	while (found != string::npos) {
127: 		while (i < found) {
128: 			new_val += val[i];
129: 			i++;
130: 		}
131: 		new_val += escape;
132: 		found = val.find(to_be_escaped, found + escape.length());
133: 	}
134: 	while (i < val.length()) {
135: 		new_val += val[i];
136: 		i++;
137: 	}
138: 	return new_val;
139: }
140: 
141: static bool RequiresQuotes(WriteCSVData &csv_data, const char *str, idx_t len) {
142: 	auto &options = csv_data.options;
143: 	// check if the string is equal to the null string
144: 	if (len == options.null_str.size() && memcmp(str, options.null_str.c_str(), len) == 0) {
145: 		return true;
146: 	}
147: 	if (csv_data.is_simple) {
148: 		// simple CSV: check for newlines, quotes and delimiter all at once
149: 		for (idx_t i = 0; i < len; i++) {
150: 			if (str[i] == '\n' || str[i] == '\r' || str[i] == options.quote[0] || str[i] == options.delimiter[0]) {
151: 				// newline, write a quoted string
152: 				return true;
153: 			}
154: 		}
155: 		// no newline, quote or delimiter in the string
156: 		// no quoting or escaping necessary
157: 		return false;
158: 	} else {
159: 		// CSV with complex quotes/delimiter (multiple bytes)
160: 
161: 		// first check for \n, \r, \n\r in string
162: 		for (idx_t i = 0; i < len; i++) {
163: 			if (str[i] == '\n' || str[i] == '\r') {
164: 				// newline, write a quoted string
165: 				return true;
166: 			}
167: 		}
168: 
169: 		// check for delimiter
170: 		if (ContainsFun::Find((const unsigned char *)str, len, (const unsigned char *)options.delimiter.c_str(),
171: 		                      options.delimiter.size()) != DConstants::INVALID_INDEX) {
172: 			return true;
173: 		}
174: 		// check for quote
175: 		if (ContainsFun::Find((const unsigned char *)str, len, (const unsigned char *)options.quote.c_str(),
176: 		                      options.quote.size()) != DConstants::INVALID_INDEX) {
177: 			return true;
178: 		}
179: 		return false;
180: 	}
181: }
182: 
183: static void WriteQuotedString(Serializer &serializer, WriteCSVData &csv_data, const char *str, idx_t len,
184:                               bool force_quote) {
185: 	auto &options = csv_data.options;
186: 	if (!force_quote) {
187: 		// force quote is disabled: check if we need to add quotes anyway
188: 		force_quote = RequiresQuotes(csv_data, str, len);
189: 	}
190: 	if (force_quote) {
191: 		// quoting is enabled: we might need to escape things in the string
192: 		bool requires_escape = false;
193: 		if (csv_data.is_simple) {
194: 			// simple CSV
195: 			// do a single loop to check for a quote or escape value
196: 			for (idx_t i = 0; i < len; i++) {
197: 				if (str[i] == options.quote[0] || str[i] == options.escape[0]) {
198: 					requires_escape = true;
199: 					break;
200: 				}
201: 			}
202: 		} else {
203: 			// complex CSV
204: 			// check for quote or escape separately
205: 			if (ContainsFun::Find((const unsigned char *)str, len, (const unsigned char *)options.quote.c_str(),
206: 			                      options.quote.size()) != DConstants::INVALID_INDEX) {
207: 				requires_escape = true;
208: 			} else if (ContainsFun::Find((const unsigned char *)str, len, (const unsigned char *)options.escape.c_str(),
209: 			                             options.escape.size()) != DConstants::INVALID_INDEX) {
210: 				requires_escape = true;
211: 			}
212: 		}
213: 		if (!requires_escape) {
214: 			// fast path: no need to escape anything
215: 			serializer.WriteBufferData(options.quote);
216: 			serializer.WriteData((const_data_ptr_t)str, len);
217: 			serializer.WriteBufferData(options.quote);
218: 			return;
219: 		}
220: 
221: 		// slow path: need to add escapes
222: 		string new_val(str, len);
223: 		new_val = AddEscapes(options.escape, options.escape, new_val);
224: 		if (options.escape != options.quote) {
225: 			// need to escape quotes separately
226: 			new_val = AddEscapes(options.quote, options.escape, new_val);
227: 		}
228: 		serializer.WriteBufferData(options.quote);
229: 		serializer.WriteBufferData(new_val);
230: 		serializer.WriteBufferData(options.quote);
231: 	} else {
232: 		serializer.WriteData((const_data_ptr_t)str, len);
233: 	}
234: }
235: 
236: //===--------------------------------------------------------------------===//
237: // Sink
238: //===--------------------------------------------------------------------===//
239: struct LocalReadCSVData : public LocalFunctionData {
240: 	//! The thread-local buffer to write data into
241: 	BufferedSerializer serializer;
242: 	//! A chunk with VARCHAR columns to cast intermediates into
243: 	DataChunk cast_chunk;
244: };
245: 
246: struct GlobalWriteCSVData : public GlobalFunctionData {
247: 	GlobalWriteCSVData(FileSystem &fs, const string &file_path, FileOpener *opener, FileCompressionType compression)
248: 	    : fs(fs) {
249: 		handle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW,
250: 		                     FileLockType::WRITE_LOCK, compression, opener);
251: 	}
252: 
253: 	void WriteData(const_data_ptr_t data, idx_t size) {
254: 		lock_guard<mutex> flock(lock);
255: 		handle->Write((void *)data, size);
256: 	}
257: 
258: 	FileSystem &fs;
259: 	//! The mutex for writing to the physical file
260: 	mutex lock;
261: 	//! The file handle to write to
262: 	unique_ptr<FileHandle> handle;
263: };
264: 
265: static unique_ptr<LocalFunctionData> WriteCSVInitializeLocal(ClientContext &context, FunctionData &bind_data) {
266: 	auto &csv_data = (WriteCSVData &)bind_data;
267: 	auto local_data = make_unique<LocalReadCSVData>();
268: 
269: 	// create the chunk with VARCHAR types
270: 	vector<LogicalType> types;
271: 	types.resize(csv_data.options.names.size(), LogicalType::VARCHAR);
272: 
273: 	local_data->cast_chunk.Initialize(types);
274: 	return move(local_data);
275: }
276: 
277: static unique_ptr<GlobalFunctionData> WriteCSVInitializeGlobal(ClientContext &context, FunctionData &bind_data,
278:                                                                const string &file_path) {
279: 	auto &csv_data = (WriteCSVData &)bind_data;
280: 	auto &options = csv_data.options;
281: 	auto global_data = make_unique<GlobalWriteCSVData>(FileSystem::GetFileSystem(context), file_path,
282: 	                                                   FileSystem::GetFileOpener(context), options.compression);
283: 
284: 	if (options.header) {
285: 		BufferedSerializer serializer;
286: 		// write the header line to the file
287: 		for (idx_t i = 0; i < csv_data.options.names.size(); i++) {
288: 			if (i != 0) {
289: 				serializer.WriteBufferData(options.delimiter);
290: 			}
291: 			WriteQuotedString(serializer, csv_data, csv_data.options.names[i].c_str(), csv_data.options.names[i].size(),
292: 			                  false);
293: 		}
294: 		serializer.WriteBufferData(csv_data.newline);
295: 
296: 		global_data->WriteData(serializer.blob.data.get(), serializer.blob.size);
297: 	}
298: 	return move(global_data);
299: }
300: 
301: static void WriteCSVSink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
302:                          LocalFunctionData &lstate, DataChunk &input) {
303: 	auto &csv_data = (WriteCSVData &)bind_data;
304: 	auto &options = csv_data.options;
305: 	auto &local_data = (LocalReadCSVData &)lstate;
306: 	auto &global_state = (GlobalWriteCSVData &)gstate;
307: 
308: 	// write data into the local buffer
309: 
310: 	// first cast the columns of the chunk to varchar
311: 	auto &cast_chunk = local_data.cast_chunk;
312: 	cast_chunk.SetCardinality(input);
313: 	for (idx_t col_idx = 0; col_idx < input.ColumnCount(); col_idx++) {
314: 		if (csv_data.sql_types[col_idx].id() == LogicalTypeId::VARCHAR) {
315: 			// VARCHAR, just create a reference
316: 			cast_chunk.data[col_idx].Reference(input.data[col_idx]);
317: 		} else {
318: 			// non varchar column, perform the cast
319: 			VectorOperations::Cast(input.data[col_idx], cast_chunk.data[col_idx], input.size());
320: 		}
321: 	}
322: 
323: 	cast_chunk.Normalify();
324: 	auto &writer = local_data.serializer;
325: 	// now loop over the vectors and output the values
326: 	for (idx_t row_idx = 0; row_idx < cast_chunk.size(); row_idx++) {
327: 		// write values
328: 		for (idx_t col_idx = 0; col_idx < cast_chunk.ColumnCount(); col_idx++) {
329: 			if (col_idx != 0) {
330: 				writer.WriteBufferData(options.delimiter);
331: 			}
332: 			if (FlatVector::IsNull(cast_chunk.data[col_idx], row_idx)) {
333: 				// write null value
334: 				writer.WriteBufferData(options.null_str);
335: 				continue;
336: 			}
337: 
338: 			// non-null value, fetch the string value from the cast chunk
339: 			auto str_data = FlatVector::GetData<string_t>(cast_chunk.data[col_idx]);
340: 			auto str_value = str_data[row_idx];
341: 			// FIXME: we could gain some performance here by checking for certain types if they ever require quotes
342: 			// (e.g. integers only require quotes if the delimiter is a number, decimals only require quotes if the
343: 			// delimiter is a number or "." character)
344: 			WriteQuotedString(writer, csv_data, str_value.GetDataUnsafe(), str_value.GetSize(),
345: 			                  csv_data.options.force_quote[col_idx]);
346: 		}
347: 		writer.WriteBufferData(csv_data.newline);
348: 	}
349: 	// check if we should flush what we have currently written
350: 	if (writer.blob.size >= csv_data.flush_size) {
351: 		global_state.WriteData(writer.blob.data.get(), writer.blob.size);
352: 		writer.Reset();
353: 	}
354: }
355: 
356: //===--------------------------------------------------------------------===//
357: // Combine
358: //===--------------------------------------------------------------------===//
359: static void WriteCSVCombine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
360:                             LocalFunctionData &lstate) {
361: 	auto &local_data = (LocalReadCSVData &)lstate;
362: 	auto &global_state = (GlobalWriteCSVData &)gstate;
363: 	auto &writer = local_data.serializer;
364: 	// flush the local writer
365: 	if (writer.blob.size > 0) {
366: 		global_state.WriteData(writer.blob.data.get(), writer.blob.size);
367: 		writer.Reset();
368: 	}
369: }
370: 
371: //===--------------------------------------------------------------------===//
372: // Finalize
373: //===--------------------------------------------------------------------===//
374: void WriteCSVFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
375: 	auto &global_state = (GlobalWriteCSVData &)gstate;
376: 
377: 	global_state.handle->Close();
378: 	global_state.handle.reset();
379: }
380: 
381: void CSVCopyFunction::RegisterFunction(BuiltinFunctions &set) {
382: 	CopyFunction info("csv");
383: 	info.copy_to_bind = WriteCSVBind;
384: 	info.copy_to_initialize_local = WriteCSVInitializeLocal;
385: 	info.copy_to_initialize_global = WriteCSVInitializeGlobal;
386: 	info.copy_to_sink = WriteCSVSink;
387: 	info.copy_to_combine = WriteCSVCombine;
388: 	info.copy_to_finalize = WriteCSVFinalize;
389: 
390: 	info.copy_from_bind = ReadCSVBind;
391: 	info.copy_from_function = ReadCSVTableFunction::GetFunction();
392: 
393: 	info.extension = "csv";
394: 
395: 	set.AddFunction(info);
396: }
397: 
398: } // namespace duckdb
[end of src/function/table/copy_csv.cpp]
[start of src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/persistent/buffered_csv_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/parser/parsed_data/copy_info.hpp"
13: #include "duckdb/function/scalar/strftime.hpp"
14: #include "duckdb/common/types/chunk_collection.hpp"
15: #include "duckdb/common/enums/file_compression_type.hpp"
16: #include "duckdb/common/map.hpp"
17: #include "duckdb/common/queue.hpp"
18: 
19: #include <sstream>
20: 
21: namespace duckdb {
22: struct CopyInfo;
23: struct CSVFileHandle;
24: struct FileHandle;
25: struct StrpTimeFormat;
26: 
27: class FileOpener;
28: class FileSystem;
29: 
30: //! The shifts array allows for linear searching of multi-byte values. For each position, it determines the next
31: //! position given that we encounter a byte with the given value.
32: /*! For example, if we have a string "ABAC", the shifts array will have the following values:
33:  *  [0] --> ['A'] = 1, all others = 0
34:  *  [1] --> ['B'] = 2, ['A'] = 1, all others = 0
35:  *  [2] --> ['A'] = 3, all others = 0
36:  *  [3] --> ['C'] = 4 (match), 'B' = 2, 'A' = 1, all others = 0
37:  * Suppose we then search in the following string "ABABAC", our progression will be as follows:
38:  * 'A' -> [1], 'B' -> [2], 'A' -> [3], 'B' -> [2], 'A' -> [3], 'C' -> [4] (match!)
39:  */
40: struct TextSearchShiftArray {
41: 	TextSearchShiftArray();
42: 	explicit TextSearchShiftArray(string search_term);
43: 
44: 	inline bool Match(uint8_t &position, uint8_t byte_value) {
45: 		if (position >= length) {
46: 			return false;
47: 		}
48: 		position = shifts[position * 255 + byte_value];
49: 		return position == length;
50: 	}
51: 
52: 	idx_t length;
53: 	unique_ptr<uint8_t[]> shifts;
54: };
55: 
56: struct BufferedCSVReaderOptions {
57: 	//===--------------------------------------------------------------------===//
58: 	// CommonCSVOptions
59: 	//===--------------------------------------------------------------------===//
60: 
61: 	//! Whether or not a delimiter was defined by the user
62: 	bool has_delimiter = false;
63: 	//! Delimiter to separate columns within each line
64: 	string delimiter = ",";
65: 	//! Whether or not a quote sign was defined by the user
66: 	bool has_quote = false;
67: 	//! Quote used for columns that contain reserved characters, e.g., delimiter
68: 	string quote = "\"";
69: 	//! Whether or not an escape character was defined by the user
70: 	bool has_escape = false;
71: 	//! Escape character to escape quote character
72: 	string escape;
73: 	//! Whether or not a header information was given by the user
74: 	bool has_header = false;
75: 	//! Whether or not the file has a header line
76: 	bool header = false;
77: 	//! Whether or not we should ignore InvalidInput errors
78: 	bool ignore_errors = false;
79: 	//! Expected number of columns
80: 	idx_t num_cols = 0;
81: 	//! Number of samples to buffer
82: 	idx_t buffer_size = STANDARD_VECTOR_SIZE * 100;
83: 	//! Specifies the string that represents a null value
84: 	string null_str;
85: 	//! Whether file is compressed or not, and if so which compression type
86: 	//! AUTO_DETECT (default; infer from file extension)
87: 	FileCompressionType compression = FileCompressionType::AUTO_DETECT;
88: 
89: 	//===--------------------------------------------------------------------===//
90: 	// ReadCSVOptions
91: 	//===--------------------------------------------------------------------===//
92: 
93: 	//! How many leading rows to skip
94: 	idx_t skip_rows = 0;
95: 	//! Maximum CSV line size: specified because if we reach this amount, we likely have wrong delimiters (default: 2MB)
96: 	//! note that this is the guaranteed line length that will succeed, longer lines may be accepted if slightly above
97: 	idx_t maximum_line_size = 2097152;
98: 	//! Whether or not header names shall be normalized
99: 	bool normalize_names = false;
100: 	//! True, if column with that index must skip null check
101: 	vector<bool> force_not_null;
102: 	//! Consider all columns to be of type varchar
103: 	bool all_varchar = false;
104: 	//! Size of sample chunk used for dialect and type detection
105: 	idx_t sample_chunk_size = STANDARD_VECTOR_SIZE;
106: 	//! Number of sample chunks used for type detection
107: 	idx_t sample_chunks = 10;
108: 	//! Whether or not to automatically detect dialect and datatypes
109: 	bool auto_detect = false;
110: 	//! The file path of the CSV file to read
111: 	string file_path;
112: 	//! Whether or not to include a file name column
113: 	bool include_file_name = false;
114: 
115: 	//===--------------------------------------------------------------------===//
116: 	// WriteCSVOptions
117: 	//===--------------------------------------------------------------------===//
118: 
119: 	//! The column names of the columns to write
120: 	vector<string> names;
121: 	//! True, if column with that index must be quoted
122: 	vector<bool> force_quote;
123: 
124: 	//! The date format to use (if any is specified)
125: 	std::map<LogicalTypeId, StrpTimeFormat> date_format = {{LogicalTypeId::DATE, {}}, {LogicalTypeId::TIMESTAMP, {}}};
126: 	//! Whether or not a type format is specified
127: 	std::map<LogicalTypeId, bool> has_format = {{LogicalTypeId::DATE, false}, {LogicalTypeId::TIMESTAMP, false}};
128: 
129: 	void SetDelimiter(const string &delimiter);
130: 	//! Set an option that is supported by both reading and writing functions, called by
131: 	//! the SetReadOption and SetWriteOption methods
132: 	bool SetBaseOption(const string &loption, const Value &value);
133: 
134: 	//! loption - lowercase string
135: 	//! set - argument(s) to the option
136: 	//! expected_names - names expected if the option is "columns"
137: 	void SetReadOption(const string &loption, const Value &value, vector<string> &expected_names);
138: 
139: 	void SetWriteOption(const string &loption, const Value &value);
140: 
141: 	std::string ToString() const;
142: };
143: 
144: enum class ParserMode : uint8_t { PARSING = 0, SNIFFING_DIALECT = 1, SNIFFING_DATATYPES = 2, PARSING_HEADER = 3 };
145: 
146: //! Buffered CSV reader is a class that reads values from a stream and parses them as a CSV file
147: class BufferedCSVReader {
148: 	//! Initial buffer read size; can be extended for long lines
149: 	static constexpr idx_t INITIAL_BUFFER_SIZE = 16384;
150: 	//! Larger buffer size for non disk files
151: 	static constexpr idx_t INITIAL_BUFFER_SIZE_LARGE = 10000000; // 10MB
152: 	ParserMode mode;
153: 
154: public:
155: 	BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options,
156: 	                  const vector<LogicalType> &requested_types = vector<LogicalType>());
157: 
158: 	BufferedCSVReader(FileSystem &fs, FileOpener *opener, BufferedCSVReaderOptions options,
159: 	                  const vector<LogicalType> &requested_types = vector<LogicalType>());
160: 	~BufferedCSVReader();
161: 
162: 	FileSystem &fs;
163: 	FileOpener *opener;
164: 	BufferedCSVReaderOptions options;
165: 	vector<LogicalType> sql_types;
166: 	vector<string> col_names;
167: 	unique_ptr<CSVFileHandle> file_handle;
168: 
169: 	unique_ptr<char[]> buffer;
170: 	idx_t buffer_size;
171: 	idx_t position;
172: 	idx_t start = 0;
173: 
174: 	idx_t linenr = 0;
175: 	bool linenr_estimated = false;
176: 
177: 	vector<idx_t> sniffed_column_counts;
178: 	bool row_empty = false;
179: 	idx_t sample_chunk_idx = 0;
180: 	bool jumping_samples = false;
181: 	bool end_of_file_reached = false;
182: 	bool bom_checked = false;
183: 
184: 	idx_t bytes_in_chunk = 0;
185: 	double bytes_per_line_avg = 0;
186: 
187: 	vector<unique_ptr<char[]>> cached_buffers;
188: 
189: 	TextSearchShiftArray delimiter_search, escape_search, quote_search;
190: 
191: 	DataChunk parse_chunk;
192: 
193: 	std::queue<unique_ptr<DataChunk>> cached_chunks;
194: 
195: public:
196: 	//! Extract a single DataChunk from the CSV file and stores it in insert_chunk
197: 	void ParseCSV(DataChunk &insert_chunk);
198: 
199: 	idx_t GetFileSize();
200: 
201: private:
202: 	//! Initialize Parser
203: 	void Initialize(const vector<LogicalType> &requested_types);
204: 	//! Initializes the parse_chunk with varchar columns and aligns info with new number of cols
205: 	void InitParseChunk(idx_t num_cols);
206: 	//! Initializes the TextSearchShiftArrays for complex parser
207: 	void PrepareComplexParser();
208: 	//! Try to parse a single datachunk from the file. Throws an exception if anything goes wrong.
209: 	void ParseCSV(ParserMode mode);
210: 	//! Try to parse a single datachunk from the file. Returns whether or not the parsing is successful
211: 	bool TryParseCSV(ParserMode mode);
212: 	//! Extract a single DataChunk from the CSV file and stores it in insert_chunk
213: 	bool TryParseCSV(ParserMode mode, DataChunk &insert_chunk, string &error_message);
214: 	//! Sniffs CSV dialect and determines skip rows, header row, column types and column names
215: 	vector<LogicalType> SniffCSV(const vector<LogicalType> &requested_types);
216: 	//! Change the date format for the type to the string
217: 	void SetDateFormat(const string &format_specifier, const LogicalTypeId &sql_type);
218: 	//! Try to cast a string value to the specified sql type
219: 	bool TryCastValue(const Value &value, const LogicalType &sql_type);
220: 	//! Try to cast a vector of values to the specified sql type
221: 	bool TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type);
222: 	//! Skips skip_rows, reads header row from input stream
223: 	void SkipRowsAndReadHeader(idx_t skip_rows, bool skip_header);
224: 	//! Jumps back to the beginning of input stream and resets necessary internal states
225: 	void JumpToBeginning(idx_t skip_rows, bool skip_header);
226: 	//! Jumps back to the beginning of input stream and resets necessary internal states
227: 	bool JumpToNextSample();
228: 	//! Resets the buffer
229: 	void ResetBuffer();
230: 	//! Resets the steam
231: 	void ResetStream();
232: 
233: 	//! Parses a CSV file with a one-byte delimiter, escape and quote character
234: 	bool TryParseSimpleCSV(DataChunk &insert_chunk, string &error_message);
235: 	//! Parses more complex CSV files with multi-byte delimiters, escapes or quotes
236: 	bool TryParseComplexCSV(DataChunk &insert_chunk, string &error_message);
237: 
238: 	//! Adds a value to the current row
239: 	void AddValue(char *str_val, idx_t length, idx_t &column, vector<idx_t> &escape_positions);
240: 	//! Adds a row to the insert_chunk, returns true if the chunk is filled as a result of this row being added
241: 	bool AddRow(DataChunk &insert_chunk, idx_t &column);
242: 	//! Finalizes a chunk, parsing all values that have been added so far and adding them to the insert_chunk
243: 	void Flush(DataChunk &insert_chunk);
244: 	//! Reads a new buffer from the CSV file if the current one has been exhausted
245: 	bool ReadBuffer(idx_t &start);
246: 
247: 	unique_ptr<CSVFileHandle> OpenCSV(const BufferedCSVReaderOptions &options);
248: 
249: 	//! First phase of auto detection: detect CSV dialect (i.e. delimiter, quote rules, etc)
250: 	void DetectDialect(const vector<LogicalType> &requested_types, BufferedCSVReaderOptions &original_options,
251: 	                   vector<BufferedCSVReaderOptions> &info_candidates, idx_t &best_num_cols);
252: 	//! Second phase of auto detection: detect candidate types for each column
253: 	void DetectCandidateTypes(const vector<LogicalType> &type_candidates,
254: 	                          const map<LogicalTypeId, vector<const char *>> &format_template_candidates,
255: 	                          const vector<BufferedCSVReaderOptions> &info_candidates,
256: 	                          BufferedCSVReaderOptions &original_options, idx_t best_num_cols,
257: 	                          vector<vector<LogicalType>> &best_sql_types_candidates,
258: 	                          std::map<LogicalTypeId, vector<string>> &best_format_candidates,
259: 	                          DataChunk &best_header_row);
260: 	//! Third phase of auto detection: detect header of CSV file
261: 	void DetectHeader(const vector<vector<LogicalType>> &best_sql_types_candidates, const DataChunk &best_header_row);
262: 	//! Fourth phase of auto detection: refine the types of each column and select which types to use for each column
263: 	vector<LogicalType> RefineTypeDetection(const vector<LogicalType> &type_candidates,
264: 	                                        const vector<LogicalType> &requested_types,
265: 	                                        vector<vector<LogicalType>> &best_sql_types_candidates,
266: 	                                        map<LogicalTypeId, vector<string>> &best_format_candidates);
267: 
268: private:
269: 	//! Whether or not the current row's columns have overflown sql_types.size()
270: 	bool error_column_overflow = false;
271: };
272: 
273: } // namespace duckdb
[end of src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp]
[start of src/include/duckdb/function/scalar/strftime.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/scalar/strftime.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/function/scalar_function.hpp"
12: #include "duckdb/common/vector.hpp"
13: 
14: #include <algorithm>
15: 
16: namespace duckdb {
17: 
18: enum class StrTimeSpecifier : uint8_t {
19: 	ABBREVIATED_WEEKDAY_NAME = 0,    // %a - Abbreviated weekday name. (Sun, Mon, ...)
20: 	FULL_WEEKDAY_NAME = 1,           // %A Full weekday name. (Sunday, Monday, ...)
21: 	WEEKDAY_DECIMAL = 2,             // %w - Weekday as a decimal number. (0, 1, ..., 6)
22: 	DAY_OF_MONTH_PADDED = 3,         // %d - Day of the month as a zero-padded decimal. (01, 02, ..., 31)
23: 	DAY_OF_MONTH = 4,                // %-d - Day of the month as a decimal number. (1, 2, ..., 30)
24: 	ABBREVIATED_MONTH_NAME = 5,      // %b - Abbreviated month name. (Jan, Feb, ..., Dec)
25: 	FULL_MONTH_NAME = 6,             // %B - Full month name. (January, February, ...)
26: 	MONTH_DECIMAL_PADDED = 7,        // %m - Month as a zero-padded decimal number. (01, 02, ..., 12)
27: 	MONTH_DECIMAL = 8,               // %-m - Month as a decimal number. (1, 2, ..., 12)
28: 	YEAR_WITHOUT_CENTURY_PADDED = 9, // %y - Year without century as a zero-padded decimal number. (00, 01, ..., 99)
29: 	YEAR_WITHOUT_CENTURY = 10,       // %-y - Year without century as a decimal number. (0, 1, ..., 99)
30: 	YEAR_DECIMAL = 11,               // %Y - Year with century as a decimal number. (2013, 2019 etc.)
31: 	HOUR_24_PADDED = 12,             // %H - Hour (24-hour clock) as a zero-padded decimal number. (00, 01, ..., 23)
32: 	HOUR_24_DECIMAL = 13,            // %-H - Hour (24-hour clock) as a decimal number. (0, 1, ..., 23)
33: 	HOUR_12_PADDED = 14,             // %I - Hour (12-hour clock) as a zero-padded decimal number. (01, 02, ..., 12)
34: 	HOUR_12_DECIMAL = 15,            // %-I - Hour (12-hour clock) as a decimal number. (1, 2, ... 12)
35: 	AM_PM = 16,                      // %p - Locale’s AM or PM. (AM, PM)
36: 	MINUTE_PADDED = 17,              // %M - Minute as a zero-padded decimal number. (00, 01, ..., 59)
37: 	MINUTE_DECIMAL = 18,             // %-M - Minute as a decimal number. (0, 1, ..., 59)
38: 	SECOND_PADDED = 19,              // %S - Second as a zero-padded decimal number. (00, 01, ..., 59)
39: 	SECOND_DECIMAL = 20,             // %-S - Second as a decimal number. (0, 1, ..., 59)
40: 	MICROSECOND_PADDED = 21,         // %f - Microsecond as a decimal number, zero-padded on the left. (000000 - 999999)
41: 	MILLISECOND_PADDED = 22,         // %g - Millisecond as a decimal number, zero-padded on the left. (000 - 999)
42: 	UTC_OFFSET = 23,                 // %z - UTC offset in the form +HHMM or -HHMM. ( )
43: 	TZ_NAME = 24,                    // %Z - Time zone name. ( )
44: 	DAY_OF_YEAR_PADDED = 25,         // %j - Day of the year as a zero-padded decimal number. (001, 002, ..., 366)
45: 	DAY_OF_YEAR_DECIMAL = 26,        // %-j - Day of the year as a decimal number. (1, 2, ..., 366)
46: 	WEEK_NUMBER_PADDED_SUN_FIRST =
47: 	    27, // %U - Week number of the year (Sunday as the first day of the week). All days in a new year preceding the
48: 	        // first Sunday are considered to be in week 0. (00, 01, ..., 53)
49: 	WEEK_NUMBER_PADDED_MON_FIRST =
50: 	    28, // %W - Week number of the year (Monday as the first day of the week). All days in a new year preceding the
51: 	        // first Monday are considered to be in week 0. (00, 01, ..., 53)
52: 	LOCALE_APPROPRIATE_DATE_AND_TIME =
53: 	    29, // %c - Locale’s appropriate date and time representation. (Mon Sep 30 07:06:05 2013)
54: 	LOCALE_APPROPRIATE_DATE = 30, // %x - Locale’s appropriate date representation. (09/30/13)
55: 	LOCALE_APPROPRIATE_TIME = 31  // %X - Locale’s appropriate time representation. (07:06:05)
56: };
57: 
58: struct StrTimeFormat {
59: public:
60: 	virtual ~StrTimeFormat() {
61: 	}
62: 
63: 	DUCKDB_API static string ParseFormatSpecifier(const string &format_string, StrTimeFormat &format);
64: 
65: 	inline bool HasFormatSpecifier(StrTimeSpecifier s) const {
66: 		return std::find(specifiers.begin(), specifiers.end(), s) != specifiers.end();
67: 	}
68: 
69: protected:
70: 	//! The format specifiers
71: 	vector<StrTimeSpecifier> specifiers;
72: 	//! The literals that appear in between the format specifiers
73: 	//! The following must hold: literals.size() = specifiers.size() + 1
74: 	//! Format is literals[0], specifiers[0], literals[1], ..., specifiers[n - 1], literals[n]
75: 	vector<string> literals;
76: 	//! The constant size that appears in the format string
77: 	idx_t constant_size = 0;
78: 	//! The max numeric width of the specifier (if it is parsed as a number), or -1 if it is not a number
79: 	vector<int> numeric_width;
80: 
81: protected:
82: 	void AddLiteral(string literal);
83: 	DUCKDB_API virtual void AddFormatSpecifier(string preceding_literal, StrTimeSpecifier specifier);
84: };
85: 
86: struct StrfTimeFormat : public StrTimeFormat {
87: 	DUCKDB_API idx_t GetLength(date_t date, dtime_t time, int32_t utc_offset, const char *tz_name);
88: 
89: 	DUCKDB_API void FormatString(date_t date, int32_t data[8], const char *tz_name, char *target);
90: 	void FormatString(date_t date, dtime_t time, char *target);
91: 
92: 	DUCKDB_API static string Format(timestamp_t timestamp, const string &format);
93: 
94: protected:
95: 	//! The variable-length specifiers. To determine total string size, these need to be checked.
96: 	vector<StrTimeSpecifier> var_length_specifiers;
97: 	//! Whether or not the current specifier is a special "date" specifier (i.e. one that requires a date_t object to
98: 	//! generate)
99: 	vector<bool> is_date_specifier;
100: 
101: protected:
102: 	DUCKDB_API void AddFormatSpecifier(string preceding_literal, StrTimeSpecifier specifier) override;
103: 	static idx_t GetSpecifierLength(StrTimeSpecifier specifier, date_t date, dtime_t time, int32_t utc_offset,
104: 	                                const char *tz_name);
105: 	char *WriteString(char *target, const string_t &str);
106: 	char *Write2(char *target, uint8_t value);
107: 	char *WritePadded2(char *target, uint32_t value);
108: 	char *WritePadded3(char *target, uint32_t value);
109: 	char *WritePadded(char *target, uint32_t value, size_t padding);
110: 	bool IsDateSpecifier(StrTimeSpecifier specifier);
111: 	char *WriteDateSpecifier(StrTimeSpecifier specifier, date_t date, char *target);
112: 	char *WriteStandardSpecifier(StrTimeSpecifier specifier, int32_t data[], const char *tz_name, char *target);
113: };
114: 
115: struct StrpTimeFormat : public StrTimeFormat {
116: public:
117: 	//! Type-safe parsing argument
118: 	struct ParseResult {
119: 		int32_t data[8]; // year, month, day, hour, min, sec, µs, offset
120: 		string tz;
121: 		string error_message;
122: 		idx_t error_position = DConstants::INVALID_INDEX;
123: 
124: 		date_t ToDate();
125: 		timestamp_t ToTimestamp();
126: 		DUCKDB_API string FormatError(string_t input, const string &format_specifier);
127: 	};
128: 
129: public:
130: 	//! The full format specifier, for error messages
131: 	string format_specifier;
132: 
133: public:
134: 	DUCKDB_API static ParseResult Parse(const string &format, const string &text);
135: 
136: 	DUCKDB_API bool Parse(string_t str, ParseResult &result);
137: 
138: 	bool TryParseDate(string_t str, date_t &result, string &error_message);
139: 	bool TryParseTimestamp(string_t str, timestamp_t &result, string &error_message);
140: 
141: 	date_t ParseDate(string_t str);
142: 	timestamp_t ParseTimestamp(string_t str);
143: 
144: protected:
145: 	static string FormatStrpTimeError(const string &input, idx_t position);
146: 	DUCKDB_API void AddFormatSpecifier(string preceding_literal, StrTimeSpecifier specifier) override;
147: 	int NumericSpecifierWidth(StrTimeSpecifier specifier);
148: 	int32_t TryParseCollection(const char *data, idx_t &pos, idx_t size, const string_t collection[],
149: 	                           idx_t collection_count);
150: };
151: 
152: } // namespace duckdb
[end of src/include/duckdb/function/scalar/strftime.hpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: