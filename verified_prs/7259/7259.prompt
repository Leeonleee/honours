You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Wrong update count in JDBC when INSERT .. ON CONFLICT DO NOTHING statement does nothing
### What happens?

This may or may not be a bug (if it's not, then I suggest documenting this behaviour).

When running an `INSERT .. ON CONFLICT DO NOTHING` statement, which produces a conflict and thus does nothing, I would expect the resulting update count to be `0`, not `1`

### To Reproduce

```java
try (Statement s = connection.createStatement()) {
    try {
        s.executeUpdate("create table t (i int primary key, j int)");

        System.out.println(s.executeUpdate("insert into t values (1, 1) on conflict do nothing"));
        System.out.println(s.executeUpdate("insert into t values (1, 2) on conflict do nothing"));
    }
    finally {
        s.executeUpdate("drop table t");
    }
}
```

This prints:

```
1
1
```

But I would have expected:

```
1
0
```

### OS:

Microsoft Windows [Version 10.0.22621.1555]

### DuckDB Version:

v0.7.2-dev2430

### DuckDB Client:

JDBC

### Full Name:

Lukas Eder

### Affiliation:

Data Geekery / jOOQ

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
11:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
12:   </a>
13:   <a href="https://discord.gg/tcvwpjfnZx">
14:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
15:   </a>
16:   <a href="https://github.com/duckdb/duckdb/releases/">
17:     <img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release">
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of src/execution/operator/persistent/physical_insert.cpp]
1: #include "duckdb/execution/operator/persistent/physical_insert.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
4: #include "duckdb/common/types/column/column_data_collection.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/execution/expression_executor.hpp"
7: #include "duckdb/storage/data_table.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/parser/parsed_data/create_table_info.hpp"
10: #include "duckdb/planner/expression/bound_constant_expression.hpp"
11: #include "duckdb/storage/table_io_manager.hpp"
12: #include "duckdb/transaction/local_storage.hpp"
13: #include "duckdb/parser/statement/insert_statement.hpp"
14: #include "duckdb/parser/statement/update_statement.hpp"
15: #include "duckdb/storage/table/scan_state.hpp"
16: #include "duckdb/common/types/conflict_manager.hpp"
17: #include "duckdb/execution/index/art/art.hpp"
18: #include "duckdb/transaction/duck_transaction.hpp"
19: #include "duckdb/storage/table/append_state.hpp"
20: 
21: namespace duckdb {
22: 
23: PhysicalInsert::PhysicalInsert(vector<LogicalType> types_p, TableCatalogEntry &table,
24:                                physical_index_vector_t<idx_t> column_index_map,
25:                                vector<unique_ptr<Expression>> bound_defaults,
26:                                vector<unique_ptr<Expression>> set_expressions, vector<PhysicalIndex> set_columns,
27:                                vector<LogicalType> set_types, idx_t estimated_cardinality, bool return_chunk,
28:                                bool parallel, OnConflictAction action_type,
29:                                unique_ptr<Expression> on_conflict_condition_p,
30:                                unique_ptr<Expression> do_update_condition_p, unordered_set<column_t> conflict_target_p,
31:                                vector<column_t> columns_to_fetch_p)
32:     : PhysicalOperator(PhysicalOperatorType::INSERT, std::move(types_p), estimated_cardinality),
33:       column_index_map(std::move(column_index_map)), insert_table(&table), insert_types(table.GetTypes()),
34:       bound_defaults(std::move(bound_defaults)), return_chunk(return_chunk), parallel(parallel),
35:       action_type(action_type), set_expressions(std::move(set_expressions)), set_columns(std::move(set_columns)),
36:       set_types(std::move(set_types)), on_conflict_condition(std::move(on_conflict_condition_p)),
37:       do_update_condition(std::move(do_update_condition_p)), conflict_target(std::move(conflict_target_p)),
38:       columns_to_fetch(std::move(columns_to_fetch_p)) {
39: 
40: 	if (action_type == OnConflictAction::THROW) {
41: 		return;
42: 	}
43: 
44: 	D_ASSERT(set_expressions.size() == set_columns.size());
45: 
46: 	// One or more columns are referenced from the existing table,
47: 	// we use the 'insert_types' to figure out which types these columns have
48: 	types_to_fetch = vector<LogicalType>(columns_to_fetch.size(), LogicalType::SQLNULL);
49: 	for (idx_t i = 0; i < columns_to_fetch.size(); i++) {
50: 		auto &id = columns_to_fetch[i];
51: 		D_ASSERT(id < insert_types.size());
52: 		types_to_fetch[i] = insert_types[id];
53: 	}
54: }
55: 
56: PhysicalInsert::PhysicalInsert(LogicalOperator &op, SchemaCatalogEntry &schema, unique_ptr<BoundCreateTableInfo> info_p,
57:                                idx_t estimated_cardinality, bool parallel)
58:     : PhysicalOperator(PhysicalOperatorType::CREATE_TABLE_AS, op.types, estimated_cardinality), insert_table(nullptr),
59:       return_chunk(false), schema(&schema), info(std::move(info_p)), parallel(parallel),
60:       action_type(OnConflictAction::THROW) {
61: 	GetInsertInfo(*info, insert_types, bound_defaults);
62: }
63: 
64: void PhysicalInsert::GetInsertInfo(const BoundCreateTableInfo &info, vector<LogicalType> &insert_types,
65:                                    vector<unique_ptr<Expression>> &bound_defaults) {
66: 	auto &create_info = (CreateTableInfo &)*info.base;
67: 	for (auto &col : create_info.columns.Physical()) {
68: 		insert_types.push_back(col.GetType());
69: 		bound_defaults.push_back(make_uniq<BoundConstantExpression>(Value(col.GetType())));
70: 	}
71: }
72: 
73: //===--------------------------------------------------------------------===//
74: // Sink
75: //===--------------------------------------------------------------------===//
76: class InsertGlobalState : public GlobalSinkState {
77: public:
78: 	explicit InsertGlobalState(ClientContext &context, const vector<LogicalType> &return_types, DuckTableEntry &table)
79: 	    : table(table), insert_count(0), initialized(false), return_collection(context, return_types) {
80: 	}
81: 
82: 	mutex lock;
83: 	DuckTableEntry &table;
84: 	idx_t insert_count;
85: 	bool initialized;
86: 	LocalAppendState append_state;
87: 	ColumnDataCollection return_collection;
88: };
89: 
90: class InsertLocalState : public LocalSinkState {
91: public:
92: 	InsertLocalState(ClientContext &context, const vector<LogicalType> &types,
93: 	                 const vector<unique_ptr<Expression>> &bound_defaults)
94: 	    : default_executor(context, bound_defaults) {
95: 		insert_chunk.Initialize(Allocator::Get(context), types);
96: 	}
97: 
98: 	DataChunk insert_chunk;
99: 	ExpressionExecutor default_executor;
100: 	TableAppendState local_append_state;
101: 	unique_ptr<RowGroupCollection> local_collection;
102: 	optional_ptr<OptimisticDataWriter> writer;
103: 	// Rows that have been updated by a DO UPDATE conflict
104: 	unordered_set<row_t> updated_rows;
105: };
106: 
107: unique_ptr<GlobalSinkState> PhysicalInsert::GetGlobalSinkState(ClientContext &context) const {
108: 	optional_ptr<TableCatalogEntry> table;
109: 	if (info) {
110: 		// CREATE TABLE AS
111: 		D_ASSERT(!insert_table);
112: 		auto &catalog = schema->catalog;
113: 		table = &catalog.CreateTable(catalog.GetCatalogTransaction(context), *schema.get_mutable(), *info)
114: 		             ->Cast<TableCatalogEntry>();
115: 	} else {
116: 		D_ASSERT(insert_table);
117: 		D_ASSERT(insert_table->IsDuckTable());
118: 		table = insert_table.get_mutable();
119: 	}
120: 	auto result = make_uniq<InsertGlobalState>(context, GetTypes(), table->Cast<DuckTableEntry>());
121: 	return std::move(result);
122: }
123: 
124: unique_ptr<LocalSinkState> PhysicalInsert::GetLocalSinkState(ExecutionContext &context) const {
125: 	return make_uniq<InsertLocalState>(context.client, insert_types, bound_defaults);
126: }
127: 
128: void PhysicalInsert::ResolveDefaults(const TableCatalogEntry &table, DataChunk &chunk,
129:                                      const physical_index_vector_t<idx_t> &column_index_map,
130:                                      ExpressionExecutor &default_executor, DataChunk &result) {
131: 	chunk.Flatten();
132: 	default_executor.SetChunk(chunk);
133: 
134: 	result.Reset();
135: 	result.SetCardinality(chunk);
136: 
137: 	if (!column_index_map.empty()) {
138: 		// columns specified by the user, use column_index_map
139: 		for (auto &col : table.GetColumns().Physical()) {
140: 			auto storage_idx = col.StorageOid();
141: 			auto mapped_index = column_index_map[col.Physical()];
142: 			if (mapped_index == DConstants::INVALID_INDEX) {
143: 				// insert default value
144: 				default_executor.ExecuteExpression(storage_idx, result.data[storage_idx]);
145: 			} else {
146: 				// get value from child chunk
147: 				D_ASSERT((idx_t)mapped_index < chunk.ColumnCount());
148: 				D_ASSERT(result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType());
149: 				result.data[storage_idx].Reference(chunk.data[mapped_index]);
150: 			}
151: 		}
152: 	} else {
153: 		// no columns specified, just append directly
154: 		for (idx_t i = 0; i < result.ColumnCount(); i++) {
155: 			D_ASSERT(result.data[i].GetType() == chunk.data[i].GetType());
156: 			result.data[i].Reference(chunk.data[i]);
157: 		}
158: 	}
159: }
160: 
161: bool AllConflictsMeetCondition(DataChunk &result) {
162: 	auto data = FlatVector::GetData<bool>(result.data[0]);
163: 	for (idx_t i = 0; i < result.size(); i++) {
164: 		if (!data[i]) {
165: 			return false;
166: 		}
167: 	}
168: 	return true;
169: }
170: 
171: void CheckOnConflictCondition(ExecutionContext &context, DataChunk &conflicts, const unique_ptr<Expression> &condition,
172:                               DataChunk &result) {
173: 	ExpressionExecutor executor(context.client, *condition);
174: 	result.Initialize(context.client, {LogicalType::BOOLEAN});
175: 	executor.Execute(conflicts, result);
176: 	result.SetCardinality(conflicts.size());
177: }
178: 
179: void PhysicalInsert::CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_chunk, DataChunk &input_chunk,
180:                                                     ClientContext &client) const {
181: 	if (types_to_fetch.empty()) {
182: 		// We have not scanned the initial table, so we can just duplicate the initial chunk
183: 		result.Initialize(client, input_chunk.GetTypes());
184: 		result.Reference(input_chunk);
185: 		result.SetCardinality(input_chunk);
186: 		return;
187: 	}
188: 	vector<LogicalType> combined_types;
189: 	combined_types.reserve(insert_types.size() + types_to_fetch.size());
190: 	combined_types.insert(combined_types.end(), insert_types.begin(), insert_types.end());
191: 	combined_types.insert(combined_types.end(), types_to_fetch.begin(), types_to_fetch.end());
192: 
193: 	result.Initialize(client, combined_types);
194: 	result.Reset();
195: 	// Add the VALUES list
196: 	for (idx_t i = 0; i < insert_types.size(); i++) {
197: 		idx_t col_idx = i;
198: 		auto &other_col = input_chunk.data[i];
199: 		auto &this_col = result.data[col_idx];
200: 		D_ASSERT(other_col.GetType() == this_col.GetType());
201: 		this_col.Reference(other_col);
202: 	}
203: 	// Add the columns from the original conflicting tuples
204: 	for (idx_t i = 0; i < types_to_fetch.size(); i++) {
205: 		idx_t col_idx = i + insert_types.size();
206: 		auto &other_col = scan_chunk.data[i];
207: 		auto &this_col = result.data[col_idx];
208: 		D_ASSERT(other_col.GetType() == this_col.GetType());
209: 		this_col.Reference(other_col);
210: 	}
211: 	// This is guaranteed by the requirement of a conflict target to have a condition or set expressions
212: 	// Only when we have any sort of condition or SET expression that references the existing table is this possible
213: 	// to not be true.
214: 	// We can have a SET expression without a conflict target ONLY if there is only 1 Index on the table
215: 	// In which case this also can't cause a discrepancy between existing tuple count and insert tuple count
216: 	D_ASSERT(input_chunk.size() == scan_chunk.size());
217: 	result.SetCardinality(input_chunk.size());
218: }
219: 
220: void PhysicalInsert::PerformOnConflictAction(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table,
221:                                              Vector &row_ids) const {
222: 	if (action_type == OnConflictAction::NOTHING) {
223: 		return;
224: 	}
225: 
226: 	DataChunk update_chunk; // contains only the to-update columns
227: 
228: 	// Check the optional condition for the DO UPDATE clause, to filter which rows will be updated
229: 	if (do_update_condition) {
230: 		DataChunk do_update_filter_result;
231: 		do_update_filter_result.Initialize(context.client, {LogicalType::BOOLEAN});
232: 		ExpressionExecutor where_executor(context.client, *do_update_condition);
233: 		where_executor.Execute(chunk, do_update_filter_result);
234: 		do_update_filter_result.SetCardinality(chunk.size());
235: 
236: 		ManagedSelection selection(chunk.size());
237: 
238: 		auto where_data = FlatVector::GetData<bool>(do_update_filter_result.data[0]);
239: 		for (idx_t i = 0; i < chunk.size(); i++) {
240: 			if (where_data[i]) {
241: 				selection.Append(i);
242: 			}
243: 		}
244: 		if (selection.Count() != selection.Size()) {
245: 			// Not all conflicts met the condition, need to filter out the ones that don't
246: 			chunk.Slice(selection.Selection(), selection.Count());
247: 			chunk.SetCardinality(selection.Count());
248: 			// Also apply this Slice to the to-update row_ids
249: 			row_ids.Slice(selection.Selection(), selection.Count());
250: 		}
251: 	}
252: 
253: 	// Execute the SET expressions
254: 	update_chunk.Initialize(context.client, set_types);
255: 	ExpressionExecutor executor(context.client, set_expressions);
256: 	executor.Execute(chunk, update_chunk);
257: 	update_chunk.SetCardinality(chunk);
258: 
259: 	auto &data_table = table.GetStorage();
260: 	// Perform the update, using the results of the SET expressions
261: 	data_table.Update(table, context.client, row_ids, set_columns, update_chunk);
262: }
263: 
264: // TODO: should we use a hash table to keep track of this instead?
265: void PhysicalInsert::RegisterUpdatedRows(InsertLocalState &lstate, const Vector &row_ids, idx_t count) const {
266: 	// Insert all rows, if any of the rows has already been updated before, we throw an error
267: 	auto data = FlatVector::GetData<row_t>(row_ids);
268: 	for (idx_t i = 0; i < count; i++) {
269: 		auto result = lstate.updated_rows.insert(data[i]);
270: 		if (result.second == false) {
271: 			throw InvalidInputException(
272: 			    "ON CONFLICT DO UPDATE can not update the same row twice in the same command, Ensure that no rows "
273: 			    "proposed for insertion within the same command have duplicate constrained values");
274: 		}
275: 	}
276: }
277: 
278: void PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context,
279:                                         InsertLocalState &lstate) const {
280: 	auto &data_table = table.GetStorage();
281: 	if (action_type == OnConflictAction::THROW) {
282: 		data_table.VerifyAppendConstraints(table, context.client, lstate.insert_chunk, nullptr);
283: 		return;
284: 	}
285: 	// Check whether any conflicts arise, and if they all meet the conflict_target + condition
286: 	// If that's not the case - We throw the first error
287: 
288: 	// We either want to do nothing, or perform an update when conflicts arise
289: 	ConflictInfo conflict_info(conflict_target);
290: 	ConflictManager conflict_manager(VerifyExistenceType::APPEND, lstate.insert_chunk.size(), &conflict_info);
291: 	data_table.VerifyAppendConstraints(table, context.client, lstate.insert_chunk, &conflict_manager);
292: 	conflict_manager.Finalize();
293: 	if (conflict_manager.ConflictCount() == 0) {
294: 		// No conflicts found
295: 		return;
296: 	}
297: 	auto &conflicts = conflict_manager.Conflicts();
298: 	auto &row_ids = conflict_manager.RowIds();
299: 
300: 	DataChunk conflict_chunk; // contains only the conflicting values
301: 	DataChunk scan_chunk;     // contains the original values, that caused the conflict
302: 	DataChunk combined_chunk; // contains conflict_chunk + scan_chunk (wide)
303: 
304: 	// Filter out everything but the conflicting rows
305: 	conflict_chunk.Initialize(context.client, lstate.insert_chunk.GetTypes());
306: 	conflict_chunk.Reference(lstate.insert_chunk);
307: 	conflict_chunk.Slice(conflicts.Selection(), conflicts.Count());
308: 	conflict_chunk.SetCardinality(conflicts.Count());
309: 
310: 	if (!types_to_fetch.empty()) {
311: 		D_ASSERT(scan_chunk.size() == 0);
312: 		// When these values are required for the conditions or the SET expressions,
313: 		// then we scan the existing table for the conflicting tuples, using the rowids
314: 		scan_chunk.Initialize(context.client, types_to_fetch);
315: 		auto fetch_state = make_uniq<ColumnFetchState>();
316: 		auto &transaction = DuckTransaction::Get(context.client, table.catalog);
317: 		data_table.Fetch(transaction, scan_chunk, columns_to_fetch, row_ids, conflicts.Count(), *fetch_state);
318: 	}
319: 
320: 	// Splice the Input chunk and the fetched chunk together
321: 	CombineExistingAndInsertTuples(combined_chunk, scan_chunk, conflict_chunk, context.client);
322: 
323: 	if (on_conflict_condition) {
324: 		DataChunk conflict_condition_result;
325: 		CheckOnConflictCondition(context, combined_chunk, on_conflict_condition, conflict_condition_result);
326: 		bool conditions_met = AllConflictsMeetCondition(conflict_condition_result);
327: 		if (!conditions_met) {
328: 			// Filter out the tuples that did pass the filter, then run the verify again
329: 			ManagedSelection sel(combined_chunk.size());
330: 			auto data = FlatVector::GetData<bool>(conflict_condition_result.data[0]);
331: 			for (idx_t i = 0; i < combined_chunk.size(); i++) {
332: 				if (!data[i]) {
333: 					// Only populate the selection vector with the tuples that did not meet the condition
334: 					sel.Append(i);
335: 				}
336: 			}
337: 			combined_chunk.Slice(sel.Selection(), sel.Count());
338: 			row_ids.Slice(sel.Selection(), sel.Count());
339: 			data_table.VerifyAppendConstraints(table, context.client, combined_chunk, nullptr);
340: 			throw InternalException("The previous operation was expected to throw but didn't");
341: 		}
342: 	}
343: 
344: 	RegisterUpdatedRows(lstate, row_ids, combined_chunk.size());
345: 
346: 	PerformOnConflictAction(context, combined_chunk, table, row_ids);
347: 
348: 	// Remove the conflicting tuples from the insert chunk
349: 	SelectionVector sel_vec(lstate.insert_chunk.size());
350: 	idx_t new_size =
351: 	    SelectionVector::Inverted(conflicts.Selection(), sel_vec, conflicts.Count(), lstate.insert_chunk.size());
352: 	lstate.insert_chunk.Slice(sel_vec, new_size);
353: 	lstate.insert_chunk.SetCardinality(new_size);
354: }
355: 
356: SinkResultType PhysicalInsert::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate_p,
357:                                     DataChunk &chunk) const {
358: 	auto &gstate = state.Cast<InsertGlobalState>();
359: 	auto &lstate = lstate_p.Cast<InsertLocalState>();
360: 
361: 	auto &table = gstate.table;
362: 	auto &storage = table.GetStorage();
363: 	PhysicalInsert::ResolveDefaults(table, chunk, column_index_map, lstate.default_executor, lstate.insert_chunk);
364: 
365: 	if (!parallel) {
366: 		if (!gstate.initialized) {
367: 			storage.InitializeLocalAppend(gstate.append_state, context.client);
368: 			gstate.initialized = true;
369: 		}
370: 
371: 		OnConflictHandling(table, context, lstate);
372: 		storage.LocalAppend(gstate.append_state, table, context.client, lstate.insert_chunk, true);
373: 
374: 		if (return_chunk) {
375: 			gstate.return_collection.Append(lstate.insert_chunk);
376: 		}
377: 		gstate.insert_count += chunk.size();
378: 	} else {
379: 		D_ASSERT(!return_chunk);
380: 		// parallel append
381: 		if (!lstate.local_collection) {
382: 			lock_guard<mutex> l(gstate.lock);
383: 			auto &table_info = storage.info;
384: 			auto &block_manager = TableIOManager::Get(storage).GetBlockManagerForRowData();
385: 			lstate.local_collection =
386: 			    make_uniq<RowGroupCollection>(table_info, block_manager, insert_types, MAX_ROW_ID);
387: 			lstate.local_collection->InitializeEmpty();
388: 			lstate.local_collection->InitializeAppend(lstate.local_append_state);
389: 			lstate.writer = &gstate.table.GetStorage().CreateOptimisticWriter(context.client);
390: 		}
391: 		OnConflictHandling(table, context, lstate);
392: 		auto new_row_group = lstate.local_collection->Append(lstate.insert_chunk, lstate.local_append_state);
393: 		if (new_row_group) {
394: 			lstate.writer->CheckFlushToDisk(*lstate.local_collection);
395: 		}
396: 	}
397: 
398: 	return SinkResultType::NEED_MORE_INPUT;
399: }
400: 
401: void PhysicalInsert::Combine(ExecutionContext &context, GlobalSinkState &gstate_p, LocalSinkState &lstate_p) const {
402: 	auto &gstate = gstate_p.Cast<InsertGlobalState>();
403: 	auto &lstate = lstate_p.Cast<InsertLocalState>();
404: 	auto &client_profiler = QueryProfiler::Get(context.client);
405: 	context.thread.profiler.Flush(*this, lstate.default_executor, "default_executor", 1);
406: 	client_profiler.Flush(context.thread.profiler);
407: 
408: 	if (!parallel) {
409: 		return;
410: 	}
411: 	if (!lstate.local_collection) {
412: 		return;
413: 	}
414: 	// parallel append: finalize the append
415: 	TransactionData tdata(0, 0);
416: 	lstate.local_collection->FinalizeAppend(tdata, lstate.local_append_state);
417: 
418: 	auto append_count = lstate.local_collection->GetTotalRows();
419: 
420: 	if (append_count < LocalStorage::MERGE_THRESHOLD) {
421: 		// we have few rows - append to the local storage directly
422: 		lock_guard<mutex> lock(gstate.lock);
423: 		gstate.insert_count += append_count;
424: 		auto &table = gstate.table;
425: 		auto &storage = table.GetStorage();
426: 		storage.InitializeLocalAppend(gstate.append_state, context.client);
427: 		auto &transaction = DuckTransaction::Get(context.client, table.catalog);
428: 		lstate.local_collection->Scan(transaction, [&](DataChunk &insert_chunk) {
429: 			storage.LocalAppend(gstate.append_state, table, context.client, insert_chunk);
430: 			return true;
431: 		});
432: 		storage.FinalizeLocalAppend(gstate.append_state);
433: 	} else {
434: 		// we have many rows - flush the row group collection to disk (if required) and merge into the transaction-local
435: 		// state
436: 		lstate.writer->FlushToDisk(*lstate.local_collection);
437: 		lstate.writer->FinalFlush();
438: 
439: 		lock_guard<mutex> lock(gstate.lock);
440: 		gstate.insert_count += append_count;
441: 		gstate.table.GetStorage().LocalMerge(context.client, *lstate.local_collection);
442: 	}
443: }
444: 
445: SinkFinalizeType PhysicalInsert::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
446:                                           GlobalSinkState &state) const {
447: 	auto &gstate = state.Cast<InsertGlobalState>();
448: 	if (!parallel && gstate.initialized) {
449: 		auto &table = gstate.table;
450: 		auto &storage = table.GetStorage();
451: 		storage.FinalizeLocalAppend(gstate.append_state);
452: 	}
453: 	return SinkFinalizeType::READY;
454: }
455: 
456: //===--------------------------------------------------------------------===//
457: // Source
458: //===--------------------------------------------------------------------===//
459: class InsertSourceState : public GlobalSourceState {
460: public:
461: 	explicit InsertSourceState(const PhysicalInsert &op) : finished(false) {
462: 		if (op.return_chunk) {
463: 			D_ASSERT(op.sink_state);
464: 			auto &g = op.sink_state->Cast<InsertGlobalState>();
465: 			g.return_collection.InitializeScan(scan_state);
466: 		}
467: 	}
468: 
469: 	ColumnDataScanState scan_state;
470: 	bool finished;
471: };
472: 
473: unique_ptr<GlobalSourceState> PhysicalInsert::GetGlobalSourceState(ClientContext &context) const {
474: 	return make_uniq<InsertSourceState>(*this);
475: }
476: 
477: void PhysicalInsert::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
478:                              LocalSourceState &lstate) const {
479: 	auto &state = gstate.Cast<InsertSourceState>();
480: 	auto &insert_gstate = sink_state->Cast<InsertGlobalState>();
481: 	if (state.finished) {
482: 		return;
483: 	}
484: 	if (!return_chunk) {
485: 		chunk.SetCardinality(1);
486: 		chunk.SetValue(0, 0, Value::BIGINT(insert_gstate.insert_count));
487: 		state.finished = true;
488: 		return;
489: 	}
490: 
491: 	insert_gstate.return_collection.Scan(state.scan_state, chunk);
492: }
493: 
494: } // namespace duckdb
[end of src/execution/operator/persistent/physical_insert.cpp]
[start of src/include/duckdb/execution/operator/persistent/physical_insert.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/persistent/physical_insert.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/planner/expression.hpp"
13: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
14: #include "duckdb/common/index_vector.hpp"
15: #include "duckdb/parser/statement/insert_statement.hpp"
16: 
17: namespace duckdb {
18: 
19: class InsertLocalState;
20: 
21: //! Physically insert a set of data into a table
22: class PhysicalInsert : public PhysicalOperator {
23: public:
24: 	static constexpr const PhysicalOperatorType TYPE = PhysicalOperatorType::INSERT;
25: 
26: public:
27: 	//! INSERT INTO
28: 	PhysicalInsert(vector<LogicalType> types, TableCatalogEntry &table, physical_index_vector_t<idx_t> column_index_map,
29: 	               vector<unique_ptr<Expression>> bound_defaults, vector<unique_ptr<Expression>> set_expressions,
30: 	               vector<PhysicalIndex> set_columns, vector<LogicalType> set_types, idx_t estimated_cardinality,
31: 	               bool return_chunk, bool parallel, OnConflictAction action_type,
32: 	               unique_ptr<Expression> on_conflict_condition, unique_ptr<Expression> do_update_condition,
33: 	               unordered_set<column_t> on_conflict_filter, vector<column_t> columns_to_fetch);
34: 	//! CREATE TABLE AS
35: 	PhysicalInsert(LogicalOperator &op, SchemaCatalogEntry &schema, unique_ptr<BoundCreateTableInfo> info,
36: 	               idx_t estimated_cardinality, bool parallel);
37: 
38: 	//! The map from insert column index to table column index
39: 	physical_index_vector_t<idx_t> column_index_map;
40: 	//! The table to insert into
41: 	optional_ptr<TableCatalogEntry> insert_table;
42: 	//! The insert types
43: 	vector<LogicalType> insert_types;
44: 	//! The default expressions of the columns for which no value is provided
45: 	vector<unique_ptr<Expression>> bound_defaults;
46: 	//! If the returning statement is present, return the whole chunk
47: 	bool return_chunk;
48: 	//! Table schema, in case of CREATE TABLE AS
49: 	optional_ptr<SchemaCatalogEntry> schema;
50: 	//! Create table info, in case of CREATE TABLE AS
51: 	unique_ptr<BoundCreateTableInfo> info;
52: 	//! Whether or not the INSERT can be executed in parallel
53: 	//! This insert is not order preserving if executed in parallel
54: 	bool parallel;
55: 	// Which action to perform on conflict
56: 	OnConflictAction action_type;
57: 
58: 	// The DO UPDATE set expressions, if 'action_type' is UPDATE
59: 	vector<unique_ptr<Expression>> set_expressions;
60: 	// Which columns are targeted by the set expressions
61: 	vector<PhysicalIndex> set_columns;
62: 	// The types of the columns targeted by a SET expression
63: 	vector<LogicalType> set_types;
64: 
65: 	// Condition for the ON CONFLICT clause
66: 	unique_ptr<Expression> on_conflict_condition;
67: 	// Condition for the DO UPDATE clause
68: 	unique_ptr<Expression> do_update_condition;
69: 	// The column ids to apply the ON CONFLICT on
70: 	unordered_set<column_t> conflict_target;
71: 
72: 	// Column ids from the original table to fetch
73: 	vector<column_t> columns_to_fetch;
74: 	// Matching types to the column ids to fetch
75: 	vector<LogicalType> types_to_fetch;
76: 
77: public:
78: 	// Source interface
79: 	unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;
80: 	void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,
81: 	             LocalSourceState &lstate) const override;
82: 
83: 	bool IsSource() const override {
84: 		return true;
85: 	}
86: 
87: public:
88: 	// Sink interface
89: 	unique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;
90: 	unique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;
91: 	SinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,
92: 	                    DataChunk &input) const override;
93: 	void Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const override;
94: 	SinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
95: 	                          GlobalSinkState &gstate) const override;
96: 
97: 	bool IsSink() const override {
98: 		return true;
99: 	}
100: 
101: 	bool ParallelSink() const override {
102: 		return parallel;
103: 	}
104: 
105: 	bool SinkOrderDependent() const override {
106: 		return true;
107: 	}
108: 
109: public:
110: 	static void GetInsertInfo(const BoundCreateTableInfo &info, vector<LogicalType> &insert_types,
111: 	                          vector<unique_ptr<Expression>> &bound_defaults);
112: 	static void ResolveDefaults(const TableCatalogEntry &table, DataChunk &chunk,
113: 	                            const physical_index_vector_t<idx_t> &column_index_map,
114: 	                            ExpressionExecutor &defaults_executor, DataChunk &result);
115: 
116: protected:
117: 	void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_chunk, DataChunk &input_chunk,
118: 	                                    ClientContext &client) const;
119: 	void OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context, InsertLocalState &lstate) const;
120: 	void PerformOnConflictAction(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table,
121: 	                             Vector &row_ids) const;
122: 	void RegisterUpdatedRows(InsertLocalState &lstate, const Vector &row_ids, idx_t count) const;
123: };
124: 
125: } // namespace duckdb
[end of src/include/duckdb/execution/operator/persistent/physical_insert.hpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: