You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Sequence missing from catalog with ATTACH
### What happens?

I'm not sure this is an issue or expected behavior, but I found the following suprising.

Attaching to database with `ATTACH` for a database with a sequence raises a Catalog Error (for a missing sequence).


### To Reproduce


**Create DB**

```sh
duckdb db.duckdb
```

```sql
CREATE SEQUENCE 'id_seq';
CREATE TABLE person (id INT DEFAULT nextval('id_seq'), age INT NOT NULL);
INSERT INTO person (age) VALUES (10);
```

**Attach in a new session**

```sh
duckdb
```

```sql
ATTACH 'db.duckdb';
# Error: Catalog Error: Sequence with name id_seq does not exist!
```

Everything works fine connecting to existing db when starting cli:

```
duckdb db.duckdb
```


### OS:

macOS 14.2 

### DuckDB Version:

v0.9.2 3c695d7ba9

### DuckDB Client:

CLI

### Full Name:

Trevor Manz

### Affiliation:

Harvard Medical School

### Have you tried this on the latest `main` branch?

I have tested with a main build

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/logo-dl/DuckDB_Logo-stacked.svg" height="120">
3: </div>
4: <br>
5: 
6: 
7: 
8: 
9: <p align="center">
10:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
11:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
12:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
13: </p>
14: 
15: ## DuckDB
16: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
17: 
18: ## Installation
19: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
20: 
21: ## Data Import
22: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
23: 
24: ```sql
25: SELECT * FROM 'myfile.csv';
26: SELECT * FROM 'myfile.parquet';
27: ```
28: 
29: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
30: 
31: ## SQL Reference
32: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
33: 
34: ## Development
35: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
36: 
37: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
38: 
39: ## Support
40: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/execution/operator/schema/physical_attach.cpp]
1: #include "duckdb/execution/operator/schema/physical_attach.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/main/attached_database.hpp"
5: #include "duckdb/main/database_manager.hpp"
6: #include "duckdb/main/extension_helper.hpp"
7: #include "duckdb/parser/parsed_data/attach_info.hpp"
8: #include "duckdb/storage/storage_extension.hpp"
9: #include "duckdb/main/database_path_and_type.hpp"
10: 
11: namespace duckdb {
12: 
13: //===--------------------------------------------------------------------===//
14: // Helper
15: //===--------------------------------------------------------------------===//
16: 
17: void ParseOptions(const unique_ptr<AttachInfo> &info, AccessMode &access_mode, string &db_type,
18:                   string &unrecognized_option) {
19: 
20: 	for (auto &entry : info->options) {
21: 
22: 		if (entry.first == "readonly" || entry.first == "read_only") {
23: 			auto read_only = BooleanValue::Get(entry.second.DefaultCastAs(LogicalType::BOOLEAN));
24: 			if (read_only) {
25: 				access_mode = AccessMode::READ_ONLY;
26: 			} else {
27: 				access_mode = AccessMode::READ_WRITE;
28: 			}
29: 			continue;
30: 		}
31: 
32: 		if (entry.first == "readwrite" || entry.first == "read_write") {
33: 			auto read_only = !BooleanValue::Get(entry.second.DefaultCastAs(LogicalType::BOOLEAN));
34: 			if (read_only) {
35: 				access_mode = AccessMode::READ_ONLY;
36: 			} else {
37: 				access_mode = AccessMode::READ_WRITE;
38: 			}
39: 			continue;
40: 		}
41: 
42: 		if (entry.first == "type") {
43: 			// extract the database type
44: 			db_type = StringValue::Get(entry.second.DefaultCastAs(LogicalType::VARCHAR));
45: 			continue;
46: 		}
47: 
48: 		// we allow unrecognized options
49: 		if (unrecognized_option.empty()) {
50: 			unrecognized_option = entry.first;
51: 		}
52: 	}
53: }
54: 
55: //===--------------------------------------------------------------------===//
56: // Source
57: //===--------------------------------------------------------------------===//
58: SourceResultType PhysicalAttach::GetData(ExecutionContext &context, DataChunk &chunk,
59:                                          OperatorSourceInput &input) const {
60: 	// parse the options
61: 	auto &config = DBConfig::GetConfig(context.client);
62: 	AccessMode access_mode = config.options.access_mode;
63: 	string db_type;
64: 	string unrecognized_option;
65: 	ParseOptions(info, access_mode, db_type, unrecognized_option);
66: 
67: 	// get the name and path of the database
68: 	auto &name = info->name;
69: 	auto &path = info->path;
70: 	if (db_type.empty()) {
71: 		DBPathAndType::ExtractExtensionPrefix(path, db_type);
72: 	}
73: 	if (name.empty()) {
74: 		auto &fs = FileSystem::GetFileSystem(context.client);
75: 		name = AttachedDatabase::ExtractDatabaseName(path, fs);
76: 	}
77: 
78: 	// check ATTACH IF NOT EXISTS
79: 	auto &db_manager = DatabaseManager::Get(context.client);
80: 	if (info->on_conflict == OnCreateConflict::IGNORE_ON_CONFLICT) {
81: 		// constant-time lookup in the catalog for the db name
82: 		auto existing_db = db_manager.GetDatabase(context.client, name);
83: 		if (existing_db) {
84: 
85: 			if ((existing_db->IsReadOnly() && access_mode == AccessMode::READ_WRITE) ||
86: 			    (!existing_db->IsReadOnly() && access_mode == AccessMode::READ_ONLY)) {
87: 
88: 				auto existing_mode = existing_db->IsReadOnly() ? AccessMode::READ_ONLY : AccessMode::READ_WRITE;
89: 				auto existing_mode_str = EnumUtil::ToString(existing_mode);
90: 				auto attached_mode = EnumUtil::ToString(access_mode);
91: 				throw BinderException("Database \"%s\" is already attached in %s mode, cannot re-attach in %s mode",
92: 				                      name, existing_mode_str, attached_mode);
93: 			}
94: 
95: 			return SourceResultType::FINISHED;
96: 		}
97: 	}
98: 
99: 	// get the database type and attach the database
100: 	db_manager.GetDatabaseType(context.client, db_type, *info, config, unrecognized_option);
101: 	auto attached_db = db_manager.AttachDatabase(context.client, *info, db_type, access_mode);
102: 	attached_db->Initialize();
103: 	return SourceResultType::FINISHED;
104: }
105: 
106: } // namespace duckdb
[end of src/execution/operator/schema/physical_attach.cpp]
[start of src/include/duckdb/main/attached_database.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/attached_database.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/case_insensitive_map.hpp"
13: #include "duckdb/common/mutex.hpp"
14: #include "duckdb/main/config.hpp"
15: #include "duckdb/catalog/catalog_entry.hpp"
16: 
17: namespace duckdb {
18: class Catalog;
19: class DatabaseInstance;
20: class StorageManager;
21: class TransactionManager;
22: class StorageExtension;
23: class DatabaseManager;
24: 
25: struct AttachInfo;
26: 
27: enum class AttachedDatabaseType {
28: 	READ_WRITE_DATABASE,
29: 	READ_ONLY_DATABASE,
30: 	SYSTEM_DATABASE,
31: 	TEMP_DATABASE,
32: };
33: 
34: //! The AttachedDatabase represents an attached database instance
35: class AttachedDatabase : public CatalogEntry {
36: public:
37: 	//! Create the built-in system attached database (without storage)
38: 	explicit AttachedDatabase(DatabaseInstance &db, AttachedDatabaseType type = AttachedDatabaseType::SYSTEM_DATABASE);
39: 	//! Create an attached database instance with the specified name and storage
40: 	AttachedDatabase(DatabaseInstance &db, Catalog &catalog, string name, string file_path, AccessMode access_mode);
41: 	//! Create an attached database instance with the specified storage extension
42: 	AttachedDatabase(DatabaseInstance &db, Catalog &catalog, StorageExtension &ext, ClientContext &context, string name,
43: 	                 const AttachInfo &info, AccessMode access_mode);
44: 	~AttachedDatabase() override;
45: 
46: 	void Initialize();
47: 	void Close();
48: 
49: 	Catalog &ParentCatalog() override;
50: 	StorageManager &GetStorageManager();
51: 	Catalog &GetCatalog();
52: 	TransactionManager &GetTransactionManager();
53: 	DatabaseInstance &GetDatabase() {
54: 		return db;
55: 	}
56: 	const string &GetName() const {
57: 		return name;
58: 	}
59: 	bool IsSystem() const;
60: 	bool IsTemporary() const;
61: 	bool IsReadOnly() const;
62: 	bool IsInitialDatabase() const;
63: 	void SetInitialDatabase();
64: 
65: 	static bool NameIsReserved(const string &name);
66: 	static string ExtractDatabaseName(const string &dbpath, FileSystem &fs);
67: 
68: private:
69: 	DatabaseInstance &db;
70: 	unique_ptr<StorageManager> storage;
71: 	unique_ptr<Catalog> catalog;
72: 	unique_ptr<TransactionManager> transaction_manager;
73: 	AttachedDatabaseType type;
74: 	optional_ptr<Catalog> parent_catalog;
75: 	bool is_initial_database = false;
76: 	bool is_closed = false;
77: };
78: 
79: } // namespace duckdb
[end of src/include/duckdb/main/attached_database.hpp]
[start of src/include/duckdb/storage/checkpoint_manager.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/checkpoint_manager.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/partial_block_manager.hpp"
12: #include "duckdb/catalog/catalog_entry/index_catalog_entry.hpp"
13: #include "duckdb/catalog/catalog.hpp"
14: 
15: namespace duckdb {
16: class DatabaseInstance;
17: class ClientContext;
18: class ColumnSegment;
19: class MetadataReader;
20: class SchemaCatalogEntry;
21: class SequenceCatalogEntry;
22: class TableCatalogEntry;
23: class ViewCatalogEntry;
24: class TypeCatalogEntry;
25: 
26: class CheckpointWriter {
27: public:
28: 	explicit CheckpointWriter(AttachedDatabase &db) : db(db) {
29: 	}
30: 	virtual ~CheckpointWriter() {
31: 	}
32: 
33: 	//! The database
34: 	AttachedDatabase &db;
35: 
36: 	virtual MetadataManager &GetMetadataManager() = 0;
37: 	virtual MetadataWriter &GetMetadataWriter() = 0;
38: 	virtual unique_ptr<TableDataWriter> GetTableDataWriter(TableCatalogEntry &table) = 0;
39: 
40: protected:
41: 	virtual void WriteEntry(CatalogEntry &entry, Serializer &serializer);
42: 	virtual void WriteSchema(SchemaCatalogEntry &schema, Serializer &serializer);
43: 	virtual void WriteTable(TableCatalogEntry &table, Serializer &serializer);
44: 	virtual void WriteView(ViewCatalogEntry &table, Serializer &serializer);
45: 	virtual void WriteSequence(SequenceCatalogEntry &table, Serializer &serializer);
46: 	virtual void WriteMacro(ScalarMacroCatalogEntry &table, Serializer &serializer);
47: 	virtual void WriteTableMacro(TableMacroCatalogEntry &table, Serializer &serializer);
48: 	virtual void WriteIndex(IndexCatalogEntry &index_catalog_entry, Serializer &serializer);
49: 	virtual void WriteType(TypeCatalogEntry &type, Serializer &serializer);
50: };
51: 
52: class CheckpointReader {
53: public:
54: 	CheckpointReader(Catalog &catalog) : catalog(catalog) {
55: 	}
56: 	virtual ~CheckpointReader() {
57: 	}
58: 
59: protected:
60: 	Catalog &catalog;
61: 
62: protected:
63: 	virtual void LoadCheckpoint(ClientContext &context, MetadataReader &reader);
64: 	virtual void ReadEntry(ClientContext &context, Deserializer &deserializer);
65: 	virtual void ReadSchema(ClientContext &context, Deserializer &deserializer);
66: 	virtual void ReadTable(ClientContext &context, Deserializer &deserializer);
67: 	virtual void ReadView(ClientContext &context, Deserializer &deserializer);
68: 	virtual void ReadSequence(ClientContext &context, Deserializer &deserializer);
69: 	virtual void ReadMacro(ClientContext &context, Deserializer &deserializer);
70: 	virtual void ReadTableMacro(ClientContext &context, Deserializer &deserializer);
71: 	virtual void ReadIndex(ClientContext &context, Deserializer &deserializer);
72: 	virtual void ReadType(ClientContext &context, Deserializer &deserializer);
73: 
74: 	virtual void ReadTableData(ClientContext &context, Deserializer &deserializer, BoundCreateTableInfo &bound_info);
75: };
76: 
77: class SingleFileCheckpointReader final : public CheckpointReader {
78: public:
79: 	explicit SingleFileCheckpointReader(SingleFileStorageManager &storage)
80: 	    : CheckpointReader(Catalog::GetCatalog(storage.GetAttached())), storage(storage) {
81: 	}
82: 
83: 	void LoadFromStorage();
84: 	MetadataManager &GetMetadataManager();
85: 
86: 	//! The database
87: 	SingleFileStorageManager &storage;
88: };
89: 
90: //! CheckpointWriter is responsible for checkpointing the database
91: class SingleFileRowGroupWriter;
92: class SingleFileTableDataWriter;
93: 
94: class SingleFileCheckpointWriter final : public CheckpointWriter {
95: 	friend class SingleFileRowGroupWriter;
96: 	friend class SingleFileTableDataWriter;
97: 
98: public:
99: 	SingleFileCheckpointWriter(AttachedDatabase &db, BlockManager &block_manager);
100: 
101: 	//! Checkpoint the current state of the WAL and flush it to the main storage. This should be called BEFORE any
102: 	//! connection is available because right now the checkpointing cannot be done online. (TODO)
103: 	void CreateCheckpoint();
104: 
105: 	virtual MetadataWriter &GetMetadataWriter() override;
106: 	virtual MetadataManager &GetMetadataManager() override;
107: 	virtual unique_ptr<TableDataWriter> GetTableDataWriter(TableCatalogEntry &table) override;
108: 
109: 	BlockManager &GetBlockManager();
110: 
111: private:
112: 	//! The metadata writer is responsible for writing schema information
113: 	unique_ptr<MetadataWriter> metadata_writer;
114: 	//! The table data writer is responsible for writing the DataPointers used by the table chunks
115: 	unique_ptr<MetadataWriter> table_metadata_writer;
116: 	//! Because this is single-file storage, we can share partial blocks across
117: 	//! an entire checkpoint.
118: 	PartialBlockManager partial_block_manager;
119: };
120: 
121: } // namespace duckdb
[end of src/include/duckdb/storage/checkpoint_manager.hpp]
[start of src/include/duckdb/storage/storage_manager.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/storage_manager.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/helper.hpp"
12: #include "duckdb/storage/buffer_manager.hpp"
13: #include "duckdb/storage/data_table.hpp"
14: #include "duckdb/storage/table_io_manager.hpp"
15: #include "duckdb/storage/write_ahead_log.hpp"
16: #include "duckdb/storage/database_size.hpp"
17: 
18: namespace duckdb {
19: class BlockManager;
20: class Catalog;
21: class CheckpointWriter;
22: class DatabaseInstance;
23: class TransactionManager;
24: class TableCatalogEntry;
25: 
26: class StorageCommitState {
27: public:
28: 	// Destruction of this object, without prior call to FlushCommit,
29: 	// will roll back the committed changes.
30: 	virtual ~StorageCommitState() {
31: 	}
32: 
33: 	// Make the commit persistent
34: 	virtual void FlushCommit() = 0;
35: };
36: 
37: //! StorageManager is responsible for managing the physical storage of the
38: //! database on disk
39: class StorageManager {
40: public:
41: 	StorageManager(AttachedDatabase &db, string path, bool read_only);
42: 	virtual ~StorageManager();
43: 
44: public:
45: 	static StorageManager &Get(AttachedDatabase &db);
46: 	static StorageManager &Get(Catalog &catalog);
47: 
48: 	//! Initialize a database or load an existing database from the given path
49: 	void Initialize();
50: 
51: 	DatabaseInstance &GetDatabase();
52: 	AttachedDatabase &GetAttached() {
53: 		return db;
54: 	}
55: 
56: 	//! Get the WAL of the StorageManager, returns nullptr if in-memory
57: 	optional_ptr<WriteAheadLog> GetWriteAheadLog();
58: 
59: 	//! Returns the database file path
60: 	string GetDBPath() {
61: 		return path;
62: 	}
63: 	//! The path to the WAL, derived from the database file path
64: 	string GetWALPath();
65: 	bool InMemory();
66: 
67: 	virtual bool AutomaticCheckpoint(idx_t estimated_wal_bytes) = 0;
68: 	virtual unique_ptr<StorageCommitState> GenStorageCommitState(Transaction &transaction, bool checkpoint) = 0;
69: 	virtual bool IsCheckpointClean(MetaBlockPointer checkpoint_id) = 0;
70: 	virtual void CreateCheckpoint(bool delete_wal = false, bool force_checkpoint = false) = 0;
71: 	virtual DatabaseSize GetDatabaseSize() = 0;
72: 	virtual vector<MetadataBlockInfo> GetMetadataInfo() = 0;
73: 	virtual shared_ptr<TableIOManager> GetTableIOManager(BoundCreateTableInfo *info) = 0;
74: 
75: protected:
76: 	virtual void LoadDatabase() = 0;
77: 
78: protected:
79: 	//! The database this storage manager belongs to
80: 	AttachedDatabase &db;
81: 	//! The path of the database
82: 	string path;
83: 	//! The WriteAheadLog of the storage manager
84: 	unique_ptr<WriteAheadLog> wal;
85: 	//! Whether or not the database is opened in read-only mode
86: 	bool read_only;
87: 	//! When loading a database, we do not yet set the wal-field. Therefore, GetWriteAheadLog must
88: 	//! return nullptr when loading a database
89: 	bool load_complete = false;
90: 
91: public:
92: 	template <class TARGET>
93: 	TARGET &Cast() {
94: 		DynamicCastCheck<TARGET>(this);
95: 		return reinterpret_cast<TARGET &>(*this);
96: 	}
97: 	template <class TARGET>
98: 	const TARGET &Cast() const {
99: 		D_ASSERT(dynamic_cast<const TARGET *>(this));
100: 		return reinterpret_cast<const TARGET &>(*this);
101: 	}
102: };
103: 
104: //! Stores database in a single file.
105: class SingleFileStorageManager : public StorageManager {
106: public:
107: 	SingleFileStorageManager(AttachedDatabase &db, string path, bool read_only);
108: 
109: 	//! The BlockManager to read/store meta information and data in blocks
110: 	unique_ptr<BlockManager> block_manager;
111: 	//! TableIoManager
112: 	unique_ptr<TableIOManager> table_io_manager;
113: 
114: public:
115: 	bool AutomaticCheckpoint(idx_t estimated_wal_bytes) override;
116: 	unique_ptr<StorageCommitState> GenStorageCommitState(Transaction &transaction, bool checkpoint) override;
117: 	bool IsCheckpointClean(MetaBlockPointer checkpoint_id) override;
118: 	void CreateCheckpoint(bool delete_wal, bool force_checkpoint) override;
119: 	DatabaseSize GetDatabaseSize() override;
120: 	vector<MetadataBlockInfo> GetMetadataInfo() override;
121: 	shared_ptr<TableIOManager> GetTableIOManager(BoundCreateTableInfo *info) override;
122: 
123: protected:
124: 	void LoadDatabase() override;
125: };
126: } // namespace duckdb
[end of src/include/duckdb/storage/storage_manager.hpp]
[start of src/main/attached_database.cpp]
1: #include "duckdb/main/attached_database.hpp"
2: 
3: #include "duckdb/catalog/duck_catalog.hpp"
4: #include "duckdb/common/constants.hpp"
5: #include "duckdb/common/file_system.hpp"
6: #include "duckdb/main/database.hpp"
7: #include "duckdb/main/database_manager.hpp"
8: #include "duckdb/parser/parsed_data/attach_info.hpp"
9: #include "duckdb/storage/storage_extension.hpp"
10: #include "duckdb/storage/storage_manager.hpp"
11: #include "duckdb/transaction/duck_transaction_manager.hpp"
12: #include "duckdb/main/database_path_and_type.hpp"
13: 
14: namespace duckdb {
15: 
16: AttachedDatabase::AttachedDatabase(DatabaseInstance &db, AttachedDatabaseType type)
17:     : CatalogEntry(CatalogType::DATABASE_ENTRY,
18:                    type == AttachedDatabaseType::SYSTEM_DATABASE ? SYSTEM_CATALOG : TEMP_CATALOG, 0),
19:       db(db), type(type) {
20: 
21: 	D_ASSERT(type == AttachedDatabaseType::TEMP_DATABASE || type == AttachedDatabaseType::SYSTEM_DATABASE);
22: 	if (type == AttachedDatabaseType::TEMP_DATABASE) {
23: 		storage = make_uniq<SingleFileStorageManager>(*this, string(IN_MEMORY_PATH), false);
24: 	}
25: 
26: 	catalog = make_uniq<DuckCatalog>(*this);
27: 	transaction_manager = make_uniq<DuckTransactionManager>(*this);
28: 	internal = true;
29: }
30: 
31: AttachedDatabase::AttachedDatabase(DatabaseInstance &db, Catalog &catalog_p, string name_p, string file_path_p,
32:                                    AccessMode access_mode)
33:     : CatalogEntry(CatalogType::DATABASE_ENTRY, catalog_p, std::move(name_p)), db(db), parent_catalog(&catalog_p) {
34: 	type = access_mode == AccessMode::READ_ONLY ? AttachedDatabaseType::READ_ONLY_DATABASE
35: 	                                            : AttachedDatabaseType::READ_WRITE_DATABASE;
36: 	catalog = make_uniq<DuckCatalog>(*this);
37: 	// do this after catalog to guarnatee we allow extension to instantionate DuckCatalog causing creation
38: 	// of the storage
39: 	storage = make_uniq<SingleFileStorageManager>(*this, std::move(file_path_p), access_mode == AccessMode::READ_ONLY);
40: 	transaction_manager = make_uniq<DuckTransactionManager>(*this);
41: 	internal = true;
42: }
43: 
44: AttachedDatabase::AttachedDatabase(DatabaseInstance &db, Catalog &catalog_p, StorageExtension &storage_extension,
45:                                    ClientContext &context, string name_p, const AttachInfo &info,
46:                                    AccessMode access_mode)
47:     : CatalogEntry(CatalogType::DATABASE_ENTRY, catalog_p, std::move(name_p)), db(db), parent_catalog(&catalog_p) {
48: 	type = access_mode == AccessMode::READ_ONLY ? AttachedDatabaseType::READ_ONLY_DATABASE
49: 	                                            : AttachedDatabaseType::READ_WRITE_DATABASE;
50: 	catalog =
51: 	    storage_extension.attach(storage_extension.storage_info.get(), context, *this, name, *info.Copy(), access_mode);
52: 	if (!catalog) {
53: 		throw InternalException("AttachedDatabase - attach function did not return a catalog");
54: 	}
55: 	if (catalog->IsDuckCatalog()) {
56: 		// DuckCatalog, instantiate storage
57: 		storage = make_uniq<SingleFileStorageManager>(*this, info.path, access_mode == AccessMode::READ_ONLY);
58: 	}
59: 	transaction_manager =
60: 	    storage_extension.create_transaction_manager(storage_extension.storage_info.get(), *this, *catalog);
61: 	if (!transaction_manager) {
62: 		throw InternalException(
63: 		    "AttachedDatabase - create_transaction_manager function did not return a transaction manager");
64: 	}
65: 	internal = true;
66: }
67: 
68: AttachedDatabase::~AttachedDatabase() {
69: 	Close();
70: }
71: 
72: bool AttachedDatabase::IsSystem() const {
73: 	D_ASSERT(!storage || type != AttachedDatabaseType::SYSTEM_DATABASE);
74: 	return type == AttachedDatabaseType::SYSTEM_DATABASE;
75: }
76: 
77: bool AttachedDatabase::IsTemporary() const {
78: 	return type == AttachedDatabaseType::TEMP_DATABASE;
79: }
80: bool AttachedDatabase::IsReadOnly() const {
81: 	return type == AttachedDatabaseType::READ_ONLY_DATABASE;
82: }
83: 
84: bool AttachedDatabase::NameIsReserved(const string &name) {
85: 	return name == DEFAULT_SCHEMA || name == TEMP_CATALOG;
86: }
87: 
88: string AttachedDatabase::ExtractDatabaseName(const string &dbpath, FileSystem &fs) {
89: 	if (dbpath.empty() || dbpath == IN_MEMORY_PATH) {
90: 		return "memory";
91: 	}
92: 	auto name = fs.ExtractBaseName(dbpath);
93: 	if (NameIsReserved(name)) {
94: 		name += "_db";
95: 	}
96: 	return name;
97: }
98: 
99: void AttachedDatabase::Initialize() {
100: 	if (IsSystem()) {
101: 		catalog->Initialize(true);
102: 	} else {
103: 		catalog->Initialize(false);
104: 	}
105: 	if (storage) {
106: 		storage->Initialize();
107: 	}
108: }
109: 
110: StorageManager &AttachedDatabase::GetStorageManager() {
111: 	if (!storage) {
112: 		throw InternalException("Internal system catalog does not have storage");
113: 	}
114: 	return *storage;
115: }
116: 
117: Catalog &AttachedDatabase::GetCatalog() {
118: 	return *catalog;
119: }
120: 
121: TransactionManager &AttachedDatabase::GetTransactionManager() {
122: 	return *transaction_manager;
123: }
124: 
125: Catalog &AttachedDatabase::ParentCatalog() {
126: 	return *parent_catalog;
127: }
128: 
129: bool AttachedDatabase::IsInitialDatabase() const {
130: 	return is_initial_database;
131: }
132: 
133: void AttachedDatabase::SetInitialDatabase() {
134: 	is_initial_database = true;
135: }
136: 
137: void AttachedDatabase::Close() {
138: 	D_ASSERT(catalog);
139: 	if (is_closed) {
140: 		return;
141: 	}
142: 	is_closed = true;
143: 
144: 	if (!IsSystem() && !catalog->InMemory()) {
145: 		db.GetDatabaseManager().EraseDatabasePath(catalog->GetDBPath());
146: 	}
147: 
148: 	if (Exception::UncaughtException()) {
149: 		return;
150: 	}
151: 	if (!storage) {
152: 		return;
153: 	}
154: 
155: 	// shutting down: attempt to checkpoint the database
156: 	// but only if we are not cleaning up as part of an exception unwind
157: 	try {
158: 		if (!storage->InMemory()) {
159: 			auto &config = DBConfig::GetConfig(db);
160: 			if (!config.options.checkpoint_on_shutdown) {
161: 				return;
162: 			}
163: 			storage->CreateCheckpoint(true);
164: 		}
165: 	} catch (...) {
166: 	}
167: }
168: 
169: } // namespace duckdb
[end of src/main/attached_database.cpp]
[start of src/storage/checkpoint_manager.cpp]
1: #include "duckdb/storage/checkpoint_manager.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/duck_index_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/scalar_macro_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
7: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
8: #include "duckdb/catalog/catalog_entry/type_catalog_entry.hpp"
9: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
10: #include "duckdb/catalog/catalog_entry/index_catalog_entry.hpp"
11: #include "duckdb/catalog/duck_catalog.hpp"
12: #include "duckdb/common/serializer/binary_deserializer.hpp"
13: #include "duckdb/common/serializer/binary_serializer.hpp"
14: #include "duckdb/main/attached_database.hpp"
15: #include "duckdb/main/client_context.hpp"
16: #include "duckdb/main/config.hpp"
17: #include "duckdb/main/connection.hpp"
18: #include "duckdb/main/database.hpp"
19: #include "duckdb/parser/parsed_data/create_schema_info.hpp"
20: #include "duckdb/parser/parsed_data/create_view_info.hpp"
21: #include "duckdb/planner/binder.hpp"
22: #include "duckdb/planner/bound_tableref.hpp"
23: #include "duckdb/planner/expression_binder/index_binder.hpp"
24: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
25: #include "duckdb/storage/block_manager.hpp"
26: #include "duckdb/storage/checkpoint/table_data_reader.hpp"
27: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
28: #include "duckdb/storage/metadata/metadata_reader.hpp"
29: #include "duckdb/storage/table/column_checkpoint_state.hpp"
30: #include "duckdb/transaction/transaction_manager.hpp"
31: #include "duckdb/execution/index/art/art.hpp"
32: #include "duckdb/execution/index/unknown_index.hpp"
33: 
34: namespace duckdb {
35: 
36: void ReorderTableEntries(catalog_entry_vector_t &tables);
37: 
38: SingleFileCheckpointWriter::SingleFileCheckpointWriter(AttachedDatabase &db, BlockManager &block_manager)
39:     : CheckpointWriter(db), partial_block_manager(block_manager, CheckpointType::FULL_CHECKPOINT) {
40: }
41: 
42: BlockManager &SingleFileCheckpointWriter::GetBlockManager() {
43: 	auto &storage_manager = db.GetStorageManager().Cast<SingleFileStorageManager>();
44: 	return *storage_manager.block_manager;
45: }
46: 
47: MetadataWriter &SingleFileCheckpointWriter::GetMetadataWriter() {
48: 	return *metadata_writer;
49: }
50: 
51: MetadataManager &SingleFileCheckpointWriter::GetMetadataManager() {
52: 	return GetBlockManager().GetMetadataManager();
53: }
54: 
55: unique_ptr<TableDataWriter> SingleFileCheckpointWriter::GetTableDataWriter(TableCatalogEntry &table) {
56: 	return make_uniq<SingleFileTableDataWriter>(*this, table, *table_metadata_writer);
57: }
58: 
59: static catalog_entry_vector_t GetCatalogEntries(vector<reference<SchemaCatalogEntry>> &schemas) {
60: 	catalog_entry_vector_t entries;
61: 	for (auto &schema_p : schemas) {
62: 		auto &schema = schema_p.get();
63: 		entries.push_back(schema);
64: 		schema.Scan(CatalogType::TYPE_ENTRY, [&](CatalogEntry &entry) {
65: 			if (entry.internal) {
66: 				return;
67: 			}
68: 			entries.push_back(entry);
69: 		});
70: 
71: 		schema.Scan(CatalogType::SEQUENCE_ENTRY, [&](CatalogEntry &entry) {
72: 			if (entry.internal) {
73: 				return;
74: 			}
75: 			entries.push_back(entry);
76: 		});
77: 
78: 		catalog_entry_vector_t tables;
79: 		vector<reference<ViewCatalogEntry>> views;
80: 		schema.Scan(CatalogType::TABLE_ENTRY, [&](CatalogEntry &entry) {
81: 			if (entry.internal) {
82: 				return;
83: 			}
84: 			if (entry.type == CatalogType::TABLE_ENTRY) {
85: 				tables.push_back(entry.Cast<TableCatalogEntry>());
86: 			} else if (entry.type == CatalogType::VIEW_ENTRY) {
87: 				views.push_back(entry.Cast<ViewCatalogEntry>());
88: 			} else {
89: 				throw NotImplementedException("Catalog type for entries");
90: 			}
91: 		});
92: 		// Reorder tables because of foreign key constraint
93: 		ReorderTableEntries(tables);
94: 		for (auto &table : tables) {
95: 			entries.push_back(table.get());
96: 		}
97: 		for (auto &view : views) {
98: 			entries.push_back(view.get());
99: 		}
100: 
101: 		schema.Scan(CatalogType::SCALAR_FUNCTION_ENTRY, [&](CatalogEntry &entry) {
102: 			if (entry.internal) {
103: 				return;
104: 			}
105: 			if (entry.type == CatalogType::MACRO_ENTRY) {
106: 				entries.push_back(entry);
107: 			}
108: 		});
109: 
110: 		schema.Scan(CatalogType::TABLE_FUNCTION_ENTRY, [&](CatalogEntry &entry) {
111: 			if (entry.internal) {
112: 				return;
113: 			}
114: 			if (entry.type == CatalogType::TABLE_MACRO_ENTRY) {
115: 				entries.push_back(entry);
116: 			}
117: 		});
118: 
119: 		schema.Scan(CatalogType::INDEX_ENTRY, [&](CatalogEntry &entry) {
120: 			D_ASSERT(!entry.internal);
121: 			entries.push_back(entry);
122: 		});
123: 	}
124: 	return entries;
125: }
126: 
127: void SingleFileCheckpointWriter::CreateCheckpoint() {
128: 	auto &config = DBConfig::Get(db);
129: 	auto &storage_manager = db.GetStorageManager().Cast<SingleFileStorageManager>();
130: 	if (storage_manager.InMemory()) {
131: 		return;
132: 	}
133: 	// assert that the checkpoint manager hasn't been used before
134: 	D_ASSERT(!metadata_writer);
135: 
136: 	auto &block_manager = GetBlockManager();
137: 	auto &metadata_manager = GetMetadataManager();
138: 
139: 	//! Set up the writers for the checkpoints
140: 	metadata_writer = make_uniq<MetadataWriter>(metadata_manager);
141: 	table_metadata_writer = make_uniq<MetadataWriter>(metadata_manager);
142: 
143: 	// get the id of the first meta block
144: 	auto meta_block = metadata_writer->GetMetaBlockPointer();
145: 
146: 	vector<reference<SchemaCatalogEntry>> schemas;
147: 	// we scan the set of committed schemas
148: 	auto &catalog = Catalog::GetCatalog(db).Cast<DuckCatalog>();
149: 	catalog.ScanSchemas([&](SchemaCatalogEntry &entry) { schemas.push_back(entry); });
150: 	// write the actual data into the database
151: 
152: 	// Create a serializer to write the checkpoint data
153: 	// The serialized format is roughly:
154: 	/*
155: 	    {
156: 	        schemas: [
157: 	            {
158: 	                schema: <schema_info>,
159: 	                custom_types: [ { type: <type_info> }, ... ],
160: 	                sequences: [ { sequence: <sequence_info> }, ... ],
161: 	                tables: [ { table: <table_info> }, ... ],
162: 	                views: [ { view: <view_info> }, ... ],
163: 	                macros: [ { macro: <macro_info> }, ... ],
164: 	                table_macros: [ { table_macro: <table_macro_info> }, ... ],
165: 	                indexes: [ { index: <index_info>, root_offset <block_ptr> }, ... ]
166: 	            }
167: 	        ]
168: 	    }
169: 	 */
170: 	auto catalog_entries = GetCatalogEntries(schemas);
171: 	BinarySerializer serializer(*metadata_writer);
172: 	serializer.Begin();
173: 	serializer.WriteList(100, "catalog_entries", catalog_entries.size(), [&](Serializer::List &list, idx_t i) {
174: 		auto &entry = catalog_entries[i];
175: 		list.WriteObject([&](Serializer &obj) { WriteEntry(entry.get(), obj); });
176: 	});
177: 	serializer.End();
178: 
179: 	partial_block_manager.FlushPartialBlocks();
180: 	metadata_writer->Flush();
181: 	table_metadata_writer->Flush();
182: 
183: 	// write a checkpoint flag to the WAL
184: 	// this protects against the rare event that the database crashes AFTER writing the file, but BEFORE truncating the
185: 	// WAL we write an entry CHECKPOINT "meta_block_id" into the WAL upon loading, if we see there is an entry
186: 	// CHECKPOINT "meta_block_id", and the id MATCHES the head idin the file we know that the database was successfully
187: 	// checkpointed, so we know that we should avoid replaying the WAL to avoid duplicating data
188: 	auto wal = storage_manager.GetWriteAheadLog();
189: 	bool wal_is_empty = wal->GetWALSize() == 0;
190: 	if (!wal_is_empty) {
191: 		wal->WriteCheckpoint(meta_block);
192: 		wal->Flush();
193: 	}
194: 
195: 	if (config.options.checkpoint_abort == CheckpointAbort::DEBUG_ABORT_BEFORE_HEADER) {
196: 		throw FatalException("Checkpoint aborted before header write because of PRAGMA checkpoint_abort flag");
197: 	}
198: 
199: 	// finally write the updated header
200: 	DatabaseHeader header;
201: 	header.meta_block = meta_block.block_pointer;
202: 	header.block_size = Storage::BLOCK_ALLOC_SIZE;
203: 	header.vector_size = STANDARD_VECTOR_SIZE;
204: 	block_manager.WriteHeader(header);
205: 
206: 	if (config.options.checkpoint_abort == CheckpointAbort::DEBUG_ABORT_BEFORE_TRUNCATE) {
207: 		throw FatalException("Checkpoint aborted before truncate because of PRAGMA checkpoint_abort flag");
208: 	}
209: 
210: 	// truncate the file
211: 	block_manager.Truncate();
212: 
213: 	// truncate the WAL
214: 	if (!wal_is_empty) {
215: 		wal->Truncate(0);
216: 	}
217: }
218: 
219: void CheckpointReader::LoadCheckpoint(ClientContext &context, MetadataReader &reader) {
220: 	BinaryDeserializer deserializer(reader);
221: 	deserializer.Begin();
222: 	deserializer.ReadList(100, "catalog_entries", [&](Deserializer::List &list, idx_t i) {
223: 		return list.ReadObject([&](Deserializer &obj) { ReadEntry(context, obj); });
224: 	});
225: 	deserializer.End();
226: }
227: 
228: MetadataManager &SingleFileCheckpointReader::GetMetadataManager() {
229: 	return storage.block_manager->GetMetadataManager();
230: }
231: 
232: void SingleFileCheckpointReader::LoadFromStorage() {
233: 	auto &block_manager = *storage.block_manager;
234: 	auto &metadata_manager = GetMetadataManager();
235: 	MetaBlockPointer meta_block(block_manager.GetMetaBlock(), 0);
236: 	if (!meta_block.IsValid()) {
237: 		// storage is empty
238: 		return;
239: 	}
240: 
241: 	Connection con(storage.GetDatabase());
242: 	con.BeginTransaction();
243: 	// create the MetadataReader to read from the storage
244: 	MetadataReader reader(metadata_manager, meta_block);
245: 	//	reader.SetContext(*con.context);
246: 	LoadCheckpoint(*con.context, reader);
247: 	con.Commit();
248: }
249: 
250: void CheckpointWriter::WriteEntry(CatalogEntry &entry, Serializer &serializer) {
251: 	serializer.WriteProperty(99, "catalog_type", entry.type);
252: 
253: 	switch (entry.type) {
254: 	case CatalogType::SCHEMA_ENTRY: {
255: 		auto &schema = entry.Cast<SchemaCatalogEntry>();
256: 		WriteSchema(schema, serializer);
257: 		break;
258: 	}
259: 	case CatalogType::TYPE_ENTRY: {
260: 		auto &custom_type = entry.Cast<TypeCatalogEntry>();
261: 		WriteType(custom_type, serializer);
262: 		break;
263: 	}
264: 	case CatalogType::SEQUENCE_ENTRY: {
265: 		auto &seq = entry.Cast<SequenceCatalogEntry>();
266: 		WriteSequence(seq, serializer);
267: 		break;
268: 	}
269: 	case CatalogType::TABLE_ENTRY: {
270: 		auto &table = entry.Cast<TableCatalogEntry>();
271: 		WriteTable(table, serializer);
272: 		break;
273: 	}
274: 	case CatalogType::VIEW_ENTRY: {
275: 		auto &view = entry.Cast<ViewCatalogEntry>();
276: 		WriteView(view, serializer);
277: 		break;
278: 	}
279: 	case CatalogType::MACRO_ENTRY: {
280: 		auto &macro = entry.Cast<ScalarMacroCatalogEntry>();
281: 		WriteMacro(macro, serializer);
282: 		break;
283: 	}
284: 	case CatalogType::TABLE_MACRO_ENTRY: {
285: 		auto &macro = entry.Cast<TableMacroCatalogEntry>();
286: 		WriteTableMacro(macro, serializer);
287: 		break;
288: 	}
289: 	case CatalogType::INDEX_ENTRY: {
290: 		auto &index = entry.Cast<IndexCatalogEntry>();
291: 		WriteIndex(index, serializer);
292: 		break;
293: 	}
294: 	default:
295: 		throw InternalException("Unrecognized catalog type in CheckpointWriter::WriteEntry");
296: 	}
297: }
298: 
299: //===--------------------------------------------------------------------===//
300: // Schema
301: //===--------------------------------------------------------------------===//
302: void CheckpointWriter::WriteSchema(SchemaCatalogEntry &schema, Serializer &serializer) {
303: 	// write the schema data
304: 	serializer.WriteProperty(100, "schema", &schema);
305: }
306: 
307: void CheckpointReader::ReadEntry(ClientContext &context, Deserializer &deserializer) {
308: 	auto type = deserializer.ReadProperty<CatalogType>(99, "type");
309: 
310: 	switch (type) {
311: 	case CatalogType::SCHEMA_ENTRY: {
312: 		ReadSchema(context, deserializer);
313: 		break;
314: 	}
315: 	case CatalogType::TYPE_ENTRY: {
316: 		ReadType(context, deserializer);
317: 		break;
318: 	}
319: 	case CatalogType::SEQUENCE_ENTRY: {
320: 		ReadSequence(context, deserializer);
321: 		break;
322: 	}
323: 	case CatalogType::TABLE_ENTRY: {
324: 		ReadTable(context, deserializer);
325: 		break;
326: 	}
327: 	case CatalogType::VIEW_ENTRY: {
328: 		ReadView(context, deserializer);
329: 		break;
330: 	}
331: 	case CatalogType::MACRO_ENTRY: {
332: 		ReadMacro(context, deserializer);
333: 		break;
334: 	}
335: 	case CatalogType::TABLE_MACRO_ENTRY: {
336: 		ReadTableMacro(context, deserializer);
337: 		break;
338: 	}
339: 	case CatalogType::INDEX_ENTRY: {
340: 		ReadIndex(context, deserializer);
341: 		break;
342: 	}
343: 	default:
344: 		throw InternalException("Unrecognized catalog type in CheckpointWriter::WriteEntry");
345: 	}
346: }
347: 
348: void CheckpointReader::ReadSchema(ClientContext &context, Deserializer &deserializer) {
349: 	// Read the schema and create it in the catalog
350: 	auto info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(100, "schema");
351: 	auto &schema_info = info->Cast<CreateSchemaInfo>();
352: 
353: 	// we set create conflict to IGNORE_ON_CONFLICT, so that we can ignore a failure when recreating the main schema
354: 	schema_info.on_conflict = OnCreateConflict::IGNORE_ON_CONFLICT;
355: 	catalog.CreateSchema(context, schema_info);
356: }
357: 
358: //===--------------------------------------------------------------------===//
359: // Views
360: //===--------------------------------------------------------------------===//
361: void CheckpointWriter::WriteView(ViewCatalogEntry &view, Serializer &serializer) {
362: 	serializer.WriteProperty(100, "view", &view);
363: }
364: 
365: void CheckpointReader::ReadView(ClientContext &context, Deserializer &deserializer) {
366: 	auto info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(100, "view");
367: 	auto &view_info = info->Cast<CreateViewInfo>();
368: 	catalog.CreateView(context, view_info);
369: }
370: 
371: //===--------------------------------------------------------------------===//
372: // Sequences
373: //===--------------------------------------------------------------------===//
374: void CheckpointWriter::WriteSequence(SequenceCatalogEntry &seq, Serializer &serializer) {
375: 	serializer.WriteProperty(100, "sequence", &seq);
376: }
377: 
378: void CheckpointReader::ReadSequence(ClientContext &context, Deserializer &deserializer) {
379: 	auto info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(100, "sequence");
380: 	auto &sequence_info = info->Cast<CreateSequenceInfo>();
381: 	catalog.CreateSequence(context, sequence_info);
382: }
383: 
384: //===--------------------------------------------------------------------===//
385: // Indexes
386: //===--------------------------------------------------------------------===//
387: void CheckpointWriter::WriteIndex(IndexCatalogEntry &index_catalog_entry, Serializer &serializer) {
388: 	// The index data is written as part of WriteTableData
389: 	// Here, we serialize the index catalog entry
390: 
391: 	// we need to keep the tag "index", even though it is slightly misleading
392: 	serializer.WriteProperty(100, "index", &index_catalog_entry);
393: }
394: 
395: void CheckpointReader::ReadIndex(ClientContext &context, Deserializer &deserializer) {
396: 
397: 	// we need to keep the tag "index", even though it is slightly misleading.
398: 	auto create_info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(100, "index");
399: 	auto &info = create_info->Cast<CreateIndexInfo>();
400: 
401: 	// also, we have to read the root_block_pointer, which will not be valid for newer storage versions.
402: 	// This leads to different code paths in this function.
403: 	auto root_block_pointer =
404: 	    deserializer.ReadPropertyWithDefault<BlockPointer>(101, "root_block_pointer", BlockPointer());
405: 
406: 	// create the index in the catalog
407: 
408: 	// look for the table in the catalog
409: 	auto &table =
410: 	    catalog.GetEntry(context, CatalogType::TABLE_ENTRY, create_info->schema, info.table).Cast<DuckTableEntry>();
411: 
412: 	// we also need to make sure the index type is loaded
413: 	// backwards compatability:
414: 	// if the index type is not specified, we default to ART
415: 	if (info.index_type.empty()) {
416: 		info.index_type = ART::TYPE_NAME;
417: 	}
418: 
419: 	// now we can look for the index in the catalog and assign the table info
420: 	auto &index = catalog.CreateIndex(context, info)->Cast<DuckIndexEntry>();
421: 	index.info = make_shared<IndexDataTableInfo>(table.GetStorage().info, info.index_name);
422: 
423: 	// insert the parsed expressions into the index so that we can (de)serialize them during consecutive checkpoints
424: 	for (auto &parsed_expr : info.parsed_expressions) {
425: 		index.parsed_expressions.push_back(parsed_expr->Copy());
426: 	}
427: 
428: 	// obtain the parsed expressions of the ART from the index metadata
429: 	vector<unique_ptr<ParsedExpression>> parsed_expressions;
430: 	for (auto &parsed_expr : info.parsed_expressions) {
431: 		parsed_expressions.push_back(parsed_expr->Copy());
432: 	}
433: 	D_ASSERT(!parsed_expressions.empty());
434: 
435: 	// add the table to the bind context to bind the parsed expressions
436: 	auto binder = Binder::CreateBinder(context);
437: 	vector<LogicalType> column_types;
438: 	vector<string> column_names;
439: 	for (auto &col : table.GetColumns().Logical()) {
440: 		column_types.push_back(col.Type());
441: 		column_names.push_back(col.Name());
442: 	}
443: 
444: 	// create a binder to bind the parsed expressions
445: 	vector<column_t> column_ids;
446: 	binder->bind_context.AddBaseTable(0, info.table, column_names, column_types, column_ids, &table);
447: 	IndexBinder idx_binder(*binder, context);
448: 
449: 	// bind the parsed expressions to create unbound expressions
450: 	vector<unique_ptr<Expression>> unbound_expressions;
451: 	unbound_expressions.reserve(parsed_expressions.size());
452: 	for (auto &expr : parsed_expressions) {
453: 		unbound_expressions.push_back(idx_binder.Bind(expr));
454: 	}
455: 
456: 	auto &data_table = table.GetStorage();
457: 	IndexStorageInfo index_storage_info;
458: 	if (root_block_pointer.IsValid()) {
459: 		// this code path is necessary to read older duckdb files
460: 		index_storage_info.name = info.index_name;
461: 		index_storage_info.root_block_ptr = root_block_pointer;
462: 
463: 	} else {
464: 		// get the matching index storage info
465: 		for (auto const &elem : data_table.info->index_storage_infos) {
466: 			if (elem.name == info.index_name) {
467: 				index_storage_info = elem;
468: 				break;
469: 			}
470: 		}
471: 	}
472: 
473: 	D_ASSERT(index_storage_info.IsValid() && !index_storage_info.name.empty());
474: 
475: 	// This is executed before any extensions can be loaded, which is why we must treat any index type that is not
476: 	// built-in (ART) as unknown
477: 	if (info.index_type == ART::TYPE_NAME) {
478: 		data_table.info->indexes.AddIndex(make_uniq<ART>(info.index_name, info.constraint_type, info.column_ids,
479: 		                                                 TableIOManager::Get(data_table), unbound_expressions,
480: 		                                                 data_table.db, nullptr, index_storage_info));
481: 	} else {
482: 		auto unknown_index = make_uniq<UnknownIndex>(info.index_name, info.index_type, info.constraint_type,
483: 		                                             info.column_ids, TableIOManager::Get(data_table),
484: 		                                             unbound_expressions, data_table.db, info, index_storage_info);
485: 
486: 		data_table.info->indexes.AddIndex(std::move(unknown_index));
487: 	}
488: }
489: 
490: //===--------------------------------------------------------------------===//
491: // Custom Types
492: //===--------------------------------------------------------------------===//
493: void CheckpointWriter::WriteType(TypeCatalogEntry &type, Serializer &serializer) {
494: 	serializer.WriteProperty(100, "type", &type);
495: }
496: 
497: void CheckpointReader::ReadType(ClientContext &context, Deserializer &deserializer) {
498: 	auto info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(100, "type");
499: 	auto &type_info = info->Cast<CreateTypeInfo>();
500: 	catalog.CreateType(context, type_info);
501: }
502: 
503: //===--------------------------------------------------------------------===//
504: // Macro's
505: //===--------------------------------------------------------------------===//
506: void CheckpointWriter::WriteMacro(ScalarMacroCatalogEntry &macro, Serializer &serializer) {
507: 	serializer.WriteProperty(100, "macro", &macro);
508: }
509: 
510: void CheckpointReader::ReadMacro(ClientContext &context, Deserializer &deserializer) {
511: 	auto info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(100, "macro");
512: 	auto &macro_info = info->Cast<CreateMacroInfo>();
513: 	catalog.CreateFunction(context, macro_info);
514: }
515: 
516: void CheckpointWriter::WriteTableMacro(TableMacroCatalogEntry &macro, Serializer &serializer) {
517: 	serializer.WriteProperty(100, "table_macro", &macro);
518: }
519: 
520: void CheckpointReader::ReadTableMacro(ClientContext &context, Deserializer &deserializer) {
521: 	auto info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(100, "table_macro");
522: 	auto &macro_info = info->Cast<CreateMacroInfo>();
523: 	catalog.CreateFunction(context, macro_info);
524: }
525: 
526: //===--------------------------------------------------------------------===//
527: // Table Metadata
528: //===--------------------------------------------------------------------===//
529: void CheckpointWriter::WriteTable(TableCatalogEntry &table, Serializer &serializer) {
530: 	// Write the table metadata
531: 	serializer.WriteProperty(100, "table", &table);
532: 
533: 	// Write the table data
534: 	if (auto writer = GetTableDataWriter(table)) {
535: 		writer->WriteTableData(serializer);
536: 	}
537: }
538: 
539: void CheckpointReader::ReadTable(ClientContext &context, Deserializer &deserializer) {
540: 	// deserialize the table meta data
541: 	auto info = deserializer.ReadProperty<unique_ptr<CreateInfo>>(100, "table");
542: 	auto binder = Binder::CreateBinder(context);
543: 	auto &schema = catalog.GetSchema(context, info->schema);
544: 	auto bound_info = binder->BindCreateTableInfo(std::move(info), schema);
545: 
546: 	// now read the actual table data and place it into the CreateTableInfo
547: 	ReadTableData(context, deserializer, *bound_info);
548: 
549: 	// finally create the table in the catalog
550: 	catalog.CreateTable(context, *bound_info);
551: }
552: 
553: void CheckpointReader::ReadTableData(ClientContext &context, Deserializer &deserializer,
554:                                      BoundCreateTableInfo &bound_info) {
555: 
556: 	// written in "SingleFileTableDataWriter::FinalizeTable"
557: 	auto table_pointer = deserializer.ReadProperty<MetaBlockPointer>(101, "table_pointer");
558: 	auto total_rows = deserializer.ReadProperty<idx_t>(102, "total_rows");
559: 
560: 	// old file read
561: 	auto index_pointers = deserializer.ReadPropertyWithDefault<vector<BlockPointer>>(103, "index_pointers", {});
562: 	// new file read
563: 	auto index_storage_infos =
564: 	    deserializer.ReadPropertyWithDefault<vector<IndexStorageInfo>>(104, "index_storage_infos", {});
565: 
566: 	if (!index_storage_infos.empty()) {
567: 		bound_info.indexes = index_storage_infos;
568: 
569: 	} else {
570: 		// old duckdb file containing index pointers
571: 		for (idx_t i = 0; i < index_pointers.size(); i++) {
572: 			IndexStorageInfo index_storage_info;
573: 			index_storage_info.root_block_ptr = index_pointers[i];
574: 			bound_info.indexes.push_back(index_storage_info);
575: 		}
576: 	}
577: 
578: 	// FIXME: icky downcast to get the underlying MetadataReader
579: 	auto &binary_deserializer = dynamic_cast<BinaryDeserializer &>(deserializer);
580: 	auto &reader = dynamic_cast<MetadataReader &>(binary_deserializer.GetStream());
581: 
582: 	MetadataReader table_data_reader(reader.GetMetadataManager(), table_pointer);
583: 	TableDataReader data_reader(table_data_reader, bound_info);
584: 	data_reader.ReadTableData();
585: 
586: 	bound_info.data->total_rows = total_rows;
587: }
588: 
589: } // namespace duckdb
[end of src/storage/checkpoint_manager.cpp]
[start of src/storage/storage_manager.cpp]
1: #include "duckdb/storage/storage_manager.hpp"
2: #include "duckdb/storage/checkpoint_manager.hpp"
3: #include "duckdb/storage/in_memory_block_manager.hpp"
4: #include "duckdb/storage/single_file_block_manager.hpp"
5: #include "duckdb/storage/object_cache.hpp"
6: 
7: #include "duckdb/catalog/catalog.hpp"
8: #include "duckdb/common/file_system.hpp"
9: #include "duckdb/main/database.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/function/function.hpp"
12: #include "duckdb/transaction/transaction_manager.hpp"
13: #include "duckdb/common/serializer/buffered_file_reader.hpp"
14: #include "duckdb/main/attached_database.hpp"
15: #include "duckdb/main/database_manager.hpp"
16: 
17: namespace duckdb {
18: 
19: StorageManager::StorageManager(AttachedDatabase &db, string path_p, bool read_only)
20:     : db(db), path(std::move(path_p)), read_only(read_only) {
21: 	if (path.empty()) {
22: 		path = IN_MEMORY_PATH;
23: 	} else {
24: 		auto &fs = FileSystem::Get(db);
25: 		this->path = fs.ExpandPath(path);
26: 	}
27: }
28: 
29: StorageManager::~StorageManager() {
30: }
31: 
32: StorageManager &StorageManager::Get(AttachedDatabase &db) {
33: 	return db.GetStorageManager();
34: }
35: StorageManager &StorageManager::Get(Catalog &catalog) {
36: 	return StorageManager::Get(catalog.GetAttached());
37: }
38: 
39: DatabaseInstance &StorageManager::GetDatabase() {
40: 	return db.GetDatabase();
41: }
42: 
43: BufferManager &BufferManager::GetBufferManager(ClientContext &context) {
44: 	return BufferManager::GetBufferManager(*context.db);
45: }
46: 
47: ObjectCache &ObjectCache::GetObjectCache(ClientContext &context) {
48: 	return context.db->GetObjectCache();
49: }
50: 
51: bool ObjectCache::ObjectCacheEnabled(ClientContext &context) {
52: 	return context.db->config.options.object_cache_enable;
53: }
54: 
55: optional_ptr<WriteAheadLog> StorageManager::GetWriteAheadLog() {
56: 	if (InMemory() || read_only || !load_complete) {
57: 		return nullptr;
58: 	}
59: 
60: 	if (wal) {
61: 		return wal.get();
62: 	}
63: 
64: 	// lazy WAL creation
65: 	wal = make_uniq<WriteAheadLog>(db, GetWALPath());
66: 	return wal.get();
67: }
68: 
69: string StorageManager::GetWALPath() {
70: 
71: 	std::size_t question_mark_pos = path.find('?');
72: 	auto wal_path = path;
73: 	if (question_mark_pos != std::string::npos) {
74: 		wal_path.insert(question_mark_pos, ".wal");
75: 	} else {
76: 		wal_path += ".wal";
77: 	}
78: 	return wal_path;
79: }
80: 
81: bool StorageManager::InMemory() {
82: 	D_ASSERT(!path.empty());
83: 	return path == IN_MEMORY_PATH;
84: }
85: 
86: void StorageManager::Initialize() {
87: 	bool in_memory = InMemory();
88: 	if (in_memory && read_only) {
89: 		throw CatalogException("Cannot launch in-memory database in read-only mode!");
90: 	}
91: 
92: 	// create or load the database from disk, if not in-memory mode
93: 	LoadDatabase();
94: }
95: 
96: ///////////////////////////////////////////////////////////////////////////
97: class SingleFileTableIOManager : public TableIOManager {
98: public:
99: 	explicit SingleFileTableIOManager(BlockManager &block_manager) : block_manager(block_manager) {
100: 	}
101: 
102: 	BlockManager &block_manager;
103: 
104: public:
105: 	BlockManager &GetIndexBlockManager() override {
106: 		return block_manager;
107: 	}
108: 	BlockManager &GetBlockManagerForRowData() override {
109: 		return block_manager;
110: 	}
111: 	MetadataManager &GetMetadataManager() override {
112: 		return block_manager.GetMetadataManager();
113: 	}
114: };
115: 
116: SingleFileStorageManager::SingleFileStorageManager(AttachedDatabase &db, string path, bool read_only)
117:     : StorageManager(db, std::move(path), read_only) {
118: }
119: 
120: void SingleFileStorageManager::LoadDatabase() {
121: 	if (InMemory()) {
122: 		block_manager = make_uniq<InMemoryBlockManager>(BufferManager::GetBufferManager(db));
123: 		table_io_manager = make_uniq<SingleFileTableIOManager>(*block_manager);
124: 		return;
125: 	}
126: 
127: 	auto &fs = FileSystem::Get(db);
128: 	auto &config = DBConfig::Get(db);
129: 	if (!config.options.enable_external_access) {
130: 		if (!db.IsInitialDatabase()) {
131: 			throw PermissionException("Attaching on-disk databases is disabled through configuration");
132: 		}
133: 	}
134: 
135: 	StorageManagerOptions options;
136: 	options.read_only = read_only;
137: 	options.use_direct_io = config.options.use_direct_io;
138: 	options.debug_initialize = config.options.debug_initialize;
139: 
140: 	// first check if the database exists
141: 	if (!read_only && !fs.FileExists(path)) {
142: 		// file does not exist and we are in read-write mode
143: 		// create a new file
144: 
145: 		// check if a WAL file already exists
146: 		auto wal_path = GetWALPath();
147: 		if (fs.FileExists(wal_path)) {
148: 			// WAL file exists but database file does not
149: 			// remove the WAL
150: 			fs.RemoveFile(wal_path);
151: 		}
152: 
153: 		// initialize the block manager while creating a new db file
154: 		auto sf_block_manager = make_uniq<SingleFileBlockManager>(db, path, options);
155: 		sf_block_manager->CreateNewDatabase();
156: 		block_manager = std::move(sf_block_manager);
157: 		table_io_manager = make_uniq<SingleFileTableIOManager>(*block_manager);
158: 	} else {
159: 		// either the file exists, or we are in read-only mode
160: 		// try to read the existing file on disk
161: 
162: 		// initialize the block manager while loading the current db file
163: 		auto sf_block_manager = make_uniq<SingleFileBlockManager>(db, path, options);
164: 		sf_block_manager->LoadExistingDatabase();
165: 		block_manager = std::move(sf_block_manager);
166: 		table_io_manager = make_uniq<SingleFileTableIOManager>(*block_manager);
167: 
168: 		// load the db from storage
169: 		auto checkpoint_reader = SingleFileCheckpointReader(*this);
170: 		checkpoint_reader.LoadFromStorage();
171: 
172: 		// check if the WAL file exists
173: 		auto wal_path = GetWALPath();
174: 		auto handle = fs.OpenFile(wal_path, FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_NULL_IF_NOT_EXISTS);
175: 		if (handle) {
176: 			// replay the WAL
177: 			if (WriteAheadLog::Replay(db, std::move(handle))) {
178: 				fs.RemoveFile(wal_path);
179: 			}
180: 		}
181: 	}
182: 
183: 	load_complete = true;
184: }
185: 
186: ///////////////////////////////////////////////////////////////////////////////
187: 
188: class SingleFileStorageCommitState : public StorageCommitState {
189: 	idx_t initial_wal_size = 0;
190: 	idx_t initial_written = 0;
191: 	optional_ptr<WriteAheadLog> log;
192: 	bool checkpoint;
193: 
194: public:
195: 	SingleFileStorageCommitState(StorageManager &storage_manager, bool checkpoint);
196: 	~SingleFileStorageCommitState() override {
197: 		// If log is non-null, then commit threw an exception before flushing.
198: 		if (log) {
199: 			auto &wal = *log.get();
200: 			wal.skip_writing = false;
201: 			if (wal.GetTotalWritten() > initial_written) {
202: 				// remove any entries written into the WAL by truncating it
203: 				wal.Truncate(initial_wal_size);
204: 			}
205: 		}
206: 	}
207: 
208: 	// Make the commit persistent
209: 	void FlushCommit() override;
210: };
211: 
212: SingleFileStorageCommitState::SingleFileStorageCommitState(StorageManager &storage_manager, bool checkpoint)
213:     : checkpoint(checkpoint) {
214: 	log = storage_manager.GetWriteAheadLog();
215: 	if (log) {
216: 		auto initial_size = log->GetWALSize();
217: 		initial_written = log->GetTotalWritten();
218: 		initial_wal_size = initial_size < 0 ? 0 : idx_t(initial_size);
219: 
220: 		if (checkpoint) {
221: 			// check if we are checkpointing after this commit
222: 			// if we are checkpointing, we don't need to write anything to the WAL
223: 			// this saves us a lot of unnecessary writes to disk in the case of large commits
224: 			log->skip_writing = true;
225: 		}
226: 	} else {
227: 		D_ASSERT(!checkpoint);
228: 	}
229: }
230: 
231: // Make the commit persistent
232: void SingleFileStorageCommitState::FlushCommit() {
233: 	if (log) {
234: 		// flush the WAL if any changes were made
235: 		if (log->GetTotalWritten() > initial_written) {
236: 			(void)checkpoint;
237: 			D_ASSERT(!checkpoint);
238: 			D_ASSERT(!log->skip_writing);
239: 			log->Flush();
240: 		}
241: 		log->skip_writing = false;
242: 	}
243: 	// Null so that the destructor will not truncate the log.
244: 	log = nullptr;
245: }
246: 
247: unique_ptr<StorageCommitState> SingleFileStorageManager::GenStorageCommitState(Transaction &transaction,
248:                                                                                bool checkpoint) {
249: 	return make_uniq<SingleFileStorageCommitState>(*this, checkpoint);
250: }
251: 
252: bool SingleFileStorageManager::IsCheckpointClean(MetaBlockPointer checkpoint_id) {
253: 	return block_manager->IsRootBlock(checkpoint_id);
254: }
255: 
256: void SingleFileStorageManager::CreateCheckpoint(bool delete_wal, bool force_checkpoint) {
257: 	if (InMemory() || read_only || !wal) {
258: 		return;
259: 	}
260: 	auto &config = DBConfig::Get(db);
261: 	if (wal->GetWALSize() > 0 || config.options.force_checkpoint || force_checkpoint) {
262: 		// we only need to checkpoint if there is anything in the WAL
263: 		try {
264: 			SingleFileCheckpointWriter checkpointer(db, *block_manager);
265: 			checkpointer.CreateCheckpoint();
266: 		} catch (std::exception &ex) {
267: 			ErrorData error(ex);
268: 			throw FatalException("Failed to create checkpoint because of error: %s", error.RawMessage());
269: 		}
270: 	}
271: 	if (delete_wal) {
272: 		wal->Delete();
273: 		wal.reset();
274: 	}
275: }
276: 
277: DatabaseSize SingleFileStorageManager::GetDatabaseSize() {
278: 	// All members default to zero
279: 	DatabaseSize ds;
280: 	if (!InMemory()) {
281: 		ds.total_blocks = block_manager->TotalBlocks();
282: 		ds.block_size = Storage::BLOCK_ALLOC_SIZE;
283: 		ds.free_blocks = block_manager->FreeBlocks();
284: 		ds.used_blocks = ds.total_blocks - ds.free_blocks;
285: 		ds.bytes = (ds.total_blocks * ds.block_size);
286: 		if (auto wal = GetWriteAheadLog()) {
287: 			ds.wal_size = wal->GetWALSize();
288: 		}
289: 	}
290: 	return ds;
291: }
292: 
293: vector<MetadataBlockInfo> SingleFileStorageManager::GetMetadataInfo() {
294: 	auto &metadata_manager = block_manager->GetMetadataManager();
295: 	return metadata_manager.GetMetadataInfo();
296: }
297: 
298: bool SingleFileStorageManager::AutomaticCheckpoint(idx_t estimated_wal_bytes) {
299: 	auto log = GetWriteAheadLog();
300: 	if (!log) {
301: 		return false;
302: 	}
303: 
304: 	auto &config = DBConfig::Get(db);
305: 	auto initial_size = log->GetWALSize();
306: 	idx_t expected_wal_size = initial_size + estimated_wal_bytes;
307: 	return expected_wal_size > config.options.checkpoint_wal_size;
308: }
309: 
310: shared_ptr<TableIOManager> SingleFileStorageManager::GetTableIOManager(BoundCreateTableInfo *info /*info*/) {
311: 	// This is an unmanaged reference. No ref/deref overhead. Lifetime of the
312: 	// TableIoManager follows lifetime of the StorageManager (this).
313: 	return shared_ptr<TableIOManager>(shared_ptr<char>(nullptr), table_io_manager.get());
314: }
315: 
316: } // namespace duckdb
[end of src/storage/storage_manager.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: