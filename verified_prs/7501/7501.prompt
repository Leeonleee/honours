You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
PRAGMA database_size in read-only CLI mode does not calculate free/used blocks correctly
### What happens?

It looks like read-only mode, at least with the CLI, ends up not calculating free/used blocks correctly.  The JDBC driver appears to do it correctly.

```
$ duckdb example.db "pragma database_size;"
┌───────────────┬────────────┬──────────────┬─────────────┬─────────────┬──────────┬──────────────┬──────────────┐
│ database_size │ block_size │ total_blocks │ used_blocks │ free_blocks │ wal_size │ memory_usage │ memory_limit │
│    varchar    │   int64    │    int64     │    int64    │    int64    │ varchar  │   varchar    │   varchar    │
├───────────────┼────────────┼──────────────┼─────────────┼─────────────┼──────────┼──────────────┼──────────────┤
│ 4.7MB         │     262144 │           18 │          12 │           6 │ 0 bytes  │ 0 bytes      │ 27.4GB       │
└───────────────┴────────────┴──────────────┴─────────────┴─────────────┴──────────┴──────────────┴──────────────┘
$ duckdb -readonly example.db "pragma database_size;"
┌───────────────┬────────────┬──────────────┬─────────────┬─────────────┬──────────┬──────────────┬──────────────┐
│ database_size │ block_size │ total_blocks │ used_blocks │ free_blocks │ wal_size │ memory_usage │ memory_limit │
│    varchar    │   int64    │    int64     │    int64    │    int64    │ varchar  │   varchar    │   varchar    │
├───────────────┼────────────┼──────────────┼─────────────┼─────────────┼──────────┼──────────────┼──────────────┤
│ 4.7MB         │     262144 │           18 │          18 │           0 │ 0 bytes  │ 0 bytes      │ 27.4GB       │
└───────────────┴────────────┴──────────────┴─────────────┴─────────────┴──────────┴──────────────┴──────────────┘
```

### To Reproduce

Compare:
`duckdb -readonly example.db "pragma database_size;"`
to
`duckdb example.db "pragma database_size;"`

### OS:

Mac OSX AMD64

### DuckDB Version:

0.6.1

### DuckDB Client:

CLI

### Full Name:

Paul Ellsworth

### Affiliation:

Tenable, Inc.

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree
PRAGMA database_size in read-only CLI mode does not calculate free/used blocks correctly
### What happens?

It looks like read-only mode, at least with the CLI, ends up not calculating free/used blocks correctly.  The JDBC driver appears to do it correctly.

```
$ duckdb example.db "pragma database_size;"
┌───────────────┬────────────┬──────────────┬─────────────┬─────────────┬──────────┬──────────────┬──────────────┐
│ database_size │ block_size │ total_blocks │ used_blocks │ free_blocks │ wal_size │ memory_usage │ memory_limit │
│    varchar    │   int64    │    int64     │    int64    │    int64    │ varchar  │   varchar    │   varchar    │
├───────────────┼────────────┼──────────────┼─────────────┼─────────────┼──────────┼──────────────┼──────────────┤
│ 4.7MB         │     262144 │           18 │          12 │           6 │ 0 bytes  │ 0 bytes      │ 27.4GB       │
└───────────────┴────────────┴──────────────┴─────────────┴─────────────┴──────────┴──────────────┴──────────────┘
$ duckdb -readonly example.db "pragma database_size;"
┌───────────────┬────────────┬──────────────┬─────────────┬─────────────┬──────────┬──────────────┬──────────────┐
│ database_size │ block_size │ total_blocks │ used_blocks │ free_blocks │ wal_size │ memory_usage │ memory_limit │
│    varchar    │   int64    │    int64     │    int64    │    int64    │ varchar  │   varchar    │   varchar    │
├───────────────┼────────────┼──────────────┼─────────────┼─────────────┼──────────┼──────────────┼──────────────┤
│ 4.7MB         │     262144 │           18 │          18 │           0 │ 0 bytes  │ 0 bytes      │ 27.4GB       │
└───────────────┴────────────┴──────────────┴─────────────┴─────────────┴──────────┴──────────────┴──────────────┘
```

### To Reproduce

Compare:
`duckdb -readonly example.db "pragma database_size;"`
to
`duckdb example.db "pragma database_size;"`

### OS:

Mac OSX AMD64

### DuckDB Version:

0.6.1

### DuckDB Client:

CLI

### Full Name:

Paul Ellsworth

### Affiliation:

Tenable, Inc.

### Have you tried this on the latest `master` branch?

- [X] I agree

### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?

- [X] I agree

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=master" alt="Github Actions Badge">
9:   </a>
10:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
11:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
12:   </a>
13:   <a href="https://discord.gg/tcvwpjfnZx">
14:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
15:   </a>
16:   <a href="https://github.com/duckdb/duckdb/releases/">
17:     <img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release">
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The detail of benchmarks is in our [Benchmark Guide](benchmark/README.md).
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
[end of README.md]
[start of src/storage/single_file_block_manager.cpp]
1: #include "duckdb/storage/single_file_block_manager.hpp"
2: 
3: #include "duckdb/common/allocator.hpp"
4: #include "duckdb/common/checksum.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/serializer/buffered_deserializer.hpp"
7: #include "duckdb/common/serializer/buffered_serializer.hpp"
8: #include "duckdb/common/field_writer.hpp"
9: #include "duckdb/storage/meta_block_reader.hpp"
10: #include "duckdb/storage/meta_block_writer.hpp"
11: #include "duckdb/storage/buffer_manager.hpp"
12: #include "duckdb/main/config.hpp"
13: 
14: #include <algorithm>
15: #include <cstring>
16: 
17: namespace duckdb {
18: 
19: const char MainHeader::MAGIC_BYTES[] = "DUCK";
20: 
21: void MainHeader::Serialize(Serializer &ser) {
22: 	ser.WriteData((data_ptr_t)MAGIC_BYTES, MAGIC_BYTE_SIZE);
23: 	ser.Write<uint64_t>(version_number);
24: 	FieldWriter writer(ser);
25: 	for (idx_t i = 0; i < FLAG_COUNT; i++) {
26: 		writer.WriteField<uint64_t>(flags[i]);
27: 	}
28: 	writer.Finalize();
29: }
30: 
31: void MainHeader::CheckMagicBytes(FileHandle &handle) {
32: 	data_t magic_bytes[MAGIC_BYTE_SIZE];
33: 	if (handle.GetFileSize() < MainHeader::MAGIC_BYTE_SIZE + MainHeader::MAGIC_BYTE_OFFSET) {
34: 		throw IOException("The file \"%s\" exists, but it is not a valid DuckDB database file!", handle.path);
35: 	}
36: 	handle.Read(magic_bytes, MainHeader::MAGIC_BYTE_SIZE, MainHeader::MAGIC_BYTE_OFFSET);
37: 	if (memcmp(magic_bytes, MainHeader::MAGIC_BYTES, MainHeader::MAGIC_BYTE_SIZE) != 0) {
38: 		throw IOException("The file \"%s\" exists, but it is not a valid DuckDB database file!", handle.path);
39: 	}
40: }
41: 
42: MainHeader MainHeader::Deserialize(Deserializer &source) {
43: 	data_t magic_bytes[MAGIC_BYTE_SIZE];
44: 	MainHeader header;
45: 	source.ReadData(magic_bytes, MainHeader::MAGIC_BYTE_SIZE);
46: 	if (memcmp(magic_bytes, MainHeader::MAGIC_BYTES, MainHeader::MAGIC_BYTE_SIZE) != 0) {
47: 		throw IOException("The file is not a valid DuckDB database file!");
48: 	}
49: 	header.version_number = source.Read<uint64_t>();
50: 	// check the version number
51: 	if (header.version_number != VERSION_NUMBER) {
52: 		auto version = GetDuckDBVersion(header.version_number);
53: 		string version_text;
54: 		if (version) {
55: 			// known version
56: 			version_text = "DuckDB version " + string(version);
57: 		} else {
58: 			version_text = string("an ") + (VERSION_NUMBER > header.version_number ? "older development" : "newer") +
59: 			               string(" version of DuckDB");
60: 		}
61: 		throw IOException(
62: 		    "Trying to read a database file with version number %lld, but we can only read version %lld.\n"
63: 		    "The database file was created with %s.\n\n"
64: 		    "The storage of DuckDB is not yet stable; newer versions of DuckDB cannot read old database files and "
65: 		    "vice versa.\n"
66: 		    "The storage will be stabilized when version 1.0 releases.\n\n"
67: 		    "For now, we recommend that you load the database file in a supported version of DuckDB, and use the "
68: 		    "EXPORT DATABASE command "
69: 		    "followed by IMPORT DATABASE on the current version of DuckDB.\n\n"
70: 		    "See the storage page for more information: https://duckdb.org/internals/storage",
71: 		    header.version_number, VERSION_NUMBER, version_text);
72: 	}
73: 	// read the flags
74: 	FieldReader reader(source);
75: 	for (idx_t i = 0; i < FLAG_COUNT; i++) {
76: 		header.flags[i] = reader.ReadRequired<uint64_t>();
77: 	}
78: 	reader.Finalize();
79: 	return header;
80: }
81: 
82: void DatabaseHeader::Serialize(Serializer &ser) {
83: 	ser.Write<uint64_t>(iteration);
84: 	ser.Write<block_id_t>(meta_block);
85: 	ser.Write<block_id_t>(free_list);
86: 	ser.Write<uint64_t>(block_count);
87: }
88: 
89: DatabaseHeader DatabaseHeader::Deserialize(Deserializer &source) {
90: 	DatabaseHeader header;
91: 	header.iteration = source.Read<uint64_t>();
92: 	header.meta_block = source.Read<block_id_t>();
93: 	header.free_list = source.Read<block_id_t>();
94: 	header.block_count = source.Read<uint64_t>();
95: 	return header;
96: }
97: 
98: template <class T>
99: void SerializeHeaderStructure(T header, data_ptr_t ptr) {
100: 	BufferedSerializer ser(ptr, Storage::FILE_HEADER_SIZE);
101: 	header.Serialize(ser);
102: }
103: 
104: template <class T>
105: T DeserializeHeaderStructure(data_ptr_t ptr) {
106: 	BufferedDeserializer source(ptr, Storage::FILE_HEADER_SIZE);
107: 	return T::Deserialize(source);
108: }
109: 
110: SingleFileBlockManager::SingleFileBlockManager(AttachedDatabase &db, string path_p, StorageManagerOptions options)
111:     : BlockManager(BufferManager::GetBufferManager(db)), db(db), path(std::move(path_p)),
112:       header_buffer(Allocator::Get(db), FileBufferType::MANAGED_BUFFER,
113:                     Storage::FILE_HEADER_SIZE - Storage::BLOCK_HEADER_SIZE),
114:       iteration_count(0), options(options) {
115: }
116: 
117: void SingleFileBlockManager::GetFileFlags(uint8_t &flags, FileLockType &lock, bool create_new) {
118: 	if (options.read_only) {
119: 		D_ASSERT(!create_new);
120: 		flags = FileFlags::FILE_FLAGS_READ;
121: 		lock = FileLockType::READ_LOCK;
122: 	} else {
123: 		flags = FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_READ;
124: 		lock = FileLockType::WRITE_LOCK;
125: 		if (create_new) {
126: 			flags |= FileFlags::FILE_FLAGS_FILE_CREATE;
127: 		}
128: 	}
129: 	if (options.use_direct_io) {
130: 		flags |= FileFlags::FILE_FLAGS_DIRECT_IO;
131: 	}
132: }
133: 
134: void SingleFileBlockManager::CreateNewDatabase() {
135: 	uint8_t flags;
136: 	FileLockType lock;
137: 	GetFileFlags(flags, lock, true);
138: 
139: 	// open the RDBMS handle
140: 	auto &fs = FileSystem::Get(db);
141: 	handle = fs.OpenFile(path, flags, lock);
142: 
143: 	// if we create a new file, we fill the metadata of the file
144: 	// first fill in the new header
145: 	header_buffer.Clear();
146: 
147: 	MainHeader main_header;
148: 	main_header.version_number = VERSION_NUMBER;
149: 	memset(main_header.flags, 0, sizeof(uint64_t) * 4);
150: 
151: 	SerializeHeaderStructure<MainHeader>(main_header, header_buffer.buffer);
152: 	// now write the header to the file
153: 	ChecksumAndWrite(header_buffer, 0);
154: 	header_buffer.Clear();
155: 
156: 	// write the database headers
157: 	// initialize meta_block and free_list to INVALID_BLOCK because the database file does not contain any actual
158: 	// content yet
159: 	DatabaseHeader h1, h2;
160: 	// header 1
161: 	h1.iteration = 0;
162: 	h1.meta_block = INVALID_BLOCK;
163: 	h1.free_list = INVALID_BLOCK;
164: 	h1.block_count = 0;
165: 	SerializeHeaderStructure<DatabaseHeader>(h1, header_buffer.buffer);
166: 	ChecksumAndWrite(header_buffer, Storage::FILE_HEADER_SIZE);
167: 	// header 2
168: 	h2.iteration = 0;
169: 	h2.meta_block = INVALID_BLOCK;
170: 	h2.free_list = INVALID_BLOCK;
171: 	h2.block_count = 0;
172: 	SerializeHeaderStructure<DatabaseHeader>(h2, header_buffer.buffer);
173: 	ChecksumAndWrite(header_buffer, Storage::FILE_HEADER_SIZE * 2);
174: 	// ensure that writing to disk is completed before returning
175: 	handle->Sync();
176: 	// we start with h2 as active_header, this way our initial write will be in h1
177: 	iteration_count = 0;
178: 	active_header = 1;
179: 	max_block = 0;
180: }
181: 
182: void SingleFileBlockManager::LoadExistingDatabase() {
183: 	uint8_t flags;
184: 	FileLockType lock;
185: 	GetFileFlags(flags, lock, false);
186: 
187: 	// open the RDBMS handle
188: 	auto &fs = FileSystem::Get(db);
189: 	handle = fs.OpenFile(path, flags, lock);
190: 
191: 	MainHeader::CheckMagicBytes(*handle);
192: 	// otherwise, we check the metadata of the file
193: 	ReadAndChecksum(header_buffer, 0);
194: 	DeserializeHeaderStructure<MainHeader>(header_buffer.buffer);
195: 
196: 	// read the database headers from disk
197: 	DatabaseHeader h1, h2;
198: 	ReadAndChecksum(header_buffer, Storage::FILE_HEADER_SIZE);
199: 	h1 = DeserializeHeaderStructure<DatabaseHeader>(header_buffer.buffer);
200: 	ReadAndChecksum(header_buffer, Storage::FILE_HEADER_SIZE * 2);
201: 	h2 = DeserializeHeaderStructure<DatabaseHeader>(header_buffer.buffer);
202: 	// check the header with the highest iteration count
203: 	if (h1.iteration > h2.iteration) {
204: 		// h1 is active header
205: 		active_header = 0;
206: 		Initialize(h1);
207: 	} else {
208: 		// h2 is active header
209: 		active_header = 1;
210: 		Initialize(h2);
211: 	}
212: 	LoadFreeList();
213: }
214: 
215: void SingleFileBlockManager::ReadAndChecksum(FileBuffer &block, uint64_t location) const {
216: 	// read the buffer from disk
217: 	block.Read(*handle, location);
218: 	// compute the checksum
219: 	auto stored_checksum = Load<uint64_t>(block.InternalBuffer());
220: 	uint64_t computed_checksum = Checksum(block.buffer, block.size);
221: 	// verify the checksum
222: 	if (stored_checksum != computed_checksum) {
223: 		throw IOException("Corrupt database file: computed checksum %llu does not match stored checksum %llu in block",
224: 		                  computed_checksum, stored_checksum);
225: 	}
226: }
227: 
228: void SingleFileBlockManager::ChecksumAndWrite(FileBuffer &block, uint64_t location) const {
229: 	// compute the checksum and write it to the start of the buffer (if not temp buffer)
230: 	uint64_t checksum = Checksum(block.buffer, block.size);
231: 	Store<uint64_t>(checksum, block.InternalBuffer());
232: 	// now write the buffer
233: 	block.Write(*handle, location);
234: }
235: 
236: void SingleFileBlockManager::Initialize(DatabaseHeader &header) {
237: 	free_list_id = header.free_list;
238: 	meta_block = header.meta_block;
239: 	iteration_count = header.iteration;
240: 	max_block = header.block_count;
241: }
242: 
243: void SingleFileBlockManager::LoadFreeList() {
244: 	if (options.read_only) {
245: 		// no need to load free list for read only db
246: 		return;
247: 	}
248: 	if (free_list_id == INVALID_BLOCK) {
249: 		// no free list
250: 		return;
251: 	}
252: 	MetaBlockReader reader(*this, free_list_id);
253: 	auto free_list_count = reader.Read<uint64_t>();
254: 	free_list.clear();
255: 	for (idx_t i = 0; i < free_list_count; i++) {
256: 		free_list.insert(reader.Read<block_id_t>());
257: 	}
258: 	auto multi_use_blocks_count = reader.Read<uint64_t>();
259: 	multi_use_blocks.clear();
260: 	for (idx_t i = 0; i < multi_use_blocks_count; i++) {
261: 		auto block_id = reader.Read<block_id_t>();
262: 		auto usage_count = reader.Read<uint32_t>();
263: 		multi_use_blocks[block_id] = usage_count;
264: 	}
265: }
266: 
267: bool SingleFileBlockManager::IsRootBlock(block_id_t root) {
268: 	return root == meta_block;
269: }
270: 
271: block_id_t SingleFileBlockManager::GetFreeBlockId() {
272: 	lock_guard<mutex> lock(block_lock);
273: 	block_id_t block;
274: 	if (!free_list.empty()) {
275: 		// free list is non empty
276: 		// take an entry from the free list
277: 		block = *free_list.begin();
278: 		// erase the entry from the free list again
279: 		free_list.erase(free_list.begin());
280: 	} else {
281: 		block = max_block++;
282: 	}
283: 	return block;
284: }
285: 
286: void SingleFileBlockManager::MarkBlockAsFree(block_id_t block_id) {
287: 	lock_guard<mutex> lock(block_lock);
288: 	D_ASSERT(block_id >= 0);
289: 	D_ASSERT(block_id < max_block);
290: 	if (free_list.find(block_id) != free_list.end()) {
291: 		throw InternalException("MarkBlockAsFree called but block %llu was already freed!", block_id);
292: 	}
293: 	multi_use_blocks.erase(block_id);
294: 	free_list.insert(block_id);
295: }
296: 
297: void SingleFileBlockManager::MarkBlockAsModified(block_id_t block_id) {
298: 	lock_guard<mutex> lock(block_lock);
299: 	D_ASSERT(block_id >= 0);
300: 	D_ASSERT(block_id < max_block);
301: 
302: 	// check if the block is a multi-use block
303: 	auto entry = multi_use_blocks.find(block_id);
304: 	if (entry != multi_use_blocks.end()) {
305: 		// it is! reduce the reference count of the block
306: 		entry->second--;
307: 		// check the reference count: is the block still a multi-use block?
308: 		if (entry->second <= 1) {
309: 			// no longer a multi-use block!
310: 			multi_use_blocks.erase(entry);
311: 		}
312: 		return;
313: 	}
314: 	// Check for multi-free
315: 	// TODO: Fix the bug that causes this assert to fire, then uncomment it.
316: 	// D_ASSERT(modified_blocks.find(block_id) == modified_blocks.end());
317: 	D_ASSERT(free_list.find(block_id) == free_list.end());
318: 	modified_blocks.insert(block_id);
319: }
320: 
321: void SingleFileBlockManager::IncreaseBlockReferenceCount(block_id_t block_id) {
322: 	lock_guard<mutex> lock(block_lock);
323: 	D_ASSERT(block_id >= 0);
324: 	D_ASSERT(block_id < max_block);
325: 	D_ASSERT(free_list.find(block_id) == free_list.end());
326: 	auto entry = multi_use_blocks.find(block_id);
327: 	if (entry != multi_use_blocks.end()) {
328: 		entry->second++;
329: 	} else {
330: 		multi_use_blocks[block_id] = 2;
331: 	}
332: }
333: 
334: block_id_t SingleFileBlockManager::GetMetaBlock() {
335: 	return meta_block;
336: }
337: 
338: idx_t SingleFileBlockManager::TotalBlocks() {
339: 	lock_guard<mutex> lock(block_lock);
340: 	return max_block;
341: }
342: 
343: idx_t SingleFileBlockManager::FreeBlocks() {
344: 	lock_guard<mutex> lock(block_lock);
345: 	return free_list.size();
346: }
347: 
348: unique_ptr<Block> SingleFileBlockManager::ConvertBlock(block_id_t block_id, FileBuffer &source_buffer) {
349: 	D_ASSERT(source_buffer.AllocSize() == Storage::BLOCK_ALLOC_SIZE);
350: 	return make_uniq<Block>(source_buffer, block_id);
351: }
352: 
353: unique_ptr<Block> SingleFileBlockManager::CreateBlock(block_id_t block_id, FileBuffer *source_buffer) {
354: 	unique_ptr<Block> result;
355: 	if (source_buffer) {
356: 		result = ConvertBlock(block_id, *source_buffer);
357: 	} else {
358: 		result = make_uniq<Block>(Allocator::Get(db), block_id);
359: 	}
360: 	result->Initialize(options.debug_initialize);
361: 	return result;
362: }
363: 
364: void SingleFileBlockManager::Read(Block &block) {
365: 	D_ASSERT(block.id >= 0);
366: 	D_ASSERT(std::find(free_list.begin(), free_list.end(), block.id) == free_list.end());
367: 	ReadAndChecksum(block, BLOCK_START + block.id * Storage::BLOCK_ALLOC_SIZE);
368: }
369: 
370: void SingleFileBlockManager::Write(FileBuffer &buffer, block_id_t block_id) {
371: 	D_ASSERT(block_id >= 0);
372: 	ChecksumAndWrite(buffer, BLOCK_START + block_id * Storage::BLOCK_ALLOC_SIZE);
373: }
374: 
375: vector<block_id_t> SingleFileBlockManager::GetFreeListBlocks() {
376: 	vector<block_id_t> free_list_blocks;
377: 
378: 	if (!free_list.empty() || !multi_use_blocks.empty() || !modified_blocks.empty()) {
379: 		// there are blocks in the free list or multi_use_blocks
380: 		// figure out how many blocks we need to write these to the file
381: 		auto free_list_size = sizeof(uint64_t) + sizeof(block_id_t) * (free_list.size() + modified_blocks.size());
382: 		auto multi_use_blocks_size =
383: 		    sizeof(uint64_t) + (sizeof(block_id_t) + sizeof(uint32_t)) * multi_use_blocks.size();
384: 		auto total_size = free_list_size + multi_use_blocks_size;
385: 		// because of potential alignment issues and needing to store a next pointer in a block we subtract
386: 		// a bit from the max block size
387: 		auto space_in_block = Storage::BLOCK_SIZE - 4 * sizeof(block_id_t);
388: 		auto total_blocks = (total_size + space_in_block - 1) / space_in_block;
389: 		D_ASSERT(total_size > 0);
390: 		D_ASSERT(total_blocks > 0);
391: 
392: 		// reserve the blocks that we are going to write
393: 		// since these blocks are no longer free we cannot just include them in the free list!
394: 		for (idx_t i = 0; i < total_blocks; i++) {
395: 			auto block_id = GetFreeBlockId();
396: 			free_list_blocks.push_back(block_id);
397: 		}
398: 	}
399: 
400: 	return free_list_blocks;
401: }
402: 
403: class FreeListBlockWriter : public MetaBlockWriter {
404: public:
405: 	FreeListBlockWriter(BlockManager &block_manager, vector<block_id_t> &free_list_blocks_p)
406: 	    : MetaBlockWriter(block_manager, free_list_blocks_p[0]), free_list_blocks(free_list_blocks_p), index(1) {
407: 	}
408: 
409: 	vector<block_id_t> &free_list_blocks;
410: 	idx_t index;
411: 
412: protected:
413: 	block_id_t GetNextBlockId() override {
414: 		if (index >= free_list_blocks.size()) {
415: 			throw InternalException(
416: 			    "Free List Block Writer ran out of blocks, this means not enough blocks were allocated up front");
417: 		}
418: 		return free_list_blocks[index++];
419: 	}
420: };
421: 
422: void SingleFileBlockManager::WriteHeader(DatabaseHeader header) {
423: 	// set the iteration count
424: 	header.iteration = ++iteration_count;
425: 
426: 	vector<block_id_t> free_list_blocks = GetFreeListBlocks();
427: 
428: 	// now handle the free list
429: 	// add all modified blocks to the free list: they can now be written to again
430: 	for (auto &block : modified_blocks) {
431: 		free_list.insert(block);
432: 	}
433: 	modified_blocks.clear();
434: 
435: 	if (!free_list_blocks.empty()) {
436: 		// there are blocks to write, either in the free_list or in the modified_blocks
437: 		// we write these blocks specifically to the free_list_blocks
438: 		// a normal MetaBlockWriter will fetch blocks to use from the free_list
439: 		// but since we are WRITING the free_list, this behavior is sub-optimal
440: 
441: 		FreeListBlockWriter writer(*this, free_list_blocks);
442: 
443: 		auto ptr = writer.GetBlockPointer();
444: 		D_ASSERT(ptr.block_id == free_list_blocks[0]);
445: 		header.free_list = ptr.block_id;
446: 		for (auto &block_id : free_list_blocks) {
447: 			modified_blocks.insert(block_id);
448: 		}
449: 
450: 		writer.Write<uint64_t>(free_list.size());
451: 		for (auto &block_id : free_list) {
452: 			writer.Write<block_id_t>(block_id);
453: 		}
454: 		writer.Write<uint64_t>(multi_use_blocks.size());
455: 		for (auto &entry : multi_use_blocks) {
456: 			writer.Write<block_id_t>(entry.first);
457: 			writer.Write<uint32_t>(entry.second);
458: 		}
459: 		writer.Flush();
460: 	} else {
461: 		// no blocks in the free list
462: 		header.free_list = INVALID_BLOCK;
463: 	}
464: 	header.block_count = max_block;
465: 
466: 	auto &config = DBConfig::Get(db);
467: 	if (config.options.checkpoint_abort == CheckpointAbort::DEBUG_ABORT_AFTER_FREE_LIST_WRITE) {
468: 		throw FatalException("Checkpoint aborted after free list write because of PRAGMA checkpoint_abort flag");
469: 	}
470: 
471: 	if (!options.use_direct_io) {
472: 		// if we are not using Direct IO we need to fsync BEFORE we write the header to ensure that all the previous
473: 		// blocks are written as well
474: 		handle->Sync();
475: 	}
476: 	// set the header inside the buffer
477: 	header_buffer.Clear();
478: 	Store<DatabaseHeader>(header, header_buffer.buffer);
479: 	// now write the header to the file, active_header determines whether we write to h1 or h2
480: 	// note that if active_header is h1 we write to h2, and vice versa
481: 	ChecksumAndWrite(header_buffer, active_header == 1 ? Storage::FILE_HEADER_SIZE : Storage::FILE_HEADER_SIZE * 2);
482: 	// switch active header to the other header
483: 	active_header = 1 - active_header;
484: 	//! Ensure the header write ends up on disk
485: 	handle->Sync();
486: }
487: 
488: } // namespace duckdb
[end of src/storage/single_file_block_manager.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: