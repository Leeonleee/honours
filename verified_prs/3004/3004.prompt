You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Make BufferedCSVReader's MAXIMUM_CSV_LINE_SIZE configurable
The `MAXIMUM_CSV_LINE_SIZE` setting in `BufferedCSVReader` is [hard coded to 1MB](https://github.com/duckdb/duckdb/blob/477c52aed0075aec926303d4f6b433cd48805a3d/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp#L114-L115). While this is probably sufficient for most, our application allows up to 1MB per column and 2MB per row. As a result, we're unable to load our CSV files using `COPY … FROM …` due to this hard coded limitation.

It would be helpful to have `read_csv_auto` and `COPY … FROM …` both extended to support a `MAXIMUM_CSV_LINE_SIZE` (or similar, perhaps shorter named) parameter to allow the application to override the 1MB default value.
Make BufferedCSVReader's MAXIMUM_CSV_LINE_SIZE configurable
The `MAXIMUM_CSV_LINE_SIZE` setting in `BufferedCSVReader` is [hard coded to 1MB](https://github.com/duckdb/duckdb/blob/477c52aed0075aec926303d4f6b433cd48805a3d/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp#L114-L115). While this is probably sufficient for most, our application allows up to 1MB per column and 2MB per row. As a result, we're unable to load our CSV files using `COPY … FROM …` due to this hard coded limitation.

It would be helpful to have `read_csv_auto` and `COPY … FROM …` both extended to support a `MAXIMUM_CSV_LINE_SIZE` (or similar, perhaps shorter named) parameter to allow the application to override the 1MB default value.

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of src/execution/operator/persistent/buffered_csv_reader.cpp]
1: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/file_system.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/types/cast_helpers.hpp"
8: #include "duckdb/common/vector_operations/unary_executor.hpp"
9: #include "duckdb/common/vector_operations/vector_operations.hpp"
10: #include "duckdb/function/scalar/strftime.hpp"
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/parser/column_definition.hpp"
13: #include "duckdb/storage/data_table.hpp"
14: #include "utf8proc_wrapper.hpp"
15: #include "utf8proc.hpp"
16: #include "duckdb/parser/keyword_helper.hpp"
17: 
18: #include <algorithm>
19: #include <cctype>
20: #include <cstring>
21: #include <fstream>
22: 
23: namespace duckdb {
24: 
25: struct CSVFileHandle {
26: public:
27: 	explicit CSVFileHandle(unique_ptr<FileHandle> file_handle_p) : file_handle(move(file_handle_p)) {
28: 		can_seek = file_handle->CanSeek();
29: 		plain_file_source = file_handle->OnDiskFile() && can_seek;
30: 		file_size = file_handle->GetFileSize();
31: 	}
32: 
33: 	bool CanSeek() {
34: 		return can_seek;
35: 	}
36: 	void Seek(idx_t position) {
37: 		if (!can_seek) {
38: 			throw InternalException("Cannot seek in this file");
39: 		}
40: 		file_handle->Seek(position);
41: 	}
42: 	idx_t SeekPosition() {
43: 		if (!can_seek) {
44: 			throw InternalException("Cannot seek in this file");
45: 		}
46: 		return file_handle->SeekPosition();
47: 	}
48: 	void Reset() {
49: 		if (plain_file_source) {
50: 			file_handle->Reset();
51: 		} else {
52: 			if (!reset_enabled) {
53: 				throw InternalException("Reset called but reset is not enabled for this CSV Handle");
54: 			}
55: 			read_position = 0;
56: 		}
57: 	}
58: 	bool PlainFileSource() {
59: 		return plain_file_source;
60: 	}
61: 
62: 	idx_t FileSize() {
63: 		return file_size;
64: 	}
65: 
66: 	idx_t Read(void *buffer, idx_t nr_bytes) {
67: 		if (!plain_file_source) {
68: 			// not a plain file source: we need to do some bookkeeping around the reset functionality
69: 			idx_t result_offset = 0;
70: 			if (read_position < buffer_size) {
71: 				// we need to read from our cached buffer
72: 				auto buffer_read_count = MinValue<idx_t>(nr_bytes, buffer_size - read_position);
73: 				memcpy(buffer, cached_buffer.get() + read_position, buffer_read_count);
74: 				result_offset += buffer_read_count;
75: 				read_position += buffer_read_count;
76: 				if (result_offset == nr_bytes) {
77: 					return nr_bytes;
78: 				}
79: 			} else if (!reset_enabled && cached_buffer) {
80: 				// reset is disabled but we still have cached data
81: 				// we can remove any cached data
82: 				cached_buffer.reset();
83: 				buffer_size = 0;
84: 				buffer_capacity = 0;
85: 				read_position = 0;
86: 			}
87: 			// we have data left to read from the file
88: 			// read directly into the buffer
89: 			auto bytes_read = file_handle->Read((char *)buffer + result_offset, nr_bytes - result_offset);
90: 			read_position += bytes_read;
91: 			if (reset_enabled) {
92: 				// if reset caching is enabled, we need to cache the bytes that we have read
93: 				if (buffer_size + bytes_read >= buffer_capacity) {
94: 					// no space; first enlarge the buffer
95: 					buffer_capacity = MaxValue<idx_t>(NextPowerOfTwo(buffer_size + bytes_read), buffer_capacity * 2);
96: 
97: 					auto new_buffer = unique_ptr<data_t[]>(new data_t[buffer_capacity]);
98: 					if (buffer_size > 0) {
99: 						memcpy(new_buffer.get(), cached_buffer.get(), buffer_size);
100: 					}
101: 					cached_buffer = move(new_buffer);
102: 				}
103: 				memcpy(cached_buffer.get() + buffer_size, (char *)buffer + result_offset, bytes_read);
104: 				buffer_size += bytes_read;
105: 			}
106: 
107: 			return result_offset + bytes_read;
108: 		} else {
109: 			return file_handle->Read(buffer, nr_bytes);
110: 		}
111: 	}
112: 
113: 	string ReadLine() {
114: 		string result;
115: 		char buffer[1];
116: 		while (true) {
117: 			idx_t tuples_read = Read(buffer, 1);
118: 			if (tuples_read == 0 || buffer[0] == '\n') {
119: 				return result;
120: 			}
121: 			if (buffer[0] != '\r') {
122: 				result += buffer[0];
123: 			}
124: 		}
125: 	}
126: 
127: 	void DisableReset() {
128: 		this->reset_enabled = false;
129: 	}
130: 
131: private:
132: 	unique_ptr<FileHandle> file_handle;
133: 	bool reset_enabled = true;
134: 	bool can_seek = false;
135: 	bool plain_file_source = false;
136: 	idx_t file_size = 0;
137: 	// reset support
138: 	unique_ptr<data_t[]> cached_buffer;
139: 	idx_t read_position = 0;
140: 	idx_t buffer_size = 0;
141: 	idx_t buffer_capacity = 0;
142: };
143: 
144: void BufferedCSVReaderOptions::SetDelimiter(const string &input) {
145: 	this->delimiter = StringUtil::Replace(input, "\\t", "\t");
146: 	this->has_delimiter = true;
147: 	if (input.empty()) {
148: 		throw BinderException("DELIM or SEP must not be empty");
149: 	}
150: }
151: 
152: std::string BufferedCSVReaderOptions::ToString() const {
153: 	return "DELIMITER='" + delimiter + (has_delimiter ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
154: 	       ", QUOTE='" + quote + (has_quote ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
155: 	       ", ESCAPE='" + escape + (has_escape ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
156: 	       ", HEADER=" + std::to_string(header) +
157: 	       (has_header ? "" : (auto_detect ? " (auto detected)" : "' (default)")) +
158: 	       ", SAMPLE_SIZE=" + std::to_string(sample_chunk_size * sample_chunks) +
159: 	       ", ALL_VARCHAR=" + std::to_string(all_varchar);
160: }
161: 
162: static string GetLineNumberStr(idx_t linenr, bool linenr_estimated) {
163: 	string estimated = (linenr_estimated ? string(" (estimated)") : string(""));
164: 	return to_string(linenr + 1) + estimated;
165: }
166: 
167: static bool StartsWithNumericDate(string &separator, const string &value) {
168: 	auto begin = value.c_str();
169: 	auto end = begin + value.size();
170: 
171: 	//	StrpTimeFormat::Parse will skip whitespace, so we can too
172: 	auto field1 = std::find_if_not(begin, end, StringUtil::CharacterIsSpace);
173: 	if (field1 == end) {
174: 		return false;
175: 	}
176: 
177: 	//	first numeric field must start immediately
178: 	if (!StringUtil::CharacterIsDigit(*field1)) {
179: 		return false;
180: 	}
181: 	auto literal1 = std::find_if_not(field1, end, StringUtil::CharacterIsDigit);
182: 	if (literal1 == end) {
183: 		return false;
184: 	}
185: 
186: 	//	second numeric field must exist
187: 	auto field2 = std::find_if(literal1, end, StringUtil::CharacterIsDigit);
188: 	if (field2 == end) {
189: 		return false;
190: 	}
191: 	auto literal2 = std::find_if_not(field2, end, StringUtil::CharacterIsDigit);
192: 	if (literal2 == end) {
193: 		return false;
194: 	}
195: 
196: 	//	third numeric field must exist
197: 	auto field3 = std::find_if(literal2, end, StringUtil::CharacterIsDigit);
198: 	if (field3 == end) {
199: 		return false;
200: 	}
201: 
202: 	//	second literal must match first
203: 	if (((field3 - literal2) != (field2 - literal1)) || strncmp(literal1, literal2, (field2 - literal1)) != 0) {
204: 		return false;
205: 	}
206: 
207: 	//	copy the literal as the separator, escaping percent signs
208: 	separator.clear();
209: 	while (literal1 < field2) {
210: 		const auto literal_char = *literal1++;
211: 		if (literal_char == '%') {
212: 			separator.push_back(literal_char);
213: 		}
214: 		separator.push_back(literal_char);
215: 	}
216: 
217: 	return true;
218: }
219: 
220: string GenerateDateFormat(const string &separator, const char *format_template) {
221: 	string format_specifier = format_template;
222: 
223: 	//	replace all dashes with the separator
224: 	for (auto pos = std::find(format_specifier.begin(), format_specifier.end(), '-'); pos != format_specifier.end();
225: 	     pos = std::find(pos + separator.size(), format_specifier.end(), '-')) {
226: 		format_specifier.replace(pos, pos + 1, separator);
227: 	}
228: 
229: 	return format_specifier;
230: }
231: 
232: TextSearchShiftArray::TextSearchShiftArray() {
233: }
234: 
235: TextSearchShiftArray::TextSearchShiftArray(string search_term) : length(search_term.size()) {
236: 	if (length > 255) {
237: 		throw Exception("Size of delimiter/quote/escape in CSV reader is limited to 255 bytes");
238: 	}
239: 	// initialize the shifts array
240: 	shifts = unique_ptr<uint8_t[]>(new uint8_t[length * 255]);
241: 	memset(shifts.get(), 0, length * 255 * sizeof(uint8_t));
242: 	// iterate over each of the characters in the array
243: 	for (idx_t main_idx = 0; main_idx < length; main_idx++) {
244: 		uint8_t current_char = (uint8_t)search_term[main_idx];
245: 		// now move over all the remaining positions
246: 		for (idx_t i = main_idx; i < length; i++) {
247: 			bool is_match = true;
248: 			// check if the prefix matches at this position
249: 			// if it does, we move to this position after encountering the current character
250: 			for (idx_t j = 0; j < main_idx; j++) {
251: 				if (search_term[i - main_idx + j] != search_term[j]) {
252: 					is_match = false;
253: 				}
254: 			}
255: 			if (!is_match) {
256: 				continue;
257: 			}
258: 			shifts[i * 255 + current_char] = main_idx + 1;
259: 		}
260: 	}
261: }
262: 
263: BufferedCSVReader::BufferedCSVReader(FileSystem &fs_p, FileOpener *opener_p, BufferedCSVReaderOptions options_p,
264:                                      const vector<LogicalType> &requested_types)
265:     : fs(fs_p), opener(opener_p), options(move(options_p)), buffer_size(0), position(0), start(0) {
266: 	file_handle = OpenCSV(options);
267: 	Initialize(requested_types);
268: }
269: 
270: BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options_p,
271:                                      const vector<LogicalType> &requested_types)
272:     : BufferedCSVReader(FileSystem::GetFileSystem(context), FileSystem::GetFileOpener(context), move(options_p),
273:                         requested_types) {
274: }
275: 
276: BufferedCSVReader::~BufferedCSVReader() {
277: }
278: 
279: idx_t BufferedCSVReader::GetFileSize() {
280: 	return file_handle ? file_handle->FileSize() : 0;
281: }
282: 
283: void BufferedCSVReader::Initialize(const vector<LogicalType> &requested_types) {
284: 	PrepareComplexParser();
285: 	if (options.auto_detect) {
286: 		sql_types = SniffCSV(requested_types);
287: 		if (sql_types.empty()) {
288: 			throw Exception("Failed to detect column types from CSV: is the file a valid CSV file?");
289: 		}
290: 		if (cached_chunks.empty()) {
291: 			JumpToBeginning(options.skip_rows, options.header);
292: 		}
293: 	} else {
294: 		sql_types = requested_types;
295: 		ResetBuffer();
296: 		SkipRowsAndReadHeader(options.skip_rows, options.header);
297: 	}
298: 	InitParseChunk(sql_types.size());
299: 	// we only need reset support during the automatic CSV type detection
300: 	// since reset support might require caching (in the case of streams), we disable it for the remainder
301: 	file_handle->DisableReset();
302: }
303: 
304: void BufferedCSVReader::PrepareComplexParser() {
305: 	delimiter_search = TextSearchShiftArray(options.delimiter);
306: 	escape_search = TextSearchShiftArray(options.escape);
307: 	quote_search = TextSearchShiftArray(options.quote);
308: }
309: 
310: unique_ptr<CSVFileHandle> BufferedCSVReader::OpenCSV(const BufferedCSVReaderOptions &options) {
311: 	auto file_handle = fs.OpenFile(options.file_path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK,
312: 	                               options.compression, this->opener);
313: 	return make_unique<CSVFileHandle>(move(file_handle));
314: }
315: 
316: // Helper function to generate column names
317: static string GenerateColumnName(const idx_t total_cols, const idx_t col_number, const string &prefix = "column") {
318: 	int max_digits = NumericHelper::UnsignedLength(total_cols - 1);
319: 	int digits = NumericHelper::UnsignedLength(col_number);
320: 	string leading_zeros = string(max_digits - digits, '0');
321: 	string value = to_string(col_number);
322: 	return string(prefix + leading_zeros + value);
323: }
324: 
325: // Helper function for UTF-8 aware space trimming
326: static string TrimWhitespace(const string &col_name) {
327: 	utf8proc_int32_t codepoint;
328: 	auto str = reinterpret_cast<const utf8proc_uint8_t *>(col_name.c_str());
329: 	idx_t size = col_name.size();
330: 	// Find the first character that is not left trimmed
331: 	idx_t begin = 0;
332: 	while (begin < size) {
333: 		auto bytes = utf8proc_iterate(str + begin, size - begin, &codepoint);
334: 		D_ASSERT(bytes > 0);
335: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
336: 			break;
337: 		}
338: 		begin += bytes;
339: 	}
340: 
341: 	// Find the last character that is not right trimmed
342: 	idx_t end;
343: 	end = begin;
344: 	for (auto next = begin; next < col_name.size();) {
345: 		auto bytes = utf8proc_iterate(str + next, size - next, &codepoint);
346: 		D_ASSERT(bytes > 0);
347: 		next += bytes;
348: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
349: 			end = next;
350: 		}
351: 	}
352: 
353: 	// return the trimmed string
354: 	return col_name.substr(begin, end - begin);
355: }
356: 
357: static string NormalizeColumnName(const string &col_name) {
358: 	// normalize UTF8 characters to NFKD
359: 	auto nfkd = utf8proc_NFKD((const utf8proc_uint8_t *)col_name.c_str(), col_name.size());
360: 	const string col_name_nfkd = string((const char *)nfkd, strlen((const char *)nfkd));
361: 	free(nfkd);
362: 
363: 	// only keep ASCII characters 0-9 a-z A-Z and replace spaces with regular whitespace
364: 	string col_name_ascii = "";
365: 	for (idx_t i = 0; i < col_name_nfkd.size(); i++) {
366: 		if (col_name_nfkd[i] == '_' || (col_name_nfkd[i] >= '0' && col_name_nfkd[i] <= '9') ||
367: 		    (col_name_nfkd[i] >= 'A' && col_name_nfkd[i] <= 'Z') ||
368: 		    (col_name_nfkd[i] >= 'a' && col_name_nfkd[i] <= 'z')) {
369: 			col_name_ascii += col_name_nfkd[i];
370: 		} else if (StringUtil::CharacterIsSpace(col_name_nfkd[i])) {
371: 			col_name_ascii += " ";
372: 		}
373: 	}
374: 
375: 	// trim whitespace and replace remaining whitespace by _
376: 	string col_name_trimmed = TrimWhitespace(col_name_ascii);
377: 	string col_name_cleaned = "";
378: 	bool in_whitespace = false;
379: 	for (idx_t i = 0; i < col_name_trimmed.size(); i++) {
380: 		if (col_name_trimmed[i] == ' ') {
381: 			if (!in_whitespace) {
382: 				col_name_cleaned += "_";
383: 				in_whitespace = true;
384: 			}
385: 		} else {
386: 			col_name_cleaned += col_name_trimmed[i];
387: 			in_whitespace = false;
388: 		}
389: 	}
390: 
391: 	// don't leave string empty; if not empty, make lowercase
392: 	if (col_name_cleaned.empty()) {
393: 		col_name_cleaned = "_";
394: 	} else {
395: 		col_name_cleaned = StringUtil::Lower(col_name_cleaned);
396: 	}
397: 
398: 	// prepend _ if name starts with a digit or is a reserved keyword
399: 	if (KeywordHelper::IsKeyword(col_name_cleaned) || (col_name_cleaned[0] >= '0' && col_name_cleaned[0] <= '9')) {
400: 		col_name_cleaned = "_" + col_name_cleaned;
401: 	}
402: 	return col_name_cleaned;
403: }
404: 
405: void BufferedCSVReader::ResetBuffer() {
406: 	buffer.reset();
407: 	buffer_size = 0;
408: 	position = 0;
409: 	start = 0;
410: 	cached_buffers.clear();
411: }
412: 
413: void BufferedCSVReader::ResetStream() {
414: 	if (!file_handle->CanSeek()) {
415: 		// seeking to the beginning appears to not be supported in all compiler/os-scenarios,
416: 		// so we have to create a new stream source here for now
417: 		file_handle->Reset();
418: 	} else {
419: 		file_handle->Seek(0);
420: 	}
421: 	linenr = 0;
422: 	linenr_estimated = false;
423: 	bytes_per_line_avg = 0;
424: 	sample_chunk_idx = 0;
425: 	jumping_samples = false;
426: }
427: 
428: void BufferedCSVReader::InitParseChunk(idx_t num_cols) {
429: 	// adapt not null info
430: 	if (options.force_not_null.size() != num_cols) {
431: 		options.force_not_null.resize(num_cols, false);
432: 	}
433: 	if (num_cols == parse_chunk.ColumnCount()) {
434: 		parse_chunk.Reset();
435: 	} else {
436: 		parse_chunk.Destroy();
437: 
438: 		// initialize the parse_chunk with a set of VARCHAR types
439: 		vector<LogicalType> varchar_types(num_cols, LogicalType::VARCHAR);
440: 		parse_chunk.Initialize(varchar_types);
441: 	}
442: }
443: 
444: void BufferedCSVReader::JumpToBeginning(idx_t skip_rows = 0, bool skip_header = false) {
445: 	ResetBuffer();
446: 	ResetStream();
447: 	SkipRowsAndReadHeader(skip_rows, skip_header);
448: 	sample_chunk_idx = 0;
449: 	bytes_in_chunk = 0;
450: 	end_of_file_reached = false;
451: 	bom_checked = false;
452: }
453: 
454: void BufferedCSVReader::SkipRowsAndReadHeader(idx_t skip_rows, bool skip_header) {
455: 	for (idx_t i = 0; i < skip_rows; i++) {
456: 		// ignore skip rows
457: 		string read_line = file_handle->ReadLine();
458: 		linenr++;
459: 	}
460: 
461: 	if (skip_header) {
462: 		// ignore the first line as a header line
463: 		InitParseChunk(sql_types.size());
464: 		ParseCSV(ParserMode::PARSING_HEADER);
465: 	}
466: }
467: 
468: bool BufferedCSVReader::JumpToNextSample() {
469: 	// get bytes contained in the previously read chunk
470: 	idx_t remaining_bytes_in_buffer = buffer_size - start;
471: 	bytes_in_chunk -= remaining_bytes_in_buffer;
472: 	if (remaining_bytes_in_buffer == 0) {
473: 		return false;
474: 	}
475: 
476: 	// assess if it makes sense to jump, based on size of the first chunk relative to size of the entire file
477: 	if (sample_chunk_idx == 0) {
478: 		idx_t bytes_first_chunk = bytes_in_chunk;
479: 		double chunks_fit = (file_handle->FileSize() / (double)bytes_first_chunk);
480: 		jumping_samples = chunks_fit >= options.sample_chunks;
481: 
482: 		// jump back to the beginning
483: 		JumpToBeginning(options.skip_rows, options.header);
484: 		sample_chunk_idx++;
485: 		return true;
486: 	}
487: 
488: 	if (end_of_file_reached || sample_chunk_idx >= options.sample_chunks) {
489: 		return false;
490: 	}
491: 
492: 	// if we deal with any other sources than plaintext files, jumping_samples can be tricky. In that case
493: 	// we just read x continuous chunks from the stream TODO: make jumps possible for zipfiles.
494: 	if (!file_handle->PlainFileSource() || !jumping_samples) {
495: 		sample_chunk_idx++;
496: 		return true;
497: 	}
498: 
499: 	// update average bytes per line
500: 	double bytes_per_line = bytes_in_chunk / (double)options.sample_chunk_size;
501: 	bytes_per_line_avg = ((bytes_per_line_avg * (sample_chunk_idx)) + bytes_per_line) / (sample_chunk_idx + 1);
502: 
503: 	// if none of the previous conditions were met, we can jump
504: 	idx_t partition_size = (idx_t)round(file_handle->FileSize() / (double)options.sample_chunks);
505: 
506: 	// calculate offset to end of the current partition
507: 	int64_t offset = partition_size - bytes_in_chunk - remaining_bytes_in_buffer;
508: 	auto current_pos = file_handle->SeekPosition();
509: 
510: 	if (current_pos + offset < file_handle->FileSize()) {
511: 		// set position in stream and clear failure bits
512: 		file_handle->Seek(current_pos + offset);
513: 
514: 		// estimate linenr
515: 		linenr += (idx_t)round((offset + remaining_bytes_in_buffer) / bytes_per_line_avg);
516: 		linenr_estimated = true;
517: 	} else {
518: 		// seek backwards from the end in last chunk and hope to catch the end of the file
519: 		// TODO: actually it would be good to make sure that the end of file is being reached, because
520: 		// messy end-lines are quite common. For this case, however, we first need a skip_end detection anyways.
521: 		file_handle->Seek(file_handle->FileSize() - bytes_in_chunk);
522: 
523: 		// estimate linenr
524: 		linenr = (idx_t)round((file_handle->FileSize() - bytes_in_chunk) / bytes_per_line_avg);
525: 		linenr_estimated = true;
526: 	}
527: 
528: 	// reset buffers and parse chunk
529: 	ResetBuffer();
530: 
531: 	// seek beginning of next line
532: 	// FIXME: if this jump ends up in a quoted linebreak, we will have a problem
533: 	string read_line = file_handle->ReadLine();
534: 	linenr++;
535: 
536: 	sample_chunk_idx++;
537: 
538: 	return true;
539: }
540: 
541: void BufferedCSVReader::SetDateFormat(const string &format_specifier, const LogicalTypeId &sql_type) {
542: 	options.has_format[sql_type] = true;
543: 	auto &date_format = options.date_format[sql_type];
544: 	date_format.format_specifier = format_specifier;
545: 	StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
546: }
547: 
548: bool BufferedCSVReader::TryCastValue(const Value &value, const LogicalType &sql_type) {
549: 	if (options.has_format[LogicalTypeId::DATE] && sql_type.id() == LogicalTypeId::DATE) {
550: 		date_t result;
551: 		string error_message;
552: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(string_t(StringValue::Get(value)), result,
553: 		                                                             error_message);
554: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type.id() == LogicalTypeId::TIMESTAMP) {
555: 		timestamp_t result;
556: 		string error_message;
557: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(string_t(StringValue::Get(value)),
558: 		                                                                       result, error_message);
559: 	} else {
560: 		Value new_value;
561: 		string error_message;
562: 		return value.TryCastAs(sql_type, new_value, &error_message, true);
563: 	}
564: }
565: 
566: struct TryCastDateOperator {
567: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, date_t &result, string &error_message) {
568: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(input, result, error_message);
569: 	}
570: };
571: 
572: struct TryCastTimestampOperator {
573: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, timestamp_t &result,
574: 	                      string &error_message) {
575: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(input, result, error_message);
576: 	}
577: };
578: 
579: template <class OP, class T>
580: static bool TemplatedTryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector,
581:                                        idx_t count, string &error_message) {
582: 	D_ASSERT(input_vector.GetType().id() == LogicalTypeId::VARCHAR);
583: 	bool all_converted = true;
584: 	UnaryExecutor::Execute<string_t, T>(input_vector, result_vector, count, [&](string_t input) {
585: 		T result;
586: 		if (!OP::Operation(options, input, result, error_message)) {
587: 			all_converted = false;
588: 		}
589: 		return result;
590: 	});
591: 	return all_converted;
592: }
593: 
594: bool TryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
595:                        string &error_message) {
596: 	return TemplatedTryCastDateVector<TryCastDateOperator, date_t>(options, input_vector, result_vector, count,
597: 	                                                               error_message);
598: }
599: 
600: bool TryCastTimestampVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
601:                             string &error_message) {
602: 	return TemplatedTryCastDateVector<TryCastTimestampOperator, timestamp_t>(options, input_vector, result_vector,
603: 	                                                                         count, error_message);
604: }
605: 
606: bool BufferedCSVReader::TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type) {
607: 	// try vector-cast from string to sql_type
608: 	Vector dummy_result(sql_type);
609: 	if (options.has_format[LogicalTypeId::DATE] && sql_type == LogicalTypeId::DATE) {
610: 		// use the date format to cast the chunk
611: 		string error_message;
612: 		return TryCastDateVector(options, parse_chunk_col, dummy_result, size, error_message);
613: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type == LogicalTypeId::TIMESTAMP) {
614: 		// use the timestamp format to cast the chunk
615: 		string error_message;
616: 		return TryCastTimestampVector(options, parse_chunk_col, dummy_result, size, error_message);
617: 	} else {
618: 		// target type is not varchar: perform a cast
619: 		string error_message;
620: 		return VectorOperations::TryCast(parse_chunk_col, dummy_result, size, &error_message, true);
621: 	}
622: }
623: 
624: enum class QuoteRule : uint8_t { QUOTES_RFC = 0, QUOTES_OTHER = 1, NO_QUOTES = 2 };
625: 
626: void BufferedCSVReader::DetectDialect(const vector<LogicalType> &requested_types,
627:                                       BufferedCSVReaderOptions &original_options,
628:                                       vector<BufferedCSVReaderOptions> &info_candidates, idx_t &best_num_cols) {
629: 	// set up the candidates we consider for delimiter and quote rules based on user input
630: 	vector<string> delim_candidates;
631: 	vector<QuoteRule> quoterule_candidates;
632: 	vector<vector<string>> quote_candidates_map;
633: 	vector<vector<string>> escape_candidates_map = {{""}, {"\\"}, {""}};
634: 
635: 	if (options.has_delimiter) {
636: 		// user provided a delimiter: use that delimiter
637: 		delim_candidates = {options.delimiter};
638: 	} else {
639: 		// no delimiter provided: try standard/common delimiters
640: 		delim_candidates = {",", "|", ";", "\t"};
641: 	}
642: 	if (options.has_quote) {
643: 		// user provided quote: use that quote rule
644: 		quote_candidates_map = {{options.quote}, {options.quote}, {options.quote}};
645: 	} else {
646: 		// no quote rule provided: use standard/common quotes
647: 		quote_candidates_map = {{"\""}, {"\"", "'"}, {""}};
648: 	}
649: 	if (options.has_escape) {
650: 		// user provided escape: use that escape rule
651: 		if (options.escape.empty()) {
652: 			quoterule_candidates = {QuoteRule::QUOTES_RFC};
653: 		} else {
654: 			quoterule_candidates = {QuoteRule::QUOTES_OTHER};
655: 		}
656: 		escape_candidates_map[static_cast<uint8_t>(quoterule_candidates[0])] = {options.escape};
657: 	} else {
658: 		// no escape provided: try standard/common escapes
659: 		quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER, QuoteRule::NO_QUOTES};
660: 	}
661: 
662: 	idx_t best_consistent_rows = 0;
663: 	for (auto quoterule : quoterule_candidates) {
664: 		const auto &quote_candidates = quote_candidates_map[static_cast<uint8_t>(quoterule)];
665: 		for (const auto &quote : quote_candidates) {
666: 			for (const auto &delim : delim_candidates) {
667: 				const auto &escape_candidates = escape_candidates_map[static_cast<uint8_t>(quoterule)];
668: 				for (const auto &escape : escape_candidates) {
669: 					BufferedCSVReaderOptions sniff_info = original_options;
670: 					sniff_info.delimiter = delim;
671: 					sniff_info.quote = quote;
672: 					sniff_info.escape = escape;
673: 
674: 					options = sniff_info;
675: 					PrepareComplexParser();
676: 
677: 					JumpToBeginning(original_options.skip_rows);
678: 					sniffed_column_counts.clear();
679: 
680: 					if (!TryParseCSV(ParserMode::SNIFFING_DIALECT)) {
681: 						continue;
682: 					}
683: 
684: 					idx_t start_row = original_options.skip_rows;
685: 					idx_t consistent_rows = 0;
686: 					idx_t num_cols = 0;
687: 
688: 					for (idx_t row = 0; row < sniffed_column_counts.size(); row++) {
689: 						if (sniffed_column_counts[row] == num_cols) {
690: 							consistent_rows++;
691: 						} else {
692: 							num_cols = sniffed_column_counts[row];
693: 							start_row = row + original_options.skip_rows;
694: 							consistent_rows = 1;
695: 						}
696: 					}
697: 
698: 					// some logic
699: 					bool more_values = (consistent_rows > best_consistent_rows && num_cols >= best_num_cols);
700: 					bool single_column_before = best_num_cols < 2 && num_cols > best_num_cols;
701: 					bool rows_consistent =
702: 					    start_row + consistent_rows - original_options.skip_rows == sniffed_column_counts.size();
703: 					bool more_than_one_row = (consistent_rows > 1);
704: 					bool more_than_one_column = (num_cols > 1);
705: 					bool start_good = !info_candidates.empty() && (start_row <= info_candidates.front().skip_rows);
706: 
707: 					if (!requested_types.empty() && requested_types.size() != num_cols) {
708: 						continue;
709: 					} else if ((more_values || single_column_before) && rows_consistent) {
710: 						sniff_info.skip_rows = start_row;
711: 						sniff_info.num_cols = num_cols;
712: 						best_consistent_rows = consistent_rows;
713: 						best_num_cols = num_cols;
714: 
715: 						info_candidates.clear();
716: 						info_candidates.push_back(sniff_info);
717: 					} else if (more_than_one_row && more_than_one_column && start_good && rows_consistent) {
718: 						bool same_quote_is_candidate = false;
719: 						for (auto &info_candidate : info_candidates) {
720: 							if (quote.compare(info_candidate.quote) == 0) {
721: 								same_quote_is_candidate = true;
722: 							}
723: 						}
724: 						if (!same_quote_is_candidate) {
725: 							sniff_info.skip_rows = start_row;
726: 							sniff_info.num_cols = num_cols;
727: 							info_candidates.push_back(sniff_info);
728: 						}
729: 					}
730: 				}
731: 			}
732: 		}
733: 	}
734: }
735: 
736: void BufferedCSVReader::DetectCandidateTypes(const vector<LogicalType> &type_candidates,
737:                                              const map<LogicalTypeId, vector<const char *>> &format_template_candidates,
738:                                              const vector<BufferedCSVReaderOptions> &info_candidates,
739:                                              BufferedCSVReaderOptions &original_options, idx_t best_num_cols,
740:                                              vector<vector<LogicalType>> &best_sql_types_candidates,
741:                                              std::map<LogicalTypeId, vector<string>> &best_format_candidates,
742:                                              DataChunk &best_header_row) {
743: 	BufferedCSVReaderOptions best_options;
744: 	idx_t min_varchar_cols = best_num_cols + 1;
745: 
746: 	// check which info candidate leads to minimum amount of non-varchar columns...
747: 	for (const auto &t : format_template_candidates) {
748: 		best_format_candidates[t.first].clear();
749: 	}
750: 	for (auto &info_candidate : info_candidates) {
751: 		options = info_candidate;
752: 		vector<vector<LogicalType>> info_sql_types_candidates(options.num_cols, type_candidates);
753: 		std::map<LogicalTypeId, bool> has_format_candidates;
754: 		std::map<LogicalTypeId, vector<string>> format_candidates;
755: 		for (const auto &t : format_template_candidates) {
756: 			has_format_candidates[t.first] = false;
757: 			format_candidates[t.first].clear();
758: 		}
759: 
760: 		// set all sql_types to VARCHAR so we can do datatype detection based on VARCHAR values
761: 		sql_types.clear();
762: 		sql_types.assign(options.num_cols, LogicalType::VARCHAR);
763: 
764: 		// jump to beginning and skip potential header
765: 		JumpToBeginning(options.skip_rows, true);
766: 		DataChunk header_row;
767: 		header_row.Initialize(sql_types);
768: 		parse_chunk.Copy(header_row);
769: 
770: 		if (header_row.size() == 0) {
771: 			continue;
772: 		}
773: 
774: 		// init parse chunk and read csv with info candidate
775: 		InitParseChunk(sql_types.size());
776: 		ParseCSV(ParserMode::SNIFFING_DATATYPES);
777: 		for (idx_t row_idx = 0; row_idx <= parse_chunk.size(); row_idx++) {
778: 			bool is_header_row = row_idx == 0;
779: 			idx_t row = row_idx - 1;
780: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
781: 				auto &col_type_candidates = info_sql_types_candidates[col];
782: 				while (col_type_candidates.size() > 1) {
783: 					const auto &sql_type = col_type_candidates.back();
784: 					// try cast from string to sql_type
785: 					Value dummy_val;
786: 					if (is_header_row) {
787: 						dummy_val = header_row.GetValue(col, 0);
788: 					} else {
789: 						dummy_val = parse_chunk.GetValue(col, row);
790: 					}
791: 					// try formatting for date types if the user did not specify one and it starts with numeric values.
792: 					string separator;
793: 					if (has_format_candidates.count(sql_type.id()) && !original_options.has_format[sql_type.id()] &&
794: 					    StartsWithNumericDate(separator, StringValue::Get(dummy_val))) {
795: 						// generate date format candidates the first time through
796: 						auto &type_format_candidates = format_candidates[sql_type.id()];
797: 						const auto had_format_candidates = has_format_candidates[sql_type.id()];
798: 						if (!has_format_candidates[sql_type.id()]) {
799: 							has_format_candidates[sql_type.id()] = true;
800: 							// order by preference
801: 							auto entry = format_template_candidates.find(sql_type.id());
802: 							if (entry != format_template_candidates.end()) {
803: 								const auto &format_template_list = entry->second;
804: 								for (const auto &t : format_template_list) {
805: 									const auto format_string = GenerateDateFormat(separator, t);
806: 									// don't parse ISO 8601
807: 									if (format_string.find("%Y-%m-%d") == string::npos) {
808: 										type_format_candidates.emplace_back(format_string);
809: 									}
810: 								}
811: 							}
812: 							//	initialise the first candidate
813: 							options.has_format[sql_type.id()] = true;
814: 							//	all formats are constructed to be valid
815: 							SetDateFormat(type_format_candidates.back(), sql_type.id());
816: 						}
817: 						// check all formats and keep the first one that works
818: 						StrpTimeFormat::ParseResult result;
819: 						auto save_format_candidates = type_format_candidates;
820: 						while (!type_format_candidates.empty()) {
821: 							//	avoid using exceptions for flow control...
822: 							auto &current_format = options.date_format[sql_type.id()];
823: 							if (current_format.Parse(StringValue::Get(dummy_val), result)) {
824: 								break;
825: 							}
826: 							//	doesn't work - move to the next one
827: 							type_format_candidates.pop_back();
828: 							options.has_format[sql_type.id()] = (!type_format_candidates.empty());
829: 							if (!type_format_candidates.empty()) {
830: 								SetDateFormat(type_format_candidates.back(), sql_type.id());
831: 							}
832: 						}
833: 						//	if none match, then this is not a value of type sql_type,
834: 						if (type_format_candidates.empty()) {
835: 							//	so restore the candidates that did work.
836: 							//	or throw them out if they were generated by this value.
837: 							if (had_format_candidates) {
838: 								type_format_candidates.swap(save_format_candidates);
839: 								if (!type_format_candidates.empty()) {
840: 									SetDateFormat(type_format_candidates.back(), sql_type.id());
841: 								}
842: 							} else {
843: 								has_format_candidates[sql_type.id()] = false;
844: 							}
845: 						}
846: 					}
847: 					// try cast from string to sql_type
848: 					if (TryCastValue(dummy_val, sql_type)) {
849: 						break;
850: 					} else {
851: 						col_type_candidates.pop_back();
852: 					}
853: 				}
854: 			}
855: 			// reset type detection, because first row could be header,
856: 			// but only do it if csv has more than one line (including header)
857: 			if (parse_chunk.size() > 0 && is_header_row) {
858: 				info_sql_types_candidates = vector<vector<LogicalType>>(options.num_cols, type_candidates);
859: 				for (auto &f : format_candidates) {
860: 					f.second.clear();
861: 				}
862: 				for (auto &h : has_format_candidates) {
863: 					h.second = false;
864: 				}
865: 			}
866: 		}
867: 
868: 		idx_t varchar_cols = 0;
869: 		for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
870: 			auto &col_type_candidates = info_sql_types_candidates[col];
871: 			// check number of varchar columns
872: 			const auto &col_type = col_type_candidates.back();
873: 			if (col_type == LogicalType::VARCHAR) {
874: 				varchar_cols++;
875: 			}
876: 		}
877: 
878: 		// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 30% of best_num_cols.
879: 		if (varchar_cols < min_varchar_cols && parse_chunk.ColumnCount() > (best_num_cols * 0.7)) {
880: 			// we have a new best_options candidate
881: 			best_options = info_candidate;
882: 			min_varchar_cols = varchar_cols;
883: 			best_sql_types_candidates = info_sql_types_candidates;
884: 			best_format_candidates = format_candidates;
885: 			best_header_row.Destroy();
886: 			auto header_row_types = header_row.GetTypes();
887: 			best_header_row.Initialize(header_row_types);
888: 			header_row.Copy(best_header_row);
889: 		}
890: 	}
891: 
892: 	options = best_options;
893: 	for (const auto &best : best_format_candidates) {
894: 		if (!best.second.empty()) {
895: 			SetDateFormat(best.second.back(), best.first);
896: 		}
897: 	}
898: }
899: 
900: void BufferedCSVReader::DetectHeader(const vector<vector<LogicalType>> &best_sql_types_candidates,
901:                                      const DataChunk &best_header_row) {
902: 	// information for header detection
903: 	bool first_row_consistent = true;
904: 	bool first_row_nulls = false;
905: 
906: 	// check if header row is all null and/or consistent with detected column data types
907: 	first_row_nulls = true;
908: 	for (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {
909: 		auto dummy_val = best_header_row.GetValue(col, 0);
910: 		if (!dummy_val.IsNull()) {
911: 			first_row_nulls = false;
912: 		}
913: 
914: 		// try cast to sql_type of column
915: 		const auto &sql_type = best_sql_types_candidates[col].back();
916: 		if (!TryCastValue(dummy_val, sql_type)) {
917: 			first_row_consistent = false;
918: 		}
919: 	}
920: 
921: 	// update parser info, and read, generate & set col_names based on previous findings
922: 	if (((!first_row_consistent || first_row_nulls) && !options.has_header) || (options.has_header && options.header)) {
923: 		options.header = true;
924: 		case_insensitive_map_t<idx_t> name_collision_count;
925: 		// get header names from CSV
926: 		for (idx_t col = 0; col < options.num_cols; col++) {
927: 			const auto &val = best_header_row.GetValue(col, 0);
928: 			string col_name = val.ToString();
929: 
930: 			// generate name if field is empty
931: 			if (col_name.empty() || val.IsNull()) {
932: 				col_name = GenerateColumnName(options.num_cols, col);
933: 			}
934: 
935: 			// normalize names or at least trim whitespace
936: 			if (options.normalize_names) {
937: 				col_name = NormalizeColumnName(col_name);
938: 			} else {
939: 				col_name = TrimWhitespace(col_name);
940: 			}
941: 
942: 			// avoid duplicate header names
943: 			const string col_name_raw = col_name;
944: 			while (name_collision_count.find(col_name) != name_collision_count.end()) {
945: 				name_collision_count[col_name] += 1;
946: 				col_name = col_name + "_" + to_string(name_collision_count[col_name]);
947: 			}
948: 
949: 			col_names.push_back(col_name);
950: 			name_collision_count[col_name] = 0;
951: 		}
952: 
953: 	} else {
954: 		options.header = false;
955: 		for (idx_t col = 0; col < options.num_cols; col++) {
956: 			string column_name = GenerateColumnName(options.num_cols, col);
957: 			col_names.push_back(column_name);
958: 		}
959: 	}
960: }
961: 
962: vector<LogicalType> BufferedCSVReader::RefineTypeDetection(const vector<LogicalType> &type_candidates,
963:                                                            const vector<LogicalType> &requested_types,
964:                                                            vector<vector<LogicalType>> &best_sql_types_candidates,
965:                                                            map<LogicalTypeId, vector<string>> &best_format_candidates) {
966: 	// for the type refine we set the SQL types to VARCHAR for all columns
967: 	sql_types.clear();
968: 	sql_types.assign(options.num_cols, LogicalType::VARCHAR);
969: 
970: 	vector<LogicalType> detected_types;
971: 
972: 	// if data types were provided, exit here if number of columns does not match
973: 	if (!requested_types.empty()) {
974: 		if (requested_types.size() != options.num_cols) {
975: 			throw InvalidInputException(
976: 			    "Error while determining column types: found %lld columns but expected %d. (%s)", options.num_cols,
977: 			    requested_types.size(), options.ToString());
978: 		} else {
979: 			detected_types = requested_types;
980: 		}
981: 	} else if (options.all_varchar) {
982: 		// return all types varchar
983: 		detected_types = sql_types;
984: 	} else {
985: 		// jump through the rest of the file and continue to refine the sql type guess
986: 		while (JumpToNextSample()) {
987: 			InitParseChunk(sql_types.size());
988: 			// if jump ends up a bad line, we just skip this chunk
989: 			if (!TryParseCSV(ParserMode::SNIFFING_DATATYPES)) {
990: 				continue;
991: 			}
992: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
993: 				vector<LogicalType> &col_type_candidates = best_sql_types_candidates[col];
994: 				while (col_type_candidates.size() > 1) {
995: 					const auto &sql_type = col_type_candidates.back();
996: 					//	narrow down the date formats
997: 					if (best_format_candidates.count(sql_type.id())) {
998: 						auto &best_type_format_candidates = best_format_candidates[sql_type.id()];
999: 						auto save_format_candidates = best_type_format_candidates;
1000: 						while (!best_type_format_candidates.empty()) {
1001: 							if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
1002: 								break;
1003: 							}
1004: 							//	doesn't work - move to the next one
1005: 							best_type_format_candidates.pop_back();
1006: 							options.has_format[sql_type.id()] = (!best_type_format_candidates.empty());
1007: 							if (!best_type_format_candidates.empty()) {
1008: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
1009: 							}
1010: 						}
1011: 						//	if none match, then this is not a column of type sql_type,
1012: 						if (best_type_format_candidates.empty()) {
1013: 							//	so restore the candidates that did work.
1014: 							best_type_format_candidates.swap(save_format_candidates);
1015: 							if (!best_type_format_candidates.empty()) {
1016: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
1017: 							}
1018: 						}
1019: 					}
1020: 
1021: 					if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
1022: 						break;
1023: 					} else {
1024: 						col_type_candidates.pop_back();
1025: 					}
1026: 				}
1027: 			}
1028: 
1029: 			if (!jumping_samples) {
1030: 				if ((sample_chunk_idx)*options.sample_chunk_size <= options.buffer_size) {
1031: 					// cache parse chunk
1032: 					// create a new chunk and fill it with the remainder
1033: 					auto chunk = make_unique<DataChunk>();
1034: 					auto parse_chunk_types = parse_chunk.GetTypes();
1035: 					chunk->Move(parse_chunk);
1036: 					cached_chunks.push(move(chunk));
1037: 				} else {
1038: 					while (!cached_chunks.empty()) {
1039: 						cached_chunks.pop();
1040: 					}
1041: 				}
1042: 			}
1043: 		}
1044: 
1045: 		// set sql types
1046: 		for (auto &best_sql_types_candidate : best_sql_types_candidates) {
1047: 			LogicalType d_type = best_sql_types_candidate.back();
1048: 			if (best_sql_types_candidate.size() == type_candidates.size()) {
1049: 				d_type = LogicalType::VARCHAR;
1050: 			}
1051: 			detected_types.push_back(d_type);
1052: 		}
1053: 	}
1054: 
1055: 	return detected_types;
1056: }
1057: 
1058: vector<LogicalType> BufferedCSVReader::SniffCSV(const vector<LogicalType> &requested_types) {
1059: 	for (auto &type : requested_types) {
1060: 		// auto detect for blobs not supported: there may be invalid UTF-8 in the file
1061: 		if (type.id() == LogicalTypeId::BLOB) {
1062: 			return requested_types;
1063: 		}
1064: 	}
1065: 
1066: 	// #######
1067: 	// ### dialect detection
1068: 	// #######
1069: 	BufferedCSVReaderOptions original_options = options;
1070: 	vector<BufferedCSVReaderOptions> info_candidates;
1071: 	idx_t best_num_cols = 0;
1072: 
1073: 	DetectDialect(requested_types, original_options, info_candidates, best_num_cols);
1074: 
1075: 	// if no dialect candidate was found, then file was most likely empty and we throw an exception
1076: 	if (info_candidates.empty()) {
1077: 		throw InvalidInputException(
1078: 		    "Error in file \"%s\": CSV options could not be auto-detected. Consider setting parser options manually.",
1079: 		    options.file_path);
1080: 	}
1081: 
1082: 	// #######
1083: 	// ### type detection (initial)
1084: 	// #######
1085: 	// type candidates, ordered by descending specificity (~ from high to low)
1086: 	vector<LogicalType> type_candidates = {
1087: 	    LogicalType::VARCHAR, LogicalType::TIMESTAMP,
1088: 	    LogicalType::DATE,    LogicalType::TIME,
1089: 	    LogicalType::DOUBLE,  /* LogicalType::FLOAT,*/ LogicalType::BIGINT,
1090: 	    LogicalType::INTEGER, /*LogicalType::SMALLINT, LogicalType::TINYINT,*/ LogicalType::BOOLEAN,
1091: 	    LogicalType::SQLNULL};
1092: 	// format template candidates, ordered by descending specificity (~ from high to low)
1093: 	std::map<LogicalTypeId, vector<const char *>> format_template_candidates = {
1094: 	    {LogicalTypeId::DATE, {"%m-%d-%Y", "%m-%d-%y", "%d-%m-%Y", "%d-%m-%y", "%Y-%m-%d", "%y-%m-%d"}},
1095: 	    {LogicalTypeId::TIMESTAMP,
1096: 	     {"%Y-%m-%d %H:%M:%S.%f", "%m-%d-%Y %I:%M:%S %p", "%m-%d-%y %I:%M:%S %p", "%d-%m-%Y %H:%M:%S",
1097: 	      "%d-%m-%y %H:%M:%S", "%Y-%m-%d %H:%M:%S", "%y-%m-%d %H:%M:%S"}},
1098: 	};
1099: 	vector<vector<LogicalType>> best_sql_types_candidates;
1100: 	map<LogicalTypeId, vector<string>> best_format_candidates;
1101: 	DataChunk best_header_row;
1102: 	DetectCandidateTypes(type_candidates, format_template_candidates, info_candidates, original_options, best_num_cols,
1103: 	                     best_sql_types_candidates, best_format_candidates, best_header_row);
1104: 
1105: 	// #######
1106: 	// ### header detection
1107: 	// #######
1108: 	options.num_cols = best_num_cols;
1109: 	DetectHeader(best_sql_types_candidates, best_header_row);
1110: 
1111: 	// #######
1112: 	// ### type detection (refining)
1113: 	// #######
1114: 	return RefineTypeDetection(type_candidates, requested_types, best_sql_types_candidates, best_format_candidates);
1115: }
1116: 
1117: bool BufferedCSVReader::TryParseComplexCSV(DataChunk &insert_chunk, string &error_message) {
1118: 	// used for parsing algorithm
1119: 	bool finished_chunk = false;
1120: 	idx_t column = 0;
1121: 	vector<idx_t> escape_positions;
1122: 	uint8_t delimiter_pos = 0, escape_pos = 0, quote_pos = 0;
1123: 	idx_t offset = 0;
1124: 
1125: 	// read values into the buffer (if any)
1126: 	if (position >= buffer_size) {
1127: 		if (!ReadBuffer(start)) {
1128: 			return true;
1129: 		}
1130: 	}
1131: 	// start parsing the first value
1132: 	start = position;
1133: 	goto value_start;
1134: value_start:
1135: 	/* state: value_start */
1136: 	// this state parses the first characters of a value
1137: 	offset = 0;
1138: 	delimiter_pos = 0;
1139: 	quote_pos = 0;
1140: 	do {
1141: 		idx_t count = 0;
1142: 		for (; position < buffer_size; position++) {
1143: 			quote_search.Match(quote_pos, buffer[position]);
1144: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1145: 			count++;
1146: 			if (delimiter_pos == options.delimiter.size()) {
1147: 				// found a delimiter, add the value
1148: 				offset = options.delimiter.size() - 1;
1149: 				goto add_value;
1150: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1151: 				// found a newline, add the row
1152: 				goto add_row;
1153: 			}
1154: 			if (count > quote_pos) {
1155: 				// did not find a quote directly at the start of the value, stop looking for the quote now
1156: 				goto normal;
1157: 			}
1158: 			if (quote_pos == options.quote.size()) {
1159: 				// found a quote, go to quoted loop and skip the initial quote
1160: 				start += options.quote.size();
1161: 				goto in_quotes;
1162: 			}
1163: 		}
1164: 	} while (ReadBuffer(start));
1165: 	// file ends while scanning for quote/delimiter, go to final state
1166: 	goto final_state;
1167: normal:
1168: 	/* state: normal parsing state */
1169: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1170: 	position++;
1171: 	do {
1172: 		for (; position < buffer_size; position++) {
1173: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1174: 			if (delimiter_pos == options.delimiter.size()) {
1175: 				offset = options.delimiter.size() - 1;
1176: 				goto add_value;
1177: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1178: 				goto add_row;
1179: 			}
1180: 		}
1181: 	} while (ReadBuffer(start));
1182: 	goto final_state;
1183: add_value:
1184: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1185: 	// increase position by 1 and move start to the new position
1186: 	offset = 0;
1187: 	start = ++position;
1188: 	if (position >= buffer_size && !ReadBuffer(start)) {
1189: 		// file ends right after delimiter, go to final state
1190: 		goto final_state;
1191: 	}
1192: 	goto value_start;
1193: add_row : {
1194: 	// check type of newline (\r or \n)
1195: 	bool carriage_return = buffer[position] == '\r';
1196: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1197: 	finished_chunk = AddRow(insert_chunk, column);
1198: 	// increase position by 1 and move start to the new position
1199: 	offset = 0;
1200: 	start = ++position;
1201: 	if (position >= buffer_size && !ReadBuffer(start)) {
1202: 		// file ends right after newline, go to final state
1203: 		goto final_state;
1204: 	}
1205: 	if (carriage_return) {
1206: 		// \r newline, go to special state that parses an optional \n afterwards
1207: 		goto carriage_return;
1208: 	} else {
1209: 		// \n newline, move to value start
1210: 		if (finished_chunk) {
1211: 			return true;
1212: 		}
1213: 		goto value_start;
1214: 	}
1215: }
1216: in_quotes:
1217: 	/* state: in_quotes */
1218: 	// this state parses the remainder of a quoted value
1219: 	quote_pos = 0;
1220: 	escape_pos = 0;
1221: 	position++;
1222: 	do {
1223: 		for (; position < buffer_size; position++) {
1224: 			quote_search.Match(quote_pos, buffer[position]);
1225: 			escape_search.Match(escape_pos, buffer[position]);
1226: 			if (quote_pos == options.quote.size()) {
1227: 				goto unquote;
1228: 			} else if (escape_pos == options.escape.size()) {
1229: 				escape_positions.push_back(position - start - (options.escape.size() - 1));
1230: 				goto handle_escape;
1231: 			}
1232: 		}
1233: 	} while (ReadBuffer(start));
1234: 	// still in quoted state at the end of the file, error:
1235: 	error_message = StringUtil::Format("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1236: 	                                   GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1237: 	return false;
1238: unquote:
1239: 	/* state: unquote */
1240: 	// this state handles the state directly after we unquote
1241: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1242: 	// or a delimiter/newline, ending the current value and moving on to the next value
1243: 	delimiter_pos = 0;
1244: 	quote_pos = 0;
1245: 	position++;
1246: 	if (position >= buffer_size && !ReadBuffer(start)) {
1247: 		// file ends right after unquote, go to final state
1248: 		offset = options.quote.size();
1249: 		goto final_state;
1250: 	}
1251: 	if (StringUtil::CharacterIsNewline(buffer[position])) {
1252: 		// quote followed by newline, add row
1253: 		offset = options.quote.size();
1254: 		goto add_row;
1255: 	}
1256: 	do {
1257: 		idx_t count = 0;
1258: 		for (; position < buffer_size; position++) {
1259: 			quote_search.Match(quote_pos, buffer[position]);
1260: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1261: 			count++;
1262: 			if (count > delimiter_pos && count > quote_pos) {
1263: 				error_message = StringUtil::Format(
1264: 				    "Error in file \"%s\" on line %s: quote should be followed by end of value, end "
1265: 				    "of row or another quote. (%s)",
1266: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1267: 				return false;
1268: 			}
1269: 			if (delimiter_pos == options.delimiter.size()) {
1270: 				// quote followed by delimiter, add value
1271: 				offset = options.quote.size() + options.delimiter.size() - 1;
1272: 				goto add_value;
1273: 			} else if (quote_pos == options.quote.size() &&
1274: 			           (options.escape.empty() || options.escape == options.quote)) {
1275: 				// quote followed by quote, go back to quoted state and add to escape
1276: 				escape_positions.push_back(position - start - (options.quote.size() - 1));
1277: 				goto in_quotes;
1278: 			}
1279: 		}
1280: 	} while (ReadBuffer(start));
1281: 	error_message = StringUtil::Format(
1282: 	    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of row or another quote. (%s)",
1283: 	    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1284: 	return false;
1285: handle_escape:
1286: 	escape_pos = 0;
1287: 	quote_pos = 0;
1288: 	position++;
1289: 	do {
1290: 		idx_t count = 0;
1291: 		for (; position < buffer_size; position++) {
1292: 			quote_search.Match(quote_pos, buffer[position]);
1293: 			escape_search.Match(escape_pos, buffer[position]);
1294: 			count++;
1295: 			if (count > escape_pos && count > quote_pos) {
1296: 				error_message = StringUtil::Format(
1297: 				    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1298: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1299: 				return false;
1300: 			}
1301: 			if (quote_pos == options.quote.size() || escape_pos == options.escape.size()) {
1302: 				// found quote or escape: move back to quoted state
1303: 				goto in_quotes;
1304: 			}
1305: 		}
1306: 	} while (ReadBuffer(start));
1307: 	error_message =
1308: 	    StringUtil::Format("Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1309: 	                       options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1310: 	return false;
1311: carriage_return:
1312: 	/* state: carriage_return */
1313: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1314: 	if (buffer[position] == '\n') {
1315: 		// newline after carriage return: skip
1316: 		start = ++position;
1317: 		if (position >= buffer_size && !ReadBuffer(start)) {
1318: 			// file ends right after newline, go to final state
1319: 			goto final_state;
1320: 		}
1321: 	}
1322: 	if (finished_chunk) {
1323: 		return true;
1324: 	}
1325: 	goto value_start;
1326: final_state:
1327: 	if (finished_chunk) {
1328: 		return true;
1329: 	}
1330: 	if (column > 0 || position > start) {
1331: 		// remaining values to be added to the chunk
1332: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1333: 		finished_chunk = AddRow(insert_chunk, column);
1334: 	}
1335: 	// final stage, only reached after parsing the file is finished
1336: 	// flush the parsed chunk and finalize parsing
1337: 	if (mode == ParserMode::PARSING) {
1338: 		Flush(insert_chunk);
1339: 	}
1340: 
1341: 	end_of_file_reached = true;
1342: 	return true;
1343: }
1344: 
1345: bool BufferedCSVReader::TryParseSimpleCSV(DataChunk &insert_chunk, string &error_message) {
1346: 	// used for parsing algorithm
1347: 	bool finished_chunk = false;
1348: 	idx_t column = 0;
1349: 	idx_t offset = 0;
1350: 	vector<idx_t> escape_positions;
1351: 
1352: 	// read values into the buffer (if any)
1353: 	if (position >= buffer_size) {
1354: 		if (!ReadBuffer(start)) {
1355: 			return true;
1356: 		}
1357: 	}
1358: 	// start parsing the first value
1359: 	goto value_start;
1360: value_start:
1361: 	offset = 0;
1362: 	/* state: value_start */
1363: 	// this state parses the first character of a value
1364: 	if (buffer[position] == options.quote[0]) {
1365: 		// quote: actual value starts in the next position
1366: 		// move to in_quotes state
1367: 		start = position + 1;
1368: 		goto in_quotes;
1369: 	} else {
1370: 		// no quote, move to normal parsing state
1371: 		start = position;
1372: 		goto normal;
1373: 	}
1374: normal:
1375: 	/* state: normal parsing state */
1376: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1377: 	do {
1378: 		for (; position < buffer_size; position++) {
1379: 			if (buffer[position] == options.delimiter[0]) {
1380: 				// delimiter: end the value and add it to the chunk
1381: 				goto add_value;
1382: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1383: 				// newline: add row
1384: 				goto add_row;
1385: 			}
1386: 		}
1387: 	} while (ReadBuffer(start));
1388: 	// file ends during normal scan: go to end state
1389: 	goto final_state;
1390: add_value:
1391: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1392: 	// increase position by 1 and move start to the new position
1393: 	offset = 0;
1394: 	start = ++position;
1395: 	if (position >= buffer_size && !ReadBuffer(start)) {
1396: 		// file ends right after delimiter, go to final state
1397: 		goto final_state;
1398: 	}
1399: 	goto value_start;
1400: add_row : {
1401: 	// check type of newline (\r or \n)
1402: 	bool carriage_return = buffer[position] == '\r';
1403: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1404: 	finished_chunk = AddRow(insert_chunk, column);
1405: 	// increase position by 1 and move start to the new position
1406: 	offset = 0;
1407: 	start = ++position;
1408: 	if (position >= buffer_size && !ReadBuffer(start)) {
1409: 		// file ends right after delimiter, go to final state
1410: 		goto final_state;
1411: 	}
1412: 	if (carriage_return) {
1413: 		// \r newline, go to special state that parses an optional \n afterwards
1414: 		goto carriage_return;
1415: 	} else {
1416: 		// \n newline, move to value start
1417: 		if (finished_chunk) {
1418: 			return true;
1419: 		}
1420: 		goto value_start;
1421: 	}
1422: }
1423: in_quotes:
1424: 	/* state: in_quotes */
1425: 	// this state parses the remainder of a quoted value
1426: 	position++;
1427: 	do {
1428: 		for (; position < buffer_size; position++) {
1429: 			if (buffer[position] == options.quote[0]) {
1430: 				// quote: move to unquoted state
1431: 				goto unquote;
1432: 			} else if (buffer[position] == options.escape[0]) {
1433: 				// escape: store the escaped position and move to handle_escape state
1434: 				escape_positions.push_back(position - start);
1435: 				goto handle_escape;
1436: 			}
1437: 		}
1438: 	} while (ReadBuffer(start));
1439: 	// still in quoted state at the end of the file, error:
1440: 	throw InvalidInputException("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1441: 	                            GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1442: unquote:
1443: 	/* state: unquote */
1444: 	// this state handles the state directly after we unquote
1445: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1446: 	// or a delimiter/newline, ending the current value and moving on to the next value
1447: 	position++;
1448: 	if (position >= buffer_size && !ReadBuffer(start)) {
1449: 		// file ends right after unquote, go to final state
1450: 		offset = 1;
1451: 		goto final_state;
1452: 	}
1453: 	if (buffer[position] == options.quote[0] && (options.escape.empty() || options.escape[0] == options.quote[0])) {
1454: 		// escaped quote, return to quoted state and store escape position
1455: 		escape_positions.push_back(position - start);
1456: 		goto in_quotes;
1457: 	} else if (buffer[position] == options.delimiter[0]) {
1458: 		// delimiter, add value
1459: 		offset = 1;
1460: 		goto add_value;
1461: 	} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1462: 		offset = 1;
1463: 		goto add_row;
1464: 	} else {
1465: 		error_message = StringUtil::Format(
1466: 		    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of "
1467: 		    "row or another quote. (%s)",
1468: 		    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1469: 		return false;
1470: 	}
1471: handle_escape:
1472: 	/* state: handle_escape */
1473: 	// escape should be followed by a quote or another escape character
1474: 	position++;
1475: 	if (position >= buffer_size && !ReadBuffer(start)) {
1476: 		error_message = StringUtil::Format(
1477: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1478: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1479: 		return false;
1480: 	}
1481: 	if (buffer[position] != options.quote[0] && buffer[position] != options.escape[0]) {
1482: 		error_message = StringUtil::Format(
1483: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1484: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1485: 		return false;
1486: 	}
1487: 	// escape was followed by quote or escape, go back to quoted state
1488: 	goto in_quotes;
1489: carriage_return:
1490: 	/* state: carriage_return */
1491: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1492: 	if (buffer[position] == '\n') {
1493: 		// newline after carriage return: skip
1494: 		// increase position by 1 and move start to the new position
1495: 		start = ++position;
1496: 		if (position >= buffer_size && !ReadBuffer(start)) {
1497: 			// file ends right after delimiter, go to final state
1498: 			goto final_state;
1499: 		}
1500: 	}
1501: 	if (finished_chunk) {
1502: 		return true;
1503: 	}
1504: 	goto value_start;
1505: final_state:
1506: 	if (finished_chunk) {
1507: 		return true;
1508: 	}
1509: 
1510: 	if (column > 0 || position > start) {
1511: 		// remaining values to be added to the chunk
1512: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1513: 		finished_chunk = AddRow(insert_chunk, column);
1514: 	}
1515: 
1516: 	// final stage, only reached after parsing the file is finished
1517: 	// flush the parsed chunk and finalize parsing
1518: 	if (mode == ParserMode::PARSING) {
1519: 		Flush(insert_chunk);
1520: 	}
1521: 
1522: 	end_of_file_reached = true;
1523: 	return true;
1524: }
1525: 
1526: bool BufferedCSVReader::ReadBuffer(idx_t &start) {
1527: 	auto old_buffer = move(buffer);
1528: 
1529: 	// the remaining part of the last buffer
1530: 	idx_t remaining = buffer_size - start;
1531: 	idx_t buffer_read_size = INITIAL_BUFFER_SIZE;
1532: 	while (remaining > buffer_read_size) {
1533: 		buffer_read_size *= 2;
1534: 	}
1535: 	if (remaining + buffer_read_size > MAXIMUM_CSV_LINE_SIZE) {
1536: 		throw InvalidInputException("Maximum line size of %llu bytes exceeded!", MAXIMUM_CSV_LINE_SIZE);
1537: 	}
1538: 	buffer = unique_ptr<char[]>(new char[buffer_read_size + remaining + 1]);
1539: 	buffer_size = remaining + buffer_read_size;
1540: 	if (remaining > 0) {
1541: 		// remaining from last buffer: copy it here
1542: 		memcpy(buffer.get(), old_buffer.get() + start, remaining);
1543: 	}
1544: 	idx_t read_count = file_handle->Read(buffer.get() + remaining, buffer_read_size);
1545: 
1546: 	bytes_in_chunk += read_count;
1547: 	buffer_size = remaining + read_count;
1548: 	buffer[buffer_size] = '\0';
1549: 	if (old_buffer) {
1550: 		cached_buffers.push_back(move(old_buffer));
1551: 	}
1552: 	start = 0;
1553: 	position = remaining;
1554: 	if (!bom_checked) {
1555: 		bom_checked = true;
1556: 		if (read_count >= 3 && buffer[0] == '\xEF' && buffer[1] == '\xBB' && buffer[2] == '\xBF') {
1557: 			position += 3;
1558: 		}
1559: 	}
1560: 
1561: 	return read_count > 0;
1562: }
1563: 
1564: void BufferedCSVReader::ParseCSV(DataChunk &insert_chunk) {
1565: 	// if no auto-detect or auto-detect with jumping samples, we have nothing cached and start from the beginning
1566: 	if (cached_chunks.empty()) {
1567: 		cached_buffers.clear();
1568: 	} else {
1569: 		auto &chunk = cached_chunks.front();
1570: 		parse_chunk.Move(*chunk);
1571: 		cached_chunks.pop();
1572: 		Flush(insert_chunk);
1573: 		return;
1574: 	}
1575: 
1576: 	string error_message;
1577: 	if (!TryParseCSV(ParserMode::PARSING, insert_chunk, error_message)) {
1578: 		throw InvalidInputException(error_message);
1579: 	}
1580: }
1581: 
1582: bool BufferedCSVReader::TryParseCSV(ParserMode mode) {
1583: 	DataChunk dummy_chunk;
1584: 	string error_message;
1585: 	return TryParseCSV(mode, dummy_chunk, error_message);
1586: }
1587: 
1588: void BufferedCSVReader::ParseCSV(ParserMode mode) {
1589: 	DataChunk dummy_chunk;
1590: 	string error_message;
1591: 	if (!TryParseCSV(mode, dummy_chunk, error_message)) {
1592: 		throw InvalidInputException(error_message);
1593: 	}
1594: }
1595: 
1596: bool BufferedCSVReader::TryParseCSV(ParserMode parser_mode, DataChunk &insert_chunk, string &error_message) {
1597: 	mode = parser_mode;
1598: 
1599: 	if (options.quote.size() <= 1 && options.escape.size() <= 1 && options.delimiter.size() == 1) {
1600: 		return TryParseSimpleCSV(insert_chunk, error_message);
1601: 	} else {
1602: 		return TryParseComplexCSV(insert_chunk, error_message);
1603: 	}
1604: }
1605: 
1606: void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vector<idx_t> &escape_positions) {
1607: 	if (length == 0 && column == 0) {
1608: 		row_empty = true;
1609: 	} else {
1610: 		row_empty = false;
1611: 	}
1612: 
1613: 	if (!sql_types.empty() && column == sql_types.size() && length == 0) {
1614: 		// skip a single trailing delimiter in last column
1615: 		return;
1616: 	}
1617: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1618: 		column++;
1619: 		return;
1620: 	}
1621: 	if (column >= sql_types.size()) {
1622: 		throw InvalidInputException("Error on line %s: expected %lld values per row, but got more. (%s)",
1623: 		                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(),
1624: 		                            options.ToString());
1625: 	}
1626: 
1627: 	// insert the line number into the chunk
1628: 	idx_t row_entry = parse_chunk.size();
1629: 
1630: 	str_val[length] = '\0';
1631: 
1632: 	// test against null string
1633: 	if (!options.force_not_null[column] && strcmp(options.null_str.c_str(), str_val) == 0) {
1634: 		FlatVector::SetNull(parse_chunk.data[column], row_entry, true);
1635: 	} else {
1636: 		auto &v = parse_chunk.data[column];
1637: 		auto parse_data = FlatVector::GetData<string_t>(v);
1638: 		if (!escape_positions.empty()) {
1639: 			// remove escape characters (if any)
1640: 			string old_val = str_val;
1641: 			string new_val = "";
1642: 			idx_t prev_pos = 0;
1643: 			for (idx_t i = 0; i < escape_positions.size(); i++) {
1644: 				idx_t next_pos = escape_positions[i];
1645: 				new_val += old_val.substr(prev_pos, next_pos - prev_pos);
1646: 
1647: 				if (options.escape.empty() || options.escape == options.quote) {
1648: 					prev_pos = next_pos + options.quote.size();
1649: 				} else {
1650: 					prev_pos = next_pos + options.escape.size();
1651: 				}
1652: 			}
1653: 			new_val += old_val.substr(prev_pos, old_val.size() - prev_pos);
1654: 			escape_positions.clear();
1655: 			parse_data[row_entry] = StringVector::AddStringOrBlob(v, string_t(new_val));
1656: 		} else {
1657: 			parse_data[row_entry] = string_t(str_val, length);
1658: 		}
1659: 	}
1660: 
1661: 	// move to the next column
1662: 	column++;
1663: }
1664: 
1665: bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {
1666: 	linenr++;
1667: 
1668: 	if (row_empty) {
1669: 		row_empty = false;
1670: 		if (sql_types.size() != 1) {
1671: 			column = 0;
1672: 			return false;
1673: 		}
1674: 	}
1675: 
1676: 	if (column < sql_types.size() && mode != ParserMode::SNIFFING_DIALECT) {
1677: 		throw InvalidInputException("Error on line %s: expected %lld values per row, but got %d. (%s)",
1678: 		                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column,
1679: 		                            options.ToString());
1680: 	}
1681: 
1682: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1683: 		sniffed_column_counts.push_back(column);
1684: 
1685: 		if (sniffed_column_counts.size() == options.sample_chunk_size) {
1686: 			return true;
1687: 		}
1688: 	} else {
1689: 		parse_chunk.SetCardinality(parse_chunk.size() + 1);
1690: 	}
1691: 
1692: 	if (mode == ParserMode::PARSING_HEADER) {
1693: 		return true;
1694: 	}
1695: 
1696: 	if (mode == ParserMode::SNIFFING_DATATYPES && parse_chunk.size() == options.sample_chunk_size) {
1697: 		return true;
1698: 	}
1699: 
1700: 	if (mode == ParserMode::PARSING && parse_chunk.size() == STANDARD_VECTOR_SIZE) {
1701: 		Flush(insert_chunk);
1702: 		return true;
1703: 	}
1704: 
1705: 	column = 0;
1706: 	return false;
1707: }
1708: 
1709: void BufferedCSVReader::Flush(DataChunk &insert_chunk) {
1710: 	if (parse_chunk.size() == 0) {
1711: 		return;
1712: 	}
1713: 	// convert the columns in the parsed chunk to the types of the table
1714: 	insert_chunk.SetCardinality(parse_chunk);
1715: 	for (idx_t col_idx = 0; col_idx < sql_types.size(); col_idx++) {
1716: 		if (sql_types[col_idx].id() == LogicalTypeId::VARCHAR) {
1717: 			// target type is varchar: no need to convert
1718: 			// just test that all strings are valid utf-8 strings
1719: 			auto parse_data = FlatVector::GetData<string_t>(parse_chunk.data[col_idx]);
1720: 			for (idx_t i = 0; i < parse_chunk.size(); i++) {
1721: 				if (!FlatVector::IsNull(parse_chunk.data[col_idx], i)) {
1722: 					auto s = parse_data[i];
1723: 					auto utf_type = Utf8Proc::Analyze(s.GetDataUnsafe(), s.GetSize());
1724: 					if (utf_type == UnicodeType::INVALID) {
1725: 						string col_name = to_string(col_idx);
1726: 						if (col_idx < col_names.size()) {
1727: 							col_name = "\"" + col_names[col_idx] + "\"";
1728: 						}
1729: 						throw InvalidInputException("Error in file \"%s\" between line %llu and %llu in column \"%s\": "
1730: 						                            "file is not valid UTF8. Parser options: %s",
1731: 						                            options.file_path, linenr - parse_chunk.size(), linenr, col_name,
1732: 						                            options.ToString());
1733: 					}
1734: 				}
1735: 			}
1736: 			insert_chunk.data[col_idx].Reference(parse_chunk.data[col_idx]);
1737: 		} else {
1738: 			string error_message;
1739: 			bool success;
1740: 			if (options.has_format[LogicalTypeId::DATE] && sql_types[col_idx].id() == LogicalTypeId::DATE) {
1741: 				// use the date format to cast the chunk
1742: 				success = TryCastDateVector(options, parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1743: 				                            parse_chunk.size(), error_message);
1744: 			} else if (options.has_format[LogicalTypeId::TIMESTAMP] &&
1745: 			           sql_types[col_idx].id() == LogicalTypeId::TIMESTAMP) {
1746: 				// use the date format to cast the chunk
1747: 				success = TryCastTimestampVector(options, parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1748: 				                                 parse_chunk.size(), error_message);
1749: 			} else {
1750: 				// target type is not varchar: perform a cast
1751: 				success = VectorOperations::TryCast(parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1752: 				                                    parse_chunk.size(), &error_message);
1753: 			}
1754: 			if (!success) {
1755: 				string col_name = to_string(col_idx);
1756: 				if (col_idx < col_names.size()) {
1757: 					col_name = "\"" + col_names[col_idx] + "\"";
1758: 				}
1759: 
1760: 				if (options.auto_detect) {
1761: 					throw InvalidInputException("%s in column %s, between line %llu and %llu. Parser "
1762: 					                            "options: %s. Consider either increasing the sample size "
1763: 					                            "(SAMPLE_SIZE=X [X rows] or SAMPLE_SIZE=-1 [all rows]), "
1764: 					                            "or skipping column conversion (ALL_VARCHAR=1)",
1765: 					                            error_message, col_name, linenr - parse_chunk.size() + 1, linenr,
1766: 					                            options.ToString());
1767: 				} else {
1768: 					throw InvalidInputException("%s between line %llu and %llu in column %s. Parser options: %s ",
1769: 					                            error_message, linenr - parse_chunk.size(), linenr, col_name,
1770: 					                            options.ToString());
1771: 				}
1772: 			}
1773: 		}
1774: 	}
1775: 	parse_chunk.Reset();
1776: }
1777: } // namespace duckdb
[end of src/execution/operator/persistent/buffered_csv_reader.cpp]
[start of src/function/table/copy_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/common/serializer/buffered_serializer.hpp"
4: #include "duckdb/function/copy_function.hpp"
5: #include "duckdb/parser/parsed_data/copy_info.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/common/file_system.hpp"
8: #include "duckdb/common/types/string_type.hpp"
9: #include "duckdb/common/vector_operations/vector_operations.hpp"
10: #include "duckdb/function/scalar/string_functions.hpp"
11: #include "duckdb/common/windows_undefs.hpp"
12: #include <limits>
13: 
14: namespace duckdb {
15: 
16: void SubstringDetection(string &str_1, string &str_2, const string &name_str_1, const string &name_str_2) {
17: 	if (str_1.empty() || str_2.empty()) {
18: 		return;
19: 	}
20: 	if (str_1.find(str_2) != string::npos || str_2.find(str_1) != std::string::npos) {
21: 		throw BinderException("%s must not appear in the %s specification and vice versa", name_str_1, name_str_2);
22: 	}
23: }
24: 
25: static bool ParseBoolean(vector<Value> &set) {
26: 	if (set.empty()) {
27: 		// no option specified: default to true
28: 		return true;
29: 	}
30: 	if (set.size() > 1) {
31: 		throw BinderException("Expected a single argument as a boolean value (e.g. TRUE or 1)");
32: 	}
33: 	if (set[0].type() == LogicalType::FLOAT || set[0].type() == LogicalType::DOUBLE ||
34: 	    set[0].type().id() == LogicalTypeId::DECIMAL) {
35: 		throw BinderException("Expected a boolean value (e.g. TRUE or 1)");
36: 	}
37: 	return BooleanValue::Get(set[0].CastAs(LogicalType::BOOLEAN));
38: }
39: 
40: static string ParseString(vector<Value> &set) {
41: 	if (set.size() != 1) {
42: 		// no option specified or multiple options specified
43: 		throw BinderException("Expected a single argument as a string value");
44: 	}
45: 	if (set[0].type().id() != LogicalTypeId::VARCHAR) {
46: 		throw BinderException("Expected a string argument!");
47: 	}
48: 	return set[0].GetValue<string>();
49: }
50: 
51: static int64_t ParseInteger(vector<Value> &set) {
52: 	if (set.size() != 1) {
53: 		// no option specified or multiple options specified
54: 		throw BinderException("Expected a single argument as a integer value");
55: 	}
56: 	return set[0].GetValue<int64_t>();
57: }
58: 
59: //===--------------------------------------------------------------------===//
60: // Bind
61: //===--------------------------------------------------------------------===//
62: static bool ParseBaseOption(BufferedCSVReaderOptions &options, string &loption, vector<Value> &set) {
63: 	if (StringUtil::StartsWith(loption, "delim") || StringUtil::StartsWith(loption, "sep")) {
64: 		options.SetDelimiter(ParseString(set));
65: 	} else if (loption == "quote") {
66: 		options.quote = ParseString(set);
67: 		options.has_quote = true;
68: 	} else if (loption == "escape") {
69: 		options.escape = ParseString(set);
70: 		options.has_escape = true;
71: 	} else if (loption == "header") {
72: 		options.header = ParseBoolean(set);
73: 		options.has_header = true;
74: 	} else if (loption == "null") {
75: 		options.null_str = ParseString(set);
76: 	} else if (loption == "encoding") {
77: 		auto encoding = StringUtil::Lower(ParseString(set));
78: 		if (encoding != "utf8" && encoding != "utf-8") {
79: 			throw BinderException("Copy is only supported for UTF-8 encoded files, ENCODING 'UTF-8'");
80: 		}
81: 	} else if (loption == "compression") {
82: 		options.compression = FileCompressionTypeFromString(ParseString(set));
83: 	} else if (loption == "skip") {
84: 		options.skip_rows = ParseInteger(set);
85: 	} else {
86: 		// unrecognized option in base CSV
87: 		return false;
88: 	}
89: 	return true;
90: }
91: 
92: void BaseCSVData::Finalize() {
93: 	// verify that the options are correct in the final pass
94: 	if (options.escape.empty()) {
95: 		options.escape = options.quote;
96: 	}
97: 	// escape and delimiter must not be substrings of each other
98: 	if (options.has_delimiter && options.has_escape) {
99: 		SubstringDetection(options.delimiter, options.escape, "DELIMITER", "ESCAPE");
100: 	}
101: 	// delimiter and quote must not be substrings of each other
102: 	if (options.has_quote && options.has_delimiter) {
103: 		SubstringDetection(options.quote, options.delimiter, "DELIMITER", "QUOTE");
104: 	}
105: 	// escape and quote must not be substrings of each other (but can be the same)
106: 	if (options.quote != options.escape && options.has_quote && options.has_escape) {
107: 		SubstringDetection(options.quote, options.escape, "QUOTE", "ESCAPE");
108: 	}
109: 	if (!options.null_str.empty()) {
110: 		// null string and delimiter must not be substrings of each other
111: 		if (options.has_delimiter) {
112: 			SubstringDetection(options.delimiter, options.null_str, "DELIMITER", "NULL");
113: 		}
114: 		// quote/escape and nullstr must not be substrings of each other
115: 		if (options.has_quote) {
116: 			SubstringDetection(options.quote, options.null_str, "QUOTE", "NULL");
117: 		}
118: 		if (options.has_escape) {
119: 			SubstringDetection(options.escape, options.null_str, "ESCAPE", "NULL");
120: 		}
121: 	}
122: }
123: 
124: static vector<bool> ParseColumnList(vector<Value> &set, vector<string> &names) {
125: 	vector<bool> result;
126: 	if (set.empty()) {
127: 		throw BinderException("Expected a column list or * as parameter");
128: 	}
129: 	if (set.size() == 1 && set[0].type().id() == LogicalTypeId::VARCHAR && set[0].GetValue<string>() == "*") {
130: 		// *, force_not_null on all columns
131: 		result.resize(names.size(), true);
132: 	} else {
133: 		// list of options: parse the list
134: 		unordered_map<string, bool> option_map;
135: 		for (idx_t i = 0; i < set.size(); i++) {
136: 			option_map[set[i].ToString()] = false;
137: 		}
138: 		result.resize(names.size(), false);
139: 		for (idx_t i = 0; i < names.size(); i++) {
140: 			auto entry = option_map.find(names[i]);
141: 			if (entry != option_map.end()) {
142: 				result[i] = true;
143: 				entry->second = true;
144: 			}
145: 		}
146: 		for (auto &entry : option_map) {
147: 			if (!entry.second) {
148: 				throw BinderException("Column %s not found in table", entry.first.c_str());
149: 			}
150: 		}
151: 	}
152: 	return result;
153: }
154: 
155: static unique_ptr<FunctionData> WriteCSVBind(ClientContext &context, CopyInfo &info, vector<string> &names,
156:                                              vector<LogicalType> &sql_types) {
157: 	auto bind_data = make_unique<WriteCSVData>(info.file_path, sql_types, names);
158: 
159: 	// check all the options in the copy info
160: 	for (auto &option : info.options) {
161: 		auto loption = StringUtil::Lower(option.first);
162: 		auto &set = option.second;
163: 		if (ParseBaseOption(bind_data->options, loption, set)) {
164: 			// parsed option in base CSV options: continue
165: 			continue;
166: 		} else if (loption == "force_quote") {
167: 			bind_data->force_quote = ParseColumnList(set, names);
168: 		} else {
169: 			throw NotImplementedException("Unrecognized option for CSV: %s", option.first.c_str());
170: 		}
171: 	}
172: 	// verify the parsed options
173: 	if (bind_data->force_quote.empty()) {
174: 		// no FORCE_QUOTE specified: initialize to false
175: 		bind_data->force_quote.resize(names.size(), false);
176: 	}
177: 	bind_data->Finalize();
178: 	bind_data->is_simple = bind_data->options.delimiter.size() == 1 && bind_data->options.escape.size() == 1 &&
179: 	                       bind_data->options.quote.size() == 1;
180: 	return move(bind_data);
181: }
182: 
183: static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, CopyInfo &info, vector<string> &expected_names,
184:                                             vector<LogicalType> &expected_types) {
185: 	auto bind_data = make_unique<ReadCSVData>();
186: 	bind_data->sql_types = expected_types;
187: 
188: 	string file_pattern = info.file_path;
189: 
190: 	auto &fs = FileSystem::GetFileSystem(context);
191: 	bind_data->files = fs.Glob(file_pattern);
192: 	if (bind_data->files.empty()) {
193: 		throw IOException("No files found that match the pattern \"%s\"", file_pattern);
194: 	}
195: 
196: 	auto &options = bind_data->options;
197: 
198: 	// check all the options in the copy info
199: 	for (auto &option : info.options) {
200: 		auto loption = StringUtil::Lower(option.first);
201: 		auto &set = option.second;
202: 		if (loption == "auto_detect") {
203: 			options.auto_detect = ParseBoolean(set);
204: 		} else if (ParseBaseOption(options, loption, set)) {
205: 			// parsed option in base CSV options: continue
206: 			continue;
207: 		} else if (loption == "sample_size") {
208: 			int64_t sample_size = ParseInteger(set);
209: 			if (sample_size < 1 && sample_size != -1) {
210: 				throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
211: 			}
212: 			if (sample_size == -1) {
213: 				options.sample_chunks = std::numeric_limits<uint64_t>::max();
214: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
215: 			} else if (sample_size <= STANDARD_VECTOR_SIZE) {
216: 				options.sample_chunk_size = sample_size;
217: 				options.sample_chunks = 1;
218: 			} else {
219: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
220: 				options.sample_chunks = sample_size / STANDARD_VECTOR_SIZE;
221: 			}
222: 		} else if (loption == "sample_chunk_size") {
223: 			options.sample_chunk_size = ParseInteger(set);
224: 			if (options.sample_chunk_size > STANDARD_VECTOR_SIZE) {
225: 				throw BinderException(
226: 				    "Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
227: 				    STANDARD_VECTOR_SIZE);
228: 			} else if (options.sample_chunk_size < 1) {
229: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be smaller than 1");
230: 			}
231: 		} else if (loption == "sample_chunks") {
232: 			options.sample_chunks = ParseInteger(set);
233: 			if (options.sample_chunks < 1) {
234: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNKS: cannot be smaller than 1");
235: 			}
236: 		} else if (loption == "force_not_null") {
237: 			options.force_not_null = ParseColumnList(set, expected_names);
238: 		} else if (loption == "date_format" || loption == "dateformat") {
239: 			string format = ParseString(set);
240: 			auto &date_format = options.date_format[LogicalTypeId::DATE];
241: 			string error = StrTimeFormat::ParseFormatSpecifier(format, date_format);
242: 			date_format.format_specifier = format;
243: 			if (!error.empty()) {
244: 				throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
245: 			}
246: 			options.has_format[LogicalTypeId::DATE] = true;
247: 		} else if (loption == "timestamp_format" || loption == "timestampformat") {
248: 			string format = ParseString(set);
249: 			auto &timestamp_format = options.date_format[LogicalTypeId::TIMESTAMP];
250: 			string error = StrTimeFormat::ParseFormatSpecifier(format, timestamp_format);
251: 			timestamp_format.format_specifier = format;
252: 			if (!error.empty()) {
253: 				throw InvalidInputException("Could not parse TIMESTAMPFORMAT: %s", error.c_str());
254: 			}
255: 			options.has_format[LogicalTypeId::TIMESTAMP] = true;
256: 		} else {
257: 			throw NotImplementedException("Unrecognized option for CSV: %s", option.first.c_str());
258: 		}
259: 	}
260: 	// verify the parsed options
261: 	if (options.force_not_null.empty()) {
262: 		// no FORCE_QUOTE specified: initialize to false
263: 		options.force_not_null.resize(expected_types.size(), false);
264: 	}
265: 	bind_data->Finalize();
266: 	return move(bind_data);
267: }
268: 
269: //===--------------------------------------------------------------------===//
270: // Helper writing functions
271: //===--------------------------------------------------------------------===//
272: static string AddEscapes(string &to_be_escaped, const string &escape, const string &val) {
273: 	idx_t i = 0;
274: 	string new_val = "";
275: 	idx_t found = val.find(to_be_escaped);
276: 
277: 	while (found != string::npos) {
278: 		while (i < found) {
279: 			new_val += val[i];
280: 			i++;
281: 		}
282: 		new_val += escape;
283: 		found = val.find(to_be_escaped, found + escape.length());
284: 	}
285: 	while (i < val.length()) {
286: 		new_val += val[i];
287: 		i++;
288: 	}
289: 	return new_val;
290: }
291: 
292: static bool RequiresQuotes(WriteCSVData &csv_data, const char *str, idx_t len) {
293: 	auto &options = csv_data.options;
294: 	// check if the string is equal to the null string
295: 	if (len == options.null_str.size() && memcmp(str, options.null_str.c_str(), len) == 0) {
296: 		return true;
297: 	}
298: 	if (csv_data.is_simple) {
299: 		// simple CSV: check for newlines, quotes and delimiter all at once
300: 		for (idx_t i = 0; i < len; i++) {
301: 			if (str[i] == '\n' || str[i] == '\r' || str[i] == options.quote[0] || str[i] == options.delimiter[0]) {
302: 				// newline, write a quoted string
303: 				return true;
304: 			}
305: 		}
306: 		// no newline, quote or delimiter in the string
307: 		// no quoting or escaping necessary
308: 		return false;
309: 	} else {
310: 		// CSV with complex quotes/delimiter (multiple bytes)
311: 
312: 		// first check for \n, \r, \n\r in string
313: 		for (idx_t i = 0; i < len; i++) {
314: 			if (str[i] == '\n' || str[i] == '\r') {
315: 				// newline, write a quoted string
316: 				return true;
317: 			}
318: 		}
319: 
320: 		// check for delimiter
321: 		if (ContainsFun::Find((const unsigned char *)str, len, (const unsigned char *)options.delimiter.c_str(),
322: 		                      options.delimiter.size()) != DConstants::INVALID_INDEX) {
323: 			return true;
324: 		}
325: 		// check for quote
326: 		if (ContainsFun::Find((const unsigned char *)str, len, (const unsigned char *)options.quote.c_str(),
327: 		                      options.quote.size()) != DConstants::INVALID_INDEX) {
328: 			return true;
329: 		}
330: 		return false;
331: 	}
332: }
333: 
334: static void WriteQuotedString(Serializer &serializer, WriteCSVData &csv_data, const char *str, idx_t len,
335:                               bool force_quote) {
336: 	auto &options = csv_data.options;
337: 	if (!force_quote) {
338: 		// force quote is disabled: check if we need to add quotes anyway
339: 		force_quote = RequiresQuotes(csv_data, str, len);
340: 	}
341: 	if (force_quote) {
342: 		// quoting is enabled: we might need to escape things in the string
343: 		bool requires_escape = false;
344: 		if (csv_data.is_simple) {
345: 			// simple CSV
346: 			// do a single loop to check for a quote or escape value
347: 			for (idx_t i = 0; i < len; i++) {
348: 				if (str[i] == options.quote[0] || str[i] == options.escape[0]) {
349: 					requires_escape = true;
350: 					break;
351: 				}
352: 			}
353: 		} else {
354: 			// complex CSV
355: 			// check for quote or escape separately
356: 			if (ContainsFun::Find((const unsigned char *)str, len, (const unsigned char *)options.quote.c_str(),
357: 			                      options.quote.size()) != DConstants::INVALID_INDEX) {
358: 				requires_escape = true;
359: 			} else if (ContainsFun::Find((const unsigned char *)str, len, (const unsigned char *)options.escape.c_str(),
360: 			                             options.escape.size()) != DConstants::INVALID_INDEX) {
361: 				requires_escape = true;
362: 			}
363: 		}
364: 		if (!requires_escape) {
365: 			// fast path: no need to escape anything
366: 			serializer.WriteBufferData(options.quote);
367: 			serializer.WriteData((const_data_ptr_t)str, len);
368: 			serializer.WriteBufferData(options.quote);
369: 			return;
370: 		}
371: 
372: 		// slow path: need to add escapes
373: 		string new_val(str, len);
374: 		new_val = AddEscapes(options.escape, options.escape, new_val);
375: 		if (options.escape != options.quote) {
376: 			// need to escape quotes separately
377: 			new_val = AddEscapes(options.quote, options.escape, new_val);
378: 		}
379: 		serializer.WriteBufferData(options.quote);
380: 		serializer.WriteBufferData(new_val);
381: 		serializer.WriteBufferData(options.quote);
382: 	} else {
383: 		serializer.WriteData((const_data_ptr_t)str, len);
384: 	}
385: }
386: 
387: //===--------------------------------------------------------------------===//
388: // Sink
389: //===--------------------------------------------------------------------===//
390: struct LocalReadCSVData : public LocalFunctionData {
391: 	//! The thread-local buffer to write data into
392: 	BufferedSerializer serializer;
393: 	//! A chunk with VARCHAR columns to cast intermediates into
394: 	DataChunk cast_chunk;
395: };
396: 
397: struct GlobalWriteCSVData : public GlobalFunctionData {
398: 	GlobalWriteCSVData(FileSystem &fs, const string &file_path, FileOpener *opener, FileCompressionType compression)
399: 	    : fs(fs) {
400: 		handle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE_NEW,
401: 		                     FileLockType::WRITE_LOCK, compression, opener);
402: 	}
403: 
404: 	void WriteData(const_data_ptr_t data, idx_t size) {
405: 		lock_guard<mutex> flock(lock);
406: 		handle->Write((void *)data, size);
407: 	}
408: 
409: 	FileSystem &fs;
410: 	//! The mutex for writing to the physical file
411: 	mutex lock;
412: 	//! The file handle to write to
413: 	unique_ptr<FileHandle> handle;
414: };
415: 
416: static unique_ptr<LocalFunctionData> WriteCSVInitializeLocal(ClientContext &context, FunctionData &bind_data) {
417: 	auto &csv_data = (WriteCSVData &)bind_data;
418: 	auto local_data = make_unique<LocalReadCSVData>();
419: 
420: 	// create the chunk with VARCHAR types
421: 	vector<LogicalType> types;
422: 	types.resize(csv_data.names.size(), LogicalType::VARCHAR);
423: 
424: 	local_data->cast_chunk.Initialize(types);
425: 	return move(local_data);
426: }
427: 
428: static unique_ptr<GlobalFunctionData> WriteCSVInitializeGlobal(ClientContext &context, FunctionData &bind_data) {
429: 	auto &csv_data = (WriteCSVData &)bind_data;
430: 	auto &options = csv_data.options;
431: 	auto global_data = make_unique<GlobalWriteCSVData>(FileSystem::GetFileSystem(context), csv_data.files[0],
432: 	                                                   FileSystem::GetFileOpener(context), options.compression);
433: 
434: 	if (options.header) {
435: 		BufferedSerializer serializer;
436: 		// write the header line to the file
437: 		for (idx_t i = 0; i < csv_data.names.size(); i++) {
438: 			if (i != 0) {
439: 				serializer.WriteBufferData(options.delimiter);
440: 			}
441: 			WriteQuotedString(serializer, csv_data, csv_data.names[i].c_str(), csv_data.names[i].size(), false);
442: 		}
443: 		serializer.WriteBufferData(csv_data.newline);
444: 
445: 		global_data->WriteData(serializer.blob.data.get(), serializer.blob.size);
446: 	}
447: 	return move(global_data);
448: }
449: 
450: static void WriteCSVSink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
451:                          LocalFunctionData &lstate, DataChunk &input) {
452: 	auto &csv_data = (WriteCSVData &)bind_data;
453: 	auto &options = csv_data.options;
454: 	auto &local_data = (LocalReadCSVData &)lstate;
455: 	auto &global_state = (GlobalWriteCSVData &)gstate;
456: 
457: 	// write data into the local buffer
458: 
459: 	// first cast the columns of the chunk to varchar
460: 	auto &cast_chunk = local_data.cast_chunk;
461: 	cast_chunk.SetCardinality(input);
462: 	for (idx_t col_idx = 0; col_idx < input.ColumnCount(); col_idx++) {
463: 		if (csv_data.sql_types[col_idx].id() == LogicalTypeId::VARCHAR) {
464: 			// VARCHAR, just create a reference
465: 			cast_chunk.data[col_idx].Reference(input.data[col_idx]);
466: 		} else {
467: 			// non varchar column, perform the cast
468: 			VectorOperations::Cast(input.data[col_idx], cast_chunk.data[col_idx], input.size());
469: 		}
470: 	}
471: 
472: 	cast_chunk.Normalify();
473: 	auto &writer = local_data.serializer;
474: 	// now loop over the vectors and output the values
475: 	for (idx_t row_idx = 0; row_idx < cast_chunk.size(); row_idx++) {
476: 		// write values
477: 		for (idx_t col_idx = 0; col_idx < cast_chunk.ColumnCount(); col_idx++) {
478: 			if (col_idx != 0) {
479: 				writer.WriteBufferData(options.delimiter);
480: 			}
481: 			if (FlatVector::IsNull(cast_chunk.data[col_idx], row_idx)) {
482: 				// write null value
483: 				writer.WriteBufferData(options.null_str);
484: 				continue;
485: 			}
486: 
487: 			// non-null value, fetch the string value from the cast chunk
488: 			auto str_data = FlatVector::GetData<string_t>(cast_chunk.data[col_idx]);
489: 			auto str_value = str_data[row_idx];
490: 			// FIXME: we could gain some performance here by checking for certain types if they ever require quotes
491: 			// (e.g. integers only require quotes if the delimiter is a number, decimals only require quotes if the
492: 			// delimiter is a number or "." character)
493: 			WriteQuotedString(writer, csv_data, str_value.GetDataUnsafe(), str_value.GetSize(),
494: 			                  csv_data.force_quote[col_idx]);
495: 		}
496: 		writer.WriteBufferData(csv_data.newline);
497: 	}
498: 	// check if we should flush what we have currently written
499: 	if (writer.blob.size >= csv_data.flush_size) {
500: 		global_state.WriteData(writer.blob.data.get(), writer.blob.size);
501: 		writer.Reset();
502: 	}
503: }
504: 
505: //===--------------------------------------------------------------------===//
506: // Combine
507: //===--------------------------------------------------------------------===//
508: static void WriteCSVCombine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
509:                             LocalFunctionData &lstate) {
510: 	auto &local_data = (LocalReadCSVData &)lstate;
511: 	auto &global_state = (GlobalWriteCSVData &)gstate;
512: 	auto &writer = local_data.serializer;
513: 	// flush the local writer
514: 	if (writer.blob.size > 0) {
515: 		global_state.WriteData(writer.blob.data.get(), writer.blob.size);
516: 		writer.Reset();
517: 	}
518: }
519: 
520: //===--------------------------------------------------------------------===//
521: // Finalize
522: //===--------------------------------------------------------------------===//
523: void WriteCSVFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
524: 	auto &global_state = (GlobalWriteCSVData &)gstate;
525: 	global_state.handle->Close();
526: 	global_state.handle.reset();
527: }
528: 
529: void CSVCopyFunction::RegisterFunction(BuiltinFunctions &set) {
530: 	CopyFunction info("csv");
531: 	info.copy_to_bind = WriteCSVBind;
532: 	info.copy_to_initialize_local = WriteCSVInitializeLocal;
533: 	info.copy_to_initialize_global = WriteCSVInitializeGlobal;
534: 	info.copy_to_sink = WriteCSVSink;
535: 	info.copy_to_combine = WriteCSVCombine;
536: 	info.copy_to_finalize = WriteCSVFinalize;
537: 
538: 	info.copy_from_bind = ReadCSVBind;
539: 	info.copy_from_function = ReadCSVTableFunction::GetFunction();
540: 
541: 	info.extension = "csv";
542: 
543: 	set.AddFunction(info);
544: }
545: 
546: } // namespace duckdb
[end of src/function/table/copy_csv.cpp]
[start of src/function/table/read_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/main/database.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/main/config.hpp"
8: #include "duckdb/parser/expression/constant_expression.hpp"
9: #include "duckdb/parser/expression/function_expression.hpp"
10: #include "duckdb/parser/tableref/table_function_ref.hpp"
11: #include "duckdb/common/windows_undefs.hpp"
12: 
13: #include <limits>
14: 
15: namespace duckdb {
16: 
17: static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, vector<Value> &inputs,
18:                                             named_parameter_map_t &named_parameters,
19:                                             vector<LogicalType> &input_table_types, vector<string> &input_table_names,
20:                                             vector<LogicalType> &return_types, vector<string> &names) {
21: 	auto &config = DBConfig::GetConfig(context);
22: 	if (!config.enable_external_access) {
23: 		throw PermissionException("Scanning CSV files is disabled through configuration");
24: 	}
25: 	auto result = make_unique<ReadCSVData>();
26: 	auto &options = result->options;
27: 
28: 	auto &file_pattern = StringValue::Get(inputs[0]);
29: 
30: 	auto &fs = FileSystem::GetFileSystem(context);
31: 	result->files = fs.Glob(file_pattern);
32: 	if (result->files.empty()) {
33: 		throw IOException("No files found that match the pattern \"%s\"", file_pattern);
34: 	}
35: 
36: 	for (auto &kv : named_parameters) {
37: 		auto loption = StringUtil::Lower(kv.first);
38: 		if (loption == "auto_detect") {
39: 			options.auto_detect = BooleanValue::Get(kv.second);
40: 		} else if (loption == "sep" || loption == "delim") {
41: 			options.SetDelimiter(StringValue::Get(kv.second));
42: 		} else if (loption == "header") {
43: 			options.header = BooleanValue::Get(kv.second);
44: 			options.has_header = true;
45: 		} else if (loption == "quote") {
46: 			options.quote = StringValue::Get(kv.second);
47: 			options.has_quote = true;
48: 		} else if (loption == "escape") {
49: 			options.escape = StringValue::Get(kv.second);
50: 			options.has_escape = true;
51: 		} else if (loption == "nullstr") {
52: 			options.null_str = StringValue::Get(kv.second);
53: 		} else if (loption == "sample_size") {
54: 			int64_t sample_size = kv.second.GetValue<int64_t>();
55: 			if (sample_size < 1 && sample_size != -1) {
56: 				throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
57: 			}
58: 			if (sample_size == -1) {
59: 				options.sample_chunks = std::numeric_limits<uint64_t>::max();
60: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
61: 			} else if (sample_size <= STANDARD_VECTOR_SIZE) {
62: 				options.sample_chunk_size = sample_size;
63: 				options.sample_chunks = 1;
64: 			} else {
65: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
66: 				options.sample_chunks = sample_size / STANDARD_VECTOR_SIZE;
67: 			}
68: 		} else if (loption == "sample_chunk_size") {
69: 			options.sample_chunk_size = kv.second.GetValue<int64_t>();
70: 			if (options.sample_chunk_size > STANDARD_VECTOR_SIZE) {
71: 				throw BinderException(
72: 				    "Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
73: 				    STANDARD_VECTOR_SIZE);
74: 			} else if (options.sample_chunk_size < 1) {
75: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be smaller than 1");
76: 			}
77: 		} else if (loption == "sample_chunks") {
78: 			options.sample_chunks = kv.second.GetValue<int64_t>();
79: 			if (options.sample_chunks < 1) {
80: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNKS: cannot be smaller than 1");
81: 			}
82: 		} else if (loption == "all_varchar") {
83: 			options.all_varchar = BooleanValue::Get(kv.second);
84: 		} else if (loption == "dateformat") {
85: 			options.has_format[LogicalTypeId::DATE] = true;
86: 			auto &date_format = options.date_format[LogicalTypeId::DATE];
87: 			date_format.format_specifier = StringValue::Get(kv.second);
88: 			string error = StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
89: 			if (!error.empty()) {
90: 				throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
91: 			}
92: 		} else if (loption == "timestampformat") {
93: 			options.has_format[LogicalTypeId::TIMESTAMP] = true;
94: 			auto &timestamp_format = options.date_format[LogicalTypeId::TIMESTAMP];
95: 			timestamp_format.format_specifier = StringValue::Get(kv.second);
96: 			string error = StrTimeFormat::ParseFormatSpecifier(timestamp_format.format_specifier, timestamp_format);
97: 			if (!error.empty()) {
98: 				throw InvalidInputException("Could not parse TIMESTAMPFORMAT: %s", error.c_str());
99: 			}
100: 		} else if (loption == "normalize_names") {
101: 			options.normalize_names = BooleanValue::Get(kv.second);
102: 		} else if (loption == "columns") {
103: 			auto &child_type = kv.second.type();
104: 			if (child_type.id() != LogicalTypeId::STRUCT) {
105: 				throw BinderException("read_csv columns requires a a struct as input");
106: 			}
107: 			auto &struct_children = StructValue::GetChildren(kv.second);
108: 			D_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());
109: 			for (idx_t i = 0; i < struct_children.size(); i++) {
110: 				auto &name = StructType::GetChildName(child_type, i);
111: 				auto &val = struct_children[i];
112: 				names.push_back(name);
113: 				if (val.type().id() != LogicalTypeId::VARCHAR) {
114: 					throw BinderException("read_csv requires a type specification as string");
115: 				}
116: 				return_types.emplace_back(TransformStringToLogicalType(StringValue::Get(val)));
117: 			}
118: 			if (names.empty()) {
119: 				throw BinderException("read_csv requires at least a single column as input!");
120: 			}
121: 		} else if (loption == "compression") {
122: 			options.compression = FileCompressionTypeFromString(StringValue::Get(kv.second));
123: 		} else if (loption == "filename") {
124: 			result->include_file_name = BooleanValue::Get(kv.second);
125: 		} else if (loption == "skip") {
126: 			options.skip_rows = kv.second.GetValue<int64_t>();
127: 		} else {
128: 			throw InternalException("Unrecognized parameter %s", kv.first);
129: 		}
130: 	}
131: 	if (!options.auto_detect && return_types.empty()) {
132: 		throw BinderException("read_csv requires columns to be specified. Use read_csv_auto or set read_csv(..., "
133: 		                      "AUTO_DETECT=TRUE) to automatically guess columns.");
134: 	}
135: 	if (options.auto_detect) {
136: 		options.file_path = result->files[0];
137: 		auto initial_reader = make_unique<BufferedCSVReader>(context, options);
138: 
139: 		return_types.assign(initial_reader->sql_types.begin(), initial_reader->sql_types.end());
140: 		if (names.empty()) {
141: 			names.assign(initial_reader->col_names.begin(), initial_reader->col_names.end());
142: 		} else {
143: 			D_ASSERT(return_types.size() == names.size());
144: 		}
145: 		result->initial_reader = move(initial_reader);
146: 	} else {
147: 		result->sql_types = return_types;
148: 		D_ASSERT(return_types.size() == names.size());
149: 	}
150: 	if (result->include_file_name) {
151: 		return_types.emplace_back(LogicalType::VARCHAR);
152: 		names.emplace_back("filename");
153: 	}
154: 	return move(result);
155: }
156: 
157: struct ReadCSVOperatorData : public FunctionOperatorData {
158: 	//! The CSV reader
159: 	unique_ptr<BufferedCSVReader> csv_reader;
160: 	//! The index of the next file to read (i.e. current file + 1)
161: 	idx_t file_index;
162: };
163: 
164: static unique_ptr<FunctionOperatorData> ReadCSVInit(ClientContext &context, const FunctionData *bind_data_p,
165:                                                     const vector<column_t> &column_ids,
166:                                                     TableFilterCollection *filters) {
167: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
168: 	auto result = make_unique<ReadCSVOperatorData>();
169: 	if (bind_data.initial_reader) {
170: 		result->csv_reader = move(bind_data.initial_reader);
171: 	} else {
172: 		bind_data.options.file_path = bind_data.files[0];
173: 		result->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);
174: 	}
175: 	bind_data.bytes_read = 0;
176: 	bind_data.file_size = result->csv_reader->GetFileSize();
177: 	result->file_index = 1;
178: 	return move(result);
179: }
180: 
181: static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, vector<Value> &inputs,
182:                                                 named_parameter_map_t &named_parameters,
183:                                                 vector<LogicalType> &input_table_types,
184:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
185:                                                 vector<string> &names) {
186: 	named_parameters["auto_detect"] = Value::BOOLEAN(true);
187: 	return ReadCSVBind(context, inputs, named_parameters, input_table_types, input_table_names, return_types, names);
188: }
189: 
190: static void ReadCSVFunction(ClientContext &context, const FunctionData *bind_data_p,
191:                             FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
192: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
193: 	auto &data = (ReadCSVOperatorData &)*operator_state;
194: 	do {
195: 		data.csv_reader->ParseCSV(output);
196: 		bind_data.bytes_read = data.csv_reader->bytes_in_chunk;
197: 		if (output.size() == 0 && data.file_index < bind_data.files.size()) {
198: 			// exhausted this file, but we have more files we can read
199: 			// open the next file and increment the counter
200: 			bind_data.options.file_path = bind_data.files[data.file_index];
201: 			data.csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, data.csv_reader->sql_types);
202: 			data.file_index++;
203: 		} else {
204: 			break;
205: 		}
206: 	} while (true);
207: 	if (bind_data.include_file_name) {
208: 		auto &col = output.data.back();
209: 		col.SetValue(0, Value(data.csv_reader->options.file_path));
210: 		col.SetVectorType(VectorType::CONSTANT_VECTOR);
211: 	}
212: }
213: 
214: static void ReadCSVAddNamedParameters(TableFunction &table_function) {
215: 	table_function.named_parameters["sep"] = LogicalType::VARCHAR;
216: 	table_function.named_parameters["delim"] = LogicalType::VARCHAR;
217: 	table_function.named_parameters["quote"] = LogicalType::VARCHAR;
218: 	table_function.named_parameters["escape"] = LogicalType::VARCHAR;
219: 	table_function.named_parameters["nullstr"] = LogicalType::VARCHAR;
220: 	table_function.named_parameters["columns"] = LogicalType::ANY;
221: 	table_function.named_parameters["header"] = LogicalType::BOOLEAN;
222: 	table_function.named_parameters["auto_detect"] = LogicalType::BOOLEAN;
223: 	table_function.named_parameters["sample_size"] = LogicalType::BIGINT;
224: 	table_function.named_parameters["sample_chunk_size"] = LogicalType::BIGINT;
225: 	table_function.named_parameters["sample_chunks"] = LogicalType::BIGINT;
226: 	table_function.named_parameters["all_varchar"] = LogicalType::BOOLEAN;
227: 	table_function.named_parameters["dateformat"] = LogicalType::VARCHAR;
228: 	table_function.named_parameters["timestampformat"] = LogicalType::VARCHAR;
229: 	table_function.named_parameters["normalize_names"] = LogicalType::BOOLEAN;
230: 	table_function.named_parameters["compression"] = LogicalType::VARCHAR;
231: 	table_function.named_parameters["filename"] = LogicalType::BOOLEAN;
232: 	table_function.named_parameters["skip"] = LogicalType::BIGINT;
233: }
234: 
235: double CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p) {
236: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
237: 	if (bind_data.file_size == 0) {
238: 		return 100;
239: 	}
240: 	auto percentage = (bind_data.bytes_read * 100.0) / bind_data.file_size;
241: 	return percentage;
242: }
243: 
244: TableFunction ReadCSVTableFunction::GetFunction() {
245: 	TableFunction read_csv("read_csv", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVBind, ReadCSVInit);
246: 	read_csv.table_scan_progress = CSVReaderProgress;
247: 	ReadCSVAddNamedParameters(read_csv);
248: 	return read_csv;
249: }
250: 
251: void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {
252: 	set.AddFunction(ReadCSVTableFunction::GetFunction());
253: 
254: 	TableFunction read_csv_auto("read_csv_auto", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVAutoBind, ReadCSVInit);
255: 	read_csv_auto.table_scan_progress = CSVReaderProgress;
256: 	ReadCSVAddNamedParameters(read_csv_auto);
257: 	set.AddFunction(read_csv_auto);
258: }
259: 
260: unique_ptr<TableFunctionRef> ReadCSVReplacement(const string &table_name, void *data) {
261: 	auto lower_name = StringUtil::Lower(table_name);
262: 	// remove any compression
263: 	if (StringUtil::EndsWith(lower_name, ".gz")) {
264: 		lower_name = lower_name.substr(0, lower_name.size() - 3);
265: 	} else if (StringUtil::EndsWith(lower_name, ".zst")) {
266: 		lower_name = lower_name.substr(0, lower_name.size() - 4);
267: 	}
268: 	if (!StringUtil::EndsWith(lower_name, ".csv") && !StringUtil::EndsWith(lower_name, ".tsv")) {
269: 		return nullptr;
270: 	}
271: 	auto table_function = make_unique<TableFunctionRef>();
272: 	vector<unique_ptr<ParsedExpression>> children;
273: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
274: 	table_function->function = make_unique<FunctionExpression>("read_csv_auto", move(children));
275: 	return table_function;
276: }
277: 
278: void BuiltinFunctions::RegisterReadFunctions() {
279: 	CSVCopyFunction::RegisterFunction(*this);
280: 	ReadCSVTableFunction::RegisterFunction(*this);
281: 
282: 	auto &config = DBConfig::GetConfig(context);
283: 	config.replacement_scans.emplace_back(ReadCSVReplacement);
284: }
285: 
286: } // namespace duckdb
[end of src/function/table/read_csv.cpp]
[start of src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/persistent/buffered_csv_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/parser/parsed_data/copy_info.hpp"
13: #include "duckdb/function/scalar/strftime.hpp"
14: #include "duckdb/common/types/chunk_collection.hpp"
15: #include "duckdb/common/enums/file_compression_type.hpp"
16: #include "duckdb/common/map.hpp"
17: 
18: #include <sstream>
19: #include <queue>
20: 
21: namespace duckdb {
22: struct CopyInfo;
23: struct CSVFileHandle;
24: struct FileHandle;
25: struct StrpTimeFormat;
26: 
27: class FileOpener;
28: class FileSystem;
29: 
30: //! The shifts array allows for linear searching of multi-byte values. For each position, it determines the next
31: //! position given that we encounter a byte with the given value.
32: /*! For example, if we have a string "ABAC", the shifts array will have the following values:
33:  *  [0] --> ['A'] = 1, all others = 0
34:  *  [1] --> ['B'] = 2, ['A'] = 1, all others = 0
35:  *  [2] --> ['A'] = 3, all others = 0
36:  *  [3] --> ['C'] = 4 (match), 'B' = 2, 'A' = 1, all others = 0
37:  * Suppose we then search in the following string "ABABAC", our progression will be as follows:
38:  * 'A' -> [1], 'B' -> [2], 'A' -> [3], 'B' -> [2], 'A' -> [3], 'C' -> [4] (match!)
39:  */
40: struct TextSearchShiftArray {
41: 	TextSearchShiftArray();
42: 	explicit TextSearchShiftArray(string search_term);
43: 
44: 	inline bool Match(uint8_t &position, uint8_t byte_value) {
45: 		if (position >= length) {
46: 			return false;
47: 		}
48: 		position = shifts[position * 255 + byte_value];
49: 		return position == length;
50: 	}
51: 
52: 	idx_t length;
53: 	unique_ptr<uint8_t[]> shifts;
54: };
55: 
56: struct BufferedCSVReaderOptions {
57: 	//! The file path of the CSV file to read
58: 	string file_path;
59: 	//! Whether file is compressed or not, and if so which compression type
60: 	//! AUTO_DETECT (default; infer from file extension)
61: 	FileCompressionType compression = FileCompressionType::AUTO_DETECT;
62: 	//! Whether or not to automatically detect dialect and datatypes
63: 	bool auto_detect = false;
64: 	//! Whether or not a delimiter was defined by the user
65: 	bool has_delimiter = false;
66: 	//! Delimiter to separate columns within each line
67: 	string delimiter = ",";
68: 	//! Whether or not a quote sign was defined by the user
69: 	bool has_quote = false;
70: 	//! Quote used for columns that contain reserved characters, e.g., delimiter
71: 	string quote = "\"";
72: 	//! Whether or not an escape character was defined by the user
73: 	bool has_escape = false;
74: 	//! Escape character to escape quote character
75: 	string escape;
76: 	//! Whether or not a header information was given by the user
77: 	bool has_header = false;
78: 	//! Whether or not the file has a header line
79: 	bool header = false;
80: 	//! Whether or not header names shall be normalized
81: 	bool normalize_names = false;
82: 	//! How many leading rows to skip
83: 	idx_t skip_rows = 0;
84: 	//! Expected number of columns
85: 	idx_t num_cols = 0;
86: 	//! Specifies the string that represents a null value
87: 	string null_str;
88: 	//! True, if column with that index must skip null check
89: 	vector<bool> force_not_null;
90: 	//! Size of sample chunk used for dialect and type detection
91: 	idx_t sample_chunk_size = STANDARD_VECTOR_SIZE;
92: 	//! Number of sample chunks used for type detection
93: 	idx_t sample_chunks = 10;
94: 	//! Number of samples to buffer
95: 	idx_t buffer_size = STANDARD_VECTOR_SIZE * 100;
96: 	//! Consider all columns to be of type varchar
97: 	bool all_varchar = false;
98: 	//! The date format to use (if any is specified)
99: 	std::map<LogicalTypeId, StrpTimeFormat> date_format = {{LogicalTypeId::DATE, {}}, {LogicalTypeId::TIMESTAMP, {}}};
100: 	//! Whether or not a type format is specified
101: 	std::map<LogicalTypeId, bool> has_format = {{LogicalTypeId::DATE, false}, {LogicalTypeId::TIMESTAMP, false}};
102: 
103: 	void SetDelimiter(const string &delimiter);
104: 
105: 	std::string ToString() const;
106: };
107: 
108: enum class ParserMode : uint8_t { PARSING = 0, SNIFFING_DIALECT = 1, SNIFFING_DATATYPES = 2, PARSING_HEADER = 3 };
109: 
110: //! Buffered CSV reader is a class that reads values from a stream and parses them as a CSV file
111: class BufferedCSVReader {
112: 	//! Initial buffer read size; can be extended for long lines
113: 	static constexpr idx_t INITIAL_BUFFER_SIZE = 16384;
114: 	//! Maximum CSV line size: specified because if we reach this amount, we likely have the wrong delimiters
115: 	static constexpr idx_t MAXIMUM_CSV_LINE_SIZE = 1048576;
116: 	ParserMode mode;
117: 
118: public:
119: 	BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options,
120: 	                  const vector<LogicalType> &requested_types = vector<LogicalType>());
121: 
122: 	BufferedCSVReader(FileSystem &fs, FileOpener *opener, BufferedCSVReaderOptions options,
123: 	                  const vector<LogicalType> &requested_types = vector<LogicalType>());
124: 	~BufferedCSVReader();
125: 
126: 	FileSystem &fs;
127: 	FileOpener *opener;
128: 	BufferedCSVReaderOptions options;
129: 	vector<LogicalType> sql_types;
130: 	vector<string> col_names;
131: 	unique_ptr<CSVFileHandle> file_handle;
132: 
133: 	unique_ptr<char[]> buffer;
134: 	idx_t buffer_size;
135: 	idx_t position;
136: 	idx_t start = 0;
137: 
138: 	idx_t linenr = 0;
139: 	bool linenr_estimated = false;
140: 
141: 	vector<idx_t> sniffed_column_counts;
142: 	bool row_empty = false;
143: 	idx_t sample_chunk_idx = 0;
144: 	bool jumping_samples = false;
145: 	bool end_of_file_reached = false;
146: 	bool bom_checked = false;
147: 
148: 	idx_t bytes_in_chunk = 0;
149: 	double bytes_per_line_avg = 0;
150: 
151: 	vector<unique_ptr<char[]>> cached_buffers;
152: 
153: 	TextSearchShiftArray delimiter_search, escape_search, quote_search;
154: 
155: 	DataChunk parse_chunk;
156: 
157: 	std::queue<unique_ptr<DataChunk>> cached_chunks;
158: 
159: public:
160: 	//! Extract a single DataChunk from the CSV file and stores it in insert_chunk
161: 	void ParseCSV(DataChunk &insert_chunk);
162: 
163: 	idx_t GetFileSize();
164: 
165: private:
166: 	//! Initialize Parser
167: 	void Initialize(const vector<LogicalType> &requested_types);
168: 	//! Initializes the parse_chunk with varchar columns and aligns info with new number of cols
169: 	void InitParseChunk(idx_t num_cols);
170: 	//! Initializes the TextSearchShiftArrays for complex parser
171: 	void PrepareComplexParser();
172: 	//! Try to parse a single datachunk from the file. Throws an exception if anything goes wrong.
173: 	void ParseCSV(ParserMode mode);
174: 	//! Try to parse a single datachunk from the file. Returns whether or not the parsing is successful
175: 	bool TryParseCSV(ParserMode mode);
176: 	//! Extract a single DataChunk from the CSV file and stores it in insert_chunk
177: 	bool TryParseCSV(ParserMode mode, DataChunk &insert_chunk, string &error_message);
178: 	//! Sniffs CSV dialect and determines skip rows, header row, column types and column names
179: 	vector<LogicalType> SniffCSV(const vector<LogicalType> &requested_types);
180: 	//! Change the date format for the type to the string
181: 	void SetDateFormat(const string &format_specifier, const LogicalTypeId &sql_type);
182: 	//! Try to cast a string value to the specified sql type
183: 	bool TryCastValue(const Value &value, const LogicalType &sql_type);
184: 	//! Try to cast a vector of values to the specified sql type
185: 	bool TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type);
186: 	//! Skips skip_rows, reads header row from input stream
187: 	void SkipRowsAndReadHeader(idx_t skip_rows, bool skip_header);
188: 	//! Jumps back to the beginning of input stream and resets necessary internal states
189: 	void JumpToBeginning(idx_t skip_rows, bool skip_header);
190: 	//! Jumps back to the beginning of input stream and resets necessary internal states
191: 	bool JumpToNextSample();
192: 	//! Resets the buffer
193: 	void ResetBuffer();
194: 	//! Resets the steam
195: 	void ResetStream();
196: 
197: 	//! Parses a CSV file with a one-byte delimiter, escape and quote character
198: 	bool TryParseSimpleCSV(DataChunk &insert_chunk, string &error_message);
199: 	//! Parses more complex CSV files with multi-byte delimiters, escapes or quotes
200: 	bool TryParseComplexCSV(DataChunk &insert_chunk, string &error_message);
201: 
202: 	//! Adds a value to the current row
203: 	void AddValue(char *str_val, idx_t length, idx_t &column, vector<idx_t> &escape_positions);
204: 	//! Adds a row to the insert_chunk, returns true if the chunk is filled as a result of this row being added
205: 	bool AddRow(DataChunk &insert_chunk, idx_t &column);
206: 	//! Finalizes a chunk, parsing all values that have been added so far and adding them to the insert_chunk
207: 	void Flush(DataChunk &insert_chunk);
208: 	//! Reads a new buffer from the CSV file if the current one has been exhausted
209: 	bool ReadBuffer(idx_t &start);
210: 
211: 	unique_ptr<CSVFileHandle> OpenCSV(const BufferedCSVReaderOptions &options);
212: 
213: 	//! First phase of auto detection: detect CSV dialect (i.e. delimiter, quote rules, etc)
214: 	void DetectDialect(const vector<LogicalType> &requested_types, BufferedCSVReaderOptions &original_options,
215: 	                   vector<BufferedCSVReaderOptions> &info_candidates, idx_t &best_num_cols);
216: 	//! Second phase of auto detection: detect candidate types for each column
217: 	void DetectCandidateTypes(const vector<LogicalType> &type_candidates,
218: 	                          const map<LogicalTypeId, vector<const char *>> &format_template_candidates,
219: 	                          const vector<BufferedCSVReaderOptions> &info_candidates,
220: 	                          BufferedCSVReaderOptions &original_options, idx_t best_num_cols,
221: 	                          vector<vector<LogicalType>> &best_sql_types_candidates,
222: 	                          std::map<LogicalTypeId, vector<string>> &best_format_candidates,
223: 	                          DataChunk &best_header_row);
224: 	//! Third phase of auto detection: detect header of CSV file
225: 	void DetectHeader(const vector<vector<LogicalType>> &best_sql_types_candidates, const DataChunk &best_header_row);
226: 	//! Fourth phase of auto detection: refine the types of each column and select which types to use for each column
227: 	vector<LogicalType> RefineTypeDetection(const vector<LogicalType> &type_candidates,
228: 	                                        const vector<LogicalType> &requested_types,
229: 	                                        vector<vector<LogicalType>> &best_sql_types_candidates,
230: 	                                        map<LogicalTypeId, vector<string>> &best_format_candidates);
231: };
232: 
233: } // namespace duckdb
[end of src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: