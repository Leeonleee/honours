{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 1773,
  "instance_id": "ClickHouse__ClickHouse-1773",
  "issue_numbers": [
    "1553"
  ],
  "base_commit": "07931cad69f1d4415cd64c3095acc164c462091e",
  "patch": "diff --git a/dbms/src/Interpreters/Context.cpp b/dbms/src/Interpreters/Context.cpp\nindex d43a1c7b916b..394a4d23c9f0 100644\n--- a/dbms/src/Interpreters/Context.cpp\n+++ b/dbms/src/Interpreters/Context.cpp\n@@ -1348,15 +1348,21 @@ std::shared_ptr<Cluster> Context::tryGetCluster(const std::string & cluster_name\n }\n \n \n+void Context::reloadClusterConfig()\n+{\n+    std::lock_guard<std::mutex> lock(shared->clusters_mutex);\n+    auto & config = shared->clusters_config ? *shared->clusters_config : getConfigRef();\n+    shared->clusters = std::make_unique<Clusters>(config, settings);\n+}\n+\n+\n Clusters & Context::getClusters() const\n {\n+    std::lock_guard<std::mutex> lock(shared->clusters_mutex);\n+    if (!shared->clusters)\n     {\n-        std::lock_guard<std::mutex> lock(shared->clusters_mutex);\n-        if (!shared->clusters)\n-        {\n-            auto & config = shared->clusters_config ? *shared->clusters_config : getConfigRef();\n-            shared->clusters = std::make_unique<Clusters>(config, settings);\n-        }\n+        auto & config = shared->clusters_config ? *shared->clusters_config : getConfigRef();\n+        shared->clusters = std::make_unique<Clusters>(config, settings);\n     }\n \n     return *shared->clusters;\ndiff --git a/dbms/src/Interpreters/Context.h b/dbms/src/Interpreters/Context.h\nindex 33e43c03df8a..374c48b1fd66 100644\n--- a/dbms/src/Interpreters/Context.h\n+++ b/dbms/src/Interpreters/Context.h\n@@ -318,6 +318,7 @@ class Context\n     Clusters & getClusters() const;\n     std::shared_ptr<Cluster> getCluster(const std::string & cluster_name) const;\n     std::shared_ptr<Cluster> tryGetCluster(const std::string & cluster_name) const;\n+    void reloadClusterConfig();\n     void setClustersConfig(const ConfigurationPtr & config);\n \n     Compiler & getCompiler();\ndiff --git a/dbms/src/Interpreters/InterpreterSystemQuery.cpp b/dbms/src/Interpreters/InterpreterSystemQuery.cpp\nindex 60888c4f1a55..419a4937a72a 100644\n--- a/dbms/src/Interpreters/InterpreterSystemQuery.cpp\n+++ b/dbms/src/Interpreters/InterpreterSystemQuery.cpp\n@@ -75,6 +75,8 @@ BlockIO InterpreterSystemQuery::execute()\n             break;\n         case Type::DROP_DNS_CACHE:\n             DNSCache::instance().drop();\n+            /// Reinitialize clusters to update their resolved_addresses\n+            context.reloadClusterConfig();\n             break;\n         case Type::DROP_MARK_CACHE:\n             context.dropMarkCache();\ndiff --git a/dbms/src/Server/ConfigReloader.cpp b/dbms/src/Server/ConfigReloader.cpp\nindex f032b8c4c516..1b841b2ba95f 100644\n--- a/dbms/src/Server/ConfigReloader.cpp\n+++ b/dbms/src/Server/ConfigReloader.cpp\n@@ -53,11 +53,18 @@ void ConfigReloader::run()\n \n     while (true)\n     {\n-        bool zk_changed = zk_node_cache.getChangedEvent().tryWait(std::chrono::milliseconds(reload_interval).count());\n-        if (quit)\n-            return;\n+        try\n+        {\n+            bool zk_changed = zk_node_cache.getChangedEvent().tryWait(std::chrono::milliseconds(reload_interval).count());\n+            if (quit)\n+                return;\n \n-        reloadIfNewer(zk_changed, /* throw_on_error = */ false, /* fallback_to_preprocessed = */ false);\n+            reloadIfNewer(zk_changed, /* throw_on_error = */ false, /* fallback_to_preprocessed = */ false);\n+        }\n+        catch (...)\n+        {\n+            tryLogCurrentException(log, __PRETTY_FUNCTION__);\n+        }\n     }\n }\n \ndiff --git a/dbms/src/Server/LocalServer.cpp b/dbms/src/Server/LocalServer.cpp\nindex e61fed500038..eee209eafd0c 100644\n--- a/dbms/src/Server/LocalServer.cpp\n+++ b/dbms/src/Server/LocalServer.cpp\n@@ -4,6 +4,8 @@\n #include <Poco/Util/HelpFormatter.h>\n #include <Poco/Util/OptionCallback.h>\n #include <Poco/String.h>\n+#include <Poco/Logger.h>\n+#include <Poco/NullChannel.h>\n #include <Databases/DatabaseOrdinary.h>\n #include <Storages/System/attachSystemTables.h>\n #include <Interpreters/Context.h>\n@@ -49,6 +51,13 @@ LocalServer::~LocalServer()\n void LocalServer::initialize(Poco::Util::Application & self)\n {\n     Poco::Util::Application::initialize(self);\n+\n+    // Turn off server logging to stderr\n+    if (config().has(\"silent\"))\n+    {\n+        Poco::Logger::root().setLevel(\"none\");\n+        Poco::Logger::root().setChannel(Poco::AutoPtr<Poco::NullChannel>(new Poco::NullChannel()));\n+    }\n }\n \n \n@@ -66,14 +75,21 @@ void LocalServer::defineOptions(Poco::Util::OptionSet& _options)\n     /// Arguments that define first query creating initial table:\n     /// (If structure argument is omitted then initial query is not generated)\n     _options.addOption(\n-        Poco::Util::Option(\"structure\", \"S\", \"Structe of initial table(list columns names with their types)\")\n+        Poco::Util::Option(\"structure\", \"S\", \"Structure of initial table(list columns names with their types)\")\n             .required(false)\n             .repeatable(false)\n             .argument(\"[name Type]\")\n             .binding(\"table-structure\"));\n \n+    /// Turn off logging\n+    _options.addOption(\n+        Poco::Util::Option(\"silent\", \"s\", \"Quiet mode, print only errors\")\n+            .required(false)\n+            .repeatable(false)\n+            .binding(\"silent\"));\n+\n     _options.addOption(\n-        Poco::Util::Option(\"table\", \"N\", \"Name of intial table\")\n+        Poco::Util::Option(\"table\", \"N\", \"Name of initial table\")\n             .required(false)\n             .repeatable(false)\n             .argument(\"[table]\")\n@@ -87,10 +103,10 @@ void LocalServer::defineOptions(Poco::Util::OptionSet& _options)\n             .binding(\"table-file\"));\n \n     _options.addOption(\n-        Poco::Util::Option(\"input-format\", \"if\", \"Input format of intial table data\")\n+        Poco::Util::Option(\"input-format\", \"if\", \"Input format of initial table data\")\n             .required(false)\n             .repeatable(false)\n-            .argument(\"[TSV]\")\n+            .argument(\"<TSV>\")\n             .binding(\"table-data-format\"));\n \n     /// List of queries to execute\n@@ -98,7 +114,7 @@ void LocalServer::defineOptions(Poco::Util::OptionSet& _options)\n         Poco::Util::Option(\"query\", \"q\", \"Queries to execute\")\n             .required(false)\n             .repeatable(false)\n-            .argument(\"<query>\", true)\n+            .argument(\"<query>\")\n             .binding(\"query\"));\n \n     /// Default Output format\n@@ -109,9 +125,9 @@ void LocalServer::defineOptions(Poco::Util::OptionSet& _options)\n             .argument(\"[TSV]\", true)\n             .binding(\"output-format\"));\n \n-    /// Alias for previous one, required for clickhouse-client compability\n+    /// Alias for previous one, required for clickhouse-client compatibility\n     _options.addOption(\n-        Poco::Util::Option(\"format\", \"\", \"Default ouput format\")\n+        Poco::Util::Option(\"format\", \"\", \"Default output format\")\n             .required(false)\n             .repeatable(false)\n             .argument(\"[TSV]\", true)\n@@ -304,7 +320,7 @@ try\n     /** Init dummy default DB\n       * NOTE: We force using isolated default database to avoid conflicts with default database from server enviroment\n       * Otherwise, metadata of temporary File(format, EXPLICIT_PATH) tables will pollute metadata/ directory;\n-      *  if such tables will not be dropped, clickhouse-server can not load them due to security reasons.\n+      *  if such tables will not be dropped, clickhouse-server will not be able to load them due to security reasons.\n       */\n     const std::string default_database = \"_local\";\n     context->addDatabase(default_database, std::make_shared<DatabaseMemory>(default_database));\ndiff --git a/dbms/src/Storages/MergeTree/MergeTreeDataPart.cpp b/dbms/src/Storages/MergeTree/MergeTreeDataPart.cpp\nindex cb279460d164..3c329cbc79d6 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeDataPart.cpp\n+++ b/dbms/src/Storages/MergeTree/MergeTreeDataPart.cpp\n@@ -759,7 +759,8 @@ void MergeTreeDataPart::loadRowsCount()\n \n             if (!(rows_count <= rows_approx && rows_approx < rows_count + storage.index_granularity))\n                 throw Exception(\n-                    \"Unexpected size of column \" + column.name + \": \" + toString(rows_count) + \" rows\",\n+                    \"Unexpected size of column \" + column.name + \": \" + toString(rows_count) + \" rows, expected \"\n+                    + toString(rows_approx) + \"+-\" + toString(storage.index_granularity) + \" rows according to the index\",\n                     ErrorCodes::LOGICAL_ERROR);\n \n             return;\ndiff --git a/dbms/src/Storages/StorageMergeTree.cpp b/dbms/src/Storages/StorageMergeTree.cpp\nindex 18a3faa7b609..d416d825a1e6 100644\n--- a/dbms/src/Storages/StorageMergeTree.cpp\n+++ b/dbms/src/Storages/StorageMergeTree.cpp\n@@ -444,6 +444,7 @@ void StorageMergeTree::clearColumnInPartition(const ASTPtr & partition, const Fi\n     for (auto & transaction : transactions)\n         transaction->commit();\n \n+    /// Recalculate columns size (not only for the modified column)\n     data.recalculateColumnSizes();\n }\n \ndiff --git a/dbms/src/Storages/StorageReplicatedMergeTree.cpp b/dbms/src/Storages/StorageReplicatedMergeTree.cpp\nindex 6ae8e9cf340a..6860f014fbc8 100644\n--- a/dbms/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/dbms/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -1467,6 +1467,7 @@ void StorageReplicatedMergeTree::executeClearColumnInPartition(const LogEntry &\n \n     LOG_DEBUG(log, \"Cleared column \" << entry.column_name << \" in \" << modified_parts << \" parts\");\n \n+    /// Recalculate columns size (not only for the modified column)\n     data.recalculateColumnSizes();\n }\n \n",
  "test_patch": "diff --git a/dbms/tests/integration/test_system_queries/configs/config.d/clusters_config.xml b/dbms/tests/integration/test_system_queries/configs/config.d/clusters_config.xml\nnew file mode 100644\nindex 000000000000..1353e2f74b5d\n--- /dev/null\n+++ b/dbms/tests/integration/test_system_queries/configs/config.d/clusters_config.xml\n@@ -0,0 +1,14 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <remote_servers>\n+    \t<lost_host_cluster>\n+    \t\t<shard>\n+    \t\t\t<internal_replication>false</internal_replication>\n+    \t\t\t<replica>\n+    \t\t\t\t<host>lost_host</host>\n+                \t<port>9000</port>\n+    \t\t\t</replica>\n+    \t\t</shard>\n+    \t</lost_host_cluster>\n+    </remote_servers>\n+</yandex>\ndiff --git a/dbms/tests/integration/test_system_queries/test.py b/dbms/tests/integration/test_system_queries/test.py\nindex b7bf350d3957..d19e2df67819 100644\n--- a/dbms/tests/integration/test_system_queries/test.py\n+++ b/dbms/tests/integration/test_system_queries/test.py\n@@ -9,6 +9,7 @@\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n from helpers.cluster import ClickHouseCluster\n from helpers.test_tools import TSV\n+from helpers.client import QueryRuntimeException\n \n \n @pytest.fixture(scope=\"module\")\n@@ -24,10 +25,12 @@ def started_cluster():\n         instance.query('CREATE DATABASE dictionaries ENGINE = Dictionary')\n         instance.query('CREATE TABLE dictionary_source (id UInt64, value UInt8) ENGINE = Memory')\n         #print instance.query('SELECT * FROM system.dictionaries FORMAT Vertical')\n+        print \"Started \", instance.ip_address\n \n         yield cluster\n \n     finally:\n+        pass\n         cluster.shutdown()\n \n \n@@ -54,13 +57,21 @@ def test_SYSTEM_RELOAD_DICTIONARY(started_cluster):\n def test_DROP_DNS_CACHE(started_cluster):\n     instance = cluster.instances['ch1']\n \n-    with pytest.raises(Exception):\n-        instance.query(\"SELECT * FROM remote('aperol', 'system', 'one')\")\n+    instance.exec_in_container(['bash', '-c', 'echo 127.255.255.255 lost_host > /etc/hosts'], privileged=True, user='root')\n \n-    instance.exec_in_container(['bash', '-c', 'echo 127.0.0.1 aperol >> /etc/hosts'], privileged=True, user='root')\n+    with pytest.raises(QueryRuntimeException):\n+        instance.query(\"SELECT * FROM remote('lost_host', 'system', 'one')\")\n+\n+    instance.query(\"CREATE TABLE distributed_lost_host (dummy UInt8) ENGINE = Distributed(lost_host_cluster, 'system', 'one')\")\n+    with pytest.raises(QueryRuntimeException):\n+        instance.query(\"SELECT * FROM distributed_lost_host\")\n+\n+    instance.exec_in_container(['bash', '-c', 'echo 127.0.0.1 lost_host > /etc/hosts'], privileged=True, user='root')\n     instance.query(\"SYSTEM DROP DNS CACHE\")\n \n-    instance.query(\"SELECT * FROM remote('aperol', 'system', 'one')\")\n+    instance.query(\"SELECT * FROM remote('lost_host', 'system', 'one')\")\n+    instance.query(\"SELECT * FROM distributed_lost_host\")\n+    assert TSV(instance.query(\"SELECT DISTINCT host_name, host_address FROM system.clusters\")) == TSV(\"lost_host\\t127.0.0.1\\n\")\n \n \n if __name__ == '__main__':\ndiff --git a/dbms/tests/perf_drafts/vert_merge/test_merges b/dbms/tests/perf_drafts/vert_merge/test_merges\nindex 7240ad9a41f3..61dbf639bce3 100755\n--- a/dbms/tests/perf_drafts/vert_merge/test_merges\n+++ b/dbms/tests/perf_drafts/vert_merge/test_merges\n@@ -22,8 +22,8 @@ table_name=\"ontime\"\n table=\"test.ontime\"\n \n function read_src_data {\n-\tclickhouse-local --file \"$SOURCE\" --input-format CSV --structure \"$STRUCT\" -of Native --query \"$@\" 2>/dev/null\n-\t#clickhouse-local --file \"$SOURCE\" --input-format Native --structure \"$STRUCT\" -of Native --query \"$@\" 2>/dev/null\n+\tclickhouse-local -s --file \"$SOURCE\" --input-format CSV --structure \"$STRUCT\" -of Native --query \"$@\"\n+\t#clickhouse-local -s --file \"$SOURCE\" --input-format Native --structure \"$STRUCT\" -of Native --query \"$@\"\n }\n \n function set_vertical_alg {\n@@ -71,7 +71,7 @@ function get_last_merge_time {\n }\n \n function total_merge_time_from_log {\n-    cat /var/log/clickhouse-server/clickhouse-server.log | grep \"(Merger): Merge sorted\" | cut -d \" \" -f 21 | clickhouse-local -S \"d Float64\" --query \"SELECT round(sum(d), 3) FROM table\" 2>/dev/null\n+    cat /var/log/clickhouse-server/clickhouse-server.log | grep \"(Merger): Merge sorted\" | cut -d \" \" -f 21 | clickhouse-local -s -S \"d Float64\" --query \"SELECT round(sum(d), 3) FROM table\"\n }\n \n function get_max_clickhouse_server_memory {\ndiff --git a/dbms/tests/queries/0_stateless/00113_shard_group_array.sql b/dbms/tests/queries/0_stateless/00113_shard_group_array.sql\nindex cfdb7b6b6fdb..858e7f82ba6a 100644\n--- a/dbms/tests/queries/0_stateless/00113_shard_group_array.sql\n+++ b/dbms/tests/queries/0_stateless/00113_shard_group_array.sql\n@@ -34,4 +34,4 @@ SELECT roundToExp2(number) AS k, length(groupArray(1)([hex(number)] AS i)), leng\n DROP TABLE test.numbers_mt;\n \n -- Check binary compability:\n--- clickhouse-client -h old -q \"SELECT arrayReduce('groupArrayState', [['1'], ['22'], ['333']]) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure \"d AggregateFunction(groupArray2, Array(String))\" -q \"SELECT groupArray2Merge(d) FROM table\"\n+-- clickhouse-client -h old -q \"SELECT arrayReduce('groupArrayState', [['1'], ['22'], ['333']]) FORMAT RowBinary\" | clickhouse-local -s --input-format RowBinary --structure \"d AggregateFunction(groupArray2, Array(String))\" -q \"SELECT groupArray2Merge(d) FROM table\"\ndiff --git a/dbms/tests/queries/0_stateless/00385_storage_file_and_clickhouse-local_app.sh b/dbms/tests/queries/0_stateless/00385_storage_file_and_clickhouse-local_app.sh\nindex 28b737a62b15..d20094b1e612 100755\n--- a/dbms/tests/queries/0_stateless/00385_storage_file_and_clickhouse-local_app.sh\n+++ b/dbms/tests/queries/0_stateless/00385_storage_file_and_clickhouse-local_app.sh\n@@ -20,8 +20,8 @@ function pack_unpack_compare()\n     local res_db_file=$(${CLICKHOUSE_CLIENT} --max_threads=1 --query \"SELECT $TABLE_HASH FROM test.buf_file\")\n \n     ${CLICKHOUSE_CLIENT} --max_threads=1 --query \"SELECT * FROM test.buf FORMAT $3\" > \"$buf_file\"\n-    local res_ch_local1=$(${CLICKHOUSE_LOCAL} --structure \"$2\" --file \"$buf_file\" --table \"my super table\" --input-format \"$3\" --output-format TabSeparated --query \"SELECT $TABLE_HASH FROM \\`my super table\\`\" 2>${CLICKHOUSE_TMP}/stderr || cat stderr 1>&2)\n-    local res_ch_local2=$(${CLICKHOUSE_LOCAL} --structure \"$2\" --table \"my super table\" --input-format \"$3\" --output-format TabSeparated --query \"SELECT $TABLE_HASH FROM \\`my super table\\`\" < \"$buf_file\" 2>${CLICKHOUSE_TMP}/stderr || cat ${CLICKHOUSE_TMP}/stderr 1>&2)\n+    local res_ch_local1=$(${CLICKHOUSE_LOCAL} -s --structure \"$2\" --file \"$buf_file\" --table \"my super table\" --input-format \"$3\" --output-format TabSeparated --query \"SELECT $TABLE_HASH FROM \\`my super table\\`\")\n+    local res_ch_local2=$(${CLICKHOUSE_LOCAL} -s --structure \"$2\" --table \"my super table\" --input-format \"$3\" --output-format TabSeparated --query \"SELECT $TABLE_HASH FROM \\`my super table\\`\" < \"$buf_file\")\n \n     ${CLICKHOUSE_CLIENT} --query \"DROP TABLE IF EXISTS test.buf\"\n     ${CLICKHOUSE_CLIENT} --query \"DROP TABLE IF EXISTS test.buf_file\"\n@@ -38,7 +38,7 @@ pack_unpack_compare \"SELECT name, is_aggregate FROM system.functions\" \"name Stri\n pack_unpack_compare \"SELECT name, is_aggregate FROM system.functions\" \"name String, is_aggregate UInt8\" \"Native\"\n pack_unpack_compare \"SELECT name, is_aggregate FROM system.functions\" \"name String, is_aggregate UInt8\" \"TSKV\"\n echo\n-${CLICKHOUSE_LOCAL} -q \"CREATE TABLE sophisticated_default\n+${CLICKHOUSE_LOCAL} -s -q \"CREATE TABLE sophisticated_default\n (\n     a UInt8 DEFAULT\n     (\n@@ -49,4 +49,7 @@ ${CLICKHOUSE_LOCAL} -q \"CREATE TABLE sophisticated_default\n         SELECT dummy+9 FROM system.one\n     ),\n     c UInt8\n-) ENGINE = Memory; SELECT count() FROM system.tables WHERE name='sophisticated_default';\" 2>/dev/null\n+) ENGINE = Memory; SELECT count() FROM system.tables WHERE name='sophisticated_default';\"\n+\n+# Help is not skipped\n+[[ `${CLICKHOUSE_LOCAL} -s --help 2>&1 | wc -l` > 100 ]]\n\\ No newline at end of file\ndiff --git a/dbms/tests/queries/0_stateless/00407_parsing_nulls.sh b/dbms/tests/queries/0_stateless/00407_parsing_nulls.sh\nindex 92e30aa10102..f4bd51e2a684 100755\n--- a/dbms/tests/queries/0_stateless/00407_parsing_nulls.sh\n+++ b/dbms/tests/queries/0_stateless/00407_parsing_nulls.sh\n@@ -5,26 +5,26 @@ set -e\n CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n . $CURDIR/../shell_config.sh\n \n-echo -ne '\\\\tHello\\t123\\t\\\\N\\n\\\\N\\t\\t2000-01-01 00:00:00\\n' | ${CLICKHOUSE_LOCAL} --input-format=TabSeparated --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne 'Hello,123,\\\\N\\n\\\\N,0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} --input-format=CSV --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne '\"\\\\Hello\",123,\\\\N\\n\"\\\\N\",0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} --input-format=CSV --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne '{\"s\" : null, \"x\" : 123}, {\"s\" : \"\\N\", \"t\":\"2000-01-01 00:00:00\"}' | ${CLICKHOUSE_LOCAL} --input-format=JSONEachRow --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo \"(NULL, 111, '2000-01-01 00:00:00'), ('\\N', NULL, NULL), ('a\\Nb', NULL, NULL)\" | ${CLICKHOUSE_LOCAL} --input-format=Values --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n+echo -ne '\\\\tHello\\t123\\t\\\\N\\n\\\\N\\t\\t2000-01-01 00:00:00\\n' | ${CLICKHOUSE_LOCAL} -s --input-format=TabSeparated --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne 'Hello,123,\\\\N\\n\\\\N,0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} -s --input-format=CSV --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne '\"\\\\Hello\",123,\\\\N\\n\"\\\\N\",0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} -s --input-format=CSV --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne '{\"s\" : null, \"x\" : 123}, {\"s\" : \"\\N\", \"t\":\"2000-01-01 00:00:00\"}' | ${CLICKHOUSE_LOCAL} -s --input-format=JSONEachRow --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo \"(NULL, 111, '2000-01-01 00:00:00'), ('\\N', NULL, NULL), ('a\\Nb', NULL, NULL)\" | ${CLICKHOUSE_LOCAL} -s --input-format=Values --output-format=TabSeparated --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n \n-echo -ne '\\\\tHello\\t123\\t\\\\N\\n\\\\N\\t\\t2000-01-01 00:00:00\\n' | ${CLICKHOUSE_LOCAL} --input-format=TabSeparated --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne 'Hello,123,\\\\N\\n\\\\N,0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} --input-format=CSV --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne '\"\\\\Hello\",123,\\\\N\\n\"\\\\N\",0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} --input-format=CSV --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne '{\"s\" : null, \"x\" : 123}, {\"s\" : \"\\N\", \"t\":\"2000-01-01 00:00:00\"}' | ${CLICKHOUSE_LOCAL} --input-format=JSONEachRow --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo \"(NULL, 111, '2000-01-01 00:00:00'), ('\\N', NULL, NULL), ('a\\Nb', NULL, NULL)\" | ${CLICKHOUSE_LOCAL} --input-format=Values --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n+echo -ne '\\\\tHello\\t123\\t\\\\N\\n\\\\N\\t\\t2000-01-01 00:00:00\\n' | ${CLICKHOUSE_LOCAL} -s --input-format=TabSeparated --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne 'Hello,123,\\\\N\\n\\\\N,0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} -s --input-format=CSV --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne '\"\\\\Hello\",123,\\\\N\\n\"\\\\N\",0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} -s --input-format=CSV --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne '{\"s\" : null, \"x\" : 123}, {\"s\" : \"\\N\", \"t\":\"2000-01-01 00:00:00\"}' | ${CLICKHOUSE_LOCAL} -s --input-format=JSONEachRow --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo \"(NULL, 111, '2000-01-01 00:00:00'), ('\\N', NULL, NULL), ('a\\Nb', NULL, NULL)\" | ${CLICKHOUSE_LOCAL} -s --input-format=Values --output-format=CSV --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n \n-echo -ne '\\\\tHello\\t123\\t\\\\N\\n\\\\N\\t\\t2000-01-01 00:00:00\\n' | ${CLICKHOUSE_LOCAL} --input-format=TabSeparated --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne 'Hello,123,\\\\N\\n\\\\N,0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} --input-format=CSV --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne '\"\\\\Hello\",123,\\\\N\\n\"\\\\N\",0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} --input-format=CSV --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne '{\"s\" : null, \"x\" : 123}, {\"s\" : \"\\N\", \"t\":\"2000-01-01 00:00:00\"}' | ${CLICKHOUSE_LOCAL} --input-format=JSONEachRow --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo \"(NULL, 111, '2000-01-01 00:00:00'), ('\\N', NULL, NULL), ('a\\Nb', NULL, NULL)\" | ${CLICKHOUSE_LOCAL} --input-format=Values --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n+echo -ne '\\\\tHello\\t123\\t\\\\N\\n\\\\N\\t\\t2000-01-01 00:00:00\\n' | ${CLICKHOUSE_LOCAL} -s --input-format=TabSeparated --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne 'Hello,123,\\\\N\\n\\\\N,0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} -s --input-format=CSV --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne '\"\\\\Hello\",123,\\\\N\\n\"\\\\N\",0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} -s --input-format=CSV --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne '{\"s\" : null, \"x\" : 123}, {\"s\" : \"\\N\", \"t\":\"2000-01-01 00:00:00\"}' | ${CLICKHOUSE_LOCAL} -s --input-format=JSONEachRow --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo \"(NULL, 111, '2000-01-01 00:00:00'), ('\\N', NULL, NULL), ('a\\Nb', NULL, NULL)\" | ${CLICKHOUSE_LOCAL} -s --input-format=Values --output-format=JSONEachRow --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n \n-echo -ne '\\\\tHello\\t123\\t\\\\N\\n\\\\N\\t\\t2000-01-01 00:00:00\\n' | ${CLICKHOUSE_LOCAL} --input-format=TabSeparated --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne 'Hello,123,\\\\N\\n\\\\N,0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} --input-format=CSV --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne '\"\\\\Hello\",123,\\\\N\\n\"\\\\N\",0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} --input-format=CSV --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo -ne '{\"s\" : null, \"x\" : 123}, {\"s\" : \"\\N\", \"t\":\"2000-01-01 00:00:00\"}' | ${CLICKHOUSE_LOCAL} --input-format=JSONEachRow --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n-echo \"(NULL, 111, '2000-01-01 00:00:00'), ('\\N', NULL, NULL), ('a\\Nb', NULL, NULL)\" | ${CLICKHOUSE_LOCAL} --input-format=Values --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\" 2>/dev/null\n+echo -ne '\\\\tHello\\t123\\t\\\\N\\n\\\\N\\t\\t2000-01-01 00:00:00\\n' | ${CLICKHOUSE_LOCAL} -s --input-format=TabSeparated --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne 'Hello,123,\\\\N\\n\\\\N,0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} -s --input-format=CSV --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne '\"\\\\Hello\",123,\\\\N\\n\"\\\\N\",0,\"2000-01-01 00:00:00\"' | ${CLICKHOUSE_LOCAL} -s --input-format=CSV --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo -ne '{\"s\" : null, \"x\" : 123}, {\"s\" : \"\\N\", \"t\":\"2000-01-01 00:00:00\"}' | ${CLICKHOUSE_LOCAL} -s --input-format=JSONEachRow --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\n+echo \"(NULL, 111, '2000-01-01 00:00:00'), ('\\N', NULL, NULL), ('a\\Nb', NULL, NULL)\" | ${CLICKHOUSE_LOCAL} -s --input-format=Values --output-format=Values --structure='s Nullable(String), x Nullable(UInt64), t Nullable(DateTime)' --query=\"SELECT * FROM table\"\ndiff --git a/dbms/tests/queries/0_stateless/00415_into_outfile.sh b/dbms/tests/queries/0_stateless/00415_into_outfile.sh\nindex 19641feca50d..ec7cef9dbbae 100755\n--- a/dbms/tests/queries/0_stateless/00415_into_outfile.sh\n+++ b/dbms/tests/queries/0_stateless/00415_into_outfile.sh\n@@ -27,7 +27,7 @@ perform \"bad_union_all\" \"SELECT 1, 2 INTO OUTFILE '${CLICKHOUSE_TMP}/test_into_o\n perform \"describe_table\" \"DESCRIBE TABLE system.one INTO OUTFILE '${CLICKHOUSE_TMP}/test_into_outfile_describe_table.out'\"\n \n echo \"performing test: clickhouse-local\"\n-echo -e '1\\t2' | ${CLICKHOUSE_LOCAL} --structure 'col1 UInt32, col2 UInt32' --query \"SELECT col1 + 1, col2 + 1 FROM table INTO OUTFILE '${CLICKHOUSE_TMP}/test_into_outfile_clickhouse-local.out'\" 2>/dev/null\n+echo -e '1\\t2' | ${CLICKHOUSE_LOCAL} -s --structure 'col1 UInt32, col2 UInt32' --query \"SELECT col1 + 1, col2 + 1 FROM table INTO OUTFILE '${CLICKHOUSE_TMP}/test_into_outfile_clickhouse-local.out'\"\n if [ \"$?\" -eq 0 ]; then\n     cat \"${CLICKHOUSE_TMP}/test_into_outfile_clickhouse-local.out\"\n else\ndiff --git a/dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.reference b/dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.reference\nindex b219c65b7ff6..8d08c18de60d 100644\n--- a/dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.reference\n+++ b/dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.reference\n@@ -1,7 +1,12 @@\n+===Ordinary case===\n+16\n 1\ta\n 2\tb\n 0\ta\n 2\tb\n+8\n+0\t0\n+===Replicated case===\n all\n 2000-01-01\t0\t\n 2000-01-01\t1\ta\ndiff --git a/dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.sql b/dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.sql\nindex 760b59cf6c39..0ace86c2e5e9 100644\n--- a/dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.sql\n+++ b/dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.sql\n@@ -1,15 +1,23 @@\n+SELECT '===Ordinary case===';\n+\n DROP TABLE IF EXISTS test.clear_column;\n CREATE TABLE test.clear_column (d Date, num Int64, str String) ENGINE = MergeTree(d, d, 8192);\n \n INSERT INTO test.clear_column VALUES ('2016-12-12', 1, 'a'), ('2016-11-12', 2, 'b');\n \n+SELECT data_uncompressed_bytes FROM system.columns WHERE (database = 'test') AND (table = 'clear_column') AND (name = 'num');\n+\n SELECT num, str FROM test.clear_column ORDER BY num;\n ALTER TABLE test.clear_column CLEAR COLUMN num IN PARTITION '201612';\n SELECT num, str FROM test.clear_column ORDER BY num;\n \n+SELECT data_uncompressed_bytes FROM system.columns WHERE (database = 'test') AND (table = 'clear_column') AND (name = 'num');\n+ALTER TABLE test.clear_column CLEAR COLUMN num IN PARTITION '201611';\n+SELECT data_compressed_bytes, data_uncompressed_bytes FROM system.columns WHERE (database = 'test') AND (table = 'clear_column') AND (name = 'num');\n+\n DROP TABLE test.clear_column;\n \n--- Replicated case\n+SELECT '===Replicated case===';\n \n DROP TABLE IF EXISTS test.clear_column1;\n DROP TABLE IF EXISTS test.clear_column2;\ndiff --git a/dbms/tests/queries/0_stateless/00512_fractional_time_zones.sh b/dbms/tests/queries/0_stateless/00512_fractional_time_zones.sh\nindex 4d7745e88921..2ac93a796c41 100755\n--- a/dbms/tests/queries/0_stateless/00512_fractional_time_zones.sh\n+++ b/dbms/tests/queries/0_stateless/00512_fractional_time_zones.sh\n@@ -3,6 +3,6 @@\n CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n . $CURDIR/../shell_config.sh\n \n-TZ=Europe/Moscow ${CLICKHOUSE_LOCAL} --query=\"SELECT toDateTime('1990-10-19 00:00:00')\" 2>/dev/null\n-TZ=Asia/Colombo ${CLICKHOUSE_LOCAL} --query=\"SELECT toDateTime('1990-10-19 00:00:00')\" 2>/dev/null\n-TZ=Asia/Kathmandu ${CLICKHOUSE_LOCAL} --query=\"SELECT toDateTime('1990-10-19 00:00:00')\" 2>/dev/null\n+TZ=Europe/Moscow ${CLICKHOUSE_LOCAL} -s --query=\"SELECT toDateTime('1990-10-19 00:00:00')\"\n+TZ=Asia/Colombo ${CLICKHOUSE_LOCAL} -s --query=\"SELECT toDateTime('1990-10-19 00:00:00')\"\n+TZ=Asia/Kathmandu ${CLICKHOUSE_LOCAL} -s --query=\"SELECT toDateTime('1990-10-19 00:00:00')\"\n",
  "problem_statement": "CLEAR COLUMN does not refresh sizes inside system.columns\nTest case (based on https://github.com/yandex/ClickHouse/blob/master/dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.sql)\r\n\r\n```\r\nDROP TABLE IF EXISTS test.clear_column;\r\nCREATE TABLE test.clear_column (d Date, num Int64, str String) ENGINE = MergeTree(d, d, 8192);\r\n\r\nSELECT data_compressed_bytes, data_uncompressed_bytes FROM system.columns WHERE (database = 'test') AND (table = 'clear_column') AND (name = 'str');\r\n// expect 0, 0\r\n\r\nINSERT INTO test.clear_column VALUES ('2016-11-12', 1, 'a');\r\n\r\nSELECT data_compressed_bytes, data_uncompressed_bytes FROM system.columns WHERE (database = 'test') AND (table = 'clear_column') AND (name = 'str');\r\n\r\n// expect: 28, 2\r\n\r\nINSERT INTO test.clear_column VALUES ('2016-12-12', 1, 'a');\r\n\r\nSELECT data_compressed_bytes, data_uncompressed_bytes FROM system.columns WHERE (database = 'test') AND (table = 'clear_column') AND (name = 'str');\r\n\r\n// expect: 56, 4\r\n\r\nALTER TABLE test.clear_column CLEAR COLUMN num IN PARTITION '201612';\r\n\r\nSELECT data_compressed_bytes, data_uncompressed_bytes FROM system.columns WHERE (database = 'test') AND (table = 'clear_column') AND (name = 'str');\r\n\r\n// expect: 28, 2 as before previous insert, currently have 56, 4\r\n\r\n```\n",
  "hints_text": "BTW: there is no documentation for CLEAR COLUMN xxx IN PARTITION yyyymm",
  "created_at": "2018-01-15T15:02:13Z",
  "modified_files": [
    "dbms/src/Interpreters/Context.cpp",
    "dbms/src/Interpreters/Context.h",
    "dbms/src/Interpreters/InterpreterSystemQuery.cpp",
    "dbms/src/Server/ConfigReloader.cpp",
    "dbms/src/Server/LocalServer.cpp",
    "dbms/src/Storages/MergeTree/MergeTreeDataPart.cpp",
    "dbms/src/Storages/StorageMergeTree.cpp",
    "dbms/src/Storages/StorageReplicatedMergeTree.cpp"
  ],
  "modified_test_files": [
    "b/dbms/tests/integration/test_system_queries/configs/config.d/clusters_config.xml",
    "dbms/tests/integration/test_system_queries/test.py",
    "dbms/tests/perf_drafts/vert_merge/test_merges",
    "dbms/tests/queries/0_stateless/00113_shard_group_array.sql",
    "dbms/tests/queries/0_stateless/00385_storage_file_and_clickhouse-local_app.sh",
    "dbms/tests/queries/0_stateless/00407_parsing_nulls.sh",
    "dbms/tests/queries/0_stateless/00415_into_outfile.sh",
    "dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.reference",
    "dbms/tests/queries/0_stateless/00446_clear_column_in_partition_zookeeper.sql",
    "dbms/tests/queries/0_stateless/00512_fractional_time_zones.sh"
  ]
}