diff --git a/programs/diagnostics/cmd/params/params_test.go b/programs/diagnostics/cmd/params/params_test.go
deleted file mode 100644
index 7671506ba596..000000000000
--- a/programs/diagnostics/cmd/params/params_test.go
+++ /dev/null
@@ -1,247 +0,0 @@
-package params_test
-
-import (
-	"os"
-	"sort"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/cmd/params"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/spf13/cobra"
-	"github.com/stretchr/testify/require"
-)
-
-var conf = map[string]config.Configuration{
-	"config": {
-		Params: []config.ConfigParam{
-			config.StringParam{
-				Value:      "",
-				Param:      config.NewParam("directory", "A directory", false),
-				AllowEmpty: true,
-			},
-		},
-	},
-	"system": {
-		Params: []config.ConfigParam{
-			config.StringListParam{
-				// nil means include everything
-				Values: nil,
-				Param:  config.NewParam("include_tables", "Include tables", false),
-			},
-			config.StringListParam{
-				Values: []string{"distributed_ddl_queue", "query_thread_log", "query_log", "asynchronous_metric_log", "zookeeper"},
-				Param:  config.NewParam("exclude_tables", "Excluded tables", false),
-			},
-			config.IntParam{
-				Value: 100000,
-				Param: config.NewParam("row_limit", "Max rows", false),
-			},
-		},
-	},
-	"reader": {
-		Params: []config.ConfigParam{
-			config.StringOptions{
-				Value:   "csv",
-				Options: []string{"csv"},
-				Param:   config.NewParam("format", "Format of imported files", false),
-			},
-			config.BoolParam{
-				Value: true,
-				Param: config.NewParam("collect_archives", "Collect archives", false),
-			},
-		},
-	},
-}
-
-func TestNewParamMap(t *testing.T) {
-	// test each of the types via NewParamMap - one with each type. the keys here can represent anything e.g. a collector name
-	t.Run("test param map correctly converts types", func(t *testing.T) {
-		paramMap := params.NewParamMap(conf)
-		require.Len(t, paramMap, 3)
-		// check config
-		require.Contains(t, paramMap, "config")
-		require.Len(t, paramMap["config"], 1)
-		require.Contains(t, paramMap["config"], "directory")
-		require.IsType(t, params.CliParam{}, paramMap["config"]["directory"])
-		require.Equal(t, "A directory", paramMap["config"]["directory"].Description)
-		require.Equal(t, "", *(paramMap["config"]["directory"].Value.(*string)))
-		require.Equal(t, "", paramMap["config"]["directory"].Default)
-		require.Equal(t, params.String, paramMap["config"]["directory"].Type)
-		// check system
-		require.Contains(t, paramMap, "system")
-		require.Len(t, paramMap["system"], 3)
-		require.IsType(t, params.CliParam{}, paramMap["system"]["include_tables"])
-
-		require.Equal(t, "Include tables", paramMap["system"]["include_tables"].Description)
-		var value []string
-		require.Equal(t, &value, paramMap["system"]["include_tables"].Value)
-		require.Equal(t, value, paramMap["system"]["include_tables"].Default)
-		require.Equal(t, params.StringList, paramMap["system"]["include_tables"].Type)
-
-		require.Equal(t, "Excluded tables", paramMap["system"]["exclude_tables"].Description)
-		require.IsType(t, params.CliParam{}, paramMap["system"]["exclude_tables"])
-		require.Equal(t, &value, paramMap["system"]["exclude_tables"].Value)
-		require.Equal(t, []string{"distributed_ddl_queue", "query_thread_log", "query_log", "asynchronous_metric_log", "zookeeper"}, paramMap["system"]["exclude_tables"].Default)
-		require.Equal(t, params.StringList, paramMap["system"]["exclude_tables"].Type)
-
-		require.Equal(t, "Max rows", paramMap["system"]["row_limit"].Description)
-		require.IsType(t, params.CliParam{}, paramMap["system"]["row_limit"])
-		var iValue int64
-		require.Equal(t, &iValue, paramMap["system"]["row_limit"].Value)
-		require.Equal(t, int64(100000), paramMap["system"]["row_limit"].Default)
-		require.Equal(t, params.Integer, paramMap["system"]["row_limit"].Type)
-
-		// check reader
-		require.Contains(t, paramMap, "reader")
-		require.Len(t, paramMap["reader"], 2)
-		require.IsType(t, params.CliParam{}, paramMap["reader"]["format"])
-		require.Equal(t, "Format of imported files", paramMap["reader"]["format"].Description)
-		require.IsType(t, params.CliParam{}, paramMap["reader"]["format"])
-		oValue := params.StringOptionsVar{
-			Options: []string{"csv"},
-			Value:   "csv",
-		}
-		require.Equal(t, &oValue, paramMap["reader"]["format"].Value)
-		require.Equal(t, "csv", paramMap["reader"]["format"].Default)
-		require.Equal(t, params.StringOptionsList, paramMap["reader"]["format"].Type)
-
-		require.IsType(t, params.CliParam{}, paramMap["reader"]["collect_archives"])
-		require.Equal(t, "Collect archives", paramMap["reader"]["collect_archives"].Description)
-		require.IsType(t, params.CliParam{}, paramMap["reader"]["collect_archives"])
-		var bVar bool
-		require.Equal(t, &bVar, paramMap["reader"]["collect_archives"].Value)
-		require.Equal(t, true, paramMap["reader"]["collect_archives"].Default)
-		require.Equal(t, params.Boolean, paramMap["reader"]["collect_archives"].Type)
-
-	})
-
-}
-
-//  test GetConfigParam
-func TestConvertParamsToConfig(t *testing.T) {
-	paramMap := params.NewParamMap(conf)
-	t.Run("test we can convert a param map back to a config", func(t *testing.T) {
-		cParam := params.ConvertParamsToConfig(paramMap)
-		// these will not be equal as we have some information loss e.g. allowEmpty
-		//require.Equal(t, conf, cParam)
-		// deep equality
-		for name := range conf {
-			require.Equal(t, len(conf[name].Params), len(cParam[name].Params))
-			// sort both consistently
-			sort.Slice(conf[name].Params, func(i, j int) bool {
-				return conf[name].Params[i].Name() < conf[name].Params[j].Name()
-			})
-			sort.Slice(cParam[name].Params, func(i, j int) bool {
-				return cParam[name].Params[i].Name() < cParam[name].Params[j].Name()
-			})
-			for i, param := range conf[name].Params {
-				require.Equal(t, param.Required(), cParam[name].Params[i].Required())
-				require.Equal(t, param.Name(), cParam[name].Params[i].Name())
-				require.Equal(t, param.Description(), cParam[name].Params[i].Description())
-			}
-		}
-	})
-}
-
-// create via NewParamMap and add to command AddParamMapToCmd - check contents
-func TestAddParamMapToCmd(t *testing.T) {
-	paramMap := params.NewParamMap(conf)
-	t.Run("test we can add hidden params to a command", func(t *testing.T) {
-		testComand := &cobra.Command{
-			Use:   "test",
-			Short: "Run a test",
-			Long:  `Longer description`,
-			Run: func(cmd *cobra.Command, args []string) {
-				os.Exit(0)
-			},
-		}
-		params.AddParamMapToCmd(paramMap, testComand, "collector", true)
-		// check we get an error on one which doesn't exist
-		_, err := testComand.Flags().GetString("collector.config.random")
-		require.NotNil(t, err)
-		// check getting incorrect type
-		_, err = testComand.Flags().GetString("collector.system.include_tables")
-		require.NotNil(t, err)
-
-		// check existence of all flags
-		directory, err := testComand.Flags().GetString("collector.config.directory")
-		require.Nil(t, err)
-		require.Equal(t, "", directory)
-
-		includeTables, err := testComand.Flags().GetStringSlice("collector.system.include_tables")
-		require.Nil(t, err)
-		require.Equal(t, []string{}, includeTables)
-
-		excludeTables, err := testComand.Flags().GetStringSlice("collector.system.exclude_tables")
-		require.Nil(t, err)
-		require.Equal(t, []string{"distributed_ddl_queue", "query_thread_log", "query_log", "asynchronous_metric_log", "zookeeper"}, excludeTables)
-
-		rowLimit, err := testComand.Flags().GetInt64("collector.system.row_limit")
-		require.Nil(t, err)
-		require.Equal(t, int64(100000), rowLimit)
-
-		format, err := testComand.Flags().GetString("collector.reader.format")
-		require.Nil(t, err)
-		require.Equal(t, "csv", format)
-
-		collectArchives, err := testComand.Flags().GetBool("collector.reader.collect_archives")
-		require.Nil(t, err)
-		require.Equal(t, true, collectArchives)
-	})
-}
-
-// test StringOptionsVar
-func TestStringOptionsVar(t *testing.T) {
-
-	t.Run("test we can set", func(t *testing.T) {
-		format := params.StringOptionsVar{
-			Options: []string{"csv", "tsv", "native"},
-			Value:   "csv",
-		}
-		require.Equal(t, "csv", format.String())
-		err := format.Set("tsv")
-		require.Nil(t, err)
-		require.Equal(t, "tsv", format.String())
-	})
-
-	t.Run("test set invalid", func(t *testing.T) {
-		format := params.StringOptionsVar{
-			Options: []string{"csv", "tsv", "native"},
-			Value:   "csv",
-		}
-		require.Equal(t, "csv", format.String())
-		err := format.Set("random")
-		require.NotNil(t, err)
-		require.Equal(t, "random is not included in options: [csv tsv native]", err.Error())
-	})
-}
-
-// test StringSliceOptionsVar
-func TestStringSliceOptionsVar(t *testing.T) {
-
-	t.Run("test we can set", func(t *testing.T) {
-		formats := params.StringSliceOptionsVar{
-			Options: []string{"csv", "tsv", "native", "qsv"},
-			Values:  []string{"csv", "tsv"},
-		}
-		require.Equal(t, "[csv,tsv]", formats.String())
-		err := formats.Set("tsv,native")
-		require.Nil(t, err)
-		require.Equal(t, "[tsv,native]", formats.String())
-	})
-
-	t.Run("test set invalid", func(t *testing.T) {
-		formats := params.StringSliceOptionsVar{
-			Options: []string{"csv", "tsv", "native", "qsv"},
-			Values:  []string{"csv", "tsv"},
-		}
-		require.Equal(t, "[csv,tsv]", formats.String())
-		err := formats.Set("tsv,random")
-		require.NotNil(t, err)
-		require.Equal(t, "[random] are not included in options: [csv tsv native qsv]", err.Error())
-		err = formats.Set("msv,random")
-		require.NotNil(t, err)
-		require.Equal(t, "[msv random] are not included in options: [csv tsv native qsv]", err.Error())
-	})
-
-}
diff --git a/programs/diagnostics/internal/collectors/clickhouse/config_test.go b/programs/diagnostics/internal/collectors/clickhouse/config_test.go
deleted file mode 100644
index 355cbb65620b..000000000000
--- a/programs/diagnostics/internal/collectors/clickhouse/config_test.go
+++ /dev/null
@@ -1,128 +0,0 @@
-package clickhouse_test
-
-import (
-	"encoding/xml"
-	"fmt"
-	"io"
-	"os"
-	"path"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/stretchr/testify/require"
-)
-
-func TestConfigConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned for config collector", func(t *testing.T) {
-		configCollector := clickhouse.NewConfigCollector(&platform.ResourceManager{})
-		conf := configCollector.Configuration()
-		require.Len(t, conf.Params, 1)
-		// check first param
-		require.IsType(t, config.StringParam{}, conf.Params[0])
-		directory, ok := conf.Params[0].(config.StringParam)
-		require.True(t, ok)
-		require.False(t, directory.Required())
-		require.Equal(t, directory.Name(), "directory")
-		require.Equal(t, "", directory.Value)
-	})
-}
-
-func TestConfigCollect(t *testing.T) {
-	configCollector := clickhouse.NewConfigCollector(&platform.ResourceManager{})
-
-	t.Run("test default file collector configuration", func(t *testing.T) {
-		diagSet, err := configCollector.Collect(config.Configuration{})
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		// we won't be able to collect the default configs preprocessed and default - even if clickhouse is installed
-		// these directories should not be readable under any permissions these tests are unrealistically executed!
-		// note: we may also pick up configs from a local clickhouse process - we thus allow a len >=2 but don't check this
-		// as its non-deterministic
-		require.GreaterOrEqual(t, len(diagSet.Frames), 2)
-		// check default key
-		require.Contains(t, diagSet.Frames, "default")
-		require.Equal(t, diagSet.Frames["default"].Name(), "/etc/clickhouse-server/")
-		require.Equal(t, diagSet.Frames["default"].Columns(), []string{"config"})
-		// collection will have failed
-		checkFrame(t, diagSet.Frames["default"], nil)
-		// check preprocessed key
-		require.Contains(t, diagSet.Frames, "preprocessed")
-		require.Equal(t, diagSet.Frames["preprocessed"].Name(), "/var/lib/clickhouse/preprocessed_configs")
-		require.Equal(t, diagSet.Frames["preprocessed"].Columns(), []string{"config"})
-		// min of 2 - might be more if a local installation of clickhouse is running
-		require.GreaterOrEqual(t, len(diagSet.Errors.Errors), 2)
-	})
-
-	t.Run("test configuration when specified", func(t *testing.T) {
-		// create some test files
-		tempDir := t.TempDir()
-		confDir := path.Join(tempDir, "conf")
-		// create an includes file
-		includesDir := path.Join(tempDir, "includes")
-		err := os.MkdirAll(includesDir, os.ModePerm)
-		require.Nil(t, err)
-		includesPath := path.Join(includesDir, "random.xml")
-		includesFile, err := os.Create(includesPath)
-		require.Nil(t, err)
-		xmlWriter := io.Writer(includesFile)
-		enc := xml.NewEncoder(xmlWriter)
-		enc.Indent("  ", "    ")
-		xmlConfig := data.XmlConfig{
-			XMLName: xml.Name{},
-			Clickhouse: data.XmlLoggerConfig{
-				XMLName:  xml.Name{},
-				ErrorLog: "/var/log/clickhouse-server/clickhouse-server.err.log",
-				Log:      "/var/log/clickhouse-server/clickhouse-server.log",
-			},
-			IncludeFrom: "",
-		}
-		err = enc.Encode(xmlConfig)
-		require.Nil(t, err)
-		// create 5 temporary config files - length is 6 for the included file
-		rows := make([][]interface{}, 6)
-		for i := 0; i < 5; i++ {
-			if i == 4 {
-				// set the includes for the last doc
-				xmlConfig.IncludeFrom = includesPath
-			}
-			// we want to check hierarchies are walked so create a simple folder for each file
-			fileDir := path.Join(confDir, fmt.Sprintf("%d", i))
-			err := os.MkdirAll(fileDir, os.ModePerm)
-			require.Nil(t, err)
-			filepath := path.Join(fileDir, fmt.Sprintf("random-%d.xml", i))
-			row := make([]interface{}, 1)
-			row[0] = data.XmlConfigFile{Path: filepath}
-			rows[i] = row
-			xmlFile, err := os.Create(filepath)
-			require.Nil(t, err)
-			// write a little xml so its valid
-			xmlConfig := xmlConfig
-			xmlWriter := io.Writer(xmlFile)
-			enc := xml.NewEncoder(xmlWriter)
-			enc.Indent("  ", "    ")
-			err = enc.Encode(xmlConfig)
-			require.Nil(t, err)
-		}
-		diagSet, err := configCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value: confDir,
-					Param: config.NewParam("directory", "File locations", false),
-				},
-			},
-		})
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Frames, 1)
-		require.Contains(t, diagSet.Frames, "user_specified")
-		require.Equal(t, diagSet.Frames["user_specified"].Name(), confDir)
-		require.Equal(t, diagSet.Frames["user_specified"].Columns(), []string{"config"})
-		iConf := make([]interface{}, 1)
-		iConf[0] = data.XmlConfigFile{Path: includesPath, Included: true}
-		rows[5] = iConf
-		checkFrame(t, diagSet.Frames["user_specified"], rows)
-	})
-}
diff --git a/programs/diagnostics/internal/collectors/clickhouse/db_logs_test.go b/programs/diagnostics/internal/collectors/clickhouse/db_logs_test.go
deleted file mode 100644
index 3fc585f3352d..000000000000
--- a/programs/diagnostics/internal/collectors/clickhouse/db_logs_test.go
+++ /dev/null
@@ -1,119 +0,0 @@
-package clickhouse_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/stretchr/testify/require"
-)
-
-func TestDbLogsConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned for summary collector", func(t *testing.T) {
-		client := test.NewFakeClickhouseClient(make(map[string][]string))
-		dbLogsCollector := clickhouse.NewDBLogsCollector(&platform.ResourceManager{
-			DbClient: client,
-		})
-		conf := dbLogsCollector.Configuration()
-		require.Len(t, conf.Params, 1)
-		require.IsType(t, config.IntParam{}, conf.Params[0])
-		rowLimit, ok := conf.Params[0].(config.IntParam)
-		require.True(t, ok)
-		require.False(t, rowLimit.Required())
-		require.Equal(t, rowLimit.Name(), "row_limit")
-		require.Equal(t, int64(100000), rowLimit.Value)
-	})
-}
-
-func TestDbLogsCollect(t *testing.T) {
-	client := test.NewFakeClickhouseClient(make(map[string][]string))
-	dbLogsCollector := clickhouse.NewDBLogsCollector(&platform.ResourceManager{
-		DbClient: client,
-	})
-	queryLogColumns := []string{"type", "event_date", "event_time", "event_time_microseconds",
-		"query_start_time", "query_start_time_microseconds", "query_duration_ms", "read_rows", "read_bytes", "written_rows", "written_bytes",
-		"result_rows", "result_bytes", "memory_usage", "current_database", "query", "formatted_query", "normalized_query_hash",
-		"query_kind", "databases", "tables", "columns", "projections", "views", "exception_code", "exception", "stack_trace",
-		"is_initial_query", "user", "query_id", "address", "port", "initial_user", "initial_query_id", "initial_address", "initial_port",
-		"initial_query_start_time", "initial_query_start_time_microseconds", "interface", "os_user", "client_hostname", "client_name",
-		"client_revision", "client_version_major", "client_version_minor", "client_version_patch", "http_method", "http_user_agent",
-		"http_referer", "forwarded_for", "quota_key", "revision", "log_comment", "thread_ids", "ProfileEvents", "Settings",
-		"used_aggregate_functions", "used_aggregate_function_combinators", "used_database_engines", "used_data_type_families",
-		"used_dictionaries", "used_formats", "used_functions", "used_storages", "used_table_functions"}
-	queryLogFrame := test.NewFakeDataFrame("queryLog", queryLogColumns,
-		[][]interface{}{
-			{"QueryStart", "2021-12-13", "2021-12-13 12:53:20", "2021-12-13 12:53:20.590579", "2021-12-13 12:53:20", "2021-12-13 12:53:20.590579", "0", "0", "0", "0", "0", "0", "0", "0", "default", "SELECT DISTINCT arrayJoin(extractAll(name, '[\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.merge_tree_settings UNION ALL SELECT name FROM system.settings UNION ALL SELECT cluster FROM system.clusters UNION ALL SELECT macro FROM system.macros UNION ALL SELECT policy_name FROM system.storage_policies UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.dictionaries LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)", "", "6666026786019643712", "Select", "['system']", "['system.aggregate_function_combinators','system.clusters','system.columns','system.data_type_families','system.databases','system.dictionaries','system.formats','system.functions','system.macros','system.merge_tree_settings','system.settings','system.storage_policies','system.table_engines','system.table_functions','system.tables']", "['system.aggregate_function_combinators.name','system.clusters.cluster','system.columns.name','system.data_type_families.name','system.databases.name','system.dictionaries.name','system.formats.name','system.functions.is_aggregate','system.functions.name','system.macros.macro','system.merge_tree_settings.name','system.settings.name','system.storage_policies.policy_name','system.table_engines.name','system.table_functions.name','system.tables.name']", "[]", "[]", "0", "", "", "1", "default", "3b5feb6d-3086-4718-adb2-17464988ff12", "::ffff:127.0.0.1", "50920", "default", "3b5feb6d-3086-4718-adb2-17464988ff12", "::ffff:127.0.0.1", "50920", "2021-12-13 12:53:30", "2021-12-13 12:53:30.590579", "1", "", "", "ClickHouse client", "54450", "21", "11", "0", "0", "", "", "", "", "54456", "", "[]", "{}", "{'load_balancing':'random','max_memory_usage':'10000000000'}", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "[]"},
-			{"QueryFinish", "2021-12-13", "2021-12-13 12:53:30", "2021-12-13 12:53:30.607292", "2021-12-13 12:53:30", "2021-12-13 12:53:30.590579", "15", "4512", "255694", "0", "0", "4358", "173248", "4415230", "default", "SELECT DISTINCT arrayJoin(extractAll(name, '[\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.merge_tree_settings UNION ALL SELECT name FROM system.settings UNION ALL SELECT cluster FROM system.clusters UNION ALL SELECT macro FROM system.macros UNION ALL SELECT policy_name FROM system.storage_policies UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.dictionaries LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)", "", "6666026786019643712", "Select", "['system']", "['system.aggregate_function_combinators','system.clusters','system.columns','system.data_type_families','system.databases','system.dictionaries','system.formats','system.functions','system.macros','system.merge_tree_settings','system.settings','system.storage_policies','system.table_engines','system.table_functions','system.tables']", "['system.aggregate_function_combinators.name','system.clusters.cluster','system.columns.name','system.data_type_families.name','system.databases.name','system.dictionaries.name','system.formats.name','system.functions.is_aggregate','system.functions.name','system.macros.macro','system.merge_tree_settings.name','system.settings.name','system.storage_policies.policy_name','system.table_engines.name','system.table_functions.name','system.tables.name']", "[]", "[]", "0", "", "", "1", "default", "3b5feb6d-3086-4718-adb2-17464988ff12", "::ffff:127.0.0.1", "50920", "default", "3b5feb6d-3086-4718-adb2-17464988ff12", "::ffff:127.0.0.1", "50920", "2021-12-13 12:53:30", "2021-12-13 12:53:30.590579", "1", "", "", "ClickHouse client", "54450", "21", "11", "0", "0", "", "", "", "", "54456", "", "[95298,95315,95587,95316,95312,95589,95318,95586,95588,95585]", "{'Query':1,'SelectQuery':1,'ArenaAllocChunks':41,'ArenaAllocBytes':401408,'FunctionExecute':62,'NetworkSendElapsedMicroseconds':463,'NetworkSendBytes':88452,'SelectedRows':4512,'SelectedBytes':255694,'RegexpCreated':6,'ContextLock':411,'RWLockAcquiredReadLocks':190,'RealTimeMicroseconds':49221,'UserTimeMicroseconds':19811,'SystemTimeMicroseconds':2817,'SoftPageFaults':1128,'OSCPUWaitMicroseconds':127,'OSCPUVirtualTimeMicroseconds':22624,'OSWriteBytes':12288,'OSWriteChars':13312}", "{'load_balancing':'random','max_memory_usage':'10000000000'}", "[]", "[]", "[]", "[]", "[]", "[]", "['concat','notEmpty','extractAll']", "[]", "[]"},
-			{"QueryStart", "2021-12-13", "2021-12-13 13:02:53", "2021-12-13 13:02:53.419528", "2021-12-13 13:02:53", "2021-12-13 13:02:53.419528", "0", "0", "0", "0", "0", "0", "0", "0", "default", "SELECT DISTINCT arrayJoin(extractAll(name, '[\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.merge_tree_settings UNION ALL SELECT name FROM system.settings UNION ALL SELECT cluster FROM system.clusters UNION ALL SELECT macro FROM system.macros UNION ALL SELECT policy_name FROM system.storage_policies UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.dictionaries LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)", "", "6666026786019643712", "Select", "['system']", "['system.aggregate_function_combinators','system.clusters','system.columns','system.data_type_families','system.databases','system.dictionaries','system.formats','system.functions','system.macros','system.merge_tree_settings','system.settings','system.storage_policies','system.table_engines','system.table_functions','system.tables']", "['system.aggregate_function_combinators.name','system.clusters.cluster','system.columns.name','system.data_type_families.name','system.databases.name','system.dictionaries.name','system.formats.name','system.functions.is_aggregate','system.functions.name','system.macros.macro','system.merge_tree_settings.name','system.settings.name','system.storage_policies.policy_name','system.table_engines.name','system.table_functions.name','system.tables.name']", "[]", "[]", "0", "", "", "1", "default", "351b58e4-6128-47d4-a7b8-03d78c1f84c6", "::ffff:127.0.0.1", "50968", "default", "351b58e4-6128-47d4-a7b8-03d78c1f84c6", "::ffff:127.0.0.1", "50968", "2021-12-13 13:02:53", "2021-12-13 13:02:53.419528", "1", "", "", "ClickHouse client", "54450", "21", "11", "0", "0", "", "", "", "", "54456", "", "[]", "{}", "{'load_balancing':'random','max_memory_usage':'10000000000'}", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "[]", "[]"},
-			{"QueryFinish", "2021-12-13", "2021-12-13 13:02:56", "2021-12-13 13:02:56.437115", "2021-12-13 13:02:56", "2021-12-13 13:02:56.419528", "16", "4629", "258376", "0", "0", "4377", "174272", "4404694", "default", "SELECT DISTINCT arrayJoin(extractAll(name, '[\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.merge_tree_settings UNION ALL SELECT name FROM system.settings UNION ALL SELECT cluster FROM system.clusters UNION ALL SELECT macro FROM system.macros UNION ALL SELECT policy_name FROM system.storage_policies UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.dictionaries LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)", "", "6666026786019643712", "Select", "['system']", "['system.aggregate_function_combinators','system.clusters','system.columns','system.data_type_families','system.databases','system.dictionaries','system.formats','system.functions','system.macros','system.merge_tree_settings','system.settings','system.storage_policies','system.table_engines','system.table_functions','system.tables']", "['system.aggregate_function_combinators.name','system.clusters.cluster','system.columns.name','system.data_type_families.name','system.databases.name','system.dictionaries.name','system.formats.name','system.functions.is_aggregate','system.functions.name','system.macros.macro','system.merge_tree_settings.name','system.settings.name','system.storage_policies.policy_name','system.table_engines.name','system.table_functions.name','system.tables.name']", "[]", "[]", "0", "", "", "1", "default", "351b58e4-6128-47d4-a7b8-03d78c1f84c6", "::ffff:127.0.0.1", "50968", "default", "351b58e4-6128-47d4-a7b8-03d78c1f84c6", "::ffff:127.0.0.1", "50968", "2021-12-13 13:02:53", "2021-12-13 13:02:53.419528", "1", "", "", "ClickHouse client", "54450", "21", "11", "0", "0", "", "", "", "", "54456", "", "[95298,95318,95315,95316,95312,95588,95589,95586,95585,95587]", "{'Query':1,'SelectQuery':1,'ArenaAllocChunks':41,'ArenaAllocBytes':401408,'FunctionExecute':62,'NetworkSendElapsedMicroseconds':740,'NetworkSendBytes':88794,'SelectedRows':4629,'SelectedBytes':258376,'ContextLock':411,'RWLockAcquiredReadLocks':194,'RealTimeMicroseconds':52469,'UserTimeMicroseconds':17179,'SystemTimeMicroseconds':4218,'SoftPageFaults':569,'OSCPUWaitMicroseconds':303,'OSCPUVirtualTimeMicroseconds':25087,'OSWriteBytes':12288,'OSWriteChars':12288}", "{'load_balancing':'random','max_memory_usage':'10000000000'}", "[]", "[]", "[]", "[]", "[]", "[]", "['concat','notEmpty','extractAll']", "[]", "[]"},
-		})
-
-	client.QueryResponses["SELECT * FROM system.query_log ORDER BY event_time_microseconds ASC LIMIT 100000"] = &queryLogFrame
-
-	textLogColumns := []string{"event_date", "event_time", "event_time_microseconds", "microseconds", "thread_name", "thread_id", "level", "query_id", "logger_name", "message", "revision", "source_file", "source_line"}
-	textLogFrame := test.NewFakeDataFrame("textLog", textLogColumns,
-		[][]interface{}{
-			{"2022-02-03", "2022-02-03 16:17:47", "2022-02-03 16:37:17.056950", "56950", "clickhouse-serv", "68947", "Information", "", "DNSCacheUpdater", "Update period 15 seconds", "54458", "../src/Interpreters/DNSCacheUpdater.cpp; void DB::DNSCacheUpdater::start()", "46"},
-			{"2022-02-03", "2022-02-03 16:27:47", "2022-02-03 16:37:27.057022", "57022", "clickhouse-serv", "68947", "Information", "", "Application", "Available RAM: 62.24 GiB; physical cores: 8; logical cores: 16.", "54458", "../programs/server/Server.cpp; virtual int DB::Server::main(const std::vector<std::string> &)", "1380"},
-			{"2022-02-03", "2022-02-03 16:37:47", "2022-02-03 16:37:37.057484", "57484", "clickhouse-serv", "68947", "Information", "", "Application", "Listening for http://[::1]:8123", "54458", "../programs/server/Server.cpp; virtual int DB::Server::main(const std::vector<std::string> &)", "1444"},
-			{"2022-02-03", "2022-02-03 16:47:47", "2022-02-03 16:37:47.057527", "57527", "clickhouse-serv", "68947", "Information", "", "Application", "Listening for native protocol (tcp): [::1]:9000", "54458", "../programs/server/Server.cpp; virtual int DB::Server::main(const std::vector<std::string> &)", "1444"},
-		})
-
-	client.QueryResponses["SELECT * FROM system.text_log ORDER BY event_time_microseconds ASC LIMIT 100000"] = &textLogFrame
-
-	// skip query_thread_log frame - often it doesn't exist anyway unless enabled
-	t.Run("test default db logs collection", func(t *testing.T) {
-		bundle, errs := dbLogsCollector.Collect(config.Configuration{})
-		require.Empty(t, errs)
-		require.NotNil(t, bundle)
-		require.Len(t, bundle.Frames, 2)
-		require.Contains(t, bundle.Frames, "text_log")
-		require.Contains(t, bundle.Frames, "query_log")
-		require.Len(t, bundle.Errors.Errors, 1)
-		// check query_log frame
-		require.Contains(t, bundle.Frames, "query_log")
-		require.Equal(t, queryLogColumns, bundle.Frames["query_log"].Columns())
-		checkFrame(t, bundle.Frames["query_log"], queryLogFrame.Rows)
-		//check text_log frame
-		require.Contains(t, bundle.Frames, "text_log")
-		require.Equal(t, textLogColumns, bundle.Frames["text_log"].Columns())
-		checkFrame(t, bundle.Frames["text_log"], textLogFrame.Rows)
-		client.Reset()
-	})
-
-	t.Run("test db logs collection with limit", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.IntParam{
-					Value: 1,
-					Param: config.NewParam("row_limit", "Maximum number of log rows to collect. Negative values mean unlimited", false),
-				},
-			},
-		}
-		bundle, err := dbLogsCollector.Collect(conf)
-		require.Empty(t, err)
-		require.NotNil(t, bundle)
-		require.Len(t, bundle.Frames, 0)
-		require.Len(t, bundle.Errors.Errors, 3)
-		// populate client
-		client.QueryResponses["SELECT * FROM system.query_log ORDER BY event_time_microseconds ASC LIMIT 1"] = &queryLogFrame
-		client.QueryResponses["SELECT * FROM system.text_log ORDER BY event_time_microseconds ASC LIMIT 1"] = &textLogFrame
-		bundle, err = dbLogsCollector.Collect(conf)
-		require.Empty(t, err)
-		require.Len(t, bundle.Frames, 2)
-		require.Len(t, bundle.Errors.Errors, 1)
-		require.Contains(t, bundle.Frames, "text_log")
-		require.Contains(t, bundle.Frames, "query_log")
-		// check query_log frame
-		require.Contains(t, bundle.Frames, "query_log")
-		require.Equal(t, queryLogColumns, bundle.Frames["query_log"].Columns())
-		checkFrame(t, bundle.Frames["query_log"], queryLogFrame.Rows[:1])
-		//check text_log frame
-		require.Contains(t, bundle.Frames, "text_log")
-		require.Equal(t, textLogColumns, bundle.Frames["text_log"].Columns())
-		checkFrame(t, bundle.Frames["text_log"], textLogFrame.Rows[:1])
-		client.Reset()
-	})
-}
diff --git a/programs/diagnostics/internal/collectors/clickhouse/logs_test.go b/programs/diagnostics/internal/collectors/clickhouse/logs_test.go
deleted file mode 100644
index 5f0be7344453..000000000000
--- a/programs/diagnostics/internal/collectors/clickhouse/logs_test.go
+++ /dev/null
@@ -1,147 +0,0 @@
-package clickhouse_test
-
-import (
-	"fmt"
-	"os"
-	"path"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/stretchr/testify/require"
-)
-
-func TestLogsConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned for logs collector", func(t *testing.T) {
-		client := test.NewFakeClickhouseClient(make(map[string][]string))
-		logsCollector := clickhouse.NewLogsCollector(&platform.ResourceManager{
-			DbClient: client,
-		})
-		conf := logsCollector.Configuration()
-		require.Len(t, conf.Params, 2)
-		// check directory
-		require.IsType(t, config.StringParam{}, conf.Params[0])
-		directory, ok := conf.Params[0].(config.StringParam)
-		require.True(t, ok)
-		require.False(t, directory.Required())
-		require.Equal(t, directory.Name(), "directory")
-		require.Empty(t, directory.Value)
-		// check collect_archives
-		require.IsType(t, config.BoolParam{}, conf.Params[1])
-		collectArchives, ok := conf.Params[1].(config.BoolParam)
-		require.True(t, ok)
-		require.False(t, collectArchives.Required())
-		require.Equal(t, collectArchives.Name(), "collect_archives")
-		require.False(t, collectArchives.Value)
-	})
-}
-
-func TestLogsCollect(t *testing.T) {
-
-	logsCollector := clickhouse.NewLogsCollector(&platform.ResourceManager{})
-
-	t.Run("test default logs collection", func(t *testing.T) {
-		// we can't rely on a local installation of clickhouse being present for tests - if it is present (and running)
-		// results maybe variable e.g. we may find a config. For now, we allow flexibility and test only default.
-		// TODO: we may want to test this within a container
-		bundle, err := logsCollector.Collect(config.Configuration{})
-		require.Nil(t, err)
-		require.NotNil(t, bundle)
-		// we will have some errors if clickhouse is installed or not. If former, permission issues - if latter missing folders.
-		require.Greater(t, len(bundle.Errors.Errors), 0)
-		require.Len(t, bundle.Frames, 1)
-		require.Contains(t, bundle.Frames, "default")
-		_, ok := bundle.Frames["default"].(data.DirectoryFileFrame)
-		require.True(t, ok)
-		// no guarantees clickhouse is installed so this bundle could have no frames
-	})
-
-	t.Run("test logs collection when directory is specified", func(t *testing.T) {
-		cwd, err := os.Getwd()
-		require.Nil(t, err)
-		logsPath := path.Join(cwd, "../../../testdata", "logs", "var", "logs")
-		bundle, err := logsCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value:      logsPath,
-					Param:      config.NewParam("directory", "Specify the location of the log files for ClickHouse Server e.g. /var/log/clickhouse-server/", false),
-					AllowEmpty: true,
-				},
-			},
-		})
-		require.Nil(t, err)
-		checkDirectoryBundle(t, bundle, logsPath, []string{"clickhouse-server.log", "clickhouse-server.err.log"})
-
-	})
-
-	t.Run("test logs collection of archives", func(t *testing.T) {
-		cwd, err := os.Getwd()
-		require.Nil(t, err)
-		logsPath := path.Join(cwd, "../../../testdata", "logs", "var", "logs")
-		bundle, err := logsCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value:      logsPath,
-					Param:      config.NewParam("directory", "Specify the location of the log files for ClickHouse Server e.g. /var/log/clickhouse-server/", false),
-					AllowEmpty: true,
-				},
-				config.BoolParam{
-					Value: true,
-					Param: config.NewParam("collect_archives", "Collect compressed log archive files", false),
-				},
-			},
-		})
-		require.Nil(t, err)
-		checkDirectoryBundle(t, bundle, logsPath, []string{"clickhouse-server.log", "clickhouse-server.err.log", "clickhouse-server.log.gz"})
-	})
-
-	t.Run("test when directory does not exist", func(t *testing.T) {
-		tmpDir := t.TempDir()
-		logsPath := path.Join(tmpDir, "random")
-		bundle, err := logsCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value:      logsPath,
-					Param:      config.NewParam("directory", "Specify the location of the log files for ClickHouse Server e.g. /var/log/clickhouse-server/", false),
-					AllowEmpty: true,
-				},
-			},
-		})
-		// not a fatal error currently
-		require.Nil(t, err)
-		require.Len(t, bundle.Errors.Errors, 1)
-		require.Equal(t, fmt.Sprintf("directory %s does not exist", logsPath), bundle.Errors.Errors[0].Error())
-	})
-}
-
-func checkDirectoryBundle(t *testing.T, bundle *data.DiagnosticBundle, logsPath string, expectedFiles []string) {
-	require.NotNil(t, bundle)
-	require.Nil(t, bundle.Errors.Errors)
-	require.Len(t, bundle.Frames, 1)
-	require.Contains(t, bundle.Frames, "user_specified")
-	dirFrame, ok := bundle.Frames["user_specified"].(data.DirectoryFileFrame)
-	require.True(t, ok)
-	require.Equal(t, logsPath, dirFrame.Directory)
-	require.Equal(t, []string{"files"}, dirFrame.Columns())
-	i := 0
-	fullPaths := make([]string, len(expectedFiles))
-	for i, filePath := range expectedFiles {
-		fullPaths[i] = path.Join(logsPath, filePath)
-	}
-	for {
-		values, ok, err := dirFrame.Next()
-		require.Nil(t, err)
-		if !ok {
-			break
-		}
-		require.Len(t, values, 1)
-		file, ok := values[0].(data.SimpleFile)
-		require.True(t, ok)
-		require.Contains(t, fullPaths, file.FilePath())
-		i += 1
-	}
-	require.Equal(t, len(fullPaths), i)
-}
diff --git a/programs/diagnostics/internal/collectors/clickhouse/summary_test.go b/programs/diagnostics/internal/collectors/clickhouse/summary_test.go
deleted file mode 100644
index 92945d987ed5..000000000000
--- a/programs/diagnostics/internal/collectors/clickhouse/summary_test.go
+++ /dev/null
@@ -1,111 +0,0 @@
-package clickhouse_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/stretchr/testify/require"
-)
-
-func TestSummaryConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned for summary collector", func(t *testing.T) {
-		client := test.NewFakeClickhouseClient(make(map[string][]string))
-		summaryCollector := clickhouse.NewSummaryCollector(&platform.ResourceManager{
-			DbClient: client,
-		})
-		conf := summaryCollector.Configuration()
-		require.Len(t, conf.Params, 1)
-		require.IsType(t, config.IntParam{}, conf.Params[0])
-		limit, ok := conf.Params[0].(config.IntParam)
-		require.True(t, ok)
-		require.False(t, limit.Required())
-		require.Equal(t, limit.Name(), "row_limit")
-		require.Equal(t, int64(20), limit.Value)
-	})
-}
-
-func TestSummaryCollection(t *testing.T) {
-
-	client := test.NewFakeClickhouseClient(make(map[string][]string))
-	versionFrame := test.NewFakeDataFrame("version", []string{"version()"},
-		[][]interface{}{
-			{"22.1.3.7"},
-		},
-	)
-	client.QueryResponses["SELECT version()"] = &versionFrame
-	databasesFrame := test.NewFakeDataFrame("databases", []string{"name", "engine", "tables", "partitions", "parts", "disk_size"},
-		[][]interface{}{
-			{"tutorial", "Atomic", 2, 2, 2, "1.70 GiB"},
-			{"default", "Atomic", 5, 5, 6, "1.08 GiB"},
-			{"system", "Atomic", 11, 24, 70, "1.05 GiB"},
-			{"INFORMATION_SCHEMA", "Memory", 0, 0, 0, "0.00 B"},
-			{"covid19db", "Atomic", 0, 0, 0, "0.00 B"},
-			{"information_schema", "Memory", 0, 0, 0, "0.00 B"}})
-
-	client.QueryResponses["SELECT name, engine, tables, partitions, parts, formatReadableSize(bytes_on_disk) \"disk_size\" "+
-		"FROM system.databases db LEFT JOIN ( SELECT database, uniq(table) \"tables\", uniq(table, partition) \"partitions\", "+
-		"count() AS parts, sum(bytes_on_disk) \"bytes_on_disk\" FROM system.parts WHERE active GROUP BY database ) AS db_stats "+
-		"ON db.name = db_stats.database ORDER BY bytes_on_disk DESC LIMIT 20"] = &databasesFrame
-
-	summaryCollector := clickhouse.NewSummaryCollector(&platform.ResourceManager{
-		DbClient: client,
-	})
-
-	t.Run("test default summary collection", func(t *testing.T) {
-		bundle, errs := summaryCollector.Collect(config.Configuration{})
-		require.Empty(t, errs)
-		require.Len(t, bundle.Errors.Errors, 30)
-		require.NotNil(t, bundle)
-		require.Len(t, bundle.Frames, 2)
-		// check version frame
-		require.Contains(t, bundle.Frames, "version")
-		require.Equal(t, []string{"version()"}, bundle.Frames["version"].Columns())
-		checkFrame(t, bundle.Frames["version"], versionFrame.Rows)
-		//check databases frame
-		require.Contains(t, bundle.Frames, "databases")
-		require.Equal(t, []string{"name", "engine", "tables", "partitions", "parts", "disk_size"}, bundle.Frames["databases"].Columns())
-		checkFrame(t, bundle.Frames["databases"], databasesFrame.Rows)
-		client.Reset()
-	})
-
-	t.Run("test summary collection with limit", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.IntParam{
-					Value: 1,
-					Param: config.NewParam("row_limit", "Limit rows on supported queries.", false),
-				},
-			},
-		}
-		bundle, errs := summaryCollector.Collect(conf)
-
-		require.Empty(t, errs)
-		require.Len(t, bundle.Errors.Errors, 31)
-		require.NotNil(t, bundle)
-		// databases will be absent due to limit
-		require.Len(t, bundle.Frames, 1)
-		// check version frame
-		require.Contains(t, bundle.Frames, "version")
-		require.Equal(t, []string{"version()"}, bundle.Frames["version"].Columns())
-		checkFrame(t, bundle.Frames["version"], versionFrame.Rows)
-
-		client.QueryResponses["SELECT name, engine, tables, partitions, parts, formatReadableSize(bytes_on_disk) \"disk_size\" "+
-			"FROM system.databases db LEFT JOIN ( SELECT database, uniq(table) \"tables\", uniq(table, partition) \"partitions\", "+
-			"count() AS parts, sum(bytes_on_disk) \"bytes_on_disk\" FROM system.parts WHERE active GROUP BY database ) AS db_stats "+
-			"ON db.name = db_stats.database ORDER BY bytes_on_disk DESC LIMIT 1"] = &databasesFrame
-		bundle, errs = summaryCollector.Collect(conf)
-		require.Empty(t, errs)
-		require.Len(t, bundle.Errors.Errors, 30)
-		require.NotNil(t, bundle)
-		require.Len(t, bundle.Frames, 2)
-		require.Contains(t, bundle.Frames, "version")
-		//check databases frame
-		require.Contains(t, bundle.Frames, "databases")
-		require.Equal(t, []string{"name", "engine", "tables", "partitions", "parts", "disk_size"}, bundle.Frames["databases"].Columns())
-		// this will parse as our mock client does not read statement (specifically the limit clause) when called with execute
-		checkFrame(t, bundle.Frames["databases"], databasesFrame.Rows)
-	})
-}
diff --git a/programs/diagnostics/internal/collectors/clickhouse/system_test.go b/programs/diagnostics/internal/collectors/clickhouse/system_test.go
deleted file mode 100644
index d1b9a6e78591..000000000000
--- a/programs/diagnostics/internal/collectors/clickhouse/system_test.go
+++ /dev/null
@@ -1,366 +0,0 @@
-package clickhouse_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/stretchr/testify/require"
-)
-
-func TestSystemConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned for system db collector", func(t *testing.T) {
-		client := test.NewFakeClickhouseClient(make(map[string][]string))
-		systemDbCollector := clickhouse.NewSystemDatabaseCollector(&platform.ResourceManager{
-			DbClient: client,
-		})
-		conf := systemDbCollector.Configuration()
-		require.Len(t, conf.Params, 3)
-		// check first param
-		require.IsType(t, config.StringListParam{}, conf.Params[0])
-		includeTables, ok := conf.Params[0].(config.StringListParam)
-		require.True(t, ok)
-		require.False(t, includeTables.Required())
-		require.Equal(t, includeTables.Name(), "include_tables")
-		require.Nil(t, includeTables.Values)
-		// check second param
-		require.IsType(t, config.StringListParam{}, conf.Params[1])
-		excludeTables, ok := conf.Params[1].(config.StringListParam)
-		require.True(t, ok)
-		require.False(t, excludeTables.Required())
-		require.Equal(t, "exclude_tables", excludeTables.Name())
-		require.Equal(t, []string{"licenses", "distributed_ddl_queue", "query_thread_log", "query_log", "asynchronous_metric_log", "zookeeper", "aggregate_function_combinators", "collations", "contributors", "data_type_families", "formats", "graphite_retentions", "numbers", "numbers_mt", "one", "parts_columns", "projection_parts", "projection_parts_columns", "table_engines", "time_zones", "zeros", "zeros_mt"}, excludeTables.Values)
-		// check third param
-		require.IsType(t, config.IntParam{}, conf.Params[2])
-		rowLimit, ok := conf.Params[2].(config.IntParam)
-		require.True(t, ok)
-		require.False(t, rowLimit.Required())
-		require.Equal(t, "row_limit", rowLimit.Name())
-		require.Equal(t, int64(100000), rowLimit.Value)
-	})
-}
-
-func TestSystemDbCollect(t *testing.T) {
-
-	diskFrame := test.NewFakeDataFrame("disks", []string{"name", "path", "free_space", "total_space", "keep_free_space", "type"},
-		[][]interface{}{
-			{"default", "/var/lib/clickhouse", 1729659346944, 1938213220352, "", "local"},
-		},
-	)
-	clusterFrame := test.NewFakeDataFrame("clusters", []string{"cluster", "shard_num", "shard_weight", "replica_num", "host_name", "host_address", "port", "is_local", "user", "default_database", "errors_count", "slowdowns_count", "estimated_recovery_time"},
-		[][]interface{}{
-			{"events", 1, 1, 1, "dalem-local-clickhouse-blue-1", "192.168.144.2", 9000, 1, "default", "", 0, 0, 0},
-			{"events", 2, 1, 1, "dalem-local-clickhouse-blue-2", "192.168.144.4", 9000, 1, "default", "", 0, 0, 0},
-			{"events", 3, 1, 1, "dalem-local-clickhouse-blue-3", "192.168.144.3", 9000, 1, "default", "", 0, 0, 0},
-		},
-	)
-	userFrame := test.NewFakeDataFrame("users", []string{"name", "id", "storage", "auth_type", "auth_params", "host_ip", "host_names", "host_names_regexp", "host_names_like"},
-		[][]interface{}{
-			{"default", "94309d50-4f52-5250-31bd-74fecac179db,users.xml,plaintext_password", "sha256_password", []string{"::0"}, []string{}, []string{}, []string{}},
-		},
-	)
-
-	dbTables := map[string][]string{
-		clickhouse.SystemDatabase: {"disks", "clusters", "users"},
-	}
-	client := test.NewFakeClickhouseClient(dbTables)
-
-	client.QueryResponses["SELECT * FROM system.disks LIMIT 100000"] = &diskFrame
-	client.QueryResponses["SELECT * FROM system.clusters LIMIT 100000"] = &clusterFrame
-	client.QueryResponses["SELECT * FROM system.users LIMIT 100000"] = &userFrame
-	systemDbCollector := clickhouse.NewSystemDatabaseCollector(&platform.ResourceManager{
-		DbClient: client,
-	})
-
-	t.Run("test default system db collection", func(t *testing.T) {
-		diagSet, err := systemDbCollector.Collect(config.Configuration{})
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 3)
-		// disks frame
-		require.Equal(t, "disks", diagSet.Frames["disks"].Name())
-		require.Equal(t, diskFrame.ColumnNames, diagSet.Frames["disks"].Columns())
-		checkFrame(t, diagSet.Frames["disks"], diskFrame.Rows)
-		// clusters frame
-		require.Equal(t, "clusters", diagSet.Frames["clusters"].Name())
-		require.Equal(t, clusterFrame.ColumnNames, diagSet.Frames["clusters"].Columns())
-		checkFrame(t, diagSet.Frames["clusters"], clusterFrame.Rows)
-		// users frame
-		require.Equal(t, "users", diagSet.Frames["users"].Name())
-		require.Equal(t, userFrame.ColumnNames, diagSet.Frames["users"].Columns())
-		checkFrame(t, diagSet.Frames["users"], userFrame.Rows)
-		client.Reset()
-	})
-
-	t.Run("test when we pass an includes", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringListParam{
-					// nil means include everything
-					Values: []string{"disks"},
-					Param:  config.NewParam("include_tables", "Exclusion", false),
-				},
-			},
-		}
-		diagSet, err := systemDbCollector.Collect(conf)
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 1)
-		// disks frame
-		require.Equal(t, "disks", diagSet.Frames["disks"].Name())
-		require.Equal(t, diskFrame.ColumnNames, diagSet.Frames["disks"].Columns())
-		checkFrame(t, diagSet.Frames["disks"], diskFrame.Rows)
-		client.Reset()
-	})
-
-	// test excludes
-	t.Run("test when we pass an excludes", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringListParam{
-					Values: []string{"disks"},
-					Param:  config.NewParam("exclude_tables", "Exclusion", false),
-				},
-			},
-		}
-		diagSet, err := systemDbCollector.Collect(conf)
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 2)
-		// clusters frame
-		require.Equal(t, "clusters", diagSet.Frames["clusters"].Name())
-		require.Equal(t, clusterFrame.ColumnNames, diagSet.Frames["clusters"].Columns())
-		checkFrame(t, diagSet.Frames["clusters"], clusterFrame.Rows)
-		// users frame
-		require.Equal(t, "users", diagSet.Frames["users"].Name())
-		require.Equal(t, userFrame.ColumnNames, diagSet.Frames["users"].Columns())
-		checkFrame(t, diagSet.Frames["users"], userFrame.Rows)
-		client.Reset()
-	})
-
-	// test includes which isn't in the list
-	t.Run("test when we pass an invalid includes", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringListParam{
-					// nil means include everything
-					Values: []string{"disks", "invalid"},
-					Param:  config.NewParam("include_tables", "Exclusion", false),
-				},
-			},
-		}
-		diagSet, err := systemDbCollector.Collect(conf)
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 1)
-		require.Equal(t, diagSet.Errors.Error(), "some tables specified in the include_tables are not in the "+
-			"system database and will not be exported: [invalid]")
-		require.Len(t, diagSet.Frames, 1)
-		// disks frame
-		require.Equal(t, "disks", diagSet.Frames["disks"].Name())
-		require.Equal(t, diskFrame.ColumnNames, diagSet.Frames["disks"].Columns())
-		checkFrame(t, diagSet.Frames["disks"], diskFrame.Rows)
-		client.Reset()
-	})
-
-	t.Run("test when we use a table with excluded fields", func(t *testing.T) {
-		excludeDefault := clickhouse.ExcludeColumns
-		client.QueryResponses["SELECT * EXCEPT(keep_free_space,type) FROM system.disks LIMIT 100000"] = &diskFrame
-		clickhouse.ExcludeColumns = map[string][]string{
-			"disks": {"keep_free_space", "type"},
-		}
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringListParam{
-					// nil means include everything
-					Values: []string{"disks"},
-					Param:  config.NewParam("include_tables", "Exclusion", false),
-				},
-			},
-		}
-		diagSet, err := systemDbCollector.Collect(conf)
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 1)
-		// disks frame
-		require.Equal(t, "disks", diagSet.Frames["disks"].Name())
-		require.Equal(t, []string{"name", "path", "free_space", "total_space"}, diagSet.Frames["disks"].Columns())
-		eDiskFrame := test.NewFakeDataFrame("disks", []string{"name", "path", "free_space", "total_space"},
-			[][]interface{}{
-				{"default", "/var/lib/clickhouse", 1729659346944, 1938213220352},
-			},
-		)
-		checkFrame(t, diagSet.Frames["disks"], eDiskFrame.Rows)
-		clickhouse.ExcludeColumns = excludeDefault
-		client.Reset()
-	})
-
-	t.Run("test with a low row limit", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.IntParam{
-					Value: 1,
-					Param: config.NewParam("row_limit", "Maximum number of rows to collect from any table. Negative values mean unlimited.", false),
-				},
-			},
-		}
-		client.QueryResponses["SELECT * FROM system.disks LIMIT 1"] = &diskFrame
-		client.QueryResponses["SELECT * FROM system.clusters LIMIT 1"] = &clusterFrame
-		client.QueryResponses["SELECT * FROM system.users LIMIT 1"] = &userFrame
-		diagSet, err := systemDbCollector.Collect(conf)
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 3)
-		// clusters frame
-		require.Equal(t, "clusters", diagSet.Frames["clusters"].Name())
-		require.Equal(t, clusterFrame.ColumnNames, diagSet.Frames["clusters"].Columns())
-		lClusterFrame := test.NewFakeDataFrame("clusters", []string{"cluster", "shard_num", "shard_weight", "replica_num", "host_name", "host_address", "port", "is_local", "user", "default_database", "errors_count", "slowdowns_count", "estimated_recovery_time"},
-			[][]interface{}{
-				{"events", 1, 1, 1, "dalem-local-clickhouse-blue-1", "192.168.144.2", 9000, 1, "default", "", 0, 0, 0},
-			})
-		checkFrame(t, diagSet.Frames["clusters"], lClusterFrame.Rows)
-		client.Reset()
-	})
-
-	t.Run("test with a negative low row limit", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.IntParam{
-					Value: -23,
-					Param: config.NewParam("row_limit", "Maximum number of rows to collect from any table. Negative values mean unlimited.", false),
-				},
-			},
-		}
-		client.QueryResponses["SELECT * FROM system.clusters"] = &clusterFrame
-		client.QueryResponses["SELECT * FROM system.disks"] = &diskFrame
-		client.QueryResponses["SELECT * FROM system.users"] = &userFrame
-		diagSet, err := systemDbCollector.Collect(conf)
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 3)
-		// disks frame
-		require.Equal(t, "disks", diagSet.Frames["disks"].Name())
-		require.Equal(t, diskFrame.ColumnNames, diagSet.Frames["disks"].Columns())
-		checkFrame(t, diagSet.Frames["disks"], diskFrame.Rows)
-		// clusters frame
-		require.Equal(t, "clusters", diagSet.Frames["clusters"].Name())
-		require.Equal(t, clusterFrame.ColumnNames, diagSet.Frames["clusters"].Columns())
-		checkFrame(t, diagSet.Frames["clusters"], clusterFrame.Rows)
-		// users frame
-		require.Equal(t, "users", diagSet.Frames["users"].Name())
-		require.Equal(t, userFrame.ColumnNames, diagSet.Frames["users"].Columns())
-		checkFrame(t, diagSet.Frames["users"], userFrame.Rows)
-		client.Reset()
-	})
-
-	t.Run("test that includes overrides excludes", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringListParam{
-					// nil means include everything
-					Values: []string{"disks"},
-					Param:  config.NewParam("exclude_tables", "Excluded", false),
-				},
-				config.StringListParam{
-					// nil means include everything
-					Values: []string{"disks", "clusters", "users"},
-					Param:  config.NewParam("include_tables", "Included", false),
-				},
-			},
-		}
-		diagSet, err := systemDbCollector.Collect(conf)
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 3)
-		client.Reset()
-	})
-
-	t.Run("test banned", func(t *testing.T) {
-		bannedDefault := clickhouse.BannedTables
-		clickhouse.BannedTables = []string{"disks"}
-		diagSet, err := systemDbCollector.Collect(config.Configuration{})
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 2)
-		require.Contains(t, diagSet.Frames, "users")
-		require.Contains(t, diagSet.Frames, "clusters")
-		clickhouse.BannedTables = bannedDefault
-		client.Reset()
-	})
-
-	t.Run("test banned unless included", func(t *testing.T) {
-		bannedDefault := clickhouse.BannedTables
-		clickhouse.BannedTables = []string{"disks"}
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringListParam{
-					// nil means include everything
-					Values: []string{"disks", "clusters", "users"},
-					Param:  config.NewParam("include_tables", "Included", false),
-				},
-			},
-		}
-		diagSet, err := systemDbCollector.Collect(conf)
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 3)
-		require.Contains(t, diagSet.Frames, "disks")
-		require.Contains(t, diagSet.Frames, "users")
-		require.Contains(t, diagSet.Frames, "clusters")
-		clickhouse.BannedTables = bannedDefault
-		client.Reset()
-	})
-
-	t.Run("tables are ordered if configured", func(t *testing.T) {
-		defaultOrderBy := clickhouse.OrderBy
-		clickhouse.OrderBy = map[string]data.OrderBy{
-			"clusters": {
-				Column: "shard_num",
-				Order:  data.Desc,
-			},
-		}
-		client.QueryResponses["SELECT * FROM system.clusters ORDER BY shard_num DESC LIMIT 100000"] = &clusterFrame
-		diagSet, err := systemDbCollector.Collect(config.Configuration{})
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 3)
-		clickhouse.OrderBy = defaultOrderBy
-		oClusterFrame := test.NewFakeDataFrame("clusters", []string{"cluster", "shard_num", "shard_weight", "replica_num", "host_name", "host_address", "port", "is_local", "user", "default_database", "errors_count", "slowdowns_count", "estimated_recovery_time"},
-			[][]interface{}{
-				{"events", 3, 1, 1, "dalem-local-clickhouse-blue-3", "192.168.144.3", 9000, 1, "default", "", 0, 0, 0},
-				{"events", 2, 1, 1, "dalem-local-clickhouse-blue-2", "192.168.144.4", 9000, 1, "default", "", 0, 0, 0},
-				{"events", 1, 1, 1, "dalem-local-clickhouse-blue-1", "192.168.144.2", 9000, 1, "default", "", 0, 0, 0},
-			},
-		)
-		checkFrame(t, diagSet.Frames["clusters"], oClusterFrame.Rows)
-		client.Reset()
-	})
-
-}
-
-func checkFrame(t *testing.T, frame data.Frame, rows [][]interface{}) {
-	i := 0
-	for {
-		values, ok, err := frame.Next()
-		require.Nil(t, err)
-		if !ok {
-			break
-		}
-		require.ElementsMatch(t, rows[i], values)
-		i += 1
-	}
-	require.Equal(t, i, len(rows))
-}
diff --git a/programs/diagnostics/internal/collectors/clickhouse/zookeeper_test.go b/programs/diagnostics/internal/collectors/clickhouse/zookeeper_test.go
deleted file mode 100644
index 3e56f6200f07..000000000000
--- a/programs/diagnostics/internal/collectors/clickhouse/zookeeper_test.go
+++ /dev/null
@@ -1,102 +0,0 @@
-package clickhouse_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/stretchr/testify/require"
-)
-
-func TestZookeeperConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned for system zookeeper collector", func(t *testing.T) {
-		client := test.NewFakeClickhouseClient(make(map[string][]string))
-		zkCollector := clickhouse.NewZookeeperCollector(&platform.ResourceManager{
-			DbClient: client,
-		})
-		conf := zkCollector.Configuration()
-		require.Len(t, conf.Params, 3)
-		// check first param
-		require.IsType(t, config.StringParam{}, conf.Params[0])
-		pathPattern, ok := conf.Params[0].(config.StringParam)
-		require.True(t, ok)
-		require.False(t, pathPattern.Required())
-		require.Equal(t, pathPattern.Name(), "path_pattern")
-		require.Equal(t, "/clickhouse/{task_queue}/**", pathPattern.Value)
-		// check second param
-		require.IsType(t, config.IntParam{}, conf.Params[1])
-		maxDepth, ok := conf.Params[1].(config.IntParam)
-		require.True(t, ok)
-		require.False(t, maxDepth.Required())
-		require.Equal(t, "max_depth", maxDepth.Name())
-		require.Equal(t, int64(8), maxDepth.Value)
-		// check third param
-		require.IsType(t, config.IntParam{}, conf.Params[2])
-		rowLimit, ok := conf.Params[2].(config.IntParam)
-		require.True(t, ok)
-		require.False(t, rowLimit.Required())
-		require.Equal(t, "row_limit", rowLimit.Name())
-		require.Equal(t, int64(10), rowLimit.Value)
-	})
-}
-
-func TestZookeeperCollect(t *testing.T) {
-	level1 := test.NewFakeDataFrame("level_1", []string{"name", "value", "czxid", "mzxid", "ctime", "mtime", "version", "cversion", "aversion", "ephemeralOwner", "dataLength", "numChildren", "pzxid", "path"},
-		[][]interface{}{
-			{"name", "value", "czxid", "mzxid", "ctime", "mtime", "version", "cversion", "aversion", "ephemeralOwner", "dataLength", "numChildren", "pzxid", "path"},
-			{"task_queue", "", "4", "4", "2022-02-22 13:30:15", "2022-02-22 13:30:15", "0", "1", "0", "0", "0", "1", "5", "/clickhouse"},
-			{"copytasks", "", "525608", "525608", "2022-03-09 13:47:39", "2022-03-09 13:47:39", "0", "7", "0", "0", "0", "7", "526100", "/clickhouse"},
-		},
-	)
-	level2 := test.NewFakeDataFrame("level_2", []string{"name", "value", "czxid", "mzxid", "ctime", "mtime", "version", "cversion", "aversion", "ephemeralOwner", "dataLength", "numChildren", "pzxid", "path"},
-		[][]interface{}{
-			{"ddl", "", "5", "5", "2022-02-22 13:30:15", "2022-02-22 13:30:15", "0", "0", "0", "0", "0", "0", "5", "/clickhouse/task_queue"},
-		},
-	)
-	level3 := test.NewFakeDataFrame("level_2", []string{"name", "value", "czxid", "mzxid", "ctime", "mtime", "version", "cversion", "aversion", "ephemeralOwner", "dataLength", "numChildren", "pzxid", "path"},
-		[][]interface{}{},
-	)
-	dbTables := map[string][]string{
-		clickhouse.SystemDatabase: {"zookeeper"},
-	}
-	client := test.NewFakeClickhouseClient(dbTables)
-
-	client.QueryResponses["SELECT name FROM system.zookeeper WHERE path='/clickhouse' LIMIT 10"] = &level1
-	// can't reuse the frame as the first frame will be iterated as part of the recursive zookeeper search performed by the collector
-	cLevel1 := test.NewFakeDataFrame("level_1", level1.Columns(), level1.Rows)
-	client.QueryResponses["SELECT * FROM system.zookeeper WHERE path='/clickhouse' LIMIT 10"] = &cLevel1
-	client.QueryResponses["SELECT name FROM system.zookeeper WHERE path='/clickhouse/task_queue' LIMIT 10"] = &level2
-	cLevel2 := test.NewFakeDataFrame("level_2", level2.Columns(), level2.Rows)
-	client.QueryResponses["SELECT * FROM system.zookeeper WHERE path='/clickhouse/task_queue' LIMIT 10"] = &cLevel2
-	client.QueryResponses["SELECT name FROM system.zookeeper WHERE path='/clickhouse/task_queue/ddl' LIMIT 10"] = &level3
-	cLevel3 := test.NewFakeDataFrame("level_3", level3.Columns(), level3.Rows)
-	client.QueryResponses["SELECT * FROM system.zookeeper WHERE path='/clickhouse/task_queue/ddl' LIMIT 10"] = &cLevel3
-
-	zKCollector := clickhouse.NewZookeeperCollector(&platform.ResourceManager{
-		DbClient: client,
-	})
-
-	t.Run("test default zookeeper collection", func(t *testing.T) {
-		diagSet, err := zKCollector.Collect(config.Configuration{})
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 1)
-		require.Contains(t, diagSet.Frames, "zookeeper_db")
-		require.Equal(t, "clickhouse", diagSet.Frames["zookeeper_db"].Name())
-		require.IsType(t, data.HierarchicalFrame{}, diagSet.Frames["zookeeper_db"])
-		checkFrame(t, diagSet.Frames["zookeeper_db"], level1.Rows)
-		require.Equal(t, level1.Columns(), diagSet.Frames["zookeeper_db"].Columns())
-		hierarchicalFrame := diagSet.Frames["zookeeper_db"].(data.HierarchicalFrame)
-		require.Len(t, hierarchicalFrame.SubFrames, 1)
-		checkFrame(t, hierarchicalFrame.SubFrames[0], cLevel2.Rows)
-		require.Equal(t, cLevel2.Columns(), hierarchicalFrame.SubFrames[0].Columns())
-		hierarchicalFrame = hierarchicalFrame.SubFrames[0]
-		require.Len(t, hierarchicalFrame.SubFrames, 1)
-		checkFrame(t, hierarchicalFrame.SubFrames[0], cLevel3.Rows)
-		require.Equal(t, cLevel3.Columns(), hierarchicalFrame.SubFrames[0].Columns())
-	})
-}
diff --git a/programs/diagnostics/internal/collectors/registry_test.go b/programs/diagnostics/internal/collectors/registry_test.go
deleted file mode 100644
index eccc5f2265dc..000000000000
--- a/programs/diagnostics/internal/collectors/registry_test.go
+++ /dev/null
@@ -1,57 +0,0 @@
-package collectors_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse"
-	_ "github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/stretchr/testify/require"
-)
-
-func TestGetCollectorNames(t *testing.T) {
-	t.Run("can get all collector names", func(t *testing.T) {
-		collectorNames := collectors.GetCollectorNames(false)
-		require.ElementsMatch(t, []string{"system_db", "config", "summary", "system", "logs", "db_logs", "file", "command", "zookeeper_db"}, collectorNames)
-	})
-
-	t.Run("can get default collector names", func(t *testing.T) {
-		collectorNames := collectors.GetCollectorNames(true)
-		require.ElementsMatch(t, []string{"system_db", "config", "summary", "system", "logs", "db_logs"}, collectorNames)
-	})
-}
-
-func TestGetCollectorByName(t *testing.T) {
-
-	t.Run("can get collector by name", func(t *testing.T) {
-		collector, err := collectors.GetCollectorByName("system_db")
-		require.Nil(t, err)
-		require.Equal(t, clickhouse.NewSystemDatabaseCollector(platform.GetResourceManager()), collector)
-	})
-
-	t.Run("fails on non existing collector", func(t *testing.T) {
-		collector, err := collectors.GetCollectorByName("random")
-		require.NotNil(t, err)
-		require.Equal(t, "random is not a valid collector name", err.Error())
-		require.Nil(t, collector)
-	})
-}
-
-func TestBuildConfigurationOptions(t *testing.T) {
-
-	t.Run("can get all collector configurations", func(t *testing.T) {
-		configs, err := collectors.BuildConfigurationOptions()
-		require.Nil(t, err)
-		require.Len(t, configs, 9)
-		require.Contains(t, configs, "system_db")
-		require.Contains(t, configs, "config")
-		require.Contains(t, configs, "summary")
-		require.Contains(t, configs, "system")
-		require.Contains(t, configs, "logs")
-		require.Contains(t, configs, "db_logs")
-		require.Contains(t, configs, "file")
-		require.Contains(t, configs, "command")
-		require.Contains(t, configs, "zookeeper_db")
-	})
-}
diff --git a/programs/diagnostics/internal/collectors/system/command_test.go b/programs/diagnostics/internal/collectors/system/command_test.go
deleted file mode 100644
index 7de00cdabf46..000000000000
--- a/programs/diagnostics/internal/collectors/system/command_test.go
+++ /dev/null
@@ -1,107 +0,0 @@
-package system_test
-
-import (
-	"fmt"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/stretchr/testify/require"
-)
-
-func TestCommandConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned for file collector", func(t *testing.T) {
-		commandCollector := system.NewCommandCollector(&platform.ResourceManager{})
-		conf := commandCollector.Configuration()
-		require.Len(t, conf.Params, 1)
-		require.IsType(t, config.StringParam{}, conf.Params[0])
-		command, ok := conf.Params[0].(config.StringParam)
-		require.True(t, ok)
-		require.True(t, command.Required())
-		require.Equal(t, command.Name(), "command")
-		require.Equal(t, "", command.Value)
-	})
-}
-
-func TestCommandCollect(t *testing.T) {
-	t.Run("test simple command with args", func(t *testing.T) {
-		commandCollector := system.NewCommandCollector(&platform.ResourceManager{})
-		bundle, err := commandCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value:      "ls -l ../../../testdata",
-					Param:      config.NewParam("command", "Command to execute", true),
-					AllowEmpty: false,
-				},
-			},
-		})
-		require.Nil(t, err)
-		require.Nil(t, bundle.Errors.Errors)
-		require.Len(t, bundle.Frames, 1)
-		require.Contains(t, bundle.Frames, "output")
-		require.Equal(t, bundle.Frames["output"].Columns(), []string{"command", "stdout", "stderr", "error"})
-		memFrame := bundle.Frames["output"].(data.MemoryFrame)
-		values, ok, err := memFrame.Next()
-		require.True(t, ok)
-		require.Nil(t, err)
-		fmt.Println(values)
-		require.Len(t, values, 4)
-		require.Equal(t, "ls -l ../../../testdata", values[0])
-		require.Contains(t, values[1], "configs")
-		require.Contains(t, values[1], "docker")
-		require.Contains(t, values[1], "log")
-		require.Equal(t, "", values[2])
-		require.Equal(t, "", values[3])
-		values, ok, err = memFrame.Next()
-		require.False(t, ok)
-		require.Nil(t, err)
-		require.Nil(t, values)
-	})
-
-	t.Run("test empty command", func(t *testing.T) {
-		commandCollector := system.NewCommandCollector(&platform.ResourceManager{})
-		bundle, err := commandCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value:      "",
-					Param:      config.NewParam("command", "Command to execute", true),
-					AllowEmpty: false,
-				},
-			},
-		})
-		require.Equal(t, "parameter command is invalid - command cannot be empty", err.Error())
-		require.Equal(t, &data.DiagnosticBundle{}, bundle)
-	})
-
-	t.Run("test invalid command", func(t *testing.T) {
-		commandCollector := system.NewCommandCollector(&platform.ResourceManager{})
-		bundle, err := commandCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value:      "ls --invalid ../../../testdata",
-					Param:      config.NewParam("command", "Command to execute", true),
-					AllowEmpty: false,
-				},
-			},
-		})
-		// commands may error with output - we still capture on stderr
-		require.Nil(t, err)
-		require.Len(t, bundle.Errors.Errors, 1)
-		require.Equal(t, "Unable to execute command: exit status 2", bundle.Errors.Errors[0].Error())
-		require.Len(t, bundle.Frames, 1)
-		require.Contains(t, bundle.Frames, "output")
-		require.Equal(t, bundle.Frames["output"].Columns(), []string{"command", "stdout", "stderr", "error"})
-		memFrame := bundle.Frames["output"].(data.MemoryFrame)
-		values, ok, err := memFrame.Next()
-		require.True(t, ok)
-		require.Nil(t, err)
-		require.Len(t, values, 4)
-		require.Equal(t, "ls --invalid ../../../testdata", values[0])
-		require.Equal(t, "", values[1])
-		// exact values here may vary on platform
-		require.NotEmpty(t, values[2])
-		require.NotEmpty(t, values[3])
-	})
-}
diff --git a/programs/diagnostics/internal/collectors/system/file_test.go b/programs/diagnostics/internal/collectors/system/file_test.go
deleted file mode 100644
index 5b1d5b3a92f0..000000000000
--- a/programs/diagnostics/internal/collectors/system/file_test.go
+++ /dev/null
@@ -1,110 +0,0 @@
-package system_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/stretchr/testify/require"
-)
-
-func TestFileConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned for file collector", func(t *testing.T) {
-		fileCollector := system.NewFileCollector(&platform.ResourceManager{})
-		conf := fileCollector.Configuration()
-		require.Len(t, conf.Params, 1)
-		require.IsType(t, config.StringParam{}, conf.Params[0])
-		filePattern, ok := conf.Params[0].(config.StringParam)
-		require.True(t, ok)
-		require.True(t, filePattern.Required())
-		require.Equal(t, filePattern.Name(), "file_pattern")
-		require.Equal(t, "", filePattern.Value)
-	})
-}
-
-func TestFileCollect(t *testing.T) {
-
-	t.Run("test filter patterns work", func(t *testing.T) {
-		fileCollector := system.NewFileCollector(&platform.ResourceManager{})
-		bundle, err := fileCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value:      "../../../testdata/**/*.xml",
-					Param:      config.NewParam("file_pattern", "Glob based pattern to specify files for collection", true),
-					AllowEmpty: false,
-				},
-			},
-		})
-		require.Nil(t, err)
-		require.Nil(t, bundle.Errors.Errors)
-		checkFileBundle(t, bundle,
-			[]string{"../../../testdata/configs/include/xml/server-include.xml",
-				"../../../testdata/configs/include/xml/user-include.xml",
-				"../../../testdata/configs/xml/config.xml",
-				"../../../testdata/configs/xml/users.xml",
-				"../../../testdata/configs/xml/users.d/default-password.xml",
-				"../../../testdata/configs/yandex_xml/config.xml",
-				"../../../testdata/docker/admin.xml",
-				"../../../testdata/docker/custom.xml"})
-	})
-
-	t.Run("invalid file patterns are detected", func(t *testing.T) {
-		fileCollector := system.NewFileCollector(&platform.ResourceManager{})
-		bundle, err := fileCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value:      "",
-					Param:      config.NewParam("file_pattern", "Glob based pattern to specify files for collection", true),
-					AllowEmpty: false,
-				},
-			},
-		})
-		require.NotNil(t, err)
-		require.Equal(t, "parameter file_pattern is invalid - file_pattern cannot be empty", err.Error())
-		require.Equal(t, &data.DiagnosticBundle{}, bundle)
-	})
-
-	t.Run("check empty matches are reported", func(t *testing.T) {
-		fileCollector := system.NewFileCollector(&platform.ResourceManager{})
-		bundle, err := fileCollector.Collect(config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value:      "../../../testdata/**/*.random",
-					Param:      config.NewParam("file_pattern", "Glob based pattern to specify files for collection", true),
-					AllowEmpty: false,
-				},
-			},
-		})
-		require.Nil(t, err)
-		require.Nil(t, bundle.Frames)
-		require.Len(t, bundle.Errors.Errors, 1)
-		require.Equal(t, "0 files match glob pattern", bundle.Errors.Errors[0].Error())
-	})
-
-}
-
-func checkFileBundle(t *testing.T, bundle *data.DiagnosticBundle, expectedFiles []string) {
-	require.NotNil(t, bundle)
-	require.Nil(t, bundle.Errors.Errors)
-	require.Len(t, bundle.Frames, 1)
-	require.Contains(t, bundle.Frames, "collection")
-	dirFrame, ok := bundle.Frames["collection"].(data.FileFrame)
-	require.True(t, ok)
-	require.Equal(t, []string{"files"}, dirFrame.Columns())
-	i := 0
-	for {
-		values, ok, err := dirFrame.Next()
-		require.Nil(t, err)
-		if !ok {
-			break
-		}
-		require.Len(t, values, 1)
-		file, ok := values[0].(data.SimpleFile)
-		require.True(t, ok)
-		require.Contains(t, expectedFiles, file.FilePath())
-		i += 1
-	}
-	require.Equal(t, len(expectedFiles), i)
-}
diff --git a/programs/diagnostics/internal/collectors/system/system_test.go b/programs/diagnostics/internal/collectors/system/system_test.go
deleted file mode 100644
index fb1e16bd1ed3..000000000000
--- a/programs/diagnostics/internal/collectors/system/system_test.go
+++ /dev/null
@@ -1,89 +0,0 @@
-package system_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/stretchr/testify/require"
-)
-
-func TestSystemConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned for system collector", func(t *testing.T) {
-		systemCollector := system.NewSystemCollector(&platform.ResourceManager{})
-		conf := systemCollector.Configuration()
-		require.Len(t, conf.Params, 0)
-		require.Equal(t, []config.ConfigParam{}, conf.Params)
-	})
-}
-
-func TestSystemCollect(t *testing.T) {
-	t.Run("test default system collection", func(t *testing.T) {
-		systemCollector := system.NewSystemCollector(&platform.ResourceManager{})
-		diagSet, err := systemCollector.Collect(config.Configuration{})
-		require.Nil(t, err)
-		require.NotNil(t, diagSet)
-		require.Len(t, diagSet.Errors.Errors, 0)
-		require.Len(t, diagSet.Frames, 7)
-		require.Contains(t, diagSet.Frames, "disks")
-		require.Contains(t, diagSet.Frames, "disk_usage")
-		require.Contains(t, diagSet.Frames, "memory")
-		require.Contains(t, diagSet.Frames, "memory_usage")
-		require.Contains(t, diagSet.Frames, "cpu")
-		require.Contains(t, diagSet.Frames, "processes")
-		require.Contains(t, diagSet.Frames, "os")
-		// responses here will vary depending on platform - mocking seems excessive so we test we have some data
-		// disks
-		require.Equal(t, []string{"name", "size", "physicalBlockSize", "driveType", "controller", "vendor", "model", "partitionName", "partitionSize", "mountPoint", "readOnly"}, diagSet.Frames["disks"].Columns())
-		diskFrames, err := countFrameRows(diagSet, "disks")
-		require.Greater(t, diskFrames, 0)
-		require.Nil(t, err)
-		// disk usage
-		require.Equal(t, []string{"filesystem", "size", "used", "avail", "use%", "mounted on"}, diagSet.Frames["disk_usage"].Columns())
-		diskUsageFrames, err := countFrameRows(diagSet, "disk_usage")
-		require.Greater(t, diskUsageFrames, 0)
-		require.Nil(t, err)
-		// memory
-		require.Equal(t, []string{"totalPhysical", "totalUsable", "supportedPageSizes"}, diagSet.Frames["memory"].Columns())
-		memoryFrames, err := countFrameRows(diagSet, "memory")
-		require.Greater(t, memoryFrames, 0)
-		require.Nil(t, err)
-		// memory_usage
-		require.Equal(t, []string{"type", "total", "used", "free"}, diagSet.Frames["memory_usage"].Columns())
-		memoryUsageFrames, err := countFrameRows(diagSet, "memory_usage")
-		require.Greater(t, memoryUsageFrames, 0)
-		require.Nil(t, err)
-		// cpu
-		require.Equal(t, []string{"processor", "vendor", "model", "core", "numThreads", "logical", "capabilities"}, diagSet.Frames["cpu"].Columns())
-		cpuFrames, err := countFrameRows(diagSet, "cpu")
-		require.Greater(t, cpuFrames, 0)
-		require.Nil(t, err)
-		// processes
-		require.Equal(t, []string{"pid", "ppid", "stime", "time", "rss", "size", "faults", "minorFaults", "majorFaults", "user", "state", "priority", "nice", "command"}, diagSet.Frames["processes"].Columns())
-		processesFrames, err := countFrameRows(diagSet, "processes")
-		require.Greater(t, processesFrames, 0)
-		require.Nil(t, err)
-		// os
-		require.Equal(t, []string{"hostname", "os", "goOs", "cpus", "core", "kernel", "platform"}, diagSet.Frames["os"].Columns())
-		osFrames, err := countFrameRows(diagSet, "os")
-		require.Greater(t, osFrames, 0)
-		require.Nil(t, err)
-	})
-}
-
-func countFrameRows(diagSet *data.DiagnosticBundle, frameName string) (int, error) {
-	frame := diagSet.Frames[frameName]
-	i := 0
-	for {
-		_, ok, err := frame.Next()
-		if !ok {
-			return i, err
-		}
-		if err != nil {
-			return i, err
-		}
-		i++
-	}
-}
diff --git a/programs/diagnostics/internal/outputs/file/simple_test.go b/programs/diagnostics/internal/outputs/file/simple_test.go
deleted file mode 100644
index 471a1c70cc1b..000000000000
--- a/programs/diagnostics/internal/outputs/file/simple_test.go
+++ /dev/null
@@ -1,468 +0,0 @@
-package file_test
-
-import (
-	"bufio"
-	"encoding/xml"
-	"fmt"
-	"io"
-	"os"
-	"path"
-	"strings"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/file"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/stretchr/testify/require"
-)
-
-var clusterFrame = test.NewFakeDataFrame("clusters", []string{"cluster", "shard_num", "shard_weight", "replica_num", "host_name", "host_address", "port", "is_local", "user", "default_database", "errors_count", "slowdowns_count", "estimated_recovery_time"},
-	[][]interface{}{
-		{"events", 1, 1, 1, "dalem-local-clickhouse-blue-1", "192.168.144.2", 9000, 1, "default", "", 0, 0, 0},
-		{"events", 2, 1, 1, "dalem-local-clickhouse-blue-2", "192.168.144.4", 9001, 1, "default", "", 0, 0, 0},
-		{"events", 3, 1, 1, "dalem-local-clickhouse-blue-3", "192.168.144.3", 9002, 1, "default", "", 0, 0, 0},
-	},
-)
-
-var diskFrame = test.NewFakeDataFrame("disks", []string{"name", "path", "free_space", "total_space", "keep_free_space", "type"},
-	[][]interface{}{
-		{"default", "/var/lib/clickhouse", 1729659346944, 1938213220352, "", "local"},
-	},
-)
-
-var userFrame = test.NewFakeDataFrame("users", []string{"name", "id", "storage", "auth_type", "auth_params", "host_ip", "host_names", "host_names_regexp", "host_names_like"},
-	[][]interface{}{
-		{"default", "94309d50-4f52-5250-31bd-74fecac179db,users.xml,plaintext_password", "sha256_password", []string{"::0"}, []string{}, []string{}, []string{}},
-	},
-)
-
-func TestConfiguration(t *testing.T) {
-	t.Run("correct configuration is returned", func(t *testing.T) {
-		output := file.SimpleOutput{}
-		conf := output.Configuration()
-		require.Len(t, conf.Params, 3)
-		// check first directory param
-		require.IsType(t, config.StringParam{}, conf.Params[0])
-		directory, ok := conf.Params[0].(config.StringParam)
-		require.True(t, ok)
-		require.False(t, directory.Required())
-		require.Equal(t, "directory", directory.Name())
-		require.Equal(t, "./", directory.Value)
-		// check second format param
-		require.IsType(t, config.StringOptions{}, conf.Params[1])
-		format, ok := conf.Params[1].(config.StringOptions)
-		require.True(t, ok)
-		require.False(t, format.Required())
-		require.Equal(t, "format", format.Name())
-		require.Equal(t, "csv", format.Value)
-		require.Equal(t, []string{"csv"}, format.Options)
-		// check third format compress
-		require.IsType(t, config.BoolParam{}, conf.Params[2])
-		skipArchive, ok := conf.Params[2].(config.BoolParam)
-		require.True(t, ok)
-		require.False(t, format.Required())
-		require.False(t, skipArchive.Value)
-	})
-}
-
-func TestWrite(t *testing.T) {
-	bundles := map[string]*data.DiagnosticBundle{
-		"systemA": {
-			Frames: map[string]data.Frame{
-				"disk":    diskFrame,
-				"cluster": clusterFrame,
-			},
-		},
-		"systemB": {
-			Frames: map[string]data.Frame{
-				"user": userFrame,
-			},
-		},
-	}
-	t.Run("test we can write simple diagnostic sets", func(t *testing.T) {
-		tempDir := t.TempDir()
-		configuration := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Param: config.NewParam("directory", "A directory", true),
-					Value: tempDir,
-				},
-				// turn compression off as the folder will be deleted by default
-				config.BoolParam{
-					Value: true,
-					Param: config.NewParam("skip_archive", "Skip archive", false),
-				},
-			},
-		}
-		output := file.SimpleOutput{FolderGenerator: staticFolderName}
-		frameErrors, err := output.Write("test", bundles, configuration)
-		require.Nil(t, err)
-		require.Equal(t, data.FrameErrors{}, frameErrors)
-		clusterFile := path.Join(tempDir, "test", "test", "systemA", "cluster.csv")
-		diskFile := path.Join(tempDir, "test", "test", "systemA", "disk.csv")
-		userFile := path.Join(tempDir, "test", "test", "systemB", "user.csv")
-		require.FileExists(t, clusterFile)
-		require.FileExists(t, diskFile)
-		require.FileExists(t, userFile)
-		diskLines, err := readFileLines(diskFile)
-		require.Nil(t, err)
-		require.Len(t, diskLines, 2)
-		usersLines, err := readFileLines(userFile)
-		require.Nil(t, err)
-		require.Len(t, usersLines, 2)
-		clusterLines, err := readFileLines(clusterFile)
-		require.Nil(t, err)
-		require.Len(t, clusterLines, 4)
-		require.Equal(t, strings.Join(clusterFrame.ColumnNames, ","), clusterLines[0])
-		require.Equal(t, "events,1,1,1,dalem-local-clickhouse-blue-1,192.168.144.2,9000,1,default,,0,0,0", clusterLines[1])
-		require.Equal(t, "events,2,1,1,dalem-local-clickhouse-blue-2,192.168.144.4,9001,1,default,,0,0,0", clusterLines[2])
-		require.Equal(t, "events,3,1,1,dalem-local-clickhouse-blue-3,192.168.144.3,9002,1,default,,0,0,0", clusterLines[3])
-		resetFrames()
-	})
-
-	t.Run("test invalid parameter", func(t *testing.T) {
-		tempDir := t.TempDir()
-		configuration := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Param: config.NewParam("directory", "A directory", true),
-					Value: tempDir,
-				},
-				config.StringOptions{
-					Value:   "random",
-					Options: []string{"csv"},
-					// TODO: add tsv and others here later
-					Param: config.NewParam("format", "Format of exported files", false),
-				},
-				config.BoolParam{
-					Value: true,
-					Param: config.NewParam("skip_archive", "Skip compressed archive", false),
-				},
-			},
-		}
-		output := file.SimpleOutput{FolderGenerator: staticFolderName}
-		frameErrors, err := output.Write("test", bundles, configuration)
-		require.Equal(t, data.FrameErrors{}, frameErrors)
-		require.NotNil(t, err)
-		require.Equal(t, "parameter format is invalid - random is not a valid value for format - [csv]", err.Error())
-		resetFrames()
-	})
-
-	t.Run("test compression", func(t *testing.T) {
-		tempDir := t.TempDir()
-		configuration := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Param: config.NewParam("directory", "A directory", true),
-					Value: tempDir,
-				},
-			},
-		}
-		output := file.SimpleOutput{FolderGenerator: staticFolderName}
-		frameErrors, err := output.Write("test", bundles, configuration)
-		require.Nil(t, err)
-		require.Equal(t, data.FrameErrors{}, frameErrors)
-		archiveFileName := path.Join(tempDir, "test", "test.tar.gz")
-		fi, err := os.Stat(archiveFileName)
-		require.Nil(t, err)
-		require.FileExists(t, archiveFileName)
-		// compression will vary so lets test range
-		require.Greater(t, int64(600), fi.Size())
-		require.Less(t, int64(200), fi.Size())
-		outputFolder := path.Join(tempDir, "test", "test")
-		// check the folder doesn't exist and is cleaned up
-		require.NoFileExists(t, outputFolder)
-		resetFrames()
-	})
-
-	t.Run("test support for directory frames", func(t *testing.T) {
-		// create 5 temporary files
-		tempDir := t.TempDir()
-		files := createRandomFiles(tempDir, 5)
-		dirFrame, errs := data.NewFileDirectoryFrame(tempDir, []string{"*.log"})
-		require.Empty(t, errs)
-		fileBundles := map[string]*data.DiagnosticBundle{
-			"systemA": {
-				Frames: map[string]data.Frame{
-					"disk":    diskFrame,
-					"cluster": clusterFrame,
-				},
-			},
-			"config": {
-				Frames: map[string]data.Frame{
-					"logs": dirFrame,
-				},
-			},
-		}
-		destDir := t.TempDir()
-		configuration := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Param: config.NewParam("directory", "A directory", true),
-					Value: destDir,
-				},
-				// turn compression off as the folder will be deleted by default
-				config.BoolParam{
-					Value: true,
-					Param: config.NewParam("skip_archive", "Skip archive", false),
-				},
-			},
-		}
-		output := file.SimpleOutput{FolderGenerator: staticFolderName}
-		frameErrors, err := output.Write("test", fileBundles, configuration)
-		require.Nil(t, err)
-		require.NotNil(t, frameErrors)
-
-		// test the usual frames still work
-		clusterFile := path.Join(destDir, "test", "test", "systemA", "cluster.csv")
-		diskFile := path.Join(destDir, "test", "test", "systemA", "disk.csv")
-		require.FileExists(t, clusterFile)
-		require.FileExists(t, diskFile)
-		diskLines, err := readFileLines(diskFile)
-		require.Nil(t, err)
-		require.Len(t, diskLines, 2)
-		clusterLines, err := readFileLines(clusterFile)
-		require.Nil(t, err)
-		require.Len(t, clusterLines, 4)
-		require.Equal(t, strings.Join(clusterFrame.ColumnNames, ","), clusterLines[0])
-		require.Equal(t, "events,1,1,1,dalem-local-clickhouse-blue-1,192.168.144.2,9000,1,default,,0,0,0", clusterLines[1])
-		require.Equal(t, "events,2,1,1,dalem-local-clickhouse-blue-2,192.168.144.4,9001,1,default,,0,0,0", clusterLines[2])
-		require.Equal(t, "events,3,1,1,dalem-local-clickhouse-blue-3,192.168.144.3,9002,1,default,,0,0,0", clusterLines[3])
-		//test our directory frame
-		for _, filepath := range files {
-			// check they were copied
-			subPath := strings.TrimPrefix(filepath, tempDir)
-			// path here will be <destDir>/<id>/test>/config/logs/<sub path>
-			newPath := path.Join(destDir, "test", "test", "config", "logs", subPath)
-			require.FileExists(t, newPath)
-		}
-		resetFrames()
-	})
-
-	t.Run("test support for config frames", func(t *testing.T) {
-		xmlConfig := data.XmlConfig{
-			XMLName: xml.Name{},
-			Clickhouse: data.XmlLoggerConfig{
-				XMLName:  xml.Name{},
-				ErrorLog: "/var/log/clickhouse-server/clickhouse-server.err.log",
-				Log:      "/var/log/clickhouse-server/clickhouse-server.log",
-			},
-			IncludeFrom: "",
-		}
-		tempDir := t.TempDir()
-		confDir := path.Join(tempDir, "conf")
-		// create an includes file
-		includesDir := path.Join(tempDir, "includes")
-		err := os.MkdirAll(includesDir, os.ModePerm)
-		require.Nil(t, err)
-		includesPath := path.Join(includesDir, "random.xml")
-		includesFile, err := os.Create(includesPath)
-		require.Nil(t, err)
-		xmlWriter := io.Writer(includesFile)
-		enc := xml.NewEncoder(xmlWriter)
-		enc.Indent("  ", "    ")
-		err = enc.Encode(xmlConfig)
-		require.Nil(t, err)
-		// create 5 temporary config files
-		files := make([]string, 5)
-		// set the includes
-		xmlConfig.IncludeFrom = includesPath
-		for i := 0; i < 5; i++ {
-			// we want to check hierarchies are preserved so create a simple folder for each file
-			fileDir := path.Join(confDir, fmt.Sprintf("%d", i))
-			err := os.MkdirAll(fileDir, os.ModePerm)
-			require.Nil(t, err)
-			filepath := path.Join(fileDir, fmt.Sprintf("random-%d.xml", i))
-			files[i] = filepath
-			xmlFile, err := os.Create(filepath)
-			require.Nil(t, err)
-			// write a little xml so its valid
-			xmlWriter := io.Writer(xmlFile)
-			enc := xml.NewEncoder(xmlWriter)
-			enc.Indent("  ", "    ")
-			err = enc.Encode(xmlConfig)
-			require.Nil(t, err)
-		}
-		configFrame, errs := data.NewConfigFileFrame(confDir)
-		require.Empty(t, errs)
-		fileBundles := map[string]*data.DiagnosticBundle{
-			"systemA": {
-				Frames: map[string]data.Frame{
-					"disk":    diskFrame,
-					"cluster": clusterFrame,
-				},
-			},
-			"config": {
-				Frames: map[string]data.Frame{
-					"user_specified": configFrame,
-				},
-			},
-		}
-		destDir := t.TempDir()
-		configuration := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Param: config.NewParam("directory", "A directory", true),
-					Value: destDir,
-				},
-				// turn compression off as the folder will be deleted by default
-				config.BoolParam{
-					Value: true,
-					Param: config.NewParam("skip_archive", "Skip archive", false),
-				},
-			},
-		}
-		output := file.SimpleOutput{FolderGenerator: staticFolderName}
-		frameErrors, err := output.Write("test", fileBundles, configuration)
-		require.Nil(t, err)
-		require.NotNil(t, frameErrors)
-		require.Empty(t, frameErrors.Errors)
-		//test our config frame
-		for _, filepath := range files {
-			// check they were copied
-			subPath := strings.TrimPrefix(filepath, confDir)
-			// path here will be <destDir>/<id>/test>/config/user_specified/file
-			newPath := path.Join(destDir, "test", "test", "config", "user_specified", subPath)
-			require.FileExists(t, newPath)
-		}
-		// check our includes file exits
-		// path here will be <destDir>/<id>/test>/config/user_specified/file/includes
-		require.FileExists(t, path.Join(destDir, "test", "test", "config", "user_specified", "includes", includesPath))
-		resetFrames()
-	})
-
-	t.Run("test support for file frames", func(t *testing.T) {
-		// create 5 temporary files
-		tempDir := t.TempDir()
-		files := createRandomFiles(tempDir, 5)
-		fileFrame := data.NewFileFrame("collection", files)
-		fileBundles := map[string]*data.DiagnosticBundle{
-			"systemA": {
-				Frames: map[string]data.Frame{
-					"disk":    diskFrame,
-					"cluster": clusterFrame,
-				},
-			},
-			"file": {
-				Frames: map[string]data.Frame{
-					"collection": fileFrame,
-				},
-			},
-		}
-		destDir := t.TempDir()
-		configuration := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Param: config.NewParam("directory", "A directory", true),
-					Value: destDir,
-				},
-				// turn compression off as the folder will be deleted by default
-				config.BoolParam{
-					Value: true,
-					Param: config.NewParam("skip_archive", "Skip archive", false),
-				},
-			},
-		}
-		output := file.SimpleOutput{FolderGenerator: staticFolderName}
-		frameErrors, err := output.Write("test", fileBundles, configuration)
-		require.Nil(t, err)
-		require.NotNil(t, frameErrors)
-		//test our directory frame
-		for _, filepath := range files {
-			// path here will be <destDir>/<id>/test>/file/collection/<sub path>
-			newPath := path.Join(destDir, "test", "test", "file", "collection", filepath)
-			require.FileExists(t, newPath)
-		}
-		resetFrames()
-	})
-
-	t.Run("test support for hierarchical frames", func(t *testing.T) {
-		bottomFrame := data.NewHierarchicalFrame("bottomLevel", userFrame, []data.HierarchicalFrame{})
-		middleFrame := data.NewHierarchicalFrame("middleLevel", diskFrame, []data.HierarchicalFrame{bottomFrame})
-		topFrame := data.NewHierarchicalFrame("topLevel", clusterFrame, []data.HierarchicalFrame{middleFrame})
-		tempDir := t.TempDir()
-		configuration := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Param: config.NewParam("directory", "A directory", true),
-					Value: tempDir,
-				},
-				// turn compression off as the folder will be deleted by default
-				config.BoolParam{
-					Value: true,
-					Param: config.NewParam("skip_archive", "Skip archive", false),
-				},
-			},
-		}
-		output := file.SimpleOutput{FolderGenerator: staticFolderName}
-		hierarchicalBundle := map[string]*data.DiagnosticBundle{
-			"systemA": {
-				Frames: map[string]data.Frame{
-					"topLevel": topFrame,
-				},
-			},
-		}
-		frameErrors, err := output.Write("test", hierarchicalBundle, configuration)
-		require.Nil(t, err)
-		require.Equal(t, data.FrameErrors{}, frameErrors)
-		topFile := path.Join(tempDir, "test", "test", "systemA", "topLevel.csv")
-		middleFile := path.Join(tempDir, "test", "test", "systemA", "middleLevel", "middleLevel.csv")
-		bottomFile := path.Join(tempDir, "test", "test", "systemA", "middleLevel", "bottomLevel", "bottomLevel.csv")
-		require.FileExists(t, topFile)
-		require.FileExists(t, middleFile)
-		require.FileExists(t, bottomFile)
-		topLines, err := readFileLines(topFile)
-		require.Nil(t, err)
-		require.Len(t, topLines, 4)
-		middleLines, err := readFileLines(middleFile)
-		require.Nil(t, err)
-		require.Len(t, middleLines, 2)
-		bottomLines, err := readFileLines(bottomFile)
-		require.Nil(t, err)
-		require.Len(t, bottomLines, 2)
-		require.Equal(t, strings.Join(clusterFrame.ColumnNames, ","), topLines[0])
-		require.Equal(t, "events,1,1,1,dalem-local-clickhouse-blue-1,192.168.144.2,9000,1,default,,0,0,0", topLines[1])
-		require.Equal(t, "events,2,1,1,dalem-local-clickhouse-blue-2,192.168.144.4,9001,1,default,,0,0,0", topLines[2])
-		require.Equal(t, "events,3,1,1,dalem-local-clickhouse-blue-3,192.168.144.3,9002,1,default,,0,0,0", topLines[3])
-		resetFrames()
-	})
-}
-
-func createRandomFiles(tempDir string, num int) []string {
-	files := make([]string, num)
-	for i := 0; i < 5; i++ {
-		// we want to check hierarchies are preserved so create a simple folder for each file
-		fileDir := path.Join(tempDir, fmt.Sprintf("%d", i))
-		os.MkdirAll(fileDir, os.ModePerm) //nolint:errcheck
-		filepath := path.Join(fileDir, fmt.Sprintf("random-%d.log", i))
-		files[i] = filepath
-		os.Create(filepath) //nolint:errcheck
-	}
-	return files
-}
-
-func resetFrames() {
-	clusterFrame.Reset()
-	userFrame.Reset()
-	diskFrame.Reset()
-}
-
-func readFileLines(filename string) ([]string, error) {
-	file, err := os.Open(filename)
-	if err != nil {
-		return nil, err
-	}
-	defer file.Close()
-
-	var lines []string
-	scanner := bufio.NewScanner(file)
-	for scanner.Scan() {
-		lines = append(lines, scanner.Text())
-	}
-	return lines, scanner.Err()
-}
-
-func staticFolderName() string {
-	return "test"
-}
diff --git a/programs/diagnostics/internal/outputs/registry_test.go b/programs/diagnostics/internal/outputs/registry_test.go
deleted file mode 100644
index ba8408e5a594..000000000000
--- a/programs/diagnostics/internal/outputs/registry_test.go
+++ /dev/null
@@ -1,45 +0,0 @@
-package outputs_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/file"
-	_ "github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/terminal"
-	"github.com/stretchr/testify/require"
-)
-
-func TestGetOutputNames(t *testing.T) {
-	t.Run("can get all output names", func(t *testing.T) {
-		outputNames := outputs.GetOutputNames()
-		require.ElementsMatch(t, []string{"simple", "report"}, outputNames)
-	})
-
-}
-
-func TestGetOutputByName(t *testing.T) {
-
-	t.Run("can get output by name", func(t *testing.T) {
-		output, err := outputs.GetOutputByName("simple")
-		require.Nil(t, err)
-		require.Equal(t, file.SimpleOutput{}, output)
-	})
-
-	t.Run("fails on non existing output", func(t *testing.T) {
-		output, err := outputs.GetOutputByName("random")
-		require.NotNil(t, err)
-		require.Equal(t, "random is not a valid output name", err.Error())
-		require.Nil(t, output)
-	})
-}
-
-func TestBuildConfigurationOptions(t *testing.T) {
-
-	t.Run("can get all output configurations", func(t *testing.T) {
-		outputs, err := outputs.BuildConfigurationOptions()
-		require.Nil(t, err)
-		require.Len(t, outputs, 2)
-		require.Contains(t, outputs, "simple")
-		require.Contains(t, outputs, "report")
-	})
-}
diff --git a/programs/diagnostics/internal/platform/config/models_test.go b/programs/diagnostics/internal/platform/config/models_test.go
deleted file mode 100644
index 916d20ec28b1..000000000000
--- a/programs/diagnostics/internal/platform/config/models_test.go
+++ /dev/null
@@ -1,182 +0,0 @@
-package config_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/stretchr/testify/require"
-)
-
-var conf = config.Configuration{
-	Params: []config.ConfigParam{
-		config.StringListParam{
-			Values: []string{"some", "values"},
-			Param:  config.NewParam("paramA", "", false),
-		},
-		config.StringParam{
-			Value: "random",
-			Param: config.NewParam("paramB", "", true),
-		},
-		config.StringParam{
-			Value:      "",
-			AllowEmpty: true,
-			Param:      config.NewParam("paramC", "", false),
-		},
-		config.StringOptions{
-			Value:      "random",
-			Options:    []string{"random", "very_random", "very_very_random"},
-			Param:      config.NewParam("paramD", "", false),
-			AllowEmpty: true,
-		},
-	},
-}
-
-func TestGetConfigParam(t *testing.T) {
-
-	t.Run("can find get config param by name", func(t *testing.T) {
-		paramA, err := conf.GetConfigParam("paramA")
-		require.Nil(t, err)
-		require.NotNil(t, paramA)
-		require.IsType(t, config.StringListParam{}, paramA)
-		stringListParam, ok := paramA.(config.StringListParam)
-		require.True(t, ok)
-		require.False(t, stringListParam.Required())
-		require.Equal(t, stringListParam.Name(), "paramA")
-		require.ElementsMatch(t, stringListParam.Values, []string{"some", "values"})
-	})
-
-	t.Run("throws error on missing element", func(t *testing.T) {
-		paramZ, err := conf.GetConfigParam("paramZ")
-		require.Nil(t, paramZ)
-		require.NotNil(t, err)
-		require.Equal(t, err.Error(), "paramZ does not exist")
-	})
-}
-
-func TestValidateConfig(t *testing.T) {
-
-	t.Run("validate adds the default and allows override", func(t *testing.T) {
-		customConf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value: "custom",
-					Param: config.NewParam("paramB", "", true),
-				},
-			},
-		}
-		newConf, err := customConf.ValidateConfig(conf)
-		require.Nil(t, err)
-		require.NotNil(t, newConf)
-		require.Len(t, newConf.Params, 4)
-		// check first param
-		require.IsType(t, config.StringListParam{}, newConf.Params[0])
-		stringListParam, ok := newConf.Params[0].(config.StringListParam)
-		require.True(t, ok)
-		require.False(t, stringListParam.Required())
-		require.Equal(t, stringListParam.Name(), "paramA")
-		require.ElementsMatch(t, stringListParam.Values, []string{"some", "values"})
-		// check second param
-		require.IsType(t, config.StringParam{}, newConf.Params[1])
-		stringParam, ok := newConf.Params[1].(config.StringParam)
-		require.True(t, ok)
-		require.True(t, stringParam.Required())
-		require.Equal(t, "paramB", stringParam.Name())
-		require.Equal(t, "custom", stringParam.Value)
-	})
-
-	t.Run("validate errors if missing param", func(t *testing.T) {
-		//missing required paramB
-		customConf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringListParam{
-					Values: []string{"some", "values"},
-					Param:  config.NewParam("paramA", "", false),
-				},
-			},
-		}
-		newConf, err := customConf.ValidateConfig(conf)
-		require.Nil(t, newConf.Params)
-		require.NotNil(t, err)
-		require.Equal(t, "missing required parameter paramB - paramB does not exist", err.Error())
-	})
-
-	t.Run("validate errors if invalid string value", func(t *testing.T) {
-		//missing required paramB
-		customConf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value: "",
-					Param: config.NewParam("paramB", "", true),
-				},
-			},
-		}
-		newConf, err := customConf.ValidateConfig(conf)
-		require.Nil(t, newConf.Params)
-		require.NotNil(t, err)
-		require.Equal(t, "parameter paramB is invalid - paramB cannot be empty", err.Error())
-	})
-
-	t.Run("allow empty string value if specified", func(t *testing.T) {
-		//missing required paramB
-		customConf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value: "",
-					Param: config.NewParam("paramC", "", true),
-				},
-				config.StringParam{
-					Value: "custom",
-					Param: config.NewParam("paramB", "", true),
-				},
-			},
-		}
-		newConf, err := customConf.ValidateConfig(conf)
-		require.NotNil(t, newConf.Params)
-		require.Nil(t, err)
-	})
-
-	t.Run("validate errors if invalid string options value", func(t *testing.T) {
-		//missing required paramB
-		customConf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value: "not_random",
-					Param: config.NewParam("paramB", "", true),
-				},
-				config.StringOptions{
-					Value: "custom",
-					// this isn't ideal we need to ensure options are set for this to validate correctly
-					Options: []string{"random", "very_random", "very_very_random"},
-					Param:   config.NewParam("paramD", "", true),
-				},
-			},
-		}
-		newConf, err := customConf.ValidateConfig(conf)
-		require.Nil(t, newConf.Params)
-		require.NotNil(t, err)
-		require.Equal(t, "parameter paramD is invalid - custom is not a valid value for paramD - [random very_random very_very_random]", err.Error())
-	})
-
-	t.Run("allow empty string value for StringOptions if specified", func(t *testing.T) {
-		//missing required paramB
-		customConf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringParam{
-					Value: "custom",
-					Param: config.NewParam("paramB", "", true),
-				},
-				config.StringOptions{
-					Param: config.Param{},
-					// this isn't ideal we need to ensure options are set for this to validate correctly
-					Options: []string{"random", "very_random", "very_very_random"},
-					Value:   "",
-				},
-			},
-		}
-		newConf, err := customConf.ValidateConfig(conf)
-		require.NotNil(t, newConf.Params)
-		require.Nil(t, err)
-	})
-
-	//TODO: Do we need to test if parameters of the same name but wrong type are passed??
-}
diff --git a/programs/diagnostics/internal/platform/config/utils_test.go b/programs/diagnostics/internal/platform/config/utils_test.go
deleted file mode 100644
index 9e03e5e69d2c..000000000000
--- a/programs/diagnostics/internal/platform/config/utils_test.go
+++ /dev/null
@@ -1,142 +0,0 @@
-package config_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/stretchr/testify/require"
-)
-
-func TestReadStringListValues(t *testing.T) {
-
-	t.Run("can find a string list param", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringListParam{
-					// nil means include everything
-					Values: nil,
-					Param:  config.NewParam("include_tables", "Specify list of tables to collect", false),
-				},
-				config.StringListParam{
-					Values: []string{"licenses", "settings"},
-					Param:  config.NewParam("exclude_tables", "Specify list of tables not to collect", false),
-				},
-			},
-		}
-		excludeTables, err := config.ReadStringListValues(conf, "exclude_tables")
-		require.Nil(t, err)
-		require.Equal(t, []string{"licenses", "settings"}, excludeTables)
-	})
-
-}
-
-func TestReadStringValue(t *testing.T) {
-
-	t.Run("can find a string param", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringListParam{
-					// nil means include everything
-					Values: nil,
-					Param:  config.NewParam("include_tables", "Specify list of tables to collect", false),
-				},
-				config.StringParam{
-					Value: "/tmp/dump",
-					Param: config.NewParam("directory", "Specify a directory", false),
-				},
-			},
-		}
-		directory, err := config.ReadStringValue(conf, "directory")
-		require.Nil(t, err)
-		require.Equal(t, "/tmp/dump", directory)
-	})
-
-}
-
-func TestReadIntValue(t *testing.T) {
-	t.Run("can find an integer param", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.IntParam{
-					// nil means include everything
-					Value: 10000,
-					Param: config.NewParam("row_limit", "Max Rows to collect", false),
-				},
-				config.StringListParam{
-					// nil means include everything
-					Values: nil,
-					Param:  config.NewParam("include_tables", "Specify list of tables to collect", false),
-				},
-				config.StringParam{
-					Value: "/tmp/dump",
-					Param: config.NewParam("directory", "Specify a directory", false),
-				},
-			},
-		}
-		rowLimit, err := config.ReadIntValue(conf, "row_limit")
-		require.Nil(t, err)
-		require.Equal(t, int64(10000), rowLimit)
-	})
-
-}
-
-func TestReadBoolValue(t *testing.T) {
-	t.Run("can find a boolean param", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.BoolParam{
-					// nil means include everything
-					Value: true,
-					Param: config.NewParam("compress", "Compress data", false),
-				},
-				config.StringListParam{
-					// nil means include everything
-					Values: nil,
-					Param:  config.NewParam("include_tables", "Specify list of tables to collect", false),
-				},
-				config.StringParam{
-					Value: "/tmp/dump",
-					Param: config.NewParam("directory", "Specify a directory", false),
-				},
-			},
-		}
-
-		compress, err := config.ReadBoolValue(conf, "compress")
-		require.Nil(t, err)
-		require.True(t, compress)
-	})
-}
-
-func TestReadStringOptionsValue(t *testing.T) {
-	t.Run("can find a string value in a list of options", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringOptions{
-					Param:      config.NewParam("format", "List of formats", false),
-					Options:    []string{"csv", "tsv", "binary", "json", "ndjson"},
-					Value:      "csv",
-					AllowEmpty: false,
-				},
-			},
-		}
-		format, err := config.ReadStringOptionsValue(conf, "format")
-		require.Nil(t, err)
-		require.Equal(t, "csv", format)
-	})
-
-	t.Run("errors on invalid value", func(t *testing.T) {
-		conf := config.Configuration{
-			Params: []config.ConfigParam{
-				config.StringOptions{
-					Param:      config.NewParam("format", "List of formats", false),
-					Options:    []string{"csv", "tsv", "binary", "json", "ndjson"},
-					Value:      "random",
-					AllowEmpty: false,
-				},
-			},
-		}
-		format, err := config.ReadStringOptionsValue(conf, "format")
-		require.Equal(t, "random is not a valid option in [csv tsv binary json ndjson] for the the parameter format", err.Error())
-		require.Equal(t, "", format)
-	})
-}
diff --git a/programs/diagnostics/internal/platform/data/bundle_test.go b/programs/diagnostics/internal/platform/data/bundle_test.go
deleted file mode 100644
index ff9cfc2cf567..000000000000
--- a/programs/diagnostics/internal/platform/data/bundle_test.go
+++ /dev/null
@@ -1,26 +0,0 @@
-package data_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/pkg/errors"
-	"github.com/stretchr/testify/require"
-)
-
-func TestBundleError(t *testing.T) {
-
-	t.Run("can get a bundle error", func(t *testing.T) {
-		errs := make([]error, 3)
-		errs[0] = errors.New("Error 1")
-		errs[1] = errors.New("Error 2")
-		errs[2] = errors.New("Error 3")
-		fErrors := data.FrameErrors{
-			Errors: errs,
-		}
-		require.Equal(t, `Error 1
-Error 2
-Error 3`, fErrors.Error())
-
-	})
-}
diff --git a/programs/diagnostics/internal/platform/data/database_test.go b/programs/diagnostics/internal/platform/data/database_test.go
deleted file mode 100644
index 57d89e78efc2..000000000000
--- a/programs/diagnostics/internal/platform/data/database_test.go
+++ /dev/null
@@ -1,86 +0,0 @@
-package data_test
-
-import (
-	"database/sql"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/DATA-DOG/go-sqlmock"
-	"github.com/stretchr/testify/require"
-)
-
-func TestString(t *testing.T) {
-	t.Run("can order by asc", func(t *testing.T) {
-		orderBy := data.OrderBy{
-			Column: "created_at",
-			Order:  data.Asc,
-		}
-		require.Equal(t, " ORDER BY created_at ASC", orderBy.String())
-	})
-
-	t.Run("can order by desc", func(t *testing.T) {
-		orderBy := data.OrderBy{
-			Column: "created_at",
-			Order:  data.Desc,
-		}
-		require.Equal(t, " ORDER BY created_at DESC", orderBy.String())
-	})
-
-}
-
-func TestNextDatabaseFrame(t *testing.T) {
-
-	t.Run("can iterate sql rows", func(t *testing.T) {
-		rowValues := [][]interface{}{
-			{int64(1), "post_1", "hello"},
-			{int64(2), "post_2", "world"},
-			{int64(3), "post_3", "goodbye"},
-			{int64(4), "post_4", "world"},
-		}
-		mockRows := sqlmock.NewRows([]string{"id", "title", "body"})
-		for i := range rowValues {
-			mockRows.AddRow(rowValues[i][0], rowValues[i][1], rowValues[i][2])
-		}
-		rows := mockRowsToSqlRows(mockRows)
-		dbFrame, err := data.NewDatabaseFrame("test", rows)
-		require.ElementsMatch(t, dbFrame.Columns(), []string{"id", "title", "body"})
-		require.Nil(t, err)
-		i := 0
-		for {
-			values, ok, err := dbFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-			require.Len(t, values, 3)
-			require.ElementsMatch(t, values, rowValues[i])
-			i++
-		}
-		require.Equal(t, 4, i)
-	})
-
-	t.Run("can iterate empty sql rows", func(t *testing.T) {
-		mockRows := sqlmock.NewRows([]string{"id", "title", "body"})
-		rows := mockRowsToSqlRows(mockRows)
-		dbFrame, err := data.NewDatabaseFrame("test", rows)
-		require.ElementsMatch(t, dbFrame.Columns(), []string{"id", "title", "body"})
-		require.Nil(t, err)
-		i := 0
-		for {
-			_, ok, err := dbFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-			i++
-		}
-		require.Equal(t, 0, i)
-	})
-}
-
-func mockRowsToSqlRows(mockRows *sqlmock.Rows) *sql.Rows {
-	db, mock, _ := sqlmock.New()
-	mock.ExpectQuery("select").WillReturnRows(mockRows)
-	rows, _ := db.Query("select")
-	return rows
-}
diff --git a/programs/diagnostics/internal/platform/data/file_test.go b/programs/diagnostics/internal/platform/data/file_test.go
deleted file mode 100644
index 9e305b1a5daa..000000000000
--- a/programs/diagnostics/internal/platform/data/file_test.go
+++ /dev/null
@@ -1,263 +0,0 @@
-package data_test
-
-import (
-	"fmt"
-	"io/ioutil"
-	"os"
-	"path"
-	"path/filepath"
-	"strings"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/stretchr/testify/require"
-)
-
-func TestNextFileDirectoryFrame(t *testing.T) {
-	t.Run("can iterate file frame", func(t *testing.T) {
-		tempDir := t.TempDir()
-		files := make([]string, 5)
-		for i := 0; i < 5; i++ {
-			fileDir := path.Join(tempDir, fmt.Sprintf("%d", i))
-			err := os.MkdirAll(fileDir, os.ModePerm)
-			require.Nil(t, err)
-			filepath := path.Join(fileDir, fmt.Sprintf("random-%d.txt", i))
-			files[i] = filepath
-			_, err = os.Create(filepath)
-			require.Nil(t, err)
-		}
-		fileFrame, errs := data.NewFileDirectoryFrame(tempDir, []string{"*.txt"})
-		require.Empty(t, errs)
-		i := 0
-		for {
-			values, ok, err := fileFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-			require.Len(t, values, 1)
-			require.Equal(t, files[i], values[0].(data.SimpleFile).Path)
-			i += 1
-		}
-		require.Equal(t, 5, i)
-	})
-
-	t.Run("can iterate file frame when empty", func(t *testing.T) {
-		// create 5 temporary files
-		tempDir := t.TempDir()
-		fileFrame, errs := data.NewFileDirectoryFrame(tempDir, []string{"*"})
-		require.Empty(t, errs)
-		i := 0
-		for {
-			_, ok, err := fileFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-		}
-		require.Equal(t, 0, i)
-	})
-}
-
-func TestNewConfigFileFrame(t *testing.T) {
-	t.Run("can iterate config file frame", func(t *testing.T) {
-		cwd, err := os.Getwd()
-		require.Nil(t, err)
-
-		configFrame, errs := data.NewConfigFileFrame(path.Join(cwd, "../../../testdata", "configs", "xml"))
-		require.Empty(t, errs)
-		i := 0
-		for {
-			values, ok, err := configFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-			require.Len(t, values, 1)
-			filePath := values[0].(data.XmlConfigFile).FilePath()
-			require.True(t, strings.Contains(filePath, ".xml"))
-			i += 1
-		}
-		// 5 not 3 due to the includes
-		require.Equal(t, 5, i)
-	})
-
-	t.Run("can iterate file frame when empty", func(t *testing.T) {
-		// create 5 temporary files
-		tempDir := t.TempDir()
-		configFrame, errs := data.NewConfigFileFrame(tempDir)
-		require.Empty(t, errs)
-		i := 0
-		for {
-			_, ok, err := configFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-		}
-		require.Equal(t, 0, i)
-	})
-}
-
-func TestConfigFileFrameCopy(t *testing.T) {
-	t.Run("can copy non-sensitive xml config files", func(t *testing.T) {
-		tmrDir := t.TempDir()
-		cwd, err := os.Getwd()
-		require.Nil(t, err)
-		configFrame, errs := data.NewConfigFileFrame(path.Join(cwd, "../../../testdata", "configs", "xml"))
-		require.Empty(t, errs)
-		for {
-			values, ok, err := configFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-			require.Nil(t, err)
-			require.True(t, ok)
-			configFile := values[0].(data.XmlConfigFile)
-			newPath := path.Join(tmrDir, filepath.Base(configFile.FilePath()))
-			err = configFile.Copy(newPath, false)
-			require.FileExists(t, newPath)
-			sourceInfo, _ := os.Stat(configFile.FilePath())
-			destInfo, _ := os.Stat(newPath)
-			require.Equal(t, sourceInfo.Size(), destInfo.Size())
-			require.Nil(t, err)
-		}
-	})
-
-	t.Run("can copy sensitive xml config files", func(t *testing.T) {
-		tmrDir := t.TempDir()
-		cwd, err := os.Getwd()
-		require.Nil(t, err)
-		configFrame, errs := data.NewConfigFileFrame(path.Join(cwd, "../../../testdata", "configs", "xml"))
-		require.Empty(t, errs)
-		i := 0
-		var checkedFiles []string
-		for {
-			values, ok, err := configFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-			require.Nil(t, err)
-			require.True(t, ok)
-			configFile := values[0].(data.XmlConfigFile)
-			fileName := filepath.Base(configFile.FilePath())
-			newPath := path.Join(tmrDir, fileName)
-			err = configFile.Copy(newPath, true)
-			require.FileExists(t, newPath)
-			require.Nil(t, err)
-			bytes, err := ioutil.ReadFile(newPath)
-			require.Nil(t, err)
-			s := string(bytes)
-			checkedFiles = append(checkedFiles, fileName)
-			if fileName == "users.xml" || fileName == "default-password.xml" || fileName == "user-include.xml" {
-				require.True(t, strings.Contains(s, "<password>Replaced</password>") ||
-					strings.Contains(s, "<password_sha256_hex>Replaced</password_sha256_hex>"))
-				require.NotContains(t, s, "<password>REPLACE_ME</password>")
-				require.NotContains(t, s, "<password_sha256_hex>REPLACE_ME</password_sha256_hex>")
-			} else if fileName == "config.xml" {
-				require.True(t, strings.Contains(s, "<access_key_id>Replaced</access_key_id>"))
-				require.True(t, strings.Contains(s, "<secret_access_key>Replaced</secret_access_key>"))
-				require.True(t, strings.Contains(s, "<secret>Replaced</secret>"))
-				require.NotContains(t, s, "<access_key_id>REPLACE_ME</access_key_id>")
-				require.NotContains(t, s, "<secret_access_key>REPLACE_ME</secret_access_key>")
-				require.NotContains(t, s, "<secret>REPLACE_ME</secret>")
-			}
-			i++
-		}
-		require.ElementsMatch(t, []string{"users.xml", "default-password.xml", "user-include.xml", "config.xml", "server-include.xml"}, checkedFiles)
-		require.Equal(t, 5, i)
-	})
-
-	t.Run("can copy sensitive yaml config files", func(t *testing.T) {
-		tmrDir := t.TempDir()
-		cwd, err := os.Getwd()
-		require.Nil(t, err)
-		configFrame, errs := data.NewConfigFileFrame(path.Join(cwd, "../../../testdata", "configs", "yaml"))
-		require.Empty(t, errs)
-		i := 0
-		var checkedFiles []string
-		for {
-			values, ok, err := configFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-			require.Nil(t, err)
-			require.True(t, ok)
-			configFile := values[0].(data.YamlConfigFile)
-			fileName := filepath.Base(configFile.FilePath())
-			newPath := path.Join(tmrDir, fileName)
-			err = configFile.Copy(newPath, true)
-			require.FileExists(t, newPath)
-			require.Nil(t, err)
-			bytes, err := ioutil.ReadFile(newPath)
-			require.Nil(t, err)
-			s := string(bytes)
-			checkedFiles = append(checkedFiles, fileName)
-			if fileName == "users.yaml" || fileName == "default-password.yaml" || fileName == "user-include.yaml" {
-				require.True(t, strings.Contains(s, "password: 'Replaced'") ||
-					strings.Contains(s, "password_sha256_hex: 'Replaced'"))
-				require.NotContains(t, s, "password: 'REPLACE_ME'")
-				require.NotContains(t, s, "password_sha256_hex: \"REPLACE_ME\"")
-			} else if fileName == "config.yaml" {
-				require.True(t, strings.Contains(s, "access_key_id: 'Replaced'"))
-				require.True(t, strings.Contains(s, "secret_access_key: 'Replaced'"))
-				require.True(t, strings.Contains(s, "secret: 'Replaced'"))
-				require.NotContains(t, s, "access_key_id: 'REPLACE_ME'")
-				require.NotContains(t, s, "secret_access_key: REPLACE_ME")
-				require.NotContains(t, s, "secret: REPLACE_ME")
-			}
-			i++
-		}
-		require.ElementsMatch(t, []string{"users.yaml", "default-password.yaml", "user-include.yaml", "config.yaml", "server-include.yaml"}, checkedFiles)
-		require.Equal(t, 5, i)
-	})
-}
-
-func TestConfigFileFrameFindLogPaths(t *testing.T) {
-	t.Run("can find xml log paths", func(t *testing.T) {
-		cwd, err := os.Getwd()
-		require.Nil(t, err)
-		configFrame, errs := data.NewConfigFileFrame(path.Join(cwd, "../../../testdata", "configs", "xml"))
-		require.Empty(t, errs)
-		paths, errs := configFrame.FindLogPaths()
-		require.Empty(t, errs)
-		require.ElementsMatch(t, []string{"/var/log/clickhouse-server/clickhouse-server.log",
-			"/var/log/clickhouse-server/clickhouse-server.err.log"}, paths)
-	})
-
-	t.Run("can handle empty log paths", func(t *testing.T) {
-		configFrame, errs := data.NewConfigFileFrame(t.TempDir())
-		require.Empty(t, errs)
-		paths, errs := configFrame.FindLogPaths()
-		require.Empty(t, errs)
-		require.Empty(t, paths)
-	})
-
-	t.Run("can find yaml log paths", func(t *testing.T) {
-		cwd, err := os.Getwd()
-		require.Nil(t, err)
-		configFrame, errs := data.NewConfigFileFrame(path.Join(cwd, "../../../testdata", "configs", "yaml"))
-		require.Empty(t, errs)
-		paths, errs := configFrame.FindLogPaths()
-		require.Empty(t, errs)
-		require.ElementsMatch(t, []string{"/var/log/clickhouse-server/clickhouse-server.log",
-			"/var/log/clickhouse-server/clickhouse-server.err.log"}, paths)
-	})
-}
-
-// test the legacy format for ClickHouse xml config files with a yandex root tag
-func TestYandexConfigFile(t *testing.T) {
-	t.Run("can find xml log paths with yandex root", func(t *testing.T) {
-		cwd, err := os.Getwd()
-		require.Nil(t, err)
-		configFrame, errs := data.NewConfigFileFrame(path.Join(cwd, "../../../testdata", "configs", "yandex_xml"))
-		require.Empty(t, errs)
-		paths, errs := configFrame.FindLogPaths()
-		require.Empty(t, errs)
-		require.ElementsMatch(t, []string{"/var/log/clickhouse-server/clickhouse-server.log",
-			"/var/log/clickhouse-server/clickhouse-server.err.log"}, paths)
-	})
-}
diff --git a/programs/diagnostics/internal/platform/data/memory_test.go b/programs/diagnostics/internal/platform/data/memory_test.go
deleted file mode 100644
index fcc02e37d324..000000000000
--- a/programs/diagnostics/internal/platform/data/memory_test.go
+++ /dev/null
@@ -1,61 +0,0 @@
-package data_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/stretchr/testify/require"
-)
-
-func TestNextMemoryFrame(t *testing.T) {
-	t.Run("can iterate memory frame", func(t *testing.T) {
-		columns := []string{"Filesystem", "Size", "Used", "Avail", "Use%", "Mounted on"}
-		rows := [][]interface{}{
-			{"sysfs", 0, 0, 0, 0, "/sys"},
-			{"proc", 0, 0, 0, 0, "/proc"},
-			{"udev", 33357840384, 0, 33357840384, 0, "/dev"},
-			{"devpts", 0, 0, 0, 0, "/dev/pts"},
-			{"tmpfs", 6682607616, 2228224, 6680379392, 1, "/run"},
-			{"/dev/mapper/system-root", 1938213220352, 118136926208, 1721548947456, 7.000000000000001, "/"},
-		}
-		memoryFrame := data.NewMemoryFrame("disks", columns, rows)
-		i := 0
-		for {
-			values, ok, err := memoryFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-			require.ElementsMatch(t, values, rows[i])
-			require.Len(t, values, 6)
-			i += 1
-		}
-		require.Equal(t, 6, i)
-	})
-
-	t.Run("can iterate memory frame when empty", func(t *testing.T) {
-		memoryFrame := data.NewMemoryFrame("test", []string{}, [][]interface{}{})
-		i := 0
-		for {
-			_, ok, err := memoryFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-		}
-		require.Equal(t, 0, i)
-	})
-
-	t.Run("can iterate memory frame when empty", func(t *testing.T) {
-		memoryFrame := data.MemoryFrame{}
-		i := 0
-		for {
-			_, ok, err := memoryFrame.Next()
-			require.Nil(t, err)
-			if !ok {
-				break
-			}
-		}
-		require.Equal(t, 0, i)
-	})
-}
diff --git a/programs/diagnostics/internal/platform/database/native_test.go b/programs/diagnostics/internal/platform/database/native_test.go
deleted file mode 100644
index 7028a4b4800b..000000000000
--- a/programs/diagnostics/internal/platform/database/native_test.go
+++ /dev/null
@@ -1,289 +0,0 @@
-//go:build !no_docker
-
-package database_test
-
-import (
-	"context"
-	"fmt"
-	"os"
-	"path"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/database"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/docker/go-connections/nat"
-	"github.com/stretchr/testify/require"
-	"github.com/testcontainers/testcontainers-go"
-	"github.com/testcontainers/testcontainers-go/wait"
-)
-
-func createClickHouseContainer(t *testing.T, ctx context.Context) (testcontainers.Container, nat.Port) {
-	// create a ClickHouse container
-	cwd, err := os.Getwd()
-	if err != nil {
-		// can't test without current directory
-		panic(err)
-	}
-
-	// for now, we test against a hardcoded database-server version but we should make this a property
-	req := testcontainers.ContainerRequest{
-		Image:        fmt.Sprintf("clickhouse/clickhouse-server:%s", test.GetClickHouseTestVersion()),
-		ExposedPorts: []string{"9000/tcp"},
-		WaitingFor:   wait.ForLog("Ready for connections"),
-		Mounts: testcontainers.ContainerMounts{
-			{
-				Source: testcontainers.GenericBindMountSource{
-					HostPath: path.Join(cwd, "../../../testdata/docker/custom.xml"),
-				},
-				Target: "/etc/clickhouse-server/config.d/custom.xml",
-			},
-			{
-				Source: testcontainers.GenericBindMountSource{
-					HostPath: path.Join(cwd, "../../../testdata/docker/admin.xml"),
-				},
-				Target: "/etc/clickhouse-server/users.d/admin.xml",
-			},
-		},
-	}
-	clickhouseContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
-		ContainerRequest: req,
-		Started:          true,
-	})
-	if err != nil {
-		// can't test without container
-		panic(err)
-	}
-
-	p, _ := clickhouseContainer.MappedPort(ctx, "9000")
-	if err != nil {
-		// can't test without container's port
-		panic(err)
-	}
-
-	t.Setenv("CLICKHOUSE_DB_PORT", p.Port())
-
-	return clickhouseContainer, p
-}
-
-func getClient(t *testing.T, mappedPort int) *database.ClickhouseNativeClient {
-	clickhouseClient, err := database.NewNativeClient("localhost", uint16(mappedPort), "", "")
-	if err != nil {
-		t.Fatalf("unable to build client : %v", err)
-	}
-	return clickhouseClient
-}
-
-func TestReadTableNamesForDatabase(t *testing.T) {
-	ctx := context.Background()
-	clickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)
-	defer clickhouseContainer.Terminate(ctx) //nolint
-
-	clickhouseClient := getClient(t, mappedPort.Int())
-	t.Run("client can read tables for a database", func(t *testing.T) {
-		tables, err := clickhouseClient.ReadTableNamesForDatabase("system")
-		require.Nil(t, err)
-		require.GreaterOrEqual(t, len(tables), 70)
-		require.Contains(t, tables, "merge_tree_settings")
-	})
-}
-
-func TestReadTable(t *testing.T) {
-	t.Run("client can get all rows for system.disks table", func(t *testing.T) {
-		ctx := context.Background()
-		clickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)
-		defer clickhouseContainer.Terminate(ctx) //nolint
-
-		clickhouseClient := getClient(t, mappedPort.Int())
-
-		// we read the table system.disks as this should contain only 1 row
-		frame, err := clickhouseClient.ReadTable("system", "disks", []string{}, data.OrderBy{}, 10)
-		require.Nil(t, err)
-		require.ElementsMatch(t, frame.Columns(), [9]string{"name", "path", "free_space", "total_space", "unreserved_space", "keep_free_space", "type", "is_encrypted", "cache_path"})
-		i := 0
-		for {
-			values, ok, err := frame.Next()
-			if i == 0 {
-				require.Nil(t, err)
-				require.True(t, ok)
-				require.Equal(t, "default", values[0])
-				require.Equal(t, "/var/lib/clickhouse/", values[1])
-				require.Greater(t, values[2], uint64(0))
-				require.Greater(t, values[3], uint64(0))
-				require.Greater(t, values[4], uint64(0))
-				require.Equal(t, values[5], uint64(0))
-				require.Equal(t, "local", values[6])
-				require.Equal(t, values[7], uint8(0))
-				require.Equal(t, values[8], "")
-			} else {
-				require.False(t, ok)
-				break
-			}
-			i += 1
-		}
-	})
-
-	t.Run("client can get all rows for system.databases table", func(t *testing.T) {
-		ctx := context.Background()
-		clickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)
-		defer clickhouseContainer.Terminate(ctx) //nolint
-
-		clickhouseClient := getClient(t, mappedPort.Int())
-
-		// we read the table system.databases as this should be small and consistent on fresh db instances
-		frame, err := clickhouseClient.ReadTable("system", "databases", []string{}, data.OrderBy{}, 10)
-		require.Nil(t, err)
-		require.ElementsMatch(t, frame.Columns(), [6]string{"name", "engine", "data_path", "metadata_path", "uuid", "comment"})
-		expectedRows := [4][3]string{{"INFORMATION_SCHEMA", "Memory", "/var/lib/clickhouse/"},
-			{"default", "Atomic", "/var/lib/clickhouse/store/"},
-			{"information_schema", "Memory", "/var/lib/clickhouse/"},
-			{"system", "Atomic", "/var/lib/clickhouse/store/"}}
-		i := 0
-		for {
-			values, ok, err := frame.Next()
-
-			if i < 4 {
-				require.Nil(t, err)
-				require.True(t, ok)
-				require.Equal(t, expectedRows[i][0], values[0])
-				require.Equal(t, expectedRows[i][1], values[1])
-				require.Equal(t, expectedRows[i][2], values[2])
-				require.NotNil(t, values[3])
-				require.NotNil(t, values[4])
-				require.Equal(t, "", values[5])
-			} else {
-				require.False(t, ok)
-				break
-			}
-			i += 1
-		}
-	})
-
-	t.Run("client can get all rows for system.databases table with except", func(t *testing.T) {
-		ctx := context.Background()
-		clickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)
-		defer clickhouseContainer.Terminate(ctx) //nolint
-
-		clickhouseClient := getClient(t, mappedPort.Int())
-
-		frame, err := clickhouseClient.ReadTable("system", "databases", []string{"data_path", "comment"}, data.OrderBy{}, 10)
-		require.Nil(t, err)
-		require.ElementsMatch(t, frame.Columns(), [4]string{"name", "engine", "metadata_path", "uuid"})
-	})
-
-	t.Run("client can limit rows for system.databases", func(t *testing.T) {
-		ctx := context.Background()
-		clickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)
-		defer clickhouseContainer.Terminate(ctx) //nolint
-
-		clickhouseClient := getClient(t, mappedPort.Int())
-
-		frame, err := clickhouseClient.ReadTable("system", "databases", []string{}, data.OrderBy{}, 1)
-		require.Nil(t, err)
-		require.ElementsMatch(t, frame.Columns(), [6]string{"name", "engine", "data_path", "metadata_path", "uuid", "comment"})
-		expectedRows := [1][3]string{{"INFORMATION_SCHEMA", "Memory", "/var/lib/clickhouse/"}}
-		i := 0
-		for {
-			values, ok, err := frame.Next()
-			if i == 0 {
-				require.Nil(t, err)
-				require.True(t, ok)
-				require.Equal(t, expectedRows[i][0], values[0])
-				require.Equal(t, expectedRows[i][1], values[1])
-				require.Equal(t, expectedRows[i][2], values[2])
-				require.NotNil(t, values[3])
-				require.NotNil(t, values[4])
-				require.Equal(t, "", values[5])
-			} else {
-				require.False(t, ok)
-				break
-			}
-			i += 1
-		}
-	})
-
-	t.Run("client can order rows for system.databases", func(t *testing.T) {
-		ctx := context.Background()
-		clickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)
-		defer clickhouseContainer.Terminate(ctx) //nolint
-
-		clickhouseClient := getClient(t, mappedPort.Int())
-
-		frame, err := clickhouseClient.ReadTable("system", "databases", []string{}, data.OrderBy{
-			Column: "engine",
-			Order:  data.Asc,
-		}, 10)
-		require.Nil(t, err)
-		require.ElementsMatch(t, frame.Columns(), [6]string{"name", "engine", "data_path", "metadata_path", "uuid", "comment"})
-		expectedRows := [4][3]string{
-			{"default", "Atomic", "/var/lib/clickhouse/store/"},
-			{"system", "Atomic", "/var/lib/clickhouse/store/"},
-			{"INFORMATION_SCHEMA", "Memory", "/var/lib/clickhouse/"},
-			{"information_schema", "Memory", "/var/lib/clickhouse/"},
-		}
-		i := 0
-		for {
-			values, ok, err := frame.Next()
-
-			if i < 4 {
-				require.Nil(t, err)
-				require.True(t, ok)
-				require.Equal(t, expectedRows[i][0], values[0])
-				require.Equal(t, expectedRows[i][1], values[1])
-				require.Equal(t, expectedRows[i][2], values[2])
-				require.NotNil(t, values[3])
-				require.NotNil(t, values[4])
-				require.Equal(t, "", values[5])
-			} else {
-				require.False(t, ok)
-				break
-			}
-			i += 1
-		}
-	})
-}
-
-func TestExecuteStatement(t *testing.T) {
-	t.Run("client can execute any statement", func(t *testing.T) {
-		ctx := context.Background()
-		clickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)
-		defer clickhouseContainer.Terminate(ctx) //nolint
-
-		clickhouseClient := getClient(t, mappedPort.Int())
-
-		statement := "SELECT path, count(*) as count FROM system.disks GROUP BY path;"
-		frame, err := clickhouseClient.ExecuteStatement("engines", statement)
-		require.Nil(t, err)
-		require.ElementsMatch(t, frame.Columns(), [2]string{"path", "count"})
-		expectedRows := [1][2]interface{}{
-			{"/var/lib/clickhouse/", uint64(1)},
-		}
-		i := 0
-		for {
-			values, ok, err := frame.Next()
-			if !ok {
-				require.Nil(t, err)
-				break
-			}
-			require.Nil(t, err)
-			require.Equal(t, expectedRows[i][0], values[0])
-			require.Equal(t, expectedRows[i][1], values[1])
-			i++
-		}
-		fmt.Println(i)
-	})
-}
-
-func TestVersion(t *testing.T) {
-	t.Run("client can read version", func(t *testing.T) {
-		ctx := context.Background()
-		clickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)
-		defer clickhouseContainer.Terminate(ctx) //nolint
-
-		clickhouseClient := getClient(t, mappedPort.Int())
-
-		version, err := clickhouseClient.Version()
-		require.Nil(t, err)
-		require.NotEmpty(t, version)
-	})
-}
diff --git a/programs/diagnostics/internal/platform/manager_test.go b/programs/diagnostics/internal/platform/manager_test.go
deleted file mode 100644
index e6c50c6e505a..000000000000
--- a/programs/diagnostics/internal/platform/manager_test.go
+++ /dev/null
@@ -1,100 +0,0 @@
-//go:build !no_docker
-
-package platform_test
-
-import (
-	"context"
-	"fmt"
-	"os"
-	"path"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/docker/go-connections/nat"
-	"github.com/stretchr/testify/require"
-	"github.com/testcontainers/testcontainers-go"
-	"github.com/testcontainers/testcontainers-go/wait"
-)
-
-// create a ClickHouse container
-func createClickHouseContainer(t *testing.T, ctx context.Context) (testcontainers.Container, nat.Port) {
-	cwd, err := os.Getwd()
-	if err != nil {
-		fmt.Println("unable to read current directory", err)
-		os.Exit(1)
-	}
-	// for now, we test against a hardcoded database-server version but we should make this a property
-	req := testcontainers.ContainerRequest{
-		Image:        fmt.Sprintf("clickhouse/clickhouse-server:%s", test.GetClickHouseTestVersion()),
-		ExposedPorts: []string{"9000/tcp"},
-		WaitingFor:   wait.ForLog("Ready for connections"),
-		Mounts: testcontainers.ContainerMounts{
-			{
-				Source: testcontainers.GenericBindMountSource{
-					HostPath: path.Join(cwd, "../../testdata/docker/custom.xml"),
-				},
-				Target: "/etc/clickhouse-server/config.d/custom.xml",
-			},
-			{
-				Source: testcontainers.GenericBindMountSource{
-					HostPath: path.Join(cwd, "../../testdata/docker/admin.xml"),
-				},
-				Target: "/etc/clickhouse-server/users.d/admin.xml",
-			},
-		},
-	}
-	clickhouseContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
-		ContainerRequest: req,
-		Started:          true,
-	})
-	if err != nil {
-		// can't test without container
-		panic(err)
-	}
-
-	p, err := clickhouseContainer.MappedPort(ctx, "9000")
-	if err != nil {
-		// can't test without a port
-		panic(err)
-	}
-
-	return clickhouseContainer, p
-}
-
-func TestConnect(t *testing.T) {
-	t.Run("can only connect once", func(t *testing.T) {
-		ctx := context.Background()
-
-		clickhouseContainer, mappedPort := createClickHouseContainer(t, ctx)
-		defer clickhouseContainer.Terminate(ctx) //nolint
-
-		t.Setenv("CLICKHOUSE_DB_PORT", mappedPort.Port())
-
-		port := mappedPort.Int()
-
-		// get before connection
-		manager := platform.GetResourceManager()
-		require.Nil(t, manager.DbClient)
-		// init connection
-		err := manager.Connect("localhost", uint16(port), "", "")
-		require.Nil(t, err)
-		require.NotNil(t, manager.DbClient)
-		// try and re-fetch connection
-		err = manager.Connect("localhost", uint16(port), "", "")
-		require.NotNil(t, err)
-		require.Equal(t, "connect can only be called once", err.Error())
-	})
-
-}
-
-func TestGetResourceManager(t *testing.T) {
-	t.Run("get resource manager", func(t *testing.T) {
-		manager := platform.GetResourceManager()
-		require.NotNil(t, manager)
-		manager2 := platform.GetResourceManager()
-		require.NotNil(t, manager2)
-		require.Equal(t, &manager, &manager2)
-	})
-
-}
diff --git a/programs/diagnostics/internal/platform/test/data.go b/programs/diagnostics/internal/platform/test/data.go
deleted file mode 100644
index 7710e9a69a1f..000000000000
--- a/programs/diagnostics/internal/platform/test/data.go
+++ /dev/null
@@ -1,166 +0,0 @@
-package test
-
-import (
-	"fmt"
-	"sort"
-	"strings"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/data"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils"
-	"github.com/pkg/errors"
-)
-
-type fakeClickhouseClient struct {
-	tables         map[string][]string
-	QueryResponses map[string]*FakeDataFrame
-}
-
-func NewFakeClickhouseClient(tables map[string][]string) fakeClickhouseClient {
-	queryResponses := make(map[string]*FakeDataFrame)
-	return fakeClickhouseClient{
-		tables:         tables,
-		QueryResponses: queryResponses,
-	}
-}
-
-func (f fakeClickhouseClient) ReadTableNamesForDatabase(databaseName string) ([]string, error) {
-	if _, ok := f.tables[databaseName]; ok {
-		return f.tables[databaseName], nil
-	}
-	return nil, fmt.Errorf("database %s does not exist", databaseName)
-}
-
-func (f fakeClickhouseClient) ReadTable(databaseName string, tableName string, excludeColumns []string, orderBy data.OrderBy, limit int64) (data.Frame, error) {
-
-	exceptClause := ""
-	if len(excludeColumns) > 0 {
-		exceptClause = fmt.Sprintf("EXCEPT(%s) ", strings.Join(excludeColumns, ","))
-	}
-	limitClause := ""
-	if limit >= 0 {
-		limitClause = fmt.Sprintf(" LIMIT %d", limit)
-	}
-	query := fmt.Sprintf("SELECT * %sFROM %s.%s%s%s", exceptClause, databaseName, tableName, orderBy.String(), limitClause)
-	frame, error := f.ExecuteStatement(fmt.Sprintf("read_table_%s.%s", databaseName, tableName), query)
-	if error != nil {
-		return frame, error
-	}
-	fFrame := *(frame.(*FakeDataFrame))
-	fFrame = fFrame.FilterColumns(excludeColumns)
-	fFrame = fFrame.Order(orderBy)
-	fFrame = fFrame.Limit(limit)
-	return fFrame, nil
-}
-
-func (f fakeClickhouseClient) ExecuteStatement(id string, statement string) (data.Frame, error) {
-	if frame, ok := f.QueryResponses[statement]; ok {
-		return frame, nil
-	}
-	return FakeDataFrame{}, errors.New(fmt.Sprintf("No recorded response for %s", statement))
-}
-
-func (f fakeClickhouseClient) Version() (string, error) {
-	return "21.12.3", nil
-}
-
-func (f fakeClickhouseClient) Reset() {
-	for key, frame := range f.QueryResponses {
-		frame.Reset()
-		f.QueryResponses[key] = frame
-	}
-}
-
-type FakeDataFrame struct {
-	i           *int
-	Rows        [][]interface{}
-	ColumnNames []string
-	name        string
-}
-
-func NewFakeDataFrame(name string, columns []string, rows [][]interface{}) FakeDataFrame {
-	i := 0
-	return FakeDataFrame{
-		i:           &i,
-		Rows:        rows,
-		ColumnNames: columns,
-		name:        name,
-	}
-}
-
-func (f FakeDataFrame) Next() ([]interface{}, bool, error) {
-	if len(f.Rows) == *(f.i) {
-		return nil, false, nil
-	}
-	value := f.Rows[*f.i]
-	*f.i++
-	return value, true, nil
-}
-
-func (f FakeDataFrame) Columns() []string {
-	return f.ColumnNames
-}
-
-func (f FakeDataFrame) Name() string {
-	return f.name
-}
-
-func (f *FakeDataFrame) Reset() {
-	i := 0
-	f.i = &i
-}
-
-func (f FakeDataFrame) FilterColumns(excludeColumns []string) FakeDataFrame {
-	// get columns we can remove
-	rColumns := utils.Intersection(f.ColumnNames, excludeColumns)
-	rIndexes := make([]int, len(rColumns))
-	// find the indexes of the columns to remove
-	for i, column := range rColumns {
-		rIndexes[i] = utils.IndexOf(f.ColumnNames, column)
-	}
-	newRows := make([][]interface{}, len(f.Rows))
-	for r, row := range f.Rows {
-		newRow := row
-		for i, index := range rIndexes {
-			newRow = utils.Remove(newRow, index-i)
-		}
-		newRows[r] = newRow
-	}
-	f.Rows = newRows
-	f.ColumnNames = utils.Distinct(f.ColumnNames, excludeColumns)
-	return f
-}
-
-func (f FakeDataFrame) Limit(rowLimit int64) FakeDataFrame {
-	if rowLimit >= 0 {
-		if int64(len(f.Rows)) > rowLimit {
-			f.Rows = f.Rows[:rowLimit]
-		}
-	}
-	return f
-}
-
-func (f FakeDataFrame) Order(orderBy data.OrderBy) FakeDataFrame {
-	if orderBy.Column == "" {
-		return f
-	}
-	cIndex := utils.IndexOf(f.ColumnNames, orderBy.Column)
-	sort.Slice(f.Rows, func(i, j int) bool {
-		left := f.Rows[i][cIndex]
-		right := f.Rows[j][cIndex]
-		if iLeft, ok := left.(int); ok {
-			if orderBy.Order == data.Asc {
-				return iLeft < right.(int)
-			}
-			return iLeft > right.(int)
-		} else {
-			// we aren't a full db - revert to string order
-			sLeft := left.(string)
-			sRight := right.(string)
-			if orderBy.Order == data.Asc {
-				return sLeft < sRight
-			}
-			return sLeft > sRight
-		}
-	})
-	return f
-}
diff --git a/programs/diagnostics/internal/platform/test/env.go b/programs/diagnostics/internal/platform/test/env.go
deleted file mode 100644
index 36b03772ab08..000000000000
--- a/programs/diagnostics/internal/platform/test/env.go
+++ /dev/null
@@ -1,16 +0,0 @@
-package test
-
-import "os"
-
-const defaultClickHouseVersion = "latest"
-
-func GetClickHouseTestVersion() string {
-	return GetEnv("CLICKHOUSE_VERSION", defaultClickHouseVersion)
-}
-
-func GetEnv(key, fallback string) string {
-	if value, ok := os.LookupEnv(key); ok {
-		return value
-	}
-	return fallback
-}
diff --git a/programs/diagnostics/internal/platform/utils/file_test.go b/programs/diagnostics/internal/platform/utils/file_test.go
deleted file mode 100644
index 8d0430090c96..000000000000
--- a/programs/diagnostics/internal/platform/utils/file_test.go
+++ /dev/null
@@ -1,134 +0,0 @@
-package utils_test
-
-import (
-	"fmt"
-	"os"
-	"path"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils"
-	"github.com/stretchr/testify/require"
-)
-
-func TestFileExists(t *testing.T) {
-	t.Run("returns true for file", func(t *testing.T) {
-		tempDir := t.TempDir()
-		filepath := path.Join(tempDir, "random.txt")
-		_, err := os.Create(filepath)
-		require.Nil(t, err)
-		exists, err := utils.FileExists(filepath)
-		require.True(t, exists)
-		require.Nil(t, err)
-	})
-
-	t.Run("doesn't return true for not existence file", func(t *testing.T) {
-		tempDir := t.TempDir()
-		file := path.Join(tempDir, "random.txt")
-		exists, err := utils.FileExists(file)
-		require.False(t, exists)
-		require.Nil(t, err)
-	})
-
-	t.Run("doesn't return true for directory", func(t *testing.T) {
-		tempDir := t.TempDir()
-		exists, err := utils.FileExists(tempDir)
-		require.False(t, exists)
-		require.NotNil(t, err)
-		require.Equal(t, fmt.Sprintf("%s is a directory", tempDir), err.Error())
-	})
-}
-
-func TestDirExists(t *testing.T) {
-	t.Run("doesn't return true for file", func(t *testing.T) {
-		tempDir := t.TempDir()
-		filepath := path.Join(tempDir, "random.txt")
-		_, err := os.Create(filepath)
-		require.Nil(t, err)
-		exists, err := utils.DirExists(filepath)
-		require.False(t, exists)
-		require.NotNil(t, err)
-		require.Equal(t, fmt.Sprintf("%s is a file", filepath), err.Error())
-	})
-
-	t.Run("returns true for directory", func(t *testing.T) {
-		tempDir := t.TempDir()
-		exists, err := utils.DirExists(tempDir)
-		require.True(t, exists)
-		require.Nil(t, err)
-	})
-
-	t.Run("doesn't return true random directory", func(t *testing.T) {
-		exists, err := utils.FileExists(fmt.Sprintf("%d", utils.MakeTimestamp()))
-		require.False(t, exists)
-		require.Nil(t, err)
-	})
-}
-
-func TestCopyFile(t *testing.T) {
-	t.Run("can copy file", func(t *testing.T) {
-		tempDir := t.TempDir()
-		sourcePath := path.Join(tempDir, "random.txt")
-		_, err := os.Create(sourcePath)
-		require.Nil(t, err)
-		destPath := path.Join(tempDir, "random-2.txt")
-		err = utils.CopyFile(sourcePath, destPath)
-		require.Nil(t, err)
-	})
-
-	t.Run("can copy nested file", func(t *testing.T) {
-		tempDir := t.TempDir()
-		sourcePath := path.Join(tempDir, "random.txt")
-		_, err := os.Create(sourcePath)
-		require.Nil(t, err)
-		destPath := path.Join(tempDir, "sub_dir", "random-2.txt")
-		err = utils.CopyFile(sourcePath, destPath)
-		require.Nil(t, err)
-	})
-
-	t.Run("fails when file does not exist", func(t *testing.T) {
-		tempDir := t.TempDir()
-		sourcePath := path.Join(tempDir, "random.txt")
-		destPath := path.Join(tempDir, "random-2.txt")
-		err := utils.CopyFile(sourcePath, destPath)
-		require.NotNil(t, err)
-		require.Equal(t, fmt.Sprintf("%s does not exist", sourcePath), err.Error())
-	})
-}
-
-func TestListFilesInDirectory(t *testing.T) {
-	tempDir := t.TempDir()
-	files := make([]string, 5)
-	for i := 0; i < 5; i++ {
-		fileDir := path.Join(tempDir, fmt.Sprintf("%d", i))
-		err := os.MkdirAll(fileDir, os.ModePerm)
-		require.Nil(t, err)
-		ext := ".txt"
-		if i%2 == 0 {
-			ext = ".csv"
-		}
-		filepath := path.Join(fileDir, fmt.Sprintf("random-%d%s", i, ext))
-		files[i] = filepath
-		_, err = os.Create(filepath)
-		require.Nil(t, err)
-	}
-
-	t.Run("can list all files", func(t *testing.T) {
-		mFiles, errs := utils.ListFilesInDirectory(tempDir, []string{"*"})
-		require.Len(t, mFiles, 5)
-		require.Empty(t, errs)
-	})
-
-	t.Run("can list by extension", func(t *testing.T) {
-		mFiles, errs := utils.ListFilesInDirectory(tempDir, []string{"*.csv"})
-		require.Len(t, mFiles, 3)
-		require.Empty(t, errs)
-		require.ElementsMatch(t, []string{files[0], files[2], files[4]}, mFiles)
-	})
-
-	t.Run("can list on multiple extensions files", func(t *testing.T) {
-		mFiles, errs := utils.ListFilesInDirectory(tempDir, []string{"*.csv", "*.txt"})
-		require.Len(t, mFiles, 5)
-		require.Empty(t, errs)
-	})
-
-}
diff --git a/programs/diagnostics/internal/platform/utils/process_test.go b/programs/diagnostics/internal/platform/utils/process_test.go
deleted file mode 100644
index 9baaa5597522..000000000000
--- a/programs/diagnostics/internal/platform/utils/process_test.go
+++ /dev/null
@@ -1,97 +0,0 @@
-//go:build !no_docker
-
-package utils_test
-
-import (
-	"context"
-	"fmt"
-	"io"
-	"os"
-	"path"
-	"strings"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/stretchr/testify/require"
-	"github.com/testcontainers/testcontainers-go"
-	"github.com/testcontainers/testcontainers-go/wait"
-)
-
-func getProcessesInContainer(t *testing.T, container testcontainers.Container) ([]string, error) {
-	result, reader, err := container.Exec(context.Background(), []string{"ps", "-aux"})
-	if err != nil {
-		return nil, err
-	}
-	require.Zero(t, result)
-	require.NotNil(t, reader)
-
-	b, err := io.ReadAll(reader)
-	if err != nil {
-		return nil, err
-	}
-	require.NotNil(t, b)
-
-	lines := strings.Split(string(b), "
")
-
-	// discard PS header
-	return lines[1:], nil
-}
-
-func TestFindClickHouseProcessesAndConfigs(t *testing.T) {
-
-	t.Run("can find ClickHouse processes and configs", func(t *testing.T) {
-		// create a ClickHouse container
-		ctx := context.Background()
-		cwd, err := os.Getwd()
-		if err != nil {
-			fmt.Println("unable to read current directory", err)
-			os.Exit(1)
-		}
-
-		// run a ClickHouse container that guarantees that it runs only for the duration of the test
-		req := testcontainers.ContainerRequest{
-			Image:        fmt.Sprintf("clickhouse/clickhouse-server:%s", test.GetClickHouseTestVersion()),
-			ExposedPorts: []string{"9000/tcp"},
-			WaitingFor:   wait.ForLog("Ready for connections"),
-			Mounts: testcontainers.ContainerMounts{
-				{
-					Source: testcontainers.GenericBindMountSource{
-						HostPath: path.Join(cwd, "../../../testdata/docker/custom.xml"),
-					},
-					Target: "/etc/clickhouse-server/config.d/custom.xml",
-				},
-			},
-		}
-		clickhouseContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
-			ContainerRequest: req,
-			Started:          true,
-		})
-		if err != nil {
-			// can't test without container
-			panic(err)
-		}
-
-		p, _ := clickhouseContainer.MappedPort(ctx, "9000")
-
-		t.Setenv("CLICKHOUSE_DB_PORT", p.Port())
-
-		defer clickhouseContainer.Terminate(ctx) //nolint
-
-		lines, err := getProcessesInContainer(t, clickhouseContainer)
-		require.Nil(t, err)
-		require.NotEmpty(t, lines)
-
-		for _, line := range lines {
-			parts := strings.Fields(line)
-			if len(parts) < 11 {
-				continue
-			}
-			if !strings.Contains(parts[10], "clickhouse-server") {
-				continue
-			}
-
-			require.Equal(t, "/usr/bin/clickhouse-server", parts[10])
-			require.Equal(t, "--config-file=/etc/clickhouse-server/config.xml", parts[11])
-		}
-	})
-}
diff --git a/programs/diagnostics/internal/platform/utils/slices_test.go b/programs/diagnostics/internal/platform/utils/slices_test.go
deleted file mode 100644
index ea5c1c81dcc4..000000000000
--- a/programs/diagnostics/internal/platform/utils/slices_test.go
+++ /dev/null
@@ -1,64 +0,0 @@
-package utils_test
-
-import (
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils"
-	"github.com/stretchr/testify/require"
-)
-
-func TestIntersection(t *testing.T) {
-	t.Run("can perform intersection", func(t *testing.T) {
-		setA := []string{"A", "b", "C", "D", "E"}
-		setB := []string{"A", "B", "F", "C", "G"}
-		setC := utils.Intersection(setA, setB)
-		require.Len(t, setC, 2)
-		require.ElementsMatch(t, []string{"A", "C"}, setC)
-	})
-}
-
-func TestDistinct(t *testing.T) {
-	t.Run("can perform distinct", func(t *testing.T) {
-		setA := []string{"A", "b", "C", "D", "E"}
-		setB := []string{"A", "B", "F", "C", "G"}
-		setC := utils.Distinct(setA, setB)
-		require.Len(t, setC, 3)
-		require.ElementsMatch(t, []string{"b", "D", "E"}, setC)
-	})
-
-	t.Run("can perform distinct on empty", func(t *testing.T) {
-		setA := []string{"A", "b", "C", "D", "E"}
-		var setB []string
-		setC := utils.Distinct(setA, setB)
-		require.Len(t, setC, 5)
-		require.ElementsMatch(t, []string{"A", "b", "C", "D", "E"}, setC)
-	})
-}
-
-func TestContains(t *testing.T) {
-	t.Run("can perform contains", func(t *testing.T) {
-		setA := []string{"A", "b", "C", "D", "E"}
-		require.True(t, utils.Contains(setA, "A"))
-		require.True(t, utils.Contains(setA, "b"))
-		require.True(t, utils.Contains(setA, "C"))
-		require.True(t, utils.Contains(setA, "D"))
-		require.True(t, utils.Contains(setA, "E"))
-		require.False(t, utils.Contains(setA, "B"))
-	})
-}
-
-func TestUnique(t *testing.T) {
-
-	t.Run("can perform unique", func(t *testing.T) {
-		setA := []string{"A", "b", "D", "D", "E", "E", "A"}
-		setC := utils.Unique(setA)
-		require.Len(t, setC, 4)
-		require.ElementsMatch(t, []string{"A", "b", "D", "E"}, setC)
-	})
-
-	t.Run("can perform unique on empty", func(t *testing.T) {
-		var setA []string
-		setC := utils.Unique(setA)
-		require.Len(t, setC, 0)
-	})
-}
diff --git a/programs/diagnostics/internal/runner_test.go b/programs/diagnostics/internal/runner_test.go
deleted file mode 100644
index 2369f8b3007d..000000000000
--- a/programs/diagnostics/internal/runner_test.go
+++ /dev/null
@@ -1,130 +0,0 @@
-//go:build !no_docker
-
-package internal_test
-
-import (
-	"context"
-	"fmt"
-	"io/ioutil"
-	"os"
-	"path"
-	"testing"
-
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors"
-	_ "github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/clickhouse"
-	_ "github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/collectors/system"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs"
-	_ "github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/outputs/file"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/config"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/test"
-	"github.com/ClickHouse/ClickHouse/programs/diagnostics/internal/platform/utils"
-	"github.com/stretchr/testify/require"
-	"github.com/testcontainers/testcontainers-go"
-	"github.com/testcontainers/testcontainers-go/wait"
-)
-
-// Execute a full default capture, with simple output, and check if a bundle is produced and it's not empty
-func TestCapture(t *testing.T) {
-	// create a ClickHouse container
-	ctx := context.Background()
-	cwd, err := os.Getwd()
-
-	if err != nil {
-		// can't test without container
-		panic(err)
-	}
-	// for now, we test against a hardcoded database-server version but we should make this a property
-	req := testcontainers.ContainerRequest{
-		Image:        fmt.Sprintf("clickhouse/clickhouse-server:%s", test.GetClickHouseTestVersion()),
-		ExposedPorts: []string{"9000/tcp"},
-		WaitingFor:   wait.ForLog("Ready for connections"),
-		Mounts: testcontainers.ContainerMounts{
-			{
-				Source: testcontainers.GenericBindMountSource{
-					HostPath: path.Join(cwd, "../testdata/docker/custom.xml"),
-				},
-				Target: "/etc/clickhouse-server/config.d/custom.xml",
-			},
-			{
-				Source: testcontainers.GenericBindMountSource{
-					HostPath: path.Join(cwd, "../testdata/docker/admin.xml"),
-				},
-				Target: "/etc/clickhouse-server/users.d/admin.xml",
-			},
-		},
-	}
-	clickhouseContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
-		ContainerRequest: req,
-		Started:          true,
-	})
-	if err != nil {
-		// can't test without container
-		panic(err)
-	}
-
-	p, _ := clickhouseContainer.MappedPort(ctx, "9000")
-
-	t.Setenv("CLICKHOUSE_DB_PORT", p.Port())
-	defer clickhouseContainer.Terminate(ctx) //nolint
-
-	tmrDir := t.TempDir()
-	port := p.Int()
-
-	// test a simple output exists
-	_, err = outputs.GetOutputByName("simple")
-	require.Nil(t, err)
-	// this relies on the simple out not changing its params - test will likely fail if so
-	outputConfig := config.Configuration{
-		Params: []config.ConfigParam{
-			config.StringParam{
-				Value: tmrDir,
-				Param: config.NewParam("directory", "Directory in which to create dump. Defaults to the current directory.", false),
-			},
-			config.StringOptions{
-				Value:   "csv",
-				Options: []string{"csv"},
-				Param:   config.NewParam("format", "Format of exported files", false),
-			},
-			config.BoolParam{
-				Value: true,
-				Param: config.NewParam("skip_archive", "Don't compress output to an archive", false),
-			},
-		},
-	}
-	// test default collectors
-	collectorNames := collectors.GetCollectorNames(true)
-	// grab all configs - only default will be used because of collectorNames
-	collectorConfigs, err := collectors.BuildConfigurationOptions()
-	require.Nil(t, err)
-	conf := internal.NewRunConfiguration("random", "localhost", uint16(port), "", "", "simple", outputConfig, collectorNames, collectorConfigs)
-	internal.Capture(conf)
-	outputDir := path.Join(tmrDir, "random")
-	_, err = os.Stat(outputDir)
-	require.Nil(t, err)
-	require.True(t, !os.IsNotExist(err))
-	files, err := ioutil.ReadDir(outputDir)
-	require.Nil(t, err)
-	require.Len(t, files, 1)
-	outputDir = path.Join(outputDir, files[0].Name())
-	// check we have a folder per collector i.e. collectorNames + diag_trace
-	files, err = ioutil.ReadDir(outputDir)
-	require.Nil(t, err)
-	require.Len(t, files, len(collectorNames)+1)
-	expectedFolders := append(collectorNames, "diag_trace")
-	for _, file := range files {
-		require.True(t, file.IsDir())
-		utils.Contains(expectedFolders, file.Name())
-	}
-	// we don't test the specific collector outputs but make sure something was written to system
-	systemFolder := path.Join(outputDir, "system")
-	files, err = ioutil.ReadDir(systemFolder)
-	require.Nil(t, err)
-	require.Greater(t, len(files), 0)
-	// test diag_trace
-	diagFolder := path.Join(outputDir, "diag_trace")
-	files, err = ioutil.ReadDir(diagFolder)
-	require.Nil(t, err)
-	require.Equal(t, 1, len(files))
-	require.FileExists(t, path.Join(diagFolder, "errors.csv"))
-}
diff --git a/programs/diagnostics/testdata/configs/include/xml/server-include.xml b/programs/diagnostics/testdata/configs/include/xml/server-include.xml
deleted file mode 100644
index 30e6587c935e..000000000000
--- a/programs/diagnostics/testdata/configs/include/xml/server-include.xml
+++ /dev/null
@@ -1,8 +0,0 @@
-<clickhouse>
-    <network_max>5000000</network_max>
-    <test_profile>
-        <test_p>
-        </test_p>
-    </test_profile>
-    <pg_port>9008</pg_port>
-</clickhouse>
\ No newline at end of file
diff --git a/programs/diagnostics/testdata/configs/include/xml/user-include.xml b/programs/diagnostics/testdata/configs/include/xml/user-include.xml
deleted file mode 100644
index b12b34a56bb9..000000000000
--- a/programs/diagnostics/testdata/configs/include/xml/user-include.xml
+++ /dev/null
@@ -1,20 +0,0 @@
-<clickhouse>
-    <test_user>
-        <networks>
-            <ip>::/0</ip>
-        </networks>
-        <profile>default</profile>
-        <quota>default</quota>
-        <password_sha256_hex>REPLACE_ME</password_sha256_hex>
-        <access_management>1</access_management>
-    </test_user>
-    <another_user>
-        <networks>
-            <ip>::/0</ip>
-        </networks>
-        <profile>default</profile>
-        <quota>default</quota>
-        <passwird>REPLACE_ME</passwird>
-        <access_management>1</access_management>
-    </another_user>
-</clickhouse>
diff --git a/programs/diagnostics/testdata/configs/include/yaml/server-include.yaml b/programs/diagnostics/testdata/configs/include/yaml/server-include.yaml
deleted file mode 100644
index 903d7b6f733f..000000000000
--- a/programs/diagnostics/testdata/configs/include/yaml/server-include.yaml
+++ /dev/null
@@ -1,1 +0,0 @@
-network_max: 5000000
diff --git a/programs/diagnostics/testdata/configs/include/yaml/user-include.yaml b/programs/diagnostics/testdata/configs/include/yaml/user-include.yaml
deleted file mode 100644
index 23b592507fab..000000000000
--- a/programs/diagnostics/testdata/configs/include/yaml/user-include.yaml
+++ /dev/null
@@ -1,7 +0,0 @@
-test_user:
-  password: 'REPLACE_ME'
-  networks:
-    ip: '::/0'
-  profile: default
-  quota: default
-  access_management: 1
diff --git a/programs/diagnostics/testdata/configs/xml/config.xml b/programs/diagnostics/testdata/configs/xml/config.xml
deleted file mode 100644
index eb7c70cf4981..000000000000
--- a/programs/diagnostics/testdata/configs/xml/config.xml
+++ /dev/null
@@ -1,1195 +0,0 @@
-<!--
-  NOTE: User and query level settings are set up in "users.xml" file.
-  If you have accidentally specified user-level settings here, server won't start.
-  You can either move the settings to the right place inside "users.xml" file
-   or add <skip_check_for_incorrect_settings>1</skip_check_for_incorrect_settings> here.
--->
-<clickhouse>
-    <include_from>../include/xml/server-include.xml</include_from>
-    <logger>
-        <!-- Possible levels [1]:
-
-          - none (turns off logging)
-          - fatal
-          - critical
-          - error
-          - warning
-          - notice
-          - information
-          - debug
-          - trace
-          - test (not for production usage)
-
-            [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114
-        -->
-        <level>trace</level>
-        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
-        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
-        <!-- Rotation policy
-             See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85
-          -->
-        <size>1000M</size>
-        <count>10</count>
-        <!-- <console>1</console> --> <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->
-
-        <!-- Per level overrides (legacy):
-
-        For example to suppress logging of the ConfigReloader you can use:
-        NOTE: levels.logger is reserved, see below.
-        -->
-        <!--
-        <levels>
-          <ConfigReloader>none</ConfigReloader>
-        </levels>
-        -->
-
-        <!-- Per level overrides:
-
-        For example to suppress logging of the RBAC for default user you can use:
-        (But please note that the logger name maybe changed from version to version, even after minor upgrade)
-        -->
-        <!--
-        <levels>
-          <logger>
-            <name>ContextAccess (default)</name>
-            <level>none</level>
-          </logger>
-          <logger>
-            <name>DatabaseOrdinary (test)</name>
-            <level>none</level>
-          </logger>
-        </levels>
-        -->
-    </logger>
-    <!-- Add headers to response in options request. OPTIONS method is used in CORS preflight requests. -->
-    <!-- It is off by default. Next headers are obligate for CORS.-->
-    <!-- http_options_response>
-        <header>
-            <name>Access-Control-Allow-Origin</name>
-            <value>*</value>
-        </header>
-        <header>
-            <name>Access-Control-Allow-Headers</name>
-            <value>origin, x-requested-with</value>
-        </header>
-        <header>
-            <name>Access-Control-Allow-Methods</name>
-            <value>POST, GET, OPTIONS</value>
-        </header>
-        <header>
-            <name>Access-Control-Max-Age</name>
-            <value>86400</value>
-        </header>
-    </http_options_response -->
-
-    <!-- It is the name that will be shown in the clickhouse-client.
-         By default, anything with "production" will be highlighted in red in query prompt.
-    -->
-    <!--display_name>production</display_name-->
-
-    <!-- Port for HTTP API. See also 'https_port' for secure connections.
-         This interface is also used by ODBC and JDBC drivers (DataGrip, Dbeaver, ...)
-         and by most of web interfaces (embedded UI, Grafana, Redash, ...).
-      -->
-    <http_port>8123</http_port>
-
-    <!-- Port for interaction by native protocol with:
-         - clickhouse-client and other native ClickHouse tools (clickhouse-benchmark);
-         - clickhouse-server with other clickhouse-servers for distributed query processing;
-         - ClickHouse drivers and applications supporting native protocol
-         (this protocol is also informally called as "the TCP protocol");
-         See also 'tcp_port_secure' for secure connections.
-    -->
-    <tcp_port>9000</tcp_port>
-
-    <!-- Compatibility with MySQL protocol.
-         ClickHouse will pretend to be MySQL for applications connecting to this port.
-    -->
-    <mysql_port>9004</mysql_port>
-
-    <!-- Compatibility with PostgreSQL protocol.
-         ClickHouse will pretend to be PostgreSQL for applications connecting to this port.
-    -->
-    <postgresql_port>9005</postgresql_port>
-
-    <!-- HTTP API with TLS (HTTPS).
-         You have to configure certificate to enable this interface.
-         See the openSSL section below.
-    -->
-    <!-- <https_port>8443</https_port> -->
-
-    <!-- Native interface with TLS.
-         You have to configure certificate to enable this interface.
-         See the openSSL section below.
-    -->
-    <!-- <tcp_port_secure>9440</tcp_port_secure> -->
-
-    <!-- Native interface wrapped with PROXYv1 protocol
-         PROXYv1 header sent for every connection.
-         ClickHouse will extract information about proxy-forwarded client address from the header.
-    -->
-    <!-- <tcp_with_proxy_port>9011</tcp_with_proxy_port> -->
-
-    <!-- Port for communication between replicas. Used for data exchange.
-         It provides low-level data access between servers.
-         This port should not be accessible from untrusted networks.
-         See also 'interserver_http_credentials'.
-         Data transferred over connections to this port should not go through untrusted networks.
-         See also 'interserver_https_port'.
-      -->
-    <interserver_http_port>9009</interserver_http_port>
-
-    <!-- Port for communication between replicas with TLS.
-         You have to configure certificate to enable this interface.
-         See the openSSL section below.
-         See also 'interserver_http_credentials'.
-      -->
-    <!-- <interserver_https_port>9010</interserver_https_port> -->
-
-    <!-- Hostname that is used by other replicas to request this server.
-         If not specified, than it is determined analogous to 'hostname -f' command.
-         This setting could be used to switch replication to another network interface
-         (the server may be connected to multiple networks via multiple addresses)
-      -->
-    <!--
-    <interserver_http_host>example.yandex.ru</interserver_http_host>
-    -->
-
-    <!-- You can specify credentials for authenthication between replicas.
-         This is required when interserver_https_port is accessible from untrusted networks,
-         and also recommended to avoid SSRF attacks from possibly compromised services in your network.
-      -->
-    <!--<interserver_http_credentials>
-        <user>interserver</user>
-        <password></password>
-    </interserver_http_credentials>-->
-
-    <!-- Listen specified address.
-         Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere.
-         Notes:
-         If you open connections from wildcard address, make sure that at least one of the following measures applied:
-         - server is protected by firewall and not accessible from untrusted networks;
-         - all users are restricted to subset of network addresses (see users.xml);
-         - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces.
-         - users without password have readonly access.
-         See also: https://www.shodan.io/search?query=clickhouse
-      -->
-    <!-- <listen_host>::</listen_host> -->
-
-    <!-- Same for hosts without support for IPv6: -->
-    <!-- <listen_host>0.0.0.0</listen_host> -->
-
-    <!-- Default values - try listen localhost on IPv4 and IPv6. -->
-    <!--
-    <listen_host>::1</listen_host>
-    <listen_host>127.0.0.1</listen_host>
-    -->
-
-    <!-- Don't exit if IPv6 or IPv4 networks are unavailable while trying to listen. -->
-    <!-- <listen_try>0</listen_try> -->
-
-    <!-- Allow multiple servers to listen on the same address:port. This is not recommended.
-      -->
-    <!-- <listen_reuse_port>0</listen_reuse_port> -->
-
-    <!-- <listen_backlog>4096</listen_backlog> -->
-
-    <max_connections>4096</max_connections>
-
-    <!-- For 'Connection: keep-alive' in HTTP 1.1 -->
-    <keep_alive_timeout>3</keep_alive_timeout>
-
-    <!-- gRPC protocol (see src/Server/grpc_protos/clickhouse_grpc.proto for the API) -->
-    <!-- <grpc_port>9100</grpc_port> -->
-    <grpc>
-        <enable_ssl>false</enable_ssl>
-
-        <!-- The following two files are used only if enable_ssl=1 -->
-        <ssl_cert_file>/path/to/ssl_cert_file</ssl_cert_file>
-        <ssl_key_file>/path/to/ssl_key_file</ssl_key_file>
-
-        <!-- Whether server will request client for a certificate -->
-        <ssl_require_client_auth>false</ssl_require_client_auth>
-
-        <!-- The following file is used only if ssl_require_client_auth=1 -->
-        <ssl_ca_cert_file>/path/to/ssl_ca_cert_file</ssl_ca_cert_file>
-
-        <!-- Default compression algorithm (applied if client doesn't specify another algorithm, see result_compression in QueryInfo).
-             Supported algorithms: none, deflate, gzip, stream_gzip -->
-        <compression>deflate</compression>
-
-        <!-- Default compression level (applied if client doesn't specify another level, see result_compression in QueryInfo).
-             Supported levels: none, low, medium, high -->
-        <compression_level>medium</compression_level>
-
-        <!-- Send/receive message size limits in bytes. -1 means unlimited -->
-        <max_send_message_size>-1</max_send_message_size>
-        <max_receive_message_size>-1</max_receive_message_size>
-
-        <!-- Enable if you want very detailed logs -->
-        <verbose_logs>false</verbose_logs>
-    </grpc>
-
-    <!-- Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71 -->
-    <openSSL>
-        <server> <!-- Used for https server AND secure tcp port -->
-            <!-- openssl req -subj "/CN=localhost" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt -->
-            <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>
-            <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile>
-            <!-- dhparams are optional. You can delete the <dhParamsFile> element.
-                 To generate dhparams, use the following command:
-                  openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096
-                 Only file format with BEGIN DH PARAMETERS is supported.
-              -->
-            <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>
-            <verificationMode>none</verificationMode>
-            <loadDefaultCAFile>true</loadDefaultCAFile>
-            <cacheSessions>true</cacheSessions>
-            <disableProtocols>sslv2,sslv3</disableProtocols>
-            <preferServerCiphers>true</preferServerCiphers>
-        </server>
-
-        <client> <!-- Used for connecting to https dictionary source and secured Zookeeper communication -->
-            <loadDefaultCAFile>true</loadDefaultCAFile>
-            <cacheSessions>true</cacheSessions>
-            <disableProtocols>sslv2,sslv3</disableProtocols>
-            <preferServerCiphers>true</preferServerCiphers>
-            <!-- Use for self-signed: <verificationMode>none</verificationMode> -->
-            <invalidCertificateHandler>
-                <!-- Use for self-signed: <name>AcceptCertificateHandler</name> -->
-                <name>RejectCertificateHandler</name>
-            </invalidCertificateHandler>
-        </client>
-    </openSSL>
-
-    <!-- Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123 -->
-    <!--
-    <http_server_default_response><![CDATA[<html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>]]></http_server_default_response>
-    -->
-
-    <!-- Maximum number of concurrent queries. -->
-    <max_concurrent_queries>100</max_concurrent_queries>
-
-    <!-- Maximum memory usage (resident set size) for server process.
-         Zero value or unset means default. Default is "max_server_memory_usage_to_ram_ratio" of available physical RAM.
-         If the value is larger than "max_server_memory_usage_to_ram_ratio" of available physical RAM, it will be cut down.
-
-         The constraint is checked on query execution time.
-         If a query tries to allocate memory and the current memory usage plus allocation is greater
-          than specified threshold, exception will be thrown.
-
-         It is not practical to set this constraint to small values like just a few gigabytes,
-          because memory allocator will keep this amount of memory in caches and the server will deny service of queries.
-      -->
-    <max_server_memory_usage>0</max_server_memory_usage>
-
-    <!-- Maximum number of threads in the Global thread pool.
-    This will default to a maximum of 10000 threads if not specified.
-    This setting will be useful in scenarios where there are a large number
-    of distributed queries that are running concurrently but are idling most
-    of the time, in which case a higher number of threads might be required.
-    -->
-
-    <max_thread_pool_size>10000</max_thread_pool_size>
-
-    <!-- On memory constrained environments you may have to set this to value larger than 1.
-      -->
-    <max_server_memory_usage_to_ram_ratio>0.9</max_server_memory_usage_to_ram_ratio>
-
-    <!-- Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes).
-         Data will be stored in system.trace_log table with query_id = empty string.
-         Zero means disabled.
-      -->
-    <total_memory_profiler_step>4194304</total_memory_profiler_step>
-
-    <!-- Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type.
-         The probability is for every alloc/free regardless to the size of the allocation.
-         Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit,
-          which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered.
-         You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling.
-      -->
-    <total_memory_tracker_sample_probability>0</total_memory_tracker_sample_probability>
-
-    <!-- Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve
-         correct maximum value. -->
-    <!-- <max_open_files>262144</max_open_files> -->
-
-    <!-- Size of cache of uncompressed blocks of data, used in tables of MergeTree family.
-         In bytes. Cache is single for server. Memory is allocated only on demand.
-         Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).
-         Uncompressed cache is advantageous only for very short queries and in rare cases.
-
-         Note: uncompressed cache can be pointless for lz4, because memory bandwidth
-         is slower than multi-core decompression on some server configurations.
-         Enabling it can sometimes paradoxically make queries slower.
-      -->
-    <uncompressed_cache_size>8589934592</uncompressed_cache_size>
-
-    <!-- Approximate size of mark cache, used in tables of MergeTree family.
-         In bytes. Cache is single for server. Memory is allocated only on demand.
-         You should not lower this value.
-      -->
-    <mark_cache_size>5368709120</mark_cache_size>
-
-
-    <!-- If you enable the `min_bytes_to_use_mmap_io` setting,
-         the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace.
-         It makes sense only for large files and helps only if data reside in page cache.
-         To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults)
-         and to reuse mappings from several threads and queries,
-         the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files).
-         The amount of data in mapped files can be monitored
-         in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics
-         and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric,
-         and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the
-         CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events.
-         Note that the amount of data in mapped files does not consume memory directly and is not accounted
-         in query or server memory usage - because this memory can be discarded similar to OS page cache.
-         The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree,
-         also it can be dropped manually by the SYSTEM DROP MMAP CACHE query.
-      -->
-    <mmap_cache_size>1000</mmap_cache_size>
-
-    <!-- Cache size in bytes for compiled expressions.-->
-    <compiled_expression_cache_size>134217728</compiled_expression_cache_size>
-
-    <!-- Cache size in elements for compiled expressions.-->
-    <compiled_expression_cache_elements_size>10000</compiled_expression_cache_elements_size>
-
-    <!-- Path to data directory, with trailing slash. -->
-    <path>/var/lib/clickhouse/</path>
-
-    <!-- Path to temporary data for processing hard queries. -->
-    <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
-
-    <!-- Policy from the <storage_configuration> for the temporary files.
-         If not set <tmp_path> is used, otherwise <tmp_path> is ignored.
-
-         Notes:
-         - move_factor              is ignored
-         - keep_free_space_bytes    is ignored
-         - max_data_part_size_bytes is ignored
-         - you must have exactly one volume in that policy
-    -->
-    <!-- <tmp_policy>tmp</tmp_policy> -->
-
-    <!-- Directory with user provided files that are accessible by 'file' table function. -->
-    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>
-
-    <!-- LDAP server definitions. -->
-    <ldap_servers>
-        <!-- List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users,
-              who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories.
-             Parameters:
-                host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty.
-                port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.
-                bind_dn - template used to construct the DN to bind to.
-                        The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual
-                         user name during each authentication attempt.
-                user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user.
-                        This is mainly used in search filters for further role mapping when the server is Active Directory. The
-                         resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default,
-                         user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected
-                         user DN value.
-                    base_dn - template used to construct the base DN for the LDAP search.
-                            The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings
-                             of the template with the actual user name and bind DN during the LDAP search.
-                    scope - scope of the LDAP search.
-                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
-                    search_filter - template used to construct the search filter for the LDAP search.
-                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}'
-                             substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.
-                            Note, that the special characters must be escaped properly in XML.
-                verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed
-                         to be successfully authenticated for all consecutive requests without contacting the LDAP server.
-                        Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request.
-                enable_tls - flag to trigger use of secure connection to the LDAP server.
-                        Specify 'no' for plain text (ldap://) protocol (not recommended).
-                        Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default).
-                        Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS).
-                tls_minimum_protocol_version - the minimum protocol version of SSL/TLS.
-                        Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default).
-                tls_require_cert - SSL/TLS peer certificate verification behavior.
-                        Accepted values are: 'never', 'allow', 'try', 'demand' (the default).
-                tls_cert_file - path to certificate file.
-                tls_key_file - path to certificate key file.
-                tls_ca_cert_file - path to CA certificate file.
-                tls_ca_cert_dir - path to the directory containing CA certificates.
-                tls_cipher_suite - allowed cipher suite (in OpenSSL notation).
-             Example:
-                <my_ldap_server>
-                    <host>localhost</host>
-                    <port>636</port>
-                    <bind_dn>uid={user_name},ou=users,dc=example,dc=com</bind_dn>
-                    <verification_cooldown>300</verification_cooldown>
-                    <enable_tls>yes</enable_tls>
-                    <tls_minimum_protocol_version>tls1.2</tls_minimum_protocol_version>
-                    <tls_require_cert>demand</tls_require_cert>
-                    <tls_cert_file>/path/to/tls_cert_file</tls_cert_file>
-                    <tls_key_file>/path/to/tls_key_file</tls_key_file>
-                    <tls_ca_cert_file>/path/to/tls_ca_cert_file</tls_ca_cert_file>
-                    <tls_ca_cert_dir>/path/to/tls_ca_cert_dir</tls_ca_cert_dir>
-                    <tls_cipher_suite>ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384</tls_cipher_suite>
-                </my_ldap_server>
-             Example (typical Active Directory with configured user DN detection for further role mapping):
-                <my_ad_server>
-                    <host>localhost</host>
-                    <port>389</port>
-                    <bind_dn>EXAMPLE\{user_name}</bind_dn>
-                    <user_dn_detection>
-                        <base_dn>CN=Users,DC=example,DC=com</base_dn>
-                        <search_filter>(&amp;(objectClass=user)(sAMAccountName={user_name}))</search_filter>
-                    </user_dn_detection>
-                    <enable_tls>no</enable_tls>
-                </my_ad_server>
-        -->
-    </ldap_servers>
-
-    <!-- To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured
-          to authenticate via Kerberos, define a single 'kerberos' section here.
-         Parameters:
-            principal - canonical service principal name, that will be acquired and used when accepting security contexts.
-                    This parameter is optional, if omitted, the default principal will be used.
-                    This parameter cannot be specified together with 'realm' parameter.
-            realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it.
-                    This parameter is optional, if omitted, no additional filtering by realm will be applied.
-                    This parameter cannot be specified together with 'principal' parameter.
-         Example:
-            <kerberos />
-         Example:
-            <kerberos>
-                <principal>HTTP/clickhouse.example.com@EXAMPLE.COM</principal>
-            </kerberos>
-         Example:
-            <kerberos>
-                <realm>EXAMPLE.COM</realm>
-            </kerberos>
-    -->
-
-    <!-- Sources to read users, roles, access rights, profiles of settings, quotas. -->
-    <user_directories>
-        <users_xml>
-            <!-- Path to configuration file with predefined users. -->
-            <path>users.xml</path>
-        </users_xml>
-        <local_directory>
-            <!-- Path to folder where users created by SQL commands are stored. -->
-            <path>/var/lib/clickhouse/access/</path>
-        </local_directory>
-
-        <!-- To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section
-              with the following parameters:
-                server - one of LDAP server names defined in 'ldap_servers' config section above.
-                        This parameter is mandatory and cannot be empty.
-                roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server.
-                        If no roles are specified here or assigned during role mapping (below), user will not be able to perform any
-                         actions after authentication.
-                role_mapping - section with LDAP search parameters and mapping rules.
-                        When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the
-                         name of the logged in user. For each entry found during that search, the value of the specified attribute is
-                         extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the
-                         value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by
-                         CREATE ROLE command.
-                        There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be
-                         applied.
-                    base_dn - template used to construct the base DN for the LDAP search.
-                            The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}'
-                             substrings of the template with the actual user name, bind DN, and user DN during each LDAP search.
-                    scope - scope of the LDAP search.
-                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
-                    search_filter - template used to construct the search filter for the LDAP search.
-                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and
-                             '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during
-                             each LDAP search.
-                            Note, that the special characters must be escaped properly in XML.
-                    attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default.
-                    prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by
-                             the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated
-                             as local role names. Empty, by default.
-             Example:
-                <ldap>
-                    <server>my_ldap_server</server>
-                    <roles>
-                        <my_local_role1 />
-                        <my_local_role2 />
-                    </roles>
-                    <role_mapping>
-                        <base_dn>ou=groups,dc=example,dc=com</base_dn>
-                        <scope>subtree</scope>
-                        <search_filter>(&amp;(objectClass=groupOfNames)(member={bind_dn}))</search_filter>
-                        <attribute>cn</attribute>
-                        <prefix>clickhouse_</prefix>
-                    </role_mapping>
-                </ldap>
-             Example (typical Active Directory with role mapping that relies on the detected user DN):
-                <ldap>
-                    <server>my_ad_server</server>
-                    <role_mapping>
-                        <base_dn>CN=Users,DC=example,DC=com</base_dn>
-                        <attribute>CN</attribute>
-                        <scope>subtree</scope>
-                        <search_filter>(&amp;(objectClass=group)(member={user_dn}))</search_filter>
-                        <prefix>clickhouse_</prefix>
-                    </role_mapping>
-                </ldap>
-        -->
-    </user_directories>
-
-    <!-- Default profile of settings. -->
-    <default_profile>default</default_profile>
-
-    <!-- Comma-separated list of prefixes for user-defined settings. -->
-    <custom_settings_prefixes></custom_settings_prefixes>
-
-    <!-- System profile of settings. This settings are used by internal processes (Distributed DDL worker and so on). -->
-    <!-- <system_profile>default</system_profile> -->
-
-    <!-- Buffer profile of settings.
-         This settings are used by Buffer storage to flush data to the underlying table.
-         Default: used from system_profile directive.
-    -->
-    <!-- <buffer_profile>default</buffer_profile> -->
-
-    <!-- Default database. -->
-    <default_database>default</default_database>
-
-    <!-- Server time zone could be set here.
-
-         Time zone is used when converting between String and DateTime types,
-          when printing DateTime in text formats and parsing DateTime from text,
-          it is used in date and time related functions, if specific time zone was not passed as an argument.
-
-         Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.
-         If not specified, system time zone at server startup is used.
-
-         Please note, that server could display time zone alias instead of specified name.
-         Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC.
-    -->
-    <!-- <timezone>Europe/Moscow</timezone> -->
-
-    <!-- You can specify umask here (see "man umask"). Server will apply it on startup.
-         Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).
-    -->
-    <!-- <umask>022</umask> -->
-
-    <!-- Perform mlockall after startup to lower first queries latency
-          and to prevent clickhouse executable from being paged out under high IO load.
-         Enabling this option is recommended but will lead to increased startup time for up to a few seconds.
-    -->
-    <mlock_executable>true</mlock_executable>
-
-    <!-- Reallocate memory for machine code ("text") using huge pages. Highly experimental. -->
-    <remap_executable>false</remap_executable>
-
-    <![CDATA[
-         Uncomment below in order to use JDBC table engine and function.
-
-         To install and run JDBC bridge in background:
-         * [Debian/Ubuntu]
-           export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge
-           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\(.*\)<.*|\1|')
-           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge_$PKG_VER-1_all.deb
-           apt install --no-install-recommends -f ./clickhouse-jdbc-bridge_$PKG_VER-1_all.deb
-           clickhouse-jdbc-bridge &
-
-         * [CentOS/RHEL]
-           export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge
-           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\(.*\)<.*|\1|')
-           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm
-           yum localinstall -y clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm
-           clickhouse-jdbc-bridge &
-
-         Please refer to https://github.com/ClickHouse/clickhouse-jdbc-bridge#usage for more information.
-    ]]>
-    <!--
-    <jdbc_bridge>
-        <host>127.0.0.1</host>
-        <port>9019</port>
-    </jdbc_bridge>
-    -->
-
-    <!-- Configuration of clusters that could be used in Distributed tables.
-         https://clickhouse.com/docs/en/operations/table_engines/distributed/
-      -->
-    <remote_servers>
-        <!-- Test only shard config for testing distributed storage -->
-        <test_shard_localhost>
-            <!-- Inter-server per-cluster secret for Distributed queries
-                 default: no secret (no authentication will be performed)
-
-                 If set, then Distributed queries will be validated on shards, so at least:
-                 - such cluster should exist on the shard,
-                 - such cluster should have the same secret.
-
-                 And also (and which is more important), the initial_user will
-                 be used as current user for the query.
-
-                 Right now the protocol is pretty simple and it only takes into account:
-                 - cluster name
-                 - query
-
-                 Also it will be nice if the following will be implemented:
-                 - source hostname (see interserver_http_host), but then it will depends from DNS,
-                   it can use IP address instead, but then the you need to get correct on the initiator node.
-                 - target hostname / ip address (same notes as for source hostname)
-                 - time-based security tokens
-            -->
-            <secret>REPLACE_ME</secret>
-
-            <shard>
-                <!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). -->
-                <!-- <internal_replication>false</internal_replication> -->
-                <!-- Optional. Shard weight when writing data. Default: 1. -->
-                <!-- <weight>1</weight> -->
-                <replica>
-                    <host>localhost</host>
-                    <port>9000</port>
-                    <!-- Optional. Priority of the replica for load_balancing. Default: 1 (less value has more priority). -->
-                    <!-- <priority>1</priority> -->
-                </replica>
-            </shard>
-        </test_shard_localhost>
-    </remote_servers>
-
-    <!-- The list of hosts allowed to use in URL-related storage engines and table functions.
-        If this section is not present in configuration, all hosts are allowed.
-    -->
-    <!--<remote_url_allow_hosts>-->
-    <!-- Host should be specified exactly as in URL. The name is checked before DNS resolution.
-        Example: "yandex.ru", "yandex.ru." and "www.yandex.ru" are different hosts.
-                If port is explicitly specified in URL, the host:port is checked as a whole.
-                If host specified here without port, any port with this host allowed.
-                "yandex.ru" -> "yandex.ru:443", "yandex.ru:80" etc. is allowed, but "yandex.ru:80" -> only "yandex.ru:80" is allowed.
-        If the host is specified as IP address, it is checked as specified in URL. Example: "[2a02:6b8:a::a]".
-        If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.
-        Host should be specified using the host xml tag:
-                <host>yandex.ru</host>
-    -->
-
-    <!-- Regular expression can be specified. RE2 engine is used for regexps.
-        Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter
-        (forgetting to do so is a common source of error).
-    -->
-    <!--</remote_url_allow_hosts>-->
-
-    <!-- If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.
-         By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.
-         Values for substitutions are specified in /clickhouse/name_of_substitution elements in that file.
-      -->
-
-    <!-- ZooKeeper is used to store metadata about replicas, when using Replicated tables.
-         Optional. If you don't use replicated tables, you could omit that.
-
-         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/
-      -->
-
-    <!--
-    <zookeeper>
-        <node>
-            <host>example1</host>
-            <port>2181</port>
-        </node>
-        <node>
-            <host>example2</host>
-            <port>2181</port>
-        </node>
-        <node>
-            <host>example3</host>
-            <port>2181</port>
-        </node>
-    </zookeeper>
-    -->
-
-    <!-- Substitutions for parameters of replicated tables.
-          Optional. If you don't use replicated tables, you could omit that.
-
-         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#creating-replicated-tables
-      -->
-    <!--
-    <macros>
-        <shard>01</shard>
-        <replica>example01-01-1</replica>
-    </macros>
-    -->
-
-
-    <!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. -->
-    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>
-
-
-    <!-- Maximum session timeout, in seconds. Default: 3600. -->
-    <max_session_timeout>3600</max_session_timeout>
-
-    <!-- Default session timeout, in seconds. Default: 60. -->
-    <default_session_timeout>60</default_session_timeout>
-
-    <!-- Sending data to Graphite for monitoring. Several sections can be defined. -->
-    <!--
-        interval - send every X second
-        root_path - prefix for keys
-        hostname_in_path - append hostname to root_path (default = true)
-        metrics - send data from table system.metrics
-        events - send data from table system.events
-        asynchronous_metrics - send data from table system.asynchronous_metrics
-    -->
-    <!--
-    <graphite>
-        <host>localhost</host>
-        <port>42000</port>
-        <timeout>0.1</timeout>
-        <interval>60</interval>
-        <root_path>one_min</root_path>
-        <hostname_in_path>true</hostname_in_path>
-
-        <metrics>true</metrics>
-        <events>true</events>
-        <events_cumulative>false</events_cumulative>
-        <asynchronous_metrics>true</asynchronous_metrics>
-    </graphite>
-    <graphite>
-        <host>localhost</host>
-        <port>42000</port>
-        <timeout>0.1</timeout>
-        <interval>1</interval>
-        <root_path>one_sec</root_path>
-
-        <metrics>true</metrics>
-        <events>true</events>
-        <events_cumulative>false</events_cumulative>
-        <asynchronous_metrics>false</asynchronous_metrics>
-    </graphite>
-    -->
-
-    <!-- Query log. Used only for queries with setting log_queries = 1. -->
-    <query_log>
-        <!-- What table to insert data. If table is not exist, it will be created.
-             When query log structure is changed after system update,
-              then old table will be renamed and new table will be created automatically.
-        -->
-        <database>system</database>
-        <table>query_log</table>
-        <!--
-            PARTITION BY expr: https://clickhouse.com/docs/en/table_engines/mergetree-family/custom_partitioning_key/
-            Example:
-                event_date
-                toMonday(event_date)
-                toYYYYMM(event_date)
-                toStartOfHour(event_time)
-        -->
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <!--
-            Table TTL specification: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl
-            Example:
-                event_date + INTERVAL 1 WEEK
-                event_date + INTERVAL 7 DAY DELETE
-                event_date + INTERVAL 2 WEEK TO DISK 'bbb'
-
-        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
-        -->
-
-        <!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,
-             Example: <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024</engine>
-          -->
-
-        <!-- Interval of flushing data. -->
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </query_log>
-
-    <!-- Trace log. Stores stack traces collected by query profilers.
-         See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->
-    <trace_log>
-        <database>system</database>
-        <table>trace_log</table>
-
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </trace_log>
-
-    <!-- Query thread log. Has information about all threads participated in query execution.
-         Used only for queries with setting log_query_threads = 1. -->
-    <query_thread_log>
-        <database>system</database>
-        <table>query_thread_log</table>
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </query_thread_log>
-
-    <!-- Query views log. Has information about all dependent views associated with a query.
-         Used only for queries with setting log_query_views = 1. -->
-    <query_views_log>
-        <database>system</database>
-        <table>query_views_log</table>
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </query_views_log>
-
-    <!-- Uncomment if use part log.
-         Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).-->
-    <part_log>
-        <database>system</database>
-        <table>part_log</table>
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </part_log>
-
-    <!-- Uncomment to write text log into table.
-         Text log contains all information from usual server log but stores it in structured and efficient way.
-         The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.
-    <text_log>
-        <database>system</database>
-        <table>text_log</table>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-        <level></level>
-    </text_log>
-    -->
-
-    <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with "collect_interval_milliseconds" interval. -->
-    <metric_log>
-        <database>system</database>
-        <table>metric_log</table>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-        <collect_interval_milliseconds>1000</collect_interval_milliseconds>
-    </metric_log>
-
-    <!--
-        Asynchronous metric log contains values of metrics from
-        system.asynchronous_metrics.
-    -->
-    <asynchronous_metric_log>
-        <database>system</database>
-        <table>asynchronous_metric_log</table>
-        <!--
-            Asynchronous metrics are updated once a minute, so there is
-            no need to flush more often.
-        -->
-        <flush_interval_milliseconds>7000</flush_interval_milliseconds>
-    </asynchronous_metric_log>
-
-    <!--
-        OpenTelemetry log contains OpenTelemetry trace spans.
-    -->
-    <opentelemetry_span_log>
-        <!--
-            The default table creation code is insufficient, this <engine> spec
-            is a workaround. There is no 'event_time' for this log, but two times,
-            start and finish. It is sorted by finish time, to avoid inserting
-            data too far away in the past (probably we can sometimes insert a span
-            that is seconds earlier than the last span in the table, due to a race
-            between several spans inserted in parallel). This gives the spans a
-            global order that we can use to e.g. retry insertion into some external
-            system.
-        -->
-        <engine>
-            engine MergeTree
-            partition by toYYYYMM(finish_date)
-            order by (finish_date, finish_time_us, trace_id)
-        </engine>
-        <database>system</database>
-        <table>opentelemetry_span_log</table>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </opentelemetry_span_log>
-
-
-    <!-- Crash log. Stores stack traces for fatal errors.
-         This table is normally empty. -->
-    <crash_log>
-        <database>system</database>
-        <table>crash_log</table>
-
-        <partition_by/>
-        <flush_interval_milliseconds>1000</flush_interval_milliseconds>
-    </crash_log>
-
-    <!-- Session log. Stores user log in (successful or not) and log out events. -->
-    <session_log>
-        <database>system</database>
-        <table>session_log</table>
-
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </session_log>
-
-    <!-- Parameters for embedded dictionaries, used in Yandex.Metrica.
-         See https://clickhouse.com/docs/en/dicts/internal_dicts/
-    -->
-
-    <!-- Path to file with region hierarchy. -->
-    <!-- <path_to_regions_hierarchy_file>/opt/geo/regions_hierarchy.txt</path_to_regions_hierarchy_file> -->
-
-    <!-- Path to directory with files containing names of regions -->
-    <!-- <path_to_regions_names_files>/opt/geo/</path_to_regions_names_files> -->
-
-
-    <!-- <top_level_domains_path>/var/lib/clickhouse/top_level_domains/</top_level_domains_path> -->
-    <!-- Custom TLD lists.
-         Format: <name>/path/to/file</name>
-
-         Changes will not be applied w/o server restart.
-         Path to the list is under top_level_domains_path (see above).
-    -->
-    <top_level_domains_lists>
-        <!--
-        <public_suffix_list>/path/to/public_suffix_list.dat</public_suffix_list>
-        -->
-    </top_level_domains_lists>
-
-    <!-- Configuration of external dictionaries. See:
-         https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts
-    -->
-    <dictionaries_config>*_dictionary.xml</dictionaries_config>
-
-    <!-- Configuration of user defined executable functions -->
-    <user_defined_executable_functions_config>*_function.xml</user_defined_executable_functions_config>
-
-    <!-- Uncomment if you want data to be compressed 30-100% better.
-         Don't do that if you just started using ClickHouse.
-      -->
-    <!--
-    <compression>
-        <!- - Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used. - ->
-        <case>
-
-            <!- - Conditions. All must be satisfied. Some conditions may be omitted. - ->
-            <min_part_size>10000000000</min_part_size>        <!- - Min part size in bytes. - ->
-            <min_part_size_ratio>0.01</min_part_size_ratio>   <!- - Min size of part relative to whole table size. - ->
-
-            <!- - What compression method to use. - ->
-            <method>zstd</method>
-        </case>
-    </compression>
-    -->
-
-    <!-- Configuration of encryption. The server executes a command to
-         obtain an encryption key at startup if such a command is
-         defined, or encryption codecs will be disabled otherwise. The
-         command is executed through /bin/sh and is expected to write
-         a Base64-encoded key to the stdout. -->
-    <encryption_codecs>
-        <!-- aes_128_gcm_siv -->
-        <!-- Example of getting hex key from env -->
-        <!-- the code should use this key and throw an exception if its length is not 16 bytes -->
-        <!--key_hex from_env="..."></key_hex -->
-
-        <!-- Example of multiple hex keys. They can be imported from env or be written down in config-->
-        <!-- the code should use these keys and throw an exception if their length is not 16 bytes -->
-        <!-- key_hex id="0">...</key_hex -->
-        <!-- key_hex id="1" from_env=".."></key_hex -->
-        <!-- key_hex id="2">...</key_hex -->
-        <!-- current_key_id>2</current_key_id -->
-
-        <!-- Example of getting hex key from config -->
-        <!-- the code should use this key and throw an exception if its length is not 16 bytes -->
-        <!-- key>...</key -->
-
-        <!-- example of adding nonce -->
-        <!-- nonce>...</nonce -->
-
-        <!-- /aes_128_gcm_siv -->
-    </encryption_codecs>
-
-    <!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.
-         Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. -->
-    <distributed_ddl>
-        <!-- Path in ZooKeeper to queue with DDL queries -->
-        <path>/clickhouse/task_queue/ddl</path>
-
-        <!-- Settings from this profile will be used to execute DDL queries -->
-        <!-- <profile>default</profile> -->
-
-        <!-- Controls how much ON CLUSTER queries can be run simultaneously. -->
-        <!-- <pool_size>1</pool_size> -->
-
-        <!--
-             Cleanup settings (active tasks will not be removed)
-        -->
-
-        <!-- Controls task TTL (default 1 week) -->
-        <!-- <task_max_lifetime>604800</task_max_lifetime> -->
-
-        <!-- Controls how often cleanup should be performed (in seconds) -->
-        <!-- <cleanup_delay_period>60</cleanup_delay_period> -->
-
-        <!-- Controls how many tasks could be in the queue -->
-        <!-- <max_tasks_in_queue>1000</max_tasks_in_queue> -->
-    </distributed_ddl>
-
-    <!-- Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h -->
-    <!--
-    <merge_tree>
-        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>
-    </merge_tree>
-    -->
-
-    <!-- Protection from accidental DROP.
-         If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.
-         If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.
-         By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.
-         The same for max_partition_size_to_drop.
-         Uncomment to disable protection.
-    -->
-    <!-- <max_table_size_to_drop>0</max_table_size_to_drop> -->
-    <!-- <max_partition_size_to_drop>0</max_partition_size_to_drop> -->
-
-    <!-- Example of parameters for GraphiteMergeTree table engine -->
-    <graphite_rollup_example>
-        <pattern>
-            <regexp>click_cost</regexp>
-            <function>any</function>
-            <retention>
-                <age>0</age>
-                <precision>3600</precision>
-            </retention>
-            <retention>
-                <age>86400</age>
-                <precision>60</precision>
-            </retention>
-        </pattern>
-        <default>
-            <function>max</function>
-            <retention>
-                <age>0</age>
-                <precision>60</precision>
-            </retention>
-            <retention>
-                <age>3600</age>
-                <precision>300</precision>
-            </retention>
-            <retention>
-                <age>86400</age>
-                <precision>3600</precision>
-            </retention>
-        </default>
-    </graphite_rollup_example>
-
-    <!-- Directory in <clickhouse-path> containing schema files for various input formats.
-         The directory will be created if it doesn't exist.
-      -->
-    <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>
-
-    <!-- Default query masking rules, matching lines would be replaced with something else in the logs
-        (both text logs and system.query_log).
-        name - name for the rule (optional)
-        regexp - RE2 compatible regular expression (mandatory)
-        replace - substitution string for sensitive data (optional, by default - six asterisks)
-    -->
-    <query_masking_rules>
-        <rule>
-            <name>hide encrypt/decrypt arguments</name>
-            <regexp>((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\s*\(\s*(?:'(?:\\'|.)+'|.*?)\s*\)</regexp>
-            <!-- or more secure, but also more invasive:
-                (aes_\w+)\s*\(.*\)
-            -->
-            <replace>\1(???)</replace>
-        </rule>
-    </query_masking_rules>
-
-    <!-- Uncomment to use custom http handlers.
-        rules are checked from top to bottom, first match runs the handler
-            url - to match request URL, you can use 'regex:' prefix to use regex match(optional)
-            methods - to match request method, you can use commas to separate multiple method matches(optional)
-            headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional)
-        handler is request handler
-            type - supported types: static, dynamic_query_handler, predefined_query_handler
-            query - use with predefined_query_handler type, executes query when the handler is called
-            query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the <query_param_name> value in HTTP request params
-            status - use with static type, response status code
-            content_type - use with static type, response content-type
-            response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client.
-
-    <http_handlers>
-        <rule>
-            <url>/</url>
-            <methods>POST,GET</methods>
-            <headers><pragma>no-cache</pragma></headers>
-            <handler>
-                <type>dynamic_query_handler</type>
-                <query_param_name>query</query_param_name>
-            </handler>
-        </rule>
-
-        <rule>
-            <url>/predefined_query</url>
-            <methods>POST,GET</methods>
-            <handler>
-                <type>predefined_query_handler</type>
-                <query>SELECT * FROM system.settings</query>
-            </handler>
-        </rule>
-
-        <rule>
-            <handler>
-                <type>static</type>
-                <status>200</status>
-                <content_type>text/plain; charset=UTF-8</content_type>
-                <response_content>config://http_server_default_response</response_content>
-            </handler>
-        </rule>
-    </http_handlers>
-    -->
-
-    <send_crash_reports>
-        <!-- Changing <enabled> to true allows sending crash reports to -->
-        <!-- the ClickHouse core developers team via Sentry https://sentry.io -->
-        <!-- Doing so at least in pre-production environments is highly appreciated -->
-        <enabled>false</enabled>
-        <!-- Change <anonymize> to true if you don't feel comfortable attaching the server hostname to the crash report -->
-        <anonymize>false</anonymize>
-        <!-- Default endpoint should be changed to different Sentry DSN only if you have -->
-        <!-- some in-house engineers or hired consultants who're going to debug ClickHouse issues for you -->
-        <endpoint>https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277</endpoint>
-    </send_crash_reports>
-
-    <!-- Uncomment to disable ClickHouse internal DNS caching. -->
-    <!-- <disable_internal_dns_cache>1</disable_internal_dns_cache> -->
-
-    <!-- You can also configure rocksdb like this: -->
-    <!--
-    <rocksdb>
-        <options>
-            <max_background_jobs>8</max_background_jobs>
-        </options>
-        <column_family_options>
-            <num_levels>2</num_levels>
-        </column_family_options>
-        <tables>
-            <table>
-                <name>TABLE</name>
-                <options>
-                    <max_background_jobs>8</max_background_jobs>
-                </options>
-                <column_family_options>
-                    <num_levels>2</num_levels>
-                </column_family_options>
-            </table>
-        </tables>
-    </rocksdb>
-    -->
-    <storage_configuration>
-        <disks>
-            <s3>
-                <type>s3</type>
-                <endpoint>https://storage.yandexcloud.net/my-bucket/root-path/</endpoint>
-                <access_key_id>REPLACE_ME</access_key_id>
-                <secret_access_key>REPLACE_ME</secret_access_key>
-                <region></region>
-                <header>Authorization: Bearer SOME-TOKEN</header>
-                <server_side_encryption_customer_key_base64>your_base64_encoded_customer_key
-                </server_side_encryption_customer_key_base64>
-                <server_side_encryption_kms_key_id>REPLACE_ME</server_side_encryption_kms_key_id>
-                <server_side_encryption_kms_encryption_context>REPLACE_ME</server_side_encryption_kms_encryption_context>
-                <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled>
-                <proxy>
-                    <uri>http://proxy1</uri>
-                    <uri>http://proxy2</uri>
-                </proxy>
-                <connect_timeout_ms>10000</connect_timeout_ms>
-                <request_timeout_ms>5000</request_timeout_ms>
-                <retry_attempts>10</retry_attempts>
-                <single_read_retries>4</single_read_retries>
-                <min_bytes_for_seek>1000</min_bytes_for_seek>
-                <metadata_path>/var/lib/clickhouse/disks/s3/</metadata_path>
-                <skip_access_check>false</skip_access_check>
-            </s3>
-        </disks>
-    </storage_configuration>
-</clickhouse>
diff --git a/programs/diagnostics/testdata/configs/xml/users.d/default-password.xml b/programs/diagnostics/testdata/configs/xml/users.d/default-password.xml
deleted file mode 100644
index 242a6a4b02e4..000000000000
--- a/programs/diagnostics/testdata/configs/xml/users.d/default-password.xml
+++ /dev/null
@@ -1,8 +0,0 @@
-<clickhouse>
-    <users>
-        <default>
-            <password remove="1"/>
-            <password_sha256_hex>REPLACE_ME</password_sha256_hex>
-        </default>
-    </users>
-</clickhouse>
\ No newline at end of file
diff --git a/programs/diagnostics/testdata/configs/xml/users.xml b/programs/diagnostics/testdata/configs/xml/users.xml
deleted file mode 100644
index cd5f17e922ec..000000000000
--- a/programs/diagnostics/testdata/configs/xml/users.xml
+++ /dev/null
@@ -1,57 +0,0 @@
-<clickhouse>
-    <!-- See also the files in users.d directory where the settings can be overridden. -->
-    <!-- Profiles of settings. -->
-    <include_from>../include/xml/user-include.xml</include_from>
-
-    <profiles>
-        <!-- Default settings. -->
-        <default>
-            <!-- Maximum memory usage for processing single query, in bytes. -->
-            <max_memory_usage>10000000000</max_memory_usage>
-
-            <load_balancing>random</load_balancing>
-            <log_query_threads>1</log_query_threads>
-        </default>
-        <!-- Profile that allows only read queries. -->
-        <readonly>
-            <readonly>1</readonly>
-        </readonly>
-    </profiles>
-    <!-- Users and ACL. -->
-    <users>
-        <test_user>
-            <include incl="test_user"></include>
-        </test_user>
-        <!-- If user name was not specified, 'default' user is used. -->
-        <default>
-            <password>REPLACE_ME</password>
-            <networks>
-                <ip>::/0</ip>
-            </networks>
-            <!-- Settings profile for user. -->
-            <profile>default</profile>
-            <!-- Quota for user. -->
-            <quota>default</quota>
-            <!-- User can create other users and grant rights to them. -->
-            <!-- <access_management>1</access_management> -->
-        </default>
-    </users>
-    <!-- Quotas. -->
-    <quotas>
-        <!-- Name of quota. -->
-        <default>
-            <!-- Limits for time interval. You could specify many intervals with different limits. -->
-            <interval>
-                <!-- Length of interval. -->
-                <duration>3600</duration>
-
-                <!-- No limits. Just calculate resource usage for time interval. -->
-                <queries>0</queries>
-                <errors>0</errors>
-                <result_rows>0</result_rows>
-                <read_rows>0</read_rows>
-                <execution_time>0</execution_time>
-            </interval>
-        </default>
-    </quotas>
-</clickhouse>
diff --git a/programs/diagnostics/testdata/configs/yaml/config.yaml b/programs/diagnostics/testdata/configs/yaml/config.yaml
deleted file mode 100644
index e577a99e675f..000000000000
--- a/programs/diagnostics/testdata/configs/yaml/config.yaml
+++ /dev/null
@@ -1,927 +0,0 @@
-# This is an example of a configuration file "config.xml" rewritten in YAML
-# You can read this documentation for detailed information about YAML configuration:
-# https://clickhouse.com/docs/en/operations/configuration-files/
-
-# NOTE: User and query level settings are set up in "users.yaml" file.
-# If you have accidentally specified user-level settings here, server won't start.
-# You can either move the settings to the right place inside "users.xml" file
-# or add skip_check_for_incorrect_settings: 1 here.
-include_from: "../include/yaml/server-include.yaml"
-logger:
-  # Possible levels [1]:
-  # - none (turns off logging)
-  # - fatal
-  # - critical
-  # - error
-  # - warning
-  # - notice
-  # - information
-  # - debug
-  # - trace
-  # [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114
-  level: trace
-  log: /var/log/clickhouse-server/clickhouse-server.log
-  errorlog: /var/log/clickhouse-server/clickhouse-server.err.log
-  # Rotation policy
-  # See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85
-  size: 1000M
-  count: 10
-  # console: 1
-  # Default behavior is autodetection (log to console if not daemon mode and is tty)
-
-  # Per level overrides (legacy):
-  # For example to suppress logging of the ConfigReloader you can use:
-  # NOTE: levels.logger is reserved, see below.
-  # levels:
-  #     ConfigReloader: none
-
-  # Per level overrides:
-  # For example to suppress logging of the RBAC for default user you can use:
-  # (But please note that the logger name maybe changed from version to version, even after minor upgrade)
-  # levels:
-  #     - logger:
-  #         name: 'ContextAccess (default)'
-  #         level: none
-  #     - logger:
-  #         name: 'DatabaseOrdinary (test)'
-  #         level: none
-
-# It is the name that will be shown in the clickhouse-client.
-# By default, anything with "production" will be highlighted in red in query prompt.
-# display_name: production
-
-# Port for HTTP API. See also 'https_port' for secure connections.
-# This interface is also used by ODBC and JDBC drivers (DataGrip, Dbeaver, ...)
-# and by most of web interfaces (embedded UI, Grafana, Redash, ...).
-http_port: 8123
-
-# Port for interaction by native protocol with:
-# - clickhouse-client and other native ClickHouse tools (clickhouse-benchmark);
-# - clickhouse-server with other clickhouse-servers for distributed query processing;
-# - ClickHouse drivers and applications supporting native protocol
-# (this protocol is also informally called as "the TCP protocol");
-# See also 'tcp_port_secure' for secure connections.
-tcp_port: 9000
-
-# Compatibility with MySQL protocol.
-# ClickHouse will pretend to be MySQL for applications connecting to this port.
-mysql_port: 9004
-
-# Compatibility with PostgreSQL protocol.
-# ClickHouse will pretend to be PostgreSQL for applications connecting to this port.
-postgresql_port: 9005
-
-# HTTP API with TLS (HTTPS).
-# You have to configure certificate to enable this interface.
-# See the openSSL section below.
-# https_port: 8443
-
-# Native interface with TLS.
-# You have to configure certificate to enable this interface.
-# See the openSSL section below.
-# tcp_port_secure: 9440
-
-# Native interface wrapped with PROXYv1 protocol
-# PROXYv1 header sent for every connection.
-# ClickHouse will extract information about proxy-forwarded client address from the header.
-# tcp_with_proxy_port: 9011
-
-# Port for communication between replicas. Used for data exchange.
-# It provides low-level data access between servers.
-# This port should not be accessible from untrusted networks.
-# See also 'interserver_http_credentials'.
-# Data transferred over connections to this port should not go through untrusted networks.
-# See also 'interserver_https_port'.
-interserver_http_port: 9009
-
-# Port for communication between replicas with TLS.
-# You have to configure certificate to enable this interface.
-# See the openSSL section below.
-# See also 'interserver_http_credentials'.
-# interserver_https_port: 9010
-
-# Hostname that is used by other replicas to request this server.
-# If not specified, than it is determined analogous to 'hostname -f' command.
-# This setting could be used to switch replication to another network interface
-# (the server may be connected to multiple networks via multiple addresses)
-# interserver_http_host: example.yandex.ru
-
-# You can specify credentials for authenthication between replicas.
-# This is required when interserver_https_port is accessible from untrusted networks,
-# and also recommended to avoid SSRF attacks from possibly compromised services in your network.
-# interserver_http_credentials:
-#     user: interserver
-#     password: ''
-
-# Listen specified address.
-# Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere.
-# Notes:
-# If you open connections from wildcard address, make sure that at least one of the following measures applied:
-# - server is protected by firewall and not accessible from untrusted networks;
-# - all users are restricted to subset of network addresses (see users.xml);
-# - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces.
-# - users without password have readonly access.
-# See also: https://www.shodan.io/search?query=clickhouse
-# listen_host: '::'
-
-# Same for hosts without support for IPv6:
-# listen_host: 0.0.0.0
-
-# Default values - try listen localhost on IPv4 and IPv6.
-# listen_host: '::1'
-# listen_host: 127.0.0.1
-
-# Don't exit if IPv6 or IPv4 networks are unavailable while trying to listen.
-# listen_try: 0
-
-# Allow multiple servers to listen on the same address:port. This is not recommended.
-# listen_reuse_port: 0
-
-# listen_backlog: 64
-max_connections: 4096
-
-# For 'Connection: keep-alive' in HTTP 1.1
-keep_alive_timeout: 3
-
-# gRPC protocol (see src/Server/grpc_protos/clickhouse_grpc.proto for the API)
-# grpc_port: 9100
-grpc:
-  enable_ssl: false
-
-  # The following two files are used only if enable_ssl=1
-  ssl_cert_file: /path/to/ssl_cert_file
-  ssl_key_file: /path/to/ssl_key_file
-
-  # Whether server will request client for a certificate
-  ssl_require_client_auth: false
-
-  # The following file is used only if ssl_require_client_auth=1
-  ssl_ca_cert_file: /path/to/ssl_ca_cert_file
-
-  # Default compression algorithm (applied if client doesn't specify another algorithm).
-  # Supported algorithms: none, deflate, gzip, stream_gzip
-  compression: deflate
-
-  # Default compression level (applied if client doesn't specify another level).
-  # Supported levels: none, low, medium, high
-  compression_level: medium
-
-  # Send/receive message size limits in bytes. -1 means unlimited
-  max_send_message_size: -1
-  max_receive_message_size: -1
-
-  # Enable if you want very detailed logs
-  verbose_logs: false
-
-# Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71
-openSSL:
-  server:
-    # Used for https server AND secure tcp port
-    # openssl req -subj "/CN=localhost" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt
-    certificateFile: /etc/clickhouse-server/server.crt
-    privateKeyFile: /etc/clickhouse-server/server.key
-
-    # dhparams are optional. You can delete the dhParamsFile: element.
-    # To generate dhparams, use the following command:
-    # openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096
-    # Only file format with BEGIN DH PARAMETERS is supported.
-    dhParamsFile: /etc/clickhouse-server/dhparam.pem
-    verificationMode: none
-    loadDefaultCAFile: true
-    cacheSessions: true
-    disableProtocols: 'sslv2,sslv3'
-    preferServerCiphers: true
-  client:
-    # Used for connecting to https dictionary source and secured Zookeeper communication
-    loadDefaultCAFile: true
-    cacheSessions: true
-    disableProtocols: 'sslv2,sslv3'
-    preferServerCiphers: true
-
-    # Use for self-signed: verificationMode: none
-    invalidCertificateHandler:
-      # Use for self-signed: name: AcceptCertificateHandler
-      name: RejectCertificateHandler
-
-# Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123
-# http_server_default_response: |-
-#     <html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>
-
-# Maximum number of concurrent queries.
-max_concurrent_queries: 100
-
-# Maximum memory usage (resident set size) for server process.
-# Zero value or unset means default. Default is "max_server_memory_usage_to_ram_ratio" of available physical RAM.
-# If the value is larger than "max_server_memory_usage_to_ram_ratio" of available physical RAM, it will be cut down.
-
-# The constraint is checked on query execution time.
-# If a query tries to allocate memory and the current memory usage plus allocation is greater
-# than specified threshold, exception will be thrown.
-
-# It is not practical to set this constraint to small values like just a few gigabytes,
-# because memory allocator will keep this amount of memory in caches and the server will deny service of queries.
-max_server_memory_usage: 0
-
-# Maximum number of threads in the Global thread pool.
-# This will default to a maximum of 10000 threads if not specified.
-# This setting will be useful in scenarios where there are a large number
-# of distributed queries that are running concurrently but are idling most
-# of the time, in which case a higher number of threads might be required.
-max_thread_pool_size: 10000
-
-# On memory constrained environments you may have to set this to value larger than 1.
-max_server_memory_usage_to_ram_ratio: 0.9
-
-# Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes).
-# Data will be stored in system.trace_log table with query_id = empty string.
-# Zero means disabled.
-total_memory_profiler_step: 4194304
-
-# Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type.
-# The probability is for every alloc/free regardless to the size of the allocation.
-# Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit,
-# which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered.
-# You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling.
-total_memory_tracker_sample_probability: 0
-
-# Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve
-# correct maximum value.
-# max_open_files: 262144
-
-# Size of cache of uncompressed blocks of data, used in tables of MergeTree family.
-# In bytes. Cache is single for server. Memory is allocated only on demand.
-# Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).
-# Uncompressed cache is advantageous only for very short queries and in rare cases.
-
-# Note: uncompressed cache can be pointless for lz4, because memory bandwidth
-# is slower than multi-core decompression on some server configurations.
-# Enabling it can sometimes paradoxically make queries slower.
-uncompressed_cache_size: 8589934592
-
-# Approximate size of mark cache, used in tables of MergeTree family.
-# In bytes. Cache is single for server. Memory is allocated only on demand.
-# You should not lower this value.
-mark_cache_size: 5368709120
-
-# If you enable the `min_bytes_to_use_mmap_io` setting,
-# the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace.
-# It makes sense only for large files and helps only if data reside in page cache.
-# To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults)
-# and to reuse mappings from several threads and queries,
-# the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files).
-# The amount of data in mapped files can be monitored
-# in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics
-# and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric,
-# and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the
-# CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events.
-# Note that the amount of data in mapped files does not consume memory directly and is not accounted
-# in query or server memory usage - because this memory can be discarded similar to OS page cache.
-# The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree,
-# also it can be dropped manually by the SYSTEM DROP MMAP CACHE query.
-mmap_cache_size: 1000
-
-# Cache size in bytes for compiled expressions.
-compiled_expression_cache_size: 134217728
-
-# Cache size in elements for compiled expressions.
-compiled_expression_cache_elements_size: 10000
-
-# Path to data directory, with trailing slash.
-path: /var/lib/clickhouse/
-
-# Path to temporary data for processing hard queries.
-tmp_path: /var/lib/clickhouse/tmp/
-
-# Policy from the <storage_configuration> for the temporary files.
-# If not set <tmp_path> is used, otherwise <tmp_path> is ignored.
-
-# Notes:
-# - move_factor              is ignored
-# - keep_free_space_bytes    is ignored
-# - max_data_part_size_bytes is ignored
-# - you must have exactly one volume in that policy
-# tmp_policy: tmp
-
-# Directory with user provided files that are accessible by 'file' table function.
-user_files_path: /var/lib/clickhouse/user_files/
-
-# LDAP server definitions.
-ldap_servers: ''
-
-# List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users,
-# who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories.
-# Parameters:
-# host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty.
-# port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.
-# bind_dn - template used to construct the DN to bind to.
-# The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual
-# user name during each authentication attempt.
-# user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user.
-# This is mainly used in search filters for further role mapping when the server is Active Directory. The
-# resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default,
-# user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected
-# user DN value.
-# base_dn - template used to construct the base DN for the LDAP search.
-# The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings
-# of the template with the actual user name and bind DN during the LDAP search.
-# scope - scope of the LDAP search.
-# Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
-# search_filter - template used to construct the search filter for the LDAP search.
-# The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}'
-# substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.
-# Note, that the special characters must be escaped properly in XML.
-# verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed
-# to be successfully authenticated for all consecutive requests without contacting the LDAP server.
-# Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request.
-# enable_tls - flag to trigger use of secure connection to the LDAP server.
-# Specify 'no' for plain text (ldap://) protocol (not recommended).
-# Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default).
-# Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS).
-# tls_minimum_protocol_version - the minimum protocol version of SSL/TLS.
-# Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default).
-# tls_require_cert - SSL/TLS peer certificate verification behavior.
-# Accepted values are: 'never', 'allow', 'try', 'demand' (the default).
-# tls_cert_file - path to certificate file.
-# tls_key_file - path to certificate key file.
-# tls_ca_cert_file - path to CA certificate file.
-# tls_ca_cert_dir - path to the directory containing CA certificates.
-# tls_cipher_suite - allowed cipher suite (in OpenSSL notation).
-# Example:
-# my_ldap_server:
-#     host: localhost
-#     port: 636
-#     bind_dn: 'uid={user_name},ou=users,dc=example,dc=com'
-#     verification_cooldown: 300
-#     enable_tls: yes
-#     tls_minimum_protocol_version: tls1.2
-#     tls_require_cert: demand
-#     tls_cert_file: /path/to/tls_cert_file
-#     tls_key_file: /path/to/tls_key_file
-#     tls_ca_cert_file: /path/to/tls_ca_cert_file
-#     tls_ca_cert_dir: /path/to/tls_ca_cert_dir
-#     tls_cipher_suite: ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384
-
-# Example (typical Active Directory with configured user DN detection for further role mapping):
-# my_ad_server:
-#     host: localhost
-#     port: 389
-#     bind_dn: 'EXAMPLE\{user_name}'
-#     user_dn_detection:
-#         base_dn: CN=Users,DC=example,DC=com
-#         search_filter: '(&amp;(objectClass=user)(sAMAccountName={user_name}))'
-#     enable_tls: no
-
-# To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured
-# to authenticate via Kerberos, define a single 'kerberos' section here.
-# Parameters:
-# principal - canonical service principal name, that will be acquired and used when accepting security contexts.
-# This parameter is optional, if omitted, the default principal will be used.
-# This parameter cannot be specified together with 'realm' parameter.
-# realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it.
-# This parameter is optional, if omitted, no additional filtering by realm will be applied.
-# This parameter cannot be specified together with 'principal' parameter.
-# Example:
-# kerberos: ''
-
-# Example:
-# kerberos:
-#     principal: HTTP/clickhouse.example.com@EXAMPLE.COM
-
-# Example:
-# kerberos:
-#     realm: EXAMPLE.COM
-
-# Sources to read users, roles, access rights, profiles of settings, quotas.
-user_directories:
-  users_xml:
-    # Path to configuration file with predefined users.
-    path: users.yaml
-  local_directory:
-    # Path to folder where users created by SQL commands are stored.
-    path: /var/lib/clickhouse/access/
-
-#   # To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section
-#   # with the following parameters:
-#   # server - one of LDAP server names defined in 'ldap_servers' config section above.
-#   # This parameter is mandatory and cannot be empty.
-#   # roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server.
-#   # If no roles are specified here or assigned during role mapping (below), user will not be able to perform any
-#   # actions after authentication.
-#   # role_mapping - section with LDAP search parameters and mapping rules.
-#   # When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the
-#   # name of the logged in user. For each entry found during that search, the value of the specified attribute is
-#   # extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the
-#   # value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by
-#   # CREATE ROLE command.
-#   # There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be
-#   # applied.
-#   # base_dn - template used to construct the base DN for the LDAP search.
-#   # The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}'
-#   # substrings of the template with the actual user name, bind DN, and user DN during each LDAP search.
-#   # scope - scope of the LDAP search.
-#   # Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
-#   # search_filter - template used to construct the search filter for the LDAP search.
-#   # The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and
-#   # '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during
-#   # each LDAP search.
-#   # Note, that the special characters must be escaped properly in XML.
-#   # attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default.
-#   # prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by
-#   # the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated
-#   # as local role names. Empty, by default.
-#   # Example:
-#   # ldap:
-#   #     server: my_ldap_server
-#   #     roles:
-#   #         my_local_role1: ''
-#   #         my_local_role2: ''
-#   #     role_mapping:
-#   #         base_dn: 'ou=groups,dc=example,dc=com'
-#   #         scope: subtree
-#   #         search_filter: '(&amp;(objectClass=groupOfNames)(member={bind_dn}))'
-#   #         attribute: cn
-#   #         prefix: clickhouse_
-#   # Example (typical Active Directory with role mapping that relies on the detected user DN):
-#   # ldap:
-#   #     server: my_ad_server
-#   #     role_mapping:
-#   #         base_dn: 'CN=Users,DC=example,DC=com'
-#   #         attribute: CN
-#   #         scope: subtree
-#   #         search_filter: '(&amp;(objectClass=group)(member={user_dn}))'
-#   #         prefix: clickhouse_
-
-# Default profile of settings.
-default_profile: default
-
-# Comma-separated list of prefixes for user-defined settings.
-# custom_settings_prefixes: ''
-# System profile of settings. This settings are used by internal processes (Distributed DDL worker and so on).
-# system_profile: default
-
-# Buffer profile of settings.
-# This settings are used by Buffer storage to flush data to the underlying table.
-# Default: used from system_profile directive.
-# buffer_profile: default
-
-# Default database.
-default_database: default
-
-# Server time zone could be set here.
-
-# Time zone is used when converting between String and DateTime types,
-# when printing DateTime in text formats and parsing DateTime from text,
-# it is used in date and time related functions, if specific time zone was not passed as an argument.
-
-# Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.
-# If not specified, system time zone at server startup is used.
-
-# Please note, that server could display time zone alias instead of specified name.
-# Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC.
-# timezone: Europe/Moscow
-
-# You can specify umask here (see "man umask"). Server will apply it on startup.
-# Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).
-# umask: 022
-
-# Perform mlockall after startup to lower first queries latency
-# and to prevent clickhouse executable from being paged out under high IO load.
-# Enabling this option is recommended but will lead to increased startup time for up to a few seconds.
-mlock_executable: true
-
-# Reallocate memory for machine code ("text") using huge pages. Highly experimental.
-remap_executable: false
-
-# Uncomment below in order to use JDBC table engine and function.
-# To install and run JDBC bridge in background:
-# * [Debian/Ubuntu]
-# export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge
-# export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\(.*\)<.*|\1|')
-# wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge_$PKG_VER-1_all.deb
-# apt install --no-install-recommends -f ./clickhouse-jdbc-bridge_$PKG_VER-1_all.deb
-# clickhouse-jdbc-bridge &
-# * [CentOS/RHEL]
-# export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge
-# export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\(.*\)<.*|\1|')
-# wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm
-# yum localinstall -y clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm
-# clickhouse-jdbc-bridge &
-# Please refer to https://github.com/ClickHouse/clickhouse-jdbc-bridge#usage for more information.
-
-# jdbc_bridge:
-#     host: 127.0.0.1
-#     port: 9019
-
-# Configuration of clusters that could be used in Distributed tables.
-# https://clickhouse.com/docs/en/operations/table_engines/distributed/
-remote_servers:
-  # Test only shard config for testing distributed storage
-  test_shard_localhost:
-    # Inter-server per-cluster secret for Distributed queries
-    # default: no secret (no authentication will be performed)
-
-    # If set, then Distributed queries will be validated on shards, so at least:
-    # - such cluster should exist on the shard,
-    # - such cluster should have the same secret.
-
-    # And also (and which is more important), the initial_user will
-    # be used as current user for the query.
-
-    # Right now the protocol is pretty simple and it only takes into account:
-    # - cluster name
-    # - query
-
-    # Also it will be nice if the following will be implemented:
-    # - source hostname (see interserver_http_host), but then it will depends from DNS,
-    # it can use IP address instead, but then the you need to get correct on the initiator node.
-    # - target hostname / ip address (same notes as for source hostname)
-    # - time-based security tokens
-    secret: 'REPLACE_ME'
-    shard:
-      # Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas).
-      # internal_replication: false
-      # Optional. Shard weight when writing data. Default: 1.
-      # weight: 1
-      replica:
-        host: localhost
-        port: 9000
-        # Optional. Priority of the replica for load_balancing. Default: 1 (less value has more priority).
-        # priority: 1
-
-# The list of hosts allowed to use in URL-related storage engines and table functions.
-# If this section is not present in configuration, all hosts are allowed.
-# remote_url_allow_hosts:
-
-# Host should be specified exactly as in URL. The name is checked before DNS resolution.
-# Example: "yandex.ru", "yandex.ru." and "www.yandex.ru" are different hosts.
-# If port is explicitly specified in URL, the host:port is checked as a whole.
-# If host specified here without port, any port with this host allowed.
-# "yandex.ru" -> "yandex.ru:443", "yandex.ru:80" etc. is allowed, but "yandex.ru:80" -> only "yandex.ru:80" is allowed.
-# If the host is specified as IP address, it is checked as specified in URL. Example: "[2a02:6b8:a::a]".
-# If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.
-
-# Regular expression can be specified. RE2 engine is used for regexps.
-# Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter
-# (forgetting to do so is a common source of error).
-
-# If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.
-# By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.
-# Values for substitutions are specified in /clickhouse/name_of_substitution elements in that file.
-
-# ZooKeeper is used to store metadata about replicas, when using Replicated tables.
-# Optional. If you don't use replicated tables, you could omit that.
-# See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/
-
-# zookeeper:
-#     - node:
-#         host: example1
-#         port: 2181
-#     - node:
-#         host: example2
-#         port: 2181
-#     - node:
-#         host: example3
-#         port: 2181
-
-# Substitutions for parameters of replicated tables.
-# Optional. If you don't use replicated tables, you could omit that.
-# See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#creating-replicated-tables
-# macros:
-#     shard: 01
-#     replica: example01-01-1
-
-# Reloading interval for embedded dictionaries, in seconds. Default: 3600.
-builtin_dictionaries_reload_interval: 3600
-
-# Maximum session timeout, in seconds. Default: 3600.
-max_session_timeout: 3600
-
-# Default session timeout, in seconds. Default: 60.
-default_session_timeout: 60
-
-# Sending data to Graphite for monitoring. Several sections can be defined.
-# interval - send every X second
-# root_path - prefix for keys
-# hostname_in_path - append hostname to root_path (default = true)
-# metrics - send data from table system.metrics
-# events - send data from table system.events
-# asynchronous_metrics - send data from table system.asynchronous_metrics
-
-# graphite:
-#     host: localhost
-#     port: 42000
-#     timeout: 0.1
-#     interval: 60
-#     root_path: one_min
-#     hostname_in_path: true
-
-#     metrics: true
-#     events: true
-#     events_cumulative: false
-#     asynchronous_metrics: true
-
-# graphite:
-#     host: localhost
-#     port: 42000
-#     timeout: 0.1
-#     interval: 1
-#     root_path: one_sec
-
-#     metrics: true
-#     events: true
-#     events_cumulative: false
-#     asynchronous_metrics: false
-
-# Serve endpoint for Prometheus monitoring.
-# endpoint - mertics path (relative to root, statring with "/")
-# port - port to setup server. If not defined or 0 than http_port used
-# metrics - send data from table system.metrics
-# events - send data from table system.events
-# asynchronous_metrics - send data from table system.asynchronous_metrics
-
-# prometheus:
-#     endpoint: /metrics
-#     port: 9363
-
-#     metrics: true
-#     events: true
-#     asynchronous_metrics: true
-
-# Query log. Used only for queries with setting log_queries = 1.
-query_log:
-  # What table to insert data. If table is not exist, it will be created.
-  # When query log structure is changed after system update,
-  # then old table will be renamed and new table will be created automatically.
-  database: system
-  table: query_log
-
-  # PARTITION BY expr: https://clickhouse.com/docs/en/table_engines/mergetree-family/custom_partitioning_key/
-  # Example:
-  # event_date
-  # toMonday(event_date)
-  # toYYYYMM(event_date)
-  # toStartOfHour(event_time)
-  partition_by: toYYYYMM(event_date)
-
-  # Table TTL specification: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl
-  # Example:
-  # event_date + INTERVAL 1 WEEK
-  # event_date + INTERVAL 7 DAY DELETE
-  # event_date + INTERVAL 2 WEEK TO DISK 'bbb'
-
-  # ttl: 'event_date + INTERVAL 30 DAY DELETE'
-
-  # Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,
-  # Example: engine: 'ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024'
-
-  # Interval of flushing data.
-  flush_interval_milliseconds: 7500
-
-# Trace log. Stores stack traces collected by query profilers.
-# See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings.
-trace_log:
-  database: system
-  table: trace_log
-  partition_by: toYYYYMM(event_date)
-  flush_interval_milliseconds: 7500
-
-# Query thread log. Has information about all threads participated in query execution.
-# Used only for queries with setting log_query_threads = 1.
-query_thread_log:
-  database: system
-  table: query_thread_log
-  partition_by: toYYYYMM(event_date)
-  flush_interval_milliseconds: 7500
-
-# Query views log. Has information about all dependent views associated with a query.
-# Used only for queries with setting log_query_views = 1.
-query_views_log:
-  database: system
-  table: query_views_log
-  partition_by: toYYYYMM(event_date)
-  flush_interval_milliseconds: 7500
-
-# Uncomment if use part log.
-# Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).
-part_log:
-  database: system
-  table: part_log
-  partition_by: toYYYYMM(event_date)
-  flush_interval_milliseconds: 7500
-
-# Uncomment to write text log into table.
-# Text log contains all information from usual server log but stores it in structured and efficient way.
-# The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.
-# text_log:
-#     database: system
-#     table: text_log
-#     flush_interval_milliseconds: 7500
-#     level: ''
-
-# Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with "collect_interval_milliseconds" interval.
-metric_log:
-  database: system
-  table: metric_log
-  flush_interval_milliseconds: 7500
-  collect_interval_milliseconds: 1000
-
-# Asynchronous metric log contains values of metrics from
-# system.asynchronous_metrics.
-asynchronous_metric_log:
-  database: system
-  table: asynchronous_metric_log
-
-  # Asynchronous metrics are updated once a minute, so there is
-  # no need to flush more often.
-  flush_interval_milliseconds: 60000
-
-# OpenTelemetry log contains OpenTelemetry trace spans.
-opentelemetry_span_log:
-
-  # The default table creation code is insufficient, this <engine> spec
-  # is a workaround. There is no 'event_time' for this log, but two times,
-  # start and finish. It is sorted by finish time, to avoid inserting
-  # data too far away in the past (probably we can sometimes insert a span
-  # that is seconds earlier than the last span in the table, due to a race
-  # between several spans inserted in parallel). This gives the spans a
-  # global order that we can use to e.g. retry insertion into some external
-  # system.
-  engine: |-
-    engine MergeTree
-         partition by toYYYYMM(finish_date)
-         order by (finish_date, finish_time_us, trace_id)
-  database: system
-  table: opentelemetry_span_log
-  flush_interval_milliseconds: 7500
-
-# Crash log. Stores stack traces for fatal errors.
-# This table is normally empty.
-crash_log:
-  database: system
-  table: crash_log
-  partition_by: ''
-  flush_interval_milliseconds: 1000
-
-# Parameters for embedded dictionaries, used in Yandex.Metrica.
-# See https://clickhouse.com/docs/en/dicts/internal_dicts/
-
-# Path to file with region hierarchy.
-# path_to_regions_hierarchy_file: /opt/geo/regions_hierarchy.txt
-
-# Path to directory with files containing names of regions
-# path_to_regions_names_files: /opt/geo/
-
-
-# top_level_domains_path: /var/lib/clickhouse/top_level_domains/
-# Custom TLD lists.
-# Format: name: /path/to/file
-
-# Changes will not be applied w/o server restart.
-# Path to the list is under top_level_domains_path (see above).
-top_level_domains_lists: ''
-
-# public_suffix_list: /path/to/public_suffix_list.dat
-
-# Configuration of external dictionaries. See:
-# https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts
-dictionaries_config: '*_dictionary.xml'
-
-# Uncomment if you want data to be compressed 30-100% better.
-# Don't do that if you just started using ClickHouse.
-
-# compression:
-#     # Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used.
-#     case:
-#         Conditions. All must be satisfied. Some conditions may be omitted.
-#         # min_part_size: 10000000000    # Min part size in bytes.
-#         # min_part_size_ratio: 0.01     # Min size of part relative to whole table size.
-#         # What compression method to use.
-#         method: zstd
-
-# Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.
-# Works only if ZooKeeper is enabled. Comment it if such functionality isn't required.
-distributed_ddl:
-  # Path in ZooKeeper to queue with DDL queries
-  path: /clickhouse/task_queue/ddl
-
-  # Settings from this profile will be used to execute DDL queries
-  # profile: default
-
-  # Controls how much ON CLUSTER queries can be run simultaneously.
-  # pool_size: 1
-
-  # Cleanup settings (active tasks will not be removed)
-
-  # Controls task TTL (default 1 week)
-  # task_max_lifetime: 604800
-
-  # Controls how often cleanup should be performed (in seconds)
-  # cleanup_delay_period: 60
-
-  # Controls how many tasks could be in the queue
-  # max_tasks_in_queue: 1000
-
-# Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h
-# merge_tree:
-#     max_suspicious_broken_parts: 5
-
-# Protection from accidental DROP.
-# If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.
-# If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.
-# By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.
-# The same for max_partition_size_to_drop.
-# Uncomment to disable protection.
-
-# max_table_size_to_drop: 0
-# max_partition_size_to_drop: 0
-
-# Example of parameters for GraphiteMergeTree table engine
-graphite_rollup_example:
-  pattern:
-    regexp: click_cost
-    function: any
-    retention:
-      - age: 0
-        precision: 3600
-      - age: 86400
-        precision: 60
-  default:
-    function: max
-    retention:
-      - age: 0
-        precision: 60
-      - age: 3600
-        precision: 300
-      - age: 86400
-        precision: 3600
-
-# Directory in <clickhouse-path> containing schema files for various input formats.
-# The directory will be created if it doesn't exist.
-format_schema_path: /var/lib/clickhouse/format_schemas/
-
-# Default query masking rules, matching lines would be replaced with something else in the logs
-# (both text logs and system.query_log).
-# name - name for the rule (optional)
-# regexp - RE2 compatible regular expression (mandatory)
-# replace - substitution string for sensitive data (optional, by default - six asterisks)
-query_masking_rules:
-  rule:
-    name: hide encrypt/decrypt arguments
-    regexp: '((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\s*\(\s*(?:''(?:\\''|.)+''|.*?)\s*\)'
-    # or more secure, but also more invasive:
-    # (aes_\w+)\s*\(.*\)
-    replace: \1(???)
-
-# Uncomment to use custom http handlers.
-# rules are checked from top to bottom, first match runs the handler
-# url - to match request URL, you can use 'regex:' prefix to use regex match(optional)
-# methods - to match request method, you can use commas to separate multiple method matches(optional)
-# headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional)
-# handler is request handler
-# type - supported types: static, dynamic_query_handler, predefined_query_handler
-# query - use with predefined_query_handler type, executes query when the handler is called
-# query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the <query_param_name> value in HTTP request params
-# status - use with static type, response status code
-# content_type - use with static type, response content-type
-# response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client.
-
-# http_handlers:
-#     - rule:
-#         url: /
-#         methods: POST,GET
-#         headers:
-#           pragma: no-cache
-#         handler:
-#           type: dynamic_query_handler
-#           query_param_name: query
-#     - rule:
-#         url: /predefined_query
-#         methods: POST,GET
-#         handler:
-#           type: predefined_query_handler
-#           query: 'SELECT * FROM system.settings'
-#     - rule:
-#         handler:
-#           type: static
-#           status: 200
-#           content_type: 'text/plain; charset=UTF-8'
-#           response_content: config://http_server_default_response
-
-send_crash_reports:
-  # Changing <enabled> to true allows sending crash reports to
-  # the ClickHouse core developers team via Sentry https://sentry.io
-  # Doing so at least in pre-production environments is highly appreciated
-  enabled: false
-  # Change <anonymize> to true if you don't feel comfortable attaching the server hostname to the crash report
-  anonymize: false
-  # Default endpoint should be changed to different Sentry DSN only if you have
-  # some in-house engineers or hired consultants who're going to debug ClickHouse issues for you
-  endpoint: 'https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277'
-  # Uncomment to disable ClickHouse internal DNS caching.
-  # disable_internal_dns_cache: 1
-
-storage_configuration:
-  disks:
-    s3:
-      secret_access_key: REPLACE_ME
-      access_key_id: 'REPLACE_ME'
diff --git a/programs/diagnostics/testdata/configs/yaml/users.d/default-password.yaml b/programs/diagnostics/testdata/configs/yaml/users.d/default-password.yaml
deleted file mode 100644
index c27bb7cb0714..000000000000
--- a/programs/diagnostics/testdata/configs/yaml/users.d/default-password.yaml
+++ /dev/null
@@ -1,6 +0,0 @@
-# Users and ACL.
-users:
-  # If user name was not specified, 'default' user is used.
-  default:
-
-    password_sha256_hex: "REPLACE_ME"
diff --git a/programs/diagnostics/testdata/configs/yaml/users.yaml b/programs/diagnostics/testdata/configs/yaml/users.yaml
deleted file mode 100644
index 82f2d67f2a4c..000000000000
--- a/programs/diagnostics/testdata/configs/yaml/users.yaml
+++ /dev/null
@@ -1,47 +0,0 @@
-include_from: "../include/yaml/user-include.yaml"
-# Profiles of settings.
-profiles:
-  # Default settings.
-  default:
-    # Maximum memory usage for processing single query, in bytes.
-    max_memory_usage: 10000000000
-    load_balancing: random
-
-    # Profile that allows only read queries.
-  readonly:
-    readonly: 1
-
-# Users and ACL.
-users:
-  # If user name was not specified, 'default' user is used.
-  default:
-
-    password: 'REPLACE_ME'
-
-    networks:
-      ip: '::/0'
-
-    # Settings profile for user.
-    profile: default
-
-    # Quota for user.
-    quota: default
-
-    # User can create other users and grant rights to them.
-    # access_management: 1
-
-# Quotas.
-quotas:
-  # Name of quota.
-  default:
-    # Limits for time interval. You could specify many intervals with different limits.
-    interval:
-      # Length of interval.
-      duration: 3600
-
-      # No limits. Just calculate resource usage for time interval.
-      queries: 0
-      errors: 0
-      result_rows: 0
-      read_rows: 0
-      execution_time: 0
diff --git a/programs/diagnostics/testdata/configs/yandex_xml/config.xml b/programs/diagnostics/testdata/configs/yandex_xml/config.xml
deleted file mode 100644
index 40d1fa34b1a9..000000000000
--- a/programs/diagnostics/testdata/configs/yandex_xml/config.xml
+++ /dev/null
@@ -1,1167 +0,0 @@
-<!--
-  NOTE: User and query level settings are set up in "users.xml" file.
-  If you have accidentally specified user-level settings here, server won't start.
-  You can either move the settings to the right place inside "users.xml" file
-   or add <skip_check_for_incorrect_settings>1</skip_check_for_incorrect_settings> here.
--->
-<yandex>
-    <include_from>../include/xml/server-include.xml</include_from>
-    <logger>
-        <!-- Possible levels [1]:
-
-          - none (turns off logging)
-          - fatal
-          - critical
-          - error
-          - warning
-          - notice
-          - information
-          - debug
-          - trace
-          - test (not for production usage)
-
-            [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114
-        -->
-        <level>trace</level>
-        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
-        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
-        <!-- Rotation policy
-             See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85
-          -->
-        <size>1000M</size>
-        <count>10</count>
-        <!-- <console>1</console> --> <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->
-
-        <!-- Per level overrides (legacy):
-
-        For example to suppress logging of the ConfigReloader you can use:
-        NOTE: levels.logger is reserved, see below.
-        -->
-        <!--
-        <levels>
-          <ConfigReloader>none</ConfigReloader>
-        </levels>
-        -->
-
-        <!-- Per level overrides:
-
-        For example to suppress logging of the RBAC for default user you can use:
-        (But please note that the logger name maybe changed from version to version, even after minor upgrade)
-        -->
-        <!--
-        <levels>
-          <logger>
-            <name>ContextAccess (default)</name>
-            <level>none</level>
-          </logger>
-          <logger>
-            <name>DatabaseOrdinary (test)</name>
-            <level>none</level>
-          </logger>
-        </levels>
-        -->
-    </logger>
-    <!-- Add headers to response in options request. OPTIONS method is used in CORS preflight requests. -->
-    <!-- It is off by default. Next headers are obligate for CORS.-->
-    <!-- http_options_response>
-        <header>
-            <name>Access-Control-Allow-Origin</name>
-            <value>*</value>
-        </header>
-        <header>
-            <name>Access-Control-Allow-Headers</name>
-            <value>origin, x-requested-with</value>
-        </header>
-        <header>
-            <name>Access-Control-Allow-Methods</name>
-            <value>POST, GET, OPTIONS</value>
-        </header>
-        <header>
-            <name>Access-Control-Max-Age</name>
-            <value>86400</value>
-        </header>
-    </http_options_response -->
-
-    <!-- It is the name that will be shown in the clickhouse-client.
-         By default, anything with "production" will be highlighted in red in query prompt.
-    -->
-    <!--display_name>production</display_name-->
-
-    <!-- Port for HTTP API. See also 'https_port' for secure connections.
-         This interface is also used by ODBC and JDBC drivers (DataGrip, Dbeaver, ...)
-         and by most of web interfaces (embedded UI, Grafana, Redash, ...).
-      -->
-    <http_port>8123</http_port>
-
-    <!-- Port for interaction by native protocol with:
-         - clickhouse-client and other native ClickHouse tools (clickhouse-benchmark);
-         - clickhouse-server with other clickhouse-servers for distributed query processing;
-         - ClickHouse drivers and applications supporting native protocol
-         (this protocol is also informally called as "the TCP protocol");
-         See also 'tcp_port_secure' for secure connections.
-    -->
-    <tcp_port>9000</tcp_port>
-
-    <!-- Compatibility with MySQL protocol.
-         ClickHouse will pretend to be MySQL for applications connecting to this port.
-    -->
-    <mysql_port>9004</mysql_port>
-
-    <!-- Compatibility with PostgreSQL protocol.
-         ClickHouse will pretend to be PostgreSQL for applications connecting to this port.
-    -->
-    <postgresql_port>9005</postgresql_port>
-
-    <!-- HTTP API with TLS (HTTPS).
-         You have to configure certificate to enable this interface.
-         See the openSSL section below.
-    -->
-    <!-- <https_port>8443</https_port> -->
-
-    <!-- Native interface with TLS.
-         You have to configure certificate to enable this interface.
-         See the openSSL section below.
-    -->
-    <!-- <tcp_port_secure>9440</tcp_port_secure> -->
-
-    <!-- Native interface wrapped with PROXYv1 protocol
-         PROXYv1 header sent for every connection.
-         ClickHouse will extract information about proxy-forwarded client address from the header.
-    -->
-    <!-- <tcp_with_proxy_port>9011</tcp_with_proxy_port> -->
-
-    <!-- Port for communication between replicas. Used for data exchange.
-         It provides low-level data access between servers.
-         This port should not be accessible from untrusted networks.
-         See also 'interserver_http_credentials'.
-         Data transferred over connections to this port should not go through untrusted networks.
-         See also 'interserver_https_port'.
-      -->
-    <interserver_http_port>9009</interserver_http_port>
-
-    <!-- Port for communication between replicas with TLS.
-         You have to configure certificate to enable this interface.
-         See the openSSL section below.
-         See also 'interserver_http_credentials'.
-      -->
-    <!-- <interserver_https_port>9010</interserver_https_port> -->
-
-    <!-- Hostname that is used by other replicas to request this server.
-         If not specified, than it is determined analogous to 'hostname -f' command.
-         This setting could be used to switch replication to another network interface
-         (the server may be connected to multiple networks via multiple addresses)
-      -->
-    <!--
-    <interserver_http_host>example.yandex.ru</interserver_http_host>
-    -->
-
-    <!-- You can specify credentials for authenthication between replicas.
-         This is required when interserver_https_port is accessible from untrusted networks,
-         and also recommended to avoid SSRF attacks from possibly compromised services in your network.
-      -->
-    <!--<interserver_http_credentials>
-        <user>interserver</user>
-        <password></password>
-    </interserver_http_credentials>-->
-
-    <!-- Listen specified address.
-         Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere.
-         Notes:
-         If you open connections from wildcard address, make sure that at least one of the following measures applied:
-         - server is protected by firewall and not accessible from untrusted networks;
-         - all users are restricted to subset of network addresses (see users.xml);
-         - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces.
-         - users without password have readonly access.
-         See also: https://www.shodan.io/search?query=clickhouse
-      -->
-    <!-- <listen_host>::</listen_host> -->
-
-    <!-- Same for hosts without support for IPv6: -->
-    <!-- <listen_host>0.0.0.0</listen_host> -->
-
-    <!-- Default values - try listen localhost on IPv4 and IPv6. -->
-    <!--
-    <listen_host>::1</listen_host>
-    <listen_host>127.0.0.1</listen_host>
-    -->
-
-    <!-- Don't exit if IPv6 or IPv4 networks are unavailable while trying to listen. -->
-    <!-- <listen_try>0</listen_try> -->
-
-    <!-- Allow multiple servers to listen on the same address:port. This is not recommended.
-      -->
-    <!-- <listen_reuse_port>0</listen_reuse_port> -->
-
-    <!-- <listen_backlog>4096</listen_backlog> -->
-
-    <max_connections>4096</max_connections>
-
-    <!-- For 'Connection: keep-alive' in HTTP 1.1 -->
-    <keep_alive_timeout>3</keep_alive_timeout>
-
-    <!-- gRPC protocol (see src/Server/grpc_protos/clickhouse_grpc.proto for the API) -->
-    <!-- <grpc_port>9100</grpc_port> -->
-    <grpc>
-        <enable_ssl>false</enable_ssl>
-
-        <!-- The following two files are used only if enable_ssl=1 -->
-        <ssl_cert_file>/path/to/ssl_cert_file</ssl_cert_file>
-        <ssl_key_file>/path/to/ssl_key_file</ssl_key_file>
-
-        <!-- Whether server will request client for a certificate -->
-        <ssl_require_client_auth>false</ssl_require_client_auth>
-
-        <!-- The following file is used only if ssl_require_client_auth=1 -->
-        <ssl_ca_cert_file>/path/to/ssl_ca_cert_file</ssl_ca_cert_file>
-
-        <!-- Default compression algorithm (applied if client doesn't specify another algorithm, see result_compression in QueryInfo).
-             Supported algorithms: none, deflate, gzip, stream_gzip -->
-        <compression>deflate</compression>
-
-        <!-- Default compression level (applied if client doesn't specify another level, see result_compression in QueryInfo).
-             Supported levels: none, low, medium, high -->
-        <compression_level>medium</compression_level>
-
-        <!-- Send/receive message size limits in bytes. -1 means unlimited -->
-        <max_send_message_size>-1</max_send_message_size>
-        <max_receive_message_size>-1</max_receive_message_size>
-
-        <!-- Enable if you want very detailed logs -->
-        <verbose_logs>false</verbose_logs>
-    </grpc>
-
-    <!-- Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71 -->
-    <openSSL>
-        <server> <!-- Used for https server AND secure tcp port -->
-            <!-- openssl req -subj "/CN=localhost" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt -->
-            <certificateFile>/etc/clickhouse-server/server.crt</certificateFile>
-            <privateKeyFile>/etc/clickhouse-server/server.key</privateKeyFile>
-            <!-- dhparams are optional. You can delete the <dhParamsFile> element.
-                 To generate dhparams, use the following command:
-                  openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096
-                 Only file format with BEGIN DH PARAMETERS is supported.
-              -->
-            <dhParamsFile>/etc/clickhouse-server/dhparam.pem</dhParamsFile>
-            <verificationMode>none</verificationMode>
-            <loadDefaultCAFile>true</loadDefaultCAFile>
-            <cacheSessions>true</cacheSessions>
-            <disableProtocols>sslv2,sslv3</disableProtocols>
-            <preferServerCiphers>true</preferServerCiphers>
-        </server>
-
-        <client> <!-- Used for connecting to https dictionary source and secured Zookeeper communication -->
-            <loadDefaultCAFile>true</loadDefaultCAFile>
-            <cacheSessions>true</cacheSessions>
-            <disableProtocols>sslv2,sslv3</disableProtocols>
-            <preferServerCiphers>true</preferServerCiphers>
-            <!-- Use for self-signed: <verificationMode>none</verificationMode> -->
-            <invalidCertificateHandler>
-                <!-- Use for self-signed: <name>AcceptCertificateHandler</name> -->
-                <name>RejectCertificateHandler</name>
-            </invalidCertificateHandler>
-        </client>
-    </openSSL>
-
-    <!-- Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123 -->
-    <!--
-    <http_server_default_response><![CDATA[<html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>]]></http_server_default_response>
-    -->
-
-    <!-- Maximum number of concurrent queries. -->
-    <max_concurrent_queries>100</max_concurrent_queries>
-
-    <!-- Maximum memory usage (resident set size) for server process.
-         Zero value or unset means default. Default is "max_server_memory_usage_to_ram_ratio" of available physical RAM.
-         If the value is larger than "max_server_memory_usage_to_ram_ratio" of available physical RAM, it will be cut down.
-
-         The constraint is checked on query execution time.
-         If a query tries to allocate memory and the current memory usage plus allocation is greater
-          than specified threshold, exception will be thrown.
-
-         It is not practical to set this constraint to small values like just a few gigabytes,
-          because memory allocator will keep this amount of memory in caches and the server will deny service of queries.
-      -->
-    <max_server_memory_usage>0</max_server_memory_usage>
-
-    <!-- Maximum number of threads in the Global thread pool.
-    This will default to a maximum of 10000 threads if not specified.
-    This setting will be useful in scenarios where there are a large number
-    of distributed queries that are running concurrently but are idling most
-    of the time, in which case a higher number of threads might be required.
-    -->
-
-    <max_thread_pool_size>10000</max_thread_pool_size>
-
-    <!-- On memory constrained environments you may have to set this to value larger than 1.
-      -->
-    <max_server_memory_usage_to_ram_ratio>0.9</max_server_memory_usage_to_ram_ratio>
-
-    <!-- Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes).
-         Data will be stored in system.trace_log table with query_id = empty string.
-         Zero means disabled.
-      -->
-    <total_memory_profiler_step>4194304</total_memory_profiler_step>
-
-    <!-- Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type.
-         The probability is for every alloc/free regardless to the size of the allocation.
-         Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit,
-          which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered.
-         You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling.
-      -->
-    <total_memory_tracker_sample_probability>0</total_memory_tracker_sample_probability>
-
-    <!-- Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve
-         correct maximum value. -->
-    <!-- <max_open_files>262144</max_open_files> -->
-
-    <!-- Size of cache of uncompressed blocks of data, used in tables of MergeTree family.
-         In bytes. Cache is single for server. Memory is allocated only on demand.
-         Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).
-         Uncompressed cache is advantageous only for very short queries and in rare cases.
-
-         Note: uncompressed cache can be pointless for lz4, because memory bandwidth
-         is slower than multi-core decompression on some server configurations.
-         Enabling it can sometimes paradoxically make queries slower.
-      -->
-    <uncompressed_cache_size>8589934592</uncompressed_cache_size>
-
-    <!-- Approximate size of mark cache, used in tables of MergeTree family.
-         In bytes. Cache is single for server. Memory is allocated only on demand.
-         You should not lower this value.
-      -->
-    <mark_cache_size>5368709120</mark_cache_size>
-
-
-    <!-- If you enable the `min_bytes_to_use_mmap_io` setting,
-         the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace.
-         It makes sense only for large files and helps only if data reside in page cache.
-         To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults)
-         and to reuse mappings from several threads and queries,
-         the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files).
-         The amount of data in mapped files can be monitored
-         in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics
-         and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric,
-         and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the
-         CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events.
-         Note that the amount of data in mapped files does not consume memory directly and is not accounted
-         in query or server memory usage - because this memory can be discarded similar to OS page cache.
-         The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree,
-         also it can be dropped manually by the SYSTEM DROP MMAP CACHE query.
-      -->
-    <mmap_cache_size>1000</mmap_cache_size>
-
-    <!-- Cache size in bytes for compiled expressions.-->
-    <compiled_expression_cache_size>134217728</compiled_expression_cache_size>
-
-    <!-- Cache size in elements for compiled expressions.-->
-    <compiled_expression_cache_elements_size>10000</compiled_expression_cache_elements_size>
-
-    <!-- Path to data directory, with trailing slash. -->
-    <path>/var/lib/clickhouse/</path>
-
-    <!-- Path to temporary data for processing hard queries. -->
-    <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
-
-    <!-- Policy from the <storage_configuration> for the temporary files.
-         If not set <tmp_path> is used, otherwise <tmp_path> is ignored.
-
-         Notes:
-         - move_factor              is ignored
-         - keep_free_space_bytes    is ignored
-         - max_data_part_size_bytes is ignored
-         - you must have exactly one volume in that policy
-    -->
-    <!-- <tmp_policy>tmp</tmp_policy> -->
-
-    <!-- Directory with user provided files that are accessible by 'file' table function. -->
-    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>
-
-    <!-- LDAP server definitions. -->
-    <ldap_servers>
-        <!-- List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users,
-              who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories.
-             Parameters:
-                host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty.
-                port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.
-                bind_dn - template used to construct the DN to bind to.
-                        The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual
-                         user name during each authentication attempt.
-                user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user.
-                        This is mainly used in search filters for further role mapping when the server is Active Directory. The
-                         resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default,
-                         user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected
-                         user DN value.
-                    base_dn - template used to construct the base DN for the LDAP search.
-                            The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings
-                             of the template with the actual user name and bind DN during the LDAP search.
-                    scope - scope of the LDAP search.
-                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
-                    search_filter - template used to construct the search filter for the LDAP search.
-                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}'
-                             substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.
-                            Note, that the special characters must be escaped properly in XML.
-                verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed
-                         to be successfully authenticated for all consecutive requests without contacting the LDAP server.
-                        Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request.
-                enable_tls - flag to trigger use of secure connection to the LDAP server.
-                        Specify 'no' for plain text (ldap://) protocol (not recommended).
-                        Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default).
-                        Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS).
-                tls_minimum_protocol_version - the minimum protocol version of SSL/TLS.
-                        Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default).
-                tls_require_cert - SSL/TLS peer certificate verification behavior.
-                        Accepted values are: 'never', 'allow', 'try', 'demand' (the default).
-                tls_cert_file - path to certificate file.
-                tls_key_file - path to certificate key file.
-                tls_ca_cert_file - path to CA certificate file.
-                tls_ca_cert_dir - path to the directory containing CA certificates.
-                tls_cipher_suite - allowed cipher suite (in OpenSSL notation).
-             Example:
-                <my_ldap_server>
-                    <host>localhost</host>
-                    <port>636</port>
-                    <bind_dn>uid={user_name},ou=users,dc=example,dc=com</bind_dn>
-                    <verification_cooldown>300</verification_cooldown>
-                    <enable_tls>yes</enable_tls>
-                    <tls_minimum_protocol_version>tls1.2</tls_minimum_protocol_version>
-                    <tls_require_cert>demand</tls_require_cert>
-                    <tls_cert_file>/path/to/tls_cert_file</tls_cert_file>
-                    <tls_key_file>/path/to/tls_key_file</tls_key_file>
-                    <tls_ca_cert_file>/path/to/tls_ca_cert_file</tls_ca_cert_file>
-                    <tls_ca_cert_dir>/path/to/tls_ca_cert_dir</tls_ca_cert_dir>
-                    <tls_cipher_suite>ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384</tls_cipher_suite>
-                </my_ldap_server>
-             Example (typical Active Directory with configured user DN detection for further role mapping):
-                <my_ad_server>
-                    <host>localhost</host>
-                    <port>389</port>
-                    <bind_dn>EXAMPLE\{user_name}</bind_dn>
-                    <user_dn_detection>
-                        <base_dn>CN=Users,DC=example,DC=com</base_dn>
-                        <search_filter>(&amp;(objectClass=user)(sAMAccountName={user_name}))</search_filter>
-                    </user_dn_detection>
-                    <enable_tls>no</enable_tls>
-                </my_ad_server>
-        -->
-    </ldap_servers>
-
-    <!-- To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured
-          to authenticate via Kerberos, define a single 'kerberos' section here.
-         Parameters:
-            principal - canonical service principal name, that will be acquired and used when accepting security contexts.
-                    This parameter is optional, if omitted, the default principal will be used.
-                    This parameter cannot be specified together with 'realm' parameter.
-            realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it.
-                    This parameter is optional, if omitted, no additional filtering by realm will be applied.
-                    This parameter cannot be specified together with 'principal' parameter.
-         Example:
-            <kerberos />
-         Example:
-            <kerberos>
-                <principal>HTTP/clickhouse.example.com@EXAMPLE.COM</principal>
-            </kerberos>
-         Example:
-            <kerberos>
-                <realm>EXAMPLE.COM</realm>
-            </kerberos>
-    -->
-
-    <!-- Sources to read users, roles, access rights, profiles of settings, quotas. -->
-    <user_directories>
-        <users_xml>
-            <!-- Path to configuration file with predefined users. -->
-            <path>users.xml</path>
-        </users_xml>
-        <local_directory>
-            <!-- Path to folder where users created by SQL commands are stored. -->
-            <path>/var/lib/clickhouse/access/</path>
-        </local_directory>
-
-        <!-- To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section
-              with the following parameters:
-                server - one of LDAP server names defined in 'ldap_servers' config section above.
-                        This parameter is mandatory and cannot be empty.
-                roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server.
-                        If no roles are specified here or assigned during role mapping (below), user will not be able to perform any
-                         actions after authentication.
-                role_mapping - section with LDAP search parameters and mapping rules.
-                        When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the
-                         name of the logged in user. For each entry found during that search, the value of the specified attribute is
-                         extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the
-                         value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by
-                         CREATE ROLE command.
-                        There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be
-                         applied.
-                    base_dn - template used to construct the base DN for the LDAP search.
-                            The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}'
-                             substrings of the template with the actual user name, bind DN, and user DN during each LDAP search.
-                    scope - scope of the LDAP search.
-                            Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
-                    search_filter - template used to construct the search filter for the LDAP search.
-                            The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and
-                             '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during
-                             each LDAP search.
-                            Note, that the special characters must be escaped properly in XML.
-                    attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default.
-                    prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by
-                             the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated
-                             as local role names. Empty, by default.
-             Example:
-                <ldap>
-                    <server>my_ldap_server</server>
-                    <roles>
-                        <my_local_role1 />
-                        <my_local_role2 />
-                    </roles>
-                    <role_mapping>
-                        <base_dn>ou=groups,dc=example,dc=com</base_dn>
-                        <scope>subtree</scope>
-                        <search_filter>(&amp;(objectClass=groupOfNames)(member={bind_dn}))</search_filter>
-                        <attribute>cn</attribute>
-                        <prefix>clickhouse_</prefix>
-                    </role_mapping>
-                </ldap>
-             Example (typical Active Directory with role mapping that relies on the detected user DN):
-                <ldap>
-                    <server>my_ad_server</server>
-                    <role_mapping>
-                        <base_dn>CN=Users,DC=example,DC=com</base_dn>
-                        <attribute>CN</attribute>
-                        <scope>subtree</scope>
-                        <search_filter>(&amp;(objectClass=group)(member={user_dn}))</search_filter>
-                        <prefix>clickhouse_</prefix>
-                    </role_mapping>
-                </ldap>
-        -->
-    </user_directories>
-
-    <!-- Default profile of settings. -->
-    <default_profile>default</default_profile>
-
-    <!-- Comma-separated list of prefixes for user-defined settings. -->
-    <custom_settings_prefixes></custom_settings_prefixes>
-
-    <!-- System profile of settings. This settings are used by internal processes (Distributed DDL worker and so on). -->
-    <!-- <system_profile>default</system_profile> -->
-
-    <!-- Buffer profile of settings.
-         This settings are used by Buffer storage to flush data to the underlying table.
-         Default: used from system_profile directive.
-    -->
-    <!-- <buffer_profile>default</buffer_profile> -->
-
-    <!-- Default database. -->
-    <default_database>default</default_database>
-
-    <!-- Server time zone could be set here.
-
-         Time zone is used when converting between String and DateTime types,
-          when printing DateTime in text formats and parsing DateTime from text,
-          it is used in date and time related functions, if specific time zone was not passed as an argument.
-
-         Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.
-         If not specified, system time zone at server startup is used.
-
-         Please note, that server could display time zone alias instead of specified name.
-         Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC.
-    -->
-    <!-- <timezone>Europe/Moscow</timezone> -->
-
-    <!-- You can specify umask here (see "man umask"). Server will apply it on startup.
-         Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).
-    -->
-    <!-- <umask>022</umask> -->
-
-    <!-- Perform mlockall after startup to lower first queries latency
-          and to prevent clickhouse executable from being paged out under high IO load.
-         Enabling this option is recommended but will lead to increased startup time for up to a few seconds.
-    -->
-    <mlock_executable>true</mlock_executable>
-
-    <!-- Reallocate memory for machine code ("text") using huge pages. Highly experimental. -->
-    <remap_executable>false</remap_executable>
-
-    <![CDATA[
-         Uncomment below in order to use JDBC table engine and function.
-
-         To install and run JDBC bridge in background:
-         * [Debian/Ubuntu]
-           export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge
-           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\(.*\)<.*|\1|')
-           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge_$PKG_VER-1_all.deb
-           apt install --no-install-recommends -f ./clickhouse-jdbc-bridge_$PKG_VER-1_all.deb
-           clickhouse-jdbc-bridge &
-
-         * [CentOS/RHEL]
-           export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge
-           export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\(.*\)<.*|\1|')
-           wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm
-           yum localinstall -y clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm
-           clickhouse-jdbc-bridge &
-
-         Please refer to https://github.com/ClickHouse/clickhouse-jdbc-bridge#usage for more information.
-    ]]>
-    <!--
-    <jdbc_bridge>
-        <host>127.0.0.1</host>
-        <port>9019</port>
-    </jdbc_bridge>
-    -->
-
-    <!-- Configuration of clusters that could be used in Distributed tables.
-         https://clickhouse.com/docs/en/operations/table_engines/distributed/
-      -->
-    <remote_servers>
-        <!-- Test only shard config for testing distributed storage -->
-        <test_shard_localhost>
-            <!-- Inter-server per-cluster secret for Distributed queries
-                 default: no secret (no authentication will be performed)
-
-                 If set, then Distributed queries will be validated on shards, so at least:
-                 - such cluster should exist on the shard,
-                 - such cluster should have the same secret.
-
-                 And also (and which is more important), the initial_user will
-                 be used as current user for the query.
-
-                 Right now the protocol is pretty simple and it only takes into account:
-                 - cluster name
-                 - query
-
-                 Also it will be nice if the following will be implemented:
-                 - source hostname (see interserver_http_host), but then it will depends from DNS,
-                   it can use IP address instead, but then the you need to get correct on the initiator node.
-                 - target hostname / ip address (same notes as for source hostname)
-                 - time-based security tokens
-            -->
-            <!-- <secret></secret> -->
-
-            <shard>
-                <!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). -->
-                <!-- <internal_replication>false</internal_replication> -->
-                <!-- Optional. Shard weight when writing data. Default: 1. -->
-                <!-- <weight>1</weight> -->
-                <replica>
-                    <host>localhost</host>
-                    <port>9000</port>
-                    <!-- Optional. Priority of the replica for load_balancing. Default: 1 (less value has more priority). -->
-                    <!-- <priority>1</priority> -->
-                </replica>
-            </shard>
-        </test_shard_localhost>
-    </remote_servers>
-
-    <!-- The list of hosts allowed to use in URL-related storage engines and table functions.
-        If this section is not present in configuration, all hosts are allowed.
-    -->
-    <!--<remote_url_allow_hosts>-->
-    <!-- Host should be specified exactly as in URL. The name is checked before DNS resolution.
-        Example: "yandex.ru", "yandex.ru." and "www.yandex.ru" are different hosts.
-                If port is explicitly specified in URL, the host:port is checked as a whole.
-                If host specified here without port, any port with this host allowed.
-                "yandex.ru" -> "yandex.ru:443", "yandex.ru:80" etc. is allowed, but "yandex.ru:80" -> only "yandex.ru:80" is allowed.
-        If the host is specified as IP address, it is checked as specified in URL. Example: "[2a02:6b8:a::a]".
-        If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.
-        Host should be specified using the host xml tag:
-                <host>yandex.ru</host>
-    -->
-
-    <!-- Regular expression can be specified. RE2 engine is used for regexps.
-        Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter
-        (forgetting to do so is a common source of error).
-    -->
-    <!--</remote_url_allow_hosts>-->
-
-    <!-- If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.
-         By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.
-         Values for substitutions are specified in /clickhouse/name_of_substitution elements in that file.
-      -->
-
-    <!-- ZooKeeper is used to store metadata about replicas, when using Replicated tables.
-         Optional. If you don't use replicated tables, you could omit that.
-
-         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/
-      -->
-
-    <!--
-    <zookeeper>
-        <node>
-            <host>example1</host>
-            <port>2181</port>
-        </node>
-        <node>
-            <host>example2</host>
-            <port>2181</port>
-        </node>
-        <node>
-            <host>example3</host>
-            <port>2181</port>
-        </node>
-    </zookeeper>
-    -->
-
-    <!-- Substitutions for parameters of replicated tables.
-          Optional. If you don't use replicated tables, you could omit that.
-
-         See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#creating-replicated-tables
-      -->
-    <!--
-    <macros>
-        <shard>01</shard>
-        <replica>example01-01-1</replica>
-    </macros>
-    -->
-
-
-    <!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. -->
-    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>
-
-
-    <!-- Maximum session timeout, in seconds. Default: 3600. -->
-    <max_session_timeout>3600</max_session_timeout>
-
-    <!-- Default session timeout, in seconds. Default: 60. -->
-    <default_session_timeout>60</default_session_timeout>
-
-    <!-- Sending data to Graphite for monitoring. Several sections can be defined. -->
-    <!--
-        interval - send every X second
-        root_path - prefix for keys
-        hostname_in_path - append hostname to root_path (default = true)
-        metrics - send data from table system.metrics
-        events - send data from table system.events
-        asynchronous_metrics - send data from table system.asynchronous_metrics
-    -->
-    <!--
-    <graphite>
-        <host>localhost</host>
-        <port>42000</port>
-        <timeout>0.1</timeout>
-        <interval>60</interval>
-        <root_path>one_min</root_path>
-        <hostname_in_path>true</hostname_in_path>
-
-        <metrics>true</metrics>
-        <events>true</events>
-        <events_cumulative>false</events_cumulative>
-        <asynchronous_metrics>true</asynchronous_metrics>
-    </graphite>
-    <graphite>
-        <host>localhost</host>
-        <port>42000</port>
-        <timeout>0.1</timeout>
-        <interval>1</interval>
-        <root_path>one_sec</root_path>
-
-        <metrics>true</metrics>
-        <events>true</events>
-        <events_cumulative>false</events_cumulative>
-        <asynchronous_metrics>false</asynchronous_metrics>
-    </graphite>
-    -->
-
-    <!-- Query log. Used only for queries with setting log_queries = 1. -->
-    <query_log>
-        <!-- What table to insert data. If table is not exist, it will be created.
-             When query log structure is changed after system update,
-              then old table will be renamed and new table will be created automatically.
-        -->
-        <database>system</database>
-        <table>query_log</table>
-        <!--
-            PARTITION BY expr: https://clickhouse.com/docs/en/table_engines/mergetree-family/custom_partitioning_key/
-            Example:
-                event_date
-                toMonday(event_date)
-                toYYYYMM(event_date)
-                toStartOfHour(event_time)
-        -->
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <!--
-            Table TTL specification: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl
-            Example:
-                event_date + INTERVAL 1 WEEK
-                event_date + INTERVAL 7 DAY DELETE
-                event_date + INTERVAL 2 WEEK TO DISK 'bbb'
-
-        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
-        -->
-
-        <!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,
-             Example: <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024</engine>
-          -->
-
-        <!-- Interval of flushing data. -->
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </query_log>
-
-    <!-- Trace log. Stores stack traces collected by query profilers.
-         See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->
-    <trace_log>
-        <database>system</database>
-        <table>trace_log</table>
-
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </trace_log>
-
-    <!-- Query thread log. Has information about all threads participated in query execution.
-         Used only for queries with setting log_query_threads = 1. -->
-    <query_thread_log>
-        <database>system</database>
-        <table>query_thread_log</table>
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </query_thread_log>
-
-    <!-- Query views log. Has information about all dependent views associated with a query.
-         Used only for queries with setting log_query_views = 1. -->
-    <query_views_log>
-        <database>system</database>
-        <table>query_views_log</table>
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </query_views_log>
-
-    <!-- Uncomment if use part log.
-         Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).-->
-    <part_log>
-        <database>system</database>
-        <table>part_log</table>
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </part_log>
-
-    <!-- Uncomment to write text log into table.
-         Text log contains all information from usual server log but stores it in structured and efficient way.
-         The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.
-    <text_log>
-        <database>system</database>
-        <table>text_log</table>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-        <level></level>
-    </text_log>
-    -->
-
-    <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with "collect_interval_milliseconds" interval. -->
-    <metric_log>
-        <database>system</database>
-        <table>metric_log</table>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-        <collect_interval_milliseconds>1000</collect_interval_milliseconds>
-    </metric_log>
-
-    <!--
-        Asynchronous metric log contains values of metrics from
-        system.asynchronous_metrics.
-    -->
-    <asynchronous_metric_log>
-        <database>system</database>
-        <table>asynchronous_metric_log</table>
-        <!--
-            Asynchronous metrics are updated once a minute, so there is
-            no need to flush more often.
-        -->
-        <flush_interval_milliseconds>7000</flush_interval_milliseconds>
-    </asynchronous_metric_log>
-
-    <!--
-        OpenTelemetry log contains OpenTelemetry trace spans.
-    -->
-    <opentelemetry_span_log>
-        <!--
-            The default table creation code is insufficient, this <engine> spec
-            is a workaround. There is no 'event_time' for this log, but two times,
-            start and finish. It is sorted by finish time, to avoid inserting
-            data too far away in the past (probably we can sometimes insert a span
-            that is seconds earlier than the last span in the table, due to a race
-            between several spans inserted in parallel). This gives the spans a
-            global order that we can use to e.g. retry insertion into some external
-            system.
-        -->
-        <engine>
-            engine MergeTree
-            partition by toYYYYMM(finish_date)
-            order by (finish_date, finish_time_us, trace_id)
-        </engine>
-        <database>system</database>
-        <table>opentelemetry_span_log</table>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </opentelemetry_span_log>
-
-
-    <!-- Crash log. Stores stack traces for fatal errors.
-         This table is normally empty. -->
-    <crash_log>
-        <database>system</database>
-        <table>crash_log</table>
-
-        <partition_by />
-        <flush_interval_milliseconds>1000</flush_interval_milliseconds>
-    </crash_log>
-
-    <!-- Session log. Stores user log in (successful or not) and log out events. -->
-    <session_log>
-        <database>system</database>
-        <table>session_log</table>
-
-        <partition_by>toYYYYMM(event_date)</partition_by>
-        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
-    </session_log>
-
-    <!-- Parameters for embedded dictionaries, used in Yandex.Metrica.
-         See https://clickhouse.com/docs/en/dicts/internal_dicts/
-    -->
-
-    <!-- Path to file with region hierarchy. -->
-    <!-- <path_to_regions_hierarchy_file>/opt/geo/regions_hierarchy.txt</path_to_regions_hierarchy_file> -->
-
-    <!-- Path to directory with files containing names of regions -->
-    <!-- <path_to_regions_names_files>/opt/geo/</path_to_regions_names_files> -->
-
-
-    <!-- <top_level_domains_path>/var/lib/clickhouse/top_level_domains/</top_level_domains_path> -->
-    <!-- Custom TLD lists.
-         Format: <name>/path/to/file</name>
-
-         Changes will not be applied w/o server restart.
-         Path to the list is under top_level_domains_path (see above).
-    -->
-    <top_level_domains_lists>
-        <!--
-        <public_suffix_list>/path/to/public_suffix_list.dat</public_suffix_list>
-        -->
-    </top_level_domains_lists>
-
-    <!-- Configuration of external dictionaries. See:
-         https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts
-    -->
-    <dictionaries_config>*_dictionary.xml</dictionaries_config>
-
-    <!-- Configuration of user defined executable functions -->
-    <user_defined_executable_functions_config>*_function.xml</user_defined_executable_functions_config>
-
-    <!-- Uncomment if you want data to be compressed 30-100% better.
-         Don't do that if you just started using ClickHouse.
-      -->
-    <!--
-    <compression>
-        <!- - Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used. - ->
-        <case>
-
-            <!- - Conditions. All must be satisfied. Some conditions may be omitted. - ->
-            <min_part_size>10000000000</min_part_size>        <!- - Min part size in bytes. - ->
-            <min_part_size_ratio>0.01</min_part_size_ratio>   <!- - Min size of part relative to whole table size. - ->
-
-            <!- - What compression method to use. - ->
-            <method>zstd</method>
-        </case>
-    </compression>
-    -->
-
-    <!-- Configuration of encryption. The server executes a command to
-         obtain an encryption key at startup if such a command is
-         defined, or encryption codecs will be disabled otherwise. The
-         command is executed through /bin/sh and is expected to write
-         a Base64-encoded key to the stdout. -->
-    <encryption_codecs>
-        <!-- aes_128_gcm_siv -->
-        <!-- Example of getting hex key from env -->
-        <!-- the code should use this key and throw an exception if its length is not 16 bytes -->
-        <!--key_hex from_env="..."></key_hex -->
-
-        <!-- Example of multiple hex keys. They can be imported from env or be written down in config-->
-        <!-- the code should use these keys and throw an exception if their length is not 16 bytes -->
-        <!-- key_hex id="0">...</key_hex -->
-        <!-- key_hex id="1" from_env=".."></key_hex -->
-        <!-- key_hex id="2">...</key_hex -->
-        <!-- current_key_id>2</current_key_id -->
-
-        <!-- Example of getting hex key from config -->
-        <!-- the code should use this key and throw an exception if its length is not 16 bytes -->
-        <!-- key>...</key -->
-
-        <!-- example of adding nonce -->
-        <!-- nonce>...</nonce -->
-
-        <!-- /aes_128_gcm_siv -->
-    </encryption_codecs>
-
-    <!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.
-         Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. -->
-    <distributed_ddl>
-        <!-- Path in ZooKeeper to queue with DDL queries -->
-        <path>/clickhouse/task_queue/ddl</path>
-
-        <!-- Settings from this profile will be used to execute DDL queries -->
-        <!-- <profile>default</profile> -->
-
-        <!-- Controls how much ON CLUSTER queries can be run simultaneously. -->
-        <!-- <pool_size>1</pool_size> -->
-
-        <!--
-             Cleanup settings (active tasks will not be removed)
-        -->
-
-        <!-- Controls task TTL (default 1 week) -->
-        <!-- <task_max_lifetime>604800</task_max_lifetime> -->
-
-        <!-- Controls how often cleanup should be performed (in seconds) -->
-        <!-- <cleanup_delay_period>60</cleanup_delay_period> -->
-
-        <!-- Controls how many tasks could be in the queue -->
-        <!-- <max_tasks_in_queue>1000</max_tasks_in_queue> -->
-    </distributed_ddl>
-
-    <!-- Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h -->
-    <!--
-    <merge_tree>
-        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>
-    </merge_tree>
-    -->
-
-    <!-- Protection from accidental DROP.
-         If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.
-         If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.
-         By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.
-         The same for max_partition_size_to_drop.
-         Uncomment to disable protection.
-    -->
-    <!-- <max_table_size_to_drop>0</max_table_size_to_drop> -->
-    <!-- <max_partition_size_to_drop>0</max_partition_size_to_drop> -->
-
-    <!-- Example of parameters for GraphiteMergeTree table engine -->
-    <graphite_rollup_example>
-        <pattern>
-            <regexp>click_cost</regexp>
-            <function>any</function>
-            <retention>
-                <age>0</age>
-                <precision>3600</precision>
-            </retention>
-            <retention>
-                <age>86400</age>
-                <precision>60</precision>
-            </retention>
-        </pattern>
-        <default>
-            <function>max</function>
-            <retention>
-                <age>0</age>
-                <precision>60</precision>
-            </retention>
-            <retention>
-                <age>3600</age>
-                <precision>300</precision>
-            </retention>
-            <retention>
-                <age>86400</age>
-                <precision>3600</precision>
-            </retention>
-        </default>
-    </graphite_rollup_example>
-
-    <!-- Directory in <clickhouse-path> containing schema files for various input formats.
-         The directory will be created if it doesn't exist.
-      -->
-    <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>
-
-    <!-- Default query masking rules, matching lines would be replaced with something else in the logs
-        (both text logs and system.query_log).
-        name - name for the rule (optional)
-        regexp - RE2 compatible regular expression (mandatory)
-        replace - substitution string for sensitive data (optional, by default - six asterisks)
-    -->
-    <query_masking_rules>
-        <rule>
-            <name>hide encrypt/decrypt arguments</name>
-            <regexp>((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\s*\(\s*(?:'(?:\\'|.)+'|.*?)\s*\)</regexp>
-            <!-- or more secure, but also more invasive:
-                (aes_\w+)\s*\(.*\)
-            -->
-            <replace>\1(???)</replace>
-        </rule>
-    </query_masking_rules>
-
-    <!-- Uncomment to use custom http handlers.
-        rules are checked from top to bottom, first match runs the handler
-            url - to match request URL, you can use 'regex:' prefix to use regex match(optional)
-            methods - to match request method, you can use commas to separate multiple method matches(optional)
-            headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional)
-        handler is request handler
-            type - supported types: static, dynamic_query_handler, predefined_query_handler
-            query - use with predefined_query_handler type, executes query when the handler is called
-            query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the <query_param_name> value in HTTP request params
-            status - use with static type, response status code
-            content_type - use with static type, response content-type
-            response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client.
-
-    <http_handlers>
-        <rule>
-            <url>/</url>
-            <methods>POST,GET</methods>
-            <headers><pragma>no-cache</pragma></headers>
-            <handler>
-                <type>dynamic_query_handler</type>
-                <query_param_name>query</query_param_name>
-            </handler>
-        </rule>
-
-        <rule>
-            <url>/predefined_query</url>
-            <methods>POST,GET</methods>
-            <handler>
-                <type>predefined_query_handler</type>
-                <query>SELECT * FROM system.settings</query>
-            </handler>
-        </rule>
-
-        <rule>
-            <handler>
-                <type>static</type>
-                <status>200</status>
-                <content_type>text/plain; charset=UTF-8</content_type>
-                <response_content>config://http_server_default_response</response_content>
-            </handler>
-        </rule>
-    </http_handlers>
-    -->
-
-    <send_crash_reports>
-        <!-- Changing <enabled> to true allows sending crash reports to -->
-        <!-- the ClickHouse core developers team via Sentry https://sentry.io -->
-        <!-- Doing so at least in pre-production environments is highly appreciated -->
-        <enabled>false</enabled>
-        <!-- Change <anonymize> to true if you don't feel comfortable attaching the server hostname to the crash report -->
-        <anonymize>false</anonymize>
-        <!-- Default endpoint should be changed to different Sentry DSN only if you have -->
-        <!-- some in-house engineers or hired consultants who're going to debug ClickHouse issues for you -->
-        <endpoint>https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277</endpoint>
-    </send_crash_reports>
-
-    <!-- Uncomment to disable ClickHouse internal DNS caching. -->
-    <!-- <disable_internal_dns_cache>1</disable_internal_dns_cache> -->
-
-    <!-- You can also configure rocksdb like this: -->
-    <!--
-    <rocksdb>
-        <options>
-            <max_background_jobs>8</max_background_jobs>
-        </options>
-        <column_family_options>
-            <num_levels>2</num_levels>
-        </column_family_options>
-        <tables>
-            <table>
-                <name>TABLE</name>
-                <options>
-                    <max_background_jobs>8</max_background_jobs>
-                </options>
-                <column_family_options>
-                    <num_levels>2</num_levels>
-                </column_family_options>
-            </table>
-        </tables>
-    </rocksdb>
-    -->
-</yandex>
diff --git a/programs/diagnostics/testdata/docker/admin.xml b/programs/diagnostics/testdata/docker/admin.xml
deleted file mode 100644
index 76aa670dcfe3..000000000000
--- a/programs/diagnostics/testdata/docker/admin.xml
+++ /dev/null
@@ -1,15 +0,0 @@
-<clickhouse>
-    <!-- Profiles of settings. -->
-    <profiles>
-        <!-- Default settings. -->
-        <default>
-            <!-- Allows us to create replicated databases. -->
-            <allow_experimental_database_replicated>1</allow_experimental_database_replicated>
-        </default>
-    </profiles>
-    <users>
-        <default>
-            <access_management>1</access_management>
-        </default>
-    </users>
-</clickhouse>
\ No newline at end of file
diff --git a/programs/diagnostics/testdata/docker/custom.xml b/programs/diagnostics/testdata/docker/custom.xml
deleted file mode 100644
index bc1051178ca8..000000000000
--- a/programs/diagnostics/testdata/docker/custom.xml
+++ /dev/null
@@ -1,8 +0,0 @@
-<clickhouse>
-    <listen_host>::</listen_host>
-    <listen_host>0.0.0.0</listen_host>
-    <listen_try>1</listen_try>
-    <logger>
-        <console>1</console>
-    </logger>
-</clickhouse>
diff --git a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.err.log b/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.err.log
deleted file mode 100644
index 1a1768fe87ef..000000000000
--- a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.err.log
+++ /dev/null
@@ -1,10 +0,0 @@
-2021.12.13 10:12:26.940169 [ 38398 ] {} <Warning> Access(local directory): File /var/lib/clickhouse/access/users.list doesn't exist
-2021.12.13 10:12:26.940204 [ 38398 ] {} <Warning> Access(local directory): Recovering lists in directory /var/lib/clickhouse/access/
-2021.12.13 10:12:40.649453 [ 38445 ] {} <Error> Access(user directories): from: 127.0.0.1, user: default: Authentication failed: Code: 193. DB::Exception: Invalid credentials. (WRONG_PASSWORD), Stack trace (when copying this message, always include the lines below):
-
-0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x9b722d4 in /usr/bin/clickhouse
-1. DB::IAccessStorage::throwInvalidCredentials() @ 0x119d9b27 in /usr/bin/clickhouse
-2. DB::IAccessStorage::loginImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&) const @ 0x119d98d7 in /usr/bin/clickhouse
-3. DB::IAccessStorage::login(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool) const @ 0x119d9084 in /usr/bin/clickhouse
-4. DB::MultipleAccessStorage::loginImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&) const @ 0x119ff93c in /usr/bin/clickhouse
-5. DB::IAccessStorage::login(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool) const @ 0x119d9084 in /usr/bin/clickhouse
diff --git a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log b/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log
deleted file mode 100644
index f6abe7764ba7..000000000000
--- a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log
+++ /dev/null
@@ -1,10 +0,0 @@
-2022.02.02 14:49:32.458680 [ 200404 ] {} <Debug> DiskLocal: Reserving 2.47 MiB on disk `default`, having unreserved 1.56 TiB.
-2022.02.02 14:49:32.459086 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTask::PrepareStage: Merging 2 parts: from 202202_147058_147549_343 to 202202_147550_147550_0 into Wide
-2022.02.02 14:49:32.459201 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTask::PrepareStage: Selected MergeAlgorithm: Horizontal
-2022.02.02 14:49:32.459262 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTreeSequentialSource: Reading 159 marks from part 202202_147058_147549_343, total 1289014 rows starting from the beginning of the part
-2022.02.02 14:49:32.459614 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202202_147550_147550_0, total 2618 rows starting from the beginning of the part
-2022.02.02 14:49:32.507755 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Debug> MergeTask::MergeProjectionsStage: Merge sorted 1291632 rows, containing 5 columns (5 merged, 0 gathered) in 0.048711404 sec., 26516008.448452853 rows/sec., 639.52 MiB/sec.
-2022.02.02 14:49:32.508332 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Trace> system.asynchronous_metric_log (de87df8b-2250-439c-9e87-df8b2250339c): Renaming temporary part tmp_merge_202202_147058_147550_344 to 202202_147058_147550_344.
-2022.02.02 14:49:32.508406 [ 200359 ] {de87df8b-2250-439c-9e87-df8b2250339c::202202_147058_147550_344} <Trace> system.asynchronous_metric_log (de87df8b-2250-439c-9e87-df8b2250339c) (MergerMutator): Merged 2 parts: from 202202_147058_147549_343 to 202202_147550_147550_0
-2022.02.02 14:49:32.508440 [ 200359 ] {} <Debug> MemoryTracker: Peak memory usage Mutate/Merge: 16.31 MiB.
-2022.02.02 14:49:33.000148 [ 200388 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 774.16 MiB, peak 2.51 GiB, will set to 772.30 MiB (RSS), difference: -1.86 MiB
diff --git a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log.gz b/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log.gz
deleted file mode 100644
index 136bf5913aaf..000000000000
--- a/programs/diagnostics/testdata/logs/var/logs/clickhouse-server.log.gz
+++ /dev/null
@@ -1,1 +0,0 @@
-dummy hz file for tests
