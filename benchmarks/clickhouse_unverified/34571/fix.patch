diff --git a/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp b/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp
index f94f6b44c9fd..56720c2af2ba 100644
--- a/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp
+++ b/src/Disks/IO/ReadBufferFromRemoteFSGather.cpp
@@ -46,7 +46,7 @@ SeekableReadBufferPtr ReadBufferFromS3Gather::createImplementationBuffer(const S
     auto remote_file_reader_creator = [=, this]()
     {
         return std::make_unique<ReadBufferFromS3>(
-            client_ptr, bucket, remote_path, max_single_read_retries,
+            client_ptr, bucket, remote_path, version_id, max_single_read_retries,
             settings, /* use_external_buffer */true, /* offset */ 0, read_until_position, /* restricted_seek */true);
     };
 
diff --git a/src/Disks/IO/ReadBufferFromRemoteFSGather.h b/src/Disks/IO/ReadBufferFromRemoteFSGather.h
index d12513cba1fe..ac3f21303609 100644
--- a/src/Disks/IO/ReadBufferFromRemoteFSGather.h
+++ b/src/Disks/IO/ReadBufferFromRemoteFSGather.h
@@ -103,6 +103,7 @@ class ReadBufferFromS3Gather final : public ReadBufferFromRemoteFSGather
     ReadBufferFromS3Gather(
         std::shared_ptr<Aws::S3::S3Client> client_ptr_,
         const String & bucket_,
+        const String & version_id_,
         const std::string & common_path_prefix_,
         const BlobsPathToSize & blobs_to_read_,
         size_t max_single_read_retries_,
@@ -110,6 +111,7 @@ class ReadBufferFromS3Gather final : public ReadBufferFromRemoteFSGather
         : ReadBufferFromRemoteFSGather(common_path_prefix_, blobs_to_read_, settings_)
         , client_ptr(std::move(client_ptr_))
         , bucket(bucket_)
+        , version_id(version_id_)
         , max_single_read_retries(max_single_read_retries_)
     {
     }
@@ -119,6 +121,7 @@ class ReadBufferFromS3Gather final : public ReadBufferFromRemoteFSGather
 private:
     std::shared_ptr<Aws::S3::S3Client> client_ptr;
     String bucket;
+    String version_id;
     UInt64 max_single_read_retries;
 };
 #endif
diff --git a/src/Disks/S3/DiskS3.cpp b/src/Disks/S3/DiskS3.cpp
index 1e8894632b8c..b74676e608d9 100644
--- a/src/Disks/S3/DiskS3.cpp
+++ b/src/Disks/S3/DiskS3.cpp
@@ -109,6 +109,7 @@ DiskS3::DiskS3(
     String name_,
     String bucket_,
     String s3_root_path_,
+    String version_id_,
     DiskPtr metadata_disk_,
     FileCachePtr cache_,
     ContextPtr context_,
@@ -116,6 +117,7 @@ DiskS3::DiskS3(
     GetDiskSettings settings_getter_)
     : IDiskRemote(name_, s3_root_path_, metadata_disk_, std::move(cache_), "DiskS3", settings_->thread_pool_size)
     , bucket(std::move(bucket_))
+    , version_id(std::move(version_id_))
     , current_settings(std::move(settings_))
     , settings_getter(settings_getter_)
     , context(context_)
@@ -196,7 +198,7 @@ std::unique_ptr<ReadBufferFromFileBase> DiskS3::readFile(const String & path, co
     }
 
     auto s3_impl = std::make_unique<ReadBufferFromS3Gather>(
-        settings->client, bucket, metadata.remote_fs_root_path, metadata.remote_fs_objects,
+        settings->client, bucket, version_id, metadata.remote_fs_root_path, metadata.remote_fs_objects,
         settings->s3_settings.max_single_read_retries, disk_read_settings);
 
     if (read_settings.remote_fs_method == RemoteFSReadMethod::threadpool)
@@ -354,6 +356,7 @@ int DiskS3::readSchemaVersion(const String & source_bucket, const String & sourc
         settings->client,
         source_bucket,
         source_path + SCHEMA_VERSION_OBJECT,
+        version_id,
         settings->s3_settings.max_single_read_retries,
         context->getReadSettings());
 
diff --git a/src/Disks/S3/DiskS3.h b/src/Disks/S3/DiskS3.h
index 8b9776b7b61a..6f11ce86b063 100644
--- a/src/Disks/S3/DiskS3.h
+++ b/src/Disks/S3/DiskS3.h
@@ -66,6 +66,7 @@ class DiskS3 final : public IDiskRemote
         String name_,
         String bucket_,
         String s3_root_path_,
+        String version_id_,
         DiskPtr metadata_disk_,
         FileCachePtr cache_,
         ContextPtr context_,
@@ -157,6 +158,8 @@ class DiskS3 final : public IDiskRemote
 
     const String bucket;
 
+    const String version_id;
+
     MultiVersion<DiskS3Settings> current_settings;
     /// Gets disk settings from context.
     GetDiskSettings settings_getter;
diff --git a/src/Disks/S3/registerDiskS3.cpp b/src/Disks/S3/registerDiskS3.cpp
index fc9108e89a7c..b9e2cf5afe29 100644
--- a/src/Disks/S3/registerDiskS3.cpp
+++ b/src/Disks/S3/registerDiskS3.cpp
@@ -195,6 +195,7 @@ void registerDiskS3(DiskFactory & factory)
             name,
             uri.bucket,
             uri.key,
+            uri.version_id,
             metadata_disk,
             std::move(cache),
             context,
diff --git a/src/IO/ReadBufferFromS3.cpp b/src/IO/ReadBufferFromS3.cpp
index d19fbd282659..52e855a85846 100644
--- a/src/IO/ReadBufferFromS3.cpp
+++ b/src/IO/ReadBufferFromS3.cpp
@@ -40,6 +40,7 @@ ReadBufferFromS3::ReadBufferFromS3(
     std::shared_ptr<Aws::S3::S3Client> client_ptr_,
     const String & bucket_,
     const String & key_,
+    const String & version_id_,
     UInt64 max_single_read_retries_,
     const ReadSettings & settings_,
     bool use_external_buffer_,
@@ -50,6 +51,7 @@ ReadBufferFromS3::ReadBufferFromS3(
     , client_ptr(std::move(client_ptr_))
     , bucket(bucket_)
     , key(key_)
+    , version_id(version_id_)
     , max_single_read_retries(max_single_read_retries_)
     , offset(offset_)
     , read_until_position(read_until_position_)
@@ -128,8 +130,15 @@ bool ReadBufferFromS3::nextImpl()
             ProfileEvents::increment(ProfileEvents::S3ReadMicroseconds, watch.elapsedMicroseconds());
             ProfileEvents::increment(ProfileEvents::S3ReadRequestsErrors, 1);
 
-            LOG_DEBUG(log, "Caught exception while reading S3 object. Bucket: {}, Key: {}, Offset: {}, Attempt: {}, Message: {}",
-                    bucket, key, getPosition(), attempt, e.message());
+            LOG_DEBUG(
+                log,
+                "Caught exception while reading S3 object. Bucket: {}, Key: {}, Version: {}, Offset: {}, Attempt: {}, Message: {}",
+                bucket,
+                key,
+                version_id.empty() ? "Latest" : version_id,
+                getPosition(),
+                attempt,
+                e.message());
 
             if (attempt + 1 == max_single_read_retries)
                 throw;
@@ -213,7 +222,7 @@ std::optional<size_t> ReadBufferFromS3::getTotalSize()
     if (file_size)
         return file_size;
 
-    auto object_size = S3::getObjectSize(client_ptr, bucket, key, false);
+    auto object_size = S3::getObjectSize(client_ptr, bucket, key, version_id, false);
 
     if (!object_size)
     {
@@ -248,6 +257,10 @@ std::unique_ptr<ReadBuffer> ReadBufferFromS3::initialize()
     Aws::S3::Model::GetObjectRequest req;
     req.SetBucket(bucket);
     req.SetKey(key);
+    if (!version_id.empty())
+    {
+        req.SetVersionId(version_id);
+    }
 
     /**
      * If remote_filesystem_read_method = 'threadpool', then for MergeTree family tables
@@ -259,13 +272,26 @@ std::unique_ptr<ReadBuffer> ReadBufferFromS3::initialize()
             throw Exception(ErrorCodes::LOGICAL_ERROR, "Attempt to read beyond right offset ({} > {})", offset, read_until_position - 1);
 
         req.SetRange(fmt::format("bytes={}-{}", offset, read_until_position - 1));
-        LOG_TEST(log, "Read S3 object. Bucket: {}, Key: {}, Range: {}-{}", bucket, key, offset, read_until_position - 1);
+        LOG_TEST(
+            log,
+            "Read S3 object. Bucket: {}, Key: {}, Version: {}, Range: {}-{}",
+            bucket,
+            key,
+            version_id.empty() ? "Latest" : version_id,
+            offset,
+            read_until_position - 1);
     }
     else
     {
         if (offset)
             req.SetRange(fmt::format("bytes={}-", offset));
-        LOG_TEST(log, "Read S3 object. Bucket: {}, Key: {}, Offset: {}", bucket, key, offset);
+        LOG_TEST(
+            log,
+            "Read S3 object. Bucket: {}, Key: {}, Version: {}, Offset: {}",
+            bucket,
+            key,
+            version_id.empty() ? "Latest" : version_id,
+            offset);
     }
 
     Aws::S3::Model::GetObjectOutcome outcome = client_ptr->GetObject(req);
@@ -293,6 +319,7 @@ SeekableReadBufferPtr ReadBufferS3Factory::getReader()
         client_ptr,
         bucket,
         key,
+        version_id,
         s3_max_single_read_retries,
         read_settings,
         false /*use_external_buffer*/,
diff --git a/src/IO/ReadBufferFromS3.h b/src/IO/ReadBufferFromS3.h
index 2a94d286da6f..8c582d3a0d2b 100644
--- a/src/IO/ReadBufferFromS3.h
+++ b/src/IO/ReadBufferFromS3.h
@@ -32,6 +32,7 @@ class ReadBufferFromS3 : public SeekableReadBufferWithSize, public WithFileName
     std::shared_ptr<Aws::S3::S3Client> client_ptr;
     String bucket;
     String key;
+    String version_id;
     UInt64 max_single_read_retries;
 
     /// These variables are atomic because they can be used for `logging only`
@@ -50,6 +51,7 @@ class ReadBufferFromS3 : public SeekableReadBufferWithSize, public WithFileName
         std::shared_ptr<Aws::S3::S3Client> client_ptr_,
         const String & bucket_,
         const String & key_,
+        const String & version_id_,
         UInt64 max_single_read_retries_,
         const ReadSettings & settings_,
         bool use_external_buffer = false,
@@ -93,6 +95,7 @@ class ReadBufferS3Factory : public ParallelReadBuffer::ReadBufferFactory, public
         std::shared_ptr<Aws::S3::S3Client> client_ptr_,
         const String & bucket_,
         const String & key_,
+        const String & version_id_,
         size_t range_step_,
         size_t object_size_,
         UInt64 s3_max_single_read_retries_,
@@ -100,6 +103,7 @@ class ReadBufferS3Factory : public ParallelReadBuffer::ReadBufferFactory, public
         : client_ptr(client_ptr_)
         , bucket(bucket_)
         , key(key_)
+        , version_id(version_id_)
         , read_settings(read_settings_)
         , range_generator(object_size_, range_step_)
         , range_step(range_step_)
@@ -122,6 +126,7 @@ class ReadBufferS3Factory : public ParallelReadBuffer::ReadBufferFactory, public
     std::shared_ptr<Aws::S3::S3Client> client_ptr;
     const String bucket;
     const String key;
+    const String version_id;
     ReadSettings read_settings;
 
     RangeGenerator range_generator;
diff --git a/src/IO/S3Common.cpp b/src/IO/S3Common.cpp
index fefc296a200b..4853a90542ed 100644
--- a/src/IO/S3Common.cpp
+++ b/src/IO/S3Common.cpp
@@ -779,13 +779,27 @@ namespace S3
         static constexpr auto OBS = "OBS";
         static constexpr auto OSS = "OSS";
 
-
         uri = uri_;
         storage_name = S3;
 
         if (uri.getHost().empty())
             throw Exception(ErrorCodes::BAD_ARGUMENTS, "Host is empty in S3 URI.");
 
+        /// Extract object version ID from query string.
+        {
+            version_id = "";
+            const String version_key = "versionId=";
+            const auto query_string = uri.getQuery();
+
+            auto start = query_string.rfind(version_key);
+            if (start != std::string::npos)
+            {
+                start += version_key.length();
+                auto end = query_string.find_first_of('&', start);
+                version_id = query_string.substr(start, end == std::string::npos ? std::string::npos : end - start);
+            }
+        }
+
         String name;
         String endpoint_authority_from_uri;
 
@@ -842,12 +856,15 @@ namespace S3
                             quoteString(bucket), !uri.empty() ? " (" + uri.toString() + ")" : "");
     }
 
-    size_t getObjectSize(std::shared_ptr<Aws::S3::S3Client> client_ptr, const String & bucket, const String & key, bool throw_on_error)
+    size_t getObjectSize(std::shared_ptr<Aws::S3::S3Client> client_ptr, const String & bucket, const String & key, const String & version_id, bool throw_on_error)
     {
         Aws::S3::Model::HeadObjectRequest req;
         req.SetBucket(bucket);
         req.SetKey(key);
 
+        if (!version_id.empty())
+            req.SetVersionId(version_id);
+
         Aws::S3::Model::HeadObjectOutcome outcome = client_ptr->HeadObject(req);
 
         if (outcome.IsSuccess())
diff --git a/src/IO/S3Common.h b/src/IO/S3Common.h
index c33ce427e66a..16134f173d56 100644
--- a/src/IO/S3Common.h
+++ b/src/IO/S3Common.h
@@ -66,6 +66,7 @@ struct URI
     String endpoint;
     String bucket;
     String key;
+    String version_id;
     String storage_name;
 
     bool is_virtual_hosted_style;
@@ -75,7 +76,7 @@ struct URI
     static void validateBucket(const String & bucket, const Poco::URI & uri);
 };
 
-size_t getObjectSize(std::shared_ptr<Aws::S3::S3Client> client_ptr, const String & bucket, const String & key, bool throw_on_error = true);
+size_t getObjectSize(std::shared_ptr<Aws::S3::S3Client> client_ptr, const String & bucket, const String & key, const String & version_id = {}, bool throw_on_error = true);
 
 }
 
diff --git a/src/Storages/StorageS3.cpp b/src/Storages/StorageS3.cpp
index dd617517acc5..e43fedebd864 100644
--- a/src/Storages/StorageS3.cpp
+++ b/src/Storages/StorageS3.cpp
@@ -232,12 +232,14 @@ StorageS3Source::StorageS3Source(
     String compression_hint_,
     const std::shared_ptr<Aws::S3::S3Client> & client_,
     const String & bucket_,
+    const String & version_id_,
     std::shared_ptr<IteratorWrapper> file_iterator_,
     const size_t download_thread_num_)
     : SourceWithProgress(getHeader(sample_block_, requested_virtual_columns_))
     , WithContext(context_)
     , name(std::move(name_))
     , bucket(bucket_)
+    , version_id(version_id_)
     , format(format_)
     , columns_desc(columns_)
     , max_block_size(max_block_size_)
@@ -291,7 +293,7 @@ bool StorageS3Source::initialize()
 
 std::unique_ptr<ReadBuffer> StorageS3Source::createS3ReadBuffer(const String & key)
 {
-    const size_t object_size = DB::S3::getObjectSize(client, bucket, key, false);
+    const size_t object_size = DB::S3::getObjectSize(client, bucket, key, version_id, false);
 
     auto download_buffer_size = getContext()->getSettings().max_download_buffer_size;
     const bool use_parallel_download = download_buffer_size > 0 && download_thread_num > 1;
@@ -299,7 +301,7 @@ std::unique_ptr<ReadBuffer> StorageS3Source::createS3ReadBuffer(const String & k
     if (!use_parallel_download || object_too_small)
     {
         LOG_TRACE(log, "Downloading object of size {} from S3 in single thread", object_size);
-        return std::make_unique<ReadBufferFromS3>(client, bucket, key, max_single_read_retries, getContext()->getReadSettings());
+        return std::make_unique<ReadBufferFromS3>(client, bucket, key, version_id, max_single_read_retries, getContext()->getReadSettings());
     }
 
     assert(object_size > 0);
@@ -311,7 +313,7 @@ std::unique_ptr<ReadBuffer> StorageS3Source::createS3ReadBuffer(const String & k
     }
 
     auto factory = std::make_unique<ReadBufferS3Factory>(
-        client, bucket, key, download_buffer_size, object_size, max_single_read_retries, getContext()->getReadSettings());
+        client, bucket, key, version_id, download_buffer_size, object_size, max_single_read_retries, getContext()->getReadSettings());
     LOG_TRACE(
         log, "Downloading from S3 in {} threads. Object size: {}, Range size: {}.", download_thread_num, object_size, download_buffer_size);
 
@@ -693,6 +695,7 @@ Pipe StorageS3::read(
             compression_method,
             s3_configuration.client,
             s3_configuration.uri.bucket,
+            s3_configuration.uri.version_id,
             iterator_wrapper,
             max_download_threads));
     }
@@ -966,7 +969,7 @@ ColumnsDescription StorageS3::getTableStructureFromDataImpl(
         first = false;
         return wrapReadBufferWithCompressionMethod(
             std::make_unique<ReadBufferFromS3>(
-                s3_configuration.client, s3_configuration.uri.bucket, key, s3_configuration.rw_settings.max_single_read_retries, ctx->getReadSettings()),
+                s3_configuration.client, s3_configuration.uri.bucket, key, s3_configuration.uri.version_id, s3_configuration.rw_settings.max_single_read_retries, ctx->getReadSettings()),
             chooseCompressionMethod(key, compression_method));
     };
 
diff --git a/src/Storages/StorageS3.h b/src/Storages/StorageS3.h
index 191806ede95d..da176d805bbc 100644
--- a/src/Storages/StorageS3.h
+++ b/src/Storages/StorageS3.h
@@ -73,6 +73,7 @@ class StorageS3Source : public SourceWithProgress, WithContext
         String compression_hint_,
         const std::shared_ptr<Aws::S3::S3Client> & client_,
         const String & bucket,
+        const String & version_id,
         std::shared_ptr<IteratorWrapper> file_iterator_,
         size_t download_thread_num);
 
@@ -85,6 +86,7 @@ class StorageS3Source : public SourceWithProgress, WithContext
 private:
     String name;
     String bucket;
+    String version_id;
     String file_path;
     String format;
     ColumnsDescription columns_desc;
