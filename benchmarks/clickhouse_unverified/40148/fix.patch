diff --git a/src/Common/ErrorCodes.cpp b/src/Common/ErrorCodes.cpp
index f8d8deab08b2..f65711a8521a 100644
--- a/src/Common/ErrorCodes.cpp
+++ b/src/Common/ErrorCodes.cpp
@@ -635,6 +635,7 @@
     M(664, ACCESS_STORAGE_DOESNT_ALLOW_BACKUP) \
     M(665, CANNOT_CONNECT_NATS) \
     M(666, CANNOT_USE_CACHE) \
+    M(667, NOT_INITIALIZED) \
     \
     M(999, KEEPER_EXCEPTION) \
     M(1000, POCO_EXCEPTION) \
diff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h
index 89081fe924f0..07659b1c9dc3 100644
--- a/src/Storages/MergeTree/MergeTreeSettings.h
+++ b/src/Storages/MergeTree/MergeTreeSettings.h
@@ -95,6 +95,7 @@ struct Settings;
     M(Seconds, replicated_fetches_http_receive_timeout, 0, "HTTP receive timeout for fetch part requests. Inherited from default profile `http_receive_timeout` if not set explicitly.", 0) \
     M(Bool, replicated_can_become_leader, true, "If true, Replicated tables replicas on this node will try to acquire leadership.", 0) \
     M(Seconds, zookeeper_session_expiration_check_period, 60, "ZooKeeper session expiration check period, in seconds.", 0) \
+    M(Seconds, initialization_retry_period, 60, "Retry period for table initialization, in seconds.", 0) \
     M(Bool, detach_old_local_parts_when_cloning_replica, true, "Do not remove old local parts when repairing lost replica.", 0) \
     M(Bool, detach_not_byte_identical_parts, false, "Do not remove non byte-idential parts for ReplicatedMergeTree, instead detach them (maybe useful for further analysis).", 0) \
     M(UInt64, max_replicated_fetches_network_bandwidth, 0, "The maximum speed of data exchange over the network in bytes per second for replicated fetches. Zero means unlimited.", 0) \
diff --git a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp
new file mode 100644
index 000000000000..566d5dc3fa3d
--- /dev/null
+++ b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp
@@ -0,0 +1,167 @@
+#include <Storages/MergeTree/ReplicatedMergeTreeAttachThread.h>
+#include <Storages/StorageReplicatedMergeTree.h>
+#include <Common/ZooKeeper/IKeeper.h>
+
+namespace DB
+{
+
+ReplicatedMergeTreeAttachThread::ReplicatedMergeTreeAttachThread(StorageReplicatedMergeTree & storage_)
+    : storage(storage_)
+    , log_name(storage.getStorageID().getFullTableName() + " (ReplicatedMergeTreeAttachThread)")
+    , log(&Poco::Logger::get(log_name))
+{
+    task = storage.getContext()->getSchedulePool().createTask(log_name, [this] { run(); });
+    const auto storage_settings = storage.getSettings();
+    retry_period = storage_settings->initialization_retry_period.totalSeconds();
+}
+
+ReplicatedMergeTreeAttachThread::~ReplicatedMergeTreeAttachThread()
+{
+    shutdown();
+}
+
+void ReplicatedMergeTreeAttachThread::shutdown()
+{
+    if (!shutdown_called.exchange(true))
+    {
+        task->deactivate();
+        LOG_INFO(log, "Attach thread finished");
+    }
+}
+
+void ReplicatedMergeTreeAttachThread::run()
+{
+    bool needs_retry{false};
+    try
+    {
+        // we delay the first reconnect if the storage failed to connect to ZK initially
+        if (!first_try_done && !storage.current_zookeeper)
+        {
+            needs_retry = true;
+        }
+        else
+        {
+            runImpl();
+            finalizeInitialization();
+        }
+    }
+    catch (const Exception & e)
+    {
+        if (const auto * coordination_exception = dynamic_cast<const Coordination::Exception *>(&e))
+            needs_retry = Coordination::isHardwareError(coordination_exception->code);
+
+        if (needs_retry)
+        {
+            LOG_ERROR(log, "Initialization failed. Error: {}", e.message());
+        }
+        else
+        {
+            LOG_ERROR(log, "Initialization failed, table will remain readonly. Error: {}", e.message());
+            storage.initialization_done = true;
+        }
+    }
+
+    if (!first_try_done.exchange(true))
+        first_try_done.notify_one();
+
+    if (shutdown_called)
+    {
+        LOG_WARNING(log, "Shutdown called, cancelling initialization");
+        return;
+    }
+
+    if (needs_retry)
+    {
+        LOG_INFO(log, "Will retry initialization in {}s", retry_period);
+        task->scheduleAfter(retry_period * 1000);
+    }
+}
+
+void ReplicatedMergeTreeAttachThread::runImpl()
+{
+    storage.setZooKeeper();
+
+    auto zookeeper = storage.getZooKeeper();
+    const auto & zookeeper_path = storage.zookeeper_path;
+    bool metadata_exists = zookeeper->exists(zookeeper_path + "/metadata");
+    if (!metadata_exists)
+    {
+        LOG_WARNING(log, "No metadata in ZooKeeper for {}: table will stay in readonly mode.", zookeeper_path);
+        storage.has_metadata_in_zookeeper = false;
+        return;
+    }
+
+    auto metadata_snapshot = storage.getInMemoryMetadataPtr();
+
+    const auto & replica_path = storage.replica_path;
+    /// May it be ZK lost not the whole root, so the upper check passed, but only the /replicas/replica
+    /// folder.
+    bool replica_path_exists = zookeeper->exists(replica_path);
+    if (!replica_path_exists)
+    {
+        LOG_WARNING(log, "No metadata in ZooKeeper for {}: table will stay in readonly mode", replica_path);
+        storage.has_metadata_in_zookeeper = false;
+        return;
+    }
+
+    storage.has_metadata_in_zookeeper = true;
+
+    /// In old tables this node may missing or be empty
+    String replica_metadata;
+    const bool replica_metadata_exists = zookeeper->tryGet(replica_path + "/metadata", replica_metadata);
+
+    if (!replica_metadata_exists || replica_metadata.empty())
+    {
+        /// We have to check shared node granularity before we create ours.
+        storage.other_replicas_fixed_granularity = storage.checkFixedGranularityInZookeeper();
+
+        ReplicatedMergeTreeTableMetadata current_metadata(storage, metadata_snapshot);
+
+        zookeeper->createOrUpdate(replica_path + "/metadata", current_metadata.toString(), zkutil::CreateMode::Persistent);
+    }
+
+    storage.checkTableStructure(replica_path, metadata_snapshot);
+    storage.checkParts(skip_sanity_checks);
+
+    if (zookeeper->exists(replica_path + "/metadata_version"))
+    {
+        storage.metadata_version = parse<int>(zookeeper->get(replica_path + "/metadata_version"));
+    }
+    else
+    {
+        /// This replica was created with old clickhouse version, so we have
+        /// to take version of global node. If somebody will alter our
+        /// table, then we will fill /metadata_version node in zookeeper.
+        /// Otherwise on the next restart we can again use version from
+        /// shared metadata node because it was not changed.
+        Coordination::Stat metadata_stat;
+        zookeeper->get(zookeeper_path + "/metadata", &metadata_stat);
+        storage.metadata_version = metadata_stat.version;
+    }
+
+    /// Temporary directories contain uninitialized results of Merges or Fetches (after forced restart),
+    /// don't allow to reinitialize them, delete each of them immediately.
+    storage.clearOldTemporaryDirectories(0, {"tmp_", "delete_tmp_", "tmp-fetch_"});
+    storage.clearOldWriteAheadLogs();
+    if (storage.getSettings()->merge_tree_enable_clear_old_broken_detached)
+        storage.clearOldBrokenPartsFromDetachedDirecory();
+
+    storage.createNewZooKeeperNodes();
+    storage.syncPinnedPartUUIDs();
+
+    storage.createTableSharedID();
+};
+
+void ReplicatedMergeTreeAttachThread::finalizeInitialization() TSA_NO_THREAD_SAFETY_ANALYSIS
+{
+    storage.startupImpl();
+    storage.initialization_done = true;
+    LOG_INFO(log, "Table is initialized");
+}
+
+void ReplicatedMergeTreeAttachThread::setSkipSanityChecks(bool skip_sanity_checks_)
+{
+    skip_sanity_checks = skip_sanity_checks_;
+}
+
+}
diff --git a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h
new file mode 100644
index 000000000000..4697077bec5b
--- /dev/null
+++ b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.h
@@ -0,0 +1,52 @@
+#pragma once
+
+#include <thread>
+#include <Core/BackgroundSchedulePool.h>
+#include <Common/ZooKeeper/ZooKeeper.h>
+#include <Common/logger_useful.h>
+
+namespace DB
+{
+
+class StorageReplicatedMergeTree;
+
+// Attach table to the existing data.
+// Initialize the table by creating all the necessary nodes and do the required checks.
+// Initialization is repeated if an operation fails because of a ZK request or connection loss.
+class ReplicatedMergeTreeAttachThread
+{
+public:
+    explicit ReplicatedMergeTreeAttachThread(StorageReplicatedMergeTree & storage_);
+
+    ~ReplicatedMergeTreeAttachThread();
+
+    void start() { task->activateAndSchedule(); }
+
+    void shutdown();
+
+    void waitFirstTry() { first_try_done.wait(false); }
+
+    void setSkipSanityChecks(bool skip_sanity_checks_);
+
+private:
+    StorageReplicatedMergeTree & storage;
+    BackgroundSchedulePool::TaskHolder task;
+
+    std::string log_name;
+    Poco::Logger * log;
+
+    std::atomic<bool> first_try_done{false};
+
+    std::atomic<bool> shutdown_called{false};
+
+    UInt64 retry_period;
+
+    bool skip_sanity_checks{false};
+
+    void run();
+    void runImpl();
+
+    void finalizeInitialization();
+};
+
+}
diff --git a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp
index 11f668bafbe1..9d95189b611e 100644
--- a/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp
+++ b/src/Storages/MergeTree/ReplicatedMergeTreeRestartingThread.cpp
@@ -103,7 +103,6 @@ void ReplicatedMergeTreeRestartingThread::run()
 }
 
 bool ReplicatedMergeTreeRestartingThread::runImpl()
-
 {
     if (!storage.is_readonly && !storage.getZooKeeper()->expired())
         return true;
diff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp
index 02b3422f7d20..863b58963f73 100644
--- a/src/Storages/StorageReplicatedMergeTree.cpp
+++ b/src/Storages/StorageReplicatedMergeTree.cpp
@@ -24,6 +24,7 @@
 #include <Storages/MergeTree/MergeTreeBackgroundExecutor.h>
 #include <Storages/MergeTree/MergedBlockOutputStream.h>
 #include <Storages/MergeTree/PinnedPartUUIDs.h>
+#include <Storages/MergeTree/ReplicatedMergeTreeAttachThread.h>
 #include <Storages/MergeTree/ReplicatedMergeTreeTableMetadata.h>
 #include <Storages/MergeTree/ReplicatedMergeTreeSink.h>
 #include <Storages/MergeTree/ReplicatedMergeTreeQuorumEntry.h>
@@ -159,6 +160,7 @@ namespace ErrorCodes
     extern const int BAD_ARGUMENTS;
     extern const int CONCURRENT_ACCESS_NOT_SUPPORTED;
     extern const int CHECKSUM_DOESNT_MATCH;
+    extern const int NOT_INITIALIZED;
 }
 
 namespace ActionLocks
@@ -296,7 +298,8 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(
     mutations_finalizing_task = getContext()->getSchedulePool().createTask(
         getStorageID().getFullTableName() + " (StorageReplicatedMergeTree::mutationsFinalizingTask)", [this] { mutationsFinalizingTask(); });
 
-    if (getContext()->hasZooKeeper() || getContext()->hasAuxiliaryZooKeeper(zookeeper_name))
+    bool has_zookeeper = getContext()->hasZooKeeper() || getContext()->hasAuxiliaryZooKeeper(zookeeper_name);
+    if (has_zookeeper)
     {
         /// It's possible for getZooKeeper() to timeout if  zookeeper host(s) can't
         /// be reached. In such cases Poco::Exception is thrown after a connection
@@ -325,8 +328,14 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(
         catch (...)
         {
             if (!attach)
+            {
                 dropIfEmpty();
-            throw;
+                throw;
+            }
+            else
+            {
+                current_zookeeper = nullptr;
+            }
         }
     }
 
@@ -359,124 +368,77 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(
             throw Exception("Can't create replicated table without ZooKeeper", ErrorCodes::NO_ZOOKEEPER);
         }
 
-        /// Do not activate the replica. It will be readonly.
-        LOG_ERROR(log, "No ZooKeeper: table will be in readonly mode.");
         has_metadata_in_zookeeper = std::nullopt;
-        return;
+
+        if (!has_zookeeper)
+        {
+            /// Do not activate the replica. It will be readonly.
+            LOG_ERROR(log, "No ZooKeeper defined: table will stay in readonly mode.");
+            return;
+        }
     }
 
-    if (attach && !current_zookeeper->exists(zookeeper_path + "/metadata"))
+    if (attach)
     {
-        LOG_WARNING(log, "No metadata in ZooKeeper for {}: table will be in readonly mode.", zookeeper_path);
-        has_metadata_in_zookeeper = false;
+        LOG_INFO(log, "Table will be in readonly mode until initialization is finished");
+        attach_thread.emplace(*this);
+        attach_thread->setSkipSanityChecks(skip_sanity_checks);
         return;
     }
 
     auto metadata_snapshot = getInMemoryMetadataPtr();
 
-    /// May it be ZK lost not the whole root, so the upper check passed, but only the /replicas/replica
-    /// folder.
-    if (attach && !current_zookeeper->exists(replica_path))
-    {
-        LOG_WARNING(log, "No metadata in ZooKeeper for {}: table will be in readonly mode", replica_path);
-        has_metadata_in_zookeeper = false;
-        return;
-    }
-
     has_metadata_in_zookeeper = true;
 
-    if (!attach)
+    if (!getDataPartsForInternalUsage().empty())
+        throw Exception("Data directory for table already contains data parts"
+            " - probably it was unclean DROP table or manual intervention."
+            " You must either clear directory by hand or use ATTACH TABLE"
+            " instead of CREATE TABLE if you need to use that parts.", ErrorCodes::INCORRECT_DATA);
+
+    try
     {
-        if (!getDataPartsForInternalUsage().empty())
-            throw Exception("Data directory for table already contains data parts"
-                " - probably it was unclean DROP table or manual intervention."
-                " You must either clear directory by hand or use ATTACH TABLE"
-                " instead of CREATE TABLE if you need to use that parts.", ErrorCodes::INCORRECT_DATA);
+        bool is_first_replica = createTableIfNotExists(metadata_snapshot);
 
         try
         {
-            bool is_first_replica = createTableIfNotExists(metadata_snapshot);
+            /// NOTE If it's the first replica, these requests to ZooKeeper look redundant, we already know everything.
 
-            try
-            {
-                /// NOTE If it's the first replica, these requests to ZooKeeper look redundant, we already know everything.
-
-                /// We have to check granularity on other replicas. If it's fixed we
-                /// must create our new replica with fixed granularity and store this
-                /// information in /replica/metadata.
-                other_replicas_fixed_granularity = checkFixedGranularityInZookeeper();
-
-                checkTableStructure(zookeeper_path, metadata_snapshot);
-
-                Coordination::Stat metadata_stat;
-                current_zookeeper->get(zookeeper_path + "/metadata", &metadata_stat);
-                metadata_version = metadata_stat.version;
-            }
-            catch (Coordination::Exception & e)
-            {
-                if (!is_first_replica && e.code == Coordination::Error::ZNONODE)
-                    throw Exception("Table " + zookeeper_path + " was suddenly removed.", ErrorCodes::ALL_REPLICAS_LOST);
-                else
-                    throw;
-            }
-
-            if (!is_first_replica)
-                createReplica(metadata_snapshot);
-        }
-        catch (...)
-        {
-            /// If replica was not created, rollback creation of data directory.
-            dropIfEmpty();
-            throw;
-        }
-    }
-    else
-    {
-        /// In old tables this node may missing or be empty
-        String replica_metadata;
-        const bool replica_metadata_exists = current_zookeeper->tryGet(replica_path + "/metadata", replica_metadata);
-
-        if (!replica_metadata_exists || replica_metadata.empty())
-        {
-            /// We have to check shared node granularity before we create ours.
+            /// We have to check granularity on other replicas. If it's fixed we
+            /// must create our new replica with fixed granularity and store this
+            /// information in /replica/metadata.
             other_replicas_fixed_granularity = checkFixedGranularityInZookeeper();
 
-            ReplicatedMergeTreeTableMetadata current_metadata(*this, metadata_snapshot);
-
-            current_zookeeper->createOrUpdate(replica_path + "/metadata", current_metadata.toString(),
-                zkutil::CreateMode::Persistent);
-        }
-
-        checkTableStructure(replica_path, metadata_snapshot);
-        checkParts(skip_sanity_checks);
+            checkTableStructure(zookeeper_path, metadata_snapshot);
 
-        if (current_zookeeper->exists(replica_path + "/metadata_version"))
-        {
-            metadata_version = parse<int>(current_zookeeper->get(replica_path + "/metadata_version"));
-        }
-        else
-        {
-            /// This replica was created with old clickhouse version, so we have
-            /// to take version of global node. If somebody will alter our
-            /// table, then we will fill /metadata_version node in zookeeper.
-            /// Otherwise on the next restart we can again use version from
-            /// shared metadata node because it was not changed.
             Coordination::Stat metadata_stat;
             current_zookeeper->get(zookeeper_path + "/metadata", &metadata_stat);
             metadata_version = metadata_stat.version;
         }
-        /// Temporary directories contain uninitialized results of Merges or Fetches (after forced restart),
-        /// don't allow to reinitialize them, delete each of them immediately.
-        clearOldTemporaryDirectories(0, {"tmp_", "delete_tmp_", "tmp-fetch_"});
-        clearOldWriteAheadLogs();
-        if (getSettings()->merge_tree_enable_clear_old_broken_detached)
-            clearOldBrokenPartsFromDetachedDirecory();
+        catch (Coordination::Exception & e)
+        {
+            if (!is_first_replica && e.code == Coordination::Error::ZNONODE)
+                throw Exception("Table " + zookeeper_path + " was suddenly removed.", ErrorCodes::ALL_REPLICAS_LOST);
+            else
+                throw;
+        }
+
+        if (!is_first_replica)
+            createReplica(metadata_snapshot);
+    }
+    catch (...)
+    {
+        /// If replica was not created, rollback creation of data directory.
+        dropIfEmpty();
+        throw;
     }
 
     createNewZooKeeperNodes();
     syncPinnedPartUUIDs();
 
     createTableSharedID();
+
+    initialization_done = true;
 }
 
 
@@ -874,7 +836,6 @@ void StorageReplicatedMergeTree::drop()
         if (!zookeeper)
             throw Exception("Can't drop readonly replicated table (need to drop data in ZooKeeper as well)", ErrorCodes::TABLE_IS_READ_ONLY);
 
-        shutdown();
         dropReplica(zookeeper, zookeeper_path, replica_name, log, getSettings());
     }
 
@@ -3481,13 +3442,15 @@ void StorageReplicatedMergeTree::removePartAndEnqueueFetch(const String & part_n
 
 void StorageReplicatedMergeTree::startBeingLeader()
 {
+    auto zookeeper = getZooKeeper();
+
     if (!getSettings()->replicated_can_become_leader)
     {
         LOG_INFO(log, "Will not enter leader election because replicated_can_become_leader=0");
         return;
     }
 
-    zkutil::checkNoOldLeaders(log, *current_zookeeper, fs::path(zookeeper_path) / "leader_election");
+    zkutil::checkNoOldLeaders(log, *zookeeper, fs::path(zookeeper_path) / "leader_election");
 
     LOG_INFO(log, "Became leader");
     is_leader = true;
@@ -4186,8 +4149,19 @@ DataPartStoragePtr StorageReplicatedMergeTree::fetchExistsPart(
     return part->data_part_storage;
 }
 
-
 void StorageReplicatedMergeTree::startup()
+{
+    if (attach_thread)
+    {
+        attach_thread->start();
+        attach_thread->waitFirstTry();
+        return;
+    }
+
+    startupImpl();
+}
+
+void StorageReplicatedMergeTree::startupImpl()
 {
     /// Do not start replication if ZooKeeper is not configured or there is no metadata in zookeeper
     if (!has_metadata_in_zookeeper.has_value() || !*has_metadata_in_zookeeper)
@@ -4195,6 +4169,7 @@ void StorageReplicatedMergeTree::startup()
 
     try
     {
+        auto zookeeper = getZooKeeper();
         InterserverIOEndpointPtr data_parts_exchange_ptr = std::make_shared<DataPartsExchange::Service>(*this);
         [[maybe_unused]] auto prev_ptr = std::atomic_exchange(&data_parts_exchange_endpoint, data_parts_exchange_ptr);
         assert(prev_ptr == nullptr);
@@ -4254,6 +4229,8 @@ void StorageReplicatedMergeTree::shutdown()
     mutations_finalizing_task->deactivate();
     stopBeingLeader();
 
+    if (attach_thread)
+        attach_thread->shutdown();
     restarting_thread.shutdown();
     background_operations_assignee.finish();
     part_moves_between_shards_orchestrator.shutdown();
@@ -4993,9 +4970,14 @@ bool StorageReplicatedMergeTree::getFakePartCoveringAllPartsInPartition(const St
 void StorageReplicatedMergeTree::restoreMetadataInZooKeeper()
 {
     LOG_INFO(log, "Restoring replica metadata");
+
+    if (!initialization_done)
+        throw Exception(ErrorCodes::NOT_INITIALIZED, "Table is not initialized yet");
+
     if (!is_readonly)
         throw Exception(ErrorCodes::BAD_ARGUMENTS, "Replica must be readonly");
 
+
     if (getZooKeeper()->exists(replica_path))
         throw Exception(ErrorCodes::BAD_ARGUMENTS,
                         "Replica path is present at {} - nothing to restore. "
@@ -5047,7 +5029,7 @@ void StorageReplicatedMergeTree::restoreMetadataInZooKeeper()
 
     LOG_INFO(log, "Attached all partitions, starting table");
 
-    startup();
+    startupImpl();
 }
 
 void StorageReplicatedMergeTree::dropPartNoWaitNoThrow(const String & part_name)
@@ -7439,7 +7421,7 @@ void StorageReplicatedMergeTree::createTableSharedID()
     if (table_shared_id != UUIDHelpers::Nil)
         throw Exception(ErrorCodes::LOGICAL_ERROR, "Table shared id already initialized");
 
-    zkutil::ZooKeeperPtr zookeeper = getZooKeeper();
+    auto zookeeper = getZooKeeper();
     String zookeeper_table_id_path = fs::path(zookeeper_path) / "table_shared_id";
     String id;
     if (!zookeeper->tryGet(zookeeper_table_id_path, id))
diff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h
index d4eb49eba0dd..24b4a4d5634b 100644
--- a/src/Storages/StorageReplicatedMergeTree.h
+++ b/src/Storages/StorageReplicatedMergeTree.h
@@ -13,6 +13,7 @@
 #include <Storages/MergeTree/ReplicatedMergeTreeQueue.h>
 #include <Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h>
 #include <Storages/MergeTree/ReplicatedMergeTreeRestartingThread.h>
+#include <Storages/MergeTree/ReplicatedMergeTreeAttachThread.h>
 #include <Storages/MergeTree/ReplicatedMergeTreeMergeStrategyPicker.h>
 #include <Storages/MergeTree/ReplicatedMergeTreePartCheckThread.h>
 #include <Storages/MergeTree/ReplicatedMergeTreeTableMetadata.h>
@@ -28,6 +29,7 @@
 #include <Common/randomSeed.h>
 #include <Common/ZooKeeper/ZooKeeper.h>
 #include <Common/Throttler.h>
+#include <base/defines.h>
 #include <Core/BackgroundSchedulePool.h>
 #include <QueryPipeline/Pipe.h>
 #include <Storages/MergeTree/BackgroundJobsAssignee.h>
@@ -339,6 +341,7 @@ class StorageReplicatedMergeTree final : public MergeTreeData
     friend class ReplicatedMergeTreeCleanupThread;
     friend class ReplicatedMergeTreeAlterThread;
     friend class ReplicatedMergeTreeRestartingThread;
+    friend class ReplicatedMergeTreeAttachThread;
     friend class ReplicatedMergeTreeMergeStrategyPicker;
     friend struct ReplicatedMergeTreeLogEntry;
     friend class ScopedPartitionMergeLock;
@@ -444,8 +447,13 @@ class StorageReplicatedMergeTree final : public MergeTreeData
     /// A thread that processes reconnection to ZooKeeper when the session expires.
     ReplicatedMergeTreeRestartingThread restarting_thread;
 
+    /// A thread that attaches the table using ZooKeeper
+    std::optional<ReplicatedMergeTreeAttachThread> attach_thread;
+
     PartMovesBetweenShardsOrchestrator part_moves_between_shards_orchestrator;
 
+    std::atomic<bool> initialization_done{false};
+
     /// True if replica was created for existing table with fixed granularity
     bool other_replicas_fixed_granularity = false;
 
@@ -835,6 +843,8 @@ class StorageReplicatedMergeTree final : public MergeTreeData
     /// Create ephemeral lock in zookeeper for part and disk which support zero copy replication.
     /// If somebody already holding the lock -- return std::nullopt.
     std::optional<ZeroCopyLock> tryCreateZeroCopyExclusiveLock(const String & part_name, const DiskPtr & disk) override;
+
+    void startupImpl();
 };
 
 String getPartNamePossiblyFake(MergeTreeDataFormatVersion format_version, const MergeTreePartInfo & part_info);
