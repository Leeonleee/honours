{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 21942,
  "instance_id": "ClickHouse__ClickHouse-21942",
  "issue_numbers": [
    "20229"
  ],
  "base_commit": "92f57fd669fe163b5237db1ccf1d5a170a9c3550",
  "patch": "diff --git a/src/Common/ErrorCodes.cpp b/src/Common/ErrorCodes.cpp\nindex 0f85ad5c792f..2ff7cfc4b17d 100644\n--- a/src/Common/ErrorCodes.cpp\n+++ b/src/Common/ErrorCodes.cpp\n@@ -548,6 +548,7 @@\n     M(578, INVALID_FORMAT_INSERT_QUERY_WITH_DATA) \\\n     M(579, INCORRECT_PART_TYPE) \\\n     M(580, CANNOT_SET_ROUNDING_MODE) \\\n+    M(581, TOO_LARGE_DISTRIBUTED_DEPTH) \\\n     \\\n     M(998, POSTGRESQL_CONNECTION_FAILURE) \\\n     M(999, KEEPER_EXCEPTION) \\\ndiff --git a/src/Core/Defines.h b/src/Core/Defines.h\nindex 468c1187e91c..e7c1c86a23e6 100644\n--- a/src/Core/Defines.h\n+++ b/src/Core/Defines.h\n@@ -81,8 +81,9 @@\n #define DBMS_MIN_REVISION_WITH_REFERER_IN_CLIENT_INFO 54447\n \n /// Version of ClickHouse TCP protocol. Increment it manually when you change the protocol.\n-#define DBMS_TCP_PROTOCOL_VERSION 54447\n+#define DBMS_TCP_PROTOCOL_VERSION 54448\n \n+#define DBMS_MIN_PROTOCOL_VERSION_WITH_DISTRIBUTED_DEPTH 54448\n /// The boundary on which the blocks for asynchronous file operations should be aligned.\n #define DEFAULT_AIO_FILE_BLOCK_SIZE 4096\n \ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 045433dc895f..81107cd56108 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -440,6 +440,7 @@ class IColumn;\n     M(Bool, engine_file_truncate_on_insert, false, \"Enables or disables truncate before insert in file engine tables\", 0) \\\n     M(Bool, allow_experimental_database_replicated, false, \"Allow to create databases with Replicated engine\", 0) \\\n     M(UInt64, database_replicated_initial_query_timeout_sec, 300, \"How long initial DDL query should wait for Replicated database to precess previous DDL queue entries\", 0) \\\n+    M(UInt64, max_distributed_depth, 5, \"Maximum distributed query depth\", 0) \\\n     M(Bool, database_replicated_always_detach_permanently, false, \"Execute DETACH TABLE as DETACH TABLE PERMANENTLY if database engine is Replicated\", 0) \\\n     M(DistributedDDLOutputMode, distributed_ddl_output_mode, DistributedDDLOutputMode::THROW, \"Format of distributed DDL query result\", 0) \\\n     M(UInt64, distributed_ddl_entry_format_version, 1, \"Version of DDL entry to write into ZooKeeper\", 0) \\\ndiff --git a/src/Interpreters/ClientInfo.cpp b/src/Interpreters/ClientInfo.cpp\nindex 5449f397f49d..223837aaf3d5 100644\n--- a/src/Interpreters/ClientInfo.cpp\n+++ b/src/Interpreters/ClientInfo.cpp\n@@ -60,6 +60,9 @@ void ClientInfo::write(WriteBuffer & out, const UInt64 server_protocol_revision)\n     if (server_protocol_revision >= DBMS_MIN_REVISION_WITH_QUOTA_KEY_IN_CLIENT_INFO)\n         writeBinary(quota_key, out);\n \n+    if (server_protocol_revision >= DBMS_MIN_PROTOCOL_VERSION_WITH_DISTRIBUTED_DEPTH)\n+        writeVarUInt(distributed_depth, out);\n+\n     if (interface == Interface::TCP)\n     {\n         if (server_protocol_revision >= DBMS_MIN_REVISION_WITH_VERSION_PATCH)\n@@ -137,6 +140,9 @@ void ClientInfo::read(ReadBuffer & in, const UInt64 client_protocol_revision)\n     if (client_protocol_revision >= DBMS_MIN_REVISION_WITH_QUOTA_KEY_IN_CLIENT_INFO)\n         readBinary(quota_key, in);\n \n+    if (client_protocol_revision >= DBMS_MIN_PROTOCOL_VERSION_WITH_DISTRIBUTED_DEPTH)\n+        readVarUInt(distributed_depth, in);\n+\n     if (interface == Interface::TCP)\n     {\n         if (client_protocol_revision >= DBMS_MIN_REVISION_WITH_VERSION_PATCH)\ndiff --git a/src/Interpreters/ClientInfo.h b/src/Interpreters/ClientInfo.h\nindex bc471dc3aa41..21aae45bfab1 100644\n--- a/src/Interpreters/ClientInfo.h\n+++ b/src/Interpreters/ClientInfo.h\n@@ -96,6 +96,8 @@ class ClientInfo\n     /// Common\n     String quota_key;\n \n+    UInt64 distributed_depth = 0;\n+\n     bool empty() const { return query_kind == QueryKind::NO_QUERY; }\n \n     /** Serialization and deserialization.\ndiff --git a/src/Interpreters/ClusterProxy/executeQuery.cpp b/src/Interpreters/ClusterProxy/executeQuery.cpp\nindex 59cbae67770d..c75aba8a79ca 100644\n--- a/src/Interpreters/ClusterProxy/executeQuery.cpp\n+++ b/src/Interpreters/ClusterProxy/executeQuery.cpp\n@@ -16,6 +16,11 @@\n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int TOO_LARGE_DISTRIBUTED_DEPTH;\n+}\n+\n namespace ClusterProxy\n {\n \n@@ -92,6 +97,9 @@ void executeQuery(\n \n     const Settings & settings = context.getSettingsRef();\n \n+    if (settings.max_distributed_depth && context.getClientInfo().distributed_depth > settings.max_distributed_depth)\n+        throw Exception(\"Maximum distributed depth exceeded\", ErrorCodes::TOO_LARGE_DISTRIBUTED_DEPTH);\n+\n     std::vector<QueryPlanPtr> plans;\n     Pipes remote_pipes;\n     Pipes delayed_pipes;\n@@ -100,6 +108,8 @@ void executeQuery(\n \n     auto new_context = updateSettingsForCluster(*query_info.cluster, context, settings, log);\n \n+    new_context->getClientInfo().distributed_depth += 1;\n+\n     ThrottlerPtr user_level_throttler;\n     if (auto * process_list_element = context.getProcessListElement())\n         user_level_throttler = process_list_element->getUserNetworkThrottler();\ndiff --git a/src/Storages/Distributed/DistributedBlockOutputStream.cpp b/src/Storages/Distributed/DistributedBlockOutputStream.cpp\nindex f8ba4221842a..a81fed62f2b1 100644\n--- a/src/Storages/Distributed/DistributedBlockOutputStream.cpp\n+++ b/src/Storages/Distributed/DistributedBlockOutputStream.cpp\n@@ -58,6 +58,7 @@ namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n     extern const int TIMEOUT_EXCEEDED;\n+    extern const int TOO_LARGE_DISTRIBUTED_DEPTH;\n }\n \n static Block adoptBlock(const Block & header, const Block & block, Poco::Logger * log)\n@@ -93,7 +94,7 @@ DistributedBlockOutputStream::DistributedBlockOutputStream(\n     const ClusterPtr & cluster_,\n     bool insert_sync_,\n     UInt64 insert_timeout_)\n-    : context(context_)\n+    : context(std::make_unique<Context>(context_))\n     , storage(storage_)\n     , metadata_snapshot(metadata_snapshot_)\n     , query_ast(query_ast_)\n@@ -103,6 +104,10 @@ DistributedBlockOutputStream::DistributedBlockOutputStream(\n     , insert_timeout(insert_timeout_)\n     , log(&Poco::Logger::get(\"DistributedBlockOutputStream\"))\n {\n+    const auto & settings = context->getSettingsRef();\n+    if (settings.max_distributed_depth && context->getClientInfo().distributed_depth > settings.max_distributed_depth)\n+        throw Exception(\"Maximum distributed depth exceeded\", ErrorCodes::TOO_LARGE_DISTRIBUTED_DEPTH);\n+    context->getClientInfo().distributed_depth += 1;\n }\n \n \n@@ -143,7 +148,7 @@ void DistributedBlockOutputStream::write(const Block & block)\n \n void DistributedBlockOutputStream::writeAsync(const Block & block)\n {\n-    const Settings & settings = context.getSettingsRef();\n+    const Settings & settings = context->getSettingsRef();\n     bool random_shard_insert = settings.insert_distributed_one_random_shard && !storage.has_sharding_key;\n \n     if (random_shard_insert)\n@@ -194,7 +199,7 @@ std::string DistributedBlockOutputStream::getCurrentStateDescription()\n \n void DistributedBlockOutputStream::initWritingJobs(const Block & first_block, size_t start, size_t end)\n {\n-    const Settings & settings = context.getSettingsRef();\n+    const Settings & settings = context->getSettingsRef();\n     const auto & addresses_with_failovers = cluster->getShardsAddresses();\n     const auto & shards_info = cluster->getShardsInfo();\n     size_t num_shards = end - start;\n@@ -303,7 +308,7 @@ DistributedBlockOutputStream::runWritingJob(DistributedBlockOutputStream::JobRep\n         }\n \n         const Block & shard_block = (num_shards > 1) ? job.current_shard_block : current_block;\n-        const Settings & settings = context.getSettingsRef();\n+        const Settings & settings = context->getSettingsRef();\n \n         /// Do not initiate INSERT for empty block.\n         if (shard_block.rows() == 0)\n@@ -343,7 +348,8 @@ DistributedBlockOutputStream::runWritingJob(DistributedBlockOutputStream::JobRep\n                 if (throttler)\n                     job.connection_entry->setThrottler(throttler);\n \n-                job.stream = std::make_shared<RemoteBlockOutputStream>(*job.connection_entry, timeouts, query_string, settings, context.getClientInfo());\n+                job.stream = std::make_shared<RemoteBlockOutputStream>(\n+                    *job.connection_entry, timeouts, query_string, settings, context->getClientInfo());\n                 job.stream->writePrefix();\n             }\n \n@@ -357,7 +363,7 @@ DistributedBlockOutputStream::runWritingJob(DistributedBlockOutputStream::JobRep\n             if (!job.stream)\n             {\n                 /// Forward user settings\n-                job.local_context = std::make_unique<Context>(context);\n+                job.local_context = std::make_unique<Context>(*context);\n \n                 /// Copying of the query AST is required to avoid race,\n                 /// in case of INSERT into multiple local shards.\n@@ -385,7 +391,7 @@ DistributedBlockOutputStream::runWritingJob(DistributedBlockOutputStream::JobRep\n \n void DistributedBlockOutputStream::writeSync(const Block & block)\n {\n-    const Settings & settings = context.getSettingsRef();\n+    const Settings & settings = context->getSettingsRef();\n     const auto & shards_info = cluster->getShardsInfo();\n     bool random_shard_insert = settings.insert_distributed_one_random_shard && !storage.has_sharding_key;\n     size_t start = 0;\n@@ -562,7 +568,7 @@ void DistributedBlockOutputStream::writeSplitAsync(const Block & block)\n void DistributedBlockOutputStream::writeAsyncImpl(const Block & block, size_t shard_id)\n {\n     const auto & shard_info = cluster->getShardsInfo()[shard_id];\n-    const auto & settings = context.getSettingsRef();\n+    const auto & settings = context->getSettingsRef();\n \n     if (shard_info.hasInternalReplication())\n     {\n@@ -598,7 +604,7 @@ void DistributedBlockOutputStream::writeAsyncImpl(const Block & block, size_t sh\n void DistributedBlockOutputStream::writeToLocal(const Block & block, size_t repeats)\n {\n     /// Async insert does not support settings forwarding yet whereas sync one supports\n-    InterpreterInsertQuery interp(query_ast, context);\n+    InterpreterInsertQuery interp(query_ast, *context);\n \n     auto block_io = interp.execute();\n \n@@ -610,7 +616,7 @@ void DistributedBlockOutputStream::writeToLocal(const Block & block, size_t repe\n \n void DistributedBlockOutputStream::writeToShard(const Block & block, const std::vector<std::string> & dir_names)\n {\n-    const auto & settings = context.getSettingsRef();\n+    const auto & settings = context->getSettingsRef();\n     const auto & distributed_settings = storage.getDistributedSettingsRef();\n \n     bool fsync = distributed_settings.fsync_after_insert;\n@@ -675,8 +681,8 @@ void DistributedBlockOutputStream::writeToShard(const Block & block, const std::\n             WriteBufferFromOwnString header_buf;\n             writeVarUInt(DBMS_TCP_PROTOCOL_VERSION, header_buf);\n             writeStringBinary(query_string, header_buf);\n-            context.getSettingsRef().write(header_buf);\n-            context.getClientInfo().write(header_buf, DBMS_TCP_PROTOCOL_VERSION);\n+            context->getSettingsRef().write(header_buf);\n+            context->getClientInfo().write(header_buf, DBMS_TCP_PROTOCOL_VERSION);\n             writeVarUInt(block.rows(), header_buf);\n             writeVarUInt(block.bytes(), header_buf);\n             writeStringBinary(block.cloneEmpty().dumpStructure(), header_buf);\n@@ -724,7 +730,7 @@ void DistributedBlockOutputStream::writeToShard(const Block & block, const std::\n     Poco::File(first_file_tmp_path).remove();\n \n     /// Notify\n-    auto sleep_ms = context.getSettingsRef().distributed_directory_monitor_sleep_time_ms;\n+    auto sleep_ms = context->getSettingsRef().distributed_directory_monitor_sleep_time_ms;\n     for (const auto & dir_name : dir_names)\n     {\n         auto & directory_monitor = storage.requireDirectoryMonitor(disk, dir_name);\n@@ -732,5 +738,4 @@ void DistributedBlockOutputStream::writeToShard(const Block & block, const std::\n     }\n }\n \n-\n }\ndiff --git a/src/Storages/Distributed/DistributedBlockOutputStream.h b/src/Storages/Distributed/DistributedBlockOutputStream.h\nindex ca57ad46fbb2..8a1cef43f44c 100644\n--- a/src/Storages/Distributed/DistributedBlockOutputStream.h\n+++ b/src/Storages/Distributed/DistributedBlockOutputStream.h\n@@ -84,7 +84,7 @@ class DistributedBlockOutputStream : public IBlockOutputStream\n     std::string getCurrentStateDescription();\n \n private:\n-    const Context & context;\n+    std::unique_ptr<Context> context;\n     StorageDistributed & storage;\n     StorageMetadataPtr metadata_snapshot;\n     ASTPtr query_ast;\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/00987_distributed_stack_overflow.sql b/tests/queries/0_stateless/00987_distributed_stack_overflow.sql\nindex 4baa6969b310..d2e2b8f37ef6 100644\n--- a/tests/queries/0_stateless/00987_distributed_stack_overflow.sql\n+++ b/tests/queries/0_stateless/00987_distributed_stack_overflow.sql\n@@ -5,13 +5,13 @@ DROP TABLE IF EXISTS distr2;\n CREATE TABLE distr (x UInt8) ENGINE = Distributed(test_shard_localhost, currentDatabase(), distr); -- { serverError 269 }\n \n CREATE TABLE distr0 (x UInt8) ENGINE = Distributed(test_shard_localhost, '', distr0);\n-SELECT * FROM distr0; -- { serverError 306 }\n+SELECT * FROM distr0; -- { serverError 581 }\n \n CREATE TABLE distr1 (x UInt8) ENGINE = Distributed(test_shard_localhost, currentDatabase(), distr2);\n CREATE TABLE distr2 (x UInt8) ENGINE = Distributed(test_shard_localhost, currentDatabase(), distr1);\n \n-SELECT * FROM distr1; -- { serverError 306 }\n-SELECT * FROM distr2; -- { serverError 306 }\n+SELECT * FROM distr1; -- { serverError 581 }\n+SELECT * FROM distr2; -- { serverError 581 }\n \n DROP TABLE distr0;\n DROP TABLE distr1;\ndiff --git a/tests/queries/0_stateless/01370_client_autocomplete_word_break_characters.expect b/tests/queries/0_stateless/01370_client_autocomplete_word_break_characters.expect\nindex 50ef009dee9e..a6d52b399187 100755\n--- a/tests/queries/0_stateless/01370_client_autocomplete_word_break_characters.expect\n+++ b/tests/queries/0_stateless/01370_client_autocomplete_word_break_characters.expect\n@@ -23,7 +23,7 @@ set is_done 0\n while {$is_done == 0} {\n     send -- \"\\t\"\n     expect {\n-        \"_connections\" {\n+        \"_\" {\n             set is_done 1\n         }\n         default {\ndiff --git a/tests/queries/0_stateless/01763_max_distributed_depth.reference b/tests/queries/0_stateless/01763_max_distributed_depth.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/01763_max_distributed_depth.sql b/tests/queries/0_stateless/01763_max_distributed_depth.sql\nnew file mode 100644\nindex 000000000000..d1bb9e4be90d\n--- /dev/null\n+++ b/tests/queries/0_stateless/01763_max_distributed_depth.sql\n@@ -0,0 +1,26 @@\n+DROP TABLE IF EXISTS tt6;\n+\n+CREATE TABLE tt6\n+(\n+\t`id` UInt32,\n+\t`first_column` UInt32,\n+\t`second_column` UInt32,\n+\t`third_column` UInt32,\n+\t`status` String\n+\n+)\n+ENGINE = Distributed('test_shard_localhost', '', 'tt6', rand());\n+\n+INSERT INTO tt6 VALUES (1, 1, 1, 1, 'ok'); -- { serverError 581 }\n+\n+SELECT * FROM tt6; -- { serverError 581 }\n+\n+SET max_distributed_depth = 0;\n+\n+-- stack overflow\n+INSERT INTO tt6 VALUES (1, 1, 1, 1, 'ok'); -- { serverError 306}\n+\n+-- stack overflow\n+SELECT * FROM tt6; -- { serverError 306 }\n+\n+DROP TABLE tt6;\n",
  "problem_statement": "Infinite loop of queries when a Distributed table looks at itself\n```\r\n        <test_segfault>\r\n            <shard>\r\n                <replica>\r\n                    <host>localhost</host>\r\n                    <port>9001</port>\r\n                </replica>\r\n            </shard>\r\n            <shard>\r\n                <replica>\r\n                    <host>localhost</host>\r\n                    <port>9002</port>\r\n                </replica>\r\n            </shard>\r\n            <shard>\r\n                <replica>\r\n                    <host>localhost</host>\r\n                    <port>9003</port>\r\n                </replica>\r\n            </shard>\r\n        </test_segfault>\r\n\r\nCREATE TABLE tt6 ON CLUSTER test_segfault\r\n(\r\n    `id` UInt32,\r\n    `first_column` UInt32,\r\n    `second_column` UInt32,\r\n    `third_column` UInt32,\r\n    `fourth_column` Decimal(9, 2),\r\n    `created_at` DateTime CODEC(DoubleDelta),\r\n    `status` String\r\n)\r\nENGINE = Distributed('test_segfault', '', 'tt6', rand())\r\n\r\nselect * from tt6 where fourth_column=0 limit 100000000, 100;\r\n```\r\n\r\nThe query errors out, but the servers continue to send the queries in a loop, eventually causing an out-of-memory error.\n",
  "hints_text": "Let's add `distributed_depth` to `ClientInfo`, pass it from client to server, increment it on every query, and throw exception if it is larger than `max_distributed_depth` setting (default = 5).",
  "created_at": "2021-03-20T16:34:26Z",
  "modified_files": [
    "src/Common/ErrorCodes.cpp",
    "src/Core/Defines.h",
    "src/Core/Settings.h",
    "src/Interpreters/ClientInfo.cpp",
    "src/Interpreters/ClientInfo.h",
    "src/Interpreters/ClusterProxy/executeQuery.cpp",
    "src/Storages/Distributed/DistributedBlockOutputStream.cpp",
    "src/Storages/Distributed/DistributedBlockOutputStream.h"
  ],
  "modified_test_files": [
    "tests/queries/0_stateless/00987_distributed_stack_overflow.sql",
    "tests/queries/0_stateless/01370_client_autocomplete_word_break_characters.expect",
    "b/tests/queries/0_stateless/01763_max_distributed_depth.sql"
  ]
}