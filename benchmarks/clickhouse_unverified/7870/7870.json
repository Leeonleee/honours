{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 7870,
  "instance_id": "ClickHouse__ClickHouse-7870",
  "issue_numbers": [
    "7817"
  ],
  "base_commit": "ea2b5b8c94aefbcd586c9c698f48fdbf40ae468e",
  "patch": "diff --git a/dbms/src/IO/PeekableReadBuffer.cpp b/dbms/src/IO/PeekableReadBuffer.cpp\nindex eff935a9de5f..7c60bb252ad2 100644\n--- a/dbms/src/IO/PeekableReadBuffer.cpp\n+++ b/dbms/src/IO/PeekableReadBuffer.cpp\n@@ -19,7 +19,6 @@ bool PeekableReadBuffer::peekNext()\n {\n     checkStateCorrect();\n \n-    size_t bytes_read = 0;\n     Position copy_from = pos;\n     size_t bytes_to_copy = sub_buf.available();\n     if (useSubbufferOnly())\n@@ -27,11 +26,9 @@ bool PeekableReadBuffer::peekNext()\n         /// Don't have to copy all data from sub-buffer if there is no data in own memory (checkpoint and pos are in sub-buffer)\n         if (checkpoint)\n             copy_from = checkpoint;\n-        bytes_read = copy_from - sub_buf.buffer().begin();\n         bytes_to_copy = sub_buf.buffer().end() - copy_from;\n         if (!bytes_to_copy)\n         {\n-            bytes += bytes_read;\n             sub_buf.position() = copy_from;\n \n             /// Both checkpoint and pos are at the end of sub-buffer. Just load next part of data.\n@@ -50,7 +47,6 @@ bool PeekableReadBuffer::peekNext()\n \n     if (useSubbufferOnly())\n     {\n-        bytes += bytes_read;\n         sub_buf.position() = copy_from;\n     }\n \n@@ -198,7 +194,6 @@ void PeekableReadBuffer::resizeOwnMemoryIfNecessary(size_t bytes_to_append)\n             /// Move unread data to the beginning of own memory instead of resize own memory\n             peeked_size -= offset;\n             memmove(memory.data(), memory.data() + offset, peeked_size);\n-            bytes += offset;\n \n             if (need_update_checkpoint)\n                 checkpoint -= offset;\n",
  "test_patch": "diff --git a/dbms/tests/queries/0_stateless/01034_values_parse_float_bug.reference b/dbms/tests/queries/0_stateless/01034_values_parse_float_bug.reference\nnew file mode 100644\nindex 000000000000..ae7a0f09264c\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/01034_values_parse_float_bug.reference\n@@ -0,0 +1,1 @@\n+-160.32605\t37.705841\ndiff --git a/dbms/tests/queries/0_stateless/01034_values_parse_float_bug.sh b/dbms/tests/queries/0_stateless/01034_values_parse_float_bug.sh\nnew file mode 100755\nindex 000000000000..8e06e126f563\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/01034_values_parse_float_bug.sh\n@@ -0,0 +1,16 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+. $CURDIR/../shell_config.sh\n+\n+\n+${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS values_floats\"\n+\n+${CLICKHOUSE_CLIENT} --query=\"CREATE TABLE values_floats (a Float32, b Float64) ENGINE = Memory\"\n+\n+${CLICKHOUSE_CLIENT} --query=\"SELECT '(-160.32605134916085,37.70584056842162),' FROM numbers(1000000)\" | ${CLICKHOUSE_CLIENT} --query=\"INSERT INTO values_floats FORMAT Values\"\n+\n+${CLICKHOUSE_CLIENT} --query=\"SELECT DISTINCT round(a, 6), round(b, 6) FROM values_floats\"\n+\n+${CLICKHOUSE_CLIENT} --query=\"DROP TABLE IF EXISTS values_floats\"\n+\n",
  "problem_statement": "Float values are sometimes altered\nWhen inserting Float32 or Float64 values over HTTP, I noticed that **_sometimes_** their decimal part is missing, or divided by 10000 or 100000, or the float becames `inf`. This is not systematic and happens only for a few rows (11 out of 500000 in the example below).\r\nThe same thing doesn't happen when inserting the same data from a CSV file. \r\n\r\nClickHouse server version : 19.16.2.2, also reproduced with 19.17.3.7.\r\n\r\n**To reproduce:**\r\n- Create Tables\r\n```sql\r\nDROP TABLE IF EXISTS strange_bug_http;\r\nCREATE TABLE strange_bug_http\r\n(\r\n    `row_id` Int32, \r\n    `longitude` Float32 CODEC(Gorilla, ZSTD), \r\n    `latitude` Float32 CODEC(Gorilla, ZSTD), \r\n    `geohash` FixedString(3) CODEC(ZSTD)\r\n)\r\nENGINE = MergeTree()\r\nORDER BY row_id;\r\n\r\n// for comparison\r\nDROP TABLE IF EXISTS strange_bug_csv;\r\nCREATE TABLE strange_bug_csv\r\n(\r\n    `row_id` Int32, \r\n    `longitude` Float32 CODEC(Gorilla, ZSTD), \r\n    `latitude` Float32 CODEC(Gorilla, ZSTD), \r\n    `geohash` FixedString(3) CODEC(ZSTD)\r\n)\r\nENGINE = MergeTree()\r\nORDER BY row_id;\r\n```\r\n- Generate data :\r\n\r\nYou can download the generated files from [here](https://github.com/yamrzou/playground/tree/master/ClickHouse/7817).\r\n```python\r\n# pip install python-geohash\r\n# pip install numpy\r\nimport numpy as np\r\nimport geohash\r\n\r\nNUM_POINTS = 500_000\r\n\r\nlongitude = np.random.rand(NUM_POINTS) * 360 - 180\r\nlatitude = np.random.rand(NUM_POINTS) * 180 - 90\r\n@np.vectorize \r\ndef geohashEncode(longitude, latitude, precision=3):\r\n    return geohash.encode(latitude, longitude, precision)\r\ngeohash = geohashEncode(longitude, latitude)\r\n\r\ndata = b','.join([b'(' + b','.join(map(lambda x: repr(x).encode(),(i,)+tup)) + b')' for i,tup in enumerate(zip(longitude, latitude, geohash))])\r\n\r\nwith open('floats.bin', 'wb') as f: \r\n    f.write(data)\r\n\r\ncsv = '\\n'.join([','.join(map(lambda x: repr(x),(i,)+tup)) for i,tup in enumerate(zip(longitude, latitude, geohash))])\r\nwith open('floats.csv', 'w') as f: \r\n    f.write(csv)\r\n```\r\n- Insert over HTTP:\r\n\r\n```python\r\n# pip install uvloop\r\n# pip install aiohttp\r\nimport asyncio\r\nimport uvloop\r\nimport aiohttp\r\n\r\nwith open('floats.bin', 'rb')  as f: \r\n    data = f.read()\r\n\r\nuvloop.install()\r\nloop = asyncio.get_event_loop()\r\n\r\nparams = {\"database\":\"default\", \"query\": \"INSERT INTO strange_bug_http VALUES\"}\r\nasync def main(data, params, url=\"http://localhost:8123/\"):\r\n    async with aiohttp.ClientSession() as session:\r\n        async with session.post(url=url, params=params, data=data) as resp:\r\n            return await resp.read()\r\n\r\nloop.run_until_complete(main(data, params))\r\n```\r\n- Insert from CSV:\r\n```bash\r\ncat floats.csv | clickhouse-client --query=\"INSERT INTO strange_bug_csv FORMAT CSV\"\r\n```\r\n- Perform the following query on both tables \r\n```sql\r\nSELECT \r\n    row_id + 1, \r\n    longitude, \r\n    latitude, \r\n    geohashEncode(longitude, latitude, 3) AS computed_geohash, \r\n    geohash\r\nFROM [strange_bug_csv/strange_bug_http]\r\nWHERE computed_geohash != geohash;\r\n```\r\nOn the table filled from CSV, it retrurns one row, which is probably due to geohash miscalculation/float precision :\r\n```\r\n\u250c\u2500plus(row_id, 1)\u2500\u252c\u2500longitude\u2500\u252c\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502          360307 \u2502 -171.5625 \u2502 -81.95313 \u2502 01q              \u2502 01m     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nOn the table filled via HTTP, it retrurns the following :\r\n```\r\n\u2500plus(row_id, 1)\u2500\u252c\u2500\u2500longitude\u2500\u252c\u2500\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502          201272 \u2502 -95.921555 \u2502         52 \u2502 cc5              \u2502 cc7     \u2502\r\n\u2502          241104 \u2502 -14.000025 \u2502  35.987846 \u2502 ewq              \u2502 ewm     \u2502\r\n\u2502          261022 \u2502   7.000061 \u2502 -69.146805 \u2502 h5s              \u2502 h5t     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500plus(row_id, 1)\u2500\u252c\u2500\u2500longitude\u2500\u252c\u2500\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502           81429 \u2502  174.77354 \u2502  49.000008 \u2502 zbs              \u2502 zbu     \u2502\r\n\u2502          101695 \u2502   35.81316 \u2502        inf \u2502 uzc              \u2502 uu9     \u2502\r\n\u2502          121610 \u2502 -109.74762 \u2502 -28.000027 \u2502 3e1              \u2502 3dc     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500plus(row_id, 1)\u2500\u252c\u2500longitude\u2500\u252c\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502          300857 \u2502 157.00002 \u2502 -69.10218 \u2502 p7x              \u2502 pe8     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500plus(row_id, 1)\u2500\u252c\u2500longitude\u2500\u252c\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502          360307 \u2502 -171.5625 \u2502 -81.95313 \u2502 01q              \u2502 01m     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500plus(row_id, 1)\u2500\u252c\u2500longitude\u2500\u252c\u2500\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502          360603 \u2502      -171 \u2502 -56.861885 \u2502 0jy              \u2502 0jv     \u2502\r\n\u2502          380518 \u2502 119.77515 \u2502 -46.000004 \u2502 nxv              \u2502 nxt     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500plus(row_id, 1)\u2500\u252c\u2500\u2500longitude\u2500\u252c\u2500\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502          420349 \u2502 -113.02339 \u2502 -11.000004 \u2502 3qp              \u2502 3mz     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500plus(row_id, 1)\u2500\u252c\u2500longitude\u2500\u252c\u2500\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502          480089 \u2502 -177.2999 \u2502 -42.000057 \u2502 209              \u2502 203     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nFor instance, the second and third rows in the results have as longitude `-14.000025` and `7.000061` respectively, which is different from the inserted values `-14.250967871479986` and `7.608598672771308`.\r\n\r\nChecking the CSV data :\r\n```shell\r\n$ sed -n 241104p floats.csv\r\n241103, -14.250967871479986, 35.98784501339779, 'ewm'\r\n\r\n$ sed -n 261022p floats.csv\r\n261021, 7.608598672771308, -69.14680134387808, 'h5t'\r\n```\r\nChecking the CSV table :\r\n```sql\r\nSELECT \r\n    row_id + 1, \r\n    longitude, \r\n    latitude, \r\n    geohashEncode(longitude, latitude, 3) AS computed_geohash, \r\n    geohash\r\nFROM strange_bug_csv\r\nWHERE (row_id + 1) IN (241104, 261022);\r\n\r\n\u250c\u2500plus(row_id, 1)\u2500\u252c\u2500\u2500longitude\u2500\u252c\u2500\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502          241104 \u2502 -14.250968 \u2502  35.987846 \u2502 ewm              \u2502 ewm     \u2502\r\n\u2502          261022 \u2502  7.6085987 \u2502 -69.146805 \u2502 h5t              \u2502 h5t     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nChecking the binary data :\r\n```python\r\ndef get_binary_row(row_id, data):\r\n    p = data.find(\"({}\".format(row_id).encode())\r\n    return data[p : p + data[p:].find(b')') + 1]\r\n\r\n>>> get_binary_row(241104-1, data)\r\nb\"(241103,-14.250967871479986,35.98784501339779,'ewm')\"\r\n>>> get_binary_row(261022-1, data)\r\nb\"(261021,7.608598672771308,-69.14680134387808,'h5t')\"\r\n```\r\n\r\nAny workaround is much appreciated.\r\nThank you.\r\n\n",
  "hints_text": "Not related to HTTP. \r\n\r\n cat f.values |clickhouse-client -q \"INSERT INTO strange_bug_val FORMAT Values\"\n```\r\nCREATE TABLE strange_bug_val\r\n(   a Float32, \r\n    b Float32)\r\nENGINE = Memory;\r\n\r\nclickhouse-client -q \"select '(-160.32605134916085,37.70584056842162),' from numbers(1000000)\" >test.values\r\ncat test.values |clickhouse-client -q \"INSERT INTO strange_bug_val FORMAT Values\"\r\n\r\nSELECT DISTINCT *\r\nFROM strange_bug_val\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500a\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500b\u2500\u2510\r\n\u2502 -160.32605 \u2502  37.70584 \u2502\r\n\u2502 -160.32605 \u2502 37.000008 \u2502\r\n\u2502       -160 \u2502  37.70584 \u2502\r\n\u2502 -160.32605 \u2502       inf \u2502\r\n\u2502       -inf \u2502  37.70584 \u2502\r\n\u2502 -160.32605 \u2502        37 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\nReproduced with 19.18.1.1667\r\nNo issue with 19.15.5.18\r\n\n@den-crane Thanks for the concise example.\r\n\r\nIt seems that with less decimals in the input, wrong values such as `37.000008` disappear, but `inf` and values without decimals persist.\nBTW, about CSV, the cause is FLOAT32 vs FLOAT64\r\n```\r\n\u250c\u2500plus(row_id, 1)\u2500\u252c\u2500longitude\u2500\u252c\u2500\u2500latitude\u2500\u252c\u2500computed_geohash\u2500\u252c\u2500geohash\u2500\u2510\r\n\u2502          360307 \u2502 -171.5625 \u2502 -81.95313 \u2502 01q              \u2502 01m     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nselect geohashEncode(cast(-171.5625053737408 as Float64) ,cast(-81.95313575217848 as Float64)) computed_geohash\r\n\r\n\u250c\u2500computed_geohash\u2500\u2510\r\n\u2502 01mvzbzuru5y     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect geohashEncode(cast(-171.5625053737408 as Float32) ,cast(-81.95313575217848 as Float32)) computed_geohash\r\n\u250c\u2500computed_geohash\u2500\u2510\r\n\u2502 01qjb0bh2hb1     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\n\nI have also encountered this  problem, my workaround is: \r\n\r\n- Create a tmp table and change all `Float32` and `Float64` columns to `String`\r\n- Load data to the tmp table\r\n- Insert rows from tmp table, converting `String` values to Float (using `toFloat64OrNull` or other functions)",
  "created_at": "2019-11-20T22:12:41Z",
  "modified_files": [
    "dbms/src/IO/PeekableReadBuffer.cpp"
  ],
  "modified_test_files": [
    "b/dbms/tests/queries/0_stateless/01034_values_parse_float_bug.reference",
    "b/dbms/tests/queries/0_stateless/01034_values_parse_float_bug.sh"
  ]
}