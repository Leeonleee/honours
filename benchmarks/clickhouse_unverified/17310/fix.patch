diff --git a/src/Columns/ColumnMap.h b/src/Columns/ColumnMap.h
index 9f1410eefe4d..c1948491db59 100644
--- a/src/Columns/ColumnMap.h
+++ b/src/Columns/ColumnMap.h
@@ -86,6 +86,9 @@ class ColumnMap final : public COWHelper<IColumn, ColumnMap>
     const ColumnArray & getNestedColumn() const { return assert_cast<const ColumnArray &>(*nested); }
     ColumnArray & getNestedColumn() { return assert_cast<ColumnArray &>(*nested); }
 
+    const ColumnPtr & getNestedColumnPtr() const { return nested; }
+    ColumnPtr & getNestedColumnPtr() { return nested; }
+
     const ColumnTuple & getNestedData() const { return assert_cast<const ColumnTuple &>(getNestedColumn().getData()); }
     ColumnTuple & getNestedData() { return assert_cast<ColumnTuple &>(getNestedColumn().getData()); }
 };
diff --git a/src/Columns/ColumnNullable.h b/src/Columns/ColumnNullable.h
index 8a17f6573409..ade2c1066275 100644
--- a/src/Columns/ColumnNullable.h
+++ b/src/Columns/ColumnNullable.h
@@ -143,9 +143,11 @@ class ColumnNullable final : public COWHelper<IColumn, ColumnNullable>
     const IColumn & getNestedColumn() const { return *nested_column; }
 
     const ColumnPtr & getNestedColumnPtr() const { return nested_column; }
+    ColumnPtr & getNestedColumnPtr() { return nested_column; }
 
     /// Return the column that represents the byte map.
     const ColumnPtr & getNullMapColumnPtr() const { return null_map; }
+    ColumnPtr & getNullMapColumnPtr() { return null_map; }
 
     ColumnUInt8 & getNullMapColumn() { return assert_cast<ColumnUInt8 &>(*null_map); }
     const ColumnUInt8 & getNullMapColumn() const { return assert_cast<const ColumnUInt8 &>(*null_map); }
diff --git a/src/Columns/ColumnTuple.h b/src/Columns/ColumnTuple.h
index 68b502f97052..f763ca3fcba9 100644
--- a/src/Columns/ColumnTuple.h
+++ b/src/Columns/ColumnTuple.h
@@ -99,6 +99,7 @@ class ColumnTuple final : public COWHelper<IColumn, ColumnTuple>
     Columns getColumnsCopy() const { return {columns.begin(), columns.end()}; }
 
     const ColumnPtr & getColumnPtr(size_t idx) const { return columns[idx]; }
+    ColumnPtr & getColumnPtr(size_t idx) { return columns[idx]; }
 
 private:
     int compareAtImpl(size_t n, size_t m, const IColumn & rhs, int nan_direction_hint, const Collator * collator=nullptr) const;
diff --git a/src/Core/NamesAndTypes.cpp b/src/Core/NamesAndTypes.cpp
index 3a55a4328a74..e96ce1824d2e 100644
--- a/src/Core/NamesAndTypes.cpp
+++ b/src/Core/NamesAndTypes.cpp
@@ -17,6 +17,29 @@ namespace ErrorCodes
     extern const int THERE_IS_NO_COLUMN;
 }
 
+NameAndTypePair::NameAndTypePair(
+    const String & name_in_storage_, const String & subcolumn_name_,
+    const DataTypePtr & type_in_storage_, const DataTypePtr & subcolumn_type_)
+    : name(name_in_storage_ + (subcolumn_name_.empty() ? "" : "." + subcolumn_name_))
+    , type(subcolumn_type_)
+    , type_in_storage(type_in_storage_)
+    , subcolumn_delimiter_position(name_in_storage_.size()) {}
+
+String NameAndTypePair::getNameInStorage() const
+{
+    if (!subcolumn_delimiter_position)
+        return name;
+
+    return name.substr(0, *subcolumn_delimiter_position);
+}
+
+String NameAndTypePair::getSubcolumnName() const
+{
+    if (!subcolumn_delimiter_position)
+        return "";
+
+    return name.substr(*subcolumn_delimiter_position + 1, name.size() - *subcolumn_delimiter_position);
+}
 
 void NamesAndTypesList::readText(ReadBuffer & buf)
 {
@@ -137,25 +160,20 @@ NamesAndTypesList NamesAndTypesList::filter(const Names & names) const
 
 NamesAndTypesList NamesAndTypesList::addTypes(const Names & names) const
 {
-    /// NOTE: It's better to make a map in `IStorage` than to create it here every time again.
-#if !defined(ARCADIA_BUILD)
-    google::dense_hash_map<StringRef, const DataTypePtr *, StringRefHash> types;
-#else
-    google::sparsehash::dense_hash_map<StringRef, const DataTypePtr *, StringRefHash> types;
-#endif
-    types.set_empty_key(StringRef());
+    std::unordered_map<std::string_view, const NameAndTypePair *> self_columns;
 
-    for (const NameAndTypePair & column : *this)
-        types[column.name] = &column.type;
+    for (const auto & column : *this)
+        self_columns[column.name] = &column;
 
     NamesAndTypesList res;
     for (const String & name : names)
     {
-        auto it = types.find(name);
-        if (it == types.end())
+        auto it = self_columns.find(name);
+        if (it == self_columns.end())
             throw Exception("No column " + name, ErrorCodes::THERE_IS_NO_COLUMN);
-        res.emplace_back(name, *it->second);
+        res.emplace_back(*it->second);
     }
+
     return res;
 }
 
diff --git a/src/Core/NamesAndTypes.h b/src/Core/NamesAndTypes.h
index 28567fed3e30..dad031a543c0 100644
--- a/src/Core/NamesAndTypes.h
+++ b/src/Core/NamesAndTypes.h
@@ -15,11 +15,19 @@ namespace DB
 
 struct NameAndTypePair
 {
-    String name;
-    DataTypePtr type;
+public:
+    NameAndTypePair() = default;
+    NameAndTypePair(const String & name_, const DataTypePtr & type_)
+        : name(name_), type(type_), type_in_storage(type_) {}
+
+    NameAndTypePair(const String & name_in_storage_, const String & subcolumn_name_,
+        const DataTypePtr & type_in_storage_, const DataTypePtr & subcolumn_type_);
 
-    NameAndTypePair() {}
-    NameAndTypePair(const String & name_, const DataTypePtr & type_) : name(name_), type(type_) {}
+    String getNameInStorage() const;
+    String getSubcolumnName() const;
+
+    bool isSubcolumn() const { return subcolumn_delimiter_position != std::nullopt; }
+    DataTypePtr getTypeInStorage() const { return type_in_storage; }
 
     bool operator<(const NameAndTypePair & rhs) const
     {
@@ -30,8 +38,26 @@ struct NameAndTypePair
     {
         return name == rhs.name && type->equals(*rhs.type);
     }
+
+    String name;
+    DataTypePtr type;
+
+private:
+    DataTypePtr type_in_storage;
+    std::optional<size_t> subcolumn_delimiter_position;
 };
 
+/// This needed to use structured bindings for NameAndTypePair
+/// const auto & [name, type] = name_and_type
+template <int I>
+decltype(auto) get(const NameAndTypePair & name_and_type)
+{
+    if constexpr (I == 0)
+        return name_and_type.name;
+    else if constexpr (I == 1)
+        return name_and_type.type;
+}
+
 using NamesAndTypes = std::vector<NameAndTypePair>;
 
 class NamesAndTypesList : public std::list<NameAndTypePair>
@@ -81,3 +107,10 @@ class NamesAndTypesList : public std::list<NameAndTypePair>
 };
 
 }
+
+namespace std
+{
+    template <> struct tuple_size<DB::NameAndTypePair> : std::integral_constant<size_t, 2> {};
+    template <> struct tuple_element<0, DB::NameAndTypePair> { using type = DB::String; };
+    template <> struct tuple_element<1, DB::NameAndTypePair> { using type = DB::DataTypePtr; };
+}
diff --git a/src/Core/Settings.h b/src/Core/Settings.h
index 5368b9fb2106..1e4b07997ab5 100644
--- a/src/Core/Settings.h
+++ b/src/Core/Settings.h
@@ -405,6 +405,7 @@ class IColumn;
     M(Bool, allow_non_metadata_alters, true, "Allow to execute alters which affects not only tables metadata, but also data on disk", 0) \
     M(Bool, enable_global_with_statement, false, "Propagate WITH statements to UNION queries and all subqueries", 0) \
     M(Bool, aggregate_functions_null_for_empty, false, "Rewrite all aggregate functions in a query, adding -OrNull suffix to them", 0) \
+    M(Bool, flatten_nested, true, "If true, columns of type Nested will be flatten to separate array columns instead of one array of tuples", 0) \
     M(Bool, asterisk_include_materialized_columns, false, "Include MATERIALIZED columns for wildcard query", 0) \
     M(Bool, asterisk_include_alias_columns, false, "Include ALIAS columns for wildcard query", 0) \
     M(Bool, optimize_skip_merged_partitions, false, "Skip partitions with one part with level > 0 in optimize final", 0) \
diff --git a/src/DataStreams/NativeBlockInputStream.cpp b/src/DataStreams/NativeBlockInputStream.cpp
index b182d5e05882..377f44514195 100644
--- a/src/DataStreams/NativeBlockInputStream.cpp
+++ b/src/DataStreams/NativeBlockInputStream.cpp
@@ -71,7 +71,7 @@ void NativeBlockInputStream::resetParser()
     is_killed.store(false);
 }
 
-void NativeBlockInputStream::readData(const IDataType & type, IColumn & column, ReadBuffer & istr, size_t rows, double avg_value_size_hint)
+void NativeBlockInputStream::readData(const IDataType & type, ColumnPtr & column, ReadBuffer & istr, size_t rows, double avg_value_size_hint)
 {
     IDataType::DeserializeBinaryBulkSettings settings;
     settings.getter = [&](IDataType::SubstreamPath) -> ReadBuffer * { return &istr; };
@@ -82,8 +82,8 @@ void NativeBlockInputStream::readData(const IDataType & type, IColumn & column,
     type.deserializeBinaryBulkStatePrefix(settings, state);
     type.deserializeBinaryBulkWithMultipleStreams(column, rows, settings, state);
 
-    if (column.size() != rows)
-        throw Exception("Cannot read all data in NativeBlockInputStream. Rows read: " + toString(column.size()) + ". Rows expected: " + toString(rows) + ".",
+    if (column->size() != rows)
+        throw Exception("Cannot read all data in NativeBlockInputStream. Rows read: " + toString(column->size()) + ". Rows expected: " + toString(rows) + ".",
             ErrorCodes::CANNOT_READ_ALL_DATA);
 }
 
@@ -158,11 +158,11 @@ Block NativeBlockInputStream::readImpl()
         }
 
         /// Data
-        MutableColumnPtr read_column = column.type->createColumn();
+        ColumnPtr read_column = column.type->createColumn();
 
         double avg_value_size_hint = avg_value_size_hints.empty() ? 0 : avg_value_size_hints[i];
         if (rows)    /// If no rows, nothing to read.
-            readData(*column.type, *read_column, istr, rows, avg_value_size_hint);
+            readData(*column.type, read_column, istr, rows, avg_value_size_hint);
 
         column.column = std::move(read_column);
 
diff --git a/src/DataStreams/NativeBlockInputStream.h b/src/DataStreams/NativeBlockInputStream.h
index 774a1cfa1cd7..8f3d2843e0f9 100644
--- a/src/DataStreams/NativeBlockInputStream.h
+++ b/src/DataStreams/NativeBlockInputStream.h
@@ -74,7 +74,7 @@ class NativeBlockInputStream : public IBlockInputStream
 
     String getName() const override { return "Native"; }
 
-    static void readData(const IDataType & type, IColumn & column, ReadBuffer & istr, size_t rows, double avg_value_size_hint);
+    static void readData(const IDataType & type, ColumnPtr & column, ReadBuffer & istr, size_t rows, double avg_value_size_hint);
 
     Block getHeader() const override;
 
diff --git a/src/DataTypes/DataTypeArray.cpp b/src/DataTypes/DataTypeArray.cpp
index 9cd56d0e2b5d..3ad84a8fcd77 100644
--- a/src/DataTypes/DataTypeArray.cpp
+++ b/src/DataTypes/DataTypeArray.cpp
@@ -10,12 +10,15 @@
 #include <DataTypes/DataTypesNumber.h>
 #include <DataTypes/DataTypeArray.h>
 #include <DataTypes/DataTypeFactory.h>
+#include <DataTypes/DataTypeOneElementTuple.h>
 
 #include <Parsers/IAST.h>
 
 #include <Common/typeid_cast.h>
 #include <Common/assert_cast.h>
 
+#include <Core/NamesAndTypes.h>
+
 
 namespace DB
 {
@@ -145,10 +148,57 @@ namespace
 
         offset_values.resize(i);
     }
+
+    ColumnPtr arrayOffsetsToSizes(const IColumn & column)
+    {
+        const auto & column_offsets = assert_cast<const ColumnArray::ColumnOffsets &>(column);
+        MutableColumnPtr column_sizes = column_offsets.cloneEmpty();
+
+        if (column_offsets.empty())
+            return column_sizes;
+
+        const auto & offsets_data = column_offsets.getData();
+        auto & sizes_data = assert_cast<ColumnArray::ColumnOffsets &>(*column_sizes).getData();
+
+        sizes_data.resize(offsets_data.size());
+
+        IColumn::Offset prev_offset = 0;
+        for (size_t i = 0, size = offsets_data.size(); i < size; ++i)
+        {
+            auto current_offset = offsets_data[i];
+            sizes_data[i] = current_offset - prev_offset;
+            prev_offset =  current_offset;
+        }
+
+        return column_sizes;
+    }
+
+    ColumnPtr arraySizesToOffsets(const IColumn & column)
+    {
+        const auto & column_sizes = assert_cast<const ColumnArray::ColumnOffsets &>(column);
+        MutableColumnPtr column_offsets = column_sizes.cloneEmpty();
+
+        if (column_sizes.empty())
+            return column_offsets;
+
+        const auto & sizes_data = column_sizes.getData();
+        auto & offsets_data = assert_cast<ColumnArray::ColumnOffsets &>(*column_offsets).getData();
+
+        offsets_data.resize(sizes_data.size());
+
+        IColumn::Offset prev_offset = 0;
+        for (size_t i = 0, size = sizes_data.size(); i < size; ++i)
+        {
+            prev_offset += sizes_data[i];
+            offsets_data[i] = prev_offset;
+        }
+
+        return column_offsets;
+    }
 }
 
 
-void DataTypeArray::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const
+void DataTypeArray::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const
 {
     path.push_back(Substream::ArraySizes);
     callback(path, *this);
@@ -158,7 +208,7 @@ void DataTypeArray::enumerateStreams(const StreamCallback & callback, SubstreamP
 }
 
 
-void DataTypeArray::serializeBinaryBulkStatePrefix(
+void DataTypeArray::serializeBinaryBulkStatePrefixImpl(
     SerializeBinaryBulkSettings & settings,
     SerializeBinaryBulkStatePtr & state) const
 {
@@ -168,7 +218,7 @@ void DataTypeArray::serializeBinaryBulkStatePrefix(
 }
 
 
-void DataTypeArray::serializeBinaryBulkStateSuffix(
+void DataTypeArray::serializeBinaryBulkStateSuffixImpl(
     SerializeBinaryBulkSettings & settings,
     SerializeBinaryBulkStatePtr & state) const
 {
@@ -178,7 +228,7 @@ void DataTypeArray::serializeBinaryBulkStateSuffix(
 }
 
 
-void DataTypeArray::deserializeBinaryBulkStatePrefix(
+void DataTypeArray::deserializeBinaryBulkStatePrefixImpl(
     DeserializeBinaryBulkSettings & settings,
     DeserializeBinaryBulkStatePtr & state) const
 {
@@ -188,7 +238,7 @@ void DataTypeArray::deserializeBinaryBulkStatePrefix(
 }
 
 
-void DataTypeArray::serializeBinaryBulkWithMultipleStreams(
+void DataTypeArray::serializeBinaryBulkWithMultipleStreamsImpl(
     const IColumn & column,
     size_t offset,
     size_t limit,
@@ -235,44 +285,52 @@ void DataTypeArray::serializeBinaryBulkWithMultipleStreams(
 }
 
 
-void DataTypeArray::deserializeBinaryBulkWithMultipleStreams(
+void DataTypeArray::deserializeBinaryBulkWithMultipleStreamsImpl(
     IColumn & column,
     size_t limit,
     DeserializeBinaryBulkSettings & settings,
-    DeserializeBinaryBulkStatePtr & state) const
+    DeserializeBinaryBulkStatePtr & state,
+    SubstreamsCache * cache) const
 {
     ColumnArray & column_array = typeid_cast<ColumnArray &>(column);
-
     settings.path.push_back(Substream::ArraySizes);
-    if (auto * stream = settings.getter(settings.path))
+
+    if (auto cached_column = getFromSubstreamsCache(cache, settings.path))
+    {
+        column_array.getOffsetsPtr() = arraySizesToOffsets(*cached_column);
+    }
+    else if (auto * stream = settings.getter(settings.path))
     {
         if (settings.position_independent_encoding)
             deserializeArraySizesPositionIndependent(column, *stream, limit);
         else
             DataTypeNumber<ColumnArray::Offset>().deserializeBinaryBulk(column_array.getOffsetsColumn(), *stream, limit, 0);
+
+        addToSubstreamsCache(cache, settings.path, arrayOffsetsToSizes(column_array.getOffsetsColumn()));
     }
 
     settings.path.back() = Substream::ArrayElements;
 
     ColumnArray::Offsets & offset_values = column_array.getOffsets();
-    IColumn & nested_column = column_array.getData();
+    ColumnPtr & nested_column = column_array.getDataPtr();
 
     /// Number of values corresponding with `offset_values` must be read.
     size_t last_offset = offset_values.back();
-    if (last_offset < nested_column.size())
+    if (last_offset < nested_column->size())
         throw Exception("Nested column is longer than last offset", ErrorCodes::LOGICAL_ERROR);
-    size_t nested_limit = last_offset - nested_column.size();
+    size_t nested_limit = last_offset - nested_column->size();
 
     /// Adjust value size hint. Divide it to the average array size.
     settings.avg_value_size_hint = nested_limit ? settings.avg_value_size_hint / nested_limit * offset_values.size() : 0;
 
-    nested->deserializeBinaryBulkWithMultipleStreams(nested_column, nested_limit, settings, state);
+    nested->deserializeBinaryBulkWithMultipleStreams(nested_column, nested_limit, settings, state, cache);
+
     settings.path.pop_back();
 
     /// Check consistency between offsets and elements subcolumns.
     /// But if elements column is empty - it's ok for columns of Nested types that was added by ALTER.
-    if (!nested_column.empty() && nested_column.size() != last_offset)
-        throw ParsingException("Cannot read all array values: read just " + toString(nested_column.size()) + " of " + toString(last_offset),
+    if (!nested_column->empty() && nested_column->size() != last_offset)
+        throw ParsingException("Cannot read all array values: read just " + toString(nested_column->size()) + " of " + toString(last_offset),
             ErrorCodes::CANNOT_READ_ALL_DATA);
 }
 
@@ -530,6 +588,44 @@ bool DataTypeArray::equals(const IDataType & rhs) const
     return typeid(rhs) == typeid(*this) && nested->equals(*static_cast<const DataTypeArray &>(rhs).nested);
 }
 
+DataTypePtr DataTypeArray::tryGetSubcolumnType(const String & subcolumn_name) const
+{
+    return tryGetSubcolumnTypeImpl(subcolumn_name, 0);
+}
+
+DataTypePtr DataTypeArray::tryGetSubcolumnTypeImpl(const String & subcolumn_name, size_t level) const
+{
+    if (subcolumn_name == "size" + std::to_string(level))
+        return createOneElementTuple(std::make_shared<DataTypeUInt64>(), subcolumn_name, false);
+
+    DataTypePtr subcolumn;
+    if (const auto * nested_array = typeid_cast<const DataTypeArray *>(nested.get()))
+        subcolumn = nested_array->tryGetSubcolumnTypeImpl(subcolumn_name, level + 1);
+    else
+        subcolumn = nested->tryGetSubcolumnType(subcolumn_name);
+
+    return (subcolumn ? std::make_shared<DataTypeArray>(std::move(subcolumn)) : subcolumn);
+}
+
+ColumnPtr DataTypeArray::getSubcolumn(const String & subcolumn_name, const IColumn & column) const
+{
+    return getSubcolumnImpl(subcolumn_name, column, 0);
+}
+
+ColumnPtr DataTypeArray::getSubcolumnImpl(const String & subcolumn_name, const IColumn & column, size_t level) const
+{
+    const auto & column_array = assert_cast<const ColumnArray &>(column);
+    if (subcolumn_name == "size" + std::to_string(level))
+        return arrayOffsetsToSizes(column_array.getOffsetsColumn());
+
+    ColumnPtr subcolumn;
+    if (const auto * nested_array = typeid_cast<const DataTypeArray *>(nested.get()))
+        subcolumn = nested_array->getSubcolumnImpl(subcolumn_name, column_array.getData(), level + 1);
+    else
+        subcolumn = nested->getSubcolumn(subcolumn_name, column_array.getData());
+
+    return ColumnArray::create(subcolumn, column_array.getOffsetsPtr());
+}
 
 size_t DataTypeArray::getNumberOfDimensions() const
 {
diff --git a/src/DataTypes/DataTypeArray.h b/src/DataTypes/DataTypeArray.h
index 1451f27dfbe5..ba19ad021be3 100644
--- a/src/DataTypes/DataTypeArray.h
+++ b/src/DataTypes/DataTypeArray.h
@@ -57,32 +57,33 @@ class DataTypeArray final : public DataTypeWithSimpleSerialization
       * This is necessary, because when implementing nested structures, several arrays can have common sizes.
       */
 
-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;
+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;
 
-    void serializeBinaryBulkStatePrefix(
+    void serializeBinaryBulkStatePrefixImpl(
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkStateSuffix(
+    void serializeBinaryBulkStateSuffixImpl(
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkStatePrefix(
+    void deserializeBinaryBulkStatePrefixImpl(
             DeserializeBinaryBulkSettings & settings,
             DeserializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkWithMultipleStreams(
+    void serializeBinaryBulkWithMultipleStreamsImpl(
             const IColumn & column,
             size_t offset,
             size_t limit,
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkWithMultipleStreams(
+    void deserializeBinaryBulkWithMultipleStreamsImpl(
             IColumn & column,
             size_t limit,
             DeserializeBinaryBulkSettings & settings,
-            DeserializeBinaryBulkStatePtr & state) const override;
+            DeserializeBinaryBulkStatePtr & state,
+            SubstreamsCache * cache) const override;
 
     void serializeProtobuf(const IColumn & column,
                            size_t row_num,
@@ -111,10 +112,17 @@ class DataTypeArray final : public DataTypeWithSimpleSerialization
         return nested->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion();
     }
 
+    DataTypePtr tryGetSubcolumnType(const String & subcolumn_name) const override;
+    ColumnPtr getSubcolumn(const String & subcolumn_name, const IColumn & column) const override;
+
     const DataTypePtr & getNestedType() const { return nested; }
 
     /// 1 for plain array, 2 for array of arrays and so on.
     size_t getNumberOfDimensions() const;
+
+private:
+    ColumnPtr getSubcolumnImpl(const String & subcolumn_name, const IColumn & column, size_t level) const;
+    DataTypePtr tryGetSubcolumnTypeImpl(const String & subcolumn_name, size_t level) const;
 };
 
 }
diff --git a/src/DataTypes/DataTypeCustom.h b/src/DataTypes/DataTypeCustom.h
index c4f846d02593..0fa2e365990d 100644
--- a/src/DataTypes/DataTypeCustom.h
+++ b/src/DataTypes/DataTypeCustom.h
@@ -3,6 +3,7 @@
 #include <memory>
 #include <cstddef>
 #include <Core/Types.h>
+#include <DataTypes/IDataType.h>
 
 namespace DB
 {
@@ -62,8 +63,51 @@ class IDataTypeCustomTextSerialization
     virtual void serializeTextXML(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const = 0;
 };
 
+/** Allows to customize an existing data type by representation with custom substreams.
+  * Customized data type will be serialized/deserialized to files with different names than base type,
+  * but binary and text representation will be unchanged.
+  * E.g it can be used for reading single subcolumns of complex types.
+  */
+class IDataTypeCustomStreams
+{
+public:
+    virtual ~IDataTypeCustomStreams() = default;
+
+    virtual void enumerateStreams(
+        const IDataType::StreamCallback & callback,
+        IDataType::SubstreamPath & path) const = 0;
+
+    virtual void serializeBinaryBulkStatePrefix(
+        IDataType::SerializeBinaryBulkSettings & settings,
+        IDataType::SerializeBinaryBulkStatePtr & state) const = 0;
+
+    virtual void serializeBinaryBulkStateSuffix(
+        IDataType::SerializeBinaryBulkSettings & settings,
+        IDataType::SerializeBinaryBulkStatePtr & state) const = 0;
+
+    virtual void deserializeBinaryBulkStatePrefix(
+        IDataType::DeserializeBinaryBulkSettings & settings,
+        IDataType::DeserializeBinaryBulkStatePtr & state) const = 0;
+
+    virtual void serializeBinaryBulkWithMultipleStreams(
+        const IColumn & column,
+        size_t offset,
+        size_t limit,
+        IDataType::SerializeBinaryBulkSettings & settings,
+        IDataType::SerializeBinaryBulkStatePtr & state) const = 0;
+
+    virtual void deserializeBinaryBulkWithMultipleStreams(
+        ColumnPtr & column,
+        size_t limit,
+        IDataType::DeserializeBinaryBulkSettings & settings,
+        IDataType::DeserializeBinaryBulkStatePtr & state,
+        IDataType::SubstreamsCache * cache) const = 0;
+};
+
 using DataTypeCustomNamePtr = std::unique_ptr<const IDataTypeCustomName>;
 using DataTypeCustomTextSerializationPtr = std::unique_ptr<const IDataTypeCustomTextSerialization>;
+using DataTypeCustomStreamsPtr = std::unique_ptr<const IDataTypeCustomStreams>;
+
 
 /** Describe a data type customization
  */
@@ -71,9 +115,15 @@ struct DataTypeCustomDesc
 {
     DataTypeCustomNamePtr name;
     DataTypeCustomTextSerializationPtr text_serialization;
-
-    DataTypeCustomDesc(DataTypeCustomNamePtr name_, DataTypeCustomTextSerializationPtr text_serialization_)
-            : name(std::move(name_)), text_serialization(std::move(text_serialization_)) {}
+    DataTypeCustomStreamsPtr streams;
+
+    DataTypeCustomDesc(
+        DataTypeCustomNamePtr name_,
+        DataTypeCustomTextSerializationPtr text_serialization_ = nullptr,
+        DataTypeCustomStreamsPtr streams_ = nullptr)
+    : name(std::move(name_))
+    , text_serialization(std::move(text_serialization_))
+    , streams(std::move(streams_)) {}
 };
 
 using DataTypeCustomDescPtr = std::unique_ptr<DataTypeCustomDesc>;
diff --git a/src/DataTypes/DataTypeCustom_fwd.h b/src/DataTypes/DataTypeCustom_fwd.h
new file mode 100644
index 000000000000..99c8eee9748b
--- /dev/null
+++ b/src/DataTypes/DataTypeCustom_fwd.h
@@ -0,0 +1,18 @@
+#pragma once
+
+#include <memory>
+
+namespace DB
+{
+
+class IDataTypeCustomName;
+class IDataTypeCustomTextSerialization;
+class IDataTypeCustomStreams;
+struct DataTypeCustomDesc;
+
+using DataTypeCustomNamePtr = std::unique_ptr<const IDataTypeCustomName>;
+using DataTypeCustomTextSerializationPtr = std::unique_ptr<const IDataTypeCustomTextSerialization>;
+using DataTypeCustomStreamsPtr = std::unique_ptr<const IDataTypeCustomStreams>;
+using DataTypeCustomDescPtr = std::unique_ptr<DataTypeCustomDesc>;
+
+}
diff --git a/src/DataTypes/DataTypeFactory.cpp b/src/DataTypes/DataTypeFactory.cpp
index 1ff00d97e84c..1a1f51178ade 100644
--- a/src/DataTypes/DataTypeFactory.cpp
+++ b/src/DataTypes/DataTypeFactory.cpp
@@ -79,6 +79,16 @@ DataTypePtr DataTypeFactory::get(const String & family_name_param, const ASTPtr
     return findCreatorByName(family_name)(parameters);
 }
 
+DataTypePtr DataTypeFactory::getCustom(DataTypeCustomDescPtr customization) const
+{
+    if (!customization->name)
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Cannot create custom type without name");
+
+    auto type = get(customization->name->getName());
+    type->setCustomization(std::move(customization));
+    return type;
+}
+
 
 void DataTypeFactory::registerDataType(const String & family_name, Value creator, CaseSensitiveness case_sensitiveness)
 {
diff --git a/src/DataTypes/DataTypeFactory.h b/src/DataTypes/DataTypeFactory.h
index 192b1beae5da..618c1f510679 100644
--- a/src/DataTypes/DataTypeFactory.h
+++ b/src/DataTypes/DataTypeFactory.h
@@ -3,6 +3,7 @@
 #include <DataTypes/IDataType.h>
 #include <Parsers/IAST_fwd.h>
 #include <Common/IFactoryWithAliases.h>
+#include <DataTypes/DataTypeCustom_fwd.h>
 
 
 #include <functional>
@@ -33,6 +34,7 @@ class DataTypeFactory final : private boost::noncopyable, public IFactoryWithAli
     DataTypePtr get(const String & full_name) const;
     DataTypePtr get(const String & family_name, const ASTPtr & parameters) const;
     DataTypePtr get(const ASTPtr & ast) const;
+    DataTypePtr getCustom(DataTypeCustomDescPtr customization) const;
 
     /// Register a type family by its name.
     void registerDataType(const String & family_name, Value creator, CaseSensitiveness case_sensitiveness = CaseSensitive);
@@ -84,5 +86,6 @@ void registerDataTypeLowCardinality(DataTypeFactory & factory);
 void registerDataTypeDomainIPv4AndIPv6(DataTypeFactory & factory);
 void registerDataTypeDomainSimpleAggregateFunction(DataTypeFactory & factory);
 void registerDataTypeDomainGeo(DataTypeFactory & factory);
+void registerDataTypeOneElementTuple(DataTypeFactory & factory);
 
 }
diff --git a/src/DataTypes/DataTypeLowCardinality.cpp b/src/DataTypes/DataTypeLowCardinality.cpp
index 8f4b2bf76366..a433d39c5611 100644
--- a/src/DataTypes/DataTypeLowCardinality.cpp
+++ b/src/DataTypes/DataTypeLowCardinality.cpp
@@ -50,7 +50,7 @@ DataTypeLowCardinality::DataTypeLowCardinality(DataTypePtr dictionary_type_)
                         + dictionary_type->getName(), ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT);
 }
 
-void DataTypeLowCardinality::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const
+void DataTypeLowCardinality::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const
 {
     path.push_back(Substream::DictionaryKeys);
     dictionary_type->enumerateStreams(callback, path);
@@ -243,7 +243,7 @@ static DeserializeStateLowCardinality * checkAndGetLowCardinalityDeserializeStat
     return low_cardinality_state;
 }
 
-void DataTypeLowCardinality::serializeBinaryBulkStatePrefix(
+void DataTypeLowCardinality::serializeBinaryBulkStatePrefixImpl(
     SerializeBinaryBulkSettings & settings,
     SerializeBinaryBulkStatePtr & state) const
 {
@@ -263,7 +263,7 @@ void DataTypeLowCardinality::serializeBinaryBulkStatePrefix(
     state = std::make_shared<SerializeStateLowCardinality>(key_version);
 }
 
-void DataTypeLowCardinality::serializeBinaryBulkStateSuffix(
+void DataTypeLowCardinality::serializeBinaryBulkStateSuffixImpl(
     SerializeBinaryBulkSettings & settings,
     SerializeBinaryBulkStatePtr & state) const
 {
@@ -289,7 +289,7 @@ void DataTypeLowCardinality::serializeBinaryBulkStateSuffix(
     }
 }
 
-void DataTypeLowCardinality::deserializeBinaryBulkStatePrefix(
+void DataTypeLowCardinality::deserializeBinaryBulkStatePrefixImpl(
     DeserializeBinaryBulkSettings & settings,
     DeserializeBinaryBulkStatePtr & state) const
 {
@@ -482,7 +482,7 @@ namespace
     }
 }
 
-void DataTypeLowCardinality::serializeBinaryBulkWithMultipleStreams(
+void DataTypeLowCardinality::serializeBinaryBulkWithMultipleStreamsImpl(
     const IColumn & column,
     size_t offset,
     size_t limit,
@@ -579,11 +579,12 @@ void DataTypeLowCardinality::serializeBinaryBulkWithMultipleStreams(
     index_version.getDataType()->serializeBinaryBulk(*positions, *indexes_stream, 0, num_rows);
 }
 
-void DataTypeLowCardinality::deserializeBinaryBulkWithMultipleStreams(
+void DataTypeLowCardinality::deserializeBinaryBulkWithMultipleStreamsImpl(
     IColumn & column,
     size_t limit,
     DeserializeBinaryBulkSettings & settings,
-    DeserializeBinaryBulkStatePtr & state) const
+    DeserializeBinaryBulkStatePtr & state,
+    SubstreamsCache * /* cache */) const
 {
     ColumnLowCardinality & low_cardinality_column = typeid_cast<ColumnLowCardinality &>(column);
 
diff --git a/src/DataTypes/DataTypeLowCardinality.h b/src/DataTypes/DataTypeLowCardinality.h
index f8c314909b85..6ed2b792ce3e 100644
--- a/src/DataTypes/DataTypeLowCardinality.h
+++ b/src/DataTypes/DataTypeLowCardinality.h
@@ -22,32 +22,33 @@ class DataTypeLowCardinality : public IDataType
     const char * getFamilyName() const override { return "LowCardinality"; }
     TypeIndex getTypeId() const override { return TypeIndex::LowCardinality; }
 
-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;
+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;
 
-    void serializeBinaryBulkStatePrefix(
+    void serializeBinaryBulkStatePrefixImpl(
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkStateSuffix(
+    void serializeBinaryBulkStateSuffixImpl(
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkStatePrefix(
+    void deserializeBinaryBulkStatePrefixImpl(
             DeserializeBinaryBulkSettings & settings,
             DeserializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkWithMultipleStreams(
+    void serializeBinaryBulkWithMultipleStreamsImpl(
             const IColumn & column,
             size_t offset,
             size_t limit,
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkWithMultipleStreams(
+    void deserializeBinaryBulkWithMultipleStreamsImpl(
             IColumn & column,
             size_t limit,
             DeserializeBinaryBulkSettings & settings,
-            DeserializeBinaryBulkStatePtr & state) const override;
+            DeserializeBinaryBulkStatePtr & state,
+            SubstreamsCache * cache) const override;
 
     void serializeBinary(const Field & field, WriteBuffer & ostr) const override;
     void deserializeBinary(Field & field, ReadBuffer & istr) const override;
diff --git a/src/DataTypes/DataTypeMap.cpp b/src/DataTypes/DataTypeMap.cpp
index b207702d9b7f..3f59e1d36549 100644
--- a/src/DataTypes/DataTypeMap.cpp
+++ b/src/DataTypes/DataTypeMap.cpp
@@ -278,34 +278,34 @@ void DataTypeMap::deserializeTextCSV(IColumn & column, ReadBuffer & istr, const
 }
 
 
-void DataTypeMap::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const
+void DataTypeMap::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const
 {
     nested->enumerateStreams(callback, path);
 }
 
-void DataTypeMap::serializeBinaryBulkStatePrefix(
+void DataTypeMap::serializeBinaryBulkStatePrefixImpl(
     SerializeBinaryBulkSettings & settings,
     SerializeBinaryBulkStatePtr & state) const
 {
     nested->serializeBinaryBulkStatePrefix(settings, state);
 }
 
-void DataTypeMap::serializeBinaryBulkStateSuffix(
+void DataTypeMap::serializeBinaryBulkStateSuffixImpl(
     SerializeBinaryBulkSettings & settings,
     SerializeBinaryBulkStatePtr & state) const
 {
     nested->serializeBinaryBulkStateSuffix(settings, state);
 }
 
-void DataTypeMap::deserializeBinaryBulkStatePrefix(
-        DeserializeBinaryBulkSettings & settings,
-        DeserializeBinaryBulkStatePtr & state) const
+void DataTypeMap::deserializeBinaryBulkStatePrefixImpl(
+    DeserializeBinaryBulkSettings & settings,
+    DeserializeBinaryBulkStatePtr & state) const
 {
     nested->deserializeBinaryBulkStatePrefix(settings, state);
 }
 
 
-void DataTypeMap::serializeBinaryBulkWithMultipleStreams(
+void DataTypeMap::serializeBinaryBulkWithMultipleStreamsImpl(
     const IColumn & column,
     size_t offset,
     size_t limit,
@@ -315,13 +315,15 @@ void DataTypeMap::serializeBinaryBulkWithMultipleStreams(
     nested->serializeBinaryBulkWithMultipleStreams(extractNestedColumn(column), offset, limit, settings, state);
 }
 
-void DataTypeMap::deserializeBinaryBulkWithMultipleStreams(
+void DataTypeMap::deserializeBinaryBulkWithMultipleStreamsImpl(
     IColumn & column,
     size_t limit,
     DeserializeBinaryBulkSettings & settings,
-    DeserializeBinaryBulkStatePtr & state) const
+    DeserializeBinaryBulkStatePtr & state,
+    SubstreamsCache * cache) const
 {
-    nested->deserializeBinaryBulkWithMultipleStreams(extractNestedColumn(column), limit, settings, state);
+    auto & column_map = assert_cast<ColumnMap &>(column);
+    nested->deserializeBinaryBulkWithMultipleStreams(column_map.getNestedColumnPtr(), limit, settings, state, cache);
 }
 
 void DataTypeMap::serializeProtobuf(const IColumn & column, size_t row_num, ProtobufWriter & protobuf, size_t & value_index) const
diff --git a/src/DataTypes/DataTypeMap.h b/src/DataTypes/DataTypeMap.h
index a52969428f41..7c51ac2f5795 100644
--- a/src/DataTypes/DataTypeMap.h
+++ b/src/DataTypes/DataTypeMap.h
@@ -46,34 +46,33 @@ class DataTypeMap final : public DataTypeWithSimpleSerialization
     void deserializeTextCSV(IColumn & column, ReadBuffer & istr, const FormatSettings &) const override;
 
 
-    /** Each sub-column in a map is serialized in separate stream.
-      */
-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;
+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;
 
-    void serializeBinaryBulkStatePrefix(
+    void serializeBinaryBulkStatePrefixImpl(
            SerializeBinaryBulkSettings & settings,
            SerializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkStateSuffix(
+    void serializeBinaryBulkStateSuffixImpl(
            SerializeBinaryBulkSettings & settings,
            SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkStatePrefix(
+    void deserializeBinaryBulkStatePrefixImpl(
            DeserializeBinaryBulkSettings & settings,
            DeserializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkWithMultipleStreams(
+    void serializeBinaryBulkWithMultipleStreamsImpl(
            const IColumn & column,
            size_t offset,
            size_t limit,
            SerializeBinaryBulkSettings & settings,
            SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkWithMultipleStreams(
+    void deserializeBinaryBulkWithMultipleStreamsImpl(
            IColumn & column,
            size_t limit,
            DeserializeBinaryBulkSettings & settings,
-           DeserializeBinaryBulkStatePtr & state) const override;
+           DeserializeBinaryBulkStatePtr & state,
+           SubstreamsCache * cache) const override;
 
     void serializeProtobuf(const IColumn & column, size_t row_num, ProtobufWriter & protobuf, size_t & value_index) const override;
     void deserializeProtobuf(IColumn & column, ProtobufReader & protobuf, bool allow_add_row, bool & row_added) const override;
diff --git a/src/DataTypes/DataTypeNested.cpp b/src/DataTypes/DataTypeNested.cpp
new file mode 100644
index 000000000000..cfbfb4c17504
--- /dev/null
+++ b/src/DataTypes/DataTypeNested.cpp
@@ -0,0 +1,76 @@
+#include <DataTypes/DataTypeNested.h>
+#include <DataTypes/DataTypeFactory.h>
+#include <DataTypes/DataTypeArray.h>
+#include <DataTypes/DataTypeTuple.h>
+#include <IO/WriteBufferFromString.h>
+#include <IO/Operators.h>
+#include <Common/quoteString.h>
+#include <Parsers/ASTNameTypePair.h>
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int EMPTY_DATA_PASSED;
+    extern const int BAD_ARGUMENTS;
+}
+
+String DataTypeNestedCustomName::getName() const
+{
+    WriteBufferFromOwnString s;
+    s << "Nested(";
+    for (size_t i = 0; i < elems.size(); ++i)
+    {
+        if (i != 0)
+            s << ", ";
+
+        s << backQuoteIfNeed(names[i]) << ' ';
+        s << elems[i]->getName();
+    }
+    s << ")";
+
+    return s.str();
+}
+
+static std::pair<DataTypePtr, DataTypeCustomDescPtr> create(const ASTPtr & arguments)
+{
+    if (!arguments || arguments->children.empty())
+        throw Exception("Nested cannot be empty", ErrorCodes::EMPTY_DATA_PASSED);
+
+    DataTypes nested_types;
+    Strings nested_names;
+    nested_types.reserve(arguments->children.size());
+    nested_names.reserve(arguments->children.size());
+
+    for (const auto & child : arguments->children)
+    {
+        const auto * name_type = child->as<ASTNameTypePair>();
+        if (!name_type)
+            throw Exception(ErrorCodes::BAD_ARGUMENTS, "Data type Nested accepts only pairs with name and type");
+
+        auto nested_type = DataTypeFactory::instance().get(name_type->type);
+        nested_types.push_back(std::move(nested_type));
+        nested_names.push_back(name_type->name);
+    }
+
+    auto data_type = std::make_shared<DataTypeArray>(std::make_shared<DataTypeTuple>(nested_types, nested_names));
+    auto custom_name = std::make_unique<DataTypeNestedCustomName>(nested_types, nested_names);
+
+    return std::make_pair(std::move(data_type), std::make_unique<DataTypeCustomDesc>(std::move(custom_name), nullptr));
+}
+
+void registerDataTypeNested(DataTypeFactory & factory)
+{
+    return factory.registerDataTypeCustom("Nested", create);
+}
+
+DataTypePtr createNested(const DataTypes & types, const Names & names)
+{
+    auto custom_desc = std::make_unique<DataTypeCustomDesc>(
+        std::make_unique<DataTypeNestedCustomName>(types, names));
+
+    return DataTypeFactory::instance().getCustom(std::move(custom_desc));
+}
+
+}
diff --git a/src/DataTypes/DataTypeNested.h b/src/DataTypes/DataTypeNested.h
new file mode 100644
index 000000000000..9fb12ad49247
--- /dev/null
+++ b/src/DataTypes/DataTypeNested.h
@@ -0,0 +1,34 @@
+#pragma once
+
+#include <DataTypes/DataTypeWithSimpleSerialization.h>
+#include <DataTypes/DataTypeCustom.h>
+
+
+namespace DB
+{
+
+class DataTypeNestedCustomName final : public IDataTypeCustomName
+{
+private:
+    DataTypes elems;
+    Strings names;
+
+public:
+    DataTypeNestedCustomName(const DataTypes & elems_, const Strings & names_)
+        : elems(elems_), names(names_)
+    {
+    }
+
+    String getName() const override;
+};
+
+DataTypePtr createNested(const DataTypes & types, const Names & names);
+
+template <typename DataType>
+inline bool isNested(const DataType & data_type)
+{
+    return typeid_cast<const DataTypeNestedCustomName *>(data_type->getCustomName()) != nullptr;
+}
+
+}
+
diff --git a/src/DataTypes/DataTypeNullable.cpp b/src/DataTypes/DataTypeNullable.cpp
index a0fc8baaf7e4..64b060e521bc 100644
--- a/src/DataTypes/DataTypeNullable.cpp
+++ b/src/DataTypes/DataTypeNullable.cpp
@@ -2,6 +2,7 @@
 #include <DataTypes/DataTypeNothing.h>
 #include <DataTypes/DataTypesNumber.h>
 #include <DataTypes/DataTypeFactory.h>
+#include <DataTypes/DataTypeOneElementTuple.h>
 #include <Columns/ColumnNullable.h>
 #include <Core/Field.h>
 #include <IO/ReadBuffer.h>
@@ -41,7 +42,7 @@ bool DataTypeNullable::onlyNull() const
 }
 
 
-void DataTypeNullable::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const
+void DataTypeNullable::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const
 {
     path.push_back(Substream::NullMap);
     callback(path, *this);
@@ -51,7 +52,7 @@ void DataTypeNullable::enumerateStreams(const StreamCallback & callback, Substre
 }
 
 
-void DataTypeNullable::serializeBinaryBulkStatePrefix(
+void DataTypeNullable::serializeBinaryBulkStatePrefixImpl(
         SerializeBinaryBulkSettings & settings,
         SerializeBinaryBulkStatePtr & state) const
 {
@@ -61,7 +62,7 @@ void DataTypeNullable::serializeBinaryBulkStatePrefix(
 }
 
 
-void DataTypeNullable::serializeBinaryBulkStateSuffix(
+void DataTypeNullable::serializeBinaryBulkStateSuffixImpl(
     SerializeBinaryBulkSettings & settings,
     SerializeBinaryBulkStatePtr & state) const
 {
@@ -71,7 +72,7 @@ void DataTypeNullable::serializeBinaryBulkStateSuffix(
 }
 
 
-void DataTypeNullable::deserializeBinaryBulkStatePrefix(
+void DataTypeNullable::deserializeBinaryBulkStatePrefixImpl(
     DeserializeBinaryBulkSettings & settings,
     DeserializeBinaryBulkStatePtr & state) const
 {
@@ -81,7 +82,7 @@ void DataTypeNullable::deserializeBinaryBulkStatePrefix(
 }
 
 
-void DataTypeNullable::serializeBinaryBulkWithMultipleStreams(
+void DataTypeNullable::serializeBinaryBulkWithMultipleStreamsImpl(
     const IColumn & column,
     size_t offset,
     size_t limit,
@@ -103,20 +104,28 @@ void DataTypeNullable::serializeBinaryBulkWithMultipleStreams(
 }
 
 
-void DataTypeNullable::deserializeBinaryBulkWithMultipleStreams(
+void DataTypeNullable::deserializeBinaryBulkWithMultipleStreamsImpl(
     IColumn & column,
     size_t limit,
     DeserializeBinaryBulkSettings & settings,
-    DeserializeBinaryBulkStatePtr & state) const
+    DeserializeBinaryBulkStatePtr & state,
+    SubstreamsCache * cache) const
 {
     ColumnNullable & col = assert_cast<ColumnNullable &>(column);
 
     settings.path.push_back(Substream::NullMap);
-    if (auto * stream = settings.getter(settings.path))
+    if (auto cached_column = getFromSubstreamsCache(cache, settings.path))
+    {
+        col.getNullMapColumnPtr() = cached_column;
+    }
+    else if (auto * stream = settings.getter(settings.path))
+    {
         DataTypeUInt8().deserializeBinaryBulk(col.getNullMapColumn(), *stream, limit, 0);
+        addToSubstreamsCache(cache, settings.path, col.getNullMapColumnPtr());
+    }
 
     settings.path.back() = Substream::NullableElements;
-    nested_data_type->deserializeBinaryBulkWithMultipleStreams(col.getNestedColumn(), limit, settings, state);
+    nested_data_type->deserializeBinaryBulkWithMultipleStreams(col.getNestedColumnPtr(), limit, settings, state, cache);
     settings.path.pop_back();
 }
 
@@ -525,6 +534,23 @@ bool DataTypeNullable::equals(const IDataType & rhs) const
     return rhs.isNullable() && nested_data_type->equals(*static_cast<const DataTypeNullable &>(rhs).nested_data_type);
 }
 
+DataTypePtr DataTypeNullable::tryGetSubcolumnType(const String & subcolumn_name) const
+{
+    if (subcolumn_name == "null")
+        return createOneElementTuple(std::make_shared<DataTypeUInt8>(), subcolumn_name, false);
+
+    return nested_data_type->tryGetSubcolumnType(subcolumn_name);
+}
+
+ColumnPtr DataTypeNullable::getSubcolumn(const String & subcolumn_name, const IColumn & column) const
+{
+    const auto & column_nullable = assert_cast<const ColumnNullable &>(column);
+    if (subcolumn_name == "null")
+        return column_nullable.getNullMapColumnPtr()->assumeMutable();
+
+    return nested_data_type->getSubcolumn(subcolumn_name, column_nullable.getNestedColumn());
+}
+
 
 static DataTypePtr create(const ASTPtr & arguments)
 {
diff --git a/src/DataTypes/DataTypeNullable.h b/src/DataTypes/DataTypeNullable.h
index 587eecdf32e9..db641faf0afb 100644
--- a/src/DataTypes/DataTypeNullable.h
+++ b/src/DataTypes/DataTypeNullable.h
@@ -18,32 +18,33 @@ class DataTypeNullable final : public IDataType
     const char * getFamilyName() const override { return "Nullable"; }
     TypeIndex getTypeId() const override { return TypeIndex::Nullable; }
 
-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;
+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;
 
-    void serializeBinaryBulkStatePrefix(
+    void serializeBinaryBulkStatePrefixImpl(
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkStateSuffix(
+    void serializeBinaryBulkStateSuffixImpl(
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkStatePrefix(
+    void deserializeBinaryBulkStatePrefixImpl(
             DeserializeBinaryBulkSettings & settings,
             DeserializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkWithMultipleStreams(
+    void serializeBinaryBulkWithMultipleStreamsImpl(
             const IColumn & column,
             size_t offset,
             size_t limit,
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkWithMultipleStreams(
+    void deserializeBinaryBulkWithMultipleStreamsImpl(
             IColumn & column,
             size_t limit,
             DeserializeBinaryBulkSettings & settings,
-            DeserializeBinaryBulkStatePtr & state) const override;
+            DeserializeBinaryBulkStatePtr & state,
+            SubstreamsCache * cache) const override;
 
     void serializeBinary(const Field & field, WriteBuffer & ostr) const override;
     void deserializeBinary(Field & field, ReadBuffer & istr) const override;
@@ -97,6 +98,8 @@ class DataTypeNullable final : public IDataType
     size_t getSizeOfValueInMemory() const override;
     bool onlyNull() const override;
     bool canBeInsideLowCardinality() const override { return nested_data_type->canBeInsideLowCardinality(); }
+    DataTypePtr tryGetSubcolumnType(const String & subcolumn_name) const override;
+    ColumnPtr getSubcolumn(const String & subcolumn_name, const IColumn & column) const override;
 
     const DataTypePtr & getNestedType() const { return nested_data_type; }
 
diff --git a/src/DataTypes/DataTypeOneElementTuple.cpp b/src/DataTypes/DataTypeOneElementTuple.cpp
new file mode 100644
index 000000000000..a41692203623
--- /dev/null
+++ b/src/DataTypes/DataTypeOneElementTuple.cpp
@@ -0,0 +1,112 @@
+#include <DataTypes/DataTypeOneElementTuple.h>
+#include <DataTypes/DataTypeFactory.h>
+#include <DataTypes/DataTypeCustom.h>
+#include <IO/WriteBufferFromString.h>
+#include <IO/Operators.h>
+#include <Common/quoteString.h>
+#include <Parsers/ASTNameTypePair.h>
+#include <Columns/IColumn.h>
+
+
+namespace DB
+{
+
+namespace
+{
+
+/** Custom substreams representation for single subcolumn.
+  * It serializes/deserializes column as a nested type, but in that way
+  * if it was a named tuple with one element and a given name.
+  */
+class DataTypeOneElementTupleStreams : public IDataTypeCustomStreams
+{
+private:
+    DataTypePtr nested;
+    String name;
+    bool escape_delimiter;
+
+public:
+    DataTypeOneElementTupleStreams(const DataTypePtr & nested_, const String & name_, bool escape_delimiter_)
+        : nested(nested_), name(name_), escape_delimiter(escape_delimiter_) {}
+
+    void enumerateStreams(
+        const IDataType::StreamCallback & callback,
+        IDataType::SubstreamPath & path) const override
+    {
+        addToPath(path);
+        nested->enumerateStreams(callback, path);
+        path.pop_back();
+    }
+
+    void serializeBinaryBulkStatePrefix(
+        IDataType:: SerializeBinaryBulkSettings & settings,
+        IDataType::SerializeBinaryBulkStatePtr & state) const override
+    {
+        addToPath(settings.path);
+        nested->serializeBinaryBulkStatePrefix(settings, state);
+        settings.path.pop_back();
+    }
+
+    void serializeBinaryBulkStateSuffix(
+        IDataType::SerializeBinaryBulkSettings & settings,
+        IDataType::SerializeBinaryBulkStatePtr & state) const override
+    {
+        addToPath(settings.path);
+        nested->serializeBinaryBulkStateSuffix(settings, state);
+        settings.path.pop_back();
+    }
+
+    void deserializeBinaryBulkStatePrefix(
+        IDataType::DeserializeBinaryBulkSettings & settings,
+        IDataType::DeserializeBinaryBulkStatePtr & state) const override
+    {
+        addToPath(settings.path);
+        nested->deserializeBinaryBulkStatePrefix(settings, state);
+        settings.path.pop_back();
+    }
+
+    void serializeBinaryBulkWithMultipleStreams(
+        const IColumn & column,
+        size_t offset,
+        size_t limit,
+        IDataType::SerializeBinaryBulkSettings & settings,
+        IDataType::SerializeBinaryBulkStatePtr & state) const override
+    {
+        addToPath(settings.path);
+        nested->serializeBinaryBulkWithMultipleStreams(column, offset, limit, settings, state);
+        settings.path.pop_back();
+    }
+
+    void deserializeBinaryBulkWithMultipleStreams(
+        ColumnPtr & column,
+        size_t limit,
+        IDataType::DeserializeBinaryBulkSettings & settings,
+        IDataType::DeserializeBinaryBulkStatePtr & state,
+        IDataType::SubstreamsCache * cache) const override
+    {
+        addToPath(settings.path);
+        nested->deserializeBinaryBulkWithMultipleStreams(column, limit, settings, state, cache);
+        settings.path.pop_back();
+    }
+
+private:
+    void addToPath(IDataType::SubstreamPath & path) const
+    {
+        path.push_back(IDataType::Substream::TupleElement);
+        path.back().tuple_element_name = name;
+        path.back().escape_tuple_delimiter = escape_delimiter;
+    }
+};
+
+}
+
+DataTypePtr createOneElementTuple(const DataTypePtr & type, const String & name, bool escape_delimiter)
+{
+    auto custom_desc = std::make_unique<DataTypeCustomDesc>(
+        std::make_unique<DataTypeCustomFixedName>(type->getName()),nullptr,
+        std::make_unique<DataTypeOneElementTupleStreams>(type, name, escape_delimiter));
+
+    return DataTypeFactory::instance().getCustom(std::move(custom_desc));
+}
+
+}
diff --git a/src/DataTypes/DataTypeOneElementTuple.h b/src/DataTypes/DataTypeOneElementTuple.h
new file mode 100644
index 000000000000..03b0511ef4ab
--- /dev/null
+++ b/src/DataTypes/DataTypeOneElementTuple.h
@@ -0,0 +1,10 @@
+#pragma once
+
+#include <DataTypes/IDataType.h>
+
+namespace DB
+{
+
+DataTypePtr createOneElementTuple(const DataTypePtr & type, const String & name, bool escape_delimiter = true);
+
+}
diff --git a/src/DataTypes/DataTypeTuple.cpp b/src/DataTypes/DataTypeTuple.cpp
index 02fc49f7e9aa..c62aa1c1187a 100644
--- a/src/DataTypes/DataTypeTuple.cpp
+++ b/src/DataTypes/DataTypeTuple.cpp
@@ -5,6 +5,7 @@
 #include <DataTypes/DataTypeTuple.h>
 #include <DataTypes/DataTypeArray.h>
 #include <DataTypes/DataTypeFactory.h>
+#include <DataTypes/DataTypeOneElementTuple.h>
 #include <Parsers/IAST.h>
 #include <Parsers/ASTNameTypePair.h>
 #include <Common/typeid_cast.h>
@@ -30,6 +31,7 @@ namespace ErrorCodes
     extern const int EMPTY_DATA_PASSED;
     extern const int LOGICAL_ERROR;
     extern const int NOT_FOUND_COLUMN_IN_BLOCK;
+    extern const int ILLEGAL_COLUMN;
     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
     extern const int SIZES_OF_COLUMNS_IN_TUPLE_DOESNT_MATCH;
 }
@@ -357,7 +359,7 @@ void DataTypeTuple::deserializeTextCSV(IColumn & column, ReadBuffer & istr, cons
     });
 }
 
-void DataTypeTuple::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const
+void DataTypeTuple::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const
 {
     path.push_back(Substream::TupleElement);
     for (const auto i : ext::range(0, ext::size(elems)))
@@ -412,7 +414,7 @@ static DeserializeBinaryBulkStateTuple * checkAndGetTupleDeserializeState(IDataT
     return tuple_state;
 }
 
-void DataTypeTuple::serializeBinaryBulkStatePrefix(
+void DataTypeTuple::serializeBinaryBulkStatePrefixImpl(
     SerializeBinaryBulkSettings & settings,
     SerializeBinaryBulkStatePtr & state) const
 {
@@ -430,7 +432,7 @@ void DataTypeTuple::serializeBinaryBulkStatePrefix(
     state = std::move(tuple_state);
 }
 
-void DataTypeTuple::serializeBinaryBulkStateSuffix(
+void DataTypeTuple::serializeBinaryBulkStateSuffixImpl(
     SerializeBinaryBulkSettings & settings,
     SerializeBinaryBulkStatePtr & state) const
 {
@@ -445,7 +447,7 @@ void DataTypeTuple::serializeBinaryBulkStateSuffix(
     settings.path.pop_back();
 }
 
-void DataTypeTuple::deserializeBinaryBulkStatePrefix(
+void DataTypeTuple::deserializeBinaryBulkStatePrefixImpl(
         DeserializeBinaryBulkSettings & settings,
         DeserializeBinaryBulkStatePtr & state) const
 {
@@ -463,7 +465,7 @@ void DataTypeTuple::deserializeBinaryBulkStatePrefix(
     state = std::move(tuple_state);
 }
 
-void DataTypeTuple::serializeBinaryBulkWithMultipleStreams(
+void DataTypeTuple::serializeBinaryBulkWithMultipleStreamsImpl(
     const IColumn & column,
     size_t offset,
     size_t limit,
@@ -482,21 +484,22 @@ void DataTypeTuple::serializeBinaryBulkWithMultipleStreams(
     settings.path.pop_back();
 }
 
-void DataTypeTuple::deserializeBinaryBulkWithMultipleStreams(
+void DataTypeTuple::deserializeBinaryBulkWithMultipleStreamsImpl(
     IColumn & column,
     size_t limit,
     DeserializeBinaryBulkSettings & settings,
-    DeserializeBinaryBulkStatePtr & state) const
+    DeserializeBinaryBulkStatePtr & state,
+    SubstreamsCache * cache) const
 {
     auto * tuple_state = checkAndGetTupleDeserializeState(state);
+    auto & column_tuple = assert_cast<ColumnTuple &>(column);
 
     settings.path.push_back(Substream::TupleElement);
     settings.avg_value_size_hint = 0;
     for (const auto i : ext::range(0, ext::size(elems)))
     {
         settings.path.back().tuple_element_name = names[i];
-        auto & element_col = extractElementColumn(column, i);
-        elems[i]->deserializeBinaryBulkWithMultipleStreams(element_col, limit, settings, tuple_state->states[i]);
+        elems[i]->deserializeBinaryBulkWithMultipleStreams(column_tuple.getColumnPtr(i), limit, settings, tuple_state->states[i], cache);
     }
     settings.path.pop_back();
 }
@@ -611,6 +614,47 @@ size_t DataTypeTuple::getSizeOfValueInMemory() const
     return res;
 }
 
+DataTypePtr DataTypeTuple::tryGetSubcolumnType(const String & subcolumn_name) const
+{
+    for (size_t i = 0; i < names.size(); ++i)
+    {
+        if (startsWith(subcolumn_name, names[i]))
+        {
+            size_t name_length = names[i].size();
+            DataTypePtr subcolumn_type;
+            if (subcolumn_name.size() == name_length)
+                subcolumn_type = elems[i];
+            else if (subcolumn_name[name_length] == '.')
+                subcolumn_type = elems[i]->tryGetSubcolumnType(subcolumn_name.substr(name_length + 1));
+
+            if (subcolumn_type)
+                return createOneElementTuple(std::move(subcolumn_type), names[i]);
+        }
+    }
+
+    return nullptr;
+}
+
+ColumnPtr DataTypeTuple::getSubcolumn(const String & subcolumn_name, const IColumn & column) const
+{
+    for (size_t i = 0; i < names.size(); ++i)
+    {
+        if (startsWith(subcolumn_name, names[i]))
+        {
+            size_t name_length = names[i].size();
+            const auto & subcolumn = extractElementColumn(column, i);
+
+            if (subcolumn_name.size() == name_length)
+                return subcolumn.assumeMutable();
+
+            if (subcolumn_name[name_length] == '.')
+                return elems[i]->getSubcolumn(subcolumn_name.substr(name_length + 1), subcolumn);
+        }
+    }
+
+    throw Exception(ErrorCodes::ILLEGAL_COLUMN, "There is no subcolumn {} in type {}", subcolumn_name, getName());
+}
+
 
 static DataTypePtr create(const ASTPtr & arguments)
 {
@@ -648,13 +692,4 @@ void registerDataTypeTuple(DataTypeFactory & factory)
     factory.registerDataType("Tuple", create);
 }
 
-void registerDataTypeNested(DataTypeFactory & factory)
-{
-    /// Nested(...) data type is just a sugar for Array(Tuple(...))
-    factory.registerDataType("Nested", [&factory](const ASTPtr & arguments)
-    {
-        return std::make_shared<DataTypeArray>(factory.get("Tuple", arguments));
-    });
-}
-
 }
diff --git a/src/DataTypes/DataTypeTuple.h b/src/DataTypes/DataTypeTuple.h
index 7e4e68651f1b..0b28ebe5a635 100644
--- a/src/DataTypes/DataTypeTuple.h
+++ b/src/DataTypes/DataTypeTuple.h
@@ -53,32 +53,33 @@ class DataTypeTuple final : public DataTypeWithSimpleSerialization
 
     /** Each sub-column in a tuple is serialized in separate stream.
       */
-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;
+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;
 
-    void serializeBinaryBulkStatePrefix(
+    void serializeBinaryBulkStatePrefixImpl(
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkStateSuffix(
+    void serializeBinaryBulkStateSuffixImpl(
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkStatePrefix(
+    void deserializeBinaryBulkStatePrefixImpl(
             DeserializeBinaryBulkSettings & settings,
             DeserializeBinaryBulkStatePtr & state) const override;
 
-    void serializeBinaryBulkWithMultipleStreams(
+    void serializeBinaryBulkWithMultipleStreamsImpl(
             const IColumn & column,
             size_t offset,
             size_t limit,
             SerializeBinaryBulkSettings & settings,
             SerializeBinaryBulkStatePtr & state) const override;
 
-    void deserializeBinaryBulkWithMultipleStreams(
+    void deserializeBinaryBulkWithMultipleStreamsImpl(
             IColumn & column,
             size_t limit,
             DeserializeBinaryBulkSettings & settings,
-            DeserializeBinaryBulkStatePtr & state) const override;
+            DeserializeBinaryBulkStatePtr & state,
+            SubstreamsCache * cache) const override;
 
     void serializeProtobuf(const IColumn & column, size_t row_num, ProtobufWriter & protobuf, size_t & value_index) const override;
     void deserializeProtobuf(IColumn & column, ProtobufReader & protobuf, bool allow_add_row, bool & row_added) const override;
@@ -98,6 +99,9 @@ class DataTypeTuple final : public DataTypeWithSimpleSerialization
     size_t getMaximumSizeOfValueInMemory() const override;
     size_t getSizeOfValueInMemory() const override;
 
+    DataTypePtr tryGetSubcolumnType(const String & subcolumn_name) const override;
+    ColumnPtr getSubcolumn(const String & subcolumn_name, const IColumn & column) const override;
+
     const DataTypes & getElements() const { return elems; }
     const Strings & getElementNames() const { return names; }
 
diff --git a/src/DataTypes/IDataType.cpp b/src/DataTypes/IDataType.cpp
index d1c9f1bde778..5582a8698e08 100644
--- a/src/DataTypes/IDataType.cpp
+++ b/src/DataTypes/IDataType.cpp
@@ -3,8 +3,10 @@
 
 #include <Common/Exception.h>
 #include <Common/escapeForFileName.h>
+#include <Common/SipHash.h>
 
 #include <IO/WriteHelpers.h>
+#include <IO/Operators.h>
 
 #include <DataTypes/IDataType.h>
 #include <DataTypes/DataTypeCustom.h>
@@ -19,9 +21,48 @@ namespace ErrorCodes
     extern const int MULTIPLE_STREAMS_REQUIRED;
     extern const int LOGICAL_ERROR;
     extern const int DATA_TYPE_CANNOT_BE_PROMOTED;
+    extern const int ILLEGAL_COLUMN;
 }
 
-IDataType::IDataType() : custom_name(nullptr), custom_text_serialization(nullptr)
+String IDataType::Substream::toString() const
+{
+    switch (type)
+    {
+        case ArrayElements:
+            return "ArrayElements";
+        case ArraySizes:
+            return "ArraySizes";
+        case NullableElements:
+            return "NullableElements";
+        case NullMap:
+            return "NullMap";
+        case TupleElement:
+            return "TupleElement(" + tuple_element_name + ", "
+                + std::to_string(escape_tuple_delimiter) + ")";
+        case DictionaryKeys:
+            return "DictionaryKeys";
+        case DictionaryIndexes:
+            return "DictionaryIndexes";
+    }
+
+    __builtin_unreachable();
+}
+
+String IDataType::SubstreamPath::toString() const
+{
+    WriteBufferFromOwnString wb;
+    wb << "{";
+    for (size_t i = 0; i < size(); ++i)
+    {
+        if (i != 0)
+            wb << ", ";
+        wb << at(i).toString();
+    }
+    wb << "}";
+    return wb.str();
+}
+
+IDataType::IDataType() : custom_name(nullptr), custom_text_serialization(nullptr), custom_streams(nullptr)
 {
 }
 
@@ -93,42 +134,89 @@ size_t IDataType::getSizeOfValueInMemory() const
     throw Exception("Value of type " + getName() + " in memory is not of fixed size.", ErrorCodes::LOGICAL_ERROR);
 }
 
+DataTypePtr IDataType::getSubcolumnType(const String & subcolumn_name) const
+{
+    auto subcolumn_type = tryGetSubcolumnType(subcolumn_name);
+    if (subcolumn_type)
+        return subcolumn_type;
+
+    throw Exception(ErrorCodes::ILLEGAL_COLUMN, "There is no subcolumn {} in type {}", subcolumn_name, getName());
+}
+
+ColumnPtr IDataType::getSubcolumn(const String & subcolumn_name, const IColumn &) const
+{
+    throw Exception(ErrorCodes::ILLEGAL_COLUMN, "There is no subcolumn {} in type {}", subcolumn_name, getName());
+}
 
-String IDataType::getFileNameForStream(const String & column_name, const IDataType::SubstreamPath & path)
+Names IDataType::getSubcolumnNames() const
 {
-    /// Sizes of arrays (elements of Nested type) are shared (all reside in single file).
-    String nested_table_name = Nested::extractTableName(column_name);
+    NameSet res;
+    enumerateStreams([&res, this](const SubstreamPath & substream_path, const IDataType & /* substream_type */)
+    {
+        SubstreamPath new_path;
+        /// Iterate over path to try to get intermediate subcolumns for complex nested types.
+        for (const auto & elem : substream_path)
+        {
+            new_path.push_back(elem);
+            auto subcolumn_name = getSubcolumnNameForStream(new_path);
+            if (!subcolumn_name.empty() && tryGetSubcolumnType(subcolumn_name))
+                res.insert(subcolumn_name);
+        }
+    });
 
-    bool is_sizes_of_nested_type =
-        path.size() == 1    /// Nested structure may have arrays as nested elements (so effectively we have multidimensional arrays).
-                            /// Sizes of arrays are shared only at first level.
-        && path[0].type == IDataType::Substream::ArraySizes
-        && nested_table_name != column_name;
+    return Names(std::make_move_iterator(res.begin()), std::make_move_iterator(res.end()));
+}
 
+static String getNameForSubstreamPath(
+    String stream_name,
+    const IDataType::SubstreamPath & path,
+    bool escape_tuple_delimiter)
+{
     size_t array_level = 0;
-    String stream_name = escapeForFileName(is_sizes_of_nested_type ? nested_table_name : column_name);
-    for (const Substream & elem : path)
+    for (const auto & elem : path)
     {
-        if (elem.type == Substream::NullMap)
+        if (elem.type == IDataType::Substream::NullMap)
             stream_name += ".null";
-        else if (elem.type == Substream::ArraySizes)
+        else if (elem.type == IDataType::Substream::ArraySizes)
             stream_name += ".size" + toString(array_level);
-        else if (elem.type == Substream::ArrayElements)
+        else if (elem.type == IDataType::Substream::ArrayElements)
             ++array_level;
-        else if (elem.type == Substream::TupleElement)
+        else if (elem.type == IDataType::Substream::DictionaryKeys)
+            stream_name += ".dict";
+        else if (elem.type == IDataType::Substream::TupleElement)
         {
-            /// For compatibility reasons, we use %2E instead of dot.
+            /// For compatibility reasons, we use %2E (escaped dot) instead of dot.
             /// Because nested data may be represented not by Array of Tuple,
             ///  but by separate Array columns with names in a form of a.b,
             ///  and name is encoded as a whole.
-            stream_name += "%2E" + escapeForFileName(elem.tuple_element_name);
+            stream_name += (escape_tuple_delimiter && elem.escape_tuple_delimiter ?
+                escapeForFileName(".") : ".") + escapeForFileName(elem.tuple_element_name);
         }
-        else if (elem.type == Substream::DictionaryKeys)
-            stream_name += ".dict";
     }
+
     return stream_name;
 }
 
+String IDataType::getFileNameForStream(const NameAndTypePair & column, const SubstreamPath & path)
+{
+    auto name_in_storage = column.getNameInStorage();
+    auto nested_storage_name = Nested::extractTableName(name_in_storage);
+
+    if (name_in_storage != nested_storage_name && (path.size() == 1 && path[0].type == IDataType::Substream::ArraySizes))
+        name_in_storage = nested_storage_name;
+
+    auto stream_name = escapeForFileName(name_in_storage);
+    return getNameForSubstreamPath(std::move(stream_name), path, true);
+}
+
+String IDataType::getSubcolumnNameForStream(const SubstreamPath & path)
+{
+    auto subcolumn_name = getNameForSubstreamPath("", path, false);
+    if (!subcolumn_name.empty())
+        subcolumn_name = subcolumn_name.substr(1); // It starts with a dot.
+
+    return subcolumn_name;
+}
 
 bool IDataType::isSpecialCompressionAllowed(const SubstreamPath & path)
 {
@@ -147,6 +235,102 @@ void IDataType::insertDefaultInto(IColumn & column) const
     column.insertDefault();
 }
 
+void IDataType::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const
+{
+    if (custom_streams)
+        custom_streams->enumerateStreams(callback, path);
+    else
+        enumerateStreamsImpl(callback, path);
+}
+
+void IDataType::serializeBinaryBulkStatePrefix(
+    SerializeBinaryBulkSettings & settings,
+    SerializeBinaryBulkStatePtr & state) const
+{
+    if (custom_streams)
+        custom_streams->serializeBinaryBulkStatePrefix(settings, state);
+    else
+        serializeBinaryBulkStatePrefixImpl(settings, state);
+}
+
+void IDataType::serializeBinaryBulkStateSuffix(
+    SerializeBinaryBulkSettings & settings,
+    SerializeBinaryBulkStatePtr & state) const
+{
+    if (custom_streams)
+        custom_streams->serializeBinaryBulkStateSuffix(settings, state);
+    else
+        serializeBinaryBulkStateSuffixImpl(settings, state);
+}
+
+void IDataType::deserializeBinaryBulkStatePrefix(
+    DeserializeBinaryBulkSettings & settings,
+    DeserializeBinaryBulkStatePtr & state) const
+{
+    if (custom_streams)
+        custom_streams->deserializeBinaryBulkStatePrefix(settings, state);
+    else
+        deserializeBinaryBulkStatePrefixImpl(settings, state);
+}
+
+void IDataType::serializeBinaryBulkWithMultipleStreams(
+    const IColumn & column,
+    size_t offset,
+    size_t limit,
+    SerializeBinaryBulkSettings & settings,
+    SerializeBinaryBulkStatePtr & state) const
+{
+    if (custom_streams)
+        custom_streams->serializeBinaryBulkWithMultipleStreams(column, offset, limit, settings, state);
+    else
+        serializeBinaryBulkWithMultipleStreamsImpl(column, offset, limit, settings, state);
+}
+
+void IDataType::deserializeBinaryBulkWithMultipleStreamsImpl(
+    IColumn & column,
+    size_t limit,
+    DeserializeBinaryBulkSettings & settings,
+    DeserializeBinaryBulkStatePtr & /* state */,
+    SubstreamsCache * /* cache */) const
+{
+    if (ReadBuffer * stream = settings.getter(settings.path))
+        deserializeBinaryBulk(column, *stream, limit, settings.avg_value_size_hint);
+}
+
+
+void IDataType::deserializeBinaryBulkWithMultipleStreams(
+    ColumnPtr & column,
+    size_t limit,
+    DeserializeBinaryBulkSettings & settings,
+    DeserializeBinaryBulkStatePtr & state,
+    SubstreamsCache * cache) const
+{
+    if (custom_streams)
+    {
+        custom_streams->deserializeBinaryBulkWithMultipleStreams(column, limit, settings, state, cache);
+        return;
+    }
+
+    /// Do not cache complex type, because they can be constructed
+    /// from their subcolumns, which are in cache.
+    if (!haveSubtypes())
+    {
+        auto cached_column = getFromSubstreamsCache(cache, settings.path);
+        if (cached_column)
+        {
+            column = cached_column;
+            return;
+        }
+    }
+
+    auto mutable_column = column->assumeMutable();
+    deserializeBinaryBulkWithMultipleStreamsImpl(*mutable_column, limit, settings, state, cache);
+    column = std::move(mutable_column);
+
+    if (!haveSubtypes())
+        addToSubstreamsCache(cache, settings.path, column);
+}
+
 void IDataType::serializeAsTextEscaped(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const
 {
     if (custom_text_serialization)
@@ -243,6 +427,27 @@ void IDataType::setCustomization(DataTypeCustomDescPtr custom_desc_) const
 
     if (custom_desc_->text_serialization)
         custom_text_serialization = std::move(custom_desc_->text_serialization);
+
+    if (custom_desc_->streams)
+        custom_streams = std::move(custom_desc_->streams);
+}
+
+void IDataType::addToSubstreamsCache(SubstreamsCache * cache, const SubstreamPath & path, ColumnPtr column)
+{
+    if (cache && !path.empty())
+        cache->emplace(getSubcolumnNameForStream(path), column);
+}
+
+ColumnPtr IDataType::getFromSubstreamsCache(SubstreamsCache * cache, const SubstreamPath & path)
+{
+    if (!cache || path.empty())
+        return nullptr;
+
+    auto it = cache->find(getSubcolumnNameForStream(path));
+    if (it == cache->end())
+        return nullptr;
+
+    return it->second;
 }
 
 }
diff --git a/src/DataTypes/IDataType.h b/src/DataTypes/IDataType.h
index b51722ed96d5..b67c5ee18462 100644
--- a/src/DataTypes/IDataType.h
+++ b/src/DataTypes/IDataType.h
@@ -3,7 +3,9 @@
 #include <memory>
 #include <Common/COW.h>
 #include <boost/noncopyable.hpp>
-#include <DataTypes/DataTypeCustom.h>
+#include <Core/Names.h>
+#include <Core/Types.h>
+#include <DataTypes/DataTypeCustom_fwd.h>
 
 
 namespace DB
@@ -27,6 +29,8 @@ using DataTypes = std::vector<DataTypePtr>;
 class ProtobufReader;
 class ProtobufWriter;
 
+struct NameAndTypePair;
+
 
 /** Properties of data type.
   * Contains methods for serialization/deserialization.
@@ -91,30 +95,42 @@ class IDataType : private boost::noncopyable
 
             TupleElement,
 
-            MapElement,
-
             DictionaryKeys,
             DictionaryIndexes,
         };
         Type type;
 
-        /// Index of tuple element, starting at 1.
+        /// Index of tuple element, starting at 1 or name.
         String tuple_element_name;
 
+        /// Do we need to escape a dot in filenames for tuple elements.
+        bool escape_tuple_delimiter = true;
+
         Substream(Type type_) : type(type_) {}
+
+        String toString() const;
+    };
+
+    struct SubstreamPath : public std::vector<Substream>
+    {
+        String toString() const;
     };
 
-    using SubstreamPath = std::vector<Substream>;
+    /// Cache for common substreams of one type, but possible different its subcolumns.
+    /// E.g. sizes of arrays of Nested data type.
+    using SubstreamsCache = std::unordered_map<String, ColumnPtr>;
 
     using StreamCallback = std::function<void(const SubstreamPath &, const IDataType &)>;
 
-    virtual void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const
-    {
-        callback(path, *this);
-    }
+    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const;
     void enumerateStreams(const StreamCallback & callback, SubstreamPath && path) const { enumerateStreams(callback, path); }
     void enumerateStreams(const StreamCallback & callback) const { enumerateStreams(callback, {}); }
 
+    virtual DataTypePtr tryGetSubcolumnType(const String & /* subcolumn_name */) const { return nullptr; }
+    DataTypePtr getSubcolumnType(const String & subcolumn_name) const;
+    virtual ColumnPtr getSubcolumn(const String & subcolumn_name, const IColumn & column) const;
+    Names getSubcolumnNames() const;
+
     using OutputStreamGetter = std::function<WriteBuffer*(const SubstreamPath &)>;
     using InputStreamGetter = std::function<ReadBuffer*(const SubstreamPath &)>;
 
@@ -155,19 +171,19 @@ class IDataType : private boost::noncopyable
     };
 
     /// Call before serializeBinaryBulkWithMultipleStreams chain to write something before first mark.
-    virtual void serializeBinaryBulkStatePrefix(
-            SerializeBinaryBulkSettings & /*settings*/,
-            SerializeBinaryBulkStatePtr & /*state*/) const {}
+    void serializeBinaryBulkStatePrefix(
+        SerializeBinaryBulkSettings & settings,
+        SerializeBinaryBulkStatePtr & state) const;
 
     /// Call after serializeBinaryBulkWithMultipleStreams chain to finish serialization.
-    virtual void serializeBinaryBulkStateSuffix(
-        SerializeBinaryBulkSettings & /*settings*/,
-        SerializeBinaryBulkStatePtr & /*state*/) const {}
+    void serializeBinaryBulkStateSuffix(
+        SerializeBinaryBulkSettings & settings,
+        SerializeBinaryBulkStatePtr & state) const;
 
     /// Call before before deserializeBinaryBulkWithMultipleStreams chain to get DeserializeBinaryBulkStatePtr.
-    virtual void deserializeBinaryBulkStatePrefix(
-        DeserializeBinaryBulkSettings & /*settings*/,
-        DeserializeBinaryBulkStatePtr & /*state*/) const {}
+    void deserializeBinaryBulkStatePrefix(
+        DeserializeBinaryBulkSettings & settings,
+        DeserializeBinaryBulkStatePtr & state) const;
 
     /** 'offset' and 'limit' are used to specify range.
       * limit = 0 - means no limit.
@@ -175,27 +191,20 @@ class IDataType : private boost::noncopyable
       * offset + limit could be greater than size of column
       *  - in that case, column is serialized till the end.
       */
-    virtual void serializeBinaryBulkWithMultipleStreams(
+    void serializeBinaryBulkWithMultipleStreams(
         const IColumn & column,
         size_t offset,
         size_t limit,
         SerializeBinaryBulkSettings & settings,
-        SerializeBinaryBulkStatePtr & /*state*/) const
-    {
-        if (WriteBuffer * stream = settings.getter(settings.path))
-            serializeBinaryBulk(column, *stream, offset, limit);
-    }
+        SerializeBinaryBulkStatePtr & state) const;
 
     /// Read no more than limit values and append them into column.
-    virtual void deserializeBinaryBulkWithMultipleStreams(
-        IColumn & column,
+    void deserializeBinaryBulkWithMultipleStreams(
+        ColumnPtr & column,
         size_t limit,
         DeserializeBinaryBulkSettings & settings,
-        DeserializeBinaryBulkStatePtr & /*state*/) const
-    {
-        if (ReadBuffer * stream = settings.getter(settings.path))
-            deserializeBinaryBulk(column, *stream, limit, settings.avg_value_size_hint);
-    }
+        DeserializeBinaryBulkStatePtr & state,
+        SubstreamsCache * cache = nullptr) const;
 
     /** Override these methods for data types that require just single stream (most of data types).
       */
@@ -268,6 +277,41 @@ class IDataType : private boost::noncopyable
 protected:
     virtual String doGetName() const;
 
+    virtual void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const
+    {
+        callback(path, *this);
+    }
+
+    virtual void serializeBinaryBulkStatePrefixImpl(
+        SerializeBinaryBulkSettings & /*settings*/,
+        SerializeBinaryBulkStatePtr & /*state*/) const {}
+
+    virtual void serializeBinaryBulkStateSuffixImpl(
+        SerializeBinaryBulkSettings & /*settings*/,
+        SerializeBinaryBulkStatePtr & /*state*/) const {}
+
+    virtual void deserializeBinaryBulkStatePrefixImpl(
+        DeserializeBinaryBulkSettings & /*settings*/,
+        DeserializeBinaryBulkStatePtr & /*state*/) const {}
+
+    virtual void serializeBinaryBulkWithMultipleStreamsImpl(
+        const IColumn & column,
+        size_t offset,
+        size_t limit,
+        SerializeBinaryBulkSettings & settings,
+        SerializeBinaryBulkStatePtr & /*state*/) const
+    {
+        if (WriteBuffer * stream = settings.getter(settings.path))
+            serializeBinaryBulk(column, *stream, offset, limit);
+    }
+
+    virtual void deserializeBinaryBulkWithMultipleStreamsImpl(
+        IColumn & column,
+        size_t limit,
+        DeserializeBinaryBulkSettings & settings,
+        DeserializeBinaryBulkStatePtr & state,
+        SubstreamsCache * cache) const;
+
     /// Default implementations of text serialization in case of 'custom_text_serialization' is not set.
 
     virtual void serializeTextEscaped(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings &) const = 0;
@@ -286,6 +330,9 @@ class IDataType : private boost::noncopyable
     }
 
 public:
+    static void addToSubstreamsCache(SubstreamsCache * cache, const SubstreamPath & path, ColumnPtr column);
+    static ColumnPtr getFromSubstreamsCache(SubstreamsCache * cache, const SubstreamPath & path);
+
     /** Create empty column for corresponding type.
       */
     virtual MutableColumnPtr createColumn() const = 0;
@@ -443,7 +490,8 @@ class IDataType : private boost::noncopyable
     /// Updates avg_value_size_hint for newly read column. Uses to optimize deserialization. Zero expected for first column.
     static void updateAvgValueSizeHint(const IColumn & column, double & avg_value_size_hint);
 
-    static String getFileNameForStream(const String & column_name, const SubstreamPath & path);
+    static String getFileNameForStream(const NameAndTypePair & column, const SubstreamPath & path);
+    static String getSubcolumnNameForStream(const SubstreamPath & path);
 
     /// Substream path supports special compression methods like codec Delta.
     /// For all other substreams (like ArraySizes, NullMasks, etc.) we use only
@@ -458,9 +506,11 @@ class IDataType : private boost::noncopyable
     /// This is mutable to allow setting custom name and serialization on `const IDataType` post construction.
     mutable DataTypeCustomNamePtr custom_name;
     mutable DataTypeCustomTextSerializationPtr custom_text_serialization;
+    mutable DataTypeCustomStreamsPtr custom_streams;
 
 public:
     const IDataTypeCustomName * getCustomName() const { return custom_name.get(); }
+    const IDataTypeCustomStreams * getCustomStreams() const { return custom_streams.get(); }
 };
 
 
diff --git a/src/DataTypes/NestedUtils.cpp b/src/DataTypes/NestedUtils.cpp
index 0537fa5cdc19..6c13eea0a1b7 100644
--- a/src/DataTypes/NestedUtils.cpp
+++ b/src/DataTypes/NestedUtils.cpp
@@ -7,6 +7,7 @@
 #include <DataTypes/DataTypeArray.h>
 #include <DataTypes/DataTypeTuple.h>
 #include <DataTypes/NestedUtils.h>
+#include <DataTypes/DataTypeNested.h>
 
 #include <Columns/ColumnArray.h>
 #include <Columns/ColumnTuple.h>
@@ -84,7 +85,8 @@ Block flatten(const Block & block)
 
     for (const auto & elem : block)
     {
-        if (const DataTypeArray * type_arr = typeid_cast<const DataTypeArray *>(elem.type.get()))
+        const DataTypeArray * type_arr = typeid_cast<const DataTypeArray *>(elem.type.get());
+        if (type_arr)
         {
             const DataTypeTuple * type_tuple = typeid_cast<const DataTypeTuple *>(type_arr->getNestedType().get());
             if (type_tuple && type_tuple->haveExplicitNames())
@@ -128,32 +130,67 @@ Block flatten(const Block & block)
     return res;
 }
 
-
-NamesAndTypesList collect(const NamesAndTypesList & names_and_types)
+namespace
 {
-    NamesAndTypesList res;
 
-    std::map<std::string, NamesAndTypesList> nested;
+using NameToDataType = std::map<String, DataTypePtr>;
+
+NameToDataType getSubcolumnsOfNested(const NamesAndTypesList & names_and_types)
+{
+    std::unordered_map<String, NamesAndTypesList> nested;
     for (const auto & name_type : names_and_types)
     {
-        bool collected = false;
-        if (const DataTypeArray * type_arr = typeid_cast<const DataTypeArray *>(name_type.type.get()))
+        const DataTypeArray * type_arr = typeid_cast<const DataTypeArray *>(name_type.type.get());
+
+        /// Ignore true Nested type, but try to unite flatten arrays to Nested type.
+        if (!isNested(name_type.type) && type_arr)
         {
             auto split = splitName(name_type.name);
             if (!split.second.empty())
-            {
                 nested[split.first].emplace_back(split.second, type_arr->getNestedType());
-                collected = true;
-            }
         }
+    }
+
+    std::map<String, DataTypePtr> nested_types;
+
+    for (const auto & [name, elems] : nested)
+        nested_types.emplace(name, createNested(elems.getTypes(), elems.getNames()));
+
+    return nested_types;
+}
+
+}
 
-        if (!collected)
+NamesAndTypesList collect(const NamesAndTypesList & names_and_types)
+{
+    NamesAndTypesList res;
+    auto nested_types = getSubcolumnsOfNested(names_and_types);
+
+    for (const auto & name_type : names_and_types)
+        if (!nested_types.count(splitName(name_type.name).first))
             res.push_back(name_type);
-    }
 
-    for (const auto & name_elems : nested)
-        res.emplace_back(name_elems.first, std::make_shared<DataTypeArray>(
-            std::make_shared<DataTypeTuple>(name_elems.second.getTypes(), name_elems.second.getNames())));
+    for (const auto & name_type : nested_types)
+        res.emplace_back(name_type.first, name_type.second);
+
+    return res;
+}
+
+NamesAndTypesList convertToSubcolumns(const NamesAndTypesList & names_and_types)
+{
+    auto nested_types = getSubcolumnsOfNested(names_and_types);
+    auto res = names_and_types;
+
+    for (auto & name_type : res)
+    {
+        auto split = splitName(name_type.name);
+        if (name_type.isSubcolumn() || split.second.empty())
+            continue;
+
+        auto it = nested_types.find(split.first);
+        if (it != nested_types.end())
+            name_type = NameAndTypePair{split.first, split.second, it->second, it->second->getSubcolumnType(split.second)};
+    }
 
     return res;
 }
diff --git a/src/DataTypes/NestedUtils.h b/src/DataTypes/NestedUtils.h
index 3039fd7f118a..b8428b96d3e9 100644
--- a/src/DataTypes/NestedUtils.h
+++ b/src/DataTypes/NestedUtils.h
@@ -23,6 +23,9 @@ namespace Nested
     /// Collect Array columns in a form of `column_name.element_name` to single Array(Tuple(...)) column.
     NamesAndTypesList collect(const NamesAndTypesList & names_and_types);
 
+    /// Convert old-style nested (single arrays with same prefix, `n.a`, `n.b`...) to subcolumns of data type Nested.
+    NamesAndTypesList convertToSubcolumns(const NamesAndTypesList & names_and_types);
+
     /// Check that sizes of arrays - elements of nested data structures - are equal.
     void validateArraySizes(const Block & block);
 }
diff --git a/src/DataTypes/ya.make b/src/DataTypes/ya.make
index db6793db6ca9..356424af8ddc 100644
--- a/src/DataTypes/ya.make
+++ b/src/DataTypes/ya.make
@@ -28,9 +28,11 @@ SRCS(
     DataTypeLowCardinality.cpp
     DataTypeLowCardinalityHelpers.cpp
     DataTypeMap.cpp
+    DataTypeNested.cpp
     DataTypeNothing.cpp
     DataTypeNullable.cpp
     DataTypeNumberBase.cpp
+    DataTypeOneElementTuple.cpp
     DataTypeString.cpp
     DataTypeTuple.cpp
     DataTypeUUID.cpp
diff --git a/src/Interpreters/InterpreterCreateQuery.cpp b/src/Interpreters/InterpreterCreateQuery.cpp
index 9b087b3d2e5a..e9a11b9eb0df 100644
--- a/src/Interpreters/InterpreterCreateQuery.cpp
+++ b/src/Interpreters/InterpreterCreateQuery.cpp
@@ -465,7 +465,8 @@ ColumnsDescription InterpreterCreateQuery::getColumnsDescription(
         res.add(std::move(column));
     }
 
-    res.flattenNested();
+    if (context.getSettingsRef().flatten_nested)
+        res.flattenNested();
 
     if (res.getAllPhysical().empty())
         throw Exception{"Cannot CREATE table without physical columns", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED};
diff --git a/src/Interpreters/TreeRewriter.cpp b/src/Interpreters/TreeRewriter.cpp
index 554fedeed64e..321bbf988198 100644
--- a/src/Interpreters/TreeRewriter.cpp
+++ b/src/Interpreters/TreeRewriter.cpp
@@ -526,7 +526,12 @@ void TreeRewriterResult::collectSourceColumns(bool add_special)
     {
         const ColumnsDescription & columns = metadata_snapshot->getColumns();
 
-        auto columns_from_storage = add_special ? columns.getAll() : columns.getAllPhysical();
+        NamesAndTypesList columns_from_storage;
+        if (storage->supportsSubcolumns())
+            columns_from_storage = add_special ? columns.getAllWithSubcolumns() : columns.getAllPhysicalWithSubcolumns();
+        else
+            columns_from_storage = add_special ? columns.getAll() : columns.getAllPhysical();
+
         if (source_columns.empty())
             source_columns.swap(columns_from_storage);
         else
diff --git a/src/Storages/AlterCommands.cpp b/src/Storages/AlterCommands.cpp
index 59b40ffdfbb7..7043a32760b3 100644
--- a/src/Storages/AlterCommands.cpp
+++ b/src/Storages/AlterCommands.cpp
@@ -320,7 +320,8 @@ void AlterCommand::apply(StorageInMemoryMetadata & metadata, const Context & con
         metadata.columns.add(column, after_column, first);
 
         /// Slow, because each time a list is copied
-        metadata.columns.flattenNested();
+        if (context.getSettingsRef().flatten_nested)
+            metadata.columns.flattenNested();
     }
     else if (type == DROP_COLUMN)
     {
diff --git a/src/Storages/ColumnsDescription.cpp b/src/Storages/ColumnsDescription.cpp
index b43aab71af2c..b4ddde7c0f31 100644
--- a/src/Storages/ColumnsDescription.cpp
+++ b/src/Storages/ColumnsDescription.cpp
@@ -19,6 +19,7 @@
 #include <DataTypes/NestedUtils.h>
 #include <DataTypes/DataTypeArray.h>
 #include <DataTypes/DataTypeTuple.h>
+#include <DataTypes/DataTypeNested.h>
 #include <Common/Exception.h>
 #include <Interpreters/Context.h>
 #include <Storages/IStorage.h>
@@ -184,6 +185,7 @@ void ColumnsDescription::add(ColumnDescription column, const String & after_colu
         insert_it = range.second;
     }
 
+    addSubcolumns(column.name, column.type);
     columns.get<0>().insert(insert_it, std::move(column));
 }
 
@@ -195,7 +197,10 @@ void ColumnsDescription::remove(const String & column_name)
             ErrorCodes::NO_SUCH_COLUMN_IN_TABLE);
 
     for (auto list_it = range.first; list_it != range.second;)
+    {
+        removeSubcolumns(list_it->name, list_it->type);
         list_it = columns.get<0>().erase(list_it);
+    }
 }
 
 void ColumnsDescription::rename(const String & column_from, const String & column_to)
@@ -268,6 +273,7 @@ void ColumnsDescription::flattenNested()
         }
 
         ColumnDescription column = std::move(*it);
+        removeSubcolumns(column.name, column.type);
         it = columns.get<0>().erase(it);
 
         const DataTypes & elements = type_tuple->getElements();
@@ -281,6 +287,7 @@ void ColumnsDescription::flattenNested()
             nested_column.name = Nested::concatenateName(column.name, names[i]);
             nested_column.type = std::make_shared<DataTypeArray>(elements[i]);
 
+            addSubcolumns(nested_column.name, nested_column.type);
             columns.get<0>().insert(it, std::move(nested_column));
         }
     }
@@ -322,10 +329,10 @@ NamesAndTypesList ColumnsDescription::getAll() const
     return ret;
 }
 
-
 bool ColumnsDescription::has(const String & column_name) const
 {
-    return columns.get<1>().find(column_name) != columns.get<1>().end();
+    return columns.get<1>().find(column_name) != columns.get<1>().end()
+        || subcolumns.find(column_name) != subcolumns.end();
 }
 
 bool ColumnsDescription::hasNested(const String & column_name) const
@@ -371,12 +378,56 @@ NameAndTypePair ColumnsDescription::getPhysical(const String & column_name) cons
     return NameAndTypePair(it->name, it->type);
 }
 
+NameAndTypePair ColumnsDescription::getPhysicalOrSubcolumn(const String & column_name) const
+{
+    if (auto it = columns.get<1>().find(column_name); it != columns.get<1>().end()
+        && it->default_desc.kind != ColumnDefaultKind::Alias)
+    {
+        return NameAndTypePair(it->name, it->type);
+    }
+
+    if (auto it = subcolumns.find(column_name); it != subcolumns.end())
+    {
+        return it->second;
+    }
+
+    throw Exception(ErrorCodes::NO_SUCH_COLUMN_IN_TABLE,
+        "There is no physical column or subcolumn {} in table.", column_name);
+}
+
 bool ColumnsDescription::hasPhysical(const String & column_name) const
 {
     auto it = columns.get<1>().find(column_name);
     return it != columns.get<1>().end() && it->default_desc.kind != ColumnDefaultKind::Alias;
 }
 
+bool ColumnsDescription::hasPhysicalOrSubcolumn(const String & column_name) const
+{
+    return hasPhysical(column_name) || subcolumns.find(column_name) != subcolumns.end();
+}
+
+static NamesAndTypesList getWithSubcolumns(NamesAndTypesList && source_list)
+{
+    NamesAndTypesList ret;
+    for (const auto & col : source_list)
+    {
+        ret.emplace_back(col.name, col.type);
+        for (const auto & subcolumn : col.type->getSubcolumnNames())
+            ret.emplace_back(col.name, subcolumn, col.type, col.type->getSubcolumnType(subcolumn));
+    }
+
+    return ret;
+}
+
+NamesAndTypesList ColumnsDescription::getAllWithSubcolumns() const
+{
+    return getWithSubcolumns(getAll());
+}
+
+NamesAndTypesList ColumnsDescription::getAllPhysicalWithSubcolumns() const
+{
+    return getWithSubcolumns(getAllPhysical());
+}
 
 bool ColumnsDescription::hasDefaults() const
 {
@@ -483,13 +534,33 @@ ColumnsDescription ColumnsDescription::parse(const String & str)
         ColumnDescription column;
         column.readText(buf);
         buf.ignore(1); /// ignore new line
-        result.add(std::move(column));
+        result.add(column);
     }
 
     assertEOF(buf);
     return result;
 }
 
+void ColumnsDescription::addSubcolumns(const String & name_in_storage, const DataTypePtr & type_in_storage)
+{
+    for (const auto & subcolumn_name : type_in_storage->getSubcolumnNames())
+    {
+        auto subcolumn = NameAndTypePair(name_in_storage, subcolumn_name,
+            type_in_storage, type_in_storage->getSubcolumnType(subcolumn_name));
+
+        if (has(subcolumn.name))
+            throw Exception(ErrorCodes::ILLEGAL_COLUMN,
+                "Cannot add subcolumn {}: column with this name already exists", subcolumn.name);
+
+        subcolumns[subcolumn.name] = subcolumn;
+    }
+}
+
+void ColumnsDescription::removeSubcolumns(const String & name_in_storage, const DataTypePtr & type_in_storage)
+{
+    for (const auto & subcolumn_name : type_in_storage->getSubcolumnNames())
+        subcolumns.erase(name_in_storage + "." + subcolumn_name);
+}
 
 Block validateColumnsDefaultsAndGetSampleBlock(ASTPtr default_expr_list, const NamesAndTypesList & all_columns, const Context & context)
 {
diff --git a/src/Storages/ColumnsDescription.h b/src/Storages/ColumnsDescription.h
index 1df6c6ae67b6..1990c565b656 100644
--- a/src/Storages/ColumnsDescription.h
+++ b/src/Storages/ColumnsDescription.h
@@ -77,6 +77,8 @@ class ColumnsDescription
     NamesAndTypesList getAliases() const;
     NamesAndTypesList getAllPhysical() const; /// ordinary + materialized.
     NamesAndTypesList getAll() const; /// ordinary + materialized + aliases
+    NamesAndTypesList getAllWithSubcolumns() const;
+    NamesAndTypesList getAllPhysicalWithSubcolumns() const;
 
     using ColumnTTLs = std::unordered_map<String, ASTPtr>;
     ColumnTTLs getColumnTTLs() const;
@@ -105,7 +107,9 @@ class ColumnsDescription
 
     Names getNamesOfPhysical() const;
     bool hasPhysical(const String & column_name) const;
+    bool hasPhysicalOrSubcolumn(const String & column_name) const;
     NameAndTypePair getPhysical(const String & column_name) const;
+    NameAndTypePair getPhysicalOrSubcolumn(const String & column_name) const;
 
     ColumnDefaults getDefaults() const; /// TODO: remove
     bool hasDefault(const String & column_name) const;
@@ -141,7 +145,12 @@ class ColumnsDescription
 private:
     Container columns;
 
+    using SubcolumnsContainer = std::unordered_map<String, NameAndTypePair>;
+    SubcolumnsContainer subcolumns;
+
     void modifyColumnOrder(const String & column_name, const String & after_column, bool first);
+    void addSubcolumns(const String & name_in_storage, const DataTypePtr & type_in_storage);
+    void removeSubcolumns(const String & name_in_storage, const DataTypePtr & type_in_storage);
 };
 
 /// Validate default expressions and corresponding types compatibility, i.e.
diff --git a/src/Storages/IStorage.h b/src/Storages/IStorage.h
index cfb4c4e9646d..031b960fac11 100644
--- a/src/Storages/IStorage.h
+++ b/src/Storages/IStorage.h
@@ -128,6 +128,9 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo
     /// Example is StorageSystemNumbers.
     virtual bool hasEvenlyDistributedRead() const { return false; }
 
+    /// Returns true if the storage supports reading of subcolumns of complex types.
+    virtual bool supportsSubcolumns() const { return false; }
+
 
     /// Optional size information of each physical column.
     /// Currently it's only used by the MergeTree family for query optimizations.
diff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp
index 9aefe69fce71..5a2c17a7b98c 100644
--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp
+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp
@@ -17,6 +17,7 @@
 #include <common/logger_useful.h>
 #include <Compression/getCompressionCodecForFile.h>
 #include <Parsers/queryToString.h>
+#include <DataTypes/NestedUtils.h>
 
 
 namespace CurrentMetrics
@@ -321,7 +322,12 @@ void IMergeTreeDataPart::setColumns(const NamesAndTypesList & new_columns)
     column_name_to_position.reserve(new_columns.size());
     size_t pos = 0;
     for (const auto & column : columns)
-        column_name_to_position.emplace(column.name, pos++);
+    {
+        column_name_to_position.emplace(column.name, pos);
+        for (const auto & subcolumn : column.type->getSubcolumnNames())
+            column_name_to_position.emplace(Nested::concatenateName(column.name, subcolumn), pos);
+        ++pos;
+    }
 }
 
 void IMergeTreeDataPart::removeIfNeeded()
@@ -454,7 +460,7 @@ String IMergeTreeDataPart::getColumnNameWithMinimumCompressedSize(const StorageM
         if (alter_conversions.isColumnRenamed(column.name))
             column_name = alter_conversions.getColumnOldName(column.name);
 
-        if (!hasColumnFiles(column_name, *column_type))
+        if (!hasColumnFiles(column))
             continue;
 
         const auto size = getColumnSize(column_name, *column_type).data_compressed;
@@ -640,7 +646,7 @@ CompressionCodecPtr IMergeTreeDataPart::detectDefaultCompressionCodec() const
             {
                 if (path_to_data_file.empty())
                 {
-                    String candidate_path = getFullRelativePath() + IDataType::getFileNameForStream(part_column.name, substream_path) + ".bin";
+                    String candidate_path = getFullRelativePath() + IDataType::getFileNameForStream(part_column, substream_path) + ".bin";
 
                     /// We can have existing, but empty .bin files. Example: LowCardinality(Nullable(...)) columns and column_name.dict.null.bin file.
                     if (volume->getDisk()->exists(candidate_path) && volume->getDisk()->getFileSize(candidate_path) != 0)
diff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h
index 9c8247ba9bdd..2f531bd83911 100644
--- a/src/Storages/MergeTree/IMergeTreeDataPart.h
+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h
@@ -330,7 +330,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar
     /// NOTE: Doesn't take column renames into account, if some column renames
     /// take place, you must take original name of column for this part from
     /// storage and pass it to this method.
-    virtual bool hasColumnFiles(const String & /* column */, const IDataType & /* type */) const { return false; }
+    virtual bool hasColumnFiles(const NameAndTypePair & /* column */) const { return false; }
 
     /// Returns true if this part shall participate in merges according to
     /// settings of given storage policy.
diff --git a/src/Storages/MergeTree/IMergeTreeReader.cpp b/src/Storages/MergeTree/IMergeTreeReader.cpp
index 4379ac09af01..0140b32e12cf 100644
--- a/src/Storages/MergeTree/IMergeTreeReader.cpp
+++ b/src/Storages/MergeTree/IMergeTreeReader.cpp
@@ -42,7 +42,14 @@ IMergeTreeReader::IMergeTreeReader(
     , all_mark_ranges(all_mark_ranges_)
     , alter_conversions(storage.getAlterConversionsForPart(data_part))
 {
-    for (const NameAndTypePair & column_from_part : data_part->getColumns())
+    auto part_columns = data_part->getColumns();
+    if (settings.convert_nested_to_subcolumns)
+    {
+        columns = Nested::convertToSubcolumns(columns);
+        part_columns = Nested::collect(part_columns);
+    }
+
+    for (const NameAndTypePair & column_from_part : part_columns)
         columns_from_part[column_from_part.name] = column_from_part.type;
 }
 
@@ -74,7 +81,6 @@ static bool arrayHasNoElementsRead(const IColumn & column)
     return last_offset != 0;
 }
 
-
 void IMergeTreeReader::fillMissingColumns(Columns & res_columns, bool & should_evaluate_missing_defaults, size_t num_rows)
 {
     try
@@ -197,19 +203,33 @@ void IMergeTreeReader::evaluateMissingDefaults(Block additional_columns, Columns
 
 NameAndTypePair IMergeTreeReader::getColumnFromPart(const NameAndTypePair & required_column) const
 {
-    if (alter_conversions.isColumnRenamed(required_column.name))
+    auto name_in_storage = required_column.getNameInStorage();
+
+    decltype(columns_from_part.begin()) it;
+    if (alter_conversions.isColumnRenamed(name_in_storage))
     {
-        String old_name = alter_conversions.getColumnOldName(required_column.name);
-        auto it = columns_from_part.find(old_name);
-        if (it != columns_from_part.end())
-            return {it->first, it->second};
+        String old_name = alter_conversions.getColumnOldName(name_in_storage);
+        it = columns_from_part.find(old_name);
     }
-    else if (auto it = columns_from_part.find(required_column.name); it != columns_from_part.end())
+    else
     {
-        return {it->first, it->second};
+        it = columns_from_part.find(name_in_storage);
+    }
+
+    if (it == columns_from_part.end())
+        return required_column;
+
+    if (required_column.isSubcolumn())
+    {
+        auto subcolumn_name = required_column.getSubcolumnName();
+        auto subcolumn_type = it->second->tryGetSubcolumnType(subcolumn_name);
+        if (!subcolumn_type)
+            subcolumn_type = required_column.type;
+
+        return {it->first, subcolumn_name, it->second, subcolumn_type};
     }
 
-    return required_column;
+    return {it->first, it->second};
 }
 
 void IMergeTreeReader::performRequiredConversions(Columns & res_columns)
diff --git a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp
index 1ea74d9fb27f..7e562ae03d63 100644
--- a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp
+++ b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp
@@ -33,7 +33,7 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(
         column.type->enumerateStreams(
             [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_path */)
             {
-                ++stream_counts[IDataType::getFileNameForStream(column.name, substream_path)];
+                ++stream_counts[IDataType::getFileNameForStream(column, substream_path)];
             },
             {});
     }
@@ -42,9 +42,13 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(
     const String mrk_extension = data_part->getMarksFileExtension();
     for (const auto & column_name : empty_columns)
     {
+        auto column_with_type = columns.tryGetByName(column_name);
+        if (!column_with_type)
+           continue;
+
         IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_path */)
         {
-            String stream_name = IDataType::getFileNameForStream(column_name, substream_path);
+            String stream_name = IDataType::getFileNameForStream(*column_with_type, substream_path);
             /// Delete files if they are no longer shared with another column.
             if (--stream_counts[stream_name] == 0)
             {
@@ -52,10 +56,9 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(
                 remove_files.emplace(stream_name + mrk_extension);
             }
         };
+
         IDataType::SubstreamPath stream_path;
-        auto column_with_type = columns.tryGetByName(column_name);
-        if (column_with_type)
-            column_with_type->type->enumerateStreams(callback, stream_path);
+        column_with_type->type->enumerateStreams(callback, stream_path);
     }
 
     /// Remove files on disk and checksums
diff --git a/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp b/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp
index ad10a437b1ec..f8b5e0a9c0a0 100644
--- a/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp
+++ b/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp
@@ -1,5 +1,6 @@
 #include <Storages/MergeTree/MergeTreeBlockReadUtils.h>
 #include <Storages/MergeTree/MergeTreeData.h>
+#include <Core/NamesAndTypes.h>
 #include <Common/checkStackSize.h>
 #include <Common/typeid_cast.h>
 #include <Columns/ColumnConst.h>
@@ -33,21 +34,30 @@ bool injectRequiredColumnsRecursively(
     /// huge AST which for some reason was not validated on parsing/interpreter
     /// stages.
     checkStackSize();
-    String column_name_in_part = column_name;
-    if (alter_conversions.isColumnRenamed(column_name_in_part))
-        column_name_in_part = alter_conversions.getColumnOldName(column_name_in_part);
 
-    /// column has files and hence does not require evaluation
-    if (storage_columns.hasPhysical(column_name) && part->hasColumnFiles(column_name_in_part, *storage_columns.getPhysical(column_name).type))
+    if (storage_columns.hasPhysicalOrSubcolumn(column_name))
     {
-        /// ensure each column is added only once
-        if (required_columns.count(column_name) == 0)
+        auto column_in_storage = storage_columns.getPhysicalOrSubcolumn(column_name);
+        auto column_name_in_part = column_in_storage.getNameInStorage();
+        if (alter_conversions.isColumnRenamed(column_name_in_part))
+            column_name_in_part = alter_conversions.getColumnOldName(column_name_in_part);
+
+        auto column_in_part = NameAndTypePair(
+            column_name_in_part, column_in_storage.getSubcolumnName(),
+            column_in_storage.getTypeInStorage(), column_in_storage.type);
+
+        /// column has files and hence does not require evaluation
+        if (part->hasColumnFiles(column_in_part))
         {
-            columns.emplace_back(column_name);
-            required_columns.emplace(column_name);
-            injected_columns.emplace(column_name);
+            /// ensure each column is added only once
+            if (required_columns.count(column_name) == 0)
+            {
+                columns.emplace_back(column_name);
+                required_columns.emplace(column_name);
+                injected_columns.emplace(column_name);
+            }
+            return true;
         }
-        return true;
     }
 
     /// Column doesn't have default value and don't exist in part
@@ -81,8 +91,8 @@ NameSet injectRequiredColumns(const MergeTreeData & storage, const StorageMetada
     for (size_t i = 0; i < columns.size(); ++i)
     {
         /// We are going to fetch only physical columns
-        if (!storage_columns.hasPhysical(columns[i]))
-            throw Exception("There is no physical column " + columns[i] + " in table.", ErrorCodes::NO_SUCH_COLUMN_IN_TABLE);
+        if (!storage_columns.hasPhysicalOrSubcolumn(columns[i]))
+            throw Exception("There is no physical column or subcolumn " + columns[i] + " in table.", ErrorCodes::NO_SUCH_COLUMN_IN_TABLE);
 
         have_at_least_one_physical_column |= injectRequiredColumnsRecursively(
             columns[i], storage_columns, alter_conversions,
@@ -285,7 +295,7 @@ MergeTreeReadTaskColumns getReadTaskColumns(
 
     if (check_columns)
     {
-        const NamesAndTypesList & physical_columns = metadata_snapshot->getColumns().getAllPhysical();
+        const NamesAndTypesList & physical_columns = metadata_snapshot->getColumns().getAllWithSubcolumns();
         result.pre_columns = physical_columns.addTypes(pre_column_names);
         result.columns = physical_columns.addTypes(column_names);
     }
diff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h
index 6098cd22c989..9d0218158887 100644
--- a/src/Storages/MergeTree/MergeTreeData.h
+++ b/src/Storages/MergeTree/MergeTreeData.h
@@ -357,6 +357,8 @@ class MergeTreeData : public IStorage
             || merging_params.mode == MergingParams::VersionedCollapsing;
     }
 
+    bool supportsSubcolumns() const override { return true; }
+
     NamesAndTypesList getVirtuals() const override;
 
     bool mayBenefitFromIndexForIn(const ASTPtr & left_in_operand, const Context &, const StorageMetadataPtr & metadata_snapshot) const override;
diff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp
index 0704ad484356..f999aa67bbe8 100644
--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp
@@ -1493,7 +1493,7 @@ NameToNameVector MergeTreeDataMergerMutator::collectFilesForRenames(
         column.type->enumerateStreams(
             [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
             {
-                ++stream_counts[IDataType::getFileNameForStream(column.name, substream_path)];
+                ++stream_counts[IDataType::getFileNameForStream(column, substream_path)];
             },
             {});
     }
@@ -1511,7 +1511,7 @@ NameToNameVector MergeTreeDataMergerMutator::collectFilesForRenames(
         {
             IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
             {
-                String stream_name = IDataType::getFileNameForStream(command.column_name, substream_path);
+                String stream_name = IDataType::getFileNameForStream({command.column_name, command.data_type}, substream_path);
                 /// Delete files if they are no longer shared with another column.
                 if (--stream_counts[stream_name] == 0)
                 {
@@ -1532,7 +1532,7 @@ NameToNameVector MergeTreeDataMergerMutator::collectFilesForRenames(
 
             IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
             {
-                String stream_from = IDataType::getFileNameForStream(command.column_name, substream_path);
+                String stream_from = IDataType::getFileNameForStream({command.column_name, command.data_type}, substream_path);
 
                 String stream_to = boost::replace_first_copy(stream_from, escaped_name_from, escaped_name_to);
 
@@ -1565,7 +1565,7 @@ NameSet MergeTreeDataMergerMutator::collectFilesToSkip(
     {
         IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
         {
-            String stream_name = IDataType::getFileNameForStream(entry.name, substream_path);
+            String stream_name = IDataType::getFileNameForStream({entry.name, entry.type}, substream_path);
             files_to_skip.insert(stream_name + ".bin");
             files_to_skip.insert(stream_name + mrk_extension);
         };
diff --git a/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp
index c42caa2d4d4a..32f54e3b7829 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp
@@ -1,4 +1,5 @@
 #include "MergeTreeDataPartCompact.h"
+#include <DataTypes/NestedUtils.h>
 #include <Storages/MergeTree/MergeTreeReaderCompact.h>
 #include <Storages/MergeTree/MergeTreeDataPartWriterCompact.h>
 #include <Poco/File.h>
@@ -121,9 +122,9 @@ void MergeTreeDataPartCompact::loadIndexGranularity()
     index_granularity.setInitialized();
 }
 
-bool MergeTreeDataPartCompact::hasColumnFiles(const String & column_name, const IDataType &) const
+bool MergeTreeDataPartCompact::hasColumnFiles(const NameAndTypePair & column) const
 {
-    if (!getColumnPosition(column_name))
+    if (!getColumnPosition(column.name))
         return false;
 
     auto bin_checksum = checksums.files.find(DATA_FILE_NAME_WITH_EXTENSION);
diff --git a/src/Storages/MergeTree/MergeTreeDataPartCompact.h b/src/Storages/MergeTree/MergeTreeDataPartCompact.h
index 2f2a2f537aac..2c0c4020bb08 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartCompact.h
+++ b/src/Storages/MergeTree/MergeTreeDataPartCompact.h
@@ -55,7 +55,7 @@ class MergeTreeDataPartCompact : public IMergeTreeDataPart
 
     bool isStoredOnDisk() const override { return true; }
 
-    bool hasColumnFiles(const String & column_name, const IDataType & type) const override;
+    bool hasColumnFiles(const NameAndTypePair & column) const override;
 
     String getFileNameForColumn(const NameAndTypePair & /* column */) const override { return DATA_FILE_NAME; }
 
diff --git a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp
index 1d70ff9d6c4d..96fa411339c7 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp
@@ -3,6 +3,7 @@
 #include <Storages/MergeTree/MergedBlockOutputStream.h>
 #include <Storages/MergeTree/MergeTreeDataPartWriterInMemory.h>
 #include <Storages/MergeTree/IMergeTreeReader.h>
+#include <DataTypes/NestedUtils.h>
 #include <Interpreters/Context.h>
 #include <Poco/File.h>
 #include <Poco/Logger.h>
diff --git a/src/Storages/MergeTree/MergeTreeDataPartInMemory.h b/src/Storages/MergeTree/MergeTreeDataPartInMemory.h
index 1fceb47cba87..397d3d2036c3 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartInMemory.h
+++ b/src/Storages/MergeTree/MergeTreeDataPartInMemory.h
@@ -32,6 +32,7 @@ class MergeTreeDataPartInMemory : public IMergeTreeDataPart
         const MergeTreeReaderSettings & reader_settings_,
         const ValueSizeMap & avg_value_size_hints,
         const ReadBufferFromFileBase::ProfileCallback & profile_callback) const override;
+
     MergeTreeWriterPtr getWriter(
         const NamesAndTypesList & columns_list,
         const StorageMetadataPtr & metadata_snapshot,
@@ -41,7 +42,7 @@ class MergeTreeDataPartInMemory : public IMergeTreeDataPart
         const MergeTreeIndexGranularity & computed_index_granularity) const override;
 
     bool isStoredOnDisk() const override { return false; }
-    bool hasColumnFiles(const String & column_name, const IDataType & /* type */) const override { return !!getColumnPosition(column_name); }
+    bool hasColumnFiles(const NameAndTypePair & column) const override { return !!getColumnPosition(column.name); }
     String getFileNameForColumn(const NameAndTypePair & /* column */) const override { return ""; }
     void renameTo(const String & new_relative_path, bool remove_new_dir_if_exists) const override;
     void makeCloneInDetached(const String & prefix, const StorageMetadataPtr & metadata_snapshot) const override;
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp
index 39898a7cc57a..5dd8c26f224e 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp
@@ -3,6 +3,8 @@
 #include <Storages/MergeTree/MergeTreeReaderWide.h>
 #include <Storages/MergeTree/MergeTreeDataPartWriterWide.h>
 #include <Storages/MergeTree/IMergeTreeDataPartWriter.h>
+#include <DataTypes/NestedUtils.h>
+#include <Core/NamesAndTypes.h>
 
 
 namespace DB
@@ -46,10 +48,13 @@ IMergeTreeDataPart::MergeTreeReaderPtr MergeTreeDataPartWide::getReader(
     const ValueSizeMap & avg_value_size_hints,
     const ReadBufferFromFileBase::ProfileCallback & profile_callback) const
 {
+    auto new_settings = reader_settings;
+    new_settings.convert_nested_to_subcolumns = true;
+
     auto ptr = std::static_pointer_cast<const MergeTreeDataPartWide>(shared_from_this());
     return std::make_unique<MergeTreeReaderWide>(
         ptr, columns_to_read, metadata_snapshot, uncompressed_cache,
-        mark_cache, mark_ranges, reader_settings,
+        mark_cache, mark_ranges, new_settings,
         avg_value_size_hints, profile_callback);
 }
 
@@ -71,15 +76,15 @@ IMergeTreeDataPart::MergeTreeWriterPtr MergeTreeDataPartWide::getWriter(
 /// Takes into account the fact that several columns can e.g. share their .size substreams.
 /// When calculating totals these should be counted only once.
 ColumnSize MergeTreeDataPartWide::getColumnSizeImpl(
-    const String & column_name, const IDataType & type, std::unordered_set<String> * processed_substreams) const
+    const NameAndTypePair & column, std::unordered_set<String> * processed_substreams) const
 {
     ColumnSize size;
     if (checksums.empty())
         return size;
 
-    type.enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
+    column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
-        String file_name = IDataType::getFileNameForStream(column_name, substream_path);
+        String file_name = IDataType::getFileNameForStream(column, substream_path);
 
         if (processed_substreams && !processed_substreams->insert(file_name).second)
             return;
@@ -157,7 +162,7 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const
                 IDataType::SubstreamPath stream_path;
                 name_type.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
                 {
-                    String file_name = IDataType::getFileNameForStream(name_type.name, substream_path);
+                    String file_name = IDataType::getFileNameForStream(name_type, substream_path);
                     String mrk_file_name = file_name + index_granularity_info.marks_file_extension;
                     String bin_file_name = file_name + ".bin";
                     if (!checksums.files.count(mrk_file_name))
@@ -179,7 +184,7 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const
         {
             name_type.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
             {
-                auto file_path = path + IDataType::getFileNameForStream(name_type.name, substream_path) + index_granularity_info.marks_file_extension;
+                auto file_path = path + IDataType::getFileNameForStream(name_type, substream_path) + index_granularity_info.marks_file_extension;
 
                 /// Missing file is Ok for case when new column was added.
                 if (volume->getDisk()->exists(file_path))
@@ -201,13 +206,13 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const
     }
 }
 
-bool MergeTreeDataPartWide::hasColumnFiles(const String & column_name, const IDataType & type) const
+bool MergeTreeDataPartWide::hasColumnFiles(const NameAndTypePair & column) const
 {
     bool res = true;
 
-    type.enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
+    column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
-        String file_name = IDataType::getFileNameForStream(column_name, substream_path);
+        String file_name = IDataType::getFileNameForStream(column, substream_path);
 
         auto bin_checksum = checksums.files.find(file_name + ".bin");
         auto mrk_checksum = checksums.files.find(file_name + index_granularity_info.marks_file_extension);
@@ -225,7 +230,7 @@ String MergeTreeDataPartWide::getFileNameForColumn(const NameAndTypePair & colum
     column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
         if (filename.empty())
-            filename = IDataType::getFileNameForStream(column.name, substream_path);
+            filename = IDataType::getFileNameForStream(column, substream_path);
     });
     return filename;
 }
@@ -235,7 +240,7 @@ void MergeTreeDataPartWide::calculateEachColumnSizes(ColumnSizeByName & each_col
     std::unordered_set<String> processed_substreams;
     for (const NameAndTypePair & column : columns)
     {
-        ColumnSize size = getColumnSizeImpl(column.name, *column.type, &processed_substreams);
+        ColumnSize size = getColumnSizeImpl(column, &processed_substreams);
         each_columns_size[column.name] = size;
         total_size.add(size);
 
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.h b/src/Storages/MergeTree/MergeTreeDataPartWide.h
index aa8c3aedea7e..30d3021d0033 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWide.h
+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.h
@@ -54,7 +54,7 @@ class MergeTreeDataPartWide : public IMergeTreeDataPart
 
     ~MergeTreeDataPartWide() override;
 
-    bool hasColumnFiles(const String & column, const IDataType & type) const override;
+    bool hasColumnFiles(const NameAndTypePair & column) const override;
 
 private:
     void checkConsistency(bool require_part_metadata) const override;
@@ -62,7 +62,7 @@ class MergeTreeDataPartWide : public IMergeTreeDataPart
     /// Loads marks index granularity into memory
     void loadIndexGranularity() override;
 
-    ColumnSize getColumnSizeImpl(const String & name, const IDataType & type, std::unordered_set<String> * processed_substreams) const;
+    ColumnSize getColumnSizeImpl(const NameAndTypePair & column, std::unordered_set<String> * processed_substreams) const;
 
     void calculateEachColumnSizes(ColumnSizeByName & each_columns_size, ColumnSize & total_size) const override;
 };
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp
index ef3b5eb7d24e..7ec5cf8920c7 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp
@@ -34,14 +34,14 @@ MergeTreeDataPartWriterCompact::MergeTreeDataPartWriterCompact(
 {
     const auto & storage_columns = metadata_snapshot->getColumns();
     for (const auto & column : columns_list)
-        addStreams(column.name, *column.type, storage_columns.getCodecDescOrDefault(column.name, default_codec));
+        addStreams(column, storage_columns.getCodecDescOrDefault(column.name, default_codec));
 }
 
-void MergeTreeDataPartWriterCompact::addStreams(const String & name, const IDataType & type, const ASTPtr & effective_codec_desc)
+void MergeTreeDataPartWriterCompact::addStreams(const NameAndTypePair & column, const ASTPtr & effective_codec_desc)
 {
     IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & substream_type)
     {
-        String stream_name = IDataType::getFileNameForStream(name, substream_path);
+        String stream_name = IDataType::getFileNameForStream(column, substream_path);
 
         /// Shared offsets for Nested type.
         if (compressed_streams.count(stream_name))
@@ -64,7 +64,7 @@ void MergeTreeDataPartWriterCompact::addStreams(const String & name, const IData
     };
 
     IDataType::SubstreamPath stream_path;
-    type.enumerateStreams(callback, stream_path);
+    column.type->enumerateStreams(callback, stream_path);
 }
 
 namespace
@@ -183,7 +183,7 @@ void MergeTreeDataPartWriterCompact::writeDataBlock(const Block & block, const G
             CompressedStreamPtr prev_stream;
             auto stream_getter = [&, this](const IDataType::SubstreamPath & substream_path) -> WriteBuffer *
             {
-                String stream_name = IDataType::getFileNameForStream(name_and_type->name, substream_path);
+                String stream_name = IDataType::getFileNameForStream(*name_and_type, substream_path);
 
                 auto & result_stream = compressed_streams[stream_name];
                 /// Write one compressed block per column in granule for more optimal reading.
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h
index 2913960fd352..8b86a9701c9c 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h
@@ -37,7 +37,7 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk
 
     void addToChecksums(MergeTreeDataPartChecksums & checksums);
 
-    void addStreams(const String & name, const IDataType & type, const ASTPtr & effective_codec_desc);
+    void addStreams(const NameAndTypePair & column, const ASTPtr & effective_codec_desc);
 
     Block header;
 
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp
index 982345f8240f..81a6539780c2 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp
@@ -80,17 +80,17 @@ MergeTreeDataPartWriterWide::MergeTreeDataPartWriterWide(
 {
     const auto & columns = metadata_snapshot->getColumns();
     for (const auto & it : columns_list)
-        addStreams(it.name, *it.type, columns.getCodecDescOrDefault(it.name, default_codec));
+        addStreams(it, columns.getCodecDescOrDefault(it.name, default_codec));
 }
 
+
 void MergeTreeDataPartWriterWide::addStreams(
-    const String & name,
-    const IDataType & type,
+    const NameAndTypePair & column,
     const ASTPtr & effective_codec_desc)
 {
     IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & substream_type)
     {
-        String stream_name = IDataType::getFileNameForStream(name, substream_path);
+        String stream_name = IDataType::getFileNameForStream(column, substream_path);
         /// Shared offsets for Nested type.
         if (column_streams.count(stream_name))
             return;
@@ -112,18 +112,18 @@ void MergeTreeDataPartWriterWide::addStreams(
     };
 
     IDataType::SubstreamPath stream_path;
-    type.enumerateStreams(callback, stream_path);
+    column.type->enumerateStreams(callback, stream_path);
 }
 
 
 IDataType::OutputStreamGetter MergeTreeDataPartWriterWide::createStreamGetter(
-        const String & name, WrittenOffsetColumns & offset_columns) const
+        const NameAndTypePair & column, WrittenOffsetColumns & offset_columns) const
 {
     return [&, this] (const IDataType::SubstreamPath & substream_path) -> WriteBuffer *
     {
         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;
 
-        String stream_name = IDataType::getFileNameForStream(name, substream_path);
+        String stream_name = IDataType::getFileNameForStream(column, substream_path);
 
         /// Don't write offsets more than one time for Nested type.
         if (is_offsets && offset_columns.count(stream_name))
@@ -210,23 +210,23 @@ void MergeTreeDataPartWriterWide::write(const Block & block, const IColumn::Perm
             if (primary_key_block.has(it->name))
             {
                 const auto & primary_column = *primary_key_block.getByName(it->name).column;
-                writeColumn(column.name, *column.type, primary_column, offset_columns, granules_to_write);
+                writeColumn(*it, primary_column, offset_columns, granules_to_write);
             }
             else if (skip_indexes_block.has(it->name))
             {
                 const auto & index_column = *skip_indexes_block.getByName(it->name).column;
-                writeColumn(column.name, *column.type, index_column, offset_columns, granules_to_write);
+                writeColumn(*it, index_column, offset_columns, granules_to_write);
             }
             else
             {
                 /// We rearrange the columns that are not included in the primary key here; Then the result is released - to save RAM.
                 ColumnPtr permuted_column = column.column->permute(*permutation, 0);
-                writeColumn(column.name, *column.type, *permuted_column, offset_columns, granules_to_write);
+                writeColumn(*it, *permuted_column, offset_columns, granules_to_write);
             }
         }
         else
         {
-            writeColumn(column.name, *column.type, *column.column, offset_columns, granules_to_write);
+            writeColumn(*it, *column.column, offset_columns, granules_to_write);
         }
     }
 
@@ -239,13 +239,12 @@ void MergeTreeDataPartWriterWide::write(const Block & block, const IColumn::Perm
 }
 
 void MergeTreeDataPartWriterWide::writeSingleMark(
-    const String & name,
-    const IDataType & type,
+    const NameAndTypePair & column,
     WrittenOffsetColumns & offset_columns,
     size_t number_of_rows,
     DB::IDataType::SubstreamPath & path)
 {
-    StreamsWithMarks marks = getCurrentMarksForColumn(name, type, offset_columns, path);
+    StreamsWithMarks marks = getCurrentMarksForColumn(column, offset_columns, path);
     for (const auto & mark : marks)
         flushMarkToFile(mark, number_of_rows);
 }
@@ -260,17 +259,16 @@ void MergeTreeDataPartWriterWide::flushMarkToFile(const StreamNameAndMark & stre
 }
 
 StreamsWithMarks MergeTreeDataPartWriterWide::getCurrentMarksForColumn(
-    const String & name,
-    const IDataType & type,
+    const NameAndTypePair & column,
     WrittenOffsetColumns & offset_columns,
     DB::IDataType::SubstreamPath & path)
 {
     StreamsWithMarks result;
-    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
+    column.type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;
 
-        String stream_name = IDataType::getFileNameForStream(name, substream_path);
+        String stream_name = IDataType::getFileNameForStream(column, substream_path);
 
         /// Don't write offsets more than one time for Nested type.
         if (is_offsets && offset_columns.count(stream_name))
@@ -294,22 +292,21 @@ StreamsWithMarks MergeTreeDataPartWriterWide::getCurrentMarksForColumn(
 }
 
 void MergeTreeDataPartWriterWide::writeSingleGranule(
-    const String & name,
-    const IDataType & type,
+    const NameAndTypePair & name_and_type,
     const IColumn & column,
     WrittenOffsetColumns & offset_columns,
     IDataType::SerializeBinaryBulkStatePtr & serialization_state,
     IDataType::SerializeBinaryBulkSettings & serialize_settings,
     const Granule & granule)
 {
-    type.serializeBinaryBulkWithMultipleStreams(column, granule.start_row, granule.rows_to_write, serialize_settings, serialization_state);
+    name_and_type.type->serializeBinaryBulkWithMultipleStreams(column, granule.start_row, granule.rows_to_write, serialize_settings, serialization_state);
 
     /// So that instead of the marks pointing to the end of the compressed block, there were marks pointing to the beginning of the next one.
-    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
+    name_and_type.type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;
 
-        String stream_name = IDataType::getFileNameForStream(name, substream_path);
+        String stream_name = IDataType::getFileNameForStream(name_and_type, substream_path);
 
         /// Don't write offsets more than one time for Nested type.
         if (is_offsets && offset_columns.count(stream_name))
@@ -321,27 +318,27 @@ void MergeTreeDataPartWriterWide::writeSingleGranule(
 
 /// Column must not be empty. (column.size() !== 0)
 void MergeTreeDataPartWriterWide::writeColumn(
-    const String & name,
-    const IDataType & type,
+    const NameAndTypePair & name_and_type,
     const IColumn & column,
     WrittenOffsetColumns & offset_columns,
     const Granules & granules)
 {
     if (granules.empty())
-        throw Exception(ErrorCodes::LOGICAL_ERROR, "Empty granules for column {}, current mark {}", backQuoteIfNeed(name), getCurrentMark());
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Empty granules for column {}, current mark {}", backQuoteIfNeed(name_and_type.name), getCurrentMark());
 
+    const auto & [name, type] = name_and_type;
     auto [it, inserted] = serialization_states.emplace(name, nullptr);
 
     if (inserted)
     {
         IDataType::SerializeBinaryBulkSettings serialize_settings;
-        serialize_settings.getter = createStreamGetter(name, offset_columns);
-        type.serializeBinaryBulkStatePrefix(serialize_settings, it->second);
+        serialize_settings.getter = createStreamGetter(name_and_type, offset_columns);
+        type->serializeBinaryBulkStatePrefix(serialize_settings, it->second);
     }
 
     const auto & global_settings = storage.global_context.getSettingsRef();
     IDataType::SerializeBinaryBulkSettings serialize_settings;
-    serialize_settings.getter = createStreamGetter(name, offset_columns);
+    serialize_settings.getter = createStreamGetter(name_and_type, offset_columns);
     serialize_settings.low_cardinality_max_dictionary_size = global_settings.low_cardinality_max_dictionary_size;
     serialize_settings.low_cardinality_use_single_dictionary_for_part = global_settings.low_cardinality_use_single_dictionary_for_part != 0;
 
@@ -353,12 +350,11 @@ void MergeTreeDataPartWriterWide::writeColumn(
         {
             if (last_non_written_marks.count(name))
                 throw Exception(ErrorCodes::LOGICAL_ERROR, "We have to add new mark for column, but already have non written mark. Current mark {}, total marks {}, offset {}", getCurrentMark(), index_granularity.getMarksCount(), rows_written_in_last_mark);
-            last_non_written_marks[name] = getCurrentMarksForColumn(name, type, offset_columns, serialize_settings.path);
+            last_non_written_marks[name] = getCurrentMarksForColumn(name_and_type, offset_columns, serialize_settings.path);
         }
 
         writeSingleGranule(
-           name,
-           type,
+           name_and_type,
            column,
            offset_columns,
            it->second,
@@ -378,12 +374,12 @@ void MergeTreeDataPartWriterWide::writeColumn(
         }
     }
 
-    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
+    name_and_type.type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;
         if (is_offsets)
         {
-            String stream_name = IDataType::getFileNameForStream(name, substream_path);
+            String stream_name = IDataType::getFileNameForStream(name_and_type, substream_path);
             offset_columns.insert(stream_name);
         }
     }, serialize_settings.path);
@@ -526,12 +522,12 @@ void MergeTreeDataPartWriterWide::finishDataSerialization(IMergeTreeDataPart::Ch
         {
             if (!serialization_states.empty())
             {
-                serialize_settings.getter = createStreamGetter(it->name, written_offset_columns ? *written_offset_columns : offset_columns);
+                serialize_settings.getter = createStreamGetter(*it, written_offset_columns ? *written_offset_columns : offset_columns);
                 it->type->serializeBinaryBulkStateSuffix(serialize_settings, serialization_states[it->name]);
             }
 
             if (write_final_mark)
-                writeFinalMark(it->name, it->type, offset_columns, serialize_settings.path);
+                writeFinalMark(*it, offset_columns, serialize_settings.path);
         }
     }
     for (auto & stream : column_streams)
@@ -567,19 +563,18 @@ void MergeTreeDataPartWriterWide::finish(IMergeTreeDataPart::Checksums & checksu
 }
 
 void MergeTreeDataPartWriterWide::writeFinalMark(
-    const std::string & column_name,
-    const DataTypePtr column_type,
+    const NameAndTypePair & column,
     WrittenOffsetColumns & offset_columns,
     DB::IDataType::SubstreamPath & path)
 {
-    writeSingleMark(column_name, *column_type, offset_columns, 0, path);
+    writeSingleMark(column, offset_columns, 0, path);
     /// Memoize information about offsets
-    column_type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
+    column.type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;
         if (is_offsets)
         {
-            String stream_name = IDataType::getFileNameForStream(column_name, substream_path);
+            String stream_name = IDataType::getFileNameForStream(column, substream_path);
             offset_columns.insert(stream_name);
         }
     }, path);
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h
index d897503a0330..e6f96f3f146f 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h
@@ -40,16 +40,14 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk
     /// Return how many marks were written and
     /// how many rows were written for last mark
     void writeColumn(
-        const String & name,
-        const IDataType & type,
+        const NameAndTypePair & name_and_type,
         const IColumn & column,
         WrittenOffsetColumns & offset_columns,
         const Granules & granules);
 
     /// Write single granule of one column.
     void writeSingleGranule(
-        const String & name,
-        const IDataType & type,
+        const NameAndTypePair & name_and_type,
         const IColumn & column,
         WrittenOffsetColumns & offset_columns,
         IDataType::SerializeBinaryBulkStatePtr & serialization_state,
@@ -58,8 +56,7 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk
 
     /// Take offsets from column and return as MarkInCompressed file with stream name
     StreamsWithMarks getCurrentMarksForColumn(
-        const String & name,
-        const IDataType & type,
+        const NameAndTypePair & column,
         WrittenOffsetColumns & offset_columns,
         DB::IDataType::SubstreamPath & path);
 
@@ -70,21 +67,18 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk
 
     /// Write mark for column taking offsets from column stream
     void writeSingleMark(
-        const String & name,
-        const IDataType & type,
+        const NameAndTypePair & column,
         WrittenOffsetColumns & offset_columns,
         size_t number_of_rows,
         DB::IDataType::SubstreamPath & path);
 
     void writeFinalMark(
-        const std::string & column_name,
-        const DataTypePtr column_type,
+        const NameAndTypePair & column,
         WrittenOffsetColumns & offset_columns,
         DB::IDataType::SubstreamPath & path);
 
     void addStreams(
-        const String & name,
-        const IDataType & type,
+        const NameAndTypePair & column,
         const ASTPtr & effective_codec_desc);
 
     /// Method for self check (used in debug-build only). Checks that written
@@ -106,7 +100,7 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk
     /// Also useful to have exact amount of rows in last (non-final) mark.
     void adjustLastMarkIfNeedAndFlushToDisk(size_t new_rows_in_last_mark);
 
-    IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenOffsetColumns & offset_columns) const;
+    IDataType::OutputStreamGetter createStreamGetter(const NameAndTypePair & column, WrittenOffsetColumns & offset_columns) const;
 
     using SerializationState = IDataType::SerializeBinaryBulkStatePtr;
     using SerializationStates = std::unordered_map<String, SerializationState>;
diff --git a/src/Storages/MergeTree/MergeTreeIOSettings.h b/src/Storages/MergeTree/MergeTreeIOSettings.h
index 9e315c086813..d82aa7dd7c2c 100644
--- a/src/Storages/MergeTree/MergeTreeIOSettings.h
+++ b/src/Storages/MergeTree/MergeTreeIOSettings.h
@@ -14,6 +14,8 @@ struct MergeTreeReaderSettings
     /// If save_marks_in_cache is false, then, if marks are not in cache,
     ///  we will load them but won't save in the cache, to avoid evicting other data.
     bool save_marks_in_cache = false;
+    /// Convert old-style nested (single arrays with same prefix, `n.a`, `n.b`...) to subcolumns of data type Nested.
+    bool convert_nested_to_subcolumns = false;
 };
 
 struct MergeTreeWriterSettings
diff --git a/src/Storages/MergeTree/MergeTreeIndexSet.cpp b/src/Storages/MergeTree/MergeTreeIndexSet.cpp
index 9aaf894a0cb6..b6706367bfa0 100644
--- a/src/Storages/MergeTree/MergeTreeIndexSet.cpp
+++ b/src/Storages/MergeTree/MergeTreeIndexSet.cpp
@@ -93,7 +93,7 @@ void MergeTreeIndexGranuleSet::deserializeBinary(ReadBuffer & istr)
     {
         const auto & column = index_sample_block.getByPosition(i);
         const auto & type = column.type;
-        auto new_column = type->createColumn();
+        ColumnPtr new_column = type->createColumn();
 
         IDataType::DeserializeBinaryBulkSettings settings;
         settings.getter = [&](IDataType::SubstreamPath) -> ReadBuffer * { return &istr; };
@@ -101,9 +101,9 @@ void MergeTreeIndexGranuleSet::deserializeBinary(ReadBuffer & istr)
 
         IDataType::DeserializeBinaryBulkStatePtr state;
         type->deserializeBinaryBulkStatePrefix(settings, state);
-        type->deserializeBinaryBulkWithMultipleStreams(*new_column, rows_to_read, settings, state);
+        type->deserializeBinaryBulkWithMultipleStreams(new_column, rows_to_read, settings, state);
 
-        block.insert(ColumnWithTypeAndName(new_column->getPtr(), type, column.name));
+        block.insert(ColumnWithTypeAndName(new_column, type, column.name));
     }
 }
 
diff --git a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp
index 87b3f0a43298..635c59cf19a2 100644
--- a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp
+++ b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp
@@ -53,14 +53,14 @@ MergeTreeReaderCompact::MergeTreeReaderCompact(
         auto name_and_type = columns.begin();
         for (size_t i = 0; i < columns_num; ++i, ++name_and_type)
         {
-            const auto & [name, type] = getColumnFromPart(*name_and_type);
-            auto position = data_part->getColumnPosition(name);
+            auto column_from_part = getColumnFromPart(*name_and_type);
 
-            if (!position && typeid_cast<const DataTypeArray *>(type.get()))
+            auto position = data_part->getColumnPosition(column_from_part.name);
+            if (!position && typeid_cast<const DataTypeArray *>(column_from_part.type.get()))
             {
                 /// If array of Nested column is missing in part,
                 /// we have to read its offsets if they exist.
-                position = findColumnForOffsets(name);
+                position = findColumnForOffsets(column_from_part.name);
                 read_only_offsets[i] = (position != std::nullopt);
             }
 
@@ -133,10 +133,8 @@ size_t MergeTreeReaderCompact::readRows(size_t from_mark, bool continue_reading,
         if (!column_positions[i])
             continue;
 
-        bool append = res_columns[i] != nullptr;
-        if (!append)
+        if (res_columns[i] == nullptr)
             res_columns[i] = getColumnFromPart(*column_it).type->createColumn();
-        mutable_columns[i] = res_columns[i]->assumeMutable();
     }
 
     while (read_rows < max_rows_to_read)
@@ -146,20 +144,18 @@ size_t MergeTreeReaderCompact::readRows(size_t from_mark, bool continue_reading,
         auto name_and_type = columns.begin();
         for (size_t pos = 0; pos < num_columns; ++pos, ++name_and_type)
         {
+            auto column_from_part = getColumnFromPart(*name_and_type);
             if (!res_columns[pos])
                 continue;
 
-            auto [name, type] = getColumnFromPart(*name_and_type);
-            auto & column = mutable_columns[pos];
-
             try
             {
+                auto & column = res_columns[pos];
                 size_t column_size_before_reading = column->size();
 
-                readData(name, *column, *type, from_mark, *column_positions[pos], rows_to_read, read_only_offsets[pos]);
+                readData(column_from_part, column, from_mark, *column_positions[pos], rows_to_read, read_only_offsets[pos]);
 
                 size_t read_rows_in_column = column->size() - column_size_before_reading;
-
                 if (read_rows_in_column < rows_to_read)
                     throw Exception("Cannot read all data in MergeTreeReaderCompact. Rows read: " + toString(read_rows_in_column) +
                         ". Rows expected: " + toString(rows_to_read) + ".", ErrorCodes::CANNOT_READ_ALL_DATA);
@@ -170,7 +166,7 @@ size_t MergeTreeReaderCompact::readRows(size_t from_mark, bool continue_reading,
                     storage.reportBrokenPart(data_part->name);
 
                 /// Better diagnostics.
-                e.addMessage("(while reading column " + name + ")");
+                e.addMessage("(while reading column " + column_from_part.name + ")");
                 throw;
             }
             catch (...)
@@ -184,24 +180,17 @@ size_t MergeTreeReaderCompact::readRows(size_t from_mark, bool continue_reading,
         read_rows += rows_to_read;
     }
 
-    for (size_t i = 0; i < num_columns; ++i)
-    {
-        auto & column = mutable_columns[i];
-        if (column && !column->empty())
-            res_columns[i] = std::move(column);
-        else
-            res_columns[i] = nullptr;
-    }
-
     next_mark = from_mark;
 
     return read_rows;
 }
 
 void MergeTreeReaderCompact::readData(
-    const String & name, IColumn & column, const IDataType & type,
+    const NameAndTypePair & name_and_type, ColumnPtr & column,
     size_t from_mark, size_t column_position, size_t rows_to_read, bool only_offsets)
 {
+    const auto & [name, type] = name_and_type;
+
     if (!isContinuousReading(from_mark, column_position))
         seekToMark(from_mark, column_position);
 
@@ -213,14 +202,25 @@ void MergeTreeReaderCompact::readData(
         return data_buffer;
     };
 
+    IDataType::DeserializeBinaryBulkStatePtr state;
     IDataType::DeserializeBinaryBulkSettings deserialize_settings;
     deserialize_settings.getter = buffer_getter;
     deserialize_settings.avg_value_size_hint = avg_value_size_hints[name];
-    deserialize_settings.position_independent_encoding = true;
 
-    IDataType::DeserializeBinaryBulkStatePtr state;
-    type.deserializeBinaryBulkStatePrefix(deserialize_settings, state);
-    type.deserializeBinaryBulkWithMultipleStreams(column, rows_to_read, deserialize_settings, state);
+    if (name_and_type.isSubcolumn())
+    {
+        auto type_in_storage = name_and_type.getTypeInStorage();
+        ColumnPtr temp_column = type_in_storage->createColumn();
+
+        type_in_storage->deserializeBinaryBulkStatePrefix(deserialize_settings, state);
+        type_in_storage->deserializeBinaryBulkWithMultipleStreams(temp_column, rows_to_read, deserialize_settings, state);
+        column = type_in_storage->getSubcolumn(name_and_type.getSubcolumnName(), *temp_column);
+    }
+    else
+    {
+        type->deserializeBinaryBulkStatePrefix(deserialize_settings, state);
+        type->deserializeBinaryBulkWithMultipleStreams(column, rows_to_read, deserialize_settings, state);
+    }
 
     /// The buffer is left in inconsistent state after reading single offsets
     if (only_offsets)
diff --git a/src/Storages/MergeTree/MergeTreeReaderCompact.h b/src/Storages/MergeTree/MergeTreeReaderCompact.h
index 9ef887165791..dbfaa7868fac 100644
--- a/src/Storages/MergeTree/MergeTreeReaderCompact.h
+++ b/src/Storages/MergeTree/MergeTreeReaderCompact.h
@@ -56,8 +56,8 @@ class MergeTreeReaderCompact : public IMergeTreeReader
 
     void seekToMark(size_t row_index, size_t column_index);
 
-    void readData(const String & name, IColumn & column, const IDataType & type,
-        size_t from_mark, size_t column_position, size_t rows_to_read, bool only_offsets = false);
+    void readData(const NameAndTypePair & name_and_type, ColumnPtr & column, size_t from_mark,
+        size_t column_position, size_t rows_to_read, bool only_offsets);
 
     /// Returns maximal value of granule size in compressed file from @mark_ranges.
     /// This value is used as size of read buffer.
diff --git a/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp b/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp
index e684205658a3..5ee4aa555e6c 100644
--- a/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp
+++ b/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp
@@ -12,6 +12,7 @@ namespace ErrorCodes
 {
     extern const int CANNOT_READ_ALL_DATA;
     extern const int ARGUMENT_OUT_OF_BOUND;
+    extern const int LOGICAL_ERROR;
 }
 
 
@@ -38,6 +39,19 @@ MergeTreeReaderInMemory::MergeTreeReaderInMemory(
     }
 }
 
+static ColumnPtr getColumnFromBlock(const Block & block, const NameAndTypePair & name_and_type)
+{
+    auto storage_name = name_and_type.getNameInStorage();
+    if (!block.has(storage_name))
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Not found column '{}' in block", storage_name);
+
+    const auto & column = block.getByName(storage_name).column;
+    if (name_and_type.isSubcolumn())
+        return name_and_type.getTypeInStorage()->getSubcolumn(name_and_type.getSubcolumnName(), *column);
+
+    return column;
+}
+
 size_t MergeTreeReaderInMemory::readRows(size_t from_mark, bool continue_reading, size_t max_rows_to_read, Columns & res_columns)
 {
     if (!continue_reading)
@@ -60,17 +74,17 @@ size_t MergeTreeReaderInMemory::readRows(size_t from_mark, bool continue_reading
     auto column_it = columns.begin();
     for (size_t i = 0; i < num_columns; ++i, ++column_it)
     {
-        auto [name, type] = getColumnFromPart(*column_it);
+        auto name_type = getColumnFromPart(*column_it);
 
         /// Copy offsets, if array of Nested column is missing in part.
-        auto offsets_it = positions_for_offsets.find(name);
-        if (offsets_it != positions_for_offsets.end())
+        auto offsets_it = positions_for_offsets.find(name_type.name);
+        if (offsets_it != positions_for_offsets.end() && !name_type.isSubcolumn())
         {
             const auto & source_offsets = assert_cast<const ColumnArray &>(
                 *part_in_memory->block.getByPosition(offsets_it->second).column).getOffsets();
 
             if (res_columns[i] == nullptr)
-                res_columns[i] = type->createColumn();
+                res_columns[i] = name_type.type->createColumn();
 
             auto mutable_column = res_columns[i]->assumeMutable();
             auto & res_offstes = assert_cast<ColumnArray &>(*mutable_column).getOffsets();
@@ -80,9 +94,9 @@ size_t MergeTreeReaderInMemory::readRows(size_t from_mark, bool continue_reading
 
             res_columns[i] = std::move(mutable_column);
         }
-        else if (part_in_memory->block.has(name))
+        else if (part_in_memory->hasColumnFiles(name_type))
         {
-            const auto & block_column = part_in_memory->block.getByName(name).column;
+            auto block_column = getColumnFromBlock(part_in_memory->block, name_type);
             if (rows_to_read == part_rows)
             {
                 res_columns[i] = block_column;
@@ -90,7 +104,7 @@ size_t MergeTreeReaderInMemory::readRows(size_t from_mark, bool continue_reading
             else
             {
                 if (res_columns[i] == nullptr)
-                    res_columns[i] = type->createColumn();
+                    res_columns[i] = name_type.type->createColumn();
 
                 auto mutable_column = res_columns[i]->assumeMutable();
                 mutable_column->insertRangeFrom(*block_column, total_rows_read, rows_to_read);
diff --git a/src/Storages/MergeTree/MergeTreeReaderWide.cpp b/src/Storages/MergeTree/MergeTreeReaderWide.cpp
index 1dacdacbae09..30db54fc8e06 100644
--- a/src/Storages/MergeTree/MergeTreeReaderWide.cpp
+++ b/src/Storages/MergeTree/MergeTreeReaderWide.cpp
@@ -9,7 +9,6 @@
 #include <Common/escapeForFileName.h>
 #include <Common/typeid_cast.h>
 
-
 namespace DB
 {
 
@@ -50,7 +49,7 @@ MergeTreeReaderWide::MergeTreeReaderWide(
         for (const NameAndTypePair & column : columns)
         {
             auto column_from_part = getColumnFromPart(column);
-            addStreams(column_from_part.name, *column_from_part.type, profile_callback_, clock_type_);
+            addStreams(column_from_part, profile_callback_, clock_type_);
         }
     }
     catch (...)
@@ -73,48 +72,26 @@ size_t MergeTreeReaderWide::readRows(size_t from_mark, bool continue_reading, si
         /// If append is true, then the value will be equal to nullptr and will be used only to
         /// check that the offsets column has been already read.
         OffsetColumns offset_columns;
+        std::unordered_map<String, IDataType::SubstreamsCache> caches;
 
         auto name_and_type = columns.begin();
         for (size_t pos = 0; pos < num_columns; ++pos, ++name_and_type)
         {
-            auto [name, type] = getColumnFromPart(*name_and_type);
+            auto column_from_part = getColumnFromPart(*name_and_type);
+            const auto & [name, type] = column_from_part;
 
             /// The column is already present in the block so we will append the values to the end.
             bool append = res_columns[pos] != nullptr;
             if (!append)
                 res_columns[pos] = type->createColumn();
 
-            /// To keep offsets shared. TODO Very dangerous. Get rid of this.
-            MutableColumnPtr column = res_columns[pos]->assumeMutable();
-
-            bool read_offsets = true;
-
-            /// For nested data structures collect pointers to offset columns.
-            if (const auto * type_arr = typeid_cast<const DataTypeArray *>(type.get()))
-            {
-                String table_name = Nested::extractTableName(name);
-
-                auto it_inserted = offset_columns.emplace(table_name, nullptr);
-
-                /// offsets have already been read on the previous iteration and we don't need to read it again
-                if (!it_inserted.second)
-                    read_offsets = false;
-
-                /// need to create new offsets
-                if (it_inserted.second && !append)
-                    it_inserted.first->second = ColumnArray::ColumnOffsets::create();
-
-                /// share offsets in all elements of nested structure
-                if (!append)
-                    column = ColumnArray::create(type_arr->getNestedType()->createColumn(),
-                                                 it_inserted.first->second)->assumeMutable();
-            }
-
+            auto & column = res_columns[pos];
             try
             {
                 size_t column_size_before_reading = column->size();
+                auto & cache = caches[column_from_part.getNameInStorage()];
 
-                readData(name, *type, *column, from_mark, continue_reading, max_rows_to_read, read_offsets);
+                readData(column_from_part, column, from_mark, continue_reading, max_rows_to_read, cache);
 
                 /// For elements of Nested, column_size_before_reading may be greater than column size
                 ///  if offsets are not empty and were already read, but elements are empty.
@@ -130,8 +107,6 @@ size_t MergeTreeReaderWide::readRows(size_t from_mark, bool continue_reading, si
 
             if (column->empty())
                 res_columns[pos] = nullptr;
-            else
-                res_columns[pos] = std::move(column);
         }
 
         /// NOTE: positions for all streams must be kept in sync.
@@ -159,12 +134,12 @@ size_t MergeTreeReaderWide::readRows(size_t from_mark, bool continue_reading, si
     return read_rows;
 }
 
-void MergeTreeReaderWide::addStreams(const String & name, const IDataType & type,
+void MergeTreeReaderWide::addStreams(const NameAndTypePair & name_and_type,
     const ReadBufferFromFileBase::ProfileCallback & profile_callback, clockid_t clock_type)
 {
     IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
-        String stream_name = IDataType::getFileNameForStream(name, substream_path);
+        String stream_name = IDataType::getFileNameForStream(name_and_type, substream_path);
 
         if (streams.count(stream_name))
             return;
@@ -186,24 +161,24 @@ void MergeTreeReaderWide::addStreams(const String & name, const IDataType & type
     };
 
     IDataType::SubstreamPath substream_path;
-    type.enumerateStreams(callback, substream_path);
+    name_and_type.type->enumerateStreams(callback, substream_path);
 }
 
 
 void MergeTreeReaderWide::readData(
-    const String & name, const IDataType & type, IColumn & column,
+    const NameAndTypePair & name_and_type, ColumnPtr & column,
     size_t from_mark, bool continue_reading, size_t max_rows_to_read,
-    bool with_offsets)
+    IDataType::SubstreamsCache & cache)
 {
     auto get_stream_getter = [&](bool stream_for_prefix) -> IDataType::InputStreamGetter
     {
         return [&, stream_for_prefix](const IDataType::SubstreamPath & substream_path) -> ReadBuffer *
         {
-            /// If offsets for arrays have already been read.
-            if (!with_offsets && substream_path.size() == 1 && substream_path[0].type == IDataType::Substream::ArraySizes)
+            /// If substream have already been read.
+            if (cache.count(IDataType::getSubcolumnNameForStream(substream_path)))
                 return nullptr;
 
-            String stream_name = IDataType::getFileNameForStream(name, substream_path);
+            String stream_name = IDataType::getFileNameForStream(name_and_type, substream_path);
 
             auto it = streams.find(stream_name);
             if (it == streams.end())
@@ -223,21 +198,21 @@ void MergeTreeReaderWide::readData(
         };
     };
 
-    double & avg_value_size_hint = avg_value_size_hints[name];
+    double & avg_value_size_hint = avg_value_size_hints[name_and_type.name];
     IDataType::DeserializeBinaryBulkSettings deserialize_settings;
     deserialize_settings.avg_value_size_hint = avg_value_size_hint;
 
-    if (deserialize_binary_bulk_state_map.count(name) == 0)
+    if (deserialize_binary_bulk_state_map.count(name_and_type.name) == 0)
     {
         deserialize_settings.getter = get_stream_getter(true);
-        type.deserializeBinaryBulkStatePrefix(deserialize_settings, deserialize_binary_bulk_state_map[name]);
+        name_and_type.type->deserializeBinaryBulkStatePrefix(deserialize_settings, deserialize_binary_bulk_state_map[name_and_type.name]);
     }
 
     deserialize_settings.getter = get_stream_getter(false);
     deserialize_settings.continuous_reading = continue_reading;
-    auto & deserialize_state = deserialize_binary_bulk_state_map[name];
-    type.deserializeBinaryBulkWithMultipleStreams(column, max_rows_to_read, deserialize_settings, deserialize_state);
-    IDataType::updateAvgValueSizeHint(column, avg_value_size_hint);
+    auto & deserialize_state = deserialize_binary_bulk_state_map[name_and_type.name];
+    name_and_type.type->deserializeBinaryBulkWithMultipleStreams(column, max_rows_to_read, deserialize_settings, deserialize_state, &cache);
+    IDataType::updateAvgValueSizeHint(*column, avg_value_size_hint);
 }
 
 }
diff --git a/src/Storages/MergeTree/MergeTreeReaderWide.h b/src/Storages/MergeTree/MergeTreeReaderWide.h
index 69652d1e954c..bf9e97035d02 100644
--- a/src/Storages/MergeTree/MergeTreeReaderWide.h
+++ b/src/Storages/MergeTree/MergeTreeReaderWide.h
@@ -37,13 +37,13 @@ class MergeTreeReaderWide : public IMergeTreeReader
 
     FileStreams streams;
 
-    void addStreams(const String & name, const IDataType & type,
+    void addStreams(const NameAndTypePair & name_and_type,
         const ReadBufferFromFileBase::ProfileCallback & profile_callback, clockid_t clock_type);
 
     void readData(
-        const String & name, const IDataType & type, IColumn & column,
+        const NameAndTypePair & name_and_type, ColumnPtr & column,
         size_t from_mark, bool continue_reading, size_t max_rows_to_read,
-        bool with_offsets = true);
+        IDataType::SubstreamsCache & cache);
 };
 
 }
diff --git a/src/Storages/MergeTree/checkDataPart.cpp b/src/Storages/MergeTree/checkDataPart.cpp
index 2838c8eb8812..c9da156dc975 100644
--- a/src/Storages/MergeTree/checkDataPart.cpp
+++ b/src/Storages/MergeTree/checkDataPart.cpp
@@ -122,7 +122,7 @@ IMergeTreeDataPart::Checksums checkDataPart(
         {
             column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
             {
-                String file_name = IDataType::getFileNameForStream(column.name, substream_path) + ".bin";
+                String file_name = IDataType::getFileNameForStream(column, substream_path) + ".bin";
                 checksums_data.files[file_name] = checksum_compressed_file(disk, path + file_name);
             }, {});
         }
diff --git a/src/Storages/StorageBuffer.cpp b/src/Storages/StorageBuffer.cpp
index 3bc89053c741..e0657f31de5c 100644
--- a/src/Storages/StorageBuffer.cpp
+++ b/src/Storages/StorageBuffer.cpp
@@ -95,7 +95,7 @@ class BufferSource : public SourceWithProgress
     BufferSource(const Names & column_names_, StorageBuffer::Buffer & buffer_, const StorageBuffer & storage, const StorageMetadataPtr & metadata_snapshot)
         : SourceWithProgress(
             metadata_snapshot->getSampleBlockForColumns(column_names_, storage.getVirtuals(), storage.getStorageID()))
-        , column_names(column_names_.begin(), column_names_.end())
+        , column_names_and_types(metadata_snapshot->getColumns().getAllWithSubcolumns().addTypes(column_names_))
         , buffer(buffer_) {}
 
     String getName() const override { return "Buffer"; }
@@ -115,10 +115,16 @@ class BufferSource : public SourceWithProgress
             return res;
 
         Columns columns;
-        columns.reserve(column_names.size());
+        columns.reserve(column_names_and_types.size());
 
-        for (const auto & name : column_names)
-            columns.push_back(buffer.data.getByName(name).column);
+        for (const auto & elem : column_names_and_types)
+        {
+            const auto & current_column = buffer.data.getByName(elem.getNameInStorage()).column;
+            if (elem.isSubcolumn())
+                columns.emplace_back(elem.getTypeInStorage()->getSubcolumn(elem.getSubcolumnName(), *current_column));
+            else
+                columns.emplace_back(std::move(current_column));
+        }
 
         UInt64 size = columns.at(0)->size();
         res.setColumns(std::move(columns), size);
@@ -127,7 +133,7 @@ class BufferSource : public SourceWithProgress
     }
 
 private:
-    Names column_names;
+    NamesAndTypesList column_names_and_types;
     StorageBuffer::Buffer & buffer;
     bool has_been_read = false;
 };
@@ -188,8 +194,8 @@ void StorageBuffer::read(
         {
             const auto & dest_columns = destination_metadata_snapshot->getColumns();
             const auto & our_columns = metadata_snapshot->getColumns();
-            return dest_columns.hasPhysical(column_name) &&
-                   dest_columns.get(column_name).type->equals(*our_columns.get(column_name).type);
+            return dest_columns.hasPhysicalOrSubcolumn(column_name) &&
+                   dest_columns.getPhysicalOrSubcolumn(column_name).type->equals(*our_columns.getPhysicalOrSubcolumn(column_name).type);
         });
 
         if (dst_has_same_structure)
diff --git a/src/Storages/StorageBuffer.h b/src/Storages/StorageBuffer.h
index ed8a405298b1..9656c78637bc 100644
--- a/src/Storages/StorageBuffer.h
+++ b/src/Storages/StorageBuffer.h
@@ -76,6 +76,8 @@ friend class BufferBlockOutputStream;
 
     bool supportsParallelInsert() const override { return true; }
 
+    bool supportsSubcolumns() const override { return true; }
+
     BlockOutputStreamPtr write(const ASTPtr & query, const StorageMetadataPtr & /*metadata_snapshot*/, const Context & context) override;
 
     void startup() override;
diff --git a/src/Storages/StorageInMemoryMetadata.cpp b/src/Storages/StorageInMemoryMetadata.cpp
index a4500e2aa7bc..2f488ce36c62 100644
--- a/src/Storages/StorageInMemoryMetadata.cpp
+++ b/src/Storages/StorageInMemoryMetadata.cpp
@@ -3,7 +3,10 @@
 #include <sparsehash/dense_hash_map>
 #include <sparsehash/dense_hash_set>
 #include <Common/quoteString.h>
+#include <Common/StringUtils/StringUtils.h>
 #include <Core/ColumnWithTypeAndName.h>
+#include <IO/ReadBufferFromString.h>
+#include <IO/ReadHelpers.h>
 #include <IO/Operators.h>
 
 
@@ -270,7 +273,7 @@ Block StorageInMemoryMetadata::getSampleBlockForColumns(
 
     std::unordered_map<String, DataTypePtr> columns_map;
 
-    NamesAndTypesList all_columns = getColumns().getAll();
+    auto all_columns = getColumns().getAllWithSubcolumns();
     for (const auto & elem : all_columns)
         columns_map.emplace(elem.name, elem.type);
 
@@ -459,7 +462,7 @@ namespace
 
 void StorageInMemoryMetadata::check(const Names & column_names, const NamesAndTypesList & virtuals, const StorageID & storage_id) const
 {
-    NamesAndTypesList available_columns = getColumns().getAllPhysical();
+    NamesAndTypesList available_columns = getColumns().getAllPhysicalWithSubcolumns();
     available_columns.insert(available_columns.end(), virtuals.begin(), virtuals.end());
 
     const String list_of_columns = listOfColumns(available_columns);
diff --git a/src/Storages/StorageLog.cpp b/src/Storages/StorageLog.cpp
index 06e9bb8a2d6d..02172517eb1d 100644
--- a/src/Storages/StorageLog.cpp
+++ b/src/Storages/StorageLog.cpp
@@ -47,7 +47,6 @@ namespace ErrorCodes
     extern const int INCORRECT_FILE_NAME;
 }
 
-
 class LogSource final : public SourceWithProgress
 {
 public:
@@ -58,7 +57,7 @@ class LogSource final : public SourceWithProgress
         for (const auto & name_type : columns)
             res.insert({ name_type.type->createColumn(), name_type.type, name_type.name });
 
-        return Nested::flatten(res);
+        return res;
     }
 
     LogSource(
@@ -91,8 +90,8 @@ class LogSource final : public SourceWithProgress
     struct Stream
     {
         Stream(const DiskPtr & disk, const String & data_path, size_t offset, size_t max_read_buffer_size_)
-            : plain(disk->readFile(data_path, std::min(max_read_buffer_size_, disk->getFileSize(data_path)))),
-            compressed(*plain)
+            : plain(disk->readFile(data_path, std::min(max_read_buffer_size_, disk->getFileSize(data_path))))
+            , compressed(*plain)
         {
             if (offset)
                 plain->seek(offset, SEEK_SET);
@@ -109,7 +108,7 @@ class LogSource final : public SourceWithProgress
     using DeserializeStates = std::map<String, DeserializeState>;
     DeserializeStates deserialize_states;
 
-    void readData(const String & name, const IDataType & type, IColumn & column, size_t max_rows_to_read);
+    void readData(const NameAndTypePair & name_and_type, ColumnPtr & column, size_t max_rows_to_read, IDataType::SubstreamsCache & cache);
 };
 
 
@@ -125,14 +124,15 @@ Chunk LogSource::generate()
 
     /// How many rows to read for the next block.
     size_t max_rows_to_read = std::min(block_size, rows_limit - rows_read);
+    std::unordered_map<String, IDataType::SubstreamsCache> caches;
 
     for (const auto & name_type : columns)
     {
-        MutableColumnPtr column = name_type.type->createColumn();
-
+        ColumnPtr column;
         try
         {
-            readData(name_type.name, *name_type.type, *column, max_rows_to_read);
+            column = name_type.type->createColumn();
+            readData(name_type, column, max_rows_to_read, caches[name_type.getNameInStorage()]);
         }
         catch (Exception & e)
         {
@@ -156,22 +156,25 @@ Chunk LogSource::generate()
         streams.clear();
     }
 
-    res = Nested::flatten(res);
     UInt64 num_rows = res.rows();
     return Chunk(res.getColumns(), num_rows);
 }
 
 
-void LogSource::readData(const String & name, const IDataType & type, IColumn & column, size_t max_rows_to_read)
+void LogSource::readData(const NameAndTypePair & name_and_type, ColumnPtr & column,
+    size_t max_rows_to_read, IDataType::SubstreamsCache & cache)
 {
     IDataType::DeserializeBinaryBulkSettings settings; /// TODO Use avg_value_size_hint.
+    const auto & [name, type] = name_and_type;
 
-    auto create_string_getter = [&](bool stream_for_prefix)
+    auto create_stream_getter = [&](bool stream_for_prefix)
     {
         return [&, stream_for_prefix] (const IDataType::SubstreamPath & path) -> ReadBuffer *
         {
-            String stream_name = IDataType::getFileNameForStream(name, path);
+            if (cache.count(IDataType::getSubcolumnNameForStream(path)))
+                return nullptr;
 
+            String stream_name = IDataType::getFileNameForStream(name_and_type, path);
             const auto & file_it = storage.files.find(stream_name);
             if (storage.files.end() == file_it)
                 throw Exception("Logical error: no information about file " + stream_name + " in StorageLog", ErrorCodes::LOGICAL_ERROR);
@@ -182,18 +185,19 @@ void LogSource::readData(const String & name, const IDataType & type, IColumn &
 
             auto & data_file_path = file_it->second.data_file_path;
             auto it = streams.try_emplace(stream_name, storage.disk, data_file_path, offset, max_read_buffer_size).first;
+
             return &it->second.compressed;
         };
     };
 
     if (deserialize_states.count(name) == 0)
     {
-        settings.getter = create_string_getter(true);
-        type.deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);
+        settings.getter = create_stream_getter(true);
+        type->deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);
     }
 
-    settings.getter = create_string_getter(false);
-    type.deserializeBinaryBulkWithMultipleStreams(column, max_rows_to_read, settings, deserialize_states[name]);
+    settings.getter = create_stream_getter(false);
+    type->deserializeBinaryBulkWithMultipleStreams(column, max_rows_to_read, settings, deserialize_states[name], &cache);
 }
 
 
@@ -282,9 +286,11 @@ class LogBlockOutputStream final : public IBlockOutputStream
     using SerializeStates = std::map<String, SerializeState>;
     SerializeStates serialize_states;
 
-    IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenStreams & written_streams);
+    IDataType::OutputStreamGetter createStreamGetter(const NameAndTypePair & name_and_type, WrittenStreams & written_streams);
 
-    void writeData(const String & name, const IDataType & type, const IColumn & column,
+    void writeData(
+        const NameAndTypePair & name_and_type,
+        const IColumn & column,
         MarksForColumns & out_marks,
         WrittenStreams & written_streams);
 
@@ -305,7 +311,7 @@ void LogBlockOutputStream::write(const Block & block)
     for (size_t i = 0; i < block.columns(); ++i)
     {
         const ColumnWithTypeAndName & column = block.safeGetByPosition(i);
-        writeData(column.name, *column.type, *column.column, marks, written_streams);
+        writeData(NameAndTypePair(column.name, column.type), *column.column, marks, written_streams);
     }
 
     writeMarks(std::move(marks));
@@ -324,7 +330,7 @@ void LogBlockOutputStream::writeSuffix()
         auto it = serialize_states.find(column.name);
         if (it != serialize_states.end())
         {
-            settings.getter = createStreamGetter(column.name, written_streams);
+            settings.getter = createStreamGetter(NameAndTypePair(column.name, column.type), written_streams);
             column.type->serializeBinaryBulkStateSuffix(settings, it->second);
         }
     }
@@ -350,12 +356,12 @@ void LogBlockOutputStream::writeSuffix()
 }
 
 
-IDataType::OutputStreamGetter LogBlockOutputStream::createStreamGetter(const String & name,
+IDataType::OutputStreamGetter LogBlockOutputStream::createStreamGetter(const NameAndTypePair & name_and_type,
                                                                        WrittenStreams & written_streams)
 {
     return [&] (const IDataType::SubstreamPath & path) -> WriteBuffer *
     {
-        String stream_name = IDataType::getFileNameForStream(name, path);
+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);
         if (written_streams.count(stream_name))
             return nullptr;
 
@@ -368,14 +374,15 @@ IDataType::OutputStreamGetter LogBlockOutputStream::createStreamGetter(const Str
 }
 
 
-void LogBlockOutputStream::writeData(const String & name, const IDataType & type, const IColumn & column,
+void LogBlockOutputStream::writeData(const NameAndTypePair & name_and_type, const IColumn & column,
     MarksForColumns & out_marks, WrittenStreams & written_streams)
 {
     IDataType::SerializeBinaryBulkSettings settings;
+    const auto & [name, type] = name_and_type;
 
-    type.enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)
+    type->enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)
     {
-        String stream_name = IDataType::getFileNameForStream(name, path);
+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);
         if (written_streams.count(stream_name))
             return;
 
@@ -384,18 +391,18 @@ void LogBlockOutputStream::writeData(const String & name, const IDataType & type
             stream_name,
             storage.disk,
             storage.files[stream_name].data_file_path,
-            columns.getCodecOrDefault(name),
+            columns.getCodecOrDefault(name_and_type.name),
             storage.max_compress_block_size);
     }, settings.path);
 
-    settings.getter = createStreamGetter(name, written_streams);
+    settings.getter = createStreamGetter(name_and_type, written_streams);
 
     if (serialize_states.count(name) == 0)
-         type.serializeBinaryBulkStatePrefix(settings, serialize_states[name]);
+         type->serializeBinaryBulkStatePrefix(settings, serialize_states[name]);
 
-    type.enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)
+    type->enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)
     {
-        String stream_name = IDataType::getFileNameForStream(name, path);
+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);
         if (written_streams.count(stream_name))
             return;
 
@@ -409,11 +416,11 @@ void LogBlockOutputStream::writeData(const String & name, const IDataType & type
         out_marks.emplace_back(file.column_index, mark);
     }, settings.path);
 
-    type.serializeBinaryBulkWithMultipleStreams(column, 0, 0, settings, serialize_states[name]);
+    type->serializeBinaryBulkWithMultipleStreams(column, 0, 0, settings, serialize_states[name]);
 
-    type.enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)
+    type->enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)
     {
-        String stream_name = IDataType::getFileNameForStream(name, path);
+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);
         if (!written_streams.emplace(stream_name).second)
             return;
 
@@ -482,21 +489,21 @@ StorageLog::StorageLog(
     }
 
     for (const auto & column : storage_metadata.getColumns().getAllPhysical())
-        addFiles(column.name, *column.type);
+        addFiles(column);
 
     marks_file_path = table_path + DBMS_STORAGE_LOG_MARKS_FILE_NAME;
 }
 
 
-void StorageLog::addFiles(const String & column_name, const IDataType & type)
+void StorageLog::addFiles(const NameAndTypePair & column)
 {
-    if (files.end() != files.find(column_name))
-        throw Exception("Duplicate column with name " + column_name + " in constructor of StorageLog.",
+    if (files.end() != files.find(column.name))
+        throw Exception("Duplicate column with name " + column.name + " in constructor of StorageLog.",
             ErrorCodes::DUPLICATE_COLUMN);
 
     IDataType::StreamCallback stream_callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
-        String stream_name = IDataType::getFileNameForStream(column_name, substream_path);
+        String stream_name = IDataType::getFileNameForStream(column, substream_path);
 
         if (!files.count(stream_name))
         {
@@ -510,7 +517,7 @@ void StorageLog::addFiles(const String & column_name, const IDataType & type)
     };
 
     IDataType::SubstreamPath substream_path;
-    type.enumerateStreams(stream_callback, substream_path);
+    column.type->enumerateStreams(stream_callback, substream_path);
 }
 
 
@@ -583,7 +590,7 @@ void StorageLog::truncate(const ASTPtr &, const StorageMetadataPtr & metadata_sn
     disk->clearDirectory(table_path);
 
     for (const auto & column : metadata_snapshot->getColumns().getAllPhysical())
-        addFiles(column.name, *column.type);
+        addFiles(column);
 
     file_checker = FileChecker{disk, table_path + "sizes.json"};
     marks_file_path = table_path + DBMS_STORAGE_LOG_MARKS_FILE_NAME;
@@ -593,8 +600,7 @@ void StorageLog::truncate(const ASTPtr &, const StorageMetadataPtr & metadata_sn
 const StorageLog::Marks & StorageLog::getMarksWithRealRowCount(const StorageMetadataPtr & metadata_snapshot) const
 {
     /// There should be at least one physical column
-    const String column_name = metadata_snapshot->getColumns().getAllPhysical().begin()->name;
-    const auto column_type = metadata_snapshot->getColumns().getAllPhysical().begin()->type;
+    auto column = *metadata_snapshot->getColumns().getAllPhysical().begin();
     String filename;
 
     /** We take marks from first column.
@@ -602,10 +608,10 @@ const StorageLog::Marks & StorageLog::getMarksWithRealRowCount(const StorageMeta
       * (Example: for Array data type, first stream is array sizes; and number of array sizes is the number of arrays).
       */
     IDataType::SubstreamPath substream_root_path;
-    column_type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
+    column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
         if (filename.empty())
-            filename = IDataType::getFileNameForStream(column_name, substream_path);
+            filename = IDataType::getFileNameForStream(column, substream_path);
     }, substream_root_path);
 
     Files::const_iterator it = files.find(filename);
@@ -640,7 +646,8 @@ Pipe StorageLog::read(
     auto lock_timeout = getLockTimeout(context);
     loadMarks(lock_timeout);
 
-    NamesAndTypesList all_columns = Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names));
+    auto all_columns = metadata_snapshot->getColumns().getAllWithSubcolumns().addTypes(column_names);
+    all_columns = Nested::convertToSubcolumns(all_columns);
 
     std::shared_lock lock(rwlock, lock_timeout);
     if (!lock)
diff --git a/src/Storages/StorageLog.h b/src/Storages/StorageLog.h
index a88b6dfb6ff6..acb03658182e 100644
--- a/src/Storages/StorageLog.h
+++ b/src/Storages/StorageLog.h
@@ -8,6 +8,7 @@
 #include <Storages/IStorage.h>
 #include <Common/FileChecker.h>
 #include <Common/escapeForFileName.h>
+#include <Core/NamesAndTypes.h>
 
 
 namespace DB
@@ -43,6 +44,7 @@ class StorageLog final : public ext::shared_ptr_helper<StorageLog>, public IStor
 
     bool storesDataOnDisk() const override { return true; }
     Strings getDataPaths() const override { return {DB::fullPath(disk, table_path)}; }
+    bool supportsSubcolumns() const override { return true; }
 
 protected:
     /** Attach the table with the appropriate name, along the appropriate path (with / at the end),
@@ -93,7 +95,7 @@ class StorageLog final : public ext::shared_ptr_helper<StorageLog>, public IStor
     String marks_file_path;
 
     /// The order of adding files should not change: it corresponds to the order of the columns in the marks file.
-    void addFiles(const String & column_name, const IDataType & type);
+    void addFiles(const NameAndTypePair & column);
 
     bool loaded_marks = false;
 
diff --git a/src/Storages/StorageMaterializedView.h b/src/Storages/StorageMaterializedView.h
index e1dd73e85806..fab9e28afe34 100644
--- a/src/Storages/StorageMaterializedView.h
+++ b/src/Storages/StorageMaterializedView.h
@@ -26,6 +26,7 @@ class StorageMaterializedView final : public ext::shared_ptr_helper<StorageMater
     bool supportsFinal() const override { return getTargetTable()->supportsFinal(); }
     bool supportsIndexForIn() const override { return getTargetTable()->supportsIndexForIn(); }
     bool supportsParallelInsert() const override { return getTargetTable()->supportsParallelInsert(); }
+    bool supportsSubcolumns() const override { return getTargetTable()->supportsSubcolumns(); }
     bool mayBenefitFromIndexForIn(const ASTPtr & left_in_operand, const Context & query_context, const StorageMetadataPtr & /* metadata_snapshot */) const override
     {
         auto target_table = getTargetTable();
diff --git a/src/Storages/StorageMemory.cpp b/src/Storages/StorageMemory.cpp
index 8651caecdfac..1474fbcee029 100644
--- a/src/Storages/StorageMemory.cpp
+++ b/src/Storages/StorageMemory.cpp
@@ -38,7 +38,7 @@ class MemorySource : public SourceWithProgress
         std::shared_ptr<std::atomic<size_t>> parallel_execution_index_,
         InitializerFunc initializer_func_ = {})
         : SourceWithProgress(metadata_snapshot->getSampleBlockForColumns(column_names_, storage.getVirtuals(), storage.getStorageID()))
-        , column_names(std::move(column_names_))
+        , column_names_and_types(metadata_snapshot->getColumns().getAllWithSubcolumns().addTypes(std::move(column_names_)))
         , data(data_)
         , parallel_execution_index(parallel_execution_index_)
         , initializer_func(std::move(initializer_func_))
@@ -65,11 +65,17 @@ class MemorySource : public SourceWithProgress
 
         const Block & src = (*data)[current_index];
         Columns columns;
-        columns.reserve(column_names.size());
+        columns.reserve(columns.size());
 
         /// Add only required columns to `res`.
-        for (const auto & name : column_names)
-            columns.push_back(src.getByName(name).column);
+        for (const auto & elem : column_names_and_types)
+        {
+            auto current_column = src.getByName(elem.getNameInStorage()).column;
+            if (elem.isSubcolumn())
+                columns.emplace_back(elem.getTypeInStorage()->getSubcolumn(elem.getSubcolumnName(), *current_column));
+            else
+                columns.emplace_back(std::move(current_column));
+        }
 
         return Chunk(std::move(columns), src.rows());
     }
@@ -87,7 +93,7 @@ class MemorySource : public SourceWithProgress
         }
     }
 
-    const Names column_names;
+    const NamesAndTypesList column_names_and_types;
     size_t execution_index = 0;
     std::shared_ptr<const Blocks> data;
     std::shared_ptr<std::atomic<size_t>> parallel_execution_index;
diff --git a/src/Storages/StorageMemory.h b/src/Storages/StorageMemory.h
index 6453e6a53e2e..702cb265ea92 100644
--- a/src/Storages/StorageMemory.h
+++ b/src/Storages/StorageMemory.h
@@ -41,6 +41,8 @@ friend struct ext::shared_ptr_helper<StorageMemory>;
 
     bool supportsParallelInsert() const override { return true; }
 
+    bool supportsSubcolumns() const override { return true; }
+
     BlockOutputStreamPtr write(const ASTPtr & query, const StorageMetadataPtr & metadata_snapshot, const Context & context) override;
 
     void drop() override;
diff --git a/src/Storages/StorageMerge.h b/src/Storages/StorageMerge.h
index 223383a6975a..3ac251fbe521 100644
--- a/src/Storages/StorageMerge.h
+++ b/src/Storages/StorageMerge.h
@@ -25,6 +25,7 @@ class StorageMerge final : public ext::shared_ptr_helper<StorageMerge>, public I
     bool supportsPrewhere() const override { return true; }
     bool supportsFinal() const override { return true; }
     bool supportsIndexForIn() const override { return true; }
+    bool supportsSubcolumns() const override { return true; }
 
     QueryProcessingStage::Enum getQueryProcessingStage(const Context &, QueryProcessingStage::Enum /*to_stage*/, SelectQueryInfo &) const override;
 
diff --git a/src/Storages/StorageTinyLog.cpp b/src/Storages/StorageTinyLog.cpp
index 6e3e9c612bb6..06e2c21b1a8c 100644
--- a/src/Storages/StorageTinyLog.cpp
+++ b/src/Storages/StorageTinyLog.cpp
@@ -64,7 +64,7 @@ class TinyLogSource final : public SourceWithProgress
         for (const auto & name_type : columns)
             res.insert({ name_type.type->createColumn(), name_type.type, name_type.name });
 
-        return Nested::flatten(res);
+        return res;
     }
 
     TinyLogSource(
@@ -113,7 +113,7 @@ class TinyLogSource final : public SourceWithProgress
     using DeserializeStates = std::map<String, DeserializeState>;
     DeserializeStates deserialize_states;
 
-    void readData(const String & name, const IDataType & type, IColumn & column, UInt64 limit);
+    void readData(const NameAndTypePair & name_and_type, ColumnPtr & column, UInt64 limit, IDataType::SubstreamsCache & cache);
 };
 
 
@@ -132,13 +132,14 @@ Chunk TinyLogSource::generate()
         return {};
     }
 
+    std::unordered_map<String, IDataType::SubstreamsCache> caches;
     for (const auto & name_type : columns)
     {
-        MutableColumnPtr column = name_type.type->createColumn();
-
+        ColumnPtr column;
         try
         {
-            readData(name_type.name, *name_type.type, *column, block_size);
+            column = name_type.type->createColumn();
+            readData(name_type, column, block_size, caches[name_type.getNameInStorage()]);
         }
         catch (Exception & e)
         {
@@ -156,32 +157,36 @@ Chunk TinyLogSource::generate()
         streams.clear();
     }
 
-    auto flatten = Nested::flatten(res);
-    return Chunk(flatten.getColumns(), flatten.rows());
+    return Chunk(res.getColumns(), res.rows());
 }
 
 
-void TinyLogSource::readData(const String & name, const IDataType & type, IColumn & column, UInt64 limit)
+void TinyLogSource::readData(const NameAndTypePair & name_and_type,
+    ColumnPtr & column, UInt64 limit, IDataType::SubstreamsCache & cache)
 {
     IDataType::DeserializeBinaryBulkSettings settings; /// TODO Use avg_value_size_hint.
+    const auto & [name, type] = name_and_type;
     settings.getter = [&] (const IDataType::SubstreamPath & path) -> ReadBuffer *
     {
-        String stream_name = IDataType::getFileNameForStream(name, path);
+        if (cache.count(IDataType::getSubcolumnNameForStream(path)))
+            return nullptr;
 
-        if (!streams.count(stream_name))
+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);
+        auto & stream = streams[stream_name];
+        if (!stream)
         {
             String file_path = storage.files[stream_name].data_file_path;
-            streams[stream_name] = std::make_unique<Stream>(
+            stream = std::make_unique<Stream>(
                 storage.disk, file_path, max_read_buffer_size, file_sizes[fileName(file_path)]);
         }
 
-        return &streams[stream_name]->compressed;
+        return &stream->compressed;
     };
 
     if (deserialize_states.count(name) == 0)
-        type.deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);
+         type->deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);
 
-    type.deserializeBinaryBulkWithMultipleStreams(column, limit, settings, deserialize_states[name]);
+    type->deserializeBinaryBulkWithMultipleStreams(column, limit, settings, deserialize_states[name], &cache);
 }
 
 
@@ -262,18 +267,18 @@ class TinyLogBlockOutputStream final : public IBlockOutputStream
 
     using WrittenStreams = std::set<String>;
 
-    IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenStreams & written_streams);
-    void writeData(const String & name, const IDataType & type, const IColumn & column, WrittenStreams & written_streams);
+    IDataType::OutputStreamGetter createStreamGetter(const NameAndTypePair & column, WrittenStreams & written_streams);
+    void writeData(const NameAndTypePair & name_and_type, const IColumn & column, WrittenStreams & written_streams);
 };
 
 
 IDataType::OutputStreamGetter TinyLogBlockOutputStream::createStreamGetter(
-    const String & name,
+    const NameAndTypePair & column,
     WrittenStreams & written_streams)
 {
     return [&] (const IDataType::SubstreamPath & path) -> WriteBuffer *
     {
-        String stream_name = IDataType::getFileNameForStream(name, path);
+        String stream_name = IDataType::getFileNameForStream(column, path);
 
         if (!written_streams.insert(stream_name).second)
             return nullptr;
@@ -283,7 +288,7 @@ IDataType::OutputStreamGetter TinyLogBlockOutputStream::createStreamGetter(
             streams[stream_name] = std::make_unique<Stream>(
                 storage.disk,
                 storage.files[stream_name].data_file_path,
-                columns.getCodecOrDefault(name),
+                columns.getCodecOrDefault(column.name),
                 storage.max_compress_block_size);
 
         return &streams[stream_name]->compressed;
@@ -291,21 +296,22 @@ IDataType::OutputStreamGetter TinyLogBlockOutputStream::createStreamGetter(
 }
 
 
-void TinyLogBlockOutputStream::writeData(const String & name, const IDataType & type, const IColumn & column, WrittenStreams & written_streams)
+void TinyLogBlockOutputStream::writeData(const NameAndTypePair & name_and_type, const IColumn & column, WrittenStreams & written_streams)
 {
     IDataType::SerializeBinaryBulkSettings settings;
+    const auto & [name, type] = name_and_type;
 
     if (serialize_states.count(name) == 0)
     {
         /// Some stream getters may be called form `serializeBinaryBulkStatePrefix`.
         /// Use different WrittenStreams set, or we get nullptr for them in `serializeBinaryBulkWithMultipleStreams`
         WrittenStreams prefix_written_streams;
-        settings.getter = createStreamGetter(name, prefix_written_streams);
-        type.serializeBinaryBulkStatePrefix(settings, serialize_states[name]);
+        settings.getter = createStreamGetter(name_and_type, prefix_written_streams);
+        type->serializeBinaryBulkStatePrefix(settings, serialize_states[name]);
     }
 
-    settings.getter = createStreamGetter(name, written_streams);
-    type.serializeBinaryBulkWithMultipleStreams(column, 0, 0, settings, serialize_states[name]);
+    settings.getter = createStreamGetter(name_and_type, written_streams);
+    type->serializeBinaryBulkWithMultipleStreams(column, 0, 0, settings, serialize_states[name]);
 }
 
 
@@ -328,7 +334,7 @@ void TinyLogBlockOutputStream::writeSuffix()
         auto it = serialize_states.find(column.name);
         if (it != serialize_states.end())
         {
-            settings.getter = createStreamGetter(column.name, written_streams);
+            settings.getter = createStreamGetter(NameAndTypePair(column.name, column.type), written_streams);
             column.type->serializeBinaryBulkStateSuffix(settings, it->second);
         }
     }
@@ -360,7 +366,7 @@ void TinyLogBlockOutputStream::write(const Block & block)
     for (size_t i = 0; i < block.columns(); ++i)
     {
         const ColumnWithTypeAndName & column = block.safeGetByPosition(i);
-        writeData(column.name, *column.type, *column.column, written_streams);
+        writeData(NameAndTypePair(column.name, column.type), *column.column, written_streams);
     }
 }
 
@@ -406,19 +412,20 @@ StorageTinyLog::StorageTinyLog(
     }
 
     for (const auto & col : storage_metadata.getColumns().getAllPhysical())
-        addFiles(col.name, *col.type);
+        addFiles(col);
 }
 
 
-void StorageTinyLog::addFiles(const String & column_name, const IDataType & type)
+void StorageTinyLog::addFiles(const NameAndTypePair & column)
 {
-    if (files.end() != files.find(column_name))
-        throw Exception("Duplicate column with name " + column_name + " in constructor of StorageTinyLog.",
+    const auto & [name, type] = column;
+    if (files.end() != files.find(name))
+        throw Exception("Duplicate column with name " + name + " in constructor of StorageTinyLog.",
             ErrorCodes::DUPLICATE_COLUMN);
 
     IDataType::StreamCallback stream_callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)
     {
-        String stream_name = IDataType::getFileNameForStream(column_name, substream_path);
+        String stream_name = IDataType::getFileNameForStream(column, substream_path);
         if (!files.count(stream_name))
         {
             ColumnData column_data;
@@ -428,7 +435,7 @@ void StorageTinyLog::addFiles(const String & column_name, const IDataType & type
     };
 
     IDataType::SubstreamPath substream_path;
-    type.enumerateStreams(stream_callback, substream_path);
+    type->enumerateStreams(stream_callback, substream_path);
 }
 
 
@@ -469,6 +476,8 @@ Pipe StorageTinyLog::read(
 {
     metadata_snapshot->check(column_names, getVirtuals(), getStorageID());
 
+    auto all_columns = metadata_snapshot->getColumns().getAllWithSubcolumns().addTypes(column_names);
+
     // When reading, we lock the entire storage, because we only have one file
     // per column and can't modify it concurrently.
     const Settings & settings = context.getSettingsRef();
@@ -480,7 +489,7 @@ Pipe StorageTinyLog::read(
     /// No need to hold lock while reading because we read fixed range of data that does not change while appending more data.
     return Pipe(std::make_shared<TinyLogSource>(
         max_block_size,
-        Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names)),
+        Nested::convertToSubcolumns(all_columns),
         *this,
         settings.max_read_buffer_size,
         file_checker.getFileSizes()));
@@ -511,7 +520,7 @@ void StorageTinyLog::truncate(
     file_checker = FileChecker{disk, table_path + "sizes.json"};
 
     for (const auto & column : metadata_snapshot->getColumns().getAllPhysical())
-        addFiles(column.name, *column.type);
+        addFiles(column);
 }
 
 
diff --git a/src/Storages/StorageTinyLog.h b/src/Storages/StorageTinyLog.h
index 1398af24f828..b76e8e34dfb8 100644
--- a/src/Storages/StorageTinyLog.h
+++ b/src/Storages/StorageTinyLog.h
@@ -41,6 +41,7 @@ class StorageTinyLog final : public ext::shared_ptr_helper<StorageTinyLog>, publ
 
     bool storesDataOnDisk() const override { return true; }
     Strings getDataPaths() const override { return {DB::fullPath(disk, table_path)}; }
+    bool supportsSubcolumns() const override { return true; }
 
     void truncate(const ASTPtr &, const StorageMetadataPtr & metadata_snapshot, const Context &, TableExclusiveLockHolder &) override;
 
@@ -73,7 +74,7 @@ class StorageTinyLog final : public ext::shared_ptr_helper<StorageTinyLog>, publ
 
     Poco::Logger * log;
 
-    void addFiles(const String & column_name, const IDataType & type);
+    void addFiles(const NameAndTypePair & column);
 };
 
 }
