diff --git a/programs/local/LocalServer.cpp b/programs/local/LocalServer.cpp
index df40ce3fa5d0..7e4b458eca6c 100644
--- a/programs/local/LocalServer.cpp
+++ b/programs/local/LocalServer.cpp
@@ -831,6 +831,9 @@ void LocalServer::processConfig()
     /// Initialize a dummy query cache.
     global_context->setQueryCache(0, 0, 0, 0);
 
+    /// Initialize a dummy query condition cache.
+    global_context->setQueryConditionCache(DEFAULT_QUERY_CONDITION_CACHE_POLICY, 0, 0);
+
     /// Initialize allowed tiers
     global_context->getAccessControl().setAllowTierSettings(server_settings[ServerSetting::allow_feature_tier]);
 
diff --git a/programs/server/Server.cpp b/programs/server/Server.cpp
index 4fc6e07886d5..c062adfeb190 100644
--- a/programs/server/Server.cpp
+++ b/programs/server/Server.cpp
@@ -279,6 +279,9 @@ namespace ServerSetting
     extern const ServerSettingsUInt64 page_cache_chunk_size;
     extern const ServerSettingsUInt64 page_cache_mmap_size;
     extern const ServerSettingsUInt64 page_cache_size;
+    extern const ServerSettingsString query_condition_cache_policy;
+    extern const ServerSettingsUInt64 query_condition_cache_size;
+    extern const ServerSettingsDouble query_condition_cache_size_ratio;
     extern const ServerSettingsBool page_cache_use_madv_free;
     extern const ServerSettingsBool page_cache_use_transparent_huge_pages;
     extern const ServerSettingsBool prepare_system_log_tables_on_startup;
@@ -1715,6 +1718,16 @@ try
     }
     global_context->setQueryCache(query_cache_max_size_in_bytes, query_cache_max_entries, query_cache_query_cache_max_entry_size_in_bytes, query_cache_max_entry_size_in_rows);
 
+    String query_condition_cache_policy = server_settings[ServerSetting::query_condition_cache_policy];
+    size_t query_condition_cache_size = server_settings[ServerSetting::query_condition_cache_size];
+    double query_condition_cache_size_ratio = server_settings[ServerSetting::query_condition_cache_size_ratio];
+    if (query_condition_cache_size > max_cache_size)
+    {
+        query_condition_cache_size = max_cache_size;
+        LOG_INFO(log, "Lowered query condition cache size to {} because the system has limited RAM", formatReadableSizeWithBinarySuffix(query_condition_cache_size));
+    }
+    global_context->setQueryConditionCache(query_condition_cache_policy, query_condition_cache_size, query_condition_cache_size_ratio);
+
 #if USE_EMBEDDED_COMPILER
     size_t compiled_expression_cache_max_size_in_bytes = server_settings[ServerSetting::compiled_expression_cache_size];
     size_t compiled_expression_cache_max_elements = server_settings[ServerSetting::compiled_expression_cache_elements_size];
@@ -2010,6 +2023,7 @@ try
             global_context->updateSkippingIndexCacheConfiguration(*config);
             global_context->updateMMappedFileCacheConfiguration(*config);
             global_context->updateQueryCacheConfiguration(*config);
+            global_context->updateQueryConditionCacheConfiguration(*config);
 
             CompressionCodecEncrypted::Configuration::instance().tryLoad(*config, "encryption_codecs");
 #if USE_SSL
diff --git a/src/Access/Common/AccessType.h b/src/Access/Common/AccessType.h
index 77edd1630fc6..32084eef9f1c 100644
--- a/src/Access/Common/AccessType.h
+++ b/src/Access/Common/AccessType.h
@@ -173,6 +173,7 @@ enum class AccessType : uint8_t
     M(SYSTEM_DROP_SKIPPING_INDEX_CACHE, "SYSTEM DROP SKIPPING INDEX CACHE, DROP SKIPPING INDEX CACHE", GLOBAL, SYSTEM_DROP_CACHE) \
     M(SYSTEM_DROP_MMAP_CACHE, "SYSTEM DROP MMAP, DROP MMAP CACHE, DROP MMAP", GLOBAL, SYSTEM_DROP_CACHE) \
     M(SYSTEM_DROP_QUERY_CACHE, "SYSTEM DROP QUERY, DROP QUERY CACHE, DROP QUERY", GLOBAL, SYSTEM_DROP_CACHE) \
+    M(SYSTEM_DROP_QUERY_CONDITION_CACHE, "SYSTEM DROP QUERY CONDITION, DROP QUERY CONDITION CACHE, DROP QUERY CONDITION", GLOBAL, SYSTEM_DROP_CACHE) \
     M(SYSTEM_DROP_COMPILED_EXPRESSION_CACHE, "SYSTEM DROP COMPILED EXPRESSION, DROP COMPILED EXPRESSION CACHE, DROP COMPILED EXPRESSIONS", GLOBAL, SYSTEM_DROP_CACHE) \
     M(SYSTEM_DROP_FILESYSTEM_CACHE, "SYSTEM DROP FILESYSTEM CACHE, DROP FILESYSTEM CACHE", GLOBAL, SYSTEM_DROP_CACHE) \
     M(SYSTEM_DROP_DISTRIBUTED_CACHE, "SYSTEM DROP DISTRIBUTED CACHE, DROP DISTRIBUTED CACHE", GLOBAL, SYSTEM_DROP_CACHE) \
diff --git a/src/Common/ProfileEvents.cpp b/src/Common/ProfileEvents.cpp
index 421793001a48..7ca525285189 100644
--- a/src/Common/ProfileEvents.cpp
+++ b/src/Common/ProfileEvents.cpp
@@ -75,6 +75,8 @@
     M(SkippingIndexCacheWeightLost, "Approximate number of bytes evicted from the secondary index cache.", ValueType::Number) \
     M(QueryCacheHits, "Number of times a query result has been found in the query cache (and query computation was avoided). Only updated for SELECT queries with SETTING use_query_cache = 1.", ValueType::Number) \
     M(QueryCacheMisses, "Number of times a query result has not been found in the query cache (and required query computation). Only updated for SELECT queries with SETTING use_query_cache = 1.", ValueType::Number) \
+    M(QueryConditionCacheHits, "Number of times an entry has been found in the query condition cache (and reading of marks can be skipped). Only updated for SELECT queries with SETTING use_query_condition_cache = 1.", ValueType::Number) \
+    M(QueryConditionCacheMisses, "Number of times an entry has not been found in the query condition cache (and reading of mark cannot be skipped). Only updated for SELECT queries with SETTING use_query_condition_cache = 1.", ValueType::Number) \
     /* Each page cache chunk access increments exactly one of the following 5 PageCacheChunk* counters. */ \
     /* Something like hit rate: (PageCacheChunkShared + PageCacheChunkDataHits) / [sum of all 5]. */ \
     M(PageCacheChunkMisses, "Number of times a chunk has not been found in the userspace page cache.", ValueType::Number) \
diff --git a/src/Core/Defines.h b/src/Core/Defines.h
index 140463c12126..583371c1c9e4 100644
--- a/src/Core/Defines.h
+++ b/src/Core/Defines.h
@@ -115,6 +115,9 @@ static constexpr auto DEFAULT_QUERY_CACHE_MAX_SIZE = 1_GiB;
 static constexpr auto DEFAULT_QUERY_CACHE_MAX_ENTRIES = 1024uz;
 static constexpr auto DEFAULT_QUERY_CACHE_MAX_ENTRY_SIZE_IN_BYTES = 1_MiB;
 static constexpr auto DEFAULT_QUERY_CACHE_MAX_ENTRY_SIZE_IN_ROWS = 30'000'000uz;
+static constexpr auto DEFAULT_QUERY_CONDITION_CACHE_POLICY = "SLRU";
+static constexpr auto DEFAULT_QUERY_CONDITION_CACHE_MAX_SIZE = 100_MiB;
+static constexpr auto DEFAULT_QUERY_CONDITION_CACHE_SIZE_RATIO = 0.5l;
 
 /// Query profiler cannot work with sanitizers.
 /// Sanitizers are using quick "frame walking" stack unwinding (this implies -fno-omit-frame-pointer)
diff --git a/src/Core/ServerSettings.cpp b/src/Core/ServerSettings.cpp
index 0135131c184b..18c9a6ac4993 100644
--- a/src/Core/ServerSettings.cpp
+++ b/src/Core/ServerSettings.cpp
@@ -511,6 +511,10 @@ namespace DB
     DECLARE(UInt64, compiled_expression_cache_size, DEFAULT_COMPILED_EXPRESSION_CACHE_MAX_SIZE, R"(Sets the cache size (in bytes) for [compiled expressions](../../operations/caches.md).)", 0) \
     \
     DECLARE(UInt64, compiled_expression_cache_elements_size, DEFAULT_COMPILED_EXPRESSION_CACHE_MAX_ENTRIES, R"(Sets the cache size (in elements) for [compiled expressions](../../operations/caches.md).)", 0) \
+    DECLARE(String, query_condition_cache_policy, DEFAULT_QUERY_CONDITION_CACHE_POLICY, "Query condition cache policy name.", 0) \
+    DECLARE(UInt64, query_condition_cache_size, DEFAULT_QUERY_CONDITION_CACHE_MAX_SIZE, "Size of the query condition cache.", 0) \
+    DECLARE(Double, query_condition_cache_size_ratio, DEFAULT_QUERY_CONDITION_CACHE_SIZE_RATIO, "The size of the protected queue in the query condition cache relative to the cache's total size.", 0) \
+    \
     DECLARE(Bool, disable_internal_dns_cache, false, "Disable internal DNS caching at all.", 0) \
     DECLARE(UInt64, dns_cache_max_entries, 10000, R"(Internal DNS cache max entries.)", 0) \
     DECLARE(Int32, dns_cache_update_period, 15, "Internal DNS cache update period in seconds.", 0) \
diff --git a/src/Core/Settings.cpp b/src/Core/Settings.cpp
index 376505392ff6..8830f0303a49 100644
--- a/src/Core/Settings.cpp
+++ b/src/Core/Settings.cpp
@@ -4377,7 +4377,14 @@ Possible values:
     DECLARE(Bool, enable_sharing_sets_for_mutations, true, R"(
 Allow sharing set objects build for IN subqueries between different tasks of the same mutation. This reduces memory usage and CPU consumption
 )", 0) \
-    \
+    DECLARE(Bool, use_query_condition_cache, false, R"(
+Enable the query condition cache.
+
+Possible values:
+
+- 0 - Disabled
+- 1 - Enabled
+)", 0) \
     DECLARE(Bool, optimize_rewrite_sum_if_to_count_if, true, R"(
 Rewrite sumIf() and sum(if()) function countIf() function when logically equivalent
 )", 0) \
diff --git a/src/Core/SettingsChangesHistory.cpp b/src/Core/SettingsChangesHistory.cpp
index eeee78a933e3..3b9188643137 100644
--- a/src/Core/SettingsChangesHistory.cpp
+++ b/src/Core/SettingsChangesHistory.cpp
@@ -79,6 +79,7 @@ const VersionToSettingsChangesMap & getSettingsChangesHistory()
             {"output_format_parquet_bloom_filter_flush_threshold_bytes", 128 * 1024 * 1024, 128 * 1024 * 1024, "New setting."},
             {"output_format_pretty_max_rows", 10000, 1000, "It is better for usability - less amount to scroll."},
             {"restore_replicated_merge_tree_to_shared_merge_tree", false, false, "New setting."},
+            {"use_query_condition_cache", false, false, "New setting."},
             {"parallel_replicas_only_with_analyzer", false, true, "Parallel replicas is supported only with analyzer enabled"},
             {"s3_allow_multipart_copy", true, true, "New setting."},
         });
diff --git a/src/Interpreters/ActionsDAG.cpp b/src/Interpreters/ActionsDAG.cpp
index a7cc531e7499..c37c3773e12f 100644
--- a/src/Interpreters/ActionsDAG.cpp
+++ b/src/Interpreters/ActionsDAG.cpp
@@ -26,6 +26,7 @@
 #include <stack>
 #include <base/sort.h>
 #include <Common/JSONBuilder.h>
+#include <Common/SipHash.h>
 #include <DataTypes/DataTypeSet.h>
 
 #include <absl/container/flat_hash_map.h>
@@ -135,6 +136,38 @@ void ActionsDAG::Node::toTree(JSONBuilder::JSONMap & map) const
         map.add("Compiled", is_function_compiled);
 }
 
+size_t ActionsDAG::Node::getHash() const
+{
+    SipHash hash_state;
+    updateHash(hash_state);
+    return hash_state.get64();
+}
+
+void ActionsDAG::Node::updateHash(SipHash & hash_state) const
+{
+    hash_state.update(type);
+
+    if (!result_name.empty())
+        hash_state.update(result_name);
+
+    if (result_type)
+        hash_state.update(result_type->getName());
+
+    if (function_base)
+        hash_state.update(function_base->getName());
+
+    if (function)
+        hash_state.update(function->getName());
+
+    hash_state.update(is_function_compiled);
+    hash_state.update(is_deterministic_constant);
+
+    if (column)
+        hash_state.update(column->getName());
+
+    for (const auto & child : children)
+        child->updateHash(hash_state);
+}
 
 ActionsDAG::ActionsDAG(const NamesAndTypesList & inputs_)
 {
diff --git a/src/Interpreters/ActionsDAG.h b/src/Interpreters/ActionsDAG.h
index a4981200cc67..ee15438cd4f5 100644
--- a/src/Interpreters/ActionsDAG.h
+++ b/src/Interpreters/ActionsDAG.h
@@ -4,6 +4,7 @@
 #include <Core/ColumnsWithTypeAndName.h>
 #include <Core/NamesAndTypes.h>
 #include <Core/Names.h>
+#include <Common/SipHash.h>
 #include <Interpreters/Context_fwd.h>
 
 #include "config.h"
@@ -93,6 +94,9 @@ class ActionsDAG
         /// If result of this not is deterministic. Checks only this node, not a subtree.
         bool isDeterministic() const;
         void toTree(JSONBuilder::JSONMap & map) const;
+        size_t getHash() const;
+    private:
+        void updateHash(SipHash & hash_state) const;
     };
 
     /// NOTE: std::list is an implementation detail.
diff --git a/src/Interpreters/Cache/QueryConditionCache.cpp b/src/Interpreters/Cache/QueryConditionCache.cpp
new file mode 100644
index 000000000000..3ebccff0f036
--- /dev/null
+++ b/src/Interpreters/Cache/QueryConditionCache.cpp
@@ -0,0 +1,101 @@
+#include <Interpreters/Cache/QueryConditionCache.h>
+#include <Storages/MergeTree/MergeTreeData.h>
+#include "Interpreters/Cache/FileSegmentInfo.h"
+
+namespace ProfileEvents
+{
+    extern const Event QueryConditionCacheHits;
+    extern const Event QueryConditionCacheMisses;
+};
+
+namespace DB
+{
+
+QueryConditionCache::QueryConditionCache(const String & cache_policy, size_t max_size_in_bytes, double size_ratio)
+    : cache(cache_policy, max_size_in_bytes, 0, size_ratio)
+{
+}
+
+std::optional<QueryConditionCache::MatchingMarks> QueryConditionCache::read(const UUID & table_id, const String & part_name, size_t condition_hash)
+{
+    Key key = {table_id, part_name, condition_hash};
+
+    if (auto entry = cache.get(key))
+    {
+        ProfileEvents::increment(ProfileEvents::QueryConditionCacheHits);
+
+        std::lock_guard lock(entry->mutex);
+        return {entry->matching_marks};
+    }
+
+    ProfileEvents::increment(ProfileEvents::QueryConditionCacheMisses);
+
+    return std::nullopt;
+}
+
+void QueryConditionCache::write(const UUID & table_id, const String & part_name, size_t condition_hash, const MarkRanges & mark_ranges, size_t marks_count, bool has_final_mark)
+{
+    Key key = {table_id, part_name, condition_hash};
+
+    auto load_func = [&](){ return std::make_shared<Entry>(marks_count); };
+    auto [entry, _] = cache.getOrSet(key, load_func);
+
+    chassert(marks_count == entry->matching_marks.size());
+
+    /// Set MarkRanges to false, so there is no need to read these marks again later.
+    {
+        std::lock_guard lock(entry->mutex);
+        for (const auto & mark_range : mark_ranges)
+            std::fill(entry->matching_marks.begin() + mark_range.begin, entry->matching_marks.begin() + mark_range.end, false);
+
+        if (has_final_mark)
+            entry->matching_marks[marks_count - 1] = false;
+
+        LOG_DEBUG(
+            logger,
+            "table_id: {}, part_name: {}, condition_hash: {}, marks_count: {}, has_final_mark: {}, (ranges: {})",
+            table_id,
+            part_name,
+            condition_hash,
+            marks_count,
+            has_final_mark,
+            toString(mark_ranges));
+    }
+}
+
+void QueryConditionCache::clear()
+{
+    cache.clear();
+}
+
+void QueryConditionCache::setMaxSizeInBytes(size_t max_size_in_bytes)
+{
+    cache.setMaxSizeInBytes(max_size_in_bytes);
+}
+
+bool QueryConditionCache::Key::operator==(const Key & other) const
+{
+    return table_id == other.table_id && part_name == other.part_name && condition_hash == other.condition_hash;
+}
+
+QueryConditionCache::Entry::Entry(size_t mark_count)
+    : matching_marks(mark_count, true) /// by default, all marks potentially are potential matches
+{
+}
+
+size_t QueryConditionCache::KeyHasher::operator()(const Key & key) const
+{
+    SipHash hash;
+    hash.update(key.table_id);
+    hash.update(key.part_name);
+    hash.update(key.condition_hash);
+    return hash.get64();
+}
+
+size_t QueryConditionCache::QueryConditionCacheEntryWeight::operator()(const Entry & entry) const
+{
+    /// Estimate the memory size of `std::vector<bool>`, for bool values, only 1 bit per element.
+    size_t dynamic_memory = (entry.matching_marks.capacity() + 7) / 8; /// Round up to bytes.
+    return sizeof(decltype(entry.matching_marks)) + dynamic_memory;
+}
+}
diff --git a/src/Interpreters/Cache/QueryConditionCache.h b/src/Interpreters/Cache/QueryConditionCache.h
new file mode 100644
index 000000000000..7fc4d14f0a36
--- /dev/null
+++ b/src/Interpreters/Cache/QueryConditionCache.h
@@ -0,0 +1,66 @@
+#pragma once
+
+#include <Common/CacheBase.h>
+#include <Storages/MergeTree/MarkRange.h>
+
+namespace DB
+{
+
+/// Cache the mark filter corresponding to the query condition,
+/// which helps to quickly filter out useless Marks and speed up the query when the index is not hit.
+class QueryConditionCache
+{
+public:
+    /// 0 means none of the rows in the mark match the predicate. We can skip such marks.
+    /// 1 means at least one row in the mark matches the predicate. We need to read such marks.
+    using MatchingMarks = std::vector<bool>;
+
+    QueryConditionCache(const String & cache_policy, size_t max_size_in_bytes, double size_ratio);
+
+    /// Read the filter and return empty if it does not exist.
+    std::optional<MatchingMarks> read(const UUID & table_id, const String & part_name, size_t condition_hash);
+
+    /// Take out the mark filter corresponding to the query condition and set it to false on the corresponding mark.
+    void write(const UUID & table_id, const String & part_name, size_t condition_hash, const MarkRanges & mark_ranges, size_t marks_count, bool has_final_mark);
+
+    void clear();
+
+    void setMaxSizeInBytes(size_t max_size_in_bytes);
+
+private:
+    struct Key
+    {
+        const UUID table_id;
+        const String part_name;
+        const size_t condition_hash;
+
+        bool operator==(const Key & other) const;
+    };
+
+    struct Entry
+    {
+        MatchingMarks matching_marks;
+        std::mutex mutex;
+
+        explicit Entry(size_t mark_count);
+    };
+
+    struct KeyHasher
+    {
+        size_t operator()(const Key & key) const;
+    };
+
+    struct QueryConditionCacheEntryWeight
+    {
+        size_t operator()(const Entry & entry) const;
+    };
+
+    using Cache = CacheBase<Key, Entry, KeyHasher, QueryConditionCacheEntryWeight>;
+    Cache cache;
+
+    LoggerPtr logger = getLogger("QueryConditionCache");
+};
+
+using QueryConditionCachePtr = std::shared_ptr<QueryConditionCache>;
+
+}
diff --git a/src/Interpreters/Context.cpp b/src/Interpreters/Context.cpp
index c957482270bc..b7464792d33f 100644
--- a/src/Interpreters/Context.cpp
+++ b/src/Interpreters/Context.cpp
@@ -54,9 +54,10 @@
 #include <Interpreters/ActionLocksManager.h>
 #include <Interpreters/ExternalLoaderXMLConfigRepository.h>
 #include <Interpreters/TemporaryDataOnDisk.h>
-#include <Interpreters/Cache/QueryCache.h>
 #include <Interpreters/Cache/FileCacheFactory.h>
 #include <Interpreters/Cache/FileCache.h>
+#include <Interpreters/Cache/QueryCache.h>
+#include <Interpreters/Cache/QueryConditionCache.h>
 #include <Interpreters/SessionTracker.h>
 #include <Core/ServerSettings.h>
 #include <Interpreters/PreparedSets.h>
@@ -442,6 +443,7 @@ struct ContextSharedPart : boost::noncopyable
     mutable QueryCachePtr query_cache TSA_GUARDED_BY(mutex);                          /// Cache of query results.
     mutable MarkCachePtr index_mark_cache TSA_GUARDED_BY(mutex);                      /// Cache of marks in compressed files of MergeTree indices.
     mutable MMappedFileCachePtr mmap_cache TSA_GUARDED_BY(mutex);                     /// Cache of mmapped files to avoid frequent open/map/unmap/close and to reuse from several threads.
+    mutable QueryConditionCachePtr query_condition_cache TSA_GUARDED_BY(mutex);       /// Mark filter for caching query conditions
     AsynchronousMetrics * asynchronous_metrics TSA_GUARDED_BY(mutex) = nullptr;       /// Points to asynchronous metrics
     mutable PageCachePtr page_cache TSA_GUARDED_BY(mutex);                            /// Userspace page cache.
     ProcessList process_list;                                   /// Executing queries at the moment.
@@ -3618,6 +3620,41 @@ void Context::clearQueryCache(const std::optional<String> & tag) const
         cache->clear(tag);
 }
 
+void Context::setQueryConditionCache(const String & cache_policy, size_t max_size_in_bytes, double size_ratio)
+{
+    std::lock_guard lock(shared->mutex);
+
+    if (shared->query_condition_cache)
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Mark filter cache has been already create.");
+
+    shared->query_condition_cache = std::make_shared<QueryConditionCache>(cache_policy, max_size_in_bytes, size_ratio);
+}
+
+QueryConditionCachePtr Context::getQueryConditionCache() const
+{
+    SharedLockGuard lock(shared->mutex);
+    return shared->query_condition_cache;
+}
+
+void Context::updateQueryConditionCacheConfiguration(const Poco::Util::AbstractConfiguration & config)
+{
+    std::lock_guard lock(shared->mutex);
+
+    if (!shared->query_condition_cache)
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Query condition cache was not created yet.");
+
+    size_t max_size_in_bytes = config.getUInt64("query_condition_cache_size", DEFAULT_QUERY_CONDITION_CACHE_MAX_SIZE);
+    shared->query_condition_cache->setMaxSizeInBytes(max_size_in_bytes);
+}
+
+void Context::clearQueryConditionCache() const
+{
+    std::lock_guard lock(shared->mutex);
+
+    if (shared->query_condition_cache)
+        shared->query_condition_cache->clear();
+}
+
 void Context::clearCaches() const
 {
     std::lock_guard lock(shared->mutex);
@@ -3651,6 +3688,10 @@ void Context::clearCaches() const
     shared->mmap_cache->clear();
 
     /// Intentionally not clearing the query cache which is transactionally inconsistent by design.
+
+    if (!shared->query_condition_cache)
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Query condition cache was not created yet.");
+    shared->query_condition_cache->clear();
 }
 
 void Context::setAsynchronousMetrics(AsynchronousMetrics * asynchronous_metrics_)
diff --git a/src/Interpreters/Context.h b/src/Interpreters/Context.h
index bb86a73aad64..14962fe0791b 100644
--- a/src/Interpreters/Context.h
+++ b/src/Interpreters/Context.h
@@ -106,6 +106,7 @@ struct Progress;
 struct FileProgress;
 class Clusters;
 class QueryCache;
+class QueryConditionCache;
 class ISystemLog;
 class QueryLog;
 class QueryMetricLog;
@@ -1128,6 +1129,11 @@ class Context: public ContextData, public std::enable_shared_from_this<Context>
     std::shared_ptr<QueryCache> getQueryCache() const;
     void clearQueryCache(const std::optional<String> & tag) const;
 
+    void setQueryConditionCache(const String & cache_policy, size_t max_size_in_bytes, double size_ratio);
+    void updateQueryConditionCacheConfiguration(const Poco::Util::AbstractConfiguration & config);
+    std::shared_ptr<QueryConditionCache> getQueryConditionCache() const;
+    void clearQueryConditionCache() const;
+
     /** Clear the caches of the uncompressed blocks and marks.
       * This is usually done when renaming tables, changing the type of columns, deleting a table.
       *  - since caches are linked to file names, and become incorrect.
diff --git a/src/Interpreters/InterpreterSystemQuery.cpp b/src/Interpreters/InterpreterSystemQuery.cpp
index fc0f9594d0b9..53d3900bfce1 100644
--- a/src/Interpreters/InterpreterSystemQuery.cpp
+++ b/src/Interpreters/InterpreterSystemQuery.cpp
@@ -414,7 +414,12 @@ BlockIO InterpreterSystemQuery::execute()
             getContext()->clearQueryCache(query.query_cache_tag);
             break;
         }
-
+        case Type::DROP_QUERY_CONDITION_CACHE:
+        {
+            getContext()->checkAccess(AccessType::SYSTEM_DROP_QUERY_CONDITION_CACHE);
+            getContext()->clearQueryConditionCache();
+            break;
+        }
         case Type::DROP_COMPILED_EXPRESSION_CACHE:
 #if USE_EMBEDDED_COMPILER
             getContext()->checkAccess(AccessType::SYSTEM_DROP_COMPILED_EXPRESSION_CACHE);
@@ -1435,6 +1440,7 @@ AccessRightsElements InterpreterSystemQuery::getRequiredAccessForDDLOnCluster()
         case Type::DROP_PRIMARY_INDEX_CACHE:
         case Type::DROP_MMAP_CACHE:
         case Type::DROP_QUERY_CACHE:
+        case Type::DROP_QUERY_CONDITION_CACHE:
         case Type::DROP_COMPILED_EXPRESSION_CACHE:
         case Type::DROP_UNCOMPRESSED_CACHE:
         case Type::DROP_INDEX_MARK_CACHE:
diff --git a/src/Parsers/ASTSystemQuery.cpp b/src/Parsers/ASTSystemQuery.cpp
index 43a100fc57ad..60d70cbec446 100644
--- a/src/Parsers/ASTSystemQuery.cpp
+++ b/src/Parsers/ASTSystemQuery.cpp
@@ -423,6 +423,7 @@ void ASTSystemQuery::formatImpl(WriteBuffer & ostr, const FormatSettings & setti
         case Type::DROP_CONNECTIONS_CACHE:
         case Type::DROP_MMAP_CACHE:
         case Type::DROP_QUERY_CACHE:
+        case Type::DROP_QUERY_CONDITION_CACHE:
         case Type::DROP_MARK_CACHE:
         case Type::DROP_PRIMARY_INDEX_CACHE:
         case Type::DROP_INDEX_MARK_CACHE:
diff --git a/src/Parsers/ASTSystemQuery.h b/src/Parsers/ASTSystemQuery.h
index fb580bf48194..4dcf7f4b8c54 100644
--- a/src/Parsers/ASTSystemQuery.h
+++ b/src/Parsers/ASTSystemQuery.h
@@ -33,6 +33,7 @@ class ASTSystemQuery : public IAST, public ASTQueryWithOnCluster
         DROP_SKIPPING_INDEX_CACHE,
         DROP_MMAP_CACHE,
         DROP_QUERY_CACHE,
+        DROP_QUERY_CONDITION_CACHE,
         DROP_COMPILED_EXPRESSION_CACHE,
         DROP_FILESYSTEM_CACHE,
         DROP_DISK_METADATA_CACHE,
diff --git a/src/Processors/Chunk.h b/src/Processors/Chunk.h
index e5802e3dcccc..e1a4f36f7165 100644
--- a/src/Processors/Chunk.h
+++ b/src/Processors/Chunk.h
@@ -2,6 +2,8 @@
 
 #include <Columns/IColumn_fwd.h>
 #include <Common/CollectionOfDerived.h>
+#include <Columns/IColumn.h>
+#include <Storages/MergeTree/MarkRange.h>
 
 #include <memory>
 
@@ -156,6 +158,24 @@ class AsyncInsertInfo : public ChunkInfoCloneable<AsyncInsertInfo>
 
 using AsyncInsertInfoPtr = std::shared_ptr<AsyncInsertInfo>;
 
+class IMergeTreeDataPart;
+
+/// The query condition cache needs to know the mark ranges of which part the chunk data comes from.
+class MarkRangesInfo : public ChunkInfoCloneable<MarkRangesInfo>
+{
+public:
+    MarkRangesInfo(std::shared_ptr<const IMergeTreeDataPart> data_part_, MarkRanges mark_ranges_)
+        : data_part(data_part_)
+        , mark_ranges(std::move(mark_ranges_))
+    {}
+
+    std::shared_ptr<const IMergeTreeDataPart> getDataPart() const { return data_part; }
+    const MarkRanges & getMarkRanges() const { return mark_ranges; }
+private:
+    std::shared_ptr<const IMergeTreeDataPart> data_part;
+    MarkRanges mark_ranges;
+};
+
 /// Converts all columns to full serialization in chunk.
 /// It's needed, when you have to access to the internals of the column,
 /// or when you need to perform operation with two columns
diff --git a/src/Processors/QueryPlan/FilterStep.cpp b/src/Processors/QueryPlan/FilterStep.cpp
index fd52a6b14ccf..983fe80c1b0c 100644
--- a/src/Processors/QueryPlan/FilterStep.cpp
+++ b/src/Processors/QueryPlan/FilterStep.cpp
@@ -173,6 +173,10 @@ void FilterStep::transformPipeline(QueryPipelineBuilder & pipeline, const BuildQ
     pipeline.addSimpleTransform([&](const Block & header, QueryPipelineBuilder::StreamType stream_type)
     {
         bool on_totals = stream_type == QueryPipelineBuilder::StreamType::Totals;
+        if (condition_hash)
+            return std::make_shared<FilterTransform>(header, expression, filter_column_name, remove_filter_column,
+                on_totals, nullptr, condition_hash);
+
         return std::make_shared<FilterTransform>(header, expression, filter_column_name, remove_filter_column, on_totals);
     });
 
@@ -248,6 +252,10 @@ void FilterStep::updateOutputHeader()
         return;
 }
 
+void FilterStep::setQueryConditionKey(size_t condition_hash_)
+{
+    condition_hash = condition_hash_;
+}
 
 bool FilterStep::canUseType(const DataTypePtr & filter_type)
 {
diff --git a/src/Processors/QueryPlan/FilterStep.h b/src/Processors/QueryPlan/FilterStep.h
index 06d796a407e7..65f35af80770 100644
--- a/src/Processors/QueryPlan/FilterStep.h
+++ b/src/Processors/QueryPlan/FilterStep.h
@@ -1,6 +1,7 @@
 #pragma once
 #include <Processors/QueryPlan/ITransformingStep.h>
 #include <Interpreters/ActionsDAG.h>
+#include <Interpreters/Cache/QueryConditionCache.h>
 
 namespace DB
 {
@@ -25,6 +26,7 @@ class FilterStep : public ITransformingStep
     ActionsDAG & getExpression() { return actions_dag; }
     const String & getFilterColumnName() const { return filter_column_name; }
     bool removesFilterColumn() const { return remove_filter_column; }
+    void setQueryConditionKey(size_t condition_hash_);
 
     static bool canUseType(const DataTypePtr & type);
 
@@ -38,6 +40,8 @@ class FilterStep : public ITransformingStep
     ActionsDAG actions_dag;
     String filter_column_name;
     bool remove_filter_column;
+
+    std::optional<size_t> condition_hash;
 };
 
 }
diff --git a/src/Processors/QueryPlan/Optimizations/Optimizations.h b/src/Processors/QueryPlan/Optimizations/Optimizations.h
index 4273143f57dc..71549c5c4c68 100644
--- a/src/Processors/QueryPlan/Optimizations/Optimizations.h
+++ b/src/Processors/QueryPlan/Optimizations/Optimizations.h
@@ -126,6 +126,7 @@ bool optimizeJoinLegacy(QueryPlan::Node & node, QueryPlan::Nodes &, const QueryP
 bool optimizeJoinLogical(QueryPlan::Node & node, QueryPlan::Nodes &, const QueryPlanOptimizationSettings &);
 bool convertLogicalJoinToPhysical(QueryPlan::Node & node, QueryPlan::Nodes &, const QueryPlanOptimizationSettings & optimization_settings);
 void optimizeDistinctInOrder(QueryPlan::Node & node, QueryPlan::Nodes &);
+void tryUpdateQueryConditionCache(const QueryPlanOptimizationSettings & optimization_settings, const Stack & stack);
 
 /// A separate tree traverse to apply sorting properties after *InOrder optimizations.
 void applyOrder(const QueryPlanOptimizationSettings & optimization_settings, QueryPlan::Node & root);
diff --git a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp
index b35cb5bf50fa..00feae4f797b 100644
--- a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp
+++ b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp
@@ -35,6 +35,8 @@ namespace Setting
     extern const SettingsString force_optimize_projection_name;
     extern const SettingsUInt64 max_limit_for_ann_queries;
     extern const SettingsUInt64 query_plan_max_optimizations_to_apply;
+    extern const SettingsBool use_query_condition_cache;
+    extern const SettingsBool allow_experimental_analyzer;
 }
 
 QueryPlanOptimizationSettings::QueryPlanOptimizationSettings(const Settings & from)
@@ -74,6 +76,7 @@ QueryPlanOptimizationSettings::QueryPlanOptimizationSettings(const Settings & fr
 
     /// These settings comes from EXPLAIN settings not query settings and outside of the scope of this class
     keep_logical_steps = false;
+    use_query_condition_cache = from[Setting::use_query_condition_cache] && from[Setting::allow_experimental_analyzer];
     is_explain = false;
 }
 
diff --git a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h
index 487adeb7fac2..2bc61ea6e7f9 100644
--- a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h
+++ b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h
@@ -69,6 +69,9 @@ struct QueryPlanOptimizationSettings
     size_t max_limit_for_ann_queries;
 
     bool keep_logical_steps;
+
+    /// If query condition cache is enabled, the query condition cache needs to be updated in the WHERE stage.
+    bool use_query_condition_cache = false;
     bool is_explain;
 };
 
diff --git a/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp b/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp
index 7d51203e71c6..5d3c28540ed3 100644
--- a/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp
+++ b/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp
@@ -123,6 +123,9 @@ void optimizeTreeSecondPass(const QueryPlanOptimizationSettings & optimization_s
     while (!stack.empty())
     {
         optimizePrimaryKeyConditionAndLimit(stack);
+
+        tryUpdateQueryConditionCache(optimization_settings, stack);
+
         /// NOTE: optimizePrewhere can modify the stack.
         /// Prewhere optimization relies on PK optimization (getConditionSelectivityEstimatorByPredicate)
         if (optimization_settings.optimize_prewhere)
diff --git a/src/Processors/QueryPlan/Optimizations/tryUpdateQueryConditionCache.cpp b/src/Processors/QueryPlan/Optimizations/tryUpdateQueryConditionCache.cpp
new file mode 100644
index 000000000000..74a6fb9924a9
--- /dev/null
+++ b/src/Processors/QueryPlan/Optimizations/tryUpdateQueryConditionCache.cpp
@@ -0,0 +1,39 @@
+#include <Processors/QueryPlan/Optimizations/Optimizations.h>
+#include <Processors/QueryPlan/FilterStep.h>
+#include <Processors/QueryPlan/ReadFromMergeTree.h>
+#include <Storages/VirtualColumnUtils.h>
+
+namespace DB::QueryPlanOptimizations
+{
+
+void tryUpdateQueryConditionCache(const QueryPlanOptimizationSettings & optimization_settings, const Stack & stack)
+{
+    if (!optimization_settings.use_query_condition_cache)
+        return;
+
+    const auto & frame = stack.back();
+
+    auto * read_from_merge_tree = dynamic_cast<ReadFromMergeTree *>(frame.node->step.get());
+    if (!read_from_merge_tree)
+        return;
+
+    const auto & query_info = read_from_merge_tree->getQueryInfo();
+    const auto & filter_actions_dag = query_info.filter_actions_dag;
+    if (!filter_actions_dag || query_info.isFinal())
+        return;
+
+    if (!VirtualColumnUtils::isDeterministic(filter_actions_dag->getOutputs().front())) /// TODO check if front() still works for >1 condition
+        return;
+
+    for (auto iter = stack.rbegin() + 1; iter != stack.rend(); ++iter)
+    {
+        if (auto * filter_step = typeid_cast<FilterStep *>(iter->node->step.get()))
+        {
+            size_t condition_hash = filter_actions_dag->getOutputs().front()->getHash();
+            filter_step->setQueryConditionKey(condition_hash);
+            return;
+        }
+    }
+}
+
+}
diff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp
index 7e6a749ba9a0..f2f2da32fcd9 100644
--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp
+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp
@@ -8,6 +8,7 @@
 #include <Interpreters/ExpressionActions.h>
 #include <Interpreters/InterpreterSelectQuery.h>
 #include <Interpreters/TreeRewriter.h>
+#include <Interpreters/Cache/QueryConditionCache.h>
 #include <Parsers/ASTFunction.h>
 #include <Parsers/ASTIdentifier.h>
 #include <Parsers/ASTSelectQuery.h>
@@ -180,6 +181,8 @@ namespace Setting
     extern const SettingsBool query_plan_merge_filters;
     extern const SettingsUInt64 merge_tree_min_read_task_size;
     extern const SettingsBool read_in_order_use_virtual_row;
+    extern const SettingsBool use_query_condition_cache;
+    extern const SettingsBool allow_experimental_analyzer;
     extern const SettingsBool merge_tree_use_deserialization_prefixes_cache;
     extern const SettingsBool merge_tree_use_prefixes_deserialization_thread_pool;
 }
@@ -212,6 +215,7 @@ static MergeTreeReaderSettings getMergeTreeReaderSettings(
             && (settings[Setting::max_streams_to_max_threads_ratio] > 1 || settings[Setting::max_streams_for_merge_tree_reading] > 1),
         .enable_multiple_prewhere_read_steps = settings[Setting::enable_multiple_prewhere_read_steps],
         .force_short_circuit_execution = settings[Setting::query_plan_merge_filters],
+        .use_query_condition_cache = settings[Setting::use_query_condition_cache] && settings[Setting::allow_experimental_analyzer],
         .use_deserialization_prefixes_cache = settings[Setting::merge_tree_use_deserialization_prefixes_cache],
         .use_prefixes_deserialization_thread_pool = settings[Setting::merge_tree_use_prefixes_deserialization_thread_pool],
     };
@@ -854,13 +858,15 @@ Pipe ReadFromMergeTree::spreadMarkRangesAmongStreams(RangesInDataParts && parts_
         = settings[Setting::merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability];
     std::bernoulli_distribution fault(read_split_ranges_into_intersecting_and_non_intersecting_injection_probability);
 
+    /// When query condition cache is enabled, split ranges into intersecting will cause incorrect results and intersecting needs to be avoided.
     if (read_type != ReadType::ParallelReplicas &&
         num_streams > 1 &&
         read_split_ranges_into_intersecting_and_non_intersecting_injection_probability > 0.0 &&
         fault(thread_local_rng) &&
         !isQueryWithFinal() &&
         data.merging_params.is_deleted_column.empty() &&
-        !prewhere_info)
+        !prewhere_info &&
+        !reader_settings.use_query_condition_cache)
     {
         NameSet column_names_set(column_names.begin(), column_names.end());
         Names in_order_column_names_to_read(column_names);
@@ -1794,6 +1800,8 @@ ReadFromMergeTree::AnalysisResultPtr ReadFromMergeTree::selectRangesToRead(
             result.index_stats,
             indexes->use_skip_indexes,
             find_exact_ranges);
+
+        MergeTreeDataSelectExecutor::filterPartsByQueryConditionCache(result.parts_with_ranges, query_info_, context_, log);
     }
 
     size_t sum_marks_pk = total_marks_pk;
@@ -2118,6 +2126,10 @@ void ReadFromMergeTree::initializePipeline(QueryPipelineBuilder & pipeline, cons
 
     auto query_id_holder = MergeTreeDataSelectExecutor::checkLimits(data, result, context);
 
+    /// When the query condition cache is enabled, the complete Mark needs to be read every time. When both where and prewhere are null, it is disabled.
+    if (reader_settings.use_query_condition_cache && !query_info.prewhere_info && !query_info.filter_actions_dag)
+        reader_settings.use_query_condition_cache = false;
+
     if (result.parts_with_ranges.empty())
     {
         pipeline.init(Pipe(std::make_shared<NullSource>(getOutputHeader())));
diff --git a/src/Processors/Transforms/FilterTransform.cpp b/src/Processors/Transforms/FilterTransform.cpp
index 756a42d40885..8786de55456c 100644
--- a/src/Processors/Transforms/FilterTransform.cpp
+++ b/src/Processors/Transforms/FilterTransform.cpp
@@ -1,11 +1,14 @@
 #include <Processors/Transforms/FilterTransform.h>
 
+#include <Interpreters/Context.h>
 #include <Interpreters/ExpressionActions.h>
+#include <Interpreters/Cache/QueryConditionCache.h>
 #include <Columns/ColumnsCommon.h>
 #include <Core/Field.h>
 #include <DataTypes/DataTypeNullable.h>
 #include <DataTypes/DataTypeLowCardinality.h>
 #include <Processors/Merges/Algorithms/ReplacingSortedAlgorithm.h>
+#include <Storages/MergeTree/MergeTreeData.h>
 
 namespace DB
 {
@@ -43,7 +46,8 @@ FilterTransform::FilterTransform(
     String filter_column_name_,
     bool remove_filter_column_,
     bool on_totals_,
-    std::shared_ptr<std::atomic<size_t>> rows_filtered_)
+    std::shared_ptr<std::atomic<size_t>> rows_filtered_,
+    std::optional<size_t> condition_hash_)
     : ISimpleTransform(
             header_,
             transformHeader(header_, expression_ ? &expression_->getActionsDAG() : nullptr, filter_column_name_, remove_filter_column_),
@@ -53,6 +57,7 @@ FilterTransform::FilterTransform(
     , remove_filter_column(remove_filter_column_)
     , on_totals(on_totals_)
     , rows_filtered(rows_filtered_)
+    , condition_hash(condition_hash_)
 {
     transformed_header = getInputPort().getHeader();
     if (expression)
@@ -62,6 +67,9 @@ FilterTransform::FilterTransform(
     auto & column = transformed_header.getByPosition(filter_column_position).column;
     if (column)
         constant_filter_description = ConstantFilterDescription(*column);
+
+    if (condition_hash.has_value())
+        query_condition_cache = Context::getGlobalContextInstance()->getQueryConditionCache();
 }
 
 IProcessor::Status FilterTransform::prepare()
@@ -108,6 +116,26 @@ void FilterTransform::doTransform(Chunk & chunk)
     auto columns = chunk.detachColumns();
     DataTypes types;
 
+    auto write_into_query_condition_cache = [&]()
+    {
+        if (!query_condition_cache)
+            return;
+
+        auto mark_info = chunk.getChunkInfos().get<MarkRangesInfo>();
+        if (!mark_info)
+            return;
+
+        const auto & data_part = mark_info->getDataPart();
+        auto storage_id = data_part->storage.getStorageID();
+        query_condition_cache->write(
+                storage_id.uuid,
+                data_part->name,
+                *condition_hash,
+                mark_info->getMarkRanges(),
+                data_part->index_granularity->getMarksCount(),
+                data_part->index_granularity->hasFinalMark());
+    };
+
     {
         Block block = getInputPort().getHeader().cloneWithColumns(columns);
         columns.clear();
@@ -137,7 +165,10 @@ void FilterTransform::doTransform(Chunk & chunk)
     constant_filter_description = ConstantFilterDescription(*filter_column);
 
     if (constant_filter_description.always_false)
+    {
+        write_into_query_condition_cache();
         return; /// Will finish at next prepare call
+    }
 
     if (constant_filter_description.always_true)
     {
@@ -184,8 +215,11 @@ void FilterTransform::doTransform(Chunk & chunk)
 
     /// If the current block is completely filtered out, let's move on to the next one.
     if (num_filtered_rows == 0)
+    {
+        write_into_query_condition_cache();
         /// SimpleTransform will skip it.
         return;
+    }
 
     /// If all the rows pass through the filter.
     if (num_filtered_rows == num_rows_before_filtration)
diff --git a/src/Processors/Transforms/FilterTransform.h b/src/Processors/Transforms/FilterTransform.h
index 72d8a367a109..8662af593ad2 100644
--- a/src/Processors/Transforms/FilterTransform.h
+++ b/src/Processors/Transforms/FilterTransform.h
@@ -9,6 +9,7 @@ class ExpressionActions;
 using ExpressionActionsPtr = std::shared_ptr<ExpressionActions>;
 
 class ActionsDAG;
+class QueryConditionCache;
 
 /** Implements WHERE, HAVING operations.
   * Takes an expression, which adds to the block one ColumnUInt8 column containing the filtering conditions.
@@ -20,7 +21,8 @@ class FilterTransform : public ISimpleTransform
 public:
     FilterTransform(
         const Block & header_, ExpressionActionsPtr expression_, String filter_column_name_,
-        bool remove_filter_column_, bool on_totals_ = false, std::shared_ptr<std::atomic<size_t>> rows_filtered_ = nullptr);
+        bool remove_filter_column_, bool on_totals_ = false, std::shared_ptr<std::atomic<size_t>> rows_filtered_ = nullptr,
+        std::optional<size_t> condition_hash_ = std::nullopt);
 
     static Block
     transformHeader(const Block & header, const ActionsDAG * expression, const String & filter_column_name, bool remove_filter_column);
@@ -44,6 +46,10 @@ class FilterTransform : public ISimpleTransform
 
     std::shared_ptr<std::atomic<size_t>> rows_filtered;
 
+    /// If `condition_hash` is not null, the query condition cache needs to be updated at runtime.
+    std::optional<size_t> condition_hash;
+    std::shared_ptr<QueryConditionCache> query_condition_cache;
+
     /// Header after expression, but before removing filter column.
     Block transformed_header;
 
diff --git a/src/Storages/MergeTree/IMergeTreeReader.h b/src/Storages/MergeTree/IMergeTreeReader.h
index ad40ae01197c..e2b59a0da861 100644
--- a/src/Storages/MergeTree/IMergeTreeReader.h
+++ b/src/Storages/MergeTree/IMergeTreeReader.h
@@ -66,6 +66,8 @@ class IMergeTreeReader : private boost::noncopyable
 
     virtual void prefetchBeginOfRange(Priority) {}
 
+    MergeTreeReaderSettings & getMergeTreeReaderSettings() { return settings; }
+
 protected:
     /// Returns true if requested column is a subcolumn with offsets of Array which is part of Nested column.
     bool isSubcolumnOffsetsOfNested(const String & name_in_storage, const String & subcolumn_name) const;
diff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp
index 0039ded87b11..e6afeedf8f2b 100644
--- a/src/Storages/MergeTree/MergeTreeData.cpp
+++ b/src/Storages/MergeTree/MergeTreeData.cpp
@@ -58,6 +58,7 @@
 #include <Interpreters/TransactionLog.h>
 #include <Interpreters/TreeRewriter.h>
 #include <Interpreters/inplaceBlockConversions.h>
+#include <Interpreters/Cache/QueryConditionCache.h>
 #include <Parsers/ASTExpressionList.h>
 #include <Parsers/ASTIndexDeclaration.h>
 #include <Parsers/ASTHelpers.h>
diff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp
index a0ecb06fc170..0419318c67b2 100644
--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp
@@ -82,6 +82,8 @@ namespace Setting
     extern const SettingsUInt64 parallel_replica_offset;
     extern const SettingsUInt64 parallel_replicas_count;
     extern const SettingsParallelReplicasMode parallel_replicas_mode;
+    extern const SettingsBool use_query_condition_cache;
+    extern const SettingsBool allow_experimental_analyzer;
     extern const SettingsBool parallel_replicas_local_plan;
     extern const SettingsBool parallel_replicas_index_analysis_only_on_coordinator;
 }
@@ -890,6 +892,103 @@ RangesInDataParts MergeTreeDataSelectExecutor::filterPartsByPrimaryKeyAndSkipInd
     return parts_with_ranges;
 }
 
+void MergeTreeDataSelectExecutor::filterPartsByQueryConditionCache(
+    RangesInDataParts & parts_with_ranges,
+    const SelectQueryInfo & select_query_info,
+    const ContextPtr & context,
+    LoggerPtr log)
+{
+    const auto & settings = context->getSettingsRef();
+    if (!settings[Setting::use_query_condition_cache] || !settings[Setting::allow_experimental_analyzer] ||
+        (!select_query_info.prewhere_info && !select_query_info.filter_actions_dag))
+        return;
+
+    QueryConditionCachePtr query_condition_cache = context->getQueryConditionCache();
+
+    struct Stat
+    {
+        size_t total_granules{0};
+        size_t granules_dropped{0};
+    };
+
+    auto drop_mark_ranges =[&](const ActionsDAG::Node * dag) -> Stat
+    {
+        size_t condition_hash = dag->getHash();
+        Stat stat;
+        for (auto it = parts_with_ranges.begin(); it != parts_with_ranges.end();)
+        {
+            auto & part_with_ranges = *it;
+            stat.total_granules += part_with_ranges.getMarksCount();
+
+            auto & data_part = part_with_ranges.data_part;
+            auto storage_id = data_part->storage.getStorageID();
+            auto matching_marks_opt = query_condition_cache->read(storage_id.uuid, data_part->name, condition_hash);
+            if (!matching_marks_opt)
+            {
+                ++it;
+                continue;
+            }
+
+            auto & matching_marks = *matching_marks_opt;
+            MarkRanges ranges;
+            for (auto & mark_range : part_with_ranges.ranges)
+            {
+                size_t begin = mark_range.begin;
+                for (size_t mark_it = begin; mark_it < mark_range.end; ++mark_it)
+                {
+                    if (!matching_marks[mark_it])
+                    {
+                        ++stat.granules_dropped;
+                        if (mark_it == begin)
+                            ++begin;
+                        else
+                        {
+                            ranges.emplace_back(begin, mark_it);
+                            begin = mark_it + 1;
+                        }
+                    }
+                }
+
+                if (begin != mark_range.begin && begin != mark_range.end)
+                    ranges.emplace_back(begin, mark_range.end);
+                else if (begin == mark_range.begin)
+                    ranges.emplace_back(begin, mark_range.end);
+            }
+
+            if (ranges.empty())
+                it = parts_with_ranges.erase(it);
+            else
+            {
+                part_with_ranges.ranges = ranges;
+                ++it;
+            }
+        }
+
+        return stat;
+    };
+
+    if (const auto & prewhere_info = select_query_info.prewhere_info)
+    {
+        for (const auto * dag : prewhere_info->prewhere_actions.getOutputs())
+        {
+            if (dag->result_name == prewhere_info->prewhere_column_name)
+            {
+                auto stat = drop_mark_ranges(dag);
+                LOG_DEBUG(log, "PREWHERE contition {} by query condition cache has dropped {}/{} granules.", prewhere_info->prewhere_column_name, stat.granules_dropped, stat.total_granules);
+                break;
+            }
+        }
+    }
+
+    if (const auto & filter_actions_dag = select_query_info.filter_actions_dag)
+    {
+        const auto * dag = filter_actions_dag->getOutputs().front();
+        auto stat = drop_mark_ranges(dag);
+        LOG_DEBUG(log, "WHERE condition {} by query condition cache has dropped {}/{} granules.", filter_actions_dag->getOutputs().front()->result_name, stat.granules_dropped, stat.total_granules);
+    }
+}
+
+
 std::shared_ptr<QueryIdHolder> MergeTreeDataSelectExecutor::checkLimits(
     const MergeTreeData & data,
     const ReadFromMergeTree::AnalysisResult & result,
diff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h
index 5f6d4ddbabcf..182bbc4a1d33 100644
--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h
+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h
@@ -201,6 +201,13 @@ class MergeTreeDataSelectExecutor
         bool use_skip_indexes,
         bool find_exact_ranges);
 
+    /// If WHERE or PREWHERE condition is deterministic, try to use query condition cache to filter parts, delete invalid mark ranges.
+    static void filterPartsByQueryConditionCache(
+        RangesInDataParts & parts_with_ranges,
+        const SelectQueryInfo & select_query_info,
+        const ContextPtr & context,
+        LoggerPtr log);
+
     /// Create expression for sampling.
     /// Also, calculate _sample_factor if needed.
     /// Also, update key condition with selected sampling range.
diff --git a/src/Storages/MergeTree/MergeTreeIOSettings.h b/src/Storages/MergeTree/MergeTreeIOSettings.h
index 16319234ee86..72d6d1bc3333 100644
--- a/src/Storages/MergeTree/MergeTreeIOSettings.h
+++ b/src/Storages/MergeTree/MergeTreeIOSettings.h
@@ -52,6 +52,8 @@ struct MergeTreeReaderSettings
     bool adjust_read_buffer_size = true;
     /// If true, it's allowed to read the whole part without reading marks.
     bool can_read_part_without_marks = false;
+    /// Prewhere condition filtered marks is written to the mark filter cache.
+    bool use_query_condition_cache = false;
     bool use_deserialization_prefixes_cache = false;
     bool use_prefixes_deserialization_thread_pool = false;
 };
diff --git a/src/Storages/MergeTree/MergeTreeRangeReader.cpp b/src/Storages/MergeTree/MergeTreeRangeReader.cpp
index 7b366ac81c15..79dc490db1dd 100644
--- a/src/Storages/MergeTree/MergeTreeRangeReader.cpp
+++ b/src/Storages/MergeTree/MergeTreeRangeReader.cpp
@@ -948,33 +948,40 @@ MergeTreeRangeReader::ReadResult MergeTreeRangeReader::startReadingChain(size_t
     /// end offsets to properly fill _part_offset column.
     UInt64 leading_begin_part_offset = 0;
     UInt64 leading_end_part_offset = 0;
-
+    std::optional<size_t> current_mark;
     if (!stream.isFinished())
     {
         leading_begin_part_offset = stream.currentPartOffset();
         leading_end_part_offset = stream.lastPartOffset();
+        current_mark = stream.current_mark;
     }
 
     /// Stream is lazy. result.num_added_rows is the number of rows added to block which is not equal to
     /// result.num_rows_read until call to stream.finalize(). Also result.num_added_rows may be less than
     /// result.num_rows_read if the last granule in range also the last in part (so we have to adjust last granule).
     {
+        bool use_query_condition_cache = merge_tree_reader->getMergeTreeReaderSettings().use_query_condition_cache;
         size_t space_left = max_rows;
         while (space_left && (!stream.isFinished() || !ranges.empty()))
         {
             if (stream.isFinished())
             {
                 result.addRows(stream.finalize(result.columns));
+                if (current_mark && *current_mark < stream.last_mark)
+                    result.addReadRange(MarkRange{*current_mark, stream.last_mark});
+
                 stream = Stream(ranges.front().begin, ranges.front().end, current_task_last_mark, merge_tree_reader);
                 result.addRange(ranges.front());
                 ranges.pop_front();
+                current_mark = stream.current_mark;
             }
 
             size_t current_space = space_left;
 
             /// If reader can't read part of granule, we have to increase number of reading rows
             ///  to read complete granules and exceed max_rows a bit.
-            if (!merge_tree_reader->canReadIncompleteGranules())
+            /// When using query condition cache, you need to ensure that the read Mark is complete.
+            if (use_query_condition_cache || !merge_tree_reader->canReadIncompleteGranules())
                 current_space = stream.ceilRowsToCompleteGranules(space_left);
 
             auto rows_to_read = std::min(current_space, stream.numPendingRowsInCurrentGranule());
@@ -987,6 +994,9 @@ MergeTreeRangeReader::ReadResult MergeTreeRangeReader::startReadingChain(size_t
     }
 
     result.addRows(stream.finalize(result.columns));
+    size_t last_mark = stream.isFinished() ? stream.last_mark : stream.current_mark;
+    if (current_mark && current_mark < last_mark)
+        result.addReadRange(MarkRange{*current_mark, last_mark});
 
     /// Last granule may be incomplete.
     if (!result.rows_per_granule.empty())
diff --git a/src/Storages/MergeTree/MergeTreeRangeReader.h b/src/Storages/MergeTree/MergeTreeRangeReader.h
index 0e0429954058..3c7a03a54c42 100644
--- a/src/Storages/MergeTree/MergeTreeRangeReader.h
+++ b/src/Storages/MergeTree/MergeTreeRangeReader.h
@@ -237,6 +237,9 @@ class MergeTreeRangeReader
         Columns columns;
         size_t num_rows = 0;
 
+        /// All read marks.
+        MarkRanges read_mark_ranges;
+
         /// The number of rows were added to block as a result of reading chain.
         size_t numReadRows() const { return num_read_rows; }
         /// The number of bytes read from disk.
@@ -264,6 +267,7 @@ class MergeTreeRangeReader
         void adjustLastGranule();
         void addRows(size_t rows) { num_read_rows += rows; }
         void addRange(const MarkRange & range) { started_ranges.push_back({rows_per_granule.size(), range}); }
+        void addReadRange(MarkRange mark_range) { read_mark_ranges.push_back(std::move(mark_range)); }
 
         /// Add current step filter to the result and then for each granule calculate the number of filtered rows at the end.
         /// Remove them and update filter.
diff --git a/src/Storages/MergeTree/MergeTreeReadTask.cpp b/src/Storages/MergeTree/MergeTreeReadTask.cpp
index 827fb8b2e3f5..f67ce7caa295 100644
--- a/src/Storages/MergeTree/MergeTreeReadTask.cpp
+++ b/src/Storages/MergeTree/MergeTreeReadTask.cpp
@@ -204,6 +204,7 @@ MergeTreeReadTask::BlockAndProgress MergeTreeReadTask::read()
 
     BlockAndProgress res = {
         .block = std::move(block),
+        .read_mark_ranges = read_result.read_mark_ranges,
         .row_count = read_result.num_rows,
         .num_read_rows = num_read_rows,
         .num_read_bytes = num_read_bytes };
@@ -211,4 +212,9 @@ MergeTreeReadTask::BlockAndProgress MergeTreeReadTask::read()
     return res;
 }
 
+void MergeTreeReadTask::addPreWhereUnmatchedMarks(MarkRanges & mark_ranges_)
+{
+    prewhere_unmatched_marks.insert(prewhere_unmatched_marks.end(), mark_ranges_.begin(), mark_ranges_.end());
+}
+
 }
diff --git a/src/Storages/MergeTree/MergeTreeReadTask.h b/src/Storages/MergeTree/MergeTreeReadTask.h
index 9a481c7ddce7..08b651b4db83 100644
--- a/src/Storages/MergeTree/MergeTreeReadTask.h
+++ b/src/Storages/MergeTree/MergeTreeReadTask.h
@@ -118,6 +118,7 @@ struct MergeTreeReadTask : private boost::noncopyable
     struct BlockAndProgress
     {
         Block block;
+        MarkRanges read_mark_ranges;
         size_t row_count = 0;
         size_t num_read_rows = 0;
         size_t num_read_bytes = 0;
@@ -139,6 +140,9 @@ struct MergeTreeReadTask : private boost::noncopyable
     const MergeTreeReadersChain & getReadersChain() const { return readers_chain; }
     const IMergeTreeReader & getMainReader() const { return *readers.main; }
 
+    void addPreWhereUnmatchedMarks(MarkRanges & mark_ranges_);
+    const MarkRanges & getPreWhereUnmatchedMarks() { return prewhere_unmatched_marks; }
+
     Readers releaseReaders() { return std::move(readers); }
 
     static Readers createReaders(const MergeTreeReadTaskInfoPtr & read_info, const Extras & extras, const MarkRanges & ranges);
@@ -160,6 +164,9 @@ struct MergeTreeReadTask : private boost::noncopyable
     /// Ranges to read from data_part
     MarkRanges mark_ranges;
 
+    /// There is no mark matching a row of data under the prewhere condition.
+    MarkRanges prewhere_unmatched_marks;
+
     BlockSizeParams block_size_params;
 
     /// Used to satistfy preferred_block_size_bytes limitation
diff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp
index c58265739925..6875484d09a4 100644
--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp
+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp
@@ -15,6 +15,7 @@
 #include <Processors/QueryPlan/SourceStepWithFilter.h>
 #include <Processors/Transforms/AggregatingTransform.h>
 #include <Storages/MergeTree/MergeTreeVirtualColumns.h>
+#include <Interpreters/Cache/QueryConditionCache.h>
 #include <city.h>
 
 namespace
@@ -163,7 +164,29 @@ ChunkAndProgress MergeTreeSelectProcessor::read()
         try
         {
             if (!task || algorithm->needNewTask(*task))
+            {
+                if (task && prewhere_info && reader_settings.use_query_condition_cache)
+                {
+                    for (const auto * dag : prewhere_info->prewhere_actions.getOutputs())
+                    {
+                        if (dag->result_name == prewhere_info->prewhere_column_name)
+                        {
+                            auto data_part = task->getInfo().data_part;
+                            auto storage_id = data_part->storage.getStorageID();
+                            auto query_condition_cache = Context::getGlobalContextInstance()->getQueryConditionCache();
+                            query_condition_cache->write(storage_id.uuid,
+                                data_part->name,
+                                dag->getHash(),
+                                task->getPreWhereUnmatchedMarks(),
+                                data_part->index_granularity->getMarksCount(),
+                                data_part->index_granularity->hasFinalMark());
+                            break;
+                        }
+                    }
+                }
+
                 task = algorithm->getNewTask(*pool, task.get());
+            }
 
             if (!task)
                 break;
@@ -192,8 +215,20 @@ ChunkAndProgress MergeTreeSelectProcessor::read()
             }
 
             auto chunk = Chunk(ordered_columns, res.row_count);
+            const auto & data_part = task->getInfo().data_part;
             if (add_part_level)
-                chunk.getChunkInfos().add(std::make_shared<MergeTreeReadInfo>(task->getInfo().data_part->info.level));
+                chunk.getChunkInfos().add(std::make_shared<MergeTreeReadInfo>(data_part->info.level));
+
+            if (reader_settings.use_query_condition_cache)
+            {
+                chunk.getChunkInfos().add(std::make_shared<MarkRangesInfo>(data_part, res.read_mark_ranges));
+                LOG_DEBUG(
+                    log,
+                    "Chunk mark ranges info, part_name: {}, num_read_rows: {}, ranges: {}",
+                    data_part->name,
+                    res.num_read_rows,
+                    toString(res.read_mark_ranges));
+            }
 
             return ChunkAndProgress{
                 .chunk = std::move(chunk),
@@ -201,6 +236,8 @@ ChunkAndProgress MergeTreeSelectProcessor::read()
                 .num_read_bytes = res.num_read_bytes,
                 .is_finished = false};
         }
+        if (reader_settings.use_query_condition_cache && prewhere_info)
+            task->addPreWhereUnmatchedMarks(res.read_mark_ranges);
 
         return {Chunk(), res.num_read_rows, res.num_read_bytes, false};
     }
diff --git a/src/Storages/VirtualColumnUtils.cpp b/src/Storages/VirtualColumnUtils.cpp
index dd03f200d81c..ce353204e1d3 100644
--- a/src/Storages/VirtualColumnUtils.cpp
+++ b/src/Storages/VirtualColumnUtils.cpp
@@ -358,6 +358,23 @@ static bool canEvaluateSubtree(const ActionsDAG::Node * node, const Block * allo
     return true;
 }
 
+bool isDeterministic(const ActionsDAG::Node * node)
+{
+    for (const auto * child : node->children)
+    {
+        if (!isDeterministic(child))
+            return false;
+    }
+
+    if (node->type != ActionsDAG::ActionType::FUNCTION)
+        return true;
+
+    if (!node->function_base->isDeterministic())
+        return false;
+
+    return true;
+}
+
 bool isDeterministicInScopeOfQuery(const ActionsDAG::Node * node)
 {
     for (const auto * child : node->children)
diff --git a/src/Storages/VirtualColumnUtils.h b/src/Storages/VirtualColumnUtils.h
index 0196b246a17b..67db9cf42acc 100644
--- a/src/Storages/VirtualColumnUtils.h
+++ b/src/Storages/VirtualColumnUtils.h
@@ -43,7 +43,10 @@ void filterBlockWithExpression(const ExpressionActionsPtr & actions, Block & blo
 /// Builds sets used by ActionsDAG inplace.
 void buildSetsForDAG(const ActionsDAG & dag, const ContextPtr & context);
 
-/// Recursively checks if all functions used in DAG are deterministic in scope of query.
+/// Checks if all functions used in DAG are deterministic.
+bool isDeterministic(const ActionsDAG::Node * node);
+
+/// Checks recursively if all functions used in DAG are deterministic in scope of query.
 bool isDeterministicInScopeOfQuery(const ActionsDAG::Node * node);
 
 /// Extract a part of predicate that can be evaluated using only columns from input_names.
