{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 81145,
  "instance_id": "ClickHouse__ClickHouse-81145",
  "issue_numbers": [
    "80727"
  ],
  "base_commit": "2d1a62e82f1f5a30dae65d7fe06f0325673ec356",
  "patch": "diff --git a/src/DataTypes/DataTypeEnum.cpp b/src/DataTypes/DataTypeEnum.cpp\nindex 40605286268a..c385596e230c 100644\n--- a/src/DataTypes/DataTypeEnum.cpp\n+++ b/src/DataTypes/DataTypeEnum.cpp\n@@ -8,6 +8,7 @@\n #include <Common/typeid_cast.h>\n #include <Common/assert_cast.h>\n #include <Common/UTF8Helpers.h>\n+#include <Columns/ColumnSparse.h>\n #include <Poco/UTF8Encoding.h>\n #include <Interpreters/Context.h>\n #include <Core/Settings.h>\n@@ -78,7 +79,23 @@ Field DataTypeEnum<Type>::getDefault() const\n template <typename Type>\n void DataTypeEnum<Type>::insertDefaultInto(IColumn & column) const\n {\n-    assert_cast<ColumnType &>(column).getData().push_back(this->getValues().front().second);\n+    const auto & default_value = this->getValues().front().second;\n+\n+    /// This code is actually bad, but unfortunately, `IDataType::insertDefaultInto`\n+    /// breaks the abstraction of the separation of data types, serializations, and columns.\n+    /// Since this method is overridden only for `DataTypeEnum` and this code\n+    /// has remained unchanged for years, so it should be okay.\n+    if (auto * sparse_column = typeid_cast<ColumnSparse *>(&column))\n+    {\n+        if (default_value == Type{})\n+            sparse_column->insertDefault();\n+        else\n+            sparse_column->insert(default_value);\n+    }\n+    else\n+    {\n+        assert_cast<ColumnType &>(column).getData().push_back(default_value);\n+    }\n }\n \n template <typename Type>\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03530_insert_sparse_enum.reference b/tests/queries/0_stateless/03530_insert_sparse_enum.reference\nnew file mode 100644\nindex 000000000000..c7c3f2cdfa6d\n--- /dev/null\n+++ b/tests/queries/0_stateless/03530_insert_sparse_enum.reference\n@@ -0,0 +1,2 @@\n+1\ta\n+2\ta\ndiff --git a/tests/queries/0_stateless/03530_insert_sparse_enum.sh b/tests/queries/0_stateless/03530_insert_sparse_enum.sh\nnew file mode 100755\nindex 000000000000..5c5f464c8a09\n--- /dev/null\n+++ b/tests/queries/0_stateless/03530_insert_sparse_enum.sh\n@@ -0,0 +1,24 @@\n+#!/usr/bin/env bash\n+\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CUR_DIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT -q \"\n+    DROP TABLE IF EXISTS test_sparse_enum;\n+\n+    CREATE TABLE test_sparse_enum\n+    (\n+        x UInt64,\n+        y Enum8('a' = 0, 'b' = 1, 'c' = 2),\n+    )\n+    ENGINE = MergeTree() ORDER BY x;\n+\"\n+\n+echo '{\"x\" : 1}' | $CLICKHOUSE_CURL -sS \"$CLICKHOUSE_URL&query=INSERT%20INTO%20test_sparse_enum%20FORMAT%20JSON\" --data-binary @-\n+echo '{\"x\" : 2}' | $CLICKHOUSE_CURL -sS \"$CLICKHOUSE_URL&query=INSERT%20INTO%20test_sparse_enum%20FORMAT%20JSON\" --data-binary @-\n+\n+$CLICKHOUSE_CLIENT -q \"\n+    SELECT * FROM test_sparse_enum ORDER BY x;\n+    DROP TABLE test_sparse_enum;\n+\"\n",
  "problem_statement": "`PipelineExecutor: Code: 49. DB::Exception: Too large size ... passed to allocator` when trying to insert JSON data over HTTP\n### Company or project name\n\nDash0\n\n### Describe what's wrong\n\nWe're using ClickHouse for our integration tests (via Golang TestContainers) in a similar setup to our development / production environment (but then single-node using `testkeeper`).\n\nWe are inserting data for the integration tests via HTTP POST, using `application/json` body; `INSERT INTO <database>.<table> FORMAT JSON`\n\nUp until version 24.9 this worked fine. But switching to 24.10 or any later version gives the following errors:\n\n```\n2025.05.23 08:41:26.905289 [ 68 ] {53956b01-55ef-4371-b612-d49762dc5fa3} <Error> executeQuery: Code: 49. DB::Exception: Too large size (18446462934113570430) passed to allocator. It indicates an error.: While executing JSONRowInputFormat. (LOGICAL_ERROR) (version 24.10.4.191 (official build)) (from 172.17.0.1:45326) (in query: INSERT INTO otel.otel_metrics_metadata3 FORMAT JSON ), Stack trace (when copying this message, always include the lines below):\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000cf827bb\n1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x0000000007ead88c\n2. DB::Exception::Exception<unsigned long&>(int, FormatStringHelperImpl<std::type_identity<unsigned long&>::type>, unsigned long&) @ 0x000000000849748b\n3. Allocator<false, false>::realloc(void*, unsigned long, unsigned long, unsigned long) @ 0x000000000cf546ad\n4. void DB::PODArrayBase<1ul, 4096ul, Allocator<false, false>, 63ul, 64ul>::reserveForNextSize<>() @ 0x0000000007ebe627\n5. DB::DataTypeEnum<_BitInt(8)>::insertDefaultInto(DB::IColumn&) const @ 0x0000000010713917\n6. DB::JSONEachRowRowInputFormat::readRow(std::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn>>>&, DB::RowReadExtension&) @ 0x0000000012afbf7a\n7. DB::IRowInputFormat::read() @ 0x0000000012a473cf\n8. DB::IInputFormat::generate() @ 0x00000000129e9356\n9. DB::ISource::tryGenerate() @ 0x00000000129c5a9b\n10. DB::ISource::work() @ 0x00000000129c57a7\n11. DB::ExecutionThreadContext::executeTask() @ 0x00000000129dff27\n12. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x00000000129d34d0\n13. DB::PipelineExecutor::execute(unsigned long, bool) @ 0x00000000129d27a4\n14. DB::CompletedPipelineExecutor::execute() @ 0x00000000129d1052\n15. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&, String const&, std::shared_ptr<DB::Context const> const&, std::optional<DB::FormatSettings> const&)>) @ 0x00000000117508e8\n16. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x00000000128c3dc3\n17. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x00000000128c7d25\n18. DB::HTTPServerConnection::run() @ 0x000000001296799d\n19. Poco::Net::TCPServerConnection::start() @ 0x000000001582cce7\n20. Poco::Net::TCPServerDispatcher::run() @ 0x000000001582d179\n21. Poco::PooledThread::run() @ 0x00000000157f9be1\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x00000000157f819d\n23. ? @ 0x00007fda7d6edac3\n24. ? @ 0x00007fda7d77ea04\n\n2025.05.23 08:41:26.905577 [ 68 ] {53956b01-55ef-4371-b612-d49762dc5fa3} <Error> DynamicQueryHandler: Code: 49. DB::Exception: Too large size (18446462934113570430) passed to allocator. It indicates an error.: While executing JSONRowInputFormat. (LOGICAL_ERROR), Stack trace (when copying this message, always include the lines below):\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000cf827bb\n1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x0000000007ead88c\n2. DB::Exception::Exception<unsigned long&>(int, FormatStringHelperImpl<std::type_identity<unsigned long&>::type>, unsigned long&) @ 0x000000000849748b\n3. Allocator<false, false>::realloc(void*, unsigned long, unsigned long, unsigned long) @ 0x000000000cf546ad\n4. void DB::PODArrayBase<1ul, 4096ul, Allocator<false, false>, 63ul, 64ul>::reserveForNextSize<>() @ 0x0000000007ebe627\n5. DB::DataTypeEnum<_BitInt(8)>::insertDefaultInto(DB::IColumn&) const @ 0x0000000010713917\n6. DB::JSONEachRowRowInputFormat::readRow(std::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn>>>&, DB::RowReadExtension&) @ 0x0000000012afbf7a\n7. DB::IRowInputFormat::read() @ 0x0000000012a473cf\n8. DB::IInputFormat::generate() @ 0x00000000129e9356\n9. DB::ISource::tryGenerate() @ 0x00000000129c5a9b\n10. DB::ISource::work() @ 0x00000000129c57a7\n11. DB::ExecutionThreadContext::executeTask() @ 0x00000000129dff27\n12. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x00000000129d34d0\n13. DB::PipelineExecutor::execute(unsigned long, bool) @ 0x00000000129d27a4\n14. DB::CompletedPipelineExecutor::execute() @ 0x00000000129d1052\n15. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&, String const&, std::shared_ptr<DB::Context const> const&, std::optional<DB::FormatSettings> const&)>) @ 0x00000000117508e8\n16. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x00000000128c3dc3\n17. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x00000000128c7d25\n18. DB::HTTPServerConnection::run() @ 0x000000001296799d\n19. Poco::Net::TCPServerConnection::start() @ 0x000000001582cce7\n20. Poco::Net::TCPServerDispatcher::run() @ 0x000000001582d179\n21. Poco::PooledThread::run() @ 0x00000000157f9be1\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x00000000157f819d\n23. ? @ 0x00007fda7d6edac3\n24. ? @ 0x00007fda7d77ea04\n (version 24.10.4.191 (official build))\n2025.05.23 08:41:26.905289 [ 68 ] {53956b01-55ef-4371-b612-d49762dc5fa3} <Error> executeQuery: Code: 49. DB::Exception: Too large size (18446462934113570430) passed to allocator. It indicates an error.: While executing JSONRowInputFormat. (LOGICAL_ERROR) (version 24.10.4.191 (official build)) (from 172.17.0.1:45326) (in query: INSERT INTO otel.otel_metrics_metadata3 FORMAT JSON ), Stack trace (when copying this message, always include the lines below):\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000cf827bb\n1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x0000000007ead88c\n2. DB::Exception::Exception<unsigned long&>(int, FormatStringHelperImpl<std::type_identity<unsigned long&>::type>, unsigned long&) @ 0x000000000849748b\n3. Allocator<false, false>::realloc(void*, unsigned long, unsigned long, unsigned long) @ 0x000000000cf546ad\n4. void DB::PODArrayBase<1ul, 4096ul, Allocator<false, false>, 63ul, 64ul>::reserveForNextSize<>() @ 0x0000000007ebe627\n5. DB::DataTypeEnum<_BitInt(8)>::insertDefaultInto(DB::IColumn&) const @ 0x0000000010713917\n6. DB::JSONEachRowRowInputFormat::readRow(std::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn>>>&, DB::RowReadExtension&) @ 0x0000000012afbf7a\n7. DB::IRowInputFormat::read() @ 0x0000000012a473cf\n8. DB::IInputFormat::generate() @ 0x00000000129e9356\n9. DB::ISource::tryGenerate() @ 0x00000000129c5a9b\n10. DB::ISource::work() @ 0x00000000129c57a7\n11. DB::ExecutionThreadContext::executeTask() @ 0x00000000129dff27\n12. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x00000000129d34d0\n13. DB::PipelineExecutor::execute(unsigned long, bool) @ 0x00000000129d27a4\n14. DB::CompletedPipelineExecutor::execute() @ 0x00000000129d1052\n15. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&, String const&, std::shared_ptr<DB::Context const> const&, std::optional<DB::FormatSettings> const&)>) @ 0x00000000117508e8\n16. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x00000000128c3dc3\n17. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x00000000128c7d25\n18. DB::HTTPServerConnection::run() @ 0x000000001296799d\n19. Poco::Net::TCPServerConnection::start() @ 0x000000001582cce7\n20. Poco::Net::TCPServerDispatcher::run() @ 0x000000001582d179\n21. Poco::PooledThread::run() @ 0x00000000157f9be1\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x00000000157f819d\n23. ? @ 0x00007fda7d6edac3\n24. ? @ 0x00007fda7d77ea04\n```\n\nNOTE: it seems if we run only (part of) a single test, so with less data getting inserted in parallel, the insert runs fine. Only if we try to insert more data do we consistently run into this.\n\n### Does it reproduce on the most recent release?\n\nYes\n\n### How to reproduce\n\n- ClickHouse >= 24.10\n- HTTP POST with JSON data\n\n\n### Expected behavior\n\nData getting inserted without issues.\n\n### Error message and/or stacktrace\n\nSee above\n\n### Additional context\n\n_No response_\n",
  "hints_text": "I'm happy to share table schemas + data etc privately, but cannot share this in public.\nHi! Thank you for the report. It would be great to have a reproducible example to be able to investigate it. You can try to obfuscate the table schema and the data so it doesn't contain any sensitive information. If it's too complex, you can send this information to me directly so I can reproduce it and investigate.\nHi,\nThanks for jumping on this so quickly.\nI've managed to create a very minimal reproducer and send this to you as zip-file via e-mail.\n\nLet me know if you can reproduce on your end, and whether you have further questions.\nThanks!\nThank you! I received it and managed to reproduce. I minimized the test case:\n```sql\nCREATE TABLE test\n(\n    x UInt64,\n    y Enum8('a' = 0, 'b' = 1, 'c' = 2),\n)\nENGINE = MergeTree() ORDER BY x;\n```\n\n```bash\necho '{\"x\" : 1}' | curl 'http://localhost:8123/?query=INSERT%20INTO%20test%20FORMAT%20JSON' --data-binary @-\necho '{\"x\" : 1}' | curl 'http://localhost:8123/?query=INSERT%20INTO%20test%20FORMAT%20JSON' --data-binary @-\n```\n\nIt leads to logical error:\n```\n2025.05.26 10:27:11.440429 [ 1134313 ] {20f153ba-e64e-4cd1-892b-fb183a1078a9} <Fatal> : Logical error: 'Bad cast from type DB::ColumnSparse to DB::ColumnVector<_BitInt(8)>'.\n2025.05.26 10:27:11.457240 [ 1134313 ] {20f153ba-e64e-4cd1-892b-fb183a1078a9} <Fatal> : Stack trace (when copying this message, always include the lines below):\n\n0. /home/avogar/ClickHouse/contrib/llvm-project/libcxx/include/__exception/exception.h:113: Poco::Exception::Exception(String const&, int) @ 0x00000000193eadb2\n1. /home/avogar/ClickHouse/src/Common/Exception.cpp:110: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000f33171b\n2. /home/avogar/ClickHouse/src/Common/Exception.h:119: DB::Exception::Exception(PreformattedMessage&&, int) @ 0x0000000008ee08ec\n3. /home/avogar/ClickHouse/src/Common/Exception.h:137: DB::Exception::Exception<String, String>(int, FormatStringHelperImpl<std::type_identity<String>::type, std::type_identity<String>::type>, String&&, String&&) @ 0x0000000008edfa0b\n4. /home/avogar/ClickHouse/src/Common/assert_cast.h:48: DB::ColumnVector<_BitInt(8)>& assert_cast<DB::ColumnVector<_BitInt(8)>&, DB::IColumn&>(DB::IColumn&) @ 0x00000000094f6a2e\n5. /home/avogar/ClickHouse/src/DataTypes/DataTypeEnum.cpp:81: DB::DataTypeEnum<_BitInt(8)>::insertDefaultInto(DB::IColumn&) const @ 0x00000000132f286f\n6. /home/avogar/ClickHouse/src/Processors/Formats/Impl/JSONEachRowRowInputFormat.cpp:238: DB::JSONEachRowRowInputFormat::readRow(std::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn>>>&, DB::RowReadExtension&) @ 0x000000001634bac3\n7. /home/avogar/ClickHouse/src/Processors/Formats/IRowInputFormat.cpp:144: DB::IRowInputFormat::read() @ 0x0000000016298b45\n8. /home/avogar/ClickHouse/src/Processors/Formats/IInputFormat.cpp:19: DB::IInputFormat::generate() @ 0x0000000016236176\n9. /home/avogar/ClickHouse/src/Processors/ISource.cpp:139: DB::ISource::tryGenerate() @ 0x0000000016211afa\n10. /home/avogar/ClickHouse/src/Processors/ISource.cpp:108: DB::ISource::work() @ 0x00000000162117e7\n11. /home/avogar/ClickHouse/src/Processors/Executors/ExecutionThreadContext.cpp:53: DB::ExecutionThreadContext::executeTask() @ 0x000000001622c71b\n12. /home/avogar/ClickHouse/src/Processors/Executors/PipelineExecutor.cpp:305: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x0000000016221552\n13. /home/avogar/ClickHouse/src/Processors/Executors/PipelineExecutor.cpp:275: DB::PipelineExecutor::executeSingleThread(unsigned long) @ 0x000000001622181e\n14. /home/avogar/ClickHouse/src/Processors/Executors/PipelineExecutor.cpp:500: DB::PipelineExecutor::executeImpl(unsigned long, bool) @ 0x0000000016220143\n15. /home/avogar/ClickHouse/src/Processors/Executors/PipelineExecutor.cpp:134: DB::PipelineExecutor::execute(unsigned long, bool) @ 0x000000001621f954\n16. /home/avogar/ClickHouse/src/Processors/Executors/CompletedPipelineExecutor.cpp:105: DB::CompletedPipelineExecutor::execute() @ 0x000000001621e9c4\n17. /home/avogar/ClickHouse/src/Interpreters/executeQuery.cpp:1988: DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::shared_ptr<DB::Context>, std::function<void (DB::QueryResultDetails const&)>, DB::QueryFlags, std::optional<DB::FormatSettings> const&, std::function<void (DB::IOutputFormat&, String const&, std::shared_ptr<DB::Context const> const&, std::optional<DB::FormatSettings> const&)>, std::function<void ()>) @ 0x000000001446292b\n18. /home/avogar/ClickHouse/src/Server/HTTPHandler.cpp:610: DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::optional<DB::CurrentThread::QueryScope>&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x00000000160d9e59\n19. /home/avogar/ClickHouse/src/Server/HTTPHandler.cpp:759: DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x00000000160dcba7\n20. /home/avogar/ClickHouse/src/Server/HTTP/HTTPServerConnection.cpp:71: DB::HTTPServerConnection::run() @ 0x0000000016177119\n21. /home/avogar/ClickHouse/base/poco/Net/src/TCPServerConnection.cpp:40: Poco::Net::TCPServerConnection::start() @ 0x000000001948c6e7\n22. /home/avogar/ClickHouse/base/poco/Net/src/TCPServerDispatcher.cpp:115: Poco::Net::TCPServerDispatcher::run() @ 0x000000001948cbbe\n23. /home/avogar/ClickHouse/base/poco/Foundation/src/ThreadPool.cpp:205: Poco::PooledThread::run() @ 0x0000000019437192\n24. /home/avogar/ClickHouse/base/poco/Foundation/src/Thread_POSIX.cpp:335: Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000019434dcf\n25. ? @ 0x00007d421e894ac3\n26. ? @ 0x00007d421e926850\n```\nAs a workaround to avoid this error you can set `ratio_of_defaults_for_sparse_serialization=1` in the MergeTree settings\nThanks! The work-around works for us.\n\nIf I understand correctly, this bug only appears when inserting JSON data, right? Using row-binary format from the Golang library shouldn't trigger it?\n(means only our tests are impacted, not our actual production usage)\nWith row-binary data it should be fine\nHi @Avogar ,\n\nThe work-around doesn't seem to completely work, or there are other (un)related issues. I'm also seeing the following issues:\n```\n2025.05.26 13:02:24.756016 [ 1269 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\n2025.05.26 13:02:24.756098 [ 1269 ] {} <Fatal> BaseDaemon: Signal description: Segmentation fault\n2025.05.26 13:02:24.756127 [ 1269 ] {} <Fatal> BaseDaemon: Stack trace: 0x000000000f691837 0x00007fbcb8929520 0x0000000014504276 0x000000001450424d 0x000000001450438f 0x0000000014504356 0x0000000014502eba 0x00000\n0001439e6cd 0x000000001439ca89 0x00000000143a8c0d 0x00000000143a8e42 0x00000000143a9068 0x00000000143a585e 0x0000000014ecbbbb 0x0000000014eca742 0x0000000014f6b65d 0x0000000014f69edf 0x0000000014e9187d 0x00000000\n1325ae36 0x000000001325a75e 0x00000000135b5168 0x00000000135ae866 0x000000001490f99e 0x000000001492e699 0x0000000018036847 0x0000000018036c99 0x0000000018001fbb 0x000000001800049d 0x00007fbcb897bac3 0x00007fbcb8a\n0ca04\n2025.05.26 13:02:24.756140 [ 1269 ] {} <Fatal> BaseDaemon: ########################################\n2025.05.26 13:02:24.756112 [ 1269 ] {} <Fatal> BaseDaemon: Address: 0x180. Access: read. Address not mapped to object.\n2025.05.26 13:02:24.756235 [ 1269 ] {} <Fatal> BaseDaemon: Stack trace: 0x000000000f691837 0x00007fbcb8929520 0x0000000014504276 0x000000001450424d 0x000000001450438f 0x0000000014504356 0x0000000014502eba 0x00000\n0001439e6cd 0x000000001439ca89 0x00000000143a8c0d 0x00000000143a8e42 0x00000000143a9068 0x00000000143a585e 0x0000000014ecbbbb 0x0000000014eca742 0x0000000014f6b65d 0x0000000014f69edf 0x0000000014e9187d 0x00000000\n1325ae36 0x000000001325a75e 0x00000000135b5168 0x00000000135ae866 0x000000001490f99e 0x000000001492e699 0x0000000018036847 0x0000000018036c99 0x0000000018001fbb 0x000000001800049d 0x00007fbcb897bac3 0x00007fbcb8a\n0ca04\n2025.05.26 13:02:24.756294 [ 1269 ] {} <Fatal> BaseDaemon: 0. signalHandler(int, siginfo_t*, void*) @ 0x000000000f691837\n2025.05.26 13:02:24.756235 [ 1269 ] {} <Fatal> BaseDaemon: Stack trace: 0x000000000f691837 0x00007fbcb8929520 0x0000000014504276 0x000000001450424d 0x000000001450438f 0x0000000014504356 0x0000000014502eba 0x00000\n0001439e6cd 0x000000001439ca89 0x00000000143a8c0d 0x00000000143a8e42 0x00000000143a9068 0x00000000143a585e 0x0000000014ecbbbb 0x0000000014eca742 0x0000000014f6b65d 0x0000000014f69edf 0x0000000014e9187d 0x000000001325ae36 0x000000001325a75e 0x00000000135b5168 0x00000000135ae866 0x000000001490f99e 0x000000001492e699 0x0000000018036847 0x0000000018036c99 0x0000000018001fbb 0x000000001800049d 0x00007fbcb897bac3 0x00007fbcb8a0ca04                                                                                                                                                                                                               2025.05.26 13:02:24.756294 [ 1269 ] {} <Fatal> BaseDaemon: 0. signalHandler(int, siginfo_t*, void*) @ 0x000000000f691837\n2025.05.26 13:02:24.756352 [ 1269 ] {} <Fatal> BaseDaemon: 1. ? @ 0x00007fbcb8929520\n2025.05.26 13:02:24.756398 [ 1269 ] {} <Fatal> BaseDaemon: 2. DB::(anonymous namespace)::appendColumnNameWithoutAlias(DB::ActionsDAG::Node const&, DB::WriteBuffer&, bool, bool) (.llvm.8103339512881675178) @ 0x000\n0000014504276                                                                                                                                                                                                       2025.05.26 13:02:24.756443 [ 1269 ] {} <Fatal> BaseDaemon: 3. DB::(anonymous namespace)::appendColumnNameWithoutAlias(DB::ActionsDAG::Node const&, DB::WriteBuffer&, bool, bool) (.llvm.8103339512881675178) @ 0x000\n000001450424d                                                                                                                                                                                                       2025.05.26 13:02:24.756489 [ 1269 ] {} <Fatal> BaseDaemon: 4. DB::(anonymous namespace)::appendColumnNameWithoutAlias(DB::ActionsDAG::Node const&, DB::WriteBuffer&, bool, bool) (.llvm.8103339512881675178) @ 0x000\n000001450438f                                                                                                                                                                                                       2025.05.26 13:02:24.756555 [ 1269 ] {} <Fatal> BaseDaemon: 5. DB::(anonymous namespace)::appendColumnNameWithoutAlias(DB::ActionsDAG::Node const&, DB::WriteBuffer&, bool, bool) (.llvm.8103339512881675178) @ 0x000\n0000014504356                                                                                                                                                                                                       2025.05.26 13:02:24.756625 [ 1269 ] {} <Fatal> BaseDaemon: 6. DB::(anonymous namespace)::getColumnNameWithoutAlias(DB::ActionsDAG::Node const&, bool, bool) (.llvm.8103339512881675178) @ 0x0000000014502eba\n2025.05.26 13:02:24.756690 [ 1269 ] {} <Fatal> BaseDaemon: 7. DB::MergeTreeIndexConditionBloomFilter::traverseTreeEquals(String const&, DB::RPNBuilderTreeNode const&, std::shared_ptr<DB::IDataType const> const&, \nDB::Field const&, DB::MergeTreeIndexConditionBloomFilter::RPNElement&, DB::RPNBuilderTreeNode const*) @ 0x000000001439e6cd                                                                                          2025.05.26 13:02:24.756845 [ 1269 ] {} <Fatal> BaseDaemon: 8. DB::MergeTreeIndexConditionBloomFilter::traverseFunction(DB::RPNBuilderTreeNode const&, DB::MergeTreeIndexConditionBloomFilter::RPNElement&, DB::RPNBu\nilderTreeNode const*) @ 0x000000001439ca89                                                                                                                                                                          2025.05.26 13:02:24.757212 [ 1269 ] {} <Fatal> BaseDaemon: 9. bool std::__function::__policy_invoker<bool (DB::RPNBuilderTreeNode const&, DB::MergeTreeIndexConditionBloomFilter::RPNElement&)>::__call_impl[abi:ne1\n90107]<std::__function::__default_alloc_func<DB::MergeTreeIndexConditionBloomFilter::MergeTreeIndexConditionBloomFilter(DB::ActionsDAG const*, std::shared_ptr<DB::Context const>, DB::Block const&, unsigned long)::$_0, bool (DB::RPNBuilderTreeNode const&, DB::MergeTreeIndexConditionBloomFilter::RPNElement&)>>(std::__function::__policy_storage const*, DB::RPNBuilderTreeNode const&, DB::MergeTreeIndexConditionBloomFilter::RPNElement&) @ 0x00000000143a8c0d                                                                                                                                                                                    2025.05.26 13:02:24.757306 [ 1269 ] {} <Fatal> BaseDaemon: 10. DB::RPNBuilder<DB::MergeTreeIndexConditionBloomFilter::RPNElement>::traverseTree(DB::RPNBuilderTreeNode const&) @ 0x00000000143a8e42\n2025.05.26 13:02:24.757371 [ 1269 ] {} <Fatal> BaseDaemon: 11. DB::RPNBuilder<DB::MergeTreeIndexConditionBloomFilter::RPNElement>::traverseTree(DB::RPNBuilderTreeNode const&) @ 0x00000000143a9068\n2025.05.26 13:02:24.757212 [ 1269 ] {} <Fatal> BaseDaemon: 9. bool std::__function::__policy_invoker<bool (DB::RPNBuilderTreeNode const&, DB::MergeTreeIndexConditionBloomFilter::RPNElement&)>::__call_impl[abi:ne1\n90107]<std::__function::__default_alloc_func<DB::MergeTreeIndexConditionBloomFilter::MergeTreeIndexConditionBloomFilter(DB::ActionsDAG const*, std::shared_ptr<DB::Context const>, DB::Block const&, unsigned long)::$_0, bool (DB::RPNBuilderTreeNode const&, DB::MergeTreeIndexConditionBloomFilter::RPNElement&)>>(std::__function::__policy_storage const*, DB::RPNBuilderTreeNode const&, DB::MergeTreeIndexConditionBloomFilter::RPNElement&) @ 0x00000000143a8c0d                                                                                                                                                                                    2025.05.26 13:02:24.757306 [ 1269 ] {} <Fatal> BaseDaemon: 10. DB::RPNBuilder<DB::MergeTreeIndexConditionBloomFilter::RPNElement>::traverseTree(DB::RPNBuilderTreeNode const&) @ 0x00000000143a8e42\n2025.05.26 13:02:24.757371 [ 1269 ] {} <Fatal> BaseDaemon: 11. DB::RPNBuilder<DB::MergeTreeIndexConditionBloomFilter::RPNElement>::traverseTree(DB::RPNBuilderTreeNode const&) @ 0x00000000143a9068\n2025.05.26 13:02:24.757414 [ 1269 ] {} <Fatal> BaseDaemon: 12. DB::MergeTreeIndexBloomFilter::createIndexCondition(DB::ActionsDAG const*, std::shared_ptr<DB::Context const>) const @ 0x00000000143a585e\n2025.05.26 13:02:24.757470 [ 1269 ] {} <Fatal> BaseDaemon: 13. DB::buildIndexes(std::optional<DB::ReadFromMergeTree::Indexes>&, DB::ActionsDAG const*, DB::MergeTreeData const&, std::vector<std::shared_ptr<DB::IMe\nrgeTreeDataPart const>, std::allocator<std::shared_ptr<DB::IMergeTreeDataPart const>>> const&, std::shared_ptr<DB::MergeTreeData::IMutationsSnapshot const> const&, std::optional<DB::VectorSearchParameters> const&, std::shared_ptr<DB::Context const> const&, DB::SelectQueryInfo const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<Poco::Logger> const&) @ 0x0000000014ecbbbb\n2025.05.26 13:02:24.757521 [ 1269 ] {} <Fatal> BaseDaemon: 14. DB::ReadFromMergeTree::applyFilters(DB::ActionDAGNodes) @ 0x0000000014eca742\n2025.05.26 13:02:24.757564 [ 1269 ] {} <Fatal> BaseDaemon: 15. DB::QueryPlanOptimizations::optimizePrimaryKeyConditionAndLimit(std::vector<DB::QueryPlanOptimizations::Frame, std::allocator<DB::QueryPlanOptimizati\nons::Frame>> const&) @ 0x0000000014f6b65d\n2025.05.26 13:02:24.757521 [ 1269 ] {} <Fatal> BaseDaemon: 14. DB::ReadFromMergeTree::applyFilters(DB::ActionDAGNodes) @ 0x0000000014eca742\n2025.05.26 13:02:24.757564 [ 1269 ] {} <Fatal> BaseDaemon: 15. DB::QueryPlanOptimizations::optimizePrimaryKeyConditionAndLimit(std::vector<DB::QueryPlanOptimizations::Frame, std::allocator<DB::QueryPlanOptimizati\nons::Frame>> const&) @ 0x0000000014f6b65d\n2025.05.26 13:02:24.757606 [ 1269 ] {} <Fatal> BaseDaemon: 16. DB::QueryPlanOptimizations::optimizeTreeSecondPass(DB::QueryPlanOptimizationSettings const&, DB::QueryPlan::Node&, std::list<DB::QueryPlan::Node, std\n::allocator<DB::QueryPlan::Node>>&) @ 0x0000000014f69edf\n2025.05.26 13:02:24.757647 [ 1269 ] {} <Fatal> BaseDaemon: 17. DB::QueryPlan::buildQueryPipeline(DB::QueryPlanOptimizationSettings const&, DB::BuildQueryPipelineSettings const&, bool) @ 0x0000000014e9187d\n2025.05.26 13:02:24.757727 [ 1269 ] {} <Fatal> BaseDaemon: 18. DB::InterpreterSelectQueryAnalyzer::buildQueryPipeline() @ 0x000000001325ae36\n2025.05.26 13:02:24.757774 [ 1269 ] {} <Fatal> BaseDaemon: 19. DB::InterpreterSelectQueryAnalyzer::execute() @ 0x000000001325a75e\n2025.05.26 13:02:24.757845 [ 1269 ] {} <Fatal> BaseDaemon: 20. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*, std::sh\nared_ptr<DB::IAST>&) @ 0x00000000135b5168\n2025.05.26 13:02:24.757917 [ 1269 ] {} <Fatal> BaseDaemon: 21. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum) @ 0x00000000135ae866\n2025.05.26 13:02:24.757952 [ 1269 ] {} <Fatal> BaseDaemon: 22. DB::TCPHandler::runImpl() @ 0x000000001490f99e\n2025.05.26 13:02:24.757990 [ 1269 ] {} <Fatal> BaseDaemon: 23. DB::TCPHandler::run() @ 0x000000001492e699\n2025.05.26 13:02:24.758029 [ 1269 ] {} <Fatal> BaseDaemon: 24. Poco::Net::TCPServerConnection::start() @ 0x0000000018036847\n2025.05.26 13:02:24.758095 [ 1269 ] {} <Fatal> BaseDaemon: 25. Poco::Net::TCPServerDispatcher::run() @ 0x0000000018036c99\n2025.05.26 13:02:24.758155 [ 1269 ] {} <Fatal> BaseDaemon: 26. Poco::PooledThread::run() @ 0x0000000018001fbb\n2025.05.26 13:02:24.758200 [ 1269 ] {} <Fatal> BaseDaemon: 27. Poco::ThreadImpl::runnableEntry(void*) @ 0x000000001800049d\n2025.05.26 13:02:24.758268 [ 1269 ] {} <Fatal> BaseDaemon: 28. ? @ 0x00007fbcb897bac3\n2025.05.26 13:02:24.758331 [ 1269 ] {} <Fatal> BaseDaemon: 29. ? @ 0x00007fbcb8a0ca04\n2025.05.26 13:02:24.928515 [ 1269 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: 4A59195AB07EE0822F9A8ACBAEE74072)\n2025.05.26 13:02:24.928793 [ 1269 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues\n2025.05.26 13:02:24.929016 [ 1269 ] {} <Fatal> BaseDaemon: Changed settings: max_insert_threads = 4, max_threads = 4, alter_sync = 1, default_table_engine = 'ReplicatedMergeTree', mutations_sync = 0, database_rep\nlicated_allow_only_replicated_engine = true, parallel_replicas_for_cluster_engines = false, allow_experimental_database_replicated = true\n2025.05.26 13:02:24.928793 [ 1269 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues\n```\n\nHappened for the query (seemingly):\n```\nSELECT groupUniqArray(PromLabelValueForKey) FROM (    WITH     arrayJoin(if(empty(HistogramExplicitBounds), [map()], arrayConcat(arrayMap(b -> map('le', toString(b))\n, HistogramExplicitBounds), [map('le', '+Inf')]))) AS HistogramExplicitBound,     arrayJoin(if(empty(SummaryQuantiles), [map()], arrayMap(b -> map('quantile', toString(b)), SummaryQuantiles))) as Quantile,     mapConcat(PrometheusMetricLabels, HistogramExplicitBound, Quantile) AS PrometheusAttributes    SELECT     PrometheusAttributes['le'] AS PromLabelValueForKey    FROM otel_metrics_metadata3)\n```\n\nStacktrace is a bit mangled due to multiple queries happening at the same time, and also logging in between from the tests itself. So hopefully you can make sense of it :-/\nBut since its hitting the same table thought there might be some relation.\nAlso happy to create a separate ticket if you think that makes sense.\n\nIt seems to be different issue. Please, create a separate issue. Also it would be great to try to create a reproducible example (you can also send it to me via email so I can try to minimize it and investigate).\nSeems this issue in indexes was fixed in https://github.com/ClickHouse/ClickHouse/pull/78485. It's already backported, but we need to make patch releases.",
  "created_at": "2025-06-02T11:39:26Z",
  "modified_files": [
    "src/DataTypes/DataTypeEnum.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/03530_insert_sparse_enum.reference",
    "b/tests/queries/0_stateless/03530_insert_sparse_enum.sh"
  ]
}