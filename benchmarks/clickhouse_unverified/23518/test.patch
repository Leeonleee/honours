diff --git a/tests/integration/helpers/cluster.py b/tests/integration/helpers/cluster.py
index ee9cdd3550a2..4ddab2a2e7a0 100644
--- a/tests/integration/helpers/cluster.py
+++ b/tests/integration/helpers/cluster.py
@@ -2196,6 +2196,7 @@ def create_dir(self, destroy_dir=True):
             odbc_bridge_volume = "- " + self.odbc_bridge_bin_path + ":/usr/share/clickhouse-odbc-bridge_fresh"
             library_bridge_volume = "- " + self.library_bridge_bin_path + ":/usr/share/clickhouse-library-bridge_fresh"
 
+
         with open(self.docker_compose_path, 'w') as docker_compose:
             docker_compose.write(DOCKER_COMPOSE_TEMPLATE.format(
                 image=self.image,
diff --git a/tests/integration/test_cluster_copier/configs/conf.d/clusters_trivial.xml b/tests/integration/test_cluster_copier/configs/conf.d/clusters_trivial.xml
new file mode 100644
index 000000000000..dea9c119f2bf
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs/conf.d/clusters_trivial.xml
@@ -0,0 +1,21 @@
+<?xml version="1.0"?>
+<yandex>
+    <remote_servers>
+        <source_trivial_cluster>
+            <shard>
+                <replica>
+                    <host>first_trivial</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+        </source_trivial_cluster>
+        <destination_trivial_cluster>
+            <shard>
+                <replica>
+                    <host>second_trivial</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+        </destination_trivial_cluster>
+    </remote_servers>
+</yandex>
diff --git a/tests/integration/test_cluster_copier/configs/config-copier.xml b/tests/integration/test_cluster_copier/configs/config-copier.xml
index 12640034104b..6db67efed6f1 100644
--- a/tests/integration/test_cluster_copier/configs/config-copier.xml
+++ b/tests/integration/test_cluster_copier/configs/config-copier.xml
@@ -1,6 +1,6 @@
 <yandex>
     <logger>
-        <level>trace</level>
+        <level>information</level>
         <log>/var/log/clickhouse-server/copier/log.log</log>
         <errorlog>/var/log/clickhouse-server/copier/log.err.log</errorlog>
         <size>1000M</size>
diff --git a/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/clusters.xml b/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/clusters.xml
new file mode 100644
index 000000000000..6993a7ad7fd8
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/clusters.xml
@@ -0,0 +1,28 @@
+<?xml version="1.0"?>
+<yandex>
+    <remote_servers>
+        <events>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>first</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>second</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>third</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+         </events>
+    </remote_servers>
+</yandex>
diff --git a/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/ddl.xml b/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/ddl.xml
new file mode 100644
index 000000000000..4bff11fb6936
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/ddl.xml
@@ -0,0 +1,6 @@
+<?xml version="1.0"?>
+<yandex>
+    <distributed_ddl>
+        <path>/clickhouse/task_queue/ddl</path>
+    </distributed_ddl>
+</yandex>
\ No newline at end of file
diff --git a/tests/integration/test_cluster_copier/configs_three_nodes/config-copier.xml b/tests/integration/test_cluster_copier/configs_three_nodes/config-copier.xml
new file mode 100644
index 000000000000..ede3dcb1228b
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs_three_nodes/config-copier.xml
@@ -0,0 +1,28 @@
+<?xml version="1.0"?>
+<yandex>
+    <logger>
+        <level>information</level>
+        <log>/var/log/clickhouse-server/copier/log.log</log>
+        <errorlog>/var/log/clickhouse-server/copier/log.err.log</errorlog>
+        <size>1000M</size>
+        <count>10</count>
+        <stderr>/var/log/clickhouse-server/copier/stderr.log</stderr>
+        <stdout>/var/log/clickhouse-server/copier/stdout.log</stdout>
+    </logger>
+
+    <zookeeper>
+        <node index="1">
+            <host>zoo1</host>
+            <port>2181</port>
+        </node>
+        <node index="2">
+            <host>zoo2</host>
+            <port>2181</port>
+        </node>
+            <node index="3">
+            <host>zoo3</host>
+            <port>2181</port>
+        </node>
+        <session_timeout_ms>2000</session_timeout_ms>
+    </zookeeper>
+</yandex>
diff --git a/tests/integration/test_cluster_copier/configs_three_nodes/users.xml b/tests/integration/test_cluster_copier/configs_three_nodes/users.xml
new file mode 100644
index 000000000000..023598304f20
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs_three_nodes/users.xml
@@ -0,0 +1,32 @@
+<?xml version="1.0"?>
+<yandex>
+    <profiles>
+        <default>
+            <log_queries>1</log_queries>
+        </default>
+    </profiles>
+
+    <users>
+        <default>
+            <password></password>
+            <networks incl="networks" replace="replace">
+                <ip>::/0</ip>
+            </networks>
+            <profile>default</profile>
+            <quota>default</quota>
+        </default>
+        <dbuser>
+            <password>12345678</password>
+            <networks incl="networks" replace="replace">
+                <ip>::/0</ip>
+            </networks>
+            <profile>default</profile>
+            <quota>default</quota>
+        </dbuser>
+    </users>
+
+    <quotas>
+        <default>
+        </default>
+    </quotas>
+</yandex>
diff --git a/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/clusters.xml b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/clusters.xml
new file mode 100644
index 000000000000..fd1321d5218d
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/clusters.xml
@@ -0,0 +1,23 @@
+<?xml version="1.0"?>
+<yandex>
+    <remote_servers>
+        <source>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>first_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </source>
+        <destination>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>second_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </destination>
+    </remote_servers>
+</yandex>
diff --git a/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/ddl.xml b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/ddl.xml
new file mode 100644
index 000000000000..4bff11fb6936
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/ddl.xml
@@ -0,0 +1,6 @@
+<?xml version="1.0"?>
+<yandex>
+    <distributed_ddl>
+        <path>/clickhouse/task_queue/ddl</path>
+    </distributed_ddl>
+</yandex>
\ No newline at end of file
diff --git a/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/storage_configuration.xml b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/storage_configuration.xml
new file mode 100644
index 000000000000..07b5c5772675
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/storage_configuration.xml
@@ -0,0 +1,34 @@
+<yandex>
+
+<storage_configuration>
+    <disks>
+        <default>
+        </default>
+        <jbod1>
+            <path>/jbod1/</path>
+        </jbod1>
+        <jbod2>
+            <path>/jbod2/</path>
+        </jbod2>
+        <external>
+            <path>/external/</path>
+        </external>
+    </disks>
+
+    <policies>
+        <external_with_jbods>
+            <volumes>
+                <external>
+                    <disk>external</disk>
+                </external>
+                <main>
+                    <disk>jbod1</disk>
+                    <disk>jbod2</disk>
+                </main>
+            </volumes>
+        </external_with_jbods>
+    </policies>
+
+</storage_configuration>
+
+</yandex>
diff --git a/tests/integration/test_cluster_copier/configs_two_nodes/config-copier.xml b/tests/integration/test_cluster_copier/configs_two_nodes/config-copier.xml
new file mode 100644
index 000000000000..642998c6d878
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs_two_nodes/config-copier.xml
@@ -0,0 +1,20 @@
+<?xml version="1.0"?>
+<yandex>
+    <logger>
+        <level>information</level>
+        <log>/var/log/clickhouse-server/copier/log.log</log>
+        <errorlog>/var/log/clickhouse-server/copier/log.err.log</errorlog>
+        <size>1000M</size>
+        <count>10</count>
+        <stderr>/var/log/clickhouse-server/copier/stderr.log</stderr>
+        <stdout>/var/log/clickhouse-server/copier/stdout.log</stdout>
+    </logger>
+
+    <zookeeper>
+        <node index="1">
+            <host>zoo1</host>
+            <port>2181</port>
+        </node>
+        <session_timeout_ms>2000</session_timeout_ms>
+    </zookeeper>
+</yandex>
diff --git a/tests/integration/test_cluster_copier/configs_two_nodes/users.xml b/tests/integration/test_cluster_copier/configs_two_nodes/users.xml
new file mode 100644
index 000000000000..023598304f20
--- /dev/null
+++ b/tests/integration/test_cluster_copier/configs_two_nodes/users.xml
@@ -0,0 +1,32 @@
+<?xml version="1.0"?>
+<yandex>
+    <profiles>
+        <default>
+            <log_queries>1</log_queries>
+        </default>
+    </profiles>
+
+    <users>
+        <default>
+            <password></password>
+            <networks incl="networks" replace="replace">
+                <ip>::/0</ip>
+            </networks>
+            <profile>default</profile>
+            <quota>default</quota>
+        </default>
+        <dbuser>
+            <password>12345678</password>
+            <networks incl="networks" replace="replace">
+                <ip>::/0</ip>
+            </networks>
+            <profile>default</profile>
+            <quota>default</quota>
+        </dbuser>
+    </users>
+
+    <quotas>
+        <default>
+        </default>
+    </quotas>
+</yandex>
diff --git a/tests/integration/test_cluster_copier/task_drop_target_partition.xml b/tests/integration/test_cluster_copier/task_drop_target_partition.xml
new file mode 100644
index 000000000000..6b2ede19d25b
--- /dev/null
+++ b/tests/integration/test_cluster_copier/task_drop_target_partition.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0"?>
+<yandex>
+<remote_servers>
+        <source>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>first_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </source>
+        <destination>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>second_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </destination>
+    </remote_servers>
+
+   <max_workers>2</max_workers>
+
+   <tables>
+     <table_events>
+         <cluster_pull>source</cluster_pull>
+         <database_pull>db_drop_target_partition</database_pull>
+         <table_pull>source</table_pull>
+
+         <cluster_push>destination</cluster_push>
+         <database_push>db_drop_target_partition</database_push>
+         <table_push>destination</table_push>
+
+         <allow_to_drop_target_partitions>true</allow_to_drop_target_partitions>
+
+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column3, Column2, Column1)</engine>
+         <sharding_key>rand()</sharding_key>
+     </table_events>
+   </tables>
+ </yandex>
diff --git a/tests/integration/test_cluster_copier/task_skip_index.xml b/tests/integration/test_cluster_copier/task_skip_index.xml
new file mode 100644
index 000000000000..5bc161813236
--- /dev/null
+++ b/tests/integration/test_cluster_copier/task_skip_index.xml
@@ -0,0 +1,40 @@
+<?xml version="1.0"?>
+<yandex>
+<remote_servers>
+        <source>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>first_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </source>
+        <destination>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>second_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </destination>
+    </remote_servers>
+
+   <max_workers>2</max_workers>
+
+   <tables>
+     <table_events>
+         <cluster_pull>source</cluster_pull>
+         <database_pull>db_skip_index</database_pull>
+         <table_pull>source</table_pull>
+
+         <cluster_push>destination</cluster_push>
+         <database_push>db_skip_index</database_push>
+         <table_push>destination</table_push>
+
+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column3, Column2, Column1)</engine>
+         <sharding_key>rand()</sharding_key>
+     </table_events>
+   </tables>
+ </yandex>
diff --git a/tests/integration/test_cluster_copier/task_taxi_data.xml b/tests/integration/test_cluster_copier/task_taxi_data.xml
new file mode 100644
index 000000000000..fafffe3ebc93
--- /dev/null
+++ b/tests/integration/test_cluster_copier/task_taxi_data.xml
@@ -0,0 +1,43 @@
+<?xml version="1.0"?>
+<yandex>
+   <remote_servers>
+         <events>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>first</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>second</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>third</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+         </events>
+   </remote_servers>
+
+   <max_workers>2</max_workers>
+
+   <tables>
+     <table_events>
+         <cluster_pull>events</cluster_pull>
+         <database_pull>dailyhistory</database_pull>
+         <table_pull>yellow_tripdata_staging</table_pull>
+         <cluster_push>events</cluster_push>
+         <database_push>monthlyhistory</database_push>
+         <table_push>yellow_tripdata_staging</table_push>
+         <engine>Engine=ReplacingMergeTree() PRIMARY KEY (tpep_pickup_datetime, id) ORDER BY (tpep_pickup_datetime, id) PARTITION BY (pickup_location_id, toYYYYMM(tpep_pickup_datetime))</engine>
+         <sharding_key>sipHash64(id) % 3</sharding_key>
+     </table_events>
+   </tables>
+ </yandex>
\ No newline at end of file
diff --git a/tests/integration/test_cluster_copier/task_trivial.xml b/tests/integration/test_cluster_copier/task_trivial.xml
index 27af6f64bf4c..ddf0d8a52a15 100644
--- a/tests/integration/test_cluster_copier/task_trivial.xml
+++ b/tests/integration/test_cluster_copier/task_trivial.xml
@@ -44,7 +44,7 @@
         <source_trivial_cluster>
             <shard>
                 <replica>
-                    <host>s0_0_0</host>
+                    <host>first_trivial</host>
                     <port>9000</port>
                 </replica>
             </shard>
@@ -54,11 +54,11 @@
         <destination_trivial_cluster>
             <shard>
                 <replica>
-                    <host>s1_0_0</host>
+                    <host>second_trivial</host>
                     <port>9000</port>
                 </replica>
             </shard>
         </destination_trivial_cluster>
     </remote_servers>
 
-</yandex>
\ No newline at end of file
+</yandex>
diff --git a/tests/integration/test_cluster_copier/task_trivial_without_arguments.xml b/tests/integration/test_cluster_copier/task_trivial_without_arguments.xml
new file mode 100644
index 000000000000..86f383e056e5
--- /dev/null
+++ b/tests/integration/test_cluster_copier/task_trivial_without_arguments.xml
@@ -0,0 +1,64 @@
+<?xml version="1.0"?>
+<yandex>
+    <!-- How many simualteneous workers are posssible -->
+    <max_workers>3</max_workers>
+
+    <!-- Common setting for pull and push operations -->
+    <settings>
+        <connect_timeout>1</connect_timeout>
+    </settings>
+
+    <!-- Setting used to fetch data -->
+    <settings_pull>
+        <max_rows_in_distinct>0</max_rows_in_distinct>
+    </settings_pull>
+
+    <!-- Setting used to insert data -->
+    <settings_push>
+    </settings_push>
+
+    <!-- Tasks -->
+    <tables>
+        <hits>
+            <cluster_pull>source_trivial_cluster</cluster_pull>
+            <database_pull>default</database_pull>
+            <table_pull>trivial_without_arguments</table_pull>
+
+            <cluster_push>destination_trivial_cluster</cluster_push>
+            <database_push>default</database_push>
+            <table_push>trivial_without_arguments</table_push>
+
+            <!-- Engine of destination tables -->
+            <engine>ENGINE=ReplicatedMergeTree() PARTITION BY d % 5 ORDER BY (d, sipHash64(d)) SAMPLE BY sipHash64(d) SETTINGS index_granularity = 16</engine>
+
+            <!-- Which sarding key to use while copying -->
+            <sharding_key>d + 1</sharding_key>
+
+            <!-- Optional expression that filter copying data -->
+            <where_condition>d - d = 0</where_condition>
+        </hits>
+    </tables>
+
+    <!-- Configuration of clusters -->
+    <remote_servers>
+        <source_trivial_cluster>
+            <shard>
+                <replica>
+                    <host>first_trivial</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+        </source_trivial_cluster>
+
+
+        <destination_trivial_cluster>
+            <shard>
+                <replica>
+                    <host>second_trivial</host>
+                    <port>9000</port>
+                </replica>
+            </shard>
+        </destination_trivial_cluster>
+    </remote_servers>
+
+</yandex>
diff --git a/tests/integration/test_cluster_copier/task_ttl_columns.xml b/tests/integration/test_cluster_copier/task_ttl_columns.xml
new file mode 100644
index 000000000000..68868877d316
--- /dev/null
+++ b/tests/integration/test_cluster_copier/task_ttl_columns.xml
@@ -0,0 +1,40 @@
+<?xml version="1.0"?>
+<yandex>
+<remote_servers>
+        <source>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>first_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </source>
+        <destination>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>second_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </destination>
+    </remote_servers>
+
+   <max_workers>2</max_workers>
+
+   <tables>
+     <table_events>
+         <cluster_pull>source</cluster_pull>
+         <database_pull>db_ttl_columns</database_pull>
+         <table_pull>source</table_pull>
+
+         <cluster_push>destination</cluster_push>
+         <database_push>db_ttl_columns</database_push>
+         <table_push>destination</table_push>
+
+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column3, Column2, Column1)</engine>
+         <sharding_key>rand()</sharding_key>
+     </table_events>
+   </tables>
+ </yandex>
diff --git a/tests/integration/test_cluster_copier/task_ttl_move_to_volume.xml b/tests/integration/test_cluster_copier/task_ttl_move_to_volume.xml
new file mode 100644
index 000000000000..051988964d2f
--- /dev/null
+++ b/tests/integration/test_cluster_copier/task_ttl_move_to_volume.xml
@@ -0,0 +1,40 @@
+<?xml version="1.0"?>
+<yandex>
+<remote_servers>
+        <source>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>first_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </source>
+        <destination>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>second_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </destination>
+    </remote_servers>
+
+   <max_workers>2</max_workers>
+
+   <tables>
+     <table_events>
+         <cluster_pull>source</cluster_pull>
+         <database_pull>db_move_to_volume</database_pull>
+         <table_pull>source</table_pull>
+
+         <cluster_push>destination</cluster_push>
+         <database_push>db_move_to_volume</database_push>
+         <table_push>destination</table_push>
+
+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column3, Column2, Column1) TTL Column3 + INTERVAL 1 MONTH TO VOLUME 'external' SETTINGS storage_policy = 'external_with_jbods'</engine>
+         <sharding_key>rand()</sharding_key>
+     </table_events>
+   </tables>
+ </yandex>
diff --git a/tests/integration/test_cluster_copier/task_with_different_schema.xml b/tests/integration/test_cluster_copier/task_with_different_schema.xml
new file mode 100644
index 000000000000..409ca7d2e997
--- /dev/null
+++ b/tests/integration/test_cluster_copier/task_with_different_schema.xml
@@ -0,0 +1,40 @@
+<?xml version="1.0"?>
+<yandex>
+<remote_servers>
+        <source>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>first_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </source>
+        <destination>
+             <shard>
+                 <internal_replication>false</internal_replication>
+                 <replica>
+                     <host>second_of_two</host>
+                     <port>9000</port>
+                 </replica>
+             </shard>
+        </destination>
+    </remote_servers>
+
+   <max_workers>2</max_workers>
+
+   <tables>
+     <table_events>
+         <cluster_pull>source</cluster_pull>
+         <database_pull>db_different_schema</database_pull>
+         <table_pull>source</table_pull>
+
+         <cluster_push>destination</cluster_push>
+         <database_push>db_different_schema</database_push>
+         <table_push>destination</table_push>
+
+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column9, Column1, Column2, Column3, Column4)</engine>
+         <sharding_key>rand()</sharding_key>
+     </table_events>
+   </tables>
+ </yandex>
diff --git a/tests/integration/test_cluster_copier/test.py b/tests/integration/test_cluster_copier/test.py
index c6068e3a6e90..7fe1d8c9d29b 100644
--- a/tests/integration/test_cluster_copier/test.py
+++ b/tests/integration/test_cluster_copier/test.py
@@ -2,21 +2,26 @@
 import random
 import sys
 import time
-from contextlib import contextmanager
-
-import docker
 import kazoo
 import pytest
+import string
+import random
+from contextlib import contextmanager
 from helpers.cluster import ClickHouseCluster
 from helpers.test_tools import TSV
 
+import docker
+
 CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))
 sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))
 
 COPYING_FAIL_PROBABILITY = 0.2
 MOVING_FAIL_PROBABILITY = 0.2
 
-cluster = ClickHouseCluster(__file__)
+cluster = ClickHouseCluster(__file__, name='copier_test')
+
+def generateRandomString(count):
+    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(count))
 
 
 def check_all_hosts_sucesfully_executed(tsv_content, num_hosts):
@@ -72,8 +77,13 @@ class Task1:
 
     def __init__(self, cluster):
         self.cluster = cluster
-        self.zk_task_path = "/clickhouse-copier/task_simple"
-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task0_description.xml'), 'r').read()
+        self.zk_task_path = "/clickhouse-copier/task_simple_" + generateRandomString(10)
+        self.container_task_file = "/task0_description.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task0_description.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
 
     def start(self):
         instance = cluster.instances['s0_0_0']
@@ -112,9 +122,14 @@ class Task2:
 
     def __init__(self, cluster, unique_zk_path):
         self.cluster = cluster
-        self.zk_task_path = "/clickhouse-copier/task_month_to_week_partition"
-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_month_to_week_description.xml'), 'r').read()
-        self.unique_zk_path = unique_zk_path
+        self.zk_task_path = "/clickhouse-copier/task_month_to_week_partition_" + generateRandomString(5)
+        self.unique_zk_path = generateRandomString(10)
+        self.container_task_file = "/task_month_to_week_description.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_month_to_week_description.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
 
     def start(self):
         instance = cluster.instances['s0_0_0']
@@ -163,9 +178,14 @@ class Task_test_block_size:
 
     def __init__(self, cluster):
         self.cluster = cluster
-        self.zk_task_path = "/clickhouse-copier/task_test_block_size"
-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_test_block_size.xml'), 'r').read()
+        self.zk_task_path = "/clickhouse-copier/task_test_block_size_" + generateRandomString(5)
         self.rows = 1000000
+        self.container_task_file = "/task_test_block_size.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_test_block_size.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
 
     def start(self):
         instance = cluster.instances['s0_0_0']
@@ -192,13 +212,19 @@ class Task_no_index:
 
     def __init__(self, cluster):
         self.cluster = cluster
-        self.zk_task_path = "/clickhouse-copier/task_no_index"
-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_no_index.xml'), 'r').read()
+        self.zk_task_path = "/clickhouse-copier/task_no_index_" + generateRandomString(5)
         self.rows = 1000000
+        self.container_task_file = "/task_no_index.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_no_index.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
 
     def start(self):
         instance = cluster.instances['s0_0_0']
-        instance.query("create table ontime (Year UInt16, FlightDate String) ENGINE = Memory")
+        instance.query("DROP TABLE IF EXISTS ontime SYNC")
+        instance.query("create table IF NOT EXISTS ontime (Year UInt16, FlightDate String) ENGINE = Memory")
         instance.query("insert into ontime values (2016, 'test6'), (2017, 'test7'), (2018, 'test8')")
 
     def check(self):
@@ -214,32 +240,44 @@ class Task_no_arg:
     def __init__(self, cluster):
         self.cluster = cluster
         self.zk_task_path = "/clickhouse-copier/task_no_arg"
-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_no_arg.xml'), 'r').read()
         self.rows = 1000000
+        self.container_task_file = "/task_no_arg.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_no_arg.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
 
     def start(self):
         instance = cluster.instances['s0_0_0']
+        instance.query("DROP TABLE IF EXISTS copier_test1 SYNC")
         instance.query(
-            "create table copier_test1 (date Date, id UInt32) engine = MergeTree PARTITION BY date ORDER BY date SETTINGS index_granularity = 8192")
+            "create table if not exists copier_test1 (date Date, id UInt32) engine = MergeTree PARTITION BY date ORDER BY date SETTINGS index_granularity = 8192")
         instance.query("insert into copier_test1 values ('2016-01-01', 10);")
 
     def check(self):
         assert TSV(self.cluster.instances['s1_1_0'].query("SELECT date FROM copier_test1_1")) == TSV("2016-01-01
")
         instance = cluster.instances['s0_0_0']
-        instance.query("DROP TABLE copier_test1")
+        instance.query("DROP TABLE copier_test1 SYNC")
         instance = cluster.instances['s1_1_0']
-        instance.query("DROP TABLE copier_test1_1")
+        instance.query("DROP TABLE copier_test1_1 SYNC")
 
 class Task_non_partitioned_table:
 
     def __init__(self, cluster):
         self.cluster = cluster
         self.zk_task_path = "/clickhouse-copier/task_non_partitoned_table"
-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_non_partitioned_table.xml'), 'r').read()
         self.rows = 1000000
+        self.container_task_file = "/task_non_partitioned_table.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_non_partitioned_table.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
 
     def start(self):
         instance = cluster.instances['s0_0_0']
+        instance.query("DROP TABLE IF EXISTS copier_test1 SYNC")
         instance.query(
             "create table copier_test1 (date Date, id UInt32) engine = MergeTree ORDER BY date SETTINGS index_granularity = 8192")
         instance.query("insert into copier_test1 values ('2016-01-01', 10);")
@@ -256,16 +294,23 @@ class Task_self_copy:
     def __init__(self, cluster):
         self.cluster = cluster
         self.zk_task_path = "/clickhouse-copier/task_self_copy"
-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_self_copy.xml'), 'r').read()
+        self.container_task_file = "/task_self_copy.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_self_copy.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
 
     def start(self):
         instance = cluster.instances['s0_0_0']
-        instance.query("CREATE DATABASE db1;")
+        instance.query("DROP DATABASE IF EXISTS db1 SYNC")
+        instance.query("DROP DATABASE IF EXISTS db2 SYNC")
+        instance.query("CREATE DATABASE IF NOT EXISTS db1;")
         instance.query(
-            "CREATE TABLE db1.source_table (`a` Int8, `b` String, `c` Int8) ENGINE = MergeTree PARTITION BY a ORDER BY a SETTINGS index_granularity = 8192")
-        instance.query("CREATE DATABASE db2;")
+            "CREATE TABLE IF NOT EXISTS db1.source_table (`a` Int8, `b` String, `c` Int8) ENGINE = MergeTree PARTITION BY a ORDER BY a SETTINGS index_granularity = 8192")
+        instance.query("CREATE DATABASE IF NOT EXISTS db2;")
         instance.query(
-            "CREATE TABLE db2.destination_table (`a` Int8, `b` String, `c` Int8) ENGINE = MergeTree PARTITION BY a ORDER BY a SETTINGS index_granularity = 8192")
+            "CREATE TABLE IF NOT EXISTS db2.destination_table (`a` Int8, `b` String, `c` Int8) ENGINE = MergeTree PARTITION BY a ORDER BY a SETTINGS index_granularity = 8192")
         instance.query("INSERT INTO db1.source_table VALUES (1, 'ClickHouse', 1);")
         instance.query("INSERT INTO db1.source_table VALUES (2, 'Copier', 2);")
 
@@ -273,8 +318,8 @@ def check(self):
         instance = cluster.instances['s0_0_0']
         assert TSV(instance.query("SELECT * FROM db2.destination_table ORDER BY a")) == TSV(instance.query("SELECT * FROM db1.source_table ORDER BY a"))
         instance = cluster.instances['s0_0_0']
-        instance.query("DROP DATABASE db1 SYNC")
-        instance.query("DROP DATABASE db2 SYNC")
+        instance.query("DROP DATABASE IF EXISTS db1 SYNC")
+        instance.query("DROP DATABASE IF EXISTS db2 SYNC")
 
 
 def execute_task(started_cluster, task, cmd_options):
@@ -283,26 +328,27 @@ def execute_task(started_cluster, task, cmd_options):
     zk = started_cluster.get_kazoo_client('zoo1')
     print("Use ZooKeeper server: {}:{}".format(zk.hosts[0][0], zk.hosts[0][1]))
 
+
     try:
         zk.delete("/clickhouse-copier", recursive=True)
     except kazoo.exceptions.NoNodeError:
         print("No node /clickhouse-copier. It is Ok in first test.")
 
-    zk_task_path = task.zk_task_path
-    zk.ensure_path(zk_task_path)
-    zk.create(zk_task_path + "/description", task.copier_task_config.encode())
-
     # Run cluster-copier processes on each node
     docker_api = started_cluster.docker_client.api
     copiers_exec_ids = []
 
     cmd = ['/usr/bin/clickhouse', 'copier',
            '--config', '/etc/clickhouse-server/config-copier.xml',
-           '--task-path', zk_task_path,
+           '--task-path', task.zk_task_path,
+           '--task-file', task.container_task_file,
+           '--task-upload-force', 'true',
            '--base-dir', '/var/log/clickhouse-server/copier']
     cmd += cmd_options
 
-    copiers = random.sample(list(cluster.instances.keys()), 3)
+    print(cmd)
+
+    copiers = random.sample(list(started_cluster.instances.keys()), 3)
 
     for instance_name in copiers:
         instance = started_cluster.instances[instance_name]
@@ -330,18 +376,12 @@ def execute_task(started_cluster, task, cmd_options):
     try:
         task.check()
     finally:
-        zk.delete(zk_task_path, recursive=True)
+        zk.delete(task.zk_task_path, recursive=True)
 
 
 # Tests
 
-@pytest.mark.parametrize(
-    ('use_sample_offset'),
-    [
-        False,
-        True
-    ]
-)
+@pytest.mark.parametrize(('use_sample_offset'), [False, True])
 def test_copy_simple(started_cluster, use_sample_offset):
     if use_sample_offset:
         execute_task(started_cluster, Task1(started_cluster), ['--experimental-use-sample-offset', '1'])
@@ -349,13 +389,7 @@ def test_copy_simple(started_cluster, use_sample_offset):
         execute_task(started_cluster, Task1(started_cluster), [])
 
 
-@pytest.mark.parametrize(
-    ('use_sample_offset'),
-    [
-        False,
-        True
-    ]
-)
+@pytest.mark.parametrize(('use_sample_offset'),[False, True])
 def test_copy_with_recovering(started_cluster, use_sample_offset):
     if use_sample_offset:
         execute_task(started_cluster, Task1(started_cluster), ['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY),
@@ -364,13 +398,7 @@ def test_copy_with_recovering(started_cluster, use_sample_offset):
         execute_task(started_cluster, Task1(started_cluster), ['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY)])
 
 
-@pytest.mark.parametrize(
-    ('use_sample_offset'),
-    [
-        False,
-        True
-    ]
-)
+@pytest.mark.parametrize(('use_sample_offset'),[False, True])
 def test_copy_with_recovering_after_move_faults(started_cluster, use_sample_offset):
     if use_sample_offset:
         execute_task(started_cluster, Task1(started_cluster), ['--move-fault-probability', str(MOVING_FAIL_PROBABILITY),
@@ -412,9 +440,3 @@ def test_non_partitioned_table(started_cluster):
 
 def test_self_copy(started_cluster):
     execute_task(started_cluster, Task_self_copy(started_cluster), [])
-
-if __name__ == '__main__':
-    with contextmanager(started_cluster)() as cluster:
-        for name, instance in list(cluster.instances.items()):
-            print(name, instance.ip_address)
-        input("Cluster created, press any key to destroy...")
diff --git a/tests/integration/test_cluster_copier/test_three_nodes.py b/tests/integration/test_cluster_copier/test_three_nodes.py
new file mode 100644
index 000000000000..acdc191154cd
--- /dev/null
+++ b/tests/integration/test_cluster_copier/test_three_nodes.py
@@ -0,0 +1,238 @@
+import os
+import sys
+import time
+import logging
+import pytest
+
+from helpers.cluster import ClickHouseCluster
+from helpers.test_tools import TSV
+
+import docker
+
+CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))
+sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))
+
+cluster = ClickHouseCluster(__file__, name='copier_test_three_nodes')
+
+@pytest.fixture(scope="module")
+def started_cluster():
+    global cluster
+    try:
+
+        for name in ["first", "second", "third"]:
+            cluster.add_instance(name,
+                main_configs=["configs_three_nodes/conf.d/clusters.xml", "configs_three_nodes/conf.d/ddl.xml"], user_configs=["configs_three_nodes/users.xml"],
+                with_zookeeper=True)
+
+        cluster.start()
+        yield cluster
+
+    finally:
+        cluster.shutdown()
+
+class Task:
+    def __init__(self, cluster):
+        self.cluster = cluster
+        self.zk_task_path = '/clickhouse-copier/task'
+        self.container_task_file = "/task_taxi_data.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_taxi_data.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
+
+
+    def start(self):
+        for name in ["first", "second", "third"]:
+            node = cluster.instances[name]
+            node.query("DROP DATABASE IF EXISTS dailyhistory SYNC;")
+            node.query("DROP DATABASE IF EXISTS monthlyhistory SYNC;")
+
+        instance = cluster.instances['first']
+
+        # daily partition database
+        instance.query("CREATE DATABASE IF NOT EXISTS dailyhistory on cluster events;")
+        instance.query("""CREATE TABLE dailyhistory.yellow_tripdata_staging ON CLUSTER events
+        (
+            id UUID DEFAULT generateUUIDv4(),
+            vendor_id String,
+            tpep_pickup_datetime DateTime('UTC'),
+            tpep_dropoff_datetime DateTime('UTC'),
+            passenger_count Nullable(Float64),
+            trip_distance String,
+            pickup_longitude Float64,
+            pickup_latitude Float64,
+            rate_code_id String,
+            store_and_fwd_flag String,
+            dropoff_longitude Float64,
+            dropoff_latitude Float64,
+            payment_type String,
+            fare_amount String,
+            extra String,
+            mta_tax String,
+            tip_amount String,
+            tolls_amount String,
+            improvement_surcharge String,
+            total_amount String,
+            pickup_location_id String,
+            dropoff_location_id String,
+            congestion_surcharge String,
+            junk1 String,  junk2 String
+        )
+        Engine = ReplacingMergeTree()
+        PRIMARY KEY (tpep_pickup_datetime, id)
+        ORDER BY (tpep_pickup_datetime, id)
+        PARTITION BY (toYYYYMMDD(tpep_pickup_datetime))""")
+
+        instance.query("""CREATE TABLE dailyhistory.yellow_tripdata
+            ON CLUSTER events
+            AS dailyhistory.yellow_tripdata_staging
+            ENGINE = Distributed('events', 'dailyhistory', yellow_tripdata_staging, sipHash64(id) % 3);""")
+
+        instance.query("""INSERT INTO dailyhistory.yellow_tripdata
+            SELECT * FROM generateRandom(
+                'id UUID DEFAULT generateUUIDv4(),
+                vendor_id String,
+                tpep_pickup_datetime DateTime(\\'UTC\\'),
+                tpep_dropoff_datetime DateTime(\\'UTC\\'),
+                passenger_count Nullable(Float64),
+                trip_distance String,
+                pickup_longitude Float64,
+                pickup_latitude Float64,
+                rate_code_id String,
+                store_and_fwd_flag String,
+                dropoff_longitude Float64,
+                dropoff_latitude Float64,
+                payment_type String,
+                fare_amount String,
+                extra String,
+                mta_tax String,
+                tip_amount String,
+                tolls_amount String,
+                improvement_surcharge String,
+                total_amount String,
+                pickup_location_id String,
+                dropoff_location_id String,
+                congestion_surcharge String,
+                junk1 String,
+                junk2 String',
+            1, 10, 2) LIMIT 50;""")
+
+        # monthly partition database
+        instance.query("create database IF NOT EXISTS monthlyhistory on cluster events;")
+        instance.query("""CREATE TABLE monthlyhistory.yellow_tripdata_staging ON CLUSTER events
+        (
+            id UUID DEFAULT generateUUIDv4(),
+            vendor_id String,
+            tpep_pickup_datetime DateTime('UTC'),
+            tpep_dropoff_datetime DateTime('UTC'),
+            passenger_count Nullable(Float64),
+            trip_distance String,
+            pickup_longitude Float64,
+            pickup_latitude Float64,
+            rate_code_id String,
+            store_and_fwd_flag String,
+            dropoff_longitude Float64,
+            dropoff_latitude Float64,
+            payment_type String,
+            fare_amount String,
+            extra String,
+            mta_tax String,
+            tip_amount String,
+            tolls_amount String,
+            improvement_surcharge String,
+            total_amount String,
+            pickup_location_id String,
+            dropoff_location_id String,
+            congestion_surcharge String,
+            junk1 String,
+            junk2 String
+        )
+        Engine = ReplacingMergeTree()
+        PRIMARY KEY (tpep_pickup_datetime, id)
+        ORDER BY (tpep_pickup_datetime, id)
+        PARTITION BY (pickup_location_id, toYYYYMM(tpep_pickup_datetime))""")
+
+        instance.query("""CREATE TABLE monthlyhistory.yellow_tripdata
+            ON CLUSTER events
+            AS monthlyhistory.yellow_tripdata_staging
+            ENGINE = Distributed('events', 'monthlyhistory', yellow_tripdata_staging, sipHash64(id) % 3);""")
+
+
+    def check(self):
+        instance = cluster.instances["first"]
+        a = TSV(instance.query("SELECT count() from dailyhistory.yellow_tripdata"))
+        b = TSV(instance.query("SELECT count() from monthlyhistory.yellow_tripdata"))
+        assert a == b, "Distributed tables"
+
+        for instance_name, instance in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            a = instance.query("SELECT count() from dailyhistory.yellow_tripdata_staging")
+            b = instance.query("SELECT count() from monthlyhistory.yellow_tripdata_staging")
+            assert a == b, "MergeTree tables on each shard"
+
+            a = TSV(instance.query("SELECT sipHash64(*) from dailyhistory.yellow_tripdata_staging ORDER BY id"))
+            b = TSV(instance.query("SELECT sipHash64(*) from monthlyhistory.yellow_tripdata_staging ORDER BY id"))
+
+            assert a == b, "Data on each shard"
+
+        for name in ["first", "second", "third"]:
+            node = cluster.instances[name]
+            node.query("DROP DATABASE IF EXISTS dailyhistory SYNC;")
+            node.query("DROP DATABASE IF EXISTS monthlyhistory SYNC;")
+
+
+
+def execute_task(started_cluster, task, cmd_options):
+    task.start()
+
+    zk = started_cluster.get_kazoo_client('zoo1')
+    print("Use ZooKeeper server: {}:{}".format(zk.hosts[0][0], zk.hosts[0][1]))
+
+    # Run cluster-copier processes on each node
+    docker_api = docker.from_env().api
+    copiers_exec_ids = []
+
+    cmd = ['/usr/bin/clickhouse', 'copier',
+           '--config', '/etc/clickhouse-server/config-copier.xml',
+           '--task-path', task.zk_task_path,
+           '--task-file', task.container_task_file,
+           '--task-upload-force', 'true',
+           '--base-dir', '/var/log/clickhouse-server/copier']
+    cmd += cmd_options
+
+    print(cmd)
+
+    for instance_name, instance in started_cluster.instances.items():
+        instance = started_cluster.instances[instance_name]
+        container = instance.get_docker_handle()
+        instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, "configs_three_nodes/config-copier.xml"), "/etc/clickhouse-server/config-copier.xml")
+        logging.info("Copied copier config to {}".format(instance.name))
+        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)
+        output = docker_api.exec_start(exec_id).decode('utf8')
+        logging.info(output)
+        copiers_exec_ids.append(exec_id)
+        logging.info("Copier for {} ({}) has started".format(instance.name, instance.ip_address))
+
+    # time.sleep(1000)
+
+    # Wait for copiers stopping and check their return codes
+    for exec_id, instance in zip(copiers_exec_ids, iter(started_cluster.instances.values())):
+        while True:
+            res = docker_api.exec_inspect(exec_id)
+            if not res['Running']:
+                break
+            time.sleep(1)
+
+        assert res['ExitCode'] == 0, "Instance: {} ({}). Info: {}".format(instance.name, instance.ip_address, repr(res))
+
+    try:
+        task.check()
+    finally:
+        zk.delete(task.zk_task_path, recursive=True)
+
+
+# Tests
+@pytest.mark.timeout(600)
+def test(started_cluster):
+    execute_task(started_cluster, Task(started_cluster), [])
diff --git a/tests/integration/test_cluster_copier/test_trivial.py b/tests/integration/test_cluster_copier/test_trivial.py
new file mode 100644
index 000000000000..e58c6edcb4de
--- /dev/null
+++ b/tests/integration/test_cluster_copier/test_trivial.py
@@ -0,0 +1,182 @@
+import os
+import sys
+import time
+import random
+import string
+
+from helpers.cluster import ClickHouseCluster
+from helpers.test_tools import TSV
+
+import kazoo
+import pytest
+import docker
+
+
+CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))
+sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))
+
+
+COPYING_FAIL_PROBABILITY = 0.1
+MOVING_FAIL_PROBABILITY = 0.1
+
+cluster = ClickHouseCluster(__file__, name='copier_test_trivial')
+
+
+def generateRandomString(count):
+    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(count))
+
+
+@pytest.fixture(scope="module")
+def started_cluster():
+    global cluster
+    try:
+        for name in ["first_trivial", "second_trivial"]:
+            instance = cluster.add_instance(name,
+                main_configs=["configs/conf.d/clusters_trivial.xml"],
+                user_configs=["configs_two_nodes/users.xml"],
+                macros={"cluster" : name, "shard" : "the_only_shard", "replica" : "the_only_replica"},
+                with_zookeeper=True)
+
+        cluster.start()
+        yield cluster
+
+    finally:
+        cluster.shutdown()
+
+
+class TaskTrivial:
+    def __init__(self, cluster):
+        self.cluster = cluster
+        self.zk_task_path = "/clickhouse-copier/task_trivial"
+        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_trivial.xml'), 'r').read()
+
+    def start(self):
+        source = cluster.instances['first_trivial']
+        destination = cluster.instances['second_trivial']
+
+        for node in [source, destination]:
+            node.query("DROP DATABASE IF EXISTS default")
+            node.query("CREATE DATABASE IF NOT EXISTS default")
+
+        source.query("CREATE TABLE trivial (d UInt64, d1 UInt64 MATERIALIZED d+1)"
+                     "ENGINE=ReplicatedMergeTree('/clickhouse/tables/source_trivial_cluster/1/trivial/{}', '1') "
+                     "PARTITION BY d % 5 ORDER BY (d, sipHash64(d)) SAMPLE BY sipHash64(d) SETTINGS index_granularity = 16".format(generateRandomString(10)))
+
+        source.query("INSERT INTO trivial SELECT * FROM system.numbers LIMIT 1002",
+                     settings={"insert_distributed_sync": 1})
+
+    def check(self):
+        zk = cluster.get_kazoo_client('zoo1')
+        status_data, _ = zk.get(self.zk_task_path + "/status")
+        assert status_data == b'{"hits":{"all_partitions_count":5,"processed_partitions_count":5}}'
+
+        source = cluster.instances['first_trivial']
+        destination = cluster.instances['second_trivial']
+
+        assert TSV(source.query("SELECT count() FROM trivial")) == TSV("1002
")
+        assert TSV(destination.query("SELECT count() FROM trivial")) == TSV("1002
")
+
+        for node in [source, destination]:
+            node.query("DROP TABLE trivial")
+
+
+class TaskReplicatedWithoutArguments:
+    def __init__(self, cluster):
+        self.cluster = cluster
+        self.zk_task_path = "/clickhouse-copier/task_trivial_without_arguments"
+        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_trivial_without_arguments.xml'), 'r').read()
+
+    def start(self):
+        source = cluster.instances['first_trivial']
+        destination = cluster.instances['second_trivial']
+
+        for node in [source, destination]:
+            node.query("DROP DATABASE IF EXISTS default")
+            node.query("CREATE DATABASE IF NOT EXISTS default")
+
+        source.query("CREATE TABLE trivial_without_arguments ON CLUSTER source_trivial_cluster (d UInt64, d1 UInt64 MATERIALIZED d+1) "
+                     "ENGINE=ReplicatedMergeTree() "
+                     "PARTITION BY d % 5 ORDER BY (d, sipHash64(d)) SAMPLE BY sipHash64(d) SETTINGS index_granularity = 16")
+
+        source.query("INSERT INTO trivial_without_arguments SELECT * FROM system.numbers LIMIT 1002",
+                     settings={"insert_distributed_sync": 1})
+
+    def check(self):
+        zk = cluster.get_kazoo_client('zoo1')
+        status_data, _ = zk.get(self.zk_task_path + "/status")
+        assert status_data == b'{"hits":{"all_partitions_count":5,"processed_partitions_count":5}}'
+
+        source = cluster.instances['first_trivial']
+        destination = cluster.instances['second_trivial']
+
+        assert TSV(source.query("SELECT count() FROM trivial_without_arguments")) == TSV("1002
")
+        assert TSV(destination.query("SELECT count() FROM trivial_without_arguments")) == TSV("1002
")
+
+        for node in [source, destination]:
+            node.query("DROP TABLE trivial_without_arguments")
+
+
+def execute_task(started_cluster, task, cmd_options):
+    task.start()
+
+    zk = started_cluster.get_kazoo_client('zoo1')
+    print("Use ZooKeeper server: {}:{}".format(zk.hosts[0][0], zk.hosts[0][1]))
+
+    try:
+        zk.delete("/clickhouse-copier", recursive=True)
+    except kazoo.exceptions.NoNodeError:
+        print("No node /clickhouse-copier. It is Ok in first test.")
+
+    zk_task_path = task.zk_task_path
+    zk.ensure_path(zk_task_path)
+    zk.create(zk_task_path + "/description", task.copier_task_config.encode())
+
+    # Run cluster-copier processes on each node
+    docker_api = started_cluster.docker_client.api
+    copiers_exec_ids = []
+
+    cmd = ['/usr/bin/clickhouse', 'copier',
+           '--config', '/etc/clickhouse-server/config-copier.xml',
+           '--task-path', zk_task_path,
+           '--base-dir', '/var/log/clickhouse-server/copier']
+    cmd += cmd_options
+
+    copiers = list(started_cluster.instances.keys())
+
+    for instance_name in copiers:
+        instance = started_cluster.instances[instance_name]
+        container = instance.get_docker_handle()
+        instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, "configs/config-copier.xml"),
+                                        "/etc/clickhouse-server/config-copier.xml")
+        print("Copied copier config to {}".format(instance.name))
+        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)
+        output = docker_api.exec_start(exec_id).decode('utf8')
+        print(output)
+        copiers_exec_ids.append(exec_id)
+        print("Copier for {} ({}) has started".format(instance.name, instance.ip_address))
+
+    # Wait for copiers stopping and check their return codes
+    for exec_id, instance_name in zip(copiers_exec_ids, copiers):
+        instance = started_cluster.instances[instance_name]
+        while True:
+            res = docker_api.exec_inspect(exec_id)
+            if not res['Running']:
+                break
+            time.sleep(0.5)
+
+        assert res['ExitCode'] == 0, "Instance: {} ({}). Info: {}".format(instance.name, instance.ip_address, repr(res))
+
+    try:
+        task.check()
+    finally:
+        zk.delete(zk_task_path, recursive=True)
+
+
+# Tests
+
+def test_trivial_copy(started_cluster):
+    execute_task(started_cluster, TaskTrivial(started_cluster), [])
+
+
+def test_trivial_without_arguments(started_cluster):
+    execute_task(started_cluster, TaskReplicatedWithoutArguments(started_cluster), [])
diff --git a/tests/integration/test_cluster_copier/test_two_nodes.py b/tests/integration/test_cluster_copier/test_two_nodes.py
new file mode 100644
index 000000000000..a6b2c82e00f9
--- /dev/null
+++ b/tests/integration/test_cluster_copier/test_two_nodes.py
@@ -0,0 +1,493 @@
+import os
+import sys
+import time
+import logging
+import pytest
+
+from helpers.cluster import ClickHouseCluster
+from helpers.test_tools import TSV
+
+import docker
+
+CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))
+sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))
+
+cluster = ClickHouseCluster(__file__, name='copier_test_two_nodes')
+
+
+@pytest.fixture(scope="module")
+def started_cluster():
+    global cluster
+    try:
+
+        for name in ["first_of_two", "second_of_two"]:
+            instance = cluster.add_instance(name,
+                main_configs=[
+                    "configs_two_nodes/conf.d/clusters.xml",
+                    "configs_two_nodes/conf.d/ddl.xml",
+                    "configs_two_nodes/conf.d/storage_configuration.xml"],
+                user_configs=["configs_two_nodes/users.xml"],
+                with_zookeeper=True)
+
+        cluster.start()
+
+        for name in ["first_of_two", "second_of_two"]:
+            instance = cluster.instances[name]
+            instance.exec_in_container(['bash', '-c', 'mkdir /jbod1'])
+            instance.exec_in_container(['bash', '-c', 'mkdir /jbod2'])
+            instance.exec_in_container(['bash', '-c', 'mkdir /external'])
+
+        yield cluster
+
+    finally:
+        cluster.shutdown()
+
+# Will copy table from `first` node to `second`
+class TaskWithDifferentSchema:
+    def __init__(self, cluster):
+        self.cluster = cluster
+        self.zk_task_path = '/clickhouse-copier/task_with_different_schema'
+        self.container_task_file = "/task_with_different_schema.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_with_different_schema.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
+
+    def start(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["second_of_two"]
+
+        first.query("DROP DATABASE IF EXISTS db_different_schema SYNC")
+        second.query("DROP DATABASE IF EXISTS db_different_schema SYNC")
+
+        first.query("CREATE DATABASE IF NOT EXISTS db_different_schema;")
+        first.query("""CREATE TABLE db_different_schema.source
+        (
+            Column1 String,
+            Column2 UInt32,
+            Column3 Date,
+            Column4 DateTime,
+            Column5 UInt16,
+            Column6 String,
+            Column7 String,
+            Column8 String,
+            Column9 String,
+            Column10 String,
+            Column11 String,
+            Column12 Decimal(3, 1),
+            Column13 DateTime,
+            Column14 UInt16
+        )
+        ENGINE = MergeTree()
+        PARTITION BY (toYYYYMMDD(Column3), Column3)
+        PRIMARY KEY (Column1, Column2, Column3, Column4, Column6, Column7, Column8, Column9)
+        ORDER BY (Column1, Column2, Column3, Column4, Column6, Column7, Column8, Column9)
+        SETTINGS index_granularity = 8192""")
+
+        first.query("""INSERT INTO db_different_schema.source SELECT * FROM generateRandom(
+            'Column1 String, Column2 UInt32, Column3 Date, Column4 DateTime, Column5 UInt16,
+            Column6 String, Column7 String, Column8 String, Column9 String, Column10 String,
+            Column11 String, Column12 Decimal(3, 1), Column13 DateTime, Column14 UInt16', 1, 10, 2) LIMIT 50;""")
+
+
+        second.query("CREATE DATABASE IF NOT EXISTS db_different_schema;")
+        second.query("""CREATE TABLE db_different_schema.destination
+        (
+            Column1 LowCardinality(String) CODEC(LZ4),
+            Column2 UInt32 CODEC(LZ4),
+            Column3 Date CODEC(DoubleDelta, LZ4),
+            Column4 DateTime CODEC(DoubleDelta, LZ4),
+            Column5 UInt16 CODEC(LZ4),
+            Column6 LowCardinality(String) CODEC(ZSTD),
+            Column7 LowCardinality(String) CODEC(ZSTD),
+            Column8 LowCardinality(String) CODEC(ZSTD),
+            Column9 LowCardinality(String) CODEC(ZSTD),
+            Column10 String CODEC(ZSTD(6)),
+            Column11 LowCardinality(String) CODEC(LZ4),
+            Column12 Decimal(3,1) CODEC(LZ4),
+            Column13 DateTime CODEC(DoubleDelta, LZ4),
+            Column14 UInt16 CODEC(LZ4)
+        ) ENGINE = MergeTree()
+        PARTITION BY toYYYYMMDD(Column3)
+        ORDER BY (Column9, Column1, Column2, Column3, Column4);""")
+
+        print("Preparation completed")
+
+    def check(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["second_of_two"]
+
+        a = first.query("SELECT count() from db_different_schema.source")
+        b = second.query("SELECT count() from db_different_schema.destination")
+        assert a == b, "Count"
+
+        a = TSV(first.query("""SELECT sipHash64(*) from db_different_schema.source
+            ORDER BY (Column1, Column2, Column3, Column4, Column5, Column6, Column7, Column8, Column9, Column10, Column11, Column12, Column13, Column14)"""))
+        b = TSV(second.query("""SELECT sipHash64(*) from db_different_schema.destination
+            ORDER BY (Column1, Column2, Column3, Column4, Column5, Column6, Column7, Column8, Column9, Column10, Column11, Column12, Column13, Column14)"""))
+        assert a == b, "Data"
+
+        first.query("DROP DATABASE IF EXISTS db_different_schema SYNC")
+        second.query("DROP DATABASE IF EXISTS db_different_schema SYNC")
+
+
+# Just simple copying, but table schema has TTL on columns
+# Also table will have slightly different schema
+class TaskTTL:
+    def __init__(self, cluster):
+        self.cluster = cluster
+        self.zk_task_path = '/clickhouse-copier/task_ttl_columns'
+        self.container_task_file = "/task_ttl_columns.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_ttl_columns.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
+
+    def start(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["second_of_two"]
+
+        first.query("DROP DATABASE IF EXISTS db_ttl_columns SYNC")
+        second.query("DROP DATABASE IF EXISTS db_ttl_columns SYNC")
+
+        first.query("CREATE DATABASE IF NOT EXISTS db_ttl_columns;")
+        first.query("""CREATE TABLE db_ttl_columns.source
+        (
+            Column1 String,
+            Column2 UInt32,
+            Column3 Date,
+            Column4 DateTime,
+            Column5 UInt16,
+            Column6 String TTL now() + INTERVAL 1 MONTH,
+            Column7 Decimal(3, 1) TTL now() + INTERVAL 1 MONTH,
+            Column8 Tuple(Float64, Float64) TTL now() + INTERVAL 1 MONTH
+        )
+        ENGINE = MergeTree()
+        PARTITION BY (toYYYYMMDD(Column3), Column3)
+        PRIMARY KEY (Column1, Column2, Column3)
+        ORDER BY (Column1, Column2, Column3)
+        SETTINGS index_granularity = 8192""")
+
+        first.query("""INSERT INTO db_ttl_columns.source SELECT * FROM generateRandom(
+            'Column1 String, Column2 UInt32, Column3 Date, Column4 DateTime, Column5 UInt16,
+            Column6 String, Column7 Decimal(3, 1), Column8 Tuple(Float64, Float64)', 1, 10, 2) LIMIT 50;""")
+
+        second.query("CREATE DATABASE IF NOT EXISTS db_ttl_columns;")
+        second.query("""CREATE TABLE db_ttl_columns.destination
+        (
+            Column1 String,
+            Column2 UInt32,
+            Column3 Date,
+            Column4 DateTime TTL now() + INTERVAL 1 MONTH,
+            Column5 UInt16 TTL now() + INTERVAL 1 MONTH,
+            Column6 String TTL now() + INTERVAL 1 MONTH,
+            Column7 Decimal(3, 1) TTL now() + INTERVAL 1 MONTH,
+            Column8 Tuple(Float64, Float64)
+        ) ENGINE = MergeTree()
+        PARTITION BY toYYYYMMDD(Column3)
+        ORDER BY (Column3, Column2, Column1);""")
+
+        print("Preparation completed")
+
+    def check(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["second_of_two"]
+
+        a = first.query("SELECT count() from db_ttl_columns.source")
+        b = second.query("SELECT count() from db_ttl_columns.destination")
+        assert a == b, "Count"
+
+        a = TSV(first.query("""SELECT sipHash64(*) from db_ttl_columns.source
+            ORDER BY (Column1, Column2, Column3, Column4, Column5, Column6, Column7, Column8)"""))
+        b = TSV(second.query("""SELECT sipHash64(*) from db_ttl_columns.destination
+            ORDER BY (Column1, Column2, Column3, Column4, Column5, Column6, Column7, Column8)"""))
+        assert a == b, "Data"
+
+        first.query("DROP DATABASE IF EXISTS db_ttl_columns SYNC")
+        second.query("DROP DATABASE IF EXISTS db_ttl_columns SYNC")
+
+
+class TaskSkipIndex:
+    def __init__(self, cluster):
+        self.cluster = cluster
+        self.zk_task_path = '/clickhouse-copier/task_skip_index'
+        self.container_task_file = "/task_skip_index.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_skip_index.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
+
+    def start(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["second_of_two"]
+
+        first.query("DROP DATABASE IF EXISTS db_skip_index SYNC")
+        second.query("DROP DATABASE IF EXISTS db_skip_index SYNC")
+
+        first.query("CREATE DATABASE IF NOT EXISTS db_skip_index;")
+        first.query("""CREATE TABLE db_skip_index.source
+        (
+            Column1 UInt64,
+            Column2 Int32,
+            Column3 Date,
+            Column4 DateTime,
+            Column5 String,
+            INDEX a (Column1 * Column2, Column5) TYPE minmax GRANULARITY 3,
+            INDEX b (Column1 * length(Column5)) TYPE set(1000) GRANULARITY 4
+        )
+        ENGINE = MergeTree()
+        PARTITION BY (toYYYYMMDD(Column3), Column3)
+        PRIMARY KEY (Column1, Column2, Column3)
+        ORDER BY (Column1, Column2, Column3)
+        SETTINGS index_granularity = 8192""")
+
+        first.query("""INSERT INTO db_skip_index.source SELECT * FROM generateRandom(
+            'Column1 UInt64, Column2 Int32, Column3 Date, Column4 DateTime, Column5 String', 1, 10, 2) LIMIT 100;""")
+
+        second.query("CREATE DATABASE IF NOT EXISTS db_skip_index;")
+        second.query("""CREATE TABLE db_skip_index.destination
+        (
+            Column1 UInt64,
+            Column2 Int32,
+            Column3 Date,
+            Column4 DateTime,
+            Column5 String,
+            INDEX a (Column1 * Column2, Column5) TYPE minmax GRANULARITY 3,
+            INDEX b (Column1 * length(Column5)) TYPE set(1000) GRANULARITY 4
+        ) ENGINE = MergeTree()
+        PARTITION BY toYYYYMMDD(Column3)
+        ORDER BY (Column3, Column2, Column1);""")
+
+        print("Preparation completed")
+
+    def check(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["second_of_two"]
+
+        a = first.query("SELECT count() from db_skip_index.source")
+        b = second.query("SELECT count() from db_skip_index.destination")
+        assert a == b, "Count"
+
+        a = TSV(first.query("""SELECT sipHash64(*) from db_skip_index.source
+            ORDER BY (Column1, Column2, Column3, Column4, Column5)"""))
+        b = TSV(second.query("""SELECT sipHash64(*) from db_skip_index.destination
+            ORDER BY (Column1, Column2, Column3, Column4, Column5)"""))
+        assert a == b, "Data"
+
+        first.query("DROP DATABASE IF EXISTS db_skip_index SYNC")
+        second.query("DROP DATABASE IF EXISTS db_skip_index SYNC")
+
+
+class TaskTTLMoveToVolume:
+    def __init__(self, cluster):
+        self.cluster = cluster
+        self.zk_task_path = '/clickhouse-copier/task_ttl_move_to_volume'
+        self.container_task_file = "/task_ttl_move_to_volume.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_ttl_move_to_volume.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
+
+    def start(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["first_of_two"]
+
+        first.query("DROP DATABASE IF EXISTS db_move_to_volume SYNC")
+        second.query("DROP DATABASE IF EXISTS db_move_to_volume SYNC")
+
+        first.query("CREATE DATABASE IF NOT EXISTS db_move_to_volume;")
+        first.query("""CREATE TABLE db_move_to_volume.source
+        (
+            Column1 UInt64,
+            Column2 Int32,
+            Column3 Date,
+            Column4 DateTime,
+            Column5 String
+        )
+        ENGINE = MergeTree()
+        PARTITION BY (toYYYYMMDD(Column3), Column3)
+        PRIMARY KEY (Column1, Column2, Column3)
+        ORDER BY (Column1, Column2, Column3)
+        TTL Column3 + INTERVAL 1 MONTH TO VOLUME 'external'
+        SETTINGS storage_policy = 'external_with_jbods';""")
+
+        first.query("""INSERT INTO db_move_to_volume.source SELECT * FROM generateRandom(
+            'Column1 UInt64, Column2 Int32, Column3 Date, Column4 DateTime, Column5 String', 1, 10, 2) LIMIT 100;""")
+
+        second.query("CREATE DATABASE IF NOT EXISTS db_move_to_volume;")
+        second.query("""CREATE TABLE db_move_to_volume.destination
+        (
+            Column1 UInt64,
+            Column2 Int32,
+            Column3 Date,
+            Column4 DateTime,
+            Column5 String
+        ) ENGINE = MergeTree()
+        PARTITION BY toYYYYMMDD(Column3)
+        ORDER BY (Column3, Column2, Column1)
+        TTL Column3 + INTERVAL 1 MONTH TO VOLUME 'external'
+        SETTINGS storage_policy = 'external_with_jbods';""")
+
+        print("Preparation completed")
+
+    def check(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["second_of_two"]
+
+        a = first.query("SELECT count() from db_move_to_volume.source")
+        b = second.query("SELECT count() from db_move_to_volume.destination")
+        assert a == b, "Count"
+
+        a = TSV(first.query("""SELECT sipHash64(*) from db_move_to_volume.source
+            ORDER BY (Column1, Column2, Column3, Column4, Column5)"""))
+        b = TSV(second.query("""SELECT sipHash64(*) from db_move_to_volume.destination
+            ORDER BY (Column1, Column2, Column3, Column4, Column5)"""))
+        assert a == b, "Data"
+
+        first.query("DROP DATABASE IF EXISTS db_move_to_volume SYNC")
+        second.query("DROP DATABASE IF EXISTS db_move_to_volume SYNC")
+
+
+class TaskDropTargetPartition:
+    def __init__(self, cluster):
+        self.cluster = cluster
+        self.zk_task_path = '/clickhouse-copier/task_drop_target_partition'
+        self.container_task_file = "/task_drop_target_partition.xml"
+
+        for instance_name, _ in cluster.instances.items():
+            instance = cluster.instances[instance_name]
+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_drop_target_partition.xml'), self.container_task_file)
+            print("Copied task file to container of '{}' instance. Path {}".format(instance_name, self.container_task_file))
+
+    def start(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["second_of_two"]
+
+        first.query("DROP DATABASE IF EXISTS db_drop_target_partition SYNC")
+        second.query("DROP DATABASE IF EXISTS db_drop_target_partition SYNC")
+
+        first.query("CREATE DATABASE IF NOT EXISTS db_drop_target_partition;")
+        first.query("""CREATE TABLE db_drop_target_partition.source
+        (
+            Column1 UInt64,
+            Column2 Int32,
+            Column3 Date,
+            Column4 DateTime,
+            Column5 String
+        )
+        ENGINE = MergeTree()
+        PARTITION BY (toYYYYMMDD(Column3), Column3)
+        PRIMARY KEY (Column1, Column2, Column3)
+        ORDER BY (Column1, Column2, Column3);""")
+
+        first.query("""INSERT INTO db_drop_target_partition.source SELECT * FROM generateRandom(
+            'Column1 UInt64, Column2 Int32, Column3 Date, Column4 DateTime, Column5 String', 1, 10, 2) LIMIT 100;""")
+
+
+        second.query("CREATE DATABASE IF NOT EXISTS db_drop_target_partition;")
+        second.query("""CREATE TABLE db_drop_target_partition.destination
+        (
+            Column1 UInt64,
+            Column2 Int32,
+            Column3 Date,
+            Column4 DateTime,
+            Column5 String
+        ) ENGINE = MergeTree()
+        PARTITION BY toYYYYMMDD(Column3)
+        ORDER BY (Column3, Column2, Column1);""")
+
+        # Insert data in target too. It has to be dropped.
+        first.query("""INSERT INTO db_drop_target_partition.destination SELECT * FROM db_drop_target_partition.source;""")
+
+        print("Preparation completed")
+
+    def check(self):
+        first = cluster.instances["first_of_two"]
+        second = cluster.instances["second_of_two"]
+
+        a = first.query("SELECT count() from db_drop_target_partition.source")
+        b = second.query("SELECT count() from db_drop_target_partition.destination")
+        assert a == b, "Count"
+
+        a = TSV(first.query("""SELECT sipHash64(*) from db_drop_target_partition.source
+            ORDER BY (Column1, Column2, Column3, Column4, Column5)"""))
+        b = TSV(second.query("""SELECT sipHash64(*) from db_drop_target_partition.destination
+            ORDER BY (Column1, Column2, Column3, Column4, Column5)"""))
+        assert a == b, "Data"
+
+        first.query("DROP DATABASE IF EXISTS db_drop_target_partition SYNC")
+        second.query("DROP DATABASE IF EXISTS db_drop_target_partition SYNC")
+
+
+def execute_task(started_cluster, task, cmd_options):
+    task.start()
+
+    zk = started_cluster.get_kazoo_client('zoo1')
+    print("Use ZooKeeper server: {}:{}".format(zk.hosts[0][0], zk.hosts[0][1]))
+
+    # Run cluster-copier processes on each node
+    docker_api = docker.from_env().api
+    copiers_exec_ids = []
+
+    cmd = ['/usr/bin/clickhouse', 'copier',
+           '--config', '/etc/clickhouse-server/config-copier.xml',
+           '--task-path', task.zk_task_path,
+           '--task-file', task.container_task_file,
+           '--task-upload-force', 'true',
+           '--base-dir', '/var/log/clickhouse-server/copier']
+    cmd += cmd_options
+
+    print(cmd)
+
+    for instance_name, instance in started_cluster.instances.items():
+        instance = started_cluster.instances[instance_name]
+        container = instance.get_docker_handle()
+        instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, "configs_two_nodes/config-copier.xml"), "/etc/clickhouse-server/config-copier.xml")
+        logging.info("Copied copier config to {}".format(instance.name))
+        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)
+        output = docker_api.exec_start(exec_id).decode('utf8')
+        logging.info(output)
+        copiers_exec_ids.append(exec_id)
+        logging.info("Copier for {} ({}) has started".format(instance.name, instance.ip_address))
+
+    # time.sleep(1000)
+
+    # Wait for copiers stopping and check their return codes
+    for exec_id, instance in zip(copiers_exec_ids, iter(started_cluster.instances.values())):
+        while True:
+            res = docker_api.exec_inspect(exec_id)
+            if not res['Running']:
+                break
+            time.sleep(1)
+
+        assert res['ExitCode'] == 0, "Instance: {} ({}). Info: {}".format(instance.name, instance.ip_address, repr(res))
+
+    try:
+        task.check()
+    finally:
+        zk.delete(task.zk_task_path, recursive=True)
+
+
+# Tests
+@pytest.mark.timeout(600)
+def test_different_schema(started_cluster):
+    execute_task(started_cluster, TaskWithDifferentSchema(started_cluster), [])
+
+
+@pytest.mark.timeout(600)
+def test_ttl_columns(started_cluster):
+    execute_task(started_cluster, TaskTTL(started_cluster), [])
+
+
+@pytest.mark.timeout(600)
+def test_skip_index(started_cluster):
+    execute_task(started_cluster, TaskSkipIndex(started_cluster), [])
+
+
+@pytest.mark.skip(reason="Too flaky :(")
+def test_ttl_move_to_volume(started_cluster):
+    execute_task(started_cluster, TaskTTLMoveToVolume(started_cluster), [])
diff --git a/tests/integration/test_cluster_copier/trivial_test.py b/tests/integration/test_cluster_copier/trivial_test.py
deleted file mode 100644
index 717ff9d8d34f..000000000000
--- a/tests/integration/test_cluster_copier/trivial_test.py
+++ /dev/null
@@ -1,180 +0,0 @@
-import os
-import sys
-import time
-from contextlib import contextmanager
-
-import docker
-import pytest
-
-CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))
-sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))
-from helpers.cluster import ClickHouseCluster
-from helpers.test_tools import TSV
-
-COPYING_FAIL_PROBABILITY = 0.33
-MOVING_FAIL_PROBABILITY = 0.1
-cluster = None
-
-
-@pytest.fixture(scope="function")
-def started_cluster():
-    global cluster
-    try:
-        clusters_schema = {
-            "0": {"0": ["0"]},
-            "1": {"0": ["0"]}
-        }
-
-        cluster = ClickHouseCluster(__file__)
-
-        for cluster_name, shards in clusters_schema.items():
-            for shard_name, replicas in shards.items():
-                for replica_name in replicas:
-                    name = "s{}_{}_{}".format(cluster_name, shard_name, replica_name)
-                    cluster.add_instance(name,
-                                         main_configs=[], user_configs=[],
-                                         macros={"cluster": cluster_name, "shard": shard_name, "replica": replica_name},
-                                         with_zookeeper=True)
-
-        cluster.start()
-        yield cluster
-
-    finally:
-        pass
-        cluster.shutdown()
-
-
-class TaskTrivial:
-    def __init__(self, cluster, use_sample_offset):
-        self.cluster = cluster
-        if use_sample_offset:
-            self.zk_task_path = "/clickhouse-copier/task_trivial_use_sample_offset"
-        else:
-            self.zk_task_path = "/clickhouse-copier/task_trivial"
-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_trivial.xml'), 'r').read()
-
-    def start(self):
-        source = cluster.instances['s0_0_0']
-        destination = cluster.instances['s1_0_0']
-
-        for node in [source, destination]:
-            node.query("DROP DATABASE IF EXISTS default")
-            node.query("CREATE DATABASE IF NOT EXISTS default")
-
-        source.query("CREATE TABLE trivial (d UInt64, d1 UInt64 MATERIALIZED d+1) "
-                     "ENGINE=ReplicatedMergeTree('/clickhouse/tables/source_trivial_cluster/1/trivial', '1') "
-                     "PARTITION BY d % 5 ORDER BY (d, sipHash64(d)) SAMPLE BY sipHash64(d) SETTINGS index_granularity = 16")
-
-        source.query("INSERT INTO trivial SELECT * FROM system.numbers LIMIT 1002",
-                     settings={"insert_distributed_sync": 1})
-
-    def check(self):
-        source = cluster.instances['s0_0_0']
-        destination = cluster.instances['s1_0_0']
-
-        assert TSV(source.query("SELECT count() FROM trivial")) == TSV("1002
")
-        assert TSV(destination.query("SELECT count() FROM trivial")) == TSV("1002
")
-
-        for node in [source, destination]:
-            node.query("DROP TABLE trivial")
-
-
-def execute_task(started_cluster, task, cmd_options):
-    task.start()
-
-    zk = started_cluster.get_kazoo_client('zoo1')
-    print("Use ZooKeeper server: {}:{}".format(zk.hosts[0][0], zk.hosts[0][1]))
-
-    zk_task_path = task.zk_task_path
-    zk.ensure_path(zk_task_path)
-    zk.create(zk_task_path + "/description", task.copier_task_config)
-
-    # Run cluster-copier processes on each node
-    docker_api = started_cluster.docker_client.api
-    copiers_exec_ids = []
-
-    cmd = ['/usr/bin/clickhouse', 'copier',
-           '--config', '/etc/clickhouse-server/config-copier.xml',
-           '--task-path', zk_task_path,
-           '--base-dir', '/var/log/clickhouse-server/copier']
-    cmd += cmd_options
-
-    print(cmd)
-
-    for instance_name, instance in started_cluster.instances.items():
-        container = instance.get_docker_handle()
-        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)
-        docker_api.exec_start(exec_id, detach=True)
-
-        copiers_exec_ids.append(exec_id)
-        print("Copier for {} ({}) has started".format(instance.name, instance.ip_address))
-
-    # Wait for copiers stopping and check their return codes
-    for exec_id, instance in zip(copiers_exec_ids, iter(started_cluster.instances.values())):
-        while True:
-            res = docker_api.exec_inspect(exec_id)
-            if not res['Running']:
-                break
-            time.sleep(1)
-
-        assert res['ExitCode'] == 0, "Instance: {} ({}). Info: {}".format(instance.name, instance.ip_address, repr(res))
-
-    try:
-        task.check()
-    finally:
-        zk.delete(zk_task_path, recursive=True)
-
-
-# Tests
-
-
-@pytest.mark.parametrize(
-    ('use_sample_offset'),
-    [
-        False,
-        True
-    ]
-)
-def test_trivial_copy(started_cluster, use_sample_offset):
-    if use_sample_offset:
-        execute_task(started_cluster, TaskTrivial(started_cluster, use_sample_offset), ['--experimental-use-sample-offset', '1'])
-    else:
-        print("AAAAA")
-        execute_task(started_cluster, TaskTrivial(started_cluster, use_sample_offset), [])
-
-
-@pytest.mark.parametrize(
-    ('use_sample_offset'),
-    [
-        False,
-        True
-    ]
-)
-def test_trivial_copy_with_copy_fault(started_cluster, use_sample_offset):
-    if use_sample_offset:
-        execute_task(started_cluster, TaskTrivial(started_cluster), ['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY),
-                                                    '--experimental-use-sample-offset', '1'])
-    else:
-        execute_task(started_cluster, TaskTrivial(started_cluster), ['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY)])
-
-
-@pytest.mark.parametrize(
-    ('use_sample_offset'),
-    [
-        False,
-        True
-    ]
-)
-def test_trivial_copy_with_move_fault(started_cluster, use_sample_offset):
-    if use_sample_offset:
-        execute_task(started_cluster, TaskTrivial(started_cluster), ['--move-fault-probability', str(MOVING_FAIL_PROBABILITY),
-                                                    '--experimental-use-sample-offset', '1'])
-    else:
-        execute_task(started_cluster, TaskTrivial(started_cluster), ['--move-fault-probability', str(MOVING_FAIL_PROBABILITY)])
-
-
-if __name__ == '__main__':
-    with contextmanager(started_cluster)() as cluster:
-        for name, instance in list(cluster.instances.items()):
-            print(name, instance.ip_address)
-        input("Cluster created, press any key to destroy...")
