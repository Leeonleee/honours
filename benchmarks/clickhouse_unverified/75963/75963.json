{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 75963,
  "instance_id": "ClickHouse__ClickHouse-75963",
  "issue_numbers": [
    "73668"
  ],
  "base_commit": "502df3fd1c001b742a49f65b7499586b52162824",
  "patch": "diff --git a/base/poco/JSON/src/ParserImpl.cpp b/base/poco/JSON/src/ParserImpl.cpp\nindex a13c4359af38..07e9f0a256da 100644\n--- a/base/poco/JSON/src/ParserImpl.cpp\n+++ b/base/poco/JSON/src/ParserImpl.cpp\n@@ -157,7 +157,9 @@ void ParserImpl::handleObject()\n \twhile (tok != JSON_OBJECT_END && checkError())\n \t{\n \t\tjson_next(_pJSON);\n-\t\tif (_pHandler) _pHandler->key(std::string(json_get_string(_pJSON, NULL)));\n+\t\tsize_t length;\n+\t\tconst char * bytes = json_get_string(_pJSON, &length);\n+\t\tif (_pHandler) _pHandler->key(std::string(bytes, length));\n \t\thandle();\n \t\ttok = json_peek(_pJSON);\n \t}\n@@ -187,7 +189,9 @@ void ParserImpl::handle()\n \t\t{\n \t\t\tif (_pHandler)\n \t\t\t{\n-\t\t\t\tstd::string str(json_get_string(_pJSON, NULL));\n+\t\t\t\tsize_t length;\n+\t\t\t\tconst char * bytes = json_get_string(_pJSON, &length);\n+\t\t\t\tstd::string str(bytes, length);\n \t\t\t\tif (str.find(_decimalPoint) != str.npos || str.find('e') != str.npos || str.find('E') != str.npos)\n \t\t\t\t{\n \t\t\t\t\t_pHandler->value(NumberParser::parseFloat(str));\n@@ -204,8 +208,12 @@ void ParserImpl::handle()\n \t\t\tbreak;\n \t\t}\n \t\tcase JSON_STRING:\n-\t\t\tif (_pHandler) _pHandler->value(std::string(json_get_string(_pJSON, NULL)));\n+\t\t{\n+\t\t\tsize_t length;\n+\t\t\tconst char * bytes = json_get_string(_pJSON, &length);\n+\t\t\tif (_pHandler) _pHandler->value(std::string(bytes, length));\n \t\t\tbreak;\n+\t\t}\n \t\tcase JSON_OBJECT:\n \t\t\tif (_pHandler) _pHandler->startObject();\n \t\t\thandleObject();\ndiff --git a/base/poco/JSON/src/pdjson.c b/base/poco/JSON/src/pdjson.c\nindex 563fa2774398..09a6afd7a318 100644\n--- a/base/poco/JSON/src/pdjson.c\n+++ b/base/poco/JSON/src/pdjson.c\n@@ -2,12 +2,15 @@\n #include <stdlib.h>\n #include <string.h>\n #include <ctype.h>\n+#include <limits.h>\n #include <errno.h>\n #include \"pdjson.h\"\n \n #define JSON_FLAG_ERROR      (1u << 0)\n #define JSON_FLAG_STREAMING  (1u << 1)\n \n+#define END_OF_SOURCE INT_MAX\n+\n \n // patched for poco 1.8.x (VS 2008)\n #if defined(_MSC_VER) && (_MSC_VER < 1900)\n@@ -106,7 +109,7 @@ static int buffer_peek(struct json_source *source)\n     if (source->position < source->source.buffer.length)\n         return source->source.buffer.buffer[source->position];\n     else\n-        return EOF;\n+        return END_OF_SOURCE;\n }\n \n static int buffer_get(struct json_source *source)\n@@ -258,7 +261,7 @@ read_unicode_cp(json_stream *json)\n         int c = json->source.get(&json->source);\n         int hc;\n \n-        if (c == EOF) {\n+        if (c == END_OF_SOURCE) {\n             json_error(json, \"%s\", \"unterminated string literal in unicode\");\n             return -1;\n         } else if ((hc = hexchar(c)) == -1) {\n@@ -289,7 +292,7 @@ static int read_unicode(json_stream *json)\n         h = cp;\n \n         int c = json->source.get(&json->source);\n-        if (c == EOF) {\n+        if (c == END_OF_SOURCE) {\n             json_error(json, \"%s\", \"unterminated string literal in unicode\");\n             return -1;\n         } else if (c != '\\\\') {\n@@ -299,7 +302,7 @@ static int read_unicode(json_stream *json)\n         }\n \n         c = json->source.get(&json->source);\n-        if (c == EOF) {\n+        if (c == END_OF_SOURCE) {\n             json_error(json, \"%s\", \"unterminated string literal in unicode\");\n             return -1;\n         } else if (c != 'u') {\n@@ -330,7 +333,7 @@ static int read_unicode(json_stream *json)\n int read_escaped(json_stream *json)\n {\n     int c = json->source.get(&json->source);\n-    if (c == EOF) {\n+    if (c == END_OF_SOURCE) {\n         json_error(json, \"%s\", \"unterminated string literal in escape\");\n         return -1;\n     } else if (c == 'u') {\n@@ -361,128 +364,6 @@ int read_escaped(json_stream *json)\n     return 0;\n }\n \n-static int\n-char_needs_escaping(int c)\n-{\n-    if ((c >= 0) && (c < 0x20 || c == 0x22 || c == 0x5c)) {\n-        return 1;\n-    }\n-\n-    return 0;\n-}\n-\n-static int\n-utf8_seq_length(char byte)\n-{\n-    unsigned char u = (unsigned char) byte;\n-    if (u < 0x80) return 1;\n-\n-    if (0x80 <= u && u <= 0xBF)\n-    {\n-        // second, third or fourth byte of a multi-byte\n-        // sequence, i.e. a \"continuation byte\"\n-        return 0;\n-    }\n-    else if (u == 0xC0 || u == 0xC1)\n-    {\n-        // overlong encoding of an ASCII byte\n-        return 0;\n-    }\n-    else if (0xC2 <= u && u <= 0xDF)\n-    {\n-        // 2-byte sequence\n-        return 2;\n-    }\n-    else if (0xE0 <= u && u <= 0xEF)\n-    {\n-        // 3-byte sequence\n-        return 3;\n-    }\n-    else if (0xF0 <= u && u <= 0xF4)\n-    {\n-        // 4-byte sequence\n-        return 4;\n-    }\n-    else\n-    {\n-        // u >= 0xF5\n-        // Restricted (start of 4-, 5- or 6-byte sequence) or invalid UTF-8\n-        return 0;\n-    }\n-}\n-\n-static int\n-is_legal_utf8(const unsigned char *bytes, int length)\n-{\n-    if (0 == bytes || 0 == length) return 0;\n-\n-    unsigned char a;\n-    const unsigned char* srcptr = bytes + length;\n-    switch (length)\n-    {\n-    default:\n-        return 0;\n-        // Everything else falls through when true.\n-    case 4:\n-        if ((a = (*--srcptr)) < 0x80 || a > 0xBF) return 0;\n-    case 3:\n-        if ((a = (*--srcptr)) < 0x80 || a > 0xBF) return 0;\n-    case 2:\n-        a = (*--srcptr);\n-        switch (*bytes)\n-        {\n-        case 0xE0:\n-            if (a < 0xA0 || a > 0xBF) return 0;\n-            break;\n-        case 0xED:\n-            if (a < 0x80 || a > 0x9F) return 0;\n-            break;\n-        case 0xF0:\n-            if (a < 0x90 || a > 0xBF) return 0;\n-            break;\n-        case 0xF4:\n-            if (a < 0x80 || a > 0x8F) return 0;\n-            break;\n-        default:\n-            if (a < 0x80 || a > 0xBF) return 0;\n-        }\n-    case 1:\n-        if (*bytes >= 0x80 && *bytes < 0xC2) return 0;\n-    }\n-    return *bytes <= 0xF4;\n-}\n-\n-static int\n-read_utf8(json_stream* json, int next_char)\n-{\n-    int count = utf8_seq_length(next_char);\n-    if (!count)\n-    {\n-        json_error(json, \"%s\", \"Bad character.\");\n-        return -1;\n-    }\n-\n-    char buffer[4];\n-    buffer[0] = next_char;\n-    for (int i = 1; i < count; ++i)\n-    {\n-        buffer[i] = json->source.get(&json->source);;\n-    }\n-\n-    if (!is_legal_utf8((unsigned char*) buffer, count))\n-    {\n-        json_error(json, \"%s\", \"No legal UTF8 found\");\n-        return -1;\n-    }\n-\n-    for (int i = 0; i < count; ++i)\n-    {\n-        if (pushchar(json, buffer[i]) != 0)\n-            return -1;\n-    }\n-    return 0;\n-}\n-\n static enum json_type\n read_string(json_stream *json)\n {\n@@ -490,31 +371,19 @@ read_string(json_stream *json)\n         return JSON_ERROR;\n     while (1) {\n         int c = json->source.get(&json->source);\n-        if (c == EOF) {\n+        if (c == END_OF_SOURCE) {\n             json_error(json, \"%s\", \"unterminated string literal\");\n             return JSON_ERROR;\n         } else if (c == '\"') {\n-            if (pushchar(json, '\\0') == 0)\n-                return JSON_STRING;\n-            else\n-                return JSON_ERROR;\n+            return JSON_STRING;\n         } else if (c == '\\\\') {\n             if (read_escaped(json) != 0)\n                 return JSON_ERROR;\n-        } else if ((unsigned) c >= 0x80) {\n-            if (read_utf8(json, c) != 0)\n-                return JSON_ERROR;\n         } else {\n-            if (char_needs_escaping(c)) {\n-                json_error(json, \"%s\", \"unescaped control character in string\");\n-                return JSON_ERROR;\n-            }\n-\n             if (pushchar(json, c) != 0)\n                 return JSON_ERROR;\n         }\n     }\n-    return JSON_ERROR;\n }\n \n static int\n@@ -563,10 +432,7 @@ read_number(json_stream *json, int c)\n     /* Up to decimal or exponent has been read. */\n     c = json->source.peek(&json->source);\n     if (strchr(\".eE\", c) == NULL) {\n-        if (pushchar(json, '\\0') != 0)\n-            return JSON_ERROR;\n-        else\n-            return JSON_NUMBER;\n+        return JSON_NUMBER;\n     }\n     if (c == '.') {\n         json->source.get(&json->source); // consume .\n@@ -596,10 +462,7 @@ read_number(json_stream *json, int c)\n             return JSON_ERROR;\n         }\n     }\n-    if (pushchar(json, '\\0') != 0)\n-        return JSON_ERROR;\n-    else\n-        return JSON_NUMBER;\n+    return JSON_NUMBER;\n }\n \n static int\n@@ -631,7 +494,7 @@ read_value(json_stream *json, int c)\n {\n     json->ntokens++;\n     switch (c) {\n-    case EOF:\n+    case END_OF_SOURCE:\n         json_error(json, \"%s\", \"unexpected end of data\");\n         return JSON_ERROR;\n     case '{':\n@@ -692,7 +555,7 @@ enum json_type json_next(json_stream *json)\n             }\n         } while (json_isspace(c));\n \n-        if (!(json->flags & JSON_FLAG_STREAMING) && c != EOF) {\n+        if (!(json->flags & JSON_FLAG_STREAMING) && c != END_OF_SOURCE) {\n             return JSON_ERROR;\n         }\n \n@@ -774,8 +637,7 @@ void json_reset(json_stream *json)\n \n const char *json_get_string(json_stream *json, size_t *length)\n {\n-    if (length != NULL)\n-        *length = json->data.string_fill;\n+    *length = json->data.string_fill;\n     if (json->data.string == NULL)\n         return \"\";\n     else\ndiff --git a/src/Access/DiskAccessStorage.cpp b/src/Access/DiskAccessStorage.cpp\nindex a9c696617253..0c974618c431 100644\n--- a/src/Access/DiskAccessStorage.cpp\n+++ b/src/Access/DiskAccessStorage.cpp\n@@ -14,7 +14,6 @@\n #include <Poco/JSON/Stringifier.h>\n #include <boost/algorithm/string/case_conv.hpp>\n #include <boost/range/adaptor/map.hpp>\n-#include <boost/range/algorithm/copy.hpp>\n #include <base/range.h>\n #include <filesystem>\n #include <fstream>\n",
  "test_patch": "diff --git a/src/Common/tests/gtest_poco_json.cpp b/src/Common/tests/gtest_poco_json.cpp\nnew file mode 100644\nindex 000000000000..7070e8a8673b\n--- /dev/null\n+++ b/src/Common/tests/gtest_poco_json.cpp\n@@ -0,0 +1,50 @@\n+#include <gtest/gtest.h>\n+\n+#include <Poco/JSON/JSON.h>\n+#include <Poco/JSON/Object.h>\n+#include <Poco/JSON/Parser.h>\n+#include <Poco/JSON/Stringifier.h>\n+\n+#include <Common/Exception.h>\n+\n+using namespace std::literals;\n+\n+\n+TEST(PocoJSON, roundtrip)\n+{\n+    /** We patched Poco library to support certain invalid JSONs\n+      * in favor of perfect roundtrip of binary data, including zero bytes and invalid UTF-8.\n+      *\n+      * This is needed for consistency with ClickHouse's JSONEachRow format,\n+      * and to allow storing SQL queries (which can contain binary data) inside serialized JSONs\n+      * without extra encoding.\n+      *\n+      * Keep in mind that binary data inside string literals still has to be escaped, at least characters \\ and \"\n+      */\n+    try\n+    {\n+        std::string source_str(\"{\\\"hello\\0\u028f\u146b\u1608\u1d86\u1d0b\u1fb0\\\\\\\"\\\": \\\"world\\\\n\\\\t\\\\r\u15b4\u160d\u1390\u166e\u15dd\u1fb4\\xFFwtf\\xAA\\xBB\\xCC\\xDD\\\"}\"sv);\n+        std::string formatted_str(\"{\\\"hello\\\\u0000\u028f\u146b\u1608\u1d86\u1d0b\u1fb0\\\\\\\"\\\":\\\"world\\\\n\\\\t\\\\r\u15b4\u160d\u1390\u166e\u15dd\u1fb4\\xFFwtf\\xAA\\xBB\\xCC\\xDD\\\"}\"sv);\n+\n+        Poco::JSON::Parser parser;\n+        Poco::Dynamic::Var res_json = parser.parse(source_str);\n+        const Poco::JSON::Object::Ptr & object = res_json.extract<Poco::JSON::Object::Ptr>();\n+\n+        std::stringstream destination;\n+        Poco::JSON::Stringifier::stringify(*object, destination);\n+\n+        EXPECT_EQ(formatted_str, destination.str());\n+\n+        Poco::Dynamic::Var res_json2 = parser.parse(destination.str());\n+        const Poco::JSON::Object::Ptr & object2 = res_json.extract<Poco::JSON::Object::Ptr>();\n+\n+        std::stringstream destination2;\n+        Poco::JSON::Stringifier::stringify(*object2, destination2);\n+\n+        EXPECT_EQ(destination.str(), destination2.str());\n+    }\n+    catch (...)\n+    {\n+        std::cerr << DB::getCurrentExceptionMessage(true) << \"\\n\";\n+    }\n+}\ndiff --git a/tests/queries/0_stateless/03356_tables_with_binary_identifiers_invalid_utf8.reference b/tests/queries/0_stateless/03356_tables_with_binary_identifiers_invalid_utf8.reference\nnew file mode 100644\nindex 000000000000..3e67fe1ac4f3\n--- /dev/null\n+++ b/tests/queries/0_stateless/03356_tables_with_binary_identifiers_invalid_utf8.reference\n@@ -0,0 +1,2 @@\n+123\n+123\ndiff --git a/tests/queries/0_stateless/03356_tables_with_binary_identifiers_invalid_utf8.sql b/tests/queries/0_stateless/03356_tables_with_binary_identifiers_invalid_utf8.sql\nnew file mode 100644\nindex 000000000000..a4257b3abad0\n--- /dev/null\n+++ b/tests/queries/0_stateless/03356_tables_with_binary_identifiers_invalid_utf8.sql\n@@ -0,0 +1,11 @@\n+-- Tags: no-random-merge-tree-settings\n+DROP TABLE IF EXISTS test;\n+CREATE TABLE test (`\\xFF\\0\u043f\u0440\u0438\u0432\u0435\u0442\ufffd\ufffd\ufffd` UInt8) ENGINE = MergeTree ORDER BY `\\xFF\\0\u043f\u0440\u0438\u0432\u0435\u0442\ufffd\ufffd\ufffd` COMMENT '\\0';\n+\n+INSERT INTO test VALUES (123);\n+SELECT * FROM test;\n+DETACH TABLE test;\n+ATTACH TABLE test;\n+\n+SELECT * FROM test;\n+DROP TABLE test;\n",
  "problem_statement": "Ensure that a valid utf8 string is used for the serialization json.\n<!---\r\nA technical comment, you are free to remove or leave it as it is when PR is created\r\nThe following categories are used in the next scripts, update them accordingly\r\nutils/changelog/changelog.py\r\ntests/ci/cancel_and_rerun_workflow_lambda/app.py\r\n-->\r\n### Changelog category (leave one):\r\n- Improvement\r\n\r\n\r\n\r\n### Changelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nEnsure that a valid utf8 string is used for serialization json.\r\n\r\n### Documentation entry for user-facing changes\r\n\r\n- [ ] Documentation is written (mandatory for new features)\r\n\r\n### Motivation\r\n\r\nBy now if the user used non-utf8 characters in the column name (with `Create` or `Alter`) then after restart, all parts with such column in `serialization.json` file will be `broken_on_start`,  so user should manually recover these parts by removing columns and moving parts from detached.\r\n\r\nAs @aalexfvk suggested (#72281), we got 2 options: prevent using non-utf8 characters or support other encodings with escaping methods. I believe that the first option is preferable, because\r\nhttps://clickhouse.com/docs/en/install#launch\r\n```\r\nThe terminal must use UTF-8 encoding.\r\n```\r\nhttps://clickhouse.com/docs/en/development/style#how-to-write-code\r\n```\r\nUse UTF-8 everywhere. Use std::string and char *. Do not use std::wstring and wchar_t. \r\n```\r\n\r\n~Actually the PR won't resolve the problem fully. But it should protect from creating the parts that will be broken after restarting the server and make the issue visible for users. \r\nWe will also make a separate with validating column names for other entities,  but it seems that at the moment the problem can only occur with objects that use json files inside.~\r\n\r\nUPD. \r\nThe PR introduces two restrictions:\r\n1. Do not allow `create  table` with non utf symbols in the column name or `alter table` which adds such columns in the existing table.\r\n2. Do not allow to insert new data in the table with invalid serialization json in parts, if such already exists. \r\n\r\nRelated issues: https://github.com/ClickHouse/ClickHouse/issues/69329 https://github.com/ClickHouse/ClickHouse/issues/72281\r\n<!---\r\nDirectly edit documentation source files in the \"docs\" folder with the same pull-request as code changes\r\n\r\nor\r\n\r\nAdd a user-readable short description of the changes that should be added to docs.clickhouse.com below.\r\n\r\nAt a minimum, the following information should be added (but add more as needed).\r\n- Motivation: Why is this function, table engine, etc. useful to ClickHouse users?\r\n\r\n- Parameters: If the feature being added takes arguments, options or is influenced by settings, please list them below with a brief explanation.\r\n\r\n- Example use: A query or command.\r\n-->\r\n\r\n\r\n> Information about CI checks: https://clickhouse.com/docs/en/development/continuous-integration/\r\n\r\n#### CI Settings (Only check the boxes if you know what you are doing):\r\n- [ ] <!---ci_set_required--> Allow: All Required Checks\r\n- [ ] <!---ci_include_stateless--> Allow: Stateless tests\r\n- [ ] <!---ci_include_stateful--> Allow: Stateful tests\r\n- [ ] <!---ci_include_integration--> Allow: Integration Tests\r\n- [ ] <!---ci_include_performance--> Allow: Performance tests\r\n- [ ] <!---ci_set_builds--> Allow: All Builds\r\n- [ ] <!---batch_0_1--> Allow: batch 1, 2 for multi-batch jobs\r\n- [ ] <!---batch_2_3--> Allow: batch 3, 4, 5, 6 for multi-batch jobs\r\n---\r\n- [ ] <!---ci_exclude_style--> Exclude: Style check\r\n- [ ] <!---ci_exclude_fast--> Exclude: Fast test\r\n- [ ] <!---ci_exclude_asan--> Exclude: All with ASAN\r\n- [ ] <!---ci_exclude_tsan|msan|ubsan|coverage--> Exclude: All with TSAN, MSAN, UBSAN, Coverage\r\n- [ ] <!---ci_exclude_aarch64|release|debug--> Exclude: All with aarch64, release, debug\r\n---\r\n- [ ] <!---ci_include_fuzzer--> Run only fuzzers related jobs (libFuzzer fuzzers, AST fuzzers, etc.)\r\n- [ ] <!---ci_exclude_ast--> Exclude: AST fuzzers\r\n---\r\n- [ ] <!---do_not_test--> Do not test\r\n- [ ] <!---woolen_wolfdog--> Woolen Wolfdog\r\n- [ ] <!---upload_all--> Upload binaries for special builds\r\n- [ ] <!---no_merge_commit--> Disable merge-commit\r\n- [ ] <!---no_ci_cache--> Disable CI cache\r\n\n",
  "hints_text": "<!-- automatic status comment for PR #73668 from MikhailBurdukov/ClickHouse:validate_serialization_json -->\n*This is an automated comment for commit 917c4ec08d25c08821ecb6cfabf0c77b245ca00a with description of existing statuses. It's updated for the latest CI running*\n\n[\u2705 Click here](https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/ci_running.html) to open a full report in a separate page\n\n<details><summary>Successful checks</summary>\n<table>\n<thead><tr><th>Check name</th><th>Description</th><th>Status</th></tr></thead>\n<tbody>\n<tr><td>AST fuzzer</td><td>Runs randomly generated queries to catch program errors. The build type is optionally given in parenthesis. If it fails, ask a maintainer for help</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/ast_fuzzer__asan_.html\">\u2705 success</a></td></tr>\n<tr><td>Builds</td><td>There's no description for the check yet, please add it to tests/ci/ci_config.py:CHECK_DESCRIPTIONS</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/builds/report.html\">\u2705 success</a></td></tr>\n<tr><td>ClickBench</td><td>Runs [ClickBench](https://github.com/ClickHouse/ClickBench/) with instant-attach table</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/clickbench__aarch64_.html\">\u2705 success</a></td></tr>\n<tr><td>Compatibility check</td><td>Checks that <b>clickhouse</b> binary runs on distributions with old libc versions. If it fails, ask a maintainer for help</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/compatibility_check__aarch64_.html\">\u2705 success</a></td></tr>\n<tr><td>Docker keeper image</td><td>The check to build and optionally push the mentioned image to docker hub</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/docker_keeper_image.html\">\u2705 success</a></td></tr>\n<tr><td>Docker server image</td><td>The check to build and optionally push the mentioned image to docker hub</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/docker_server_image.html\">\u2705 success</a></td></tr>\n<tr><td>Docs check</td><td>Builds and tests the documentation</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/0/a9c1f9e08c236e9ad30d694d9daa970b526d08d7/docs_check.html\">\u2705 success</a></td></tr>\n<tr><td>Fast test</td><td>Normally this is the first check that is ran for a PR. It builds ClickHouse and runs most of <a href=\"https://clickhouse.com/docs/en/development/tests#functional-tests\">stateless functional tests</a>, omitting some. If it fails, further checks are not started until it is fixed. Look at the report to see which tests fail, then reproduce the failure locally as described <a href=\"https://clickhouse.com/docs/en/development/tests#functional-test-locally\">here</a></td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/fast_test.html\">\u2705 success</a></td></tr>\n<tr><td>Flaky tests</td><td>Checks if new added or modified tests are flaky by running them repeatedly, in parallel, with more randomization. Functional tests are run 100 times with address sanitizer, and additional randomization of thread scheduling. Integration tests are run up to 10 times. If at least once a new test has failed, or was too long, this check will be red. We don't allow flaky tests, read <a href=\"https://clickhouse.com/blog/decorating-a-christmas-tree-with-the-help-of-flaky-tests/\">the doc</a></td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/integration_tests_flaky_check__asan_.html\">\u2705 success</a></td></tr>\n<tr><td>Install packages</td><td>Checks that the built packages are installable in a clear environment</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/install_packages__aarch64_.html\">\u2705 success</a></td></tr>\n<tr><td>Integration tests</td><td>The integration tests report. In parenthesis the package type is given, and in square brackets are the optional part/total tests</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/integration_tests__aarch64__[1_6].html\">\u2705 success</a></td></tr>\n<tr><td>Performance Comparison</td><td>Measure changes in query performance. The performance test report is described in detail <a href=\"https://github.com/ClickHouse/ClickHouse/tree/master/docker/test/performance-comparison#how-to-read-the-report\">here</a>. In square brackets are the optional part/total tests</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/performance_comparison__release__[1_4]/report.html\">\u2705 success</a></td></tr>\n<tr><td>Stateful tests</td><td>Runs stateful functional tests for ClickHouse binaries built in various configurations -- release, debug, with sanitizers, etc</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/stateful_tests__aarch64_.html\">\u2705 success</a></td></tr>\n<tr><td>Stateless tests</td><td>Runs stateless functional tests for ClickHouse binaries built in various configurations -- release, debug, with sanitizers, etc</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/stateless_tests__aarch64_.html\">\u2705 success</a></td></tr>\n<tr><td>Stress test</td><td>Runs stateless functional tests concurrently from several clients to detect concurrency-related errors</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/stress_test__debug_.html\">\u2705 success</a></td></tr>\n<tr><td>Style check</td><td>Runs a set of checks to keep the code style clean. If some of tests failed, see the related log from the report</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/style_check.html\">\u2705 success</a></td></tr>\n<tr><td>Unit tests</td><td>Runs the unit tests for different release types</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/unit_tests__asan_.html\">\u2705 success</a></td></tr>\n<tr><td>Upgrade check</td><td>Runs stress tests on server version from last release and then tries to upgrade it to the version from the PR. It checks if the new server can successfully startup without any errors, crashes or sanitizer asserts</td><td><a href=\"https://s3.amazonaws.com/clickhouse-test-reports/73668/917c4ec08d25c08821ecb6cfabf0c77b245ca00a/upgrade_check__asan_.html\">\u2705 success</a></td></tr>\n<tbody>\n</table>\n</details>\n\n@alexey-milovidov, @nikitamikhaylov, I like the idea of this PR but I am extremely worried about backward compatibility. WDYT? Is it ok to hope that the clients usually do not use non-utf symbols in their DDL queries? I do not think we want to break existing tables even though there is a directive to use utf-8 in the documentation  \nHi, @MikhailBurdukov. Could you please consider additional JSON string requirements for column_name validation? While UTF-8 validation is important, we should also handle control characters (0x00-0x1F) and proper escaping of quotes and backslashes to ensure proper JSON serialization. This would help prevent potential issues down the line.\n> I do not think we want to break existing tables even though there is a directive to use utf-8 in the documentation\r\n\r\nBut they seems already broken and the problem  are non-visible for users, parts will `broken_on_start` after restart. Suppose it could be critical issue and with this PR we are trying to notify users about it. It's better not to insert data at all than to insert it and then suddenly lose it.\r\n\r\n>  While UTF-8 validation is important, we should also handle control characters (0x00-0x1F) and proper escaping of quotes and backslashes to ensure proper JSON serialization.\r\n\r\nAgree we should handle it, but as for me it has nothing to do with this PR. Let's make it in separate PR? \r\n\r\n\n@MikhailBurdukov, we discussed your PR in our reliability meeting and reached the following conclusions:\r\n\r\n1) We'll focus first on UTF-8 encoding validation and address escape characters later\r\n2) We agree with forbidding non-UTF-8 encoding at table creation time\r\n3) This will be a forward-only change (I've removed the backport tags and marked it as backward-incompatible)\r\n4) The setting should be internal and not exposed to users. @nikitamikhaylov, @Algunenano - could you please advise on the best way to implement this?\r\n\r\nThe PR is ready to merge once we resolve the setting visibility issue, as all other issues are already addressed.\nClickHouse is always charset-agnostic - we allow arbitrary binary data in the String data type; - there is no special distinction for UTF-8. Compared to other programming languages (Python, Java, JavaScript, Perl, PHP, Rust) and data formats (Parquet, Arrow), this is the right decision. But there is a problem - somehow, we let JSON for certain metadata files, and because it does not support arbitrary binary data, this is a mistake. We should not have used JSON in the first place.\r\n\r\nThe solution should be to allow arbitrary binary data in the JSON files.\r\nFor example, if you want to write a string containing two 0x80 bytes in JSON, followed by `\"`, it will be serialized as follows:\r\n\r\n```\r\nmilovidov@milovidov-pc:~/Photo/Iceland$ ch -q \"SELECT '\\x80\\x80\\\"' AS x FORMAT JSONEachRow\" | xxd\r\n00000000: 7b22 7822 3a22 8080 5c22 227d 0a         {\"x\":\"..\\\"\"}.\r\n```\r\n\r\nIt should be able to deserialize back transparently.\nThe first step should be - to change JSON parsers for all internal metadata files so they will support arbitrary binary data.\r\nThe second step should be - to forbid creating columns with invalid UTF-8, because it will be a good restriction anyway.\r\n\n> The first step should be - to change JSON parsers for all internal metadata files so they will support arbitrary binary data.\r\n\r\nI don't think that this PR is about changing json metadata parsers. It is much more difficult than the initial fix. If done, this should be done in another one\r\n\r\n@nikitamikhaylov, @alexey-milovidov should we postpone merging this PR until all internal json metadata parsers are able to work with arbitrary binary data? I do not understand how this is a blocker for the current PR, tbh\nYes.\n> I do not understand how this is a blocker for the current PR, tbh\r\n\r\nI guess my vision is close to @divanik 's one\r\n\r\n>The first step should be - to change JSON parsers for all internal metadata files so they will support arbitrary binary data.\r\nThe second step should be - to forbid creating columns with invalid UTF-8, because it will be a good restriction anyway.\r\n\r\ndon't understand why we have to do in this particular order. In my opinion, these improvements don't really block each other.\r\nMoreover changing the metadata format can be a long-term task, but the problem can be really critical:  without restrictions, the user will lose all their data in cases where an encoding other than utf 8 is used. And all this time, while we are json is used for serialization, there will be a critical bug in the code.\r\n@alexey-milovidov \r\nCould you please share why we can't merge pr until the first step is completed? \r\n\r\n",
  "created_at": "2025-02-12T05:11:22Z",
  "modified_files": [
    "base/poco/JSON/src/ParserImpl.cpp",
    "base/poco/JSON/src/pdjson.c",
    "src/Access/DiskAccessStorage.cpp"
  ],
  "modified_test_files": [
    "b/src/Common/tests/gtest_poco_json.cpp",
    "b/tests/queries/0_stateless/03356_tables_with_binary_identifiers_invalid_utf8.reference",
    "b/tests/queries/0_stateless/03356_tables_with_binary_identifiers_invalid_utf8.sql"
  ]
}