{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 55893,
  "instance_id": "ClickHouse__ClickHouse-55893",
  "issue_numbers": [
    "55838"
  ],
  "base_commit": "3d0631a37da0b9a341d3cdd320aec563d31a82ef",
  "patch": "diff --git a/docs/en/operations/settings/settings.md b/docs/en/operations/settings/settings.md\nindex ccbf54843e4b..5c9472600b80 100644\n--- a/docs/en/operations/settings/settings.md\n+++ b/docs/en/operations/settings/settings.md\n@@ -4708,18 +4708,6 @@ SELECT toFloat64('1.7091'), toFloat64('1.5008753E7') SETTINGS precise_float_pars\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n ```\n \n-## partial_result_update_duration_ms\n-\n-Interval (in milliseconds) for sending updates with partial data about the result table to the client (in interactive mode) during query execution. Setting to 0 disables partial results. Only supported for single-threaded GROUP BY without key, ORDER BY, LIMIT and OFFSET.\n-\n-:::note\n-It's an experimental feature. Enable `allow_experimental_partial_result` setting first to use it.\n-:::\n-\n-## max_rows_in_partial_result\n-\n-Maximum rows to show in the partial result after every real-time update while the query runs (use partial result limit + OFFSET as a value in case of OFFSET in the query).\n-\n ## validate_tcp_client_information {#validate-tcp-client-information}\n \n Determines whether validation of client information enabled when query packet is received from a client using a TCP connection.\ndiff --git a/src/Client/ClientBase.cpp b/src/Client/ClientBase.cpp\nindex a350654cdda2..16229c5e44c6 100644\n--- a/src/Client/ClientBase.cpp\n+++ b/src/Client/ClientBase.cpp\n@@ -449,20 +449,7 @@ void ClientBase::onData(Block & block, ASTPtr parsed_query)\n     if (!block)\n         return;\n \n-    if (block.rows() == 0 && partial_result_mode == PartialResultMode::Active)\n-    {\n-        partial_result_mode = PartialResultMode::Inactive;\n-        if (is_interactive)\n-        {\n-            progress_indication.clearProgressOutput(*tty_buf);\n-            std::cout << \"Full result:\" << std::endl;\n-            progress_indication.writeProgress(*tty_buf);\n-        }\n-    }\n-\n-    if (partial_result_mode == PartialResultMode::Inactive)\n-        processed_rows += block.rows();\n-\n+    processed_rows += block.rows();\n     /// Even if all blocks are empty, we still need to initialize the output stream to write empty resultset.\n     initOutputFormat(block, parsed_query);\n \n@@ -472,20 +459,13 @@ void ClientBase::onData(Block & block, ASTPtr parsed_query)\n     if (block.rows() == 0 || (query_fuzzer_runs != 0 && processed_rows >= 100))\n         return;\n \n-    if (!is_interactive && partial_result_mode == PartialResultMode::Active)\n-        return;\n-\n     /// If results are written INTO OUTFILE, we can avoid clearing progress to avoid flicker.\n     if (need_render_progress && tty_buf && (!select_into_file || select_into_file_and_stdout))\n         progress_indication.clearProgressOutput(*tty_buf);\n \n     try\n     {\n-        if (partial_result_mode == PartialResultMode::Active)\n-            output_format->writePartialResult(materializeBlock(block));\n-        else\n-            output_format->write(materializeBlock(block));\n-\n+        output_format->write(materializeBlock(block));\n         written_first_block = true;\n     }\n     catch (const Exception &)\n@@ -549,9 +529,6 @@ void ClientBase::onProfileInfo(const ProfileInfo & profile_info)\n void ClientBase::initOutputFormat(const Block & block, ASTPtr parsed_query)\n try\n {\n-    if (partial_result_mode == PartialResultMode::NotInit)\n-        partial_result_mode = PartialResultMode::Active;\n-\n     if (!output_format)\n     {\n         /// Ignore all results when fuzzing as they can be huge.\n@@ -994,14 +971,6 @@ void ClientBase::processOrdinaryQuery(const String & query_to_execute, ASTPtr pa\n \n     const auto & settings = global_context->getSettingsRef();\n     const Int32 signals_before_stop = settings.partial_result_on_first_cancel ? 2 : 1;\n-    bool has_partial_result_setting = settings.partial_result_update_duration_ms.totalMilliseconds() > 0;\n-\n-    if (has_partial_result_setting)\n-    {\n-        partial_result_mode = PartialResultMode::NotInit;\n-        if (is_interactive)\n-            std::cout << \"Partial result:\" << std::endl;\n-    }\n \n     int retries_left = 10;\n     while (retries_left)\n@@ -1828,7 +1797,6 @@ void ClientBase::processParsedSingleQuery(const String & full_query, const Strin\n     }\n \n     processed_rows = 0;\n-    partial_result_mode = PartialResultMode::Inactive;\n     written_first_block = false;\n     progress_indication.resetProgress();\n     profile_events.watch.restart();\ndiff --git a/src/Client/ClientBase.h b/src/Client/ClientBase.h\nindex 604c8cf4d5c2..2156aae71817 100644\n--- a/src/Client/ClientBase.h\n+++ b/src/Client/ClientBase.h\n@@ -275,21 +275,6 @@ class ClientBase : public Poco::Util::Application, public IHints<2>\n     size_t processed_rows = 0; /// How many rows have been read or written.\n     bool print_num_processed_rows = false; /// Whether to print the number of processed rows at\n \n-    enum class PartialResultMode: UInt8\n-    {\n-        /// Query doesn't show partial result before the first block with 0 rows.\n-        /// The first block with 0 rows initializes the output table format using its header.\n-        NotInit,\n-\n-        /// Query shows partial result after the first and before the second block with 0 rows.\n-        /// The second block with 0 rows indicates that that receiving blocks with partial result has been completed and next blocks will be with the full result.\n-        Active,\n-\n-        /// Query doesn't show partial result at all.\n-        Inactive,\n-    };\n-    PartialResultMode partial_result_mode = PartialResultMode::Inactive;\n-\n     bool print_stack_trace = false;\n     /// The last exception that was received from the server. Is used for the\n     /// return code in batch mode.\ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex b58705614d8e..f1b031eaf938 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -314,10 +314,6 @@ class IColumn;\n     \\\n     M(Bool, partial_result_on_first_cancel, false, \"Allows query to return a partial result after cancel.\", 0) \\\n     \\\n-    M(Bool, allow_experimental_partial_result, 0, \"Enable experimental feature: partial results for running queries.\", 0) \\\n-    M(Milliseconds, partial_result_update_duration_ms, 0, \"Interval (in milliseconds) for sending updates with partial data about the result table to the client (in interactive mode) during query execution. Setting to 0 disables partial results. Only supported for single-threaded GROUP BY without key, ORDER BY, LIMIT and OFFSET.\", 0) \\\n-    M(UInt64, max_rows_in_partial_result, 10, \"Maximum rows to show in the partial result after every real-time update while the query runs (use partial result limit + OFFSET as a value in case of OFFSET in the query).\", 0) \\\n-    \\\n     M(Bool, ignore_on_cluster_for_replicated_udf_queries, false, \"Ignore ON CLUSTER clause for replicated UDF management queries.\", 0) \\\n     M(Bool, ignore_on_cluster_for_replicated_access_entities_queries, false, \"Ignore ON CLUSTER clause for replicated access entities management queries.\", 0) \\\n     /** Settings for testing hedged requests */ \\\ndiff --git a/src/Interpreters/Aggregator.cpp b/src/Interpreters/Aggregator.cpp\nindex e16064db713b..ac0ebc44bec1 100644\n--- a/src/Interpreters/Aggregator.cpp\n+++ b/src/Interpreters/Aggregator.cpp\n@@ -2334,29 +2334,6 @@ Block Aggregator::prepareBlockAndFillWithoutKey(AggregatedDataVariants & data_va\n     return block;\n }\n \n-Block Aggregator::prepareBlockAndFillWithoutKeySnapshot(AggregatedDataVariants & data_variants) const\n-{\n-    size_t rows = 1;\n-    bool final = true;\n-\n-    auto && out_cols\n-        = prepareOutputBlockColumns(params, aggregate_functions, getHeader(final), data_variants.aggregates_pools, final, rows);\n-    auto && [key_columns, raw_key_columns, aggregate_columns, final_aggregate_columns, aggregate_columns_data] = out_cols;\n-\n-    AggregatedDataWithoutKey & data = data_variants.without_key;\n-\n-    /// Always single-thread. It's safe to pass current arena from 'aggregates_pool'.\n-    for (size_t insert_i = 0; insert_i < params.aggregates_size; ++insert_i)\n-        aggregate_functions[insert_i]->insertResultInto(\n-            data + offsets_of_aggregate_states[insert_i],\n-            *final_aggregate_columns[insert_i],\n-            data_variants.aggregates_pool);\n-\n-    Block block = finalizeBlock(params, getHeader(final), std::move(out_cols), final, rows);\n-\n-    return block;\n-}\n-\n template <bool return_single_block>\n Aggregator::ConvertToBlockRes<return_single_block>\n Aggregator::prepareBlockAndFillSingleLevel(AggregatedDataVariants & data_variants, bool final) const\ndiff --git a/src/Interpreters/Aggregator.h b/src/Interpreters/Aggregator.h\nindex 4acf73ce50fe..ab53f76d2cec 100644\n--- a/src/Interpreters/Aggregator.h\n+++ b/src/Interpreters/Aggregator.h\n@@ -1217,7 +1217,6 @@ class Aggregator final\n     friend class ConvertingAggregatedToChunksSource;\n     friend class ConvertingAggregatedToChunksWithMergingSource;\n     friend class AggregatingInOrderTransform;\n-    friend class AggregatingPartialResultTransform;\n \n     /// Data structure of source blocks.\n     Block header;\n@@ -1402,7 +1401,6 @@ class Aggregator final\n         std::atomic<bool> * is_cancelled = nullptr) const;\n \n     Block prepareBlockAndFillWithoutKey(AggregatedDataVariants & data_variants, bool final, bool is_overflows) const;\n-    Block prepareBlockAndFillWithoutKeySnapshot(AggregatedDataVariants & data_variants) const;\n     BlocksList prepareBlocksAndFillTwoLevel(AggregatedDataVariants & data_variants, bool final, ThreadPool * thread_pool) const;\n \n     template <bool return_single_block>\ndiff --git a/src/Processors/Chunk.cpp b/src/Processors/Chunk.cpp\nindex cd442085eca1..3839a8963b24 100644\n--- a/src/Processors/Chunk.cpp\n+++ b/src/Processors/Chunk.cpp\n@@ -14,8 +14,7 @@ namespace ErrorCodes\n     extern const int POSITION_OUT_OF_BOUND;\n }\n \n-Chunk::Chunk(DB::Columns columns_, UInt64 num_rows_)\n-    : columns(std::move(columns_)), num_rows(num_rows_)\n+Chunk::Chunk(DB::Columns columns_, UInt64 num_rows_) : columns(std::move(columns_)), num_rows(num_rows_)\n {\n     checkNumRowsIsConsistent();\n }\ndiff --git a/src/Processors/Executors/CompletedPipelineExecutor.cpp b/src/Processors/Executors/CompletedPipelineExecutor.cpp\nindex c30586e194e6..598a51bf0c75 100644\n--- a/src/Processors/Executors/CompletedPipelineExecutor.cpp\n+++ b/src/Processors/Executors/CompletedPipelineExecutor.cpp\n@@ -75,7 +75,7 @@ void CompletedPipelineExecutor::execute()\n     if (interactive_timeout_ms)\n     {\n         data = std::make_unique<Data>();\n-        data->executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element, pipeline.partial_result_duration_ms);\n+        data->executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element);\n         data->executor->setReadProgressCallback(pipeline.getReadProgressCallback());\n \n         /// Avoid passing this to lambda, copy ptr to data instead.\n@@ -105,7 +105,7 @@ void CompletedPipelineExecutor::execute()\n     }\n     else\n     {\n-        PipelineExecutor executor(pipeline.processors, pipeline.process_list_element, pipeline.partial_result_duration_ms);\n+        PipelineExecutor executor(pipeline.processors, pipeline.process_list_element);\n         executor.setReadProgressCallback(pipeline.getReadProgressCallback());\n         executor.execute(pipeline.getNumThreads(), pipeline.getConcurrencyControl());\n     }\ndiff --git a/src/Processors/Executors/ExecutingGraph.cpp b/src/Processors/Executors/ExecutingGraph.cpp\nindex 6a946b4a4b9b..27f6a454b244 100644\n--- a/src/Processors/Executors/ExecutingGraph.cpp\n+++ b/src/Processors/Executors/ExecutingGraph.cpp\n@@ -260,6 +260,7 @@ bool ExecutingGraph::updateNode(uint64_t pid, Queue & queue, Queue & async_queue\n         {\n             pid = updated_processors.top();\n             updated_processors.pop();\n+\n             /// In this method we have ownership on node.\n             auto & node = *nodes[pid];\n \ndiff --git a/src/Processors/Executors/ExecutionThreadContext.h b/src/Processors/Executors/ExecutionThreadContext.h\nindex 85788a707714..eb048f8ab091 100644\n--- a/src/Processors/Executors/ExecutionThreadContext.h\n+++ b/src/Processors/Executors/ExecutionThreadContext.h\n@@ -30,12 +30,6 @@ class ExecutionThreadContext\n     /// Callback for read progress.\n     ReadProgressCallback * read_progress_callback = nullptr;\n \n-    /// Timer that stops optimization of running local tasks instead of queuing them.\n-    /// It provides local progress for each IProcessor task, allowing the partial result of the request to be always sended to the user.\n-    Stopwatch watch;\n-    /// Time period that limits the maximum allowed duration for optimizing the scheduling of local tasks within the executor\n-    const UInt64 partial_result_duration_ms;\n-\n public:\n #ifndef NDEBUG\n     /// Time for different processing stages.\n@@ -68,13 +62,8 @@ class ExecutionThreadContext\n     void setException(std::exception_ptr exception_) { exception = exception_; }\n     void rethrowExceptionIfHas();\n \n-    bool needWatchRestartForPartialResultProgress() { return partial_result_duration_ms != 0 && partial_result_duration_ms < watch.elapsedMilliseconds(); }\n-    void restartWatch() { watch.restart(); }\n-\n-    explicit ExecutionThreadContext(size_t thread_number_, bool profile_processors_, bool trace_processors_, ReadProgressCallback * callback, UInt64 partial_result_duration_ms_)\n+    explicit ExecutionThreadContext(size_t thread_number_, bool profile_processors_, bool trace_processors_, ReadProgressCallback * callback)\n         : read_progress_callback(callback)\n-        , watch(CLOCK_MONOTONIC)\n-        , partial_result_duration_ms(partial_result_duration_ms_)\n         , thread_number(thread_number_)\n         , profile_processors(profile_processors_)\n         , trace_processors(trace_processors_)\ndiff --git a/src/Processors/Executors/ExecutorTasks.cpp b/src/Processors/Executors/ExecutorTasks.cpp\nindex 089205923911..e61d225a968c 100644\n--- a/src/Processors/Executors/ExecutorTasks.cpp\n+++ b/src/Processors/Executors/ExecutorTasks.cpp\n@@ -108,15 +108,8 @@ void ExecutorTasks::pushTasks(Queue & queue, Queue & async_queue, ExecutionThrea\n {\n     context.setTask(nullptr);\n \n-    /// If sending partial results is allowed and local tasks scheduling optimization is repeated longer than the limit\n-    /// or new task need to send partial result later, skip optimization for this iteration.\n-    /// Otherwise take local task from queue if has one.\n-    if ((!queue.empty() && queue.front()->processor->isPartialResultProcessor())\n-        || context.needWatchRestartForPartialResultProgress())\n-    {\n-        context.restartWatch();\n-    }\n-    else if (!queue.empty() && !context.hasAsyncTasks())\n+    /// Take local task from queue if has one.\n+    if (!queue.empty() && !context.hasAsyncTasks())\n     {\n         context.setTask(queue.front());\n         queue.pop();\n@@ -146,7 +139,7 @@ void ExecutorTasks::pushTasks(Queue & queue, Queue & async_queue, ExecutionThrea\n     }\n }\n \n-void ExecutorTasks::init(size_t num_threads_, size_t use_threads_, bool profile_processors, bool trace_processors, ReadProgressCallback * callback, UInt64 partial_result_duration_ms)\n+void ExecutorTasks::init(size_t num_threads_, size_t use_threads_, bool profile_processors, bool trace_processors, ReadProgressCallback * callback)\n {\n     num_threads = num_threads_;\n     use_threads = use_threads_;\n@@ -158,7 +151,7 @@ void ExecutorTasks::init(size_t num_threads_, size_t use_threads_, bool profile_\n \n         executor_contexts.reserve(num_threads);\n         for (size_t i = 0; i < num_threads; ++i)\n-            executor_contexts.emplace_back(std::make_unique<ExecutionThreadContext>(i, profile_processors, trace_processors, callback, partial_result_duration_ms));\n+            executor_contexts.emplace_back(std::make_unique<ExecutionThreadContext>(i, profile_processors, trace_processors, callback));\n     }\n }\n \ndiff --git a/src/Processors/Executors/ExecutorTasks.h b/src/Processors/Executors/ExecutorTasks.h\nindex ab6d5e914119..d35f8de94d15 100644\n--- a/src/Processors/Executors/ExecutorTasks.h\n+++ b/src/Processors/Executors/ExecutorTasks.h\n@@ -58,7 +58,7 @@ class ExecutorTasks\n     void tryGetTask(ExecutionThreadContext & context);\n     void pushTasks(Queue & queue, Queue & async_queue, ExecutionThreadContext & context);\n \n-    void init(size_t num_threads_, size_t use_threads_, bool profile_processors, bool trace_processors, ReadProgressCallback * callback, UInt64 partial_result_duration_ms);\n+    void init(size_t num_threads_, size_t use_threads_, bool profile_processors, bool trace_processors, ReadProgressCallback * callback);\n     void fill(Queue & queue);\n     void upscale(size_t use_threads_);\n \ndiff --git a/src/Processors/Executors/PipelineExecutor.cpp b/src/Processors/Executors/PipelineExecutor.cpp\nindex 77779e2cec22..37af391fba31 100644\n--- a/src/Processors/Executors/PipelineExecutor.cpp\n+++ b/src/Processors/Executors/PipelineExecutor.cpp\n@@ -33,9 +33,8 @@ namespace ErrorCodes\n }\n \n \n-PipelineExecutor::PipelineExecutor(std::shared_ptr<Processors> & processors, QueryStatusPtr elem, UInt64 partial_result_duration_ms_)\n+PipelineExecutor::PipelineExecutor(std::shared_ptr<Processors> & processors, QueryStatusPtr elem)\n     : process_list_element(std::move(elem))\n-    , partial_result_duration_ms(partial_result_duration_ms_)\n {\n     if (process_list_element)\n     {\n@@ -329,7 +328,7 @@ void PipelineExecutor::initializeExecution(size_t num_threads, bool concurrency_\n     Queue queue;\n     graph->initializeExecution(queue);\n \n-    tasks.init(num_threads, use_threads, profile_processors, trace_processors, read_progress_callback.get(), partial_result_duration_ms);\n+    tasks.init(num_threads, use_threads, profile_processors, trace_processors, read_progress_callback.get());\n     tasks.fill(queue);\n \n     if (num_threads > 1)\ndiff --git a/src/Processors/Executors/PipelineExecutor.h b/src/Processors/Executors/PipelineExecutor.h\nindex 6cb0e6c4ac1e..dee12dad2829 100644\n--- a/src/Processors/Executors/PipelineExecutor.h\n+++ b/src/Processors/Executors/PipelineExecutor.h\n@@ -33,7 +33,7 @@ class PipelineExecutor\n     /// During pipeline execution new processors can appear. They will be added to existing set.\n     ///\n     /// Explicit graph representation is built in constructor. Throws if graph is not correct.\n-    explicit PipelineExecutor(std::shared_ptr<Processors> & processors, QueryStatusPtr elem, UInt64 partial_result_duration_ms_ = 0);\n+    explicit PipelineExecutor(std::shared_ptr<Processors> & processors, QueryStatusPtr elem);\n     ~PipelineExecutor();\n \n     /// Execute pipeline in multiple threads. Must be called once.\n@@ -90,9 +90,6 @@ class PipelineExecutor\n \n     ReadProgressCallbackPtr read_progress_callback;\n \n-    /// Duration between sending partial result through the pipeline\n-    const UInt64 partial_result_duration_ms;\n-\n     using Queue = std::queue<ExecutingGraph::Node *>;\n \n     void initializeExecution(size_t num_threads, bool concurrency_control); /// Initialize executor contexts and task_queue.\ndiff --git a/src/Processors/Executors/PullingAsyncPipelineExecutor.cpp b/src/Processors/Executors/PullingAsyncPipelineExecutor.cpp\nindex 95a2022bf936..345bec395b24 100644\n--- a/src/Processors/Executors/PullingAsyncPipelineExecutor.cpp\n+++ b/src/Processors/Executors/PullingAsyncPipelineExecutor.cpp\n@@ -41,13 +41,12 @@ struct PullingAsyncPipelineExecutor::Data\n     }\n };\n \n-PullingAsyncPipelineExecutor::PullingAsyncPipelineExecutor(QueryPipeline & pipeline_, bool has_partial_result_setting) : pipeline(pipeline_)\n+PullingAsyncPipelineExecutor::PullingAsyncPipelineExecutor(QueryPipeline & pipeline_) : pipeline(pipeline_)\n {\n     if (!pipeline.pulling())\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Pipeline for PullingAsyncPipelineExecutor must be pulling\");\n \n-    lazy_format = std::make_shared<LazyOutputFormat>(pipeline.output->getHeader(), /*is_partial_result_protocol_active*/ has_partial_result_setting);\n-\n+    lazy_format = std::make_shared<LazyOutputFormat>(pipeline.output->getHeader());\n     pipeline.complete(lazy_format);\n }\n \n@@ -104,7 +103,7 @@ bool PullingAsyncPipelineExecutor::pull(Chunk & chunk, uint64_t milliseconds)\n     if (!data)\n     {\n         data = std::make_unique<Data>();\n-        data->executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element, pipeline.partial_result_duration_ms);\n+        data->executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element);\n         data->executor->setReadProgressCallback(pipeline.getReadProgressCallback());\n         data->lazy_format = lazy_format.get();\n \ndiff --git a/src/Processors/Executors/PullingAsyncPipelineExecutor.h b/src/Processors/Executors/PullingAsyncPipelineExecutor.h\nindex 202ecbf281b1..361bcc0155c6 100644\n--- a/src/Processors/Executors/PullingAsyncPipelineExecutor.h\n+++ b/src/Processors/Executors/PullingAsyncPipelineExecutor.h\n@@ -21,7 +21,7 @@ struct ProfileInfo;\n class PullingAsyncPipelineExecutor\n {\n public:\n-    explicit PullingAsyncPipelineExecutor(QueryPipeline & pipeline_, bool has_partial_result_setting = false);\n+    explicit PullingAsyncPipelineExecutor(QueryPipeline & pipeline_);\n     ~PullingAsyncPipelineExecutor();\n \n     /// Get structure of returned block or chunk.\ndiff --git a/src/Processors/Executors/PullingPipelineExecutor.cpp b/src/Processors/Executors/PullingPipelineExecutor.cpp\nindex f79f15c19bfc..cbf73c5cb079 100644\n--- a/src/Processors/Executors/PullingPipelineExecutor.cpp\n+++ b/src/Processors/Executors/PullingPipelineExecutor.cpp\n@@ -44,7 +44,7 @@ bool PullingPipelineExecutor::pull(Chunk & chunk)\n {\n     if (!executor)\n     {\n-        executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element, pipeline.partial_result_duration_ms);\n+        executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element);\n         executor->setReadProgressCallback(pipeline.getReadProgressCallback());\n     }\n \ndiff --git a/src/Processors/Executors/PushingAsyncPipelineExecutor.cpp b/src/Processors/Executors/PushingAsyncPipelineExecutor.cpp\nindex f3ed24e7e961..a816ab9ca7f7 100644\n--- a/src/Processors/Executors/PushingAsyncPipelineExecutor.cpp\n+++ b/src/Processors/Executors/PushingAsyncPipelineExecutor.cpp\n@@ -167,7 +167,7 @@ void PushingAsyncPipelineExecutor::start()\n     started = true;\n \n     data = std::make_unique<Data>();\n-    data->executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element, pipeline.partial_result_duration_ms);\n+    data->executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element);\n     data->executor->setReadProgressCallback(pipeline.getReadProgressCallback());\n     data->source = pushing_source.get();\n \ndiff --git a/src/Processors/Executors/PushingPipelineExecutor.cpp b/src/Processors/Executors/PushingPipelineExecutor.cpp\nindex f2b018792c7f..696932932df5 100644\n--- a/src/Processors/Executors/PushingPipelineExecutor.cpp\n+++ b/src/Processors/Executors/PushingPipelineExecutor.cpp\n@@ -87,7 +87,7 @@ void PushingPipelineExecutor::start()\n         return;\n \n     started = true;\n-    executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element, pipeline.partial_result_duration_ms);\n+    executor = std::make_shared<PipelineExecutor>(pipeline.processors, pipeline.process_list_element);\n     executor->setReadProgressCallback(pipeline.getReadProgressCallback());\n \n     if (!executor->executeStep(&input_wait_flag))\ndiff --git a/src/Processors/Formats/IOutputFormat.cpp b/src/Processors/Formats/IOutputFormat.cpp\nindex e691e32a7bc8..88a6fb1e92fd 100644\n--- a/src/Processors/Formats/IOutputFormat.cpp\n+++ b/src/Processors/Formats/IOutputFormat.cpp\n@@ -1,90 +1,41 @@\n #include <Processors/Formats/IOutputFormat.h>\n #include <IO/WriteBuffer.h>\n-#include <IO/WriteHelpers.h>\n \n \n namespace DB\n {\n \n-IOutputFormat::IOutputFormat(const Block & header_, WriteBuffer & out_, bool is_partial_result_protocol_active_)\n-    : IProcessor({header_, header_, header_, header_}, {})\n-    , out(out_)\n-    , is_partial_result_protocol_active(is_partial_result_protocol_active_)\n+IOutputFormat::IOutputFormat(const Block & header_, WriteBuffer & out_)\n+    : IProcessor({header_, header_, header_}, {}), out(out_)\n {\n }\n \n-void IOutputFormat::setCurrentChunk(InputPort & input, PortKind kind)\n+IOutputFormat::Status IOutputFormat::prepare()\n {\n-    current_chunk = input.pull(true);\n-    current_block_kind = kind;\n-    has_input = true;\n-}\n+    if (has_input)\n+        return Status::Ready;\n \n-IOutputFormat::Status IOutputFormat::prepareMainAndPartialResult()\n-{\n-    bool need_data = false;\n-    for (auto kind : {Main, PartialResult})\n+    for (auto kind : {Main, Totals, Extremes})\n     {\n         auto & input = getPort(kind);\n \n-        if (input.isFinished())\n+        if (kind != Main && !input.isConnected())\n             continue;\n \n-        if (kind == PartialResult && main_input_activated)\n-        {\n-            input.close();\n+        if (input.isFinished())\n             continue;\n-        }\n \n         input.setNeeded();\n-        need_data = true;\n-\n-        if (!input.hasData())\n-            continue;\n-\n-        setCurrentChunk(input, kind);\n-        return Status::Ready;\n-    }\n-\n-    if (need_data)\n-        return Status::NeedData;\n-\n-    return Status::Finished;\n-}\n-\n-IOutputFormat::Status IOutputFormat::prepareTotalsAndExtremes()\n-{\n-    for (auto kind : {Totals, Extremes})\n-    {\n-        auto & input = getPort(kind);\n-\n-        if (!input.isConnected() || input.isFinished())\n-            continue;\n \n-        input.setNeeded();\n         if (!input.hasData())\n             return Status::NeedData;\n \n-        setCurrentChunk(input, kind);\n+        current_chunk = input.pull(true);\n+        current_block_kind = kind;\n+        has_input = true;\n         return Status::Ready;\n     }\n \n-    return Status::Finished;\n-}\n-\n-IOutputFormat::Status IOutputFormat::prepare()\n-{\n-    if (has_input)\n-        return Status::Ready;\n-\n-    auto status = prepareMainAndPartialResult();\n-    if (status != Status::Finished)\n-        return status;\n-\n-    status = prepareTotalsAndExtremes();\n-    if (status != Status::Finished)\n-        return status;\n-\n     finished = true;\n \n     if (!finalized)\n@@ -132,18 +83,8 @@ void IOutputFormat::work()\n         case Main:\n             result_rows += current_chunk.getNumRows();\n             result_bytes += current_chunk.allocatedBytes();\n-            if (is_partial_result_protocol_active && !main_input_activated && current_chunk.hasRows())\n-            {\n-                /// Sending an empty block signals to the client that partial results are terminated,\n-                /// and only data from the main pipeline will be forwarded.\n-                consume(Chunk(current_chunk.cloneEmptyColumns(), 0));\n-                main_input_activated = true;\n-            }\n             consume(std::move(current_chunk));\n             break;\n-        case PartialResult:\n-            consumePartialResult(std::move(current_chunk));\n-            break;\n         case Totals:\n             writeSuffixIfNeeded();\n             if (auto totals = prepareTotals(std::move(current_chunk)))\n@@ -178,15 +119,6 @@ void IOutputFormat::write(const Block & block)\n         flush();\n }\n \n-void IOutputFormat::writePartialResult(const Block & block)\n-{\n-    writePrefixIfNeeded();\n-    consumePartialResult(Chunk(block.getColumns(), block.rows()));\n-\n-    if (auto_flush)\n-        flush();\n-}\n-\n void IOutputFormat::finalize()\n {\n     if (finalized)\ndiff --git a/src/Processors/Formats/IOutputFormat.h b/src/Processors/Formats/IOutputFormat.h\nindex e642132fb646..cae2ab7691e6 100644\n--- a/src/Processors/Formats/IOutputFormat.h\n+++ b/src/Processors/Formats/IOutputFormat.h\n@@ -23,9 +23,9 @@ class WriteBuffer;\n class IOutputFormat : public IProcessor\n {\n public:\n-    enum PortKind { Main = 0, Totals = 1, Extremes = 2, PartialResult = 3 };\n+    enum PortKind { Main = 0, Totals = 1, Extremes = 2 };\n \n-    IOutputFormat(const Block & header_, WriteBuffer & out_, bool is_partial_result_protocol_active_ = false);\n+    IOutputFormat(const Block & header_, WriteBuffer & out_);\n \n     Status prepare() override;\n     void work() override;\n@@ -54,7 +54,6 @@ class IOutputFormat : public IProcessor\n     /// TODO: separate formats and processors.\n \n     void write(const Block & block);\n-    void writePartialResult(const Block & block);\n \n     void finalize();\n \n@@ -122,7 +121,6 @@ class IOutputFormat : public IProcessor\n     virtual void consume(Chunk) = 0;\n     virtual void consumeTotals(Chunk) {}\n     virtual void consumeExtremes(Chunk) {}\n-    virtual void consumePartialResult(Chunk) {}\n     virtual void finalizeImpl() {}\n     virtual void finalizeBuffers() {}\n     virtual void writePrefix() {}\n@@ -176,7 +174,6 @@ class IOutputFormat : public IProcessor\n \n     Chunk current_chunk;\n     PortKind current_block_kind = PortKind::Main;\n-    bool main_input_activated = false;\n     bool has_input = false;\n     bool finished = false;\n     bool finalized = false;\n@@ -191,15 +188,9 @@ class IOutputFormat : public IProcessor\n     Statistics statistics;\n \n private:\n-    void setCurrentChunk(InputPort & input, PortKind kind);\n-    IOutputFormat::Status prepareMainAndPartialResult();\n-    IOutputFormat::Status prepareTotalsAndExtremes();\n-\n     size_t rows_read_before = 0;\n     bool are_totals_written = false;\n \n-    bool is_partial_result_protocol_active = false;\n-\n     /// Counters for consumed chunks. Are used for QueryLog.\n     size_t result_rows = 0;\n     size_t result_bytes = 0;\ndiff --git a/src/Processors/Formats/Impl/PrettyBlockOutputFormat.cpp b/src/Processors/Formats/Impl/PrettyBlockOutputFormat.cpp\nindex 6fa891297f66..14648e68f94a 100644\n--- a/src/Processors/Formats/Impl/PrettyBlockOutputFormat.cpp\n+++ b/src/Processors/Formats/Impl/PrettyBlockOutputFormat.cpp\n@@ -134,8 +134,7 @@ void PrettyBlockOutputFormat::write(Chunk chunk, PortKind port_kind)\n {\n     if (total_rows >= format_settings.pretty.max_rows)\n     {\n-        if (port_kind != PortKind::PartialResult)\n-            total_rows += chunk.getNumRows();\n+        total_rows += chunk.getNumRows();\n         return;\n     }\n     if (mono_block)\n@@ -316,8 +315,7 @@ void PrettyBlockOutputFormat::writeChunk(const Chunk & chunk, PortKind port_kind\n     }\n     writeString(bottom_separator_s, out);\n \n-    if (port_kind != PortKind::PartialResult)\n-        total_rows += num_rows;\n+    total_rows += num_rows;\n }\n \n \n@@ -390,34 +388,6 @@ void PrettyBlockOutputFormat::consumeExtremes(Chunk chunk)\n     write(std::move(chunk), PortKind::Extremes);\n }\n \n-void PrettyBlockOutputFormat::clearLastLines(size_t lines_number)\n-{\n-    /// http://en.wikipedia.org/wiki/ANSI_escape_code\n-    #define MOVE_TO_PREV_LINE \"\\033[A\"\n-    #define CLEAR_TO_END_OF_LINE \"\\033[K\"\n-\n-    static const char * clear_prev_line = MOVE_TO_PREV_LINE \\\n-                                          CLEAR_TO_END_OF_LINE;\n-\n-    /// Move cursor to the beginning of line\n-    writeCString(\"\\r\", out);\n-\n-    for (size_t line = 0; line < lines_number; ++line)\n-    {\n-        writeCString(clear_prev_line, out);\n-    }\n-}\n-\n-void PrettyBlockOutputFormat::consumePartialResult(Chunk chunk)\n-{\n-    if (prev_partial_block_rows > 0)\n-        /// number of rows + header line + footer line\n-        clearLastLines(prev_partial_block_rows + 2);\n-\n-    prev_partial_block_rows = chunk.getNumRows();\n-    write(std::move(chunk), PortKind::PartialResult);\n-}\n-\n \n void PrettyBlockOutputFormat::writeMonoChunkIfNeeded()\n {\ndiff --git a/src/Processors/Formats/Impl/PrettyBlockOutputFormat.h b/src/Processors/Formats/Impl/PrettyBlockOutputFormat.h\nindex 92466dce3ff4..dfb23ac63f92 100644\n--- a/src/Processors/Formats/Impl/PrettyBlockOutputFormat.h\n+++ b/src/Processors/Formats/Impl/PrettyBlockOutputFormat.h\n@@ -28,12 +28,7 @@ class PrettyBlockOutputFormat : public IOutputFormat\n     void consumeTotals(Chunk) override;\n     void consumeExtremes(Chunk) override;\n \n-    void clearLastLines(size_t lines_number);\n-    void consumePartialResult(Chunk) override;\n-\n     size_t total_rows = 0;\n-    size_t prev_partial_block_rows = 0;\n-\n     size_t row_number_width = 7; // \"10000. \"\n \n     const FormatSettings format_settings;\n@@ -60,7 +55,6 @@ class PrettyBlockOutputFormat : public IOutputFormat\n     void resetFormatterImpl() override\n     {\n         total_rows = 0;\n-        prev_partial_block_rows = 0;\n     }\n \n private:\ndiff --git a/src/Processors/Formats/Impl/PrettyCompactBlockOutputFormat.cpp b/src/Processors/Formats/Impl/PrettyCompactBlockOutputFormat.cpp\nindex 3a04d86b1adb..2ba9ec725e24 100644\n--- a/src/Processors/Formats/Impl/PrettyCompactBlockOutputFormat.cpp\n+++ b/src/Processors/Formats/Impl/PrettyCompactBlockOutputFormat.cpp\n@@ -194,8 +194,7 @@ void PrettyCompactBlockOutputFormat::writeChunk(const Chunk & chunk, PortKind po\n \n     writeBottom(max_widths);\n \n-    if (port_kind != PortKind::PartialResult)\n-        total_rows += num_rows;\n+    total_rows += num_rows;\n }\n \n \ndiff --git a/src/Processors/Formats/LazyOutputFormat.h b/src/Processors/Formats/LazyOutputFormat.h\nindex bbcfdbb7193e..9cf609ed2d7c 100644\n--- a/src/Processors/Formats/LazyOutputFormat.h\n+++ b/src/Processors/Formats/LazyOutputFormat.h\n@@ -14,8 +14,8 @@ class LazyOutputFormat : public IOutputFormat\n {\n \n public:\n-    explicit LazyOutputFormat(const Block & header, bool is_partial_result_protocol_active = false)\n-        : IOutputFormat(header, out, is_partial_result_protocol_active), queue(2) {}\n+    explicit LazyOutputFormat(const Block & header)\n+        : IOutputFormat(header, out), queue(2) {}\n \n     String getName() const override { return \"LazyOutputFormat\"; }\n \n@@ -49,7 +49,6 @@ class LazyOutputFormat : public IOutputFormat\n \n     void consumeTotals(Chunk chunk) override { totals = std::move(chunk); }\n     void consumeExtremes(Chunk chunk) override { extremes = std::move(chunk); }\n-    void consumePartialResult(Chunk chunk) override { consume(std::move(chunk)); }\n \n private:\n \ndiff --git a/src/Processors/IProcessor.cpp b/src/Processors/IProcessor.cpp\nindex 2f294a32531c..8b160153733a 100644\n--- a/src/Processors/IProcessor.cpp\n+++ b/src/Processors/IProcessor.cpp\n@@ -40,10 +40,5 @@ std::string IProcessor::statusToName(Status status)\n     UNREACHABLE();\n }\n \n-ProcessorPtr IProcessor::getPartialResultProcessorPtr(const ProcessorPtr & current_processor, UInt64 partial_result_limit, UInt64 partial_result_duration_ms)\n-{\n-    return current_processor->getPartialResultProcessor(current_processor, partial_result_limit, partial_result_duration_ms);\n-}\n-\n }\n \ndiff --git a/src/Processors/IProcessor.h b/src/Processors/IProcessor.h\nindex 51a0bb1c1217..c6bef1868775 100644\n--- a/src/Processors/IProcessor.h\n+++ b/src/Processors/IProcessor.h\n@@ -164,8 +164,6 @@ class IProcessor\n \n     static std::string statusToName(Status status);\n \n-    static ProcessorPtr getPartialResultProcessorPtr(const ProcessorPtr & current_processor, UInt64 partial_result_limit, UInt64 partial_result_duration_ms);\n-\n     /** Method 'prepare' is responsible for all cheap (\"instantaneous\": O(1) of data volume, no wait) calculations.\n       *\n       * It may access input and output ports,\n@@ -237,22 +235,6 @@ class IProcessor\n         throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method 'expandPipeline' is not implemented for {} processor\", getName());\n     }\n \n-    enum class PartialResultStatus\n-    {\n-        /// Processor currently doesn't support work with the partial result pipeline.\n-        NotSupported,\n-\n-        /// Processor can be skipped in the partial result pipeline.\n-        SkipSupported,\n-\n-        /// Processor creates a light-weight copy of itself in the partial result pipeline.\n-        /// The copy can create snapshots of the original processor or transform small blocks of data in the same way as the original processor\n-        FullSupported,\n-    };\n-\n-    virtual bool isPartialResultProcessor() const { return false; }\n-    virtual PartialResultStatus getPartialResultProcessorSupportStatus() const { return PartialResultStatus::NotSupported; }\n-\n     /// In case if query was cancelled executor will wait till all processors finish their jobs.\n     /// Generally, there is no reason to check this flag. However, it may be reasonable for long operations (e.g. i/o).\n     bool isCancelled() const { return is_cancelled.load(std::memory_order_acquire); }\n@@ -387,11 +369,6 @@ class IProcessor\n protected:\n     virtual void onCancel() {}\n \n-    virtual ProcessorPtr getPartialResultProcessor(const ProcessorPtr & /*current_processor*/, UInt64 /*partial_result_limit*/, UInt64 /*partial_result_duration_ms*/)\n-    {\n-        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method 'getPartialResultProcessor' is not implemented for {} processor\", getName());\n-    }\n-\n private:\n     /// For:\n     /// - elapsed_us\ndiff --git a/src/Processors/LimitTransform.cpp b/src/Processors/LimitTransform.cpp\nindex b2bf3c28eee3..5e24062d67a4 100644\n--- a/src/Processors/LimitTransform.cpp\n+++ b/src/Processors/LimitTransform.cpp\n@@ -1,5 +1,5 @@\n #include <Processors/LimitTransform.h>\n-#include <Processors/Transforms/LimitPartialResultTransform.h>\n+\n \n namespace DB\n {\n@@ -180,6 +180,7 @@ LimitTransform::Status LimitTransform::preparePair(PortsData & data)\n         return Status::NeedData;\n \n     data.current_chunk = input.pull(true);\n+\n     auto rows = data.current_chunk.getNumRows();\n \n     if (rows_before_limit_at_least && !data.input_port_has_counter)\n@@ -366,11 +367,5 @@ bool LimitTransform::sortColumnsEqualAt(const ColumnRawPtrs & current_chunk_sort\n     return true;\n }\n \n-ProcessorPtr LimitTransform::getPartialResultProcessor(const ProcessorPtr & /*current_processor*/, UInt64 partial_result_limit, UInt64 partial_result_duration_ms)\n-{\n-    const auto & header = inputs.front().getHeader();\n-    return std::make_shared<LimitPartialResultTransform>(header, partial_result_limit, partial_result_duration_ms, limit, offset);\n-}\n-\n }\n \ndiff --git a/src/Processors/LimitTransform.h b/src/Processors/LimitTransform.h\nindex cfacc9634f94..33ff968985f5 100644\n--- a/src/Processors/LimitTransform.h\n+++ b/src/Processors/LimitTransform.h\n@@ -55,8 +55,6 @@ class LimitTransform final : public IProcessor\n     ColumnRawPtrs extractSortColumns(const Columns & columns) const;\n     bool sortColumnsEqualAt(const ColumnRawPtrs & current_chunk_sort_columns, UInt64 current_chunk_row_num) const;\n \n-    ProcessorPtr getPartialResultProcessor(const ProcessorPtr & current_processor, UInt64 partial_result_limit, UInt64 partial_result_duration_ms) override;\n-\n public:\n     LimitTransform(\n         const Block & header_, UInt64 limit_, UInt64 offset_, size_t num_streams = 1,\n@@ -75,14 +73,6 @@ class LimitTransform final : public IProcessor\n \n     void setRowsBeforeLimitCounter(RowsBeforeLimitCounterPtr counter) override { rows_before_limit_at_least.swap(counter); }\n     void setInputPortHasCounter(size_t pos) { ports_data[pos].input_port_has_counter = true; }\n-\n-    PartialResultStatus getPartialResultProcessorSupportStatus() const override\n-    {\n-        /// Currently LimitPartialResultTransform support only single-thread work.\n-        bool is_partial_result_supported = inputs.size() == 1 && outputs.size() == 1;\n-\n-        return is_partial_result_supported ? PartialResultStatus::FullSupported : PartialResultStatus::NotSupported;\n-    }\n };\n \n }\ndiff --git a/src/Processors/QueryPlan/BuildQueryPipelineSettings.cpp b/src/Processors/QueryPlan/BuildQueryPipelineSettings.cpp\nindex cd911e4cdf4c..fb3ed7f80fc8 100644\n--- a/src/Processors/QueryPlan/BuildQueryPipelineSettings.cpp\n+++ b/src/Processors/QueryPlan/BuildQueryPipelineSettings.cpp\n@@ -6,24 +6,10 @@\n namespace DB\n {\n \n-namespace ErrorCodes\n-{\n-    extern const int FUNCTION_NOT_ALLOWED;\n-}\n-\n BuildQueryPipelineSettings BuildQueryPipelineSettings::fromContext(ContextPtr from)\n {\n     BuildQueryPipelineSettings settings;\n-\n-    const auto & context_settings = from->getSettingsRef();\n-    settings.partial_result_limit = context_settings.max_rows_in_partial_result;\n-    settings.partial_result_duration_ms = context_settings.partial_result_update_duration_ms.totalMilliseconds();\n-    if (settings.partial_result_duration_ms && !context_settings.allow_experimental_partial_result)\n-        throw Exception(ErrorCodes::FUNCTION_NOT_ALLOWED,\n-            \"Partial results are not allowed by default, it's an experimental feature. \"\n-            \"Setting 'allow_experimental_partial_result' must be enabled to use 'partial_result_update_duration_ms'\");\n-\n-    settings.actions_settings = ExpressionActionsSettings::fromSettings(context_settings, CompileExpressions::yes);\n+    settings.actions_settings = ExpressionActionsSettings::fromSettings(from->getSettingsRef(), CompileExpressions::yes);\n     settings.process_list_element = from->getProcessListElement();\n     settings.progress_callback = from->getProgressCallback();\n     return settings;\ndiff --git a/src/Processors/QueryPlan/BuildQueryPipelineSettings.h b/src/Processors/QueryPlan/BuildQueryPipelineSettings.h\nindex 0410bf925d16..3b5e4e06953c 100644\n--- a/src/Processors/QueryPlan/BuildQueryPipelineSettings.h\n+++ b/src/Processors/QueryPlan/BuildQueryPipelineSettings.h\n@@ -19,9 +19,6 @@ struct BuildQueryPipelineSettings\n     QueryStatusPtr process_list_element;\n     ProgressCallback progress_callback = nullptr;\n \n-    UInt64 partial_result_limit = 0;\n-    UInt64 partial_result_duration_ms = 0;\n-\n     const ExpressionActionsSettings & getActionsSettings() const { return actions_settings; }\n     static BuildQueryPipelineSettings fromContext(ContextPtr from);\n };\ndiff --git a/src/Processors/QueryPlan/QueryPlan.cpp b/src/Processors/QueryPlan/QueryPlan.cpp\nindex ec82c233ce46..2d2dc66a8c99 100644\n--- a/src/Processors/QueryPlan/QueryPlan.cpp\n+++ b/src/Processors/QueryPlan/QueryPlan.cpp\n@@ -168,8 +168,6 @@ QueryPipelineBuilderPtr QueryPlan::buildQueryPipeline(\n \n     QueryPipelineBuilderPtr last_pipeline;\n \n-    bool has_partial_result_setting = build_pipeline_settings.partial_result_duration_ms > 0;\n-\n     std::stack<Frame> stack;\n     stack.push(Frame{.node = root});\n \n@@ -196,9 +194,6 @@ QueryPipelineBuilderPtr QueryPlan::buildQueryPipeline(\n         }\n         else\n             stack.push(Frame{.node = frame.node->children[next_child]});\n-\n-        if (has_partial_result_setting && last_pipeline && !last_pipeline->isPartialResultActive())\n-            last_pipeline->activatePartialResult(build_pipeline_settings.partial_result_limit, build_pipeline_settings.partial_result_duration_ms);\n     }\n \n     last_pipeline->setProgressCallback(build_pipeline_settings.progress_callback);\ndiff --git a/src/Processors/QueryPlan/SortingStep.h b/src/Processors/QueryPlan/SortingStep.h\nindex a72cab05754f..371a24ac6f2d 100644\n--- a/src/Processors/QueryPlan/SortingStep.h\n+++ b/src/Processors/QueryPlan/SortingStep.h\n@@ -27,8 +27,6 @@ class SortingStep : public ITransformingStep\n         size_t max_bytes_before_external_sort = 0;\n         TemporaryDataOnDiskScopePtr tmp_data = nullptr;\n         size_t min_free_disk_space = 0;\n-        UInt64 partial_result_limit = 0;\n-        UInt64 partial_result_duration_ms = 0;\n \n         explicit Settings(const Context & context);\n         explicit Settings(size_t max_block_size_);\ndiff --git a/src/Processors/Transforms/AggregatingPartialResultTransform.cpp b/src/Processors/Transforms/AggregatingPartialResultTransform.cpp\ndeleted file mode 100644\nindex cf8ce72e096e..000000000000\n--- a/src/Processors/Transforms/AggregatingPartialResultTransform.cpp\n+++ /dev/null\n@@ -1,47 +0,0 @@\n-#include <Processors/Transforms/AggregatingPartialResultTransform.h>\n-\n-namespace DB\n-{\n-\n-AggregatingPartialResultTransform::AggregatingPartialResultTransform(\n-    const Block & input_header, const Block & output_header, AggregatingTransformPtr aggregating_transform_,\n-    UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_)\n-    : PartialResultTransform(input_header, output_header, partial_result_limit_, partial_result_duration_ms_)\n-    , aggregating_transform(std::move(aggregating_transform_))\n-    , transform_aggregator(input_header, aggregating_transform->params->params)\n-    {}\n-\n-void AggregatingPartialResultTransform::transformPartialResult(Chunk & chunk)\n-{\n-    auto & params = aggregating_transform->params->params;\n-\n-    bool no_more_keys = false;\n-    AggregatedDataVariants variants;\n-    ColumnRawPtrs key_columns(params.keys_size);\n-    Aggregator::AggregateColumns aggregate_columns(params.aggregates_size);\n-\n-    const UInt64 num_rows = chunk.getNumRows();\n-    transform_aggregator.executeOnBlock(chunk.detachColumns(), 0, num_rows, variants, key_columns, aggregate_columns, no_more_keys);\n-\n-    auto transformed_block = transform_aggregator.convertToBlocks(variants, /*final*/ true, /*max_threads*/ 1).front();\n-\n-    chunk = convertToChunk(transformed_block);\n-}\n-\n-PartialResultTransform::ShaphotResult AggregatingPartialResultTransform::getRealProcessorSnapshot()\n-{\n-    std::lock_guard lock(aggregating_transform->snapshot_mutex);\n-    if (aggregating_transform->is_generate_initialized)\n-        return {{}, SnaphotStatus::Stopped};\n-\n-    if (aggregating_transform->variants.empty())\n-        return {{}, SnaphotStatus::NotReady};\n-\n-    auto & snapshot_aggregator = aggregating_transform->params->aggregator;\n-    auto & snapshot_variants = aggregating_transform->many_data->variants;\n-    auto block = snapshot_aggregator.prepareBlockAndFillWithoutKeySnapshot(*snapshot_variants.at(0));\n-\n-    return {convertToChunk(block), SnaphotStatus::Ready};\n-}\n-\n-}\ndiff --git a/src/Processors/Transforms/AggregatingPartialResultTransform.h b/src/Processors/Transforms/AggregatingPartialResultTransform.h\ndeleted file mode 100644\nindex f7bac3a53943..000000000000\n--- a/src/Processors/Transforms/AggregatingPartialResultTransform.h\n+++ /dev/null\n@@ -1,29 +0,0 @@\n-#pragma once\n-\n-#include <Interpreters/Aggregator.h>\n-#include <Processors/Transforms/AggregatingTransform.h>\n-#include <Processors/Transforms/PartialResultTransform.h>\n-\n-namespace DB\n-{\n-\n-class AggregatingPartialResultTransform : public PartialResultTransform\n-{\n-public:\n-    using AggregatingTransformPtr = std::shared_ptr<AggregatingTransform>;\n-\n-    AggregatingPartialResultTransform(\n-        const Block & input_header, const Block & output_header, AggregatingTransformPtr aggregating_transform_,\n-        UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_);\n-\n-    String getName() const override { return \"AggregatingPartialResultTransform\"; }\n-\n-    void transformPartialResult(Chunk & chunk) override;\n-    ShaphotResult getRealProcessorSnapshot() override;\n-\n-private:\n-    AggregatingTransformPtr aggregating_transform;\n-    Aggregator transform_aggregator;\n-};\n-\n-}\ndiff --git a/src/Processors/Transforms/AggregatingTransform.cpp b/src/Processors/Transforms/AggregatingTransform.cpp\nindex 9546d3965233..bf475c57d367 100644\n--- a/src/Processors/Transforms/AggregatingTransform.cpp\n+++ b/src/Processors/Transforms/AggregatingTransform.cpp\n@@ -1,4 +1,3 @@\n-#include <Processors/Transforms/AggregatingPartialResultTransform.h>\n #include <Processors/Transforms/AggregatingTransform.h>\n \n #include <Formats/NativeReader.h>\n@@ -10,6 +9,7 @@\n \n #include <Processors/Transforms/SquashingChunksTransform.h>\n \n+\n namespace ProfileEvents\n {\n     extern const Event ExternalAggregationMerge;\n@@ -660,8 +660,6 @@ void AggregatingTransform::consume(Chunk chunk)\n     src_rows += num_rows;\n     src_bytes += chunk.bytes();\n \n-    std::lock_guard lock(snapshot_mutex);\n-\n     if (params->params.only_merge)\n     {\n         auto block = getInputs().front().getHeader().cloneWithColumns(chunk.detachColumns());\n@@ -681,10 +679,6 @@ void AggregatingTransform::initGenerate()\n     if (is_generate_initialized.load(std::memory_order_acquire))\n         return;\n \n-    std::lock_guard lock(snapshot_mutex);\n-    if (is_generate_initialized.load(std::memory_order_relaxed))\n-        return;\n-\n     is_generate_initialized.store(true, std::memory_order_release);\n \n     /// If there was no data, and we aggregate without keys, and we must return single row with the result of empty aggregation.\n@@ -815,12 +809,4 @@ void AggregatingTransform::initGenerate()\n     }\n }\n \n-ProcessorPtr AggregatingTransform::getPartialResultProcessor(const ProcessorPtr & current_processor, UInt64 partial_result_limit, UInt64 partial_result_duration_ms)\n-{\n-    const auto & input_header = inputs.front().getHeader();\n-    const auto & output_header = outputs.front().getHeader();\n-    auto aggregating_processor = std::dynamic_pointer_cast<AggregatingTransform>(current_processor);\n-    return std::make_shared<AggregatingPartialResultTransform>(input_header, output_header, std::move(aggregating_processor), partial_result_limit, partial_result_duration_ms);\n-}\n-\n }\ndiff --git a/src/Processors/Transforms/AggregatingTransform.h b/src/Processors/Transforms/AggregatingTransform.h\nindex 7b13b1a34f67..3420cdeaa501 100644\n--- a/src/Processors/Transforms/AggregatingTransform.h\n+++ b/src/Processors/Transforms/AggregatingTransform.h\n@@ -170,23 +170,9 @@ class AggregatingTransform : public IProcessor\n     void work() override;\n     Processors expandPipeline() override;\n \n-    PartialResultStatus getPartialResultProcessorSupportStatus() const override\n-    {\n-        /// Currently AggregatingPartialResultTransform support only single-thread aggregation without key.\n-\n-        /// TODO: check that insert results from aggregator.prepareBlockAndFillWithoutKey return values without\n-        /// changing of the aggregator state when aggregation with keys will be supported in AggregatingPartialResultTransform.\n-        bool is_partial_result_supported = params->params.keys_size == 0 /// Aggregation without key.\n-                                    && many_data->variants.size() == 1; /// Use only one stream for aggregation.\n-\n-        return is_partial_result_supported ? PartialResultStatus::FullSupported : PartialResultStatus::NotSupported;\n-    }\n-\n protected:\n     void consume(Chunk chunk);\n \n-    ProcessorPtr getPartialResultProcessor(const ProcessorPtr & current_processor, UInt64 partial_result_limit, UInt64 partial_result_duration_ms) override;\n-\n private:\n     /// To read the data that was flushed into the temporary data file.\n     Processors processors;\n@@ -226,13 +212,6 @@ class AggregatingTransform : public IProcessor\n \n     bool is_consume_started = false;\n \n-    friend class AggregatingPartialResultTransform;\n-    /// The mutex protects variables that are used for creating a snapshot of the current processor.\n-    /// The current implementation of AggregatingPartialResultTransform uses the 'is_generate_initialized' variable to check\n-    /// whether the processor has started sending data through the main pipeline, and the corresponding partial result processor should stop creating snapshots.\n-    /// Additionally, the mutex protects the 'params->aggregator' and 'many_data->variants' variables, which are used to get data from them for a snapshot.\n-    std::mutex snapshot_mutex;\n-\n     void initGenerate();\n };\n \ndiff --git a/src/Processors/Transforms/ExpressionTransform.cpp b/src/Processors/Transforms/ExpressionTransform.cpp\nindex bd92267a7339..0d3341b000c9 100644\n--- a/src/Processors/Transforms/ExpressionTransform.cpp\n+++ b/src/Processors/Transforms/ExpressionTransform.cpp\n@@ -25,14 +25,6 @@ void ExpressionTransform::transform(Chunk & chunk)\n     chunk.setColumns(block.getColumns(), num_rows);\n }\n \n-ProcessorPtr ExpressionTransform::getPartialResultProcessor(const ProcessorPtr & /*current_processor*/, UInt64 /*partial_result_limit*/, UInt64 /*partial_result_duration_ms*/)\n-{\n-    const auto & header = getInputPort().getHeader();\n-    auto result = std::make_shared<ExpressionTransform>(header, expression);\n-    result->setDescription(\"(Partial result)\");\n-    return result;\n-}\n-\n ConvertingTransform::ConvertingTransform(const Block & header_, ExpressionActionsPtr expression_)\n     : ExceptionKeepingTransform(header_, ExpressionTransform::transformHeader(header_, expression_->getActionsDAG()))\n     , expression(std::move(expression_))\ndiff --git a/src/Processors/Transforms/ExpressionTransform.h b/src/Processors/Transforms/ExpressionTransform.h\nindex 8250f25f0f84..791c7d7ba731 100644\n--- a/src/Processors/Transforms/ExpressionTransform.h\n+++ b/src/Processors/Transforms/ExpressionTransform.h\n@@ -26,15 +26,10 @@ class ExpressionTransform final : public ISimpleTransform\n \n     static Block transformHeader(Block header, const ActionsDAG & expression);\n \n-    PartialResultStatus getPartialResultProcessorSupportStatus() const override { return PartialResultStatus::FullSupported; }\n-\n protected:\n     void transform(Chunk & chunk) override;\n \n-    ProcessorPtr getPartialResultProcessor(const ProcessorPtr & current_processor, UInt64 partial_result_limit, UInt64 partial_result_duration_ms) override;\n-\n private:\n-\n     ExpressionActionsPtr expression;\n };\n \ndiff --git a/src/Processors/Transforms/LimitPartialResultTransform.cpp b/src/Processors/Transforms/LimitPartialResultTransform.cpp\ndeleted file mode 100644\nindex c9eaa9dc7dd9..000000000000\n--- a/src/Processors/Transforms/LimitPartialResultTransform.cpp\n+++ /dev/null\n@@ -1,42 +0,0 @@\n-#include <Processors/LimitTransform.h>\n-#include <Processors/Transforms/LimitPartialResultTransform.h>\n-\n-namespace DB\n-{\n-\n-LimitPartialResultTransform::LimitPartialResultTransform(\n-    const Block & header,\n-    UInt64 partial_result_limit_,\n-    UInt64 partial_result_duration_ms_,\n-    UInt64 limit_,\n-    UInt64 offset_)\n-    : PartialResultTransform(header, partial_result_limit_, partial_result_duration_ms_)\n-    , limit(limit_)\n-    , offset(offset_)\n-    {}\n-\n-void LimitPartialResultTransform::transformPartialResult(Chunk & chunk)\n-{\n-    UInt64 num_rows = chunk.getNumRows();\n-    if (num_rows < offset || limit == 0)\n-    {\n-        chunk = {};\n-        return;\n-    }\n-\n-    UInt64 length = std::min(limit, num_rows - offset);\n-\n-    /// Check if some rows should be removed\n-    if (length < num_rows)\n-    {\n-        UInt64 num_columns = chunk.getNumColumns();\n-        auto columns = chunk.detachColumns();\n-\n-        for (UInt64 i = 0; i < num_columns; ++i)\n-            columns[i] = columns[i]->cut(offset, length);\n-\n-        chunk.setColumns(std::move(columns), length);\n-    }\n-}\n-\n-}\ndiff --git a/src/Processors/Transforms/LimitPartialResultTransform.h b/src/Processors/Transforms/LimitPartialResultTransform.h\ndeleted file mode 100644\nindex 3a0116b624d9..000000000000\n--- a/src/Processors/Transforms/LimitPartialResultTransform.h\n+++ /dev/null\n@@ -1,36 +0,0 @@\n-#pragma once\n-\n-#include <Processors/Transforms/PartialResultTransform.h>\n-\n-namespace DB\n-{\n-\n-class LimitTransform;\n-\n-/// Currently support only single thread implementation with one input and one output ports\n-class LimitPartialResultTransform : public PartialResultTransform\n-{\n-public:\n-    using LimitTransformPtr = std::shared_ptr<LimitTransform>;\n-\n-    LimitPartialResultTransform(\n-        const Block & header,\n-        UInt64 partial_result_limit_,\n-        UInt64 partial_result_duration_ms_,\n-        UInt64 limit_,\n-        UInt64 offset_);\n-\n-    String getName() const override { return \"LimitPartialResultTransform\"; }\n-\n-    void transformPartialResult(Chunk & chunk) override;\n-    /// LimitsTransform doesn't have a state which can be snapshoted\n-    ShaphotResult getRealProcessorSnapshot() override { return {{}, SnaphotStatus::Stopped}; }\n-\n-private:\n-    UInt64 limit;\n-    UInt64 offset;\n-\n-    LimitTransformPtr limit_transform;\n-};\n-\n-}\ndiff --git a/src/Processors/Transforms/LimitsCheckingTransform.cpp b/src/Processors/Transforms/LimitsCheckingTransform.cpp\nindex 0557f3f291ed..02d2fef808ce 100644\n--- a/src/Processors/Transforms/LimitsCheckingTransform.cpp\n+++ b/src/Processors/Transforms/LimitsCheckingTransform.cpp\n@@ -1,5 +1,4 @@\n #include <Processors/Transforms/LimitsCheckingTransform.h>\n-#include <Processors/Transforms/PartialResultTransform.h>\n #include <Access/EnabledQuota.h>\n \n namespace DB\ndiff --git a/src/Processors/Transforms/LimitsCheckingTransform.h b/src/Processors/Transforms/LimitsCheckingTransform.h\nindex eabb988dab61..2f96a17c17be 100644\n--- a/src/Processors/Transforms/LimitsCheckingTransform.h\n+++ b/src/Processors/Transforms/LimitsCheckingTransform.h\n@@ -33,8 +33,6 @@ class LimitsCheckingTransform : public ISimpleTransform\n \n     void setQuota(const std::shared_ptr<const EnabledQuota> & quota_) { quota = quota_; }\n \n-    PartialResultStatus getPartialResultProcessorSupportStatus() const override { return PartialResultStatus::SkipSupported; }\n-\n protected:\n     void transform(Chunk & chunk) override;\n \ndiff --git a/src/Processors/Transforms/MergeSortingPartialResultTransform.cpp b/src/Processors/Transforms/MergeSortingPartialResultTransform.cpp\ndeleted file mode 100644\nindex 44b34ce3f584..000000000000\n--- a/src/Processors/Transforms/MergeSortingPartialResultTransform.cpp\n+++ /dev/null\n@@ -1,54 +0,0 @@\n-#include <Processors/Transforms/MergeSortingPartialResultTransform.h>\n-\n-namespace DB\n-{\n-\n-MergeSortingPartialResultTransform::MergeSortingPartialResultTransform(\n-    const Block & header, MergeSortingTransformPtr merge_sorting_transform_,\n-    UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_)\n-    : PartialResultTransform(header, partial_result_limit_, partial_result_duration_ms_)\n-    , merge_sorting_transform(std::move(merge_sorting_transform_))\n-    {}\n-\n-PartialResultTransform::ShaphotResult MergeSortingPartialResultTransform::getRealProcessorSnapshot()\n-{\n-    std::lock_guard lock(merge_sorting_transform->snapshot_mutex);\n-    if (merge_sorting_transform->generated_prefix)\n-        return {{}, SnaphotStatus::Stopped};\n-\n-    if (merge_sorting_transform->chunks.empty())\n-        return {{}, SnaphotStatus::NotReady};\n-\n-    /// Sort all input data\n-    merge_sorting_transform->remerge();\n-\n-    /// It's possible that we had only empty chunks before remerge\n-    if (merge_sorting_transform->chunks.empty())\n-        return {{}, SnaphotStatus::NotReady};\n-\n-    /// Add a copy of the first `partial_result_limit` rows to a generated_chunk\n-    /// to send it later as a partial result in the next prepare stage of the current processor\n-    auto generated_columns = merge_sorting_transform->chunks[0].cloneEmptyColumns();\n-\n-    size_t total_rows = 0;\n-    for (const auto & merged_chunk : merge_sorting_transform->chunks)\n-    {\n-        size_t rows = std::min(merged_chunk.getNumRows(), partial_result_limit - total_rows);\n-        if (rows == 0)\n-            break;\n-\n-        for (size_t position = 0; position < generated_columns.size(); ++position)\n-        {\n-            auto column = merged_chunk.getColumns()[position];\n-            generated_columns[position]->insertRangeFrom(*column, 0, rows);\n-        }\n-\n-        total_rows += rows;\n-    }\n-\n-    auto partial_result = Chunk(std::move(generated_columns), total_rows, merge_sorting_transform->chunks[0].getChunkInfo());\n-    merge_sorting_transform->enrichChunkWithConstants(partial_result);\n-    return {std::move(partial_result), SnaphotStatus::Ready};\n-}\n-\n-}\ndiff --git a/src/Processors/Transforms/MergeSortingPartialResultTransform.h b/src/Processors/Transforms/MergeSortingPartialResultTransform.h\ndeleted file mode 100644\nindex 781aa8e1265d..000000000000\n--- a/src/Processors/Transforms/MergeSortingPartialResultTransform.h\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-#pragma once\n-\n-#include <Processors/Transforms/MergeSortingTransform.h>\n-#include <Processors/Transforms/PartialResultTransform.h>\n-\n-namespace DB\n-{\n-\n-class MergeSortingPartialResultTransform : public PartialResultTransform\n-{\n-public:\n-    using MergeSortingTransformPtr = std::shared_ptr<MergeSortingTransform>;\n-\n-    MergeSortingPartialResultTransform(\n-        const Block & header, MergeSortingTransformPtr merge_sorting_transform_,\n-        UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_);\n-\n-    String getName() const override { return \"MergeSortingPartialResultTransform\"; }\n-\n-    /// MergeSortingTransform always receives chunks in a sorted state, so transformation is not needed\n-    void transformPartialResult(Chunk & /*chunk*/) override {}\n-    ShaphotResult getRealProcessorSnapshot() override;\n-\n-private:\n-    MergeSortingTransformPtr merge_sorting_transform;\n-};\n-\n-}\ndiff --git a/src/Processors/Transforms/MergeSortingTransform.cpp b/src/Processors/Transforms/MergeSortingTransform.cpp\nindex e801e5e16d5f..de77711d1294 100644\n--- a/src/Processors/Transforms/MergeSortingTransform.cpp\n+++ b/src/Processors/Transforms/MergeSortingTransform.cpp\n@@ -1,5 +1,4 @@\n #include <Processors/Transforms/MergeSortingTransform.h>\n-#include <Processors/Transforms/MergeSortingPartialResultTransform.h>\n #include <Processors/IAccumulatingTransform.h>\n #include <Processors/Merges/MergingSortedTransform.h>\n #include <Common/ProfileEvents.h>\n@@ -137,8 +136,6 @@ void MergeSortingTransform::consume(Chunk chunk)\n \n     /// If there were only const columns in sort description, then there is no need to sort.\n     /// Return the chunk as is.\n-    std::lock_guard lock(snapshot_mutex);\n-\n     if (description.empty())\n     {\n         generated_chunk = std::move(chunk);\n@@ -216,8 +213,6 @@ void MergeSortingTransform::serialize()\n \n void MergeSortingTransform::generate()\n {\n-    std::lock_guard lock(snapshot_mutex);\n-\n     if (!generated_prefix)\n     {\n         size_t num_tmp_files = tmp_data ? tmp_data->getStreams().size() : 0;\n@@ -278,11 +273,4 @@ void MergeSortingTransform::remerge()\n     sum_bytes_in_blocks = new_sum_bytes_in_blocks;\n }\n \n-ProcessorPtr MergeSortingTransform::getPartialResultProcessor(const ProcessorPtr & current_processor, UInt64 partial_result_limit, UInt64 partial_result_duration_ms)\n-{\n-    const auto & header = inputs.front().getHeader();\n-    auto merge_sorting_processor = std::dynamic_pointer_cast<MergeSortingTransform>(current_processor);\n-    return std::make_shared<MergeSortingPartialResultTransform>(header, std::move(merge_sorting_processor), partial_result_limit, partial_result_duration_ms);\n-}\n-\n }\ndiff --git a/src/Processors/Transforms/MergeSortingTransform.h b/src/Processors/Transforms/MergeSortingTransform.h\nindex 67f098b43624..e8c180b69032 100644\n--- a/src/Processors/Transforms/MergeSortingTransform.h\n+++ b/src/Processors/Transforms/MergeSortingTransform.h\n@@ -33,8 +33,6 @@ class MergeSortingTransform : public SortingTransform\n \n     String getName() const override { return \"MergeSortingTransform\"; }\n \n-    PartialResultStatus getPartialResultProcessorSupportStatus() const override { return PartialResultStatus::FullSupported; }\n-\n protected:\n     void consume(Chunk chunk) override;\n     void serialize() override;\n@@ -42,8 +40,6 @@ class MergeSortingTransform : public SortingTransform\n \n     Processors expandPipeline() override;\n \n-    ProcessorPtr getPartialResultProcessor(const ProcessorPtr & current_processor, UInt64 partial_result_limit, UInt64 partial_result_duration_ms) override;\n-\n private:\n     size_t max_bytes_before_remerge;\n     double remerge_lowered_memory_bytes_ratio;\n@@ -63,13 +59,6 @@ class MergeSortingTransform : public SortingTransform\n     void remerge();\n \n     ProcessorPtr external_merging_sorted;\n-\n-    friend class MergeSortingPartialResultTransform;\n-    /// The mutex protects variables that are used for creating a snapshot of the current processor.\n-    /// The current implementation of MergeSortingPartialResultTransform uses the 'generated_prefix' variable to check\n-    /// whether the processor has started sending data through the main pipeline, and the corresponding partial result processor should stop creating snapshots.\n-    /// Additionally, the mutex protects the 'chunks' variable and all variables in the 'remerge' function, which is used to transition 'chunks' to a sorted state.\n-    std::mutex snapshot_mutex;\n };\n \n }\ndiff --git a/src/Processors/Transforms/PartialResultTransform.cpp b/src/Processors/Transforms/PartialResultTransform.cpp\ndeleted file mode 100644\nindex 97ff79dee54f..000000000000\n--- a/src/Processors/Transforms/PartialResultTransform.cpp\n+++ /dev/null\n@@ -1,80 +0,0 @@\n-#include <Processors/Transforms/PartialResultTransform.h>\n-\n-namespace DB\n-{\n-\n-\n-PartialResultTransform::PartialResultTransform(const Block & header, UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_)\n-    : PartialResultTransform(header, header, partial_result_limit_, partial_result_duration_ms_) {}\n-\n-PartialResultTransform::PartialResultTransform(const Block & input_header, const Block & output_header, UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_)\n-    : IProcessor({input_header}, {output_header})\n-    , input(inputs.front())\n-    , output(outputs.front())\n-    , partial_result_limit(partial_result_limit_)\n-    , partial_result_duration_ms(partial_result_duration_ms_)\n-    , watch(CLOCK_MONOTONIC)\n-    {}\n-\n-IProcessor::Status PartialResultTransform::prepare()\n-{\n-    if (output.isFinished())\n-    {\n-        input.close();\n-        return Status::Finished;\n-    }\n-\n-    if (finished_getting_snapshots)\n-    {\n-        output.finish();\n-        return Status::Finished;\n-    }\n-\n-    if (!output.canPush())\n-    {\n-        input.setNotNeeded();\n-        return Status::PortFull;\n-    }\n-\n-    /// If input data from previous partial result processor is finished then\n-    /// PartialResultTransform ready to create snapshots and send them as a partial result\n-    if (input.isFinished())\n-    {\n-        if (partial_result.snapshot_status == SnaphotStatus::Ready)\n-        {\n-            partial_result.snapshot_status = SnaphotStatus::NotReady;\n-            output.push(std::move(partial_result.chunk));\n-            return Status::PortFull;\n-        }\n-\n-        return Status::Ready;\n-    }\n-\n-    input.setNeeded();\n-    if (!input.hasData())\n-        return Status::NeedData;\n-\n-    partial_result.chunk = input.pull();\n-    transformPartialResult(partial_result.chunk);\n-    if (partial_result.chunk.getNumRows() > 0)\n-    {\n-        output.push(std::move(partial_result.chunk));\n-        return Status::PortFull;\n-    }\n-\n-    return Status::NeedData;\n-}\n-\n-void PartialResultTransform::work()\n-{\n-    if (partial_result_duration_ms < watch.elapsedMilliseconds())\n-    {\n-        partial_result = getRealProcessorSnapshot();\n-        if (partial_result.snapshot_status == SnaphotStatus::Stopped)\n-            finished_getting_snapshots = true;\n-\n-        watch.restart();\n-    }\n-}\n-\n-}\ndiff --git a/src/Processors/Transforms/PartialResultTransform.h b/src/Processors/Transforms/PartialResultTransform.h\ndeleted file mode 100644\nindex 4fe87638f38d..000000000000\n--- a/src/Processors/Transforms/PartialResultTransform.h\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-#pragma once\n-\n-#include <Processors/IProcessor.h>\n-\n-namespace DB\n-{\n-\n-/// Processors of this type are used to construct an auxiliary pipeline with processors corresponding to those in the main pipeline.\n-/// These processors work in two modes:\n-/// 1) Creating a snapshot of the corresponding processor from the main pipeline once per partial_result_duration_ms (period in milliseconds), and then sending the snapshot through the partial result pipeline.\n-/// 2) Transforming small blocks of data in the same way as the original processor and sending the transformed data through the partial result pipeline.\n-/// All processors of this type rely on the invariant that a new block from the previous processor of the partial result pipeline overwrites information about the previous block of the same previous processor.\n-class PartialResultTransform : public IProcessor\n-{\n-public:\n-    PartialResultTransform(const Block & header, UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_);\n-    PartialResultTransform(const Block & input_header, const Block & output_header, UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_);\n-\n-    String getName() const override { return \"PartialResultTransform\"; }\n-\n-    Status prepare() override;\n-    void work() override;\n-\n-    bool isPartialResultProcessor() const override { return true; }\n-\n-protected:\n-    enum class SnaphotStatus\n-    {\n-        NotReady, // Waiting for data from the previous partial result processor or awaiting a timer before creating the snapshot.\n-        Ready,    // Current partial result processor has received a snapshot from the processor in the main pipeline.\n-        Stopped,  // The processor from the main pipeline has started sending data, and the pipeline for partial results should use data from the next processors of the main pipeline.\n-    };\n-\n-    struct ShaphotResult\n-    {\n-        Chunk chunk;\n-        SnaphotStatus snapshot_status;\n-    };\n-\n-    InputPort & input;\n-    OutputPort & output;\n-\n-    UInt64 partial_result_limit;\n-    UInt64 partial_result_duration_ms;\n-\n-    ShaphotResult partial_result = {{}, SnaphotStatus::NotReady};\n-\n-    bool finished_getting_snapshots = false;\n-\n-    virtual void transformPartialResult(Chunk & /*chunk*/) = 0;\n-    virtual ShaphotResult getRealProcessorSnapshot() = 0; // { return {{}, SnaphotStatus::Stopped}; }\n-\n-private:\n-    Stopwatch watch;\n-};\n-\n-}\ndiff --git a/src/QueryPipeline/Pipe.cpp b/src/QueryPipeline/Pipe.cpp\nindex b22c135e8650..b1c82d7a7e8a 100644\n--- a/src/QueryPipeline/Pipe.cpp\n+++ b/src/QueryPipeline/Pipe.cpp\n@@ -12,7 +12,6 @@\n #include <Processors/QueryPlan/QueryPlan.h>\n #include <QueryPipeline/ReadProgressCallback.h>\n #include <Columns/ColumnConst.h>\n-#include <Common/logger_useful.h>\n \n #include <QueryPipeline/printPipeline.h>\n \n@@ -170,9 +169,12 @@ Pipe::Pipe(ProcessorPtr source)\n {\n     checkSource(*source);\n \n+    if (collected_processors)\n+        collected_processors->emplace_back(source);\n+\n     output_ports.push_back(&source->getOutputs().front());\n     header = output_ports.front()->getHeader();\n-    addProcessor(std::move(source));\n+    processors->emplace_back(std::move(source));\n     max_parallel_streams = 1;\n }\n \n@@ -313,18 +315,6 @@ Pipe Pipe::unitePipes(Pipes pipes, Processors * collected_processors, bool allow\n \n     for (auto & pipe : pipes)\n     {\n-        if (res.isPartialResultActive() && pipe.isPartialResultActive())\n-        {\n-            res.partial_result_ports.insert(res.partial_result_ports.end(), pipe.partial_result_ports.begin(), pipe.partial_result_ports.end());\n-        }\n-        else\n-        {\n-            if (pipe.isPartialResultActive())\n-                pipe.dropPartialResult();\n-            if (res.isPartialResultActive())\n-                res.dropPartialResult();\n-        }\n-\n         if (!allow_empty_header || pipe.header)\n             assertCompatibleHeader(pipe.header, res.header, \"Pipe::unitePipes\");\n \n@@ -364,11 +354,11 @@ void Pipe::addSource(ProcessorPtr source)\n     else\n         assertBlocksHaveEqualStructure(header, source_header, \"Pipes\");\n \n-    output_ports.push_back(&source->getOutputs().front());\n-    if (isPartialResultActive())\n-        partial_result_ports.push_back(nullptr);\n+    if (collected_processors)\n+        collected_processors->emplace_back(source);\n \n-    addProcessor(std::move(source));\n+    output_ports.push_back(&source->getOutputs().front());\n+    processors->emplace_back(std::move(source));\n \n     max_parallel_streams = std::max<size_t>(max_parallel_streams, output_ports.size());\n }\n@@ -386,9 +376,11 @@ void Pipe::addTotalsSource(ProcessorPtr source)\n \n     assertBlocksHaveEqualStructure(header, source_header, \"Pipes\");\n \n-    totals_port = &source->getOutputs().front();\n+    if (collected_processors)\n+        collected_processors->emplace_back(source);\n \n-    addProcessor(std::move(source));\n+    totals_port = &source->getOutputs().front();\n+    processors->emplace_back(std::move(source));\n }\n \n void Pipe::addExtremesSource(ProcessorPtr source)\n@@ -404,20 +396,11 @@ void Pipe::addExtremesSource(ProcessorPtr source)\n \n     assertBlocksHaveEqualStructure(header, source_header, \"Pipes\");\n \n-    extremes_port = &source->getOutputs().front();\n-\n-    addProcessor(std::move(source));\n-}\n-\n-void Pipe::activatePartialResult(UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_)\n-{\n-    if (is_partial_result_active)\n-        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Partial result for Pipe should be initialized only once\");\n+    if (collected_processors)\n+        collected_processors->emplace_back(source);\n \n-    is_partial_result_active = true;\n-    partial_result_limit = partial_result_limit_;\n-    partial_result_duration_ms = partial_result_duration_ms_;\n-    partial_result_ports.assign(output_ports.size(), nullptr);\n+    extremes_port = &source->getOutputs().front();\n+    processors->emplace_back(std::move(source));\n }\n \n static void dropPort(OutputPort *& port, Processors & processors, Processors * collected_processors)\n@@ -445,15 +428,6 @@ void Pipe::dropExtremes()\n     dropPort(extremes_port, *processors, collected_processors);\n }\n \n-void Pipe::dropPartialResult()\n-{\n-    for (auto & port : partial_result_ports)\n-        dropPort(port, *processors, collected_processors);\n-\n-    is_partial_result_active = false;\n-    partial_result_ports.clear();\n-}\n-\n void Pipe::addTransform(ProcessorPtr transform)\n {\n     addTransform(std::move(transform), static_cast<OutputPort *>(nullptr), static_cast<OutputPort *>(nullptr));\n@@ -484,8 +458,6 @@ void Pipe::addTransform(ProcessorPtr transform, OutputPort * totals, OutputPort\n     if (extremes)\n         extremes_port = extremes;\n \n-    addPartialResultTransform(transform);\n-\n     size_t next_output = 0;\n     for (auto & input : inputs)\n     {\n@@ -536,7 +508,10 @@ void Pipe::addTransform(ProcessorPtr transform, OutputPort * totals, OutputPort\n     if (extremes_port)\n         assertBlocksHaveEqualStructure(header, extremes_port->getHeader(), \"Pipes\");\n \n-    addProcessor(std::move(transform));\n+    if (collected_processors)\n+        collected_processors->emplace_back(transform);\n+\n+    processors->emplace_back(std::move(transform));\n \n     max_parallel_streams = std::max<size_t>(max_parallel_streams, output_ports.size());\n }\n@@ -573,8 +548,6 @@ void Pipe::addTransform(ProcessorPtr transform, InputPort * totals, InputPort *\n         extremes_port = nullptr;\n     }\n \n-    addPartialResultTransform(transform);\n-\n     bool found_totals = false;\n     bool found_extremes = false;\n \n@@ -624,117 +597,12 @@ void Pipe::addTransform(ProcessorPtr transform, InputPort * totals, InputPort *\n     if (extremes_port)\n         assertBlocksHaveEqualStructure(header, extremes_port->getHeader(), \"Pipes\");\n \n-    addProcessor(std::move(transform));\n-\n-    max_parallel_streams = std::max<size_t>(max_parallel_streams, output_ports.size());\n-}\n-\n-void Pipe::addPartialResultSimpleTransform(const ProcessorPtr & transform, size_t partial_result_port_id)\n-{\n-    if (isPartialResultActive())\n-    {\n-        auto & partial_result_port = partial_result_ports[partial_result_port_id];\n-        auto partial_result_status = transform->getPartialResultProcessorSupportStatus();\n-\n-        if (partial_result_status == IProcessor::PartialResultStatus::NotSupported)\n-            dropPort(partial_result_port, *processors, collected_processors);\n-\n-        if (partial_result_status != IProcessor::PartialResultStatus::FullSupported)\n-            return;\n-\n-        auto partial_result_transform = IProcessor::getPartialResultProcessorPtr(transform, partial_result_limit, partial_result_duration_ms);\n-\n-        connectPartialResultPort(partial_result_port, partial_result_transform->getInputs().front());\n-\n-        partial_result_port = &partial_result_transform->getOutputs().front();\n-\n-        addProcessor(std::move(partial_result_transform));\n-    }\n-}\n-\n-void Pipe::addPartialResultTransform(const ProcessorPtr & transform)\n-{\n-    if (isPartialResultActive())\n-    {\n-        size_t new_outputs_size = 0;\n-        for (const auto & output : transform->getOutputs())\n-        {\n-            /// We do not use totals_port and extremes_port in partial result\n-            if ((totals_port && totals_port == &output) || (extremes_port && extremes_port == &output))\n-                continue;\n-            ++new_outputs_size;\n-        }\n-\n-        auto partial_result_status = transform->getPartialResultProcessorSupportStatus();\n-\n-        if (partial_result_status == IProcessor::PartialResultStatus::SkipSupported && new_outputs_size != partial_result_ports.size())\n-            throw Exception(\n-                ErrorCodes::LOGICAL_ERROR,\n-                \"Cannot skip transform {} in the partial result part of the Pipe because it has {} output ports, but the partial result part expects {} output ports\",\n-                transform->getName(),\n-                new_outputs_size,\n-                partial_result_ports.size());\n-\n-        if (partial_result_status == IProcessor::PartialResultStatus::NotSupported)\n-        {\n-            for (auto & partial_result_port : partial_result_ports)\n-                dropPort(partial_result_port, *processors, collected_processors);\n-\n-            partial_result_ports.assign(new_outputs_size, nullptr);\n-            return;\n-        }\n-\n-        if (partial_result_status != IProcessor::PartialResultStatus::FullSupported)\n-            return;\n-\n-        auto partial_result_transform = IProcessor::getPartialResultProcessorPtr(transform, partial_result_limit, partial_result_duration_ms);\n-        auto & inputs = partial_result_transform->getInputs();\n-\n-        if (inputs.size() != partial_result_ports.size())\n-        {\n-            WriteBufferFromOwnString out;\n-            if (processors && !processors->empty())\n-                printPipeline(*processors, out);\n-\n-            throw Exception(\n-                ErrorCodes::LOGICAL_ERROR,\n-                \"Cannot add partial result transform {} to Pipe because it has {} input ports, but {} expected\\n{}\",\n-                partial_result_transform->getName(),\n-                inputs.size(),\n-                partial_result_ports.size(), out.str());\n-        }\n-\n-        size_t next_port = 0;\n-        for (auto & input : inputs)\n-        {\n-            connectPartialResultPort(partial_result_ports[next_port], input);\n-            ++next_port;\n-        }\n-\n-        partial_result_ports.assign(new_outputs_size, nullptr);\n-\n-        next_port = 0;\n-        for (auto & new_partial_result_port : partial_result_transform->getOutputs())\n-        {\n-            partial_result_ports[next_port] = &new_partial_result_port;\n-            ++next_port;\n-        }\n-\n-        addProcessor(std::move(partial_result_transform));\n-    }\n-}\n-\n-void Pipe::connectPartialResultPort(OutputPort * partial_result_port, InputPort & partial_result_transform_port)\n-{\n-    if (partial_result_port == nullptr)\n-    {\n-        auto source = std::make_shared<NullSource>(getHeader());\n-        partial_result_port = &source->getPort();\n+    if (collected_processors)\n+        collected_processors->emplace_back(transform);\n \n-        addProcessor(std::move(source));\n-    }\n+    processors->emplace_back(std::move(transform));\n \n-    connect(*partial_result_port, partial_result_transform_port);\n+    max_parallel_streams = std::max<size_t>(max_parallel_streams, output_ports.size());\n }\n \n void Pipe::addSimpleTransform(const ProcessorGetterWithStreamKind & getter)\n@@ -744,7 +612,7 @@ void Pipe::addSimpleTransform(const ProcessorGetterWithStreamKind & getter)\n \n     Block new_header;\n \n-    auto add_transform = [&](OutputPort *& port, size_t partial_result_port_id, StreamType stream_type)\n+    auto add_transform = [&](OutputPort *& port, StreamType stream_type)\n     {\n         if (!port)\n             return;\n@@ -780,22 +648,19 @@ void Pipe::addSimpleTransform(const ProcessorGetterWithStreamKind & getter)\n         {\n             connect(*port, transform->getInputs().front());\n             port = &transform->getOutputs().front();\n-            if (stream_type == StreamType::Main)\n-                addPartialResultSimpleTransform(transform, partial_result_port_id);\n \n-            addProcessor(std::move(transform));\n+            if (collected_processors)\n+                collected_processors->emplace_back(transform);\n+\n+            processors->emplace_back(std::move(transform));\n         }\n     };\n \n-    size_t partial_result_port_id = 0;\n     for (auto & port : output_ports)\n-    {\n-        add_transform(port, partial_result_port_id, StreamType::Main);\n-        ++partial_result_port_id;\n-    }\n+        add_transform(port, StreamType::Main);\n \n-    add_transform(totals_port, 0, StreamType::Totals);\n-    add_transform(extremes_port, 0, StreamType::Extremes);\n+    add_transform(totals_port, StreamType::Totals);\n+    add_transform(extremes_port, StreamType::Extremes);\n \n     header = std::move(new_header);\n }\n@@ -816,7 +681,6 @@ void Pipe::addChains(std::vector<Chain> chains)\n \n     dropTotals();\n     dropExtremes();\n-    dropPartialResult();\n \n     size_t max_parallel_streams_for_chains = 0;\n \n@@ -835,21 +699,18 @@ void Pipe::addChains(std::vector<Chain> chains)\n \n         auto added_processors = Chain::getProcessors(std::move(chains[i]));\n         for (auto & transform : added_processors)\n-            addProcessor(std::move(transform));\n+        {\n+            if (collected_processors)\n+                collected_processors->emplace_back(transform);\n+\n+            processors->emplace_back(std::move(transform));\n+        }\n     }\n \n     header = std::move(new_header);\n     max_parallel_streams = std::max(max_parallel_streams, max_parallel_streams_for_chains);\n }\n \n-void Pipe::addProcessor(ProcessorPtr processor)\n-{\n-    if (collected_processors)\n-        collected_processors->emplace_back(processor);\n-\n-    processors->emplace_back(std::move(processor));\n-}\n-\n void Pipe::resize(size_t num_streams, bool force, bool strict)\n {\n     if (output_ports.empty())\n@@ -910,9 +771,6 @@ void Pipe::setSinks(const Pipe::ProcessorGetterWithStreamKind & getter)\n     add_transform(totals_port, StreamType::Totals);\n     add_transform(extremes_port, StreamType::Extremes);\n \n-    for (auto & port : partial_result_ports)\n-        add_transform(port, StreamType::PartialResult);\n-\n     output_ports.clear();\n     header.clear();\n }\n@@ -922,9 +780,6 @@ void Pipe::transform(const Transformer & transformer, bool check_ports)\n     if (output_ports.empty())\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot transform empty Pipe\");\n \n-    /// TODO: Add functionality to work with partial result ports in transformer.\n-    dropPartialResult();\n-\n     auto new_processors = transformer(output_ports);\n \n     /// Create hash table with new processors.\n@@ -1014,10 +869,5 @@ void Pipe::transform(const Transformer & transformer, bool check_ports)\n     max_parallel_streams = std::max<size_t>(max_parallel_streams, output_ports.size());\n }\n \n-OutputPort * Pipe::getPartialResultPort(size_t pos) const\n-{\n-    return partial_result_ports.empty() ? nullptr : partial_result_ports[pos];\n-}\n-\n \n }\ndiff --git a/src/QueryPipeline/Pipe.h b/src/QueryPipeline/Pipe.h\nindex a6bd46a325bd..09931e385782 100644\n--- a/src/QueryPipeline/Pipe.h\n+++ b/src/QueryPipeline/Pipe.h\n@@ -48,9 +48,6 @@ class Pipe\n     OutputPort * getOutputPort(size_t pos) const { return output_ports[pos]; }\n     OutputPort * getTotalsPort() const { return totals_port; }\n     OutputPort * getExtremesPort() const { return extremes_port; }\n-    OutputPort * getPartialResultPort(size_t pos) const;\n-\n-    bool isPartialResultActive() { return is_partial_result_active; }\n \n     /// Add processor to list, add it output ports to output_ports.\n     /// Processor shouldn't have input ports, output ports shouldn't be connected.\n@@ -61,13 +58,9 @@ class Pipe\n     void addTotalsSource(ProcessorPtr source);\n     void addExtremesSource(ProcessorPtr source);\n \n-    /// Activate sending partial result during main pipeline execution\n-    void activatePartialResult(UInt64 partial_result_limit_, UInt64 partial_result_duration_ms_);\n-\n-    /// Drop totals, extremes and partial result (create NullSink for them).\n+    /// Drop totals and extremes (create NullSink for them).\n     void dropTotals();\n     void dropExtremes();\n-    void dropPartialResult();\n \n     /// Add processor to list. It should have size() input ports with compatible header.\n     /// Output ports should have same headers.\n@@ -76,16 +69,11 @@ class Pipe\n     void addTransform(ProcessorPtr transform, OutputPort * totals, OutputPort * extremes);\n     void addTransform(ProcessorPtr transform, InputPort * totals, InputPort * extremes);\n \n-    void addPartialResultTransform(const ProcessorPtr & transform);\n-    void addPartialResultSimpleTransform(const ProcessorPtr & transform, size_t partial_result_port_id);\n-    void connectPartialResultPort(OutputPort * partial_result_port, InputPort & partial_result_transform_port);\n-\n     enum class StreamType\n     {\n         Main = 0, /// Stream for query data. There may be several streams of this type.\n         Totals,  /// Stream for totals. No more than one.\n         Extremes, /// Stream for extremes. No more than one.\n-        PartialResult, /// Stream for partial result data. There may be several streams of this type.\n     };\n \n     using ProcessorGetter = std::function<ProcessorPtr(const Block & header)>;\n@@ -121,17 +109,10 @@ class Pipe\n     Block header;\n     std::shared_ptr<Processors> processors;\n \n-    /// If the variable is true, then each time a processor is added pipe will try\n-    /// to add processor which will send partial result from original processor\n-    bool is_partial_result_active = false;\n-    UInt64 partial_result_limit = 0;\n-    UInt64 partial_result_duration_ms = 0;\n-\n-    /// Output ports. Totals, extremes and partial results are allowed to be empty.\n+    /// Output ports. Totals and extremes are allowed to be empty.\n     OutputPortRawPtrs output_ports;\n     OutputPort * totals_port = nullptr;\n     OutputPort * extremes_port = nullptr;\n-    OutputPortRawPtrs partial_result_ports;\n \n     /// It is the max number of processors which can be executed in parallel for each step.\n     /// Usually, it's the same as the number of output ports.\n@@ -147,8 +128,6 @@ class Pipe\n     static Pipe unitePipes(Pipes pipes, Processors * collected_processors, bool allow_empty_header);\n     void setSinks(const Pipe::ProcessorGetterWithStreamKind & getter);\n \n-    void addProcessor(ProcessorPtr processor);\n-\n     friend class QueryPipelineBuilder;\n     friend class QueryPipeline;\n };\ndiff --git a/src/QueryPipeline/QueryPipeline.cpp b/src/QueryPipeline/QueryPipeline.cpp\nindex 4ce0aa029bed..935c006c2178 100644\n--- a/src/QueryPipeline/QueryPipeline.cpp\n+++ b/src/QueryPipeline/QueryPipeline.cpp\n@@ -73,8 +73,7 @@ static void checkPulling(\n     Processors & processors,\n     OutputPort * output,\n     OutputPort * totals,\n-    OutputPort * extremes,\n-    OutputPort * partial_result)\n+    OutputPort * extremes)\n {\n     if (!output || output->isConnected())\n         throw Exception(\n@@ -91,15 +90,9 @@ static void checkPulling(\n             ErrorCodes::LOGICAL_ERROR,\n             \"Cannot create pulling QueryPipeline because its extremes port is connected\");\n \n-    if (partial_result && partial_result->isConnected())\n-        throw Exception(\n-            ErrorCodes::LOGICAL_ERROR,\n-            \"Cannot create pulling QueryPipeline because its partial_result port is connected\");\n-\n     bool found_output = false;\n     bool found_totals = false;\n     bool found_extremes = false;\n-    bool found_partial_result = false;\n     for (const auto & processor : processors)\n     {\n         for (const auto & in : processor->getInputs())\n@@ -113,8 +106,6 @@ static void checkPulling(\n                 found_totals = true;\n             else if (extremes && &out == extremes)\n                 found_extremes = true;\n-            else if (partial_result && &out == partial_result)\n-                found_partial_result = true;\n             else\n                 checkOutput(out, processor, processors);\n         }\n@@ -132,10 +123,6 @@ static void checkPulling(\n         throw Exception(\n             ErrorCodes::LOGICAL_ERROR,\n             \"Cannot create pulling QueryPipeline because its extremes port does not belong to any processor\");\n-    if (partial_result && !found_partial_result)\n-        throw Exception(\n-            ErrorCodes::LOGICAL_ERROR,\n-            \"Cannot create pulling QueryPipeline because its partial result port does not belong to any processor\");\n }\n \n static void checkCompleted(Processors & processors)\n@@ -338,20 +325,17 @@ QueryPipeline::QueryPipeline(\n     std::shared_ptr<Processors> processors_,\n     OutputPort * output_,\n     OutputPort * totals_,\n-    OutputPort * extremes_,\n-    OutputPort * partial_result_)\n+    OutputPort * extremes_)\n     : resources(std::move(resources_))\n     , processors(std::move(processors_))\n     , output(output_)\n     , totals(totals_)\n     , extremes(extremes_)\n-    , partial_result(partial_result_)\n {\n-    checkPulling(*processors, output, totals, extremes, partial_result);\n+    checkPulling(*processors, output, totals, extremes);\n }\n \n QueryPipeline::QueryPipeline(Pipe pipe)\n-    : partial_result_duration_ms(pipe.partial_result_duration_ms)\n {\n     if (pipe.numOutputPorts() > 0)\n     {\n@@ -359,11 +343,8 @@ QueryPipeline::QueryPipeline(Pipe pipe)\n         output = pipe.getOutputPort(0);\n         totals = pipe.getTotalsPort();\n         extremes = pipe.getExtremesPort();\n-        partial_result = pipe.getPartialResultPort(0);\n-        num_threads = pipe.max_parallel_streams;\n-\n         processors = std::move(pipe.processors);\n-        checkPulling(*processors, output, totals, extremes, partial_result);\n+        checkPulling(*processors, output, totals, extremes);\n     }\n     else\n     {\n@@ -395,7 +376,6 @@ QueryPipeline::QueryPipeline(std::shared_ptr<IOutputFormat> format)\n     auto & format_main = format->getPort(IOutputFormat::PortKind::Main);\n     auto & format_totals = format->getPort(IOutputFormat::PortKind::Totals);\n     auto & format_extremes = format->getPort(IOutputFormat::PortKind::Extremes);\n-    auto & format_partial_result = format->getPort(IOutputFormat::PortKind::PartialResult);\n \n     if (!totals)\n     {\n@@ -411,21 +391,12 @@ QueryPipeline::QueryPipeline(std::shared_ptr<IOutputFormat> format)\n         processors->emplace_back(std::move(source));\n     }\n \n-    if (!partial_result)\n-    {\n-        auto source = std::make_shared<NullSource>(format_partial_result.getHeader());\n-        partial_result = &source->getPort();\n-        processors->emplace_back(std::move(source));\n-    }\n-\n     connect(*totals, format_totals);\n     connect(*extremes, format_extremes);\n-    connect(*partial_result, format_partial_result);\n \n     input = &format_main;\n     totals = nullptr;\n     extremes = nullptr;\n-    partial_result = nullptr;\n \n     output_format = format.get();\n \n@@ -453,7 +424,6 @@ void QueryPipeline::complete(std::shared_ptr<ISink> sink)\n \n     drop(totals, *processors);\n     drop(extremes, *processors);\n-    drop(partial_result, *processors);\n \n     connect(*output, sink->getPort());\n     processors->emplace_back(std::move(sink));\n@@ -469,7 +439,6 @@ void QueryPipeline::complete(Chain chain)\n \n     drop(totals, *processors);\n     drop(extremes, *processors);\n-    drop(partial_result, *processors);\n \n     processors->reserve(processors->size() + chain.getProcessors().size() + 1);\n     for (auto processor : chain.getProcessors())\n@@ -495,7 +464,6 @@ void QueryPipeline::complete(Pipe pipe)\n     pipe.resize(1);\n     pipe.dropExtremes();\n     pipe.dropTotals();\n-    pipe.dropPartialResult();\n     connect(*pipe.getOutputPort(0), *input);\n     input = nullptr;\n \n@@ -524,13 +492,11 @@ void QueryPipeline::complete(std::shared_ptr<IOutputFormat> format)\n         addMaterializing(output, *processors);\n         addMaterializing(totals, *processors);\n         addMaterializing(extremes, *processors);\n-        addMaterializing(partial_result, *processors);\n     }\n \n     auto & format_main = format->getPort(IOutputFormat::PortKind::Main);\n     auto & format_totals = format->getPort(IOutputFormat::PortKind::Totals);\n     auto & format_extremes = format->getPort(IOutputFormat::PortKind::Extremes);\n-    auto & format_partial_result = format->getPort(IOutputFormat::PortKind::PartialResult);\n \n     if (!totals)\n     {\n@@ -546,22 +512,13 @@ void QueryPipeline::complete(std::shared_ptr<IOutputFormat> format)\n         processors->emplace_back(std::move(source));\n     }\n \n-    if (!partial_result)\n-    {\n-        auto source = std::make_shared<NullSource>(format_partial_result.getHeader());\n-        partial_result = &source->getPort();\n-        processors->emplace_back(std::move(source));\n-    }\n-\n     connect(*output, format_main);\n     connect(*totals, format_totals);\n     connect(*extremes, format_extremes);\n-    connect(*partial_result, format_partial_result);\n \n     output = nullptr;\n     totals = nullptr;\n     extremes = nullptr;\n-    partial_result = nullptr;\n \n     initRowsBeforeLimit(format.get());\n     output_format = format.get();\n@@ -733,7 +690,6 @@ void QueryPipeline::convertStructureTo(const ColumnsWithTypeAndName & columns)\n     addExpression(output, actions, *processors);\n     addExpression(totals, actions, *processors);\n     addExpression(extremes, actions, *processors);\n-    addExpression(partial_result, actions, *processors);\n }\n \n std::unique_ptr<ReadProgressCallback> QueryPipeline::getReadProgressCallback() const\ndiff --git a/src/QueryPipeline/QueryPipeline.h b/src/QueryPipeline/QueryPipeline.h\nindex 20e58bc0f59f..f14cf61aac24 100644\n--- a/src/QueryPipeline/QueryPipeline.h\n+++ b/src/QueryPipeline/QueryPipeline.h\n@@ -75,8 +75,7 @@ class QueryPipeline\n         std::shared_ptr<Processors> processors_,\n         OutputPort * output_,\n         OutputPort * totals_ = nullptr,\n-        OutputPort * extremes_ = nullptr,\n-        OutputPort * partial_result_ = nullptr);\n+        OutputPort * extremes_ = nullptr);\n \n     bool initialized() const { return !processors->empty(); }\n     /// When initialized, exactly one of the following is true.\n@@ -155,7 +154,6 @@ class QueryPipeline\n     OutputPort * output = nullptr;\n     OutputPort * totals = nullptr;\n     OutputPort * extremes = nullptr;\n-    OutputPort * partial_result = nullptr;\n \n     QueryStatusPtr process_list_element;\n \n@@ -164,9 +162,6 @@ class QueryPipeline\n     size_t num_threads = 0;\n     bool concurrency_control = false;\n \n-    UInt64 partial_result_limit = 0;\n-    UInt64 partial_result_duration_ms = 0;\n-\n     friend class PushingPipelineExecutor;\n     friend class PullingPipelineExecutor;\n     friend class PushingAsyncPipelineExecutor;\ndiff --git a/src/QueryPipeline/QueryPipelineBuilder.cpp b/src/QueryPipeline/QueryPipelineBuilder.cpp\nindex e176e8585f59..f9726339872d 100644\n--- a/src/QueryPipeline/QueryPipelineBuilder.cpp\n+++ b/src/QueryPipeline/QueryPipelineBuilder.cpp\n@@ -110,15 +110,6 @@ void QueryPipelineBuilder::init(QueryPipeline & pipeline)\n         pipe.header = {};\n     }\n \n-    if (pipeline.partial_result)\n-    {\n-        /// Set partial result ports only after activation because when activated, it is set to nullptr\n-        pipe.activatePartialResult(pipeline.partial_result_limit, pipeline.partial_result_duration_ms);\n-        pipe.partial_result_ports = {pipeline.partial_result};\n-    }\n-    else\n-        pipe.dropPartialResult();\n-\n     pipe.totals_port = pipeline.totals;\n     pipe.extremes_port = pipeline.extremes;\n     pipe.max_parallel_streams = pipeline.num_threads;\n@@ -361,10 +352,6 @@ std::unique_ptr<QueryPipelineBuilder> QueryPipelineBuilder::joinPipelinesYShaped\n     left->checkInitializedAndNotCompleted();\n     right->checkInitializedAndNotCompleted();\n \n-    /// TODO: Support joining of partial results from different pipelines.\n-    left->pipe.dropPartialResult();\n-    right->pipe.dropPartialResult();\n-\n     left->pipe.dropExtremes();\n     right->pipe.dropExtremes();\n     if (left->getNumStreams() != 1 || right->getNumStreams() != 1)\n@@ -377,7 +364,6 @@ std::unique_ptr<QueryPipelineBuilder> QueryPipelineBuilder::joinPipelinesYShaped\n \n     auto joining = std::make_shared<MergeJoinTransform>(join, inputs, out_header, max_block_size);\n \n-    /// TODO: Support partial results in merge pipelines after joining support above.\n     return mergePipelines(std::move(left), std::move(right), std::move(joining), collected_processors);\n }\n \n@@ -398,10 +384,6 @@ std::unique_ptr<QueryPipelineBuilder> QueryPipelineBuilder::joinPipelinesRightLe\n     left->pipe.dropExtremes();\n     right->pipe.dropExtremes();\n \n-    /// TODO: Support joining of partial results from different pipelines.\n-    left->pipe.dropPartialResult();\n-    right->pipe.dropPartialResult();\n-\n     left->pipe.collected_processors = collected_processors;\n \n     /// Collect the NEW processors for the right pipeline.\n@@ -652,7 +634,7 @@ PipelineExecutorPtr QueryPipelineBuilder::execute()\n     if (!isCompleted())\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot execute pipeline because it is not completed\");\n \n-    return std::make_shared<PipelineExecutor>(pipe.processors, process_list_element, pipe.partial_result_duration_ms);\n+    return std::make_shared<PipelineExecutor>(pipe.processors, process_list_element);\n }\n \n Pipe QueryPipelineBuilder::getPipe(QueryPipelineBuilder pipeline, QueryPlanResourceHolder & resources)\ndiff --git a/src/QueryPipeline/QueryPipelineBuilder.h b/src/QueryPipeline/QueryPipelineBuilder.h\nindex cee545ac29d5..5d273df70686 100644\n--- a/src/QueryPipeline/QueryPipelineBuilder.h\n+++ b/src/QueryPipeline/QueryPipelineBuilder.h\n@@ -85,15 +85,6 @@ class QueryPipelineBuilder\n     /// Pipeline will be completed after this transformation.\n     void setSinks(const Pipe::ProcessorGetterWithStreamKind & getter);\n \n-    /// Activate building separate pipeline for sending partial result.\n-    void activatePartialResult(UInt64 partial_result_limit, UInt64 partial_result_duration_ms)\n-    {\n-        pipe.activatePartialResult(partial_result_limit, partial_result_duration_ms);\n-    }\n-\n-    /// Check if building of a pipeline for sending partial result active.\n-    bool isPartialResultActive() { return pipe.isPartialResultActive(); }\n-\n     /// Add totals which returns one chunk with single row with defaults.\n     void addDefaultTotals();\n \ndiff --git a/src/Server/TCPHandler.cpp b/src/Server/TCPHandler.cpp\nindex 4908bf82b460..871606c62982 100644\n--- a/src/Server/TCPHandler.cpp\n+++ b/src/Server/TCPHandler.cpp\n@@ -10,8 +10,6 @@\n #include <vector>\n #include <string_view>\n #include <cstring>\n-#include <base/types.h>\n-#include <base/scope_guard.h>\n #include <Poco/Net/NetException.h>\n #include <Poco/Net/SocketAddress.h>\n #include <Poco/Util/LayeredConfiguration.h>\n@@ -106,7 +104,6 @@ namespace DB::ErrorCodes\n     extern const int TIMEOUT_EXCEEDED;\n     extern const int SUPPORT_IS_DISABLED;\n     extern const int UNSUPPORTED_METHOD;\n-    extern const int FUNCTION_NOT_ALLOWED;\n }\n \n namespace\n@@ -965,14 +962,7 @@ void TCPHandler::processOrdinaryQueryWithProcessors()\n     std::unique_lock progress_lock(task_callback_mutex, std::defer_lock);\n \n     {\n-        const auto & settings = query_context->getSettingsRef();\n-        bool has_partial_result_setting = settings.partial_result_update_duration_ms.totalMilliseconds() > 0;\n-        if (has_partial_result_setting && !settings.allow_experimental_partial_result)\n-            throw Exception(ErrorCodes::FUNCTION_NOT_ALLOWED,\n-                \"Partial results are not allowed by default, it's an experimental feature. \"\n-                \"Setting 'allow_experimental_partial_result' must be enabled to use 'partial_result_update_duration_ms'\");\n-\n-        PullingAsyncPipelineExecutor executor(pipeline, has_partial_result_setting);\n+        PullingAsyncPipelineExecutor executor(pipeline);\n         CurrentMetrics::Increment query_thread_metric_increment{CurrentMetrics::QueryThread};\n \n         Block block;\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02010_lc_native.python b/tests/queries/0_stateless/02010_lc_native.python\nindex 219fdf044722..6c4220855c86 100755\n--- a/tests/queries/0_stateless/02010_lc_native.python\n+++ b/tests/queries/0_stateless/02010_lc_native.python\n@@ -1,33 +1,227 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n \n+import socket\n import os\n-import sys\n+import uuid\n \n-CURDIR = os.path.dirname(os.path.realpath(__file__))\n-sys.path.insert(0, os.path.join(CURDIR, \"helpers\"))\n+CLICKHOUSE_HOST = os.environ.get(\"CLICKHOUSE_HOST\", \"127.0.0.1\")\n+CLICKHOUSE_PORT = int(os.environ.get(\"CLICKHOUSE_PORT_TCP\", \"900000\"))\n+CLICKHOUSE_DATABASE = os.environ.get(\"CLICKHOUSE_DATABASE\", \"default\")\n+CLIENT_NAME = \"simple native protocol\"\n \n-from tcp_client import (\n-    TCPClient,\n-    CLICKHOUSE_DATABASE,\n-    writeVarUInt,\n-    writeStringBinary,\n-    serializeBlockInfo,\n-    assertPacket,\n-)\n+\n+def writeVarUInt(x, ba):\n+    for _ in range(0, 9):\n+        byte = x & 0x7F\n+        if x > 0x7F:\n+            byte |= 0x80\n+\n+        ba.append(byte)\n+\n+        x >>= 7\n+        if x == 0:\n+            return\n+\n+\n+def writeStringBinary(s, ba):\n+    b = bytes(s, \"utf-8\")\n+    writeVarUInt(len(s), ba)\n+    ba.extend(b)\n+\n+\n+def readStrict(s, size=1):\n+    res = bytearray()\n+    while size:\n+        cur = s.recv(size)\n+        # if not res:\n+        #     raise \"Socket is closed\"\n+        size -= len(cur)\n+        res.extend(cur)\n+\n+    return res\n+\n+\n+def readUInt(s, size=1):\n+    res = readStrict(s, size)\n+    val = 0\n+    for i in range(len(res)):\n+        val += res[i] << (i * 8)\n+    return val\n+\n+\n+def readUInt8(s):\n+    return readUInt(s)\n+\n+\n+def readUInt16(s):\n+    return readUInt(s, 2)\n+\n+\n+def readUInt32(s):\n+    return readUInt(s, 4)\n+\n+\n+def readUInt64(s):\n+    return readUInt(s, 8)\n+\n+\n+def readVarUInt(s):\n+    x = 0\n+    for i in range(9):\n+        byte = readStrict(s)[0]\n+        x |= (byte & 0x7F) << (7 * i)\n+\n+        if not byte & 0x80:\n+            return x\n+\n+    return x\n+\n+\n+def readStringBinary(s):\n+    size = readVarUInt(s)\n+    s = readStrict(s, size)\n+    return s.decode(\"utf-8\")\n+\n+\n+def sendHello(s):\n+    ba = bytearray()\n+    writeVarUInt(0, ba)  # Hello\n+    writeStringBinary(CLIENT_NAME, ba)\n+    writeVarUInt(21, ba)\n+    writeVarUInt(9, ba)\n+    writeVarUInt(54449, ba)\n+    writeStringBinary(\"default\", ba)  # database\n+    writeStringBinary(\"default\", ba)  # user\n+    writeStringBinary(\"\", ba)  # pwd\n+    s.sendall(ba)\n+\n+\n+def receiveHello(s):\n+    p_type = readVarUInt(s)\n+    assert p_type == 0  # Hello\n+    server_name = readStringBinary(s)\n+    # print(\"Server name: \", server_name)\n+    server_version_major = readVarUInt(s)\n+    # print(\"Major: \", server_version_major)\n+    server_version_minor = readVarUInt(s)\n+    # print(\"Minor: \", server_version_minor)\n+    server_revision = readVarUInt(s)\n+    # print(\"Revision: \", server_revision)\n+    server_timezone = readStringBinary(s)\n+    # print(\"Timezone: \", server_timezone)\n+    server_display_name = readStringBinary(s)\n+    # print(\"Display name: \", server_display_name)\n+    server_version_patch = readVarUInt(s)\n+    # print(\"Version patch: \", server_version_patch)\n+\n+\n+def serializeClientInfo(ba, query_id):\n+    writeStringBinary(\"default\", ba)  # initial_user\n+    writeStringBinary(query_id, ba)  # initial_query_id\n+    writeStringBinary(\"127.0.0.1:9000\", ba)  # initial_address\n+    ba.extend([0] * 8)  # initial_query_start_time_microseconds\n+    ba.append(1)  # TCP\n+    writeStringBinary(\"os_user\", ba)  # os_user\n+    writeStringBinary(\"client_hostname\", ba)  # client_hostname\n+    writeStringBinary(CLIENT_NAME, ba)  # client_name\n+    writeVarUInt(21, ba)\n+    writeVarUInt(9, ba)\n+    writeVarUInt(54449, ba)\n+    writeStringBinary(\"\", ba)  # quota_key\n+    writeVarUInt(0, ba)  # distributed_depth\n+    writeVarUInt(1, ba)  # client_version_patch\n+    ba.append(0)  # No telemetry\n+\n+\n+def sendQuery(s, query):\n+    ba = bytearray()\n+    query_id = uuid.uuid4().hex\n+    writeVarUInt(1, ba)  # query\n+    writeStringBinary(query_id, ba)\n+\n+    ba.append(1)  # INITIAL_QUERY\n+\n+    # client info\n+    serializeClientInfo(ba, query_id)\n+\n+    writeStringBinary(\"\", ba)  # No settings\n+    writeStringBinary(\"\", ba)  # No interserver secret\n+    writeVarUInt(2, ba)  # Stage - Complete\n+    ba.append(0)  # No compression\n+    writeStringBinary(query, ba)  # query, finally\n+    s.sendall(ba)\n+\n+\n+def serializeBlockInfo(ba):\n+    writeVarUInt(1, ba)  # 1\n+    ba.append(0)  # is_overflows\n+    writeVarUInt(2, ba)  # 2\n+    writeVarUInt(0, ba)  # 0\n+    ba.extend([0] * 4)  # bucket_num\n+\n+\n+def sendEmptyBlock(s):\n+    ba = bytearray()\n+    writeVarUInt(2, ba)  # Data\n+    writeStringBinary(\"\", ba)\n+    serializeBlockInfo(ba)\n+    writeVarUInt(0, ba)  # rows\n+    writeVarUInt(0, ba)  # columns\n+    s.sendall(ba)\n+\n+\n+def assertPacket(packet, expected):\n+    assert packet == expected, packet\n+\n+\n+def readHeader(s):\n+    packet_type = readVarUInt(s)\n+    if packet_type == 2:  # Exception\n+        raise RuntimeError(readException(s))\n+    assertPacket(packet_type, 1)  # Data\n+\n+    readStringBinary(s)  # external table name\n+    # BlockInfo\n+    assertPacket(readVarUInt(s), 1)  # 1\n+    assertPacket(readUInt8(s), 0)  # is_overflows\n+    assertPacket(readVarUInt(s), 2)  # 2\n+    assertPacket(readUInt32(s), 4294967295)  # bucket_num\n+    assertPacket(readVarUInt(s), 0)  # 0\n+    columns = readVarUInt(s)  # rows\n+    rows = readVarUInt(s)  # columns\n+    print(\"Rows {} Columns {}\".format(rows, columns))\n+    for _ in range(columns):\n+        col_name = readStringBinary(s)\n+        type_name = readStringBinary(s)\n+        print(\"Column {} type {}\".format(col_name, type_name))\n+\n+\n+def readException(s):\n+    code = readUInt32(s)\n+    name = readStringBinary(s)\n+    text = readStringBinary(s)\n+    readStringBinary(s)  # trace\n+    assertPacket(readUInt8(s), 0)  # has_nested\n+    return \"code {}: {}\".format(code, text.replace(\"DB::Exception:\", \"\"))\n \n \n def insertValidLowCardinalityRow():\n-    with TCPClient() as client:\n-        client.sendQuery(\n+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n+        s.settimeout(30)\n+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))\n+        sendHello(s)\n+        receiveHello(s)\n+        sendQuery(\n+            s,\n             \"insert into {}.tab settings input_format_defaults_for_omitted_fields=0 format TSV\".format(\n                 CLICKHOUSE_DATABASE\n             ),\n         )\n \n         # external tables\n-        client.sendEmptyBlock()\n-        client.readHeader()\n+        sendEmptyBlock(s)\n+        readHeader(s)\n \n         # Data\n         ba = bytearray()\n@@ -46,25 +240,31 @@ def insertValidLowCardinalityRow():\n         writeStringBinary(\"hello\", ba)  # key\n         ba.extend([1] + [0] * 7)  # num_indexes\n         ba.extend([0] * 8)  # UInt64 index (0 for 'hello')\n-        client.send(ba)\n+        s.sendall(ba)\n \n         # Fin block\n-        client.sendEmptyBlock()\n+        sendEmptyBlock(s)\n \n-        assertPacket(client.readVarUInt(), 5)  # End of stream\n+        assertPacket(readVarUInt(s), 5)  # End of stream\n+        s.close()\n \n \n def insertLowCardinalityRowWithIndexOverflow():\n-    with TCPClient() as client:\n-        client.sendQuery(\n+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n+        s.settimeout(30)\n+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))\n+        sendHello(s)\n+        receiveHello(s)\n+        sendQuery(\n+            s,\n             \"insert into {}.tab settings input_format_defaults_for_omitted_fields=0 format TSV\".format(\n                 CLICKHOUSE_DATABASE\n             ),\n         )\n \n         # external tables\n-        client.sendEmptyBlock()\n-        client.readHeader()\n+        sendEmptyBlock(s)\n+        readHeader(s)\n \n         # Data\n         ba = bytearray()\n@@ -83,23 +283,29 @@ def insertLowCardinalityRowWithIndexOverflow():\n         writeStringBinary(\"hello\", ba)  # key\n         ba.extend([1] + [0] * 7)  # num_indexes\n         ba.extend([0] * 7 + [1])  # UInt64 index (overflow)\n-        client.send(ba)\n+        s.sendall(ba)\n \n-        assertPacket(client.readVarUInt(), 2)  # Exception\n-        print(client.readException())\n+        assertPacket(readVarUInt(s), 2)\n+        print(readException(s))\n+        s.close()\n \n \n def insertLowCardinalityRowWithIncorrectDictType():\n-    with TCPClient() as client:\n-        client.sendQuery(\n+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n+        s.settimeout(30)\n+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))\n+        sendHello(s)\n+        receiveHello(s)\n+        sendQuery(\n+            s,\n             \"insert into {}.tab settings input_format_defaults_for_omitted_fields=0 format TSV\".format(\n                 CLICKHOUSE_DATABASE\n             ),\n         )\n \n         # external tables\n-        client.sendEmptyBlock()\n-        client.readHeader()\n+        sendEmptyBlock(s)\n+        readHeader(s)\n \n         # Data\n         ba = bytearray()\n@@ -118,23 +324,29 @@ def insertLowCardinalityRowWithIncorrectDictType():\n         writeStringBinary(\"hello\", ba)  # key\n         ba.extend([1] + [0] * 7)  # num_indexes\n         ba.extend([0] * 8)  # UInt64 index (overflow)\n-        client.send(ba)\n+        s.sendall(ba)\n \n-        assertPacket(client.readVarUInt(), 2)  # Exception\n-        print(client.readException())\n+        assertPacket(readVarUInt(s), 2)\n+        print(readException(s))\n+        s.close()\n \n \n def insertLowCardinalityRowWithIncorrectAdditionalKeys():\n-    with TCPClient() as client:\n-        client.sendQuery(\n+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n+        s.settimeout(30)\n+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))\n+        sendHello(s)\n+        receiveHello(s)\n+        sendQuery(\n+            s,\n             \"insert into {}.tab settings input_format_defaults_for_omitted_fields=0 format TSV\".format(\n                 CLICKHOUSE_DATABASE\n             ),\n         )\n \n         # external tables\n-        client.sendEmptyBlock()\n-        client.readHeader()\n+        sendEmptyBlock(s)\n+        readHeader(s)\n \n         # Data\n         ba = bytearray()\n@@ -153,10 +365,11 @@ def insertLowCardinalityRowWithIncorrectAdditionalKeys():\n         writeStringBinary(\"hello\", ba)  # key\n         ba.extend([1] + [0] * 7)  # num_indexes\n         ba.extend([0] * 8)  # UInt64 index (0 for 'hello')\n-        client.send(ba)\n+        s.sendall(ba)\n \n-        assertPacket(client.readVarUInt(), 2)  # Exception\n-        print(client.readException())\n+        assertPacket(readVarUInt(s), 2)\n+        print(readException(s))\n+        s.close()\n \n \n def main():\ndiff --git a/tests/queries/0_stateless/02210_processors_profile_log.reference b/tests/queries/0_stateless/02210_processors_profile_log.reference\nindex f480236111f0..41543d0706ae 100644\n--- a/tests/queries/0_stateless/02210_processors_profile_log.reference\n+++ b/tests/queries/0_stateless/02210_processors_profile_log.reference\n@@ -38,5 +38,4 @@ LazyOutputFormat\t1\t1\t1\t0\t0\n LimitsCheckingTransform\t1\t1\t1\t1\t1\n NullSource\t1\t0\t0\t0\t0\n NullSource\t1\t0\t0\t0\t0\n-NullSource\t0\t0\t0\t0\t0\n SourceFromSingleChunk\t1\t0\t0\t1\t1\ndiff --git a/tests/queries/0_stateless/02458_insert_select_progress_tcp.python b/tests/queries/0_stateless/02458_insert_select_progress_tcp.python\nindex fdc64a8dba86..92240e109c1e 100644\n--- a/tests/queries/0_stateless/02458_insert_select_progress_tcp.python\n+++ b/tests/queries/0_stateless/02458_insert_select_progress_tcp.python\n@@ -1,30 +1,188 @@\n #!/usr/bin/env python3\n \n-import json\n+import socket\n import os\n-import sys\n+import uuid\n+import json\n+\n+CLICKHOUSE_HOST = os.environ.get(\"CLICKHOUSE_HOST\", \"127.0.0.1\")\n+CLICKHOUSE_PORT = int(os.environ.get(\"CLICKHOUSE_PORT_TCP\", \"900000\"))\n+CLICKHOUSE_DATABASE = os.environ.get(\"CLICKHOUSE_DATABASE\", \"default\")\n+CLIENT_NAME = \"simple native protocol\"\n+\n+\n+def writeVarUInt(x, ba):\n+    for _ in range(0, 9):\n+        byte = x & 0x7F\n+        if x > 0x7F:\n+            byte |= 0x80\n+\n+        ba.append(byte)\n+\n+        x >>= 7\n+        if x == 0:\n+            return\n+\n+\n+def writeStringBinary(s, ba):\n+    b = bytes(s, \"utf-8\")\n+    writeVarUInt(len(s), ba)\n+    ba.extend(b)\n+\n+\n+def readStrict(s, size=1):\n+    res = bytearray()\n+    while size:\n+        cur = s.recv(size)\n+        # if not res:\n+        #     raise \"Socket is closed\"\n+        size -= len(cur)\n+        res.extend(cur)\n+\n+    return res\n+\n+\n+def readUInt(s, size=1):\n+    res = readStrict(s, size)\n+    val = 0\n+    for i in range(len(res)):\n+        val += res[i] << (i * 8)\n+    return val\n+\n+\n+def readUInt8(s):\n+    return readUInt(s)\n+\n+\n+def readUInt16(s):\n+    return readUInt(s, 2)\n+\n+\n+def readUInt32(s):\n+    return readUInt(s, 4)\n+\n+\n+def readUInt64(s):\n+    return readUInt(s, 8)\n+\n+\n+def readVarUInt(s):\n+    x = 0\n+    for i in range(9):\n+        byte = readStrict(s)[0]\n+        x |= (byte & 0x7F) << (7 * i)\n+\n+        if not byte & 0x80:\n+            return x\n+\n+    return x\n+\n \n-CURDIR = os.path.dirname(os.path.realpath(__file__))\n-sys.path.insert(0, os.path.join(CURDIR, \"helpers\"))\n+def readStringBinary(s):\n+    size = readVarUInt(s)\n+    s = readStrict(s, size)\n+    return s.decode(\"utf-8\")\n \n-from tcp_client import TCPClient\n+\n+def sendHello(s):\n+    ba = bytearray()\n+    writeVarUInt(0, ba)  # Hello\n+    writeStringBinary(CLIENT_NAME, ba)\n+    writeVarUInt(21, ba)\n+    writeVarUInt(9, ba)\n+    writeVarUInt(54449, ba)\n+    writeStringBinary(CLICKHOUSE_DATABASE, ba)  # database\n+    writeStringBinary(\"default\", ba)  # user\n+    writeStringBinary(\"\", ba)  # pwd\n+    s.sendall(ba)\n+\n+\n+def receiveHello(s):\n+    p_type = readVarUInt(s)\n+    assert p_type == 0  # Hello\n+    server_name = readStringBinary(s)\n+    # print(\"Server name: \", server_name)\n+    server_version_major = readVarUInt(s)\n+    # print(\"Major: \", server_version_major)\n+    server_version_minor = readVarUInt(s)\n+    # print(\"Minor: \", server_version_minor)\n+    server_revision = readVarUInt(s)\n+    # print(\"Revision: \", server_revision)\n+    server_timezone = readStringBinary(s)\n+    # print(\"Timezone: \", server_timezone)\n+    server_display_name = readStringBinary(s)\n+    # print(\"Display name: \", server_display_name)\n+    server_version_patch = readVarUInt(s)\n+    # print(\"Version patch: \", server_version_patch)\n+\n+\n+def serializeClientInfo(ba, query_id):\n+    writeStringBinary(\"default\", ba)  # initial_user\n+    writeStringBinary(query_id, ba)  # initial_query_id\n+    writeStringBinary(\"127.0.0.1:9000\", ba)  # initial_address\n+    ba.extend([0] * 8)  # initial_query_start_time_microseconds\n+    ba.append(1)  # TCP\n+    writeStringBinary(\"os_user\", ba)  # os_user\n+    writeStringBinary(\"client_hostname\", ba)  # client_hostname\n+    writeStringBinary(CLIENT_NAME, ba)  # client_name\n+    writeVarUInt(21, ba)\n+    writeVarUInt(9, ba)\n+    writeVarUInt(54449, ba)\n+    writeStringBinary(\"\", ba)  # quota_key\n+    writeVarUInt(0, ba)  # distributed_depth\n+    writeVarUInt(1, ba)  # client_version_patch\n+    ba.append(0)  # No telemetry\n+\n+\n+def sendQuery(s, query):\n+    ba = bytearray()\n+    query_id = uuid.uuid4().hex\n+    writeVarUInt(1, ba)  # query\n+    writeStringBinary(query_id, ba)\n+\n+    ba.append(1)  # INITIAL_QUERY\n+\n+    # client info\n+    serializeClientInfo(ba, query_id)\n+\n+    writeStringBinary(\"\", ba)  # No settings\n+    writeStringBinary(\"\", ba)  # No interserver secret\n+    writeVarUInt(2, ba)  # Stage - Complete\n+    ba.append(0)  # No compression\n+    writeStringBinary(query, ba)  # query, finally\n+    s.sendall(ba)\n+\n+\n+def serializeBlockInfo(ba):\n+    writeVarUInt(1, ba)  # 1\n+    ba.append(0)  # is_overflows\n+    writeVarUInt(2, ba)  # 2\n+    writeVarUInt(0, ba)  # 0\n+    ba.extend([0] * 4)  # bucket_num\n+\n+\n+def sendEmptyBlock(s):\n+    ba = bytearray()\n+    writeVarUInt(2, ba)  # Data\n+    writeStringBinary(\"\", ba)\n+    serializeBlockInfo(ba)\n+    writeVarUInt(0, ba)  # rows\n+    writeVarUInt(0, ba)  # columns\n+    s.sendall(ba)\n+\n+\n+def assertPacket(packet, expected):\n+    assert packet == expected, packet\n \n \n class Progress:\n-    def __init__(\n-        self,\n-        read_rows=0,\n-        read_bytes=0,\n-        total_rows_to_read=0,\n-        written_rows=0,\n-        written_bytes=0,\n-    ):\n+    def __init__(self):\n         # NOTE: this is done in ctor to initialize __dict__\n-        self.read_rows = read_rows\n-        self.read_bytes = read_bytes\n-        self.total_rows_to_read = total_rows_to_read\n-        self.written_rows = written_rows\n-        self.written_bytes = written_bytes\n+        self.read_rows = 0\n+        self.read_bytes = 0\n+        self.total_rows_to_read = 0\n+        self.written_rows = 0\n+        self.written_bytes = 0\n \n     def __str__(self):\n         return json.dumps(self.__dict__)\n@@ -37,6 +195,13 @@ class Progress:\n         self.written_bytes += b.written_bytes\n         return self\n \n+    def readPacket(self, s):\n+        self.read_rows += readVarUInt(s)\n+        self.read_bytes += readVarUInt(s)\n+        self.total_rows_to_read += readVarUInt(s)\n+        self.written_rows += readVarUInt(s)\n+        self.written_bytes += readVarUInt(s)\n+\n     def __bool__(self):\n         return (\n             self.read_rows > 0\n@@ -47,25 +212,52 @@ class Progress:\n         )\n \n \n+def readProgress(s):\n+    packet_type = readVarUInt(s)\n+    if packet_type == 2:  # Exception\n+        raise RuntimeError(readException(s))\n+\n+    if packet_type == 5:  # End stream\n+        return None\n+\n+    assertPacket(packet_type, 3)  # Progress\n+\n+    progress = Progress()\n+    progress.readPacket(s)\n+    return progress\n+\n+\n+def readException(s):\n+    code = readUInt32(s)\n+    name = readStringBinary(s)\n+    text = readStringBinary(s)\n+    readStringBinary(s)  # trace\n+    assertPacket(readUInt8(s), 0)  # has_nested\n+    return \"code {}: {}\".format(code, text.replace(\"DB::Exception:\", \"\"))\n+\n+\n def main():\n-    with TCPClient() as client:\n+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n+        s.settimeout(30)\n+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))\n+        sendHello(s)\n+        receiveHello(s)\n         # For 1 second sleep and 1000ms of interactive_delay we definitelly should have non zero progress packet.\n         # NOTE: interactive_delay=0 cannot be used since in this case CompletedPipelineExecutor will not call cancelled callback.\n-        client.sendQuery(\n+        sendQuery(\n+            s,\n             \"insert into function null('_ Int') select sleep(1) from numbers(2) settings max_block_size=1, interactive_delay=1000\",\n         )\n \n         # external tables\n-        client.sendEmptyBlock()\n+        sendEmptyBlock(s)\n \n         summary_progress = Progress()\n         non_empty_progress_packets = 0\n         while True:\n-            progress_info = client.readProgress()\n-            if progress_info is None:\n+            progress = readProgress(s)\n+            if progress is None:\n                 break\n-\n-            progress = Progress(*progress_info)\n             summary_progress += progress\n             if progress:\n                 non_empty_progress_packets += 1\n@@ -76,6 +268,8 @@ def main():\n         # - 1 or 2 for each SELECT block\n         assert non_empty_progress_packets in (3, 4), f\"{non_empty_progress_packets=:}\"\n \n+        s.close()\n+\n \n if __name__ == \"__main__\":\n     main()\ndiff --git a/tests/queries/0_stateless/02750_settings_alias_tcp_protocol.python b/tests/queries/0_stateless/02750_settings_alias_tcp_protocol.python\nindex 1736807410f0..48b27d434eca 100644\n--- a/tests/queries/0_stateless/02750_settings_alias_tcp_protocol.python\n+++ b/tests/queries/0_stateless/02750_settings_alias_tcp_protocol.python\n@@ -1,23 +1,217 @@\n #!/usr/bin/env python3\n \n-\n+import socket\n import os\n-import sys\n+import uuid\n+import json\n+\n+CLICKHOUSE_HOST = os.environ.get(\"CLICKHOUSE_HOST\", \"127.0.0.1\")\n+CLICKHOUSE_PORT = int(os.environ.get(\"CLICKHOUSE_PORT_TCP\", \"900000\"))\n+CLICKHOUSE_DATABASE = os.environ.get(\"CLICKHOUSE_DATABASE\", \"default\")\n+CLIENT_NAME = \"simple native protocol\"\n+\n+\n+def writeVarUInt(x, ba):\n+    for _ in range(0, 9):\n+        byte = x & 0x7F\n+        if x > 0x7F:\n+            byte |= 0x80\n+\n+        ba.append(byte)\n+\n+        x >>= 7\n+        if x == 0:\n+            return\n+\n+\n+def writeStringBinary(s, ba):\n+    b = bytes(s, \"utf-8\")\n+    writeVarUInt(len(s), ba)\n+    ba.extend(b)\n+\n+\n+def readStrict(s, size=1):\n+    res = bytearray()\n+    while size:\n+        cur = s.recv(size)\n+        # if not res:\n+        #     raise \"Socket is closed\"\n+        size -= len(cur)\n+        res.extend(cur)\n+\n+    return res\n+\n+\n+def readUInt(s, size=1):\n+    res = readStrict(s, size)\n+    val = 0\n+    for i in range(len(res)):\n+        val += res[i] << (i * 8)\n+    return val\n+\n+\n+def readUInt8(s):\n+    return readUInt(s)\n+\n+\n+def readUInt16(s):\n+    return readUInt(s, 2)\n+\n+\n+def readUInt32(s):\n+    return readUInt(s, 4)\n+\n+\n+def readUInt64(s):\n+    return readUInt(s, 8)\n+\n+\n+def readVarUInt(s):\n+    x = 0\n+    for i in range(9):\n+        byte = readStrict(s)[0]\n+        x |= (byte & 0x7F) << (7 * i)\n+\n+        if not byte & 0x80:\n+            return x\n+\n+    return x\n \n-CURDIR = os.path.dirname(os.path.realpath(__file__))\n-sys.path.insert(0, os.path.join(CURDIR, \"helpers\"))\n \n-from tcp_client import TCPClient\n+def readStringBinary(s):\n+    size = readVarUInt(s)\n+    s = readStrict(s, size)\n+    return s.decode(\"utf-8\")\n+\n+\n+def sendHello(s):\n+    ba = bytearray()\n+    writeVarUInt(0, ba)  # Hello\n+    writeStringBinary(CLIENT_NAME, ba)\n+    writeVarUInt(21, ba)\n+    writeVarUInt(9, ba)\n+    writeVarUInt(54449, ba)\n+    writeStringBinary(CLICKHOUSE_DATABASE, ba)  # database\n+    writeStringBinary(\"default\", ba)  # user\n+    writeStringBinary(\"\", ba)  # pwd\n+    s.sendall(ba)\n+\n+\n+def receiveHello(s):\n+    p_type = readVarUInt(s)\n+    assert p_type == 0  # Hello\n+    _server_name = readStringBinary(s)\n+    _server_version_major = readVarUInt(s)\n+    _server_version_minor = readVarUInt(s)\n+    _server_revision = readVarUInt(s)\n+    _server_timezone = readStringBinary(s)\n+    _server_display_name = readStringBinary(s)\n+    _server_version_patch = readVarUInt(s)\n+\n+\n+def serializeClientInfo(ba, query_id):\n+    writeStringBinary(\"default\", ba)  # initial_user\n+    writeStringBinary(query_id, ba)  # initial_query_id\n+    writeStringBinary(\"127.0.0.1:9000\", ba)  # initial_address\n+    ba.extend([0] * 8)  # initial_query_start_time_microseconds\n+    ba.append(1)  # TCP\n+    writeStringBinary(\"os_user\", ba)  # os_user\n+    writeStringBinary(\"client_hostname\", ba)  # client_hostname\n+    writeStringBinary(CLIENT_NAME, ba)  # client_name\n+    writeVarUInt(21, ba)\n+    writeVarUInt(9, ba)\n+    writeVarUInt(54449, ba)\n+    writeStringBinary(\"\", ba)  # quota_key\n+    writeVarUInt(0, ba)  # distributed_depth\n+    writeVarUInt(1, ba)  # client_version_patch\n+    ba.append(0)  # No telemetry\n+\n+\n+def sendQuery(s, query, settings):\n+    ba = bytearray()\n+    query_id = uuid.uuid4().hex\n+    writeVarUInt(1, ba)  # query\n+    writeStringBinary(query_id, ba)\n+\n+    ba.append(1)  # INITIAL_QUERY\n+\n+    # client info\n+    serializeClientInfo(ba, query_id)\n+\n+    # Settings\n+    for key, value in settings.items():\n+        writeStringBinary(key, ba)\n+        writeVarUInt(1, ba)  # is_important\n+        writeStringBinary(str(value), ba)\n+    writeStringBinary(\"\", ba)  # End of settings\n+\n+    writeStringBinary(\"\", ba)  # No interserver secret\n+    writeVarUInt(2, ba)  # Stage - Complete\n+    ba.append(0)  # No compression\n+    writeStringBinary(query, ba)  # query, finally\n+    s.sendall(ba)\n+\n+\n+def serializeBlockInfo(ba):\n+    writeVarUInt(1, ba)  # 1\n+    ba.append(0)  # is_overflows\n+    writeVarUInt(2, ba)  # 2\n+    writeVarUInt(0, ba)  # 0\n+    ba.extend([0] * 4)  # bucket_num\n+\n+\n+def sendEmptyBlock(s):\n+    ba = bytearray()\n+    writeVarUInt(2, ba)  # Data\n+    writeStringBinary(\"\", ba)\n+    serializeBlockInfo(ba)\n+    writeVarUInt(0, ba)  # rows\n+    writeVarUInt(0, ba)  # columns\n+    s.sendall(ba)\n+\n+\n+def assertPacket(packet, expected):\n+    assert packet == expected, \"Got: {}, expected: {}\".format(packet, expected)\n+\n+\n+def readResponse(s):\n+    packet_type = readVarUInt(s)\n+    if packet_type == 2:  # Exception\n+        raise RuntimeError(readException(s))\n+\n+    if packet_type == 1:  # Data\n+        return None\n+    if packet_type == 3:  # Progress\n+        return None\n+    if packet_type == 5:  # End stream\n+        return None\n+\n+    raise RuntimeError(\"Unexpected packet: {}\".format(packet_type))\n+\n+\n+def readException(s):\n+    code = readUInt32(s)\n+    _name = readStringBinary(s)\n+    text = readStringBinary(s)\n+    readStringBinary(s)  # trace\n+    assertPacket(readUInt8(s), 0)  # has_nested\n+    return \"code {}: {}\".format(code, text.replace(\"DB::Exception:\", \"\"))\n \n \n def main():\n-    with TCPClient() as client:\n-        client.sendQuery(\"select 1\", {\"replication_alter_partitions_sync\": 1})\n+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n+        s.settimeout(30)\n+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))\n+        sendHello(s)\n+        receiveHello(s)\n+        sendQuery(s, \"select 1\", {\"replication_alter_partitions_sync\": 1})\n         # external tables\n-        client.sendEmptyBlock()\n+        sendEmptyBlock(s)\n \n-        while client.readResponse() is not None:\n+        while readResponse(s) is not None:\n             pass\n+\n+        s.close()\n     print(\"OK\")\n \n \ndiff --git a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.python b/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.python\ndeleted file mode 100755\nindex 61ba0e14605d..000000000000\n--- a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.python\n+++ /dev/null\n@@ -1,95 +0,0 @@\n-#!/usr/bin/env python3\n-# -*- coding: utf-8 -*-\n-\n-\n-import os\n-import sys\n-\n-CURDIR = os.path.dirname(os.path.realpath(__file__))\n-sys.path.insert(0, os.path.join(CURDIR, \"helpers\"))\n-\n-from tcp_client import TCPClient\n-\n-\n-def run_query_without_errors(query, support_partial_result):\n-    with TCPClient() as client:\n-        client.sendQuery(query, settings={\"allow_experimental_partial_result\": True})\n-\n-        # external tables\n-        client.sendEmptyBlock()\n-        client.readHeader()\n-\n-        # Partial result\n-        partial_result = client.readDataWithoutProgress()[0]\n-        if support_partial_result:\n-            assert (\n-                len(partial_result.value) > 0\n-            ), \"Expected at least one block with a non-empty partial result before getting the full result\"\n-\n-            while True:\n-                assert all(\n-                    a >= b\n-                    for a, b in zip(partial_result.value, partial_result.value[1:])\n-                ), \"Partial result always should be sorted for this test\"\n-\n-                new_partial_result = client.readDataWithoutProgress(\n-                    need_print_info=False\n-                )[0]\n-                if len(new_partial_result.value) == 0:\n-                    break\n-\n-                data_size = len(partial_result.value)\n-                assert all(\n-                    partial_result.value[i] <= new_partial_result.value[i]\n-                    for i in range(data_size)\n-                ), f\"New partial result values should always be greater then old one because a new block contains more information about the full data. New result {new_partial_result}. Previous result {partial_result}\"\n-\n-                partial_result = new_partial_result\n-        else:\n-            block_rows = len(partial_result.value)\n-            assert (\n-                block_rows == 0\n-            ), f\"Expected only empty partial result block before getting the full result, but block has {block_rows} rows\"\n-\n-        # Full result\n-        full_result = client.readDataWithoutProgress()[0]\n-\n-        data_size = len(partial_result.value)\n-        assert all(\n-            partial_result.value[i] <= full_result.value[i] for i in range(data_size)\n-        ), f\"Full result values should always be greater then partial result values. Full result {full_result}. Partial result {partial_result}\"\n-\n-        for result in full_result.value:\n-            print(result)\n-\n-\n-def main():\n-    rows_number = 2e7 + 1\n-\n-    # Request with partial result limit less then full limit\n-    run_query_without_errors(\n-        f\"SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 5 SETTINGS max_threads = 1, partial_result_update_duration_ms = 1, max_rows_in_partial_result = 3\",\n-        support_partial_result=True,\n-    )\n-\n-    # Request with partial result limit greater then full limit\n-    run_query_without_errors(\n-        f\"SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 3 SETTINGS max_threads = 1, partial_result_update_duration_ms = 1, max_rows_in_partial_result = 5\",\n-        support_partial_result=True,\n-    )\n-\n-    # Request with OFFSET\n-    run_query_without_errors(\n-        f\"SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 3 OFFSET 1 SETTINGS max_threads = 1, partial_result_update_duration_ms = 1, max_rows_in_partial_result = 5\",\n-        support_partial_result=True,\n-    )\n-\n-    # Request with OFFSET greater then partial result limit (partial result pipeline use blocks with less then OFFSET, so there will be no elements in block after LimitPartialResultTransform)\n-    run_query_without_errors(\n-        f\"SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 3 OFFSET 15 SETTINGS max_threads = 1, partial_result_update_duration_ms = 1, max_rows_in_partial_result = 5\",\n-        support_partial_result=False,\n-    )\n-\n-\n-if __name__ == \"__main__\":\n-    main()\ndiff --git a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.reference b/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.reference\ndeleted file mode 100644\nindex dd3a343560f0..000000000000\n--- a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.reference\n+++ /dev/null\n@@ -1,38 +0,0 @@\n-Rows 0 Columns 1\n-Column number type UInt64\n-Rows 3 Columns 1\n-Column number type UInt64\n-Rows 5 Columns 1\n-Column number type UInt64\n-20000000\n-19999999\n-19999998\n-19999997\n-19999996\n-Rows 0 Columns 1\n-Column number type UInt64\n-Rows 3 Columns 1\n-Column number type UInt64\n-Rows 3 Columns 1\n-Column number type UInt64\n-20000000\n-19999999\n-19999998\n-Rows 0 Columns 1\n-Column number type UInt64\n-Rows 3 Columns 1\n-Column number type UInt64\n-Rows 3 Columns 1\n-Column number type UInt64\n-19999999\n-19999998\n-19999997\n-Rows 0 Columns 1\n-Column number type UInt64\n-Rows 0 Columns 1\n-Column number type UInt64\n-Rows 3 Columns 1\n-Column number type UInt64\n-19999985\n-19999984\n-19999983\ndiff --git a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.sh b/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.sh\ndeleted file mode 100755\nindex 1ed15197dbff..000000000000\n--- a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.sh\n+++ /dev/null\n@@ -1,8 +0,0 @@\n-#!/usr/bin/env bash\n-\n-CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n-# shellcheck source=../shell_config.sh\n-. \"$CURDIR\"/../shell_config.sh\n-\n-# We should have correct env vars from shell_config.sh to run this test\n-python3 \"$CURDIR\"/02833_partial_sorting_result_during_query_execution.python\ndiff --git a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.python b/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.python\ndeleted file mode 100644\nindex a33c714e89c2..000000000000\n--- a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.python\n+++ /dev/null\n@@ -1,129 +0,0 @@\n-#!/usr/bin/env python3\n-# -*- coding: utf-8 -*-\n-\n-\n-import os\n-import sys\n-\n-CURDIR = os.path.dirname(os.path.realpath(__file__))\n-sys.path.insert(0, os.path.join(CURDIR, \"helpers\"))\n-\n-from tcp_client import TCPClient\n-\n-\n-def get_keys(results):\n-    return [key for key, _ in results]\n-\n-\n-def check_new_result(new_results, old_results, invariants, rows_limit):\n-    if rows_limit is not None:\n-        assert (\n-            len(new_results[0].value) <= rows_limit\n-        ), f\"Result should have no more then {rows_limit} rows. But it has {len(new_results[0].value)} rows\"\n-\n-    for new_result, old_result in zip(new_results, old_results):\n-        assert (\n-            new_result.key == old_result.key\n-        ), f\"Keys in blocks should be in the same order. Full results keys {get_keys(full_results)}. Partial results keys  {get_keys(partial_results)}\"\n-\n-        key = new_result.key\n-        if key in invariants:\n-            new_value = new_result.value\n-            old_value = old_result.value\n-            assert invariants[key](\n-                old_value, new_value\n-            ), f\"Problem with the invariant between new and old result for key: {key}. New value {new_value}. Old value {old_value}\"\n-\n-\n-def run_query_without_errors(\n-    query, support_partial_result, invariants=None, rows_limit=None\n-):\n-    if invariants is None:\n-        invariants = {}\n-\n-    with TCPClient() as client:\n-        client.sendQuery(query, settings={\"allow_experimental_partial_result\": True})\n-\n-        # external tables\n-        client.sendEmptyBlock()\n-        client.readHeader()\n-\n-        # Partial result\n-        partial_results = client.readDataWithoutProgress()\n-        if support_partial_result:\n-            assert (\n-                len(partial_results) > 0 and len(partial_results[0].value) > 0\n-            ), \"Expected at least one block with a non-empty partial result before getting the full result\"\n-            while True:\n-                new_partial_results = client.readDataWithoutProgress(\n-                    need_print_info=False\n-                )\n-                if len(new_partial_results[0].value) == 0:\n-                    break\n-\n-                check_new_result(\n-                    new_partial_results, partial_results, invariants, rows_limit\n-                )\n-                partial_results = new_partial_results\n-        else:\n-            block_rows = len(partial_results[0].value)\n-            assert (\n-                block_rows == 0\n-            ), f\"Expected only empty partial result block before getting the full result, but block has {block_rows} rows\"\n-\n-        # Full result\n-        full_results = client.readDataWithoutProgress()\n-        if support_partial_result:\n-            check_new_result(full_results, partial_results, invariants, rows_limit)\n-\n-        for data in full_results:\n-            if isinstance(data.value[0], int):\n-                print(data.key, data.value)\n-\n-\n-def supported_scenarios_without_key():\n-    rows_number = 2e7 + 1\n-\n-    # Simple aggregation query\n-    query = f\"select median(number), stddevSamp(number), stddevPop(number), max(number), min(number), any(number), count(number), avg(number), sum(number) from numbers_mt({rows_number}) settings max_threads = 1, partial_result_update_duration_ms = 1\"\n-    invariants = {\n-        \"median(number)\": lambda old_value, new_value: old_value <= new_value,\n-        \"max(number)\": lambda old_value, new_value: old_value <= new_value,\n-        \"min(number)\": lambda old_value, new_value: old_value >= new_value,\n-        \"count(number)\": lambda old_value, new_value: old_value <= new_value,\n-        \"avg(number)\": lambda old_value, new_value: old_value <= new_value,\n-        \"sum(number)\": lambda old_value, new_value: old_value <= new_value,\n-    }\n-    run_query_without_errors(\n-        query, support_partial_result=True, invariants=invariants, rows_limit=1\n-    )\n-\n-    # Aggregation query with a nested ORDER BY subquery\n-    query = f\"select median(number), stddevSamp(number), stddevPop(number), max(number), min(number), any(number), count(number), avg(number), sum(number) FROM (SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 3) settings max_threads = 1, partial_result_update_duration_ms=1\"\n-\n-    # Aggregation receives small partial result blocks from ORDER BY which always sends blocks with bigger values\n-    invariants[\"min(number)\"] = lambda old_value, new_value: old_value <= new_value\n-    run_query_without_errors(\n-        query, support_partial_result=True, invariants=invariants, rows_limit=1\n-    )\n-\n-\n-def unsupported_scenarios():\n-    rows_number = 2e7 + 1\n-\n-    # Currently aggregator for partial result supports only single thread aggregation without key\n-    # Update test when multithreading or aggregation with GROUP BY will be supported for partial result updates\n-    multithread_query = f\"select sum(number) from numbers_mt({rows_number}) settings max_threads = 2, partial_result_update_duration_ms = 100\"\n-    run_query_without_errors(multithread_query, support_partial_result=False)\n-\n-    group_with_key_query = f\"select mod2, sum(number) from numbers_mt({rows_number}) group by number % 2 as mod2 settings max_threads = 1, partial_result_update_duration_ms = 100\"\n-    run_query_without_errors(group_with_key_query, support_partial_result=False)\n-\n-\n-def main():\n-    supported_scenarios_without_key()\n-    unsupported_scenarios()\n-\n-\n-if __name__ == \"__main__\":\n-    main()\ndiff --git a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.reference b/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.reference\ndeleted file mode 100644\nindex aea61fad42f8..000000000000\n--- a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.reference\n+++ /dev/null\n@@ -1,88 +0,0 @@\n-Rows 0 Columns 9\n-Column median(number) type Float64\n-Column stddevSamp(number) type Float64\n-Column stddevPop(number) type Float64\n-Column max(number) type UInt64\n-Column min(number) type UInt64\n-Column any(number) type UInt64\n-Column count(number) type UInt64\n-Column avg(number) type Float64\n-Column sum(number) type UInt64\n-Rows 1 Columns 9\n-Column median(number) type Float64\n-Column stddevSamp(number) type Float64\n-Column stddevPop(number) type Float64\n-Column max(number) type UInt64\n-Column min(number) type UInt64\n-Column any(number) type UInt64\n-Column count(number) type UInt64\n-Column avg(number) type Float64\n-Column sum(number) type UInt64\n-Rows 1 Columns 9\n-Column median(number) type Float64\n-Column stddevSamp(number) type Float64\n-Column stddevPop(number) type Float64\n-Column max(number) type UInt64\n-Column min(number) type UInt64\n-Column any(number) type UInt64\n-Column count(number) type UInt64\n-Column avg(number) type Float64\n-Column sum(number) type UInt64\n-max(number) [20000000]\n-min(number) [0]\n-any(number) [0]\n-count(number) [20000001]\n-sum(number) [200000010000000]\n-Rows 0 Columns 9\n-Column median(number) type Float64\n-Column stddevSamp(number) type Float64\n-Column stddevPop(number) type Float64\n-Column max(number) type UInt64\n-Column min(number) type UInt64\n-Column any(number) type UInt64\n-Column count(number) type UInt64\n-Column avg(number) type Float64\n-Column sum(number) type UInt64\n-Rows 1 Columns 9\n-Column median(number) type Float64\n-Column stddevSamp(number) type Float64\n-Column stddevPop(number) type Float64\n-Column max(number) type UInt64\n-Column min(number) type UInt64\n-Column any(number) type UInt64\n-Column count(number) type UInt64\n-Column avg(number) type Float64\n-Column sum(number) type UInt64\n-Rows 1 Columns 9\n-Column median(number) type Float64\n-Column stddevSamp(number) type Float64\n-Column stddevPop(number) type Float64\n-Column max(number) type UInt64\n-Column min(number) type UInt64\n-Column any(number) type UInt64\n-Column count(number) type UInt64\n-Column avg(number) type Float64\n-Column sum(number) type UInt64\n-max(number) [20000000]\n-min(number) [19999998]\n-any(number) [20000000]\n-count(number) [3]\n-sum(number) [59999997]\n-Rows 0 Columns 1\n-Column sum(number) type UInt64\n-Rows 0 Columns 1\n-Column sum(number) type UInt64\n-Rows 1 Columns 1\n-Column sum(number) type UInt64\n-sum(number) [200000010000000]\n-Rows 0 Columns 2\n-Column mod2 type UInt8\n-Column sum(number) type UInt64\n-Rows 0 Columns 2\n-Column mod2 type UInt8\n-Column sum(number) type UInt64\n-Rows 2 Columns 2\n-Column mod2 type UInt8\n-Column sum(number) type UInt64\n-mod2 [0, 1]\n-sum(number) [100000010000000, 100000000000000]\ndiff --git a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.sh b/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.sh\ndeleted file mode 100755\nindex e70a3c53ec43..000000000000\n--- a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.sh\n+++ /dev/null\n@@ -1,8 +0,0 @@\n-#!/usr/bin/env bash\n-\n-CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n-# shellcheck source=../shell_config.sh\n-. \"$CURDIR\"/../shell_config.sh\n-\n-# We should have correct env vars from shell_config.sh to run this test\n-python3 \"$CURDIR\"/02834_partial_aggregating_result_during_query_execution.python\ndiff --git a/tests/queries/0_stateless/02876_experimental_partial_result.reference b/tests/queries/0_stateless/02876_experimental_partial_result.reference\ndeleted file mode 100644\nindex e69de29bb2d1..000000000000\ndiff --git a/tests/queries/0_stateless/02876_experimental_partial_result.sql b/tests/queries/0_stateless/02876_experimental_partial_result.sql\ndeleted file mode 100644\nindex 8418f07c7502..000000000000\n--- a/tests/queries/0_stateless/02876_experimental_partial_result.sql\n+++ /dev/null\n@@ -1,4 +0,0 @@\n-\n-SET partial_result_update_duration_ms = 10;\n-\n-SELECT sum(number) FROM numbers_mt(100_000) SETTINGS max_threads = 1; -- { serverError FUNCTION_NOT_ALLOWED }\ndiff --git a/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.reference b/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.reference\ndeleted file mode 100644\nindex e69de29bb2d1..000000000000\ndiff --git a/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.sql b/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.sql\ndeleted file mode 100644\nindex 9e665e0ae20e..000000000000\n--- a/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.sql\n+++ /dev/null\n@@ -1,11 +0,0 @@\n-drop table if exists data;\n-create table data (key Int) engine=MergeTree() order by key;\n-insert into data select * from numbers(1);\n-insert into data select * from numbers(1);\n-system stop merges data;\n--- need sleep to trigger partial results to uncover the bug with empty chunk after remerge due to empty array join, i.e.:\n---\n---   MergeSortingTransform: Re-merging intermediate ORDER BY data (1 blocks with 0 rows) to save memory consumption\n---   MergeSortingTransform: Memory usage is lowered from 4.26 KiB to 0.00 B\n---\n-select key, sleepEachRow(1) from data array join [] as x order by key settings optimize_read_in_order=0, allow_experimental_partial_result=1, partial_result_update_duration_ms=1, max_threads=1, max_execution_time=0, max_block_size=1;\ndiff --git a/tests/queries/0_stateless/helpers/tcp_client.py b/tests/queries/0_stateless/helpers/tcp_client.py\ndeleted file mode 100644\nindex fdc4ab28e042..000000000000\n--- a/tests/queries/0_stateless/helpers/tcp_client.py\n+++ /dev/null\n@@ -1,313 +0,0 @@\n-import socket\n-import os\n-import uuid\n-import struct\n-\n-CLICKHOUSE_HOST = os.environ.get(\"CLICKHOUSE_HOST\", \"127.0.0.1\")\n-CLICKHOUSE_PORT = int(os.environ.get(\"CLICKHOUSE_PORT_TCP\", \"900000\"))\n-CLICKHOUSE_DATABASE = os.environ.get(\"CLICKHOUSE_DATABASE\", \"default\")\n-CLIENT_NAME = \"simple native protocol\"\n-\n-\n-def writeVarUInt(x, ba):\n-    for _ in range(0, 9):\n-        byte = x & 0x7F\n-        if x > 0x7F:\n-            byte |= 0x80\n-\n-        ba.append(byte)\n-\n-        x >>= 7\n-        if x == 0:\n-            return\n-\n-\n-def writeStringBinary(s, ba):\n-    b = bytes(s, \"utf-8\")\n-    writeVarUInt(len(s), ba)\n-    ba.extend(b)\n-\n-\n-def serializeClientInfo(ba, query_id):\n-    writeStringBinary(\"default\", ba)  # initial_user\n-    writeStringBinary(query_id, ba)  # initial_query_id\n-    writeStringBinary(\"127.0.0.1:9000\", ba)  # initial_address\n-    ba.extend([0] * 8)  # initial_query_start_time_microseconds\n-    ba.append(1)  # TCP\n-    writeStringBinary(\"os_user\", ba)  # os_user\n-    writeStringBinary(\"client_hostname\", ba)  # client_hostname\n-    writeStringBinary(CLIENT_NAME, ba)  # client_name\n-    writeVarUInt(21, ba)\n-    writeVarUInt(9, ba)\n-    writeVarUInt(54449, ba)\n-    writeStringBinary(\"\", ba)  # quota_key\n-    writeVarUInt(0, ba)  # distributed_depth\n-    writeVarUInt(1, ba)  # client_version_patch\n-    ba.append(0)  # No telemetry\n-\n-\n-def serializeBlockInfo(ba):\n-    writeVarUInt(1, ba)  # 1\n-    ba.append(0)  # is_overflows\n-    writeVarUInt(2, ba)  # 2\n-    writeVarUInt(0, ba)  # 0\n-    ba.extend([0] * 4)  # bucket_num\n-\n-\n-def assertPacket(packet, expected):\n-    assert packet == expected, \"Got: {}, expected: {}\".format(packet, expected)\n-\n-\n-class Data(object):\n-    def __init__(self, key, value):\n-        self.key = key\n-        self.value = value\n-\n-\n-class TCPClient(object):\n-    def __init__(self, timeout=30):\n-        self.timeout = timeout\n-        self.socket = None\n-\n-    def __enter__(self):\n-        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n-        self.socket.settimeout(self.timeout)\n-        self.socket.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))\n-\n-        self.sendHello()\n-        self.receiveHello()\n-\n-        return self\n-\n-    def __exit__(self, exc_type, exc_value, traceback):\n-        if self.socket:\n-            self.socket.close()\n-\n-    def readStrict(self, size=1):\n-        res = bytearray()\n-        while size:\n-            cur = self.socket.recv(size)\n-            # if not res:\n-            #     raise \"Socket is closed\"\n-            size -= len(cur)\n-            res.extend(cur)\n-\n-        return res\n-\n-    def readUInt(self, size=1):\n-        res = self.readStrict(size)\n-        val = 0\n-        for i in range(len(res)):\n-            val += res[i] << (i * 8)\n-        return val\n-\n-    def readUInt8(self):\n-        return self.readUInt()\n-\n-    def readUInt16(self):\n-        return self.readUInt(2)\n-\n-    def readUInt32(self):\n-        return self.readUInt(4)\n-\n-    def readUInt64(self):\n-        return self.readUInt(8)\n-\n-    def readFloat16(self):\n-        return struct.unpack(\"e\", self.readStrict(2))\n-\n-    def readFloat32(self):\n-        return struct.unpack(\"f\", self.readStrict(4))\n-\n-    def readFloat64(self):\n-        return struct.unpack(\"d\", self.readStrict(8))\n-\n-    def readVarUInt(self):\n-        x = 0\n-        for i in range(9):\n-            byte = self.readStrict()[0]\n-            x |= (byte & 0x7F) << (7 * i)\n-\n-            if not byte & 0x80:\n-                return x\n-\n-        return x\n-\n-    def readStringBinary(self):\n-        size = self.readVarUInt()\n-        s = self.readStrict(size)\n-        return s.decode(\"utf-8\")\n-\n-    def send(self, byte_array):\n-        self.socket.sendall(byte_array)\n-\n-    def sendHello(self):\n-        ba = bytearray()\n-        writeVarUInt(0, ba)  # Hello\n-        writeStringBinary(CLIENT_NAME, ba)\n-        writeVarUInt(21, ba)\n-        writeVarUInt(9, ba)\n-        writeVarUInt(54449, ba)\n-        writeStringBinary(CLICKHOUSE_DATABASE, ba)  # database\n-        writeStringBinary(\"default\", ba)  # user\n-        writeStringBinary(\"\", ba)  # pwd\n-        self.send(ba)\n-\n-    def receiveHello(self):\n-        p_type = self.readVarUInt()\n-        assert p_type == 0  # Hello\n-        _server_name = self.readStringBinary()\n-        _server_version_major = self.readVarUInt()\n-        _server_version_minor = self.readVarUInt()\n-        _server_revision = self.readVarUInt()\n-        _server_timezone = self.readStringBinary()\n-        _server_display_name = self.readStringBinary()\n-        _server_version_patch = self.readVarUInt()\n-\n-    def sendQuery(self, query, settings=None):\n-        if settings == None:\n-            settings = {}  # No settings\n-\n-        ba = bytearray()\n-        query_id = uuid.uuid4().hex\n-        writeVarUInt(1, ba)  # query\n-        writeStringBinary(query_id, ba)\n-\n-        ba.append(1)  # INITIAL_QUERY\n-\n-        # client info\n-        serializeClientInfo(ba, query_id)\n-\n-        # Settings\n-        for key, value in settings.items():\n-            writeStringBinary(key, ba)\n-            writeVarUInt(1, ba)  # is_important\n-            writeStringBinary(str(value), ba)\n-        writeStringBinary(\"\", ba)  # End of settings\n-\n-        writeStringBinary(\"\", ba)  # No interserver secret\n-        writeVarUInt(2, ba)  # Stage - Complete\n-        ba.append(0)  # No compression\n-        writeStringBinary(query, ba)  # query, finally\n-        self.send(ba)\n-\n-    def sendEmptyBlock(self):\n-        ba = bytearray()\n-        writeVarUInt(2, ba)  # Data\n-        writeStringBinary(\"\", ba)\n-        serializeBlockInfo(ba)\n-        writeVarUInt(0, ba)  # rows\n-        writeVarUInt(0, ba)  # columns\n-        self.send(ba)\n-\n-    def readException(self):\n-        code = self.readUInt32()\n-        _name = self.readStringBinary()\n-        text = self.readStringBinary()\n-        self.readStringBinary()  # trace\n-        assertPacket(self.readUInt8(), 0)  # has_nested\n-        return \"code {}: {}\".format(code, text.replace(\"DB::Exception:\", \"\"))\n-\n-    def readPacketType(self):\n-        packet_type = self.readVarUInt()\n-        if packet_type == 2:  # Exception\n-            raise RuntimeError(self.readException())\n-\n-        return packet_type\n-\n-    def readResponse(self):\n-        packet_type = self.readPacketType()\n-        if packet_type == 1:  # Data\n-            return None\n-        if packet_type == 3:  # Progress\n-            return None\n-        if packet_type == 5:  # End stream\n-            return None\n-\n-        raise RuntimeError(\"Unexpected packet: {}\".format(packet_type))\n-\n-    def readProgressData(self):\n-        read_rows = self.readVarUInt()\n-        read_bytes = self.readVarUInt()\n-        total_rows_to_read = self.readVarUInt()\n-        written_rows = self.readVarUInt()\n-        written_bytes = self.readVarUInt()\n-\n-        return read_rows, read_bytes, total_rows_to_read, written_rows, written_bytes\n-\n-    def readProgress(self):\n-        packet_type = self.readPacketType()\n-        if packet_type == 5:  # End stream\n-            return None\n-        assertPacket(packet_type, 3)  # Progress\n-        return self.readProgressData()\n-\n-    def readHeaderInfo(self):\n-        self.readStringBinary()  # external table name\n-        # BlockInfo\n-        assertPacket(self.readVarUInt(), 1)  # field number 1\n-        assertPacket(self.readUInt8(), 0)  # is_overflows\n-        assertPacket(self.readVarUInt(), 2)  # field number 2\n-        assertPacket(self.readUInt32(), 4294967295)  # bucket_num\n-        assertPacket(self.readVarUInt(), 0)  # 0\n-        columns = self.readVarUInt()  # rows\n-        rows = self.readVarUInt()  # columns\n-\n-        return columns, rows\n-\n-    def readHeader(self):\n-        packet_type = self.readPacketType()\n-        assertPacket(packet_type, 1)  # Data\n-\n-        columns, rows = self.readHeaderInfo()\n-        print(\"Rows {} Columns {}\".format(rows, columns))\n-        for _ in range(columns):\n-            col_name = self.readStringBinary()\n-            type_name = self.readStringBinary()\n-            print(\"Column {} type {}\".format(col_name, type_name))\n-\n-    def readRow(self, row_type, rows):\n-        supported_row_types = {\n-            \"UInt8\": self.readUInt8,\n-            \"UInt16\": self.readUInt16,\n-            \"UInt32\": self.readUInt32,\n-            \"UInt64\": self.readUInt64,\n-            \"Float16\": self.readFloat16,\n-            \"Float32\": self.readFloat32,\n-            \"Float64\": self.readFloat64,\n-        }\n-        if row_type in supported_row_types:\n-            read_type = supported_row_types[row_type]\n-            row = [read_type() for _ in range(rows)]\n-            return row\n-        else:\n-            raise RuntimeError(\n-                \"Current python version of tcp client doesn't support the following type of row: {}\".format(\n-                    row_type\n-                )\n-            )\n-\n-    def readDataWithoutProgress(self, need_print_info=True):\n-        packet_type = self.readPacketType()\n-        while packet_type == 3:  # Progress\n-            self.readProgressData()\n-            packet_type = self.readPacketType()\n-\n-        if packet_type == 5:  # End stream\n-            return None\n-        assertPacket(packet_type, 1)  # Data\n-\n-        columns, rows = self.readHeaderInfo()\n-        data = []\n-        if need_print_info:\n-            print(\"Rows {} Columns {}\".format(rows, columns))\n-\n-        for _ in range(columns):\n-            col_name = self.readStringBinary()\n-            type_name = self.readStringBinary()\n-            if need_print_info:\n-                print(\"Column {} type {}\".format(col_name, type_name))\n-\n-            data.append(Data(col_name, self.readRow(type_name, rows)))\n-\n-        return data\n",
  "problem_statement": "Optimise memory consumption during loading of hierarchical dictionaries\n### Changelog category (leave one):\r\n- Improvement\r\n\r\n\r\n### Changelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nReduced memory consumption during loading of hierarchical dictionaries.\n",
  "hints_text": "",
  "created_at": "2023-10-21T01:16:18Z",
  "modified_files": [
    "docs/en/operations/settings/settings.md",
    "src/Client/ClientBase.cpp",
    "src/Client/ClientBase.h",
    "src/Core/Settings.h",
    "src/Interpreters/Aggregator.cpp",
    "src/Interpreters/Aggregator.h",
    "src/Processors/Chunk.cpp",
    "src/Processors/Executors/CompletedPipelineExecutor.cpp",
    "src/Processors/Executors/ExecutingGraph.cpp",
    "src/Processors/Executors/ExecutionThreadContext.h",
    "src/Processors/Executors/ExecutorTasks.cpp",
    "src/Processors/Executors/ExecutorTasks.h",
    "src/Processors/Executors/PipelineExecutor.cpp",
    "src/Processors/Executors/PipelineExecutor.h",
    "src/Processors/Executors/PullingAsyncPipelineExecutor.cpp",
    "src/Processors/Executors/PullingAsyncPipelineExecutor.h",
    "src/Processors/Executors/PullingPipelineExecutor.cpp",
    "src/Processors/Executors/PushingAsyncPipelineExecutor.cpp",
    "src/Processors/Executors/PushingPipelineExecutor.cpp",
    "src/Processors/Formats/IOutputFormat.cpp",
    "src/Processors/Formats/IOutputFormat.h",
    "src/Processors/Formats/Impl/PrettyBlockOutputFormat.cpp",
    "src/Processors/Formats/Impl/PrettyBlockOutputFormat.h",
    "src/Processors/Formats/Impl/PrettyCompactBlockOutputFormat.cpp",
    "src/Processors/Formats/LazyOutputFormat.h",
    "src/Processors/IProcessor.cpp",
    "src/Processors/IProcessor.h",
    "src/Processors/LimitTransform.cpp",
    "src/Processors/LimitTransform.h",
    "src/Processors/QueryPlan/BuildQueryPipelineSettings.cpp",
    "src/Processors/QueryPlan/BuildQueryPipelineSettings.h",
    "src/Processors/QueryPlan/QueryPlan.cpp",
    "src/Processors/QueryPlan/SortingStep.h",
    "src/Processors/Transforms/AggregatingPartialResultTransform.cpp",
    "src/Processors/Transforms/AggregatingPartialResultTransform.h",
    "src/Processors/Transforms/AggregatingTransform.cpp",
    "src/Processors/Transforms/AggregatingTransform.h",
    "src/Processors/Transforms/ExpressionTransform.cpp",
    "src/Processors/Transforms/ExpressionTransform.h",
    "src/Processors/Transforms/LimitPartialResultTransform.cpp",
    "src/Processors/Transforms/LimitPartialResultTransform.h",
    "src/Processors/Transforms/LimitsCheckingTransform.cpp",
    "src/Processors/Transforms/LimitsCheckingTransform.h",
    "src/Processors/Transforms/MergeSortingPartialResultTransform.cpp",
    "src/Processors/Transforms/MergeSortingPartialResultTransform.h",
    "src/Processors/Transforms/MergeSortingTransform.cpp",
    "src/Processors/Transforms/MergeSortingTransform.h",
    "src/Processors/Transforms/PartialResultTransform.cpp",
    "src/Processors/Transforms/PartialResultTransform.h",
    "src/QueryPipeline/Pipe.cpp",
    "src/QueryPipeline/Pipe.h",
    "src/QueryPipeline/QueryPipeline.cpp",
    "src/QueryPipeline/QueryPipeline.h",
    "src/QueryPipeline/QueryPipelineBuilder.cpp",
    "src/QueryPipeline/QueryPipelineBuilder.h",
    "src/Server/TCPHandler.cpp"
  ],
  "modified_test_files": [
    "tests/queries/0_stateless/02010_lc_native.python",
    "tests/queries/0_stateless/02210_processors_profile_log.reference",
    "tests/queries/0_stateless/02458_insert_select_progress_tcp.python",
    "tests/queries/0_stateless/02750_settings_alias_tcp_protocol.python",
    "tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.python",
    "tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.reference",
    "tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.sh",
    "tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.python",
    "tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.reference",
    "tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.sh",
    "tests/queries/0_stateless/02876_experimental_partial_result.sql",
    "tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.sql",
    "tests/queries/0_stateless/helpers/tcp_client.py"
  ]
}