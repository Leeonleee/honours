{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 34068,
  "instance_id": "ClickHouse__ClickHouse-34068",
  "issue_numbers": [
    "33536"
  ],
  "base_commit": "63d8d750011cb7bdcbdc89c328fbb2698cab6fbf",
  "patch": "diff --git a/src/Processors/Formats/Impl/NativeFormat.cpp b/src/Processors/Formats/Impl/NativeFormat.cpp\nindex 19e2ede6b654..bd95cfd63765 100644\n--- a/src/Processors/Formats/Impl/NativeFormat.cpp\n+++ b/src/Processors/Formats/Impl/NativeFormat.cpp\n@@ -15,21 +15,22 @@ namespace DB\n class NativeInputFormat final : public IInputFormat\n {\n public:\n-    NativeInputFormat(ReadBuffer & buf, const Block & header)\n-        : IInputFormat(header, buf)\n-        , reader(buf, header, 0) {}\n+    NativeInputFormat(ReadBuffer & buf, const Block & header_)\n+        : IInputFormat(header_, buf)\n+        , reader(std::make_unique<NativeReader>(buf, header_, 0))\n+        , header(header_) {}\n \n     String getName() const override { return \"Native\"; }\n \n     void resetParser() override\n     {\n         IInputFormat::resetParser();\n-        reader.resetParser();\n+        reader->resetParser();\n     }\n \n     Chunk generate() override\n     {\n-        auto block = reader.read();\n+        auto block = reader->read();\n         if (!block)\n             return {};\n \n@@ -40,8 +41,15 @@ class NativeInputFormat final : public IInputFormat\n         return Chunk(block.getColumns(), num_rows);\n     }\n \n+    void setReadBuffer(ReadBuffer & in_) override\n+    {\n+        reader = std::make_unique<NativeReader>(in_, header, 0);\n+        IInputFormat::setReadBuffer(in_);\n+    }\n+\n private:\n-    NativeReader reader;\n+    std::unique_ptr<NativeReader> reader;\n+    Block header;\n };\n \n class NativeOutputFormat final : public IOutputFormat\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02187_async_inserts_all_formats.python b/tests/queries/0_stateless/02187_async_inserts_all_formats.python\nnew file mode 100644\nindex 000000000000..0a9094512593\n--- /dev/null\n+++ b/tests/queries/0_stateless/02187_async_inserts_all_formats.python\n@@ -0,0 +1,50 @@\n+#!/usr/bin/env python3\n+import os\n+import sys\n+\n+CURDIR = os.path.dirname(os.path.realpath(__file__))\n+sys.path.insert(0, os.path.join(CURDIR, 'helpers'))\n+\n+CLICKHOUSE_URL = os.environ.get('CLICKHOUSE_URL')\n+CLICKHOUSE_TMP = os.environ.get('CLICKHOUSE_TMP')\n+\n+from pure_http_client import ClickHouseClient\n+\n+client = ClickHouseClient()\n+\n+def run_test(data_format, gen_data_template, settings):\n+    print(data_format)\n+    client.query(\"TRUNCATE TABLE t_async_insert\")\n+\n+    expected = client.query(gen_data_template.format(\"TSV\")).strip()\n+    data = client.query(gen_data_template.format(data_format), settings=settings,binary_result=True)\n+\n+    insert_query = \"INSERT INTO t_async_insert FORMAT {}\".format(data_format)\n+    client.query_with_data(insert_query, data, settings=settings)\n+\n+    result = client.query(\"SELECT * FROM t_async_insert FORMAT TSV\").strip()\n+    if result != expected:\n+        print(\"Failed for format {}.\\nExpected:\\n{}\\nGot:\\n{}\\n\".format(data_format, expected, result))\n+        exit(1)\n+\n+formats = client.query(\"SELECT name FROM system.formats WHERE is_input AND is_output \\\n+    AND name NOT IN ('CapnProto', 'RawBLOB', 'Template', 'ProtobufSingle', 'LineAsString', 'Protobuf') ORDER BY name\").strip().split('\\n')\n+\n+# Generic formats\n+client.query(\"DROP TABLE IF EXISTS t_async_insert\")\n+client.query(\"CREATE TABLE t_async_insert (id UInt64, s String, arr Array(UInt64)) ENGINE = Memory\")\n+gen_data_query = \"SELECT number AS id, toString(number) AS s, range(number) AS arr FROM numbers(10) FORMAT {}\"\n+\n+for data_format in formats:\n+    run_test(data_format, gen_data_query, settings={\"async_insert\": 1, \"wait_for_async_insert\": 1})\n+\n+# LineAsString\n+client.query(\"DROP TABLE IF EXISTS t_async_insert\")\n+client.query(\"CREATE TABLE t_async_insert (s String) ENGINE = Memory\")\n+gen_data_query = \"SELECT toString(number) AS s FROM numbers(10) FORMAT {}\"\n+\n+run_test('LineAsString', gen_data_query, settings={\"async_insert\": 1, \"wait_for_async_insert\": 1})\n+\n+# TODO: add CapnProto and Protobuf\n+\n+print(\"OK\")\ndiff --git a/tests/queries/0_stateless/02187_async_inserts_all_formats.reference b/tests/queries/0_stateless/02187_async_inserts_all_formats.reference\nnew file mode 100644\nindex 000000000000..b4a5b6c3a426\n--- /dev/null\n+++ b/tests/queries/0_stateless/02187_async_inserts_all_formats.reference\n@@ -0,0 +1,40 @@\n+Arrow\n+ArrowStream\n+Avro\n+CSV\n+CSVWithNames\n+CSVWithNamesAndTypes\n+CustomSeparated\n+CustomSeparatedWithNames\n+CustomSeparatedWithNamesAndTypes\n+JSONCompactEachRow\n+JSONCompactEachRowWithNames\n+JSONCompactEachRowWithNamesAndTypes\n+JSONCompactStringsEachRow\n+JSONCompactStringsEachRowWithNames\n+JSONCompactStringsEachRowWithNamesAndTypes\n+JSONEachRow\n+JSONStringsEachRow\n+MsgPack\n+Native\n+ORC\n+Parquet\n+RowBinary\n+RowBinaryWithNames\n+RowBinaryWithNamesAndTypes\n+TSKV\n+TSV\n+TSVRaw\n+TSVRawWithNames\n+TSVRawWithNamesAndTypes\n+TSVWithNames\n+TSVWithNamesAndTypes\n+TabSeparated\n+TabSeparatedRaw\n+TabSeparatedRawWithNames\n+TabSeparatedRawWithNamesAndTypes\n+TabSeparatedWithNames\n+TabSeparatedWithNamesAndTypes\n+Values\n+LineAsString\n+OK\ndiff --git a/tests/queries/0_stateless/02187_async_inserts_all_formats.sh b/tests/queries/0_stateless/02187_async_inserts_all_formats.sh\nnew file mode 100755\nindex 000000000000..4b0b8d84c58f\n--- /dev/null\n+++ b/tests/queries/0_stateless/02187_async_inserts_all_formats.sh\n@@ -0,0 +1,9 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest, long\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# We should have correct env vars from shell_config.sh to run this test\n+python3 \"$CURDIR\"/02187_async_inserts_all_formats.python\ndiff --git a/tests/queries/0_stateless/helpers/pure_http_client.py b/tests/queries/0_stateless/helpers/pure_http_client.py\nindex 9f79c4ac5294..3335f141bb5b 100644\n--- a/tests/queries/0_stateless/helpers/pure_http_client.py\n+++ b/tests/queries/0_stateless/helpers/pure_http_client.py\n@@ -14,22 +14,23 @@ class ClickHouseClient:\n     def __init__(self, host = CLICKHOUSE_SERVER_URL_STR):\n         self.host = host\n \n-    def query(self, query, connection_timeout = 1500):\n+    def query(self, query, connection_timeout=1500, settings=dict(), binary_result=False):\n         NUMBER_OF_TRIES = 30\n         DELAY = 10\n \n+        params = {\n+            'timeout_before_checking_execution_speed': 120,\n+            'max_execution_time': 6000,\n+            'database': CLICKHOUSE_DATABASE,\n+        }\n+\n+        # Add extra settings to params\n+        params = {**params, **settings}\n+\n         for i in range(NUMBER_OF_TRIES):\n-            r = requests.post(\n-                self.host,\n-                params = {\n-                    'timeout_before_checking_execution_speed': 120,\n-                    'max_execution_time': 6000,\n-                    'database': CLICKHOUSE_DATABASE\n-                },\n-                timeout = connection_timeout,\n-                data = query)\n+            r = requests.post(self.host, params=params, timeout=connection_timeout, data=query)\n             if r.status_code == 200:\n-                return r.text\n+                return r.content if binary_result else r.text\n             else:\n                 print('ATTENTION: try #%d failed' % i)\n                 if i != (NUMBER_OF_TRIES-1):\n@@ -44,9 +45,22 @@ def query_return_df(self, query, connection_timeout = 1500):\n         df = pd.read_csv(io.StringIO(data), sep = '\\t')\n         return df\n \n-    def query_with_data(self, query, content):\n-        content = content.encode('utf-8')\n-        r = requests.post(self.host, data=content)\n+    def query_with_data(self, query, data, connection_timeout=1500, settings=dict()):\n+        params = {\n+            'query': query,\n+            'timeout_before_checking_execution_speed': 120,\n+            'max_execution_time': 6000,\n+            'database': CLICKHOUSE_DATABASE,\n+        }\n+\n+        headers = {\n+            \"Content-Type\": \"application/binary\"\n+        }\n+\n+        # Add extra settings to params\n+        params = {**params, **settings}\n+\n+        r = requests.post(self.host, params=params, timeout=connection_timeout, data=data, headers=headers)\n         result = r.text\n         if r.status_code == 200:\n             return result\n",
  "problem_statement": "INSERT INTO FORMAT Native with async_insert=1 appears to succeed, but does nothing\nI am trying to move to `async_insert` to avoid having a batching component merge many small inserts (the exact use case of `async_insert`). It works as intended over the HTTP protocol using both row-based input formats (CSV, TSV) and the column-based Parquet format (for testing, I generate Parquet data by piping CSV through clickhouse-local). However, if I try the more efficient and easier to generate Native input format, the HTTP query succeeds and returns a query id via the `X-ClickHouse-Query-Id` header, a corresponding entry in the `system.asynchronous_inserts` table is created and then deleted, and CH logs appear to indicate that the insert succeeded (see below), but no data is inserted into the table. If `async_insert` is set to 0, everything works correctly.\r\n\r\n**How to reproduce**\r\nCH version 21.12.3.32 (official build).\r\n\r\n```\r\nclickhouse-client -q \"CREATE TABLE async_inserts (id UInt32, s String) ENGINE = Memory\"\r\nfunction insert() {\r\n  curl -i --data-binary @- \"http://localhost:8123?query=INSERT+INTO+async_inserts+FORMAT+$1&async_insert=$2\"\r\n}\r\nfunction convert() {\r\n  clickhouse-local --input-format=CSV -N t -S 'id UInt32,s String' -q \"SELECT * FROM t FORMAT $1\"\r\n}\r\n\r\n# with async_insert=1\r\necho '1, \"a\"' | insert CSV 1\r\necho '2, \"b\"' | convert Parquet | insert Parquet 1\r\necho '3, \"c\"' | convert Native | insert Native 1\r\n\r\n# with async_insert=0\r\necho '4, \"a\"' | insert CSV 0\r\necho '5, \"b\"' | convert Parquet | insert Parquet 0\r\necho '6, \"c\"' | convert Native | insert Native 0\r\n\r\nclickhouse-client -q \"SELECT * FROM async_inserts FORMAT TSV\"\r\n# output:\r\n# 1   a\r\n# 2   b\r\n# 4   a\r\n# 5   b\r\n# 6   c\r\n```\r\n\r\n**Example records from /var/log/clickhouse-server/clickhouse-server.log**\r\n2022.01.11 21:35:54.615405 [ 9514 ] {d8f833c7-1b68-49ad-a8b4-abbb4e79c3e8} <Debug> executeQuery: (from [::ffff:127.0.0.1]:53242) INSERT INTO async_inserts FORMAT Native\r\n2022.01.11 21:35:54.615449 [ 9514 ] {d8f833c7-1b68-49ad-a8b4-abbb4e79c3e8} <Trace> ContextAccess (default): Access granted: INSERT(id, s) ON default.async_inserts\r\n2022.01.11 21:35:54.615500 [ 9514 ] {d8f833c7-1b68-49ad-a8b4-abbb4e79c3e8} <Trace> AsynchronousInsertQueue: Have 1 pending inserts with total 27 bytes of data for query 'INSERT INTO default.async_inserts FORMAT Native'\r\n2022.01.11 21:35:54.817042 [ 9514 ] {d8f833c7-1b68-49ad-a8b4-abbb4e79c3e8} <Debug> DynamicQueryHandler: Done processing query\r\n2022.01.11 21:35:54.817072 [ 9514 ] {d8f833c7-1b68-49ad-a8b4-abbb4e79c3e8} <Debug> MemoryTracker: Peak memory usage (for query): 3.00 MiB.\n",
  "hints_text": "@CurtizJ Anton, is this something that can be fixed in the near term?\nYes, I will fix it soon. It shouldn't be hard.\nThank you so much Anton, we really appreciate it! ",
  "created_at": "2022-01-27T19:15:28Z",
  "modified_files": [
    "src/Processors/Formats/Impl/NativeFormat.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/02187_async_inserts_all_formats.python",
    "b/tests/queries/0_stateless/02187_async_inserts_all_formats.reference",
    "b/tests/queries/0_stateless/02187_async_inserts_all_formats.sh",
    "tests/queries/0_stateless/helpers/pure_http_client.py"
  ]
}