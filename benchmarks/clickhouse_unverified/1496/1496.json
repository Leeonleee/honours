{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 1496,
  "instance_id": "ClickHouse__ClickHouse-1496",
  "issue_numbers": [
    "827"
  ],
  "base_commit": "ebec370fa53b06f51bf849e982b3c41b3119389e",
  "patch": "diff --git a/dbms/src/Common/ZooKeeper/ZooKeeper.cpp b/dbms/src/Common/ZooKeeper/ZooKeeper.cpp\nindex d1e3e7c3203d..abf05c848289 100644\n--- a/dbms/src/Common/ZooKeeper/ZooKeeper.cpp\n+++ b/dbms/src/Common/ZooKeeper/ZooKeeper.cpp\n@@ -902,7 +902,7 @@ ZooKeeper::GetChildrenFuture ZooKeeper::asyncGetChildren(const std::string & pat\n     return future;\n }\n \n-ZooKeeper::RemoveFuture ZooKeeper::asyncRemove(const std::string & path)\n+ZooKeeper::RemoveFuture ZooKeeper::asyncRemove(const std::string & path, int32_t version)\n {\n     RemoveFuture future {\n         [path] (int rc)\n@@ -912,7 +912,7 @@ ZooKeeper::RemoveFuture ZooKeeper::asyncRemove(const std::string & path)\n         }};\n \n     int32_t code = zoo_adelete(\n-        impl, path.c_str(), -1,\n+        impl, path.c_str(), version,\n         [] (int rc, const void * data)\n         {\n             RemoveFuture::TaskPtr owned_task =\n@@ -930,6 +930,36 @@ ZooKeeper::RemoveFuture ZooKeeper::asyncRemove(const std::string & path)\n     return future;\n }\n \n+ZooKeeper::TryRemoveFuture ZooKeeper::asyncTryRemove(const std::string & path, int32_t version)\n+{\n+    TryRemoveFuture future {\n+        [path] (int rc)\n+        {\n+            if (rc != ZOK && rc != ZNONODE && rc != ZBADVERSION && rc != ZNOTEMPTY)\n+                throw KeeperException(rc, path);\n+\n+            return rc;\n+        }};\n+\n+    int32_t code = zoo_adelete(\n+        impl, path.c_str(), version,\n+        [] (int rc, const void * data)\n+        {\n+            TryRemoveFuture::TaskPtr owned_task =\n+                std::move(const_cast<TryRemoveFuture::TaskPtr &>(*static_cast<const TryRemoveFuture::TaskPtr *>(data)));\n+            (*owned_task)(rc);\n+        },\n+        future.task.get());\n+\n+    ProfileEvents::increment(ProfileEvents::ZooKeeperRemove);\n+    ProfileEvents::increment(ProfileEvents::ZooKeeperTransactions);\n+\n+    if (code != ZOK)\n+        throw KeeperException(code, path);\n+\n+    return future;\n+}\n+\n ZooKeeper::MultiFuture ZooKeeper::asyncMultiImpl(const zkutil::Ops & ops_, bool throw_exception)\n {\n     size_t count = ops_.size();\ndiff --git a/dbms/src/Common/ZooKeeper/ZooKeeper.h b/dbms/src/Common/ZooKeeper/ZooKeeper.h\nindex e896a8702801..05ae522c75d5 100644\n--- a/dbms/src/Common/ZooKeeper/ZooKeeper.h\n+++ b/dbms/src/Common/ZooKeeper/ZooKeeper.h\n@@ -325,8 +325,14 @@ class ZooKeeper\n \n \n     using RemoveFuture = Future<void, int>;\n-    RemoveFuture asyncRemove(const std::string & path);\n+    RemoveFuture asyncRemove(const std::string & path, int32_t version = -1);\n \n+    /// Doesn't throw in the following cases:\n+    /// * The node doesn't exist\n+    /// * The versions do not match\n+    /// * The node has children\n+    using TryRemoveFuture = Future<int32_t, int>;\n+    TryRemoveFuture asyncTryRemove(const std::string & path, int32_t version = -1);\n \n     struct OpResultsAndCode\n     {\ndiff --git a/dbms/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp b/dbms/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp\nindex 5ff5c86ee98f..b13d90243f68 100644\n--- a/dbms/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp\n+++ b/dbms/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp\n@@ -130,8 +130,9 @@ void ReplicatedMergeTreeBlockOutputStream::write(const Block & block)\n             } hash_value;\n             hash.get128(hash_value.bytes);\n \n-            /// We take the hash from the data as ID. That is, do not insert the same data twice.\n-            block_id = toString(hash_value.words[0]) + \"_\" + toString(hash_value.words[1]);\n+            /// We add the hash from the data and partition identifier to deduplication ID.\n+            /// That is, do not insert the same data to the same partition twice.\n+            block_id = part->info.partition_id + \"_\" + toString(hash_value.words[0]) + \"_\" + toString(hash_value.words[1]);\n \n             LOG_DEBUG(log, \"Wrote block with ID '\" << block_id << \"', \" << block.rows() << \" rows\");\n         }\n@@ -331,7 +332,7 @@ void ReplicatedMergeTreeBlockOutputStream::commitPart(zkutil::ZooKeeperPtr & zoo\n     }\n     catch (const zkutil::KeeperException & e)\n     {\n-        /** If the connection is lost, and we do not know if the changes were applied, you can not delete the local chunk\n+        /** If the connection is lost, and we do not know if the changes were applied, you can not delete the local part\n             *  if the changes were applied, the inserted block appeared in `/blocks/`, and it can not be inserted again.\n             */\n         if (e.code == ZOPERATIONTIMEOUT ||\ndiff --git a/dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp b/dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp\nindex f9dffcbed09f..c336aea93a71 100644\n--- a/dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp\n+++ b/dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp\n@@ -16,8 +16,9 @@ namespace ErrorCodes\n ReplicatedMergeTreeCleanupThread::ReplicatedMergeTreeCleanupThread(StorageReplicatedMergeTree & storage_)\n     : storage(storage_),\n     log(&Logger::get(storage.database_name + \".\" + storage.table_name + \" (StorageReplicatedMergeTree, CleanupThread)\")),\n-    thread([this] { run(); }),\n-    cached_block_stats(std::make_unique<NodesStatCache>()) {}\n+    thread([this] { run(); })\n+{\n+}\n \n \n void ReplicatedMergeTreeCleanupThread::run()\n@@ -110,131 +111,104 @@ void ReplicatedMergeTreeCleanupThread::clearOldLogs()\n }\n \n \n-namespace\n-{\n-\n-/// Just a subset of zkutil::Stat fields required for the cache\n-struct RequiredStat\n-{\n-    int64_t ctime = 0;\n-    int32_t numChildren = 0;\n-\n-    RequiredStat() = default;\n-    RequiredStat(const RequiredStat &) = default;\n-    explicit RequiredStat(const zkutil::Stat & s) : ctime(s.ctime), numChildren(s.numChildren) {};\n-    explicit RequiredStat(Int64 ctime_) : ctime(ctime_) {}\n-};\n-\n-}\n-\n-/// Just a node name with its ZooKeeper's stat\n struct ReplicatedMergeTreeCleanupThread::NodeWithStat\n {\n     String node;\n-    RequiredStat stat;\n+    Int64 ctime = 0;\n \n-    NodeWithStat() = default;\n-    NodeWithStat(const String & node_, const RequiredStat & stat_) : node(node_), stat(stat_) {}\n+    NodeWithStat(String node_, Int64 ctime_) : node(std::move(node_)), ctime(ctime_) {}\n \n-    static bool greaterByTime (const NodeWithStat & lhs, const NodeWithStat & rhs)\n+    static bool greaterByTime(const NodeWithStat & lhs, const NodeWithStat & rhs)\n     {\n-        return std::greater<void>()(std::forward_as_tuple(lhs.stat.ctime, lhs.node), std::forward_as_tuple(rhs.stat.ctime, rhs.node));\n+        return std::forward_as_tuple(lhs.ctime, lhs.node) > std::forward_as_tuple(rhs.ctime, rhs.node);\n     }\n };\n \n-/// Use simple map node_name -> zkutil::Stat (only required fields) as the cache\n-/// It is not declared in the header explicitly to hide extra implementation dependent structs like RequiredStat\n-class ReplicatedMergeTreeCleanupThread::NodesStatCache : public std::map<String, RequiredStat> {};\n-\n-\n void ReplicatedMergeTreeCleanupThread::clearOldBlocks()\n {\n     auto zookeeper = storage.getZooKeeper();\n \n     std::vector<NodeWithStat> timed_blocks;\n-    getBlocksSortedByTime(zookeeper, timed_blocks);\n+    getBlocksSortedByTime(*zookeeper, timed_blocks);\n \n     if (timed_blocks.empty())\n         return;\n \n     /// Use ZooKeeper's first node (last according to time) timestamp as \"current\" time.\n-    Int64 current_time = timed_blocks.front().stat.ctime;\n+    Int64 current_time = timed_blocks.front().ctime;\n     Int64 time_threshold = std::max(static_cast<Int64>(0), current_time - static_cast<Int64>(1000 * storage.data.settings.replicated_deduplication_window_seconds));\n \n     /// Virtual node, all nodes that are \"greater\" than this one will be deleted\n-    NodeWithStat block_threshold(\"\", RequiredStat(time_threshold));\n+    NodeWithStat block_threshold{{}, time_threshold};\n \n     size_t current_deduplication_window = std::min(timed_blocks.size(), storage.data.settings.replicated_deduplication_window.value);\n     auto first_outdated_block_fixed_threshold = timed_blocks.begin() + current_deduplication_window;\n     auto first_outdated_block_time_threshold = std::upper_bound(timed_blocks.begin(), timed_blocks.end(), block_threshold, NodeWithStat::greaterByTime);\n     auto first_outdated_block = std::min(first_outdated_block_fixed_threshold, first_outdated_block_time_threshold);\n \n-    /// TODO After about half a year, we could remain only multi op, because there will be no obsolete children nodes.\n-    zkutil::Ops ops;\n+    std::vector<std::pair<String, zkutil::ZooKeeper::TryRemoveFuture>> try_remove_futures;\n     for (auto it = first_outdated_block; it != timed_blocks.end(); ++it)\n     {\n         String path = storage.zookeeper_path + \"/blocks/\" + it->node;\n-\n-        if (it->stat.numChildren == 0)\n-        {\n-            ops.emplace_back(new zkutil::Op::Remove(path, -1));\n-            if (ops.size() >= zkutil::MULTI_BATCH_SIZE)\n-            {\n-                zookeeper->multi(ops);\n-                ops.clear();\n-            }\n-        }\n-        else\n-            zookeeper->removeRecursive(path);\n+        try_remove_futures.emplace_back(path, zookeeper->asyncTryRemove(path));\n     }\n \n-    if (!ops.empty())\n+    for (auto & pair : try_remove_futures)\n     {\n-        zookeeper->multi(ops);\n-        ops.clear();\n+        const String & path = pair.first;\n+        int32_t rc = pair.second.get();\n+        if (rc == ZNOTEMPTY)\n+        {\n+             /// Can happen if there are leftover block nodes with children created by previous server versions.\n+            zookeeper->removeRecursive(path);\n+        }\n+        else if (rc != ZOK)\n+            LOG_WARNING(log,\n+                \"Error while deleting ZooKeeper path `\" << path << \"`: \" + zkutil::ZooKeeper::error2string(rc) << \", ignoring.\");\n     }\n \n     auto num_nodes_to_delete = timed_blocks.end() - first_outdated_block;\n-    LOG_TRACE(log, \"Cleared \" << num_nodes_to_delete << \" old blocks from ZooKeeper\");\n+    if (num_nodes_to_delete)\n+        LOG_TRACE(log, \"Cleared \" << num_nodes_to_delete << \" old blocks from ZooKeeper\");\n }\n \n \n-void ReplicatedMergeTreeCleanupThread::getBlocksSortedByTime(zkutil::ZooKeeperPtr & zookeeper, std::vector<NodeWithStat> & timed_blocks)\n+void ReplicatedMergeTreeCleanupThread::getBlocksSortedByTime(zkutil::ZooKeeper & zookeeper, std::vector<NodeWithStat> & timed_blocks)\n {\n     timed_blocks.clear();\n \n     Strings blocks;\n     zkutil::Stat stat;\n-    if (ZOK != zookeeper->tryGetChildren(storage.zookeeper_path + \"/blocks\", blocks, &stat))\n+    if (ZOK != zookeeper.tryGetChildren(storage.zookeeper_path + \"/blocks\", blocks, &stat))\n         throw Exception(storage.zookeeper_path + \"/blocks doesn't exist\", ErrorCodes::NOT_FOUND_NODE);\n \n     /// Clear already deleted blocks from the cache, cached_block_ctime should be subset of blocks\n     {\n         NameSet blocks_set(blocks.begin(), blocks.end());\n-        for (auto it = cached_block_stats->begin(); it != cached_block_stats->end();)\n+        for (auto it = cached_block_stats.begin(); it != cached_block_stats.end();)\n         {\n             if (!blocks_set.count(it->first))\n-                it = cached_block_stats->erase(it);\n+                it = cached_block_stats.erase(it);\n             else\n                 ++it;\n         }\n     }\n \n-    auto not_cached_blocks = stat.numChildren - cached_block_stats->size();\n+    auto not_cached_blocks = stat.numChildren - cached_block_stats.size();\n     if (not_cached_blocks)\n     {\n         LOG_TRACE(log, \"Checking \" << stat.numChildren << \" blocks (\" << not_cached_blocks << \" are not cached)\"\n-                                   << \" to clear old ones from ZooKeeper. This might take several minutes.\");\n+                                   << \" to clear old ones from ZooKeeper.\");\n     }\n \n     std::vector<std::pair<String, zkutil::ZooKeeper::ExistsFuture>> exists_futures;\n     for (const String & block : blocks)\n     {\n-        auto it = cached_block_stats->find(block);\n-        if (it == cached_block_stats->end())\n+        auto it = cached_block_stats.find(block);\n+        if (it == cached_block_stats.end())\n         {\n-            /// New block. Fetch its stat stat asynchronously\n-            exists_futures.emplace_back(block, zookeeper->asyncExists(storage.zookeeper_path + \"/blocks/\" + block));\n+            /// New block. Fetch its stat asynchronously.\n+            exists_futures.emplace_back(block, zookeeper.asyncExists(storage.zookeeper_path + \"/blocks/\" + block));\n         }\n         else\n         {\n@@ -247,11 +221,11 @@ void ReplicatedMergeTreeCleanupThread::getBlocksSortedByTime(zkutil::ZooKeeperPt\n     for (auto & elem : exists_futures)\n     {\n         zkutil::ZooKeeper::StatAndExists status = elem.second.get();\n-        if (!status.exists)\n-            throw zkutil::KeeperException(\"A block node was suddenly deleted\", ZNONODE);\n-\n-        cached_block_stats->emplace(elem.first, RequiredStat(status.stat));\n-        timed_blocks.emplace_back(elem.first, RequiredStat(status.stat));\n+        if (status.exists)\n+        {\n+            cached_block_stats.emplace(elem.first, status.stat.ctime);\n+            timed_blocks.emplace_back(elem.first, status.stat.ctime);\n+        }\n     }\n \n     std::sort(timed_blocks.begin(), timed_blocks.end(), NodeWithStat::greaterByTime);\ndiff --git a/dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h b/dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h\nindex 443a2af8ec40..b9fbda531a98 100644\n--- a/dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h\n+++ b/dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h\n@@ -33,15 +33,15 @@ class ReplicatedMergeTreeCleanupThread\n     /// Remove old records from ZooKeeper.\n     void clearOldLogs();\n \n-    /// Remove old block hashes from ZooKeeper. This makes a leading replica.\n+    /// Remove old block hashes from ZooKeeper. This is done by the leader replica.\n     void clearOldBlocks();\n \n-    class NodesStatCache;\n-    struct NodeWithStat;\n-    std::unique_ptr<NodesStatCache> cached_block_stats;\n+    using NodeCTimeCache = std::map<String, Int64>;\n+    NodeCTimeCache cached_block_stats;\n \n-    /// Returns list of blocks (with their stat) sorted by ctime in descending order\n-    void getBlocksSortedByTime(std::shared_ptr<zkutil::ZooKeeper> & zookeeper, std::vector<NodeWithStat> & timed_blocks);\n+    struct NodeWithStat;\n+    /// Returns list of blocks (with their stat) sorted by ctime in descending order.\n+    void getBlocksSortedByTime(zkutil::ZooKeeper & zookeeper, std::vector<NodeWithStat> & timed_blocks);\n \n     /// TODO Removing old quorum/failed_parts\n     /// TODO Removing old nonincrement_block_numbers\ndiff --git a/dbms/src/Storages/StorageReplicatedMergeTree.cpp b/dbms/src/Storages/StorageReplicatedMergeTree.cpp\nindex 70ce91cb9a77..f77c4e6f2c7b 100644\n--- a/dbms/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/dbms/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -2669,7 +2669,8 @@ static String getFakePartNameCoveringPartRange(\n }\n \n \n-String StorageReplicatedMergeTree::getFakePartNameCoveringAllPartsInPartition(const String & partition_id)\n+String StorageReplicatedMergeTree::getFakePartNameCoveringAllPartsInPartition(\n+    const String & partition_id, Int64 * out_min_block, Int64 * out_max_block)\n {\n     /// Even if there is no data in the partition, you still need to mark the range for deletion.\n     /// - Because before executing DETACH, tasks for downloading parts to this partition can be executed.\n@@ -2695,6 +2696,11 @@ String StorageReplicatedMergeTree::getFakePartNameCoveringAllPartsInPartition(co\n         return {};\n \n     --right;\n+\n+    if (out_min_block)\n+        *out_min_block = left;\n+    if (out_max_block)\n+        *out_max_block = right;\n     return getFakePartNameCoveringPartRange(data.format_version, partition_id, left, right);\n }\n \n@@ -2740,6 +2746,8 @@ void StorageReplicatedMergeTree::dropPartition(const ASTPtr & query, const ASTPt\n {\n     assertNotReadonly();\n \n+    zkutil::ZooKeeperPtr zookeeper = getZooKeeper();\n+\n     if (!is_leader_node)\n     {\n         sendRequestToLeaderReplica(query, context.getSettingsRef());\n@@ -2747,7 +2755,10 @@ void StorageReplicatedMergeTree::dropPartition(const ASTPtr & query, const ASTPt\n     }\n \n     String partition_id = data.getPartitionIDFromQuery(partition, context);\n-    String fake_part_name = getFakePartNameCoveringAllPartsInPartition(partition_id);\n+\n+    Int64 min_block = 0;\n+    Int64 max_block = 0;\n+    String fake_part_name = getFakePartNameCoveringAllPartsInPartition(partition_id, &min_block, &max_block);\n \n     if (fake_part_name.empty())\n     {\n@@ -2755,6 +2766,8 @@ void StorageReplicatedMergeTree::dropPartition(const ASTPtr & query, const ASTPt\n         return;\n     }\n \n+    clearBlocksInPartition(*zookeeper, partition_id, min_block, max_block);\n+\n     /** Forbid to choose the parts to be deleted for merging.\n       * Invariant: after the `DROP_RANGE` entry appears in the log, merge of deleted parts will not appear in the log.\n       */\n@@ -2773,7 +2786,7 @@ void StorageReplicatedMergeTree::dropPartition(const ASTPtr & query, const ASTPt\n     entry.detach = detach;\n     entry.create_time = time(nullptr);\n \n-    String log_znode_path = getZooKeeper()->create(zookeeper_path + \"/log/log-\", entry.toString(), zkutil::CreateMode::PersistentSequential);\n+    String log_znode_path = zookeeper->create(zookeeper_path + \"/log/log-\", entry.toString(), zkutil::CreateMode::PersistentSequential);\n     entry.znode_name = log_znode_path.substr(log_znode_path.find_last_of('/') + 1);\n \n     /// If necessary, wait until the operation is performed on itself or on all replicas.\n@@ -4037,4 +4050,56 @@ void StorageReplicatedMergeTree::removePartsFromZooKeeper(zkutil::ZooKeeperPtr &\n }\n \n \n+void StorageReplicatedMergeTree::clearBlocksInPartition(\n+    zkutil::ZooKeeper & zookeeper, const String & partition_id, Int64 min_block_num, Int64 max_block_num)\n+{\n+    Strings blocks;\n+    if (ZOK != zookeeper.tryGetChildren(zookeeper_path + \"/blocks\", blocks))\n+        throw Exception(zookeeper_path + \"/blocks doesn't exist\", ErrorCodes::NOT_FOUND_NODE);\n+\n+    String partition_prefix = partition_id + \"_\";\n+    std::vector<std::pair<String, zkutil::ZooKeeper::TryGetFuture>> get_futures;\n+    for (const String & block_id : blocks)\n+    {\n+        if (startsWith(block_id, partition_prefix))\n+        {\n+            String path = zookeeper_path + \"/blocks/\" + block_id;\n+            get_futures.emplace_back(path, zookeeper.asyncTryGet(path));\n+        }\n+    }\n+\n+    std::vector<std::pair<String, zkutil::ZooKeeper::TryRemoveFuture>> to_delete_futures;\n+    for (auto & pair : get_futures)\n+    {\n+        const String & path = pair.first;\n+        zkutil::ZooKeeper::ValueAndStatAndExists result = pair.second.get();\n+\n+        if (!result.exists)\n+            continue;\n+\n+        ReadBufferFromString buf(result.value);\n+        Int64 block_num = 0;\n+        bool parsed = tryReadIntText(block_num, buf) && buf.eof();\n+        if (!parsed || (min_block_num <= block_num && block_num <= max_block_num))\n+            to_delete_futures.emplace_back(path, zookeeper.asyncTryRemove(path));\n+    }\n+\n+    for (auto & pair : to_delete_futures)\n+    {\n+        const String & path = pair.first;\n+        int32_t rc = pair.second.get();\n+        if (rc == ZNOTEMPTY)\n+        {\n+             /// Can happen if there are leftover block nodes with children created by previous server versions.\n+            zookeeper.removeRecursive(path);\n+        }\n+        else if (rc != ZOK)\n+            LOG_WARNING(log,\n+                \"Error while deleting ZooKeeper path `\" << path << \"`: \" + zkutil::ZooKeeper::error2string(rc) << \", ignoring.\");\n+    }\n+\n+    LOG_TRACE(log, \"Deleted \" << to_delete_futures.size() << \" deduplication block IDs in partition ID \" << partition_id);\n+}\n+\n+\n }\ndiff --git a/dbms/src/Storages/StorageReplicatedMergeTree.h b/dbms/src/Storages/StorageReplicatedMergeTree.h\nindex 745ef7f34876..2529452a49b8 100644\n--- a/dbms/src/Storages/StorageReplicatedMergeTree.h\n+++ b/dbms/src/Storages/StorageReplicatedMergeTree.h\n@@ -289,7 +289,7 @@ class StorageReplicatedMergeTree : public ext::shared_ptr_helper<StorageReplicat\n     /// Limiting parallel fetches per one table\n     std::atomic_uint current_table_fetches {0};\n \n-    /// Streams\n+    /// Threads.\n \n     /// A thread that keeps track of the updates in the logs of all replicas and loads them into the queue.\n     std::thread queue_updating_thread;\n@@ -455,14 +455,18 @@ class StorageReplicatedMergeTree : public ext::shared_ptr_helper<StorageReplicat\n     void assertNotReadonly() const;\n \n     /// The name of an imaginary part covering all parts in the specified partition (at the call moment).\n-    /// Returns empty string if partition is empty.\n-    String getFakePartNameCoveringAllPartsInPartition(const String & partition_id);\n+    /// Returns empty string if the partition doesn't exist yet.\n+    String getFakePartNameCoveringAllPartsInPartition(\n+        const String & partition_id, Int64 * out_min_block = nullptr, Int64 * out_max_block = nullptr);\n \n     /// Check for a node in ZK. If it is, remember this information, and then immediately answer true.\n     std::unordered_set<std::string> existing_nodes_cache;\n     std::mutex existing_nodes_cache_mutex;\n     bool existsNodeCached(const std::string & path);\n \n+    /// Remove block IDs from `blocks/` in ZooKeeper for the given partition ID in the given block number range.\n+    void clearBlocksInPartition(\n+        zkutil::ZooKeeper & zookeeper, const String & partition_id, Int64 min_block_num, Int64 max_block_num);\n \n     /// Resharding.\n     struct ReplicaSpaceInfo\n",
  "test_patch": "diff --git a/dbms/tests/queries/0_stateless/00516_deduplication_after_drop_partition_zookeeper.reference b/dbms/tests/queries/0_stateless/00516_deduplication_after_drop_partition_zookeeper.reference\nnew file mode 100644\nindex 000000000000..2396dc2b5dae\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/00516_deduplication_after_drop_partition_zookeeper.reference\n@@ -0,0 +1,21 @@\n+*** Before DROP PARTITION ***\n+2000-01-01\t1\n+2000-01-01\t2\n+2000-01-01\t3\n+2000-02-01\t3\n+2000-02-01\t4\n+2000-02-01\t5\n+*** After DROP PARTITION ***\n+2000-02-01\t3\n+2000-02-01\t4\n+2000-02-01\t5\n+*** After INSERT ***\n+2000-01-01\t1\n+2000-01-01\t2\n+2000-01-01\t3\n+2000-01-01\t4\n+2000-02-01\t3\n+2000-02-01\t4\n+2000-02-01\t5\n+2000-02-01\t6\n+2000-02-01\t7\ndiff --git a/dbms/tests/queries/0_stateless/00516_deduplication_after_drop_partition_zookeeper.sql b/dbms/tests/queries/0_stateless/00516_deduplication_after_drop_partition_zookeeper.sql\nnew file mode 100644\nindex 000000000000..f132237c6241\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/00516_deduplication_after_drop_partition_zookeeper.sql\n@@ -0,0 +1,34 @@\n+DROP TABLE IF EXISTS test.deduplication_by_partition;\n+CREATE TABLE test.deduplication_by_partition(d Date, x UInt32) ENGINE =\n+    ReplicatedMergeTree('/clickhouse/tables/test/deduplication_by_partition', 'r1', d, x, 8192);\n+\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-01-01', 1);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-01-01', 2), ('2000-01-01', 3);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-01-01', 1);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-01-01', 2), ('2000-01-01', 3);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-02-01', 3), ('2000-02-01', 4), ('2000-02-01', 5);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-02-01', 3), ('2000-02-01', 4), ('2000-02-01', 5);\n+\n+SELECT '*** Before DROP PARTITION ***';\n+\n+SELECT * FROM test.deduplication_by_partition ORDER BY d, x;\n+\n+ALTER TABLE test.deduplication_by_partition DROP PARTITION 200001;\n+\n+SELECT '*** After DROP PARTITION ***';\n+\n+SELECT * FROM test.deduplication_by_partition ORDER BY d, x;\n+\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-01-01', 1);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-01-01', 1);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-01-01', 2), ('2000-01-01', 3);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-01-01', 2), ('2000-01-01', 3);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-01-01', 4);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-02-01', 3), ('2000-02-01', 4), ('2000-02-01', 5);\n+INSERT INTO test.deduplication_by_partition VALUES ('2000-02-01', 6), ('2000-02-01', 7);\n+\n+SELECT '*** After INSERT ***';\n+\n+SELECT * FROM test.deduplication_by_partition ORDER BY d, x;\n+\n+DROP TABLE test.deduplication_by_partition;\n",
  "problem_statement": "Cannot Insert data into ReplicatedMergeTree\n> (Replicated OutputStream): Block with ID 9125361514582012557_14309323673096342990 already exists; ignoring it (removing part 20170506_20170506_265_265_0)\r\n\r\nI would like to know if this is normal, and in what scenario will happen?\n",
  "hints_text": "It happens, when you insert duplicate block - with exactly the same content as before.\r\n\r\nDocumentation:\r\n\r\n> Blocks of data are duplicated. For multiple writes of the same data block (data blocks of the same size containing the same rows in the same order), the block is only written once. The reason for this is in case of network failures when the client application doesn't know if the data was written to the DB, so the INSERT query can simply be repeated. It doesn't matter which replica INSERTs were sent to with identical data - INSERTs are idempotent.\r\n\r\nhttps://clickhouse.yandex/reference_en.html#Data%20replication\n@alexey-milovidov but, before insert same block, Has been successfully executed **alter table x drop partition 201705**\nYes, this is the issue. `DROP PARTITION` doesn't clear set of blocks for deduplication.\r\nCurrently, the only solution, is to clear all nodes in ZooKeeper inside `/blocks/` in table path.\nWhich nodes in Zookeeper could be clear is difficult to select.\r\nis there prossible to delete nodes in Zookeeper during droping partition?\nYes, I plan to fix it soon.\nAny updates?\nWe are doing this task right now.",
  "created_at": "2017-11-15T18:23:08Z",
  "modified_files": [
    "dbms/src/Common/ZooKeeper/ZooKeeper.cpp",
    "dbms/src/Common/ZooKeeper/ZooKeeper.h",
    "dbms/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp",
    "dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.cpp",
    "dbms/src/Storages/MergeTree/ReplicatedMergeTreeCleanupThread.h",
    "dbms/src/Storages/StorageReplicatedMergeTree.cpp",
    "dbms/src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "b/dbms/tests/queries/0_stateless/00516_deduplication_after_drop_partition_zookeeper.reference",
    "b/dbms/tests/queries/0_stateless/00516_deduplication_after_drop_partition_zookeeper.sql"
  ]
}