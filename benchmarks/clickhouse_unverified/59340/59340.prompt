You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Vertical merge could consume a lot of memory
See the stack trace with memory allocations.
https://pastila.nl/?0001d4ae/376da778773f6223011c64914c06ec3e#xGswcayE+r52tA2oBpGs6A==

Stack trace in short:
There are big allocations (up to 2,5Gb) happen in 
```
Allocator<false, false>, 63ul, 64ul>::resize<>(unsigned long)
DB::ColumnString::insertRangeFrom(DB::IColumn const&, unsigned long, unsigned long) | DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)
DB::ColumnString::gather(DB::ColumnGathererStream&)','DB::ColumnGathererStream::merge()
```

That happen because the value size is highly different. Table has a string column with 
```
SELECT quantilesExactExclusive(0.5, 0.9, 0.99, 0.999)(length(column))
FROM table

┌─quantilesExactExclusive(0.5, 0.9, 0.99, 0.999)(length(column))─┐
│ [79064,121150,135871,1413296]                                      │
└────────────────────────────────────────────────────────────────────┘
```

```
SELECT
    max(length(column))
FROM table
┌─max(length(column))─┐
│      9834237        │
└─────────────────────┘
```

There is a big string could happen.

ColumnGathererStream doesn't care about column size in bytes. It always takes `#define DEFAULT_BLOCK_SIZE 65409` rows in gathered column.

That is very annoying reason why 8Gb memory is not enough for small databases. it leads to the situation when all data is read into the memory.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
