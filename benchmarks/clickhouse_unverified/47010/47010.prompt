You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
New replica download parts to default disk instead of external disk
**Describe what's wrong**

Newly added replica downloads all data to default disk regardless of its location on other replicas.

**Does it reproduce on recent release?**

Experienced in our production on version 22.8.11.15


**How to reproduce**
Consider two replicas with following config for externa storage:
```
<yandex>
    <merge_tree>
        <allow_remote_fs_zero_copy_replication>true</allow_remote_fs_zero_copy_replication>
    </merge_tree>
    <storage_configuration>
        <disks>
            <s3_debug>
                <type>s3</type>
                <endpoint>https://s3.some-host/test/debug/001/</endpoint>
                <access_key_id>xxxxxxxxxxxxxxxxxxxx</access_key_id>
                <secret_access_key>xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</secret_access_key>
            </s3_debug>
        </disks>
        <policies>
            <s3_debug>
                <volumes>
                    <default>
                        <disk>default</disk>
                    </default>
                    <external>
                        <disk>s3_debug</disk>
                        <prefer_not_to_merge>False</prefer_not_to_merge>
                        <perform_ttl_move_on_insert>True</perform_ttl_move_on_insert>
                    </external>
                </volumes>
            </s3_debug>
        </policies>
    </storage_configuration>
</yandex>
```

Cretate test table on one replica and insert some data:
```
CREATE DATABASE debug;

CREATE TABLE debug.test1 (EventDate Date, CounterID UInt32)
ENGINE = ReplicatedMergeTree('/clickhouse-tables/{shard}/test1', '{replica}')
PARTITION BY toMonday(EventDate)
ORDER BY (CounterID, EventDate)
SAMPLE BY intHash32(CounterID)
SETTINGS index_granularity = 8192, storage_policy = 's3_debug'

INSERT INTO debug.test1 SELECT toDate('2023-01-01') + toIntervalDay(number), number + 1000 from system.numbers limit 20;
```

Look at partitions:
```
SELECT
        database,
        table,
        partition,
        disk_name,
        count(*) AS parts,
        sum(rows) AS total_rows,
        sum(bytes_on_disk) AS total_bytes_on_disk,
        min(min_date) AS min_date_in_partition,
        max(max_date) AS max_date_in_partition
    FROM system.parts
    WHERE (database = 'debug') AND (table = 'test1') AND (active = 1)
    GROUP BY
        database,
        table,
        partition,
        disk_name
    ORDER BY partition ASC

┌─database─┬─table─┬─partition──┬─disk_name─┬─parts─┬─total_rows─┬─total_bytes_on_disk─┬─min_date_in_partition─┬─max_date_in_partition─┐
│ debug    │ test1 │ 2022-12-26 │ default   │     1 │          1 │                 173 │            2023-01-01 │            2023-01-01 │
│ debug    │ test1 │ 2023-01-02 │ default   │     1 │          7 │                 209 │            2023-01-02 │            2023-01-08 │
│ debug    │ test1 │ 2023-01-09 │ default   │     1 │          7 │                 209 │            2023-01-09 │            2023-01-15 │
│ debug    │ test1 │ 2023-01-16 │ default   │     1 │          5 │                 197 │            2023-01-16 │            2023-01-20 │
└──────────┴───────┴────────────┴───────────┴───────┴────────────┴─────────────────────┴───────────────────────┴───────────────────────┘
```
all data is local.

Move partitions to external storage:

```
ALTER TABLE debug.test1 MOVE PARTITION '2022-12-26' TO DISK 's3_debug';
ALTER TABLE debug.test1 MOVE PARTITION '2023-01-02' TO DISK 's3_debug';
ALTER TABLE debug.test1 MOVE PARTITION '2023-01-09' TO DISK 's3_debug';
```
And look again at partitons:
```
┌─database─┬─table─┬─partition──┬─disk_name─┬─parts─┬─total_rows─┬─total_bytes_on_disk─┬─min_date_in_partition─┬─max_date_in_partition─┐
│ debug    │ test1 │ 2022-12-26 │ s3_debug  │     1 │          1 │                 173 │            2023-01-01 │            2023-01-01 │
│ debug    │ test1 │ 2023-01-02 │ s3_debug  │     1 │          7 │                 209 │            2023-01-02 │            2023-01-08 │
│ debug    │ test1 │ 2023-01-09 │ s3_debug  │     1 │          7 │                 209 │            2023-01-09 │            2023-01-15 │
│ debug    │ test1 │ 2023-01-16 │ default   │     1 │          5 │                 197 │            2023-01-16 │            2023-01-20 │
└──────────┴───────┴────────────┴───────────┴───────┴────────────┴─────────────────────┴───────────────────────┴───────────────────────┘
```
Some data on external storage.

Then switch to second replica and create table:

```
CREATE DATABASE debug;

CREATE TABLE debug.test1 (EventDate Date, CounterID UInt32)
ENGINE = ReplicatedMergeTree('/clickhouse-tables/{shard}/test1', '{replica}')
PARTITION BY toMonday(EventDate)
ORDER BY (CounterID, EventDate)
SAMPLE BY intHash32(CounterID)
SETTINGS index_granularity = 8192, storage_policy = 's3_debug'
```

And look at partitions of newly created replica
```
┌─database─┬─table─┬─partition──┬─disk_name─┬─parts─┬─total_rows─┬─total_bytes_on_disk─┬─min_date_in_partition─┬─max_date_in_partition─┐
│ debug    │ test1 │ 2022-12-26 │ default   │     1 │          1 │                 173 │            2023-01-01 │            2023-01-01 │
│ debug    │ test1 │ 2023-01-02 │ default   │     1 │          7 │                 209 │            2023-01-02 │            2023-01-08 │
│ debug    │ test1 │ 2023-01-09 │ default   │     1 │          7 │                 209 │            2023-01-09 │            2023-01-15 │
│ debug    │ test1 │ 2023-01-16 │ default   │     1 │          5 │                 197 │            2023-01-16 │            2023-01-20 │
└──────────┴───────┴────────────┴───────────┴───────┴────────────┴─────────────────────┴───────────────────────┴───────────────────────┘
```

The data is here, but on default disk instead of external storage.



**Expected behavior**

The data on newly created replica shoud be on external storage as on first replica.


**Additional context**

There will be some complications because of possible different storage_policy settings and different storage policies itself.

But it is a blocker for scenario of replica redeployment when we need to reinstall operating system from scratch and recreate replicated tables in case of hardware failures. It will be impossible to redeploy if total amount of data is more than local disk size.

Probably CREATE TABLE should fail in case of different storage_policy settings and differences in storage policies.

And it will be more complicated in case of more than two replicas.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
