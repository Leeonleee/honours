{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 47970,
  "instance_id": "ClickHouse__ClickHouse-47970",
  "issue_numbers": [
    "47616"
  ],
  "base_commit": "52541e5e238b271b801882dfa3a9ba4e6c163852",
  "patch": "diff --git a/src/Bridge/IBridge.cpp b/src/Bridge/IBridge.cpp\nindex daae05de8923..1ea77573e5f3 100644\n--- a/src/Bridge/IBridge.cpp\n+++ b/src/Bridge/IBridge.cpp\n@@ -4,21 +4,22 @@\n #include <Poco/Net/NetException.h>\n #include <Poco/Util/HelpFormatter.h>\n \n-#include <base/range.h>\n-\n-#include <Common/StringUtils/StringUtils.h>\n #include <Common/SensitiveDataMasker.h>\n-#include \"config.h\"\n+#include <Common/StringUtils/StringUtils.h>\n #include <Common/logger_useful.h>\n-#include <base/errnoToString.h>\n-#include <IO/ReadHelpers.h>\n #include <Formats/registerFormats.h>\n-#include <Server/HTTP/HTTPServer.h>\n+#include <IO/ReadHelpers.h>\n #include <IO/WriteBufferFromFile.h>\n #include <IO/WriteHelpers.h>\n+#include <Server/HTTP/HTTPServer.h>\n+#include <base/errnoToString.h>\n+#include <base/range.h>\n+\n #include <sys/time.h>\n #include <sys/resource.h>\n \n+#include \"config.h\"\n+\n #if USE_ODBC\n #    include <Poco/Data/ODBC/Connector.h>\n #endif\n@@ -89,7 +90,7 @@ void IBridge::defineOptions(Poco::Util::OptionSet & options)\n         Poco::Util::Option(\"listen-host\", \"\", \"hostname or address to listen, default 127.0.0.1\").argument(\"listen-host\").binding(\"listen-host\"));\n \n     options.addOption(\n-        Poco::Util::Option(\"http-timeout\", \"\", \"http timeout for socket, default 1800\").argument(\"http-timeout\").binding(\"http-timeout\"));\n+        Poco::Util::Option(\"http-timeout\", \"\", \"http timeout for socket, default 180\").argument(\"http-timeout\").binding(\"http-timeout\"));\n \n     options.addOption(\n         Poco::Util::Option(\"max-server-connections\", \"\", \"max connections to server, default 1024\").argument(\"max-server-connections\").binding(\"max-server-connections\"));\n@@ -97,6 +98,9 @@ void IBridge::defineOptions(Poco::Util::OptionSet & options)\n     options.addOption(\n         Poco::Util::Option(\"keep-alive-timeout\", \"\", \"keepalive timeout, default 10\").argument(\"keep-alive-timeout\").binding(\"keep-alive-timeout\"));\n \n+    options.addOption(\n+        Poco::Util::Option(\"http-max-field-value-size\", \"\", \"max http field value size, default 1048576\").argument(\"http-max-field-value-size\").binding(\"http-max-field-value-size\"));\n+\n     options.addOption(\n         Poco::Util::Option(\"log-level\", \"\", \"sets log level, default info\") .argument(\"log-level\").binding(\"logger.level\"));\n \n@@ -165,6 +169,7 @@ void IBridge::initialize(Application & self)\n     http_timeout = config().getUInt64(\"http-timeout\", DEFAULT_HTTP_READ_BUFFER_TIMEOUT);\n     max_server_connections = config().getUInt(\"max-server-connections\", 1024);\n     keep_alive_timeout = config().getUInt64(\"keep-alive-timeout\", 10);\n+    http_max_field_value_size = config().getUInt64(\"http-max-field-value-size\", 1048576);\n \n     struct rlimit limit;\n     const UInt64 gb = 1024 * 1024 * 1024;\n@@ -226,6 +231,10 @@ int IBridge::main(const std::vector<std::string> & /*args*/)\n     auto context = Context::createGlobal(shared_context.get());\n     context->makeGlobalContext();\n \n+    auto settings = context->getSettings();\n+    settings.set(\"http_max_field_value_size\", http_max_field_value_size);\n+    context->setSettings(settings);\n+\n     if (config().has(\"query_masking_rules\"))\n         SensitiveDataMasker::setInstance(std::make_unique<SensitiveDataMasker>(config(), \"query_masking_rules\"));\n \ndiff --git a/src/Bridge/IBridge.h b/src/Bridge/IBridge.h\nindex 68af8860cb32..e6855b69dddb 100644\n--- a/src/Bridge/IBridge.h\n+++ b/src/Bridge/IBridge.h\n@@ -45,6 +45,7 @@ class IBridge : public BaseDaemon\n     std::string log_level;\n     unsigned max_server_connections;\n     size_t http_timeout;\n+    size_t http_max_field_value_size;\n \n     Poco::Logger * log;\n };\ndiff --git a/src/BridgeHelper/IBridgeHelper.cpp b/src/BridgeHelper/IBridgeHelper.cpp\nindex 3a8a8b8fdd2c..844e2505e9c2 100644\n--- a/src/BridgeHelper/IBridgeHelper.cpp\n+++ b/src/BridgeHelper/IBridgeHelper.cpp\n@@ -67,6 +67,8 @@ std::unique_ptr<ShellCommand> IBridgeHelper::startBridgeCommand()\n     cmd_args.push_back(config.getString(configPrefix() + \".listen_host\", DEFAULT_HOST));\n     cmd_args.push_back(\"--http-timeout\");\n     cmd_args.push_back(std::to_string(getHTTPTimeout().totalMicroseconds()));\n+    cmd_args.push_back(\"--http-max-field-value-size\");\n+    cmd_args.push_back(\"99999999999999999\"); // something \"big\" to accept large datasets (issue 47616)\n     if (config.has(\"logger.\" + configPrefix() + \"_log\"))\n     {\n         cmd_args.push_back(\"--log-path\");\ndiff --git a/utils/changelog-simple/format-changelog.py b/utils/changelog-simple/format-changelog.py\nindex d5e1518270e1..01f2694dd0ff 100755\n--- a/utils/changelog-simple/format-changelog.py\n+++ b/utils/changelog-simple/format-changelog.py\n@@ -20,6 +20,7 @@\n )\n args = parser.parse_args()\n \n+\n # This function mirrors the PR description checks in ClickhousePullRequestTrigger.\n # Returns False if the PR should not be mentioned changelog.\n def parse_one_pull_request(item):\ndiff --git a/utils/keeper-overload/keeper-overload.py b/utils/keeper-overload/keeper-overload.py\nindex bdb4563c713e..0a059b105889 100755\n--- a/utils/keeper-overload/keeper-overload.py\n+++ b/utils/keeper-overload/keeper-overload.py\n@@ -166,7 +166,7 @@ def main(args):\n     keeper_bench_path = args.keeper_bench_path\n \n     keepers = []\n-    for (port, server_id) in zip(PORTS, SERVER_IDS):\n+    for port, server_id in zip(PORTS, SERVER_IDS):\n         keepers.append(\n             Keeper(\n                 keeper_binary_path, server_id, port, workdir, args.with_thread_fuzzer\n",
  "test_patch": "diff --git a/docker/test/performance-comparison/perf.py b/docker/test/performance-comparison/perf.py\nindex 65bf49c29140..7a4e6386d0df 100755\n--- a/docker/test/performance-comparison/perf.py\n+++ b/docker/test/performance-comparison/perf.py\n@@ -26,6 +26,7 @@\n total_start_seconds = time.perf_counter()\n stage_start_seconds = total_start_seconds\n \n+\n # Thread executor that does not hides exception that happens during function\n # execution, and rethrows it after join()\n class SafeThread(Thread):\n@@ -158,6 +159,7 @@ def tsv_escape(s):\n \n     available_parameters[name] = values\n \n+\n # Takes parallel lists of templates, substitutes them with all combos of\n # parameters. The set of parameters is determined based on the first list.\n # Note: keep the order of queries -- sometimes we have DROP IF EXISTS\ndiff --git a/docker/test/performance-comparison/report.py b/docker/test/performance-comparison/report.py\nindex 782cf29863c1..214f2d550b4d 100755\n--- a/docker/test/performance-comparison/report.py\n+++ b/docker/test/performance-comparison/report.py\n@@ -670,7 +670,6 @@ def add_test_times():\n     )\n \n elif args.report == \"all-queries\":\n-\n     print((header_template.format()))\n \n     add_tested_commits()\ndiff --git a/tests/ci/clickhouse_helper.py b/tests/ci/clickhouse_helper.py\nindex d60a9e6afd15..64b64896f661 100644\n--- a/tests/ci/clickhouse_helper.py\n+++ b/tests/ci/clickhouse_helper.py\n@@ -141,7 +141,6 @@ def prepare_tests_results_for_clickhouse(\n     report_url: str,\n     check_name: str,\n ) -> List[dict]:\n-\n     pull_request_url = \"https://github.com/ClickHouse/ClickHouse/commits/master\"\n     base_ref = \"master\"\n     head_ref = \"master\"\ndiff --git a/tests/ci/docker_images_check.py b/tests/ci/docker_images_check.py\nindex 192d216614e1..f2b1105b3b0f 100644\n--- a/tests/ci/docker_images_check.py\n+++ b/tests/ci/docker_images_check.py\n@@ -96,7 +96,6 @@ def get_images_dict(repo_path: str, image_file_path: str) -> ImagesDict:\n def get_changed_docker_images(\n     pr_info: PRInfo, images_dict: ImagesDict\n ) -> Set[DockerImage]:\n-\n     if not images_dict:\n         return set()\n \ndiff --git a/tests/ci/get_previous_release_tag.py b/tests/ci/get_previous_release_tag.py\nindex c6fe6cd5fb59..c2d279f7fece 100755\n--- a/tests/ci/get_previous_release_tag.py\n+++ b/tests/ci/get_previous_release_tag.py\n@@ -51,7 +51,6 @@ def find_previous_release(\n \n     for release in releases:\n         if release.version < server_version:\n-\n             # Check if the artifact exists on GitHub.\n             # It can be not true for a short period of time\n             # after creating a tag for a new release before uploading the packages.\ndiff --git a/tests/ci/report.py b/tests/ci/report.py\nindex 947fb33d9057..ddee035d26fa 100644\n--- a/tests/ci/report.py\n+++ b/tests/ci/report.py\n@@ -473,7 +473,7 @@ def create_build_html_report(\n     commit_url: str,\n ) -> str:\n     rows = \"\"\n-    for (build_result, build_log_url, artifact_urls) in zip(\n+    for build_result, build_log_url, artifact_urls in zip(\n         build_results, build_logs_urls, artifact_urls_list\n     ):\n         row = \"<tr>\"\ndiff --git a/tests/integration/helpers/cluster.py b/tests/integration/helpers/cluster.py\nindex dc5ada819956..a9a996e0a5fd 100644\n--- a/tests/integration/helpers/cluster.py\n+++ b/tests/integration/helpers/cluster.py\n@@ -63,6 +63,7 @@\n \n SANITIZER_SIGN = \"==================\"\n \n+\n # to create docker-compose env file\n def _create_env_file(path, variables):\n     logging.debug(f\"Env {variables} stored in {path}\")\n@@ -1454,7 +1455,6 @@ def add_instance(\n         config_root_name=\"clickhouse\",\n         extra_configs=[],\n     ) -> \"ClickHouseInstance\":\n-\n         \"\"\"Add an instance to the cluster.\n \n         name - the name of the instance directory and the value of the 'instance' macro in ClickHouse.\n@@ -3089,7 +3089,6 @@ def __init__(\n         config_root_name=\"clickhouse\",\n         extra_configs=[],\n     ):\n-\n         self.name = name\n         self.base_cmd = cluster.base_cmd\n         self.docker_id = cluster.get_instance_docker_id(self.name)\ndiff --git a/tests/integration/helpers/network.py b/tests/integration/helpers/network.py\nindex e408c9beec1b..471aa2bdc2e6 100644\n--- a/tests/integration/helpers/network.py\n+++ b/tests/integration/helpers/network.py\n@@ -216,7 +216,6 @@ def __init__(\n         container_exit_timeout=60,\n         docker_api_version=os.environ.get(\"DOCKER_API_VERSION\"),\n     ):\n-\n         self.container_expire_timeout = container_expire_timeout\n         self.container_exit_timeout = container_exit_timeout\n \n@@ -232,7 +231,6 @@ def __init__(\n \n     def _ensure_container(self):\n         if self._container is None or self._container_expire_time <= time.time():\n-\n             for i in range(5):\n                 if self._container is not None:\n                     try:\ndiff --git a/tests/integration/helpers/pytest_xdist_logging_to_separate_files.py b/tests/integration/helpers/pytest_xdist_logging_to_separate_files.py\nindex d424ad58fa46..370aa23a014d 100644\n--- a/tests/integration/helpers/pytest_xdist_logging_to_separate_files.py\n+++ b/tests/integration/helpers/pytest_xdist_logging_to_separate_files.py\n@@ -1,6 +1,7 @@\n import logging\n import os.path\n \n+\n # Makes the parallel workers of pytest-xdist to log to separate files.\n # Without this function all workers will log to the same log file\n # and mix everything together making it much more difficult for troubleshooting.\ndiff --git a/tests/integration/test_backward_compatibility/test_detach_part_wrong_partition_id.py b/tests/integration/test_backward_compatibility/test_detach_part_wrong_partition_id.py\nindex 02fccfae4e54..a6f7a8653daa 100644\n--- a/tests/integration/test_backward_compatibility/test_detach_part_wrong_partition_id.py\n+++ b/tests/integration/test_backward_compatibility/test_detach_part_wrong_partition_id.py\n@@ -24,7 +24,6 @@ def start_cluster():\n \n \n def test_detach_part_wrong_partition_id(start_cluster):\n-\n     # Here we create table with partition by UUID.\n     node_21_6.query(\n         \"create table tab (id UUID, value UInt32) engine = MergeTree PARTITION BY (id) order by tuple()\"\ndiff --git a/tests/integration/test_catboost_evaluate/test.py b/tests/integration/test_catboost_evaluate/test.py\nindex a0915977ab6c..7412d34dd405 100644\n--- a/tests/integration/test_catboost_evaluate/test.py\n+++ b/tests/integration/test_catboost_evaluate/test.py\n@@ -279,7 +279,7 @@ def testAmazonModelManyRows(ch_cluster):\n     )\n \n     result = instance.query(\n-        \"insert into amazon select number % 256, number, number, number, number, number, number, number, number, number from numbers(7500)\"\n+        \"insert into amazon select number % 256, number, number, number, number, number, number, number, number, number from numbers(750000)\"\n     )\n \n     # First compute prediction, then as a very crude way to fingerprint and compare the result: sum and floor\n@@ -288,7 +288,7 @@ def testAmazonModelManyRows(ch_cluster):\n         \"SELECT floor(sum(catboostEvaluate('/etc/clickhouse-server/model/amazon_model.bin', RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME, ROLE_TITLE, ROLE_FAMILY_DESC, ROLE_FAMILY, ROLE_CODE))) FROM amazon\"\n     )\n \n-    expected = \"5834\\n\"\n+    expected = \"583092\\n\"\n     assert result == expected\n \n     result = instance.query(\"drop table if exists amazon\")\ndiff --git a/tests/integration/test_cluster_copier/test_three_nodes.py b/tests/integration/test_cluster_copier/test_three_nodes.py\nindex 31d6c0448f41..e7d07757adbd 100644\n--- a/tests/integration/test_cluster_copier/test_three_nodes.py\n+++ b/tests/integration/test_cluster_copier/test_three_nodes.py\n@@ -19,7 +19,6 @@\n def started_cluster():\n     global cluster\n     try:\n-\n         for name in [\"first\", \"second\", \"third\"]:\n             cluster.add_instance(\n                 name,\ndiff --git a/tests/integration/test_cluster_copier/test_two_nodes.py b/tests/integration/test_cluster_copier/test_two_nodes.py\nindex 10ab7d03b006..2b6fcf6cac23 100644\n--- a/tests/integration/test_cluster_copier/test_two_nodes.py\n+++ b/tests/integration/test_cluster_copier/test_two_nodes.py\n@@ -19,7 +19,6 @@\n def started_cluster():\n     global cluster\n     try:\n-\n         for name in [\"first_of_two\", \"second_of_two\"]:\n             instance = cluster.add_instance(\n                 name,\ndiff --git a/tests/integration/test_composable_protocols/test.py b/tests/integration/test_composable_protocols/test.py\nindex bc87fea52968..df74cfffa541 100644\n--- a/tests/integration/test_composable_protocols/test.py\n+++ b/tests/integration/test_composable_protocols/test.py\n@@ -63,7 +63,6 @@ def netcat(hostname, port, content):\n \n \n def test_connections():\n-\n     client = Client(server.ip_address, 9000, command=cluster.client_bin_path)\n     assert client.query(\"SELECT 1\") == \"1\\n\"\n \ndiff --git a/tests/integration/test_create_query_constraints/test.py b/tests/integration/test_create_query_constraints/test.py\nindex 8df043fd24b9..33c41b4f161f 100644\n--- a/tests/integration/test_create_query_constraints/test.py\n+++ b/tests/integration/test_create_query_constraints/test.py\n@@ -25,7 +25,6 @@ def start_cluster():\n \n \n def test_create_query_const_constraints():\n-\n     instance.query(\"CREATE USER u_const SETTINGS max_threads = 1 CONST\")\n     instance.query(\"GRANT ALL ON *.* TO u_const\")\n \n@@ -57,7 +56,6 @@ def test_create_query_const_constraints():\n \n \n def test_create_query_minmax_constraints():\n-\n     instance.query(\"CREATE USER u_minmax SETTINGS max_threads = 4 MIN 2 MAX 6\")\n     instance.query(\"GRANT ALL ON *.* TO u_minmax\")\n \ndiff --git a/tests/integration/test_dictionaries_all_layouts_separate_sources/common.py b/tests/integration/test_dictionaries_all_layouts_separate_sources/common.py\nindex b38e81b0227d..01addae25429 100644\n--- a/tests/integration/test_dictionaries_all_layouts_separate_sources/common.py\n+++ b/tests/integration/test_dictionaries_all_layouts_separate_sources/common.py\n@@ -348,7 +348,6 @@ def __init__(self, test_name):\n         self.layouts = LAYOUTS_RANGED\n \n     def execute(self, layout_name, node):\n-\n         if layout_name not in self.layout_to_dictionary:\n             raise RuntimeError(\"Source doesn't support layout: {}\".format(layout_name))\n \ndiff --git a/tests/integration/test_disks_app_func/test.py b/tests/integration/test_disks_app_func/test.py\nindex 027ef8feed0e..2428c53854e1 100644\n--- a/tests/integration/test_disks_app_func/test.py\n+++ b/tests/integration/test_disks_app_func/test.py\n@@ -7,7 +7,6 @@\n def started_cluster():\n     global cluster\n     try:\n-\n         cluster = ClickHouseCluster(__file__)\n         cluster.add_instance(\n             \"disks_app_test\", main_configs=[\"config.xml\"], with_minio=True\ndiff --git a/tests/integration/test_distributed_ddl_parallel/test.py b/tests/integration/test_distributed_ddl_parallel/test.py\nindex 6ebfe472e090..eb98dd3e230a 100644\n--- a/tests/integration/test_distributed_ddl_parallel/test.py\n+++ b/tests/integration/test_distributed_ddl_parallel/test.py\n@@ -10,6 +10,7 @@\n \n cluster = ClickHouseCluster(__file__)\n \n+\n # By default the exceptions that was throwed in threads will be ignored\n # (they will not mark the test as failed, only printed to stderr).\n #\ndiff --git a/tests/integration/test_fetch_memory_usage/test.py b/tests/integration/test_fetch_memory_usage/test.py\nindex a43711401507..7591cc0e8a90 100644\n--- a/tests/integration/test_fetch_memory_usage/test.py\n+++ b/tests/integration/test_fetch_memory_usage/test.py\n@@ -18,7 +18,6 @@ def started_cluster():\n \n \n def test_huge_column(started_cluster):\n-\n     if (\n         node.is_built_with_thread_sanitizer()\n         or node.is_built_with_memory_sanitizer()\ndiff --git a/tests/integration/test_host_regexp_multiple_ptr_records_concurrent/scripts/stress_test.py b/tests/integration/test_host_regexp_multiple_ptr_records_concurrent/scripts/stress_test.py\nindex b8bafb3d0c1f..fe69d72c1c72 100644\n--- a/tests/integration/test_host_regexp_multiple_ptr_records_concurrent/scripts/stress_test.py\n+++ b/tests/integration/test_host_regexp_multiple_ptr_records_concurrent/scripts/stress_test.py\n@@ -13,7 +13,6 @@\n \n \n def perform_request():\n-\n     buffer = BytesIO()\n     crl = pycurl.Curl()\n     crl.setopt(pycurl.INTERFACE, client_ip)\ndiff --git a/tests/integration/test_jbod_balancer/test.py b/tests/integration/test_jbod_balancer/test.py\nindex e746698611a3..df34a075d5a9 100644\n--- a/tests/integration/test_jbod_balancer/test.py\n+++ b/tests/integration/test_jbod_balancer/test.py\n@@ -45,7 +45,6 @@ def start_cluster():\n \n \n def check_balance(node, table):\n-\n     partitions = node.query(\n         \"\"\"\n         WITH\ndiff --git a/tests/integration/test_keeper_and_access_storage/test.py b/tests/integration/test_keeper_and_access_storage/test.py\nindex 6ec307f7082f..0314825b6b7f 100644\n--- a/tests/integration/test_keeper_and_access_storage/test.py\n+++ b/tests/integration/test_keeper_and_access_storage/test.py\n@@ -10,6 +10,7 @@\n     \"node1\", main_configs=[\"configs/keeper.xml\"], stay_alive=True\n )\n \n+\n # test that server is able to start\n @pytest.fixture(scope=\"module\")\n def started_cluster():\ndiff --git a/tests/integration/test_keeper_back_to_back/test.py b/tests/integration/test_keeper_back_to_back/test.py\nindex 73fface02b4c..b737ac284d29 100644\n--- a/tests/integration/test_keeper_back_to_back/test.py\n+++ b/tests/integration/test_keeper_back_to_back/test.py\n@@ -546,7 +546,6 @@ def test_random_requests(started_cluster):\n \n \n def test_end_of_session(started_cluster):\n-\n     fake_zk1 = None\n     fake_zk2 = None\n     genuine_zk1 = None\n@@ -685,6 +684,7 @@ def create_path_and_watch(i):\n             nonlocal watches_created\n             nonlocal all_paths_created\n             fake_zk.ensure_path(global_path + \"/\" + str(i))\n+\n             # new function each time\n             def dumb_watch(event):\n                 nonlocal dumb_watch_triggered_counter\ndiff --git a/tests/integration/test_keeper_persistent_log/test.py b/tests/integration/test_keeper_persistent_log/test.py\nindex 70cc14fe26df..4164ffb33d31 100644\n--- a/tests/integration/test_keeper_persistent_log/test.py\n+++ b/tests/integration/test_keeper_persistent_log/test.py\n@@ -163,7 +163,6 @@ def test_state_duplicate_restart(started_cluster):\n \n # http://zookeeper-user.578899.n2.nabble.com/Why-are-ephemeral-nodes-written-to-disk-tp7583403p7583418.html\n def test_ephemeral_after_restart(started_cluster):\n-\n     try:\n         node_zk = None\n         node_zk2 = None\ndiff --git a/tests/integration/test_keeper_zookeeper_converter/test.py b/tests/integration/test_keeper_zookeeper_converter/test.py\nindex 063421bf9228..de5a94161195 100644\n--- a/tests/integration/test_keeper_zookeeper_converter/test.py\n+++ b/tests/integration/test_keeper_zookeeper_converter/test.py\n@@ -114,7 +114,6 @@ def start_clickhouse():\n \n \n def copy_zookeeper_data(make_zk_snapshots):\n-\n     if make_zk_snapshots:  # force zookeeper to create snapshot\n         generate_zk_snapshot()\n     else:\ndiff --git a/tests/integration/test_merge_tree_load_parts/test.py b/tests/integration/test_merge_tree_load_parts/test.py\nindex 777b6f14fc6e..dfbe00c8e28c 100644\n--- a/tests/integration/test_merge_tree_load_parts/test.py\n+++ b/tests/integration/test_merge_tree_load_parts/test.py\n@@ -148,17 +148,17 @@ def get_part_name(node, partition, min_block, max_block):\n     node1.query(\"SYSTEM WAIT LOADING PARTS mt_load_parts_2\")\n \n     def check_parts_loading(node, partition, loaded, failed, skipped):\n-        for (min_block, max_block) in loaded:\n+        for min_block, max_block in loaded:\n             part_name = f\"{partition}_{min_block}_{max_block}\"\n             assert node.contains_in_log(f\"Loading Active part {part_name}\")\n             assert node.contains_in_log(f\"Finished loading Active part {part_name}\")\n \n-        for (min_block, max_block) in failed:\n+        for min_block, max_block in failed:\n             part_name = f\"{partition}_{min_block}_{max_block}\"\n             assert node.contains_in_log(f\"Loading Active part {part_name}\")\n             assert not node.contains_in_log(f\"Finished loading Active part {part_name}\")\n \n-        for (min_block, max_block) in skipped:\n+        for min_block, max_block in skipped:\n             part_name = f\"{partition}_{min_block}_{max_block}\"\n             assert not node.contains_in_log(f\"Loading Active part {part_name}\")\n             assert not node.contains_in_log(f\"Finished loading Active part {part_name}\")\ndiff --git a/tests/integration/test_merge_tree_s3_failover/s3_endpoint/endpoint.py b/tests/integration/test_merge_tree_s3_failover/s3_endpoint/endpoint.py\nindex b6567dfebc57..4613fdb850b1 100644\n--- a/tests/integration/test_merge_tree_s3_failover/s3_endpoint/endpoint.py\n+++ b/tests/integration/test_merge_tree_s3_failover/s3_endpoint/endpoint.py\n@@ -42,7 +42,6 @@ def delete(_bucket):\n \n @route(\"/<_bucket>/<_path:path>\", [\"GET\", \"POST\", \"PUT\", \"DELETE\"])\n def server(_bucket, _path):\n-\n     # It's delete query for failed part\n     if _path.endswith(\"delete\"):\n         response.set_header(\"Location\", \"http://minio1:9001/\" + _bucket + \"/\" + _path)\ndiff --git a/tests/integration/test_merge_tree_settings_constraints/test.py b/tests/integration/test_merge_tree_settings_constraints/test.py\nindex 0bb0179108dc..be6e2a318731 100644\n--- a/tests/integration/test_merge_tree_settings_constraints/test.py\n+++ b/tests/integration/test_merge_tree_settings_constraints/test.py\n@@ -20,7 +20,6 @@ def start_cluster():\n \n \n def test_merge_tree_settings_constraints():\n-\n     assert \"Setting storage_policy should not be changed\" in instance.query_and_get_error(\n         f\"CREATE TABLE wrong_table (number Int64) engine = MergeTree() ORDER BY number SETTINGS storage_policy = 'secret_policy'\"\n     )\ndiff --git a/tests/integration/test_old_parts_finally_removed/test.py b/tests/integration/test_old_parts_finally_removed/test.py\nindex 108b72c5ccd0..5347d4334198 100644\n--- a/tests/integration/test_old_parts_finally_removed/test.py\n+++ b/tests/integration/test_old_parts_finally_removed/test.py\n@@ -63,7 +63,6 @@ def test_part_finally_removed(started_cluster):\n     )\n \n     for i in range(60):\n-\n         if (\n             node1.query(\n                 \"SELECT count() from system.parts WHERE table = 'drop_outdated_part'\"\ndiff --git a/tests/integration/test_partition/test.py b/tests/integration/test_partition/test.py\nindex ae4393fc6f67..a34141c61890 100644\n--- a/tests/integration/test_partition/test.py\n+++ b/tests/integration/test_partition/test.py\n@@ -528,7 +528,9 @@ def test_make_clone_in_detached(started_cluster):\n         [\"cp\", \"-r\", path + \"all_0_0_0\", path + \"detached/broken_all_0_0_0\"]\n     )\n     assert_eq_with_retry(instance, \"select * from clone_in_detached\", \"\\n\")\n-    assert [\"broken_all_0_0_0\",] == sorted(\n+    assert [\n+        \"broken_all_0_0_0\",\n+    ] == sorted(\n         instance.exec_in_container([\"ls\", path + \"detached/\"]).strip().split(\"\\n\")\n     )\n \ndiff --git a/tests/integration/test_password_constraints/test.py b/tests/integration/test_password_constraints/test.py\nindex e3628861b286..9cdff51caa11 100644\n--- a/tests/integration/test_password_constraints/test.py\n+++ b/tests/integration/test_password_constraints/test.py\n@@ -17,7 +17,6 @@ def start_cluster():\n \n \n def test_complexity_rules(start_cluster):\n-\n     error_message = \"DB::Exception: Invalid password. The password should: be at least 12 characters long, contain at least 1 numeric character, contain at least 1 lowercase character, contain at least 1 uppercase character, contain at least 1 special character\"\n     assert error_message in node.query_and_get_error(\n         \"CREATE USER u_1 IDENTIFIED WITH plaintext_password BY ''\"\ndiff --git a/tests/integration/test_read_only_table/test.py b/tests/integration/test_read_only_table/test.py\nindex 914c6a995089..df084f9dbbd6 100644\n--- a/tests/integration/test_read_only_table/test.py\n+++ b/tests/integration/test_read_only_table/test.py\n@@ -49,7 +49,6 @@ def start_cluster():\n \n \n def test_restart_zookeeper(start_cluster):\n-\n     for table_id in range(NUM_TABLES):\n         node1.query(\n             f\"INSERT INTO test_table_{table_id} VALUES (1), (2), (3), (4), (5);\"\ndiff --git a/tests/integration/test_reload_auxiliary_zookeepers/test.py b/tests/integration/test_reload_auxiliary_zookeepers/test.py\nindex bb1455333fca..476c5dee99eb 100644\n--- a/tests/integration/test_reload_auxiliary_zookeepers/test.py\n+++ b/tests/integration/test_reload_auxiliary_zookeepers/test.py\n@@ -20,7 +20,6 @@ def start_cluster():\n \n \n def test_reload_auxiliary_zookeepers(start_cluster):\n-\n     node.query(\n         \"CREATE TABLE simple (date Date, id UInt32) ENGINE = ReplicatedMergeTree('/clickhouse/tables/0/simple', 'node') ORDER BY tuple() PARTITION BY date;\"\n     )\ndiff --git a/tests/integration/test_s3_aws_sdk_has_slightly_unreliable_behaviour/s3_endpoint/endpoint.py b/tests/integration/test_s3_aws_sdk_has_slightly_unreliable_behaviour/s3_endpoint/endpoint.py\nindex d6a732cc6817..1d33ca02f861 100644\n--- a/tests/integration/test_s3_aws_sdk_has_slightly_unreliable_behaviour/s3_endpoint/endpoint.py\n+++ b/tests/integration/test_s3_aws_sdk_has_slightly_unreliable_behaviour/s3_endpoint/endpoint.py\n@@ -1,6 +1,7 @@\n #!/usr/bin/env python3\n from bottle import request, route, run, response\n \n+\n # Handle for MultipleObjectsDelete.\n @route(\"/<_bucket>\", [\"POST\"])\n def delete(_bucket):\ndiff --git a/tests/integration/test_s3_with_proxy/test.py b/tests/integration/test_s3_with_proxy/test.py\nindex 1102d190a87e..1af040c3c302 100644\n--- a/tests/integration/test_s3_with_proxy/test.py\n+++ b/tests/integration/test_s3_with_proxy/test.py\n@@ -5,6 +5,7 @@\n import pytest\n from helpers.cluster import ClickHouseCluster\n \n+\n # Runs simple proxy resolver in python env container.\n def run_resolver(cluster):\n     container_id = cluster.get_container_id(\"resolver\")\ndiff --git a/tests/integration/test_ssl_cert_authentication/test.py b/tests/integration/test_ssl_cert_authentication/test.py\nindex 7c62ca0d8b61..b3570b6e281c 100644\n--- a/tests/integration/test_ssl_cert_authentication/test.py\n+++ b/tests/integration/test_ssl_cert_authentication/test.py\n@@ -87,7 +87,6 @@ def execute_query_https(\n \n \n def execute_query_native(node, query, user, cert_name):\n-\n     config_path = f\"{SCRIPT_DIR}/configs/client.xml\"\n \n     formatted = config.format(\ndiff --git a/tests/integration/test_storage_kafka/kafka_pb2.py b/tests/integration/test_storage_kafka/kafka_pb2.py\nindex 7de1363bbf1e..3e47af6c1e04 100644\n--- a/tests/integration/test_storage_kafka/kafka_pb2.py\n+++ b/tests/integration/test_storage_kafka/kafka_pb2.py\n@@ -21,7 +21,6 @@\n     DESCRIPTOR, \"clickhouse_path.format_schemas.kafka_pb2\", globals()\n )\n if _descriptor._USE_C_DESCRIPTORS == False:\n-\n     DESCRIPTOR._options = None\n     _KEYVALUEPAIR._serialized_start = 46\n     _KEYVALUEPAIR._serialized_end = 88\ndiff --git a/tests/integration/test_storage_kafka/message_with_repeated_pb2.py b/tests/integration/test_storage_kafka/message_with_repeated_pb2.py\nindex 4d1a23c0b43d..3715a9bea043 100644\n--- a/tests/integration/test_storage_kafka/message_with_repeated_pb2.py\n+++ b/tests/integration/test_storage_kafka/message_with_repeated_pb2.py\n@@ -21,7 +21,6 @@\n     DESCRIPTOR, \"clickhouse_path.format_schemas.message_with_repeated_pb2\", globals()\n )\n if _descriptor._USE_C_DESCRIPTORS == False:\n-\n     DESCRIPTOR._options = None\n     DESCRIPTOR._serialized_options = b\"H\\001\"\n     _MESSAGE._serialized_start = 62\ndiff --git a/tests/integration/test_storage_kafka/social_pb2.py b/tests/integration/test_storage_kafka/social_pb2.py\nindex 830ade81d335..f91a7bd0539f 100644\n--- a/tests/integration/test_storage_kafka/social_pb2.py\n+++ b/tests/integration/test_storage_kafka/social_pb2.py\n@@ -21,7 +21,6 @@\n     DESCRIPTOR, \"clickhouse_path.format_schemas.social_pb2\", globals()\n )\n if _descriptor._USE_C_DESCRIPTORS == False:\n-\n     DESCRIPTOR._options = None\n     _USER._serialized_start = 47\n     _USER._serialized_end = 90\ndiff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex 51952ac1eb7e..3a4fa6c6bfe0 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -121,7 +121,7 @@ def kafka_create_topic(\n \n def kafka_delete_topic(admin_client, topic, max_retries=50):\n     result = admin_client.delete_topics([topic])\n-    for (topic, e) in result.topic_error_codes:\n+    for topic, e in result.topic_error_codes:\n         if e == 0:\n             logging.debug(f\"Topic {topic} deleted\")\n         else:\n@@ -917,9 +917,7 @@ def describe_consumer_group(kafka_cluster, name):\n         member_info[\"client_id\"] = client_id\n         member_info[\"client_host\"] = client_host\n         member_topics_assignment = []\n-        for (topic, partitions) in MemberAssignment.decode(\n-            member_assignment\n-        ).assignment:\n+        for topic, partitions in MemberAssignment.decode(member_assignment).assignment:\n             member_topics_assignment.append({\"topic\": topic, \"partitions\": partitions})\n         member_info[\"assignment\"] = member_topics_assignment\n         res.append(member_info)\n@@ -1537,7 +1535,6 @@ def test_kafka_protobuf_no_delimiter(kafka_cluster):\n \n \n def test_kafka_materialized_view(kafka_cluster):\n-\n     instance.query(\n         \"\"\"\n         DROP TABLE IF EXISTS test.view;\n@@ -2315,7 +2312,6 @@ def test_kafka_virtual_columns2(kafka_cluster):\n \n \n def test_kafka_produce_key_timestamp(kafka_cluster):\n-\n     admin_client = KafkaAdminClient(\n         bootstrap_servers=\"localhost:{}\".format(kafka_cluster.kafka_port)\n     )\n@@ -2444,7 +2440,6 @@ def test_kafka_insert_avro(kafka_cluster):\n \n \n def test_kafka_produce_consume_avro(kafka_cluster):\n-\n     admin_client = KafkaAdminClient(\n         bootstrap_servers=\"localhost:{}\".format(kafka_cluster.kafka_port)\n     )\n@@ -4031,7 +4026,6 @@ def test_kafka_predefined_configuration(kafka_cluster):\n \n # https://github.com/ClickHouse/ClickHouse/issues/26643\n def test_issue26643(kafka_cluster):\n-\n     # for backporting:\n     # admin_client = KafkaAdminClient(bootstrap_servers=\"localhost:9092\")\n     admin_client = KafkaAdminClient(\n@@ -4313,7 +4307,6 @@ def test_row_based_formats(kafka_cluster):\n         \"RowBinaryWithNamesAndTypes\",\n         \"MsgPack\",\n     ]:\n-\n         print(format_name)\n \n         kafka_create_topic(admin_client, format_name)\n@@ -4438,7 +4431,6 @@ def test_block_based_formats_2(kafka_cluster):\n         \"ORC\",\n         \"JSONCompactColumns\",\n     ]:\n-\n         kafka_create_topic(admin_client, format_name)\n \n         instance.query(\ndiff --git a/tests/integration/test_storage_nats/nats_pb2.py b/tests/integration/test_storage_nats/nats_pb2.py\nindex 4330ff57950f..e9e5cb723633 100644\n--- a/tests/integration/test_storage_nats/nats_pb2.py\n+++ b/tests/integration/test_storage_nats/nats_pb2.py\n@@ -31,7 +31,6 @@\n _sym_db.RegisterMessage(ProtoKeyValue)\n \n if _descriptor._USE_C_DESCRIPTORS == False:\n-\n     DESCRIPTOR._options = None\n     _PROTOKEYVALUE._serialized_start = 45\n     _PROTOKEYVALUE._serialized_end = 88\ndiff --git a/tests/integration/test_storage_postgresql_replica/test.py b/tests/integration/test_storage_postgresql_replica/test.py\nindex 5df8b9029e60..8666d7ae58c0 100644\n--- a/tests/integration/test_storage_postgresql_replica/test.py\n+++ b/tests/integration/test_storage_postgresql_replica/test.py\n@@ -706,7 +706,6 @@ def test_abrupt_connection_loss_while_heavy_replication(started_cluster):\n \n \n def test_abrupt_server_restart_while_heavy_replication(started_cluster):\n-\n     # FIXME (kssenii) temporary disabled\n     if instance.is_built_with_sanitizer():\n         pytest.skip(\"Temporary disabled (FIXME)\")\ndiff --git a/tests/integration/test_storage_rabbitmq/rabbitmq_pb2.py b/tests/integration/test_storage_rabbitmq/rabbitmq_pb2.py\nindex e017b4e66c28..a5845652eef2 100644\n--- a/tests/integration/test_storage_rabbitmq/rabbitmq_pb2.py\n+++ b/tests/integration/test_storage_rabbitmq/rabbitmq_pb2.py\n@@ -21,7 +21,6 @@\n     DESCRIPTOR, \"clickhouse_path.format_schemas.rabbitmq_pb2\", globals()\n )\n if _descriptor._USE_C_DESCRIPTORS == False:\n-\n     DESCRIPTOR._options = None\n     _KEYVALUEPROTO._serialized_start = 49\n     _KEYVALUEPROTO._serialized_end = 92\ndiff --git a/tests/integration/test_storage_rabbitmq/test.py b/tests/integration/test_storage_rabbitmq/test.py\nindex 2e54f21787ae..53b6c4109efb 100644\n--- a/tests/integration/test_storage_rabbitmq/test.py\n+++ b/tests/integration/test_storage_rabbitmq/test.py\n@@ -2864,7 +2864,6 @@ def test_rabbitmq_predefined_configuration(rabbitmq_cluster):\n \n \n def test_rabbitmq_msgpack(rabbitmq_cluster):\n-\n     instance.query(\n         \"\"\"\n         drop table if exists rabbit_in;\n@@ -2908,7 +2907,6 @@ def test_rabbitmq_msgpack(rabbitmq_cluster):\n \n \n def test_rabbitmq_address(rabbitmq_cluster):\n-\n     instance2.query(\n         \"\"\"\n         drop table if exists rabbit_in;\n@@ -3243,7 +3241,6 @@ def test_block_based_formats_2(rabbitmq_cluster):\n         \"ORC\",\n         \"JSONCompactColumns\",\n     ]:\n-\n         print(format_name)\n \n         instance.query(\ndiff --git a/tests/integration/test_storage_s3/test.py b/tests/integration/test_storage_s3/test.py\nindex 8b20727a7b5a..4d493d9526b5 100644\n--- a/tests/integration/test_storage_s3/test.py\n+++ b/tests/integration/test_storage_s3/test.py\n@@ -18,6 +18,7 @@\n \n SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n \n+\n # Creates S3 bucket for tests and allows anonymous read-write access to it.\n def prepare_s3_bucket(started_cluster):\n     # Allows read-write access for bucket without authorization.\ndiff --git a/tests/integration/test_storage_s3/test_invalid_env_credentials.py b/tests/integration/test_storage_s3/test_invalid_env_credentials.py\nindex 2f5d9349904f..aa6479a2ed3a 100644\n--- a/tests/integration/test_storage_s3/test_invalid_env_credentials.py\n+++ b/tests/integration/test_storage_s3/test_invalid_env_credentials.py\n@@ -11,6 +11,7 @@\n \n SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n \n+\n # Creates S3 bucket for tests and allows anonymous read-write access to it.\n def prepare_s3_bucket(started_cluster):\n     # Allows read-write access for bucket without authorization.\ndiff --git a/tests/integration/test_system_merges/test.py b/tests/integration/test_system_merges/test.py\nindex 0a469bd7bbd6..ff303afe19e9 100644\n--- a/tests/integration/test_system_merges/test.py\n+++ b/tests/integration/test_system_merges/test.py\n@@ -171,7 +171,6 @@ def test_mutation_simple(started_cluster, replicated):\n     starting_block = 0 if replicated else 1\n \n     try:\n-\n         for node in nodes:\n             node.query(\n                 f\"create table {name} (a Int64) engine={engine} order by tuple()\"\ndiff --git a/tests/integration/test_ttl_move/test.py b/tests/integration/test_ttl_move/test.py\nindex 99978cbf6dc5..898242933201 100644\n--- a/tests/integration/test_ttl_move/test.py\n+++ b/tests/integration/test_ttl_move/test.py\n@@ -1863,7 +1863,7 @@ def test_ttl_move_if_exists(started_cluster, name, dest_type):\n                 )\n             )\n \n-        for (node, policy) in zip(\n+        for node, policy in zip(\n             [node1, node2], [\"only_jbod_1\", \"small_jbod_with_external\"]\n         ):\n             node.query(\ndiff --git a/tests/integration/test_zero_copy_fetch/test.py b/tests/integration/test_zero_copy_fetch/test.py\nindex b71752528d38..9b9aa5e0da7f 100644\n--- a/tests/integration/test_zero_copy_fetch/test.py\n+++ b/tests/integration/test_zero_copy_fetch/test.py\n@@ -16,7 +16,6 @@\n @pytest.fixture(scope=\"module\")\n def started_cluster():\n     try:\n-\n         cluster.add_instance(\n             \"node1\",\n             main_configs=[\"configs/storage_conf.xml\"],\n",
  "problem_statement": "Catboost: HTML Form Exception: Field value too long when trying to use on dataset\nWhen Try to execute function `catboostEvaluate` on dataset (approximately with more then 2150 records), its throw exception:\r\n\r\n`Code: 86. DB::HTTPException: Received error from remote server /catboost_request?version=1&method=catboost_libEvaluate. HTTP status code: 500 Poco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long,. (RECEIVED_ERROR_FROM_REMOTE_IO_SERVER) (version 22.12.3.5 (official build)) , server ClickHouseNode [uri=http://warehouse.moneyman.ru:8126/default, options={socket_timeout=3000000,session_id=DataGrip_9c45cb7e-1db0-43c5-bf64-4242ce1fbce8}]@-2019543159`\r\n\r\nIf it use with less records its work fine (ex: limit 2000).\r\n And problem not in records trying with different offset, and with my data I get error always near ~2150 records (with less work fine).\r\n\r\nSeems, like its using http protocol for communication and I should tune some params, can you please tell how to figure out this case?\r\n\r\nAlready tried increasing params like: \r\n```\r\n- http_max_field_value_size \r\n- http_max_chunk_size \r\n```\r\n\r\nbut they dont help(\n",
  "hints_text": "Any suggestions @kssenii @rschu1ze  ?\nYes, function `catboostEvaluate` communicates with the library bridge (which loads the catboost library) using HTTP (original PR: #39629).\r\n\r\nCould you please include the callstack of the exception? (it should be printed into the server log)\r\nAs far as I read the code, a large enough value for setting `http_max_field_value_size` should fix the problem, but I would like to verify.\r\n\r\nWhat would be even better (if the data is not sensitive) is if you could provide a little repro:\r\n- the query you use to invoke `catboostEvaluate` and\r\n- the table (or subset) with feature data which is send to catboost for evaluation.\r\n\r\nThanks a lot!\n> Yes, function `catboostEvaluate` communicates with the library bridge (which loads the catboost library) using HTTP (original PR: #39629).\r\n> \r\n> Could you please include the callstack of the exception? (it should be printed into the server log) As far as I read the code, a large enough value for setting `http_max_field_value_size` should fix the problem, but I would like to verify.\r\n> \r\n> What would be even better (if the data is not sensitive) is if you could provide a little repro:\r\n> \r\n> * the query you use to invoke `catboostEvaluate` and\r\n> * the table (or subset) with feature data which is send to catboost for evaluation.\r\n> \r\n> Thanks a lot!\r\n\r\n\r\n\r\n_Query:_\r\n```\r\nselect catboostEvaluate(\r\n        '/etc/clickhouse-server/models/catboost_model_uplift.cbm',\r\n        if(isNull(toFloat32(age)) or isInfinite(toFloat32(age)), nan,\r\n           toFloat32(age)),\r\n        if(isNull(toFloat32(timezone)) or isInfinite(toFloat32(timezone)), nan,\r\n           toFloat32(timezone)),\r\n        if(isNull(toFloat32(family_status)) or isInfinite(toFloat32(family_status)), nan,\r\n           toFloat32(family_status)),\r\n        if(isNull(toFloat32(dd_last_date_repaid)) or isInfinite(toFloat32(dd_last_date_repaid)), nan,\r\n           toFloat32(dd_last_date_repaid)),\r\n        if(isNull(toFloat32(dd_first_date_requested)) or\r\n           isInfinite(toFloat32(dd_first_date_requested)), nan,\r\n           toFloat32(dd_first_date_requested)),\r\n        if(isNull(toFloat32(last_credit_number)) or isInfinite(toFloat32(last_credit_number)), nan,\r\n           toFloat32(last_credit_number)),\r\n        if(isNull(toFloat32(last_sent_amount)) or isInfinite(toFloat32(last_sent_amount)), nan,\r\n           toFloat32(last_sent_amount)),\r\n        if(isNull(toFloat32(last_discount_value)) or isInfinite(toFloat32(last_discount_value)), nan,\r\n           toFloat32(last_discount_value)),\r\n        if(isNull(toFloat32(last_credit_count_days)) or isInfinite(toFloat32(last_credit_count_days)),\r\n           nan,\r\n           toFloat32(last_credit_count_days)),\r\n        if(isNull(toFloat32(last_npl)) or isInfinite(toFloat32(last_npl)), nan,\r\n           toFloat32(last_npl)),\r\n        if(isNull(toFloat32(last_term)) or isInfinite(toFloat32(last_term)), nan,\r\n           toFloat32(last_term)),\r\n        if(isNull(toFloat32(avg_dd_comp_app)) or isInfinite(toFloat32(avg_dd_comp_app)), nan,\r\n           toFloat32(avg_dd_comp_app)),\r\n        if(isNull(toFloat32(cvar_dd_comp_app)) or isInfinite(toFloat32(cvar_dd_comp_app)), nan,\r\n           toFloat32(cvar_dd_comp_app)),\r\n        if(isNull(toFloat32(avg_fact_term)) or isInfinite(toFloat32(avg_fact_term)), nan,\r\n           toFloat32(avg_fact_term)),\r\n        if(isNull(toFloat32(cvar_fact_term)) or isInfinite(toFloat32(cvar_fact_term)), nan,\r\n           toFloat32(cvar_fact_term)),\r\n        if(isNull(toFloat32(flg_login_wo_app)) or isInfinite(toFloat32(flg_login_wo_app)), nan,\r\n           toFloat32(flg_login_wo_app)),\r\n        if(isNull(toFloat32(discount_rate)) or isInfinite(toFloat32(discount_rate)), nan,\r\n           toFloat32(discount_rate)),\r\n        if(isNull(toFloat32(cnt_loan_last_3m)) or isInfinite(toFloat32(cnt_loan_last_3m)), nan,\r\n           toFloat32(cnt_loan_last_3m)),\r\n        if(isNull(toFloat32(cnt_loan_last_6m)) or isInfinite(toFloat32(cnt_loan_last_6m)), nan,\r\n           toFloat32(cnt_loan_last_6m)),\r\n        if(isNull(toFloat32(last_product_type == 'INSTALLMENT_LOAN')) or\r\n           isInfinite(toFloat32(last_product_type == 'INSTALLMENT_LOAN')), nan,\r\n           toFloat32(last_product_type == 'INSTALLMENT_LOAN')),\r\n        if(isNull(toFloat32(last_product_type == 'PAYDAY_LOAN')) or\r\n           isInfinite(toFloat32(last_product_type == 'PAYDAY_LOAN')), nan,\r\n           toFloat32(last_product_type == 'PAYDAY_LOAN'))\r\n    ) as prediction\r\nfrom risk_db.uplift_v2_data_prod\r\norder by user_account_id\r\n```\r\n\r\n_clickhouse-server.err.log:_\r\n```\r\n2023.03.16 13:51:11.958972 [ 272 ] {a52b94cc-7b10-4806-87fe-e6fe6bcfc1c6} <Error> ReadWriteBufferFromHTTP: HTTP request to `http://127.0.0.1:9012/catboost_request?version=1&method=catboost_libEvaluate` failed at try 1/1 with bytes read: 0/unknown. Error: DB::HTTPException: Received error from remote server /catboost_request?version=1&method=catboost_libEvaluate. HTTP status code: 500 Poco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):, body: 0. ? @ 0x487db98 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x348d779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4bfba98 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6d29834 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6d2b15b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x8694c3d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x86931dd in /usr/bin/clickhouse-library-bridge\r\n7. ? @ 0x7fe9f12d6609 in ?\r\n8. __clone @ 0x7fe9f11fb133 in ?\r\n (version 22.12.3.5 (official build))\r\nDate: Thu, 16 Mar 2023 13:51:11 GMT\r\nConnection: Close\r\nX-ClickHouse-Summary: {\"read_rows\":\"0\",\"read_bytes\":\"0\",\"written_rows\":\"0\",\"written_bytes\":\"0\",\"total_rows_to_read\":\"0\",\"result_rows\":\"0\",\"result_bytes\":\"0\"}\r\n\r\nPoco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ? @ 0x487db98 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x348d779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4bfba98 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6d29834 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6d2b15b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x8694c3d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x86931dd in /usr/bin/clickhouse-library-bridge\r\n7. ? @ 0x7fe9f12d6609 in ?\r\n8. __clone @ 0x7fe9f11fb133 in ?\r\n (version 22.12.3.5 (official build)). (Current backoff wait is 100/1600 ms)\r\n2023.03.16 13:51:12.059700 [ 1319 ] {a52b94cc-7b10-4806-87fe-e6fe6bcfc1c6} <Error> executeQuery: Code: 86. DB::HTTPException: Received error from remote server /catboost_request?version=1&method=catboost_libEvaluate. HTTP status code: 500 Poco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):, body: 0. ? @ 0x487db98 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x348d779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4bfba98 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6d29834 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6d2b15b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x8694c3d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x86931dd in /usr/bin/clickhouse-library-bridge\r\n7. ? @ 0x7fe9f12d6609 in ?\r\n8. __clone @ 0x7fe9f11fb133 in ?\r\n (version 22.12.3.5 (official build))\r\nDate: Thu, 16 Mar 2023 13:51:11 GMT\r\nConnection: Close\r\nX-ClickHouse-Summary: {\"read_rows\":\"0\",\"read_bytes\":\"0\",\"written_rows\":\"0\",\"written_bytes\":\"0\",\"total_rows_to_read\":\"0\",\"result_rows\":\"0\",\"result_bytes\":\"0\"}\r\n\r\nPoco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ? @ 0x487db98 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x348d779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4bfba98 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6d29834 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6d2b15b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x8694c3d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x86931dd in /usr/bin/clickhouse-library-bridge\r\n7. ? @ 0x7fe9f12d6609 in ?\r\n8. __clone @ 0x7fe9f11fb133 in ?\r\n (version 22.12.3.5 (official build)): while executing 'FUNCTION catboostEvaluate('/etc/clickhouse-server/models/catboost_model_uplift.cbm' :: 20, if(or(isNull(toFloat32(age)), isInfinite(toFloat32(age))), nan, toFloat32(age)) :: 53, if(or(isNull(toFloat32(timezone)), isInfinite(toFloat32(timezone))), nan, toFloat32(timezone)) :: 24, if(or(isNull(toFloat32(family_status)), isInfinite(toFloat32(family_status))), nan, toFloat32(family_status)) :: 0, if(or(isNull(toFloat32(dd_last_date_repaid)), isInfinite(toFloat32(dd_last_date_repaid))), nan, toFloat32(dd_last_date_repaid)) :: 1, if(or(isNull(toFloat32(dd_first_date_requested)), isInfinite(toFloat32(dd_first_date_requested))), nan, toFloat32(dd_first_date_requested)) :: 2, if(or(isNull(toFloat32(last_credit_number)), isInfinite(toFloat32(last_credit_number))), nan, toFloat32(last_credit_number)) :: 3, if(or(isNull(toFloat32(last_sent_amount)), isInfinite(toFloat32(last_sent_amount))), nan, toFloat32(last_sent_amount)) :: 4, if(or(isNull(toFloat32(last_discount_value)), isInfinite(toFloat32(last_discount_value))), nan, toFloat32(last_discount_value)) :: 5, if(or(isNull(toFloat32(last_credit_count_days)), isInfinite(toFloat32(last_credit_count_days))), nan, toFloat32(last_credit_count_days)) :: 7, if(or(isNull(toFloat32(last_npl)), isInfinite(toFloat32(last_npl))), nan, toFloat32(last_npl)) :: 8, if(or(isNull(toFloat32(last_term)), isInfinite(toFloat32(last_term))), nan, toFloat32(last_term)) :: 9, if(or(isNull(toFloat32(avg_dd_comp_app)), isInfinite(toFloat32(avg_dd_comp_app))), nan, toFloat32(avg_dd_comp_app)) :: 10, if(or(isNull(toFloat32(cvar_dd_comp_app)), isInfinite(toFloat32(cvar_dd_comp_app))), nan, toFloat32(cvar_dd_comp_app)) :: 11, if(or(isNull(toFloat32(avg_fact_term)), isInfinite(toFloat32(avg_fact_term))), nan, toFloat32(avg_fact_term)) :: 12, if(or(isNull(toFloat32(cvar_fact_term)), isInfinite(toFloat32(cvar_fact_term))), nan, toFloat32(cvar_fact_term)) :: 13, if(or(isNull(toFloat32(flg_login_wo_app)), isInfinite(toFloat32(flg_login_wo_app))), nan, toFloat32(flg_login_wo_app)) :: 14, if(or(isNull(toFloat32(discount_rate)), isInfinite(toFloat32(discount_rate))), nan, toFloat32(discount_rate)) :: 15, if(or(isNull(toFloat32(cnt_loan_last_3m)), isInfinite(toFloat32(cnt_loan_last_3m))), nan, toFloat32(cnt_loan_last_3m)) :: 16, if(or(isNull(toFloat32(cnt_loan_last_6m)), isInfinite(toFloat32(cnt_loan_last_6m))), nan, toFloat32(cnt_loan_last_6m)) :: 17, if(or(isNull(toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN'))), isInfinite(toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN')))), nan, toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN'))) :: 55, if(or(isNull(toFloat32(equals(last_product_type, 'PAYDAY_LOAN'))), isInfinite(toFloat32(equals(last_product_type, 'PAYDAY_LOAN')))), nan, toFloat32(equals(last_product_type, 'PAYDAY_LOAN'))) :: 61) -> catboostEvaluate('/etc/clickhouse-server/models/catboost_model_uplift.cbm', if(or(isNull(toFloat32(age)), isInfinite(toFloat32(age))), nan, toFloat32(age)), if(or(isNull(toFloat32(timezone)), isInfinite(toFloat32(timezone))), nan, toFloat32(timezone)), if(or(isNull(toFloat32(family_status)), isInfinite(toFloat32(family_status))), nan, toFloat32(family_status)), if(or(isNull(toFloat32(dd_last_date_repaid)), isInfinite(toFloat32(dd_last_date_repaid))), nan, toFloat32(dd_last_date_repaid)), if(or(isNull(toFloat32(dd_first_date_requested)), isInfinite(toFloat32(dd_first_date_requested))), nan, toFloat32(dd_first_date_requested)), if(or(isNull(toFloat32(last_credit_number)), isInfinite(toFloat32(last_credit_number))), nan, toFloat32(last_credit_number)), if(or(isNull(toFloat32(last_sent_amount)), isInfinite(toFloat32(last_sent_amount))), nan, toFloat32(last_sent_amount)), if(or(isNull(toFloat32(last_discount_value)), isInfinite(toFloat32(last_discount_value))), nan, toFloat32(last_discount_value)), if(or(isNull(toFloat32(last_credit_count_days)), isInfinite(toFloat32(last_credit_count_days))), nan, toFloat32(last_credit_count_days)), if(or(isNull(toFloat32(last_npl)), isInfinite(toFloat32(last_npl))), nan, toFloat32(last_npl)), if(or(isNull(toFloat32(last_term)), isInfinite(toFloat32(last_term))), nan, toFloat32(last_term)), if(or(isNull(toFloat32(avg_dd_comp_app)), isInfinite(toFloat32(avg_dd_comp_app))), nan, toFloat32(avg_dd_comp_app)), if(or(isNull(toFloat32(cvar_dd_comp_app)), isInfinite(toFloat32(cvar_dd_comp_app))), nan, toFloat32(cvar_dd_comp_app)), if(or(isNull(toFloat32(avg_fact_term)), isInfinite(toFloat32(avg_fact_term))), nan, toFloat32(avg_fact_term)), if(or(isNull(toFloat32(cvar_fact_term)), isInfinite(toFloat32(cvar_fact_term))), nan, toFloat32(cvar_fact_term)), if(or(isNull(toFloat32(flg_login_wo_app)), isInfinite(toFloat32(flg_login_wo_app))), nan, toFloat32(flg_login_wo_app)), if(or(isNull(toFloat32(discount_rate)), isInfinite(toFloat32(discount_rate))), nan, toFloat32(discount_rate)), if(or(isNull(toFloat32(cnt_loan_last_3m)), isInfinite(toFloat32(cnt_loan_last_3m))), nan, toFloat32(cnt_loan_last_3m)), if(or(isNull(toFloat32(cnt_loan_last_6m)), isInfinite(toFloat32(cnt_loan_last_6m))), nan, toFloat32(cnt_loan_last_6m)), if(or(isNull(toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN'))), isInfinite(toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN')))), nan, toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN'))), if(or(isNull(toFloat32(equals(last_product_type, 'PAYDAY_LOAN'))), isInfinite(toFloat32(equals(last_product_type, 'PAYDAY_LOAN')))), nan, toFloat32(equals(last_product_type, 'PAYDAY_LOAN')))) Nullable(Float64) : 19'. (RECEIVED_ERROR_FROM_REMOTE_IO_SERVER) (version 22.12.3.5 (official build)) (from 93.125.2.75:57187) (in query: select catboostEvaluate( '/etc/clickhouse-server/models/catboost_model_uplift.cbm', if(isNull(toFloat32(age)) or isInfinite(toFloat32(age)), nan, toFloat32(age)), if(isNull(toFloat32(timezone)) or isInfinite(toFloat32(timezone)), nan, toFloat32(timezone)), if(isNull(toFloat32(family_status)) or isInfinite(toFloat32(family_status)), nan, toFloat32(family_status)), if(isNull(toFloat32(dd_last_date_repaid)) or isInfinite(toFloat32(dd_last_date_repaid)), nan, toFloat32(dd_last_date_repaid)), if(isNull(toFloat32(dd_first_date_requested)) or isInfinite(toFloat32(dd_first_date_requested)), nan, toFloat32(dd_first_date_requested)), if(isNull(toFloat32(last_credit_number)) or isInfinite(toFloat32(last_credit_number)), nan, toFloat32(last_credit_number)), if(isNull(toFloat32(last_sent_amount)) or isInfinite(toFloat32(last_sent_amount)), nan, toFloat32(last_sent_amount)), if(isNull(toFloat32(last_discount_value)) or isInfinite(toFloat32(last_discount_value)), nan, toFloat32(last_discount_value)), if(isNull(toFloat32(last_credit_count_days)) or isInfinite(toFloat32(last_credit_count_days)), nan, toFloat32(last_credit_count_days)), if(isNull(toFloat32(last_npl)) or isInfinite(toFloat32(last_npl)), nan, toFloat32(last_npl)), if(isNull(toFloat32(last_term)) or isInfinite(toFloat32(last_term)), nan, toFloat32(last_term)), if(isNull(toFloat32(avg_dd_comp_app)) or isInfinite(toFloat32(avg_dd_comp_app)), nan, toFloat32(avg_dd_comp_app)), if(isNull(toFloat32(cvar_dd_comp_app)) or isInfinite(toFloat32(cvar_dd_comp_app)), nan, toFloat32(cvar_dd_comp_app)), if(isNull(toFloat32(avg_fact_term)) or isInfinite(toFloat32(avg_fact_term)), nan, toFloat32(avg_fact_term)), if(isNull(toFloat32(cvar_fact_term)) or isInfinite(toFloat32(cvar_fact_term)), nan, toFloat32(cvar_fact_term)), if(isNull(toFloat32(flg_login_wo_app)) or isInfinite(toFloat32(flg_login_wo_app)), nan, toFloat32(flg_login_wo_app)), if(isNull(toFloat32(discount_rate)) or isInfinite(toFloat32(discount_rate)), nan, toFloat32(discount_rate)), if(isNull(toFloat32(cnt_loan_last_3m)) or isInfinite(toFloat32(cnt_loan_last_3m)), nan, toFloat32(cnt_loan_last_3m)), if(isNull(toFloat32(cnt_loan_last_6m)) or isInfinite(toFloat32(cnt_loan_last_6m)), nan, toFloat32(cnt_loan_last_6m)), if(isNull(toFloat32(last_product_type == 'INSTALLMENT_LOAN')) or isInfinite(toFloat32(last_product_type == 'INSTALLMENT_LOAN')), nan, toFloat32(last_product_type == 'INSTALLMENT_LOAN')), if(isNull(toFloat32(last_product_type == 'PAYDAY_LOAN')) or isInfinite(toFloat32(last_product_type == 'PAYDAY_LOAN')), nan, toFloat32(last_product_type == 'PAYDAY_LOAN')) ) as prediction from risk_db.uplift_v2_data_prod order by user_account_id limit 2150), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked const&, int, bool) @ 0xe750cda in /usr/bin/clickhouse\r\n1. ? @ 0xe960404 in /usr/bin/clickhouse\r\n2. DB::assertResponseIsOk(Poco::Net::HTTPRequest const&, Poco::Net::HTTPResponse&, std::__1::basic_istream<char, std::__1::char_traits<char>>&, bool) @ 0xe9602c4 in /usr/bin/clickhouse\r\n3. ? @ 0x1113d6ed in /usr/bin/clickhouse\r\n4. ? @ 0x111375cc in /usr/bin/clickhouse\r\n5. ? @ 0x11135c81 in /usr/bin/clickhouse\r\n6. ? @ 0x11133964 in /usr/bin/clickhouse\r\n7. DB::NativeReader::read() @ 0x153baa08 in /usr/bin/clickhouse\r\n8. DB::CatBoostLibraryBridgeHelper::evaluate(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&) @ 0x152c0190 in /usr/bin/clickhouse\r\n9. ? @ 0xafcda2b in /usr/bin/clickhouse\r\n10. ? @ 0x7eb6a6e in /usr/bin/clickhouse\r\n11. DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1313abeb in /usr/bin/clickhouse\r\n12. DB::IExecutableFunction::executeWithoutSparseColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1313b5b5 in /usr/bin/clickhouse\r\n13. DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1313c7db in /usr/bin/clickhouse\r\n14. DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const @ 0x13a158e2 in /usr/bin/clickhouse\r\n15. DB::ExpressionTransform::transform(DB::Chunk&) @ 0x155d1a12 in /usr/bin/clickhouse\r\n16. ? @ 0x110f9730 in /usr/bin/clickhouse\r\n17. DB::ISimpleTransform::work() @ 0x153d009f in /usr/bin/clickhouse\r\n18. DB::ExecutionThreadContext::executeTask() @ 0x153ee2a6 in /usr/bin/clickhouse\r\n19. DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) @ 0x153e349c in /usr/bin/clickhouse\r\n20. ? @ 0x153e55bd in /usr/bin/clickhouse\r\n21. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xe809f16 in /usr/bin/clickhouse\r\n22. ? @ 0xe80f0e1 in /usr/bin/clickhouse\r\n23. ? @ 0x7ff34615c609 in ?\r\n24. clone @ 0x7ff346081133 in ?\r\n\r\n2023.03.16 13:51:12.061540 [ 1319 ] {a52b94cc-7b10-4806-87fe-e6fe6bcfc1c6} <Error> DynamicQueryHandler: Code: 86. DB::HTTPException: Received error from remote server /catboost_request?version=1&method=catboost_libEvaluate. HTTP status code: 500 Poco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):, body: 0. ? @ 0x487db98 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x348d779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4bfba98 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6d29834 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6d2b15b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x8694c3d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x86931dd in /usr/bin/clickhouse-library-bridge\r\n7. ? @ 0x7fe9f12d6609 in ?\r\n8. __clone @ 0x7fe9f11fb133 in ?\r\n (version 22.12.3.5 (official build))\r\nDate: Thu, 16 Mar 2023 13:51:11 GMT\r\nConnection: Close\r\nX-ClickHouse-Summary: {\"read_rows\":\"0\",\"read_bytes\":\"0\",\"written_rows\":\"0\",\"written_bytes\":\"0\",\"total_rows_to_read\":\"0\",\"result_rows\":\"0\",\"result_bytes\":\"0\"}\r\n\r\nPoco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ? @ 0x487db98 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x348d779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4bfba98 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6d29834 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6d2b15b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x8694c3d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x86931dd in /usr/bin/clickhouse-library-bridge\r\n7. ? @ 0x7fe9f12d6609 in ?\r\n8. __clone @ 0x7fe9f11fb133 in ?\r\n (version 22.12.3.5 (official build)): while executing 'FUNCTION catboostEvaluate('/etc/clickhouse-server/models/catboost_model_uplift.cbm' :: 20, if(or(isNull(toFloat32(age)), isInfinite(toFloat32(age))), nan, toFloat32(age)) :: 53, if(or(isNull(toFloat32(timezone)), isInfinite(toFloat32(timezone))), nan, toFloat32(timezone)) :: 24, if(or(isNull(toFloat32(family_status)), isInfinite(toFloat32(family_status))), nan, toFloat32(family_status)) :: 0, if(or(isNull(toFloat32(dd_last_date_repaid)), isInfinite(toFloat32(dd_last_date_repaid))), nan, toFloat32(dd_last_date_repaid)) :: 1, if(or(isNull(toFloat32(dd_first_date_requested)), isInfinite(toFloat32(dd_first_date_requested))), nan, toFloat32(dd_first_date_requested)) :: 2, if(or(isNull(toFloat32(last_credit_number)), isInfinite(toFloat32(last_credit_number))), nan, toFloat32(last_credit_number)) :: 3, if(or(isNull(toFloat32(last_sent_amount)), isInfinite(toFloat32(last_sent_amount))), nan, toFloat32(last_sent_amount)) :: 4, if(or(isNull(toFloat32(last_discount_value)), isInfinite(toFloat32(last_discount_value))), nan, toFloat32(last_discount_value)) :: 5, if(or(isNull(toFloat32(last_credit_count_days)), isInfinite(toFloat32(last_credit_count_days))), nan, toFloat32(last_credit_count_days)) :: 7, if(or(isNull(toFloat32(last_npl)), isInfinite(toFloat32(last_npl))), nan, toFloat32(last_npl)) :: 8, if(or(isNull(toFloat32(last_term)), isInfinite(toFloat32(last_term))), nan, toFloat32(last_term)) :: 9, if(or(isNull(toFloat32(avg_dd_comp_app)), isInfinite(toFloat32(avg_dd_comp_app))), nan, toFloat32(avg_dd_comp_app)) :: 10, if(or(isNull(toFloat32(cvar_dd_comp_app)), isInfinite(toFloat32(cvar_dd_comp_app))), nan, toFloat32(cvar_dd_comp_app)) :: 11, if(or(isNull(toFloat32(avg_fact_term)), isInfinite(toFloat32(avg_fact_term))), nan, toFloat32(avg_fact_term)) :: 12, if(or(isNull(toFloat32(cvar_fact_term)), isInfinite(toFloat32(cvar_fact_term))), nan, toFloat32(cvar_fact_term)) :: 13, if(or(isNull(toFloat32(flg_login_wo_app)), isInfinite(toFloat32(flg_login_wo_app))), nan, toFloat32(flg_login_wo_app)) :: 14, if(or(isNull(toFloat32(discount_rate)), isInfinite(toFloat32(discount_rate))), nan, toFloat32(discount_rate)) :: 15, if(or(isNull(toFloat32(cnt_loan_last_3m)), isInfinite(toFloat32(cnt_loan_last_3m))), nan, toFloat32(cnt_loan_last_3m)) :: 16, if(or(isNull(toFloat32(cnt_loan_last_6m)), isInfinite(toFloat32(cnt_loan_last_6m))), nan, toFloat32(cnt_loan_last_6m)) :: 17, if(or(isNull(toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN'))), isInfinite(toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN')))), nan, toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN'))) :: 55, if(or(isNull(toFloat32(equals(last_product_type, 'PAYDAY_LOAN'))), isInfinite(toFloat32(equals(last_product_type, 'PAYDAY_LOAN')))), nan, toFloat32(equals(last_product_type, 'PAYDAY_LOAN'))) :: 61) -> catboostEvaluate('/etc/clickhouse-server/models/catboost_model_uplift.cbm', if(or(isNull(toFloat32(age)), isInfinite(toFloat32(age))), nan, toFloat32(age)), if(or(isNull(toFloat32(timezone)), isInfinite(toFloat32(timezone))), nan, toFloat32(timezone)), if(or(isNull(toFloat32(family_status)), isInfinite(toFloat32(family_status))), nan, toFloat32(family_status)), if(or(isNull(toFloat32(dd_last_date_repaid)), isInfinite(toFloat32(dd_last_date_repaid))), nan, toFloat32(dd_last_date_repaid)), if(or(isNull(toFloat32(dd_first_date_requested)), isInfinite(toFloat32(dd_first_date_requested))), nan, toFloat32(dd_first_date_requested)), if(or(isNull(toFloat32(last_credit_number)), isInfinite(toFloat32(last_credit_number))), nan, toFloat32(last_credit_number)), if(or(isNull(toFloat32(last_sent_amount)), isInfinite(toFloat32(last_sent_amount))), nan, toFloat32(last_sent_amount)), if(or(isNull(toFloat32(last_discount_value)), isInfinite(toFloat32(last_discount_value))), nan, toFloat32(last_discount_value)), if(or(isNull(toFloat32(last_credit_count_days)), isInfinite(toFloat32(last_credit_count_days))), nan, toFloat32(last_credit_count_days)), if(or(isNull(toFloat32(last_npl)), isInfinite(toFloat32(last_npl))), nan, toFloat32(last_npl)), if(or(isNull(toFloat32(last_term)), isInfinite(toFloat32(last_term))), nan, toFloat32(last_term)), if(or(isNull(toFloat32(avg_dd_comp_app)), isInfinite(toFloat32(avg_dd_comp_app))), nan, toFloat32(avg_dd_comp_app)), if(or(isNull(toFloat32(cvar_dd_comp_app)), isInfinite(toFloat32(cvar_dd_comp_app))), nan, toFloat32(cvar_dd_comp_app)), if(or(isNull(toFloat32(avg_fact_term)), isInfinite(toFloat32(avg_fact_term))), nan, toFloat32(avg_fact_term)), if(or(isNull(toFloat32(cvar_fact_term)), isInfinite(toFloat32(cvar_fact_term))), nan, toFloat32(cvar_fact_term)), if(or(isNull(toFloat32(flg_login_wo_app)), isInfinite(toFloat32(flg_login_wo_app))), nan, toFloat32(flg_login_wo_app)), if(or(isNull(toFloat32(discount_rate)), isInfinite(toFloat32(discount_rate))), nan, toFloat32(discount_rate)), if(or(isNull(toFloat32(cnt_loan_last_3m)), isInfinite(toFloat32(cnt_loan_last_3m))), nan, toFloat32(cnt_loan_last_3m)), if(or(isNull(toFloat32(cnt_loan_last_6m)), isInfinite(toFloat32(cnt_loan_last_6m))), nan, toFloat32(cnt_loan_last_6m)), if(or(isNull(toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN'))), isInfinite(toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN')))), nan, toFloat32(equals(last_product_type, 'INSTALLMENT_LOAN'))), if(or(isNull(toFloat32(equals(last_product_type, 'PAYDAY_LOAN'))), isInfinite(toFloat32(equals(last_product_type, 'PAYDAY_LOAN')))), nan, toFloat32(equals(last_product_type, 'PAYDAY_LOAN')))) Nullable(Float64) : 19'. (RECEIVED_ERROR_FROM_REMOTE_IO_SERVER), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked const&, int, bool) @ 0xe750cda in /usr/bin/clickhouse\r\n1. ? @ 0xe960404 in /usr/bin/clickhouse\r\n2. DB::assertResponseIsOk(Poco::Net::HTTPRequest const&, Poco::Net::HTTPResponse&, std::__1::basic_istream<char, std::__1::char_traits<char>>&, bool) @ 0xe9602c4 in /usr/bin/clickhouse\r\n3. ? @ 0x1113d6ed in /usr/bin/clickhouse\r\n4. ? @ 0x111375cc in /usr/bin/clickhouse\r\n5. ? @ 0x11135c81 in /usr/bin/clickhouse\r\n6. ? @ 0x11133964 in /usr/bin/clickhouse\r\n7. DB::NativeReader::read() @ 0x153baa08 in /usr/bin/clickhouse\r\n8. DB::CatBoostLibraryBridgeHelper::evaluate(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&) @ 0x152c0190 in /usr/bin/clickhouse\r\n9. ? @ 0xafcda2b in /usr/bin/clickhouse\r\n10. ? @ 0x7eb6a6e in /usr/bin/clickhouse\r\n11. DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1313abeb in /usr/bin/clickhouse\r\n12. DB::IExecutableFunction::executeWithoutSparseColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1313b5b5 in /usr/bin/clickhouse\r\n13. DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>> const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1313c7db in /usr/bin/clickhouse\r\n14. DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const @ 0x13a158e2 in /usr/bin/clickhouse\r\n15. DB::ExpressionTransform::transform(DB::Chunk&) @ 0x155d1a12 in /usr/bin/clickhouse\r\n16. ? @ 0x110f9730 in /usr/bin/clickhouse\r\n17. DB::ISimpleTransform::work() @ 0x153d009f in /usr/bin/clickhouse\r\n18. DB::ExecutionThreadContext::executeTask() @ 0x153ee2a6 in /usr/bin/clickhouse\r\n19. DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) @ 0x153e349c in /usr/bin/clickhouse\r\n20. ? @ 0x153e55bd in /usr/bin/clickhouse\r\n21. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xe809f16 in /usr/bin/clickhouse\r\n22. ? @ 0xe80f0e1 in /usr/bin/clickhouse\r\n23. ? @ 0x7ff34615c609 in ?\r\n24. clone @ 0x7ff346081133 in ?\r\n (version 22.12.3.5 (official build))\r\n```\r\n\r\n_clickhouse-library-bridge.err.log_\r\n```\r\n2023.03.16 13:51:11.958740 [ 1314 ] {} <Error> CatBoostLibraryBridgeRequestHandler: Failed to process request. Error: Poco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ? @ 0x487db98 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x348d779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4bfba98 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6d29834 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6d2b15b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x8694c3d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x86931dd in /usr/bin/clickhouse-library-bridge\r\n7. ? @ 0x7fe9f12d6609 in ?\r\n8. __clone @ 0x7fe9f11fb133 in ?\r\n (version 22.12.3.5 (official build))\r\n ```\r\n \r\n Updating settings (via default user profile):\r\n - http_max_field_value_size \r\n - http_max_chunk_size \r\n \r\n<img width=\"1164\" alt=\"image\" src=\"https://user-images.githubusercontent.com/14272376/225639071-3f4953f9-3574-4293-8904-42d74ea08e71.png\">\r\n\r\n\r\nbut return error on the same size of records... :(\r\n\r\n unfortunatelly data is sensetive and I cant share it now :(\r\n \r\n \r\n-------------------------\r\nUpd: but dataset in this case doesnt play any role, as all my features is numeric I can reproduce it like:\r\n```\r\nselect catboostEvaluate(\r\n        '/etc/clickhouse-server/models/catboost_model_uplift.cbm',\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number,\r\n        number\r\n    ) as prediction\r\nfrom system.numbers\r\nlimit 2200\r\n``\r\n\r\nin my case with limit 2100 its work, with 2200 error as above\nI am having the same issue with clickhouse server v22.11.1.1360 and catboost v1.1.1.\r\n\r\nSimilarly to @oleg-savko I tried tweaking the values for http_max_field_value_size and http_max_chunk_size to no avail.\r\n\r\nAlso for me when I set a lower limit to the number of rows on which to run the model evaluation it work, but starts failing when I increase the range to a larger number of rows.\r\n\r\nTo reproduce the issue I am sharing a sample of the table on which I am running the analysis and the model I am applying:\r\n* Sample of 10k rows from the table: https://share.riseup.net/#yYaEK0QCL12qriAzHGIbog\r\n* Catboost model: https://share.riseup.net/#PlunQ4-yb1IhP7V91zUcQA\r\n\r\nThis is the query I am running:\r\n```\r\nSELECT report_id, input, measurement_uid,avg_answer_count,\r\nq50_answer_count,\r\nq90_answer_count,\r\nmax_answer_count,\r\nmin_answer_count,\r\ndns_answers_all_asn_count,\r\ndns_answers_ip_match_all_count,\r\ndns_answers_ip_match_tls_consistent_count,\r\ndns_answers_ip_match_tls_consistent_include_probe_count,\r\ndns_answers_ip_match_tls_inconsistent_include_probe_count,\r\ndns_answers_ip_match_domain_cc_count,\r\ndns_answers_ip_match_q50_domain_count,\r\ndns_answers_ip_match_q90_domain_count,\r\ndns_answers_ip_match_domain_max_count,\r\ndns_answers_ip_match_domain_min_count,\r\ndns_answers_ip_match_tls_consistent_domain_total_count,\r\ndns_answers_ip_match_tls_consistent_domain_max_count,\r\ndns_answers_ip_match_ctrl_count,\r\ndns_answers_asn_match_all_count,\r\ndns_answers_asn_match_tls_consistent_count,\r\ndns_answers_as_org_name_match_all_count,\r\ndns_answers_as_org_name_match_tls_consistent_count,\r\nfailure_asn_count,\r\nnxdomain_asn_count,\r\nok_asn_count,\r\nctrl_matching_failures_count,\r\nctrl_failure_count,\r\ncoalesce(exp_dns_answers_ipv6_only, 0) as exp_dns_answers_ipv6_only,\r\ncoalesce(exp_answer_contains_bogon, 0) as exp_answer_contains_bogon,\r\ncoalesce(exp_answer_contains_matching_probe_cc, 0) as exp_answer_contains_matching_probe_cc,\r\ncoalesce(exp_answer_contains_matching_probe_asn, 0) as exp_answer_contains_matching_probe_asn,\r\ncoalesce(exp_answer_contains_matching_probe_as_org_name, 0) as exp_answer_contains_matching_probe_as_org_name,\r\ncoalesce(exp_answer_contains_matching_resolver_as_cc, 0) as exp_answer_contains_matching_resolver_as_cc,\r\ncoalesce(exp_answer_contains_matching_resolver_asn, 0) as exp_answer_contains_matching_resolver_asn,\r\ncoalesce(exp_answer_contains_matching_resolver_as_org_name, 0) as exp_answer_contains_matching_resolver_as_org_name, \r\n\r\n\r\ncatboostEvaluate('/path/to/20230315-dns-analysis-model.cbm', \r\n  avg_answer_count,\r\n  q50_answer_count,\r\n  q90_answer_count,\r\n  max_answer_count,\r\n  min_answer_count,\r\n  dns_answers_all_asn_count,\r\n  dns_answers_ip_match_all_count,\r\n  dns_answers_ip_match_tls_consistent_count,\r\n  dns_answers_ip_match_tls_consistent_include_probe_count,\r\n  dns_answers_ip_match_tls_inconsistent_include_probe_count,\r\n  dns_answers_ip_match_domain_cc_count,\r\n  dns_answers_ip_match_q50_domain_count,\r\n  dns_answers_ip_match_q90_domain_count,\r\n  dns_answers_ip_match_domain_max_count,\r\n  dns_answers_ip_match_domain_min_count,\r\n  dns_answers_ip_match_tls_consistent_domain_total_count,\r\n  dns_answers_ip_match_tls_consistent_domain_max_count,\r\n  dns_answers_ip_match_ctrl_count,\r\n  dns_answers_asn_match_all_count,\r\n  dns_answers_asn_match_tls_consistent_count,\r\n  dns_answers_as_org_name_match_all_count,\r\n  dns_answers_as_org_name_match_tls_consistent_count,\r\n  failure_asn_count,\r\n  nxdomain_asn_count,\r\n  ok_asn_count,\r\n  ctrl_matching_failures_count,\r\n  ctrl_failure_count,\r\n  coalesce(exp_dns_answers_ipv6_only, 0) as exp_dns_answers_ipv6_only,\r\n  coalesce(exp_answer_contains_bogon, 0) as exp_answer_contains_bogon,\r\n  coalesce(exp_answer_contains_matching_probe_cc, 0) as exp_answer_contains_matching_probe_cc,\r\n  coalesce(exp_answer_contains_matching_probe_asn, 0) as exp_answer_contains_matching_probe_asn,\r\n  coalesce(exp_answer_contains_matching_probe_as_org_name, 0) as exp_answer_contains_matching_probe_as_org_name,\r\n  coalesce(exp_answer_contains_matching_resolver_as_cc, 0) as exp_answer_contains_matching_resolver_as_cc,\r\n  coalesce(exp_answer_contains_matching_resolver_asn, 0) as exp_answer_contains_matching_resolver_asn,\r\n  coalesce(exp_answer_contains_matching_resolver_as_org_name, 0) as exp_answer_contains_matching_resolver_as_org_name\r\n) AS prediction\r\nFROM dns_analysis_final\r\n```\r\n\r\nStacktrace from the catboost library bridge:\r\n```\r\n2023.03.17 16:28:34.706490 [ 538074 ] {} <Error> CatBoostLibraryBridgeRequestHandler: Failed to process request. Error: Poco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ? @ 0x47cee18 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x345c779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4b21798 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6c38e54 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6c3a77b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x85a361d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x85a1bbd in /usr/bin/clickhouse-library-bridge\r\n7. start_thread @ 0x8ea7 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n8. __clone @ 0xfddef in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 22.11.1.1360 (official build))\r\n2023.03.17 16:29:27.988544 [ 538074 ] {} <Error> CatBoostLibraryBridgeRequestHandler: Failed to process request. Error: Poco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ? @ 0x47cee18 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x345c779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4b21798 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6c38e54 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6c3a77b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x85a361d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x85a1bbd in /usr/bin/clickhouse-library-bridge\r\n7. start_thread @ 0x8ea7 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n8. __clone @ 0xfddef in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 22.11.1.1360 (official build))\r\n2023.03.17 16:37:00.047900 [ 538074 ] {} <Error> CatBoostLibraryBridgeRequestHandler: Failed to process request. Error: Poco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ? @ 0x47cee18 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x345c779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4b21798 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6c38e54 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6c3a77b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x85a361d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x85a1bbd in /usr/bin/clickhouse-library-bridge\r\n7. start_thread @ 0x8ea7 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n8. __clone @ 0xfddef in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 22.11.1.1360 (official build))\r\n```\r\n\r\nStacktrace from the clickhouse server logs:\r\n```\r\n2023.03.17 16:37:00.048119 [ 537567 ] {oonidata-0b9e27fc-3526-43a3-b68e-8efb5c2494ed} <Error> ReadWriteBufferFromHTTP: HTTP request to `http://127.0.0.1:9012/catboost_request?version=1&method=catboost_libEvaluate`\r\n failed at try 1/1 with bytes read: 0/unknown. Error: DB::Exception: Received error from remote server /catboost_request?version=1&method=catboost_libEvaluate. HTTP status code: 500 Poco::Exception. Code: 1000, e.\r\ncode() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):, body: 0. ? @ 0x47cee18 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x345c779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4b21798 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6c38e54 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6c3a77b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x85a361d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x85a1bbd in /usr/bin/clickhouse-library-bridge\r\n7. start_thread @ 0x8ea7 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n8. __clone @ 0xfddef in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 22.11.1.1360 (official build))\r\nDate: Fri, 17 Mar 2023 15:37:00 GMT\r\nConnection: Close\r\nX-ClickHouse-Summary: {\"read_rows\":\"0\",\"read_bytes\":\"0\",\"written_rows\":\"0\",\"written_bytes\":\"0\",\"total_rows_to_read\":\"0\",\"result_rows\":\"0\",\"result_bytes\":\"0\"}\r\n\r\n<D1>^EPoco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ? @ 0x47cee18 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x345c779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4b21798 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6c38e54 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6c3a77b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x85a361d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x85a1bbd in /usr/bin/clickhouse-library-bridge\r\n7. start_thread @ 0x8ea7 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n8. __clone @ 0xfddef in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 22.11.1.1360 (official build)). (Current backoff wait is 100/1600 ms)\r\n2023.03.17 16:37:00.148961 [ 537298 ] {oonidata-0b9e27fc-3526-43a3-b68e-8efb5c2494ed} <Error> executeQuery: Code: 86. DB::Exception: Received error from remote server /catboost_request?version=1&method=catboost_li\r\nbEvaluate. HTTP status code: 500 Poco::Exception. Code: 1000, e.code() = 0, HTML Form Exception: Field value too long, Stack trace (when copying this message, always include the lines below):, body: 0. ? @ 0x47cee\r\n18 in /usr/bin/clickhouse-library-bridge\r\n1. ? @ 0x345c779 in /usr/bin/clickhouse-library-bridge\r\n2. ? @ 0x4b21798 in /usr/bin/clickhouse-library-bridge\r\n3. ? @ 0x6c38e54 in /usr/bin/clickhouse-library-bridge\r\n4. ? @ 0x6c3a77b in /usr/bin/clickhouse-library-bridge\r\n5. ? @ 0x85a361d in /usr/bin/clickhouse-library-bridge\r\n6. ? @ 0x85a1bbd in /usr/bin/clickhouse-library-bridge\r\n7. start_thread @ 0x8ea7 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n8. __clone @ 0xfddef in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 22.11.1.1360 (official build))\r\nDate: Fri, 17 Mar 2023 15:37:00 GMT\r\nConnection: Close\r\nX-ClickHouse-Summary: {\"read_rows\":\"0\",\"read_bytes\":\"0\",\"written_rows\":\"0\",\"written_bytes\":\"0\",\"total_rows_to_read\":\"0\",\"result_rows\":\"0\",\"result_bytes\":\"0\"}\r\n```\r\n\r\nLet me know if you need any further assistance to reproduce I am happy share more information or data.\n@rschu1ze any new or suggestion?\r\n\r\nps: currently, like workaround, I emulate batches in my query using group by + arrays, and then it can be performed:\r\n```\r\nselect \r\n      arrayMap((...) -> \r\n              catboostEvaluate(\r\n                  '/etc/clickhouse-server/models/catboost_model_uplift.cbm',\r\n                  ...\r\n              ),\r\n                  groupArray(feat_1),\r\n                  ...\r\n                  groupArray(feat_n)\r\n              )                       as predictions,\r\n      group_num\r\nfrom score_dataset\r\ngroup by group_num\r\n```\n@hellais Thanks ... unfortunately the repro you provided on riseup is gone. Would you like to check why? (permission problem?). In the meantime I'll try to repro the issue by myself with some test data from CI.\nFound a repro (tests/integration/test_catboost_evaluate/test.py, `testAmazonModelManyRows()`, insert many more than 7500 rows).\nThe exception is generated on the library-bridge side and the server merely reports it. It looks like the library-bridge doesn't consider changed settings like `http_max_field_value_size`, it always uses the default setting values. Hard-coding `settings.http_max_field_value_size` in the code to `std::numeric_limits<size_t>::max()` enables catboost on bigger datasets (I tried up to 100k rows in my local experiment).\nI think the problem is somewhere in `IBridge::main()` which creates the `Context` (that holds the settings) for the library-bridge process.\r\n\r\n```cpp\r\nauto shared_context = Context::createShared();\r\nauto context = Context::createGlobal(shared_context.get());\r\ncontext->makeGlobalContext();\r\n```\r\n\r\nI suspect the code lacks some event notifier registration that updates the settings when the cfg file changes.\nAfter digging further, I understood that the library-bridge process is not supposed to read the server XML/YAML config (which makes sense if one considers that 99% of all settings contained in these files are server-specific). The library-bridge only accepts a bunch of cmd line arguments. When the server starts the library-bridge (after getting timeouts in pings), it reads its own config and passes library-bridge-specific settings to the cmd line.\r\n\r\nSo the fix will be to add a new cmd line argument for `http_max_field_value_size` (+ some related options) and pass a sufficiently large value.",
  "created_at": "2023-03-24T10:59:27Z",
  "modified_files": [
    "src/Bridge/IBridge.cpp",
    "src/Bridge/IBridge.h",
    "src/BridgeHelper/IBridgeHelper.cpp",
    "utils/changelog-simple/format-changelog.py",
    "utils/keeper-overload/keeper-overload.py"
  ],
  "modified_test_files": [
    "docker/test/performance-comparison/perf.py",
    "docker/test/performance-comparison/report.py",
    "tests/ci/clickhouse_helper.py",
    "tests/ci/docker_images_check.py",
    "tests/ci/get_previous_release_tag.py",
    "tests/ci/report.py",
    "tests/integration/helpers/cluster.py",
    "tests/integration/helpers/network.py",
    "tests/integration/helpers/pytest_xdist_logging_to_separate_files.py",
    "tests/integration/test_backward_compatibility/test_detach_part_wrong_partition_id.py",
    "tests/integration/test_catboost_evaluate/test.py",
    "tests/integration/test_cluster_copier/test_three_nodes.py",
    "tests/integration/test_cluster_copier/test_two_nodes.py",
    "tests/integration/test_composable_protocols/test.py",
    "tests/integration/test_create_query_constraints/test.py",
    "tests/integration/test_dictionaries_all_layouts_separate_sources/common.py",
    "tests/integration/test_disks_app_func/test.py",
    "tests/integration/test_distributed_ddl_parallel/test.py",
    "tests/integration/test_fetch_memory_usage/test.py",
    "tests/integration/test_host_regexp_multiple_ptr_records_concurrent/scripts/stress_test.py",
    "tests/integration/test_jbod_balancer/test.py",
    "tests/integration/test_keeper_and_access_storage/test.py",
    "tests/integration/test_keeper_back_to_back/test.py",
    "tests/integration/test_keeper_persistent_log/test.py",
    "tests/integration/test_keeper_zookeeper_converter/test.py",
    "tests/integration/test_merge_tree_load_parts/test.py",
    "tests/integration/test_merge_tree_s3_failover/s3_endpoint/endpoint.py",
    "tests/integration/test_merge_tree_settings_constraints/test.py",
    "tests/integration/test_old_parts_finally_removed/test.py",
    "tests/integration/test_partition/test.py",
    "tests/integration/test_password_constraints/test.py",
    "tests/integration/test_read_only_table/test.py",
    "tests/integration/test_reload_auxiliary_zookeepers/test.py",
    "tests/integration/test_s3_aws_sdk_has_slightly_unreliable_behaviour/s3_endpoint/endpoint.py",
    "tests/integration/test_s3_with_proxy/test.py",
    "tests/integration/test_ssl_cert_authentication/test.py",
    "tests/integration/test_storage_kafka/kafka_pb2.py",
    "tests/integration/test_storage_kafka/message_with_repeated_pb2.py",
    "tests/integration/test_storage_kafka/social_pb2.py",
    "tests/integration/test_storage_kafka/test.py",
    "tests/integration/test_storage_nats/nats_pb2.py",
    "tests/integration/test_storage_postgresql_replica/test.py",
    "tests/integration/test_storage_rabbitmq/rabbitmq_pb2.py",
    "tests/integration/test_storage_rabbitmq/test.py",
    "tests/integration/test_storage_s3/test.py",
    "tests/integration/test_storage_s3/test_invalid_env_credentials.py",
    "tests/integration/test_system_merges/test.py",
    "tests/integration/test_ttl_move/test.py",
    "tests/integration/test_zero_copy_fetch/test.py"
  ]
}