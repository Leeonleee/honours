{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 56988,
  "instance_id": "ClickHouse__ClickHouse-56988",
  "issue_numbers": [
    "50747"
  ],
  "base_commit": "e64466c67670de17ddba33f44a33f0d9d503b3bb",
  "patch": "diff --git a/docs/en/operations/backup.md b/docs/en/operations/backup.md\nindex d45885ee8162..44df05a72600 100644\n--- a/docs/en/operations/backup.md\n+++ b/docs/en/operations/backup.md\n@@ -451,3 +451,24 @@ To disallow concurrent backup/restore, you can use these settings respectively.\n \n The default value for both is true, so by default concurrent backup/restores are allowed.\n When these settings are false on a cluster, only 1 backup/restore is allowed to run on a cluster at a time.\n+\n+## Configuring BACKUP/RESTORE to use an AzureBlobStorage Endpoint\n+\n+To write backups to an AzureBlobStorage container you need the following pieces of information:\n+- AzureBlobStorage endpoint connection string / url,\n+- Container,\n+- Path,\n+- Account name (if url is specified)\n+- Account Key (if url is specified)\n+\n+The destination for a backup will be specified like this:\n+```\n+AzureBlobStorage('<connection string>/<url>', '<container>', '<path>', '<account name>', '<account key>')\n+```\n+\n+```sql\n+BACKUP TABLE data TO AzureBlobStorage('DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://azurite1:10000/devstoreaccount1/;',\n+    'test_container', 'data_backup');\n+RESTORE TABLE data AS data_restored FROM AzureBlobStorage('DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://azurite1:10000/devstoreaccount1/;',\n+    'test_container', 'data_backup');\n+```\ndiff --git a/src/Backups/BackupFactory.cpp b/src/Backups/BackupFactory.cpp\nindex 898ac7bc4907..31e87a21fc26 100644\n--- a/src/Backups/BackupFactory.cpp\n+++ b/src/Backups/BackupFactory.cpp\n@@ -33,11 +33,13 @@ void BackupFactory::registerBackupEngine(const String & engine_name, const Creat\n \n void registerBackupEnginesFileAndDisk(BackupFactory &);\n void registerBackupEngineS3(BackupFactory &);\n+void registerBackupEngineAzureBlobStorage(BackupFactory &);\n \n void registerBackupEngines(BackupFactory & factory)\n {\n     registerBackupEnginesFileAndDisk(factory);\n     registerBackupEngineS3(factory);\n+    registerBackupEngineAzureBlobStorage(factory);\n }\n \n BackupFactory::BackupFactory()\ndiff --git a/src/Backups/BackupIO_AzureBlobStorage.cpp b/src/Backups/BackupIO_AzureBlobStorage.cpp\nnew file mode 100644\nindex 000000000000..52ce20d51086\n--- /dev/null\n+++ b/src/Backups/BackupIO_AzureBlobStorage.cpp\n@@ -0,0 +1,320 @@\n+#include <Backups/BackupIO_AzureBlobStorage.h>\n+\n+#if USE_AZURE_BLOB_STORAGE\n+#include <Common/quoteString.h>\n+#include <Interpreters/threadPoolCallbackRunner.h>\n+#include <Interpreters/Context.h>\n+#include <IO/SharedThreadPools.h>\n+#include <IO/HTTPHeaderEntries.h>\n+#include <Storages/StorageAzureBlobCluster.h>\n+#include <Disks/IO/ReadBufferFromAzureBlobStorage.h>\n+#include <Disks/IO/WriteBufferFromAzureBlobStorage.h>\n+#include <IO/AzureBlobStorage/copyAzureBlobStorageFile.h>\n+#include <Disks/IDisk.h>\n+#include <Disks/DiskType.h>\n+\n+#include <Poco/Util/AbstractConfiguration.h>\n+\n+#include <filesystem>\n+\n+\n+namespace fs = std::filesystem;\n+\n+namespace DB\n+{\n+namespace ErrorCodes\n+{\n+    extern const int AZURE_BLOB_STORAGE_ERROR;\n+    extern const int LOGICAL_ERROR;\n+}\n+\n+BackupReaderAzureBlobStorage::BackupReaderAzureBlobStorage(\n+    StorageAzureBlob::Configuration configuration_,\n+    const ReadSettings & read_settings_,\n+    const WriteSettings & write_settings_,\n+    const ContextPtr & context_)\n+    : BackupReaderDefault(read_settings_, write_settings_, getLogger(\"BackupReaderAzureBlobStorage\"))\n+    , data_source_description{DataSourceType::ObjectStorage, ObjectStorageType::Azure, MetadataStorageType::None, configuration_.container, false, false}\n+    , configuration(configuration_)\n+{\n+    auto client_ptr = StorageAzureBlob::createClient(configuration, /* is_read_only */ false);\n+    object_storage = std::make_unique<AzureObjectStorage>(\"BackupReaderAzureBlobStorage\",\n+                                                          std::move(client_ptr),\n+                                                          StorageAzureBlob::createSettings(context_),\n+                                                          configuration_.container);\n+    client = object_storage->getAzureBlobStorageClient();\n+    settings = object_storage->getSettings();\n+}\n+\n+BackupReaderAzureBlobStorage::~BackupReaderAzureBlobStorage() = default;\n+\n+bool BackupReaderAzureBlobStorage::fileExists(const String & file_name)\n+{\n+    String key;\n+    if (startsWith(file_name, \".\"))\n+    {\n+        key= configuration.blob_path + file_name;\n+    }\n+    else\n+    {\n+        key = file_name;\n+    }\n+    return object_storage->exists(StoredObject(key));\n+}\n+\n+UInt64 BackupReaderAzureBlobStorage::getFileSize(const String & file_name)\n+{\n+    String key;\n+    if (startsWith(file_name, \".\"))\n+    {\n+        key= configuration.blob_path + file_name;\n+    }\n+    else\n+    {\n+        key = file_name;\n+    }\n+    ObjectMetadata object_metadata = object_storage->getObjectMetadata(key);\n+    return object_metadata.size_bytes;\n+}\n+\n+std::unique_ptr<SeekableReadBuffer> BackupReaderAzureBlobStorage::readFile(const String & file_name)\n+{\n+    String key;\n+    if (startsWith(file_name, \".\"))\n+    {\n+        key= configuration.blob_path + file_name;\n+    }\n+    else\n+    {\n+        key = file_name;\n+    }\n+    return std::make_unique<ReadBufferFromAzureBlobStorage>(\n+        client, key, read_settings, settings->max_single_read_retries,\n+        settings->max_single_download_retries);\n+}\n+\n+void BackupReaderAzureBlobStorage::copyFileToDisk(const String & path_in_backup, size_t file_size, bool encrypted_in_backup,\n+                                    DiskPtr destination_disk, const String & destination_path, WriteMode write_mode)\n+{\n+    auto destination_data_source_description = destination_disk->getDataSourceDescription();\n+    if ((destination_data_source_description.type == DataSourceType::ObjectStorage)\n+        && (destination_data_source_description.object_storage_type == ObjectStorageType::Azure)\n+        && (destination_data_source_description.is_encrypted == encrypted_in_backup))\n+    {\n+        LOG_TRACE(log, \"Copying {} from AzureBlobStorage to disk {}\", path_in_backup, destination_disk->getName());\n+        auto write_blob_function = [&](const Strings & blob_path, WriteMode mode, const std::optional<ObjectAttributes> &) -> size_t\n+        {\n+            /// Object storage always uses mode `Rewrite` because it simulates append using metadata and different files.\n+            if (blob_path.size() != 2 || mode != WriteMode::Rewrite)\n+                throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                                \"Blob writing function called with unexpected blob_path.size={} or mode={}\",\n+                                blob_path.size(), mode);\n+\n+            copyAzureBlobStorageFile(\n+                client,\n+                destination_disk->getObjectStorage()->getAzureBlobStorageClient(),\n+                configuration.container,\n+                fs::path(configuration.blob_path) / path_in_backup,\n+                0,\n+                file_size,\n+                /* dest_container */ blob_path[1],\n+                /* dest_path */ blob_path[0],\n+                settings,\n+                read_settings,\n+                threadPoolCallbackRunner<void>(getBackupsIOThreadPool().get(), \"BackupRDAzure\"),\n+                /* for_disk_azure_blob_storage= */ true);\n+\n+            return file_size;\n+        };\n+\n+        destination_disk->writeFileUsingBlobWritingFunction(destination_path, write_mode, write_blob_function);\n+        return; /// copied!\n+    }\n+\n+    /// Fallback to copy through buffers.\n+    BackupReaderDefault::copyFileToDisk(path_in_backup, file_size, encrypted_in_backup, destination_disk, destination_path, write_mode);\n+}\n+\n+\n+BackupWriterAzureBlobStorage::BackupWriterAzureBlobStorage(\n+    StorageAzureBlob::Configuration configuration_,\n+    const ReadSettings & read_settings_,\n+    const WriteSettings & write_settings_,\n+    const ContextPtr & context_)\n+    : BackupWriterDefault(read_settings_, write_settings_, getLogger(\"BackupWriterAzureBlobStorage\"))\n+    , data_source_description{DataSourceType::ObjectStorage, ObjectStorageType::Azure, MetadataStorageType::None, configuration_.container, false, false}\n+    , configuration(configuration_)\n+{\n+    auto client_ptr = StorageAzureBlob::createClient(configuration, /* is_read_only */ false);\n+    object_storage = std::make_unique<AzureObjectStorage>(\"BackupWriterAzureBlobStorage\",\n+                                                          std::move(client_ptr),\n+                                                          StorageAzureBlob::createSettings(context_),\n+                                                          configuration_.container);\n+    client = object_storage->getAzureBlobStorageClient();\n+    settings = object_storage->getSettings();\n+}\n+\n+void BackupWriterAzureBlobStorage::copyFileFromDisk(const String & path_in_backup, DiskPtr src_disk, const String & src_path,\n+                                      bool copy_encrypted, UInt64 start_pos, UInt64 length)\n+{\n+    /// Use the native copy as a more optimal way to copy a file from AzureBlobStorage to AzureBlobStorage if it's possible.\n+    auto source_data_source_description = src_disk->getDataSourceDescription();\n+    if (source_data_source_description.sameKind(data_source_description) && (source_data_source_description.is_encrypted == copy_encrypted))\n+    {\n+        /// getBlobPath() can return more than 3 elements if the file is stored as multiple objects in AzureBlobStorage container.\n+        /// In this case we can't use the native copy.\n+        if (auto blob_path = src_disk->getBlobPath(src_path); blob_path.size() == 2)\n+        {\n+            LOG_TRACE(log, \"Copying file {} from disk {} to AzureBlobStorag\", src_path, src_disk->getName());\n+            copyAzureBlobStorageFile(\n+                src_disk->getObjectStorage()->getAzureBlobStorageClient(),\n+                client,\n+                /* src_container */ blob_path[1],\n+                /* src_path */ blob_path[0],\n+                start_pos,\n+                length,\n+                configuration.container,\n+                fs::path(configuration.blob_path) / path_in_backup,\n+                settings,\n+                read_settings,\n+                threadPoolCallbackRunner<void>(getBackupsIOThreadPool().get(), \"BackupWRAzure\"));\n+            return; /// copied!\n+        }\n+    }\n+\n+    /// Fallback to copy through buffers.\n+    BackupWriterDefault::copyFileFromDisk(path_in_backup, src_disk, src_path, copy_encrypted, start_pos, length);\n+}\n+\n+void BackupWriterAzureBlobStorage::copyFile(const String & destination, const String & source, size_t size)\n+{\n+    LOG_TRACE(log, \"Copying file inside backup from {} to {} \", source, destination);\n+    copyAzureBlobStorageFile(\n+       client,\n+       client,\n+       configuration.container,\n+       fs::path(source),\n+       0,\n+       size,\n+       /* dest_container */ configuration.container,\n+       /* dest_path */ destination,\n+       settings,\n+       read_settings,\n+       threadPoolCallbackRunner<void>(getBackupsIOThreadPool().get(), \"BackupWRAzure\"),\n+       /* for_disk_azure_blob_storage= */ true);\n+}\n+\n+void BackupWriterAzureBlobStorage::copyDataToFile(const String & path_in_backup, const CreateReadBufferFunction & create_read_buffer, UInt64 start_pos, UInt64 length)\n+{\n+    copyDataToAzureBlobStorageFile(create_read_buffer, start_pos, length, client, configuration.container, path_in_backup, settings,\n+                     threadPoolCallbackRunner<void>(getBackupsIOThreadPool().get(), \"BackupWRAzure\"));\n+}\n+\n+BackupWriterAzureBlobStorage::~BackupWriterAzureBlobStorage() = default;\n+\n+bool BackupWriterAzureBlobStorage::fileExists(const String & file_name)\n+{\n+    String key;\n+    if (startsWith(file_name, \".\"))\n+    {\n+        key= configuration.blob_path + file_name;\n+    }\n+    else\n+    {\n+        key = file_name;\n+    }\n+    return object_storage->exists(StoredObject(key));\n+}\n+\n+UInt64 BackupWriterAzureBlobStorage::getFileSize(const String & file_name)\n+{\n+    String key;\n+    if (startsWith(file_name, \".\"))\n+    {\n+        key= configuration.blob_path + file_name;\n+    }\n+    else\n+    {\n+        key = file_name;\n+    }\n+    RelativePathsWithMetadata children;\n+    object_storage->listObjects(key,children,/*max_keys*/0);\n+    if (children.empty())\n+        throw Exception(ErrorCodes::AZURE_BLOB_STORAGE_ERROR, \"Object must exist\");\n+    return children[0].metadata.size_bytes;\n+}\n+\n+std::unique_ptr<ReadBuffer> BackupWriterAzureBlobStorage::readFile(const String & file_name, size_t /*expected_file_size*/)\n+{\n+    String key;\n+    if (startsWith(file_name, \".\"))\n+    {\n+        key= configuration.blob_path + file_name;\n+    }\n+    else\n+    {\n+        key = file_name;\n+    }\n+\n+    return std::make_unique<ReadBufferFromAzureBlobStorage>(\n+        client, key, read_settings, settings->max_single_read_retries,\n+        settings->max_single_download_retries);\n+}\n+\n+std::unique_ptr<WriteBuffer> BackupWriterAzureBlobStorage::writeFile(const String & file_name)\n+{\n+    String key;\n+    if (startsWith(file_name, \".\"))\n+    {\n+        key= configuration.blob_path + file_name;\n+    }\n+    else\n+    {\n+        key = file_name;\n+    }\n+    return std::make_unique<WriteBufferFromAzureBlobStorage>(\n+        client,\n+        key,\n+        settings->max_single_part_upload_size,\n+        settings->max_unexpected_write_error_retries,\n+        DBMS_DEFAULT_BUFFER_SIZE,\n+        write_settings);\n+}\n+\n+void BackupWriterAzureBlobStorage::removeFile(const String & file_name)\n+{\n+    String key;\n+    if (startsWith(file_name, \".\"))\n+    {\n+        key= configuration.blob_path + file_name;\n+    }\n+    else\n+    {\n+        key = file_name;\n+    }\n+    StoredObject object(key);\n+    object_storage->removeObjectIfExists(object);\n+}\n+\n+void BackupWriterAzureBlobStorage::removeFiles(const Strings & file_names)\n+{\n+    StoredObjects objects;\n+    for (const auto & file_name : file_names)\n+        objects.emplace_back(file_name);\n+\n+    object_storage->removeObjectsIfExist(objects);\n+\n+}\n+\n+void BackupWriterAzureBlobStorage::removeFilesBatch(const Strings & file_names)\n+{\n+    StoredObjects objects;\n+    for (const auto & file_name : file_names)\n+        objects.emplace_back(file_name);\n+\n+    object_storage->removeObjectsIfExist(objects);\n+}\n+\n+}\n+\n+#endif\ndiff --git a/src/Backups/BackupIO_AzureBlobStorage.h b/src/Backups/BackupIO_AzureBlobStorage.h\nnew file mode 100644\nindex 000000000000..95325044a62b\n--- /dev/null\n+++ b/src/Backups/BackupIO_AzureBlobStorage.h\n@@ -0,0 +1,68 @@\n+#pragma once\n+\n+#include \"config.h\"\n+\n+#if USE_AZURE_BLOB_STORAGE\n+#include <Backups/BackupIO_Default.h>\n+#include <Disks/DiskType.h>\n+#include <Storages/StorageAzureBlobCluster.h>\n+#include <Interpreters/Context_fwd.h>\n+\n+\n+namespace DB\n+{\n+\n+/// Represents a backup stored to Azure\n+class BackupReaderAzureBlobStorage : public BackupReaderDefault\n+{\n+public:\n+    BackupReaderAzureBlobStorage(StorageAzureBlob::Configuration configuration_, const ReadSettings & read_settings_, const WriteSettings & write_settings_, const ContextPtr & context_);\n+    ~BackupReaderAzureBlobStorage() override;\n+\n+    bool fileExists(const String & file_name) override;\n+    UInt64 getFileSize(const String & file_name) override;\n+    std::unique_ptr<SeekableReadBuffer> readFile(const String & file_name) override;\n+\n+    void copyFileToDisk(const String & path_in_backup, size_t file_size, bool encrypted_in_backup,\n+                        DiskPtr destination_disk, const String & destination_path, WriteMode write_mode) override;\n+\n+private:\n+    const DataSourceDescription data_source_description;\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> client;\n+    StorageAzureBlob::Configuration configuration;\n+    std::unique_ptr<AzureObjectStorage> object_storage;\n+    std::shared_ptr<const AzureObjectStorageSettings> settings;\n+};\n+\n+class BackupWriterAzureBlobStorage : public BackupWriterDefault\n+{\n+public:\n+    BackupWriterAzureBlobStorage(StorageAzureBlob::Configuration configuration_, const ReadSettings & read_settings_, const WriteSettings & write_settings_, const ContextPtr & context_);\n+    ~BackupWriterAzureBlobStorage() override;\n+\n+    bool fileExists(const String & file_name) override;\n+    UInt64 getFileSize(const String & file_name) override;\n+    std::unique_ptr<WriteBuffer> writeFile(const String & file_name) override;\n+\n+    void copyDataToFile(const String & path_in_backup, const CreateReadBufferFunction & create_read_buffer, UInt64 start_pos, UInt64 length) override;\n+    void copyFileFromDisk(const String & path_in_backup, DiskPtr src_disk, const String & src_path,\n+                          bool copy_encrypted, UInt64 start_pos, UInt64 length) override;\n+\n+    void copyFile(const String & destination, const String & source, size_t size) override;\n+\n+    void removeFile(const String & file_name) override;\n+    void removeFiles(const Strings & file_names) override;\n+\n+private:\n+    std::unique_ptr<ReadBuffer> readFile(const String & file_name, size_t expected_file_size) override;\n+    void removeFilesBatch(const Strings & file_names);\n+    const DataSourceDescription data_source_description;\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> client;\n+    StorageAzureBlob::Configuration configuration;\n+    std::unique_ptr<AzureObjectStorage> object_storage;\n+    std::shared_ptr<const AzureObjectStorageSettings> settings;\n+};\n+\n+}\n+\n+#endif\ndiff --git a/src/Backups/BackupImpl.cpp b/src/Backups/BackupImpl.cpp\nindex 0961c867cab6..8a4ed31bfd7c 100644\n--- a/src/Backups/BackupImpl.cpp\n+++ b/src/Backups/BackupImpl.cpp\n@@ -939,12 +939,12 @@ void BackupImpl::writeFile(const BackupFileInfo & info, BackupEntryPtr entry)\n     }\n     else if (src_disk && from_immutable_file)\n     {\n-        LOG_TRACE(log, \"Writing backup for file {} from {} (disk {}): data file #{}\", info.data_file_name, src_file_desc, src_disk->getName(), info.data_file_index);\n+        LOG_INFO(log, \"Writing backup for file {} from {} (disk {}): data file #{}\", info.data_file_name, src_file_desc, src_disk->getName(), info.data_file_index);\n         writer->copyFileFromDisk(info.data_file_name, src_disk, src_file_path, info.encrypted_by_disk, info.base_size, info.size - info.base_size);\n     }\n     else\n     {\n-        LOG_TRACE(log, \"Writing backup for file {} from {}: data file #{}\", info.data_file_name, src_file_desc, info.data_file_index);\n+        LOG_INFO(log, \"Writing backup for file {} from {}: data file #{}\", info.data_file_name, src_file_desc, info.data_file_index);\n         auto create_read_buffer = [entry, read_settings = writer->getReadSettings()] { return entry->getReadBuffer(read_settings); };\n         writer->copyDataToFile(info.data_file_name, create_read_buffer, info.base_size, info.size - info.base_size);\n     }\ndiff --git a/src/Backups/registerBackupEngineAzureBlobStorage.cpp b/src/Backups/registerBackupEngineAzureBlobStorage.cpp\nnew file mode 100644\nindex 000000000000..48f665693049\n--- /dev/null\n+++ b/src/Backups/registerBackupEngineAzureBlobStorage.cpp\n@@ -0,0 +1,172 @@\n+#include \"config.h\"\n+\n+#include <Backups/BackupFactory.h>\n+#include <Common/Exception.h>\n+\n+#if USE_AZURE_BLOB_STORAGE\n+#include <Backups/BackupIO_AzureBlobStorage.h>\n+#include <Storages/StorageAzureBlob.h>\n+#include <Backups/BackupImpl.h>\n+#include <IO/Archives/hasRegisteredArchiveFileExtension.h>\n+#include <Interpreters/Context.h>\n+#include <Poco/Util/AbstractConfiguration.h>\n+#include <filesystem>\n+#endif\n+\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int BAD_ARGUMENTS;\n+    extern const int SUPPORT_IS_DISABLED;\n+    extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n+}\n+\n+#if USE_AZURE_BLOB_STORAGE\n+namespace\n+{\n+    String removeFileNameFromURL(String & url)\n+    {\n+        Poco::URI url2{url};\n+        String path = url2.getPath();\n+        size_t slash_pos = path.find_last_of('/');\n+        String file_name = path.substr(slash_pos + 1);\n+        path.resize(slash_pos + 1);\n+        url2.setPath(path);\n+        url = url2.toString();\n+        return file_name;\n+    }\n+}\n+#endif\n+\n+void registerBackupEngineAzureBlobStorage(BackupFactory & factory)\n+{\n+    auto creator_fn = []([[maybe_unused]] const BackupFactory::CreateParams & params) -> std::unique_ptr<IBackup>\n+    {\n+#if USE_AZURE_BLOB_STORAGE\n+        const String & id_arg = params.backup_info.id_arg;\n+        const auto & args = params.backup_info.args;\n+\n+        StorageAzureBlob::Configuration configuration;\n+\n+        if (!id_arg.empty())\n+        {\n+            const auto & config = params.context->getConfigRef();\n+            auto config_prefix = \"named_collections.\" + id_arg;\n+\n+            if (!config.has(config_prefix))\n+                throw Exception(ErrorCodes::BAD_ARGUMENTS, \"There is no collection named `{}` in config\", id_arg);\n+\n+            if (config.has(config_prefix + \".connection_string\"))\n+            {\n+                configuration.connection_url = config.getString(config_prefix + \".connection_string\");\n+                configuration.is_connection_string = true;\n+                configuration.container = config.getString(config_prefix + \".container\");\n+            }\n+            else\n+            {\n+                configuration.connection_url = config.getString(config_prefix + \".storage_account_url\");\n+                configuration.is_connection_string = false;\n+                configuration.container =  config.getString(config_prefix + \".container\");\n+                configuration.account_name = config.getString(config_prefix + \".account_name\");\n+                configuration.account_key =  config.getString(config_prefix + \".account_key\");\n+            }\n+\n+            if (args.size() > 1)\n+                throw Exception(ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH, \"Backup AzureBlobStorage requires 1 or 2 arguments: named_collection, [filename]\");\n+\n+            if (args.size() == 1)\n+                configuration.blob_path = args[0].safeGet<String>();\n+\n+        }\n+        else\n+        {\n+            if (args.size() == 3)\n+            {\n+                configuration.connection_url = args[0].safeGet<String>();\n+                configuration.is_connection_string = true;\n+\n+                configuration.container =  args[1].safeGet<String>();\n+                configuration.blob_path = args[2].safeGet<String>();\n+            }\n+            else if (args.size() == 5)\n+            {\n+                configuration.connection_url = args[0].safeGet<String>();\n+                configuration.is_connection_string = false;\n+\n+                configuration.container =  args[1].safeGet<String>();\n+                configuration.blob_path = args[2].safeGet<String>();\n+                configuration.account_name = args[3].safeGet<String>();\n+                configuration.account_key = args[4].safeGet<String>();\n+\n+            }\n+            else\n+            {\n+                throw Exception(ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH,\n+                                    \"Backup AzureBlobStorage requires 3 or 5 arguments: connection string>/<url, container, path, [account name], [account key]\");\n+            }\n+        }\n+\n+        BackupImpl::ArchiveParams archive_params;\n+        if (hasRegisteredArchiveFileExtension(configuration.blob_path))\n+        {\n+            if (params.is_internal_backup)\n+                throw Exception(ErrorCodes::SUPPORT_IS_DISABLED, \"Using archives with backups on clusters is disabled\");\n+\n+            archive_params.archive_name = removeFileNameFromURL(configuration.blob_path);\n+            archive_params.compression_method = params.compression_method;\n+            archive_params.compression_level = params.compression_level;\n+            archive_params.password = params.password;\n+        }\n+        else\n+        {\n+            if (!params.password.empty())\n+                throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Password is not applicable, backup cannot be encrypted\");\n+        }\n+\n+\n+        if (params.open_mode == IBackup::OpenMode::READ)\n+        {\n+            auto reader = std::make_shared<BackupReaderAzureBlobStorage>(configuration,\n+                                                           params.read_settings,\n+                                                           params.write_settings,\n+                                                           params.context);\n+\n+            return std::make_unique<BackupImpl>(\n+                params.backup_info,\n+                archive_params,\n+                params.base_backup_info,\n+                reader,\n+                params.context,\n+                /* use_same_s3_credentials_for_base_backup*/ false);\n+        }\n+        else\n+        {\n+            auto writer = std::make_shared<BackupWriterAzureBlobStorage>(configuration,\n+                                                           params.read_settings,\n+                                                           params.write_settings,\n+                                                           params.context);\n+\n+            return std::make_unique<BackupImpl>(\n+                params.backup_info,\n+                archive_params,\n+                params.base_backup_info,\n+                writer,\n+                params.context,\n+                params.is_internal_backup,\n+                params.backup_coordination,\n+                params.backup_uuid,\n+                params.deduplicate_files,\n+                /* use_same_s3_credentials_for_base_backup */ false);\n+        }\n+#else\n+        throw Exception(ErrorCodes::SUPPORT_IS_DISABLED, \"AzureBlobStorage support is disabled\");\n+#endif\n+    };\n+\n+    factory.registerBackupEngine(\"AzureBlobStorage\", creator_fn);\n+}\n+\n+}\ndiff --git a/src/CMakeLists.txt b/src/CMakeLists.txt\nindex 5eae5ab6bff7..08913ed1b5a4 100644\n--- a/src/CMakeLists.txt\n+++ b/src/CMakeLists.txt\n@@ -89,6 +89,7 @@ add_headers_and_sources(clickhouse_common_io Common/SSH)\n add_headers_and_sources(clickhouse_common_io IO)\n add_headers_and_sources(clickhouse_common_io IO/Archives)\n add_headers_and_sources(clickhouse_common_io IO/S3)\n+add_headers_and_sources(clickhouse_common_io IO/AzureBlobStorage)\n list (REMOVE_ITEM clickhouse_common_io_sources Common/malloc.cpp Common/new_delete.cpp)\n \n \n@@ -141,6 +142,7 @@ endif()\n \n if (TARGET ch_contrib::azure_sdk)\n     add_headers_and_sources(dbms Disks/ObjectStorages/AzureBlobStorage)\n+    add_headers_and_sources(dbms IO/AzureBlobStorage)\n endif()\n \n if (TARGET ch_contrib::hdfs)\n@@ -496,6 +498,7 @@ if (TARGET ch_contrib::aws_s3)\n endif()\n \n if (TARGET ch_contrib::azure_sdk)\n+    target_link_libraries (clickhouse_common_io PUBLIC ch_contrib::azure_sdk)\n     dbms_target_link_libraries (PRIVATE ch_contrib::azure_sdk)\n endif()\n \ndiff --git a/src/Common/ProfileEvents.cpp b/src/Common/ProfileEvents.cpp\nindex 8782f895f3f8..912d425aae31 100644\n--- a/src/Common/ProfileEvents.cpp\n+++ b/src/Common/ProfileEvents.cpp\n@@ -375,6 +375,10 @@ The server successfully detected this situation and will download merged part fr\n     M(S3PutObject, \"Number of S3 API PutObject calls.\") \\\n     M(S3GetObject, \"Number of S3 API GetObject calls.\") \\\n     \\\n+    M(AzureUploadPart, \"Number of Azure blob storage API UploadPart calls\") \\\n+    M(DiskAzureUploadPart, \"Number of Disk Azure blob storage API UploadPart calls\") \\\n+    M(AzureCopyObject, \"Number of Azure blob storage API CopyObject calls\") \\\n+    M(DiskAzureCopyObject, \"Number of Disk Azure blob storage API CopyObject calls\") \\\n     M(AzureDeleteObjects, \"Number of Azure blob storage API DeleteObject(s) calls.\") \\\n     M(AzureListObjects, \"Number of Azure blob storage API ListObjects calls.\") \\\n     \\\ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 2b36b83edd2d..9e51034ddc6c 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -82,7 +82,8 @@ class IColumn;\n     M(UInt64, s3_upload_part_size_multiply_parts_count_threshold, 500, \"Each time this number of parts was uploaded to S3, s3_min_upload_part_size is multiplied by s3_upload_part_size_multiply_factor.\", 0) \\\n     M(UInt64, s3_max_inflight_parts_for_one_file, 20, \"The maximum number of a concurrent loaded parts in multipart upload request. 0 means unlimited. You \", 0) \\\n     M(UInt64, s3_max_single_part_upload_size, 32*1024*1024, \"The maximum size of object to upload using singlepart upload to S3.\", 0) \\\n-    M(UInt64, azure_max_single_part_upload_size, 100*1024*1024, \"The maximum size of object to upload using singlepart upload to Azure blob storage.\", 0) \\\n+    M(UInt64, azure_max_single_part_upload_size, 100*1024*1024, \"The maximum size of object to upload using singlepart upload to Azure blob storage.\", 0)                                                                             \\\n+    M(UInt64, azure_max_single_part_copy_size, 256*1024*1024, \"The maximum size of object to copy using single part copy to Azure blob storage.\", 0) \\\n     M(UInt64, s3_max_single_read_retries, 4, \"The maximum number of retries during single S3 read.\", 0) \\\n     M(UInt64, azure_max_single_read_retries, 4, \"The maximum number of retries during single Azure blob storage read.\", 0) \\\n     M(UInt64, azure_max_unexpected_write_error_retries, 4, \"The maximum number of retries in case of unexpected errors during Azure blob storage write\", 0) \\\ndiff --git a/src/Disks/ObjectStorages/AzureBlobStorage/AzureBlobStorageAuth.cpp b/src/Disks/ObjectStorages/AzureBlobStorage/AzureBlobStorageAuth.cpp\nindex e0199fde98b8..72c4abee5c92 100644\n--- a/src/Disks/ObjectStorages/AzureBlobStorage/AzureBlobStorageAuth.cpp\n+++ b/src/Disks/ObjectStorages/AzureBlobStorage/AzureBlobStorageAuth.cpp\n@@ -166,6 +166,9 @@ std::unique_ptr<AzureObjectStorageSettings> getAzureBlobStorageSettings(const Po\n         config.getInt(config_prefix + \".max_single_read_retries\", 3),\n         config.getInt(config_prefix + \".max_single_download_retries\", 3),\n         config.getInt(config_prefix + \".list_object_keys_size\", 1000),\n+        config.getUInt64(config_prefix + \".max_upload_part_size\", 5ULL * 1024 * 1024 * 1024),\n+        config.getUInt64(config_prefix + \".max_single_part_copy_size\", context->getSettings().azure_max_single_part_copy_size),\n+        config.getBool(config_prefix + \".use_native_copy\", false),\n         config.getUInt64(config_prefix + \".max_unexpected_write_error_retries\", context->getSettings().azure_max_unexpected_write_error_retries)\n     );\n }\ndiff --git a/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.cpp b/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.cpp\nindex 182d47d02db5..74389aedb646 100644\n--- a/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.cpp\n+++ b/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.cpp\n@@ -92,10 +92,12 @@ class AzureIteratorAsync final : public IObjectStorageIteratorAsync\n AzureObjectStorage::AzureObjectStorage(\n     const String & name_,\n     AzureClientPtr && client_,\n-    SettingsPtr && settings_)\n+    SettingsPtr && settings_,\n+    const String & container_)\n     : name(name_)\n     , client(std::move(client_))\n     , settings(std::move(settings_))\n+    , container(container_)\n     , log(getLogger(\"AzureObjectStorage\"))\n {\n }\n@@ -376,7 +378,8 @@ std::unique_ptr<IObjectStorage> AzureObjectStorage::cloneObjectStorage(const std\n     return std::make_unique<AzureObjectStorage>(\n         name,\n         getAzureBlobContainerClient(config, config_prefix),\n-        getAzureBlobStorageSettings(config, config_prefix, context)\n+        getAzureBlobStorageSettings(config, config_prefix, context),\n+        container\n     );\n }\n \ndiff --git a/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.h b/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.h\nindex 0d6d2d7fa47a..f16c35fb52c3 100644\n--- a/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.h\n+++ b/src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.h\n@@ -24,12 +24,18 @@ struct AzureObjectStorageSettings\n         int max_single_read_retries_,\n         int max_single_download_retries_,\n         int list_object_keys_size_,\n+        size_t max_upload_part_size_,\n+        size_t max_single_part_copy_size_,\n+        bool use_native_copy_,\n         size_t max_unexpected_write_error_retries_)\n         : max_single_part_upload_size(max_single_part_upload_size_)\n         , min_bytes_for_seek(min_bytes_for_seek_)\n         , max_single_read_retries(max_single_read_retries_)\n         , max_single_download_retries(max_single_download_retries_)\n         , list_object_keys_size(list_object_keys_size_)\n+        , max_upload_part_size(max_upload_part_size_)\n+        , max_single_part_copy_size(max_single_part_copy_size_)\n+        , use_native_copy(use_native_copy_)\n         , max_unexpected_write_error_retries (max_unexpected_write_error_retries_)\n     {\n     }\n@@ -41,6 +47,10 @@ struct AzureObjectStorageSettings\n     size_t max_single_read_retries = 3;\n     size_t max_single_download_retries = 3;\n     int list_object_keys_size = 1000;\n+    size_t min_upload_part_size = 16 * 1024 * 1024;\n+    size_t max_upload_part_size = 5ULL * 1024 * 1024 * 1024;\n+    size_t max_single_part_copy_size = 256 * 1024 * 1024;\n+    bool use_native_copy = false;\n     size_t max_unexpected_write_error_retries = 4;\n };\n \n@@ -56,7 +66,8 @@ class AzureObjectStorage : public IObjectStorage\n     AzureObjectStorage(\n         const String & name_,\n         AzureClientPtr && client_,\n-        SettingsPtr && settings_);\n+        SettingsPtr && settings_,\n+        const String & container_);\n \n     void listObjects(const std::string & path, RelativePathsWithMetadata & children, int max_keys) const override;\n \n@@ -119,7 +130,7 @@ class AzureObjectStorage : public IObjectStorage\n         const std::string & config_prefix,\n         ContextPtr context) override;\n \n-    String getObjectsNamespace() const override { return \"\"; }\n+    String getObjectsNamespace() const override { return container ; }\n \n     std::unique_ptr<IObjectStorage> cloneObjectStorage(\n         const std::string & new_namespace,\n@@ -131,11 +142,19 @@ class AzureObjectStorage : public IObjectStorage\n \n     bool isRemote() const override { return true; }\n \n+    std::shared_ptr<const AzureObjectStorageSettings> getSettings() { return settings.get(); }\n+\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> getAzureBlobStorageClient() override\n+    {\n+        return client.get();\n+    }\n+\n private:\n     const String name;\n     /// client used to access the files in the Blob Storage cloud\n     MultiVersion<Azure::Storage::Blobs::BlobContainerClient> client;\n     MultiVersion<AzureObjectStorageSettings> settings;\n+    const String container;\n \n     LoggerPtr log;\n };\ndiff --git a/src/Disks/ObjectStorages/Cached/CachedObjectStorage.h b/src/Disks/ObjectStorages/Cached/CachedObjectStorage.h\nindex 7b231b688051..437baead7be3 100644\n--- a/src/Disks/ObjectStorages/Cached/CachedObjectStorage.h\n+++ b/src/Disks/ObjectStorages/Cached/CachedObjectStorage.h\n@@ -3,6 +3,7 @@\n #include <Disks/ObjectStorages/IObjectStorage.h>\n #include <Interpreters/Cache/FileCacheKey.h>\n #include <Interpreters/Cache/FileCacheSettings.h>\n+#include \"config.h\"\n \n namespace Poco\n {\n@@ -120,6 +121,13 @@ class CachedObjectStorage final : public IObjectStorage\n \n     static bool canUseReadThroughCache(const ReadSettings & settings);\n \n+#if USE_AZURE_BLOB_STORAGE\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> getAzureBlobStorageClient() override\n+    {\n+        return object_storage->getAzureBlobStorageClient();\n+    }\n+#endif\n+\n private:\n     FileCacheKey getCacheKey(const std::string & path) const;\n \ndiff --git a/src/Disks/ObjectStorages/IObjectStorage.h b/src/Disks/ObjectStorages/IObjectStorage.h\nindex e5a393d3a594..049935ad60c4 100644\n--- a/src/Disks/ObjectStorages/IObjectStorage.h\n+++ b/src/Disks/ObjectStorages/IObjectStorage.h\n@@ -23,11 +23,22 @@\n #include <Disks/DirectoryIterator.h>\n #include <Common/ThreadPool.h>\n #include <Interpreters/threadPoolCallbackRunner.h>\n+#include <Common/Exception.h>\n+#include \"config.h\"\n \n+#if USE_AZURE_BLOB_STORAGE\n+#include <Common/MultiVersion.h>\n+#include <azure/storage/blobs.hpp>\n+#endif\n \n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int NOT_IMPLEMENTED;\n+}\n+\n class ReadBufferFromFileBase;\n class WriteBufferFromFileBase;\n \n@@ -214,6 +225,14 @@ class IObjectStorage\n \n     virtual WriteSettings patchSettings(const WriteSettings & write_settings) const;\n \n+#if USE_AZURE_BLOB_STORAGE\n+    virtual std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> getAzureBlobStorageClient()\n+    {\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"This function is only implemented for AzureBlobStorage\");\n+    }\n+#endif\n+\n+\n private:\n     mutable std::mutex throttlers_mutex;\n     ThrottlerPtr remote_read_throttler;\ndiff --git a/src/Disks/ObjectStorages/ObjectStorageFactory.cpp b/src/Disks/ObjectStorages/ObjectStorageFactory.cpp\nindex ec6f7081c85c..866373db44a5 100644\n--- a/src/Disks/ObjectStorages/ObjectStorageFactory.cpp\n+++ b/src/Disks/ObjectStorages/ObjectStorageFactory.cpp\n@@ -213,10 +213,12 @@ void registerAzureObjectStorage(ObjectStorageFactory & factory)\n         const ContextPtr & context,\n         bool /* skip_access_check */) -> ObjectStoragePtr\n     {\n+        String container_name = config.getString(config_prefix + \".container_name\", \"default-container\");\n         return std::make_unique<AzureObjectStorage>(\n             name,\n             getAzureBlobContainerClient(config, config_prefix),\n-            getAzureBlobStorageSettings(config, config_prefix, context));\n+            getAzureBlobStorageSettings(config, config_prefix, context),\n+            container_name);\n \n     });\n }\ndiff --git a/src/IO/AzureBlobStorage/copyAzureBlobStorageFile.cpp b/src/IO/AzureBlobStorage/copyAzureBlobStorageFile.cpp\nnew file mode 100644\nindex 000000000000..4714c7959278\n--- /dev/null\n+++ b/src/IO/AzureBlobStorage/copyAzureBlobStorageFile.cpp\n@@ -0,0 +1,340 @@\n+#include <IO/AzureBlobStorage/copyAzureBlobStorageFile.h>\n+\n+#if USE_AZURE_BLOB_STORAGE\n+\n+#include <Common/ProfileEvents.h>\n+#include <Common/typeid_cast.h>\n+#include <Interpreters/Context.h>\n+#include <IO/LimitSeekableReadBuffer.h>\n+#include <IO/SeekableReadBuffer.h>\n+#include <IO/StdStreamFromReadBuffer.h>\n+#include <Disks/IO/ReadBufferFromAzureBlobStorage.h>\n+#include <Disks/IO/WriteBufferFromAzureBlobStorage.h>\n+#include <Common/getRandomASCIIString.h>\n+#include <IO/SharedThreadPools.h>\n+\n+namespace ProfileEvents\n+{\n+    extern const Event AzureCopyObject;\n+    extern const Event AzureUploadPart;\n+\n+    extern const Event DiskAzureCopyObject;\n+    extern const Event DiskAzureUploadPart;\n+}\n+\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int INVALID_CONFIG_PARAMETER;\n+    extern const int AZURE_BLOB_STORAGE_ERROR;\n+}\n+\n+namespace\n+{\n+    class UploadHelper\n+    {\n+    public:\n+        UploadHelper(\n+            const CreateReadBuffer & create_read_buffer_,\n+            std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> client_,\n+            size_t offset_,\n+            size_t total_size_,\n+            const String & dest_container_for_logging_,\n+            const String & dest_blob_,\n+            std::shared_ptr<const AzureObjectStorageSettings> settings_,\n+            ThreadPoolCallbackRunner<void> schedule_,\n+            bool for_disk_azure_blob_storage_,\n+            const Poco::Logger * log_)\n+            : create_read_buffer(create_read_buffer_)\n+            , client(client_)\n+            , offset (offset_)\n+            , total_size (total_size_)\n+            , dest_container_for_logging(dest_container_for_logging_)\n+            , dest_blob(dest_blob_)\n+            , settings(settings_)\n+            , schedule(schedule_)\n+            , for_disk_azure_blob_storage(for_disk_azure_blob_storage_)\n+            , log(log_)\n+            , max_single_part_upload_size(settings_->max_single_part_upload_size)\n+        {\n+        }\n+\n+        virtual ~UploadHelper() = default;\n+\n+    protected:\n+        std::function<std::unique_ptr<SeekableReadBuffer>()> create_read_buffer;\n+        std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> client;\n+        size_t offset;\n+        size_t total_size;\n+        const String & dest_container_for_logging;\n+        const String & dest_blob;\n+        std::shared_ptr<const AzureObjectStorageSettings> settings;\n+        ThreadPoolCallbackRunner<void> schedule;\n+        bool for_disk_azure_blob_storage;\n+        const Poco::Logger * log;\n+        size_t max_single_part_upload_size;\n+\n+        struct UploadPartTask\n+        {\n+            size_t part_offset;\n+            size_t part_size;\n+            std::vector<std::string> block_ids;\n+            bool is_finished = false;\n+            std::exception_ptr exception;\n+        };\n+\n+        size_t normal_part_size;\n+        std::vector<std::string> block_ids;\n+\n+        std::list<UploadPartTask> TSA_GUARDED_BY(bg_tasks_mutex) bg_tasks;\n+        int num_added_bg_tasks TSA_GUARDED_BY(bg_tasks_mutex) = 0;\n+        int num_finished_bg_tasks TSA_GUARDED_BY(bg_tasks_mutex) = 0;\n+        std::mutex bg_tasks_mutex;\n+        std::condition_variable bg_tasks_condvar;\n+\n+        void calculatePartSize()\n+        {\n+            auto max_upload_part_size = settings->max_upload_part_size;\n+            if (!max_upload_part_size)\n+                throw Exception(ErrorCodes::INVALID_CONFIG_PARAMETER, \"max_upload_part_size must not be 0\");\n+            /// We've calculated the size of a normal part (the final part can be smaller).\n+            normal_part_size = max_upload_part_size;\n+        }\n+\n+    public:\n+        void performCopy()\n+        {\n+            performMultipartUpload();\n+        }\n+\n+        void completeMultipartUpload()\n+        {\n+            auto block_blob_client = client->GetBlockBlobClient(dest_blob);\n+            block_blob_client.CommitBlockList(block_ids);\n+        }\n+\n+        void performMultipartUpload()\n+        {\n+            calculatePartSize();\n+\n+            size_t position = offset;\n+            size_t end_position = offset + total_size;\n+\n+            try\n+            {\n+                while (position < end_position)\n+                {\n+                    size_t next_position = std::min(position + normal_part_size, end_position);\n+                    size_t part_size = next_position - position; /// `part_size` is either `normal_part_size` or smaller if it's the final part.\n+\n+                    uploadPart(position, part_size);\n+\n+                    position = next_position;\n+                }\n+            }\n+            catch (...)\n+            {\n+                tryLogCurrentException(__PRETTY_FUNCTION__);\n+                waitForAllBackgroundTasks();\n+                throw;\n+            }\n+\n+            waitForAllBackgroundTasks();\n+            completeMultipartUpload();\n+        }\n+\n+\n+        void uploadPart(size_t part_offset, size_t part_size)\n+        {\n+            LOG_TRACE(log, \"Writing part. Container: {}, Blob: {}, Size: {}\", dest_container_for_logging, dest_blob, part_size);\n+\n+            if (!part_size)\n+            {\n+                LOG_TRACE(log, \"Skipping writing an empty part.\");\n+                return;\n+            }\n+\n+            if (schedule)\n+            {\n+                UploadPartTask *  task = nullptr;\n+\n+                {\n+                    std::lock_guard lock(bg_tasks_mutex);\n+                    task = &bg_tasks.emplace_back();\n+                    ++num_added_bg_tasks;\n+                }\n+\n+                /// Notify waiting thread when task finished\n+                auto task_finish_notify = [this, task]()\n+                {\n+                    std::lock_guard lock(bg_tasks_mutex);\n+                    task->is_finished = true;\n+                    ++num_finished_bg_tasks;\n+\n+                    /// Notification under mutex is important here.\n+                    /// Otherwise, WriteBuffer could be destroyed in between\n+                    /// Releasing lock and condvar notification.\n+                    bg_tasks_condvar.notify_one();\n+                };\n+\n+                try\n+                {\n+                    task->part_offset = part_offset;\n+                    task->part_size = part_size;\n+\n+                    schedule([this, task, task_finish_notify]()\n+                    {\n+                        try\n+                        {\n+                            processUploadPartRequest(*task);\n+                        }\n+                        catch (...)\n+                        {\n+                            task->exception = std::current_exception();\n+                        }\n+                        task_finish_notify();\n+                    }, Priority{});\n+                }\n+                catch (...)\n+                {\n+                    task_finish_notify();\n+                    throw;\n+                }\n+            }\n+            else\n+            {\n+                UploadPartTask task;\n+                task.part_offset = part_offset;\n+                task.part_size = part_size;\n+                processUploadPartRequest(task);\n+                block_ids.insert(block_ids.end(),task.block_ids.begin(), task.block_ids.end());\n+            }\n+        }\n+\n+        void processUploadPartRequest(UploadPartTask & task)\n+        {\n+            ProfileEvents::increment(ProfileEvents::AzureUploadPart);\n+            if (for_disk_azure_blob_storage)\n+                ProfileEvents::increment(ProfileEvents::DiskAzureUploadPart);\n+\n+            auto block_blob_client = client->GetBlockBlobClient(dest_blob);\n+            auto read_buffer = std::make_unique<LimitSeekableReadBuffer>(create_read_buffer(), task.part_offset, task.part_size);\n+            while (!read_buffer->eof())\n+            {\n+                  auto size = read_buffer->available();\n+                  if (size > 0)\n+                  {\n+                      auto block_id = getRandomASCIIString(64);\n+                      Azure::Core::IO::MemoryBodyStream memory(reinterpret_cast<const uint8_t *>(read_buffer->position()), size);\n+                      block_blob_client.StageBlock(block_id, memory);\n+                      task.block_ids.emplace_back(block_id);\n+                      read_buffer->ignore(size);\n+                      LOG_TRACE(log, \"Writing part. Container: {}, Blob: {}, block_id: {}\", dest_container_for_logging, dest_blob, block_id);\n+                  }\n+            }\n+            std::lock_guard lock(bg_tasks_mutex); /// Protect bg_tasks from race\n+            LOG_TRACE(log, \"Writing part finished. Container: {}, Blob: {}, Parts: {}\", dest_container_for_logging, dest_blob, bg_tasks.size());\n+        }\n+\n+\n+        void waitForAllBackgroundTasks()\n+        {\n+            if (!schedule)\n+                return;\n+\n+            std::unique_lock lock(bg_tasks_mutex);\n+            /// Suppress warnings because bg_tasks_mutex is actually hold, but tsa annotations do not understand std::unique_lock\n+            bg_tasks_condvar.wait(lock, [this]() {return TSA_SUPPRESS_WARNING_FOR_READ(num_added_bg_tasks) == TSA_SUPPRESS_WARNING_FOR_READ(num_finished_bg_tasks); });\n+\n+            auto & tasks = TSA_SUPPRESS_WARNING_FOR_WRITE(bg_tasks);\n+            for (auto & task : tasks)\n+            {\n+                if (task.exception)\n+                    std::rethrow_exception(task.exception);\n+                block_ids.insert(block_ids.end(),task.block_ids.begin(), task.block_ids.end());\n+            }\n+        }\n+    };\n+}\n+\n+\n+void copyDataToAzureBlobStorageFile(\n+    const std::function<std::unique_ptr<SeekableReadBuffer>()> & create_read_buffer,\n+    size_t offset,\n+    size_t size,\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> dest_client,\n+    const String & dest_container_for_logging,\n+    const String & dest_blob,\n+    std::shared_ptr<const AzureObjectStorageSettings> settings,\n+    ThreadPoolCallbackRunner<void> schedule,\n+    bool for_disk_azure_blob_storage)\n+{\n+    UploadHelper helper{create_read_buffer, dest_client, offset, size, dest_container_for_logging, dest_blob, settings, schedule, for_disk_azure_blob_storage, &Poco::Logger::get(\"copyDataToAzureBlobStorageFile\")};\n+    helper.performCopy();\n+}\n+\n+\n+void copyAzureBlobStorageFile(\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> src_client,\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> dest_client,\n+    const String & src_container_for_logging,\n+    const String & src_blob,\n+    size_t offset,\n+    size_t size,\n+    const String & dest_container_for_logging,\n+    const String & dest_blob,\n+    std::shared_ptr<const AzureObjectStorageSettings> settings,\n+    const ReadSettings & read_settings,\n+    ThreadPoolCallbackRunner<void> schedule,\n+    bool for_disk_azure_blob_storage)\n+{\n+\n+    if (settings->use_native_copy)\n+    {\n+        ProfileEvents::increment(ProfileEvents::AzureCopyObject);\n+        if (for_disk_azure_blob_storage)\n+            ProfileEvents::increment(ProfileEvents::DiskAzureCopyObject);\n+\n+        auto block_blob_client_src = src_client->GetBlockBlobClient(src_blob);\n+        auto block_blob_client_dest = dest_client->GetBlockBlobClient(dest_blob);\n+        auto source_uri = block_blob_client_src.GetUrl();\n+\n+        if (size < settings->max_single_part_copy_size)\n+        {\n+            block_blob_client_dest.CopyFromUri(source_uri);\n+        }\n+        else\n+        {\n+            Azure::Storage::Blobs::StartBlobCopyOperation operation = block_blob_client_dest.StartCopyFromUri(source_uri);\n+\n+            // Wait for the operation to finish, checking for status every 100 second.\n+            auto copy_response = operation.PollUntilDone(std::chrono::milliseconds(100));\n+            auto properties_model = copy_response.Value;\n+\n+            if (properties_model.CopySource.HasValue())\n+            {\n+                throw Exception(ErrorCodes::AZURE_BLOB_STORAGE_ERROR, \"Copy failed\");\n+            }\n+\n+        }\n+    }\n+    else\n+    {\n+        LOG_TRACE(&Poco::Logger::get(\"copyAzureBlobStorageFile\"), \"Reading from Container: {}, Blob: {}\", src_container_for_logging, src_blob);\n+        auto create_read_buffer = [&]\n+        {\n+            return std::make_unique<ReadBufferFromAzureBlobStorage>(src_client, src_blob, read_settings, settings->max_single_read_retries,\n+            settings->max_single_download_retries);\n+        };\n+\n+        UploadHelper helper{create_read_buffer, dest_client, offset, size, dest_container_for_logging, dest_blob, settings, schedule, for_disk_azure_blob_storage, &Poco::Logger::get(\"copyAzureBlobStorageFile\")};\n+        helper.performCopy();\n+    }\n+}\n+\n+}\n+\n+#endif\ndiff --git a/src/IO/AzureBlobStorage/copyAzureBlobStorageFile.h b/src/IO/AzureBlobStorage/copyAzureBlobStorageFile.h\nnew file mode 100644\nindex 000000000000..83814f42693f\n--- /dev/null\n+++ b/src/IO/AzureBlobStorage/copyAzureBlobStorageFile.h\n@@ -0,0 +1,56 @@\n+#pragma once\n+\n+#include \"config.h\"\n+\n+#if USE_AZURE_BLOB_STORAGE\n+\n+#include <Storages/StorageAzureBlobCluster.h>\n+#include <Storages/StorageAzureBlob.h>\n+#include <Interpreters/threadPoolCallbackRunner.h>\n+#include <base/types.h>\n+#include <functional>\n+#include <memory>\n+\n+\n+namespace DB\n+{\n+class SeekableReadBuffer;\n+\n+using CreateReadBuffer = std::function<std::unique_ptr<SeekableReadBuffer>()>;\n+\n+/// Copies a file from AzureBlobStorage to AzureBlobStorage.\n+/// The parameters `src_offset` and `src_size` specify a part in the source to copy.\n+void copyAzureBlobStorageFile(\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> src_client,\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> dest_client,\n+    const String & src_container_for_logging,\n+    const String & src_blob,\n+    size_t src_offset,\n+    size_t src_size,\n+    const String & dest_container_for_logging,\n+    const String & dest_blob,\n+    std::shared_ptr<const AzureObjectStorageSettings> settings,\n+    const ReadSettings & read_settings,\n+    ThreadPoolCallbackRunner<void> schedule_ = {},\n+    bool for_disk_azure_blob_storage = false);\n+\n+\n+/// Copies data from any seekable source to AzureBlobStorage.\n+/// The same functionality can be done by using the function copyData() and the class WriteBufferFromS3\n+/// however copyDataToS3File() is faster and spends less memory.\n+/// The callback `create_read_buffer` can be called from multiple threads in parallel, so that should be thread-safe.\n+/// The parameters `offset` and `size` specify a part in the source to copy.\n+void copyDataToAzureBlobStorageFile(\n+    const std::function<std::unique_ptr<SeekableReadBuffer>()> & create_read_buffer,\n+    size_t offset,\n+    size_t size,\n+    std::shared_ptr<const Azure::Storage::Blobs::BlobContainerClient> client,\n+    const String & dest_container_for_logging,\n+    const String & dest_blob,\n+    std::shared_ptr<const AzureObjectStorageSettings> settings,\n+    ThreadPoolCallbackRunner<void> schedule_ = {},\n+    bool for_disk_azure_blob_storage = false);\n+\n+}\n+\n+#endif\ndiff --git a/src/Storages/StorageAzureBlob.cpp b/src/Storages/StorageAzureBlob.cpp\nindex 01c31eab2b1e..c09db0bfb7b2 100644\n--- a/src/Storages/StorageAzureBlob.cpp\n+++ b/src/Storages/StorageAzureBlob.cpp\n@@ -297,7 +297,7 @@ void registerStorageAzureBlob(StorageFactory & factory)\n \n         return std::make_shared<StorageAzureBlob>(\n             std::move(configuration),\n-            std::make_unique<AzureObjectStorage>(\"AzureBlobStorage\", std::move(client), std::move(settings)),\n+            std::make_unique<AzureObjectStorage>(\"AzureBlobStorage\", std::move(client), std::move(settings),configuration.container),\n             args.getContext(),\n             args.table_id,\n             args.columns,\ndiff --git a/src/TableFunctions/TableFunctionAzureBlobStorage.cpp b/src/TableFunctions/TableFunctionAzureBlobStorage.cpp\nindex d394c8363695..b098cac5144a 100644\n--- a/src/TableFunctions/TableFunctionAzureBlobStorage.cpp\n+++ b/src/TableFunctions/TableFunctionAzureBlobStorage.cpp\n@@ -262,7 +262,7 @@ ColumnsDescription TableFunctionAzureBlobStorage::getActualTableStructure(Contex\n         auto client = StorageAzureBlob::createClient(configuration, !is_insert_query);\n         auto settings = StorageAzureBlob::createSettings(context);\n \n-        auto object_storage = std::make_unique<AzureObjectStorage>(\"AzureBlobStorageTableFunction\", std::move(client), std::move(settings));\n+        auto object_storage = std::make_unique<AzureObjectStorage>(\"AzureBlobStorageTableFunction\", std::move(client), std::move(settings), configuration.container);\n         return StorageAzureBlob::getTableStructureFromData(object_storage.get(), configuration, std::nullopt, context, false);\n     }\n \n@@ -293,7 +293,7 @@ StoragePtr TableFunctionAzureBlobStorage::executeImpl(const ASTPtr & /*ast_funct\n \n     StoragePtr storage = std::make_shared<StorageAzureBlob>(\n         configuration,\n-        std::make_unique<AzureObjectStorage>(table_name, std::move(client), std::move(settings)),\n+        std::make_unique<AzureObjectStorage>(table_name, std::move(client), std::move(settings), configuration.container),\n         context,\n         StorageID(getDatabaseName(), table_name),\n         columns,\ndiff --git a/src/TableFunctions/TableFunctionAzureBlobStorageCluster.cpp b/src/TableFunctions/TableFunctionAzureBlobStorageCluster.cpp\nindex eee585967c2f..1c3b302a186e 100644\n--- a/src/TableFunctions/TableFunctionAzureBlobStorageCluster.cpp\n+++ b/src/TableFunctions/TableFunctionAzureBlobStorageCluster.cpp\n@@ -40,7 +40,7 @@ StoragePtr TableFunctionAzureBlobStorageCluster::executeImpl(\n         /// On worker node this filename won't contains globs\n         storage = std::make_shared<StorageAzureBlob>(\n             configuration,\n-            std::make_unique<AzureObjectStorage>(table_name, std::move(client), std::move(settings)),\n+            std::make_unique<AzureObjectStorage>(table_name, std::move(client), std::move(settings), configuration.container),\n             context,\n             StorageID(getDatabaseName(), table_name),\n             columns,\n@@ -55,7 +55,7 @@ StoragePtr TableFunctionAzureBlobStorageCluster::executeImpl(\n         storage = std::make_shared<StorageAzureBlobCluster>(\n             cluster_name,\n             configuration,\n-            std::make_unique<AzureObjectStorage>(table_name, std::move(client), std::move(settings)),\n+            std::make_unique<AzureObjectStorage>(table_name, std::move(client), std::move(settings), configuration.container),\n             StorageID(getDatabaseName(), table_name),\n             columns,\n             ConstraintsDescription{},\n",
  "test_patch": "diff --git a/tests/integration/test_backup_restore_azure_blob_storage/__init__.py b/tests/integration/test_backup_restore_azure_blob_storage/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_backup_restore_azure_blob_storage/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_backup_restore_azure_blob_storage/test.py b/tests/integration/test_backup_restore_azure_blob_storage/test.py\nnew file mode 100644\nindex 000000000000..a7c7b4395604\n--- /dev/null\n+++ b/tests/integration/test_backup_restore_azure_blob_storage/test.py\n@@ -0,0 +1,270 @@\n+#!/usr/bin/env python3\n+\n+import gzip\n+import json\n+import logging\n+import os\n+import io\n+import random\n+import threading\n+import time\n+\n+from azure.storage.blob import BlobServiceClient\n+import helpers.client\n+import pytest\n+from helpers.cluster import ClickHouseCluster, ClickHouseInstance\n+from helpers.network import PartitionManager\n+from helpers.mock_servers import start_mock_servers\n+from helpers.test_tools import exec_query_with_retry\n+\n+\n+def generate_cluster_def(port):\n+    path = os.path.join(\n+        os.path.dirname(os.path.realpath(__file__)),\n+        \"./_gen/named_collections.xml\",\n+    )\n+    os.makedirs(os.path.dirname(path), exist_ok=True)\n+    with open(path, \"w\") as f:\n+        f.write(\n+            f\"\"\"<clickhouse>\n+    <named_collections>\n+        <azure_conf1>\n+            <connection_string>DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://azurite1:{port}/devstoreaccount1;</connection_string>\n+            <container>cont</container>\n+            <format>CSV</format>\n+        </azure_conf1>\n+        <azure_conf2>\n+            <storage_account_url>http://azurite1:{port}/devstoreaccount1</storage_account_url>\n+            <container>cont</container>\n+            <format>CSV</format>\n+            <account_name>devstoreaccount1</account_name>\n+            <account_key>Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==</account_key>\n+        </azure_conf2>\n+    </named_collections>\n+</clickhouse>\n+\"\"\"\n+        )\n+    return path\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def cluster():\n+    try:\n+        cluster = ClickHouseCluster(__file__)\n+        port = cluster.azurite_port\n+        path = generate_cluster_def(port)\n+        cluster.add_instance(\n+            \"node\",\n+            main_configs=[path],\n+            with_azurite=True,\n+        )\n+        cluster.start()\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def azure_query(\n+    node, query, expect_error=\"false\", try_num=10, settings={}, query_on_retry=None\n+):\n+    for i in range(try_num):\n+        try:\n+            if expect_error == \"true\":\n+                return node.query_and_get_error(query, settings=settings)\n+            else:\n+                return node.query(query, settings=settings)\n+        except Exception as ex:\n+            retriable_errors = [\n+                \"DB::Exception: Azure::Core::Http::TransportException: Connection was closed by the server while trying to read a response\",\n+                \"DB::Exception: Azure::Core::Http::TransportException: Connection closed before getting full response or response is less than expected\",\n+                \"DB::Exception: Azure::Core::Http::TransportException: Connection was closed by the server while trying to read a response\",\n+                \"DB::Exception: Azure::Core::Http::TransportException: Error while polling for socket ready read\",\n+                \"Azure::Core::Http::TransportException, e.what() = Connection was closed by the server while trying to read a response\",\n+                \"Azure::Core::Http::TransportException, e.what() = Connection closed before getting full response or response is less than expected\",\n+                \"Azure::Core::Http::TransportException, e.what() = Connection was closed by the server while trying to read a response\",\n+                \"Azure::Core::Http::TransportException, e.what() = Error while polling for socket ready read\",\n+            ]\n+            retry = False\n+            for error in retriable_errors:\n+                if error in str(ex):\n+                    retry = True\n+                    print(f\"Try num: {i}. Having retriable error: {ex}\")\n+                    time.sleep(i)\n+                    break\n+            if not retry or i == try_num - 1:\n+                raise Exception(ex)\n+            if query_on_retry is not None:\n+                node.query(query_on_retry)\n+            continue\n+\n+\n+def get_azure_file_content(filename, port):\n+    container_name = \"cont\"\n+    connection_string = (\n+        f\"DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;\"\n+        f\"AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;\"\n+        f\"BlobEndpoint=http://127.0.0.1:{port}/devstoreaccount1;\"\n+    )\n+    blob_service_client = BlobServiceClient.from_connection_string(\n+        str(connection_string)\n+    )\n+    container_client = blob_service_client.get_container_client(container_name)\n+    blob_client = container_client.get_blob_client(filename)\n+    download_stream = blob_client.download_blob()\n+    return download_stream.readall().decode(\"utf-8\")\n+\n+\n+def put_azure_file_content(filename, port, data):\n+    container_name = \"cont\"\n+    connection_string = (\n+        f\"DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;\"\n+        f\"AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;\"\n+        f\"BlobEndpoint=http://127.0.0.1:{port}/devstoreaccount1;\"\n+    )\n+    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n+    try:\n+        container_client = blob_service_client.create_container(container_name)\n+    except:\n+        container_client = blob_service_client.get_container_client(container_name)\n+\n+    blob_client = container_client.get_blob_client(filename)\n+    buf = io.BytesIO(data)\n+    blob_client.upload_blob(buf)\n+\n+\n+@pytest.fixture(autouse=True, scope=\"function\")\n+def delete_all_files(cluster):\n+    port = cluster.env_variables[\"AZURITE_PORT\"]\n+    connection_string = (\n+        f\"DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;\"\n+        f\"AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;\"\n+        f\"BlobEndpoint=http://127.0.0.1:{port}/devstoreaccount1;\"\n+    )\n+    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n+    containers = blob_service_client.list_containers()\n+    for container in containers:\n+        container_client = blob_service_client.get_container_client(container)\n+        blob_list = container_client.list_blobs()\n+        for blob in blob_list:\n+            print(blob)\n+            blob_client = container_client.get_blob_client(blob)\n+            blob_client.delete_blob()\n+\n+        assert len(list(container_client.list_blobs())) == 0\n+\n+    yield\n+\n+\n+def test_backup_restore(cluster):\n+    node = cluster.instances[\"node\"]\n+    port = cluster.env_variables[\"AZURITE_PORT\"]\n+    azure_query(\n+        node,\n+        f\"CREATE TABLE test_simple_write_connection_string (key UInt64, data String) Engine = AzureBlobStorage('{cluster.env_variables['AZURITE_CONNECTION_STRING']}', 'cont', 'test_simple_write_c.csv', 'CSV')\",\n+    )\n+    azure_query(\n+        node, f\"INSERT INTO test_simple_write_connection_string VALUES (1, 'a')\"\n+    )\n+    print(get_azure_file_content(\"test_simple_write_c.csv\", port))\n+    assert get_azure_file_content(\"test_simple_write_c.csv\", port) == '1,\"a\"\\n'\n+\n+    backup_destination = f\"AzureBlobStorage('{cluster.env_variables['AZURITE_CONNECTION_STRING']}', 'cont', 'test_simple_write_c_backup.csv')\"\n+    azure_query(\n+        node,\n+        f\"BACKUP TABLE test_simple_write_connection_string TO {backup_destination}\",\n+    )\n+    print(get_azure_file_content(\"test_simple_write_c_backup.csv.backup\", port))\n+    azure_query(\n+        node,\n+        f\"RESTORE TABLE test_simple_write_connection_string AS test_simple_write_connection_string_restored FROM {backup_destination};\",\n+    )\n+    assert (\n+        azure_query(node, f\"SELECT * from test_simple_write_connection_string_restored\")\n+        == \"1\\ta\\n\"\n+    )\n+\n+\n+def test_backup_restore_diff_container(cluster):\n+    node = cluster.instances[\"node\"]\n+    port = cluster.env_variables[\"AZURITE_PORT\"]\n+    azure_query(\n+        node,\n+        f\"CREATE TABLE test_simple_write_connection_string_cont1 (key UInt64, data String) Engine = AzureBlobStorage('{cluster.env_variables['AZURITE_CONNECTION_STRING']}', 'cont', 'test_simple_write_c_cont1.csv', 'CSV')\",\n+    )\n+    azure_query(\n+        node, f\"INSERT INTO test_simple_write_connection_string_cont1 VALUES (1, 'a')\"\n+    )\n+    backup_destination = f\"AzureBlobStorage('{cluster.env_variables['AZURITE_CONNECTION_STRING']}', 'cont1', 'test_simple_write_c_backup_cont1.csv')\"\n+    azure_query(\n+        node,\n+        f\"BACKUP TABLE test_simple_write_connection_string_cont1 TO {backup_destination}\",\n+    )\n+    azure_query(\n+        node,\n+        f\"RESTORE TABLE test_simple_write_connection_string_cont1 AS test_simple_write_connection_string_restored_cont1 FROM {backup_destination};\",\n+    )\n+    assert (\n+        azure_query(\n+            node, f\"SELECT * from test_simple_write_connection_string_restored_cont1\"\n+        )\n+        == \"1\\ta\\n\"\n+    )\n+\n+\n+def test_backup_restore_with_named_collection_azure_conf1(cluster):\n+    node = cluster.instances[\"node\"]\n+    port = cluster.env_variables[\"AZURITE_PORT\"]\n+    azure_query(\n+        node,\n+        f\"CREATE TABLE test_write_connection_string (key UInt64, data String) Engine = AzureBlobStorage('{cluster.env_variables['AZURITE_CONNECTION_STRING']}', 'cont', 'test_simple_write.csv', 'CSV')\",\n+    )\n+    azure_query(node, f\"INSERT INTO test_write_connection_string VALUES (1, 'a')\")\n+    print(get_azure_file_content(\"test_simple_write.csv\", port))\n+    assert get_azure_file_content(\"test_simple_write.csv\", port) == '1,\"a\"\\n'\n+\n+    backup_destination = (\n+        f\"AzureBlobStorage(azure_conf1, 'test_simple_write_nc_backup.csv')\"\n+    )\n+    azure_query(\n+        node,\n+        f\"BACKUP TABLE test_write_connection_string TO {backup_destination}\",\n+    )\n+    print(get_azure_file_content(\"test_simple_write_nc_backup.csv.backup\", port))\n+    azure_query(\n+        node,\n+        f\"RESTORE TABLE test_write_connection_string AS test_write_connection_string_restored FROM {backup_destination};\",\n+    )\n+    assert (\n+        azure_query(node, f\"SELECT * from test_write_connection_string_restored\")\n+        == \"1\\ta\\n\"\n+    )\n+\n+\n+def test_backup_restore_with_named_collection_azure_conf2(cluster):\n+    node = cluster.instances[\"node\"]\n+    port = cluster.env_variables[\"AZURITE_PORT\"]\n+    azure_query(\n+        node,\n+        f\"CREATE TABLE test_write_connection_string_2 (key UInt64, data String) Engine = AzureBlobStorage('{cluster.env_variables['AZURITE_CONNECTION_STRING']}', 'cont', 'test_simple_write_2.csv', 'CSV')\",\n+    )\n+    azure_query(node, f\"INSERT INTO test_write_connection_string_2 VALUES (1, 'a')\")\n+    print(get_azure_file_content(\"test_simple_write_2.csv\", port))\n+    assert get_azure_file_content(\"test_simple_write_2.csv\", port) == '1,\"a\"\\n'\n+\n+    backup_destination = (\n+        f\"AzureBlobStorage(azure_conf2, 'test_simple_write_nc_backup_2.csv')\"\n+    )\n+    azure_query(\n+        node,\n+        f\"BACKUP TABLE test_write_connection_string_2 TO {backup_destination}\",\n+    )\n+    print(get_azure_file_content(\"test_simple_write_nc_backup_2.csv.backup\", port))\n+    azure_query(\n+        node,\n+        f\"RESTORE TABLE test_write_connection_string_2 AS test_write_connection_string_restored_2 FROM {backup_destination};\",\n+    )\n+    assert (\n+        azure_query(node, f\"SELECT * from test_write_connection_string_restored_2\")\n+        == \"1\\ta\\n\"\n+    )\n",
  "problem_statement": "Azure blob storage as destination for embedded backup\n**Use case**\r\n\r\nBACKUP TABLE xxx TO AzureBlobStorage(....)\r\n\r\n**Describe the solution you'd like**\r\n\r\nNative support for backups to Blob Storage\r\n\r\n**Describe alternatives you've considered**\r\n\r\nBackup to blob storage based disks, but it needs to backup local metadata\r\n\n",
  "hints_text": "Implement 2 classes similar to https://github.com/clickhouse/ClickHouse/blob/master/src/Backups/BackupIO_S3.h#L17-L64",
  "created_at": "2023-11-20T10:01:44Z",
  "modified_files": [
    "docs/en/operations/backup.md",
    "src/Backups/BackupFactory.cpp",
    "b/src/Backups/BackupIO_AzureBlobStorage.cpp",
    "b/src/Backups/BackupIO_AzureBlobStorage.h",
    "src/Backups/BackupImpl.cpp",
    "b/src/Backups/registerBackupEngineAzureBlobStorage.cpp",
    "src/CMakeLists.txt",
    "src/Common/ProfileEvents.cpp",
    "src/Core/Settings.h",
    "src/Disks/ObjectStorages/AzureBlobStorage/AzureBlobStorageAuth.cpp",
    "src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.cpp",
    "src/Disks/ObjectStorages/AzureBlobStorage/AzureObjectStorage.h",
    "src/Disks/ObjectStorages/Cached/CachedObjectStorage.h",
    "src/Disks/ObjectStorages/IObjectStorage.h",
    "src/Disks/ObjectStorages/ObjectStorageFactory.cpp",
    "b/src/IO/AzureBlobStorage/copyAzureBlobStorageFile.cpp",
    "b/src/IO/AzureBlobStorage/copyAzureBlobStorageFile.h",
    "src/Storages/StorageAzureBlob.cpp",
    "src/TableFunctions/TableFunctionAzureBlobStorage.cpp",
    "src/TableFunctions/TableFunctionAzureBlobStorageCluster.cpp"
  ],
  "modified_test_files": [
    "b/tests/integration/test_backup_restore_azure_blob_storage/__init__.py",
    "b/tests/integration/test_backup_restore_azure_blob_storage/test.py"
  ]
}