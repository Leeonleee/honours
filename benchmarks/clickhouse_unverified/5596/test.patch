diff --git a/dbms/tests/integration/helpers/cluster.py b/dbms/tests/integration/helpers/cluster.py
index aadd2e70a529..4131ee086532 100644
--- a/dbms/tests/integration/helpers/cluster.py
+++ b/dbms/tests/integration/helpers/cluster.py
@@ -225,12 +225,12 @@ def _replace(self, path, what, to):
     def restart_instance_with_ip_change(self, node, new_ip):
         if '::' in new_ip:
             if node.ipv6_address is None:
-                raise Exception("You shoud specity ipv6_address in add_node method")
+                raise Exception("You should specity ipv6_address in add_node method")
             self._replace(node.docker_compose_path, node.ipv6_address, new_ip)
             node.ipv6_address = new_ip
         else:
             if node.ipv4_address is None:
-                raise Exception("You shoud specity ipv4_address in add_node method")
+                raise Exception("You should specity ipv4_address in add_node method")
             self._replace(node.docker_compose_path, node.ipv4_address, new_ip)
             node.ipv4_address = new_ip
         subprocess.check_call(self.base_cmd + ["stop", node.name])
diff --git a/dbms/tests/integration/runner b/dbms/tests/integration/runner
index 0d0ec929b960..071df8b1fd02 100755
--- a/dbms/tests/integration/runner
+++ b/dbms/tests/integration/runner
@@ -107,4 +107,4 @@ if __name__ == "__main__":
     )
 
     #print(cmd)
-    subprocess.check_call(cmd, shell=True)
\ No newline at end of file
+    subprocess.check_call(cmd, shell=True)
diff --git a/dbms/tests/integration/test_storage_s3/__init__.py b/dbms/tests/integration/test_storage_s3/__init__.py
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/dbms/tests/integration/test_storage_s3/configs/min_chunk_size.xml b/dbms/tests/integration/test_storage_s3/configs/min_chunk_size.xml
new file mode 100644
index 000000000000..f61fcd2c5c91
--- /dev/null
+++ b/dbms/tests/integration/test_storage_s3/configs/min_chunk_size.xml
@@ -0,0 +1,3 @@
+<yandex>
+    <s3_minimum_upload_part_size>1000000</s3_minimum_upload_part_size>
+</yandex>
diff --git a/dbms/tests/integration/test_storage_s3/test.py b/dbms/tests/integration/test_storage_s3/test.py
new file mode 100644
index 000000000000..84f6bf72f60f
--- /dev/null
+++ b/dbms/tests/integration/test_storage_s3/test.py
@@ -0,0 +1,159 @@
+import httplib
+import json
+import logging
+import os
+import time
+import traceback
+
+import pytest
+
+from helpers.cluster import ClickHouseCluster
+
+
+logging.getLogger().setLevel(logging.INFO)
+logging.getLogger().addHandler(logging.StreamHandler())
+
+
+def get_communication_data(started_cluster):
+    conn = httplib.HTTPConnection(started_cluster.instances["dummy"].ip_address, started_cluster.communication_port)
+    conn.request("GET", "/")
+    r = conn.getresponse()
+    raw_data = r.read()
+    conn.close()
+    return json.loads(raw_data)
+
+
+def put_communication_data(started_cluster, body):
+    conn = httplib.HTTPConnection(started_cluster.instances["dummy"].ip_address, started_cluster.communication_port)
+    conn.request("PUT", "/", body)
+    r = conn.getresponse()
+    conn.close()
+
+
+@pytest.fixture(scope="module")
+def started_cluster():
+    try:
+        cluster = ClickHouseCluster(__file__)
+        instance = cluster.add_instance("dummy", config_dir="configs", main_configs=["configs/min_chunk_size.xml"])
+        cluster.start()
+
+        cluster.communication_port = 10000
+        instance.copy_file_to_container(os.path.join(os.path.dirname(__file__), "test_server.py"), "test_server.py")
+        cluster.bucket = "abc"
+        instance.exec_in_container(["python", "test_server.py", str(cluster.communication_port), cluster.bucket], detach=True)
+        cluster.mock_host = instance.ip_address
+
+        for i in range(10):
+            try:
+                data = get_communication_data(cluster)
+                cluster.redirecting_to_http_port = data["redirecting_to_http_port"]
+                cluster.preserving_data_port = data["preserving_data_port"]
+                cluster.multipart_preserving_data_port = data["multipart_preserving_data_port"]
+                cluster.redirecting_preserving_data_port = data["redirecting_preserving_data_port"]
+            except:
+                logging.error(traceback.format_exc())
+                time.sleep(0.5)
+            else:
+                break
+        else:
+            assert False, "Could not initialize mock server"
+
+        yield cluster
+
+    finally:
+        cluster.shutdown()
+
+
+def run_query(instance, query, stdin=None):
+    logging.info("Running query '{}'...".format(query))
+    result = instance.query(query, stdin=stdin)
+    logging.info("Query finished")
+    return result
+
+
+def test_get_with_redirect(started_cluster):
+    instance = started_cluster.instances["dummy"]
+    format = "column1 UInt32, column2 UInt32, column3 UInt32"
+
+    put_communication_data(started_cluster, "=== Get with redirect test ===")
+    query = "select *, column1*column2*column3 from s3('http://{}:{}/', 'CSV', '{}')".format(started_cluster.mock_host, started_cluster.redirecting_to_http_port, format)
+    stdout = run_query(instance, query)
+    data = get_communication_data(started_cluster)
+    expected = [ [str(row[0]), str(row[1]), str(row[2]), str(row[0]*row[1]*row[2])] for row in data["redirect_csv_data"] ]
+    assert list(map(str.split, stdout.splitlines())) == expected
+    
+
+def test_put(started_cluster):
+    instance = started_cluster.instances["dummy"]
+    format = "column1 UInt32, column2 UInt32, column3 UInt32"
+
+    logging.info("Phase 3")
+    put_communication_data(started_cluster, "=== Put test ===")
+    values = "(1, 2, 3), (3, 2, 1), (78, 43, 45)"
+    put_query = "insert into table function s3('http://{}:{}/{}/test.csv', 'CSV', '{}') values {}".format(started_cluster.mock_host, started_cluster.preserving_data_port, started_cluster.bucket, format, values)
+    run_query(instance, put_query)
+    data = get_communication_data(started_cluster)
+    received_data_completed = data["received_data_completed"]
+    received_data = data["received_data"]
+    finalize_data = data["finalize_data"]
+    finalize_data_query = data["finalize_data_query"]
+    assert received_data[-1].decode() == "1,2,3
3,2,1
78,43,45
"
+    assert received_data_completed
+    assert finalize_data == "<CompleteMultipartUpload><Part><PartNumber>1</PartNumber><ETag>hello-etag</ETag></Part></CompleteMultipartUpload>"
+    assert finalize_data_query == "uploadId=TEST"
+
+    
+def test_put_csv(started_cluster):
+    instance = started_cluster.instances["dummy"]
+    format = "column1 UInt32, column2 UInt32, column3 UInt32"
+
+    put_communication_data(started_cluster, "=== Put test CSV ===")
+    put_query = "insert into table function s3('http://{}:{}/{}/test.csv', 'CSV', '{}') format CSV".format(started_cluster.mock_host, started_cluster.preserving_data_port, started_cluster.bucket, format)
+    csv_data = "8,9,16
11,18,13
22,14,2
"
+    run_query(instance, put_query, stdin=csv_data)
+    data = get_communication_data(started_cluster)
+    received_data_completed = data["received_data_completed"]
+    received_data = data["received_data"]
+    finalize_data = data["finalize_data"]
+    finalize_data_query = data["finalize_data_query"]
+    assert received_data[-1].decode() == csv_data
+    assert received_data_completed
+    assert finalize_data == "<CompleteMultipartUpload><Part><PartNumber>1</PartNumber><ETag>hello-etag</ETag></Part></CompleteMultipartUpload>"
+    assert finalize_data_query == "uploadId=TEST"
+
+    
+def test_put_with_redirect(started_cluster):
+    instance = started_cluster.instances["dummy"]
+    format = "column1 UInt32, column2 UInt32, column3 UInt32"
+
+    put_communication_data(started_cluster, "=== Put with redirect test ===")
+    other_values = "(1, 1, 1), (1, 1, 1), (11, 11, 11)"
+    query = "insert into table function s3('http://{}:{}/{}/test.csv', 'CSV', '{}') values {}".format(started_cluster.mock_host, started_cluster.redirecting_preserving_data_port, started_cluster.bucket, format, other_values)
+    run_query(instance, query)
+
+    query = "select *, column1*column2*column3 from s3('http://{}:{}/{}/test.csv', 'CSV', '{}')".format(started_cluster.mock_host, started_cluster.preserving_data_port, started_cluster.bucket, format)
+    stdout = run_query(instance, query)
+    assert list(map(str.split, stdout.splitlines())) == [
+        ["1", "1", "1", "1"],
+        ["1", "1", "1", "1"],
+        ["11", "11", "11", "1331"],
+    ]
+    data = get_communication_data(started_cluster)
+    received_data = data["received_data"]
+    assert received_data[-1].decode() == "1,1,1
1,1,1
11,11,11
"
+
+
+def test_multipart_put(started_cluster):
+    instance = started_cluster.instances["dummy"]
+    format = "column1 UInt32, column2 UInt32, column3 UInt32"
+
+    put_communication_data(started_cluster, "=== Multipart test ===")
+    long_data = [[i, i+1, i+2] for i in range(100000)]
+    long_values = "".join([ "{},{},{}
".format(x,y,z) for x, y, z in long_data ])
+    put_query = "insert into table function s3('http://{}:{}/{}/test.csv', 'CSV', '{}') format CSV".format(started_cluster.mock_host, started_cluster.multipart_preserving_data_port, started_cluster.bucket, format)
+    run_query(instance, put_query, stdin=long_values)
+    data = get_communication_data(started_cluster)
+    assert "multipart_received_data" in data
+    received_data = data["multipart_received_data"]
+    assert received_data[-1].decode() == "".join([ "{},{},{}
".format(x, y, z) for x, y, z in long_data ])
+    assert 1 < data["multipart_parts"] < 10000
diff --git a/dbms/tests/integration/test_storage_s3/test_server.py b/dbms/tests/integration/test_storage_s3/test_server.py
new file mode 100644
index 000000000000..8896af9c23e7
--- /dev/null
+++ b/dbms/tests/integration/test_storage_s3/test_server.py
@@ -0,0 +1,365 @@
+try:
+    from BaseHTTPServer import BaseHTTPRequestHandler
+except ImportError:
+    from http.server import BaseHTTPRequestHandler
+
+try:
+    from BaseHTTPServer import HTTPServer
+except ImportError:
+    from http.server import HTTPServer
+
+try:
+    import urllib.parse as urlparse
+except ImportError:
+    import urlparse
+
+import json
+import logging
+import os
+import socket
+import sys
+import threading
+import time
+import uuid
+import xml.etree.ElementTree
+
+
+logging.getLogger().setLevel(logging.INFO)
+file_handler = logging.FileHandler("/var/log/clickhouse-server/test-server.log", "a", encoding="utf-8")
+file_handler.setFormatter(logging.Formatter("%(asctime)s %(message)s"))
+logging.getLogger().addHandler(file_handler)
+logging.getLogger().addHandler(logging.StreamHandler())
+
+communication_port = int(sys.argv[1])
+bucket = sys.argv[2]
+
+def GetFreeTCPPortsAndIP(n):
+    result = []
+    sockets = []
+    for i in range(n):
+        tcp = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        tcp.bind((socket.gethostname(), 0))
+        addr, port = tcp.getsockname()
+        result.append(port)
+        sockets.append(tcp)
+    [ s.close() for s in sockets ]
+    return result, addr
+
+(
+    redirecting_to_http_port,
+    simple_server_port,
+    preserving_data_port,
+    multipart_preserving_data_port,
+    redirecting_preserving_data_port
+), localhost = GetFreeTCPPortsAndIP(5)
+
+data = {
+    "redirecting_to_http_port": redirecting_to_http_port,
+    "preserving_data_port": preserving_data_port,
+    "multipart_preserving_data_port": multipart_preserving_data_port,
+    "redirecting_preserving_data_port": redirecting_preserving_data_port,
+}
+
+
+class SimpleHTTPServerHandler(BaseHTTPRequestHandler):
+    def do_GET(self):
+        logging.info("GET {}".format(self.path))
+        if self.path == "/milovidov/test.csv":
+             self.send_response(200)
+             self.send_header("Content-type", "text/plain")
+             self.end_headers()
+             data["redirect_csv_data"] = [[42, 87, 44], [55, 33, 81], [1, 0, 9]]
+             self.wfile.write("".join([ "{},{},{}
".format(*row) for row in data["redirect_csv_data"]]))
+        else:
+             self.send_response(404)
+             self.end_headers()
+        self.finish()
+
+
+class RedirectingToHTTPHandler(BaseHTTPRequestHandler):
+    def do_GET(self):
+        self.send_response(307)
+        self.send_header("Content-type", "text/xml")
+        self.send_header("Location", "http://{}:{}/milovidov/test.csv".format(localhost, simple_server_port))
+        self.end_headers()
+        self.wfile.write(r"""<?xml version="1.0" encoding="UTF-8"?>
+<Error>
+  <Code>TemporaryRedirect</Code>
+  <Message>Please re-send this request to the specified temporary endpoint.
+  Continue to use the original request endpoint for future requests.</Message>
+  <Endpoint>storage.yandexcloud.net</Endpoint>
+</Error>""".encode())
+        self.finish()
+
+
+class PreservingDataHandler(BaseHTTPRequestHandler):
+    protocol_version = "HTTP/1.1"
+
+    def parse_request(self):
+        result = BaseHTTPRequestHandler.parse_request(self)
+        # Adaptation to Python 3.
+        if sys.version_info.major == 2 and result == True:
+            expect = self.headers.get("Expect", "")
+            if (expect.lower() == "100-continue" and self.protocol_version >= "HTTP/1.1" and self.request_version >= "HTTP/1.1"):
+                if not self.handle_expect_100():
+                    return False
+        return result
+
+    def send_response_only(self, code, message=None):
+        if message is None:
+            if code in self.responses:
+                message = self.responses[code][0]
+            else:
+                message = ""
+        if self.request_version != "HTTP/0.9":
+            self.wfile.write("%s %d %s\r
" % (self.protocol_version, code, message))
+
+    def handle_expect_100(self):
+        logging.info("Received Expect-100")
+        self.send_response_only(100)
+        self.end_headers()
+        return True
+
+    def do_POST(self):
+        self.send_response(200)
+        query = urlparse.urlparse(self.path).query
+        logging.info("PreservingDataHandler POST ?" + query)
+        if query == "uploads":
+            post_data = r"""<?xml version="1.0" encoding="UTF-8"?>
+<hi><UploadId>TEST</UploadId></hi>""".encode()
+            self.send_header("Content-length", str(len(post_data)))
+            self.send_header("Content-type", "text/plain")
+            self.end_headers()
+            self.wfile.write(post_data)
+        else:
+            post_data = self.rfile.read(int(self.headers.get("Content-Length")))
+            self.send_header("Content-type", "text/plain")
+            self.end_headers()
+            data["received_data_completed"] = True
+            data["finalize_data"] = post_data
+            data["finalize_data_query"] = query
+        self.finish()
+ 
+    def do_PUT(self):
+        self.send_response(200)
+        self.send_header("Content-type", "text/plain")
+        self.send_header("ETag", "hello-etag")
+        self.end_headers()
+        query = urlparse.urlparse(self.path).query
+        path = urlparse.urlparse(self.path).path
+        logging.info("Content-Length = " + self.headers.get("Content-Length"))
+        logging.info("PUT " + query)
+        assert self.headers.get("Content-Length")
+        assert self.headers["Expect"] == "100-continue"
+        put_data = self.rfile.read()
+        data.setdefault("received_data", []).append(put_data)
+        logging.info("PUT to {}".format(path))
+        self.server.storage[path] = put_data
+        self.finish()
+
+    def do_GET(self):
+        path = urlparse.urlparse(self.path).path
+        if path in self.server.storage:
+            self.send_response(200)
+            self.send_header("Content-type", "text/plain")
+            self.send_header("Content-length", str(len(self.server.storage[path])))
+            self.end_headers()
+            self.wfile.write(self.server.storage[path])
+        else:
+            self.send_response(404)
+            self.end_headers()
+        self.finish()
+
+
+class MultipartPreservingDataHandler(BaseHTTPRequestHandler):
+    protocol_version = "HTTP/1.1"
+
+    def parse_request(self):
+        result = BaseHTTPRequestHandler.parse_request(self)
+        # Adaptation to Python 3.
+        if sys.version_info.major == 2 and result == True:
+            expect = self.headers.get("Expect", "")
+            if (expect.lower() == "100-continue" and self.protocol_version >= "HTTP/1.1" and self.request_version >= "HTTP/1.1"):
+                if not self.handle_expect_100():
+                    return False
+        return result
+
+    def send_response_only(self, code, message=None):
+        if message is None:
+            if code in self.responses:
+                message = self.responses[code][0]
+            else:
+                message = ""
+        if self.request_version != "HTTP/0.9":
+            self.wfile.write("%s %d %s\r
" % (self.protocol_version, code, message))
+
+    def handle_expect_100(self):
+        logging.info("Received Expect-100")
+        self.send_response_only(100)
+        self.end_headers()
+        return True
+
+    def do_POST(self):
+        query = urlparse.urlparse(self.path).query
+        logging.info("MultipartPreservingDataHandler POST ?" + query)
+        if query == "uploads":
+            self.send_response(200)
+            post_data = r"""<?xml version="1.0" encoding="UTF-8"?>
+<hi><UploadId>TEST</UploadId></hi>""".encode()
+            self.send_header("Content-length", str(len(post_data)))
+            self.send_header("Content-type", "text/plain")
+            self.end_headers()
+            self.wfile.write(post_data)
+        else:
+            try:
+                assert query == "uploadId=TEST"
+                logging.info("Content-Length = " + self.headers.get("Content-Length"))
+                post_data = self.rfile.read(int(self.headers.get("Content-Length")))
+                root = xml.etree.ElementTree.fromstring(post_data)
+                assert root.tag == "CompleteMultipartUpload"
+                assert len(root) > 1
+                content = ""
+                for i, part in enumerate(root):
+                    assert part.tag == "Part"
+                    assert len(part) == 2
+                    assert part[0].tag == "PartNumber"
+                    assert part[1].tag == "ETag"
+                    assert int(part[0].text) == i + 1
+                    content += self.server.storage["@"+part[1].text]
+                data.setdefault("multipart_received_data", []).append(content)
+                data["multipart_parts"] = len(root)
+                self.send_response(200)
+                self.send_header("Content-type", "text/plain")
+                self.end_headers()
+                logging.info("Sending 200")
+            except:
+                logging.error("Sending 500")
+                self.send_response(500)
+        self.finish()
+ 
+    def do_PUT(self):
+        uid = uuid.uuid4()
+        self.send_response(200)
+        self.send_header("Content-type", "text/plain")
+        self.send_header("ETag", str(uid))
+        self.end_headers()
+        query = urlparse.urlparse(self.path).query
+        path = urlparse.urlparse(self.path).path
+        logging.info("Content-Length = " + self.headers.get("Content-Length"))
+        logging.info("PUT " + query)
+        assert self.headers.get("Content-Length")
+        assert self.headers["Expect"] == "100-continue"
+        put_data = self.rfile.read()
+        data.setdefault("received_data", []).append(put_data)
+        logging.info("PUT to {}".format(path))
+        self.server.storage["@"+str(uid)] = put_data
+        self.finish()
+
+    def do_GET(self):
+        path = urlparse.urlparse(self.path).path
+        if path in self.server.storage:
+            self.send_response(200)
+            self.send_header("Content-type", "text/plain")
+            self.send_header("Content-length", str(len(self.server.storage[path])))
+            self.end_headers()
+            self.wfile.write(self.server.storage[path])
+        else:
+            self.send_response(404)
+            self.end_headers()
+        self.finish()
+
+
+class RedirectingPreservingDataHandler(BaseHTTPRequestHandler):
+    protocol_version = "HTTP/1.1"
+
+    def parse_request(self):
+        result = BaseHTTPRequestHandler.parse_request(self)
+        # Adaptation to Python 3.
+        if sys.version_info.major == 2 and result == True:
+            expect = self.headers.get("Expect", "")
+            if (expect.lower() == "100-continue" and self.protocol_version >= "HTTP/1.1" and self.request_version >= "HTTP/1.1"):
+                if not self.handle_expect_100():
+                    return False
+        return result
+
+    def send_response_only(self, code, message=None):
+        if message is None:
+            if code in self.responses:
+                message = self.responses[code][0]
+            else:
+                message = ""
+        if self.request_version != "HTTP/0.9":
+            self.wfile.write("%s %d %s\r
" % (self.protocol_version, code, message))
+
+    def handle_expect_100(self):
+        logging.info("Received Expect-100")
+        return True
+
+    def do_POST(self):
+        query = urlparse.urlparse(self.path).query
+        if query:
+            query = "?{}".format(query)
+        self.send_response(307)
+        self.send_header("Content-type", "text/xml")
+        self.send_header("Location", "http://{host}:{port}/{bucket}/test.csv{query}".format(host=localhost, port=preserving_data_port, bucket=bucket, query=query))
+        self.end_headers()
+        self.wfile.write(r"""<?xml version="1.0" encoding="UTF-8"?>
+<Error>
+  <Code>TemporaryRedirect</Code>
+  <Message>Please re-send this request to the specified temporary endpoint.
+  Continue to use the original request endpoint for future requests.</Message>
+  <Endpoint>{host}:{port}</Endpoint>
+</Error>""".format(host=localhost, port=preserving_data_port).encode())
+        self.finish()
+
+    def do_PUT(self):
+        query = urlparse.urlparse(self.path).query
+        if query:
+            query = "?{}".format(query)
+        self.send_response(307)
+        self.send_header("Content-type", "text/xml")
+        self.send_header("Location", "http://{host}:{port}/{bucket}/test.csv{query}".format(host=localhost, port=preserving_data_port, bucket=bucket, query=query))
+        self.end_headers()
+        self.wfile.write(r"""<?xml version="1.0" encoding="UTF-8"?>
+<Error>
+  <Code>TemporaryRedirect</Code>
+  <Message>Please re-send this request to the specified temporary endpoint.
+  Continue to use the original request endpoint for future requests.</Message>
+  <Endpoint>{host}:{port}</Endpoint>
+</Error>""".format(host=localhost, port=preserving_data_port).encode())
+        self.finish()
+
+
+class CommunicationServerHandler(BaseHTTPRequestHandler):
+    def do_GET(self):
+        self.send_response(200)
+        self.end_headers()
+        self.wfile.write(json.dumps(data))
+        self.finish()
+
+    def do_PUT(self):
+        self.send_response(200)
+        self.end_headers()
+        logging.info(self.rfile.read())
+        self.finish()
+
+
+servers = []
+servers.append(HTTPServer((localhost, communication_port), CommunicationServerHandler))
+servers.append(HTTPServer((localhost, redirecting_to_http_port), RedirectingToHTTPHandler))
+servers.append(HTTPServer((localhost, preserving_data_port), PreservingDataHandler))
+servers[-1].storage = {}
+servers.append(HTTPServer((localhost, multipart_preserving_data_port), MultipartPreservingDataHandler))
+servers[-1].storage = {}
+servers.append(HTTPServer((localhost, simple_server_port), SimpleHTTPServerHandler))
+servers.append(HTTPServer((localhost, redirecting_preserving_data_port), RedirectingPreservingDataHandler))
+jobs = [ threading.Thread(target=server.serve_forever) for server in servers ]
+[ job.start() for job in jobs ]
+
+time.sleep(60) # Timeout
+
+logging.info("Shutting down")
+[ server.shutdown() for server in servers ]
+logging.info("Joining threads")
+[ job.join() for job in jobs ]
+logging.info("Done")
