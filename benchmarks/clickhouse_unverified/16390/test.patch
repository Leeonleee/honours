diff --git a/tests/integration/test_limited_replicated_fetches/__init__.py b/tests/integration/test_limited_replicated_fetches/__init__.py
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/tests/integration/test_limited_replicated_fetches/test.py b/tests/integration/test_limited_replicated_fetches/test.py
new file mode 100644
index 000000000000..2091c65857eb
--- /dev/null
+++ b/tests/integration/test_limited_replicated_fetches/test.py
@@ -0,0 +1,71 @@
+#!/usr/bin/env python3
+
+import pytest
+import time
+from helpers.cluster import ClickHouseCluster
+from helpers.network import PartitionManager
+import random
+import string
+
+cluster = ClickHouseCluster(__file__)
+node1 = cluster.add_instance('node1', with_zookeeper=True)
+node2 = cluster.add_instance('node2', with_zookeeper=True)
+
+DEFAULT_MAX_THREADS_FOR_FETCH = 3
+
+@pytest.fixture(scope="module")
+def started_cluster():
+    try:
+        cluster.start()
+
+        yield cluster
+
+    finally:
+        cluster.shutdown()
+
+
+def get_random_string(length):
+    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(length))
+
+
+def test_limited_fetches(started_cluster):
+    """
+        Test checks that that we utilize all available threads for fetches
+    """
+    node1.query("CREATE TABLE t (key UInt64, data String) ENGINE = ReplicatedMergeTree('/clickhouse/test/t', '1') ORDER BY tuple() PARTITION BY key")
+    node2.query("CREATE TABLE t (key UInt64, data String) ENGINE = ReplicatedMergeTree('/clickhouse/test/t', '2') ORDER BY tuple() PARTITION BY key")
+
+    with PartitionManager() as pm:
+        node2.query("SYSTEM STOP FETCHES t")
+        node1.query("INSERT INTO t SELECT 1, '{}' FROM numbers(5000)".format(get_random_string(104857)))
+        node1.query("INSERT INTO t SELECT 2, '{}' FROM numbers(5000)".format(get_random_string(104857)))
+        node1.query("INSERT INTO t SELECT 3, '{}' FROM numbers(5000)".format(get_random_string(104857)))
+        node1.query("INSERT INTO t SELECT 4, '{}' FROM numbers(5000)".format(get_random_string(104857)))
+        node1.query("INSERT INTO t SELECT 5, '{}' FROM numbers(5000)".format(get_random_string(104857)))
+        node1.query("INSERT INTO t SELECT 6, '{}' FROM numbers(5000)".format(get_random_string(104857)))
+        pm.add_network_delay(node1, 80)
+        node2.query("SYSTEM START FETCHES t")
+        fetches_result = []
+        background_fetches_metric = []
+        fetched_parts = set([])
+        for _ in range(1000):
+            result = node2.query("SELECT result_part_name FROM system.replicated_fetches").strip().split()
+            background_fetches_metric.append(int(node2.query("select value from system.metrics where metric = 'BackgroundFetchesPoolTask'").strip()))
+            if not result:
+                if len(fetched_parts) == 6:
+                    break
+                time.sleep(0.1)
+            else:
+                for part in result:
+                    fetched_parts.add(part)
+                fetches_result.append(result)
+                print(fetches_result[-1])
+                print(background_fetches_metric[-1])
+                time.sleep(0.1)
+
+    for concurrently_fetching_parts in fetches_result:
+        if len(concurrently_fetching_parts) > DEFAULT_MAX_THREADS_FOR_FETCH:
+            assert False, "Found more than {} concurrently fetching parts: {}".format(DEFAULT_MAX_THREADS_FOR_FETCH, ', '.join(concurrently_fetching_parts))
+
+    assert max([len(parts) for parts in fetches_result]) == 3, "Strange, but we don't utilize max concurrent threads for fetches"
+    assert(max(background_fetches_metric)) == 3, "Just checking metric consistent with table"
diff --git a/tests/queries/0_stateless/01459_manual_write_to_replicas.sh b/tests/queries/0_stateless/01459_manual_write_to_replicas.sh
index c402e19c3dc9..1cf0ed56bc5e 100755
--- a/tests/queries/0_stateless/01459_manual_write_to_replicas.sh
+++ b/tests/queries/0_stateless/01459_manual_write_to_replicas.sh
@@ -16,7 +16,9 @@ done
 
 function thread {
     for x in {0..99}; do
-        $CLICKHOUSE_CLIENT --query "INSERT INTO r$1 SELECT $x % $NUM_REPLICAS = $1 ? $x - 1 : $x"  # Replace some records as duplicates so they will be written by other replicas
+        # sometimes we can try to commit obsolete part if fetches will be quite fast,
+        # so supress warning messages like "Tried to commit obsolete part ... covered by ..."
+        $CLICKHOUSE_CLIENT --query "INSERT INTO r$1 SELECT $x % $NUM_REPLICAS = $1 ? $x - 1 : $x" 2>/dev/null  # Replace some records as duplicates so they will be written by other replicas
     done
 }
 
