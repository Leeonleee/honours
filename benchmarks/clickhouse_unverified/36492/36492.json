{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 36492,
  "instance_id": "ClickHouse__ClickHouse-36492",
  "issue_numbers": [
    "35707"
  ],
  "base_commit": "18d094d79d239bbb8d14ea509b6dc7bf2a74e0fb",
  "patch": "diff --git a/programs/keeper/keeper_config.xml b/programs/keeper/keeper_config.xml\nindex bb80850d01e3..3d728e2bfdf1 100644\n--- a/programs/keeper/keeper_config.xml\n+++ b/programs/keeper/keeper_config.xml\n@@ -44,6 +44,8 @@\n                 <!-- All settings listed in https://github.com/ClickHouse/ClickHouse/blob/master/src/Coordination/CoordinationSettings.h -->\n             </coordination_settings>\n \n+            <!-- enable sanity hostname checks for cluster configuration (e.g. if localhost is used with remote endpoints) -->\n+            <hostname_checks_enabled>true</hostname_checks_enabled>\n             <raft_configuration>\n                 <server>\n                     <id>1</id>\ndiff --git a/src/Coordination/KeeperStateManager.cpp b/src/Coordination/KeeperStateManager.cpp\nindex 7304f7e0e6bd..aad37049a89b 100644\n--- a/src/Coordination/KeeperStateManager.cpp\n+++ b/src/Coordination/KeeperStateManager.cpp\n@@ -1,10 +1,10 @@\n #include <Coordination/KeeperStateManager.h>\n \n+#include <filesystem>\n #include <Coordination/Defines.h>\n+#include <Common/DNSResolver.h>\n #include <Common/Exception.h>\n-#include <filesystem>\n #include <Common/isLocalAddress.h>\n-#include <Common/DNSResolver.h>\n \n namespace DB\n {\n@@ -17,20 +17,6 @@ namespace ErrorCodes\n namespace\n {\n \n-bool isLoopback(const std::string & hostname)\n-{\n-    try\n-    {\n-        return DNSResolver::instance().resolveHost(hostname).isLoopback();\n-    }\n-    catch (...)\n-    {\n-        tryLogCurrentException(__PRETTY_FUNCTION__);\n-    }\n-\n-    return false;\n-}\n-\n bool isLocalhost(const std::string & hostname)\n {\n     try\n@@ -47,18 +33,19 @@ bool isLocalhost(const std::string & hostname)\n \n std::unordered_map<UInt64, std::string> getClientPorts(const Poco::Util::AbstractConfiguration & config)\n {\n-    static const char * config_port_names[] = {\n-        \"keeper_server.tcp_port\",\n-        \"keeper_server.tcp_port_secure\",\n-        \"interserver_http_port\",\n-        \"interserver_https_port\",\n-        \"tcp_port\",\n-        \"tcp_with_proxy_port\",\n-        \"tcp_port_secure\",\n-        \"mysql_port\",\n-        \"postgresql_port\",\n-        \"grpc_port\",\n-        \"prometheus.port\",\n+    using namespace std::string_literals;\n+    static const std::array config_port_names = {\n+        \"keeper_server.tcp_port\"s,\n+        \"keeper_server.tcp_port_secure\"s,\n+        \"interserver_http_port\"s,\n+        \"interserver_https_port\"s,\n+        \"tcp_port\"s,\n+        \"tcp_with_proxy_port\"s,\n+        \"tcp_port_secure\"s,\n+        \"mysql_port\"s,\n+        \"postgresql_port\"s,\n+        \"grpc_port\"s,\n+        \"prometheus.port\"s,\n     };\n \n     std::unordered_map<UInt64, std::string> ports;\n@@ -78,8 +65,11 @@ std::unordered_map<UInt64, std::string> getClientPorts(const Poco::Util::Abstrac\n /// 3. Raft internal port is not equal to any other port for client\n /// 4. No duplicate IDs\n /// 5. Our ID present in hostnames list\n-KeeperStateManager::KeeperConfigurationWrapper KeeperStateManager::parseServersConfiguration(const Poco::Util::AbstractConfiguration & config, bool allow_without_us) const\n+KeeperStateManager::KeeperConfigurationWrapper\n+KeeperStateManager::parseServersConfiguration(const Poco::Util::AbstractConfiguration & config, bool allow_without_us) const\n {\n+    const bool hostname_checks_enabled = config.getBool(config_prefix + \".hostname_checks_enabled\", true);\n+\n     KeeperConfigurationWrapper result;\n     result.cluster_config = std::make_shared<nuraft::cluster_config>();\n     Poco::Util::AbstractConfiguration::Keys keys;\n@@ -93,7 +83,7 @@ KeeperStateManager::KeeperConfigurationWrapper KeeperStateManager::parseServersC\n     std::unordered_map<std::string, int> check_duplicated_hostnames;\n \n     size_t total_servers = 0;\n-    std::string loopback_hostname;\n+    bool localhost_present = false;\n     std::string non_local_hostname;\n     size_t local_address_counter = 0;\n     for (const auto & server_key : keys)\n@@ -111,22 +101,29 @@ KeeperStateManager::KeeperConfigurationWrapper KeeperStateManager::parseServersC\n \n         if (client_ports.contains(port))\n         {\n-            throw Exception(ErrorCodes::RAFT_ERROR, \"Raft configuration contains hostname '{}' with port '{}' which is equal to '{}' in server configuration\",\n-                            hostname, port, client_ports[port]);\n+            throw Exception(\n+                ErrorCodes::RAFT_ERROR,\n+                \"Raft configuration contains hostname '{}' with port '{}' which is equal to '{}' in server configuration\",\n+                hostname,\n+                port,\n+                client_ports[port]);\n         }\n \n-        if (isLoopback(hostname))\n+        if (hostname_checks_enabled)\n         {\n-            loopback_hostname = hostname;\n-            local_address_counter++;\n-        }\n-        else if (isLocalhost(hostname))\n-        {\n-            local_address_counter++;\n-        }\n-        else\n-        {\n-            non_local_hostname = hostname;\n+            if (hostname == \"localhost\")\n+            {\n+                localhost_present = true;\n+                local_address_counter++;\n+            }\n+            else if (isLocalhost(hostname))\n+            {\n+                local_address_counter++;\n+            }\n+            else\n+            {\n+                non_local_hostname = hostname;\n+            }\n         }\n \n         if (start_as_follower)\n@@ -135,9 +132,13 @@ KeeperStateManager::KeeperConfigurationWrapper KeeperStateManager::parseServersC\n         auto endpoint = hostname + \":\" + std::to_string(port);\n         if (check_duplicated_hostnames.contains(endpoint))\n         {\n-            throw Exception(ErrorCodes::RAFT_ERROR, \"Raft config contains duplicate endpoints: \"\n-                            \"endpoint {} has been already added with id {}, but going to add it one more time with id {}\",\n-                            endpoint, check_duplicated_hostnames[endpoint], new_server_id);\n+            throw Exception(\n+                ErrorCodes::RAFT_ERROR,\n+                \"Raft config contains duplicate endpoints: \"\n+                \"endpoint {} has been already added with id {}, but going to add it one more time with id {}\",\n+                endpoint,\n+                check_duplicated_hostnames[endpoint],\n+                new_server_id);\n         }\n         else\n         {\n@@ -145,8 +146,13 @@ KeeperStateManager::KeeperConfigurationWrapper KeeperStateManager::parseServersC\n             for (const auto & [id_endpoint, id] : check_duplicated_hostnames)\n             {\n                 if (new_server_id == id)\n-                    throw Exception(ErrorCodes::RAFT_ERROR, \"Raft config contains duplicate ids: id {} has been already added with endpoint {}, \"\n-                                    \"but going to add it one more time with endpoint {}\", id, id_endpoint, endpoint);\n+                    throw Exception(\n+                        ErrorCodes::RAFT_ERROR,\n+                        \"Raft config contains duplicate ids: id {} has been already added with endpoint {}, \"\n+                        \"but going to add it one more time with endpoint {}\",\n+                        id,\n+                        id_endpoint,\n+                        endpoint);\n             }\n             check_duplicated_hostnames.emplace(endpoint, new_server_id);\n         }\n@@ -168,31 +174,33 @@ KeeperStateManager::KeeperConfigurationWrapper KeeperStateManager::parseServersC\n     if (result.servers_start_as_followers.size() == total_servers)\n         throw Exception(ErrorCodes::RAFT_ERROR, \"At least one of servers should be able to start as leader (without <start_as_follower>)\");\n \n-    if (!loopback_hostname.empty() && !non_local_hostname.empty())\n+    if (hostname_checks_enabled)\n     {\n-        throw Exception(\n-            ErrorCodes::RAFT_ERROR,\n-            \"Mixing loopback and non-local hostnames ('{}' and '{}') in raft_configuration is not allowed. \"\n-            \"Different hosts can resolve it to themselves so it's not allowed.\",\n-            loopback_hostname, non_local_hostname);\n-    }\n+        if (localhost_present && !non_local_hostname.empty())\n+        {\n+            throw Exception(\n+                ErrorCodes::RAFT_ERROR,\n+                \"Mixing 'localhost' and non-local hostnames ('{}') in raft_configuration is not allowed. \"\n+                \"Different hosts can resolve 'localhost' to themselves so it's not allowed.\",\n+                non_local_hostname);\n+        }\n \n-    if (!non_local_hostname.empty() && local_address_counter > 1)\n-    {\n-        throw Exception(\n-            ErrorCodes::RAFT_ERROR,\n-            \"Local address specified more than once ({} times) and non-local hostnames also exists ('{}') in raft_configuration. \"\n-            \"Such configuration is not allowed because single host can vote multiple times.\",\n-            local_address_counter, non_local_hostname);\n+        if (!non_local_hostname.empty() && local_address_counter > 1)\n+        {\n+            throw Exception(\n+                ErrorCodes::RAFT_ERROR,\n+                \"Local address specified more than once ({} times) and non-local hostnames also exists ('{}') in raft_configuration. \"\n+                \"Such configuration is not allowed because single host can vote multiple times.\",\n+                local_address_counter,\n+                non_local_hostname);\n+        }\n     }\n \n     return result;\n }\n \n KeeperStateManager::KeeperStateManager(int server_id_, const std::string & host, int port, const std::string & logs_path)\n-: my_server_id(server_id_)\n-, secure(false)\n-, log_store(nuraft::cs_new<KeeperLogStore>(logs_path, 5000, false, false))\n+    : my_server_id(server_id_), secure(false), log_store(nuraft::cs_new<KeeperLogStore>(logs_path, 5000, false, false))\n {\n     auto peer_config = nuraft::cs_new<nuraft::srv_config>(my_server_id, host + \":\" + std::to_string(port));\n     configuration_wrapper.cluster_config = nuraft::cs_new<nuraft::cluster_config>();\n@@ -212,10 +220,10 @@ KeeperStateManager::KeeperStateManager(\n     , config_prefix(config_prefix_)\n     , configuration_wrapper(parseServersConfiguration(config, false))\n     , log_store(nuraft::cs_new<KeeperLogStore>(\n-                    log_storage_path,\n-                    coordination_settings->rotate_log_storage_interval,\n-                    coordination_settings->force_sync,\n-                    coordination_settings->compress_logs))\n+          log_storage_path,\n+          coordination_settings->rotate_log_storage_interval,\n+          coordination_settings->force_sync,\n+          coordination_settings->compress_logs))\n {\n }\n \n",
  "test_patch": "diff --git a/tests/integration/test_keeper_incorrect_config/test.py b/tests/integration/test_keeper_incorrect_config/test.py\nindex 52c76c84e236..e0a28b00b4fe 100644\n--- a/tests/integration/test_keeper_incorrect_config/test.py\n+++ b/tests/integration/test_keeper_incorrect_config/test.py\n@@ -80,6 +80,73 @@ def started_cluster():\n </clickhouse>\n \"\"\"\n \n+LOCALHOST_WITH_REMOTE = \"\"\"\n+<clickhouse>\n+    <keeper_server>\n+        <tcp_port>9181</tcp_port>\n+        <server_id>1</server_id>\n+        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n+        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n+\n+        <coordination_settings>\n+            <operation_timeout_ms>5000</operation_timeout_ms>\n+            <session_timeout_ms>10000</session_timeout_ms>\n+            <raft_logs_level>trace</raft_logs_level>\n+        </coordination_settings>\n+\n+        <hostname_checks_enabled>true</hostname_checks_enabled>\n+        <raft_configuration>\n+            <server>\n+                <id>1</id>\n+                <hostname>localhost</hostname>\n+                <port>9234</port>\n+            </server>\n+            <server>\n+                <id>2</id>\n+                <hostname>127.0.0.2</hostname>\n+                <port>9234</port>\n+            </server>\n+        </raft_configuration>\n+    </keeper_server>\n+</clickhouse>\n+\"\"\"\n+\n+MULTIPLE_LOCAL_WITH_REMOTE = \"\"\"\n+<clickhouse>\n+    <keeper_server>\n+        <tcp_port>9181</tcp_port>\n+        <server_id>1</server_id>\n+        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\n+        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\n+\n+        <coordination_settings>\n+            <operation_timeout_ms>5000</operation_timeout_ms>\n+            <session_timeout_ms>10000</session_timeout_ms>\n+            <raft_logs_level>trace</raft_logs_level>\n+        </coordination_settings>\n+\n+        <hostname_checks_enabled>true</hostname_checks_enabled>\n+        <raft_configuration>\n+            <server>\n+                <id>1</id>\n+                <hostname>127.0.0.1</hostname>\n+                <port>9234</port>\n+            </server>\n+            <server>\n+                <id>2</id>\n+                <hostname>127.0.1.1</hostname>\n+                <port>9234</port>\n+            </server>\n+            <server>\n+                <id>3</id>\n+                <hostname>127.0.0.2</hostname>\n+                <port>9234</port>\n+            </server>\n+        </raft_configuration>\n+    </keeper_server>\n+</clickhouse>\n+\"\"\"\n+\n NORMAL_CONFIG = \"\"\"\n <clickhouse>\n     <keeper_server>\n@@ -108,18 +175,18 @@ def started_cluster():\n \n def test_duplicate_endpoint(started_cluster):\n     node1.stop_clickhouse()\n-    node1.replace_config(\n-        \"/etc/clickhouse-server/config.d/enable_keeper1.xml\", DUPLICATE_ENDPOINT_CONFIG\n-    )\n \n-    with pytest.raises(Exception):\n-        node1.start_clickhouse(start_wait_sec=10)\n-\n-    node1.replace_config(\n-        \"/etc/clickhouse-server/config.d/enable_keeper1.xml\", DUPLICATE_ID_CONFIG\n-    )\n-    with pytest.raises(Exception):\n-        node1.start_clickhouse(start_wait_sec=10)\n+    def assert_config_fails(config):\n+        node1.replace_config(\n+            \"/etc/clickhouse-server/config.d/enable_keeper1.xml\", config\n+        )\n+        with pytest.raises(Exception):\n+            node1.start_clickhouse(start_wait_sec=10)\n+\n+    assert_config_fails(DUPLICATE_ENDPOINT_CONFIG)\n+    assert_config_fails(DUPLICATE_ID_CONFIG)\n+    assert_config_fails(LOCALHOST_WITH_REMOTE)\n+    assert_config_fails(MULTIPLE_LOCAL_WITH_REMOTE)\n \n     node1.replace_config(\n         \"/etc/clickhouse-server/config.d/enable_keeper1.xml\", NORMAL_CONFIG\n",
  "problem_statement": "Cannot mix loopback and non-local hostnames in raft_configuration (ClickHouse-Keeper)\nHi!\r\n\r\nI heard on the last release meetup that ClickHouse-Keeper was ready for production and I wanted to try it out. However, I am running into some issues that I cannot really explain.\r\n\r\nMy staging setup is 3 VMs running Ubuntu 20.04 in Vagrant (Virtualbox). I am deploying ClickHouse-Keeper into the VMs as docker containers running the image `clickhouse/clickhouse-server:22.3.2.2`. The configuration looks like this.\r\n\r\n```xml\r\n<clickhouse>\r\n    <listen_host>0.0.0.0</listen_host>\r\n    <keeper_server>\r\n        <tcp_port>2181</tcp_port>\r\n        <server_id>1</server_id>\r\n\r\n        <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\r\n        <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\r\n\r\n        <coordination_settings>\r\n            <raft_logs_level>trace</raft_logs_level>\r\n        </coordination_settings>\r\n\r\n        <raft_configuration>\r\n            <server>\r\n                <id>1</id>\r\n                <hostname>dev-zoo01</hostname>\r\n                <port>9234</port>\r\n            </server>\r\n            <server>\r\n                <id>2</id>\r\n                <hostname>dev-zoo02</hostname>\r\n                <port>9234</port>\r\n            </server>\r\n            <server>\r\n                <id>3</id>\r\n                <hostname>dev-zoo03</hostname>\r\n                <port>9234</port>\r\n            </server>\r\n        </raft_configuration>\r\n    </keeper_server>\r\n</clickhouse>\r\n```\r\n\r\nThe docker container is running with `--network=host` on each of the 3 Ubuntu VMs.\r\n\r\nThis is the same setup that I had when running ClickHouse-Keeper in docker containers locally which worked fine without issue. \r\nHowever,  in the VMs I am running into this error immediately on startup. Additionally, I ran into the same error when testing on production VMs (same setup which worked with Apache Zookeeper).\r\n\r\n```\r\nProcessing configuration file '/etc/clickhouse-keeper/config.xml'.\r\nSending crash reports is disabled\r\nStarting ClickHouse Keeper 22.3.2.1 with revision 54460, build id: B537AFA18EAC3AF4, PID 1\r\nstarting up\r\nOS Name = Linux, OS Version = 5.4.0-65-generic, OS Architecture = x86_64\r\nDB::Exception: Mixing loopback and non-local hostnames ('dev-zoo01' and 'dev-zoo03') in raft_configuration is not allowed. Different hosts can resolve it to themselves so it's not allowed.\r\nshutting down\r\nStop SignalListener thread\r\n```\r\n\r\nThis error is very strange to me and not something that I have encountered before. It seems to be caused by the following line the `/etc/hosts` on the ubuntu VMs\r\n\r\n```\r\n127.0.1.1 dev-zoo01.company.com dev-zoo01\r\n```\r\n\r\nIf I remove that line from the `/etc/hosts` file it seems to work but I am not sure how that will affect the rest of the system. Is this expected behavior? What is the recommended solution to this?\n",
  "hints_text": "This works as expected in clickhouse/clickhouse-server:22.2.2.1 so I would like to reclassify this as a bug instead of a question.\nYeah, it's 100% bug. Actually all your hosts are local and you don't mix anything. I'll try to fix this and make this check opt-out.",
  "created_at": "2022-04-21T13:50:08Z",
  "modified_files": [
    "programs/keeper/keeper_config.xml",
    "src/Coordination/KeeperStateManager.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_keeper_incorrect_config/test.py"
  ]
}