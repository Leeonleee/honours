diff --git a/.github/workflows/master.yml b/.github/workflows/master.yml
index 5816a58081d8..737a61735a5a 100644
--- a/.github/workflows/master.yml
+++ b/.github/workflows/master.yml
@@ -911,6 +911,34 @@ jobs:
           docker rm -f "$(docker ps -a -q)" ||:
           sudo rm -fr "$TEMP_PATH" "$CACHES_PATH"
 ############################################################################################
+##################################### Docker images  #######################################
+############################################################################################
+  DockerServerImages:
+    needs:
+      - BuilderDebRelease
+      - BuilderDebAarch64
+    runs-on: [self-hosted, style-checker]
+    steps:
+      - name: Clear repository
+        run: |
+          sudo rm -fr "$GITHUB_WORKSPACE" && mkdir "$GITHUB_WORKSPACE"
+      - name: Check out repository code
+        uses: actions/checkout@v2
+        with:
+          fetch-depth: 0  # otherwise we will have no version info
+      - name: Check docker clickhouse/clickhouse-server building
+        run: |
+          cd "$GITHUB_WORKSPACE/tests/ci"
+          python3 docker_server.py --release-type head
+          python3 docker_server.py --release-type head --no-ubuntu \
+            --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper
+      - name: Cleanup
+        if: always()
+        run: |
+          docker kill "$(docker ps -q)" ||:
+          docker rm -f "$(docker ps -a -q)" ||:
+          sudo rm -fr "$TEMP_PATH"
+############################################################################################
 ##################################### BUILD REPORTER #######################################
 ############################################################################################
   BuilderReport:
diff --git a/.github/workflows/pull_request.yml b/.github/workflows/pull_request.yml
index 4a3880543c4d..6c55b80090a0 100644
--- a/.github/workflows/pull_request.yml
+++ b/.github/workflows/pull_request.yml
@@ -4,7 +4,7 @@ env:
   # Force the stdout and stderr streams to be unbuffered
   PYTHONUNBUFFERED: 1
 
-on: # yamllint disable-line rule:truthy
+on:  # yamllint disable-line rule:truthy
   pull_request:
     types:
       - synchronize
@@ -956,6 +956,34 @@ jobs:
           docker rm -f "$(docker ps -a -q)" ||:
           sudo rm -fr "$TEMP_PATH" "$CACHES_PATH"
 ############################################################################################
+##################################### Docker images  #######################################
+############################################################################################
+  DockerServerImages:
+    needs:
+      - BuilderDebRelease
+      - BuilderDebAarch64
+    runs-on: [self-hosted, style-checker]
+    steps:
+      - name: Clear repository
+        run: |
+          sudo rm -fr "$GITHUB_WORKSPACE" && mkdir "$GITHUB_WORKSPACE"
+      - name: Check out repository code
+        uses: actions/checkout@v2
+        with:
+          fetch-depth: 0  # otherwise we will have no version info
+      - name: Check docker clickhouse/clickhouse-server building
+        run: |
+          cd "$GITHUB_WORKSPACE/tests/ci"
+          python3 docker_server.py --release-type head --no-push
+          python3 docker_server.py --release-type head --no-push --no-ubuntu \
+            --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper
+      - name: Cleanup
+        if: always()
+        run: |
+          docker kill "$(docker ps -q)" ||:
+          docker rm -f "$(docker ps -a -q)" ||:
+          sudo rm -fr "$TEMP_PATH"
+############################################################################################
 ##################################### BUILD REPORTER #######################################
 ############################################################################################
   BuilderReport:
@@ -3016,6 +3044,7 @@ jobs:
     needs:
       - StyleCheck
       - DockerHubPush
+      - DockerServerImages
       - CheckLabels
       - BuilderReport
       - FastTest
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 0fe025080dd5..29e3d0c4358a 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@ -32,7 +32,32 @@ jobs:
       uses: svenstaro/upload-release-action@v2
       with:
         repo_token: ${{ secrets.GITHUB_TOKEN }}
-        file: ${{runner.temp}}/release_packages/*
+        file: ${{runner.temp}}/push_to_artifactory/*
         overwrite: true
         tag: ${{ github.ref }}
         file_glob: true
+  ############################################################################################
+  ##################################### Docker images  #######################################
+  ############################################################################################
+  DockerServerImages:
+    runs-on: [self-hosted, style-checker]
+    steps:
+    - name: Clear repository
+      run: |
+        sudo rm -fr "$GITHUB_WORKSPACE" && mkdir "$GITHUB_WORKSPACE"
+    - name: Check out repository code
+      uses: actions/checkout@v2
+      with:
+        fetch-depth: 0  # otherwise we will have no version info
+    - name: Check docker clickhouse/clickhouse-server building
+      run: |
+        cd "$GITHUB_WORKSPACE/tests/ci"
+        python3 docker_server.py --release-type auto
+        python3 docker_server.py --release-type auto --no-ubuntu \
+          --image-repo clickhouse/clickhouse-keeper --image-path docker/keeper
+    - name: Cleanup
+      if: always()
+      run: |
+        docker kill "$(docker ps -q)" ||:
+        docker rm -f "$(docker ps -a -q)" ||:
+        sudo rm -fr "$TEMP_PATH"
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 9649fc32d746..a9ce64b87ba7 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -261,13 +261,16 @@ endif ()
 # Add a section with the hash of the compiled machine code for integrity checks.
 # Only for official builds, because adding a section can be time consuming (rewrite of several GB).
 # And cross compiled binaries are not supported (since you cannot execute clickhouse hash-binary)
-if (OBJCOPY_PATH AND YANDEX_OFFICIAL_BUILD AND (NOT CMAKE_TOOLCHAIN_FILE))
-    set (USE_BINARY_HASH 1)
+if (OBJCOPY_PATH AND CLICKHOUSE_OFFICIAL_BUILD AND (NOT CMAKE_TOOLCHAIN_FILE OR CMAKE_TOOLCHAIN_FILE MATCHES "linux/toolchain-x86_64.cmake$"))
+    set (USE_BINARY_HASH 1 CACHE STRING "Calculate binary hash and store it in the separate section")
 endif ()
 
 # Allows to build stripped binary in a separate directory
-if (OBJCOPY_PATH AND READELF_PATH)
-    set(BUILD_STRIPPED_BINARIES_PREFIX "" CACHE STRING "Build stripped binaries with debug info in separate directory")
+if (OBJCOPY_PATH AND STRIP_PATH)
+    option(INSTALL_STRIPPED_BINARIES "Build stripped binaries with debug info in separate directory" OFF)
+    if (INSTALL_STRIPPED_BINARIES)
+        set(STRIPPED_BINARIES_OUTPUT "stripped" CACHE STRING "A separate directory for stripped information")
+    endif()
 endif()
 
 cmake_host_system_information(RESULT AVAILABLE_PHYSICAL_MEMORY QUERY AVAILABLE_PHYSICAL_MEMORY) # Not available under freebsd
diff --git a/base/glibc-compatibility/CMakeLists.txt b/base/glibc-compatibility/CMakeLists.txt
index ddec09121e1f..ef7ec6d7fc03 100644
--- a/base/glibc-compatibility/CMakeLists.txt
+++ b/base/glibc-compatibility/CMakeLists.txt
@@ -51,6 +51,6 @@ if (GLIBC_COMPATIBILITY)
 
     message (STATUS "Some symbols from glibc will be replaced for compatibility")
 
-elseif (YANDEX_OFFICIAL_BUILD)
+elseif (CLICKHOUSE_OFFICIAL_BUILD)
     message (WARNING "Option GLIBC_COMPATIBILITY must be turned on for production builds.")
 endif ()
diff --git a/benchmark/greenplum/result_parser.py b/benchmark/greenplum/result_parser.py
index 8af20d265a02..4ed1aa5c4a56 100755
--- a/benchmark/greenplum/result_parser.py
+++ b/benchmark/greenplum/result_parser.py
@@ -4,11 +4,12 @@
 import sys
 import json
 
+
 def parse_block(block=[], options=[]):
 
-    #print('block is here', block)
-    #show_query = False
-    #show_query = options.show_query
+    # print('block is here', block)
+    # show_query = False
+    # show_query = options.show_query
     result = []
     query = block[0].strip()
     if len(block) > 4:
@@ -20,9 +21,9 @@ def parse_block(block=[], options=[]):
         timing2 = block[2].strip().split()[1]
         timing3 = block[3].strip().split()[1]
     if options.show_queries:
-        result.append( query )
+        result.append(query)
     if not options.show_first_timings:
-        result += [ timing1 , timing2, timing3 ]
+        result += [timing1, timing2, timing3]
     else:
         result.append(timing1)
     return result
@@ -37,12 +38,12 @@ def read_stats_file(options, fname):
 
         for line in f.readlines():
 
-            if 'SELECT' in line:
+            if "SELECT" in line:
                 if len(block) > 1:
-                    result.append( parse_block(block, options) )
-                block = [ line ]
-            elif 'Time:' in line:
-                block.append( line )
+                    result.append(parse_block(block, options))
+                block = [line]
+            elif "Time:" in line:
+                block.append(line)
 
     return result
 
@@ -50,7 +51,7 @@ def read_stats_file(options, fname):
 def compare_stats_files(options, arguments):
     result = []
     file_output = []
-    pyplot_colors = ['y', 'b', 'g', 'r']
+    pyplot_colors = ["y", "b", "g", "r"]
     for fname in arguments[1:]:
         file_output.append((read_stats_file(options, fname)))
     if len(file_output[0]) > 0:
@@ -58,65 +59,92 @@ def compare_stats_files(options, arguments):
     for idx, data_set in enumerate(file_output):
         int_result = []
         for timing in data_set:
-           int_result.append(float(timing[0])) #y values
-        result.append([[x for x in range(0, len(int_result)) ], int_result,
-pyplot_colors[idx] + '^' ] )
-#        result.append([x for x in range(1, len(int_result)) ]) #x values
-#        result.append( pyplot_colors[idx] + '^' )
+            int_result.append(float(timing[0]))  # y values
+        result.append(
+            [
+                [x for x in range(0, len(int_result))],
+                int_result,
+                pyplot_colors[idx] + "^",
+            ]
+        )
+    #        result.append([x for x in range(1, len(int_result)) ]) #x values
+    #        result.append( pyplot_colors[idx] + '^' )
 
     return result
 
+
 def parse_args():
     from optparse import OptionParser
-    parser = OptionParser(usage='usage: %prog [options] [result_file_path]..')
-    parser.add_option("-q", "--show-queries", help="Show statements along with timings", action="store_true", dest="show_queries")
-    parser.add_option("-f", "--show-first-timings", help="Show only first tries timings", action="store_true", dest="show_first_timings")
-    parser.add_option("-c", "--compare-mode", help="Prepare output for pyplot comparing result files.", action="store", dest="compare_mode")
+
+    parser = OptionParser(usage="usage: %prog [options] [result_file_path]..")
+    parser.add_option(
+        "-q",
+        "--show-queries",
+        help="Show statements along with timings",
+        action="store_true",
+        dest="show_queries",
+    )
+    parser.add_option(
+        "-f",
+        "--show-first-timings",
+        help="Show only first tries timings",
+        action="store_true",
+        dest="show_first_timings",
+    )
+    parser.add_option(
+        "-c",
+        "--compare-mode",
+        help="Prepare output for pyplot comparing result files.",
+        action="store",
+        dest="compare_mode",
+    )
     (options, arguments) = parser.parse_args(sys.argv)
     if len(arguments) < 2:
         parser.print_usage()
         sys.exit(1)
-    return ( options, arguments )
+    return (options, arguments)
+
 
 def gen_pyplot_code(options, arguments):
-    result = ''
+    result = ""
     data_sets = compare_stats_files(options, arguments)
     for idx, data_set in enumerate(data_sets, start=0):
         x_values, y_values, line_style = data_set
-        result += '
plt.plot('
-        result += '%s, %s, \'%s\'' % ( x_values, y_values, line_style )
-        result += ', label=\'%s try\')' % idx
-    print('import matplotlib.pyplot as plt')
+        result += "
plt.plot("
+        result += "%s, %s, '%s'" % (x_values, y_values, line_style)
+        result += ", label='%s try')" % idx
+    print("import matplotlib.pyplot as plt")
     print(result)
-    print( 'plt.xlabel(\'Try number\')' )
-    print( 'plt.ylabel(\'Timing\')' )
-    print( 'plt.title(\'Benchmark query timings\')' )
-    print('plt.legend()')
-    print('plt.show()')
+    print("plt.xlabel('Try number')")
+    print("plt.ylabel('Timing')")
+    print("plt.title('Benchmark query timings')")
+    print("plt.legend()")
+    print("plt.show()")
 
 
 def gen_html_json(options, arguments):
     tuples = read_stats_file(options, arguments[1])
-    print('{')
+    print("{")
     print('"system:       GreenPlum(x2),')
-    print(('"version":      "%s",' % '4.3.9.1'))
+    print(('"version":      "%s",' % "4.3.9.1"))
     print('"data_size":    10000000,')
     print('"time":         "",')
     print('"comments":     "",')
     print('"result":')
-    print('[')
+    print("[")
     for s in tuples:
         print(s)
-    print(']')
-    print('}')
+    print("]")
+    print("}")
 
 
 def main():
-    ( options, arguments ) = parse_args()
+    (options, arguments) = parse_args()
     if len(arguments) > 2:
         gen_pyplot_code(options, arguments)
     else:
         gen_html_json(options, arguments)
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     main()
diff --git a/cmake/strip.sh b/cmake/strip.sh
deleted file mode 100755
index de5968871593..000000000000
--- a/cmake/strip.sh
+++ /dev/null
@@ -1,25 +0,0 @@
-#!/usr/bin/env bash
-
-BINARY_PATH=$1
-BINARY_NAME=$(basename $BINARY_PATH)
-DESTINATION_STRIPPED_DIR=$2
-OBJCOPY_PATH=${3:objcopy}
-READELF_PATH=${4:readelf}
-
-BUILD_ID=$($READELF_PATH -n $1 | sed -n '/Build ID/ { s/.*: //p; q; }')
-BUILD_ID_PREFIX=${BUILD_ID:0:2}
-BUILD_ID_SUFFIX=${BUILD_ID:2}
-TEMP_BINARY_PATH="${BINARY_PATH}_temp"
-
-DESTINATION_DEBUG_INFO_DIR="$DESTINATION_STRIPPED_DIR/lib/debug/.build-id"
-DESTINATION_STRIP_BINARY_DIR="$DESTINATION_STRIPPED_DIR/bin"
-
-mkdir -p "$DESTINATION_DEBUG_INFO_DIR/$BUILD_ID_PREFIX"
-mkdir -p "$DESTINATION_STRIP_BINARY_DIR"
-
-$OBJCOPY_PATH --only-keep-debug "$BINARY_PATH" "$DESTINATION_DEBUG_INFO_DIR/$BUILD_ID_PREFIX/$BUILD_ID_SUFFIX.debug"
-
-touch "$TEMP_BINARY_PATH"
-$OBJCOPY_PATH --add-gnu-debuglink "$DESTINATION_DEBUG_INFO_DIR/$BUILD_ID_PREFIX/$BUILD_ID_SUFFIX.debug" "$BINARY_PATH" "$TEMP_BINARY_PATH"
-$OBJCOPY_PATH --strip-all "$TEMP_BINARY_PATH" "$DESTINATION_STRIP_BINARY_DIR/$BINARY_NAME"
-rm -f "$TEMP_BINARY_PATH"
diff --git a/cmake/strip_binary.cmake b/cmake/strip_binary.cmake
index e430807772dd..1f24790a1590 100644
--- a/cmake/strip_binary.cmake
+++ b/cmake/strip_binary.cmake
@@ -11,16 +11,43 @@ macro(clickhouse_strip_binary)
        message(FATAL_ERROR "A binary path name must be provided for stripping binary")
    endif()
 
-
    if (NOT DEFINED STRIP_DESTINATION_DIR)
        message(FATAL_ERROR "Destination directory for stripped binary must be provided")
    endif()
 
    add_custom_command(TARGET ${STRIP_TARGET} POST_BUILD
-     COMMAND bash ${ClickHouse_SOURCE_DIR}/cmake/strip.sh ${STRIP_BINARY_PATH} ${STRIP_DESTINATION_DIR} ${OBJCOPY_PATH} ${READELF_PATH}
-     COMMENT "Stripping clickhouse binary" VERBATIM
+       COMMAND mkdir -p "${STRIP_DESTINATION_DIR}/lib/debug/bin"
+       COMMAND mkdir -p "${STRIP_DESTINATION_DIR}/bin"
+       COMMAND cp "${STRIP_BINARY_PATH}" "${STRIP_DESTINATION_DIR}/bin/${STRIP_TARGET}"
+       COMMAND "${OBJCOPY_PATH}" --only-keep-debug --compress-debug-sections "${STRIP_DESTINATION_DIR}/bin/${STRIP_TARGET}" "${STRIP_DESTINATION_DIR}/lib/debug/bin/${STRIP_TARGET}.debug"
+       COMMAND chmod 0644 "${STRIP_DESTINATION_DIR}/lib/debug/bin/${STRIP_TARGET}.debug"
+       COMMAND "${STRIP_PATH}" --remove-section=.comment --remove-section=.note "${STRIP_DESTINATION_DIR}/bin/${STRIP_TARGET}"
+       COMMAND "${OBJCOPY_PATH}" --add-gnu-debuglink "${STRIP_DESTINATION_DIR}/lib/debug/bin/${STRIP_TARGET}.debug" "${STRIP_DESTINATION_DIR}/bin/${STRIP_TARGET}"
+       COMMENT "Stripping clickhouse binary" VERBATIM
    )
 
    install(PROGRAMS ${STRIP_DESTINATION_DIR}/bin/${STRIP_TARGET} DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT clickhouse)
-   install(DIRECTORY ${STRIP_DESTINATION_DIR}/lib/debug DESTINATION ${CMAKE_INSTALL_LIBDIR} COMPONENT clickhouse)
+   install(FILES ${STRIP_DESTINATION_DIR}/lib/debug/bin/${STRIP_TARGET}.debug DESTINATION ${CMAKE_INSTALL_LIBDIR}/debug/${CMAKE_INSTALL_FULL_BINDIR}/${STRIP_TARGET}.debug COMPONENT clickhouse)
+endmacro()
+
+
+macro(clickhouse_make_empty_debug_info_for_nfpm)
+   set(oneValueArgs TARGET DESTINATION_DIR)
+   cmake_parse_arguments(EMPTY_DEBUG "" "${oneValueArgs}" "" ${ARGN})
+
+   if (NOT DEFINED EMPTY_DEBUG_TARGET)
+       message(FATAL_ERROR "A target name must be provided for stripping binary")
+   endif()
+
+   if (NOT DEFINED EMPTY_DEBUG_DESTINATION_DIR)
+       message(FATAL_ERROR "Destination directory for empty debug must be provided")
+   endif()
+
+   add_custom_command(TARGET ${EMPTY_DEBUG_TARGET} POST_BUILD
+       COMMAND mkdir -p "${EMPTY_DEBUG_DESTINATION_DIR}/lib/debug"
+       COMMAND touch "${EMPTY_DEBUG_DESTINATION_DIR}/lib/debug/${EMPTY_DEBUG_TARGET}.debug"
+       COMMENT "Addiding empty debug info for NFPM" VERBATIM
+   )
+
+   install(FILES "${EMPTY_DEBUG_DESTINATION_DIR}/lib/debug/${EMPTY_DEBUG_TARGET}.debug" DESTINATION "${CMAKE_INSTALL_LIBDIR}/debug/${CMAKE_INSTALL_FULL_BINDIR}" COMPONENT clickhouse)
 endmacro()
diff --git a/cmake/tools.cmake b/cmake/tools.cmake
index d6fddd0509e8..d571a46ad26d 100644
--- a/cmake/tools.cmake
+++ b/cmake/tools.cmake
@@ -170,32 +170,32 @@ else ()
     message (FATAL_ERROR "Cannot find objcopy.")
 endif ()
 
-# Readelf (FIXME copypaste)
+# Strip (FIXME copypaste)
 
 if (COMPILER_GCC)
-    find_program (READELF_PATH NAMES "llvm-readelf" "llvm-readelf-13" "llvm-readelf-12" "llvm-readelf-11" "readelf")
+    find_program (STRIP_PATH NAMES "llvm-strip" "llvm-strip-13" "llvm-strip-12" "llvm-strip-11" "strip")
 else ()
-    find_program (READELF_PATH NAMES "llvm-readelf-${COMPILER_VERSION_MAJOR}" "llvm-readelf" "readelf")
+    find_program (STRIP_PATH NAMES "llvm-strip-${COMPILER_VERSION_MAJOR}" "llvm-strip" "strip")
 endif ()
 
-if (NOT READELF_PATH AND OS_DARWIN)
+if (NOT STRIP_PATH AND OS_DARWIN)
     find_program (BREW_PATH NAMES "brew")
     if (BREW_PATH)
         execute_process (COMMAND ${BREW_PATH} --prefix llvm ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE OUTPUT_VARIABLE LLVM_PREFIX)
         if (LLVM_PREFIX)
-            find_program (READELF_PATH NAMES "llvm-readelf" PATHS "${LLVM_PREFIX}/bin" NO_DEFAULT_PATH)
+            find_program (STRIP_PATH NAMES "llvm-strip" PATHS "${LLVM_PREFIX}/bin" NO_DEFAULT_PATH)
         endif ()
-        if (NOT READELF_PATH)
+        if (NOT STRIP_PATH)
             execute_process (COMMAND ${BREW_PATH} --prefix binutils ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE OUTPUT_VARIABLE BINUTILS_PREFIX)
             if (BINUTILS_PREFIX)
-                find_program (READELF_PATH NAMES "readelf" PATHS "${BINUTILS_PREFIX}/bin" NO_DEFAULT_PATH)
+                find_program (STRIP_PATH NAMES "strip" PATHS "${BINUTILS_PREFIX}/bin" NO_DEFAULT_PATH)
             endif ()
         endif ()
     endif ()
 endif ()
 
-if (READELF_PATH)
-    message (STATUS "Using readelf: ${READELF_PATH}")
+if (STRIP_PATH)
+    message (STATUS "Using strip: ${STRIP_PATH}")
 else ()
-    message (FATAL_ERROR "Cannot find readelf.")
+    message (FATAL_ERROR "Cannot find strip.")
 endif ()
diff --git a/cmake/version.cmake b/cmake/version.cmake
index 963f291c0f30..acaa772ff2ff 100644
--- a/cmake/version.cmake
+++ b/cmake/version.cmake
@@ -18,6 +18,6 @@ set (VERSION_STRING_SHORT "${VERSION_MAJOR}.${VERSION_MINOR}")
 
 math (EXPR VERSION_INTEGER "${VERSION_PATCH} + ${VERSION_MINOR}*1000 + ${VERSION_MAJOR}*1000000")
 
-if(YANDEX_OFFICIAL_BUILD)
+if(CLICKHOUSE_OFFICIAL_BUILD)
     set(VERSION_OFFICIAL " (official build)")
 endif()
diff --git a/docker/keeper/Dockerfile b/docker/keeper/Dockerfile
new file mode 100644
index 000000000000..207dddce1bbe
--- /dev/null
+++ b/docker/keeper/Dockerfile
@@ -0,0 +1,72 @@
+FROM ubuntu:20.04 AS glibc-donor
+
+ARG TARGETARCH
+RUN arch=${TARGETARCH:-amd64} \
+    && case $arch in \
+        amd64) rarch=x86_64 ;; \
+        arm64) rarch=aarch64 ;; \
+    esac \
+    && ln -s "${rarch}-linux-gnu" /lib/linux-gnu
+
+
+FROM alpine
+
+ENV LANG=en_US.UTF-8 \
+    LANGUAGE=en_US:en \
+    LC_ALL=en_US.UTF-8 \
+    TZ=UTC \
+    CLICKHOUSE_CONFIG=/etc/clickhouse-server/config.xml
+
+COPY --from=glibc-donor /lib/linux-gnu/libc.so.6 /lib/linux-gnu/libdl.so.2 /lib/linux-gnu/libm.so.6 /lib/linux-gnu/libpthread.so.0 /lib/linux-gnu/librt.so.1 /lib/linux-gnu/libnss_dns.so.2 /lib/linux-gnu/libnss_files.so.2 /lib/linux-gnu/libresolv.so.2 /lib/linux-gnu/ld-2.31.so /lib/
+COPY --from=glibc-donor /etc/nsswitch.conf /etc/
+COPY entrypoint.sh /entrypoint.sh
+RUN arch=${TARGETARCH:-amd64} \
+    && case $arch in \
+        amd64) mkdir -p /lib64 && ln -sf /lib/ld-2.31.so /lib64/ld-linux-x86-64.so.2 ;; \
+        arm64) ln -sf /lib/ld-2.31.so /lib/ld-linux-aarch64.so.1 ;; \
+    esac
+
+ARG REPOSITORY="https://s3.amazonaws.com/clickhouse-builds/22.4/31c367d3cd3aefd316778601ff6565119fe36682/package_release"
+ARG VERSION="22.4.1.917"
+ARG PACKAGES="clickhouse-keeper"
+
+# user/group precreated explicitly with fixed uid/gid on purpose.
+# It is especially important for rootless containers: in that case entrypoint
+# can't do chown and owners of mounted volumes should be configured externally.
+# We do that in advance at the begining of Dockerfile before any packages will be
+# installed to prevent picking those uid / gid by some unrelated software.
+# The same uid / gid (101) is used both for alpine and ubuntu.
+
+
+ARG TARGETARCH
+RUN arch=${TARGETARCH:-amd64} \
+    && for package in ${PACKAGES}; do \
+        { \
+            { echo "Get ${REPOSITORY}/${package}-${VERSION}-${arch}.tgz" \
+                && wget -c -q "${REPOSITORY}/${package}-${VERSION}-${arch}.tgz" -O "/tmp/${package}-${VERSION}-${arch}.tgz" \
+                && tar xvzf "/tmp/${package}-${VERSION}-${arch}.tgz" --strip-components=1 -C / ; \
+            } || \
+            { echo "Fallback to ${REPOSITORY}/${package}-${VERSION}.tgz" \
+                && wget -c -q "${REPOSITORY}/${package}-${VERSION}.tgz" -O "/tmp/${package}-${VERSION}.tgz" \
+                && tar xvzf "/tmp/${package}-${VERSION}.tgz" --strip-components=2 -C / ; \
+            } ; \
+        } || exit 1 \
+    ; done \
+    && rm /tmp/*.tgz /install -r \
+    && addgroup -S -g 101 clickhouse \
+    && adduser -S -h /var/lib/clickhouse -s /bin/bash -G clickhouse -g "ClickHouse keeper" -u 101 clickhouse \
+    && mkdir -p /var/lib/clickhouse /var/log/clickhouse-keeper /etc/clickhouse-keeper \
+    && chown clickhouse:clickhouse /var/lib/clickhouse \
+    && chown root:clickhouse /var/log/clickhouse-keeper \
+    && chmod +x /entrypoint.sh \
+    && apk add --no-cache su-exec bash tzdata \
+    && cp /usr/share/zoneinfo/UTC /etc/localtime \
+    && echo "UTC" > /etc/timezone \
+    && chmod ugo+Xrw -R /var/lib/clickhouse /var/log/clickhouse-keeper /etc/clickhouse-keeper
+
+
+EXPOSE 2181 10181 44444
+
+VOLUME /var/lib/clickhouse /var/log/clickhouse-keeper /etc/clickhouse-keeper
+
+ENTRYPOINT ["/entrypoint.sh"]
diff --git a/docker/keeper/Dockerfile.alpine b/docker/keeper/Dockerfile.alpine
new file mode 120000
index 000000000000..1d1fe94df492
--- /dev/null
+++ b/docker/keeper/Dockerfile.alpine
@@ -0,0 +1,1 @@
+Dockerfile
\ No newline at end of file
diff --git a/docker/keeper/entrypoint.sh b/docker/keeper/entrypoint.sh
new file mode 100644
index 000000000000..3aacf655c281
--- /dev/null
+++ b/docker/keeper/entrypoint.sh
@@ -0,0 +1,93 @@
+#!/bin/bash
+
+set +x
+set -eo pipefail
+shopt -s nullglob
+
+DO_CHOWN=1
+if [ "${CLICKHOUSE_DO_NOT_CHOWN:-0}" = "1" ]; then
+    DO_CHOWN=0
+fi
+
+CLICKHOUSE_UID="${CLICKHOUSE_UID:-"$(id -u clickhouse)"}"
+CLICKHOUSE_GID="${CLICKHOUSE_GID:-"$(id -g clickhouse)"}"
+
+# support --user
+if [ "$(id -u)" = "0" ]; then
+    USER=$CLICKHOUSE_UID
+    GROUP=$CLICKHOUSE_GID
+    if command -v gosu &> /dev/null; then
+        gosu="gosu $USER:$GROUP"
+    elif command -v su-exec &> /dev/null; then
+        gosu="su-exec $USER:$GROUP"
+    else
+        echo "No gosu/su-exec detected!"
+        exit 1
+    fi
+else
+    USER="$(id -u)"
+    GROUP="$(id -g)"
+    gosu=""
+    DO_CHOWN=0
+fi
+
+KEEPER_CONFIG="${KEEPER_CONFIG:-/etc/clickhouse-keeper/config.yaml}"
+
+if [ -f "$KEEPER_CONFIG" ] && ! $gosu test -f "$KEEPER_CONFIG" -a -r "$KEEPER_CONFIG"; then
+    echo "Configuration file '$KEEPER_CONFIG' isn't readable by user with id '$USER'"
+    exit 1
+fi
+
+DATA_DIR="${CLICKHOUSE_DATA_DIR:-/var/lib/clickhouse}"
+LOG_DIR="${LOG_DIR:-/var/log/clickhouse-keeper}"
+LOG_PATH="${LOG_DIR}/clickhouse-keeper.log"
+ERROR_LOG_PATH="${LOG_DIR}/clickhouse-keeper.err.log"
+COORDINATION_LOG_DIR="${DATA_DIR}/coordination/log"
+COORDINATION_SNAPSHOT_DIR="${DATA_DIR}/coordination/snapshots"
+CLICKHOUSE_WATCHDOG_ENABLE=${CLICKHOUSE_WATCHDOG_ENABLE:-0}
+
+for dir in "$DATA_DIR" \
+  "$LOG_DIR" \
+  "$TMP_DIR" \
+  "$COORDINATION_LOG_DIR" \
+  "$COORDINATION_SNAPSHOT_DIR"
+do
+    # check if variable not empty
+    [ -z "$dir" ] && continue
+    # ensure directories exist
+    if ! mkdir -p "$dir"; then
+        echo "Couldn't create necessary directory: $dir"
+        exit 1
+    fi
+
+    if [ "$DO_CHOWN" = "1" ]; then
+        # ensure proper directories permissions
+        # but skip it for if directory already has proper premissions, cause recursive chown may be slow
+        if [ "$(stat -c %u "$dir")" != "$USER" ] || [ "$(stat -c %g "$dir")" != "$GROUP" ]; then
+            chown -R "$USER:$GROUP" "$dir"
+        fi
+    elif ! $gosu test -d "$dir" -a -w "$dir" -a -r "$dir"; then
+        echo "Necessary directory '$dir' isn't accessible by user with id '$USER'"
+        exit 1
+    fi
+done
+
+# if no args passed to `docker run` or first argument start with `--`, then the user is passing clickhouse-server arguments
+if [[ $# -lt 1 ]] || [[ "$1" == "--"* ]]; then
+    # Watchdog is launched by default, but does not send SIGINT to the main process,
+    # so the container can't be finished by ctrl+c
+    export CLICKHOUSE_WATCHDOG_ENABLE
+
+    cd /var/lib/clickhouse
+
+    # There is a config file. It is already tested with gosu (if it is readably by keeper user)
+    if [ -f "$KEEPER_CONFIG" ]; then
+        exec $gosu /usr/bin/clickhouse-keeper --config-file="$KEEPER_CONFIG" --log-file="$LOG_PATH" --errorlog-file="$ERROR_LOG_PATH" "$@"
+    fi
+
+    # There is no config file. Will use embedded one
+    exec $gosu /usr/bin/clickhouse-keeper --log-file="$LOG_PATH" --errorlog-file="$ERROR_LOG_PATH" "$@"
+fi
+
+# Otherwise, we assume the user want to run his own process, for example a `bash` shell to explore this image
+exec "$@"
diff --git a/docker/packager/binary/Dockerfile b/docker/packager/binary/Dockerfile
index e3e2e689b175..a57a734e3dff 100644
--- a/docker/packager/binary/Dockerfile
+++ b/docker/packager/binary/Dockerfile
@@ -95,6 +95,14 @@ RUN add-apt-repository ppa:ubuntu-toolchain-r/test --yes \
     && apt-get install gcc-11 g++-11 --yes \
     && apt-get clean
 
+# Architecture of the image when BuildKit/buildx is used
+ARG TARGETARCH
+ARG NFPM_VERSION=2.15.0
+
+RUN arch=${TARGETARCH:-amd64} \
+  && curl -Lo /tmp/nfpm.deb "https://github.com/goreleaser/nfpm/releases/download/v${NFPM_VERSION}/nfpm_${arch}.deb" \
+  && dpkg -i /tmp/nfpm.deb \
+  && rm /tmp/nfpm.deb
 
 COPY build.sh /
-CMD ["bash", "-c", "/build.sh 2>&1 | ts"]
+CMD ["bash", "-c", "/build.sh 2>&1"]
diff --git a/docker/packager/binary/build.sh b/docker/packager/binary/build.sh
index 2f18b07ffe19..31416e1a0ee7 100755
--- a/docker/packager/binary/build.sh
+++ b/docker/packager/binary/build.sh
@@ -1,7 +1,13 @@
 #!/usr/bin/env bash
 
+exec &> >(ts)
 set -x -e
 
+cache_status () {
+    ccache --show-config ||:
+    ccache --show-stats ||:
+}
+
 mkdir -p build/cmake/toolchain/darwin-x86_64
 tar xJf MacOSX11.0.sdk.tar.xz -C build/cmake/toolchain/darwin-x86_64 --strip-components=1
 ln -sf darwin-x86_64 build/cmake/toolchain/darwin-aarch64
@@ -19,15 +25,23 @@ read -ra CMAKE_FLAGS <<< "${CMAKE_FLAGS:-}"
 env
 cmake --debug-trycompile --verbose=1 -DCMAKE_VERBOSE_MAKEFILE=1 -LA "-DCMAKE_BUILD_TYPE=$BUILD_TYPE" "-DSANITIZE=$SANITIZER" -DENABLE_CHECK_HEAVY_BUILDS=1 "${CMAKE_FLAGS[@]}" ..
 
-ccache --show-config ||:
-ccache --show-stats ||:
+cache_status
+# clear cache stats
 ccache --zero-stats ||:
 
-# shellcheck disable=SC2086 # No quotes because I want it to expand to nothing if empty.
+# No quotes because I want it to expand to nothing if empty.
+# shellcheck disable=SC2086
 ninja $NINJA_FLAGS clickhouse-bundle
 
-ccache --show-config ||:
-ccache --show-stats ||:
+cache_status
+
+if [ -n "$MAKE_DEB" ]; then
+  rm -rf /build/packages/root
+  # No quotes because I want it to expand to nothing if empty.
+  # shellcheck disable=SC2086
+  DESTDIR=/build/packages/root ninja $NINJA_FLAGS install
+  bash -x /build/packages/build
+fi
 
 mv ./programs/clickhouse* /output
 mv ./src/unit_tests_dbms /output ||: # may not exist for some binary builds
@@ -84,8 +98,7 @@ fi
 #   ../docker/packager/other/fuzzer.sh
 # fi
 
-ccache --show-config ||:
-ccache --show-stats ||:
+cache_status
 
 if [ "${CCACHE_DEBUG:-}" == "1" ]
 then
diff --git a/docker/packager/packager b/docker/packager/packager
index 05b2e02df969..f82d402d613a 100755
--- a/docker/packager/packager
+++ b/docker/packager/packager
@@ -1,5 +1,5 @@
 #!/usr/bin/env python3
-#-*- coding: utf-8 -*-
+# -*- coding: utf-8 -*-
 import subprocess
 import os
 import argparse
@@ -8,36 +8,39 @@ import sys
 
 SCRIPT_PATH = os.path.realpath(__file__)
 
-IMAGE_MAP = {
-    "deb": "clickhouse/deb-builder",
-    "binary": "clickhouse/binary-builder",
-}
 
 def check_image_exists_locally(image_name):
     try:
-        output = subprocess.check_output("docker images -q {} 2> /dev/null".format(image_name), shell=True)
+        output = subprocess.check_output(
+            f"docker images -q {image_name} 2> /dev/null", shell=True
+        )
         return output != ""
-    except subprocess.CalledProcessError as ex:
+    except subprocess.CalledProcessError:
         return False
 
+
 def pull_image(image_name):
     try:
-        subprocess.check_call("docker pull {}".format(image_name), shell=True)
+        subprocess.check_call(f"docker pull {image_name}", shell=True)
         return True
-    except subprocess.CalledProcessError as ex:
-        logging.info("Cannot pull image {}".format(image_name))
+    except subprocess.CalledProcessError:
+        logging.info(f"Cannot pull image {image_name}".format())
         return False
 
+
 def build_image(image_name, filepath):
     context = os.path.dirname(filepath)
-    build_cmd = "docker build --network=host -t {} -f {} {}".format(image_name, filepath, context)
-    logging.info("Will build image with cmd: '{}'".format(build_cmd))
+    build_cmd = f"docker build --network=host -t {image_name} -f {filepath} {context}"
+    logging.info("Will build image with cmd: '%s'", build_cmd)
     subprocess.check_call(
         build_cmd,
         shell=True,
     )
 
-def run_docker_image_with_env(image_name, output, env_variables, ch_root, ccache_dir, docker_image_version):
+
+def run_docker_image_with_env(
+    image_name, output, env_variables, ch_root, ccache_dir, docker_image_version
+):
     env_part = " -e ".join(env_variables)
     if env_part:
         env_part = " -e " + env_part
@@ -47,28 +50,52 @@ def run_docker_image_with_env(image_name, output, env_variables, ch_root, ccache
     else:
         interactive = ""
 
-    cmd = "docker run --network=host --rm --volume={output_path}:/output --volume={ch_root}:/build --volume={ccache_dir}:/ccache {env} {interactive} {img_name}".format(
-        output_path=output,
-        ch_root=ch_root,
-        ccache_dir=ccache_dir,
-        env=env_part,
-        img_name=image_name + ":" + docker_image_version,
-        interactive=interactive
+    cmd = (
+        f"docker run --network=host --rm --volume={output}:/output "
+        f"--volume={ch_root}:/build --volume={ccache_dir}:/ccache {env_part} "
+        f"{interactive} {image_name}:{docker_image_version}"
     )
 
-    logging.info("Will build ClickHouse pkg with cmd: '{}'".format(cmd))
+    logging.info("Will build ClickHouse pkg with cmd: '%s'", cmd)
 
     subprocess.check_call(cmd, shell=True)
 
-def parse_env_variables(build_type, compiler, sanitizer, package_type, image_type, cache, distcc_hosts, split_binary, clang_tidy, version, author, official, alien_pkgs, with_coverage, with_binaries):
+
+def is_release_build(build_type, package_type, sanitizer, split_binary):
+    return (
+        build_type == ""
+        and package_type == "deb"
+        and sanitizer == ""
+        and not split_binary
+    )
+
+
+def parse_env_variables(
+    build_type,
+    compiler,
+    sanitizer,
+    package_type,
+    image_type,
+    cache,
+    distcc_hosts,
+    split_binary,
+    clang_tidy,
+    version,
+    author,
+    official,
+    additional_pkgs,
+    with_coverage,
+    with_binaries,
+):
     DARWIN_SUFFIX = "-darwin"
     DARWIN_ARM_SUFFIX = "-darwin-aarch64"
     ARM_SUFFIX = "-aarch64"
     FREEBSD_SUFFIX = "-freebsd"
-    PPC_SUFFIX = '-ppc64le'
+    PPC_SUFFIX = "-ppc64le"
 
     result = []
-    cmake_flags = ['$CMAKE_FLAGS']
+    result.append("OUTPUT_DIR=/output")
+    cmake_flags = ["$CMAKE_FLAGS"]
 
     is_cross_darwin = compiler.endswith(DARWIN_SUFFIX)
     is_cross_darwin_arm = compiler.endswith(DARWIN_ARM_SUFFIX)
@@ -77,46 +104,73 @@ def parse_env_variables(build_type, compiler, sanitizer, package_type, image_typ
     is_cross_freebsd = compiler.endswith(FREEBSD_SUFFIX)
 
     if is_cross_darwin:
-        cc = compiler[:-len(DARWIN_SUFFIX)]
+        cc = compiler[: -len(DARWIN_SUFFIX)]
         cmake_flags.append("-DCMAKE_AR:FILEPATH=/cctools/bin/x86_64-apple-darwin-ar")
-        cmake_flags.append("-DCMAKE_INSTALL_NAME_TOOL=/cctools/bin/x86_64-apple-darwin-install_name_tool")
-        cmake_flags.append("-DCMAKE_RANLIB:FILEPATH=/cctools/bin/x86_64-apple-darwin-ranlib")
+        cmake_flags.append(
+            "-DCMAKE_INSTALL_NAME_TOOL=/cctools/bin/"
+            "x86_64-apple-darwin-install_name_tool"
+        )
+        cmake_flags.append(
+            "-DCMAKE_RANLIB:FILEPATH=/cctools/bin/x86_64-apple-darwin-ranlib"
+        )
         cmake_flags.append("-DLINKER_NAME=/cctools/bin/x86_64-apple-darwin-ld")
-        cmake_flags.append("-DCMAKE_TOOLCHAIN_FILE=/build/cmake/darwin/toolchain-x86_64.cmake")
+        cmake_flags.append(
+            "-DCMAKE_TOOLCHAIN_FILE=/build/cmake/darwin/toolchain-x86_64.cmake"
+        )
     elif is_cross_darwin_arm:
-        cc = compiler[:-len(DARWIN_ARM_SUFFIX)]
+        cc = compiler[: -len(DARWIN_ARM_SUFFIX)]
         cmake_flags.append("-DCMAKE_AR:FILEPATH=/cctools/bin/aarch64-apple-darwin-ar")
-        cmake_flags.append("-DCMAKE_INSTALL_NAME_TOOL=/cctools/bin/aarch64-apple-darwin-install_name_tool")
-        cmake_flags.append("-DCMAKE_RANLIB:FILEPATH=/cctools/bin/aarch64-apple-darwin-ranlib")
+        cmake_flags.append(
+            "-DCMAKE_INSTALL_NAME_TOOL=/cctools/bin/"
+            "aarch64-apple-darwin-install_name_tool"
+        )
+        cmake_flags.append(
+            "-DCMAKE_RANLIB:FILEPATH=/cctools/bin/aarch64-apple-darwin-ranlib"
+        )
         cmake_flags.append("-DLINKER_NAME=/cctools/bin/aarch64-apple-darwin-ld")
-        cmake_flags.append("-DCMAKE_TOOLCHAIN_FILE=/build/cmake/darwin/toolchain-aarch64.cmake")
+        cmake_flags.append(
+            "-DCMAKE_TOOLCHAIN_FILE=/build/cmake/darwin/toolchain-aarch64.cmake"
+        )
     elif is_cross_arm:
-        cc = compiler[:-len(ARM_SUFFIX)]
-        cmake_flags.append("-DCMAKE_TOOLCHAIN_FILE=/build/cmake/linux/toolchain-aarch64.cmake")
-        result.append("DEB_ARCH_FLAG=-aarm64")
+        cc = compiler[: -len(ARM_SUFFIX)]
+        cmake_flags.append(
+            "-DCMAKE_TOOLCHAIN_FILE=/build/cmake/linux/toolchain-aarch64.cmake"
+        )
+        result.append("DEB_ARCH=arm64")
     elif is_cross_freebsd:
-        cc = compiler[:-len(FREEBSD_SUFFIX)]
-        cmake_flags.append("-DCMAKE_TOOLCHAIN_FILE=/build/cmake/freebsd/toolchain-x86_64.cmake")
+        cc = compiler[: -len(FREEBSD_SUFFIX)]
+        cmake_flags.append(
+            "-DCMAKE_TOOLCHAIN_FILE=/build/cmake/freebsd/toolchain-x86_64.cmake"
+        )
     elif is_cross_ppc:
-        cc = compiler[:-len(PPC_SUFFIX)]
-        cmake_flags.append("-DCMAKE_TOOLCHAIN_FILE=/build/cmake/linux/toolchain-ppc64le.cmake")
+        cc = compiler[: -len(PPC_SUFFIX)]
+        cmake_flags.append(
+            "-DCMAKE_TOOLCHAIN_FILE=/build/cmake/linux/toolchain-ppc64le.cmake"
+        )
     else:
         cc = compiler
-        result.append("DEB_ARCH_FLAG=-aamd64")
+        result.append("DEB_ARCH=amd64")
 
-    cxx = cc.replace('gcc', 'g++').replace('clang', 'clang++')
+    cxx = cc.replace("gcc", "g++").replace("clang", "clang++")
 
     if image_type == "deb":
-        result.append("DEB_CC={}".format(cc))
-        result.append("DEB_CXX={}".format(cxx))
-        # For building fuzzers
-        result.append("CC={}".format(cc))
-        result.append("CXX={}".format(cxx))
-    elif image_type == "binary":
-        result.append("CC={}".format(cc))
-        result.append("CXX={}".format(cxx))
-        cmake_flags.append('-DCMAKE_C_COMPILER=`which {}`'.format(cc))
-        cmake_flags.append('-DCMAKE_CXX_COMPILER=`which {}`'.format(cxx))
+        result.append("MAKE_DEB=true")
+        cmake_flags.append("-DENABLE_TESTS=0")
+        cmake_flags.append("-DENABLE_UTILS=0")
+        cmake_flags.append("-DCMAKE_EXPORT_NO_PACKAGE_REGISTRY=ON")
+        cmake_flags.append("-DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON")
+        cmake_flags.append("-DCMAKE_AUTOGEN_VERBOSE=ON")
+        cmake_flags.append("-DCMAKE_INSTALL_PREFIX=/usr")
+        cmake_flags.append("-DCMAKE_INSTALL_SYSCONFDIR=/etc")
+        cmake_flags.append("-DCMAKE_INSTALL_LOCALSTATEDIR=/var")
+        cmake_flags.append("-DBUILD_STANDALONE_KEEPER=ON")
+        if is_release_build(build_type, package_type, sanitizer, split_binary):
+            cmake_flags.append("-DINSTALL_STRIPPED_BINARIES=ON")
+
+    result.append(f"CC={cc}")
+    result.append(f"CXX={cxx}")
+    cmake_flags.append(f"-DCMAKE_C_COMPILER={cc}")
+    cmake_flags.append(f"-DCMAKE_CXX_COMPILER={cxx}")
 
     # Create combined output archive for split build and for performance tests.
     if package_type == "performance":
@@ -126,12 +180,14 @@ def parse_env_variables(build_type, compiler, sanitizer, package_type, image_typ
         result.append("COMBINED_OUTPUT=shared_build")
 
     if sanitizer:
-        result.append("SANITIZER={}".format(sanitizer))
+        result.append(f"SANITIZER={sanitizer}")
     if build_type:
-        result.append("BUILD_TYPE={}".format(build_type))
+        result.append(f"BUILD_TYPE={build_type.capitalize()}")
+    else:
+        result.append("BUILD_TYPE=None")
 
-    if cache == 'distcc':
-        result.append("CCACHE_PREFIX={}".format(cache))
+    if cache == "distcc":
+        result.append(f"CCACHE_PREFIX={cache}")
 
     if cache:
         result.append("CCACHE_DIR=/ccache")
@@ -142,109 +198,188 @@ def parse_env_variables(build_type, compiler, sanitizer, package_type, image_typ
         # result.append("CCACHE_UMASK=777")
 
     if distcc_hosts:
-        hosts_with_params = ["{}/24,lzo".format(host) for host in distcc_hosts] + ["localhost/`nproc`"]
-        result.append('DISTCC_HOSTS="{}"'.format(" ".join(hosts_with_params)))
+        hosts_with_params = [f"{host}/24,lzo" for host in distcc_hosts] + [
+            "localhost/`nproc`"
+        ]
+        result.append('DISTCC_HOSTS="' + " ".join(hosts_with_params) + '"')
     elif cache == "distcc":
-        result.append('DISTCC_HOSTS="{}"'.format("localhost/`nproc`"))
+        result.append('DISTCC_HOSTS="localhost/`nproc`"')
 
-    if alien_pkgs:
-        result.append("ALIEN_PKGS='" + ' '.join(['--' + pkg for pkg in alien_pkgs]) + "'")
+    if additional_pkgs:
+        result.append("MAKE_APK=true")
+        result.append("MAKE_RPM=true")
+        result.append("MAKE_TGZ=true")
 
     if with_binaries == "programs":
-        result.append('BINARY_OUTPUT=programs')
+        result.append("BINARY_OUTPUT=programs")
     elif with_binaries == "tests":
-        result.append('ENABLE_TESTS=1')
-        result.append('BINARY_OUTPUT=tests')
-        cmake_flags.append('-DENABLE_TESTS=1')
+        result.append("ENABLE_TESTS=1")
+        result.append("BINARY_OUTPUT=tests")
+        cmake_flags.append("-DENABLE_TESTS=1")
 
     if split_binary:
-        cmake_flags.append('-DUSE_STATIC_LIBRARIES=0 -DSPLIT_SHARED_LIBRARIES=1 -DCLICKHOUSE_SPLIT_BINARY=1')
+        cmake_flags.append(
+            "-DUSE_STATIC_LIBRARIES=0 -DSPLIT_SHARED_LIBRARIES=1 "
+            "-DCLICKHOUSE_SPLIT_BINARY=1"
+        )
         # We can't always build utils because it requires too much space, but
         # we have to build them at least in some way in CI. The split build is
         # probably the least heavy disk-wise.
-        cmake_flags.append('-DENABLE_UTILS=1')
+        cmake_flags.append("-DENABLE_UTILS=1")
 
     if clang_tidy:
-        cmake_flags.append('-DENABLE_CLANG_TIDY=1')
-        cmake_flags.append('-DENABLE_UTILS=1')
-        cmake_flags.append('-DENABLE_TESTS=1')
-        cmake_flags.append('-DENABLE_EXAMPLES=1')
+        cmake_flags.append("-DENABLE_CLANG_TIDY=1")
+        cmake_flags.append("-DENABLE_UTILS=1")
+        cmake_flags.append("-DENABLE_TESTS=1")
+        cmake_flags.append("-DENABLE_EXAMPLES=1")
         # Don't stop on first error to find more clang-tidy errors in one run.
-        result.append('NINJA_FLAGS=-k0')
+        result.append("NINJA_FLAGS=-k0")
 
     if with_coverage:
-        cmake_flags.append('-DWITH_COVERAGE=1')
+        cmake_flags.append("-DWITH_COVERAGE=1")
 
     if version:
-        result.append("VERSION_STRING='{}'".format(version))
+        result.append(f"VERSION_STRING='{version}'")
 
     if author:
-        result.append("AUTHOR='{}'".format(author))
+        result.append(f"AUTHOR='{author}'")
 
     if official:
-        cmake_flags.append('-DYANDEX_OFFICIAL_BUILD=1')
+        cmake_flags.append("-DCLICKHOUSE_OFFICIAL_BUILD=1")
 
-    result.append('CMAKE_FLAGS="' + ' '.join(cmake_flags) + '"')
+    result.append('CMAKE_FLAGS="' + " ".join(cmake_flags) + '"')
 
     return result
 
+
 if __name__ == "__main__":
-    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')
-    parser = argparse.ArgumentParser(description="ClickHouse building script using prebuilt Docker image")
-    # 'performance' creates a combined .tgz with server and configs to be used for performance test.
-    parser.add_argument("--package-type", choices=['deb', 'binary', 'performance'], required=True)
-    parser.add_argument("--clickhouse-repo-path", default=os.path.join(os.path.dirname(os.path.abspath(__file__)), os.pardir, os.pardir))
+    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
+        description="ClickHouse building script using prebuilt Docker image",
+    )
+    # 'performance' creates a combined .tgz with server
+    # and configs to be used for performance test.
+    parser.add_argument(
+        "--package-type",
+        choices=("deb", "binary", "performance"),
+        required=True,
+        help="a build type",
+    )
+    parser.add_argument(
+        "--clickhouse-repo-path",
+        default=os.path.join(
+            os.path.dirname(os.path.abspath(__file__)), os.pardir, os.pardir
+        ),
+        help="ClickHouse git repository",
+    )
     parser.add_argument("--output-dir", required=True)
     parser.add_argument("--build-type", choices=("debug", ""), default="")
-    parser.add_argument("--compiler", choices=("clang-11", "clang-11-darwin", "clang-11-darwin-aarch64", "clang-11-aarch64",
-                                               "clang-12", "clang-12-darwin", "clang-12-darwin-aarch64", "clang-12-aarch64",
-                                               "clang-13", "clang-13-darwin", "clang-13-darwin-aarch64", "clang-13-aarch64", "clang-13-ppc64le",
-                                               "clang-11-freebsd", "clang-12-freebsd", "clang-13-freebsd", "gcc-11"), default="clang-13")
-    parser.add_argument("--sanitizer", choices=("address", "thread", "memory", "undefined", ""), default="")
+    parser.add_argument(
+        "--compiler",
+        choices=(
+            "clang-11",
+            "clang-11-darwin",
+            "clang-11-darwin-aarch64",
+            "clang-11-aarch64",
+            "clang-12",
+            "clang-12-darwin",
+            "clang-12-darwin-aarch64",
+            "clang-12-aarch64",
+            "clang-13",
+            "clang-13-darwin",
+            "clang-13-darwin-aarch64",
+            "clang-13-aarch64",
+            "clang-13-ppc64le",
+            "clang-11-freebsd",
+            "clang-12-freebsd",
+            "clang-13-freebsd",
+            "gcc-11",
+        ),
+        default="clang-13",
+        help="a compiler to use",
+    )
+    parser.add_argument(
+        "--sanitizer",
+        choices=("address", "thread", "memory", "undefined", ""),
+        default="",
+    )
     parser.add_argument("--split-binary", action="store_true")
     parser.add_argument("--clang-tidy", action="store_true")
-    parser.add_argument("--cache", choices=("", "ccache", "distcc"), default="")
-    parser.add_argument("--ccache_dir", default= os.getenv("HOME", "") + '/.ccache')
+    parser.add_argument("--cache", choices=("ccache", "distcc", ""), default="")
+    parser.add_argument(
+        "--ccache_dir",
+        default=os.getenv("HOME", "") + "/.ccache",
+        help="a directory with ccache",
+    )
     parser.add_argument("--distcc-hosts", nargs="+")
     parser.add_argument("--force-build-image", action="store_true")
     parser.add_argument("--version")
-    parser.add_argument("--author", default="clickhouse")
+    parser.add_argument("--author", default="clickhouse", help="a package author")
     parser.add_argument("--official", action="store_true")
-    parser.add_argument("--alien-pkgs", nargs='+', default=[])
+    parser.add_argument("--additional-pkgs", action="store_true")
     parser.add_argument("--with-coverage", action="store_true")
-    parser.add_argument("--with-binaries", choices=("programs", "tests", ""), default="")
-    parser.add_argument("--docker-image-version", default="latest")
+    parser.add_argument(
+        "--with-binaries", choices=("programs", "tests", ""), default=""
+    )
+    parser.add_argument(
+        "--docker-image-version", default="latest", help="docker image tag to use"
+    )
 
     args = parser.parse_args()
     if not os.path.isabs(args.output_dir):
         args.output_dir = os.path.abspath(os.path.join(os.getcwd(), args.output_dir))
 
-    image_type = 'binary' if args.package_type == 'performance' else args.package_type
-    image_name = IMAGE_MAP[image_type]
+    image_type = "binary" if args.package_type == "performance" else args.package_type
+    image_name = "clickhouse/binary-builder"
 
     if not os.path.isabs(args.clickhouse_repo_path):
         ch_root = os.path.abspath(os.path.join(os.getcwd(), args.clickhouse_repo_path))
     else:
         ch_root = args.clickhouse_repo_path
 
-    if args.alien_pkgs and not image_type == "deb":
-        raise Exception("Can add alien packages only in deb build")
+    if args.additional_pkgs and image_type != "deb":
+        raise Exception("Can build additional packages only in deb build")
 
-    if args.with_binaries != "" and not image_type == "deb":
+    if args.with_binaries != "" and image_type != "deb":
         raise Exception("Can add additional binaries only in deb build")
 
     if args.with_binaries != "" and image_type == "deb":
-        logging.info("Should place {} to output".format(args.with_binaries))
+        logging.info("Should place %s to output", args.with_binaries)
 
     dockerfile = os.path.join(ch_root, "docker/packager", image_type, "Dockerfile")
     image_with_version = image_name + ":" + args.docker_image_version
-    if image_type != "freebsd" and not check_image_exists_locally(image_name) or args.force_build_image:
+    if (
+        image_type != "freebsd"
+        and not check_image_exists_locally(image_name)
+        or args.force_build_image
+    ):
         if not pull_image(image_with_version) or args.force_build_image:
             build_image(image_with_version, dockerfile)
     env_prepared = parse_env_variables(
-        args.build_type, args.compiler, args.sanitizer, args.package_type, image_type,
-        args.cache, args.distcc_hosts, args.split_binary, args.clang_tidy,
-        args.version, args.author, args.official, args.alien_pkgs, args.with_coverage, args.with_binaries)
+        args.build_type,
+        args.compiler,
+        args.sanitizer,
+        args.package_type,
+        image_type,
+        args.cache,
+        args.distcc_hosts,
+        args.split_binary,
+        args.clang_tidy,
+        args.version,
+        args.author,
+        args.official,
+        args.additional_pkgs,
+        args.with_coverage,
+        args.with_binaries,
+    )
 
-    run_docker_image_with_env(image_name, args.output_dir, env_prepared, ch_root, args.ccache_dir, args.docker_image_version)
-    logging.info("Output placed into {}".format(args.output_dir))
+    run_docker_image_with_env(
+        image_name,
+        args.output_dir,
+        env_prepared,
+        ch_root,
+        args.ccache_dir,
+        args.docker_image_version,
+    )
+    logging.info("Output placed into %s", args.output_dir)
diff --git a/docker/server/.gitignore b/docker/server/.gitignore
deleted file mode 100644
index 692758d55aa1..000000000000
--- a/docker/server/.gitignore
+++ /dev/null
@@ -1,2 +0,0 @@
-alpine-root/*
-tgz-packages/*
diff --git a/docker/server/Dockerfile b/docker/server/Dockerfile
deleted file mode 100644
index 5b7990ab0300..000000000000
--- a/docker/server/Dockerfile
+++ /dev/null
@@ -1,122 +0,0 @@
-FROM ubuntu:20.04
-
-# ARG for quick switch to a given ubuntu mirror
-ARG apt_archive="http://archive.ubuntu.com"
-RUN sed -i "s|http://archive.ubuntu.com|$apt_archive|g" /etc/apt/sources.list
-
-ARG repository="deb https://packages.clickhouse.com/deb stable main"
-ARG version=22.1.1.*
-
-# set non-empty deb_location_url url to create a docker image
-# from debs created by CI build, for example:
-# docker build . --network host --build-arg version="21.4.1.6282" --build-arg deb_location_url="https://clickhouse-builds.s3.yandex.net/21852/069cfbff388b3d478d1a16dc7060b48073f5d522/clickhouse_build_check/clang-11_relwithdebuginfo_none_bundled_unsplitted_disable_False_deb/" -t filimonovq/clickhouse-server:pr21852
-ARG deb_location_url=""
-
-# set non-empty single_binary_location_url to create docker image
-# from a single binary url (useful for non-standard builds - with sanitizers, for arm64).
-# for example (run on aarch64 server):
-# docker build . --network host --build-arg single_binary_location_url="https://builds.clickhouse.com/master/aarch64/clickhouse" -t altinity/clickhouse-server:master-testing-arm
-# note: clickhouse-odbc-bridge is not supported there.
-ARG single_binary_location_url=""
-
-# see https://github.com/moby/moby/issues/4032#issuecomment-192327844
-ARG DEBIAN_FRONTEND=noninteractive
-
-# user/group precreated explicitly with fixed uid/gid on purpose.
-# It is especially important for rootless containers: in that case entrypoint
-# can't do chown and owners of mounted volumes should be configured externally.
-# We do that in advance at the begining of Dockerfile before any packages will be
-# installed to prevent picking those uid / gid by some unrelated software.
-# The same uid / gid (101) is used both for alpine and ubuntu.
-
-# To drop privileges, we need 'su' command, that simply changes uid and gid.
-# In fact, the 'su' command from Linux is not so simple, due to inherent vulnerability in Linux:
-# https://ruderich.org/simon/notes/su-sudo-from-root-tty-hijacking
-# It has to mitigate this drawback of Linux, and to do this, 'su' command is creating it's own pseudo-terminal
-# and forwarding commands. Due to some ridiculous curcumstances, it does not work in Docker (or it does)
-# and for these reasons people are using alternatives to the 'su' command in Docker,
-# that don't mess with the terminal, don't care about closing the opened files, etc...
-# but can only be safe to drop privileges inside Docker.
-# The question - what implementation of 'su' command to use.
-# It should be a simple script doing about just two syscalls.
-# Some people tend to use 'gosu' tool that is written in Go.
-# It is not used for several reasons:
-# 1. Dependency on some foreign code in yet another programming language - does not sound alright.
-# 2. Anselmo D. Adams suggested not to use it due to false positive alarms in some undisclosed security scanners.
-
-COPY su-exec.c /su-exec.c
-
-RUN groupadd -r clickhouse --gid=101 \
-    && useradd -r -g clickhouse --uid=101 --home-dir=/var/lib/clickhouse --shell=/bin/bash clickhouse \
-    && apt-get update \
-    && apt-get install --yes --no-install-recommends \
-        apt-transport-https \
-        ca-certificates \
-        dirmngr \
-        gnupg \
-        locales \
-        wget \
-        tzdata \
-    && mkdir -p /etc/apt/sources.list.d \
-    && apt-key adv --keyserver keyserver.ubuntu.com --recv 8919F6BD2B48D754 \
-    && echo $repository > /etc/apt/sources.list.d/clickhouse.list \
-    && if [ -n "$deb_location_url" ]; then \
-            echo "installing from custom url with deb packages: $deb_location_url" \
-            rm -rf /tmp/clickhouse_debs \
-            && mkdir -p /tmp/clickhouse_debs \
-            && wget --progress=bar:force:noscroll "${deb_location_url}/clickhouse-common-static_${version}_amd64.deb" -P /tmp/clickhouse_debs \
-            && wget --progress=bar:force:noscroll "${deb_location_url}/clickhouse-client_${version}_all.deb" -P /tmp/clickhouse_debs \
-            && wget --progress=bar:force:noscroll "${deb_location_url}/clickhouse-server_${version}_all.deb" -P /tmp/clickhouse_debs \
-            && dpkg -i /tmp/clickhouse_debs/*.deb ; \
-       elif [ -n "$single_binary_location_url" ]; then \
-            echo "installing from single binary url: $single_binary_location_url" \
-            && rm -rf /tmp/clickhouse_binary \
-            && mkdir -p /tmp/clickhouse_binary \
-            && wget --progress=bar:force:noscroll "$single_binary_location_url" -O /tmp/clickhouse_binary/clickhouse \
-            && chmod +x /tmp/clickhouse_binary/clickhouse \
-            && /tmp/clickhouse_binary/clickhouse install --user "clickhouse" --group "clickhouse" ; \
-       else \
-           echo "installing from repository: $repository" \
-           && apt-get update \
-           && apt-get --yes -o "Dpkg::Options::=--force-confdef" -o "Dpkg::Options::=--force-confold" upgrade \
-           && apt-get install --allow-unauthenticated --yes --no-install-recommends \
-                clickhouse-common-static=$version \
-                clickhouse-client=$version \
-                clickhouse-server=$version ; \
-       fi \
-    && apt-get install -y --no-install-recommends tcc libc-dev && \
-        tcc /su-exec.c -o /bin/su-exec && \
-        chown root:root /bin/su-exec && \
-        chmod 0755 /bin/su-exec && \
-        rm /su-exec.c && \
-        apt-get purge -y --auto-remove tcc libc-dev libc-dev-bin libc6-dev linux-libc-dev \
-    && clickhouse-local -q 'SELECT * FROM system.build_options' \
-    && rm -rf \
-        /var/lib/apt/lists/* \
-        /var/cache/debconf \
-        /tmp/* \
-    && apt-get clean \
-    && mkdir -p /var/lib/clickhouse /var/log/clickhouse-server /etc/clickhouse-server /etc/clickhouse-client \
-    && chmod ugo+Xrw -R /var/lib/clickhouse /var/log/clickhouse-server /etc/clickhouse-server /etc/clickhouse-client
-
-# we need to allow "others" access to clickhouse folder, because docker container
-# can be started with arbitrary uid (openshift usecase)
-
-RUN locale-gen en_US.UTF-8
-ENV LANG en_US.UTF-8
-ENV LANGUAGE en_US:en
-ENV LC_ALL en_US.UTF-8
-ENV TZ UTC
-
-RUN mkdir /docker-entrypoint-initdb.d
-
-COPY docker_related_config.xml /etc/clickhouse-server/config.d/
-COPY entrypoint.sh /entrypoint.sh
-RUN chmod +x /entrypoint.sh
-
-EXPOSE 9000 8123 9009
-VOLUME /var/lib/clickhouse
-
-ENV CLICKHOUSE_CONFIG /etc/clickhouse-server/config.xml
-
-ENTRYPOINT ["/entrypoint.sh"]
diff --git a/docker/server/Dockerfile b/docker/server/Dockerfile
new file mode 120000
index 000000000000..fd45f0f7c7cd
--- /dev/null
+++ b/docker/server/Dockerfile
@@ -0,0 +1,1 @@
+Dockerfile.ubuntu
\ No newline at end of file
diff --git a/docker/server/Dockerfile.alpine b/docker/server/Dockerfile.alpine
index cd192c0c9da4..5aaf5dd55115 100644
--- a/docker/server/Dockerfile.alpine
+++ b/docker/server/Dockerfile.alpine
@@ -1,3 +1,14 @@
+FROM ubuntu:20.04 AS glibc-donor
+ARG TARGETARCH
+
+RUN arch=${TARGETARCH:-amd64} \
+    && case $arch in \
+        amd64) rarch=x86_64 ;; \
+        arm64) rarch=aarch64 ;; \
+    esac \
+    && ln -s "${rarch}-linux-gnu" /lib/linux-gnu
+
+
 FROM alpine
 
 ENV LANG=en_US.UTF-8 \
@@ -6,7 +17,24 @@ ENV LANG=en_US.UTF-8 \
     TZ=UTC \
     CLICKHOUSE_CONFIG=/etc/clickhouse-server/config.xml
 
-COPY alpine-root/ /
+COPY --from=glibc-donor /lib/linux-gnu/libc.so.6 /lib/linux-gnu/libdl.so.2 /lib/linux-gnu/libm.so.6 /lib/linux-gnu/libpthread.so.0 /lib/linux-gnu/librt.so.1 /lib/linux-gnu/libnss_dns.so.2 /lib/linux-gnu/libnss_files.so.2 /lib/linux-gnu/libresolv.so.2 /lib/linux-gnu/ld-2.31.so /lib/
+COPY --from=glibc-donor /etc/nsswitch.conf /etc/
+COPY docker_related_config.xml /etc/clickhouse-server/config.d/
+COPY entrypoint.sh /entrypoint.sh
+
+ARG TARGETARCH
+
+RUN arch=${TARGETARCH:-amd64} \
+    && case $arch in \
+        amd64) mkdir -p /lib64 && ln -sf /lib/ld-2.31.so /lib64/ld-linux-x86-64.so.2 ;; \
+        arm64) ln -sf /lib/ld-2.31.so /lib/ld-linux-aarch64.so.1 ;; \
+    esac
+
+# lts / testing / prestable / etc
+ARG REPO_CHANNEL="stable"
+ARG REPOSITORY="https://packages.clickhouse.com/tgz/${REPO_CHANNEL}"
+ARG VERSION="20.9.3.45"
+ARG PACKAGES="clickhouse-client clickhouse-server clickhouse-common-static"
 
 # user/group precreated explicitly with fixed uid/gid on purpose.
 # It is especially important for rootless containers: in that case entrypoint
@@ -15,9 +43,23 @@ COPY alpine-root/ /
 # installed to prevent picking those uid / gid by some unrelated software.
 # The same uid / gid (101) is used both for alpine and ubuntu.
 
-RUN addgroup -S -g 101 clickhouse \
+RUN arch=${TARGETARCH:-amd64} \
+    && for package in ${PACKAGES}; do \
+        { \
+            { echo "Get ${REPOSITORY}/${package}-${VERSION}-${arch}.tgz" \
+                && wget -c -q "${REPOSITORY}/${package}-${VERSION}-${arch}.tgz" -O "/tmp/${package}-${VERSION}-${arch}.tgz" \
+                && tar xvzf "/tmp/${package}-${VERSION}-${arch}.tgz" --strip-components=1 -C / ; \
+            } || \
+            { echo "Fallback to ${REPOSITORY}/${package}-${VERSION}.tgz" \
+                && wget -c -q "${REPOSITORY}/${package}-${VERSION}.tgz" -O "/tmp/${package}-${VERSION}.tgz" \
+                && tar xvzf "/tmp/${package}-${VERSION}.tgz" --strip-components=2 -C / ; \
+            } ; \
+        } || exit 1 \
+    ; done \
+    && rm /tmp/*.tgz /install -r \
+    && addgroup -S -g 101 clickhouse \
     && adduser -S -h /var/lib/clickhouse -s /bin/bash -G clickhouse -g "ClickHouse server" -u 101 clickhouse \
-    && mkdir -p /var/lib/clickhouse /var/log/clickhouse-server /etc/clickhouse-server /etc/clickhouse-client \
+    && mkdir -p /var/lib/clickhouse /var/log/clickhouse-server /etc/clickhouse-server/config.d /etc/clickhouse-server/users.d /etc/clickhouse-client /docker-entrypoint-initdb.d \
     && chown clickhouse:clickhouse /var/lib/clickhouse \
     && chown root:clickhouse /var/log/clickhouse-server \
     && chmod +x /entrypoint.sh \
diff --git a/docker/server/Dockerfile.ubuntu b/docker/server/Dockerfile.ubuntu
new file mode 100644
index 000000000000..cc198772251a
--- /dev/null
+++ b/docker/server/Dockerfile.ubuntu
@@ -0,0 +1,128 @@
+FROM ubuntu:20.04
+
+# see https://github.com/moby/moby/issues/4032#issuecomment-192327844
+ARG DEBIAN_FRONTEND=noninteractive
+
+COPY su-exec.c /su-exec.c
+
+# ARG for quick switch to a given ubuntu mirror
+ARG apt_archive="http://archive.ubuntu.com"
+RUN sed -i "s|http://archive.ubuntu.com|${apt_archive}|g" /etc/apt/sources.list \
+    && groupadd -r clickhouse --gid=101 \
+    && useradd -r -g clickhouse --uid=101 --home-dir=/var/lib/clickhouse --shell=/bin/bash clickhouse \
+    && apt-get update \
+    && apt-get install --yes --no-install-recommends \
+        apt-transport-https \
+        ca-certificates \
+        dirmngr \
+        gnupg \
+        locales \
+        wget \
+        tzdata \
+    && apt-get install -y --no-install-recommends tcc libc-dev && \
+        tcc /su-exec.c -o /bin/su-exec && \
+        chown root:root /bin/su-exec && \
+        chmod 0755 /bin/su-exec && \
+        rm /su-exec.c && \
+        apt-get purge -y --auto-remove tcc libc-dev libc-dev-bin libc6-dev linux-libc-dev \
+    && apt-get clean
+
+ARG REPO_CHANNEL="stable"
+ARG REPOSITORY="deb https://packages.clickhouse.com/deb ${REPO_CHANNEL} main"
+ARG VERSION=22.1.1.*
+ARG PACKAGES="clickhouse-client clickhouse-server clickhouse-common-static"
+
+# set non-empty deb_location_url url to create a docker image
+# from debs created by CI build, for example:
+# docker build . --network host --build-arg version="21.4.1.6282" --build-arg deb_location_url="https://clickhouse-builds.s3.yandex.net/21852/069cfbff388b3d478d1a16dc7060b48073f5d522/clickhouse_build_check/clang-11_relwithdebuginfo_none_bundled_unsplitted_disable_False_deb/" -t filimonovq/clickhouse-server:pr21852
+ARG deb_location_url=""
+
+# set non-empty single_binary_location_url to create docker image
+# from a single binary url (useful for non-standard builds - with sanitizers, for arm64).
+# for example (run on aarch64 server):
+# docker build . --network host --build-arg single_binary_location_url="https://builds.clickhouse.com/master/aarch64/clickhouse" -t altinity/clickhouse-server:master-testing-arm
+# note: clickhouse-odbc-bridge is not supported there.
+ARG single_binary_location_url=""
+
+# user/group precreated explicitly with fixed uid/gid on purpose.
+# It is especially important for rootless containers: in that case entrypoint
+# can't do chown and owners of mounted volumes should be configured externally.
+# We do that in advance at the begining of Dockerfile before any packages will be
+# installed to prevent picking those uid / gid by some unrelated software.
+# The same uid / gid (101) is used both for alpine and ubuntu.
+
+# To drop privileges, we need 'su' command, that simply changes uid and gid.
+# In fact, the 'su' command from Linux is not so simple, due to inherent vulnerability in Linux:
+# https://ruderich.org/simon/notes/su-sudo-from-root-tty-hijacking
+# It has to mitigate this drawback of Linux, and to do this, 'su' command is creating it's own pseudo-terminal
+# and forwarding commands. Due to some ridiculous curcumstances, it does not work in Docker (or it does)
+# and for these reasons people are using alternatives to the 'su' command in Docker,
+# that don't mess with the terminal, don't care about closing the opened files, etc...
+# but can only be safe to drop privileges inside Docker.
+# The question - what implementation of 'su' command to use.
+# It should be a simple script doing about just two syscalls.
+# Some people tend to use 'gosu' tool that is written in Go.
+# It is not used for several reasons:
+# 1. Dependency on some foreign code in yet another programming language - does not sound alright.
+# 2. Anselmo D. Adams suggested not to use it due to false positive alarms in some undisclosed security scanners.
+
+ARG TARGETARCH
+
+RUN arch=${TARGETARCH:-amd64} \
+    && if [ -n "${deb_location_url}" ]; then \
+        echo "installing from custom url with deb packages: ${deb_location_url}" \
+        rm -rf /tmp/clickhouse_debs \
+        && mkdir -p /tmp/clickhouse_debs \
+        && for package in ${PACKAGES}; do \
+            { wget --progress=bar:force:noscroll "${deb_location_url}/${package}_${VERSION}_${arch}.deb" -P /tmp/clickhouse_debs || \
+                wget --progress=bar:force:noscroll "${deb_location_url}/${package}_${VERSION}_all.deb" -P /tmp/clickhouse_debs ; } \
+            || exit 1 \
+        ; done \
+        && dpkg -i /tmp/clickhouse_debs/*.deb ; \
+    elif [ -n "${single_binary_location_url}" ]; then \
+        echo "installing from single binary url: ${single_binary_location_url}" \
+        && rm -rf /tmp/clickhouse_binary \
+        && mkdir -p /tmp/clickhouse_binary \
+        && wget --progress=bar:force:noscroll "${single_binary_location_url}" -O /tmp/clickhouse_binary/clickhouse \
+        && chmod +x /tmp/clickhouse_binary/clickhouse \
+        && /tmp/clickhouse_binary/clickhouse install --user "clickhouse" --group "clickhouse" ; \
+    else \
+        mkdir -p /etc/apt/sources.list.d \
+        && apt-key adv --keyserver keyserver.ubuntu.com --recv 8919F6BD2B48D754 \
+        && echo ${REPOSITORY} > /etc/apt/sources.list.d/clickhouse.list \
+        && echo "installing from repository: ${REPOSITORY}" \
+        && apt-get update \
+        && apt-get --yes -o "Dpkg::Options::=--force-confdef" -o "Dpkg::Options::=--force-confold" upgrade \
+        && for package in ${PACKAGES}; do \
+            apt-get install --allow-unauthenticated --yes --no-install-recommends "${package}=${VERSION}" || exit 1 \
+        ; done \
+    ; fi \
+    && clickhouse-local -q 'SELECT * FROM system.build_options' \
+    && rm -rf \
+        /var/lib/apt/lists/* \
+        /var/cache/debconf \
+        /tmp/* \
+    && mkdir -p /var/lib/clickhouse /var/log/clickhouse-server /etc/clickhouse-server /etc/clickhouse-client \
+    && chmod ugo+Xrw -R /var/lib/clickhouse /var/log/clickhouse-server /etc/clickhouse-server /etc/clickhouse-client
+
+# we need to allow "others" access to clickhouse folder, because docker container
+# can be started with arbitrary uid (openshift usecase)
+
+RUN locale-gen en_US.UTF-8
+ENV LANG en_US.UTF-8
+ENV LANGUAGE en_US:en
+ENV LC_ALL en_US.UTF-8
+ENV TZ UTC
+
+RUN mkdir /docker-entrypoint-initdb.d
+
+COPY docker_related_config.xml /etc/clickhouse-server/config.d/
+COPY entrypoint.sh /entrypoint.sh
+RUN chmod +x /entrypoint.sh
+
+EXPOSE 9000 8123 9009
+VOLUME /var/lib/clickhouse
+
+ENV CLICKHOUSE_CONFIG /etc/clickhouse-server/config.xml
+
+ENTRYPOINT ["/entrypoint.sh"]
diff --git a/docker/server/alpine-build.sh b/docker/server/alpine-build.sh
deleted file mode 100755
index 1b448c61fbbd..000000000000
--- a/docker/server/alpine-build.sh
+++ /dev/null
@@ -1,63 +0,0 @@
-#!/bin/bash
-set -x
-
-REPO_CHANNEL="${REPO_CHANNEL:-stable}" # lts / testing / prestable / etc
-REPO_URL="${REPO_URL:-"https://repo.yandex.ru/clickhouse/tgz/${REPO_CHANNEL}"}"
-VERSION="${VERSION:-20.9.3.45}"
-DOCKER_IMAGE="${DOCKER_IMAGE:-clickhouse/clickhouse-server}"
-
-# where original files live
-DOCKER_BUILD_FOLDER="${BASH_SOURCE%/*}"
-
-# we will create root for our image here
-CONTAINER_ROOT_FOLDER="${DOCKER_BUILD_FOLDER}/alpine-root"
-
-# clean up the root from old runs, it's reconstructed each time
-rm -rf "$CONTAINER_ROOT_FOLDER"
-mkdir -p "$CONTAINER_ROOT_FOLDER"
-
-# where to put downloaded tgz
-TGZ_PACKAGES_FOLDER="${DOCKER_BUILD_FOLDER}/tgz-packages"
-mkdir -p "$TGZ_PACKAGES_FOLDER"
-
-PACKAGES=( "clickhouse-client" "clickhouse-server" "clickhouse-common-static" )
-
-# download tars from the repo
-for package in "${PACKAGES[@]}"
-do
-    wget -c -q --show-progress "${REPO_URL}/${package}-${VERSION}.tgz" -O "${TGZ_PACKAGES_FOLDER}/${package}-${VERSION}.tgz"
-done
-
-# unpack tars
-for package in "${PACKAGES[@]}"
-do
-    tar xvzf "${TGZ_PACKAGES_FOLDER}/${package}-${VERSION}.tgz" --strip-components=2 -C "$CONTAINER_ROOT_FOLDER"
-done
-
-# prepare few more folders
-mkdir -p "${CONTAINER_ROOT_FOLDER}/etc/clickhouse-server/users.d" \
-         "${CONTAINER_ROOT_FOLDER}/etc/clickhouse-server/config.d" \
-         "${CONTAINER_ROOT_FOLDER}/var/log/clickhouse-server" \
-         "${CONTAINER_ROOT_FOLDER}/var/lib/clickhouse" \
-         "${CONTAINER_ROOT_FOLDER}/docker-entrypoint-initdb.d" \
-         "${CONTAINER_ROOT_FOLDER}/lib64"
-
-cp "${DOCKER_BUILD_FOLDER}/docker_related_config.xml" "${CONTAINER_ROOT_FOLDER}/etc/clickhouse-server/config.d/"
-cp "${DOCKER_BUILD_FOLDER}/entrypoint.sh"             "${CONTAINER_ROOT_FOLDER}/entrypoint.sh"
-
-## get glibc components from ubuntu 20.04 and put them to expected place
-docker pull ubuntu:20.04
-ubuntu20image=$(docker create --rm ubuntu:20.04)
-docker cp -L "${ubuntu20image}":/lib/x86_64-linux-gnu/libc.so.6       "${CONTAINER_ROOT_FOLDER}/lib"
-docker cp -L "${ubuntu20image}":/lib/x86_64-linux-gnu/libdl.so.2      "${CONTAINER_ROOT_FOLDER}/lib"
-docker cp -L "${ubuntu20image}":/lib/x86_64-linux-gnu/libm.so.6       "${CONTAINER_ROOT_FOLDER}/lib"
-docker cp -L "${ubuntu20image}":/lib/x86_64-linux-gnu/libpthread.so.0 "${CONTAINER_ROOT_FOLDER}/lib"
-docker cp -L "${ubuntu20image}":/lib/x86_64-linux-gnu/librt.so.1      "${CONTAINER_ROOT_FOLDER}/lib"
-docker cp -L "${ubuntu20image}":/lib/x86_64-linux-gnu/libnss_dns.so.2 "${CONTAINER_ROOT_FOLDER}/lib"
-docker cp -L "${ubuntu20image}":/lib/x86_64-linux-gnu/libnss_files.so.2 "${CONTAINER_ROOT_FOLDER}/lib"
-docker cp -L "${ubuntu20image}":/lib/x86_64-linux-gnu/libresolv.so.2  "${CONTAINER_ROOT_FOLDER}/lib"
-docker cp -L "${ubuntu20image}":/lib64/ld-linux-x86-64.so.2           "${CONTAINER_ROOT_FOLDER}/lib64"
-docker cp -L "${ubuntu20image}":/etc/nsswitch.conf                    "${CONTAINER_ROOT_FOLDER}/etc"
-
-docker build "$DOCKER_BUILD_FOLDER" -f Dockerfile.alpine -t "${DOCKER_IMAGE}:${VERSION}-alpine" --pull
-rm -rf "$CONTAINER_ROOT_FOLDER"
diff --git a/docker/server/local.Dockerfile b/docker/server/local.Dockerfile
deleted file mode 100644
index 0d86c9ce45a3..000000000000
--- a/docker/server/local.Dockerfile
+++ /dev/null
@@ -1,47 +0,0 @@
-# Since right now we can't set volumes to the docker during build, we split building container in stages:
-# 1. build base container
-# 2. run base conatiner with mounted volumes
-# 3. commit container as image
-# 4. build final container atop that image
-# Middle steps are performed by the bash script.
-
-FROM ubuntu:18.04 as clickhouse-server-base
-ARG gosu_ver=1.14
-
-VOLUME /packages/
-
-# update to allow installing dependencies of clickhouse automatically
-RUN apt update; \
-    DEBIAN_FRONTEND=noninteractive \
-    apt install -y locales;
-
-ADD https://github.com/tianon/gosu/releases/download/${gosu_ver}/gosu-amd64 /bin/gosu
-
-RUN locale-gen en_US.UTF-8
-ENV LANG en_US.UTF-8
-ENV LANGUAGE en_US:en
-ENV LC_ALL en_US.UTF-8
-
-# installing via apt to simulate real-world scenario, where user installs deb package and all it's dependecies automatically.
-CMD DEBIAN_FRONTEND=noninteractive \
-    apt install -y \
-        /packages/clickhouse-common-static_*.deb \
-        /packages/clickhouse-server_*.deb ;
-
-FROM clickhouse-server-base:postinstall as clickhouse-server
-
-RUN mkdir /docker-entrypoint-initdb.d
-
-COPY docker_related_config.xml /etc/clickhouse-server/config.d/
-COPY entrypoint.sh /entrypoint.sh
-
-RUN chmod +x \
-    /entrypoint.sh \
-    /bin/gosu
-
-EXPOSE 9000 8123 9009
-VOLUME /var/lib/clickhouse
-
-ENV CLICKHOUSE_CONFIG /etc/clickhouse-server/config.xml
-
-ENTRYPOINT ["/entrypoint.sh"]
diff --git a/docs/tools/amp.py b/docs/tools/amp.py
index 224174079460..584a40c4bba3 100644
--- a/docs/tools/amp.py
+++ b/docs/tools/amp.py
@@ -15,24 +15,24 @@
 
 def prepare_amp_html(lang, args, root, site_temp, main_site_dir):
     src_path = root
-    src_index = os.path.join(src_path, 'index.html')
+    src_index = os.path.join(src_path, "index.html")
     rel_path = os.path.relpath(src_path, site_temp)
-    dst_path = os.path.join(main_site_dir, rel_path, 'amp')
-    dst_index = os.path.join(dst_path, 'index.html')
+    dst_path = os.path.join(main_site_dir, rel_path, "amp")
+    dst_index = os.path.join(dst_path, "index.html")
 
-    logging.debug(f'Generating AMP version for {rel_path} ({lang})')
+    logging.debug(f"Generating AMP version for {rel_path} ({lang})")
     os.makedirs(dst_path)
-    with open(src_index, 'r') as f:
+    with open(src_index, "r") as f:
         content = f.read()
-    css_in = ' '.join(website.get_css_in(args))
+    css_in = " ".join(website.get_css_in(args))
     command = f"purifycss --min {css_in} '{src_index}'"
     logging.debug(command)
-    inline_css = subprocess.check_output(command, shell=True).decode('utf-8')
-    inline_css = inline_css.replace('!important', '').replace('/*!', '/*')
+    inline_css = subprocess.check_output(command, shell=True).decode("utf-8")
+    inline_css = inline_css.replace("!important", "").replace("/*!", "/*")
     inline_css = cssmin.cssmin(inline_css)
-    content = content.replace('CUSTOM_CSS_PLACEHOLDER', inline_css)
+    content = content.replace("CUSTOM_CSS_PLACEHOLDER", inline_css)
 
-    with open(dst_index, 'w') as f:
+    with open(dst_index, "w") as f:
         f.write(content)
 
     return dst_index
@@ -40,15 +40,12 @@ def prepare_amp_html(lang, args, root, site_temp, main_site_dir):
 
 def build_amp(lang, args, cfg):
     # AMP docs: https://amp.dev/documentation/
-    logging.info(f'Building AMP version for {lang}')
+    logging.info(f"Building AMP version for {lang}")
     with util.temp_dir() as site_temp:
-        extra = cfg.data['extra']
-        main_site_dir = cfg.data['site_dir']
-        extra['is_amp'] = True
-        cfg.load_dict({
-            'site_dir': site_temp,
-            'extra': extra
-        })
+        extra = cfg.data["extra"]
+        main_site_dir = cfg.data["site_dir"]
+        extra["is_amp"] = True
+        cfg.load_dict({"site_dir": site_temp, "extra": extra})
 
         try:
             mkdocs.commands.build.build(cfg)
@@ -60,50 +57,49 @@ def build_amp(lang, args, cfg):
 
         paths = []
         for root, _, filenames in os.walk(site_temp):
-            if 'index.html' in filenames:
-                paths.append(prepare_amp_html(lang, args, root, site_temp, main_site_dir))
-    logging.info(f'Finished building AMP version for {lang}')
+            if "index.html" in filenames:
+                paths.append(
+                    prepare_amp_html(lang, args, root, site_temp, main_site_dir)
+                )
+    logging.info(f"Finished building AMP version for {lang}")
 
 
 def html_to_amp(content):
-    soup = bs4.BeautifulSoup(
-        content,
-        features='html.parser'
-    )
+    soup = bs4.BeautifulSoup(content, features="html.parser")
 
     for tag in soup.find_all():
-        if tag.attrs.get('id') == 'tostring':
-            tag.attrs['id'] = '_tostring'
-        if tag.name == 'img':
-            tag.name = 'amp-img'
-            tag.attrs['layout'] = 'responsive'
-            src = tag.attrs['src']
-            if not (src.startswith('/') or src.startswith('http')):
-                tag.attrs['src'] = f'../{src}'
-            if not tag.attrs.get('width'):
-                tag.attrs['width'] = '640'
-            if not tag.attrs.get('height'):
-                tag.attrs['height'] = '320'
-        if tag.name == 'iframe':
-            tag.name = 'amp-iframe'
-            tag.attrs['layout'] = 'responsive'
-            del tag.attrs['alt']
-            del tag.attrs['allowfullscreen']
-            if not tag.attrs.get('width'):
-                tag.attrs['width'] = '640'
-            if not tag.attrs.get('height'):
-                tag.attrs['height'] = '320'
-        elif tag.name == 'a':
-            href = tag.attrs.get('href')
+        if tag.attrs.get("id") == "tostring":
+            tag.attrs["id"] = "_tostring"
+        if tag.name == "img":
+            tag.name = "amp-img"
+            tag.attrs["layout"] = "responsive"
+            src = tag.attrs["src"]
+            if not (src.startswith("/") or src.startswith("http")):
+                tag.attrs["src"] = f"../{src}"
+            if not tag.attrs.get("width"):
+                tag.attrs["width"] = "640"
+            if not tag.attrs.get("height"):
+                tag.attrs["height"] = "320"
+        if tag.name == "iframe":
+            tag.name = "amp-iframe"
+            tag.attrs["layout"] = "responsive"
+            del tag.attrs["alt"]
+            del tag.attrs["allowfullscreen"]
+            if not tag.attrs.get("width"):
+                tag.attrs["width"] = "640"
+            if not tag.attrs.get("height"):
+                tag.attrs["height"] = "320"
+        elif tag.name == "a":
+            href = tag.attrs.get("href")
             if href:
-                if not (href.startswith('/') or href.startswith('http')):
-                    if '#' in href:
-                        href, anchor = href.split('#')
+                if not (href.startswith("/") or href.startswith("http")):
+                    if "#" in href:
+                        href, anchor = href.split("#")
                     else:
                         anchor = None
-                    href = f'../{href}amp/'
+                    href = f"../{href}amp/"
                     if anchor:
-                        href = f'{href}#{anchor}'
-                    tag.attrs['href'] = href
+                        href = f"{href}#{anchor}"
+                    tag.attrs["href"] = href
     content = str(soup)
     return website.minify_html(content)
diff --git a/docs/tools/blog.py b/docs/tools/blog.py
index b58523504a39..d1fc540d8bf8 100644
--- a/docs/tools/blog.py
+++ b/docs/tools/blog.py
@@ -17,54 +17,52 @@
 
 
 def build_for_lang(lang, args):
-    logging.info(f'Building {lang} blog')
+    logging.info(f"Building {lang} blog")
 
     try:
         theme_cfg = {
-            'name': None,
-            'custom_dir': os.path.join(os.path.dirname(__file__), '..', args.theme_dir),
-            'language': lang,
-            'direction': 'ltr',
-            'static_templates': ['404.html'],
-            'extra': {
-                'now': int(time.mktime(datetime.datetime.now().timetuple()))  # TODO better way to avoid caching
-            }
+            "name": None,
+            "custom_dir": os.path.join(os.path.dirname(__file__), "..", args.theme_dir),
+            "language": lang,
+            "direction": "ltr",
+            "static_templates": ["404.html"],
+            "extra": {
+                "now": int(
+                    time.mktime(datetime.datetime.now().timetuple())
+                )  # TODO better way to avoid caching
+            },
         }
 
         # the following list of languages is sorted according to
         # https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers
-        languages = {
-            'en': 'English'
-        }
+        languages = {"en": "English"}
 
-        site_names = {
-            'en': 'ClickHouse Blog'
-        }
+        site_names = {"en": "ClickHouse Blog"}
 
         assert len(site_names) == len(languages)
 
         site_dir = os.path.join(args.blog_output_dir, lang)
 
-        plugins = ['macros']
+        plugins = ["macros"]
         if args.htmlproofer:
-            plugins.append('htmlproofer')
+            plugins.append("htmlproofer")
 
-        website_url = 'https://clickhouse.com'
-        site_name = site_names.get(lang, site_names['en'])
+        website_url = "https://clickhouse.com"
+        site_name = site_names.get(lang, site_names["en"])
         blog_nav, post_meta = nav.build_blog_nav(lang, args)
         raw_config = dict(
             site_name=site_name,
-            site_url=f'{website_url}/blog/{lang}/',
+            site_url=f"{website_url}/blog/{lang}/",
             docs_dir=os.path.join(args.blog_dir, lang),
             site_dir=site_dir,
             strict=True,
             theme=theme_cfg,
             nav=blog_nav,
-            copyright='©2016–2022 ClickHouse, Inc.',
+            copyright="©2016–2022 ClickHouse, Inc.",
             use_directory_urls=True,
-            repo_name='ClickHouse/ClickHouse',
-            repo_url='https://github.com/ClickHouse/ClickHouse/',
-            edit_uri=f'edit/master/website/blog/{lang}',
+            repo_name="ClickHouse/ClickHouse",
+            repo_url="https://github.com/ClickHouse/ClickHouse/",
+            edit_uri=f"edit/master/website/blog/{lang}",
             markdown_extensions=mdx_clickhouse.MARKDOWN_EXTENSIONS,
             plugins=plugins,
             extra=dict(
@@ -75,12 +73,12 @@ def build_for_lang(lang, args):
                 website_url=website_url,
                 events=args.events,
                 languages=languages,
-                includes_dir=os.path.join(os.path.dirname(__file__), '..', '_includes'),
+                includes_dir=os.path.join(os.path.dirname(__file__), "..", "_includes"),
                 is_amp=False,
                 is_blog=True,
                 post_meta=post_meta,
-                today=datetime.date.today().isoformat()
-            )
+                today=datetime.date.today().isoformat(),
+            ),
         )
 
         cfg = config.load_config(**raw_config)
@@ -89,21 +87,28 @@ def build_for_lang(lang, args):
         redirects.build_blog_redirects(args)
 
         env = util.init_jinja2_env(args)
-        with open(os.path.join(args.website_dir, 'templates', 'blog', 'rss.xml'), 'rb') as f:
-            rss_template_string = f.read().decode('utf-8').strip()
+        with open(
+            os.path.join(args.website_dir, "templates", "blog", "rss.xml"), "rb"
+        ) as f:
+            rss_template_string = f.read().decode("utf-8").strip()
         rss_template = env.from_string(rss_template_string)
-        with open(os.path.join(args.blog_output_dir, lang, 'rss.xml'), 'w') as f:
-            f.write(rss_template.render({'config': raw_config}))
+        with open(os.path.join(args.blog_output_dir, lang, "rss.xml"), "w") as f:
+            f.write(rss_template.render({"config": raw_config}))
 
-        logging.info(f'Finished building {lang} blog')
+        logging.info(f"Finished building {lang} blog")
 
     except exceptions.ConfigurationError as e:
-        raise SystemExit('
' + str(e))
+        raise SystemExit("
" + str(e))
 
 
 def build_blog(args):
     tasks = []
-    for lang in args.blog_lang.split(','):
+    for lang in args.blog_lang.split(","):
         if lang:
-            tasks.append((lang, args,))
+            tasks.append(
+                (
+                    lang,
+                    args,
+                )
+            )
     util.run_function_in_parallel(build_for_lang, tasks, threads=False)
diff --git a/docs/tools/build.py b/docs/tools/build.py
index e4f6718699ab..612be0229d3d 100755
--- a/docs/tools/build.py
+++ b/docs/tools/build.py
@@ -30,76 +30,76 @@
 
 from cmake_in_clickhouse_generator import generate_cmake_flags_files
 
+
 class ClickHouseMarkdown(markdown.extensions.Extension):
     class ClickHousePreprocessor(markdown.util.Processor):
         def run(self, lines):
             for line in lines:
-                if '<!--hide-->' not in line:
+                if "<!--hide-->" not in line:
                     yield line
 
     def extendMarkdown(self, md):
-        md.preprocessors.register(self.ClickHousePreprocessor(), 'clickhouse_preprocessor', 31)
+        md.preprocessors.register(
+            self.ClickHousePreprocessor(), "clickhouse_preprocessor", 31
+        )
 
 
 markdown.extensions.ClickHouseMarkdown = ClickHouseMarkdown
 
 
 def build_for_lang(lang, args):
-    logging.info(f'Building {lang} docs')
-    os.environ['SINGLE_PAGE'] = '0'
+    logging.info(f"Building {lang} docs")
+    os.environ["SINGLE_PAGE"] = "0"
 
     try:
         theme_cfg = {
-            'name': None,
-            'custom_dir': os.path.join(os.path.dirname(__file__), '..', args.theme_dir),
-            'language': lang,
-            'direction': 'rtl' if lang == 'fa' else 'ltr',
-            'static_templates': ['404.html'],
-            'extra': {
-                'now': int(time.mktime(datetime.datetime.now().timetuple()))  # TODO better way to avoid caching
-            }
+            "name": None,
+            "custom_dir": os.path.join(os.path.dirname(__file__), "..", args.theme_dir),
+            "language": lang,
+            "direction": "rtl" if lang == "fa" else "ltr",
+            "static_templates": ["404.html"],
+            "extra": {
+                "now": int(
+                    time.mktime(datetime.datetime.now().timetuple())
+                )  # TODO better way to avoid caching
+            },
         }
 
         # the following list of languages is sorted according to
         # https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers
-        languages = {
-            'en': 'English',
-            'zh': '中文',
-            'ru': 'Русский',
-            'ja': '日本語'
-        }
+        languages = {"en": "English", "zh": "中文", "ru": "Русский", "ja": "日本語"}
 
         site_names = {
-            'en': 'ClickHouse %s Documentation',
-            'zh': 'ClickHouse文档 %s',
-            'ru': 'Документация ClickHouse %s',
-            'ja': 'ClickHouseドキュメント %s'
+            "en": "ClickHouse %s Documentation",
+            "zh": "ClickHouse文档 %s",
+            "ru": "Документация ClickHouse %s",
+            "ja": "ClickHouseドキュメント %s",
         }
 
         assert len(site_names) == len(languages)
 
         site_dir = os.path.join(args.docs_output_dir, lang)
 
-        plugins = ['macros']
+        plugins = ["macros"]
         if args.htmlproofer:
-            plugins.append('htmlproofer')
+            plugins.append("htmlproofer")
 
-        website_url = 'https://clickhouse.com'
-        site_name = site_names.get(lang, site_names['en']) % ''
-        site_name = site_name.replace('  ', ' ')
+        website_url = "https://clickhouse.com"
+        site_name = site_names.get(lang, site_names["en"]) % ""
+        site_name = site_name.replace("  ", " ")
 
         raw_config = dict(
             site_name=site_name,
-            site_url=f'{website_url}/docs/{lang}/',
+            site_url=f"{website_url}/docs/{lang}/",
             docs_dir=os.path.join(args.docs_dir, lang),
             site_dir=site_dir,
             strict=True,
             theme=theme_cfg,
-            copyright='©2016–2022 ClickHouse, Inc.',
+            copyright="©2016–2022 ClickHouse, Inc.",
             use_directory_urls=True,
-            repo_name='ClickHouse/ClickHouse',
-            repo_url='https://github.com/ClickHouse/ClickHouse/',
-            edit_uri=f'edit/master/docs/{lang}',
+            repo_name="ClickHouse/ClickHouse",
+            repo_url="https://github.com/ClickHouse/ClickHouse/",
+            edit_uri=f"edit/master/docs/{lang}",
             markdown_extensions=mdx_clickhouse.MARKDOWN_EXTENSIONS,
             plugins=plugins,
             extra=dict(
@@ -111,16 +111,16 @@ def build_for_lang(lang, args):
                 website_url=website_url,
                 events=args.events,
                 languages=languages,
-                includes_dir=os.path.join(os.path.dirname(__file__), '..', '_includes'),
+                includes_dir=os.path.join(os.path.dirname(__file__), "..", "_includes"),
                 is_amp=False,
-                is_blog=False
-            )
+                is_blog=False,
+            ),
         )
 
         # Clean to be safe if last build finished abnormally
         single_page.remove_temporary_files(lang, args)
 
-        raw_config['nav'] = nav.build_docs_nav(lang, args)
+        raw_config["nav"] = nav.build_docs_nav(lang, args)
 
         cfg = config.load_config(**raw_config)
 
@@ -131,21 +131,28 @@ def build_for_lang(lang, args):
             amp.build_amp(lang, args, cfg)
 
         if not args.skip_single_page:
-            single_page.build_single_page_version(lang, args, raw_config.get('nav'), cfg)
+            single_page.build_single_page_version(
+                lang, args, raw_config.get("nav"), cfg
+            )
 
         mdx_clickhouse.PatchedMacrosPlugin.disabled = False
 
-        logging.info(f'Finished building {lang} docs')
+        logging.info(f"Finished building {lang} docs")
 
     except exceptions.ConfigurationError as e:
-        raise SystemExit('
' + str(e))
+        raise SystemExit("
" + str(e))
 
 
 def build_docs(args):
     tasks = []
-    for lang in args.lang.split(','):
+    for lang in args.lang.split(","):
         if lang:
-            tasks.append((lang, args,))
+            tasks.append(
+                (
+                    lang,
+                    args,
+                )
+            )
     util.run_function_in_parallel(build_for_lang, tasks, threads=False)
     redirects.build_docs_redirects(args)
 
@@ -171,56 +178,64 @@ def build(args):
         redirects.build_static_redirects(args)
 
 
-if __name__ == '__main__':
-    os.chdir(os.path.join(os.path.dirname(__file__), '..'))
+if __name__ == "__main__":
+    os.chdir(os.path.join(os.path.dirname(__file__), ".."))
 
     # A root path to ClickHouse source code.
-    src_dir = '..'
+    src_dir = ".."
 
-    website_dir = os.path.join(src_dir, 'website')
+    website_dir = os.path.join(src_dir, "website")
 
     arg_parser = argparse.ArgumentParser()
-    arg_parser.add_argument('--lang', default='en,ru,zh,ja')
-    arg_parser.add_argument('--blog-lang', default='en')
-    arg_parser.add_argument('--docs-dir', default='.')
-    arg_parser.add_argument('--theme-dir', default=website_dir)
-    arg_parser.add_argument('--website-dir', default=website_dir)
-    arg_parser.add_argument('--src-dir', default=src_dir)
-    arg_parser.add_argument('--blog-dir', default=os.path.join(website_dir, 'blog'))
-    arg_parser.add_argument('--output-dir', default='build')
-    arg_parser.add_argument('--nav-limit', type=int, default='0')
-    arg_parser.add_argument('--skip-multi-page', action='store_true')
-    arg_parser.add_argument('--skip-single-page', action='store_true')
-    arg_parser.add_argument('--skip-amp', action='store_true')
-    arg_parser.add_argument('--skip-website', action='store_true')
-    arg_parser.add_argument('--skip-blog', action='store_true')
-    arg_parser.add_argument('--skip-git-log', action='store_true')
-    arg_parser.add_argument('--skip-docs', action='store_true')
-    arg_parser.add_argument('--test-only', action='store_true')
-    arg_parser.add_argument('--minify', action='store_true')
-    arg_parser.add_argument('--htmlproofer', action='store_true')
-    arg_parser.add_argument('--no-docs-macros', action='store_true')
-    arg_parser.add_argument('--save-raw-single-page', type=str)
-    arg_parser.add_argument('--livereload', type=int, default='0')
-    arg_parser.add_argument('--verbose', action='store_true')
+    arg_parser.add_argument("--lang", default="en,ru,zh,ja")
+    arg_parser.add_argument("--blog-lang", default="en")
+    arg_parser.add_argument("--docs-dir", default=".")
+    arg_parser.add_argument("--theme-dir", default=website_dir)
+    arg_parser.add_argument("--website-dir", default=website_dir)
+    arg_parser.add_argument("--src-dir", default=src_dir)
+    arg_parser.add_argument("--blog-dir", default=os.path.join(website_dir, "blog"))
+    arg_parser.add_argument("--output-dir", default="build")
+    arg_parser.add_argument("--nav-limit", type=int, default="0")
+    arg_parser.add_argument("--skip-multi-page", action="store_true")
+    arg_parser.add_argument("--skip-single-page", action="store_true")
+    arg_parser.add_argument("--skip-amp", action="store_true")
+    arg_parser.add_argument("--skip-website", action="store_true")
+    arg_parser.add_argument("--skip-blog", action="store_true")
+    arg_parser.add_argument("--skip-git-log", action="store_true")
+    arg_parser.add_argument("--skip-docs", action="store_true")
+    arg_parser.add_argument("--test-only", action="store_true")
+    arg_parser.add_argument("--minify", action="store_true")
+    arg_parser.add_argument("--htmlproofer", action="store_true")
+    arg_parser.add_argument("--no-docs-macros", action="store_true")
+    arg_parser.add_argument("--save-raw-single-page", type=str)
+    arg_parser.add_argument("--livereload", type=int, default="0")
+    arg_parser.add_argument("--verbose", action="store_true")
 
     args = arg_parser.parse_args()
     args.minify = False  # TODO remove
 
     logging.basicConfig(
-        level=logging.DEBUG if args.verbose else logging.INFO,
-        stream=sys.stderr
+        level=logging.DEBUG if args.verbose else logging.INFO, stream=sys.stderr
     )
 
-    logging.getLogger('MARKDOWN').setLevel(logging.INFO)
+    logging.getLogger("MARKDOWN").setLevel(logging.INFO)
 
-    args.docs_output_dir = os.path.join(os.path.abspath(args.output_dir), 'docs')
-    args.blog_output_dir = os.path.join(os.path.abspath(args.output_dir), 'blog')
+    args.docs_output_dir = os.path.join(os.path.abspath(args.output_dir), "docs")
+    args.blog_output_dir = os.path.join(os.path.abspath(args.output_dir), "blog")
 
     from github import get_events
-    args.rev = subprocess.check_output('git rev-parse HEAD', shell=True).decode('utf-8').strip()
-    args.rev_short = subprocess.check_output('git rev-parse --short HEAD', shell=True).decode('utf-8').strip()
-    args.rev_url = f'https://github.com/ClickHouse/ClickHouse/commit/{args.rev}'
+
+    args.rev = (
+        subprocess.check_output("git rev-parse HEAD", shell=True)
+        .decode("utf-8")
+        .strip()
+    )
+    args.rev_short = (
+        subprocess.check_output("git rev-parse --short HEAD", shell=True)
+        .decode("utf-8")
+        .strip()
+    )
+    args.rev_url = f"https://github.com/ClickHouse/ClickHouse/commit/{args.rev}"
     args.events = get_events(args)
 
     if args.test_only:
@@ -233,18 +248,20 @@ def build(args):
         mdx_clickhouse.PatchedMacrosPlugin.skip_git_log = True
 
     from build import build
+
     build(args)
 
     if args.livereload:
-        new_args = [arg for arg in sys.argv if not arg.startswith('--livereload')]
-        new_args = sys.executable + ' ' + ' '.join(new_args)
+        new_args = [arg for arg in sys.argv if not arg.startswith("--livereload")]
+        new_args = sys.executable + " " + " ".join(new_args)
 
         server = livereload.Server()
-        server.watch(args.docs_dir + '**/*', livereload.shell(new_args, cwd='tools', shell=True))
-        server.watch(args.website_dir + '**/*', livereload.shell(new_args, cwd='tools', shell=True))
-        server.serve(
-            root=args.output_dir,
-            host='0.0.0.0',
-            port=args.livereload
+        server.watch(
+            args.docs_dir + "**/*", livereload.shell(new_args, cwd="tools", shell=True)
+        )
+        server.watch(
+            args.website_dir + "**/*",
+            livereload.shell(new_args, cwd="tools", shell=True),
         )
+        server.serve(root=args.output_dir, host="0.0.0.0", port=args.livereload)
         sys.exit(0)
diff --git a/docs/tools/cmake_in_clickhouse_generator.py b/docs/tools/cmake_in_clickhouse_generator.py
index aa4cbbddd189..9bbc94fd2069 100644
--- a/docs/tools/cmake_in_clickhouse_generator.py
+++ b/docs/tools/cmake_in_clickhouse_generator.py
@@ -6,11 +6,13 @@
 Entity = Tuple[str, str, str]
 
 # https://regex101.com/r/R6iogw/12
-cmake_option_regex: str = r"^\s*option\s*\(([A-Z_0-9${}]+)\s*(?:\"((?:.|
)*?)\")?\s*(.*)?\).*$"
+cmake_option_regex: str = (
+    r"^\s*option\s*\(([A-Z_0-9${}]+)\s*(?:\"((?:.|
)*?)\")?\s*(.*)?\).*$"
+)
 
 ch_master_url: str = "https://github.com/clickhouse/clickhouse/blob/master/"
 
-name_str: str = "<a name=\"{anchor}\"></a>[`{name}`](" + ch_master_url + "{path}#L{line})"
+name_str: str = '<a name="{anchor}"></a>[`{name}`](' + ch_master_url + "{path}#L{line})"
 default_anchor_str: str = "[`{name}`](#{anchor})"
 
 comment_var_regex: str = r"\${(.+)}"
@@ -27,11 +29,15 @@
 
 
 def make_anchor(t: str) -> str:
-    return "".join(["-" if i == "_" else i.lower() for i in t if i.isalpha() or i == "_"])
+    return "".join(
+        ["-" if i == "_" else i.lower() for i in t if i.isalpha() or i == "_"]
+    )
+
 
 def process_comment(comment: str) -> str:
     return re.sub(comment_var_regex, comment_var_replace, comment, flags=re.MULTILINE)
 
+
 def build_entity(path: str, entity: Entity, line_comment: Tuple[int, str]) -> None:
     (line, comment) = line_comment
     (name, description, default) = entity
@@ -47,22 +53,22 @@ def build_entity(path: str, entity: Entity, line_comment: Tuple[int, str]) -> No
         formatted_default: str = "`" + default + "`"
 
     formatted_name: str = name_str.format(
-        anchor=make_anchor(name),
-        name=name,
-        path=path,
-        line=line)
+        anchor=make_anchor(name), name=name, path=path, line=line
+    )
 
     formatted_description: str = "".join(description.split("
"))
 
     formatted_comment: str = process_comment(comment)
 
     formatted_entity: str = "| {} | {} | {} | {} |".format(
-        formatted_name, formatted_default, formatted_description, formatted_comment)
+        formatted_name, formatted_default, formatted_description, formatted_comment
+    )
 
     entities[name] = path, formatted_entity
 
+
 def process_file(root_path: str, file_path: str, file_name: str) -> None:
-    with open(os.path.join(file_path, file_name), 'r') as cmake_file:
+    with open(os.path.join(file_path, file_name), "r") as cmake_file:
         contents: str = cmake_file.read()
 
         def get_line_and_comment(target: str) -> Tuple[int, str]:
@@ -70,10 +76,10 @@ def get_line_and_comment(target: str) -> Tuple[int, str]:
             comment: str = ""
 
             for n, line in enumerate(contents_list):
-                if 'option' not in line.lower() or target not in line:
+                if "option" not in line.lower() or target not in line:
                     continue
 
-                for maybe_comment_line in contents_list[n - 1::-1]:
+                for maybe_comment_line in contents_list[n - 1 :: -1]:
                     if not re.match("\s*#\s*", maybe_comment_line):
                         break
 
@@ -82,16 +88,22 @@ def get_line_and_comment(target: str) -> Tuple[int, str]:
                 # line numbering starts with 1
                 return n + 1, comment
 
-        matches: Optional[List[Entity]] = re.findall(cmake_option_regex, contents, re.MULTILINE)
-
+        matches: Optional[List[Entity]] = re.findall(
+            cmake_option_regex, contents, re.MULTILINE
+        )
 
-        file_rel_path_with_name: str = os.path.join(file_path[len(root_path):], file_name)
-        if file_rel_path_with_name.startswith('/'):
+        file_rel_path_with_name: str = os.path.join(
+            file_path[len(root_path) :], file_name
+        )
+        if file_rel_path_with_name.startswith("/"):
             file_rel_path_with_name = file_rel_path_with_name[1:]
 
         if matches:
             for entity in matches:
-                build_entity(file_rel_path_with_name, entity, get_line_and_comment(entity[0]))
+                build_entity(
+                    file_rel_path_with_name, entity, get_line_and_comment(entity[0])
+                )
+
 
 def process_folder(root_path: str, name: str) -> None:
     for root, _, files in os.walk(os.path.join(root_path, name)):
@@ -99,12 +111,19 @@ def process_folder(root_path: str, name: str) -> None:
             if f == "CMakeLists.txt" or ".cmake" in f:
                 process_file(root_path, root, f)
 
-def generate_cmake_flags_files() -> None:
-    root_path: str = os.path.join(os.path.dirname(__file__), '..', '..')
 
-    output_file_name: str = os.path.join(root_path, "docs/en/development/cmake-in-clickhouse.md")
-    header_file_name: str = os.path.join(root_path, "docs/_includes/cmake_in_clickhouse_header.md")
-    footer_file_name: str = os.path.join(root_path, "docs/_includes/cmake_in_clickhouse_footer.md")
+def generate_cmake_flags_files() -> None:
+    root_path: str = os.path.join(os.path.dirname(__file__), "..", "..")
+
+    output_file_name: str = os.path.join(
+        root_path, "docs/en/development/cmake-in-clickhouse.md"
+    )
+    header_file_name: str = os.path.join(
+        root_path, "docs/_includes/cmake_in_clickhouse_header.md"
+    )
+    footer_file_name: str = os.path.join(
+        root_path, "docs/_includes/cmake_in_clickhouse_footer.md"
+    )
 
     process_file(root_path, root_path, "CMakeLists.txt")
     process_file(root_path, os.path.join(root_path, "programs"), "CMakeLists.txt")
@@ -127,8 +146,10 @@ def generate_cmake_flags_files() -> None:
                 f.write(entities[k][1] + "
")
                 ignored_keys.append(k)
 
-        f.write("
### External libraries
Note that ClickHouse uses forks of these libraries, see https://github.com/ClickHouse-Extras.
" +
-            table_header)
+        f.write(
+            "
### External libraries
Note that ClickHouse uses forks of these libraries, see https://github.com/ClickHouse-Extras.
"
+            + table_header
+        )
 
         for k in sorted_keys:
             if k.startswith("ENABLE_") and ".cmake" in entities[k][0]:
@@ -143,15 +164,18 @@ def generate_cmake_flags_files() -> None:
         with open(footer_file_name, "r") as footer:
             f.write(footer.read())
 
-        other_languages = ["docs/ja/development/cmake-in-clickhouse.md",
-                           "docs/zh/development/cmake-in-clickhouse.md",
-                           "docs/ru/development/cmake-in-clickhouse.md"]
+        other_languages = [
+            "docs/ja/development/cmake-in-clickhouse.md",
+            "docs/zh/development/cmake-in-clickhouse.md",
+            "docs/ru/development/cmake-in-clickhouse.md",
+        ]
 
         for lang in other_languages:
             other_file_name = os.path.join(root_path, lang)
             if os.path.exists(other_file_name):
-                os.unlink(other_file_name)    
+                os.unlink(other_file_name)
             os.symlink(output_file_name, other_file_name)
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     generate_cmake_flags_files()
diff --git a/docs/tools/easy_diff.py b/docs/tools/easy_diff.py
index 22d305d3da3f..14e3ca917769 100755
--- a/docs/tools/easy_diff.py
+++ b/docs/tools/easy_diff.py
@@ -8,7 +8,7 @@
 from git import cmd
 from tempfile import NamedTemporaryFile
 
-SCRIPT_DESCRIPTION = '''
+SCRIPT_DESCRIPTION = """
     usage: ./easy_diff.py language/document path
 
     Show the difference between a language document and an English document.
@@ -53,16 +53,16 @@
     OPTIONS:
         -h, --help  show this help message and exit
         --no-pager  use stdout as difference result output
-'''
+"""
 
 SCRIPT_PATH = os.path.abspath(__file__)
-CLICKHOUSE_REPO_HOME = os.path.join(os.path.dirname(SCRIPT_PATH), '..', '..')
+CLICKHOUSE_REPO_HOME = os.path.join(os.path.dirname(SCRIPT_PATH), "..", "..")
 SCRIPT_COMMAND_EXECUTOR = cmd.Git(CLICKHOUSE_REPO_HOME)
 
 SCRIPT_COMMAND_PARSER = argparse.ArgumentParser(add_help=False)
-SCRIPT_COMMAND_PARSER.add_argument('path', type=bytes, nargs='?', default=None)
-SCRIPT_COMMAND_PARSER.add_argument('--no-pager', action='store_true', default=False)
-SCRIPT_COMMAND_PARSER.add_argument('-h', '--help', action='store_true', default=False)
+SCRIPT_COMMAND_PARSER.add_argument("path", type=bytes, nargs="?", default=None)
+SCRIPT_COMMAND_PARSER.add_argument("--no-pager", action="store_true", default=False)
+SCRIPT_COMMAND_PARSER.add_argument("-h", "--help", action="store_true", default=False)
 
 
 def execute(commands):
@@ -70,19 +70,41 @@ def execute(commands):
 
 
 def get_hash(file_name):
-    return execute(['git', 'log', '-n', '1', '--pretty=format:"%H"', file_name])
+    return execute(["git", "log", "-n", "1", '--pretty=format:"%H"', file_name])
 
 
 def diff_file(reference_file, working_file, out):
     if not os.path.exists(reference_file):
-        raise RuntimeError('reference file [' + os.path.abspath(reference_file) + '] is not exists.')
+        raise RuntimeError(
+            "reference file [" + os.path.abspath(reference_file) + "] is not exists."
+        )
 
     if os.path.islink(working_file):
         out.writelines(["Need translate document:" + os.path.abspath(reference_file)])
     elif not os.path.exists(working_file):
-        out.writelines(['Need link document ' + os.path.abspath(reference_file) + ' to ' + os.path.abspath(working_file)])
+        out.writelines(
+            [
+                "Need link document "
+                + os.path.abspath(reference_file)
+                + " to "
+                + os.path.abspath(working_file)
+            ]
+        )
     elif get_hash(working_file) != get_hash(reference_file):
-        out.writelines([(execute(['git', 'diff', get_hash(working_file).strip('"'), reference_file]).encode('utf-8'))])
+        out.writelines(
+            [
+                (
+                    execute(
+                        [
+                            "git",
+                            "diff",
+                            get_hash(working_file).strip('"'),
+                            reference_file,
+                        ]
+                    ).encode("utf-8")
+                )
+            ]
+        )
 
     return 0
 
@@ -94,20 +116,30 @@ def diff_directory(reference_directory, working_directory, out):
     for list_item in os.listdir(reference_directory):
         working_item = os.path.join(working_directory, list_item)
         reference_item = os.path.join(reference_directory, list_item)
-        if diff_file(reference_item, working_item, out) if os.path.isfile(reference_item) else diff_directory(reference_item, working_item, out) != 0:
+        if (
+            diff_file(reference_item, working_item, out)
+            if os.path.isfile(reference_item)
+            else diff_directory(reference_item, working_item, out) != 0
+        ):
             return 1
 
     return 0
 
 
-def find_language_doc(custom_document, other_language='en', children=[]):
+def find_language_doc(custom_document, other_language="en", children=[]):
     if len(custom_document) == 0:
-        raise RuntimeError('The ' + os.path.join(custom_document, *children) + " is not in docs directory.")
-
-    if os.path.samefile(os.path.join(CLICKHOUSE_REPO_HOME, 'docs'), custom_document):
-        return os.path.join(CLICKHOUSE_REPO_HOME, 'docs', other_language, *children[1:])
+        raise RuntimeError(
+            "The "
+            + os.path.join(custom_document, *children)
+            + " is not in docs directory."
+        )
+
+    if os.path.samefile(os.path.join(CLICKHOUSE_REPO_HOME, "docs"), custom_document):
+        return os.path.join(CLICKHOUSE_REPO_HOME, "docs", other_language, *children[1:])
     children.insert(0, os.path.split(custom_document)[1])
-    return find_language_doc(os.path.split(custom_document)[0], other_language, children)
+    return find_language_doc(
+        os.path.split(custom_document)[0], other_language, children
+    )
 
 
 class ToPager:
@@ -119,7 +151,7 @@ def writelines(self, lines):
 
     def close(self):
         self.temp_named_file.flush()
-        git_pager = execute(['git', 'var', 'GIT_PAGER'])
+        git_pager = execute(["git", "var", "GIT_PAGER"])
         subprocess.check_call([git_pager, self.temp_named_file.name])
         self.temp_named_file.close()
 
@@ -135,12 +167,20 @@ def __init__(self, system_stdout_stream):
         self.system_stdout_stream = system_stdout_stream
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     arguments = SCRIPT_COMMAND_PARSER.parse_args()
     if arguments.help or not arguments.path:
         sys.stdout.write(SCRIPT_DESCRIPTION)
         sys.exit(0)
 
-    working_language = os.path.join(CLICKHOUSE_REPO_HOME, 'docs', arguments.path)
-    with contextlib.closing(ToStdOut(sys.stdout) if arguments.no_pager else ToPager(NamedTemporaryFile('r+'))) as writer:
-        exit(diff_directory(find_language_doc(working_language), working_language, writer))
+    working_language = os.path.join(CLICKHOUSE_REPO_HOME, "docs", arguments.path)
+    with contextlib.closing(
+        ToStdOut(sys.stdout)
+        if arguments.no_pager
+        else ToPager(NamedTemporaryFile("r+"))
+    ) as writer:
+        exit(
+            diff_directory(
+                find_language_doc(working_language), working_language, writer
+            )
+        )
diff --git a/docs/tools/github.py b/docs/tools/github.py
index 465695d15126..3a6f155e25d1 100644
--- a/docs/tools/github.py
+++ b/docs/tools/github.py
@@ -16,27 +16,26 @@
 def get_events(args):
     events = []
     skip = True
-    with open(os.path.join(args.docs_dir, '..', 'README.md')) as f:
+    with open(os.path.join(args.docs_dir, "..", "README.md")) as f:
         for line in f:
             if skip:
-                if 'Upcoming Events' in line:
+                if "Upcoming Events" in line:
                     skip = False
             else:
                 if not line:
                     continue
-                line = line.strip().split('](')
+                line = line.strip().split("](")
                 if len(line) == 2:
-                    tail = line[1].split(') ')
-                    events.append({
-                        'signup_link': tail[0],
-                        'event_name':  line[0].replace('* [', ''),
-                        'event_date':  tail[1].replace('on ', '').replace('.', '')
-                    })
+                    tail = line[1].split(") ")
+                    events.append(
+                        {
+                            "signup_link": tail[0],
+                            "event_name": line[0].replace("* [", ""),
+                            "event_date": tail[1].replace("on ", "").replace(".", ""),
+                        }
+                    )
     return events
 
 
-if __name__ == '__main__':
-    logging.basicConfig(
-        level=logging.DEBUG,
-        stream=sys.stderr
-    )
+if __name__ == "__main__":
+    logging.basicConfig(level=logging.DEBUG, stream=sys.stderr)
diff --git a/docs/tools/mdx_clickhouse.py b/docs/tools/mdx_clickhouse.py
index 18ecc890b6ef..6b5a5bb5813d 100755
--- a/docs/tools/mdx_clickhouse.py
+++ b/docs/tools/mdx_clickhouse.py
@@ -16,74 +16,79 @@
 
 
 def slugify(value, separator):
-    return slugify_impl.slugify(value, separator=separator, word_boundary=True, save_order=True)
+    return slugify_impl.slugify(
+        value, separator=separator, word_boundary=True, save_order=True
+    )
 
 
 MARKDOWN_EXTENSIONS = [
-    'mdx_clickhouse',
-    'admonition',
-    'attr_list',
-    'def_list',
-    'codehilite',
-    'nl2br',
-    'sane_lists',
-    'pymdownx.details',
-    'pymdownx.magiclink',
-    'pymdownx.superfences',
-    'extra',
-    {
-        'toc': {
-            'permalink': True,
-            'slugify': slugify
-        }
-    }
+    "mdx_clickhouse",
+    "admonition",
+    "attr_list",
+    "def_list",
+    "codehilite",
+    "nl2br",
+    "sane_lists",
+    "pymdownx.details",
+    "pymdownx.magiclink",
+    "pymdownx.superfences",
+    "extra",
+    {"toc": {"permalink": True, "slugify": slugify}},
 ]
 
 
 class ClickHouseLinkMixin(object):
-
     def handleMatch(self, m, data):
-        single_page = (os.environ.get('SINGLE_PAGE') == '1')
+        single_page = os.environ.get("SINGLE_PAGE") == "1"
         try:
             el, start, end = super(ClickHouseLinkMixin, self).handleMatch(m, data)
         except IndexError:
             return
 
         if el is not None:
-            href = el.get('href') or ''
-            is_external = href.startswith('http:') or href.startswith('https:')
+            href = el.get("href") or ""
+            is_external = href.startswith("http:") or href.startswith("https:")
             if is_external:
-                if not href.startswith('https://clickhouse.com'):
-                    el.set('rel', 'external nofollow noreferrer')
+                if not href.startswith("https://clickhouse.com"):
+                    el.set("rel", "external nofollow noreferrer")
             elif single_page:
-                if '#' in href:
-                    el.set('href', '#' + href.split('#', 1)[1])
+                if "#" in href:
+                    el.set("href", "#" + href.split("#", 1)[1])
                 else:
-                    el.set('href', '#' + href.replace('/index.md', '/').replace('.md', '/'))
+                    el.set(
+                        "href", "#" + href.replace("/index.md", "/").replace(".md", "/")
+                    )
         return el, start, end
 
 
-class ClickHouseAutolinkPattern(ClickHouseLinkMixin, markdown.inlinepatterns.AutolinkInlineProcessor):
+class ClickHouseAutolinkPattern(
+    ClickHouseLinkMixin, markdown.inlinepatterns.AutolinkInlineProcessor
+):
     pass
 
 
-class ClickHouseLinkPattern(ClickHouseLinkMixin, markdown.inlinepatterns.LinkInlineProcessor):
+class ClickHouseLinkPattern(
+    ClickHouseLinkMixin, markdown.inlinepatterns.LinkInlineProcessor
+):
     pass
 
 
 class ClickHousePreprocessor(markdown.util.Processor):
     def run(self, lines):
         for line in lines:
-            if '<!--hide-->' not in line:
+            if "<!--hide-->" not in line:
                 yield line
 
 
 class ClickHouseMarkdown(markdown.extensions.Extension):
-
     def extendMarkdown(self, md, md_globals):
-        md.preprocessors['clickhouse'] = ClickHousePreprocessor()
-        md.inlinePatterns['link'] = ClickHouseLinkPattern(markdown.inlinepatterns.LINK_RE, md)
-        md.inlinePatterns['autolink'] = ClickHouseAutolinkPattern(markdown.inlinepatterns.AUTOLINK_RE, md)
+        md.preprocessors["clickhouse"] = ClickHousePreprocessor()
+        md.inlinePatterns["link"] = ClickHouseLinkPattern(
+            markdown.inlinepatterns.LINK_RE, md
+        )
+        md.inlinePatterns["autolink"] = ClickHouseAutolinkPattern(
+            markdown.inlinepatterns.AUTOLINK_RE, md
+        )
 
 
 def makeExtension(**kwargs):
@@ -92,10 +97,8 @@ def makeExtension(**kwargs):
 
 def get_translations(dirname, lang):
     import babel.support
-    return babel.support.Translations.load(
-        dirname=dirname,
-        locales=[lang, 'en']
-    )
+
+    return babel.support.Translations.load(dirname=dirname, locales=[lang, "en"])
 
 
 class PatchedMacrosPlugin(macros.plugin.MacrosPlugin):
@@ -104,22 +107,22 @@ class PatchedMacrosPlugin(macros.plugin.MacrosPlugin):
 
     def on_config(self, config):
         super(PatchedMacrosPlugin, self).on_config(config)
-        self.env.comment_start_string = '{##'
-        self.env.comment_end_string = '##}'
-        self.env.loader = jinja2.FileSystemLoader([
-            os.path.join(config.data['site_dir']),
-            os.path.join(config.data['extra']['includes_dir'])
-        ])
+        self.env.comment_start_string = "{##"
+        self.env.comment_end_string = "##}"
+        self.env.loader = jinja2.FileSystemLoader(
+            [
+                os.path.join(config.data["site_dir"]),
+                os.path.join(config.data["extra"]["includes_dir"]),
+            ]
+        )
 
     def on_env(self, env, config, files):
         import util
-        env.add_extension('jinja2.ext.i18n')
-        dirname = os.path.join(config.data['theme'].dirs[0], 'locale')
-        lang = config.data['theme']['language']
-        env.install_gettext_translations(
-            get_translations(dirname, lang),
-            newstyle=True
-        )
+
+        env.add_extension("jinja2.ext.i18n")
+        dirname = os.path.join(config.data["theme"].dirs[0], "locale")
+        lang = config.data["theme"]["language"]
+        env.install_gettext_translations(get_translations(dirname, lang), newstyle=True)
         util.init_jinja2_filters(env)
         return env
 
@@ -130,13 +133,17 @@ def render(self, markdown):
             return markdown
 
     def on_page_markdown(self, markdown, page, config, files):
-        markdown = super(PatchedMacrosPlugin, self).on_page_markdown(markdown, page, config, files)
+        markdown = super(PatchedMacrosPlugin, self).on_page_markdown(
+            markdown, page, config, files
+        )
 
         if os.path.islink(page.file.abs_src_path):
-            lang = config.data['theme']['language']
-            page.canonical_url = page.canonical_url.replace(f'/{lang}/', '/en/', 1)
+            lang = config.data["theme"]["language"]
+            page.canonical_url = page.canonical_url.replace(f"/{lang}/", "/en/", 1)
 
-        if config.data['extra'].get('version_prefix') or config.data['extra'].get('single_page'):
+        if config.data["extra"].get("version_prefix") or config.data["extra"].get(
+            "single_page"
+        ):
             return markdown
         if self.skip_git_log:
             return markdown
diff --git a/docs/tools/nav.py b/docs/tools/nav.py
index db64d1ba4042..e3df85bbe4e5 100644
--- a/docs/tools/nav.py
+++ b/docs/tools/nav.py
@@ -10,57 +10,59 @@
 
 
 def find_first_header(content):
-    for line in content.split('
'):
-        if line.startswith('#'):
-            no_hash = line.lstrip('#')
-            return no_hash.split('{', 1)[0].strip()
+    for line in content.split("
"):
+        if line.startswith("#"):
+            no_hash = line.lstrip("#")
+            return no_hash.split("{", 1)[0].strip()
 
 
 def build_nav_entry(root, args):
-    if root.endswith('images'):
+    if root.endswith("images"):
         return None, None, None
     result_items = []
-    index_meta, index_content = util.read_md_file(os.path.join(root, 'index.md'))
-    current_title = index_meta.get('toc_folder_title', index_meta.get('toc_title'))
-    current_title = current_title or index_meta.get('title', find_first_header(index_content))
+    index_meta, index_content = util.read_md_file(os.path.join(root, "index.md"))
+    current_title = index_meta.get("toc_folder_title", index_meta.get("toc_title"))
+    current_title = current_title or index_meta.get(
+        "title", find_first_header(index_content)
+    )
     for filename in os.listdir(root):
         path = os.path.join(root, filename)
         if os.path.isdir(path):
             prio, title, payload = build_nav_entry(path, args)
             if title and payload:
                 result_items.append((prio, title, payload))
-        elif filename.endswith('.md'):
+        elif filename.endswith(".md"):
             path = os.path.join(root, filename)
 
-            meta = ''
-            content = ''
+            meta = ""
+            content = ""
 
             try:
                 meta, content = util.read_md_file(path)
             except:
-                print('Error in file: {}'.format(path))
+                print("Error in file: {}".format(path))
                 raise
 
-            path = path.split('/', 2)[-1]
-            title = meta.get('toc_title', find_first_header(content))
+            path = path.split("/", 2)[-1]
+            title = meta.get("toc_title", find_first_header(content))
             if title:
-                title = title.strip().rstrip('.')
+                title = title.strip().rstrip(".")
             else:
-                title = meta.get('toc_folder_title', 'hidden')
-            prio = meta.get('toc_priority', 9999)
-            logging.debug(f'Nav entry: {prio}, {title}, {path}')
-            if meta.get('toc_hidden') or not content.strip():
-                title = 'hidden'
-            if title == 'hidden':
-                title = 'hidden-' + hashlib.sha1(content.encode('utf-8')).hexdigest()
+                title = meta.get("toc_folder_title", "hidden")
+            prio = meta.get("toc_priority", 9999)
+            logging.debug(f"Nav entry: {prio}, {title}, {path}")
+            if meta.get("toc_hidden") or not content.strip():
+                title = "hidden"
+            if title == "hidden":
+                title = "hidden-" + hashlib.sha1(content.encode("utf-8")).hexdigest()
             if args.nav_limit and len(result_items) >= args.nav_limit:
                 break
             result_items.append((prio, title, path))
     result_items = sorted(result_items, key=lambda x: (x[0], x[1]))
     result = collections.OrderedDict([(item[1], item[2]) for item in result_items])
-    if index_meta.get('toc_hidden_folder'):
-        current_title += '|hidden-folder'
-    return index_meta.get('toc_priority', 10000), current_title, result
+    if index_meta.get("toc_hidden_folder"):
+        current_title += "|hidden-folder"
+    return index_meta.get("toc_priority", 10000), current_title, result
 
 
 def build_docs_nav(lang, args):
@@ -70,7 +72,7 @@ def build_docs_nav(lang, args):
     index_key = None
     for key, value in list(nav.items()):
         if key and value:
-            if value == 'index.md':
+            if value == "index.md":
                 index_key = key
                 continue
             result.append({key: value})
@@ -78,7 +80,7 @@ def build_docs_nav(lang, args):
             break
     if index_key:
         key = list(result[0].keys())[0]
-        result[0][key][index_key] = 'index.md'
+        result[0][key][index_key] = "index.md"
         result[0][key].move_to_end(index_key, last=False)
     return result
 
@@ -86,7 +88,7 @@ def build_docs_nav(lang, args):
 def build_blog_nav(lang, args):
     blog_dir = os.path.join(args.blog_dir, lang)
     years = sorted(os.listdir(blog_dir), reverse=True)
-    result_nav = [{'hidden': 'index.md'}]
+    result_nav = [{"hidden": "index.md"}]
     post_meta = collections.OrderedDict()
     for year in years:
         year_dir = os.path.join(blog_dir, year)
@@ -97,38 +99,53 @@ def build_blog_nav(lang, args):
         post_meta_items = []
         for post in os.listdir(year_dir):
             post_path = os.path.join(year_dir, post)
-            if not post.endswith('.md'):
-                raise RuntimeError(f'Unexpected non-md file in posts folder: {post_path}')
+            if not post.endswith(".md"):
+                raise RuntimeError(
+                    f"Unexpected non-md file in posts folder: {post_path}"
+                )
             meta, _ = util.read_md_file(post_path)
-            post_date = meta['date']
-            post_title = meta['title']
+            post_date = meta["date"]
+            post_title = meta["title"]
             if datetime.date.fromisoformat(post_date) > datetime.date.today():
                 continue
             posts.append(
-                (post_date, post_title, os.path.join(year, post),)
+                (
+                    post_date,
+                    post_title,
+                    os.path.join(year, post),
+                )
             )
             if post_title in post_meta:
-                raise RuntimeError(f'Duplicate post title: {post_title}')
-            if not post_date.startswith(f'{year}-'):
-                raise RuntimeError(f'Post date {post_date} doesn\'t match the folder year {year}: {post_title}')
-            post_url_part = post.replace('.md', '')
-            post_meta_items.append((post_date, {
-                'date': post_date,
-                'title': post_title,
-                'image': meta.get('image'),
-                'url': f'/blog/{lang}/{year}/{post_url_part}/'
-            },))
+                raise RuntimeError(f"Duplicate post title: {post_title}")
+            if not post_date.startswith(f"{year}-"):
+                raise RuntimeError(
+                    f"Post date {post_date} doesn't match the folder year {year}: {post_title}"
+                )
+            post_url_part = post.replace(".md", "")
+            post_meta_items.append(
+                (
+                    post_date,
+                    {
+                        "date": post_date,
+                        "title": post_title,
+                        "image": meta.get("image"),
+                        "url": f"/blog/{lang}/{year}/{post_url_part}/",
+                    },
+                )
+            )
         for _, title, path in sorted(posts, reverse=True):
             result_nav[-1][year][title] = path
-        for _, post_meta_item in sorted(post_meta_items,
-                                        reverse=True,
-                                        key=lambda item: item[0]):
-            post_meta[post_meta_item['title']] = post_meta_item
+        for _, post_meta_item in sorted(
+            post_meta_items, reverse=True, key=lambda item: item[0]
+        ):
+            post_meta[post_meta_item["title"]] = post_meta_item
     return result_nav, post_meta
 
 
 def _custom_get_navigation(files, config):
-    nav_config = config['nav'] or mkdocs.structure.nav.nest_paths(f.src_path for f in files.documentation_pages())
+    nav_config = config["nav"] or mkdocs.structure.nav.nest_paths(
+        f.src_path for f in files.documentation_pages()
+    )
     items = mkdocs.structure.nav._data_to_navigation(nav_config, files, config)
     if not isinstance(items, list):
         items = [items]
@@ -138,19 +155,25 @@ def _custom_get_navigation(files, config):
     mkdocs.structure.nav._add_previous_and_next_links(pages)
     mkdocs.structure.nav._add_parent_links(items)
 
-    missing_from_config = [file for file in files.documentation_pages() if file.page is None]
+    missing_from_config = [
+        file for file in files.documentation_pages() if file.page is None
+    ]
     if missing_from_config:
-        files._files = [file for file in files._files if file not in missing_from_config]
+        files._files = [
+            file for file in files._files if file not in missing_from_config
+        ]
 
     links = mkdocs.structure.nav._get_by_type(items, mkdocs.structure.nav.Link)
     for link in links:
-        scheme, netloc, path, params, query, fragment = mkdocs.structure.nav.urlparse(link.url)
+        scheme, netloc, path, params, query, fragment = mkdocs.structure.nav.urlparse(
+            link.url
+        )
         if scheme or netloc:
             mkdocs.structure.nav.log.debug(
                 "An external link to '{}' is included in "
                 "the 'nav' configuration.".format(link.url)
             )
-        elif link.url.startswith('/'):
+        elif link.url.startswith("/"):
             mkdocs.structure.nav.log.debug(
                 "An absolute path to '{}' is included in the 'nav' configuration, "
                 "which presumably points to an external resource.".format(link.url)
diff --git a/docs/tools/redirects.py b/docs/tools/redirects.py
index 1f0a3bb4b746..5d2223766838 100644
--- a/docs/tools/redirects.py
+++ b/docs/tools/redirects.py
@@ -7,8 +7,9 @@ def write_redirect_html(out_path, to_url):
         os.makedirs(out_dir)
     except OSError:
         pass
-    with open(out_path, 'w') as f:
-        f.write(f'''<!--[if IE 6]> Redirect: {to_url} <![endif]-->
+    with open(out_path, "w") as f:
+        f.write(
+            f"""<!--[if IE 6]> Redirect: {to_url} <![endif]-->
 <!DOCTYPE HTML>
 <html lang="en-US">
     <head>
@@ -22,18 +23,20 @@ def write_redirect_html(out_path, to_url):
     <body>
         If you are not redirected automatically, follow this <a href="{to_url}">link</a>.
     </body>
-</html>''')
+</html>"""
+        )
 
 
 def build_redirect_html(args, base_prefix, lang, output_dir, from_path, to_path):
     out_path = os.path.join(
-        output_dir, lang,
-        from_path.replace('/index.md', '/index.html').replace('.md', '/index.html')
+        output_dir,
+        lang,
+        from_path.replace("/index.md", "/index.html").replace(".md", "/index.html"),
     )
-    target_path = to_path.replace('/index.md', '/').replace('.md', '/')
+    target_path = to_path.replace("/index.md", "/").replace(".md", "/")
 
-    if target_path[0:7] != 'http://' and target_path[0:8] != 'https://':
-        to_url = f'/{base_prefix}/{lang}/{target_path}'
+    if target_path[0:7] != "http://" and target_path[0:8] != "https://":
+        to_url = f"/{base_prefix}/{lang}/{target_path}"
     else:
         to_url = target_path
 
@@ -42,33 +45,48 @@ def build_redirect_html(args, base_prefix, lang, output_dir, from_path, to_path)
 
 
 def build_docs_redirects(args):
-    with open(os.path.join(args.docs_dir, 'redirects.txt'), 'r') as f:
+    with open(os.path.join(args.docs_dir, "redirects.txt"), "r") as f:
         for line in f:
-            for lang in args.lang.split(','):
-                from_path, to_path = line.split(' ', 1)
-                build_redirect_html(args, 'docs', lang, args.docs_output_dir, from_path, to_path)
+            for lang in args.lang.split(","):
+                from_path, to_path = line.split(" ", 1)
+                build_redirect_html(
+                    args, "docs", lang, args.docs_output_dir, from_path, to_path
+                )
 
 
 def build_blog_redirects(args):
-    for lang in args.blog_lang.split(','):
-        redirects_path = os.path.join(args.blog_dir, lang, 'redirects.txt')
+    for lang in args.blog_lang.split(","):
+        redirects_path = os.path.join(args.blog_dir, lang, "redirects.txt")
         if os.path.exists(redirects_path):
-            with open(redirects_path, 'r') as f:
+            with open(redirects_path, "r") as f:
                 for line in f:
-                    from_path, to_path = line.split(' ', 1)
-                    build_redirect_html(args, 'blog', lang, args.blog_output_dir, from_path, to_path)
+                    from_path, to_path = line.split(" ", 1)
+                    build_redirect_html(
+                        args, "blog", lang, args.blog_output_dir, from_path, to_path
+                    )
 
 
 def build_static_redirects(args):
     for static_redirect in [
-        ('benchmark.html', '/benchmark/dbms/'),
-        ('benchmark_hardware.html', '/benchmark/hardware/'),
-        ('tutorial.html', '/docs/en/getting_started/tutorial/',),
-        ('reference_en.html', '/docs/en/single/', ),
-        ('reference_ru.html', '/docs/ru/single/',),
-        ('docs/index.html', '/docs/en/',),
+        ("benchmark.html", "/benchmark/dbms/"),
+        ("benchmark_hardware.html", "/benchmark/hardware/"),
+        (
+            "tutorial.html",
+            "/docs/en/getting_started/tutorial/",
+        ),
+        (
+            "reference_en.html",
+            "/docs/en/single/",
+        ),
+        (
+            "reference_ru.html",
+            "/docs/ru/single/",
+        ),
+        (
+            "docs/index.html",
+            "/docs/en/",
+        ),
     ]:
         write_redirect_html(
-            os.path.join(args.output_dir, static_redirect[0]),
-            static_redirect[1]
+            os.path.join(args.output_dir, static_redirect[0]), static_redirect[1]
         )
diff --git a/docs/tools/single_page.py b/docs/tools/single_page.py
index 3d32ba30a21a..ed285fce9f8f 100644
--- a/docs/tools/single_page.py
+++ b/docs/tools/single_page.py
@@ -12,7 +12,8 @@
 import util
 import website
 
-TEMPORARY_FILE_NAME = 'single.md'
+TEMPORARY_FILE_NAME = "single.md"
+
 
 def recursive_values(item):
     if isinstance(item, dict):
@@ -25,11 +26,14 @@ def recursive_values(item):
         yield item
 
 
-anchor_not_allowed_chars = re.compile(r'[^\w\-]')
+anchor_not_allowed_chars = re.compile(r"[^\w\-]")
+
+
 def generate_anchor_from_path(path):
-    return re.sub(anchor_not_allowed_chars, '-', path)
+    return re.sub(anchor_not_allowed_chars, "-", path)
+
 
-absolute_link = re.compile(r'^https?://')
+absolute_link = re.compile(r"^https?://")
 
 
 def replace_link(match, path):
@@ -40,46 +44,55 @@ def replace_link(match, path):
     if re.search(absolute_link, link):
         return match.group(0)
 
-    if link.endswith('/'):
-        link = link[0:-1] + '.md'
+    if link.endswith("/"):
+        link = link[0:-1] + ".md"
 
-    return '{}(#{})'.format(title, generate_anchor_from_path(os.path.normpath(os.path.join(os.path.dirname(path), link))))
+    return "{}(#{})".format(
+        title,
+        generate_anchor_from_path(
+            os.path.normpath(os.path.join(os.path.dirname(path), link))
+        ),
+    )
 
 
 # Concatenates Markdown files to a single file.
 def concatenate(lang, docs_path, single_page_file, nav):
     lang_path = os.path.join(docs_path, lang)
 
-    proj_config = f'{docs_path}/toc_{lang}.yml'
+    proj_config = f"{docs_path}/toc_{lang}.yml"
     if os.path.exists(proj_config):
         with open(proj_config) as cfg_file:
-            nav = yaml.full_load(cfg_file.read())['nav']
+            nav = yaml.full_load(cfg_file.read())["nav"]
 
     files_to_concatenate = list(recursive_values(nav))
     files_count = len(files_to_concatenate)
-    logging.info(f'{files_count} files will be concatenated into single md-file for {lang}.')
-    logging.debug('Concatenating: ' + ', '.join(files_to_concatenate))
-    assert files_count > 0, f'Empty single-page for {lang}'
+    logging.info(
+        f"{files_count} files will be concatenated into single md-file for {lang}."
+    )
+    logging.debug("Concatenating: " + ", ".join(files_to_concatenate))
+    assert files_count > 0, f"Empty single-page for {lang}"
 
-    link_regexp = re.compile(r'(\[[^\]]+\])\(([^)#]+)(?:#[^\)]+)?\)')
+    link_regexp = re.compile(r"(\[[^\]]+\])\(([^)#]+)(?:#[^\)]+)?\)")
 
     for path in files_to_concatenate:
         try:
             with open(os.path.join(lang_path, path)) as f:
                 # Insert a horizontal ruler. Then insert an anchor that we will link to. Its name will be a path to the .md file.
-                single_page_file.write('
______
<a name="%s"></a>
' % generate_anchor_from_path(path))
+                single_page_file.write(
+                    '
______
<a name="%s"></a>
' % generate_anchor_from_path(path)
+                )
 
                 in_metadata = False
                 for line in f:
                     # Skip YAML metadata.
-                    if line == '---
':
+                    if line == "---
":
                         in_metadata = not in_metadata
                         continue
 
                     if not in_metadata:
                         # Increase the level of headers.
-                        if line.startswith('#'):
-                            line = '#' + line
+                        if line.startswith("#"):
+                            line = "#" + line
 
                         # Replace links within the docs.
 
@@ -87,14 +100,19 @@ def concatenate(lang, docs_path, single_page_file, nav):
                             line = re.sub(
                                 link_regexp,
                                 lambda match: replace_link(match, path),
-                                line)
+                                line,
+                            )
 
                             # If failed to replace the relative link, print to log
                             # But with some exceptions:
                             # - "../src/" -- for cmake-in-clickhouse.md (link to sources)
                             # - "../usr/share" -- changelog entry that has "../usr/share/zoneinfo"
-                            if '../' in line and (not '../usr/share' in line) and (not '../src/' in line):
-                                logging.info('Failed to resolve relative link:')
+                            if (
+                                "../" in line
+                                and (not "../usr/share" in line)
+                                and (not "../src/" in line)
+                            ):
+                                logging.info("Failed to resolve relative link:")
                                 logging.info(path)
                                 logging.info(line)
 
@@ -105,9 +123,11 @@ def concatenate(lang, docs_path, single_page_file, nav):
 
     single_page_file.flush()
 
+
 def get_temporary_file_name(lang, args):
     return os.path.join(args.docs_dir, lang, TEMPORARY_FILE_NAME)
 
+
 def remove_temporary_files(lang, args):
     single_md_path = get_temporary_file_name(lang, args)
     if os.path.exists(single_md_path):
@@ -115,14 +135,14 @@ def remove_temporary_files(lang, args):
 
 
 def build_single_page_version(lang, args, nav, cfg):
-    logging.info(f'Building single page version for {lang}')
-    os.environ['SINGLE_PAGE'] = '1'
-    extra = cfg.data['extra']
-    extra['single_page'] = True
-    extra['is_amp'] = False
+    logging.info(f"Building single page version for {lang}")
+    os.environ["SINGLE_PAGE"] = "1"
+    extra = cfg.data["extra"]
+    extra["single_page"] = True
+    extra["is_amp"] = False
 
     single_md_path = get_temporary_file_name(lang, args)
-    with open(single_md_path, 'w') as single_md:
+    with open(single_md_path, "w") as single_md:
         concatenate(lang, args.docs_dir, single_md, nav)
 
         with util.temp_dir() as site_temp:
@@ -132,72 +152,83 @@ def build_single_page_version(lang, args, nav, cfg):
                 shutil.copytree(docs_src_lang, docs_temp_lang)
                 for root, _, filenames in os.walk(docs_temp_lang):
                     for filename in filenames:
-                        if filename != 'single.md' and filename.endswith('.md'):
+                        if filename != "single.md" and filename.endswith(".md"):
                             os.unlink(os.path.join(root, filename))
 
-                cfg.load_dict({
-                    'docs_dir': docs_temp_lang,
-                    'site_dir': site_temp,
-                    'extra': extra,
-                    'nav': [
-                        {cfg.data.get('site_name'): 'single.md'}
-                    ]
-                })
+                cfg.load_dict(
+                    {
+                        "docs_dir": docs_temp_lang,
+                        "site_dir": site_temp,
+                        "extra": extra,
+                        "nav": [{cfg.data.get("site_name"): "single.md"}],
+                    }
+                )
 
                 if not args.test_only:
                     mkdocs.commands.build.build(cfg)
 
-                    single_page_output_path = os.path.join(args.docs_dir, args.docs_output_dir, lang, 'single')
+                    single_page_output_path = os.path.join(
+                        args.docs_dir, args.docs_output_dir, lang, "single"
+                    )
 
                     if os.path.exists(single_page_output_path):
                         shutil.rmtree(single_page_output_path)
 
                     shutil.copytree(
-                        os.path.join(site_temp, 'single'),
-                        single_page_output_path
+                        os.path.join(site_temp, "single"), single_page_output_path
                     )
 
-                    single_page_index_html = os.path.join(single_page_output_path, 'index.html')
-                    single_page_content_js = os.path.join(single_page_output_path, 'content.js')
+                    single_page_index_html = os.path.join(
+                        single_page_output_path, "index.html"
+                    )
+                    single_page_content_js = os.path.join(
+                        single_page_output_path, "content.js"
+                    )
 
-                    with open(single_page_index_html, 'r') as f:
-                        sp_prefix, sp_js, sp_suffix = f.read().split('<!-- BREAK -->')
+                    with open(single_page_index_html, "r") as f:
+                        sp_prefix, sp_js, sp_suffix = f.read().split("<!-- BREAK -->")
 
-                    with open(single_page_index_html, 'w') as f:
+                    with open(single_page_index_html, "w") as f:
                         f.write(sp_prefix)
                         f.write(sp_suffix)
 
-                    with open(single_page_content_js, 'w') as f:
+                    with open(single_page_content_js, "w") as f:
                         if args.minify:
                             import jsmin
+
                             sp_js = jsmin.jsmin(sp_js)
                         f.write(sp_js)
 
-                logging.info(f'Re-building single page for {lang} pdf/test')
+                logging.info(f"Re-building single page for {lang} pdf/test")
                 with util.temp_dir() as test_dir:
-                    extra['single_page'] = False
-                    cfg.load_dict({
-                        'docs_dir': docs_temp_lang,
-                        'site_dir': test_dir,
-                        'extra': extra,
-                        'nav': [
-                            {cfg.data.get('site_name'): 'single.md'}
-                        ]
-                    })
+                    extra["single_page"] = False
+                    cfg.load_dict(
+                        {
+                            "docs_dir": docs_temp_lang,
+                            "site_dir": test_dir,
+                            "extra": extra,
+                            "nav": [{cfg.data.get("site_name"): "single.md"}],
+                        }
+                    )
                     mkdocs.commands.build.build(cfg)
 
-                    css_in = ' '.join(website.get_css_in(args))
-                    js_in = ' '.join(website.get_js_in(args))
-                    subprocess.check_call(f'cat {css_in} > {test_dir}/css/base.css', shell=True)
-                    subprocess.check_call(f'cat {js_in} > {test_dir}/js/base.js', shell=True)
+                    css_in = " ".join(website.get_css_in(args))
+                    js_in = " ".join(website.get_js_in(args))
+                    subprocess.check_call(
+                        f"cat {css_in} > {test_dir}/css/base.css", shell=True
+                    )
+                    subprocess.check_call(
+                        f"cat {js_in} > {test_dir}/js/base.js", shell=True
+                    )
 
                     if args.save_raw_single_page:
                         shutil.copytree(test_dir, args.save_raw_single_page)
 
-                    logging.info(f'Running tests for {lang}')
+                    logging.info(f"Running tests for {lang}")
                     test.test_single_page(
-                        os.path.join(test_dir, 'single', 'index.html'), lang)
+                        os.path.join(test_dir, "single", "index.html"), lang
+                    )
 
-        logging.info(f'Finished building single page version for {lang}')
+        logging.info(f"Finished building single page version for {lang}")
 
         remove_temporary_files(lang, args)
diff --git a/docs/tools/util.py b/docs/tools/util.py
index 25961561f992..fb2f135c85ee 100644
--- a/docs/tools/util.py
+++ b/docs/tools/util.py
@@ -15,7 +15,7 @@
 
 @contextlib.contextmanager
 def temp_dir():
-    path = tempfile.mkdtemp(dir=os.environ.get('TEMP'))
+    path = tempfile.mkdtemp(dir=os.environ.get("TEMP"))
     try:
         yield path
     finally:
@@ -34,7 +34,7 @@ def cd(new_cwd):
 
 def get_free_port():
     with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
-        s.bind(('', 0))
+        s.bind(("", 0))
         s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
         return s.getsockname()[1]
 
@@ -61,12 +61,12 @@ def read_md_file(path):
     meta_text = []
     content = []
     if os.path.exists(path):
-        with open(path, 'r') as f:
+        with open(path, "r") as f:
             for line in f:
-                if line.startswith('---'):
+                if line.startswith("---"):
                     if in_meta:
                         in_meta = False
-                        meta = yaml.full_load(''.join(meta_text))
+                        meta = yaml.full_load("".join(meta_text))
                     else:
                         in_meta = True
                 else:
@@ -74,7 +74,7 @@ def read_md_file(path):
                         meta_text.append(line)
                     else:
                         content.append(line)
-    return meta, ''.join(content)
+    return meta, "".join(content)
 
 
 def write_md_file(path, meta, content):
@@ -82,13 +82,13 @@ def write_md_file(path, meta, content):
     if not os.path.exists(dirname):
         os.makedirs(dirname)
 
-    with open(path, 'w') as f:
+    with open(path, "w") as f:
         if meta:
-            print('---', file=f)
+            print("---", file=f)
             yaml.dump(meta, f)
-            print('---', file=f)
-            if not content.startswith('
'):
-                print('', file=f)
+            print("---", file=f)
+            if not content.startswith("
"):
+                print("", file=f)
         f.write(content)
 
 
@@ -100,7 +100,7 @@ def represent_ordereddict(dumper, data):
 
         value.append((node_key, node_value))
 
-    return yaml.nodes.MappingNode(u'tag:yaml.org,2002:map', value)
+    return yaml.nodes.MappingNode("tag:yaml.org,2002:map", value)
 
 
 yaml.add_representer(collections.OrderedDict, represent_ordereddict)
@@ -109,30 +109,31 @@ def represent_ordereddict(dumper, data):
 def init_jinja2_filters(env):
     import amp
     import website
+
     chunk_size = 10240
-    env.filters['chunks'] = lambda line: [line[i:i + chunk_size] for i in range(0, len(line), chunk_size)]
-    env.filters['html_to_amp'] = amp.html_to_amp
-    env.filters['adjust_markdown_html'] = website.adjust_markdown_html
-    env.filters['to_rfc882'] = lambda d: datetime.datetime.strptime(d, '%Y-%m-%d').strftime('%a, %d %b %Y %H:%M:%S GMT')
+    env.filters["chunks"] = lambda line: [
+        line[i : i + chunk_size] for i in range(0, len(line), chunk_size)
+    ]
+    env.filters["html_to_amp"] = amp.html_to_amp
+    env.filters["adjust_markdown_html"] = website.adjust_markdown_html
+    env.filters["to_rfc882"] = lambda d: datetime.datetime.strptime(
+        d, "%Y-%m-%d"
+    ).strftime("%a, %d %b %Y %H:%M:%S GMT")
 
 
 def init_jinja2_env(args):
     import mdx_clickhouse
+
     env = jinja2.Environment(
-        loader=jinja2.FileSystemLoader([
-            args.website_dir,
-            os.path.join(args.docs_dir, '_includes')
-        ]),
-        extensions=[
-            'jinja2.ext.i18n',
-            'jinja2_highlight.HighlightExtension'
-        ]
+        loader=jinja2.FileSystemLoader(
+            [args.website_dir, os.path.join(args.docs_dir, "_includes")]
+        ),
+        extensions=["jinja2.ext.i18n", "jinja2_highlight.HighlightExtension"],
     )
-    env.extend(jinja2_highlight_cssclass='syntax p-3 my-3')
-    translations_dir = os.path.join(args.website_dir, 'locale')
+    env.extend(jinja2_highlight_cssclass="syntax p-3 my-3")
+    translations_dir = os.path.join(args.website_dir, "locale")
     env.install_gettext_translations(
-        mdx_clickhouse.get_translations(translations_dir, 'en'),
-        newstyle=True
+        mdx_clickhouse.get_translations(translations_dir, "en"), newstyle=True
     )
     init_jinja2_filters(env)
     return env
diff --git a/docs/tools/website.py b/docs/tools/website.py
index de4cc14670c8..2c748d96414c 100644
--- a/docs/tools/website.py
+++ b/docs/tools/website.py
@@ -17,108 +17,112 @@
 
 
 def handle_iframe(iframe, soup):
-    allowed_domains = ['https://www.youtube.com/', 'https://datalens.yandex/']
+    allowed_domains = ["https://www.youtube.com/", "https://datalens.yandex/"]
     illegal_domain = True
-    iframe_src = iframe.attrs['src']
+    iframe_src = iframe.attrs["src"]
     for domain in allowed_domains:
         if iframe_src.startswith(domain):
             illegal_domain = False
             break
     if illegal_domain:
-        raise RuntimeError(f'iframe from illegal domain: {iframe_src}')
-    wrapper = soup.new_tag('div')
-    wrapper.attrs['class'] = ['embed-responsive', 'embed-responsive-16by9']
+        raise RuntimeError(f"iframe from illegal domain: {iframe_src}")
+    wrapper = soup.new_tag("div")
+    wrapper.attrs["class"] = ["embed-responsive", "embed-responsive-16by9"]
     iframe.insert_before(wrapper)
     iframe.extract()
     wrapper.insert(0, iframe)
-    if 'width' in iframe.attrs:
-        del iframe.attrs['width']
-    if 'height' in iframe.attrs:
-        del iframe.attrs['height']
-    iframe.attrs['allow'] = 'accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture'
-    iframe.attrs['class'] = 'embed-responsive-item'
-    iframe.attrs['frameborder'] = '0'
-    iframe.attrs['allowfullscreen'] = '1'
+    if "width" in iframe.attrs:
+        del iframe.attrs["width"]
+    if "height" in iframe.attrs:
+        del iframe.attrs["height"]
+    iframe.attrs[
+        "allow"
+    ] = "accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
+    iframe.attrs["class"] = "embed-responsive-item"
+    iframe.attrs["frameborder"] = "0"
+    iframe.attrs["allowfullscreen"] = "1"
 
 
 def adjust_markdown_html(content):
-    soup = bs4.BeautifulSoup(
-        content,
-        features='html.parser'
-    )
-
-    for a in soup.find_all('a'):
-        a_class = a.attrs.get('class')
-        a_href = a.attrs.get('href')
-        if a_class and 'headerlink' in a_class:
-            a.string = '\xa0'
-        if a_href and a_href.startswith('http'):
-            a.attrs['target'] = '_blank'
-
-    for code in soup.find_all('code'):
-        code_class = code.attrs.get('class')
+    soup = bs4.BeautifulSoup(content, features="html.parser")
+
+    for a in soup.find_all("a"):
+        a_class = a.attrs.get("class")
+        a_href = a.attrs.get("href")
+        if a_class and "headerlink" in a_class:
+            a.string = "\xa0"
+        if a_href and a_href.startswith("http"):
+            a.attrs["target"] = "_blank"
+
+    for code in soup.find_all("code"):
+        code_class = code.attrs.get("class")
         if code_class:
-            code.attrs['class'] = code_class + ['syntax']
+            code.attrs["class"] = code_class + ["syntax"]
         else:
-            code.attrs['class'] = 'syntax'
+            code.attrs["class"] = "syntax"
 
-    for iframe in soup.find_all('iframe'):
+    for iframe in soup.find_all("iframe"):
         handle_iframe(iframe, soup)
 
-    for img in soup.find_all('img'):
-        if img.attrs.get('alt') == 'iframe':
-            img.name = 'iframe'
-            img.string = ''
+    for img in soup.find_all("img"):
+        if img.attrs.get("alt") == "iframe":
+            img.name = "iframe"
+            img.string = ""
             handle_iframe(img, soup)
             continue
-        img_class = img.attrs.get('class')
+        img_class = img.attrs.get("class")
         if img_class:
-            img.attrs['class'] = img_class + ['img-fluid']
+            img.attrs["class"] = img_class + ["img-fluid"]
         else:
-            img.attrs['class'] = 'img-fluid'
+            img.attrs["class"] = "img-fluid"
 
-    for details in soup.find_all('details'):
-        for summary in details.find_all('summary'):
+    for details in soup.find_all("details"):
+        for summary in details.find_all("summary"):
             if summary.parent != details:
                 summary.extract()
                 details.insert(0, summary)
 
-    for dd in soup.find_all('dd'):
-        dd_class = dd.attrs.get('class')
+    for dd in soup.find_all("dd"):
+        dd_class = dd.attrs.get("class")
         if dd_class:
-            dd.attrs['class'] = dd_class + ['pl-3']
+            dd.attrs["class"] = dd_class + ["pl-3"]
         else:
-            dd.attrs['class'] = 'pl-3'
+            dd.attrs["class"] = "pl-3"
 
-    for div in soup.find_all('div'):
-        div_class = div.attrs.get('class')
-        is_admonition = div_class and 'admonition' in div.attrs.get('class')
+    for div in soup.find_all("div"):
+        div_class = div.attrs.get("class")
+        is_admonition = div_class and "admonition" in div.attrs.get("class")
         if is_admonition:
-            for a in div.find_all('a'):
-                a_class = a.attrs.get('class')
+            for a in div.find_all("a"):
+                a_class = a.attrs.get("class")
                 if a_class:
-                    a.attrs['class'] = a_class + ['alert-link']
+                    a.attrs["class"] = a_class + ["alert-link"]
                 else:
-                    a.attrs['class'] = 'alert-link'
-
-        for p in div.find_all('p'):
-            p_class = p.attrs.get('class')
-            if is_admonition and p_class and ('admonition-title' in p_class):
-                p.attrs['class'] = p_class + ['alert-heading', 'display-4', 'text-reset', 'mb-2']
+                    a.attrs["class"] = "alert-link"
+
+        for p in div.find_all("p"):
+            p_class = p.attrs.get("class")
+            if is_admonition and p_class and ("admonition-title" in p_class):
+                p.attrs["class"] = p_class + [
+                    "alert-heading",
+                    "display-4",
+                    "text-reset",
+                    "mb-2",
+                ]
 
         if is_admonition:
-            div.attrs['role'] = 'alert'
-            if ('info' in div_class) or ('note' in div_class):
-                mode = 'alert-primary'
-            elif ('attention' in div_class) or ('warning' in div_class):
-                mode = 'alert-warning'
-            elif 'important' in div_class:
-                mode = 'alert-danger'
-            elif 'tip' in div_class:
-                mode = 'alert-info'
+            div.attrs["role"] = "alert"
+            if ("info" in div_class) or ("note" in div_class):
+                mode = "alert-primary"
+            elif ("attention" in div_class) or ("warning" in div_class):
+                mode = "alert-warning"
+            elif "important" in div_class:
+                mode = "alert-danger"
+            elif "tip" in div_class:
+                mode = "alert-info"
             else:
-                mode = 'alert-secondary'
-            div.attrs['class'] = div_class + ['alert', 'pb-0', 'mb-4', mode]
+                mode = "alert-secondary"
+            div.attrs["class"] = div_class + ["alert", "pb-0", "mb-4", mode]
 
     return str(soup)
 
@@ -128,61 +132,63 @@ def minify_html(content):
 
 
 def build_website(args):
-    logging.info('Building website')
+    logging.info("Building website")
     env = util.init_jinja2_env(args)
 
     shutil.copytree(
         args.website_dir,
         args.output_dir,
         ignore=shutil.ignore_patterns(
-            '*.md',
-            '*.sh',
-            '*.css',
-            '*.json',
-            'js/*.js',
-            'build',
-            'docs',
-            'public',
-            'node_modules',
-            'src',
-            'templates',
-            'locale',
-            '.gitkeep'
-        )
+            "*.md",
+            "*.sh",
+            "*.css",
+            "*.json",
+            "js/*.js",
+            "build",
+            "docs",
+            "public",
+            "node_modules",
+            "src",
+            "templates",
+            "locale",
+            ".gitkeep",
+        ),
     )
 
     shutil.copytree(
-        os.path.join(args.website_dir, 'images'),
-        os.path.join(args.output_dir, 'docs', 'images')
+        os.path.join(args.website_dir, "images"),
+        os.path.join(args.output_dir, "docs", "images"),
     )
 
     # This file can be requested to check for available ClickHouse releases.
     shutil.copy2(
-        os.path.join(args.src_dir, 'utils', 'list-versions', 'version_date.tsv'),
-        os.path.join(args.output_dir, 'data', 'version_date.tsv'))
+        os.path.join(args.src_dir, "utils", "list-versions", "version_date.tsv"),
+        os.path.join(args.output_dir, "data", "version_date.tsv"),
+    )
 
     # This file can be requested to install ClickHouse.
     shutil.copy2(
-        os.path.join(args.src_dir, 'docs', '_includes', 'install', 'universal.sh'),
-        os.path.join(args.output_dir, 'data', 'install.sh'))
+        os.path.join(args.src_dir, "docs", "_includes", "install", "universal.sh"),
+        os.path.join(args.output_dir, "data", "install.sh"),
+    )
 
     for root, _, filenames in os.walk(args.output_dir):
         for filename in filenames:
-            if filename == 'main.html':
+            if filename == "main.html":
                 continue
 
             path = os.path.join(root, filename)
-            if not filename.endswith('.html'):
+            if not filename.endswith(".html"):
                 continue
-            logging.info('Processing %s', path)
-            with open(path, 'rb') as f:
-                content = f.read().decode('utf-8')
+            logging.info("Processing %s", path)
+            with open(path, "rb") as f:
+                content = f.read().decode("utf-8")
 
             template = env.from_string(content)
             content = template.render(args.__dict__)
 
-            with open(path, 'wb') as f:
-                f.write(content.encode('utf-8'))
+            with open(path, "wb") as f:
+                f.write(content.encode("utf-8"))
 
 
 def get_css_in(args):
@@ -193,7 +199,7 @@ def get_css_in(args):
         f"'{args.website_dir}/css/blog.css'",
         f"'{args.website_dir}/css/docs.css'",
         f"'{args.website_dir}/css/highlight.css'",
-        f"'{args.website_dir}/css/main.css'"
+        f"'{args.website_dir}/css/main.css'",
     ]
 
 
@@ -207,42 +213,41 @@ def get_js_in(args):
         f"'{args.website_dir}/js/index.js'",
         f"'{args.website_dir}/js/docsearch.js'",
         f"'{args.website_dir}/js/docs.js'",
-        f"'{args.website_dir}/js/main.js'"
+        f"'{args.website_dir}/js/main.js'",
     ]
 
 
 def minify_file(path, css_digest, js_digest):
-    if not (
-        path.endswith('.html') or
-        path.endswith('.css')
-    ):
+    if not (path.endswith(".html") or path.endswith(".css")):
         return
 
-    logging.info('Minifying %s', path)
-    with open(path, 'rb') as f:
-        content = f.read().decode('utf-8')
-    if path.endswith('.html'):
+    logging.info("Minifying %s", path)
+    with open(path, "rb") as f:
+        content = f.read().decode("utf-8")
+    if path.endswith(".html"):
         content = minify_html(content)
-        content = content.replace('base.css?css_digest', f'base.css?{css_digest}')
-        content = content.replace('base.js?js_digest', f'base.js?{js_digest}')
-# TODO: restore cssmin
-#     elif path.endswith('.css'):
-#         content = cssmin.cssmin(content)
-# TODO: restore jsmin
-#     elif path.endswith('.js'):
-#         content = jsmin.jsmin(content)
-    with open(path, 'wb') as f:
-        f.write(content.encode('utf-8'))
+        content = content.replace("base.css?css_digest", f"base.css?{css_digest}")
+        content = content.replace("base.js?js_digest", f"base.js?{js_digest}")
+    # TODO: restore cssmin
+    #     elif path.endswith('.css'):
+    #         content = cssmin.cssmin(content)
+    # TODO: restore jsmin
+    #     elif path.endswith('.js'):
+    #         content = jsmin.jsmin(content)
+    with open(path, "wb") as f:
+        f.write(content.encode("utf-8"))
 
 
 def minify_website(args):
-    css_in = ' '.join(get_css_in(args))
-    css_out = f'{args.output_dir}/docs/css/base.css'
-    os.makedirs(f'{args.output_dir}/docs/css')
+    css_in = " ".join(get_css_in(args))
+    css_out = f"{args.output_dir}/docs/css/base.css"
+    os.makedirs(f"{args.output_dir}/docs/css")
 
     if args.minify and False:  # TODO: return closure
-        command = f"purifycss -w '*algolia*' --min {css_in} '{args.output_dir}/*.html' " \
+        command = (
+            f"purifycss -w '*algolia*' --min {css_in} '{args.output_dir}/*.html' "
             f"'{args.output_dir}/docs/en/**/*.html' '{args.website_dir}/js/**/*.js' > {css_out}"
+        )
         logging.info(css_in)
         logging.info(command)
         output = subprocess.check_output(command, shell=True)
@@ -251,51 +256,60 @@ def minify_website(args):
     else:
         command = f"cat {css_in}"
         output = subprocess.check_output(command, shell=True)
-        with open(css_out, 'wb+') as f:
+        with open(css_out, "wb+") as f:
             f.write(output)
 
-    with open(css_out, 'rb') as f:
+    with open(css_out, "rb") as f:
         css_digest = hashlib.sha3_224(f.read()).hexdigest()[0:8]
 
-    js_in = ' '.join(get_js_in(args))
-    js_out = f'{args.output_dir}/docs/js/base.js'
-    os.makedirs(f'{args.output_dir}/docs/js')
+    js_in = " ".join(get_js_in(args))
+    js_out = f"{args.output_dir}/docs/js/base.js"
+    os.makedirs(f"{args.output_dir}/docs/js")
 
     if args.minify and False:  # TODO: return closure
         js_in = [js[1:-1] for js in js_in]
         closure_args = [
-            '--js', *js_in, '--js_output_file', js_out,
-            '--compilation_level', 'SIMPLE',
-            '--dependency_mode', 'NONE',
-            '--third_party', '--use_types_for_optimization',
-            '--isolation_mode', 'IIFE'
+            "--js",
+            *js_in,
+            "--js_output_file",
+            js_out,
+            "--compilation_level",
+            "SIMPLE",
+            "--dependency_mode",
+            "NONE",
+            "--third_party",
+            "--use_types_for_optimization",
+            "--isolation_mode",
+            "IIFE",
         ]
         logging.info(closure_args)
         if closure.run(*closure_args):
-            raise RuntimeError('failed to run closure compiler')
-        with open(js_out, 'r') as f:
+            raise RuntimeError("failed to run closure compiler")
+        with open(js_out, "r") as f:
             js_content = jsmin.jsmin(f.read())
-        with open(js_out, 'w') as f:
+        with open(js_out, "w") as f:
             f.write(js_content)
 
     else:
         command = f"cat {js_in}"
         output = subprocess.check_output(command, shell=True)
-        with open(js_out, 'wb+') as f:
+        with open(js_out, "wb+") as f:
             f.write(output)
 
-    with open(js_out, 'rb') as f:
+    with open(js_out, "rb") as f:
         js_digest = hashlib.sha3_224(f.read()).hexdigest()[0:8]
         logging.info(js_digest)
 
     if args.minify:
-        logging.info('Minifying website')
+        logging.info("Minifying website")
         with concurrent.futures.ThreadPoolExecutor() as executor:
             futures = []
             for root, _, filenames in os.walk(args.output_dir):
                 for filename in filenames:
                     path = os.path.join(root, filename)
-                    futures.append(executor.submit(minify_file, path, css_digest, js_digest))
+                    futures.append(
+                        executor.submit(minify_file, path, css_digest, js_digest)
+                    )
             for future in futures:
                 exc = future.exception()
                 if exc:
@@ -304,24 +318,28 @@ def minify_website(args):
 
 
 def process_benchmark_results(args):
-    benchmark_root = os.path.join(args.website_dir, 'benchmark')
+    benchmark_root = os.path.join(args.website_dir, "benchmark")
     required_keys = {
-        'dbms': ['result'],
-        'hardware': ['result', 'system', 'system_full', 'kind']
+        "dbms": ["result"],
+        "hardware": ["result", "system", "system_full", "kind"],
     }
-    for benchmark_kind in ['dbms', 'hardware']:
+    for benchmark_kind in ["dbms", "hardware"]:
         results = []
-        results_root = os.path.join(benchmark_root, benchmark_kind, 'results')
+        results_root = os.path.join(benchmark_root, benchmark_kind, "results")
         for result in sorted(os.listdir(results_root)):
             result_file = os.path.join(results_root, result)
-            logging.debug(f'Reading benchmark result from {result_file}')
-            with open(result_file, 'r') as f:
+            logging.debug(f"Reading benchmark result from {result_file}")
+            with open(result_file, "r") as f:
                 result = json.loads(f.read())
                 for item in result:
                     for required_key in required_keys[benchmark_kind]:
-                        assert required_key in item, f'No "{required_key}" in {result_file}'
+                        assert (
+                            required_key in item
+                        ), f'No "{required_key}" in {result_file}'
                 results += result
-        results_js = os.path.join(args.output_dir, 'benchmark', benchmark_kind, 'results.js')
-        with open(results_js, 'w') as f:
+        results_js = os.path.join(
+            args.output_dir, "benchmark", benchmark_kind, "results.js"
+        )
+        with open(results_js, "w") as f:
             data = json.dumps(results)
-            f.write(f'var results = {data};')
+            f.write(f"var results = {data};")
diff --git a/packages/.gitignore b/packages/.gitignore
new file mode 100644
index 000000000000..355164c12651
--- /dev/null
+++ b/packages/.gitignore
@@ -0,0 +1,1 @@
+*/
diff --git a/packages/build b/packages/build
new file mode 100755
index 000000000000..53a7538f80e3
--- /dev/null
+++ b/packages/build
@@ -0,0 +1,156 @@
+#!/usr/bin/env bash
+
+set -e
+
+# Avoid dependency on locale
+LC_ALL=C
+
+# Normalize output directory
+if [ -n "$OUTPUT_DIR" ]; then
+    OUTPUT_DIR=$(realpath -m "$OUTPUT_DIR")
+fi
+
+CUR_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
+cd "$CUR_DIR"
+
+ROOT_DIR=$(readlink -f "$(git rev-parse --show-cdup)")
+
+PKG_ROOT='root'
+
+DEB_ARCH=${DEB_ARCH:-amd64}
+OUTPUT_DIR=${OUTPUT_DIR:-$ROOT_DIR}
+[ -d "${OUTPUT_DIR}" ] || mkdir -p "${OUTPUT_DIR}"
+SANITIZER=${SANITIZER:-""}
+SOURCE=${SOURCE:-$PKG_ROOT}
+
+HELP="${0} [--test] [--rpm] [-h|--help]
+  --test - adds '+test' prefix to version
+  --apk - build APK packages
+  --rpm - build RPM packages
+  --tgz - build tarball package
+  --help - show this help and exit
+
+Used envs:
+  DEB_ARCH='${DEB_ARCH}'
+  OUTPUT_DIR='${OUTPUT_DIR}' - where the artifact will be placed
+  SANITIZER='${SANITIZER}' - if any sanitizer is used, affects version string
+  SOURCE='${SOURCE}' - directory with sources tree
+  VERSION_STRING='${VERSION_STRING}' - the package version to overwrite
+"
+
+if [ -z "${VERSION_STRING}" ]; then
+    # Get CLICKHOUSE_VERSION_STRING from the current git repo
+    eval "$("$ROOT_DIR/tests/ci/version_helper.py" -e)"
+else
+    CLICKHOUSE_VERSION_STRING=${VERSION_STRING}
+fi
+export CLICKHOUSE_VERSION_STRING
+
+
+
+while [[ $1 == --* ]]
+do
+    case "$1" in
+        --test )
+            VERSION_POSTFIX+='+test'
+            shift ;;
+        --apk )
+            MAKE_APK=1
+            shift ;;
+        --rpm )
+            MAKE_RPM=1
+            shift ;;
+        --tgz )
+            MAKE_TGZ=1
+            shift ;;
+        --help )
+            echo "$HELP"
+            exit ;;
+        * )
+            echo "Unknown option $1"
+            exit 2 ;;
+    esac
+done
+
+function deb2tgz {
+    local FILE PKG_NAME PKG_DIR PKG_PATH TARBALL
+    FILE=$1
+    PKG_NAME=${FILE##*/}; PKG_NAME=${PKG_NAME%%_*}
+    PKG_DIR="$PKG_NAME-$CLICKHOUSE_VERSION_STRING"
+    PKG_PATH="$OUTPUT_DIR/$PKG_NAME-$CLICKHOUSE_VERSION_STRING"
+    TARBALL="$OUTPUT_DIR/$PKG_NAME-$CLICKHOUSE_VERSION_STRING-$DEB_ARCH.tgz"
+    rm -rf "$PKG_PATH"
+    dpkg-deb -R "$FILE" "$PKG_PATH"
+    mkdir -p "$PKG_PATH/install"
+    cat > "$PKG_PATH/install/doinst.sh" << 'EOF'
+#!/bin/sh
+set -e
+
+SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
+for filepath in `find $SCRIPTPATH/.. -type f -or -type l | grep -v "\.\./install/"`; do
+    destpath=${filepath##$SCRIPTPATH/..}
+    mkdir -p $(dirname "$destpath")
+    cp -r "$filepath" "$destpath"
+done
+EOF
+    chmod +x "$PKG_PATH/install/doinst.sh"
+    if [ -f "$PKG_PATH/DEBIAN/postinst" ]; then
+        tail +2 "$PKG_PATH/DEBIAN/postinst" > "$PKG_PATH/install/doinst.sh"
+    fi
+    rm -rf "$PKG_PATH/DEBIAN"
+    if [ -f "/usr/bin/pigz" ]; then
+        tar --use-compress-program=pigz -cf "$TARBALL" -C "$OUTPUT_DIR" "$PKG_DIR"
+    else
+        tar -czf "$TARBALL" -C "$OUTPUT_DIR" "$PKG_DIR"
+    fi
+
+    rm -r "$PKG_PATH"
+}
+
+# Build options
+if [ -n "$SANITIZER" ]; then
+    if [[ "$SANITIZER" == "address" ]]; then VERSION_POSTFIX+="+asan"
+    elif [[ "$SANITIZER" == "thread" ]]; then VERSION_POSTFIX+="+tsan"
+    elif [[ "$SANITIZER" == "memory" ]]; then VERSION_POSTFIX+="+msan"
+    elif [[ "$SANITIZER" == "undefined" ]]; then VERSION_POSTFIX+="+ubsan"
+    else
+        echo "Unknown value of SANITIZER variable: $SANITIZER"
+        exit 3
+    fi
+elif [[ $BUILD_TYPE == 'debug' ]]; then
+    VERSION_POSTFIX+="+debug"
+fi
+
+if [[ "$PKG_ROOT" != "$SOURCE" ]]; then
+    # packages are built only from PKG_SOURCE
+    rm -rf "./$PKG_ROOT"
+    ln -sf "$SOURCE" "$PKG_SOURCE"
+fi
+
+CLICKHOUSE_VERSION_STRING+=$VERSION_POSTFIX
+echo -e "
Current version is $CLICKHOUSE_VERSION_STRING"
+
+for config in clickhouse*.yaml; do
+    echo "Building deb package for $config"
+
+    # Preserve package path
+    exec 9>&1
+    PKG_PATH=$(nfpm package --target "$OUTPUT_DIR" --config "$config" --packager deb | tee /dev/fd/9)
+    PKG_PATH=${PKG_PATH##*created package: }
+    exec 9>&-
+
+    if [ -n "$MAKE_APK" ]; then
+      echo "Building apk package for $config"
+      nfpm package --target "$OUTPUT_DIR" --config "$config" --packager apk
+    fi
+    if [ -n "$MAKE_RPM" ]; then
+      echo "Building rpm package for $config"
+      nfpm package --target "$OUTPUT_DIR" --config "$config" --packager rpm
+    fi
+    if [ -n "$MAKE_TGZ" ]; then
+      echo "Building tarball for $config"
+      deb2tgz "$PKG_PATH"
+    fi
+done
+
+# vim: ts=4: sw=4: sts=4: expandtab
diff --git a/packages/clickhouse-client.yaml b/packages/clickhouse-client.yaml
new file mode 100644
index 000000000000..2a1389b66250
--- /dev/null
+++ b/packages/clickhouse-client.yaml
@@ -0,0 +1,57 @@
+# package sources should be placed in ${PWD}/root
+# nfpm should run from the same directory with a config
+name: "clickhouse-client"
+arch: "all"
+platform: "linux"
+version: "${CLICKHOUSE_VERSION_STRING}"
+vendor: "ClickHouse Inc."
+homepage: "https://clickhouse.com"
+license: "Apache"
+section: "database"
+priority: "optional"
+
+replaces:
+- clickhouse-compressor
+conflicts:
+- clickhouse-compressor
+
+maintainer: "ClickHouse Dev Team <packages+linux@clickhouse.com>"
+description: |
+  Client binary for ClickHouse
+    ClickHouse is a column-oriented database management system
+    that allows generating analytical data reports in real time.
+    This package provides clickhouse-client , clickhouse-local and clickhouse-benchmark
+
+overrides:
+  deb:
+    depends:
+    - clickhouse-common-static (= ${CLICKHOUSE_VERSION_STRING})
+  rpm:
+    depends:
+    - clickhouse-common-static = ${CLICKHOUSE_VERSION_STRING}
+
+contents:
+- src: root/etc/clickhouse-client/config.xml
+  dst: /etc/clickhouse-client/config.xml
+  type: config
+- src: root/usr/bin/clickhouse-benchmark
+  dst: /usr/bin/clickhouse-benchmark
+- src: root/usr/bin/clickhouse-compressor
+  dst: /usr/bin/clickhouse-compressor
+- src: root/usr/bin/clickhouse-format
+  dst: /usr/bin/clickhouse-format
+- src: root/usr/bin/clickhouse-client
+  dst: /usr/bin/clickhouse-client
+- src: root/usr/bin/clickhouse-local
+  dst: /usr/bin/clickhouse-local
+- src: root/usr/bin/clickhouse-obfuscator
+  dst: /usr/bin/clickhouse-obfuscator
+# docs
+- src: ../AUTHORS
+  dst: /usr/share/doc/clickhouse-client/AUTHORS
+- src: ../CHANGELOG.md
+  dst: /usr/share/doc/clickhouse-client/CHANGELOG.md
+- src: ../LICENSE
+  dst: /usr/share/doc/clickhouse-client/LICENSE
+- src: ../README.md
+  dst: /usr/share/doc/clickhouse-client/README.md
diff --git a/packages/clickhouse-common-static-dbg.yaml b/packages/clickhouse-common-static-dbg.yaml
new file mode 100644
index 000000000000..12a1594bd301
--- /dev/null
+++ b/packages/clickhouse-common-static-dbg.yaml
@@ -0,0 +1,38 @@
+# package sources should be placed in ${PWD}/root
+# nfpm should run from the same directory with a config
+name: "clickhouse-common-static-dbg"
+arch: "${DEB_ARCH}" # amd64, arm64
+platform: "linux"
+version: "${CLICKHOUSE_VERSION_STRING}"
+vendor: "ClickHouse Inc."
+homepage: "https://clickhouse.com"
+license: "Apache"
+section: "database"
+priority: "optional"
+
+replaces:
+- clickhouse-common-dbg
+conflicts:
+- clickhouse-common-dbg
+
+maintainer: "ClickHouse Dev Team <packages+linux@clickhouse.com>"
+description: |
+  debugging symbols for clickhouse-common-static
+    This package contains the debugging symbols for clickhouse-common.
+
+contents:
+- src: root/usr/lib/debug/usr/bin/clickhouse.debug
+  dst: /usr/lib/debug/usr/bin/clickhouse.debug
+- src: root/usr/lib/debug/usr/bin/clickhouse-odbc-bridge.debug
+  dst: /usr/lib/debug/usr/bin/clickhouse-odbc-bridge.debug
+- src: root/usr/lib/debug/usr/bin/clickhouse-library-bridge.debug
+  dst: /usr/lib/debug/usr/bin/clickhouse-library-bridge.debug
+# docs
+- src: ../AUTHORS
+  dst: /usr/share/doc/clickhouse-common-static-dbg/AUTHORS
+- src: ../CHANGELOG.md
+  dst: /usr/share/doc/clickhouse-common-static-dbg/CHANGELOG.md
+- src: ../LICENSE
+  dst: /usr/share/doc/clickhouse-common-static-dbg/LICENSE
+- src: ../README.md
+  dst: /usr/share/doc/clickhouse-common-static-dbg/README.md
diff --git a/packages/clickhouse-common-static.yaml b/packages/clickhouse-common-static.yaml
new file mode 100644
index 000000000000..269d4318e5e0
--- /dev/null
+++ b/packages/clickhouse-common-static.yaml
@@ -0,0 +1,48 @@
+# package sources should be placed in ${PWD}/root
+# nfpm should run from the same directory with a config
+name: "clickhouse-common-static"
+arch: "${DEB_ARCH}" # amd64, arm64
+platform: "linux"
+version: "${CLICKHOUSE_VERSION_STRING}"
+vendor: "ClickHouse Inc."
+homepage: "https://clickhouse.com"
+license: "Apache"
+section: "database"
+priority: "optional"
+
+replaces:
+- clickhouse-common
+- clickhouse-server-base
+provides:
+- clickhouse-common
+- clickhouse-server-base
+suggests:
+- clickhouse-common-static-dbg
+
+maintainer: "ClickHouse Dev Team <packages+linux@clickhouse.com>"
+description: |
+  Common files for ClickHouse
+    ClickHouse is a column-oriented database management system
+    that allows generating analytical data reports in real time.
+    This package provides common files for both clickhouse server and client
+
+contents:
+- src: root/usr/bin/clickhouse
+  dst: /usr/bin/clickhouse
+- src: root/usr/bin/clickhouse-odbc-bridge
+  dst: /usr/bin/clickhouse-odbc-bridge
+- src: root/usr/bin/clickhouse-library-bridge
+  dst: /usr/bin/clickhouse-library-bridge
+- src: root/usr/bin/clickhouse-extract-from-config
+  dst: /usr/bin/clickhouse-extract-from-config
+- src: root/usr/share/bash-completion/completions
+  dst: /usr/share/bash-completion/completions
+# docs
+- src: ../AUTHORS
+  dst: /usr/share/doc/clickhouse-common-static/AUTHORS
+- src: ../CHANGELOG.md
+  dst: /usr/share/doc/clickhouse-common-static/CHANGELOG.md
+- src: ../LICENSE
+  dst: /usr/share/doc/clickhouse-common-static/LICENSE
+- src: ../README.md
+  dst: /usr/share/doc/clickhouse-common-static/README.md
diff --git a/packages/clickhouse-keeper-dbg.yaml b/packages/clickhouse-keeper-dbg.yaml
new file mode 100644
index 000000000000..2c70b7ad4aa4
--- /dev/null
+++ b/packages/clickhouse-keeper-dbg.yaml
@@ -0,0 +1,28 @@
+# package sources should be placed in ${PWD}/root
+# nfpm should run from the same directory with a config
+name: "clickhouse-keeper-dbg"
+arch: "${DEB_ARCH}" # amd64, arm64
+platform: "linux"
+version: "${CLICKHOUSE_VERSION_STRING}"
+vendor: "ClickHouse Inc."
+homepage: "https://clickhouse.com"
+license: "Apache"
+section: "database"
+priority: "optional"
+maintainer: "ClickHouse Dev Team <packages+linux@clickhouse.com>"
+description: |
+  debugging symbols for clickhouse-keeper
+    This package contains the debugging symbols for clickhouse-keeper.
+
+contents:
+- src: root/usr/lib/debug/usr/bin/clickhouse-keeper.debug
+  dst: /usr/lib/debug/usr/bin/clickhouse-keeper.debug
+# docs
+- src: ../AUTHORS
+  dst: /usr/share/doc/clickhouse-keeper-dbg/AUTHORS
+- src: ../CHANGELOG.md
+  dst: /usr/share/doc/clickhouse-keeper-dbg/CHANGELOG.md
+- src: ../LICENSE
+  dst: /usr/share/doc/clickhouse-keeper-dbg/LICENSE
+- src: ../README.md
+  dst: /usr/share/doc/clickhouse-keeper-dbg/README.md
diff --git a/packages/clickhouse-keeper.yaml b/packages/clickhouse-keeper.yaml
new file mode 100644
index 000000000000..e99ac30f9443
--- /dev/null
+++ b/packages/clickhouse-keeper.yaml
@@ -0,0 +1,40 @@
+# package sources should be placed in ${PWD}/root
+# nfpm should run from the same directory with a config
+name: "clickhouse-keeper"
+arch: "${DEB_ARCH}" # amd64, arm64
+platform: "linux"
+version: "${CLICKHOUSE_VERSION_STRING}"
+vendor: "ClickHouse Inc."
+homepage: "https://clickhouse.com"
+license: "Apache"
+section: "database"
+priority: "optional"
+
+conflicts:
+- clickhouse-server
+depends:
+- adduser
+suggests:
+- clickhouse-keeper-dbg
+
+maintainer: "ClickHouse Dev Team <packages+linux@clickhouse.com>"
+description: |
+  Static clickhouse-keeper binary
+    A stand-alone clickhouse-keeper package
+
+
+contents:
+- src: root/etc/clickhouse-keeper
+  dst: /etc/clickhouse-keeper
+  type: config
+- src: root/usr/bin/clickhouse-keeper
+  dst: /usr/bin/clickhouse-keeper
+# docs
+- src: ../AUTHORS
+  dst: /usr/share/doc/clickhouse-keeper/AUTHORS
+- src: ../CHANGELOG.md
+  dst: /usr/share/doc/clickhouse-keeper/CHANGELOG.md
+- src: ../LICENSE
+  dst: /usr/share/doc/clickhouse-keeper/LICENSE
+- src: ../README.md
+  dst: /usr/share/doc/clickhouse-keeper/README.md
diff --git a/packages/clickhouse-server.init b/packages/clickhouse-server.init
new file mode 100755
index 000000000000..1695f6286b87
--- /dev/null
+++ b/packages/clickhouse-server.init
@@ -0,0 +1,227 @@
+#!/bin/sh
+### BEGIN INIT INFO
+# Provides:          clickhouse-server
+# Default-Start:     2 3 4 5
+# Default-Stop:      0 1 6
+# Should-Start:      $time $network
+# Should-Stop:       $network
+# Short-Description: clickhouse-server daemon
+### END INIT INFO
+#
+# NOTES:
+# - Should-* -- script can start if the listed facilities are missing, unlike Required-*
+#
+# For the documentation [1]:
+#
+#   [1]: https://wiki.debian.org/LSBInitScripts
+
+CLICKHOUSE_USER=clickhouse
+CLICKHOUSE_GROUP=${CLICKHOUSE_USER}
+SHELL=/bin/bash
+PROGRAM=clickhouse-server
+CLICKHOUSE_GENERIC_PROGRAM=clickhouse
+CLICKHOUSE_PROGRAM_ENV=""
+EXTRACT_FROM_CONFIG=${CLICKHOUSE_GENERIC_PROGRAM}-extract-from-config
+CLICKHOUSE_CONFDIR=/etc/$PROGRAM
+CLICKHOUSE_LOGDIR=/var/log/clickhouse-server
+CLICKHOUSE_LOGDIR_USER=root
+CLICKHOUSE_DATADIR=/var/lib/clickhouse
+if [ -d "/var/lock" ]; then
+    LOCALSTATEDIR=/var/lock
+else
+    LOCALSTATEDIR=/run/lock
+fi
+
+if [ ! -d "$LOCALSTATEDIR" ]; then
+    mkdir -p "$LOCALSTATEDIR"
+fi
+
+CLICKHOUSE_BINDIR=/usr/bin
+CLICKHOUSE_CRONFILE=/etc/cron.d/clickhouse-server
+CLICKHOUSE_CONFIG=$CLICKHOUSE_CONFDIR/config.xml
+LOCKFILE=$LOCALSTATEDIR/$PROGRAM
+CLICKHOUSE_PIDDIR=/var/run/$PROGRAM
+CLICKHOUSE_PIDFILE="$CLICKHOUSE_PIDDIR/$PROGRAM.pid"
+# CLICKHOUSE_STOP_TIMEOUT=60 # Disabled by default. Place to /etc/default/clickhouse if you need.
+
+# Some systems lack "flock"
+command -v flock >/dev/null && FLOCK=flock
+
+# Override defaults from optional config file
+test -f /etc/default/clickhouse && . /etc/default/clickhouse
+
+
+die()
+{
+    echo $1 >&2
+    exit 1
+}
+
+
+# Check that configuration file is Ok.
+check_config()
+{
+    if [ -x "$CLICKHOUSE_BINDIR/$EXTRACT_FROM_CONFIG" ]; then
+        su -s $SHELL ${CLICKHOUSE_USER} -c "$CLICKHOUSE_BINDIR/$EXTRACT_FROM_CONFIG --config-file=\"$CLICKHOUSE_CONFIG\" --key=path" >/dev/null || die "Configuration file ${CLICKHOUSE_CONFIG} doesn't parse successfully. Won't restart server. You may use forcerestart if you are sure.";
+    fi
+}
+
+
+initdb()
+{
+    ${CLICKHOUSE_GENERIC_PROGRAM} install --user "${CLICKHOUSE_USER}" --pid-path "${CLICKHOUSE_PIDDIR}" --config-path "${CLICKHOUSE_CONFDIR}" --binary-path "${CLICKHOUSE_BINDIR}"
+}
+
+
+start()
+{
+    ${CLICKHOUSE_GENERIC_PROGRAM} start --user "${CLICKHOUSE_USER}" --pid-path "${CLICKHOUSE_PIDDIR}" --config-path "${CLICKHOUSE_CONFDIR}" --binary-path "${CLICKHOUSE_BINDIR}"
+}
+
+
+stop()
+{
+    ${CLICKHOUSE_GENERIC_PROGRAM} stop --pid-path "${CLICKHOUSE_PIDDIR}"
+}
+
+
+restart()
+{
+    ${CLICKHOUSE_GENERIC_PROGRAM} restart --user "${CLICKHOUSE_USER}" --pid-path "${CLICKHOUSE_PIDDIR}" --config-path "${CLICKHOUSE_CONFDIR}" --binary-path "${CLICKHOUSE_BINDIR}"
+}
+
+
+forcestop()
+{
+    ${CLICKHOUSE_GENERIC_PROGRAM} stop --force --pid-path "${CLICKHOUSE_PIDDIR}"
+}
+
+
+service_or_func()
+{
+    if [ -x "/bin/systemctl" ] && [ -f /etc/systemd/system/clickhouse-server.service ] && [ -d /run/systemd/system ]; then
+        systemctl $1 $PROGRAM
+    else
+        $1
+    fi
+}
+
+forcerestart()
+{
+    forcestop
+    # Should not use 'start' function if systemd active
+    service_or_func start
+}
+
+use_cron()
+{
+    # 1. running systemd
+    if [ -x "/bin/systemctl" ] && [ -f /etc/systemd/system/clickhouse-server.service ] && [ -d /run/systemd/system ]; then
+        return 1
+    fi
+    # 2. disabled by config
+    if [ -z "$CLICKHOUSE_CRONFILE" ]; then
+        return 2
+    fi
+    return 0
+}
+# returns false if cron disabled (with systemd)
+enable_cron()
+{
+    use_cron && sed -i 's/^#*//' "$CLICKHOUSE_CRONFILE"
+}
+# returns false if cron disabled (with systemd)
+disable_cron()
+{
+    use_cron && sed -i 's/^#*/#/' "$CLICKHOUSE_CRONFILE"
+}
+
+
+is_cron_disabled()
+{
+    use_cron || return 0
+
+    # Assumes that either no lines are commented or all lines are commented.
+    # Also please note, that currently cron file for ClickHouse has only one line (but some time ago there was more).
+    grep -q -E '^#' "$CLICKHOUSE_CRONFILE";
+}
+
+
+main()
+{
+    # See how we were called.
+    EXIT_STATUS=0
+    case "$1" in
+    start)
+        service_or_func start && enable_cron
+        ;;
+    stop)
+        disable_cron
+        service_or_func stop
+        ;;
+    restart)
+        service_or_func restart && enable_cron
+        ;;
+    forcestop)
+        disable_cron
+        forcestop
+        ;;
+    forcerestart)
+        forcerestart && enable_cron
+        ;;
+    reload)
+        service_or_func restart
+        ;;
+    condstart)
+        service_or_func start
+        ;;
+    condstop)
+        service_or_func stop
+        ;;
+    condrestart)
+        service_or_func restart
+        ;;
+    condreload)
+        service_or_func restart
+        ;;
+    initdb)
+        initdb
+        ;;
+    enable_cron)
+        enable_cron
+        ;;
+    disable_cron)
+        disable_cron
+        ;;
+    *)
+        echo "Usage: $0 {start|stop|status|restart|forcestop|forcerestart|reload|condstart|condstop|condrestart|condreload|initdb}"
+        exit 2
+        ;;
+    esac
+
+    exit $EXIT_STATUS
+}
+
+
+status()
+{
+    ${CLICKHOUSE_GENERIC_PROGRAM} status --pid-path "${CLICKHOUSE_PIDDIR}"
+}
+
+
+# Running commands without need of locking
+case "$1" in
+status)
+    status
+    exit 0
+    ;;
+esac
+
+
+(
+    if $FLOCK -n 9; then
+        main "$@"
+    else
+        echo "Init script is already running" && exit 1
+    fi
+) 9> $LOCKFILE
diff --git a/packages/clickhouse-server.postinstall b/packages/clickhouse-server.postinstall
new file mode 100644
index 000000000000..419c13e3daf1
--- /dev/null
+++ b/packages/clickhouse-server.postinstall
@@ -0,0 +1,47 @@
+#!/bin/sh
+set -e
+# set -x
+
+PROGRAM=clickhouse-server
+CLICKHOUSE_USER=${CLICKHOUSE_USER:=clickhouse}
+CLICKHOUSE_GROUP=${CLICKHOUSE_GROUP:=${CLICKHOUSE_USER}}
+# Please note that we don't support paths with whitespaces. This is rather ignorant.
+CLICKHOUSE_CONFDIR=${CLICKHOUSE_CONFDIR:=/etc/clickhouse-server}
+CLICKHOUSE_DATADIR=${CLICKHOUSE_DATADIR:=/var/lib/clickhouse}
+CLICKHOUSE_LOGDIR=${CLICKHOUSE_LOGDIR:=/var/log/clickhouse-server}
+CLICKHOUSE_BINDIR=${CLICKHOUSE_BINDIR:=/usr/bin}
+CLICKHOUSE_GENERIC_PROGRAM=${CLICKHOUSE_GENERIC_PROGRAM:=clickhouse}
+EXTRACT_FROM_CONFIG=${CLICKHOUSE_GENERIC_PROGRAM}-extract-from-config
+CLICKHOUSE_CONFIG=$CLICKHOUSE_CONFDIR/config.xml
+CLICKHOUSE_PIDDIR=/var/run/$PROGRAM
+
+[ -f /usr/share/debconf/confmodule ] && . /usr/share/debconf/confmodule
+[ -f /etc/default/clickhouse ] && . /etc/default/clickhouse
+
+if [ ! -f "/etc/debian_version" ]; then
+    not_deb_os=1
+fi
+
+if [ "$1" = configure ] || [ -n "$not_deb_os" ]; then
+
+    ${CLICKHOUSE_GENERIC_PROGRAM} install --user "${CLICKHOUSE_USER}" --group "${CLICKHOUSE_GROUP}" --pid-path "${CLICKHOUSE_PIDDIR}" --config-path "${CLICKHOUSE_CONFDIR}" --binary-path "${CLICKHOUSE_BINDIR}" --log-path "${CLICKHOUSE_LOGDIR}" --data-path "${CLICKHOUSE_DATADIR}"
+
+    if [ -x "/bin/systemctl" ] && [ -f /etc/systemd/system/clickhouse-server.service ] && [ -d /run/systemd/system ]; then
+        # if old rc.d service present - remove it
+        if [ -x "/etc/init.d/clickhouse-server" ] && [ -x "/usr/sbin/update-rc.d" ]; then
+            /usr/sbin/update-rc.d clickhouse-server remove
+        fi
+
+        /bin/systemctl daemon-reload
+        /bin/systemctl enable clickhouse-server
+    else
+        # If you downgrading to version older than 1.1.54336 run: systemctl disable clickhouse-server
+        if [ -x "/etc/init.d/clickhouse-server" ]; then
+            if [ -x "/usr/sbin/update-rc.d" ]; then
+                /usr/sbin/update-rc.d clickhouse-server defaults 19 19 >/dev/null || exit $?
+            else
+                echo # Other OS
+            fi
+        fi
+    fi
+fi
diff --git a/packages/clickhouse-server.service b/packages/clickhouse-server.service
new file mode 100644
index 000000000000..a9400b24270f
--- /dev/null
+++ b/packages/clickhouse-server.service
@@ -0,0 +1,27 @@
+[Unit]
+Description=ClickHouse Server (analytic DBMS for big data)
+Requires=network-online.target
+# NOTE: that After/Wants=time-sync.target is not enough, you need to ensure
+# that the time was adjusted already, if you use systemd-timesyncd you are
+# safe, but if you use ntp or some other daemon, you should configure it
+# additionaly.
+After=time-sync.target network-online.target
+Wants=time-sync.target
+
+[Service]
+Type=simple
+User=clickhouse
+Group=clickhouse
+Restart=always
+RestartSec=30
+RuntimeDirectory=clickhouse-server
+ExecStart=/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --pid-file=/run/clickhouse-server/clickhouse-server.pid
+# Minus means that this file is optional.
+EnvironmentFile=-/etc/default/clickhouse
+LimitCORE=infinity
+LimitNOFILE=500000
+CapabilityBoundingSet=CAP_NET_ADMIN CAP_IPC_LOCK CAP_SYS_NICE
+
+[Install]
+# ClickHouse should not start from the rescue shell (rescue.target).
+WantedBy=multi-user.target
diff --git a/packages/clickhouse-server.yaml b/packages/clickhouse-server.yaml
new file mode 100644
index 000000000000..ed56eb27e545
--- /dev/null
+++ b/packages/clickhouse-server.yaml
@@ -0,0 +1,68 @@
+# package sources should be placed in ${PWD}/root
+# nfpm should run from the same directory with a config
+name: "clickhouse-server"
+arch: "all"
+platform: "linux"
+version: "${CLICKHOUSE_VERSION_STRING}"
+vendor: "ClickHouse Inc."
+homepage: "https://clickhouse.com"
+license: "Apache"
+section: "database"
+priority: "optional"
+
+conflicts:
+- clickhouse-keeper
+depends:
+- adduser
+replaces:
+- clickhouse-server-common
+- clickhouse-server-base
+provides:
+- clickhouse-server-common
+recommends:
+- libcap2-bin
+
+maintainer: "ClickHouse Dev Team <packages+linux@clickhouse.com>"
+description: |
+  Server binary for ClickHouse
+    ClickHouse is a column-oriented database management system
+    that allows generating analytical data reports in real time.
+    This package provides clickhouse common configuration files
+
+overrides:
+  deb:
+    depends:
+    - clickhouse-common-static (= ${CLICKHOUSE_VERSION_STRING})
+  rpm:
+    depends:
+    - clickhouse-common-static = ${CLICKHOUSE_VERSION_STRING}
+
+contents:
+- src: root/etc/clickhouse-server
+  dst: /etc/clickhouse-server
+  type: config
+- src: clickhouse-server.init
+  dst: /etc/init.d/clickhouse-server
+- src: clickhouse-server.service
+  dst: /lib/systemd/system/clickhouse-server.service
+- src: root/usr/bin/clickhouse-copier
+  dst: /usr/bin/clickhouse-copier
+- src: clickhouse
+  dst: /usr/bin/clickhouse-keeper
+  type: symlink
+- src: root/usr/bin/clickhouse-report
+  dst: /usr/bin/clickhouse-report
+- src: root/usr/bin/clickhouse-server
+  dst: /usr/bin/clickhouse-server
+# docs
+- src: ../AUTHORS
+  dst: /usr/share/doc/clickhouse-server/AUTHORS
+- src: ../CHANGELOG.md
+  dst: /usr/share/doc/clickhouse-server/CHANGELOG.md
+- src: ../LICENSE
+  dst: /usr/share/doc/clickhouse-server/LICENSE
+- src: ../README.md
+  dst: /usr/share/doc/clickhouse-server/README.md
+
+scripts:
+  postinstall: ./clickhouse-server.postinstall
diff --git a/programs/CMakeLists.txt b/programs/CMakeLists.txt
index 0890b9c95d30..cca7be97b61b 100644
--- a/programs/CMakeLists.txt
+++ b/programs/CMakeLists.txt
@@ -460,10 +460,6 @@ else ()
         list(APPEND CLICKHOUSE_BUNDLE clickhouse-keeper-converter)
     endif ()
 
-    if (NOT BUILD_STRIPPED_BINARIES_PREFIX)
-        install (TARGETS clickhouse RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT clickhouse)
-    endif()
-
     add_custom_target (clickhouse-bundle ALL DEPENDS ${CLICKHOUSE_BUNDLE})
 
     if (USE_GDB_ADD_INDEX)
@@ -474,13 +470,14 @@ else ()
         add_custom_command(TARGET clickhouse POST_BUILD COMMAND ./clickhouse hash-binary > hash && ${OBJCOPY_PATH} --add-section .note.ClickHouse.hash=hash clickhouse COMMENT "Adding .note.ClickHouse.hash to clickhouse" VERBATIM)
     endif()
 
-    if (BUILD_STRIPPED_BINARIES_PREFIX)
-        clickhouse_strip_binary(TARGET clickhouse DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/${BUILD_STRIPPED_BINARIES_PREFIX} BINARY_PATH clickhouse)
+    if (INSTALL_STRIPPED_BINARIES)
+        clickhouse_strip_binary(TARGET clickhouse DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/${STRIPPED_BINARIES_OUTPUT} BINARY_PATH clickhouse)
+    else()
+        clickhouse_make_empty_debug_info_for_nfpm(TARGET clickhouse DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/${STRIPPED_BINARIES_OUTPUT})
+        install (TARGETS clickhouse RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT clickhouse)
     endif()
 endif()
 
-
-
 if (ENABLE_TESTS)
     set (CLICKHOUSE_UNIT_TESTS_TARGETS unit_tests_dbms)
     add_custom_target (clickhouse-tests ALL DEPENDS ${CLICKHOUSE_UNIT_TESTS_TARGETS})
diff --git a/programs/keeper/CMakeLists.txt b/programs/keeper/CMakeLists.txt
index 92bb5dc45a38..b82b13d9607a 100644
--- a/programs/keeper/CMakeLists.txt
+++ b/programs/keeper/CMakeLists.txt
@@ -71,17 +71,11 @@ if (BUILD_STANDALONE_KEEPER)
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressedReadBuffer.cpp
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressedReadBufferFromFile.cpp
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressedWriteBuffer.cpp
-        ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionCodecDelta.cpp
-        ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionCodecDoubleDelta.cpp
-        ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionCodecEncrypted.cpp
-        ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionCodecGorilla.cpp
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionCodecLZ4.cpp
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionCodecMultiple.cpp
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionCodecNone.cpp
-        ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionCodecT64.cpp
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionCodecZSTD.cpp
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/CompressionFactory.cpp
-        ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/getCompressionCodecForFile.cpp
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/ICompressionCodec.cpp
         ${CMAKE_CURRENT_SOURCE_DIR}/../../src/Compression/LZ4_decompress_faster.cpp
 
@@ -137,5 +131,10 @@ if (BUILD_STANDALONE_KEEPER)
     add_dependencies(clickhouse-keeper clickhouse_keeper_configs)
     set_target_properties(clickhouse-keeper PROPERTIES RUNTIME_OUTPUT_DIRECTORY ../)
 
-    install(TARGETS clickhouse-keeper RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT clickhouse)
+    if (INSTALL_STRIPPED_BINARIES)
+        clickhouse_strip_binary(TARGET clickhouse-keeper DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/../${STRIPPED_BINARIES_OUTPUT} BINARY_PATH ../clickhouse-keeper)
+    else()
+        clickhouse_make_empty_debug_info_for_nfpm(TARGET clickhouse-keeper DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/../${STRIPPED_BINARIES_OUTPUT})
+        install(TARGETS clickhouse-keeper RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT clickhouse)
+    endif()
 endif()
diff --git a/programs/library-bridge/CMakeLists.txt b/programs/library-bridge/CMakeLists.txt
index d7e104685c59..90ce3d8be7fd 100644
--- a/programs/library-bridge/CMakeLists.txt
+++ b/programs/library-bridge/CMakeLists.txt
@@ -24,10 +24,9 @@ target_link_libraries(clickhouse-library-bridge PRIVATE
 
 set_target_properties(clickhouse-library-bridge PROPERTIES RUNTIME_OUTPUT_DIRECTORY ..)
 
-if (BUILD_STRIPPED_BINARIES_PREFIX)
-    clickhouse_strip_binary(TARGET clickhouse-library-bridge DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/../${BUILD_STRIPPED_BINARIES_PREFIX} BINARY_PATH ../clickhouse-library-bridge)
-endif()
-
-if (NOT BUILD_STRIPPED_BINARIES_PREFIX)
+if (INSTALL_STRIPPED_BINARIES)
+    clickhouse_strip_binary(TARGET clickhouse-library-bridge DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/../${STRIPPED_BINARIES_OUTPUT} BINARY_PATH ../clickhouse-library-bridge)
+else()
+    clickhouse_make_empty_debug_info_for_nfpm(TARGET clickhouse-library-bridge DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/../${STRIPPED_BINARIES_OUTPUT})
     install(TARGETS clickhouse-library-bridge RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT clickhouse)
 endif()
diff --git a/programs/odbc-bridge/CMakeLists.txt b/programs/odbc-bridge/CMakeLists.txt
index 44493d7ab8a2..b530e08ca265 100644
--- a/programs/odbc-bridge/CMakeLists.txt
+++ b/programs/odbc-bridge/CMakeLists.txt
@@ -39,11 +39,10 @@ if (USE_GDB_ADD_INDEX)
     add_custom_command(TARGET clickhouse-odbc-bridge POST_BUILD COMMAND ${GDB_ADD_INDEX_EXE} ../clickhouse-odbc-bridge COMMENT "Adding .gdb-index to clickhouse-odbc-bridge" VERBATIM)
 endif()
 
-if (BUILD_STRIPPED_BINARIES_PREFIX)
-    clickhouse_strip_binary(TARGET clickhouse-odbc-bridge DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/../${BUILD_STRIPPED_BINARIES_PREFIX} BINARY_PATH ../clickhouse-odbc-bridge)
-endif()
-
-if (NOT BUILD_STRIPPED_BINARIES_PREFIX)
+if (INSTALL_STRIPPED_BINARIES)
+    clickhouse_strip_binary(TARGET clickhouse-odbc-bridge DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/../${STRIPPED_BINARIES_OUTPUT} BINARY_PATH ../clickhouse-odbc-bridge)
+else()
+    clickhouse_make_empty_debug_info_for_nfpm(TARGET clickhouse-odbc-bridge DESTINATION_DIR ${CMAKE_CURRENT_BINARY_DIR}/../${STRIPPED_BINARIES_OUTPUT})
     install(TARGETS clickhouse-odbc-bridge RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT clickhouse)
 endif()
 
diff --git a/programs/server/Server.cpp b/programs/server/Server.cpp
index 1b11453dde49..1b957e6379ed 100644
--- a/programs/server/Server.cpp
+++ b/programs/server/Server.cpp
@@ -1022,8 +1022,8 @@ if (ThreadFuzzer::instance().isEffective())
                         std::make_unique<TCPServer>(
                             new KeeperTCPHandlerFactory(
                                 config_getter, global_context->getKeeperDispatcher(),
-                                global_context->getSettingsRef().receive_timeout,
-                                global_context->getSettingsRef().send_timeout,
+                                global_context->getSettingsRef().receive_timeout.totalSeconds(),
+                                global_context->getSettingsRef().send_timeout.totalSeconds(),
                                 false), server_pool, socket));
                 });
 
@@ -1045,8 +1045,8 @@ if (ThreadFuzzer::instance().isEffective())
                         std::make_unique<TCPServer>(
                             new KeeperTCPHandlerFactory(
                                 config_getter, global_context->getKeeperDispatcher(),
-                                global_context->getSettingsRef().receive_timeout,
-                                global_context->getSettingsRef().send_timeout, true), server_pool, socket));
+                                global_context->getSettingsRef().receive_timeout.totalSeconds(),
+                                global_context->getSettingsRef().send_timeout.totalSeconds(), true), server_pool, socket));
 #else
                     UNUSED(port);
                     throw Exception{"SSL support for TCP protocol is disabled because Poco library was built without NetSSL support.",
diff --git a/src/Common/Dwarf.cpp b/src/Common/Dwarf.cpp
index ce8a0974870c..7dd99b3ffa94 100644
--- a/src/Common/Dwarf.cpp
+++ b/src/Common/Dwarf.cpp
@@ -25,7 +25,6 @@
 #include <Common/Dwarf.h>
 #include <Common/Exception.h>
 
-
 #define DW_CHILDREN_no 0
 #define DW_FORM_addr 1
 #define DW_FORM_block1 0x0a
@@ -124,7 +123,7 @@ const uint32_t kMaxAbbreviationEntries = 1000;
 template <typename T>
 std::enable_if_t<std::is_trivial_v<T> && std::is_standard_layout_v<T>, T> read(std::string_view & sp)
 {
-    SAFE_CHECK(sp.size() >= sizeof(T), "underflow");
+    SAFE_CHECK(sp.size() >= sizeof(T), fmt::format("underflow: expected bytes {}, got bytes {}", sizeof(T), sp.size()));
     T x;
     memcpy(&x, sp.data(), sizeof(T));
     sp.remove_prefix(sizeof(T));
@@ -689,7 +688,7 @@ bool Dwarf::findDebugInfoOffset(uintptr_t address, std::string_view aranges, uin
 
 Dwarf::Die Dwarf::getDieAtOffset(const CompilationUnit & cu, uint64_t offset) const
 {
-    SAFE_CHECK(offset < info_.size(), "unexpected offset");
+    SAFE_CHECK(offset < info_.size(), fmt::format("unexpected offset {}, info size {}", offset, info_.size()));
     Die die;
     std::string_view sp{info_.data() + offset, cu.offset + cu.size - offset};
     die.offset = offset;
@@ -707,19 +706,6 @@ Dwarf::Die Dwarf::getDieAtOffset(const CompilationUnit & cu, uint64_t offset) co
     return die;
 }
 
-Dwarf::Die Dwarf::findDefinitionDie(const CompilationUnit & cu, const Die & die) const
-{
-    // Find the real definition instead of declaration.
-    // DW_AT_specification: Incomplete, non-defining, or separate declaration
-    // corresponding to a declaration
-    auto offset = getAttribute<uint64_t>(cu, die, DW_AT_specification);
-    if (!offset)
-    {
-        return die;
-    }
-    return getDieAtOffset(cu, cu.offset + offset.value());
-}
-
 /**
  * Find the @locationInfo for @address in the compilation unit represented
  * by the @sp .debug_info entry.
@@ -860,7 +846,10 @@ bool Dwarf::findLocation(
                     SymbolizedFrame inline_frame;
                     inline_frame.found = true;
                     inline_frame.addr = address;
-                    inline_frame.name = call_location.name.data();
+                    if (!call_location.name.empty())
+                        inline_frame.name = call_location.name.data();
+                    else
+                        inline_frame.name = nullptr;
                     inline_frame.location.has_file_and_line = true;
                     inline_frame.location.file = call_location.file;
                     inline_frame.location.line = call_location.line;
@@ -1033,17 +1022,54 @@ void Dwarf::findInlinedSubroutineDieForAddress(
         location.file = line_vm.getFullFileName(*call_file);
         location.line = *call_line;
 
+        /// Something wrong with receiving debug info about inline.
+        /// If set to true we stop parsing DWARF.
+        bool die_for_inline_broken = false;
+
         auto get_function_name = [&](const CompilationUnit & srcu, uint64_t die_offset)
         {
-            auto decl_die = getDieAtOffset(srcu, die_offset);
+            Die decl_die = getDieAtOffset(srcu, die_offset);
+            auto & die_to_look_for_name = decl_die;
+
+            Die def_die;
             // Jump to the actual function definition instead of declaration for name
             // and line info.
-            auto def_die = findDefinitionDie(srcu, decl_die);
+            // DW_AT_specification: Incomplete, non-defining, or separate declaration
+            // corresponding to a declaration
+            auto offset = getAttribute<uint64_t>(srcu, decl_die, DW_AT_specification);
+            if (offset)
+            {
+                /// FIXME: actually it's a bug in our DWARF parser.
+                ///
+                /// Most of the times compilation unit offset (srcu.offset) is some big number inside .debug_info (like 434782255).
+                /// Offset of DIE definition is some small relative number to srcu.offset (like 3518).
+                /// However in some unknown cases offset looks like global, non relative number (like 434672579) and in this
+                /// case we obviously doing something wrong parsing DWARF.
+                ///
+                /// What is important -- this bug? reproduces only with -flto=thin in release mode.
+                /// Also llvm-dwarfdump --verify ./clickhouse says that our DWARF is ok, so it's another prove
+                /// that we just doing something wrong.
+                ///
+                /// FIXME: Currently we just give up parsing DWARF for inlines when we got into this situation.
+                if (srcu.offset + offset.value() >= info_.size())
+                {
+                    die_for_inline_broken = true;
+                }
+                else
+                {
+                    def_die = getDieAtOffset(srcu, srcu.offset + offset.value());
+                    die_to_look_for_name = def_die;
+                }
+            }
 
             std::string_view name;
+
+            if (die_for_inline_broken)
+                return name;
+
             // The file and line will be set in the next inline subroutine based on
             // its DW_AT_call_file and DW_AT_call_line.
-            forEachAttribute(srcu, def_die, [&](const Attribute & attr)
+            forEachAttribute(srcu, die_to_look_for_name, [&](const Attribute & attr)
             {
                 switch (attr.spec.name)
                 {
@@ -1082,6 +1108,10 @@ void Dwarf::findInlinedSubroutineDieForAddress(
             ? get_function_name(cu, cu.offset + *abstract_origin)
             : get_function_name(findCompilationUnit(info_, *abstract_origin), *abstract_origin);
 
+        /// FIXME: see comment above
+        if (die_for_inline_broken)
+            return false;
+
         locations.push_back(location);
 
         findInlinedSubroutineDieForAddress(cu, child_die, line_vm, address, base_addr_cu, locations, max_size);
diff --git a/src/Common/Dwarf.h b/src/Common/Dwarf.h
index e57e58e438a4..3d8a50236ff9 100644
--- a/src/Common/Dwarf.h
+++ b/src/Common/Dwarf.h
@@ -260,11 +260,6 @@ class Dwarf final
     /** cu must exist during the life cycle of created detail::Die. */
     Die getDieAtOffset(const CompilationUnit & cu, uint64_t offset) const;
 
-    /**
-     * Find the actual definition DIE instead of declaration for the given die.
-     */
-    Die findDefinitionDie(const CompilationUnit & cu, const Die & die) const;
-
     bool findLocation(
         uintptr_t address,
         LocationInfoMode mode,
diff --git a/src/Compression/CompressionFactory.cpp b/src/Compression/CompressionFactory.cpp
index ca5e5176d13c..abf5e38a8c33 100644
--- a/src/Compression/CompressionFactory.cpp
+++ b/src/Compression/CompressionFactory.cpp
@@ -165,25 +165,36 @@ void registerCodecNone(CompressionCodecFactory & factory);
 void registerCodecLZ4(CompressionCodecFactory & factory);
 void registerCodecLZ4HC(CompressionCodecFactory & factory);
 void registerCodecZSTD(CompressionCodecFactory & factory);
+void registerCodecMultiple(CompressionCodecFactory & factory);
+
+
+/// Keeper use only general-purpose codecs, so we don't need these special codecs
+/// in standalone build
+#ifndef KEEPER_STANDALONE_BUILD
+
 void registerCodecDelta(CompressionCodecFactory & factory);
 void registerCodecT64(CompressionCodecFactory & factory);
 void registerCodecDoubleDelta(CompressionCodecFactory & factory);
 void registerCodecGorilla(CompressionCodecFactory & factory);
 void registerCodecEncrypted(CompressionCodecFactory & factory);
-void registerCodecMultiple(CompressionCodecFactory & factory);
+
+#endif
 
 CompressionCodecFactory::CompressionCodecFactory()
 {
-    registerCodecLZ4(*this);
     registerCodecNone(*this);
+    registerCodecLZ4(*this);
     registerCodecZSTD(*this);
     registerCodecLZ4HC(*this);
+    registerCodecMultiple(*this);
+
+#ifndef KEEPER_STANDALONE_BUILD
     registerCodecDelta(*this);
     registerCodecT64(*this);
     registerCodecDoubleDelta(*this);
     registerCodecGorilla(*this);
     registerCodecEncrypted(*this);
-    registerCodecMultiple(*this);
+#endif
 
     default_codec = get("LZ4", {});
 }
diff --git a/src/Functions/addressToLineWithInlines.cpp b/src/Functions/addressToLineWithInlines.cpp
index c3e62bd802eb..4877268989d0 100644
--- a/src/Functions/addressToLineWithInlines.cpp
+++ b/src/Functions/addressToLineWithInlines.cpp
@@ -75,7 +75,7 @@ class FunctionAddressToLineWithInlines: public FunctionAddressToLineBase<StringR
         writeChar(':', out);
         writeIntText(location.line, out);
 
-        if (frame)
+        if (frame && frame->name != nullptr)
         {
             writeChar(':', out);
             int status = 0;
diff --git a/src/Parsers/fuzzers/codegen_fuzzer/gen.py b/src/Parsers/fuzzers/codegen_fuzzer/gen.py
index 95936247489d..84ee09916c47 100644
--- a/src/Parsers/fuzzers/codegen_fuzzer/gen.py
+++ b/src/Parsers/fuzzers/codegen_fuzzer/gen.py
@@ -7,16 +7,14 @@
 TOKEN_TEXT = 1
 TOKEN_VAR = 2
 
-TOKEN_COLON = ':'
-TOKEN_SEMI = ';'
-TOKEN_OR = '|'
-TOKEN_QUESTIONMARK = '?'
-TOKEN_ROUND_BRACKET_OPEN = '('
-TOKEN_ROUND_BRACKET_CLOSE = ')'
-TOKEN_ASTERISK = '*'
-TOKEN_SLASH = '/'
-
-
+TOKEN_COLON = ":"
+TOKEN_SEMI = ";"
+TOKEN_OR = "|"
+TOKEN_QUESTIONMARK = "?"
+TOKEN_ROUND_BRACKET_OPEN = "("
+TOKEN_ROUND_BRACKET_CLOSE = ")"
+TOKEN_ASTERISK = "*"
+TOKEN_SLASH = "/"
 
 
 class TextValue:
@@ -27,9 +25,9 @@ def __init__(self, t):
     def get_slug(self):
         if self.slug is not None:
             return self.slug
-        slug = ''
+        slug = ""
         for c in self.t:
-            slug += c if c in string.ascii_letters else '_'
+            slug += c if c in string.ascii_letters else "_"
         self.slug = slug
         return slug
 
@@ -37,12 +35,12 @@ def get_name(self):
         return f"TextValue_{self.get_slug()}"
 
     def __repr__(self):
-        return f"TextValue(\"{self.t}\")"
+        return f'TextValue("{self.t}")'
 
 
 class Var:
     def __init__(self, id_):
-       self.id_ = id_
+        self.id_ = id_
 
     def __repr__(self):
         return f"Var({self.id_})"
@@ -59,8 +57,8 @@ def __init__(self):
         self.cur_tok = None
         self.includes = []
 
-        self.proto = ''
-        self.cpp = ''
+        self.proto = ""
+        self.cpp = ""
 
     def parse_file(self, filename):
         with open(filename) as f:
@@ -81,7 +79,7 @@ def get_next_token(self):
         if self.text[0] == '"':
             return self.parse_txt_value()
 
-        if self.text[0] == '$':
+        if self.text[0] == "$":
             return self.parse_var_value()
 
         c, self.text = self.text[0], self.text[1:]
@@ -89,9 +87,9 @@ def get_next_token(self):
         return c
 
     def parse_var_value(self):
-        i = self.text.find(' ')
+        i = self.text.find(" ")
 
-        id_, self.text = self.text[1:i], self.text[i+1:]
+        id_, self.text = self.text[1:i], self.text[i + 1 :]
         self.var_id = int(id_)
         self.cur_tok = TOKEN_VAR
         return TOKEN_VAR
@@ -100,12 +98,12 @@ def parse_txt_value(self):
         if self.text[0] != '"':
             raise Exception("parse_txt_value: expected quote at the start")
 
-        self.t = ''
+        self.t = ""
         self.text = self.text[1:]
 
         while self.text[0] != '"':
-            if self.text[0] == '\\':
-                if self.text[1] == 'x':
+            if self.text[0] == "\\":
+                if self.text[1] == "x":
                     self.t += self.text[:4]
                     self.text = self.text[4:]
                 elif self.text[1] in 'nt\\"':
@@ -123,7 +121,7 @@ def parse_txt_value(self):
 
     def skip_ws(self):
         while self.text and self.text[0] in string.whitespace:
-            if self.text[0] == '
':
+            if self.text[0] == "
":
                 self.line += 1
                 self.col = 0
             self.text = self.text[1:]
@@ -134,10 +132,9 @@ def skip_ws(self):
 
     def skip_line(self):
         self.line += 1
-        index = self.text.find('
')
+        index = self.text.find("
")
         self.text = self.text[index:]
 
-
     def parse_statement(self):
         if self.skip_ws() is None:
             return None
@@ -164,52 +161,54 @@ def parse_statement(self):
 
     def generate(self):
         self.proto = 'syntax = "proto3";

'
-        self.cpp = '#include <iostream>
#include <string>
#include <vector>

#include <libfuzzer/libfuzzer_macro.h>

'
+        self.cpp = "#include <iostream>
#include <string>
#include <vector>

#include <libfuzzer/libfuzzer_macro.h>

"
 
         for incl_file in self.includes:
             self.cpp += f'#include "{incl_file}"
'
-        self.cpp += '
'
+        self.cpp += "
"
 
-        self.proto += 'message Word {
'
-        self.proto += '\tenum Value {
'
+        self.proto += "message Word {
"
+        self.proto += "\tenum Value {
"
 
-        self.cpp += 'void GenerateWord(const Word&, std::string&, int);

'
+        self.cpp += "void GenerateWord(const Word&, std::string&, int);

"
 
-        self.cpp += 'void GenerateSentence(const Sentence& stc, std::string &s, int depth) {
'
-        self.cpp += '\tfor (int i = 0; i < stc.words_size(); i++ ) {
'
-        self.cpp += '\t\tGenerateWord(stc.words(i), s, ++depth);
'
-        self.cpp += '\t}
'
-        self.cpp += '}
'
+        self.cpp += (
+            "void GenerateSentence(const Sentence& stc, std::string &s, int depth) {
"
+        )
+        self.cpp += "\tfor (int i = 0; i < stc.words_size(); i++ ) {
"
+        self.cpp += "\t\tGenerateWord(stc.words(i), s, ++depth);
"
+        self.cpp += "\t}
"
+        self.cpp += "}
"
 
-        self.cpp += 'void GenerateWord(const Word& word, std::string &s, int depth) {
'
+        self.cpp += "void GenerateWord(const Word& word, std::string &s, int depth) {
"
 
-        self.cpp += '\tif (depth > 5) return;

'
-        self.cpp += '\tswitch (word.value()) {
'
+        self.cpp += "\tif (depth > 5) return;

"
+        self.cpp += "\tswitch (word.value()) {
"
 
         for idx, chain in enumerate(self.chains):
-            self.proto += f'\t\tvalue_{idx} = {idx};
'
+            self.proto += f"\t\tvalue_{idx} = {idx};
"
 
-            self.cpp += f'\t\tcase {idx}: {{
'
+            self.cpp += f"\t\tcase {idx}: {{
"
             num_var = 0
             for item in chain:
                 if isinstance(item, TextValue):
                     self.cpp += f'\t\t\ts += "{item.t}";
'
                 elif isinstance(item, Var):
-                    self.cpp += f'\t\t\tif (word.inner().words_size() > {num_var})\t\t\t\tGenerateWord(word.inner().words({num_var}), s, ++depth);
'
+                    self.cpp += f"\t\t\tif (word.inner().words_size() > {num_var})\t\t\t\tGenerateWord(word.inner().words({num_var}), s, ++depth);
"
                     num_var += 1
                 else:
                     raise Exception("unknown token met during generation")
-            self.cpp += '\t\t\tbreak;
\t\t}
'
-        self.cpp += '\t\tdefault: break;
'
+            self.cpp += "\t\t\tbreak;
\t\t}
"
+        self.cpp += "\t\tdefault: break;
"
 
-        self.cpp += '\t}
'
+        self.cpp += "\t}
"
 
-        self.proto += '\t}
'
-        self.proto += '\tValue value = 1;
'
-        self.proto += '\tSentence inner = 2;
'
-        self.proto += '}
message Sentence {
\trepeated Word words = 1;
}'
+        self.proto += "\t}
"
+        self.proto += "\tValue value = 1;
"
+        self.proto += "\tSentence inner = 2;
"
+        self.proto += "}
message Sentence {
\trepeated Word words = 1;
}"
 
-        self.cpp += '}
'
+        self.cpp += "}
"
         return self.cpp, self.proto
 
     def fatal_parsing_error(self, msg):
@@ -220,7 +219,7 @@ def fatal_parsing_error(self, msg):
 def main(args):
     input_file, outfile_cpp, outfile_proto = args
 
-    if not outfile_proto.endswith('.proto'):
+    if not outfile_proto.endswith(".proto"):
         raise Exception("outfile_proto (argv[3]) should end with `.proto`")
 
     include_filename = outfile_proto[:-6] + ".pb.h"
@@ -231,17 +230,17 @@ def main(args):
 
     cpp, proto = p.generate()
 
-    proto = proto.replace('\t', ' ' * 4)
-    cpp = cpp.replace('\t', ' ' * 4)
+    proto = proto.replace("\t", " " * 4)
+    cpp = cpp.replace("\t", " " * 4)
 
-    with open(outfile_cpp, 'w') as f:
+    with open(outfile_cpp, "w") as f:
         f.write(cpp)
 
-    with open(outfile_proto, 'w') as f:
+    with open(outfile_proto, "w") as f:
         f.write(proto)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     if len(sys.argv) < 3:
         print(f"Usage {sys.argv[0]} <input_file> <outfile.cpp> <outfile.proto>")
         sys.exit(1)
diff --git a/src/Server/KeeperTCPHandlerFactory.h b/src/Server/KeeperTCPHandlerFactory.h
index 76309ffc1194..eb9f92bdd250 100644
--- a/src/Server/KeeperTCPHandlerFactory.h
+++ b/src/Server/KeeperTCPHandlerFactory.h
@@ -32,14 +32,14 @@ class KeeperTCPHandlerFactory : public TCPServerConnectionFactory
     KeeperTCPHandlerFactory(
         ConfigGetter config_getter_,
         std::shared_ptr<KeeperDispatcher> keeper_dispatcher_,
-        Poco::Timespan receive_timeout_,
-        Poco::Timespan send_timeout_,
+        uint64_t receive_timeout_seconds,
+        uint64_t send_timeout_seconds,
         bool secure)
         : config_getter(config_getter_)
         , keeper_dispatcher(keeper_dispatcher_)
         , log(&Poco::Logger::get(std::string{"KeeperTCP"} + (secure ? "S" : "") + "HandlerFactory"))
-        , receive_timeout(receive_timeout_)
-        , send_timeout(send_timeout_)
+        , receive_timeout(/* seconds = */ receive_timeout_seconds, /* microseconds = */ 0)
+        , send_timeout(/* seconds = */ send_timeout_seconds, /* microseconds = */ 0)
     {
     }
 
diff --git a/src/Storages/examples/active_parts.py b/src/Storages/examples/active_parts.py
index a818a76017db..d82c5ca96bf5 100644
--- a/src/Storages/examples/active_parts.py
+++ b/src/Storages/examples/active_parts.py
@@ -9,7 +9,9 @@
 
 parts = {}
 for s in sys.stdin.read().split():
-    m = re.match('^([0-9]{6})[0-9]{2}_([0-9]{6})[0-9]{2}_([0-9]+)_([0-9]+)_([0-9]+)$', s)
+    m = re.match(
+        "^([0-9]{6})[0-9]{2}_([0-9]{6})[0-9]{2}_([0-9]+)_([0-9]+)_([0-9]+)$", s
+    )
     if m == None:
         continue
     m1 = m.group(1)
@@ -18,7 +20,7 @@
     i2 = int(m.group(4))
     l = int(m.group(5))
     if m1 != m2:
-        raise Exception('not in single month: ' + s)
+        raise Exception("not in single month: " + s)
     if m1 not in parts:
         parts[m1] = []
     parts[m1].append((i1, i2, l, s))
@@ -27,13 +29,13 @@
     ps.sort(key=lambda i1_i2_l_s: (i1_i2_l_s[0], -i1_i2_l_s[1], -i1_i2_l_s[2]))
     (x2, y2, l2, s2) = (-1, -1, -1, -1)
     for x1, y1, l1, s1 in ps:
-        if x1 >= x2 and y1 <= y2 and l1 < l2 and (x1, y1) != (x2, y2): # 2 contains 1
+        if x1 >= x2 and y1 <= y2 and l1 < l2 and (x1, y1) != (x2, y2):  # 2 contains 1
             pass
-        elif x1 > y2: # 1 is to the right of 2
+        elif x1 > y2:  # 1 is to the right of 2
             if x1 != y2 + 1 and y2 != -1:
-                print() # to see the missing numbers
+                print()  # to see the missing numbers
             (x2, y2, l2, s2) = (x1, y1, l1, s1)
             print(s1)
         else:
-            raise Exception('invalid parts intersection: ' + s1 + ' and ' + s2)
+            raise Exception("invalid parts intersection: " + s1 + " and " + s2)
     print()
diff --git a/utils/changelog/format-changelog.py b/utils/changelog/format-changelog.py
index 56fe973eb6f7..ef1340d48dd8 100755
--- a/utils/changelog/format-changelog.py
+++ b/utils/changelog/format-changelog.py
@@ -9,25 +9,37 @@
 import re
 import sys
 
-parser = argparse.ArgumentParser(description='Format changelog for given PRs.')
-parser.add_argument('file', metavar='FILE', type=argparse.FileType('r', encoding='utf-8'), nargs='?', default=sys.stdin, help='File with PR numbers, one per line.')
+parser = argparse.ArgumentParser(description="Format changelog for given PRs.")
+parser.add_argument(
+    "file",
+    metavar="FILE",
+    type=argparse.FileType("r", encoding="utf-8"),
+    nargs="?",
+    default=sys.stdin,
+    help="File with PR numbers, one per line.",
+)
 args = parser.parse_args()
 
 # This function mirrors the PR description checks in ClickhousePullRequestTrigger.
 # Returns False if the PR should not be mentioned changelog.
 def parse_one_pull_request(item):
-    description = item['body']
+    description = item["body"]
     # Don't skip empty lines because they delimit parts of description
-    lines = [line for line in [x.strip() for x in (description.split('
') if description else [])]]
-    lines = [re.sub(r'\s+', ' ', l) for l in lines]
+    lines = [
+        line
+        for line in [
+            x.strip() for x in (description.split("
") if description else [])
+        ]
+    ]
+    lines = [re.sub(r"\s+", " ", l) for l in lines]
 
-    category = ''
-    entry = ''
+    category = ""
+    entry = ""
 
     if lines:
         i = 0
         while i < len(lines):
-            if re.match(r'(?i)^[>*_ ]*change\s*log\s*category', lines[i]):
+            if re.match(r"(?i)^[>*_ ]*change\s*log\s*category", lines[i]):
                 i += 1
                 if i >= len(lines):
                     break
@@ -36,9 +48,11 @@ def parse_one_pull_request(item):
                     i += 1
                     if i >= len(lines):
                         break
-                category = re.sub(r'^[-*\s]*', '', lines[i])
+                category = re.sub(r"^[-*\s]*", "", lines[i])
                 i += 1
-            elif re.match(r'(?i)^[>*_ ]*(short\s*description|change\s*log\s*entry)', lines[i]):
+            elif re.match(
+                r"(?i)^[>*_ ]*(short\s*description|change\s*log\s*entry)", lines[i]
+            ):
                 i += 1
                 # Can have one empty line between header and the entry itself. Filter it out.
                 if i < len(lines) and not lines[i]:
@@ -48,7 +62,7 @@ def parse_one_pull_request(item):
                 while i < len(lines) and lines[i]:
                     entry_lines.append(lines[i])
                     i += 1
-                entry = ' '.join(entry_lines)
+                entry = " ".join(entry_lines)
             else:
                 i += 1
 
@@ -58,48 +72,59 @@ def parse_one_pull_request(item):
         category = "NO CL CATEGORY"
 
     # Filter out the PR categories that are not for changelog.
-    if re.match(r'(?i)doc|((non|in|not|un)[-\s]*significant)|(not[ ]*for[ ]*changelog)', category):
+    if re.match(
+        r"(?i)doc|((non|in|not|un)[-\s]*significant)|(not[ ]*for[ ]*changelog)",
+        category,
+    ):
         return False
 
     if not entry:
         # Shouldn't happen, because description check in CI should catch such PRs.
         category = "NO CL ENTRY"
-        entry = "NO CL ENTRY:  '" + item['title'] + "'"
+        entry = "NO CL ENTRY:  '" + item["title"] + "'"
 
     entry = entry.strip()
-    if entry[-1] != '.':
-        entry += '.'
+    if entry[-1] != ".":
+        entry += "."
 
-    item['entry'] = entry
-    item['category'] = category
+    item["entry"] = entry
+    item["category"] = category
 
     return True
 
+
 # This array gives the preferred category order, and is also used to
 # normalize category names.
-categories_preferred_order = ['Backward Incompatible Change',
-    'New Feature', 'Performance Improvement', 'Improvement', 'Bug Fix',
-    'Build/Testing/Packaging Improvement', 'Other']
+categories_preferred_order = [
+    "Backward Incompatible Change",
+    "New Feature",
+    "Performance Improvement",
+    "Improvement",
+    "Bug Fix",
+    "Build/Testing/Packaging Improvement",
+    "Other",
+]
 
 category_to_pr = collections.defaultdict(lambda: [])
 users = {}
 for line in args.file:
-    pr = json.loads(open(f'pr{line.strip()}.json').read())
-    assert(pr['number'])
+    pr = json.loads(open(f"pr{line.strip()}.json").read())
+    assert pr["number"]
     if not parse_one_pull_request(pr):
         continue
 
-    assert(pr['category'])
+    assert pr["category"]
 
     # Normalize category name
     for c in categories_preferred_order:
-        if fuzzywuzzy.fuzz.ratio(pr['category'].lower(), c.lower()) >= 90:
-            pr['category'] = c
+        if fuzzywuzzy.fuzz.ratio(pr["category"].lower(), c.lower()) >= 90:
+            pr["category"] = c
             break
 
-    category_to_pr[pr['category']].append(pr)
-    user_id = pr['user']['id']
-    users[user_id] = json.loads(open(f'user{user_id}.json').read())
+    category_to_pr[pr["category"]].append(pr)
+    user_id = pr["user"]["id"]
+    users[user_id] = json.loads(open(f"user{user_id}.json").read())
+
 
 def print_category(category):
     print(("#### " + category))
@@ -110,14 +135,25 @@ def print_category(category):
 
         # Substitute issue links.
         # 1) issue number w/o markdown link
-        pr["entry"] = re.sub(r'([^[])#([0-9]{4,})', r'\1[#\2](https://github.com/ClickHouse/ClickHouse/issues/\2)', pr["entry"])
+        pr["entry"] = re.sub(
+            r"([^[])#([0-9]{4,})",
+            r"\1[#\2](https://github.com/ClickHouse/ClickHouse/issues/\2)",
+            pr["entry"],
+        )
         # 2) issue URL w/o markdown link
-        pr["entry"] = re.sub(r'([^(])https://github.com/ClickHouse/ClickHouse/issues/([0-9]{4,})', r'\1[#\2](https://github.com/ClickHouse/ClickHouse/issues/\2)', pr["entry"])
+        pr["entry"] = re.sub(
+            r"([^(])https://github.com/ClickHouse/ClickHouse/issues/([0-9]{4,})",
+            r"\1[#\2](https://github.com/ClickHouse/ClickHouse/issues/\2)",
+            pr["entry"],
+        )
 
-        print(f'* {pr["entry"]} [#{pr["number"]}]({pr["html_url"]}) ([{user_name}]({user["html_url"]})).')
+        print(
+            f'* {pr["entry"]} [#{pr["number"]}]({pr["html_url"]}) ([{user_name}]({user["html_url"]})).'
+        )
 
     print()
 
+
 # Print categories in preferred order
 for category in categories_preferred_order:
     if category in category_to_pr:
diff --git a/utils/check-style/check-black b/utils/check-style/check-black
new file mode 100755
index 000000000000..45e7820469b7
--- /dev/null
+++ b/utils/check-style/check-black
@@ -0,0 +1,13 @@
+#!/usr/bin/env bash
+
+set -e
+
+# We check only our code, that's why we skip contrib
+GIT_ROOT=$(git rev-parse --show-cdup)
+GIT_ROOT=${GIT_ROOT:-.}
+tmp=$(mktemp)
+if ! find "$GIT_ROOT" -name '*.py' -not -path "$GIT_ROOT/contrib/*" -exec black --check --diff {} + 1>"$tmp" 2>&1; then
+  # Show the result only if some files need formatting
+  cat "$tmp"
+fi
+rm "$tmp"
diff --git a/utils/check-style/check-workflows b/utils/check-style/check-workflows
index c0399829c28c..6e9cb87ed364 100755
--- a/utils/check-style/check-workflows
+++ b/utils/check-style/check-workflows
@@ -1,6 +1,9 @@
 #!/usr/bin/env bash
 
+set -e
+
 GIT_ROOT=$(git rev-parse --show-cdup)
+GIT_ROOT=${GIT_ROOT:-.}
 act --list --directory="$GIT_ROOT" 1>/dev/null 2>&1 || act --list --directory="$GIT_ROOT" 2>&1
 
-actionlint
+actionlint || :
diff --git a/utils/github/backport.py b/utils/github/backport.py
index 9227dbf4108c..615c0d19ffac 100644
--- a/utils/github/backport.py
+++ b/utils/github/backport.py
@@ -17,7 +17,9 @@
 
 class Backport:
     def __init__(self, token, owner, name, team):
-        self._gh = RemoteRepo(token, owner=owner, name=name, team=team, max_page_size=30, min_page_size=7)
+        self._gh = RemoteRepo(
+            token, owner=owner, name=name, team=team, max_page_size=30, min_page_size=7
+        )
         self._token = token
         self.default_branch_name = self._gh.default_branch
         self.ssh_url = self._gh.ssh_url
@@ -28,7 +30,7 @@ def getPullRequests(self, from_commit):
     def getBranchesWithRelease(self):
         branches = set()
         for pull_request in self._gh.find_pull_requests("release"):
-            branches.add(pull_request['headRefName'])
+            branches.add(pull_request["headRefName"])
         return branches
 
     def execute(self, repo, upstream, until_commit, run_cherrypick):
@@ -44,11 +46,11 @@ def execute(self, repo, upstream, until_commit, run_cherrypick):
                 branches.append(branch)
 
         if not branches:
-            logging.info('No release branches found!')
+            logging.info("No release branches found!")
             return
 
         for branch in branches:
-            logging.info('Found release branch: %s', branch[0])
+            logging.info("Found release branch: %s", branch[0])
 
         if not until_commit:
             until_commit = branches[0][1]
@@ -56,73 +58,128 @@ def execute(self, repo, upstream, until_commit, run_cherrypick):
 
         backport_map = {}
 
-        RE_MUST_BACKPORT = re.compile(r'^v(\d+\.\d+)-must-backport$')
-        RE_NO_BACKPORT = re.compile(r'^v(\d+\.\d+)-no-backport$')
-        RE_BACKPORTED = re.compile(r'^v(\d+\.\d+)-backported$')
+        RE_MUST_BACKPORT = re.compile(r"^v(\d+\.\d+)-must-backport$")
+        RE_NO_BACKPORT = re.compile(r"^v(\d+\.\d+)-no-backport$")
+        RE_BACKPORTED = re.compile(r"^v(\d+\.\d+)-backported$")
 
         # pull-requests are sorted by ancestry from the most recent.
         for pr in pull_requests:
-            while repo.comparator(branches[-1][1]) >= repo.comparator(pr['mergeCommit']['oid']):
-                logging.info("PR #{} is already inside {}. Dropping this branch for further PRs".format(pr['number'], branches[-1][0]))
+            while repo.comparator(branches[-1][1]) >= repo.comparator(
+                pr["mergeCommit"]["oid"]
+            ):
+                logging.info(
+                    "PR #{} is already inside {}. Dropping this branch for further PRs".format(
+                        pr["number"], branches[-1][0]
+                    )
+                )
                 branches.pop()
 
-            logging.info("Processing PR #{}".format(pr['number']))
+            logging.info("Processing PR #{}".format(pr["number"]))
 
             assert len(branches)
 
             branch_set = set([branch[0] for branch in branches])
 
             # First pass. Find all must-backports
-            for label in pr['labels']['nodes']:
-                if label['name'] == 'pr-must-backport':
-                    backport_map[pr['number']] = branch_set.copy()
+            for label in pr["labels"]["nodes"]:
+                if label["name"] == "pr-must-backport":
+                    backport_map[pr["number"]] = branch_set.copy()
                     continue
-                matched = RE_MUST_BACKPORT.match(label['name'])
+                matched = RE_MUST_BACKPORT.match(label["name"])
                 if matched:
-                    if pr['number'] not in backport_map:
-                        backport_map[pr['number']] = set()
-                    backport_map[pr['number']].add(matched.group(1))
+                    if pr["number"] not in backport_map:
+                        backport_map[pr["number"]] = set()
+                    backport_map[pr["number"]].add(matched.group(1))
 
             # Second pass. Find all no-backports
-            for label in pr['labels']['nodes']:
-                if label['name'] == 'pr-no-backport' and pr['number'] in backport_map:
-                    del backport_map[pr['number']]
+            for label in pr["labels"]["nodes"]:
+                if label["name"] == "pr-no-backport" and pr["number"] in backport_map:
+                    del backport_map[pr["number"]]
                     break
-                matched_no_backport = RE_NO_BACKPORT.match(label['name'])
-                matched_backported = RE_BACKPORTED.match(label['name'])
-                if matched_no_backport and pr['number'] in backport_map and matched_no_backport.group(1) in backport_map[pr['number']]:
-                    backport_map[pr['number']].remove(matched_no_backport.group(1))
-                    logging.info('\tskipping %s because of forced no-backport', matched_no_backport.group(1))
-                elif matched_backported and pr['number'] in backport_map and matched_backported.group(1) in backport_map[pr['number']]:
-                    backport_map[pr['number']].remove(matched_backported.group(1))
-                    logging.info('\tskipping %s because it\'s already backported manually', matched_backported.group(1))
+                matched_no_backport = RE_NO_BACKPORT.match(label["name"])
+                matched_backported = RE_BACKPORTED.match(label["name"])
+                if (
+                    matched_no_backport
+                    and pr["number"] in backport_map
+                    and matched_no_backport.group(1) in backport_map[pr["number"]]
+                ):
+                    backport_map[pr["number"]].remove(matched_no_backport.group(1))
+                    logging.info(
+                        "\tskipping %s because of forced no-backport",
+                        matched_no_backport.group(1),
+                    )
+                elif (
+                    matched_backported
+                    and pr["number"] in backport_map
+                    and matched_backported.group(1) in backport_map[pr["number"]]
+                ):
+                    backport_map[pr["number"]].remove(matched_backported.group(1))
+                    logging.info(
+                        "\tskipping %s because it's already backported manually",
+                        matched_backported.group(1),
+                    )
 
         for pr, branches in list(backport_map.items()):
-            logging.info('PR #%s needs to be backported to:', pr)
+            logging.info("PR #%s needs to be backported to:", pr)
             for branch in branches:
-                logging.info('\t%s, and the status is: %s', branch, run_cherrypick(self._token, pr, branch))
+                logging.info(
+                    "\t%s, and the status is: %s",
+                    branch,
+                    run_cherrypick(self._token, pr, branch),
+                )
 
         # print API costs
-        logging.info('
GitHub API total costs per query:')
+        logging.info("
GitHub API total costs per query:")
         for name, value in list(self._gh.api_costs.items()):
-            logging.info('%s : %s', name, value)
+            logging.info("%s : %s", name, value)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
-    parser.add_argument('--token',     type=str, required=True, help='token for Github access')
-    parser.add_argument('--repo',      type=str, required=True, help='path to full repository', metavar='PATH')
-    parser.add_argument('--til',       type=str,                help='check PRs from HEAD til this commit', metavar='COMMIT')
-    parser.add_argument('--dry-run',   action='store_true',     help='do not create or merge any PRs', default=False)
-    parser.add_argument('--verbose', '-v', action='store_true', help='more verbose output', default=False)
-    parser.add_argument('--upstream', '-u', type=str,           help='remote name of upstream in repository', default='origin')
+    parser.add_argument(
+        "--token", type=str, required=True, help="token for Github access"
+    )
+    parser.add_argument(
+        "--repo",
+        type=str,
+        required=True,
+        help="path to full repository",
+        metavar="PATH",
+    )
+    parser.add_argument(
+        "--til", type=str, help="check PRs from HEAD til this commit", metavar="COMMIT"
+    )
+    parser.add_argument(
+        "--dry-run",
+        action="store_true",
+        help="do not create or merge any PRs",
+        default=False,
+    )
+    parser.add_argument(
+        "--verbose",
+        "-v",
+        action="store_true",
+        help="more verbose output",
+        default=False,
+    )
+    parser.add_argument(
+        "--upstream",
+        "-u",
+        type=str,
+        help="remote name of upstream in repository",
+        default="origin",
+    )
     args = parser.parse_args()
 
     if args.verbose:
-        logging.basicConfig(format='%(message)s', stream=sys.stdout, level=logging.DEBUG)
+        logging.basicConfig(
+            format="%(message)s", stream=sys.stdout, level=logging.DEBUG
+        )
     else:
-        logging.basicConfig(format='%(message)s', stream=sys.stdout, level=logging.INFO)
+        logging.basicConfig(format="%(message)s", stream=sys.stdout, level=logging.INFO)
 
-    cherrypick_run = lambda token, pr, branch: CherryPick(token, 'ClickHouse', 'ClickHouse', 'core', pr, branch).execute(args.repo, args.dry_run)
-    bp = Backport(args.token, 'ClickHouse', 'ClickHouse', 'core')
+    cherrypick_run = lambda token, pr, branch: CherryPick(
+        token, "ClickHouse", "ClickHouse", "core", pr, branch
+    ).execute(args.repo, args.dry_run)
+    bp = Backport(args.token, "ClickHouse", "ClickHouse", "core")
     bp.execute(args.repo, args.upstream, args.til, cherrypick_run)
diff --git a/utils/github/cherrypick.py b/utils/github/cherrypick.py
index 8bedf54fefae..c6469fa62a99 100644
--- a/utils/github/cherrypick.py
+++ b/utils/github/cherrypick.py
@@ -1,6 +1,6 @@
 # -*- coding: utf-8 -*-
 
-'''
+"""
 Backports changes from PR to release branch.
 Requires multiple separate runs as part of the implementation.
 
@@ -12,7 +12,7 @@
 Second run checks PR from previous run to be merged or at least being mergeable. If it's not merged then try to merge it.
 
 Third run creates PR from backport branch (with merged previous PR) to release branch.
-'''
+"""
 
 try:
     from clickhouse.utils.github.query import Query as RemoteRepo
@@ -29,13 +29,13 @@
 
 class CherryPick:
     class Status(Enum):
-        DISCARDED = 'discarded'
-        NOT_INITIATED = 'not started'
-        FIRST_MERGEABLE = 'waiting for 1st stage'
-        FIRST_CONFLICTS = 'conflicts on 1st stage'
-        SECOND_MERGEABLE = 'waiting for 2nd stage'
-        SECOND_CONFLICTS = 'conflicts on 2nd stage'
-        MERGED = 'backported'
+        DISCARDED = "discarded"
+        NOT_INITIATED = "not started"
+        FIRST_MERGEABLE = "waiting for 1st stage"
+        FIRST_CONFLICTS = "conflicts on 1st stage"
+        SECOND_MERGEABLE = "waiting for 2nd stage"
+        SECOND_CONFLICTS = "conflicts on 2nd stage"
+        MERGED = "backported"
 
     def _run(self, args):
         out = subprocess.check_output(args).rstrip()
@@ -50,51 +50,90 @@ def __init__(self, token, owner, name, team, pr_number, target_branch):
 
         # TODO: check if pull-request is merged.
 
-        self.merge_commit_oid = self._pr['mergeCommit']['oid']
+        self.merge_commit_oid = self._pr["mergeCommit"]["oid"]
 
         self.target_branch = target_branch
-        self.backport_branch = 'backport/{branch}/{pr}'.format(branch=target_branch, pr=pr_number)
-        self.cherrypick_branch = 'cherrypick/{branch}/{oid}'.format(branch=target_branch, oid=self.merge_commit_oid)
+        self.backport_branch = "backport/{branch}/{pr}".format(
+            branch=target_branch, pr=pr_number
+        )
+        self.cherrypick_branch = "cherrypick/{branch}/{oid}".format(
+            branch=target_branch, oid=self.merge_commit_oid
+        )
 
     def getCherryPickPullRequest(self):
-        return self._gh.find_pull_request(base=self.backport_branch, head=self.cherrypick_branch)
+        return self._gh.find_pull_request(
+            base=self.backport_branch, head=self.cherrypick_branch
+        )
 
     def createCherryPickPullRequest(self, repo_path):
         DESCRIPTION = (
-            'This pull-request is a first step of an automated backporting.
'
-            'It contains changes like after calling a local command `git cherry-pick`.
'
-            'If you intend to continue backporting this changes, then resolve all conflicts if any.
'
-            'Otherwise, if you do not want to backport them, then just close this pull-request.
'
-            '
'
-            'The check results does not matter at this step - you can safely ignore them.
'
-            'Also this pull-request will be merged automatically as it reaches the mergeable state, but you always can merge it manually.
'
+            "This pull-request is a first step of an automated backporting.
"
+            "It contains changes like after calling a local command `git cherry-pick`.
"
+            "If you intend to continue backporting this changes, then resolve all conflicts if any.
"
+            "Otherwise, if you do not want to backport them, then just close this pull-request.
"
+            "
"
+            "The check results does not matter at this step - you can safely ignore them.
"
+            "Also this pull-request will be merged automatically as it reaches the mergeable state, but you always can merge it manually.
"
         )
 
         # FIXME: replace with something better than os.system()
-        git_prefix = ['git', '-C', repo_path, '-c', 'user.email=robot-clickhouse@yandex-team.ru', '-c', 'user.name=robot-clickhouse']
-        base_commit_oid = self._pr['mergeCommit']['parents']['nodes'][0]['oid']
+        git_prefix = [
+            "git",
+            "-C",
+            repo_path,
+            "-c",
+            "user.email=robot-clickhouse@yandex-team.ru",
+            "-c",
+            "user.name=robot-clickhouse",
+        ]
+        base_commit_oid = self._pr["mergeCommit"]["parents"]["nodes"][0]["oid"]
 
         # Create separate branch for backporting, and make it look like real cherry-pick.
-        self._run(git_prefix + ['checkout', '-f', self.target_branch])
-        self._run(git_prefix + ['checkout', '-B', self.backport_branch])
-        self._run(git_prefix + ['merge', '-s', 'ours', '--no-edit', base_commit_oid])
+        self._run(git_prefix + ["checkout", "-f", self.target_branch])
+        self._run(git_prefix + ["checkout", "-B", self.backport_branch])
+        self._run(git_prefix + ["merge", "-s", "ours", "--no-edit", base_commit_oid])
 
         # Create secondary branch to allow pull request with cherry-picked commit.
-        self._run(git_prefix + ['branch', '-f', self.cherrypick_branch, self.merge_commit_oid])
+        self._run(
+            git_prefix + ["branch", "-f", self.cherrypick_branch, self.merge_commit_oid]
+        )
 
-        self._run(git_prefix + ['push', '-f', 'origin', '{branch}:{branch}'.format(branch=self.backport_branch)])
-        self._run(git_prefix + ['push', '-f', 'origin', '{branch}:{branch}'.format(branch=self.cherrypick_branch)])
+        self._run(
+            git_prefix
+            + [
+                "push",
+                "-f",
+                "origin",
+                "{branch}:{branch}".format(branch=self.backport_branch),
+            ]
+        )
+        self._run(
+            git_prefix
+            + [
+                "push",
+                "-f",
+                "origin",
+                "{branch}:{branch}".format(branch=self.cherrypick_branch),
+            ]
+        )
 
         # Create pull-request like a local cherry-pick
-        pr = self._gh.create_pull_request(source=self.cherrypick_branch, target=self.backport_branch,
-                                          title='Cherry pick #{number} to {target}: {title}'.format(
-                                              number=self._pr['number'], target=self.target_branch,
-                                              title=self._pr['title'].replace('"', '\\"')),
-                                          description='Original pull-request #{}

{}'.format(self._pr['number'], DESCRIPTION))
+        pr = self._gh.create_pull_request(
+            source=self.cherrypick_branch,
+            target=self.backport_branch,
+            title="Cherry pick #{number} to {target}: {title}".format(
+                number=self._pr["number"],
+                target=self.target_branch,
+                title=self._pr["title"].replace('"', '\\"'),
+            ),
+            description="Original pull-request #{}

{}".format(
+                self._pr["number"], DESCRIPTION
+            ),
+        )
 
         # FIXME: use `team` to leave a single eligible assignee.
-        self._gh.add_assignee(pr, self._pr['author'])
-        self._gh.add_assignee(pr, self._pr['mergedBy'])
+        self._gh.add_assignee(pr, self._pr["author"])
+        self._gh.add_assignee(pr, self._pr["mergedBy"])
 
         self._gh.set_label(pr, "do not test")
         self._gh.set_label(pr, "pr-cherrypick")
@@ -102,36 +141,76 @@ def createCherryPickPullRequest(self, repo_path):
         return pr
 
     def mergeCherryPickPullRequest(self, cherrypick_pr):
-        return self._gh.merge_pull_request(cherrypick_pr['id'])
+        return self._gh.merge_pull_request(cherrypick_pr["id"])
 
     def getBackportPullRequest(self):
-        return self._gh.find_pull_request(base=self.target_branch, head=self.backport_branch)
+        return self._gh.find_pull_request(
+            base=self.target_branch, head=self.backport_branch
+        )
 
     def createBackportPullRequest(self, cherrypick_pr, repo_path):
         DESCRIPTION = (
-            'This pull-request is a last step of an automated backporting.
'
-            'Treat it as a standard pull-request: look at the checks and resolve conflicts.
'
-            'Merge it only if you intend to backport changes to the target branch, otherwise just close it.
'
+            "This pull-request is a last step of an automated backporting.
"
+            "Treat it as a standard pull-request: look at the checks and resolve conflicts.
"
+            "Merge it only if you intend to backport changes to the target branch, otherwise just close it.
"
         )
 
-        git_prefix = ['git', '-C', repo_path, '-c', 'user.email=robot-clickhouse@clickhouse.com', '-c', 'user.name=robot-clickhouse']
-
-        pr_title = 'Backport #{number} to {target}: {title}'.format(
-            number=self._pr['number'], target=self.target_branch,
-            title=self._pr['title'].replace('"', '\\"'))
+        git_prefix = [
+            "git",
+            "-C",
+            repo_path,
+            "-c",
+            "user.email=robot-clickhouse@clickhouse.com",
+            "-c",
+            "user.name=robot-clickhouse",
+        ]
+
+        pr_title = "Backport #{number} to {target}: {title}".format(
+            number=self._pr["number"],
+            target=self.target_branch,
+            title=self._pr["title"].replace('"', '\\"'),
+        )
 
-        self._run(git_prefix + ['checkout', '-f', self.backport_branch])
-        self._run(git_prefix + ['pull', '--ff-only', 'origin', self.backport_branch])
-        self._run(git_prefix + ['reset', '--soft', self._run(git_prefix + ['merge-base', 'origin/' + self.target_branch, self.backport_branch])])
-        self._run(git_prefix + ['commit', '-a', '--allow-empty', '-m', pr_title])
-        self._run(git_prefix + ['push', '-f', 'origin', '{branch}:{branch}'.format(branch=self.backport_branch)])
+        self._run(git_prefix + ["checkout", "-f", self.backport_branch])
+        self._run(git_prefix + ["pull", "--ff-only", "origin", self.backport_branch])
+        self._run(
+            git_prefix
+            + [
+                "reset",
+                "--soft",
+                self._run(
+                    git_prefix
+                    + [
+                        "merge-base",
+                        "origin/" + self.target_branch,
+                        self.backport_branch,
+                    ]
+                ),
+            ]
+        )
+        self._run(git_prefix + ["commit", "-a", "--allow-empty", "-m", pr_title])
+        self._run(
+            git_prefix
+            + [
+                "push",
+                "-f",
+                "origin",
+                "{branch}:{branch}".format(branch=self.backport_branch),
+            ]
+        )
 
-        pr = self._gh.create_pull_request(source=self.backport_branch, target=self.target_branch, title=pr_title,
-                                          description='Original pull-request #{}
Cherry-pick pull-request #{}

{}'.format(self._pr['number'], cherrypick_pr['number'], DESCRIPTION))
+        pr = self._gh.create_pull_request(
+            source=self.backport_branch,
+            target=self.target_branch,
+            title=pr_title,
+            description="Original pull-request #{}
Cherry-pick pull-request #{}

{}".format(
+                self._pr["number"], cherrypick_pr["number"], DESCRIPTION
+            ),
+        )
 
         # FIXME: use `team` to leave a single eligible assignee.
-        self._gh.add_assignee(pr, self._pr['author'])
-        self._gh.add_assignee(pr, self._pr['mergedBy'])
+        self._gh.add_assignee(pr, self._pr["author"])
+        self._gh.add_assignee(pr, self._pr["mergedBy"])
 
         self._gh.set_label(pr, "pr-backport")
 
@@ -142,23 +221,43 @@ def execute(self, repo_path, dry_run=False):
         if not pr1:
             if not dry_run:
                 pr1 = self.createCherryPickPullRequest(repo_path)
-                logging.debug('Created PR with cherry-pick of %s to %s: %s', self._pr['number'], self.target_branch, pr1['url'])
+                logging.debug(
+                    "Created PR with cherry-pick of %s to %s: %s",
+                    self._pr["number"],
+                    self.target_branch,
+                    pr1["url"],
+                )
             else:
                 return CherryPick.Status.NOT_INITIATED
         else:
-            logging.debug('Found PR with cherry-pick of %s to %s: %s', self._pr['number'], self.target_branch, pr1['url'])
-
-        if not pr1['merged'] and pr1['mergeable'] == 'MERGEABLE' and not pr1['closed']:
+            logging.debug(
+                "Found PR with cherry-pick of %s to %s: %s",
+                self._pr["number"],
+                self.target_branch,
+                pr1["url"],
+            )
+
+        if not pr1["merged"] and pr1["mergeable"] == "MERGEABLE" and not pr1["closed"]:
             if not dry_run:
                 pr1 = self.mergeCherryPickPullRequest(pr1)
-                logging.debug('Merged PR with cherry-pick of %s to %s: %s', self._pr['number'], self.target_branch, pr1['url'])
-
-        if not pr1['merged']:
-            logging.debug('Waiting for PR with cherry-pick of %s to %s: %s', self._pr['number'], self.target_branch, pr1['url'])
-
-            if pr1['closed']:
+                logging.debug(
+                    "Merged PR with cherry-pick of %s to %s: %s",
+                    self._pr["number"],
+                    self.target_branch,
+                    pr1["url"],
+                )
+
+        if not pr1["merged"]:
+            logging.debug(
+                "Waiting for PR with cherry-pick of %s to %s: %s",
+                self._pr["number"],
+                self.target_branch,
+                pr1["url"],
+            )
+
+            if pr1["closed"]:
                 return CherryPick.Status.DISCARDED
-            elif pr1['mergeable'] == 'CONFLICTING':
+            elif pr1["mergeable"] == "CONFLICTING":
                 return CherryPick.Status.FIRST_CONFLICTS
             else:
                 return CherryPick.Status.FIRST_MERGEABLE
@@ -167,31 +266,58 @@ def execute(self, repo_path, dry_run=False):
         if not pr2:
             if not dry_run:
                 pr2 = self.createBackportPullRequest(pr1, repo_path)
-                logging.debug('Created PR with backport of %s to %s: %s', self._pr['number'], self.target_branch, pr2['url'])
+                logging.debug(
+                    "Created PR with backport of %s to %s: %s",
+                    self._pr["number"],
+                    self.target_branch,
+                    pr2["url"],
+                )
             else:
                 return CherryPick.Status.FIRST_MERGEABLE
         else:
-            logging.debug('Found PR with backport of %s to %s: %s', self._pr['number'], self.target_branch, pr2['url'])
-
-        if pr2['merged']:
+            logging.debug(
+                "Found PR with backport of %s to %s: %s",
+                self._pr["number"],
+                self.target_branch,
+                pr2["url"],
+            )
+
+        if pr2["merged"]:
             return CherryPick.Status.MERGED
-        elif pr2['closed']:
+        elif pr2["closed"]:
             return CherryPick.Status.DISCARDED
-        elif pr2['mergeable'] == 'CONFLICTING':
+        elif pr2["mergeable"] == "CONFLICTING":
             return CherryPick.Status.SECOND_CONFLICTS
         else:
             return CherryPick.Status.SECOND_MERGEABLE
 
 
 if __name__ == "__main__":
-    logging.basicConfig(format='%(message)s', stream=sys.stdout, level=logging.DEBUG)
+    logging.basicConfig(format="%(message)s", stream=sys.stdout, level=logging.DEBUG)
 
     parser = argparse.ArgumentParser()
-    parser.add_argument('--token',  '-t', type=str, required=True, help='token for Github access')
-    parser.add_argument('--pr',           type=str, required=True, help='PR# to cherry-pick')
-    parser.add_argument('--branch', '-b', type=str, required=True, help='target branch name for cherry-pick')
-    parser.add_argument('--repo',   '-r', type=str, required=True, help='path to full repository', metavar='PATH')
+    parser.add_argument(
+        "--token", "-t", type=str, required=True, help="token for Github access"
+    )
+    parser.add_argument("--pr", type=str, required=True, help="PR# to cherry-pick")
+    parser.add_argument(
+        "--branch",
+        "-b",
+        type=str,
+        required=True,
+        help="target branch name for cherry-pick",
+    )
+    parser.add_argument(
+        "--repo",
+        "-r",
+        type=str,
+        required=True,
+        help="path to full repository",
+        metavar="PATH",
+    )
     args = parser.parse_args()
 
-    cp = CherryPick(args.token, 'ClickHouse', 'ClickHouse', 'core', args.pr, args.branch)
+    cp = CherryPick(
+        args.token, "ClickHouse", "ClickHouse", "core", args.pr, args.branch
+    )
     cp.execute(args.repo)
diff --git a/utils/github/local.py b/utils/github/local.py
index 2ad8d4b8b715..571c9102ba0f 100644
--- a/utils/github/local.py
+++ b/utils/github/local.py
@@ -20,13 +20,14 @@ def cmp(x, y):
                 return -1
             else:
                 return 1
+
         self.comparator = functools.cmp_to_key(cmp)
 
     def get_head_commit(self):
         return self._repo.commit(self._default)
 
     def iterate(self, begin, end):
-        rev_range = '{}...{}'.format(begin, end)
+        rev_range = "{}...{}".format(begin, end)
         for commit in self._repo.iter_commits(rev_range, first_parent=True):
             yield commit
 
@@ -39,27 +40,35 @@ def __init__(self, repo_path, remote_name, default_branch_name):
         self._default = self._remote.refs[default_branch_name]
 
     def get_release_branches(self):
-        '''
+        """
         Returns sorted list of tuples:
          * remote branch (git.refs.remote.RemoteReference),
          * base commit (git.Commit),
          * head (git.Commit)).
         List is sorted by commits in ascending order.
-        '''
+        """
         release_branches = []
 
-        RE_RELEASE_BRANCH_REF = re.compile(r'^refs/remotes/.+/\d+\.\d+$')
+        RE_RELEASE_BRANCH_REF = re.compile(r"^refs/remotes/.+/\d+\.\d+$")
 
-        for branch in [r for r in self._remote.refs if RE_RELEASE_BRANCH_REF.match(r.path)]:
+        for branch in [
+            r for r in self._remote.refs if RE_RELEASE_BRANCH_REF.match(r.path)
+        ]:
             base = self._repo.merge_base(self._default, self._repo.commit(branch))
             if not base:
-                logging.info('Branch %s is not based on branch %s. Ignoring.', branch.path, self._default)
+                logging.info(
+                    "Branch %s is not based on branch %s. Ignoring.",
+                    branch.path,
+                    self._default,
+                )
             elif len(base) > 1:
-                logging.info('Branch %s has more than one base commit. Ignoring.', branch.path)
+                logging.info(
+                    "Branch %s has more than one base commit. Ignoring.", branch.path
+                )
             else:
                 release_branches.append((os.path.basename(branch.name), base[0]))
 
-        return sorted(release_branches, key=lambda x : self.comparator(x[1]))
+        return sorted(release_branches, key=lambda x: self.comparator(x[1]))
 
 
 class BareRepository(RepositoryBase):
@@ -68,24 +77,32 @@ def __init__(self, repo_path, default_branch_name):
         self._default = self._repo.branches[default_branch_name]
 
     def get_release_branches(self):
-        '''
+        """
         Returns sorted list of tuples:
          * branch (git.refs.head?),
          * base commit (git.Commit),
          * head (git.Commit)).
         List is sorted by commits in ascending order.
-        '''
+        """
         release_branches = []
 
-        RE_RELEASE_BRANCH_REF = re.compile(r'^refs/heads/\d+\.\d+$')
+        RE_RELEASE_BRANCH_REF = re.compile(r"^refs/heads/\d+\.\d+$")
 
-        for branch in [r for r in self._repo.branches if RE_RELEASE_BRANCH_REF.match(r.path)]:
+        for branch in [
+            r for r in self._repo.branches if RE_RELEASE_BRANCH_REF.match(r.path)
+        ]:
             base = self._repo.merge_base(self._default, self._repo.commit(branch))
             if not base:
-                logging.info('Branch %s is not based on branch %s. Ignoring.', branch.path, self._default)
+                logging.info(
+                    "Branch %s is not based on branch %s. Ignoring.",
+                    branch.path,
+                    self._default,
+                )
             elif len(base) > 1:
-                logging.info('Branch %s has more than one base commit. Ignoring.', branch.path)
+                logging.info(
+                    "Branch %s has more than one base commit. Ignoring.", branch.path
+                )
             else:
                 release_branches.append((os.path.basename(branch.name), base[0]))
 
-        return sorted(release_branches, key=lambda x : self.comparator(x[1]))
+        return sorted(release_branches, key=lambda x: self.comparator(x[1]))
diff --git a/utils/github/parser.py b/utils/github/parser.py
index 570410ba23d4..d8348e6d9646 100644
--- a/utils/github/parser.py
+++ b/utils/github/parser.py
@@ -1,19 +1,20 @@
 # -*- coding: utf-8 -*-
 
+
 class Description:
-    '''Parsed description representation
-    '''
+    """Parsed description representation"""
+
     MAP_CATEGORY_TO_LABEL = {
-        'New Feature': 'pr-feature',
-        'Bug Fix': 'pr-bugfix',
-        'Improvement': 'pr-improvement',
-        'Performance Improvement': 'pr-performance',
+        "New Feature": "pr-feature",
+        "Bug Fix": "pr-bugfix",
+        "Improvement": "pr-improvement",
+        "Performance Improvement": "pr-performance",
         # 'Backward Incompatible Change': doesn't match anything
-        'Build/Testing/Packaging Improvement': 'pr-build',
-        'Non-significant (changelog entry is not needed)': 'pr-non-significant',
-        'Non-significant (changelog entry is not required)': 'pr-non-significant',
-        'Non-significant': 'pr-non-significant',
-        'Documentation (changelog entry is not required)': 'pr-documentation',
+        "Build/Testing/Packaging Improvement": "pr-build",
+        "Non-significant (changelog entry is not needed)": "pr-non-significant",
+        "Non-significant (changelog entry is not required)": "pr-non-significant",
+        "Non-significant": "pr-non-significant",
+        "Documentation (changelog entry is not required)": "pr-documentation",
         # 'Other': doesn't match anything
     }
 
@@ -21,7 +22,7 @@ def __init__(self, pull_request):
         self.label_name = str()
         self.legal = False
 
-        self._parse(pull_request['bodyText'])
+        self._parse(pull_request["bodyText"])
 
     def _parse(self, text):
         lines = text.splitlines()
@@ -38,14 +39,17 @@ def _parse(self, text):
                 category = stripped
                 next_category = False
 
-            if stripped == 'I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en':
+            if (
+                stripped
+                == "I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en"
+            ):
                 self.legal = True
 
             category_headers = (
-                'Category (leave one):',
-                'Changelog category (leave one):',
-                'Changelog category:',
-                'Category:'
+                "Category (leave one):",
+                "Changelog category (leave one):",
+                "Changelog category:",
+                "Category:",
             )
 
             if stripped in category_headers:
@@ -55,6 +59,6 @@ def _parse(self, text):
             self.label_name = Description.MAP_CATEGORY_TO_LABEL[category]
         else:
             if not category:
-                print('Cannot find category in pr description')
+                print("Cannot find category in pr description")
             else:
-                print(('Unknown category: ' + category))
+                print(("Unknown category: " + category))
diff --git a/utils/github/query.py b/utils/github/query.py
index 39b1d0ce0034..7afbc57781c6 100644
--- a/utils/github/query.py
+++ b/utils/github/query.py
@@ -4,11 +4,11 @@
 
 
 class Query:
-    '''
+    """
     Implements queries to the Github API using GraphQL
-    '''
+    """
 
-    _PULL_REQUEST = '''
+    _PULL_REQUEST = """
         author {{
             ... on User {{
                 id
@@ -46,7 +46,7 @@ class Query:
         number
         title
         url
-    '''
+    """
 
     def __init__(self, token, owner, name, team, max_page_size=100, min_page_size=10):
         self._PULL_REQUEST = Query._PULL_REQUEST.format(min_page_size=min_page_size)
@@ -62,14 +62,14 @@ def __init__(self, token, owner, name, team, max_page_size=100, min_page_size=10
         self.api_costs = {}
 
         repo = self.get_repository()
-        self._id = repo['id']
-        self.ssh_url = repo['sshUrl']
-        self.default_branch = repo['defaultBranchRef']['name']
+        self._id = repo["id"]
+        self.ssh_url = repo["sshUrl"]
+        self.default_branch = repo["defaultBranchRef"]["name"]
 
         self.members = set(self.get_members())
 
     def get_repository(self):
-        _QUERY = '''
+        _QUERY = """
             repository(owner: "{owner}" name: "{name}") {{
                 defaultBranchRef {{
                     name
@@ -77,19 +77,19 @@ def get_repository(self):
                 id
                 sshUrl
             }}
-        '''
+        """
 
         query = _QUERY.format(owner=self._owner, name=self._name)
-        return self._run(query)['repository']
+        return self._run(query)["repository"]
 
     def get_members(self):
-        '''Get all team members for organization
+        """Get all team members for organization
 
         Returns:
             members: a map of members' logins to ids
-        '''
+        """
 
-        _QUERY = '''
+        _QUERY = """
             organization(login: "{organization}") {{
                 team(slug: "{team}") {{
                     members(first: {max_page_size} {next}) {{
@@ -104,43 +104,54 @@ def get_members(self):
                     }}
                 }}
             }}
-        '''
+        """
 
         members = {}
         not_end = True
-        query = _QUERY.format(organization=self._owner, team=self._team,
-                              max_page_size=self._max_page_size,
-                              next='')
+        query = _QUERY.format(
+            organization=self._owner,
+            team=self._team,
+            max_page_size=self._max_page_size,
+            next="",
+        )
 
         while not_end:
-            result = self._run(query)['organization']['team']
+            result = self._run(query)["organization"]["team"]
             if result is None:
                 break
-            result = result['members']
-            not_end = result['pageInfo']['hasNextPage']
-            query = _QUERY.format(organization=self._owner, team=self._team,
-                                  max_page_size=self._max_page_size,
-                                  next='after: "{}"'.format(result["pageInfo"]["endCursor"]))
+            result = result["members"]
+            not_end = result["pageInfo"]["hasNextPage"]
+            query = _QUERY.format(
+                organization=self._owner,
+                team=self._team,
+                max_page_size=self._max_page_size,
+                next='after: "{}"'.format(result["pageInfo"]["endCursor"]),
+            )
 
-            members += dict([(node['login'], node['id']) for node in result['nodes']])
+            members += dict([(node["login"], node["id"]) for node in result["nodes"]])
 
         return members
 
     def get_pull_request(self, number):
-        _QUERY = '''
+        _QUERY = """
             repository(owner: "{owner}" name: "{name}") {{
                 pullRequest(number: {number}) {{
                     {pull_request_data}
                 }}
             }}
-        '''
+        """
 
-        query = _QUERY.format(owner=self._owner, name=self._name, number=number,
-                              pull_request_data=self._PULL_REQUEST, min_page_size=self._min_page_size)
-        return self._run(query)['repository']['pullRequest']
+        query = _QUERY.format(
+            owner=self._owner,
+            name=self._name,
+            number=number,
+            pull_request_data=self._PULL_REQUEST,
+            min_page_size=self._min_page_size,
+        )
+        return self._run(query)["repository"]["pullRequest"]
 
     def find_pull_request(self, base, head):
-        _QUERY = '''
+        _QUERY = """
             repository(owner: "{owner}" name: "{name}") {{
                 pullRequests(first: {min_page_size} baseRefName: "{base}" headRefName: "{head}") {{
                     nodes {{
@@ -149,21 +160,27 @@ def find_pull_request(self, base, head):
                     totalCount
                 }}
             }}
-        '''
-
-        query = _QUERY.format(owner=self._owner, name=self._name, base=base, head=head,
-                              pull_request_data=self._PULL_REQUEST, min_page_size=self._min_page_size)
-        result = self._run(query)['repository']['pullRequests']
-        if result['totalCount'] > 0:
-            return result['nodes'][0]
+        """
+
+        query = _QUERY.format(
+            owner=self._owner,
+            name=self._name,
+            base=base,
+            head=head,
+            pull_request_data=self._PULL_REQUEST,
+            min_page_size=self._min_page_size,
+        )
+        result = self._run(query)["repository"]["pullRequests"]
+        if result["totalCount"] > 0:
+            return result["nodes"][0]
         else:
             return {}
 
     def find_pull_requests(self, label_name):
-        '''
+        """
         Get all pull-requests filtered by label name
-        '''
-        _QUERY = '''
+        """
+        _QUERY = """
             repository(owner: "{owner}" name: "{name}") {{
                 pullRequests(first: {min_page_size} labels: "{label_name}" states: OPEN) {{
                     nodes {{
@@ -171,18 +188,23 @@ def find_pull_requests(self, label_name):
                     }}
                 }}
             }}
-        '''
+        """
 
-        query = _QUERY.format(owner=self._owner, name=self._name, label_name=label_name,
-                              pull_request_data=self._PULL_REQUEST, min_page_size=self._min_page_size)
-        return self._run(query)['repository']['pullRequests']['nodes']
+        query = _QUERY.format(
+            owner=self._owner,
+            name=self._name,
+            label_name=label_name,
+            pull_request_data=self._PULL_REQUEST,
+            min_page_size=self._min_page_size,
+        )
+        return self._run(query)["repository"]["pullRequests"]["nodes"]
 
     def get_pull_requests(self, before_commit):
-        '''
+        """
         Get all merged pull-requests from the HEAD of default branch to the last commit (excluding)
-        '''
+        """
 
-        _QUERY = '''
+        _QUERY = """
             repository(owner: "{owner}" name: "{name}") {{
                 defaultBranchRef {{
                     target {{
@@ -220,44 +242,60 @@ def get_pull_requests(self, before_commit):
                     }}
                 }}
             }}
-        '''
+        """
 
         pull_requests = []
         not_end = True
-        query = _QUERY.format(owner=self._owner, name=self._name,
-                              max_page_size=self._max_page_size,
-                              min_page_size=self._min_page_size,
-                              pull_request_data=self._PULL_REQUEST,
-                              next='')
+        query = _QUERY.format(
+            owner=self._owner,
+            name=self._name,
+            max_page_size=self._max_page_size,
+            min_page_size=self._min_page_size,
+            pull_request_data=self._PULL_REQUEST,
+            next="",
+        )
 
         while not_end:
-            result = self._run(query)['repository']['defaultBranchRef']['target']['history']
-            not_end = result['pageInfo']['hasNextPage']
-            query = _QUERY.format(owner=self._owner, name=self._name,
-                                  max_page_size=self._max_page_size,
-                                  min_page_size=self._min_page_size,
-                                  pull_request_data=self._PULL_REQUEST,
-                                  next='after: "{}"'.format(result["pageInfo"]["endCursor"]))
-
-            for commit in result['nodes']:
+            result = self._run(query)["repository"]["defaultBranchRef"]["target"][
+                "history"
+            ]
+            not_end = result["pageInfo"]["hasNextPage"]
+            query = _QUERY.format(
+                owner=self._owner,
+                name=self._name,
+                max_page_size=self._max_page_size,
+                min_page_size=self._min_page_size,
+                pull_request_data=self._PULL_REQUEST,
+                next='after: "{}"'.format(result["pageInfo"]["endCursor"]),
+            )
+
+            for commit in result["nodes"]:
                 # FIXME: maybe include `before_commit`?
-                if str(commit['oid']) == str(before_commit):
+                if str(commit["oid"]) == str(before_commit):
                     not_end = False
                     break
 
                 # TODO: fetch all pull-requests that were merged in a single commit.
-                assert commit['associatedPullRequests']['totalCount'] <= self._min_page_size
-
-                for pull_request in commit['associatedPullRequests']['nodes']:
-                    if(pull_request['baseRepository']['nameWithOwner'] == '{}/{}'.format(self._owner, self._name) and
-                       pull_request['baseRefName'] == self.default_branch and
-                       pull_request['mergeCommit']['oid'] == commit['oid']):
+                assert (
+                    commit["associatedPullRequests"]["totalCount"]
+                    <= self._min_page_size
+                )
+
+                for pull_request in commit["associatedPullRequests"]["nodes"]:
+                    if (
+                        pull_request["baseRepository"]["nameWithOwner"]
+                        == "{}/{}".format(self._owner, self._name)
+                        and pull_request["baseRefName"] == self.default_branch
+                        and pull_request["mergeCommit"]["oid"] == commit["oid"]
+                    ):
                         pull_requests.append(pull_request)
 
         return pull_requests
 
-    def create_pull_request(self, source, target, title, description="", draft=False, can_modify=True):
-        _QUERY = '''
+    def create_pull_request(
+        self, source, target, title, description="", draft=False, can_modify=True
+    ):
+        _QUERY = """
             createPullRequest(input: {{
                 baseRefName: "{target}",
                 headRefName: "{source}",
@@ -271,15 +309,22 @@ def create_pull_request(self, source, target, title, description="", draft=False
                     {pull_request_data}
                 }}
             }}
-        '''
-
-        query = _QUERY.format(target=target, source=source, id=self._id, title=title, body=description,
-                              draft="true" if draft else "false", modify="true" if can_modify else "false",
-                              pull_request_data=self._PULL_REQUEST)
-        return self._run(query, is_mutation=True)['createPullRequest']['pullRequest']
+        """
+
+        query = _QUERY.format(
+            target=target,
+            source=source,
+            id=self._id,
+            title=title,
+            body=description,
+            draft="true" if draft else "false",
+            modify="true" if can_modify else "false",
+            pull_request_data=self._PULL_REQUEST,
+        )
+        return self._run(query, is_mutation=True)["createPullRequest"]["pullRequest"]
 
     def merge_pull_request(self, id):
-        _QUERY = '''
+        _QUERY = """
             mergePullRequest(input: {{
                 pullRequestId: "{id}"
             }}) {{
@@ -287,35 +332,35 @@ def merge_pull_request(self, id):
                     {pull_request_data}
                 }}
             }}
-        '''
+        """
 
         query = _QUERY.format(id=id, pull_request_data=self._PULL_REQUEST)
-        return self._run(query, is_mutation=True)['mergePullRequest']['pullRequest']
+        return self._run(query, is_mutation=True)["mergePullRequest"]["pullRequest"]
 
     # FIXME: figure out how to add more assignees at once
     def add_assignee(self, pr, assignee):
-        _QUERY = '''
+        _QUERY = """
             addAssigneesToAssignable(input: {{
                 assignableId: "{id1}",
                 assigneeIds: "{id2}"
             }}) {{
                 clientMutationId
             }}
-        '''
+        """
 
-        query = _QUERY.format(id1=pr['id'], id2=assignee['id'])
+        query = _QUERY.format(id1=pr["id"], id2=assignee["id"])
         self._run(query, is_mutation=True)
 
     def set_label(self, pull_request, label_name):
-        '''
+        """
         Set label by name to the pull request
 
         Args:
             pull_request: JSON object returned by `get_pull_requests()`
             label_name (string): label name
-        '''
+        """
 
-        _GET_LABEL = '''
+        _GET_LABEL = """
             repository(owner: "{owner}" name: "{name}") {{
                 labels(first: {max_page_size} {next} query: "{label_name}") {{
                     pageInfo {{
@@ -329,36 +374,44 @@ def set_label(self, pull_request, label_name):
                     }}
                 }}
             }}
-        '''
+        """
 
-        _SET_LABEL = '''
+        _SET_LABEL = """
             addLabelsToLabelable(input: {{
                 labelableId: "{pr_id}",
                 labelIds: "{label_id}"
             }}) {{
                 clientMutationId
             }}
-        '''
+        """
 
         labels = []
         not_end = True
-        query = _GET_LABEL.format(owner=self._owner, name=self._name, label_name=label_name,
-                                  max_page_size=self._max_page_size,
-                                  next='')
+        query = _GET_LABEL.format(
+            owner=self._owner,
+            name=self._name,
+            label_name=label_name,
+            max_page_size=self._max_page_size,
+            next="",
+        )
 
         while not_end:
-            result = self._run(query)['repository']['labels']
-            not_end = result['pageInfo']['hasNextPage']
-            query = _GET_LABEL.format(owner=self._owner, name=self._name, label_name=label_name,
-                                      max_page_size=self._max_page_size,
-                                      next='after: "{}"'.format(result["pageInfo"]["endCursor"]))
+            result = self._run(query)["repository"]["labels"]
+            not_end = result["pageInfo"]["hasNextPage"]
+            query = _GET_LABEL.format(
+                owner=self._owner,
+                name=self._name,
+                label_name=label_name,
+                max_page_size=self._max_page_size,
+                next='after: "{}"'.format(result["pageInfo"]["endCursor"]),
+            )
 
-            labels += [label for label in result['nodes']]
+            labels += [label for label in result["nodes"]]
 
         if not labels:
             return
 
-        query = _SET_LABEL.format(pr_id=pull_request['id'], label_id=labels[0]['id'])
+        query = _SET_LABEL.format(pr_id=pull_request["id"], label_id=labels[0]["id"])
         self._run(query, is_mutation=True)
 
     def _run(self, query, is_mutation=False):
@@ -380,19 +433,21 @@ def requests_retry_session(
                 status_forcelist=status_forcelist,
             )
             adapter = HTTPAdapter(max_retries=retry)
-            session.mount('http://', adapter)
-            session.mount('https://', adapter)
+            session.mount("http://", adapter)
+            session.mount("https://", adapter)
             return session
 
-        headers = {'Authorization': 'bearer {}'.format(self._token)}
+        headers = {"Authorization": "bearer {}".format(self._token)}
         if is_mutation:
-            query = '''
+            query = """
             mutation {{
                 {query}
             }}
-            '''.format(query=query)
+            """.format(
+                query=query
+            )
         else:
-            query = '''
+            query = """
             query {{
                 {query}
                 rateLimit {{
@@ -400,23 +455,38 @@ def requests_retry_session(
                     remaining
                 }}
             }}
-            '''.format(query=query)
+            """.format(
+                query=query
+            )
 
         while True:
-            request = requests_retry_session().post('https://api.github.com/graphql', json={'query': query}, headers=headers)
+            request = requests_retry_session().post(
+                "https://api.github.com/graphql", json={"query": query}, headers=headers
+            )
             if request.status_code == 200:
                 result = request.json()
-                if 'errors' in result:
-                    raise Exception('Errors occurred: {}
Original query: {}'.format(result["errors"], query))
+                if "errors" in result:
+                    raise Exception(
+                        "Errors occurred: {}
Original query: {}".format(
+                            result["errors"], query
+                        )
+                    )
 
                 if not is_mutation:
                     import inspect
+
                     caller = inspect.getouterframes(inspect.currentframe(), 2)[1][3]
                     if caller not in list(self.api_costs.keys()):
                         self.api_costs[caller] = 0
-                    self.api_costs[caller] += result['data']['rateLimit']['cost']
+                    self.api_costs[caller] += result["data"]["rateLimit"]["cost"]
 
-                return result['data']
+                return result["data"]
             else:
                 import json
-                raise Exception('Query failed with code {code}:
{json}'.format(code=request.status_code, json=json.dumps(request.json(), indent=4)))
+
+                raise Exception(
+                    "Query failed with code {code}:
{json}".format(
+                        code=request.status_code,
+                        json=json.dumps(request.json(), indent=4),
+                    )
+                )
diff --git a/utils/grpc-client/clickhouse-grpc-client.py b/utils/grpc-client/clickhouse-grpc-client.py
index dfaa7ed4e01f..0caa9e6fdca4 100755
--- a/utils/grpc-client/clickhouse-grpc-client.py
+++ b/utils/grpc-client/clickhouse-grpc-client.py
@@ -14,14 +14,14 @@
 import grpc_tools  # pip3 install grpcio-tools
 import argparse, cmd, os, signal, subprocess, sys, threading, time, uuid
 
-DEFAULT_HOST = 'localhost'
+DEFAULT_HOST = "localhost"
 DEFAULT_PORT = 9100
-DEFAULT_USER_NAME = 'default'
-DEFAULT_OUTPUT_FORMAT_FOR_INTERACTIVE_MODE = 'PrettyCompact'
-HISTORY_FILENAME = '~/.clickhouse_grpc_history'
+DEFAULT_USER_NAME = "default"
+DEFAULT_OUTPUT_FORMAT_FOR_INTERACTIVE_MODE = "PrettyCompact"
+HISTORY_FILENAME = "~/.clickhouse_grpc_history"
 HISTORY_SIZE = 1000
 STDIN_BUFFER_SIZE = 1048576
-DEFAULT_ENCODING = 'utf-8'
+DEFAULT_ENCODING = "utf-8"
 
 
 class ClickHouseGRPCError(Exception):
@@ -51,10 +51,20 @@ def error_print(*args, **kwargs):
 
 
 class ClickHouseGRPCClient(cmd.Cmd):
-    prompt="grpc :) "
-
-    def __init__(self, host=DEFAULT_HOST, port=DEFAULT_PORT, user_name=DEFAULT_USER_NAME, password='', 
-                 database='', output_format='', settings='', verbatim=False, show_debug_info=False):
+    prompt = "grpc :) "
+
+    def __init__(
+        self,
+        host=DEFAULT_HOST,
+        port=DEFAULT_PORT,
+        user_name=DEFAULT_USER_NAME,
+        password="",
+        database="",
+        output_format="",
+        settings="",
+        verbatim=False,
+        show_debug_info=False,
+    ):
         super(ClickHouseGRPCClient, self).__init__(completekey=None)
         self.host = host
         self.port = port
@@ -80,11 +90,20 @@ def __exit__(self, exc_type, exc_value, traceback):
 
     # Executes a simple query and returns its output.
     def get_simple_query_output(self, query_text):
-        result = self.stub.ExecuteQuery(clickhouse_grpc_pb2.QueryInfo(query=query_text, user_name=self.user_name, password=self.password,
-                                                                      database=self.database, output_format='TabSeparated', settings=self.settings,
-                                                                      session_id=self.session_id, query_id=str(uuid.uuid4())))
+        result = self.stub.ExecuteQuery(
+            clickhouse_grpc_pb2.QueryInfo(
+                query=query_text,
+                user_name=self.user_name,
+                password=self.password,
+                database=self.database,
+                output_format="TabSeparated",
+                settings=self.settings,
+                session_id=self.session_id,
+                query_id=str(uuid.uuid4()),
+            )
+        )
         if self.show_debug_info:
-            print('
result={}'.format(result))
+            print("
result={}".format(result))
         ClickHouseGRPCClient.__check_no_errors(result)
         return result.output.decode(DEFAULT_ENCODING)
 
@@ -110,11 +129,19 @@ def keyboard_interrupt_handler():
 
         with KeyboardInterruptHandlerOverride(keyboard_interrupt_handler):
             try:
+
                 def send_query_info():
                     # send main query info
-                    info = clickhouse_grpc_pb2.QueryInfo(query=query_text, user_name=self.user_name, password=self.password,
-                                                         database=self.database, output_format=self.output_format, settings=self.settings,
-                                                         session_id=self.session_id, query_id=str(uuid.uuid4()))
+                    info = clickhouse_grpc_pb2.QueryInfo(
+                        query=query_text,
+                        user_name=self.user_name,
+                        password=self.password,
+                        database=self.database,
+                        output_format=self.output_format,
+                        settings=self.settings,
+                        session_id=self.session_id,
+                        query_id=str(uuid.uuid4()),
+                    )
                     # send input data
                     if not sys.stdin.isatty():
                         while True:
@@ -130,10 +157,10 @@ def send_query_info():
                         cancel_event.wait()
                         if cancel_tries > 0:
                             yield clickhouse_grpc_pb2.QueryInfo(cancel=True)
-            
+
                 for result in self.stub.ExecuteQueryWithStreamIO(send_query_info()):
                     if self.show_debug_info:
-                        print('
result={}'.format(result))
+                        print("
result={}".format(result))
                     ClickHouseGRPCClient.__check_no_errors(result)
                     sys.stdout.buffer.write(result.output)
                     sys.stdout.flush()
@@ -144,7 +171,11 @@ def send_query_info():
                 cancel_event.set()
                 if not cancelled:
                     execution_time = time.time() - start_time
-                    self.verbatim_print('
Elapsed: {execution_time} sec.
'.format(execution_time=execution_time))
+                    self.verbatim_print(
+                        "
Elapsed: {execution_time} sec.
".format(
+                            execution_time=execution_time
+                        )
+                    )
 
             except Exception as e:
                 if raise_exceptions:
@@ -153,24 +184,38 @@ def send_query_info():
 
     # Establish connection.
     def __connect(self):
-        self.verbatim_print("Connecting to {host}:{port} as user {user_name}.".format(host=self.host, port=self.port, user_name=self.user_name))
+        self.verbatim_print(
+            "Connecting to {host}:{port} as user {user_name}.".format(
+                host=self.host, port=self.port, user_name=self.user_name
+            )
+        )
         # Secure channels are supported by server but not supported by this client.
         start_time = time.time()
-        self.channel = grpc.insecure_channel(self.host + ':' + str(self.port))
+        self.channel = grpc.insecure_channel(self.host + ":" + str(self.port))
         connection_time = 0
-        timeout=5
+        timeout = 5
         while True:
             try:
                 grpc.channel_ready_future(self.channel).result(timeout=timeout)
-                break;
+                break
             except grpc.FutureTimeoutError:
                 connection_time += timeout
-                self.verbatim_print("Couldn't connect to ClickHouse server in {connection_time} seconds.".format(connection_time=connection_time))
+                self.verbatim_print(
+                    "Couldn't connect to ClickHouse server in {connection_time} seconds.".format(
+                        connection_time=connection_time
+                    )
+                )
         self.stub = clickhouse_grpc_pb2_grpc.ClickHouseStub(self.channel)
         connection_time = time.time() - start_time
         if self.verbatim:
-            version = self.get_simple_query_output("SELECT version() FORMAT TabSeparated").rstrip('
')
-            self.verbatim_print("Connected to ClickHouse server version {version} via gRPC protocol in {connection_time}.".format(version=version, connection_time=connection_time))
+            version = self.get_simple_query_output(
+                "SELECT version() FORMAT TabSeparated"
+            ).rstrip("
")
+            self.verbatim_print(
+                "Connected to ClickHouse server version {version} via gRPC protocol in {connection_time}.".format(
+                    version=version, connection_time=connection_time
+                )
+            )
 
     def __disconnect(self):
         if self.channel:
@@ -181,32 +226,39 @@ def __disconnect(self):
 
     @staticmethod
     def __check_no_errors(result):
-        if result.HasField('exception'):
+        if result.HasField("exception"):
             raise ClickHouseGRPCError(result.exception.display_text)
 
     # Use grpcio-tools to generate *pb2.py files from *.proto.
     @staticmethod
     def __generate_pb2():
         script_dir = os.path.dirname(os.path.realpath(__file__))
-        proto_dir = os.path.join(script_dir, './protos')
-        gen_dir = os.path.join(script_dir, './_gen')
-        if os.path.exists(os.path.join(gen_dir, 'clickhouse_grpc_pb2_grpc.py')):
+        proto_dir = os.path.join(script_dir, "./protos")
+        gen_dir = os.path.join(script_dir, "./_gen")
+        if os.path.exists(os.path.join(gen_dir, "clickhouse_grpc_pb2_grpc.py")):
             return
         os.makedirs(gen_dir, exist_ok=True)
-        cmd = ['python3', '-m', 'grpc_tools.protoc', '-I'+proto_dir, '--python_out='+gen_dir, '--grpc_python_out='+gen_dir,
-               proto_dir+'/clickhouse_grpc.proto']
+        cmd = [
+            "python3",
+            "-m",
+            "grpc_tools.protoc",
+            "-I" + proto_dir,
+            "--python_out=" + gen_dir,
+            "--grpc_python_out=" + gen_dir,
+            proto_dir + "/clickhouse_grpc.proto",
+        ]
         p = subprocess.Popen(cmd, stderr=subprocess.PIPE)
         # We don't want to show grpc_tools warnings.
-        errors = p.stderr.read().decode().strip('
').split('
')
-        only_warnings = all(('Warning' in error) for error in errors)
+        errors = p.stderr.read().decode().strip("
").split("
")
+        only_warnings = all(("Warning" in error) for error in errors)
         if not only_warnings:
-            error_print('
'.join(errors))
+            error_print("
".join(errors))
 
     # Import the generated *pb2.py files.
     @staticmethod
     def __import_pb2():
         script_dir = os.path.dirname(os.path.realpath(__file__))
-        gen_dir = os.path.join(script_dir, './_gen')
+        gen_dir = os.path.join(script_dir, "./_gen")
         sys.path.append(gen_dir)
         global clickhouse_grpc_pb2, clickhouse_grpc_pb2_grpc
         import clickhouse_grpc_pb2, clickhouse_grpc_pb2_grpc
@@ -231,9 +283,9 @@ def postloop(self):
     # Overrides Cmd.onecmd(). Runs single command.
     def onecmd(self, line):
         stripped = line.strip()
-        if stripped == 'exit' or stripped == 'quit':
+        if stripped == "exit" or stripped == "quit":
             return True
-        if stripped == '':
+        if stripped == "":
             return False
         self.run_query(line, raise_exceptions=False, allow_cancel=True)
         return False
@@ -261,17 +313,61 @@ def __write_history():
 
 # MAIN
 
+
 def main(args):
-    parser = argparse.ArgumentParser(description='ClickHouse client accessing server through gRPC protocol.', add_help=False)
-    parser.add_argument('--help', help='Show this help message and exit', action='store_true')
-    parser.add_argument('--host', '-h', help='The server name, ‘localhost’ by default. You can use either the name or the IPv4 or IPv6 address.', default='localhost')
-    parser.add_argument('--port', help='The port to connect to. This port should be enabled on the ClickHouse server (see grpc_port in the config).', default=9100)
-    parser.add_argument('--user', '-u', dest='user_name', help='The username. Default value: ‘default’.', default='default')
-    parser.add_argument('--password', help='The password. Default value: empty string.', default='')
-    parser.add_argument('--query', '-q', help='The query to process when using non-interactive mode.', default='')
-    parser.add_argument('--database', '-d', help='Select the current default database. Default value: the current database from the server settings (‘default’ by default).', default='')
-    parser.add_argument('--format', '-f', dest='output_format', help='Use the specified default format to output the result.', default='')
-    parser.add_argument('--debug', dest='show_debug_info', help='Enables showing the debug information.', action='store_true')
+    parser = argparse.ArgumentParser(
+        description="ClickHouse client accessing server through gRPC protocol.",
+        add_help=False,
+    )
+    parser.add_argument(
+        "--help", help="Show this help message and exit", action="store_true"
+    )
+    parser.add_argument(
+        "--host",
+        "-h",
+        help="The server name, ‘localhost’ by default. You can use either the name or the IPv4 or IPv6 address.",
+        default="localhost",
+    )
+    parser.add_argument(
+        "--port",
+        help="The port to connect to. This port should be enabled on the ClickHouse server (see grpc_port in the config).",
+        default=9100,
+    )
+    parser.add_argument(
+        "--user",
+        "-u",
+        dest="user_name",
+        help="The username. Default value: ‘default’.",
+        default="default",
+    )
+    parser.add_argument(
+        "--password", help="The password. Default value: empty string.", default=""
+    )
+    parser.add_argument(
+        "--query",
+        "-q",
+        help="The query to process when using non-interactive mode.",
+        default="",
+    )
+    parser.add_argument(
+        "--database",
+        "-d",
+        help="Select the current default database. Default value: the current database from the server settings (‘default’ by default).",
+        default="",
+    )
+    parser.add_argument(
+        "--format",
+        "-f",
+        dest="output_format",
+        help="Use the specified default format to output the result.",
+        default="",
+    )
+    parser.add_argument(
+        "--debug",
+        dest="show_debug_info",
+        help="Enables showing the debug information.",
+        action="store_true",
+    )
     args = parser.parse_args(args)
 
     if args.help:
@@ -284,11 +380,18 @@ def main(args):
     output_format = args.output_format
     if not output_format and interactive_mode:
         output_format = DEFAULT_OUTPUT_FORMAT_FOR_INTERACTIVE_MODE
-   
+
     try:
-        with ClickHouseGRPCClient(host=args.host, port=args.port, user_name=args.user_name, password=args.password,
-                                  database=args.database, output_format=output_format, verbatim=verbatim,
-                                  show_debug_info=args.show_debug_info) as client:
+        with ClickHouseGRPCClient(
+            host=args.host,
+            port=args.port,
+            user_name=args.user_name,
+            password=args.password,
+            database=args.database,
+            output_format=output_format,
+            verbatim=verbatim,
+            show_debug_info=args.show_debug_info,
+        ) as client:
             if interactive_mode:
                 client.cmdloop()
             else:
@@ -301,5 +404,6 @@ def main(args):
     if verbatim:
         print("
Bye")
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     main(sys.argv[1:])
diff --git a/utils/kafka/consume.py b/utils/kafka/consume.py
index c82901f9e0eb..74542baf218b 100755
--- a/utils/kafka/consume.py
+++ b/utils/kafka/consume.py
@@ -9,24 +9,40 @@
 
 
 def main():
-    parser = argparse.ArgumentParser(description='Kafka Producer client')
-    parser.add_argument('--server', type=str, metavar='HOST', default='localhost',
-        help='Kafka bootstrap-server address')
-    parser.add_argument('--port', type=int, metavar='PORT', default=9092,
-        help='Kafka bootstrap-server port')
-    parser.add_argument('--client', type=str, default='ch-kafka-python',
-        help='custom client id for this producer')
-    parser.add_argument('--topic', type=str, required=True,
-        help='name of Kafka topic to store in')
-    parser.add_argument('--group', type=str, required=True,
-        help='name of the consumer group')
+    parser = argparse.ArgumentParser(description="Kafka Producer client")
+    parser.add_argument(
+        "--server",
+        type=str,
+        metavar="HOST",
+        default="localhost",
+        help="Kafka bootstrap-server address",
+    )
+    parser.add_argument(
+        "--port",
+        type=int,
+        metavar="PORT",
+        default=9092,
+        help="Kafka bootstrap-server port",
+    )
+    parser.add_argument(
+        "--client",
+        type=str,
+        default="ch-kafka-python",
+        help="custom client id for this producer",
+    )
+    parser.add_argument(
+        "--topic", type=str, required=True, help="name of Kafka topic to store in"
+    )
+    parser.add_argument(
+        "--group", type=str, required=True, help="name of the consumer group"
+    )
 
     args = parser.parse_args()
     config = {
-        'bootstrap_servers': f'{args.server}:{args.port}',
-        'client_id': args.client,
-        'group_id': args.group,
-        'auto_offset_reset': 'earliest',
+        "bootstrap_servers": f"{args.server}:{args.port}",
+        "client_id": args.client,
+        "group_id": args.group,
+        "auto_offset_reset": "earliest",
     }
     client = kafka.KafkaConsumer(**config)
 
diff --git a/utils/kafka/manage.py b/utils/kafka/manage.py
index 7458bdceb74b..578a7df73101 100755
--- a/utils/kafka/manage.py
+++ b/utils/kafka/manage.py
@@ -8,24 +8,48 @@
 
 
 def main():
-    parser = argparse.ArgumentParser(description='Kafka Topic manager')
-    parser.add_argument('--server', type=str, metavar='HOST', default='localhost',
-        help='Kafka bootstrap-server address')
-    parser.add_argument('--port', type=int, metavar='PORT', default=9092,
-        help='Kafka bootstrap-server port')
-    parser.add_argument('--client', type=str, default='ch-kafka-python',
-        help='custom client id for this producer')
+    parser = argparse.ArgumentParser(description="Kafka Topic manager")
+    parser.add_argument(
+        "--server",
+        type=str,
+        metavar="HOST",
+        default="localhost",
+        help="Kafka bootstrap-server address",
+    )
+    parser.add_argument(
+        "--port",
+        type=int,
+        metavar="PORT",
+        default=9092,
+        help="Kafka bootstrap-server port",
+    )
+    parser.add_argument(
+        "--client",
+        type=str,
+        default="ch-kafka-python",
+        help="custom client id for this producer",
+    )
 
     commands = parser.add_mutually_exclusive_group()
-    commands.add_argument('--create', type=str, metavar='TOPIC', nargs='+',
-        help='create new topic(s) in the cluster')
-    commands.add_argument('--delete', type=str, metavar='TOPIC', nargs='+',
-        help='delete existing topic(s) from the cluster')
+    commands.add_argument(
+        "--create",
+        type=str,
+        metavar="TOPIC",
+        nargs="+",
+        help="create new topic(s) in the cluster",
+    )
+    commands.add_argument(
+        "--delete",
+        type=str,
+        metavar="TOPIC",
+        nargs="+",
+        help="delete existing topic(s) from the cluster",
+    )
 
     args = parser.parse_args()
     config = {
-        'bootstrap_servers': f'{args.server}:{args.port}',
-        'client_id': args.client,
+        "bootstrap_servers": f"{args.server}:{args.port}",
+        "client_id": args.client,
     }
 
     client = kafka.KafkaAdminClient(**config)
diff --git a/utils/kafka/produce.py b/utils/kafka/produce.py
index 97e2e6b7705a..f82e56d8478b 100755
--- a/utils/kafka/produce.py
+++ b/utils/kafka/produce.py
@@ -13,50 +13,82 @@
 
 
 class Sync(enum.Enum):
-    NONE = 'none'
-    LEAD = 'leader'
-    ALL = 'all'
+    NONE = "none"
+    LEAD = "leader"
+    ALL = "all"
 
     def __str__(self):
         return self.value
 
     def convert(self):
         values = {
-            str(Sync.NONE): '0',
-            str(Sync.LEAD): '1',
-            str(Sync.ALL): 'all',
+            str(Sync.NONE): "0",
+            str(Sync.LEAD): "1",
+            str(Sync.ALL): "all",
         }
         return values[self.value]
 
 
 def main():
-    parser = argparse.ArgumentParser(description='Produce a single message taken from input')
-    parser.add_argument('--server', type=str, metavar='HOST', default='localhost',
-        help='Kafka bootstrap-server address')
-    parser.add_argument('--port', type=int, metavar='PORT', default=9092,
-        help='Kafka bootstrap-server port')
-    parser.add_argument('--client', type=str, default='ch-kafka-python',
-        help='custom client id for this producer')
-    parser.add_argument('--topic', type=str, required=True,
-        help='name of Kafka topic to store in')
-    parser.add_argument('--retries', type=int, default=0,
-        help='number of retries to send on failure')
-    parser.add_argument('--multiply', type=int, default=1,
-        help='multiplies incoming string many times')
-    parser.add_argument('--repeat', type=int, default=1,
-        help='send same (multiplied) message many times')
+    parser = argparse.ArgumentParser(
+        description="Produce a single message taken from input"
+    )
+    parser.add_argument(
+        "--server",
+        type=str,
+        metavar="HOST",
+        default="localhost",
+        help="Kafka bootstrap-server address",
+    )
+    parser.add_argument(
+        "--port",
+        type=int,
+        metavar="PORT",
+        default=9092,
+        help="Kafka bootstrap-server port",
+    )
+    parser.add_argument(
+        "--client",
+        type=str,
+        default="ch-kafka-python",
+        help="custom client id for this producer",
+    )
+    parser.add_argument(
+        "--topic", type=str, required=True, help="name of Kafka topic to store in"
+    )
+    parser.add_argument(
+        "--retries", type=int, default=0, help="number of retries to send on failure"
+    )
+    parser.add_argument(
+        "--multiply", type=int, default=1, help="multiplies incoming string many times"
+    )
+    parser.add_argument(
+        "--repeat",
+        type=int,
+        default=1,
+        help="send same (multiplied) message many times",
+    )
 
     mode_group = parser.add_mutually_exclusive_group()
-    mode_group.add_argument('--jobs', type=int, default=multiprocessing.cpu_count(),
-        help='number of concurrent jobs')
-    mode_group.add_argument('--delay', type=int, metavar='SECONDS', default=0,
-        help='delay before sending next message')
+    mode_group.add_argument(
+        "--jobs",
+        type=int,
+        default=multiprocessing.cpu_count(),
+        help="number of concurrent jobs",
+    )
+    mode_group.add_argument(
+        "--delay",
+        type=int,
+        metavar="SECONDS",
+        default=0,
+        help="delay before sending next message",
+    )
 
     args = parser.parse_args()
     config = {
-        'bootstrap_servers': f'{args.server}:{args.port}',
-        'client_id': args.client,
-        'retries': args.retries,
+        "bootstrap_servers": f"{args.server}:{args.port}",
+        "client_id": args.client,
+        "retries": args.retries,
     }
     client = kafka.KafkaProducer(**config)
 
@@ -66,7 +98,7 @@ def send(num):
         if args.delay > 0:
             time.sleep(args.delay)
         client.send(topic=args.topic, value=message)
-        print(f'iteration {num}: sent a message multiplied {args.multiply} times')
+        print(f"iteration {num}: sent a message multiplied {args.multiply} times")
 
     if args.delay > 0:
         args.jobs = 1
diff --git a/utils/kafka/status.py b/utils/kafka/status.py
index 28ba3c9c36f9..12ea3d23bdfb 100755
--- a/utils/kafka/status.py
+++ b/utils/kafka/status.py
@@ -8,18 +8,34 @@
 
 
 def main():
-    parser = argparse.ArgumentParser(description='Kafka client to get groups and topics status')
-    parser.add_argument('--server', type=str, metavar='HOST', default='localhost',
-        help='Kafka bootstrap-server address')
-    parser.add_argument('--port', type=int, metavar='PORT', default=9092,
-        help='Kafka bootstrap-server port')
-    parser.add_argument('--client', type=str, default='ch-kafka-python',
-        help='custom client id for this producer')
+    parser = argparse.ArgumentParser(
+        description="Kafka client to get groups and topics status"
+    )
+    parser.add_argument(
+        "--server",
+        type=str,
+        metavar="HOST",
+        default="localhost",
+        help="Kafka bootstrap-server address",
+    )
+    parser.add_argument(
+        "--port",
+        type=int,
+        metavar="PORT",
+        default=9092,
+        help="Kafka bootstrap-server port",
+    )
+    parser.add_argument(
+        "--client",
+        type=str,
+        default="ch-kafka-python",
+        help="custom client id for this producer",
+    )
 
     args = parser.parse_args()
     config = {
-        'bootstrap_servers': f'{args.server}:{args.port}',
-        'client_id': args.client,
+        "bootstrap_servers": f"{args.server}:{args.port}",
+        "client_id": args.client,
     }
 
     client = kafka.KafkaAdminClient(**config)
@@ -28,10 +44,13 @@ def main():
 
     topics = cluster.topics()
     for topic in topics:
-        print(f'Topic "{topic}":', end='')
+        print(f'Topic "{topic}":', end="")
         for partition in cluster.partitions_for_topic(topic):
             tp = kafka.TopicPartition(topic, partition)
-            print(f' {partition} (begin: {consumer.beginning_offsets([tp])[tp]}, end: {consumer.end_offsets([tp])[tp]})', end='')
+            print(
+                f" {partition} (begin: {consumer.beginning_offsets([tp])[tp]}, end: {consumer.end_offsets([tp])[tp]})",
+                end="",
+            )
         print()
 
     groups = client.list_consumer_groups()
@@ -41,7 +60,9 @@ def main():
         consumer = kafka.KafkaConsumer(**config, group_id=group[0])
         offsets = client.list_consumer_group_offsets(group[0])
         for topic, offset in offsets.items():
-            print(f'\t{topic.topic}[{topic.partition}]: {consumer.beginning_offsets([topic])[topic]}, {offset.offset}, {consumer.end_offsets([topic])[topic]}')
+            print(
+                f"\t{topic.topic}[{topic.partition}]: {consumer.beginning_offsets([topic])[topic]}, {offset.offset}, {consumer.end_offsets([topic])[topic]}"
+            )
         consumer.close()
 
     client.close()
diff --git a/utils/zero_copy/zero_copy_schema_converter.py b/utils/zero_copy/zero_copy_schema_converter.py
index c5edef78cce7..6103ac69c6e3 100755
--- a/utils/zero_copy/zero_copy_schema_converter.py
+++ b/utils/zero_copy/zero_copy_schema_converter.py
@@ -10,56 +10,90 @@ def parse_args():
     Parse command-line arguments.
     """
     parser = argparse.ArgumentParser()
-    parser.add_argument('--hosts', default=socket.getfqdn() + ':2181', help='ZooKeeper hosts (host:port,host:port,...)')
-    parser.add_argument('-s', '--secure', default=False, action='store_true', help='Use secure connection')
-    parser.add_argument('--cert', default='', help='Client TLS certificate file')
-    parser.add_argument('--key', default='', help='Client TLS key file')
-    parser.add_argument('--ca', default='', help='Client TLS ca file')
-    parser.add_argument('-u', '--user', default='', help='ZooKeeper ACL user')
-    parser.add_argument('-p', '--password', default='', help='ZooKeeper ACL password')
-    parser.add_argument('-r', '--root', default='/clickhouse', help='ZooKeeper root path for ClickHouse')
-    parser.add_argument('-z', '--zcroot', default='clickhouse/zero_copy', help='ZooKeeper node for new zero-copy data')
-    parser.add_argument('--dryrun', default=False, action='store_true', help='Do not perform any actions')
-    parser.add_argument('--cleanup', default=False, action='store_true', help='Clean old nodes')
-    parser.add_argument('-v', '--verbose', action='store_true', default=False, help='Verbose mode')
+    parser.add_argument(
+        "--hosts",
+        default=socket.getfqdn() + ":2181",
+        help="ZooKeeper hosts (host:port,host:port,...)",
+    )
+    parser.add_argument(
+        "-s",
+        "--secure",
+        default=False,
+        action="store_true",
+        help="Use secure connection",
+    )
+    parser.add_argument("--cert", default="", help="Client TLS certificate file")
+    parser.add_argument("--key", default="", help="Client TLS key file")
+    parser.add_argument("--ca", default="", help="Client TLS ca file")
+    parser.add_argument("-u", "--user", default="", help="ZooKeeper ACL user")
+    parser.add_argument("-p", "--password", default="", help="ZooKeeper ACL password")
+    parser.add_argument(
+        "-r", "--root", default="/clickhouse", help="ZooKeeper root path for ClickHouse"
+    )
+    parser.add_argument(
+        "-z",
+        "--zcroot",
+        default="clickhouse/zero_copy",
+        help="ZooKeeper node for new zero-copy data",
+    )
+    parser.add_argument(
+        "--dryrun",
+        default=False,
+        action="store_true",
+        help="Do not perform any actions",
+    )
+    parser.add_argument(
+        "--cleanup", default=False, action="store_true", help="Clean old nodes"
+    )
+    parser.add_argument(
+        "-v", "--verbose", action="store_true", default=False, help="Verbose mode"
+    )
 
     return parser.parse_args()
 
 
 # Several folders to heuristic that zookeepr node is folder node
 # May be false positive when someone creates set of tables with same paths
-table_nodes = ['alter_partition_version', 'block_numbers', 'blocks', 'columns', 'leader_election']
-zc_nodes = ['zero_copy_s3', 'zero_copy_hdfs']
+table_nodes = [
+    "alter_partition_version",
+    "block_numbers",
+    "blocks",
+    "columns",
+    "leader_election",
+]
+zc_nodes = ["zero_copy_s3", "zero_copy_hdfs"]
 
 
 def convert_node(client, args, path, zc_node):
-    base_path = f'{path}/{zc_node}/shared'
+    base_path = f"{path}/{zc_node}/shared"
     parts = client.get_children(base_path)
-    table_id_path = f'{path}/table_shared_id'
-    table_id = ''
+    table_id_path = f"{path}/table_shared_id"
+    table_id = ""
     if client.exists(table_id_path):
-        table_id = client.get(table_id_path)[0].decode('UTF-8')
+        table_id = client.get(table_id_path)[0].decode("UTF-8")
     else:
         table_id = str(uuid.uuid4())
         if args.verbose:
             print(f'Make table_id "{table_id_path}" = "{table_id}"')
         if not args.dryrun:
-            client.create(table_id_path, bytes(table_id, 'UTF-8'))
+            client.create(table_id_path, bytes(table_id, "UTF-8"))
     for part in parts:
-        part_path = f'{base_path}/{part}'
+        part_path = f"{base_path}/{part}"
         uniq_ids = client.get_children(part_path)
         for uniq_id in uniq_ids:
-            uniq_path = f'{part_path}/{uniq_id}'
+            uniq_path = f"{part_path}/{uniq_id}"
             replicas = client.get_children(uniq_path)
             for replica in replicas:
-                replica_path = f'{uniq_path}/{replica}'
-                new_path = f'{args.root}/{args.zcroot}/{zc_node}/{table_id}/{part}/{uniq_id}/{replica}'
+                replica_path = f"{uniq_path}/{replica}"
+                new_path = f"{args.root}/{args.zcroot}/{zc_node}/{table_id}/{part}/{uniq_id}/{replica}"
                 if not client.exists(new_path):
                     if args.verbose:
                         print(f'Make node "{new_path}"')
                     if not args.dryrun:
-                        client.ensure_path(f'{args.root}/{args.zcroot}/{zc_node}/{table_id}/{part}/{uniq_id}')
-                        client.create(new_path, value=b'lock')
+                        client.ensure_path(
+                            f"{args.root}/{args.zcroot}/{zc_node}/{table_id}/{part}/{uniq_id}"
+                        )
+                        client.create(new_path, value=b"lock")
                 if args.cleanup:
                     if args.verbose:
                         print(f'Remove node "{replica_path}"')
@@ -71,7 +105,7 @@ def convert_node(client, args, path, zc_node):
             client.delete(part_path)
     if args.cleanup and not args.dryrun:
         client.delete(base_path)
-        client.delete(f'{path}/{zc_node}')
+        client.delete(f"{path}/{zc_node}")
 
 
 def convert_table(client, args, path, nodes):
@@ -94,29 +128,30 @@ def scan_recursive(client, args, path):
         convert_table(client, args, path, nodes)
     else:
         for node in nodes:
-            scan_recursive(client, args, f'{path}/{node}')
+            scan_recursive(client, args, f"{path}/{node}")
 
 
 def scan(client, args):
     nodes = client.get_children(args.root)
     for node in nodes:
         if node != args.zcroot:
-            scan_recursive(client, args, f'{args.root}/{node}')
+            scan_recursive(client, args, f"{args.root}/{node}")
 
 
 def get_client(args):
-    client = KazooClient(connection_retry=3,
-                         command_retry=3,
-                         timeout=1,
-                         hosts=args.hosts,
-                         use_ssl=args.secure,
-                         certfile=args.cert,
-                         keyfile=args.key,
-                         ca=args.ca
-                         )
+    client = KazooClient(
+        connection_retry=3,
+        command_retry=3,
+        timeout=1,
+        hosts=args.hosts,
+        use_ssl=args.secure,
+        certfile=args.cert,
+        keyfile=args.key,
+        ca=args.ca,
+    )
     client.start()
-    if (args.user and args.password):
-        client.add_auth('digest', f'{args.user}:{args.password}')
+    if args.user and args.password:
+        client.add_auth("digest", f"{args.user}:{args.password}")
     return client
 
 
@@ -126,5 +161,5 @@ def main():
     scan(client, args)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
