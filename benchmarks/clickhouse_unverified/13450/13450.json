{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 13450,
  "instance_id": "ClickHouse__ClickHouse-13450",
  "issue_numbers": [
    "11884",
    "7228",
    "13361",
    "6704"
  ],
  "base_commit": "d0b6ba35d19a1a0112b7e73a34334f329d6c04c3",
  "patch": "diff --git a/src/Interpreters/DDLWorker.cpp b/src/Interpreters/DDLWorker.cpp\nindex 6e957aee67b8..c826d83a0816 100644\n--- a/src/Interpreters/DDLWorker.cpp\n+++ b/src/Interpreters/DDLWorker.cpp\n@@ -863,45 +863,53 @@ bool DDLWorker::tryExecuteQueryOnLeaderReplica(\n     String shard_node_name = get_shard_name(task.cluster->getShardsAddresses().at(task.host_shard_num));\n     String shard_path = node_path + \"/shards/\" + shard_node_name;\n     String is_executed_path = shard_path + \"/executed\";\n+    String tries_to_execute_path = shard_path + \"/tries_to_execute\";\n     zookeeper->createAncestors(shard_path + \"/\");\n \n-    auto is_already_executed = [&]() -> bool\n-    {\n-        String executed_by;\n-        if (zookeeper->tryGet(is_executed_path, executed_by))\n-        {\n-            LOG_DEBUG(log, \"Task {} has already been executed by leader replica ({}) of the same shard.\", task.entry_name, executed_by);\n-            return true;\n-        }\n+    /// Node exists, or we will create or we will get an exception\n+    zookeeper->tryCreate(tries_to_execute_path, \"0\", zkutil::CreateMode::Persistent);\n \n-        return false;\n-    };\n+    static constexpr int MAX_TRIES_TO_EXECUTE = 3;\n+\n+    String executed_by;\n+\n+    zkutil::EventPtr event = std::make_shared<Poco::Event>();\n+    if (zookeeper->tryGet(is_executed_path, executed_by))\n+    {\n+        LOG_DEBUG(log, \"Task {} has already been executed by replica ({}) of the same shard.\", task.entry_name, executed_by);\n+        return true;\n+    }\n \n     pcg64 rng(randomSeed());\n \n     auto lock = createSimpleZooKeeperLock(zookeeper, shard_path, \"lock\", task.host_id_str);\n-    static const size_t max_tries = 20;\n+\n     bool executed_by_leader = false;\n-    for (size_t num_tries = 0; num_tries < max_tries; ++num_tries)\n+    while (true)\n     {\n-        if (is_already_executed())\n-        {\n-            executed_by_leader = true;\n-            break;\n-        }\n-\n         StorageReplicatedMergeTree::Status status;\n         replicated_storage->getStatus(status);\n \n-        /// Leader replica take lock\n+        /// Any replica which is leader tries to take lock\n         if (status.is_leader && lock->tryLock())\n         {\n-            if (is_already_executed())\n+            /// In replicated merge tree we can have multiple leaders. So we can\n+            /// be \"leader\", but another \"leader\" replica may already execute\n+            /// this task.\n+            if (zookeeper->tryGet(is_executed_path, executed_by))\n             {\n+                LOG_DEBUG(log, \"Task {} has already been executed by replica ({}) of the same shard.\", task.entry_name, executed_by);\n                 executed_by_leader = true;\n                 break;\n             }\n \n+            /// Doing it exclusively\n+            size_t counter = parse<int>(zookeeper->get(tries_to_execute_path));\n+            if (counter > MAX_TRIES_TO_EXECUTE)\n+                break;\n+\n+            zookeeper->set(tries_to_execute_path, toString(counter + 1));\n+\n             /// If the leader will unexpectedly changed this method will return false\n             /// and on the next iteration new leader will take lock\n             if (tryExecuteQuery(rewritten_query, task, task.execution_status))\n@@ -910,20 +918,31 @@ bool DDLWorker::tryExecuteQueryOnLeaderReplica(\n                 executed_by_leader = true;\n                 break;\n             }\n+\n+            lock->unlock();\n         }\n \n-        /// Does nothing if wasn't previously locked\n-        lock->unlock();\n-        std::this_thread::sleep_for(std::chrono::milliseconds(std::uniform_int_distribution<int>(0, 1000)(rng)));\n+\n+        if (event->tryWait(std::uniform_int_distribution<int>(0, 1000)(rng)))\n+        {\n+            executed_by_leader = true;\n+            break;\n+        }\n+        else if (parse<int>(zookeeper->get(tries_to_execute_path)) > MAX_TRIES_TO_EXECUTE)\n+        {\n+            /// Nobody will try to execute query again\n+            break;\n+        }\n     }\n \n     /// Not executed by leader so was not executed at all\n     if (!executed_by_leader)\n     {\n-        task.execution_status = ExecutionStatus(ErrorCodes::NOT_IMPLEMENTED,\n-                                                \"Cannot execute replicated DDL query on leader\");\n+        task.execution_status = ExecutionStatus(ErrorCodes::NOT_IMPLEMENTED, \"Cannot execute replicated DDL query\");\n         return false;\n     }\n+\n+    LOG_DEBUG(log, \"Task {} has already been executed by replica ({}) of the same shard.\", task.entry_name, zookeeper->get(is_executed_path));\n     return true;\n }\n \n",
  "test_patch": "diff --git a/tests/integration/test_on_cluster_timeouts/__init__.py b/tests/integration/test_on_cluster_timeouts/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_on_cluster_timeouts/configs/remote_servers.xml b/tests/integration/test_on_cluster_timeouts/configs/remote_servers.xml\nnew file mode 100644\nindex 000000000000..4c3de4b39054\n--- /dev/null\n+++ b/tests/integration/test_on_cluster_timeouts/configs/remote_servers.xml\n@@ -0,0 +1,28 @@\n+<yandex>\n+    <remote_servers>\n+        <test_cluster>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>node1</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node2</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+            <shard>\n+                <internal_replication>true</internal_replication>\n+                <replica>\n+                    <host>node3</host>\n+                    <port>9000</port>\n+                </replica>\n+                <replica>\n+                    <host>node4</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </test_cluster>\n+    </remote_servers>\n+</yandex>\ndiff --git a/tests/integration/test_on_cluster_timeouts/configs/users_config.xml b/tests/integration/test_on_cluster_timeouts/configs/users_config.xml\nnew file mode 100644\nindex 000000000000..3a2ad0325f7f\n--- /dev/null\n+++ b/tests/integration/test_on_cluster_timeouts/configs/users_config.xml\n@@ -0,0 +1,23 @@\n+<yandex>\n+    <profiles>\n+        <default>\n+            <mutations_sync>2</mutations_sync>\n+        </default>\n+    </profiles>\n+\n+    <users>\n+        <default>\n+            <password></password>\n+            <networks incl=\"networks\" replace=\"replace\">\n+                <ip>::/0</ip>\n+            </networks>\n+            <profile>default</profile>\n+            <quota>default</quota>\n+        </default>\n+    </users>\n+\n+    <quotas>\n+        <default>\n+        </default>\n+    </quotas>\n+</yandex>\ndiff --git a/tests/integration/test_on_cluster_timeouts/test.py b/tests/integration/test_on_cluster_timeouts/test.py\nnew file mode 100644\nindex 000000000000..965bf8fae1bf\n--- /dev/null\n+++ b/tests/integration/test_on_cluster_timeouts/test.py\n@@ -0,0 +1,42 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.test_tools import assert_eq_with_retry\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance('node1', main_configs=['configs/remote_servers.xml'], user_configs=['configs/users_config.xml'], with_zookeeper=True)\n+node2 = cluster.add_instance('node2', main_configs=['configs/remote_servers.xml'], user_configs=['configs/users_config.xml'], with_zookeeper=True)\n+node3 = cluster.add_instance('node3', main_configs=['configs/remote_servers.xml'], user_configs=['configs/users_config.xml'], with_zookeeper=True)\n+node4 = cluster.add_instance('node4', main_configs=['configs/remote_servers.xml'], user_configs=['configs/users_config.xml'], with_zookeeper=True)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_long_query(started_cluster):\n+    node1.query(\"CREATE TABLE cluster_table (key UInt64, value String) ENGINE = ReplicatedMergeTree('/test/1/cluster_table', '1') ORDER BY tuple()\")\n+    node2.query(\"CREATE TABLE cluster_table (key UInt64, value String) ENGINE = ReplicatedMergeTree('/test/1/cluster_table', '2') ORDER BY tuple()\")\n+\n+    node1.query(\"INSERT INTO cluster_table SELECT number, toString(number) FROM numbers(20)\")\n+    node2.query(\"SYSTEM SYNC REPLICA cluster_table\")\n+\n+    node3.query(\"CREATE TABLE cluster_table (key UInt64, value String) ENGINE = ReplicatedMergeTree('/test/2/cluster_table', '1') ORDER BY tuple()\")\n+\n+    node4.query(\"CREATE TABLE cluster_table (key UInt64, value String) ENGINE = ReplicatedMergeTree('/test/2/cluster_table', '2') ORDER BY tuple()\")\n+    node3.query(\"INSERT INTO cluster_table SELECT number, toString(number) FROM numbers(20)\")\n+    node4.query(\"SYSTEM SYNC REPLICA cluster_table\")\n+\n+    node1.query(\"ALTER TABLE cluster_table ON CLUSTER 'test_cluster' UPDATE key = 1 WHERE sleepEachRow(1) == 0\", settings={\"mutations_sync\": \"2\"})\n+\n+    assert node1.query(\"SELECT SUM(key) FROM cluster_table\") == \"20\\n\"\n+    assert node2.query(\"SELECT SUM(key) FROM cluster_table\") == \"20\\n\"\n+    assert node3.query(\"SELECT SUM(key) FROM cluster_table\") == \"20\\n\"\n+    assert node4.query(\"SELECT SUM(key) FROM cluster_table\") == \"20\\n\"\n",
  "problem_statement": "Long running Distributed DDL fail with `Cannot execute replicated DDL query on leader`\nHello,\r\n\r\nI'm running into the following issue with some Distributed DDL queries, ie ALTER MODIFY COLUMN which are long running due to the need of rewriting data. They fail with `Cannot execute replicated DDL query on leader` error, but they don't have to.\r\n\r\nThe issues is here: https://github.com/ClickHouse/Clickhouse/blob/42b8ed3ec64d7077422afb898db174edf6c191b0/src/Interpreters/DDLWorker.cpp#L776 replicas wait only 20 seconds (20 tries with 1 second sleep after each) when checking the status of the migration, since those migrations are taking longer than that it fails with the above mentioned error.\r\n\r\nMaybe we should wait as long as the execution lock is held by the a leader instead?\r\n\r\nExample query:\r\n\r\n```sql\r\nALTER TABLE test\r\n    ON CLUSTER 'test'\r\n    MODIFY COLUMN float_as_uint UInt16;\r\n```\r\n\r\nNote: I'm testing this on v19 but I believe current master is affected as well.\nOPTIMIZE TABLE ON CLUSTER fail\nHello,\r\nI have a problem when I play an OPTIMIZE ON CLUSTER on a ReplicatedMergeTree table. Here is the log:\r\n\r\n```\r\n2019.10.08 14:04:09.509533 [ 71 ] {9330b2b2-c41f-4b3f-8e29-540fb07b2c38} <Debug> DDLWorker: Processing task query-0001193846 (OPTIMIZE TABLE db.table_name ON CLUSTER lo01  FINAL)\r\n\r\n2019.10.08 14:04:09.511245 [ 71 ] {9330b2b2-c41f-4b3f-8e29-540fb07b2c38} <Warning> DDLWorker: An error occurred while processing task query-0001193846 (OPTIMIZE TABLE db.table_name ON CLUSTER lo01 FINAL) : Code: 999, e.displayText() = Coordination::Exception: Transaction failed (No node): Op #1, path: /clickhouse/task_queue/ddl/query-0001193846/finished/clkh%2D01%2D03:9000, Stack trace:\r\n\r\n0. /usr/bin/clickhouse-server(StackTrace::StackTrace()+0x22) [0x781c272]\r\n1. /usr/bin/clickhouse-server() [0x6f3eda1]\r\n2. /usr/bin/clickhouse-server(Coordination::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int)+0x16e) [0x6f3f8be]\r\n3. /usr/bin/clickhouse-server(zkutil::KeeperMultiException::KeeperMultiException(int, std::vector<std::shared_ptr<Coordination::Request>, std::allocator<std::shared_ptr<Coordination::Request> > > const&, std::vector<std::shared_ptr<Coordination::Response>, std::allocator<std::shared_ptr<Coordination::Response> > > const&)+0x56) [0x6f4e0a6]\r\n4. /usr/bin/clickhouse-server(zkutil::KeeperMultiException::check(int, std::vector<std::shared_ptr<Coordination::Request>, std::allocator<std::shared_ptr<Coordination::Request> > > const&, std::vector<std::shared_ptr<Coordination::Response>, std::allocator<std::shared_ptr<Coordination::Response> > > const&)+0x41) [0x6f4e5e1]\r\n5. /usr/bin/clickhouse-server(zkutil::ZooKeeper::multi(std::vector<std::shared_ptr<Coordination::Request>, std::allocator<std::shared_ptr<Coordination::Request> > > const&)+0x37) [0x6f4e657]\r\n6. /usr/bin/clickhouse-server(DB::DDLWorker::processTask(DB::DDLTask&, std::shared_ptr<zkutil::ZooKeeper> const&)+0x36e) [0x674680e]\r\n7. /usr/bin/clickhouse-server(DB::DDLWorker::processTasks()+0x475) [0x6747d75]\r\n8. /usr/bin/clickhouse-server(DB::DDLWorker::runMainThread()+0xf8) [0x67486b8]\r\n9. /usr/bin/clickhouse-server(ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::{lambda()#1}::operator()() const+0x49) [0x6748cd9]\r\n10. /usr/bin/clickhouse-server(ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>)+0x1af) [0x71c778f]\r\n11. /usr/bin/clickhouse-server() [0xb2ac5bf]\r\n12. /lib/x86_64-linux-gnu/libpthread.so.0(+0x76db) [0x7f091e9d16db]\r\n13. /lib/x86_64-linux-gnu/libc.so.6(clone+0x3f) [0x7f091e15888f]\r\n (version 19.11.3.11 (official build))\r\n2019.10.08 14:04:09.511305 [ 71 ] {9330b2b2-c41f-4b3f-8e29-540fb07b2c38} <Error> DDLWorker: ZooKeeper error: Code: 999, e.displayText() = Coordination::Exception: Transaction failed (No node): Op #1, path: /clickhouse/task_queue/ddl/query-0001193846/finished/clkh%2D01%2D03:9000, Stack trace:\r\n\r\n0. /usr/bin/clickhouse-server(StackTrace::StackTrace()+0x22) [0x781c272]\r\n1. /usr/bin/clickhouse-server() [0x6f3eda1]\r\n2. /usr/bin/clickhouse-server(Coordination::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int)+0x16e) [0x6f3f8be]\r\n3. /usr/bin/clickhouse-server(zkutil::KeeperMultiException::KeeperMultiException(int, std::vector<std::shared_ptr<Coordination::Request>, std::allocator<std::shared_ptr<Coordination::Request> > > const&, std::vector<std::shared_ptr<Coordination::Response>, std::allocator<std::shared_ptr<Coordination::Response> > > const&)+0x56) [0x6f4e0a6]\r\n4. /usr/bin/clickhouse-server(zkutil::KeeperMultiException::check(int, std::vector<std::shared_ptr<Coordination::Request>, std::allocator<std::shared_ptr<Coordination::Request> > > const&, std::vector<std::shared_ptr<Coordination::Response>, std::allocator<std::shared_ptr<Coordination::Response> > > const&)+0x41) [0x6f4e5e1]\r\n5. /usr/bin/clickhouse-server(zkutil::ZooKeeper::multi(std::vector<std::shared_ptr<Coordination::Request>, std::allocator<std::shared_ptr<Coordination::Request> > > const&)+0x37) [0x6f4e657]\r\n6. /usr/bin/clickhouse-server(DB::DDLWorker::processTask(DB::DDLTask&, std::shared_ptr<zkutil::ZooKeeper> const&)+0x36e) [0x674680e]\r\n7. /usr/bin/clickhouse-server(DB::DDLWorker::processTasks()+0x475) [0x6747d75]\r\n8. /usr/bin/clickhouse-server(DB::DDLWorker::runMainThread()+0xf8) [0x67486b8]\r\n9. /usr/bin/clickhouse-server(ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::{lambda()#1}::operator()() const+0x49) [0x6748cd9]\r\n10. /usr/bin/clickhouse-server(ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>)+0x1af) [0x71c778f]\r\n11. /usr/bin/clickhouse-server() [0xb2ac5bf]\r\n12. /lib/x86_64-linux-gnu/libpthread.so.0(+0x76db) [0x7f091e9d16db]\r\n13. /lib/x86_64-linux-gnu/libc.so.6(clone+0x3f) [0x7f091e15888f]\r\n (version 19.11.3.11 (official build))\r\n2019.10.08 14:04:09.511321 [ 71 ] {9330b2b2-c41f-4b3f-8e29-540fb07b2c38} <Debug> DDLWorker: Processing tasks\r\n2019.10.08 14:04:09.511336 [ 5 ] {} <Trace> DDLWorker: Too early to clean queue, will do it later.\r\n2019.10.08 14:04:09.511924 [ 71 ] {9330b2b2-c41f-4b3f-8e29-540fb07b2c38} <Information> DDLWorker: Trying to process task query-0001193846 again\r\n```\r\n\r\nDo you have any idea what the problem might be? Thanks\r\n\r\nThe request is tempted to be played indefinitely and no other OPTIMIZE request can be played.\r\n\nCannot execute replicated DDL query on leader\nI deleted all the zookeeper's for clickhouse path because its hard disk was full, after that I restarted the cluster.\r\nRight now the data can not be inserted because the path is not created\nFailed to execute ALTER TABLE ADD INDEX ON CLUSTER\nCREATE TABLE test ON CLUSTER default\r\n(\r\n    `timestamp` DateTime,\r\n    `field` UUID\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/db/test', '{replica}')\r\nPARTITION BY toYYYYMM(timestamp)\r\nORDER BY timestamp\r\nSETTINGS index_granularity = 8192\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 chi-clickhouse-db-default-1-0 \u2502 9000 \u2502      0 \u2502       \u2502                   3 \u2502                0 \u2502\r\n\u2502 chi-clickhouse-db-default-1-1 \u2502 9000 \u2502      0 \u2502       \u2502                   2 \u2502                0 \u2502\r\n\u2502 chi-clickhouse-db-default-0-0 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\r\n\u2502 chi-clickhouse-db-default-0-1 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n4 rows in set. Elapsed: 0.119 sec.\r\n\r\nALTER TABLE test ON CLUSTER default\r\n    ADD INDEX\r\n    field_idx field TYPE minmax GRANULARITY 3\r\n\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 chi-clickhouse-db-default-1-0 \u2502 9000 \u2502     48 \u2502 Cannot execute replicated DDL query on leader \u2502                   3 \u2502                0 \u2502\r\n\u2502 chi-clickhouse-db-default-1-1 \u2502 9000 \u2502     48 \u2502 Cannot execute replicated DDL query on leader \u2502                   2 \u2502                0 \u2502\r\n\u2502 chi-clickhouse-db-default-0-0 \u2502 9000 \u2502     48 \u2502 Cannot execute replicated DDL query on leader \u2502                   1 \u2502                0 \u2502\r\n\u2502 chi-clickhouse-db-default-0-1 \u2502 9000 \u2502     48 \u2502 Cannot execute replicated DDL query on leader \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u2193 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.)                                                                                                                                                                                    0%Received exception from server (version 19.11.8):\r\nCode: 48. DB::Exception: Received from localhost:9000. DB::Exception: There was an error on [chi-clickhouse-db-default-1-0:9000]: Cannot execute replicated DDL query on leader.\r\n\r\n4 rows in set. Elapsed: 10.502 sec.\n",
  "hints_text": "\n@alesapin I remember some rewrite around distributed DDL that requires query on leader. Is it related?\n\n",
  "created_at": "2020-08-07T09:29:18Z",
  "modified_files": [
    "src/Interpreters/DDLWorker.cpp"
  ],
  "modified_test_files": [
    "b/tests/integration/test_on_cluster_timeouts/configs/remote_servers.xml",
    "b/tests/integration/test_on_cluster_timeouts/configs/users_config.xml",
    "b/tests/integration/test_on_cluster_timeouts/test.py"
  ]
}