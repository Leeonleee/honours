{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 36976,
  "instance_id": "ClickHouse__ClickHouse-36976",
  "issue_numbers": [
    "33747"
  ],
  "base_commit": "a489ab682c9ef03ed0ccacb973ed872cb2239c48",
  "patch": "diff --git a/src/Databases/DatabaseReplicatedWorker.cpp b/src/Databases/DatabaseReplicatedWorker.cpp\nindex 43e8b6316a44..96b4e273ce73 100644\n--- a/src/Databases/DatabaseReplicatedWorker.cpp\n+++ b/src/Databases/DatabaseReplicatedWorker.cpp\n@@ -104,15 +104,17 @@ bool DatabaseReplicatedDDLWorker::waitForReplicaToProcessAllEntries(UInt64 timeo\n     auto max_log =  DDLTask::getLogEntryName(max_log_ptr);\n     LOG_TRACE(log, \"Waiting for worker thread to process all entries before {}, current task is {}\", max_log, current_task);\n \n-    std::unique_lock lock{mutex};\n-    bool processed = wait_current_task_change.wait_for(lock, std::chrono::milliseconds(timeout_ms), [&]()\n     {\n-        assert(zookeeper->expired() || current_task <= max_log);\n-        return zookeeper->expired() || current_task == max_log || stop_flag;\n-    });\n+        std::unique_lock lock{mutex};\n+        bool processed = wait_current_task_change.wait_for(lock, std::chrono::milliseconds(timeout_ms), [&]()\n+        {\n+            assert(zookeeper->expired() || current_task <= max_log);\n+            return zookeeper->expired() || current_task == max_log || stop_flag;\n+        });\n \n-    if (!processed)\n-        return false;\n+        if (!processed)\n+            return false;\n+    }\n \n     LOG_TRACE(log, \"Waiting for worker thread to process all entries before {}, current task is {}\", max_log, current_task);\n \ndiff --git a/src/Interpreters/DDLTask.cpp b/src/Interpreters/DDLTask.cpp\nindex 42cf9e92b327..71fcd7a18840 100644\n--- a/src/Interpreters/DDLTask.cpp\n+++ b/src/Interpreters/DDLTask.cpp\n@@ -383,7 +383,7 @@ ContextMutablePtr DatabaseReplicatedTask::makeQueryContext(ContextPtr from_conte\n         txn->addOp(zkutil::makeSetRequest(database->zookeeper_path + \"/max_log_ptr\", toString(getLogEntryNumber(entry_name)), -1));\n     }\n \n-    txn->addOp(zkutil::makeSetRequest(database->replica_path + \"/log_ptr\", toString(getLogEntryNumber(entry_name)), -1));\n+    txn->addOp(getOpToUpdateLogPointer());\n \n     for (auto & op : ops)\n         txn->addOp(std::move(op));\n@@ -392,6 +392,11 @@ ContextMutablePtr DatabaseReplicatedTask::makeQueryContext(ContextPtr from_conte\n     return query_context;\n }\n \n+Coordination::RequestPtr DatabaseReplicatedTask::getOpToUpdateLogPointer()\n+{\n+    return zkutil::makeSetRequest(database->replica_path + \"/log_ptr\", toString(getLogEntryNumber(entry_name)), -1);\n+}\n+\n String DDLTaskBase::getLogEntryName(UInt32 log_entry_number)\n {\n     return zkutil::getSequentialNodeName(\"query-\", log_entry_number);\ndiff --git a/src/Interpreters/DDLTask.h b/src/Interpreters/DDLTask.h\nindex ee49274707a8..d3728918a2d3 100644\n--- a/src/Interpreters/DDLTask.h\n+++ b/src/Interpreters/DDLTask.h\n@@ -107,6 +107,7 @@ struct DDLTaskBase\n     virtual String getShardID() const = 0;\n \n     virtual ContextMutablePtr makeQueryContext(ContextPtr from_context, const ZooKeeperPtr & zookeeper);\n+    virtual Coordination::RequestPtr getOpToUpdateLogPointer() { return nullptr; }\n \n     inline String getActiveNodePath() const { return fs::path(entry_path) / \"active\" / host_id_str; }\n     inline String getFinishedNodePath() const { return fs::path(entry_path) / \"finished\" / host_id_str; }\n@@ -145,6 +146,7 @@ struct DatabaseReplicatedTask : public DDLTaskBase\n     String getShardID() const override;\n     void parseQueryFromEntry(ContextPtr context) override;\n     ContextMutablePtr makeQueryContext(ContextPtr from_context, const ZooKeeperPtr & zookeeper) override;\n+    Coordination::RequestPtr getOpToUpdateLogPointer() override;\n \n     DatabaseReplicated * database;\n };\ndiff --git a/src/Interpreters/DDLWorker.cpp b/src/Interpreters/DDLWorker.cpp\nindex 406a4e904420..23a91b36b198 100644\n--- a/src/Interpreters/DDLWorker.cpp\n+++ b/src/Interpreters/DDLWorker.cpp\n@@ -715,6 +715,8 @@ bool DDLWorker::tryExecuteQueryOnLeaderReplica(\n     if (zookeeper->exists(is_executed_path, nullptr, event))\n     {\n         LOG_DEBUG(log, \"Task {} has already been executed by replica ({}) of the same shard.\", task.entry_name, zookeeper->get(is_executed_path));\n+        if (auto op = task.getOpToUpdateLogPointer())\n+            task.ops.push_back(op);\n         return true;\n     }\n \n@@ -759,6 +761,8 @@ bool DDLWorker::tryExecuteQueryOnLeaderReplica(\n             {\n                 LOG_DEBUG(log, \"Task {} has already been executed by replica ({}) of the same shard.\", task.entry_name, executed_by);\n                 executed_by_other_leader = true;\n+                if (auto op = task.getOpToUpdateLogPointer())\n+                    task.ops.push_back(op);\n                 break;\n             }\n \n@@ -786,6 +790,8 @@ bool DDLWorker::tryExecuteQueryOnLeaderReplica(\n         {\n             LOG_DEBUG(log, \"Task {} has already been executed by replica ({}) of the same shard.\", task.entry_name, zookeeper->get(is_executed_path));\n             executed_by_other_leader = true;\n+            if (auto op = task.getOpToUpdateLogPointer())\n+                task.ops.push_back(op);\n             break;\n         }\n         else\ndiff --git a/src/Parsers/ASTSystemQuery.cpp b/src/Parsers/ASTSystemQuery.cpp\nindex fd19e2166df3..8a7891edfe1a 100644\n--- a/src/Parsers/ASTSystemQuery.cpp\n+++ b/src/Parsers/ASTSystemQuery.cpp\n@@ -181,6 +181,10 @@ void ASTSystemQuery::formatImpl(const FormatSettings & settings, FormatState &,\n         else if (!disk.empty())\n             print_identifier(disk);\n     }\n+    else if (type == Type::SYNC_DATABASE_REPLICA)\n+    {\n+        print_identifier(database->as<ASTIdentifier>()->name());\n+    }\n     else if (type == Type::DROP_REPLICA)\n     {\n         print_drop_replica();\ndiff --git a/src/Parsers/ParserSystemQuery.cpp b/src/Parsers/ParserSystemQuery.cpp\nindex 5e4c3b96e462..599de6ec8284 100644\n--- a/src/Parsers/ParserSystemQuery.cpp\n+++ b/src/Parsers/ParserSystemQuery.cpp\n@@ -247,6 +247,7 @@ bool ParserSystemQuery::parseImpl(IParser::Pos & pos, ASTPtr & node, Expected &\n \n         case Type::SYNC_DATABASE_REPLICA:\n         {\n+            parseQueryWithOnCluster(res, pos, expected);\n             if (!parseDatabaseAsAST(pos, expected, res->database))\n                 return false;\n             break;\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 5702d2798041..600b829e7d21 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -5077,6 +5077,21 @@ void StorageReplicatedMergeTree::dropPart(const String & part_name, bool detach,\n     waitForLogEntryToBeProcessedIfNecessary(entry, query_context);\n }\n \n+void StorageReplicatedMergeTree::dropAllPartitionsImpl(const zkutil::ZooKeeperPtr & zookeeper, bool detach, ContextPtr query_context)\n+{\n+    Strings partitions = zookeeper->getChildren(fs::path(zookeeper_path) / \"block_numbers\");\n+\n+    std::vector<LogEntryPtr> entries;\n+    dropAllPartsInPartitions(*zookeeper, partitions, entries, query_context, detach);\n+\n+    for (const auto & entry : entries)\n+    {\n+        waitForLogEntryToBeProcessedIfNecessary(*entry, query_context);\n+        auto drop_range_info = MergeTreePartInfo::fromPartName(entry->new_part_name, format_version);\n+        cleanLastPartNode(drop_range_info.partition_id);\n+    }\n+}\n+\n void StorageReplicatedMergeTree::dropPartition(const ASTPtr & partition, bool detach, ContextPtr query_context)\n {\n     assertNotReadonly();\n@@ -5088,31 +5103,15 @@ void StorageReplicatedMergeTree::dropPartition(const ASTPtr & partition, bool de\n     const auto * partition_ast = partition->as<ASTPartition>();\n     if (partition_ast && partition_ast->all)\n     {\n-        Strings partitions = zookeeper->getChildren(fs::path(zookeeper_path) / \"block_numbers\");\n-\n-        std::vector<std::pair<String, std::unique_ptr<LogEntry>>> entries_with_partitionid_to_drop;\n-        entries_with_partitionid_to_drop.reserve(partitions.size());\n-        for (String & partition_id : partitions)\n-        {\n-            auto entry = std::make_unique<LogEntry>();\n-            if (dropAllPartsInPartition(*zookeeper, partition_id, *entry, query_context, detach))\n-                entries_with_partitionid_to_drop.emplace_back(partition_id, std::move(entry));\n-        }\n-\n-        for (const auto & entry : entries_with_partitionid_to_drop)\n-        {\n-            waitForLogEntryToBeProcessedIfNecessary(*entry.second, query_context);\n-            cleanLastPartNode(entry.first);\n-        }\n+        dropAllPartitionsImpl(zookeeper, detach, query_context);\n     }\n     else\n     {\n-        LogEntry entry;\n         String partition_id = getPartitionIDFromQuery(partition, query_context);\n-        bool did_drop = dropAllPartsInPartition(*zookeeper, partition_id, entry, query_context, detach);\n-        if (did_drop)\n+        auto entry = dropAllPartsInPartition(*zookeeper, partition_id, query_context, detach);\n+        if (entry)\n         {\n-            waitForLogEntryToBeProcessedIfNecessary(entry, query_context);\n+            waitForLogEntryToBeProcessedIfNecessary(*entry, query_context);\n             cleanLastPartNode(partition_id);\n         }\n     }\n@@ -5129,20 +5128,7 @@ void StorageReplicatedMergeTree::truncate(\n         throw Exception(\"TRUNCATE cannot be done on this replica because it is not a leader\", ErrorCodes::NOT_A_LEADER);\n \n     zkutil::ZooKeeperPtr zookeeper = getZooKeeperAndAssertNotReadonly();\n-\n-    Strings partitions = zookeeper->getChildren(fs::path(zookeeper_path) / \"block_numbers\");\n-\n-    std::vector<std::unique_ptr<LogEntry>> entries_to_wait;\n-    entries_to_wait.reserve(partitions.size());\n-    for (String & partition_id : partitions)\n-    {\n-        auto entry = std::make_unique<LogEntry>();\n-        if (dropAllPartsInPartition(*zookeeper, partition_id, *entry, query_context, false))\n-            entries_to_wait.push_back(std::move(entry));\n-    }\n-\n-    for (const auto & entry : entries_to_wait)\n-        waitForLogEntryToBeProcessedIfNecessary(*entry, query_context);\n+    dropAllPartitionsImpl(zookeeper, /* detach */ false, query_context);\n }\n \n \n@@ -7163,44 +7149,67 @@ bool StorageReplicatedMergeTree::dropPartImpl(\n     }\n }\n \n-bool StorageReplicatedMergeTree::dropAllPartsInPartition(\n-    zkutil::ZooKeeper & zookeeper, String & partition_id, LogEntry & entry, ContextPtr query_context, bool detach)\n+bool StorageReplicatedMergeTree::addOpsToDropAllPartsInPartition(\n+    zkutil::ZooKeeper & zookeeper, const String & partition_id, bool detach,\n+    Coordination::Requests & ops, std::vector<LogEntryPtr> & entries,\n+    std::vector<EphemeralLockInZooKeeper> & delimiting_block_locks,\n+    std::vector<size_t> & log_entry_ops_idx)\n {\n-    /// Retry if alter_partition_version changes\n-    for (size_t retry = 0; retry < 1000; ++retry)\n+    MergeTreePartInfo drop_range_info;\n+\n+    /// It would prevent other replicas from assigning merges which intersect locked block number.\n+    std::optional<EphemeralLockInZooKeeper> delimiting_block_lock;\n+\n+    if (!getFakePartCoveringAllPartsInPartition(partition_id, drop_range_info, delimiting_block_lock))\n     {\n-        String alter_partition_version_path = zookeeper_path + \"/alter_partition_version\";\n-        Coordination::Stat alter_partition_version_stat;\n-        zookeeper.get(alter_partition_version_path, &alter_partition_version_stat);\n+        LOG_INFO(log, \"Will not drop partition {}, it is empty.\", partition_id);\n+        return false;\n+    }\n \n-        MergeTreePartInfo drop_range_info;\n+    clearBlocksInPartition(zookeeper, partition_id, drop_range_info.min_block, drop_range_info.max_block);\n \n-        /// It would prevent other replicas from assigning merges which intersect locked block number.\n-        std::optional<EphemeralLockInZooKeeper> delimiting_block_lock;\n+    String drop_range_fake_part_name = getPartNamePossiblyFake(format_version, drop_range_info);\n \n-        if (!getFakePartCoveringAllPartsInPartition(partition_id, drop_range_info, delimiting_block_lock))\n-        {\n-            LOG_INFO(log, \"Will not drop partition {}, it is empty.\", partition_id);\n-            return false;\n-        }\n+    LOG_DEBUG(log, \"Disabled merges covered by range {}\", drop_range_fake_part_name);\n \n-        clearBlocksInPartition(zookeeper, partition_id, drop_range_info.min_block, drop_range_info.max_block);\n+    /// Finally, having achieved the necessary invariants, you can put an entry in the log.\n+    auto entry = std::make_shared<LogEntry>();\n+    entry->type = LogEntry::DROP_RANGE;\n+    entry->source_replica = replica_name;\n+    entry->new_part_name = drop_range_fake_part_name;\n+    entry->detach = detach;\n+    entry->create_time = time(nullptr);\n \n-        String drop_range_fake_part_name = getPartNamePossiblyFake(format_version, drop_range_info);\n+    log_entry_ops_idx.push_back(ops.size());\n+    ops.emplace_back(zkutil::makeCreateRequest(fs::path(zookeeper_path) / \"log/log-\", entry->toString(),\n+                                               zkutil::CreateMode::PersistentSequential));\n+    delimiting_block_lock->getUnlockOps(ops);\n+    delimiting_block_locks.push_back(std::move(*delimiting_block_lock));\n+    entries.push_back(std::move(entry));\n+    return true;\n+}\n \n-        LOG_DEBUG(log, \"Disabled merges covered by range {}\", drop_range_fake_part_name);\n+void StorageReplicatedMergeTree::dropAllPartsInPartitions(\n+    zkutil::ZooKeeper & zookeeper, const Strings partition_ids, std::vector<LogEntryPtr> & entries, ContextPtr query_context, bool detach)\n+{\n+    entries.reserve(partition_ids.size());\n \n-        /// Finally, having achieved the necessary invariants, you can put an entry in the log.\n-        entry.type = LogEntry::DROP_RANGE;\n-        entry.source_replica = replica_name;\n-        entry.new_part_name = drop_range_fake_part_name;\n-        entry.detach = detach;\n-        entry.create_time = time(nullptr);\n+    /// Retry if alter_partition_version changes\n+    for (size_t retry = 0; retry < 1000; ++retry)\n+    {\n+        entries.clear();\n+        String alter_partition_version_path = zookeeper_path + \"/alter_partition_version\";\n+        Coordination::Stat alter_partition_version_stat;\n+        zookeeper.get(alter_partition_version_path, &alter_partition_version_stat);\n \n         Coordination::Requests ops;\n-\n-        ops.emplace_back(zkutil::makeCreateRequest(fs::path(zookeeper_path) / \"log/log-\", entry.toString(),\n-            zkutil::CreateMode::PersistentSequential));\n+        std::vector<EphemeralLockInZooKeeper> delimiting_block_locks;\n+        std::vector<size_t> log_entry_ops_idx;\n+        ops.reserve(partition_ids.size() * 2);\n+        delimiting_block_locks.reserve(partition_ids.size());\n+        log_entry_ops_idx.reserve(partition_ids.size());\n+        for (const auto & partition_id : partition_ids)\n+            addOpsToDropAllPartsInPartition(zookeeper, partition_id, detach, ops, entries, delimiting_block_locks, log_entry_ops_idx);\n \n         /// Check and update version to avoid race with REPLACE_RANGE.\n         /// Otherwise new parts covered by drop_range_info may appear after execution of current DROP_RANGE entry\n@@ -7209,7 +7218,6 @@ bool StorageReplicatedMergeTree::dropAllPartsInPartition(\n \n         /// Just update version, because merges assignment relies on it\n         ops.emplace_back(zkutil::makeSetRequest(fs::path(zookeeper_path) / \"log\", \"\", -1));\n-        delimiting_block_lock->getUnlockOps(ops);\n \n         if (auto txn = query_context->getZooKeeperMetadataTransaction())\n             txn->moveOpsTo(ops);\n@@ -7218,7 +7226,10 @@ bool StorageReplicatedMergeTree::dropAllPartsInPartition(\n         Coordination::Error code = zookeeper.tryMulti(ops, responses);\n \n         if (code == Coordination::Error::ZOK)\n-            delimiting_block_lock->assumeUnlocked();\n+        {\n+            for (auto & lock : delimiting_block_locks)\n+                lock.assumeUnlocked();\n+        }\n         else if (code == Coordination::Error::ZBADVERSION)\n         {\n             /// Cannot retry automatically, because some zookeeper ops were lost on the first attempt. Will retry on DDLWorker-level.\n@@ -7231,15 +7242,31 @@ bool StorageReplicatedMergeTree::dropAllPartsInPartition(\n         else\n             zkutil::KeeperMultiException::check(code, ops, responses);\n \n-        String log_znode_path = dynamic_cast<const Coordination::CreateResponse &>(*responses.front()).path_created;\n-        entry.znode_name = log_znode_path.substr(log_znode_path.find_last_of('/') + 1);\n+        assert(entries.size() == log_entry_ops_idx.size());\n+        for (size_t i = 0; i < entries.size(); ++i)\n+        {\n+            String log_znode_path = dynamic_cast<const Coordination::CreateResponse &>(*responses[log_entry_ops_idx[i]]).path_created;\n+            entries[i]->znode_name = log_znode_path.substr(log_znode_path.find_last_of('/') + 1);\n \n-        getContext()->getMergeList().cancelInPartition(getStorageID(), partition_id, drop_range_info.max_block);\n+            auto drop_range_info = MergeTreePartInfo::fromPartName(entries[i]->new_part_name, format_version);\n+            getContext()->getMergeList().cancelInPartition(getStorageID(), drop_range_info.partition_id, drop_range_info.max_block);\n+        }\n \n-        return true;\n+        return;\n     }\n     throw Exception(ErrorCodes::CANNOT_ASSIGN_ALTER,\n-        \"Cannot assign ALTER PARTITION because another ALTER PARTITION query was concurrently executed\");\n+                    \"Cannot assign ALTER PARTITION because another ALTER PARTITION query was concurrently executed\");\n+}\n+\n+StorageReplicatedMergeTree::LogEntryPtr StorageReplicatedMergeTree::dropAllPartsInPartition(\n+    zkutil::ZooKeeper & zookeeper, const String & partition_id, ContextPtr query_context, bool detach)\n+{\n+    Strings partition_ids = {partition_id};\n+    std::vector<LogEntryPtr> entries;\n+    dropAllPartsInPartitions(zookeeper, partition_ids, entries, query_context, detach);\n+    if (entries.empty())\n+        return {};\n+    return entries[0];\n }\n \n void StorageReplicatedMergeTree::enqueuePartForCheck(const String & part_name, time_t delay_to_check_seconds)\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 54c2e928b220..ce240b4e59e7 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -723,8 +723,19 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     /// Info about how other replicas can access this one.\n     ReplicatedMergeTreeAddress getReplicatedMergeTreeAddress() const;\n \n-    bool dropAllPartsInPartition(\n-        zkutil::ZooKeeper & zookeeper, String & partition_id, LogEntry & entry, ContextPtr query_context, bool detach);\n+    bool addOpsToDropAllPartsInPartition(\n+        zkutil::ZooKeeper & zookeeper, const String & partition_id, bool detach,\n+        Coordination::Requests & ops, std::vector<LogEntryPtr> & entries,\n+        std::vector<EphemeralLockInZooKeeper> & delimiting_block_locks,\n+        std::vector<size_t> & log_entry_ops_idx);\n+    void dropAllPartsInPartitions(\n+        zkutil::ZooKeeper & zookeeper, const Strings partition_ids, std::vector<LogEntryPtr> & entries, ContextPtr query_context, bool detach);\n+\n+    LogEntryPtr dropAllPartsInPartition(\n+        zkutil::ZooKeeper & zookeeper, const String & partition_id, ContextPtr query_context, bool detach);\n+\n+\n+    void dropAllPartitionsImpl(const zkutil::ZooKeeperPtr & zookeeper, bool detach, ContextPtr query_context);\n \n     void dropPartNoWaitNoThrow(const String & part_name) override;\n     void dropPart(const String & part_name, bool detach, ContextPtr query_context) override;\n",
  "test_patch": "diff --git a/tests/integration/test_replicated_database/test.py b/tests/integration/test_replicated_database/test.py\nindex 02301236389e..92c109974e14 100644\n--- a/tests/integration/test_replicated_database/test.py\n+++ b/tests/integration/test_replicated_database/test.py\n@@ -825,6 +825,9 @@ def test_sync_replica(started_cluster):\n                 settings=settings,\n             )\n \n+    # wait for host to reconnect\n+    dummy_node.query_with_retry(\"SELECT * FROM system.zookeeper WHERE path='/'\")\n+\n     dummy_node.query(\"SYSTEM SYNC DATABASE REPLICA test_sync_database\")\n \n     assert dummy_node.query(\n@@ -834,3 +837,30 @@ def test_sync_replica(started_cluster):\n     assert main_node.query(\n         \"SELECT count() FROM system.tables where database='test_sync_database'\"\n     ).strip() == str(number_of_tables)\n+\n+    engine_settings = {\"default_table_engine\": \"ReplicatedMergeTree\"}\n+    dummy_node.query(\n+        \"CREATE TABLE test_sync_database.table (n int, primary key n) partition by n\",\n+        settings=engine_settings,\n+    )\n+    main_node.query(\"INSERT INTO test_sync_database.table SELECT * FROM numbers(10)\")\n+    dummy_node.query(\"TRUNCATE TABLE test_sync_database.table\", settings=settings)\n+    dummy_node.query(\n+        \"ALTER TABLE test_sync_database.table ADD COLUMN m int\", settings=settings\n+    )\n+\n+    main_node.query(\n+        \"SYSTEM SYNC DATABASE REPLICA ON CLUSTER test_sync_database test_sync_database\"\n+    )\n+\n+    lp1 = main_node.query(\n+        \"select value from system.zookeeper where path='/clickhouse/databases/test1/replicas/shard1|replica1' and name='log_ptr'\"\n+    )\n+    lp2 = main_node.query(\n+        \"select value from system.zookeeper where path='/clickhouse/databases/test1/replicas/shard1|replica2' and name='log_ptr'\"\n+    )\n+    max_lp = main_node.query(\n+        \"select value from system.zookeeper where path='/clickhouse/databases/test1/' and name='max_log_ptr'\"\n+    )\n+    assert lp1 == max_lp\n+    assert lp2 == max_lp\ndiff --git a/tests/queries/0_stateless/01166_truncate_multiple_partitions.reference b/tests/queries/0_stateless/01166_truncate_multiple_partitions.reference\nnew file mode 100644\nindex 000000000000..27f26b32e1e3\n--- /dev/null\n+++ b/tests/queries/0_stateless/01166_truncate_multiple_partitions.reference\n@@ -0,0 +1,8 @@\n+20\t190\n+0\t0\n+8\t52\n+0\t0\n+20\t190\n+0\t0\n+8\t52\n+0\t0\ndiff --git a/tests/queries/0_stateless/01166_truncate_multiple_partitions.sql b/tests/queries/0_stateless/01166_truncate_multiple_partitions.sql\nnew file mode 100644\nindex 000000000000..1a7e3ed3bc46\n--- /dev/null\n+++ b/tests/queries/0_stateless/01166_truncate_multiple_partitions.sql\n@@ -0,0 +1,31 @@\n+drop table if exists trunc;\n+\n+set default_table_engine='ReplicatedMergeTree';\n+create table trunc (n int, primary key n) engine=ReplicatedMergeTree('/test/1166/{database}', '1') partition by n % 10;\n+insert into trunc select * from numbers(20);\n+select count(), sum(n) from trunc;\n+alter table trunc detach partition all;\n+select count(), sum(n) from trunc;\n+alter table trunc attach partition id '0';\n+alter table trunc attach partition id '1';\n+alter table trunc attach partition id '2';\n+alter table trunc attach partition id '3';\n+select count(), sum(n) from trunc;\n+truncate trunc;\n+select count(), sum(n) from trunc;\n+drop table trunc;\n+\n+set default_table_engine='MergeTree';\n+create table trunc (n int, primary key n) partition by n % 10;\n+insert into trunc select * from numbers(20);\n+select count(), sum(n) from trunc;\n+alter table trunc detach partition all;\n+select count(), sum(n) from trunc;\n+alter table trunc attach partition id '0';\n+alter table trunc attach partition id '1';\n+alter table trunc attach partition id '2';\n+alter table trunc attach partition id '3';\n+select count(), sum(n) from trunc;\n+truncate trunc;\n+select count(), sum(n) from trunc;\n+drop table trunc;\n\\ No newline at end of file\ndiff --git a/tests/queries/0_stateless/01293_show_clusters.sql b/tests/queries/0_stateless/01293_show_clusters.sql\nindex ad5e51531e3e..e1ef8621a164 100644\n--- a/tests/queries/0_stateless/01293_show_clusters.sql\n+++ b/tests/queries/0_stateless/01293_show_clusters.sql\n@@ -1,3 +1,4 @@\n -- don't show all clusters to reduce dependency on the configuration of server\n+set send_logs_level = 'fatal';\n show clusters like 'test_shard%' limit 1;\n show cluster 'test_shard_localhost';\n",
  "problem_statement": "TRUNCATE TABLE in Replicated DB if multiple partitions exist raises exception: Cannot add ZooKeeper operation because query is executed\n### Version\r\n21.12.3.32\r\n\r\n### Details\r\nCreate Replicated DB\r\n```\r\nCREATE DATABASE IF NOT EXISTS test ON CLUSTER 'test_shard_localhost'\r\nENGINE = Replicated('/clickhouse/{cluster}/db/test', '{shard}', '{replica}') ;\r\n```\r\n\r\nCreate Table in Replicated DB\r\n```\r\nCREATE TABLE IF NOT EXISTS test.events\r\n(\r\n    event_day Date,\r\n    event_id String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{uuid}/{shard}', '{replica}')\r\nPARTITION BY (toYYYYMM(event_day))\r\nORDER BY (event_day);\r\n```\r\n\r\nTry to insert data and do truncate.  \r\nIf adding data that fits into one partition => truncate works OK:\r\n```\r\nINSERT INTO test.events(event_day, event_id)\r\nVALUES \r\n\t(toDate('2021-01-01'), 'e1');\r\n\r\nTRUNCATE TABLE test.events;\r\n```\r\n\r\nIf adding data that fits multiple partitions => truncate fails, only first partition removed:\r\n[clickhouse-server.err.log](https://github.com/ClickHouse/ClickHouse/files/7891425/clickhouse-server.err.log)\r\n\r\n```\r\nINSERT INTO test.events(event_day, event_id)\r\nVALUES \r\n\t(toDate('2021-01-01'), 'e1'),\r\n\t(toDate('2022-01-01'), 'e2');\r\n\r\nTRUNCATE TABLE test.events;\r\n => exception\r\n DB::Exception: Cannot add ZooKeeper operation because query is executed. It's a bug. (LOGICAL_ERROR)\r\n\r\nSELECT * from system.parts \r\nWHERE database = 'test' and table ='events';  \r\n=> 1 part left still, if trying additional TRUNCATE statements - those would fail also\r\n\n",
  "hints_text": "CC @tavplubix ",
  "created_at": "2022-05-06T16:39:30Z",
  "modified_files": [
    "src/Databases/DatabaseReplicatedWorker.cpp",
    "src/Interpreters/DDLTask.cpp",
    "src/Interpreters/DDLTask.h",
    "src/Interpreters/DDLWorker.cpp",
    "src/Parsers/ASTSystemQuery.cpp",
    "src/Parsers/ParserSystemQuery.cpp",
    "src/Storages/StorageReplicatedMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "tests/integration/test_replicated_database/test.py",
    "b/tests/queries/0_stateless/01166_truncate_multiple_partitions.reference",
    "b/tests/queries/0_stateless/01166_truncate_multiple_partitions.sql",
    "tests/queries/0_stateless/01293_show_clusters.sql"
  ]
}