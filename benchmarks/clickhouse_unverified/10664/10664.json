{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 10664,
  "instance_id": "ClickHouse__ClickHouse-10664",
  "issue_numbers": [
    "10655"
  ],
  "base_commit": "089354e3909cf590c6df86f4e0cfb91747da6139",
  "patch": "diff --git a/src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.cpp b/src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.cpp\nindex 79dec00ba0da..6496c32738ef 100644\n--- a/src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.cpp\n+++ b/src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.cpp\n@@ -31,6 +31,8 @@ GroupingAggregatedTransform::GroupingAggregatedTransform(\n void GroupingAggregatedTransform::readFromAllInputs()\n {\n     auto in = inputs.begin();\n+    read_from_all_inputs = true;\n+\n     for (size_t i = 0; i < num_inputs; ++i, ++in)\n     {\n         if (in->isFinished())\n@@ -42,14 +44,15 @@ void GroupingAggregatedTransform::readFromAllInputs()\n         in->setNeeded();\n \n         if (!in->hasData())\n-            return;\n+        {\n+            read_from_all_inputs = false;\n+            continue;\n+        }\n \n         auto chunk = in->pull();\n         read_from_input[i] = true;\n         addChunk(std::move(chunk), i);\n     }\n-\n-    read_from_all_inputs = true;\n }\n \n void GroupingAggregatedTransform::pushData(Chunks chunks, Int32 bucket, bool is_overflows)\n@@ -273,6 +276,7 @@ void GroupingAggregatedTransform::addChunk(Chunk chunk, size_t input)\n \n void GroupingAggregatedTransform::work()\n {\n+    /// Convert single level data to two level.\n     if (!single_level_chunks.empty())\n     {\n         const auto & header = getInputs().front().getHeader();  /// Take header from input port. Output header is empty.\ndiff --git a/src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.h b/src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.h\nindex 0c5986c2156c..bf23b67e2040 100644\n--- a/src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.h\n+++ b/src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.h\n@@ -28,11 +28,11 @@ class GroupingAggregatedTransform : public IProcessor\n     size_t num_inputs;\n     AggregatingTransformParamsPtr params;\n \n-    std::vector<Int32> last_bucket_number;\n-    std::map<Int32, Chunks> chunks_map;\n+    std::vector<Int32> last_bucket_number; /// Last bucket read from each input.\n+    std::map<Int32, Chunks> chunks_map; /// bucket -> chunks\n     Chunks overflow_chunks;\n     Chunks single_level_chunks;\n-    Int32 current_bucket = 0;\n+    Int32 current_bucket = 0; /// Currently processing bucket.\n     Int32 next_bucket_to_push = 0; /// Always <= current_bucket.\n     bool has_two_level = false;\n \n@@ -42,11 +42,17 @@ class GroupingAggregatedTransform : public IProcessor\n \n     bool expect_several_chunks_for_single_bucket_per_source = false;\n \n+    /// Add chunk read from input to chunks_map, overflow_chunks or single_level_chunks according to it's chunk info.\n     void addChunk(Chunk chunk, size_t input);\n+    /// Read from all inputs first chunk. It is needed to detect if any source has two-level aggregation.\n     void readFromAllInputs();\n+    /// Push chunks if all inputs has single level.\n     bool tryPushSingleLevelData();\n+    /// Push chunks from ready bucket if has one.\n     bool tryPushTwoLevelData();\n+    /// Push overflow chunks if has any.\n     bool tryPushOverflowData();\n+    /// Push chunks from bucket to output port.\n     void pushData(Chunks chunks, Int32 bucket, bool is_overflows);\n };\n \n",
  "test_patch": "diff --git a/tests/performance/distributed_aggregation_memory_efficient.xml b/tests/performance/distributed_aggregation_memory_efficient.xml\nnew file mode 100644\nindex 000000000000..c4b4159e76b5\n--- /dev/null\n+++ b/tests/performance/distributed_aggregation_memory_efficient.xml\n@@ -0,0 +1,3 @@\n+<test>\n+    <query>select sum(number) from remote('127.0.0{2,3,4,5}', numbers_mt(1000000000)) settings max_threads=4, distributed_aggregation_memory_efficient=1</query>\n+</test>\n",
  "problem_statement": "20.3 Consecutive execution of distributed queries\nWith experimental_use_processors=1 and distributed_aggregation_memory_efficient=1 distributed queries are sent to shards consecutively, not at the same time.\r\nCH version is 20.3.8\r\n```\r\nselect hostName() as node, type, event_time, query_duration_ms \r\nfrom cluster(all_nodes, system.query_log) \r\nwhere event_date=today() and initial_query_id='e0237894-d864-4873-8fc7-c8108555e1e0'\r\n\r\nnode   | type        | event_time          | query_duration_ms\r\n-------+-------------+---------------------+------------------\r\nnode03 | QueryStart  | 2020-05-01 17:50:11 |                 0\r\nnode04 | QueryStart  | 2020-05-01 17:50:13 |                 0\r\nnode04 | QueryFinish | 2020-05-01 17:50:14 |              1416\r\nnode05 | QueryStart  | 2020-05-01 17:50:14 |                 0\r\nnode05 | QueryFinish | 2020-05-01 17:50:16 |              1497\r\nnode06 | QueryStart  | 2020-05-01 17:50:16 |                 0\r\nnode06 | QueryFinish | 2020-05-01 17:50:17 |              1464\r\nnode07 | QueryStart  | 2020-05-01 17:50:17 |                 0\r\nnode07 | QueryFinish | 2020-05-01 17:50:19 |              1451\r\nnode08 | QueryStart  | 2020-05-01 17:50:19 |                 0\r\nnode08 | QueryFinish | 2020-05-01 17:50:20 |              1466\r\nnode03 | QueryFinish | 2020-05-01 17:50:20 |              8635\r\n```\n",
  "hints_text": "",
  "created_at": "2020-05-04T18:28:01Z",
  "modified_files": [
    "src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.cpp",
    "src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.h"
  ],
  "modified_test_files": [
    "b/tests/performance/distributed_aggregation_memory_efficient.xml"
  ]
}