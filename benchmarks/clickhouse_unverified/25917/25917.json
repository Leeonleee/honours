{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 25917,
  "instance_id": "ClickHouse__ClickHouse-25917",
  "issue_numbers": [
    "17317"
  ],
  "base_commit": "ee1b3696a251b3bf24086daedf7decad5391dc8e",
  "patch": "diff --git a/src/Storages/MergeTree/MergeTreeReadPool.cpp b/src/Storages/MergeTree/MergeTreeReadPool.cpp\nindex d9a250e3f7ac..5f51cfc93247 100644\n--- a/src/Storages/MergeTree/MergeTreeReadPool.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReadPool.cpp\n@@ -231,6 +231,19 @@ std::vector<size_t> MergeTreeReadPool::fillPerPartInfo(\n         auto [required_columns, required_pre_columns, should_reorder] =\n             getReadTaskColumns(data, metadata_snapshot, part.data_part, column_names, prewhere_info, check_columns);\n \n+        if (predict_block_size_bytes)\n+        {\n+            const auto & required_column_names = required_columns.getNames();\n+            const auto & required_pre_column_names = required_pre_columns.getNames();\n+            NameSet complete_column_names(required_column_names.begin(), required_column_names.end());\n+            complete_column_names.insert(required_pre_column_names.begin(), required_pre_column_names.end());\n+\n+            per_part_size_predictor.emplace_back(std::make_unique<MergeTreeBlockSizePredictor>(\n+                part.data_part, Names(complete_column_names.begin(), complete_column_names.end()), sample_block));\n+        }\n+        else\n+            per_part_size_predictor.emplace_back(nullptr);\n+\n         /// will be used to distinguish between PREWHERE and WHERE columns when applying filter\n         const auto & required_column_names = required_columns.getNames();\n         per_part_column_name_set.emplace_back(required_column_names.begin(), required_column_names.end());\n@@ -240,14 +253,6 @@ std::vector<size_t> MergeTreeReadPool::fillPerPartInfo(\n         per_part_should_reorder.push_back(should_reorder);\n \n         parts_with_idx.push_back({ part.data_part, part.part_index_in_query });\n-\n-        if (predict_block_size_bytes)\n-        {\n-            per_part_size_predictor.emplace_back(std::make_unique<MergeTreeBlockSizePredictor>(\n-                part.data_part, column_names, sample_block));\n-        }\n-        else\n-            per_part_size_predictor.emplace_back(nullptr);\n     }\n \n     return per_part_sum_marks;\ndiff --git a/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp\nindex e9527efaa4a2..980237519882 100644\n--- a/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp\n@@ -93,9 +93,17 @@ try\n     MarkRanges mark_ranges_for_task = { all_mark_ranges.back() };\n     all_mark_ranges.pop_back();\n \n-    auto size_predictor = (preferred_block_size_bytes == 0)\n-        ? nullptr\n-        : std::make_unique<MergeTreeBlockSizePredictor>(data_part, ordered_names, metadata_snapshot->getSampleBlock());\n+    std::unique_ptr<MergeTreeBlockSizePredictor> size_predictor;\n+    if (preferred_block_size_bytes)\n+    {\n+        const auto & required_column_names = task_columns.columns.getNames();\n+        const auto & required_pre_column_names = task_columns.pre_columns.getNames();\n+        NameSet complete_column_names(required_column_names.begin(), required_column_names.end());\n+        complete_column_names.insert(required_pre_column_names.begin(), required_pre_column_names.end());\n+\n+        size_predictor = std::make_unique<MergeTreeBlockSizePredictor>(\n+            data_part, Names(complete_column_names.begin(), complete_column_names.end()), metadata_snapshot->getSampleBlock());\n+    }\n \n     task = std::make_unique<MergeTreeReadTask>(\n         data_part, mark_ranges_for_task, part_index_in_query, ordered_names, column_name_set,\ndiff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\nindex 980afa170e93..47ed2ae933fa 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n@@ -71,9 +71,17 @@ try\n         storage, metadata_snapshot, data_part,\n         required_columns, prewhere_info, check_columns);\n \n-    auto size_predictor = (preferred_block_size_bytes == 0)\n-        ? nullptr\n-        : std::make_unique<MergeTreeBlockSizePredictor>(data_part, ordered_names, metadata_snapshot->getSampleBlock());\n+    std::unique_ptr<MergeTreeBlockSizePredictor> size_predictor;\n+    if (preferred_block_size_bytes)\n+    {\n+        const auto & required_column_names = task_columns.columns.getNames();\n+        const auto & required_pre_column_names = task_columns.pre_columns.getNames();\n+        NameSet complete_column_names(required_column_names.begin(), required_column_names.end());\n+        complete_column_names.insert(required_pre_column_names.begin(), required_pre_column_names.end());\n+\n+        size_predictor = std::make_unique<MergeTreeBlockSizePredictor>(\n+            data_part, Names(complete_column_names.begin(), complete_column_names.end()), metadata_snapshot->getSampleBlock());\n+    }\n \n     /// will be used to distinguish between PREWHERE and WHERE columns when applying filter\n     const auto & column_names = task_columns.columns.getNames();\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01903_correct_block_size_prediction_with_default.reference b/tests/queries/0_stateless/01903_correct_block_size_prediction_with_default.reference\nnew file mode 100644\nindex 000000000000..b70a1cb7c752\n--- /dev/null\n+++ b/tests/queries/0_stateless/01903_correct_block_size_prediction_with_default.reference\n@@ -0,0 +1,3 @@\n+8\n+4\n+4\ndiff --git a/tests/queries/0_stateless/01903_correct_block_size_prediction_with_default.sql b/tests/queries/0_stateless/01903_correct_block_size_prediction_with_default.sql\nnew file mode 100644\nindex 000000000000..7aa1b0112a65\n--- /dev/null\n+++ b/tests/queries/0_stateless/01903_correct_block_size_prediction_with_default.sql\n@@ -0,0 +1,11 @@\n+CREATE TABLE test_extract(str String,  arr Array(Array(String)) ALIAS extractAllGroupsHorizontal(str, '\\\\W(\\\\w+)=(\"[^\"]*?\"|[^\",}]*)')) ENGINE=MergeTree() PARTITION BY tuple() ORDER BY tuple();\n+\n+INSERT INTO test_extract (str) WITH range(8) as range_arr, arrayMap(x-> concat(toString(x),'Id'), range_arr) as key, arrayMap(x -> rand() % 8, range_arr) as val, arrayStringConcat(arrayMap((x,y) -> concat(x,'=',toString(y)), key, val),',') as str SELECT str FROM numbers(500000);\n+\n+ALTER TABLE test_extract ADD COLUMN `15Id` Nullable(UInt16) DEFAULT toUInt16OrNull(arrayFirst((v, k) -> (k = '4Id'), arr[2], arr[1]));\n+\n+SELECT uniq(15Id) FROM test_extract SETTINGS max_threads=1, max_memory_usage=100000000;\n+\n+SELECT uniq(15Id) FROM test_extract PREWHERE 15Id < 4 SETTINGS max_threads=1, max_memory_usage=100000000;\n+\n+SELECT uniq(15Id) FROM test_extract WHERE 15Id < 4 SETTINGS max_threads=1, max_memory_usage=100000000;\n",
  "problem_statement": "On the fly DEFAULT values calculation uses 8 times more memory than expression itself.\n**Describe the bug**\r\nIf we add a new column to table via ALTER TABLE ADD COLUMN command and use in DEFAULT section some expression with array functions, it requires much more memory than using that expression by itself in query.\r\n\r\n**How to reproduce**\r\nClickhouse version 20.12, 20.8\r\n```\r\nCREATE TABLE test_extract(str String,  arr Array(Array(String)) ALIAS extractAllGroupsHorizontal(str, '\\\\W(\\\\w+)=(\"[^\"]*?\"|[^\",}]*)')) ENGINE=MergeTree() PARTITION BY tuple() ORDER BY tuple() ;\r\nINSERT INTO test_extract WITH range(30) as range_arr, arrayMap(x-> concat(toString(x),'Id'), range_arr) as key, arrayMap(x -> rand() % 30, range_arr) as val, arrayStringConcat(arrayMap((x,y) -> concat(x,'=',toString(y)), key, val),',') as str SELECT str FROM numbers(2000000);\r\nALTER TABLE test_extract  add column `15Id` Nullable(UInt16) DEFAULT toUInt16OrNull(arrayFirst((v, k) -> (k = '15Id'),arr[2],arr[1]));\r\nSELECT DISTINCT 15Id FROM test_extract;\r\n2020.11.23 18:34:24.541648 [ 10823 ] {2b0a29fb-885e-4e3b-8acf-7379b3996c63} <Information> executeQuery: Read 2000000 rows, 440.62 MiB in 8.5199527 sec., 234743 rows/sec., 51.72 MiB/sec.\r\n2020.11.23 18:34:24.541787 [ 10823 ] {2b0a29fb-885e-4e3b-8acf-7379b3996c63} <Debug> MemoryTracker: Peak memory usage (for query): 906.80 MiB.\r\n\r\n30 rows in set. Elapsed: 8.521 sec. Processed 2.00 million rows, 462.03 MB (234.71 thousand rows/s., 54.22 MB/s.)\r\n\r\nSELECT DISTINCT toUInt16OrNull(arrayFirst((v, k) -> (k = '15Id'),arr[2],arr[1])) FROM test_extract;\r\n\r\n 2020.11.23 18:34:56.722182 [ 10823 ] {f9520e0f-1982-46f5-9123-cb6abbb732b2} <Information> executeQuery: Read 2000000 rows, 434.90 MiB in 6.9211621 sec., 288968 rows/sec., 62.84 MiB/sec.\r\n2020.11.23 18:34:56.722235 [ 10823 ] {f9520e0f-1982-46f5-9123-cb6abbb732b2} <Debug> MemoryTracker: Peak memory usage (for query): 115.43 MiB.\r\n\r\n30 rows in set. Elapsed: 6.923 sec. Processed 2.00 million rows, 456.03 MB (288.87 thousand rows/s., 65.87 MB/s.)\r\n```\r\n\r\n\r\n**Expected behavior**\r\nBoth queries would use similar amounts of RAM.\r\n\n",
  "hints_text": "Maybe different block sizes (or where vs prewhere) + https://github.com/ClickHouse/ClickHouse/issues/5105#issuecomment-486454768 ? \nQuery duration is almost the same.\r\n\r\nAnd if we set both settings:\r\n```\r\nset max_threads=1;\r\n\r\n\r\nSELECT DISTINCT toUInt16OrNull(arrayFirst((v, k) -> (k = '15Id'),arr[2],arr[1])) FROM test_extract;\r\n2020.11.24 12:20:35.999961 [ 13607 ] {df5a2bad-cc9a-495d-ac24-78dfaa461096} <Information> executeQuery: Read 2000000 rows, 434.90 MiB in 23.4636177 sec., 85238 rows/sec., 18.54 MiB/sec.\r\n2020.11.24 12:20:36.000007 [ 13607 ] {df5a2bad-cc9a-495d-ac24-78dfaa461096} <Debug> MemoryTracker: Peak memory usage (for query): 30.28 MiB.\r\n\r\n30 rows in set. Elapsed: 23.465 sec. Processed 2.00 million rows, 456.03 MB (85.23 thousand rows/s., 19.43 MB/s.)\r\n\r\nSELECT DISTINCT 15Id FROM test_extract;\r\n2020.11.24 12:21:04.311403 [ 13607 ] {0eab4d07-9767-4d29-8f42-54551a7a27b6} <Information> executeQuery: Read 2000000 rows, 440.62 MiB in 23.1529704 sec., 86382 rows/sec., 19.03 MiB/sec.\r\n2020.11.24 12:21:04.311461 [ 13607 ] {0eab4d07-9767-4d29-8f42-54551a7a27b6} <Debug> MemoryTracker: Peak memory usage (for query): 239.10 MiB.\r\n\r\n30 rows in set. Elapsed: 23.154 sec. Processed 2.00 million rows, 462.03 MB (86.38 thousand rows/s., 19.95 MB/s.)\r\n\r\n\r\nset max_block_size='32k', max_threads=1;\r\n\r\nSELECT DISTINCT toUInt16OrNull(arrayFirst((v, k) -> (k = '15Id'),arr[2],arr[1])) FROM test_extract;\r\n2020.11.24 12:15:54.072714 [ 10825 ] {23305380-6d77-4cd0-b4fe-b9c06d585b89} <Information> executeQuery: Read 2000000 rows, 434.90 MiB in 24.0814239 sec., 83051 rows/sec., 18.06 MiB/sec.\r\n2020.11.24 12:15:54.072815 [ 10825 ] {23305380-6d77-4cd0-b4fe-b9c06d585b89} <Debug> MemoryTracker: Peak memory usage (for query): 30.28 MiB.\r\n\r\n30 rows in set. Elapsed: 24.086 sec. Processed 2.00 million rows, 456.03 MB (83.04 thousand rows/s., 18.93 MB/s.)\r\n\r\n\r\nSELECT DISTINCT 15Id FROM test_extract;\r\n2020.11.24 12:17:40.740635 [ 10825 ] {2df33e67-2053-4986-af3c-313236839338} <Information> executeQuery: Read 2000000 rows, 440.62 MiB in 22.9129938 sec., 87286 rows/sec., 19.23 MiB/sec.\r\n2020.11.24 12:17:40.740686 [ 10825 ] {2df33e67-2053-4986-af3c-313236839338} <Debug> MemoryTracker: Peak memory usage (for query): 120.60 MiB.\r\n\r\n30 rows in set. Elapsed: 22.914 sec. Processed 2.00 million rows, 462.03 MB (87.28 thousand rows/s., 20.16 MB/s.)\r\n\r\n```\r\n\r\nSo it looks like that changing `max_threads` affects both queries in the same matter, but `max_block_size` mostly affects query with DEFAULT value.\nMaybe also related to greedy execution of alias columns. \r\nhttps://github.com/ClickHouse/ClickHouse/issues/2582\n21.2.6\r\n\r\n```\r\nSELECT DISTINCT 15Id FROM test_extract;\r\n\r\nReceived exception from server (version 21.2.6):\r\nCode: 128. DB::Exception: Received from localhost:9000. DB::Exception: Too large array size in the result of function extractAllGroupsHorizontal: while executing 'FUNCTION extractAllGroupsHorizontal(str :: 0, '\\\\W(\\\\w+)=(\"[^\"]*?\"|[^\",}]*)' :: 1) -> extractAllGroupsHorizontal(str, '\\\\W(\\\\w+)=(\"[^\"]*?\"|[^\",}]*)') Array(Array(String)) : 7': (while reading from part /var/lib/clickhouse/store/abe/abe0e7db-cd4f-4d91-9549-b7d0bdeac420/all_2_2_0/): While executing MergeTreeThread.\r\n```\n21.4.1\r\n```\r\nSELECT\r\n    str,\r\n    `15Id`\r\nFROM test_extract\r\nFORMAT Null\r\n\r\nQuery id: 9704556a-f5eb-4247-895c-9be653b8f015\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 2.794 sec. Processed 2.00 million rows, 462.01 MB (715.81 thousand rows/s., 165.36 MB/s.)\r\n\r\nSELECT `15Id`\r\nFROM test_extract\r\nFORMAT Null\r\n\r\nQuery id: 05868b8c-fafb-4925-a400-496c826ff923\r\n\r\n\r\n0 rows in set. Elapsed: 0.158 sec.\r\n\r\nReceived exception from server (version 21.4.1):\r\nCode: 128. DB::Exception: Received from localhost:9000. DB::Exception: Too large array size in the result of function extractAllGroupsHorizontal: while executing 'FUNCTION extractAllGroupsHorizontal(str :: 0, '\\\\W(\\\\w+)=(\"[^\"]*?\"|[^\",}]*)' :: 1) -> extractAllGroupsHorizontal(str, '\\\\W(\\\\w+)=(\"[^\"]*?\"|[^\",}]*)') Array(Array(String)) : 8': (while reading from part /var/lib/clickhouse/data/default/test_extract/all_2_2_0/): While executing MergeTreeThread.\r\n\r\n```\nStill relevant in version 21.6 testing.\n> Maybe also related to greedy execution of alias columns. #2582\r\n\r\nNo, it's not the reason.\n`MergeTree` reader <s>creates a column</s>makes extra work for expanding DEFAULT column each time separately when it reads from data part from mark to mark. One can increase `index_granularity` in order to save some memory which is used by that:\r\n```\r\nSELECT DISTINCT `15Id`\r\nFROM test_extract\r\n\r\n[cristal] 2021.06.24 09:50:30.254721 [ 251976 ] {ddaead68-be6a-4f95-bcea-d4a101e7e244} <Debug> MemoryTracker: Peak memory usage (for query): 228.65 MiB.\r\n```\r\nvs\r\n```\r\nSELECT DISTINCT toUInt16OrNull(arrayFirst((v, k) -> (k = '15Id'), arr[2], arr[1]))\r\nFROM test_extract\r\n\r\n[cristal] 2021.06.24 09:51:16.976202 [ 251976 ] {03074434-b25b-4e1c-bae4-425e05a57792} <Debug> MemoryTracker: Peak memory usage (for query): 164.82 MiB.\r\n```\r\n\r\n@alexey-milovidov what do you think?\n```\r\nCREATE TABLE test_extract(str String,  arr Array(Array(String)) ALIAS extractAllGroupsHorizontal(str, '\\\\W(\\\\w+)=(\"[^\"]*?\"|[^\",}]*)')) ENGINE=MergeTree() PARTITION BY tuple() ORDER BY tuple() SETTINGS index_granularity=819200;\r\nINSERT INTO test_extract WITH range(30) as range_arr, arrayMap(x-> concat(toString(x),'Id'), range_arr) as key, arrayMap(x -> rand() % 30, range_arr) as val, arrayStringConcat(arrayMap((x,y) -> concat(x,'=',toString(y)), key, val),',') as str SELECT str FROM numbers(2000000);\r\nALTER TABLE test_extract  add column `15Id` Nullable(UInt16) DEFAULT toUInt16OrNull(arrayFirst((v, k) -> (k = '15Id'),arr[2],arr[1]));\r\n\r\nSELECT DISTINCT 15Id FROM test_extract;\r\n-- MemoryTracker: Peak memory usage (for query): 441.80 MiB.\r\n\r\nSELECT DISTINCT toUInt16OrNull(arrayFirst((v, k) -> (k = '15Id'),arr[2],arr[1])) FROM test_extract;\r\n-- MemoryTracker: Peak memory usage (for query): 329.61 MiB.",
  "created_at": "2021-07-02T13:57:42Z",
  "modified_files": [
    "src/Storages/MergeTree/MergeTreeReadPool.cpp",
    "src/Storages/MergeTree/MergeTreeReverseSelectProcessor.cpp",
    "src/Storages/MergeTree/MergeTreeSelectProcessor.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/01903_correct_block_size_prediction_with_default.reference",
    "b/tests/queries/0_stateless/01903_correct_block_size_prediction_with_default.sql"
  ]
}