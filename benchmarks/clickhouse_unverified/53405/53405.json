{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 53405,
  "instance_id": "ClickHouse__ClickHouse-53405",
  "issue_numbers": [
    "47420",
    "53303"
  ],
  "base_commit": "e7d0edfce6b8ceb9231f3e7a2cdeeb0f73d9ab69",
  "patch": "diff --git a/docs/en/operations/settings/merge-tree-settings.md b/docs/en/operations/settings/merge-tree-settings.md\nindex 8ea599b9861e..8889e04dba1b 100644\n--- a/docs/en/operations/settings/merge-tree-settings.md\n+++ b/docs/en/operations/settings/merge-tree-settings.md\n@@ -623,6 +623,19 @@ Possible values:\n \n Default value: false\n \n+## number_of_free_entries_in_pool_to_execute_optimize_entire_partition {#number_of_free_entries_in_pool_to_execute_optimize_entire_partition}\n+\n+When there is less than specified number of free entries in pool, do not execute optimizing entire partition in the background (this task generated when set `min_age_to_force_merge_seconds` and enable `min_age_to_force_merge_on_partition_only`). This is to leave free threads for regular merges and avoid \"Too many parts\".\n+\n+Possible values:\n+\n+- Positive integer.\n+\n+Default value: 25\n+\n+The value of the `number_of_free_entries_in_pool_to_execute_optimize_entire_partition` setting should be less than the value of the [background_pool_size](/docs/en/operations/server-configuration-parameters/settings.md/#background_pool_size) * [background_merges_mutations_concurrency_ratio](/docs/en/operations/server-configuration-parameters/settings.md/#background_merges_mutations_concurrency_ratio). Otherwise, ClickHouse throws an exception.\n+\n+\n ## allow_floating_point_partition_key {#allow_floating_point_partition_key}\n \n Enables to allow floating-point number as a partition key.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\nindex e89cd8da2327..b3a91add8794 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n@@ -534,11 +534,22 @@ SelectPartsDecision MergeTreeDataMergerMutator::selectPartsToMergeFromRanges(\n String MergeTreeDataMergerMutator::getBestPartitionToOptimizeEntire(\n     const PartitionsInfo & partitions_info) const\n {\n-    const auto data_settings = data.getSettings();\n+    const auto & data_settings = data.getSettings();\n     if (!data_settings->min_age_to_force_merge_on_partition_only)\n         return {};\n     if (!data_settings->min_age_to_force_merge_seconds)\n         return {};\n+    size_t occupied = CurrentMetrics::values[CurrentMetrics::BackgroundMergesAndMutationsPoolTask].load(std::memory_order_relaxed);\n+    size_t max_tasks_count = data.getContext()->getMergeMutateExecutor()->getMaxTasksCount();\n+    if (occupied > 1 && max_tasks_count - occupied < data_settings->number_of_free_entries_in_pool_to_execute_optimize_entire_partition)\n+    {\n+        LOG_INFO(\n+            log,\n+            \"Not enough idle threads to execute optimizing entire partition. See settings \"\n+            \"'number_of_free_entries_in_pool_to_execute_optimize_entire_partition' \"\n+            \"and 'background_pool_size'\");\n+        return {};\n+    }\n \n     auto best_partition_it = std::max_element(\n         partitions_info.begin(),\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.cpp b/src/Storages/MergeTree/MergeTreeSettings.cpp\nindex 6df841059b99..1906f130101b 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSettings.cpp\n@@ -141,6 +141,18 @@ void MergeTreeSettings::sanityCheck(size_t background_pool_tasks) const\n             background_pool_tasks);\n     }\n \n+    if (number_of_free_entries_in_pool_to_execute_optimize_entire_partition > background_pool_tasks)\n+    {\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"The value of 'number_of_free_entries_in_pool_to_execute_optimize_entire_partition' setting\"\n+            \" ({}) (default values are defined in <merge_tree> section of config.xml\"\n+            \" or the value can be specified per table in SETTINGS section of CREATE TABLE query)\"\n+            \" is greater than the value of 'background_pool_size'*'background_merges_mutations_concurrency_ratio'\"\n+            \" ({}) (the value is defined in users.xml for default profile).\"\n+            \" This indicates incorrect configuration because the maximum size of merge will be always lowered.\",\n+            number_of_free_entries_in_pool_to_execute_optimize_entire_partition,\n+            background_pool_tasks);\n+    }\n+\n     // Zero index_granularity is nonsensical.\n     if (index_granularity < 1)\n     {\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex bf67b6a0f520..4b86045604f2 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -65,6 +65,7 @@ struct Settings;\n     M(UInt64, merge_tree_clear_old_broken_detached_parts_ttl_timeout_seconds, 1ULL * 3600 * 24 * 30, \"Remove old broken detached parts in the background if they remained intouched for a specified by this setting period of time.\", 0) \\\n     M(UInt64, min_age_to_force_merge_seconds, 0, \"If all parts in a certain range are older than this value, range will be always eligible for merging. Set to 0 to disable.\", 0) \\\n     M(Bool, min_age_to_force_merge_on_partition_only, false, \"Whether min_age_to_force_merge_seconds should be applied only on the entire partition and not on subset.\", false) \\\n+    M(UInt64, number_of_free_entries_in_pool_to_execute_optimize_entire_partition, 25, \"When there is less than specified number of free entries in pool, do not try to execute optimize entire partition with a merge (this merge is created when set min_age_to_force_merge_seconds > 0 and min_age_to_force_merge_on_partition_only = true). This is to leave free threads for regular merges and avoid \\\"Too many parts\\\"\", 0) \\\n     M(UInt64, merge_tree_enable_clear_old_broken_detached, false, \"Enable clearing old broken detached parts operation in background.\", 0) \\\n     M(Bool, remove_rolled_back_parts_immediately, 1, \"Setting for an incomplete experimental feature.\", 0) \\\n     M(CleanDeletedRows, clean_deleted_rows, CleanDeletedRows::Never, \"Is the Replicated Merge cleanup has to be done automatically at each merge or manually (possible values are 'Always'/'Never' (default))\", 0) \\\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01419_merge_tree_settings_sanity_check.sql b/tests/queries/0_stateless/01419_merge_tree_settings_sanity_check.sql\nindex 686594f435de..5655a8af3d65 100644\n--- a/tests/queries/0_stateless/01419_merge_tree_settings_sanity_check.sql\n+++ b/tests/queries/0_stateless/01419_merge_tree_settings_sanity_check.sql\n@@ -22,6 +22,17 @@ PARTITION BY toYYYYMM(eventday)\n ORDER BY (eventday, user_id)\n SETTINGS number_of_free_entries_in_pool_to_lower_max_size_of_merge = 100; -- { serverError 36 }\n \n+CREATE TABLE mytable_local\n+(\n+    created          DateTime,\n+    eventday         Date,\n+    user_id          UInt32\n+)\n+ENGINE = MergeTree()\n+PARTITION BY toYYYYMM(eventday)\n+ORDER BY (eventday, user_id)\n+SETTINGS number_of_free_entries_in_pool_to_execute_optimize_entire_partition = 100; -- { serverError 36 }\n+\n CREATE TABLE mytable_local\n (\n     created          DateTime,\n",
  "problem_statement": "Setup to implement \"archive\" merges that merges high level parts that are old\nThe current merge algorithm follows a tree approach. It works well, but this means that some data will not be merged for a very long period of time. \r\n\r\nTo force merging old data, there is a setting, min_age_to_force_merge_seconds that can be applied per table. On paper this setting works perfectly, but it is also a timebomb. As i understand, force merges means \"queue merge immediatly as soon as the condition fires\". There is no way to limit concurrency of force merges, so any spike of ingest can trigger a bomb months later where suddently \"archive\" merges will starve normal merges, and there is no way to smooth that. \r\n\r\nDoes this usecase make sense ? Or is it because mergeSelecting settings are not tuned well enough for the data ? I think something like max_number_of_merges_force_in_pool would solve this. \r\n\nConsider adding a separated task for historical merges\n**Use case**\r\n\r\nThe merged algorithm in ClickHouse was designed to favour smaller and more recent parts. This is obvious because we need to minimise number of parts.\r\n\r\nHowever, there's a case when merging old parts also important. For example, when we have a `ReplacingMergeTree` and the `SELECT ... FINAL` spans across several partitions. Thanks to `do_not_merge_across_partitions_select_final`, if a partition only has 1 part, we don't need to apply `FINAL` logic on that partition, which significantly improve the query performance. Therefore we want that at some point, the \"old\" partition (last week, last month, etc...) will only contain 1 part.\r\n\r\n**What we have now**\r\n\r\nI think the use case has been raised in #35836, and there's already a solution with `min_age_to_force_merge_seconds` in #42423. With `min_age_to_force_merge_seconds`, once the parts stay long enough,  they will be allowed to merged without respecting the criteria (total size, age...).\r\n\r\nHowever, this setting can lead to merge starving for recent parts, because: historical merges are usually big (not to say super big), and number of concurrent merges are limited.\r\n\r\n```\r\n-- Future merge tasks, mixing between small (recent parts) merges and big (old parts) merges\r\n@@@@@@@@@\r\n@\r\n@@@@@@@@@@@\r\n@\r\n@@\r\n@@@@@@@@@@@@@@\r\n@@\r\n-- Current merge tasks queue - running in round robin manner for each @\r\n@\r\n@@\r\n@@@@\r\n@@@@@\r\n@@@@@@\r\n@@@@@@\r\n@@@@@@@@@@\r\n\r\n-- Current merge tasks at some points\r\n@@@@@@@@\r\n@@@@@@@@@\r\n@@@@@@@@@@@\r\n@@@@@@\r\n@@@@@@\r\n@@@@@@\r\n@@@@@@@\r\n``` \r\n\r\nIf we change `background_merges_mutations_scheduling_policy` to `shortest_task_first`, then we face the same old issue again: small merges are alway preferred, while big merges can stuck in merge queue forever (ever if they're scheduled).\r\n\r\n**Describe the solution you'd like**\r\n\r\nEach table having a separated task to optimize old partitions. The task will get thread from common schedule pool, and it will scan from oldest partition -> most recent partition to find which partition it can merge to single part. Whether to active this task or not will be controlled by a table setting. Merging old partition will takes some times, but eventually we will reach a point where every old partitions only have 1 part.\r\n\r\nI think this is the original idea implemented in #35836, but then changed to current solution.\r\n\r\nI don't know if there's a better solution, appreciate any comments [THANKS]!\r\n\r\n\n",
  "hints_text": "> As i understand, force merges means\r\n\r\nDid you test it? What was the exact behavior?\nI did yes, it went quite badly. Initially i had merge pool too big so this was my bad killed the cluster immediatly, but then even with lower pool what i am seeing is :\r\n - ~80% of merges slot are occupied by those force_merge.\r\n - Those merges are ~10 times bigger than usual merges, which makes sense, but they fill the pool and run for a long time\r\n - there is not enough slots to handle spikes of normal merges. \r\n\r\nI'm not sure how to tune this, since having over provisionned merges slot works well for spikes, and the merge selector does a great job prioritizing small merges, but enabling min_age_to_force_merge_seconds will immedialy fill most of the pool. \r\n \r\nLetting it run for 1 day, number of level 0 part increased by half, which caused some issues. And it is not only enabling, i am afraid that in next period it will start again the same. \n",
  "created_at": "2023-08-14T11:06:36Z",
  "modified_files": [
    "docs/en/operations/settings/merge-tree-settings.md",
    "src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp",
    "src/Storages/MergeTree/MergeTreeSettings.cpp",
    "src/Storages/MergeTree/MergeTreeSettings.h"
  ],
  "modified_test_files": [
    "tests/queries/0_stateless/01419_merge_tree_settings_sanity_check.sql"
  ]
}