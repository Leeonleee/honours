diff --git a/src/Storages/tests/gtest_SplitTokenExtractor.cpp b/src/Storages/tests/gtest_SplitTokenExtractor.cpp
index b5a26c9cd8ea..ee6a55f50b88 100644
--- a/src/Storages/tests/gtest_SplitTokenExtractor.cpp
+++ b/src/Storages/tests/gtest_SplitTokenExtractor.cpp
@@ -61,12 +61,12 @@ TEST_P(SplitTokenExtractorTest, next)
     for (const auto & expected_token : param.tokens)
     {
         SCOPED_TRACE(++i);
-        ASSERT_TRUE(token_extractor.next(data->data(), data->size(), &pos, &token_start, &token_len));
+        ASSERT_TRUE(token_extractor.nextInColumn(data->data(), data->size(), &pos, &token_start, &token_len));
 
         EXPECT_EQ(expected_token, std::string_view(data->data() + token_start, token_len))
                 << " token_start:" << token_start << " token_len: " << token_len;
     }
-    ASSERT_FALSE(token_extractor.next(data->data(), data->size(), &pos, &token_start, &token_len))
+    ASSERT_FALSE(token_extractor.nextInColumn(data->data(), data->size(), &pos, &token_start, &token_len))
             << "
\t=> \"" << param.source.substr(token_start, token_len) << "\""
             << "
\t" << token_start << ", " << token_len << ", " << pos << ", " << data->size();
 }
diff --git a/tests/queries/0_stateless/01781_token_extractor_buffer_overflow.reference b/tests/queries/0_stateless/01781_token_extractor_buffer_overflow.reference
new file mode 100644
index 000000000000..aa47d0d46d47
--- /dev/null
+++ b/tests/queries/0_stateless/01781_token_extractor_buffer_overflow.reference
@@ -0,0 +1,2 @@
+0
+0
diff --git a/tests/queries/0_stateless/01781_token_extractor_buffer_overflow.sql b/tests/queries/0_stateless/01781_token_extractor_buffer_overflow.sql
new file mode 100644
index 000000000000..4cc216955b32
--- /dev/null
+++ b/tests/queries/0_stateless/01781_token_extractor_buffer_overflow.sql
@@ -0,0 +1,10 @@
+SET max_block_size = 10, min_insert_block_size_rows = 0, min_insert_block_size_bytes = 0, max_threads = 20;
+
+DROP TABLE IF EXISTS bloom_filter;
+CREATE TABLE bloom_filter (`id` UInt64, `s` String, INDEX tok_bf (s, lower(s)) TYPE tokenbf_v1(512, 3, 0) GRANULARITY 1) ENGINE = MergeTree ORDER BY id SETTINGS index_granularity = 8;
+INSERT INTO bloom_filter SELECT number, 'yyy,uuu' FROM numbers(1024);
+
+SELECT max(id) FROM bloom_filter WHERE hasToken(s, 'abc');
+SELECT max(id) FROM bloom_filter WHERE hasToken(s, 'abcabcabcabcabcabcabcab\0');
+
+DROP TABLE bloom_filter;
