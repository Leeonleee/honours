{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 38265,
  "instance_id": "ClickHouse__ClickHouse-38265",
  "issue_numbers": [
    "33710"
  ],
  "base_commit": "e29582daadf7ff91c0c8a1aa4681cada017e3375",
  "patch": "diff --git a/docs/en/operations/server-configuration-parameters/settings.md b/docs/en/operations/server-configuration-parameters/settings.md\nindex ad879679a3dc..d3a50969a392 100644\n--- a/docs/en/operations/server-configuration-parameters/settings.md\n+++ b/docs/en/operations/server-configuration-parameters/settings.md\n@@ -193,6 +193,35 @@ Sets the delay before remove table data in seconds. If the query has `SYNC` modi\n \n Default value: `480` (8 minute).\n \n+## database_catalog_unused_dir_hide_timeout_sec {#database_catalog_unused_dir_hide_timeout_sec}\n+\n+Parameter of a task that cleans up garbage from `store/` directory.\n+If some subdirectory is not used by clickhouse-server and this directory was not modified for last\n+`database_catalog_unused_dir_hide_timeout_sec` seconds, the task will \"hide\" this directory by \n+removing all access rights. It also works for directories that clickhouse-server does not\n+expect to see inside `store/`. Zero means \"immediately\".\n+\n+Default value: `3600` (1 hour).\n+\n+## database_catalog_unused_dir_rm_timeout_sec {#database_catalog_unused_dir_rm_timeout_sec}\n+\n+Parameter of a task that cleans up garbage from `store/` directory.\n+If some subdirectory is not used by clickhouse-server and it was previousely \"hidden\" \n+(see [database_catalog_unused_dir_hide_timeout_sec](../../operations/server-configuration-parameters/settings.md#database_catalog_unused_dir_hide_timeout_sec)) \n+and this directory was not modified for last\n+`database_catalog_unused_dir_rm_timeout_sec` seconds, the task will remove this directory. \n+It also works for directories that clickhouse-server does not\n+expect to see inside `store/`. Zero means \"never\".\n+\n+Default value: `2592000` (30 days).\n+\n+## database_catalog_unused_dir_cleanup_period_sec {#database_catalog_unused_dir_cleanup_period_sec}\n+\n+Parameter of a task that cleans up garbage from `store/` directory.\n+Sets scheduling period of the task. Zero means \"never\".\n+\n+Default value: `86400` (1 day).\n+\n ## default_database {#default-database}\n \n The default database.\ndiff --git a/src/Access/DiskAccessStorage.cpp b/src/Access/DiskAccessStorage.cpp\nindex a9b7a6a265b9..231e325196d9 100644\n--- a/src/Access/DiskAccessStorage.cpp\n+++ b/src/Access/DiskAccessStorage.cpp\n@@ -153,15 +153,7 @@ namespace\n \n     bool tryParseUUID(const String & str, UUID & id)\n     {\n-        try\n-        {\n-            id = parseFromString<UUID>(str);\n-            return true;\n-        }\n-        catch (...)\n-        {\n-            return false;\n-        }\n+        return tryParse(id, str);\n     }\n }\n \ndiff --git a/src/Common/filesystemHelpers.cpp b/src/Common/filesystemHelpers.cpp\nindex 3b05e6713847..b917a0a1d13d 100644\n--- a/src/Common/filesystemHelpers.cpp\n+++ b/src/Common/filesystemHelpers.cpp\n@@ -1,12 +1,10 @@\n #include \"filesystemHelpers.h\"\n \n #if defined(OS_LINUX)\n-#    include <cstdio>\n #    include <mntent.h>\n #    include <sys/sysmacros.h>\n #endif\n #include <cerrno>\n-#include <Poco/Version.h>\n #include <Poco/Timestamp.h>\n #include <filesystem>\n #include <fcntl.h>\ndiff --git a/src/Databases/DatabaseAtomic.cpp b/src/Databases/DatabaseAtomic.cpp\nindex ea887c84111c..a4fa1fa267b8 100644\n--- a/src/Databases/DatabaseAtomic.cpp\n+++ b/src/Databases/DatabaseAtomic.cpp\n@@ -292,7 +292,6 @@ void DatabaseAtomic::commitCreateTable(const ASTCreateQuery & query, const Stora\n {\n     DetachedTables not_in_use;\n     auto table_data_path = getTableDataPath(query);\n-    bool locked_uuid = false;\n     try\n     {\n         std::unique_lock lock{mutex};\n@@ -302,9 +301,7 @@ void DatabaseAtomic::commitCreateTable(const ASTCreateQuery & query, const Stora\n         /// Do some checks before renaming file from .tmp to .sql\n         not_in_use = cleanupDetachedTables();\n         assertDetachedTableNotInUse(query.uuid);\n-        /// We will get en exception if some table with the same UUID exists (even if it's detached table or table from another database)\n-        DatabaseCatalog::instance().addUUIDMapping(query.uuid);\n-        locked_uuid = true;\n+        chassert(DatabaseCatalog::instance().hasUUIDMapping(query.uuid));\n \n         auto txn = query_context->getZooKeeperMetadataTransaction();\n         if (txn && !query_context->isInternalSubquery())\n@@ -321,8 +318,6 @@ void DatabaseAtomic::commitCreateTable(const ASTCreateQuery & query, const Stora\n     catch (...)\n     {\n         fs::remove(table_metadata_tmp_path);\n-        if (locked_uuid)\n-            DatabaseCatalog::instance().removeUUIDMappingFinally(query.uuid);\n         throw;\n     }\n     if (table->storesDataOnDisk())\ndiff --git a/src/Databases/DatabaseOnDisk.cpp b/src/Databases/DatabaseOnDisk.cpp\nindex 9484da8ec2d0..64bc9a4a0946 100644\n--- a/src/Databases/DatabaseOnDisk.cpp\n+++ b/src/Databases/DatabaseOnDisk.cpp\n@@ -395,6 +395,18 @@ void DatabaseOnDisk::renameTable(\n         if (auto * target_db = dynamic_cast<DatabaseOnDisk *>(&to_database))\n             target_db->checkMetadataFilenameAvailability(to_table_name);\n \n+        /// This place is actually quite dangerous. Since data directory is moved to store/\n+        /// DatabaseCatalog may try to clean it up as unused. We add UUID mapping to avoid this.\n+        /// However, we may fail after data directory move, but before metadata file creation in the destination db.\n+        /// In this case nothing will protect data directory (except 30-days timeout).\n+        /// But this situation (when table in Ordinary database is partially renamed) require manual intervention anyway.\n+        if (from_ordinary_to_atomic)\n+        {\n+            DatabaseCatalog::instance().addUUIDMapping(create.uuid);\n+            if (table->storesDataOnDisk())\n+                LOG_INFO(log, \"Moving table from {} to {}\", table_data_relative_path, to_database.getTableDataPath(create));\n+        }\n+\n         /// Notify the table that it is renamed. It will move data to new path (if it stores data on disk) and update StorageID\n         table->rename(to_database.getTableDataPath(create), StorageID(create));\n     }\ndiff --git a/src/Databases/DatabaseOrdinary.cpp b/src/Databases/DatabaseOrdinary.cpp\nindex ada9030905da..2b88fbbfcf7d 100644\n--- a/src/Databases/DatabaseOrdinary.cpp\n+++ b/src/Databases/DatabaseOrdinary.cpp\n@@ -29,6 +29,12 @@ namespace fs = std::filesystem;\n \n namespace DB\n {\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+\n static constexpr size_t METADATA_FILE_BUFFER_SIZE = 32768;\n \n namespace\n@@ -83,7 +89,7 @@ void DatabaseOrdinary::loadStoredObjects(\n       */\n \n     ParsedTablesMetadata metadata;\n-    loadTablesMetadata(local_context, metadata);\n+    loadTablesMetadata(local_context, metadata, force_attach);\n \n     size_t total_tables = metadata.parsed_tables.size() - metadata.total_dictionaries;\n \n@@ -151,12 +157,12 @@ void DatabaseOrdinary::loadStoredObjects(\n     }\n }\n \n-void DatabaseOrdinary::loadTablesMetadata(ContextPtr local_context, ParsedTablesMetadata & metadata)\n+void DatabaseOrdinary::loadTablesMetadata(ContextPtr local_context, ParsedTablesMetadata & metadata, bool is_startup)\n {\n     size_t prev_tables_count = metadata.parsed_tables.size();\n     size_t prev_total_dictionaries = metadata.total_dictionaries;\n \n-    auto process_metadata = [&metadata, this](const String & file_name)\n+    auto process_metadata = [&metadata, is_startup, this](const String & file_name)\n     {\n         fs::path path(getMetadataPath());\n         fs::path file_path(file_name);\n@@ -170,12 +176,26 @@ void DatabaseOrdinary::loadTablesMetadata(ContextPtr local_context, ParsedTables\n                 auto * create_query = ast->as<ASTCreateQuery>();\n                 create_query->setDatabase(database_name);\n \n-                if (fs::exists(full_path.string() + detached_suffix))\n+                /// Even if we don't load the table we can still mark the uuid of it as taken.\n+                if (create_query->uuid != UUIDHelpers::Nil)\n                 {\n-                    /// FIXME: even if we don't load the table we can still mark the uuid of it as taken.\n-                    /// if (create_query->uuid != UUIDHelpers::Nil)\n-                    ///     DatabaseCatalog::instance().addUUIDMapping(create_query->uuid);\n+                    /// A bit tricky way to distinguish ATTACH DATABASE and server startup (actually it's \"force_attach\" flag).\n+                    if (is_startup)\n+                    {\n+                        /// Server is starting up. Lock UUID used by permanently detached table.\n+                        DatabaseCatalog::instance().addUUIDMapping(create_query->uuid);\n+                    }\n+                    else if (!DatabaseCatalog::instance().hasUUIDMapping(create_query->uuid))\n+                    {\n+                        /// It's ATTACH DATABASE. UUID for permanently detached table must be already locked.\n+                        /// FIXME MaterializedPostgreSQL works with UUIDs incorrectly and breaks invariants\n+                        if (getEngineName() != \"MaterializedPostgreSQL\")\n+                            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot find UUID mapping for {}, it's a bug\", create_query->uuid);\n+                    }\n+                }\n \n+                if (fs::exists(full_path.string() + detached_suffix))\n+                {\n                     const std::string table_name = unescapeForFileName(file_name.substr(0, file_name.size() - 4));\n                     LOG_DEBUG(log, \"Skipping permanently detached table {}.\", backQuote(table_name));\n                     return;\ndiff --git a/src/Databases/DatabaseOrdinary.h b/src/Databases/DatabaseOrdinary.h\nindex 982be2024ce8..6e524ae18b0c 100644\n--- a/src/Databases/DatabaseOrdinary.h\n+++ b/src/Databases/DatabaseOrdinary.h\n@@ -25,7 +25,7 @@ class DatabaseOrdinary : public DatabaseOnDisk\n \n     bool supportsLoadingInTopologicalOrder() const override { return true; }\n \n-    void loadTablesMetadata(ContextPtr context, ParsedTablesMetadata & metadata) override;\n+    void loadTablesMetadata(ContextPtr context, ParsedTablesMetadata & metadata, bool is_startup) override;\n \n     void loadTableFromMetadata(ContextMutablePtr local_context, const String & file_path, const QualifiedTableName & name, const ASTPtr & ast, bool force_restore) override;\n \ndiff --git a/src/Databases/IDatabase.h b/src/Databases/IDatabase.h\nindex 38c85cf3d055..a81df0a389af 100644\n--- a/src/Databases/IDatabase.h\n+++ b/src/Databases/IDatabase.h\n@@ -148,7 +148,7 @@ class IDatabase : public std::enable_shared_from_this<IDatabase>\n     {\n     }\n \n-    virtual void loadTablesMetadata(ContextPtr /*local_context*/, ParsedTablesMetadata & /*metadata*/)\n+    virtual void loadTablesMetadata(ContextPtr /*local_context*/, ParsedTablesMetadata & /*metadata*/, bool /*is_startup*/)\n     {\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Not implemented\");\n     }\ndiff --git a/src/Databases/PostgreSQL/DatabaseMaterializedPostgreSQL.cpp b/src/Databases/PostgreSQL/DatabaseMaterializedPostgreSQL.cpp\nindex 60d2fa0d2c82..db184342a97a 100644\n--- a/src/Databases/PostgreSQL/DatabaseMaterializedPostgreSQL.cpp\n+++ b/src/Databases/PostgreSQL/DatabaseMaterializedPostgreSQL.cpp\n@@ -259,6 +259,7 @@ void DatabaseMaterializedPostgreSQL::createTable(ContextPtr local_context, const\n     auto * create_query = assert_cast<ASTCreateQuery *>(query_copy.get());\n     create_query->attach = false;\n     create_query->attach_short_syntax = false;\n+    DatabaseCatalog::instance().addUUIDMapping(create->uuid);\n     DatabaseAtomic::createTable(StorageMaterializedPostgreSQL::makeNestedTableContext(local_context), table_name, table, query_copy);\n \n     /// Attach MaterializedPostgreSQL table.\ndiff --git a/src/Databases/TablesLoader.cpp b/src/Databases/TablesLoader.cpp\nindex 898376f8e0f6..e973c9211be1 100644\n--- a/src/Databases/TablesLoader.cpp\n+++ b/src/Databases/TablesLoader.cpp\n@@ -93,7 +93,7 @@ void TablesLoader::loadTables()\n     for (auto & database_name : databases_to_load)\n     {\n         databases[database_name]->beforeLoadingMetadata(global_context, force_restore, force_attach);\n-        databases[database_name]->loadTablesMetadata(global_context, metadata);\n+        databases[database_name]->loadTablesMetadata(global_context, metadata, force_attach);\n     }\n \n     LOG_INFO(log, \"Parsed metadata of {} tables in {} databases in {} sec\",\ndiff --git a/src/IO/ReadHelpers.h b/src/IO/ReadHelpers.h\nindex 8fe8e2aa23e6..4cd07dddf253 100644\n--- a/src/IO/ReadHelpers.h\n+++ b/src/IO/ReadHelpers.h\n@@ -1065,6 +1065,8 @@ inline bool tryReadText(is_integer auto & x, ReadBuffer & buf)\n     return tryReadIntText(x, buf);\n }\n \n+inline bool tryReadText(UUID & x, ReadBuffer & buf) { return tryReadUUIDText(x, buf); }\n+\n inline void readText(is_floating_point auto & x, ReadBuffer & buf) { readFloatText(x, buf); }\n \n inline void readText(String & x, ReadBuffer & buf) { readEscapedString(x, buf); }\ndiff --git a/src/Interpreters/DatabaseCatalog.cpp b/src/Interpreters/DatabaseCatalog.cpp\nindex fa9444a7e66b..13340221f381 100644\n--- a/src/Interpreters/DatabaseCatalog.cpp\n+++ b/src/Interpreters/DatabaseCatalog.cpp\n@@ -16,8 +16,12 @@\n #include <Common/CurrentMetrics.h>\n #include <Common/logger_useful.h>\n #include <Poco/Util/AbstractConfiguration.h>\n-#include <filesystem>\n #include <Common/filesystemHelpers.h>\n+#include <Common/noexcept_scope.h>\n+\n+#include <fcntl.h>\n+#include <sys/stat.h>\n+#include <utime.h>\n \n #include \"config_core.h\"\n \n@@ -27,12 +31,10 @@\n #endif\n \n #if USE_LIBPQXX\n-#    include <Storages/PostgreSQL/StorageMaterializedPostgreSQL.h>\n #    include <Databases/PostgreSQL/DatabaseMaterializedPostgreSQL.h>\n+#    include <Storages/PostgreSQL/StorageMaterializedPostgreSQL.h>\n #endif\n \n-namespace fs = std::filesystem;\n-\n namespace CurrentMetrics\n {\n     extern const Metric TablesToDropQueueSize;\n@@ -77,6 +79,7 @@ TemporaryTableHolder::TemporaryTableHolder(ContextPtr context_, const TemporaryT\n     }\n     auto table_id = StorageID(DatabaseCatalog::TEMPORARY_DATABASE, global_name, id);\n     auto table = creator(table_id);\n+    DatabaseCatalog::instance().addUUIDMapping(id);\n     temporary_tables->createTable(getContext(), global_name, table, original_create);\n     table->startup();\n }\n@@ -143,6 +146,9 @@ StoragePtr TemporaryTableHolder::getTable() const\n void DatabaseCatalog::initializeAndLoadTemporaryDatabase()\n {\n     drop_delay_sec = getContext()->getConfigRef().getInt(\"database_atomic_delay_before_drop_table_sec\", default_drop_delay_sec);\n+    unused_dir_hide_timeout_sec = getContext()->getConfigRef().getInt(\"database_catalog_unused_dir_hide_timeout_sec\", unused_dir_hide_timeout_sec);\n+    unused_dir_rm_timeout_sec = getContext()->getConfigRef().getInt(\"database_catalog_unused_dir_rm_timeout_sec\", unused_dir_rm_timeout_sec);\n+    unused_dir_cleanup_period_sec = getContext()->getConfigRef().getInt(\"database_catalog_unused_dir_cleanup_period_sec\", unused_dir_cleanup_period_sec);\n \n     auto db_for_temporary_and_external_tables = std::make_shared<DatabaseMemory>(TEMPORARY_DATABASE, getContext());\n     attachDatabase(TEMPORARY_DATABASE, db_for_temporary_and_external_tables);\n@@ -150,6 +156,16 @@ void DatabaseCatalog::initializeAndLoadTemporaryDatabase()\n \n void DatabaseCatalog::loadDatabases()\n {\n+    if (Context::getGlobalContextInstance()->getApplicationType() == Context::ApplicationType::SERVER && unused_dir_cleanup_period_sec)\n+    {\n+        auto cleanup_task_holder\n+            = getContext()->getSchedulePool().createTask(\"DatabaseCatalog\", [this]() { this->cleanupStoreDirectoryTask(); });\n+        cleanup_task = std::make_unique<BackgroundSchedulePoolTaskHolder>(std::move(cleanup_task_holder));\n+        (*cleanup_task)->activate();\n+        /// Do not start task immediately on server startup, it's not urgent.\n+        (*cleanup_task)->scheduleAfter(unused_dir_hide_timeout_sec * 1000);\n+    }\n+\n     auto task_holder = getContext()->getSchedulePool().createTask(\"DatabaseCatalog\", [this](){ this->dropTableDataTask(); });\n     drop_task = std::make_unique<BackgroundSchedulePoolTaskHolder>(std::move(task_holder));\n     (*drop_task)->activate();\n@@ -166,6 +182,9 @@ void DatabaseCatalog::shutdownImpl()\n {\n     TemporaryLiveViewCleaner::shutdown();\n \n+    if (cleanup_task)\n+        (*cleanup_task)->deactivate();\n+\n     if (drop_task)\n         (*drop_task)->deactivate();\n \n@@ -189,19 +208,25 @@ void DatabaseCatalog::shutdownImpl()\n     tables_marked_dropped.clear();\n \n     std::lock_guard lock(databases_mutex);\n+    for (const auto & db : databases)\n+    {\n+        UUID db_uuid = db.second->getUUID();\n+        if (db_uuid != UUIDHelpers::Nil)\n+            removeUUIDMapping(db_uuid);\n+    }\n     assert(std::find_if(uuid_map.begin(), uuid_map.end(), [](const auto & elem)\n     {\n         /// Ensure that all UUID mappings are empty (i.e. all mappings contain nullptr instead of a pointer to storage)\n         const auto & not_empty_mapping = [] (const auto & mapping)\n         {\n+            auto & db = mapping.second.first;\n             auto & table = mapping.second.second;\n-            return table;\n+            return db || table;\n         };\n         auto it = std::find_if(elem.map.begin(), elem.map.end(), not_empty_mapping);\n         return it != elem.map.end();\n     }) == uuid_map.end());\n     databases.clear();\n-    db_uuid_map.clear();\n     view_dependencies.clear();\n }\n \n@@ -331,9 +356,10 @@ void DatabaseCatalog::attachDatabase(const String & database_name, const Databas\n     std::lock_guard lock{databases_mutex};\n     assertDatabaseDoesntExistUnlocked(database_name);\n     databases.emplace(database_name, database);\n+    NOEXCEPT_SCOPE;\n     UUID db_uuid = database->getUUID();\n     if (db_uuid != UUIDHelpers::Nil)\n-        db_uuid_map.emplace(db_uuid, database);\n+        addUUIDMapping(db_uuid, database, nullptr);\n }\n \n \n@@ -347,7 +373,9 @@ DatabasePtr DatabaseCatalog::detachDatabase(ContextPtr local_context, const Stri\n         std::lock_guard lock{databases_mutex};\n         assertDatabaseExistsUnlocked(database_name);\n         db = databases.find(database_name)->second;\n-        db_uuid_map.erase(db->getUUID());\n+        UUID db_uuid = db->getUUID();\n+        if (db_uuid != UUIDHelpers::Nil)\n+            removeUUIDMapping(db_uuid);\n         databases.erase(database_name);\n     }\n \n@@ -372,6 +400,8 @@ DatabasePtr DatabaseCatalog::detachDatabase(ContextPtr local_context, const Stri\n \n     if (drop)\n     {\n+        UUID db_uuid = db->getUUID();\n+\n         /// Delete the database.\n         db->drop(local_context);\n \n@@ -381,6 +411,9 @@ DatabasePtr DatabaseCatalog::detachDatabase(ContextPtr local_context, const Stri\n         fs::remove(database_metadata_dir);\n         fs::path database_metadata_file = fs::path(getContext()->getPath()) / \"metadata\" / (escapeForFileName(database_name) + \".sql\");\n         fs::remove(database_metadata_file);\n+\n+        if (db_uuid != UUIDHelpers::Nil)\n+            removeUUIDMappingFinally(db_uuid);\n     }\n \n     return db;\n@@ -427,21 +460,19 @@ DatabasePtr DatabaseCatalog::tryGetDatabase(const String & database_name) const\n \n DatabasePtr DatabaseCatalog::getDatabase(const UUID & uuid) const\n {\n-    std::lock_guard lock{databases_mutex};\n-    auto it = db_uuid_map.find(uuid);\n-    if (it == db_uuid_map.end())\n+    auto db_and_table = tryGetByUUID(uuid);\n+    if (!db_and_table.first || db_and_table.second)\n         throw Exception(ErrorCodes::UNKNOWN_DATABASE, \"Database UUID {} does not exist\", toString(uuid));\n-    return it->second;\n+    return db_and_table.first;\n }\n \n DatabasePtr DatabaseCatalog::tryGetDatabase(const UUID & uuid) const\n {\n     assert(uuid != UUIDHelpers::Nil);\n-    std::lock_guard lock{databases_mutex};\n-    auto it = db_uuid_map.find(uuid);\n-    if (it == db_uuid_map.end())\n+    auto db_and_table = tryGetByUUID(uuid);\n+    if (!db_and_table.first || db_and_table.second)\n         return {};\n-    return it->second;\n+    return db_and_table.first;\n }\n \n bool DatabaseCatalog::isDatabaseExist(const String & database_name) const\n@@ -496,18 +527,22 @@ void DatabaseCatalog::addUUIDMapping(const UUID & uuid)\n void DatabaseCatalog::addUUIDMapping(const UUID & uuid, const DatabasePtr & database, const StoragePtr & table)\n {\n     assert(uuid != UUIDHelpers::Nil && getFirstLevelIdx(uuid) < uuid_map.size());\n-    assert((database && table) || (!database && !table));\n+    assert(database || !table);\n     UUIDToStorageMapPart & map_part = uuid_map[getFirstLevelIdx(uuid)];\n     std::lock_guard lock{map_part.mutex};\n     auto [it, inserted] = map_part.map.try_emplace(uuid, database, table);\n     if (inserted)\n+    {\n+        /// Mapping must be locked before actually inserting something\n+        chassert((!database && !table));\n         return;\n+    }\n \n     auto & prev_database = it->second.first;\n     auto & prev_table = it->second.second;\n-    assert((prev_database && prev_table) || (!prev_database && !prev_table));\n+    assert(prev_database || !prev_table);\n \n-    if (!prev_table && table)\n+    if (!prev_database && database)\n     {\n         /// It's empty mapping, it was created to \"lock\" UUID and prevent collision. Just update it.\n         prev_database = database;\n@@ -515,8 +550,8 @@ void DatabaseCatalog::addUUIDMapping(const UUID & uuid, const DatabasePtr & data\n         return;\n     }\n \n-    /// We are trying to replace existing mapping (prev_table != nullptr), it's logical error\n-    if (table)\n+    /// We are trying to replace existing mapping (prev_database != nullptr), it's logical error\n+    if (database || table)\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Mapping for table with UUID={} already exists\", toString(uuid));\n     /// Normally this should never happen, but it's possible when the same UUIDs are explicitly specified in different CREATE queries,\n     /// so it's not LOGICAL_ERROR\n@@ -560,6 +595,14 @@ void DatabaseCatalog::updateUUIDMapping(const UUID & uuid, DatabasePtr database,\n     prev_table = std::move(table);\n }\n \n+bool DatabaseCatalog::hasUUIDMapping(const UUID & uuid)\n+{\n+    assert(uuid != UUIDHelpers::Nil && getFirstLevelIdx(uuid) < uuid_map.size());\n+    UUIDToStorageMapPart & map_part = uuid_map[getFirstLevelIdx(uuid)];\n+    std::lock_guard lock{map_part.mutex};\n+    return map_part.map.contains(uuid);\n+}\n+\n std::unique_ptr<DatabaseCatalog> DatabaseCatalog::database_catalog;\n \n DatabaseCatalog::DatabaseCatalog(ContextMutablePtr global_context_)\n@@ -701,6 +744,8 @@ DatabaseAndTable DatabaseCatalog::tryGetDatabaseAndTable(const StorageID & table\n \n void DatabaseCatalog::loadMarkedAsDroppedTables()\n {\n+    assert(!cleanup_task);\n+\n     /// /clickhouse_root/metadata_dropped/ contains files with metadata of tables,\n     /// which where marked as dropped by Atomic databases.\n     /// Data directories of such tables still exists in store/\n@@ -780,6 +825,7 @@ void DatabaseCatalog::enqueueDroppedTableCleanup(StorageID table_id, StoragePtr\n     time_t drop_time;\n     if (table)\n     {\n+        chassert(hasUUIDMapping(table_id.uuid));\n         drop_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());\n         table->is_dropped = true;\n     }\n@@ -1072,6 +1118,166 @@ void DatabaseCatalog::updateLoadingDependencies(const StorageID & table_id, Tabl\n     old_dependencies = std::move(new_dependencies);\n }\n \n+void DatabaseCatalog::cleanupStoreDirectoryTask()\n+{\n+    fs::path store_path = fs::path(getContext()->getPath()) / \"store\";\n+    size_t affected_dirs = 0;\n+    for (const auto & prefix_dir : fs::directory_iterator{store_path})\n+    {\n+        String prefix = prefix_dir.path().filename();\n+        bool expected_prefix_dir = prefix_dir.is_directory() &&\n+            prefix.size() == 3 &&\n+            isHexDigit(prefix[0]) &&\n+            isHexDigit(prefix[1]) &&\n+            isHexDigit(prefix[2]);\n+\n+        if (!expected_prefix_dir)\n+        {\n+            LOG_WARNING(log, \"Found invalid directory {}, will try to remove it\", prefix_dir.path().string());\n+            affected_dirs += maybeRemoveDirectory(prefix_dir.path());\n+            continue;\n+        }\n+\n+        for (const auto & uuid_dir : fs::directory_iterator{prefix_dir.path()})\n+        {\n+            String uuid_str = uuid_dir.path().filename();\n+            UUID uuid;\n+            bool parsed = tryParse(uuid, uuid_str);\n+\n+            bool expected_dir = uuid_dir.is_directory() &&\n+                parsed &&\n+                uuid != UUIDHelpers::Nil &&\n+                uuid_str.starts_with(prefix);\n+\n+            if (!expected_dir)\n+            {\n+                LOG_WARNING(log, \"Found invalid directory {}, will try to remove it\", uuid_dir.path().string());\n+                affected_dirs += maybeRemoveDirectory(uuid_dir.path());\n+                continue;\n+            }\n+\n+            /// Order is important\n+            if (!hasUUIDMapping(uuid))\n+            {\n+                /// We load uuids even for detached and permanently detached tables,\n+                /// so it looks safe enough to remove directory if we don't have uuid mapping for it.\n+                /// No table or database using this directory should concurrently appear,\n+                /// because creation of new table would fail with \"directory already exists\".\n+                affected_dirs += maybeRemoveDirectory(uuid_dir.path());\n+            }\n+        }\n+    }\n+\n+    if (affected_dirs)\n+        LOG_INFO(log, \"Cleaned up {} directories from store/\", affected_dirs);\n+\n+    (*cleanup_task)->scheduleAfter(unused_dir_cleanup_period_sec * 1000);\n+}\n+\n+bool DatabaseCatalog::maybeRemoveDirectory(const fs::path & unused_dir)\n+{\n+    /// \"Safe\" automatic removal of some directory.\n+    /// At first we do not remove anything and only revoke all access right.\n+    /// And remove only if nobody noticed it after, for example, one month.\n+\n+    struct stat st;\n+    if (stat(unused_dir.string().c_str(), &st))\n+    {\n+        LOG_ERROR(log, \"Failed to stat {}, errno: {}\", unused_dir.string(), errno);\n+        return false;\n+    }\n+\n+    if (st.st_uid != geteuid())\n+    {\n+        /// Directory is not owned by clickhouse, it's weird, let's ignore it (chmod will likely fail anyway).\n+        LOG_WARNING(log, \"Found directory {} with unexpected owner (uid={})\", unused_dir.string(), st.st_uid);\n+        return false;\n+    }\n+\n+    time_t max_modification_time = std::max(st.st_atime, std::max(st.st_mtime, st.st_ctime));\n+    time_t current_time = time(nullptr);\n+    if (st.st_mode & (S_IRWXU | S_IRWXG | S_IRWXO))\n+    {\n+        if (current_time <= max_modification_time + unused_dir_hide_timeout_sec)\n+            return false;\n+\n+        LOG_INFO(log, \"Removing access rights for unused directory {} (will remove it when timeout exceed)\", unused_dir.string());\n+\n+        /// Explicitly update modification time just in case\n+\n+        struct utimbuf tb;\n+        tb.actime = current_time;\n+        tb.modtime = current_time;\n+        if (utime(unused_dir.string().c_str(), &tb) != 0)\n+            LOG_ERROR(log, \"Failed to utime {}, errno: {}\", unused_dir.string(), errno);\n+\n+        /// Remove all access right\n+        if (chmod(unused_dir.string().c_str(), 0))\n+            LOG_ERROR(log, \"Failed to chmod {}, errno: {}\", unused_dir.string(), errno);\n+\n+        return true;\n+    }\n+    else\n+    {\n+        if (!unused_dir_rm_timeout_sec)\n+            return false;\n+\n+        if (current_time <= max_modification_time + unused_dir_rm_timeout_sec)\n+            return false;\n+\n+        LOG_INFO(log, \"Removing unused directory {}\", unused_dir.string());\n+\n+        /// We have to set these access rights to make recursive removal work\n+        if (chmod(unused_dir.string().c_str(), S_IRWXU))\n+            LOG_ERROR(log, \"Failed to chmod {}, errno: {}\", unused_dir.string(), errno);\n+\n+        fs::remove_all(unused_dir);\n+\n+        return true;\n+    }\n+}\n+\n+static void maybeUnlockUUID(UUID uuid)\n+{\n+    if (uuid == UUIDHelpers::Nil)\n+        return;\n+\n+    chassert(DatabaseCatalog::instance().hasUUIDMapping(uuid));\n+    auto db_and_table = DatabaseCatalog::instance().tryGetByUUID(uuid);\n+    if (!db_and_table.first && !db_and_table.second)\n+    {\n+        DatabaseCatalog::instance().removeUUIDMappingFinally(uuid);\n+        return;\n+    }\n+    chassert(db_and_table.first || !db_and_table.second);\n+}\n+\n+TemporaryLockForUUIDDirectory::TemporaryLockForUUIDDirectory(UUID uuid_)\n+    : uuid(uuid_)\n+{\n+    if (uuid != UUIDHelpers::Nil)\n+        DatabaseCatalog::instance().addUUIDMapping(uuid);\n+}\n+\n+TemporaryLockForUUIDDirectory::~TemporaryLockForUUIDDirectory()\n+{\n+    maybeUnlockUUID(uuid);\n+}\n+\n+TemporaryLockForUUIDDirectory::TemporaryLockForUUIDDirectory(TemporaryLockForUUIDDirectory && rhs) noexcept\n+    : uuid(rhs.uuid)\n+{\n+    rhs.uuid = UUIDHelpers::Nil;\n+}\n+\n+TemporaryLockForUUIDDirectory & TemporaryLockForUUIDDirectory::operator = (TemporaryLockForUUIDDirectory && rhs) noexcept\n+{\n+    maybeUnlockUUID(uuid);\n+    uuid = rhs.uuid;\n+    rhs.uuid = UUIDHelpers::Nil;\n+    return *this;\n+}\n+\n \n DDLGuard::DDLGuard(Map & map_, std::shared_mutex & db_mutex_, std::unique_lock<std::mutex> guards_lock_, const String & elem, const String & database_name)\n         : map(map_), db_mutex(db_mutex_), guards_lock(std::move(guards_lock_))\ndiff --git a/src/Interpreters/DatabaseCatalog.h b/src/Interpreters/DatabaseCatalog.h\nindex 79ba4052038c..d82a0594c9a7 100644\n--- a/src/Interpreters/DatabaseCatalog.h\n+++ b/src/Interpreters/DatabaseCatalog.h\n@@ -20,7 +20,9 @@\n #include <shared_mutex>\n #include <unordered_map>\n #include <unordered_set>\n+#include <filesystem>\n \n+namespace fs = std::filesystem;\n \n namespace DB\n {\n@@ -203,6 +205,8 @@ class DatabaseCatalog : boost::noncopyable, WithMutableContext\n     /// this method will throw an exception.\n     void addUUIDMapping(const UUID & uuid);\n \n+    bool hasUUIDMapping(const UUID & uuid);\n+\n     static String getPathForUUID(const UUID & uuid);\n \n     DatabaseAndTable tryGetByUUID(const UUID & uuid) const;\n@@ -261,17 +265,17 @@ class DatabaseCatalog : boost::noncopyable, WithMutableContext\n     void dropTableDataTask();\n     void dropTableFinally(const TableMarkedAsDropped & table);\n \n+    void cleanupStoreDirectoryTask();\n+    bool maybeRemoveDirectory(const fs::path & unused_dir);\n+\n     static constexpr size_t reschedule_time_ms = 100;\n     static constexpr time_t drop_error_cooldown_sec = 5;\n \n-    using UUIDToDatabaseMap = std::unordered_map<UUID, DatabasePtr>;\n-\n     mutable std::mutex databases_mutex;\n \n     ViewDependencies view_dependencies;\n \n     Databases databases;\n-    UUIDToDatabaseMap db_uuid_map;\n     UUIDToStorageMap uuid_map;\n \n     DependenciesInfos loading_dependencies;\n@@ -298,6 +302,33 @@ class DatabaseCatalog : boost::noncopyable, WithMutableContext\n     static constexpr time_t default_drop_delay_sec = 8 * 60;\n     time_t drop_delay_sec = default_drop_delay_sec;\n     std::condition_variable wait_table_finally_dropped;\n+\n+    std::unique_ptr<BackgroundSchedulePoolTaskHolder> cleanup_task;\n+    static constexpr time_t default_unused_dir_hide_timeout_sec = 60 * 60;              /// 1 hour\n+    time_t unused_dir_hide_timeout_sec = default_unused_dir_hide_timeout_sec;\n+    static constexpr time_t default_unused_dir_rm_timeout_sec = 30 * 24 * 60 * 60;      /// 30 days\n+    time_t unused_dir_rm_timeout_sec = default_unused_dir_rm_timeout_sec;\n+    static constexpr time_t default_unused_dir_cleanup_period_sec = 24 * 60 * 60;       /// 1 day\n+    time_t unused_dir_cleanup_period_sec = default_unused_dir_cleanup_period_sec;\n+};\n+\n+/// This class is useful when creating a table or database.\n+/// Usually we create IStorage/IDatabase object first and then add it to IDatabase/DatabaseCatalog.\n+/// But such object may start using a directory in store/ since its creation.\n+/// To avoid race with cleanupStoreDirectoryTask() we have to mark UUID as used first.\n+/// Then we can either add DatabasePtr/StoragePtr to the created UUID mapping\n+/// or remove the lock if creation failed.\n+/// See also addUUIDMapping(...)\n+class TemporaryLockForUUIDDirectory : private boost::noncopyable\n+{\n+    UUID uuid = UUIDHelpers::Nil;\n+public:\n+    TemporaryLockForUUIDDirectory() = default;\n+    TemporaryLockForUUIDDirectory(UUID uuid_);\n+    ~TemporaryLockForUUIDDirectory();\n+\n+    TemporaryLockForUUIDDirectory(TemporaryLockForUUIDDirectory && rhs) noexcept;\n+    TemporaryLockForUUIDDirectory & operator = (TemporaryLockForUUIDDirectory && rhs) noexcept;\n };\n \n }\ndiff --git a/src/Interpreters/InterpreterCreateQuery.cpp b/src/Interpreters/InterpreterCreateQuery.cpp\nindex b29f7372d38e..75d00fcb8a75 100644\n--- a/src/Interpreters/InterpreterCreateQuery.cpp\n+++ b/src/Interpreters/InterpreterCreateQuery.cpp\n@@ -249,13 +249,22 @@ BlockIO InterpreterCreateQuery::createDatabase(ASTCreateQuery & create)\n                         \"Enable allow_experimental_database_materialized_postgresql to use it.\", ErrorCodes::UNKNOWN_DATABASE_ENGINE);\n     }\n \n+    bool need_write_metadata = !create.attach || !fs::exists(metadata_file_path);\n+    bool need_lock_uuid = internal || need_write_metadata;\n+\n+    /// Lock uuid, so we will known it's already in use.\n+    /// We do it when attaching databases on server startup (internal) and on CREATE query (!create.attach);\n+    TemporaryLockForUUIDDirectory uuid_lock;\n+    if (need_lock_uuid)\n+        uuid_lock = TemporaryLockForUUIDDirectory{create.uuid};\n+    else if (create.uuid != UUIDHelpers::Nil && !DatabaseCatalog::instance().hasUUIDMapping(create.uuid))\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot find UUID mapping for {}, it's a bug\", create.uuid);\n+\n     DatabasePtr database = DatabaseFactory::get(create, metadata_path / \"\", getContext());\n \n     if (create.uuid != UUIDHelpers::Nil)\n         create.setDatabase(TABLE_WITH_UUID_NAME_PLACEHOLDER);\n \n-    bool need_write_metadata = !create.attach || !fs::exists(metadata_file_path);\n-\n     if (need_write_metadata)\n     {\n         create.attach = true;\n@@ -1163,80 +1172,76 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)\n bool InterpreterCreateQuery::doCreateTable(ASTCreateQuery & create,\n                                            const InterpreterCreateQuery::TableProperties & properties)\n {\n+    if (create.temporary)\n+    {\n+        if (create.if_not_exists && getContext()->tryResolveStorageID({\"\", create.getTable()}, Context::ResolveExternal))\n+            return false;\n+\n+        String temporary_table_name = create.getTable();\n+        auto temporary_table = TemporaryTableHolder(getContext(), properties.columns, properties.constraints, query_ptr);\n+        getContext()->getSessionContext()->addExternalTable(temporary_table_name, std::move(temporary_table));\n+        return true;\n+    }\n+\n     std::unique_ptr<DDLGuard> guard;\n \n     String data_path;\n     DatabasePtr database;\n \n-    bool need_add_to_database = !create.temporary;\n-    if (need_add_to_database)\n-    {\n-        /** If the request specifies IF NOT EXISTS, we allow concurrent CREATE queries (which do nothing).\n-          * If table doesn't exist, one thread is creating table, while others wait in DDLGuard.\n-          */\n-        guard = DatabaseCatalog::instance().getDDLGuard(create.getDatabase(), create.getTable());\n+    /** If the request specifies IF NOT EXISTS, we allow concurrent CREATE queries (which do nothing).\n+      * If table doesn't exist, one thread is creating table, while others wait in DDLGuard.\n+      */\n+    guard = DatabaseCatalog::instance().getDDLGuard(create.getDatabase(), create.getTable());\n \n-        database = DatabaseCatalog::instance().getDatabase(create.getDatabase());\n-        assertOrSetUUID(create, database);\n+    database = DatabaseCatalog::instance().getDatabase(create.getDatabase());\n+    assertOrSetUUID(create, database);\n \n-        String storage_name = create.is_dictionary ? \"Dictionary\" : \"Table\";\n-        auto storage_already_exists_error_code = create.is_dictionary ? ErrorCodes::DICTIONARY_ALREADY_EXISTS : ErrorCodes::TABLE_ALREADY_EXISTS;\n+    String storage_name = create.is_dictionary ? \"Dictionary\" : \"Table\";\n+    auto storage_already_exists_error_code = create.is_dictionary ? ErrorCodes::DICTIONARY_ALREADY_EXISTS : ErrorCodes::TABLE_ALREADY_EXISTS;\n \n-        /// Table can be created before or it can be created concurrently in another thread, while we were waiting in DDLGuard.\n-        if (database->isTableExist(create.getTable(), getContext()))\n-        {\n-            /// TODO Check structure of table\n-            if (create.if_not_exists)\n-                return false;\n-            else if (create.replace_view)\n-            {\n-                /// when executing CREATE OR REPLACE VIEW, drop current existing view\n-                auto drop_ast = std::make_shared<ASTDropQuery>();\n-                drop_ast->setDatabase(create.getDatabase());\n-                drop_ast->setTable(create.getTable());\n-                drop_ast->no_ddl_lock = true;\n-\n-                auto drop_context = Context::createCopy(context);\n-                InterpreterDropQuery interpreter(drop_ast, drop_context);\n-                interpreter.execute();\n-            }\n-            else\n-                throw Exception(storage_already_exists_error_code,\n-                    \"{} {}.{} already exists\", storage_name, backQuoteIfNeed(create.getDatabase()), backQuoteIfNeed(create.getTable()));\n-        }\n-        else if (!create.attach)\n+    /// Table can be created before or it can be created concurrently in another thread, while we were waiting in DDLGuard.\n+    if (database->isTableExist(create.getTable(), getContext()))\n+    {\n+        /// TODO Check structure of table\n+        if (create.if_not_exists)\n+            return false;\n+        else if (create.replace_view)\n         {\n-            /// Checking that table may exists in detached/detached permanently state\n-            try\n-            {\n-                database->checkMetadataFilenameAvailability(create.getTable());\n-            }\n-            catch (const Exception &)\n-            {\n-                if (create.if_not_exists)\n-                    return false;\n-                throw;\n-            }\n+            /// when executing CREATE OR REPLACE VIEW, drop current existing view\n+            auto drop_ast = std::make_shared<ASTDropQuery>();\n+            drop_ast->setDatabase(create.getDatabase());\n+            drop_ast->setTable(create.getTable());\n+            drop_ast->no_ddl_lock = true;\n+\n+            auto drop_context = Context::createCopy(context);\n+            InterpreterDropQuery interpreter(drop_ast, drop_context);\n+            interpreter.execute();\n         }\n-\n-\n-        data_path = database->getTableDataPath(create);\n-\n-        if (!create.attach && !data_path.empty() && fs::exists(fs::path{getContext()->getPath()} / data_path))\n+        else\n             throw Exception(storage_already_exists_error_code,\n-                \"Directory for {} data {} already exists\", Poco::toLower(storage_name), String(data_path));\n+                \"{} {}.{} already exists\", storage_name, backQuoteIfNeed(create.getDatabase()), backQuoteIfNeed(create.getTable()));\n     }\n-    else\n+    else if (!create.attach)\n     {\n-        if (create.if_not_exists && getContext()->tryResolveStorageID({\"\", create.getTable()}, Context::ResolveExternal))\n-            return false;\n-\n-        String temporary_table_name = create.getTable();\n-        auto temporary_table = TemporaryTableHolder(getContext(), properties.columns, properties.constraints, query_ptr);\n-        getContext()->getSessionContext()->addExternalTable(temporary_table_name, std::move(temporary_table));\n-        return true;\n+        /// Checking that table may exists in detached/detached permanently state\n+        try\n+        {\n+            database->checkMetadataFilenameAvailability(create.getTable());\n+        }\n+        catch (const Exception &)\n+        {\n+            if (create.if_not_exists)\n+                return false;\n+            throw;\n+        }\n     }\n \n+    data_path = database->getTableDataPath(create);\n+\n+    if (!create.attach && !data_path.empty() && fs::exists(fs::path{getContext()->getPath()} / data_path))\n+        throw Exception(storage_already_exists_error_code,\n+            \"Directory for {} data {} already exists\", Poco::toLower(storage_name), String(data_path));\n+\n     bool from_path = create.attach_from_path.has_value();\n     String actual_data_path = data_path;\n     if (from_path)\n@@ -1261,6 +1266,19 @@ bool InterpreterCreateQuery::doCreateTable(ASTCreateQuery & create,\n             database->checkDetachedTableNotInUse(create.uuid);\n     }\n \n+    /// We should lock UUID on CREATE query (because for ATTACH it must be already locked previously).\n+    /// But ATTACH without create.attach_short_syntax flag works like CREATE actually, that's why we check it.\n+    bool need_lock_uuid = !create.attach_short_syntax;\n+    TemporaryLockForUUIDDirectory uuid_lock;\n+    if (need_lock_uuid)\n+        uuid_lock = TemporaryLockForUUIDDirectory{create.uuid};\n+    else if (create.uuid != UUIDHelpers::Nil && !DatabaseCatalog::instance().hasUUIDMapping(create.uuid))\n+    {\n+        /// FIXME MaterializedPostgreSQL works with UUIDs incorrectly and breaks invariants\n+        if (database->getEngineName() != \"MaterializedPostgreSQL\")\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot find UUID mapping for {}, it's a bug\", create.uuid);\n+    }\n+\n     StoragePtr res;\n     /// NOTE: CREATE query may be rewritten by Storage creator or table function\n     if (create.as_table_function)\ndiff --git a/src/Storages/System/attachSystemTablesImpl.h b/src/Storages/System/attachSystemTablesImpl.h\nindex fcc1ab43a649..a1fae985d920 100644\n--- a/src/Storages/System/attachSystemTablesImpl.h\n+++ b/src/Storages/System/attachSystemTablesImpl.h\n@@ -22,6 +22,7 @@ void attach(ContextPtr context, IDatabase & system_database, const String & tabl\n         /// NOTE: UUIDs are not persistent, but it's ok since no data are stored on disk for these storages\n         /// and path is actually not used\n         auto table_id = StorageID(DatabaseCatalog::SYSTEM_DATABASE, table_name, UUIDHelpers::generateV4());\n+        DatabaseCatalog::instance().addUUIDMapping(table_id.uuid);\n         String path = \"store/\" + DatabaseCatalog::getPathForUUID(table_id.uuid);\n         system_database.attachTable(context, table_name, std::make_shared<StorageT>(table_id, std::forward<StorageArgs>(args)...), path);\n     }\n",
  "test_patch": "diff --git a/tests/clickhouse-test b/tests/clickhouse-test\nindex 3e0d4e822b4d..75159053f26f 100755\n--- a/tests/clickhouse-test\n+++ b/tests/clickhouse-test\n@@ -2172,4 +2172,7 @@ if __name__ == \"__main__\":\n     if args.jobs is None:\n         args.jobs = multiprocessing.cpu_count()\n \n+    if args.db_engine and args.db_engine == \"Ordinary\":\n+        MESSAGES_TO_RETRY.append(\" locking attempt on \")\n+\n     main(args)\ndiff --git a/tests/config/config.d/database_atomic.xml b/tests/config/config.d/database_atomic.xml\nindex b3f51d51a79a..a551e710ca37 100644\n--- a/tests/config/config.d/database_atomic.xml\n+++ b/tests/config/config.d/database_atomic.xml\n@@ -1,3 +1,8 @@\n <clickhouse>\n     <database_atomic_delay_before_drop_table_sec>60</database_atomic_delay_before_drop_table_sec>\n+\n+    <!-- Aggressive cleanup for tests to catch more issues -->\n+    <database_catalog_unused_dir_hide_timeout_sec>0</database_catalog_unused_dir_hide_timeout_sec>\n+    <database_catalog_unused_dir_rm_timeout_sec>5</database_catalog_unused_dir_rm_timeout_sec>\n+    <database_catalog_unused_dir_cleanup_period_sec>10</database_catalog_unused_dir_cleanup_period_sec>\n </clickhouse>\ndiff --git a/tests/integration/test_broken_detached_part_clean_up/configs/store_cleanup.xml b/tests/integration/test_broken_detached_part_clean_up/configs/store_cleanup.xml\nnew file mode 100644\nindex 000000000000..3b0260dd07a3\n--- /dev/null\n+++ b/tests/integration/test_broken_detached_part_clean_up/configs/store_cleanup.xml\n@@ -0,0 +1,11 @@\n+<clickhouse>\n+    <database_catalog_unused_dir_hide_timeout_sec>0</database_catalog_unused_dir_hide_timeout_sec>\n+    <database_catalog_unused_dir_rm_timeout_sec>15</database_catalog_unused_dir_rm_timeout_sec>\n+    <database_catalog_unused_dir_cleanup_period_sec>1</database_catalog_unused_dir_cleanup_period_sec>\n+\n+    <!-- We don't really need [Zoo]Keeper for this test.\n+    And it makes sense to have at least one test with TestKeeper. -->\n+    <zookeeper>\n+        <implementation>testkeeper</implementation>\n+    </zookeeper>\n+</clickhouse>\n\\ No newline at end of file\ndiff --git a/tests/integration/test_broken_detached_part_clean_up/test.py b/tests/integration/test_broken_detached_part_clean_up/test.py\nindex 3d9134bdc547..167d10ec7d1b 100644\n--- a/tests/integration/test_broken_detached_part_clean_up/test.py\n+++ b/tests/integration/test_broken_detached_part_clean_up/test.py\n@@ -1,14 +1,15 @@\n import pytest\n \n from helpers.cluster import ClickHouseCluster\n-from multiprocessing.dummy import Pool\n from helpers.corrupt_part_data_on_disk import corrupt_part_data_on_disk\n from helpers.corrupt_part_data_on_disk import break_part\n import time\n \n cluster = ClickHouseCluster(__file__)\n \n-node1 = cluster.add_instance(\"node1\", stay_alive=True, with_zookeeper=True)\n+node1 = cluster.add_instance(\n+    \"node1\", stay_alive=True, main_configs=[\"configs/store_cleanup.xml\"]\n+)\n \n path_to_data = \"/var/lib/clickhouse/\"\n \n@@ -147,3 +148,181 @@ def test_remove_broken_detached_part_replicated_merge_tree(started_cluster):\n         )\n \n     remove_broken_detached_part_impl(\"replicated_mt\", node1, \"broken\")\n+\n+\n+def test_store_cleanup(started_cluster):\n+    node1.query(\"CREATE DATABASE db UUID '10000000-1000-4000-8000-000000000001'\")\n+    node1.query(\n+        \"CREATE TABLE db.log UUID '10000000-1000-4000-8000-000000000002' ENGINE=Log AS SELECT 1\"\n+    )\n+    node1.query(\n+        \"CREATE TABLE db.mt UUID '10000000-1000-4000-8000-000000000003' ENGINE=MergeTree ORDER BY tuple() AS SELECT 1\"\n+    )\n+    node1.query(\n+        \"CREATE TABLE db.mem UUID '10000000-1000-4000-8000-000000000004' ENGINE=Memory AS SELECT 1\"\n+    )\n+\n+    node1.query(\"CREATE DATABASE db2 UUID '20000000-1000-4000-8000-000000000001'\")\n+    node1.query(\n+        \"CREATE TABLE db2.log UUID '20000000-1000-4000-8000-000000000002' ENGINE=Log AS SELECT 1\"\n+    )\n+    node1.query(\"DETACH DATABASE db2\")\n+\n+    node1.query(\"CREATE DATABASE db3 UUID '30000000-1000-4000-8000-000000000001'\")\n+    node1.query(\n+        \"CREATE TABLE db3.log UUID '30000000-1000-4000-8000-000000000002' ENGINE=Log AS SELECT 1\"\n+    )\n+    node1.query(\n+        \"CREATE TABLE db3.log2 UUID '30000000-1000-4000-8000-000000000003' ENGINE=Log AS SELECT 1\"\n+    )\n+    node1.query(\"DETACH TABLE db3.log\")\n+    node1.query(\"DETACH TABLE db3.log2 PERMANENTLY\")\n+\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store\"]\n+    )\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/100\"]\n+    )\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/200\"]\n+    )\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/300\"]\n+    )\n+\n+    node1.stop_clickhouse(kill=True)\n+    # All dirs related to `db` will be removed\n+    node1.exec_in_container([\"rm\", f\"{path_to_data}/metadata/db.sql\"])\n+\n+    node1.exec_in_container([\"mkdir\", f\"{path_to_data}/store/kek\"])\n+    node1.exec_in_container([\"touch\", f\"{path_to_data}/store/12\"])\n+    node1.exec_in_container([\"mkdir\", f\"{path_to_data}/store/456\"])\n+    node1.exec_in_container([\"mkdir\", f\"{path_to_data}/store/456/testgarbage\"])\n+    node1.exec_in_container(\n+        [\"mkdir\", f\"{path_to_data}/store/456/30000000-1000-4000-8000-000000000003\"]\n+    )\n+    node1.exec_in_container(\n+        [\"touch\", f\"{path_to_data}/store/456/45600000-1000-4000-8000-000000000003\"]\n+    )\n+    node1.exec_in_container(\n+        [\"mkdir\", f\"{path_to_data}/store/456/45600000-1000-4000-8000-000000000004\"]\n+    )\n+\n+    node1.start_clickhouse()\n+    node1.query(\"DETACH DATABASE db2\")\n+    node1.query(\"DETACH TABLE db3.log\")\n+\n+    node1.wait_for_log_line(\"Removing access rights for unused directory\")\n+    time.sleep(1)\n+    node1.wait_for_log_line(\"testgarbage\")\n+    node1.wait_for_log_line(\"directories from store\")\n+\n+    store = node1.exec_in_container([\"ls\", f\"{path_to_data}/store\"])\n+    assert \"100\" in store\n+    assert \"200\" in store\n+    assert \"300\" in store\n+    assert \"456\" in store\n+    assert \"kek\" in store\n+    assert \"12\" in store\n+    assert \"d---------\" in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store\"]\n+    )\n+    assert \"d---------\" in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/456\"]\n+    )\n+\n+    # Metadata is removed, so store/100 contains garbage\n+    store100 = node1.exec_in_container([\"ls\", f\"{path_to_data}/store/100\"])\n+    assert \"10000000-1000-4000-8000-000000000001\" in store100\n+    assert \"10000000-1000-4000-8000-000000000002\" in store100\n+    assert \"10000000-1000-4000-8000-000000000003\" in store100\n+    assert \"d---------\" in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/100\"]\n+    )\n+\n+    # Database is detached, nothing to clean up\n+    store200 = node1.exec_in_container([\"ls\", f\"{path_to_data}/store/200\"])\n+    assert \"20000000-1000-4000-8000-000000000001\" in store200\n+    assert \"20000000-1000-4000-8000-000000000002\" in store200\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/200\"]\n+    )\n+\n+    # Tables are detached, nothing to clean up\n+    store300 = node1.exec_in_container([\"ls\", f\"{path_to_data}/store/300\"])\n+    assert \"30000000-1000-4000-8000-000000000001\" in store300\n+    assert \"30000000-1000-4000-8000-000000000002\" in store300\n+    assert \"30000000-1000-4000-8000-000000000003\" in store300\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/300\"]\n+    )\n+\n+    # Manually created garbage\n+    store456 = node1.exec_in_container([\"ls\", f\"{path_to_data}/store/456\"])\n+    assert \"30000000-1000-4000-8000-000000000003\" in store456\n+    assert \"45600000-1000-4000-8000-000000000003\" in store456\n+    assert \"45600000-1000-4000-8000-000000000004\" in store456\n+    assert \"testgarbage\" in store456\n+    assert \"----------\" in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/456\"]\n+    )\n+\n+    node1.wait_for_log_line(\"Removing unused directory\")\n+    time.sleep(1)\n+    node1.wait_for_log_line(\"directories from store\")\n+\n+    store = node1.exec_in_container([\"ls\", f\"{path_to_data}/store\"])\n+    assert \"100\" in store\n+    assert \"200\" in store\n+    assert \"300\" in store\n+    assert \"456\" in store\n+    assert \"kek\" not in store  # changed\n+    assert \"\\n12\\n\" not in store  # changed\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store\"]\n+    )  # changed\n+\n+    # Metadata is removed, so store/100 contains garbage\n+    store100 = node1.exec_in_container([\"ls\", f\"{path_to_data}/store/100\"])  # changed\n+    assert \"10000000-1000-4000-8000-000000000001\" not in store100  # changed\n+    assert \"10000000-1000-4000-8000-000000000002\" not in store100  # changed\n+    assert \"10000000-1000-4000-8000-000000000003\" not in store100  # changed\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/100\"]\n+    )  # changed\n+\n+    # Database is detached, nothing to clean up\n+    store200 = node1.exec_in_container([\"ls\", f\"{path_to_data}/store/200\"])\n+    assert \"20000000-1000-4000-8000-000000000001\" in store200\n+    assert \"20000000-1000-4000-8000-000000000002\" in store200\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/200\"]\n+    )\n+\n+    # Tables are detached, nothing to clean up\n+    store300 = node1.exec_in_container([\"ls\", f\"{path_to_data}/store/300\"])\n+    assert \"30000000-1000-4000-8000-000000000001\" in store300\n+    assert \"30000000-1000-4000-8000-000000000002\" in store300\n+    assert \"30000000-1000-4000-8000-000000000003\" in store300\n+    assert \"d---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/300\"]\n+    )\n+\n+    # Manually created garbage\n+    store456 = node1.exec_in_container([\"ls\", f\"{path_to_data}/store/456\"])\n+    assert \"30000000-1000-4000-8000-000000000003\" not in store456  # changed\n+    assert \"45600000-1000-4000-8000-000000000003\" not in store456  # changed\n+    assert \"45600000-1000-4000-8000-000000000004\" not in store456  # changed\n+    assert \"testgarbage\" not in store456  # changed\n+    assert \"---------\" not in node1.exec_in_container(\n+        [\"ls\", \"-l\", f\"{path_to_data}/store/456\"]\n+    )  # changed\n+\n+    node1.query(\"ATTACH TABLE db3.log2\")\n+    node1.query(\"ATTACH DATABASE db2\")\n+    node1.query(\"ATTACH TABLE db3.log\")\n+\n+    assert \"1\\n\" == node1.query(\"SELECT * FROM db3.log\")\n+    assert \"1\\n\" == node1.query(\"SELECT * FROM db3.log2\")\n+    assert \"1\\n\" == node1.query(\"SELECT * FROM db2.log\")\ndiff --git a/tests/queries/0_stateless/replication.lib b/tests/queries/0_stateless/replication.lib\nindex 6bf3c35f3441..fd32fa28ba00 100755\n--- a/tests/queries/0_stateless/replication.lib\n+++ b/tests/queries/0_stateless/replication.lib\n@@ -45,8 +45,8 @@ function check_replication_consistency()\n     while [[ $($CLICKHOUSE_CLIENT -q \"SELECT count() FROM system.processes WHERE current_database=currentDatabase() AND query LIKE '%$table_name_prefix%'\") -ne 1 ]]; do\n         sleep 0.5;\n         num_tries=$((num_tries+1))\n-        if [ $num_tries -eq 100 ]; then\n-            $CLICKHOUSE_CLIENT -q \"SELECT count() FROM system.processes WHERE current_database=currentDatabase() AND query LIKE '%$table_name_prefix%' FORMAT Vertical\"\n+        if [ $num_tries -eq 200 ]; then\n+            $CLICKHOUSE_CLIENT -q \"SELECT * FROM system.processes WHERE current_database=currentDatabase() AND query LIKE '%$table_name_prefix%' FORMAT Vertical\"\n             break\n         fi\n     done\ndiff --git a/tests/queries/shell_config.sh b/tests/queries/shell_config.sh\nindex 87c999c2032e..866fba506e49 100644\n--- a/tests/queries/shell_config.sh\n+++ b/tests/queries/shell_config.sh\n@@ -138,7 +138,7 @@ function wait_for_queries_to_finish()\n         sleep 0.5;\n         num_tries=$((num_tries+1))\n         if [ $num_tries -eq 20 ]; then\n-            $CLICKHOUSE_CLIENT -q \"SELECT count() FROM system.processes WHERE current_database=currentDatabase() AND query NOT LIKE '%system.processes%' FORMAT Vertical\"\n+            $CLICKHOUSE_CLIENT -q \"SELECT * FROM system.processes WHERE current_database=currentDatabase() AND query NOT LIKE '%system.processes%' FORMAT Vertical\"\n             break\n         fi\n     done\n",
  "problem_statement": "Garbage on FS after replicated merge tree table creation with invalid zk path\nGarbage on a filesystem after error with invalid zk path, you can't create a table with fixed path due to `Directory for table data data/test_db/test_table/ already exists.` error.\r\n\r\n> A clear and concise description of what works not as it is supposed to.\r\n\r\nI reproduced on 21.8.13.6 and 21.11.9.1\r\n\r\n[The list of releases](https://github.com/ClickHouse/ClickHouse/blob/master/utils/list-versions/version_date.tsv)\r\n\r\n**How to reproduce**\r\n\r\n1. Create Ordinary database(issue less significant on Atomic engine, because it generates new uid on retry)\r\n2. Create ReplicatedMergeTree table with invalid zk path(i.e. without leading /\r\n`CREATE TABLE test_db.test_table (id String) ENGINE = ReplicatedMergeTree('default.test_table', '{replica}') ORDER BY id SETTINGS index_granularity = 8192`, get error `DB::Exception: ZooKeeper path must starts with '/', got 'default.test_table'`\r\n4. Retry query(possibly with fixed zk path), get error `DB::Exception: Directory for table data data/test_db/test_table/ already exists.`\r\n\r\n**Expected behavior**\r\nThe failed table should not create garbage on fs.\n",
  "hints_text": "This issue has very low priority because it does not happen with Atomic databases, that are by default.\n> **Expected behavior**\r\n> The failed table should not create garbage on fs.\r\n\r\nApplicable for Atomic databases as well. It's just don't lead to errors \"table already exists\" on retry.\r\n\r\n\nwa (removal of garbage folder):\r\n```\r\nattach table test_table(s String) Engine=Log;\r\ndrop table test_table;\r\n```",
  "created_at": "2022-06-21T10:46:03Z",
  "modified_files": [
    "docs/en/operations/server-configuration-parameters/settings.md",
    "src/Access/DiskAccessStorage.cpp",
    "src/Common/filesystemHelpers.cpp",
    "src/Databases/DatabaseAtomic.cpp",
    "src/Databases/DatabaseOnDisk.cpp",
    "src/Databases/DatabaseOrdinary.cpp",
    "src/Databases/DatabaseOrdinary.h",
    "src/Databases/IDatabase.h",
    "src/Databases/PostgreSQL/DatabaseMaterializedPostgreSQL.cpp",
    "src/Databases/TablesLoader.cpp",
    "src/IO/ReadHelpers.h",
    "src/Interpreters/DatabaseCatalog.cpp",
    "src/Interpreters/DatabaseCatalog.h",
    "src/Interpreters/InterpreterCreateQuery.cpp",
    "src/Storages/System/attachSystemTablesImpl.h"
  ],
  "modified_test_files": [
    "tests/clickhouse-test",
    "tests/config/config.d/database_atomic.xml",
    "b/tests/integration/test_broken_detached_part_clean_up/configs/store_cleanup.xml",
    "tests/integration/test_broken_detached_part_clean_up/test.py",
    "tests/queries/0_stateless/replication.lib",
    "tests/queries/shell_config.sh"
  ]
}