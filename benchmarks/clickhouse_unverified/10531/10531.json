{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 10531,
  "instance_id": "ClickHouse__ClickHouse-10531",
  "issue_numbers": [
    "10487"
  ],
  "base_commit": "eee86c02dafbe1f10fa56397696572977a7f3264",
  "patch": "diff --git a/src/Storages/MergeTree/IMergeTreeDataPartWriter.cpp b/src/Storages/MergeTree/IMergeTreeDataPartWriter.cpp\nindex 8187799f4be9..597b5a94d8a5 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPartWriter.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPartWriter.cpp\n@@ -71,8 +71,7 @@ IMergeTreeDataPartWriter::IMergeTreeDataPartWriter(\n     const String & marks_file_extension_,\n     const CompressionCodecPtr & default_codec_,\n     const MergeTreeWriterSettings & settings_,\n-    const MergeTreeIndexGranularity & index_granularity_,\n-    bool need_finish_last_granule_)\n+    const MergeTreeIndexGranularity & index_granularity_)\n     : disk(std::move(disk_))\n     , part_path(part_path_)\n     , storage(storage_)\n@@ -84,7 +83,6 @@ IMergeTreeDataPartWriter::IMergeTreeDataPartWriter(\n     , settings(settings_)\n     , compute_granularity(index_granularity.empty())\n     , with_final_mark(storage.getSettings()->write_final_mark && settings.can_use_adaptive_granularity)\n-    , need_finish_last_granule(need_finish_last_granule_)\n {\n     if (settings.blocks_are_granules_size && !index_granularity.empty())\n         throw Exception(\"Can't take information about index granularity from blocks, when non empty index_granularity array specified\", ErrorCodes::LOGICAL_ERROR);\n@@ -95,15 +93,15 @@ IMergeTreeDataPartWriter::IMergeTreeDataPartWriter(\n \n IMergeTreeDataPartWriter::~IMergeTreeDataPartWriter() = default;\n \n-static void fillIndexGranularityImpl(\n+/// Implemetation is splitted into static functions for ability\n+/// of making unit tests without creation instance of IMergeTreeDataPartWriter,\n+/// which requires a lot of dependencies and access to filesystem.\n+static size_t computeIndexGranularityImpl(\n     const Block & block,\n     size_t index_granularity_bytes,\n     size_t fixed_index_granularity_rows,\n     bool blocks_are_granules,\n-    size_t index_offset,\n-    MergeTreeIndexGranularity & index_granularity,\n-    bool can_use_adaptive_index_granularity,\n-    bool need_finish_last_granule = false)\n+    bool can_use_adaptive_index_granularity)\n {\n     size_t rows_in_block = block.rows();\n     size_t index_granularity_for_block;\n@@ -130,43 +128,37 @@ static void fillIndexGranularityImpl(\n \n     /// We should be less or equal than fixed index granularity\n     index_granularity_for_block = std::min(fixed_index_granularity_rows, index_granularity_for_block);\n+    return index_granularity_for_block;\n+}\n \n-    size_t current_row;\n-    for (current_row = index_offset; current_row < rows_in_block; current_row += index_granularity_for_block)\n-    {\n-        size_t rows_left_in_block = rows_in_block - current_row;\n-\n-        /// Try to extend last granule if it's needed and block is large enough\n-        ///  or it shouldn't be first in granule (index_offset != 0).\n-        if (need_finish_last_granule && rows_left_in_block < index_granularity_for_block\n-            && (rows_in_block >= index_granularity_for_block || index_offset != 0))\n-        {\n-            // If enough rows are left, create a new granule. Otherwise, extend previous granule.\n-            // So, real size of granule differs from index_granularity_for_block not more than 50%.\n-            if (rows_left_in_block * 2 >= index_granularity_for_block)\n-                index_granularity.appendMark(rows_left_in_block);\n-            else\n-                index_granularity.addRowsToLastMark(rows_left_in_block);\n-        }\n-        else\n-        {\n-            index_granularity.appendMark(index_granularity_for_block);\n-        }\n-    }\n+static void fillIndexGranularityImpl(\n+    MergeTreeIndexGranularity & index_granularity,\n+    size_t index_offset,\n+    size_t index_granularity_for_block,\n+    size_t rows_in_block)\n+{\n+    for (size_t current_row = index_offset; current_row < rows_in_block; current_row += index_granularity_for_block)\n+        index_granularity.appendMark(index_granularity_for_block);\n }\n \n-void IMergeTreeDataPartWriter::fillIndexGranularity(const Block & block)\n+size_t IMergeTreeDataPartWriter::computeIndexGranularity(const Block & block)\n {\n     const auto storage_settings = storage.getSettings();\n+    return computeIndexGranularityImpl(\n+            block,\n+            storage_settings->index_granularity_bytes,\n+            storage_settings->index_granularity,\n+            settings.blocks_are_granules_size,\n+            settings.can_use_adaptive_granularity);\n+}\n+\n+void IMergeTreeDataPartWriter::fillIndexGranularity(size_t index_granularity_for_block, size_t rows_in_block)\n+{\n     fillIndexGranularityImpl(\n-        block,\n-        storage_settings->index_granularity_bytes,\n-        storage_settings->index_granularity,\n-        settings.blocks_are_granules_size,\n-        index_offset,\n         index_granularity,\n-        settings.can_use_adaptive_granularity,\n-        need_finish_last_granule);\n+        index_offset,\n+        index_granularity_for_block,\n+        rows_in_block);\n }\n \n void IMergeTreeDataPartWriter::initPrimaryIndex()\n@@ -225,21 +217,22 @@ void IMergeTreeDataPartWriter::calculateAndSerializePrimaryIndex(const Block & p\n \n     /// Write index. The index contains Primary Key value for each `index_granularity` row.\n \n-    for (size_t i = index_offset; i < rows;)\n+    size_t current_row = index_offset;\n+    size_t total_marks = index_granularity.getMarksCount();\n+\n+    while (index_mark < total_marks && current_row < rows)\n     {\n         if (storage.hasPrimaryKey())\n         {\n             for (size_t j = 0; j < primary_columns_num; ++j)\n             {\n                 const auto & primary_column = primary_index_block.getByPosition(j);\n-                index_columns[j]->insertFrom(*primary_column.column, i);\n-                primary_column.type->serializeBinary(*primary_column.column, i, *index_stream);\n+                index_columns[j]->insertFrom(*primary_column.column, current_row);\n+                primary_column.type->serializeBinary(*primary_column.column, current_row, *index_stream);\n             }\n         }\n \n-        i += index_granularity.getMarkRows(current_mark++);\n-        if (current_mark >= index_granularity.getMarksCount())\n-            break;\n+        current_row += index_granularity.getMarkRows(index_mark++);\n     }\n \n     /// store last index row to write final mark at the end of column\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\nindex 3e3496c88da4..c1cf127d7214 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h\n@@ -69,8 +69,7 @@ class IMergeTreeDataPartWriter : private boost::noncopyable\n         const String & marks_file_extension,\n         const CompressionCodecPtr & default_codec,\n         const MergeTreeWriterSettings & settings,\n-        const MergeTreeIndexGranularity & index_granularity,\n-        bool need_finish_last_granule);\n+        const MergeTreeIndexGranularity & index_granularity);\n \n     virtual ~IMergeTreeDataPartWriter();\n \n@@ -87,9 +86,6 @@ class IMergeTreeDataPartWriter : private boost::noncopyable\n     ///  calling calculations of primary and skip indices.\n     void next();\n \n-    /// Count index_granularity for block and store in `index_granularity`\n-    void fillIndexGranularity(const Block & block);\n-\n     const MergeTreeIndexGranularity & getIndexGranularity() const { return index_granularity; }\n \n     Columns releaseIndexColumns()\n@@ -112,6 +108,13 @@ class IMergeTreeDataPartWriter : private boost::noncopyable\n     void finishSkipIndicesSerialization(MergeTreeData::DataPart::Checksums & checksums);\n \n protected:\n+    /// Count index_granularity for block and store in `index_granularity`\n+    size_t computeIndexGranularity(const Block & block);\n+    virtual void fillIndexGranularity(size_t index_granularity_for_block, size_t rows_in_block);\n+\n+    size_t getCurrentMark() const { return current_mark; }\n+    size_t getIndexOffset() const { return index_offset; }\n+\n     using SerializationState = IDataType::SerializeBinaryBulkStatePtr;\n     using SerializationStates = std::unordered_map<String, SerializationState>;\n \n@@ -131,12 +134,6 @@ class IMergeTreeDataPartWriter : private boost::noncopyable\n \n     bool compute_granularity;\n     bool with_final_mark;\n-    bool need_finish_last_granule;\n-\n-    size_t current_mark = 0;\n-\n-    /// The offset to the first row of the block for which you want to write the index.\n-    size_t index_offset = 0;\n \n     size_t next_mark = 0;\n     size_t next_index_offset = 0;\n@@ -163,6 +160,14 @@ class IMergeTreeDataPartWriter : private boost::noncopyable\n \n     /// To correctly write Nested elements column-by-column.\n     WrittenOffsetColumns * written_offset_columns = nullptr;\n+\n+private:\n+    /// Data is already written up to this mark.\n+    size_t current_mark = 0;\n+    /// The offset to the first row of the block for which you want to write the index.\n+    size_t index_offset = 0;\n+    /// Index is already serialized up to this mark.\n+    size_t index_mark = 0;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\nindex e33d4a97cacd..2f708ac69545 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n@@ -18,7 +18,7 @@ MergeTreeDataPartWriterCompact::MergeTreeDataPartWriterCompact(\n : IMergeTreeDataPartWriter(disk_, part_path_,\n     storage_, columns_list_,\n     indices_to_recalc_, marks_file_extension_,\n-    default_codec_, settings_, index_granularity_, true)\n+    default_codec_, settings_, index_granularity_)\n {\n     using DataPart = MergeTreeDataPartCompact;\n     String data_file_name = DataPart::DATA_FILE_NAME;\n@@ -42,7 +42,10 @@ void MergeTreeDataPartWriterCompact::write(\n     /// if it's unknown (in case of insert data or horizontal merge,\n     /// but not in case of vertical merge)\n     if (compute_granularity)\n-        fillIndexGranularity(block);\n+    {\n+        size_t index_granularity_for_block = computeIndexGranularity(block);\n+        fillIndexGranularity(index_granularity_for_block, block.rows());\n+    }\n \n     Block result_block;\n \n@@ -88,7 +91,7 @@ void MergeTreeDataPartWriterCompact::write(\n void MergeTreeDataPartWriterCompact::writeBlock(const Block & block)\n {\n     size_t total_rows = block.rows();\n-    size_t from_mark = current_mark;\n+    size_t from_mark = getCurrentMark();\n     size_t current_row = 0;\n \n     while (current_row < total_rows)\n@@ -163,6 +166,44 @@ void MergeTreeDataPartWriterCompact::finishDataSerialization(IMergeTreeDataPart:\n     stream.reset();\n }\n \n+static void fillIndexGranularityImpl(\n+    MergeTreeIndexGranularity & index_granularity,\n+    size_t index_offset,\n+    size_t index_granularity_for_block,\n+    size_t rows_in_block)\n+{\n+    for (size_t current_row = index_offset; current_row < rows_in_block; current_row += index_granularity_for_block)\n+    {\n+        size_t rows_left_in_block = rows_in_block - current_row;\n+\n+        /// Try to extend last granule if block is large enough\n+        ///  or it isn't first in granule (index_offset != 0).\n+        if (rows_left_in_block < index_granularity_for_block &&\n+            (rows_in_block >= index_granularity_for_block || index_offset != 0))\n+        {\n+            // If enough rows are left, create a new granule. Otherwise, extend previous granule.\n+            // So, real size of granule differs from index_granularity_for_block not more than 50%.\n+            if (rows_left_in_block * 2 >= index_granularity_for_block)\n+                index_granularity.appendMark(rows_left_in_block);\n+            else\n+                index_granularity.addRowsToLastMark(rows_left_in_block);\n+        }\n+        else\n+        {\n+            index_granularity.appendMark(index_granularity_for_block);\n+        }\n+    }\n+}\n+\n+void MergeTreeDataPartWriterCompact::fillIndexGranularity(size_t index_granularity_for_block, size_t rows_in_block)\n+{\n+    fillIndexGranularityImpl(\n+        index_granularity,\n+        getIndexOffset(),\n+        index_granularity_for_block,\n+        rows_in_block);\n+}\n+\n void MergeTreeDataPartWriterCompact::ColumnsBuffer::add(MutableColumns && columns)\n {\n     if (accumulated_columns.empty())\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\nindex 0aff55588aaa..45d72d90b1e6 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n@@ -23,6 +23,9 @@ class MergeTreeDataPartWriterCompact : public IMergeTreeDataPartWriter\n \n     void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums) override;\n \n+protected:\n+    void fillIndexGranularity(size_t index_granularity_for_block, size_t rows_in_block) override;\n+\n private:\n     /// Write single granule of one column (rows between 2 marks)\n     void writeColumnSingleGranule(\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex 1e5640b4e23c..e5eececacfb8 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -24,7 +24,7 @@ MergeTreeDataPartWriterWide::MergeTreeDataPartWriterWide(\n     const MergeTreeIndexGranularity & index_granularity_)\n     : IMergeTreeDataPartWriter(disk_, part_path_,\n         storage_, columns_list_, indices_to_recalc_,\n-        marks_file_extension_, default_codec_, settings_, index_granularity_, false)\n+        marks_file_extension_, default_codec_, settings_, index_granularity_)\n {\n     const auto & columns = storage.getColumns();\n     for (const auto & it : columns_list)\n@@ -85,7 +85,10 @@ void MergeTreeDataPartWriterWide::write(const Block & block,\n     /// if it's unknown (in case of insert data or horizontal merge,\n     /// but not in case of vertical merge)\n     if (compute_granularity)\n-        fillIndexGranularity(block);\n+    {\n+        size_t index_granularity_for_block = computeIndexGranularity(block);\n+        fillIndexGranularity(index_granularity_for_block, block.rows());\n+    }\n \n     auto offset_columns = written_offset_columns ? *written_offset_columns : WrittenOffsetColumns{};\n \n@@ -206,17 +209,18 @@ void MergeTreeDataPartWriterWide::writeColumn(\n \n     size_t total_rows = column.size();\n     size_t current_row = 0;\n-    size_t current_column_mark = current_mark;\n+    size_t current_column_mark = getCurrentMark();\n+    size_t current_index_offset = getIndexOffset();\n     while (current_row < total_rows)\n     {\n         size_t rows_to_write;\n         bool write_marks = true;\n \n         /// If there is `index_offset`, then the first mark goes not immediately, but after this number of rows.\n-        if (current_row == 0 && index_offset != 0)\n+        if (current_row == 0 && current_index_offset != 0)\n         {\n             write_marks = false;\n-            rows_to_write = index_offset;\n+            rows_to_write = current_index_offset;\n         }\n         else\n         {\n",
  "test_patch": "diff --git a/src/Storages/tests/gtest_aux_funcs_for_adaptive_granularity.cpp b/src/Storages/tests/gtest_aux_funcs_for_adaptive_granularity.cpp\nindex 0579fd05f5d5..7488b6ea44a6 100644\n--- a/src/Storages/tests/gtest_aux_funcs_for_adaptive_granularity.cpp\n+++ b/src/Storages/tests/gtest_aux_funcs_for_adaptive_granularity.cpp\n@@ -25,14 +25,16 @@ TEST(AdaptiveIndexGranularity, FillGranularityToyTests)\n     EXPECT_EQ(block1.bytes(), 80);\n     { /// Granularity bytes are not set. Take default index_granularity.\n         MergeTreeIndexGranularity index_granularity;\n-        fillIndexGranularityImpl(block1, 0, 100, false, 0, index_granularity, false);\n+        auto granularity = computeIndexGranularityImpl(block1, 0, 100, false, false);\n+        fillIndexGranularityImpl(index_granularity, 0, granularity, block1.rows());\n         EXPECT_EQ(index_granularity.getMarksCount(), 1);\n         EXPECT_EQ(index_granularity.getMarkRows(0), 100);\n     }\n \n     { /// Granule size is less than block size. Block contains multiple granules.\n         MergeTreeIndexGranularity index_granularity;\n-        fillIndexGranularityImpl(block1, 16, 100, false, 0, index_granularity, true);\n+        auto granularity = computeIndexGranularityImpl(block1, 16, 100, false, true);\n+        fillIndexGranularityImpl(index_granularity, 0, granularity, block1.rows());\n         EXPECT_EQ(index_granularity.getMarksCount(), 5); /// First granule with 8 rows, and second with 1 row\n         for (size_t i = 0; i < index_granularity.getMarksCount(); ++i)\n             EXPECT_EQ(index_granularity.getMarkRows(i), 2);\n@@ -41,7 +43,8 @@ TEST(AdaptiveIndexGranularity, FillGranularityToyTests)\n     { /// Granule size is more than block size. Whole block (and maybe more) can be placed in single granule.\n \n         MergeTreeIndexGranularity index_granularity;\n-        fillIndexGranularityImpl(block1, 512, 100, false, 0, index_granularity, true);\n+        auto granularity = computeIndexGranularityImpl(block1, 512, 100, false, true);\n+        fillIndexGranularityImpl(index_granularity, 0, granularity, block1.rows());\n         EXPECT_EQ(index_granularity.getMarksCount(), 1);\n         for (size_t i = 0; i < index_granularity.getMarksCount(); ++i)\n             EXPECT_EQ(index_granularity.getMarkRows(i), 64);\n@@ -50,7 +53,8 @@ TEST(AdaptiveIndexGranularity, FillGranularityToyTests)\n     { /// Blocks with granule size\n \n         MergeTreeIndexGranularity index_granularity;\n-        fillIndexGranularityImpl(block1, 1, 100, true, 0, index_granularity, true);\n+        auto granularity = computeIndexGranularityImpl(block1, 1, 100, true, true);\n+        fillIndexGranularityImpl(index_granularity, 0, granularity, block1.rows());\n         EXPECT_EQ(index_granularity.getMarksCount(), 1);\n         for (size_t i = 0; i < index_granularity.getMarksCount(); ++i)\n             EXPECT_EQ(index_granularity.getMarkRows(i), block1.rows());\n@@ -58,7 +62,8 @@ TEST(AdaptiveIndexGranularity, FillGranularityToyTests)\n \n     { /// Shift in index offset\n         MergeTreeIndexGranularity index_granularity;\n-        fillIndexGranularityImpl(block1, 16, 100, false, 6, index_granularity, true);\n+        auto granularity = computeIndexGranularityImpl(block1, 16, 100, false, true);\n+        fillIndexGranularityImpl(index_granularity, 6, granularity, block1.rows());\n         EXPECT_EQ(index_granularity.getMarksCount(), 2);\n         for (size_t i = 0; i < index_granularity.getMarksCount(); ++i)\n             EXPECT_EQ(index_granularity.getMarkRows(i), 2);\n@@ -74,7 +79,10 @@ TEST(AdaptiveIndexGranularity, FillGranularitySequenceOfBlocks)\n         auto block3 = getBlockWithSize(65536, 8);\n         MergeTreeIndexGranularity index_granularity;\n         for (const auto & block : {block1, block2, block3})\n-            fillIndexGranularityImpl(block, 1024, 8192, false, 0, index_granularity, true);\n+        {\n+            auto granularity = computeIndexGranularityImpl(block, 1024, 8192, false, true);\n+            fillIndexGranularityImpl(index_granularity, 0, granularity, block.rows());\n+        }\n \n         EXPECT_EQ(index_granularity.getMarksCount(), 192); /// granules\n         for (size_t i = 0; i < index_granularity.getMarksCount(); ++i)\n@@ -87,7 +95,10 @@ TEST(AdaptiveIndexGranularity, FillGranularitySequenceOfBlocks)\n         EXPECT_EQ(block1.rows() + block2.rows() + block3.rows(), 3136);\n         MergeTreeIndexGranularity index_granularity;\n         for (const auto & block : {block1, block2, block3})\n-            fillIndexGranularityImpl(block, 1024, 8192, false, 0, index_granularity, true);\n+        {\n+            auto granularity = computeIndexGranularityImpl(block, 1024, 8192, false, true);\n+            fillIndexGranularityImpl(index_granularity, 0, granularity, block.rows());\n+        }\n \n         EXPECT_EQ(index_granularity.getMarksCount(), 98); /// granules\n         for (size_t i = 0; i < index_granularity.getMarksCount(); ++i)\n@@ -105,7 +116,8 @@ TEST(AdaptiveIndexGranularity, FillGranularitySequenceOfBlocks)\n         size_t index_offset = 0;\n         for (const auto & block : {block1, block2, block3})\n         {\n-            fillIndexGranularityImpl(block, 16384, 8192, false, index_offset, index_granularity, true);\n+            auto granularity = computeIndexGranularityImpl(block, 16384, 8192, false, true);\n+            fillIndexGranularityImpl(index_granularity, index_offset, granularity, block.rows());\n             index_offset = index_granularity.getLastMarkRows() - block.rows();\n         }\n         EXPECT_EQ(index_granularity.getMarksCount(), 1); /// granules\ndiff --git a/src/Storages/tests/gtest_aux_funcs_for_adaptive_granularity_compact_parts.cpp b/src/Storages/tests/gtest_aux_funcs_for_adaptive_granularity_compact_parts.cpp\nnew file mode 100644\nindex 000000000000..f87293dcd5d7\n--- /dev/null\n+++ b/src/Storages/tests/gtest_aux_funcs_for_adaptive_granularity_compact_parts.cpp\n@@ -0,0 +1,81 @@\n+#include <gtest/gtest.h>\n+#include <Core/Block.h>\n+#include <Columns/ColumnVector.h>\n+\n+// I know that inclusion of .cpp is not good at all\n+#include <Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp>\n+\n+using namespace DB;\n+\n+TEST(IndexGranularityCompactParts, FillGranularitySequenceOfBlocks)\n+{\n+    { /// Three blocks in one granule.\n+        size_t rows = 8;\n+        size_t granularity = 32;\n+\n+        MergeTreeIndexGranularity index_granularity;\n+        size_t index_offset = 0;\n+        size_t rows_written = 0;\n+        for (size_t i = 0; i < 3; ++i)\n+        {\n+            fillIndexGranularityImpl(index_granularity, index_offset, granularity, rows);\n+            rows_written += rows;\n+            index_offset = granularity - rows_written;\n+        }\n+\n+        EXPECT_EQ(index_granularity.getMarksCount(), 1); /// granules\n+        /// It's ok, that granularity is higher than actual number of row.\n+        /// It will be corrected in CompactWriter.\n+        EXPECT_EQ(index_granularity.getMarkRows(0), granularity);\n+    }\n+\n+    { /// Granule is extended with small block\n+        size_t rows1 = 30;\n+        size_t rows2 = 8;\n+        size_t granularity = 32;\n+\n+        MergeTreeIndexGranularity index_granularity;\n+        size_t index_offset = 0;\n+\n+        fillIndexGranularityImpl(index_granularity, index_offset, granularity, rows1);\n+        index_offset = granularity - rows1;\n+\n+        fillIndexGranularityImpl(index_granularity, index_offset, granularity, rows2);\n+\n+        EXPECT_EQ(index_granularity.getMarksCount(), 1);\n+        EXPECT_EQ(index_granularity.getMarkRows(0), rows1 + rows2);\n+    }\n+\n+    { /// New granule is created with large block;\n+        size_t rows1 = 30;\n+        size_t rows2 = 25;\n+        size_t granularity = 32;\n+\n+        MergeTreeIndexGranularity index_granularity;\n+        size_t index_offset = 0;\n+\n+        fillIndexGranularityImpl(index_granularity, index_offset, granularity, rows1);\n+        index_offset = granularity - rows1;\n+\n+        fillIndexGranularityImpl(index_granularity, index_offset, granularity, rows2);\n+\n+        EXPECT_EQ(index_granularity.getMarksCount(), 2);\n+        EXPECT_EQ(index_granularity.getMarkRows(0), granularity);\n+        EXPECT_EQ(index_granularity.getMarkRows(1), rows1 + rows2 - granularity);\n+    }\n+\n+     { /// Three large blocks\n+        size_t rows = 40;\n+        size_t granularity = 32;\n+\n+        MergeTreeIndexGranularity index_granularity;\n+        size_t index_offset = 0;\n+\n+        for (size_t i = 0; i < 3; ++i)\n+            fillIndexGranularityImpl(index_granularity, index_offset, granularity, rows);\n+\n+        EXPECT_EQ(index_granularity.getMarksCount(), 3);\n+        for (size_t i = 0; i < 3; ++i)\n+            EXPECT_EQ(index_granularity.getMarkRows(i), rows);\n+    }\n+}\ndiff --git a/tests/integration/test_polymorphic_parts/test.py b/tests/integration/test_polymorphic_parts/test.py\nindex f7256de9d9a4..ebb04f498761 100644\n--- a/tests/integration/test_polymorphic_parts/test.py\n+++ b/tests/integration/test_polymorphic_parts/test.py\n@@ -2,6 +2,8 @@\n import pytest\n import random\n import string\n+import os\n+import struct\n \n from helpers.test_tools import TSV\n from helpers.test_tools import assert_eq_with_retry\n@@ -260,3 +262,24 @@ def test_polymorphic_parts_non_adaptive(start_cluster):\n         \"WHERE table = 'non_adaptive_table' AND active GROUP BY part_type ORDER BY part_type\")) == TSV(\"Wide\\t2\\n\")\n \n     assert node1.contains_in_log(\"<Warning> default.non_adaptive_table: Table can't create parts with adaptive granularity\")\n+\n+\n+def test_polymorphic_parts_index(start_cluster):\n+    node1.query('''\n+        CREATE TABLE index_compact(a UInt32, s String) \n+        ENGINE = MergeTree ORDER BY a \n+        SETTINGS min_rows_for_wide_part = 1000, index_granularity = 128, merge_max_block_size = 100''')\n+\n+    node1.query(\"INSERT INTO index_compact SELECT number, toString(number) FROM numbers(100)\")\n+    node1.query(\"INSERT INTO index_compact SELECT number, toString(number) FROM numbers(30)\")\n+    node1.query(\"OPTIMIZE TABLE index_compact FINAL\")\n+\n+    assert node1.query(\"SELECT part_type FROM system.parts WHERE table = 'index_compact' AND active\") == \"Compact\\n\"\n+    assert node1.query(\"SELECT marks FROM system.parts WHERE table = 'index_compact' AND active\") == \"2\\n\"\n+\n+    index_path = os.path.join(node1.path, \"database/data/default/index_compact/all_1_2_1/primary.idx\")\n+    f = open(index_path, 'rb')\n+\n+    assert os.path.getsize(index_path) == 8\n+    assert struct.unpack('I', f.read(4))[0] == 0\n+    assert struct.unpack('I', f.read(4))[0] == 99\n",
  "problem_statement": "Couple of broken parts appear per day without any log indication\n**Describe the bug**\r\nParts are broken either due to upgrade or switching to compact mode. No logs associated with timestamp of those parts is available, they are only identified on startup of clickhouse (and if number of broken parts exceed limit, they block the table initialization). \r\n\r\n\r\n**How to reproduce**\r\nCurrently unclear, random parts are broken across days, and when clickhouse is restarted it gives error. \r\nTwo things that may have caused it:\r\n- System was upgraded from 19.17.6.36 to 20.3.7, then 20.3.8 (both had issue)\r\n- min_rows_for_wide_part was configured to 90000 (average insert size is 50k rows, maximum 100-150k rows)\r\n\r\n* 20.3.7, 20.3.8\r\n* Data loading is being made using clickhouse-client\r\n* min_rows_for_wide_part - but it's unclear that problem is due to that. It's worth noting that all broken parts were compact. However, there were many other compact parts that were not broken. \r\n* all of broken parts fail with same error: \"path/primary.idx is unexpectedly long\"\r\n* Not caused by any specific external event, no correlation to kernel or clickhouse logs. \r\n\r\n**Expected behavior**\r\nThere shall be no broken parts. \r\n\r\n**Error message and/or stacktrace**\r\n\r\ntable/schema has been anonimized. \r\n\r\n```\r\n\r\n2020.04.24 10:58:16.110142 [ 177089 ] {} <Error> DB::MergeTreeData::loadDataParts(bool)::<lambda()>: Code: 4, e.displayText() = DB::Exception: Index file /datadisk/data/SCHEMA/table_name/20200420-22-0_1084805_1084810_2/primary.idx is unexpectedly long, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x10542450 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x8f4272d in /usr/bin/clickhouse\r\n2. ? @ 0xd8cf004 in /usr/bin/clickhouse\r\n3. DB::IMergeTreeDataPart::loadColumnsChecksumsIndexes(bool, bool) @ 0xd8ca19d in /usr/bin/clickhouse\r\n4. ? @ 0xd8fbba2 in /usr/bin/clickhouse\r\n5. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x8f6792b in /usr/bin/clickhouse\r\n6. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const @ 0x8f68608 in /usr/bin/clickhouse\r\n7. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8f667eb in /usr/bin/clickhouse\r\n8. ? @ 0x8f64c33 in /usr/bin/clickhouse\r\n9. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n10. __clone @ 0xfe8cd in /usr/lib64/libc-2.17.so\r\n (version 20.3.8.53 (official build))\r\n2020.04.24 10:58:16.110191 [ 177089 ] {} <Error> SCHEMA.table_name: Part /datadisk/data/SCHEMA/table_name/20200420-22-0_1084805_1084810_2 is broken. Looking for parts to replace it.\r\n2020.04.24 10:58:16.111045 [ 177089 ] {} <Error> SCHEMA.table_name: Detaching broken part /datadisk/data/SCHEMA/table_name/20200420-22-0_1084805_1084810_2 because it covers less than 2 parts. You need to resolve this manually \r\n```\r\n\r\nAnonymized DDL of the table. \r\n[clickhouse-issue-10487.sql.txt](https://github.com/ClickHouse/ClickHouse/files/4531010/clickhouse-issue-10487.sql.txt)\r\n\r\n**Additional context**\r\nCurrently it's not so clear on how to reproduce the problem. \r\n\n",
  "hints_text": "",
  "created_at": "2020-04-27T13:45:43Z",
  "modified_files": [
    "src/Storages/MergeTree/IMergeTreeDataPartWriter.cpp",
    "src/Storages/MergeTree/IMergeTreeDataPartWriter.h",
    "src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h",
    "src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp"
  ],
  "modified_test_files": [
    "src/Storages/tests/gtest_aux_funcs_for_adaptive_granularity.cpp",
    "b/src/Storages/tests/gtest_aux_funcs_for_adaptive_granularity_compact_parts.cpp",
    "tests/integration/test_polymorphic_parts/test.py"
  ]
}