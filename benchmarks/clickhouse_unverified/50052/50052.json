{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 50052,
  "instance_id": "ClickHouse__ClickHouse-50052",
  "issue_numbers": [
    "49913",
    "46413"
  ],
  "base_commit": "0df785f8ba051b1554b6b7d5e003ee0a281730fa",
  "patch": "diff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\nindex 4f2ad823c3a9..f2e35e2dcd27 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\n@@ -130,12 +130,7 @@ void Service::processQuery(const HTMLForm & params, ReadBuffer & /*body*/, Write\n \n     auto report_broken_part = [&]()\n     {\n-        if (part && part->isProjectionPart())\n-        {\n-            auto parent_part = part->getParentPart()->shared_from_this();\n-            data.reportBrokenPart(parent_part);\n-        }\n-        else if (part)\n+        if (part)\n             data.reportBrokenPart(part);\n         else\n             LOG_TRACE(log, \"Part {} was not found, do not report it as broken\", part_name);\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex cf94e41a9929..b21f44baeb51 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -7420,8 +7420,14 @@ Strings MergeTreeData::getDataPaths() const\n }\n \n \n-void MergeTreeData::reportBrokenPart(MergeTreeData::DataPartPtr & data_part) const\n+void MergeTreeData::reportBrokenPart(MergeTreeData::DataPartPtr data_part) const\n {\n+    if (!data_part)\n+        return;\n+\n+    if (data_part->isProjectionPart())\n+        data_part = data_part->getParentPart()->shared_from_this();\n+\n     if (data_part->getDataPartStorage().isBroken())\n     {\n         auto parts = getDataPartsForInternalUsage();\n@@ -7433,7 +7439,7 @@ void MergeTreeData::reportBrokenPart(MergeTreeData::DataPartPtr & data_part) con\n                 broken_part_callback(part->name);\n         }\n     }\n-    else if (data_part && data_part->getState() == MergeTreeDataPartState::Active)\n+    else if (data_part->getState() == MergeTreeDataPartState::Active)\n         broken_part_callback(data_part->name);\n     else\n         LOG_DEBUG(log, \"Will not check potentially broken part {} because it's not active\", data_part->getNameWithState());\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 04b008b623c9..5488ce726313 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -718,7 +718,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// Should be called if part data is suspected to be corrupted.\n     /// Has the ability to check all other parts\n     /// which reside on the same disk of the suspicious part.\n-    void reportBrokenPart(MergeTreeData::DataPartPtr & data_part) const;\n+    void reportBrokenPart(MergeTreeData::DataPartPtr data_part) const;\n \n     /// TODO (alesap) Duplicate method required for compatibility.\n     /// Must be removed.\n",
  "test_patch": "diff --git a/tests/integration/test_projection_report_broken_part/__init__.py b/tests/integration/test_projection_report_broken_part/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_projection_report_broken_part/configs/testkeeper.xml b/tests/integration/test_projection_report_broken_part/configs/testkeeper.xml\nnew file mode 100644\nindex 000000000000..617371b13fa3\n--- /dev/null\n+++ b/tests/integration/test_projection_report_broken_part/configs/testkeeper.xml\n@@ -0,0 +1,6 @@\n+<clickhouse>\n+    <zookeeper>\n+        <!-- Don't need real [Zoo]Keeper for this test -->\n+        <implementation>testkeeper</implementation>\n+    </zookeeper>\n+</clickhouse>\ndiff --git a/tests/integration/test_projection_report_broken_part/test.py b/tests/integration/test_projection_report_broken_part/test.py\nnew file mode 100644\nindex 000000000000..f376adf4f1a8\n--- /dev/null\n+++ b/tests/integration/test_projection_report_broken_part/test.py\n@@ -0,0 +1,65 @@\n+# pylint: disable=unused-argument\n+# pylint: disable=redefined-outer-name\n+# pylint: disable=line-too-long\n+\n+import pytest\n+import time\n+\n+from helpers.client import QueryRuntimeException\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node = cluster.add_instance(\n+    \"node\",\n+    main_configs=[\n+        \"configs/testkeeper.xml\",\n+    ],\n+)\n+\n+\n+@pytest.fixture(scope=\"module\", autouse=True)\n+def start_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_projection_broken_part():\n+    node.query(\n+        \"\"\"\n+        create table test_projection_broken_parts_1 (a int, b int, projection ab (select a, sum(b) group by a))\n+        engine = ReplicatedMergeTree('/clickhouse-tables/test_projection_broken_parts', 'r1')\n+        order by a settings index_granularity = 1;\n+\n+        create table test_projection_broken_parts_2 (a int, b int, projection ab (select a, sum(b) group by a))\n+        engine ReplicatedMergeTree('/clickhouse-tables/test_projection_broken_parts', 'r2')\n+        order by a settings index_granularity = 1;\n+\n+        insert into test_projection_broken_parts_1 values (1, 1), (1, 2), (1, 3);\n+\n+        system sync replica test_projection_broken_parts_2;\n+    \"\"\"\n+    )\n+\n+    # break projection part\n+    node.exec_in_container(\n+        [\n+            \"bash\",\n+            \"-c\",\n+            \"rm /var/lib/clickhouse/data/default/test_projection_broken_parts_1/all_0_0_0/ab.proj/data.bin\",\n+        ]\n+    )\n+\n+    expected_error = \"No such file or directory\"\n+    assert expected_error in node.query_and_get_error(\n+        \"select sum(b) from test_projection_broken_parts_1 group by a\"\n+    )\n+\n+    time.sleep(2)\n+\n+    assert (\n+        int(node.query(\"select sum(b) from test_projection_broken_parts_1 group by a\"))\n+        == 6\n+    )\n",
  "problem_statement": "Unexpected part name: <projection name>\n**INPUTS**\r\n- `ReplicatedMergeTree` table projections (works fine)\r\n- CH ver `23.1.6.42`\r\n- system.mutations for `MATERIALIZE projection` has `is_done` equals to `0` for unexists parts\r\n\r\n**Unexpected behaviour**\r\nHanging MATERIALIZE projection MUTATION in system.mutations for unexists parts\r\n\r\n**Expected behavior**\r\n0 system.mutations. \r\n\r\n**Additional info**\r\n\r\n1. create particular table projections\r\n2. MATERIALIZE them\r\n3. wait a bit and check system.mutations\r\n```sql\r\nselect mutation_id, command, is_done, parts_to_do, parts_to_do_names, latest_fail_reason from system.mutations where is_done=0\r\n\u250c\u2500mutation_id\u2500\u252c\u2500command\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500is_done\u2500\u252c\u2500parts_to_do\u2500\u252c\u2500parts_to_do_names\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500latest_fail_reason\u2500\u2510\r\n\u2502 0000000005  \u2502 MATERIALIZE PROJECTION device_agregation          \u2502       0 \u2502           2 \u2502 ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'] \u2502                    \u2502\r\n\u2502 0000000006  \u2502 MATERIALIZE PROJECTION users_agregation           \u2502       0 \u2502           2 \u2502 ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'] \u2502                    \u2502\r\n\u2502 0000000007  \u2502 MATERIALIZE PROJECTION messages_agregation        \u2502       0 \u2502           2 \u2502 ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'] \u2502                    \u2502\r\n\u2502 0000000008  \u2502 MATERIALIZE PROJECTION overlay_session_agregation \u2502       0 \u2502           2 \u2502 ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'] \u2502                    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n4. check CH tasks/processes\r\n```sql\r\nSELECT * FROM system.processes WHERE query not like 'SELECT%' LIMIT 10 FORMAT Vertical\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.003 sec. \r\n```\r\n5.  get part info for stucked parts\r\n```sql\r\nselect partition, name, marks, rows, modification_time from system.parts where name IN('b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1')\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.010 sec. Processed 3.64 thousand rows, 324.88 KB (357.75 thousand rows/s., 31.97 MB/s.)\r\n```\r\n6.  check if there any parts in this partition\r\n```sql\r\nselect partition, name, marks, rows, modification_time from system.parts where partition_id IN('b0d8617590929340bbb5fb58cf52bce4', 'b9186c325b4cfa4ca55dcba0e78fd4a3')\r\n\u250c\u2500partition\u2500\u2500\u252c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500marks\u2500\u252c\u2500\u2500\u2500rows\u2500\u252c\u2500\u2500\u2500modification_time\u2500\u2510\r\n\u2502 20230410PM \u2502 b0d8617590929340bbb5fb58cf52bce4_0_11_2_75       \u2502    75 \u2502 592459 \u2502 2023-05-16 09:50:37 \u2502\r\n\u2502 20230410PM \u2502 b0d8617590929340bbb5fb58cf52bce4_12_17_1_75      \u2502    52 \u2502 409492 \u2502 2023-05-16 09:50:37 \u2502\r\n\u2502 20230410PM \u2502 b0d8617590929340bbb5fb58cf52bce4_18_59_7_75      \u2502    70 \u2502 561446 \u2502 2023-05-16 09:50:37 \u2502\r\n\u2502 20230410PM \u2502 b0d8617590929340bbb5fb58cf52bce4_60_64_1         \u2502    32 \u2502 250081 \u2502 2023-05-11 14:17:26 \u2502\r\n\u2502 20230410PM \u2502 b0d8617590929340bbb5fb58cf52bce4_65_65_0         \u2502     2 \u2502    377 \u2502 2023-05-11 14:17:25 \u2502\r\n\u2502 20230410PM \u2502 b0d8617590929340bbb5fb58cf52bce4_66_66_0_75      \u2502     2 \u2502    159 \u2502 2023-05-16 09:50:37 \u2502\r\n\u2502 20230512AM \u2502 b9186c325b4cfa4ca55dcba0e78fd4a3_0_0_0           \u2502     2 \u2502   7950 \u2502 2023-05-12 06:59:15 \u2502\r\n\u2502 20230512AM \u2502 b9186c325b4cfa4ca55dcba0e78fd4a3_1_1_0           \u2502     2 \u2502      2 \u2502 2023-05-12 07:19:39 \u2502\r\n\u2502 20230512AM \u2502 b9186c325b4cfa4ca55dcba0e78fd4a3_2_2_0           \u2502     2 \u2502      3 \u2502 2023-05-12 07:19:50 \u2502\r\n\u2502 20230512AM \u2502 b9186c325b4cfa4ca55dcba0e78fd4a3_3_3_0           \u2502     2 \u2502      1 \u2502 2023-05-12 07:19:50 \u2502\r\n\u2502 20230512AM \u2502 b9186c325b4cfa4ca55dcba0e78fd4a3_4_4_0           \u2502     2 \u2502      3 \u2502 2023-05-12 07:20:00 \u2502\r\n\u2502 20230512AM \u2502 b9186c325b4cfa4ca55dcba0e78fd4a3_5_5_0           \u2502     2 \u2502      2 \u2502 2023-05-12 07:20:04 \u2502\r\n\u2502 20230512AM \u2502 b9186c325b4cfa4ca55dcba0e78fd4a3_6_3855_763_3860 \u2502     2 \u2502   7493 \u2502 2023-05-16 09:50:40 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n7. materialize device_agregation projection once again and grab some parts to check whether they are exists or not\r\n```sql\r\nALTER TABLE analytics_local MATERIALIZE projection device_agregation\r\nselect parts_to_do, parts_to_do_names from system.mutations where is_done=0\r\n\r\n-- 1864, '1e8ea56ef92907db5039c205765fda33_17_17_0_26','1ebc42839fc25ba80c59f25c930876bd_0_0_0_9','1eeded7867995ae3bf35178ebae6a49a_0_0_0_9',...\r\n\r\nselect partition, name, marks, rows, modification_time from system.parts where name IN('1e8ea56ef92907db5039c205765fda33_17_17_0_26','1ebc42839fc25ba80c59f25c930876bd_0_0_0_9')\r\n\r\n\u250c\u2500partition\u2500\u2500\u252c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500marks\u2500\u252c\u2500rows\u2500\u252c\u2500\u2500\u2500modification_time\u2500\u2510\r\n\u2502 20220307AM \u2502 1e8ea56ef92907db5039c205765fda33_17_17_0_26 \u2502     2 \u2502   10 \u2502 2023-05-16 09:49:30 \u2502\r\n\u2502 20210714AM \u2502 1ebc42839fc25ba80c59f25c930876bd_0_0_0_9    \u2502     2 \u2502 1830 \u2502 2023-05-16 09:49:30 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n8. wait a bit and check `system.mutations`\r\n```sql\r\nselect mutation_id, command, is_done, parts_to_do, parts_to_do_names, latest_fail_reason from system.mutations where is_done=0\r\n\r\n\u250c\u2500mutation_id\u2500\u252c\u2500command\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500is_done\u2500\u252c\u2500parts_to_do\u2500\u252c\u2500parts_to_do_names\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500latest_fail_reason\u2500\u2510\r\n\u2502 0000000005  \u2502 MATERIALIZE PROJECTION device_agregation          \u2502       0 \u2502           2 \u2502 ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'] \u2502                    \u2502\r\n\u2502 0000000006  \u2502 MATERIALIZE PROJECTION users_agregation           \u2502       0 \u2502           2 \u2502 ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'] \u2502                    \u2502\r\n\u2502 0000000007  \u2502 MATERIALIZE PROJECTION messages_agregation        \u2502       0 \u2502           2 \u2502 ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'] \u2502                    \u2502\r\n\u2502 0000000008  \u2502 MATERIALIZE PROJECTION overlay_session_agregation \u2502       0 \u2502           2 \u2502 ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'] \u2502                    \u2502\r\n\u2502 0000000009  \u2502 MATERIALIZE PROJECTION device_agregation          \u2502       0 \u2502           2 \u2502 ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'] \u2502                    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```sql\r\nSELECT version()\r\n\u250c\u2500version()\u2500\u2510\r\n\u2502 23.1.6.42 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n** Work Around ** \r\nAs we dont have any procs/tasks related to particular mutations we can just kill mutations\r\n```sql\r\nKILL MUTATION ON CLUSTER '{cluster}' WHERE mutation_id IN(select mutation_id from system.mutations where is_done=0)\r\n```\nSELECT queries using a projection raise a `BAD_DATA_PART_NAME` error\n**Describe what's wrong**\r\n\r\nWe have implemented a projection using the `GROUP BY` clause.\r\n\r\nQueries activating the projection have been running without fault for months. But since a few weeks ago, sometimes we get a `BAD_DATA_PART_NAME` exception.\r\n\r\nThis has been the case for various queries that use the projection. \r\n\r\n**Does it reproduce on recent release?**\r\n\r\nUnfortunately, I can't test on the latest. The CH version used is `22.12.3.5` \r\n\r\n**How to reproduce**\r\n\r\nWe found it really hard to reproduce. If we run an identical query 10 times, it only breaks 1 time.\r\n\r\n**Expected behavior**\r\n\r\nQueries using the projection always run.\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\nDB::Exception: Unexpected part name: my_projection: While executing MergeTreeThread. (BAD_DATA_PART_NAME), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked const&, int, bool) @ 0xe750cda in /usr/bin/clickhouse\r\n1. ? @ 0x82ec0c0 in /usr/bin/clickhouse\r\n2. DB::MergeTreePartInfo::fromPartName(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, StrongTypedef<unsigned int, DB::MergeTreeDataFormatVersionTag>) @ 0x14f4b32a in /usr/bin/clickhouse\r\n3. DB::StorageReplicatedMergeTree::enqueuePartForCheck(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, long) @ 0x14a88c53 in /usr/bin/clickhouse\r\n4. DB::MergeTreeReaderCompact::MergeTreeReaderCompact(std::__1::shared_ptr<DB::IMergeTreeDataPartInfoForReader>, DB::NamesAndTypesList, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::UncompressedCache*, DB::MarkCache*, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange>>, DB::MergeTreeReaderSettings, ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>*, std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, double, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const, double>>>, std::__1::function<void (DB::ReadBufferFromFileBase::ProfileInfo)> const&, int) @ 0x14f5e04c in /usr/bin/clickhouse\r\n5. DB::MergeTreeDataPartCompact::getReader(DB::NamesAndTypesList const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange>> const&, DB::UncompressedCache*, DB::MarkCache*, DB::MergeTreeReaderSettings const&, std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, double, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const, double>>> const&, std::__1::function<void (DB::ReadBufferFromFileBase::ProfileInfo)> const&) const @ 0x14ea22d3 in /usr/bin/clickhouse\r\n6. DB::IMergeTreeSelectAlgorithm::initializeMergeTreeReadersForPart(std::__1::shared_ptr<DB::IMergeTreeDataPart const>&, DB::MergeTreeReadTaskColumns const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange>> const&, std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, double, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const, double>>> const&, std::__1::function<void (DB::ReadBufferFromFileBase::ProfileInfo)> const&) @ 0x15771c66 in /usr/bin/clickhouse\r\n7. DB::MergeTreeThreadSelectAlgorithm::finalizeNewTask() @ 0x1579e5b9 in /usr/bin/clickhouse\r\n8. DB::IMergeTreeSelectAlgorithm::read() @ 0x15770dfe in /usr/bin/clickhouse\r\n9. DB::MergeTreeSource::tryGenerate() @ 0x1579f7bc in /usr/bin/clickhouse\r\n10. DB::ISource::work() @ 0x153d3246 in /usr/bin/clickhouse\r\n11. DB::ExecutionThreadContext::executeTask() @ 0x153ee2a6 in /usr/bin/clickhouse\r\n12. DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) @ 0x153e349c in /usr/bin/clickhouse\r\n13. ? @ 0x153e55bd in /usr/bin/clickhouse\r\n14. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xe809f16 in /usr/bin/clickhouse\r\n15. ? @ 0xe80f0e1 in /usr/bin/clickhouse\r\n16. start_thread @ 0x74a4 in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n17. clone @ 0xe8d0f in /lib/x86_64-linux-gnu/libc-2.24.so\r\n```\r\n\r\n**Additional context**\r\n\r\nWe run a 4-node cluster. \r\n\n",
  "hints_text": "`select * from system.replication_queue where table='your_table'` would be really helpful, but it may not show anything since you killed the mutation.\r\n\r\n> parts_to_do_names\r\n> ['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1']\r\n\r\nThe mutation has to mutate two parts that are not ready yet. \r\n\r\n> system.parts\r\n> b0d8617590929340bbb5fb58cf52bce4_60_64_1\r\n> b0d8617590929340bbb5fb58cf52bce4_65_65_0\r\n\r\nThese parts are going to be merged into `b0d8617590929340bbb5fb58cf52bce4_60_65_2`\r\n\r\n> system.parts\r\n> b9186c325b4cfa4ca55dcba0e78fd4a3_0_0_0\r\n> b9186c325b4cfa4ca55dcba0e78fd4a3_2_2_0\r\n> ...\r\n> b9186c325b4cfa4ca55dcba0e78fd4a3_5_5_0\r\n\r\nAnd these parts are going to be merged into `b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1`\r\n\r\nAnd the mutation was waiting for these merges, and the root cause may be unrelated to mutations and projections at all. Please check `system.replication_queue`, you will find the reason if the merges are still there\nThanks. \r\nI've chacked `system.replication_queue` you mantioned. \r\nThe root cause is `Code: 233. BAD_DATA_PART_NAME` for all new upcoming parts in `parts_to_do_names` that been stucked in `system.mutations`:\r\n```\r\n['b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1']\r\n```\r\n\r\n```\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nreplica_name:           chi-clickhouse-analytics-0-0\r\nposition:               0\r\nnode_name:              queue-0000062230\r\ntype:                   MERGE_PARTS\r\ncreate_time:            2023-05-12 07:23:40\r\nrequired_quorum:        0\r\nnew_part_name:          b0d8617590929340bbb5fb58cf52bce4_60_65_2\r\nparts_to_merge:         ['b0d8617590929340bbb5fb58cf52bce4_60_64_1','b0d8617590929340bbb5fb58cf52bce4_65_65_0']\r\nis_detach:              0\r\nis_currently_executing: 1\r\nnum_tries:              2422595\r\nlast_exception:         Code: 233. DB::Exception: Unexpected part name: device_agregation for format version: 1. (BAD_DATA_PART_NAME) (version 23.1.6.42 (official build))\r\nlast_attempt_time:      2023-05-16 12:39:30\r\nnum_postponed:          0\r\npostpone_reason:        \r\nlast_postpone_time:     1970-01-01 00:00:00\r\nmerge_type:             Regular\r\n\r\nRow 2:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nreplica_name:           chi-clickhouse-analytics-0-0\r\nposition:               1\r\nnode_name:              queue-0000062167\r\ntype:                   MERGE_PARTS\r\ncreate_time:            2023-05-12 07:20:05\r\nrequired_quorum:        0\r\nnew_part_name:          b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1\r\nparts_to_merge:         ['b9186c325b4cfa4ca55dcba0e78fd4a3_0_0_0','b9186c325b4cfa4ca55dcba0e78fd4a3_1_1_0','b9186c325b4cfa4ca55dcba0e78fd4a3_2_2_0','b9186c325b4cfa4ca55dcba0e78fd4a3_3_3_0','b9186c325b4cfa4ca55dcba0e78fd4a3_4_4_0','b9186c325b4cfa4ca55dcba0e78fd4a3_5_5_0']\r\nis_detach:              0\r\nis_currently_executing: 0\r\nnum_tries:              4088672\r\nlast_exception:         Code: 233. DB::Exception: Unexpected part name: device_agregation for format version: 1. (BAD_DATA_PART_NAME) (version 23.1.6.42 (official build))\r\nlast_attempt_time:      2023-05-16 12:39:30\r\nnum_postponed:          0\r\npostpone_reason:        \r\nlast_postpone_time:     1970-01-01 00:00:00\r\nmerge_type:             Regular\r\n```\nThis exception looks weird. Please find the stacktrace:\r\n`grep -A50 -Fa \"Unexpected part name: device_agregation for format version\" /var/log/clickhouse-server/clickhouse-server.log`\n```\r\n2023.05.16 12:08:11.849732 [ 75 ] {analytics.analytics_local::b0d8617590929340bbb5fb58cf52bce4_60_65_2} <Debug> MergeTreeSequentialSource: Reading 2 marks from part device_agregation, total 1 rows starting from the beginning of the part\r\n2023.05.16 12:08:11.850132 [ 75 ] {} <Error> analytics.analytics_local::b0d8617590929340bbb5fb58cf52bce4_60_65_2 (MergeFromLogEntryTask): virtual bool DB::ReplicatedMergeMutateTaskBase::executeStep(): Code: 233. DB::Exception: Unexpected part name: device_agregation for format version: 1. (BAD_DATA_PART_NAME), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0xddd7ab5 in /usr/bin/clickhouse\r\n1. ? @ 0x1429466c in /usr/bin/clickhouse\r\n2. DB::MergeTreePartInfo::fromPartName(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, StrongTypedef<unsigned int, DB::MergeTreeDataFormatVersionTag>) @ 0x14294143 in /usr/bin/clickhouse\r\n3. DB::StorageReplicatedMergeTree::enqueuePartForCheck(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, long) @ 0x13d9345d in /usr/bin/clickhouse\r\n4. DB::MergeTreeReaderCompact::MergeTreeReaderCompact(std::__1::shared_ptr<DB::IMergeTreeDataPartInfoForReader>, DB::NamesAndTypesList, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::UncompressedCache*, DB::MarkCache*, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange>>, DB::MergeTreeReaderSettings, ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>*, std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, double, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const, double>>>, std::__1::function<void (DB::ReadBufferFromFileBase::ProfileInfo)> const&, int) @ 0x142a775a in /usr/bin/clickhouse\r\n5. DB::MergeTreeDataPartCompact::getReader(DB::NamesAndTypesList const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange>> const&, DB::UncompressedCache*, DB::MarkCache*, DB::MergeTreeReaderSettings const&, std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, double, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const, double>>> const&, std::__1::function<void (DB::ReadBufferFromFileBase::ProfileInfo)> const&) const @ 0x141d7813 in /usr/bin/clickhouse\r\n6. DB::MergeTreeSequentialSource::MergeTreeSequentialSource(DB::MergeTreeData const&, std::__1::shared_ptr<DB::StorageSnapshot> const&, std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>>, std::__1::optional<std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange>>>, bool, bool, bool, bool) @ 0x142ba7b4 in /usr/bin/clickhouse\r\n7. DB::createMergeTreeSequentialSource(DB::MergeTreeData const&, std::__1::shared_ptr<DB::StorageSnapshot> const&, std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>>, bool, bool, bool, std::__1::shared_ptr<std::__1::atomic<unsigned long>>) @ 0x142bb91b in /usr/bin/clickhouse\r\n8. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::createMergedStream() @ 0x14111c7e in /usr/bin/clickhouse\r\n9. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::prepare() @ 0x1410fba4 in /usr/bin/clickhouse\r\n10. bool std::__1::__function::__policy_invoker<bool ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::MergeTask::ExecuteAndFinalizeHorizontalPart::subtasks::'lambda'(), bool ()>>(std::__1::__function::__policy_storage const*) @ 0x1411d929 in /usr/bin/clickhouse\r\n11. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::execute() @ 0x14114aeb in /usr/bin/clickhouse\r\n12. DB::MergeTask::execute() @ 0x14119fda in /usr/bin/clickhouse\r\n13. bool std::__1::__function::__policy_invoker<bool ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::MergeTask::MergeProjectionsStage::subtasks::'lambda0'(), bool ()>>(std::__1::__function::__policy_storage const*) @ 0x1411dbed in /usr/bin/clickhouse\r\n14. DB::MergeTask::MergeProjectionsStage::execute() @ 0x1411a58b in /usr/bin/clickhouse\r\n15. DB::MergeTask::execute() @ 0x14119fda in /usr/bin/clickhouse\r\n16. DB::ReplicatedMergeMutateTaskBase::executeImpl() @ 0x1443d3ec in /usr/bin/clickhouse\r\n17. DB::ReplicatedMergeMutateTaskBase::executeStep() @ 0x1443c4e2 in /usr/bin/clickhouse\r\n18. DB::MergeTreeBackgroundExecutor<DB::MergeMutateRuntimeQueue>::routine(std::__1::shared_ptr<DB::TaskRuntimeData>) @ 0x883440c in /usr/bin/clickhouse\r\n19. DB::MergeTreeBackgroundExecutor<DB::MergeMutateRuntimeQueue>::threadFunction() @ 0x8833e4c in /usr/bin/clickhouse\r\n20. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__1::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0xdea8616 in /usr/bin/clickhouse\r\n21. void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::__1::function<void ()>, long, std::__1::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__1::__function::__policy_storage const*) @ 0xdeaaeb5 in /usr/bin/clickhouse\r\n22. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xdea4b36 in /usr/bin/clickhouse\r\n23. ? @ 0xdea9e61 in /usr/bin/clickhouse\r\n24. ? @ 0x7f9d141d8609 in ?\r\n25. clone @ 0x7f9d140fd133 in ?\r\n (version 23.1.6.42 (official build))\r\n2023.05.16 12:08:11.851325 [ 75 ] {} <Debug> MemoryTracker: Peak memory usage to apply mutate/merge in analytics.analytics_local::b0d8617590929340bbb5fb58cf52bce4_60_65_2: 83.05 MiB.\r\n```\r\n[Bad parts](https://gist.github.com/2b1q/2feadf41f950466caa1772946e6f4747/#bad-parts) additional debug info\nWell, it's a bug in projections. Related to https://github.com/ClickHouse/ClickHouse/issues/46413\r\n\r\ncc: @amosbird \r\n\r\nI don't know a good workaround, but it makes sense to drop these projections. If it does not help, then the table can be recovered manually like this:\r\n - detach affected partitions\r\n - remove all `*.proj` subdirs and `checksums.txt` files from the detached parts (`/path_to_table_data/detached/part_name/`)\r\n - attach partitions back\r\n\r\n\nI have some ideas on how to fix this problem. Will try to make a PR this week.\nSeems it works as should. \r\nwhat was done\r\n1. had dropped `device_agregation` projection which rises MERGE PARTS exceptions. And got new stucked mutation `DROP PROJECTION device_agregation`  with same parts `'b0d8617590929340bbb5fb58cf52bce4_60_65_2','b9186c325b4cfa4ca55dcba0e78fd4a3_0_5_1'`\r\n2. had dropped partitions with stucked MERGE PARTS in `system.replication_queue`\r\n```sql\r\nALTER TABLE <table> ON CLUSTER '{cluster}' DROP PARTITION '<partition_with_stucked_merge_parts>' \r\n-- checked system.replication_queue and system.mutations\r\n-- 0 rows\r\n```\r\n3. had restored data related to <partition_with_stucked_merge_parts> (copied from backuped table)\r\n4. had materialized the rest three projections I had in my table with new resored parts\r\n```sql\r\nALTER TABLE <t> ON CLUSTER '{cluster}' MATERIALIZE projection users_agregation \r\nOK (system.mutations: 0, system.replication_queue: 0)\r\n-//-//- MATERIALIZE projection messages_agregation\r\nOK (system.mutations: 0, system.replication_queue: 0)\r\n-//-//- MATERIALIZE projection overlay_session_agregation\r\nOK (system.mutations: 0, system.replication_queue: 0)\r\n```\r\n5. created `device_agregation` projection once again and materialized it\r\n6. have checked system.mutations, system.replication_queue some time later. Zero and Zero. \r\n7. had checked whether my query hit `device_agregation` projection\r\n```sql\r\nEXPLAIN json = 1, indexes = 1  <base select from t1 with device_agregation projection>\r\n[\r\n  {\r\n    \"Plan\": {\r\n      \"Node Type\": \"Expression\",\r\n      \"Description\": \"(Projection + Before ORDER BY)\",\r\n      \"Plans\": [\r\n        {\r\n          \"Node Type\": \"ReadFromStorage\",\r\n          \"Description\": \"MergeTree(with Aggregate projection device_agregation)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n] \r\n+ EXPLAIN ESTIMATE \r\n(SelectExecutor): Choose complete Aggregate projection device_agregation\r\n(SelectExecutor): projection required columns: eventId, organizationId, uniqExact(deviceId)\r\n(SelectExecutor): Selected 1890/1890 parts by partition key, 392 parts by primary key, 392/1890 marks by primary key, 392 marks to read from 392 ranges\r\n(SelectExecutor): Reading approx. 5270 rows with 16 streams\r\n```\r\n8. same as 7 from t2 (same data set and DDL excluding projections)\r\n```\r\n(SelectExecutor): Selected 6/896 parts by partition key, 6 parts by primary key, 57310/57310 marks by primary key, 57310 marks to read from 6 ranges\r\n(SelectExecutor): Reading approx. 466460237 rows with 16 streams\r\n```\nPlease, at least provide `show create table` and the query.\nApologies, here is the `show create table`:\r\n\r\n```sql\r\nCREATE TABLE shard.reporting_be_local\r\n(\r\n    `timestamp` DateTime64(3, 'UTC'),\r\n    `event` LowCardinality(String),\r\n    `event_type` LowCardinality(String),\r\n    `session_id` UInt64,\r\n    `session_start_ts` DateTime64(3, 'UTC') DEFAULT toDateTime64('1970-01-01', 3, 'UTC'),\r\n    `owner_key` LowCardinality(String),\r\n    `device_id` String,\r\n    `app_version` LowCardinality(String),\r\n    `account_id` String,\r\n    `profile_id` String,\r\n    `role` LowCardinality(String),\r\n    `is_free_trial` Int8,\r\n    `experiment_id` Nullable(String),\r\n    `experiment_group` LowCardinality(Nullable(String)),\r\n    `time_in_video` Nullable(UInt64),\r\n    `player_session_id` Nullable(Int64),\r\n    `player_type` LowCardinality(Nullable(String)),\r\n    `channel_id` Nullable(UInt64),\r\n    `channel_name` LowCardinality(Nullable(String)),\r\n    `channel_category` LowCardinality(Nullable(String)),\r\n    `channel_subcategory` LowCardinality(Nullable(String)),\r\n    `channel_updated_flag` Nullable(Bool),\r\n    `genre_filter` LowCardinality(Nullable(String)),\r\n    `decade_filter` LowCardinality(Nullable(String)),\r\n    `style_filter` LowCardinality(Nullable(String)),\r\n    `start_video_index` Nullable(UInt32),\r\n    `is_shuffle_start` Nullable(Bool),\r\n    `search_query` Nullable(String),\r\n    `search_result_size` Nullable(UInt32),\r\n    `video_id` Nullable(UInt32),\r\n    `video_type` LowCardinality(Nullable(String)),\r\n    `video_title` Nullable(String),\r\n    `video_artist` Nullable(String),\r\n    `video_is_on_demand` Nullable(Int8),\r\n    `video_distributor` Nullable(String),\r\n    `video_duration` Nullable(UInt64),\r\n    `video_isrc` Nullable(String),\r\n    `ad_pod_id` Nullable(UInt64),\r\n    `ad_id` Nullable(String),\r\n    `idfa` Nullable(String),\r\n    `device_info` Nullable(String),\r\n    `init_source` LowCardinality(Nullable(String)),\r\n    `init_url` String DEFAULT '',\r\n    `request_url` Nullable(String),\r\n    `consent_type` LowCardinality(Nullable(String)),\r\n    `is_consented` Nullable(Int8),\r\n    `recommender_name` LowCardinality(Nullable(String)) DEFAULT NULL,\r\n    `recommended_at` Nullable(DateTime64(3, 'UTC')) DEFAULT NULL,\r\n    `inserted_at` Nullable(DateTime64(3, 'UTC')) DEFAULT toDateTime(now(), 'UTC'),\r\n    `mat_country` LowCardinality(String) MATERIALIZED if(length(splitByChar('-', owner_key)) < 3, splitByChar('-', owner_key)[2], ''),\r\n    `mat_platform` LowCardinality(String) MATERIALIZED splitByChar('-', owner_key)[1],\r\n    `mat_video_artist_title` String MATERIALIZED concat(if(video_type = 'ad', '', coalesce(video_artist, 'NO_ARTIST')), if(video_type = 'ad', '', ' - '), coalesce(video_title, 'NO_TITLE')),\r\n    `mat_time_in_video_min` Nullable(Float32) MATERIALIZED time_in_video / 60000,\r\n    `mat_true_ad_duration` UInt64 MATERIALIZED multiIf((video_type = 'ad') AND (event = 'play') AND (video_duration IS NOT NULL), video_duration, (video_type = 'ad') AND (event != 'play') AND (time_in_video IS NOT NULL), time_in_video, 0),\r\n    `mat_user_type` LowCardinality(String) MATERIALIZED multiIf((role = 'role1') AND (NOT role2), 'SVOD', (role = 'role1') AND role2, 'role2', 'AVOD'),\r\n    `mat_rating` UInt8 MATERIALIZED multiIf(player_type = 'pt1', 5, player_type = 'pt2', 4, player_type = 'pt3', 3, player_type = 'pt4', 2, player_type = 'pt5', 1, 0),\r\n    `mat_insert_lag_sec` Nullable(Int64) MATERIALIZED if(inserted_at IS NULL, -1, dateDiff('second', timestamp, inserted_at, 'UTC')),\r\n    `mat_dl_type` LowCardinality(String) MATERIALIZED multiIf(match(init_url, '[/,]videoId[/:]'), 'video', init_source = 'deeplink', coalesce(nullIf(extract(init_url, '.type:(.*?),'), ''), nullIf(extract(init_url, '.playerType=(.*?)$'), ''), if(match(init_url, '/channelId/'), 'channel', NULL), 'NO_TYPE'), ''),\r\n    `mat_dl_channel_id` Nullable(UInt64) MATERIALIZED toUInt64OrNull(coalesce(nullIf(extract(init_url, '.,channelId:(.*?)(,|$)'), ''), nullIf(extract(init_url, '.channelId=(.*?)&'), ''), nullIf(extract(init_url, '.channelId/(.*?)(/|$)'), ''))),\r\n    `mat_dl_video_id` Nullable(UInt32) MATERIALIZED toUInt32OrNull(coalesce(nullIf(extract(init_url, '.,videoId:(.*?)$'), ''), nullIf(extract(init_url, './videoId/(.*)$'), ''))),\r\n    `ts_date` Date DEFAULT toDate(timestamp),\r\n    `session_start_date` Date DEFAULT toDate(session_start_ts),\r\n    `session_start_date_hour` DateTime64(3, 'UTC') DEFAULT toStartOfHour(session_start_ts),\r\n    `mat_device_model` String MATERIALIZED coalesce(nullIf(JSONExtractString(device_info, 'deviceModelId'), ''), nullIf(JSONExtractString(device_info, 'model'), ''), if(mat_platform = 'p1', splitByString(', ', JSONExtractString(coalesce(device_info, ''), 'userAgent'))[-2], 'UNKNOWN'), 'UNKNOWN'),\r\n    `is_first_experience` Nullable(Bool) DEFAULT false,\r\n    `is_first_session` Nullable(Bool),\r\n    INDEX skip_index_session_start_minmax session_start_ts TYPE minmax GRANULARITY 10,\r\n    INDEX skip_index_session_start_hour_minmax session_start_date_hour TYPE minmax GRANULARITY 10,\r\n    PROJECTION my_projection\r\n    (\r\n        SELECT\r\n            profile_id,\r\n            video_id,\r\n            owner_key,\r\n            video_type,\r\n            event_type,\r\n            channel_category,\r\n            channel_name,\r\n            max(mat_rating),\r\n            argMax(event, timestamp),\r\n            toDate(min(timestamp)),\r\n            toDate(max(timestamp)),\r\n            anyLast(mat_country)\r\n        GROUP BY\r\n            profile_id,\r\n            channel_category,\r\n            video_type,\r\n            event_type,\r\n            channel_name,\r\n            video_id,\r\n            owner_key\r\n    )\r\n)\r\nENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{cluster}/{shard}/reporting_be_local_tmp', '{replica}')\r\nPARTITION BY toYYYYMMDD(timestamp)\r\nORDER BY (owner_key, session_id, event, timestamp)\r\nTTL toDate(timestamp) + toIntervalMonth(6) TO DISK 'default'\r\nSETTINGS index_granularity = 8192, storage_policy = 'moving_from_hdd_to_sdd'\r\n```\r\n\r\nThe distributed table create query:\r\n```sql\r\nCREATE TABLE reporting.reporting_be ON CLUSTER '{cluster}' AS shard.reporting_be_local\r\nENGINE = Distributed('{cluster}', shard, reporting_be_local, murmurHash3_64(session_id));\r\n```\r\n\r\nA query that breaks:\r\n```sql\r\nSELECT \r\n    profile_id, \r\n    video_id, \r\n    toDateTime(MAX(timestamp)) as last_streamed_dt \r\nFROM reporting.reporting_be \r\nWHERE video_type = 'vt1' \r\n    AND profile_id IN ('p1', 'p2', 'p3', 'p4', 'p5') \r\n    AND owner_key NOT LIKE '%demo%' \r\n    AND isNotNull(video_id) \r\nGROUP BY profile_id, video_id\r\n```\r\n\r\nWhen running the query with clickhouse-client and `send_logs_level = 'trace'`, these are the last logs before the error is thrown:\r\n```\r\n2023.02.15 09:45:07.900821 [ 30725 ] {ddf7adac-48d7-459b-bad6-e7062c886605} <Trace> Aggregator: Converting aggregation data to two-level.\r\n2023.02.15 09:45:14.451212 [ 30909 ] {ddf7adac-48d7-459b-bad6-e7062c886605} <Trace> Aggregator: Converting aggregation data to two-level.\r\n```\r\n\r\nSome more things to note:\r\n* query failure was observed on both the local and the distributed table \r\n* we have tried re-materializing the projection, but it didn't help\r\n* another thing we thought was that the materialized columns were messing with the projection (even though they aren't used in the query), but we haven't tested this yet\r\n\r\n",
  "created_at": "2023-05-21T12:58:02Z",
  "modified_files": [
    "src/Storages/MergeTree/DataPartsExchange.cpp",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/MergeTree/MergeTreeData.h"
  ],
  "modified_test_files": [
    "b/tests/integration/test_projection_report_broken_part/configs/testkeeper.xml",
    "b/tests/integration/test_projection_report_broken_part/test.py"
  ]
}