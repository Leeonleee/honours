{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 79099,
  "instance_id": "ClickHouse__ClickHouse-79099",
  "issue_numbers": [
    "52035",
    "78791"
  ],
  "base_commit": "79902f2844869a57db8a4f04f8c1e1b2de1376ac",
  "patch": "diff --git a/src/Interpreters/Context.cpp b/src/Interpreters/Context.cpp\nindex b5d6cc5de03c..a77f421b00a8 100644\n--- a/src/Interpreters/Context.cpp\n+++ b/src/Interpreters/Context.cpp\n@@ -1663,12 +1663,13 @@ void Context::setCurrentRolesWithLock(const std::vector<UUID> & new_current_role\n \n void Context::setExternalRolesWithLock(const std::vector<UUID> & new_external_roles, const std::lock_guard<ContextSharedMutex> &)\n {\n+    // External roles are roles received from other node, current roles is a collection of roles that were assigned locally\n     if (!new_external_roles.empty())\n     {\n-        if (current_roles)\n-            current_roles->insert(current_roles->end(), new_external_roles.begin(), new_external_roles.end());\n+        if (external_roles)\n+            external_roles->insert(external_roles->end(), new_external_roles.begin(), new_external_roles.end());\n         else\n-            current_roles = std::make_shared<std::vector<UUID>>(new_external_roles);\n+            external_roles = std::make_shared<std::vector<UUID>>(new_external_roles);\n         need_recalculate_access = true;\n     }\n }\ndiff --git a/src/QueryPipeline/RemoteQueryExecutor.cpp b/src/QueryPipeline/RemoteQueryExecutor.cpp\nindex 03bad0650b62..69bc9e7f1436 100644\n--- a/src/QueryPipeline/RemoteQueryExecutor.cpp\n+++ b/src/QueryPipeline/RemoteQueryExecutor.cpp\n@@ -421,7 +421,7 @@ void RemoteQueryExecutor::sendQueryUnlocked(ClientInfo::QueryKind query_kind, As\n \n     // Collect all roles granted on this node and pass those to the remote node\n     std::vector<String> local_granted_roles;\n-    if (context->getSettingsRef()[Setting::push_external_roles_in_interserver_queries] && !modified_client_info.initial_user.empty())\n+    if (context->getSettingsRef()[Setting::push_external_roles_in_interserver_queries])\n     {\n         auto user = context->getAccessControl().read<User>(modified_client_info.initial_user, false);\n         boost::container::flat_set<String> granted_roles;\ndiff --git a/src/Server/TCPHandler.cpp b/src/Server/TCPHandler.cpp\nindex a6639fca7ae9..8f77f3795643 100644\n--- a/src/Server/TCPHandler.cpp\n+++ b/src/Server/TCPHandler.cpp\n@@ -2125,8 +2125,8 @@ void TCPHandler::processQuery(std::optional<QueryState> & state)\n         }\n         else\n         {\n-            // In a cluster, query originator may have an access to the external auth provider (like LDAP server),\n-            // that grants specific roles to the user. We want these roles to be granted to the user on other nodes of cluster when\n+            // In a cluster, query originator may have an access to the external auth provider with role mapping (like LDAP server),\n+            // that grants specific roles to the user. We want these roles to be granted to the effective user on other nodes of cluster when\n             // query is executed.\n             Strings external_roles;\n             if (!received_extra_roles.empty())\n",
  "test_patch": "diff --git a/tests/integration/test_ldap_external_user_directory/configs/ldap_no_role_mapping.xml b/tests/integration/test_ldap_external_user_directory/configs/ldap_no_role_mapping.xml\nnew file mode 100644\nindex 000000000000..3922e5877e95\n--- /dev/null\n+++ b/tests/integration/test_ldap_external_user_directory/configs/ldap_no_role_mapping.xml\n@@ -0,0 +1,15 @@\n+<clickhouse>\n+    <ldap_servers>\n+        <openldap>\n+            <host>openldap</host>\n+            <port>1389</port>\n+            <bind_dn>cn={user_name},ou=users,dc=example,dc=org</bind_dn>\n+            <enable_tls>no</enable_tls>\n+        </openldap>\n+    </ldap_servers>\n+    <user_directories>\n+        <ldap>\n+            <server>openldap</server>\n+        </ldap>\n+    </user_directories>\n+</clickhouse>\ndiff --git a/tests/integration/test_ldap_external_user_directory/configs/remote_servers.xml b/tests/integration/test_ldap_external_user_directory/configs/remote_servers.xml\nindex cf1bdf9dcb19..802c1f566776 100644\n--- a/tests/integration/test_ldap_external_user_directory/configs/remote_servers.xml\n+++ b/tests/integration/test_ldap_external_user_directory/configs/remote_servers.xml\n@@ -1,6 +1,8 @@\n <clickhouse>\n+    <interserver_http_port>9009</interserver_http_port>\n     <remote_servers>\n         <test_ldap_cluster>\n+            <secret>my_secret</secret>\n             <shard>\n                 <replica>\n                     <host>instance1</host>\ndiff --git a/tests/integration/test_ldap_external_user_directory/configs/users.xml b/tests/integration/test_ldap_external_user_directory/configs/users.xml\nnew file mode 100644\nindex 000000000000..e284afb67a6b\n--- /dev/null\n+++ b/tests/integration/test_ldap_external_user_directory/configs/users.xml\n@@ -0,0 +1,11 @@\n+<clickhouse>\n+    <users>\n+        <default remove=\"remove\">\n+        </default>\n+        <common_user>\n+            <password>qwerty</password>\n+            <access_management>1</access_management>\n+            <readonly>0</readonly>\n+        </common_user>\n+    </users>\n+</clickhouse>\n\\ No newline at end of file\ndiff --git a/tests/integration/test_ldap_external_user_directory/test.py b/tests/integration/test_ldap_external_user_directory/test.py\nindex 6b76466e6086..9ffdaf00f25b 100644\n--- a/tests/integration/test_ldap_external_user_directory/test.py\n+++ b/tests/integration/test_ldap_external_user_directory/test.py\n@@ -13,6 +13,7 @@\n instance1 = cluster.add_instance(\n     \"instance1\",\n     main_configs=[\"configs/ldap_with_role_mapping.xml\", \"configs/remote_servers.xml\"],\n+    user_configs=[\"configs/users.xml\"],\n     macros={\"shard\": 1, \"replica\": \"instance1\"},\n     stay_alive=True,\n     with_ldap=True,\n@@ -21,7 +22,8 @@\n \n instance2 = cluster.add_instance(\n     \"instance2\",\n-    main_configs=[\"configs/remote_servers.xml\"],\n+    main_configs=[\"configs/ldap_no_role_mapping.xml\", \"configs/remote_servers.xml\"],\n+    user_configs=[\"configs/users.xml\"],\n     macros={\"shard\": 1, \"replica\": \"instance2\"},\n     stay_alive=True,\n     with_zookeeper=True,\n@@ -87,6 +89,10 @@ def delete_ldap_group(ldap_cluster, group_cn):\n     assert code == 0\n \n \n+# NOTE: In this test suite we have default user explicitly disabled because of `test_push_role_to_other_nodes`.\n+# We do it to be sure that it is not used in interserver query (this user has very permissive privileges)\n+# and that external roles are really passed and applied.\n+\n def test_authentication_pass():\n     assert instance1.query(\n         \"SELECT currentUser()\", user=\"janedoe\", password=\"qwerty\"\n@@ -106,11 +112,11 @@ def test_authentication_fail():\n \n \n def test_role_mapping(ldap_cluster):\n-    instance1.query(\"DROP ROLE IF EXISTS role_1\")\n-    instance1.query(\"DROP ROLE IF EXISTS role_2\")\n-    instance1.query(\"DROP ROLE IF EXISTS role_3\")\n-    instance1.query(\"CREATE ROLE role_1\")\n-    instance1.query(\"CREATE ROLE role_2\")\n+    instance1.query(\"DROP ROLE IF EXISTS role_1\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"DROP ROLE IF EXISTS role_2\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"DROP ROLE IF EXISTS role_3\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"CREATE ROLE role_1\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"CREATE ROLE role_2\", user=\"common_user\", password=\"qwerty\")\n     add_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_1\", member_cn=\"johndoe\")\n     add_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_2\", member_cn=\"johndoe\")\n \n@@ -124,7 +130,7 @@ def test_role_mapping(ldap_cluster):\n         password=\"qwertz\",\n     ) == TSV([[\"role_1\"], [\"role_2\"]])\n \n-    instance1.query(\"CREATE ROLE role_3\")\n+    instance1.query(\"CREATE ROLE role_3\", user=\"common_user\", password=\"qwerty\")\n     add_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_3\", member_cn=\"johndoe\")\n     # Check that non-existing role in ClickHouse is ignored during role update\n     # See https://github.com/ClickHouse/ClickHouse/issues/54318\n@@ -136,9 +142,9 @@ def test_role_mapping(ldap_cluster):\n         password=\"qwertz\",\n     ) == TSV([[\"role_1\"], [\"role_2\"], [\"role_3\"]])\n \n-    instance1.query(\"DROP ROLE role_1\")\n-    instance1.query(\"DROP ROLE role_2\")\n-    instance1.query(\"DROP ROLE role_3\")\n+    instance1.query(\"DROP ROLE role_1\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"DROP ROLE role_2\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"DROP ROLE role_3\", user=\"common_user\", password=\"qwerty\")\n \n     delete_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_1\")\n     delete_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_2\")\n@@ -147,41 +153,58 @@ def test_role_mapping(ldap_cluster):\n \n \n def test_push_role_to_other_nodes(ldap_cluster):\n-    instance1.query(\"DROP TABLE IF EXISTS distributed_table SYNC\")\n-    instance1.query(\"DROP TABLE IF EXISTS local_table SYNC\")\n-    instance2.query(\"DROP TABLE IF EXISTS local_table SYNC\")\n-    instance1.query(\"DROP ROLE IF EXISTS role_read\")\n+    add_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_read\", member_cn=\"johndoe\")\n \n-    instance1.query(\"CREATE ROLE role_read\")\n-    instance1.query(\"GRANT SELECT ON *.* TO role_read\")\n+    instance2.query(\"DROP USER IF EXISTS remote_user\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"CREATE USER remote_user IDENTIFIED WITH plaintext_password BY 'qwerty'\", user=\"common_user\", password=\"qwerty\")\n \n-    add_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_read\", member_cn=\"johndoe\")\n+    instance1.query(\"DROP TABLE IF EXISTS distributed_table SYNC\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"DROP TABLE IF EXISTS local_table SYNC\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"DROP TABLE IF EXISTS local_table SYNC\", user=\"common_user\", password=\"qwerty\")\n+\n+    instance1.query(\"DROP ROLE IF EXISTS role_read\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"DROP ROLE IF EXISTS role_read\", user=\"common_user\", password=\"qwerty\")\n+\n+    # On both instances create a role and grant the SELECT privilege.\n+    instance1.query(\"CREATE ROLE role_read\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"GRANT SELECT ON *.* TO role_read\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"CREATE ROLE role_read\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"GRANT SELECT ON *.* TO role_read\", user=\"common_user\", password=\"qwerty\")\n \n+    # Verify that instance1 resolves johndoe correctly.\n     assert instance1.query(\n-        \"select currentUser()\", user=\"johndoe\", password=\"qwertz\"\n+        \"SELECT currentUser()\", user=\"johndoe\", password=\"qwertz\"\n     ) == TSV([[\"johndoe\"]])\n \n+    # Create the underlying table on both nodes.\n     instance1.query(\n-        \"CREATE TABLE IF NOT EXISTS local_table (id UInt32) ENGINE = MergeTree() ORDER BY id\"\n+        \"CREATE TABLE IF NOT EXISTS local_table (id UInt32) ENGINE = MergeTree() ORDER BY id\", user=\"common_user\", password=\"qwerty\"\n     )\n     instance2.query(\n-        \"CREATE TABLE IF NOT EXISTS local_table (id UInt32) ENGINE = MergeTree() ORDER BY id\"\n+        \"CREATE TABLE IF NOT EXISTS local_table (id UInt32) ENGINE = MergeTree() ORDER BY id\", user=\"common_user\", password=\"qwerty\"\n     )\n-    instance2.query(\"INSERT INTO local_table VALUES (1), (2), (3)\")\n+\n+    # Insert some test data, only on remote node.\n+    instance2.query(\"INSERT INTO local_table VALUES (1), (2), (3)\", user=\"common_user\", password=\"qwerty\")\n+\n+    # Create a Distributed table on instance1 that points to local_table.\n     instance1.query(\n-        \"CREATE TABLE IF NOT EXISTS distributed_table AS local_table ENGINE = Distributed(test_ldap_cluster, default, local_table)\"\n+        \"CREATE TABLE IF NOT EXISTS distributed_table AS local_table ENGINE = Distributed(test_ldap_cluster, default, local_table)\", user=\"common_user\", password=\"qwerty\"\n     )\n \n+    # Now, run the distributed query as johndoe.\n+    # The coordinator (instance1) will resolve johndoe's LDAP mapping,\n+    # and push the external role (role_read) to instance2.\n+    # Even though instance2 does not have role mapping, it shall honor the pushed role.\n     result = instance1.query(\n         \"SELECT sum(id) FROM distributed_table\", user=\"johndoe\", password=\"qwertz\"\n     )\n     assert result.strip() == \"6\"\n \n-    instance1.query(\"DROP TABLE IF EXISTS distributed_table SYNC\")\n-    instance1.query(\"DROP TABLE IF EXISTS local_table SYNC\")\n-    instance2.query(\"DROP TABLE IF EXISTS local_table SYNC\")\n-    instance2.query(\"DROP ROLE IF EXISTS role_read\")\n-\n+    instance1.query(\"DROP TABLE IF EXISTS distributed_table SYNC\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"DROP TABLE IF EXISTS local_table SYNC\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"DROP TABLE IF EXISTS local_table SYNC\", user=\"common_user\", password=\"qwerty\")\n+    instance1.query(\"DROP ROLE IF EXISTS role_read\", user=\"common_user\", password=\"qwerty\")\n     delete_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_read\")\n \n \n@@ -189,19 +212,20 @@ def test_remote_query_user_does_not_exist_locally(ldap_cluster):\n     \"\"\"\n     Check that even if user does not exist locally, using it to execute remote queries is still possible\n     \"\"\"\n-    instance2.query(\"DROP USER IF EXISTS non_local\")\n-    instance2.query(\"DROP TABLE IF EXISTS test_table sync\")\n+    instance2.query(\"DROP USER IF EXISTS non_local\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"DROP TABLE IF EXISTS test_table sync\", user=\"common_user\", password=\"qwerty\")\n \n-    instance2.query(\"CREATE USER non_local\")\n-    instance2.query(\"CREATE TABLE test_table (id Int16) ENGINE=Memory\")\n-    instance2.query(\"INSERT INTO test_table VALUES (123)\")\n-    instance2.query(\"GRANT SELECT ON default.test_table TO non_local\")\n+    instance2.query(\"CREATE USER non_local\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"CREATE TABLE test_table (id Int16) ENGINE=Memory\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"INSERT INTO test_table VALUES (123)\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"GRANT SELECT ON default.test_table TO non_local\", user=\"common_user\", password=\"qwerty\")\n \n     # serialize_query_plan is disabled because analysis requiers that local table exists.\n     result = instance1.query(\n-        \"SELECT * FROM remote('instance2', 'default.test_table', 'non_local') settings serialize_query_plan = 0\"\n+        \"SELECT * FROM remote('instance2', 'default.test_table', 'non_local') settings serialize_query_plan = 0\",\n+        user=\"common_user\", password=\"qwerty\"\n     )\n     assert result.strip() == \"123\"\n \n-    instance2.query(\"DROP USER IF EXISTS non_local\")\n-    instance2.query(\"DROP TABLE IF EXISTS test_table SYNC\")\n+    instance2.query(\"DROP USER IF EXISTS non_local\", user=\"common_user\", password=\"qwerty\")\n+    instance2.query(\"DROP TABLE IF EXISTS test_table SYNC\", user=\"common_user\", password=\"qwerty\")\n",
  "problem_statement": "Can't use external LDAP-user for remote child queries of a Distributed query\nClickHouse version 22.2.2.1\r\n1) inter-server per-cluster **secret** (for Distributed queries) is used\r\n(i.e. **initial user** as current user for a child query of **distr. queries**);\r\n\r\n2) Initial user - an **external ldap-user** 'my_user', authorization under such user is OK on all instances, both for http and native protocol;\r\n\r\n3) https port 8440, tcpSecure port 9440;\r\n\r\n4) 'database.table' - Distributed table with remote tables (there are 1 or more remote shard to read for sure);\r\n 'my_host' - host from which query is initiated; 'my_host_2' - remote host;\r\n\r\nselect count(1) from database.table;\r\n\r\nCode 516. DB::Exception: Received from 'my_host_2':9440. DB::Exception: 'my_user': Authentication failed: password is incorrect or there is no user with such name. (AUTHENTICATION FAILED) (version 22.2.2.1), server(addr='my_host':8440, db=default)\r\n\r\n------------\r\nI don't want to use default/tech_user for inter-server queries and I would like ldap-users to stay external (i.e. **not** locally created as 'identified with <ldap_server>'). Is there any workaround in that case?\nExternal roles are not transferred during LDAP authentication when querying a distributed table.\n### Company or project name\n\nClickhouse v.25.2\n\n### Describe what's wrong\n\nA db.t1d  distributed table has been created on host: host1. \ntables t1 are created on hosts: host1 and host2\nLDAP is configured on both hosts and a role has been created: Users\ngrant select on db.* to Users\nAn error occurs when executing a request by a user ldap_user with LDAP authentication. \n\nhost1 :) select count()  from db.t1d\n\nSELECT count()\nFROM db.t1d\n\nQuery id: ccaac33d-b067-4ef7-b60a-c75c657ea093\n\n\nElapsed: 0.021 sec.\n\nReceived exception from server (version 25.2.2):\nCode: 497. DB::Exception: Received from localhost:9000. DB::Exception: Received from host2:9000. DB::Exception: ldap_user: Not enough privileges. To execute this query, it's necessary to have the grant SELECT for at least one column on db.t1. (ACCESS_DENIED)\n\nAt the same time, the ldap_user user is created on host2 and the following entry is created in the host2 log:\n\n2025.04.07 18:06:08.737248 [ 2334554 ] {} <Debug> TCP_INTERSERVER-Session-a217c140-5333-491d-b770-e75ee17ca7ac: Authenticating user 'ldap_user' from 127.0.0.1:58794\n2025.04.07 18:06:08.737313 [ 2334554 ] {} <Debug> TCP_INTERSERVER-Session-a217c140-5333-491d-b770-e75ee17ca7ac: a217c140-5333-491d-b770-e75ee17ca7ac Authenticated with global context as user 5048189d-b640-2f22-3dc8-cfcb2c39d9a9\n2025.04.07 18:06:08.737325 [ 2334554 ] {} <Debug> TCP_INTERSERVER-Session-a217c140-5333-491d-b770-e75ee17ca7ac: User 5048189d-b640-2f22-3dc8-cfcb2c39d9a9 has external_roles applied: [Users] (1)\n2025.04.07 18:06:08.737430 [ 2334554 ] {} <Trace> ContextAccess (ldap_user): Settings: readonly = 0, allow_ddl = true, allow_introspection_functions = false\n2025.04.07 18:06:08.737448 [ 2334554 ] {} <Trace> ContextAccess (ldap_user): List of all grants: GRANT USAGE ON *.*\n2025.04.07 18:06:08.737537 [ 2334554 ] {} <Trace> ContextAccess (16726303): List of all grants including implicit: GRANT TABLE ENGINE ON *, REVOKE TABLE ENGINE ON AzureBlobStorage, REVOKE TABLE ENGINE ON Distributed, REVOKE TABLE ENGINE ON File, REVOKE TABLE ENGINE ON HDFS, REVOKE TABLE ENGINE ON Hive, REVOKE TABLE ENGINE ON JDBC, REVOKE TABLE ENGINE ON Kafka, REVOKE TABLE ENGINE ON MongoDB, REVOKE TABLE ENGINE ON MySQL, REVOKE TABLE ENGINE ON NATS, REVOKE TABLE ENGINE ON ODBC, REVOKE TABLE ENGINE ON PostgreSQL, REVOKE TABLE ENGINE ON RabbitMQ, REVOKE TABLE ENGINE ON Redis, REVOKE TABLE ENGINE ON S3, REVOKE TABLE ENGINE ON SQLite, REVOKE TABLE ENGINE ON URL, GRANT SELECT ON system.aggregate_function_combinators, GRANT SELECT ON system.collations, GRANT SELECT ON system.columns, GRANT SELECT ON system.contributors, GRANT SELECT ON system.current_roles, GRANT SELECT ON system.data_type_families, GRANT SELECT ON system.database_engines, GRANT SELECT ON system.databases, GRANT SELECT ON system.enabled_roles, GRANT SELECT ON system.formats, GRANT SELECT ON system.functions, GRANT SELECT ON system.licenses, GRANT SELECT ON system.one, GRANT SELECT ON system.privileges, GRANT SELECT ON system.quota_usage, GRANT SELECT ON system.settings, GRANT SELECT ON system.table_engines, GRANT SELECT ON system.table_functions, GRANT SELECT ON system.tables, GRANT SELECT ON system.time_zones\n2025.04.07 18:06:08.737914 [ 2334554 ] {7a2cc98d-b03c-46e5-8fe0-f0640c98b68a} <Debug> executeQuery: (from 127.0.0.1:58794, user: ldap_user, initial_query_id: 224d78d1-b060-40ba-a116-41ffed7d4069) (query 1, line 1) SELECT count() AS `count()` FROM `db`.`t1` AS `__table1` (stage: WithMergeableState)\n2025.04.07 18:06:08.738257 [ 2334554 ] {7a2cc98d-b03c-46e5-8fe0-f0640c98b68a} <Trace> Planner: Query to stage WithMergeableState\n2025.04.07 18:06:08.738741 [ 2334554 ] {7a2cc98d-b03c-46e5-8fe0-f0640c98b68a} <Error> executeQuery: Code: 497. DB::Exception: ldap_user: Not enough privileges. To execute this query, it's necessary to have the grant SELECT for at least one column on db.t1. (ACCESS_DENIED) (version 25.3.2.39 (official build)) (from 127.0.0.1:58794) (query 1, line 1) (in query: SELECT count() AS `count()` FROM `db`.`t1` AS `__table1`), Stack trace (when copying this message, always include the lines below):\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000f48dcdb\n1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x0000000009f6520c\n2. DB::Exception::Exception<String, String>(int, FormatStringHelperImpl<std::type_identity<String>::type, std::type_identity<String>::type>, String&&, String&&) @ 0x0000000009f64bcb\n3. DB::buildJoinTreeQueryPlan(std::shared_ptr<DB::IQueryTreeNode> const&, DB::SelectQueryInfo const&, DB::SelectQueryOptions&, std::unordered_set<String, std::hash<String>, std::equal_to<String>, std::allocator<String>> const&, std::shared_ptr<DB::PlannerContext>&) @ 0x000000001357b197\n4. DB::Planner::buildPlanForQueryNode() @ 0x0000000013561f05\n5. DB::Planner::buildQueryPlanIfNeeded() @ 0x000000001355d57e\n6. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*, std::shared_ptr<DB::IAST>&) @ 0x0000000013901d99\n7. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum) @ 0x00000000138fd3a4\n8. DB::TCPHandler::runImpl() @ 0x0000000014c2d92c\n9. DB::TCPHandler::run() @ 0x0000000014c4bb19\n10. Poco::Net::TCPServerConnection::start() @ 0x0000000018376567\n11. Poco::Net::TCPServerDispatcher::run() @ 0x00000000183769b9\n12. Poco::PooledThread::run() @ 0x0000000018342cfb\n13. Poco::ThreadImpl::runnableEntry(void*) @ 0x00000000183411dd\n14. start_thread @ 0x00000000000081ca\n15. __clone @ 0x00000000000398d3\n \nIs there a mistake in transferring external roles here?\nsrc/Interpreters/Context.cpp\n\n```\nvoid Context::setExternalRolesWithLock(const std::vector & new_external_roles, const std::lock_guard &)\n{\nif (!new_external_roles.empty())\n{\nif (current_roles)\ncurrent_roles->insert(current_roles->end(), new_external_roles.begin(), new_external_roles.end());\nelse\ncurrent_roles = std::make_shared<std::vector>(new_external_roles);\nneed_recalculate_access = true;\n}\n}\n```\nmaybe there should be external_roles instead of current_roles?\n\nI replaced current_roles->external_roles in this function and rebuilt clickhouse and this functionality is working and roles were transferred when querying the distributed table\n\n\n\n\n### Does it reproduce on the most recent release?\n\nYes\n\n### How to reproduce\n\n25.2 and 25.3\n\n### Expected behavior\n\n_No response_\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\n_No response_\n",
  "hints_text": "Same error on distributed talbes. With local tables no problem.\n\n```\n2025.03.27 15:50:46.344813 [ 2689368 ] {} <Error> Access(user directories): from: 127.0.0.1, user: user1: Authentication failed: Code: 36. DB::Exception: Unable to map external roles\n. (BAD_ARGUMENTS), Stack trace (when copying this message, always include the lines below):\n\n2025.03.27 15:50:46.345127 [ 2689368 ] {} <Error> ServerErrorHandler: Code: 516. DB::Exception: user1: Authentication failed: password is incorrect, or there is no user with such name. (AUTHENTICATION_FAILED), Stack trace (when copying this message, always include the lines below):\n\n2025.03.27 15:50:46.347026 [ 2675487 ] {4fb5ed40-46d0-4060-9ef6-22561374fac5} <Error> executeQuery: Code: 210. DB::NetException: Connection reset by peer, while reading from socket (peer: 176.9.222.238:9000, local: 176.9.222.238:36126): while receiving packet from clickhouse-bi-3:9000. (NETWORK_ERROR) (version 24.8.12.28 (official build)) (from 127.0.0.1:46524) (in query: SELECT x.* FROM `default`.test_table x), Stack trace (when copying this message, always include the lines below):\n```\nsolved by WA\n\n```\n<remote_servers>\n  <secrets></secret>\n  <shard>\n    <replica>\n      <host>host1</host>\n      <user>SERVICE_USER_WA_FOR_LDAP<user>\n    </replica>\n    <replica>\n      <host>host2</host>\n      <user>SERVICE_USER_WA_FOR_LDAP<user>\n    </replica>\n  </shard>\n</remote_servers>\n```\nActually, it should be covered by https://github.com/ClickHouse/ClickHouse/pull/70332\nCan you take a look @zvonand \n> maybe there should be external_roles instead of current_roles?\n\nWhat exactly do you mean? As far as I remember, the logic here is to add external roles to current roles.\n\n> Actually, it should be covered by https://github.com/ClickHouse/ClickHouse/pull/70332\n\nYes, it looks like it should. Also, there is a test in the PR that does pretty much the same thing as described there -- and the test is green, which is strange.\n\n@UnamedRus Btw, isn't it necessary to have rights to read underlying tables when reading from a distributed table? In my test, I grant such rights to the role. Maybe this can be the issue here?\n@zvonand \nIt seems to me that you added the external_roles property in the Context Data class, but when external roles appear, you add them immediately to current_roles in Context.cpp not using external_roles:\n\nvoid Context::setExternalRolesWithLock(const std::vector<UUID> & new_external_roles, const std::lock_guard<ContextSharedMutex> &)\n{\n    if (!new_external_roles.empty())\n    {\n        if (current_roles)\n            current_roles->insert(current_roles->end(), new_external_roles.begin(), new_external_roles.end());\n        else\n            current_roles = std::make_shared<std::vector<UUID>>(new_external_roles);\n        need_recalculate_access = true;\n    }\n} \n\nAnd you add external_roles to current_role elsewhere :\n\nvoid ContextAccess::setUser(const UserPtr & user_) const\n{.....\n if (params.external_roles && !params.external_roles->empty())\n    {\n        current_roles.insert(current_roles.end(), params.external_roles->begin(), params.external_roles->end());\n        auto new_granted_with_admin_option = user->granted_roles.findGrantedWithAdminOption(*params.external_roles);\n        current_roles_with_admin_option.insert(current_roles_with_admin_option.end(), new_granted_with_admin_option.begin(), new_granted_with_admin_option.end());\n    } ......\n\nIn this case, I get a failure at select count() of the distributed table due to lack of privileges from host2.\n\nIf I correct the context function:\n\nvoid Context::setExternalRolesWithLock(const std::vector<UUID> & new_external_roles, const std::lock_guard<ContextSharedMutex> &)\n{\n    if (!new_external_roles.empty())\n    {\n        if (external_roles)\n            external_roles->insert(external_roles->end(), new_external_roles.begin(), new_external_roles.end());\n        else\n //pg        \n      external_roles = std::make_shared<std::vector<UUID>>(new_external_roles);\n      need_recalculate_access = true;\n    }\n}\n\nAfter that, I rebuild Clickhouse and get success on the same query with the same parameters: \n\nhost1 :) select count()  from db.t1d\n\nSELECT count()\nFROM db.t1d\n\nQuery id: 566f7bd8-b760-45b1-ab67-11f6114fdb4f\n\n   \u250c\u2500count()\u2500\u2510\n1. \u2502  100201 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n1 row in set. Elapsed: 0.010 sec.\n\nhost1 :)\n\nWhat is the truth!\n\nIn the log from host2, I see:\n\n2025.04.08 09:36:24.384480 [ 2538490 ] {} <Debug> TCPHandler: Connected ClickHouse server version 25.2.0, revision: 54476, user:  INTERSERVER SECRET .\n2025.04.08 09:36:24.385411 [ 2538490 ] {} <Debug> AsyncLoader: Prioritize load job 'startup table db.t1': BackgrndStartup -> ForegroundLoad\n2025.04.08 09:36:24.385429 [ 2538490 ] {} <Debug> AsyncLoader: Prioritize load job 'load table db.t1': BackgroundLoad -> ForegroundLoad\n2025.04.08 09:36:24.386171 [ 2538490 ] {} <Debug> TCPHandler: Parsed extra roles [Users]\n2025.04.08 09:36:24.386195 [ 2538490 ] {} <Debug> TCPHandler: User (initial, interserver mode): ldap_user(client: 10.55.61.224:42452)\n2025.04.08 09:36:24.386209 [ 2538490 ] {} <Debug> TCP_INTERSERVER-Session-3f3a7678-6b22-429d-8904-c7fcde0f4c44: Authenticating user 'ldap_user' from 127.0.0.1:50072\n2025.04.08 09:36:24.386265 [ 2538490 ] {} <Debug> TCP_INTERSERVER-Session-3f3a7678-6b22-429d-8904-c7fcde0f4c44: 3f3a7678-6b22-429d-8904-c7fcde0f4c44 Authenticated with global context as user 11479201-8221-6840-b763-371e95b1b787\n2025.04.08 09:36:24.386277 [ 2538490 ] {} <Debug> TCP_INTERSERVER-Session-3f3a7678-6b22-429d-8904-c7fcde0f4c44: User 11479201-8221-6840-b763-371e95b1b787 has external_roles applied: [Users] (1)\n2025.04.08 09:36:24.386517 [ 2538490 ] {} <Trace> ContextAccess (ldap_user): Current_roles: Users, enabled_roles: Users\n2025.04.08 09:36:24.386529 [ 2538490 ] {} <Trace> ContextAccess (ldap_user): Settings: readonly = 0, allow_ddl = true, allow_introspection_functions = false\n2025.04.08 09:36:24.386541 [ 2538490 ] {} <Trace> ContextAccess (ldap_user): List of all grants: GRANT ALL ON *.*\n2025.04.08 09:36:24.386547 [ 2538490 ] {} <Trace> ContextAccess (ldap_user): List of all grants including implicit: GRANT ALL ON *.*\n2025.04.08 09:36:24.386959 [ 2538490 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Debug> executeQuery: (from 127.0.0.1:50072, user: ldap_user, initial_query_id: 566f7bd8-b760-45b1-ab67-11f6114fdb4f) (query 1, line 1) SELECT count() AS `count()` FROM `db`.`t1` AS `__table1` (stage: WithMergeableState)\n2025.04.08 09:36:24.387310 [ 2538490 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Trace> Planner: Query to stage WithMergeableState\n2025.04.08 09:36:24.387424 [ 2538490 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Trace> Planner: Query from stage FetchColumns to stage WithMergeableState\n2025.04.08 09:36:24.387494 [ 2538490 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Trace> Aggregator: Adjusting memory limit before external aggregation with 13.83 GiB (ratio: 0.5, available system memory: 27.66 GiB)\n2025.04.08 09:36:24.387648 [ 2538490 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Trace> IInterpreterUnionOrSelectQuery: The new analyzer is enabled, but the old interpreter is used. It can be a bug, please report it. Will disable 'allow_experimental_analyzer' setting (for query: SELECT min(id), max(id), count() SETTINGS aggregate_functions_null_for_empty = false, transform_null_in = false, legacy_column_name_of_tuple_literal = false)\n2025.04.08 09:36:24.388428 [ 2539165 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Trace> AggregatingTransform: Aggregating\n2025.04.08 09:36:24.388465 [ 2539165 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Trace> Aggregator: Aggregation method: without_key\n2025.04.08 09:36:24.388494 [ 2539165 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Trace> AggregatingTransform: Aggregated. 1 to 1 rows (from 24.00 B) in 0.000441059 sec. (2267.270 rows/sec., 53.14 KiB/sec.)\n2025.04.08 09:36:24.388501 [ 2539165 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Trace> Aggregator: Merging aggregated data\n2025.04.08 09:36:24.388512 [ 2539165 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Trace> HashTablesStatistics: Statistics updated for key=5607576572748926475: new sum_of_sizes=1, median_size=1\n2025.04.08 09:36:24.388860 [ 2538490 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Debug> executeQuery: Read 1 rows, 24.00 B in 0.00199 sec., 502.51256281407035 rows/sec., 11.78 KiB/sec.\n2025.04.08 09:36:24.389408 [ 2538490 ] {d2441afc-2216-4bfc-bd42-9fe5382ab817} <Debug> TCPHandler: Processed in 0.003485055 sec.\n2025.04.08 09:36:24.389480 [ 2538490 ] {} <Debug> TCP_INTERSERVER-Session-3f3a7678-6b22-429d-8904-c7fcde0f4c44: 3f3a7678-6b22-429d-8904-c7fcde0f4c44 Logout, user_id: 11479201-8221-6840-b763-371e95b1b787\n2025.04.08 09:36:24.902638 [ 2539145 ] {} <Trace> SystemLog (system.asynchronous_metric_log): Flushing system log, 2233 entries to flush up to offset 10966\n2\n\nAt the same time, if I drop role 'Users' on host2, I get no privileges when I request again from host1:\nhost1 :) select count()  from db.t1d\n\nSELECT count()\nFROM db.t1d\n\nQuery id: 2f310bb0-c7fa-4b2e-9be7-359cff872368\n\n\nElapsed: 0.008 sec.\n\nReceived exception from server (version 25.2.2):\nCode: 497. DB::Exception: Received from localhost:9000. DB::Exception: Received from host2:9000. DB::Exception: ldap_user: Not enough privileges. To execute this query, it's necessary to have the grant SELECT for at least one column on db.t1. (ACCESS_DENIED)\n\nhost1 :)\n\nAnd this is also the truth!\nI have the same problem in version 24.11, which should contain the [fix in PR 70332](https://github.com/ClickHouse/ClickHouse/pull/70332), but also throws me an error with missing rights on the second node  when authenticating and mapping roles from ldap. In the log of the remote node I also see just before the error that the user has been assigned the correct `external_roles`, but apparently the role was not applied during the query.\n\nI would also like to add that before the mentioned fix (in server version before 24.11), the query on the remote node ended with a non-existent user. After the fix, authentication via ldap works on the remote node, but the problem with the role mapping remains.",
  "created_at": "2025-04-13T23:50:24Z",
  "modified_files": [
    "src/Interpreters/Context.cpp",
    "src/QueryPipeline/RemoteQueryExecutor.cpp",
    "src/Server/TCPHandler.cpp"
  ],
  "modified_test_files": [
    "b/tests/integration/test_ldap_external_user_directory/configs/ldap_no_role_mapping.xml",
    "tests/integration/test_ldap_external_user_directory/configs/remote_servers.xml",
    "b/tests/integration/test_ldap_external_user_directory/configs/users.xml",
    "tests/integration/test_ldap_external_user_directory/test.py"
  ]
}