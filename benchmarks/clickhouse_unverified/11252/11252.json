{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 11252,
  "instance_id": "ClickHouse__ClickHouse-11252",
  "issue_numbers": [
    "7258",
    "10723"
  ],
  "base_commit": "545c9e5bace6c2112cffb1878373c880f6332f8f",
  "patch": "diff --git a/src/Storages/Kafka/KafkaSettings.h b/src/Storages/Kafka/KafkaSettings.h\nindex 93983fb60808..43984f81e054 100644\n--- a/src/Storages/Kafka/KafkaSettings.h\n+++ b/src/Storages/Kafka/KafkaSettings.h\n@@ -19,6 +19,7 @@ struct KafkaSettings : public SettingsCollection<KafkaSettings>\n     M(SettingString, kafka_broker_list, \"\", \"A comma-separated list of brokers for Kafka engine.\", 0) \\\n     M(SettingString, kafka_topic_list, \"\", \"A list of Kafka topics.\", 0) \\\n     M(SettingString, kafka_group_name, \"\", \"A group of Kafka consumers.\", 0) \\\n+    M(SettingString, kafka_client_id, \"\", \"A client id of Kafka consumer.\", 0) \\\n     M(SettingString, kafka_format, \"\", \"The message format for Kafka engine.\", 0) \\\n     M(SettingChar, kafka_row_delimiter, '\\0', \"The character to be considered as a delimiter in Kafka message.\", 0) \\\n     M(SettingString, kafka_schema, \"\", \"Schema identifier (used by schema-based formats) for Kafka engine\", 0) \\\ndiff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp\nindex 7731cf3c06a9..f69ac2686bc4 100644\n--- a/src/Storages/Kafka/StorageKafka.cpp\n+++ b/src/Storages/Kafka/StorageKafka.cpp\n@@ -34,6 +34,7 @@\n #include <Common/quoteString.h>\n #include <Processors/Sources/SourceFromInputStream.h>\n #include <librdkafka/rdkafka.h>\n+#include <common/getFQDNOrHostName.h>\n \n \n namespace DB\n@@ -118,6 +119,7 @@ StorageKafka::StorageKafka(\n     const ColumnsDescription & columns_,\n     const String & brokers_,\n     const String & group_,\n+    const String & client_id_,\n     const Names & topics_,\n     const String & format_name_,\n     char row_delimiter_,\n@@ -132,6 +134,7 @@ StorageKafka::StorageKafka(\n     , topics(global_context.getMacros()->expand(topics_))\n     , brokers(global_context.getMacros()->expand(brokers_))\n     , group(global_context.getMacros()->expand(group_))\n+    , client_id(client_id_.empty() ? getDefaultClientId(table_id_) : global_context.getMacros()->expand(client_id_))\n     , format_name(global_context.getMacros()->expand(format_name_))\n     , row_delimiter(row_delimiter_)\n     , schema_name(global_context.getMacros()->expand(schema_name_))\n@@ -149,6 +152,13 @@ StorageKafka::StorageKafka(\n     task->deactivate();\n }\n \n+String StorageKafka::getDefaultClientId(const StorageID & table_id_)\n+{\n+    std::stringstream ss;\n+    ss << VERSION_NAME << \"-\" << getFQDNOrHostName() << \"-\" << table_id_.database_name << \"-\" << table_id_.table_name;\n+    return ss.str();\n+}\n+\n \n Pipes StorageKafka::read(\n     const Names & column_names,\n@@ -194,7 +204,7 @@ void StorageKafka::startup()\n     {\n         try\n         {\n-            pushReadBuffer(createReadBuffer());\n+            pushReadBuffer(createReadBuffer(i));\n             ++num_created_consumers;\n         }\n         catch (const cppkafka::Exception &)\n@@ -262,7 +272,7 @@ ProducerBufferPtr StorageKafka::createWriteBuffer(const Block & header)\n     cppkafka::Configuration conf;\n     conf.set(\"metadata.broker.list\", brokers);\n     conf.set(\"group.id\", group);\n-    conf.set(\"client.id\", VERSION_FULL);\n+    conf.set(\"client.id\", client_id);\n     // TODO: fill required settings\n     updateConfiguration(conf);\n \n@@ -275,13 +285,22 @@ ProducerBufferPtr StorageKafka::createWriteBuffer(const Block & header)\n }\n \n \n-ConsumerBufferPtr StorageKafka::createReadBuffer()\n+ConsumerBufferPtr StorageKafka::createReadBuffer(const size_t consumer_number)\n {\n     cppkafka::Configuration conf;\n \n     conf.set(\"metadata.broker.list\", brokers);\n     conf.set(\"group.id\", group);\n-    conf.set(\"client.id\", VERSION_FULL);\n+    if (num_consumers > 1)\n+    {\n+        std::stringstream ss;\n+        ss << client_id << \"-\" << consumer_number;\n+        conf.set(\"client.id\", ss.str());\n+    }\n+    else\n+    {\n+        conf.set(\"client.id\", client_id);\n+    }\n \n     conf.set(\"auto.offset.reset\", \"smallest\");     // If no offset stored for this group, read all messages from the start\n \n@@ -503,6 +522,7 @@ void registerStorageKafka(StorageFactory & factory)\n           * - Kafka broker list\n           * - List of topics\n           * - Group ID (may be a constaint expression with a string result)\n+          * - Client ID\n           * - Message format (string)\n           * - Row delimiter\n           * - Schema (optional, if the format supports it)\n@@ -709,9 +729,12 @@ void registerStorageKafka(StorageFactory & factory)\n             }\n         }\n \n+        // Get and check client id\n+        String client_id = kafka_settings.kafka_client_id.value;\n+\n         return StorageKafka::create(\n             args.table_id, args.context, args.columns,\n-            brokers, group, topics, format, row_delimiter, schema, num_consumers, max_block_size, skip_broken, intermediate_commit);\n+            brokers, group, client_id, topics, format, row_delimiter, schema, num_consumers, max_block_size, skip_broken, intermediate_commit);\n     };\n \n     factory.registerStorage(\"Kafka\", creator_fn, StorageFactory::StorageFeatures{ .supports_settings = true, });\ndiff --git a/src/Storages/Kafka/StorageKafka.h b/src/Storages/Kafka/StorageKafka.h\nindex c813ed0033df..1ea7d6dcad70 100644\n--- a/src/Storages/Kafka/StorageKafka.h\n+++ b/src/Storages/Kafka/StorageKafka.h\n@@ -67,6 +67,7 @@ class StorageKafka final : public ext::shared_ptr_helper<StorageKafka>, public I\n         const ColumnsDescription & columns_,\n         const String & brokers_,\n         const String & group_,\n+        const String & client_id_,\n         const Names & topics_,\n         const String & format_name_,\n         char row_delimiter_,\n@@ -83,6 +84,7 @@ class StorageKafka final : public ext::shared_ptr_helper<StorageKafka>, public I\n     Names topics;\n     const String brokers;\n     const String group;\n+    const String client_id;\n     const String format_name;\n     char row_delimiter; /// optional row delimiter for generating char delimited stream in order to make various input stream parsers happy.\n     const String schema_name;\n@@ -108,12 +110,13 @@ class StorageKafka final : public ext::shared_ptr_helper<StorageKafka>, public I\n     BackgroundSchedulePool::TaskHolder task;\n     std::atomic<bool> stream_cancelled{false};\n \n-    ConsumerBufferPtr createReadBuffer();\n+    ConsumerBufferPtr createReadBuffer(const size_t consumer_number);\n \n     // Update Kafka configuration with values from CH user configuration.\n     void updateConfiguration(cppkafka::Configuration & conf);\n \n     void threadFunc();\n+    static String getDefaultClientId(const StorageID & table_id_);\n     bool streamToViews();\n     bool checkDependencies(const StorageID & table_id);\n };\n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex 13577864870f..57551789b91c 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -12,8 +12,11 @@\n import json\n import subprocess\n import kafka.errors\n-from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer\n+from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer, BrokerConnection\n from kafka.admin import NewTopic\n+from kafka.protocol.admin import DescribeGroupsResponse_v1, DescribeGroupsRequest_v1\n+from kafka.protocol.group import MemberAssignment\n+import socket\n from google.protobuf.internal.encoder import _VarintBytes\n \n \"\"\"\n@@ -110,6 +113,32 @@ def  kafka_check_result(result, check=False, ref_file='test_kafka_json.reference\n         else:\n             return TSV(result) == TSV(reference)\n \n+# https://stackoverflow.com/a/57692111/1555175\n+def describe_consumer_group(name):\n+    client = BrokerConnection('localhost', 9092, socket.AF_INET)\n+    client.connect_blocking()\n+\n+    list_members_in_groups = DescribeGroupsRequest_v1(groups=[name])\n+    future = client.send(list_members_in_groups)\n+    while not future.is_done:\n+        for resp, f in client.recv():\n+            f.success(resp)\n+\n+    (error_code, group_id, state, protocol_type, protocol, members) = future.value.groups[0]\n+\n+    res = []\n+    for member in members:\n+        (member_id, client_id, client_host, member_metadata, member_assignment) = member\n+        member_info = {}\n+        member_info['member_id'] = member_id\n+        member_info['client_id'] = client_id\n+        member_info['client_host'] = client_host\n+        member_topics_assignment = []\n+        for (topic, partitions) in MemberAssignment.decode(member_assignment).assignment:\n+            member_topics_assignment.append({'topic':topic, 'partitions':partitions})\n+        member_info['assignment'] = member_topics_assignment\n+        res.append(member_info)\n+    return res\n \n # Fixtures\n \n@@ -161,6 +190,9 @@ def test_kafka_settings_old_syntax(kafka_cluster):\n \n     kafka_check_result(result, True)\n \n+    members = describe_consumer_group('old')\n+    assert members[0]['client_id'] == u'ClickHouse-instance-test-kafka'\n+    # text_desc = kafka_cluster.exec_in_container(kafka_cluster.get_container_id('kafka1'),\"kafka-consumer-groups --bootstrap-server localhost:9092 --describe --members --group old --verbose\"))\n \n @pytest.mark.timeout(180)\n def test_kafka_settings_new_syntax(kafka_cluster):\n@@ -172,6 +204,7 @@ def test_kafka_settings_new_syntax(kafka_cluster):\n                      kafka_group_name = 'new',\n                      kafka_format = 'JSONEachRow',\n                      kafka_row_delimiter = '\\\\n',\n+                     kafka_client_id = '{instance} test 1234',\n                      kafka_skip_broken_messages = 1;\n         ''')\n \n@@ -197,6 +230,8 @@ def test_kafka_settings_new_syntax(kafka_cluster):\n \n     kafka_check_result(result, True)\n \n+    members = describe_consumer_group('new')\n+    assert members[0]['client_id'] == u'instance test 1234'\n \n @pytest.mark.timeout(180)\n def test_kafka_consumer_hang(kafka_cluster):\n@@ -837,6 +872,7 @@ def test_kafka_virtual_columns2(kafka_cluster):\n             SETTINGS kafka_broker_list = 'kafka1:19092',\n                      kafka_topic_list = 'virt2_0,virt2_1',\n                      kafka_group_name = 'virt2',\n+                     kafka_num_consumers = 2,\n                      kafka_format = 'JSONEachRow';\n \n         CREATE MATERIALIZED VIEW test.view Engine=Log AS\n@@ -866,6 +902,11 @@ def test_kafka_virtual_columns2(kafka_cluster):\n \n     time.sleep(10)\n \n+    members = describe_consumer_group('virt2')\n+    #pprint.pprint(members)\n+    members[0]['client_id'] = u'ClickHouse-instance-test-kafka-0'\n+    members[1]['client_id'] = u'ClickHouse-instance-test-kafka-1'\n+\n     result = instance.query(\"SELECT * FROM test.view ORDER BY value\", ignore_error=True)\n \n     expected = '''\\\n",
  "problem_statement": "Kafka: Setup kafka clientid\nThen multiple kafka consumers are working on the same host - there is no way to distinguish them (uuid in consumerid is not very useful). That makes tools to monitor consumer groups useless\r\n```\r\nTOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                               \r\nHOST            CLIENT-ID\r\ntopic_0001_TSV  6          2306323         2318702         12379           ClickHouse 19.15.2.2-fdf5808f-4e7a-4f5e-80a1-49a31b19cfa8 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  11         2290148         2302363         12215           ClickHouse 19.15.2.2-fdf5808f-4e7a-4f5e-80a1-49a31b19cfa8 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  10         2296029         2307799         11770           ClickHouse 19.15.2.2-fdf5808f-4e7a-4f5e-80a1-49a31b19cfa8 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  9          2304122         2316335         12213           ClickHouse 19.15.2.2-fdf5808f-4e7a-4f5e-80a1-49a31b19cfa8 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  8          2286242         2298347         12105           ClickHouse 19.15.2.2-fdf5808f-4e7a-4f5e-80a1-49a31b19cfa8 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  7          2295906         2307539         11633           ClickHouse 19.15.2.2-fdf5808f-4e7a-4f5e-80a1-49a31b19cfa8 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  3          2308505         2320769         12264           ClickHouse 19.15.2.2-5416bb29-8e48-46e8-a3cc-6a2a641b5299 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  2          2290786         2302961         12175           ClickHouse 19.15.2.2-5416bb29-8e48-46e8-a3cc-6a2a641b5299 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  0          2308357         2320601         12244           ClickHouse 19.15.2.2-5416bb29-8e48-46e8-a3cc-6a2a641b5299 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  5          2291479         2303764         12285           ClickHouse 19.15.2.2-5416bb29-8e48-46e8-a3cc-6a2a641b5299 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  4          2299537         2311144         11607           ClickHouse 19.15.2.2-5416bb29-8e48-46e8-a3cc-6a2a641b5299 /172.18.0.7     ClickHouse 19.15.2.2\r\ntopic_0001_TSV  1          2299958         2312007         12049           ClickHouse 19.15.2.2-5416bb29-8e48-46e8-a3cc-6a2a641b5299 /172.18.0.7     ClickHouse 19.15.2.2\r\n```\r\n\r\nWe need to setup clientid to something like:\r\n```\r\nclickhouse-{version}-{host_name}-{db_name}-{table_name}-{consumer id in that table if num_consumers > 1}\r\n```\nSupport kafka client.id parameter? \nI'm loading data from kafka to ck, and created this table : \r\n```\r\nCREATE TABLE xxx (\r\n...\r\n) ENGINE = Kafka\r\n  SETTINGS\r\n    kafka_broker_list = 'xxx:9092',\r\n    kafka_topic_list = 'xxx',\r\n    kafka_group_name = 'xxx',\r\n    kafka_format = 'CSV',\r\n    kafka_num_consumers = 1;\r\n```\r\n\r\nI have two questions:\r\n\r\n1. kafka client.id\r\n\r\nDoes clickhouse kafka consumer support `client.id` parameter like this: \r\n\r\n`bin/kafka-console-consumer.sh --bootstrap-server ${host}:9092 --topic ${topic} --group ${groupid} --consumer-property client.id=${clientid}`\r\n\r\n2. csv delimiter\r\n\r\nI want to set csv delimiter ( like `--format_csv_delimiter`)  to '0x01' in the create table statement\uff0chow to set this parameter?  '0x01' is an unprintable character.\r\n\n",
  "hints_text": "\n1. Duplicate of https://github.com/ClickHouse/ClickHouse/issues/7258\r\nWill add that soon \r\n\r\n2. format_csv_delimiter = '\\x01'\r\n\n> 2\\. format_csv_delimiter = '\\x01'\r\n\r\n@filimonov do you mean this: \r\n\r\n```\r\nENGINE = Kafka\r\n  SETTINGS\r\n    kafka_broker_list = 'xxx:9092',\r\n    kafka_topic_list = 'xxx',\r\n    kafka_group_name = 'xxx',\r\n    kafka_format = 'CSV',\r\n    format_csv_delimiter = '\\x01', \r\n    kafka_num_consumers = 1;\r\n```\r\n\r\ngot an error:\r\n\r\n```\r\nReceived exception from server (version 20.2.1):\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception: Unknown setting format_csv_delimiter for storage Kafka.\r\n```\r\n\nNo, right now you can only make that change globally in default profile (users.xml)",
  "created_at": "2020-05-28T13:47:15Z",
  "modified_files": [
    "src/Storages/Kafka/KafkaSettings.h",
    "src/Storages/Kafka/StorageKafka.cpp",
    "src/Storages/Kafka/StorageKafka.h"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_kafka/test.py"
  ]
}