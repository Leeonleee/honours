{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 11260,
  "instance_id": "ClickHouse__ClickHouse-11260",
  "issue_numbers": [
    "10565"
  ],
  "base_commit": "d7cc7032339af4e6786960668ec8848e4736d104",
  "patch": "diff --git a/src/Storages/Kafka/KafkaBlockInputStream.cpp b/src/Storages/Kafka/KafkaBlockInputStream.cpp\nindex a2403e66c504..6ae7e2606b69 100644\n--- a/src/Storages/Kafka/KafkaBlockInputStream.cpp\n+++ b/src/Storages/Kafka/KafkaBlockInputStream.cpp\n@@ -19,8 +19,8 @@ KafkaBlockInputStream::KafkaBlockInputStream(\n     , column_names(columns)\n     , max_block_size(max_block_size_)\n     , commit_in_suffix(commit_in_suffix_)\n-    , non_virtual_header(storage.getSampleBlockNonMaterialized()) /// FIXME: add materialized columns support\n-    , virtual_header(storage.getSampleBlockForColumns({\"_topic\", \"_key\", \"_offset\", \"_partition\", \"_timestamp\"}))\n+    , non_virtual_header(storage.getSampleBlockNonMaterialized())\n+    , virtual_header(storage.getSampleBlockForColumns({\"_topic\", \"_key\", \"_offset\", \"_partition\", \"_timestamp\",\"_timestamp_ms\"}))\n \n {\n     context.setSetting(\"input_format_skip_unknown_fields\", 1u); // Always skip unknown fields regardless of the context (JSON or TSKV)\n@@ -141,8 +141,7 @@ Block KafkaBlockInputStream::readImpl()\n         auto offset        = buffer->currentOffset();\n         auto partition     = buffer->currentPartition();\n         auto timestamp_raw = buffer->currentTimestamp();\n-        auto timestamp     = timestamp_raw ? std::chrono::duration_cast<std::chrono::seconds>(timestamp_raw->get_timestamp()).count()\n-                                                : 0;\n+\n         for (size_t i = 0; i < new_rows; ++i)\n         {\n             virtual_columns[0]->insert(topic);\n@@ -151,11 +150,14 @@ Block KafkaBlockInputStream::readImpl()\n             virtual_columns[3]->insert(partition);\n             if (timestamp_raw)\n             {\n-                virtual_columns[4]->insert(timestamp);\n+                auto ts = timestamp_raw->get_timestamp();\n+                virtual_columns[4]->insert(std::chrono::duration_cast<std::chrono::seconds>(ts).count());\n+                virtual_columns[5]->insert(DecimalField<Decimal64>(std::chrono::duration_cast<std::chrono::milliseconds>(ts).count(),3));\n             }\n             else\n             {\n                 virtual_columns[4]->insertDefault();\n+                virtual_columns[5]->insertDefault();\n             }\n         }\n \ndiff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp\nindex 7731cf3c06a9..d1f350b02a66 100644\n--- a/src/Storages/Kafka/StorageKafka.cpp\n+++ b/src/Storages/Kafka/StorageKafka.cpp\n@@ -6,6 +6,7 @@\n #include <DataStreams/UnionBlockInputStream.h>\n #include <DataStreams/copyData.h>\n #include <DataTypes/DataTypeDateTime.h>\n+#include <DataTypes/DataTypeDateTime64.h>\n #include <DataTypes/DataTypeNullable.h>\n #include <DataTypes/DataTypesNumber.h>\n #include <DataTypes/DataTypeString.h>\n@@ -724,7 +725,8 @@ NamesAndTypesList StorageKafka::getVirtuals() const\n         {\"_key\", std::make_shared<DataTypeString>()},\n         {\"_offset\", std::make_shared<DataTypeUInt64>()},\n         {\"_partition\", std::make_shared<DataTypeUInt64>()},\n-        {\"_timestamp\", std::make_shared<DataTypeNullable>(std::make_shared<DataTypeDateTime>())}\n+        {\"_timestamp\", std::make_shared<DataTypeNullable>(std::make_shared<DataTypeDateTime>())},\n+        {\"_timestamp_ms\", std::make_shared<DataTypeNullable>(std::make_shared<DataTypeDateTime64>(3))}\n     };\n }\n \n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex 13577864870f..d89684e21315 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -840,28 +840,28 @@ def test_kafka_virtual_columns2(kafka_cluster):\n                      kafka_format = 'JSONEachRow';\n \n         CREATE MATERIALIZED VIEW test.view Engine=Log AS\n-        SELECT value, _key, _topic, _partition, _offset, toUnixTimestamp(_timestamp) FROM test.kafka;\n+        SELECT value, _key, _topic, _partition, _offset, toUnixTimestamp(_timestamp), toUnixTimestamp64Milli(_timestamp_ms) FROM test.kafka;\n         ''')\n \n     producer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n \n-    producer.send(topic='virt2_0', value=json.dumps({'value': 1}), partition=0, key='k1', timestamp_ms=1577836801000)\n-    producer.send(topic='virt2_0', value=json.dumps({'value': 2}), partition=0, key='k2', timestamp_ms=1577836802000)\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 1}), partition=0, key='k1', timestamp_ms=1577836801001)\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 2}), partition=0, key='k2', timestamp_ms=1577836802002)\n     producer.flush()\n     time.sleep(1)\n \n-    producer.send(topic='virt2_0', value=json.dumps({'value': 3}), partition=1, key='k3', timestamp_ms=1577836803000)\n-    producer.send(topic='virt2_0', value=json.dumps({'value': 4}), partition=1, key='k4', timestamp_ms=1577836804000)\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 3}), partition=1, key='k3', timestamp_ms=1577836803003)\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 4}), partition=1, key='k4', timestamp_ms=1577836804004)\n     producer.flush()\n     time.sleep(1)\n \n-    producer.send(topic='virt2_1', value=json.dumps({'value': 5}), partition=0, key='k5', timestamp_ms=1577836805000)\n-    producer.send(topic='virt2_1', value=json.dumps({'value': 6}), partition=0, key='k6', timestamp_ms=1577836806000)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 5}), partition=0, key='k5', timestamp_ms=1577836805005)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 6}), partition=0, key='k6', timestamp_ms=1577836806006)\n     producer.flush()\n     time.sleep(1)\n \n-    producer.send(topic='virt2_1', value=json.dumps({'value': 7}), partition=1, key='k7', timestamp_ms=1577836807000)\n-    producer.send(topic='virt2_1', value=json.dumps({'value': 8}), partition=1, key='k8', timestamp_ms=1577836808000)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 7}), partition=1, key='k7', timestamp_ms=1577836807007)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 8}), partition=1, key='k8', timestamp_ms=1577836808008)\n     producer.flush()\n \n     time.sleep(10)\n@@ -869,14 +869,14 @@ def test_kafka_virtual_columns2(kafka_cluster):\n     result = instance.query(\"SELECT * FROM test.view ORDER BY value\", ignore_error=True)\n \n     expected = '''\\\n-1\tk1\tvirt2_0\t0\t0\t1577836801\n-2\tk2\tvirt2_0\t0\t1\t1577836802\n-3\tk3\tvirt2_0\t1\t0\t1577836803\n-4\tk4\tvirt2_0\t1\t1\t1577836804\n-5\tk5\tvirt2_1\t0\t0\t1577836805\n-6\tk6\tvirt2_1\t0\t1\t1577836806\n-7\tk7\tvirt2_1\t1\t0\t1577836807\n-8\tk8\tvirt2_1\t1\t1\t1577836808\n+1\tk1\tvirt2_0\t0\t0\t1577836801\t1577836801001\n+2\tk2\tvirt2_0\t0\t1\t1577836802\t1577836802002\n+3\tk3\tvirt2_0\t1\t0\t1577836803\t1577836803003\n+4\tk4\tvirt2_0\t1\t1\t1577836804\t1577836804004\n+5\tk5\tvirt2_1\t0\t0\t1577836805\t1577836805005\n+6\tk6\tvirt2_1\t0\t1\t1577836806\t1577836806006\n+7\tk7\tvirt2_1\t1\t0\t1577836807\t1577836807007\n+8\tk8\tvirt2_1\t1\t1\t1577836808\t1577836808008\n '''\n \n     assert TSV(result) == TSV(expected)\n",
  "problem_statement": "Kafka engine virtual column \"_timestamp\" is only precise to the nearest second\n**Describe the bug**\r\nThe virtual column ```_timestamp``` created by the Kafka engine always casts the timestamp retrieved from the Kafka record to the nearest second, regardless of their exact values. This level of accuracy is arbitrary and undocumented and leads to inaccurate timestamps.\r\n\r\n**Expected behavior**\r\nI believe the timestamp of a Kafka record always represents the number of milliseconds since epoch, so one of the following should happen:\r\n\r\n1) Simply let ```_timestamp```  be ```UInt64```.\r\n2) Convert to the nearest millisecond, i.e ```_timestamp``` should be ```DateTime64(3)```.\r\n3) Make it user-configurable.\n",
  "hints_text": "1 - will brake backward compatibity\r\n2 - may work (some extra checks needed)\r\n3 - quite safe option, but schema migration may be tricky from the user perspecive.\r\n\r\nalso there is a 4: create a new columns `_timestamp_ms` or similar.",
  "created_at": "2020-05-28T17:31:33Z",
  "modified_files": [
    "src/Storages/Kafka/KafkaBlockInputStream.cpp",
    "src/Storages/Kafka/StorageKafka.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_kafka/test.py"
  ]
}