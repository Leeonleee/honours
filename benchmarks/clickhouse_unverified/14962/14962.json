{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 14962,
  "instance_id": "ClickHouse__ClickHouse-14962",
  "issue_numbers": [
    "6802"
  ],
  "base_commit": "a5b45dabf20f7eed913f69f061f06790eddc9a1c",
  "patch": "diff --git a/src/Common/FileChecker.cpp b/src/Common/FileChecker.cpp\nindex 6cbec3bda77b..b306c3af9903 100644\n--- a/src/Common/FileChecker.cpp\n+++ b/src/Common/FileChecker.cpp\n@@ -41,6 +41,11 @@ void FileChecker::setEmpty(const String & full_file_path)\n     map[fileName(full_file_path)] = 0;\n }\n \n+FileChecker::Map FileChecker::getFileSizes() const\n+{\n+    return map;\n+}\n+\n CheckResults FileChecker::check() const\n {\n     // Read the files again every time you call `check` - so as not to violate the constancy.\ndiff --git a/src/Common/FileChecker.h b/src/Common/FileChecker.h\nindex 015d4cadb079..59e7331952e7 100644\n--- a/src/Common/FileChecker.h\n+++ b/src/Common/FileChecker.h\n@@ -27,10 +27,12 @@ class FileChecker\n     /// The purpose of this function is to rollback a group of unfinished writes.\n     void repair();\n \n-private:\n     /// File name -> size.\n     using Map = std::map<String, UInt64>;\n \n+    Map getFileSizes() const;\n+\n+private:\n     void initialize();\n     void updateImpl(const String & file_path);\n     void load(Map & local_map, const String & path) const;\ndiff --git a/src/Storages/StorageFile.cpp b/src/Storages/StorageFile.cpp\nindex cc47047dc783..7b094f9bc06b 100644\n--- a/src/Storages/StorageFile.cpp\n+++ b/src/Storages/StorageFile.cpp\n@@ -52,6 +52,7 @@ namespace ErrorCodes\n     extern const int UNKNOWN_IDENTIFIER;\n     extern const int INCORRECT_FILE_NAME;\n     extern const int FILE_DOESNT_EXIST;\n+    extern const int TIMEOUT_EXCEEDED;\n }\n \n namespace\n@@ -199,6 +200,17 @@ StorageFile::StorageFile(CommonArguments args)\n     setInMemoryMetadata(storage_metadata);\n }\n \n+\n+static std::chrono::seconds getLockTimeout(const Context & context)\n+{\n+    const Settings & settings = context.getSettingsRef();\n+    Int64 lock_timeout = settings.lock_acquire_timeout.totalSeconds();\n+    if (settings.max_execution_time.totalSeconds() != 0 && settings.max_execution_time.totalSeconds() < lock_timeout)\n+        lock_timeout = settings.max_execution_time.totalSeconds();\n+    return std::chrono::seconds{lock_timeout};\n+}\n+\n+\n class StorageFileSource : public SourceWithProgress\n {\n public:\n@@ -245,7 +257,9 @@ class StorageFileSource : public SourceWithProgress\n     {\n         if (storage->use_table_fd)\n         {\n-            unique_lock = std::unique_lock(storage->rwlock);\n+            unique_lock = std::unique_lock(storage->rwlock, getLockTimeout(context));\n+            if (!unique_lock)\n+                throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n \n             /// We could use common ReadBuffer and WriteBuffer in storage to leverage cache\n             ///  and add ability to seek unseekable files, but cache sync isn't supported.\n@@ -264,7 +278,9 @@ class StorageFileSource : public SourceWithProgress\n         }\n         else\n         {\n-            shared_lock = std::shared_lock(storage->rwlock);\n+            shared_lock = std::shared_lock(storage->rwlock, getLockTimeout(context));\n+            if (!shared_lock)\n+                throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n         }\n     }\n \n@@ -373,8 +389,8 @@ class StorageFileSource : public SourceWithProgress\n \n     bool finished_generate = false;\n \n-    std::shared_lock<std::shared_mutex> shared_lock;\n-    std::unique_lock<std::shared_mutex> unique_lock;\n+    std::shared_lock<std::shared_timed_mutex> shared_lock;\n+    std::unique_lock<std::shared_timed_mutex> unique_lock;\n };\n \n \n@@ -417,7 +433,7 @@ Pipe StorageFile::read(\n \n     for (size_t i = 0; i < num_streams; ++i)\n         pipes.emplace_back(std::make_shared<StorageFileSource>(\n-                this_ptr, metadata_snapshot, context, max_block_size, files_info, metadata_snapshot->getColumns().getDefaults()));\n+            this_ptr, metadata_snapshot, context, max_block_size, files_info, metadata_snapshot->getColumns().getDefaults()));\n \n     return Pipe::unitePipes(std::move(pipes));\n }\n@@ -429,12 +445,16 @@ class StorageFileBlockOutputStream : public IBlockOutputStream\n     explicit StorageFileBlockOutputStream(\n         StorageFile & storage_,\n         const StorageMetadataPtr & metadata_snapshot_,\n+        std::unique_lock<std::shared_timed_mutex> && lock_,\n         const CompressionMethod compression_method,\n         const Context & context)\n         : storage(storage_)\n         , metadata_snapshot(metadata_snapshot_)\n-        , lock(storage.rwlock)\n+        , lock(std::move(lock_))\n     {\n+        if (!lock)\n+            throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n         std::unique_ptr<WriteBufferFromFileDescriptor> naked_buffer = nullptr;\n         if (storage.use_table_fd)\n         {\n@@ -488,7 +508,7 @@ class StorageFileBlockOutputStream : public IBlockOutputStream\n private:\n     StorageFile & storage;\n     StorageMetadataPtr metadata_snapshot;\n-    std::unique_lock<std::shared_mutex> lock;\n+    std::unique_lock<std::shared_timed_mutex> lock;\n     std::unique_ptr<WriteBuffer> write_buf;\n     BlockOutputStreamPtr writer;\n     bool prefix_written{false};\n@@ -506,7 +526,7 @@ BlockOutputStreamPtr StorageFile::write(\n     if (!paths.empty())\n         path = paths[0];\n \n-    return std::make_shared<StorageFileBlockOutputStream>(*this, metadata_snapshot,\n+    return std::make_shared<StorageFileBlockOutputStream>(*this, metadata_snapshot, std::unique_lock{rwlock, getLockTimeout(context)},\n         chooseCompressionMethod(path, compression_method), context);\n }\n \n@@ -529,8 +549,6 @@ void StorageFile::rename(const String & new_path_to_table_data, const StorageID\n     if (path_new == paths[0])\n         return;\n \n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n     Poco::File(Poco::Path(path_new).parent()).createDirectories();\n     Poco::File(paths[0]).renameTo(path_new);\n \n@@ -547,8 +565,6 @@ void StorageFile::truncate(\n     if (paths.size() != 1)\n         throw Exception(\"Can't truncate table '\" + getStorageID().getNameForLogs() + \"' in readonly mode\", ErrorCodes::DATABASE_ACCESS_DENIED);\n \n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n     if (use_table_fd)\n     {\n         if (0 != ::ftruncate(table_fd, 0))\ndiff --git a/src/Storages/StorageFile.h b/src/Storages/StorageFile.h\nindex ea70dcd5311d..babc56e3a11d 100644\n--- a/src/Storages/StorageFile.h\n+++ b/src/Storages/StorageFile.h\n@@ -89,7 +89,7 @@ class StorageFile final : public ext::shared_ptr_helper<StorageFile>, public ISt\n     std::atomic<bool> table_fd_was_used{false}; /// To detect repeating reads from stdin\n     off_t table_fd_init_offset = -1;            /// Initial position of fd, used for repeating reads\n \n-    mutable std::shared_mutex rwlock;\n+    mutable std::shared_timed_mutex rwlock;\n \n     Poco::Logger * log = &Poco::Logger::get(\"StorageFile\");\n };\ndiff --git a/src/Storages/StorageLog.cpp b/src/Storages/StorageLog.cpp\nindex e437bfb05f14..2fbce21655cb 100644\n--- a/src/Storages/StorageLog.cpp\n+++ b/src/Storages/StorageLog.cpp\n@@ -39,6 +39,7 @@ namespace DB\n \n namespace ErrorCodes\n {\n+    extern const int TIMEOUT_EXCEEDED;\n     extern const int LOGICAL_ERROR;\n     extern const int DUPLICATE_COLUMN;\n     extern const int SIZES_OF_MARKS_FILES_ARE_INCONSISTENT;\n@@ -50,7 +51,6 @@ namespace ErrorCodes\n class LogSource final : public SourceWithProgress\n {\n public:\n-\n     static Block getHeader(const NamesAndTypesList & columns)\n     {\n         Block res;\n@@ -116,13 +116,16 @@ class LogSource final : public SourceWithProgress\n class LogBlockOutputStream final : public IBlockOutputStream\n {\n public:\n-    explicit LogBlockOutputStream(StorageLog & storage_, const StorageMetadataPtr & metadata_snapshot_)\n+    explicit LogBlockOutputStream(\n+        StorageLog & storage_, const StorageMetadataPtr & metadata_snapshot_, std::unique_lock<std::shared_timed_mutex> && lock_)\n         : storage(storage_)\n         , metadata_snapshot(metadata_snapshot_)\n-        , lock(storage.rwlock)\n+        , lock(std::move(lock_))\n         , marks_stream(\n             storage.disk->writeFile(storage.marks_file_path, 4096, WriteMode::Rewrite))\n     {\n+        if (!lock)\n+            throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n     }\n \n     ~LogBlockOutputStream() override\n@@ -149,7 +152,7 @@ class LogBlockOutputStream final : public IBlockOutputStream\n private:\n     StorageLog & storage;\n     StorageMetadataPtr metadata_snapshot;\n-    std::unique_lock<std::shared_mutex> lock;\n+    std::unique_lock<std::shared_timed_mutex> lock;\n     bool done = false;\n \n     struct Stream\n@@ -507,9 +510,11 @@ void StorageLog::addFiles(const String & column_name, const IDataType & type)\n }\n \n \n-void StorageLog::loadMarks()\n+void StorageLog::loadMarks(std::chrono::seconds lock_timeout)\n {\n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n+    std::unique_lock lock(rwlock, lock_timeout);\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n \n     if (loaded_marks)\n         return;\n@@ -552,8 +557,6 @@ void StorageLog::rename(const String & new_path_to_table_data, const StorageID &\n {\n     assert(table_path != new_path_to_table_data);\n     {\n-        std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n         disk->moveDirectory(table_path, new_path_to_table_data);\n \n         table_path = new_path_to_table_data;\n@@ -569,8 +572,6 @@ void StorageLog::rename(const String & new_path_to_table_data, const StorageID &\n \n void StorageLog::truncate(const ASTPtr &, const StorageMetadataPtr & metadata_snapshot, const Context &, TableExclusiveLockHolder &)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n-\n     files.clear();\n     file_count = 0;\n     loaded_marks = false;\n@@ -610,6 +611,17 @@ const StorageLog::Marks & StorageLog::getMarksWithRealRowCount(const StorageMeta\n     return it->second.marks;\n }\n \n+\n+static std::chrono::seconds getLockTimeout(const Context & context)\n+{\n+    const Settings & settings = context.getSettingsRef();\n+    Int64 lock_timeout = settings.lock_acquire_timeout.totalSeconds();\n+    if (settings.max_execution_time.totalSeconds() != 0 && settings.max_execution_time.totalSeconds() < lock_timeout)\n+        lock_timeout = settings.max_execution_time.totalSeconds();\n+    return std::chrono::seconds{lock_timeout};\n+}\n+\n+\n Pipe StorageLog::read(\n     const Names & column_names,\n     const StorageMetadataPtr & metadata_snapshot,\n@@ -620,11 +632,15 @@ Pipe StorageLog::read(\n     unsigned num_streams)\n {\n     metadata_snapshot->check(column_names, getVirtuals(), getStorageID());\n-    loadMarks();\n+\n+    auto lock_timeout = getLockTimeout(context);\n+    loadMarks(lock_timeout);\n \n     NamesAndTypesList all_columns = Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names));\n \n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, lock_timeout);\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n \n     Pipes pipes;\n \n@@ -653,18 +669,28 @@ Pipe StorageLog::read(\n             max_read_buffer_size));\n     }\n \n+    /// No need to hold lock while reading because we read fixed range of data that does not change while appending more data.\n     return Pipe::unitePipes(std::move(pipes));\n }\n \n-BlockOutputStreamPtr StorageLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & /*context*/)\n+BlockOutputStreamPtr StorageLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & context)\n {\n-    loadMarks();\n-    return std::make_shared<LogBlockOutputStream>(*this, metadata_snapshot);\n+    auto lock_timeout = getLockTimeout(context);\n+    loadMarks(lock_timeout);\n+\n+    std::unique_lock lock(rwlock, lock_timeout);\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n+    return std::make_shared<LogBlockOutputStream>(*this, metadata_snapshot, std::move(lock));\n }\n \n-CheckResults StorageLog::checkData(const ASTPtr & /* query */, const Context & /* context */)\n+CheckResults StorageLog::checkData(const ASTPtr & /* query */, const Context & context)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n     return file_checker.check();\n }\n \ndiff --git a/src/Storages/StorageLog.h b/src/Storages/StorageLog.h\nindex 49fc9a576c5b..3553426b9e64 100644\n--- a/src/Storages/StorageLog.h\n+++ b/src/Storages/StorageLog.h\n@@ -83,7 +83,7 @@ class StorageLog final : public ext::shared_ptr_helper<StorageLog>, public IStor\n     DiskPtr disk;\n     String table_path;\n \n-    mutable std::shared_mutex rwlock;\n+    mutable std::shared_timed_mutex rwlock;\n \n     Files files;\n \n@@ -104,7 +104,7 @@ class StorageLog final : public ext::shared_ptr_helper<StorageLog>, public IStor\n     /// Read marks files if they are not already read.\n     /// It is done lazily, so that with a large number of tables, the server starts quickly.\n     /// You can not call with a write locked `rwlock`.\n-    void loadMarks();\n+    void loadMarks(std::chrono::seconds lock_timeout);\n \n     /** For normal columns, the number of rows in the block is specified in the marks.\n       * For array columns and nested structures, there are more than one group of marks that correspond to different files\ndiff --git a/src/Storages/StorageStripeLog.cpp b/src/Storages/StorageStripeLog.cpp\nindex c4344cf6f1fd..8ff8035c1285 100644\n--- a/src/Storages/StorageStripeLog.cpp\n+++ b/src/Storages/StorageStripeLog.cpp\n@@ -47,13 +47,13 @@ namespace ErrorCodes\n {\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n     extern const int INCORRECT_FILE_NAME;\n+    extern const int TIMEOUT_EXCEEDED;\n }\n \n \n class StripeLogSource final : public SourceWithProgress\n {\n public:\n-\n     static Block getHeader(\n         StorageStripeLog & storage,\n         const StorageMetadataPtr & metadata_snapshot,\n@@ -157,10 +157,11 @@ class StripeLogSource final : public SourceWithProgress\n class StripeLogBlockOutputStream final : public IBlockOutputStream\n {\n public:\n-    explicit StripeLogBlockOutputStream(StorageStripeLog & storage_, const StorageMetadataPtr & metadata_snapshot_)\n+    explicit StripeLogBlockOutputStream(\n+        StorageStripeLog & storage_, const StorageMetadataPtr & metadata_snapshot_, std::unique_lock<std::shared_timed_mutex> && lock_)\n         : storage(storage_)\n         , metadata_snapshot(metadata_snapshot_)\n-        , lock(storage.rwlock)\n+        , lock(std::move(lock_))\n         , data_out_file(storage.table_path + \"data.bin\")\n         , data_out_compressed(storage.disk->writeFile(data_out_file, DBMS_DEFAULT_BUFFER_SIZE, WriteMode::Append))\n         , data_out(std::make_unique<CompressedWriteBuffer>(\n@@ -170,6 +171,8 @@ class StripeLogBlockOutputStream final : public IBlockOutputStream\n         , index_out(std::make_unique<CompressedWriteBuffer>(*index_out_compressed))\n         , block_out(*data_out, 0, metadata_snapshot->getSampleBlock(), false, index_out.get(), storage.disk->getFileSize(data_out_file))\n     {\n+        if (!lock)\n+            throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n     }\n \n     ~StripeLogBlockOutputStream() override\n@@ -223,7 +226,7 @@ class StripeLogBlockOutputStream final : public IBlockOutputStream\n private:\n     StorageStripeLog & storage;\n     StorageMetadataPtr metadata_snapshot;\n-    std::unique_lock<std::shared_mutex> lock;\n+    std::unique_lock<std::shared_timed_mutex> lock;\n \n     String data_out_file;\n     std::unique_ptr<WriteBuffer> data_out_compressed;\n@@ -286,8 +289,6 @@ void StorageStripeLog::rename(const String & new_path_to_table_data, const Stora\n {\n     assert(table_path != new_path_to_table_data);\n     {\n-        std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n         disk->moveDirectory(table_path, new_path_to_table_data);\n \n         table_path = new_path_to_table_data;\n@@ -297,6 +298,16 @@ void StorageStripeLog::rename(const String & new_path_to_table_data, const Stora\n }\n \n \n+static std::chrono::seconds getLockTimeout(const Context & context)\n+{\n+    const Settings & settings = context.getSettingsRef();\n+    Int64 lock_timeout = settings.lock_acquire_timeout.totalSeconds();\n+    if (settings.max_execution_time.totalSeconds() != 0 && settings.max_execution_time.totalSeconds() < lock_timeout)\n+        lock_timeout = settings.max_execution_time.totalSeconds();\n+    return std::chrono::seconds{lock_timeout};\n+}\n+\n+\n Pipe StorageStripeLog::read(\n     const Names & column_names,\n     const StorageMetadataPtr & metadata_snapshot,\n@@ -306,7 +317,9 @@ Pipe StorageStripeLog::read(\n     const size_t /*max_block_size*/,\n     unsigned num_streams)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n \n     metadata_snapshot->check(column_names, getVirtuals(), getStorageID());\n \n@@ -345,24 +358,28 @@ Pipe StorageStripeLog::read(\n }\n \n \n-BlockOutputStreamPtr StorageStripeLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & /*context*/)\n+BlockOutputStreamPtr StorageStripeLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & context)\n {\n-    return std::make_shared<StripeLogBlockOutputStream>(*this, metadata_snapshot);\n+    std::unique_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n+    return std::make_shared<StripeLogBlockOutputStream>(*this, metadata_snapshot, std::move(lock));\n }\n \n \n-CheckResults StorageStripeLog::checkData(const ASTPtr & /* query */, const Context & /* context */)\n+CheckResults StorageStripeLog::checkData(const ASTPtr & /* query */, const Context & context)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n     return file_checker.check();\n }\n \n void StorageStripeLog::truncate(const ASTPtr &, const StorageMetadataPtr &, const Context &, TableExclusiveLockHolder &)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n-\n     disk->clearDirectory(table_path);\n-\n     file_checker = FileChecker{disk, table_path + \"sizes.json\"};\n }\n \ndiff --git a/src/Storages/StorageStripeLog.h b/src/Storages/StorageStripeLog.h\nindex f88120a932ec..ca3bfe4ff75f 100644\n--- a/src/Storages/StorageStripeLog.h\n+++ b/src/Storages/StorageStripeLog.h\n@@ -67,7 +67,7 @@ class StorageStripeLog final : public ext::shared_ptr_helper<StorageStripeLog>,\n     size_t max_compress_block_size;\n \n     FileChecker file_checker;\n-    mutable std::shared_mutex rwlock;\n+    mutable std::shared_timed_mutex rwlock;\n \n     Poco::Logger * log;\n };\ndiff --git a/src/Storages/StorageTinyLog.cpp b/src/Storages/StorageTinyLog.cpp\nindex 0bdcab8abf40..4d646c7451ef 100644\n--- a/src/Storages/StorageTinyLog.cpp\n+++ b/src/Storages/StorageTinyLog.cpp\n@@ -13,6 +13,7 @@\n \n #include <IO/ReadBufferFromFileBase.h>\n #include <IO/WriteBufferFromFileBase.h>\n+#include <IO/LimitReadBuffer.h>\n #include <Compression/CompressionFactory.h>\n #include <Compression/CompressedReadBuffer.h>\n #include <Compression/CompressedWriteBuffer.h>\n@@ -46,6 +47,7 @@ namespace DB\n \n namespace ErrorCodes\n {\n+    extern const int TIMEOUT_EXCEEDED;\n     extern const int DUPLICATE_COLUMN;\n     extern const int INCORRECT_FILE_NAME;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n@@ -55,7 +57,6 @@ namespace ErrorCodes\n class TinyLogSource final : public SourceWithProgress\n {\n public:\n-\n     static Block getHeader(const NamesAndTypesList & columns)\n     {\n         Block res;\n@@ -66,10 +67,17 @@ class TinyLogSource final : public SourceWithProgress\n         return Nested::flatten(res);\n     }\n \n-    TinyLogSource(size_t block_size_, const NamesAndTypesList & columns_, StorageTinyLog & storage_, size_t max_read_buffer_size_)\n+    TinyLogSource(\n+        size_t block_size_,\n+        const NamesAndTypesList & columns_,\n+        StorageTinyLog & storage_,\n+        size_t max_read_buffer_size_,\n+        FileChecker::Map file_sizes_)\n         : SourceWithProgress(getHeader(columns_))\n-        , block_size(block_size_), columns(columns_), storage(storage_), lock(storage_.rwlock)\n-        , max_read_buffer_size(max_read_buffer_size_) {}\n+        , block_size(block_size_), columns(columns_), storage(storage_)\n+        , max_read_buffer_size(max_read_buffer_size_), file_sizes(std::move(file_sizes_))\n+    {\n+    }\n \n     String getName() const override { return \"TinyLog\"; }\n \n@@ -80,19 +88,21 @@ class TinyLogSource final : public SourceWithProgress\n     size_t block_size;\n     NamesAndTypesList columns;\n     StorageTinyLog & storage;\n-    std::shared_lock<std::shared_mutex> lock;\n     bool is_finished = false;\n     size_t max_read_buffer_size;\n+    FileChecker::Map file_sizes;\n \n     struct Stream\n     {\n-        Stream(const DiskPtr & disk, const String & data_path, size_t max_read_buffer_size_)\n+        Stream(const DiskPtr & disk, const String & data_path, size_t max_read_buffer_size_, size_t file_size)\n             : plain(disk->readFile(data_path, std::min(max_read_buffer_size_, disk->getFileSize(data_path)))),\n+            limited(std::make_unique<LimitReadBuffer>(*plain, file_size, false)),\n             compressed(*plain)\n         {\n         }\n \n         std::unique_ptr<ReadBuffer> plain;\n+        std::unique_ptr<ReadBuffer> limited;\n         CompressedReadBuffer compressed;\n     };\n \n@@ -110,9 +120,14 @@ class TinyLogSource final : public SourceWithProgress\n class TinyLogBlockOutputStream final : public IBlockOutputStream\n {\n public:\n-    explicit TinyLogBlockOutputStream(StorageTinyLog & storage_, const StorageMetadataPtr & metadata_snapshot_)\n-        : storage(storage_), metadata_snapshot(metadata_snapshot_), lock(storage_.rwlock)\n+    explicit TinyLogBlockOutputStream(\n+        StorageTinyLog & storage_,\n+        const StorageMetadataPtr & metadata_snapshot_,\n+        std::unique_lock<std::shared_timed_mutex> && lock_)\n+        : storage(storage_), metadata_snapshot(metadata_snapshot_), lock(std::move(lock_))\n     {\n+        if (!lock)\n+            throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n     }\n \n     ~TinyLogBlockOutputStream() override\n@@ -140,7 +155,7 @@ class TinyLogBlockOutputStream final : public IBlockOutputStream\n private:\n     StorageTinyLog & storage;\n     StorageMetadataPtr metadata_snapshot;\n-    std::unique_lock<std::shared_mutex> lock;\n+    std::unique_lock<std::shared_timed_mutex> lock;\n     bool done = false;\n \n     struct Stream\n@@ -231,13 +246,17 @@ void TinyLogSource::readData(const String & name, const IDataType & type, IColum\n         String stream_name = IDataType::getFileNameForStream(name, path);\n \n         if (!streams.count(stream_name))\n-            streams[stream_name] = std::make_unique<Stream>(storage.disk, storage.files[stream_name].data_file_path, max_read_buffer_size);\n+        {\n+            String file_path = storage.files[stream_name].data_file_path;\n+            streams[stream_name] = std::make_unique<Stream>(\n+                storage.disk, file_path, max_read_buffer_size, file_sizes[fileName(file_path)]);\n+        }\n \n         return &streams[stream_name]->compressed;\n     };\n \n     if (deserialize_states.count(name) == 0)\n-         type.deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);\n+        type.deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);\n \n     type.deserializeBinaryBulkWithMultipleStreams(column, limit, settings, deserialize_states[name]);\n }\n@@ -410,8 +429,6 @@ void StorageTinyLog::rename(const String & new_path_to_table_data, const Storage\n {\n     assert(table_path != new_path_to_table_data);\n     {\n-        std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n         disk->moveDirectory(table_path, new_path_to_table_data);\n \n         table_path = new_path_to_table_data;\n@@ -424,6 +441,16 @@ void StorageTinyLog::rename(const String & new_path_to_table_data, const Storage\n }\n \n \n+static std::chrono::seconds getLockTimeout(const Context & context)\n+{\n+    const Settings & settings = context.getSettingsRef();\n+    Int64 lock_timeout = settings.lock_acquire_timeout.totalSeconds();\n+    if (settings.max_execution_time.totalSeconds() != 0 && settings.max_execution_time.totalSeconds() < lock_timeout)\n+        lock_timeout = settings.max_execution_time.totalSeconds();\n+    return std::chrono::seconds{lock_timeout};\n+}\n+\n+\n Pipe StorageTinyLog::read(\n     const Names & column_names,\n     const StorageMetadataPtr & metadata_snapshot,\n@@ -437,28 +464,40 @@ Pipe StorageTinyLog::read(\n \n     // When reading, we lock the entire storage, because we only have one file\n     // per column and can't modify it concurrently.\n+    const Settings & settings = context.getSettingsRef();\n+\n+    std::shared_lock lock{rwlock, getLockTimeout(context)};\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n+    /// No need to hold lock while reading because we read fixed range of data that does not change while appending more data.\n     return Pipe(std::make_shared<TinyLogSource>(\n-        max_block_size, Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names)), *this, context.getSettingsRef().max_read_buffer_size));\n+        max_block_size,\n+        Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names)),\n+        *this,\n+        settings.max_read_buffer_size,\n+        file_checker.getFileSizes()));\n }\n \n \n-BlockOutputStreamPtr StorageTinyLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & /*context*/)\n+BlockOutputStreamPtr StorageTinyLog::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, const Context & context)\n {\n-    return std::make_shared<TinyLogBlockOutputStream>(*this, metadata_snapshot);\n+    return std::make_shared<TinyLogBlockOutputStream>(*this, metadata_snapshot, std::unique_lock{rwlock, getLockTimeout(context)});\n }\n \n \n-CheckResults StorageTinyLog::checkData(const ASTPtr & /* query */, const Context & /* context */)\n+CheckResults StorageTinyLog::checkData(const ASTPtr & /* query */, const Context & context)\n {\n-    std::shared_lock<std::shared_mutex> lock(rwlock);\n+    std::shared_lock lock(rwlock, getLockTimeout(context));\n+    if (!lock)\n+        throw Exception(\"Lock timeout exceeded\", ErrorCodes::TIMEOUT_EXCEEDED);\n+\n     return file_checker.check();\n }\n \n void StorageTinyLog::truncate(\n     const ASTPtr &, const StorageMetadataPtr & metadata_snapshot, const Context &, TableExclusiveLockHolder &)\n {\n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n-\n     disk->clearDirectory(table_path);\n \n     files.clear();\n@@ -468,14 +507,6 @@ void StorageTinyLog::truncate(\n         addFiles(column.name, *column.type);\n }\n \n-void StorageTinyLog::drop()\n-{\n-    std::unique_lock<std::shared_mutex> lock(rwlock);\n-    if (disk->exists(table_path))\n-        disk->removeRecursive(table_path);\n-    files.clear();\n-}\n-\n \n void registerStorageTinyLog(StorageFactory & factory)\n {\ndiff --git a/src/Storages/StorageTinyLog.h b/src/Storages/StorageTinyLog.h\nindex dc6ff1015038..95b7d9f29412 100644\n--- a/src/Storages/StorageTinyLog.h\n+++ b/src/Storages/StorageTinyLog.h\n@@ -43,8 +43,6 @@ class StorageTinyLog final : public ext::shared_ptr_helper<StorageTinyLog>, publ\n \n     void truncate(const ASTPtr &, const StorageMetadataPtr & metadata_snapshot, const Context &, TableExclusiveLockHolder &) override;\n \n-    void drop() override;\n-\n protected:\n     StorageTinyLog(\n         DiskPtr disk_,\n@@ -70,7 +68,7 @@ class StorageTinyLog final : public ext::shared_ptr_helper<StorageTinyLog>, publ\n     Files files;\n \n     FileChecker file_checker;\n-    mutable std::shared_mutex rwlock;\n+    mutable std::shared_timed_mutex rwlock;\n \n     Poco::Logger * log;\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01499_log_deadlock.reference b/tests/queries/0_stateless/01499_log_deadlock.reference\nnew file mode 100644\nindex 000000000000..166be640db57\n--- /dev/null\n+++ b/tests/queries/0_stateless/01499_log_deadlock.reference\n@@ -0,0 +1,3 @@\n+6\n+6\n+6\ndiff --git a/tests/queries/0_stateless/01499_log_deadlock.sql b/tests/queries/0_stateless/01499_log_deadlock.sql\nnew file mode 100644\nindex 000000000000..e98b37f24554\n--- /dev/null\n+++ b/tests/queries/0_stateless/01499_log_deadlock.sql\n@@ -0,0 +1,26 @@\n+DROP TABLE IF EXISTS t;\n+CREATE TABLE t (x UInt8) ENGINE = TinyLog;\n+\n+INSERT INTO t VALUES (1), (2), (3);\n+INSERT INTO t SELECT * FROM t;\n+SELECT count() FROM t;\n+\n+DROP TABLE t;\n+\n+\n+CREATE TABLE t (x UInt8) ENGINE = Log;\n+\n+INSERT INTO t VALUES (1), (2), (3);\n+INSERT INTO t SELECT * FROM t;\n+SELECT count() FROM t;\n+\n+DROP TABLE t;\n+\n+\n+CREATE TABLE t (x UInt8) ENGINE = StripeLog;\n+\n+INSERT INTO t VALUES (1), (2), (3);\n+INSERT INTO t SELECT * FROM t;\n+SELECT count() FROM t;\n+\n+DROP TABLE t;\ndiff --git a/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.reference b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.reference\nnew file mode 100644\nindex 000000000000..4bf85ae79f3a\n--- /dev/null\n+++ b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.reference\n@@ -0,0 +1,6 @@\n+Testing TinyLog\n+Done TinyLog\n+Testing StripeLog\n+Done StripeLog\n+Testing Log\n+Done Log\ndiff --git a/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.sh b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.sh\nnew file mode 100755\nindex 000000000000..29c5f8686174\n--- /dev/null\n+++ b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.sh\n@@ -0,0 +1,85 @@\n+#!/usr/bin/env bash\n+\n+set -e\n+\n+CLICKHOUSE_CLIENT_SERVER_LOGS_LEVEL=fatal\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+. \"$CURDIR\"/../shell_config.sh\n+\n+\n+function thread_create {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"CREATE TABLE IF NOT EXISTS $1 (x UInt64, s Array(Nullable(String))) ENGINE = $2\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|57)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_drop {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"DROP TABLE IF EXISTS $1\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|57)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_rename {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"RENAME TABLE $1 TO $2\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|57)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_select {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"SELECT * FROM $1 FORMAT Null\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|218)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_insert {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"INSERT INTO $1 SELECT rand64(1), [toString(rand64(2))] FROM numbers($2)\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|218)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+function thread_insert_select {\n+    while true; do\n+        $CLICKHOUSE_CLIENT --query \"INSERT INTO $1 SELECT * FROM $2\" 2>&1 | grep -v -F 'Received exception from server' | grep -v -P 'Code: (60|218)'\n+        sleep 0.0$RANDOM\n+    done\n+}\n+\n+export -f thread_create\n+export -f thread_drop\n+export -f thread_rename\n+export -f thread_select\n+export -f thread_insert\n+export -f thread_insert_select\n+\n+\n+# Do randomized queries and expect nothing extraordinary happens.\n+\n+function test_with_engine {\n+    echo \"Testing $1\"\n+\n+    timeout 10 bash -c \"thread_create t1 $1\" &\n+    timeout 10 bash -c \"thread_create t2 $1\" &\n+    timeout 10 bash -c 'thread_drop t1' &\n+    timeout 10 bash -c 'thread_drop t2' &\n+    timeout 10 bash -c 'thread_rename t1 t2' &\n+    timeout 10 bash -c 'thread_rename t2 t1' &\n+    timeout 10 bash -c 'thread_select t1' &\n+    timeout 10 bash -c 'thread_select t2' &\n+    timeout 10 bash -c 'thread_insert t1 5' &\n+    timeout 10 bash -c 'thread_insert t2 10' &\n+    timeout 10 bash -c 'thread_insert_select t1 t2' &\n+    timeout 10 bash -c 'thread_insert_select t2 t1' &\n+\n+    wait\n+    echo \"Done $1\"\n+}\n+\n+test_with_engine TinyLog\n+test_with_engine StripeLog\n+test_with_engine Log\ndiff --git a/tests/queries/0_stateless/01505_log_distributed_deadlock.reference b/tests/queries/0_stateless/01505_log_distributed_deadlock.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/01505_log_distributed_deadlock.sql b/tests/queries/0_stateless/01505_log_distributed_deadlock.sql\nnew file mode 100644\nindex 000000000000..2b0b2b97188c\n--- /dev/null\n+++ b/tests/queries/0_stateless/01505_log_distributed_deadlock.sql\n@@ -0,0 +1,12 @@\n+DROP TABLE IF EXISTS t_local;\n+DROP TABLE IF EXISTS t_dist;\n+\n+create table t_local(a int) engine Log;\n+create table t_dist (a int) engine Distributed(test_shard_localhost, currentDatabase(), 't_local', cityHash64(a));\n+\n+set insert_distributed_sync = 1;\n+\n+insert into t_dist values (1);\n+\n+DROP TABLE t_local;\n+DROP TABLE t_dist;\ndiff --git a/tests/queries/0_stateless/arcadia_skip_list.txt b/tests/queries/0_stateless/arcadia_skip_list.txt\nindex 69391ca9fd4c..6d1c6444d1bc 100644\n--- a/tests/queries/0_stateless/arcadia_skip_list.txt\n+++ b/tests/queries/0_stateless/arcadia_skip_list.txt\n@@ -145,3 +145,4 @@\n 01461_query_start_time_microseconds\n 01455_shard_leaf_max_rows_bytes_to_read\n 01505_distributed_local_type_conversion_enum\n+01505_log_distributed_deadlock\n",
  "problem_statement": "Fix insert select deadlock for log family engine (former PR #5097)\nI hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nFor changelog. Remove if this is non-significant change.\r\n\r\nCategory (leave one):\r\n- Bug Fix\r\n\r\nResuming PR #5097 originally started by @zhang2014 \r\n\n",
  "hints_text": "@Akazz \r\nHow are the questions listed here for https://github.com/yandex/ClickHouse/pull/5097#issuecomment-524422831 considered? Sorry, I haven't replied since I haven't come up with a good idea\n> @Akazz\r\n> How are the questions listed here for [#5097 (comment)](https://github.com/yandex/ClickHouse/pull/5097#issuecomment-524422831) considered?\r\n\r\nWe decided to proceed with your PR because it fixed deadlock scenarios as much as for Log + StripeLog engines. Commits are to follow soon\r\n\r\n> Sorry, I haven't replied since I haven't come up with a good idea\r\n\r\nNo problem. Don't worry about that!\n@Akazz This is still to do. And only a few steps remain.\n@Akazz ?\nI am going to pick this up _asap_!\r\n\r\n> @Akazz This is still to do. And only a few steps remain.\r\n\n\"ASAP\" is going to be nearest days.\n@Akazz It's unclear what you are going to do with this PR.",
  "created_at": "2020-09-17T20:34:09Z",
  "modified_files": [
    "src/Common/FileChecker.cpp",
    "src/Common/FileChecker.h",
    "src/Storages/StorageFile.cpp",
    "src/Storages/StorageFile.h",
    "src/Storages/StorageLog.cpp",
    "src/Storages/StorageLog.h",
    "src/Storages/StorageStripeLog.cpp",
    "src/Storages/StorageStripeLog.h",
    "src/Storages/StorageTinyLog.cpp",
    "src/Storages/StorageTinyLog.h"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/01499_log_deadlock.reference",
    "b/tests/queries/0_stateless/01499_log_deadlock.sql",
    "b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.reference",
    "b/tests/queries/0_stateless/01502_long_log_tinylog_deadlock_race.sh",
    "b/tests/queries/0_stateless/01505_log_distributed_deadlock.sql",
    "tests/queries/0_stateless/arcadia_skip_list.txt"
  ]
}