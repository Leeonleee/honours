diff --git a/dbms/src/Client/ConnectionPoolWithFailover.cpp b/dbms/src/Client/ConnectionPoolWithFailover.cpp
index acba75086b28..ee8c3607c43e 100644
--- a/dbms/src/Client/ConnectionPoolWithFailover.cpp
+++ b/dbms/src/Client/ConnectionPoolWithFailover.cpp
@@ -4,6 +4,7 @@
 #include <Poco/Net/DNS.h>
 
 #include <Common/getFQDNOrHostName.h>
+#include <Common/isLocalAddress.h>
 #include <Common/ProfileEvents.h>
 #include <Interpreters/Settings.h>
 
@@ -39,14 +40,7 @@ ConnectionPoolWithFailover::ConnectionPoolWithFailover(
     for (size_t i = 0; i < nested_pools.size(); ++i)
     {
         ConnectionPool & connection_pool = dynamic_cast<ConnectionPool &>(*nested_pools[i]);
-        const std::string & host = connection_pool.getHost();
-
-        size_t hostname_difference = 0;
-        for (size_t i = 0; i < std::min(local_hostname.length(), host.length()); ++i)
-            if (local_hostname[i] != host[i])
-                ++hostname_difference;
-
-        hostname_differences[i] = hostname_difference;
+        hostname_differences[i] = getHostNameDifference(local_hostname, connection_pool.getHost());
     }
 }
 
diff --git a/dbms/src/Common/ZooKeeper/Types.h b/dbms/src/Common/ZooKeeper/Types.h
index dd6e6c51068c..4097627f395a 100644
--- a/dbms/src/Common/ZooKeeper/Types.h
+++ b/dbms/src/Common/ZooKeeper/Types.h
@@ -138,11 +138,12 @@ struct Op::Check : public Op
 
 struct OpResult : public zoo_op_result_t
 {
-    /// Указатели в этой структуре указывают на поля в классе Op.
-    /// Поэтому деструктор не нужен
+    /// Pointers in this class point to fields of class Op.
+    /// Op instances have the same (or longer lifetime), therefore destructor is not required.
 };
 
-using Ops = std::vector<std::unique_ptr<Op>>;
+using OpPtr = std::unique_ptr<Op>;
+using Ops = std::vector<OpPtr>;
 using OpResults = std::vector<OpResult>;
 using OpResultsPtr = std::shared_ptr<OpResults>;
 using Strings = std::vector<std::string>;
diff --git a/dbms/src/Common/getFQDNOrHostName.h b/dbms/src/Common/getFQDNOrHostName.h
index a4367a726221..fe164a642046 100644
--- a/dbms/src/Common/getFQDNOrHostName.h
+++ b/dbms/src/Common/getFQDNOrHostName.h
@@ -2,6 +2,7 @@
 
 #include <string>
 
+
 /** Get the FQDN for the local server by resolving DNS hostname - similar to calling the 'hostname' tool with the -f flag.
   * If it does not work, return hostname - similar to calling 'hostname' without flags or 'uname -n'.
   */
diff --git a/dbms/src/Common/isLocalAddress.cpp b/dbms/src/Common/isLocalAddress.cpp
index 25644400bc54..742eac967ba8 100644
--- a/dbms/src/Common/isLocalAddress.cpp
+++ b/dbms/src/Common/isLocalAddress.cpp
@@ -10,25 +10,35 @@
 namespace DB
 {
 
-bool isLocalAddress(const Poco::Net::SocketAddress & address, UInt16 clickhouse_port)
+bool isLocalAddress(const Poco::Net::SocketAddress & address)
 {
     static auto interfaces = Poco::Net::NetworkInterface::list();
 
-    if (clickhouse_port == address.port())
-    {
-        return interfaces.end() != std::find_if(interfaces.begin(), interfaces.end(),
-            [&] (const Poco::Net::NetworkInterface & interface)
-            {
-                /** Compare the addresses without taking into account `scope`.
-                  * Theoretically, this may not be correct - depends on `route` setting
-                  *  - through which interface we will actually access the specified address.
-                  */
-                return interface.address().length() == address.host().length()
-                    && 0 == memcmp(interface.address().addr(), address.host().addr(), address.host().length());
-            });
-    }
-
-    return false;
+    return interfaces.end() != std::find_if(interfaces.begin(), interfaces.end(),
+                [&] (const Poco::Net::NetworkInterface & interface)
+                {
+                    /** Compare the addresses without taking into account `scope`.
+                      * Theoretically, this may not be correct - depends on `route` setting
+                      *  - through which interface we will actually access the specified address.
+                      */
+                    return interface.address().length() == address.host().length()
+                        && 0 == memcmp(interface.address().addr(), address.host().addr(), address.host().length());
+                });
+}
+
+bool isLocalAddress(const Poco::Net::SocketAddress & address, UInt16 clickhouse_port)
+{
+    return clickhouse_port == address.port() && isLocalAddress(address);
+}
+
+
+size_t getHostNameDifference(const std::string & local_hostname, const std::string & host)
+{
+    size_t hostname_difference = 0;
+    for (size_t i = 0; i < std::min(local_hostname.length(), host.length()); ++i)
+        if (local_hostname[i] != host[i])
+            ++hostname_difference;
+    return hostname_difference;
 }
 
 }
diff --git a/dbms/src/Common/isLocalAddress.h b/dbms/src/Common/isLocalAddress.h
index e6d85432ce80..d90d7aef63aa 100644
--- a/dbms/src/Common/isLocalAddress.h
+++ b/dbms/src/Common/isLocalAddress.h
@@ -23,4 +23,8 @@ namespace DB
      */
     bool isLocalAddress(const Poco::Net::SocketAddress & address, UInt16 clickhouse_port);
 
+    bool isLocalAddress(const Poco::Net::SocketAddress & address);
+
+    /// Returns number of different bytes in hostnames, used for load balancing
+    size_t getHostNameDifference(const std::string & local_hostname, const std::string & host);
 }
diff --git a/dbms/src/DataStreams/SquashingBlockInputStream.cpp b/dbms/src/DataStreams/SquashingBlockInputStream.cpp
index 3759ba1d1a3a..61af8e1a0f0e 100644
--- a/dbms/src/DataStreams/SquashingBlockInputStream.cpp
+++ b/dbms/src/DataStreams/SquashingBlockInputStream.cpp
@@ -4,7 +4,8 @@
 namespace DB
 {
 
-SquashingBlockInputStream::SquashingBlockInputStream(BlockInputStreamPtr & src, size_t min_block_size_rows, size_t min_block_size_bytes)
+SquashingBlockInputStream::SquashingBlockInputStream(const BlockInputStreamPtr & src,
+                                                     size_t min_block_size_rows, size_t min_block_size_bytes)
     : transform(min_block_size_rows, min_block_size_bytes)
 {
     children.emplace_back(src);
diff --git a/dbms/src/DataStreams/SquashingBlockInputStream.h b/dbms/src/DataStreams/SquashingBlockInputStream.h
index 7d2583d35bac..3da819d21677 100644
--- a/dbms/src/DataStreams/SquashingBlockInputStream.h
+++ b/dbms/src/DataStreams/SquashingBlockInputStream.h
@@ -12,7 +12,7 @@ namespace DB
 class SquashingBlockInputStream : public IProfilingBlockInputStream
 {
 public:
-    SquashingBlockInputStream(BlockInputStreamPtr & src, size_t min_block_size_rows, size_t min_block_size_bytes);
+    SquashingBlockInputStream(const BlockInputStreamPtr & src, size_t min_block_size_rows, size_t min_block_size_bytes);
 
     String getName() const override { return "Squashing"; }
 
diff --git a/dbms/src/DataStreams/copyData.cpp b/dbms/src/DataStreams/copyData.cpp
index dcf908b1f9a0..cf015c38b790 100644
--- a/dbms/src/DataStreams/copyData.cpp
+++ b/dbms/src/DataStreams/copyData.cpp
@@ -16,20 +16,21 @@ bool isAtomicSet(std::atomic<bool> * val)
 
 }
 
-void copyData(IBlockInputStream & from, IBlockOutputStream & to, std::atomic<bool> * is_cancelled)
+template <typename Pred>
+void copyDataImpl(IBlockInputStream & from, IBlockOutputStream & to, Pred && is_cancelled)
 {
     from.readPrefix();
     to.writePrefix();
 
     while (Block block = from.read())
     {
-        if (isAtomicSet(is_cancelled))
+        if (is_cancelled())
             break;
 
         to.write(block);
     }
 
-    if (isAtomicSet(is_cancelled))
+    if (is_cancelled())
         return;
 
     /// For outputting additional information in some formats.
@@ -42,11 +43,28 @@ void copyData(IBlockInputStream & from, IBlockOutputStream & to, std::atomic<boo
         to.setExtremes(input->getExtremes());
     }
 
-    if (isAtomicSet(is_cancelled))
+    if (is_cancelled())
         return;
 
     from.readSuffix();
     to.writeSuffix();
 }
 
+
+void copyData(IBlockInputStream & from, IBlockOutputStream & to, std::atomic<bool> * is_cancelled)
+{
+    auto is_cancelled_pred = [is_cancelled] ()
+    {
+        return isAtomicSet(is_cancelled);
+    };
+
+    copyDataImpl(from, to, is_cancelled_pred);
+}
+
+
+void copyData(IBlockInputStream & from, IBlockOutputStream & to, const std::function<bool()> & is_cancelled)
+{
+    copyDataImpl(from, to, is_cancelled);
+}
+
 }
diff --git a/dbms/src/DataStreams/copyData.h b/dbms/src/DataStreams/copyData.h
index 2a42ef191cb1..6e8d54806c42 100644
--- a/dbms/src/DataStreams/copyData.h
+++ b/dbms/src/DataStreams/copyData.h
@@ -1,6 +1,7 @@
 #pragma once
 
 #include <atomic>
+#include <functional>
 
 
 namespace DB
@@ -14,4 +15,6 @@ class IBlockOutputStream;
   */
 void copyData(IBlockInputStream & from, IBlockOutputStream & to, std::atomic<bool> * is_cancelled = nullptr);
 
+void copyData(IBlockInputStream & from, IBlockOutputStream & to, const std::function<bool()> & is_cancelled);
+
 }
diff --git a/dbms/src/Databases/DatabaseDictionary.cpp b/dbms/src/Databases/DatabaseDictionary.cpp
index 43c1c1247aa6..69890ba075cc 100644
--- a/dbms/src/Databases/DatabaseDictionary.cpp
+++ b/dbms/src/Databases/DatabaseDictionary.cpp
@@ -173,4 +173,9 @@ void DatabaseDictionary::drop()
     /// Additional actions to delete database are not required.
 }
 
+String DatabaseDictionary::getDataPath(const Context &) const
+{
+    return {};
+}
+
 }
diff --git a/dbms/src/Databases/DatabaseDictionary.h b/dbms/src/Databases/DatabaseDictionary.h
index 0aeba8db629e..d6a8944fa0dd 100644
--- a/dbms/src/Databases/DatabaseDictionary.h
+++ b/dbms/src/Databases/DatabaseDictionary.h
@@ -93,6 +93,8 @@ class DatabaseDictionary : public IDatabase
         const Context & context,
         const String & table_name) const override;
 
+    String getDataPath(const Context & context) const override;
+
     void shutdown() override;
     void drop() override;
 };
diff --git a/dbms/src/Databases/DatabaseFactory.cpp b/dbms/src/Databases/DatabaseFactory.cpp
index c9259642e03e..f9976de9029e 100644
--- a/dbms/src/Databases/DatabaseFactory.cpp
+++ b/dbms/src/Databases/DatabaseFactory.cpp
@@ -15,11 +15,11 @@ namespace ErrorCodes
 DatabasePtr DatabaseFactory::get(
     const String & engine_name,
     const String & database_name,
-    const String & path,
+    const String & metadata_path,
     Context & context)
 {
     if (engine_name == "Ordinary")
-        return std::make_shared<DatabaseOrdinary>(database_name, path);
+        return std::make_shared<DatabaseOrdinary>(database_name, metadata_path, context);
     else if (engine_name == "Memory")
         return std::make_shared<DatabaseMemory>(database_name);
     else if (engine_name == "Dictionary")
diff --git a/dbms/src/Databases/DatabaseFactory.h b/dbms/src/Databases/DatabaseFactory.h
index 5e8d02ed1380..00265a2454b2 100644
--- a/dbms/src/Databases/DatabaseFactory.h
+++ b/dbms/src/Databases/DatabaseFactory.h
@@ -13,7 +13,7 @@ class DatabaseFactory
     static DatabasePtr get(
         const String & engine_name,
         const String & database_name,
-        const String & path,
+        const String & metadata_path,
         Context & context);
 };
 
diff --git a/dbms/src/Databases/DatabaseMemory.cpp b/dbms/src/Databases/DatabaseMemory.cpp
index cdbfedb6326e..33a12947ca77 100644
--- a/dbms/src/Databases/DatabaseMemory.cpp
+++ b/dbms/src/Databases/DatabaseMemory.cpp
@@ -152,4 +152,9 @@ void DatabaseMemory::drop()
     /// Additional actions to delete database are not required.
 }
 
+String DatabaseMemory::getDataPath(const Context &) const
+{
+    return {};
+}
+
 }
diff --git a/dbms/src/Databases/DatabaseMemory.h b/dbms/src/Databases/DatabaseMemory.h
index 662d8d0b6ae0..ef8207c86bf0 100644
--- a/dbms/src/Databases/DatabaseMemory.h
+++ b/dbms/src/Databases/DatabaseMemory.h
@@ -84,6 +84,8 @@ class DatabaseMemory : public IDatabase
         const Context & context,
         const String & table_name) const override;
 
+    String getDataPath(const Context & context) const override;
+
     void shutdown() override;
     void drop() override;
 };
diff --git a/dbms/src/Databases/DatabaseOrdinary.cpp b/dbms/src/Databases/DatabaseOrdinary.cpp
index 3509a9356ebb..37ae8024ee03 100644
--- a/dbms/src/Databases/DatabaseOrdinary.cpp
+++ b/dbms/src/Databases/DatabaseOrdinary.cpp
@@ -90,10 +90,10 @@ static void loadTable(
 }
 
 
-DatabaseOrdinary::DatabaseOrdinary(
-    const String & name_, const String & path_)
-    : DatabaseMemory(name_), path(path_)
+DatabaseOrdinary::DatabaseOrdinary(const String & name_, const String & metadata_path_, const Context & context)
+    : DatabaseMemory(name_), metadata_path(metadata_path_), data_path(context.getPath() + "data/" + escapeForFileName(name_) + "/")
 {
+    Poco::File(data_path).createDirectory();
 }
 
 
@@ -108,7 +108,7 @@ void DatabaseOrdinary::loadTables(
     FileNames file_names;
 
     Poco::DirectoryIterator dir_end;
-    for (Poco::DirectoryIterator dir_it(path); dir_it != dir_end; ++dir_it)
+    for (Poco::DirectoryIterator dir_it(metadata_path); dir_it != dir_end; ++dir_it)
     {
         /// For '.svn', '.gitignore' directory and similar.
         if (dir_it.name().at(0) == '.')
@@ -130,7 +130,7 @@ void DatabaseOrdinary::loadTables(
         if (endsWith(dir_it.name(), ".sql"))
             file_names.push_back(dir_it.name());
         else
-            throw Exception("Incorrect file extension: " + dir_it.name() + " in metadata directory " + path,
+            throw Exception("Incorrect file extension: " + dir_it.name() + " in metadata directory " + metadata_path,
                 ErrorCodes::INCORRECT_FILE_NAME);
     }
 
@@ -162,7 +162,7 @@ void DatabaseOrdinary::loadTables(
                 watch.restart();
             }
 
-            loadTable(context, path, *this, name, data_path, table, has_force_restore_data_flag);
+            loadTable(context, metadata_path, *this, name, data_path, table, has_force_restore_data_flag);
         }
     };
 
@@ -269,7 +269,7 @@ void DatabaseOrdinary::createTable(
             throw Exception("Table " + name + "." + table_name + " already exists.", ErrorCodes::TABLE_ALREADY_EXISTS);
     }
 
-    String table_metadata_path = getTableMetadataPath(path, table_name);
+    String table_metadata_path = getTableMetadataPath(metadata_path, table_name);
     String table_metadata_tmp_path = table_metadata_path + ".tmp";
     String statement;
 
@@ -312,7 +312,7 @@ void DatabaseOrdinary::removeTable(
 {
     StoragePtr res = detachTable(table_name);
 
-    String table_metadata_path = getTableMetadataPath(path, table_name);
+    String table_metadata_path = getTableMetadataPath(metadata_path, table_name);
 
     try
     {
@@ -374,7 +374,7 @@ void DatabaseOrdinary::renameTable(
         throw Exception{e};
     }
 
-    ASTPtr ast = getCreateQueryImpl(path, table_name);
+    ASTPtr ast = getCreateQueryImpl(metadata_path, table_name);
     ASTCreateQuery & ast_create_query = typeid_cast<ASTCreateQuery &>(*ast);
     ast_create_query.table = to_table_name;
 
@@ -388,7 +388,7 @@ time_t DatabaseOrdinary::getTableMetadataModificationTime(
     const Context & /*context*/,
     const String & table_name)
 {
-    String table_metadata_path = getTableMetadataPath(path, table_name);
+    String table_metadata_path = getTableMetadataPath(metadata_path, table_name);
     Poco::File meta_file(table_metadata_path);
 
     if (meta_file.exists())
@@ -406,7 +406,7 @@ ASTPtr DatabaseOrdinary::getCreateQuery(
     const Context & /*context*/,
     const String & table_name) const
 {
-    ASTPtr ast = getCreateQueryImpl(path, table_name);
+    ASTPtr ast = getCreateQueryImpl(metadata_path, table_name);
 
     ASTCreateQuery & ast_create_query = typeid_cast<ASTCreateQuery &>(*ast);
     ast_create_query.attach = false;
@@ -455,8 +455,8 @@ void DatabaseOrdinary::alterTable(
     /// Read the definition of the table and replace the necessary parts with new ones.
 
     String table_name_escaped = escapeForFileName(name);
-    String table_metadata_tmp_path = path + "/" + table_name_escaped + ".sql.tmp";
-    String table_metadata_path = path + "/" + table_name_escaped + ".sql";
+    String table_metadata_tmp_path = metadata_path + "/" + table_name_escaped + ".sql.tmp";
+    String table_metadata_path = metadata_path + "/" + table_name_escaped + ".sql";
     String statement;
 
     {
@@ -499,4 +499,9 @@ void DatabaseOrdinary::alterTable(
     }
 }
 
+String DatabaseOrdinary::getDataPath(const Context &) const
+{
+    return data_path;
+}
+
 }
diff --git a/dbms/src/Databases/DatabaseOrdinary.h b/dbms/src/Databases/DatabaseOrdinary.h
index 323e012b2691..efaabc590334 100644
--- a/dbms/src/Databases/DatabaseOrdinary.h
+++ b/dbms/src/Databases/DatabaseOrdinary.h
@@ -13,10 +13,11 @@ namespace DB
 class DatabaseOrdinary : public DatabaseMemory
 {
 protected:
-    const String path;
+    const String metadata_path;
+    const String data_path;
 
 public:
-    DatabaseOrdinary(const String & name_, const String & path_);
+    DatabaseOrdinary(const String & name_, const String & metadata_path_, const Context & context);
 
     String getEngineName() const override { return "Ordinary"; }
 
@@ -58,6 +59,8 @@ class DatabaseOrdinary : public DatabaseMemory
         const Context & context,
         const String & table_name) const override;
 
+    String getDataPath(const Context & context) const override;
+
     void shutdown() override;
     void drop() override;
 
diff --git a/dbms/src/Databases/IDatabase.h b/dbms/src/Databases/IDatabase.h
index f1c3b90f6aa2..3f30e83dfcbc 100644
--- a/dbms/src/Databases/IDatabase.h
+++ b/dbms/src/Databases/IDatabase.h
@@ -129,6 +129,9 @@ class IDatabase : public std::enable_shared_from_this<IDatabase>
         const Context & context,
         const String & name) const = 0;
 
+    /// Returns path for persistent data storage if the database supports it, empty string otherwise
+    virtual String getDataPath(const Context & context) const = 0;
+
     /// Ask all tables to complete the background threads they are using and delete all table objects.
     virtual void shutdown() = 0;
 
diff --git a/dbms/src/Interpreters/Cluster.cpp b/dbms/src/Interpreters/Cluster.cpp
index 81954444c750..7b4857431b62 100644
--- a/dbms/src/Interpreters/Cluster.cpp
+++ b/dbms/src/Interpreters/Cluster.cpp
@@ -132,19 +132,26 @@ Clusters::Clusters(Poco::Util::AbstractConfiguration & config, const Settings &
 
 ClusterPtr Clusters::getCluster(const std::string & cluster_name) const
 {
-    std::lock_guard<std::mutex> lock(mutex);
+    std::lock_guard lock(mutex);
 
     auto it = impl.find(cluster_name);
     return (it != impl.end()) ? it->second : nullptr;
 }
 
 
+void Clusters::setCluster(const String & cluster_name, const std::shared_ptr<Cluster> & cluster)
+{
+    std::lock_guard lock(mutex);
+    impl[cluster_name] = cluster;
+}
+
+
 void Clusters::updateClusters(Poco::Util::AbstractConfiguration & config, const Settings & settings, const String & config_name)
 {
     Poco::Util::AbstractConfiguration::Keys config_keys;
     config.keys(config_name, config_keys);
 
-    std::lock_guard<std::mutex> lock(mutex);
+    std::lock_guard lock(mutex);
 
     for (const auto & key : config_keys)
     {
@@ -163,11 +170,12 @@ void Clusters::updateClusters(Poco::Util::AbstractConfiguration & config, const
 
 Clusters::Impl Clusters::getContainer() const
 {
-    std::lock_guard<std::mutex> lock(mutex);
+    std::lock_guard lock(mutex);
     /// The following line copies container of shared_ptrs to return value under lock
     return impl;
 }
 
+
 /// Implementation of `Cluster` class
 
 Cluster::Cluster(Poco::Util::AbstractConfiguration & config, const Settings & settings, const String & cluster_name)
diff --git a/dbms/src/Interpreters/Cluster.h b/dbms/src/Interpreters/Cluster.h
index 18a418fc47b0..658ffc08a137 100644
--- a/dbms/src/Interpreters/Cluster.h
+++ b/dbms/src/Interpreters/Cluster.h
@@ -97,6 +97,7 @@ class Cluster
         UInt32 shard_num;
         UInt32 weight;
         Addresses local_addresses;
+        /// nullptr if there are no remote addresses
         ConnectionPoolWithFailoverPtr pool;
         bool has_internal_replication;
     };
@@ -168,8 +169,9 @@ class Clusters
     Clusters & operator=(const Clusters &) = delete;
 
     ClusterPtr getCluster(const std::string & cluster_name) const;
+    void setCluster(const String & cluster_name, const ClusterPtr & cluster);
 
-    void updateClusters(Poco::Util::AbstractConfiguration & config, const Settings & settings, const String & config_name = "remote_servers");
+    void updateClusters(Poco::Util::AbstractConfiguration & config, const Settings & settings, const String & config_name);
 
 public:
     using Impl = std::map<String, ClusterPtr>;
diff --git a/dbms/src/Interpreters/Context.cpp b/dbms/src/Interpreters/Context.cpp
index edb3acdd45ac..c3bc60dadd54 100644
--- a/dbms/src/Interpreters/Context.cpp
+++ b/dbms/src/Interpreters/Context.cpp
@@ -1370,13 +1370,27 @@ Clusters & Context::getClusters() const
 
 
 /// On repeating calls updates existing clusters and adds new clusters, doesn't delete old clusters
-void Context::setClustersConfig(const ConfigurationPtr & config)
+void Context::setClustersConfig(const ConfigurationPtr & config, const String & config_name)
 {
     std::lock_guard<std::mutex> lock(shared->clusters_mutex);
 
     shared->clusters_config = config;
-    if (shared->clusters)
-        shared->clusters->updateClusters(*shared->clusters_config, settings);
+
+    if (!shared->clusters)
+        shared->clusters = std::make_unique<Clusters>(*shared->clusters_config, settings, config_name);
+    else
+        shared->clusters->updateClusters(*shared->clusters_config, settings, config_name);
+}
+
+
+void Context::setCluster(const String & cluster_name, const std::shared_ptr<Cluster> & cluster)
+{
+    std::lock_guard<std::mutex> lock(shared->clusters_mutex);
+
+    if (!shared->clusters)
+        throw Exception("Clusters are not set", ErrorCodes::LOGICAL_ERROR);
+
+    shared->clusters->setCluster(cluster_name, cluster);
 }
 
 
diff --git a/dbms/src/Interpreters/Context.h b/dbms/src/Interpreters/Context.h
index 374c48b1fd66..7e7e3622e7b1 100644
--- a/dbms/src/Interpreters/Context.h
+++ b/dbms/src/Interpreters/Context.h
@@ -318,8 +318,10 @@ class Context
     Clusters & getClusters() const;
     std::shared_ptr<Cluster> getCluster(const std::string & cluster_name) const;
     std::shared_ptr<Cluster> tryGetCluster(const std::string & cluster_name) const;
+    void setClustersConfig(const ConfigurationPtr & config, const String & config_name = "remote_servers");
+    /// Sets custom cluster, but doesn't update configuration
+    void setCluster(const String & cluster_name, const std::shared_ptr<Cluster> & cluster);
     void reloadClusterConfig();
-    void setClustersConfig(const ConfigurationPtr & config);
 
     Compiler & getCompiler();
     QueryLog & getQueryLog();
diff --git a/dbms/src/Interpreters/InterpreterCheckQuery.h b/dbms/src/Interpreters/InterpreterCheckQuery.h
index 95a09e2bff79..dc2e9cc8bb1d 100644
--- a/dbms/src/Interpreters/InterpreterCheckQuery.h
+++ b/dbms/src/Interpreters/InterpreterCheckQuery.h
@@ -7,11 +7,13 @@ namespace DB
 {
 
 class Context;
+class Cluster;
 
 class InterpreterCheckQuery : public IInterpreter
 {
 public:
     InterpreterCheckQuery(const ASTPtr & query_ptr_, const Context & context_);
+
     BlockIO execute() override;
 
 private:
@@ -19,6 +21,7 @@ class InterpreterCheckQuery : public IInterpreter
 
 private:
     ASTPtr query_ptr;
+
     const Context & context;
     Block result;
 };
diff --git a/dbms/src/Interpreters/InterpreterCreateQuery.cpp b/dbms/src/Interpreters/InterpreterCreateQuery.cpp
index 4558741cdcaf..8e803a39f7de 100644
--- a/dbms/src/Interpreters/InterpreterCreateQuery.cpp
+++ b/dbms/src/Interpreters/InterpreterCreateQuery.cpp
@@ -103,13 +103,10 @@ BlockIO InterpreterCreateQuery::createDatabase(ASTCreateQuery & create)
 
     String database_name_escaped = escapeForFileName(database_name);
 
-    /// Create directories for tables data and metadata.
+    /// Create directories for tables metadata.
     String path = context.getPath();
-    String data_path = path + "data/" + database_name_escaped + "/";
     String metadata_path = path + "metadata/" + database_name_escaped + "/";
-
     Poco::File(metadata_path).createDirectory();
-    Poco::File(data_path).createDirectory();
 
     DatabasePtr database = DatabaseFactory::get(database_engine_name, database_name, metadata_path, context);
 
@@ -458,13 +455,9 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)
     String current_database = context.getCurrentDatabase();
 
     String database_name = create.database.empty() ? current_database : create.database;
-    String database_name_escaped = escapeForFileName(database_name);
     String table_name = create.table;
     String table_name_escaped = escapeForFileName(table_name);
 
-    String data_path = path + "data/" + database_name_escaped + "/";
-    String metadata_path = path + "metadata/" + database_name_escaped + "/" + table_name_escaped + ".sql";
-
     // If this is a stub ATTACH query, read the query definition from the database
     if (create.attach && !create.storage && !create.columns)
     {
@@ -511,9 +504,13 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)
     {
         std::unique_ptr<DDLGuard> guard;
 
+        String data_path;
+        DatabasePtr database;
+
         if (!create.is_temporary)
         {
-            context.assertDatabaseExists(database_name);
+            database = context.getDatabase(database_name);
+            data_path = database->getDataPath(context);
 
             /** If the table already exists, and the request specifies IF NOT EXISTS,
               *  then we allow concurrent CREATE queries (which do nothing).
@@ -548,7 +545,7 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)
         if (create.is_temporary)
             context.getSessionContext().addExternalTable(table_name, res);
         else
-            context.getDatabase(database_name)->createTable(context, table_name, res, query_ptr);
+            database->createTable(context, table_name, res, query_ptr);
     }
 
     res->startup();
diff --git a/dbms/src/Interpreters/InterpreterDropQuery.cpp b/dbms/src/Interpreters/InterpreterDropQuery.cpp
index 053f0e24d54f..303081a54842 100644
--- a/dbms/src/Interpreters/InterpreterDropQuery.cpp
+++ b/dbms/src/Interpreters/InterpreterDropQuery.cpp
@@ -66,6 +66,7 @@ BlockIO InterpreterDropQuery::execute()
 
     String data_path = path + "data/" + database_name_escaped + "/";
     String metadata_path = path + "metadata/" + database_name_escaped + "/";
+    String database_metadata_path = path + "metadata/" + database_name_escaped + ".sql";
 
     auto database = context.tryGetDatabase(database_name);
     if (!database && !drop.if_exists)
@@ -163,6 +164,11 @@ BlockIO InterpreterDropQuery::execute()
 
         Poco::File(data_path).remove(false);
         Poco::File(metadata_path).remove(false);
+
+        /// Old ClickHouse versions did not store database.sql files
+        Poco::File database_metadata_file(database_metadata_path);
+        if (database_metadata_file.exists())
+            database_metadata_file.remove(false);
     }
 
     return {};
diff --git a/dbms/src/Interpreters/loadMetadata.cpp b/dbms/src/Interpreters/loadMetadata.cpp
index 3f4d6d78b43f..efe8698d70fa 100644
--- a/dbms/src/Interpreters/loadMetadata.cpp
+++ b/dbms/src/Interpreters/loadMetadata.cpp
@@ -136,7 +136,7 @@ void loadMetadataSystem(Context & context)
         Poco::File(global_path + "data/" SYSTEM_DATABASE).createDirectories();
         Poco::File(global_path + "metadata/" SYSTEM_DATABASE).createDirectories();
 
-        auto system_database = std::make_shared<DatabaseOrdinary>(SYSTEM_DATABASE, global_path + "metadata/" SYSTEM_DATABASE);
+        auto system_database = std::make_shared<DatabaseOrdinary>(SYSTEM_DATABASE, global_path + "metadata/" SYSTEM_DATABASE, context);
         context.addDatabase(SYSTEM_DATABASE, system_database);
     }
 
diff --git a/dbms/src/Parsers/ExpressionListParsers.cpp b/dbms/src/Parsers/ExpressionListParsers.cpp
index 5476d0cc3180..50f5ce81e513 100644
--- a/dbms/src/Parsers/ExpressionListParsers.cpp
+++ b/dbms/src/Parsers/ExpressionListParsers.cpp
@@ -82,7 +82,7 @@ bool ParserList::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)
     auto list = std::make_shared<ASTExpressionList>();
     node = list;
 
-    while (1)
+    while (true)
     {
         if (first)
         {
diff --git a/dbms/src/Parsers/parseQuery.cpp b/dbms/src/Parsers/parseQuery.cpp
index 6a886b9b266e..56af3c5d991c 100644
--- a/dbms/src/Parsers/parseQuery.cpp
+++ b/dbms/src/Parsers/parseQuery.cpp
@@ -317,6 +317,21 @@ ASTPtr parseQuery(
 }
 
 
+ASTPtr parseQuery(
+    IParser & parser,
+    const std::string & query,
+    const std::string & query_description)
+{
+    return parseQuery(parser, query.data(), query.data() + query.size(), query_description);
+}
+
+
+ASTPtr parseQuery(IParser & parser, const std::string & query)
+{
+    return parseQuery(parser, query.data(), query.data() + query.size(), parser.getName());
+}
+
+
 std::pair<const char *, bool> splitMultipartQuery(const std::string & queries, std::vector<std::string> & queries_list)
 {
     ASTPtr ast;
@@ -357,4 +372,5 @@ std::pair<const char *, bool> splitMultipartQuery(const std::string & queries, s
     return std::make_pair(begin, pos == end);
 }
 
+
 }
diff --git a/dbms/src/Parsers/parseQuery.h b/dbms/src/Parsers/parseQuery.h
index e13f2993f9ce..727d38ceb2fd 100644
--- a/dbms/src/Parsers/parseQuery.h
+++ b/dbms/src/Parsers/parseQuery.h
@@ -32,6 +32,15 @@ ASTPtr parseQuery(
     const char * end,
     const std::string & description);
 
+ASTPtr parseQuery(
+    IParser & parser,
+    const std::string & query,
+    const std::string & query_description);
+
+ASTPtr parseQuery(
+    IParser & parser,
+    const std::string & query);
+
 
 /** Split queries separated by ; on to list of single queries
   * Returns pointer to the end of last sucessfuly parsed query (first), and true if all queries are sucessfuly parsed (second)
diff --git a/dbms/src/Server/CMakeLists.txt b/dbms/src/Server/CMakeLists.txt
index d40683855055..18440450b176 100644
--- a/dbms/src/Server/CMakeLists.txt
+++ b/dbms/src/Server/CMakeLists.txt
@@ -44,6 +44,9 @@ target_link_libraries (clickhouse-compressor-lib clickhouse_common_io ${Boost_PR
 add_library (clickhouse-format-lib ${SPLIT_SHARED} Format.cpp)
 target_link_libraries (clickhouse-format-lib clickhouse_common_io ${Boost_PROGRAM_OPTIONS_LIBRARY})
 
+add_library (clickhouse-cluster-copier-lib ClusterCopier.cpp)
+target_link_libraries (clickhouse-cluster-copier-lib clickhouse-server-lib clickhouse_functions clickhouse_aggregate_functions clickhouse_table_functions)
+
 if (USE_EMBEDDED_COMPILER)
     link_directories (${LLVM_LIBRARY_DIRS})
     add_subdirectory ("Compiler-${LLVM_VERSION}")
@@ -67,8 +70,11 @@ if (CLICKHOUSE_SPLIT_BINARY)
     target_link_libraries (clickhouse-compressor clickhouse-compressor-lib)
     add_executable (clickhouse-format clickhouse-format.cpp)
     target_link_libraries (clickhouse-format clickhouse-format-lib dbms)
+    add_executable (clickhouse-cluster-copier clickhouse-cluster-copier.cpp)
+    target_link_libraries (clickhouse-cluster-copier clickhouse-cluster-copier-lib)
 
-    set (CLICKHOUSE_ALL_TARGETS clickhouse-server clickhouse-client clickhouse-local clickhouse-benchmark clickhouse-performance-test clickhouse-extract-from-config clickhouse-format)
+    set (CLICKHOUSE_ALL_TARGETS clickhouse-server clickhouse-client clickhouse-local clickhouse-benchmark clickhouse-performance-test
+            clickhouse-extract-from-config clickhouse-format clickhouse-cluster-copier)
 
     if (USE_EMBEDDED_COMPILER)
         add_executable (clickhouse-clang clickhouse-clang.cpp)
@@ -100,6 +106,7 @@ else ()
         clickhouse-extract-from-config-lib
         clickhouse-compressor-lib
         clickhouse-format-lib
+        clickhouse-cluster-copier-lib
     )
 
     add_custom_target (clickhouse-server ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-server DEPENDS clickhouse)
@@ -110,6 +117,7 @@ else ()
     add_custom_target (clickhouse-extract-from-config ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-extract-from-config DEPENDS clickhouse)
     add_custom_target (clickhouse-compressor ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-compressor DEPENDS clickhouse)
     add_custom_target (clickhouse-format ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-format DEPENDS clickhouse)
+    add_custom_target (clickhouse-cluster-copier ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-cluster-copier DEPENDS clickhouse)
     # install always because depian package want this files:
     add_custom_target (clickhouse-clang ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-clang DEPENDS clickhouse)
     add_custom_target (clickhouse-lld ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-lld DEPENDS clickhouse)
@@ -124,6 +132,7 @@ else ()
        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-extract-from-config
        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-compressor
        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-format
+       ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-cluster-copier
        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-clang
        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-lld
        DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT clickhouse)
diff --git a/dbms/src/Server/ClusterCopier.cpp b/dbms/src/Server/ClusterCopier.cpp
new file mode 100644
index 000000000000..8af339d1a730
--- /dev/null
+++ b/dbms/src/Server/ClusterCopier.cpp
@@ -0,0 +1,1641 @@
+#include "ClusterCopier.h"
+
+#include <chrono>
+
+#include <Poco/Util/XMLConfiguration.h>
+#include <Poco/Logger.h>
+#include <Poco/ConsoleChannel.h>
+#include <Poco/FormattingChannel.h>
+#include <Poco/PatternFormatter.h>
+#include <Poco/UUIDGenerator.h>
+#include <Poco/File.h>
+#include <Poco/Process.h>
+#include <Poco/FileChannel.h>
+#include <Poco/SplitterChannel.h>
+#include <Poco/Util/HelpFormatter.h>
+
+#include <boost/algorithm/string.hpp>
+#include <pcg_random.hpp>
+
+#include <Common/Exception.h>
+#include <Common/ZooKeeper/ZooKeeper.h>
+#include <Common/getFQDNOrHostName.h>
+#include <Client/Connection.h>
+#include <Interpreters/Context.h>
+#include <Interpreters/Cluster.h>
+#include <Interpreters/InterpreterSelectQuery.h>
+#include <Interpreters/InterpreterInsertQuery.h>
+#include <Interpreters/InterpreterFactory.h>
+#include <Interpreters/InterpreterDropQuery.h>
+#include <Interpreters/InterpreterCreateQuery.h>
+
+#include <common/logger_useful.h>
+#include <common/ThreadPool.h>
+#include <Common/typeid_cast.h>
+#include <Common/ClickHouseRevision.h>
+#include <Common/escapeForFileName.h>
+#include <Columns/ColumnString.h>
+#include <Columns/ColumnsNumber.h>
+#include <Storages/StorageDistributed.h>
+#include <Parsers/ParserCreateQuery.h>
+#include <Parsers/parseQuery.h>
+#include <Parsers/ParserQuery.h>
+#include <Parsers/ASTCreateQuery.h>
+#include <Parsers/queryToString.h>
+#include <Parsers/ASTDropQuery.h>
+#include <Parsers/ASTLiteral.h>
+#include <Databases/DatabaseMemory.h>
+#include <DataStreams/RemoteBlockInputStream.h>
+#include <DataStreams/SquashingBlockInputStream.h>
+#include <Common/isLocalAddress.h>
+#include <DataStreams/copyData.h>
+#include <DataStreams/NullBlockOutputStream.h>
+#include <IO/Operators.h>
+#include <IO/ReadBufferFromString.h>
+#include <Functions/registerFunctions.h>
+#include <TableFunctions/registerTableFunctions.h>
+#include <AggregateFunctions/registerAggregateFunctions.h>
+#include <Server/StatusFile.h>
+#include <Storages/registerStorages.h>
+
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int NO_ZOOKEEPER;
+    extern const int BAD_ARGUMENTS;
+    extern const int UNKNOWN_TABLE;
+    extern const int UNFINISHED;
+    extern const int UNKNOWN_ELEMENT_IN_CONFIG;
+}
+
+
+using ConfigurationPtr = Poco::AutoPtr<Poco::Util::AbstractConfiguration>;
+
+static ConfigurationPtr getConfigurationFromXMLString(const std::string & xml_data)
+{
+    std::stringstream ss(xml_data);
+    Poco::XML::InputSource input_source{ss};
+    return {new Poco::Util::XMLConfiguration{&input_source}};
+}
+
+namespace
+{
+
+using DatabaseAndTableName = std::pair<String, String>;
+
+
+enum class TaskState
+{
+    Started = 0,
+    Finished,
+    Unknown
+};
+
+
+/// Used to mark status of shard partition tasks
+struct TaskStateWithOwner
+{
+    TaskStateWithOwner() = default;
+    TaskStateWithOwner(TaskState state, const String & owner) : state(state), owner(owner) {}
+
+    TaskState state{TaskState::Unknown};
+    String owner;
+
+    static String getData(TaskState state, const String & owner)
+    {
+        return TaskStateWithOwner(state, owner).toString();
+    }
+
+    String toString()
+    {
+        WriteBufferFromOwnString wb;
+        wb << static_cast<UInt32>(state) << "
" << escape << owner;
+        return wb.str();
+    }
+
+    static TaskStateWithOwner fromString(const String & data)
+    {
+        ReadBufferFromString rb(data);
+        TaskStateWithOwner res;
+        UInt32 state;
+
+        rb >> state >> "
" >> escape >> res.owner;
+
+        if (state >= static_cast<int>(TaskState::Unknown))
+            throw Exception("Unknown state " + data, ErrorCodes::LOGICAL_ERROR);
+
+        res.state = static_cast<TaskState>(state);
+        return res;
+    }
+};
+
+
+/// Hierarchical description of the tasks
+struct TaskPartition;
+struct TaskShard;
+struct TaskTable;
+struct TaskCluster;
+
+using TasksPartition = std::map<String, TaskPartition>;
+using ShardInfo = Cluster::ShardInfo;
+using TaskShardPtr = std::shared_ptr<TaskShard>;
+using TasksShard = std::vector<TaskShardPtr>;
+using TasksTable = std::list<TaskTable>;
+using PartitionToShards = std::map<String, TasksShard>;
+
+struct TaskPartition
+{
+    TaskPartition(TaskShard & parent, const String & name_) : task_shard(parent), name(name_) {}
+
+    String getPartitionPath() const;
+    String getCommonPartitionIsDirtyPath() const;
+    String getPartitionActiveWorkersPath() const;
+    String getActiveWorkerPath() const;
+    String getPartitionShardsPath() const;
+    String getShardStatusPath() const;
+
+    TaskShard & task_shard;
+    String name;
+};
+
+
+struct ShardPriority
+{
+    UInt8 is_remote = 1;
+    size_t hostname_difference = 0;
+    UInt8 random = 0;
+
+    static bool greaterPriority(const ShardPriority & current, const ShardPriority & other)
+    {
+        return std::forward_as_tuple(current.is_remote, current.hostname_difference, current.random)
+               < std::forward_as_tuple(other.is_remote, other.hostname_difference, other.random);
+    }
+};
+
+
+struct TaskShard
+{
+    TaskShard(TaskTable & parent, const ShardInfo & info_) : task_table(parent), info(info_) {}
+
+    TaskTable & task_table;
+
+    ShardInfo info;
+    UInt32 numberInCluster() const { return info.shard_num; }
+    UInt32 indexInCluster() const { return info.shard_num - 1; }
+
+    TasksPartition partitions;
+
+    ShardPriority priority;
+};
+
+struct TaskTable
+{
+    TaskTable(TaskCluster & parent, const Poco::Util::AbstractConfiguration & config, const String & prefix,
+                  const String & table_key);
+
+    TaskCluster & task_cluster;
+
+    String getPartitionPath(const String & partition_name) const;
+    String getPartitionIsDirtyPath(const String & partition_name) const;
+
+    /// Used as task ID
+    String name_in_config;
+
+    /// Source cluster and table
+    String cluster_pull_name;
+    DatabaseAndTableName table_pull;
+
+    /// Destination cluster and table
+    String cluster_push_name;
+    DatabaseAndTableName table_push;
+
+    /// Storage of destination table
+    String engine_push_str;
+    ASTPtr engine_push_ast;
+
+    /// Local Distributed table used to split data
+    DatabaseAndTableName table_split;
+    String sharding_key_str;
+    ASTPtr sharding_key_ast;
+    ASTPtr engine_split_ast;
+
+    /// Additional WHERE expression to filter input data
+    String where_condition_str;
+    ASTPtr where_condition_ast;
+
+    /// Resolved clusters
+    ClusterPtr cluster_pull;
+    ClusterPtr cluster_push;
+
+    /// Filter partitions that should be copied
+    bool has_enabled_partitions = false;
+    NameSet enabled_partitions;
+
+    /// Prioritized list of shards
+    TasksShard all_shards;
+    TasksShard local_shards;
+
+    PartitionToShards partition_to_shards;
+
+    template <typename RandomEngine>
+    void initShards(RandomEngine && random_engine);
+};
+
+struct TaskCluster
+{
+    TaskCluster(const String & task_zookeeper_path_, const Poco::Util::AbstractConfiguration & config, const String & base_key, const String & default_local_database_);
+
+    /// Base node for all tasks. Its structure:
+    ///  workers/ - directory with active workers (amount of them is less or equal max_workers)
+    ///  description - node with task configuration
+    ///  table_table1/ - directories with per-partition copying status
+    String task_zookeeper_path;
+
+    /// Limits number of simultaneous workers
+    size_t max_workers = 0;
+
+    /// Settings used to fetch data
+    Settings settings_pull;
+    /// Settings used to insert data
+    Settings settings_push;
+
+    /// Subtasks
+    TasksTable table_tasks;
+
+    /// Database used to create temporary Distributed tables
+    String default_local_database;
+
+    /// Path to remote_servers in task config
+    String clusters_prefix;
+
+    std::random_device random_device;
+    pcg64 random_engine;
+};
+
+
+String getDatabaseDotTable(const String & database, const String & table)
+{
+    return backQuoteIfNeed(database) + "." + backQuoteIfNeed(table);
+}
+
+String getDatabaseDotTable(const DatabaseAndTableName & db_and_table)
+{
+    return getDatabaseDotTable(db_and_table.first, db_and_table.second);
+}
+
+
+/// Atomically checks that is_dirty node is not exists, and made the remaining op
+/// Returns relative number of failed operation in the second field (the passed op has 0 index)
+static void checkNoNodeAndCommit(
+    const zkutil::ZooKeeperPtr & zookeeper,
+    const String & checking_node_path,
+    zkutil::OpPtr && op,
+    zkutil::MultiTransactionInfo & info)
+{
+    zkutil::Ops ops;
+    ops.emplace_back(std::make_unique<zkutil::Op::Create>(checking_node_path, "", zookeeper->getDefaultACL(), zkutil::CreateMode::Persistent));
+    ops.emplace_back(std::make_unique<zkutil::Op::Remove>(checking_node_path, -1));
+    ops.emplace_back(std::move(op));
+
+    zookeeper->tryMultiUnsafe(ops, info);
+    if (info.code != ZOK && !zkutil::isUserError(info.code))
+        throw info.getException();
+}
+
+
+// Creates AST representing 'ENGINE = Distributed(cluster, db, table, [sharding_key])
+std::shared_ptr<ASTStorage> createASTStorageDistributed(
+    const String & cluster_name, const String & database, const String & table, const ASTPtr & sharding_key_ast = nullptr)
+{
+    auto args = std::make_shared<ASTExpressionList>();
+    args->children.emplace_back(std::make_shared<ASTLiteral>(StringRange(nullptr, nullptr), cluster_name));
+    args->children.emplace_back(std::make_shared<ASTIdentifier>(StringRange(nullptr, nullptr), database));
+    args->children.emplace_back(std::make_shared<ASTIdentifier>(StringRange(nullptr, nullptr), table));
+    if (sharding_key_ast)
+        args->children.emplace_back(sharding_key_ast);
+
+    auto engine = std::make_shared<ASTFunction>();
+    engine->name = "Distributed";
+    engine->arguments = args;
+
+    auto storage = std::make_shared<ASTStorage>();
+    storage->set(storage->engine, engine);
+
+    return storage;
+}
+
+
+BlockInputStreamPtr squashStreamIntoOneBlock(const BlockInputStreamPtr & stream)
+{
+    return std::make_shared<SquashingBlockInputStream>(
+        stream,
+        std::numeric_limits<size_t>::max(),
+        std::numeric_limits<size_t>::max()
+    );
+}
+
+Block getBlockWithAllStreamData(const BlockInputStreamPtr & stream)
+{
+    return squashStreamIntoOneBlock(stream)->read();
+}
+
+// Path getters
+
+String TaskTable::getPartitionPath(const String & partition_name) const
+{
+    return task_cluster.task_zookeeper_path                     // root
+           + "/tables/" + escapeForFileName(name_in_config)     // tables/table_hits
+           + "/" + partition_name;                              // 201701
+}
+
+String TaskPartition::getPartitionPath() const
+{
+    return task_shard.task_table.getPartitionPath(name);
+}
+
+String TaskPartition::getShardStatusPath() const
+{
+    // /root/table_test.hits/201701/1
+    return getPartitionPath() + "/shards/" + toString(task_shard.numberInCluster());
+}
+
+String TaskPartition::getPartitionShardsPath() const
+{
+    return getPartitionPath() + "/shards";
+}
+
+String TaskPartition::getPartitionActiveWorkersPath() const
+{
+    return getPartitionPath() + "/partition_active_workers";
+}
+
+String TaskPartition::getActiveWorkerPath() const
+{
+    return getPartitionActiveWorkersPath() + "/" + toString(task_shard.numberInCluster());
+}
+
+String TaskPartition::getCommonPartitionIsDirtyPath() const
+{
+    return getPartitionPath() + "/is_dirty";
+}
+
+String TaskTable::getPartitionIsDirtyPath(const String & partition_name) const
+{
+    return getPartitionPath(partition_name) + "/is_dirty";
+}
+
+
+TaskTable::TaskTable(TaskCluster & parent, const Poco::Util::AbstractConfiguration & config, const String & prefix_,
+                     const String & table_key)
+: task_cluster(parent)
+{
+    String table_prefix = prefix_ + "." + table_key + ".";
+
+    name_in_config = table_key;
+
+    cluster_pull_name = config.getString(table_prefix + "cluster_pull");
+    cluster_push_name = config.getString(table_prefix + "cluster_push");
+
+    table_pull.first = config.getString(table_prefix + "database_pull");
+    table_pull.second = config.getString(table_prefix + "table_pull");
+
+    table_push.first = config.getString(table_prefix + "database_push");
+    table_push.second = config.getString(table_prefix + "table_push");
+
+    engine_push_str = config.getString(table_prefix + "engine");
+    {
+        ParserStorage parser_storage;
+        engine_push_ast = parseQuery(parser_storage, engine_push_str);
+    }
+
+    sharding_key_str = config.getString(table_prefix + "sharding_key");
+    {
+        ParserExpressionWithOptionalAlias parser_expression(false);
+        sharding_key_ast = parseQuery(parser_expression, sharding_key_str);
+        engine_split_ast = createASTStorageDistributed(cluster_push_name, table_push.first, table_push.second, sharding_key_ast);
+
+        table_split = DatabaseAndTableName(task_cluster.default_local_database, ".split." + name_in_config);
+    }
+
+    where_condition_str = config.getString(table_prefix + "where_condition", "");
+    if (!where_condition_str.empty())
+    {
+        ParserExpressionWithOptionalAlias parser_expression(false);
+        where_condition_ast = parseQuery(parser_expression, where_condition_str);
+
+        // Will use canonical expression form
+        where_condition_str = queryToString(where_condition_ast);
+    }
+
+    String enabled_partitions_prefix = table_prefix + "enabled_partitions";
+    has_enabled_partitions = config.has(enabled_partitions_prefix);
+
+    if (has_enabled_partitions)
+    {
+        Strings keys;
+        config.keys(enabled_partitions_prefix, keys);
+
+        Strings partitions;
+        if (keys.empty())
+        {
+            /// Parse list of partition from space-separated string
+            String partitions_str = config.getString(table_prefix + "enabled_partitions");
+            boost::trim_if(partitions_str, isWhitespaceASCII);
+            boost::split(partitions, partitions_str, isWhitespaceASCII, boost::token_compress_on);
+        }
+        else
+        {
+            /// Parse sequence of <partition>...</partition>
+            for (const String & key : keys)
+            {
+                if (!startsWith(key, "partition"))
+                    throw Exception("Unknown key " + key + " in " + enabled_partitions_prefix, ErrorCodes::UNKNOWN_ELEMENT_IN_CONFIG);
+
+                partitions.emplace_back(config.getString(enabled_partitions_prefix + "." + key));
+            }
+        }
+
+        std::copy(partitions.begin(), partitions.end(), std::inserter(enabled_partitions, enabled_partitions.begin()));
+    }
+
+}
+
+
+static ShardPriority getReplicasPriority(const Cluster::Addresses & replicas, const std::string & local_hostname, UInt8 random)
+{
+    ShardPriority res;
+
+    if (replicas.empty())
+        return res;
+
+    res.is_remote = 1;
+    for (auto & replica : replicas)
+    {
+        if (isLocalAddress(replica.resolved_address))
+        {
+            res.is_remote = 0;
+            break;
+        }
+    }
+
+    res.hostname_difference = std::numeric_limits<size_t>::max();
+    for (auto & replica : replicas)
+    {
+        size_t difference = getHostNameDifference(local_hostname, replica.host_name);
+        res.hostname_difference = std::min(difference, res.hostname_difference);
+    }
+
+    res.random = random;
+    return res;
+}
+
+template<typename RandomEngine>
+void TaskTable::initShards(RandomEngine && random_engine)
+{
+    const String & fqdn_name = getFQDNOrHostName();
+    std::uniform_int_distribution<UInt8> get_urand(0, std::numeric_limits<UInt8>::max());
+
+    // Compute the priority
+    for (auto & shard_info : cluster_pull->getShardsInfo())
+    {
+        TaskShardPtr task_shard = std::make_shared<TaskShard>(*this, shard_info);
+        const auto & replicas = cluster_pull->getShardsAddresses().at(task_shard->indexInCluster());
+        task_shard->priority = getReplicasPriority(replicas, fqdn_name, get_urand(random_engine));
+
+        all_shards.emplace_back(task_shard);
+    }
+
+    // Sort by priority
+    std::sort(all_shards.begin(), all_shards.end(),
+        [] (const TaskShardPtr & lhs, const TaskShardPtr & rhs)
+        {
+            return ShardPriority::greaterPriority(lhs->priority, rhs->priority);
+        });
+
+    // Cut local shards
+    auto it_first_remote = std::lower_bound(all_shards.begin(), all_shards.end(), 1,
+        [] (const TaskShardPtr & lhs, UInt8 is_remote)
+        {
+            return lhs->priority.is_remote < is_remote;
+        });
+
+    local_shards.assign(all_shards.begin(), it_first_remote);
+}
+
+TaskCluster::TaskCluster(const String & task_zookeeper_path_, const Poco::Util::AbstractConfiguration & config, const String & base_key,
+                         const String & default_local_database_)
+{
+    String prefix = base_key.empty() ? "" : base_key + ".";
+
+    task_zookeeper_path = task_zookeeper_path_;
+
+    default_local_database = default_local_database_;
+
+    max_workers = config.getUInt64(prefix + "max_workers");
+
+    if (config.has(prefix + "settings"))
+    {
+        settings_pull.loadSettingsFromConfig(prefix + "settings", config);
+        settings_push.loadSettingsFromConfig(prefix + "settings", config);
+    }
+
+    if (config.has(prefix + "settings_pull"))
+        settings_pull.loadSettingsFromConfig(prefix + "settings_pull", config);
+
+    if (config.has(prefix + "settings_push"))
+        settings_push.loadSettingsFromConfig(prefix + "settings_push", config);
+
+    clusters_prefix = prefix + "remote_servers";
+
+    if (!config.has(clusters_prefix))
+        throw Exception("You should specify list of clusters in " + clusters_prefix, ErrorCodes::BAD_ARGUMENTS);
+
+    Poco::Util::AbstractConfiguration::Keys tables_keys;
+    config.keys(prefix + "tables", tables_keys);
+
+    for (const auto & table_key : tables_keys)
+    {
+        table_tasks.emplace_back(*this, config, prefix + "tables", table_key);
+    }
+}
+
+} // end of an anonymous namespace
+
+
+class ClusterCopier
+{
+public:
+
+    ClusterCopier(const ConfigurationPtr & zookeeper_config_,
+                  const String & task_path_,
+                  const String & host_id_,
+                  const String & proxy_database_name_,
+                  Context & context_)
+    :
+        zookeeper_config(zookeeper_config_),
+        task_zookeeper_path(task_path_),
+        host_id(host_id_),
+        working_database_name(proxy_database_name_),
+        context(context_),
+        log(&Poco::Logger::get("ClusterCopier"))
+    {
+        initZooKeeper();
+    }
+
+    void init()
+    {
+        String description_path = task_zookeeper_path + "/description";
+        String task_config_str = getZooKeeper()->get(description_path);
+
+        task_cluster_config = getConfigurationFromXMLString(task_config_str);
+        task_cluster = std::make_unique<TaskCluster>(task_zookeeper_path, *task_cluster_config, "", working_database_name);
+
+        /// Override important settings
+        Settings & settings_pull = task_cluster->settings_pull;
+        settings_pull.load_balancing = LoadBalancing::NEAREST_HOSTNAME;
+        settings_pull.limits.readonly = 1;
+        settings_pull.max_threads = 1;
+        settings_pull.max_block_size = std::min(8192UL, settings_pull.max_block_size.value);
+        settings_pull.preferred_block_size_bytes = 0;
+
+        Settings & settings_push = task_cluster->settings_push;
+        settings_push.insert_distributed_timeout = 0;
+        settings_push.insert_distributed_sync = 1;
+
+        /// Set up clusters
+        context.setClustersConfig(task_cluster_config, task_cluster->clusters_prefix);
+
+        /// Set up shards and their priority
+        task_cluster->random_engine.seed(task_cluster->random_device());
+        for (auto & task_table : task_cluster->table_tasks)
+        {
+            task_table.cluster_pull = context.getCluster(task_table.cluster_pull_name);
+            task_table.cluster_push = context.getCluster(task_table.cluster_push_name);
+            task_table.initShards(task_cluster->random_engine);
+        }
+
+        LOG_DEBUG(log, "Loaded " << task_cluster->table_tasks.size() << " table tasks");
+
+        /// Compute set of partitions, set of partitions aren't changed
+        for (auto & task_table : task_cluster->table_tasks)
+        {
+            for (const TaskShardPtr & task_shard : task_table.all_shards)
+            {
+                if (task_shard->info.pool == nullptr)
+                {
+                    throw Exception("It is impossible to have only local shards, at least port number must be different",
+                                    ErrorCodes::LOGICAL_ERROR);
+                }
+
+                LOG_DEBUG(log, "Set up table task " << task_table.name_in_config << " ("
+                               << "cluster " << task_table.cluster_pull_name
+                               << ", table " << getDatabaseDotTable(task_table.table_pull)
+                               << ", shard " << task_shard->info.shard_num << ")");
+
+                LOG_DEBUG(log, "There are "
+                    << task_table.all_shards.size() << " shards, "
+                    << task_table.local_shards.size() << " of them are remote ones");
+
+                auto connection_entry = task_shard->info.pool->get(&task_cluster->settings_pull);
+                LOG_DEBUG(log, "Will get meta information for shard " << task_shard->numberInCluster()
+                               << " from replica " << connection_entry->getDescription());
+
+                Strings partitions = getRemotePartitions(task_table.table_pull, *connection_entry, &task_cluster->settings_pull);
+                for (const String & partition_name : partitions)
+                {
+                    /// Do not process partition if it is not in enabled_partitions list
+                    if (task_table.has_enabled_partitions && !task_table.enabled_partitions.count(partition_name))
+                    {
+                        LOG_DEBUG(log, "Will skip partition " << partition_name);
+                        continue;
+                    }
+
+                    task_shard->partitions.emplace(partition_name, TaskPartition(*task_shard, partition_name));
+                    task_table.partition_to_shards[partition_name].emplace_back(task_shard);
+                }
+
+                LOG_DEBUG(log, "Will fetch " << task_shard->partitions.size() << " partitions");
+            }
+        }
+
+        auto zookeeper = getZooKeeper();
+        zookeeper->createAncestors(getWorkersPath() + "/");
+    }
+
+    void process()
+    {
+        for (TaskTable & task_table : task_cluster->table_tasks)
+        {
+            if (task_table.all_shards.empty())
+                continue;
+
+            /// An optimization: first of all, try to process all partitions of the local shards
+//            for (const TaskShardPtr & shard : task_table.local_shards)
+//            {
+//                for (auto & task_partition : shard->partitions)
+//                {
+//                    LOG_DEBUG(log, "Processing partition " << task_partition.first << " for local shard " << shard->numberInCluster());
+//                    processPartitionTask(task_partition.second);
+//                }
+//            }
+
+            /// Then check and copy all shards until the whole partition is copied
+            for (const auto & partition_with_shards : task_table.partition_to_shards)
+            {
+                const String & partition_name = partition_with_shards.first;
+                const TasksShard & shards_with_partition = partition_with_shards.second;
+                bool is_done;
+
+                size_t num_tries = 0;
+                constexpr size_t max_tries = 1000;
+
+                Stopwatch watch;
+
+                do
+                {
+                    LOG_DEBUG(log, "Processing partition " << partition_name << " for the whole cluster"
+                        << " (" << shards_with_partition.size() << " shards)");
+
+                    size_t num_successful_shards = 0;
+
+                    for (const TaskShardPtr & shard : shards_with_partition)
+                    {
+                        auto it_shard_partition = shard->partitions.find(partition_name);
+                        if (it_shard_partition == shard->partitions.end())
+                            throw Exception("There are no such partition in a shard. This is a bug.", ErrorCodes::LOGICAL_ERROR);
+
+                        TaskPartition & task_shard_partition = it_shard_partition->second;
+                        if (processPartitionTask(task_shard_partition))
+                            ++num_successful_shards;
+                    }
+
+                    try
+                    {
+                        is_done = (num_successful_shards == shards_with_partition.size())
+                            && checkPartitionIsDone(task_table, partition_name, shards_with_partition);
+                    }
+                    catch (...)
+                    {
+                        tryLogCurrentException(log);
+                        is_done = false;
+                    }
+
+                    if (!is_done)
+                        std::this_thread::sleep_for(default_sleep_time);
+
+                    ++num_tries;
+                } while (!is_done && num_tries < max_tries);
+
+                if (!is_done)
+                    throw Exception("Too many retries while copying partition", ErrorCodes::UNFINISHED);
+                else
+                    LOG_INFO(log, "It took " << watch.elapsedSeconds() << " seconds to copy partition " << partition_name);
+            }
+        }
+    }
+
+    /// Disables DROP PARTITION commands that used to clear data after errors
+    void setSafeMode(bool is_safe_mode_ = true)
+    {
+        is_safe_mode = is_safe_mode_;
+    }
+
+    void setCopyFaultProbability(double copy_fault_probability_)
+    {
+        copy_fault_probability = copy_fault_probability_;
+    }
+
+    /** Checks that the whole partition of a table was copied. We should do it carefully due to dirty lock.
+     * State of some task could be changed during the processing.
+     * We have to ensure that all shards have the finished state and there are no dirty flag.
+     * Moreover, we have to check status twice and check zxid, because state could be changed during the checking.
+     */
+    bool checkPartitionIsDone(const TaskTable & task_table, const String & partition_name, const TasksShard & shards_with_partition)
+    {
+        LOG_DEBUG(log, "Check that all shards processed partition " << partition_name << " successfully");
+
+        auto zookeeper = getZooKeeper();
+
+        Strings status_paths;
+        for (auto & shard : shards_with_partition)
+        {
+            TaskPartition & task_shard_partition = shard->partitions.find(partition_name)->second;
+            status_paths.emplace_back(task_shard_partition.getShardStatusPath());
+        }
+
+        zkutil::Stat stat;
+        std::vector<int64_t> zxid1, zxid2;
+
+        try
+        {
+            // Check that state is Finished and remember zxid
+            for (const String & path : status_paths)
+            {
+                TaskStateWithOwner status = TaskStateWithOwner::fromString(zookeeper->get(path, &stat));
+                if (status.state != TaskState::Finished)
+                {
+                    LOG_INFO(log, "The task " << path << " is being rewritten by " << status.owner
+                                               << ". Partition will be rechecked");
+                    return false;
+                }
+                zxid1.push_back(stat.pzxid);
+            }
+
+            // Check that partition is not dirty
+            if (zookeeper->exists(task_table.getPartitionIsDirtyPath(partition_name)))
+            {
+                LOG_INFO(log, "Partition " << partition_name << " become dirty");
+                return false;
+            }
+
+            // Remember zxid of states again
+            for (const auto & path : status_paths)
+            {
+                zookeeper->exists(path, &stat);
+                zxid2.push_back(stat.pzxid);
+            }
+        }
+        catch (const zkutil::KeeperException & e)
+        {
+            LOG_INFO(log, "A ZooKeeper error occurred while checking partition " << partition_name
+                          << ". Will recheck the partition. Error: " << e.what());
+            return false;
+        }
+
+        // If all task is finished and zxid is not changed then partition could not become dirty again
+        for (size_t shard_num = 0; shard_num < status_paths.size(); ++shard_num)
+        {
+            if (zxid1[shard_num] != zxid2[shard_num])
+            {
+                LOG_INFO(log, "The task " << status_paths[shard_num] << " is being modified now. Partition will be rechecked");
+                return false;
+            }
+        }
+
+        LOG_INFO(log, "Partition " << partition_name << " is copied successfully");
+        return true;
+    }
+
+protected:
+
+    String getWorkersPath() const
+    {
+        return task_cluster->task_zookeeper_path + "/task_active_workers";
+    }
+
+    String getCurrentWorkerNodePath() const
+    {
+        return getWorkersPath() + "/" + host_id;
+    }
+
+    zkutil::EphemeralNodeHolder::Ptr createWorkerNodeAndWaitIfNeed(const zkutil::ZooKeeperPtr & zookeeper, const String & task_description)
+    {
+        while (true)
+        {
+            zkutil::Stat stat;
+            zookeeper->get(getWorkersPath(), &stat);
+
+            if (static_cast<size_t>(stat.numChildren) >= task_cluster->max_workers)
+            {
+                LOG_DEBUG(log, "Too many workers (" << stat.numChildren << ", maximum " << task_cluster->max_workers << ")"
+                    << ". Postpone processing " << task_description);
+                std::this_thread::sleep_for(default_sleep_time);
+            }
+            else
+            {
+                return std::make_shared<zkutil::EphemeralNodeHolder>(getCurrentWorkerNodePath(), *zookeeper, true, false, task_description);
+            }
+        }
+    }
+
+    std::shared_ptr<ASTCreateQuery> rewriteCreateQueryStorage(const ASTPtr & create_query_pull, const DatabaseAndTableName & new_table,
+                                     const ASTPtr & new_storage_ast)
+    {
+        auto & create = typeid_cast<ASTCreateQuery &>(*create_query_pull);
+        auto res = std::make_shared<ASTCreateQuery>(create);
+
+        if (create.storage == nullptr || new_storage_ast == nullptr)
+            throw Exception("Storage is not specified", ErrorCodes::LOGICAL_ERROR);
+
+        res->database = new_table.first;
+        res->table = new_table.second;
+
+        res->children.clear();
+        res->set(res->columns, create.columns->clone());
+        res->set(res->storage, new_storage_ast->clone());
+
+        return res;
+    }
+
+    bool tryDropPartition(TaskPartition & task_partition, const zkutil::ZooKeeperPtr & zookeeper)
+    {
+        if (is_safe_mode)
+            throw Exception("DROP PARTITION is prohibited in safe mode", ErrorCodes::NOT_IMPLEMENTED);
+
+        TaskTable & task_table = task_partition.task_shard.task_table;
+
+        String current_shards_path = task_partition.getPartitionShardsPath();
+        String current_partition_active_workers_dir = task_partition.getPartitionActiveWorkersPath();
+        String is_dirty_flag_path = task_partition.getCommonPartitionIsDirtyPath();
+        String dirt_cleaner_path = is_dirty_flag_path + "/cleaner";
+
+        zkutil::EphemeralNodeHolder::Ptr cleaner_holder;
+        try
+        {
+            cleaner_holder = zkutil::EphemeralNodeHolder::create(dirt_cleaner_path, *zookeeper, host_id);
+        }
+        catch (zkutil::KeeperException & e)
+        {
+            if (e.code == ZNODEEXISTS)
+            {
+                LOG_DEBUG(log, "Partition " << task_partition.name << " is cleaning now by somebody, sleep");
+                std::this_thread::sleep_for(default_sleep_time);
+                return false;
+            }
+
+            throw;
+        }
+
+        zkutil::Stat stat;
+        if (zookeeper->exists(current_partition_active_workers_dir, &stat))
+        {
+            if (stat.numChildren != 0)
+            {
+                LOG_DEBUG(log, "Partition " << task_partition.name << " contains " << stat.numChildren << " active workers, sleep");
+                std::this_thread::sleep_for(default_sleep_time);
+                return false;
+            }
+        }
+
+        /// Remove all status nodes
+        zookeeper->tryRemoveRecursive(current_shards_path);
+
+        String query = "ALTER TABLE " + getDatabaseDotTable(task_table.table_push);
+        query += " DROP PARTITION " + task_partition.name + "";
+
+        /// TODO: use this statement after servers will be updated up to 1.1.54310
+        // query += " DROP PARTITION ID '" + task_partition.name + "'";
+
+        ClusterPtr & cluster_push = task_table.cluster_push;
+        Settings settings_push = task_cluster->settings_push;
+
+        /// It is important, DROP PARTITION must be done synchronously
+        settings_push.replication_alter_partitions_sync = 2;
+
+        LOG_DEBUG(log, "Execute distributed DROP PARTITION: " << query);
+        /// Limit number of max executing replicas to 1
+        size_t num_shards = executeQueryOnCluster(cluster_push, query, nullptr, &settings_push, PoolMode::GET_ALL, 1);
+
+        if (num_shards < cluster_push->getShardCount())
+        {
+            LOG_INFO(log, "DROP PARTITION wasn't successfully executed on " << cluster_push->getShardCount() - num_shards << " shards");
+            return false;
+        }
+
+        /// Remove the locking node
+        cleaner_holder.reset();
+        zookeeper->remove(is_dirty_flag_path);
+
+        LOG_INFO(log, "Partition " << task_partition.name << " was dropped on cluster " << task_table.cluster_push_name);
+        return true;
+    }
+
+    bool processPartitionTask(TaskPartition & task_partition)
+    {
+        try
+        {
+            return processPartitionTaskImpl(task_partition);
+        }
+        catch (...)
+        {
+            tryLogCurrentException(log, "An error occurred while processing partition " + task_partition.name);
+            return false;
+        }
+    }
+
+    bool processPartitionTaskImpl(TaskPartition & task_partition)
+    {
+        TaskShard & task_shard = task_partition.task_shard;
+        TaskTable & task_table = task_shard.task_table;
+
+        auto zookeeper = getZooKeeper();
+        auto acl = zookeeper->getDefaultACL();
+
+        String is_dirty_flag_path = task_partition.getCommonPartitionIsDirtyPath();
+        String current_task_is_active_path = task_partition.getActiveWorkerPath();
+        String current_task_status_path = task_partition.getShardStatusPath();
+
+        /// Auxiliary functions:
+
+        /// Creates is_dirty node to initialize DROP PARTITION
+        auto create_is_dirty_node = [&] ()
+        {
+            auto code = zookeeper->tryCreate(is_dirty_flag_path, current_task_status_path, zkutil::CreateMode::Persistent);
+            if (code != ZOK && code != ZNODEEXISTS)
+                throw zkutil::KeeperException(code, is_dirty_flag_path);
+        };
+
+        /// Returns SELECT query filtering current partition and applying user filter
+        auto get_select_query = [&] (const DatabaseAndTableName & from_table, const String & fields, String limit = "")
+        {
+            String query;
+            query += "SELECT " + fields + " FROM " + getDatabaseDotTable(from_table);
+            query += " WHERE (_part LIKE '" + task_partition.name + "%')";
+            if (!task_table.where_condition_str.empty())
+                query += " AND (" + task_table.where_condition_str + ")";
+            if (!limit.empty())
+                query += " LIMIT " + limit;
+
+            ParserQuery p_query(query.data() + query.size());
+            return parseQuery(p_query, query);
+        };
+
+
+        /// Load balancing
+        auto worker_node_holder = createWorkerNodeAndWaitIfNeed(zookeeper, current_task_status_path);
+
+        LOG_DEBUG(log, "Processing " << current_task_status_path);
+
+        /// Do not start if partition is dirty, try to clean it
+        if (zookeeper->exists(is_dirty_flag_path))
+        {
+            LOG_DEBUG(log, "Partition " << task_partition.name << " is dirty, try to drop it");
+
+            try
+            {
+                tryDropPartition(task_partition, zookeeper);
+            }
+            catch (...)
+            {
+                tryLogCurrentException(log, "An error occurred while clean partition");
+            }
+
+            return false;
+        }
+
+        /// Create ephemeral node to mark that we are active and process the partition
+        zookeeper->createAncestors(current_task_is_active_path);
+        zkutil::EphemeralNodeHolderPtr partition_task_node_holder;
+        try
+        {
+            partition_task_node_holder = zkutil::EphemeralNodeHolder::create(current_task_is_active_path, *zookeeper, host_id);
+        }
+        catch (const zkutil::KeeperException & e)
+        {
+            if (e.code == ZNODEEXISTS)
+            {
+                LOG_DEBUG(log, "Someone is already processing " << current_task_is_active_path);
+                return false;
+            }
+
+            throw;
+        }
+
+        /// Exit if task has been already processed, create blocking node if it is abandoned
+        {
+            String status_data;
+            if (zookeeper->tryGet(current_task_status_path, status_data))
+            {
+                TaskStateWithOwner status = TaskStateWithOwner::fromString(status_data);
+                if (status.state == TaskState::Finished)
+                {
+                    LOG_DEBUG(log, "Task " << current_task_status_path << " has been successfully executed by " << status.owner);
+                    return true;
+                }
+
+                // Task is abandoned, initialize DROP PARTITION
+                LOG_DEBUG(log, "Task " << current_task_status_path << " has not been successfully finished by " << status.owner);
+
+                create_is_dirty_node();
+                return false;
+            }
+        }
+
+        zookeeper->createAncestors(current_task_status_path);
+
+        /// We need to update table definitions for each part, it could be changed after ALTER
+        ASTPtr create_query_pull_ast;
+        {
+            /// Fetch and parse (possibly) new definition
+            auto connection_entry = task_shard.info.pool->get(&task_cluster->settings_pull);
+            String create_query_pull_str = getRemoteCreateTable(task_table.table_pull, *connection_entry, &task_cluster->settings_pull);
+
+            ParserCreateQuery parser_create_query;
+            create_query_pull_ast = parseQuery(parser_create_query, create_query_pull_str);
+        }
+
+        /// Create local Distributed tables:
+        ///  a table fetching data from current shard and a table inserting data to the whole destination cluster
+        DatabaseAndTableName table_shard(working_database_name, ".read_shard." + task_table.name_in_config);
+        DatabaseAndTableName table_split(working_database_name, ".split." + task_table.name_in_config);
+        {
+            /// Create special cluster with single shard
+            String shard_read_cluster_name = ".read_shard." + task_table.cluster_pull_name;
+            ClusterPtr cluster_pull_current_shard = task_table.cluster_pull->getClusterWithSingleShard(task_shard.indexInCluster());
+            context.setCluster(shard_read_cluster_name, cluster_pull_current_shard);
+
+            auto storage_shard_ast = createASTStorageDistributed(shard_read_cluster_name, task_table.table_pull.first, task_table.table_pull.second);
+            const auto & storage_split_ast = task_table.engine_split_ast;
+
+            auto create_table_pull_ast = rewriteCreateQueryStorage(create_query_pull_ast, table_shard, storage_shard_ast);
+            auto create_table_split_ast = rewriteCreateQueryStorage(create_query_pull_ast, table_split, storage_split_ast);
+
+            //LOG_DEBUG(log, "Create shard reading table. Query: " << queryToString(create_table_pull_ast));
+            dropAndCreateLocalTable(create_table_pull_ast);
+
+            //LOG_DEBUG(log, "Create split table. Query: " << queryToString(create_table_split_ast));
+            dropAndCreateLocalTable(create_table_split_ast);
+        }
+
+        /// Check that destination partition is empty if we are first worker
+        /// NOTE: this check is incorrect if pull and push tables have different partition key!
+        {
+            ASTPtr query_select_ast = get_select_query(table_split, "count()");
+            UInt64 count;
+            {
+                Context local_context = context;
+                // Use pull (i.e. readonly) settings, but fetch data from destination servers
+                local_context.getSettingsRef() = task_cluster->settings_pull;
+                local_context.getSettingsRef().skip_unavailable_shards = true;
+
+                InterpreterSelectQuery interperter(query_select_ast, local_context);
+                BlockIO io = interperter.execute();
+
+                Block block = getBlockWithAllStreamData(io.in);
+                count = (block) ? block.safeGetByPosition(0).column->getUInt(0) : 0;
+            }
+
+            if (count != 0)
+            {
+                zkutil::Stat stat_shards;
+                zookeeper->get(task_partition.getPartitionShardsPath(), &stat_shards);
+
+                if (stat_shards.numChildren == 0)
+                {
+                    LOG_WARNING(log, "There are no any workers for partition " << task_partition.name
+                                     << ", but destination table contains " << count << " rows"
+                                     << ". Partition will be dropped and refilled.");
+
+                    create_is_dirty_node();
+                    return false;
+                }
+            }
+        }
+
+        /// Try start processing, create node about it
+        {
+            String start_state = TaskStateWithOwner::getData(TaskState::Started, host_id);
+            auto op_create = std::make_unique<zkutil::Op::Create>(current_task_status_path, start_state, acl, zkutil::CreateMode::Persistent);
+
+            zkutil::MultiTransactionInfo info;
+            checkNoNodeAndCommit(zookeeper, is_dirty_flag_path, std::move(op_create), info);
+
+            if (info.code != ZOK)
+            {
+                if (info.getFailedOp().getPath() == is_dirty_flag_path)
+                {
+                    LOG_INFO(log, "Partition " << task_partition.name << " is dirty and will be dropped and refilled");
+                    return false;
+                }
+
+                throw zkutil::KeeperException(info.code, current_task_status_path);
+            }
+        }
+
+        /// Try create table (if not exists) on each shard
+        {
+            auto create_query_push_ast = rewriteCreateQueryStorage(create_query_pull_ast, task_table.table_push, task_table.engine_push_ast);
+            typeid_cast<ASTCreateQuery &>(*create_query_push_ast).if_not_exists = true;
+            String query = queryToString(create_query_push_ast);
+
+            LOG_DEBUG(log, "Create remote push tables. Query: " << query);
+            executeQueryOnCluster(task_table.cluster_push, query, create_query_push_ast, &task_cluster->settings_push);
+        }
+
+        /// Do the copying
+        {
+            bool inject_fault = false;
+            if (copy_fault_probability > 0)
+            {
+                std::uniform_real_distribution<> get_urand(0, 1);
+                double value = get_urand(task_table.task_cluster.random_engine);
+                inject_fault = value < copy_fault_probability;
+            }
+
+            // Select all fields
+            ASTPtr query_select_ast = get_select_query(table_shard, "*", inject_fault ? "1" : "");
+
+            LOG_DEBUG(log, "Executing SELECT query: " << queryToString(query_select_ast));
+
+            ASTPtr query_insert_ast;
+            {
+                String query;
+                query += "INSERT INTO " + getDatabaseDotTable(table_split) + " VALUES ";
+
+                ParserQuery p_query(query.data() + query.size());
+                query_insert_ast = parseQuery(p_query, query);
+
+                LOG_DEBUG(log, "Executing INSERT query: " << query);
+            }
+
+            try
+            {
+                /// Custom INSERT SELECT implementation
+                Context context_select = context;
+                context_select.getSettingsRef() = task_cluster->settings_pull;
+
+                Context context_insert = context;
+                context_insert.getSettingsRef() = task_cluster->settings_push;
+
+                InterpreterSelectQuery interpreter_select(query_select_ast, context_select);
+                BlockIO io_select = interpreter_select.execute();
+
+                InterpreterInsertQuery interpreter_insert(query_insert_ast, context_insert);
+                BlockIO io_insert = interpreter_insert.execute();
+
+                using ExistsFuture = zkutil::ZooKeeper::ExistsFuture;
+                auto future_is_dirty_checker = std::make_unique<ExistsFuture>(zookeeper->asyncExists(is_dirty_flag_path));
+
+                Stopwatch watch(CLOCK_MONOTONIC_COARSE);
+                constexpr size_t check_period_milliseconds = 500;
+
+                /// Will asynchronously check that ZooKeeper connection and is_dirty flag appearing while copy data
+                auto cancel_check = [&] ()
+                {
+                    if (zookeeper->expired())
+                        throw Exception("ZooKeeper session is expired, cancel INSERT SELECT", ErrorCodes::UNFINISHED);
+
+                    if (future_is_dirty_checker != nullptr)
+                    {
+                        zkutil::ZooKeeper::StatAndExists status;
+                        try
+                        {
+                            status = future_is_dirty_checker->get();
+                            future_is_dirty_checker.reset();
+                        }
+                        catch (zkutil::KeeperException & e)
+                        {
+                            future_is_dirty_checker.reset();
+
+                            if (e.isTemporaryError())
+                                LOG_INFO(log, "ZooKeeper is lagging: " << e.displayText());
+                            else
+                                throw;
+                        }
+
+                        if (status.exists)
+                            throw Exception("Partition is dirty, cancel INSERT SELECT", ErrorCodes::UNFINISHED);
+                    }
+
+                    if (watch.elapsedMilliseconds() >= check_period_milliseconds)
+                    {
+                        watch.restart();
+                        future_is_dirty_checker = std::make_unique<ExistsFuture>(zookeeper->asyncExists(is_dirty_flag_path));
+                    }
+
+                    return false;
+                };
+
+                /// Main work is here
+                copyData(*io_select.in, *io_insert.out, cancel_check);
+
+                // Just in case
+                if (future_is_dirty_checker != nullptr)
+                    future_is_dirty_checker.get();
+
+                if (inject_fault)
+                    throw Exception("Copy fault injection is activated", ErrorCodes::UNFINISHED);
+            }
+            catch (...)
+            {
+                tryLogCurrentException(log, "An error occurred during copying, partition will be marked as dirty");
+                return false;
+            }
+        }
+
+        /// Finalize the processing, change state of current partition task (and also check is_dirty flag)
+        {
+            String state_finished = TaskStateWithOwner::getData(TaskState::Finished, host_id);
+            auto op_set = std::make_unique<zkutil::Op::SetData>(current_task_status_path, state_finished, 0);
+            zkutil::MultiTransactionInfo info;
+            checkNoNodeAndCommit(zookeeper, is_dirty_flag_path, std::move(op_set), info);
+
+            if (info.code != ZOK)
+            {
+                if (info.getFailedOp().getPath() == is_dirty_flag_path)
+                    LOG_INFO(log, "Partition " << task_partition.name << " became dirty and will be dropped and refilled");
+                else
+                    LOG_INFO(log, "Someone made the node abandoned. Will refill partition. " << zkutil::ZooKeeper::error2string(info.code));
+
+                return false;
+            }
+        }
+
+        LOG_INFO(log, "Partition " << task_partition.name << " copied");
+        return true;
+    }
+
+    void dropAndCreateLocalTable(const ASTPtr & create_ast)
+    {
+        auto & create = typeid_cast<ASTCreateQuery &>(*create_ast);
+        dropLocalTableIfExists({create.database, create.table});
+
+        InterpreterCreateQuery interpreter(create_ast, context);
+        interpreter.execute();
+    }
+
+    void dropLocalTableIfExists(const DatabaseAndTableName & table_name) const
+    {
+        auto drop_ast = std::make_shared<ASTDropQuery>();
+        drop_ast->if_exists = true;
+        drop_ast->database = table_name.first;
+        drop_ast->table = table_name.second;
+
+        InterpreterDropQuery interpreter(drop_ast, context);
+        interpreter.execute();
+    }
+
+    bool existsRemoteTable(const DatabaseAndTableName & table, Connection & connection)
+    {
+        String query = "EXISTS " + getDatabaseDotTable(table);
+        Block block = getBlockWithAllStreamData(std::make_shared<RemoteBlockInputStream>(connection, query, context));
+        return block.safeGetByPosition(0).column->getUInt(0) != 0;
+    }
+
+    String getRemoteCreateTable(const DatabaseAndTableName & table, Connection & connection, const Settings * settings = nullptr)
+    {
+        String query = "SHOW CREATE TABLE " + getDatabaseDotTable(table);
+        Block block = getBlockWithAllStreamData(std::make_shared<RemoteBlockInputStream>(connection, query, context, settings));
+
+        return typeid_cast<const ColumnString &>(*block.safeGetByPosition(0).column).getDataAt(0).toString();
+    }
+
+    Strings getRemotePartitions(const DatabaseAndTableName & table, Connection & connection, const Settings * settings = nullptr)
+    {
+        Block block;
+        {
+            WriteBufferFromOwnString wb;
+            wb << "SELECT DISTINCT partition FROM system.parts WHERE"
+               << " database = " << DB::quote << table.first
+               << " AND table = " << DB::quote << table.second;
+
+            block = getBlockWithAllStreamData(std::make_shared<RemoteBlockInputStream>(connection, wb.str(), context, settings));
+        }
+
+        Strings res;
+        if (block)
+        {
+            auto & partition_col = typeid_cast<const ColumnString &>(*block.getByName("partition").column);
+            for (size_t i = 0; i < partition_col.size(); ++i)
+                res.push_back(partition_col.getDataAt(i).toString());
+        }
+        else
+        {
+            if (!existsRemoteTable(table, connection))
+            {
+                throw Exception("Table " + getDatabaseDotTable(table) + " is not exists on server "
+                                + connection.getDescription(), ErrorCodes::UNKNOWN_TABLE);
+            }
+        }
+
+        return res;
+    }
+
+    /** Executes simple query (without output streams, for example DDL queries) on each shard of the cluster
+     * Returns number of shards for which at least one replica executed query successfully
+     */
+    size_t executeQueryOnCluster(
+        const ClusterPtr & cluster,
+        const String & query,
+        const ASTPtr & query_ast_ = nullptr,
+        const Settings * settings = nullptr,
+        PoolMode pool_mode = PoolMode::GET_ALL,
+        size_t max_successful_executions_per_shard = 0) const
+    {
+        auto num_shards = cluster->getShardsInfo().size();
+        std::vector<size_t> per_shard_num_successful_replicas(num_shards, 0);
+
+        ASTPtr query_ast;
+        if (query_ast_ == nullptr)
+        {
+            ParserQuery p_query(query.data() + query.size());
+            query_ast = parseQuery(p_query, query);
+        }
+        else
+            query_ast = query_ast_;
+
+
+        /// We need to execute query on one replica at least
+        auto do_for_shard = [&] (size_t shard_index)
+        {
+            const Cluster::ShardInfo & shard = cluster->getShardsInfo().at(shard_index);
+            size_t & num_successful_executions = per_shard_num_successful_replicas.at(shard_index);
+            num_successful_executions = 0;
+
+            auto increment_and_check_exit = [&] ()
+            {
+                ++num_successful_executions;
+                return max_successful_executions_per_shard && num_successful_executions >= max_successful_executions_per_shard;
+            };
+
+            /// In that case we don't have local replicas, but do it just in case
+            for (size_t i = 0; i < shard.getLocalNodeCount(); ++i)
+            {
+                auto interpreter = InterpreterFactory::get(query_ast, context);
+                interpreter->execute();
+
+                if (increment_and_check_exit())
+                    return;
+            }
+
+            /// Will try to make as many as possible queries
+            if (shard.hasRemoteConnections())
+            {
+                std::vector<IConnectionPool::Entry> connections = shard.pool->getMany(settings, pool_mode);
+
+                for (auto & connection : connections)
+                {
+                    if (!connection.isNull())
+                    {
+                        try
+                        {
+                            RemoteBlockInputStream stream(*connection, query, context, settings);
+                            NullBlockOutputStream output;
+                            copyData(stream, output);
+
+                            if (increment_and_check_exit())
+                                return;
+                        }
+                        catch (const Exception & e)
+                        {
+                            LOG_INFO(log, getCurrentExceptionMessage(false, true));
+                        }
+                    }
+                }
+            }
+        };
+
+        {
+            ThreadPool thread_pool(std::min(num_shards, getNumberOfPhysicalCPUCores()));
+
+            for (size_t shard_index = 0; shard_index < num_shards; ++shard_index)
+                thread_pool.schedule([=] { do_for_shard(shard_index); });
+
+            thread_pool.wait();
+        }
+
+        size_t successful_shards = 0;
+        for (size_t num_replicas : per_shard_num_successful_replicas)
+            successful_shards += (num_replicas > 0);
+
+        return successful_shards;
+    }
+
+    void initZooKeeper()
+    {
+        current_zookeeper = std::make_shared<zkutil::ZooKeeper>(*zookeeper_config, "zookeeper");
+    }
+
+    const zkutil::ZooKeeperPtr & getZooKeeper()
+    {
+        if (!current_zookeeper)
+            throw Exception("Cannot get ZooKeeper", ErrorCodes::NO_ZOOKEEPER);
+
+        return current_zookeeper;
+    }
+
+private:
+    ConfigurationPtr zookeeper_config;
+    String task_zookeeper_path;
+    String host_id;
+    String working_database_name;
+
+    bool is_safe_mode = false;
+    double copy_fault_probability = 0.0;
+
+    ConfigurationPtr task_cluster_config;
+    std::unique_ptr<TaskCluster> task_cluster;
+
+    zkutil::ZooKeeperPtr current_zookeeper;
+
+    Context & context;
+    Poco::Logger * log;
+
+    std::chrono::milliseconds default_sleep_time{1000};
+};
+
+
+/// ClusterCopierApp
+
+
+void ClusterCopierApp::initialize(Poco::Util::Application & self)
+{
+    Poco::Util::Application::initialize(self);
+
+    is_help = config().has("help");
+    if (is_help)
+        return;
+
+    config_xml_path = config().getString("config-file");
+    task_path = config().getString("task-path");
+    log_level = config().getString("log-level", "debug");
+    is_safe_mode = config().has("safe-mode");
+    if (config().has("copy-fault-probability"))
+        copy_fault_probability = std::max(std::min(config().getDouble("copy-fault-probability"), 1.0), 0.0);
+    base_dir = (config().has("base-dir")) ? config().getString("base-dir") : Poco::Path::current();
+
+    // process_id is '<hostname>#<pid>_<start_timestamp>'
+    process_id = std::to_string(Poco::Process::id()) + "_" + std::to_string(Poco::Timestamp().epochTime());
+    host_id = escapeForFileName(getFQDNOrHostName()) + '#' + process_id;
+    process_path = Poco::Path(base_dir + "/clickhouse-copier_" + process_id).absolute().toString();
+    Poco::File(process_path).createDirectories();
+
+    setupLogging();
+
+    std::string stderr_path = process_path + "/stderr";
+    if (!freopen(stderr_path.c_str(), "a+", stderr))
+        throw Poco::OpenFileException("Cannot attach stderr to " + stderr_path);
+}
+
+
+void ClusterCopierApp::handleHelp(const std::string &, const std::string &)
+{
+    Poco::Util::HelpFormatter helpFormatter(options());
+    helpFormatter.setCommand(commandName());
+    helpFormatter.setHeader("Copies tables from one cluster to another");
+    helpFormatter.setUsage("--config-file <config-file> --task-path <task-path>");
+    helpFormatter.format(std::cerr);
+
+    stopOptionsProcessing();
+}
+
+
+void ClusterCopierApp::defineOptions(Poco::Util::OptionSet & options)
+{
+    options.addOption(Poco::Util::Option("config-file", "c", "path to config file with ZooKeeper config", true)
+                          .argument("config-file").binding("config-file"));
+    options.addOption(Poco::Util::Option("task-path", "", "path to task in ZooKeeper")
+                          .argument("task-path").binding("task-path"));
+    options.addOption(Poco::Util::Option("safe-mode", "", "disables ALTER DROP PARTITION in case of errors")
+                          .binding("safe-mode"));
+    options.addOption(Poco::Util::Option("copy-fault-probability", "", "the copying fails with specified probability (used to test partition state recovering)")
+                          .argument("copy-fault-probability").binding("copy-fault-probability"));
+    options.addOption(Poco::Util::Option("log-level", "", "sets log level")
+                          .argument("log-level").binding("log-level"));
+    options.addOption(Poco::Util::Option("base-dir", "", "base directory for copiers, consequitive copier launches will populate /base-dir/launch_id/* directories")
+                          .argument("base-dir").binding("base-dir"));
+
+    using Me = std::decay_t<decltype(*this)>;
+    options.addOption(Poco::Util::Option("help", "", "produce this help message").binding("help")
+                          .callback(Poco::Util::OptionCallback<Me>(this, &Me::handleHelp)));
+}
+
+
+void ClusterCopierApp::setupLogging()
+{
+    Poco::AutoPtr<Poco::SplitterChannel> split_channel(new Poco::SplitterChannel);
+
+    Poco::AutoPtr<Poco::FileChannel> log_file_channel(new Poco::FileChannel);
+    log_file_channel->setProperty("path", process_path + "/log.log");
+    split_channel->addChannel(log_file_channel);
+    log_file_channel->open();
+
+    if (!config().getBool("application.runAsService", true))
+    {
+        Poco::AutoPtr<Poco::ConsoleChannel> console_channel(new Poco::ConsoleChannel);
+        split_channel->addChannel(console_channel);
+        console_channel->open();
+    }
+
+    Poco::AutoPtr<Poco::PatternFormatter> formatter(new Poco::PatternFormatter);
+    formatter->setProperty("pattern", "%L%Y-%m-%d %H:%M:%S.%i <%p> %s: %t");
+    Poco::AutoPtr<Poco::FormattingChannel> formatting_channel(new Poco::FormattingChannel(formatter));
+    formatting_channel->setChannel(split_channel);
+    split_channel->open();
+
+    Poco::Logger::root().setChannel(formatting_channel);
+    Poco::Logger::root().setLevel(log_level);
+}
+
+
+void ClusterCopierApp::mainImpl()
+{
+    ConfigurationPtr zookeeper_configuration(new Poco::Util::XMLConfiguration(config_xml_path));
+    auto log = &logger();
+
+    StatusFile status_file(process_path + "/status");
+
+    LOG_INFO(log, "Starting clickhouse-copier ("
+        << "id " << process_id << ", "
+        << "host_id " << host_id << ", "
+        << "path " << process_path << ", "
+        << "revision " << ClickHouseRevision::get() << ")");
+
+    auto context = std::make_unique<Context>(Context::createGlobal());
+    SCOPE_EXIT(context->shutdown());
+
+    context->setGlobalContext(*context);
+    context->setApplicationType(Context::ApplicationType::LOCAL);
+    context->setPath(process_path);
+
+    registerFunctions();
+    registerAggregateFunctions();
+    registerTableFunctions();
+    registerStorages();
+
+    static const std::string default_database = "_local";
+    context->addDatabase(default_database, std::make_shared<DatabaseMemory>(default_database));
+    context->setCurrentDatabase(default_database);
+
+    std::unique_ptr<ClusterCopier> copier(new ClusterCopier(
+        zookeeper_configuration, task_path, host_id, default_database, *context));
+
+    copier->setSafeMode(is_safe_mode);
+    copier->setCopyFaultProbability(copy_fault_probability);
+    copier->init();
+    copier->process();
+}
+
+
+int ClusterCopierApp::main(const std::vector<std::string> &)
+{
+    if (is_help)
+        return 0;
+
+    try
+    {
+        mainImpl();
+    }
+    catch (...)
+    {
+        std::cerr << DB::getCurrentExceptionMessage(true) << "
";
+        auto code = getCurrentExceptionCode();
+
+        return (code) ? code : -1;
+    }
+
+    return 0;
+}
+
+
+}
+
+
+int mainEntryClickHouseClusterCopier(int argc, char ** argv)
+{
+    try
+    {
+        DB::ClusterCopierApp app;
+        return app.run(argc, argv);
+    }
+    catch (...)
+    {
+        std::cerr << DB::getCurrentExceptionMessage(true) << "
";
+        auto code = DB::getCurrentExceptionCode();
+
+        return (code) ? code : -1;
+    }
+}
diff --git a/dbms/src/Server/ClusterCopier.h b/dbms/src/Server/ClusterCopier.h
new file mode 100644
index 000000000000..d0fb82ed04be
--- /dev/null
+++ b/dbms/src/Server/ClusterCopier.h
@@ -0,0 +1,205 @@
+#pragma once
+#include <Poco/Util/ServerApplication.h>
+
+/* = clickhouse-cluster-copier util =
+ * Copies tables data from one cluster to new tables of other (possibly the same) cluster in distributed fault-tolerant manner.
+ *
+ * Configuration of copying tasks is set in special ZooKeeper node (called the description node).
+ * A ZooKeeper path to the description node is specified via --task-path </task/path> parameter.
+ * So, node /task/path/description should contain special XML content describing copying tasks.
+ *
+ * Simultaneously many clickhouse-cluster-copier processes located on any servers could execute the same task.
+ * ZooKeeper node /task/path/ is used by the processes to coordinate their work.
+ * You must not add additional child nodes to /task/path/.
+ *
+ * Currently you are responsible for launching cluster-copier processes.
+ * You can launch as many processes as you want, whenever and wherever you want.
+ * Each process try to select nearest available shard of source cluster and copy some part of data (partition) from it to the whole
+ * destination cluster with resharding.
+ * Therefore it makes sense to launch cluster-copier processes on the source cluster nodes to reduce the network usage.
+ *
+ * Since the workers coordinate their work via ZooKeeper, in addition to --task-path </task/path> you have to specify ZooKeeper
+ * configuration via --config-file <zookeeper.xml> parameter. Example of zookeeper.xml:
+
+   <yandex>
+    <zookeeper>
+        <node index="1">
+            <host>127.0.0.1</host>
+            <port>2181</port>
+        </node>
+    </zookeeper>
+   </yandex>
+
+ * When you run clickhouse-cluster-copier --config-file <zookeeper.xml> --task-path </task/path>
+ * the process connects to ZooKeeper, reads tasks config from /task/path/description and executes them.
+ *
+ *
+ * = Format of task config =
+
+<yandex>
+    <!-- Configuration of clusters as in an ordinary server config -->
+    <remote_servers>
+        <source_cluster>
+            <shard>
+                <internal_replication>false</internal_replication>
+                    <replica>
+                        <host>127.0.0.1</host>
+                        <port>9000</port>
+                    </replica>
+            </shard>
+            ...
+        </source_cluster>
+
+        <destination_cluster>
+        ...
+        </destination_cluster>
+    </remote_servers>
+
+    <!-- How many simultaneously active workers are possible. If you run more workers superfluous workers will sleep. -->
+    <max_workers>2</max_workers>
+
+    <!-- Setting used to fetch (pull) data from source cluster tables -->
+    <settings_pull>
+        <readonly>1</readonly>
+    </settings_pull>
+
+    <!-- Setting used to insert (push) data to destination cluster tables -->
+    <settings_push>
+        <readonly>0</readonly>
+    </settings_push>
+
+    <!-- Common setting for fetch (pull) and insert (push) operations.
+         They are overlaid by <settings_pull/> and <settings_push/> respectively -->
+    <settings>
+        <insert_distributed_sync>1</insert_distributed_sync>
+    </settings>
+
+    <!-- Copying tasks description.
+         You could specify several table task in the same task description (in the same ZooKeeper node), they will be performed
+         sequentially.
+    -->
+    <tables>
+        <!-- Name of the table task, it must be an unique name suitable for ZooKeeper node name -->
+        <table_hits>
+            <-- Source cluster name (from <remote_servers/> section) and tables in it that should be copied -->
+            <cluster_pull>source_cluster</cluster_pull>
+            <database_pull>test</database_pull>
+            <table_pull>hits</table_pull>
+
+            <-- Destination cluster name and tables in which the data should be inserted -->
+            <cluster_push>destination_cluster</cluster_push>
+            <database_push>test</database_push>
+            <table_push>hits2</table_push>
+
+            <!-- Engine of destination tables.
+                 If destination tables have not be created, workers create them using columns definition from source tables and engine
+                 definition from here.
+
+                 NOTE: If the first worker starts insert data and detects that destination partition is not empty then the partition will
+                 be dropped and refilled, take it into account if you already have some data in destination tables. You could directly
+                 specify partitions that should be copied in <enabled_partitions/>.
+
+                 NOTE: Currently partition key of source and destination tables should be the same.
+            -->
+            <engine>ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/hits2/{shard}/hits2', '{replica}', EventDate, (CounterID, EventDate), 8192)</engine>
+
+            <!-- Sharding key used to insert data to destination cluster -->
+            <sharding_key>intHash32(UserID)</sharding_key>
+
+            <!-- Optional expression that filter data while pull them from source servers -->
+            <where_condition>CounterID != 0</where_condition>
+
+            <!-- Optional section, it specifies partitions that should be copied, other partition will be ignored -->
+            <enabled_partitions>
+                <partition>201712</partition>
+                <partition>201801</partition>
+                ...
+            </enabled_partitions>
+        </table_hits>
+
+        </table_visits>
+        ...
+        </table_visits>
+        ...
+    </tables>
+</yandex>
+
+
+ * = Implementation details =
+ *
+ * cluster-copier workers pull each partition of each shard of the source cluster and push it to the destination cluster through
+ * Distributed table (to preform data resharding). So, worker job is a partition of a source shard.
+ * A job has three states: Active, Finished and Abandoned. Abandoned means that worker died and did not finish the job.
+ *
+ * If an error occurred during the copying (a worker failed or a worker did not finish the INSERT), then the whole partition (on
+ * all destination servers) should be dropped and refilled. So, copying entity is a partition of all destination shards.
+ * If a failure is detected a special /is_dirty node is created in ZooKeeper signalling that other workers copying the same partition
+ * should stop, after a refilling procedure should start.
+ *
+ * ZooKeeper task node has the following structure:
+ *  /task/path_root                     - path passed in --task-path parameter
+ *      /description                    - contains user-defined XML config of the task
+ *      /task_active_workers            - contains ephemeral nodes of all currently active workers, used to implement max_workers limitation
+ *          /server_fqdn#PID_timestamp  - cluster-copier worker ID
+ *          ...
+ *      /tables             - directory with table tasks
+ *      /table_hits         - directory of table_hits task
+ *          /partition1     - directory for partition1
+ *              /shards     - directory for source cluster shards
+ *                  /1      - worker job for the first shard of partition1 of table test.hits
+ *                            Contains info about current status (Active or Finished) and worker ID.
+ *                  /2
+ *                  ...
+ *              /partition_active_workers
+ *                  /1      - for each job in /shards a corresponding ephemeral node created in /partition_active_workers
+ *                            It is used to detect Abandoned jobs (if there is Active node in /shards and there is no node in
+ *                            /partition_active_workers).
+ *                            Also, it is used to track active workers in the partition (when we need to refill the partition we do
+ *                            not DROP PARTITION while there are active workers)
+ *                  /2
+ *                  ...
+ *              /is_dirty   - the node is set if some worker detected that an error occurred (the INSERT is failed or an Abandoned node is
+ *                            detected). If the node appeared workers in this partition should stop and start cleaning and refilling
+ *                            partition procedure.
+ *                            During this procedure a single 'cleaner' worker is selected. The worker waits for stopping all partition
+ *                            workers, removes /shards node, executes DROP PARTITION on each destination node and removes /is_dirty node.
+ *                  /cleaner- An ephemeral node used to select 'cleaner' worker. Contains ID of the worker.
+ *      /test_visits
+ *          ...
+ */
+
+namespace DB
+{
+
+class ClusterCopierApp : public Poco::Util::ServerApplication
+{
+public:
+
+    void initialize(Poco::Util::Application & self) override;
+
+    void handleHelp(const std::string &, const std::string &);
+
+    void defineOptions(Poco::Util::OptionSet & options) override;
+
+    int main(const std::vector<std::string> &) override;
+
+private:
+
+    void mainImpl();
+
+    void setupLogging();
+
+    std::string config_xml_path;
+    std::string task_path;
+    std::string log_level = "debug";
+    bool is_safe_mode = false;
+    double copy_fault_probability = 0;
+    bool is_help = false;
+
+    std::string base_dir;
+    std::string process_path;
+    std::string process_id;
+    std::string host_id;
+};
+
+}
diff --git a/dbms/src/Server/LocalServer.cpp b/dbms/src/Server/LocalServer.cpp
index ff8649b3753c..e3dc7c35f206 100644
--- a/dbms/src/Server/LocalServer.cpp
+++ b/dbms/src/Server/LocalServer.cpp
@@ -463,6 +463,14 @@ static const char * minimal_default_user_xml =
 "</yandex>";
 
 
+static ConfigurationPtr getConfigurationFromXMLString(const char * xml_data)
+{
+    std::stringstream ss{std::string{xml_data}};
+    Poco::XML::InputSource input_source{ss};
+    return {new Poco::Util::XMLConfiguration{&input_source}};
+}
+
+
 void LocalServer::setupUsers()
 {
     ConfigurationPtr users_config;
@@ -477,11 +485,7 @@ void LocalServer::setupUsers()
     }
     else
     {
-        std::stringstream default_user_stream;
-        default_user_stream << minimal_default_user_xml;
-
-        Poco::XML::InputSource default_user_source(default_user_stream);
-        users_config = ConfigurationPtr(new Poco::Util::XMLConfiguration(&default_user_source));
+        users_config = getConfigurationFromXMLString(minimal_default_user_xml);
     }
 
     if (users_config)
diff --git a/dbms/src/Server/clickhouse-cluster-copier.cpp b/dbms/src/Server/clickhouse-cluster-copier.cpp
new file mode 100644
index 000000000000..653a0128aa4e
--- /dev/null
+++ b/dbms/src/Server/clickhouse-cluster-copier.cpp
@@ -0,0 +1,2 @@
+int mainEntryClickHouseClusterCopier(int argc, char ** argv);
+int main(int argc_, char ** argv_) { return mainEntryClickHouseClusterCopier(argc_, argv_); }
diff --git a/dbms/src/Server/main.cpp b/dbms/src/Server/main.cpp
index f923c75d87e6..d84c0ab732d6 100644
--- a/dbms/src/Server/main.cpp
+++ b/dbms/src/Server/main.cpp
@@ -18,6 +18,7 @@ int mainEntryClickHousePerformanceTest(int argc, char ** argv);
 int mainEntryClickHouseExtractFromConfig(int argc, char ** argv);
 int mainEntryClickHouseCompressor(int argc, char ** argv);
 int mainEntryClickHouseFormat(int argc, char ** argv);
+int mainEntryClickHouseClusterCopier(int argc, char ** argv);
 
 #if USE_EMBEDDED_COMPILER
     int mainEntryClickHouseClang(int argc, char ** argv);
@@ -41,6 +42,7 @@ std::pair<const char *, MainFunc> clickhouse_applications[] =
     {"extract-from-config", mainEntryClickHouseExtractFromConfig},
     {"compressor", mainEntryClickHouseCompressor},
     {"format", mainEntryClickHouseFormat},
+    {"copier", mainEntryClickHouseClusterCopier},
 #if USE_EMBEDDED_COMPILER
     {"clang", mainEntryClickHouseClang},
     {"lld", mainEntryClickHouseLLD},
diff --git a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp
index 156e016fac3e..2d2a5c454341 100644
--- a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp
+++ b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp
@@ -53,9 +53,10 @@ namespace ErrorCodes
 }
 
 
-DistributedBlockOutputStream::DistributedBlockOutputStream(StorageDistributed & storage, const ASTPtr & query_ast,
-                                                           const ClusterPtr & cluster_, bool insert_sync_, UInt64 insert_timeout_)
-    : storage(storage), query_ast(query_ast), cluster(cluster_), insert_sync(insert_sync_), insert_timeout(insert_timeout_)
+DistributedBlockOutputStream::DistributedBlockOutputStream(StorageDistributed & storage, const ASTPtr & query_ast, const ClusterPtr & cluster_,
+                                                           const Settings & settings_, bool insert_sync_, UInt64 insert_timeout_)
+    : storage(storage), query_ast(query_ast), cluster(cluster_), settings(settings_), insert_sync(insert_sync_),
+      insert_timeout(insert_timeout_)
 {
 }
 
@@ -392,9 +393,21 @@ void DistributedBlockOutputStream::writeAsyncImpl(const Block & block, const siz
 
 void DistributedBlockOutputStream::writeToLocal(const Block & block, const size_t repeats)
 {
-    InterpreterInsertQuery interp{query_ast, storage.context};
+    std::unique_ptr<Context> local_context;
+    std::optional<InterpreterInsertQuery> interp;
 
-    auto block_io = interp.execute();
+    /// Async insert does not support settings forwarding yet whereas sync one supports
+    if (insert_sync)
+        interp.emplace(query_ast, storage.context);
+    else
+    {
+        /// Overwrite global settings by user settings
+        local_context = std::make_unique<Context>(storage.context);
+        local_context->setSettings(settings);
+        interp.emplace(query_ast, *local_context);
+    }
+
+    auto block_io = interp->execute();
     block_io.out->writePrefix();
 
     for (size_t i = 0; i < repeats; ++i)
@@ -410,7 +423,7 @@ void DistributedBlockOutputStream::writeToShardSync(const Block & block, const s
     auto connection = pool->get();
 
     const auto & query_string = queryToString(query_ast);
-    RemoteBlockOutputStream remote{*connection, query_string};
+    RemoteBlockOutputStream remote{*connection, query_string, &settings};
 
     CurrentMetrics::Increment metric_increment{CurrentMetrics::DistributedSend};
 
diff --git a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.h b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.h
index 526ccb92c752..de8dc12649b7 100644
--- a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.h
+++ b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.h
@@ -32,7 +32,8 @@ class StorageDistributed;
 class DistributedBlockOutputStream : public IBlockOutputStream
 {
 public:
-    DistributedBlockOutputStream(StorageDistributed & storage, const ASTPtr & query_ast, const ClusterPtr & cluster_, bool insert_sync_, UInt64 insert_timeout_);
+    DistributedBlockOutputStream(StorageDistributed & storage, const ASTPtr & query_ast, const ClusterPtr & cluster_,
+                                 const Settings & settings_, bool insert_sync_, UInt64 insert_timeout_);
 
     void write(const Block & block) override;
 
@@ -88,6 +89,7 @@ class DistributedBlockOutputStream : public IBlockOutputStream
     StorageDistributed & storage;
     ASTPtr query_ast;
     ClusterPtr cluster;
+    const Settings & settings;
     bool insert_sync;
     UInt64 insert_timeout;
     size_t blocks_inserted = 0;
diff --git a/dbms/src/Storages/ITableDeclaration.cpp b/dbms/src/Storages/ITableDeclaration.cpp
index b9948c89bfe4..8bc53a8163cf 100644
--- a/dbms/src/Storages/ITableDeclaration.cpp
+++ b/dbms/src/Storages/ITableDeclaration.cpp
@@ -21,6 +21,7 @@ namespace ErrorCodes
     extern const int TYPE_MISMATCH;
     extern const int DUPLICATE_COLUMN;
     extern const int NOT_FOUND_COLUMN_IN_BLOCK;
+    extern const int EMPTY_LIST_OF_COLUMNS_PASSED;
 }
 
 
@@ -289,4 +290,15 @@ void ITableDeclaration::check(const Block & block, bool need_all) const
     }
 }
 
+ITableDeclaration::ITableDeclaration(const NamesAndTypesList & columns, const NamesAndTypesList & materialized_columns,
+                                     const NamesAndTypesList & alias_columns, const ColumnDefaults & column_defaults)
+    : columns{columns},
+      materialized_columns{materialized_columns},
+      alias_columns{alias_columns},
+      column_defaults{column_defaults}
+{
+    if (columns.empty())
+        throw Exception("Empty list of columns passed to storage constructor", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED);
+}
+
 }
diff --git a/dbms/src/Storages/ITableDeclaration.h b/dbms/src/Storages/ITableDeclaration.h
index 4861a36205f4..19c57d45e40e 100644
--- a/dbms/src/Storages/ITableDeclaration.h
+++ b/dbms/src/Storages/ITableDeclaration.h
@@ -84,20 +84,21 @@ class ITableDeclaration
 
     ITableDeclaration() = default;
     ITableDeclaration(
+        const NamesAndTypesList & columns,
         const NamesAndTypesList & materialized_columns,
         const NamesAndTypesList & alias_columns,
-        const ColumnDefaults & column_defaults)
-        : materialized_columns{materialized_columns},
-          alias_columns{alias_columns},
-          column_defaults{column_defaults}
-    {}
+        const ColumnDefaults & column_defaults);
 
+    NamesAndTypesList columns;
     NamesAndTypesList materialized_columns{};
     NamesAndTypesList alias_columns{};
     ColumnDefaults column_defaults{};
 
 private:
-    virtual const NamesAndTypesList & getColumnsListImpl() const = 0;
+    virtual const NamesAndTypesList & getColumnsListImpl() const
+    {
+        return columns;
+    }
 
     using ColumnsListRange = boost::range::joined_range<const NamesAndTypesList, const NamesAndTypesList>;
     /// Returns a lazily joined range of table's ordinary and materialized columns, without unnecessary copying
diff --git a/dbms/src/Storages/MergeTree/MergeTreeData.cpp b/dbms/src/Storages/MergeTree/MergeTreeData.cpp
index c3afacf499e7..aa1698778812 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeData.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeData.cpp
@@ -92,7 +92,8 @@ MergeTreeData::MergeTreeData(
     bool require_part_metadata_,
     bool attach,
     BrokenPartCallback broken_part_callback_)
-    : ITableDeclaration{materialized_columns_, alias_columns_, column_defaults_}, context(context_),
+    : ITableDeclaration{columns_, materialized_columns_, alias_columns_, column_defaults_},
+    context(context_),
     sampling_expression(sampling_expression_),
     index_granularity(settings_.index_granularity),
     merging_params(merging_params_),
@@ -101,7 +102,7 @@ MergeTreeData::MergeTreeData(
     partition_expr_ast(partition_expr_ast_),
     require_part_metadata(require_part_metadata_),
     database_name(database_), table_name(table_),
-    full_path(full_path_), columns(columns_),
+    full_path(full_path_),
     broken_part_callback(broken_part_callback_),
     log_name(database_name + "." + table_name), log(&Logger::get(log_name + " (Data)")),
     data_parts_by_name(data_parts_indexes.get<TagByName>()),
diff --git a/dbms/src/Storages/MergeTree/MergeTreeData.h b/dbms/src/Storages/MergeTree/MergeTreeData.h
index 0a468f99f258..b9797dc710ff 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeData.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeData.h
@@ -550,8 +550,6 @@ class MergeTreeData : public ITableDeclaration
     String table_name;
     String full_path;
 
-    NamesAndTypesList columns;
-
     /// Current column sizes in compressed and uncompressed form.
     ColumnSizes column_sizes;
 
diff --git a/dbms/src/Storages/StorageBuffer.cpp b/dbms/src/Storages/StorageBuffer.cpp
index b932324892df..91e112210848 100644
--- a/dbms/src/Storages/StorageBuffer.cpp
+++ b/dbms/src/Storages/StorageBuffer.cpp
@@ -56,8 +56,8 @@ StorageBuffer::StorageBuffer(const std::string & name_, const NamesAndTypesList
     Context & context_,
     size_t num_shards_, const Thresholds & min_thresholds_, const Thresholds & max_thresholds_,
     const String & destination_database_, const String & destination_table_, bool allow_materialized_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
-    name(name_), columns(columns_), context(context_),
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
+    name(name_), context(context_),
     num_shards(num_shards_), buffers(num_shards_),
     min_thresholds(min_thresholds_), max_thresholds(max_thresholds_),
     destination_database(destination_database_), destination_table(destination_table_),
diff --git a/dbms/src/Storages/StorageBuffer.h b/dbms/src/Storages/StorageBuffer.h
index 98cbcff661c5..9d9f72340c59 100644
--- a/dbms/src/Storages/StorageBuffer.h
+++ b/dbms/src/Storages/StorageBuffer.h
@@ -53,8 +53,6 @@ friend class BufferBlockOutputStream;
     std::string getName() const override { return "Buffer"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -82,7 +80,6 @@ friend class BufferBlockOutputStream;
 
 private:
     String name;
-    NamesAndTypesList columns;
 
     Context & context;
 
diff --git a/dbms/src/Storages/StorageCatBoostPool.h b/dbms/src/Storages/StorageCatBoostPool.h
index 685b951aaf24..34aed30693c9 100644
--- a/dbms/src/Storages/StorageCatBoostPool.h
+++ b/dbms/src/Storages/StorageCatBoostPool.h
@@ -14,8 +14,6 @@ class StorageCatBoostPool : public ext::shared_ptr_helper<StorageCatBoostPool>,
 
     std::string getTableName() const override { return table_name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(const Names & column_names,
                            const SelectQueryInfo & query_info,
                            const Context & context,
@@ -25,7 +23,7 @@ class StorageCatBoostPool : public ext::shared_ptr_helper<StorageCatBoostPool>,
 
 private:
     String table_name;
-    NamesAndTypesList columns;
+
     String column_description_file_name;
     String data_description_file_name;
     Block sample_block;
diff --git a/dbms/src/Storages/StorageDictionary.cpp b/dbms/src/Storages/StorageDictionary.cpp
index 2eb96faf7331..2bb69748c6d8 100644
--- a/dbms/src/Storages/StorageDictionary.cpp
+++ b/dbms/src/Storages/StorageDictionary.cpp
@@ -30,8 +30,8 @@ StorageDictionary::StorageDictionary(
     const ColumnDefaults & column_defaults_,
     const DictionaryStructure & dictionary_structure_,
     const String & dictionary_name_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),
-    columns(columns_), dictionary_name(dictionary_name_),
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),
+    dictionary_name(dictionary_name_),
     logger(&Poco::Logger::get("StorageDictionary"))
 {
     checkNamesAndTypesCompatibleWithDictionary(dictionary_structure_);
diff --git a/dbms/src/Storages/StorageDictionary.h b/dbms/src/Storages/StorageDictionary.h
index 5394e1abfaff..0f21373f546c 100644
--- a/dbms/src/Storages/StorageDictionary.h
+++ b/dbms/src/Storages/StorageDictionary.h
@@ -37,7 +37,6 @@ class StorageDictionary : public ext::shared_ptr_helper<StorageDictionary>, publ
     using Ptr = MultiVersion<IDictionaryBase>::Version;
 
     String table_name;
-    NamesAndTypesList columns;
     String dictionary_name;
     Poco::Logger * logger;
 
diff --git a/dbms/src/Storages/StorageDistributed.cpp b/dbms/src/Storages/StorageDistributed.cpp
index 954b45e269d2..4728463c007f 100644
--- a/dbms/src/Storages/StorageDistributed.cpp
+++ b/dbms/src/Storages/StorageDistributed.cpp
@@ -50,6 +50,8 @@ namespace DB
 namespace ErrorCodes
 {
     extern const int STORAGE_REQUIRES_PARAMETER;
+    extern const int BAD_ARGUMENTS;
+    extern const int READONLY;
     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
     extern const int INCORRECT_NUMBER_OF_COLUMNS;
 }
@@ -141,8 +143,8 @@ StorageDistributed::StorageDistributed(
     const Context & context_,
     const ASTPtr & sharding_key_,
     const String & data_path_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
-    name(name_), columns(columns_),
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
+    name(name_),
     remote_database(remote_database_), remote_table(remote_table_),
     context(context_), cluster_name(context.getMacros().expand(cluster_name_)), has_sharding_key(sharding_key_),
     sharding_key_expr(sharding_key_ ? ExpressionAnalyzer(sharding_key_, context, nullptr, columns).getActions(false) : nullptr),
@@ -197,7 +199,7 @@ BlockInputStreams StorageDistributed::read(
         external_tables = context.getExternalTables();
 
     ClusterProxy::SelectStreamFactory select_stream_factory(
-        processed_stage,  QualifiedTableName{remote_database, remote_table}, external_tables);
+        processed_stage, QualifiedTableName{remote_database, remote_table}, external_tables);
 
     return ClusterProxy::executeQuery(
             select_stream_factory, cluster, modified_query_ast, context, settings);
@@ -206,25 +208,29 @@ BlockInputStreams StorageDistributed::read(
 
 BlockOutputStreamPtr StorageDistributed::write(const ASTPtr & query, const Settings & settings)
 {
-    auto cluster = owned_cluster ? owned_cluster : context.getCluster(cluster_name);
+    auto cluster = (owned_cluster) ? owned_cluster : context.getCluster(cluster_name);
 
-    /// TODO: !path.empty() can be replaced by !owned_cluster or !cluster_name.empty() ?
-    /// owned_cluster for remote table function use sync insertion => doesn't need a path.
-    bool write_enabled = (!path.empty() || owned_cluster)
-                         && (((cluster->getLocalShardCount() + cluster->getRemoteShardCount()) < 2) || has_sharding_key);
+    /// Ban an attempt to make async insert into the table belonging to DatabaseMemory
+    if (path.empty() && !owned_cluster && !settings.insert_distributed_sync.value)
+    {
+        throw Exception("Storage " + getName() + " must has own data directory to enable asynchronous inserts",
+                        ErrorCodes::BAD_ARGUMENTS);
+    }
 
-    if (!write_enabled)
-        throw Exception{
-            "Method write is not supported by storage " + getName() +
-            " with more than one shard and no sharding key provided",
-            ErrorCodes::STORAGE_REQUIRES_PARAMETER};
+    /// If sharding key is not specified, then you can only write to a shard containing only one shard
+    if (!has_sharding_key && ((cluster->getLocalShardCount() + cluster->getRemoteShardCount()) >= 2))
+    {
+        throw Exception("Method write is not supported by storage " + getName() + " with more than one shard and no sharding key provided",
+                        ErrorCodes::STORAGE_REQUIRES_PARAMETER);
+    }
 
+    /// Force sync insertion if it is remote() table function
     bool insert_sync = settings.insert_distributed_sync || owned_cluster;
     auto timeout = settings.insert_distributed_timeout;
 
     /// DistributedBlockOutputStream will not own cluster, but will own ConnectionPools of the cluster
     return std::make_shared<DistributedBlockOutputStream>(
-        *this, rewriteInsertQuery(query, remote_database, remote_table), cluster, insert_sync, timeout);
+        *this, rewriteInsertQuery(query, remote_database, remote_table), cluster, settings, insert_sync, timeout);
 }
 
 
diff --git a/dbms/src/Storages/StorageDistributed.h b/dbms/src/Storages/StorageDistributed.h
index 09c18857f143..744903a077a4 100644
--- a/dbms/src/Storages/StorageDistributed.h
+++ b/dbms/src/Storages/StorageDistributed.h
@@ -94,7 +94,6 @@ class StorageDistributed : public ext::shared_ptr_helper<StorageDistributed>, pu
 
 
     String name;
-    NamesAndTypesList columns;
     String remote_database;
     String remote_table;
 
diff --git a/dbms/src/Storages/StorageFile.cpp b/dbms/src/Storages/StorageFile.cpp
index 199e2a972bb2..57d567ff9039 100644
--- a/dbms/src/Storages/StorageFile.cpp
+++ b/dbms/src/Storages/StorageFile.cpp
@@ -31,6 +31,8 @@ namespace ErrorCodes
     extern const int DATABASE_ACCESS_DENIED;
     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
     extern const int UNKNOWN_IDENTIFIER;
+    extern const int INCORRECT_FILE_NAME;
+    extern const int EMPTY_LIST_OF_COLUMNS_PASSED;
 };
 
 
@@ -57,8 +59,8 @@ StorageFile::StorageFile(
         const NamesAndTypesList & alias_columns_,
         const ColumnDefaults & column_defaults_,
         Context & context_)
-    : IStorage(materialized_columns_, alias_columns_, column_defaults_),
-    table_name(table_name_), format_name(format_name_), columns(columns_), context_global(context_), table_fd(table_fd_)
+    : IStorage(columns_, materialized_columns_, alias_columns_, column_defaults_),
+    table_name(table_name_), format_name(format_name_), context_global(context_), table_fd(table_fd_)
 {
     if (table_fd < 0) /// Will use file
     {
@@ -72,6 +74,9 @@ StorageFile::StorageFile(
         }
         else /// Is DB's file
         {
+            if (db_dir_path.empty())
+                throw Exception("Storage " + getName() + " requires data path", ErrorCodes::INCORRECT_FILE_NAME);
+
             path = getTablePath(db_dir_path, table_name, format_name);
             is_db_table = true;
             Poco::File(Poco::Path(path).parent()).createDirectories();
diff --git a/dbms/src/Storages/StorageFile.h b/dbms/src/Storages/StorageFile.h
index 58bb8bc1e9dd..4ab458261b2a 100644
--- a/dbms/src/Storages/StorageFile.h
+++ b/dbms/src/Storages/StorageFile.h
@@ -77,7 +77,6 @@ class StorageFile : public ext::shared_ptr_helper<StorageFile>, public IStorage
 
     std::string table_name;
     std::string format_name;
-    NamesAndTypesList columns;
     Context & context_global;
 
     std::string path;
diff --git a/dbms/src/Storages/StorageKafka.cpp b/dbms/src/Storages/StorageKafka.cpp
index 3d603b53adfb..61236d34c323 100644
--- a/dbms/src/Storages/StorageKafka.cpp
+++ b/dbms/src/Storages/StorageKafka.cpp
@@ -228,9 +228,9 @@ StorageKafka::StorageKafka(
     const ColumnDefaults & column_defaults_,
     const String & brokers_, const String & group_, const Names & topics_,
     const String & format_name_, const String & schema_name_, size_t num_consumers_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
     table_name(table_name_), database_name(database_name_), context(context_),
-    columns(columns_), topics(topics_), brokers(brokers_), group(group_), format_name(format_name_), schema_name(schema_name_),
+    topics(topics_), brokers(brokers_), group(group_), format_name(format_name_), schema_name(schema_name_),
     num_consumers(num_consumers_), log(&Logger::get("StorageKafka (" + table_name_ + ")")),
     semaphore(0, num_consumers_), mutex(), consumers(), event_update()
 {
diff --git a/dbms/src/Storages/StorageKafka.h b/dbms/src/Storages/StorageKafka.h
index a41fee8f7471..39df9c136a14 100644
--- a/dbms/src/Storages/StorageKafka.h
+++ b/dbms/src/Storages/StorageKafka.h
@@ -32,8 +32,6 @@ friend class KafkaBlockOutputStream;
     std::string getTableName() const override { return table_name; }
     std::string getDatabaseName() const { return database_name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     void startup() override;
     void shutdown() override;
 
@@ -73,7 +71,6 @@ friend class KafkaBlockOutputStream;
     String table_name;
     String database_name;
     Context & context;
-    NamesAndTypesList columns;
     Names topics;
     const String brokers;
     const String group;
diff --git a/dbms/src/Storages/StorageLog.cpp b/dbms/src/Storages/StorageLog.cpp
index 1f477aedd199..6fdcaa2ed6ac 100644
--- a/dbms/src/Storages/StorageLog.cpp
+++ b/dbms/src/Storages/StorageLog.cpp
@@ -41,6 +41,7 @@ namespace ErrorCodes
     extern const int DUPLICATE_COLUMN;
     extern const int SIZES_OF_MARKS_FILES_ARE_INCONSISTENT;
     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
+    extern const int INCORRECT_FILE_NAME;
 }
 
 
@@ -363,15 +364,15 @@ StorageLog::StorageLog(
     const NamesAndTypesList & alias_columns_,
     const ColumnDefaults & column_defaults_,
     size_t max_compress_block_size_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
-    path(path_), name(name_), columns(columns_),
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
+    path(path_), name(name_),
     max_compress_block_size(max_compress_block_size_),
     file_checker(path + escapeForFileName(name) + '/' + "sizes.json")
 {
-    if (columns.empty())
-        throw Exception("Empty list of columns passed to StorageLog constructor", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED);
+    if (path.empty())
+        throw Exception("Storage " + getName() + " requires data path", ErrorCodes::INCORRECT_FILE_NAME);
 
-    /// create files if they do not exist
+     /// create files if they do not exist
     Poco::File(path + escapeForFileName(name) + '/').createDirectories();
 
     for (const auto & column : getColumnsList())
diff --git a/dbms/src/Storages/StorageLog.h b/dbms/src/Storages/StorageLog.h
index 740454f6fce3..2d362c09272a 100644
--- a/dbms/src/Storages/StorageLog.h
+++ b/dbms/src/Storages/StorageLog.h
@@ -26,8 +26,6 @@ friend class LogBlockOutputStream;
     std::string getName() const override { return "Log"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -59,7 +57,6 @@ friend class LogBlockOutputStream;
 private:
     String path;
     String name;
-    NamesAndTypesList columns;
 
     mutable std::shared_mutex rwlock;
 
diff --git a/dbms/src/Storages/StorageMaterializedView.cpp b/dbms/src/Storages/StorageMaterializedView.cpp
index 101a6566d472..f1326a52933f 100644
--- a/dbms/src/Storages/StorageMaterializedView.cpp
+++ b/dbms/src/Storages/StorageMaterializedView.cpp
@@ -65,8 +65,8 @@ StorageMaterializedView::StorageMaterializedView(
     const NamesAndTypesList & alias_columns_,
     const ColumnDefaults & column_defaults_,
     bool attach_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),
-    database_name(database_name_), global_context(local_context.getGlobalContext()), columns(columns_)
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),
+    database_name(database_name_), global_context(local_context.getGlobalContext())
 {
     if (!query.select)
         throw Exception("SELECT query is not specified for " + getName(), ErrorCodes::INCORRECT_QUERY);
diff --git a/dbms/src/Storages/StorageMaterializedView.h b/dbms/src/Storages/StorageMaterializedView.h
index b153a4e864b1..b2a2069166aa 100644
--- a/dbms/src/Storages/StorageMaterializedView.h
+++ b/dbms/src/Storages/StorageMaterializedView.h
@@ -51,7 +51,6 @@ class StorageMaterializedView : public ext::shared_ptr_helper<StorageMaterialize
     String database_name;
     ASTPtr inner_query;
     Context & global_context;
-    NamesAndTypesList columns;
     bool has_inner_table = false;
 
 protected:
diff --git a/dbms/src/Storages/StorageMemory.cpp b/dbms/src/Storages/StorageMemory.cpp
index 75afcdbcc8be..db4be02635cc 100644
--- a/dbms/src/Storages/StorageMemory.cpp
+++ b/dbms/src/Storages/StorageMemory.cpp
@@ -87,8 +87,8 @@ StorageMemory::StorageMemory(
     const NamesAndTypesList & materialized_columns_,
     const NamesAndTypesList & alias_columns_,
     const ColumnDefaults & column_defaults_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
-    name(name_), columns(columns_)
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
+    name(name_)
 {
 }
 
diff --git a/dbms/src/Storages/StorageMemory.h b/dbms/src/Storages/StorageMemory.h
index 255434e9e344..956e4354727b 100644
--- a/dbms/src/Storages/StorageMemory.h
+++ b/dbms/src/Storages/StorageMemory.h
@@ -26,8 +26,6 @@ friend class MemoryBlockOutputStream;
     std::string getName() const override { return "Memory"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     size_t getSize() const { return data.size(); }
 
     BlockInputStreams read(
@@ -45,7 +43,6 @@ friend class MemoryBlockOutputStream;
 
 private:
     String name;
-    NamesAndTypesList columns;
 
     /// The data itself. `list` - so that when inserted to the end, the existing iterators are not invalidated.
     BlocksList data;
diff --git a/dbms/src/Storages/StorageMerge.cpp b/dbms/src/Storages/StorageMerge.cpp
index 88f646af0bdb..1e9a590714bd 100644
--- a/dbms/src/Storages/StorageMerge.cpp
+++ b/dbms/src/Storages/StorageMerge.cpp
@@ -42,8 +42,8 @@ StorageMerge::StorageMerge(
     const String & source_database_,
     const String & table_name_regexp_,
     const Context & context_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
-    name(name_), columns(columns_), source_database(source_database_),
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
+    name(name_), source_database(source_database_),
     table_name_regexp(table_name_regexp_), context(context_)
 {
 }
diff --git a/dbms/src/Storages/StorageMerge.h b/dbms/src/Storages/StorageMerge.h
index 9ceae8f92eaf..9bef645c0e00 100644
--- a/dbms/src/Storages/StorageMerge.h
+++ b/dbms/src/Storages/StorageMerge.h
@@ -47,7 +47,6 @@ class StorageMerge : public ext::shared_ptr_helper<StorageMerge>, public IStorag
 
 private:
     String name;
-    NamesAndTypesList columns;
     String source_database;
     OptimizedRegularExpression table_name_regexp;
     const Context & context;
diff --git a/dbms/src/Storages/StorageMergeTree.cpp b/dbms/src/Storages/StorageMergeTree.cpp
index 4fec351d6a12..50e316706d58 100644
--- a/dbms/src/Storages/StorageMergeTree.cpp
+++ b/dbms/src/Storages/StorageMergeTree.cpp
@@ -27,6 +27,7 @@ namespace ErrorCodes
     extern const int ABORTED;
     extern const int BAD_ARGUMENTS;
     extern const int INCORRECT_DATA;
+    extern const int INCORRECT_FILE_NAME;
     extern const int CANNOT_ASSIGN_OPTIMIZE;
 }
 
@@ -48,7 +49,7 @@ StorageMergeTree::StorageMergeTree(
     const MergeTreeData::MergingParams & merging_params_,
     const MergeTreeSettings & settings_,
     bool has_force_restore_data_flag)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
     path(path_), database_name(database_name_), table_name(table_name_), full_path(path + escapeForFileName(table_name) + '/'),
     context(context_), background_pool(context_.getBackgroundPool()),
     data(database_name, table_name,
@@ -60,6 +61,9 @@ StorageMergeTree::StorageMergeTree(
     reader(data), writer(data), merger(data, context.getBackgroundPool()),
     log(&Logger::get(database_name_ + "." + table_name + " (StorageMergeTree)"))
 {
+    if (path_.empty())
+        throw Exception("MergeTree storages require data path", ErrorCodes::INCORRECT_FILE_NAME);
+
     data.loadDataParts(has_force_restore_data_flag);
 
     if (!attach)
diff --git a/dbms/src/Storages/StorageMySQL.cpp b/dbms/src/Storages/StorageMySQL.cpp
index 694489ba8959..f3f18518d836 100644
--- a/dbms/src/Storages/StorageMySQL.cpp
+++ b/dbms/src/Storages/StorageMySQL.cpp
@@ -23,11 +23,11 @@ StorageMySQL::StorageMySQL(
     mysqlxx::Pool && pool,
     const std::string & remote_database_name,
     const std::string & remote_table_name,
-    const NamesAndTypesList & columns)
-    : name(name)
+    const NamesAndTypesList & columns_)
+    : IStorage{columns_, {}, {}, {}}
+    , name(name)
     , remote_database_name(remote_database_name)
     , remote_table_name(remote_table_name)
-    , columns(columns)
     , pool(std::move(pool))
 {
 }
diff --git a/dbms/src/Storages/StorageMySQL.h b/dbms/src/Storages/StorageMySQL.h
index 29d0d35e69d7..9558605ff0e2 100644
--- a/dbms/src/Storages/StorageMySQL.h
+++ b/dbms/src/Storages/StorageMySQL.h
@@ -24,11 +24,10 @@ class StorageMySQL : public ext::shared_ptr_helper<StorageMySQL>, public IStorag
         mysqlxx::Pool && pool,
         const std::string & remote_database_name,
         const std::string & remote_table_name,
-        const NamesAndTypesList & columns);
+        const NamesAndTypesList & columns_);
 
     std::string getName() const override { return "MySQL"; }
     std::string getTableName() const override { return name; }
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
     BlockInputStreams read(
         const Names & column_names,
@@ -44,7 +43,7 @@ class StorageMySQL : public ext::shared_ptr_helper<StorageMySQL>, public IStorag
     std::string remote_database_name;
     std::string remote_table_name;
 
-    NamesAndTypesList columns;
+
     mysqlxx::Pool pool;
 };
 
diff --git a/dbms/src/Storages/StorageNull.h b/dbms/src/Storages/StorageNull.h
index 3df037a46bec..b2d3fb557bf2 100644
--- a/dbms/src/Storages/StorageNull.h
+++ b/dbms/src/Storages/StorageNull.h
@@ -20,8 +20,6 @@ class StorageNull : public ext::shared_ptr_helper<StorageNull>, public IStorage
     std::string getName() const override { return "Null"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names &,
         const SelectQueryInfo &,
@@ -47,7 +45,6 @@ class StorageNull : public ext::shared_ptr_helper<StorageNull>, public IStorage
 
 private:
     String name;
-    NamesAndTypesList columns;
 
 protected:
     StorageNull(
@@ -56,7 +53,7 @@ class StorageNull : public ext::shared_ptr_helper<StorageNull>, public IStorage
         const NamesAndTypesList & materialized_columns_,
         const NamesAndTypesList & alias_columns_,
         const ColumnDefaults & column_defaults_)
-        : IStorage{materialized_columns_, alias_columns_, column_defaults_}, name(name_), columns(columns_) {}
+        : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, name(name_)  {}
 };
 
 }
diff --git a/dbms/src/Storages/StorageODBC.cpp b/dbms/src/Storages/StorageODBC.cpp
index 31ee1531dcf9..67362bf2f902 100644
--- a/dbms/src/Storages/StorageODBC.cpp
+++ b/dbms/src/Storages/StorageODBC.cpp
@@ -21,11 +21,11 @@ StorageODBC::StorageODBC(
     const std::string & connection_string,
     const std::string & remote_database_name,
     const std::string & remote_table_name,
-    const NamesAndTypesList & columns)
-    : name(name)
+    const NamesAndTypesList & columns_)
+    : IStorage{columns_, {}, {}, {}}
+    , name(name)
     , remote_database_name(remote_database_name)
     , remote_table_name(remote_table_name)
-    , columns(columns)
 {
     pool = createAndCheckResizePocoSessionPool([&]
     {
diff --git a/dbms/src/Storages/StorageODBC.h b/dbms/src/Storages/StorageODBC.h
index d411a45f766e..f582c74a2635 100644
--- a/dbms/src/Storages/StorageODBC.h
+++ b/dbms/src/Storages/StorageODBC.h
@@ -30,11 +30,10 @@ class StorageODBC : public ext::shared_ptr_helper<StorageODBC>, public IStorage
         const std::string & connection_string,
         const std::string & remote_database_name,
         const std::string & remote_table_name,
-        const NamesAndTypesList & columns);
+        const NamesAndTypesList & columns_);
 
     std::string getName() const override { return "ODBC"; }
     std::string getTableName() const override { return name; }
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
     BlockInputStreams read(
         const Names & column_names,
@@ -46,12 +45,9 @@ class StorageODBC : public ext::shared_ptr_helper<StorageODBC>, public IStorage
 
 private:
     std::string name;
-
     std::string remote_database_name;
     std::string remote_table_name;
 
-    NamesAndTypesList columns;
-
     std::shared_ptr<Poco::Data::SessionPool> pool;
 };
 }
diff --git a/dbms/src/Storages/StorageReplicatedMergeTree.cpp b/dbms/src/Storages/StorageReplicatedMergeTree.cpp
index 308088d5b254..b419545efb54 100644
--- a/dbms/src/Storages/StorageReplicatedMergeTree.cpp
+++ b/dbms/src/Storages/StorageReplicatedMergeTree.cpp
@@ -95,6 +95,7 @@ namespace ErrorCodes
     extern const int TOO_MUCH_FETCHES;
     extern const int BAD_DATA_PART_NAME;
     extern const int PART_IS_TEMPORARILY_LOCKED;
+    extern const int INCORRECT_FILE_NAME;
     extern const int CANNOT_ASSIGN_OPTIMIZE;
 }
 
@@ -178,7 +179,7 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(
     const MergeTreeData::MergingParams & merging_params_,
     const MergeTreeSettings & settings_,
     bool has_force_restore_data_flag)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_}, context(context_),
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, context(context_),
     current_zookeeper(context.getZooKeeper()), database_name(database_name_),
     table_name(name_), full_path(path_ + escapeForFileName(table_name) + '/'),
     zookeeper_path(context.getMacros().expand(zookeeper_path_)),
@@ -195,6 +196,9 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(
     shutdown_event(false), part_check_thread(*this),
     log(&Logger::get(database_name + "." + table_name + " (StorageReplicatedMergeTree)"))
 {
+    if (path_.empty())
+        throw Exception("ReplicatedMergeTree storages require data path", ErrorCodes::INCORRECT_FILE_NAME);
+
     if (!zookeeper_path.empty() && zookeeper_path.back() == '/')
         zookeeper_path.resize(zookeeper_path.size() - 1);
     /// If zookeeper chroot prefix is used, path should starts with '/', because chroot concatenates without it.
diff --git a/dbms/src/Storages/StorageSet.cpp b/dbms/src/Storages/StorageSet.cpp
index 581b28766c13..fbd7f0803f3c 100644
--- a/dbms/src/Storages/StorageSet.cpp
+++ b/dbms/src/Storages/StorageSet.cpp
@@ -21,6 +21,12 @@ namespace ErrorCodes
 }
 
 
+namespace ErrorCodes
+{
+    extern const int INCORRECT_FILE_NAME;
+}
+
+
 class SetOrJoinBlockOutputStream : public IBlockOutputStream
 {
 public:
@@ -86,9 +92,13 @@ StorageSetOrJoinBase::StorageSetOrJoinBase(
     const NamesAndTypesList & materialized_columns_,
     const NamesAndTypesList & alias_columns_,
     const ColumnDefaults & column_defaults_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
-    path(path_ + escapeForFileName(name_) + '/'), name(name_), columns(columns_)
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
+    name(name_)
 {
+    if (path_.empty())
+        throw Exception("Join and Set storages require data path", ErrorCodes::INCORRECT_FILE_NAME);
+
+    path = path_ + escapeForFileName(name_) + '/';
 }
 
 
diff --git a/dbms/src/Storages/StorageSet.h b/dbms/src/Storages/StorageSet.h
index bd5045208def..0b08f3af2dbd 100644
--- a/dbms/src/Storages/StorageSet.h
+++ b/dbms/src/Storages/StorageSet.h
@@ -20,7 +20,6 @@ class StorageSetOrJoinBase : public IStorage
 
 public:
     String getTableName() const override { return name; }
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
     void rename(const String & new_path_to_db, const String & new_database_name, const String & new_table_name) override;
 
@@ -37,7 +36,6 @@ class StorageSetOrJoinBase : public IStorage
 
     String path;
     String name;
-    NamesAndTypesList columns;
 
     UInt64 increment = 0;    /// For the backup file names.
 
diff --git a/dbms/src/Storages/StorageStripeLog.cpp b/dbms/src/Storages/StorageStripeLog.cpp
index 948832f2cac4..62e9c3cc9985 100644
--- a/dbms/src/Storages/StorageStripeLog.cpp
+++ b/dbms/src/Storages/StorageStripeLog.cpp
@@ -39,6 +39,7 @@ namespace ErrorCodes
     extern const int EMPTY_LIST_OF_COLUMNS_PASSED;
     extern const int CANNOT_CREATE_DIRECTORY;
     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
+    extern const int INCORRECT_FILE_NAME;
 }
 
 
@@ -183,14 +184,14 @@ StorageStripeLog::StorageStripeLog(
     const ColumnDefaults & column_defaults_,
     bool attach,
     size_t max_compress_block_size_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
-    path(path_), name(name_), columns(columns_),
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
+    path(path_), name(name_),
     max_compress_block_size(max_compress_block_size_),
     file_checker(path + escapeForFileName(name) + '/' + "sizes.json"),
     log(&Logger::get("StorageStripeLog"))
 {
-    if (columns.empty())
-        throw Exception("Empty list of columns passed to StorageStripeLog constructor", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED);
+    if (path.empty())
+        throw Exception("Storage " + getName() + " requires data path", ErrorCodes::INCORRECT_FILE_NAME);
 
     String full_path = path + escapeForFileName(name) + '/';
     if (!attach)
diff --git a/dbms/src/Storages/StorageStripeLog.h b/dbms/src/Storages/StorageStripeLog.h
index f8cad8a4d70c..e1e72873794d 100644
--- a/dbms/src/Storages/StorageStripeLog.h
+++ b/dbms/src/Storages/StorageStripeLog.h
@@ -28,8 +28,6 @@ friend class StripeLogBlockOutputStream;
     std::string getName() const override { return "StripeLog"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -56,7 +54,6 @@ friend class StripeLogBlockOutputStream;
 private:
     String path;
     String name;
-    NamesAndTypesList columns;
 
     size_t max_compress_block_size;
 
diff --git a/dbms/src/Storages/StorageTinyLog.cpp b/dbms/src/Storages/StorageTinyLog.cpp
index 986cbcc97a79..23d1e0c44759 100644
--- a/dbms/src/Storages/StorageTinyLog.cpp
+++ b/dbms/src/Storages/StorageTinyLog.cpp
@@ -46,6 +46,7 @@ namespace ErrorCodes
     extern const int CANNOT_READ_ALL_DATA;
     extern const int DUPLICATE_COLUMN;
     extern const int LOGICAL_ERROR;
+    extern const int INCORRECT_FILE_NAME;
     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
 }
 
@@ -284,14 +285,14 @@ StorageTinyLog::StorageTinyLog(
     const ColumnDefaults & column_defaults_,
     bool attach,
     size_t max_compress_block_size_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},
-    path(path_), name(name_), columns(columns_),
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},
+    path(path_), name(name_),
     max_compress_block_size(max_compress_block_size_),
     file_checker(path + escapeForFileName(name) + '/' + "sizes.json"),
     log(&Logger::get("StorageTinyLog"))
 {
-    if (columns.empty())
-        throw Exception("Empty list of columns passed to StorageTinyLog constructor", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED);
+    if (path.empty())
+        throw Exception("Storage " + getName() + " requires data path", ErrorCodes::INCORRECT_FILE_NAME);
 
     String full_path = path + escapeForFileName(name) + '/';
     if (!attach)
diff --git a/dbms/src/Storages/StorageTinyLog.h b/dbms/src/Storages/StorageTinyLog.h
index 7d240fae0176..84e52afe265d 100644
--- a/dbms/src/Storages/StorageTinyLog.h
+++ b/dbms/src/Storages/StorageTinyLog.h
@@ -27,8 +27,6 @@ friend class TinyLogBlockOutputStream;
     std::string getName() const override { return "TinyLog"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -55,7 +53,6 @@ friend class TinyLogBlockOutputStream;
 private:
     String path;
     String name;
-    NamesAndTypesList columns;
 
     size_t max_compress_block_size;
 
diff --git a/dbms/src/Storages/StorageView.cpp b/dbms/src/Storages/StorageView.cpp
index 6920c38fd5ca..3001bc293041 100644
--- a/dbms/src/Storages/StorageView.cpp
+++ b/dbms/src/Storages/StorageView.cpp
@@ -25,8 +25,8 @@ StorageView::StorageView(
     const NamesAndTypesList & materialized_columns_,
     const NamesAndTypesList & alias_columns_,
     const ColumnDefaults & column_defaults_)
-    : IStorage{materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),
-    database_name(database_name_), columns(columns_)
+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),
+    database_name(database_name_)
 {
     if (!query.select)
         throw Exception("SELECT query is not specified for " + getName(), ErrorCodes::INCORRECT_QUERY);
diff --git a/dbms/src/Storages/StorageView.h b/dbms/src/Storages/StorageView.h
index 675f92f3cad5..854420206830 100644
--- a/dbms/src/Storages/StorageView.h
+++ b/dbms/src/Storages/StorageView.h
@@ -16,7 +16,6 @@ class StorageView : public ext::shared_ptr_helper<StorageView>, public IStorage
 public:
     std::string getName() const override { return "View"; }
     std::string getTableName() const override { return table_name; }
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
     /// It is passed inside the query and solved at its level.
     bool supportsSampling() const override { return true; }
@@ -36,7 +35,6 @@ class StorageView : public ext::shared_ptr_helper<StorageView>, public IStorage
     String table_name;
     String database_name;
     ASTPtr inner_query;
-    NamesAndTypesList columns;
 
 protected:
     StorageView(
diff --git a/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.cpp b/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.cpp
index 5b35a7ebb679..44f8d933c997 100644
--- a/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.cpp
+++ b/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.cpp
@@ -14,13 +14,12 @@ namespace DB
 
 StorageSystemAsynchronousMetrics::StorageSystemAsynchronousMetrics(const std::string & name_, const AsynchronousMetrics & async_metrics_)
     : name(name_),
-    columns
-    {
-        {"metric", std::make_shared<DataTypeString>()},
-        {"value", std::make_shared<DataTypeFloat64>()},
-    },
     async_metrics(async_metrics_)
 {
+    columns = NamesAndTypesList{
+        {"metric", std::make_shared<DataTypeString>()},
+        {"value", std::make_shared<DataTypeFloat64>()},
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.h b/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.h
index 560265e1dab9..60e500961435 100644
--- a/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.h
+++ b/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.h
@@ -19,8 +19,6 @@ class StorageSystemAsynchronousMetrics : public ext::shared_ptr_helper<StorageSy
     std::string getName() const override { return "SystemAsynchronousMetrics"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -31,7 +29,6 @@ class StorageSystemAsynchronousMetrics : public ext::shared_ptr_helper<StorageSy
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
     const AsynchronousMetrics & async_metrics;
 
 protected:
diff --git a/dbms/src/Storages/System/StorageSystemBuildOptions.cpp b/dbms/src/Storages/System/StorageSystemBuildOptions.cpp
index 029620ff4ad7..c8da9f12c404 100644
--- a/dbms/src/Storages/System/StorageSystemBuildOptions.cpp
+++ b/dbms/src/Storages/System/StorageSystemBuildOptions.cpp
@@ -12,12 +12,11 @@ namespace DB
 
 StorageSystemBuildOptions::StorageSystemBuildOptions(const std::string & name_)
     : name(name_)
-    , columns
-    {
+{
+    columns = NamesAndTypesList{
         { "name", std::make_shared<DataTypeString>() },
         { "value", std::make_shared<DataTypeString>() },
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemBuildOptions.h b/dbms/src/Storages/System/StorageSystemBuildOptions.h
index a1ecae77b3a3..d772b2553830 100644
--- a/dbms/src/Storages/System/StorageSystemBuildOptions.h
+++ b/dbms/src/Storages/System/StorageSystemBuildOptions.h
@@ -18,8 +18,6 @@ class StorageSystemBuildOptions : public ext::shared_ptr_helper<StorageSystemBui
     std::string getName() const override { return "SystemBuildOptions"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemBuildOptions : public ext::shared_ptr_helper<StorageSystemBui
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemBuildOptions(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemClusters.cpp b/dbms/src/Storages/System/StorageSystemClusters.cpp
index 5cd79fa03723..084f295de74d 100644
--- a/dbms/src/Storages/System/StorageSystemClusters.cpp
+++ b/dbms/src/Storages/System/StorageSystemClusters.cpp
@@ -13,7 +13,8 @@ namespace DB
 
 StorageSystemClusters::StorageSystemClusters(const std::string & name_)
     : name(name_)
-    , columns{
+{
+    columns = NamesAndTypesList{
         { "cluster",      std::make_shared<DataTypeString>() },
         { "shard_num",    std::make_shared<DataTypeUInt32>() },
         { "shard_weight", std::make_shared<DataTypeUInt32>() },
@@ -24,8 +25,7 @@ StorageSystemClusters::StorageSystemClusters(const std::string & name_)
         { "is_local",     std::make_shared<DataTypeUInt8>() },
         { "user",         std::make_shared<DataTypeString>() },
         { "default_database", std::make_shared<DataTypeString>() }
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemClusters.h b/dbms/src/Storages/System/StorageSystemClusters.h
index c8113d4d0953..1e36269ded2d 100644
--- a/dbms/src/Storages/System/StorageSystemClusters.h
+++ b/dbms/src/Storages/System/StorageSystemClusters.h
@@ -18,7 +18,6 @@ class StorageSystemClusters : public ext::shared_ptr_helper<StorageSystemCluster
 public:
     std::string getName() const override { return "SystemClusters"; }
     std::string getTableName() const override { return name; }
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
     BlockInputStreams read(
         const Names & column_names,
@@ -30,7 +29,6 @@ class StorageSystemClusters : public ext::shared_ptr_helper<StorageSystemCluster
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemClusters(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemColumns.cpp b/dbms/src/Storages/System/StorageSystemColumns.cpp
index f7a38cd42955..5f9116b08a59 100644
--- a/dbms/src/Storages/System/StorageSystemColumns.cpp
+++ b/dbms/src/Storages/System/StorageSystemColumns.cpp
@@ -17,7 +17,8 @@ namespace DB
 
 StorageSystemColumns::StorageSystemColumns(const std::string & name_)
     : name(name_)
-    , columns{
+{
+    columns = NamesAndTypesList{
         { "database",           std::make_shared<DataTypeString>() },
         { "table",              std::make_shared<DataTypeString>() },
         { "name",               std::make_shared<DataTypeString>() },
@@ -27,8 +28,7 @@ StorageSystemColumns::StorageSystemColumns(const std::string & name_)
         { "data_compressed_bytes",      std::make_shared<DataTypeUInt64>() },
         { "data_uncompressed_bytes",    std::make_shared<DataTypeUInt64>() },
         { "marks_bytes",                std::make_shared<DataTypeUInt64>() },
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemColumns.h b/dbms/src/Storages/System/StorageSystemColumns.h
index bd7dfbf21ce1..ba187f7306fa 100644
--- a/dbms/src/Storages/System/StorageSystemColumns.h
+++ b/dbms/src/Storages/System/StorageSystemColumns.h
@@ -16,7 +16,6 @@ class StorageSystemColumns : public ext::shared_ptr_helper<StorageSystemColumns>
 public:
     std::string getName() const override { return "SystemColumns"; }
     std::string getTableName() const override { return name; }
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
     BlockInputStreams read(
         const Names & column_names,
@@ -31,7 +30,6 @@ class StorageSystemColumns : public ext::shared_ptr_helper<StorageSystemColumns>
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 };
 
 }
diff --git a/dbms/src/Storages/System/StorageSystemDatabases.cpp b/dbms/src/Storages/System/StorageSystemDatabases.cpp
index 34a7d566c827..c4c41f3375da 100644
--- a/dbms/src/Storages/System/StorageSystemDatabases.cpp
+++ b/dbms/src/Storages/System/StorageSystemDatabases.cpp
@@ -11,13 +11,12 @@ namespace DB
 
 
 StorageSystemDatabases::StorageSystemDatabases(const std::string & name_)
-    : name(name_),
-    columns
-    {
-        {"name",     std::make_shared<DataTypeString>()},
-        {"engine",     std::make_shared<DataTypeString>()},
-    }
+    : name(name_)
 {
+    columns = NamesAndTypesList{
+        {"name",    std::make_shared<DataTypeString>()},
+        {"engine",  std::make_shared<DataTypeString>()},
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemDatabases.h b/dbms/src/Storages/System/StorageSystemDatabases.h
index c574861a94d1..621e490963aa 100644
--- a/dbms/src/Storages/System/StorageSystemDatabases.h
+++ b/dbms/src/Storages/System/StorageSystemDatabases.h
@@ -18,8 +18,6 @@ class StorageSystemDatabases : public ext::shared_ptr_helper<StorageSystemDataba
     std::string getName() const override { return "SystemDatabases"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemDatabases : public ext::shared_ptr_helper<StorageSystemDataba
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemDatabases(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemDictionaries.cpp b/dbms/src/Storages/System/StorageSystemDictionaries.cpp
index 2ef9a8c4db67..93c742ef1a79 100644
--- a/dbms/src/Storages/System/StorageSystemDictionaries.cpp
+++ b/dbms/src/Storages/System/StorageSystemDictionaries.cpp
@@ -19,24 +19,24 @@ namespace DB
 {
 
 StorageSystemDictionaries::StorageSystemDictionaries(const std::string & name)
-    : name{name},
-      columns{
-          { "name", std::make_shared<DataTypeString>() },
-          { "origin", std::make_shared<DataTypeString>() },
-          { "type", std::make_shared<DataTypeString>() },
-          { "key", std::make_shared<DataTypeString>() },
-          { "attribute.names", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>()) },
-          { "attribute.types", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>()) },
-          { "bytes_allocated", std::make_shared<DataTypeUInt64>() },
-          { "query_count", std::make_shared<DataTypeUInt64>() },
-          { "hit_rate", std::make_shared<DataTypeFloat64>() },
-          { "element_count", std::make_shared<DataTypeUInt64>() },
-          { "load_factor", std::make_shared<DataTypeFloat64>() },
-          { "creation_time", std::make_shared<DataTypeDateTime>() },
-          { "source", std::make_shared<DataTypeString>() },
-          { "last_exception", std::make_shared<DataTypeString>() }
-    }
+    : name{name}
 {
+    columns = NamesAndTypesList{
+        { "name", std::make_shared<DataTypeString>() },
+        { "origin", std::make_shared<DataTypeString>() },
+        { "type", std::make_shared<DataTypeString>() },
+        { "key", std::make_shared<DataTypeString>() },
+        { "attribute.names", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>()) },
+        { "attribute.types", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>()) },
+        { "bytes_allocated", std::make_shared<DataTypeUInt64>() },
+        { "query_count", std::make_shared<DataTypeUInt64>() },
+        { "hit_rate", std::make_shared<DataTypeFloat64>() },
+        { "element_count", std::make_shared<DataTypeUInt64>() },
+        { "load_factor", std::make_shared<DataTypeFloat64>() },
+        { "creation_time", std::make_shared<DataTypeDateTime>() },
+        { "source", std::make_shared<DataTypeString>() },
+        { "last_exception", std::make_shared<DataTypeString>() }
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemDictionaries.h b/dbms/src/Storages/System/StorageSystemDictionaries.h
index a27bfbd21a4a..57ac9b0b6eb9 100644
--- a/dbms/src/Storages/System/StorageSystemDictionaries.h
+++ b/dbms/src/Storages/System/StorageSystemDictionaries.h
@@ -26,9 +26,6 @@ class StorageSystemDictionaries : public ext::shared_ptr_helper<StorageSystemDic
 
 private:
     const std::string name;
-    const NamesAndTypesList columns;
-
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
 protected:
     StorageSystemDictionaries(const std::string & name);
diff --git a/dbms/src/Storages/System/StorageSystemEvents.cpp b/dbms/src/Storages/System/StorageSystemEvents.cpp
index 38f134fe356a..96a02d723327 100644
--- a/dbms/src/Storages/System/StorageSystemEvents.cpp
+++ b/dbms/src/Storages/System/StorageSystemEvents.cpp
@@ -11,13 +11,13 @@ namespace DB
 
 
 StorageSystemEvents::StorageSystemEvents(const std::string & name_)
-    : name(name_),
-    columns
+    : name(name_)
+{
+    columns = NamesAndTypesList
     {
         {"event", std::make_shared<DataTypeString>()},
         {"value", std::make_shared<DataTypeUInt64>()}
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemEvents.h b/dbms/src/Storages/System/StorageSystemEvents.h
index 4a0f2672cc1b..b987151e4007 100644
--- a/dbms/src/Storages/System/StorageSystemEvents.h
+++ b/dbms/src/Storages/System/StorageSystemEvents.h
@@ -18,8 +18,6 @@ class StorageSystemEvents : public ext::shared_ptr_helper<StorageSystemEvents>,
     std::string getName() const override { return "SystemEvents"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemEvents : public ext::shared_ptr_helper<StorageSystemEvents>,
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemEvents(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemFunctions.cpp b/dbms/src/Storages/System/StorageSystemFunctions.cpp
index aa9d7da9b1b3..74e58338d40f 100644
--- a/dbms/src/Storages/System/StorageSystemFunctions.cpp
+++ b/dbms/src/Storages/System/StorageSystemFunctions.cpp
@@ -15,11 +15,11 @@ namespace DB
 
 StorageSystemFunctions::StorageSystemFunctions(const std::string & name_)
     : name(name_)
-    , columns{
+{
+    columns = NamesAndTypesList{
         { "name",         std::make_shared<DataTypeString>() },
         { "is_aggregate", std::make_shared<DataTypeUInt8>()  }
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemFunctions.h b/dbms/src/Storages/System/StorageSystemFunctions.h
index c60515b92f3a..f77b95364538 100644
--- a/dbms/src/Storages/System/StorageSystemFunctions.h
+++ b/dbms/src/Storages/System/StorageSystemFunctions.h
@@ -18,7 +18,6 @@ class StorageSystemFunctions : public ext::shared_ptr_helper<StorageSystemFuncti
 public:
     std::string getName() const override { return "SystemFunctions"; }
     std::string getTableName() const override { return name; }
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
     BlockInputStreams read(
         const Names & column_names,
@@ -33,7 +32,6 @@ class StorageSystemFunctions : public ext::shared_ptr_helper<StorageSystemFuncti
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 };
 
 }
diff --git a/dbms/src/Storages/System/StorageSystemGraphite.cpp b/dbms/src/Storages/System/StorageSystemGraphite.cpp
index e57d31434997..76f3defe8852 100644
--- a/dbms/src/Storages/System/StorageSystemGraphite.cpp
+++ b/dbms/src/Storages/System/StorageSystemGraphite.cpp
@@ -124,16 +124,16 @@ static Strings getAllGraphiteSections(const AbstractConfiguration & config)
 
 StorageSystemGraphite::StorageSystemGraphite(const std::string & name_)
     : name(name_)
-    , columns
-    {
+{
+    columns = NamesAndTypesList{
         {"config_name", std::make_shared<DataTypeString>()},
         {"regexp",      std::make_shared<DataTypeString>()},
         {"function",    std::make_shared<DataTypeString>()},
         {"age",         std::make_shared<DataTypeUInt64>()},
         {"precision",   std::make_shared<DataTypeUInt64>()},
         {"priority",    std::make_shared<DataTypeUInt16>()},
-        {"is_default",  std::make_shared<DataTypeUInt8>()}}
-{
+        {"is_default",  std::make_shared<DataTypeUInt8>()}
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemGraphite.h b/dbms/src/Storages/System/StorageSystemGraphite.h
index 4d4f930679fe..8c7a625de540 100644
--- a/dbms/src/Storages/System/StorageSystemGraphite.h
+++ b/dbms/src/Storages/System/StorageSystemGraphite.h
@@ -12,7 +12,6 @@ class StorageSystemGraphite : public ext::shared_ptr_helper<StorageSystemGraphit
 public:
     std::string getName() const override { return "SystemGraphite"; }
     std::string getTableName() const override { return name; }
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
     BlockInputStreams read(
         const Names & column_names,
@@ -24,7 +23,6 @@ class StorageSystemGraphite : public ext::shared_ptr_helper<StorageSystemGraphit
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemGraphite(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemMerges.cpp b/dbms/src/Storages/System/StorageSystemMerges.cpp
index 0071f4794ae8..f6667c199081 100644
--- a/dbms/src/Storages/System/StorageSystemMerges.cpp
+++ b/dbms/src/Storages/System/StorageSystemMerges.cpp
@@ -13,7 +13,8 @@ namespace DB
 
 StorageSystemMerges::StorageSystemMerges(const std::string & name)
     : name{name}
-    , columns{
+{
+    columns = NamesAndTypesList{
         { "database",                    std::make_shared<DataTypeString>() },
         { "table",                       std::make_shared<DataTypeString>() },
         { "elapsed",                     std::make_shared<DataTypeFloat64>() },
@@ -30,8 +31,7 @@ StorageSystemMerges::StorageSystemMerges(const std::string & name)
         { "columns_written",             std::make_shared<DataTypeUInt64>() },
         { "memory_usage",                std::make_shared<DataTypeUInt64>() },
         { "thread_number",               std::make_shared<DataTypeUInt64>() },
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemMerges.h b/dbms/src/Storages/System/StorageSystemMerges.h
index d4925c1e821c..d48c97bfa17f 100644
--- a/dbms/src/Storages/System/StorageSystemMerges.h
+++ b/dbms/src/Storages/System/StorageSystemMerges.h
@@ -16,7 +16,6 @@ class StorageSystemMerges : public ext::shared_ptr_helper<StorageSystemMerges>,
     std::string getName() const override { return "SystemMerges"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -27,7 +26,6 @@ class StorageSystemMerges : public ext::shared_ptr_helper<StorageSystemMerges>,
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemMerges(const std::string & name);
diff --git a/dbms/src/Storages/System/StorageSystemMetrics.cpp b/dbms/src/Storages/System/StorageSystemMetrics.cpp
index eb3aab1df15c..c60d7f4b4f5d 100644
--- a/dbms/src/Storages/System/StorageSystemMetrics.cpp
+++ b/dbms/src/Storages/System/StorageSystemMetrics.cpp
@@ -12,13 +12,12 @@ namespace DB
 
 
 StorageSystemMetrics::StorageSystemMetrics(const std::string & name_)
-    : name(name_),
-    columns
-    {
+    : name(name_)
+{
+    columns = NamesAndTypesList{
         {"metric", std::make_shared<DataTypeString>()},
         {"value",  std::make_shared<DataTypeInt64>()},
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemMetrics.h b/dbms/src/Storages/System/StorageSystemMetrics.h
index 0dac0d56f920..7b6058de9e5b 100644
--- a/dbms/src/Storages/System/StorageSystemMetrics.h
+++ b/dbms/src/Storages/System/StorageSystemMetrics.h
@@ -18,8 +18,6 @@ class StorageSystemMetrics : public ext::shared_ptr_helper<StorageSystemMetrics>
     std::string getName() const override { return "SystemMetrics"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemMetrics : public ext::shared_ptr_helper<StorageSystemMetrics>
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemMetrics(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemModels.cpp b/dbms/src/Storages/System/StorageSystemModels.cpp
index e0f1e666ad18..da0249b17059 100644
--- a/dbms/src/Storages/System/StorageSystemModels.cpp
+++ b/dbms/src/Storages/System/StorageSystemModels.cpp
@@ -12,15 +12,15 @@ namespace DB
 {
 
 StorageSystemModels::StorageSystemModels(const std::string & name)
-    : name{name},
-    columns{
+    : name{name}
+{
+    columns = NamesAndTypesList{
         { "name", std::make_shared<DataTypeString>() },
         { "origin", std::make_shared<DataTypeString>() },
         { "type", std::make_shared<DataTypeString>() },
         { "creation_time", std::make_shared<DataTypeDateTime>() },
         { "last_exception", std::make_shared<DataTypeString>() }
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemModels.h b/dbms/src/Storages/System/StorageSystemModels.h
index 6827043f11c4..b32c5a804ce4 100644
--- a/dbms/src/Storages/System/StorageSystemModels.h
+++ b/dbms/src/Storages/System/StorageSystemModels.h
@@ -26,9 +26,6 @@ class StorageSystemModels : public ext::shared_ptr_helper<StorageSystemModels>,
 
 private:
     const std::string name;
-    const NamesAndTypesList columns;
-
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
 
 protected:
     StorageSystemModels(const std::string & name);
diff --git a/dbms/src/Storages/System/StorageSystemNumbers.cpp b/dbms/src/Storages/System/StorageSystemNumbers.cpp
index ff5c4206cd19..4a9e85049e76 100644
--- a/dbms/src/Storages/System/StorageSystemNumbers.cpp
+++ b/dbms/src/Storages/System/StorageSystemNumbers.cpp
@@ -41,8 +41,9 @@ class NumbersBlockInputStream : public IProfilingBlockInputStream
 
 
 StorageSystemNumbers::StorageSystemNumbers(const std::string & name_, bool multithreaded_, size_t limit_)
-    : name(name_), columns{{"number", std::make_shared<DataTypeUInt64>()}}, multithreaded(multithreaded_), limit(limit_)
+    : name(name_), multithreaded(multithreaded_), limit(limit_)
 {
+    columns = NamesAndTypesList{{"number", std::make_shared<DataTypeUInt64>()}};
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemNumbers.h b/dbms/src/Storages/System/StorageSystemNumbers.h
index 37ef809736f6..e0769fb39682 100644
--- a/dbms/src/Storages/System/StorageSystemNumbers.h
+++ b/dbms/src/Storages/System/StorageSystemNumbers.h
@@ -25,8 +25,6 @@ class StorageSystemNumbers : public ext::shared_ptr_helper<StorageSystemNumbers>
     std::string getName() const override { return "SystemNumbers"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -37,7 +35,6 @@ class StorageSystemNumbers : public ext::shared_ptr_helper<StorageSystemNumbers>
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
     bool multithreaded;
     size_t limit;
 
diff --git a/dbms/src/Storages/System/StorageSystemOne.cpp b/dbms/src/Storages/System/StorageSystemOne.cpp
index 58e3e5bfeddd..4ddc64031e3b 100644
--- a/dbms/src/Storages/System/StorageSystemOne.cpp
+++ b/dbms/src/Storages/System/StorageSystemOne.cpp
@@ -11,8 +11,9 @@ namespace DB
 
 
 StorageSystemOne::StorageSystemOne(const std::string & name_)
-    : name(name_), columns{{"dummy", std::make_shared<DataTypeUInt8>()}}
+    : name(name_)
 {
+    columns = NamesAndTypesList{{"dummy", std::make_shared<DataTypeUInt8>()}};
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemOne.h b/dbms/src/Storages/System/StorageSystemOne.h
index ad3ba3c7a929..346f7a869824 100644
--- a/dbms/src/Storages/System/StorageSystemOne.h
+++ b/dbms/src/Storages/System/StorageSystemOne.h
@@ -21,8 +21,6 @@ class StorageSystemOne : public ext::shared_ptr_helper<StorageSystemOne>, public
     std::string getName() const override { return "SystemOne"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -33,7 +31,6 @@ class StorageSystemOne : public ext::shared_ptr_helper<StorageSystemOne>, public
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemOne(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemPartsBase.cpp b/dbms/src/Storages/System/StorageSystemPartsBase.cpp
index 30281cc0960b..b185cca8be22 100644
--- a/dbms/src/Storages/System/StorageSystemPartsBase.cpp
+++ b/dbms/src/Storages/System/StorageSystemPartsBase.cpp
@@ -277,4 +277,10 @@ bool StorageSystemPartsBase::hasColumn(const String & column_name) const
     return ITableDeclaration::hasColumn(column_name);
 }
 
+StorageSystemPartsBase::StorageSystemPartsBase(std::string name_, NamesAndTypesList && columns_)
+    : name(std::move(name_))
+{
+    columns = columns_;
+}
+
 }
diff --git a/dbms/src/Storages/System/StorageSystemPartsBase.h b/dbms/src/Storages/System/StorageSystemPartsBase.h
index 4079f7a8e634..4ff9c7e48991 100644
--- a/dbms/src/Storages/System/StorageSystemPartsBase.h
+++ b/dbms/src/Storages/System/StorageSystemPartsBase.h
@@ -18,8 +18,6 @@ class StorageSystemPartsBase : public IStorage
 public:
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     NameAndTypePair getColumn(const String & column_name) const override;
 
     bool hasColumn(const String & column_name) const override;
@@ -50,12 +48,12 @@ class StorageSystemPartsBase : public IStorage
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
+
 
     bool hasStateColumn(const Names & column_names);
 
 protected:
-    StorageSystemPartsBase(std::string name_, NamesAndTypesList && columns) : name(std::move(name_)), columns(columns) {}
+    StorageSystemPartsBase(std::string name_, NamesAndTypesList && columns_);
 
     virtual void processNextStorage(MutableColumns & columns, const StoragesInfo & info, bool has_state_column) = 0;
 };
diff --git a/dbms/src/Storages/System/StorageSystemProcesses.cpp b/dbms/src/Storages/System/StorageSystemProcesses.cpp
index 82a0ef229749..af02417af4f1 100644
--- a/dbms/src/Storages/System/StorageSystemProcesses.cpp
+++ b/dbms/src/Storages/System/StorageSystemProcesses.cpp
@@ -13,7 +13,8 @@ namespace DB
 
 StorageSystemProcesses::StorageSystemProcesses(const std::string & name_)
     : name(name_)
-    , columns{
+{
+    columns = NamesAndTypesList{
         { "is_initial_query",     std::make_shared<DataTypeUInt8>() },
 
         { "user",                 std::make_shared<DataTypeString>() },
@@ -48,8 +49,7 @@ StorageSystemProcesses::StorageSystemProcesses(const std::string & name_)
         { "written_bytes",        std::make_shared<DataTypeUInt64>() },
         { "memory_usage",         std::make_shared<DataTypeInt64>() },
         { "query",                std::make_shared<DataTypeString>() }
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemProcesses.h b/dbms/src/Storages/System/StorageSystemProcesses.h
index 73da8cfc1705..f8f26d13d358 100644
--- a/dbms/src/Storages/System/StorageSystemProcesses.h
+++ b/dbms/src/Storages/System/StorageSystemProcesses.h
@@ -18,8 +18,6 @@ class StorageSystemProcesses : public ext::shared_ptr_helper<StorageSystemProces
     std::string getName() const override { return "SystemProcesses"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemProcesses : public ext::shared_ptr_helper<StorageSystemProces
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemProcesses(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemReplicas.cpp b/dbms/src/Storages/System/StorageSystemReplicas.cpp
index cbfc0a8ef812..6dc11214b7ce 100644
--- a/dbms/src/Storages/System/StorageSystemReplicas.cpp
+++ b/dbms/src/Storages/System/StorageSystemReplicas.cpp
@@ -16,7 +16,8 @@ namespace DB
 
 StorageSystemReplicas::StorageSystemReplicas(const std::string & name_)
     : name(name_)
-    , columns{
+{
+    columns = NamesAndTypesList{
         { "database",                             std::make_shared<DataTypeString>()   },
         { "table",                                std::make_shared<DataTypeString>()   },
         { "engine",                               std::make_shared<DataTypeString>()   },
@@ -43,8 +44,7 @@ StorageSystemReplicas::StorageSystemReplicas(const std::string & name_)
         { "absolute_delay",                       std::make_shared<DataTypeUInt64>()   },
         { "total_replicas",                       std::make_shared<DataTypeUInt8>()    },
         { "active_replicas",                      std::make_shared<DataTypeUInt8>()    },
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemReplicas.h b/dbms/src/Storages/System/StorageSystemReplicas.h
index 0140e94f9df7..465b6baa5819 100644
--- a/dbms/src/Storages/System/StorageSystemReplicas.h
+++ b/dbms/src/Storages/System/StorageSystemReplicas.h
@@ -18,8 +18,6 @@ class StorageSystemReplicas : public ext::shared_ptr_helper<StorageSystemReplica
     std::string getName() const override { return "SystemReplicas"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemReplicas : public ext::shared_ptr_helper<StorageSystemReplica
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemReplicas(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemReplicationQueue.cpp b/dbms/src/Storages/System/StorageSystemReplicationQueue.cpp
index 88683c742af0..a24325250444 100644
--- a/dbms/src/Storages/System/StorageSystemReplicationQueue.cpp
+++ b/dbms/src/Storages/System/StorageSystemReplicationQueue.cpp
@@ -19,7 +19,8 @@ namespace DB
 
 StorageSystemReplicationQueue::StorageSystemReplicationQueue(const std::string & name_)
     : name(name_)
-    , columns{
+{
+    columns = NamesAndTypesList{
         /// Table properties.
         { "database",                std::make_shared<DataTypeString>() },
         { "table",                   std::make_shared<DataTypeString>() },
@@ -42,8 +43,7 @@ StorageSystemReplicationQueue::StorageSystemReplicationQueue(const std::string &
         { "num_postponed",           std::make_shared<DataTypeUInt32>() },
         { "postpone_reason",         std::make_shared<DataTypeString>() },
         { "last_postpone_time",      std::make_shared<DataTypeDateTime>() },
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemReplicationQueue.h b/dbms/src/Storages/System/StorageSystemReplicationQueue.h
index 369ace0ec366..8554361e0dfa 100644
--- a/dbms/src/Storages/System/StorageSystemReplicationQueue.h
+++ b/dbms/src/Storages/System/StorageSystemReplicationQueue.h
@@ -18,8 +18,6 @@ class StorageSystemReplicationQueue : public ext::shared_ptr_helper<StorageSyste
     std::string getName() const override { return "SystemReplicationQueue"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemReplicationQueue : public ext::shared_ptr_helper<StorageSyste
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemReplicationQueue(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemSettings.cpp b/dbms/src/Storages/System/StorageSystemSettings.cpp
index 699c8983ccab..58c3eb504b9a 100644
--- a/dbms/src/Storages/System/StorageSystemSettings.cpp
+++ b/dbms/src/Storages/System/StorageSystemSettings.cpp
@@ -13,13 +13,13 @@ namespace DB
 
 StorageSystemSettings::StorageSystemSettings(const std::string & name_)
     : name(name_)
-    , columns{
+{
+    columns = NamesAndTypesList{
         { "name",        std::make_shared<DataTypeString>() },
         { "value",       std::make_shared<DataTypeString>() },
         { "changed",     std::make_shared<DataTypeUInt8>() },
-        { "description", std::make_shared<DataTypeString>() },
-    }
-{
+        { "description", std::make_shared<DataTypeString>() }
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemSettings.h b/dbms/src/Storages/System/StorageSystemSettings.h
index f48611c2519d..153b9213ef8c 100644
--- a/dbms/src/Storages/System/StorageSystemSettings.h
+++ b/dbms/src/Storages/System/StorageSystemSettings.h
@@ -18,8 +18,6 @@ class StorageSystemSettings : public ext::shared_ptr_helper<StorageSystemSetting
     std::string getName() const override { return "SystemSettings"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemSettings : public ext::shared_ptr_helper<StorageSystemSetting
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemSettings(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemTables.cpp b/dbms/src/Storages/System/StorageSystemTables.cpp
index c4eac3d7ec7a..b40cf29498f9 100644
--- a/dbms/src/Storages/System/StorageSystemTables.cpp
+++ b/dbms/src/Storages/System/StorageSystemTables.cpp
@@ -13,15 +13,14 @@ namespace DB
 {
 
 StorageSystemTables::StorageSystemTables(const std::string & name_)
-    : name(name_),
-    columns
-    {
+    : name(name_)
+{
+    columns = NamesAndTypesList{
         {"database", std::make_shared<DataTypeString>()},
         {"name", std::make_shared<DataTypeString>()},
         {"engine", std::make_shared<DataTypeString>()},
         {"metadata_modification_time", std::make_shared<DataTypeDateTime>()}
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemTables.h b/dbms/src/Storages/System/StorageSystemTables.h
index 3dd66e0f409c..d66ca54be683 100644
--- a/dbms/src/Storages/System/StorageSystemTables.h
+++ b/dbms/src/Storages/System/StorageSystemTables.h
@@ -18,8 +18,6 @@ class StorageSystemTables : public ext::shared_ptr_helper<StorageSystemTables>,
     std::string getName() const override { return "SystemTables"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemTables : public ext::shared_ptr_helper<StorageSystemTables>,
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemTables(const std::string & name_);
diff --git a/dbms/src/Storages/System/StorageSystemZooKeeper.cpp b/dbms/src/Storages/System/StorageSystemZooKeeper.cpp
index 7f7dfa1aa422..80ca3bc2f0e5 100644
--- a/dbms/src/Storages/System/StorageSystemZooKeeper.cpp
+++ b/dbms/src/Storages/System/StorageSystemZooKeeper.cpp
@@ -21,7 +21,8 @@ namespace DB
 
 StorageSystemZooKeeper::StorageSystemZooKeeper(const std::string & name_)
     : name(name_)
-    , columns{
+{
+    columns = NamesAndTypesList{
         { "name",           std::make_shared<DataTypeString>() },
         { "value",          std::make_shared<DataTypeString>() },
         { "czxid",          std::make_shared<DataTypeInt64>() },
@@ -36,8 +37,7 @@ StorageSystemZooKeeper::StorageSystemZooKeeper(const std::string & name_)
         { "numChildren",    std::make_shared<DataTypeInt32>() },
         { "pzxid",          std::make_shared<DataTypeInt64>() },
         { "path",           std::make_shared<DataTypeString>() },
-    }
-{
+    };
 }
 
 
diff --git a/dbms/src/Storages/System/StorageSystemZooKeeper.h b/dbms/src/Storages/System/StorageSystemZooKeeper.h
index 72913ab8beb7..45625ebab128 100644
--- a/dbms/src/Storages/System/StorageSystemZooKeeper.h
+++ b/dbms/src/Storages/System/StorageSystemZooKeeper.h
@@ -18,8 +18,6 @@ class StorageSystemZooKeeper : public ext::shared_ptr_helper<StorageSystemZooKee
     std::string getName() const override { return "SystemZooKeeper"; }
     std::string getTableName() const override { return name; }
 
-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }
-
     BlockInputStreams read(
         const Names & column_names,
         const SelectQueryInfo & query_info,
@@ -30,7 +28,6 @@ class StorageSystemZooKeeper : public ext::shared_ptr_helper<StorageSystemZooKee
 
 private:
     const std::string name;
-    NamesAndTypesList columns;
 
 protected:
     StorageSystemZooKeeper(const std::string & name_);
