{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 72226,
  "instance_id": "ClickHouse__ClickHouse-72226",
  "issue_numbers": [
    "72174"
  ],
  "base_commit": "d65a1c214eee18e068d8124af3a0833b9c3dc0d4",
  "patch": "diff --git a/src/Interpreters/Squashing.cpp b/src/Interpreters/Squashing.cpp\nindex 02d1ae528acd..6f877f4d2db1 100644\n--- a/src/Interpreters/Squashing.cpp\n+++ b/src/Interpreters/Squashing.cpp\n@@ -145,7 +145,19 @@ Chunk Squashing::squash(std::vector<Chunk> && input_chunks, Chunk::ChunkInfoColl\n         auto columns = input_chunks[i].detachColumns();\n         for (size_t j = 0; j != num_columns; ++j)\n         {\n-            have_same_serialization[j] &= ISerialization::getKind(*columns[j]) == ISerialization::getKind(*mutable_columns[j]);\n+            /// IColumn::structureEquals is not implemented for deprecated object type, ignore it and always convert to non-sparse.\n+            bool has_object_deprecated = columns[j]->getDataType() == TypeIndex::ObjectDeprecated ||\n+                mutable_columns[j]->getDataType() == TypeIndex::ObjectDeprecated;\n+            auto has_object_deprecated_lambda = [&has_object_deprecated](const auto & subcolumn)\n+            {\n+                has_object_deprecated = has_object_deprecated || subcolumn.getDataType() == TypeIndex::ObjectDeprecated;\n+            };\n+            columns[j]->forEachSubcolumnRecursively(has_object_deprecated_lambda);\n+            mutable_columns[j]->forEachSubcolumnRecursively(has_object_deprecated_lambda);\n+\n+            /// Need to check if there are any sparse columns in subcolumns,\n+            /// since `IColumn::isSparse` is not recursive but sparse column can be inside a tuple, for example.\n+            have_same_serialization[j] &= !has_object_deprecated && columns[j]->structureEquals(*mutable_columns[j]);\n             source_columns_list[j].emplace_back(std::move(columns[j]));\n         }\n     }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03274_squashing_transform_sparse_bug.reference b/tests/queries/0_stateless/03274_squashing_transform_sparse_bug.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/03274_squashing_transform_sparse_bug.sql b/tests/queries/0_stateless/03274_squashing_transform_sparse_bug.sql\nnew file mode 100644\nindex 000000000000..4ea6dcc80e25\n--- /dev/null\n+++ b/tests/queries/0_stateless/03274_squashing_transform_sparse_bug.sql\n@@ -0,0 +1,18 @@\n+\n+DROP TABLE IF EXISTS t0;\n+DROP TABLE IF EXISTS t1;\n+\n+SET max_insert_block_size = 1;\n+SET min_insert_block_size_rows = 1;\n+SET min_insert_block_size_bytes = 1;\n+\n+CREATE TABLE t0 (x UInt64, y Tuple(UInt64, UInt64) ) ENGINE = MergeTree ORDER BY x SETTINGS ratio_of_defaults_for_sparse_serialization = 0.5;\n+SYSTEM STOP MERGES t0;\n+INSERT INTO t0 SELECT if(number % 2 = 0, 0, number) as x, (x, 0) from numbers(200) SETTINGS max_block_size = 1;\n+\n+CREATE TABLE t1 (x UInt64, y Tuple(UInt64, UInt64) ) ENGINE = MergeTree ORDER BY x;\n+\n+SET min_joined_block_size_bytes = 100;\n+\n+SET join_algorithm = 'parallel_hash';\n+SELECT sum(ignore(*)) FROM t0 a FULL JOIN t1 b ON a.x = b.x FORMAT Null;\n",
  "problem_statement": "Logical error in SquashingTransform in JOIN\n\r\nhttps://s3.amazonaws.com/clickhouse-test-reports/72145/f45bd58849e2062e5875e921fb469aace6b7c5fe/stateless_tests__debug__s3_storage_.html\r\n\r\n<details>\r\n<summary>Logs</summary>\r\n\r\n\r\n\r\n```\r\n2024.11.20 20:48:21.403625 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> executeQuery: Query span trace_id for opentelemetry log: f97433a7-85af-f115-9355-e37765c132d8\r\n2024.11.20 20:48:21.404051 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> executeQuery: (from [::1]:50732) (TID: (659, 34, 0ece01b8-09e0-4ce3-b83f-9a4dde5071c2), TIDH: 3128261553354376449) (comment: 01172_transaction_counters.sql) select 6, system.parts.name, txn_counters.creation_tid = system.parts.creation_tid from txn_counters join system.parts on txn_counters._part = system.parts.name where database=currentDatabase() and table='txn_counters' order by system.parts.name; (stage: Complete)\r\n2024.11.20 20:48:21.404829 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> Planner: Query to stage Complete\r\n2024.11.20 20:48:21.405128 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> HashJoin: Keys: [(_CAST(__table1._part, 'String'_String)) = (__table2.name)], datatype: EMPTY, kind: Inner, strictness: All, right header: __table2.name String String(size = 0), __table2.database String String(size = 0), __table2.table String String(size = 0), __table2.creation_tid Tuple(UInt64, UInt64, UUID) Tuple(size = 0, UInt64(size = 0), UInt64(size = 0), UUID(size = 0))\r\n2024.11.20 20:48:21.405242 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> Planner: Query from stage FetchColumns to stage Complete\r\n2024.11.20 20:48:21.405669 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> QueryPlanOptimizations: Pushed down filter and(equals(__table2.database, _CAST('test_nk0sp28r'_String, 'String'_String)), equals(__table2.table, 'txn_counters'_String)) to the Right side of join\r\n2024.11.20 20:48:21.405931 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Key condition: unknown\r\n2024.11.20 20:48:21.405963 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Filtering marks by primary and secondary keys\r\n2024.11.20 20:48:21.407179 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Selected 3/3 parts by partition key, 3 parts by primary key, 3/3 marks by primary key, 3 marks to read from 3 ranges\r\n2024.11.20 20:48:21.407204 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Spreading mark ranges among streams (default reading)\r\n2024.11.20 20:48:21.407272 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Reading approx. 3 rows with 3 streams\r\n2024.11.20 20:48:21.481882 [ 272308 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Fatal> : Logical error: '(isConst() || isSparse()) ? getDataType() == rhs.getDataType() : typeid(*this) == typeid(rhs)'.\r\n\r\n2024.11.20 20:48:21.498156 [ 272308 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Fatal> : Stack trace (when copying this message, always include the lines below):\r\n\r\n0. /build/src/Common/StackTrace.cpp:380: StackTrace::tryCapture() @ 0x000000000c9f6172\r\n1. /build/src/Common/Exception.cpp:53: DB::abortOnFailedAssertion(String const&) @ 0x000000000c9c5f75\r\n2. /build/src/Columns/IColumn.h:749: DB::IColumn::assertTypeEquality(DB::IColumn const&) const @ 0x000000000729d442\r\n3. /build/src/Columns/IColumn.h:233: DB::ColumnTuple::doInsertRangeFrom(DB::IColumn const&, unsigned long, unsigned long) @ 0x0000000012d8a43d\r\n4. /build/src/Columns/IColumn.h:234: DB::Squashing::squash(std::vector<DB::Chunk, std::allocator<DB::Chunk>>&&, DB::CollectionOfDerivedItems<DB::ChunkInfo>&&) @ 0x000000001229874d\r\n5. /build/src/Interpreters/Squashing.cpp:45: DB::Squashing::squash(DB::Chunk&&) @ 0x0000000012297d62\r\n6. /build/src/Processors/Transforms/SquashingTransform.cpp:86: DB::SimpleSquashingChunksTransform::getRemaining() @ 0x0000000013e8bfab\r\n7. /build/src/Processors/IInflatingTransform.cpp:86: DB::IInflatingTransform::work() @ 0x0000000013e4440d\r\n8. /build/src/Processors/Executors/ExecutionThreadContext.cpp:49: DB::ExecutionThreadContext::executeTask() @ 0x0000000013bcc5a9\r\n9. /build/src/Processors/Executors/PipelineExecutor.cpp:289: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x0000000013bc181d\r\n10. /build/src/Processors/Executors/PipelineExecutor.cpp:255: DB::PipelineExecutor::executeSingleThread(unsigned long) @ 0x0000000013bc1abd\r\n11. /build/src/Processors/Executors/PipelineExecutor.cpp:403: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x0000000013bc241f\r\n12. /build/contrib/llvm-project/libcxx/include/__functional/function.h:848: ? @ 0x000000000ca7dc3f\r\n13. /build/contrib/llvm-project/libcxx/include/__functional/invoke.h:359: ? @ 0x000000000ca82dc2\r\n14. /build/contrib/llvm-project/libcxx/include/__functional/function.h:848: ? @ 0x000000000ca7b9ce\r\n15. /build/contrib/llvm-project/libcxx/include/__functional/invoke.h:359: ? @ 0x000000000ca80e32\r\n16. ? @ 0x00007f467a1f8ac3\r\n17. ? @ 0x00007f467a28a850\r\n\r\n\r\n2024.11.20 20:50:23.025802 [ 443534 ] <Fatal> BaseDaemon: Changed settings: connect_timeout_with_failover_ms = 2000, connect_timeout_with_failover_secure_ms = 3000, idle_connection_timeout = 36000, s3_max_get_rps = 1000000, s3_max_get_burst = 2000000, s3_max_put_rps = 1000000, s3_max_put_burst = 2000000, s3_check_objects_after_upload = true, max_remote_read_network_bandwidth = 1000000000000, max_remote_write_network_bandwidth = 1000000000000, max_local_read_bandwidth = 1000000000000, max_local_write_bandwidth = 1000000000000, stream_like_engine_allow_direct_select = true, replication_wait_for_inactive_replica_timeout = 30, allow_nonconst_timezone_arguments = true, log_queries = true, insert_quorum_timeout = 60000, fsync_metadata = false, http_send_timeout = 60., http_receive_timeout = 60., use_index_for_in_with_subqueries_max_values = 1000000000, opentelemetry_start_trace_probability = 0.10000000149011612, max_rows_to_read = 20000000, max_bytes_to_read = 1000000000000, max_bytes_to_read_leaf = 1000000000000, max_rows_to_group_by = 10000000000, max_bytes_before_external_group_by = 10000000000, max_rows_to_sort = 10000000000, max_bytes_to_sort = 10000000000, max_bytes_before_external_sort = 10000000000, max_result_rows = 1000000000, max_result_bytes = 1000000000, max_execution_time = 600., max_execution_time_leaf = 600., max_execution_speed = 100000000000, max_execution_speed_bytes = 10000000000000, timeout_before_checking_execution_speed = 300., max_estimated_execution_time = 600., max_columns_to_read = 20000, max_temporary_columns = 20000, max_temporary_non_const_columns = 20000, max_rows_in_set = 10000000000, max_bytes_in_set = 10000000000, max_rows_in_join = 10000000000, max_bytes_in_join = 10000000000, max_rows_to_transfer = 1000000000, max_bytes_to_transfer = 1000000000, max_rows_in_distinct = 10000000000, max_bytes_in_distinct = 10000000000, max_memory_usage = 5000000000, max_memory_usage_for_user = 32000000000, max_untracked_memory = 1048576, memory_profiler_step = 1048576, max_network_bandwidth = 100000000000, max_network_bytes = 1000000000000, max_network_bandwidth_for_user = 100000000000, max_network_bandwidth_for_all_users = 100000000000, max_temporary_data_on_disk_size_for_user = 100000000000, max_temporary_data_on_disk_size_for_query = 100000000000, max_backup_bandwidth = 100000000000, log_comment = '01172_transaction_counters.sql', send_logs_level = 'warning', max_hyperscan_regexp_length = 1000000, max_hyperscan_regexp_total_length = 10000000, allow_introspection_functions = true, database_atomic_wait_for_drop_and_detach_synchronously = true, query_cache_max_size_in_bytes = 10000000, query_cache_max_entries = 100000, distributed_ddl_entry_format_version = 6, external_storage_max_read_rows = 10000000000, external_storage_max_read_bytes = 10000000000, async_insert_busy_timeout_max_ms = 5000, enable_filesystem_cache = true, enable_filesystem_cache_on_write_operations = true, filesystem_cache_segments_batch_size = 10, load_marks_asynchronously = true, allow_prefetched_read_pool_for_remote_filesystem = false, allow_prefetched_read_pool_for_local_filesystem = false, filesystem_prefetch_max_memory_usage = 1073741824, max_streams_for_merge_tree_reading = 1000, insert_keeper_max_retries = 100, insert_keeper_retry_initial_backoff_ms = 1, insert_keeper_retry_max_backoff_ms = 10, insert_keeper_fault_injection_probability = 0.009999999776482582, throw_on_unsupported_query_inside_transaction = false\r\n```\r\n\r\n\r\n</details>\n",
  "hints_text": "Another one: https://s3.amazonaws.com/clickhouse-test-reports/72046/f84083d174faf4814100b3e8fcc9170a8369f178/stateless_tests__tsan__[2_4].html\r\n\r\nWe've broken something\r\n\r\n```\r\n2024.11.20 19:20:34.007003 [ 1861 ] {1d236bb4-da10-4330-8625-c4d2cbda714e} <Fatal> : Logical error: '(isConst() || isSparse()) ? getDataType() == rhs.getDataType() : typeid(*this) == typeid(rhs)'.\r\n2024.11.20 19:20:34.212136 [ 1861 ] {1d236bb4-da10-4330-8625-c4d2cbda714e} <Fatal> : Stack trace (when copying this message, always include the lines below):\r\n2024.11.20 19:22:38.887219 [ 57402 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2024.11.20 19:22:38.980537 [ 57402 ] {} <Fatal> BaseDaemon: (version 24.11.1.2372, build id: 05B8C49B453D0116688B00619A0A4A748545D0A9, git hash: 7aabd0cb8980d471310820de8cad4677444f4ead, architecture: x86_64) (from thread 1861) Received signal 6\r\n2024.11.20 19:22:38.980687 [ 57402 ] {} <Fatal> BaseDaemon: Signal description: Aborted\r\n2024.11.20 19:22:38.980750 [ 57402 ] {} <Fatal> BaseDaemon: \r\n2024.11.20 19:22:39.007678 [ 57402 ] {} <Fatal> BaseDaemon: Stack trace: 0x000055d35f8be1fd 0x000055d35fc4ea02 0x000055d3571648f6 0x000055d357164e16 0x00007fa26a250520 0x00007fa26a2a49fd 0x00007fa26a250476 0x00007fa26a2367f3 0x000055d3571634ff 0x000055d35f878848 0x000055d35f878b37 0x000055d358be8967 0x000055d369b605e2 0x000055d368959c80 0x000055d368958ac0 0x000055d36b66813d 0x000055d36b606768 0x000055d36b20a345 0x000055d36b1fa015 0x000055d36b1fb193 0x000055d35f98a4e8 0x000055d35f991ec6 0x000055d35f991de2 0x000055d35f98705e 0x000055d35f98effc 0x000055d35715cf2f 0x00007fa26a2a2ac3 0x00007fa26a334850\r\n2024.11.20 19:22:39.008178 [ 57402 ] {} <Fatal> BaseDaemon: ########################################\r\n2024.11.20 19:22:39.329072 [ 57402 ] {} <Fatal> BaseDaemon: (version 24.11.1.2372, build id: 05B8C49B453D0116688B00619A0A4A748545D0A9, git hash: 7aabd0cb8980d471310820de8cad4677444f4ead) (from thread 1861) (query_id: 1d236bb4-da10-4330-8625-c4d2cbda714e) (query: select 6, system.parts.name, txn_counters.creation_tid = system.parts.creation_tid from txn_counters join system.parts on txn_counters._part = system.parts.name where database=currentDatabase() and table='txn_counters' order by system.parts.name;) Received signal Aborted (6)\r\n2024.11.20 19:22:39.329383 [ 57402 ] {} <Fatal> BaseDaemon: \r\n2024.11.20 19:22:39.329553 [ 57402 ] {} <Fatal> BaseDaemon: Stack trace: 0x000055d35f8be1fd 0x000055d35fc4ea02 0x000055d3571648f6 0x000055d357164e16 0x00007fa26a250520 0x00007fa26a2a49fd 0x00007fa26a250476 0x00007fa26a2367f3 0x000055d3571634ff 0x000055d35f878848 0x000055d35f878b37 0x000055d358be8967 0x000055d369b605e2 0x000055d368959c80 0x000055d368958ac0 0x000055d36b66813d 0x000055d36b606768 0x000055d36b20a345 0x000055d36b1fa015 0x000055d36b1fb193 0x000055d35f98a4e8 0x000055d35f991ec6 0x000055d35f991de2 0x000055d35f98705e 0x000055d35f98effc 0x000055d35715cf2f 0x00007fa26a2a2ac3 0x00007fa26a334850\r\n2024.11.20 19:22:39.655375 [ 57402 ] {} <Fatal> BaseDaemon: 0.0. inlined from ./build_docker/./src/Common/StackTrace.cpp:380: StackTrace::tryCapture()\r\n2024.11.20 19:22:39.655617 [ 57402 ] {} <Fatal> BaseDaemon: 0. ./build_docker/./src/Common/StackTrace.cpp:349: StackTrace::StackTrace(ucontext_t const&) @ 0x000000000fedf1fd\r\n2024.11.20 19:22:39.812418 [ 57402 ] {} <Fatal> BaseDaemon: 1. ./build_docker/./src/Common/SignalHandlers.cpp:85: signalHandler(int, siginfo_t*, void*) @ 0x000000001026fa02\r\n2024.11.20 19:22:40.879468 [ 57402 ] {} <Fatal> BaseDaemon: 2. __tsan::CallUserSignalHandler(__tsan::ThreadState*, bool, bool, int, __sanitizer::__sanitizer_siginfo*, void*) @ 0x00000000077858f6\r\n2024.11.20 19:22:41.402736 [ 57402 ] {} <Fatal> BaseDaemon: 3. sighandler(int, __sanitizer::__sanitizer_siginfo*, void*) @ 0x0000000007785e16\r\n2024.11.20 19:22:41.402952 [ 57402 ] {} <Fatal> BaseDaemon: 4. ? @ 0x00007fa26a250520\r\n2024.11.20 19:22:41.403091 [ 57402 ] {} <Fatal> BaseDaemon: 5. ? @ 0x00007fa26a2a49fd\r\n2024.11.20 19:22:41.403193 [ 57402 ] {} <Fatal> BaseDaemon: 6. ? @ 0x00007fa26a250476\r\n2024.11.20 19:22:41.403298 [ 57402 ] {} <Fatal> BaseDaemon: 7. ? @ 0x00007fa26a2367f3\r\n2024.11.20 19:22:41.588209 [ 57402 ] {} <Fatal> BaseDaemon: 8. __interceptor_abort @ 0x00000000077844ff\r\n2024.11.20 19:22:41.856061 [ 57402 ] {} <Fatal> BaseDaemon: 9. ./build_docker/./src/Common/Exception.cpp:48: DB::abortOnFailedAssertion(String const&, void* const*, unsigned long, unsigned long) @ 0x000000000fe99848\r\n2024.11.20 19:22:42.026816 [ 57402 ] {} <Fatal> BaseDaemon: 10. ./build_docker/./src/Common/Exception.cpp:54: DB::abortOnFailedAssertion(String const&) @ 0x000000000fe99b37\r\n2024.11.20 19:22:42.160974 [ 57402 ] {} <Fatal> BaseDaemon: 11. DB::IColumn::insertRangeFrom(DB::IColumn const&, unsigned long, unsigned long) @ 0x0000000009209967\r\n2024.11.20 19:22:42.305429 [ 57402 ] {} <Fatal> BaseDaemon: 12. ./build_docker/./src/Columns/ColumnTuple.cpp:370: DB::ColumnTuple::doInsertRangeFrom(DB::IColumn const&, unsigned long, unsigned long) @ 0x000000001a1815e2\r\n2024.11.20 19:22:42.440623 [ 57402 ] {} <Fatal> BaseDaemon: 13.0. inlined from ./src/Columns/IColumn.h:234: DB::IColumn::insertRangeFrom(DB::IColumn const&, unsigned long, unsigned long)\r\n2024.11.20 19:22:42.440846 [ 57402 ] {} <Fatal> BaseDaemon: 13. ./build_docker/./src/Interpreters/Squashing.cpp:167: DB::Squashing::squash(std::vector<DB::Chunk, std::allocator<DB::Chunk>>&&, DB::CollectionOfDerivedItems<DB::ChunkInfo>&&) @ 0x0000000018f7ac80\r\n2024.11.20 19:22:42.545190 [ 57402 ] {} <Fatal> BaseDaemon: 14. ./build_docker/./src/Interpreters/Squashing.cpp:45: DB::Squashing::squash(DB::Chunk&&) @ 0x0000000018f79ac0\r\n2024.11.20 19:22:42.756979 [ 57402 ] {} <Fatal> BaseDaemon: 15. ./build_docker/./src/Processors/Transforms/SquashingTransform.cpp:86: DB::SimpleSquashingChunksTransform::getRemaining() @ 0x000000001bc8913d\r\n2024.11.20 19:22:42.879395 [ 57402 ] {} <Fatal> BaseDaemon: 16. ./build_docker/./src/Processors/IInflatingTransform.cpp:86: DB::IInflatingTransform::work() @ 0x000000001bc27768\r\n2024.11.20 19:22:42.924534 [ 57402 ] {} <Fatal> BaseDaemon: 17.0. inlined from ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:49: DB::executeJob(DB::ExecutingGraph::Node*, DB::ReadProgressCallback*)\r\n2024.11.20 19:22:42.924779 [ 57402 ] {} <Fatal> BaseDaemon: 17. ./build_docker/./src/Processors/Executors/ExecutionThreadContext.cpp:98: DB::ExecutionThreadContext::executeTask() @ 0x000000001b82b345\r\n2024.11.20 19:22:43.051420 [ 57402 ] {} <Fatal> BaseDaemon: 18. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:289: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x000000001b81b015\r\n2024.11.20 19:22:43.180317 [ 57402 ] {} <Fatal> BaseDaemon: 19.0. inlined from ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:255: DB::PipelineExecutor::executeSingleThread(unsigned long)\r\n2024.11.20 19:22:43.180537 [ 57402 ] {} <Fatal> BaseDaemon: 19.1. inlined from ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:403: operator()\r\n2024.11.20 19:22:43.180716 [ 57402 ] {} <Fatal> BaseDaemon: 19.2. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ?\r\n2024.11.20 19:22:43.180893 [ 57402 ] {} <Fatal> BaseDaemon: 19.3. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:479: ?\r\n2024.11.20 19:22:43.181084 [ 57402 ] {} <Fatal> BaseDaemon: 19.4. inlined from ./contrib/llvm-project/libcxx/include/__functional/function.h:235: ?\r\n2024.11.20 19:22:43.181250 [ 57402 ] {} <Fatal> BaseDaemon: 19. ./contrib/llvm-project/libcxx/include/__functional/function.h:716: ? @ 0x000000001b81c193\r\n2024.11.20 19:22:43.351816 [ 57402 ] {} <Fatal> BaseDaemon: 20.0. inlined from ./contrib/llvm-project/libcxx/include/__functional/function.h:848: ?\r\n2024.11.20 19:22:43.352019 [ 57402 ] {} <Fatal> BaseDaemon: 20.1. inlined from ./contrib/llvm-project/libcxx/include/__functional/function.h:1197: ?\r\n2024.11.20 19:22:43.352193 [ 57402 ] {} <Fatal> BaseDaemon: 20. ./build_docker/./src/Common/ThreadPool.cpp:775: ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::worker() @ 0x000000000ffab4e8\r\n2024.11.20 19:22:43.454856 [ 57402 ] {} <Fatal> BaseDaemon: 21.0. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:359: ?\r\n2024.11.20 19:22:43.455164 [ 57402 ] {} <Fatal> BaseDaemon: 21.1. inlined from ./contrib/llvm-project/libcxx/include/tuple:1789: decltype(auto) std::__apply_tuple_impl[abi:v15007]<void (ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::*&)(), std::tuple<ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool*>&, 0ul>(void (ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::*&)(), std::tuple<ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool*>&, std::__tuple_indices<0ul>)\r\n2024.11.20 19:22:43.455457 [ 57402 ] {} <Fatal> BaseDaemon: 21.2. inlined from ./contrib/llvm-project/libcxx/include/tuple:1798: decltype(auto) std::apply[abi:v15007]<void (ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::*&)(), std::tuple<ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool*>&>(void (ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::*&)(), std::tuple<ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool*>&)\r\n2024.11.20 19:22:43.455659 [ 57402 ] {} <Fatal> BaseDaemon: 21. ./src/Common/ThreadPool.h:311: ThreadFromGlobalPoolImpl<false, true>::ThreadFromGlobalPoolImpl<void (ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::*)(), ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool*>(void (ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool::*&&)(), ThreadPoolImpl<ThreadFromGlobalPoolImpl<false, true>>::ThreadFromThreadPool*&&)::'lambda'()::operator()() @ 0x000000000ffb2ec6\r\n2024.11.20 19:22:43.606367 [ 57402 ] {} <Fatal> BaseDaemon: 22.0. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ?\r\n2024.11.20 19:22:43.606562 [ 57402 ] {} <Fatal> BaseDaemon: 22.1. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:479: ?\r\n2024.11.20 19:22:43.606697 [ 57402 ] {} <Fatal> BaseDaemon: 22.2. inlined from ./contrib/llvm-project/libcxx/include/__functional/function.h:235: ?\r\n2024.11.20 19:22:43.606794 [ 57402 ] {} <Fatal> BaseDaemon: 22. ./contrib/llvm-project/libcxx/include/__functional/function.h:716: ? @ 0x000000000ffb2de2\r\n2024.11.20 19:22:43.699172 [ 57402 ] {} <Fatal> BaseDaemon: 23.0. inlined from ./contrib/llvm-project/libcxx/include/__functional/function.h:848: ?\r\n2024.11.20 19:22:43.699434 [ 57402 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./contrib/llvm-project/libcxx/include/__functional/function.h:1197: ?\r\n2024.11.20 19:22:43.699565 [ 57402 ] {} <Fatal> BaseDaemon: 23. ./build_docker/./src/Common/ThreadPool.cpp:785: ThreadPoolImpl<std::thread>::ThreadFromThreadPool::worker() @ 0x000000000ffa805e\r\n2024.11.20 19:22:43.869388 [ 57402 ] {} <Fatal> BaseDaemon: 24.0. inlined from ./contrib/llvm-project/libcxx/include/__functional/invoke.h:359: ?\r\n2024.11.20 19:22:43.869626 [ 57402 ] {} <Fatal> BaseDaemon: 24.1. inlined from ./contrib/llvm-project/libcxx/include/thread:284: void std::__thread_execute[abi:v15007]<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*, 2ul>(std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>&, std::__tuple_indices<2ul>)\r\n2024.11.20 19:22:43.869784 [ 57402 ] {} <Fatal> BaseDaemon: 24. ./contrib/llvm-project/libcxx/include/thread:295: void* std::__thread_proxy[abi:v15007]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void (ThreadPoolImpl<std::thread>::ThreadFromThreadPool::*)(), ThreadPoolImpl<std::thread>::ThreadFromThreadPool*>>(void*) @ 0x000000000ffafffc\r\n2024.11.20 19:22:44.752653 [ 57402 ] {} <Fatal> BaseDaemon: 25. __tsan_thread_start_func @ 0x000000000777df2f\r\n2024.11.20 19:22:44.753028 [ 57402 ] {} <Fatal> BaseDaemon: 26. ? @ 0x00007fa26a2a2ac3\r\n2024.11.20 19:22:44.753167 [ 57402 ] {} <Fatal> BaseDaemon: 27. ? @ 0x00007fa26a334850\r\n2024.11.20 19:22:44.753340 [ 57402 ] {} <Fatal> BaseDaemon: Integrity check of the executable skipped because the reference checksum could not be read.\r\n2024.11.20 19:22:48.812512 [ 57402 ] {} <Fatal> BaseDaemon: This ClickHouse version is not official and should be upgraded to the official build.\r\n2024.11.20 19:22:48.813697 [ 57402 ] {} <Fatal> BaseDaemon: Changed settings: min_compress_block_size = 1551754, max_compress_block_size = 1708605, max_block_size = 87427, min_external_table_block_size_bytes = 0, max_joined_block_size_rows = 62505, max_insert_threads = 2, max_threads = 3, max_parsing_threads = 10, max_read_buffer_size = 1038258, connect_timeout_with_failover_ms = 2000, connect_timeout_with_failover_secure_ms = 3000, idle_connection_timeout = 36000, s3_max_get_rps = 1000000, s3_max_get_burst = 2000000, s3_max_put_rps = 1000000, s3_max_put_burst = 2000000, s3_check_objects_after_upload = true, max_remote_read_network_bandwidth = 1000000000000, max_remote_write_network_bandwidth = 1000000000000, max_local_read_bandwidth = 1000000000000, max_local_write_bandwidth = 1000000000000, stream_like_engine_allow_direct_select = true, enable_multiple_prewhere_read_steps = false, replication_wait_for_inactive_replica_timeout = 30, min_count_to_compile_aggregate_expression = 0, compile_sort_description = false, min_count_to_compile_sort_description = 0, group_by_two_level_threshold = 568044, group_by_two_level_threshold_bytes = 1, enable_memory_bound_merging_of_aggregation_results = false, allow_nonconst_timezone_arguments = true, min_chunk_bytes_for_parallel_parsing = 16217904, merge_tree_coarse_index_granularity = 29, min_bytes_to_use_direct_io = 10737418240, min_bytes_to_use_mmap_io = 1, log_queries = true, insert_quorum_timeout = 60000, merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability = 0.10000000149011612, http_response_buffer_size = 4410313, fsync_metadata = true, http_send_timeout = 60., http_receive_timeout = 60., use_index_for_in_with_subqueries_max_values = 1000000000, opentelemetry_start_trace_probability = 0.10000000149011612, enable_vertical_final = false, max_rows_to_read = 20000000, max_bytes_to_read = 1000000000000, max_bytes_to_read_leaf = 1000000000000, max_rows_to_group_by = 10000000000, max_bytes_before_external_group_by = 10737418240, max_rows_to_sort = 10000000000, max_bytes_to_sort = 10000000000, prefer_external_sort_block_bytes = 0, max_bytes_before_external_sort = 7247662025, max_bytes_before_remerge_sort = 1544157853, max_result_rows = 1000000000, max_result_bytes = 1000000000, max_execution_time = 600., max_execution_time_leaf = 600., max_execution_speed = 100000000000, max_execution_speed_bytes = 10000000000000, timeout_before_checking_execution_speed = 300., max_estimated_execution_time = 600., max_columns_to_read = 20000, max_temporary_columns = 20000, max_temporary_non_const_columns = 20000, max_rows_in_set = 10000000000, max_bytes_in_set = 10000000000, max_rows_in_join = 10000000000, max_bytes_in_join = 10000000000, cross_join_min_rows_to_compress = 1, cross_join_min_bytes_to_compress = 100000000, max_rows_to_transfer = 1000000000, max_bytes_to_transfer = 1000000000, max_rows_in_distinct = 10000000000, max_bytes_in_distinct = 10000000000, max_memory_usage = 5000000000, max_memory_usage_for_user = 32000000000, max_untracked_memory = 1048576, memory_profiler_step = 1048576, max_network_bandwidth = 100000000000, max_network_bytes = 1000000000000, max_network_bandwidth_for_user = 100000000000, max_network_bandwidth_for_all_users = 100000000000, max_temporary_data_on_disk_size_for_user = 100000000000, max_temporary_data_on_disk_size_for_query = 100000000000, max_backup_bandwidth = 100000000000, log_comment = '01172_transaction_counters.sql', send_logs_level = 'warning', prefer_localhost_replica = false, optimize_read_in_order = false, aggregation_in_order_max_block_bytes = 859747, read_in_order_two_level_merge_threshold = 95, max_hyperscan_regexp_length = 1000000, max_hyperscan_regexp_total_length = 10000000, allow_introspection_functions = true, database_atomic_wait_for_drop_and_detach_synchronously = true, optimize_or_like_chain = true, query_cache_max_size_in_bytes = 10000000, query_cache_max_entries = 100000, distributed_ddl_entry_format_version = 6, external_storage_max_read_rows = 10000000000, external_storage_max_read_bytes = 10000000000, local_filesystem_read_method = 'mmap', local_filesystem_read_prefetch = true, remote_filesystem_read_prefetch = false, merge_tree_min_bytes_per_task_for_remote_reading = 1048576, merge_tree_compact_parts_min_granules_to_multibuffer_read = 24, async_insert_busy_timeout_max_ms = 5000, enable_filesystem_cache = true, enable_filesystem_cache_on_write_operations = true, throw_on_error_from_cache_on_write_operations = true, filesystem_cache_segments_batch_size = 100, use_page_cache_for_disks_without_file_cache = true, load_marks_asynchronously = true, allow_prefetched_read_pool_for_remote_filesystem = true, allow_prefetched_read_pool_for_local_filesystem = false, filesystem_prefetch_max_memory_usage = 67108864, filesystem_prefetches_limit = 10, max_streams_for_merge_tree_reading = 1000, optimize_sorting_by_input_stream_properties = false, insert_keeper_max_retries = 100, insert_keeper_retry_initial_backoff_ms = 1, insert_keeper_retry_max_backoff_ms = 10, insert_keeper_fault_injection_probability = 0.009999999776482582, optimize_distinct_in_order = false, session_timezone = 'Africa/Khartoum', throw_on_unsupported_query_inside_transaction = false\r\n\r\n```\nSquashing added here https://github.com/ClickHouse/ClickHouse/pull/67782 \nif you guys were able to reproduce - pls tell me how\nI tried running `01172_transaction` (both reports fail on the same query) a few hundred times but didn't reproduce it. But now seeing #72213 I don't remember if I was running with a debug build or normal build\nI managed to got failure locally in debug build with setting specified in the original comment's log\r\n\r\n<details><summary>Details</summary>\r\n<p>\r\n\r\n```bash\r\n./tests/clickhouse-test -b ~/clickhouse --test-runs=16 --no-random-settings --client-option \\\r\nconnect_timeout_with_failover_ms='2000' \\\r\nconnect_timeout_with_failover_secure_ms='3000' \\\r\nidle_connection_timeout='36000' \\\r\ns3_max_get_rps='1000000' \\\r\ns3_max_get_burst='2000000' \\\r\ns3_max_put_rps='1000000' \\\r\ns3_max_put_burst='2000000' \\\r\ns3_check_objects_after_upload='true' \\\r\nmax_remote_read_network_bandwidth='1000000000000' \\\r\nmax_remote_write_network_bandwidth='1000000000000' \\\r\nmax_local_read_bandwidth='1000000000000' \\\r\nmax_local_write_bandwidth='1000000000000' \\\r\nstream_like_engine_allow_direct_select='true' \\\r\nreplication_wait_for_inactive_replica_timeout='30' \\\r\nallow_nonconst_timezone_arguments='true' \\\r\nlog_queries='true' \\\r\ninsert_quorum_timeout='60000' \\\r\nfsync_metadata='false' \\\r\nhttp_send_timeout='60' \\\r\nhttp_receive_timeout='60' \\\r\nuse_index_for_in_with_subqueries_max_values='1000000000' \\\r\nopentelemetry_start_trace_probability='0.10000000149011612' \\\r\nmax_rows_to_read='20000000' \\\r\nmax_bytes_to_read='1000000000000' \\\r\nmax_bytes_to_read_leaf='1000000000000' \\\r\nmax_rows_to_group_by='10000000000' \\\r\nmax_bytes_before_external_group_by='10000000000' \\\r\nmax_rows_to_sort='10000000000' \\\r\nmax_bytes_to_sort='10000000000' \\\r\nmax_bytes_before_external_sort='10000000000' \\\r\nmax_result_rows='1000000000' \\\r\nmax_result_bytes='1000000000' \\\r\nmax_execution_time='600' \\\r\nmax_execution_time_leaf='600' \\\r\nmax_execution_speed='100000000000' \\\r\nmax_execution_speed_bytes='10000000000000' \\\r\ntimeout_before_checking_execution_speed='300' \\\r\nmax_estimated_execution_time='600' \\\r\nmax_columns_to_read='20000' \\\r\nmax_temporary_columns='20000' \\\r\nmax_temporary_non_const_columns='20000' \\\r\nmax_rows_in_set='10000000000' \\\r\nmax_bytes_in_set='10000000000' \\\r\nmax_rows_in_join='10000000000' \\\r\nmax_bytes_in_join='10000000000' \\\r\nmax_rows_to_transfer='1000000000' \\\r\nmax_bytes_to_transfer='1000000000' \\\r\nmax_rows_in_distinct='10000000000' \\\r\nmax_bytes_in_distinct='10000000000' \\\r\nmax_memory_usage='5000000000' \\\r\nmax_memory_usage_for_user='32000000000' \\\r\nmax_untracked_memory='1048576' \\\r\nmemory_profiler_step='1048576' \\\r\nmax_network_bandwidth='100000000000' \\\r\nmax_network_bytes='1000000000000' \\\r\nmax_network_bandwidth_for_user='100000000000' \\\r\nmax_network_bandwidth_for_all_users='100000000000' \\\r\nmax_temporary_data_on_disk_size_for_user='100000000000' \\\r\nmax_temporary_data_on_disk_size_for_query='100000000000' \\\r\nmax_backup_bandwidth='100000000000' \\\r\nmax_hyperscan_regexp_length='1000000' \\\r\nmax_hyperscan_regexp_total_length='10000000' \\\r\nallow_introspection_functions='true' \\\r\ndatabase_atomic_wait_for_drop_and_detach_synchronously='true' \\\r\nquery_cache_max_size_in_bytes='10000000' \\\r\nquery_cache_max_entries='100000' \\\r\ndistributed_ddl_entry_format_version='6' \\\r\nexternal_storage_max_read_rows='10000000000' \\\r\nexternal_storage_max_read_bytes='10000000000' \\\r\nasync_insert_busy_timeout_max_ms='5000' \\\r\nenable_filesystem_cache='true' \\\r\nenable_filesystem_cache_on_write_operations='true' \\\r\nfilesystem_cache_segments_batch_size='10' \\\r\nload_marks_asynchronously='true' \\\r\nallow_prefetched_read_pool_for_remote_filesystem='false' \\\r\nallow_prefetched_read_pool_for_local_filesystem='false' \\\r\nfilesystem_prefetch_max_memory_usage='1073741824' \\\r\nmax_streams_for_merge_tree_reading='1000' \\\r\ninsert_keeper_max_retries='100' \\\r\ninsert_keeper_retry_initial_backoff_ms='1' \\\r\ninsert_keeper_retry_max_backoff_ms='10' \\\r\ninsert_keeper_fault_injection_probability='0.009999999776482582' \\\r\nthrow_on_unsupported_query_inside_transaction='false' \\\r\n-- 01172_transaction\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\nNot really stable, just rerunning command several times (even though it has `test-runs`)",
  "created_at": "2024-11-21T20:17:39Z",
  "modified_files": [
    "src/Interpreters/Squashing.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/03274_squashing_transform_sparse_bug.sql"
  ]
}