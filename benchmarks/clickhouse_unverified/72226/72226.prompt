You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Logical error in SquashingTransform in JOIN

https://s3.amazonaws.com/clickhouse-test-reports/72145/f45bd58849e2062e5875e921fb469aace6b7c5fe/stateless_tests__debug__s3_storage_.html

<details>
<summary>Logs</summary>



```
2024.11.20 20:48:21.403625 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> executeQuery: Query span trace_id for opentelemetry log: f97433a7-85af-f115-9355-e37765c132d8
2024.11.20 20:48:21.404051 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> executeQuery: (from [::1]:50732) (TID: (659, 34, 0ece01b8-09e0-4ce3-b83f-9a4dde5071c2), TIDH: 3128261553354376449) (comment: 01172_transaction_counters.sql) select 6, system.parts.name, txn_counters.creation_tid = system.parts.creation_tid from txn_counters join system.parts on txn_counters._part = system.parts.name where database=currentDatabase() and table='txn_counters' order by system.parts.name; (stage: Complete)
2024.11.20 20:48:21.404829 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> Planner: Query to stage Complete
2024.11.20 20:48:21.405128 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> HashJoin: Keys: [(_CAST(__table1._part, 'String'_String)) = (__table2.name)], datatype: EMPTY, kind: Inner, strictness: All, right header: __table2.name String String(size = 0), __table2.database String String(size = 0), __table2.table String String(size = 0), __table2.creation_tid Tuple(UInt64, UInt64, UUID) Tuple(size = 0, UInt64(size = 0), UInt64(size = 0), UUID(size = 0))
2024.11.20 20:48:21.405242 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> Planner: Query from stage FetchColumns to stage Complete
2024.11.20 20:48:21.405669 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> QueryPlanOptimizations: Pushed down filter and(equals(__table2.database, _CAST('test_nk0sp28r'_String, 'String'_String)), equals(__table2.table, 'txn_counters'_String)) to the Right side of join
2024.11.20 20:48:21.405931 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Key condition: unknown
2024.11.20 20:48:21.405963 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Filtering marks by primary and secondary keys
2024.11.20 20:48:21.407179 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Selected 3/3 parts by partition key, 3 parts by primary key, 3/3 marks by primary key, 3 marks to read from 3 ranges
2024.11.20 20:48:21.407204 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Trace> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Spreading mark ranges among streams (default reading)
2024.11.20 20:48:21.407272 [ 129688 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Debug> test_nk0sp28r.txn_counters (abed88dd-481e-42dd-9df6-518e66efc978) (SelectExecutor): Reading approx. 3 rows with 3 streams
2024.11.20 20:48:21.481882 [ 272308 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Fatal> : Logical error: '(isConst() || isSparse()) ? getDataType() == rhs.getDataType() : typeid(*this) == typeid(rhs)'.

2024.11.20 20:48:21.498156 [ 272308 ] {f6a06c6b-2710-4f97-828b-2ee25207af14} <Fatal> : Stack trace (when copying this message, always include the lines below):

0. /build/src/Common/StackTrace.cpp:380: StackTrace::tryCapture() @ 0x000000000c9f6172
1. /build/src/Common/Exception.cpp:53: DB::abortOnFailedAssertion(String const&) @ 0x000000000c9c5f75
2. /build/src/Columns/IColumn.h:749: DB::IColumn::assertTypeEquality(DB::IColumn const&) const @ 0x000000000729d442
3. /build/src/Columns/IColumn.h:233: DB::ColumnTuple::doInsertRangeFrom(DB::IColumn const&, unsigned long, unsigned long) @ 0x0000000012d8a43d
4. /build/src/Columns/IColumn.h:234: DB::Squashing::squash(std::vector<DB::Chunk, std::allocator<DB::Chunk>>&&, DB::CollectionOfDerivedItems<DB::ChunkInfo>&&) @ 0x000000001229874d
5. /build/src/Interpreters/Squashing.cpp:45: DB::Squashing::squash(DB::Chunk&&) @ 0x0000000012297d62
6. /build/src/Processors/Transforms/SquashingTransform.cpp:86: DB::SimpleSquashingChunksTransform::getRemaining() @ 0x0000000013e8bfab
7. /build/src/Processors/IInflatingTransform.cpp:86: DB::IInflatingTransform::work() @ 0x0000000013e4440d
8. /build/src/Processors/Executors/ExecutionThreadContext.cpp:49: DB::ExecutionThreadContext::executeTask() @ 0x0000000013bcc5a9
9. /build/src/Processors/Executors/PipelineExecutor.cpp:289: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x0000000013bc181d
10. /build/src/Processors/Executors/PipelineExecutor.cpp:255: DB::PipelineExecutor::executeSingleThread(unsigned long) @ 0x0000000013bc1abd
11. /build/src/Processors/Executors/PipelineExecutor.cpp:403: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x0000000013bc241f
12. /build/contrib/llvm-project/libcxx/include/__functional/function.h:848: ? @ 0x000000000ca7dc3f
13. /build/contrib/llvm-project/libcxx/include/__functional/invoke.h:359: ? @ 0x000000000ca82dc2
14. /build/contrib/llvm-project/libcxx/include/__functional/function.h:848: ? @ 0x000000000ca7b9ce
15. /build/contrib/llvm-project/libcxx/include/__functional/invoke.h:359: ? @ 0x000000000ca80e32
16. ? @ 0x00007f467a1f8ac3
17. ? @ 0x00007f467a28a850


2024.11.20 20:50:23.025802 [ 443534 ] <Fatal> BaseDaemon: Changed settings: connect_timeout_with_failover_ms = 2000, connect_timeout_with_failover_secure_ms = 3000, idle_connection_timeout = 36000, s3_max_get_rps = 1000000, s3_max_get_burst = 2000000, s3_max_put_rps = 1000000, s3_max_put_burst = 2000000, s3_check_objects_after_upload = true, max_remote_read_network_bandwidth = 1000000000000, max_remote_write_network_bandwidth = 1000000000000, max_local_read_bandwidth = 1000000000000, max_local_write_bandwidth = 1000000000000, stream_like_engine_allow_direct_select = true, replication_wait_for_inactive_replica_timeout = 30, allow_nonconst_timezone_arguments = true, log_queries = true, insert_quorum_timeout = 60000, fsync_metadata = false, http_send_timeout = 60., http_receive_timeout = 60., use_index_for_in_with_subqueries_max_values = 1000000000, opentelemetry_start_trace_probability = 0.10000000149011612, max_rows_to_read = 20000000, max_bytes_to_read = 1000000000000, max_bytes_to_read_leaf = 1000000000000, max_rows_to_group_by = 10000000000, max_bytes_before_external_group_by = 10000000000, max_rows_to_sort = 10000000000, max_bytes_to_sort = 10000000000, max_bytes_before_external_sort = 10000000000, max_result_rows = 1000000000, max_result_bytes = 1000000000, max_execution_time = 600., max_execution_time_leaf = 600., max_execution_speed = 100000000000, max_execution_speed_bytes = 10000000000000, timeout_before_checking_execution_speed = 300., max_estimated_execution_time = 600., max_columns_to_read = 20000, max_temporary_columns = 20000, max_temporary_non_const_columns = 20000, max_rows_in_set = 10000000000, max_bytes_in_set = 10000000000, max_rows_in_join = 10000000000, max_bytes_in_join = 10000000000, max_rows_to_transfer = 1000000000, max_bytes_to_transfer = 1000000000, max_rows_in_distinct = 10000000000, max_bytes_in_distinct = 10000000000, max_memory_usage = 5000000000, max_memory_usage_for_user = 32000000000, max_untracked_memory = 1048576, memory_profiler_step = 1048576, max_network_bandwidth = 100000000000, max_network_bytes = 1000000000000, max_network_bandwidth_for_user = 100000000000, max_network_bandwidth_for_all_users = 100000000000, max_temporary_data_on_disk_size_for_user = 100000000000, max_temporary_data_on_disk_size_for_query = 100000000000, max_backup_bandwidth = 100000000000, log_comment = '01172_transaction_counters.sql', send_logs_level = 'warning', max_hyperscan_regexp_length = 1000000, max_hyperscan_regexp_total_length = 10000000, allow_introspection_functions = true, database_atomic_wait_for_drop_and_detach_synchronously = true, query_cache_max_size_in_bytes = 10000000, query_cache_max_entries = 100000, distributed_ddl_entry_format_version = 6, external_storage_max_read_rows = 10000000000, external_storage_max_read_bytes = 10000000000, async_insert_busy_timeout_max_ms = 5000, enable_filesystem_cache = true, enable_filesystem_cache_on_write_operations = true, filesystem_cache_segments_batch_size = 10, load_marks_asynchronously = true, allow_prefetched_read_pool_for_remote_filesystem = false, allow_prefetched_read_pool_for_local_filesystem = false, filesystem_prefetch_max_memory_usage = 1073741824, max_streams_for_merge_tree_reading = 1000, insert_keeper_max_retries = 100, insert_keeper_retry_initial_backoff_ms = 1, insert_keeper_retry_max_backoff_ms = 10, insert_keeper_fault_injection_probability = 0.009999999776482582, throw_on_unsupported_query_inside_transaction = false
```


</details>
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
