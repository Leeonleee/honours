{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 20335,
  "instance_id": "ClickHouse__ClickHouse-20335",
  "issue_numbers": [
    "20203"
  ],
  "base_commit": "17af32a59ba93f0e910aeb154df4ba35a410bbad",
  "patch": "diff --git a/src/Storages/MergeTree/BackgroundJobsExecutor.cpp b/src/Storages/MergeTree/BackgroundJobsExecutor.cpp\nindex 3e3f693addde..8e5a0e8a3b80 100644\n--- a/src/Storages/MergeTree/BackgroundJobsExecutor.cpp\n+++ b/src/Storages/MergeTree/BackgroundJobsExecutor.cpp\n@@ -98,11 +98,21 @@ try\n                 {\n                     try /// We don't want exceptions in background pool\n                     {\n-                        job();\n+                        bool job_success = job();\n                         /// Job done, decrement metric and reset no_work counter\n                         CurrentMetrics::values[pool_config.tasks_metric]--;\n-                        /// Job done, new empty space in pool, schedule background task\n-                        runTaskWithoutDelay();\n+\n+                        if (job_success)\n+                        {\n+                            /// Job done, new empty space in pool, schedule background task\n+                            runTaskWithoutDelay();\n+                        }\n+                        else\n+                        {\n+                            /// Job done, but failed, schedule with backoff\n+                            scheduleTask(/* with_backoff = */ true);\n+                        }\n+\n                     }\n                     catch (...)\n                     {\ndiff --git a/src/Storages/MergeTree/BackgroundJobsExecutor.h b/src/Storages/MergeTree/BackgroundJobsExecutor.h\nindex 85067188f094..da22c752e1b6 100644\n--- a/src/Storages/MergeTree/BackgroundJobsExecutor.h\n+++ b/src/Storages/MergeTree/BackgroundJobsExecutor.h\n@@ -36,10 +36,12 @@ enum class PoolType\n     FETCH,\n };\n \n+using BackgroundJobFunc = std::function<bool()>;\n+\n /// Result from background job providers. Function which will be executed in pool and pool type.\n struct JobAndPool\n {\n-    ThreadPool::Job job;\n+    BackgroundJobFunc job;\n     PoolType pool_type;\n };\n \ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex c6e77a56db65..4458b5735bb6 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -3796,7 +3796,7 @@ std::optional<JobAndPool> MergeTreeData::getDataMovingJob()\n \n     return JobAndPool{[this, moving_tagger] () mutable\n     {\n-        moveParts(moving_tagger);\n+        return moveParts(moving_tagger);\n     }, PoolType::MOVE};\n }\n \ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 11a159d4a6c5..202e909af0fe 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -962,9 +962,11 @@ std::optional<JobAndPool> StorageMergeTree::getDataProcessingJob()\n         return JobAndPool{[this, metadata_snapshot, merge_entry, mutate_entry, share_lock] () mutable\n         {\n             if (merge_entry)\n-                mergeSelectedParts(metadata_snapshot, false, {}, *merge_entry, share_lock);\n+                return mergeSelectedParts(metadata_snapshot, false, {}, *merge_entry, share_lock);\n             else if (mutate_entry)\n-                mutateSelectedPart(metadata_snapshot, *mutate_entry, share_lock);\n+                return mutateSelectedPart(metadata_snapshot, *mutate_entry, share_lock);\n+\n+            __builtin_unreachable();\n         }, PoolType::MERGE_MUTATE};\n     }\n     else if (auto lock = time_after_previous_cleanup.compareAndRestartDeferred(1))\n@@ -978,6 +980,7 @@ std::optional<JobAndPool> StorageMergeTree::getDataProcessingJob()\n             clearOldWriteAheadLogs();\n             clearOldMutations();\n             clearEmptyParts();\n+            return true;\n         }, PoolType::MERGE_MUTATE};\n     }\n     return {};\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 53104efeb433..097b7679899d 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -2682,7 +2682,7 @@ std::optional<JobAndPool> StorageReplicatedMergeTree::getDataProcessingJob()\n \n     return JobAndPool{[this, selected_entry] () mutable\n     {\n-        processQueueEntry(selected_entry);\n+        return processQueueEntry(selected_entry);\n     }, pool_type};\n }\n \n",
  "test_patch": "diff --git a/src/Storages/tests/gtest_background_executor.cpp b/src/Storages/tests/gtest_background_executor.cpp\nindex bf9a305ccc9e..0ddf2d9ea2a1 100644\n--- a/src/Storages/tests/gtest_background_executor.cpp\n+++ b/src/Storages/tests/gtest_background_executor.cpp\n@@ -32,7 +32,7 @@ class TestJobExecutor : public IBackgroundJobExecutor\n \n     std::optional<JobAndPool> getBackgroundJob() override\n     {\n-        return JobAndPool{[] { std::this_thread::sleep_for(1s); counter++; }, PoolType::MERGE_MUTATE};\n+        return JobAndPool{[] { std::this_thread::sleep_for(1s); counter++; return true; }, PoolType::MERGE_MUTATE};\n     }\n };\n \ndiff --git a/tests/queries/0_stateless/01715_background_checker_blather_zookeeper.reference b/tests/queries/0_stateless/01715_background_checker_blather_zookeeper.reference\nnew file mode 100644\nindex 000000000000..d00491fd7e5b\n--- /dev/null\n+++ b/tests/queries/0_stateless/01715_background_checker_blather_zookeeper.reference\n@@ -0,0 +1,1 @@\n+1\ndiff --git a/tests/queries/0_stateless/01715_background_checker_blather_zookeeper.sql b/tests/queries/0_stateless/01715_background_checker_blather_zookeeper.sql\nnew file mode 100644\nindex 000000000000..66b533695176\n--- /dev/null\n+++ b/tests/queries/0_stateless/01715_background_checker_blather_zookeeper.sql\n@@ -0,0 +1,28 @@\n+DROP TABLE IF EXISTS i20203_1;\n+DROP TABLE IF EXISTS i20203_2;\n+\n+CREATE TABLE i20203_1 (a Int8)\n+ENGINE = ReplicatedMergeTree('/clickhouse/01715_background_checker_i20203', 'r1')\n+ORDER BY tuple();\n+\n+CREATE TABLE i20203_2 (a Int8)\n+ENGINE = ReplicatedMergeTree('/clickhouse/01715_background_checker_i20203', 'r2')\n+ORDER BY tuple();\n+\n+DETACH TABLE i20203_2;\n+INSERT INTO i20203_1 VALUES (2);\n+\n+DETACH TABLE i20203_1;\n+ATTACH TABLE i20203_2;\n+\n+-- sleep 10 seconds\n+SELECT number from numbers(10) where sleepEachRow(1) Format Null;\n+\n+SELECT num_tries < 50\n+FROM system.replication_queue\n+WHERE table = 'i20203_2' AND database = currentDatabase();\n+\n+ATTACH TABLE i20203_1;\n+\n+DROP TABLE IF EXISTS i20203_1;\n+DROP TABLE IF EXISTS i20203_2;\ndiff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json\nindex d76603bf633a..53fcfe8b13f1 100644\n--- a/tests/queries/skip_list.json\n+++ b/tests/queries/skip_list.json\n@@ -572,6 +572,7 @@\n         \"01603_rename_overwrite_bug\",\n         \"01646_system_restart_replicas_smoke\", // system restart replicas is a global query\n         \"01676_dictget_in_default_expression\",\n+        \"01715_background_checker_blather_zookeeper\",\n         \"attach\",\n         \"ddl_dictionaries\",\n         \"dictionary\",\n",
  "problem_statement": "ReplicatedMergeTreePartCheckThread spam in log\nAfter update to version 20.12.4.5, Clickhouse started spamming to log messages \r\n```\r\n2021.02.08 10:42:16.976140 [ 40737 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 20180521_267052_277907_5309. Hoping that it will eventually appear as a result of a merge.\r\n2021.02.08 10:42:16.976225 [ 40806 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking part 20180521_267052_277903_5305\r\n2021.02.08 10:42:16.977196 [ 40806 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 20180521_267052_277903_5305 or covering part.\r\n2021.02.08 10:42:16.980575 [ 40806 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 20180521_267052_277903_5305. Hoping that it will eventually appear as a result of a merge.\r\n2021.02.08 10:42:16.980663 [ 40878 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking part 20180521_267052_277905_5307\r\n2021.02.08 10:42:16.981219 [ 40878 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 20180521_267052_277905_5307 or covering part.\r\n2021.02.08 10:42:16.984490 [ 40878 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 20180521_267052_277905_5307. Hoping that it will eventually appear as a result of a merge.\r\n2021.02.08 10:42:16.984565 [ 40890 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking part 20180521_267052_277904_5306\r\n2021.02.08 10:42:16.985117 [ 40890 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 20180521_267052_277904_5306 or covering part.\r\n2021.02.08 10:42:16.988831 [ 40890 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 20180521_267052_277904_5306. Hoping that it will eventually appear as a result of a merge.\r\n2021.02.08 10:42:16.988913 [ 40884 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking part 20180521_267052_277906_5308\r\n2021.02.08 10:42:16.989523 [ 40884 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 20180521_267052_277906_5308 or covering part.\r\n2021.02.08 10:42:16.992210 [ 40884 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 20180521_267052_277906_5308. Hoping that it will eventually appear as a result of a merge.\r\n2021.02.08 10:42:16.992319 [ 40806 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking part 20180521_267052_277907_5309\r\n2021.02.08 10:42:16.993439 [ 40806 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 20180521_267052_277907_5309 or covering part.\r\n2021.02.08 10:42:16.996080 [ 40806 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 20180521_267052_277907_5309. Hoping that it will eventually appear as a result of a merge.\r\n2021.02.08 10:42:16.996177 [ 40730 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking part 20180521_267052_277905_5307\r\n2021.02.08 10:42:16.997928 [ 40730 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 20180521_267052_277905_5307 or covering part.\r\n2021.02.08 10:42:17.001236 [ 40730 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 20180521_267052_277905_5307. Hoping that it will eventually appear as a result of a merge.\r\n2021.02.08 10:42:17.001429 [ 40785 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking part 20180521_267052_277903_5305\r\n2021.02.08 10:42:17.002155 [ 40785 ] {} <Warning> base.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 20180521_267052_277903_5305 or covering part.\r\n```\r\nThese messages have appeared before, but not in such numbers. After update log grows by 1GB in hours.\r\nCan we solve problem that causes them?\r\nAlso we have huge network traffic on that CH nodes after update\r\nhttps://imgur.com/sdt8DuJ\r\n\r\nwe try update to 21.2.2.8 but nothing changes\n",
  "hints_text": "Do you have any non-default settings? For example `always_fetch_merged_part` ?\r\nWhat do you see in system.replication_queue? In system.merges? In system.replicas?\r\n\r\n`20180521_267052_277905_5307 ` - can you see that part in system.parts? \r\n\r\nWhat is the count of active/inactive parts in system.parts? \r\n\r\nDo you run lot of mutations? \n> 20180521_267052_277905_5307 - can you see that part in system.parts?\r\n\r\nno, this data deleted a long ago\r\n\r\n> What do you see in system.replication_queue\r\n\r\ni see 5 tasks for these failed parts, i think we need delete them. I should delete them from zookeeper?\r\ntask like this\r\n```\r\n{\r\n            \"database\": \"db\",\r\n            \"table\": \"table\",\r\n            \"replica_name\": \"ch07\",\r\n            \"position\": 3,\r\n            \"node_name\": \"queue-0001493187\",\r\n            \"type\": \"GET_PART\",\r\n            \"create_time\": \"2018-05-24 09:04:07\",\r\n            \"required_quorum\": 0,\r\n            \"source_replica\": \"\",\r\n            \"new_part_name\": \"20180521_260038_266440_1515\",\r\n            \"parts_to_merge\": [],\r\n            \"is_detach\": 0,\r\n            \"is_currently_executing\": 1,\r\n            \"num_tries\": **1639302**,\r\n            \"last_exception\": \"Code: 234, e.displayText() = DB::Exception: No active replica has part 20180521_260038_266440_1515 or covering part (version 21.2.2.8 (official build))\",\r\n            \"last_attempt_time\": \"2021-02-08 13:13:00\",\r\n            \"num_postponed\": 2366739,\r\n            \"postpone_reason\": \"Not executing log entry queue-0001493187 for part 20180521_260038_266440_1515 because it is covered by part 20180521_260038_266441_1516 that is currently executing.\",\r\n            \"last_postpone_time\": \"2021-02-08 13:13:00\",\r\n            \"merge_type\": \"\"\r\n        }\r\n```\r\n> What is the count of active/inactive parts in system.parts?\r\n\r\n22000/500\r\n\n> i see 5 tasks for these failed parts, i think we need delete them. I should delete them from zookeeper?\r\n\r\nIt looks like your table is partitioned by a date.\r\n`alter table ... drop partition id '20180521'` should delete the task\n> It looks like your table is partitioned by a date.\r\n> alter table ... drop partition id '20180521' should delete the task\r\n\r\nnot work, after i try this - receive error\r\n`Code: 232. DB::Exception: Received from ch07:9000. DB::Exception: No part 20180521_260038_266441_1516 in committed stat`\r\nBut after i delete tasks from zookeeper and restart clickhouse, problem was solved. Warnings in log disappeared, network traffic \r\n has gone. Everything looks good now.\nIt seems ClickHouse works faster than ever now, postponing a task several times per second:\r\n```\r\n2021.02.09 14:40:32.483971 [ 6336 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Not executing log entry queue-0000211656 of type MERGE_PARTS for part 20210208_379_416_2 because merges and mutations are cancelled now.\r\n2021.02.09 14:40:33.484395 [ 6334 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Not executing log entry queue-0000211656 of type MERGE_PARTS for part 20210208_379_416_2 because merges and mutations are cancelled now.\r\n2021.02.09 14:40:34.333846 [ 6328 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Not executing log entry queue-0000211656 of type MERGE_PARTS for part 20210208_379_416_2 because merges and mutations are cancelled now.\r\n2021.02.09 14:40:34.648256 [ 6322 ] {} <Debug> db.table (ReplicatedMergeTreeQueue): Not executing log entry queue-0000211656 of type MERGE_PARTS for part 20210208_379_416_2 because merges and mutations are cancelled now.\r\n```\n20.12 is affected as well\r\n```\r\ncreate table i20203_1 (a Int8) engine=ReplicatedMergeTree('/clickhouse/tables/i20203','r1') order by tuple();\r\ncreate table i20203_2 (a Int8) engine=ReplicatedMergeTree('/clickhouse/tables/i20203','r2') order by tuple();\r\ndetach table i20203_2;\r\ninsert into i20203_1 values (2);\r\ndetach table i20203_1;\r\nattach table i20203_2;\r\n\r\nSELECT\r\n    create_time,\r\n    num_tries,\r\n    last_attempt_time\r\nFROM system.replication_queue\r\nWHERE table = 'i20203_2'\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500create_time\u2500\u252c\u2500num_tries\u2500\u252c\u2500\u2500\u2500last_attempt_time\u2500\u2510\r\n\u2502 2021-02-10 16:16:55 \u2502     58480 \u2502 2021-02-10 16:18:14 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\nhttps://github.com/ClickHouse/ClickHouse/pull/15983/ ? \n20.11.5.18\r\n```\r\nSELECT\r\n    create_time,\r\n    num_tries,\r\n    last_attempt_time\r\nFROM system.replication_queue\r\nWHERE table = 'i20203_2'\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500create_time\u2500\u252c\u2500num_tries\u2500\u252c\u2500\u2500\u2500last_attempt_time\u2500\u2510\r\n\u2502 2021-02-10 16:26:56 \u2502        36 \u2502 2021-02-10 16:28:36 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```",
  "created_at": "2021-02-11T11:48:12Z",
  "modified_files": [
    "src/Storages/MergeTree/BackgroundJobsExecutor.cpp",
    "src/Storages/MergeTree/BackgroundJobsExecutor.h",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/StorageMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.cpp"
  ],
  "modified_test_files": [
    "src/Storages/tests/gtest_background_executor.cpp",
    "b/tests/queries/0_stateless/01715_background_checker_blather_zookeeper.reference",
    "b/tests/queries/0_stateless/01715_background_checker_blather_zookeeper.sql",
    "tests/queries/skip_list.json"
  ]
}