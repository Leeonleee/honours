{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 51823,
  "instance_id": "ClickHouse__ClickHouse-51823",
  "issue_numbers": [
    "46102"
  ],
  "base_commit": "7d4c97e8f37b1b249f49ba5cee51ab46ba4a5fb5",
  "patch": "diff --git a/docs/en/operations/settings/query-complexity.md b/docs/en/operations/settings/query-complexity.md\nindex 15f39b53e076..82cf4953aaea 100644\n--- a/docs/en/operations/settings/query-complexity.md\n+++ b/docs/en/operations/settings/query-complexity.md\n@@ -163,7 +163,27 @@ If you set `timeout_before_checking_execution_speed `to 0, ClickHouse will use c\n \n ## timeout_overflow_mode {#timeout-overflow-mode}\n \n-What to do if the query is run longer than \u2018max_execution_time\u2019: \u2018throw\u2019 or \u2018break\u2019. By default, throw.\n+What to do if the query is run longer than `max_execution_time`: `throw` or `break`. By default, `throw`.\n+\n+# max_execution_time_leaf\n+\n+Similar semantic to `max_execution_time` but only apply on leaf node for distributed or remote queries.\n+\n+For example, if we want to limit execution time on leaf node to `10s` but no limit on the initial node, instead of having `max_execution_time` in the nested subquery settings:\n+\n+``` sql\n+SELECT count() FROM cluster(cluster, view(SELECT * FROM t SETTINGS max_execution_time = 10));\n+```\n+\n+We can use `max_execution_time_leaf` as the query settings:\n+\n+``` sql\n+SELECT count() FROM cluster(cluster, view(SELECT * FROM t)) SETTINGS max_execution_time_leaf = 10;\n+```\n+\n+# timeout_overflow_mode_leaf\n+\n+What to do when the query in leaf node run longer than `max_execution_time_leaf`: `throw` or `break`. By default, `throw`.\n \n ## min_execution_speed {#min-execution-speed}\n \ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 7387c8a791e2..cc972c4f4279 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -364,16 +364,16 @@ class IColumn;\n     M(UInt64, max_bytes_to_read, 0, \"Limit on read bytes (after decompression) from the most 'deep' sources. That is, only in the deepest subquery. When reading from a remote server, it is only checked on a remote server.\", 0) \\\n     M(OverflowMode, read_overflow_mode, OverflowMode::THROW, \"What to do when the limit is exceeded.\", 0) \\\n     \\\n-    M(UInt64, max_rows_to_read_leaf, 0, \"Limit on read rows on the leaf nodes for distributed queries. Limit is applied for local reads only excluding the final merge stage on the root node. Note, the setting is unstable with prefer_localhost_replica=1.\", 0) \\\n-    M(UInt64, max_bytes_to_read_leaf, 0, \"Limit on read bytes (after decompression) on the leaf nodes for distributed queries. Limit is applied for local reads only excluding the final merge stage on the root node. Note, the setting is unstable with prefer_localhost_replica=1.\", 0) \\\n+    M(UInt64, max_rows_to_read_leaf, 0, \"Limit on read rows on the leaf nodes for distributed queries. Limit is applied for local reads only, excluding the final merge stage on the root node. Note, the setting is unstable with prefer_localhost_replica=1.\", 0) \\\n+    M(UInt64, max_bytes_to_read_leaf, 0, \"Limit on read bytes (after decompression) on the leaf nodes for distributed queries. Limit is applied for local reads only, excluding the final merge stage on the root node. Note, the setting is unstable with prefer_localhost_replica=1.\", 0) \\\n     M(OverflowMode, read_overflow_mode_leaf, OverflowMode::THROW, \"What to do when the leaf limit is exceeded.\", 0) \\\n     \\\n-    M(UInt64, max_rows_to_group_by, 0, \"If aggregation during GROUP BY is generating more than specified number of rows (unique GROUP BY keys), the behavior will be determined by the 'group_by_overflow_mode' which by default is - throw an exception, but can be also switched to an approximate GROUP BY mode.\", 0) \\\n+    M(UInt64, max_rows_to_group_by, 0, \"If aggregation during GROUP BY is generating more than the specified number of rows (unique GROUP BY keys), the behavior will be determined by the 'group_by_overflow_mode' which by default is - throw an exception, but can be also switched to an approximate GROUP BY mode.\", 0) \\\n     M(OverflowModeGroupBy, group_by_overflow_mode, OverflowMode::THROW, \"What to do when the limit is exceeded.\", 0) \\\n     M(UInt64, max_bytes_before_external_group_by, 0, \"If memory usage during GROUP BY operation is exceeding this threshold in bytes, activate the 'external aggregation' mode (spill data to disk). Recommended value is half of available system memory.\", 0) \\\n     \\\n-    M(UInt64, max_rows_to_sort, 0, \"If more than specified amount of records have to be processed for ORDER BY operation, the behavior will be determined by the 'sort_overflow_mode' which by default is - throw an exception\", 0) \\\n-    M(UInt64, max_bytes_to_sort, 0, \"If more than specified amount of (uncompressed) bytes have to be processed for ORDER BY operation, the behavior will be determined by the 'sort_overflow_mode' which by default is - throw an exception\", 0) \\\n+    M(UInt64, max_rows_to_sort, 0, \"If more than the specified amount of records have to be processed for ORDER BY operation, the behavior will be determined by the 'sort_overflow_mode' which by default is - throw an exception\", 0) \\\n+    M(UInt64, max_bytes_to_sort, 0, \"If more than the specified amount of (uncompressed) bytes have to be processed for ORDER BY operation, the behavior will be determined by the 'sort_overflow_mode' which by default is - throw an exception\", 0) \\\n     M(OverflowMode, sort_overflow_mode, OverflowMode::THROW, \"What to do when the limit is exceeded.\", 0) \\\n     M(UInt64, max_bytes_before_external_sort, 0, \"If memory usage during ORDER BY operation is exceeding this threshold in bytes, activate the 'external sorting' mode (spill data to disk). Recommended value is half of available system memory.\", 0) \\\n     M(UInt64, max_bytes_before_remerge_sort, 1000000000, \"In case of ORDER BY with LIMIT, when memory usage is higher than specified threshold, perform additional steps of merging blocks before final merge to keep just top LIMIT rows.\", 0) \\\n@@ -384,8 +384,10 @@ class IColumn;\n     M(OverflowMode, result_overflow_mode, OverflowMode::THROW, \"What to do when the limit is exceeded.\", 0) \\\n     \\\n     /* TODO: Check also when merging and finalizing aggregate functions. */ \\\n-    M(Seconds, max_execution_time, 0, \"If query run time exceeded the specified number of seconds, the behavior will be determined by the 'timeout_overflow_mode' which by default is - throw an exception. Note that the timeout is checked and query can stop only in designated places during data processing. It currently cannot stop during merging of aggregation states or during query analysis, and the actual run time will be higher than the value of this setting.\", 0) \\\n+    M(Seconds, max_execution_time, 0, \"If query runtime exceeds the specified number of seconds, the behavior will be determined by the 'timeout_overflow_mode', which by default is - throw an exception. Note that the timeout is checked and query can stop only in designated places during data processing. It currently cannot stop during merging of aggregation states or during query analysis, and the actual run time will be higher than the value of this setting.\", 0) \\\n     M(OverflowMode, timeout_overflow_mode, OverflowMode::THROW, \"What to do when the limit is exceeded.\", 0) \\\n+    M(Seconds, max_execution_time_leaf, 0, \"Similar semantic to max_execution_time but only apply on leaf node for distributed queries, the time out behavior will be determined by 'timeout_overflow_mode_leaf' which by default is - throw an exception\", 0) \\\n+    M(OverflowMode, timeout_overflow_mode_leaf, OverflowMode::THROW, \"What to do when the leaf limit is exceeded.\", 0) \\\n     \\\n     M(UInt64, min_execution_speed, 0, \"Minimum number of execution rows per second.\", 0) \\\n     M(UInt64, max_execution_speed, 0, \"Maximum number of execution rows per second.\", 0) \\\n@@ -399,7 +401,7 @@ class IColumn;\n     \\\n     M(UInt64, max_sessions_for_user, 0, \"Maximum number of simultaneous sessions for a user.\", 0) \\\n     \\\n-    M(UInt64, max_subquery_depth, 100, \"If a query has more than specified number of nested subqueries, throw an exception. This allows you to have a sanity check to protect the users of your cluster from going insane with their queries.\", 0) \\\n+    M(UInt64, max_subquery_depth, 100, \"If a query has more than the specified number of nested subqueries, throw an exception. This allows you to have a sanity check to protect the users of your cluster from going insane with their queries.\", 0) \\\n     M(UInt64, max_analyze_depth, 5000, \"Maximum number of analyses performed by interpreter.\", 0) \\\n     M(UInt64, max_ast_depth, 1000, \"Maximum depth of query syntax tree. Checked after parsing.\", 0) \\\n     M(UInt64, max_ast_elements, 50000, \"Maximum size of query syntax tree in number of nodes. Checked after parsing.\", 0) \\\ndiff --git a/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp b/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp\nindex 3935028f27c6..4edc9d4d4e5d 100644\n--- a/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp\n+++ b/src/Interpreters/ClusterProxy/SelectStreamFactory.cpp\n@@ -1,20 +1,21 @@\n-#include <Interpreters/ClusterProxy/SelectStreamFactory.h>\n-#include <Interpreters/Cluster.h>\n #include <Storages/StorageReplicatedMergeTree.h>\n #include <Storages/VirtualColumnUtils.h>\n #include <Common/Exception.h>\n #include <Common/ProfileEvents.h>\n #include <Common/checkStackSize.h>\n+#include <Common/logger_useful.h>\n+#include <Common/FailPoint.h>\n #include <TableFunctions/TableFunctionFactory.h>\n #include <IO/ConnectionTimeouts.h>\n+#include <Interpreters/ClusterProxy/SelectStreamFactory.h>\n+#include <Interpreters/Cluster.h>\n #include <Interpreters/AddDefaultDatabaseVisitor.h>\n #include <Interpreters/RequiredSourceColumnsVisitor.h>\n #include <Interpreters/TranslateQualifiedNamesVisitor.h>\n #include <DataTypes/ObjectUtils.h>\n-\n #include <Client/IConnections.h>\n-#include <Common/logger_useful.h>\n-#include <Common/FailPoint.h>\n+#include <Parsers/ASTSelectQuery.h>\n+#include <Parsers/ASTSetQuery.h>\n #include <Processors/QueryPlan/QueryPlan.h>\n #include <Processors/QueryPlan/ReadFromRemote.h>\n #include <Processors/QueryPlan/ExpressionStep.h>\n@@ -22,6 +23,7 @@\n #include <Processors/QueryPlan/DistributedCreateLocalPlan.h>\n #include <Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h>\n \n+\n namespace ProfileEvents\n {\n     extern const Event DistributedConnectionMissingTable;\n@@ -121,6 +123,7 @@ void SelectStreamFactory::createForShard(\n     if (it != objects_by_shard.end())\n         replaceMissedSubcolumnsByConstants(storage_snapshot->object_columns, it->second, query_ast);\n \n+\n     auto emplace_local_stream = [&]()\n     {\n         local_plans.emplace_back(createLocalPlan(\ndiff --git a/src/Interpreters/ClusterProxy/executeQuery.cpp b/src/Interpreters/ClusterProxy/executeQuery.cpp\nindex 41235d107cd5..420bb4470274 100644\n--- a/src/Interpreters/ClusterProxy/executeQuery.cpp\n+++ b/src/Interpreters/ClusterProxy/executeQuery.cpp\n@@ -141,6 +141,14 @@ ContextMutablePtr updateSettingsForCluster(const Cluster & cluster,\n             new_settings.allow_experimental_parallel_reading_from_replicas = false;\n     }\n \n+    if (settings.max_execution_time_leaf.value > 0)\n+    {\n+        /// Replace 'max_execution_time' of this sub-query with 'max_execution_time_leaf' and 'timeout_overflow_mode'\n+        /// with 'timeout_overflow_mode_leaf'\n+        new_settings.max_execution_time = settings.max_execution_time_leaf;\n+        new_settings.timeout_overflow_mode = settings.timeout_overflow_mode_leaf;\n+    }\n+\n     auto new_context = Context::createCopy(context);\n     new_context->setSettings(new_settings);\n     return new_context;\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02786_max_execution_time_leaf.reference b/tests/queries/0_stateless/02786_max_execution_time_leaf.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02786_max_execution_time_leaf.sql b/tests/queries/0_stateless/02786_max_execution_time_leaf.sql\nnew file mode 100644\nindex 000000000000..1d02e82569c8\n--- /dev/null\n+++ b/tests/queries/0_stateless/02786_max_execution_time_leaf.sql\n@@ -0,0 +1,4 @@\n+-- Tags: no-fasttest\n+SELECT count() FROM cluster('test_cluster_two_shards', view( SELECT * FROM numbers(100000000000) )) SETTINGS max_execution_time_leaf = 1; -- { serverError 159 }\n+-- Can return partial result\n+SELECT count() FROM cluster('test_cluster_two_shards', view( SELECT * FROM numbers(100000000000) )) FORMAT Null SETTINGS max_execution_time_leaf = 1, timeout_overflow_mode_leaf = 'break';\n",
  "problem_statement": "Add max_execution_time_leaf\n\r\n**Use case**\r\nCurrently there is max_execution_time setting, that can be set in subqueries. But it makes it hard to use this setting with distributed sub-queries, as it is likely that timeout will be reached before distributed subquery finished. \r\n\r\n**Describe the solution you'd like**\r\n\r\nAdd max_execution_time_leaf settings similar to the other restrictions on query complexity that only apply to distributed sub queries.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nIt is possible to workaround this using features like view, or dyanmically with cluster(view()) to set manually timeout in the subquery since we have control over them and they are not generated internally. But it is much less handy than simply using distributed tables, and for some reason other max_* seettings have a leaf version.\r\n\n",
  "hints_text": "",
  "created_at": "2023-07-05T11:07:18Z",
  "modified_files": [
    "docs/en/operations/settings/query-complexity.md",
    "src/Core/Settings.h",
    "src/Interpreters/ClusterProxy/SelectStreamFactory.cpp",
    "src/Interpreters/ClusterProxy/executeQuery.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/02786_max_execution_time_leaf.sql"
  ]
}