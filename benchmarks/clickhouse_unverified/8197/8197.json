{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 8197,
  "instance_id": "ClickHouse__ClickHouse-8197",
  "issue_numbers": [
    "8180"
  ],
  "base_commit": "399995d60b9661828d6562d79c1984dbcfcf3eca",
  "patch": "diff --git a/dbms/src/Storages/Kafka/StorageKafka.cpp b/dbms/src/Storages/Kafka/StorageKafka.cpp\nindex f33a237a56a3..239eade0354c 100644\n--- a/dbms/src/Storages/Kafka/StorageKafka.cpp\n+++ b/dbms/src/Storages/Kafka/StorageKafka.cpp\n@@ -96,7 +96,8 @@ StorageKafka::StorageKafka(\n                             {\"_timestamp\", std::make_shared<DataTypeNullable>(std::make_shared<DataTypeDateTime>())}}, true))\n     , table_name(table_name_)\n     , database_name(database_name_)\n-    , global_context(context_)\n+    , global_context(context_.getGlobalContext())\n+    , kafka_context(Context(global_context))\n     , topics(global_context.getMacros()->expand(topics_))\n     , brokers(global_context.getMacros()->expand(brokers_))\n     , group(global_context.getMacros()->expand(group_))\n@@ -110,6 +111,8 @@ StorageKafka::StorageKafka(\n     , skip_broken(skip_broken_)\n     , intermediate_commit(intermediate_commit_)\n {\n+    kafka_context.makeQueryContext();\n+\n     setColumns(columns_);\n     task = global_context.getSchedulePool().createTask(log->name(), [this]{ threadFunc(); });\n     task->deactivate();\n@@ -375,7 +378,7 @@ bool StorageKafka::streamToViews()\n \n     // Create a stream for each consumer and join them in a union stream\n     // Only insert into dependent views and expect that input blocks contain virtual columns\n-    InterpreterInsertQuery interpreter(insert, global_context, false, true, true);\n+    InterpreterInsertQuery interpreter(insert, kafka_context, false, true, true);\n     auto block_io = interpreter.execute();\n \n     // Create a stream for each consumer and join them in a union stream\n@@ -384,7 +387,7 @@ bool StorageKafka::streamToViews()\n     for (size_t i = 0; i < num_created_consumers; ++i)\n     {\n         auto stream\n-            = std::make_shared<KafkaBlockInputStream>(*this, global_context, block_io.out->getHeader().getNames(), block_size, false);\n+            = std::make_shared<KafkaBlockInputStream>(*this, kafka_context, block_io.out->getHeader().getNames(), block_size, false);\n         streams.emplace_back(stream);\n \n         // Limit read batch to maximum block size to allow DDL\ndiff --git a/dbms/src/Storages/Kafka/StorageKafka.h b/dbms/src/Storages/Kafka/StorageKafka.h\nindex 492d1d654111..224b5c0d7093 100644\n--- a/dbms/src/Storages/Kafka/StorageKafka.h\n+++ b/dbms/src/Storages/Kafka/StorageKafka.h\n@@ -82,6 +82,7 @@ class StorageKafka : public ext::shared_ptr_helper<StorageKafka>, public IStorag\n     String table_name;\n     String database_name;\n     Context global_context;\n+    Context kafka_context;\n     Names topics;\n     const String brokers;\n     const String group;\n",
  "test_patch": "diff --git a/dbms/tests/integration/test_storage_kafka/test.py b/dbms/tests/integration/test_storage_kafka/test.py\nindex 51325000f939..cf438bf3c559 100644\n--- a/dbms/tests/integration/test_storage_kafka/test.py\n+++ b/dbms/tests/integration/test_storage_kafka/test.py\n@@ -355,6 +355,43 @@ def test_kafka_materialized_view(kafka_cluster):\n     kafka_check_result(result, True)\n \n \n+@pytest.mark.timeout(180)\n+def test_kafka_materialized_view_with_subquery(kafka_cluster):\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.view;\n+        DROP TABLE IF EXISTS test.consumer;\n+        CREATE TABLE test.kafka (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'mvsq',\n+                     kafka_group_name = 'mvsq',\n+                     kafka_format = 'JSONEachRow',\n+                     kafka_row_delimiter = '\\\\n';\n+        CREATE TABLE test.view (key UInt64, value UInt64)\n+            ENGINE = MergeTree()\n+            ORDER BY key;\n+        CREATE MATERIALIZED VIEW test.consumer TO test.view AS\n+            SELECT * FROM (SELECT * FROM test.kafka);\n+    ''')\n+\n+    messages = []\n+    for i in range(50):\n+        messages.append(json.dumps({'key': i, 'value': i}))\n+    kafka_produce('mvsq', messages)\n+\n+    while True:\n+        result = instance.query('SELECT * FROM test.view')\n+        if kafka_check_result(result):\n+            break\n+\n+    instance.query('''\n+        DROP TABLE test.consumer;\n+        DROP TABLE test.view;\n+    ''')\n+\n+    kafka_check_result(result, True)\n+\n+\n @pytest.mark.timeout(180)\n def test_kafka_many_materialized_views(kafka_cluster):\n     instance.query('''\n",
  "problem_statement": "MV from Kafka gives \"There is no query: while pushing to view\"\nHi,\r\n\r\nI have data coming in via Kafka that has an incremental version field for each unique id. I'm using argMaxState in a MV to populate a table I can use to find the last record (see below, I can select argMaxMerge from to_table). Everything works correct up to this point.\r\n\r\nI then want to add a MV that can maintain an aggregate based on another field (see sum_table below). Because I can't do sumState(argMaxMerge(value)) I've written the MV using a subquery (see sum_table_mv below) - however, if I do this, then I see the error \"There is no query: while pushing to view default.sum_table_mv: while write prefix to view default.to_table_mv\" as new data arrives. \r\n\r\nI have verified that the same structure works correctly when from_table is not using the Kafka engine. Also, it only seems to error when the subquery is used in the MV.\r\n\r\nIs this a bug? Alternatively, is there a better approach I can try?\r\n\r\n```\r\nCREATE TABLE from_table\r\n(\r\n\tid UInt32,\r\n\tgid UInt32,\r\n\tversion UInt32,\r\n\tvalue UInt32\r\n)\r\nENGINE = Kafka('localhost:9092', 'from_table', 'groupId', 'JSONEachRow');\r\n\r\nCREATE TABLE to_table\r\n(\r\n\tid UInt32,\r\n\tgid UInt32,\r\n\tvalue AggregateFunction(argMax, UInt32, UInt32)\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY tuple()\r\norder by id;\r\n\r\nCREATE MATERIALIZED VIEW to_table_mv TO to_table\r\nAS \r\nSELECT \tid, \r\n\t\tany(gid) as gid, \r\n\t\targMaxState(value,version) as value\r\nFROM from_table \r\nGROUP BY id;\r\n\r\nCREATE TABLE sum_table\r\n(\r\n\tgid UInt32,\r\n\tvalue AggregateFunction(sum, UInt32)\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY tuple()\r\nORDER BY gid;\r\n\r\nCREATE MATERIALIZED VIEW sum_table_mv TO sum_table \r\nAS\r\nSELECT \tgid, \r\n\t\tsumState(value) as value \r\nFROM \r\n(\r\n\tselect \tgid, \r\n\t\t\targMaxMerge(value) as value \r\n\tFROM to_table \r\n\tGROUP BY gid\r\n)\r\nGROUP BY gid\r\n\r\necho '{\"id\":1,\"gid\":1,\"version\":0,\"value\":5}' | ./bin/kafka-console-producer --topic from_table --broker-list localhost:9092\r\n```\r\nError below - I can supply a full stack trace if needed.\r\n\r\n`2019.12.12 16:52:00.052859 [ 16 ] {} <Error> void DB::StorageKafka::threadFunc(): Code: 393, e.displayText() = DB::Exception: There is no query: while pushing to view default.sum_table_mv: while write prefix to view default.to_table_mv, Stack trace:\r\n`\r\n\r\nRunning server version 19.17.4 revision 54428.\r\n\r\nThanks, S\n",
  "hints_text": "Subselects in MV above Kafka table don't work (yet). They will (soon). ",
  "created_at": "2019-12-13T01:29:19Z",
  "modified_files": [
    "dbms/src/Storages/Kafka/StorageKafka.cpp",
    "dbms/src/Storages/Kafka/StorageKafka.h"
  ],
  "modified_test_files": [
    "dbms/tests/integration/test_storage_kafka/test.py"
  ]
}