{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 82670,
  "instance_id": "ClickHouse__ClickHouse-82670",
  "issue_numbers": [
    "82637"
  ],
  "base_commit": "27f935f349cfd3f3289fe3da173299173592cd60",
  "patch": "diff --git a/src/Databases/DatabaseAtomic.cpp b/src/Databases/DatabaseAtomic.cpp\nindex d9a3cf0ed935..06d11443dcce 100644\n--- a/src/Databases/DatabaseAtomic.cpp\n+++ b/src/Databases/DatabaseAtomic.cpp\n@@ -120,7 +120,7 @@ void DatabaseAtomic::drop(ContextPtr)\n     auto db_disk = getDisk();\n     try\n     {\n-        if (db_disk->isSymlinkSupported())\n+        if (db_disk->isSymlinkSupported() && !db_disk->isReadOnly())\n         {\n             db_disk->removeFileIfExists(path_to_metadata_symlink);\n             db_disk->removeRecursive(path_to_table_symlinks);\n@@ -130,7 +130,8 @@ void DatabaseAtomic::drop(ContextPtr)\n     {\n         LOG_WARNING(log, getCurrentExceptionMessageAndPattern(/* with_stacktrace */ true));\n     }\n-    db_disk->removeRecursive(getMetadataPath());\n+    if (!db_disk->isReadOnly())\n+        db_disk->removeRecursive(getMetadataPath());\n }\n \n void DatabaseAtomic::attachTable(ContextPtr /* context_ */, const String & name, const StoragePtr & table, const String & relative_table_path)\ndiff --git a/src/Databases/DatabaseOrdinary.cpp b/src/Databases/DatabaseOrdinary.cpp\nindex a4a7db931e40..5d4de78b79ca 100644\n--- a/src/Databases/DatabaseOrdinary.cpp\n+++ b/src/Databases/DatabaseOrdinary.cpp\n@@ -277,7 +277,8 @@ void DatabaseOrdinary::loadTablesMetadata(ContextPtr local_context, ParsedTables\n                 if (create_query->uuid != UUIDHelpers::Nil)\n                 {\n                     /// A bit tricky way to distinguish ATTACH DATABASE and server startup (actually it's \"force_attach\" flag).\n-                    if (is_startup)\n+                    /// When attaching a database with a read-only disk, the UUIDs do not exist, we add them manually.\n+                    if (is_startup || (db_disk->isReadOnly() && !DatabaseCatalog::instance().hasUUIDMapping(create_query->uuid)))\n                     {\n                         /// Server is starting up. Lock UUID used by permanently detached table.\n                         DatabaseCatalog::instance().addUUIDMapping(create_query->uuid);\ndiff --git a/src/Interpreters/InterpreterDropQuery.cpp b/src/Interpreters/InterpreterDropQuery.cpp\nindex a7950b7106d1..026c19ded3fa 100644\n--- a/src/Interpreters/InterpreterDropQuery.cpp\n+++ b/src/Interpreters/InterpreterDropQuery.cpp\n@@ -447,6 +447,8 @@ BlockIO InterpreterDropQuery::executeToDatabaseImpl(const ASTDropQuery & query,\n         // For truncate operation on database, drop the tables\n         if (truncate)\n             query_for_table.kind = query.has_tables ? ASTDropQuery::Kind::Truncate : ASTDropQuery::Kind::Drop;\n+        if (database->getDisk()->isReadOnly())\n+            query_for_table.kind = ASTDropQuery::Detach;\n         query_for_table.if_exists = true;\n         query_for_table.if_empty = false;\n         query_for_table.setDatabase(database_name);\n",
  "test_patch": "diff --git a/tests/integration/test_database_disk_setting/configs/database_disk.xml b/tests/integration/test_database_disk_setting/configs/database_disk.xml\nindex 49a80c7b4900..1b19076ea2ce 100644\n--- a/tests/integration/test_database_disk_setting/configs/database_disk.xml\n+++ b/tests/integration/test_database_disk_setting/configs/database_disk.xml\n@@ -19,6 +19,14 @@\n                 <access_key_id>minio</access_key_id>\n                 <secret_access_key>ClickHouse_Minio_P@ssw0rd</secret_access_key>\n             </s3>\n+            <s3_plain_rewritable>\n+                <type>object_storage</type>\n+                <object_storage_type>s3</object_storage_type>\n+                <metadata_type>plain_rewritable</metadata_type>\n+                <endpoint>http://minio1:9001/root/data/disks/disk_s3_plain_rewritable/</endpoint>\n+                <access_key_id>minio</access_key_id>\n+                <secret_access_key>ClickHouse_Minio_P@ssw0rd</secret_access_key>\n+            </s3_plain_rewritable>\n         </disks>\n     </storage_configuration>\n </clickhouse>\ndiff --git a/tests/integration/test_database_disk_setting/configs/disk_app_config.xml b/tests/integration/test_database_disk_setting/configs/disk_app_config.xml\nindex b1cefc64733d..e1ee90251da8 100644\n--- a/tests/integration/test_database_disk_setting/configs/disk_app_config.xml\n+++ b/tests/integration/test_database_disk_setting/configs/disk_app_config.xml\n@@ -20,6 +20,14 @@\n                 <secret_access_key>ClickHouse_Minio_P@ssw0rd</secret_access_key>\n                 <readonly>true</readonly>\n             </s3>\n+            <s3_plain_rewritable>\n+                <type>object_storage</type>\n+                <object_storage_type>s3</object_storage_type>\n+                <metadata_type>plain_rewritable</metadata_type>\n+                <endpoint>http://minio1:9001/root/data/disks/disk_s3_plain_rewritable/</endpoint>\n+                <access_key_id>minio</access_key_id>\n+                <secret_access_key>ClickHouse_Minio_P@ssw0rd</secret_access_key>\n+            </s3_plain_rewritable>\n         </disks>\n     </storage_configuration>\n </clickhouse>\ndiff --git a/tests/integration/test_database_disk_setting/test.py b/tests/integration/test_database_disk_setting/test.py\nindex be7262d3d5c2..4b9b35d2b762 100644\n--- a/tests/integration/test_database_disk_setting/test.py\n+++ b/tests/integration/test_database_disk_setting/test.py\n@@ -19,6 +19,16 @@\n     stay_alive=True,\n )\n \n+node2 = cluster.add_instance(\n+    \"node2\",\n+    main_configs=[\n+        \"configs/database_disk.xml\",\n+    ],\n+    with_remote_database_disk=False,\n+    with_minio=True,\n+    stay_alive=True,\n+)\n+\n disk_config_file_path =  \"/tmp/disk_app_config.xml\"\n \n @pytest.fixture(scope=\"module\")\n@@ -29,6 +39,10 @@ def start_cluster():\n             os.path.join(SCRIPT_DIR, \"configs/disk_app_config.xml\"),\n             disk_config_file_path,\n         )\n+        node2.copy_file_to_container(\n+            os.path.join(SCRIPT_DIR, \"configs/disk_app_config.xml\"),\n+            disk_config_file_path,\n+        )\n         yield cluster\n     finally:\n         cluster.shutdown()\n@@ -54,7 +68,7 @@ def validate_table_metadata_path(node, disk_name : str, db : str, table: str):\n     assert directory_exists(node, disk_name, table_metadata_path)\n     if(disk_name != \"global_db_disk\"):\n         assert not directory_exists(node1, \"global_db_disk\", table_metadata_path)\n-        \n+\n def validate_db_path(node, disk_name : str, db : str, support_symlink: bool = True):\n     db_data_path = node.query(\n         f\"SELECT metadata_path FROM system.databases WHERE database='{db}'\"\n@@ -64,15 +78,15 @@ def validate_db_path(node, disk_name : str, db : str, support_symlink: bool = Tr\n     if support_symlink:\n         assert directory_exists(node, disk_name, os.path.join(\"metadata\", db))\n         assert directory_exists(node, disk_name, os.path.join( \"data\", db))\n-        \n+\n @pytest.mark.parametrize(\"engine\", [\"Atomic\", \"Ordinary\"])\n @pytest.mark.parametrize(\"db_disk_name\", [\"db_disk\", \"global_db_disk\", \"\"])\n def test_db_disk_setting(start_cluster, engine: str, db_disk_name: str):\n     db_name = f\"db_{db_disk_name}_{engine.lower()}\"\n-    \n+\n     node1.query(f\"DROP DATABASE IF EXISTS {db_name} SYNC\")\n     node1.query(f\"DROP DATABASE IF EXISTS {db_name}_rename SYNC\")\n-    \n+\n     disk_setting = f\"disk='{db_disk_name}'\"\n     if len(db_disk_name) == 0:\n         disk_setting = \"disk=disk(type='local', path='/var/lib/clickhouse/disks/custom_db_disk/')\"\n@@ -86,21 +100,20 @@ def test_db_disk_setting(start_cluster, engine: str, db_disk_name: str):\n \n     validate_db_path(node1, db_disk_name, db_name)\n     validate_table_metadata_path(node1, db_disk_name, db_name, 'test')\n-    \n+\n     # Ordinay DB doesn't support renaming\n     if(engine != \"Ordinary\"):\n         node1.query(f\"RENAME DATABASE {db_name} TO {db_name}_rename\")\n         validate_db_path(node1, db_disk_name, f\"{db_name}_rename\")\n         validate_table_metadata_path(node1, db_disk_name, f\"{db_name}_rename\", 'test')\n-        \n+\n         node1.query(f\"RENAME DATABASE {db_name}_rename TO {db_name}\")\n         validate_db_path(node1, db_disk_name, db_name)\n         validate_table_metadata_path(node1, db_disk_name, db_name, 'test')\n-    \n+\n     node1.query(f\"RENAME TABLE {db_name}.test TO {db_name}.test_rename\")\n     validate_db_path(node1, db_disk_name, db_name)\n     validate_table_metadata_path(node1, db_disk_name, db_name, 'test_rename')\n-    \n \n     node1.query(f\"DROP DATABASE IF EXISTS {db_name} SYNC\")\n     node1.query(f\"DROP DATABASE IF EXISTS {db_name}_rename SYNC\")\n@@ -121,7 +134,7 @@ def read_file(node, disk_name: str, metadata_path: str):\n     return node.exec_in_container(\n         [\"bash\", \"-c\", f\"{disk_cmd_prefix} 'read --path-from {metadata_path}'\"]\n     )\n-    \n+\n def write_to_file(node, disk_name: str, file_path: str, content: str):\n     # Escape backticks to avoid command substitution\n     escaped_content = content.replace('\"', r\"\\\"\").replace(\"`\", r\"\\`\")\n@@ -133,7 +146,7 @@ def write_to_file(node, disk_name: str, file_path: str, content: str):\n             f\"\"\"printf \"%s\" \"{escaped_content}\" | {disk_cmd_prefix} 'w --path-to {file_path}'\"\"\",\n         ]\n     )\n-        \n+\n def remove_file(node, disk_name: str, file_path: str):\n     # Escape backticks to avoid command substitution\n     disk_cmd_prefix = f\"/usr/bin/clickhouse disks -C {disk_config_file_path} --save-logs --disk {disk_name} --query \"\n@@ -141,41 +154,49 @@ def remove_file(node, disk_name: str, file_path: str):\n         [\"bash\", \"-c\", f\"{disk_cmd_prefix} 'remove {file_path}'\"]\n     )\n \n-@pytest.mark.skip(reason=\"'s3' disk doesn't support moveFile, so the DB with 's3' disk cannot be dropped.\")\n-def test_db_disk_setting_with_s3(start_cluster):\n+\n+def test_attach_db_from_readonly_remote_disk(start_cluster):\n     db_name = f\"db_test\"\n-    \n+\n     node1.query(f\"DROP DATABASE IF EXISTS {db_name} SYNC\")\n-    \n-    node1.query(f\"CREATE DATABASE {db_name} ENGINE= Atomic SETTINGS disk='db_disk'\")\n-    node1.query(f\"CREATE TABLE {db_name}.test (x INT) ENGINE=MergeTree ORDER BY x\")\n-    \n-    table_metadata_path = node1.query(\n-        f\"SELECT metadata_path FROM system.tables WHERE database='{db_name}' AND table='test'\"\n+\n+    node1.query(\n+        f\"CREATE DATABASE {db_name} ENGINE=Atomic SETTINGS disk='s3_plain_rewritable'\"\n+    )\n+    db_uuid = node1.query(\n+        f\"SELECT uuid FROM system.databases WHERE database='{db_name}'\"\n     ).strip()\n-    \n-     \n-    print(f\"table_metadata_path: {table_metadata_path}\")\n-    \n-    node1.stop_clickhouse()\n-    \n-    # Update disk of the DB to 's3'\n-    replace_text_in_metadata(node1, \"global_db_disk\", f\"metadata/{db_name}.sql\", \"db_disk\", \"s3\")\n-    \n-    # Copy the table metadata file into 's3' disk\n-    table_metadata_content = read_file(node1, \"db_disk\", table_metadata_path)\n-    print(f\"table_metadata_content: {table_metadata_content}\")\n-    write_to_file(node1, 's3', table_metadata_path, table_metadata_content)\n-    \n-    # Remove metadata file on the DB disk\n-    remove_file(node1, \"db_disk\", table_metadata_path)\n-    \n-    node1.start_clickhouse()\n-    \n-    validate_db_path(node1, \"s3\", db_name, False)\n-    assert directory_exists(node1, \"s3\", table_metadata_path)\n-    \n-    assert node1.query(\"SELECT count() FROM system.tables WHERE table='test'\").strip() == \"1\"\n-    \n+    table_disk_setting = 'disk(type = \"s3_plain_rewritable\", endpoint = \"http://minio1:9001/root/data/disks/disk_s3_plain_rewritable/\", access_key_id=\"minio\", secret_access_key = \"ClickHouse_Minio_P@ssw0rd\")'\n+    node1.query(\n+        f\"CREATE TABLE {db_name}.test (x INT, y INT) ENGINE=MergeTree ORDER BY x SETTINGS disk={table_disk_setting}\"\n+    )\n+    node1.query(f\"INSERT INTO {db_name}.test VALUES (1, 1)\")\n+\n+    assert node1.query(f\"SELECT * FROM {db_name}.test\") == \"1\\t1\\n\"\n+    assert (\n+        node1.query(\"SELECT count() FROM system.tables WHERE table='test'\").strip()\n+        == \"1\"\n+    )\n+\n+    node2.restart_clickhouse()\n+    readonly_disk_setting = 'disk(readonly = 1, type = \"s3_plain_rewritable\", endpoint = \"http://minio1:9001/root/data/disks/disk_s3_plain_rewritable/\", access_key_id=\"minio\", secret_access_key = \"ClickHouse_Minio_P@ssw0rd\")'\n+    node2.query(\n+        f\"ATTACH DATABASE {db_name} UUID '{db_uuid}' ENGINE=Atomic SETTINGS disk={readonly_disk_setting}\"\n+    )\n+    assert (\n+        node2.query(\"SELECT count() FROM system.tables WHERE table='test'\").strip()\n+        == \"1\"\n+    )\n+    assert node2.query(f\"SELECT * FROM {db_name}.test\") == \"1\\t1\\n\"\n+\n+    node2.query(f\"DETACH DATABASE {db_name}\")\n+    assert node1.query(f\"SELECT * FROM {db_name}.test\") == \"1\\t1\\n\"\n+\n+    node2.query(f\"ATTACH DATABASE {db_name}\")\n+    assert node1.query(f\"SELECT * FROM {db_name}.test\") == \"1\\t1\\n\"\n+    assert node2.query(f\"SELECT * FROM {db_name}.test\") == \"1\\t1\\n\"\n+\n+    node2.query(f\"DROP DATABASE {db_name} SYNC\")\n+    assert node1.query(f\"SELECT * FROM {db_name}.test\") == \"1\\t1\\n\"\n+\n     node1.query(f\"DROP DATABASE IF EXISTS {db_name} SYNC\")\n-    \n\\ No newline at end of file\n",
  "problem_statement": "Attaching a database leads to a logical error\n### Company or project name\n\nClickHouse\n\n### Describe the unexpected behaviour\n\n```\n:) ATTACH DATABASE clicklake_test UUID 'c2169e93-c365-48bd-8a35-71235c2207b5' ENGINE = Atomic SETTINGS disk = disk(readonly = 1, type = 's3_plain_rewritable', endpoint = 'https://clicklake-test-2.s3.eu-central-1.amazonaws.com/');\n\nATTACH DATABASE clicklake_test UUID 'c2169e93-c365-48bd-8a35-71235c2207b5'\nENGINE = Atomic\nSETTINGS disk = disk(readonly = 1, type = 's3_plain_rewritable', endpoint = 'https://clicklake-test-2.s3.eu-central-1.amazonaws.com/')\n\nQuery id: 799ca6f6-488c-4fb3-a1e7-a433b3adb94e\n\n\nElapsed: 0.083 sec. \n\nReceived exception:\nCode: 49. DB::Exception: Cannot find UUID mapping for 66491946-56e3-4790-a112-d2dc3963e68a, it's a bug: Cannot parse definition from metadata file store/c21/c2169e93-c365-48bd-8a35-71235c2207b5/hackernews_history.sql. (LOGICAL_ERROR)\n```\n\n### Which ClickHouse versions are affected?\n\n.\n\n### How to reproduce\n\n.\n\n### Expected behavior\n\n_No response_\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\n_No response_\n",
  "hints_text": "",
  "created_at": "2025-06-26T15:20:11Z",
  "modified_files": [
    "src/Databases/DatabaseAtomic.cpp",
    "src/Databases/DatabaseOrdinary.cpp",
    "src/Interpreters/InterpreterDropQuery.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_database_disk_setting/configs/database_disk.xml",
    "tests/integration/test_database_disk_setting/configs/disk_app_config.xml",
    "tests/integration/test_database_disk_setting/test.py"
  ]
}