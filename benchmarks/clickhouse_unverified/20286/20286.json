{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 20286,
  "instance_id": "ClickHouse__ClickHouse-20286",
  "issue_numbers": [
    "19719"
  ],
  "base_commit": "420f2958e213539d2a7e7069d7eee6c207db5943",
  "patch": "diff --git a/src/Formats/JSONEachRowUtils.cpp b/src/Formats/JSONEachRowUtils.cpp\nindex 6017f3983c6f..56bef9e09eae 100644\n--- a/src/Formats/JSONEachRowUtils.cpp\n+++ b/src/Formats/JSONEachRowUtils.cpp\n@@ -3,6 +3,10 @@\n \n namespace DB\n {\n+namespace ErrorCodes\n+{\n+    extern const int INCORRECT_DATA;\n+}\n \n std::pair<bool, size_t> fileSegmentationEngineJSONEachRowImpl(ReadBuffer & in, DB::Memory<> & memory, size_t min_chunk_size)\n {\n@@ -15,6 +19,12 @@ std::pair<bool, size_t> fileSegmentationEngineJSONEachRowImpl(ReadBuffer & in, D\n \n     while (loadAtPosition(in, memory, pos) && (balance || memory.size() + static_cast<size_t>(pos - in.position()) < min_chunk_size))\n     {\n+        const auto current_object_size = memory.size() + static_cast<size_t>(pos - in.position());\n+        if (current_object_size > 10 * min_chunk_size)\n+            throw ParsingException(\"Size of JSON object is extremely large. Expected not greater than \" +\n+            std::to_string(min_chunk_size) + \" bytes, but current is \" + std::to_string(current_object_size) +\n+            \" bytes per row. Increase the value setting 'min_chunk_bytes_for_parallel_parsing' or check your data manually, most likely JSON is malformed\", ErrorCodes::INCORRECT_DATA);\n+\n         if (quotes)\n         {\n             pos = find_first_symbols<'\\\\', '\"'>(pos, in.buffer().end());\ndiff --git a/src/IO/ReadHelpers.cpp b/src/IO/ReadHelpers.cpp\nindex 5a159defe06c..baa12297718a 100644\n--- a/src/IO/ReadHelpers.cpp\n+++ b/src/IO/ReadHelpers.cpp\n@@ -1104,9 +1104,9 @@ void saveUpToPosition(ReadBuffer & in, DB::Memory<> & memory, char * current)\n     assert(current >= in.position());\n     assert(current <= in.buffer().end());\n \n-    const int old_bytes = memory.size();\n-    const int additional_bytes = current - in.position();\n-    const int new_bytes = old_bytes + additional_bytes;\n+    const size_t old_bytes = memory.size();\n+    const size_t additional_bytes = current - in.position();\n+    const size_t new_bytes = old_bytes + additional_bytes;\n     /// There are no new bytes to add to memory.\n     /// No need to do extra stuff.\n     if (new_bytes == 0)\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01701_parallel_parsing_infinite_segmentation.reference b/tests/queries/0_stateless/01701_parallel_parsing_infinite_segmentation.reference\nnew file mode 100644\nindex 000000000000..587579af915b\n--- /dev/null\n+++ b/tests/queries/0_stateless/01701_parallel_parsing_infinite_segmentation.reference\n@@ -0,0 +1,1 @@\n+Ok.\ndiff --git a/tests/queries/0_stateless/01701_parallel_parsing_infinite_segmentation.sh b/tests/queries/0_stateless/01701_parallel_parsing_infinite_segmentation.sh\nnew file mode 100755\nindex 000000000000..d3e634eb5607\n--- /dev/null\n+++ b/tests/queries/0_stateless/01701_parallel_parsing_infinite_segmentation.sh\n@@ -0,0 +1,9 @@\n+#!/usr/bin/env bash                                                                                                                                                                                                                                           \n+                                                                                                                                                                                                                                                              \n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)                                                                                                                                                                                                          \n+# shellcheck source=../shell_config.sh                                                                                                                                                                                                                        \n+. \"$CURDIR\"/../shell_config.sh   \n+\n+${CLICKHOUSE_CLIENT} -q \"create table insert_big_json(a String, b String) engine=MergeTree() order by tuple()\";\n+\n+python3 -c \"[print('{{\\\"a\\\":\\\"{}\\\", \\\"b\\\":\\\"{}\\\"'.format('clickhouse'* 1000000, 'dbms' * 1000000)) for i in range(10)]; [print('{{\\\"a\\\":\\\"{}\\\", \\\"b\\\":\\\"{}\\\"}}'.format('clickhouse'* 100000, 'dbms' * 100000)) for i in range(10)]\" 2>/dev/null  | ${CLICKHOUSE_CLIENT} --input_format_parallel_parsing=1 --max_memory_usage=0 -q \"insert into insert_big_json FORMAT JSONEachRow\" 2>&1 | grep -q \"min_chunk_bytes_for_parallel_parsing\" && echo \"Ok.\" || echo \"FAIL\" ||:\n\\ No newline at end of file\n",
  "problem_statement": "Error on JSON import\n**Describe the bug**\r\nWhen importing a jsonlines file (gzip-compressed around 2GB), I always get an error message indicating some issues regarding memory allocation. The machine has 512GB RAM and most of it is not used, therefore, it could be related with the configuration or just simply a software bug. I am using the default config, but did not change it since a year (has something changed in the default values that could be relevant here?). Either way, I am not sure how to fix it. I am using the most recent version (Client & Server 21.1.2.15) and I can reproduce the error. However, due to copyright issues, I cannot share the original dataset. \r\n\r\nI import the file as follows\r\n```bash\r\nzcat dataset.jsonl.gz|clickhouse-client --input_format_skip_unknown_fields=1 --input_format_allow_errors_num=1000 -q \"INSERT INTO mydataset.mytable FORMAT JSONEachRow\"\r\n```\r\n\r\n**Error message and/or stacktrace**\r\nAnd after a while, I always receive the following error message:\r\n\r\n```\r\nCode: 49, e.displayText() = DB::Exception: Too large size (18446744071562077831) passed to allocator. It indicates an error., Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception<unsigned long&>(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long&) @ 0x864dfc7 in /usr/bin/clickhouse\r\n1. Allocator<false, false>::checkSize(unsigned long) @ 0x864dd7e in /usr/bin/clickhouse\r\n2. Allocator<false, false>::realloc(void*, unsigned long, unsigned long, unsigned long) @ 0x865c784 in /usr/bin/clickhouse\r\n3. DB::loadAtPosition(DB::ReadBuffer&, DB::Memory<Allocator<false, false> >&, char*&) @ 0x865bbfa in /usr/bin/clickhouse\r\n4. DB::fileSegmentationEngineJSONEachRowImpl(DB::ReadBuffer&, DB::Memory<Allocator<false, false> >&, unsigned long) @ 0xf98a612 in /usr/bin/clickhouse\r\n5. DB::ParallelParsingInputFormat::segmentatorThreadFunction(std::__1::shared_ptr<DB::ThreadGroupStatus>) @ 0xf9aed84 in /usr/bin/clickhouse\r\n6. ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelParsingInputFormat::*)(std::__1::shared_ptr<DB::ThreadGroupStatus>), DB::ParallelParsingInputFormat*, std::__1::shared_ptr<DB::ThreadGroupStatus> >(void (DB::ParallelParsingInputFormat::*&&)(std::__1::shared_ptr<DB::ThreadGroupStatus>), DB::ParallelParsingInputFormat*&&, std::__1::shared_ptr<DB::ThreadGroupStatus>&&)::'lambda'()::operator()() @ 0xf8a9677 in /usr/bin/clickhouse\r\n7. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x86415ed in /usr/bin/clickhouse\r\n8. ? @ 0x86451a3 in /usr/bin/clickhouse\r\n9. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n10. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 21.1.2.15 (official build))\r\nCode: 49, e.displayText() = DB::Exception: Too large size (18446744071562077831) passed to allocator. It indicates an error., Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception<unsigned long&>(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long&) @ 0x864dfc7 in /usr/bin/clickhouse\r\n1. Allocator<false, false>::checkSize(unsigned long) @ 0x864dd7e in /usr/bin/clickhouse\r\n2. Allocator<false, false>::realloc(void*, unsigned long, unsigned long, unsigned long) @ 0x865c784 in /usr/bin/clickhouse\r\n3. DB::loadAtPosition(DB::ReadBuffer&, DB::Memory<Allocator<false, false> >&, char*&) @ 0x865bbfa in /usr/bin/clickhouse\r\n4. DB::fileSegmentationEngineJSONEachRowImpl(DB::ReadBuffer&, DB::Memory<Allocator<false, false> >&, unsigned long) @ 0xf98a612 in /usr/bin/clickhouse\r\n5. DB::ParallelParsingInputFormat::segmentatorThreadFunction(std::__1::shared_ptr<DB::ThreadGroupStatus>) @ 0xf9aed84 in /usr/bin/clickhouse\r\n6. ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelParsingInputFormat::*)(std::__1::shared_ptr<DB::ThreadGroupStatus>), DB::ParallelParsingInputFormat*, std::__1::shared_ptr<DB::ThreadGroupStatus> >(void (DB::ParallelParsingInputFormat::*&&)(std::__1::shared_ptr<DB::ThreadGroupStatus>), DB::ParallelParsingInputFormat*&&, std::__1::shared_ptr<DB::ThreadGroupStatus>&&)::'lambda'()::operator()() @ 0xf8a9677 in /usr/bin/clickhouse\r\n7. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x86415ed in /usr/bin/clickhouse\r\n8. ? @ 0x86451a3 in /usr/bin/clickhouse\r\n9. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n10. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 21.1.2.15 (official build))\r\nCode: 49. DB::Exception: Too large size (18446744071562077831) passed to allocator. It indicates an error.: data for INSERT was parsed from stdin\r\n```\r\n\n",
  "hints_text": "Does it help to set `--input_format_parallel_parsing 0`?\nIndeed, the flag solved the problem!\n@inkrement Hello, maybe you can estimate the number of characters in a single JSON in your dataset? Maybe you have too large strings or simply many fields. I see a potential bug in code, but It could appear only on extremely big JSON's \nHi! the documents should be rather small (max 30 fields & max. 1000 chars each). However, I already recognized some broken documents (some other processes inferred at the output). If there is something broken, does the CH-parser detect it directly at the end of the current JSON document or could it be the case that it keeps reading? As the single-threaded version works fine, I guess this has to do with distribution of lines/documents.\n@inkrement  Thanks! The parallelized version of parsers consists of several pieces and one of the is `Segmentator` which reads whole file and count the 'balance' of curly braces. If it is 0, then we have read a complete JSON object and can parse it. I think in your case with broken document the balance of braces was non zero for a long period of document, that's why the `Segmentator` kept reading the whole document into memory. \r\n\r\n> If there is something broken, does the CH-parser detect it directly at the end of the current JSON document or could it be the case that it keeps reading?\r\n\r\nIt detects the line in document where the error occurred, but your case is unusual. \r\n\r\n\nJust to be sure: Are we talking about JSON or JSONEachRow?",
  "created_at": "2021-02-10T14:21:29Z",
  "modified_files": [
    "src/Formats/JSONEachRowUtils.cpp",
    "src/IO/ReadHelpers.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/01701_parallel_parsing_infinite_segmentation.reference",
    "b/tests/queries/0_stateless/01701_parallel_parsing_infinite_segmentation.sh"
  ]
}