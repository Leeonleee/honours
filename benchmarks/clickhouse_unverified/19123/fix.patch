diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp
index ef64ec28e794..982345f8240f 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp
@@ -26,7 +26,7 @@ Granules getGranulesToWrite(const MergeTreeIndexGranularity & index_granularity,
 
     Granules result;
     size_t current_row = 0;
-    /// When our last mark is not finished yet and we have to write in rows into it
+    /// When our last mark is not finished yet and we have to write rows into it
     if (rows_written_in_last_mark > 0)
     {
         size_t rows_left_in_last_mark = index_granularity.getMarkRows(current_mark) - rows_written_in_last_mark;
@@ -183,8 +183,8 @@ void MergeTreeDataPartWriterWide::write(const Block & block, const IColumn::Perm
                 /// adjust last mark rows and flush to disk.
                 if (rows_written_in_last_mark >= index_granularity_for_block)
                     adjustLastMarkIfNeedAndFlushToDisk(rows_written_in_last_mark);
-                else /// We still can write some rows from new block into previous granule.
-                    adjustLastMarkIfNeedAndFlushToDisk(index_granularity_for_block - rows_written_in_last_mark);
+                else /// We still can write some rows from new block into previous granule. So the granule size will be block granularity size.
+                    adjustLastMarkIfNeedAndFlushToDisk(index_granularity_for_block);
             }
         }
 
@@ -440,6 +440,13 @@ void MergeTreeDataPartWriterWide::validateColumnOfFixedSize(const String & name,
                         "Still have {} rows in bin stream, last mark #{} index granularity size {}, last rows {}", column->size(), mark_num, index_granularity.getMarksCount(), index_granularity_rows);
         }
 
+        if (index_granularity_rows > data_part->index_granularity_info.fixed_index_granularity)
+        {
+            throw Exception(ErrorCodes::LOGICAL_ERROR,
+                            "Mark #{} has {} rows, but max fixed granularity is {}, index granularity size {}",
+                            mark_num, index_granularity_rows, data_part->index_granularity_info.fixed_index_granularity, index_granularity.getMarksCount());
+        }
+
         if (index_granularity_rows != index_granularity.getMarkRows(mark_num))
             throw Exception(
                 ErrorCodes::LOGICAL_ERROR, "Incorrect mark rows for part {} for mark #{} (compressed offset {}, decompressed offset {}), in-memory {}, on disk {}, total marks {}",
@@ -607,6 +614,11 @@ void MergeTreeDataPartWriterWide::fillIndexGranularity(size_t index_granularity_
 
 void MergeTreeDataPartWriterWide::adjustLastMarkIfNeedAndFlushToDisk(size_t new_rows_in_last_mark)
 {
+    /// We don't want to split already written granules to smaller
+    if (rows_written_in_last_mark > new_rows_in_last_mark)
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "Tryin to make mark #{} smaller ({} rows) then it already has {}",
+                        getCurrentMark(), new_rows_in_last_mark, rows_written_in_last_mark);
+
     /// We can adjust marks only if we computed granularity for blocks.
     /// Otherwise we cannot change granularity because it will differ from
     /// other columns
