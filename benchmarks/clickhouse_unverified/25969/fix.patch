diff --git a/CMakeLists.txt b/CMakeLists.txt
index 7808edeff9bb..1790ddc04515 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -581,6 +581,7 @@ include (cmake/find/yaml-cpp.cmake)
 include (cmake/find/s2geometry.cmake)
 include (cmake/find/nlp.cmake)
 include (cmake/find/bzip2.cmake)
+include (cmake/find/filelog.cmake)
 
 if(NOT USE_INTERNAL_PARQUET_LIBRARY)
     set (ENABLE_ORC OFF CACHE INTERNAL "")
diff --git a/cmake/find/filelog.cmake b/cmake/find/filelog.cmake
new file mode 100644
index 000000000000..4d2f94f3f20a
--- /dev/null
+++ b/cmake/find/filelog.cmake
@@ -0,0 +1,15 @@
+option (ENABLE_FILELOG "Enable FILELOG" ON)
+
+if (NOT ENABLE_FILELOG)
+	message (${RECONFIGURE_MESSAGE_LEVEL} "Can't use StorageFileLog with ENABLE_FILELOG=OFF")
+    return()
+endif()
+
+# StorageFileLog only support Linux platform
+if (OS_LINUX)
+    set (USE_FILELOG 1)
+    message (STATUS "Using StorageFileLog = 1")
+else()
+    message(STATUS "StorageFileLog is only supported on Linux")
+endif ()
+
diff --git a/contrib/poco b/contrib/poco
index 46c80daf1b01..39fd359765a3 160000
--- a/contrib/poco
+++ b/contrib/poco
@@ -1,1 +1,1 @@
-Subproject commit 46c80daf1b015aa10474ce82e3d24b578c6ae422
+Subproject commit 39fd359765a3a77b46d94ec3c5def3c7802a920f
diff --git a/src/CMakeLists.txt b/src/CMakeLists.txt
index 09aaa85c394b..24014db98cd3 100644
--- a/src/CMakeLists.txt
+++ b/src/CMakeLists.txt
@@ -109,6 +109,10 @@ if (USE_HDFS)
     add_headers_and_sources(dbms Disks/HDFS)
 endif()
 
+if(USE_FILELOG)
+    add_headers_and_sources(dbms Storages/FileLog)
+endif()
+
 list (APPEND clickhouse_common_io_sources ${CONFIG_BUILD})
 list (APPEND clickhouse_common_io_headers ${CONFIG_VERSION} ${CONFIG_COMMON})
 
diff --git a/src/Common/ErrorCodes.cpp b/src/Common/ErrorCodes.cpp
index 1aff14601253..2aadda426e75 100644
--- a/src/Common/ErrorCodes.cpp
+++ b/src/Common/ErrorCodes.cpp
@@ -591,6 +591,8 @@
     M(621, CANNOT_NORMALIZE_STRING) \
     M(622, CANNOT_PARSE_CAPN_PROTO_SCHEMA) \
     M(623, CAPN_PROTO_BAD_CAST) \
+    M(624, BAD_FILE_TYPE) \
+    M(625, IO_SETUP_ERROR) \
     \
     M(999, KEEPER_EXCEPTION) \
     M(1000, POCO_EXCEPTION) \
diff --git a/src/Core/config_core.h.in b/src/Core/config_core.h.in
index cc9c993b2055..11dd9bf96f13 100644
--- a/src/Core/config_core.h.in
+++ b/src/Core/config_core.h.in
@@ -17,3 +17,4 @@
 #cmakedefine01 USE_NURAFT
 #cmakedefine01 USE_NLP
 #cmakedefine01 USE_KRB5
+#cmakedefine01 USE_FILELOG
diff --git a/src/Databases/DatabaseAtomic.cpp b/src/Databases/DatabaseAtomic.cpp
index 5c75f6f1036b..ae90f1a69008 100644
--- a/src/Databases/DatabaseAtomic.cpp
+++ b/src/Databases/DatabaseAtomic.cpp
@@ -140,6 +140,9 @@ void DatabaseAtomic::dropTable(ContextPtr local_context, const String & table_na
     if (table->storesDataOnDisk())
         tryRemoveSymlink(table_name);
 
+    if (table->dropTableImmediately())
+        table->drop();
+
     /// Notify DatabaseCatalog that table was dropped. It will remove table data in background.
     /// Cleanup is performed outside of database to allow easily DROP DATABASE without waiting for cleanup to complete.
     DatabaseCatalog::instance().enqueueDroppedTableCleanup(table->getStorageID(), table, table_metadata_path_drop, no_delay);
diff --git a/src/Storages/FileLog/Buffer_fwd.h b/src/Storages/FileLog/Buffer_fwd.h
new file mode 100644
index 000000000000..ec644aa7d369
--- /dev/null
+++ b/src/Storages/FileLog/Buffer_fwd.h
@@ -0,0 +1,10 @@
+#pragma once
+
+#include <memory>
+
+namespace DB
+{
+class ReadBufferFromFileLog;
+
+using ReadBufferFromFileLogPtr = std::shared_ptr<ReadBufferFromFileLog>;
+}
diff --git a/src/Storages/FileLog/DirectoryWatcherBase.cpp b/src/Storages/FileLog/DirectoryWatcherBase.cpp
new file mode 100644
index 000000000000..c459079ec06d
--- /dev/null
+++ b/src/Storages/FileLog/DirectoryWatcherBase.cpp
@@ -0,0 +1,148 @@
+#include <Interpreters/Context.h>
+#include <Storages/FileLog/DirectoryWatcherBase.h>
+#include <Storages/FileLog/FileLogDirectoryWatcher.h>
+#include <Storages/FileLog/StorageFileLog.h>
+#include <base/sleep.h>
+
+#include <filesystem>
+#include <unistd.h>
+#include <sys/inotify.h>
+#include <sys/poll.h>
+
+namespace DB
+{
+namespace ErrorCodes
+{
+    extern const int FILE_DOESNT_EXIST;
+    extern const int BAD_FILE_TYPE;
+    extern const int IO_SETUP_ERROR;
+}
+
+static constexpr int buffer_size = 4096;
+
+DirectoryWatcherBase::DirectoryWatcherBase(
+    FileLogDirectoryWatcher & owner_, const std::string & path_, ContextPtr context_, int event_mask_)
+    : WithContext(context_)
+    , owner(owner_)
+    , path(path_)
+    , event_mask(event_mask_)
+    , milliseconds_to_wait(owner.storage.getFileLogSettings()->poll_directory_watch_events_backoff_init.totalMilliseconds())
+{
+    if (!std::filesystem::exists(path))
+        throw Exception(ErrorCodes::FILE_DOESNT_EXIST, "Path {} does not exist", path);
+
+    if (!std::filesystem::is_directory(path))
+        throw Exception(ErrorCodes::BAD_FILE_TYPE, "Path {} is not a directory", path);
+
+    fd = inotify_init();
+    if (fd == -1)
+        throw Exception("Cannot initialize inotify", ErrorCodes::IO_SETUP_ERROR);
+
+    watch_task = getContext()->getSchedulePool().createTask("directory_watch", [this] { watchFunc(); });
+    start();
+}
+
+void DirectoryWatcherBase::watchFunc()
+{
+    int mask = 0;
+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_ADDED)
+        mask |= IN_CREATE;
+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_REMOVED)
+        mask |= IN_DELETE;
+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_MODIFIED)
+        mask |= IN_MODIFY;
+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_MOVED_FROM)
+        mask |= IN_MOVED_FROM;
+    if (eventMask() & DirectoryWatcherBase::DW_ITEM_MOVED_TO)
+        mask |= IN_MOVED_TO;
+
+    int wd = inotify_add_watch(fd, path.c_str(), mask);
+    if (wd == -1)
+    {
+        owner.onError(Exception(ErrorCodes::IO_SETUP_ERROR, "Watch directory {} failed", path));
+    }
+
+    std::string buffer;
+    buffer.resize(buffer_size);
+    pollfd pfd;
+    pfd.fd = fd;
+    pfd.events = POLLIN;
+    while (!stopped)
+    {
+        const auto & settings = owner.storage.getFileLogSettings();
+        if (poll(&pfd, 1, milliseconds_to_wait) > 0 && pfd.revents & POLLIN)
+        {
+            milliseconds_to_wait = settings->poll_directory_watch_events_backoff_init.totalMilliseconds();
+            int n = read(fd, buffer.data(), buffer.size());
+            int i = 0;
+            if (n > 0)
+            {
+                while (n > 0)
+                {
+                    struct inotify_event * p_event = reinterpret_cast<struct inotify_event *>(buffer.data() + i);
+
+                    if (p_event->len > 0)
+                    {
+                        if ((p_event->mask & IN_CREATE) && (eventMask() & DirectoryWatcherBase::DW_ITEM_ADDED))
+                        {
+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_ADDED);
+                            owner.onItemAdded(ev);
+                        }
+                        if ((p_event->mask & IN_DELETE) && (eventMask() & DirectoryWatcherBase::DW_ITEM_REMOVED))
+                        {
+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_REMOVED);
+                            owner.onItemRemoved(ev);
+                        }
+                        if ((p_event->mask & IN_MODIFY) && (eventMask() & DirectoryWatcherBase::DW_ITEM_MODIFIED))
+                        {
+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_MODIFIED);
+                            owner.onItemModified(ev);
+                        }
+                        if ((p_event->mask & IN_MOVED_FROM) && (eventMask() & DirectoryWatcherBase::DW_ITEM_MOVED_FROM))
+                        {
+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_MOVED_FROM);
+                            owner.onItemMovedFrom(ev);
+                        }
+                        if ((p_event->mask & IN_MOVED_TO) && (eventMask() & DirectoryWatcherBase::DW_ITEM_MOVED_TO))
+                        {
+                            DirectoryWatcherBase::DirectoryEvent ev(p_event->name, DirectoryWatcherBase::DW_ITEM_MOVED_TO);
+                            owner.onItemMovedTo(ev);
+                        }
+                    }
+
+                    i += sizeof(inotify_event) + p_event->len;
+                    n -= sizeof(inotify_event) + p_event->len;
+                }
+            }
+
+            /// Wake up reader thread
+            owner.storage.wakeUp();
+        }
+        else
+        {
+            if (milliseconds_to_wait < static_cast<uint64_t>(settings->poll_directory_watch_events_backoff_max.totalMilliseconds()))
+                milliseconds_to_wait *= settings->poll_directory_watch_events_backoff_factor.value;
+        }
+    }
+}
+
+DirectoryWatcherBase::~DirectoryWatcherBase()
+{
+    stop();
+    close(fd);
+}
+
+void DirectoryWatcherBase::start()
+{
+    if (watch_task)
+        watch_task->activateAndSchedule();
+}
+
+void DirectoryWatcherBase::stop()
+{
+    stopped = true;
+    if (watch_task)
+        watch_task->deactivate();
+}
+
+}
diff --git a/src/Storages/FileLog/DirectoryWatcherBase.h b/src/Storages/FileLog/DirectoryWatcherBase.h
new file mode 100644
index 000000000000..a640f686c8a8
--- /dev/null
+++ b/src/Storages/FileLog/DirectoryWatcherBase.h
@@ -0,0 +1,108 @@
+#pragma once
+
+#include <Core/BackgroundSchedulePool.h>
+
+#include <atomic>
+#include <memory>
+#include <string>
+
+namespace DB
+{
+class FileLogDirectoryWatcher;
+
+class DirectoryWatcherBase : WithContext
+{
+    /// Most of code in this class is copy from the Poco project:
+    /// https://github.com/ClickHouse-Extras/poco/blob/clickhouse/Foundation/src/DirectoryWatcher.cpp
+    /// This class is used to get notifications about changes
+    /// to the filesystem, more precisely, to a specific
+    /// directory. Changes to a directory are reported via
+    /// events.
+    ///
+    /// A thread will be created that watches the specified
+    /// directory for changes. Events are reported in the context
+    /// of this thread.
+    ///
+    /// Note that changes to files in subdirectories of the watched
+    /// directory are not reported. Separate DirectoryWatcher objects
+    /// must be created for these directories if they should be watched.
+public:
+    enum DirectoryEventType
+    {
+        DW_ITEM_ADDED = 1,
+        /// A new item has been created and added to the directory.
+
+        DW_ITEM_REMOVED = 2,
+        /// An item has been removed from the directory.
+
+        DW_ITEM_MODIFIED = 4,
+        /// An item has been modified.
+
+        DW_ITEM_MOVED_FROM = 8,
+        /// An item has been renamed or moved. This event delivers the old name.
+
+        DW_ITEM_MOVED_TO = 16
+        /// An item has been renamed or moved. This event delivers the new name.
+    };
+
+    enum DirectoryEventMask
+    {
+        /// Enables all event types.
+        DW_FILTER_ENABLE_ALL = 31,
+
+        /// Disables all event types.
+        DW_FILTER_DISABLE_ALL = 0
+    };
+
+    struct DirectoryEvent
+    {
+        DirectoryEvent(const std::string & f, DirectoryEventType ev) : path(f), event(ev) { }
+
+        /// The directory or file that has been changed.
+        const std::string path;
+        /// The kind of event.
+        DirectoryEventType event;
+    };
+
+
+    DirectoryWatcherBase() = delete;
+    DirectoryWatcherBase(const DirectoryWatcherBase &) = delete;
+    DirectoryWatcherBase & operator=(const DirectoryWatcherBase &) = delete;
+
+    /// Creates a DirectoryWatcher for the directory given in path.
+    /// To enable only specific events, an eventMask can be specified by
+    /// OR-ing the desired event IDs (e.g., DW_ITEM_ADDED | DW_ITEM_MODIFIED).
+    explicit DirectoryWatcherBase(
+        FileLogDirectoryWatcher & owner_, const std::string & path_, ContextPtr context_, int event_mask_ = DW_FILTER_ENABLE_ALL);
+
+    ~DirectoryWatcherBase();
+
+    /// Returns the value of the eventMask passed to the constructor.
+    int eventMask() const { return event_mask; }
+
+    /// Returns the directory being watched.
+    const std::string & directory() const;
+
+    void watchFunc();
+
+protected:
+    void start();
+    void stop();
+
+private:
+    FileLogDirectoryWatcher & owner;
+
+    using TaskThread = BackgroundSchedulePool::TaskHolder;
+    TaskThread watch_task;
+
+    std::atomic<bool> stopped{false};
+
+
+    const std::string path;
+    int event_mask;
+    uint64_t milliseconds_to_wait;
+
+    int fd;
+};
+
+}
diff --git a/src/Storages/FileLog/FileLogDirectoryWatcher.cpp b/src/Storages/FileLog/FileLogDirectoryWatcher.cpp
new file mode 100644
index 000000000000..192721f9f3c3
--- /dev/null
+++ b/src/Storages/FileLog/FileLogDirectoryWatcher.cpp
@@ -0,0 +1,139 @@
+#include <Storages/FileLog/FileLogDirectoryWatcher.h>
+
+namespace DB
+{
+FileLogDirectoryWatcher::FileLogDirectoryWatcher(const std::string & path_, StorageFileLog & storage_, ContextPtr context_)
+    : path(path_)
+    , storage(storage_)
+    , log(&Poco::Logger::get("FileLogDirectoryWatcher(" + path + ")"))
+    , dw(std::make_unique<DirectoryWatcherBase>(*this, path, context_))
+{
+}
+
+FileLogDirectoryWatcher::Events FileLogDirectoryWatcher::getEventsAndReset()
+{
+    std::lock_guard<std::mutex> lock(mutex);
+    Events res;
+    res.swap(events);
+    return res;
+}
+
+FileLogDirectoryWatcher::Error FileLogDirectoryWatcher::getErrorAndReset()
+{
+    std::lock_guard<std::mutex> lock(mutex);
+    Error old_error = error;
+    error = {};
+    return old_error;
+}
+
+const std::string & FileLogDirectoryWatcher::getPath() const
+{
+    return path;
+}
+
+void FileLogDirectoryWatcher::onItemAdded(DirectoryWatcherBase::DirectoryEvent ev)
+{
+    std::lock_guard<std::mutex> lock(mutex);
+
+    EventInfo info{ev.event, "onItemAdded"};
+    std::string event_path = ev.path;
+
+    if (auto it = events.find(event_path); it != events.end())
+    {
+        it->second.file_events.emplace_back(info);
+    }
+    else
+    {
+        events.emplace(event_path, FileEvents{.file_events = std::vector<EventInfo>{info}});
+    }
+}
+
+
+void FileLogDirectoryWatcher::onItemRemoved(DirectoryWatcherBase::DirectoryEvent ev)
+{
+    std::lock_guard<std::mutex> lock(mutex);
+
+    EventInfo info{ev.event, "onItemRemoved"};
+    std::string event_path = ev.path;
+
+    if (auto it = events.find(event_path); it != events.end())
+    {
+        it->second.file_events.emplace_back(info);
+    }
+    else
+    {
+        events.emplace(event_path, FileEvents{.file_events = std::vector<EventInfo>{info}});
+    }
+}
+
+/// Optimize for MODIFY event, during a streamToViews period, since the log files
+/// are append only, there are may a lots of MODIFY events produced for one file.
+/// For example, appending 10000 logs into one file will result in 10000 MODIFY event.
+/// So, if we record all of these events, it will use a lot of memory, and then we
+/// need to handle it one by one in StorageFileLog::updateFileInfos, this is unnecessary
+/// because it is equal to just record and handle one MODIY event
+void FileLogDirectoryWatcher::onItemModified(DirectoryWatcherBase::DirectoryEvent ev)
+{
+    std::lock_guard<std::mutex> lock(mutex);
+
+    auto event_path = ev.path;
+    EventInfo info{ev.event, "onItemModified"};
+    if (auto it = events.find(event_path); it != events.end())
+    {
+        /// Already have MODIFY event for this file
+        if (it->second.received_modification_event)
+            return;
+        else
+        {
+            it->second.received_modification_event = true;
+            it->second.file_events.emplace_back(info);
+        }
+    }
+    else
+    {
+        events.emplace(event_path, FileEvents{.received_modification_event = true, .file_events = std::vector<EventInfo>{info}});
+    }
+}
+
+void FileLogDirectoryWatcher::onItemMovedFrom(DirectoryWatcherBase::DirectoryEvent ev)
+{
+    std::lock_guard<std::mutex> lock(mutex);
+
+    EventInfo info{ev.event, "onItemMovedFrom"};
+    std::string event_path = ev.path;
+
+    if (auto it = events.find(event_path); it != events.end())
+    {
+        it->second.file_events.emplace_back(info);
+    }
+    else
+    {
+        events.emplace(event_path, FileEvents{.file_events = std::vector<EventInfo>{info}});
+    }
+}
+
+void FileLogDirectoryWatcher::onItemMovedTo(DirectoryWatcherBase::DirectoryEvent ev)
+{
+    std::lock_guard<std::mutex> lock(mutex);
+
+    EventInfo info{ev.event, "onItemMovedTo"};
+    std::string event_path = ev.path;
+
+    if (auto it = events.find(event_path); it != events.end())
+    {
+        it->second.file_events.emplace_back(info);
+    }
+    else
+    {
+        events.emplace(event_path, FileEvents{.file_events = std::vector<EventInfo>{info}});
+    }
+}
+
+void FileLogDirectoryWatcher::onError(Exception e)
+{
+    std::lock_guard<std::mutex> lock(mutex);
+    LOG_ERROR(log, "Error happened during watching directory: {}", error.error_msg);
+    error.has_error = true;
+    error.error_msg = e.message();
+}
+}
diff --git a/src/Storages/FileLog/FileLogDirectoryWatcher.h b/src/Storages/FileLog/FileLogDirectoryWatcher.h
new file mode 100644
index 000000000000..0b0c86397aab
--- /dev/null
+++ b/src/Storages/FileLog/FileLogDirectoryWatcher.h
@@ -0,0 +1,77 @@
+#pragma once
+
+#include <Storages/FileLog/DirectoryWatcherBase.h>
+
+#include <base/logger_useful.h>
+
+#include <memory>
+#include <mutex>
+
+namespace DB
+{
+class StorageFileLog;
+
+class FileLogDirectoryWatcher
+{
+public:
+    struct EventInfo
+    {
+        DirectoryWatcherBase::DirectoryEventType type;
+        std::string callback;
+    };
+
+    struct FileEvents
+    {
+        bool received_modification_event = false;
+        std::vector<EventInfo> file_events;
+    };
+
+    using Events = std::unordered_map<std::string, FileEvents>;
+
+    struct Error
+    {
+        bool has_error = false;
+        std::string error_msg = {};
+    };
+
+    FileLogDirectoryWatcher(const std::string & path_, StorageFileLog & storage_, ContextPtr context_);
+    ~FileLogDirectoryWatcher() = default;
+
+    Events getEventsAndReset();
+
+    Error getErrorAndReset();
+
+    const std::string & getPath() const;
+
+private:
+    friend class DirectoryWatcherBase;
+    /// Here must pass by value, otherwise will lead to stack-use-of-scope
+    void onItemAdded(DirectoryWatcherBase::DirectoryEvent ev);
+    void onItemRemoved(DirectoryWatcherBase::DirectoryEvent ev);
+    void onItemModified(DirectoryWatcherBase::DirectoryEvent ev);
+    void onItemMovedFrom(DirectoryWatcherBase::DirectoryEvent ev);
+    void onItemMovedTo(DirectoryWatcherBase::DirectoryEvent ev);
+    void onError(Exception);
+
+    const std::string path;
+
+    StorageFileLog & storage;
+
+    /// Note, in order to avoid data race found by fuzzer, put events before dw,
+    /// such that when this class destruction, dw will be destructed before events.
+    /// The data race is because dw create a separate thread to monitor file events
+    /// and put into events, then if we destruct events first, the monitor thread still
+    /// running, it may access events during events destruction, leads to data race.
+    /// And we should put other members before dw as well, because all of them can be
+    /// accessed in thread created by dw.
+    Events events;
+
+    Poco::Logger * log;
+
+    std::mutex mutex;
+
+    Error error;
+
+    std::unique_ptr<DirectoryWatcherBase> dw;
+};
+}
diff --git a/src/Storages/FileLog/FileLogSettings.cpp b/src/Storages/FileLog/FileLogSettings.cpp
new file mode 100644
index 000000000000..2cd42c35870c
--- /dev/null
+++ b/src/Storages/FileLog/FileLogSettings.cpp
@@ -0,0 +1,41 @@
+#include <Parsers/ASTCreateQuery.h>
+#include <Parsers/ASTFunction.h>
+#include <Parsers/ASTSetQuery.h>
+#include <Storages/FileLog/FileLogSettings.h>
+#include <Common/Exception.h>
+
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int UNKNOWN_SETTING;
+}
+
+IMPLEMENT_SETTINGS_TRAITS(FileLogSettingsTraits, LIST_OF_FILELOG_SETTINGS)
+
+void FileLogSettings::loadFromQuery(ASTStorage & storage_def)
+{
+    if (storage_def.settings)
+    {
+        try
+        {
+            applyChanges(storage_def.settings->changes);
+        }
+        catch (Exception & e)
+        {
+            if (e.code() == ErrorCodes::UNKNOWN_SETTING)
+                e.addMessage("for storage " + storage_def.engine->name);
+            throw;
+        }
+    }
+    else
+    {
+        auto settings_ast = std::make_shared<ASTSetQuery>();
+        settings_ast->is_standalone = false;
+        storage_def.set(storage_def.settings, settings_ast);
+    }
+}
+
+}
diff --git a/src/Storages/FileLog/FileLogSettings.h b/src/Storages/FileLog/FileLogSettings.h
new file mode 100644
index 000000000000..d14120d0ba0d
--- /dev/null
+++ b/src/Storages/FileLog/FileLogSettings.h
@@ -0,0 +1,37 @@
+#pragma once
+
+#include <Core/BaseSettings.h>
+#include <Core/Settings.h>
+
+
+namespace DB
+{
+class ASTStorage;
+
+
+#define FILELOG_RELATED_SETTINGS(M) \
+    /* default is stream_poll_timeout_ms */ \
+    M(Milliseconds, poll_timeout_ms, 0, "Timeout for single poll from StorageFileLog.", 0) \
+    M(UInt64, poll_max_batch_size, 0, "Maximum amount of messages to be polled in a single StorageFileLog poll.", 0) \
+    M(UInt64, max_block_size, 0, "Number of row collected by poll(s) for flushing data from StorageFileLog.", 0) \
+    M(UInt64, max_threads, 8, "Number of max threads to parse files, default is 8", 0) \
+    M(Milliseconds, poll_directory_watch_events_backoff_init, 500, "The initial sleep value for watch directory thread.", 0) \
+    M(Milliseconds, poll_directory_watch_events_backoff_max, 32000, "The max sleep value for watch directory thread.", 0) \
+    M(UInt64, poll_directory_watch_events_backoff_factor, 2, "The speed of backoff, exponential by default", 0)
+
+#define LIST_OF_FILELOG_SETTINGS(M) \
+    FILELOG_RELATED_SETTINGS(M) \
+    FORMAT_FACTORY_SETTINGS(M)
+
+DECLARE_SETTINGS_TRAITS(FileLogSettingsTraits, LIST_OF_FILELOG_SETTINGS)
+
+
+/** Settings for the FileLog engine.
+  * Could be loaded from a CREATE TABLE query (SETTINGS clause).
+  */
+struct FileLogSettings : public BaseSettings<FileLogSettingsTraits>
+{
+    void loadFromQuery(ASTStorage & storage_def);
+};
+
+}
diff --git a/src/Storages/FileLog/FileLogSource.cpp b/src/Storages/FileLog/FileLogSource.cpp
new file mode 100644
index 000000000000..a8da34f32041
--- /dev/null
+++ b/src/Storages/FileLog/FileLogSource.cpp
@@ -0,0 +1,143 @@
+#include <Formats/FormatFactory.h>
+#include <Interpreters/Context.h>
+#include <Processors/Executors/StreamingFormatExecutor.h>
+#include <Storages/FileLog/FileLogSource.h>
+#include <Storages/FileLog/ReadBufferFromFileLog.h>
+#include <Common/Stopwatch.h>
+#include <base/logger_useful.h>
+
+namespace DB
+{
+static constexpr auto MAX_FAILED_POLL_ATTEMPTS = 10;
+
+FileLogSource::FileLogSource(
+    StorageFileLog & storage_,
+    const StorageMetadataPtr & metadata_snapshot_,
+    const ContextPtr & context_,
+    const Names & columns,
+    size_t max_block_size_,
+    size_t poll_time_out_,
+    size_t stream_number_,
+    size_t max_streams_number_)
+    : SourceWithProgress(metadata_snapshot_->getSampleBlockForColumns(columns, storage_.getVirtuals(), storage_.getStorageID()))
+    , storage(storage_)
+    , metadata_snapshot(metadata_snapshot_)
+    , context(context_)
+    , column_names(columns)
+    , max_block_size(max_block_size_)
+    , poll_time_out(poll_time_out_)
+    , stream_number(stream_number_)
+    , max_streams_number(max_streams_number_)
+    , non_virtual_header(metadata_snapshot_->getSampleBlockNonMaterialized())
+    , virtual_header(
+          metadata_snapshot->getSampleBlockForColumns(storage.getVirtualColumnNames(), storage.getVirtuals(), storage.getStorageID()))
+{
+    buffer = std::make_unique<ReadBufferFromFileLog>(storage, max_block_size, poll_time_out, context, stream_number_, max_streams_number_);
+
+    const auto & file_infos = storage.getFileInfos();
+
+    size_t files_per_stream = file_infos.file_names.size() / max_streams_number;
+    start = stream_number * files_per_stream;
+    end = stream_number == max_streams_number - 1 ? file_infos.file_names.size() : (stream_number + 1) * files_per_stream;
+
+    storage.increaseStreams();
+}
+
+FileLogSource::~FileLogSource()
+{
+    try
+    {
+        if (!finished)
+            onFinish();
+    }
+    catch (...)
+    {
+        tryLogCurrentException(__PRETTY_FUNCTION__);
+    }
+    storage.reduceStreams();
+}
+
+void FileLogSource::onFinish()
+{
+    storage.closeFilesAndStoreMeta(start, end);
+    finished = true;
+}
+
+Chunk FileLogSource::generate()
+{
+    /// Store metas of last written chunk into disk
+    storage.storeMetas(start, end);
+
+    if (!buffer || buffer->noRecords())
+    {
+        /// There is no onFinish for ISource, we call it
+        /// when no records return to close files
+        onFinish();
+        return {};
+    }
+
+    MutableColumns virtual_columns = virtual_header.cloneEmptyColumns();
+
+    auto input_format
+        = FormatFactory::instance().getInputFormat(storage.getFormatName(), *buffer, non_virtual_header, context, max_block_size);
+
+    StreamingFormatExecutor executor(non_virtual_header, input_format);
+
+    size_t total_rows = 0;
+    size_t failed_poll_attempts = 0;
+
+    Stopwatch watch;
+    while (true)
+    {
+        size_t new_rows = 0;
+        if (buffer->poll())
+            new_rows = executor.execute();
+
+        if (new_rows)
+        {
+            auto file_name = buffer->getFileName();
+            auto offset = buffer->getOffset();
+            for (size_t i = 0; i < new_rows; ++i)
+            {
+                virtual_columns[0]->insert(file_name);
+                virtual_columns[1]->insert(offset);
+            }
+            total_rows = total_rows + new_rows;
+        }
+        else /// poll succeed, but parse failed
+        {
+            ++failed_poll_attempts;
+        }
+
+        if (!buffer->hasMorePolledRecords()
+            && ((total_rows >= max_block_size) || watch.elapsedMilliseconds() > poll_time_out
+                || failed_poll_attempts >= MAX_FAILED_POLL_ATTEMPTS))
+        {
+            break;
+        }
+    }
+
+    if (total_rows == 0)
+    {
+        onFinish();
+        return {};
+    }
+
+    auto result_block = non_virtual_header.cloneWithColumns(executor.getResultColumns());
+    auto virtual_block = virtual_header.cloneWithColumns(std::move(virtual_columns));
+
+    for (const auto & column : virtual_block.getColumnsWithTypeAndName())
+        result_block.insert(column);
+
+    auto converting_dag = ActionsDAG::makeConvertingActions(
+        result_block.cloneEmpty().getColumnsWithTypeAndName(),
+        getPort().getHeader().getColumnsWithTypeAndName(),
+        ActionsDAG::MatchColumnsMode::Name);
+
+    auto converting_actions = std::make_shared<ExpressionActions>(std::move(converting_dag));
+    converting_actions->execute(result_block);
+
+    return Chunk(result_block.getColumns(), result_block.rows());
+}
+
+}
diff --git a/src/Storages/FileLog/FileLogSource.h b/src/Storages/FileLog/FileLogSource.h
new file mode 100644
index 000000000000..e1348de5bdf7
--- /dev/null
+++ b/src/Storages/FileLog/FileLogSource.h
@@ -0,0 +1,61 @@
+#pragma once
+
+#include <Processors/Sources/SourceWithProgress.h>
+
+#include <Storages/FileLog/ReadBufferFromFileLog.h>
+#include <Storages/FileLog/StorageFileLog.h>
+
+namespace Poco
+{
+    class Logger;
+}
+namespace DB
+{
+class FileLogSource : public SourceWithProgress
+{
+public:
+    FileLogSource(
+        StorageFileLog & storage_,
+        const StorageMetadataPtr & metadata_snapshot_,
+        const ContextPtr & context_,
+        const Names & columns,
+        size_t max_block_size_,
+        size_t poll_time_out_,
+        size_t stream_number_,
+        size_t max_streams_number_);
+
+    String getName() const override { return "FileLog"; }
+
+    bool noRecords() { return !buffer || buffer->noRecords(); }
+
+    void onFinish();
+
+    virtual ~FileLogSource() override;
+
+protected:
+    Chunk generate() override;
+
+private:
+    StorageFileLog & storage;
+    StorageMetadataPtr metadata_snapshot;
+    ContextPtr context;
+    Names column_names;
+    UInt64 max_block_size;
+
+    size_t poll_time_out;
+
+    size_t stream_number;
+    size_t max_streams_number;
+
+    std::unique_ptr<ReadBufferFromFileLog> buffer;
+
+    Block non_virtual_header;
+    Block virtual_header;
+
+    /// The start pos and end pos of files responsible by this stream,
+    /// does not include end
+    size_t start;
+    size_t end;
+};
+
+}
diff --git a/src/Storages/FileLog/ReadBufferFromFileLog.cpp b/src/Storages/FileLog/ReadBufferFromFileLog.cpp
new file mode 100644
index 000000000000..a55df9fe09e2
--- /dev/null
+++ b/src/Storages/FileLog/ReadBufferFromFileLog.cpp
@@ -0,0 +1,176 @@
+#include <Interpreters/Context.h>
+#include <Storages/FileLog/ReadBufferFromFileLog.h>
+#include <Common/Stopwatch.h>
+
+#include <base/logger_useful.h>
+
+#include <algorithm>
+#include <filesystem>
+#include <boost/algorithm/string/join.hpp>
+
+namespace DB
+{
+namespace ErrorCodes
+{
+    extern const int CANNOT_READ_ALL_DATA;
+}
+
+ReadBufferFromFileLog::ReadBufferFromFileLog(
+    StorageFileLog & storage_,
+    size_t max_batch_size,
+    size_t poll_timeout_,
+    ContextPtr context_,
+    size_t stream_number_,
+    size_t max_streams_number_)
+    : ReadBuffer(nullptr, 0)
+    , log(&Poco::Logger::get("ReadBufferFromFileLog " + toString(stream_number_)))
+    , storage(storage_)
+    , batch_size(max_batch_size)
+    , poll_timeout(poll_timeout_)
+    , context(context_)
+    , stream_number(stream_number_)
+    , max_streams_number(max_streams_number_)
+{
+    current = records.begin();
+    allowed = false;
+}
+
+bool ReadBufferFromFileLog::poll()
+{
+    if (hasMorePolledRecords())
+    {
+        allowed = true;
+        return true;
+    }
+
+    auto new_records = pollBatch(batch_size);
+    if (new_records.empty())
+    {
+        buffer_status = BufferStatus::NO_RECORD_RETURNED;
+        LOG_TRACE(log, "No new records to read");
+        return false;
+    }
+    else
+    {
+        records = std::move(new_records);
+        current = records.begin();
+
+        LOG_TRACE(log, "Polled batch of {} records. ", records.size());
+
+        buffer_status = BufferStatus::POLLED_OK;
+        allowed = true;
+        return true;
+    }
+}
+
+ReadBufferFromFileLog::Records ReadBufferFromFileLog::pollBatch(size_t batch_size_)
+{
+    Records new_records;
+    new_records.reserve(batch_size_);
+
+    readNewRecords(new_records, batch_size);
+    if (new_records.size() == batch_size_ || stream_out)
+        return new_records;
+
+    Stopwatch watch;
+    while (watch.elapsedMilliseconds() < poll_timeout && new_records.size() != batch_size_)
+    {
+        readNewRecords(new_records, batch_size);
+        /// All ifstrem reach end, no need to wait for timeout,
+        /// since file status can not be updated during a streamToViews
+        if (stream_out)
+            break;
+    }
+
+    return new_records;
+}
+
+void ReadBufferFromFileLog::readNewRecords(ReadBufferFromFileLog::Records & new_records, size_t batch_size_)
+{
+    size_t need_records_size = batch_size_ - new_records.size();
+    size_t read_records_size = 0;
+
+    auto & file_infos = storage.getFileInfos();
+
+    size_t files_per_stream = file_infos.file_names.size() / max_streams_number;
+    size_t start = stream_number * files_per_stream;
+    size_t end = stream_number == max_streams_number - 1 ? file_infos.file_names.size() : (stream_number + 1) * files_per_stream;
+
+    for (size_t i = start; i < end; ++i)
+    {
+        const auto & file_name = file_infos.file_names[i];
+
+        auto & file_ctx = StorageFileLog::findInMap(file_infos.context_by_name, file_name);
+        if (file_ctx.status == StorageFileLog::FileStatus::NO_CHANGE)
+            continue;
+
+        auto & file_meta = StorageFileLog::findInMap(file_infos.meta_by_inode, file_ctx.inode);
+
+        if (!file_ctx.reader)
+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, "Ifstream for file {} is not initialized", file_meta.file_name);
+
+        auto & reader = file_ctx.reader.value();
+        StorageFileLog::assertStreamGood(reader);
+
+        Record record;
+        while (read_records_size < need_records_size)
+        {
+            /// Need to get offset before reading record from stream
+            auto offset = reader.tellg();
+            if (static_cast<UInt64>(offset) >= file_meta.last_open_end)
+                break;
+            record.offset = offset;
+            StorageFileLog::assertStreamGood(reader);
+
+            record.file_name = file_name;
+
+
+            std::getline(reader, record.data);
+            StorageFileLog::assertStreamGood(reader);
+
+            new_records.emplace_back(record);
+            ++read_records_size;
+        }
+
+        UInt64 current_position = reader.tellg();
+        StorageFileLog::assertStreamGood(reader);
+
+        file_meta.last_writen_position = current_position;
+
+        /// stream reach to end
+        if (current_position == file_meta.last_open_end)
+        {
+            file_ctx.status = StorageFileLog::FileStatus::NO_CHANGE;
+        }
+
+        /// All ifstream reach end
+        if (i == end - 1 && (file_ctx.status == StorageFileLog::FileStatus::NO_CHANGE))
+        {
+            stream_out = true;
+        }
+
+        if (read_records_size == need_records_size)
+        {
+            break;
+        }
+    }
+}
+
+bool ReadBufferFromFileLog::nextImpl()
+{
+    if (!allowed || !hasMorePolledRecords())
+        return false;
+
+    auto * new_position = const_cast<char *>(current->data.data());
+    BufferBase::set(new_position, current->data.size(), 0);
+    allowed = false;
+
+    current_file = current->file_name;
+    current_offset = current->offset;
+
+    ++current;
+
+    return true;
+}
+
+}
diff --git a/src/Storages/FileLog/ReadBufferFromFileLog.h b/src/Storages/FileLog/ReadBufferFromFileLog.h
new file mode 100644
index 000000000000..117a858de3b1
--- /dev/null
+++ b/src/Storages/FileLog/ReadBufferFromFileLog.h
@@ -0,0 +1,93 @@
+#pragma once
+
+#include <Core/BackgroundSchedulePool.h>
+#include <Core/Names.h>
+#include <IO/ReadBuffer.h>
+#include <Storages/FileLog/StorageFileLog.h>
+#include <base/types.h>
+
+#include <fstream>
+#include <mutex>
+
+namespace Poco
+{
+    class Logger;
+}
+
+namespace DB
+{
+class ReadBufferFromFileLog : public ReadBuffer
+{
+public:
+    ReadBufferFromFileLog(
+        StorageFileLog & storage_,
+        size_t max_batch_size,
+        size_t poll_timeout_,
+        ContextPtr context_,
+        size_t stream_number_,
+        size_t max_streams_number_);
+
+    ~ReadBufferFromFileLog() override = default;
+
+    auto pollTimeout() const { return poll_timeout; }
+
+    bool hasMorePolledRecords() const { return current != records.end(); }
+
+    bool poll();
+
+    bool noRecords() { return buffer_status == BufferStatus::NO_RECORD_RETURNED; }
+
+    auto getFileName() const { return current_file; }
+    auto getOffset() const { return current_offset; }
+
+private:
+    enum class BufferStatus
+    {
+        INIT,
+        NO_RECORD_RETURNED,
+        POLLED_OK,
+    };
+
+    BufferStatus buffer_status = BufferStatus::INIT;
+
+    Poco::Logger * log;
+
+    StorageFileLog & storage;
+
+    bool stream_out = false;
+
+    size_t batch_size;
+    size_t poll_timeout;
+
+    ContextPtr context;
+
+    size_t stream_number;
+    size_t max_streams_number;
+
+    bool allowed = true;
+
+    using RecordData = std::string;
+    struct Record
+    {
+        RecordData data;
+        std::string file_name;
+        /// Offset is the start of a row, which is needed for virtual columns.
+        UInt64 offset;
+    };
+    using Records = std::vector<Record>;
+
+    Records records;
+    Records::const_iterator current;
+
+    String current_file;
+    UInt64 current_offset = 0;
+
+    using TaskThread = BackgroundSchedulePool::TaskHolder;
+
+    Records pollBatch(size_t batch_size_);
+
+    void readNewRecords(Records & new_records, size_t batch_size_);
+
+    bool nextImpl() override;
+};
+}
diff --git a/src/Storages/FileLog/StorageFileLog.cpp b/src/Storages/FileLog/StorageFileLog.cpp
new file mode 100644
index 000000000000..463ad77b1426
--- /dev/null
+++ b/src/Storages/FileLog/StorageFileLog.cpp
@@ -0,0 +1,976 @@
+#include <DataTypes/DataTypeString.h>
+#include <DataTypes/DataTypesNumber.h>
+#include <Disks/StoragePolicy.h>
+#include <IO/ReadBufferFromFile.h>
+#include <IO/ReadHelpers.h>
+#include <IO/WriteBufferFromFile.h>
+#include <IO/WriteIntText.h>
+#include <Interpreters/Context.h>
+#include <Interpreters/InterpreterInsertQuery.h>
+#include <Interpreters/evaluateConstantExpression.h>
+#include <Parsers/ASTCreateQuery.h>
+#include <Parsers/ASTExpressionList.h>
+#include <Parsers/ASTInsertQuery.h>
+#include <Parsers/ASTLiteral.h>
+#include <Processors/Executors/CompletedPipelineExecutor.h>
+#include <QueryPipeline/Pipe.h>
+#include <Storages/FileLog/FileLogSource.h>
+#include <Storages/FileLog/ReadBufferFromFileLog.h>
+#include <Storages/FileLog/StorageFileLog.h>
+#include <Storages/StorageFactory.h>
+#include <Storages/StorageMaterializedView.h>
+#include <base/logger_useful.h>
+#include <Common/Exception.h>
+#include <Common/Macros.h>
+#include <Common/filesystemHelpers.h>
+#include <Common/getNumberOfPhysicalCPUCores.h>
+#include <Common/quoteString.h>
+#include <Common/typeid_cast.h>
+
+#include <sys/stat.h>
+
+namespace DB
+{
+
+namespace ErrorCodes
+{
+    extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;
+    extern const int BAD_ARGUMENTS;
+    extern const int CANNOT_STAT;
+    extern const int BAD_FILE_TYPE;
+    extern const int CANNOT_READ_ALL_DATA;
+    extern const int LOGICAL_ERROR;
+    extern const int TABLE_METADATA_ALREADY_EXISTS;
+    extern const int CANNOT_SELECT;
+    extern const int QUERY_NOT_ALLOWED;
+}
+
+namespace
+{
+    const auto MAX_THREAD_WORK_DURATION_MS = 60000;
+}
+
+StorageFileLog::StorageFileLog(
+    const StorageID & table_id_,
+    ContextPtr context_,
+    const ColumnsDescription & columns_,
+    const String & path_,
+    const String & format_name_,
+    std::unique_ptr<FileLogSettings> settings,
+    const String & comment,
+    bool attach)
+    : IStorage(table_id_)
+    , WithContext(context_->getGlobalContext())
+    , filelog_settings(std::move(settings))
+    , path(path_)
+    , format_name(format_name_)
+    , log(&Poco::Logger::get("StorageFileLog (" + table_id_.table_name + ")"))
+    , milliseconds_to_wait(filelog_settings->poll_directory_watch_events_backoff_init.totalMilliseconds())
+{
+    StorageInMemoryMetadata storage_metadata;
+    storage_metadata.setColumns(columns_);
+    storage_metadata.setComment(comment);
+    setInMemoryMetadata(storage_metadata);
+
+    try
+    {
+        loadMetaFiles(attach);
+        loadFiles();
+
+        assert(file_infos.file_names.size() == file_infos.meta_by_inode.size());
+        assert(file_infos.file_names.size() == file_infos.context_by_name.size());
+
+        if (path_is_directory)
+            directory_watch = std::make_unique<FileLogDirectoryWatcher>(root_data_path, *this, getContext());
+
+        auto thread = getContext()->getSchedulePool().createTask(log->name(), [this] { threadFunc(); });
+        task = std::make_shared<TaskContext>(std::move(thread));
+    }
+    catch (...)
+    {
+        if (!attach)
+            throw;
+        tryLogCurrentException(__PRETTY_FUNCTION__);
+    }
+}
+
+void StorageFileLog::loadMetaFiles(bool attach)
+{
+    const auto & storage = getStorageID();
+    root_meta_path
+        = std::filesystem::path(getContext()->getPath()) / ".filelog_storage_metadata" / storage.getDatabaseName() / storage.getTableName();
+
+    /// Attach table
+    if (attach)
+    {
+        /// Meta file may lost, log and create directory
+        if (!std::filesystem::exists(root_meta_path))
+        {
+            /// Create root_meta_path directory when store meta data
+            LOG_ERROR(log, "Metadata files of table {} are lost.", getStorageID().getTableName());
+        }
+        /// Load all meta info to file_infos;
+        deserialize();
+    }
+    /// Create table, just create meta data directory
+    else
+    {
+        if (std::filesystem::exists(root_meta_path))
+        {
+            throw Exception(
+                ErrorCodes::TABLE_METADATA_ALREADY_EXISTS,
+                "Metadata files already exist by path: {}, remove them manually if it is intended",
+                root_meta_path);
+        }
+        std::filesystem::create_directories(root_meta_path);
+    }
+}
+
+void StorageFileLog::loadFiles()
+{
+    if (!fileOrSymlinkPathStartsWith(path, getContext()->getUserFilesPath()))
+    {
+        throw Exception(
+            ErrorCodes::BAD_ARGUMENTS, "The absolute data path should be inside `user_files_path`({})", getContext()->getUserFilesPath());
+    }
+
+    auto absolute_path = std::filesystem::absolute(path);
+    absolute_path = absolute_path.lexically_normal(); /// Normalize path.
+
+    if (std::filesystem::is_regular_file(absolute_path))
+    {
+        path_is_directory = false;
+        root_data_path = absolute_path.parent_path();
+
+        file_infos.file_names.push_back(absolute_path.filename());
+    }
+    else if (std::filesystem::is_directory(absolute_path))
+    {
+        root_data_path = absolute_path;
+        /// Just consider file with depth 1
+        for (const auto & dir_entry : std::filesystem::directory_iterator{absolute_path})
+        {
+            if (dir_entry.is_regular_file())
+            {
+                file_infos.file_names.push_back(dir_entry.path().filename());
+            }
+        }
+    }
+    else
+    {
+        throw Exception(ErrorCodes::BAD_ARGUMENTS, "The path {} neither a regular file, nor a directory", absolute_path.c_str());
+    }
+
+    /// Get files inode
+    for (const auto & file : file_infos.file_names)
+    {
+        auto inode = getInode(getFullDataPath(file));
+        file_infos.context_by_name.emplace(file, FileContext{.inode = inode});
+    }
+
+    /// Update file meta or create file meta
+    for (const auto & [file, ctx] : file_infos.context_by_name)
+    {
+        if (auto it = file_infos.meta_by_inode.find(ctx.inode); it != file_infos.meta_by_inode.end())
+        {
+            /// data file have been renamed, need update meta file's name
+            if (it->second.file_name != file)
+            {
+                std::filesystem::rename(getFullMetaPath(it->second.file_name), getFullMetaPath(file));
+                it->second.file_name = file;
+            }
+        }
+        /// New file
+        else
+        {
+            FileMeta meta{file, 0, 0};
+            file_infos.meta_by_inode.emplace(ctx.inode, meta);
+        }
+    }
+
+    /// Clear unneeded meta file, because data files may be deleted
+    if (file_infos.meta_by_inode.size() > file_infos.context_by_name.size())
+    {
+        InodeToFileMeta valid_metas;
+        valid_metas.reserve(file_infos.context_by_name.size());
+        for (const auto & [inode, meta] : file_infos.meta_by_inode)
+        {
+            /// Note, here we need to use inode to judge does the meta file is valid.
+            /// In the case that when a file deleted, then we create new file with the
+            /// same name, it will have different inode number with stored meta file,
+            /// so the stored meta file is invalid
+            if (auto it = file_infos.context_by_name.find(meta.file_name);
+                it != file_infos.context_by_name.end() && it->second.inode == inode)
+                valid_metas.emplace(inode, meta);
+            /// Delete meta file from filesystem
+            else
+                std::filesystem::remove(getFullMetaPath(meta.file_name));
+        }
+        file_infos.meta_by_inode.swap(valid_metas);
+    }
+}
+
+void StorageFileLog::serialize() const
+{
+    if (!std::filesystem::exists(root_meta_path))
+    {
+        std::filesystem::create_directories(root_meta_path);
+    }
+    for (const auto & [inode, meta] : file_infos.meta_by_inode)
+    {
+        auto full_name = getFullMetaPath(meta.file_name);
+        if (!std::filesystem::exists(full_name))
+        {
+            FS::createFile(full_name);
+        }
+        else
+        {
+            checkOffsetIsValid(full_name, meta.last_writen_position);
+        }
+        WriteBufferFromFile out(full_name);
+        writeIntText(inode, out);
+        writeChar('
', out);
+        writeIntText(meta.last_writen_position, out);
+    }
+}
+
+void StorageFileLog::serialize(UInt64 inode, const FileMeta & file_meta) const
+{
+    if (!std::filesystem::exists(root_meta_path))
+    {
+        std::filesystem::create_directories(root_meta_path);
+    }
+    auto full_name = getFullMetaPath(file_meta.file_name);
+    if (!std::filesystem::exists(full_name))
+    {
+        FS::createFile(full_name);
+    }
+    else
+    {
+        checkOffsetIsValid(full_name, file_meta.last_writen_position);
+    }
+    WriteBufferFromFile out(full_name);
+    writeIntText(inode, out);
+    writeChar('
', out);
+    writeIntText(file_meta.last_writen_position, out);
+}
+
+void StorageFileLog::deserialize()
+{
+    if (!std::filesystem::exists(root_meta_path))
+        return;
+    /// In case of single file (not a watched directory),
+    /// iterated directory always has one file inside.
+    for (const auto & dir_entry : std::filesystem::directory_iterator{root_meta_path})
+    {
+        if (!dir_entry.is_regular_file())
+        {
+            throw Exception(
+                ErrorCodes::BAD_FILE_TYPE,
+                "The file {} under {} is not a regular file when deserializing meta files",
+                dir_entry.path().c_str(),
+                root_meta_path);
+        }
+
+        ReadBufferFromFile in(dir_entry.path().c_str());
+        FileMeta meta;
+        UInt64 inode, last_written_pos;
+
+        if (!tryReadIntText(inode, in))
+        {
+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, "Read meta file {} failed", dir_entry.path().c_str());
+        }
+        assertChar('
', in);
+        if (!tryReadIntText(last_written_pos, in))
+        {
+            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, "Read meta file {} failed", dir_entry.path().c_str());
+        }
+
+        meta.file_name = dir_entry.path().filename();
+        meta.last_writen_position = last_written_pos;
+
+        file_infos.meta_by_inode.emplace(inode, meta);
+    }
+}
+
+UInt64 StorageFileLog::getInode(const String & file_name)
+{
+    struct stat file_stat;
+    if (stat(file_name.c_str(), &file_stat))
+    {
+        throw Exception(ErrorCodes::CANNOT_STAT, "Can not get stat info of file {}", file_name);
+    }
+    return file_stat.st_ino;
+}
+
+Pipe StorageFileLog::read(
+    const Names & column_names,
+    const StorageMetadataPtr & metadata_snapshot,
+    SelectQueryInfo & /* query_info */,
+    ContextPtr local_context,
+    QueryProcessingStage::Enum /* processed_stage */,
+    size_t /* max_block_size */,
+    unsigned /* num_streams */)
+{
+    /// If there are MVs depended on this table, we just forbid reading
+    if (has_dependent_mv)
+    {
+        throw Exception(
+            ErrorCodes::QUERY_NOT_ALLOWED,
+            "Can not make `SELECT` query from table {}, because it has attached dependencies. Remove dependent materialized views if "
+            "needed",
+            getStorageID().getTableName());
+    }
+
+    std::lock_guard<std::mutex> lock(file_infos_mutex);
+    if (running_streams)
+    {
+        throw Exception("Another select query is running on this table, need to wait it finish.", ErrorCodes::CANNOT_SELECT);
+    }
+
+    updateFileInfos();
+
+    /// No files to parse
+    if (file_infos.file_names.empty())
+    {
+        LOG_WARNING(log, "There is a idle table named {}, no files need to parse.", getName());
+        return Pipe{};
+    }
+
+    auto modified_context = Context::createCopy(local_context);
+
+    auto max_streams_number = std::min<UInt64>(filelog_settings->max_threads, file_infos.file_names.size());
+
+    /// Each stream responsible for closing it's files and store meta
+    openFilesAndSetPos();
+
+    Pipes pipes;
+    pipes.reserve(max_streams_number);
+    for (size_t stream_number = 0; stream_number < max_streams_number; ++stream_number)
+    {
+        pipes.emplace_back(std::make_shared<FileLogSource>(
+            *this,
+            metadata_snapshot,
+            modified_context,
+            column_names,
+            getMaxBlockSize(),
+            getPollTimeoutMillisecond(),
+            stream_number,
+            max_streams_number));
+    }
+
+    return Pipe::unitePipes(std::move(pipes));
+}
+
+void StorageFileLog::increaseStreams()
+{
+    running_streams += 1;
+}
+
+void StorageFileLog::reduceStreams()
+{
+    running_streams -= 1;
+}
+
+void StorageFileLog::drop()
+{
+    try
+    {
+        if (std::filesystem::exists(root_meta_path))
+            std::filesystem::remove_all(root_meta_path);
+    }
+    catch (...)
+    {
+        tryLogCurrentException(__PRETTY_FUNCTION__);
+    }
+}
+
+void StorageFileLog::startup()
+{
+    try
+    {
+        if (task)
+        {
+            task->holder->activateAndSchedule();
+        }
+    }
+    catch (...)
+    {
+        tryLogCurrentException(__PRETTY_FUNCTION__);
+    }
+}
+
+void StorageFileLog::shutdown()
+{
+    try
+    {
+        if (task)
+        {
+            task->stream_cancelled = true;
+
+            /// Reader thread may wait for wake up
+            wakeUp();
+
+            LOG_TRACE(log, "Waiting for cleanup");
+            task->holder->deactivate();
+        }
+        /// If no reading call and threadFunc, the log files will never
+        /// be opened, also just leave the work of close files and
+        /// store meta to streams. because if we close files in here,
+        /// may result in data race with unfinishing reading pipeline
+    }
+    catch (...)
+    {
+        tryLogCurrentException(__PRETTY_FUNCTION__);
+        task->holder->deactivate();
+    }
+}
+
+void StorageFileLog::assertStreamGood(const std::ifstream & reader)
+{
+    if (!reader.good())
+    {
+        throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, "Stream is in bad state");
+    }
+}
+
+void StorageFileLog::openFilesAndSetPos()
+{
+    for (const auto & file : file_infos.file_names)
+    {
+        auto & file_ctx = findInMap(file_infos.context_by_name, file);
+        if (file_ctx.status != FileStatus::NO_CHANGE)
+        {
+            file_ctx.reader.emplace(getFullDataPath(file));
+            auto & reader = file_ctx.reader.value();
+            assertStreamGood(reader);
+
+            reader.seekg(0, reader.end);
+            assertStreamGood(reader);
+
+            auto file_end = reader.tellg();
+            assertStreamGood(reader);
+
+            auto & meta = findInMap(file_infos.meta_by_inode, file_ctx.inode);
+            if (meta.last_writen_position > static_cast<UInt64>(file_end))
+            {
+                throw Exception(
+                    ErrorCodes::CANNOT_READ_ALL_DATA,
+                    "Last saved offsset for File {} is bigger than file size ({} > {})",
+                    file,
+                    meta.last_writen_position,
+                    file_end);
+            }
+            /// update file end at the moment, used in ReadBuffer and serialize
+            meta.last_open_end = file_end;
+
+            reader.seekg(meta.last_writen_position);
+            assertStreamGood(reader);
+        }
+    }
+    serialize();
+}
+
+void StorageFileLog::closeFilesAndStoreMeta(size_t start, size_t end)
+{
+    assert(start >= 0);
+    assert(start < end);
+    assert(end <= file_infos.file_names.size());
+
+    for (size_t i = start; i < end; ++i)
+    {
+        auto & file_ctx = findInMap(file_infos.context_by_name, file_infos.file_names[i]);
+
+        if (file_ctx.reader)
+        {
+            if (file_ctx.reader->is_open())
+                file_ctx.reader->close();
+        }
+
+        auto & meta = findInMap(file_infos.meta_by_inode, file_ctx.inode);
+        serialize(file_ctx.inode, meta);
+    }
+}
+
+void StorageFileLog::storeMetas(size_t start, size_t end)
+{
+    assert(start >= 0);
+    assert(start < end);
+    assert(end <= file_infos.file_names.size());
+
+    for (size_t i = start; i < end; ++i)
+    {
+        auto & file_ctx = findInMap(file_infos.context_by_name, file_infos.file_names[i]);
+
+        auto & meta = findInMap(file_infos.meta_by_inode, file_ctx.inode);
+        serialize(file_ctx.inode, meta);
+    }
+}
+
+void StorageFileLog::checkOffsetIsValid(const String & full_name, UInt64 offset)
+{
+    ReadBufferFromFile in(full_name);
+    UInt64 _, last_written_pos;
+
+    if (!tryReadIntText(_, in))
+    {
+        throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, "Read meta file {} failed", full_name);
+    }
+    assertChar('
', in);
+    if (!tryReadIntText(last_written_pos, in))
+    {
+        throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, "Read meta file {} failed", full_name);
+    }
+    if (last_written_pos > offset)
+        throw Exception(
+            ErrorCodes::LOGICAL_ERROR, "Last stored last_written_pos in meta file {} is bigger than current last_written_pos", full_name);
+}
+
+size_t StorageFileLog::getMaxBlockSize() const
+{
+    return filelog_settings->max_block_size.changed ? filelog_settings->max_block_size.value
+                                                    : getContext()->getSettingsRef().max_insert_block_size.value;
+}
+
+size_t StorageFileLog::getPollMaxBatchSize() const
+{
+    size_t batch_size = filelog_settings->poll_max_batch_size.changed ? filelog_settings->poll_max_batch_size.value
+                                                                      : getContext()->getSettingsRef().max_block_size.value;
+    return std::min(batch_size, getMaxBlockSize());
+}
+
+size_t StorageFileLog::getPollTimeoutMillisecond() const
+{
+    return filelog_settings->poll_timeout_ms.changed ? filelog_settings->poll_timeout_ms.totalMilliseconds()
+                                                     : getContext()->getSettingsRef().stream_poll_timeout_ms.totalMilliseconds();
+}
+
+bool StorageFileLog::checkDependencies(const StorageID & table_id)
+{
+    // Check if all dependencies are attached
+    auto dependencies = DatabaseCatalog::instance().getDependencies(table_id);
+    if (dependencies.empty())
+        return true;
+
+    for (const auto & storage : dependencies)
+    {
+        auto table = DatabaseCatalog::instance().tryGetTable(storage, getContext());
+        if (!table)
+            return false;
+
+        // If it materialized view, check it's target table
+        auto * materialized_view = dynamic_cast<StorageMaterializedView *>(table.get());
+        if (materialized_view && !materialized_view->tryGetTargetTable())
+            return false;
+
+        // Check all its dependencies
+        if (!checkDependencies(storage))
+            return false;
+    }
+
+    return true;
+}
+
+size_t StorageFileLog::getTableDependentCount() const
+{
+    auto table_id = getStorageID();
+    // Check if at least one direct dependency is attached
+    return DatabaseCatalog::instance().getDependencies(table_id).size();
+}
+
+void StorageFileLog::threadFunc()
+{
+    bool reschedule = false;
+    try
+    {
+        auto table_id = getStorageID();
+
+        auto dependencies_count = getTableDependentCount();
+
+        if (dependencies_count)
+        {
+            has_dependent_mv = true;
+            auto start_time = std::chrono::steady_clock::now();
+
+            // Keep streaming as long as there are attached views and streaming is not cancelled
+            while (!task->stream_cancelled)
+            {
+                if (!checkDependencies(table_id))
+                {
+                    /// For this case, we can not wait for watch thread to wake up
+                    reschedule = true;
+                    break;
+                }
+
+                LOG_DEBUG(log, "Started streaming to {} attached views", dependencies_count);
+
+                if (streamToViews())
+                {
+                    LOG_TRACE(log, "Stream stalled. Reschedule.");
+                    if (milliseconds_to_wait
+                        < static_cast<uint64_t>(filelog_settings->poll_directory_watch_events_backoff_max.totalMilliseconds()))
+                        milliseconds_to_wait *= filelog_settings->poll_directory_watch_events_backoff_factor.value;
+                    break;
+                }
+                else
+                {
+                    milliseconds_to_wait = filelog_settings->poll_directory_watch_events_backoff_init.totalMilliseconds();
+                }
+
+                auto ts = std::chrono::steady_clock::now();
+                auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(ts-start_time);
+                if (duration.count() > MAX_THREAD_WORK_DURATION_MS)
+                {
+                    LOG_TRACE(log, "Thread work duration limit exceeded. Reschedule.");
+                    reschedule = true;
+                    break;
+                }
+            }
+        }
+    }
+    catch (...)
+    {
+        tryLogCurrentException(__PRETTY_FUNCTION__);
+    }
+
+    // Wait for attached views
+    if (!task->stream_cancelled)
+    {
+        if (path_is_directory)
+        {
+            if (!getTableDependentCount() || reschedule)
+                task->holder->scheduleAfter(milliseconds_to_wait);
+            else
+            {
+                std::unique_lock<std::mutex> lock(mutex);
+                /// Waiting for watch directory thread to wake up
+                cv.wait(lock, [this] { return has_new_events; });
+                has_new_events = false;
+
+                if (task->stream_cancelled)
+                    return;
+                task->holder->schedule();
+            }
+        }
+        else
+            task->holder->scheduleAfter(milliseconds_to_wait);
+    }
+}
+
+bool StorageFileLog::streamToViews()
+{
+    std::lock_guard<std::mutex> lock(file_infos_mutex);
+    if (running_streams)
+    {
+        LOG_INFO(log, "Another select query is running on this table, need to wait it finish.");
+        return true;
+    }
+
+    Stopwatch watch;
+
+    auto table_id = getStorageID();
+    auto table = DatabaseCatalog::instance().getTable(table_id, getContext());
+    if (!table)
+        throw Exception("Engine table " + table_id.getNameForLogs() + " doesn't exist", ErrorCodes::LOGICAL_ERROR);
+    auto metadata_snapshot = getInMemoryMetadataPtr();
+
+    auto max_streams_number = std::min<UInt64>(filelog_settings->max_threads.value, file_infos.file_names.size());
+    /// No files to parse
+    if (max_streams_number == 0)
+    {
+        LOG_INFO(log, "There is a idle table named {}, no files need to parse.", getName());
+        return updateFileInfos();
+    }
+
+    // Create an INSERT query for streaming data
+    auto insert = std::make_shared<ASTInsertQuery>();
+    insert->table_id = table_id;
+
+    auto new_context = Context::createCopy(getContext());
+
+    InterpreterInsertQuery interpreter(insert, new_context, false, true, true);
+    auto block_io = interpreter.execute();
+
+    /// Each stream responsible for closing it's files and store meta
+    openFilesAndSetPos();
+
+    Pipes pipes;
+    pipes.reserve(max_streams_number);
+    for (size_t stream_number = 0; stream_number < max_streams_number; ++stream_number)
+    {
+        pipes.emplace_back(std::make_shared<FileLogSource>(
+            *this,
+            metadata_snapshot,
+            new_context,
+            block_io.pipeline.getHeader().getNames(),
+            getPollMaxBatchSize(),
+            getPollTimeoutMillisecond(),
+            stream_number,
+            max_streams_number));
+    }
+
+    auto input= Pipe::unitePipes(std::move(pipes));
+
+    assertBlocksHaveEqualStructure(input.getHeader(), block_io.pipeline.getHeader(), "StorageFileLog streamToViews");
+
+    size_t rows = 0;
+    {
+        block_io.pipeline.complete(std::move(input));
+        block_io.pipeline.setProgressCallback([&](const Progress & progress) { rows += progress.read_rows.load(); });
+        CompletedPipelineExecutor executor(block_io.pipeline);
+        executor.execute();
+    }
+
+    UInt64 milliseconds = watch.elapsedMilliseconds();
+    LOG_DEBUG(log, "Pushing {} rows to {} took {} ms.", rows, table_id.getNameForLogs(), milliseconds);
+
+    return updateFileInfos();
+}
+
+void StorageFileLog::wakeUp()
+{
+    std::unique_lock<std::mutex> lock(mutex);
+    has_new_events = true;
+    lock.unlock();
+    cv.notify_one();
+}
+
+void registerStorageFileLog(StorageFactory & factory)
+{
+    auto creator_fn = [](const StorageFactory::Arguments & args)
+    {
+        ASTs & engine_args = args.engine_args;
+        size_t args_count = engine_args.size();
+
+        bool has_settings = args.storage_def->settings;
+
+        auto filelog_settings = std::make_unique<FileLogSettings>();
+        if (has_settings)
+        {
+            filelog_settings->loadFromQuery(*args.storage_def);
+        }
+
+        auto physical_cpu_cores = getNumberOfPhysicalCPUCores();
+        auto num_threads = filelog_settings->max_threads.value;
+
+        if (num_threads > physical_cpu_cores)
+        {
+            throw Exception(ErrorCodes::BAD_ARGUMENTS, "Number of threads to parse files can not be bigger than {}", physical_cpu_cores);
+        }
+        else if (num_threads < 1)
+        {
+            throw Exception("Number of threads to parse files can not be lower than 1", ErrorCodes::BAD_ARGUMENTS);
+        }
+
+        if (filelog_settings->max_block_size.changed && filelog_settings->max_block_size.value < 1)
+        {
+            throw Exception("filelog_max_block_size can not be lower than 1", ErrorCodes::BAD_ARGUMENTS);
+        }
+
+        if (filelog_settings->poll_max_batch_size.changed && filelog_settings->poll_max_batch_size.value < 1)
+        {
+            throw Exception("filelog_poll_max_batch_size can not be lower than 1", ErrorCodes::BAD_ARGUMENTS);
+        }
+
+        size_t init_sleep_time = filelog_settings->poll_directory_watch_events_backoff_init.totalMilliseconds();
+        size_t max_sleep_time = filelog_settings->poll_directory_watch_events_backoff_max.totalMilliseconds();
+        if (init_sleep_time > max_sleep_time)
+        {
+            throw Exception(
+                "poll_directory_watch_events_backoff_init can not be greater than poll_directory_watch_events_backoff_max",
+                ErrorCodes::BAD_ARGUMENTS);
+        }
+
+        if (filelog_settings->poll_directory_watch_events_backoff_factor.changed
+            && !filelog_settings->poll_directory_watch_events_backoff_factor.value)
+            throw Exception("poll_directory_watch_events_backoff_factor can not be 0", ErrorCodes::BAD_ARGUMENTS);
+
+        if (args_count != 2)
+            throw Exception(
+                "Arguments size of StorageFileLog should be 2, path and format name", ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH);
+
+        auto path_ast = evaluateConstantExpressionAsLiteral(engine_args[0], args.getContext());
+        auto format_ast = evaluateConstantExpressionAsLiteral(engine_args[1], args.getContext());
+
+        auto path = path_ast->as<ASTLiteral &>().value.safeGet<String>();
+        auto format = format_ast->as<ASTLiteral &>().value.safeGet<String>();
+
+        return StorageFileLog::create(
+            args.table_id,
+            args.getContext(),
+            args.columns,
+            path,
+            format,
+            std::move(filelog_settings),
+            args.comment,
+            args.attach);
+    };
+
+    factory.registerStorage(
+        "FileLog",
+        creator_fn,
+        StorageFactory::StorageFeatures{
+            .supports_settings = true,
+        });
+}
+
+bool StorageFileLog::updateFileInfos()
+{
+    if (!directory_watch)
+    {
+        /// For table just watch one file, we can not use directory monitor to watch it
+        if (!path_is_directory)
+        {
+            assert(file_infos.file_names.size() == file_infos.meta_by_inode.size());
+            assert(file_infos.file_names.size() == file_infos.context_by_name.size());
+            assert(file_infos.file_names.size() == 1);
+
+            if (auto it = file_infos.context_by_name.find(file_infos.file_names[0]); it != file_infos.context_by_name.end())
+            {
+                it->second.status = FileStatus::UPDATED;
+                return true;
+            }
+        }
+        return false;
+    }
+    /// Do not need to hold file_status lock, since it will be holded
+    /// by caller when call this function
+    auto error = directory_watch->getErrorAndReset();
+    if (error.has_error)
+        LOG_ERROR(log, "Error happened during watching directory {}: {}", directory_watch->getPath(), error.error_msg);
+
+    /// These file infos should always have same size(one for one) before update and after update
+    assert(file_infos.file_names.size() == file_infos.meta_by_inode.size());
+    assert(file_infos.file_names.size() == file_infos.context_by_name.size());
+
+    auto events = directory_watch->getEventsAndReset();
+
+    for (const auto & [file_name, event_infos] : events)
+    {
+        String file_path = getFullDataPath(file_name);
+        for (const auto & event_info : event_infos.file_events)
+        {
+            switch (event_info.type)
+            {
+                case DirectoryWatcherBase::DW_ITEM_ADDED:
+                {
+                    LOG_TRACE(log, "New event {} watched, file_name: {}", event_info.callback, file_name);
+                    /// Check if it is a regular file, and new file may be renamed or removed
+                    if (std::filesystem::is_regular_file(file_path))
+                    {
+                        auto inode = getInode(file_path);
+
+                        file_infos.file_names.push_back(file_name);
+
+                        if (auto it = file_infos.meta_by_inode.find(inode); it != file_infos.meta_by_inode.end())
+                            it->second = FileMeta{.file_name = file_name};
+                        else
+                            file_infos.meta_by_inode.emplace(inode, FileMeta{.file_name = file_name});
+
+                        if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())
+                            it->second = FileContext{.status = FileStatus::OPEN, .inode = inode};
+                        else
+                            file_infos.context_by_name.emplace(file_name, FileContext{.inode = inode});
+                    }
+                    break;
+                }
+
+                case DirectoryWatcherBase::DW_ITEM_MODIFIED:
+                {
+                    LOG_TRACE(log, "New event {} watched, file_name: {}", event_info.callback, file_name);
+                    /// When new file added and appended, it has two event: DW_ITEM_ADDED
+                    /// and DW_ITEM_MODIFIED, since the order of these two events in the
+                    /// sequence is uncentain, so we may can not find it in file_infos, just
+                    /// skip it, the file info will be handled in DW_ITEM_ADDED case.
+                    if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())
+                        it->second.status = FileStatus::UPDATED;
+                    break;
+                }
+
+                case DirectoryWatcherBase::DW_ITEM_REMOVED:
+                case DirectoryWatcherBase::DW_ITEM_MOVED_FROM:
+                {
+                    LOG_TRACE(log, "New event {} watched, file_name: {}", event_info.callback, file_name);
+                    if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())
+                        it->second.status = FileStatus::REMOVED;
+                    break;
+                }
+                case DirectoryWatcherBase::DW_ITEM_MOVED_TO:
+                {
+                    LOG_TRACE(log, "New event {} watched, file_name: {}", event_info.callback, file_name);
+
+                    /// Similar to DW_ITEM_ADDED, but if it removed from an old file
+                    /// should obtain old meta file and rename meta file
+                    if (std::filesystem::is_regular_file(file_path))
+                    {
+                        file_infos.file_names.push_back(file_name);
+                        auto inode = getInode(file_path);
+
+                        if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())
+                            it->second = FileContext{.inode = inode};
+                        else
+                            file_infos.context_by_name.emplace(file_name, FileContext{.inode = inode});
+
+                        /// File has been renamed, we should also rename meta file
+                        if (auto it = file_infos.meta_by_inode.find(inode); it != file_infos.meta_by_inode.end())
+                        {
+                            auto old_name = it->second.file_name;
+                            it->second.file_name = file_name;
+                            if (std::filesystem::exists(getFullMetaPath(old_name)))
+                                std::filesystem::rename(getFullMetaPath(old_name), getFullMetaPath(file_name));
+                        }
+                        /// May move from other place, adding new meta info
+                        else
+                            file_infos.meta_by_inode.emplace(inode, FileMeta{.file_name = file_name});
+                    }
+                }
+            }
+        }
+    }
+    std::vector<String> valid_files;
+
+    /// Remove file infos with REMOVE status
+    for (const auto & file_name : file_infos.file_names)
+    {
+        if (auto it = file_infos.context_by_name.find(file_name); it != file_infos.context_by_name.end())
+        {
+            if (it->second.status == FileStatus::REMOVED)
+            {
+                /// We need to check that this inode does not hold by other file(mv),
+                /// otherwise, we can not destroy it.
+                auto inode = it->second.inode;
+                /// If it's now hold by other file, than the file_name should has
+                /// been changed during updating file_infos
+                if (auto meta = file_infos.meta_by_inode.find(inode);
+                    meta != file_infos.meta_by_inode.end() && meta->second.file_name == file_name)
+                    file_infos.meta_by_inode.erase(meta);
+
+                if (std::filesystem::exists(getFullMetaPath(file_name)))
+                    std::filesystem::remove(getFullMetaPath(file_name));
+                file_infos.context_by_name.erase(it);
+            }
+            else
+            {
+                valid_files.push_back(file_name);
+            }
+        }
+    }
+    file_infos.file_names.swap(valid_files);
+
+    /// These file infos should always have same size(one for one)
+    assert(file_infos.file_names.size() == file_infos.meta_by_inode.size());
+    assert(file_infos.file_names.size() == file_infos.context_by_name.size());
+
+    return events.empty() || file_infos.file_names.empty();
+}
+
+NamesAndTypesList StorageFileLog::getVirtuals() const
+{
+    return NamesAndTypesList{{"_filename", std::make_shared<DataTypeString>()}, {"_offset", std::make_shared<DataTypeUInt64>()}};
+}
+
+Names StorageFileLog::getVirtualColumnNames()
+{
+    return {"_filename", "_offset"};
+}
+}
diff --git a/src/Storages/FileLog/StorageFileLog.h b/src/Storages/FileLog/StorageFileLog.h
new file mode 100644
index 000000000000..e1d95ae736fe
--- /dev/null
+++ b/src/Storages/FileLog/StorageFileLog.h
@@ -0,0 +1,215 @@
+#pragma once
+
+#include <Storages/FileLog/Buffer_fwd.h>
+#include <Storages/FileLog/FileLogDirectoryWatcher.h>
+#include <Storages/FileLog/FileLogSettings.h>
+
+#include <Core/BackgroundSchedulePool.h>
+#include <Storages/IStorage.h>
+#include <Common/SettingsChanges.h>
+
+#include <base/shared_ptr_helper.h>
+
+#include <atomic>
+#include <condition_variable>
+#include <filesystem>
+#include <fstream>
+#include <mutex>
+#include <optional>
+
+namespace DB
+{
+namespace ErrorCodes
+{
+    extern const int LOGICAL_ERROR;
+}
+
+class FileLogDirectoryWatcher;
+
+class StorageFileLog final : public shared_ptr_helper<StorageFileLog>, public IStorage, WithContext
+{
+    friend struct shared_ptr_helper<StorageFileLog>;
+
+public:
+
+    using Files = std::vector<String>;
+
+    std::string getName() const override { return "FileLog"; }
+
+    bool noPushingToViews() const override { return true; }
+
+    void startup() override;
+    void shutdown() override;
+
+    Pipe read(
+        const Names & column_names,
+        const StorageMetadataPtr & /*metadata_snapshot*/,
+        SelectQueryInfo & query_info,
+        ContextPtr context,
+        QueryProcessingStage::Enum processed_stage,
+        size_t max_block_size,
+        unsigned num_streams) override;
+
+    void drop() override;
+
+    /// We need to call drop() immediately to remove meta data directory,
+    /// otherwise, if another filelog table with same name created before
+    /// the table be dropped finally, then its meta data directory will
+    /// be deleted by this table drop finally
+    bool dropTableImmediately() override { return true; }
+
+    const auto & getFormatName() const { return format_name; }
+
+    enum class FileStatus
+    {
+        OPEN, /// first time open file after table start up
+        NO_CHANGE,
+        UPDATED,
+        REMOVED,
+    };
+
+    struct FileContext
+    {
+        FileStatus status = FileStatus::OPEN;
+        UInt64 inode{};
+        std::optional<std::ifstream> reader = std::nullopt;
+    };
+
+    struct FileMeta
+    {
+        String file_name;
+        UInt64 last_writen_position = 0;
+        UInt64 last_open_end = 0;
+    };
+
+    using InodeToFileMeta = std::unordered_map<UInt64, FileMeta>;
+    using FileNameToContext = std::unordered_map<String, FileContext>;
+
+    struct FileInfos
+    {
+        InodeToFileMeta meta_by_inode;
+        FileNameToContext context_by_name;
+        /// file names without path
+        Names file_names;
+    };
+
+    auto & getFileInfos() { return file_infos; }
+
+    String getFullMetaPath(const String & file_name) const { return std::filesystem::path(root_meta_path) / file_name; }
+    String getFullDataPath(const String & file_name) const { return std::filesystem::path(root_data_path) / file_name; }
+
+    NamesAndTypesList getVirtuals() const override;
+
+    static Names getVirtualColumnNames();
+
+    static UInt64 getInode(const String & file_name);
+
+    void openFilesAndSetPos();
+
+    /// Used in FileLogSource when finish generating all blocks.
+    /// Each stream responsible for close its files and store meta.
+    void closeFilesAndStoreMeta(size_t start, size_t end);
+
+    /// Used in FileLogSource after generating every block
+    void storeMetas(size_t start, size_t end);
+
+    static void assertStreamGood(const std::ifstream & reader);
+
+    template <typename K, typename V>
+    static V & findInMap(std::unordered_map<K, V> & map, const K & key)
+    {
+        if (auto it = map.find(key); it != map.end())
+            return it->second;
+        else
+            throw Exception(ErrorCodes::LOGICAL_ERROR, "The key {} doesn't exist.", key);
+    }
+
+    void increaseStreams();
+    void reduceStreams();
+
+    void wakeUp();
+
+    const auto & getFileLogSettings() const { return filelog_settings; }
+
+protected:
+    StorageFileLog(
+        const StorageID & table_id_,
+        ContextPtr context_,
+        const ColumnsDescription & columns_,
+        const String & path_,
+        const String & format_name_,
+        std::unique_ptr<FileLogSettings> settings,
+        const String & comment,
+        bool attach);
+
+private:
+    std::unique_ptr<FileLogSettings> filelog_settings;
+
+    const String path;
+    bool path_is_directory = true;
+
+    /// If path argument of the table is a regular file, it equals to user_files_path
+    /// otherwise, it equals to user_files_path/ + path_argument/, e.g. path
+    String root_data_path;
+    String root_meta_path;
+
+    FileInfos file_infos;
+
+    const String format_name;
+    Poco::Logger * log;
+
+    uint64_t milliseconds_to_wait;
+
+    /// In order to avoid data race, using a naive trick to forbid execute two select
+    /// simultaneously, although read is not useful in this engine. Using an atomic
+    /// variable to records current unfinishing streams, then if have unfinishing streams,
+    /// later select should forbid to execute.
+    std::atomic<int> running_streams = 0;
+
+    std::mutex mutex;
+    bool has_new_events = false;
+    std::condition_variable cv;
+
+    bool has_dependent_mv = false;
+
+    std::mutex file_infos_mutex;
+
+    struct TaskContext
+    {
+        BackgroundSchedulePool::TaskHolder holder;
+        std::atomic<bool> stream_cancelled {false};
+        explicit TaskContext(BackgroundSchedulePool::TaskHolder&& task_) : holder(std::move(task_))
+        {
+        }
+    };
+    std::shared_ptr<TaskContext> task;
+
+    std::unique_ptr<FileLogDirectoryWatcher> directory_watch = nullptr;
+
+    void loadFiles();
+
+    void loadMetaFiles(bool attach);
+
+    void threadFunc();
+
+    size_t getPollMaxBatchSize() const;
+    size_t getMaxBlockSize() const;
+    size_t getPollTimeoutMillisecond() const;
+
+    bool streamToViews();
+    bool checkDependencies(const StorageID & table_id);
+
+    bool updateFileInfos();
+
+    size_t getTableDependentCount() const;
+
+    /// Used in shutdown()
+    void serialize() const;
+    /// Used in FileSource closeFileAndStoreMeta(file_name);
+    void serialize(UInt64 inode, const FileMeta & file_meta) const;
+
+    void deserialize();
+    static void checkOffsetIsValid(const String & full_name, UInt64 offset);
+};
+
+}
diff --git a/src/Storages/IStorage.h b/src/Storages/IStorage.h
index 74e17442fe8e..701cf1275217 100644
--- a/src/Storages/IStorage.h
+++ b/src/Storages/IStorage.h
@@ -585,6 +585,10 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo
     /// Does not takes underlying Storage (if any) into account.
     virtual std::optional<UInt64> lifetimeBytes() const { return {}; }
 
+    /// Should table->drop be called at once or with delay (in case of atomic database engine).
+    /// Needed for integration engines, when there must be no delay for calling drop() method.
+    virtual bool dropTableImmediately() { return false; }
+
 private:
     /// Lock required for alter queries (lockForAlter). Always taken for write
     /// (actually can be replaced with std::mutex, but for consistency we use
diff --git a/src/Storages/System/StorageSystemBuildOptions.generated.cpp.in b/src/Storages/System/StorageSystemBuildOptions.generated.cpp.in
index 6bb97355151d..e087475695c1 100644
--- a/src/Storages/System/StorageSystemBuildOptions.generated.cpp.in
+++ b/src/Storages/System/StorageSystemBuildOptions.generated.cpp.in
@@ -50,6 +50,7 @@ const char * auto_config_build[]
     "USE_LDAP", "@USE_LDAP@",
     "TZDATA_VERSION", "@TZDATA_VERSION@",
     "USE_KRB5", "@USE_KRB5@",
+    "USE_FILELOG", "@USE_FILELOG@",
     "USE_BZIP2", "@USE_BZIP2@",
 
     nullptr, nullptr
diff --git a/src/Storages/registerStorages.cpp b/src/Storages/registerStorages.cpp
index af2e47328032..9f6c18f53d3e 100644
--- a/src/Storages/registerStorages.cpp
+++ b/src/Storages/registerStorages.cpp
@@ -68,6 +68,10 @@ void registerStorageMaterializedPostgreSQL(StorageFactory & factory);
 void registerStorageExternalDistributed(StorageFactory & factory);
 #endif
 
+#if USE_FILELOG
+void registerStorageFileLog(StorageFactory & factory);
+#endif
+
 #if USE_SQLITE
 void registerStorageSQLite(StorageFactory & factory);
 #endif
@@ -119,7 +123,11 @@ void registerStorages()
     registerStorageKafka(factory);
     #endif
 
-    #if USE_AMQPCPP
+#if USE_FILELOG
+    registerStorageFileLog(factory);
+#endif
+
+#if USE_AMQPCPP
     registerStorageRabbitMQ(factory);
     #endif
 
