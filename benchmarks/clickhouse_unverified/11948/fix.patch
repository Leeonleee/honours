diff --git a/src/Common/FileSyncGuard.h b/src/Common/FileSyncGuard.h
new file mode 100644
index 000000000000..6451f6ebf36b
--- /dev/null
+++ b/src/Common/FileSyncGuard.h
@@ -0,0 +1,41 @@
+#pragma once
+
+#include <Disks/IDisk.h>
+
+namespace DB
+{
+
+/// Helper class, that recieves file descriptor and does fsync for it in destructor.
+/// It's used to keep descriptor open, while doing some operations with it, and do fsync at the end.
+/// Guaranties of sequence 'close-reopen-fsync' may depend on kernel version.
+/// Source: linux-fsdevel mailing-list https://marc.info/?l=linux-fsdevel&m=152535409207496
+class FileSyncGuard
+{
+public:
+    /// NOTE: If you have already opened descriptor, it's preffered to use
+    /// this constructor instead of construnctor with path.
+    FileSyncGuard(const DiskPtr & disk_, int fd_) : disk(disk_), fd(fd_) {}
+
+    FileSyncGuard(const DiskPtr & disk_, const String & path)
+        : disk(disk_), fd(disk_->open(path, O_RDWR)) {}
+
+    ~FileSyncGuard()
+    {
+        try
+        {
+            disk->sync(fd);
+            disk->close(fd);
+        }
+        catch (...)
+        {
+            tryLogCurrentException(__PRETTY_FUNCTION__);
+        }
+    }
+
+private:
+    DiskPtr disk;
+    int fd = -1;
+};
+
+}
+
diff --git a/src/Disks/DiskDecorator.cpp b/src/Disks/DiskDecorator.cpp
index e55534e347fa..7f2ea58d7cfe 100644
--- a/src/Disks/DiskDecorator.cpp
+++ b/src/Disks/DiskDecorator.cpp
@@ -165,4 +165,19 @@ void DiskDecorator::truncateFile(const String & path, size_t size)
     delegate->truncateFile(path, size);
 }
 
+int DiskDecorator::open(const String & path, mode_t mode) const
+{
+    return delegate->open(path, mode);
+}
+
+void DiskDecorator::close(int fd) const
+{
+    delegate->close(fd);
+}
+
+void DiskDecorator::sync(int fd) const
+{
+    delegate->sync(fd);
+}
+
 }
diff --git a/src/Disks/DiskDecorator.h b/src/Disks/DiskDecorator.h
index 71bb100c576f..f1ddfff49520 100644
--- a/src/Disks/DiskDecorator.h
+++ b/src/Disks/DiskDecorator.h
@@ -42,6 +42,9 @@ class DiskDecorator : public IDisk
     void setReadOnly(const String & path) override;
     void createHardLink(const String & src_path, const String & dst_path) override;
     void truncateFile(const String & path, size_t size) override;
+    int open(const String & path, mode_t mode) const override;
+    void close(int fd) const override;
+    void sync(int fd) const override;
     const String getType() const override { return delegate->getType(); }
 
 protected:
diff --git a/src/Disks/DiskLocal.cpp b/src/Disks/DiskLocal.cpp
index f9e988211da2..a09ab7c5ac5f 100644
--- a/src/Disks/DiskLocal.cpp
+++ b/src/Disks/DiskLocal.cpp
@@ -8,7 +8,7 @@
 
 #include <IO/createReadBufferFromFileBase.h>
 #include <IO/createWriteBufferFromFileBase.h>
-
+#include <unistd.h>
 
 namespace DB
 {
@@ -19,6 +19,10 @@ namespace ErrorCodes
     extern const int EXCESSIVE_ELEMENT_IN_CONFIG;
     extern const int PATH_ACCESS_DENIED;
     extern const int INCORRECT_DISK_INDEX;
+    extern const int FILE_DOESNT_EXIST;
+    extern const int CANNOT_OPEN_FILE;
+    extern const int CANNOT_FSYNC;
+    extern const int CANNOT_CLOSE_FILE;
     extern const int CANNOT_TRUNCATE_FILE;
 }
 
@@ -292,6 +296,28 @@ void DiskLocal::copy(const String & from_path, const std::shared_ptr<IDisk> & to
         IDisk::copy(from_path, to_disk, to_path); /// Copy files through buffers.
 }
 
+int DiskLocal::open(const String & path, mode_t mode) const
+{
+    String full_path = disk_path + path;
+    int fd = ::open(full_path.c_str(), mode);
+    if (-1 == fd)
+        throwFromErrnoWithPath("Cannot open file " + full_path, full_path,
+                        errno == ENOENT ? ErrorCodes::FILE_DOESNT_EXIST : ErrorCodes::CANNOT_OPEN_FILE);
+    return fd;
+}
+
+void DiskLocal::close(int fd) const
+{
+    if (-1 == ::close(fd))
+        throw Exception("Cannot close file", ErrorCodes::CANNOT_CLOSE_FILE);
+}
+
+void DiskLocal::sync(int fd) const
+{
+    if (-1 == ::fsync(fd))
+        throw Exception("Cannot fsync", ErrorCodes::CANNOT_FSYNC);
+}
+
 DiskPtr DiskLocalReservation::getDisk(size_t i) const
 {
     if (i != 0)
diff --git a/src/Disks/DiskLocal.h b/src/Disks/DiskLocal.h
index 71c4dc0aec9b..762a8502faae 100644
--- a/src/Disks/DiskLocal.h
+++ b/src/Disks/DiskLocal.h
@@ -99,6 +99,10 @@ class DiskLocal : public IDisk
 
     void createHardLink(const String & src_path, const String & dst_path) override;
 
+    int open(const String & path, mode_t mode) const override;
+    void close(int fd) const override;
+    void sync(int fd) const override;
+
     void truncateFile(const String & path, size_t size) override;
 
     const String getType() const override { return "local"; }
diff --git a/src/Disks/DiskMemory.cpp b/src/Disks/DiskMemory.cpp
index 96d9e22c414f..d185263d48c9 100644
--- a/src/Disks/DiskMemory.cpp
+++ b/src/Disks/DiskMemory.cpp
@@ -408,6 +408,21 @@ void DiskMemory::setReadOnly(const String &)
     throw Exception("Method setReadOnly is not implemented for memory disks", ErrorCodes::NOT_IMPLEMENTED);
 }
 
+int DiskMemory::open(const String & /*path*/, mode_t /*mode*/) const
+{
+    throw Exception("Method open is not implemented for memory disks", ErrorCodes::NOT_IMPLEMENTED);
+}
+
+void DiskMemory::close(int /*fd*/) const
+{
+    throw Exception("Method close is not implemented for memory disks", ErrorCodes::NOT_IMPLEMENTED);
+}
+
+void DiskMemory::sync(int /*fd*/) const
+{
+    throw Exception("Method sync is not implemented for memory disks", ErrorCodes::NOT_IMPLEMENTED);
+}
+
 void DiskMemory::truncateFile(const String & path, size_t size)
 {
     std::lock_guard lock(mutex);
diff --git a/src/Disks/DiskMemory.h b/src/Disks/DiskMemory.h
index fc265ddef031..4d4b947098b2 100644
--- a/src/Disks/DiskMemory.h
+++ b/src/Disks/DiskMemory.h
@@ -90,6 +90,10 @@ class DiskMemory : public IDisk
 
     void createHardLink(const String & src_path, const String & dst_path) override;
 
+    int open(const String & path, mode_t mode) const override;
+    void close(int fd) const override;
+    void sync(int fd) const override;
+
     void truncateFile(const String & path, size_t size) override;
 
     const String getType() const override { return "memory"; }
diff --git a/src/Disks/IDisk.h b/src/Disks/IDisk.h
index 17de6db3487b..47387fb370a8 100644
--- a/src/Disks/IDisk.h
+++ b/src/Disks/IDisk.h
@@ -177,6 +177,15 @@ class IDisk : public Space
     /// Create hardlink from `src_path` to `dst_path`.
     virtual void createHardLink(const String & src_path, const String & dst_path) = 0;
 
+    /// Wrapper for POSIX open
+    virtual int open(const String & path, mode_t mode) const = 0;
+
+    /// Wrapper for POSIX close
+    virtual void close(int fd) const = 0;
+
+    /// Wrapper for POSIX fsync
+    virtual void sync(int fd) const = 0;
+
     /// Truncate file to specified size.
     virtual void truncateFile(const String & path, size_t size);
 
diff --git a/src/Disks/S3/DiskS3.cpp b/src/Disks/S3/DiskS3.cpp
index 3dcb55c2c441..6abb72efeb05 100644
--- a/src/Disks/S3/DiskS3.cpp
+++ b/src/Disks/S3/DiskS3.cpp
@@ -33,6 +33,7 @@ namespace ErrorCodes
     extern const int CANNOT_SEEK_THROUGH_FILE;
     extern const int UNKNOWN_FORMAT;
     extern const int INCORRECT_DISK_INDEX;
+    extern const int NOT_IMPLEMENTED;
 }
 
 namespace
@@ -746,6 +747,21 @@ void DiskS3::setReadOnly(const String & path)
     Poco::File(metadata_path + path).setReadOnly(true);
 }
 
+int DiskS3::open(const String & /*path*/, mode_t /*mode*/) const
+{
+    throw Exception("Method open is not implemented for S3 disks", ErrorCodes::NOT_IMPLEMENTED);
+}
+
+void DiskS3::close(int /*fd*/) const
+{
+    throw Exception("Method close is not implemented for S3 disks", ErrorCodes::NOT_IMPLEMENTED);
+}
+
+void DiskS3::sync(int /*fd*/) const
+{
+    throw Exception("Method sync is not implemented for S3 disks", ErrorCodes::NOT_IMPLEMENTED);
+}
+
 void DiskS3::shutdown()
 {
     /// This call stops any next retry attempts for ongoing S3 requests.
diff --git a/src/Disks/S3/DiskS3.h b/src/Disks/S3/DiskS3.h
index db352feb0635..2d9c7f798652 100644
--- a/src/Disks/S3/DiskS3.h
+++ b/src/Disks/S3/DiskS3.h
@@ -100,6 +100,10 @@ class DiskS3 : public IDisk
 
     void setReadOnly(const String & path) override;
 
+    int open(const String & path, mode_t mode) const override;
+    void close(int fd) const override;
+    void sync(int fd) const override;
+
     const String getType() const override { return "s3"; }
 
     void shutdown() override;
diff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp
index e4bdedc17f80..0e42d2677290 100644
--- a/src/Storages/MergeTree/DataPartsExchange.cpp
+++ b/src/Storages/MergeTree/DataPartsExchange.cpp
@@ -5,6 +5,7 @@
 #include <Disks/SingleDiskVolume.h>
 #include <Common/CurrentMetrics.h>
 #include <Common/NetException.h>
+#include <Common/FileSyncGuard.h>
 #include <DataStreams/NativeBlockOutputStream.h>
 #include <IO/HTTPCommon.h>
 #include <ext/scope_guard.h>
@@ -263,9 +264,9 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchPart(
     int server_protocol_version = parse<int>(in.getResponseCookie("server_protocol_version", "0"));
 
     ReservationPtr reservation;
+    size_t sum_files_size = 0;
     if (server_protocol_version >= REPLICATION_PROTOCOL_VERSION_WITH_PARTS_SIZE)
     {
-        size_t sum_files_size;
         readBinary(sum_files_size, in);
         if (server_protocol_version >= REPLICATION_PROTOCOL_VERSION_WITH_PARTS_SIZE_AND_TTL_INFOS)
         {
@@ -286,12 +287,15 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchPart(
         reservation = data.makeEmptyReservationOnLargestDisk();
     }
 
+    bool sync = (data_settings->min_compressed_bytes_to_fsync_after_fetch
+                    && sum_files_size >= data_settings->min_compressed_bytes_to_fsync_after_fetch);
+
     String part_type = "Wide";
     if (server_protocol_version >= REPLICATION_PROTOCOL_VERSION_WITH_PARTS_TYPE)
         readStringBinary(part_type, in);
 
     return part_type == "InMemory" ? downloadPartToMemory(part_name, metadata_snapshot, std::move(reservation), in)
-        : downloadPartToDisk(part_name, replica_path, to_detached, tmp_prefix_, std::move(reservation), in);
+        : downloadPartToDisk(part_name, replica_path, to_detached, tmp_prefix_, sync, std::move(reservation), in);
 }
 
 MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToMemory(
@@ -330,6 +334,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToDisk(
     const String & replica_path,
     bool to_detached,
     const String & tmp_prefix_,
+    bool sync,
     const ReservationPtr reservation,
     PooledReadWriteBufferFromHTTP & in)
 {
@@ -351,6 +356,10 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToDisk(
 
     disk->createDirectories(part_download_path);
 
+    std::optional<FileSyncGuard> sync_guard;
+    if (data.getSettings()->fsync_part_directory)
+        sync_guard.emplace(disk, part_download_path);
+
     MergeTreeData::DataPart::Checksums checksums;
     for (size_t i = 0; i < files; ++i)
     {
@@ -392,6 +401,9 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToDisk(
             file_name != "columns.txt" &&
             file_name != IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME)
             checksums.addFile(file_name, file_size, expected_hash);
+
+        if (sync)
+            hashing_out.sync();
     }
 
     assertEOF(in);
diff --git a/src/Storages/MergeTree/DataPartsExchange.h b/src/Storages/MergeTree/DataPartsExchange.h
index b1db64596399..52a34a2239a2 100644
--- a/src/Storages/MergeTree/DataPartsExchange.h
+++ b/src/Storages/MergeTree/DataPartsExchange.h
@@ -74,6 +74,7 @@ class Fetcher final
             const String & replica_path,
             bool to_detached,
             const String & tmp_prefix_,
+            bool sync,
             const ReservationPtr reservation,
             PooledReadWriteBufferFromHTTP & in);
 
diff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp
index 872e34adb830..486e444763d2 100644
--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp
+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp
@@ -11,6 +11,7 @@
 #include <Storages/MergeTree/checkDataPart.h>
 #include <Common/StringUtils/StringUtils.h>
 #include <Common/escapeForFileName.h>
+#include <Common/FileSyncGuard.h>
 #include <common/JSON.h>
 #include <common/logger_useful.h>
 #include <Compression/getCompressionCodecForFile.h>
@@ -778,6 +779,10 @@ void IMergeTreeDataPart::renameTo(const String & new_relative_path, bool remove_
     String from = getFullRelativePath();
     String to = storage.relative_data_path + new_relative_path + "/";
 
+    std::optional<FileSyncGuard> sync_guard;
+    if (storage.getSettings()->fsync_part_directory)
+        sync_guard.emplace(volume->getDisk(), to);
+
     if (!volume->getDisk()->exists(from))
         throw Exception("Part directory " + fullPath(volume->getDisk(), from) + " doesn't exist. Most likely it is logical error.", ErrorCodes::FILE_DOESNT_EXIST);
 
diff --git a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h
index 959d6af9ed2f..4a42a58a65b3 100644
--- a/src/Storages/MergeTree/IMergeTreeDataPartWriter.h
+++ b/src/Storages/MergeTree/IMergeTreeDataPartWriter.h
@@ -51,9 +51,9 @@ class IMergeTreeDataPartWriter : private boost::noncopyable
     virtual void initSkipIndices() {}
     virtual void initPrimaryIndex() {}
 
-    virtual void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums) = 0;
-    virtual void finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & /* checksums */) {}
-    virtual void finishSkipIndicesSerialization(MergeTreeData::DataPart::Checksums & /* checksums */) {}
+    virtual void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync) = 0;
+    virtual void finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & /* checksums */, bool /* sync */) {}
+    virtual void finishSkipIndicesSerialization(MergeTreeData::DataPart::Checksums & /* checksums */, bool /* sync */) {}
 
     Columns releaseIndexColumns();
     const MergeTreeIndexGranularity & getIndexGranularity() const { return index_granularity; }
diff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp
index 03da0033f9dc..5969ecc5baf7 100644
--- a/src/Storages/MergeTree/MergeTreeData.cpp
+++ b/src/Storages/MergeTree/MergeTreeData.cpp
@@ -221,6 +221,8 @@ MergeTreeData::MergeTreeData(
         format_version = min_format_version;
         auto buf = version_file.second->writeFile(version_file.first);
         writeIntText(format_version.toUnderType(), *buf);
+        if (global_context.getSettingsRef().fsync_metadata)
+            buf->sync();
     }
     else
     {
diff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp
index a8f7e265f68e..35ab90a80838 100644
--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp
@@ -29,6 +29,7 @@
 #include <Common/interpolate.h>
 #include <Common/typeid_cast.h>
 #include <Common/escapeForFileName.h>
+#include <Common/FileSyncGuard.h>
 #include <Parsers/queryToString.h>
 
 #include <cmath>
@@ -613,6 +614,13 @@ class MergeProgressCallback
     }
 };
 
+static bool needSyncPart(const size_t input_rows, size_t input_bytes, const MergeTreeSettings & settings)
+{
+    return ((settings.min_rows_to_fsync_after_merge && input_rows >= settings.min_rows_to_fsync_after_merge)
+        || (settings.min_compressed_bytes_to_fsync_after_merge && input_bytes >= settings.min_compressed_bytes_to_fsync_after_merge));
+}
+
+
 /// parts should be sorted.
 MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTemporaryPart(
     const FutureMergedMutatedPart & future_part,
@@ -698,6 +706,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
     }
 
     size_t sum_input_rows_upper_bound = merge_entry->total_rows_count;
+    size_t sum_compressed_bytes_upper_bound = merge_entry->total_size_bytes_compressed;
     MergeAlgorithm merge_alg = chooseMergeAlgorithm(parts, sum_input_rows_upper_bound, gathering_columns, deduplicate, need_remove_expired_values);
 
     LOG_DEBUG(log, "Selected MergeAlgorithm: {}", ((merge_alg == MergeAlgorithm::Vertical) ? "Vertical" : "Horizontal"));
@@ -735,6 +744,10 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
         gathering_column_names.clear();
     }
 
+    std::optional<FileSyncGuard> sync_guard;
+    if (data.getSettings()->fsync_part_directory)
+        sync_guard.emplace(disk, new_part_tmp_path);
+
     /** Read from all parts, merge and write into a new one.
       * In passing, we calculate expression for sorting.
       */
@@ -858,7 +871,6 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
     if (need_remove_expired_values)
         merged_stream = std::make_shared<TTLBlockInputStream>(merged_stream, data, metadata_snapshot, new_data_part, time_of_merge, force_ttl);
 
-
     if (metadata_snapshot->hasSecondaryIndices())
     {
         const auto & indices = metadata_snapshot->getSecondaryIndices();
@@ -918,6 +930,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
     if (need_remove_expired_values && ttl_merges_blocker.isCancelled())
         throw Exception("Cancelled merging parts with expired TTL", ErrorCodes::ABORTED);
 
+    bool need_sync = needSyncPart(sum_input_rows_upper_bound, sum_compressed_bytes_upper_bound, *data_settings);
     MergeTreeData::DataPart::Checksums checksums_gathered_columns;
 
     /// Gather ordinary columns
@@ -1001,7 +1014,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
                 throw Exception("Cancelled merging parts", ErrorCodes::ABORTED);
 
             column_gathered_stream.readSuffix();
-            auto changed_checksums = column_to.writeSuffixAndGetChecksums(new_data_part, checksums_gathered_columns);
+            auto changed_checksums = column_to.writeSuffixAndGetChecksums(new_data_part, checksums_gathered_columns, need_sync);
             checksums_gathered_columns.add(std::move(changed_checksums));
 
             if (rows_written != column_elems_written)
@@ -1038,9 +1051,9 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mergePartsToTempor
     }
 
     if (merge_alg != MergeAlgorithm::Vertical)
-        to.writeSuffixAndFinalizePart(new_data_part);
+        to.writeSuffixAndFinalizePart(new_data_part, need_sync);
     else
-        to.writeSuffixAndFinalizePart(new_data_part, &storage_columns, &checksums_gathered_columns);
+        to.writeSuffixAndFinalizePart(new_data_part, need_sync, &storage_columns, &checksums_gathered_columns);
 
     return new_data_part;
 }
@@ -1130,10 +1143,14 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mutatePartToTempor
 
     disk->createDirectories(new_part_tmp_path);
 
+    std::optional<FileSyncGuard> sync_guard;
+    if (data.getSettings()->fsync_part_directory)
+        sync_guard.emplace(disk, new_part_tmp_path);
+
     /// Don't change granularity type while mutating subset of columns
     auto mrk_extension = source_part->index_granularity_info.is_adaptive ? getAdaptiveMrkExtension(new_data_part->getType())
                                                                          : getNonAdaptiveMrkExtension();
-
+    bool need_sync = needSyncPart(source_part->rows_count, source_part->getBytesOnDisk(), *data_settings);
     bool need_remove_expired_values = false;
 
     if (in && shouldExecuteTTL(metadata_snapshot, in->getHeader().getNamesAndTypesList().getNames(), commands_for_part))
@@ -1158,7 +1175,8 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mutatePartToTempor
             time_of_mutation,
             compression_codec,
             merge_entry,
-            need_remove_expired_values);
+            need_remove_expired_values,
+            need_sync);
 
         /// no finalization required, because mutateAllPartColumns use
         /// MergedBlockOutputStream which finilaze all part fields itself
@@ -1214,7 +1232,8 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataMergerMutator::mutatePartToTempor
                 time_of_mutation,
                 compression_codec,
                 merge_entry,
-                need_remove_expired_values);
+                need_remove_expired_values,
+                need_sync);
         }
 
         for (const auto & [rename_from, rename_to] : files_to_rename)
@@ -1695,7 +1714,8 @@ void MergeTreeDataMergerMutator::mutateAllPartColumns(
     time_t time_of_mutation,
     const CompressionCodecPtr & compression_codec,
     MergeListEntry & merge_entry,
-    bool need_remove_expired_values) const
+    bool need_remove_expired_values,
+    bool need_sync) const
 {
     if (mutating_stream == nullptr)
         throw Exception("Cannot mutate part columns with uninitialized mutations stream. It's a bug", ErrorCodes::LOGICAL_ERROR);
@@ -1731,7 +1751,7 @@ void MergeTreeDataMergerMutator::mutateAllPartColumns(
 
     new_data_part->minmax_idx = std::move(minmax_idx);
     mutating_stream->readSuffix();
-    out.writeSuffixAndFinalizePart(new_data_part);
+    out.writeSuffixAndFinalizePart(new_data_part, need_sync);
 }
 
 void MergeTreeDataMergerMutator::mutateSomePartColumns(
@@ -1744,7 +1764,8 @@ void MergeTreeDataMergerMutator::mutateSomePartColumns(
     time_t time_of_mutation,
     const CompressionCodecPtr & compression_codec,
     MergeListEntry & merge_entry,
-    bool need_remove_expired_values) const
+    bool need_remove_expired_values,
+    bool need_sync) const
 {
     if (mutating_stream == nullptr)
         throw Exception("Cannot mutate part columns with uninitialized mutations stream. It's a bug", ErrorCodes::LOGICAL_ERROR);
@@ -1778,10 +1799,9 @@ void MergeTreeDataMergerMutator::mutateSomePartColumns(
 
     mutating_stream->readSuffix();
 
-    auto changed_checksums = out.writeSuffixAndGetChecksums(new_data_part, new_data_part->checksums);
+    auto changed_checksums = out.writeSuffixAndGetChecksums(new_data_part, new_data_part->checksums, need_sync);
 
     new_data_part->checksums.add(std::move(changed_checksums));
-
 }
 
 void MergeTreeDataMergerMutator::finalizeMutatedPart(
diff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.h b/src/Storages/MergeTree/MergeTreeDataMergerMutator.h
index 96ab14ba57bf..bfbaccd1b1ea 100644
--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.h
+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.h
@@ -196,7 +196,8 @@ class MergeTreeDataMergerMutator
         time_t time_of_mutation,
         const CompressionCodecPtr & codec,
         MergeListEntry & merge_entry,
-        bool need_remove_expired_values) const;
+        bool need_remove_expired_values,
+        bool need_sync) const;
 
     /// Mutate some columns of source part with mutation_stream
     void mutateSomePartColumns(
@@ -209,7 +210,8 @@ class MergeTreeDataMergerMutator
         time_t time_of_mutation,
         const CompressionCodecPtr & codec,
         MergeListEntry & merge_entry,
-        bool need_remove_expired_values) const;
+        bool need_remove_expired_values,
+        bool need_sync) const;
 
     /// Initialize and write to disk new part fields like checksums, columns,
     /// etc.
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp
index 9c3325d3d5a3..235c76e744b9 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp
@@ -160,7 +160,7 @@ void MergeTreeDataPartWriterCompact::writeColumnSingleGranule(
     column.type->serializeBinaryBulkStateSuffix(serialize_settings, state);
 }
 
-void MergeTreeDataPartWriterCompact::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums)
+void MergeTreeDataPartWriterCompact::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync)
 {
     if (columns_buffer.size() != 0)
         writeBlock(header.cloneWithColumns(columns_buffer.releaseColumns()));
@@ -184,6 +184,12 @@ void MergeTreeDataPartWriterCompact::finishDataSerialization(IMergeTreeDataPart:
     plain_file->next();
     marks.next();
     addToChecksums(checksums);
+
+    if (sync)
+    {
+        plain_file->sync();
+        marks_file->sync();
+    }
 }
 
 static void fillIndexGranularityImpl(
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h
index fecf5ce40e86..4beb0dba340a 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h
@@ -20,7 +20,7 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk
     void write(const Block & block, const IColumn::Permutation * permutation,
         const Block & primary_key_block, const Block & skip_indexes_block) override;
 
-    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums) override;
+    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync) override;
 
 protected:
     void fillIndexGranularity(size_t index_granularity_for_block, size_t rows_in_block) override;
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp
index a74861587373..f0738a1130a4 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.cpp
@@ -70,7 +70,7 @@ void MergeTreeDataPartWriterInMemory::calculateAndSerializePrimaryIndex(const Bl
     }
 }
 
-void MergeTreeDataPartWriterInMemory::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums)
+void MergeTreeDataPartWriterInMemory::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool)
 {
     /// If part is empty we still need to initialize block by empty columns.
     if (!part_in_memory->block)
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h
index 92e4228a90db..6e59cdd08a93 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterInMemory.h
@@ -18,7 +18,7 @@ class MergeTreeDataPartWriterInMemory : public IMergeTreeDataPartWriter
     void write(const Block & block, const IColumn::Permutation * permutation,
         const Block & primary_key_block, const Block & skip_indexes_block) override;
 
-    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums) override;
+    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync) override;
 
     void calculateAndSerializePrimaryIndex(const Block & primary_index_block) override;
 
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp
index d773e76f7b3f..8295b881d874 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.cpp
@@ -308,7 +308,8 @@ void MergeTreeDataPartWriterOnDisk::calculateAndSerializeSkipIndices(const Block
     skip_index_data_mark = skip_index_current_data_mark;
 }
 
-void MergeTreeDataPartWriterOnDisk::finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & checksums)
+void MergeTreeDataPartWriterOnDisk::finishPrimaryIndexSerialization(
+        MergeTreeData::DataPart::Checksums & checksums, bool sync)
 {
     bool write_final_mark = (with_final_mark && data_written);
     if (write_final_mark && compute_granularity)
@@ -330,12 +331,14 @@ void MergeTreeDataPartWriterOnDisk::finishPrimaryIndexSerialization(MergeTreeDat
         index_stream->next();
         checksums.files["primary.idx"].file_size = index_stream->count();
         checksums.files["primary.idx"].file_hash = index_stream->getHash();
+        if (sync)
+            index_file_stream->sync();
         index_stream = nullptr;
     }
 }
 
 void MergeTreeDataPartWriterOnDisk::finishSkipIndicesSerialization(
-        MergeTreeData::DataPart::Checksums & checksums)
+        MergeTreeData::DataPart::Checksums & checksums, bool sync)
 {
     for (size_t i = 0; i < skip_indices.size(); ++i)
     {
@@ -348,6 +351,8 @@ void MergeTreeDataPartWriterOnDisk::finishSkipIndicesSerialization(
     {
         stream->finalize();
         stream->addToChecksums(checksums);
+        if (sync)
+            stream->sync();
     }
 
     skip_indices_streams.clear();
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h
index 8598a5143cb9..f4bd900977f3 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterOnDisk.h
@@ -81,8 +81,8 @@ class MergeTreeDataPartWriterOnDisk : public IMergeTreeDataPartWriter
     void initSkipIndices() final;
     void initPrimaryIndex() final;
 
-    void finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & checksums) final;
-    void finishSkipIndicesSerialization(MergeTreeData::DataPart::Checksums & checksums) final;
+    void finishPrimaryIndexSerialization(MergeTreeData::DataPart::Checksums & checksums, bool sync) final;
+    void finishSkipIndicesSerialization(MergeTreeData::DataPart::Checksums & checksums, bool sync) final;
 
     void setWrittenOffsetColumns(WrittenOffsetColumns * written_offset_columns_)
     {
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp
index 835139345d4d..a558c57b5f0b 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp
@@ -265,7 +265,7 @@ void MergeTreeDataPartWriterWide::writeColumn(
     next_index_offset = current_row - total_rows;
 }
 
-void MergeTreeDataPartWriterWide::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums)
+void MergeTreeDataPartWriterWide::finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync)
 {
     const auto & global_settings = storage.global_context.getSettingsRef();
     IDataType::SerializeBinaryBulkSettings serialize_settings;
@@ -296,6 +296,8 @@ void MergeTreeDataPartWriterWide::finishDataSerialization(IMergeTreeDataPart::Ch
     {
         stream.second->finalize();
         stream.second->addToChecksums(checksums);
+        if (sync)
+            stream.second->sync();
     }
 
     column_streams.clear();
diff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h
index ab40f1c7d2a3..02ab2a7ca567 100644
--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h
+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h
@@ -23,7 +23,7 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk
     void write(const Block & block, const IColumn::Permutation * permutation,
         const Block & primary_key_block, const Block & skip_indexes_block) override;
 
-    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums) override;
+    void finishDataSerialization(IMergeTreeDataPart::Checksums & checksums, bool sync) override;
 
     IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenOffsetColumns & offset_columns);
 
diff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp
index 1d7bf545009b..e5b684a13619 100644
--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp
+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp
@@ -12,6 +12,7 @@
 #include <IO/WriteHelpers.h>
 #include <Poco/File.h>
 #include <Common/typeid_cast.h>
+#include <Common/FileSyncGuard.h>
 
 #include <Parsers/queryToString.h>
 
@@ -251,6 +252,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataWriter::writeTempPart(BlockWithPa
     new_data_part->minmax_idx = std::move(minmax_idx);
     new_data_part->is_temp = true;
 
+    std::optional<FileSyncGuard> sync_guard;
     if (new_data_part->isStoredOnDisk())
     {
         /// The name could be non-unique in case of stale files from previous runs.
@@ -262,7 +264,11 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataWriter::writeTempPart(BlockWithPa
             new_data_part->volume->getDisk()->removeRecursive(full_path);
         }
 
-        new_data_part->volume->getDisk()->createDirectories(full_path);
+        const auto disk = new_data_part->volume->getDisk();
+        disk->createDirectories(full_path);
+
+        if (data.getSettings()->fsync_part_directory)
+            sync_guard.emplace(disk, full_path);
     }
 
     /// If we need to calculate some columns to sort.
@@ -311,10 +317,11 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataWriter::writeTempPart(BlockWithPa
 
     const auto & index_factory = MergeTreeIndexFactory::instance();
     MergedBlockOutputStream out(new_data_part, metadata_snapshot, columns, index_factory.getMany(metadata_snapshot->getSecondaryIndices()), compression_codec);
+    bool sync_on_insert = data.getSettings()->fsync_after_insert;
 
     out.writePrefix();
     out.writeWithPermutation(block, perm_ptr);
-    out.writeSuffixAndFinalizePart(new_data_part);
+    out.writeSuffixAndFinalizePart(new_data_part, sync_on_insert);
 
     ProfileEvents::increment(ProfileEvents::MergeTreeDataWriterRows, block.rows());
     ProfileEvents::increment(ProfileEvents::MergeTreeDataWriterUncompressedBytes, block.bytes());
diff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h
index 49847617d30b..8652a6ef6918 100644
--- a/src/Storages/MergeTree/MergeTreeSettings.h
+++ b/src/Storages/MergeTree/MergeTreeSettings.h
@@ -40,6 +40,14 @@ struct Settings;
     M(Seconds, old_parts_lifetime, 8 * 60, "How many seconds to keep obsolete parts.", 0) \
     M(Seconds, temporary_directories_lifetime, 86400, "How many seconds to keep tmp_-directories.", 0) \
     M(Seconds, lock_acquire_timeout_for_background_operations, DBMS_DEFAULT_LOCK_ACQUIRE_TIMEOUT_SEC, "For background operations like merges, mutations etc. How many seconds before failing to acquire table locks.", 0) \
+    M(UInt64, min_rows_to_fsync_after_merge, 0, "Minimal number of rows to do fsync for part after merge (0 - disabled)", 0) \
+    M(UInt64, min_compressed_bytes_to_fsync_after_merge, 0, "Minimal number of compressed bytes to do fsync for part after merge (0 - disabled)", 0) \
+    M(UInt64, min_compressed_bytes_to_fsync_after_fetch, 0, "Minimal number of compressed bytes to do fsync for part after fetch (0 - disabled)", 0) \
+    M(Bool, fsync_after_insert, false, "Do fsync for every inserted part. Significantly decreases performance of inserts, not recommended to use with wide parts.", 0) \
+    M(Bool, fsync_part_directory, false, "Do fsync for part directory after all part operations (writes, renames, etc.).", 0) \
+    M(UInt64, write_ahead_log_bytes_to_fsync, 100ULL * 1024 * 1024, "Amount of bytes, accumulated in WAL to do fsync.", 0) \
+    M(UInt64, write_ahead_log_interval_ms_to_fsync, 100, "Interval in milliseconds after which fsync for WAL is being done.", 0) \
+    M(Bool, in_memory_parts_insert_sync, false, "If true insert of part with in-memory format will wait for fsync of WAL", 0) \
     \
     /** Inserts settings. */ \
     M(UInt64, parts_to_delay_insert, 150, "If table contains at least that many active parts in single partition, artificially slow down insert into table.", 0) \
diff --git a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp
index 53ef72f32081..35fadb999b45 100644
--- a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp
+++ b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp
@@ -4,6 +4,7 @@
 #include <Storages/MergeTree/MergedBlockOutputStream.h>
 #include <IO/ReadHelpers.h>
 #include <Poco/File.h>
+#include <sys/time.h>
 
 namespace DB
 {
@@ -16,17 +17,31 @@ namespace ErrorCodes
     extern const int CORRUPTED_DATA;
 }
 
-
 MergeTreeWriteAheadLog::MergeTreeWriteAheadLog(
-    const MergeTreeData & storage_,
+    MergeTreeData & storage_,
     const DiskPtr & disk_,
     const String & name_)
     : storage(storage_)
     , disk(disk_)
     , name(name_)
     , path(storage.getRelativeDataPath() + name_)
+    , pool(storage.global_context.getSchedulePool())
 {
     init();
+    sync_task = pool.createTask("MergeTreeWriteAheadLog::sync", [this]
+    {
+        std::lock_guard lock(write_mutex);
+        out->sync();
+        sync_scheduled = false;
+        sync_cv.notify_all();
+    });
+}
+
+MergeTreeWriteAheadLog::~MergeTreeWriteAheadLog()
+{
+    std::unique_lock lock(write_mutex);
+    if (sync_scheduled)
+        sync_cv.wait(lock, [this] { return !sync_scheduled; });
 }
 
 void MergeTreeWriteAheadLog::init()
@@ -38,11 +53,12 @@ void MergeTreeWriteAheadLog::init()
     block_out = std::make_unique<NativeBlockOutputStream>(*out, 0, Block{});
     min_block_number = std::numeric_limits<Int64>::max();
     max_block_number = -1;
+    bytes_at_last_sync = 0;
 }
 
 void MergeTreeWriteAheadLog::addPart(const Block & block, const String & part_name)
 {
-    std::lock_guard lock(write_mutex);
+    std::unique_lock lock(write_mutex);
 
     auto part_info = MergeTreePartInfo::fromPartName(part_name, storage.format_version);
     min_block_number = std::min(min_block_number, part_info.min_block);
@@ -53,6 +69,7 @@ void MergeTreeWriteAheadLog::addPart(const Block & block, const String & part_na
     writeStringBinary(part_name, *out);
     block_out->write(block);
     block_out->flush();
+    sync(lock);
 
     auto max_wal_bytes = storage.getSettings()->write_ahead_log_max_bytes;
     if (out->count() > max_wal_bytes)
@@ -61,14 +78,15 @@ void MergeTreeWriteAheadLog::addPart(const Block & block, const String & part_na
 
 void MergeTreeWriteAheadLog::dropPart(const String & part_name)
 {
-    std::lock_guard lock(write_mutex);
+    std::unique_lock lock(write_mutex);
 
     writeIntBinary(static_cast<UInt8>(0), *out);
     writeIntBinary(static_cast<UInt8>(ActionType::DROP_PART), *out);
     writeStringBinary(part_name, *out);
+    sync(lock);
 }
 
-void MergeTreeWriteAheadLog::rotate(const std::lock_guard<std::mutex> &)
+void MergeTreeWriteAheadLog::rotate(const std::unique_lock<std::mutex> &)
 {
     String new_name = String(WAL_FILE_NAME) + "_"
         + toString(min_block_number) + "_"
@@ -80,7 +98,7 @@ void MergeTreeWriteAheadLog::rotate(const std::lock_guard<std::mutex> &)
 
 MergeTreeData::MutableDataPartsVector MergeTreeWriteAheadLog::restore(const StorageMetadataPtr & metadata_snapshot)
 {
-    std::lock_guard lock(write_mutex);
+    std::unique_lock lock(write_mutex);
 
     MergeTreeData::MutableDataPartsVector parts;
     auto in = disk->readFile(path, DBMS_DEFAULT_BUFFER_SIZE);
@@ -175,6 +193,27 @@ MergeTreeData::MutableDataPartsVector MergeTreeWriteAheadLog::restore(const Stor
     return result;
 }
 
+void MergeTreeWriteAheadLog::sync(std::unique_lock<std::mutex> & lock)
+{
+    size_t bytes_to_sync = storage.getSettings()->write_ahead_log_bytes_to_fsync;
+    time_t time_to_sync = storage.getSettings()->write_ahead_log_interval_ms_to_fsync;
+    size_t current_bytes = out->count();
+
+    if (bytes_to_sync && current_bytes - bytes_at_last_sync > bytes_to_sync)
+    {
+        sync_task->schedule();
+        bytes_at_last_sync = current_bytes;
+    }
+    else if (time_to_sync && !sync_scheduled)
+    {
+        sync_task->scheduleAfter(time_to_sync);
+        sync_scheduled = true;
+    }
+
+    if (storage.getSettings()->in_memory_parts_insert_sync)
+        sync_cv.wait(lock, [this] { return !sync_scheduled; });
+}
+
 std::optional<MergeTreeWriteAheadLog::MinMaxBlockNumber>
 MergeTreeWriteAheadLog::tryParseMinMaxBlockNumber(const String & filename)
 {
diff --git a/src/Storages/MergeTree/MergeTreeWriteAheadLog.h b/src/Storages/MergeTree/MergeTreeWriteAheadLog.h
index 2cc3c2b4181a..77c7c7e11e74 100644
--- a/src/Storages/MergeTree/MergeTreeWriteAheadLog.h
+++ b/src/Storages/MergeTree/MergeTreeWriteAheadLog.h
@@ -3,6 +3,7 @@
 #include <DataStreams/NativeBlockInputStream.h>
 #include <DataStreams/NativeBlockOutputStream.h>
 #include <Storages/MergeTree/IMergeTreeDataPart.h>
+#include <Core/BackgroundSchedulePool.h>
 #include <Disks/IDisk.h>
 
 namespace DB
@@ -31,9 +32,11 @@ class MergeTreeWriteAheadLog
     constexpr static auto WAL_FILE_EXTENSION = ".bin";
     constexpr static auto DEFAULT_WAL_FILE_NAME = "wal.bin";
 
-    MergeTreeWriteAheadLog(const MergeTreeData & storage_, const DiskPtr & disk_,
+    MergeTreeWriteAheadLog(MergeTreeData & storage_, const DiskPtr & disk_,
         const String & name = DEFAULT_WAL_FILE_NAME);
 
+    ~MergeTreeWriteAheadLog();
+
     void addPart(const Block & block, const String & part_name);
     void dropPart(const String & part_name);
     std::vector<MergeTreeMutableDataPartPtr> restore(const StorageMetadataPtr & metadata_snapshot);
@@ -43,7 +46,8 @@ class MergeTreeWriteAheadLog
 
 private:
     void init();
-    void rotate(const std::lock_guard<std::mutex> & lock);
+    void rotate(const std::unique_lock<std::mutex> & lock);
+    void sync(std::unique_lock<std::mutex> & lock);
 
     const MergeTreeData & storage;
     DiskPtr disk;
@@ -56,6 +60,13 @@ class MergeTreeWriteAheadLog
     Int64 min_block_number = std::numeric_limits<Int64>::max();
     Int64 max_block_number = -1;
 
+    BackgroundSchedulePool & pool;
+    BackgroundSchedulePoolTaskHolder sync_task;
+    std::condition_variable sync_cv;
+
+    size_t bytes_at_last_sync = 0;
+    bool sync_scheduled = false;
+
     mutable std::mutex write_mutex;
 };
 
diff --git a/src/Storages/MergeTree/MergedBlockOutputStream.cpp b/src/Storages/MergeTree/MergedBlockOutputStream.cpp
index b78b23b20809..c91ed545ac53 100644
--- a/src/Storages/MergeTree/MergedBlockOutputStream.cpp
+++ b/src/Storages/MergeTree/MergedBlockOutputStream.cpp
@@ -91,6 +91,7 @@ void MergedBlockOutputStream::writeSuffix()
 
 void MergedBlockOutputStream::writeSuffixAndFinalizePart(
         MergeTreeData::MutableDataPartPtr & new_part,
+        bool sync,
         const NamesAndTypesList * total_columns_list,
         MergeTreeData::DataPart::Checksums * additional_column_checksums)
 {
@@ -101,9 +102,9 @@ void MergedBlockOutputStream::writeSuffixAndFinalizePart(
         checksums = std::move(*additional_column_checksums);
 
     /// Finish columns serialization.
-    writer->finishDataSerialization(checksums);
-    writer->finishPrimaryIndexSerialization(checksums);
-    writer->finishSkipIndicesSerialization(checksums);
+    writer->finishDataSerialization(checksums, sync);
+    writer->finishPrimaryIndexSerialization(checksums, sync);
+    writer->finishSkipIndicesSerialization(checksums, sync);
 
     NamesAndTypesList part_columns;
     if (!total_columns_list)
@@ -112,7 +113,7 @@ void MergedBlockOutputStream::writeSuffixAndFinalizePart(
         part_columns = *total_columns_list;
 
     if (new_part->isStoredOnDisk())
-        finalizePartOnDisk(new_part, part_columns, checksums);
+        finalizePartOnDisk(new_part, part_columns, checksums, sync);
 
     new_part->setColumns(part_columns);
     new_part->rows_count = rows_count;
@@ -129,7 +130,8 @@ void MergedBlockOutputStream::writeSuffixAndFinalizePart(
 void MergedBlockOutputStream::finalizePartOnDisk(
     const MergeTreeData::MutableDataPartPtr & new_part,
     NamesAndTypesList & part_columns,
-    MergeTreeData::DataPart::Checksums & checksums)
+    MergeTreeData::DataPart::Checksums & checksums,
+    bool sync)
 {
     if (storage.format_version >= MERGE_TREE_DATA_MIN_FORMAT_VERSION_WITH_CUSTOM_PARTITIONING || isCompactPart(new_part))
     {
@@ -146,6 +148,8 @@ void MergedBlockOutputStream::finalizePartOnDisk(
         count_out_hashing.next();
         checksums.files["count.txt"].file_size = count_out_hashing.count();
         checksums.files["count.txt"].file_hash = count_out_hashing.getHash();
+        if (sync)
+            count_out->sync();
     }
 
     if (!new_part->ttl_infos.empty())
@@ -156,6 +160,8 @@ void MergedBlockOutputStream::finalizePartOnDisk(
         new_part->ttl_infos.write(out_hashing);
         checksums.files["ttl.txt"].file_size = out_hashing.count();
         checksums.files["ttl.txt"].file_hash = out_hashing.getHash();
+        if (sync)
+            out->sync();
     }
 
     removeEmptyColumnsFromPart(new_part, part_columns, checksums);
@@ -164,6 +170,8 @@ void MergedBlockOutputStream::finalizePartOnDisk(
         /// Write a file with a description of columns.
         auto out = volume->getDisk()->writeFile(part_path + "columns.txt", 4096);
         part_columns.writeText(*out);
+        if (sync)
+            out->sync();
     }
 
     if (default_codec != nullptr)
@@ -181,6 +189,8 @@ void MergedBlockOutputStream::finalizePartOnDisk(
         /// Write file with checksums.
         auto out = volume->getDisk()->writeFile(part_path + "checksums.txt", 4096);
         checksums.write(*out);
+        if (sync)
+            out->sync();
     }
 }
 
diff --git a/src/Storages/MergeTree/MergedBlockOutputStream.h b/src/Storages/MergeTree/MergedBlockOutputStream.h
index 6addd4688577..e12d937239d2 100644
--- a/src/Storages/MergeTree/MergedBlockOutputStream.h
+++ b/src/Storages/MergeTree/MergedBlockOutputStream.h
@@ -46,6 +46,7 @@ class MergedBlockOutputStream final : public IMergedBlockOutputStream
     /// Finilize writing part and fill inner structures
     void writeSuffixAndFinalizePart(
             MergeTreeData::MutableDataPartPtr & new_part,
+            bool sync = false,
             const NamesAndTypesList * total_columns_list = nullptr,
             MergeTreeData::DataPart::Checksums * additional_column_checksums = nullptr);
 
@@ -58,7 +59,8 @@ class MergedBlockOutputStream final : public IMergedBlockOutputStream
     void finalizePartOnDisk(
             const MergeTreeData::MutableDataPartPtr & new_part,
             NamesAndTypesList & part_columns,
-            MergeTreeData::DataPart::Checksums & checksums);
+            MergeTreeData::DataPart::Checksums & checksums,
+            bool sync);
 
 private:
     NamesAndTypesList columns_list;
diff --git a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp
index cacecf07b1f4..8ce4ea126ed7 100644
--- a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp
+++ b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp
@@ -67,12 +67,15 @@ void MergedColumnOnlyOutputStream::writeSuffix()
 }
 
 MergeTreeData::DataPart::Checksums
-MergedColumnOnlyOutputStream::writeSuffixAndGetChecksums(MergeTreeData::MutableDataPartPtr & new_part, MergeTreeData::DataPart::Checksums & all_checksums)
+MergedColumnOnlyOutputStream::writeSuffixAndGetChecksums(
+    MergeTreeData::MutableDataPartPtr & new_part,
+    MergeTreeData::DataPart::Checksums & all_checksums,
+    bool sync)
 {
     /// Finish columns serialization.
     MergeTreeData::DataPart::Checksums checksums;
-    writer->finishDataSerialization(checksums);
-    writer->finishSkipIndicesSerialization(checksums);
+    writer->finishDataSerialization(checksums, sync);
+    writer->finishSkipIndicesSerialization(checksums, sync);
 
     auto columns = new_part->getColumns();
 
diff --git a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.h b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.h
index 902138ced9d6..507a964ede0c 100644
--- a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.h
+++ b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.h
@@ -27,7 +27,7 @@ class MergedColumnOnlyOutputStream final : public IMergedBlockOutputStream
     void write(const Block & block) override;
     void writeSuffix() override;
     MergeTreeData::DataPart::Checksums
-    writeSuffixAndGetChecksums(MergeTreeData::MutableDataPartPtr & new_part, MergeTreeData::DataPart::Checksums & all_checksums);
+    writeSuffixAndGetChecksums(MergeTreeData::MutableDataPartPtr & new_part, MergeTreeData::DataPart::Checksums & all_checksums, bool sync = false);
 
 private:
     Block header;
