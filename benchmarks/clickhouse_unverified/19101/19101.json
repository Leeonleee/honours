{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 19101,
  "instance_id": "ClickHouse__ClickHouse-19101",
  "issue_numbers": [
    "18340"
  ],
  "base_commit": "dfc1e8ef1d62c7166aaf2a15ce2bca054c5061ad",
  "patch": "diff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex 5d0f79f46795..3bd06c7f7585 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -549,14 +549,26 @@ CompressionCodecPtr IMergeTreeDataPart::detectDefaultCompressionCodec() const\n         auto column_size = getColumnSize(part_column.name, *part_column.type);\n         if (column_size.data_compressed != 0 && !storage_columns.hasCompressionCodec(part_column.name))\n         {\n-            String path_to_data_file = getFullRelativePath() + getFileNameForColumn(part_column) + \".bin\";\n-            if (!volume->getDisk()->exists(path_to_data_file))\n+            String path_to_data_file;\n+            part_column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n-                LOG_WARNING(storage.log, \"Part's {} column {} has non zero data compressed size, but data file {} doesn't exists\", name, backQuoteIfNeed(part_column.name), path_to_data_file);\n+                if (path_to_data_file.empty())\n+                {\n+                    String candidate_path = getFullRelativePath() + IDataType::getFileNameForStream(part_column.name, substream_path) + \".bin\";\n+\n+                    /// We can have existing, but empty .bin files. Example: LowCardinality(Nullable(...)) columns and column_name.dict.null.bin file.\n+                    if (volume->getDisk()->exists(candidate_path) && volume->getDisk()->getFileSize(candidate_path) != 0)\n+                        path_to_data_file = candidate_path;\n+                }\n+            });\n+\n+            if (path_to_data_file.empty())\n+            {\n+                LOG_WARNING(storage.log, \"Part's {} column {} has non zero data compressed size, but all data files don't exist or empty\", name, backQuoteIfNeed(part_column.name));\n                 continue;\n             }\n \n-            result = getCompressionCodecForFile(volume->getDisk(), getFullRelativePath() + getFileNameForColumn(part_column) + \".bin\");\n+            result = getCompressionCodecForFile(volume->getDisk(), path_to_data_file);\n             break;\n         }\n     }\n",
  "test_patch": "diff --git a/tests/integration/test_compression_codec_read/__init__.py b/tests/integration/test_compression_codec_read/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_compression_codec_read/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_compression_codec_read/test.py b/tests/integration/test_compression_codec_read/test.py\nnew file mode 100644\nindex 000000000000..0eb1f5aa867f\n--- /dev/null\n+++ b/tests/integration/test_compression_codec_read/test.py\n@@ -0,0 +1,46 @@\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.test_tools import assert_eq_with_retry\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance('node1', image='yandex/clickhouse-server', tag='20.8.11.17', with_installed_binary=True, stay_alive=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+def test_default_codec_read(start_cluster):\n+    node1.query(\"\"\"\n+        CREATE TABLE test_18340\n+        (\n+            `lns` LowCardinality(Nullable(String)),\n+            `ns` Nullable(String),\n+            `s` String,\n+            `ni64` Nullable(Int64),\n+            `ui64` UInt64,\n+            `alns` Array(LowCardinality(Nullable(String))),\n+            `ans` Array(Nullable(String)),\n+            `dt` DateTime,\n+            `i32` Int32\n+        )\n+        ENGINE = MergeTree()\n+        PARTITION BY i32\n+        ORDER BY (s, farmHash64(s))\n+        SAMPLE BY farmHash64(s)\n+    \"\"\")\n+\n+    node1.query(\"insert into test_18340 values ('test', 'test', 'test', 0, 0, ['a'], ['a'], now(), 0)\")\n+\n+\n+    assert node1.query(\"SELECT COUNT() FROM test_18340\") == \"1\\n\"\n+\n+    node1.restart_with_latest_version()\n+\n+    assert node1.query(\"SELECT COUNT() FROM test_18340\") == \"1\\n\"\n",
  "problem_statement": "EOF from detectDefaultCompressionCodec start happening between v20.8.2.3 and v20.13.1.1?\nI tried to upgrade ClickHouse from v20.8.2.3 to v20.13.1.1 but there are some errors while reading a merge_tree table..\r\n\r\n**Error Messages:**\r\n```\r\n2020.12.22 11:19:36.084760 [ 20114 ] {} <Error> DB::MergeTreeData::loadDataParts(bool)::<lambda()>: Code: 32, e.displayText() = DB::Exception: Attempt to read after eof, Stack trace (when copying this message\r\n, always include the lines below):                                                    \r\n                                                                                      \r\n0. /home/deploy/sources/ClickHouse.achimbab/build/../contrib/poco/Foundation/src/Exception.cpp:27: Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char\r\n> > const&, int) @ 0xd00a9ac in /home/deploy/bin/clickhouse-limit-pushdown-test            \r\n1. /home/deploy/sources/ClickHouse.achimbab/build/../src/Common/Exception.cpp:54: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @\r\n0x4322081 in /home/deploy/bin/clickhouse-limit-pushdown-test                          \r\n2. /home/deploy/sources/ClickHouse.achimbab/build/../contrib/libcxx/include/string:2134: DB::ReadBuffer::throwReadAfterEOF() @ 0x3963061 in /home/deploy/bin/clickhouse-limit-pushdown-test\r\n3. /home/deploy/sources/ClickHouse.achimbab/build/../src/IO/ReadBuffer.h:108: DB::getCompressionCodecForFile(std::__1::shared_ptr<DB::IDisk> const&, std::__1::basic_string<char, std::__1::char_traits<char>, s\r\ntd::__1::allocator<char> > const&) @ 0xa52d62e in /home/deploy/bin/clickhouse-limit-pushdown-test\r\n4. /home/deploy/sources/ClickHouse.achimbab/build/../contrib/libcxx/include/type_traits:3696: DB::IMergeTreeDataPart::detectDefaultCompressionCodec() const @ 0xa0231f5 in /home/deploy/bin/clickhouse-limit-pus\r\nhdown-test                                                                                     \r\n5. /home/deploy/sources/ClickHouse.achimbab/build/../contrib/libcxx/include/type_traits:3696: DB::IMergeTreeDataPart::loadDefaultCompressionCodec() @ 0xa023a72 in /home/deploy/bin/clickhouse-limit-pushdown-te\r\nst                                                                                             \r\n6. /home/deploy/sources/ClickHouse.achimbab/build/../src/Common/MemoryTracker.h:142: DB::IMergeTreeDataPart::loadColumnsChecksumsIndexes(bool, bool) @ 0xa02a360 in /home/deploy/bin/clickhouse-limit-pushdown-t\r\nest                                                                                            \r\n7. /home/deploy/sources/ClickHouse.achimbab/build/../src/Storages/MergeTree/MergeTreeData.cpp:868: DB::MergeTreeData::loadDataParts(bool)::'lambda'()::operator()() const @ 0xa0743aa in /home/deploy/bin/clickh\r\nouse-limit-pushdown-test                                                                       \r\n8. /home/deploy/sources/ClickHouse.achimbab/build/../contrib/libcxx/include/functional:1853: ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x432b487 in\r\n /home/deploy/bin/clickhouse-limit-pushdown-test                                             \r\n9. /home/deploy/sources/ClickHouse.achimbab/build/../src/Common/ThreadPool.h:177: ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<vo\r\nid ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambd\r\na1'()&&...)::'lambda'()::operator()() @ 0x432bc5a in /home/deploy/bin/clickhouse-limit-pushdown-test\r\n10. /home/deploy/sources/ClickHouse.achimbab/build/../contrib/libcxx/include/functional:1853: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x432a967 in /home/\r\ndeploy/bin/clickhouse-limit-pushdown-test\r\n11. /home/deploy/sources/ClickHouse.achimbab/build/../contrib/libcxx/include/memory:2615: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delet\r\ne<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) @ 0x432907f in /home/deplo\r\ny/bin/clickhouse-limit-pushdown-test\r\n12. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n13. /build/glibc-2ORdQG/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97: __clone @ 0x121a3f in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n (version 20.13.1.1)\r\n```\r\n\r\nWas the data layout changed between v20.8.2.3 and v20.13.1.1?\r\n\r\nThank you.\n",
  "hints_text": "Are you sure the part is ok? 20.8 can load that table? Other versions? \nMaybe related to https://github.com/ClickHouse/ClickHouse/pull/14049\r\n\r\n\nCan you show your table schema? Also it would be nice to have a part which fails to load with this error.\nProbably you are using compact parts or in-memory parts?\n@alesapin \r\nI will try to find some test-cases reproducing this exception and then share the test-cases.\r\nWish you a Marry Christmas. :)\r\n\n@achimbab Any news?\n@alesapin \r\nI got a higher priority issue #18465 in my company. \r\nSorry for the delay.\n@alesapin \r\nIn my environment, It can be reproduced by following steps.\r\n\r\n1. Start ClickHouse server v20.8.2.3.\r\n2. Create table.\r\n```SQL\r\nCREATE TABLE test_18340\r\n(\r\n    `lns` LowCardinality(Nullable(String)),\r\n    `ns` Nullable(String),\r\n    `s` String,\r\n    `ni64` Nullable(Int64),\r\n    `ui64` UInt64,\r\n    `alns` Array(LowCardinality(Nullable(String))),\r\n    `ans` Array(Nullable(String)),\r\n    `dt` DateTime,\r\n    `i32` Int32\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY i32\r\nORDER BY (s, farmHash64(s))\r\nSAMPLE BY farmHash64(s)\r\n```\r\n3. Insert data\r\n```SQL\r\ninsert into test_18340 values ('test', 'test', 'test', 0, 0, ['a'], ['a'], now(), 0)\r\n```\r\n4. Shutdown ClickHouse server v20.8.2.3.\r\n5. Start ClickHouse server v20.13.1.1.  <-- **At this step, \"DB::Exception: Attempt to read after eof\" could occur.**\r\n",
  "created_at": "2021-01-15T09:06:23Z",
  "modified_files": [
    "src/Storages/MergeTree/IMergeTreeDataPart.cpp"
  ],
  "modified_test_files": [
    "b/tests/integration/test_compression_codec_read/__init__.py",
    "b/tests/integration/test_compression_codec_read/test.py"
  ]
}