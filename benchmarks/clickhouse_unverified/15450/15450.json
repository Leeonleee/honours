{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 15450,
  "instance_id": "ClickHouse__ClickHouse-15450",
  "issue_numbers": [
    "14345"
  ],
  "base_commit": "a1d9eee0b590aea1abad0ba25b5e4f7caafd1d2e",
  "patch": "diff --git a/src/DataStreams/ITTLAlgorithm.cpp b/src/DataStreams/ITTLAlgorithm.cpp\nnew file mode 100644\nindex 000000000000..7513e0c6ce03\n--- /dev/null\n+++ b/src/DataStreams/ITTLAlgorithm.cpp\n@@ -0,0 +1,65 @@\n+#include <DataStreams/ITTLAlgorithm.h>\n+#include <Columns/ColumnVector.h>\n+#include <Columns/ColumnConst.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+\n+ITTLAlgorithm::ITTLAlgorithm(\n+    const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_)\n+    : description(description_)\n+    , old_ttl_info(old_ttl_info_)\n+    , current_time(current_time_)\n+    , force(force_)\n+    , date_lut(DateLUT::instance())\n+{\n+}\n+\n+bool ITTLAlgorithm::isTTLExpired(time_t ttl) const\n+{\n+    return (ttl && (ttl <= current_time));\n+}\n+\n+ColumnPtr ITTLAlgorithm::executeExpressionAndGetColumn(\n+    const ExpressionActionsPtr & expression, const Block & block, const String & result_column)\n+{\n+    if (!expression)\n+        return nullptr;\n+\n+    if (block.has(result_column))\n+        return block.getByName(result_column).column;\n+\n+    Block block_copy;\n+    for (const auto & column_name : expression->getRequiredColumns())\n+        block_copy.insert(block.getByName(column_name));\n+\n+    /// Keep number of rows for const expression.\n+    size_t num_rows = block.rows();\n+    expression->execute(block_copy, num_rows);\n+\n+    return block_copy.getByName(result_column).column;\n+}\n+\n+UInt32 ITTLAlgorithm::getTimestampByIndex(const IColumn * column, size_t index) const\n+{\n+    if (const ColumnUInt16 * column_date = typeid_cast<const ColumnUInt16 *>(column))\n+        return date_lut.fromDayNum(DayNum(column_date->getData()[index]));\n+    else if (const ColumnUInt32 * column_date_time = typeid_cast<const ColumnUInt32 *>(column))\n+        return column_date_time->getData()[index];\n+    else if (const ColumnConst * column_const = typeid_cast<const ColumnConst *>(column))\n+    {\n+        if (typeid_cast<const ColumnUInt16 *>(&column_const->getDataColumn()))\n+            return date_lut.fromDayNum(DayNum(column_const->getValue<UInt16>()));\n+        else if (typeid_cast<const ColumnUInt32 *>(&column_const->getDataColumn()))\n+            return column_const->getValue<UInt32>();\n+    }\n+\n+    throw Exception(\"Unexpected type of result TTL column\", ErrorCodes::LOGICAL_ERROR);\n+}\n+\n+}\ndiff --git a/src/DataStreams/ITTLAlgorithm.h b/src/DataStreams/ITTLAlgorithm.h\nnew file mode 100644\nindex 000000000000..d87d43d8c0c4\n--- /dev/null\n+++ b/src/DataStreams/ITTLAlgorithm.h\n@@ -0,0 +1,54 @@\n+#pragma once\n+\n+#include <Storages/TTLDescription.h>\n+#include <Storages/MergeTree/MergeTreeDataPartTTLInfo.h>\n+#include <Storages/MergeTree/IMergeTreeDataPart.h>\n+#include <common/DateLUT.h>\n+\n+namespace DB\n+{\n+\n+/**\n+ * Represents the actions, which are required to do\n+ * with data, when TTL is expired: delete, aggregate, etc.\n+ */\n+class ITTLAlgorithm\n+{\n+public:\n+    using TTLInfo = IMergeTreeDataPart::TTLInfo;\n+    using MutableDataPartPtr = MergeTreeMutableDataPartPtr;\n+\n+    ITTLAlgorithm(const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_);\n+    virtual ~ITTLAlgorithm() = default;\n+\n+    virtual void execute(Block & block) = 0;\n+\n+    /// Updates TTL metadata of the data_part.\n+    virtual void finalize(const MutableDataPartPtr & data_part) const = 0;\n+\n+    bool isMinTTLExpired() const { return force || isTTLExpired(old_ttl_info.min); }\n+    bool isMaxTTLExpired() const { return isTTLExpired(old_ttl_info.max); }\n+\n+    /** This function is needed to avoid a conflict between already calculated columns and columns that needed to execute TTL.\n+      * If result column is absent in block, all required columns are copied to new block and expression is executed on new block.\n+      */\n+    static ColumnPtr executeExpressionAndGetColumn(\n+        const ExpressionActionsPtr & expression, const Block & block, const String & result_column);\n+\n+protected:\n+    bool isTTLExpired(time_t ttl) const;\n+    UInt32 getTimestampByIndex(const IColumn * column, size_t index) const;\n+\n+    const TTLDescription description;\n+    const TTLInfo old_ttl_info;\n+    const time_t current_time;\n+    const bool force;\n+    TTLInfo new_ttl_info;\n+\n+private:\n+    const DateLUTImpl & date_lut;\n+};\n+\n+using TTLAlgorithmPtr = std::unique_ptr<ITTLAlgorithm>;\n+\n+}\ndiff --git a/src/DataStreams/TTLAggregationAlgorithm.cpp b/src/DataStreams/TTLAggregationAlgorithm.cpp\nnew file mode 100644\nindex 000000000000..ebe08159c55f\n--- /dev/null\n+++ b/src/DataStreams/TTLAggregationAlgorithm.cpp\n@@ -0,0 +1,173 @@\n+#include <DataStreams/TTLAggregationAlgorithm.h>\n+\n+namespace DB\n+{\n+\n+TTLAggregationAlgorithm::TTLAggregationAlgorithm(\n+    const TTLDescription & description_,\n+    const TTLInfo & old_ttl_info_,\n+    time_t current_time_,\n+    bool force_,\n+    const Block & header_,\n+    const MergeTreeData & storage_)\n+    : ITTLAlgorithm(description_, old_ttl_info_, current_time_, force_)\n+    , header(header_)\n+{\n+    current_key_value.resize(description.group_by_keys.size());\n+\n+    ColumnNumbers keys;\n+    for (const auto & key : description.group_by_keys)\n+        keys.push_back(header.getPositionByName(key));\n+\n+    key_columns.resize(description.group_by_keys.size());\n+    AggregateDescriptions aggregates = description.aggregate_descriptions;\n+\n+    for (auto & descr : aggregates)\n+        if (descr.arguments.empty())\n+            for (const auto & name : descr.argument_names)\n+                descr.arguments.push_back(header.getPositionByName(name));\n+\n+    columns_for_aggregator.resize(description.aggregate_descriptions.size());\n+    const Settings & settings = storage_.global_context.getSettingsRef();\n+\n+    Aggregator::Params params(header, keys, aggregates,\n+        false, settings.max_rows_to_group_by, settings.group_by_overflow_mode, 0, 0,\n+        settings.max_bytes_before_external_group_by, settings.empty_result_for_aggregation_by_empty_set,\n+        storage_.global_context.getTemporaryVolume(), settings.max_threads, settings.min_free_disk_space_for_temporary_data);\n+\n+    aggregator = std::make_unique<Aggregator>(params);\n+}\n+\n+void TTLAggregationAlgorithm::execute(Block & block)\n+{\n+    if (!block)\n+    {\n+        if (!aggregation_result.empty())\n+        {\n+            MutableColumns result_columns = header.cloneEmptyColumns();\n+            finalizeAggregates(result_columns);\n+            block = header.cloneWithColumns(std::move(result_columns));\n+        }\n+\n+        return;\n+    }\n+\n+    const auto & column_names = header.getNames();\n+    MutableColumns result_columns = header.cloneEmptyColumns();\n+    MutableColumns aggregate_columns = header.cloneEmptyColumns();\n+\n+    auto ttl_column = executeExpressionAndGetColumn(description.expression, block, description.result_column);\n+    auto where_column = executeExpressionAndGetColumn(description.where_expression, block, description.where_result_column);\n+\n+    size_t rows_aggregated = 0;\n+    size_t current_key_start = 0;\n+    size_t rows_with_current_key = 0;\n+\n+    for (size_t i = 0; i < block.rows(); ++i)\n+    {\n+        UInt32 cur_ttl = getTimestampByIndex(ttl_column.get(), i);\n+        bool where_filter_passed = !where_column || where_column->getBool(i);\n+        bool ttl_expired = isTTLExpired(cur_ttl) && where_filter_passed;\n+\n+        bool same_as_current = true;\n+        for (size_t j = 0; j < description.group_by_keys.size(); ++j)\n+        {\n+            const String & key_column = description.group_by_keys[j];\n+            const IColumn * values_column = block.getByName(key_column).column.get();\n+            if (!same_as_current || (*values_column)[i] != current_key_value[j])\n+            {\n+                values_column->get(i, current_key_value[j]);\n+                same_as_current = false;\n+            }\n+        }\n+\n+        if (!same_as_current)\n+        {\n+            if (rows_with_current_key)\n+                calculateAggregates(aggregate_columns, current_key_start, rows_with_current_key);\n+            finalizeAggregates(result_columns);\n+\n+            current_key_start = rows_aggregated;\n+            rows_with_current_key = 0;\n+        }\n+\n+        if (ttl_expired)\n+        {\n+            ++rows_with_current_key;\n+            ++rows_aggregated;\n+            for (const auto & name : column_names)\n+            {\n+                const IColumn * values_column = block.getByName(name).column.get();\n+                auto & column = aggregate_columns[header.getPositionByName(name)];\n+                column->insertFrom(*values_column, i);\n+            }\n+        }\n+        else\n+        {\n+            new_ttl_info.update(cur_ttl);\n+            for (const auto & name : column_names)\n+            {\n+                const IColumn * values_column = block.getByName(name).column.get();\n+                auto & column = result_columns[header.getPositionByName(name)];\n+                column->insertFrom(*values_column, i);\n+            }\n+        }\n+    }\n+\n+    if (rows_with_current_key)\n+        calculateAggregates(aggregate_columns, current_key_start, rows_with_current_key);\n+\n+    block = header.cloneWithColumns(std::move(result_columns));\n+}\n+\n+void TTLAggregationAlgorithm::calculateAggregates(const MutableColumns & aggregate_columns, size_t start_pos, size_t length)\n+{\n+    Columns aggregate_chunk;\n+    aggregate_chunk.reserve(aggregate_columns.size());\n+    for (const auto & name : header.getNames())\n+    {\n+        const auto & column = aggregate_columns[header.getPositionByName(name)];\n+        ColumnPtr chunk_column = column->cut(start_pos, length);\n+        aggregate_chunk.emplace_back(std::move(chunk_column));\n+    }\n+\n+    aggregator->executeOnBlock(aggregate_chunk, length, aggregation_result, key_columns,\n+                               columns_for_aggregator, no_more_keys);\n+}\n+\n+void TTLAggregationAlgorithm::finalizeAggregates(MutableColumns & result_columns)\n+{\n+    if (!aggregation_result.empty())\n+    {\n+        auto aggregated_res = aggregator->convertToBlocks(aggregation_result, true, 1);\n+        for (auto & agg_block : aggregated_res)\n+        {\n+            for (const auto & it : description.set_parts)\n+                it.expression->execute(agg_block);\n+\n+            for (const auto & name : description.group_by_keys)\n+            {\n+                const IColumn * values_column = agg_block.getByName(name).column.get();\n+                auto & result_column = result_columns[header.getPositionByName(name)];\n+                result_column->insertRangeFrom(*values_column, 0, agg_block.rows());\n+            }\n+\n+            for (const auto & it : description.set_parts)\n+            {\n+                const IColumn * values_column = agg_block.getByName(it.expression_result_column_name).column.get();\n+                auto & result_column = result_columns[header.getPositionByName(it.column_name)];\n+                result_column->insertRangeFrom(*values_column, 0, agg_block.rows());\n+            }\n+        }\n+    }\n+\n+    aggregation_result.invalidate();\n+}\n+\n+void TTLAggregationAlgorithm::finalize(const MutableDataPartPtr & data_part) const\n+{\n+    data_part->ttl_infos.group_by_ttl[description.result_column] = new_ttl_info;\n+    data_part->ttl_infos.updatePartMinMaxTTL(new_ttl_info.min, new_ttl_info.max);\n+}\n+\n+}\ndiff --git a/src/DataStreams/TTLAggregationAlgorithm.h b/src/DataStreams/TTLAggregationAlgorithm.h\nnew file mode 100644\nindex 000000000000..c2f40bab6b95\n--- /dev/null\n+++ b/src/DataStreams/TTLAggregationAlgorithm.h\n@@ -0,0 +1,42 @@\n+#pragma once\n+\n+#include <DataStreams/ITTLAlgorithm.h>\n+#include <Interpreters/Aggregator.h>\n+#include <Storages/MergeTree/MergeTreeData.h>\n+\n+namespace DB\n+{\n+\n+/// Aggregates rows according to 'TTL expr GROUP BY key' description.\n+/// Aggregation key must be the prefix of the sorting key.\n+class TTLAggregationAlgorithm final : public ITTLAlgorithm\n+{\n+public:\n+    TTLAggregationAlgorithm(\n+        const TTLDescription & description_,\n+        const TTLInfo & old_ttl_info_,\n+        time_t current_time_,\n+        bool force_,\n+        const Block & header_,\n+        const MergeTreeData & storage_);\n+\n+    void execute(Block & block) override;\n+    void finalize(const MutableDataPartPtr & data_part) const override;\n+\n+private:\n+    // Calculate aggregates of aggregate_columns into aggregation_result\n+    void calculateAggregates(const MutableColumns & aggregate_columns, size_t start_pos, size_t length);\n+\n+    /// Finalize aggregation_result into result_columns\n+    void finalizeAggregates(MutableColumns & result_columns);\n+\n+    const Block header;\n+    std::unique_ptr<Aggregator> aggregator;\n+    Row current_key_value;\n+    AggregatedDataVariants aggregation_result;\n+    ColumnRawPtrs key_columns;\n+    Aggregator::AggregateColumns columns_for_aggregator;\n+    bool no_more_keys = false;\n+};\n+\n+}\ndiff --git a/src/DataStreams/TTLBlockInputStream.cpp b/src/DataStreams/TTLBlockInputStream.cpp\nindex 38479409f84e..4f141a03475c 100644\n--- a/src/DataStreams/TTLBlockInputStream.cpp\n+++ b/src/DataStreams/TTLBlockInputStream.cpp\n@@ -8,14 +8,13 @@\n #include <Storages/TTLMode.h>\n #include <Interpreters/Context.h>\n \n-namespace DB\n-{\n+#include <DataStreams/TTLDeleteAlgorithm.h>\n+#include <DataStreams/TTLColumnAlgorithm.h>\n+#include <DataStreams/TTLAggregationAlgorithm.h>\n+#include <DataStreams/TTLUpdateInfoAlgorithm.h>\n \n-namespace ErrorCodes\n+namespace DB\n {\n-    extern const int LOGICAL_ERROR;\n-}\n-\n \n TTLBlockInputStream::TTLBlockInputStream(\n     const BlockInputStreamPtr & input_,\n@@ -24,83 +23,69 @@ TTLBlockInputStream::TTLBlockInputStream(\n     const MergeTreeData::MutableDataPartPtr & data_part_,\n     time_t current_time_,\n     bool force_)\n-    : storage(storage_)\n-    , metadata_snapshot(metadata_snapshot_)\n-    , data_part(data_part_)\n-    , current_time(current_time_)\n-    , force(force_)\n-    , old_ttl_infos(data_part->ttl_infos)\n-    , log(&Poco::Logger::get(storage.getLogName() + \" (TTLBlockInputStream)\"))\n-    , date_lut(DateLUT::instance())\n+    : data_part(data_part_)\n+    , log(&Poco::Logger::get(storage_.getLogName() + \" (TTLBlockInputStream)\"))\n {\n     children.push_back(input_);\n     header = children.at(0)->getHeader();\n+    auto old_ttl_infos = data_part->ttl_infos;\n \n-    const auto & storage_columns = metadata_snapshot->getColumns();\n-    const auto & column_defaults = storage_columns.getDefaults();\n-\n-    ASTPtr default_expr_list = std::make_shared<ASTExpressionList>();\n-    for (const auto & [name, _] : metadata_snapshot->getColumnTTLs())\n+    if (metadata_snapshot_->hasRowsTTL())\n     {\n-        auto it = column_defaults.find(name);\n-        if (it != column_defaults.end())\n-        {\n-            auto column = storage_columns.get(name);\n-            auto expression = it->second.expression->clone();\n-            default_expr_list->children.emplace_back(setAlias(addTypeConversionToAST(std::move(expression), column.type->getName()), it->first));\n-        }\n-    }\n+        const auto & rows_ttl = metadata_snapshot_->getRowsTTL();\n+        auto algorithm = std::make_unique<TTLDeleteAlgorithm>(\n+            rows_ttl, old_ttl_infos.table_ttl, current_time_, force_);\n \n-    for (const auto & [name, ttl_info] : old_ttl_infos.columns_ttl)\n-    {\n-        if (force || isTTLExpired(ttl_info.min))\n-        {\n-            new_ttl_infos.columns_ttl.emplace(name, IMergeTreeDataPart::TTLInfo{});\n-            empty_columns.emplace(name);\n-        }\n-        else\n-            new_ttl_infos.columns_ttl.emplace(name, ttl_info);\n+        /// Skip all data if table ttl is expired for part\n+        if (algorithm->isMaxTTLExpired() && !rows_ttl.where_expression)\n+            all_data_dropped = true;\n+\n+        delete_algorithm = algorithm.get();\n+        algorithms.emplace_back(std::move(algorithm));\n     }\n \n-    if (!force && !isTTLExpired(old_ttl_infos.table_ttl.min))\n-        new_ttl_infos.table_ttl = old_ttl_infos.table_ttl;\n+    for (const auto & where_ttl : metadata_snapshot_->getRowsWhereTTLs())\n+        algorithms.emplace_back(std::make_unique<TTLDeleteAlgorithm>(\n+            where_ttl, old_ttl_infos.rows_where_ttl[where_ttl.result_column], current_time_, force_));\n \n-    if (!default_expr_list->children.empty())\n-    {\n-        auto syntax_result = TreeRewriter(storage.global_context).analyze(default_expr_list, metadata_snapshot->getColumns().getAllPhysical());\n-        defaults_expression = ExpressionAnalyzer{default_expr_list, syntax_result, storage.global_context}.getActions(true);\n-    }\n+    for (const auto & group_by_ttl : metadata_snapshot_->getGroupByTTLs())\n+        algorithms.emplace_back(std::make_unique<TTLAggregationAlgorithm>(\n+            group_by_ttl, old_ttl_infos.group_by_ttl[group_by_ttl.result_column], current_time_, force_, header, storage_));\n \n-    auto storage_rows_ttl = metadata_snapshot->getRowsTTL();\n-    if (metadata_snapshot->hasRowsTTL() && storage_rows_ttl.mode == TTLMode::GROUP_BY)\n+    if (metadata_snapshot_->hasAnyColumnTTL())\n     {\n-        current_key_value.resize(storage_rows_ttl.group_by_keys.size());\n+        const auto & storage_columns = metadata_snapshot_->getColumns();\n+        const auto & column_defaults = storage_columns.getDefaults();\n \n-        ColumnNumbers keys;\n-        for (const auto & key : storage_rows_ttl.group_by_keys)\n-            keys.push_back(header.getPositionByName(key));\n-        agg_key_columns.resize(storage_rows_ttl.group_by_keys.size());\n-\n-        AggregateDescriptions aggregates = storage_rows_ttl.aggregate_descriptions;\n-        for (auto & descr : aggregates)\n-            if (descr.arguments.empty())\n-                for (const auto & name : descr.argument_names)\n-                    descr.arguments.push_back(header.getPositionByName(name));\n-        agg_aggregate_columns.resize(storage_rows_ttl.aggregate_descriptions.size());\n+        for (const auto & [name, description] : metadata_snapshot_->getColumnTTLs())\n+        {\n+            ExpressionActionsPtr default_expression;\n+            String default_column_name;\n+            auto it = column_defaults.find(name);\n+            if (it != column_defaults.end())\n+            {\n+                const auto & column = storage_columns.get(name);\n+                auto default_ast = it->second.expression->clone();\n+                default_ast = addTypeConversionToAST(std::move(default_ast), column.type->getName());\n \n-        const Settings & settings = storage.global_context.getSettingsRef();\n+                auto syntax_result = TreeRewriter(storage_.global_context).analyze(default_ast, metadata_snapshot_->getColumns().getAllPhysical());\n+                default_expression = ExpressionAnalyzer{default_ast, syntax_result, storage_.global_context}.getActions(true);\n+                default_column_name = default_ast->getColumnName();\n+            }\n \n-        Aggregator::Params params(header, keys, aggregates,\n-            false, settings.max_rows_to_group_by, settings.group_by_overflow_mode, 0, 0,\n-            settings.max_bytes_before_external_group_by, settings.empty_result_for_aggregation_by_empty_set,\n-            storage.global_context.getTemporaryVolume(), settings.max_threads, settings.min_free_disk_space_for_temporary_data);\n-        aggregator = std::make_unique<Aggregator>(params);\n+            algorithms.emplace_back(std::make_unique<TTLColumnAlgorithm>(\n+                description, old_ttl_infos.columns_ttl[name], current_time_,\n+                force_, name, default_expression, default_column_name));\n+        }\n     }\n-}\n \n-bool TTLBlockInputStream::isTTLExpired(time_t ttl) const\n-{\n-    return (ttl && (ttl <= current_time));\n+    for (const auto & move_ttl : metadata_snapshot_->getMoveTTLs())\n+        algorithms.emplace_back(std::make_unique<TTLMoveAlgorithm>(\n+            move_ttl, old_ttl_infos.moves_ttl[move_ttl.result_column], current_time_, force_));\n+\n+    for (const auto & recompression_ttl : metadata_snapshot_->getRecompressionTTLs())\n+        algorithms.emplace_back(std::make_unique<TTLRecompressionAlgorithm>(\n+            recompression_ttl, old_ttl_infos.recompression_ttl[recompression_ttl.result_column], current_time_, force_));\n }\n \n Block reorderColumns(Block block, const Block & header)\n@@ -114,321 +99,30 @@ Block reorderColumns(Block block, const Block & header)\n \n Block TTLBlockInputStream::readImpl()\n {\n-    /// Skip all data if table ttl is expired for part\n-    auto storage_rows_ttl = metadata_snapshot->getRowsTTL();\n-    if (metadata_snapshot->hasRowsTTL() && !storage_rows_ttl.where_expression && storage_rows_ttl.mode != TTLMode::GROUP_BY\n-        && isTTLExpired(old_ttl_infos.table_ttl.max))\n-    {\n-        rows_removed = data_part->rows_count;\n+    if (all_data_dropped)\n         return {};\n-    }\n \n+    auto block = children.at(0)->read();\n+    for (const auto & algorithm : algorithms)\n+        algorithm->execute(block);\n \n-    Block block = children.at(0)->read();\n     if (!block)\n-    {\n-        if (aggregator && !agg_result.empty())\n-        {\n-            MutableColumns result_columns = header.cloneEmptyColumns();\n-            finalizeAggregates(result_columns);\n-            block = header.cloneWithColumns(std::move(result_columns));\n-        }\n-\n         return block;\n-    }\n-\n-    if (metadata_snapshot->hasRowsTTL() && (force || isTTLExpired(old_ttl_infos.table_ttl.min)))\n-        removeRowsWithExpiredTableTTL(block);\n-\n-    removeValuesWithExpiredColumnTTL(block);\n-\n-    updateMovesTTL(block);\n-    updateRecompressionTTL(block);\n \n     return reorderColumns(std::move(block), header);\n }\n \n void TTLBlockInputStream::readSuffixImpl()\n {\n-    for (const auto & elem : new_ttl_infos.columns_ttl)\n-        new_ttl_infos.updatePartMinMaxTTL(elem.second.min, elem.second.max);\n-\n-    new_ttl_infos.updatePartMinMaxTTL(new_ttl_infos.table_ttl.min, new_ttl_infos.table_ttl.max);\n+    data_part->ttl_infos = {};\n+    for (const auto & algorithm : algorithms)\n+        algorithm->finalize(data_part);\n \n-    data_part->ttl_infos = std::move(new_ttl_infos);\n-    data_part->expired_columns = std::move(empty_columns);\n-\n-    if (rows_removed)\n-        LOG_DEBUG(log, \"Removed {} rows with expired TTL from part {}\", rows_removed, data_part->name);\n-}\n-\n-void TTLBlockInputStream::removeRowsWithExpiredTableTTL(Block & block)\n-{\n-    auto rows_ttl = metadata_snapshot->getRowsTTL();\n-\n-    rows_ttl.expression->execute(block);\n-    if (rows_ttl.where_expression)\n-        rows_ttl.where_expression->execute(block);\n-\n-    const IColumn * ttl_column =\n-        block.getByName(rows_ttl.result_column).column.get();\n-\n-    const IColumn * where_result_column = rows_ttl.where_expression ?\n-        block.getByName(rows_ttl.where_result_column).column.get() : nullptr;\n-\n-    const auto & column_names = header.getNames();\n-\n-    if (!aggregator)\n-    {\n-        MutableColumns result_columns;\n-        result_columns.reserve(column_names.size());\n-        for (auto it = column_names.begin(); it != column_names.end(); ++it)\n-        {\n-            const IColumn * values_column = block.getByName(*it).column.get();\n-            MutableColumnPtr result_column = values_column->cloneEmpty();\n-            result_column->reserve(block.rows());\n-\n-            for (size_t i = 0; i < block.rows(); ++i)\n-            {\n-                UInt32 cur_ttl = getTimestampByIndex(ttl_column, i);\n-                bool where_filter_passed = !where_result_column || where_result_column->getBool(i);\n-                if (!isTTLExpired(cur_ttl) || !where_filter_passed)\n-                {\n-                    new_ttl_infos.table_ttl.update(cur_ttl);\n-                    result_column->insertFrom(*values_column, i);\n-                }\n-                else if (it == column_names.begin())\n-                    ++rows_removed;\n-            }\n-            result_columns.emplace_back(std::move(result_column));\n-        }\n-        block = header.cloneWithColumns(std::move(result_columns));\n-    }\n-    else\n+    if (delete_algorithm)\n     {\n-        MutableColumns result_columns = header.cloneEmptyColumns();\n-        MutableColumns aggregate_columns = header.cloneEmptyColumns();\n-\n-        size_t rows_aggregated = 0;\n-        size_t current_key_start = 0;\n-        size_t rows_with_current_key = 0;\n-        auto storage_rows_ttl = metadata_snapshot->getRowsTTL();\n-        for (size_t i = 0; i < block.rows(); ++i)\n-        {\n-            UInt32 cur_ttl = getTimestampByIndex(ttl_column, i);\n-            bool where_filter_passed = !where_result_column || where_result_column->getBool(i);\n-            bool ttl_expired = isTTLExpired(cur_ttl) && where_filter_passed;\n-\n-            bool same_as_current = true;\n-            for (size_t j = 0; j < storage_rows_ttl.group_by_keys.size(); ++j)\n-            {\n-                const String & key_column = storage_rows_ttl.group_by_keys[j];\n-                const IColumn * values_column = block.getByName(key_column).column.get();\n-                if (!same_as_current || (*values_column)[i] != current_key_value[j])\n-                {\n-                    values_column->get(i, current_key_value[j]);\n-                    same_as_current = false;\n-                }\n-            }\n-            if (!same_as_current)\n-            {\n-                if (rows_with_current_key)\n-                    calculateAggregates(aggregate_columns, current_key_start, rows_with_current_key);\n-                finalizeAggregates(result_columns);\n-\n-                current_key_start = rows_aggregated;\n-                rows_with_current_key = 0;\n-            }\n-\n-            if (ttl_expired)\n-            {\n-                ++rows_with_current_key;\n-                ++rows_aggregated;\n-                for (const auto & name : column_names)\n-                {\n-                    const IColumn * values_column = block.getByName(name).column.get();\n-                    auto & column = aggregate_columns[header.getPositionByName(name)];\n-                    column->insertFrom(*values_column, i);\n-                }\n-            }\n-            else\n-            {\n-                new_ttl_infos.table_ttl.update(cur_ttl);\n-                for (const auto & name : column_names)\n-                {\n-                    const IColumn * values_column = block.getByName(name).column.get();\n-                    auto & column = result_columns[header.getPositionByName(name)];\n-                    column->insertFrom(*values_column, i);\n-                }\n-            }\n-        }\n-\n-        if (rows_with_current_key)\n-            calculateAggregates(aggregate_columns, current_key_start, rows_with_current_key);\n-\n-        block = header.cloneWithColumns(std::move(result_columns));\n-    }\n-}\n-\n-void TTLBlockInputStream::calculateAggregates(const MutableColumns & aggregate_columns, size_t start_pos, size_t length)\n-{\n-    Columns aggregate_chunk;\n-    aggregate_chunk.reserve(aggregate_columns.size());\n-    for (const auto & name : header.getNames())\n-    {\n-        const auto & column = aggregate_columns[header.getPositionByName(name)];\n-        ColumnPtr chunk_column = column->cut(start_pos, length);\n-        aggregate_chunk.emplace_back(std::move(chunk_column));\n-    }\n-    aggregator->executeOnBlock(aggregate_chunk, length, agg_result, agg_key_columns,\n-                               agg_aggregate_columns, agg_no_more_keys);\n-}\n-\n-void TTLBlockInputStream::finalizeAggregates(MutableColumns & result_columns)\n-{\n-    if (!agg_result.empty())\n-    {\n-        auto aggregated_res = aggregator->convertToBlocks(agg_result, true, 1);\n-        auto storage_rows_ttl = metadata_snapshot->getRowsTTL();\n-        for (auto & agg_block : aggregated_res)\n-        {\n-            for (const auto & it : storage_rows_ttl.set_parts)\n-                it.expression->execute(agg_block);\n-            for (const auto & name : storage_rows_ttl.group_by_keys)\n-            {\n-                const IColumn * values_column = agg_block.getByName(name).column.get();\n-                auto & result_column = result_columns[header.getPositionByName(name)];\n-                result_column->insertRangeFrom(*values_column, 0, agg_block.rows());\n-            }\n-            for (const auto & it : storage_rows_ttl.set_parts)\n-            {\n-                const IColumn * values_column = agg_block.getByName(it.expression_result_column_name).column.get();\n-                auto & result_column = result_columns[header.getPositionByName(it.column_name)];\n-                result_column->insertRangeFrom(*values_column, 0, agg_block.rows());\n-            }\n-        }\n-    }\n-    agg_result.invalidate();\n-}\n-\n-void TTLBlockInputStream::removeValuesWithExpiredColumnTTL(Block & block)\n-{\n-    Block block_with_defaults;\n-    if (defaults_expression)\n-    {\n-        block_with_defaults = block;\n-        defaults_expression->execute(block_with_defaults);\n-    }\n-\n-    std::vector<String> columns_to_remove;\n-    for (const auto & [name, ttl_entry] : metadata_snapshot->getColumnTTLs())\n-    {\n-        /// If we read not all table columns. E.g. while mutation.\n-        if (!block.has(name))\n-            continue;\n-\n-        const auto & old_ttl_info = old_ttl_infos.columns_ttl[name];\n-        auto & new_ttl_info = new_ttl_infos.columns_ttl[name];\n-\n-        /// Nothing to do\n-        if (!force && !isTTLExpired(old_ttl_info.min))\n-            continue;\n-\n-        /// Later drop full column\n-        if (isTTLExpired(old_ttl_info.max))\n-            continue;\n-\n-        if (!block.has(ttl_entry.result_column))\n-        {\n-            columns_to_remove.push_back(ttl_entry.result_column);\n-            ttl_entry.expression->execute(block);\n-        }\n-\n-        ColumnPtr default_column = nullptr;\n-        if (block_with_defaults.has(name))\n-            default_column = block_with_defaults.getByName(name).column->convertToFullColumnIfConst();\n-\n-        auto & column_with_type = block.getByName(name);\n-        const IColumn * values_column = column_with_type.column.get();\n-        MutableColumnPtr result_column = values_column->cloneEmpty();\n-        result_column->reserve(block.rows());\n-\n-        const IColumn * ttl_column = block.getByName(ttl_entry.result_column).column.get();\n-\n-        for (size_t i = 0; i < block.rows(); ++i)\n-        {\n-            UInt32 cur_ttl = getTimestampByIndex(ttl_column, i);\n-            if (isTTLExpired(cur_ttl))\n-            {\n-                if (default_column)\n-                    result_column->insertFrom(*default_column, i);\n-                else\n-                    result_column->insertDefault();\n-            }\n-            else\n-            {\n-                new_ttl_info.update(cur_ttl);\n-                empty_columns.erase(name);\n-                result_column->insertFrom(*values_column, i);\n-            }\n-        }\n-        column_with_type.column = std::move(result_column);\n-    }\n-\n-    for (const String & column : columns_to_remove)\n-        block.erase(column);\n-}\n-\n-void TTLBlockInputStream::updateTTLWithDescriptions(Block & block, const TTLDescriptions & descriptions, TTLInfoMap & ttl_info_map)\n-{\n-    std::vector<String> columns_to_remove;\n-    for (const auto & ttl_entry : descriptions)\n-    {\n-        auto & new_ttl_info = ttl_info_map[ttl_entry.result_column];\n-        if (!block.has(ttl_entry.result_column))\n-        {\n-            columns_to_remove.push_back(ttl_entry.result_column);\n-            ttl_entry.expression->execute(block);\n-        }\n-\n-        const IColumn * ttl_column = block.getByName(ttl_entry.result_column).column.get();\n-\n-        for (size_t i = 0; i < block.rows(); ++i)\n-        {\n-            UInt32 cur_ttl = getTimestampByIndex(ttl_column, i);\n-            new_ttl_info.update(cur_ttl);\n-        }\n-    }\n-\n-    for (const String & column : columns_to_remove)\n-        block.erase(column);\n-}\n-\n-void TTLBlockInputStream::updateMovesTTL(Block & block)\n-{\n-    updateTTLWithDescriptions(block, metadata_snapshot->getMoveTTLs(), new_ttl_infos.moves_ttl);\n-}\n-\n-void TTLBlockInputStream::updateRecompressionTTL(Block & block)\n-{\n-    updateTTLWithDescriptions(block, metadata_snapshot->getRecompressionTTLs(), new_ttl_infos.recompression_ttl);\n-}\n-\n-UInt32 TTLBlockInputStream::getTimestampByIndex(const IColumn * column, size_t ind)\n-{\n-    if (const ColumnUInt16 * column_date = typeid_cast<const ColumnUInt16 *>(column))\n-        return date_lut.fromDayNum(DayNum(column_date->getData()[ind]));\n-    else if (const ColumnUInt32 * column_date_time = typeid_cast<const ColumnUInt32 *>(column))\n-        return column_date_time->getData()[ind];\n-    else if (const ColumnConst * column_const = typeid_cast<const ColumnConst *>(column))\n-    {\n-        if (typeid_cast<const ColumnUInt16 *>(&column_const->getDataColumn()))\n-            return date_lut.fromDayNum(DayNum(column_const->getValue<UInt16>()));\n-        else if (typeid_cast<const ColumnUInt32 *>(&column_const->getDataColumn()))\n-            return column_const->getValue<UInt32>();\n+        size_t rows_removed = all_data_dropped ? data_part->rows_count : delete_algorithm->getNumberOfRemovedRows();\n+        LOG_DEBUG(log, \"Removed {} rows with expired TTL from part {}\", rows_removed, data_part->name);\n     }\n-\n-    throw Exception(\"Unexpected type of result TTL column\", ErrorCodes::LOGICAL_ERROR);\n }\n \n }\ndiff --git a/src/DataStreams/TTLBlockInputStream.h b/src/DataStreams/TTLBlockInputStream.h\nindex 1d3b69f61c5f..da86b8d5710c 100644\n--- a/src/DataStreams/TTLBlockInputStream.h\n+++ b/src/DataStreams/TTLBlockInputStream.h\n@@ -3,8 +3,9 @@\n #include <Storages/MergeTree/MergeTreeData.h>\n #include <Storages/MergeTree/IMergeTreeDataPart.h>\n #include <Core/Block.h>\n-#include <Interpreters/Aggregator.h>\n #include <Storages/MergeTree/MergeTreeDataPartTTLInfo.h>\n+#include <DataStreams/ITTLAlgorithm.h>\n+#include <DataStreams/TTLDeleteAlgorithm.h>\n \n #include <common/DateLUT.h>\n \n@@ -24,7 +25,6 @@ class TTLBlockInputStream : public IBlockInputStream\n     );\n \n     String getName() const override { return \"TTL\"; }\n-\n     Block getHeader() const override { return header; }\n \n protected:\n@@ -34,60 +34,14 @@ class TTLBlockInputStream : public IBlockInputStream\n     void readSuffixImpl() override;\n \n private:\n-    const MergeTreeData & storage;\n-    StorageMetadataPtr metadata_snapshot;\n+    std::vector<TTLAlgorithmPtr> algorithms;\n+    const TTLDeleteAlgorithm * delete_algorithm = nullptr;\n+    bool all_data_dropped = false;\n \n     /// ttl_infos and empty_columns are updating while reading\n     const MergeTreeData::MutableDataPartPtr & data_part;\n-\n-    time_t current_time;\n-    bool force;\n-\n-    std::unique_ptr<Aggregator> aggregator;\n-    std::vector<Field> current_key_value;\n-    AggregatedDataVariants agg_result;\n-    ColumnRawPtrs agg_key_columns;\n-    Aggregator::AggregateColumns agg_aggregate_columns;\n-    bool agg_no_more_keys = false;\n-\n-    IMergeTreeDataPart::TTLInfos old_ttl_infos;\n-    IMergeTreeDataPart::TTLInfos new_ttl_infos;\n-    NameSet empty_columns;\n-\n-    size_t rows_removed = 0;\n     Poco::Logger * log;\n-    const DateLUTImpl & date_lut;\n-\n-    /// TODO rewrite defaults logic to evaluteMissingDefaults\n-    std::unordered_map<String, String> defaults_result_column;\n-    ExpressionActionsPtr defaults_expression;\n-\n     Block header;\n-private:\n-    /// Removes values with expired ttl and computes new_ttl_infos and empty_columns for part\n-    void removeValuesWithExpiredColumnTTL(Block & block);\n-\n-    /// Removes rows with expired table ttl and computes new ttl_infos for part\n-    void removeRowsWithExpiredTableTTL(Block & block);\n-\n-    // Calculate aggregates of aggregate_columns into agg_result\n-    void calculateAggregates(const MutableColumns & aggregate_columns, size_t start_pos, size_t length);\n-\n-    /// Finalize agg_result into result_columns\n-    void finalizeAggregates(MutableColumns & result_columns);\n-\n-    /// Execute description expressions on block and update ttl's in\n-    /// ttl_info_map with expression results.\n-    void updateTTLWithDescriptions(Block & block, const TTLDescriptions & descriptions, TTLInfoMap & ttl_info_map);\n-\n-    /// Updates TTL for moves\n-    void updateMovesTTL(Block & block);\n-\n-    /// Update values for recompression TTL using data from block.\n-    void updateRecompressionTTL(Block & block);\n-\n-    UInt32 getTimestampByIndex(const IColumn * column, size_t ind);\n-    bool isTTLExpired(time_t ttl) const;\n };\n \n }\ndiff --git a/src/DataStreams/TTLColumnAlgorithm.cpp b/src/DataStreams/TTLColumnAlgorithm.cpp\nnew file mode 100644\nindex 000000000000..140631ac0bf1\n--- /dev/null\n+++ b/src/DataStreams/TTLColumnAlgorithm.cpp\n@@ -0,0 +1,83 @@\n+#include <DataStreams/TTLColumnAlgorithm.h>\n+\n+namespace DB\n+{\n+\n+TTLColumnAlgorithm::TTLColumnAlgorithm(\n+    const TTLDescription & description_,\n+    const TTLInfo & old_ttl_info_,\n+    time_t current_time_,\n+    bool force_,\n+    const String & column_name_,\n+    const ExpressionActionsPtr & default_expression_,\n+    const String & default_column_name_)\n+    : ITTLAlgorithm(description_, old_ttl_info_, current_time_, force_)\n+    , column_name(column_name_)\n+    , default_expression(default_expression_)\n+    , default_column_name(default_column_name_)\n+{\n+    if (!isMinTTLExpired())\n+    {\n+        new_ttl_info = old_ttl_info;\n+        is_fully_empty = false;\n+    }\n+}\n+\n+void TTLColumnAlgorithm::execute(Block & block)\n+{\n+    if (!block)\n+        return;\n+\n+    /// If we read not all table columns. E.g. while mutation.\n+    if (!block.has(column_name))\n+        return;\n+\n+    /// Nothing to do\n+    if (!isMinTTLExpired())\n+        return;\n+\n+    /// Later drop full column\n+    if (isMaxTTLExpired())\n+        return;\n+\n+    auto default_column = executeExpressionAndGetColumn(default_expression, block, default_column_name);\n+    if (default_column)\n+        default_column = default_column->convertToFullColumnIfConst();\n+\n+    auto ttl_column = executeExpressionAndGetColumn(description.expression, block, description.result_column);\n+\n+    auto & column_with_type = block.getByName(column_name);\n+    const IColumn * values_column = column_with_type.column.get();\n+    MutableColumnPtr result_column = values_column->cloneEmpty();\n+    result_column->reserve(block.rows());\n+\n+    for (size_t i = 0; i < block.rows(); ++i)\n+    {\n+        UInt32 cur_ttl = getTimestampByIndex(ttl_column.get(), i);\n+        if (isTTLExpired(cur_ttl))\n+        {\n+            if (default_column)\n+                result_column->insertFrom(*default_column, i);\n+            else\n+                result_column->insertDefault();\n+        }\n+        else\n+        {\n+            new_ttl_info.update(cur_ttl);\n+            is_fully_empty = false;\n+            result_column->insertFrom(*values_column, i);\n+        }\n+    }\n+\n+    column_with_type.column = std::move(result_column);\n+}\n+\n+void TTLColumnAlgorithm::finalize(const MutableDataPartPtr & data_part) const\n+{\n+    data_part->ttl_infos.columns_ttl[column_name] = new_ttl_info;\n+    data_part->ttl_infos.updatePartMinMaxTTL(new_ttl_info.min, new_ttl_info.max);\n+    if (is_fully_empty)\n+        data_part->expired_columns.insert(column_name);\n+}\n+\n+}\ndiff --git a/src/DataStreams/TTLColumnAlgorithm.h b/src/DataStreams/TTLColumnAlgorithm.h\nnew file mode 100644\nindex 000000000000..e09dd663af0b\n--- /dev/null\n+++ b/src/DataStreams/TTLColumnAlgorithm.h\n@@ -0,0 +1,33 @@\n+#pragma once\n+\n+#include <DataStreams/ITTLAlgorithm.h>\n+\n+namespace DB\n+{\n+\n+/// Deletes (replaces to default) values in column according to column's TTL description.\n+/// If all values in column are replaced with defaults, this column won't be written to part.\n+class TTLColumnAlgorithm final : public ITTLAlgorithm\n+{\n+public:\n+    TTLColumnAlgorithm(\n+        const TTLDescription & description_,\n+        const TTLInfo & old_ttl_info_,\n+        time_t current_time_,\n+        bool force_,\n+        const String & column_name_,\n+        const ExpressionActionsPtr & default_expression_,\n+        const String & default_column_name_);\n+\n+    void execute(Block & block) override;\n+    void finalize(const MutableDataPartPtr & data_part) const override;\n+\n+private:\n+    const String column_name;\n+    const ExpressionActionsPtr default_expression;\n+    const String default_column_name;\n+\n+    bool is_fully_empty = true;\n+};\n+\n+}\ndiff --git a/src/DataStreams/TTLDeleteAlgorithm.cpp b/src/DataStreams/TTLDeleteAlgorithm.cpp\nnew file mode 100644\nindex 000000000000..c364bb06f3e7\n--- /dev/null\n+++ b/src/DataStreams/TTLDeleteAlgorithm.cpp\n@@ -0,0 +1,62 @@\n+#include <DataStreams/TTLDeleteAlgorithm.h>\n+\n+namespace DB\n+{\n+\n+TTLDeleteAlgorithm::TTLDeleteAlgorithm(\n+    const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_)\n+    : ITTLAlgorithm(description_, old_ttl_info_, current_time_, force_)\n+{\n+    if (!isMinTTLExpired())\n+        new_ttl_info = old_ttl_info;\n+}\n+\n+void TTLDeleteAlgorithm::execute(Block & block)\n+{\n+    if (!block || !isMinTTLExpired())\n+        return;\n+\n+    auto ttl_column = executeExpressionAndGetColumn(description.expression, block, description.result_column);\n+    auto where_column = executeExpressionAndGetColumn(description.where_expression, block, description.where_result_column);\n+\n+    MutableColumns result_columns;\n+    const auto & column_names = block.getNames();\n+\n+    result_columns.reserve(column_names.size());\n+    for (auto it = column_names.begin(); it != column_names.end(); ++it)\n+    {\n+        const IColumn * values_column = block.getByName(*it).column.get();\n+        MutableColumnPtr result_column = values_column->cloneEmpty();\n+        result_column->reserve(block.rows());\n+\n+        for (size_t i = 0; i < block.rows(); ++i)\n+        {\n+            UInt32 cur_ttl = getTimestampByIndex(ttl_column.get(), i);\n+            bool where_filter_passed = !where_column || where_column->getBool(i);\n+\n+            if (!isTTLExpired(cur_ttl) || !where_filter_passed)\n+            {\n+                new_ttl_info.update(cur_ttl);\n+                result_column->insertFrom(*values_column, i);\n+            }\n+            else if (it == column_names.begin())\n+                ++rows_removed;\n+        }\n+\n+        result_columns.emplace_back(std::move(result_column));\n+    }\n+\n+    block = block.cloneWithColumns(std::move(result_columns));\n+}\n+\n+void TTLDeleteAlgorithm::finalize(const MutableDataPartPtr & data_part) const\n+{\n+    if (description.where_expression)\n+        data_part->ttl_infos.rows_where_ttl[description.result_column] = new_ttl_info;\n+    else\n+        data_part->ttl_infos.table_ttl = new_ttl_info;\n+\n+    data_part->ttl_infos.updatePartMinMaxTTL(new_ttl_info.min, new_ttl_info.max);\n+}\n+\n+}\ndiff --git a/src/DataStreams/TTLDeleteAlgorithm.h b/src/DataStreams/TTLDeleteAlgorithm.h\nnew file mode 100644\nindex 000000000000..8ab3f8b63e85\n--- /dev/null\n+++ b/src/DataStreams/TTLDeleteAlgorithm.h\n@@ -0,0 +1,23 @@\n+#pragma once\n+\n+#include <DataStreams/ITTLAlgorithm.h>\n+\n+namespace DB\n+{\n+\n+/// Deletes rows according to table TTL description with\n+/// possible optional condition in 'WHERE' clause.\n+class TTLDeleteAlgorithm final : public ITTLAlgorithm\n+{\n+public:\n+    TTLDeleteAlgorithm(const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_);\n+\n+    void execute(Block & block) override;\n+    void finalize(const MutableDataPartPtr & data_part) const override;\n+    size_t getNumberOfRemovedRows() const { return rows_removed; }\n+\n+private:\n+    size_t rows_removed = 0;\n+};\n+\n+}\ndiff --git a/src/DataStreams/TTLUpdateInfoAlgorithm.cpp b/src/DataStreams/TTLUpdateInfoAlgorithm.cpp\nnew file mode 100644\nindex 000000000000..d5feb14658b3\n--- /dev/null\n+++ b/src/DataStreams/TTLUpdateInfoAlgorithm.cpp\n@@ -0,0 +1,47 @@\n+#include <DataStreams/TTLUpdateInfoAlgorithm.h>\n+\n+namespace DB\n+{\n+\n+TTLUpdateInfoAlgorithm::TTLUpdateInfoAlgorithm(\n+    const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_)\n+    : ITTLAlgorithm(description_, old_ttl_info_, current_time_, force_)\n+{\n+}\n+\n+void TTLUpdateInfoAlgorithm::execute(Block & block)\n+{\n+    if (!block)\n+        return;\n+\n+    auto ttl_column = executeExpressionAndGetColumn(description.expression, block, description.result_column);\n+    for (size_t i = 0; i < block.rows(); ++i)\n+    {\n+        UInt32 cur_ttl = ITTLAlgorithm::getTimestampByIndex(ttl_column.get(), i);\n+        new_ttl_info.update(cur_ttl);\n+    }\n+}\n+\n+TTLMoveAlgorithm::TTLMoveAlgorithm(\n+    const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_)\n+    : TTLUpdateInfoAlgorithm(description_, old_ttl_info_, current_time_, force_)\n+{\n+}\n+\n+void TTLMoveAlgorithm::finalize(const MutableDataPartPtr & data_part) const\n+{\n+    data_part->ttl_infos.moves_ttl[description.result_column] = new_ttl_info;\n+}\n+\n+TTLRecompressionAlgorithm::TTLRecompressionAlgorithm(\n+    const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_)\n+    : TTLUpdateInfoAlgorithm(description_, old_ttl_info_, current_time_, force_)\n+{\n+}\n+\n+void TTLRecompressionAlgorithm::finalize(const MutableDataPartPtr & data_part) const\n+{\n+    data_part->ttl_infos.recompression_ttl[description.result_column] = new_ttl_info;\n+}\n+\n+}\ndiff --git a/src/DataStreams/TTLUpdateInfoAlgorithm.h b/src/DataStreams/TTLUpdateInfoAlgorithm.h\nnew file mode 100644\nindex 000000000000..c1ef0e1c90dd\n--- /dev/null\n+++ b/src/DataStreams/TTLUpdateInfoAlgorithm.h\n@@ -0,0 +1,32 @@\n+#pragma once\n+\n+#include <DataStreams/ITTLAlgorithm.h>\n+\n+namespace DB\n+{\n+\n+/// Calculates new ttl_info and does nothing with data.\n+class TTLUpdateInfoAlgorithm : public ITTLAlgorithm\n+{\n+public:\n+    TTLUpdateInfoAlgorithm(const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_);\n+\n+    void execute(Block & block) override;\n+    void finalize(const MutableDataPartPtr & data_part) const override = 0;\n+};\n+\n+class TTLMoveAlgorithm final : public TTLUpdateInfoAlgorithm\n+{\n+public:\n+    TTLMoveAlgorithm(const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_);\n+    void finalize(const MutableDataPartPtr & data_part) const override;\n+};\n+\n+class TTLRecompressionAlgorithm final : public TTLUpdateInfoAlgorithm\n+{\n+public:\n+    TTLRecompressionAlgorithm(const TTLDescription & description_, const TTLInfo & old_ttl_info_, time_t current_time_, bool force_);\n+    void finalize(const MutableDataPartPtr & data_part) const override;\n+};\n+\n+}\ndiff --git a/src/DataStreams/ya.make b/src/DataStreams/ya.make\nindex 0af72f25d3d7..29e6eb3afc3f 100644\n--- a/src/DataStreams/ya.make\n+++ b/src/DataStreams/ya.make\n@@ -27,6 +27,7 @@ SRCS(\n     ExecutionSpeedLimits.cpp\n     ExpressionBlockInputStream.cpp\n     IBlockInputStream.cpp\n+    ITTLAlgorithm.cpp\n     InputStreamFromASTInsertQuery.cpp\n     InternalTextLogsRowOutputStream.cpp\n     LimitBlockInputStream.cpp\n@@ -44,7 +45,11 @@ SRCS(\n     SquashingBlockInputStream.cpp\n     SquashingBlockOutputStream.cpp\n     SquashingTransform.cpp\n+    TTLAggregationAlgorithm.cpp\n     TTLBlockInputStream.cpp\n+    TTLColumnAlgorithm.cpp\n+    TTLDeleteAlgorithm.cpp\n+    TTLUpdateInfoAlgorithm.cpp\n     copyData.cpp\n     finalizeBlock.cpp\n     materializeBlock.cpp\ndiff --git a/src/Parsers/ASTTTLElement.cpp b/src/Parsers/ASTTTLElement.cpp\nindex 39283a3168e6..2d22c1b43078 100644\n--- a/src/Parsers/ASTTTLElement.cpp\n+++ b/src/Parsers/ASTTTLElement.cpp\n@@ -20,7 +20,7 @@ ASTPtr ASTTTLElement::clone() const\n \n     for (auto & expr : clone->group_by_key)\n         expr = expr->clone();\n-    for (auto & [name, expr] : clone->group_by_aggregations)\n+    for (auto & expr : clone->group_by_assignments)\n         expr = expr->clone();\n \n     return clone;\n@@ -46,15 +46,15 @@ void ASTTTLElement::formatImpl(const FormatSettings & settings, FormatState & st\n                 settings.ostr << \", \";\n             (*it)->formatImpl(settings, state, frame);\n         }\n-        if (!group_by_aggregations.empty())\n+\n+        if (!group_by_assignments.empty())\n         {\n             settings.ostr << \" SET \";\n-            for (auto it = group_by_aggregations.begin(); it != group_by_aggregations.end(); ++it)\n+            for (auto it = group_by_assignments.begin(); it != group_by_assignments.end(); ++it)\n             {\n-                if (it != group_by_aggregations.begin())\n+                if (it != group_by_assignments.begin())\n                     settings.ostr << \", \";\n-                settings.ostr << it->first << \" = \";\n-                it->second->formatImpl(settings, state, frame);\n+                (*it)->formatImpl(settings, state, frame);\n             }\n         }\n     }\ndiff --git a/src/Parsers/ASTTTLElement.h b/src/Parsers/ASTTTLElement.h\nindex aadd019b59ca..ce011d76c7b4 100644\n--- a/src/Parsers/ASTTTLElement.h\n+++ b/src/Parsers/ASTTTLElement.h\n@@ -18,7 +18,7 @@ class ASTTTLElement : public IAST\n     String destination_name;\n \n     ASTs group_by_key;\n-    std::vector<std::pair<String, ASTPtr>> group_by_aggregations;\n+    ASTs group_by_assignments;\n \n     ASTPtr recompression_codec;\n \ndiff --git a/src/Parsers/ExpressionElementParsers.cpp b/src/Parsers/ExpressionElementParsers.cpp\nindex 39f8a3c951cd..420c3ac69c7f 100644\n--- a/src/Parsers/ExpressionElementParsers.cpp\n+++ b/src/Parsers/ExpressionElementParsers.cpp\n@@ -24,6 +24,7 @@\n #include <Parsers/ASTTTLElement.h>\n #include <Parsers/ASTWindowDefinition.h>\n #include <Parsers/IAST.h>\n+#include <Parsers/ASTAssignment.h>\n \n #include <Parsers/parseIdentifierOrStringLiteral.h>\n #include <Parsers/parseIntervalKind.h>\n@@ -2008,9 +2009,12 @@ bool ParserTTLElement::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n     ParserIdentifier parser_identifier;\n     ParserStringLiteral parser_string_literal;\n     ParserExpression parser_exp;\n-    ParserExpressionList parser_expression_list(false);\n+    ParserExpressionList parser_keys_list(false);\n     ParserCodec parser_codec;\n \n+    ParserList parser_assignment_list(\n+        std::make_unique<ParserAssignment>(), std::make_unique<ParserToken>(TokenType::Comma));\n+\n     ASTPtr ttl_expr;\n     if (!parser_exp.parse(pos, ttl_expr, expected))\n         return false;\n@@ -2044,9 +2048,9 @@ bool ParserTTLElement::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n     }\n \n     ASTPtr where_expr;\n-    ASTPtr ast_group_by_key;\n+    ASTPtr group_by_key;\n     ASTPtr recompression_codec;\n-    std::vector<std::pair<String, ASTPtr>> group_by_aggregations;\n+    ASTPtr group_by_assignments;\n \n     if (mode == TTLMode::MOVE)\n     {\n@@ -2058,30 +2062,13 @@ bool ParserTTLElement::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n     }\n     else if (mode == TTLMode::GROUP_BY)\n     {\n-        if (!parser_expression_list.parse(pos, ast_group_by_key, expected))\n+        if (!parser_keys_list.parse(pos, group_by_key, expected))\n             return false;\n \n         if (s_set.ignore(pos))\n         {\n-            while (true)\n-            {\n-                if (!group_by_aggregations.empty() && !s_comma.ignore(pos))\n-                    break;\n-\n-                ASTPtr name;\n-                ASTPtr value;\n-                if (!parser_identifier.parse(pos, name, expected))\n-                    return false;\n-                if (!s_eq.ignore(pos))\n-                    return false;\n-                if (!parser_exp.parse(pos, value, expected))\n-                    return false;\n-\n-                String name_str;\n-                if (!tryGetIdentifierNameInto(name, name_str))\n-                    return false;\n-                group_by_aggregations.emplace_back(name_str, std::move(value));\n-            }\n+            if (!parser_assignment_list.parse(pos, group_by_assignments, expected))\n+                return false;\n         }\n     }\n     else if (mode == TTLMode::DELETE && s_where.ignore(pos))\n@@ -2105,8 +2092,9 @@ bool ParserTTLElement::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n \n     if (mode == TTLMode::GROUP_BY)\n     {\n-        ttl_element->group_by_key = std::move(ast_group_by_key->children);\n-        ttl_element->group_by_aggregations = std::move(group_by_aggregations);\n+        ttl_element->group_by_key = std::move(group_by_key->children);\n+        if (group_by_assignments)\n+            ttl_element->group_by_assignments = std::move(group_by_assignments->children);\n     }\n \n     if (mode == TTLMode::RECOMPRESS)\n@@ -2141,4 +2129,31 @@ bool ParserIdentifierWithOptionalParameters::parseImpl(Pos & pos, ASTPtr & node,\n     return false;\n }\n \n+bool ParserAssignment::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n+{\n+    auto assignment = std::make_shared<ASTAssignment>();\n+    node = assignment;\n+\n+    ParserIdentifier p_identifier;\n+    ParserToken s_equals(TokenType::Equals);\n+    ParserExpression p_expression;\n+\n+    ASTPtr column;\n+    if (!p_identifier.parse(pos, column, expected))\n+        return false;\n+\n+    if (!s_equals.ignore(pos, expected))\n+        return false;\n+\n+    ASTPtr expression;\n+    if (!p_expression.parse(pos, expression, expected))\n+        return false;\n+\n+    tryGetIdentifierNameInto(column, assignment->column_name);\n+    if (expression)\n+        assignment->children.push_back(expression);\n+\n+    return true;\n+}\n+\n }\ndiff --git a/src/Parsers/ExpressionElementParsers.h b/src/Parsers/ExpressionElementParsers.h\nindex 6369e14aa58e..ba18fc2cdddb 100644\n--- a/src/Parsers/ExpressionElementParsers.h\n+++ b/src/Parsers/ExpressionElementParsers.h\n@@ -483,4 +483,12 @@ class ParserTTLElement : public IParserBase\n     bool parseImpl(Pos & pos, ASTPtr & node, Expected & expected) override;\n };\n \n+/// Part of the UPDATE command or TTL with GROUP BY of the form: col_name = expr\n+class ParserAssignment : public IParserBase\n+{\n+protected:\n+    const char * getName() const  override{ return \"column assignment\"; }\n+    bool parseImpl(Pos & pos, ASTPtr & node, Expected & expected) override;\n+};\n+\n }\ndiff --git a/src/Parsers/ParserAlterQuery.cpp b/src/Parsers/ParserAlterQuery.cpp\nindex f916537f4382..5d20e27e4862 100644\n--- a/src/Parsers/ParserAlterQuery.cpp\n+++ b/src/Parsers/ParserAlterQuery.cpp\n@@ -11,7 +11,6 @@\n #include <Parsers/ASTIndexDeclaration.h>\n #include <Parsers/ASTAlterQuery.h>\n #include <Parsers/ASTLiteral.h>\n-#include <Parsers/ASTAssignment.h>\n #include <Parsers/parseDatabaseAndTableName.h>\n \n \n@@ -651,34 +650,6 @@ bool ParserAlterCommandList::parseImpl(Pos & pos, ASTPtr & node, Expected & expe\n }\n \n \n-bool ParserAssignment::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n-{\n-    auto assignment = std::make_shared<ASTAssignment>();\n-    node = assignment;\n-\n-    ParserIdentifier p_identifier;\n-    ParserToken s_equals(TokenType::Equals);\n-    ParserExpression p_expression;\n-\n-    ASTPtr column;\n-    if (!p_identifier.parse(pos, column, expected))\n-        return false;\n-\n-    if (!s_equals.ignore(pos, expected))\n-        return false;\n-\n-    ASTPtr expression;\n-    if (!p_expression.parse(pos, expression, expected))\n-        return false;\n-\n-    tryGetIdentifierNameInto(column, assignment->column_name);\n-    if (expression)\n-        assignment->children.push_back(expression);\n-\n-    return true;\n-}\n-\n-\n bool ParserAlterQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n {\n     auto query = std::make_shared<ASTAlterQuery>();\ndiff --git a/src/Parsers/ParserAlterQuery.h b/src/Parsers/ParserAlterQuery.h\nindex 514ef8764309..b22b1c6ded2e 100644\n--- a/src/Parsers/ParserAlterQuery.h\n+++ b/src/Parsers/ParserAlterQuery.h\n@@ -63,12 +63,4 @@ class ParserAlterCommand : public IParserBase\n };\n \n \n-/// Part of the UPDATE command of the form: col_name = expr\n-class ParserAssignment : public IParserBase\n-{\n-protected:\n-    const char * getName() const  override{ return \"column assignment\"; }\n-    bool parseImpl(Pos & pos, ASTPtr & node, Expected & expected) override;\n-};\n-\n }\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex 7c37a0673608..2f6513bbb12c 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -1278,6 +1278,18 @@ bool IMergeTreeDataPart::checkAllTTLCalculated(const StorageMetadataPtr & metada\n             return false;\n     }\n \n+    for (const auto & group_by_desc : metadata_snapshot->getGroupByTTLs())\n+    {\n+        if (!ttl_infos.group_by_ttl.count(group_by_desc.result_column))\n+            return false;\n+    }\n+\n+    for (const auto & rows_where_desc : metadata_snapshot->getRowsWhereTTLs())\n+    {\n+        if (!ttl_infos.rows_where_ttl.count(rows_where_desc.result_column))\n+            return false;\n+    }\n+\n     return true;\n }\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.cpp b/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.cpp\nindex 92c8a66e828f..e130fbc17987 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.cpp\n@@ -17,13 +17,23 @@ void MergeTreeDataPartTTLInfos::update(const MergeTreeDataPartTTLInfos & other_i\n         updatePartMinMaxTTL(ttl_info.min, ttl_info.max);\n     }\n \n+    for (const auto & [name, ttl_info] : other_infos.rows_where_ttl)\n+    {\n+        rows_where_ttl[name].update(ttl_info);\n+        updatePartMinMaxTTL(ttl_info.min, ttl_info.max);\n+    }\n+\n+    for (const auto & [name, ttl_info] : other_infos.group_by_ttl)\n+    {\n+        group_by_ttl[name].update(ttl_info);\n+        updatePartMinMaxTTL(ttl_info.min, ttl_info.max);\n+    }\n+\n     for (const auto & [name, ttl_info] : other_infos.recompression_ttl)\n         recompression_ttl[name].update(ttl_info);\n \n     for (const auto & [expression, ttl_info] : other_infos.moves_ttl)\n-    {\n         moves_ttl[expression].update(ttl_info);\n-    }\n \n     table_ttl.update(other_infos.table_ttl);\n     updatePartMinMaxTTL(table_ttl.min, table_ttl.max);\n@@ -59,29 +69,41 @@ void MergeTreeDataPartTTLInfos::read(ReadBuffer & in)\n \n         updatePartMinMaxTTL(table_ttl.min, table_ttl.max);\n     }\n-    if (json.has(\"moves\"))\n+\n+    auto fill_ttl_info_map = [this](const JSON & json_part, TTLInfoMap & ttl_info_map, bool update_min_max)\n     {\n-        const JSON & moves = json[\"moves\"];\n-        for (auto move : moves) // NOLINT\n+        for (auto elem : json_part) // NOLINT\n         {\n             MergeTreeDataPartTTLInfo ttl_info;\n-            ttl_info.min = move[\"min\"].getUInt();\n-            ttl_info.max = move[\"max\"].getUInt();\n-            String expression = move[\"expression\"].getString();\n-            moves_ttl.emplace(expression, ttl_info);\n+            ttl_info.min = elem[\"min\"].getUInt();\n+            ttl_info.max = elem[\"max\"].getUInt();\n+            String expression = elem[\"expression\"].getString();\n+            ttl_info_map.emplace(expression, ttl_info);\n+\n+            if (update_min_max)\n+                updatePartMinMaxTTL(ttl_info.min, ttl_info.max);\n         }\n+    };\n+\n+    if (json.has(\"moves\"))\n+    {\n+        const JSON & moves = json[\"moves\"];\n+        fill_ttl_info_map(moves, moves_ttl, false);\n     }\n     if (json.has(\"recompression\"))\n     {\n         const JSON & recompressions = json[\"recompression\"];\n-        for (auto recompression : recompressions) // NOLINT\n-        {\n-            MergeTreeDataPartTTLInfo ttl_info;\n-            ttl_info.min = recompression[\"min\"].getUInt();\n-            ttl_info.max = recompression[\"max\"].getUInt();\n-            String expression = recompression[\"expression\"].getString();\n-            recompression_ttl.emplace(expression, ttl_info);\n-        }\n+        fill_ttl_info_map(recompressions, recompression_ttl, false);\n+    }\n+    if (json.has(\"group_by\"))\n+    {\n+        const JSON & group_by = json[\"group_by\"];\n+        fill_ttl_info_map(group_by, group_by_ttl, true);\n+    }\n+    if (json.has(\"rows_where\"))\n+    {\n+        const JSON & rows_where = json[\"rows_where\"];\n+        fill_ttl_info_map(rows_where, rows_where_ttl, true);\n     }\n }\n \n@@ -118,14 +140,17 @@ void MergeTreeDataPartTTLInfos::write(WriteBuffer & out) const\n         writeIntText(table_ttl.max, out);\n         writeString(\"}\", out);\n     }\n-    if (!moves_ttl.empty())\n+\n+    auto write_infos = [&out](const TTLInfoMap & infos, const String & type, bool is_first)\n     {\n-        if (!columns_ttl.empty() || table_ttl.min)\n+        if (!is_first)\n             writeString(\",\", out);\n-        writeString(R\"(\"moves\":[)\", out);\n-        for (auto it = moves_ttl.begin(); it != moves_ttl.end(); ++it)\n+\n+        writeDoubleQuotedString(type, out);\n+        writeString(\":[\", out);\n+        for (auto it = infos.begin(); it != infos.end(); ++it)\n         {\n-            if (it != moves_ttl.begin())\n+            if (it != infos.begin())\n                 writeString(\",\", out);\n \n             writeString(R\"({\"expression\":)\", out);\n@@ -137,28 +162,30 @@ void MergeTreeDataPartTTLInfos::write(WriteBuffer & out) const\n             writeString(\"}\", out);\n         }\n         writeString(\"]\", out);\n+    };\n+\n+    bool is_first = columns_ttl.empty() && !table_ttl.min;\n+    if (!moves_ttl.empty())\n+    {\n+        write_infos(moves_ttl, \"moves\", is_first);\n+        is_first = false;\n     }\n+\n     if (!recompression_ttl.empty())\n     {\n-        if (!moves_ttl.empty() || !columns_ttl.empty() || table_ttl.min)\n-            writeString(\",\", out);\n-\n-        writeString(R\"(\"recompression\":[)\", out);\n-        for (auto it = recompression_ttl.begin(); it != recompression_ttl.end(); ++it)\n-        {\n-            if (it != recompression_ttl.begin())\n-                writeString(\",\", out);\n+        write_infos(recompression_ttl, \"recompression\", is_first);\n+        is_first = false;\n+    }\n \n-            writeString(R\"({\"expression\":)\", out);\n-            writeString(doubleQuoteString(it->first), out);\n-            writeString(R\"(,\"min\":)\", out);\n-            writeIntText(it->second.min, out);\n-            writeString(R\"(,\"max\":)\", out);\n-            writeIntText(it->second.max, out);\n-            writeString(\"}\", out);\n-        }\n-        writeString(\"]\", out);\n+    if (!group_by_ttl.empty())\n+    {\n+        write_infos(group_by_ttl, \"group_by\", is_first);\n+        is_first = false;\n     }\n+\n+    if (!rows_where_ttl.empty())\n+        write_infos(rows_where_ttl, \"rows_where\", is_first);\n+\n     writeString(\"}\", out);\n }\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.h b/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.h\nindex 17239e2618a7..9d1606ee44a9 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartTTLInfo.h\n@@ -45,14 +45,17 @@ struct MergeTreeDataPartTTLInfos\n     time_t part_min_ttl = 0;\n     time_t part_max_ttl = 0;\n \n+    TTLInfoMap rows_where_ttl;\n+\n     TTLInfoMap moves_ttl;\n \n     TTLInfoMap recompression_ttl;\n \n+    TTLInfoMap group_by_ttl;\n+\n     /// Return the smallest max recompression TTL value\n     time_t getMinimalMaxRecompressionTTL() const;\n \n-\n     void read(ReadBuffer & in);\n     void write(WriteBuffer & out) const;\n     void update(const MergeTreeDataPartTTLInfos & other_infos);\n@@ -68,6 +71,7 @@ struct MergeTreeDataPartTTLInfos\n \n     bool empty() const\n     {\n+        /// part_min_ttl in minimum of rows, rows_where and group_by TTLs\n         return !part_min_ttl && moves_ttl.empty() && recompression_ttl.empty();\n     }\n };\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex b49c07bc918b..5a9bdd90bc86 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -12,6 +12,7 @@\n #include <IO/WriteHelpers.h>\n #include <Poco/File.h>\n #include <Common/typeid_cast.h>\n+#include <DataStreams/ITTLAlgorithm.h>\n \n #include <Parsers/queryToString.h>\n \n@@ -91,31 +92,23 @@ void updateTTL(\n     const TTLDescription & ttl_entry,\n     IMergeTreeDataPart::TTLInfos & ttl_infos,\n     DB::MergeTreeDataPartTTLInfo & ttl_info,\n-    Block & block,\n+    const Block & block,\n     bool update_part_min_max_ttls)\n {\n-    bool remove_column = false;\n-    if (!block.has(ttl_entry.result_column))\n-    {\n-        ttl_entry.expression->execute(block);\n-        remove_column = true;\n-    }\n-\n-    const auto & current = block.getByName(ttl_entry.result_column);\n+    auto ttl_column = ITTLAlgorithm::executeExpressionAndGetColumn(ttl_entry.expression, block, ttl_entry.result_column);\n \n-    const IColumn * column = current.column.get();\n-    if (const ColumnUInt16 * column_date = typeid_cast<const ColumnUInt16 *>(column))\n+    if (const ColumnUInt16 * column_date = typeid_cast<const ColumnUInt16 *>(ttl_column.get()))\n     {\n         const auto & date_lut = DateLUT::instance();\n         for (const auto & val : column_date->getData())\n             ttl_info.update(date_lut.fromDayNum(DayNum(val)));\n     }\n-    else if (const ColumnUInt32 * column_date_time = typeid_cast<const ColumnUInt32 *>(column))\n+    else if (const ColumnUInt32 * column_date_time = typeid_cast<const ColumnUInt32 *>(ttl_column.get()))\n     {\n         for (const auto & val : column_date_time->getData())\n             ttl_info.update(val);\n     }\n-    else if (const ColumnConst * column_const = typeid_cast<const ColumnConst *>(column))\n+    else if (const ColumnConst * column_const = typeid_cast<const ColumnConst *>(ttl_column.get()))\n     {\n         if (typeid_cast<const ColumnUInt16 *>(&column_const->getDataColumn()))\n         {\n@@ -134,9 +127,6 @@ void updateTTL(\n \n     if (update_part_min_max_ttls)\n         ttl_infos.updatePartMinMaxTTL(ttl_info.min, ttl_info.max);\n-\n-    if (remove_column)\n-        block.erase(ttl_entry.result_column);\n }\n \n }\n@@ -383,6 +373,12 @@ MergeTreeData::MutableDataPartPtr MergeTreeDataWriter::writeTempPart(BlockWithPa\n     if (metadata_snapshot->hasRowsTTL())\n         updateTTL(metadata_snapshot->getRowsTTL(), new_data_part->ttl_infos, new_data_part->ttl_infos.table_ttl, block, true);\n \n+    for (const auto & ttl_entry : metadata_snapshot->getGroupByTTLs())\n+        updateTTL(ttl_entry, new_data_part->ttl_infos, new_data_part->ttl_infos.group_by_ttl[ttl_entry.result_column], block, true);\n+\n+    for (const auto & ttl_entry : metadata_snapshot->getRowsWhereTTLs())\n+        updateTTL(ttl_entry, new_data_part->ttl_infos, new_data_part->ttl_infos.rows_where_ttl[ttl_entry.result_column], block, true);\n+\n     for (const auto & [name, ttl_entry] : metadata_snapshot->getColumnTTLs())\n         updateTTL(ttl_entry, new_data_part->ttl_infos, new_data_part->ttl_infos.columns_ttl[name], block, true);\n \ndiff --git a/src/Storages/StorageInMemoryMetadata.cpp b/src/Storages/StorageInMemoryMetadata.cpp\nindex 2f488ce36c62..871ff38c07f6 100644\n--- a/src/Storages/StorageInMemoryMetadata.cpp\n+++ b/src/Storages/StorageInMemoryMetadata.cpp\n@@ -128,7 +128,7 @@ TTLTableDescription StorageInMemoryMetadata::getTableTTLs() const\n \n bool StorageInMemoryMetadata::hasAnyTableTTL() const\n {\n-    return hasAnyMoveTTL() || hasRowsTTL() || hasAnyRecompressionTTL();\n+    return hasAnyMoveTTL() || hasRowsTTL() || hasAnyRecompressionTTL() || hasAnyGroupByTTL() || hasAnyRowsWhereTTL();\n }\n \n TTLColumnsDescription StorageInMemoryMetadata::getColumnTTLs() const\n@@ -151,6 +151,16 @@ bool StorageInMemoryMetadata::hasRowsTTL() const\n     return table_ttl.rows_ttl.expression != nullptr;\n }\n \n+TTLDescriptions StorageInMemoryMetadata::getRowsWhereTTLs() const\n+{\n+    return table_ttl.rows_where_ttl;\n+}\n+\n+bool StorageInMemoryMetadata::hasAnyRowsWhereTTL() const\n+{\n+    return !table_ttl.rows_where_ttl.empty();\n+}\n+\n TTLDescriptions StorageInMemoryMetadata::getMoveTTLs() const\n {\n     return table_ttl.move_ttl;\n@@ -171,6 +181,16 @@ bool StorageInMemoryMetadata::hasAnyRecompressionTTL() const\n     return !table_ttl.recompression_ttl.empty();\n }\n \n+TTLDescriptions StorageInMemoryMetadata::getGroupByTTLs() const\n+{\n+    return table_ttl.group_by_ttl;\n+}\n+\n+bool StorageInMemoryMetadata::hasAnyGroupByTTL() const\n+{\n+    return !table_ttl.group_by_ttl.empty();\n+}\n+\n ColumnDependencies StorageInMemoryMetadata::getColumnDependencies(const NameSet & updated_columns) const\n {\n     if (updated_columns.empty())\ndiff --git a/src/Storages/StorageInMemoryMetadata.h b/src/Storages/StorageInMemoryMetadata.h\nindex 4c5edf31efec..038416aff7d6 100644\n--- a/src/Storages/StorageInMemoryMetadata.h\n+++ b/src/Storages/StorageInMemoryMetadata.h\n@@ -109,6 +109,9 @@ struct StorageInMemoryMetadata\n     TTLDescription getRowsTTL() const;\n     bool hasRowsTTL() const;\n \n+    TTLDescriptions getRowsWhereTTLs() const;\n+    bool hasAnyRowsWhereTTL() const;\n+\n     /// Just wrapper for table TTLs, return moves (to disks or volumes) parts of\n     /// table TTL.\n     TTLDescriptions getMoveTTLs() const;\n@@ -118,6 +121,10 @@ struct StorageInMemoryMetadata\n     TTLDescriptions getRecompressionTTLs() const;\n     bool hasAnyRecompressionTTL() const;\n \n+    // Just wrapper for table TTLs, return info about recompression ttl\n+    TTLDescriptions getGroupByTTLs() const;\n+    bool hasAnyGroupByTTL() const;\n+\n     /// Returns columns, which will be needed to calculate dependencies (skip\n     /// indices, TTL expressions) if we update @updated_columns set of columns.\n     ColumnDependencies getColumnDependencies(const NameSet & updated_columns) const;\ndiff --git a/src/Storages/System/StorageSystemParts.cpp b/src/Storages/System/StorageSystemParts.cpp\nindex 5b9461b5c259..bc5a96c6159a 100644\n--- a/src/Storages/System/StorageSystemParts.cpp\n+++ b/src/Storages/System/StorageSystemParts.cpp\n@@ -68,6 +68,14 @@ StorageSystemParts::StorageSystemParts(const StorageID & table_id_)\n         {\"recompression_ttl_info.expression\",           std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>())},\n         {\"recompression_ttl_info.min\",                  std::make_shared<DataTypeArray>(std::make_shared<DataTypeDateTime>())},\n         {\"recompression_ttl_info.max\",                  std::make_shared<DataTypeArray>(std::make_shared<DataTypeDateTime>())},\n+\n+        {\"group_by_ttl_info.expression\",                std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>())},\n+        {\"group_by_ttl_info.min\",                       std::make_shared<DataTypeArray>(std::make_shared<DataTypeDateTime>())},\n+        {\"group_by_ttl_info.max\",                       std::make_shared<DataTypeArray>(std::make_shared<DataTypeDateTime>())},\n+\n+        {\"rows_where_ttl_info.expression\",              std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>())},\n+        {\"rows_where_ttl_info.min\",                     std::make_shared<DataTypeArray>(std::make_shared<DataTypeDateTime>())},\n+        {\"rows_where_ttl_info.max\",                     std::make_shared<DataTypeArray>(std::make_shared<DataTypeDateTime>())}\n     }\n     )\n {\n@@ -181,6 +189,8 @@ void StorageSystemParts::processNextStorage(MutableColumns & columns_, const Sto\n         columns_[i++]->insert(queryToString(part->default_codec->getCodecDesc()));\n \n         add_ttl_info_map(part->ttl_infos.recompression_ttl);\n+        add_ttl_info_map(part->ttl_infos.group_by_ttl);\n+        add_ttl_info_map(part->ttl_infos.rows_where_ttl);\n \n         /// _state column should be the latest.\n         if (has_state_column)\ndiff --git a/src/Storages/TTLDescription.cpp b/src/Storages/TTLDescription.cpp\nindex b1c6a033d8bf..41c20b2714bd 100644\n--- a/src/Storages/TTLDescription.cpp\n+++ b/src/Storages/TTLDescription.cpp\n@@ -1,15 +1,21 @@\n #include <Storages/TTLDescription.h>\n \n+#include <AggregateFunctions/AggregateFunctionFactory.h>\n #include <Functions/IFunction.h>\n #include <Interpreters/ExpressionAnalyzer.h>\n #include <Interpreters/TreeRewriter.h>\n+#include <Interpreters/InDepthNodeVisitor.h>\n+#include <Interpreters/addTypeConversionToAST.h>\n #include <Parsers/ASTExpressionList.h>\n #include <Parsers/ASTFunction.h>\n #include <Parsers/ASTTTLElement.h>\n #include <Parsers/ASTIdentifier.h>\n+#include <Parsers/ASTAssignment.h>\n+#include <Parsers/ASTLiteral.h>\n #include <Storages/ColumnsDescription.h>\n #include <Interpreters/Context.h>\n \n+#include <Parsers/queryToString.h>\n \n #include <DataTypes/DataTypeDate.h>\n #include <DataTypes/DataTypeDateTime.h>\n@@ -77,6 +83,24 @@ void checkTTLExpression(const ExpressionActionsPtr & ttl_expression, const Strin\n     }\n }\n \n+class FindAggregateFunctionData\n+{\n+public:\n+    using TypeToVisit = ASTFunction;\n+    bool has_aggregate_function = false;\n+\n+    void visit(const ASTFunction & func, ASTPtr &)\n+    {\n+        /// Do not throw if found aggregate function inside another aggregate function,\n+        /// because it will be checked, while creating expressions.\n+        if (AggregateFunctionFactory::instance().isAggregateFunctionName(func.name))\n+            has_aggregate_function = true;\n+    }\n+};\n+\n+using FindAggregateFunctionFinderMatcher = OneTypeMatcher<FindAggregateFunctionData>;\n+using FindAggregateFunctionVisitor = InDepthNodeVisitor<FindAggregateFunctionFinderMatcher, true>;\n+\n }\n \n TTLDescription::TTLDescription(const TTLDescription & other)\n@@ -182,11 +206,8 @@ TTLDescription TTLDescription::getTTLFromAST(\n             if (ttl_element->group_by_key.size() > pk_columns.size())\n                 throw Exception(\"TTL Expression GROUP BY key should be a prefix of primary key\", ErrorCodes::BAD_TTL_EXPRESSION);\n \n-            NameSet primary_key_columns_set(pk_columns.begin(), pk_columns.end());\n             NameSet aggregation_columns_set;\n-\n-            for (const auto & column : primary_key.expression->getRequiredColumns())\n-                primary_key_columns_set.insert(column);\n+            NameSet used_primary_key_columns_set;\n \n             for (size_t i = 0; i < ttl_element->group_by_key.size(); ++i)\n             {\n@@ -194,61 +215,54 @@ TTLDescription TTLDescription::getTTLFromAST(\n                     throw Exception(\n                         \"TTL Expression GROUP BY key should be a prefix of primary key\",\n                         ErrorCodes::BAD_TTL_EXPRESSION);\n+\n+                used_primary_key_columns_set.insert(pk_columns[i]);\n             }\n \n-            for (const auto & [name, value] : ttl_element->group_by_aggregations)\n+            std::vector<std::pair<String, ASTPtr>> aggregations;\n+            for (const auto & ast : ttl_element->group_by_assignments)\n             {\n-                if (primary_key_columns_set.count(name))\n-                    throw Exception(\n-                        \"Can not set custom aggregation for column in primary key in TTL Expression\",\n-                        ErrorCodes::BAD_TTL_EXPRESSION);\n+                const auto assignment = ast->as<const ASTAssignment &>();\n+                auto expression = assignment.expression();\n \n-                aggregation_columns_set.insert(name);\n+                FindAggregateFunctionVisitor::Data data{false};\n+                FindAggregateFunctionVisitor(data).visit(expression);\n+\n+                if (!data.has_aggregate_function)\n+                    throw Exception(ErrorCodes::BAD_TTL_EXPRESSION,\n+                    \"Invalid expression for assignment of column {}. Should contain an aggregate function\", assignment.column_name);\n+\n+                expression = addTypeConversionToAST(std::move(expression), columns.getPhysical(assignment.column_name).type->getName());\n+                aggregations.emplace_back(assignment.column_name, std::move(expression));\n+                aggregation_columns_set.insert(assignment.column_name);\n             }\n \n-            if (aggregation_columns_set.size() != ttl_element->group_by_aggregations.size())\n+            if (aggregation_columns_set.size() != ttl_element->group_by_assignments.size())\n                 throw Exception(\n                     \"Multiple aggregations set for one column in TTL Expression\",\n                     ErrorCodes::BAD_TTL_EXPRESSION);\n \n-\n             result.group_by_keys = Names(pk_columns.begin(), pk_columns.begin() + ttl_element->group_by_key.size());\n \n-            auto aggregations = ttl_element->group_by_aggregations;\n+            const auto & primary_key_expressions = primary_key.expression_list_ast->children;\n \n-            for (size_t i = 0; i < pk_columns.size(); ++i)\n+            /// Wrap with 'any' aggregate function primary key columns,\n+            /// which are not in 'GROUP BY' key and was not set explicitly.\n+            /// The separate step, because not all primary key columns are ordinary columns.\n+            for (size_t i = ttl_element->group_by_key.size(); i < primary_key_expressions.size(); ++i)\n             {\n-                ASTPtr value = primary_key.expression_list_ast->children[i]->clone();\n-\n-                if (i >= ttl_element->group_by_key.size())\n-                {\n-                    ASTPtr value_max = makeASTFunction(\"max\", value->clone());\n-                    aggregations.emplace_back(value->getColumnName(), std::move(value_max));\n-                }\n-\n-                if (value->as<ASTFunction>())\n+                if (!aggregation_columns_set.count(pk_columns[i]))\n                 {\n-                    auto syntax_result = TreeRewriter(context).analyze(value, columns.getAllPhysical(), {}, {}, true);\n-                    auto expr_actions = ExpressionAnalyzer(value, syntax_result, context).getActions(false);\n-                    for (const auto & column : expr_actions->getRequiredColumns())\n-                    {\n-                        if (i < ttl_element->group_by_key.size())\n-                        {\n-                            ASTPtr expr = makeASTFunction(\"any\", std::make_shared<ASTIdentifier>(column));\n-                            aggregations.emplace_back(column, std::move(expr));\n-                        }\n-                        else\n-                        {\n-                            ASTPtr expr = makeASTFunction(\"argMax\", std::make_shared<ASTIdentifier>(column), value->clone());\n-                            aggregations.emplace_back(column, std::move(expr));\n-                        }\n-                    }\n+                    ASTPtr expr = makeASTFunction(\"any\", primary_key_expressions[i]->clone());\n+                    aggregations.emplace_back(pk_columns[i], std::move(expr));\n+                    aggregation_columns_set.insert(pk_columns[i]);\n                 }\n             }\n \n-            for (const auto & column : columns.getAllPhysical())\n+            /// Wrap with 'any' aggregate function other columns, which was not set explicitly.\n+            for (const auto & column : columns.getOrdinary())\n             {\n-                if (!primary_key_columns_set.count(column.name) && !aggregation_columns_set.count(column.name))\n+                if (!aggregation_columns_set.count(column.name) && !used_primary_key_columns_set.count(column.name))\n                 {\n                     ASTPtr expr = makeASTFunction(\"any\", std::make_shared<ASTIdentifier>(column.name));\n                     aggregations.emplace_back(column.name, std::move(expr));\n@@ -280,8 +294,6 @@ TTLDescription TTLDescription::getTTLFromAST(\n     }\n \n     checkTTLExpression(result.expression, result.result_column);\n-\n-\n     return result;\n }\n \n@@ -289,8 +301,10 @@ TTLDescription TTLDescription::getTTLFromAST(\n TTLTableDescription::TTLTableDescription(const TTLTableDescription & other)\n  : definition_ast(other.definition_ast ? other.definition_ast->clone() : nullptr)\n  , rows_ttl(other.rows_ttl)\n+ , rows_where_ttl(other.rows_where_ttl)\n  , move_ttl(other.move_ttl)\n  , recompression_ttl(other.recompression_ttl)\n+ , group_by_ttl(other.group_by_ttl)\n {\n }\n \n@@ -305,8 +319,10 @@ TTLTableDescription & TTLTableDescription::operator=(const TTLTableDescription &\n         definition_ast.reset();\n \n     rows_ttl = other.rows_ttl;\n+    rows_where_ttl = other.rows_where_ttl;\n     move_ttl = other.move_ttl;\n     recompression_ttl = other.recompression_ttl;\n+    group_by_ttl = other.group_by_ttl;\n \n     return *this;\n }\n@@ -323,21 +339,33 @@ TTLTableDescription TTLTableDescription::getTTLForTableFromAST(\n \n     result.definition_ast = definition_ast->clone();\n \n-    bool seen_delete_ttl = false;\n+    bool have_unconditional_delete_ttl = false;\n     for (const auto & ttl_element_ptr : definition_ast->children)\n     {\n         auto ttl = TTLDescription::getTTLFromAST(ttl_element_ptr, columns, context, primary_key);\n-        if (ttl.mode == TTLMode::DELETE || ttl.mode == TTLMode::GROUP_BY)\n+        if (ttl.mode == TTLMode::DELETE)\n         {\n-            if (seen_delete_ttl)\n-                throw Exception(\"More than one DELETE TTL expression is not allowed\", ErrorCodes::BAD_TTL_EXPRESSION);\n-            result.rows_ttl = ttl;\n-            seen_delete_ttl = true;\n+            if (!ttl.where_expression)\n+            {\n+                if (have_unconditional_delete_ttl)\n+                    throw Exception(\"More than one DELETE TTL expression without WHERE expression is not allowed\", ErrorCodes::BAD_TTL_EXPRESSION);\n+\n+                have_unconditional_delete_ttl = true;\n+                result.rows_ttl = ttl;\n+            }\n+            else\n+            {\n+                result.rows_where_ttl.emplace_back(std::move(ttl));\n+            }\n         }\n         else if (ttl.mode == TTLMode::RECOMPRESS)\n         {\n             result.recompression_ttl.emplace_back(std::move(ttl));\n         }\n+        else if (ttl.mode == TTLMode::GROUP_BY)\n+        {\n+            result.group_by_ttl.emplace_back(std::move(ttl));\n+        }\n         else\n         {\n             result.move_ttl.emplace_back(std::move(ttl));\ndiff --git a/src/Storages/TTLDescription.h b/src/Storages/TTLDescription.h\nindex 4b0d4370a700..a2340ad6bcd3 100644\n--- a/src/Storages/TTLDescription.h\n+++ b/src/Storages/TTLDescription.h\n@@ -99,14 +99,19 @@ struct TTLTableDescription\n     /// ^~~~~~~~~~~~~~~definition~~~~~~~~~~~~~~~^\n     ASTPtr definition_ast;\n \n-    /// Rows removing TTL\n+    /// Unconditional main removing rows TTL. Can be only one for table.\n     TTLDescription rows_ttl;\n \n+    /// Conditional removing rows TTLs.\n+    TTLDescriptions rows_where_ttl;\n+\n     /// Moving data TTL (to other disks or volumes)\n     TTLDescriptions move_ttl;\n \n     TTLDescriptions recompression_ttl;\n \n+    TTLDescriptions group_by_ttl;\n+\n     TTLTableDescription() = default;\n     TTLTableDescription(const TTLTableDescription & other);\n     TTLTableDescription & operator=(const TTLTableDescription & other);\n",
  "test_patch": "diff --git a/tests/integration/test_ttl_replicated/test.py b/tests/integration/test_ttl_replicated/test.py\nindex 9418aeaaf011..389e249790f5 100644\n--- a/tests/integration/test_ttl_replicated/test.py\n+++ b/tests/integration/test_ttl_replicated/test.py\n@@ -9,6 +9,11 @@\n node1 = cluster.add_instance('node1', with_zookeeper=True)\n node2 = cluster.add_instance('node2', with_zookeeper=True)\n \n+node3 = cluster.add_instance('node3', with_zookeeper=True)\n+node4 = cluster.add_instance('node4', with_zookeeper=True, image='yandex/clickhouse-server', tag='20.12.4.5', stay_alive=True, with_installed_binary=True)\n+\n+node5 = cluster.add_instance('node5', with_zookeeper=True, image='yandex/clickhouse-server', tag='20.12.4.5', stay_alive=True, with_installed_binary=True)\n+node6 = cluster.add_instance('node6', with_zookeeper=True, image='yandex/clickhouse-server', tag='20.12.4.5', stay_alive=True, with_installed_binary=True)\n \n @pytest.fixture(scope=\"module\")\n def started_cluster():\n@@ -329,3 +334,73 @@ def test_ttl_empty_parts(started_cluster):\n     error_msg = '<Error> default.test_ttl_empty_parts (ReplicatedMergeTreeCleanupThread)'\n     assert not node1.contains_in_log(error_msg)\n     assert not node2.contains_in_log(error_msg)\n+\n+@pytest.mark.parametrize(\n+    ('node_left', 'node_right', 'num_run'),\n+    [(node1, node2, 0), (node3, node4, 1), (node5, node6, 2)]\n+)\n+def test_ttl_compatibility(started_cluster, node_left, node_right, num_run):\n+    drop_table([node_left, node_right], \"test_ttl_delete\")\n+    drop_table([node_left, node_right], \"test_ttl_group_by\")\n+    drop_table([node_left, node_right], \"test_ttl_where\")\n+\n+    for node in [node_left, node_right]:\n+        node.query(\n+            '''\n+                CREATE TABLE test_ttl_delete(date DateTime, id UInt32)\n+                ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/test_ttl_delete_{suff}', '{replica}')\n+                ORDER BY id PARTITION BY toDayOfMonth(date)\n+                TTL date + INTERVAL 3 SECOND\n+            '''.format(suff=num_run, replica=node.name))\n+\n+        node.query(\n+            '''\n+                CREATE TABLE test_ttl_group_by(date DateTime, id UInt32, val UInt64)\n+                ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/test_ttl_group_by_{suff}', '{replica}')\n+                ORDER BY id PARTITION BY toDayOfMonth(date)\n+                TTL date + INTERVAL 3 SECOND GROUP BY id SET val = sum(val)\n+            '''.format(suff=num_run, replica=node.name))\n+\n+        node.query(\n+            '''\n+                CREATE TABLE test_ttl_where(date DateTime, id UInt32)\n+                ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/test_ttl_where_{suff}', '{replica}')\n+                ORDER BY id PARTITION BY toDayOfMonth(date)\n+                TTL date + INTERVAL 3 SECOND DELETE WHERE id % 2 = 1\n+            '''.format(suff=num_run, replica=node.name))\n+\n+    node_left.query(\"INSERT INTO test_ttl_delete VALUES (now(), 1)\")\n+    node_left.query(\"INSERT INTO test_ttl_delete VALUES (toDateTime('2100-10-11 10:00:00'), 2)\")\n+    node_right.query(\"INSERT INTO test_ttl_delete VALUES (now(), 3)\")\n+    node_right.query(\"INSERT INTO test_ttl_delete VALUES (toDateTime('2100-10-11 10:00:00'), 4)\")\n+\n+    node_left.query(\"INSERT INTO test_ttl_group_by VALUES (now(), 0, 1)\")\n+    node_left.query(\"INSERT INTO test_ttl_group_by VALUES (now(), 0, 2)\")\n+    node_right.query(\"INSERT INTO test_ttl_group_by VALUES (now(), 0, 3)\")\n+    node_right.query(\"INSERT INTO test_ttl_group_by VALUES (now(), 0, 4)\")\n+\n+    node_left.query(\"INSERT INTO test_ttl_where VALUES (now(), 1)\")\n+    node_left.query(\"INSERT INTO test_ttl_where VALUES (now(), 2)\")\n+    node_right.query(\"INSERT INTO test_ttl_where VALUES (now(), 3)\")\n+    node_right.query(\"INSERT INTO test_ttl_where VALUES (now(), 4)\")\n+\n+    if node_left.with_installed_binary:\n+        node_left.restart_with_latest_version()\n+\n+    if node_right.with_installed_binary:\n+        node_right.restart_with_latest_version()\n+    \n+    time.sleep(5) # Wait for TTL\n+\n+    node_right.query(\"OPTIMIZE TABLE test_ttl_delete FINAL\")\n+    node_right.query(\"OPTIMIZE TABLE test_ttl_group_by FINAL\")\n+    node_right.query(\"OPTIMIZE TABLE test_ttl_where FINAL\")\n+\n+    assert node_left.query(\"SELECT id FROM test_ttl_delete ORDER BY id\") == \"2\\n4\\n\"\n+    assert node_right.query(\"SELECT id FROM test_ttl_delete ORDER BY id\") == \"2\\n4\\n\"\n+\n+    assert node_left.query(\"SELECT val FROM test_ttl_group_by ORDER BY id\") == \"10\\n\"\n+    assert node_right.query(\"SELECT val FROM test_ttl_group_by ORDER BY id\") == \"10\\n\"\n+\n+    assert node_left.query(\"SELECT id FROM test_ttl_where ORDER BY id\") == \"2\\n4\\n\"\n+    assert node_right.query(\"SELECT id FROM test_ttl_where ORDER BY id\") == \"2\\n4\\n\"\ndiff --git a/tests/queries/0_stateless/01280_ttl_where_group_by.reference b/tests/queries/0_stateless/01280_ttl_where_group_by.reference\nindex ad20d38f2e64..7fe00709dee0 100644\n--- a/tests/queries/0_stateless/01280_ttl_where_group_by.reference\n+++ b/tests/queries/0_stateless/01280_ttl_where_group_by.reference\n@@ -1,20 +1,26 @@\n+ttl_01280_1\n 1\t1\t0\t4\n 1\t2\t3\t7\n 1\t3\t0\t5\n 2\t1\t0\t1\n 2\t1\t20\t1\n+ttl_01280_2\n 1\t1\t[0,2,3]\t4\n 1\t1\t[5,4,1]\t13\n 1\t3\t[1,0,1,0]\t17\n 2\t1\t[3,1,0,3]\t8\n 3\t1\t[2,4,5]\t8\n+ttl_01280_3\n 1\t1\t0\t4\n-1\t3\t10\t6\n+1\t1\t10\t6\n 2\t1\t0\t3\n-3\t5\t8\t2\n+3\t1\t8\t2\n+ttl_01280_4\n 1\t1\t0\t4\n-3\t3\t13\t9\n+10\t2\t13\t9\n+ttl_01280_5\n 1\t2\t7\t5\n 2\t3\t6\t5\n-1\t2\t3\t5\n-2\t3\t3\t5\n+ttl_01280_6\n+1\t5\t3\t5\n+2\t10\t3\t5\ndiff --git a/tests/queries/0_stateless/01280_ttl_where_group_by.sh b/tests/queries/0_stateless/01280_ttl_where_group_by.sh\nindex 5ca79951a464..9f30c7c58722 100755\n--- a/tests/queries/0_stateless/01280_ttl_where_group_by.sh\n+++ b/tests/queries/0_stateless/01280_ttl_where_group_by.sh\n@@ -14,6 +14,7 @@ function optimize()\n     done\n }\n \n+echo \"ttl_01280_1\"\n $CLICKHOUSE_CLIENT -n --query \"\n create table ttl_01280_1 (a Int, b Int, x Int, y Int, d DateTime) engine = MergeTree order by (a, b) ttl d + interval 1 second delete where x % 10 == 0 and y > 5;\n insert into ttl_01280_1 values (1, 1, 0, 4, now() + 10);\n@@ -30,6 +31,7 @@ $CLICKHOUSE_CLIENT --query \"select a, b, x, y from ttl_01280_1 ORDER BY a, b, x,\n \n $CLICKHOUSE_CLIENT --query \"drop table if exists ttl_01280_2\"\n \n+echo \"ttl_01280_2\"\n $CLICKHOUSE_CLIENT -n --query \"\n create table ttl_01280_2 (a Int, b Int, x Array(Int32), y Double, d DateTime) engine = MergeTree order by (a, b) ttl d + interval 1 second group by a, b set x = minForEach(x), y = sum(y), d = max(d);\n insert into ttl_01280_2 values (1, 1, array(0, 2, 3), 4, now() + 10);\n@@ -48,8 +50,9 @@ $CLICKHOUSE_CLIENT --query \"select a, b, x, y from ttl_01280_2 ORDER BY a, b, x,\n \n $CLICKHOUSE_CLIENT --query \"drop table if exists ttl_01280_3\"\n \n+echo \"ttl_01280_3\"\n $CLICKHOUSE_CLIENT -n --query \"\n-create table ttl_01280_3 (a Int, b Int, x Int64, y Int, d DateTime) engine = MergeTree order by (a, b) ttl d + interval 1 second group by a set x = argMax(x, d), y = argMax(y, d), d = max(d);\n+create table ttl_01280_3 (a Int, b Int, x Int64, y Int, d DateTime) engine = MergeTree order by (a, b) ttl d + interval 1 second group by a set b = min(b), x = argMax(x, d), y = argMax(y, d), d = max(d);\n insert into ttl_01280_3 values (1, 1, 0, 4, now() + 10);\n insert into ttl_01280_3 values (1, 1, 10, 6, now() + 1);\n insert into ttl_01280_3 values (1, 2, 3, 7, now());\n@@ -66,6 +69,7 @@ $CLICKHOUSE_CLIENT --query \"select a, b, x, y from ttl_01280_3 ORDER BY a, b, x,\n \n $CLICKHOUSE_CLIENT --query \"drop table if exists ttl_01280_4\"\n \n+echo \"ttl_01280_4\"\n $CLICKHOUSE_CLIENT -n --query \"\n create table ttl_01280_4 (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (toDate(d), -(a + b)) ttl d + interval 1 second group by toDate(d) set x = sum(x), y = max(y);\n insert into ttl_01280_4 values (1, 1, 0, 4, now() + 10);\n@@ -80,7 +84,8 @@ $CLICKHOUSE_CLIENT --query \"select a, b, x, y from ttl_01280_4 ORDER BY a, b, x,\n \n $CLICKHOUSE_CLIENT --query \"drop table if exists ttl_01280_5\"\n \n-$CLICKHOUSE_CLIENT -n --query \"create table ttl_01280_5 (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (toDate(d), a, -b) ttl d + interval 1 second group by toDate(d), a set x = sum(x);\n+echo \"ttl_01280_5\"\n+$CLICKHOUSE_CLIENT -n --query \"create table ttl_01280_5 (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (toDate(d), a, -b) ttl d + interval 1 second group by toDate(d), a set x = sum(x), b = argMax(b, -b);\n insert into ttl_01280_5 values (1, 2, 3, 5, now());\n insert into ttl_01280_5 values (2, 10, 1, 5, now());\n insert into ttl_01280_5 values (2, 3, 5, 5, now());\n@@ -92,6 +97,7 @@ $CLICKHOUSE_CLIENT --query \"select a, b, x, y from ttl_01280_5 ORDER BY a, b, x,\n \n $CLICKHOUSE_CLIENT --query \"drop table if exists ttl_01280_6\"\n \n+echo \"ttl_01280_6\"\n $CLICKHOUSE_CLIENT -n --query \"\n create table ttl_01280_6 (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (toDate(d), a, -b) ttl d + interval 1 second group by toDate(d), a;\n insert into ttl_01280_6 values (1, 2, 3, 5, now());\ndiff --git a/tests/queries/0_stateless/01280_ttl_where_group_by_negative.sql b/tests/queries/0_stateless/01280_ttl_where_group_by_negative.sql\nindex f2c26a3d4955..b273e065bcce 100644\n--- a/tests/queries/0_stateless/01280_ttl_where_group_by_negative.sql\n+++ b/tests/queries/0_stateless/01280_ttl_where_group_by_negative.sql\n@@ -1,7 +1,4 @@\n create table ttl_01280_error (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (a, b) ttl d + interval 1 second group by x set y = max(y); -- { serverError 450}\n create table ttl_01280_error (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (a, b) ttl d + interval 1 second group by b set y = max(y); -- { serverError 450}\n create table ttl_01280_error (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (a, b) ttl d + interval 1 second group by a, b, x set y = max(y); -- { serverError 450}\n-create table ttl_01280_error (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (a, b) ttl d + interval 1 second group by a set b = min(b), y = max(y); -- { serverError 450}\n create table ttl_01280_error (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (a, b) ttl d + interval 1 second group by a, b set y = max(y), y = max(y); -- { serverError 450}\n-create table ttl_01280_error (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (toDate(d), a) ttl d + interval 1 second group by toDate(d), a set d = min(d), b = max(b); -- { serverError 450}\n-create table ttl_01280_error (a Int, b Int, x Int64, y Int64, d DateTime) engine = MergeTree order by (d, -(a + b)) ttl d + interval 1 second group by d, -(a + b) set a = sum(a), b = min(b); -- { serverError 450}\ndiff --git a/tests/queries/0_stateless/01506_ttl_same_with_order_by.reference b/tests/queries/0_stateless/01506_ttl_same_with_order_by.reference\nnew file mode 100644\nindex 000000000000..f8f36434a82d\n--- /dev/null\n+++ b/tests/queries/0_stateless/01506_ttl_same_with_order_by.reference\n@@ -0,0 +1,4 @@\n+2020-01-01 00:00:00\t3\n+2020-01-01 00:00:00\t2020-01-01 00:00:00\t111\n+1\n+0\ndiff --git a/tests/queries/0_stateless/01506_ttl_same_with_order_by.sql b/tests/queries/0_stateless/01506_ttl_same_with_order_by.sql\nnew file mode 100644\nindex 000000000000..7a0fb86330b5\n--- /dev/null\n+++ b/tests/queries/0_stateless/01506_ttl_same_with_order_by.sql\n@@ -0,0 +1,78 @@\n+DROP TABLE IF EXISTS derived_metrics_local;\n+\n+CREATE TABLE derived_metrics_local\n+(\n+  timestamp DateTime,\n+  bytes UInt64\n+)\n+ENGINE=SummingMergeTree()\n+PARTITION BY toYYYYMMDD(timestamp)\n+ORDER BY (toStartOfHour(timestamp), timestamp)\n+TTL toStartOfHour(timestamp) + INTERVAL 1 HOUR GROUP BY toStartOfHour(timestamp)\n+SET bytes=max(bytes);\n+\n+INSERT INTO derived_metrics_local values('2020-01-01 00:00:00', 1);\n+INSERT INTO derived_metrics_local values('2020-01-01 00:01:00', 3);\n+INSERT INTO derived_metrics_local values('2020-01-01 00:02:00', 2);\n+\n+OPTIMIZE TABLE derived_metrics_local FINAL;\n+SELECT * FROM derived_metrics_local;\n+\n+DROP TABLE derived_metrics_local;\n+\n+CREATE TABLE derived_metrics_local \n+(\n+  timestamp DateTime,\n+  timestamp_h DateTime materialized toStartOfHour(timestamp),\n+  bytes UInt64\n+)\n+ENGINE=SummingMergeTree()\n+PARTITION BY toYYYYMMDD(timestamp)\n+ORDER BY (timestamp_h, timestamp)\n+TTL toStartOfHour(timestamp) + INTERVAL 1 HOUR GROUP BY timestamp_h\n+SET bytes=max(bytes), timestamp = toStartOfHour(any(timestamp));\n+\n+INSERT INTO derived_metrics_local values('2020-01-01 00:01:00', 111);\n+INSERT INTO derived_metrics_local values('2020-01-01 00:19:22', 22);\n+INSERT INTO derived_metrics_local values('2020-01-01 00:59:02', 1);\n+\n+OPTIMIZE TABLE derived_metrics_local FINAL;\n+SELECT timestamp, timestamp_h, bytes FROM derived_metrics_local;\n+\n+DROP TABLE IF EXISTS derived_metrics_local;\n+\n+CREATE TABLE derived_metrics_local\n+(\n+  timestamp DateTime,\n+  bytes UInt64 TTL toStartOfHour(timestamp) + INTERVAL 1 HOUR\n+)\n+ENGINE=MergeTree()\n+ORDER BY (toStartOfHour(timestamp), timestamp)\n+SETTINGS min_bytes_for_wide_part = 0;\n+\n+INSERT INTO derived_metrics_local values('2020-01-01 00:01:00', 111) ('2020-01-01 00:19:22', 22) ('2100-01-01 00:19:22', 1);\n+\n+OPTIMIZE TABLE derived_metrics_local FINAL;\n+SELECT sum(bytes) FROM derived_metrics_local;\n+\n+DROP TABLE IF EXISTS derived_metrics_local;\n+\n+CREATE TABLE derived_metrics_local\n+(\n+  timestamp DateTime,\n+  bytes UInt64\n+)\n+ENGINE=MergeTree()\n+PARTITION BY toYYYYMMDD(timestamp)\n+ORDER BY (toStartOfHour(timestamp), timestamp)\n+TTL toStartOfHour(timestamp) + INTERVAL 1 HOUR\n+SETTINGS min_bytes_for_wide_part = 0;\n+\n+INSERT INTO derived_metrics_local values('2020-01-01 00:01:00', 111);\n+INSERT INTO derived_metrics_local values('2020-01-01 00:19:22', 22);\n+INSERT INTO derived_metrics_local values('2020-01-01 00:59:02', 1);\n+\n+OPTIMIZE TABLE derived_metrics_local FINAL;\n+SELECT count() FROM derived_metrics_local;\n+\n+DROP TABLE IF EXISTS derived_metrics_local;\ndiff --git a/tests/queries/0_stateless/01622_multiple_ttls.reference b/tests/queries/0_stateless/01622_multiple_ttls.reference\nnew file mode 100644\nindex 000000000000..d9ebb694584a\n--- /dev/null\n+++ b/tests/queries/0_stateless/01622_multiple_ttls.reference\n@@ -0,0 +1,22 @@\n+TTL WHERE\n+1970-10-10\t2\n+1970-10-10\t5\n+1970-10-10\t8\n+2000-10-10\t1\n+2000-10-10\t2\n+2000-10-10\t4\n+2000-10-10\t5\n+2000-10-10\t7\n+2000-10-10\t8\n+TTL GROUP BY\n+1970-10-01\t0\t4950\n+2000-10-01\t0\t450\n+2000-10-01\t1\t460\n+2000-10-01\t2\t470\n+2000-10-01\t3\t480\n+2000-10-01\t4\t490\n+2000-10-01\t5\t500\n+2000-10-01\t6\t510\n+2000-10-01\t7\t520\n+2000-10-01\t8\t530\n+2000-10-01\t9\t540\ndiff --git a/tests/queries/0_stateless/01622_multiple_ttls.sql b/tests/queries/0_stateless/01622_multiple_ttls.sql\nnew file mode 100644\nindex 000000000000..aa2eeb5759b3\n--- /dev/null\n+++ b/tests/queries/0_stateless/01622_multiple_ttls.sql\n@@ -0,0 +1,44 @@\n+SELECT 'TTL WHERE';\n+DROP TABLE IF EXISTS ttl_where;\n+\n+CREATE TABLE ttl_where\n+(\n+    `d` Date,\n+    `i` UInt32\n+)\n+ENGINE = MergeTree\n+ORDER BY tuple()\n+TTL d + toIntervalYear(10) DELETE WHERE i % 3 = 0,\n+    d + toIntervalYear(40) DELETE WHERE i % 3 = 1;\n+\n+-- This test will fail at 2040-10-10\n+\n+INSERT INTO ttl_where SELECT toDate('2000-10-10'), number FROM numbers(10);\n+INSERT INTO ttl_where SELECT toDate('1970-10-10'), number FROM numbers(10);\n+OPTIMIZE TABLE ttl_where FINAL;\n+\n+SELECT * FROM ttl_where ORDER BY d, i;\n+\n+DROP TABLE ttl_where;\n+\n+SELECT 'TTL GROUP BY';\n+DROP TABLE IF EXISTS ttl_group_by;\n+\n+CREATE TABLE ttl_group_by\n+(\n+    `d` Date,\n+    `i` UInt32,\n+    `v` UInt64\n+)\n+ENGINE = MergeTree\n+ORDER BY (toStartOfMonth(d), i % 10)\n+TTL d + toIntervalYear(10) GROUP BY toStartOfMonth(d), i % 10 SET d = any(toStartOfMonth(d)), i = any(i % 10), v = sum(v),\n+    d + toIntervalYear(40) GROUP BY toStartOfMonth(d) SET d = any(toStartOfMonth(d)), v = sum(v);\n+\n+INSERT INTO ttl_group_by SELECT toDate('2000-10-10'), number, number FROM numbers(100);\n+INSERT INTO ttl_group_by SELECT toDate('1970-10-10'), number, number FROM numbers(100);\n+OPTIMIZE TABLE ttl_group_by FINAL;\n+\n+SELECT * FROM ttl_group_by ORDER BY d, i;\n+\n+DROP TABLE ttl_group_by;\n",
  "problem_statement": "TTL GROUP BY / DB::Exception: Column 'toStartOfHour(timestamp)' already exists.\n```\r\nCREATE TABLE derived_metrics_local \r\n(\r\n  timestamp DateTime,\r\n  bytes UInt64\r\n)\r\nENGINE=SummingMergeTree()\r\nPARTITION BY toYYYYMMDD(timestamp)\r\nORDER BY (toStartOfHour(timestamp), timestamp)\r\nTTL toStartOfHour(timestamp) + INTERVAL 1 HOUR GROUP BY toStartOfHour(timestamp)\r\nSET bytes=max(bytes);\r\n\r\ninsert into derived_metrics_local values('2020-01-01 00:00:00', 1);\r\nDB::Exception: Column 'toStartOfHour(timestamp)' already exists.\r\n```\r\n\r\n\r\n---\r\n\r\n\r\nAnd it would be nice to be able to set PK columns in SET as well\r\n\r\n` SET bytes = max(bytes), timestamp = toStartOfHour(timestamp) `\r\n\r\nbecause `timestamp` now is set to some max/any? value after ttl\r\n\r\n```\r\nCREATE TABLE derived_metrics_local \r\n(\r\n  timestamp DateTime,\r\n  timestamp_h DateTime materialized toStartOfHour(timestamp),\r\n  bytes UInt64\r\n)\r\nENGINE=SummingMergeTree()\r\nPARTITION BY toYYYYMMDD(timestamp)\r\nORDER BY (timestamp_h, timestamp)\r\nTTL toStartOfHour(timestamp) + INTERVAL 1 HOUR GROUP BY timestamp_h\r\nSET bytes=max(bytes);\r\n\r\ninsert into derived_metrics_local values('2020-01-01 00:00:00', 111);\r\ninsert into derived_metrics_local values('2020-01-01 00:59:02', 1);\r\n\r\nOPTIMIZE TABLE derived_metrics_local FINAL;\r\n\r\nselect timestamp, timestamp_h, bytes  from derived_metrics_local\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500timestamp\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500timestamp_h\u2500\u252c\u2500bytes\u2500\u2510\r\n\u2502 2020-01-01 00:59:02 \u2502 2020-01-01 00:00:00 \u2502   111 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\n00:59:02 is not related to 111\r\n```\n",
  "hints_text": "@expl0si0nn",
  "created_at": "2020-09-29T16:46:00Z",
  "modified_files": [
    "b/src/DataStreams/ITTLAlgorithm.cpp",
    "b/src/DataStreams/ITTLAlgorithm.h",
    "b/src/DataStreams/TTLAggregationAlgorithm.cpp",
    "b/src/DataStreams/TTLAggregationAlgorithm.h",
    "src/DataStreams/TTLBlockInputStream.cpp",
    "src/DataStreams/TTLBlockInputStream.h",
    "b/src/DataStreams/TTLColumnAlgorithm.cpp",
    "b/src/DataStreams/TTLColumnAlgorithm.h",
    "b/src/DataStreams/TTLDeleteAlgorithm.cpp",
    "b/src/DataStreams/TTLDeleteAlgorithm.h",
    "b/src/DataStreams/TTLUpdateInfoAlgorithm.cpp",
    "b/src/DataStreams/TTLUpdateInfoAlgorithm.h",
    "src/DataStreams/ya.make",
    "src/Parsers/ASTTTLElement.cpp",
    "src/Parsers/ASTTTLElement.h",
    "src/Parsers/ExpressionElementParsers.cpp",
    "src/Parsers/ExpressionElementParsers.h",
    "src/Parsers/ParserAlterQuery.cpp",
    "src/Parsers/ParserAlterQuery.h",
    "src/Storages/MergeTree/IMergeTreeDataPart.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartTTLInfo.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartTTLInfo.h",
    "src/Storages/MergeTree/MergeTreeDataWriter.cpp",
    "src/Storages/StorageInMemoryMetadata.cpp",
    "src/Storages/StorageInMemoryMetadata.h",
    "src/Storages/System/StorageSystemParts.cpp",
    "src/Storages/TTLDescription.cpp",
    "src/Storages/TTLDescription.h"
  ],
  "modified_test_files": [
    "tests/integration/test_ttl_replicated/test.py",
    "tests/queries/0_stateless/01280_ttl_where_group_by.reference",
    "tests/queries/0_stateless/01280_ttl_where_group_by.sh",
    "tests/queries/0_stateless/01280_ttl_where_group_by_negative.sql",
    "b/tests/queries/0_stateless/01506_ttl_same_with_order_by.reference",
    "b/tests/queries/0_stateless/01506_ttl_same_with_order_by.sql",
    "b/tests/queries/0_stateless/01622_multiple_ttls.reference",
    "b/tests/queries/0_stateless/01622_multiple_ttls.sql"
  ]
}