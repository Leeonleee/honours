{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 78764,
  "instance_id": "ClickHouse__ClickHouse-78764",
  "issue_numbers": [
    "78740"
  ],
  "base_commit": "51d55d7cdb03a383ca57a7ceeaa48b8f77c963fd",
  "patch": "diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp\nindex 650a5c4af033..d4414fb43651 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp\n@@ -271,8 +271,15 @@ ManifestFileContent::ManifestFileContent(\n         for (const auto & [column_id, bounds] : value_for_bounds)\n         {\n             DB::NameAndTypePair name_and_type = schema_processor.getFieldCharacteristics(schema_id, column_id);\n-            auto left = deserializeFieldFromBinaryRepr(bounds.first.safeGet<std::string>(), name_and_type.type, true);\n-            auto right = deserializeFieldFromBinaryRepr(bounds.second.safeGet<std::string>(), name_and_type.type, false);\n+\n+            String left_str;\n+            String right_str;\n+            /// lower_bound and upper_bound may be NULL.\n+            if (!bounds.first.tryGet(left_str) || !bounds.second.tryGet(right_str))\n+                continue;\n+\n+            auto left = deserializeFieldFromBinaryRepr(left_str, name_and_type.type, true);\n+            auto right = deserializeFieldFromBinaryRepr(right_str, name_and_type.type, false);\n             if (!left || !right)\n                 continue;\n \n",
  "test_patch": "diff --git a/tests/integration/test_storage_iceberg/test.py b/tests/integration/test_storage_iceberg/test.py\nindex 3efeb1664a9d..9ce626b058d8 100644\n--- a/tests/integration/test_storage_iceberg/test.py\n+++ b/tests/integration/test_storage_iceberg/test.py\n@@ -2774,3 +2774,141 @@ def test_explicit_metadata_file(started_cluster, storage_type):\n         create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster, explicit_metadata_path=chr(0) + chr(1))\n     with pytest.raises(Exception):\n         create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster, explicit_metadata_path=\"../metadata/v11.metadata.json\")\n+\n+@pytest.mark.parametrize(\"storage_type\", [\"s3\", \"azure\", \"local\"])\n+def test_minmax_pruning_with_null(started_cluster, storage_type):\n+    instance = started_cluster.instances[\"node1\"]\n+    spark = started_cluster.spark_session\n+    TABLE_NAME = \"test_minmax_pruning_with_null\" + storage_type + \"_\" + get_uuid_str()\n+\n+    def execute_spark_query(query: str):\n+        spark.sql(query)\n+        default_upload_directory(\n+            started_cluster,\n+            storage_type,\n+            f\"/iceberg_data/default/{TABLE_NAME}/\",\n+            f\"/iceberg_data/default/{TABLE_NAME}/\",\n+        )\n+        return\n+\n+    execute_spark_query(\n+        f\"\"\"\n+            CREATE TABLE {TABLE_NAME} (\n+                tag INT,\n+                date DATE,\n+                ts TIMESTAMP,\n+                time_struct struct<a : DATE, b : TIMESTAMP>,\n+                name VARCHAR(50),\n+                number BIGINT\n+            )\n+            USING iceberg\n+            OPTIONS('format-version'='2')\n+        \"\"\"\n+    )\n+\n+    # min-max value of time_struct in manifest file is null.\n+    execute_spark_query(\n+        f\"\"\"\n+        INSERT INTO {TABLE_NAME} VALUES\n+        (1, DATE '2024-01-20',\n+        TIMESTAMP '2024-02-20 10:00:00', null, 'vasya', 5)\n+    \"\"\"\n+    )\n+\n+    execute_spark_query(\n+        f\"\"\"\n+        INSERT INTO {TABLE_NAME} VALUES\n+        (2, DATE '2024-02-20',\n+        TIMESTAMP '2024-03-20 15:00:00', null, 'vasilisa', 6)\n+    \"\"\"\n+    )\n+\n+    execute_spark_query(\n+        f\"\"\"\n+        INSERT INTO {TABLE_NAME} VALUES\n+        (3, DATE '2025-03-20',\n+        TIMESTAMP '2024-04-30 14:00:00', null, 'icebreaker', 7)\n+    \"\"\"\n+    )\n+    execute_spark_query(\n+        f\"\"\"\n+        INSERT INTO {TABLE_NAME} VALUES\n+        (4, DATE '2025-04-20',\n+        TIMESTAMP '2024-05-30 14:00:00', null, 'iceberg', 8)\n+    \"\"\"\n+    )\n+\n+    execute_spark_query(\n+        f\"\"\"\n+        INSERT INTO {TABLE_NAME} VALUES\n+        (1, DATE '2024-01-20',\n+        TIMESTAMP '2024-02-20 10:00:00', named_struct('a', DATE '2024-02-20', 'b', TIMESTAMP '2024-02-20 10:00:00'), 'vasya', 5)\n+    \"\"\"\n+    )\n+\n+    creation_expression = get_creation_expression(\n+        storage_type, TABLE_NAME, started_cluster, table_function=True\n+    )\n+\n+    def check_validity_and_get_prunned_files(select_expression):\n+        query_id1 = f\"{TABLE_NAME}-{uuid.uuid4()}\"\n+        query_id2 = f\"{TABLE_NAME}-{uuid.uuid4()}\"\n+\n+        data1 = instance.query(\n+            select_expression,\n+            query_id=query_id1,\n+            settings={\"use_iceberg_partition_pruning\": 0, \"input_format_parquet_bloom_filter_push_down\": 0, \"input_format_parquet_filter_push_down\": 0},\n+        )\n+        data1 = list(\n+            map(\n+                lambda x: x.split(\"\\t\"),\n+                filter(lambda x: len(x) > 0, data1.strip().split(\"\\n\")),\n+            )\n+        )\n+\n+        data2 = instance.query(\n+            select_expression,\n+            query_id=query_id2,\n+            settings={\"use_iceberg_partition_pruning\": 1, \"input_format_parquet_bloom_filter_push_down\": 0, \"input_format_parquet_filter_push_down\": 0},\n+        )\n+        data2 = list(\n+            map(\n+                lambda x: x.split(\"\\t\"),\n+                filter(lambda x: len(x) > 0, data2.strip().split(\"\\n\")),\n+            )\n+        )\n+\n+        assert data1 == data2\n+\n+        instance.query(\"SYSTEM FLUSH LOGS\")\n+\n+        print(\n+            \"Unprunned: \",\n+            instance.query(\n+                f\"SELECT ProfileEvents['IcebergMinMaxIndexPrunnedFiles'] FROM system.query_log WHERE query_id = '{query_id1}' AND type = 'QueryFinish'\"\n+            ),\n+        )\n+        print(\n+            \"Prunned: \",\n+            instance.query(\n+                f\"SELECT ProfileEvents['IcebergMinMaxIndexPrunnedFiles'] FROM system.query_log WHERE query_id = '{query_id2}' AND type = 'QueryFinish'\"\n+            ),\n+        )\n+\n+        assert 0 == int(\n+            instance.query(\n+                f\"SELECT ProfileEvents['IcebergMinMaxIndexPrunnedFiles'] FROM system.query_log WHERE query_id = '{query_id1}' AND type = 'QueryFinish'\"\n+            )\n+        )\n+        return int(\n+            instance.query(\n+                f\"SELECT ProfileEvents['IcebergMinMaxIndexPrunnedFiles'] FROM system.query_log WHERE query_id = '{query_id2}' AND type = 'QueryFinish'\"\n+            )\n+        )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE time_struct.a <= '2024-02-01' ORDER BY ALL\"\n+        )\n+        == 1\n+    )\n",
  "problem_statement": "Iceberg function now fail on public table (worked until recently)\n### Company or project name\n\nAgnostic\n\n### Describe what's wrong\n\nThe following query fails:\n\n```sql\n:) select count(*) from iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs')\n\nSELECT count(*)\nFROM iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs')\n\nQuery id: 09ff1974-0035-430b-8c2a-9cf9c258f9dd\n\n\nElapsed: 0.530 sec.\n\nReceived exception:\nCode: 170. DB::Exception: Bad get: has Null, requested String. (BAD_GET)\n```\n\nIt used to work with every stable version until recently (post 2025-03).\nHere is an example with ` 25.2.2.39`:\n\n```sql\n:) select count(*) from iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs')\n\nSELECT count(*)\nFROM iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs')\n\nQuery id: 68f815d3-6bbb-4cf5-978f-886ec17c77b3\n\n   \u250c\u2500\u2500\u2500\u2500count()\u2500\u2510\n1. \u2502 4593462586 \u2502 -- 4.59 billion\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n1 row in set. Elapsed: 0.546 sec. Processed 2.44 billion rows, 180.00 B (4.46 billion rows/s., 329.58 B/s.)\nPeak memory usage: 20.34 MiB.\n```\n\n### Does it reproduce on the most recent release?\n\nYes\n\n### How to reproduce\n\n```sql\nselect count(*) from iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs')\n```\n\n### Expected behavior\n\nThe query runs without any issue and yield a correct result.\n\n### Error message and/or stacktrace\n\n```\nReceived exception:\nCode: 170. DB::Exception: Bad get: has Null, requested String. (BAD_GET)\n```\n\nAnd there is no stack trace, which is weird.\n\n### Additional context\n\n_No response_\n",
  "hints_text": "Has your table been mutated, or did it receive delete? \nHi @melvynator \n\nIf you are talking about row-level deletes then no.\nThe table has been appended to and overwritten (replace some files) by compaction operation.\nHere is the metadata file:\n\n```json\n{\n  \"last-sequence-number\": 2,\n  \"format-version\": 2,\n  \"table-uuid\": \"01960276-4f34-73b1-8302-aae78a9669a5\",\n  \"location\": \"s3://agnostic-data-ice-ethereum-mainnet/decoded_logs\",\n  \"last-updated-ms\": 1743815374306,\n  \"last-column-id\": 13,\n  \"schemas\": [\n    {\n      \"type\": \"struct\",\n      \"fields\": [\n        {\n          \"type\": \"date\",\n          \"id\": 1,\n          \"name\": \"date\",\n          \"required\": true\n        },\n        {\n          \"type\": \"timestamptz\",\n          \"id\": 2,\n          \"name\": \"timestamp\",\n          \"required\": true\n        },\n        {\n          \"type\": \"fixed[32]\",\n          \"id\": 3,\n          \"name\": \"block_hash\",\n          \"required\": true\n        },\n        {\n          \"type\": \"long\",\n          \"id\": 4,\n          \"name\": \"block_number\",\n          \"required\": true\n        },\n        {\n          \"type\": \"fixed[20]\",\n          \"id\": 5,\n          \"name\": \"transaction_from\",\n          \"required\": true\n        },\n        {\n          \"type\": \"int\",\n          \"id\": 6,\n          \"name\": \"transaction_status\",\n          \"required\": true\n        },\n        {\n          \"type\": \"fixed[32]\",\n          \"id\": 7,\n          \"name\": \"transaction_hash\",\n          \"required\": true\n        },\n        {\n          \"type\": \"int\",\n          \"id\": 8,\n          \"name\": \"transaction_index\",\n          \"required\": true\n        },\n        {\n          \"type\": \"int\",\n          \"id\": 9,\n          \"name\": \"log_index\",\n          \"required\": true\n        },\n        {\n          \"type\": \"fixed[20]\",\n          \"id\": 10,\n          \"name\": \"address\",\n          \"required\": true\n        },\n        {\n          \"type\": \"binary\",\n          \"id\": 11,\n          \"name\": \"signature\",\n          \"required\": true\n        },\n        {\n          \"type\": \"binary\",\n          \"id\": 12,\n          \"name\": \"fullsig\",\n          \"required\": true\n        },\n        {\n          \"type\": \"binary\",\n          \"id\": 13,\n          \"name\": \"inputs\",\n          \"required\": true\n        }\n      ],\n      \"schema-id\": 0,\n      \"identifier-field-ids\": []\n    }\n  ],\n  \"current-schema-id\": 0,\n  \"partition-specs\": [\n    {\n      \"spec-id\": 0,\n      \"fields\": []\n    }\n  ],\n  \"default-spec-id\": 0,\n  \"last-partition-id\": 999,\n  \"properties\": {\n    \"schema.name-mapping.default\": \"[{\\\"names\\\":[\\\"date\\\"],\\\"field-id\\\":1},{\\\"names\\\":[\\\"timestamp\\\"],\\\"field-id\\\":2},{\\\"names\\\":[\\\"block_hash\\\"],\\\"field-id\\\":3},{\\\"names\\\":[\\\"block_number\\\"],\\\"field-id\\\":4},{\\\"names\\\":[\\\"transaction_from\\\"],\\\"field-id\\\":5},{\\\"names\\\":[\\\"transaction_status\\\"],\\\"field-id\\\":6},{\\\"names\\\":[\\\"transaction_hash\\\"],\\\"field-id\\\":7},{\\\"names\\\":[\\\"transaction_index\\\"],\\\"field-id\\\":8},{\\\"names\\\":[\\\"log_index\\\"],\\\"field-id\\\":9},{\\\"names\\\":[\\\"address\\\"],\\\"field-id\\\":10},{\\\"names\\\":[\\\"signature\\\"],\\\"field-id\\\":11},{\\\"names\\\":[\\\"fullsig\\\"],\\\"field-id\\\":12},{\\\"names\\\":[\\\"inputs\\\"],\\\"field-id\\\":13}]\"\n  },\n  \"snapshots\": [\n    {\n      \"snapshot-id\": 1187496556664270058,\n      \"sequence-number\": 1,\n      \"timestamp-ms\": 1743798042398,\n      \"manifest-list\": \"s3://agnostic-data-ice-ethereum-mainnet/decoded_logs/metadata/snap-1187496556664270058-0-7f27e12e-4206-490f-b7da-bc502e745a18.avro\",\n      \"summary\": {\n        \"added-data-files\": \"33\",\n        \"added-files-size\": \"305542621123\",\n        \"added-records\": \"4593462586\",\n        \"operation\": \"append\",\n        \"total-data-files\": \"33\",\n        \"total-delete-files\": \"0\",\n        \"total-equality-deletes\": \"0\",\n        \"total-files-size\": \"305542621123\",\n        \"total-position-deletes\": \"0\",\n        \"total-records\": \"4593462586\"\n      },\n      \"schema-id\": 0\n    },\n    {\n      \"snapshot-id\": 5997099256135212838,\n      \"parent-snapshot-id\": 1187496556664270058,\n      \"sequence-number\": 2,\n      \"timestamp-ms\": 1743815374306,\n      \"manifest-list\": \"s3://agnostic-data-ice-ethereum-mainnet/decoded_logs/metadata/snap-5997099256135212838-0-9aa65d95-645b-4e67-b11c-e1d033c63884.avro\",\n      \"summary\": {\n        \"added-data-files\": \"1\",\n        \"added-files-size\": \"6754104609\",\n        \"added-records\": \"100412893\",\n        \"deleted-data-files\": \"2\",\n        \"deleted-records\": \"100412893\",\n        \"operation\": \"overwrite\",\n        \"removed-files-size\": \"6734983513\",\n        \"total-data-files\": \"32\",\n        \"total-delete-files\": \"0\",\n        \"total-equality-deletes\": \"0\",\n        \"total-files-size\": \"305561742219\",\n        \"total-position-deletes\": \"0\",\n        \"total-records\": \"4593462586\"\n      },\n      \"schema-id\": 0\n    }\n  ],\n  \"current-snapshot-id\": 5997099256135212838,\n  \"snapshot-log\": [\n    {\n      \"snapshot-id\": 1187496556664270058,\n      \"timestamp-ms\": 1743798042398\n    },\n    {\n      \"snapshot-id\": 5997099256135212838,\n      \"timestamp-ms\": 1743815374306\n    }\n  ],\n  \"sort-orders\": [\n    {\n      \"order-id\": 0,\n      \"fields\": []\n    }\n  ],\n  \"default-sort-order-id\": 0,\n  \"refs\": {\n    \"main\": {\n      \"snapshot-id\": 5997099256135212838,\n      \"type\": \"branch\"\n    }\n  }\n}\n```\nThanks for sharing. \n\nI can't seem to reproduce in some of the latest PR:\n\n```\nSELECT\n    count(*),\n    version()\nFROM iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs')\n\nQuery id: d897ccdf-90c7-48cf-bd41-df22547b3293\n\n   \u250c\u2500\u2500\u2500\u2500count()\u2500\u252c\u2500version()\u2500\u2500\u2510\n1. \u2502 4593462586 \u2502 25.4.1.176 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n1 row in set. Elapsed: 2.891 sec. Processed 4.59 billion rows, 702.42 KB (1.59 billion rows/s., 243.00 KB/s.)\nPeak memory usage: 283.52 KiB.\n```\nIt's may introduced in https://github.com/ClickHouse/ClickHouse/pull/78242.\nYes probable because I also can't reproduce in 25.3:\n\n```\nSELECT\n    count(*),\n    version()\nFROM iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs')\n\nQuery id: 153b1d55-c231-48eb-a66d-5eb19354a2d8\n\n   \u250c\u2500\u2500\u2500\u2500count()\u2500\u252c\u2500version()\u2500\u2500\u2500\u2510\n1. \u2502 4593462586 \u2502 25.3.1.2703 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n1 row in set. Elapsed: 0.536 sec. Processed 4.59 billion rows, 702.42 KB (8.58 billion rows/s., 1.31 MB/s.)\nPeak memory usage: 283.52 KiB.\n```\n\nWill test this specific PR @ucasfl \nI was able to reproduce on the PR:\n\n```\nSELECT\n    count(*),\n    version()\nFROM iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs')\n\nQuery id: 4368fbd7-64b7-4f49-acc7-567f68521ee5\n\n[ip-172-31-42-241] 2025.04.07 09:57:05.798514 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> executeQuery: (from 127.0.0.1:43640) (query 1, line 1) SELECT count(*), version() FROM iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs') (stage: Complete)\n[ip-172-31-42-241] 2025.04.07 09:57:05.799132 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Warning> AwsAuthSTSAssumeRoleWebIdentityCredentialsProvider: Token file must be specified to use STS AssumeRole web identity creds provider.\n[ip-172-31-42-241] 2025.04.07 09:57:05.799173 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSClient: SSOBearerTokenProvider: Setting sso bearerToken provider to read config from default\n[ip-172-31-42-241] 2025.04.07 09:57:05.799194 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Trace> SSOCredentialsProvider: Setting sso credentials provider to read config from default\n[ip-172-31-42-241] 2025.04.07 09:57:05.799215 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> S3CredentialsProviderChain: The environment variable value AWS_CONTAINER_CREDENTIALS_RELATIVE_URI is\n[ip-172-31-42-241] 2025.04.07 09:57:05.799230 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> S3CredentialsProviderChain: The environment variable value AWS_CONTAINER_CREDENTIALS_FULL_URI is\n[ip-172-31-42-241] 2025.04.07 09:57:05.799245 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> S3CredentialsProviderChain: The environment variable value AWS_EC2_METADATA_DISABLED is\n[ip-172-31-42-241] 2025.04.07 09:57:05.799277 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSClient: ClientConfiguration: User agent is overridden in the config: ClickHouse 25.4.1.1\n[ip-172-31-42-241] 2025.04.07 09:57:05.799298 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSClient: AWSHttpResourceClient: Creating AWSHttpResourceClient with max connections 2 and scheme http\n[ip-172-31-42-241] 2025.04.07 09:57:05.799314 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Information> AWSInstanceProfileCredentialsProvider: Creating Instance with injected EC2MetadataClient and refresh rate.\n[ip-172-31-42-241] 2025.04.07 09:57:05.799336 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Information> S3CredentialsProviderChain: Added EC2 metadata service credentials provider to the provider chain.\n[ip-172-31-42-241] 2025.04.07 09:57:05.799353 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSClient: Aws::Config::AWSConfigFileProfileConfigLoader: Initializing config loader against fileName /root/.aws/credentials and using profilePrefix = 0\n[ip-172-31-42-241] 2025.04.07 09:57:05.799370 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSClient: ProfileConfigFileAWSCredentialsProvider: Setting provider to read credentials from /root/.aws/credentials for credentials file and /root/.aws/config for the config file , for use with profile default\n[ip-172-31-42-241] 2025.04.07 09:57:05.799421 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Information> AWSInstanceProfileCredentialsProvider: Credentials have expired attempting to repull from EC2 Metadata Service.\n[ip-172-31-42-241] 2025.04.07 09:57:05.799444 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Trace> AWSEC2InstanceProfileConfigLoader: Calling EC2MetadataService to get token.\n[ip-172-31-42-241] 2025.04.07 09:57:05.803384 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Information> AWSClient: Response status: 404, Not Found\n[ip-172-31-42-241] 2025.04.07 09:57:05.803428 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Information> AWSClient: AWSHttpResourceClient: Http request to retrieve credentials failed with error code 404\n[ip-172-31-42-241] 2025.04.07 09:57:05.803452 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Information> AWSClient: AWSHttpResourceClient: Can not retrieve resource from http://169.254.169.254/latest/meta-data/iam/security-credentials\n[ip-172-31-42-241] 2025.04.07 09:57:05.803488 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSEC2InstanceProfileConfigLoader: Calling EC2MetadataService resource, /latest/meta-data/iam/security-credentials with token returned profile string .\n[ip-172-31-42-241] 2025.04.07 09:57:05.803516 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Warning> AWSEC2InstanceProfileConfigLoader: Calling EC2Metadataservice to get profiles failed.\n[ip-172-31-42-241] 2025.04.07 09:57:05.803537 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSClient: Aws::Config::AWSProfileConfigLoaderBase: Failed to reload configuration.\n[ip-172-31-42-241] 2025.04.07 09:57:05.803561 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSClient: Aws::Config::AWSConfigFileProfileConfigLoader: Unable to open config file /root/.aws/credentials for reading.\n[ip-172-31-42-241] 2025.04.07 09:57:05.803572 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSClient: Aws::Config::AWSProfileConfigLoaderBase: Failed to reload configuration.\n[ip-172-31-42-241] 2025.04.07 09:57:05.803682 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> AWSClient: ClientConfiguration: User agent is overridden in the config: ClickHouse 25.4.1.1\n[ip-172-31-42-241] 2025.04.07 09:57:05.804989 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Trace> S3Client: Provider type: Unknown\n[ip-172-31-42-241] 2025.04.07 09:57:05.805013 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Trace> S3Client: API mode of the S3 client: AWS\n[ip-172-31-42-241] 2025.04.07 09:57:06.313964 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Trace> DataLakeCommon: Listed 3 files (decoded_logs/metadata/000000000000-01960276-4f34-775e-91f5-22027753a129.metadata.json, decoded_logs/metadata/000000000001-01960276-81bd-7c11-83ad-f5c0edc41efa.metadata.json, decoded_logs/metadata/000000000002-0196037e-f7b7-7952-9bea-ceb07f7db633.metadata.json)\n[ip-172-31-42-241] 2025.04.07 09:57:07.260692 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Error> executeQuery: Code: 170. DB::Exception: Bad get: has Null, requested String. (BAD_GET) (version 25.4.1.1) (from 127.0.0.1:43640) (query 1, line 1) (in query: SELECT count(*), version() FROM iceberg('https://data.agnostic.dev/agnostic-data-ice-ethereum-mainnet/decoded_logs')), Stack trace (when copying this message, always include the lines below):\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000f56079b\n1. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x0000000009f17d0c\n2. DB::Exception::Exception<std::basic_string_view<char, std::char_traits<char>>, std::basic_string_view<char, std::char_traits<char>>>(int, FormatStringHelperImpl<std::type_identity<std::basic_string_view<char, std::char_traits<char>>>::type, std::type_identity<std::basic_string_view<char, std::char_traits<char>>>::type>, std::basic_string_view<char, std::char_traits<char>>&&, std::basic_string_view<char, std::char_traits<char>>&&) @ 0x00000000123fd14b\n3. Iceberg::ManifestFileContent::ManifestFileContent(Iceberg::AvroForIcebergDeserializer const&, int, String const&, int, DB::IcebergSchemaProcessor const&, long, String const&, std::shared_ptr<DB::Context const>) @ 0x0000000011f825f7\n4. DB::IcebergMetadata::updateSnapshot() @ 0x0000000011f75e51\n5. DB::IcebergMetadata::updateState(std::shared_ptr<DB::Context const> const&) @ 0x0000000011f70c72\n6. DB::IcebergMetadata::create(std::shared_ptr<DB::IObjectStorage> const&, std::weak_ptr<DB::StorageObjectStorage::Configuration> const&, std::shared_ptr<DB::Context const> const&) @ 0x0000000011f7913f\n7. DB::DataLakeConfiguration<DB::StorageS3Configuration, DB::IcebergMetadata>::updateMetadataObjectIfNeeded(std::shared_ptr<DB::IObjectStorage>, std::shared_ptr<DB::Context const>) @ 0x00000000113b8b81\n8. DB::DataLakeConfiguration<DB::StorageS3Configuration, DB::IcebergMetadata>::update(std::shared_ptr<DB::IObjectStorage>, std::shared_ptr<DB::Context const>) @ 0x00000000113b884e\n9. DB::StorageObjectStorage::StorageObjectStorage(std::shared_ptr<DB::StorageObjectStorage::Configuration>, std::shared_ptr<DB::IObjectStorage>, std::shared_ptr<DB::Context const>, DB::StorageID const&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, String const&, std::optional<DB::FormatSettings>, DB::LoadingStrictnessLevel, bool, std::shared_ptr<DB::IAST>, bool, bool)::$_0::operator()() const @ 0x0000000011e85b0e\n10. DB::StorageObjectStorage::StorageObjectStorage(std::shared_ptr<DB::StorageObjectStorage::Configuration>, std::shared_ptr<DB::IObjectStorage>, std::shared_ptr<DB::Context const>, DB::StorageID const&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, String const&, std::optional<DB::FormatSettings>, DB::LoadingStrictnessLevel, bool, std::shared_ptr<DB::IAST>, bool, bool) @ 0x0000000011e84884\n11. std::shared_ptr<DB::StorageObjectStorage> std::allocate_shared[abi:ne190107]<DB::StorageObjectStorage, std::allocator<DB::StorageObjectStorage>, std::shared_ptr<DB::StorageObjectStorage::Configuration>&, std::shared_ptr<DB::IObjectStorage>, std::shared_ptr<DB::Context const>&, DB::StorageID, DB::ColumnsDescription&, DB::ConstraintsDescription, String, std::nullopt_t const&, DB::LoadingStrictnessLevel, bool const&, std::nullptr_t, bool, 0>(std::allocator<DB::StorageObjectStorage> const&, std::shared_ptr<DB::StorageObjectStorage::Configuration>&, std::shared_ptr<DB::IObjectStorage>&&, std::shared_ptr<DB::Context const>&, DB::StorageID&&, DB::ColumnsDescription&, DB::ConstraintsDescription&&, String&&, std::nullopt_t const&, DB::LoadingStrictnessLevel&&, bool const&, std::nullptr_t&&, bool&&) @ 0x00000000113c4e21\n12. DB::TableFunctionObjectStorage<DB::IcebergDefinition, DB::DataLakeConfiguration<DB::StorageS3Configuration, DB::IcebergMetadata>>::executeImpl(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, String const&, DB::ColumnsDescription, bool) const @ 0x00000000113c74c3\n13. DB::ITableFunction::execute(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, String const&, DB::ColumnsDescription, bool, bool) const @ 0x0000000011da786d\n14. DB::Context::executeTableFunction(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::ITableFunction> const&) @ 0x0000000012f1d69f\n15. DB::QueryAnalyzer::resolveTableFunction(std::shared_ptr<DB::IQueryTreeNode>&, DB::IdentifierResolveScope&, DB::QueryExpressionsAliasVisitor&, bool) @ 0x0000000012b34351\n16. DB::QueryAnalyzer::resolveQueryJoinTreeNode(std::shared_ptr<DB::IQueryTreeNode>&, DB::IdentifierResolveScope&, DB::QueryExpressionsAliasVisitor&) @ 0x0000000012b5ea19\n17. DB::QueryAnalyzer::resolveQuery(std::shared_ptr<DB::IQueryTreeNode> const&, DB::IdentifierResolveScope&) @ 0x0000000012b244af\n18. DB::QueryAnalyzer::resolve(std::shared_ptr<DB::IQueryTreeNode>&, std::shared_ptr<DB::IQueryTreeNode> const&, std::shared_ptr<DB::Context const>) @ 0x0000000012b227cc\n19. DB::QueryAnalysisPass::run(std::shared_ptr<DB::IQueryTreeNode>&, std::shared_ptr<DB::Context const>) @ 0x0000000012b22033\n20. DB::QueryTreePassManager::run(std::shared_ptr<DB::IQueryTreeNode>) @ 0x0000000012b76e16\n21. DB::buildQueryTreeAndRunPasses(std::shared_ptr<DB::IAST> const&, DB::SelectQueryOptions const&, std::shared_ptr<DB::Context const> const&, std::shared_ptr<DB::IStorage> const&) (.llvm.11587452827065239245) @ 0x0000000013427dc5\n22. DB::InterpreterSelectQueryAnalyzer::InterpreterSelectQueryAnalyzer(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013425d5c\n23. std::unique_ptr<DB::IInterpreter, std::default_delete<DB::IInterpreter>> std::__function::__policy_invoker<std::unique_ptr<DB::IInterpreter, std::default_delete<DB::IInterpreter>> (DB::InterpreterFactory::Arguments const&)>::__call_impl[abi:ne190107]<std::__function::__default_alloc_func<DB::registerInterpreterSelectQueryAnalyzer(DB::InterpreterFactory&)::$_0, std::unique_ptr<DB::IInterpreter, std::default_delete<DB::IInterpreter>> (DB::InterpreterFactory::Arguments const&)>>(std::__function::__policy_storage const*, DB::InterpreterFactory::Arguments const&) (.llvm.11587452827065239245) @ 0x00000000134296a2\n24. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000133d08eb\n25. DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*, std::shared_ptr<DB::IAST>&) @ 0x00000000137d314a\n26. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum) @ 0x00000000137ce924\n27. DB::TCPHandler::runImpl() @ 0x0000000014b19b8c\n28. DB::TCPHandler::run() @ 0x0000000014b37e79\n29. Poco::Net::TCPServerConnection::start() @ 0x00000000182076a7\n30. Poco::Net::TCPServerDispatcher::run() @ 0x0000000018207af9\n31. Poco::PooledThread::run() @ 0x00000000181d2dfb\n\n[ip-172-31-42-241] 2025.04.07 09:57:07.260870 [ 1569 ] {4368fbd7-64b7-4f49-acc7-567f68521ee5} <Debug> TCPHandler: Processed in 1.462847037 sec.\n\nElapsed: 1.465 sec.\n\nReceived exception from server (version 25.4.1):\nCode: 170. DB::Exception: Received from localhost:9000. DB::Exception: Bad get: has Null, requested String. (BAD_GET)\n```",
  "created_at": "2025-04-07T10:43:10Z",
  "modified_files": [
    "src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_iceberg/test.py"
  ]
}