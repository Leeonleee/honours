You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Throttle recovery speed
We're in a process of migration from older hardware under ClickHouse to the newer generation. 

Older machines have 12x6T disks, 128GB RAM and 2x10G NICs, newer machines have 12x10T disks, 256GB RAM and 2x25G NICs. Dataset per replica is around 35TiB. Each shard is 3 replicas.

Our process is:

1. Stop one replica from shard.
2. Clear it from zookeeper.
3. Remove it from cluster topology (znode update for `remote_servers`).
4. Add new replica to cluster topology.
5. Start new replica and let it replicate all the data from peers.

The issue we're seeing is that source replicas saturate disks, starving user queries and merges.

It takes ~7h to replicate full dataset, below are the graphs for 12h around that time:

![image](https://user-images.githubusercontent.com/89186/35352138-d88ca43c-00f7-11e8-8aa9-fd755fb6e4a7.png)

![image](https://user-images.githubusercontent.com/89186/35352198-11003a04-00f8-11e8-9d19-eb94db646119.png)

![image](https://user-images.githubusercontent.com/89186/35352253-3a13a5c0-00f8-11e8-941e-371a251b1a7d.png)

![image](https://user-images.githubusercontent.com/89186/35352271-481fe08e-00f8-11e8-8f95-c271c43bc415.png)

![image](https://user-images.githubusercontent.com/89186/35352296-62049efe-00f8-11e8-83d5-548e833ef99f.png)

Source peer:

![image](https://user-images.githubusercontent.com/89186/35352345-82c44928-00f8-11e8-96b9-a63022a35a04.png)

![image](https://user-images.githubusercontent.com/89186/35352369-9019f596-00f8-11e8-8b5f-586e37ae4b93.png)

Target peer:

![image](https://user-images.githubusercontent.com/89186/35352422-b394c9f6-00f8-11e8-98e2-d589cee15439.png)

![image](https://user-images.githubusercontent.com/89186/35352437-c5fade50-00f8-11e8-8590-0168e801ef2c.png)

Naturally, source peers are not great at IO in the first place (that's why we're upgrading), but having 7h of degraded service is not great. It'd be nice to be able to set recovery speed, so it doesn't starve  other activities like user queries and merges.

Moreover, max number of parts in partition quickly reaches the max (we set to 1000) on the target replica, where inserts are throttled, which doesn't make things any better. With throttled recovery at lower speed but with higher duration, this will probably be even longer period. Maybe we should split threads that do merges and threads that do replication, it seems like whole pool is busy just replicating.

It is also possible that we're just doing it wrong, then it'd be great to have a guide describing the process.

cc @vavrusa, @dqminh, @bocharov
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
