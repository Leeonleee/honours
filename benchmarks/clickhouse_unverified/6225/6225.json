{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 6225,
  "instance_id": "ClickHouse__ClickHouse-6225",
  "issue_numbers": [
    "3764"
  ],
  "base_commit": "bccc7ae9bb200526229fd1780af884379b3e3af9",
  "patch": "diff --git a/dbms/src/DataStreams/CubeBlockInputStream.cpp b/dbms/src/DataStreams/CubeBlockInputStream.cpp\nindex c32378d97e67..50a6c0a970b2 100644\n--- a/dbms/src/DataStreams/CubeBlockInputStream.cpp\n+++ b/dbms/src/DataStreams/CubeBlockInputStream.cpp\n@@ -36,43 +36,58 @@ Block CubeBlockInputStream::getHeader() const\n \n Block CubeBlockInputStream::readImpl()\n {\n-    /** After reading a block from input stream,\n+    /** After reading all blocks from input stream,\n       * we will calculate all subsets of columns on next iterations of readImpl\n       * by zeroing columns at positions, where bits are zero in current bitmask.\n       */\n-    if (mask)\n+\n+    if (!is_data_read)\n     {\n-        --mask;\n-        Block cube_block = source_block;\n-        for (size_t i = 0; i < keys.size(); ++i)\n+        BlocksList source_blocks;\n+        while (auto block = children[0]->read())\n+            source_blocks.push_back(block);\n+\n+        if (source_blocks.empty())\n+            return {};\n+\n+        is_data_read = true;\n+        mask = (1 << keys.size()) - 1;\n+\n+        if (source_blocks.size() > 1)\n+            source_block = aggregator.mergeBlocks(source_blocks, false);\n+        else\n+            source_block = std::move(source_blocks.front());\n+\n+        zero_block = source_block.cloneEmpty();\n+        for (auto key : keys)\n         {\n-            if (!((mask >> i) & 1))\n-            {\n-                size_t pos = keys.size() - i - 1;\n-                auto & current = cube_block.getByPosition(keys[pos]);\n-                current.column = zero_block.getByPosition(keys[pos]).column;\n-            }\n+            auto & current = zero_block.getByPosition(key);\n+            current.column = current.column->cloneResized(source_block.rows());\n         }\n \n-        BlocksList cube_blocks = { cube_block };\n-        Block finalized = aggregator.mergeBlocks(cube_blocks, true);\n+        auto finalized = source_block;\n+        finalizeBlock(finalized);\n         return finalized;\n     }\n \n-    source_block = children[0]->read();\n-    if (!source_block)\n-        return source_block;\n+    if (!mask)\n+        return {};\n+\n+    --mask;\n+    auto cube_block = source_block;\n \n-    zero_block = source_block.cloneEmpty();\n-    for (auto key : keys)\n+    for (size_t i = 0; i < keys.size(); ++i)\n     {\n-        auto & current = zero_block.getByPosition(key);\n-        current.column = current.column->cloneResized(source_block.rows());\n+        if (!((mask >> i) & 1))\n+        {\n+            size_t pos = keys.size() - i - 1;\n+            auto & current = cube_block.getByPosition(keys[pos]);\n+            current.column = zero_block.getByPosition(keys[pos]).column;\n+        }\n     }\n-    Block finalized = source_block;\n-    finalizeBlock(finalized);\n-    mask = (1 << keys.size()) - 1;\n \n+    BlocksList cube_blocks = { cube_block };\n+    Block finalized = aggregator.mergeBlocks(cube_blocks, true);\n     return finalized;\n }\n }\ndiff --git a/dbms/src/DataStreams/CubeBlockInputStream.h b/dbms/src/DataStreams/CubeBlockInputStream.h\nindex 2f435a6031cf..7e62950e8ee6 100644\n--- a/dbms/src/DataStreams/CubeBlockInputStream.h\n+++ b/dbms/src/DataStreams/CubeBlockInputStream.h\n@@ -36,6 +36,7 @@ class CubeBlockInputStream : public IBlockInputStream\n     UInt32 mask = 0;\n     Block source_block;\n     Block zero_block;\n+    bool is_data_read = false;\n };\n \n }\ndiff --git a/dbms/src/DataStreams/RollupBlockInputStream.cpp b/dbms/src/DataStreams/RollupBlockInputStream.cpp\nindex e43aa51e6177..a913dc727fa0 100644\n--- a/dbms/src/DataStreams/RollupBlockInputStream.cpp\n+++ b/dbms/src/DataStreams/RollupBlockInputStream.cpp\n@@ -33,26 +33,40 @@ Block RollupBlockInputStream::readImpl()\n       * by zeroing out every column one-by-one and re-merging a block.\n       */\n \n-    if (current_key >= 0)\n+    if (!is_data_read)\n     {\n-        auto & current = rollup_block.getByPosition(keys[current_key]);\n-        current.column = current.column->cloneEmpty()->cloneResized(rollup_block.rows());\n-        --current_key;\n+        BlocksList source_blocks;\n+        while (auto block = children[0]->read())\n+            source_blocks.push_back(block);\n \n-        BlocksList rollup_blocks = { rollup_block };\n-        rollup_block = aggregator.mergeBlocks(rollup_blocks, false);\n+        if (source_blocks.empty())\n+            return {};\n \n-        Block finalized = rollup_block;\n+        is_data_read = true;\n+        if (source_blocks.size() > 1)\n+            rollup_block = aggregator.mergeBlocks(source_blocks, false);\n+        else\n+            rollup_block = std::move(source_blocks.front());\n+\n+        current_key = keys.size() - 1;\n+\n+        auto finalized = rollup_block;\n         finalizeBlock(finalized);\n         return finalized;\n     }\n \n-    Block block = children[0]->read();\n-    current_key = keys.size() - 1;\n+    if (current_key < 0)\n+        return {};\n+\n+    auto & current = rollup_block.getByPosition(keys[current_key]);\n+    current.column = current.column->cloneEmpty()->cloneResized(rollup_block.rows());\n+    --current_key;\n \n-    rollup_block = block;\n-    finalizeBlock(block);\n+    BlocksList rollup_blocks = { rollup_block };\n+    rollup_block = aggregator.mergeBlocks(rollup_blocks, false);\n \n-    return block;\n+    auto finalized = rollup_block;\n+    finalizeBlock(finalized);\n+    return finalized;\n }\n }\ndiff --git a/dbms/src/DataStreams/RollupBlockInputStream.h b/dbms/src/DataStreams/RollupBlockInputStream.h\nindex 1c1e29e7a134..dabf1e392a31 100644\n--- a/dbms/src/DataStreams/RollupBlockInputStream.h\n+++ b/dbms/src/DataStreams/RollupBlockInputStream.h\n@@ -35,6 +35,7 @@ class RollupBlockInputStream : public IBlockInputStream\n     ColumnNumbers keys;\n     ssize_t current_key = -1;\n     Block rollup_block;\n+    bool is_data_read = false;\n };\n \n }\n",
  "test_patch": "diff --git a/dbms/tests/queries/0_stateless/00701_rollup.reference b/dbms/tests/queries/0_stateless/00701_rollup.reference\nindex ec07ad52cae2..637ae0bcb52d 100644\n--- a/dbms/tests/queries/0_stateless/00701_rollup.reference\n+++ b/dbms/tests/queries/0_stateless/00701_rollup.reference\n@@ -25,3 +25,13 @@ a\t70\t4\n b\t50\t4\n \n \t120\t8\n+\t120\t8\n+a\t70\t4\n+b\t50\t4\n+\t0\t120\t8\n+a\t0\t70\t4\n+a\t1\t25\t2\n+a\t2\t45\t2\n+b\t0\t50\t4\n+b\t1\t15\t2\n+b\t2\t35\t2\ndiff --git a/dbms/tests/queries/0_stateless/00701_rollup.sql b/dbms/tests/queries/0_stateless/00701_rollup.sql\nindex 3f4df923f903..fa7f3a21657f 100644\n--- a/dbms/tests/queries/0_stateless/00701_rollup.sql\n+++ b/dbms/tests/queries/0_stateless/00701_rollup.sql\n@@ -1,14 +1,9 @@\n DROP TABLE IF EXISTS rollup;\n CREATE TABLE rollup(a String, b Int32, s Int32) ENGINE = Memory;\n \n-INSERT INTO rollup  VALUES('a', 1, 10);\n-INSERT INTO rollup  VALUES('a', 1, 15);\n-INSERT INTO rollup  VALUES('a', 2, 20);\n-INSERT INTO rollup  VALUES('a', 2, 25);\n-INSERT INTO rollup  VALUES('b', 1, 10);\n-INSERT INTO rollup  VALUES('b', 1, 5);\n-INSERT INTO rollup  VALUES('b', 2, 20);\n-INSERT INTO rollup  VALUES('b', 2, 15);\n+INSERT INTO rollup VALUES ('a', 1, 10), ('a', 1, 15), ('a', 2, 20);\n+INSERT INTO rollup VALUES ('a', 2, 25), ('b', 1, 10), ('b', 1, 5);\n+INSERT INTO rollup VALUES ('b', 2, 20), ('b', 2, 15);\n \n SELECT a, b, sum(s), count() from rollup GROUP BY ROLLUP(a, b) ORDER BY a, b;\n \n@@ -20,4 +15,9 @@ SELECT a, sum(s), count() from rollup GROUP BY a WITH ROLLUP ORDER BY a;\n \n SELECT a, sum(s), count() from rollup GROUP BY a WITH ROLLUP WITH TOTALS ORDER BY a;\n \n+SET group_by_two_level_threshold = 1;\n+\n+SELECT a, sum(s), count() from rollup GROUP BY a WITH ROLLUP ORDER BY a;\n+SELECT a, b, sum(s), count() from rollup GROUP BY a, b WITH ROLLUP ORDER BY a, b;\n+\n DROP TABLE rollup;\ndiff --git a/dbms/tests/queries/0_stateless/00720_with_cube.reference b/dbms/tests/queries/0_stateless/00720_with_cube.reference\nindex a0b951978f9b..818e8626dde8 100644\n--- a/dbms/tests/queries/0_stateless/00720_with_cube.reference\n+++ b/dbms/tests/queries/0_stateless/00720_with_cube.reference\n@@ -18,8 +18,8 @@ b\t1\t15\t2\n b\t2\t35\t2\n \n \t0\t120\t8\n-\t1\t40\t4\n \t0\t120\t8\n+\t1\t40\t4\n \t2\t80\t4\n a\t0\t70\t4\n a\t1\t25\t2\n@@ -27,8 +27,8 @@ a\t2\t45\t2\n b\t0\t50\t4\n b\t1\t15\t2\n b\t2\t35\t2\n-\t1\t40\t4\n \t0\t120\t8\n+\t1\t40\t4\n \t2\t80\t4\n a\t0\t70\t4\n a\t1\t25\t2\n@@ -38,3 +38,12 @@ b\t1\t15\t2\n b\t2\t35\t2\n \n \t0\t120\t8\n+\t0\t120\t8\n+\t1\t40\t4\n+\t2\t80\t4\n+a\t0\t70\t4\n+a\t1\t25\t2\n+a\t2\t45\t2\n+b\t0\t50\t4\n+b\t1\t15\t2\n+b\t2\t35\t2\ndiff --git a/dbms/tests/queries/0_stateless/00720_with_cube.sql b/dbms/tests/queries/0_stateless/00720_with_cube.sql\nindex bcde617803e3..42b65c8222c4 100644\n--- a/dbms/tests/queries/0_stateless/00720_with_cube.sql\n+++ b/dbms/tests/queries/0_stateless/00720_with_cube.sql\n@@ -1,21 +1,21 @@\n-DROP TABLE IF EXISTS rollup;\n-CREATE TABLE rollup(a String, b Int32, s Int32) ENGINE = Memory;\n+DROP TABLE IF EXISTS cube;\n+CREATE TABLE cube(a String, b Int32, s Int32) ENGINE = Memory;\n \n-INSERT INTO rollup  VALUES('a', 1, 10);\n-INSERT INTO rollup  VALUES('a', 1, 15);\n-INSERT INTO rollup  VALUES('a', 2, 20);\n-INSERT INTO rollup  VALUES('a', 2, 25);\n-INSERT INTO rollup  VALUES('b', 1, 10);\n-INSERT INTO rollup  VALUES('b', 1, 5);\n-INSERT INTO rollup  VALUES('b', 2, 20);\n-INSERT INTO rollup  VALUES('b', 2, 15);\n+-- SET experimental_use_processors=1;\n \n-SELECT a, b, sum(s), count() from rollup GROUP BY CUBE(a, b) ORDER BY a, b;\n+INSERT INTO cube VALUES ('a', 1, 10), ('a', 1, 15), ('a', 2, 20);\n+INSERT INTO cube VALUES ('a', 2, 25), ('b', 1, 10), ('b', 1, 5);\n+INSERT INTO cube VALUES ('b', 2, 20), ('b', 2, 15);\n \n-SELECT a, b, sum(s), count() from rollup GROUP BY CUBE(a, b) WITH TOTALS ORDER BY a, b;\n+SELECT a, b, sum(s), count() from cube GROUP BY CUBE(a, b) ORDER BY a, b;\n \n-SELECT a, b, sum(s), count() from rollup GROUP BY a, b WITH CUBE ORDER BY a;\n+SELECT a, b, sum(s), count() from cube GROUP BY CUBE(a, b) WITH TOTALS ORDER BY a, b;\n \n-SELECT a, b, sum(s), count() from rollup GROUP BY a, b WITH CUBE WITH TOTALS ORDER BY a;\n+SELECT a, b, sum(s), count() from cube GROUP BY a, b WITH CUBE ORDER BY a, b;\n \n-DROP TABLE rollup;\n+SELECT a, b, sum(s), count() from cube GROUP BY a, b WITH CUBE WITH TOTALS ORDER BY a, b;\n+\n+SET group_by_two_level_threshold = 1;\n+SELECT a, b, sum(s), count() from cube GROUP BY a, b WITH CUBE ORDER BY a, b;\n+\n+DROP TABLE cube;\n",
  "problem_statement": "cube/rollup got wrong result in large data with multi-threads\nMy data is large, more than 100M lines, \r\n\r\n=> when 'SET max_threads = 1', result is right\r\n=> SELECT \r\n    gender, \r\n    platform, \r\n    count()\r\nFROM test.xxx\r\nWHERE ((gender = 'M') OR (gender = 'F')) AND (platform = 'IPHONE')\r\nGROUP BY \r\n    gender, \r\n    platform\r\n    WITH CUBE\r\n\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 M      \u2502 IPHONE   \u2502 Num1 \u2502\r\n\u2502 F      \u2502 IPHONE   \u2502  Num2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 F      \u2502 \u1d3a\u1d41\u1d38\u1d38     \u2502  Num2 \u2502\r\n\u2502 M      \u2502 \u1d3a\u1d41\u1d38\u1d38     \u2502  Num1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502        \u2502 IPHONE   \u2502  Num1 + Num2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502        \u2502 \u1d3a\u1d41\u1d38\u1d38     \u2502 Num1 + Num2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n6 rows in set. Elapsed: 0.253 sec. \r\n\r\n=> SET max_threads = 50, got wrong result\r\n=> SELECT \r\n    gender, \r\n    platform, \r\n    count()\r\nFROM ks_dws_test.party_allprod_user_active_di_local \r\nWHERE ((gender = 'M') OR (gender = 'F')) AND (platform = 'IPHONE')\r\nGROUP BY \r\n    gender, \r\n    platform\r\n    WITH CUBE\r\n\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 M      \u2502 IPHONE   \u2502  Num1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 M      \u2502 \u1d3a\u1d41\u1d38\u1d38     \u2502  Num1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502        \u2502 IPHONE   \u2502  Num1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502        \u2502 \u1d3a\u1d41\u1d38\u1d38     \u2502  Num1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 F      \u2502 IPHONE   \u2502  Num2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 F      \u2502 \u1d3a\u1d41\u1d38\u1d38     \u2502  Num2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502        \u2502 IPHONE   \u2502  Num2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502        \u2502 \u1d3a\u1d41\u1d38\u1d38     \u2502  Num2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n8 rows in set. Elapsed: 0.075 sec. \r\n\r\nI guess, In the result of '50 threads', the line 1 and line 5 are somehow in different blocks, so they are calculated by 'Cube' operator separately, so the final result can't merged. \n",
  "hints_text": "this problem is very wired, keep max_threads=52, set max_block_size=65529, the result sometimes is right, sometimes is wrong, if set max_block_size larger, the result is always wrong, is set max_block_size smaller, the result is always right. btw, our defult max_block_size = 5242880.\r\nif set max_threads=1, the result is right\nsomehow, if these 2 lines are in one block, like following, the result is right,\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 M      \u2502 IPHONE   \u2502  Num1 \u2502\r\n\u2502 F      \u2502 IPHONE   \u2502  Num2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nif these 2 lines are in different blocks, the result is wrong, like following:\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 M      \u2502 IPHONE   \u2502  Num1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500gender\u2500\u252c\u2500platform\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 F     \u2502 IPHONE   \u2502  Num2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\nthe wrong result are always occurred with  'Aggregator: Converting aggregation data to two-level.'\nyou can reproduce my problem by 'set group_by_two_level_threshold=1;'\r\n\r\nusing a table like following:\r\nselect * from test.r2;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500id\u2500\u2510\r\n\u2502 2018-10-01 \u2502  1 \u2502\r\n\u2502 2018-10-02 \u2502  2 \u2502\r\n\u2502 2018-11-01 \u2502  1 \u2502\r\n\u2502 2018-11-03 \u2502  2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\r\nset group_by_two_level_threshold=1;\r\nselect date,id,count() from test.r2 group by date,id with cube\nIn line 62 of CubeBlockInputStream.cpp,\r\n source_block = children[0]->read();\r\nCH get a block from its underlying stream, and do some mask on it, but we can't guarantee all related data (with the same group by key) is in one block, maybe there are too many rows or some related data crossed blocks. \nactually I have write a small fix to this problem\nhere is my small fix, maybe you can check whether they are right:\r\n\r\nadd these fields to CubeBlockInputStream.h:\r\n    BlocksList ori_blocks;\r\n    bool hasMasked = false;\r\n\r\nadd new read method of CubeBlockInputStream.cpp:\r\n````\r\nBlock CubeBlockInputStream::readImpl()\r\n{\r\n    if (hasMasked){\r\n        return {};\r\n    }else {\r\n        source_block = children[0]->read();\r\n        if (!source_block) {\r\n            BlocksList cube_blocks;\r\n            for (auto &block : ori_blocks) {\r\n                zero_block = block.cloneEmpty();\r\n                for (auto key : keys)\r\n                {\r\n                    auto & current = zero_block.getByPosition(key);\r\n                    current.column = current.column->cloneResized(block.rows());\r\n                }\r\n\r\n                mask = (1 << keys.size()) - 1;\r\n                while (mask) {\r\n                    --mask;\r\n                    Block cube_block = block;\r\n                    for (size_t i = 0; i < keys.size(); ++i) {\r\n                        if (!((mask >> i) & 1)) {\r\n                            size_t pos = keys.size() - i - 1;\r\n                            auto &current = cube_block.getByPosition(keys[pos]);\r\n                            current.column = zero_block.getByPosition(keys[pos]).column;\r\n                        }\r\n                    }\r\n                    cube_blocks.push_back(cube_block);\r\n                }\r\n            }\r\n\r\n            hasMasked = true;\r\n\r\n            Block finalized = aggregator.mergeBlocks(cube_blocks, true);\r\n            return finalized;\r\n        } else {\r\n            ori_blocks.push_back(source_block);\r\n\r\n            Block finalized = source_block;\r\n            finalizeBlock(finalized);\r\n\r\n            return finalized;\r\n        }\r\n    }\r\n}\r\n```\n> here is my small fix, maybe you can check whether they are right:\r\n\r\nGreat that you find the exact reason and even create fix for that!\r\n\r\nCould you create a PR with your changes (also with a test-case, please)? That will allow running the tests suite with your changes and will allow making a code review (and do any adjustments if will be needed) in a simple and straight-ahead way.\nsure, let me try\nHi, @filimonov, you can see my fix at https://github.com/shangshujie365/ClickHouse/commit/4f143af9a00d36c115c239d2466131d8509016d5.\r\nThe bug has been fixed, but introduce another bug, \r\nif gender or platform has 'null' value, the partial cube result (gender=null, platform-cubed-to-null), (gender-cubed-to-null, platform-cubed-to-null) will be aggregated together, so the result will be wrong.\r\nMay be we should introduce a new column 'cubeId', its value is the column not be cubed, in this case, it should be 'gender,platform', 'gender', 'platform', 'null'\nhi @shangshujie365 \r\n\r\nWe have the same problem, is the problem fixed now?\n@viongpanzi \r\nnot yet, if your column data has no null value, my fix can work.\r\nIf your data has null value, the null value result will be wrong.\n@shangshujie365 thx~\n@blinkov @KochetovNicolai this should be very high priority, please.  We have to run `SET max_threads=1;` before *all* of our `WITH ROLLUP` queries because otherwise ClickHouse returns unusable data.  Setting max_threads to 1 adversely affects ClickHouse performance.\nActually, we can get accurate results without changing _max_threads_ to 1 but rather disabling _group_by_two_level_ completely: \r\n```\r\nSET group_by_two_level_threshold = 0;\r\nSET group_by_two_level_threshold_bytes = 0;\r\n```\r\nAre there bad side effects to doing this?\nNo response yet, so I'll ask again: what are the consequences of disabling `group_by_two_level`?  Since this issue is not fixed, it is the only workaround we have, and searching through ClickHouse documentation hasn't yielded anything.  Thanks.\n> No response yet, so I'll ask again: what are the consequences of disabling `group_by_two_level`?\r\n\r\nShould be mostly performance and scalability.\n> what are the consequences of disabling group_by_two_level\r\n\r\nSevere performance degradation of GROUP BY with large result (merging will be done single-threaded).\nWould be great with a patch for this. @shangshujie365 do you plan on submitting yours?",
  "created_at": "2019-07-30T16:45:42Z",
  "modified_files": [
    "dbms/src/DataStreams/CubeBlockInputStream.cpp",
    "dbms/src/DataStreams/CubeBlockInputStream.h",
    "dbms/src/DataStreams/RollupBlockInputStream.cpp",
    "dbms/src/DataStreams/RollupBlockInputStream.h"
  ],
  "modified_test_files": [
    "dbms/tests/queries/0_stateless/00701_rollup.reference",
    "dbms/tests/queries/0_stateless/00701_rollup.sql",
    "dbms/tests/queries/0_stateless/00720_with_cube.reference",
    "dbms/tests/queries/0_stateless/00720_with_cube.sql"
  ]
}