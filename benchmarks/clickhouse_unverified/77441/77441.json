{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 77441,
  "instance_id": "ClickHouse__ClickHouse-77441",
  "issue_numbers": [
    "75575"
  ],
  "base_commit": "72b580293e94f1a9ea1e6b2c924bbaebb5ada63f",
  "patch": "diff --git a/src/Storages/FileLog/StorageFileLog.cpp b/src/Storages/FileLog/StorageFileLog.cpp\nindex 70e8aa9c8640..351ec19e35b7 100644\n--- a/src/Storages/FileLog/StorageFileLog.cpp\n+++ b/src/Storages/FileLog/StorageFileLog.cpp\n@@ -18,6 +18,7 @@\n #include <Processors/Executors/CompletedPipelineExecutor.h>\n #include <Processors/QueryPlan/QueryPlan.h>\n #include <Processors/QueryPlan/ReadFromStreamLikeEngine.h>\n+#include <Processors/Sources/NullSource.h>\n #include <QueryPipeline/Pipe.h>\n #include <Storages/FileLog/FileLogSettings.h>\n #include <Storages/FileLog/FileLogSource.h>\n@@ -124,7 +125,11 @@ class ReadFromStorageFileLog final : public ReadFromStreamLikeEngine\n         if (file_log.file_infos.file_names.empty())\n         {\n             LOG_WARNING(file_log.log, \"There is a idle table named {}, no files need to parse.\", getName());\n-            return Pipe{};\n+            Header header;\n+            auto column_names_and_types = storage_snapshot->getColumnsByNames(GetColumnsOptions::All, column_names);\n+            for (const auto & [name, type] : column_names_and_types)\n+                header.insert(ColumnWithTypeAndName(type, name));\n+            return Pipe(std::make_unique<NullSource>(header));\n         }\n \n         auto modified_context = Context::createCopy(file_log.filelog_context);\n",
  "test_patch": "diff --git a/tests/clickhouse-test b/tests/clickhouse-test\nindex 7bf0a6f5e13d..fcb5dd827746 100755\n--- a/tests/clickhouse-test\n+++ b/tests/clickhouse-test\n@@ -1851,6 +1851,10 @@ class TestCase:\n                 + database\n                 + \"_1\"\n             )\n+            for env_to_param in [\"CLICKHOUSE_USER_FILES\", \"CLICKHOUSE_USER_FILES_UNIQUE\"]:\n+                value = os.environ.get(env_to_param, None)\n+                if value:\n+                    query_params += f\" --param_{env_to_param}={value}\"\n \n         params = {\n             \"client\": client + \" --database=\" + database + query_params,\n@@ -2266,7 +2270,7 @@ class TestSuite:\n                 try:\n                     if filepath.endswith(\".sql\"):\n                         for line in lines:\n-                            if \"{CLICKHOUSE_DATABASE\" in line:\n+                            if \"{CLICKHOUSE_\" in line:\n                                 need_query_params = True\n                 except UnicodeDecodeError:\n                     pass\ndiff --git a/tests/queries/0_stateless/03381_file_log_merge_empty.reference b/tests/queries/0_stateless/03381_file_log_merge_empty.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/03381_file_log_merge_empty.sh b/tests/queries/0_stateless/03381_file_log_merge_empty.sh\nnew file mode 100755\nindex 000000000000..ae2fe211ad8e\n--- /dev/null\n+++ b/tests/queries/0_stateless/03381_file_log_merge_empty.sh\n@@ -0,0 +1,16 @@\n+#!/usr/bin/env bash\n+\n+set -eu\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+mkdir -p ${USER_FILES_PATH}/${CLICKHOUSE_TEST_UNIQUE_NAME}\n+\n+${CLICKHOUSE_CLIENT} --query \"DROP TABLE IF EXISTS file_log;\"\n+\n+${CLICKHOUSE_CLIENT} --query \"CREATE TABLE file_log  (key UInt8, value UInt8) ENGINE = FileLog('${USER_FILES_PATH}/${CLICKHOUSE_TEST_UNIQUE_NAME}', 'JSONEachRow') SETTINGS handle_error_mode = 'stream'\"\n+\n+# send_logs_level: reading from empty file log produces warning\n+${CLICKHOUSE_CLIENT} --stream_like_engine_allow_direct_select=1 --query \"select * from merge('', 'file_log') SETTINGS send_logs_level='error';\"\n",
  "problem_statement": "Can't initialize pipeline with empty pipe\n### Describe the bug\n\nhttps://s3.amazonaws.com/clickhouse-test-reports/0/fbb8b68b4c5925b499436dc568f6744f01af7505/stress_test__msan_.html\n\n```\n2025.02.04 17:09:37.139502 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Debug> executeQuery: (from [::1]:36078) (comment: 01551_context_uaf.sql) (query 6, line 7) select * from merge('', 'f'); (stage: Complete)\n2025.02.04 17:09:37.140197 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Test> CancellationChecker: Added to set. query: select * from merge('', 'f');, timeout: 60000 milliseconds\n2025.02.04 17:09:37.146020 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Trace> QueryCache: No query result found for query \"SELECT * FROM merge('', 'f')\"\n2025.02.04 17:09:37.252628 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Trace> Planner: Query to stage Complete\n2025.02.04 17:09:37.259176 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Trace> Planner: Query from stage FetchColumns to stage Complete\n2025.02.04 17:09:37.381400 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Debug> test_11.column_modify_test (b73aed94-e52b-4af2-a520-83c67a873299) (SelectExecutor): Key condition: unknown\n2025.02.04 17:09:37.381744 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Trace> test_11.column_modify_test (b73aed94-e52b-4af2-a520-83c67a873299) (SelectExecutor): Filtering marks by primary and secondary keys\n2025.02.04 17:09:37.382347 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Debug> test_11.column_modify_test (b73aed94-e52b-4af2-a520-83c67a873299) (SelectExecutor): Selected 3/3 parts by partition key, 3 parts by primary key, 3/3 marks by primary key, 3 marks to read from 3 ranges\n2025.02.04 17:09:37.382644 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Trace> test_11.column_modify_test (b73aed94-e52b-4af2-a520-83c67a873299) (SelectExecutor): Spreading mark ranges among streams (default reading)\n2025.02.04 17:09:37.383439 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Test> MergeTreeReadPoolBase: Will use min_marks_per_task=51\n2025.02.04 17:09:37.383696 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Test> MergeTreeReadPoolBase: Will use min_marks_per_task=51\n2025.02.04 17:09:37.383842 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Test> MergeTreeReadPoolBase: Will use min_marks_per_task=51\n2025.02.04 17:09:37.384113 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Trace> test_11.column_modify_test (b73aed94-e52b-4af2-a520-83c67a873299) (SelectExecutor): Reading 1 ranges in order from part all_1_1_0_5, approx. 1 rows starting from 0\n2025.02.04 17:09:37.384512 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Trace> test_11.column_modify_test (b73aed94-e52b-4af2-a520-83c67a873299) (SelectExecutor): Reading 1 ranges in order from part all_2_2_0_5, approx. 1 rows starting from 0\n2025.02.04 17:09:37.384718 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Trace> test_11.column_modify_test (b73aed94-e52b-4af2-a520-83c67a873299) (SelectExecutor): Reading 1 ranges in order from part all_4_4_0_5, approx. 1 rows starting from 0\n2025.02.04 17:09:37.397707 [ 4270 ] {038fa712-629d-4900-9892-1120701e3021} <Warning> StorageFileLog (file_log): There is a idle table named ReadFromStorageFileLog, no files need to parse.\n\n2025.02.04 17:11:12.612304 [ 41134 ] {} <Fatal> BaseDaemon: ########################################\n2025.02.04 17:11:12.613027 [ 41134 ] {} <Fatal> BaseDaemon: (version 25.2.1.964 (official build), build id: 5BD3595889D58D5F7E141CD7BCF5DF2A5066E287, git hash: fbb8b68b4c5925b499436dc568f6744f01af7505) (from thread 4270) (query_id: 038fa712-629d-4900-9892-1120701e3021) (query: select * from merge('', 'f');) Received signal Aborted (6)\n2025.02.04 17:11:12.613574 [ 41134 ] {} <Fatal> BaseDaemon: \n2025.02.04 17:11:12.614045 [ 41134 ] {} <Fatal> BaseDaemon: Stack trace: 0x00005623d38c3ffe 0x00005623d42dc037 0x00005623bb7a6bae 0x00007f07f43fd520 0x00007f07f44519fd 0x00007f07f43fd476 0x00007f07f43e37f3 0x00005623d381738b 0x00005623d3818de1 0x00005623bb7cdf55 0x00005623bb7f3bc4 0x00005623e82a18e4 0x00005623f55cd151 0x00005623f52c6e98 0x00005623f537786c 0x00005623f17e007f 0x00005623f17dc00e 0x00005623f52c6e98 0x00005623f537786c 0x00005623ebc7abc5 0x00005623ebc7a561 0x00005623ec8c9e58 0x00005623ec8ba539 0x00005623f3f48901 0x00005623f3fafebc 0x00005623fa67c300 0x00005623fa67d232 0x00005623fa55c635 0x00005623fa55924e 0x00005623fa555df1 0x00007f07f444fac3 0x00007f07f44e1850\n2025.02.04 17:11:12.962407 [ 41134 ] {} <Fatal> BaseDaemon: 0.0. inlined from ./build_docker/./src/Common/StackTrace.cpp:381: StackTrace::tryCapture()\n2025.02.04 17:11:12.963119 [ 41134 ] {} <Fatal> BaseDaemon: 0. ./build_docker/./src/Common/StackTrace.cpp:350: StackTrace::StackTrace(ucontext_t const&) @ 0x0000000020d2cffe\n2025.02.04 17:11:13.313386 [ 41134 ] {} <Fatal> BaseDaemon: 1. ./build_docker/./src/Common/SignalHandlers.cpp:104: signalHandler(int, siginfo_t*, void*) @ 0x0000000021745037\n2025.02.04 17:11:13.730550 [ 41134 ] {} <Fatal> BaseDaemon: 2. SignalAction(int, void*, void*) @ 0x0000000008c0fbae\n2025.02.04 17:11:13.730685 [ 41134 ] {} <Fatal> BaseDaemon: 3. ? @ 0x00007f07f43fd520\n2025.02.04 17:11:13.730748 [ 41134 ] {} <Fatal> BaseDaemon: 4. ? @ 0x00007f07f44519fd\n2025.02.04 17:11:13.730817 [ 41134 ] {} <Fatal> BaseDaemon: 5. ? @ 0x00007f07f43fd476\n2025.02.04 17:11:13.730878 [ 41134 ] {} <Fatal> BaseDaemon: 6. ? @ 0x00007f07f43e37f3\n2025.02.04 17:11:14.241988 [ 41134 ] {} <Fatal> BaseDaemon: 7. ./build_docker/./src/Common/Exception.cpp:48: DB::abortOnFailedAssertion(String const&, void* const*, unsigned long, unsigned long) @ 0x0000000020c8038b\n2025.02.04 17:11:14.731226 [ 41134 ] {} <Fatal> BaseDaemon: 8.0. inlined from ./build_docker/./src/Common/Exception.cpp:70: DB::handle_error_code(String const&, int, bool, std::vector<void*, std::allocator<void*>> const&)\n2025.02.04 17:11:14.731391 [ 41134 ] {} <Fatal> BaseDaemon: 8. ./build_docker/./src/Common/Exception.cpp:112: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x0000000020c81de1\n2025.02.04 17:11:14.878809 [ 41134 ] {} <Fatal> BaseDaemon: 9. DB::Exception::Exception(PreformattedMessage&&, int) @ 0x0000000008c36f55\n2025.02.04 17:11:15.048801 [ 41134 ] {} <Fatal> BaseDaemon: 10. DB::Exception::Exception<>(int, FormatStringHelperImpl<>) @ 0x0000000008c5cbc4\n2025.02.04 17:11:15.732547 [ 41134 ] {} <Fatal> BaseDaemon: 11. ./build_docker/./src/QueryPipeline/QueryPipelineBuilder.cpp:92: DB::QueryPipelineBuilder::init(DB::Pipe) @ 0x000000003570a8e4\n2025.02.04 17:11:15.854546 [ 41134 ] {} <Fatal> BaseDaemon: 12. ./build_docker/./src/Processors/QueryPlan/ReadFromStreamLikeEngine.cpp:46: DB::ReadFromStreamLikeEngine::initializePipeline(DB::QueryPipelineBuilder&, DB::BuildQueryPipelineSettings const&) @ 0x0000000042a36151\n2025.02.04 17:11:15.982890 [ 41134 ] {} <Fatal> BaseDaemon: 13. ./build_docker/./src/Processors/QueryPlan/ISourceStep.cpp:20: DB::ISourceStep::updatePipeline(std::vector<std::unique_ptr<DB::QueryPipelineBuilder, std::default_delete<DB::QueryPipelineBuilder>>, std::allocator<std::unique_ptr<DB::QueryPipelineBuilder, std::default_delete<DB::QueryPipelineBuilder>>>>, DB::BuildQueryPipelineSettings const&) @ 0x000000004272fe98\n2025.02.04 17:11:16.565890 [ 41134 ] {} <Fatal> BaseDaemon: 14. ./build_docker/./src/Processors/QueryPlan/QueryPlan.cpp:202: DB::QueryPlan::buildQueryPipeline(DB::QueryPlanOptimizationSettings const&, DB::BuildQueryPipelineSettings const&) @ 0x00000000427e086c\n2025.02.04 17:11:18.259423 [ 41134 ] {} <Fatal> BaseDaemon: 15. ./build_docker/./src/Storages/StorageMerge.cpp:1191: DB::ReadFromMerge::buildPipeline(DB::ReadFromMerge::ChildPlan&, DB::QueryProcessingStage::Enum) const @ 0x000000003ec4907f\n2025.02.04 17:11:20.465980 [ 41134 ] {} <Fatal> BaseDaemon: 16. ./build_docker/./src/Storages/StorageMerge.cpp:511: DB::ReadFromMerge::initializePipeline(DB::QueryPipelineBuilder&, DB::BuildQueryPipelineSettings const&) @ 0x000000003ec4500e\n2025.02.04 17:11:20.525751 [ 41134 ] {} <Fatal> BaseDaemon: 17. ./build_docker/./src/Processors/QueryPlan/ISourceStep.cpp:20: DB::ISourceStep::updatePipeline(std::vector<std::unique_ptr<DB::QueryPipelineBuilder, std::default_delete<DB::QueryPipelineBuilder>>, std::allocator<std::unique_ptr<DB::QueryPipelineBuilder, std::default_delete<DB::QueryPipelineBuilder>>>>, DB::BuildQueryPipelineSettings const&) @ 0x000000004272fe98\n2025.02.04 17:11:21.068455 [ 41134 ] {} <Fatal> BaseDaemon: 18. ./build_docker/./src/Processors/QueryPlan/QueryPlan.cpp:202: DB::QueryPlan::buildQueryPipeline(DB::QueryPlanOptimizationSettings const&, DB::BuildQueryPipelineSettings const&) @ 0x00000000427e086c\n2025.02.04 17:11:21.705571 [ 41134 ] {} <Fatal> BaseDaemon: 19. ./build_docker/./src/Interpreters/InterpreterSelectQueryAnalyzer.cpp:274: DB::InterpreterSelectQueryAnalyzer::buildQueryPipeline() @ 0x00000000390e3bc5\n2025.02.04 17:11:21.999749 [ 41134 ] {} <Fatal> BaseDaemon: 20. ./build_docker/./src/Interpreters/InterpreterSelectQueryAnalyzer.cpp:241: DB::InterpreterSelectQueryAnalyzer::execute() @ 0x00000000390e3561\n2025.02.04 17:11:22.794835 [ 41134 ] {} <Fatal> BaseDaemon: 21. ./build_docker/./src/Interpreters/executeQuery.cpp:1455: DB::executeQueryImpl(char const*, char const*, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum, DB::ReadBuffer*, std::shared_ptr<DB::IAST>&) @ 0x0000000039d32e58\n2025.02.04 17:11:23.953403 [ 41134 ] {} <Fatal> BaseDaemon: 22. ./build_docker/./src/Interpreters/executeQuery.cpp:1621: DB::executeQuery(String const&, std::shared_ptr<DB::Context>, DB::QueryFlags, DB::QueryProcessingStage::Enum) @ 0x0000000039d23539\n2025.02.04 17:11:24.818819 [ 41134 ] {} <Fatal> BaseDaemon: 23. ./build_docker/./src/Server/TCPHandler.cpp:662: DB::TCPHandler::runImpl() @ 0x00000000413b1901\n2025.02.04 17:11:25.906200 [ 41134 ] {} <Fatal> BaseDaemon: 24. ./build_docker/./src/Server/TCPHandler.cpp:2624: DB::TCPHandler::run() @ 0x0000000041418ebc\n2025.02.04 17:11:25.930033 [ 41134 ] {} <Fatal> BaseDaemon: 25. ./build_docker/./base/poco/Net/src/TCPServerConnection.cpp:40: Poco::Net::TCPServerConnection::start() @ 0x0000000047ae5300\n2025.02.04 17:11:25.957074 [ 41134 ] {} <Fatal> BaseDaemon: 26. ./build_docker/./base/poco/Net/src/TCPServerDispatcher.cpp:115: Poco::Net::TCPServerDispatcher::run() @ 0x0000000047ae6232\n2025.02.04 17:11:25.984931 [ 41134 ] {} <Fatal> BaseDaemon: 27. ./build_docker/./base/poco/Foundation/src/ThreadPool.cpp:205: Poco::PooledThread::run() @ 0x00000000479c5635\n2025.02.04 17:11:26.015892 [ 41134 ] {} <Fatal> BaseDaemon: 28. ./build_docker/./base/poco/Foundation/src/Thread.cpp:45: Poco::(anonymous namespace)::RunnableHolder::run() @ 0x00000000479c224e\n2025.02.04 17:11:26.095226 [ 41134 ] {} <Fatal> BaseDaemon: 29. ./base/poco/Foundation/src/Thread_POSIX.cpp:335: Poco::ThreadImpl::runnableEntry(void*) @ 0x00000000479bedf1\n2025.02.04 17:11:26.095354 [ 41134 ] {} <Fatal> BaseDaemon: 30. ? @ 0x00007f07f444fac3\n2025.02.04 17:11:26.095407 [ 41134 ] {} <Fatal> BaseDaemon: 31. ? @ 0x00007f07f44e1850\n2025.02.04 17:11:28.967222 [ 41134 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: 9F2D9E93D182CCE6CA78D0320B459545)\n2025.02.04 17:11:40.066640 [ 41134 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues\n2025.02.04 17:11:40.067878 [ 41134 ] {} <Fatal> BaseDaemon: Changed settings: min_compress_block_size = 74655, max_compress_block_size = 2711722, max_block_size = 42211, min_external_table_block_size_bytes = 1, max_joined_block_size_rows = 39713, max_insert_threads = 2, max_threads = 3, max_read_buffer_size = 633459, connect_timeout_with_failover_ms = 2000, connect_timeout_with_failover_secure_ms = 3000, idle_connection_timeout = 36000, s3_max_get_rps = 1000000, s3_max_get_burst = 2000000, s3_max_put_rps = 1000000, s3_max_put_burst = 2000000, s3_check_objects_after_upload = true, use_uncompressed_cache = true, max_remote_read_network_bandwidth = 1000000000000, max_remote_write_network_bandwidth = 1000000000000, max_local_read_bandwidth = 1000000000000, max_local_write_bandwidth = 1000000000000, stream_like_engine_allow_direct_select = true, replication_wait_for_inactive_replica_timeout = 30, min_count_to_compile_expression = 0, compile_sort_description = false, group_by_two_level_threshold = 695409, distributed_aggregation_memory_efficient = false, enable_memory_bound_merging_of_aggregation_results = false, allow_nonconst_timezone_arguments = true, group_by_use_nulls = true, input_format_parallel_parsing = false, min_chunk_bytes_for_parallel_parsing = 8103084, merge_tree_coarse_index_granularity = 13, min_bytes_to_use_direct_io = 10737418240, min_bytes_to_use_mmap_io = 10737418240, log_queries = true, insert_quorum_timeout = 60000, table_function_remote_max_addresses = 200, memory_tracker_fault_probability = 0.0010000000474974513, merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability = 0.5600000023841858, http_response_buffer_size = 2317851, fsync_metadata = false, query_plan_join_swap_table = true, http_send_timeout = 60., http_receive_timeout = 60., use_index_for_in_with_subqueries_max_values = 1000000000, enable_zstd_qat_codec = true, opentelemetry_start_trace_probability = 0.10000000149011612, enable_vertical_final = false, max_rows_to_read = 20000000, max_bytes_to_read = 1000000000000, max_bytes_to_read_leaf = 1000000000000, max_rows_to_group_by = 10000000000, max_bytes_ratio_before_external_group_by = 0., max_rows_to_sort = 10000000000, max_bytes_to_sort = 10000000000, prefer_external_sort_block_bytes = 100000000, max_bytes_ratio_before_external_sort = 0., max_bytes_before_remerge_sort = 824046304, max_result_rows = 1000000000, max_result_bytes = 1000000000, max_execution_time = 60., max_execution_time_leaf = 600., max_execution_speed = 100000000000, max_execution_speed_bytes = 10000000000000, timeout_before_checking_execution_speed = 300., max_estimated_execution_time = 600., max_columns_to_read = 20000, max_temporary_columns = 20000, max_temporary_non_const_columns = 20000, max_rows_in_set = 10000000000, max_bytes_in_set = 10000000000, max_rows_in_join = 10000000000, max_bytes_in_join = 10000000000, join_algorithm = 'parallel_hash', cross_join_min_rows_to_compress = 100000000, cross_join_min_bytes_to_compress = 100000000, max_rows_to_transfer = 1000000000, max_bytes_to_transfer = 1000000000, max_rows_in_distinct = 10000000000, max_bytes_in_distinct = 10000000000, max_memory_usage = 10000000000, max_memory_usage_for_user = 19896744345, max_untracked_memory = 1048576, memory_profiler_step = 1048576, max_network_bandwidth = 100000000000, max_network_bytes = 1000000000000, max_network_bandwidth_for_user = 100000000000, max_network_bandwidth_for_all_users = 100000000000, max_temporary_data_on_disk_size_for_user = 100000000000, max_temporary_data_on_disk_size_for_query = 100000000000, max_backup_bandwidth = 100000000000, log_comment = '01551_context_uaf.sql', send_logs_level = 'warning', prefer_localhost_replica = false, optimize_aggregation_in_order = true, aggregation_in_order_max_block_bytes = 33467556, read_in_order_two_level_merge_threshold = 41, max_hyperscan_regexp_length = 1000000, max_hyperscan_regexp_total_length = 10000000, allow_introspection_functions = true, database_atomic_wait_for_drop_and_detach_synchronously = true, optimize_or_like_chain = true, optimize_functions_to_subcolumns = false, optimize_append_index = true, use_query_cache = true, query_cache_nondeterministic_function_handling = 'ignore', query_cache_system_table_handling = 'ignore', query_cache_max_size_in_bytes = 10000000, query_cache_max_entries = 100000, distributed_ddl_entry_format_version = 6, external_storage_max_read_rows = 10000000000, external_storage_max_read_bytes = 10000000000, local_filesystem_read_method = 'io_uring', merge_tree_min_bytes_per_task_for_remote_reading = 8388608, merge_tree_compact_parts_min_granules_to_multibuffer_read = 85, async_insert_busy_timeout_max_ms = 5000, enable_filesystem_cache = true, enable_filesystem_cache_on_write_operations = true, filesystem_cache_segments_batch_size = 5, use_page_cache_for_disks_without_file_cache = true, page_cache_inject_eviction = true, load_marks_asynchronously = true, allow_prefetched_read_pool_for_remote_filesystem = false, allow_prefetched_read_pool_for_local_filesystem = false, filesystem_prefetch_step_bytes = 104857600, filesystem_prefetch_max_memory_usage = 134217728, filesystem_prefetches_limit = 0, allow_deprecated_database_ordinary = true, max_streams_for_merge_tree_reading = 1000, optimize_sorting_by_input_stream_properties = false, insert_keeper_max_retries = 100, insert_keeper_retry_initial_backoff_ms = 1, insert_keeper_retry_max_backoff_ms = 10, insert_keeper_fault_injection_probability = 0.009999999776482582, ignore_drop_queries_probability = 0.5, parallel_replicas_local_plan = false, session_timezone = 'Africa/Juba'\n2025.02.04 17:11:44.140037 [ 3214 ] {} <Fatal> Application: Child process was terminated by signal 6.\n```\n\n### How to reproduce\n\n_No response_\n\n### Error message and/or stacktrace\n\n_No response_\n",
  "hints_text": "Repro \n```SQL\n\nDROP TABLE IF EXISTS file_log;\nCREATE TABLE file_log  (`key` UInt8, `value` UInt8) ENGINE = FileLog('./user_files/02889_file_log_save_errors_test_11/', 'JSONEachRow') SETTINGS handle_error_mode = 'stream'\n;\n\nSET stream_like_engine_allow_direct_select = 1;\nselect * from merge('', 'f');\n```",
  "created_at": "2025-03-11T16:16:37Z",
  "modified_files": [
    "src/Storages/FileLog/StorageFileLog.cpp"
  ],
  "modified_test_files": [
    "tests/clickhouse-test",
    "b/tests/queries/0_stateless/03381_file_log_merge_empty.sh"
  ]
}