diff --git a/dbms/tests/integration/test_replicated_mutations/configs/merge_tree_max_parts.xml b/dbms/tests/integration/test_replicated_mutations/configs/merge_tree_max_parts.xml
new file mode 100644
index 000000000000..60047dcab2ce
--- /dev/null
+++ b/dbms/tests/integration/test_replicated_mutations/configs/merge_tree_max_parts.xml
@@ -0,0 +1,6 @@
+<yandex>
+    <merge_tree>
+        <parts_to_delay_insert>50</parts_to_delay_insert>
+        <parts_to_throw_insert>50</parts_to_throw_insert>
+    </merge_tree>
+</yandex>
\ No newline at end of file
diff --git a/dbms/tests/integration/test_replicated_mutations/test.py b/dbms/tests/integration/test_replicated_mutations/test.py
index 351ceff36083..0347ba4782c5 100644
--- a/dbms/tests/integration/test_replicated_mutations/test.py
+++ b/dbms/tests/integration/test_replicated_mutations/test.py
@@ -10,21 +10,29 @@
 
 cluster = ClickHouseCluster(__file__)
 
-node1 = cluster.add_instance('node1', with_zookeeper=True)
+node1 = cluster.add_instance('node1', macros={'cluster': 'test1'}, with_zookeeper=True)
 # Check, that limits on max part size for merges doesn`t affect mutations
-node2 = cluster.add_instance('node2', main_configs=["configs/merge_tree.xml"], with_zookeeper=True)
-nodes = [node1, node2]
+node2 = cluster.add_instance('node2', macros={'cluster': 'test1'}, main_configs=["configs/merge_tree.xml"], with_zookeeper=True)
+
+node3 = cluster.add_instance('node3', macros={'cluster': 'test2'}, main_configs=["configs/merge_tree_max_parts.xml"], with_zookeeper=True)
+node4 = cluster.add_instance('node4', macros={'cluster': 'test2'}, main_configs=["configs/merge_tree_max_parts.xml"], with_zookeeper=True)
+
+node5 = cluster.add_instance('node5', macros={'cluster': 'test3'}, main_configs=["configs/merge_tree_max_parts.xml"])
+
+all_nodes = [node1, node2, node3, node4, node5]
 
 @pytest.fixture(scope="module")
 def started_cluster():
     try:
         cluster.start()
 
-        for node in nodes:
+        for node in all_nodes:
             node.query("DROP TABLE IF EXISTS test_mutations")
 
-        for node in nodes:
-            node.query("CREATE TABLE test_mutations(d Date, x UInt32, i UInt32) ENGINE ReplicatedMergeTree('/clickhouse/tables/test/test_mutations', '{instance}') ORDER BY x PARTITION BY toYYYYMM(d)")
+        for node in [node1, node2, node3, node4]:
+            node.query("CREATE TABLE test_mutations(d Date, x UInt32, i UInt32) ENGINE ReplicatedMergeTree('/clickhouse/{cluster}/tables/test/test_mutations', '{instance}') ORDER BY x PARTITION BY toYYYYMM(d)")
+
+        node5.query("CREATE TABLE test_mutations(d Date, x UInt32, i UInt32) ENGINE MergeTree() ORDER BY x PARTITION BY toYYYYMM(d)")
 
         yield cluster
 
@@ -33,7 +41,8 @@ def started_cluster():
 
 
 class Runner:
-    def __init__(self):
+    def __init__(self, nodes):
+        self.nodes = nodes
         self.mtx = threading.Lock()
         self.total_inserted_xs = 0
         self.total_inserted_rows = 0
@@ -49,7 +58,9 @@ def __init__(self):
 
         self.stop_ev = threading.Event()
 
-    def do_insert(self, thread_num):
+        self.exceptions = []
+
+    def do_insert(self, thread_num, partitions_num):
         self.stop_ev.wait(random.random())
 
         # Each thread inserts a small random number of rows with random year, month 01 and day determined
@@ -67,7 +78,7 @@ def do_insert(self, thread_num):
                 for x in xs:
                     self.currently_inserting_xs[x] += 1
 
-            year = 2000 + random.randint(0, 10)
+            year = 2000 + random.randint(0, partitions_num)
             date_str = '{year}-{month}-{day}'.format(year=year, month=month, day=day)
             payload = ''
             for x in xs:
@@ -76,7 +87,7 @@ def do_insert(self, thread_num):
 
             try:
                 print 'thread {}: insert for {}: {}'.format(thread_num, date_str, ','.join(str(x) for x in xs))
-                random.choice(nodes).query("INSERT INTO test_mutations FORMAT TSV", payload)
+                random.choice(self.nodes).query("INSERT INTO test_mutations FORMAT TSV", payload)
 
                 with self.mtx:
                     for x in xs:
@@ -86,6 +97,7 @@ def do_insert(self, thread_num):
 
             except Exception, e:
                 print 'Exception while inserting,', e
+                self.exceptions.append(e)
             finally:
                 with self.mtx:
                     for x in xs:
@@ -113,7 +125,7 @@ def do_delete(self, thread_num):
 
             try:
                 print 'thread {}: delete {} * {}'.format(thread_num, to_delete_count, x)
-                random.choice(nodes).query("ALTER TABLE test_mutations DELETE WHERE x = {}".format(x))
+                random.choice(self.nodes).query("ALTER TABLE test_mutations DELETE WHERE x = {}".format(x))
 
                 with self.mtx:
                     self.total_mutations += 1
@@ -130,14 +142,27 @@ def do_delete(self, thread_num):
             self.stop_ev.wait(1.0 + random.random() * 2)
 
 
+def wait_for_mutations(nodes, number_of_mutations):
+    for i in range(100):  # wait for replication 80 seconds max
+        time.sleep(0.8)
+
+        def get_done_mutations(node):
+            return int(node.query("SELECT sum(is_done) FROM system.mutations WHERE table = 'test_mutations'").rstrip())
+
+        if all([get_done_mutations(n) == number_of_mutations for n in nodes]):
+            return True
+    return False
+
+
 def test_mutations(started_cluster):
     DURATION_SECONDS = 30
+    nodes = [node1, node2]
 
-    runner = Runner()
+    runner = Runner(nodes)
 
     threads = []
     for thread_num in range(5):
-        threads.append(threading.Thread(target=runner.do_insert, args=(thread_num, )))
+        threads.append(threading.Thread(target=runner.do_insert, args=(thread_num, 10)))
 
     for thread_num in (11, 12, 13):
         threads.append(threading.Thread(target=runner.do_delete, args=(thread_num,)))
@@ -155,18 +180,11 @@ def test_mutations(started_cluster):
     assert runner.total_inserted_rows > 0
     assert runner.total_mutations > 0
 
-    all_done = False
-    for i in range(100): # wait for replication 80 seconds max
-        time.sleep(0.8)
-
-        def get_done_mutations(node):
-            return int(node.query("SELECT sum(is_done) FROM system.mutations WHERE table = 'test_mutations'").rstrip())
+    all_done = wait_for_mutations(nodes, runner.total_mutations)
 
-        if all([get_done_mutations(n) == runner.total_mutations for n in nodes]):
-            all_done = True
-            break
-
-    print node1.query("SELECT mutation_id, command, parts_to_do, is_done FROM system.mutations WHERE table = 'test_mutations' FORMAT TSVWithNames")
+    print "Total mutations: ", runner.total_mutations
+    for node in nodes:
+        print node.query("SELECT mutation_id, command, parts_to_do, is_done FROM system.mutations WHERE table = 'test_mutations' FORMAT TSVWithNames")
     assert all_done
 
     expected_sum = runner.total_inserted_xs - runner.total_deleted_xs
@@ -174,3 +192,44 @@ def get_done_mutations(node):
     for i, node in enumerate(nodes):
         actual_sums.append(int(node.query("SELECT sum(x) FROM test_mutations").rstrip()))
         assert actual_sums[i] == expected_sum
+
+
+@pytest.mark.parametrize(
+    ('nodes', ),
+    [
+        ([node5, ], ),          # MergeTree
+        ([node3, node4], ),     # ReplicatedMergeTree
+    ]
+)
+def test_mutations_dont_prevent_merges(started_cluster, nodes):
+    for year in range(2000, 2016):
+        rows = ''
+        date_str = '{}-01-{}'.format(year, random.randint(1, 10))
+        for i in range(10):
+            rows += '{}	{}	{}
'.format(date_str, random.randint(1, 10), i)
+        nodes[0].query("INSERT INTO test_mutations FORMAT TSV", rows)
+
+    # will run mutations of 16 parts in parallel, mutations will sleep for about 20 seconds
+    nodes[0].query("ALTER TABLE test_mutations UPDATE i = sleepEachRow(2) WHERE 1")
+
+    runner = Runner(nodes)
+    threads = []
+    for thread_num in range(2):
+        threads.append(threading.Thread(target=runner.do_insert, args=(thread_num, 0)))
+
+    # will insert approx 8-10 new parts per 1 second into one partition
+    for t in threads:
+        t.start()
+
+    all_done = wait_for_mutations(nodes, 1)
+
+    runner.stop_ev.set()
+    for t in threads:
+        t.join()
+
+    for node in nodes:
+        print node.query("SELECT mutation_id, command, parts_to_do, is_done FROM system.mutations WHERE table = 'test_mutations' FORMAT TSVWithNames")
+        print node.query("SELECT partition, count(name), sum(active), sum(active*rows) FROM system.parts WHERE table ='test_mutations' GROUP BY partition FORMAT TSVWithNames")
+
+    assert all_done
+    assert all([str(e).find("Too many parts") < 0 for e in runner.exceptions])
