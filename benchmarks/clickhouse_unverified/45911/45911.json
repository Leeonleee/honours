{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 45911,
  "instance_id": "ClickHouse__ClickHouse-45911",
  "issue_numbers": [
    "34342"
  ],
  "base_commit": "4217e65c5a7aafd787f2ba2c9148dbf1fb8788c8",
  "patch": "diff --git a/src/Storages/MergeTree/AlterConversions.cpp b/src/Storages/MergeTree/AlterConversions.cpp\nnew file mode 100644\nindex 000000000000..7a298b0f6ca0\n--- /dev/null\n+++ b/src/Storages/MergeTree/AlterConversions.cpp\n@@ -0,0 +1,55 @@\n+#include <Storages/MergeTree/AlterConversions.h>\n+#include <Common/Exception.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+\n+bool AlterConversions::columnHasNewName(const std::string & old_name) const\n+{\n+    for (const auto & [new_name, prev_name] : rename_map)\n+    {\n+        if (old_name == prev_name)\n+            return true;\n+    }\n+\n+    return false;\n+}\n+\n+std::string AlterConversions::getColumnNewName(const std::string & old_name) const\n+{\n+    for (const auto & [new_name, prev_name] : rename_map)\n+    {\n+        if (old_name == prev_name)\n+            return new_name;\n+    }\n+\n+    throw Exception(ErrorCodes::LOGICAL_ERROR, \"Column {} was not renamed\", old_name);\n+}\n+\n+\n+bool AlterConversions::isColumnRenamed(const std::string & new_name) const\n+{\n+    for (const auto & [name_to, name_from] : rename_map)\n+    {\n+        if (name_to == new_name)\n+            return true;\n+    }\n+    return false;\n+}\n+/// Get column old name before rename (lookup by key in rename_map)\n+std::string AlterConversions::getColumnOldName(const std::string & new_name) const\n+{\n+    for (const auto & [name_to, name_from] : rename_map)\n+    {\n+        if (name_to == new_name)\n+            return name_from;\n+    }\n+    throw Exception(ErrorCodes::LOGICAL_ERROR, \"Column {} was not renamed\", new_name);\n+}\n+\n+}\ndiff --git a/src/Storages/MergeTree/AlterConversions.h b/src/Storages/MergeTree/AlterConversions.h\nindex 0d58499d424b..ada385d61008 100644\n--- a/src/Storages/MergeTree/AlterConversions.h\n+++ b/src/Storages/MergeTree/AlterConversions.h\n@@ -14,11 +14,22 @@ namespace DB\n /// part->getColumns() and storage->getColumns().\n struct AlterConversions\n {\n+    struct RenamePair\n+    {\n+        std::string rename_to;\n+        std::string rename_from;\n+    };\n     /// Rename map new_name -> old_name\n-    std::unordered_map<std::string, std::string> rename_map;\n+    std::vector<RenamePair> rename_map;\n \n-    bool isColumnRenamed(const std::string & new_name) const { return rename_map.count(new_name) > 0; }\n-    std::string getColumnOldName(const std::string & new_name) const { return rename_map.at(new_name); }\n+    /// Column was renamed (lookup by value in rename_map)\n+    bool columnHasNewName(const std::string & old_name) const;\n+    /// Get new name for column (lookup by value in rename_map)\n+    std::string getColumnNewName(const std::string & old_name) const;\n+    /// Is this name is new name of column (lookup by key in rename_map)\n+    bool isColumnRenamed(const std::string & new_name) const;\n+    /// Get column old name before rename (lookup by key in rename_map)\n+    std::string getColumnOldName(const std::string & new_name) const;\n };\n \n }\ndiff --git a/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp b/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp\nindex 175df9b6e28d..7a9f927ae0fc 100644\n--- a/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp\n+++ b/src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp\n@@ -683,6 +683,7 @@ void DataPartStorageOnDiskBase::clearDirectory(\n         request.emplace_back(fs::path(dir) / \"default_compression_codec.txt\", true);\n         request.emplace_back(fs::path(dir) / \"delete-on-destroy.txt\", true);\n         request.emplace_back(fs::path(dir) / \"txn_version.txt\", true);\n+        request.emplace_back(fs::path(dir) / \"metadata_version.txt\", true);\n \n         disk->removeSharedFiles(request, !can_remove_shared_data, names_not_to_remove);\n         disk->removeDirectory(dir);\ndiff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\nindex cbbdb9119740..35c627c32102 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\n@@ -63,8 +63,9 @@ constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_DEFAULT_COMPRESSION = 4;\n constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_UUID = 5;\n constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_ZERO_COPY = 6;\n constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PROJECTION = 7;\n+constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_METADATA_VERSION = 8;\n // Reserved for ALTER PRIMARY KEY\n-// constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PRIMARY_KEY = 8;\n+// constexpr auto REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PRIMARY_KEY = 9;\n \n std::string getEndpointId(const std::string & node_id)\n {\n@@ -120,7 +121,7 @@ void Service::processQuery(const HTMLForm & params, ReadBuffer & /*body*/, Write\n     MergeTreePartInfo::fromPartName(part_name, data.format_version);\n \n     /// We pretend to work as older server version, to be sure that client will correctly process our version\n-    response.addCookie({\"server_protocol_version\", toString(std::min(client_protocol_version, REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PROJECTION))});\n+    response.addCookie({\"server_protocol_version\", toString(std::min(client_protocol_version, REPLICATION_PROTOCOL_VERSION_WITH_METADATA_VERSION))});\n \n     LOG_TRACE(log, \"Sending part {}\", part_name);\n \n@@ -280,6 +281,10 @@ MergeTreeData::DataPart::Checksums Service::sendPartFromDisk(\n             && name == IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME)\n             continue;\n \n+        if (client_protocol_version < REPLICATION_PROTOCOL_VERSION_WITH_METADATA_VERSION\n+            && name == IMergeTreeDataPart::METADATA_VERSION_FILE_NAME)\n+            continue;\n+\n         files_to_replicate.insert(name);\n     }\n \n@@ -407,7 +412,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchSelectedPart(\n     {\n         {\"endpoint\",                getEndpointId(replica_path)},\n         {\"part\",                    part_name},\n-        {\"client_protocol_version\", toString(REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PROJECTION)},\n+        {\"client_protocol_version\", toString(REPLICATION_PROTOCOL_VERSION_WITH_METADATA_VERSION)},\n         {\"compress\",                \"false\"}\n     });\n \n@@ -692,7 +697,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToMemory(\n     auto block = block_in.read();\n     throttler->add(block.bytes());\n \n-    new_data_part->setColumns(block.getNamesAndTypesList(), {});\n+    new_data_part->setColumns(block.getNamesAndTypesList(), {}, metadata_snapshot->getMetadataVersion());\n \n     if (!is_projection)\n     {\n@@ -768,7 +773,8 @@ void Fetcher::downloadBaseOrProjectionPartToDisk(\n \n         if (file_name != \"checksums.txt\" &&\n             file_name != \"columns.txt\" &&\n-            file_name != IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME)\n+            file_name != IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME &&\n+            file_name != IMergeTreeDataPart::METADATA_VERSION_FILE_NAME)\n             checksums.addFile(file_name, file_size, expected_hash);\n     }\n \ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex 5de13020a1db..94aa2a72949e 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -416,10 +416,11 @@ std::pair<time_t, time_t> IMergeTreeDataPart::getMinMaxTime() const\n }\n \n \n-void IMergeTreeDataPart::setColumns(const NamesAndTypesList & new_columns, const SerializationInfoByName & new_infos)\n+void IMergeTreeDataPart::setColumns(const NamesAndTypesList & new_columns, const SerializationInfoByName & new_infos, int32_t metadata_version_)\n {\n     columns = new_columns;\n     serialization_infos = new_infos;\n+    metadata_version = metadata_version_;\n \n     column_name_to_position.clear();\n     column_name_to_position.reserve(new_columns.size());\n@@ -660,6 +661,7 @@ void IMergeTreeDataPart::appendFilesOfColumnsChecksumsIndexes(Strings & files, b\n         appendFilesOfPartitionAndMinMaxIndex(files);\n         appendFilesOfTTLInfos(files);\n         appendFilesOfDefaultCompressionCodec(files);\n+        appendFilesOfMetadataVersion(files);\n     }\n \n     if (!parent_part && include_projection)\n@@ -798,6 +800,9 @@ NameSet IMergeTreeDataPart::getFileNamesWithoutChecksums() const\n     if (getDataPartStorage().exists(TXN_VERSION_METADATA_FILE_NAME))\n         result.emplace(TXN_VERSION_METADATA_FILE_NAME);\n \n+    if (getDataPartStorage().exists(METADATA_VERSION_FILE_NAME))\n+        result.emplace(METADATA_VERSION_FILE_NAME);\n+\n     return result;\n }\n \n@@ -971,11 +976,22 @@ void IMergeTreeDataPart::removeVersionMetadata()\n     getDataPartStorage().removeFileIfExists(\"txn_version.txt\");\n }\n \n+\n+void IMergeTreeDataPart::removeMetadataVersion()\n+{\n+    getDataPartStorage().removeFileIfExists(METADATA_VERSION_FILE_NAME);\n+}\n+\n void IMergeTreeDataPart::appendFilesOfDefaultCompressionCodec(Strings & files)\n {\n     files.push_back(DEFAULT_COMPRESSION_CODEC_FILE_NAME);\n }\n \n+void IMergeTreeDataPart::appendFilesOfMetadataVersion(Strings & files)\n+{\n+    files.push_back(METADATA_VERSION_FILE_NAME);\n+}\n+\n CompressionCodecPtr IMergeTreeDataPart::detectDefaultCompressionCodec() const\n {\n     /// In memory parts doesn't have any compression\n@@ -1288,8 +1304,7 @@ void IMergeTreeDataPart::loadColumns(bool require)\n         metadata_snapshot = metadata_snapshot->projections.get(name).metadata;\n     NamesAndTypesList loaded_columns;\n \n-    bool exists = metadata_manager->exists(\"columns.txt\");\n-    if (!exists)\n+    if (!metadata_manager->exists(\"columns.txt\"))\n     {\n         /// We can get list of columns only from columns.txt in compact parts.\n         if (require || part_type == Type::Compact)\n@@ -1322,16 +1337,32 @@ void IMergeTreeDataPart::loadColumns(bool require)\n     };\n \n     SerializationInfoByName infos(loaded_columns, settings);\n-    exists =  metadata_manager->exists(SERIALIZATION_FILE_NAME);\n-    if (exists)\n+    if (metadata_manager->exists(SERIALIZATION_FILE_NAME))\n     {\n         auto in = metadata_manager->read(SERIALIZATION_FILE_NAME);\n         infos.readJSON(*in);\n     }\n \n-    setColumns(loaded_columns, infos);\n+    int32_t loaded_metadata_version;\n+    if (metadata_manager->exists(METADATA_VERSION_FILE_NAME))\n+    {\n+        auto in = metadata_manager->read(METADATA_VERSION_FILE_NAME);\n+        readIntText(loaded_metadata_version, *in);\n+    }\n+    else\n+    {\n+        loaded_metadata_version = metadata_snapshot->getMetadataVersion();\n+\n+        writeMetadata(METADATA_VERSION_FILE_NAME, {}, [loaded_metadata_version](auto & buffer)\n+        {\n+            writeIntText(loaded_metadata_version, buffer);\n+        });\n+    }\n+\n+    setColumns(loaded_columns, infos, loaded_metadata_version);\n }\n \n+\n /// Project part / part with project parts / compact part doesn't support LWD.\n bool IMergeTreeDataPart::supportLightweightDeleteMutate() const\n {\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex ea1fd209a20b..4326aa630f9e 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -137,7 +137,11 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     String getTypeName() const { return getType().toString(); }\n \n-    void setColumns(const NamesAndTypesList & new_columns, const SerializationInfoByName & new_infos);\n+    /// We could have separate method like setMetadata, but it's much more convenient to set it up with columns\n+    void setColumns(const NamesAndTypesList & new_columns, const SerializationInfoByName & new_infos, int32_t metadata_version_);\n+\n+    /// Version of metadata for part (columns, pk and so on)\n+    int32_t getMetadataVersion() const { return metadata_version; }\n \n     const NamesAndTypesList & getColumns() const { return columns; }\n     const ColumnsDescription & getColumnsDescription() const { return columns_description; }\n@@ -308,6 +312,9 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     mutable VersionMetadata version;\n \n+    /// Version of part metadata (columns, pk and so on). Managed properly only for replicated merge tree.\n+    int32_t metadata_version;\n+\n     /// For data in RAM ('index')\n     UInt64 getIndexSizeInBytes() const;\n     UInt64 getIndexSizeInAllocatedBytes() const;\n@@ -379,8 +386,12 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n     /// (number of rows, number of rows with default values, etc).\n     static inline constexpr auto SERIALIZATION_FILE_NAME = \"serialization.json\";\n \n+    /// Version used for transactions.\n     static inline constexpr auto TXN_VERSION_METADATA_FILE_NAME = \"txn_version.txt\";\n \n+\n+    static inline constexpr auto METADATA_VERSION_FILE_NAME = \"metadata_version.txt\";\n+\n     /// One of part files which is used to check how many references (I'd like\n     /// to say hardlinks, but it will confuse even more) we have for the part\n     /// for zero copy replication. Sadly it's very complex.\n@@ -443,7 +454,11 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     void writeDeleteOnDestroyMarker();\n     void removeDeleteOnDestroyMarker();\n+    /// It may look like a stupid joke. but these two methods are absolutely unrelated.\n+    /// This one is about removing file with metadata about part version (for transactions)\n     void removeVersionMetadata();\n+    /// This one is about removing file with version of part's metadata (columns, pk and so on)\n+    void removeMetadataVersion();\n \n     mutable std::atomic<DataPartRemovalState> removal_state = DataPartRemovalState::NOT_ATTEMPTED;\n \n@@ -582,6 +597,8 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     static void appendFilesOfDefaultCompressionCodec(Strings & files);\n \n+    static void appendFilesOfMetadataVersion(Strings & files);\n+\n     /// Found column without specific compression and return codec\n     /// for this column with default parameters.\n     CompressionCodecPtr detectDefaultCompressionCodec() const;\ndiff --git a/src/Storages/MergeTree/MergeTask.cpp b/src/Storages/MergeTree/MergeTask.cpp\nindex bd57e1572fcf..5e1098ea3939 100644\n--- a/src/Storages/MergeTree/MergeTask.cpp\n+++ b/src/Storages/MergeTree/MergeTask.cpp\n@@ -205,7 +205,7 @@ bool MergeTask::ExecuteAndFinalizeHorizontalPart::prepare()\n         infos.add(part->getSerializationInfos());\n     }\n \n-    global_ctx->new_data_part->setColumns(global_ctx->storage_columns, infos);\n+    global_ctx->new_data_part->setColumns(global_ctx->storage_columns, infos, global_ctx->metadata_snapshot->getMetadataVersion());\n \n     const auto & local_part_min_ttl = global_ctx->new_data_part->ttl_infos.part_min_ttl;\n     if (local_part_min_ttl && local_part_min_ttl <= global_ctx->time_of_merge)\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex c3c4cd3082de..4be995a3d21f 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -4386,6 +4386,11 @@ MergeTreeData::DataPartPtr MergeTreeData::getPartIfExistsUnlocked(const MergeTre\n \n static void loadPartAndFixMetadataImpl(MergeTreeData::MutableDataPartPtr part)\n {\n+    /// Remove metadata version file and take it from table.\n+    /// Currently we cannot attach parts with different schema, so\n+    /// we can assume that it's equal to table's current schema.\n+    part->removeMetadataVersion();\n+\n     part->loadColumnsChecksumsIndexes(false, true);\n     part->modification_time = part->getDataPartStorage().getLastModified().epochTime();\n     part->removeDeleteOnDestroyMarker();\n@@ -7592,15 +7597,23 @@ bool MergeTreeData::canUsePolymorphicParts(const MergeTreeSettings & settings, S\n \n AlterConversions MergeTreeData::getAlterConversionsForPart(const MergeTreeDataPartPtr part) const\n {\n-    MutationCommands commands = getFirstAlterMutationCommandsForPart(part);\n+    std::map<int64_t, MutationCommands> commands_map = getAlterMutationCommandsForPart(part);\n \n     AlterConversions result{};\n-    for (const auto & command : commands)\n-        /// Currently we need explicit conversions only for RENAME alter\n-        /// all other conversions can be deduced from diff between part columns\n-        /// and columns in storage.\n-        if (command.type == MutationCommand::Type::RENAME_COLUMN)\n-            result.rename_map[command.rename_to] = command.column_name;\n+    auto & rename_map = result.rename_map;\n+    for (const auto & [version, commands] : commands_map)\n+    {\n+        for (const auto & command : commands)\n+        {\n+            /// Currently we need explicit conversions only for RENAME alter\n+            /// all other conversions can be deduced from diff between part columns\n+            /// and columns in storage.\n+            if (command.type == MutationCommand::Type::RENAME_COLUMN)\n+            {\n+                rename_map.emplace_back(AlterConversions::RenamePair{command.rename_to, command.column_name});\n+            }\n+        }\n+    }\n \n     return result;\n }\n@@ -7987,7 +8000,7 @@ MergeTreeData::MutableDataPartPtr MergeTreeData::createEmptyPart(\n     if (settings->assign_part_uuids)\n         new_data_part->uuid = UUIDHelpers::generateV4();\n \n-    new_data_part->setColumns(columns, {});\n+    new_data_part->setColumns(columns, {}, metadata_snapshot->getMetadataVersion());\n     new_data_part->rows_count = block.rows();\n \n     new_data_part->partition = partition;\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 34bc3d24d663..ad97aede0a9a 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -1309,7 +1309,7 @@ class MergeTreeData : public IStorage, public WithMutableContext\n     /// Used to receive AlterConversions for part and apply them on fly. This\n     /// method has different implementations for replicated and non replicated\n     /// MergeTree because they store mutations in different way.\n-    virtual MutationCommands getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const = 0;\n+    virtual std::map<int64_t, MutationCommands> getAlterMutationCommandsForPart(const DataPartPtr & part) const = 0;\n     /// Moves part to specified space, used in ALTER ... MOVE ... queries\n     bool movePartsToSpace(const DataPartsVector & parts, SpacePtr space);\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\nindex 20049976acfc..5b1054d0a0e4 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\n@@ -73,7 +73,7 @@ MutableDataPartStoragePtr MergeTreeDataPartInMemory::flushToDisk(const String &\n     new_data_part_storage->beginTransaction();\n \n     new_data_part->uuid = uuid;\n-    new_data_part->setColumns(columns, {});\n+    new_data_part->setColumns(columns, {}, metadata_snapshot->getMetadataVersion());\n     new_data_part->partition.value = partition.value;\n     new_data_part->minmax_idx = minmax_idx;\n \n@@ -104,7 +104,7 @@ MutableDataPartStoragePtr MergeTreeDataPartInMemory::flushToDisk(const String &\n                 .build();\n \n             new_projection_part->is_temp = false; // clean up will be done on parent part\n-            new_projection_part->setColumns(projection->getColumns(), {});\n+            new_projection_part->setColumns(projection->getColumns(), {}, metadata_snapshot->getMetadataVersion());\n \n             auto new_projection_part_storage = new_projection_part->getDataPartStoragePtr();\n             if (new_projection_part_storage->exists())\ndiff --git a/src/Storages/MergeTree/MergeTreeDataWriter.cpp b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\nindex 93b0abeca35c..ba344dc70aab 100644\n--- a/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataWriter.cpp\n@@ -465,7 +465,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeTempPartImpl(\n     SerializationInfoByName infos(columns, settings);\n     infos.add(block);\n \n-    new_data_part->setColumns(columns, infos);\n+    new_data_part->setColumns(columns, infos, metadata_snapshot->getMetadataVersion());\n     new_data_part->rows_count = block.rows();\n     new_data_part->partition = std::move(partition);\n     new_data_part->minmax_idx = std::move(minmax_idx);\n@@ -587,7 +587,7 @@ MergeTreeDataWriter::TemporaryPart MergeTreeDataWriter::writeProjectionPartImpl(\n     SerializationInfoByName infos(columns, settings);\n     infos.add(block);\n \n-    new_data_part->setColumns(columns, infos);\n+    new_data_part->setColumns(columns, infos, metadata_snapshot->getMetadataVersion());\n \n     if (new_data_part->isStoredOnDisk())\n     {\ndiff --git a/src/Storages/MergeTree/MergeTreeMarksLoader.cpp b/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\nindex 3fc7ff54c356..1d85ac1bd345 100644\n--- a/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\n+++ b/src/Storages/MergeTree/MergeTreeMarksLoader.cpp\n@@ -102,6 +102,15 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n \n     auto res = std::make_shared<MarksInCompressedFile>(marks_count * columns_in_mark);\n \n+    if (file_size == 0 && marks_count != 0)\n+    {\n+        throw Exception(\n+            ErrorCodes::CORRUPTED_DATA,\n+            \"Empty marks file '{}': {}, must be: {}\",\n+            std::string(fs::path(data_part_storage->getFullPath()) / mrk_path),\n+            file_size, expected_uncompressed_size);\n+    }\n+\n     if (!index_granularity_info.mark_type.compressed && expected_uncompressed_size != file_size)\n         throw Exception(\n             ErrorCodes::CORRUPTED_DATA,\n@@ -138,7 +147,12 @@ MarkCache::MappedPtr MergeTreeMarksLoader::loadMarksImpl()\n         }\n \n         if (i * mark_size != expected_uncompressed_size)\n-            throw Exception(ErrorCodes::CANNOT_READ_ALL_DATA, \"Cannot read all marks from file {}\", mrk_path);\n+        {\n+            throw Exception(\n+                ErrorCodes::CANNOT_READ_ALL_DATA,\n+                \"Cannot read all marks from file {}, marks expected {} (bytes size {}), marks read {} (bytes size {})\",\n+                mrk_path, marks_count, expected_uncompressed_size, i, reader->count());\n+        }\n     }\n \n     res->protect();\ndiff --git a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\nindex fabf2acdad3c..87d9bb0f1680 100644\n--- a/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n+++ b/src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp\n@@ -229,7 +229,7 @@ MergeTreeData::MutableDataPartsVector MergeTreeWriteAheadLog::restore(\n \n             part->minmax_idx->update(block, storage.getMinMaxColumnsNames(metadata_snapshot->getPartitionKey()));\n             part->partition.create(metadata_snapshot, block, 0, context);\n-            part->setColumns(block.getNamesAndTypesList(), {});\n+            part->setColumns(block.getNamesAndTypesList(), {}, metadata_snapshot->getMetadataVersion());\n             if (metadata_snapshot->hasSortingKey())\n                 metadata_snapshot->getSortingKey().expression->execute(block);\n \ndiff --git a/src/Storages/MergeTree/MergedBlockOutputStream.cpp b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\nindex ced43ae25b01..53d4a32fc0eb 100644\n--- a/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedBlockOutputStream.cpp\n@@ -175,7 +175,7 @@ MergedBlockOutputStream::Finalizer MergedBlockOutputStream::finalizePartAsync(\n         serialization_infos.replaceData(new_serialization_infos);\n         files_to_remove_after_sync = removeEmptyColumnsFromPart(new_part, part_columns, serialization_infos, checksums);\n \n-        new_part->setColumns(part_columns, serialization_infos);\n+        new_part->setColumns(part_columns, serialization_infos, metadata_snapshot->getMetadataVersion());\n     }\n \n     auto finalizer = std::make_unique<Finalizer::Impl>(*writer, new_part, files_to_remove_after_sync, sync);\n@@ -289,6 +289,14 @@ MergedBlockOutputStream::WrittenFiles MergedBlockOutputStream::finalizePartOnDis\n         written_files.emplace_back(std::move(out));\n     }\n \n+    {\n+        /// Write a file with a description of columns.\n+        auto out = new_part->getDataPartStorage().writeFile(IMergeTreeDataPart::METADATA_VERSION_FILE_NAME, 4096, write_settings);\n+        DB::writeIntText(new_part->getMetadataVersion(), *out);\n+        out->preFinalize();\n+        written_files.emplace_back(std::move(out));\n+    }\n+\n     if (default_codec != nullptr)\n     {\n         auto out = new_part->getDataPartStorage().writeFile(IMergeTreeDataPart::DEFAULT_COMPRESSION_CODEC_FILE_NAME, 4096, write_settings);\ndiff --git a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\nindex 03829f1daf90..c70c5187b8bc 100644\n--- a/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n+++ b/src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp\n@@ -85,7 +85,7 @@ MergedColumnOnlyOutputStream::fillChecksums(\n             all_checksums.files.erase(removed_file);\n     }\n \n-    new_part->setColumns(columns, serialization_infos);\n+    new_part->setColumns(columns, serialization_infos, metadata_snapshot->getMetadataVersion());\n \n     return checksums;\n }\ndiff --git a/src/Storages/MergeTree/MutateTask.cpp b/src/Storages/MergeTree/MutateTask.cpp\nindex 4a7224b0722b..01f7b47a170b 100644\n--- a/src/Storages/MergeTree/MutateTask.cpp\n+++ b/src/Storages/MergeTree/MutateTask.cpp\n@@ -52,7 +52,7 @@ static bool checkOperationIsNotCanceled(ActionBlocker & merges_blocker, MergeLis\n *   First part should be executed by mutations interpreter.\n *   Other is just simple drop/renames, so they can be executed without interpreter.\n */\n-static void splitMutationCommands(\n+static void splitAndModifyMutationCommands(\n     MergeTreeData::DataPartPtr part,\n     const MutationCommands & commands,\n     MutationCommands & for_interpreter,\n@@ -97,25 +97,48 @@ static void splitMutationCommands(\n                     else\n                         mutated_columns.emplace(command.column_name);\n                 }\n+            }\n+        }\n+\n+        auto alter_conversions = part->storage.getAlterConversionsForPart(part);\n \n-                if (command.type == MutationCommand::Type::RENAME_COLUMN)\n+        /// We don't add renames from commands, instead we take them from rename_map.\n+        /// It's important because required renames depend not only on part's data version (i.e. mutation version)\n+        /// but also on part's metadata version. Why we have such logic only for renames? Because all other types of alter\n+        /// can be deduced based on difference between part's schema and table schema.\n+        for (const auto & [rename_to, rename_from] : alter_conversions.rename_map)\n+        {\n+            if (part_columns.has(rename_from))\n+            {\n+                /// Actual rename\n+                for_interpreter.push_back(\n                 {\n-                    for_interpreter.push_back(\n-                    {\n-                        .type = MutationCommand::Type::READ_COLUMN,\n-                        .column_name = command.rename_to,\n-                    });\n-                    part_columns.rename(command.column_name, command.rename_to);\n-                }\n+                    .type = MutationCommand::Type::READ_COLUMN,\n+                    .column_name = rename_to,\n+                });\n+\n+                /// Not needed for compact parts (not executed), added here only to produce correct\n+                /// set of columns for new part and their serializations\n+                for_file_renames.push_back(\n+                {\n+                     .type = MutationCommand::Type::RENAME_COLUMN,\n+                     .column_name = rename_from,\n+                     .rename_to = rename_to\n+                });\n+\n+                part_columns.rename(rename_from, rename_to);\n             }\n         }\n+\n         /// If it's compact part, then we don't need to actually remove files\n         /// from disk we just don't read dropped columns\n-        for (const auto & column : part->getColumns())\n+        for (const auto & column : part_columns)\n         {\n             if (!mutated_columns.contains(column.name))\n+            {\n                 for_interpreter.emplace_back(\n                     MutationCommand{.type = MutationCommand::Type::READ_COLUMN, .column_name = column.name, .data_type = column.type});\n+            }\n         }\n     }\n     else\n@@ -141,13 +164,57 @@ static void splitMutationCommands(\n             {\n                 if (command.type == MutationCommand::Type::READ_COLUMN)\n                     for_interpreter.push_back(command);\n-                else if (command.type == MutationCommand::Type::RENAME_COLUMN)\n-                    part_columns.rename(command.column_name, command.rename_to);\n \n                 for_file_renames.push_back(command);\n             }\n         }\n+\n+        auto alter_conversions = part->storage.getAlterConversionsForPart(part);\n+        /// We don't add renames from commands, instead we take them from rename_map.\n+        /// It's important because required renames depend not only on part's data version (i.e. mutation version)\n+        /// but also on part's metadata version. Why we have such logic only for renames? Because all other types of alter\n+        /// can be deduced based on difference between part's schema and table schema.\n+\n+        for (const auto & [rename_to, rename_from] : alter_conversions.rename_map)\n+        {\n+            for_file_renames.push_back({.type = MutationCommand::Type::RENAME_COLUMN, .column_name = rename_from, .rename_to = rename_to});\n+        }\n+    }\n+}\n+\n+/// It's legal to squash renames because commands with rename are always \"barrier\"\n+/// and executed separately from other types of commands.\n+static MutationCommands squashRenamesInCommands(const MutationCommands & commands)\n+{\n+    NameToNameMap squashed_renames;\n+    for (const auto & command : commands)\n+    {\n+        std::string result_name = command.rename_to;\n+\n+        bool squashed = false;\n+        for (const auto & [name_from, name_to] : squashed_renames)\n+        {\n+            if (name_to == command.column_name)\n+            {\n+                squashed = true;\n+                squashed_renames[name_from] = result_name;\n+                break;\n+            }\n+        }\n+        if (!squashed)\n+            squashed_renames[command.column_name] = result_name;\n+    }\n+\n+    MutationCommands squashed_commands;\n+    for (const auto & command : commands)\n+    {\n+        if (squashed_renames.contains(command.column_name))\n+        {\n+            squashed_commands.push_back(command);\n+            squashed_commands.back().rename_to = squashed_renames[command.column_name];\n+        }\n     }\n+    return squashed_commands;\n }\n \n /// Get the columns list of the resulting part in the same order as storage_columns.\n@@ -157,8 +224,13 @@ getColumnsForNewDataPart(\n     const Block & updated_header,\n     NamesAndTypesList storage_columns,\n     const SerializationInfoByName & serialization_infos,\n+    const MutationCommands & commands_for_interpreter,\n     const MutationCommands & commands_for_removes)\n {\n+    MutationCommands all_commands;\n+    all_commands.insert(all_commands.end(), commands_for_interpreter.begin(), commands_for_interpreter.end());\n+    all_commands.insert(all_commands.end(), commands_for_removes.begin(), commands_for_removes.end());\n+\n     NameSet removed_columns;\n     NameToNameMap renamed_columns_to_from;\n     NameToNameMap renamed_columns_from_to;\n@@ -174,8 +246,9 @@ getColumnsForNewDataPart(\n             storage_columns.emplace_back(column);\n     }\n \n-    /// All commands are validated in AlterCommand so we don't care about order\n-    for (const auto & command : commands_for_removes)\n+    MutationCommands squashed_commands = squashRenamesInCommands(all_commands);\n+\n+    for (const auto & command : squashed_commands)\n     {\n         if (command.type == MutationCommand::UPDATE)\n         {\n@@ -268,20 +341,38 @@ getColumnsForNewDataPart(\n                 /// should it's previous version should be dropped or removed\n                 if (renamed_columns_to_from.contains(it->name) && !was_renamed && !was_removed)\n                     throw Exception(\n-                                    ErrorCodes::LOGICAL_ERROR,\n-                                    \"Incorrect mutation commands, trying to rename column {} to {}, \"\n-                                    \"but part {} already has column {}\",\n-                                    renamed_columns_to_from[it->name], it->name, source_part->name, it->name);\n+                        ErrorCodes::LOGICAL_ERROR,\n+                        \"Incorrect mutation commands, trying to rename column {} to {}, \"\n+                        \"but part {} already has column {}\",\n+                        renamed_columns_to_from[it->name], it->name, source_part->name, it->name);\n \n                 /// Column was renamed and no other column renamed to it's name\n                 /// or column is dropped.\n                 if (!renamed_columns_to_from.contains(it->name) && (was_renamed || was_removed))\n+                {\n                     it = storage_columns.erase(it);\n+                }\n                 else\n                 {\n-                    /// Take a type from source part column.\n-                    /// It may differ from column type in storage.\n-                    it->type = source_col->second;\n+\n+                    if (was_removed)\n+                    { /// DROP COLUMN xxx, RENAME COLUMN yyy TO xxx\n+                        auto renamed_from = renamed_columns_to_from.at(it->name);\n+                        auto maybe_name_and_type = source_columns.tryGetByName(renamed_from);\n+                        if (!maybe_name_and_type)\n+                            throw Exception(\n+                                ErrorCodes::LOGICAL_ERROR,\n+                                \"Got incorrect mutation commands, column {} was renamed from {}, but it doesn't exist in source columns {}\",\n+                                it->name, renamed_from, source_columns.toString());\n+\n+                        it->type = maybe_name_and_type->type;\n+                    }\n+                    else\n+                    {\n+                        /// Take a type from source part column.\n+                        /// It may differ from column type in storage.\n+                        it->type = source_col->second;\n+                    }\n                     ++it;\n                 }\n             }\n@@ -539,27 +630,36 @@ static NameToNameVector collectFilesForRenames(\n     /// Collect counts for shared streams of different columns. As an example, Nested columns have shared stream with array sizes.\n     auto stream_counts = getStreamCounts(source_part, source_part->getColumns().getNames());\n     NameToNameVector rename_vector;\n+    NameSet collected_names;\n+\n+    auto add_rename = [&rename_vector, &collected_names] (const std::string & file_rename_from, const std::string & file_rename_to)\n+    {\n+        if (collected_names.emplace(file_rename_from).second)\n+            rename_vector.emplace_back(file_rename_from, file_rename_to);\n+    };\n+\n+    MutationCommands squashed_commands = squashRenamesInCommands(commands_for_removes);\n \n     /// Remove old data\n-    for (const auto & command : commands_for_removes)\n+    for (const auto & command : squashed_commands)\n     {\n         if (command.type == MutationCommand::Type::DROP_INDEX)\n         {\n             if (source_part->checksums.has(INDEX_FILE_PREFIX + command.column_name + \".idx2\"))\n             {\n-                rename_vector.emplace_back(INDEX_FILE_PREFIX + command.column_name + \".idx2\", \"\");\n-                rename_vector.emplace_back(INDEX_FILE_PREFIX + command.column_name + mrk_extension, \"\");\n+                add_rename(INDEX_FILE_PREFIX + command.column_name + \".idx2\", \"\");\n+                add_rename(INDEX_FILE_PREFIX + command.column_name + mrk_extension, \"\");\n             }\n             else if (source_part->checksums.has(INDEX_FILE_PREFIX + command.column_name + \".idx\"))\n             {\n-                rename_vector.emplace_back(INDEX_FILE_PREFIX + command.column_name + \".idx\", \"\");\n-                rename_vector.emplace_back(INDEX_FILE_PREFIX + command.column_name + mrk_extension, \"\");\n+                add_rename(INDEX_FILE_PREFIX + command.column_name + \".idx\", \"\");\n+                add_rename(INDEX_FILE_PREFIX + command.column_name + mrk_extension, \"\");\n             }\n         }\n         else if (command.type == MutationCommand::Type::DROP_PROJECTION)\n         {\n             if (source_part->checksums.has(command.column_name + \".proj\"))\n-                rename_vector.emplace_back(command.column_name + \".proj\", \"\");\n+                add_rename(command.column_name + \".proj\", \"\");\n         }\n         else if (command.type == MutationCommand::Type::DROP_COLUMN)\n         {\n@@ -569,8 +669,8 @@ static NameToNameVector collectFilesForRenames(\n                 /// Delete files if they are no longer shared with another column.\n                 if (--stream_counts[stream_name] == 0)\n                 {\n-                    rename_vector.emplace_back(stream_name + \".bin\", \"\");\n-                    rename_vector.emplace_back(stream_name + mrk_extension, \"\");\n+                    add_rename(stream_name + \".bin\", \"\");\n+                    add_rename(stream_name + mrk_extension, \"\");\n                 }\n             };\n \n@@ -589,8 +689,8 @@ static NameToNameVector collectFilesForRenames(\n \n                 if (stream_from != stream_to)\n                 {\n-                    rename_vector.emplace_back(stream_from + \".bin\", stream_to + \".bin\");\n-                    rename_vector.emplace_back(stream_from + mrk_extension, stream_to + mrk_extension);\n+                    add_rename(stream_from + \".bin\", stream_to + \".bin\");\n+                    add_rename(stream_from + mrk_extension, stream_to + mrk_extension);\n                 }\n             };\n \n@@ -610,8 +710,8 @@ static NameToNameVector collectFilesForRenames(\n             {\n                 if (!new_streams.contains(old_stream) && --stream_counts[old_stream] == 0)\n                 {\n-                    rename_vector.emplace_back(old_stream + \".bin\", \"\");\n-                    rename_vector.emplace_back(old_stream + mrk_extension, \"\");\n+                    add_rename(old_stream + \".bin\", \"\");\n+                    add_rename(old_stream + mrk_extension, \"\");\n                 }\n             }\n         }\n@@ -628,6 +728,7 @@ void finalizeMutatedPart(\n     ExecuteTTLType execute_ttl_type,\n     const CompressionCodecPtr & codec,\n     ContextPtr context,\n+    StorageMetadataPtr metadata_snapshot,\n     bool sync)\n {\n     std::vector<std::unique_ptr<WriteBufferFromFileBase>> written_files;\n@@ -676,6 +777,12 @@ void finalizeMutatedPart(\n         written_files.push_back(std::move(out_comp));\n     }\n \n+    {\n+        auto out_metadata = new_data_part->getDataPartStorage().writeFile(IMergeTreeDataPart::METADATA_VERSION_FILE_NAME, 4096, context->getWriteSettings());\n+        DB::writeText(metadata_snapshot->getMetadataVersion(), *out_metadata);\n+        written_files.push_back(std::move(out_metadata));\n+    }\n+\n     {\n         /// Write a file with a description of columns.\n         auto out_columns = new_data_part->getDataPartStorage().writeFile(\"columns.txt\", 4096, context->getWriteSettings());\n@@ -1312,13 +1419,27 @@ class MutateSomePartColumnsTask : public IExecutableTask\n         ctx->new_data_part->storeVersionMetadata();\n \n         NameSet hardlinked_files;\n+\n+        /// NOTE: Renames must be done in order\n+        for (const auto & [rename_from, rename_to] : ctx->files_to_rename)\n+        {\n+            if (rename_to.empty()) /// It's DROP COLUMN\n+            {\n+                /// pass\n+            }\n+            else\n+            {\n+                ctx->new_data_part->getDataPartStorage().createHardLinkFrom(\n+                    ctx->source_part->getDataPartStorage(), rename_from, rename_to);\n+                hardlinked_files.insert(rename_from);\n+            }\n+        }\n         /// Create hardlinks for unchanged files\n         for (auto it = ctx->source_part->getDataPartStorage().iterate(); it->isValid(); it->next())\n         {\n             if (ctx->files_to_skip.contains(it->name()))\n                 continue;\n \n-            String destination;\n             String file_name = it->name();\n \n             auto rename_it = std::find_if(ctx->files_to_rename.begin(), ctx->files_to_rename.end(), [&file_name](const auto & rename_pair)\n@@ -1328,20 +1449,17 @@ class MutateSomePartColumnsTask : public IExecutableTask\n \n             if (rename_it != ctx->files_to_rename.end())\n             {\n-                if (rename_it->second.empty())\n-                    continue;\n-                destination = rename_it->second;\n-            }\n-            else\n-            {\n-                destination = it->name();\n+                /// RENAMEs and DROPs already processed\n+                continue;\n             }\n \n+            String destination = it->name();\n+\n             if (it->isFile())\n             {\n                 ctx->new_data_part->getDataPartStorage().createHardLinkFrom(\n-                    ctx->source_part->getDataPartStorage(), it->name(), destination);\n-                hardlinked_files.insert(it->name());\n+                    ctx->source_part->getDataPartStorage(), file_name, destination);\n+                hardlinked_files.insert(file_name);\n             }\n             else if (!endsWith(it->name(), \".tmp_proj\")) // ignore projection tmp merge dir\n             {\n@@ -1437,7 +1555,7 @@ class MutateSomePartColumnsTask : public IExecutableTask\n             }\n         }\n \n-        MutationHelpers::finalizeMutatedPart(ctx->source_part, ctx->new_data_part, ctx->execute_ttl_type, ctx->compression_codec, ctx->context, ctx->need_sync);\n+        MutationHelpers::finalizeMutatedPart(ctx->source_part, ctx->new_data_part, ctx->execute_ttl_type, ctx->compression_codec, ctx->context, ctx->metadata_snapshot, ctx->need_sync);\n     }\n \n \n@@ -1599,7 +1717,7 @@ bool MutateTask::prepare()\n     context_for_reading->setSetting(\"allow_asynchronous_read_from_io_pool_for_merge_tree\", false);\n     context_for_reading->setSetting(\"max_streams_for_merge_tree_reading\", Field(0));\n \n-    MutationHelpers::splitMutationCommands(ctx->source_part, ctx->commands_for_part, ctx->for_interpreter, ctx->for_file_renames);\n+    MutationHelpers::splitAndModifyMutationCommands(ctx->source_part, ctx->commands_for_part, ctx->for_interpreter, ctx->for_file_renames);\n \n     ctx->stage_progress = std::make_unique<MergeStageProgress>(1.0);\n \n@@ -1644,9 +1762,9 @@ bool MutateTask::prepare()\n \n     auto [new_columns, new_infos] = MutationHelpers::getColumnsForNewDataPart(\n         ctx->source_part, ctx->updated_header, ctx->storage_columns,\n-        ctx->source_part->getSerializationInfos(), ctx->commands_for_part);\n+        ctx->source_part->getSerializationInfos(), ctx->for_interpreter, ctx->for_file_renames);\n \n-    ctx->new_data_part->setColumns(new_columns, new_infos);\n+    ctx->new_data_part->setColumns(new_columns, new_infos, ctx->metadata_snapshot->getMetadataVersion());\n     ctx->new_data_part->partition.assign(ctx->source_part->partition);\n \n     /// Don't change granularity type while mutating subset of columns\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\nindex 557123ddae27..f1bdc9f43af1 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp\n@@ -149,7 +149,7 @@ void ReplicatedMergeTreeAttachThread::runImpl()\n     const bool replica_metadata_version_exists = zookeeper->tryGet(replica_path + \"/metadata_version\", replica_metadata_version);\n     if (replica_metadata_version_exists)\n     {\n-        storage.metadata_version = parse<int>(replica_metadata_version);\n+        storage.setInMemoryMetadata(metadata_snapshot->withMetadataVersion(parse<int>(replica_metadata_version)));\n     }\n     else\n     {\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\nindex 59a46540b71a..c146cfb4ab80 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n@@ -11,6 +11,7 @@\n #include <Parsers/formatAST.h>\n #include <base/sort.h>\n \n+#include <ranges>\n \n namespace DB\n {\n@@ -1770,19 +1771,40 @@ ReplicatedMergeTreeMergePredicate ReplicatedMergeTreeQueue::getMergePredicate(zk\n }\n \n \n-MutationCommands ReplicatedMergeTreeQueue::getFirstAlterMutationCommandsForPart(const MergeTreeData::DataPartPtr & part) const\n+std::map<int64_t, MutationCommands> ReplicatedMergeTreeQueue::getAlterMutationCommandsForPart(const MergeTreeData::DataPartPtr & part) const\n {\n-    std::lock_guard lock(state_mutex);\n+    std::unique_lock lock(state_mutex);\n     auto in_partition = mutations_by_partition.find(part->info.partition_id);\n     if (in_partition == mutations_by_partition.end())\n-        return MutationCommands{};\n+        return {};\n \n-    Int64 part_version = part->info.getDataVersion();\n-    for (auto [mutation_version, mutation_status] : in_partition->second)\n-        if (mutation_version > part_version && mutation_status->entry->alter_version != -1)\n-            return mutation_status->entry->commands;\n+    Int64 part_metadata_version = part->getMetadataVersion();\n+    std::map<int64_t, MutationCommands> result;\n+    /// Here we return mutation commands for part which has bigger alter version than part metadata version.\n+    /// Please note, we don't use getDataVersion(). It's because these alter commands are used for in-fly conversions\n+    /// of part's metadata.\n+    for (const auto & [mutation_version, mutation_status] : in_partition->second | std::views::reverse)\n+    {\n+        int32_t alter_version = mutation_status->entry->alter_version;\n+        if (alter_version != -1)\n+        {\n+            if (!alter_sequence.canExecuteDataAlter(alter_version, lock))\n+                continue;\n \n-    return MutationCommands{};\n+            /// we take commands with bigger metadata version\n+            if (alter_version > part_metadata_version)\n+            {\n+                result[mutation_version] = mutation_status->entry->commands;\n+            }\n+            else\n+            {\n+                /// entries are ordered, we processing them in reverse order so we can break\n+                break;\n+            }\n+        }\n+    }\n+\n+    return result;\n }\n \n MutationCommands ReplicatedMergeTreeQueue::getMutationCommands(\n@@ -1824,7 +1846,18 @@ MutationCommands ReplicatedMergeTreeQueue::getMutationCommands(\n \n     MutationCommands commands;\n     for (auto it = begin; it != end; ++it)\n-        commands.insert(commands.end(), it->second->entry->commands.begin(), it->second->entry->commands.end());\n+    {\n+        const auto & commands_from_entry = it->second->entry->commands;\n+\n+        if (commands_from_entry.containBarrierCommand())\n+        {\n+            if (commands.empty())\n+                commands.insert(commands.end(), commands_from_entry.begin(), commands_from_entry.end());\n+            break;\n+        }\n+        else\n+            commands.insert(commands.end(), commands_from_entry.begin(), commands_from_entry.end());\n+    }\n \n     return commands;\n }\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\nindex aa1adde0106a..efe792641535 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\n@@ -394,10 +394,10 @@ class ReplicatedMergeTreeQueue\n \n     MutationCommands getMutationCommands(const MergeTreeData::DataPartPtr & part, Int64 desired_mutation_version) const;\n \n-    /// Return mutation commands for part with smallest mutation version bigger\n-    /// than data part version. Used when we apply alter commands on fly,\n+    /// Return mutation commands for part which could be not applied to\n+    /// it according to part mutation version. Used when we apply alter commands on fly,\n     /// without actual data modification on disk.\n-    MutationCommands getFirstAlterMutationCommandsForPart(const MergeTreeData::DataPartPtr & part) const;\n+    std::map<int64_t, MutationCommands> getAlterMutationCommandsForPart(const MergeTreeData::DataPartPtr & part) const;\n \n     /// Mark finished mutations as done. If the function needs to be called again at some later time\n     /// (because some mutations are probably done but we are not sure yet), returns true.\ndiff --git a/src/Storages/MutationCommands.cpp b/src/Storages/MutationCommands.cpp\nindex 0c9e9223929f..aa77988348dd 100644\n--- a/src/Storages/MutationCommands.cpp\n+++ b/src/Storages/MutationCommands.cpp\n@@ -23,6 +23,12 @@ namespace ErrorCodes\n     extern const int MULTIPLE_ASSIGNMENTS_TO_COLUMN;\n }\n \n+\n+bool MutationCommand::isBarrierCommand() const\n+{\n+    return type == RENAME_COLUMN;\n+}\n+\n std::optional<MutationCommand> MutationCommand::parse(ASTAlterCommand * command, bool parse_alter_commands)\n {\n     if (command->type == ASTAlterCommand::DELETE)\n@@ -212,4 +218,14 @@ bool MutationCommands::hasNonEmptyMutationCommands() const\n     return false;\n }\n \n+bool MutationCommands::containBarrierCommand() const\n+{\n+    for (const auto & command : *this)\n+    {\n+        if (command.isBarrierCommand())\n+            return true;\n+    }\n+    return false;\n+}\n+\n }\ndiff --git a/src/Storages/MutationCommands.h b/src/Storages/MutationCommands.h\nindex aca91c16e85d..5ef0cfda1be6 100644\n--- a/src/Storages/MutationCommands.h\n+++ b/src/Storages/MutationCommands.h\n@@ -67,6 +67,9 @@ struct MutationCommand\n \n     /// If parse_alter_commands, than consider more Alter commands as mutation commands\n     static std::optional<MutationCommand> parse(ASTAlterCommand * command, bool parse_alter_commands = false);\n+\n+    /// This command shouldn't stick with other commands\n+    bool isBarrierCommand() const;\n };\n \n /// Multiple mutation commands, possible from different ALTER queries\n@@ -79,6 +82,11 @@ class MutationCommands : public std::vector<MutationCommand>\n     void readText(ReadBuffer & in);\n     std::string toString() const;\n     bool hasNonEmptyMutationCommands() const;\n+\n+    /// These set of commands contain barrier command and shouldn't\n+    /// stick with other commands. Commands from one set have already been validated\n+    /// to be executed without issues on the creation state.\n+    bool containBarrierCommand() const;\n };\n \n using MutationCommandsConstPtr = std::shared_ptr<MutationCommands>;\ndiff --git a/src/Storages/StorageInMemoryMetadata.cpp b/src/Storages/StorageInMemoryMetadata.cpp\nindex f6550c6cd5da..45abd4bebef7 100644\n--- a/src/Storages/StorageInMemoryMetadata.cpp\n+++ b/src/Storages/StorageInMemoryMetadata.cpp\n@@ -41,6 +41,7 @@ StorageInMemoryMetadata::StorageInMemoryMetadata(const StorageInMemoryMetadata &\n     , settings_changes(other.settings_changes ? other.settings_changes->clone() : nullptr)\n     , select(other.select)\n     , comment(other.comment)\n+    , metadata_version(other.metadata_version)\n {\n }\n \n@@ -69,6 +70,7 @@ StorageInMemoryMetadata & StorageInMemoryMetadata::operator=(const StorageInMemo\n         settings_changes.reset();\n     select = other.select;\n     comment = other.comment;\n+    metadata_version = other.metadata_version;\n     return *this;\n }\n \n@@ -122,6 +124,18 @@ void StorageInMemoryMetadata::setSelectQuery(const SelectQueryDescription & sele\n     select = select_;\n }\n \n+void StorageInMemoryMetadata::setMetadataVersion(int32_t metadata_version_)\n+{\n+    metadata_version = metadata_version_;\n+}\n+\n+StorageInMemoryMetadata StorageInMemoryMetadata::withMetadataVersion(int32_t metadata_version_) const\n+{\n+    StorageInMemoryMetadata copy(*this);\n+    copy.setMetadataVersion(metadata_version_);\n+    return copy;\n+}\n+\n const ColumnsDescription & StorageInMemoryMetadata::getColumns() const\n {\n     return columns;\ndiff --git a/src/Storages/StorageInMemoryMetadata.h b/src/Storages/StorageInMemoryMetadata.h\nindex eadce5813347..25618c5b03fd 100644\n--- a/src/Storages/StorageInMemoryMetadata.h\n+++ b/src/Storages/StorageInMemoryMetadata.h\n@@ -50,6 +50,10 @@ struct StorageInMemoryMetadata\n \n     String comment;\n \n+    /// Version of metadata. Managed properly by ReplicatedMergeTree only\n+    /// (zero-initialization is important)\n+    int32_t metadata_version = 0;\n+\n     StorageInMemoryMetadata() = default;\n \n     StorageInMemoryMetadata(const StorageInMemoryMetadata & other);\n@@ -58,7 +62,7 @@ struct StorageInMemoryMetadata\n     StorageInMemoryMetadata(StorageInMemoryMetadata && other) = default;\n     StorageInMemoryMetadata & operator=(StorageInMemoryMetadata && other) = default;\n \n-    /// NOTE: Thread unsafe part. You should modify same StorageInMemoryMetadata\n+    /// NOTE: Thread unsafe part. You should not modify same StorageInMemoryMetadata\n     /// structure from different threads. It should be used as MultiVersion\n     /// object. See example in IStorage.\n \n@@ -90,6 +94,11 @@ struct StorageInMemoryMetadata\n     /// Set SELECT query for (Materialized)View\n     void setSelectQuery(const SelectQueryDescription & select_);\n \n+    /// Set version of metadata.\n+    void setMetadataVersion(int32_t metadata_version_);\n+    /// Get copy of current metadata with metadata_version_\n+    StorageInMemoryMetadata withMetadataVersion(int32_t metadata_version_) const;\n+\n     /// Returns combined set of columns\n     const ColumnsDescription & getColumns() const;\n \n@@ -218,6 +227,9 @@ struct StorageInMemoryMetadata\n     const SelectQueryDescription & getSelectQuery() const;\n     bool hasSelectQuery() const;\n \n+    /// Get version of metadata\n+    int32_t getMetadataVersion() const { return metadata_version; }\n+\n     /// Check that all the requested names are in the table and have the correct types.\n     void check(const NamesAndTypesList & columns) const;\n \ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 125322281d00..3ee56a2e62d0 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -1141,9 +1141,24 @@ MergeMutateSelectedEntryPtr StorageMergeTree::selectPartsToMutate(\n             if (current_ast_elements + commands_size >= max_ast_elements)\n                 break;\n \n-            current_ast_elements += commands_size;\n-            commands->insert(commands->end(), it->second.commands.begin(), it->second.commands.end());\n-            last_mutation_to_apply = it;\n+            const auto & single_mutation_commands = it->second.commands;\n+\n+            if (single_mutation_commands.containBarrierCommand())\n+            {\n+                if (commands->empty())\n+                {\n+                    commands->insert(commands->end(), single_mutation_commands.begin(), single_mutation_commands.end());\n+                    last_mutation_to_apply = it;\n+                }\n+                break;\n+            }\n+            else\n+            {\n+                current_ast_elements += commands_size;\n+                commands->insert(commands->end(), single_mutation_commands.begin(), single_mutation_commands.end());\n+                last_mutation_to_apply = it;\n+            }\n+\n         }\n \n         assert(commands->empty() == (last_mutation_to_apply == mutations_end_it));\n@@ -1238,7 +1253,10 @@ bool StorageMergeTree::scheduleDataProcessingJob(BackgroundJobsAssignee & assign\n     }\n     if (mutate_entry)\n     {\n-        auto task = std::make_shared<MutatePlainMergeTreeTask>(*this, metadata_snapshot, mutate_entry, shared_lock, common_assignee_trigger);\n+        /// We take new metadata snapshot here. It's because mutation commands can be executed only with metadata snapshot\n+        /// which is equal or more fresh than commands themselves. In extremely rare case it can happen that we will have alter\n+        /// in between we took snapshot above and selected commands. That is why we take new snapshot here.\n+        auto task = std::make_shared<MutatePlainMergeTreeTask>(*this, getInMemoryMetadataPtr(), mutate_entry, shared_lock, common_assignee_trigger);\n         assignee.scheduleMergeMutateTask(task);\n         return true;\n     }\n@@ -2089,14 +2107,22 @@ void StorageMergeTree::attachRestoredParts(MutableDataPartsVector && parts)\n }\n \n \n-MutationCommands StorageMergeTree::getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const\n+std::map<int64_t, MutationCommands> StorageMergeTree::getAlterMutationCommandsForPart(const DataPartPtr & part) const\n {\n     std::lock_guard lock(currently_processing_in_background_mutex);\n \n-    auto it = current_mutations_by_version.upper_bound(part->info.getDataVersion());\n-    if (it == current_mutations_by_version.end())\n-        return {};\n-    return it->second.commands;\n+    Int64 part_data_version = part->info.getDataVersion();\n+\n+    std::map<int64_t, MutationCommands> result;\n+    if (!current_mutations_by_version.empty())\n+    {\n+        const auto & [latest_mutation_id, latest_commands] = *current_mutations_by_version.rbegin();\n+        if (part_data_version < static_cast<int64_t>(latest_mutation_id))\n+        {\n+            result[latest_mutation_id] = latest_commands.commands;\n+        }\n+    }\n+    return result;\n }\n \n void StorageMergeTree::startBackgroundMovesIfNeeded()\ndiff --git a/src/Storages/StorageMergeTree.h b/src/Storages/StorageMergeTree.h\nindex 1dff6323e4c3..9385542dcac6 100644\n--- a/src/Storages/StorageMergeTree.h\n+++ b/src/Storages/StorageMergeTree.h\n@@ -265,7 +265,7 @@ class StorageMergeTree final : public MergeTreeData\n \n protected:\n \n-    MutationCommands getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const override;\n+    std::map<int64_t, MutationCommands> getAlterMutationCommandsForPart(const DataPartPtr & part) const override;\n };\n \n }\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex f35c68ce7d6a..dd0102133606 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -459,7 +459,7 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n \n             Coordination::Stat metadata_stat;\n             current_zookeeper->get(zookeeper_path + \"/metadata\", &metadata_stat);\n-            metadata_version = metadata_stat.version;\n+            setInMemoryMetadata(metadata_snapshot->withMetadataVersion(metadata_stat.version));\n         }\n         catch (Coordination::Exception & e)\n         {\n@@ -779,7 +779,7 @@ bool StorageReplicatedMergeTree::createTableIfNotExists(const StorageMetadataPtr\n             zkutil::CreateMode::Persistent));\n         ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/columns\", metadata_snapshot->getColumns().toString(),\n             zkutil::CreateMode::Persistent));\n-        ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/metadata_version\", std::to_string(metadata_version),\n+        ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/metadata_version\", toString(metadata_snapshot->getMetadataVersion()),\n             zkutil::CreateMode::Persistent));\n \n         /// The following 3 nodes were added in version 1.1.xxx, so we create them here, not in createNewZooKeeperNodes()\n@@ -852,7 +852,7 @@ void StorageReplicatedMergeTree::createReplica(const StorageMetadataPtr & metada\n             zkutil::CreateMode::Persistent));\n         ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/columns\", metadata_snapshot->getColumns().toString(),\n             zkutil::CreateMode::Persistent));\n-        ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/metadata_version\", std::to_string(metadata_version),\n+        ops.emplace_back(zkutil::makeCreateRequest(replica_path + \"/metadata_version\", toString(metadata_snapshot->getMetadataVersion()),\n             zkutil::CreateMode::Persistent));\n \n         /// The following 3 nodes were added in version 1.1.xxx, so we create them here, not in createNewZooKeeperNodes()\n@@ -1147,16 +1147,19 @@ void StorageReplicatedMergeTree::checkTableStructure(const String & zookeeper_pr\n }\n \n void StorageReplicatedMergeTree::setTableStructure(const StorageID & table_id, const ContextPtr & local_context,\n-    ColumnsDescription new_columns, const ReplicatedMergeTreeTableMetadata::Diff & metadata_diff)\n+    ColumnsDescription new_columns, const ReplicatedMergeTreeTableMetadata::Diff & metadata_diff, int32_t new_metadata_version)\n {\n     StorageInMemoryMetadata old_metadata = getInMemoryMetadata();\n+\n     StorageInMemoryMetadata new_metadata = metadata_diff.getNewMetadata(new_columns, local_context, old_metadata);\n+    new_metadata.setMetadataVersion(new_metadata_version);\n \n     /// Even if the primary/sorting/partition keys didn't change we must reinitialize it\n     /// because primary/partition key column types might have changed.\n     checkTTLExpressions(new_metadata, old_metadata);\n     setProperties(new_metadata, old_metadata);\n \n+\n     DatabaseCatalog::instance().getDatabase(table_id.database_name)->alterTable(local_context, table_id, new_metadata);\n }\n \n@@ -2766,8 +2769,9 @@ void StorageReplicatedMergeTree::cloneMetadataIfNeeded(const String & source_rep\n         return;\n     }\n \n+    auto metadata_snapshot = getInMemoryMetadataPtr();\n     Int32 source_metadata_version = parse<Int32>(source_metadata_version_str);\n-    if (metadata_version == source_metadata_version)\n+    if (metadata_snapshot->getMetadataVersion() == source_metadata_version)\n         return;\n \n     /// Our metadata it not up to date with source replica metadata.\n@@ -2785,7 +2789,7 @@ void StorageReplicatedMergeTree::cloneMetadataIfNeeded(const String & source_rep\n     /// if all such entries were cleaned up from the log and source_queue.\n \n     LOG_WARNING(log, \"Metadata version ({}) on replica is not up to date with metadata ({}) on source replica {}\",\n-                metadata_version, source_metadata_version, source_replica);\n+                metadata_snapshot->getMetadataVersion(), source_metadata_version, source_replica);\n \n     String source_metadata;\n     String source_columns;\n@@ -4956,14 +4960,15 @@ bool StorageReplicatedMergeTree::optimize(\n \n bool StorageReplicatedMergeTree::executeMetadataAlter(const StorageReplicatedMergeTree::LogEntry & entry)\n {\n-    if (entry.alter_version < metadata_version)\n+    auto current_metadata = getInMemoryMetadataPtr();\n+    if (entry.alter_version < current_metadata->getMetadataVersion())\n     {\n         /// TODO Can we replace it with LOGICAL_ERROR?\n         /// As for now, it may rarely happen due to reordering of ALTER_METADATA entries in the queue of\n         /// non-initial replica and also may happen after stale replica recovery.\n         LOG_WARNING(log, \"Attempt to update metadata of version {} \"\n                          \"to older version {} when processing log entry {}: {}\",\n-                         metadata_version, entry.alter_version, entry.znode_name, entry.toString());\n+                         current_metadata->getMetadataVersion(), entry.alter_version, entry.znode_name, entry.toString());\n         return true;\n     }\n \n@@ -5011,10 +5016,10 @@ bool StorageReplicatedMergeTree::executeMetadataAlter(const StorageReplicatedMer\n         LOG_INFO(log, \"Metadata changed in ZooKeeper. Applying changes locally.\");\n \n         auto metadata_diff = ReplicatedMergeTreeTableMetadata(*this, getInMemoryMetadataPtr()).checkAndFindDiff(metadata_from_entry, getInMemoryMetadataPtr()->getColumns(), getContext());\n-        setTableStructure(table_id, alter_context, std::move(columns_from_entry), metadata_diff);\n-        metadata_version = entry.alter_version;\n+        setTableStructure(table_id, alter_context, std::move(columns_from_entry), metadata_diff, entry.alter_version);\n \n-        LOG_INFO(log, \"Applied changes to the metadata of the table. Current metadata version: {}\", metadata_version);\n+        current_metadata = getInMemoryMetadataPtr();\n+        LOG_INFO(log, \"Applied changes to the metadata of the table. Current metadata version: {}\", current_metadata->getMetadataVersion());\n     }\n \n     {\n@@ -5026,7 +5031,7 @@ bool StorageReplicatedMergeTree::executeMetadataAlter(const StorageReplicatedMer\n \n     /// This transaction may not happen, but it's OK, because on the next retry we will eventually create/update this node\n     /// TODO Maybe do in in one transaction for Replicated database?\n-    zookeeper->createOrUpdate(fs::path(replica_path) / \"metadata_version\", std::to_string(metadata_version), zkutil::CreateMode::Persistent);\n+    zookeeper->createOrUpdate(fs::path(replica_path) / \"metadata_version\", std::to_string(current_metadata->getMetadataVersion()), zkutil::CreateMode::Persistent);\n \n     return true;\n }\n@@ -5150,7 +5155,7 @@ void StorageReplicatedMergeTree::alter(\n         size_t mutation_path_idx = std::numeric_limits<size_t>::max();\n \n         String new_metadata_str = future_metadata_in_zk.toString();\n-        ops.emplace_back(zkutil::makeSetRequest(fs::path(zookeeper_path) / \"metadata\", new_metadata_str, metadata_version));\n+        ops.emplace_back(zkutil::makeSetRequest(fs::path(zookeeper_path) / \"metadata\", new_metadata_str, current_metadata->getMetadataVersion()));\n \n         String new_columns_str = future_metadata.columns.toString();\n         ops.emplace_back(zkutil::makeSetRequest(fs::path(zookeeper_path) / \"columns\", new_columns_str, -1));\n@@ -5166,7 +5171,7 @@ void StorageReplicatedMergeTree::alter(\n \n         /// We can be sure, that in case of successful commit in zookeeper our\n         /// version will increments by 1. Because we update with version check.\n-        int new_metadata_version = metadata_version + 1;\n+        int new_metadata_version = current_metadata->getMetadataVersion() + 1;\n \n         alter_entry->type = LogEntry::ALTER_METADATA;\n         alter_entry->source_replica = replica_name;\n@@ -7936,9 +7941,9 @@ bool StorageReplicatedMergeTree::canUseAdaptiveGranularity() const\n }\n \n \n-MutationCommands StorageReplicatedMergeTree::getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const\n+std::map<int64_t, MutationCommands> StorageReplicatedMergeTree::getAlterMutationCommandsForPart(const DataPartPtr & part) const\n {\n-    return queue.getFirstAlterMutationCommandsForPart(part);\n+    return queue.getAlterMutationCommandsForPart(part);\n }\n \n \ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 1e31fad66695..925cef500769 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -214,8 +214,6 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     /// It's used if not set in engine's arguments while creating a replicated table.\n     static String getDefaultReplicaName(const ContextPtr & context_);\n \n-    int getMetadataVersion() const { return metadata_version; }\n-\n     /// Modify a CREATE TABLE query to make a variant which must be written to a backup.\n     void adjustCreateQueryForBackup(ASTPtr & create_query) const override;\n \n@@ -425,7 +423,6 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     std::atomic<bool> shutdown_called {false};\n     std::atomic<bool> flush_called {false};\n \n-    int metadata_version = 0;\n     /// Threads.\n \n     /// A task that keeps track of the updates in the logs of all replicas and loads them into the queue.\n@@ -502,8 +499,10 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n \n     /// A part of ALTER: apply metadata changes only (data parts are altered separately).\n     /// Must be called under IStorage::lockForAlter() lock.\n-    void setTableStructure(const StorageID & table_id, const ContextPtr & local_context,\n-                           ColumnsDescription new_columns, const ReplicatedMergeTreeTableMetadata::Diff & metadata_diff);\n+    void setTableStructure(\n+        const StorageID & table_id, const ContextPtr & local_context,\n+        ColumnsDescription new_columns, const ReplicatedMergeTreeTableMetadata::Diff & metadata_diff,\n+        int32_t new_metadata_version);\n \n     /** Check that the set of parts corresponds to that in ZK (/replicas/me/parts/).\n       * If any parts described in ZK are not locally, throw an exception.\n@@ -830,7 +829,7 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     void waitMutationToFinishOnReplicas(\n         const Strings & replicas, const String & mutation_id) const;\n \n-    MutationCommands getFirstAlterMutationCommandsForPart(const DataPartPtr & part) const override;\n+    std::map<int64_t, MutationCommands> getAlterMutationCommandsForPart(const DataPartPtr & part) const override;\n \n     void startBackgroundMovesIfNeeded() override;\n \n",
  "test_patch": "diff --git a/tests/integration/test_merge_tree_hdfs/test.py b/tests/integration/test_merge_tree_hdfs/test.py\nindex 3950077e619b..782237539fa6 100644\n--- a/tests/integration/test_merge_tree_hdfs/test.py\n+++ b/tests/integration/test_merge_tree_hdfs/test.py\n@@ -43,8 +43,18 @@ def create_table(cluster, table_name, additional_settings=None):\n \n FILES_OVERHEAD = 1\n FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files\n-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1\n-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1\n+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1\n+FILES_OVERHEAD_METADATA_VERSION = 1\n+FILES_OVERHEAD_PER_PART_WIDE = (\n+    FILES_OVERHEAD_PER_COLUMN * 3\n+    + 2\n+    + 6\n+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC\n+    + FILES_OVERHEAD_METADATA_VERSION\n+)\n+FILES_OVERHEAD_PER_PART_COMPACT = (\n+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION\n+)\n \n \n @pytest.fixture(scope=\"module\")\ndiff --git a/tests/integration/test_merge_tree_s3/test.py b/tests/integration/test_merge_tree_s3/test.py\nindex f0f81100320a..696c016f7603 100644\n--- a/tests/integration/test_merge_tree_s3/test.py\n+++ b/tests/integration/test_merge_tree_s3/test.py\n@@ -52,8 +52,18 @@ def cluster():\n \n FILES_OVERHEAD = 1\n FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files\n-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1\n-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1\n+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1\n+FILES_OVERHEAD_METADATA_VERSION = 1\n+FILES_OVERHEAD_PER_PART_WIDE = (\n+    FILES_OVERHEAD_PER_COLUMN * 3\n+    + 2\n+    + 6\n+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC\n+    + FILES_OVERHEAD_METADATA_VERSION\n+)\n+FILES_OVERHEAD_PER_PART_COMPACT = (\n+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION\n+)\n \n \n def create_table(node, table_name, **additional_settings):\n@@ -232,7 +242,6 @@ def test_insert_same_partition_and_merge(cluster, merge_vertical, node_name):\n def test_alter_table_columns(cluster, node_name):\n     node = cluster.instances[node_name]\n     create_table(node, \"s3_test\")\n-    minio = cluster.minio_client\n \n     node.query(\n         \"INSERT INTO s3_test VALUES {}\".format(generate_values(\"2020-01-03\", 4096))\ndiff --git a/tests/integration/test_merge_tree_s3_failover/test.py b/tests/integration/test_merge_tree_s3_failover/test.py\nindex c61cacc9d8c8..fef2e26bd2ec 100644\n--- a/tests/integration/test_merge_tree_s3_failover/test.py\n+++ b/tests/integration/test_merge_tree_s3_failover/test.py\n@@ -89,7 +89,7 @@ def drop_table(cluster):\n \n \n # S3 request will be failed for an appropriate part file write.\n-FILES_PER_PART_BASE = 5  # partition.dat, default_compression_codec.txt, count.txt, columns.txt, checksums.txt\n+FILES_PER_PART_BASE = 6  # partition.dat, metadata_version.txt, default_compression_codec.txt, count.txt, columns.txt, checksums.txt\n FILES_PER_PART_WIDE = (\n     FILES_PER_PART_BASE + 1 + 1 + 3 * 2\n )  # Primary index, MinMax, Mark and data file for column(s)\ndiff --git a/tests/integration/test_partition/test.py b/tests/integration/test_partition/test.py\nindex ae4393fc6f67..b59cc21f39fb 100644\n--- a/tests/integration/test_partition/test.py\n+++ b/tests/integration/test_partition/test.py\n@@ -105,6 +105,8 @@ def partition_complex_assert_checksums():\n         \"c4ca4238a0b923820dcc509a6f75849b\\tshadow/1/data/test/partition_complex/19700102_2_2_0/count.txt\\n\"\n         \"c4ca4238a0b923820dcc509a6f75849b\\tshadow/1/data/test/partition_complex/19700201_1_1_0/count.txt\\n\"\n         \"cfcb770c3ecd0990dcceb1bde129e6c6\\tshadow/1/data/test/partition_complex/19700102_2_2_0/p.bin\\n\"\n+        \"cfcd208495d565ef66e7dff9f98764da\\tshadow/1/data/test/partition_complex/19700102_2_2_0/metadata_version.txt\\n\"\n+        \"cfcd208495d565ef66e7dff9f98764da\\tshadow/1/data/test/partition_complex/19700201_1_1_0/metadata_version.txt\\n\"\n         \"e2af3bef1fd129aea73a890ede1e7a30\\tshadow/1/data/test/partition_complex/19700201_1_1_0/k.bin\\n\"\n         \"f2312862cc01adf34a93151377be2ddf\\tshadow/1/data/test/partition_complex/19700201_1_1_0/minmax_p.idx\\n\"\n     )\ndiff --git a/tests/integration/test_replicated_merge_tree_s3/test.py b/tests/integration/test_replicated_merge_tree_s3/test.py\nindex 0d978bb6967d..b90e28dfdb20 100644\n--- a/tests/integration/test_replicated_merge_tree_s3/test.py\n+++ b/tests/integration/test_replicated_merge_tree_s3/test.py\n@@ -44,8 +44,18 @@ def cluster():\n \n FILES_OVERHEAD = 1\n FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files\n-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1\n-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1\n+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1\n+FILES_OVERHEAD_METADATA_VERSION = 1\n+FILES_OVERHEAD_PER_PART_WIDE = (\n+    FILES_OVERHEAD_PER_COLUMN * 3\n+    + 2\n+    + 6\n+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC\n+    + FILES_OVERHEAD_METADATA_VERSION\n+)\n+FILES_OVERHEAD_PER_PART_COMPACT = (\n+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION\n+)\n \n \n def random_string(length):\ndiff --git a/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py b/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py\nindex 60a1b9b97460..1b80b80987de 100644\n--- a/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py\n+++ b/tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py\n@@ -45,8 +45,18 @@ def cluster():\n \n FILES_OVERHEAD = 1\n FILES_OVERHEAD_PER_COLUMN = 2  # Data and mark files\n-FILES_OVERHEAD_PER_PART_WIDE = FILES_OVERHEAD_PER_COLUMN * 3 + 2 + 6 + 1\n-FILES_OVERHEAD_PER_PART_COMPACT = 10 + 1\n+FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC = 1\n+FILES_OVERHEAD_METADATA_VERSION = 1\n+FILES_OVERHEAD_PER_PART_WIDE = (\n+    FILES_OVERHEAD_PER_COLUMN * 3\n+    + 2\n+    + 6\n+    + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC\n+    + FILES_OVERHEAD_METADATA_VERSION\n+)\n+FILES_OVERHEAD_PER_PART_COMPACT = (\n+    10 + FILES_OVERHEAD_DEFAULT_COMPRESSION_CODEC + FILES_OVERHEAD_METADATA_VERSION\n+)\n \n \n def random_string(length):\ndiff --git a/tests/integration/test_s3_zero_copy_ttl/test.py b/tests/integration/test_s3_zero_copy_ttl/test.py\nindex 9a782aacef6b..7dcf3734653d 100644\n--- a/tests/integration/test_s3_zero_copy_ttl/test.py\n+++ b/tests/integration/test_s3_zero_copy_ttl/test.py\n@@ -86,9 +86,9 @@ def test_ttl_move_and_s3(started_cluster):\n \n         print(f\"Total objects: {counter}\")\n \n-        if counter == 300:\n+        if counter == 330:\n             break\n \n         print(f\"Attempts remaining: {attempt}\")\n \n-    assert counter == 300\n+    assert counter == 330\ndiff --git a/tests/queries/0_stateless/01278_alter_rename_combination.reference b/tests/queries/0_stateless/01278_alter_rename_combination.reference\nindex cc912e9b265b..e70c2d2e6f8f 100644\n--- a/tests/queries/0_stateless/01278_alter_rename_combination.reference\n+++ b/tests/queries/0_stateless/01278_alter_rename_combination.reference\n@@ -1,7 +1,7 @@\n-CREATE TABLE default.rename_table\\n(\\n    `key` Int32,\\n    `old_value1` Int32,\\n    `value1` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table\\n(\\n    `key` Int32,\\n    `old_value1` Int32,\\n    `value1` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n key\told_value1\tvalue1\n 1\t2\t3\n-CREATE TABLE default.rename_table\\n(\\n    `k` Int32,\\n    `v1` Int32,\\n    `v2` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table\\n(\\n    `k` Int32,\\n    `v1` Int32,\\n    `v2` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n k\tv1\tv2\n 1\t2\t3\n 4\t5\t6\ndiff --git a/tests/queries/0_stateless/01278_alter_rename_combination.sql b/tests/queries/0_stateless/01278_alter_rename_combination.sql\nindex fa73362622c8..51322f5d86f1 100644\n--- a/tests/queries/0_stateless/01278_alter_rename_combination.sql\n+++ b/tests/queries/0_stateless/01278_alter_rename_combination.sql\n@@ -1,6 +1,6 @@\n DROP TABLE IF EXISTS rename_table;\n \n-CREATE TABLE rename_table (key Int32, value1 Int32, value2 Int32) ENGINE = MergeTree ORDER BY tuple();\n+CREATE TABLE rename_table (key Int32, value1 Int32, value2 Int32) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0;\n \n INSERT INTO rename_table VALUES (1, 2, 3);\n \ndiff --git a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference\nindex bf3358aea60a..532b8ce87123 100644\n--- a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference\n+++ b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference\n@@ -1,11 +1,11 @@\n-CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2` Int32\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n key\tvalue1_string\tvalue2\n 1\t2\t3\n-CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2_old` Int32,\\n    `value2` Int64 DEFAULT 7\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2_old` Int32,\\n    `value2` Int64 DEFAULT 7\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n key\tvalue1_string\tvalue2_old\tvalue2\n 1\t2\t3\t7\n 4\t5\t6\t7\n-CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2_old` Int64 DEFAULT 7\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS index_granularity = 8192\n+CREATE TABLE default.rename_table_multiple\\n(\\n    `key` Int32,\\n    `value1_string` String,\\n    `value2_old` Int64 DEFAULT 7\\n)\\nENGINE = MergeTree\\nORDER BY tuple()\\nSETTINGS min_bytes_for_wide_part = 0, index_granularity = 8192\n key\tvalue1_string\tvalue2_old\n 1\t2\t7\n 4\t5\t7\ndiff --git a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql\nindex f9462f0478e3..b0ccd7751ab0 100644\n--- a/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql\n+++ b/tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql\n@@ -1,6 +1,6 @@\n DROP TABLE IF EXISTS rename_table_multiple;\n \n-CREATE TABLE rename_table_multiple (key Int32, value1 String, value2 Int32) ENGINE = MergeTree ORDER BY tuple();\n+CREATE TABLE rename_table_multiple (key Int32, value1 String, value2 Int32) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0;\n \n INSERT INTO rename_table_multiple VALUES (1, 2, 3);\n \ndiff --git a/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference b/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference\nindex bbca9bbbfee7..f3fac9b32d31 100644\n--- a/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference\n+++ b/tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference\n@@ -7,25 +7,25 @@ file_segment_range_begin: 0\n file_segment_range_end:   745\n size:                     746\n state:                    DOWNLOADED\n-7\n-7\n+8\n+8\n 0\n 2\n 2\n-7\n+8\n Row 1:\n \u2500\u2500\u2500\u2500\u2500\u2500\n file_segment_range_begin: 0\n file_segment_range_end:   1659\n size:                     1660\n state:                    DOWNLOADED\n-7\n-7\n-7\n-7\n-21\n-31\n-38\n+8\n+8\n+8\n+8\n+24\n+35\n+43\n 5010500\n 18816\n Using storage policy: local_cache\n@@ -37,24 +37,24 @@ file_segment_range_begin: 0\n file_segment_range_end:   745\n size:                     746\n state:                    DOWNLOADED\n-7\n-7\n+8\n+8\n 0\n 2\n 2\n-7\n+8\n Row 1:\n \u2500\u2500\u2500\u2500\u2500\u2500\n file_segment_range_begin: 0\n file_segment_range_end:   1659\n size:                     1660\n state:                    DOWNLOADED\n-7\n-7\n-7\n-7\n-21\n-31\n-38\n+8\n+8\n+8\n+8\n+24\n+35\n+43\n 5010500\n 18816\ndiff --git a/tests/queries/0_stateless/02361_fsync_profile_events.sh b/tests/queries/0_stateless/02361_fsync_profile_events.sh\nindex 44a1bd58d36e..5b603133f6c7 100755\n--- a/tests/queries/0_stateless/02361_fsync_profile_events.sh\n+++ b/tests/queries/0_stateless/02361_fsync_profile_events.sh\n@@ -44,8 +44,8 @@ for i in {1..100}; do\n     \")\"\n \n     # Non retriable errors\n-    if [[ $FileSync -ne 7 ]]; then\n-        echo \"FileSync: $FileSync != 11\" >&2\n+    if [[ $FileSync -ne 8 ]]; then\n+        echo \"FileSync: $FileSync != 8\" >&2\n         exit 2\n     fi\n     # Check that all files was synced\ndiff --git a/tests/queries/0_stateless/02538_alter_rename_sequence.reference b/tests/queries/0_stateless/02538_alter_rename_sequence.reference\nnew file mode 100644\nindex 000000000000..73aa1b7e8d8c\n--- /dev/null\n+++ b/tests/queries/0_stateless/02538_alter_rename_sequence.reference\n@@ -0,0 +1,8 @@\n+1\t2\t3\n+4\t5\t6\n+{\"column1_renamed\":\"1\",\"column2_renamed\":\"2\",\"column3\":\"3\"}\n+{\"column1_renamed\":\"4\",\"column2_renamed\":\"5\",\"column3\":\"6\"}\n+1\t2\t3\n+4\t5\t6\n+{\"column1_renamed\":\"1\",\"column2_renamed\":\"2\",\"column3\":\"3\"}\n+{\"column1_renamed\":\"4\",\"column2_renamed\":\"5\",\"column3\":\"6\"}\ndiff --git a/tests/queries/0_stateless/02538_alter_rename_sequence.sql b/tests/queries/0_stateless/02538_alter_rename_sequence.sql\nnew file mode 100644\nindex 000000000000..d7df27dc7020\n--- /dev/null\n+++ b/tests/queries/0_stateless/02538_alter_rename_sequence.sql\n@@ -0,0 +1,59 @@\n+DROP TABLE IF EXISTS wrong_metadata;\n+\n+CREATE TABLE wrong_metadata(\n+    column1 UInt64,\n+    column2 UInt64,\n+    column3 UInt64\n+)\n+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata', '1')\n+ORDER BY tuple();\n+\n+INSERT INTO wrong_metadata VALUES (1, 2, 3);\n+\n+SYSTEM STOP REPLICATION QUEUES wrong_metadata;\n+\n+ALTER TABLE wrong_metadata RENAME COLUMN column1 TO column1_renamed SETTINGS replication_alter_partitions_sync = 0;\n+\n+INSERT INTO wrong_metadata VALUES (4, 5, 6);\n+\n+SELECT * FROM wrong_metadata ORDER BY column1;\n+\n+SYSTEM START REPLICATION QUEUES wrong_metadata;\n+\n+SYSTEM SYNC REPLICA wrong_metadata;\n+\n+ALTER TABLE wrong_metadata RENAME COLUMN column2 to column2_renamed SETTINGS replication_alter_partitions_sync = 2;\n+\n+SELECT * FROM wrong_metadata ORDER BY column1_renamed FORMAT JSONEachRow;\n+\n+DROP TABLE IF EXISTS wrong_metadata;\n+\n+\n+CREATE TABLE wrong_metadata_wide(\n+    column1 UInt64,\n+    column2 UInt64,\n+    column3 UInt64\n+)\n+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata_wide', '1')\n+ORDER BY tuple()\n+SETTINGS min_bytes_for_wide_part = 0;\n+\n+INSERT INTO wrong_metadata_wide VALUES (1, 2, 3);\n+\n+SYSTEM STOP REPLICATION QUEUES wrong_metadata_wide;\n+\n+ALTER TABLE wrong_metadata_wide RENAME COLUMN column1 TO column1_renamed SETTINGS replication_alter_partitions_sync = 0;\n+\n+INSERT INTO wrong_metadata_wide VALUES (4, 5, 6);\n+\n+SELECT * FROM wrong_metadata_wide ORDER by column1;\n+\n+SYSTEM START REPLICATION QUEUES wrong_metadata_wide;\n+\n+SYSTEM SYNC REPLICA wrong_metadata_wide;\n+\n+ALTER TABLE wrong_metadata_wide RENAME COLUMN column2 to column2_renamed SETTINGS replication_alter_partitions_sync = 2;\n+\n+SELECT * FROM wrong_metadata_wide ORDER BY column1_renamed FORMAT JSONEachRow;\n+\n+DROP TABLE IF EXISTS wrong_metadata_wide;\ndiff --git a/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference\nnew file mode 100644\nindex 000000000000..156128e3dd28\n--- /dev/null\n+++ b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference\n@@ -0,0 +1,1 @@\n+{\"v\":\"1\",\"v2\":\"77\"}\ndiff --git a/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh\nnew file mode 100755\nindex 000000000000..adaf1846552f\n--- /dev/null\n+++ b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh\n@@ -0,0 +1,58 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS table_to_rename\"\n+\n+$CLICKHOUSE_CLIENT --query=\"CREATE TABLE table_to_rename(v UInt64, v1 UInt64)ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part = 0\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO table_to_rename VALUES (1, 1)\"\n+\n+\n+# we want to following mutations to stuck\n+# That is why we stop merges and wait in loops until they actually start\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM STOP MERGES table_to_rename\"\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE table_to_rename RENAME COLUMN v1 to v2\" &\n+\n+counter=0 retries=60\n+\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"show create table table_to_rename\")\n+    if [[ $result == *\"v2\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE table_to_rename UPDATE v2 = 77 WHERE 1 = 1 SETTINGS mutations_sync = 2\" &\n+\n+counter=0 retries=60\n+\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SELECT count() from system.mutations where database='${CLICKHOUSE_DATABASE}' and table='table_to_rename'\")\n+    if [[ $result == \"2\" ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM START MERGES table_to_rename\"\n+\n+wait\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM table_to_rename FORMAT JSONEachRow\"\n+\n+\n+ $CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS table_to_rename\"\ndiff --git a/tests/queries/0_stateless/02555_davengers_rename_chain.reference b/tests/queries/0_stateless/02555_davengers_rename_chain.reference\nnew file mode 100644\nindex 000000000000..a9fc4b395e20\n--- /dev/null\n+++ b/tests/queries/0_stateless/02555_davengers_rename_chain.reference\n@@ -0,0 +1,26 @@\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+{\"a1\":\"4\",\"b1\":\"5\",\"c\":\"6\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+{\"a1\":\"4\",\"b1\":\"5\",\"c\":\"6\"}\n+{\"a1\":\"7\",\"b1\":\"8\",\"c\":\"9\"}\n+~~~~~~~\n+{\"b\":\"1\",\"a\":\"2\",\"c\":\"3\"}\n+{\"b\":\"4\",\"a\":\"5\",\"c\":\"6\"}\n+{\"b\":\"7\",\"a\":\"8\",\"c\":\"9\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+{\"a1\":\"4\",\"b1\":\"5\",\"c\":\"6\"}\n+~~~~~~~\n+{\"a1\":\"1\",\"b1\":\"2\",\"c\":\"3\"}\n+{\"a1\":\"4\",\"b1\":\"5\",\"c\":\"6\"}\n+{\"a1\":\"7\",\"b1\":\"8\",\"c\":\"9\"}\n+~~~~~~~\n+{\"b\":\"1\",\"a\":\"2\",\"c\":\"3\"}\n+{\"b\":\"4\",\"a\":\"5\",\"c\":\"6\"}\n+{\"b\":\"7\",\"a\":\"8\",\"c\":\"9\"}\n+~~~~~~~\ndiff --git a/tests/queries/0_stateless/02555_davengers_rename_chain.sh b/tests/queries/0_stateless/02555_davengers_rename_chain.sh\nnew file mode 100755\nindex 000000000000..b23f8085fd70\n--- /dev/null\n+++ b/tests/queries/0_stateless/02555_davengers_rename_chain.sh\n@@ -0,0 +1,143 @@\n+#!/usr/bin/env bash\n+# Tags: replica\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CUR_DIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS wrong_metadata\"\n+\n+$CLICKHOUSE_CLIENT -n --query=\"CREATE TABLE wrong_metadata(\n+    a UInt64,\n+    b UInt64,\n+    c UInt64\n+)\n+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata', '1')\n+ORDER BY tuple()\n+SETTINGS min_bytes_for_wide_part = 0\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata VALUES (1, 2, 3)\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM STOP MERGES wrong_metadata\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE wrong_metadata RENAME COLUMN a TO a1, RENAME COLUMN b to b1 SETTINGS replication_alter_partitions_sync = 0\"\n+\n+counter=0 retries=60\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SHOW CREATE TABLE wrong_metadata\")\n+    if [[ $result == *\"\\`a1\\` UInt64\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata ORDER BY a1 FORMAT JSONEachRow\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata VALUES (4, 5, 6)\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata ORDER BY a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE wrong_metadata RENAME COLUMN a1 TO b, RENAME COLUMN b1 to a SETTINGS replication_alter_partitions_sync = 0\"\n+\n+counter=0 retries=60\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SELECT * FROM system.mutations WHERE table = 'wrong_metadata' AND database='${CLICKHOUSE_DATABASE}'\")\n+    if [[ $result == *\"b1 TO a\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata VALUES (7, 8, 9)\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata ORDER by a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM START MERGES wrong_metadata\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM SYNC REPLICA wrong_metadata\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata order by a FORMAT JSONEachRow\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS wrong_metadata\"\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS wrong_metadata_compact\"\n+\n+$CLICKHOUSE_CLIENT -n --query=\"CREATE TABLE wrong_metadata_compact(\n+    a UInt64,\n+    b UInt64,\n+    c UInt64\n+)\n+ENGINE ReplicatedMergeTree('/test/{database}/tables/wrong_metadata_compact', '1')\n+ORDER BY tuple()\n+SETTINGS min_bytes_for_wide_part = 10000000\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata_compact VALUES (1, 2, 3)\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM STOP MERGES wrong_metadata_compact\"\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE wrong_metadata_compact RENAME COLUMN a TO a1, RENAME COLUMN b to b1 SETTINGS replication_alter_partitions_sync = 0\"\n+\n+counter=0 retries=60\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SHOW CREATE TABLE wrong_metadata_compact\")\n+    if [[ $result == *\"\\`a1\\` UInt64\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata_compact ORDER BY a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata_compact VALUES (4, 5, 6)\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata_compact ORDER BY a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"ALTER TABLE wrong_metadata_compact RENAME COLUMN a1 TO b, RENAME COLUMN b1 to a SETTINGS replication_alter_partitions_sync = 0\"\n+\n+counter=0 retries=60\n+I=0\n+while [[ $counter -lt $retries ]]; do\n+    I=$((I + 1))\n+    result=$($CLICKHOUSE_CLIENT --query \"SELECT * FROM system.mutations WHERE table = 'wrong_metadata_compact' AND database='${CLICKHOUSE_DATABASE}'\")\n+    if [[ $result == *\"b1 TO a\"* ]]; then\n+        break;\n+    fi\n+    sleep 0.1\n+    ((++counter))\n+done\n+\n+$CLICKHOUSE_CLIENT --query=\"INSERT INTO wrong_metadata_compact VALUES (7, 8, 9)\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata_compact ORDER by a1 FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM START MERGES wrong_metadata_compact\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SYSTEM SYNC REPLICA wrong_metadata_compact\"\n+\n+$CLICKHOUSE_CLIENT --query=\"SELECT * FROM wrong_metadata_compact order by a FORMAT JSONEachRow\"\n+$CLICKHOUSE_CLIENT --query=\"SELECT '~~~~~~~'\"\n+\n+$CLICKHOUSE_CLIENT --query=\"DROP TABLE IF EXISTS wrong_metadata_compact\"\n",
  "problem_statement": "Mutations may fail when coalesced together\n**Describe what's wrong**\r\n\r\nThe following example should succeed. If merges are not stopped and each mutation is given opportunity to complete, then no errors arise. However, they fail when ClickHouse decides to coalesce them and run in a single task.\r\n\r\n**How to reproduce**\r\n\r\n```sql\r\nCREATE TABLE t\r\n(\r\n    `v` DateTime,\r\n    `v1` DateTime\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(v)\r\nORDER BY tuple()\r\nSETTINGS min_bytes_for_wide_part = 0;\r\n\r\nINSERT INTO t VALUES (1,1);\r\nSYSTEM STOP MERGES;\r\nALTER TABLE t RENAME COLUMN v1 to v2; -- this blocks, start a new client/connection for the next query\r\nALTER TABLE t UPDATE v2 = 1 WHERE 1 = 1;\r\nSYSTEM START MERGES;\r\n```\r\n\r\n```sql\r\nSELECT *\r\nFROM system.mutations\r\n\r\nQuery id: 3c225cd0-a5ba-4395-80e5-4a272446b5d2\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u252c\u2500mutation_id\u2500\u2500\u2500\u2500\u252c\u2500command\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500create_time\u2500\u252c\u2500block_numbers.partition_id\u2500\u252c\u2500block_numbers.number\u2500\u252c\u2500parts_to_do_names\u2500\u252c\u2500parts_to_do\u2500\u252c\u2500is_done\u2500\u252c\u2500latest_failed_part\u2500\u252c\u2500\u2500\u2500\u2500latest_fail_time\u2500\u252c\u2500latest_fail_reason\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 default  \u2502 t     \u2502 mutation_2.txt \u2502 RENAME COLUMN v1 TO v2    \u2502 2022-02-05 21:19:11 \u2502 ['']                       \u2502 [2]                  \u2502 ['197001_1_1_0']  \u2502           1 \u2502       0 \u2502 197001_1_1_0       \u2502 2022-02-05 21:19:23 \u2502 Code: 246. DB::Exception: Bad size of marks file '/var/lib/clickhouse/store/366/3668606c-16fe-4e35-8a4e-3b2bd9def292/197001_1_1_0/v1.mrk2': 0, must be: 48: While executing MergeTreeInOrder. (CORRUPTED_DATA) (version 22.1.3.7 (official build)) \u2502\r\n\u2502 default  \u2502 t     \u2502 mutation_3.txt \u2502 UPDATE v2 = 1 WHERE 1 = 1 \u2502 2022-02-05 21:19:16 \u2502 ['']                       \u2502 [3]                  \u2502 ['197001_1_1_0']  \u2502           1 \u2502       0 \u2502 197001_1_1_0       \u2502 2022-02-05 21:19:23 \u2502 Code: 246. DB::Exception: Bad size of marks file '/var/lib/clickhouse/store/366/3668606c-16fe-4e35-8a4e-3b2bd9def292/197001_1_1_0/v1.mrk2': 0, must be: 48: While executing MergeTreeInOrder. (CORRUPTED_DATA) (version 22.1.3.7 (official build)) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nand logs\r\n\r\n```\r\n2022.02.05 21:19:28.592575 [ 73 ] {3668606c-16fe-4e35-8a4e-3b2bd9def292::197001_1_1_0_3} <Error> default.t (3668606c-16fe-4e35-8a4e-3b2bd9def292): Cannot quickly remove directory /var/lib/clickhouse/store/366/3668606c-16fe-4e35-8a4e-3b2bd9def292/delete_tmp_tmp_mut_197001_1_1_0_3 by removing files; fallback to recursive removal. Reason: Code: 458. DB::ErrnoException: Cannot unlink file /var/lib/clickhouse/store/366/3668606c-16fe-4e35-8a4e-3b2bd9def292/delete_tmp_tmp_mut_197001_1_1_0_3/v1.bin, errno: 2, strerror: No such file or directory. (CANNOT_UNLINK) (version 22.1.3.7 (official build))\r\n```\n",
  "hints_text": "@alesapin the solution can be very simple: don't coalesce mutations involving RENAME column.\r\nThe mutation with RENAME will work as a barrier, it will split everything into three parts: everything before it, itself, and everything after it.",
  "created_at": "2023-02-01T17:22:09Z",
  "modified_files": [
    "b/src/Storages/MergeTree/AlterConversions.cpp",
    "src/Storages/MergeTree/AlterConversions.h",
    "src/Storages/MergeTree/DataPartStorageOnDiskBase.cpp",
    "src/Storages/MergeTree/DataPartsExchange.cpp",
    "src/Storages/MergeTree/IMergeTreeDataPart.cpp",
    "src/Storages/MergeTree/IMergeTreeDataPart.h",
    "src/Storages/MergeTree/MergeTask.cpp",
    "src/Storages/MergeTree/MergeTreeData.cpp",
    "src/Storages/MergeTree/MergeTreeData.h",
    "src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp",
    "src/Storages/MergeTree/MergeTreeDataWriter.cpp",
    "src/Storages/MergeTree/MergeTreeMarksLoader.cpp",
    "src/Storages/MergeTree/MergeTreeWriteAheadLog.cpp",
    "src/Storages/MergeTree/MergedBlockOutputStream.cpp",
    "src/Storages/MergeTree/MergedColumnOnlyOutputStream.cpp",
    "src/Storages/MergeTree/MutateTask.cpp",
    "src/Storages/MergeTree/ReplicatedMergeTreeAttachThread.cpp",
    "src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp",
    "src/Storages/MergeTree/ReplicatedMergeTreeQueue.h",
    "src/Storages/MutationCommands.cpp",
    "src/Storages/MutationCommands.h",
    "src/Storages/StorageInMemoryMetadata.cpp",
    "src/Storages/StorageInMemoryMetadata.h",
    "src/Storages/StorageMergeTree.cpp",
    "src/Storages/StorageMergeTree.h",
    "src/Storages/StorageReplicatedMergeTree.cpp",
    "src/Storages/StorageReplicatedMergeTree.h"
  ],
  "modified_test_files": [
    "tests/integration/test_merge_tree_hdfs/test.py",
    "tests/integration/test_merge_tree_s3/test.py",
    "tests/integration/test_merge_tree_s3_failover/test.py",
    "tests/integration/test_partition/test.py",
    "tests/integration/test_replicated_merge_tree_s3/test.py",
    "tests/integration/test_replicated_merge_tree_s3_zero_copy/test.py",
    "tests/integration/test_s3_zero_copy_ttl/test.py",
    "tests/queries/0_stateless/01278_alter_rename_combination.reference",
    "tests/queries/0_stateless/01278_alter_rename_combination.sql",
    "tests/queries/0_stateless/01281_alter_rename_and_other_renames.reference",
    "tests/queries/0_stateless/01281_alter_rename_and_other_renames.sql",
    "tests/queries/0_stateless/02241_filesystem_cache_on_write_operations.reference",
    "tests/queries/0_stateless/02361_fsync_profile_events.sh",
    "b/tests/queries/0_stateless/02538_alter_rename_sequence.reference",
    "b/tests/queries/0_stateless/02538_alter_rename_sequence.sql",
    "b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.reference",
    "b/tests/queries/0_stateless/02543_alter_rename_modify_stuck.sh",
    "b/tests/queries/0_stateless/02555_davengers_rename_chain.reference",
    "b/tests/queries/0_stateless/02555_davengers_rename_chain.sh"
  ]
}