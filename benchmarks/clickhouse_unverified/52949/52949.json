{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 52949,
  "instance_id": "ClickHouse__ClickHouse-52949",
  "issue_numbers": [
    "52590"
  ],
  "base_commit": "1b71c038f97905aa3e10be36d1f3f95f5a2a042a",
  "patch": "diff --git a/src/Interpreters/inplaceBlockConversions.cpp b/src/Interpreters/inplaceBlockConversions.cpp\nindex 5bbd2667f553..4cac2f0e20c8 100644\n--- a/src/Interpreters/inplaceBlockConversions.cpp\n+++ b/src/Interpreters/inplaceBlockConversions.cpp\n@@ -306,7 +306,9 @@ void fillMissingColumns(\n                     return;\n \n                 size_t level = ISerialization::getArrayLevel(subpath);\n-                assert(level < num_dimensions);\n+                /// It can happen if element of Array is Map.\n+                if (level >= num_dimensions)\n+                    return;\n \n                 auto stream_name = ISerialization::getFileNameForStream(*requested_column, subpath);\n                 auto it = offsets_columns.find(stream_name);\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex af6906e004da..9243c91987b5 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -89,7 +89,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     virtual MergeTreeReaderPtr getReader(\n         const NamesAndTypesList & columns_,\n-        const StorageMetadataPtr & metadata_snapshot,\n+        const StorageSnapshotPtr & storage_snapshot,\n         const MarkRanges & mark_ranges,\n         UncompressedCache * uncompressed_cache,\n         MarkCache * mark_cache,\ndiff --git a/src/Storages/MergeTree/IMergeTreeReader.cpp b/src/Storages/MergeTree/IMergeTreeReader.cpp\nindex f9b97a6a05db..4bb8c4006910 100644\n--- a/src/Storages/MergeTree/IMergeTreeReader.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeReader.cpp\n@@ -24,7 +24,7 @@ namespace ErrorCodes\n IMergeTreeReader::IMergeTreeReader(\n     MergeTreeDataPartInfoForReaderPtr data_part_info_for_read_,\n     const NamesAndTypesList & columns_,\n-    const StorageMetadataPtr & metadata_snapshot_,\n+    const StorageSnapshotPtr & storage_snapshot_,\n     UncompressedCache * uncompressed_cache_,\n     MarkCache * mark_cache_,\n     const MarkRanges & all_mark_ranges_,\n@@ -35,7 +35,7 @@ IMergeTreeReader::IMergeTreeReader(\n     , uncompressed_cache(uncompressed_cache_)\n     , mark_cache(mark_cache_)\n     , settings(settings_)\n-    , metadata_snapshot(metadata_snapshot_)\n+    , storage_snapshot(storage_snapshot_)\n     , all_mark_ranges(all_mark_ranges_)\n     , alter_conversions(data_part_info_for_read->getAlterConversions())\n     /// For wide parts convert plain arrays of Nested to subcolumns\n@@ -71,7 +71,7 @@ void IMergeTreeReader::fillMissingColumns(Columns & res_columns, bool & should_e\n             res_columns, num_rows,\n             Nested::convertToSubcolumns(requested_columns),\n             Nested::convertToSubcolumns(available_columns),\n-            partially_read_columns, metadata_snapshot);\n+            partially_read_columns, storage_snapshot->metadata);\n \n         should_evaluate_missing_defaults = std::any_of(\n             res_columns.begin(), res_columns.end(), [](const auto & column) { return column == nullptr; });\n@@ -110,7 +110,10 @@ void IMergeTreeReader::evaluateMissingDefaults(Block additional_columns, Columns\n         }\n \n         auto dag = DB::evaluateMissingDefaults(\n-                additional_columns, requested_columns, metadata_snapshot->getColumns(), data_part_info_for_read->getContext());\n+            additional_columns, requested_columns,\n+            storage_snapshot->metadata->getColumns(),\n+            data_part_info_for_read->getContext());\n+\n         if (dag)\n         {\n             dag->addMaterializingOutputActions();\n@@ -216,7 +219,7 @@ void IMergeTreeReader::performRequiredConversions(Columns & res_columns) const\n     }\n }\n \n-IMergeTreeReader::ColumnPositionLevel IMergeTreeReader::findColumnForOffsets(const NameAndTypePair & required_column) const\n+IMergeTreeReader::ColumnNameLevel IMergeTreeReader::findColumnForOffsets(const NameAndTypePair & required_column) const\n {\n     auto get_offsets_streams = [](const auto & serialization, const auto & name_in_storage)\n     {\n@@ -238,11 +241,11 @@ IMergeTreeReader::ColumnPositionLevel IMergeTreeReader::findColumnForOffsets(con\n     auto required_offsets_streams = get_offsets_streams(getSerializationInPart(required_column), required_name_in_storage);\n \n     size_t max_matched_streams = 0;\n-    ColumnPositionLevel position_level;\n+    ColumnNameLevel name_level;\n \n     /// Find column that has maximal number of matching\n     /// offsets columns with required_column.\n-    for (const auto & part_column : data_part_info_for_read->getColumns())\n+    for (const auto & part_column : Nested::convertToSubcolumns(data_part_info_for_read->getColumns()))\n     {\n         auto name_in_storage = Nested::extractTableName(part_column.name);\n         if (name_in_storage != required_name_in_storage)\n@@ -261,14 +264,14 @@ IMergeTreeReader::ColumnPositionLevel IMergeTreeReader::findColumnForOffsets(con\n             it = current_it;\n         }\n \n-        if (i && (!position_level || i > max_matched_streams))\n+        if (i && (!name_level || i > max_matched_streams))\n         {\n             max_matched_streams = i;\n-            position_level.emplace(*data_part_info_for_read->getColumnPosition(part_column.name), it->second);\n+            name_level.emplace(part_column.name, it->second);\n         }\n     }\n \n-    return position_level;\n+    return name_level;\n }\n \n void IMergeTreeReader::checkNumberOfColumns(size_t num_columns_to_read) const\ndiff --git a/src/Storages/MergeTree/IMergeTreeReader.h b/src/Storages/MergeTree/IMergeTreeReader.h\nindex fcab35fb4c20..dafe7b2d4ba5 100644\n--- a/src/Storages/MergeTree/IMergeTreeReader.h\n+++ b/src/Storages/MergeTree/IMergeTreeReader.h\n@@ -24,7 +24,7 @@ class IMergeTreeReader : private boost::noncopyable\n     IMergeTreeReader(\n         MergeTreeDataPartInfoForReaderPtr data_part_info_for_read_,\n         const NamesAndTypesList & columns_,\n-        const StorageMetadataPtr & metadata_snapshot_,\n+        const StorageSnapshotPtr & storage_snapshot_,\n         UncompressedCache * uncompressed_cache_,\n         MarkCache * mark_cache_,\n         const MarkRanges & all_mark_ranges_,\n@@ -92,22 +92,23 @@ class IMergeTreeReader : private boost::noncopyable\n \n     MergeTreeReaderSettings settings;\n \n-    StorageMetadataPtr metadata_snapshot;\n+    StorageSnapshotPtr storage_snapshot;\n     MarkRanges all_mark_ranges;\n \n     /// Position and level (of nesting).\n-    using ColumnPositionLevel = std::optional<std::pair<size_t, size_t>>;\n+    using ColumnNameLevel = std::optional<std::pair<String, size_t>>;\n+\n     /// In case of part of the nested column does not exists, offsets should be\n     /// read, but only the offsets for the current column, that is why it\n     /// returns pair of size_t, not just one.\n-    ColumnPositionLevel findColumnForOffsets(const NameAndTypePair & column) const;\n+    ColumnNameLevel findColumnForOffsets(const NameAndTypePair & column) const;\n \n     NameSet partially_read_columns;\n \n-private:\n     /// Alter conversions, which must be applied on fly if required\n     AlterConversionsPtr alter_conversions;\n \n+private:\n     /// Columns that are requested to read.\n     NamesAndTypesList requested_columns;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp\nindex 3eba9a9de24a..d7836ac01b0a 100644\n--- a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp\n@@ -191,7 +191,6 @@ ChunkAndProgress IMergeTreeSelectAlgorithm::read()\n }\n \n void IMergeTreeSelectAlgorithm::initializeMergeTreeReadersForCurrentTask(\n-    const StorageMetadataPtr & metadata_snapshot,\n     const IMergeTreeReader::ValueSizeMap & value_size_map,\n     const ReadBufferFromFileBase::ProfileCallback & profile_callback)\n {\n@@ -206,7 +205,7 @@ void IMergeTreeSelectAlgorithm::initializeMergeTreeReadersForCurrentTask(\n     else\n     {\n         reader = task->data_part->getReader(\n-            task->task_columns.columns, metadata_snapshot, task->mark_ranges,\n+            task->task_columns.columns, storage_snapshot, task->mark_ranges,\n             owned_uncompressed_cache.get(), owned_mark_cache.get(),\n             task->alter_conversions, reader_settings, value_size_map, profile_callback);\n     }\n@@ -222,8 +221,8 @@ void IMergeTreeSelectAlgorithm::initializeMergeTreeReadersForCurrentTask(\n     {\n         initializeMergeTreePreReadersForPart(\n             task->data_part, task->alter_conversions,\n-            task->task_columns, metadata_snapshot,\n-            task->mark_ranges, value_size_map, profile_callback);\n+            task->task_columns, task->mark_ranges,\n+            value_size_map, profile_callback);\n     }\n }\n \n@@ -231,18 +230,17 @@ void IMergeTreeSelectAlgorithm::initializeMergeTreeReadersForPart(\n     const MergeTreeData::DataPartPtr & data_part,\n     const AlterConversionsPtr & alter_conversions,\n     const MergeTreeReadTaskColumns & task_columns,\n-    const StorageMetadataPtr & metadata_snapshot,\n     const MarkRanges & mark_ranges,\n     const IMergeTreeReader::ValueSizeMap & value_size_map,\n     const ReadBufferFromFileBase::ProfileCallback & profile_callback)\n {\n     reader = data_part->getReader(\n-        task_columns.columns, metadata_snapshot, mark_ranges,\n+        task_columns.columns, storage_snapshot, mark_ranges,\n         owned_uncompressed_cache.get(), owned_mark_cache.get(),\n         alter_conversions, reader_settings, value_size_map, profile_callback);\n \n     initializeMergeTreePreReadersForPart(\n-        data_part, alter_conversions, task_columns, metadata_snapshot,\n+        data_part, alter_conversions, task_columns,\n         mark_ranges, value_size_map, profile_callback);\n }\n \n@@ -250,7 +248,6 @@ void IMergeTreeSelectAlgorithm::initializeMergeTreePreReadersForPart(\n     const MergeTreeData::DataPartPtr & data_part,\n     const AlterConversionsPtr & alter_conversions,\n     const MergeTreeReadTaskColumns & task_columns,\n-    const StorageMetadataPtr & metadata_snapshot,\n     const MarkRanges & mark_ranges,\n     const IMergeTreeReader::ValueSizeMap & value_size_map,\n     const ReadBufferFromFileBase::ProfileCallback & profile_callback)\n@@ -262,7 +259,7 @@ void IMergeTreeSelectAlgorithm::initializeMergeTreePreReadersForPart(\n     {\n         pre_reader_for_step.push_back(\n             data_part->getReader(\n-                {LightweightDeleteDescription::FILTER_COLUMN}, metadata_snapshot,\n+                {LightweightDeleteDescription::FILTER_COLUMN}, storage_snapshot,\n                 mark_ranges, owned_uncompressed_cache.get(), owned_mark_cache.get(),\n                 alter_conversions, reader_settings, value_size_map, profile_callback));\n     }\n@@ -271,7 +268,7 @@ void IMergeTreeSelectAlgorithm::initializeMergeTreePreReadersForPart(\n     {\n         pre_reader_for_step.push_back(\n             data_part->getReader(\n-                pre_columns_per_step, metadata_snapshot, mark_ranges,\n+                pre_columns_per_step, storage_snapshot, mark_ranges,\n                 owned_uncompressed_cache.get(), owned_mark_cache.get(),\n                 alter_conversions, reader_settings, value_size_map, profile_callback));\n     }\ndiff --git a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h\nindex 7b6dc50060ac..42043c03e854 100644\n--- a/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h\n+++ b/src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h\n@@ -120,7 +120,6 @@ class IMergeTreeSelectAlgorithm\n \n     /// Sets up data readers for each step of prewhere and where\n     void initializeMergeTreeReadersForCurrentTask(\n-        const StorageMetadataPtr & metadata_snapshot,\n         const IMergeTreeReader::ValueSizeMap & value_size_map,\n         const ReadBufferFromFileBase::ProfileCallback & profile_callback);\n \n@@ -128,7 +127,6 @@ class IMergeTreeSelectAlgorithm\n         const MergeTreeData::DataPartPtr & data_part,\n         const AlterConversionsPtr & alter_conversions,\n         const MergeTreeReadTaskColumns & task_columns,\n-        const StorageMetadataPtr & metadata_snapshot,\n         const MarkRanges & mark_ranges,\n         const IMergeTreeReader::ValueSizeMap & value_size_map,\n         const ReadBufferFromFileBase::ProfileCallback & profile_callback);\n@@ -207,7 +205,6 @@ class IMergeTreeSelectAlgorithm\n         const MergeTreeData::DataPartPtr & data_part,\n         const AlterConversionsPtr & alter_conversions,\n         const MergeTreeReadTaskColumns & task_columns,\n-        const StorageMetadataPtr & metadata_snapshot,\n         const MarkRanges & mark_ranges,\n         const IMergeTreeReader::ValueSizeMap & value_size_map,\n         const ReadBufferFromFileBase::ProfileCallback & profile_callback);\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp\nindex 9c47608e3647..fc8bfcc925a7 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp\n@@ -30,7 +30,7 @@ MergeTreeDataPartCompact::MergeTreeDataPartCompact(\n \n IMergeTreeDataPart::MergeTreeReaderPtr MergeTreeDataPartCompact::getReader(\n     const NamesAndTypesList & columns_to_read,\n-    const StorageMetadataPtr & metadata_snapshot,\n+    const StorageSnapshotPtr & storage_snapshot,\n     const MarkRanges & mark_ranges,\n     UncompressedCache * uncompressed_cache,\n     MarkCache * mark_cache,\n@@ -43,7 +43,7 @@ IMergeTreeDataPart::MergeTreeReaderPtr MergeTreeDataPartCompact::getReader(\n     auto * load_marks_threadpool = reader_settings.read_settings.load_marks_asynchronously ? &read_info->getContext()->getLoadMarksThreadpool() : nullptr;\n \n     return std::make_unique<MergeTreeReaderCompact>(\n-        read_info, columns_to_read, metadata_snapshot, uncompressed_cache,\n+        read_info, columns_to_read, storage_snapshot, uncompressed_cache,\n         mark_cache, mark_ranges, reader_settings, load_marks_threadpool,\n         avg_value_size_hints, profile_callback);\n }\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartCompact.h b/src/Storages/MergeTree/MergeTreeDataPartCompact.h\nindex 08764eedb434..2bbac766c8ef 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartCompact.h\n@@ -30,7 +30,7 @@ class MergeTreeDataPartCompact : public IMergeTreeDataPart\n \n     MergeTreeReaderPtr getReader(\n         const NamesAndTypesList & columns,\n-        const StorageMetadataPtr & metadata_snapshot,\n+        const StorageSnapshotPtr & storage_snapshot,\n         const MarkRanges & mark_ranges,\n         UncompressedCache * uncompressed_cache,\n         MarkCache * mark_cache,\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\nindex 468747a6c36c..ba300b110d78 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\n@@ -32,7 +32,7 @@ MergeTreeDataPartInMemory::MergeTreeDataPartInMemory(\n \n IMergeTreeDataPart::MergeTreeReaderPtr MergeTreeDataPartInMemory::getReader(\n     const NamesAndTypesList & columns_to_read,\n-    const StorageMetadataPtr & metadata_snapshot,\n+    const StorageSnapshotPtr & storage_snapshot,\n     const MarkRanges & mark_ranges,\n     UncompressedCache * /* uncompressed_cache */,\n     MarkCache * /* mark_cache */,\n@@ -45,7 +45,7 @@ IMergeTreeDataPart::MergeTreeReaderPtr MergeTreeDataPartInMemory::getReader(\n     auto ptr = std::static_pointer_cast<const MergeTreeDataPartInMemory>(shared_from_this());\n \n     return std::make_unique<MergeTreeReaderInMemory>(\n-        read_info, ptr, columns_to_read, metadata_snapshot, mark_ranges, reader_settings);\n+        read_info, ptr, columns_to_read, storage_snapshot, mark_ranges, reader_settings);\n }\n \n IMergeTreeDataPart::MergeTreeWriterPtr MergeTreeDataPartInMemory::getWriter(\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartInMemory.h b/src/Storages/MergeTree/MergeTreeDataPartInMemory.h\nindex 2698b69b38ea..81549eeed3ee 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartInMemory.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartInMemory.h\n@@ -19,7 +19,7 @@ class MergeTreeDataPartInMemory : public IMergeTreeDataPart\n \n     MergeTreeReaderPtr getReader(\n         const NamesAndTypesList & columns,\n-        const StorageMetadataPtr & metadata_snapshot,\n+        const StorageSnapshotPtr & storage_snapshot,\n         const MarkRanges & mark_ranges,\n         UncompressedCache * uncompressed_cache,\n         MarkCache * mark_cache,\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\nindex 2d886e2058bb..f62582d48ccc 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n@@ -29,7 +29,7 @@ MergeTreeDataPartWide::MergeTreeDataPartWide(\n \n IMergeTreeDataPart::MergeTreeReaderPtr MergeTreeDataPartWide::getReader(\n     const NamesAndTypesList & columns_to_read,\n-    const StorageMetadataPtr & metadata_snapshot,\n+    const StorageSnapshotPtr & storage_snapshot,\n     const MarkRanges & mark_ranges,\n     UncompressedCache * uncompressed_cache,\n     MarkCache * mark_cache,\n@@ -41,7 +41,7 @@ IMergeTreeDataPart::MergeTreeReaderPtr MergeTreeDataPartWide::getReader(\n     auto read_info = std::make_shared<LoadedMergeTreeDataPartInfoForReader>(shared_from_this(), alter_conversions);\n     return std::make_unique<MergeTreeReaderWide>(\n         read_info, columns_to_read,\n-        metadata_snapshot, uncompressed_cache,\n+        storage_snapshot, uncompressed_cache,\n         mark_cache, mark_ranges, reader_settings,\n         avg_value_size_hints, profile_callback);\n }\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.h b/src/Storages/MergeTree/MergeTreeDataPartWide.h\nindex 0b2ffeb4b181..2076a1ec0282 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWide.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.h\n@@ -25,7 +25,7 @@ class MergeTreeDataPartWide : public IMergeTreeDataPart\n \n     MergeTreeReaderPtr getReader(\n         const NamesAndTypesList & columns,\n-        const StorageMetadataPtr & metadata_snapshot,\n+        const StorageSnapshotPtr & storage_snapshot,\n         const MarkRanges & mark_ranges,\n         UncompressedCache * uncompressed_cache,\n         MarkCache * mark_cache,\ndiff --git a/src/Storages/MergeTree/MergeTreePrefetchedReadPool.cpp b/src/Storages/MergeTree/MergeTreePrefetchedReadPool.cpp\nindex e9e2138d9955..901801d8187b 100644\n--- a/src/Storages/MergeTree/MergeTreePrefetchedReadPool.cpp\n+++ b/src/Storages/MergeTree/MergeTreePrefetchedReadPool.cpp\n@@ -97,7 +97,7 @@ std::future<MergeTreeReaderPtr> MergeTreePrefetchedReadPool::createPrefetchedRea\n     Priority priority) const\n {\n     auto reader = data_part.getReader(\n-        columns, storage_snapshot->metadata, required_ranges,\n+        columns, storage_snapshot, required_ranges,\n         uncompressed_cache, mark_cache, alter_conversions, reader_settings,\n         IMergeTreeReader::ValueSizeMap{}, profile_callback);\n \ndiff --git a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp\nindex f65e66ff52d4..feefca68e7b2 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp\n@@ -17,7 +17,7 @@ namespace ErrorCodes\n MergeTreeReaderCompact::MergeTreeReaderCompact(\n     MergeTreeDataPartInfoForReaderPtr data_part_info_for_read_,\n     NamesAndTypesList columns_,\n-    const StorageMetadataPtr & metadata_snapshot_,\n+    const StorageSnapshotPtr & storage_snapshot_,\n     UncompressedCache * uncompressed_cache_,\n     MarkCache * mark_cache_,\n     MarkRanges mark_ranges_,\n@@ -29,7 +29,7 @@ MergeTreeReaderCompact::MergeTreeReaderCompact(\n     : IMergeTreeReader(\n         data_part_info_for_read_,\n         columns_,\n-        metadata_snapshot_,\n+        storage_snapshot_,\n         uncompressed_cache_,\n         mark_cache_,\n         mark_ranges_,\n@@ -130,7 +130,7 @@ void MergeTreeReaderCompact::fillColumnPositions()\n     size_t columns_num = columns_to_read.size();\n \n     column_positions.resize(columns_num);\n-    read_only_offsets.resize(columns_num);\n+    columns_for_offsets.resize(columns_num);\n \n     for (size_t i = 0; i < columns_num; ++i)\n     {\n@@ -149,20 +149,48 @@ void MergeTreeReaderCompact::fillColumnPositions()\n                 position.reset();\n         }\n \n+        /// If array of Nested column is missing in part,\n+        /// we have to read its offsets if they exist.\n         if (!position && is_array)\n         {\n-            /// If array of Nested column is missing in part,\n-            /// we have to read its offsets if they exist.\n-            auto position_level = findColumnForOffsets(column_to_read);\n-            if (position_level.has_value())\n+            NameAndTypePair column_to_read_with_subcolumns = column_to_read;\n+            auto [name_in_storage, subcolumn_name] = Nested::splitName(column_to_read.name);\n+\n+            /// If it is a part of Nested, we need to get the column from\n+            /// storage metatadata which is converted to Nested type with subcolumns.\n+            /// It is required for proper counting of shared streams.\n+            if (!subcolumn_name.empty())\n+            {\n+                /// If column is renamed get the new name from storage metadata.\n+                if (alter_conversions->columnHasNewName(name_in_storage))\n+                    name_in_storage = alter_conversions->getColumnNewName(name_in_storage);\n+\n+                if (!storage_columns_with_collected_nested)\n+                {\n+                    auto options = GetColumnsOptions(GetColumnsOptions::AllPhysical).withExtendedObjects();\n+                    auto storage_columns_list = Nested::collect(storage_snapshot->getColumns(options));\n+                    storage_columns_with_collected_nested = ColumnsDescription(std::move(storage_columns_list));\n+                }\n+\n+                column_to_read_with_subcolumns = storage_columns_with_collected_nested\n+                    ->getColumnOrSubcolumn(\n+                        GetColumnsOptions::All,\n+                        Nested::concatenateName(name_in_storage, subcolumn_name));\n+            }\n+\n+            auto name_level_for_offsets = findColumnForOffsets(column_to_read_with_subcolumns);\n+\n+            if (name_level_for_offsets.has_value())\n             {\n-                column_positions[i].emplace(position_level->first);\n-                read_only_offsets[i].emplace(position_level->second);\n+                column_positions[i] = data_part_info_for_read->getColumnPosition(name_level_for_offsets->first);\n+                columns_for_offsets[i] = name_level_for_offsets;\n                 partially_read_columns.insert(column_to_read.name);\n             }\n         }\n         else\n+        {\n             column_positions[i] = std::move(position);\n+        }\n     }\n }\n \n@@ -203,7 +231,7 @@ size_t MergeTreeReaderCompact::readRows(\n                 auto & column = res_columns[pos];\n                 size_t column_size_before_reading = column->size();\n \n-                readData(columns_to_read[pos], column, from_mark, current_task_last_mark, *column_positions[pos], rows_to_read, read_only_offsets[pos]);\n+                readData(columns_to_read[pos], column, from_mark, current_task_last_mark, *column_positions[pos], rows_to_read, columns_for_offsets[pos]);\n \n                 size_t read_rows_in_column = column->size() - column_size_before_reading;\n                 if (read_rows_in_column != rows_to_read)\n@@ -239,23 +267,37 @@ size_t MergeTreeReaderCompact::readRows(\n void MergeTreeReaderCompact::readData(\n     const NameAndTypePair & name_and_type, ColumnPtr & column,\n     size_t from_mark, size_t current_task_last_mark, size_t column_position, size_t rows_to_read,\n-    std::optional<size_t> only_offsets_level)\n+    ColumnNameLevel name_level_for_offsets)\n {\n     const auto & [name, type] = name_and_type;\n+    std::optional<NameAndTypePair> column_for_offsets;\n+\n+    if (name_level_for_offsets.has_value())\n+    {\n+        const auto & part_columns = data_part_info_for_read->getColumnsDescription();\n+        column_for_offsets = part_columns.getPhysical(name_level_for_offsets->first);\n+    }\n \n     adjustUpperBound(current_task_last_mark); /// Must go before seek.\n \n     if (!isContinuousReading(from_mark, column_position))\n         seekToMark(from_mark, column_position);\n \n+    /// If we read only offsets we have to read prefix anyway\n+    /// to preserve correctness of serialization.\n+    auto buffer_getter_for_prefix = [&](const auto &) -> ReadBuffer *\n+    {\n+        return data_buffer;\n+    };\n+\n     auto buffer_getter = [&](const ISerialization::SubstreamPath & substream_path) -> ReadBuffer *\n     {\n         /// Offset stream from another column could be read, in case of current\n         /// column does not exists (see findColumnForOffsets() in\n         /// MergeTreeReaderCompact::fillColumnPositions())\n-        bool is_offsets = !substream_path.empty() && substream_path.back().type == ISerialization::Substream::ArraySizes;\n-        if (only_offsets_level.has_value())\n+        if (name_level_for_offsets.has_value())\n         {\n+            bool is_offsets = !substream_path.empty() && substream_path.back().type == ISerialization::Substream::ArraySizes;\n             if (!is_offsets)\n                 return nullptr;\n \n@@ -275,7 +317,7 @@ void MergeTreeReaderCompact::readData(\n             ///\n             /// Here only_offsets_level is the level of the alternative stream,\n             /// and substream_path.size() is the level of the current stream.\n-            if (only_offsets_level.value() < ISerialization::getArrayLevel(substream_path))\n+            if (name_level_for_offsets->second < ISerialization::getArrayLevel(substream_path))\n                 return nullptr;\n         }\n \n@@ -283,22 +325,32 @@ void MergeTreeReaderCompact::readData(\n     };\n \n     ISerialization::DeserializeBinaryBulkStatePtr state;\n+    ISerialization::DeserializeBinaryBulkStatePtr state_for_prefix;\n+\n     ISerialization::DeserializeBinaryBulkSettings deserialize_settings;\n-    deserialize_settings.getter = buffer_getter;\n     deserialize_settings.avg_value_size_hint = avg_value_size_hints[name];\n \n     if (name_and_type.isSubcolumn())\n     {\n-        const auto & type_in_storage = name_and_type.getTypeInStorage();\n-        const auto & name_in_storage = name_and_type.getNameInStorage();\n+        NameAndTypePair name_type_in_storage{name_and_type.getNameInStorage(), name_and_type.getTypeInStorage()};\n \n-        auto serialization = getSerializationInPart({name_in_storage, type_in_storage});\n-        ColumnPtr temp_column = type_in_storage->createColumn(*serialization);\n+        /// In case of reading onlys offset use the correct serialization for reading of the prefix\n+        auto serialization = getSerializationInPart(name_type_in_storage);\n+        ColumnPtr temp_column = name_type_in_storage.type->createColumn(*serialization);\n \n+        if (column_for_offsets)\n+        {\n+            auto serialization_for_prefix = getSerializationInPart(*column_for_offsets);\n+\n+            deserialize_settings.getter = buffer_getter_for_prefix;\n+            serialization_for_prefix->deserializeBinaryBulkStatePrefix(deserialize_settings, state_for_prefix);\n+        }\n+\n+        deserialize_settings.getter = buffer_getter;\n         serialization->deserializeBinaryBulkStatePrefix(deserialize_settings, state);\n         serialization->deserializeBinaryBulkWithMultipleStreams(temp_column, rows_to_read, deserialize_settings, state, nullptr);\n \n-        auto subcolumn = type_in_storage->getSubcolumn(name_and_type.getSubcolumnName(), temp_column);\n+        auto subcolumn = name_type_in_storage.type->getSubcolumn(name_and_type.getSubcolumnName(), temp_column);\n \n         /// TODO: Avoid extra copying.\n         if (column->empty())\n@@ -308,13 +360,24 @@ void MergeTreeReaderCompact::readData(\n     }\n     else\n     {\n+        /// In case of reading only offsets use the correct serialization for reading the prefix\n         auto serialization = getSerializationInPart(name_and_type);\n+\n+        if (column_for_offsets)\n+        {\n+            auto serialization_for_prefix = getSerializationInPart(*column_for_offsets);\n+\n+            deserialize_settings.getter = buffer_getter_for_prefix;\n+            serialization_for_prefix->deserializeBinaryBulkStatePrefix(deserialize_settings, state_for_prefix);\n+        }\n+\n+        deserialize_settings.getter = buffer_getter;\n         serialization->deserializeBinaryBulkStatePrefix(deserialize_settings, state);\n         serialization->deserializeBinaryBulkWithMultipleStreams(column, rows_to_read, deserialize_settings, state, nullptr);\n     }\n \n     /// The buffer is left in inconsistent state after reading single offsets\n-    if (only_offsets_level.has_value())\n+    if (name_level_for_offsets.has_value())\n         last_read_granule.reset();\n     else\n         last_read_granule.emplace(from_mark, column_position);\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderCompact.h b/src/Storages/MergeTree/MergeTreeReaderCompact.h\nindex f180d7508f76..cf7065263632 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeReaderCompact.h\n@@ -21,7 +21,7 @@ class MergeTreeReaderCompact : public IMergeTreeReader\n     MergeTreeReaderCompact(\n         MergeTreeDataPartInfoForReaderPtr data_part_info_for_read_,\n         NamesAndTypesList columns_,\n-        const StorageMetadataPtr & metadata_snapshot_,\n+        const StorageSnapshotPtr & storage_snapshot_,\n         UncompressedCache * uncompressed_cache_,\n         MarkCache * mark_cache_,\n         MarkRanges mark_ranges_,\n@@ -52,12 +52,19 @@ class MergeTreeReaderCompact : public IMergeTreeReader\n \n     MergeTreeMarksLoader marks_loader;\n \n+    /// Storage columns with collected separate arrays of Nested to columns of Nested type.\n+    /// They maybe be needed for finding offsets of missed Nested columns in parts.\n+    /// They are rarely used and are heavy to initialized, so we create them\n+    /// only on demand and cache in this field.\n+    std::optional<ColumnsDescription> storage_columns_with_collected_nested;\n+\n     /// Positions of columns in part structure.\n     using ColumnPositions = std::vector<std::optional<size_t>>;\n     ColumnPositions column_positions;\n+\n     /// Should we read full column or only it's offsets.\n     /// Element of the vector is the level of the alternative stream.\n-    std::vector<std::optional<size_t>> read_only_offsets;\n+    std::vector<ColumnNameLevel> columns_for_offsets;\n \n     /// For asynchronous reading from remote fs. Same meaning as in MergeTreeReaderStream.\n     std::optional<size_t> last_right_offset;\n@@ -68,8 +75,8 @@ class MergeTreeReaderCompact : public IMergeTreeReader\n     void seekToMark(size_t row_index, size_t column_index);\n \n     void readData(const NameAndTypePair & name_and_type, ColumnPtr & column, size_t from_mark,\n-        size_t current_task_last_mark, size_t column_position, size_t rows_to_read,\n-        std::optional<size_t> only_offsets_level);\n+        size_t current_task_last_mark, size_t column_position,\n+        size_t rows_to_read, ColumnNameLevel name_level_for_offsets);\n \n     /// Returns maximal value of granule size in compressed file from @mark_ranges.\n     /// This value is used as size of read buffer.\n@@ -84,7 +91,6 @@ class MergeTreeReaderCompact : public IMergeTreeReader\n \n     ReadBufferFromFileBase::ProfileCallback profile_callback;\n     clockid_t clock_type;\n-\n     bool initialized = false;\n };\n \ndiff --git a/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp b/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp\nindex fed8032fb17a..bacd86511f5b 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp\n@@ -19,13 +19,13 @@ MergeTreeReaderInMemory::MergeTreeReaderInMemory(\n     MergeTreeDataPartInfoForReaderPtr data_part_info_for_read_,\n     DataPartInMemoryPtr data_part_,\n     NamesAndTypesList columns_,\n-    const StorageMetadataPtr & metadata_snapshot_,\n+    const StorageSnapshotPtr & storage_snapshot_,\n     MarkRanges mark_ranges_,\n     MergeTreeReaderSettings settings_)\n     : IMergeTreeReader(\n         data_part_info_for_read_,\n         columns_,\n-        metadata_snapshot_,\n+        storage_snapshot_,\n         nullptr,\n         nullptr,\n         mark_ranges_,\n@@ -42,7 +42,7 @@ MergeTreeReaderInMemory::MergeTreeReaderInMemory(\n         {\n             if (auto offsets_position = findColumnForOffsets(column_to_read))\n             {\n-                positions_for_offsets[column_to_read.name] = offsets_position->first;\n+                positions_for_offsets[column_to_read.name] = *data_part_info_for_read->getColumnPosition(offsets_position->first);\n                 partially_read_columns.insert(column_to_read.name);\n             }\n         }\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderInMemory.h b/src/Storages/MergeTree/MergeTreeReaderInMemory.h\nindex cb67bc46eaee..e26a98f0916d 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderInMemory.h\n+++ b/src/Storages/MergeTree/MergeTreeReaderInMemory.h\n@@ -18,7 +18,7 @@ class MergeTreeReaderInMemory : public IMergeTreeReader\n         MergeTreeDataPartInfoForReaderPtr data_part_info_for_read_,\n         DataPartInMemoryPtr data_part_,\n         NamesAndTypesList columns_,\n-        const StorageMetadataPtr & metadata_snapshot_,\n+        const StorageSnapshotPtr & storage_snapshot_,\n         MarkRanges mark_ranges_,\n         MergeTreeReaderSettings settings_);\n \ndiff --git a/src/Storages/MergeTree/MergeTreeReaderWide.cpp b/src/Storages/MergeTree/MergeTreeReaderWide.cpp\nindex 140fb6da5df0..a0fe2dc63b4f 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReaderWide.cpp\n@@ -24,7 +24,7 @@ namespace\n MergeTreeReaderWide::MergeTreeReaderWide(\n     MergeTreeDataPartInfoForReaderPtr data_part_info_,\n     NamesAndTypesList columns_,\n-    const StorageMetadataPtr & metadata_snapshot_,\n+    const StorageSnapshotPtr & storage_snapshot_,\n     UncompressedCache * uncompressed_cache_,\n     MarkCache * mark_cache_,\n     MarkRanges mark_ranges_,\n@@ -35,7 +35,7 @@ MergeTreeReaderWide::MergeTreeReaderWide(\n     : IMergeTreeReader(\n         data_part_info_,\n         columns_,\n-        metadata_snapshot_,\n+        storage_snapshot_,\n         uncompressed_cache_,\n         mark_cache_,\n         mark_ranges_,\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderWide.h b/src/Storages/MergeTree/MergeTreeReaderWide.h\nindex c31b1baf32e1..2a850cc2814e 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderWide.h\n+++ b/src/Storages/MergeTree/MergeTreeReaderWide.h\n@@ -17,7 +17,7 @@ class MergeTreeReaderWide : public IMergeTreeReader\n     MergeTreeReaderWide(\n         MergeTreeDataPartInfoForReaderPtr data_part_info_for_read_,\n         NamesAndTypesList columns_,\n-        const StorageMetadataPtr & metadata_snapshot_,\n+        const StorageSnapshotPtr & storage_snapshot_,\n         UncompressedCache * uncompressed_cache_,\n         MarkCache * mark_cache_,\n         MarkRanges mark_ranges_,\ndiff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\nindex ce4ba69c08c2..e5a45ad55546 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n@@ -65,7 +65,7 @@ void MergeTreeSelectAlgorithm::initializeReaders()\n \n     initializeMergeTreeReadersForPart(\n         data_part, alter_conversions, task_columns,\n-        storage_snapshot->getMetadataForQuery(), all_mark_ranges, {}, {});\n+        all_mark_ranges, /*value_size_map=*/ {}, /*profile_callback=*/ {});\n }\n \n \ndiff --git a/src/Storages/MergeTree/MergeTreeSequentialSource.cpp b/src/Storages/MergeTree/MergeTreeSequentialSource.cpp\nindex 5a6d59bf0be9..88f6eaaf49fc 100644\n--- a/src/Storages/MergeTree/MergeTreeSequentialSource.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSequentialSource.cpp\n@@ -150,7 +150,7 @@ MergeTreeSequentialSource::MergeTreeSequentialSource(\n         mark_ranges.emplace(MarkRanges{MarkRange(0, data_part->getMarksCount())});\n \n     reader = data_part->getReader(\n-        columns_for_reader, storage_snapshot->metadata,\n+        columns_for_reader, storage_snapshot,\n         *mark_ranges, /* uncompressed_cache = */ nullptr,\n         mark_cache.get(), alter_conversions, reader_settings, {}, {});\n }\ndiff --git a/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.cpp\nindex 01094d65ac51..892ae9ead879 100644\n--- a/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeThreadSelectProcessor.cpp\n@@ -45,8 +45,6 @@ void MergeTreeThreadSelectAlgorithm::finalizeNewTask()\n \n     /// Allows pool to reduce number of threads in case of too slow reads.\n     auto profile_callback = [this](ReadBufferFromFileBase::ProfileInfo info_) { pool->profileFeedback(info_); };\n-    const auto & metadata_snapshot = storage_snapshot->metadata;\n-\n     IMergeTreeReader::ValueSizeMap value_size_map;\n \n     if (reader && part_name != last_read_part_name)\n@@ -57,7 +55,7 @@ void MergeTreeThreadSelectAlgorithm::finalizeNewTask()\n     /// task->reader.valid() means there is a prefetched reader in this test, use it.\n     const bool init_new_readers = !reader || task->reader.valid() || part_name != last_read_part_name;\n     if (init_new_readers)\n-        initializeMergeTreeReadersForCurrentTask(metadata_snapshot, value_size_map, profile_callback);\n+        initializeMergeTreeReadersForCurrentTask(value_size_map, profile_callback);\n \n     last_read_part_name = part_name;\n }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02835_nested_array_lowcardinality.reference b/tests/queries/0_stateless/02835_nested_array_lowcardinality.reference\nnew file mode 100644\nindex 000000000000..c2936da0b4f5\n--- /dev/null\n+++ b/tests/queries/0_stateless/02835_nested_array_lowcardinality.reference\n@@ -0,0 +1,30 @@\n+[]\t[]\n+['0']\t['']\n+['0','1']\t['','']\n+['0','1','2']\t['','','']\n+['0','1','2','3']\t['','','','']\n+['0','1','2','3','4']\t['','','','','']\n+['0','1','2','3','4','5']\t['','','','','','']\n+['0','1','2','3','4','5','6']\t['','','','','','','']\n+['0','1','2','3','4','5','6','7']\t['','','','','','','','']\n+['0','1','2','3','4','5','6','7','8']\t['','','','','','','','','']\n+[]\t[]\n+[[]]\t[[]]\n+[[],['0']]\t[[],[]]\n+[[],['0'],['0','1']]\t[[],[],[]]\n+[[],['0'],['0','1'],['0','1','2']]\t[[],[],[],[]]\n+[[],['0'],['0','1'],['0','1','2'],[]]\t[[],[],[],[],[]]\n+[[],['0'],['0','1'],['0','1','2'],[],['0']]\t[[],[],[],[],[],[]]\n+[[],['0'],['0','1'],['0','1','2'],[],['0'],['0','1']]\t[[],[],[],[],[],[],[]]\n+[[],['0'],['0','1'],['0','1','2'],[],['0'],['0','1'],['0','1','2']]\t[[],[],[],[],[],[],[],[]]\n+[[],['0'],['0','1'],['0','1','2'],[],['0'],['0','1'],['0','1','2'],[]]\t[[],[],[],[],[],[],[],[],[]]\n+[]\t[]\n+[{}]\t[{}]\n+[{},{'k0':0}]\t[{},{}]\n+[{},{'k0':0},{'k0':0,'k1':1}]\t[{},{},{}]\n+[{},{'k0':0},{'k0':0,'k1':1},{'k0':0,'k1':1,'k2':2}]\t[{},{},{},{}]\n+[{},{'k0':0},{'k0':0,'k1':1},{'k0':0,'k1':1,'k2':2},{}]\t[{},{},{},{},{}]\n+[{},{'k0':0},{'k0':0,'k1':1},{'k0':0,'k1':1,'k2':2},{},{'k0':0}]\t[{},{},{},{},{},{}]\n+[{},{'k0':0},{'k0':0,'k1':1},{'k0':0,'k1':1,'k2':2},{},{'k0':0},{'k0':0,'k1':1}]\t[{},{},{},{},{},{},{}]\n+[{},{'k0':0},{'k0':0,'k1':1},{'k0':0,'k1':1,'k2':2},{},{'k0':0},{'k0':0,'k1':1},{'k0':0,'k1':1,'k2':2}]\t[{},{},{},{},{},{},{},{}]\n+[{},{'k0':0},{'k0':0,'k1':1},{'k0':0,'k1':1,'k2':2},{},{'k0':0},{'k0':0,'k1':1},{'k0':0,'k1':1,'k2':2},{}]\t[{},{},{},{},{},{},{},{},{}]\ndiff --git a/tests/queries/0_stateless/02835_nested_array_lowcardinality.sql b/tests/queries/0_stateless/02835_nested_array_lowcardinality.sql\nnew file mode 100644\nindex 000000000000..36c1eb39cfd0\n--- /dev/null\n+++ b/tests/queries/0_stateless/02835_nested_array_lowcardinality.sql\n@@ -0,0 +1,49 @@\n+DROP TABLE IF EXISTS cool_table;\n+\n+CREATE TABLE IF NOT EXISTS cool_table\n+(\n+    id UInt64,\n+    n Nested(n UInt64, lc1 LowCardinality(String))\n+)\n+ENGINE = MergeTree\n+ORDER BY id;\n+\n+INSERT INTO cool_table SELECT number, range(number), range(number) FROM numbers(10);\n+\n+ALTER TABLE cool_table ADD COLUMN IF NOT EXISTS `n.lc2` Array(LowCardinality(String));\n+\n+SELECT n.lc1, n.lc2 FROM cool_table ORDER BY id;\n+\n+DROP TABLE IF EXISTS cool_table;\n+\n+CREATE TABLE IF NOT EXISTS cool_table\n+(\n+    id UInt64,\n+    n Nested(n UInt64, lc1 Array(LowCardinality(String)))\n+)\n+ENGINE = MergeTree\n+ORDER BY id;\n+\n+INSERT INTO cool_table SELECT number, range(number), arrayMap(x -> range(x % 4), range(number)) FROM numbers(10);\n+\n+ALTER TABLE cool_table ADD COLUMN IF NOT EXISTS `n.lc2` Array(Array(LowCardinality(String)));\n+\n+SELECT n.lc1, n.lc2 FROM cool_table ORDER BY id;\n+\n+DROP TABLE IF EXISTS cool_table;\n+\n+CREATE TABLE IF NOT EXISTS cool_table\n+(\n+    id UInt64,\n+    n Nested(n UInt64, lc1 Map(LowCardinality(String), UInt64))\n+)\n+ENGINE = MergeTree\n+ORDER BY id;\n+\n+INSERT INTO cool_table SELECT number, range(number), arrayMap(x -> (arrayMap(y -> 'k' || toString(y), range(x % 4)), range(x % 4))::Map(LowCardinality(String), UInt64), range(number)) FROM numbers(10);\n+\n+ALTER TABLE cool_table ADD COLUMN IF NOT EXISTS `n.lc2` Array(Map(LowCardinality(String), UInt64));\n+\n+SELECT n.lc1, n.lc2 FROM cool_table ORDER BY id;\n+\n+DROP TABLE IF EXISTS cool_table;\n",
  "problem_statement": "Nested column: Segfault after adding new column to existing ReplicatedMergeTree table\n**Describe what's wrong**\r\n\r\nAfter adding a new column to an existing, long-lived, and steadily written-to `ReplicatedMergeTree` table, we saw segfaults that seemed to occur in worker threads related to replication (or part merging?). \r\n\r\n> A link to reproducer in [https://fiddle.clickhouse.com/](https://fiddle.clickhouse.com/).\r\n\r\nTODO: This is not a crash experienced at query time. Is it possible to construct a fiddler environment to repro replication/merging related crashes like this? Open to all feedback!\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nWe are experiencing this on a LTS version of Clickhouse: `clickhouse-server:23.3.2.37`. We are also seeing this on the most recent patch version of that `23.3` release: `23.3.8.21`.\r\n\r\n**Enable crash reporting**\r\n\r\nTODO/TBD\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use\r\n\r\n`23.3`: `23.3.2.37` or `23.3.8.21` confirmed\r\n\r\n* Which interface to use, if matters\r\n\r\nN/A\r\n\r\n* Non-default settings, if any\r\n\r\nN/A \r\n\r\n* `CREATE TABLE` statements for all tables involved\r\n\r\nCreate a table with this shape (NB: nested column of a map type)\r\n```\r\nCREATE TABLE IF NOT EXISTS cool_table(\r\n \"nested_things\" Nested(\r\n   otherColumns...\r\n   \"string_map\" Map(LowCardinality(String), String),\r\n   otherColumns...\r\n ) \r\n) ReplicatedMergeTree(...)\r\n```\r\n\r\nWrite some data to this table, let it replicate/merge/etc, and then run an alter like this to add a new column (which you don't need to insert any values into). The intent of this DDL is to add a map from low cardinality strings to floats within the nested type that already exists on the table.\r\n```\r\nALTER TABLE cool_table\r\n  ADD COLUMN IF NOT EXISTS `nested_things.float_map` Array(Map(LowCardinality(String), Float64)) AFTER `string_map`;\r\n```\r\n\r\n* Sample data for all these tables, use [clickhouse-obfuscator](https://github.com/ClickHouse/ClickHouse/blob/master/programs/obfuscator/Obfuscator.cpp#L42-L80) if necessary\r\n\r\nTODO/TBD\r\n\r\n* Queries to run that lead to unexpected result\r\n\r\nNo queries needed, but you could consider one stream of inserts into the columns of the original table schema (not needing to start inserting values into the new column, for example).\r\n\r\n**Expected behavior**\r\n\r\nReplication/merging after additive DDLs (adding a column, and not even writing to it yet) should not crash the server process, even if there are existing file parts on-disk that predate the schema change.\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\n########################################\r\n2023-07-19 14:36:03.599 EDT (version 23.3.2.37 (official build), build id: 2F54F417C7E2A810B4069A69AC827CF045266F9E) (from thread 243) (query_id: 10d2e694-52db-49ed-812b-446ce4901294::78110_0_2675_6) (query: ) Received signal Segmentation fault (11)\r\n2023-07-19 14:36:03.599 EDT Address: 0x44e3380. Access: write. Attempted access has violated the permissions assigned to the memory area.\r\n2023-07-19 14:36:03.599 EDT Stack trace: 0x13c1c1c7 0x12b09f83 0x12b243c4 0x12b2498c 0x12b0a430 0x12b02c41 0x13c1a105 0x146852ea 0x1469414e 0x14a14f95 0x14a14b06 0x14a2e70a 0x14a2377b 0x14a22b68 0x14a33757 0x14a33913 0x144e93ef 0x144e934b 0x144ee3ba 0x1478614c 0x14785014 0x89d430c 0x89d3f8a 0xe2b5625 0xe2b8195 0xe2b13f3 0xe2b7061 0x7f87b006eb43 0x7f87b0100a00\r\n2023-07-19 14:36:03.599 EDT 2. ? @ 0x13c1c1c7 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 3. DB::SerializationArray::enumerateStreams(DB::ISerialization::EnumerateStreamsSettings&, std::function<void (DB::ISerialization::SubstreamPath const&)> const&, DB::ISerialization::SubstreamData const&) const @ 0x12b09f83 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 4. DB::SerializationMap::enumerateStreams(DB::ISerialization::EnumerateStreamsSettings&, std::function<void (DB::ISerialization::SubstreamPath const&)> const&, DB::ISerialization::SubstreamData const&) const @ 0x12b243c4 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 5. DB::SerializationNamed::enumerateStreams(DB::ISerialization::EnumerateStreamsSettings&, std::function<void (DB::ISerialization::SubstreamPath const&)> const&, DB::ISerialization::SubstreamData const&) const @ 0x12b2498c in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 6. DB::SerializationArray::enumerateStreams(DB::ISerialization::EnumerateStreamsSettings&, std::function<void (DB::ISerialization::SubstreamPath const&)> const&, DB::ISerialization::SubstreamData const&) const @ 0x12b0a430 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 7. DB::ISerialization::enumerateStreams(std::function<void (DB::ISerialization::SubstreamPath const&)> const&, std::shared_ptr<DB::IDataType const> const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&) const @ 0x12b02c41 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 8. DB::fillMissingColumns(std::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn>>>&, unsigned long, DB::NamesAndTypesList const&, DB::NamesAndTypesList const&, std::unordered_set<String, std::hash<String>, std::equal_to<String>, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const>) @ 0x13c1a105 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 9. DB::IMergeTreeReader::fillMissingColumns(std::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn>>>&, bool&, unsigned long) const @ 0x146852ea in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 10. DB::MergeTreeSequentialSource::generate() @ 0x1469414e in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 11. DB::ISource::tryGenerate() @ 0x14a14f95 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 12. DB::ISource::work() @ 0x14a14b06 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 13. DB::ExecutionThreadContext::executeTask() @ 0x14a2e70a in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 14. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x14a2377b in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 15. DB::PipelineExecutor::executeStep(std::atomic<bool>*) @ 0x14a22b68 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 16. DB::PullingPipelineExecutor::pull(DB::Chunk&) @ 0x14a33757 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 17. DB::PullingPipelineExecutor::pull(DB::Block&) @ 0x14a33913 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 18. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::executeImpl() @ 0x144e93ef in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 19. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::execute() @ 0x144e934b in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 20. DB::MergeTask::execute() @ 0x144ee3ba in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 21. DB::ReplicatedMergeMutateTaskBase::executeImpl() @ 0x1478614c in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 22. DB::ReplicatedMergeMutateTaskBase::executeStep() @ 0x14785014 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 23. DB::MergeTreeBackgroundExecutor<DB::DynamicRuntimeQueue>::routine(std::shared_ptr<DB::TaskRuntimeData>) @ 0x89d430c in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 24. DB::MergeTreeBackgroundExecutor<DB::DynamicRuntimeQueue>::threadFunction() @ 0x89d3f8a in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 25. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0xe2b5625 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 26. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, long, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0xe2b8195 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 27. ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0xe2b13f3 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 28. ? @ 0xe2b7061 in /usr/bin/clickhouse\r\n2023-07-19 14:36:03.599 EDT 29. ? @ 0x7f87b006eb43 in ?\r\n2023-07-19 14:36:03.599 EDT 30. ? @ 0x7f87b0100a00 in ?\r\n2023-07-19 14:36:03.748 EDT Integrity check of the executable successfully passed (checksum: 2391C2E4C30B9E637CFE4E448BCD472A)\r\n```\r\n\r\n**Additional context**\r\n\r\nThis segfault leads to a k8s crash loop. Upon restart the server, would resume trying to work against the replication queue and reproduce the crash. The container would be stabilized/healthy once the table was detached (something we've learned can be done a few ways). Note: we hadn't started inserting values into this column yet. Dropping the column and cleaning up ZK metadata using `SYSTEM` commands on the replica stabilized the container. Restoring the column reliably reproduces the issue in some of our higher-write-volume (and larger-shard-count, e.g. 8 shards x 2 replicas) installations, but interestingly we did not see this is some low-write 2x2 shard*replica installations.\r\n\r\n\n",
  "hints_text": "A couple extra data points to help investigation:\r\n\r\nBy following the thread ID in the stack trace \"(from thread 243)\", we can find information of the offending data parts, so we copied those potential data parts and created a `MergeTree` table (NOT ReplicatedMergeTree) with the old table schema to investigate this. (EDIT: I was testing with the new table schema with the extra column here, which sort of explains the `fillMissingColumns` in the stack trace)\r\n\r\nWe tested that we can move the data parts into the `detached` folder and safely attached them with the `alter table attach data part XXX` command.\r\n\r\nBut if we move the data parts into the `/store/table-uuid` folder and restart the server, we can consistently reproduce the segfault and we can find a `tmp_merge_XXX` file in the `/store/table-uuid` folder, it seems that the merge failed half way.\r\n\r\n\nout of the offending data parts being merged, one of them has significantly less files in the folder, e.g. most parts have 583 files consistently.\r\n```\r\nls -l chbackup_78130_3425_3427_2_3429/ | wc -l\r\n583\r\n```\r\nwhich contains several files per column + metadata files etc,\r\n\r\nbut one of them has only\r\n```\r\nls -l chbackup_78130_3428_3428_0_3429/ | wc -l\r\n15\r\n```\r\nwhich contains:\r\n- a couple metadata files: checksums.txt, columns.txt, ttl.txt, partition.dat, default_compression_codec.txt, count.txt \r\n- a couple index files: .idx, .mrk3\r\n- a tiny data.bin (8K) and data.mrk3 (4K)\r\n- no per column data files.\n>a couple index files: .idx, .mrk3\r\n>a tiny data.bin (8K) and data.mrk3 (4K)\r\n>no per column data files.\r\n\r\nIt's normal.\r\nIt's compact format. All columns in a single data.bin file.\r\nIt's for a small parts (inserts).\r\nUse `cat count.txt` to check number of rows.\n> with the old table schema to investigate this.\r\n\r\nDo you mean Ordinary database?\nI guess you should send `createtable.sql` and parts to support@clickhouse.com\n> > a couple index files: .idx, .mrk3\r\n> > a tiny data.bin (8K) and data.mrk3 (4K)\r\n> > no per column data files.\r\n> \r\n> It's normal. It's compact format. All columns in a single data.bin file. It's for a small parts (inserts). Use `cat count.txt` to check number of rows.\r\n\r\nGotcha, yeah `count.txt` has only 1\n> > with the old table schema to investigate this.\r\n> \r\n> Do you mean Ordinary database?\r\n\r\nno I meant that before the segfault, we had schema running in v1 for months, we then did a `alter table add column` to create a v2 schema which then led to 10+ nodes segfaulting despite the new columns was not written to yet. In this investigation I created a MergeTree table with schema v1 and was still able to get segfaults.\r\n\r\nEdit: I was testing with v2 instead\nReproduced somethting (`Logical error: 'Found non-equal columns with offsets (sizes: 100 and 100) for stream nested_things.size0`): https://pastila.nl/?00796184/5f1ae37f57dc799a6f41b61ed47a790e\r\n\r\ncc: @CurtizJ \nGreat to see that the very low-quality \"sketch\" of a table was actually sufficient to reproduce. Thanks @tavplubix and thanks everyone else for being so responsive.",
  "created_at": "2023-08-02T16:51:13Z",
  "modified_files": [
    "src/Interpreters/inplaceBlockConversions.cpp",
    "src/Storages/MergeTree/IMergeTreeDataPart.h",
    "src/Storages/MergeTree/IMergeTreeReader.cpp",
    "src/Storages/MergeTree/IMergeTreeReader.h",
    "src/Storages/MergeTree/MergeTreeBaseSelectProcessor.cpp",
    "src/Storages/MergeTree/MergeTreeBaseSelectProcessor.h",
    "src/Storages/MergeTree/MergeTreeDataPartCompact.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartCompact.h",
    "src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartInMemory.h",
    "src/Storages/MergeTree/MergeTreeDataPartWide.cpp",
    "src/Storages/MergeTree/MergeTreeDataPartWide.h",
    "src/Storages/MergeTree/MergeTreePrefetchedReadPool.cpp",
    "src/Storages/MergeTree/MergeTreeReaderCompact.cpp",
    "src/Storages/MergeTree/MergeTreeReaderCompact.h",
    "src/Storages/MergeTree/MergeTreeReaderInMemory.cpp",
    "src/Storages/MergeTree/MergeTreeReaderInMemory.h",
    "src/Storages/MergeTree/MergeTreeReaderWide.cpp",
    "src/Storages/MergeTree/MergeTreeReaderWide.h",
    "src/Storages/MergeTree/MergeTreeSelectProcessor.cpp",
    "src/Storages/MergeTree/MergeTreeSequentialSource.cpp",
    "src/Storages/MergeTree/MergeTreeThreadSelectProcessor.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/02835_nested_array_lowcardinality.reference",
    "b/tests/queries/0_stateless/02835_nested_array_lowcardinality.sql"
  ]
}