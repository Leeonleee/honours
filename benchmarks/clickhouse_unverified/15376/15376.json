{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 15376,
  "instance_id": "ClickHouse__ClickHouse-15376",
  "issue_numbers": [
    "13871"
  ],
  "base_commit": "cfdff17cf28d438f2010575906d44a17eb5a9ddc",
  "patch": "diff --git a/src/IO/WriteBufferFromS3.cpp b/src/IO/WriteBufferFromS3.cpp\nindex bac14acb9cd5..1ca8d8988d99 100644\n--- a/src/IO/WriteBufferFromS3.cpp\n+++ b/src/IO/WriteBufferFromS3.cpp\n@@ -56,7 +56,6 @@ WriteBufferFromS3::WriteBufferFromS3(\n         initiate();\n }\n \n-\n void WriteBufferFromS3::nextImpl()\n {\n     if (!offset())\n@@ -79,23 +78,31 @@ void WriteBufferFromS3::nextImpl()\n     }\n }\n \n-\n void WriteBufferFromS3::finalize()\n {\n-    next();\n+    finalizeImpl();\n+}\n \n-    if (is_multipart)\n-        writePart(temporary_buffer->str());\n+void WriteBufferFromS3::finalizeImpl()\n+{\n+    if (!finalized)\n+    {\n+        next();\n \n-    complete();\n-}\n+        if (is_multipart)\n+            writePart(temporary_buffer->str());\n \n+        complete();\n+\n+        finalized = true;\n+    }\n+}\n \n WriteBufferFromS3::~WriteBufferFromS3()\n {\n     try\n     {\n-        next();\n+        finalizeImpl();\n     }\n     catch (...)\n     {\n@@ -103,7 +110,6 @@ WriteBufferFromS3::~WriteBufferFromS3()\n     }\n }\n \n-\n void WriteBufferFromS3::initiate()\n {\n     Aws::S3::Model::CreateMultipartUploadRequest req;\ndiff --git a/src/IO/WriteBufferFromS3.h b/src/IO/WriteBufferFromS3.h\nindex 93a6947609e9..1a1e859d9132 100644\n--- a/src/IO/WriteBufferFromS3.h\n+++ b/src/IO/WriteBufferFromS3.h\n@@ -57,9 +57,13 @@ class WriteBufferFromS3 : public BufferWithOwnMemory<WriteBuffer>\n     ~WriteBufferFromS3() override;\n \n private:\n+    bool finalized = false;\n+\n     void initiate();\n     void writePart(const String & data);\n     void complete();\n+\n+    void finalizeImpl();\n };\n \n }\ndiff --git a/src/Storages/StorageS3.cpp b/src/Storages/StorageS3.cpp\nindex 6d17a17409f2..469350d6eaa7 100644\n--- a/src/Storages/StorageS3.cpp\n+++ b/src/Storages/StorageS3.cpp\n@@ -197,7 +197,7 @@ StorageS3::StorageS3(\n     const ColumnsDescription & columns_,\n     const ConstraintsDescription & constraints_,\n     Context & context_,\n-    const String & compression_method_ = \"\")\n+    const String & compression_method_)\n     : IStorage(table_id_)\n     , uri(uri_)\n     , context_global(context_)\n@@ -349,8 +349,6 @@ void registerStorageS3Impl(const String & name, StorageFactory & factory)\n         Poco::URI uri (url);\n         S3::URI s3_uri (uri);\n \n-        String format_name = engine_args[engine_args.size() - 1]->as<ASTLiteral &>().value.safeGet<String>();\n-\n         String access_key_id;\n         String secret_access_key;\n         if (engine_args.size() >= 4)\n@@ -362,12 +360,30 @@ void registerStorageS3Impl(const String & name, StorageFactory & factory)\n         UInt64 min_upload_part_size = args.local_context.getSettingsRef().s3_min_upload_part_size;\n \n         String compression_method;\n+        String format_name;\n         if (engine_args.size() == 3 || engine_args.size() == 5)\n+        {\n             compression_method = engine_args.back()->as<ASTLiteral &>().value.safeGet<String>();\n+            format_name = engine_args[engine_args.size() - 2]->as<ASTLiteral &>().value.safeGet<String>();\n+        }\n         else\n+        {\n             compression_method = \"auto\";\n+            format_name = engine_args.back()->as<ASTLiteral &>().value.safeGet<String>();\n+        }\n \n-        return StorageS3::create(s3_uri, access_key_id, secret_access_key, args.table_id, format_name, min_upload_part_size, args.columns, args.constraints, args.context);\n+        return StorageS3::create(\n+            s3_uri,\n+            access_key_id,\n+            secret_access_key,\n+            args.table_id,\n+            format_name,\n+            min_upload_part_size,\n+            args.columns,\n+            args.constraints,\n+            args.context,\n+            compression_method\n+        );\n     },\n     {\n         .source_access_type = AccessType::S3,\ndiff --git a/src/Storages/StorageS3.h b/src/Storages/StorageS3.h\nindex a172d951642b..5a702aa8785e 100644\n--- a/src/Storages/StorageS3.h\n+++ b/src/Storages/StorageS3.h\n@@ -34,7 +34,7 @@ class StorageS3 : public ext::shared_ptr_helper<StorageS3>, public IStorage\n         const ColumnsDescription & columns_,\n         const ConstraintsDescription & constraints_,\n         Context & context_,\n-        const String & compression_method_);\n+        const String & compression_method_ = \"\");\n \n     String getName() const override\n     {\n",
  "test_patch": "diff --git a/tests/integration/test_storage_s3/test.py b/tests/integration/test_storage_s3/test.py\nindex e39296525d0f..f752e72c6779 100644\n--- a/tests/integration/test_storage_s3/test.py\n+++ b/tests/integration/test_storage_s3/test.py\n@@ -1,8 +1,11 @@\n+import gzip\n import json\n import logging\n import os\n import random\n+import StringIO\n import threading\n+import time\n \n import helpers.client\n import pytest\n@@ -57,6 +60,11 @@ def prepare_s3_bucket(cluster):\n     minio_client.make_bucket(cluster.minio_restricted_bucket)\n \n \n+def put_s3_file_content(cluster, bucket, filename, data):\n+    buf = StringIO.StringIO(data)\n+    cluster.minio_client.put_object(bucket, filename, buf, len(data))\n+\n+\n # Returns content of given S3 file as string.\n def get_s3_file_content(cluster, bucket, filename):\n     # type: (ClickHouseCluster, str) -> str\n@@ -344,3 +352,116 @@ def test_infinite_redirect(cluster):\n         exception_raised = True\n     finally:\n         assert exception_raised\n+\n+\n+def test_storage_s3_get_gzip(cluster):\n+    bucket = cluster.minio_bucket\n+    instance = cluster.instances[\"dummy\"]\n+    filename = \"test_get_gzip.bin\"\n+    name = \"test_get_gzip\"\n+    data = [\n+        \"Sophia Intrieri,55\",\n+        \"Jack Taylor,71\",\n+        \"Christopher Silva,66\",\n+        \"Clifton Purser,35\",\n+        \"Richard Aceuedo,43\",\n+        \"Lisa Hensley,31\",\n+        \"Alice Wehrley,1\",\n+        \"Mary Farmer,47\",\n+        \"Samara Ramirez,19\",\n+        \"Shirley Lloyd,51\",\n+        \"Santos Cowger,0\",\n+        \"Richard Mundt,88\",\n+        \"Jerry Gonzalez,15\",\n+        \"Angela James,10\",\n+        \"Norman Ortega,33\",\n+        \"\"\n+    ]\n+    buf = StringIO.StringIO()\n+    compressed = gzip.GzipFile(fileobj=buf, mode=\"wb\")\n+    compressed.write(\"\\n\".join(data))\n+    compressed.close()\n+    put_s3_file_content(cluster, bucket, filename, buf.getvalue())\n+\n+    try:\n+        run_query(instance, \"CREATE TABLE {} (name String, id UInt32) ENGINE = S3('http://{}:{}/{}/{}', 'CSV', 'gzip')\".format(\n+            name, cluster.minio_host, cluster.minio_port, bucket, filename))\n+\n+        run_query(instance, \"SELECT sum(id) FROM {}\".format(name)).splitlines() == [\"565\"]\n+\n+    finally:\n+        run_query(instance, \"DROP TABLE {}\".format(name))\n+\n+\n+def test_storage_s3_put_uncompressed(cluster):\n+    bucket = cluster.minio_bucket\n+    instance = cluster.instances[\"dummy\"]\n+    filename = \"test_put_uncompressed.bin\"\n+    name = \"test_put_uncompressed\"\n+    data = [\n+        \"'Gloria Thompson',99\",\n+        \"'Matthew Tang',98\",\n+        \"'Patsy Anderson',23\",\n+        \"'Nancy Badillo',93\",\n+        \"'Roy Hunt',5\",\n+        \"'Adam Kirk',51\",\n+        \"'Joshua Douds',28\",\n+        \"'Jolene Ryan',0\",\n+        \"'Roxanne Padilla',50\",\n+        \"'Howard Roberts',41\",\n+        \"'Ricardo Broughton',13\",\n+        \"'Roland Speer',83\",\n+        \"'Cathy Cohan',58\",\n+        \"'Kathie Dawson',100\",\n+        \"'Gregg Mcquistion',11\",\n+    ]\n+    try:\n+        run_query(instance, \"CREATE TABLE {} (name String, id UInt32) ENGINE = S3('http://{}:{}/{}/{}', 'CSV')\".format(\n+            name, cluster.minio_host, cluster.minio_port, bucket, filename))\n+\n+        run_query(instance, \"INSERT INTO {} VALUES ({})\".format(name, \"),(\".join(data)))\n+\n+        run_query(instance, \"SELECT sum(id) FROM {}\".format(name)).splitlines() == [\"753\"]\n+\n+        uncompressed_content = get_s3_file_content(cluster, bucket, filename)\n+        assert sum([ int(i.split(',')[1]) for i in uncompressed_content.splitlines() ]) == 753\n+    finally:\n+        run_query(instance, \"DROP TABLE {}\".format(name))\n+\n+\n+def test_storage_s3_put_gzip(cluster):\n+    bucket = cluster.minio_bucket\n+    instance = cluster.instances[\"dummy\"]\n+    filename = \"test_put_gzip.bin\"\n+    name = \"test_put_gzip\"\n+    data = [\n+        \"'Joseph Tomlinson',5\",\n+        \"'Earnest Essary',44\",\n+        \"'Matha Pannell',24\",\n+        \"'Michael Shavers',46\",\n+        \"'Elias Groce',38\",\n+        \"'Pamela Bramlet',50\",\n+        \"'Lewis Harrell',49\",\n+        \"'Tamara Fyall',58\",\n+        \"'George Dixon',38\",\n+        \"'Alice Walls',49\",\n+        \"'Paula Mais',24\",\n+        \"'Myrtle Pelt',93\",\n+        \"'Sylvia Naffziger',18\",\n+        \"'Amanda Cave',83\",\n+        \"'Yolanda Joseph',89\"\n+    ]\n+    try:\n+        run_query(instance, \"CREATE TABLE {} (name String, id UInt32) ENGINE = S3('http://{}:{}/{}/{}', 'CSV', 'gzip')\".format(\n+            name, cluster.minio_host, cluster.minio_port, bucket, filename))\n+\n+        run_query(instance, \"INSERT INTO {} VALUES ({})\".format(name, \"),(\".join(data)))\n+\n+        run_query(instance, \"SELECT sum(id) FROM {}\".format(name)).splitlines() == [\"708\"]\n+\n+        buf = StringIO.StringIO(get_s3_file_content(cluster, bucket, filename))\n+        f = gzip.GzipFile(fileobj=buf, mode=\"rb\")\n+        uncompressed_content = f.read()\n+        assert sum([ int(i.split(',')[1]) for i in uncompressed_content.splitlines() ]) == 708\n+    finally:\n+        run_query(instance, \"DROP TABLE {}\".format(name))\n",
  "problem_statement": "S3 table engine compression doesn't work.\n**Describe the bug**\r\nClickhouse expect last parameter of S3 table engine to be data format, not compression type.\r\n\r\n**How to reproduce**\r\nClickhouse version 20.5.4.40\r\n\r\n```\r\nCREATE TABLE test_s3(S_SUPPKEY       UInt32,         S_NAME          String,         S_ADDRESS       String,         S_CITY          LowCardinality(String),         S_NATION        LowCardinality(String),         S_REGION        LowCardinality(String),         S_PHONE         String) ENGINE=S3('https://s3.amazonaws.com/{some_bucket_path}.csv.gz','CSV','gzip');\r\nSELECT * FROM test_s3;\r\nReceived exception from server (version 20.5.4):\r\nCode: 73. DB::Exception: Received from localhost:9000. DB::Exception: Unknown format gzip.\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n```\r\n\r\n**Error message and/or stacktrace**\r\n```\r\n Code: 73, e.displayText() = DB::Exception: Unknown format gzip (version 20.5.4.40 (official build)) (from 127.0.0.1:56140) (in query: select * from test_s3;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x11b9acc0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x9f3e2cd in /usr/bin/clickhouse\r\n2. ? @ 0xf36cd04 in /usr/bin/clickhouse\r\n3. DB::FormatFactory::getInput(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ReadBuffer&, DB::Block const&, DB::Context const&, unsigned long, std::__1::function<void ()>) const @ 0xf36b0c9 in /usr/bin/clickhouse\r\n4. DB::StorageS3::read(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, DB::SelectQueryInfo const&, DB::Context const&, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) @ 0xefb7520 in /usr/bin/clickhouse\r\n5. DB::ReadFromStorageStep::ReadFromStorageStep(DB::TableStructureReadLockHolder, DB::SelectQueryOptions, std::__1::shared_ptr<DB::IStorage>, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context>, DB::QueryProcessingStage::Enum, unsigned long, unsigned long) @ 0xf68f3fd in /usr/bin/clickhouse\r\n6. DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&, std::__1::shared_ptr<DB::PrewhereInfo> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xea608c1 in /usr/bin/clickhouse\r\n7. DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>) @ 0xea647a2 in /usr/bin/clickhouse\r\n8. DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&) @ 0xea65d54 in /usr/bin/clickhouse\r\n9. DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&) @ 0xebce0f4 in /usr/bin/clickhouse\r\n10. DB::InterpreterSelectWithUnionQuery::execute() @ 0xebce44c in /usr/bin/clickhouse\r\n11. ? @ 0xed3c7ed in /usr/bin/clickhouse\r\n12. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0xed3fe2a in /usr/bin/clickhouse\r\n13. DB::TCPHandler::runImpl() @ 0xf36443c in /usr/bin/clickhouse\r\n14. DB::TCPHandler::run() @ 0xf365190 in /usr/bin/clickhouse\r\n15. Poco::Net::TCPServerConnection::start() @ 0x11ab8aeb in /usr/bin/clickhouse\r\n16. Poco::Net::TCPServerDispatcher::run() @ 0x11ab8f7b in /usr/bin/clickhouse\r\n17. Poco::PooledThread::run() @ 0x11c37aa6 in /usr/bin/clickhouse\r\n18. Poco::ThreadImpl::runnableEntry(void*) @ 0x11c32ea0 in /usr/bin/clickhouse\r\n19. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n20. clone @ 0x121a3f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n\r\n```\r\n\r\n**Additional context**\r\nAuto detect of compression type from file extension doesnt work either.\r\n```\r\nCREATE TABLE test_s3(S_SUPPKEY       UInt32,         S_NAME          String,         S_ADDRESS       String,         S_CITY          LowCardinality(String),         S_NATION        LowCardinality(String),         S_REGION        LowCardinality(String),         S_PHONE         String) ENGINE=S3('https://s3.amazonaws.com/{some_bucket_path}.csv.gz','CSV');\r\nCode: 27, e.displayText() = DB::Exception: Cannot parse input: expected ',' before: '\u001f\ufffd\\b\\bvY!_\\0\u0003supplier.csv\\0\ufffd\ufffdG\ufffd\ufffdJ\ufffd\ufffd\ufffd\ufffd\u0015*\ufffd\ufffd\\\\\ufffd\ufffd\ufffdR9\u7b1dr\ufffdY\ufffd\ufffd5\ufffd\ufffd\ufffdR\ufffd\ufffd\u0011=U\ufffd;\ufffd3fh6\ufffd\\b\u001f\\0\u0012u\u001a\ufffd\ufffd~\ufffd\ufffd\u001c\ufffd\ufffd\ufffd\ufffdq\ufffd\ufffdi|\ufffdl\ufffd\ufffdd}4=.\ufffd\u05de[(F\u0017\ufffd\ufffdWS\ufffdV\u0004\ufffd\ufffd~\ufffd\ufffd\ufffd\ufffd\ufffdR\ufffd\\\\\"\ufffd\ufffd\ufffd}\ufffd\ufffd\ufffd8\ufffdM}\ufffds\ufffd\ufffd\ufffd\ufffd\ufffdT\ufffd?\ufffd\ufffd\u04bc\ufffd}\ufffd\ufffd\"}\ufffd': (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: S_SUPPKEY, type: UInt32,                 ERROR: text \"<0x1F>\ufffd<BACKSPACE><BACKSPACE>vY!_<ASCII NUL><0x03>\" is not like UInt32\r\n\r\n: While executing S3 (version 20.5.4.40 (official build)) (from 127.0.0.1:56140) (in query: select * from test_s3;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x11b9acc0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x9f3e2cd in /usr/bin/clickhouse\r\n2. ? @ 0x9f808fd in /usr/bin/clickhouse\r\n3. ? @ 0xf45bd5d in /usr/bin/clickhouse\r\n4. DB::CSVRowInputFormat::readRow(std::__1::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn> > >&, DB::RowReadExtension&) @ 0xf45d179 in /usr/bin/clickhouse\r\n5. DB::IRowInputFormat::generate() @ 0xf42cab1 in /usr/bin/clickhouse\r\n6. DB::ISource::work() @ 0xf3aa2bb in /usr/bin/clickhouse\r\n7. DB::InputStreamFromInputFormat::readImpl() @ 0xf36fe8d in /usr/bin/clickhouse\r\n8. DB::IBlockInputStream::read() @ 0xe65fb3d in /usr/bin/clickhouse\r\n9. DB::ParallelParsingBlockInputStream::parserThreadFunction(std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) @ 0xf374738 in /usr/bin/clickhouse\r\n10. ? @ 0xf375494 in /usr/bin/clickhouse\r\n11. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x9f6ca37 in /usr/bin/clickhouse\r\n12. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const @ 0x9f6d1aa in /usr/bin/clickhouse\r\n13. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x9f6bf47 in /usr/bin/clickhouse\r\n14. ? @ 0x9f6a433 in /usr/bin/clickhouse\r\n15. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n16. clone @ 0x121a3f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n\r\n\r\nReceived exception from server (version 20.5.4):\r\nCode: 27. DB::Exception: Received from localhost:9000. DB::Exception: Cannot parse input: expected ',' before: '\u001f\ufffd\\b\\bvY!_\\0\u0003supplier.csv\\0\ufffd\ufffdG\ufffd\ufffdJ\ufffd\ufffd\ufffd\ufffd\u0015*\ufffd\ufffd\\\\\ufffd\ufffd\ufffdR9\u7b1dr\ufffdY\ufffd\ufffd5\ufffd\ufffd\ufffdR\ufffd\ufffd\u0011=U\ufffd;\ufffd3fh6\ufffd\\b\u001f\\0\u0012u\u001a\ufffd\ufffd~\ufffd\ufffd\u001c\ufffd\ufffd\ufffd\ufffdq\ufffd\ufffdi|\ufffdl\ufffd\ufffdd}4=.\ufffd\u05de[(F\u0017\ufffd\ufffdWS\ufffdV\u0004\ufffd\ufffd~\ufffd\ufffd\ufffd\ufffd\ufffdR\ufffd\\\\\"\ufffd\ufffd\ufffd}\ufffd\ufffd\ufffd8\ufffdM}\ufffds\ufffd\ufffd\ufffd\ufffd\ufffdT\ufffd?\ufffd\ufffd\u04bc\ufffd}\ufffd\ufffd\"}\ufffd': (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: S_SUPPKEY, type: UInt32,                 ERROR: text \"<0x1F>\ufffd<BACKSPACE><BACKSPACE>vY!_<ASCII NUL><0x03>\" is not like UInt32\r\n\r\n: While executing S3.\r\n\r\n0 rows in set. Elapsed: 0.702 sec.\r\n```\r\nIf we in both CREATE table queries replace S3 with URL, SELECT queries will work fine.\r\n\n",
  "hints_text": "",
  "created_at": "2020-09-28T07:39:18Z",
  "modified_files": [
    "src/IO/WriteBufferFromS3.cpp",
    "src/IO/WriteBufferFromS3.h",
    "src/Storages/StorageS3.cpp",
    "src/Storages/StorageS3.h"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_s3/test.py"
  ]
}