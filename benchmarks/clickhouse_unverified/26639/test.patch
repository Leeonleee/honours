diff --git a/src/Storages/tests/gtest_storage_log.cpp b/src/Storages/tests/gtest_storage_log.cpp
index 16902eafc98c..b3ceef7e6972 100644
--- a/src/Storages/tests/gtest_storage_log.cpp
+++ b/src/Storages/tests/gtest_storage_log.cpp
@@ -128,6 +128,7 @@ std::string readData(DB::StoragePtr & table, const DB::ContextPtr context)
     {
         ColumnWithTypeAndName col;
         col.type = std::make_shared<DataTypeUInt64>();
+        col.name = "a";
         sample.insert(std::move(col));
     }
 
diff --git a/tests/integration/test_dictionaries_postgresql/configs/dictionaries/postgres_dict.xml b/tests/integration/test_dictionaries_postgresql/configs/dictionaries/postgres_dict.xml
index 4ee07d0972a3..734da0cff707 100644
--- a/tests/integration/test_dictionaries_postgresql/configs/dictionaries/postgres_dict.xml
+++ b/tests/integration/test_dictionaries_postgresql/configs/dictionaries/postgres_dict.xml
@@ -19,10 +19,10 @@
       <structure>
          <id>
             <name>id</name>
-            <type>UInt32</type>
+            <!--<type>UInt32</type>-->
          </id>
          <attribute>
-            <name>id</name>
+            <name>key</name>
             <type>UInt32</type>
             <null_value></null_value>
          </attribute>
@@ -65,10 +65,10 @@
       <structure>
          <id>
             <name>id</name>
-            <type>UInt32</type>
+            <!--<type>UInt32</type>-->
          </id>
          <attribute>
-            <name>id</name>
+            <name>key</name>
             <type>UInt32</type>
             <null_value></null_value>
          </attribute>
diff --git a/tests/integration/test_dictionaries_postgresql/test.py b/tests/integration/test_dictionaries_postgresql/test.py
index 6eb4a04ed2c8..58a503bd571b 100644
--- a/tests/integration/test_dictionaries_postgresql/test.py
+++ b/tests/integration/test_dictionaries_postgresql/test.py
@@ -13,11 +13,11 @@
 
 postgres_dict_table_template = """
     CREATE TABLE IF NOT EXISTS {} (
-    id Integer NOT NULL, value Integer NOT NULL, PRIMARY KEY (id))
+    id Integer NOT NULL, key Integer NOT NULL, value Integer NOT NULL, PRIMARY KEY (id))
     """
 click_dict_table_template = """
     CREATE TABLE IF NOT EXISTS `test`.`dict_table_{}` (
-        `id` UInt64, `value` UInt32
+        `key` UInt32, `value` UInt32
     ) ENGINE = Dictionary({})
     """
 
@@ -43,7 +43,7 @@ def create_and_fill_postgres_table(cursor, table_name, port, host):
     create_postgres_table(cursor, table_name)
     # Fill postgres table using clickhouse postgres table function and check
     table_func = '''postgresql('{}:{}', 'clickhouse', '{}', 'postgres', 'mysecretpassword')'''.format(host, port, table_name)
-    node1.query('''INSERT INTO TABLE FUNCTION {} SELECT number, number from numbers(10000)
+    node1.query('''INSERT INTO TABLE FUNCTION {} SELECT number, number, number from numbers(10000)
             '''.format(table_func, table_name))
     result = node1.query("SELECT count() FROM {}".format(table_func))
     assert result.rstrip() == '10000'
@@ -82,7 +82,7 @@ def test_load_dictionaries(started_cluster):
 
     node1.query("SYSTEM RELOAD DICTIONARY {}".format(dict_name))
     assert node1.query("SELECT count() FROM `test`.`dict_table_{}`".format(table_name)).rstrip() == '10000'
-    assert node1.query("SELECT dictGetUInt32('{}', 'id', toUInt64(0))".format(dict_name)) == '0
'
+    assert node1.query("SELECT dictGetUInt32('{}', 'key', toUInt64(0))".format(dict_name)) == '0
'
     assert node1.query("SELECT dictGetUInt32('{}', 'value', toUInt64(9999))".format(dict_name)) == '9999
'
 
     cursor.execute("DROP TABLE IF EXISTS {}".format(table_name))
@@ -252,11 +252,11 @@ def test_dictionary_with_replicas(started_cluster):
     create_postgres_table(cursor1, 'test1')
     create_postgres_table(cursor2, 'test1')
 
-    cursor1.execute('INSERT INTO test1 select i, i from generate_series(0, 99) as t(i);');
-    cursor2.execute('INSERT INTO test1 select i, i from generate_series(100, 199) as t(i);');
+    cursor1.execute('INSERT INTO test1 select i, i, i from generate_series(0, 99) as t(i);')
+    cursor2.execute('INSERT INTO test1 select i, i, i from generate_series(100, 199) as t(i);')
 
     create_dict('test1', 1)
-    result = node1.query("SELECT * FROM `test`.`dict_table_test1` ORDER BY id")
+    result = node1.query("SELECT * FROM `test`.`dict_table_test1` ORDER BY key")
 
     # priority 0 - non running port
     assert node1.contains_in_log('PostgreSQLConnectionPool: Connection error*')
diff --git a/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml b/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml
index 6aad3ad9917a..a65360b0e26a 100644
--- a/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml
+++ b/tests/integration/test_odbc_interaction/configs/dictionaries/postgres_odbc_hashed_dictionary.xml
@@ -18,7 +18,7 @@
 
        <structure>
            <id>
-               <name>column1</name>
+               <name>id</name>
            </id>
 
            <attribute>
diff --git a/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_cached_dictionary.xml b/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_cached_dictionary.xml
index 45f3966ee8af..3a505b79304f 100644
--- a/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_cached_dictionary.xml
+++ b/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_cached_dictionary.xml
@@ -20,7 +20,7 @@
 
        <structure>
            <id>
-               <name>X</name>
+               <name>id</name>
            </id>
 
            <attribute>
diff --git a/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml b/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml
index 18a14b896bda..5b53818cf13c 100644
--- a/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml
+++ b/tests/integration/test_odbc_interaction/configs/dictionaries/sqlite3_odbc_hashed_dictionary.xml
@@ -20,7 +20,7 @@
 
        <structure>
            <id>
-               <name>X</name>
+               <name>id</name>
            </id>
 
            <attribute>
diff --git a/tests/integration/test_odbc_interaction/test.py b/tests/integration/test_odbc_interaction/test.py
index 39a283448f5e..4d2f70ad08cd 100644
--- a/tests/integration/test_odbc_interaction/test.py
+++ b/tests/integration/test_odbc_interaction/test.py
@@ -99,19 +99,19 @@ def started_cluster():
 
         logging.debug(f"sqlite data received: {sqlite_db}")
         node1.exec_in_container(
-            ["sqlite3", sqlite_db, "CREATE TABLE t1(x INTEGER PRIMARY KEY ASC, y, z);"],
+            ["sqlite3", sqlite_db, "CREATE TABLE t1(id INTEGER PRIMARY KEY ASC, x INTEGER, y, z);"],
             privileged=True, user='root')
         node1.exec_in_container(
-            ["sqlite3", sqlite_db, "CREATE TABLE t2(X INTEGER PRIMARY KEY ASC, Y, Z);"],
+            ["sqlite3", sqlite_db, "CREATE TABLE t2(id INTEGER PRIMARY KEY ASC, X INTEGER, Y, Z);"],
             privileged=True, user='root')
         node1.exec_in_container(
-            ["sqlite3", sqlite_db, "CREATE TABLE t3(X INTEGER PRIMARY KEY ASC, Y, Z);"],
+            ["sqlite3", sqlite_db, "CREATE TABLE t3(id INTEGER PRIMARY KEY ASC, X INTEGER, Y, Z);"],
             privileged=True, user='root')
         node1.exec_in_container(
-            ["sqlite3", sqlite_db, "CREATE TABLE t4(X INTEGER PRIMARY KEY ASC, Y, Z);"],
+            ["sqlite3", sqlite_db, "CREATE TABLE t4(id INTEGER PRIMARY KEY ASC, X INTEGER, Y, Z);"],
             privileged=True, user='root')
         node1.exec_in_container(
-            ["sqlite3", sqlite_db, "CREATE TABLE tf1(x INTEGER PRIMARY KEY ASC, y, z);"],
+            ["sqlite3", sqlite_db, "CREATE TABLE tf1(id INTEGER PRIMARY KEY ASC, x INTEGER, y, z);"],
             privileged=True, user='root')
         logging.debug("sqlite tables created")
         mysql_conn = get_mysql_conn()
@@ -128,7 +128,7 @@ def started_cluster():
 
         cursor = postgres_conn.cursor()
         cursor.execute(
-            "create table if not exists clickhouse.test_table (column1 int primary key, column2 varchar(40) not null)")
+            "create table if not exists clickhouse.test_table (id int primary key, column1 int not null, column2 varchar(40) not null)")
 
         yield cluster
 
@@ -210,9 +210,9 @@ def test_sqlite_simple_select_function_works(started_cluster):
     sqlite_setup = node1.odbc_drivers["SQLite3"]
     sqlite_db = sqlite_setup["Database"]
 
-    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t1 values(1, 2, 3);"],
+    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t1 values(1, 1, 2, 3);"],
                             privileged=True, user='root')
-    assert node1.query("select * from odbc('DSN={}', '{}')".format(sqlite_setup["DSN"], 't1')) == "1\t2\t3
"
+    assert node1.query("select * from odbc('DSN={}', '{}')".format(sqlite_setup["DSN"], 't1')) == "1\t1\t2\t3
"
 
     assert node1.query("select y from odbc('DSN={}', '{}')".format(sqlite_setup["DSN"], 't1')) == "2
"
     assert node1.query("select z from odbc('DSN={}', '{}')".format(sqlite_setup["DSN"], 't1')) == "3
"
@@ -228,10 +228,10 @@ def test_sqlite_table_function(started_cluster):
     sqlite_setup = node1.odbc_drivers["SQLite3"]
     sqlite_db = sqlite_setup["Database"]
 
-    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO tf1 values(1, 2, 3);"],
+    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO tf1 values(1, 1, 2, 3);"],
                             privileged=True, user='root')
     node1.query("create table odbc_tf as odbc('DSN={}', '{}')".format(sqlite_setup["DSN"], 'tf1'))
-    assert node1.query("select * from odbc_tf") == "1\t2\t3
"
+    assert node1.query("select * from odbc_tf") == "1\t1\t2\t3
"
 
     assert node1.query("select y from odbc_tf") == "2
"
     assert node1.query("select z from odbc_tf") == "3
"
@@ -246,7 +246,7 @@ def test_sqlite_simple_select_storage_works(started_cluster):
     sqlite_setup = node1.odbc_drivers["SQLite3"]
     sqlite_db = sqlite_setup["Database"]
 
-    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t4 values(1, 2, 3);"],
+    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t4 values(1, 1, 2, 3);"],
                             privileged=True, user='root')
     node1.query("create table SqliteODBC (x Int32, y String, z String) engine = ODBC('DSN={}', '', 't4')".format(
         sqlite_setup["DSN"]))
@@ -264,7 +264,7 @@ def test_sqlite_odbc_hashed_dictionary(started_cluster):
     skip_test_msan(node1)
 
     sqlite_db = node1.odbc_drivers["SQLite3"]["Database"]
-    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t2 values(1, 2, 3);"],
+    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t2 values(1, 1, 2, 3);"],
                             privileged=True, user='root')
 
     node1.query("SYSTEM RELOAD DICTIONARY sqlite3_odbc_hashed")
@@ -282,7 +282,7 @@ def test_sqlite_odbc_hashed_dictionary(started_cluster):
         logging.debug("Waiting dictionary to update for the second time")
         time.sleep(0.1)
 
-    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t2 values(200, 2, 7);"],
+    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t2 values(200, 200, 2, 7);"],
                             privileged=True, user='root')
 
     # No reload because of invalidate query
@@ -299,7 +299,7 @@ def test_sqlite_odbc_hashed_dictionary(started_cluster):
     assert_eq_with_retry(node1, "select dictGetUInt8('sqlite3_odbc_hashed', 'Z', toUInt64(1))", "3")
     assert_eq_with_retry(node1, "select dictGetUInt8('sqlite3_odbc_hashed', 'Z', toUInt64(200))", "1") # still default
 
-    node1.exec_in_container(["sqlite3", sqlite_db, "REPLACE INTO t2 values(1, 2, 5);"],
+    node1.exec_in_container(["sqlite3", sqlite_db, "REPLACE INTO t2 values(1, 1, 2, 5);"],
                             privileged=True, user='root')
 
     assert_eq_with_retry(node1, "select dictGetUInt8('sqlite3_odbc_hashed', 'Z', toUInt64(1))", "5")
@@ -310,7 +310,7 @@ def test_sqlite_odbc_cached_dictionary(started_cluster):
     skip_test_msan(node1)
 
     sqlite_db = node1.odbc_drivers["SQLite3"]["Database"]
-    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t3 values(1, 2, 3);"],
+    node1.exec_in_container(["sqlite3", sqlite_db, "INSERT INTO t3 values(1, 1, 2, 3);"],
                             privileged=True, user='root')
 
     assert node1.query("select dictGetUInt8('sqlite3_odbc_cached', 'Z', toUInt64(1))") == "3
"
@@ -319,12 +319,12 @@ def test_sqlite_odbc_cached_dictionary(started_cluster):
     node1.exec_in_container(["chmod", "a+rw", "/tmp"], privileged=True, user='root')
     node1.exec_in_container(["chmod", "a+rw", sqlite_db], privileged=True, user='root')
 
-    node1.query("insert into table function odbc('DSN={};ReadOnly=0', '', 't3') values (200, 2, 7)".format(
+    node1.query("insert into table function odbc('DSN={};ReadOnly=0', '', 't3') values (200, 200, 2, 7)".format(
         node1.odbc_drivers["SQLite3"]["DSN"]))
 
     assert node1.query("select dictGetUInt8('sqlite3_odbc_cached', 'Z', toUInt64(200))") == "7
"  # new value
 
-    node1.exec_in_container(["sqlite3", sqlite_db, "REPLACE INTO t3 values(1, 2, 12);"],
+    node1.exec_in_container(["sqlite3", sqlite_db, "REPLACE INTO t3 values(1, 1, 2, 12);"],
                             privileged=True, user='root')
 
     assert_eq_with_retry(node1, "select dictGetUInt8('sqlite3_odbc_cached', 'Z', toUInt64(1))", "12")
@@ -336,7 +336,7 @@ def test_postgres_odbc_hashed_dictionary_with_schema(started_cluster):
     conn = get_postgres_conn(started_cluster)
     cursor = conn.cursor()
     cursor.execute("truncate table clickhouse.test_table")
-    cursor.execute("insert into clickhouse.test_table values(1, 'hello'),(2, 'world')")
+    cursor.execute("insert into clickhouse.test_table values(1, 1, 'hello'),(2, 2, 'world')")
     node1.query("SYSTEM RELOAD DICTIONARY postgres_odbc_hashed")
     assert_eq_with_retry(node1, "select dictGetString('postgres_odbc_hashed', 'column2', toUInt64(1))", "hello")
     assert_eq_with_retry(node1, "select dictGetString('postgres_odbc_hashed', 'column2', toUInt64(2))", "world")
@@ -348,7 +348,7 @@ def test_postgres_odbc_hashed_dictionary_no_tty_pipe_overflow(started_cluster):
     conn = get_postgres_conn(started_cluster)
     cursor = conn.cursor()
     cursor.execute("truncate table clickhouse.test_table")
-    cursor.execute("insert into clickhouse.test_table values(3, 'xxx')")
+    cursor.execute("insert into clickhouse.test_table values(3, 3, 'xxx')")
     for i in range(100):
         try:
             node1.query("system reload dictionary postgres_odbc_hashed", timeout=15)
@@ -369,13 +369,13 @@ def test_postgres_insert(started_cluster):
     # reconstruction of connection string.
 
     node1.query(
-        "create table pg_insert (column1 UInt8, column2 String) engine=ODBC('DSN=postgresql_odbc;Servername=postgre-sql.local', 'clickhouse', 'test_table')")
-    node1.query("insert into pg_insert values (1, 'hello'), (2, 'world')")
-    assert node1.query("select * from pg_insert") == '1\thello
2\tworld
'
-    node1.query("insert into table function odbc('DSN=postgresql_odbc', 'clickhouse', 'test_table') format CSV 3,test")
+        "create table pg_insert (id UInt64, column1 UInt8, column2 String) engine=ODBC('DSN=postgresql_odbc;Servername=postgre-sql.local', 'clickhouse', 'test_table')")
+    node1.query("insert into pg_insert values (1, 1, 'hello'), (2, 2, 'world')")
+    assert node1.query("select * from pg_insert") == '1\t1\thello
2\t2\tworld
'
+    node1.query("insert into table function odbc('DSN=postgresql_odbc', 'clickhouse', 'test_table') format CSV 3,3,test")
     node1.query(
         "insert into table function odbc('DSN=postgresql_odbc;Servername=postgre-sql.local', 'clickhouse', 'test_table')" \
-        " select number, 's' || toString(number) from numbers (4, 7)")
+        " select number, number, 's' || toString(number) from numbers (4, 7)")
     assert node1.query("select sum(column1), count(column1) from pg_insert") == "55\t10
"
     assert node1.query(
         "select sum(n), count(n) from (select (*,).1 as n from (select * from odbc('DSN=postgresql_odbc', 'clickhouse', 'test_table')))") == "55\t10
"
@@ -426,19 +426,19 @@ def test_odbc_postgres_date_data_type(started_cluster):
 
     conn = get_postgres_conn(started_cluster);
     cursor = conn.cursor()
-    cursor.execute("CREATE TABLE IF NOT EXISTS clickhouse.test_date (column1 integer, column2 date)")
+    cursor.execute("CREATE TABLE IF NOT EXISTS clickhouse.test_date (id integer, column1 integer, column2 date)")
 
-    cursor.execute("INSERT INTO clickhouse.test_date VALUES (1, '2020-12-01')")
-    cursor.execute("INSERT INTO clickhouse.test_date VALUES (2, '2020-12-02')")
-    cursor.execute("INSERT INTO clickhouse.test_date VALUES (3, '2020-12-03')")
+    cursor.execute("INSERT INTO clickhouse.test_date VALUES (1, 1, '2020-12-01')")
+    cursor.execute("INSERT INTO clickhouse.test_date VALUES (2, 2, '2020-12-02')")
+    cursor.execute("INSERT INTO clickhouse.test_date VALUES (3, 3, '2020-12-03')")
     conn.commit()
 
     node1.query(
         '''
-        CREATE TABLE test_date (column1 UInt64, column2 Date)
+        CREATE TABLE test_date (id UInt64, column1 UInt64, column2 Date)
         ENGINE=ODBC('DSN=postgresql_odbc; Servername=postgre-sql.local', 'clickhouse', 'test_date')''')
 
-    expected = '1\t2020-12-01
2\t2020-12-02
3\t2020-12-03
'
+    expected = '1\t1\t2020-12-01
2\t2\t2020-12-02
3\t3\t2020-12-03
'
     result = node1.query('SELECT * FROM test_date');
     assert(result == expected)
     cursor.execute("DROP TABLE IF EXISTS clickhouse.test_date")
diff --git a/tests/queries/0_stateless/00818_alias_bug_4110.reference b/tests/queries/0_stateless/00818_alias_bug_4110.reference
index e6013d269c23..210fc67db665 100644
--- a/tests/queries/0_stateless/00818_alias_bug_4110.reference
+++ b/tests/queries/0_stateless/00818_alias_bug_4110.reference
@@ -4,7 +4,6 @@
 11	12
 12	11
 10	10
-10	11	11
 12	11
 10	12
 11	12
diff --git a/tests/queries/0_stateless/00818_alias_bug_4110.sql b/tests/queries/0_stateless/00818_alias_bug_4110.sql
index 7b2fd5d38648..df7e70cb2759 100644
--- a/tests/queries/0_stateless/00818_alias_bug_4110.sql
+++ b/tests/queries/0_stateless/00818_alias_bug_4110.sql
@@ -5,7 +5,7 @@ select s.a + 1 as b, s.a + 2 as a from (select 10 as a) s;
 select s.a + 2 as b, s.a + 1 as a from (select 10 as a) s;
 
 select a, a as a from (select 10 as a);
-select s.a, a, a + 1 as a from (select 10 as a) as s;
+select s.a, a, a + 1 as a from (select 10 as a) as s; -- { serverError 352 }
 select s.a + 2 as b, b - 1 as a from (select 10 as a) s;
 select s.a as a, s.a + 2 as b from (select 10 as a) s;
 select s.a + 1 as a, s.a + 2 as b from (select 10 as a) s;
diff --git a/tests/queries/0_stateless/01101_literal_column_clash.reference b/tests/queries/0_stateless/01101_literal_column_clash.reference
index b89f59abb182..8f76d98575c7 100644
--- a/tests/queries/0_stateless/01101_literal_column_clash.reference
+++ b/tests/queries/0_stateless/01101_literal_column_clash.reference
@@ -3,7 +3,6 @@
 7	0
 7	1
 xyzabc	2
-1	3
 1	2	0	0
 1	0	0	3
 \N	1	2	\N	0
diff --git a/tests/queries/0_stateless/01101_literal_column_clash.sql b/tests/queries/0_stateless/01101_literal_column_clash.sql
index 4a6064141ea6..b9645e3609ec 100644
--- a/tests/queries/0_stateless/01101_literal_column_clash.sql
+++ b/tests/queries/0_stateless/01101_literal_column_clash.sql
@@ -7,7 +7,7 @@ join (select '1' as sid) as t2 on t2.sid = cast(t1.iid as String);
 select cast(7 as String), * from (select 3 "'String'");
 select cast(7 as String), * from (select number "'String'" FROM numbers(2));
 SELECT concat('xyz', 'abc'), * FROM (SELECT 2 AS "'xyz'");
-with 3 as "1" select 1, "1";
+with 3 as "1" select 1, "1"; -- { serverError 352 }
 
 -- https://github.com/ClickHouse/ClickHouse/issues/9953
 select 1, * from (select 2 x) a left join (select 1, 3 y) b on y = x;
diff --git a/tests/queries/0_stateless/01236_graphite_mt.sql b/tests/queries/0_stateless/01236_graphite_mt.sql
index ccf7c066e757..a6dd4b8b6fbc 100644
--- a/tests/queries/0_stateless/01236_graphite_mt.sql
+++ b/tests/queries/0_stateless/01236_graphite_mt.sql
@@ -13,7 +13,7 @@ WITH dates AS
                today - INTERVAL 3 day as older_date
     )
     -- Newer than 2 days are kept in windows of 600 seconds
-    select 1, 'sum_1', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all
+    select 1 AS key, 'sum_1' AS s, today - number * 60 - 30, number, 1, number from dates, numbers(300) union all
     select 2, 'sum_1', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all
     select 1, 'sum_2', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all
     select 2, 'sum_2', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all
@@ -23,7 +23,7 @@ WITH dates AS
     select 2, 'max_2', today - number * 60 - 30, number, 1, number from dates, numbers(300) union all
 
     -- Older than 2 days use 6000 second windows
-    select 1, 'sum_1', older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all
+    select 1 AS key, 'sum_1' AS s, older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all
     select 2, 'sum_1', older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all
     select 1, 'sum_2', older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all
     select 2, 'sum_2', older_date - number * 60 - 30, number, 1, number from dates, numbers(1200) union all
diff --git a/tests/queries/0_stateless/01950_aliases_bad_cast.reference b/tests/queries/0_stateless/01950_aliases_bad_cast.reference
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/tests/queries/0_stateless/01950_aliases_bad_cast.sql b/tests/queries/0_stateless/01950_aliases_bad_cast.sql
new file mode 100644
index 000000000000..bdd2339f855d
--- /dev/null
+++ b/tests/queries/0_stateless/01950_aliases_bad_cast.sql
@@ -0,0 +1,2 @@
+SELECT 1, * FROM (SELECT NULL AS `1`); -- { serverError 352 }
+SELECT '7', 'xyz', * FROM (SELECT NULL AS `'xyz'`); -- { serverError 352 }
diff --git a/tests/queries/0_stateless/01950_kill_large_group_by_query.reference b/tests/queries/0_stateless/01950_kill_large_group_by_query.reference
index 1602d6587ad8..f1df26588973 100644
--- a/tests/queries/0_stateless/01950_kill_large_group_by_query.reference
+++ b/tests/queries/0_stateless/01950_kill_large_group_by_query.reference
@@ -1,2 +1,2 @@
-finished	test_01948_tcp_default	default	SELECT * FROM
    (
        SELECT a.name as n
        FROM
        (
            SELECT \'Name\' as name, number FROM system.numbers LIMIT 2000000
        ) AS a,
        (
            SELECT \'Name\' as name, number FROM system.numbers LIMIT 2000000
        ) as b
        GROUP BY n
    )
    LIMIT 20
    FORMAT Null
-finished	test_01948_http_default	default	SELECT * FROM
    (
        SELECT a.name as n
        FROM
        (
            SELECT \'Name\' as name, number FROM system.numbers LIMIT 2000000
        ) AS a,
        (
            SELECT \'Name\' as name, number FROM system.numbers LIMIT 2000000
        ) as b
        GROUP BY n
    )
    LIMIT 20
    FORMAT Null
+finished	test_01948_tcp_default	default	SELECT * FROM
    (
        SELECT a.name as n
        FROM
        (
            SELECT \'Name\' as name, number FROM system.numbers LIMIT 2000000
        ) AS a,
        (
            SELECT \'Name\' as name2, number FROM system.numbers LIMIT 2000000
        ) as b
        GROUP BY n
    )
    LIMIT 20
    FORMAT Null
+finished	test_01948_http_default	default	SELECT * FROM
    (
        SELECT a.name as n
        FROM
        (
            SELECT \'Name\' as name, number FROM system.numbers LIMIT 2000000
        ) AS a,
        (
            SELECT \'Name\' as name2, number FROM system.numbers LIMIT 2000000
        ) as b
        GROUP BY n
    )
    LIMIT 20
    FORMAT Null
diff --git a/tests/queries/0_stateless/01950_kill_large_group_by_query.sh b/tests/queries/0_stateless/01950_kill_large_group_by_query.sh
index 465b923187e2..0b369c7257e0 100755
--- a/tests/queries/0_stateless/01950_kill_large_group_by_query.sh
+++ b/tests/queries/0_stateless/01950_kill_large_group_by_query.sh
@@ -23,7 +23,7 @@ $CLICKHOUSE_CLIENT --max_execution_time 10 --query_id "test_01948_tcp_$CLICKHOUS
             SELECT 'Name' as name, number FROM system.numbers LIMIT 2000000
         ) AS a,
         (
-            SELECT 'Name' as name, number FROM system.numbers LIMIT 2000000
+            SELECT 'Name' as name2, number FROM system.numbers LIMIT 2000000
         ) as b
         GROUP BY n
     )
@@ -44,7 +44,7 @@ ${CLICKHOUSE_CURL_COMMAND} -q --max-time 10 -sS "$CLICKHOUSE_URL&query_id=test_0
             SELECT 'Name' as name, number FROM system.numbers LIMIT 2000000
         ) AS a,
         (
-            SELECT 'Name' as name, number FROM system.numbers LIMIT 2000000
+            SELECT 'Name' as name2, number FROM system.numbers LIMIT 2000000
         ) as b
         GROUP BY n
     )
