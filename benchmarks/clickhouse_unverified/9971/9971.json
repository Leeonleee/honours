{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 9971,
  "instance_id": "ClickHouse__ClickHouse-9971",
  "issue_numbers": [
    "9965"
  ],
  "base_commit": "2d640a9bf821f9feb08c3fe5dbfde206b11a0c8b",
  "patch": "diff --git a/dbms/src/Processors/Pipe.cpp b/dbms/src/Processors/Pipe.cpp\nindex f3ffb6ee2017..4461d7142641 100644\n--- a/dbms/src/Processors/Pipe.cpp\n+++ b/dbms/src/Processors/Pipe.cpp\n@@ -63,6 +63,7 @@ Pipe::Pipe(ProcessorPtr source)\n         totals = &source->getOutputs().back();\n \n     processors.emplace_back(std::move(source));\n+    max_parallel_streams = 1;\n }\n \n Pipe::Pipe(Processors processors_, OutputPort * output_port_, OutputPort * totals_)\n@@ -82,6 +83,7 @@ Pipe::Pipe(Pipes && pipes, ProcessorPtr transform)\n         connect(*pipe.output_port, *it);\n         ++it;\n \n+        max_parallel_streams += pipe.max_parallel_streams;\n         processors.insert(processors.end(), pipe.processors.begin(), pipe.processors.end());\n     }\n \ndiff --git a/dbms/src/Processors/Pipe.h b/dbms/src/Processors/Pipe.h\nindex f30eaef678ff..60715d986aff 100644\n--- a/dbms/src/Processors/Pipe.h\n+++ b/dbms/src/Processors/Pipe.h\n@@ -50,6 +50,8 @@ class Pipe\n     void setTotalsPort(OutputPort * totals_) { totals = totals_; }\n     OutputPort * getTotalsPort() const { return totals; }\n \n+    size_t maxParallelStreams() const { return max_parallel_streams; }\n+\n     /// Do not allow to change the table while the processors of pipe are alive.\n     /// TODO: move it to pipeline.\n     void addTableLock(const TableStructureReadLockHolder & lock) { table_locks.push_back(lock); }\n@@ -66,6 +68,9 @@ class Pipe\n     OutputPort * output_port = nullptr;\n     OutputPort * totals = nullptr;\n \n+    /// It is the max number of processors which can be executed in parallel for each step. See QueryPipeline::Streams.\n+    size_t max_parallel_streams = 0;\n+\n     std::vector<TableStructureReadLockHolder> table_locks;\n \n     /// Some processors may implicitly use Context or temporary Storage created by Interpreter.\ndiff --git a/dbms/src/Processors/QueryPipeline.cpp b/dbms/src/Processors/QueryPipeline.cpp\nindex fe7d466cbe64..ee6938a48a6c 100644\n--- a/dbms/src/Processors/QueryPipeline.cpp\n+++ b/dbms/src/Processors/QueryPipeline.cpp\n@@ -98,7 +98,7 @@ void QueryPipeline::init(Pipes pipes)\n             totals.emplace_back(totals_port);\n         }\n \n-        streams.addStream(&pipe.getPort());\n+        streams.addStream(&pipe.getPort(), pipe.maxParallelStreams());\n         auto cur_processors = std::move(pipe).detachProcessors();\n         processors.insert(processors.end(), cur_processors.begin(), cur_processors.end());\n     }\n@@ -226,7 +226,7 @@ void QueryPipeline::addPipe(Processors pipe)\n     streams.reserve(last->getOutputs().size());\n     for (auto & output : last->getOutputs())\n     {\n-        streams.addStream(&output);\n+        streams.addStream(&output, 0);\n         if (header)\n             assertBlocksHaveEqualStructure(header, output.getHeader(), \"QueryPipeline\");\n         else\n@@ -245,7 +245,7 @@ void QueryPipeline::addDelayedStream(ProcessorPtr source)\n     assertBlocksHaveEqualStructure(current_header, source->getOutputs().front().getHeader(), \"QueryPipeline\");\n \n     IProcessor::PortNumbers delayed_streams = { streams.size() };\n-    streams.addStream(&source->getOutputs().front());\n+    streams.addStream(&source->getOutputs().front(), 0);\n     processors.emplace_back(std::move(source));\n \n     auto processor = std::make_shared<DelayedPortsProcessor>(current_header, streams.size(), delayed_streams);\n@@ -275,7 +275,7 @@ void QueryPipeline::resize(size_t num_streams, bool force, bool strict)\n     streams.clear();\n     streams.reserve(num_streams);\n     for (auto & output : resize->getOutputs())\n-        streams.addStream(&output);\n+        streams.addStream(&output, 0);\n \n     processors.emplace_back(std::move(resize));\n }\n@@ -645,6 +645,7 @@ Pipe QueryPipeline::getPipe() &&\n {\n     resize(1);\n     Pipe pipe(std::move(processors), streams.at(0), totals_having_port);\n+    pipe.max_parallel_streams = streams.maxParallelStreams();\n \n     for (auto & lock : table_locks)\n         pipe.addTableLock(lock);\ndiff --git a/dbms/src/Processors/QueryPipeline.h b/dbms/src/Processors/QueryPipeline.h\nindex e01087b717a7..9ce12e75b913 100644\n--- a/dbms/src/Processors/QueryPipeline.h\n+++ b/dbms/src/Processors/QueryPipeline.h\n@@ -38,16 +38,16 @@ class QueryPipeline\n         void clear() { data.clear(); }\n         void reserve(size_t size_) { data.reserve(size_); }\n \n-        void addStream(OutputPort * port)\n+        void addStream(OutputPort * port, size_t port_max_parallel_streams)\n         {\n             data.push_back(port);\n-            max_parallel_streams = std::max<size_t>(max_parallel_streams, data.size());\n+            max_parallel_streams = std::max<size_t>(max_parallel_streams + port_max_parallel_streams, data.size());\n         }\n \n         void addStreams(Streams & other)\n         {\n             data.insert(data.end(), other.begin(), other.end());\n-            max_parallel_streams = std::max<size_t>(max_parallel_streams, data.size());\n+            max_parallel_streams = std::max<size_t>(max_parallel_streams + other.max_parallel_streams, data.size());\n         }\n \n         void assign(std::initializer_list<OutputPort *> list)\n",
  "test_patch": "diff --git a/dbms/tests/performance/distributed_aggregation.xml b/dbms/tests/performance/distributed_aggregation.xml\nnew file mode 100644\nindex 000000000000..6fdd9fda7c5e\n--- /dev/null\n+++ b/dbms/tests/performance/distributed_aggregation.xml\n@@ -0,0 +1,18 @@\n+<test>\n+    <stop_conditions>\n+        <all_of>\n+            <iterations>10</iterations>\n+            <min_time_not_changing_for_ms>1000</min_time_not_changing_for_ms>\n+        </all_of>\n+        <any_of>\n+            <iterations>50</iterations>\n+            <total_time_ms>60000</total_time_ms>\n+        </any_of>\n+    </stop_conditions>\n+\n+    <query>select count() from (select sipHash64(zero) from zeros_mt(100000000) union all select sipHash64(zero) from zeros_mt(100000000))</query>\n+    <query>select count(sipHash64(zero)) from remote('127.0.0.{{1,1}}', zeros_mt(100000000))</query>\n+    <query>select count(sipHash64(zero)) from remote('127.0.0.{{1,2}}', zeros_mt(100000000))</query>\n+    <query>select count(sipHash64(zero)) from remote('127.0.0.{{2,3}}', zeros_mt(100000000))</query>\n+\n+</test>\n",
  "problem_statement": "processors distribued tables slow down\nschema:\r\n```sql\r\ncreate table data engine=MergeTree order by tuple() as select number, randomPrintableASCII(1024) as _raw from numbers(10000000);\r\ncreate table data_dist Engine=Distributed('test_cluster_two_shards','default','data') as data;\r\n```\r\n\r\nquery:\r\n```sql\r\nselect count( extract(_raw, '([ab]{10})') ) from data_dist ;\r\n```\r\n\r\nset experimental_use_processors=1;\r\nElapsed: 21.389 sec.\r\n\r\nset experimental_use_processors=0;\r\nElapsed: 10.924 sec.\r\n\n",
  "hints_text": "",
  "created_at": "2020-03-31T16:41:15Z",
  "modified_files": [
    "dbms/src/Processors/Pipe.cpp",
    "dbms/src/Processors/Pipe.h",
    "dbms/src/Processors/QueryPipeline.cpp",
    "dbms/src/Processors/QueryPipeline.h"
  ],
  "modified_test_files": [
    "b/dbms/tests/performance/distributed_aggregation.xml"
  ]
}