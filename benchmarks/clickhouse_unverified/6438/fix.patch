diff --git a/dbms/src/Storages/MergeTree/MergeTreeData.cpp b/dbms/src/Storages/MergeTree/MergeTreeData.cpp
index 8af8284efdd4..44e610b105c2 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeData.cpp
+++ b/dbms/src/Storages/MergeTree/MergeTreeData.cpp
@@ -734,112 +734,135 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks)
         part_file_names.push_back(it.name());
     }
 
+    auto part_lock = lockParts();
+    data_parts_indexes.clear();
+
+    if (part_file_names.empty())
+    {
+        LOG_DEBUG(log, "There is no data parts");
+        return;
+    }
+
+    /// Parallel loading of data parts.
+    size_t num_threads = std::min(size_t(settings.max_part_loading_threads), part_file_names.size());
+
+    std::mutex mutex;
+
     DataPartsVector broken_parts_to_remove;
     DataPartsVector broken_parts_to_detach;
     size_t suspicious_broken_parts = 0;
 
-    auto lock = lockParts();
-    data_parts_indexes.clear();
+    std::atomic<bool> has_adaptive_parts = false;
+    std::atomic<bool> has_non_adaptive_parts = false;
+
+    ThreadPool pool(num_threads);
 
-    bool has_adaptive_parts = false;
-    bool has_non_adaptive_parts = false;
     for (const String & file_name : part_file_names)
     {
-        MergeTreePartInfo part_info;
-        if (!MergeTreePartInfo::tryParsePartName(file_name, &part_info, format_version))
-            continue;
-
-        MutableDataPartPtr part = std::make_shared<DataPart>(*this, file_name, part_info);
-        part->relative_path = file_name;
-        bool broken = false;
-
-        try
-        {
-            part->loadColumnsChecksumsIndexes(require_part_metadata, true);
-        }
-        catch (const Exception & e)
-        {
-            /// Don't count the part as broken if there is not enough memory to load it.
-            /// In fact, there can be many similar situations.
-            /// But it is OK, because there is a safety guard against deleting too many parts.
-            if (e.code() == ErrorCodes::MEMORY_LIMIT_EXCEEDED
-                || e.code() == ErrorCodes::CANNOT_ALLOCATE_MEMORY
-                || e.code() == ErrorCodes::CANNOT_MUNMAP
-                || e.code() == ErrorCodes::CANNOT_MREMAP)
-                throw;
-
-            broken = true;
-            tryLogCurrentException(__PRETTY_FUNCTION__);
-        }
-        catch (...)
+        pool.schedule([&]
         {
-            broken = true;
-            tryLogCurrentException(__PRETTY_FUNCTION__);
-        }
+            MergeTreePartInfo part_info;
+            if (!MergeTreePartInfo::tryParsePartName(file_name, &part_info, format_version))
+                return;
 
-        /// Ignore and possibly delete broken parts that can appear as a result of hard server restart.
-        if (broken)
-        {
-            if (part->info.level == 0)
+            MutableDataPartPtr part = std::make_shared<DataPart>(*this, file_name, part_info);
+            part->relative_path = file_name;
+            bool broken = false;
+
+            try
             {
-                /// It is impossible to restore level 0 parts.
-                LOG_ERROR(log, "Considering to remove broken part " << full_path + file_name << " because it's impossible to repair.");
-                broken_parts_to_remove.push_back(part);
+                part->loadColumnsChecksumsIndexes(require_part_metadata, true);
             }
-            else
+            catch (const Exception & e)
             {
-                /// Count the number of parts covered by the broken part. If it is at least two, assume that
-                /// the broken part was created as a result of merging them and we won't lose data if we
-                /// delete it.
-                size_t contained_parts = 0;
-
-                LOG_ERROR(log, "Part " << full_path + file_name << " is broken. Looking for parts to replace it.");
+                /// Don't count the part as broken if there is not enough memory to load it.
+                /// In fact, there can be many similar situations.
+                /// But it is OK, because there is a safety guard against deleting too many parts.
+                if (e.code() == ErrorCodes::MEMORY_LIMIT_EXCEEDED
+                    || e.code() == ErrorCodes::CANNOT_ALLOCATE_MEMORY
+                    || e.code() == ErrorCodes::CANNOT_MUNMAP
+                    || e.code() == ErrorCodes::CANNOT_MREMAP)
+                    throw;
+
+                broken = true;
+                tryLogCurrentException(__PRETTY_FUNCTION__);
+            }
+            catch (...)
+            {
+                broken = true;
+                tryLogCurrentException(__PRETTY_FUNCTION__);
+            }
 
-                for (const String & contained_name : part_file_names)
+            /// Ignore and possibly delete broken parts that can appear as a result of hard server restart.
+            if (broken)
+            {
+                if (part->info.level == 0)
+                {
+                    /// It is impossible to restore level 0 parts.
+                    LOG_ERROR(log, "Considering to remove broken part " << full_path << file_name << " because it's impossible to repair.");
+                    std::lock_guard loading_lock(mutex);
+                    broken_parts_to_remove.push_back(part);
+                }
+                else
                 {
-                    if (contained_name == file_name)
-                        continue;
+                    /// Count the number of parts covered by the broken part. If it is at least two, assume that
+                    /// the broken part was created as a result of merging them and we won't lose data if we
+                    /// delete it.
+                    size_t contained_parts = 0;
 
-                    MergeTreePartInfo contained_part_info;
-                    if (!MergeTreePartInfo::tryParsePartName(contained_name, &contained_part_info, format_version))
-                        continue;
+                    LOG_ERROR(log, "Part " << full_path << file_name << " is broken. Looking for parts to replace it.");
 
-                    if (part->info.contains(contained_part_info))
+                    for (const String & contained_name : part_file_names)
                     {
-                        LOG_ERROR(log, "Found part " << full_path + contained_name);
-                        ++contained_parts;
+                        if (contained_name == file_name)
+                            continue;
+
+                        MergeTreePartInfo contained_part_info;
+                        if (!MergeTreePartInfo::tryParsePartName(contained_name, &contained_part_info, format_version))
+                            continue;
+
+                        if (part->info.contains(contained_part_info))
+                        {
+                            LOG_ERROR(log, "Found part " << full_path << contained_name);
+                            ++contained_parts;
+                        }
                     }
-                }
 
-                if (contained_parts >= 2)
-                {
-                    LOG_ERROR(log, "Considering to remove broken part " << full_path + file_name << " because it covers at least 2 other parts");
-                    broken_parts_to_remove.push_back(part);
-                }
-                else
-                {
-                    LOG_ERROR(log, "Detaching broken part " << full_path + file_name
-                        << " because it covers less than 2 parts. You need to resolve this manually");
-                    broken_parts_to_detach.push_back(part);
-                    ++suspicious_broken_parts;
+                    if (contained_parts >= 2)
+                    {
+                        LOG_ERROR(log, "Considering to remove broken part " << full_path << file_name << " because it covers at least 2 other parts");
+                        std::lock_guard loading_lock(mutex);
+                        broken_parts_to_remove.push_back(part);
+                    }
+                    else
+                    {
+                        LOG_ERROR(log, "Detaching broken part " << full_path << file_name
+                            << " because it covers less than 2 parts. You need to resolve this manually");
+                        std::lock_guard loading_lock(mutex);
+                        broken_parts_to_detach.push_back(part);
+                        ++suspicious_broken_parts;
+                    }
                 }
-            }
 
-            continue;
-        }
-        if (!part->index_granularity_info.is_adaptive)
-            has_non_adaptive_parts = true;
-        else
-            has_adaptive_parts = true;
+                return;
+            }
+            if (!part->index_granularity_info.is_adaptive)
+                has_non_adaptive_parts.store(true, std::memory_order_relaxed);
+            else
+                has_adaptive_parts.store(true, std::memory_order_relaxed);
 
-        part->modification_time = Poco::File(full_path + file_name).getLastModified().epochTime();
-        /// Assume that all parts are Committed, covered parts will be detected and marked as Outdated later
-        part->state = DataPartState::Committed;
+            part->modification_time = Poco::File(full_path + file_name).getLastModified().epochTime();
+            /// Assume that all parts are Committed, covered parts will be detected and marked as Outdated later
+            part->state = DataPartState::Committed;
 
-        if (!data_parts_indexes.insert(part).second)
-            throw Exception("Part " + part->name + " already exists", ErrorCodes::DUPLICATE_DATA_PART);
+            std::lock_guard loading_lock(mutex);
+            if (!data_parts_indexes.insert(part).second)
+                throw Exception("Part " + part->name + " already exists", ErrorCodes::DUPLICATE_DATA_PART);
+        });
     }
 
+    pool.wait();
+
     if (has_non_adaptive_parts && has_adaptive_parts && !settings.enable_mixed_granularity_parts)
         throw Exception("Table contains parts with adaptive and non adaptive marks, but `setting enable_mixed_granularity_parts` is disabled", ErrorCodes::LOGICAL_ERROR);
 
@@ -1063,15 +1086,40 @@ void MergeTreeData::removePartsFinally(const MergeTreeData::DataPartsVector & pa
 
 void MergeTreeData::clearOldPartsFromFilesystem()
 {
-    auto parts_to_remove = grabOldParts();
+    DataPartsVector parts_to_remove = grabOldParts();
+    clearPartsFromFilesystem(parts_to_remove);
+    removePartsFinally(parts_to_remove);
+}
 
-    for (const DataPartPtr & part : parts_to_remove)
+void MergeTreeData::clearPartsFromFilesystem(const DataPartsVector & parts_to_remove)
+{
+    if (parts_to_remove.size() > 1 && settings.max_part_removal_threads > 1 && parts_to_remove.size() > settings.concurrent_part_removal_threshold)
     {
-        LOG_DEBUG(log, "Removing part from filesystem " << part->name);
-        part->remove();
-    }
+        /// Parallel parts removal.
 
-    removePartsFinally(parts_to_remove);
+        size_t num_threads = std::min(size_t(settings.max_part_removal_threads), parts_to_remove.size());
+        ThreadPool pool(num_threads);
+
+        /// NOTE: Under heavy system load you may get "Cannot schedule a task" from ThreadPool.
+        for (const DataPartPtr & part : parts_to_remove)
+        {
+            pool.schedule([&]
+            {
+                LOG_DEBUG(log, "Removing part from filesystem " << part->name);
+                part->remove();
+            });
+        }
+
+        pool.wait();
+    }
+    else
+    {
+        for (const DataPartPtr & part : parts_to_remove)
+        {
+            LOG_DEBUG(log, "Removing part from filesystem " << part->name);
+            part->remove();
+        }
+    }
 }
 
 void MergeTreeData::setPath(const String & new_full_path)
@@ -1093,6 +1141,8 @@ void MergeTreeData::dropAllData()
 
     LOG_TRACE(log, "dropAllData: removing data from memory.");
 
+    DataPartsVector all_parts(data_parts_by_info.begin(), data_parts_by_info.end());
+
     data_parts_indexes.clear();
     column_sizes.clear();
 
@@ -1101,8 +1151,7 @@ void MergeTreeData::dropAllData()
     LOG_TRACE(log, "dropAllData: removing data from filesystem.");
 
     /// Removing of each data part before recursive removal of directory is to speed-up removal, because there will be less number of syscalls.
-    for (DataPartPtr part : data_parts_by_info) /// a copy intended
-        part->remove();
+    clearPartsFromFilesystem(all_parts);
 
     Poco::File(full_path).remove(true);
 
diff --git a/dbms/src/Storages/MergeTree/MergeTreeData.h b/dbms/src/Storages/MergeTree/MergeTreeData.h
index c6b85e6d98fe..b43851eb7d9a 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeData.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeData.h
@@ -477,6 +477,7 @@ class MergeTreeData : public IStorage
 
     /// Delete irrelevant parts from memory and disk.
     void clearOldPartsFromFilesystem();
+    void clearPartsFromFilesystem(const DataPartsVector & parts);
 
     /// Delete all directories which names begin with "tmp"
     /// Set non-negative parameter value to override MergeTreeSettings temporary_directories_lifetime
diff --git a/dbms/src/Storages/MergeTree/MergeTreeSettings.h b/dbms/src/Storages/MergeTree/MergeTreeSettings.h
index 9bd58e77f9c4..e670000ecc5b 100644
--- a/dbms/src/Storages/MergeTree/MergeTreeSettings.h
+++ b/dbms/src/Storages/MergeTree/MergeTreeSettings.h
@@ -80,7 +80,10 @@ struct MergeTreeSettings : public SettingsCollection<MergeTreeSettings>
     M(SettingUInt64, index_granularity_bytes, 10 * 1024 * 1024, "Approximate amount of bytes in single granule (0 - disabled).") \
     M(SettingInt64, merge_with_ttl_timeout, 3600 * 24, "Minimal time in seconds, when merge with TTL can be repeated.") \
     M(SettingBool, write_final_mark, 1, "Write final mark after end of column (0 - disabled, do nothing if index_granularity_bytes=0)") \
-    M(SettingBool, enable_mixed_granularity_parts, 0, "Enable parts with adaptive and non adaptive granularity")
+    M(SettingBool, enable_mixed_granularity_parts, 0, "Enable parts with adaptive and non adaptive granularity") \
+    M(SettingMaxThreads, max_part_loading_threads, 0, "The number of theads to load data parts at startup.") \
+    M(SettingMaxThreads, max_part_removal_threads, 0, "The number of theads for concurrent removal of inactive data parts. One is usually enough, but in 'Google Compute Environment SSD Persistent Disks' file removal (unlink) operation is extraordinarily slow and you probably have to increase this number (recommended is up to 16).") \
+    M(SettingUInt64, concurrent_part_removal_threshold, 100, "Activate concurrent part removal (see 'max_part_removal_threads') only if the number of inactive data parts is at least this.") \
 
     DECLARE_SETTINGS_COLLECTION(LIST_OF_MERGE_TREE_SETTINGS)
 
diff --git a/dbms/src/Storages/StorageMergeTree.cpp b/dbms/src/Storages/StorageMergeTree.cpp
index 83bfadc482b3..bb5e4c7f869f 100644
--- a/dbms/src/Storages/StorageMergeTree.cpp
+++ b/dbms/src/Storages/StorageMergeTree.cpp
@@ -761,7 +761,7 @@ BackgroundProcessingPoolTaskResult StorageMergeTree::backgroundTask()
         else
             return BackgroundProcessingPoolTaskResult::ERROR;
     }
-    catch (Exception & e)
+    catch (const Exception & e)
     {
         if (e.code() == ErrorCodes::ABORTED)
         {
