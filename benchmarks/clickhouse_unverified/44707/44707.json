{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 44707,
  "instance_id": "ClickHouse__ClickHouse-44707",
  "issue_numbers": [
    "44443"
  ],
  "base_commit": "d12f5cb119b1a4a8033ef1b298a1f2a0d62468c2",
  "patch": "diff --git a/src/IO/WriteBufferFromS3.cpp b/src/IO/WriteBufferFromS3.cpp\nindex 37bc8c78cf4d..ec77fc44de63 100644\n--- a/src/IO/WriteBufferFromS3.cpp\n+++ b/src/IO/WriteBufferFromS3.cpp\n@@ -18,6 +18,7 @@\n #include <aws/s3/model/PutObjectRequest.h>\n #include <aws/s3/model/UploadPartRequest.h>\n #include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/StorageClass.h>\n \n #include <utility>\n \n@@ -473,6 +474,8 @@ void WriteBufferFromS3::fillPutRequest(Aws::S3::Model::PutObjectRequest & req)\n     req.SetBody(temporary_buffer);\n     if (object_metadata.has_value())\n         req.SetMetadata(object_metadata.value());\n+    if (!settings.storage_class_name.empty())\n+        req.SetStorageClass(Aws::S3::Model::StorageClassMapper::GetStorageClassForName(settings.storage_class_name));\n \n     /// If we don't do it, AWS SDK can mistakenly set it to application/xml, see https://github.com/aws/aws-sdk-cpp/issues/1840\n     req.SetContentType(\"binary/octet-stream\");\ndiff --git a/src/Storages/StorageS3Settings.cpp b/src/Storages/StorageS3Settings.cpp\nindex 8c1974527b63..9ae08ef41596 100644\n--- a/src/Storages/StorageS3Settings.cpp\n+++ b/src/Storages/StorageS3Settings.cpp\n@@ -40,6 +40,10 @@ S3Settings::RequestSettings::PartUploadSettings::PartUploadSettings(\n     max_single_part_upload_size = config.getUInt64(key + \"max_single_part_upload_size\", max_single_part_upload_size);\n     max_single_operation_copy_size = config.getUInt64(key + \"max_single_operation_copy_size\", max_single_operation_copy_size);\n \n+    /// This configuration is only applicable to s3. Other types of object storage are not applicable or have different meanings.\n+    storage_class_name = config.getString(config_prefix + \".s3_storage_class\", storage_class_name);\n+    storage_class_name = Poco::toUpperInPlace(storage_class_name);\n+\n     validate();\n }\n \n@@ -50,6 +54,10 @@ S3Settings::RequestSettings::PartUploadSettings::PartUploadSettings(const NamedC\n     upload_part_size_multiply_parts_count_threshold = collection.getOrDefault<UInt64>(\"upload_part_size_multiply_parts_count_threshold\", upload_part_size_multiply_parts_count_threshold);\n     max_single_part_upload_size = collection.getOrDefault<UInt64>(\"max_single_part_upload_size\", max_single_part_upload_size);\n \n+    /// This configuration is only applicable to s3. Other types of object storage are not applicable or have different meanings.\n+    storage_class_name = collection.getOrDefault<String>(\"s3_storage_class\", storage_class_name);\n+    storage_class_name = Poco::toUpperInPlace(storage_class_name);\n+\n     validate();\n }\n \n@@ -137,6 +145,13 @@ void S3Settings::RequestSettings::PartUploadSettings::validate()\n             \"Setting upload_part_size_multiply_factor is too big ({}). Multiplication to max_upload_part_size ({}) will cause integer overflow\",\n             ReadableSize(max_part_number), ReadableSize(max_part_number_limit));\n \n+    std::unordered_set<String> storage_class_names {\"STANDARD\", \"INTELLIGENT_TIERING\"};\n+    if (!storage_class_name.empty() && !storage_class_names.contains(storage_class_name))\n+        throw Exception(\n+            ErrorCodes::INVALID_SETTING_VALUE,\n+            \"Setting storage_class has invalid value {} which only supports STANDARD and INTELLIGENT_TIERING\",\n+            storage_class_name);\n+\n     /// TODO: it's possible to set too small limits. We can check that max possible object size is not too small.\n }\n \ndiff --git a/src/Storages/StorageS3Settings.h b/src/Storages/StorageS3Settings.h\nindex 368fcfaf469c..6fa74f4f0d01 100644\n--- a/src/Storages/StorageS3Settings.h\n+++ b/src/Storages/StorageS3Settings.h\n@@ -36,6 +36,7 @@ struct S3Settings\n             size_t max_part_number = 10000;\n             size_t max_single_part_upload_size = 32 * 1024 * 1024;\n             size_t max_single_operation_copy_size = 5ULL * 1024 * 1024 * 1024;\n+            String storage_class_name;\n \n             void updateFromSettings(const Settings & settings) { updateFromSettingsImpl(settings, true); }\n             void validate();\n",
  "test_patch": "diff --git a/tests/integration/test_s3_storage_class/__init__.py b/tests/integration/test_s3_storage_class/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_s3_storage_class/configs/config.d/minio.xml b/tests/integration/test_s3_storage_class/configs/config.d/minio.xml\nnew file mode 100644\nindex 000000000000..6a7c2d684be4\n--- /dev/null\n+++ b/tests/integration/test_s3_storage_class/configs/config.d/minio.xml\n@@ -0,0 +1,25 @@\n+<?xml version=\"1.0\"?>\n+\n+<!-- Using named collections 22.4+ -->\n+<clickhouse>\n+    <storage_configuration>\n+        <disks>\n+            <use_s3_storage_class>\n+                <type>s3</type>\n+                <endpoint>http://minio1:9001/root/data/</endpoint>\n+                <access_key_id>minio</access_key_id>\n+                <secret_access_key>minio123</secret_access_key>\n+                <s3_storage_class>STANDARD</s3_storage_class>\n+            </use_s3_storage_class>\n+        </disks>\n+        <policies>\n+            <use_s3_storage_class>\n+                <volumes>\n+                    <main>\n+                        <disk>use_s3_storage_class</disk>\n+                    </main>\n+                </volumes>\n+            </use_s3_storage_class>\n+        </policies>\n+    </storage_configuration>\n+</clickhouse>\ndiff --git a/tests/integration/test_s3_storage_class/test.py b/tests/integration/test_s3_storage_class/test.py\nnew file mode 100644\nindex 000000000000..f28ae189c921\n--- /dev/null\n+++ b/tests/integration/test_s3_storage_class/test.py\n@@ -0,0 +1,52 @@\n+import os\n+import logging\n+\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+\n+logging.getLogger().setLevel(logging.INFO)\n+logging.getLogger().addHandler(logging.StreamHandler())\n+\n+cluster = ClickHouseCluster(__file__)\n+node = cluster.add_instance(\n+    \"node\",\n+    main_configs=[\"configs/config.d/minio.xml\"],\n+    stay_alive=True,\n+    with_minio=True,\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_s3_storage_class_right(started_cluster):\n+    node.query(\n+        \"\"\"\n+            CREATE TABLE test_s3_storage_class\n+            (\n+                `id` UInt64,\n+                `value` String\n+            )\n+            ENGINE = MergeTree\n+            ORDER BY id\n+            SETTINGS storage_policy='use_s3_storage_class';\n+        \"\"\",\n+    )\n+    node.query(\n+        \"\"\"\n+            INSERT INTO test_s3_storage_class VALUES (1, 'a');\n+        \"\"\",\n+    )\n+    result = node.query(\n+        \"\"\"\n+            SELECT id FROM test_s3_storage_class;\n+        \"\"\"\n+    )\n+\n+    assert result == \"1\\n\"\n",
  "problem_statement": "Possibillity to configure S3 Storage class \nThere are different tiers in S3 with different pricing, see https://aws.amazon.com/s3/storage-classes/  \r\n\r\nBy default, objects in S3 are created with the default Standard tier.\r\n\r\n> It is possible to create a policy on the bucket to move them to Intelligent Tiering after 30 days, however we found with other projects that we can save quite a lot by creating them in the Intelligent Tier. The only way to do that is set the storage class when creating the object (a lifecycle policy only allows to change it after 30 days), which we don\u2019t have control over.\r\n> It\u2019s a simple change in the PutObjectRequest that could be exposed via configuration of the S3 disk settings, and default to Standard\r\n\r\nSee [`SetStorageClass`](https://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_s3_1_1_model_1_1_put_object_request.html#a6cf76adbf31f1ee08ca20d7c8bb8459a)\r\n\r\nAnd\r\nhttps://github.com/ClickHouse/ClickHouse/blob/ce56ae61c68e23512cc8ec3bfe72f5c935f12379/src/IO/WriteBufferFromS3.cpp#L468\r\n\r\n\r\n\r\n\n",
  "hints_text": "Hi Team, can i pick this task.",
  "created_at": "2022-12-29T12:45:25Z",
  "modified_files": [
    "src/IO/WriteBufferFromS3.cpp",
    "src/Storages/StorageS3Settings.cpp",
    "src/Storages/StorageS3Settings.h"
  ],
  "modified_test_files": [
    "b/tests/integration/test_s3_storage_class/configs/config.d/minio.xml",
    "b/tests/integration/test_s3_storage_class/test.py"
  ]
}