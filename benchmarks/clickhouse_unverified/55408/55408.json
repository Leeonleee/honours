{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 55408,
  "instance_id": "ClickHouse__ClickHouse-55408",
  "issue_numbers": [
    "54318"
  ],
  "base_commit": "ec652c92b88fee6dac9e8b654d4b1055e9a02663",
  "patch": "diff --git a/src/Access/LDAPAccessStorage.h b/src/Access/LDAPAccessStorage.h\nindex 21413070f4c3..478a91a86d33 100644\n--- a/src/Access/LDAPAccessStorage.h\n+++ b/src/Access/LDAPAccessStorage.h\n@@ -61,7 +61,7 @@ class LDAPAccessStorage : public IAccessStorage\n     bool areLDAPCredentialsValidNoLock(const User & user, const Credentials & credentials,\n         const ExternalAuthenticators & external_authenticators, LDAPClient::SearchResultsList & role_search_results) const;\n \n-    mutable std::recursive_mutex mutex;\n+    mutable std::recursive_mutex mutex; // Note: Reentrace possible by internal role lookup via access_control\n     AccessControl & access_control;\n     String ldap_server_name;\n     LDAPClient::RoleSearchParamsList role_search_params;\ndiff --git a/src/Access/MemoryAccessStorage.h b/src/Access/MemoryAccessStorage.h\nindex b63132147da2..5282314b33d4 100644\n--- a/src/Access/MemoryAccessStorage.h\n+++ b/src/Access/MemoryAccessStorage.h\n@@ -48,12 +48,12 @@ class MemoryAccessStorage : public IAccessStorage\n     bool removeImpl(const UUID & id, bool throw_if_not_exists) override;\n     bool updateImpl(const UUID & id, const UpdateFunc & update_func, bool throw_if_not_exists) override;\n \n-    bool insertNoLock(const UUID & id, const AccessEntityPtr & entity, bool replace_if_exists, bool throw_if_exists) TSA_REQUIRES(mutex);\n-    bool removeNoLock(const UUID & id, bool throw_if_not_exists) TSA_REQUIRES(mutex);\n-    bool updateNoLock(const UUID & id, const UpdateFunc & update_func, bool throw_if_not_exists) TSA_REQUIRES(mutex);\n+    bool insertNoLock(const UUID & id, const AccessEntityPtr & entity, bool replace_if_exists, bool throw_if_exists);\n+    bool removeNoLock(const UUID & id, bool throw_if_not_exists);\n+    bool updateNoLock(const UUID & id, const UpdateFunc & update_func, bool throw_if_not_exists);\n \n-    void removeAllExceptNoLock(const std::vector<UUID> & ids_to_keep) TSA_REQUIRES(mutex);\n-    void removeAllExceptNoLock(const boost::container::flat_set<UUID> & ids_to_keep) TSA_REQUIRES(mutex);\n+    void removeAllExceptNoLock(const std::vector<UUID> & ids_to_keep);\n+    void removeAllExceptNoLock(const boost::container::flat_set<UUID> & ids_to_keep);\n \n     struct Entry\n     {\n@@ -61,9 +61,9 @@ class MemoryAccessStorage : public IAccessStorage\n         AccessEntityPtr entity;\n     };\n \n-    mutable std::mutex mutex;\n-    std::unordered_map<UUID, Entry> entries_by_id TSA_GUARDED_BY(mutex); /// We want to search entries both by ID and by the pair of name and type.\n-    std::unordered_map<String, Entry *> entries_by_name_and_type[static_cast<size_t>(AccessEntityType::MAX)] TSA_GUARDED_BY(mutex);\n+    mutable std::recursive_mutex mutex; // Note: Reentrace possible via LDAPAccessStorage\n+    std::unordered_map<UUID, Entry> entries_by_id; /// We want to search entries both by ID and by the pair of name and type.\n+    std::unordered_map<String, Entry *> entries_by_name_and_type[static_cast<size_t>(AccessEntityType::MAX)];\n     AccessChangesNotifier & changes_notifier;\n     const bool backup_allowed = false;\n };\n",
  "test_patch": "diff --git a/docker/test/integration/runner/compose/docker_compose_ldap.yml b/docker/test/integration/runner/compose/docker_compose_ldap.yml\nnew file mode 100644\nindex 000000000000..857f8fdce624\n--- /dev/null\n+++ b/docker/test/integration/runner/compose/docker_compose_ldap.yml\n@@ -0,0 +1,16 @@\n+version: '2.3'\n+services:\n+    openldap:\n+        image: bitnami/openldap:2.6.6\n+        restart: always\n+        environment:\n+            LDAP_ROOT: dc=example,dc=org\n+            LDAP_ADMIN_DN: cn=admin,dc=example,dc=org\n+            LDAP_ADMIN_USERNAME: admin\n+            LDAP_ADMIN_PASSWORD: clickhouse\n+            LDAP_USER_DC: users\n+            LDAP_USERS: janedoe,johndoe\n+            LDAP_PASSWORDS: qwerty,qwertz\n+            LDAP_PORT_NUMBER: ${LDAP_INTERNAL_PORT:-1389}\n+        ports:\n+            - ${LDAP_EXTERNAL_PORT:-1389}:${LDAP_INTERNAL_PORT:-1389}\ndiff --git a/tests/integration/helpers/cluster.py b/tests/integration/helpers/cluster.py\nindex a9a996e0a5fd..4f14e3543da0 100644\n--- a/tests/integration/helpers/cluster.py\n+++ b/tests/integration/helpers/cluster.py\n@@ -426,6 +426,7 @@ def __init__(\n         self.with_net_trics = False\n         self.with_redis = False\n         self.with_cassandra = False\n+        self.with_ldap = False\n         self.with_jdbc_bridge = False\n         self.with_nginx = False\n         self.with_hive = False\n@@ -508,6 +509,13 @@ def __init__(\n         self.cassandra_ip = None\n         self.cassandra_id = self.get_instance_docker_id(self.cassandra_host)\n \n+        # available when with_ldap == True\n+        self.ldap_host = \"openldap\"\n+        self.ldap_ip = None\n+        self.ldap_container = None\n+        self.ldap_port = 1389\n+        self.ldap_id = self.get_instance_docker_id(self.ldap_host)\n+\n         # available when with_rabbitmq == True\n         self.rabbitmq_host = \"rabbitmq1\"\n         self.rabbitmq_ip = None\n@@ -1347,6 +1355,23 @@ def setup_cassandra_cmd(self, instance, env_variables, docker_compose_yml_dir):\n         ]\n         return self.base_cassandra_cmd\n \n+    def setup_ldap_cmd(self, instance, env_variables, docker_compose_yml_dir):\n+        self.with_ldap = True\n+        env_variables[\"LDAP_EXTERNAL_PORT\"] = str(self.ldap_port)\n+        self.base_cmd.extend(\n+            [\"--file\", p.join(docker_compose_yml_dir, \"docker_compose_ldap.yml\")]\n+        )\n+        self.base_ldap_cmd = [\n+            \"docker-compose\",\n+            \"--env-file\",\n+            instance.env_file,\n+            \"--project-name\",\n+            self.project_name,\n+            \"--file\",\n+            p.join(docker_compose_yml_dir, \"docker_compose_ldap.yml\"),\n+        ]\n+        return self.base_ldap_cmd\n+\n     def setup_jdbc_bridge_cmd(self, instance, env_variables, docker_compose_yml_dir):\n         self.with_jdbc_bridge = True\n         env_variables[\"JDBC_DRIVER_LOGS\"] = self.jdbc_driver_logs_dir\n@@ -1432,6 +1457,7 @@ def add_instance(\n         with_minio=False,\n         with_azurite=False,\n         with_cassandra=False,\n+        with_ldap=False,\n         with_jdbc_bridge=False,\n         with_hive=False,\n         with_coredns=False,\n@@ -1521,6 +1547,7 @@ def add_instance(\n             with_jdbc_bridge=with_jdbc_bridge,\n             with_hive=with_hive,\n             with_coredns=with_coredns,\n+            with_ldap=with_ldap,\n             server_bin_path=self.server_bin_path,\n             odbc_bridge_bin_path=self.odbc_bridge_bin_path,\n             library_bridge_bin_path=self.library_bridge_bin_path,\n@@ -1748,6 +1775,11 @@ def add_instance(\n                 )\n             )\n \n+        if with_ldap and not self.with_ldap:\n+            cmds.append(\n+                self.setup_ldap_cmd(instance, env_variables, docker_compose_yml_dir)\n+            )\n+\n         if with_jdbc_bridge and not self.with_jdbc_bridge:\n             cmds.append(\n                 self.setup_jdbc_bridge_cmd(\n@@ -2448,6 +2480,32 @@ def wait_cassandra_to_start(self, timeout=180):\n \n         raise Exception(\"Can't wait Cassandra to start\")\n \n+    def wait_ldap_to_start(self, timeout=180):\n+        self.ldap_ip = self.get_instance_ip(self.ldap_host)\n+        self.ldap_container = self.get_docker_handle(self.ldap_id)\n+        start = time.time()\n+        while time.time() - start < timeout:\n+            try:\n+                logging.info(\n+                    f\"Check LDAP Online {self.ldap_id} {self.ldap_ip} {self.ldap_port}\"\n+                )\n+                self.exec_in_container(\n+                    self.ldap_id,\n+                    [\n+                        \"bash\",\n+                        \"-c\",\n+                        f\"/opt/bitnami/openldap/bin/ldapsearch -x -H ldap://{self.ldap_ip}:{self.ldap_port} -D cn=admin,dc=example,dc=org -w clickhouse -b dc=example,dc=org\",\n+                    ],\n+                    user=\"root\",\n+                )\n+                logging.info(\"LDAP Online\")\n+                return\n+            except Exception as ex:\n+                logging.warning(\"Can't connect to LDAP: %s\", str(ex))\n+                time.sleep(1)\n+\n+        raise Exception(\"Can't wait LDAP to start\")\n+\n     def start(self):\n         pytest_xdist_logging_to_separate_files.setup()\n         logging.info(\"Running tests in {}\".format(self.base_path))\n@@ -2776,6 +2834,11 @@ def start(self):\n                 self.up_called = True\n                 self.wait_cassandra_to_start()\n \n+            if self.with_ldap and self.base_ldap_cmd:\n+                subprocess_check_call(self.base_ldap_cmd + [\"up\", \"-d\"])\n+                self.up_called = True\n+                self.wait_ldap_to_start()\n+\n             if self.with_jdbc_bridge and self.base_jdbc_bridge_cmd:\n                 os.makedirs(self.jdbc_driver_logs_dir)\n                 os.chmod(self.jdbc_driver_logs_dir, stat.S_IRWXU | stat.S_IRWXO)\n@@ -3064,6 +3127,7 @@ def __init__(\n         with_hive,\n         with_coredns,\n         with_cassandra,\n+        with_ldap,\n         server_bin_path,\n         odbc_bridge_bin_path,\n         library_bridge_bin_path,\n@@ -3146,6 +3210,7 @@ def __init__(\n         self.with_minio = with_minio\n         self.with_azurite = with_azurite\n         self.with_cassandra = with_cassandra\n+        self.with_ldap = with_ldap\n         self.with_jdbc_bridge = with_jdbc_bridge\n         self.with_hive = with_hive\n         self.with_coredns = with_coredns\ndiff --git a/tests/integration/test_ldap_external_user_directory/__init__.py b/tests/integration/test_ldap_external_user_directory/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_ldap_external_user_directory/configs/ldap_with_role_mapping.xml b/tests/integration/test_ldap_external_user_directory/configs/ldap_with_role_mapping.xml\nnew file mode 100644\nindex 000000000000..28b8f7d00308\n--- /dev/null\n+++ b/tests/integration/test_ldap_external_user_directory/configs/ldap_with_role_mapping.xml\n@@ -0,0 +1,22 @@\n+<clickhouse>\n+    <ldap_servers>\n+        <openldap>\n+            <host>openldap</host>\n+            <port>1389</port>\n+            <bind_dn>cn={user_name},ou=users,dc=example,dc=org</bind_dn>\n+            <enable_tls>no</enable_tls>\n+        </openldap>\n+    </ldap_servers>\n+    <user_directories>\n+        <ldap>\n+            <server>openldap</server>\n+            <role_mapping>\n+                <base_dn>dc=example,dc=org</base_dn>\n+                <scope>subtree</scope>\n+                <search_filter>(&amp;(objectClass=groupOfNames)(member={bind_dn}))</search_filter>\n+                <attribute>cn</attribute>\n+                <prefix>clickhouse-</prefix>\n+            </role_mapping>\n+        </ldap>\n+    </user_directories>\n+</clickhouse>\ndiff --git a/tests/integration/test_ldap_external_user_directory/test.py b/tests/integration/test_ldap_external_user_directory/test.py\nnew file mode 100644\nindex 000000000000..39753794d633\n--- /dev/null\n+++ b/tests/integration/test_ldap_external_user_directory/test.py\n@@ -0,0 +1,95 @@\n+import logging\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+from helpers.test_tools import TSV\n+\n+LDAP_ADMIN_BIND_DN = \"cn=admin,dc=example,dc=org\"\n+LDAP_ADMIN_PASSWORD = \"clickhouse\"\n+\n+cluster = ClickHouseCluster(__file__)\n+instance = cluster.add_instance(\n+    \"instance\", main_configs=[\"configs/ldap_with_role_mapping.xml\"], with_ldap=True\n+)\n+\n+\n+@pytest.fixture(scope=\"module\", autouse=True)\n+def ldap_cluster():\n+    try:\n+        cluster.start()\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def add_ldap_group(ldap_cluster, group_cn, member_cn):\n+    code, (stdout, stderr) = ldap_cluster.ldap_container.exec_run(\n+        [\n+            \"sh\",\n+            \"-c\",\n+            \"\"\"echo \"dn: cn={group_cn},dc=example,dc=org\n+objectClass: top\n+objectClass: groupOfNames\n+member: cn={member_cn},ou=users,dc=example,dc=org\" | \\\n+ldapadd -H ldap://{host}:{port} -D \"{admin_bind_dn}\" -x -w {admin_password}\n+    \"\"\".format(\n+                host=ldap_cluster.ldap_host,\n+                port=ldap_cluster.ldap_port,\n+                admin_bind_dn=LDAP_ADMIN_BIND_DN,\n+                admin_password=LDAP_ADMIN_PASSWORD,\n+                group_cn=group_cn,\n+                member_cn=member_cn,\n+            ),\n+        ],\n+        demux=True,\n+    )\n+    logging.debug(\n+        f\"test_ldap_external_user_directory code:{code} stdout:{stdout}, stderr:{stderr}\"\n+    )\n+    assert code == 0\n+\n+\n+def test_authentication_pass():\n+    assert instance.query(\n+        \"select currentUser()\", user=\"janedoe\", password=\"qwerty\"\n+    ) == TSV([[\"janedoe\"]])\n+\n+\n+def test_authentication_fail():\n+    # User doesn't exist.\n+    assert \"doesnotexist: Authentication failed\" in instance.query_and_get_error(\n+        \"SELECT currentUser()\", user=\"doesnotexist\"\n+    )\n+\n+    # Wrong password.\n+    assert \"janedoe: Authentication failed\" in instance.query_and_get_error(\n+        \"SELECT currentUser()\", user=\"janedoe\", password=\"123\"\n+    )\n+\n+\n+def test_role_mapping(ldap_cluster):\n+    instance.query(\"CREATE ROLE role_1\")\n+    instance.query(\"CREATE ROLE role_2\")\n+    add_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_1\", member_cn=\"johndoe\")\n+    add_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_2\", member_cn=\"johndoe\")\n+\n+    assert instance.query(\n+        \"select currentUser()\", user=\"johndoe\", password=\"qwertz\"\n+    ) == TSV([[\"johndoe\"]])\n+\n+    assert instance.query(\n+        \"select role_name from system.current_roles ORDER BY role_name\",\n+        user=\"johndoe\",\n+        password=\"qwertz\",\n+    ) == TSV([[\"role_1\"], [\"role_2\"]])\n+\n+    instance.query(\"CREATE ROLE role_3\")\n+    add_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_3\", member_cn=\"johndoe\")\n+    # Check that non-existing role in ClickHouse is ignored during role update\n+    # See https://github.com/ClickHouse/ClickHouse/issues/54318\n+    add_ldap_group(ldap_cluster, group_cn=\"clickhouse-role_4\", member_cn=\"johndoe\")\n+\n+    assert instance.query(\n+        \"select role_name from system.current_roles ORDER BY role_name\",\n+        user=\"johndoe\",\n+        password=\"qwertz\",\n+    ) == TSV([[\"role_1\"], [\"role_2\"], [\"role_3\"]])\n",
  "problem_statement": "Deadlock in LDAP assigned role update\nWe've been experiencing problems where some of our ClickHouse nodes suddenly become unresponsive and require a restart to resolve the issue. When becoming unresponsive, the host system is basically idle (no CPU/memory/IO usage) and connection attempts via `clickhouse-client` get stuck at (no smiley prompt):\r\n\r\n```\r\nClickHouse client version 23.3.6.7 (official build).\r\nConnecting to localhost:9000 as user [...].\r\nConnected to ClickHouse server version 23.3.6 revision 54462.\r\n```\r\n\r\nThe `/ping` endpoint, which we use for health checks, still returns `200 OK` and affected nodes eventually get behind replication as reported by the `/replicas_status` endpoint. A restart seems to be the only way to resolve it. \r\n\r\n**Analysis**\r\n\r\nWe eventually tracked it down to the LDAP integration and the update of assigned roles:\r\n```\r\n*GIVEN* an LDAP user that successfully connected to clickhouse node n\r\n*AND* the verification cooldown expired (if configured)\r\n*AND* the LDAP groups of the user changed compared to the last connection\r\n*AND* the LDAP user has groups matching the configured role mapping prefix\r\n*AND* any of the matching groups does not exist in ClickHouse (important!) \r\n*WHEN* the LDAP user connects to clickhouse node n again\r\n*THEN* deadlock\r\n```\r\nThe reason for this can be found in [LDAPAccessStorage::updateAssignedRolesNoLock](https://github.com/ClickHouse/ClickHouse/blob/70d1adf/src/Access/LDAPAccessStorage.cpp#L304-L315). The `update_func`, which we pass to [MemoryAccessStorage::updateImpl](https://github.com/ClickHouse/ClickHouse/blob/13d9952/src/Access/MemoryAccessStorage.cpp#L177-L181), calls [LDAPAccessStorage::assignRolesNoLock](https://github.com/ClickHouse/ClickHouse/blob/70d1adf/src/Access/LDAPAccessStorage.cpp#L207), which [tries to find](https://github.com/ClickHouse/ClickHouse/blob/70d1adf/src/Access/LDAPAccessStorage.cpp#L218) the ClickHouse role via `access_control.find<Role>(role_name)`. [AccessControl](https://github.com/ClickHouse/ClickHouse/blob/2f46ae8/src/Access/AccessControl.h#L50) inherits the implementation from [MultipleAccessStorage::findImpl](https://github.com/ClickHouse/ClickHouse/blob/583d9b3/src/Access/MultipleAccessStorage.cpp#L101-L115), which simply iterates all known storages, including `LDAPAccessStorage`, which [delegates](https://github.com/ClickHouse/ClickHouse/blob/70d1adf/src/Access/LDAPAccessStorage.cpp#L418-L422) to [MemoryAccessStorage::findImpl](https://github.com/ClickHouse/ClickHouse/blob/13d9952227162b63ae5e4ae98b0a40677c169b97/src/Access/MemoryAccessStorage.cpp#L18-L28), which tries to [acquire](https://github.com/ClickHouse/ClickHouse/blob/13d9952227162b63ae5e4ae98b0a40677c169b97/src/Access/MemoryAccessStorage.cpp#L20) the already by [MemoryAccessStorage::updateImpl](https://github.com/ClickHouse/ClickHouse/blob/13d9952/src/Access/MemoryAccessStorage.cpp#L179) acquired mutex, which leads to a deadlock. Further connection attempts then wait for the same mutex and eventually all processing stops.\r\n\r\nThere is a [simple fix](https://github.com/jmaicher/ClickHouse/commit/3fdd1afd8016bc821bde5cc6adae3b73c15de9db#diff-358f3c1661944b123f22ef200938d1d8f3e27534603b5e885e465b35f43a4875R60): We can use [std::recursive_mutex](https://en.cppreference.com/w/cpp/thread/recursive_mutex) instead of [std::mutex](https://en.cppreference.com/w/cpp/thread/mutex) in [MemoryAccessStorage](https://github.com/ClickHouse/ClickHouse/blob/bb57caa/src/Access/MemoryAccessStorage.h#L60), which allows the same thread to acquire the lock again. I verified this with a local build, and it works. But there might be a better solution.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes, reproduced on master.\r\n\r\n**How to reproduce**\r\n\r\nSee description above.\r\n\r\n**Expected behavior**\r\n\r\nThe roles are updated without deadlock.\r\n\n",
  "hints_text": "",
  "created_at": "2023-10-09T14:13:05Z"
}