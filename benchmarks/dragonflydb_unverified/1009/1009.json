{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 1009,
  "instance_id": "dragonflydb__dragonfly-1009",
  "issue_numbers": [
    "997"
  ],
  "base_commit": "0312b662440a10a990fbbb136814ab1c6faec0cb",
  "patch": "diff --git a/src/server/transaction.cc b/src/server/transaction.cc\nindex 9681ec9e9a2e..4f6750c2598a 100644\n--- a/src/server/transaction.cc\n+++ b/src/server/transaction.cc\n@@ -371,13 +371,13 @@ void Transaction::StartMultiNonAtomic() {\n \n void Transaction::MultiSwitchCmd(const CommandId* cid) {\n   DCHECK(multi_);\n-  DCHECK(!cb_);\n+  DCHECK(!cb_ptr_);\n \n   unique_shard_id_ = 0;\n   unique_shard_cnt_ = 0;\n   args_.clear();\n   cid_ = cid;\n-  cb_ = nullptr;\n+  cb_ptr_ = nullptr;\n \n   if (multi_->mode == NON_ATOMIC) {\n     for (auto& sd : shard_data_) {\n@@ -399,7 +399,7 @@ string Transaction::DebugId() const {\n // Runs in the dbslice thread. Returns true if transaction needs to be kept in the queue.\n bool Transaction::RunInShard(EngineShard* shard) {\n   DCHECK_GT(run_count_.load(memory_order_relaxed), 0u);\n-  CHECK(cb_) << DebugId();\n+  CHECK(cb_ptr_) << DebugId();\n   DCHECK_GT(txid_, 0u);\n \n   // Unlike with regular transactions we do not acquire locks upon scheduling\n@@ -447,10 +447,10 @@ bool Transaction::RunInShard(EngineShard* shard) {\n \n     // if a transaction is suspended, we still run it because of brpoplpush/blmove case\n     // that needs to run lpush on its suspended shard.\n-    status = cb_(this, shard);\n+    status = (*cb_ptr_)(this, shard);\n \n     if (unique_shard_cnt_ == 1) {\n-      cb_ = nullptr;  // We can do it because only a single thread runs the callback.\n+      cb_ptr_ = nullptr;  // We can do it because only a single thread runs the callback.\n       local_result_ = status;\n     } else {\n       if (status == OpStatus::OUT_OF_MEMORY) {\n@@ -639,8 +639,8 @@ bool Transaction::MultiData::IsIncrLocks() const {\n // transactions like set/mset/mget etc. Does not apply for more complicated cases like RENAME or\n // BLPOP where a data must be read from multiple shards before performing another hop.\n OpStatus Transaction::ScheduleSingleHop(RunnableType cb) {\n-  DCHECK(!cb_);\n-  cb_ = std::move(cb);\n+  DCHECK(!cb_ptr_);\n+  cb_ptr_ = &cb;\n \n   DCHECK(IsAtomicMulti() || (coordinator_state_ & COORD_SCHED) == 0);  // Multi schedule in advance.\n   coordinator_state_ |= (COORD_EXEC | COORD_EXEC_CONCLUDING);  // Single hop means we conclude.\n@@ -671,10 +671,10 @@ OpStatus Transaction::ScheduleSingleHop(RunnableType cb) {\n       bool run_fast = ScheduleUniqueShard(EngineShard::tlocal());\n       if (run_fast) {\n         was_ooo = true;\n-        // it's important to DecreaseRunCnt only for run_fast and after run_eager is assigned.\n+        // it's important to DecreaseRunCnt only for run_fast and after was_ooo is assigned.\n         // If DecreaseRunCnt were called before ScheduleUniqueShard finishes\n         // then WaitForShardCallbacks below could exit before schedule_cb assigns return value\n-        // to run_eager and cause stack corruption.\n+        // to was_ooo and cause stack corruption.\n         CHECK_GE(DecreaseRunCnt(), 1u);\n       }\n     };\n@@ -695,10 +695,14 @@ OpStatus Transaction::ScheduleSingleHop(RunnableType cb) {\n   WaitForShardCallbacks();\n   DVLOG(2) << \"ScheduleSingleHop after Wait \" << DebugId();\n \n-  if (was_ooo)\n+  if (was_ooo) {\n     coordinator_state_ |= COORD_OOO;\n+  }\n \n-  cb_ = nullptr;\n+  if (schedule_fast) {\n+    CHECK(!cb_ptr_);  // we should have reset it within the callback.\n+  }\n+  cb_ptr_ = nullptr;\n   return local_result_;\n }\n \n@@ -756,8 +760,9 @@ void Transaction::Schedule() {\n // Runs in coordinator thread.\n void Transaction::Execute(RunnableType cb, bool conclude) {\n   DCHECK(coordinator_state_ & COORD_SCHED);\n+  DCHECK(!cb_ptr_);\n \n-  cb_ = std::move(cb);\n+  cb_ptr_ = &cb;\n   coordinator_state_ |= COORD_EXEC;\n \n   if (conclude) {\n@@ -772,7 +777,7 @@ void Transaction::Execute(RunnableType cb, bool conclude) {\n   WaitForShardCallbacks();\n   DVLOG(1) << \"Wait on Exec \" << DebugId() << \" completed\";\n \n-  cb_ = nullptr;\n+  cb_ptr_ = nullptr;\n }\n \n // Runs in coordinator thread.\n@@ -857,11 +862,11 @@ void Transaction::RunQuickie(EngineShard* shard) {\n   DCHECK_EQ(0, sd.local_mask & (KEYLOCK_ACQUIRED | OUT_OF_ORDER));\n \n   DVLOG(1) << \"RunQuickSingle \" << DebugId() << \" \" << shard->shard_id() << \" \" << args_[0];\n-  CHECK(cb_) << DebugId() << \" \" << shard->shard_id() << \" \" << args_[0];\n+  DCHECK(cb_ptr_) << DebugId() << \" \" << shard->shard_id() << \" \" << args_[0];\n \n   // Calling the callback in somewhat safe way\n   try {\n-    local_result_ = cb_(this, shard);\n+    local_result_ = (*cb_ptr_)(this, shard);\n   } catch (std::bad_alloc&) {\n     LOG_FIRST_N(ERROR, 16) << \" out of memory\";\n     local_result_ = OpStatus::OUT_OF_MEMORY;\n@@ -872,7 +877,7 @@ void Transaction::RunQuickie(EngineShard* shard) {\n   LogAutoJournalOnShard(shard);\n \n   sd.is_armed.store(false, memory_order_relaxed);\n-  cb_ = nullptr;  // We can do it because only a single shard runs the callback.\n+  cb_ptr_ = nullptr;  // We can do it because only a single shard runs the callback.\n }\n \n // runs in coordinator thread.\n@@ -1206,6 +1211,10 @@ inline uint32_t Transaction::DecreaseRunCnt() {\n   ::boost::intrusive_ptr guard(this);\n \n   // We use release so that no stores will be reordered after.\n+  // It's needed because we need to enforce that all stores executed before this point\n+  // are visible right after run_count_ is unblocked in the coordinator thread.\n+  // The fact that run_ec_.notify() does release operation is not enough, because\n+  // WaitForCallbacks might skip reading run_ec_ if run_count_ is already 0.\n   uint32_t res = run_count_.fetch_sub(1, memory_order_release);\n   if (res == 1) {\n     run_ec_.notify();\ndiff --git a/src/server/transaction.h b/src/server/transaction.h\nindex 5ea143290fc7..707a35f1e6ca 100644\n--- a/src/server/transaction.h\n+++ b/src/server/transaction.h\n@@ -7,6 +7,7 @@\n #include <absl/container/flat_hash_map.h>\n #include <absl/container/flat_hash_set.h>\n #include <absl/container/inlined_vector.h>\n+#include <absl/functional/function_ref.h>\n \n #include <string_view>\n #include <variant>\n@@ -71,7 +72,7 @@ class Transaction {\n  public:\n   using time_point = ::std::chrono::steady_clock::time_point;\n   // Runnable that is run on shards during hop executions (often named callback).\n-  using RunnableType = std::function<OpStatus(Transaction* t, EngineShard*)>;\n+  using RunnableType = absl::FunctionRef<OpStatus(Transaction* t, EngineShard*)>;\n   // Provides keys to block on for specific shard.\n   using WaitKeysProvider = std::function<ArgSlice(Transaction*, EngineShard* shard)>;\n \n@@ -406,7 +407,13 @@ class Transaction {\n   void WaitForShardCallbacks() {\n     run_ec_.await([this] { return 0 == run_count_.load(std::memory_order_relaxed); });\n \n-    seqlock_.fetch_add(1, std::memory_order_release);\n+    // no reads after this fence will be reordered before it, and if a store operation sequenced\n+    // before some release operation that happened before the fence in another thread, this store\n+    // will be visible after the fence.\n+    // In this specific case we synchronize with DecreaseRunCnt that releases run_count_.\n+    // See #997 before changing it.\n+    std::atomic_thread_fence(std::memory_order_acquire);\n+    seqlock_.fetch_add(1, std::memory_order_relaxed);\n   }\n \n   // Log command in shard's journal, if this is a write command with auto-journaling enabled.\n@@ -471,7 +478,7 @@ class Transaction {\n   // Reverse argument mapping for ReverseArgIndex to convert from shard index to original index.\n   std::vector<uint32_t> reverse_index_;\n \n-  RunnableType cb_;                   // Run on shard threads\n+  RunnableType* cb_ptr_ = nullptr;    // Run on shard threads\n   const CommandId* cid_;              // Underlying command\n   std::unique_ptr<MultiData> multi_;  // Initialized when the transaction is multi/exec.\n \n",
  "test_patch": "diff --git a/src/facade/facade_test.h b/src/facade/facade_test.h\nindex 929c25079335..0f910468a973 100644\n--- a/src/facade/facade_test.h\n+++ b/src/facade/facade_test.h\n@@ -32,8 +32,8 @@ class RespMatcher {\n   RespExpr::Type type_;\n \n   std::string exp_str_;\n-  int64_t exp_int_;\n-  double_t exp_double_;\n+  int64_t exp_int_ = 0;\n+  double_t exp_double_ = 0;\n };\n \n class RespTypeMatcher {\n",
  "problem_statement": "Crash due to data race\nI'm running a redis cluster with rps ~ 240k; and when start trying with dragonfly, I got coredump frequently (process lived no more than 30minutes).\r\n\r\nThe dragonfly/redis mostly uses command HGET to get data. Commands pattern\r\n- hget: >99.9%\r\n- hset/hdel/hgetall/smembers/sadd/srem/ping: <0.1%\r\n\r\n\r\nBelow is what getting from console log\r\n\r\n> mimalloc: error: thread 0x21e1a520: corrupted thread-free list\r\n> mimalloc: error: thread 0x21e1a520: corrupted thread-free list\r\n> mimalloc: error: thread 0x21e19010: corrupted thread-free list\r\n> mimalloc: error: thread 0x21e19010: corrupted thread-free list\r\n> mimalloc: error: thread 0x21e18eb0: corrupted thread-free list\r\n> *** SIGBUS received at time=1679890128 on cpu 2 ***\r\n> [failure_signal_handler.cc : 332] RAW: Signal 11 raised at PC=0x68cf34 while already in AbslFailureSignalHandler()\r\n> PC: @           0x68ab90  (unknown)  _mi_free_block_mt\r\n>     @           0x7524cc       4928  absl::lts_20230125::AbslFailureSignalHandler()\r\n>     @     0xffffb4915804         32  (unknown)\r\n>     @           0x51980c         32  _ZNSt17_Function_handlerIFN6facade8OpStatusEPN4dfly11TransactionEPNS2_11EngineShardEEZNS3_18ScheduleSingleHopTIZNS2_10HSetFamily4HGetEN4absl12lts_202301254SpanINSC_IcEEEEPNS2_17ConnectionContextEEUlS4_S6_E_EEDTclfp_fpTLDnEEEOT_EUlS4_S6_E_E10_M_managerERSt9_Any_dataRKSN_St18_Manager_operation\r\n>     @           0x5e4640        208  dfly::Transaction::RunQuickie()\r\n>     @           0x5e4978        112  dfly::Transaction::ScheduleUniqueShard()\r\n>     @           0x5e7870        160  std::_Function_handler<>::_M_invoke()\r\n>     @           0x644638        240  util::fibers_ext::FiberQueue::Run()\r\n>     @           0x5c8260        256  boost::fibers::worker_context<>::run_()\r\n>     @           0x5c7984  (unknown)  boost::context::detail::fiber_entry<>()\r\n\r\nAlso trying to get the coredump file but still not successfully.\r\nsystemd threw msgs like this \"**_systemd-coredump[xxxxxxx]: Failed to get EXE, ignoring: No such process_**\"\r\n\r\n**Run command**\r\n`./dragonfly-latest --bind 0.0.0.0 --dbnum 1 --proactor_threads 12 --maxmemory 5368709120 --port 3800 --save_schedule \"*:30\" --dbfilename dump.rdb --df_snapshot_format=false --dir /data/redis-3800/ --log_dir /data/redis-3800/  >/data/redis-3800/console.log 2>&1 &`\r\n\r\n**Environment:**\r\n - OS: [Amazon Linux 3 (AL2023)]\r\n - Kernel: 6.1.15-28.43.amzn2023.aarch64\r\n - EC2 instance c7g.4xlarge (ARM64)\r\n - Dragonfly Version: [latest code as of 2023-03-27]\r\n\r\nPls help to check.\r\nTks.\n",
  "hints_text": "Hi @vvhungy  sorry about that. Can you please add here your command line?\nAlso, can you please join our discord and ping me at #general channel? \r\n\nRun command:\r\n`./dragonfly-latest --bind 0.0.0.0 --dbnum 1 --proactor_threads 12 --maxmemory 5368709120 --port 3800 --save_schedule \"*:30\" --dbfilename dump.rdb --df_snapshot_format=false --dir /data/redis-3800/ --log_dir /data/redis-3800/  >/data/redis-3800/console.log 2>&1 &`\r\n\r\nAlso after start, I enable replication from the redis-master to this dragonfly instance from redis-cli: REPLICAOF host port.\nI see. Can you please disable `--save_schedule \"*:30\"` and see if the problem persists? \r\nAlso dm'ed you :)\nConfirming that I succeed to reproduce it with an empty database on c7g.4xlarge.\r\nTo reproduce (in opt mode) run:\r\n `memtier_benchmark -t 8 --command \"hget __key__ foo\"   --hide-histogram -c 10 -n 800000`",
  "created_at": "2023-03-29T13:37:57Z",
  "modified_files": [
    "src/server/transaction.cc",
    "src/server/transaction.h"
  ],
  "modified_test_files": [
    "src/facade/facade_test.h"
  ]
}