{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 1347,
  "instance_id": "dragonflydb__dragonfly-1347",
  "issue_numbers": [
    "1231"
  ],
  "base_commit": "5471e296d802dfe1f8c6ca8d3db6ae82a3010edb",
  "patch": "diff --git a/src/server/error.h b/src/server/error.h\nindex 868228c8ae25..13a77f555e03 100644\n--- a/src/server/error.h\n+++ b/src/server/error.h\n@@ -46,6 +46,7 @@ enum errc {\n   empty_key = 10,\n   out_of_memory = 11,\n   bad_json_string = 12,\n+  unsupported_operation = 13,\n };\n \n }  // namespace rdb\ndiff --git a/src/server/rdb_load.cc b/src/server/rdb_load.cc\nindex 4c27f5846057..06d4b95cea5f 100644\n--- a/src/server/rdb_load.cc\n+++ b/src/server/rdb_load.cc\n@@ -17,6 +17,7 @@ extern \"C\" {\n #include \"redis/zset.h\"\n }\n #include <absl/cleanup/cleanup.h>\n+#include <absl/strings/match.h>\n #include <absl/strings/str_cat.h>\n #include <lz4frame.h>\n #include <zstd.h>\n@@ -2060,6 +2061,17 @@ error_code RdbLoaderBase::HandleJournalBlob(Service* service) {\n   while (done < num_entries) {\n     journal::ParsedEntry entry{};\n     SET_OR_RETURN(journal_reader_.ReadEntry(), entry);\n+\n+    if (!entry.cmd.cmd_args.empty()) {\n+      if (absl::EqualsIgnoreCase(facade::ToSV(entry.cmd.cmd_args[0]), \"FLUSHALL\") ||\n+          absl::EqualsIgnoreCase(facade::ToSV(entry.cmd.cmd_args[0]), \"FLUSHDB\")) {\n+        // Applying a flush* operation in the middle of a load can cause out-of-sync deletions of\n+        // data that should not be deleted, see https://github.com/dragonflydb/dragonfly/issues/1231\n+        // By returning an error we are effectively restarting the replication.\n+        return RdbError(errc::unsupported_operation);\n+      }\n+    }\n+\n     ex.Execute(entry.dbid, entry.cmd);\n     VLOG(1) << \"Reading item: \" << entry.ToString();\n     done++;\n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex c395046e318f..fad916c7dd2f 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -945,3 +945,56 @@ async def test_replication_info(df_local_factory, df_seeder_factory, n_keys=2000\n \n     await c_master.connection_pool.disconnect()\n     await c_replica.connection_pool.disconnect()\n+\n+\n+\"\"\"\n+Test flushall command that's invoked while in full sync mode.\n+This can cause an issue because it will be executed on each shard independently.\n+More details in https://github.com/dragonflydb/dragonfly/issues/1231\n+\"\"\"\n+@pytest.mark.asyncio\n+async def test_flushall_in_full_sync(df_local_factory, df_seeder_factory):\n+    master = df_local_factory.create(port=BASE_PORT, proactor_threads=4, logtostdout=True)\n+    replica = df_local_factory.create(port=BASE_PORT+1, proactor_threads=2, logtostdout=True)\n+\n+    # Start master\n+    master.start()\n+    c_master = aioredis.Redis(port=master.port)\n+\n+    # Fill master with test data\n+    seeder = df_seeder_factory.create(port=master.port, keys=10_000, dbcount=1)\n+    await seeder.run(target_deviation=0.1)\n+\n+    # Start replica\n+    replica.start()\n+    c_replica = aioredis.Redis(port=replica.port)\n+    await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n+\n+    async def get_sync_mode(c_replica):\n+        result = await c_replica.execute_command(\"role\")\n+        return result[3]\n+\n+    async def is_full_sync_mode(c_replica):\n+        return await get_sync_mode(c_replica) == b'full_sync'\n+\n+    # Wait for full sync to start\n+    while not await is_full_sync_mode(c_replica):\n+        await asyncio.sleep(0.0)\n+\n+    syncid, _ = await c_replica.execute_command(\"DEBUG REPLICA OFFSET\")\n+\n+    # Issue FLUSHALL and push some more entries\n+    await c_master.execute_command(\"FLUSHALL\")\n+\n+    await asyncio.sleep(1.0)\n+\n+    # Make sure that a new sync ID is present, meaning replication restarted following FLUSHALL\n+    new_syncid, _ = await c_replica.execute_command(\"DEBUG REPLICA OFFSET\")\n+    assert new_syncid != syncid\n+\n+    post_seeder = df_seeder_factory.create(port=master.port, keys=10, dbcount=1)\n+    await post_seeder.run(target_deviation=0.1)\n+\n+    await check_all_replicas_finished([c_replica], c_master)\n+\n+    await check_data(post_seeder, [replica], [c_replica])\n",
  "problem_statement": "Replication not consistent when running flushall on full sync phase\nWhen creating a snapshot in master for replica if flush all command is executed we write the command to all shard flows.\r\nOn replica side rdb loader reads the journal command flush all and executes it.\r\nBecause we are reading rdb data per shard and execute the journal changes in each shard, we execute the flushall on replica many times without sync with other rdb loaders and this can cause flushing data which should exist and causes replica to have some missing entries.\r\n\r\njournal shard data example:\r\nshard1 data\r\nset x 1\r\nflushall\r\nset x 2\r\n\r\nshard2 data\r\nset y 1\r\nflushall\r\nset y 2\r\n\r\nin the above example running the flushall of shard2 data can be after running set x 2 of shard 1\r\ntherefore the replica will not be consistent with master\n",
  "hints_text": "@romange creating sync between rdb loaders of different shards for the flush commands is not simple. Do you think we can block flushall while creating snapshot for replication? or even does it makes sense to block the command when creating a snapshot?\nIt's weird that flushall is hard to support because what it says effectively to replica is that full sync should be dropped. I understand that it requires a special handling but it is not a hard case. \nAs discussed with Roman we will cancel the replication when flushall/flushdb is executed for replicas in full sync stage and start replication from start",
  "created_at": "2023-06-04T06:24:58Z",
  "modified_files": [
    "src/server/error.h",
    "src/server/rdb_load.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}