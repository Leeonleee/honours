{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4636,
  "instance_id": "dragonflydb__dragonfly-4636",
  "issue_numbers": [
    "4501"
  ],
  "base_commit": "a22daaf49de66ead85b4365897695b19f8087911",
  "patch": "diff --git a/src/server/replica.cc b/src/server/replica.cc\nindex d7de6d96d151..817f52f37d7f 100644\n--- a/src/server/replica.cc\n+++ b/src/server/replica.cc\n@@ -124,11 +124,12 @@ error_code Replica::Start(facade::SinkReplyBuilder* builder) {\n   ec = Greet();\n   RETURN_ON_ERR(check_connection_error(ec, \"could not greet master \"));\n \n-  // 4. Spawn main coordination fiber.\n-  sync_fb_ = fb2::Fiber(\"main_replication\", &Replica::MainReplicationFb, this);\n+  return {};\n+}\n \n+void Replica::StartMainReplicationFiber(facade::SinkReplyBuilder* builder) {\n+  sync_fb_ = fb2::Fiber(\"main_replication\", &Replica::MainReplicationFb, this);\n   builder->SendOk();\n-  return {};\n }\n \n void Replica::EnableReplication(facade::SinkReplyBuilder* builder) {\ndiff --git a/src/server/replica.h b/src/server/replica.h\nindex dbb976b3d25a..9cffde818ec6 100644\n--- a/src/server/replica.h\n+++ b/src/server/replica.h\n@@ -59,6 +59,7 @@ class Replica : ProtocolClient {\n   // Returns true if initial link with master has been established or\n   // false if it has failed.\n   std::error_code Start(facade::SinkReplyBuilder* builder);\n+  void StartMainReplicationFiber(facade::SinkReplyBuilder* builder);\n \n   // Sets the server state to have replication enabled.\n   // It is like Start(), but does not attempt to establish\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex a1cd3b5feef1..c64379a35075 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -2751,6 +2751,7 @@ void ServerFamily::AddReplicaOf(CmdArgList args, const CommandContext& cmd_cntx)\n                                           master_replid(), replicaof_args->slot_range);\n   error_code ec = add_replica->Start(cmd_cntx.rb);\n   if (!ec) {\n+    add_replica->StartMainReplicationFiber(cmd_cntx.rb);\n     cluster_replicas_.push_back(std::move(add_replica));\n   }\n }\n@@ -2806,12 +2807,6 @@ void ServerFamily::ReplicaOfInternal(CmdArgList args, Transaction* tx, SinkReply\n       return;\n     }\n \n-    // If we are called by \"Replicate\", tx will be null but we do not need\n-    // to flush anything.\n-    if (tx) {\n-      Drakarys(tx, DbSlice::kDbAll);\n-    }\n-\n     // Create a new replica and assing it\n     new_replica = make_shared<Replica>(replicaof_args->host, replicaof_args->port, &service_,\n                                        master_replid(), replicaof_args->slot_range);\n@@ -2830,7 +2825,7 @@ void ServerFamily::ReplicaOfInternal(CmdArgList args, Transaction* tx, SinkReply\n     case ActionOnConnectionFail::kReturnOnError:\n       ec = new_replica->Start(builder);\n       break;\n-    case ActionOnConnectionFail::kContinueReplication:  // set DF to replicate, and forget about it\n+    case ActionOnConnectionFail::kContinueReplication:\n       new_replica->EnableReplication(builder);\n       break;\n   };\n@@ -2842,6 +2837,17 @@ void ServerFamily::ReplicaOfInternal(CmdArgList args, Transaction* tx, SinkReply\n     service_.SwitchState(GlobalState::LOADING, GlobalState::ACTIVE);\n     SetMasterFlagOnAllThreads(true);\n     replica_.reset();\n+    return;\n+  }\n+  // Successfully connected now we flush\n+  // If we are called by \"Replicate\", tx will be null but we do not need\n+  // to flush anything.\n+  if (tx) {\n+    Drakarys(tx, DbSlice::kDbAll);\n+  }\n+\n+  if (on_err == ActionOnConnectionFail::kReturnOnError) {\n+    replica_->StartMainReplicationFiber(builder);\n   }\n }\n \n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 5e3dbc3eeb9e..231a0084ab50 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -2809,3 +2809,24 @@ async def test_stream_approximate_trimming(df_factory):\n     master_data = await StaticSeeder.capture(c_master)\n     replica_data = await StaticSeeder.capture(c_replica)\n     assert master_data == replica_data\n+\n+\n+@dfly_args({\"proactor_threads\": 2})\n+async def test_replicaof_does_not_flush_if_it_fails_to_connect(df_factory):\n+    master = df_factory.create(proactor_threads=2)\n+    replica = df_factory.create(proactor_threads=2)\n+\n+    df_factory.start_all([master, replica])\n+    c_master = master.client()\n+    c_replica = replica.client()\n+\n+    await c_master.execute_command(\"SET foo bar\")\n+    await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n+    await check_all_replicas_finished([c_replica], c_master)\n+\n+    res = await c_replica.execute_command(\"dbsize\")\n+    assert res == 1\n+    with pytest.raises(redis.exceptions.ResponseError):\n+        await c_replica.execute_command(f\"REPLICAOF localhost {replica.port}\")\n+    res = await c_replica.execute_command(\"dbsize\")\n+    assert res == 1\n",
  "problem_statement": "replicaof <myself> flushes the dataset\n`replicaof localhost 6379` fails but before that it flushes whatever data it has.\n\n\nCurrently the replication flow is designed in such way,\n that we first flush the dataset and only then check if handshake succeeds.\nIdeally we should first succeed with the handshake and only then flush the database.\n\n",
  "hints_text": "Can i help with this?",
  "created_at": "2025-02-20T12:09:09Z",
  "modified_files": [
    "src/server/replica.cc",
    "src/server/replica.h",
    "src/server/server_family.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}