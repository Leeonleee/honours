{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 523,
  "instance_id": "dragonflydb__dragonfly-523",
  "issue_numbers": [
    "448"
  ],
  "base_commit": "20e4e189c77b9a675890c287818c3a54c58224c3",
  "patch": "diff --git a/src/core/compact_object.cc b/src/core/compact_object.cc\nindex f48095c4cfa7..996e6a6d6e36 100644\n--- a/src/core/compact_object.cc\n+++ b/src/core/compact_object.cc\n@@ -39,6 +39,7 @@ using absl::GetFlag;\n namespace {\n \n constexpr XXH64_hash_t kHashSeed = 24061983;\n+constexpr size_t kAlignSize = 8u;\n \n // Approximation since does not account for listpacks.\n size_t QlMAllocSize(quicklist* ql) {\n@@ -217,7 +218,7 @@ struct TL {\n \n thread_local TL tl;\n \n-constexpr bool kUseSmallStrings = true;\n+constexpr bool kUseSmallStrings = false;\n \n /// TODO: Ascii encoding becomes slow for large blobs. We should factor it out into a separate\n /// file and implement with SIMD instructions.\n@@ -380,6 +381,23 @@ void RobjWrapper::SetString(string_view s, pmr::memory_resource* mr) {\n   }\n }\n \n+bool RobjWrapper::DefragIfNeeded(float ratio) {\n+  if (type() == OBJ_STRING) {  // only applicable to strings\n+    if (zmalloc_page_is_underutilized(inner_obj(), ratio)) {\n+      return Reallocate(tl.local_mr);\n+    }\n+  }\n+  return false;\n+}\n+\n+bool RobjWrapper::Reallocate(std::pmr::memory_resource* mr) {\n+  void* old_ptr = inner_obj_;\n+  inner_obj_ = mr->allocate(sz_, kAlignSize);\n+  memcpy(inner_obj_, old_ptr, sz_);\n+  mr->deallocate(old_ptr, 0, kAlignSize);\n+  return true;\n+}\n+\n void RobjWrapper::Init(unsigned type, unsigned encoding, void* inner) {\n   type_ = type;\n   encoding_ = encoding;\n@@ -398,13 +416,13 @@ void RobjWrapper::MakeInnerRoom(size_t current_cap, size_t desired, pmr::memory_\n       desired += SDS_MAX_PREALLOC;\n   }\n \n-  void* newp = mr->allocate(desired, 8);\n+  void* newp = mr->allocate(desired, kAlignSize);\n   if (sz_) {\n     memcpy(newp, inner_obj_, sz_);\n   }\n \n   if (current_cap) {\n-    mr->deallocate(inner_obj_, current_cap, 8);\n+    mr->deallocate(inner_obj_, current_cap, kAlignSize);\n   }\n   inner_obj_ = newp;\n }\n@@ -659,7 +677,7 @@ robj* CompactObj::AsRObj() const {\n   res->type = u_.r_obj.type();\n \n   if (res->type == OBJ_SET) {\n-    LOG(FATAL) << \"Should not call AsRObj for type \" <<  res->type;\n+    LOG(FATAL) << \"Should not call AsRObj for type \" << res->type;\n   }\n \n   if (res->type == OBJ_HASH) {\n@@ -850,6 +868,28 @@ string_view CompactObj::GetSlice(string* scratch) const {\n   return string_view{};\n }\n \n+bool CompactObj::DefragIfNeeded(float ratio) {\n+  switch (taglen_) {\n+    case ROBJ_TAG:\n+      // currently only these objet types are supported for this operation\n+      if (u_.r_obj.inner_obj() != nullptr) {\n+        return u_.r_obj.DefragIfNeeded(ratio);\n+      }\n+      return false;\n+    case SMALL_TAG:\n+      // TODO - support this later\n+      return false;\n+    case INT_TAG:\n+      // this is not relevant in this case\n+      return false;\n+    case EXTERNAL_TAG:\n+      return false;\n+    default:\n+      // This is the case when the object is at inline_str\n+      return false;\n+  }\n+}\n+\n bool CompactObj::HasAllocated() const {\n   if (IsRef() || taglen_ == INT_TAG || IsInline() || taglen_ == EXTERNAL_TAG ||\n       (taglen_ == ROBJ_TAG && u_.r_obj.inner_obj() == nullptr))\ndiff --git a/src/core/compact_object.h b/src/core/compact_object.h\nindex 6f56a16432ca..087df9ee7e44 100644\n--- a/src/core/compact_object.h\n+++ b/src/core/compact_object.h\n@@ -16,7 +16,7 @@ typedef struct redisObject robj;\n namespace dfly {\n \n constexpr unsigned kEncodingIntSet = 0;\n-constexpr unsigned kEncodingStrMap = 1;  // for set/map encodings of strings\n+constexpr unsigned kEncodingStrMap = 1;   // for set/map encodings of strings\n constexpr unsigned kEncodingStrMap2 = 2;  // for set/map encodings of strings using DenseSet\n constexpr unsigned kEncodingListPack = 3;\n \n@@ -52,7 +52,10 @@ class RobjWrapper {\n     return std::string_view{reinterpret_cast<char*>(inner_obj_), sz_};\n   }\n \n+  bool DefragIfNeeded(float ratio);\n+\n  private:\n+  bool Reallocate(std::pmr::memory_resource* mr);\n   size_t InnerObjMallocUsed() const;\n   void MakeInnerRoom(size_t current_cap, size_t desired, std::pmr::memory_resource* mr);\n \n@@ -208,6 +211,8 @@ class CompactObj {\n     return mask_ & IO_PENDING;\n   }\n \n+  bool DefragIfNeeded(float ratio);\n+\n   void SetIoPending(bool b) {\n     if (b) {\n       mask_ |= IO_PENDING;\ndiff --git a/src/server/dfly_main.cc b/src/server/dfly_main.cc\nindex ae91cbfd217c..09bab7c59e26 100644\n--- a/src/server/dfly_main.cc\n+++ b/src/server/dfly_main.cc\n@@ -341,6 +341,7 @@ Usage: dragonfly [FLAGS]\n   }\n   mi_option_enable(mi_option_show_errors);\n   mi_option_set(mi_option_max_warnings, 0);\n+  mi_option_set(mi_option_decommit_delay, 0);\n \n   base::sys::KernelVersion kver;\n   base::sys::GetKernelVersion(&kver);\ndiff --git a/src/server/engine_shard_set.cc b/src/server/engine_shard_set.cc\nindex 2c702f8309b6..489998d2d2e6 100644\n--- a/src/server/engine_shard_set.cc\n+++ b/src/server/engine_shard_set.cc\n@@ -51,7 +51,8 @@ using absl::GetFlag;\n \n namespace {\n \n-constexpr DbIndex DEFAULT_DB_INDEX = 0;\n+constexpr DbIndex kDefaultDbIndex = 0;\n+constexpr uint64_t kCursorDoneState = 0u;\n \n vector<EngineShardSet::CachedStats> cached_stats;  // initialized in EngineShardSet::Init\n \n@@ -80,7 +81,7 @@ bool EngineShard::DefragTaskState::IsRequired() {\n   const uint64_t threshold_mem = max_memory_limit * GetFlag(FLAGS_mem_defrag_threshold);\n   const double commit_use_threshold = GetFlag(FLAGS_commit_use_threshold);\n \n-  if (cursor > 0) {\n+  if (cursor > kCursorDoneState) {\n     return true;\n   }\n \n@@ -100,34 +101,73 @@ bool EngineShard::DefragTaskState::IsRequired() {\n \n // for now this does nothing\n bool EngineShard::DoDefrag() {\n-  // TODO - Impl!!\n-  return defrag_state_.cursor > 0;\n-}\n-\n-void EngineShard::DefragTaskState::Init() {\n-  cursor = 0u;\n+  // --------------------------------------------------------------------------\n+  // NOTE: This task is running with exclusive access to the shard.\n+  // i.e. - Since we are using shared noting access here, and all access\n+  // are done using fibers, This fiber is run only when no other fiber in the\n+  // context of the controlling thread will access this shard!\n+  // --------------------------------------------------------------------------\n+\n+  constexpr size_t kMaxTraverses = 50;\n+  const float threshold = GetFlag(FLAGS_mem_utilization_threshold);\n+\n+  auto& slice = db_slice();\n+  DCHECK(slice.IsDbValid(kDefaultDbIndex));\n+  auto [prime_table, expire_table] = slice.GetTables(kDefaultDbIndex);\n+  PrimeTable::Cursor cur = defrag_state_.cursor;\n+  uint64_t defrag_count = 0;\n+  unsigned traverses_count = 0;\n+\n+  do {\n+    cur = prime_table->Traverse(cur, [&](PrimeIterator it) {\n+      // for each value check whether we should move it because it\n+      // seats on underutilized page of memory, and if so, do it.\n+      bool did = it->second.DefragIfNeeded(threshold);\n+      if (did) {\n+        defrag_count++;\n+      }\n+    });\n+    traverses_count++;\n+  } while (traverses_count < kMaxTraverses && cur);\n+\n+  defrag_state_.cursor = cur.value();\n+  if (defrag_count > 0) {\n+    VLOG(1) << \"shard \" << slice.shard_id() << \": successfully defrag  \" << defrag_count\n+            << \" times, did it in \" << traverses_count << \" cursor is at the \"\n+            << (defrag_state_.cursor == 0 ? \"end\" : \"in progress\");\n+  } else {\n+    VLOG(1) << \"shard \" << slice.shard_id() << \": run the defrag \" << traverses_count\n+            << \" times out of maximum \" << kMaxTraverses << \", with cursor at \"\n+            << (defrag_state_.cursor == 0 ? \"end\" : \"in progress\")\n+            << \" but no location for defrag were found\";\n+  }\n+  defrag_state_.stats.success_count += defrag_count;\n+  defrag_state_.stats.tries++;\n+  return defrag_state_.cursor > kCursorDoneState;\n }\n \n // the memory defragmentation task is as follow:\n //  1. Check if memory usage is high enough\n //  2. Check if diff between commited and used memory is high enough\n //  3. Check if we have memory changes (to ensure that we not running endlessly). - TODO\n-//  4. if all the above pass -> run on the shard and try to defragmented memory by re-allocating\n-//  values\n-//     if the cursor for this is signal that we are not done, schedule the task to run at high\n-//     priority otherwise lower the task priority so that it would not use the CPU when not required\n+//  4. if all the above pass -> scan this shard and try to find whether we can move pointer to\n+//  underutilized pages values\n+//     if the cursor returned from scan is not in done state, schedule the task to run at high\n+//     priority.\n+//     otherwise lower the task priority so that it would not use the CPU when not required\n uint32_t EngineShard::DefragTask() {\n+  constexpr uint32_t kRunAtLowPriority = 0u;\n+\n   const auto shard_id = db_slice().shard_id();\n-  bool required_state = defrag_state_.IsRequired();\n-  if (required_state) {\n+  if (defrag_state_.IsRequired()) {\n     VLOG(1) << shard_id << \": need to run defrag memory cursor state: \" << defrag_state_.cursor;\n     if (DoDefrag()) {\n       // we didn't finish the scan\n       return util::ProactorBase::kOnIdleMaxLevel;\n     }\n   }\n-  // by default we just want to not get in the way..\n-  return 0u;\n+\n+  return kRunAtLowPriority;\n }\n \n EngineShard::EngineShard(util::ProactorBase* pb, bool update_db_time, mi_heap_t* heap)\n@@ -150,7 +190,6 @@ EngineShard::EngineShard(util::ProactorBase* pb, bool update_db_time, mi_heap_t*\n \n   db_slice_.UpdateExpireBase(absl::GetCurrentTimeNanos() / 1000000, 0);\n   // start the defragmented task here\n-  defrag_state_.Init();\n   defrag_task_ = pb->AddOnIdleTask([this]() { return this->DefragTask(); });\n }\n \n@@ -170,9 +209,7 @@ void EngineShard::Shutdown() {\n     ProactorBase::me()->CancelPeriodic(periodic_task_);\n   }\n \n-  if (defrag_task_ != 0) {\n-    ProactorBase::me()->RemoveOnIdleTask(defrag_task_);\n-  }\n+  ProactorBase::me()->RemoveOnIdleTask(defrag_task_);\n }\n \n void EngineShard::InitThreadLocal(ProactorBase* pb, bool update_db_time) {\ndiff --git a/src/server/engine_shard_set.h b/src/server/engine_shard_set.h\nindex 77a3fb9215c3..e319d0aaafd2 100644\n--- a/src/server/engine_shard_set.h\n+++ b/src/server/engine_shard_set.h\n@@ -158,8 +158,6 @@ class EngineShard {\n     uint64_t cursor = 0u;\n     DefragStats stats;\n \n-    void Init();\n-\n     // check the current threshold and return true if\n     // we need to do the de-fermentation\n     bool IsRequired();\n",
  "test_patch": "diff --git a/src/core/compact_object_test.cc b/src/core/compact_object_test.cc\nindex 8740959e9231..34d4c73cc2d0 100644\n--- a/src/core/compact_object_test.cc\n+++ b/src/core/compact_object_test.cc\n@@ -342,7 +342,7 @@ TEST_F(CompactObjectTest, MimallocUnderutilzationWithRealloc) {\n   bool found = HasUnderutilizedMemory(ptrs, kUnderUtilizedRatio);\n   ASSERT_FALSE(found);\n   DeallocateAtRandom(kRandomStep, &ptrs);\n-  // TestMiMallocUnderutilized(ptrs, run_reallocation, allocation_size);\n+\n   //  This is another case, where we are filling the \"gaps\" by doing re-allocations\n   //  in this case, since we are not setting all the values back it should still have\n   //  places that are not used. Plus since we are not looking at the first page\ndiff --git a/src/server/dragonfly_test.cc b/src/server/dragonfly_test.cc\nindex 0f4ff745b766..924b3b53e8aa 100644\n--- a/src/server/dragonfly_test.cc\n+++ b/src/server/dragonfly_test.cc\n@@ -758,21 +758,66 @@ TEST_F(DflyEngineTest, Bug496) {\n TEST_F(DefragDflyEngineTest, TestDefragOption) {\n   // Fill data into dragonfly and then check if we have\n   // any location in memory to defrag. See issue #448 for details about this.\n-  EXPECT_GE(max_memory_limit, 100'000);\n-  const int NUMBER_OF_KEYS = 184000;\n+  max_memory_limit = 300'000;             // control memory size so no need for too many keys\n+  constexpr int kNumberOfKeys = 100'000;  // this fill the memory\n+  constexpr int kKeySize = 137;\n+  constexpr int kMaxDefragTriesForTests = 10;\n+\n+  std::vector<std::string> keys2delete;\n+  keys2delete.push_back(\"del\");\n+\n+  // Generate a list of keys that would be deleted\n+  // The keys that we will delete are all in the form of \"key-name:1<other digits>\"\n+  // This is because we are populating keys that has this format, but we don't want\n+  // to delete all keys, only some random keys so we deleting those that start with 1\n+  constexpr int kFactor = 10;\n+  int kMaxNumKeysToDelete = 10'000;\n+  int current_step = kFactor;\n+  for (int i = 1; i < kMaxNumKeysToDelete; current_step *= kFactor) {\n+    for (; i < current_step; i++) {\n+      int j = i - 1 + current_step;\n+      keys2delete.push_back(\"key-name:\" + std::to_string(j));\n+    }\n+  }\n+\n+  std::vector<std::string_view> keys(keys2delete.begin(), keys2delete.end());\n \n-  RespExpr resp = Run({\"DEBUG\", \"POPULATE\", std::to_string(NUMBER_OF_KEYS), \"key-name\", \"130\"});\n+  RespExpr resp = Run(\n+      {\"DEBUG\", \"POPULATE\", std::to_string(kNumberOfKeys), \"key-name\", std::to_string(kKeySize)});\n   ASSERT_EQ(resp, \"OK\");\n   resp = Run({\"DBSIZE\"});\n-  EXPECT_THAT(resp, IntArg(NUMBER_OF_KEYS));\n+  EXPECT_THAT(resp, IntArg(kNumberOfKeys));\n \n   shard_set->pool()->AwaitFiberOnAll([&](unsigned index, ProactorBase* base) {\n     EngineShard* shard = EngineShard::tlocal();\n     ASSERT_FALSE(shard == nullptr);  // we only have one and its should not be empty!\n+    this_fiber::sleep_for(100ms);\n     EXPECT_EQ(shard->GetDefragStats().success_count, 0);\n-    // we are not running stats yet\n-    EXPECT_EQ(shard->GetDefragStats().tries, 0);\n-    EXPECT_GT(GetMallocCurrentCommitted(), NUMBER_OF_KEYS);\n+    // we are expecting to have at least one try by now\n+    EXPECT_GT(shard->GetDefragStats().tries, 0);\n+  });\n+\n+  ArgSlice delete_cmd(keys);\n+  auto r = CheckedInt(delete_cmd);\n+  // the first element in this is the command del so size is one less\n+  ASSERT_EQ(r, keys2delete.size() - 1);\n+\n+  // At this point we need to see whether we did running the task and whether the task did something\n+  shard_set->pool()->AwaitFiberOnAll([&](unsigned index, ProactorBase* base) {\n+    EngineShard* shard = EngineShard::tlocal();\n+    ASSERT_FALSE(shard == nullptr);  // we only have one and its should not be empty!\n+    // a \"busy wait\" to ensure that memory defragmentations was successful:\n+    // the task ran and did it work\n+    auto stats = shard->GetDefragStats();\n+    for (int i = 0; i < kMaxDefragTriesForTests; i++) {\n+      stats = shard->GetDefragStats();\n+      if (stats.success_count > 0) {\n+        break;\n+      }\n+      this_fiber::sleep_for(220ms);\n+    }\n+    // make sure that we successfully found places to defrag in memory\n+    EXPECT_GT(stats.success_count, 0);\n   });\n }\n \ndiff --git a/src/server/test_utils.cc b/src/server/test_utils.cc\nindex 02364dc4b97e..c0de0bdae76f 100644\n--- a/src/server/test_utils.cc\n+++ b/src/server/test_utils.cc\n@@ -26,6 +26,15 @@ ABSL_FLAG(bool, force_epoll, false, \"If true, uses epoll api instead iouring to\n \n namespace dfly {\n \n+std::ostream& operator<<(std::ostream& os, ArgSlice& list) {\n+  os << \"[\";\n+  if (!list.empty()) {\n+    std::for_each(list.begin(), list.end() - 1, [&os](const auto& val) { os << val << \", \"; });\n+    os << (*(list.end() - 1));\n+  }\n+  return os << \"]\";\n+}\n+\n extern unsigned kInitSegmentLog;\n \n using MP = MemcacheParser;\n@@ -267,7 +276,7 @@ auto BaseFamilyTest::GetMC(MP::CmdType cmd_type, std::initializer_list<std::stri\n   return conn->SplitLines();\n }\n \n-int64_t BaseFamilyTest::CheckedInt(std::initializer_list<std::string_view> list) {\n+int64_t BaseFamilyTest::CheckedInt(ArgSlice list) {\n   RespExpr resp = Run(list);\n   if (resp.type == RespExpr::INT64) {\n     return get<int64_t>(resp.u);\ndiff --git a/src/server/test_utils.h b/src/server/test_utils.h\nindex d4dc2957284c..975307992f76 100644\n--- a/src/server/test_utils.h\n+++ b/src/server/test_utils.h\n@@ -56,7 +56,10 @@ class BaseFamilyTest : public ::testing::Test {\n   MCResponse RunMC(MemcacheParser::CmdType cmd_type, std::string_view key = std::string_view{});\n   MCResponse GetMC(MemcacheParser::CmdType cmd_type, std::initializer_list<std::string_view> list);\n \n-  int64_t CheckedInt(std::initializer_list<std::string_view> list);\n+  int64_t CheckedInt(std::initializer_list<std::string_view> list) {\n+    return CheckedInt(ArgSlice{list.begin(), list.size()});\n+  }\n+  int64_t CheckedInt(ArgSlice list);\n \n   bool IsLocked(DbIndex db_index, std::string_view key) const;\n   ConnectionContext::DebugInfo GetDebugInfo(const std::string& id) const;\n",
  "problem_statement": "Introduce active defragmentation\nFollowing feedback from our customers - Dragonfly suffers from external fragmentation in some cases. \r\nThis problem is common for any [slab allocator](https://en.wikipedia.org/wiki/Slab_allocation) like tcmalloc, jemalloc and mimalloc. \r\nConsider the following scenario:\r\n\r\n1. an application allocates lots of \"blocks\" (from [mimalloc jargon](https://www.microsoft.com/en-us/research/uploads/prod/2019/06/mimalloc-tr-v1.pdf)) of size 384. Then the allocator mmaps (allocates from the OS) lots of slabs that handle blocks of size 384 and those slabs take up gigabytes of memory. This is a healthy situation because `usedmemory  ~= rss memory`.\r\n2. Then the application frees the vast majority of blocks across all the slabs of size 384. Most slabs are \"almost\" free but the allocator can not return it to the OS because some blocks are still busy. \r\n3. As a result one can observe external fragmentation of the \"unused\" memory hidden in those slabs (or pages in mimalloc jargon).\r\nsee the graph below. This situation is problematic and this is what we need to fix.\r\n\r\nI do not see any other solution besides active defragmentation, i.e. go over all the allocated items in the heap and grouping them together, thus compressing them in fewer slabs. Something similar is done by active-defrag in Redis using `je_get_defrag_hint` utility function.\r\n\r\nWe need to be smart about initial trigger conditions for the sweep and how we estimate the effectiveness of this defragmentation procedure in order not to introduce infinite loops.  \r\n\r\n![image](https://user-images.githubusercontent.com/3674760/198889901-c402c418-ef16-49b7-b5b1-e89dcb74eb08.png)\r\n\n",
  "hints_text": "The solution is to leverage helio's `OnIdle` task management framework.\r\nIn addition, we should add a heuristic that can categorize which pages (areas, slabs) should be defragmented.\r\nThis is how it was initially done in redis https://github.com/redis/redis/pull/5065/commits/e8099cabd19c4e3a46c94c39e69e13191d43f5eb though it evolved later.\r\n\r\nI suggest the following heuristic as a base line for mimalloc:\r\n\r\n```cpp\r\n\r\n// taken from mimalloc private code.\r\nstatic inline mi_slice_t* mi_slice_first(const mi_slice_t* slice) {\r\n  mi_slice_t* start = (mi_slice_t*)((uint8_t*)slice - slice->slice_offset);\r\n  return start;\r\n}\r\n\r\n// taken from mimalloc private code.\r\nstatic inline mi_segment_t* _mi_ptr_segment(const void* p) {\r\n  return (mi_segment_t*)((uintptr_t)p & ~MI_SEGMENT_MASK);\r\n}\r\n\r\n#define mi_likely(x) __builtin_expect(!!(x), true)\r\n\r\n\r\n// returns true if page is not active, and its used blocks <= capacity * ratio\r\nbool mi_heap_page_is_underutilized(mi_heap_t* heap, void* p, float ratio) {\r\n  mi_segment_t* const segment = _mi_ptr_segment(p);\r\n\r\n  // from _mi_segment_page_of\r\n  ptrdiff_t diff = (uint8_t*)p - (uint8_t*)segment;\r\n  size_t idx = (size_t)diff >> MI_SEGMENT_SLICE_SHIFT;\r\n  mi_slice_t* slice0 = (mi_slice_t*)&segment->slices[idx];\r\n  mi_slice_t* slice = mi_slice_first(slice0);  // adjust to the block that holds the page data\r\n  mi_page_t* page = (mi_page_t*)slice;\r\n  // end from _mi_segment_page_of //\r\n\r\n  // from mi_page_heap\r\n  mi_heap_t* page_heap = (mi_heap_t*)(mi_atomic_load_relaxed(&(page)->xheap));\r\n\r\n  // the heap id matches and it is not a full page\r\n  if (mi_likely(page_heap == heap && page->flags.x.in_full == 0)) {\r\n    // mi_page_queue_t* pq = mi_heap_page_queue_of(heap, page);\r\n\r\n    // first in the list, meaning it's the head of page queue, thus being used for malloc\r\n    if (page->prev == NULL)\r\n      return false;\r\n\r\n    // this page belong to this heap and is not first in the page queue. Lets check its\r\n    // utilization.\r\n    return page->used <= unsigned(page->capacity * ratio);\r\n  }\r\n  return false;\r\n}\r\n```\r\n\n\r\n a. We can trigger defragmentation based on thread-local conditions like RSS/Used ratio in a specific thread.\r\nWe can easily track \"used per thread\" via `EngineShard::UsedMemory`.  RSS usage can be approximated via `committed` stats in  `mi_heap_t.tls->stats`. The thing is that we call `mi_stats_merge` from all the shard threads and I suspect it resets thread local heap stats during the merge. We should fix per-thread accounting if we want to use thread-local triggers.\r\n\r\n b. Another option is to use process-global conditions like `GetMallocCurrentCommitted` and `used_mem_current`, but I prefer to explore option (a) first.\n- Will add idle task to check whether we need to run this operation. In case this is not required, we will return the lowest value (meaning let it wait for as long as possible for this task), if we do need to do the operation, it would return the highest value for the task so that this task will start executing until the memory defrag is done.\r\n- There is one issue with the above options: option a will not work, see [this issue with mi_malloc](https://github.com/microsoft/mimalloc/issues/645). so we will restore for the second option - using global from GetMallocCurrentCommitted and the value used_mem_current to calculate whether to run this or not.\nThis will implemented as follow:\r\n1. Check if memory usage is high enough\r\n2. Check if diff between commited and used memory is high enough\r\n3. Check if we have memory changes (to ensure that we not running endlessly).\r\n4. if all the above pass -> run on the shard and try to de-fragment memory by re-allocating values\r\n- if the cursor for this is signal that we are not done, schedule the task to run at high\r\n- priority otherwise lower the task priority so that it would not use the CPU when not required\r\n\r\nThe first version will not include the actual step of the memory re-allocation, to make this a simpler version of the \r\nalgorithm. The main reason for this is to be able to verify that we can use the Idle task and to identify that cases where this option is needed. Once we establish that this is indeed working in term of new task in the system, the low level details including the code above will be added.",
  "created_at": "2022-12-01T08:41:38Z",
  "modified_files": [
    "src/core/compact_object.cc",
    "src/core/compact_object.h",
    "src/server/dfly_main.cc",
    "src/server/engine_shard_set.cc",
    "src/server/engine_shard_set.h"
  ],
  "modified_test_files": [
    "src/core/compact_object_test.cc",
    "src/server/dragonfly_test.cc",
    "src/server/test_utils.cc",
    "src/server/test_utils.h"
  ]
}