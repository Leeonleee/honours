{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4877,
  "instance_id": "dragonflydb__dragonfly-4877",
  "issue_numbers": [
    "4775"
  ],
  "base_commit": "3eb84174534665601546474d03911357e21f6f4f",
  "patch": "diff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex 22ddc6547337..5d16b09cefa2 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -355,20 +355,9 @@ SliceEvents& SliceEvents::operator+=(const SliceEvents& o) {\n \n class DbSlice::PrimeBumpPolicy {\n  public:\n-  PrimeBumpPolicy(absl::flat_hash_set<uint64_t, FpHasher>* items) : fetched_items_(items) {\n-  }\n-\n-  // returns true if we can change the object location in dash table.\n   bool CanBump(const CompactObj& obj) const {\n-    if (obj.IsSticky()) {\n-      return false;\n-    }\n-    auto hc = obj.HashCode();\n-    return fetched_items_->insert(hc).second;\n+    return !obj.IsSticky();\n   }\n-\n- private:\n-  mutable absl::flat_hash_set<uint64_t, FpHasher>* fetched_items_;\n };\n \n DbSlice::DbSlice(uint32_t index, bool cache_mode, EngineShard* owner)\n@@ -565,21 +554,9 @@ auto DbSlice::FindInternal(const Context& cntx, string_view key, optional<unsign\n   }\n \n   DCHECK(IsValid(res.it));\n-  if (IsCacheMode()) {\n-    if (!change_cb_.empty()) {\n-      auto bump_cb = [&](PrimeTable::bucket_iterator bit) {\n-        CallChangeCallbacks(cntx.db_index, key, bit);\n-      };\n-      db.prime.CVCUponBump(change_cb_.back().first, res.it, bump_cb);\n-    }\n \n-    // We must not change the bucket's internal order during serialization\n-    serialization_latch_.Wait();\n-    auto bump_it = db.prime.BumpUp(res.it, PrimeBumpPolicy{&fetched_items_});\n-    if (bump_it != res.it) {  // the item was bumped\n-      res.it = bump_it;\n-      ++events_.bumpups;\n-    }\n+  if (IsCacheMode()) {\n+    fetched_items_.insert({res.it->first.HashCode(), cntx.db_index});\n   }\n \n   switch (stats_mode) {\n@@ -1714,10 +1691,40 @@ void DbSlice::PerformDeletion(Iterator del_it, DbTable* table) {\n   PerformDeletionAtomic(del_it, exp_it, table);\n }\n \n-void DbSlice::OnCbFinish() {\n-  // TBD update bumpups logic we can not clear now after cb finish as cb can preempt\n-  // btw what do we do with inline?\n-  fetched_items_.clear();\n+void DbSlice::OnCbFinishBlocking() {\n+  if (IsCacheMode()) {\n+    // move fetched items to local variable\n+    auto fetched_items = std::move(fetched_items_);\n+    for (const auto& [key_hash, db_index] : fetched_items) {\n+      auto& db = *db_arr_[db_index];\n+\n+      // We intentionally don't do extra key checking on this callback to speedup\n+      // fetching. Probability of having hash collision is quite low and for bumpup\n+      // purposes it should be fine if different key (with same hash) is returned.\n+      auto predicate = [](const PrimeKey&) { return true; };\n+\n+      PrimeIterator it = db.prime.FindFirst(key_hash, predicate);\n+\n+      if (!IsValid(it)) {\n+        continue;\n+      }\n+\n+      if (!change_cb_.empty()) {\n+        auto key = it->first.ToString();\n+        auto bump_cb = [&](PrimeTable::bucket_iterator bit) {\n+          CallChangeCallbacks(db_index, key, bit);\n+        };\n+        db.prime.CVCUponBump(change_cb_.back().first, it, bump_cb);\n+      }\n+\n+      // We must not change the bucket's internal order during serialization\n+      serialization_latch_.Wait();\n+      auto bump_it = db.prime.BumpUp(it, PrimeBumpPolicy{});\n+      if (bump_it != it) {  // the item was bumped\n+        ++events_.bumpups;\n+      }\n+    }\n+  }\n \n   if (!pending_send_map_.empty()) {\n     SendQueuedInvalidationMessages();\ndiff --git a/src/server/db_slice.h b/src/server/db_slice.h\nindex 623a537a78aa..b246c2727366 100644\n--- a/src/server/db_slice.h\n+++ b/src/server/db_slice.h\n@@ -357,7 +357,7 @@ class DbSlice {\n     return shard_id_;\n   }\n \n-  void OnCbFinish();\n+  void OnCbFinishBlocking();\n \n   bool Acquire(IntentLock::Mode m, const KeyLockArgs& lock_args);\n   void Release(IntentLock::Mode m, const KeyLockArgs& lock_args);\n@@ -615,10 +615,16 @@ class DbSlice {\n \n   DbTableArray db_arr_;\n \n+  // key for bump up items pair contains <key hash, db_index>\n+  using FetchedItemKey = std::pair<uint64_t, DbIndex>;\n+\n   struct FpHasher {\n     size_t operator()(uint64_t val) const {\n       return val;\n     }\n+    size_t operator()(const FetchedItemKey& val) const {\n+      return val.first;\n+    }\n   };\n \n   // Used in temporary computations in Acquire/Release.\n@@ -635,7 +641,7 @@ class DbSlice {\n   // for operations that preempt in the middle we have another mechanism -\n   // auto laundering iterators, so in case of preemption we do not mind that fetched_items are\n   // cleared or changed.\n-  mutable absl::flat_hash_set<uint64_t, FpHasher> fetched_items_;\n+  mutable absl::flat_hash_set<FetchedItemKey, FpHasher> fetched_items_;\n \n   // Registered by shard indices on when first document index is created.\n   DocDeletionCallback doc_del_cb_;\ndiff --git a/src/server/debugcmd.cc b/src/server/debugcmd.cc\nindex 892424a3ce1a..f0377c8c06f3 100644\n--- a/src/server/debugcmd.cc\n+++ b/src/server/debugcmd.cc\n@@ -903,11 +903,11 @@ void DebugCmd::PopulateRangeFiber(uint64_t from, uint64_t num_of_keys,\n \n   ess.AwaitRunningOnShardQueue([&](EngineShard* shard) {\n     DoPopulateBatch(options, ps[shard->shard_id()]);\n-    // Debug populate does not use transaction framework therefore we call OnCbFinish manually\n-    // after running the callback\n-    // Note that running debug populate while running flushall/db can cause dcheck fail because the\n-    // finish cb is executed just when we finish populating the database.\n-    cntx_->ns->GetDbSlice(shard->shard_id()).OnCbFinish();\n+    // Debug populate does not use transaction framework therefore we call OnCbFinishBlocking\n+    // manually after running the callback Note that running debug populate while running\n+    // flushall/db can cause dcheck fail because the finish cb is executed just when we finish\n+    // populating the database.\n+    cntx_->ns->GetDbSlice(shard->shard_id()).OnCbFinishBlocking();\n   });\n }\n \ndiff --git a/src/server/transaction.cc b/src/server/transaction.cc\nindex cb631e3780fc..af27812ae936 100644\n--- a/src/server/transaction.cc\n+++ b/src/server/transaction.cc\n@@ -695,7 +695,7 @@ void Transaction::RunCallback(EngineShard* shard) {\n   }\n \n   auto& db_slice = GetDbSlice(shard->shard_id());\n-  db_slice.OnCbFinish();\n+  db_slice.OnCbFinishBlocking();\n \n   // Handle result flags to alter behaviour.\n   if (result.flags & RunnableResult::AVOID_CONCLUDING) {\n@@ -1364,7 +1364,7 @@ OpStatus Transaction::RunSquashedMultiCb(RunnableType cb) {\n   auto& db_slice = GetDbSlice(shard->shard_id());\n \n   auto result = cb(this, shard);\n-  db_slice.OnCbFinish();\n+  db_slice.OnCbFinishBlocking();\n \n   LogAutoJournalOnShard(shard, result);\n   MaybeInvokeTrackingCb();\n",
  "test_patch": "diff --git a/src/server/string_family_test.cc b/src/server/string_family_test.cc\nindex 57343aae3003..d546644cfd12 100644\n--- a/src/server/string_family_test.cc\n+++ b/src/server/string_family_test.cc\n@@ -283,9 +283,9 @@ TEST_F(StringFamilyTest, MGetCachingModeBug2276) {\n \n   resp = Run({\"info\", \"stats\"});\n   size_t bumps1 = get_bump_ups(resp.GetString());\n-  EXPECT_GT(bumps1, 0);\n-  EXPECT_LT(bumps1, 10);  // we assume that some bumps are blocked because items reside next to each\n-                          // other in the slot.\n+\n+  EXPECT_GE(bumps1, 0);\n+  EXPECT_LE(bumps1, 10);\n \n   for (int i = 0; i < 10; ++i) {\n     auto get_resp = Run({\"get\", vec[i]});\n@@ -332,7 +332,7 @@ TEST_F(StringFamilyTest, MGetCachingModeBug2465) {\n \n   resp = Run({\"info\", \"stats\"});\n   size_t bumps = get_bump_ups(resp.GetString());\n-  EXPECT_EQ(bumps, 3);  // one bump for del and one for get and one for mget\n+  EXPECT_EQ(bumps, 2);  // one bump for get and one for mget\n }\n \n TEST_F(StringFamilyTest, MSetGet) {\ndiff --git a/tests/dragonfly/generic_test.py b/tests/dragonfly/generic_test.py\nindex 0f463efe6caf..ce785372bfaa 100644\n--- a/tests/dragonfly/generic_test.py\n+++ b/tests/dragonfly/generic_test.py\n@@ -114,7 +114,6 @@ async def block(id):\n             tasks.append(block(i))\n         await asyncio.gather(*tasks)\n \n-\n     # produce is constantly waking up consumers. It is used to trigger the\n     # flow that creates wake ups on a differrent database in the\n     # middle of continuation transaction.\n@@ -122,11 +121,12 @@ async def tasks_produce(num, iters):\n         LPUSH_SCRIPT = \"\"\"\n             redis.call('LPUSH', KEYS[1], \"val\")\n         \"\"\"\n+\n         async def produce(id):\n             c = df_server.client(db=1)  # important to be on a different db\n             for i in range(iters):\n                 # Must be a lua script and not multi-exec for some reason.\n-                await c.eval(LPUSH_SCRIPT, 1,  f\"list{{{id}}}\")\n+                await c.eval(LPUSH_SCRIPT, 1, f\"list{{{id}}}\")\n \n         tasks = []\n         for i in range(num):\n@@ -151,7 +151,6 @@ async def drain(id, iters):\n         await asyncio.gather(*tasks)\n         logging.info(\"Finished consuming\")\n \n-\n     num_keys = 32\n     num_iters = 200\n     async_task1 = asyncio.create_task(blmove_task_loose(num_keys))\n@@ -288,3 +287,58 @@ async def test_rename_huge_values(df_factory, type):\n     target_data = await StaticSeeder.capture(client)\n \n     assert source_data == target_data\n+\n+\n+@pytest.mark.asyncio\n+async def test_key_bump_ups(df_factory):\n+    master = df_factory.create(\n+        proactor_threads=2,\n+        cache_mode=\"true\",\n+    )\n+    df_factory.start_all([master])\n+    c_master = master.client()\n+\n+    await c_master.execute_command(\"DEBUG POPULATE 18000 KEY 32 RAND\")\n+\n+    info = await c_master.info(\"stats\")\n+    assert info[\"bump_ups\"] == 0\n+\n+    keys = await c_master.execute_command(\"SCAN 0\")\n+    keys = keys[1][0:10]\n+\n+    # Bump keys\n+    for key in keys:\n+        await c_master.execute_command(\"GET \" + key)\n+    info = await c_master.info(\"stats\")\n+    assert info[\"bump_ups\"] <= 10\n+\n+    # Multi get bump\n+    await c_master.execute_command(\"MGET \" + \" \".join(keys))\n+    info = await c_master.info(\"stats\")\n+    assert info[\"bump_ups\"] >= 10 and info[\"bump_ups\"] <= 20\n+    last_bump_ups = info[\"bump_ups\"]\n+\n+    for key in keys:\n+        await c_master.execute_command(\"DEL \" + key)\n+\n+    # DEL should not bump up any key\n+    info = await c_master.info(\"stats\")\n+    assert last_bump_ups == info[\"bump_ups\"]\n+\n+    #  Find key that has slot > 0 and bump it\n+    while True:\n+        keys = await c_master.execute_command(\"SCAN 0\")\n+        key = keys[1][0]\n+\n+        debug_key_info = await c_master.execute_command(\"DEBUG OBJECT \" + key)\n+        slot_id = int(dict(map(lambda s: s.split(\":\"), debug_key_info.split()))[\"slot\"])\n+        if slot_id == 0:\n+            # delete the key and continue\n+            await c_master.execute_command(\"DEL \" + key)\n+            continue\n+\n+        await c_master.execute_command(\"GET \" + key)\n+        debug_key_info = await c_master.execute_command(\"DEBUG OBJECT \" + key)\n+        new_slot_id = int(dict(map(lambda s: s.split(\":\"), debug_key_info.split()))[\"slot\"])\n+        assert new_slot_id + 1 == slot_id\n+        break\n",
  "problem_statement": "Move bumpup logic out of FindInternal\nWe should try moving it into `DbSlice::OnCbFinish`. \nwe do not need precise logic for bump ups and we can use fetched_items_ as a set of fingerprints that needs \nto be bumped up in `OnCbFinish`\n\nThis will greatly simplify `FindInternal`\n",
  "hints_text": "",
  "created_at": "2025-04-02T07:34:32Z",
  "modified_files": [
    "src/server/db_slice.cc",
    "src/server/db_slice.h",
    "src/server/debugcmd.cc",
    "src/server/transaction.cc"
  ],
  "modified_test_files": [
    "src/server/string_family_test.cc",
    "tests/dragonfly/generic_test.py"
  ]
}