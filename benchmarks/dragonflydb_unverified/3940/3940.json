{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 3940,
  "instance_id": "dragonflydb__dragonfly-3940",
  "issue_numbers": [
    "3934"
  ],
  "base_commit": "84e22aa658abca945a9d731af478acff727c70e8",
  "patch": "diff --git a/src/facade/reply_builder.h b/src/facade/reply_builder.h\nindex 534ac2f036d8..181e0338f742 100644\n--- a/src/facade/reply_builder.h\n+++ b/src/facade/reply_builder.h\n@@ -442,7 +442,7 @@ class RedisReplyBuilder2Base : public SinkReplyBuilder2, public RedisReplyBuilde\n   }\n \n   void StartAggregate() override {\n-    aggregators_.emplace_back(SinkReplyBuilder2::ReplyAggregator(this));\n+    aggregators_.emplace_back(this);\n   }\n \n   void StopAggregate() override {\n",
  "test_patch": "diff --git a/tests/dragonfly/connection_test.py b/tests/dragonfly/connection_test.py\nindex d10774e8e0d7..1c3cc89ef3d9 100755\n--- a/tests/dragonfly/connection_test.py\n+++ b/tests/dragonfly/connection_test.py\n@@ -523,6 +523,59 @@ async def test_keyspace_events_config_set(async_client: aioredis.Redis):\n         pass\n \n \n+async def test_reply_count(async_client: aioredis.Redis):\n+    \"\"\"Make sure reply aggregations reduce reply counts for common cases\"\"\"\n+\n+    async def get_reply_count():\n+        return (await async_client.info(\"STATS\"))[\"reply_count\"]\n+\n+    async def measure(aw):\n+        before = await get_reply_count()\n+        await aw\n+        return await get_reply_count() - before - 1\n+\n+    base = await get_reply_count()\n+    info_diff = await get_reply_count() - base\n+    assert info_diff == 1\n+\n+    # Warm client buffer up\n+    await async_client.lpush(\"warmup\", *(i for i in range(500)))\n+    await async_client.lrange(\"warmup\", 0, -1)\n+\n+    # Integer list\n+    await async_client.lpush(\"list-1\", *(i for i in range(100)))\n+    assert await measure(async_client.lrange(\"list-1\", 0, -1)) == 1\n+\n+    # Integer set\n+    await async_client.sadd(\"set-1\", *(i for i in range(100)))\n+    assert await measure(async_client.smembers(\"set-1\")) == 1\n+\n+    # Sorted sets\n+    await async_client.zadd(\"zset-1\", mapping={str(i): i for i in range(50)})\n+    assert await measure(async_client.zrange(\"zset-1\", 0, -1, withscores=True)) == 1\n+\n+    # Exec call\n+    e = async_client.pipeline(transaction=True)\n+    for _ in range(100):\n+        e.incr(\"num-1\")\n+    assert await measure(e.execute()) == 2  # OK + Response\n+\n+    # Just pipeline\n+    p = async_client.pipeline(transaction=False)\n+    for _ in range(100):\n+        p.incr(\"num-1\")\n+    assert await measure(p.execute()) == 1\n+\n+    # Script result\n+    assert await measure(async_client.eval('return {1,2,{3,4},5,6,7,8,\"nine\"}', 0)) == 1\n+\n+    # Search results\n+    await async_client.execute_command(\"FT.CREATE i1 SCHEMA name text\")\n+    for i in range(50):\n+        await async_client.hset(f\"key-{i}\", \"name\", f\"name number {i}\")\n+    assert await measure(async_client.ft(\"i1\").search(\"*\")) == 1\n+\n+\n async def test_big_command(df_server, size=8 * 1024):\n     reader, writer = await asyncio.open_connection(\"127.0.0.1\", df_server.port)\n \n",
  "problem_statement": "regression: long latency with experimental_new_io=true\nThe latency is measured for this call :\r\nhttps://github.com/dragonflydb/dragonfly/blob/7870f594660539f48894e37a8fd9d6a133fa21ff/src/server/main_service.cc#L2163\r\n\r\nwith `experimental_new_io=true` - eval replies with latencies reaching 100ms and more.\r\nwith `experimental_new_io=false` - eval has sub-ms latency.\r\n\r\nThe bug is easily reproduced by running the following command via redis-cli for both modes: \r\n`EVAL \"local result = {}; for i = 1, 1000 do result[i] = i; end; return result;\" 0`\r\n\r\nand then checking `redis-cli SCRIPT LATENCY` \n",
  "hints_text": "",
  "created_at": "2024-10-16T16:58:06Z",
  "modified_files": [
    "src/facade/reply_builder.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/connection_test.py"
  ]
}