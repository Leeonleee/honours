{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 5308,
  "instance_id": "dragonflydb__dragonfly-5308",
  "issue_numbers": [
    "5293"
  ],
  "base_commit": "b6b22479de20f7ac82b2eb19e0c6ab2a28225064",
  "patch": "diff --git a/src/server/dflycmd.cc b/src/server/dflycmd.cc\nindex 0f168ba68725..2295c1f26c3d 100644\n--- a/src/server/dflycmd.cc\n+++ b/src/server/dflycmd.cc\n@@ -658,7 +658,8 @@ void DflyCmd::StartStableSyncInThread(FlowInfo* flow, ExecutionState* exec_st, E\n   DCHECK(shard);\n   DCHECK(flow->conn);\n \n-  flow->streamer.reset(new JournalStreamer(sf_->journal(), exec_st, JournalStreamer::SendLsn::YES));\n+  flow->streamer.reset(\n+      new JournalStreamer(sf_->journal(), exec_st, JournalStreamer::SendLsn::YES, true));\n   flow->streamer->Start(flow->conn->socket());\n \n   // Register cleanup.\ndiff --git a/src/server/journal/pending_buf.h b/src/server/journal/pending_buf.h\nindex 6049bd05d6b2..55114e64425a 100644\n--- a/src/server/journal/pending_buf.h\n+++ b/src/server/journal/pending_buf.h\n@@ -36,10 +36,9 @@ class PendingBuf {\n     if (bufs_.back().buf.size() == Buf::kMaxBufSize) {\n       bufs_.emplace_back();\n     }\n-    auto& fron_buf = bufs_.back();\n-\n-    fron_buf.mem_size += str.size();\n-    fron_buf.buf.push_back(std::move(str));\n+    auto& front_buf = bufs_.back();\n+    front_buf.mem_size += str.size();\n+    front_buf.buf.push_back(std::move(str));\n   }\n \n   // should be called to get the next buffer for sending\n@@ -51,6 +50,10 @@ class PendingBuf {\n     return bufs_.front();\n   }\n \n+  size_t FrontBufSize() const {\n+    return bufs_.front().mem_size;\n+  }\n+\n   // should be called when the buf from PrepareSendingBuf() method was sent\n   void Pop() {\n     DCHECK(bufs_.size() >= 2);\ndiff --git a/src/server/journal/streamer.cc b/src/server/journal/streamer.cc\nindex db57b6128a57..79dfeec33252 100644\n--- a/src/server/journal/streamer.cc\n+++ b/src/server/journal/streamer.cc\n@@ -24,6 +24,9 @@ ABSL_FLAG(uint32_t, replication_stream_output_limit, 64_KB,\n ABSL_FLAG(uint32_t, migration_buckets_serialization_threshold, 100,\n           \"The Number of buckets to serialize on each iteration before yielding\");\n \n+ABSL_FLAG(uint32_t, replication_dispatch_threshold, 1500,\n+          \"Number of bytes to aggregate before replication\");\n+\n namespace dfly {\n using namespace util;\n using namespace journal;\n@@ -36,13 +39,18 @@ iovec IoVec(io::Bytes src) {\n \n uint32_t replication_stream_output_limit_cached = 64_KB;\n uint32_t migration_buckets_serialization_threshold_cached = 100;\n+uint32_t replication_dispatch_threshold = 1500;\n+uint32_t stalled_writer_base_period_ms = 10;\n \n }  // namespace\n \n-JournalStreamer::JournalStreamer(journal::Journal* journal, ExecutionState* cntx, SendLsn send_lsn)\n-    : cntx_(cntx), journal_(journal), send_lsn_(send_lsn) {\n+JournalStreamer::JournalStreamer(journal::Journal* journal, ExecutionState* cntx, SendLsn send_lsn,\n+                                 bool is_stable_sync)\n+    : cntx_(cntx), journal_(journal), is_stable_sync_(is_stable_sync), send_lsn_(send_lsn) {\n   // cache the flag to avoid accessing it later.\n   replication_stream_output_limit_cached = absl::GetFlag(FLAGS_replication_stream_output_limit);\n+  replication_dispatch_threshold = absl::GetFlag(FLAGS_replication_dispatch_threshold);\n+  last_async_write_time_ = fb2::ProactorBase::GetMonotonicTimeNs() / 1000000;\n }\n \n JournalStreamer::~JournalStreamer() {\n@@ -75,27 +83,72 @@ void JournalStreamer::Start(util::FiberSocketBase* dest) {\n   CHECK(dest_ == nullptr && dest != nullptr);\n   dest_ = dest;\n   journal_cb_id_ = journal_->RegisterOnChange(this);\n+  StartStalledDataWriterFiber();\n }\n \n void JournalStreamer::Cancel() {\n   VLOG(1) << \"JournalStreamer::Cancel\";\n   waker_.notifyAll();\n   journal_->UnregisterOnChange(journal_cb_id_);\n-  if (!cntx_->IsError()) {\n-    WaitForInflightToComplete();\n-  }\n+  StopStalledDataWriterFiber();\n+  WaitForInflightToComplete();\n }\n \n size_t JournalStreamer::UsedBytes() const {\n   return pending_buf_.Size();\n }\n \n-void JournalStreamer::AsyncWrite() {\n-  DCHECK(!pending_buf_.Empty());\n+void JournalStreamer::Write(std::string str) {\n+  DCHECK(!str.empty());\n+  DVLOG(3) << \"Writing \" << str.size() << \" bytes\";\n+\n+  pending_buf_.Push(std::move(str));\n+  AsyncWrite(false);\n+}\n+\n+void JournalStreamer::StartStalledDataWriterFiber() {\n+  if (is_stable_sync_ && !stalled_data_writer_.IsJoinable()) {\n+    auto pb = fb2::ProactorBase::me();\n+    std::chrono::milliseconds period_us(stalled_writer_base_period_ms);\n+    stalled_data_writer_ = MakeFiber([this, index = pb->GetPoolIndex(), period_us]() mutable {\n+      ThisFiber::SetName(absl::StrCat(\"fiber_periodic_journal_writer_\", index));\n+      this->StalledDataWriterFiber(period_us, &stalled_data_writer_done_);\n+    });\n+  }\n+}\n+\n+void JournalStreamer::StalledDataWriterFiber(std::chrono::milliseconds period_ms,\n+                                             util::fb2::Done* waiter) {\n+  while (cntx_->IsRunning()) {\n+    if (waiter->WaitFor(period_ms)) {\n+      if (!cntx_->IsRunning()) {\n+        return;\n+      }\n+    }\n+\n+    // We don't want to force async write to replicate if last data\n+    // was written recent. Data needs to be stalled for period_ms duration.\n+    if (!pending_buf_.Size() || in_flight_bytes_ > 0 ||\n+        ((last_async_write_time_ + period_ms.count()) >\n+         (fb2::ProactorBase::GetMonotonicTimeNs() / 1000000))) {\n+      continue;\n+    }\n \n+    AsyncWrite(true);\n+  }\n+}\n+\n+void JournalStreamer::AsyncWrite(bool force_send) {\n+  // Stable sync or RestoreStreamer replication can't write data until\n+  // previous AsyncWriter finished.\n   if (in_flight_bytes_ > 0) {\n-    // We can not flush data while there are in flight requests because AsyncWrite\n-    // is not atomic. Therefore, we just aggregate.\n+    return;\n+  }\n+\n+  // Writing in stable sync and outside of fiber needs to check\n+  // threshold before writing data.\n+  if (is_stable_sync_ && !force_send &&\n+      pending_buf_.FrontBufSize() < replication_dispatch_threshold) {\n     return;\n   }\n \n@@ -103,6 +156,7 @@ void JournalStreamer::AsyncWrite() {\n \n   in_flight_bytes_ = cur_buf.mem_size;\n   total_sent_ += in_flight_bytes_;\n+  last_async_write_time_ = fb2::ProactorBase::GetMonotonicTimeNs() / 1000000;\n \n   const auto v_size = cur_buf.buf.size();\n   absl::InlinedVector<iovec, 8> v(v_size);\n@@ -112,18 +166,8 @@ void JournalStreamer::AsyncWrite() {\n     v[i] = IoVec(io::Bytes(uptr, cur_buf.buf[i].size()));\n   }\n \n-  dest_->AsyncWrite(v.data(), v.size(), [this, len = in_flight_bytes_](std::error_code ec) {\n-    OnCompletion(std::move(ec), len);\n-  });\n-}\n-\n-void JournalStreamer::Write(std::string str) {\n-  DCHECK(!str.empty());\n-  DVLOG(3) << \"Writing \" << str.size() << \" bytes\";\n-\n-  pending_buf_.Push(std::move(str));\n-\n-  AsyncWrite();\n+  dest_->AsyncWrite(v.data(), v.size(),\n+                    [this, len = in_flight_bytes_](std::error_code ec) { OnCompletion(ec, len); });\n }\n \n void JournalStreamer::OnCompletion(std::error_code ec, size_t len) {\n@@ -136,7 +180,7 @@ void JournalStreamer::OnCompletion(std::error_code ec, size_t len) {\n     if (ec) {\n       cntx_->ReportError(ec);\n     } else if (!pending_buf_.Empty()) {\n-      AsyncWrite();\n+      AsyncWrite(false);\n     }\n   }\n \n@@ -176,13 +220,22 @@ void JournalStreamer::WaitForInflightToComplete() {\n   }\n }\n \n+void JournalStreamer::StopStalledDataWriterFiber() {\n+  if (is_stable_sync_ && stalled_data_writer_.IsJoinable()) {\n+    stalled_data_writer_done_.Notify();\n+    if (stalled_data_writer_.IsJoinable()) {\n+      stalled_data_writer_.Join();\n+    }\n+  }\n+}\n+\n bool JournalStreamer::IsStalled() const {\n   return pending_buf_.Size() >= replication_stream_output_limit_cached;\n }\n \n RestoreStreamer::RestoreStreamer(DbSlice* slice, cluster::SlotSet slots, journal::Journal* journal,\n                                  ExecutionState* cntx)\n-    : JournalStreamer(journal, cntx, JournalStreamer::SendLsn::NO),\n+    : JournalStreamer(journal, cntx, JournalStreamer::SendLsn::NO, false),\n       db_slice_(slice),\n       my_slots_(std::move(slots)) {\n   DCHECK(slice != nullptr);\ndiff --git a/src/server/journal/streamer.h b/src/server/journal/streamer.h\nindex d06f5c32efff..ea48bac5865c 100644\n--- a/src/server/journal/streamer.h\n+++ b/src/server/journal/streamer.h\n@@ -21,7 +21,8 @@ namespace dfly {\n class JournalStreamer : public journal::JournalConsumerInterface {\n  public:\n   enum class SendLsn { NO = 0, YES = 1 };\n-  JournalStreamer(journal::Journal* journal, ExecutionState* cntx, SendLsn send_lsn);\n+  JournalStreamer(journal::Journal* journal, ExecutionState* cntx, SendLsn send_lsn,\n+                  bool is_stable_sync);\n   virtual ~JournalStreamer();\n \n   // Self referential.\n@@ -60,16 +61,26 @@ class JournalStreamer : public journal::JournalConsumerInterface {\n   ExecutionState* cntx_;\n \n  private:\n-  void AsyncWrite();\n+  void AsyncWrite(bool force_send);\n   void OnCompletion(std::error_code ec, size_t len);\n \n   bool IsStalled() const;\n \n   journal::Journal* journal_;\n \n+  util::fb2::Fiber stalled_data_writer_;\n+  util::fb2::Done stalled_data_writer_done_;\n+  void StartStalledDataWriterFiber();\n+  void StopStalledDataWriterFiber();\n+  void StalledDataWriterFiber(std::chrono::milliseconds period_ms, util::fb2::Done* waiter);\n+\n   PendingBuf pending_buf_;\n \n+  // If we are replication in stable sync we can aggregate data before sending\n+  bool is_stable_sync_;\n   size_t in_flight_bytes_ = 0, total_sent_ = 0;\n+  // Last time that send data in milliseconds\n+  uint64_t last_async_write_time_ = 0;\n   time_t last_lsn_time_ = 0;\n   LSN last_lsn_writen_ = 0;\n   util::fb2::EventCount waker_;\n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex 6fed1286f432..2977f9c047ce 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -835,7 +835,7 @@ async def test_cluster_replica_sets_non_owned_keys(df_factory: DflyInstanceFacto\n         assert re.match(r\"MOVED \\d+ localhost:1111\", e.value.args[0])\n \n         await push_config(replica_config, [c_master_admin])\n-        await asyncio.sleep(0.5)\n+        await check_all_replicas_finished([c_replica], c_master)\n         assert await c_master.execute_command(\"dbsize\") == 0\n         assert await c_replica.execute_command(\"dbsize\") == 0\n \n@@ -941,7 +941,7 @@ async def test_cluster_flush_slots_after_config_change(df_factory: DflyInstanceF\n     \"\"\"\n     await push_config(config, [c_master_admin, c_replica_admin])\n \n-    await asyncio.sleep(0.5)\n+    await check_all_replicas_finished([c_replica], c_master)\n \n     assert await c_master.execute_command(\"dbsize\") == (100_000 - slot_0_size)\n     assert await c_replica.execute_command(\"dbsize\") == (100_000 - slot_0_size)\n@@ -1146,6 +1146,10 @@ async def test_random_keys():\n             assert await client.get(key) == \"value\"\n \n     await test_random_keys()\n+\n+    for i in range(3):\n+        await check_all_replicas_finished([c_replicas[i]], c_masters_admin[i])\n+\n     await asyncio.gather(*(wait_available_async(c) for c in c_replicas))\n \n     # Make sure that getting a value from a replica works as well.\ndiff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 7b9196158e5d..1b64a33db56b 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -3016,6 +3016,8 @@ async def test_replica_snapshot_with_big_values_while_seeding(df_factory: DflyIn\n     await seeder.stop(c_master)\n     await stream_task\n \n+    await check_all_replicas_finished([c_replica], c_master)\n+\n     # Check that everything is in sync\n     hashes = await asyncio.gather(*(SeederV2.capture(c) for c in [c_master, c_replica]))\n     assert len(set(hashes)) == 1\n",
  "problem_statement": "Aggregate data until threshold is reached\nCurrently we are replicating data as it is executed in JournalStreamer (with exception that we can aggregate if previous command replicating is currently in progress). Idea would be to aggregate data until threshold is reached and than replicate this bigger chunk of data. \n\nAlso closes #4974 \n",
  "hints_text": "",
  "created_at": "2025-06-16T09:09:06Z",
  "modified_files": [
    "src/server/dflycmd.cc",
    "src/server/journal/pending_buf.h",
    "src/server/journal/streamer.cc",
    "src/server/journal/streamer.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py",
    "tests/dragonfly/replication_test.py"
  ]
}