diff --git a/src/server/dragonfly_test.cc b/src/server/dragonfly_test.cc
index df5827c527fe..ee61612c1754 100644
--- a/src/server/dragonfly_test.cc
+++ b/src/server/dragonfly_test.cc
@@ -457,9 +457,12 @@ TEST_F(DflyEngineTest, OOM) {
 /// and then written with the same key.
 TEST_F(DflyEngineTest, Bug207) {
   max_memory_limit = 300000;
-  shard_set->TEST_EnableCacheMode();
+
   absl::FlagSaver fs;
   absl::SetFlag(&FLAGS_oom_deny_ratio, 4);
+  ResetService();
+
+  shard_set->TEST_EnableCacheMode();
 
   ssize_t i = 0;
   RespExpr resp;
@@ -486,11 +489,11 @@ TEST_F(DflyEngineTest, Bug207) {
 }
 
 TEST_F(DflyEngineTest, StickyEviction) {
-  shard_set->TEST_EnableCacheMode();
+  max_memory_limit = 300000;
   absl::FlagSaver fs;
   absl::SetFlag(&FLAGS_oom_deny_ratio, 4);
-
-  max_memory_limit = 300000;
+  ResetService();
+  shard_set->TEST_EnableCacheMode();
 
   string tmp_val(100, '.');
 
diff --git a/src/server/test_utils.cc b/src/server/test_utils.cc
index 8659ba409352..b24b4f7ef2c0 100644
--- a/src/server/test_utils.cc
+++ b/src/server/test_utils.cc
@@ -28,6 +28,7 @@ extern "C" {
 using namespace std;
 
 ABSL_DECLARE_FLAG(string, dbfilename);
+ABSL_DECLARE_FLAG(double, rss_oom_deny_ratio);
 ABSL_DECLARE_FLAG(uint32_t, num_shards);
 ABSL_FLAG(bool, force_epoll, false, "If true, uses epoll api instead iouring to run tests");
 ABSL_DECLARE_FLAG(size_t, acllog_max_len);
@@ -152,6 +153,7 @@ BaseFamilyTest::~BaseFamilyTest() {
 void BaseFamilyTest::SetUpTestSuite() {
   kInitSegmentLog = 1;
 
+  absl::SetFlag(&FLAGS_rss_oom_deny_ratio, -1);
   absl::SetFlag(&FLAGS_dbfilename, "");
   init_zmalloc_threadlocal(mi_heap_get_backing());
 
diff --git a/tests/dragonfly/generic_test.py b/tests/dragonfly/generic_test.py
index 2de1cab1e5f9..5b95d23cd534 100644
--- a/tests/dragonfly/generic_test.py
+++ b/tests/dragonfly/generic_test.py
@@ -125,11 +125,15 @@ async def test_restricted_commands(df_factory):
 @pytest.mark.asyncio
 async def test_reply_guard_oom(df_factory, df_seeder_factory):
     master = df_factory.create(
-        proactor_threads=1, cache_mode="true", maxmemory="256mb", enable_heartbeat_eviction="false"
+        proactor_threads=1,
+        cache_mode="true",
+        maxmemory="256mb",
+        enable_heartbeat_eviction="false",
+        rss_oom_deny_ratio=2,
     )
     df_factory.start_all([master])
     c_master = master.client()
-    await c_master.execute_command("DEBUG POPULATE 6000 size 44000")
+    await c_master.execute_command("DEBUG POPULATE 6000 size 40000")
 
     seeder = df_seeder_factory.create(
         port=master.port, keys=5000, val_size=1000, stop_on_failure=False
diff --git a/tests/dragonfly/memory_test.py b/tests/dragonfly/memory_test.py
index 060af26af150..f2f82587eb49 100644
--- a/tests/dragonfly/memory_test.py
+++ b/tests/dragonfly/memory_test.py
@@ -2,6 +2,7 @@
 from redis import asyncio as aioredis
 from .utility import *
 import logging
+from . import dfly_args
 
 
 @pytest.mark.opt_only
@@ -47,3 +48,62 @@ async def test_rss_used_mem_gap(df_factory, type, keys, val_size, elements):
         assert delta < max_unaccounted
 
     await disconnect_clients(client)
+
+
+@pytest.mark.asyncio
+@dfly_args(
+    {
+        "maxmemory": "512mb",
+        "proactor_threads": 2,
+        "rss_oom_deny_ratio": 0.5,
+    }
+)
+@pytest.mark.parametrize("admin_port", [0, 1112])
+async def test_rss_oom_ratio(df_factory, admin_port):
+    """
+    Test dragonfly rejects denyoom commands and new connections when rss memory is above maxmemory*rss_oom_deny_ratio
+    Test dragonfly does not rejects when rss memory goes below threshold
+    """
+    df_server = df_factory.create(admin_port=admin_port)
+    df_server.start()
+
+    client = aioredis.Redis(port=df_server.port)
+    await client.execute_command("DEBUG POPULATE 10000 key 40000 RAND")
+
+    await asyncio.sleep(1)  # Wait for another RSS heartbeat update in Dragonfly
+
+    port = df_server.admin_port if admin_port else df_server.port
+    new_client = aioredis.Redis(port=port)
+    await new_client.ping()
+
+    info = await new_client.info("memory")
+    logging.debug(f'Used memory {info["used_memory"]}, rss {info["used_memory_rss"]}')
+
+    reject_limit = 256 * 1024 * 1024  # 256mb
+    assert info["used_memory_rss"] > reject_limit
+
+    # get command from existing connection should not be rejected
+    await client.execute_command("get x")
+
+    # reject set due to oom
+    with pytest.raises(redis.exceptions.ResponseError):
+        await client.execute_command("set x y")
+
+    if admin_port:
+        # new client create should also fail if admin port was set
+        client = aioredis.Redis(port=df_server.port)
+        with pytest.raises(redis.exceptions.ConnectionError):
+            await client.ping()
+
+    # flush to free memory
+    await new_client.flushall()
+
+    await asyncio.sleep(2)  # Wait for another RSS heartbeat update in Dragonfly
+
+    info = await new_client.info("memory")
+    logging.debug(f'Used memory {info["used_memory"]}, rss {info["used_memory_rss"]}')
+    assert info["used_memory_rss"] < reject_limit
+
+    # new client create shoud not fail after memory usage decrease
+    client = aioredis.Redis(port=df_server.port)
+    await client.execute_command("set x y")
diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py
index b87979855a6f..e2d3264b6446 100644
--- a/tests/dragonfly/replication_test.py
+++ b/tests/dragonfly/replication_test.py
@@ -2085,6 +2085,7 @@ async def test_policy_based_eviction_propagation(df_factory, df_seeder_factory):
         maxmemory="512mb",
         logtostdout="true",
         enable_heartbeat_eviction="false",
+        rss_oom_deny_ratio=1.3,
     )
     replica = df_factory.create(proactor_threads=2)
     df_factory.start_all([master, replica])
diff --git a/tests/dragonfly/snapshot_test.py b/tests/dragonfly/snapshot_test.py
index 017a4134c30d..e44dd29f7566 100644
--- a/tests/dragonfly/snapshot_test.py
+++ b/tests/dragonfly/snapshot_test.py
@@ -567,6 +567,7 @@ async def test_tiered_entries_throttle(async_client: aioredis.Redis):
         ("LIST"),
     ],
 )
+@pytest.mark.slow
 async def test_big_value_serialization_memory_limit(df_factory, query):
     dbfilename = f"dump_{tmp_file_name()}"
     instance = df_factory.create(dbfilename=dbfilename)
