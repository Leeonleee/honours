{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 5094,
  "instance_id": "dragonflydb__dragonfly-5094",
  "issue_numbers": [
    "4992"
  ],
  "base_commit": "8ce8ee68df9844e93c6cdeea6f16200782507bd1",
  "patch": "diff --git a/src/server/rdb_load.cc b/src/server/rdb_load.cc\nindex 01b2d8653584..a3444e92aeeb 100644\n--- a/src/server/rdb_load.cc\n+++ b/src/server/rdb_load.cc\n@@ -482,11 +482,12 @@ void RdbLoaderBase::OpaqueObjLoader::CreateHMap(const LoadTrace* ltrace) {\n       }\n     });\n     std::string key;\n+    std::string val;\n     for (size_t i = 0; i < ltrace->arr.size(); i += increment) {\n       // ToSV may reference an internal buffer, therefore we can use only before the\n       // next call to ToSV. To workaround, copy the key locally.\n       key = ToSV(ltrace->arr[i].rdb_var);\n-      string_view val = ToSV(ltrace->arr[i + 1].rdb_var);\n+      val = ToSV(ltrace->arr[i + 1].rdb_var);\n \n       if (ec_)\n         return;\n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 3e1bba44440d..6a31de0404bb 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -1,23 +1,18 @@\n-import random\n-\n-from itertools import chain, repeat\n-import re\n-import pytest\n-import asyncio\n-import async_timeout\n import platform\n-import pymemcache\n-import logging\n+import shutil\n import tarfile\n import urllib.request\n-import shutil\n-from redis import asyncio as aioredis\n-from .utility import *\n-from .instance import DflyInstanceFactory, DflyInstance\n-from .seeder import Seeder as SeederV2\n+from itertools import chain, repeat\n+\n+import async_timeout\n+import pymemcache\n+\n from . import dfly_args\n+from .instance import DflyInstanceFactory, DflyInstance\n from .proxy import Proxy\n from .seeder import DebugPopulateSeeder\n+from .seeder import Seeder as SeederV2\n+from .utility import *\n \n ADMIN_PORT = 1211\n \n@@ -3115,3 +3110,23 @@ async def test_partial_replication_on_same_source_master(df_factory, use_takeove\n         lines = replica2.find_in_logs(f\"Started full with localhost:{replica1.port}\")\n         assert len(lines) == 0\n         assert len(replica1.find_in_logs(\"No partial sync due to diff\")) > 0\n+\n+\n+async def test_replicate_hset_with_expiry(df_factory: DflyInstanceFactory):\n+    master = df_factory.create(proactor_threads=2)\n+    replica = df_factory.create(proactor_threads=2)\n+\n+    master.start()\n+    replica.start()\n+\n+    cm = master.client()\n+    await cm.execute_command(\"HSETEX key 86400 name 1234\")\n+\n+    cr = replica.client()\n+    await cr.execute_command(f\"REPLICAOF localhost {master.port}\")\n+    await wait_available_async(cr)\n+\n+    result = await cr.hgetall(\"key\")\n+\n+    assert \"name\" in result\n+    assert result[\"name\"] == \"1234\"\n",
  "problem_statement": "HSETEX values become \"7\" when syncing nodes\n**Describe the bug**\nHello, we are experiencing a weird bug where all our values become \u201c7\u201d using the HSETEX command when syncing dragonfly nodes. \n\n**To Reproduce**\n\nWe have two 8gb dragonfly nodes in kubernetes and one operator. Version v1.1.9 of Helm Chart for operator, and we are running with backup: dbfilename=anbdb with hourly snapshot.\n\nPopulate the database with:\n`HSETEX a 86400 x 1 y 3 z 4`\nand let it sync, then  if I delete one of the pods, then the values after syncing in the deleted pod will become all \u201c7\u201d:\n```\nhgetall a\n1) \"x\"\n2) \"7\"\n3) \"z\"\n4) \"7\"\n5) \"y\"\n6) \"7\"\n```\n\n**Expected behavior**\nExpected values to be as before sync.\n\n**Screenshots**\nNA\n\n**Environment (please complete the following information):**\n - OS: Ubuntu 22.04.5 LTS\n - Kernel: 5.15.0-1086-azure\n\n - Containerized?: kubernetes\n - Dragonfly Version: 1.26.2\n\n\nwe experienced this half a year ago too (didnt report it though) but all the values were then \"6\" - so at least now we know its not the devil roaming in there\n",
  "hints_text": "Thanks for reporting this @kristjgr \nruning the command\n HSETEX a 86400 x 1 y 3 z 4\nrestart dragonfly that loads the snapshot will result with this bug\nIt looks like we store the data in the dfs file correctly, and we even load it back correctly, but while loading the `loadtrace` object into hashmap, we create a string view on the same pointer, first for the value and then for the ttl. The ttl overwrites the value, the 7 is the first character of the ttl string.",
  "created_at": "2025-05-10T05:42:26Z",
  "modified_files": [
    "src/server/rdb_load.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}