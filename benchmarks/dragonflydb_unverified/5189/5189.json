{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 5189,
  "instance_id": "dragonflydb__dragonfly-5189",
  "issue_numbers": [
    "5135"
  ],
  "base_commit": "e241efc420ee8193f4bd81fb45cc7dd6303baf4a",
  "patch": "diff --git a/src/server/dflycmd.cc b/src/server/dflycmd.cc\nindex 9e154024e86e..4e0a34941bce 100644\n--- a/src/server/dflycmd.cc\n+++ b/src/server/dflycmd.cc\n@@ -300,6 +300,14 @@ void DflyCmd::Flow(CmdArgList args, RedisReplyBuilder* rb, ConnectionContext* cn\n     flow.eof_token = eof_token;\n     flow.version = replica_ptr->version;\n \n+    if (!cntx->conn()->Migrate(shard_set->pool()->at(flow_id))) {\n+      // Listener::PreShutdown() triggered\n+      if (cntx->conn()->socket()->IsOpen()) {\n+        return rb->SendError(kInvalidState);\n+      }\n+      return;\n+    }\n+\n #if 0  // Partial synchronization is disabled\n   if (seqid.has_value()) {\n     if (sf_->journal()->IsLSNInBuffer(*seqid) || sf_->journal()->GetLsn() == *seqid) {\n@@ -346,13 +354,6 @@ void DflyCmd::Flow(CmdArgList args, RedisReplyBuilder* rb, ConnectionContext* cn\n     }\n   }\n \n-  if (!cntx->conn()->Migrate(shard_set->pool()->at(flow_id))) {\n-    // Listener::PreShutdown() triggered\n-    if (cntx->conn()->socket()->IsOpen()) {\n-      return rb->SendError(kInvalidState);\n-    }\n-    return;\n-  }\n   sf_->journal()->StartInThread();\n \n   rb->StartArray(2);\ndiff --git a/src/server/snapshot.cc b/src/server/snapshot.cc\nindex abfe5a9814ee..b809f8364ea7 100644\n--- a/src/server/snapshot.cc\n+++ b/src/server/snapshot.cc\n@@ -236,7 +236,6 @@ void SliceSnapshot::SwitchIncrementalFb(LSN lsn) {\n         std::make_error_code(errc::state_not_recoverable),\n         absl::StrCat(\"Partial sync was unsuccessful because entry #\", lsn,\n                      \" was dropped from the buffer. Current lsn=\", journal->GetLsn()));\n-    FinalizeJournalStream(true);\n   }\n }\n \n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex f5d35d172d3f..90951bf04007 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -3116,6 +3116,41 @@ async def test_partial_replication_on_same_source_master(df_factory, use_takeove\n         assert len(replica1.find_in_logs(\"No partial sync due to diff\")) > 0\n \n \n+async def test_partial_replication_on_same_source_master_with_replica_lsn_inc(df_factory):\n+    server1 = df_factory.create()\n+    server2 = df_factory.create()\n+    server3 = df_factory.create()\n+    server4 = df_factory.create()\n+\n+    df_factory.start_all([server1, server2, server3, server4])\n+    c_s2 = server2.client()\n+    c_s3 = server3.client()\n+    c_s4 = server4.client()\n+\n+    logging.debug(\"Start replication and wait for full sync\")\n+    await c_s2.execute_command(f\"REPLICAOF localhost {server1.port}\")\n+    await wait_for_replicas_state(c_s2)\n+    await c_s3.execute_command(f\"REPLICAOF localhost {server1.port}\")\n+    await wait_for_replicas_state(c_s3)\n+\n+    # Promote server 2 to master\n+    await c_s2.execute_command(f\"REPLICAOF NO ONE\")\n+    # Make server 4 replica of server 2\n+    await c_s4.execute_command(f\"REPLICAOF localhost {server2.port}\")\n+    # Send some write command for lsn inc\n+    for i in range(100):\n+        await c_s2.set(i, \"val\")\n+    # Make server 3 replica of server 2\n+    await c_s3.execute_command(f\"REPLICAOF localhost {server2.port}\")\n+    await check_all_replicas_finished([c_s3], c_s2)\n+    await check_all_replicas_finished([c_s4], c_s2)\n+\n+    server3.stop()\n+    # Check logs for partial replication\n+    lines = server3.find_in_logs(f\"Started partial sync with localhost:{server2.port}\")\n+    assert len(lines) == 1\n+\n+\n async def test_replicate_hset_with_expiry(df_factory: DflyInstanceFactory):\n     master = df_factory.create(proactor_threads=2)\n     replica = df_factory.create(proactor_threads=2)\n",
  "problem_statement": "Dragonfly crash during replication: v1.30.0\nDragonfly crashed during replication on v1.30.0:\n- Environment: staging\n- Datastore ID: `dst_u1xe53xd3`\n- Node ID: `node_plg7e8tdu`\n\n```\n1747226665146\t2025-05-14T12:44:25.146Z\tPC: @     0x7ffff7c9eb2c  (unknown)  pthread_kill\n1747226665146\t2025-05-14T12:44:25.146Z\t    @     0x555555748dd6  dfly::RdbSaver::Impl::~Impl()\n1747226665146\t2025-05-14T12:44:25.146Z\t    @     0x555555748766  dfly::RdbSaver::Impl::CleanShardSnapshots()\n1747226665146\t2025-05-14T12:44:25.146Z\t    @     0x555555748766  dfly::RdbSaver::Impl::CleanShardSnapshots()\n1747226665146\t2025-05-14T12:44:25.146Z\t    @     0x555555748c37  dfly::RdbSaver::Impl::~Impl()\n1747226665146\t2025-05-14T12:44:25.146Z\t*** SIGABRT received at time=1747226665 on cpu 6 ***\n1747226665146\t2025-05-14T12:44:25.146Z\t    @     0x555555ff5f3f  make_fcontext\n1747226665146\t2025-05-14T12:44:25.146Z\t    @     0x555555748c37  dfly::RdbSaver::Impl::~Impl()\n1747226665146\t2025-05-14T12:44:25.146Z\t    @     0x555555748766  dfly::RdbSaver::Impl::CleanShardSnapshots()\n1747226665146\t2025-05-14T12:44:25.146Z\t    @     0x555555748c37  dfly::RdbSaver::Impl::~Impl()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x555555979375  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly14EngineShardSet21RunBlockingInParallelIZNSA_7DflyCmd11ReplicaInfo6CancelEvEUlPNSA_11EngineShardEE_ZNSB_21RunBlockingInParallelISH_EEvOT_EUlSJ_E_EEvSK_OT0_EUlvE_JEEC4IS7_EESt17basic_string_viewIcSt11char_traitsIcEENS6_13FiberPriorityERKNS0_12preallocatedESK_OSO_EUlOS4_E_EEEEvNS1_10transfer_tE\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x555555748766  dfly::RdbSaver::Impl::CleanShardSnapshots()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x555555977242  _ZNSt17_Function_handlerIFvvEZN4dfly7DflyCmd21StartFullSyncInThreadEPNS1_8FlowInfoEPNS1_14ExecutionStateEPNS1_11EngineShardEEUlvE_E9_M_invokeERKSt9_Any_data\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x555555748dd6  dfly::RdbSaver::Impl::~Impl()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x555555748c37  dfly::RdbSaver::Impl::~Impl()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x555555748766  dfly::RdbSaver::Impl::CleanShardSnapshots()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x555555748766  dfly::RdbSaver::Impl::CleanShardSnapshots()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x555555748c37  dfly::RdbSaver::Impl::~Impl()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x555555748766  dfly::RdbSaver::Impl::CleanShardSnapshots()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665145\t2025-05-14T12:44:25.145Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x555555748766  dfly::RdbSaver::Impl::CleanShardSnapshots()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x555555748766  dfly::RdbSaver::Impl::CleanShardSnapshots()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x5555557595f6  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665144\t2025-05-14T12:44:25.144Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x55555575931b  dfly::SliceSnapshot::~SliceSnapshot()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561e9347  google::LogMessage::Flush()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561e9347  google::LogMessage::Flush()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561e9347  google::LogMessage::Flush()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561f0ea3  google::LogMessage::SendToLog()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561f0ea3  google::LogMessage::SendToLog()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n1747226665143\t2025-05-14T12:44:25.143Z\t    @     0x5555561f0ea3  google::LogMessage::SendToLog()\n1747226665142\t2025-05-14T12:44:25.142Z\t    @     0x5555561e9347  google::LogMessage::Flush()\n1747226665142\t2025-05-14T12:44:25.142Z\t    @     0x555555f8f46c  util::fb2::Fiber::~Fiber()\n1747226665142\t2025-05-14T12:44:25.142Z\t    @     0x5555561e9347  google::LogMessage::Flush()\n1747226665142\t2025-05-14T12:44:25.142Z\t    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n1747226665142\t2025-05-14T12:44:25.142Z\t*** Check failure stack trace: ***\n```\n\nI've attached the error logs for the node:\n[Logs-2025-05-15 07_37_16.txt](https://github.com/user-attachments/files/20224038/Logs-2025-05-15.07_37_16.txt)\n\nStill trying to understand details of when it crashed, will add to issue\n",
  "hints_text": "This has happened twice in the last 2 days\n\nEdit: Happens consistently\nTheres a core dump available too as normal\nSo it seems like it followed:\n1. Master promoted\n2. Replica added and syncs\n3. A second replica added, then the master crashed\nLooking at the core dump, the `snapshot_fb_` fiber is not joined when `SliceSnapshot` is destroyed, it has a valid `impl_` member, which causes a `CHECK(!IsJoinable())` failure. \n\nThe destructor of the snapshot object is reached via `flow->saver.reset();` \n\nRight before this reset we call `CancelInShard` on the same object:\n\n```\nflow->saver->CancelInShard(shard);\n```\n\nWhich should have joined the fiber so we seem to skip joining the fiber somehow.\n\nThere are two boolean conditions which can prevent the join:\n\n1. `journal_cb_id_` is 0\n2. `SliceSnapshot` for the shard is reset already - it doesn't look like this can happen as the fiber would be gone already if the `SliceSnapshot` was destroyed\nIn the attached error log there is an entry:\n\n```\nReportError: State not recoverable: Partial sync was unsuccessful because entry #3 was dropped from the buffer. Current lsn=8\n```\n\nThis code path is when the journal lsn does not match the expected value. In this code path we also call:\n\n```\nFinalizeJournalStream(true);\n```\n\nSo we end up calling `Finalize` twice, once from the fiber itself and once from `CancelInShard`. Since the `FinalizeJournalStream` has a check in the beginning to avoid multiple calls, this should be fine.\nIt looks like every case of the crash is preceded by the log\n\n```\nPartial sync was unsuccessful because entry #14 was dropped from the buffer. Current lsn=6\n```\n\nI suspect the duplicate call to `FinalizeJournalStream` is causing the fiber join to somehow be skipped, but I haven't been able to reproduce it so far locally.\n> It looks like every case of the crash is preceded by the log\n> \n> ```\n> Partial sync was unsuccessful because entry #14 was dropped from the buffer. Current lsn=6\n> ```\n> \n> I suspect the duplicate call to `FinalizeJournalStream` is causing the fiber join to somehow be skipped, but I haven't been able to reproduce it so far locally.\n\nThis call to `FinalizeJournalStream` does not seem related to this crash, adding a condition there in which the call to finalize is skipped if the journal id is 0 still results in the fiber ending up not joined. In the core dump the fiber interface trace is `TRACE_TERMINATED` so it seems that at least the function attached to fiber has finished.\nThis seems to be related to the error reporting done within `SliceSnapshot::SwitchIncrementalFb`. In the code path where the journal lsn does not match the one requested by the replica, the following error report is made:\n\n```\ncntx_->ReportError(std::make_error_code(errc::state_not_recoverable),\n                       absl::StrCat(\"Partial sync was unsuccessful because entry #\", lsn,\n                                    \" was dropped from the buffer. Current lsn=\", journal->GetLsn(),\n                                    \" current journal id=\", journal_cb_id_));\n```\n\nIn this call chain, in the context the `err_handler_fb_` field is assigned a new fiber which will run the error handler. \n\nRemoving the above call makes the test pass. This is not new or recently changed code, so perhaps condition not matching  in snapshot fiber: `journal->GetLsn() == lsn` is a result of recent changes and it has uncovered this unrelated issue with the fiber not joining.\nIt looks like the following chain of events is what happens here:\n\n1. `SliceSnapshot::SwitchIncrementalFb` goes into the error path, calls `ReportError` and then calls `SliceSnapshot::FinalizeJournalStream`. Within `SliceSnapshot::FinalizeJournalStream`  `journal_cb_id_` is set to 0, and then there is `snapshot_fb_.JoinIfNeeded()`.\n2. `ReportError` triggers the context's error handler in a separate fiber. This error handler calls `DflyCmd::StopReplication`, which ends up calling `DflyCmd::ReplicaInfo::Cancel`, which calls `flow->cleanup()`. Here we have:\n\n```\nflow->saver->CancelInShard(shard);\nflow->saver.reset();\n```\n\n`CancelInShard` will also call `SliceSnapshot::FinalizeJournalStream` (this is the second time we call this), but this time as journal cb id is 0 it does an early return. This clears the next statement above to run which resets the `flow->saver` calling the destructor for snapshot object while step 1 may still be blocked on the join. This should trigger the assertion.\n\nOn printing the stack trace just before the assertion is triggered, the `stop_replication` fiber from step 2 above is seen\n\n```\nI20250523 03:54:06.795753  1520 scheduler.cc:487] ------------ Fiber stop_replication (suspended:31ms) ------------\n0x555555fa53dc  util::fb2::detail::FiberInterface::SwitchTo()\n0x555555fa1a73  util::fb2::detail::Scheduler::Preempt()\n0x5555556a605d  util::fb2::EventCount::await<>()\n0x5555559809f7  dfly::DflyCmd::ReplicaInfo::Cancel()\n0x555555987050  dfly::DflyCmd::StopReplication()\nI20250523 03:54:06.796679  1520 scheduler.cc:487] ------------ Fiber Dispatched (active:33ms) ------------\n0x555555fa33a7  util::fb2::detail::Scheduler::ExecuteOnAllFiberStacks()\n0x555555fa371a  util::fb2::detail::Scheduler::PrintAllFiberStackTraces()\n0x555555759a1c  dfly::SliceSnapshot::~SliceSnapshot()\n0x555555759d66  dfly::SliceSnapshot::~SliceSnapshot()\n0x555555748e56  dfly::RdbSaver::Impl::CleanShardSnapshots()\n0x555555749327  dfly::RdbSaver::Impl::~Impl()\n0x5555557494c6  dfly::RdbSaver::Impl::~Impl()\n0x55555597a5c2  std::_Function_handler<>::_M_invoke()\nF20250523 03:54:06.799253  1520 fibers.cc:15] Check failed: !IsJoinable() \n```\n\nBut it is blocked on acquiring the lock. So it is not clear where the bottom fiber which runs through the destructors comes from. \nIt seems the problem has a simpler cause: if incremental snapshot is started and goes in the error path, `journal_cb_id_` is never set and remains 0, it is only set in the success path. \n\nIn this case any calls to `SliceSnapshot::FinalizeJournalStream` never join the snapshot fiber because of an early return. We simply need to wait for the snapshot fiber even if `journal_cb_id_` is 0.\nopened pr https://github.com/dragonflydb/dragonfly/pull/5171 to resolve the crash. \n\nNote that there is another underlying issue causing the incremental snapshot to stop early, which should be investigated separately, we have logs from this error like:\n\n```\nPartial sync was unsuccessful because entry #14 was dropped from the buffer. Current lsn=6\n```\n@abhijat we still a crash on v1.30.2 (`dst_rii67o3ak`), looks like it happens at the same time, but with a different stack trace:\n```\n\t\t\nPC: @     0x7ffff7c9eb2c  (unknown)  pthread_kill\n\t\n[failure_signal_handler.cc : 345] RAW: Signal 6 raised at PC=0x7ffff7c9eb2c while already in AbslFailureSignalHandler()\n\t\n    @     0x555555ff614f  make_fcontext\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n    @     0x55555575acf5  dfly::SliceSnapshot::FinalizeJournalStream()\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n*** SIGABRT received at time=1748262301 on cpu 12 ***\n\t\n    @     0x555555ff614f  make_fcontext\n\t\n    @     0x55555575acf5  dfly::SliceSnapshot::FinalizeJournalStream()\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n    @     0x55555575b267  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly13SliceSnapshot16StartIncrementalEmEUlvE_JEEC4IS7_EESt17basic_string_viewIcSt11char_traitsIcEENS6_13FiberPriorityERKNS0_12preallocatedEOT_OSC_EUlOS4_E_EEEEvNS1_10transfer_tE\n\t\n    @     0x55555575b095  dfly::SliceSnapshot::SwitchIncrementalFb()\n\t\n    @     0x55555575acf5  dfly::SliceSnapshot::FinalizeJournalStream()\n\t\n    @     0x555555f9b6ef  util::fb2::detail::FiberInterface::Join()\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n    @     0x55555575acf5  dfly::SliceSnapshot::FinalizeJournalStream()\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n    @     0x555555f9b6ef  util::fb2::detail::FiberInterface::Join()\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n    @     0x55555575b267  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly13SliceSnapshot16StartIncrementalEmEUlvE_JEEC4IS7_EESt17basic_string_viewIcSt11char_traitsIcEENS6_13FiberPriorityERKNS0_12preallocatedEOT_OSC_EUlOS4_E_EEEEvNS1_10transfer_tE\n\t\n    @     0x55555575b095  dfly::SliceSnapshot::SwitchIncrementalFb()\n\t\n    @     0x555555f9b6ef  util::fb2::detail::FiberInterface::Join()\n\t\n    @     0x55555575b267  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly13SliceSnapshot16StartIncrementalEmEUlvE_JEEC4IS7_EESt17basic_string_viewIcSt11char_traitsIcEENS6_13FiberPriorityERKNS0_12preallocatedEOT_OSC_EUlOS4_E_EEEEvNS1_10transfer_tE\n\t\n    @     0x555555f9b6ef  util::fb2::detail::FiberInterface::Join()\n\t\n    @     0x55555575b095  dfly::SliceSnapshot::SwitchIncrementalFb()\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n    @     0x55555575acf5  dfly::SliceSnapshot::FinalizeJournalStream()\n\t\n    @     0x555555f9b6ef  util::fb2::detail::FiberInterface::Join()\n\t\n    @     0x55555575acf5  dfly::SliceSnapshot::FinalizeJournalStream()\n\t\n    @     0x555555f9b6ef  util::fb2::detail::FiberInterface::Join()\n\t\n    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n\t\n    @     0x55555575acf5  dfly::SliceSnapshot::FinalizeJournalStream()\n\t\n    @     0x55555575acf5  dfly::SliceSnapshot::FinalizeJournalStream()\n\t\n    @     0x555555f9b6ef  util::fb2::detail::FiberInterface::Join()\n\t\n    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n    @     0x5555561e9347  google::LogMessage::Flush()\n\t\n    @     0x5555561e9347  google::LogMessage::Flush()\n\t\n    @     0x555555f8f7ce  util::fb2::Fiber::Join()\n\t\n    @     0x5555561e9347  google::LogMessage::Flush()\n\t\n    @     0x5555561f0ea3  google::LogMessage::SendToLog()\n\t\n    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n\t\n    @     0x555555f9b6ef  util::fb2::detail::FiberInterface::Join()\n\t\n    @     0x5555561f0ea3  google::LogMessage::SendToLog()\n\t\n    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n\t\n    @     0x5555561f0ea3  google::LogMessage::SendToLog()\n\t\n    @     0x555555f9b6ef  util::fb2::detail::FiberInterface::Join()\n\t\n    @     0x5555561f0ea3  google::LogMessage::SendToLog()\n\t\n    @     0x5555561eaccf  google::LogMessageFatal::~LogMessageFatal()\n\n\t\t\n*** Check failure stack trace: ***\n\t\n20250526 12:25:01.705154  1826 fiber_interface.cc:264] Check failed: active != this F20250526 12:25:01.705372  1835 fiber_interface.cc:264] Check failed: active != this F20250526 12:25:01.705457  1832 fiber_interface.cc:264] Check failed: active != this F20250526 12:25:01.705509  1841 fiber_interface.cc:264] Check failed: active != this F20250526 12:25:01.706125  1836 fiber_interface.cc:264] Check failed: active != this F20250526 12:25:01.706192  1827 fiber_interface.cc:264] Check failed: active != this F20250526 12:25:01.706255  1839 fiber_interface.cc:264] Check failed: active != this\n```\nIt looks like calling\n\n```\n    FinalizeJournalStream(true);\n```\n\nduring `SliceSnapshot::SwitchIncrementalFb` ends up with the snapshot fiber joining on itself which triggers the check. I don't see it on a manually run dftest though. We don't need to make the call to `FinalizeJournalStream(true)` at that point, as the context `ReportError` ends up calling the same method from another fiber. \n\nI would need to see the error in a manual run though so the potential fix can be tried out.\nYep, the clean up is done by cntx error handler. We can just remove `FinalizeJournalStream(true)` :) See https://github.com/dragonflydb/dragonfly/tree/kpr3\nIt crashes on 1.30.2 build but not on the one used to build the previous fix. This is probably because in that branch I removed the call: https://github.com/dragonflydb/dragonfly/commit/facd4b8680c8ae0a57c64611e3a3cb8556069815#diff-f67586dd571688e986aba8f508c6ee198e0adbfa01ebd362b55403bac5f22b84R248-R252 \nwith changes in https://github.com/dragonflydb/dragonfly/pull/5183 the crash does not occur, will do a few more runs to be sure",
  "created_at": "2025-05-27T18:49:47Z",
  "modified_files": [
    "src/server/dflycmd.cc",
    "src/server/snapshot.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}