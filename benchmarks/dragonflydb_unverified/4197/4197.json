{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4197,
  "instance_id": "dragonflydb__dragonfly-4197",
  "issue_numbers": [
    "4100"
  ],
  "base_commit": "a3ef239ac7d94e908fae467816c673ae71c07d83",
  "patch": "diff --git a/src/server/journal/streamer.cc b/src/server/journal/streamer.cc\nindex 91480a181341..c2df86b39464 100644\n--- a/src/server/journal/streamer.cc\n+++ b/src/server/journal/streamer.cc\n@@ -218,6 +218,12 @@ void RestoreStreamer::Run() {\n       if (fiber_cancelled_)  // Could have been cancelled in above call too\n         return;\n \n+      std::lock_guard guard(big_value_mu_);\n+\n+      // Locking this never preempts. See snapshot.cc for why we need it.\n+      auto* blocking_counter = db_slice_->BlockingCounter();\n+      std::lock_guard blocking_counter_guard(*blocking_counter);\n+\n       WriteBucket(it);\n     });\n \n@@ -281,7 +287,6 @@ bool RestoreStreamer::ShouldWrite(cluster::SlotId slot_id) const {\n \n void RestoreStreamer::WriteBucket(PrimeTable::bucket_iterator it) {\n   if (it.GetVersion() < snapshot_version_) {\n-    FiberAtomicGuard fg;\n     it.SetVersion(snapshot_version_);\n     string key_buffer;  // we can reuse it\n     for (; !it.is_done(); ++it) {\n@@ -302,6 +307,7 @@ void RestoreStreamer::WriteBucket(PrimeTable::bucket_iterator it) {\n }\n \n void RestoreStreamer::OnDbChange(DbIndex db_index, const DbSlice::ChangeReq& req) {\n+  std::lock_guard guard(big_value_mu_);\n   DCHECK_EQ(db_index, 0) << \"Restore migration only allowed in cluster mode in db0\";\n \n   PrimeTable* table = db_slice_->GetTables(0).first;\n@@ -319,8 +325,12 @@ void RestoreStreamer::OnDbChange(DbIndex db_index, const DbSlice::ChangeReq& req\n \n void RestoreStreamer::WriteEntry(string_view key, const PrimeValue& pk, const PrimeValue& pv,\n                                  uint64_t expire_ms) {\n-  CmdSerializer serializer([&](std::string s) { Write(std::move(s)); },\n-                           ServerState::tlocal()->serialization_max_chunk_size);\n+  CmdSerializer serializer(\n+      [&](std::string s) {\n+        Write(std::move(s));\n+        ThrottleIfNeeded();\n+      },\n+      ServerState::tlocal()->serialization_max_chunk_size);\n   serializer.SerializeEntry(key, pk, pv, expire_ms);\n }\n \ndiff --git a/src/server/journal/streamer.h b/src/server/journal/streamer.h\nindex a18615a053f1..907b6e65eee2 100644\n--- a/src/server/journal/streamer.h\n+++ b/src/server/journal/streamer.h\n@@ -112,6 +112,7 @@ class RestoreStreamer : public JournalStreamer {\n   cluster::SlotSet my_slots_;\n   bool fiber_cancelled_ = false;\n   bool snapshot_finished_ = false;\n+  ThreadLocalMutex big_value_mu_;\n };\n \n }  // namespace dfly\n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex b90d486650a2..a7262aa5dfe6 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -14,8 +14,7 @@\n from redis.cluster import RedisCluster\n from redis.cluster import ClusterNode\n from .proxy import Proxy\n-from .seeder import SeederBase\n-from .seeder import StaticSeeder\n+from .seeder import Seeder, SeederBase, StaticSeeder\n \n from . import dfly_args\n \n@@ -33,6 +32,11 @@ def monotonically_increasing_port_number():\n next_port = monotonically_increasing_port_number()\n \n \n+async def get_memory(client, field):\n+    info = await client.info(\"memory\")\n+    return info[field]\n+\n+\n class RedisClusterNode:\n     def __init__(self, port):\n         self.port = port\n@@ -1981,6 +1985,7 @@ async def node1size0():\n \n @dfly_args({\"proactor_threads\": 2, \"cluster_mode\": \"yes\"})\n @pytest.mark.asyncio\n+@pytest.mark.opt_only\n async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory):\n     instances = [\n         df_factory.create(port=next(next_port), admin_port=next(next_port)) for i in range(2)\n@@ -1995,7 +2000,7 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)\n \n     logging.debug(\"Generating huge containers\")\n     seeder = StaticSeeder(\n-        key_target=10,\n+        key_target=100,\n         data_size=10_000_000,\n         collection_size=10_000,\n         variance=1,\n@@ -2005,6 +2010,8 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)\n     await seeder.run(nodes[0].client)\n     source_data = await StaticSeeder.capture(nodes[0].client)\n \n+    mem_before = await get_memory(nodes[0].client, \"used_memory_rss\")\n+\n     nodes[0].migrations = [\n         MigrationInfo(\"127.0.0.1\", instances[1].admin_port, [(0, 16383)], nodes[1].id)\n     ]\n@@ -2017,6 +2024,74 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)\n     target_data = await StaticSeeder.capture(nodes[1].client)\n     assert source_data == target_data\n \n+    # Get peak memory, because migration removes the data\n+    mem_after = await get_memory(nodes[0].client, \"used_memory_peak_rss\")\n+    logging.debug(f\"Memory before {mem_before} after {mem_after}\")\n+    assert mem_after < mem_before * 1.1\n+\n+\n+@dfly_args({\"proactor_threads\": 2, \"cluster_mode\": \"yes\"})\n+@pytest.mark.parametrize(\"chunk_size\", [1_000_000, 30])\n+@pytest.mark.asyncio\n+async def test_cluster_migration_while_seeding(\n+    df_factory: DflyInstanceFactory, df_seeder_factory: DflySeederFactory, chunk_size\n+):\n+    instances = [\n+        df_factory.create(\n+            port=next(next_port),\n+            admin_port=next(next_port),\n+            serialization_max_chunk_size=chunk_size,\n+        )\n+        for _ in range(2)\n+    ]\n+    df_factory.start_all(instances)\n+\n+    nodes = [await create_node_info(instance) for instance in instances]\n+    nodes[0].slots = [(0, 16383)]\n+    nodes[1].slots = []\n+    client0 = nodes[0].client\n+    client1 = nodes[1].client\n+\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    logging.debug(\"Seeding cluster\")\n+    seeder = df_seeder_factory.create(\n+        keys=10_000, port=instances[0].port, cluster_mode=True, mirror_to_fake_redis=True\n+    )\n+    await seeder.run(target_deviation=0.1)\n+\n+    seed = asyncio.create_task(seeder.run())\n+    await asyncio.sleep(1)\n+\n+    nodes[0].migrations = [\n+        MigrationInfo(\"127.0.0.1\", instances[1].admin_port, [(0, 16383)], nodes[1].id)\n+    ]\n+    logging.debug(\"Migrating slots\")\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    logging.debug(\"Waiting for migration to finish\")\n+    await wait_for_status(nodes[0].admin_client, nodes[1].id, \"FINISHED\", timeout=300)\n+    logging.debug(\"Migration finished\")\n+\n+    logging.debug(\"Finalizing migration\")\n+    nodes[0].slots = []\n+    nodes[1].slots = [(0, 16383)]\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    await asyncio.sleep(1)  # Let seeder feed dest before migration finishes\n+\n+    seeder.stop()\n+    await seed\n+    logging.debug(\"Seeding finished\")\n+\n+    assert (\n+        await get_memory(client0, \"used_memory_peak_rss\")\n+        < await get_memory(client0, \"used_memory_rss\") * 1.1\n+    )\n+\n+    capture = await seeder.capture_fake_redis()\n+    assert await seeder.compare(capture, instances[1].port)\n+\n \n def parse_lag(replication_info: str):\n     lags = re.findall(\"lag=([0-9]+)\\r\\n\", replication_info)\ndiff --git a/tests/dragonfly/requirements.txt b/tests/dragonfly/requirements.txt\nindex cfbbd8262986..25fb8f69e251 100644\n--- a/tests/dragonfly/requirements.txt\n+++ b/tests/dragonfly/requirements.txt\n@@ -25,3 +25,4 @@ pytest-emoji==0.2.0\n pytest-icdiff==0.8\n pytest-timeout==2.2.0\n asyncio==3.4.3\n+fakeredis[json]==2.26.2\ndiff --git a/tests/dragonfly/seeder/__init__.py b/tests/dragonfly/seeder/__init__.py\nindex 4154e4d1cf93..351fc38a8458 100644\n--- a/tests/dragonfly/seeder/__init__.py\n+++ b/tests/dragonfly/seeder/__init__.py\n@@ -177,14 +177,16 @@ async def run(self, client: aioredis.Redis, target_ops=None, target_deviation=No\n         ]\n \n         sha = await client.script_load(Seeder._load_script(\"generate\"))\n-        await asyncio.gather(\n-            *(self._run_unit(client, sha, unit, using_stopkey, args) for unit in self.units)\n-        )\n+        for unit in self.units:\n+            # Must be serial, otherwise cluster clients throws an exception\n+            await self._run_unit(client, sha, unit, using_stopkey, args)\n \n     async def stop(self, client: aioredis.Redis):\n         \"\"\"Request seeder seeder if it's running without a target, future returned from start() must still be awaited\"\"\"\n \n-        await asyncio.gather(*(client.set(unit.stop_key, \"X\") for unit in self.units))\n+        for unit in self.units:\n+            # Must be serial, otherwise cluster clients throws an exception\n+            await client.set(unit.stop_key, \"X\")\n \n     def change_key_target(self, target: int):\n         \"\"\"Change key target, applied only on succeeding runs\"\"\"\ndiff --git a/tests/dragonfly/seeder_test.py b/tests/dragonfly/seeder_test.py\nindex 61557270f747..3d35242da54e 100644\n--- a/tests/dragonfly/seeder_test.py\n+++ b/tests/dragonfly/seeder_test.py\n@@ -4,6 +4,8 @@\n from redis import asyncio as aioredis\n from . import dfly_args\n from .seeder import Seeder, StaticSeeder\n+from .instance import DflyInstanceFactory, DflyInstance\n+from .utility import *\n \n \n @dfly_args({\"proactor_threads\": 4})\n@@ -114,3 +116,22 @@ async def set_data():\n     # Do another change\n     await async_client.spop(\"set1\")\n     assert capture != await Seeder.capture(async_client)\n+\n+\n+@pytest.mark.asyncio\n+@dfly_args({\"proactor_threads\": 2})\n+async def test_seeder_fake_redis(\n+    df_factory: DflyInstanceFactory, df_seeder_factory: DflySeederFactory\n+):\n+    instance = df_factory.create()\n+    df_factory.start_all([instance])\n+\n+    seeder = df_seeder_factory.create(\n+        keys=100, port=instance.port, unsupported_types=[ValueType.JSON], mirror_to_fake_redis=True\n+    )\n+\n+    await seeder.run(target_ops=5_000)\n+\n+    capture = await seeder.capture_fake_redis()\n+\n+    assert await seeder.compare(capture, instance.port)\ndiff --git a/tests/dragonfly/utility.py b/tests/dragonfly/utility.py\nindex 197d2a3d02c1..40f8ce1dd97e 100644\n--- a/tests/dragonfly/utility.py\n+++ b/tests/dragonfly/utility.py\n@@ -14,6 +14,7 @@\n import subprocess\n import pytest\n import os\n+import fakeredis\n from typing import Iterable, Union\n from enum import Enum\n \n@@ -271,7 +272,7 @@ def gen_shrink_cmd(self):\n         (\"LPUSH {k} {val}\", ValueType.LIST),\n         (\"LPOP {k}\", ValueType.LIST),\n         (\"SADD {k} {val}\", ValueType.SET),\n-        (\"SPOP {k}\", ValueType.SET),\n+        # (\"SPOP {k}\", ValueType.SET),  # Disabled because it is inconsistent\n         (\"HSETNX {k} v0 {val}\", ValueType.HSET),\n         (\"HINCRBY {k} v1 1\", ValueType.HSET),\n         (\"ZPOPMIN {k} 1\", ValueType.ZSET),\n@@ -423,6 +424,7 @@ def __init__(\n         unsupported_types=[],\n         stop_on_failure=True,\n         cluster_mode=False,\n+        mirror_to_fake_redis=False,\n     ):\n         if cluster_mode:\n             max_multikey = 1\n@@ -436,11 +438,16 @@ def __init__(\n         self.multi_transaction_probability = multi_transaction_probability\n         self.stop_flag = False\n         self.stop_on_failure = stop_on_failure\n+        self.fake_redis = None\n \n         self.log_file = log_file\n         if self.log_file is not None:\n             open(self.log_file, \"w\").close()\n \n+        if mirror_to_fake_redis:\n+            logging.debug(\"Creating FakeRedis instance\")\n+            self.fake_redis = fakeredis.FakeAsyncRedis()\n+\n     async def run(self, target_ops=None, target_deviation=None):\n         \"\"\"\n         Run a seeding cycle on all dbs either until stop(), a fixed number of commands (target_ops)\n@@ -474,6 +481,14 @@ def reset(self):\n         \"\"\"Reset internal state. Needs to be called after flush or restart\"\"\"\n         self.gen.reset()\n \n+    async def capture_fake_redis(self):\n+        keys = sorted(list(self.gen.keys_and_types()))\n+        # TODO: support multiple databases\n+        assert self.dbcount == 1\n+        assert self.fake_redis != None\n+        capture = DataCapture(await self._capture_entries(self.fake_redis, keys))\n+        return [capture]\n+\n     async def capture(self, port=None):\n         \"\"\"Create DataCapture for all dbs\"\"\"\n \n@@ -588,12 +603,19 @@ async def _executor_task(self, db, queue):\n                 queue.task_done()\n                 break\n \n-            pipe = client.pipeline(transaction=tx_data[1])\n-            for cmd in tx_data[0]:\n-                pipe.execute_command(*cmd)\n-\n             try:\n-                await pipe.execute()\n+                if self.fake_redis is None:\n+                    pipe = client.pipeline(transaction=tx_data[1])\n+                    for cmd in tx_data[0]:\n+                        pipe.execute_command(*cmd)\n+                    await pipe.execute()\n+                else:\n+                    # To mirror consistently to Fake Redis we must only send to it successful\n+                    # commands. We can't use pipes because they might succeed partially.\n+                    for cmd in tx_data[0]:\n+                        dfly_resp = await client.execute_command(*cmd)\n+                        fake_resp = await self.fake_redis.execute_command(*cmd)\n+                        assert dfly_resp == fake_resp\n             except (redis.exceptions.ConnectionError, redis.exceptions.ResponseError) as e:\n                 if self.stop_on_failure:\n                     await self._close_client(client)\n",
  "problem_statement": "Big value serialization in migration\n\n",
  "hints_text": "",
  "created_at": "2024-11-26T14:27:47Z",
  "modified_files": [
    "src/server/journal/streamer.cc",
    "src/server/journal/streamer.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py",
    "tests/dragonfly/requirements.txt",
    "tests/dragonfly/seeder/__init__.py",
    "tests/dragonfly/seeder_test.py",
    "tests/dragonfly/utility.py"
  ]
}