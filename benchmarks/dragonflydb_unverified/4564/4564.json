{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4564,
  "instance_id": "dragonflydb__dragonfly-4564",
  "issue_numbers": [
    "4554"
  ],
  "base_commit": "6d1c22b64cbcc33daa4f89210cd86200616ff410",
  "patch": "diff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex 4e2c57577819..7266a6453a7a 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -283,7 +283,6 @@ DbSlice::DbSlice(uint32_t index, bool cache_mode, EngineShard* owner)\n       cache_mode_(cache_mode),\n       owner_(owner),\n       client_tracking_map_(owner->memory_resource()) {\n-  load_in_progress_ = false;\n   db_arr_.emplace_back();\n   CreateDb(0);\n   expire_base_[0] = expire_base_[1] = 0;\n@@ -795,7 +794,8 @@ void DbSlice::FlushDbIndexes(const std::vector<DbIndex>& indexes) {\n     std::swap(db_arr_[index]->trans_locks, flush_db_arr[index]->trans_locks);\n   }\n \n-  CHECK(fetched_items_.empty());\n+  LOG_IF(DFATAL, !fetched_items_.empty())\n+      << \"Some operation might bumped up items outside of a transaction\";\n \n   auto cb = [indexes, flush_db_arr = std::move(flush_db_arr)]() mutable {\n     flush_db_arr.clear();\ndiff --git a/src/server/db_slice.h b/src/server/db_slice.h\nindex 0cae9d6f0f38..94ed5ed1d231 100644\n--- a/src/server/db_slice.h\n+++ b/src/server/db_slice.h\n@@ -473,11 +473,19 @@ class DbSlice {\n \n   bool IsCacheMode() const {\n     // During loading time we never bump elements.\n-    return cache_mode_ && !load_in_progress_;\n+    return cache_mode_ && (load_ref_count_ == 0);\n   }\n \n-  void SetLoadInProgress(bool in_progress) {\n-    load_in_progress_ = in_progress;\n+  void IncrLoadInProgress() {\n+    ++load_ref_count_;\n+  }\n+\n+  void DecrLoadInProgress() {\n+    --load_ref_count_;\n+  }\n+\n+  bool IsLoadRefCountZero() const {\n+    return load_ref_count_ == 0;\n   }\n \n   // Test hook to inspect last locked keys.\n@@ -585,7 +593,6 @@ class DbSlice {\n \n   ShardId shard_id_;\n   uint8_t cache_mode_ : 1;\n-  uint8_t load_in_progress_ : 1;\n \n   EngineShard* owner_;\n \n@@ -598,6 +605,7 @@ class DbSlice {\n   size_t soft_budget_limit_ = 0;\n   size_t table_memory_ = 0;\n   uint64_t entries_count_ = 0;\n+  unsigned load_ref_count_ = 0;\n \n   mutable SliceEvents events_;  // we may change this even for const operations.\n \ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex cc705a38c9d6..b3851b4b94fa 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -2508,7 +2508,14 @@ GlobalState Service::SwitchState(GlobalState from, GlobalState to) {\n   VLOG(1) << \"Switching state from \" << from << \" to \" << to;\n   global_state_ = to;\n \n-  pp_.Await([&](ProactorBase*) { ServerState::tlocal()->set_gstate(to); });\n+  pp_.Await([&](ProactorBase*) {\n+    ServerState::tlocal()->set_gstate(to);\n+    auto* es = EngineShard::tlocal();\n+    if (es && to == GlobalState::ACTIVE) {\n+      DbSlice& db = namespaces->GetDefaultNamespace().GetDbSlice(es->shard_id());\n+      DCHECK(db.IsLoadRefCountZero());\n+    }\n+  });\n   return to;\n }\n \ndiff --git a/src/server/rdb_load.cc b/src/server/rdb_load.cc\nindex 192df90b041f..4189332e747f 100644\n--- a/src/server/rdb_load.cc\n+++ b/src/server/rdb_load.cc\n@@ -2034,9 +2034,11 @@ error_code RdbLoader::Load(io::Source* src) {\n \n   auto cleanup = absl::Cleanup([&] { FinishLoad(start, &keys_loaded); });\n \n-  shard_set->AwaitRunningOnShardQueue([](EngineShard* es) {\n-    namespaces->GetDefaultNamespace().GetCurrentDbSlice().SetLoadInProgress(true);\n-  });\n+  // Increment local one if it exists\n+  if (EngineShard* es = EngineShard::tlocal(); es) {\n+    namespaces->GetDefaultNamespace().GetCurrentDbSlice().IncrLoadInProgress();\n+  }\n+\n   while (!stop_early_.load(memory_order_relaxed)) {\n     if (pause_) {\n       ThisFiber::SleepFor(100ms);\n@@ -2226,12 +2228,13 @@ void RdbLoader::FinishLoad(absl::Time start_time, size_t* keys_loaded) {\n     FlushShardAsync(i);\n \n     // Send sentinel callbacks to ensure that all previous messages have been processed.\n-    shard_set->Add(i, [bc]() mutable {\n-      namespaces->GetDefaultNamespace().GetCurrentDbSlice().SetLoadInProgress(false);\n-      bc->Dec();\n-    });\n+    shard_set->Add(i, [bc]() mutable { bc->Dec(); });\n   }\n   bc->Wait();  // wait for sentinels to report.\n+  // Decrement local one if it exists\n+  if (EngineShard* es = EngineShard::tlocal(); es) {\n+    namespaces->GetDefaultNamespace().GetCurrentDbSlice().DecrLoadInProgress();\n+  }\n \n   absl::Duration dur = absl::Now() - start_time;\n   load_time_ = double(absl::ToInt64Milliseconds(dur)) / 1000;\n@@ -2515,7 +2518,12 @@ void RdbLoader::FlushShardAsync(ShardId sid) {\n     return;\n \n   auto cb = [indx = this->cur_db_index_, this, ib = std::move(out_buf)] {\n+    // Before we start loading, increment LoadInProgress.\n+    // This is required because FlushShardAsync dispatches to multiple shards, and those shards\n+    // might have not yet have their state (load in progress) incremented.\n+    namespaces->GetDefaultNamespace().GetCurrentDbSlice().IncrLoadInProgress();\n     this->LoadItemsBuffer(indx, ib);\n+    namespaces->GetDefaultNamespace().GetCurrentDbSlice().DecrLoadInProgress();\n \n     // Block, if tiered storage is active, but can't keep up\n     while (EngineShard::tlocal()->ShouldThrottleForTiering()) {\n@@ -2554,6 +2562,8 @@ void RdbLoader::LoadItemsBuffer(DbIndex db_ind, const ItemsBuf& ib) {\n   DbContext db_cntx{&namespaces->GetDefaultNamespace(), db_ind, GetCurrentTimeMs()};\n   DbSlice& db_slice = db_cntx.GetDbSlice(es->shard_id());\n \n+  DCHECK(!db_slice.IsCacheMode());\n+\n   auto error_msg = [](const auto* item, auto db_ind) {\n     return absl::StrCat(\"Found empty key: \", item->key, \" in DB \", db_ind, \" rdb_type \",\n                         item->val.rdb_type);\ndiff --git a/src/server/replica.cc b/src/server/replica.cc\nindex 5a627777d596..5f8438a5df91 100644\n--- a/src/server/replica.cc\n+++ b/src/server/replica.cc\n@@ -542,6 +542,7 @@ error_code Replica::InitiateDflySync() {\n     // Lock to prevent the error handler from running instantly\n     // while the flows are in a mixed state.\n     lock_guard lk{flows_op_mu_};\n+\n     shard_set->pool()->AwaitFiberOnAll(std::move(shard_cb));\n \n     size_t num_full_flows =\n",
  "test_patch": "diff --git a/src/server/rdb_test.cc b/src/server/rdb_test.cc\nindex f2a23ce62126..c4feca4b9de4 100644\n--- a/src/server/rdb_test.cc\n+++ b/src/server/rdb_test.cc\n@@ -735,4 +735,19 @@ TEST_F(RdbTest, HugeKeyIssue4497) {\n   EXPECT_EQ(Run({\"flushall\"}), \"OK\");\n }\n \n+TEST_F(RdbTest, HugeKeyIssue4554) {\n+  SetTestFlag(\"cache_mode\", \"true\");\n+  // We need to stress one flow/shard such that the others finish early. Lock on hashtags allows\n+  // that.\n+  SetTestFlag(\"lock_on_hashtags\", \"true\");\n+  ResetService();\n+\n+  EXPECT_EQ(\n+      Run({\"debug\", \"populate\", \"20\", \"{tmp}\", \"20\", \"rand\", \"type\", \"set\", \"elements\", \"10000\"}),\n+      \"OK\");\n+  EXPECT_EQ(Run({\"save\", \"df\", \"hugekey\"}), \"OK\");\n+  EXPECT_EQ(Run({\"dfly\", \"load\", \"hugekey-summary.dfs\"}), \"OK\");\n+  EXPECT_EQ(Run({\"flushall\"}), \"OK\");\n+}\n+\n }  // namespace dfly\n",
  "problem_statement": "SIGSEGV with 1.26.2 and cache_mode=true\nOpening a new issue because #4497 is closed.\n\nThis issue seems not totally fixed.\n\nhere is output with Dragonfly 1.26.2 launched with : \n`--dbfilename=dump    - --cache_mode=true    - --vmodule=streamer=1,dflycmd=1,replica=1`\n\nIt affects replica instance.\n```\nI20250204 08:06:42.981666     1 dfly_main.cc:691] Starting dragonfly df-v1.26.2-096fde172300de91850c42dab24aa09ffee254d0\nI20250204 08:06:42.981844     1 dfly_main.cc:735] maxmemory has not been specified. Deciding myself....\nI20250204 08:06:42.981854     1 dfly_main.cc:744] Found 2.00GiB available memory. Setting maxmemory to 1.60GiB\nW20250204 08:06:42.981900     1 dfly_main.cc:368] Weird error 1 switching to epoll\nI20250204 08:06:43.059904     1 proactor_pool.cc:147] Running 2 io threads\nI20250204 08:06:43.062503     1 dfly_main.cc:272] Listening on admin socket any:9999\nI20250204 08:06:43.063760     1 server_family.cc:835] Host OS: Linux 5.14.0-503.22.1.el9_5.x86_64 x86_64 with 2 threads\nI20250204 08:06:43.068403     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\nI20250204 08:06:43.073527     1 server_family.cc:1114] Loading /dragonfly/snapshots/dump-summary.dfs\nI20250204 08:06:43.087914     6 listener_interface.cc:101] sock[8] AcceptServer - listening on port 6379\nI20250204 08:06:43.089705     7 listener_interface.cc:101] sock[7] AcceptServer - listening on port 9999\nI20250204 08:06:43.089743     7 listener_interface.cc:101] sock[9] AcceptServer - listening on port 11211\nI20250204 08:06:44.393589     6 server_family.cc:1154] Load finished, num keys read: 572201\nI20250204 08:06:56.683905     7 server_family.cc:2769] Replicating 10.233.137.19:9999\nF20250204 08:06:56.685020     7 db_slice.cc:784] Check failed: fetched_items_.empty() \n*** Check failure stack trace: ***\n    @     0x558c1fd18923  google::LogMessage::SendToLog()\n    @     0x558c1fd110e7  google::LogMessage::Flush()\n    @     0x558c1fd12a6f  google::LogMessageFatal::~LogMessageFatal()\n    @     0x558c1f550aef  dfly::DbSlice::FlushDbIndexes()\n    @     0x558c1f550c82  dfly::DbSlice::FlushDb()\n    @     0x558c1f2d02ba  _ZN4absl12lts_2024011619functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\n    @     0x558c1f590bf9  dfly::Transaction::RunCallback()\n    @     0x558c1f5939db  dfly::Transaction::RunInShard()\n    @     0x558c1f4d1b60  dfly::EngineShard::PollExecution()\n    @     0x558c1f58c7e1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\n    @     0x558c1fafe395  util::fb2::FiberQueue::Run()\n    @     0x558c1f5deb70  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\n    @     0x558c1fb1d69f  make_fcontext\n*** SIGABRT received at time=1738656416 on cpu 1 ***\nPC: @     0x7ff0adabe9fc  (unknown)  pthread_kill\n[failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7ff0ada50898 while already in AbslFailureSignalHandler()\n*** SIGSEGV received at time=1738656416 on cpu 1 ***\nPC: @     0x7ff0ada50898  (unknown)  abort\n```\n\nThe pod crash \"sometimes\". Here, it crashed once and is now running correctly : \n```\nI20250204 08:07:18.717423     1 server_family.cc:835] Host OS: Linux 5.14.0-503.22.1.el9_5.x86_64 x86_64 with 2 threads\nI20250204 08:07:18.721555     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\nI20250204 08:07:18.724478     1 server_family.cc:1114] Loading /dragonfly/snapshots/dump-summary.dfs\nI20250204 08:07:18.733318     6 listener_interface.cc:101] sock[8] AcceptServer - listening on port 6379\nI20250204 08:07:18.733692     7 listener_interface.cc:101] sock[7] AcceptServer - listening on port 9999\nI20250204 08:07:18.733723     7 listener_interface.cc:101] sock[9] AcceptServer - listening on port 11211\nI20250204 08:07:20.000798     6 server_family.cc:1154] Load finished, num keys read: 572201\nI20250204 08:07:36.684935     7 server_family.cc:2769] Replicating 10.233.137.19:9999\nI20250204 08:07:36.743476     7 replica.cc:94] Starting replication\nI20250204 08:07:36.743537     7 replica.cc:116] Resolving master DNS\nI20250204 08:07:36.743656     7 replica.cc:121] Connecting to master\nI20250204 08:07:36.744189     7 replica.cc:126] Greeting\nI20250204 08:07:36.744204     7 replica.cc:281] greeting message handling\nI20250204 08:07:36.745363     7 replica.cc:360] Master id: 9d33ab3e04153c86a6b2f8f4825a4a7e03b92786, sync id: SYNC1, num journals: 2, version: 3\nI20250204 08:07:36.745939     7 replica.cc:193] Main replication fiber started\nI20250204 08:07:36.746312     7 replica.cc:741] Sending on flow 9d33ab3e04153c86a6b2f8f4825a4a7e03b92786 SYNC1 1\nI20250204 08:07:36.746456     6 replica.cc:741] Sending on flow 9d33ab3e04153c86a6b2f8f4825a4a7e03b92786 SYNC1 0\nI20250204 08:07:36.747501     7 replica.cc:724] Sending: DFLY SYNC SYNC1\nI20250204 08:07:36.750665     7 replica.cc:569] Started full sync with 10.233.137.19:9999\nI20250204 08:07:36.750690     7 replica.cc:573] Waiting for all full sync cut confirmations\nI20250204 08:07:36.803468     7 server_family.cc:2769] Replicating 10.233.137.19:9999\nI20250204 08:07:36.803522     7 replica.cc:148] Stopping replication\nI20250204 08:07:36.809708     6 replica.cc:843] FullSyncDflyFb finished after reading 3067928 bytes\nI20250204 08:07:36.815779     7 replica.cc:843] FullSyncDflyFb finished after reading 3485050 bytes\nW20250204 08:07:36.815847     7 replica.cc:246] Error syncing with 10.233.137.19:9999 generic:125 Operation canceled\nI20250204 08:07:36.815882     7 replica.cc:276] Main replication fiber finished\nI20250204 08:07:36.825311     7 replica.cc:94] Starting replication\nI20250204 08:07:36.825345     7 replica.cc:116] Resolving master DNS\nI20250204 08:07:36.825459     7 replica.cc:121] Connecting to master\nI20250204 08:07:36.825807     7 replica.cc:126] Greeting\nI20250204 08:07:36.825820     7 replica.cc:281] greeting message handling\nI20250204 08:07:36.826691     7 replica.cc:360] Master id: 9d33ab3e04153c86a6b2f8f4825a4a7e03b92786, sync id: SYNC2, num journals: 2, version: 3\nI20250204 08:07:36.827033     7 replica.cc:193] Main replication fiber started\nI20250204 08:07:36.827302     7 replica.cc:741] Sending on flow 9d33ab3e04153c86a6b2f8f4825a4a7e03b92786 SYNC2 1\nI20250204 08:07:36.827404     6 replica.cc:741] Sending on flow 9d33ab3e04153c86a6b2f8f4825a4a7e03b92786 SYNC2 0\nI20250204 08:07:36.828095     7 replica.cc:724] Sending: DFLY SYNC SYNC2\nI20250204 08:07:36.830502     7 replica.cc:569] Started full sync with 10.233.137.19:9999\nI20250204 08:07:36.830538     7 replica.cc:573] Waiting for all full sync cut confirmations\nI20250204 08:07:37.742475     7 replica.cc:724] Sending: DFLY STARTSTABLE SYNC2\nI20250204 08:07:37.743093     7 replica.cc:843] FullSyncDflyFb finished after reading 75630512 bytes\nI20250204 08:07:37.743252     6 replica.cc:843] FullSyncDflyFb finished after reading 56034881 bytes\nI20250204 08:07:37.744261     7 replica.cc:589] full sync finished in 917 ms\nI20250204 08:07:37.744330     7 replica.cc:679] Transitioned into stable sync\nI20250204 08:07:37.745183     7 replica.cc:921] Sending an ACK with offset=17 forced=0\nI20250204 08:07:37.745688     6 replica.cc:921] Sending an ACK with offset=18 forced=0\nI20250204 08:07:38.745687     7 replica.cc:921] Sending an ACK with offset=64 forced=0\nI20250204 08:07:38.745963     6 replica.cc:921] Sending an ACK with offset=36 forced=0\nI20250204 08:07:39.745767     7 replica.cc:921] Sending an ACK with offset=139 forced=0\nI20250204 08:07:39.746300     6 replica.cc:921] Sending an ACK with offset=104 forced=0\nI20250204 08:07:40.746562     7 replica.cc:921] Sending an ACK with offset=209 forced=0\nI20250204 08:07:40.746655     6 replica.cc:921] Sending an ACK with offset=172 forced=0\nI20250204 08:07:41.245782     7 server_family.cc:2769] Replicating 10.233.137.19:9999\nI20250204 08:07:41.245841     7 replica.cc:148] Stopping replication\nI20250204 08:07:41.246042     7 replica.cc:703] Exit stable sync\nI20250204 08:07:41.246091     7 replica.cc:276] Main replication fiber finished\nI20250204 08:07:41.318176     7 replica.cc:94] Starting replication\nI20250204 08:07:41.318238     7 replica.cc:116] Resolving master DNS\nI20250204 08:07:41.318380     7 replica.cc:121] Connecting to master\nI20250204 08:07:41.318848     7 replica.cc:126] Greeting\nI20250204 08:07:41.318864     7 replica.cc:281] greeting message handling\nI20250204 08:07:41.320183     7 replica.cc:360] Master id: 9d33ab3e04153c86a6b2f8f4825a4a7e03b92786, sync id: SYNC3, num journals: 2, version: 3\nI20250204 08:07:41.321235     7 replica.cc:193] Main replication fiber started\nI20250204 08:07:41.321892     7 replica.cc:741] Sending on flow 9d33ab3e04153c86a6b2f8f4825a4a7e03b92786 SYNC3 1\nI20250204 08:07:41.322031     6 replica.cc:741] Sending on flow 9d33ab3e04153c86a6b2f8f4825a4a7e03b92786 SYNC3 0\nI20250204 08:07:41.323082     7 replica.cc:724] Sending: DFLY SYNC SYNC3\nI20250204 08:07:41.326880     7 replica.cc:569] Started full sync with 10.233.137.19:9999\nI20250204 08:07:41.326922     7 replica.cc:573] Waiting for all full sync cut confirmations\nI20250204 08:07:42.240965     7 replica.cc:724] Sending: DFLY STARTSTABLE SYNC3\nI20250204 08:07:42.241829     7 replica.cc:843] FullSyncDflyFb finished after reading 75717753 bytes\nI20250204 08:07:42.242180     6 replica.cc:843] FullSyncDflyFb finished after reading 55223393 bytes\nI20250204 08:07:42.243268     7 replica.cc:589] full sync finished in 921 ms\nI20250204 08:07:42.243342     7 replica.cc:679] Transitioned into stable sync\nI20250204 08:07:42.244382     7 replica.cc:921] Sending an ACK with offset=256 forced=0\nI20250204 08:07:42.245657     6 replica.cc:921] Sending an ACK with offset=203 forced=0\nI20250204 08:07:43.245344     7 replica.cc:921] Sending an ACK with offset=281 forced=0\nI20250204 08:07:43.246227     6 replica.cc:921] Sending an ACK with offset=222 forced=0\nI20250204 08:07:44.245911     7 replica.cc:921] Sending an ACK with offset=383 forced=0\nI20250204 08:07:44.246526     6 replica.cc:921] Sending an ACK with offset=299 forced=0\n[ last mesage repeating ]\n```\n\n\n",
  "hints_text": "Hey @odoucet, thanks for reporting this issue. I'm sorry that the previous fix does not cover this case :(\n\nA few follow up questions to help us focus:\n* How frequently does it crash?\n* Does it only crash during the initial replication process (following `REPLICAOF`), or can it also crash in arbitrary/random points in time?\n* Can you please confirm that if you run the replica **without** `--cache_mode=true` it never crashes?\n* Same question as above, but if you run it with `--dbfilename=` (i.e. that it won't try to save/load RDB snapshots before it starts to replicate)\n* Would you be able to share the RDB file with us, privately if needed, so we can examine it?\n\nThanks!\n> How frequently does it crash?\n\nOnce every few hours, but 100% after first restart.\n\n> Does it only crash during the initial replication process (following REPLICAOF), or can it also crash in arbitrary/random points in time?\n\nThe first question answers this I  guess : the master has a high uptime, the replica is failing at every restart + once in a while.\n\n> Can you please confirm that if you run the replica without --cache_mode=true it never crashes?\n\nCannot confirm yet, will try later.\n\n> Same question as above, but if you run it with --dbfilename= (i.e. that it won't try to save/load RDB snapshots before it starts to replicate)\n\nCannot confirm yet, will try later.\n\n> Would you be able to share the RDB file with us, privately if needed, so we can examine it?\n\nI would need to reproduce this on our lab but definitely doable, I get back to you when I know more.",
  "created_at": "2025-02-05T12:17:19Z",
  "modified_files": [
    "src/server/db_slice.cc",
    "src/server/db_slice.h",
    "src/server/main_service.cc",
    "src/server/rdb_load.cc",
    "src/server/replica.cc"
  ],
  "modified_test_files": [
    "src/server/rdb_test.cc"
  ]
}