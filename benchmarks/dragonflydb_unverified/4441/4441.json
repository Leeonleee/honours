{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4441,
  "instance_id": "dragonflydb__dragonfly-4441",
  "issue_numbers": [
    "4424"
  ],
  "base_commit": "e39e68276eab2a665b5e4553f2a085936ec5e48a",
  "patch": "diff --git a/src/facade/reply_builder.cc b/src/facade/reply_builder.cc\nindex ba51e49c1f75..3ae3d32783aa 100644\n--- a/src/facade/reply_builder.cc\n+++ b/src/facade/reply_builder.cc\n@@ -108,12 +108,20 @@ template <typename... Ts> void SinkReplyBuilder::WritePieces(Ts&&... pieces) {\n   if (size_t required = (piece_size(pieces) + ...); buffer_.AppendLen() <= required)\n     Flush(required);\n \n+  auto iovec_end = [](const iovec& v) { return reinterpret_cast<char*>(v.iov_base) + v.iov_len; };\n+\n   // Ensure last iovec points to buffer segment\n   char* dest = reinterpret_cast<char*>(buffer_.AppendBuffer().data());\n-  if (vecs_.empty() || ((char*)vecs_.back().iov_base) + vecs_.back().iov_len != dest)\n-    NextVec({dest, 0});\n+  if (vecs_.empty()) {\n+    vecs_.push_back(iovec{dest, 0});\n+  } else if (iovec_end(vecs_.back()) != dest) {\n+    if (vecs_.size() >= IOV_MAX - 2)\n+      Flush();\n+    dest = reinterpret_cast<char*>(buffer_.AppendBuffer().data());\n+    vecs_.push_back(iovec{dest, 0});\n+  }\n \n-  dest = reinterpret_cast<char*>(buffer_.AppendBuffer().data());\n+  DCHECK(iovec_end(vecs_.back()) == dest);\n   char* ptr = dest;\n   ([&]() { ptr = write_piece(pieces, ptr); }(), ...);\n \n@@ -124,7 +132,9 @@ template <typename... Ts> void SinkReplyBuilder::WritePieces(Ts&&... pieces) {\n }\n \n void SinkReplyBuilder::WriteRef(std::string_view str) {\n-  NextVec(str);\n+  if (vecs_.size() >= IOV_MAX - 2)\n+    Flush();\n+  vecs_.push_back(iovec{const_cast<char*>(str.data()), str.size()});\n   total_size_ += str.size();\n }\n \n@@ -183,7 +193,7 @@ void SinkReplyBuilder::FinishScope() {\n   if (ref_bytes > buffer_.AppendLen())\n     return Flush(ref_bytes);\n \n-  // Copy all extenral references to buffer to safely keep batching\n+  // Copy all external references to buffer to safely keep batching\n   for (size_t i = guaranteed_pieces_; i < vecs_.size(); i++) {\n     auto ib = buffer_.InputBuffer();\n     if (vecs_[i].iov_base >= ib.data() && vecs_[i].iov_base <= ib.data() + ib.size())\n@@ -198,12 +208,6 @@ void SinkReplyBuilder::FinishScope() {\n   guaranteed_pieces_ = vecs_.size();  // all vecs are pieces\n }\n \n-void SinkReplyBuilder::NextVec(std::string_view str) {\n-  if (vecs_.size() >= IOV_MAX - 2)\n-    Flush();\n-  vecs_.push_back(iovec{const_cast<char*>(str.data()), str.size()});\n-}\n-\n MCReplyBuilder::MCReplyBuilder(::io::Sink* sink) : SinkReplyBuilder(sink), all_(0) {\n }\n \n@@ -312,6 +316,7 @@ void RedisReplyBuilderBase::SendBulkString(std::string_view str) {\n   if (str.size() <= kMaxInlineSize)\n     return WritePieces(kLengthPrefix, uint32_t(str.size()), kCRLF, str, kCRLF);\n \n+  DVLOG(1) << \"SendBulk \" << str.size();\n   WritePieces(kLengthPrefix, uint32_t(str.size()), kCRLF);\n   WriteRef(str);\n   WritePieces(kCRLF);\ndiff --git a/src/facade/reply_builder.h b/src/facade/reply_builder.h\nindex ec3ce6580e09..3b1a7b464e01 100644\n--- a/src/facade/reply_builder.h\n+++ b/src/facade/reply_builder.h\n@@ -129,9 +129,7 @@ class SinkReplyBuilder {\n   void WritePieces(Ts&&... pieces);     // Copy pieces into buffer and reference buffer\n   void WriteRef(std::string_view str);  // Add iovec bypassing buffer\n \n-  void FinishScope();  // Called when scope ends\n-  void NextVec(std::string_view str);\n-\n+  void FinishScope();  // Called when scope ends to flush buffer if needed\n   void Send();\n \n  protected:\n",
  "test_patch": "diff --git a/src/facade/reply_builder_test.cc b/src/facade/reply_builder_test.cc\nindex f1a3c5b49b25..ee83885d3a97 100644\n--- a/src/facade/reply_builder_test.cc\n+++ b/src/facade/reply_builder_test.cc\n@@ -917,6 +917,23 @@ TEST_F(RedisReplyBuilderTest, Issue3449) {\n   EXPECT_EQ(10000, parse_result.args.size());\n }\n \n+TEST_F(RedisReplyBuilderTest, Issue4424) {\n+  vector<string> records;\n+  for (unsigned i = 0; i < 800; ++i) {\n+    records.push_back(string(100, 'a'));\n+  }\n+\n+  for (unsigned j = 0; j < 2; ++j) {\n+    builder_->SendBulkStrArr(records);\n+    ASSERT_TRUE(NoErrors());\n+    ParsingResults parse_result = Parse();\n+    ASSERT_FALSE(parse_result.IsError()) << int(parse_result.result);\n+    ASSERT_TRUE(parse_result.Verify(SinkSize()));\n+    EXPECT_EQ(800, parse_result.args.size());\n+    sink_.Clear();\n+  }\n+}\n+\n static void BM_FormatDouble(benchmark::State& state) {\n   vector<double> values;\n   char buf[64];\n",
  "problem_statement": "Invalid bulk string terminator\n**Describe the bug**\r\n\r\nAfter upgrading our dragonfly instances from version v1.20.1 to version v1.25.5 we noticed the appearance of this exception (using the .net driver):\r\n\"StackExchange.Redis.RedisConnectionException: Invalid bulk string terminator\"\r\n\r\nWe performed tests with versions v1.24.0 and v1.26.0 which also presented the same problem.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Just run the application. We use version 2.7.33 of the .net driver (https://github.com/StackExchange/StackExchange.Redis)\r\n\r\n**Expected behavior**\r\nWe expect the application to run normally, as it did in version v1.20.1.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Oracle Linux Server 8.8\r\n - Kernel: 5.15.0-200.131.27.1.el8uek.x86_64\r\n - Containerized?: Kubernetes\r\n - Dragonfly Version: v1.25.5\r\n\r\n**Additional context**\r\n```\r\nStackExchange.Redis.RedisConnectionException: Invalid bulk string terminator\r\nat StackExchange.Redis.PhysicalConnection.ReadBulkString(ResultType type, ResultFlags flags, BufferReader& reader, Boolean includeDetailInExceptions, ServerEndPoint server) in /_/src/StackExchange.Redis/PhysicalConnection.cs:line 2000\r\nat StackExchange.Redis.PhysicalConnection.TryParseResult(ResultFlags flags, Arena`1 arena, ReadOnlySequence`1& buffer, BufferReader& reader, Boolean includeDetilInExceptions, ServerEndPoint server, Boolean allowInlineProtocol) in /_/src/StackExchange.Redis/PhysicalConnection.cs:line 2078\r\nat StackExchange.Redis.PhysicalConnection.ReadArray(ResultType resultType, ResultFlags flags, Arena`1 arena, ReadOnlySequence`1& buffer, BufferReader& reader, Boolean includeDetailInExceptions, ServerEndPoint server) in /_/src/StackExchange.Redis/PhysicalConnection.cs:line 1951\r\nat StackExchange.Redis.PhysicalConnection.TryParseResult(ResultFlags flags, Arena`1 arena, ReadOnlySequence`1& buffer, BufferReader& reader, Boolean includeDetilInExceptions, ServerEndPoint server, Boolean allowInlineProtocol) in /_/src/StackExchange.Redis/PhysicalConnection.cs:line 2081\r\nat StackExchange.Redis.PhysicalConnection.ProcessBuffer(ReadOnlySequence`1& buffer) in /_/src/StackExchange.Redis/PhysicalConnection.cs:line 1857\r\n```\r\n\n",
  "hints_text": "thank you for reporting the problem.\r\n\r\nTo fix it we need to know exactly the response that causes the problem.\r\n\r\nluckily I found this helper tool in .Net client library\r\nhttps://github.com/StackExchange/StackExchange.Redis/blob/main/docs/RespLogging.md#logging-resp-streams\r\n\r\nwill it be possible to record the response stream so we could reproduce the problem locally?\nBasic Information:\r\n- Dragonfly v1.26.0 Docker on AWS.\r\n- .NET StackExchange.Redis, version 2.8.24\r\n\r\nWe've encountered an consistent error with `LRANGE` using `ListRangeAsync` in StackExchange.Redis during the initial implementation phase. List retrieval alternates between success and failure (e.g., first attempt succeeds, second fails) for a list of 1075 objects. We're investigating further and will provide more details as they become available.\nWe also experienced this issue last year when we upgraded to Dragonfly v1.22.2. \r\n\r\nUpgrading StackExchange.Redis version had no impact. \r\n\r\nOnly solution we found at the time was to rollback to v1.21.4 which seemed to be the last version to not experience the issue.\r\n\r\nFor us the error was inconsistent - we didn't see it in testing but appeared sporadically during a production rollout. Even then the error did not appear always even repeating the same application steps with the same dataset, so it has been difficult to capture.\r\n\r\nThe error was most likely to appear when we used ZRANGEBYSCORE (via StackExchange.Redis SortedSetRangeByScoreAsync method) on a Sorted Set with a larger limit (200+) of JSON encoded value objects.\r\n\r\nIf we are able to capture more details, I'll add them here.\nWe switched to a new code that renders RESP replies in more efficient way. I am confident the bug is there. With upto 1.25.6 it is possible to workaround the issue  using `--experimental_new_io=false` but starting from 1.26.0 we removed the old code, because we did not know of any issues related to it :( \r\nI wish I knew how we can reproduce it. @MaksTenne  if you can provide us with a reproducible example, this would really help!\nHey @romange,\r\n\r\nI've created a reproducible public repository to address the \"Invalid bulk string terminator\" issue encountered in the \"LRANGE\" ListRangeAsync implementation of StackExchange.Redis. You can find the repo here: [Invalid-bulk-string-terminator](https://github.com/MaksTenne/Invalid-bulk-string-terminator).\r\n\r\nIf you need any additional information or assistance to help fix the problem, please feel free to reach out!\n@MaksTenne  thanks! Currently I am stuck at causing the webapp to run correctly.\r\nThis is what I do:\r\n1. `docker build .  -t app.net` - created the container\r\n2. `docker run --network=host -e ASPNETCORE_ENVIRONMENT=Development  app.net`, it prints:\r\n  ```\r\n  info: Microsoft.Hosting.Lifetime[14]\r\n      Now listening on: http://[::]:8080\r\ninfo: Microsoft.Hosting.Lifetime[0]\r\n      Application started. Press Ctrl+C to shut down.\r\ninfo: Microsoft.Hosting.Lifetime[0]\r\n      Hosting environment: Development\r\ninfo: Microsoft.Hosting.Lifetime[0]\r\n      Content root path: /app\r\n```\r\n\r\nI at this point I know that the app is running because if I do not run dragonfly on 6379, it exits with an error. So far so good.\r\nbut then whenever I run any curl command, it results in 400.\r\n\r\n```\r\ncurl -v      http://[::]:8080/app/Redis/data\r\ncurl -v     -H \"Content-Type: application/json\" -d '{\"someData\": \"value\"}'  http://[::]:8080/app/Redis/generate\r\n``` \r\nor any other url - results in error 400:\r\n```\r\n*   Trying :::8080...\r\n* Connected to :: (::1) port 8080 (#0)\r\n> POST /app/Redis/generate HTTP/1.1\r\n> Host: [::]:8080\r\n> User-Agent: curl/7.81.0\r\n> Accept: */*\r\n> Content-Type: application/json\r\n> Content-Length: 21\r\n> \r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 400 Bad Request\r\n< Content-Length: 0\r\n< Connection: close\r\n< Date: Fri, 10 Jan 2025 06:53:36 GMT\r\n< Server: Kestrel\r\n< \r\n* Closing connection 0\r\n```\r\n\n@romange\r\n\r\nInstead of using `--network=host`, I use `-p 8080:8080` because `--network` doesn't work for me.\r\n\r\nURLs:\r\n- `curl -v http://localhost:8080/api/Redis/generate`\r\n- `curl -v http://localhost:8080/api/Redis/data`\r\n\r\nI accidentally configured the `Generate` method as `HttpPost` instead of `HttpGet`.\r\n\r\nPull the repo again and it should be changed.\nthanks, I succeeded to run both handlers and both succeed for me. ` http://localhost:8080/api/Redis/data` returns a big json object. do you see something else @MaksTenne ?  how the failure looks like?\r\n\nAh, I see - the second request fails.",
  "created_at": "2025-01-11T15:05:58Z",
  "modified_files": [
    "src/facade/reply_builder.cc",
    "src/facade/reply_builder.h"
  ],
  "modified_test_files": [
    "src/facade/reply_builder_test.cc"
  ]
}