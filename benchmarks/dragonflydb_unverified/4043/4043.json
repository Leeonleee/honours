{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4043,
  "instance_id": "dragonflydb__dragonfly-4043",
  "issue_numbers": [
    "3983"
  ],
  "base_commit": "4859077122025d29a81430efd37fece1ba6e9c18",
  "patch": "diff --git a/src/server/search/search_family.cc b/src/server/search/search_family.cc\nindex 6ef550715c10..c710d03f569c 100644\n--- a/src/server/search/search_family.cc\n+++ b/src/server/search/search_family.cc\n@@ -226,49 +226,49 @@ search::QueryParams ParseQueryParams(CmdArgParser* parser) {\n   return params;\n }\n \n-optional<SearchParams> ParseSearchParamsOrReply(CmdArgParser parser, SinkReplyBuilder* builder) {\n+optional<SearchParams> ParseSearchParamsOrReply(CmdArgParser* parser, SinkReplyBuilder* builder) {\n   SearchParams params;\n \n-  while (parser.HasNext()) {\n+  while (parser->HasNext()) {\n     // [LIMIT offset total]\n-    if (parser.Check(\"LIMIT\")) {\n-      params.limit_offset = parser.Next<size_t>();\n-      params.limit_total = parser.Next<size_t>();\n-    } else if (parser.Check(\"LOAD\")) {\n+    if (parser->Check(\"LIMIT\")) {\n+      params.limit_offset = parser->Next<size_t>();\n+      params.limit_total = parser->Next<size_t>();\n+    } else if (parser->Check(\"LOAD\")) {\n       if (params.return_fields) {\n         builder->SendError(\"LOAD cannot be applied after RETURN\");\n         return std::nullopt;\n       }\n \n-      ParseLoadFields(&parser, &params.load_fields);\n-    } else if (parser.Check(\"RETURN\")) {\n+      ParseLoadFields(parser, &params.load_fields);\n+    } else if (parser->Check(\"RETURN\")) {\n       if (params.load_fields) {\n         builder->SendError(\"RETURN cannot be applied after LOAD\");\n         return std::nullopt;\n       }\n \n       // RETURN {num} [{ident} AS {name}...]\n-      size_t num_fields = parser.Next<size_t>();\n+      size_t num_fields = parser->Next<size_t>();\n       params.return_fields.emplace();\n       while (params.return_fields->size() < num_fields) {\n-        string_view ident = parser.Next();\n-        string_view alias = parser.Check(\"AS\") ? parser.Next() : ident;\n+        string_view ident = parser->Next();\n+        string_view alias = parser->Check(\"AS\") ? parser->Next() : ident;\n         params.return_fields->emplace_back(ident, alias);\n       }\n-    } else if (parser.Check(\"NOCONTENT\")) {  // NOCONTENT\n+    } else if (parser->Check(\"NOCONTENT\")) {  // NOCONTENT\n       params.load_fields.emplace();\n       params.return_fields.emplace();\n-    } else if (parser.Check(\"PARAMS\")) {  // [PARAMS num(ignored) name(ignored) knn_vector]\n-      params.query_params = ParseQueryParams(&parser);\n-    } else if (parser.Check(\"SORTBY\")) {\n-      params.sort_option = search::SortOption{string{parser.Next()}, bool(parser.Check(\"DESC\"))};\n+    } else if (parser->Check(\"PARAMS\")) {  // [PARAMS num(ignored) name(ignored) knn_vector]\n+      params.query_params = ParseQueryParams(parser);\n+    } else if (parser->Check(\"SORTBY\")) {\n+      params.sort_option = search::SortOption{string{parser->Next()}, bool(parser->Check(\"DESC\"))};\n     } else {\n       // Unsupported parameters are ignored for now\n-      parser.Skip(1);\n+      parser->Skip(1);\n     }\n   }\n \n-  if (auto err = parser.Error(); err) {\n+  if (auto err = parser->Error(); err) {\n     builder->SendError(err->MakeReply());\n     return nullopt;\n   }\n@@ -716,10 +716,11 @@ void SearchFamily::FtList(CmdArgList args, Transaction* tx, SinkReplyBuilder* bu\n }\n \n void SearchFamily::FtSearch(CmdArgList args, Transaction* tx, SinkReplyBuilder* builder) {\n-  string_view index_name = ArgS(args, 0);\n-  string_view query_str = ArgS(args, 1);\n+  CmdArgParser parser{args};\n+  string_view index_name = parser.Next();\n+  string_view query_str = parser.Next();\n \n-  auto params = ParseSearchParamsOrReply(args.subspan(2), builder);\n+  auto params = ParseSearchParamsOrReply(&parser, builder);\n   if (!params.has_value())\n     return;\n \n@@ -749,77 +750,129 @@ void SearchFamily::FtSearch(CmdArgList args, Transaction* tx, SinkReplyBuilder*\n   }\n \n   if (auto agg = search_algo.HasAggregation(); agg)\n-    ReplySorted(std::move(*agg), *params, absl::MakeSpan(docs), builder);\n+    ReplySorted(*agg, *params, absl::MakeSpan(docs), builder);\n   else\n     ReplyWithResults(*params, absl::MakeSpan(docs), builder);\n }\n \n void SearchFamily::FtProfile(CmdArgList args, Transaction* tx, SinkReplyBuilder* builder) {\n-  string_view index_name = ArgS(args, 0);\n-  string_view query_str = ArgS(args, 3);\n+  CmdArgParser parser{args};\n+\n+  string_view index_name = parser.Next();\n \n-  optional<SearchParams> params = ParseSearchParamsOrReply(args.subspan(4), builder);\n+  if (!parser.Check(\"SEARCH\") && !parser.Check(\"AGGREGATE\")) {\n+    return builder->SendError(\"no `SEARCH` or `AGGREGATE` provided\");\n+  }\n+\n+  parser.Check(\"LIMITED\");  // TODO: Implement limited profiling\n+  parser.ExpectTag(\"QUERY\");\n+\n+  string_view query_str = parser.Next();\n+\n+  optional<SearchParams> params = ParseSearchParamsOrReply(&parser, builder);\n   if (!params.has_value())\n     return;\n \n   search::SearchAlgorithm search_algo;\n   search::SortOption* sort_opt = params->sort_option.has_value() ? &*params->sort_option : nullptr;\n   if (!search_algo.Init(query_str, &params->query_params, sort_opt))\n-    return builder->SendError(\"Query syntax error\");\n+    return builder->SendError(\"query syntax error\");\n \n   search_algo.EnableProfiling();\n \n   absl::Time start = absl::Now();\n-  atomic_uint total_docs = 0;\n-  atomic_uint total_serialized = 0;\n+  const size_t shards_count = shard_set->size();\n \n-  vector<pair<search::AlgorithmProfile, absl::Duration>> results(shard_set->size());\n+  // Because our coordinator thread may not have a shard, we can't check ahead if the index exists.\n+  std::atomic<bool> index_not_found{false};\n+  std::vector<SearchResult> search_results(shards_count);\n+  std::vector<absl::Duration> profile_results(shards_count);\n \n   tx->ScheduleSingleHop([&](Transaction* t, EngineShard* es) {\n     auto* index = es->search_indices()->GetIndex(index_name);\n-    if (!index)\n+    if (!index) {\n+      index_not_found.store(true, memory_order_relaxed);\n       return OpStatus::OK;\n+    }\n \n-    auto shard_start = absl::Now();\n-    auto res = index->Search(t->GetOpArgs(es), *params, &search_algo);\n-\n-    total_docs.fetch_add(res.total_hits);\n-    total_serialized.fetch_add(res.docs.size());\n+    const ShardId shard_id = es->shard_id();\n \n-    DCHECK(res.profile);\n-    results[es->shard_id()] = {std::move(*res.profile), absl::Now() - shard_start};\n+    auto shard_start = absl::Now();\n+    search_results[shard_id] = index->Search(t->GetOpArgs(es), *params, &search_algo);\n+    profile_results[shard_id] = {absl::Now() - shard_start};\n \n     return OpStatus::OK;\n   });\n \n+  if (index_not_found.load())\n+    return builder->SendError(std::string{index_name} + \": no such index\");\n+\n   auto took = absl::Now() - start;\n+\n+  bool result_is_empty = false;\n+  size_t total_docs = 0;\n+  size_t total_serialized = 0;\n+  for (const auto& result : search_results) {\n+    if (!result.error) {\n+      total_docs += result.total_hits;\n+      total_serialized += result.docs.size();\n+    } else {\n+      result_is_empty = true;\n+    }\n+  }\n+\n   auto* rb = static_cast<RedisReplyBuilder*>(builder);\n-  rb->StartArray(results.size() + 1);\n+  // First element -> Result of the search command\n+  // Second element -> Profile information\n+  rb->StartArray(2);\n+\n+  // Result of the search command\n+  if (!result_is_empty) {\n+    auto agg = search_algo.HasAggregation();\n+    if (agg) {\n+      ReplySorted(*agg, *params, absl::MakeSpan(search_results), builder);\n+    } else {\n+      ReplyWithResults(*params, absl::MakeSpan(search_results), builder);\n+    }\n+  } else {\n+    rb->StartArray(1);\n+    rb->SendLong(0);\n+  }\n+\n+  // Profile information\n+  rb->StartArray(shards_count + 1);\n \n   // General stats\n   rb->StartCollection(3, RedisReplyBuilder::MAP);\n   rb->SendBulkString(\"took\");\n   rb->SendLong(absl::ToInt64Microseconds(took));\n   rb->SendBulkString(\"hits\");\n-  rb->SendLong(total_docs);\n+  rb->SendLong(static_cast<long>(total_docs));\n   rb->SendBulkString(\"serialized\");\n-  rb->SendLong(total_serialized);\n+  rb->SendLong(static_cast<long>(total_serialized));\n \n   // Per-shard stats\n-  for (const auto& [profile, shard_took] : results) {\n+  for (size_t shard_id = 0; shard_id < shards_count; shard_id++) {\n     rb->StartCollection(2, RedisReplyBuilder::MAP);\n     rb->SendBulkString(\"took\");\n-    rb->SendLong(absl::ToInt64Microseconds(shard_took));\n+    rb->SendLong(absl::ToInt64Microseconds(profile_results[shard_id]));\n     rb->SendBulkString(\"tree\");\n \n-    for (size_t i = 0; i < profile.events.size(); i++) {\n-      const auto& event = profile.events[i];\n+    const auto& search_result = search_results[shard_id];\n+    if (search_result.error || !search_result.profile || search_result.profile->events.empty()) {\n+      rb->SendEmptyArray();\n+      continue;\n+    }\n+\n+    const auto& events = search_result.profile->events;\n+    for (size_t i = 0; i < events.size(); i++) {\n+      const auto& event = events[i];\n \n       size_t children = 0;\n-      for (size_t j = i + 1; j < profile.events.size(); j++) {\n-        if (profile.events[j].depth == event.depth)\n+      for (size_t j = i + 1; j < events.size(); j++) {\n+        if (events[j].depth == event.depth)\n           break;\n-        if (profile.events[j].depth == event.depth + 1)\n+        if (events[j].depth == event.depth + 1)\n           children++;\n       }\n \n",
  "test_patch": "diff --git a/src/server/search/search_family_test.cc b/src/server/search/search_family_test.cc\nindex dbe063ab22e5..7b11980357d0 100644\n--- a/src/server/search/search_family_test.cc\n+++ b/src/server/search/search_family_test.cc\n@@ -22,6 +22,27 @@ class SearchFamilyTest : public BaseFamilyTest {\n \n const auto kNoResults = IntArg(0);  // tests auto destruct single element arrays\n \n+/* Asserts that response is array of two arrays. Used to test FT.PROFILE response */\n+::testing::AssertionResult AssertArrayOfTwoArrays(const RespExpr& resp) {\n+  if (resp.GetVec().size() != 2) {\n+    return ::testing::AssertionFailure()\n+           << \"Expected response array length to be 2, but was \" << resp.GetVec().size();\n+  }\n+\n+  const auto& vec = resp.GetVec();\n+  if (vec[0].type != RespExpr::ARRAY) {\n+    return ::testing::AssertionFailure()\n+           << \"Expected resp[0] to be an array, but was \" << vec[0].type;\n+  }\n+  if (vec[1].type != RespExpr::ARRAY) {\n+    return ::testing::AssertionFailure()\n+           << \"Expected resp[1] to be an array, but was \" << vec[1].type;\n+  }\n+  return ::testing::AssertionSuccess();\n+}\n+\n+#define ASSERT_ARRAY_OF_TWO_ARRAYS(resp) ASSERT_PRED1(AssertArrayOfTwoArrays, resp)\n+\n MATCHER_P2(DocIds, total, arg_ids, \"\") {\n   if (arg_ids.empty()) {\n     if (auto res = arg.GetInt(); !res || *res != 0) {\n@@ -790,20 +811,55 @@ TEST_F(SearchFamilyTest, FtProfile) {\n   Run({\"ft.create\", \"i1\", \"schema\", \"name\", \"text\"});\n \n   auto resp = Run({\"ft.profile\", \"i1\", \"search\", \"query\", \"(a | b) c d\"});\n+  ASSERT_ARRAY_OF_TWO_ARRAYS(resp);\n \n   const auto& top_level = resp.GetVec();\n-  EXPECT_EQ(top_level.size(), shard_set->size() + 1);\n+  EXPECT_THAT(top_level[0], IsMapWithSize());\n+\n+  const auto& profile_result = top_level[1].GetVec();\n+  EXPECT_EQ(profile_result.size(), shard_set->size() + 1);\n \n-  EXPECT_THAT(top_level[0].GetVec(), ElementsAre(\"took\", _, \"hits\", _, \"serialized\", _));\n+  EXPECT_THAT(profile_result[0].GetVec(), ElementsAre(\"took\", _, \"hits\", _, \"serialized\", _));\n \n   for (size_t sid = 0; sid < shard_set->size(); sid++) {\n-    const auto& shard_resp = top_level[sid + 1].GetVec();\n+    const auto& shard_resp = profile_result[sid + 1].GetVec();\n     EXPECT_THAT(shard_resp, ElementsAre(\"took\", _, \"tree\", _));\n \n     const auto& tree = shard_resp[3].GetVec();\n     EXPECT_THAT(tree[0].GetString(), HasSubstr(\"Logical{n=3,o=and}\"sv));\n     EXPECT_EQ(tree[1].GetVec().size(), 3);\n   }\n+\n+  // Test LIMITED throws no errors\n+  resp = Run({\"ft.profile\", \"i1\", \"search\", \"limited\", \"query\", \"(a | b) c d\"});\n+  ASSERT_ARRAY_OF_TWO_ARRAYS(resp);\n+}\n+\n+TEST_F(SearchFamilyTest, FtProfileInvalidQuery) {\n+  Run({\"json.set\", \"j1\", \".\", R\"({\"id\":\"1\"})\"});\n+  Run({\"ft.create\", \"i1\", \"on\", \"json\", \"schema\", \"$.id\", \"as\", \"id\", \"tag\"});\n+\n+  auto resp = Run({\"ft.profile\", \"i1\", \"search\", \"query\", \"@id:[1 1]\"});\n+  ASSERT_ARRAY_OF_TWO_ARRAYS(resp);\n+\n+  EXPECT_THAT(resp.GetVec()[0], IsMapWithSize());\n+\n+  resp = Run({\"ft.profile\", \"i1\", \"search\", \"query\", \"@{invalid13289}\"});\n+  EXPECT_THAT(resp, ErrArg(\"query syntax error\"));\n+}\n+\n+TEST_F(SearchFamilyTest, FtProfileErrorReply) {\n+  Run({\"ft.create\", \"i1\", \"schema\", \"name\", \"text\"});\n+  ;\n+\n+  auto resp = Run({\"ft.profile\", \"i1\", \"not_search\", \"query\", \"(a | b) c d\"});\n+  EXPECT_THAT(resp, ErrArg(\"no `SEARCH` or `AGGREGATE` provided\"));\n+\n+  resp = Run({\"ft.profile\", \"i1\", \"search\", \"not_query\", \"(a | b) c d\"});\n+  EXPECT_THAT(resp, ErrArg(\"syntax error\"));\n+\n+  resp = Run({\"ft.profile\", \"non_existent_key\", \"search\", \"query\", \"(a | b) c d\"});\n+  EXPECT_THAT(resp, ErrArg(\"non_existent_key: no such index\"));\n }\n \n TEST_F(SearchFamilyTest, SimpleExpiry) {\n",
  "problem_statement": "SIGSEGV when quering a TAG indexed field using a NUMERIC field syntax\n**Describe the bug**\r\n\r\nWhen quering a tag indexed field in the wrong way the `FT.SEARCH` command returns this error `ERR Wrong access type for field: id`. However when using `FT.PROFILE` this crashes the entire instance. \r\n\r\nThis is the log output of my docker instance\r\n```\r\ndragonfly-1  | *** SIGSEGV received at time=1729770213 on cpu 12 ***\r\ndragonfly-1  | PC: @     0x55b98c1ea10b  (unknown)  dfly::SearchFamily::FtProfile()\r\ndragonfly-1 exited with code 139\r\n```\r\n\r\n**To Reproduce**\r\nStart dragonfly with docker `docker.dragonflydb.io/dragonflydb/dragonfly`\r\nConnect to dragonfly\r\nRun `FT.CREATE idx ON JSON SCHEMA $.id AS id TAG` to create an index\r\nRun `FT.PROFILE idx SEARCH QUERY \"@id:[1 1]\"` this causes the crash\r\n\r\n**Expected behavior**\r\n\r\nSame as the `FT.SEARCH` command this should return the `ERR Wrong access type for field: id` error.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: `Ubuntu 22.04`\r\n - Kernel: `Linux 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux`\r\n - Containerized: `Docker`\r\n - Dragonfly Version: `v1.24.0`\r\n\r\n**Reproducible Code Snippet**\r\n```\r\nFT.CREATE idx ON JSON SCHEMA $.id AS id TAG\r\nFT.PROFILE idx SEARCH QUERY \"@id:[1 1]\"\r\n```\r\n\r\n\n",
  "hints_text": "",
  "created_at": "2024-11-03T14:09:14Z",
  "modified_files": [
    "src/server/search/search_family.cc"
  ],
  "modified_test_files": [
    "src/server/search/search_family_test.cc"
  ]
}