{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 3204,
  "instance_id": "dragonflydb__dragonfly-3204",
  "issue_numbers": [
    "3202"
  ],
  "base_commit": "48c6f4bf74f8b059e2d0a432904b0c1b052cb6bb",
  "patch": "diff --git a/src/server/replica.cc b/src/server/replica.cc\nindex 4a5ce135f4ad..d7e02fa1d497 100644\n--- a/src/server/replica.cc\n+++ b/src/server/replica.cc\n@@ -42,6 +42,9 @@ ABSL_FLAG(bool, break_replication_on_master_restart, false,\n           \"When in replica mode, and master restarts, break replication from master to avoid \"\n           \"flushing the replica's data.\");\n ABSL_DECLARE_FLAG(int32_t, port);\n+ABSL_FLAG(\n+    int, replica_priority, 100,\n+    \"Published by info command for sentinel to pick replica based on score during a failover\");\n \n // TODO: Remove this flag on release >= 1.22\n ABSL_FLAG(bool, replica_reconnect_on_master_restart, false,\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex ac2a16afd730..5f7c4725bd00 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -132,6 +132,7 @@ ABSL_DECLARE_FLAG(uint32_t, hz);\n ABSL_DECLARE_FLAG(bool, tls);\n ABSL_DECLARE_FLAG(string, tls_ca_cert_file);\n ABSL_DECLARE_FLAG(string, tls_ca_cert_dir);\n+ABSL_DECLARE_FLAG(int, replica_priority);\n \n bool AbslParseFlag(std::string_view in, ReplicaOfFlag* flag, std::string* err) {\n #define RETURN_ON_ERROR(cond, m)                                           \\\n@@ -2284,6 +2285,8 @@ void ServerFamily::Info(CmdArgList args, ConnectionContext* cntx) {\n         append(\"master_last_io_seconds_ago\", rinfo.master_last_io_sec);\n         append(\"master_sync_in_progress\", rinfo.full_sync_in_progress);\n         append(\"master_replid\", rinfo.master_id);\n+        append(\"slave_priority\", GetFlag(FLAGS_replica_priority));\n+        append(\"slave_read_only\", 1);\n       };\n       replication_info_cb(replica_->GetInfo());\n       for (const auto& replica : cluster_replicas_) {\n",
  "test_patch": "diff --git a/tests/dragonfly/sentinel_test.py b/tests/dragonfly/sentinel_test.py\nindex d65137c84938..8395225ed24a 100644\n--- a/tests/dragonfly/sentinel_test.py\n+++ b/tests/dragonfly/sentinel_test.py\n@@ -8,6 +8,7 @@\n from datetime import datetime\n from sys import stderr\n import logging\n+from . import dfly_args\n \n \n # Helper function to parse some sentinel cli commands output as key value dictionaries.\n@@ -63,6 +64,7 @@ def start(self):\n             f\"port {self.port}\",\n             f\"sentinel monitor {self.default_deployment} 127.0.0.1 {self.initial_master_port} 1\",\n             f\"sentinel down-after-milliseconds {self.default_deployment} 3000\",\n+            f\"slave-priority 100\",\n         ]\n         self.config_file.write_text(\"\\n\".join(config))\n \n@@ -228,7 +230,7 @@ async def test_master_failure(df_local_factory, sentinel, port_picker):\n     # Simulate master failure.\n     master.stop()\n \n-    # Verify replica pormoted.\n+    # Verify replica promoted.\n     await await_for(\n         lambda: sentinel.live_master_port(),\n         lambda p: p == replica.port,\n@@ -239,3 +241,54 @@ async def test_master_failure(df_local_factory, sentinel, port_picker):\n     # Verify we can now write to replica.\n     await replica_client.set(\"key\", \"value\")\n     assert await replica_client.get(\"key\") == b\"value\"\n+\n+\n+@dfly_args({\"info_replication_valkey_compatible\": True})\n+@pytest.mark.asyncio\n+async def test_priority_on_failover(df_local_factory, sentinel, port_picker):\n+    master = df_local_factory.create(port=sentinel.initial_master_port)\n+    # lower priority is the best candidate for sentinel\n+    low_priority_repl = df_local_factory.create(\n+        port=port_picker.get_available_port(), replica_priority=20\n+    )\n+    mid_priority_repl = df_local_factory.create(\n+        port=port_picker.get_available_port(), replica_priority=60\n+    )\n+    high_priority_repl = df_local_factory.create(\n+        port=port_picker.get_available_port(), replica_priority=80\n+    )\n+\n+    master.start()\n+    low_priority_repl.start()\n+    mid_priority_repl.start()\n+    high_priority_repl.start()\n+\n+    high_client = aioredis.Redis(port=high_priority_repl.port)\n+    await high_client.execute_command(\"REPLICAOF localhost \" + str(master.port))\n+\n+    mid_client = aioredis.Redis(port=mid_priority_repl.port)\n+    await mid_client.execute_command(\"REPLICAOF localhost \" + str(master.port))\n+\n+    low_client = aioredis.Redis(port=low_priority_repl.port)\n+    await low_client.execute_command(\"REPLICAOF localhost \" + str(master.port))\n+\n+    assert sentinel.live_master_port() == master.port\n+\n+    # Verify sentinel picked up replica.\n+    await await_for(\n+        lambda: sentinel.master(),\n+        lambda m: m[\"num-slaves\"] == \"3\",\n+        timeout_sec=15,\n+        timeout_msg=\"Timeout waiting for sentinel to pick up replica.\",\n+    )\n+\n+    # Simulate master failure.\n+    master.stop()\n+\n+    # Verify replica promoted.\n+    await await_for(\n+        lambda: sentinel.live_master_port(),\n+        lambda p: p == low_priority_repl.port,\n+        timeout_sec=30,\n+        timeout_msg=\"Timeout waiting for sentinel to report replica as master.\",\n+    )\n",
  "problem_statement": "slave-priority / replica priority like redis configuration\n**Did you search GitHub Issues and GitHub Discussions First?**\r\nYes, I have searched for the issue and have had discussions using Discord and I was recommended to open the issue here\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nyes, that's right, I use dragonfly replication / dragonfly cluster with the master slave method which can do auto failover with sentinel merging to find out the master and slave on dragonfly, so I have 3 nodes that are used as replication with 1 master, then can do auto failover if The master has a problem, the funds will make the slave the next master, with this configuration I have problems because I can't determine which one will be the next master from the best slave, because the failover selects the slave as the next master randomly.\r\n\r\nThe configuration that can determine which slave will be made the next master is the slave-priority / replica-priority configuration which I didn't find in Dragonfly, but I can find this configuration in Redis.\r\n\r\nIf this configuration can be applied to Dragonfly, the slave-priority / replica-priority configuration is very helpful in implementing cluster or master slave replication with auto failover, and can determine the next slave that you want to make master.\r\n\r\n**Describe the solution you'd like**\r\nI really hope that the slave-priority or replica-priority configuration is available on Dragonfly, making it easier to do failover or of course switchover so that I can find out the next master of the selected slave\r\n\r\n**Describe alternatives you've considered**\r\nConfiguration slave-priority / replica-priority like redis\r\n\r\n**Additional context**\r\n![gambar](https://github.com/dragonflydb/dragonfly/assets/74635658/70811b46-85ee-42c6-b351-6369f9bdce66)\r\n\r\n\r\nThanks you\r\n\r\nRegards\r\nLevi khulaifi\r\n\n",
  "hints_text": "",
  "created_at": "2024-06-21T10:27:54Z",
  "modified_files": [
    "src/server/replica.cc",
    "src/server/server_family.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/sentinel_test.py"
  ]
}