{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4874,
  "instance_id": "dragonflydb__dragonfly-4874",
  "issue_numbers": [
    "4828"
  ],
  "base_commit": "be11fa02cd0721aab2a3cbf7d23fb84f1f51d432",
  "patch": "diff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex 3299c1e28f06..27c321539bdd 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -793,6 +793,7 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n   config_registry.RegisterMutable(\"table_growth_margin\");\n   config_registry.RegisterMutable(\"tcp_keepalive\");\n   config_registry.RegisterMutable(\"timeout\");\n+  config_registry.RegisterMutable(\"send_timeout\");\n   config_registry.RegisterMutable(\"managed_service_info\");\n \n   config_registry.RegisterMutable(\ndiff --git a/src/server/server_state.cc b/src/server/server_state.cc\nindex 433bf98a0c65..8fdbabfc23b9 100644\n--- a/src/server/server_state.cc\n+++ b/src/server/server_state.cc\n@@ -23,6 +23,8 @@ extern \"C\" {\n ABSL_FLAG(uint32_t, interpreter_per_thread, 10, \"Lua interpreters per thread\");\n ABSL_FLAG(uint32_t, timeout, 0,\n           \"Close the connection after it is idle for N seconds (0 to disable)\");\n+ABSL_FLAG(uint32_t, send_timeout, 0,\n+          \"Close the connection after it is stuck on send for N seconds (0 to disable)\");\n \n namespace dfly {\n \n@@ -239,7 +241,8 @@ void ServerState::ConnectionsWatcherFb(util::ListenerInterface* main) {\n     }\n \n     uint32_t timeout = absl::GetFlag(FLAGS_timeout);\n-    if (timeout == 0) {\n+    uint32_t send_timeout = absl::GetFlag(FLAGS_send_timeout);\n+    if (timeout == 0 && send_timeout == 0) {\n       continue;\n     }\n \n@@ -262,8 +265,11 @@ void ServerState::ConnectionsWatcherFb(util::ListenerInterface* main) {\n         is_replica = dfly_conn->cntx()->replica_conn;\n       }\n \n-      if ((phase == Phase::READ_SOCKET || dfly_conn->IsSending()) && !is_replica &&\n-          dfly_conn->idle_time() > timeout) {\n+      bool idle_read = timeout != 0 && !is_replica && phase == Phase::READ_SOCKET &&\n+                       dfly_conn->idle_time() > timeout;\n+      bool stuck_sending = send_timeout != 0 && !is_replica && dfly_conn->IsSending() &&\n+                           dfly_conn->idle_time() > send_timeout;\n+      if (idle_read || stuck_sending) {\n         conn_refs.push_back(dfly_conn->Borrow());\n       }\n     };\n",
  "test_patch": "diff --git a/tests/dragonfly/connection_test.py b/tests/dragonfly/connection_test.py\nindex a7b48b7bf7ea..7c8c1fb7d048 100755\n--- a/tests/dragonfly/connection_test.py\n+++ b/tests/dragonfly/connection_test.py\n@@ -1059,6 +1059,13 @@ async def test_hiredis(df_factory):\n     client.ping()\n \n \n+@assert_eventually()\n+async def wait_for_conn_drop(async_client):\n+    clients = await async_client.client_list()\n+    logging.info(\"wait_for_conn_drop clients: %s\", clients)\n+    assert len(clients) <= 1\n+\n+\n @dfly_args({\"timeout\": 1})\n async def test_timeout(df_server: DflyInstance, async_client: aioredis.Redis):\n     another_client = df_server.client()\n@@ -1068,15 +1075,46 @@ async def test_timeout(df_server: DflyInstance, async_client: aioredis.Redis):\n \n     await asyncio.sleep(2)\n \n-    @assert_eventually\n-    async def wait_for_conn_drop():\n+    await wait_for_conn_drop(async_client)\n+    info = await async_client.info(\"clients\")\n+    assert int(info[\"timeout_disconnects\"]) >= 1\n+\n+\n+@dfly_args({\"send_timeout\": 3})\n+async def test_send_timeout(df_server, async_client: aioredis.Redis):\n+    reader, writer = await asyncio.open_connection(\"127.0.0.1\", df_server.port)\n+    writer.write(f\"client setname writer_test\\n\".encode())\n+    await writer.drain()\n+    assert \"OK\" in (await reader.readline()).decode()\n+    clients = await async_client.client_list()\n+    assert len(clients) == 2\n+    size = 1024 * 1024\n+    writer.write(f\"SET a {'v'*size}\\n\".encode())\n+    await writer.drain()\n+\n+    async def get_task():\n+        while True:\n+            writer.write(f\"GET a\\n\".encode())\n+            await writer.drain()\n+            await asyncio.sleep(0.1)\n+\n+    get = asyncio.create_task(get_task())\n+\n+    @assert_eventually(times=600)\n+    async def wait_for_stuck_on_send():\n         clients = await async_client.client_list()\n-        logging.info(\"clients: %s\", clients)\n-        assert len(clients) <= 1\n+        logging.info(\"wait_for_stuck_on_send clients: %s\", clients)\n+        phase = next(\n+            (client[\"phase\"] for client in clients if client[\"name\"] == \"writer_test\"), None\n+        )\n+        assert phase == \"send\"\n \n-    await wait_for_conn_drop()\n+    await wait_for_stuck_on_send()\n+    await wait_for_conn_drop(async_client)\n     info = await async_client.info(\"clients\")\n     assert int(info[\"timeout_disconnects\"]) >= 1\n+    logging.info(\"finished disconnect\")\n+    get.cancel()\n \n \n # Test that the cache pipeline does not grow or shrink under constant pipeline load.\n",
  "problem_statement": "separate timeout and send_timeout parameters\nCurrently our timeout argument in server_state.cc refers to both the send and receive states see (ServerState::ConnectionsWatcherFb).\n\nIt's problematic because sometimes a connection can deadlock on sending huge pipelines and then dragonfly can be stuck on send.\nOften we see that due to client library bugs, the connections can leak on the client side - they do not close them but the code that handles them throws, meaning they stay indefinitely. We could break them with `timeout` but usually `timeout` is order of 10min+ which is too long. I suggest to introduce `send_timeout` flag (should also be registered as mutable config) that if set would break connections stuck at send after shorter period of time. \n",
  "hints_text": "",
  "created_at": "2025-04-01T12:25:34Z",
  "modified_files": [
    "src/server/main_service.cc",
    "src/server/server_state.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/connection_test.py"
  ]
}