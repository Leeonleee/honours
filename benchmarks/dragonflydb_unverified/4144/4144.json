{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4144,
  "instance_id": "dragonflydb__dragonfly-4144",
  "issue_numbers": [
    "4100"
  ],
  "base_commit": "581cfbf6c5f6d3412497efd37a2c69511c7003b2",
  "patch": "diff --git a/src/server/CMakeLists.txt b/src/server/CMakeLists.txt\nindex 670077b4c8d7..96c1a543793b 100644\n--- a/src/server/CMakeLists.txt\n+++ b/src/server/CMakeLists.txt\n@@ -28,7 +28,7 @@ endif()\n \n add_library(dfly_transaction db_slice.cc blocking_controller.cc\n             command_registry.cc  cluster/cluster_utility.cc\n-            journal/tx_executor.cc namespaces.cc\n+            journal/cmd_serializer.cc journal/tx_executor.cc namespaces.cc\n             common.cc journal/journal.cc journal/types.cc journal/journal_slice.cc\n             server_state.cc table.cc  top_keys.cc transaction.cc tx_base.cc\n             serializer_commons.cc journal/serializer.cc journal/executor.cc journal/streamer.cc\ndiff --git a/src/server/container_utils.cc b/src/server/container_utils.cc\nindex beb540cc3980..07c5e51ba55d 100644\n--- a/src/server/container_utils.cc\n+++ b/src/server/container_utils.cc\n@@ -270,6 +270,36 @@ bool IterateSortedSet(const detail::RobjWrapper* robj_wrapper, const IterateSort\n   return false;\n }\n \n+bool IterateMap(const PrimeValue& pv, const IterateKVFunc& func) {\n+  bool finished = true;\n+\n+  if (pv.Encoding() == kEncodingListPack) {\n+    uint8_t intbuf[LP_INTBUF_SIZE];\n+    uint8_t* lp = (uint8_t*)pv.RObjPtr();\n+    uint8_t* fptr = lpFirst(lp);\n+    while (fptr) {\n+      string_view key = LpGetView(fptr, intbuf);\n+      fptr = lpNext(lp, fptr);\n+      string_view val = LpGetView(fptr, intbuf);\n+      fptr = lpNext(lp, fptr);\n+      if (!func(ContainerEntry{key.data(), key.size()}, ContainerEntry{val.data(), val.size()})) {\n+        finished = false;\n+        break;\n+      }\n+    }\n+  } else {\n+    StringMap* sm = static_cast<StringMap*>(pv.RObjPtr());\n+    for (const auto& k_v : *sm) {\n+      if (!func(ContainerEntry{k_v.first, sdslen(k_v.first)},\n+                ContainerEntry{k_v.second, sdslen(k_v.second)})) {\n+        finished = false;\n+        break;\n+      }\n+    }\n+  }\n+  return finished;\n+}\n+\n StringMap* GetStringMap(const PrimeValue& pv, const DbContext& db_context) {\n   DCHECK_EQ(pv.Encoding(), kEncodingStrMap2);\n   StringMap* res = static_cast<StringMap*>(pv.RObjPtr());\ndiff --git a/src/server/container_utils.h b/src/server/container_utils.h\nindex d2933766276e..e9f6ef44a558 100644\n--- a/src/server/container_utils.h\n+++ b/src/server/container_utils.h\n@@ -54,6 +54,7 @@ struct ContainerEntry {\n \n using IterateFunc = std::function<bool(ContainerEntry)>;\n using IterateSortedFunc = std::function<bool(ContainerEntry, double)>;\n+using IterateKVFunc = std::function<bool(ContainerEntry, ContainerEntry)>;\n \n // Iterate over all values and call func(val). Iteration stops as soon\n // as func return false. Returns true if it successfully processed all elements\n@@ -72,6 +73,8 @@ bool IterateSortedSet(const detail::RobjWrapper* robj_wrapper, const IterateSort\n                       int32_t start = 0, int32_t end = -1, bool reverse = false,\n                       bool use_score = false);\n \n+bool IterateMap(const PrimeValue& pv, const IterateKVFunc& func);\n+\n // Get StringMap pointer from primetable value. Sets expire time from db_context\n StringMap* GetStringMap(const PrimeValue& pv, const DbContext& db_context);\n \ndiff --git a/src/server/journal/cmd_serializer.cc b/src/server/journal/cmd_serializer.cc\nnew file mode 100644\nindex 000000000000..18674e4f4632\n--- /dev/null\n+++ b/src/server/journal/cmd_serializer.cc\n@@ -0,0 +1,206 @@\n+// Copyright 2024, DragonflyDB authors.  All rights reserved.\n+// See LICENSE for licensing terms.\n+//\n+\n+#include \"server/journal/cmd_serializer.h\"\n+\n+#include \"server/container_utils.h\"\n+#include \"server/journal/serializer.h\"\n+#include \"server/rdb_save.h\"\n+\n+namespace dfly {\n+\n+namespace {\n+using namespace std;\n+\n+class CommandAggregator {\n+ public:\n+  using WriteCmdCallback = std::function<void(absl::Span<const string_view>)>;\n+\n+  CommandAggregator(string_view key, WriteCmdCallback cb) : key_(key), cb_(cb) {\n+  }\n+\n+  ~CommandAggregator() {\n+    CommitPending();\n+  }\n+\n+  enum class CommitMode { kAuto, kNoCommit };\n+  void AddArg(string arg, CommitMode commit_mode = CommitMode::kAuto) {\n+    agg_bytes_ += arg.size();\n+    members_.push_back(std::move(arg));\n+\n+    if (commit_mode != CommitMode::kNoCommit && agg_bytes_ >= serialization_max_chunk_size) {\n+      CommitPending();\n+    }\n+  }\n+\n+ private:\n+  void CommitPending() {\n+    if (members_.empty()) {\n+      return;\n+    }\n+\n+    args_.clear();\n+    args_.reserve(members_.size() + 1);\n+    args_.push_back(key_);\n+    for (string_view member : members_) {\n+      args_.push_back(member);\n+    }\n+    cb_(args_);\n+    members_.clear();\n+  }\n+\n+  string_view key_;\n+  WriteCmdCallback cb_;\n+  vector<string> members_;\n+  absl::InlinedVector<string_view, 5> args_;\n+  size_t agg_bytes_ = 0;\n+};\n+\n+}  // namespace\n+\n+CmdSerializer::CmdSerializer(FlushSerialized cb) : cb_(std::move(cb)) {\n+}\n+\n+void CmdSerializer::SerializeEntry(string_view key, const PrimeValue& pk, const PrimeValue& pv,\n+                                   uint64_t expire_ms) {\n+  // We send RESTORE commands for small objects, or objects we don't support breaking.\n+  bool use_restore_serialization = true;\n+  if (serialization_max_chunk_size > 0 && pv.MallocUsed() > serialization_max_chunk_size) {\n+    switch (pv.ObjType()) {\n+      case OBJ_SET:\n+        SerializeSet(key, pv);\n+        use_restore_serialization = false;\n+        break;\n+      case OBJ_ZSET:\n+        SerializeZSet(key, pv);\n+        use_restore_serialization = false;\n+        break;\n+      case OBJ_HASH:\n+        SerializeHash(key, pv);\n+        use_restore_serialization = false;\n+        break;\n+      case OBJ_LIST:\n+        SerializeList(key, pv);\n+        use_restore_serialization = false;\n+        break;\n+      case OBJ_STRING:\n+      case OBJ_STREAM:\n+      case OBJ_JSON:\n+      case OBJ_SBF:\n+      default:\n+        // These types are unsupported wrt splitting huge values to multiple commands, so we send\n+        // them as a RESTORE command.\n+        break;\n+    }\n+  }\n+\n+  if (use_restore_serialization) {\n+    // RESTORE sets STICK and EXPIRE as part of the command.\n+    SerializeRestore(key, pk, pv, expire_ms);\n+  } else {\n+    SerializeStickIfNeeded(key, pk);\n+    SerializeExpireIfNeeded(key, expire_ms);\n+  }\n+}\n+\n+void CmdSerializer::SerializeCommand(string_view cmd, absl::Span<const string_view> args) {\n+  journal::Entry entry(0,                     // txid\n+                       journal::Op::COMMAND,  // single command\n+                       0,                     // db index\n+                       1,                     // shard count\n+                       0,                     // slot-id, but it is ignored at this level\n+                       journal::Entry::Payload(cmd, ArgSlice(args)));\n+\n+  // Serialize into a string\n+  io::StringSink cmd_sink;\n+  JournalWriter writer{&cmd_sink};\n+  writer.Write(entry);\n+\n+  cb_(std::move(cmd_sink).str());\n+}\n+\n+void CmdSerializer::SerializeStickIfNeeded(string_view key, const PrimeValue& pk) {\n+  if (!pk.IsSticky()) {\n+    return;\n+  }\n+\n+  SerializeCommand(\"STICK\", {key});\n+}\n+\n+void CmdSerializer::SerializeExpireIfNeeded(string_view key, uint64_t expire_ms) {\n+  if (expire_ms == 0) {\n+    return;\n+  }\n+\n+  SerializeCommand(\"PEXIRE\", {key, absl::StrCat(expire_ms)});\n+}\n+\n+void CmdSerializer::SerializeSet(string_view key, const PrimeValue& pv) {\n+  CommandAggregator aggregator(\n+      key, [&](absl::Span<const string_view> args) { SerializeCommand(\"SADD\", args); });\n+\n+  container_utils::IterateSet(pv, [&](container_utils::ContainerEntry ce) {\n+    aggregator.AddArg(ce.ToString());\n+    return true;\n+  });\n+}\n+\n+void CmdSerializer::SerializeZSet(string_view key, const PrimeValue& pv) {\n+  CommandAggregator aggregator(\n+      key, [&](absl::Span<const string_view> args) { SerializeCommand(\"ZADD\", args); });\n+\n+  container_utils::IterateSortedSet(\n+      pv.GetRobjWrapper(),\n+      [&](container_utils::ContainerEntry ce, double score) {\n+        aggregator.AddArg(absl::StrCat(score), CommandAggregator::CommitMode::kNoCommit);\n+        aggregator.AddArg(ce.ToString());\n+        return true;\n+      },\n+      /*start=*/0, /*end=*/-1, /*reverse=*/false, /*use_score=*/true);\n+}\n+\n+void CmdSerializer::SerializeHash(string_view key, const PrimeValue& pv) {\n+  CommandAggregator aggregator(\n+      key, [&](absl::Span<const string_view> args) { SerializeCommand(\"HSET\", args); });\n+\n+  container_utils::IterateMap(\n+      pv, [&](container_utils::ContainerEntry k, container_utils::ContainerEntry v) {\n+        aggregator.AddArg(k.ToString(), CommandAggregator::CommitMode::kNoCommit);\n+        aggregator.AddArg(v.ToString());\n+        return true;\n+      });\n+}\n+\n+void CmdSerializer::SerializeList(string_view key, const PrimeValue& pv) {\n+  CommandAggregator aggregator(\n+      key, [&](absl::Span<const string_view> args) { SerializeCommand(\"RPUSH\", args); });\n+\n+  container_utils::IterateList(pv, [&](container_utils::ContainerEntry ce) {\n+    aggregator.AddArg(ce.ToString());\n+    return true;\n+  });\n+}\n+\n+void CmdSerializer::SerializeRestore(string_view key, const PrimeValue& pk, const PrimeValue& pv,\n+                                     uint64_t expire_ms) {\n+  absl::InlinedVector<string_view, 5> args;\n+  args.push_back(key);\n+\n+  string expire_str = absl::StrCat(expire_ms);\n+  args.push_back(expire_str);\n+\n+  io::StringSink value_dump_sink;\n+  SerializerBase::DumpObject(pv, &value_dump_sink);\n+  args.push_back(value_dump_sink.str());\n+\n+  args.push_back(\"ABSTTL\");  // Means expire string is since epoch\n+\n+  if (pk.IsSticky()) {\n+    args.push_back(\"STICK\");\n+  }\n+\n+  SerializeCommand(\"RESTORE\", args);\n+}\n+\n+}  // namespace dfly\ndiff --git a/src/server/journal/cmd_serializer.h b/src/server/journal/cmd_serializer.h\nnew file mode 100644\nindex 000000000000..14e08088a4d5\n--- /dev/null\n+++ b/src/server/journal/cmd_serializer.h\n@@ -0,0 +1,44 @@\n+// Copyright 2024, DragonflyDB authors.  All rights reserved.\n+// See LICENSE for licensing terms.\n+//\n+\n+#pragma once\n+\n+#include <absl/types/span.h>\n+\n+#include <string>\n+#include <string_view>\n+\n+#include \"server/table.h\"\n+\n+namespace dfly {\n+\n+// CmdSerializer serializes DB entries (key+value) into command(s) in RESP format string.\n+// Small entries are serialized as RESTORE commands, while bigger ones (see\n+// serialization_max_chunk_size) are split into multiple commands (like rpush, hset, etc).\n+// Expiration and stickiness are also serialized into commands.\n+class CmdSerializer {\n+ public:\n+  using FlushSerialized = std::function<void(std::string)>;\n+\n+  explicit CmdSerializer(FlushSerialized cb);\n+\n+  void SerializeEntry(std::string_view key, const PrimeValue& pk, const PrimeValue& pv,\n+                      uint64_t expire_ms);\n+\n+ private:\n+  void SerializeCommand(std::string_view cmd, absl::Span<const std::string_view> args);\n+  void SerializeStickIfNeeded(std::string_view key, const PrimeValue& pk);\n+  void SerializeExpireIfNeeded(std::string_view key, uint64_t expire_ms);\n+\n+  void SerializeSet(std::string_view key, const PrimeValue& pv);\n+  void SerializeZSet(std::string_view key, const PrimeValue& pv);\n+  void SerializeHash(std::string_view key, const PrimeValue& pv);\n+  void SerializeList(std::string_view key, const PrimeValue& pv);\n+  void SerializeRestore(std::string_view key, const PrimeValue& pk, const PrimeValue& pv,\n+                        uint64_t expire_ms);\n+\n+  FlushSerialized cb_;\n+};\n+\n+}  // namespace dfly\ndiff --git a/src/server/journal/streamer.cc b/src/server/journal/streamer.cc\nindex 6d4bb7f81af9..e8e142d49cbe 100644\n--- a/src/server/journal/streamer.cc\n+++ b/src/server/journal/streamer.cc\n@@ -9,6 +9,7 @@\n #include \"base/flags.h\"\n #include \"base/logging.h\"\n #include \"server/cluster/cluster_defs.h\"\n+#include \"server/journal/cmd_serializer.h\"\n #include \"util/fibers/synchronization.h\"\n \n using namespace facade;\n@@ -317,37 +318,8 @@ void RestoreStreamer::OnDbChange(DbIndex db_index, const DbSlice::ChangeReq& req\n \n void RestoreStreamer::WriteEntry(string_view key, const PrimeValue& pk, const PrimeValue& pv,\n                                  uint64_t expire_ms) {\n-  absl::InlinedVector<string_view, 5> args;\n-  args.push_back(key);\n-\n-  string expire_str = absl::StrCat(expire_ms);\n-  args.push_back(expire_str);\n-\n-  io::StringSink restore_cmd_sink;\n-  {  // to destroy extra copy\n-    io::StringSink value_dump_sink;\n-    SerializerBase::DumpObject(pv, &value_dump_sink);\n-    args.push_back(value_dump_sink.str());\n-\n-    args.push_back(\"ABSTTL\");  // Means expire string is since epoch\n-\n-    if (pk.IsSticky()) {\n-      args.push_back(\"STICK\");\n-    }\n-\n-    journal::Entry entry(0,                     // txid\n-                         journal::Op::COMMAND,  // single command\n-                         0,                     // db index\n-                         1,                     // shard count\n-                         0,                     // slot-id, but it is ignored at this level\n-                         journal::Entry::Payload(\"RESTORE\", ArgSlice(args)));\n-\n-    JournalWriter writer{&restore_cmd_sink};\n-    writer.Write(entry);\n-  }\n-  // TODO: From DumpObject to till Write we tripple copy the PrimeValue. It's very inefficient and\n-  // will burn CPU for large values.\n-  Write(restore_cmd_sink.str());\n+  CmdSerializer serializer([&](std::string s) { Write(s); });\n+  serializer.SerializeEntry(key, pk, pv, expire_ms);\n }\n \n }  // namespace dfly\ndiff --git a/src/server/journal/streamer.h b/src/server/journal/streamer.h\nindex c625b60c5157..a5ef1ad978a2 100644\n--- a/src/server/journal/streamer.h\n+++ b/src/server/journal/streamer.h\n@@ -98,7 +98,8 @@ class RestoreStreamer : public JournalStreamer {\n \n   // Returns whether anything was written\n   void WriteBucket(PrimeTable::bucket_iterator it);\n-  void WriteEntry(string_view key, const PrimeValue& pk, const PrimeValue& pv, uint64_t expire_ms);\n+  void WriteEntry(std::string_view key, const PrimeValue& pk, const PrimeValue& pv,\n+                  uint64_t expire_ms);\n \n   DbSlice* db_slice_;\n   DbTableArray db_array_;\n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex 83acd11f71b3..1fb39dff4cc7 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -1294,10 +1294,14 @@ async def test_network_disconnect_during_migration(df_factory, df_seeder_factory\n \n \n @pytest.mark.parametrize(\n-    \"node_count, segments, keys\",\n+    \"node_count, segments, keys, huge_values\",\n     [\n-        pytest.param(3, 16, 20_000),\n-        pytest.param(5, 20, 30_000, marks=[pytest.mark.slow, pytest.mark.opt_only]),\n+        pytest.param(3, 16, 20_000, 10),\n+        # 1mb effectively disables breakdown of huge values.\n+        # TODO: add a test that mixes huge and small values, see\n+        # https://github.com/dragonflydb/dragonfly/pull/4144/files/11e5e387d31bcf1bc53dfbb28cf3bcaf094d77fa#r1850130930\n+        pytest.param(3, 16, 20_000, 1_000_000),\n+        pytest.param(5, 20, 30_000, 1_000_000, marks=[pytest.mark.slow, pytest.mark.opt_only]),\n     ],\n )\n @dfly_args({\"proactor_threads\": 4, \"cluster_mode\": \"yes\"})\n@@ -1307,12 +1311,15 @@ async def test_cluster_fuzzymigration(\n     node_count: int,\n     segments: int,\n     keys: int,\n+    huge_values: int,\n ):\n     instances = [\n         df_factory.create(\n             port=BASE_PORT + i,\n             admin_port=BASE_PORT + i + 1000,\n             vmodule=\"outgoing_slot_migration=9,cluster_family=9,incoming_slot_migration=9\",\n+            serialization_max_chunk_size=huge_values,\n+            replication_stream_output_limit=10,\n         )\n         for i in range(node_count)\n     ]\n",
  "problem_statement": "Big value serialization in migration\n\n",
  "hints_text": "",
  "created_at": "2024-11-18T13:53:42Z",
  "modified_files": [
    "src/server/CMakeLists.txt",
    "src/server/container_utils.cc",
    "src/server/container_utils.h",
    "b/src/server/journal/cmd_serializer.cc",
    "b/src/server/journal/cmd_serializer.h",
    "src/server/journal/streamer.cc",
    "src/server/journal/streamer.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py"
  ]
}