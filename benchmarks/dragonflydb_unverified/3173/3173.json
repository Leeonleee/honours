{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 3173,
  "instance_id": "dragonflydb__dragonfly-3173",
  "issue_numbers": [
    "3132"
  ],
  "base_commit": "6291c0401676205f0678b4f9afc19d0e795d9971",
  "patch": "diff --git a/src/server/cluster/cluster_family.cc b/src/server/cluster/cluster_family.cc\nindex a69525c81df3..b50765160ffd 100644\n--- a/src/server/cluster/cluster_family.cc\n+++ b/src/server/cluster/cluster_family.cc\n@@ -791,15 +791,11 @@ bool RemoveIncomingMigrationImpl(std::vector<std::shared_ptr<IncomingSlotMigrati\n \n   // TODO make it outside in one run with other slots that should be flushed\n   if (!removed.Empty()) {\n-    auto removed_ranges = make_shared<SlotRanges>(removed.ToSlotRanges());\n+    auto removed_ranges = removed.ToSlotRanges();\n     LOG_IF(WARNING, migration->GetState() == MigrationState::C_FINISHED)\n         << \"Flushing slots of removed FINISHED migration \" << migration->GetSourceID()\n-        << \", slots: \" << SlotRange::ToString(*removed_ranges);\n-    shard_set->pool()->DispatchOnAll([removed_ranges](unsigned, ProactorBase*) {\n-      if (EngineShard* shard = EngineShard::tlocal(); shard) {\n-        shard->db_slice().FlushSlots(*removed_ranges);\n-      }\n-    });\n+        << \", slots: \" << SlotRange::ToString(removed_ranges);\n+    DeleteSlots(removed_ranges);\n   }\n \n   return true;\n@@ -844,7 +840,7 @@ void ClusterFamily::InitMigration(CmdArgList args, ConnectionContext* cntx) {\n \n   lock_guard lk(migration_mu_);\n   auto was_removed = RemoveIncomingMigrationImpl(incoming_migrations_jobs_, source_id);\n-  LOG_IF(WARNING, was_removed) << \"Reinit was happen for migration from:\" << source_id;\n+  LOG_IF(WARNING, was_removed) << \"Reinit issued for migration from:\" << source_id;\n \n   incoming_migrations_jobs_.emplace_back(make_shared<IncomingSlotMigration>(\n       std::move(source_id), &server_family_->service(), std::move(slots), flows_num));\ndiff --git a/src/server/cluster/outgoing_slot_migration.cc b/src/server/cluster/outgoing_slot_migration.cc\nindex 7261138a67d7..3920f6977905 100644\n--- a/src/server/cluster/outgoing_slot_migration.cc\n+++ b/src/server/cluster/outgoing_slot_migration.cc\n@@ -58,17 +58,11 @@ class OutgoingMigration::SliceSlotMigration : private ProtocolClient {\n       return;\n     }\n \n-    // Check if migration was cancelled while we yielded so far.\n-    if (cancelled_) {\n-      return;\n-    }\n-\n     streamer_.Start(Sock());\n   }\n \n   void Cancel() {\n     streamer_.Cancel();\n-    cancelled_ = true;\n   }\n \n   void Finalize() {\n@@ -81,7 +75,6 @@ class OutgoingMigration::SliceSlotMigration : private ProtocolClient {\n \n  private:\n   RestoreStreamer streamer_;\n-  bool cancelled_ = false;\n };\n \n OutgoingMigration::OutgoingMigration(MigrationInfo info, ClusterFamily* cf, ServerFamily* sf)\n@@ -94,6 +87,13 @@ OutgoingMigration::OutgoingMigration(MigrationInfo info, ClusterFamily* cf, Serv\n \n OutgoingMigration::~OutgoingMigration() {\n   main_sync_fb_.JoinIfNeeded();\n+\n+  // Destroy each flow in its dedicated thread, because we could be the last owner of the db tables\n+  shard_set->pool()->AwaitFiberOnAll([this](util::ProactorBase* pb) {\n+    if (const auto* shard = EngineShard::tlocal(); shard) {\n+      slot_migrations_[shard->shard_id()].reset();\n+    }\n+  });\n }\n \n bool OutgoingMigration::ChangeState(MigrationState new_state) {\ndiff --git a/src/server/journal/journal_slice.cc b/src/server/journal/journal_slice.cc\nindex f7c068f4613b..e916a09a6c95 100644\n--- a/src/server/journal/journal_slice.cc\n+++ b/src/server/journal/journal_slice.cc\n@@ -165,6 +165,7 @@ void JournalSlice::AddLogRecord(const Entry& entry, bool await) {\n     item = &dummy;\n     item->opcode = entry.opcode;\n     item->lsn = lsn_++;\n+    item->cmd = entry.payload.cmd;\n     item->slot = entry.slot;\n \n     io::BufSink buf_sink{&ring_serialize_buf_};\ndiff --git a/src/server/journal/streamer.cc b/src/server/journal/streamer.cc\nindex c00c2ae877af..a3f42b9ab505 100644\n--- a/src/server/journal/streamer.cc\n+++ b/src/server/journal/streamer.cc\n@@ -34,7 +34,7 @@ uint32_t replication_stream_output_limit_cached = 64_KB;\n }  // namespace\n \n JournalStreamer::JournalStreamer(journal::Journal* journal, Context* cntx)\n-    : journal_(journal), cntx_(cntx) {\n+    : cntx_(cntx), journal_(journal) {\n   // cache the flag to avoid accessing it later.\n   replication_stream_output_limit_cached = absl::GetFlag(FLAGS_replication_stream_output_limit);\n }\n@@ -44,7 +44,7 @@ JournalStreamer::~JournalStreamer() {\n   VLOG(1) << \"~JournalStreamer\";\n }\n \n-void JournalStreamer::Start(io::AsyncSink* dest, bool send_lsn) {\n+void JournalStreamer::Start(util::FiberSocketBase* dest, bool send_lsn) {\n   CHECK(dest_ == nullptr && dest != nullptr);\n   dest_ = dest;\n   journal_cb_id_ =\n@@ -188,9 +188,13 @@ RestoreStreamer::RestoreStreamer(DbSlice* slice, cluster::SlotSet slots, journal\n                                  Context* cntx)\n     : JournalStreamer(journal, cntx), db_slice_(slice), my_slots_(std::move(slots)) {\n   DCHECK(slice != nullptr);\n+  db_array_ = slice->databases();  // Inc ref to make sure DB isn't deleted while we use it\n }\n \n-void RestoreStreamer::Start(io::AsyncSink* dest, bool send_lsn) {\n+void RestoreStreamer::Start(util::FiberSocketBase* dest, bool send_lsn) {\n+  if (fiber_cancelled_)\n+    return;\n+\n   VLOG(1) << \"RestoreStreamer start\";\n   auto db_cb = absl::bind_front(&RestoreStreamer::OnDbChange, this);\n   snapshot_version_ = db_slice_->RegisterOnChange(std::move(db_cb));\n@@ -199,7 +203,7 @@ void RestoreStreamer::Start(io::AsyncSink* dest, bool send_lsn) {\n \n   PrimeTable::Cursor cursor;\n   uint64_t last_yield = 0;\n-  PrimeTable* pt = &db_slice_->databases()[0]->prime;\n+  PrimeTable* pt = &db_array_[0]->prime;\n \n   do {\n     if (fiber_cancelled_)\n@@ -244,14 +248,22 @@ RestoreStreamer::~RestoreStreamer() {\n void RestoreStreamer::Cancel() {\n   auto sver = snapshot_version_;\n   snapshot_version_ = 0;  // to prevent double cancel in another fiber\n+  fiber_cancelled_ = true;\n   if (sver != 0) {\n-    fiber_cancelled_ = true;\n     db_slice_->UnregisterOnChange(sver);\n     JournalStreamer::Cancel();\n   }\n }\n \n bool RestoreStreamer::ShouldWrite(const journal::JournalItem& item) const {\n+  if (item.cmd == \"FLUSHALL\" || item.cmd == \"FLUSHDB\") {\n+    // On FLUSH* we restart the migration\n+    CHECK(dest_ != nullptr);\n+    cntx_->ReportError(\"FLUSH command during migration\");\n+    dest_->Shutdown(SHUT_RDWR);\n+    return false;\n+  }\n+\n   if (!item.slot.has_value()) {\n     return false;\n   }\ndiff --git a/src/server/journal/streamer.h b/src/server/journal/streamer.h\nindex 7cb8b34bf5d4..aa61f24fe0b2 100644\n--- a/src/server/journal/streamer.h\n+++ b/src/server/journal/streamer.h\n@@ -23,7 +23,7 @@ class JournalStreamer {\n   JournalStreamer(JournalStreamer&& other) = delete;\n \n   // Register journal listener and start writer in fiber.\n-  virtual void Start(io::AsyncSink* dest, bool send_lsn);\n+  virtual void Start(util::FiberSocketBase* dest, bool send_lsn);\n \n   // Must be called on context cancellation for unblocking\n   // and manual cleanup.\n@@ -48,6 +48,9 @@ class JournalStreamer {\n \n   void WaitForInflightToComplete();\n \n+  util::FiberSocketBase* dest_ = nullptr;\n+  Context* cntx_;\n+\n  private:\n   void OnCompletion(std::error_code ec, size_t len);\n \n@@ -58,8 +61,6 @@ class JournalStreamer {\n   bool IsStalled() const;\n \n   journal::Journal* journal_;\n-  Context* cntx_;\n-  io::AsyncSink* dest_ = nullptr;\n   std::vector<uint8_t> pending_buf_;\n   size_t in_flight_bytes_ = 0;\n   time_t last_lsn_time_ = 0;\n@@ -74,7 +75,7 @@ class RestoreStreamer : public JournalStreamer {\n   RestoreStreamer(DbSlice* slice, cluster::SlotSet slots, journal::Journal* journal, Context* cntx);\n   ~RestoreStreamer() override;\n \n-  void Start(io::AsyncSink* dest, bool send_lsn = false) override;\n+  void Start(util::FiberSocketBase* dest, bool send_lsn = false) override;\n   // Cancel() must be called if Start() is called\n   void Cancel() override;\n \n@@ -96,6 +97,7 @@ class RestoreStreamer : public JournalStreamer {\n   void WriteCommand(journal::Entry::Payload cmd_payload);\n \n   DbSlice* db_slice_;\n+  DbTableArray db_array_;\n   uint64_t snapshot_version_ = 0;\n   cluster::SlotSet my_slots_;\n   bool fiber_cancelled_ = false;\ndiff --git a/src/server/journal/types.h b/src/server/journal/types.h\nindex aeb0286ca65f..63c35b9befc9 100644\n--- a/src/server/journal/types.h\n+++ b/src/server/journal/types.h\n@@ -95,6 +95,7 @@ struct JournalItem {\n   LSN lsn;\n   Op opcode;\n   std::string data;\n+  std::string_view cmd;\n   std::optional<cluster::SlotId> slot;\n };\n \n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex aa2f65fc8f7f..c5bfa738ffb0 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -19,6 +19,16 @@\n BASE_PORT = 30001\n \n \n+async def assert_eventually(e):\n+    iterations = 0\n+    while True:\n+        if await e():\n+            return\n+        iterations += 1\n+        assert iterations < 500\n+        await asyncio.sleep(0.1)\n+\n+\n class RedisClusterNode:\n     def __init__(self, port):\n         self.port = port\n@@ -1026,6 +1036,59 @@ async def test_config_consistency(df_local_factory: DflyInstanceFactory):\n     await close_clients(*[node.client for node in nodes], *[node.admin_client for node in nodes])\n \n \n+@dfly_args({\"proactor_threads\": 4, \"cluster_mode\": \"yes\"})\n+async def test_cluster_flushall_during_migration(\n+    df_local_factory: DflyInstanceFactory, df_seeder_factory\n+):\n+    # Check data migration from one node to another\n+    instances = [\n+        df_local_factory.create(\n+            port=BASE_PORT + i,\n+            admin_port=BASE_PORT + i + 1000,\n+            vmodule=\"cluster_family=9,cluster_slot_migration=9,outgoing_slot_migration=9\",\n+            logtostdout=True,\n+        )\n+        for i in range(2)\n+    ]\n+\n+    df_local_factory.start_all(instances)\n+\n+    nodes = [(await create_node_info(instance)) for instance in instances]\n+    nodes[0].slots = [(0, 16383)]\n+    nodes[1].slots = []\n+\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    seeder = df_seeder_factory.create(keys=10_000, port=nodes[0].instance.port, cluster_mode=True)\n+    await seeder.run(target_deviation=0.1)\n+\n+    nodes[0].migrations.append(\n+        MigrationInfo(\"127.0.0.1\", nodes[1].instance.admin_port, [(0, 16383)], nodes[1].id)\n+    )\n+\n+    logging.debug(\"Start migration\")\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    await nodes[0].client.execute_command(\"flushall\")\n+\n+    assert \"FINISHED\" not in await nodes[1].admin_client.execute_command(\n+        \"DFLYCLUSTER\", \"SLOT-MIGRATION-STATUS\", nodes[0].id\n+    ), \"Weak test case - finished migration too early\"\n+\n+    await wait_for_status(nodes[0].admin_client, nodes[1].id, \"FINISHED\")\n+\n+    logging.debug(\"Finalizing migration\")\n+    nodes[0].migrations = []\n+    nodes[0].slots = []\n+    nodes[1].slots = [(0, 16383)]\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+    logging.debug(\"Migration finalized\")\n+\n+    assert await nodes[0].client.dbsize() == 0\n+\n+    await close_clients(*[node.client for node in nodes], *[node.admin_client for node in nodes])\n+\n+\n @dfly_args({\"proactor_threads\": 4, \"cluster_mode\": \"yes\"})\n async def test_cluster_data_migration(df_local_factory: DflyInstanceFactory):\n     # Check data migration from one node to another\n@@ -1065,12 +1128,12 @@ async def test_cluster_data_migration(df_local_factory: DflyInstanceFactory):\n         await nodes[0].admin_client.execute_command(\n             \"DFLYCLUSTER\", \"SLOT-MIGRATION-STATUS\", nodes[1].id\n         )\n-    ).startswith(f\"\"\"out {nodes[1].id} FINISHED keys:7\"\"\")\n+    ).startswith(f\"out {nodes[1].id} FINISHED keys:7\")\n     assert (\n         await nodes[1].admin_client.execute_command(\n             \"DFLYCLUSTER\", \"SLOT-MIGRATION-STATUS\", nodes[0].id\n         )\n-    ).startswith(f\"\"\"in {nodes[0].id} FINISHED keys:7\"\"\")\n+    ).startswith(f\"in {nodes[0].id} FINISHED keys:7\")\n \n     nodes[0].migrations = []\n     nodes[0].slots = [(0, 2999)]\n@@ -1232,23 +1295,15 @@ async def list_counter(key, client: aioredis.RedisCluster):\n     logging.debug(\"start migrations\")\n     await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n \n-    iterations = 0\n-    while True:\n-        is_all_finished = True\n+    async def all_finished():\n         for node in nodes:\n             states = await node.admin_client.execute_command(\"DFLYCLUSTER\", \"SLOT-MIGRATION-STATUS\")\n             logging.debug(states)\n-            is_all_finished = is_all_finished and (\n-                all(\"FINISHED\" in s for s in states) or states == \"NO_STATE\"\n-            )\n+            if not all(\"FINISHED\" in s for s in states) or states == \"NO_STATE\":\n+                return False\n+        return True\n \n-        if is_all_finished:\n-            break\n-\n-        iterations += 1\n-        assert iterations < 500\n-\n-        await asyncio.sleep(0.1)\n+    await assert_eventually(all_finished)\n \n     for counter in counters:\n         counter.cancel()\n@@ -1359,13 +1414,11 @@ async def test_cluster_migration_cancel(df_local_factory: DflyInstanceFactory):\n     nodes[0].migrations = []\n     await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n     assert SIZE == await nodes[0].client.dbsize()\n-    while True:\n-        db_size = await nodes[1].client.dbsize()\n-        if 0 == db_size:\n-            break\n-        logging.debug(f\"target dbsize is {db_size}\")\n-        logging.debug(await nodes[1].client.execute_command(\"KEYS\", \"*\"))\n-        await asyncio.sleep(0.1)\n+\n+    async def node1size0():\n+        return await nodes[1].client.dbsize() == 0\n+\n+    await assert_eventually(node1size0)\n \n     logging.debug(\"Reissuing migration\")\n     nodes[0].migrations.append(\n",
  "problem_statement": "FLUSHALL during slot migration causes assert failure\n```cpp\nauto DashTable<_Key, _Value, Policy>::Traverse(Cursor curs, Cb&& cb) -> Cursor {\n  ...\n\n  // We fix bid and go over all segments. Once we reach the end we increase bid and repeat.\n  do {\n    SegmentType* s = segment_[sid];\n    assert(s);\n```\n\nOpen questions:\n1. Should we forbid running FLUSHALL when a migration takes place?\n2. Could this `assert` happen also in other flows / scenarios?\n",
  "hints_text": "@dranikpg  thinks that maybe the dbtable is dropped and you're referencing a deleted pointer? snapshot copies it explicitly (the intrusive poiner)",
  "created_at": "2024-06-13T12:29:45Z",
  "modified_files": [
    "src/server/cluster/cluster_family.cc",
    "src/server/cluster/outgoing_slot_migration.cc",
    "src/server/journal/journal_slice.cc",
    "src/server/journal/streamer.cc",
    "src/server/journal/streamer.h",
    "src/server/journal/types.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py"
  ]
}