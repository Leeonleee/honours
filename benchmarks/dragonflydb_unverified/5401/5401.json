{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 5401,
  "instance_id": "dragonflydb__dragonfly-5401",
  "issue_numbers": [
    "5394"
  ],
  "base_commit": "8a5fda06233e1638233c934f0ae28a540e7bd732",
  "patch": "diff --git a/src/server/rdb_save.cc b/src/server/rdb_save.cc\nindex 0536f415ac11..427d7ceb9be6 100644\n--- a/src/server/rdb_save.cc\n+++ b/src/server/rdb_save.cc\n@@ -309,6 +309,10 @@ io::Result<uint8_t> RdbSerializer::SaveEntry(const PrimeKey& pk, const PrimeValu\n     return make_unexpected(ec);\n   }\n \n+  // We flush here because if the next element in the bucket we are serializing is a container,\n+  // it will first serialize the first entry and then flush the internal buffer, even if\n+  // crossed the limit.\n+  FlushIfNeeded(FlushState::kFlushEndEntry);\n   return rdb_type;\n }\n \n@@ -776,6 +780,10 @@ error_code SerializerBase::FlushToSink(io::Sink* sink, SerializerBase::FlushStat\n   if (bytes.empty())\n     return error_code{};\n \n+  if (bytes.size() > serialization_peak_bytes_) {\n+    serialization_peak_bytes_ = bytes.size();\n+  }\n+\n   DVLOG(2) << \"FlushToSink \" << bytes.size() << \" bytes\";\n \n   // interrupt point.\ndiff --git a/src/server/rdb_save.h b/src/server/rdb_save.h\nindex a25c4a1d5d2a..fe8728e17f91 100644\n--- a/src/server/rdb_save.h\n+++ b/src/server/rdb_save.h\n@@ -183,6 +183,10 @@ class SerializerBase {\n     return SaveString(io::View(io::Bytes{buf, len}));\n   }\n \n+  uint64_t GetSerializationPeakBytes() const {\n+    return serialization_peak_bytes_;\n+  }\n+\n  protected:\n   // Prepare internal buffer for flush. Compress it.\n   io::Bytes PrepareFlush(FlushState flush_state);\n@@ -210,6 +214,8 @@ class SerializerBase {\n   base::PODArray<uint8_t> tmp_buf_;\n   std::unique_ptr<LZF_HSLOT[]> lzf_;\n   size_t number_of_chunks_ = 0;\n+\n+  uint64_t serialization_peak_bytes_ = 0;\n };\n \n class RdbSerializer : public SerializerBase {\ndiff --git a/src/server/snapshot.cc b/src/server/snapshot.cc\nindex cdaf4a205745..d24663ee7c40 100644\n--- a/src/server/snapshot.cc\n+++ b/src/server/snapshot.cc\n@@ -64,7 +64,7 @@ size_t SliceSnapshot::GetThreadLocalMemoryUsage() {\n }\n \n bool SliceSnapshot::IsSnaphotInProgress() {\n-  return tl_slice_snapshots.size() > 0;\n+  return !tl_slice_snapshots.empty();\n }\n \n void SliceSnapshot::Start(bool stream_journal, SnapshotFlush allow_flush) {\n@@ -114,6 +114,7 @@ void SliceSnapshot::Start(bool stream_journal, SnapshotFlush allow_flush) {\n       db_slice_->UnregisterOnMoved(moved_cb_id_);\n     }\n     consumer_->Finalize();\n+    VLOG(1) << \"Serialization peak bytes: \" << serializer_->GetSerializationPeakBytes();\n   });\n }\n \n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 27fa6725854f..27cccccf500f 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -3387,3 +3387,56 @@ async def get_keys():\n         moved_saved = extract_int_after_prefix(\"moved_saved \", line)\n         logging.debug(f\"Moved saves {moved_saved}\")\n         assert moved_saved > 0\n+\n+\n+@dfly_args({\"proactor_threads\": 1})\n+async def test_big_strings(df_factory):\n+    master = df_factory.create(\n+        proactor_threads=1, serialization_max_chunk_size=1, vmodule=\"snapshot=1\"\n+    )\n+    replica = df_factory.create(proactor_threads=1)\n+\n+    df_factory.start_all([master, replica])\n+    c_master = master.client()\n+    c_replica = replica.client()\n+\n+    # 500kb\n+    value_size = 500_000\n+\n+    async def get_memory(client, field):\n+        info = await client.info(\"memory\")\n+        return info[field]\n+\n+    capacity = await get_memory(c_master, \"prime_capacity\")\n+\n+    seeder = DebugPopulateSeeder(\n+        key_target=int(capacity * 0.8),\n+        data_size=value_size,\n+        collection_size=1,\n+        variance=1,\n+        samples=1,\n+        types=[\"STRING\"],\n+    )\n+    await seeder.run(c_master)\n+\n+    # sanity\n+    capacity = await get_memory(c_master, \"prime_capacity\")\n+    assert capacity < 8000\n+\n+    await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n+    await wait_for_replicas_state(c_replica)\n+\n+    # Check if replica data is consistent\n+    replica_data = await DebugPopulateSeeder.capture(c_replica)\n+    master_data = await DebugPopulateSeeder.capture(c_master)\n+    assert master_data == replica_data\n+\n+    replica.stop()\n+    master.stop()\n+\n+    lines = master.find_in_logs(\"Serialization peak bytes: \")\n+    assert len(lines) == 1\n+    # We test the serializtion path of command execution\n+    line = lines[0]\n+    peak_bytes = extract_int_after_prefix(\"Serialization peak bytes: \", line)\n+    assert peak_bytes < value_size\n",
  "problem_statement": "Big value serialization - Add Flushing Support While Iterating Bucket Entries\nCurrently, we only flush during serialization of container data types (e.g., sets, hashes, etc.).\nWe intentionally avoid flushing in the middle of a string. However, if a bucket contains only string entries, we end up not flushing at all during that bucket's serialization. This can lead to exceeding the serialization_max_chunk_size.\n\nTo address this, we should introduce flushing while iterating over individual entries within a bucket\u2014even for simple types like strings.\n",
  "hints_text": "",
  "created_at": "2025-07-02T09:02:47Z",
  "modified_files": [
    "src/server/rdb_save.cc",
    "src/server/rdb_save.h",
    "src/server/snapshot.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}