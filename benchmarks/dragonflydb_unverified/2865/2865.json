{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 2865,
  "instance_id": "dragonflydb__dragonfly-2865",
  "issue_numbers": [
    "2836"
  ],
  "base_commit": "604e9c6e971ddc90e0c9e8842cba21b35beaae36",
  "patch": "diff --git a/src/server/replica.cc b/src/server/replica.cc\nindex 967c6d391d11..6392c80f3d60 100644\n--- a/src/server/replica.cc\n+++ b/src/server/replica.cc\n@@ -359,7 +359,8 @@ error_code Replica::InitiatePSync() {\n   int64_t offs = -1;\n   if (!master_context_.master_repl_id.empty()) {  // in case we synced before\n     id = master_context_.master_repl_id;          // provide the replication offset and master id\n-    offs = repl_offs_;                            // to try incremental sync.\n+    // TBD: for incremental sync send repl_offs_, not supported yet.\n+    // offs = repl_offs_;\n   }\n \n   RETURN_ON_ERR(SendCommand(StrCat(\"PSYNC \", id, \" \", offs)));\n@@ -1136,6 +1137,11 @@ error_code Replica::ParseReplicationHeader(base::IoBuf* io_buf, PSyncResponse* d\n     // That could change due to redis failovers.\n     // TODO: part sync\n     dest->fullsync.emplace<size_t>(0);\n+    LOG(ERROR) << \"Partial replication not supported yet\";\n+    return std::make_error_code(std::errc::not_supported);\n+  } else {\n+    LOG(ERROR) << \"Unknown replication header\";\n+    return bad_header();\n   }\n \n   return error_code{};\n",
  "test_patch": "diff --git a/tests/dragonfly/redis_replication_test.py b/tests/dragonfly/redis_replication_test.py\nindex 71a40dcc5247..4305d41f899e 100644\n--- a/tests/dragonfly/redis_replication_test.py\n+++ b/tests/dragonfly/redis_replication_test.py\n@@ -5,6 +5,7 @@\n import subprocess\n from .utility import *\n from .instance import DflyInstanceFactory\n+from .proxy import Proxy\n \n \n class RedisServer:\n@@ -55,8 +56,6 @@ async def await_synced(c_master: aioredis.Redis, c_replica: aioredis.Redis, dbco\n             timeout -= 1\n         await c_master.close()\n         await c_replica.close()\n-        if timeout == 0:\n-            breakpoint()\n         assert timeout > 0, \"Timeout while waiting for replica to sync\"\n \n \n@@ -229,7 +228,7 @@ async def run_replication(c_replica):\n \n \n @pytest.mark.parametrize(\"t_replicas, t_disconnect, seeder_config\", master_disconnect_cases)\n-async def test_disconnect_master(\n+async def test_redis_master_restart(\n     df_local_factory,\n     df_seeder_factory,\n     redis_server,\n@@ -286,3 +285,74 @@ async def run_replication(c_replica):\n     await asyncio.gather(*(asyncio.create_task(wait_available_async(c)) for c in c_replicas))\n     await await_synced_all(c_master, c_replicas)\n     await check_data(seeder, replicas, c_replicas)\n+\n+\n+master_disconnect_cases = [\n+    ([6], dict(keys=4_000, dbcount=1, unsupported_types=[ValueType.JSON])),\n+    pytest.param(\n+        [1, 4, 6],\n+        dict(keys=1_000, dbcount=2, unsupported_types=[ValueType.JSON]),\n+        marks=pytest.mark.slow,\n+    ),\n+]\n+\n+\n+@pytest.mark.parametrize(\"t_replicas, seeder_config\", master_disconnect_cases)\n+async def test_disconnect_master(\n+    df_local_factory,\n+    df_seeder_factory,\n+    redis_server,\n+    t_replicas,\n+    seeder_config,\n+    port_picker,\n+):\n+    master = redis_server\n+    c_master = aioredis.Redis(port=master.port)\n+    assert await c_master.ping()\n+\n+    proxy = Proxy(\"127.0.0.1\", 1114, \"127.0.0.1\", master.port)\n+    await proxy.start()\n+    proxy_task = asyncio.create_task(proxy.serve())\n+\n+    replicas = [\n+        df_local_factory.create(port=port_picker.get_available_port(), proactor_threads=t)\n+        for i, t in enumerate(t_replicas)\n+    ]\n+\n+    # Fill master with test data\n+    seeder = df_seeder_factory.create(port=master.port, **seeder_config)\n+    await seeder.run(target_deviation=0.1)\n+\n+    # Start replicas\n+    df_local_factory.start_all(replicas)\n+\n+    c_replicas = [aioredis.Redis(port=replica.port) for replica in replicas]\n+\n+    # Start data stream\n+    stream_task = asyncio.create_task(seeder.run())\n+    await asyncio.sleep(0.5)\n+\n+    # Start replication\n+    async def run_replication(c_replica):\n+        await c_replica.execute_command(\"REPLICAOF localhost \" + str(proxy.port))\n+        await wait_available_async(c_replica)\n+\n+    await asyncio.gather(*(asyncio.create_task(run_replication(c)) for c in c_replicas))\n+\n+    # Break the connection between master and replica\n+    await proxy.close(proxy_task)\n+    await asyncio.sleep(2)\n+    await proxy.start()\n+    proxy_task = asyncio.create_task(proxy.serve())\n+\n+    # finish streaming data\n+    await asyncio.sleep(1)\n+    seeder.stop()\n+    await stream_task\n+\n+    # Check data after stable state stream\n+    await asyncio.gather(*(asyncio.create_task(wait_available_async(c)) for c in c_replicas))\n+    await await_synced_all(c_master, c_replicas)\n+    await check_data(seeder, replicas, c_replicas)\n+\n+    await proxy.close(proxy_task)\n",
  "problem_statement": "valkey -> dragonfly replication bugs\nUsability - when using disk-based replication from Valkey, we print \"Starting full sync with Redis master` - could be nice to print the snapshot size as well.\n",
  "hints_text": "another problem: introduce a setup: `DF(M) -> DF(S)`\r\n\r\nThen, start a valkey master with data on `host/port`. Then run on `DF(S)`: `replicaof host port` to replicate from the valkey master.\r\nExpected result - `DF(S)` becomes a replica of valkey server. Actual result, DF is totally confused and still uses DF(M)\nif a DF replica switches to stable sync even though it had full sync errors on its way:\r\n```\r\nE20240403 12:25:14.634215 3123617 protocol_client.cc:336] Error while calling Recv(sock_.get(), io_buf): Software caused connection abort\r\nE20240403 12:25:14.634274 3123617 replica.cc:1106] Error while calling ReadLine(io_buf, &str): Software caused connection abort\r\nE20240403 12:25:14.634282 3123617 replica.cc:370] Error while calling ParseReplicationHeader(&io_buf, &repl_header): Software caused connection abort\r\nW20240403 12:25:14.634294 3123617 replica.cc:225] Error syncing with 167.235.36.197:6379 system:103 Software caused connection abort\r\nI20240403 12:25:15.624325 3123617 replica.cc:427] Re-established sync with Redis master with ID=c46b63bb357cdce4439a564c11ac98b39c4ed84e\r\nI20240403 12:25:15.674441 3123617 replica.cc:578] Transitioned into stable sync\r\n```",
  "created_at": "2024-04-08T11:03:51Z",
  "modified_files": [
    "src/server/replica.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/redis_replication_test.py"
  ]
}