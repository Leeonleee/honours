You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
chore: more pipeline latency coverage (#5632)
Partially covers #5630

Add `pipeline_dispatch_flush_duration_seconds` metrics that bridge the gap between pipeline_commands_duration_seconds and cmd_squash_hop_duration_seconds + pipeline_queue_wait_duration_seconds.

In summary,
pipeline_commands_duration_seconds roughly equals to `dragonfly_cmd_squash_hop_duration_seconds + cmd_squash_hop_duration_seconds + pipeline_queue_wait_duration_seconds`.

Also update the local dashboard.
Finally, cause AsyncFiber to wait for 20usec to give the I/O fiber the opportunity to read more data so that the pipeline will be larger.

<!--
**Commits Must Be Signed and Your PR title must conform to the conventional commit spec**
  * See: https://github.com/dragonflydb/dragonfly/blob/main/CONTRIBUTING.md
  * Please follow the section on `pre-commit hooks`, a linter will validate before you push

  Example PR Title: <type>(<scope>)!: <description>

  * `type` = bug, chore, feat, fix, docs, build, style, refactor, perf, test
  * `!` = OPTIONAL: signals a breaking change
  * `scope` = Optional when `type` is "chore" or "docs"
  * `description` = short description of the change

Examples:

  * chore(examples): Clarify `docker` usage #120
  * docs(readme): Fix Example Links #121
  * feat(ingest)!: Add new ingest #122
  * fix(ingest): Refactor for loop to list comprehension #123
-->
Dragonfly Crash After Changing Record Type to JSON and Setting TTL
**Describe the bug**
Dragonfly crashes when attempting to set a TTL on a key after its data type has been changed from a string to a JSON object. 

**To Reproduce**
Steps to reproduce the behavior:
1. Start Dragonfly:`docker run --rm -p 6379:6379 --ulimit memlock=-1  docker.dragonflydb.io/dragonflydb/dragonfly:latest`
2. Connect to Dragonfly: `redis-cli`
3. Execute the following Redis commands in sequence:
```
SET test foo EX 100    # Set 'test' as a string with a 100-second TTL
JSON.SET test $ '{}'   # Change 'test' to a JSON object
EXPIRE test 100        # Attempt to set a 100-second TTL (CRASH OCCURS HERE)
```
Error Logs:
```
I20250811 12:26:02.859664     1 init.cc:127] dragonfly running in opt mode.
I20250811 12:26:02.859730     1 dfly_main.cc:757] Starting dragonfly df-v1.32.0-960ddf41c41ce27d5c038856957b4d740629f94b
                   .--::--.                   
   :+*=:          =@@@@@@@@=          :+*+:   
  %@@@@@@%*=.     =@@@@@@@@-     .=*%@@@@@@#  
  @@@@@@@@@@@@#+-. .%@@@@#. .-+#@@@@@@@@@@@%  
  -@@@@@@@@@@@@@@@@*:#@@#:*@@@@@@@@@@@@@@@@-  
    :+*********####-%@%%@%-####********++.    
   .%@@@@@@@@@@@@@%:@@@@@@:@@@@@@@@@@@@@@%    
   .@@@@@@@@%*+-:   =@@@@=  .:-+*%@@@@@@@%.   
     =*+-:           ###*          .:-+*=     
                     %@@%                     
                     *@@*                     
                     +@@=                     
                     :##:                     
                     :@@:                     
                      @@                      
                      ..                      
* Logs will be written to the first available of the following paths:
/tmp/dragonfly.*
./dragonfly.*
* For the available flags type dragonfly [--help | --helpfull]
* Documentation can be found at: https://www.dragonflydb.io/docs
I20250811 12:26:02.859808     1 dfly_main.cc:799] maxmemory has not been specified. Deciding myself....
I20250811 12:26:02.859820     1 dfly_main.cc:808] Found 7.29GiB available memory. Setting maxmemory to 5.83GiB
W20250811 12:26:02.859839     1 dfly_main.cc:380] Weird error 1 switching to epoll
I20250811 12:26:02.860366     1 proactor_pool.cc:149] Running 4 io threads
I20250811 12:26:02.862628     1 server_family.cc:1073] Host OS: Linux 6.8.0-50-generic aarch64 with 4 threads
I20250811 12:26:02.862906     1 snapshot_storage.cc:186] Load snapshot: Searching for snapshot in directory: "/data"
W20250811 12:26:02.862937     1 server_family.cc:1173] Load snapshot: No snapshot found
I20250811 12:26:02.875483    12 listener_interface.cc:102] sock[11] AcceptServer - listening on 0.0.0.0:6379
F20250811 12:27:01.479359    14 db_slice.cc:948] Check failed: db.expire.Insert(main_it->first.AsRef(), ExpirePeriod(delta)).second 
*** Check failure stack trace: ***
    @     0xc304ad11ee2c  google::LogMessage::SendToLog()
    @     0xc304ad117d10  google::LogMessage::Flush()
    @     0xc304ad11964c  google::LogMessageFatal::~LogMessageFatal()
    @     0xc304acb91d54  dfly::DbSlice::AddExpire()
    @     0xc304acb9c84c  dfly::DbSlice::UpdateExpire()
    @     0xc304ac560da4  dfly::(anonymous namespace)::OpExpire()
    @     0xc304ac56116c  _ZN4absl12lts_2025051219functional_internal12InvokeObjectIZN4dfly13GenericFamily6ExpireENS0_4SpanIKSt17basic_string_viewIcSt11char_traitsIcEEEERKNS3_14CommandContextEEUlPNS3_11TransactionEPNS3_11EngineShardEE_NSF_14RunnableResultEJSG_SI_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE
    @     0xc304acbd01c8  dfly::Transaction::RunCallback()
    @     0xc304acbd09b0  dfly::Transaction::ScheduleInShard()
    @     0xc304acbd29a8  dfly::Transaction::ScheduleBatchInShard()
    @     0xc304acd7e3c0  util::fb2::FiberQueue::Run()
    @     0xc304acc222fc  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_NS6_13FiberPriorityERKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE
    @     0xc304ace43dec  make_fcontext
*** SIGABRT received at time=1754915221 on cpu 3 ***
PC: @     0xfc42d172f1f0  (unknown)  (unknown)
    @     0xc304ad177784        464  absl::lts_20250512::AbslFailureSignalHandler()
    @     0xfc42d19ac8f8       4960  (unknown)
    @     0xfc42d16ea67c        208  gsignal
    @     0xfc42d16d7130         32  abort
    @     0xc304ad1248ec        336  google::DumpStackTraceAndExit()
    @     0xc304ad11830c        192  google::LogMessage::Fail()
    @     0xc304ad11ee2c         16  google::LogMessage::SendToLog()
    @     0xc304ad117d10        208  google::LogMessage::Flush()
    @     0xc304ad11964c         80  google::LogMessageFatal::~LogMessageFatal()
    @     0xc304acb91d54         16  dfly::DbSlice::AddExpire()
    @     0xc304acb9c84c        256  dfly::DbSlice::UpdateExpire()
    @     0xc304ac560da4        192  dfly::(anonymous namespace)::OpExpire()
    @     0xc304ac56116c        496  absl::lts_20250512::functional_internal::InvokeObject<>()
    @     0xc304acbd01c8         96  dfly::Transaction::RunCallback()
    @     0xc304acbd09b0        288  dfly::Transaction::ScheduleInShard()
    @     0xc304acbd29a8        592  dfly::Transaction::ScheduleBatchInShard()
    @     0xc304acd7e3c0         96  util::fb2::FiberQueue::Run()
    @     0xc304acc222fc        304  boost::context::detail::fiber_entry<>()
[failure_signal_handler.cc : 377] RAW: Signal 5 raised at PC=0xfc42d16d71ec while already in AbslFailureSignalHandler()
*** SIGTRAP received at time=1754915221 on cpu 3 ***
PC: @     0xfc42d16d71ec  (unknown)  abort
    @     0xc304ad177784        464  absl::lts_20250512::AbslFailureSignalHandler()
    @     0xfc42d19ac8f8       4960  (unknown)
    @     0xc304ad1248ec        336  google::DumpStackTraceAndExit()
    @     0xc304ad11830c        192  google::LogMessage::Fail()
    @     0xc304ad11ee2c         16  google::LogMessage::SendToLog()
    @     0xc304ad117d10        208  google::LogMessage::Flush()
    @     0xc304ad11964c         80  google::LogMessageFatal::~LogMessageFatal()
    @     0xc304acb91d54         16  dfly::DbSlice::AddExpire()
    @     0xc304acb9c84c        256  dfly::DbSlice::UpdateExpire()
    @     0xc304ac560da4        192  dfly::(anonymous namespace)::OpExpire()
    @     0xc304ac56116c        496  absl::lts_20250512::functional_internal::InvokeObject<>()
    @     0xc304acbd01c8         96  dfly::Transaction::RunCallback()
    @     0xc304acbd09b0        288  dfly::Transaction::ScheduleInShard()
    @     0xc304acbd29a8        592  dfly::Transaction::ScheduleBatchInShard()
    @     0xc304acd7e3c0         96  util::fb2::FiberQueue::Run()
    @     0xc304acc222fc        304  boost::context::detail::fiber_entry<>()
```

Expected Behavior:

The EXPIRE test 100 command should successfully set a TTL of 100 seconds for the test key, even after its data type has been changed to JSON.

Actual Behavior:

Dragonfly crashes after executing the EXPIRE test 100 command, following the sequence of commands outlined above.


**Environment (please complete the following information):**
 - OS: Linux / Darwin (Production / Local Development)
 - Containerized?: Kubernetes / Docker
 - Dragonfly Version: v1.31.2, also occurs with latest version v1.32.0


**Additional context**
The issue does not occur when changing a record from JSON to a string and then setting the TTL. 
The following sequence works without error:
```
JSON.SET test $ '{}'    # Set 'test' as a JSON object
EXPIRE test 100         # Set a 100-second TTL
SET test foo EX 100     # Change 'test' to a string with a 100-second TTL (No crash)
```
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
