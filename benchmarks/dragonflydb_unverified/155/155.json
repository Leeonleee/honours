{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 155,
  "instance_id": "dragonflydb__dragonfly-155",
  "issue_numbers": [
    "150"
  ],
  "base_commit": "d37a0bbc29950bec42ad607bf067867e50f52bd5",
  "patch": "diff --git a/src/core/tx_queue.h b/src/core/tx_queue.h\nindex 73fc138a82f7..0df6d00f560c 100644\n--- a/src/core/tx_queue.h\n+++ b/src/core/tx_queue.h\n@@ -76,6 +76,10 @@ class TxQueue {\n     return head_;\n   }\n \n+  Iterator Next(Iterator it) const {\n+    return vec_[it].next;\n+  }\n+\n  private:\n   enum { TRANS_TAG = 0, UINT_TAG = 11, FREE_TAG = 12 };\n \ndiff --git a/src/facade/conn_context.h b/src/facade/conn_context.h\nindex 7bd63020dbd7..542c3492b623 100644\n--- a/src/facade/conn_context.h\n+++ b/src/facade/conn_context.h\n@@ -53,7 +53,7 @@ class ConnectionContext {\n \n   virtual void OnClose() {}\n \n-  std::string GetContextInfo() const { return std::string{}; }\n+  virtual std::string GetContextInfo() const { return std::string{}; }\n \n  private:\n   Connection* owner_;\ndiff --git a/src/facade/dragonfly_connection.cc b/src/facade/dragonfly_connection.cc\nindex d426ef35e4d6..77ac104ed51d 100644\n--- a/src/facade/dragonfly_connection.cc\n+++ b/src/facade/dragonfly_connection.cc\n@@ -349,7 +349,9 @@ void Connection::ConnectionFlow(FiberSocketBase* peer) {\n   // After the client disconnected.\n   cc_->conn_closing = true;  // Signal dispatch to close.\n   evc_.notify();\n+  VLOG(1) << \"Before dispatch_fb.join()\";\n   dispatch_fb.join();\n+  VLOG(1) << \"After dispatch_fb.join()\";\n   cc_->OnClose();\n \n   stats->read_buf_capacity -= io_buf_.Capacity();\ndiff --git a/src/server/CMakeLists.txt b/src/server/CMakeLists.txt\nindex 5af0078205d9..4ac3a88e60c8 100644\n--- a/src/server/CMakeLists.txt\n+++ b/src/server/CMakeLists.txt\n@@ -10,7 +10,7 @@ add_library(dragonfly_lib blocking_controller.cc channel_slice.cc command_regist\n             set_family.cc stream_family.cc string_family.cc table.cc tiered_storage.cc\n             transaction.cc zset_family.cc version.cc)\n \n-cxx_link(dragonfly_lib dfly_core dfly_facade redis_lib strings_lib)\n+cxx_link(dragonfly_lib dfly_core dfly_facade redis_lib strings_lib html_lib)\n \n add_library(dfly_test_lib test_utils.cc)\n cxx_link(dfly_test_lib dragonfly_lib facade_test gtest_main_ext)\ndiff --git a/src/server/conn_context.cc b/src/server/conn_context.cc\nindex fa05d223207b..def9ae96cce4 100644\n--- a/src/server/conn_context.cc\n+++ b/src/server/conn_context.cc\n@@ -232,4 +232,16 @@ void ConnectionContext::OnClose() {\n   }\n }\n \n+string ConnectionContext::GetContextInfo() const {\n+  char buf[16] = {0};\n+  unsigned index = 0;\n+  if (async_dispatch)\n+    buf[index++] = 'a';\n+\n+  if (conn_closing)\n+    buf[index++] = 't';\n+\n+  return index ? absl::StrCat(\"flags:\", buf) : string();\n+}\n+\n }  // namespace dfly\ndiff --git a/src/server/conn_context.h b/src/server/conn_context.h\nindex 57536b3644e2..187ccbbccd30 100644\n--- a/src/server/conn_context.h\n+++ b/src/server/conn_context.h\n@@ -101,6 +101,8 @@ class ConnectionContext : public facade::ConnectionContext {\n \n   bool is_replicating = false;\n \n+  std::string GetContextInfo() const override;\n+\n  private:\n   void SendSubscriptionChangedResponse(std::string_view action,\n                                        std::optional<std::string_view> topic,\ndiff --git a/src/server/engine_shard_set.cc b/src/server/engine_shard_set.cc\nindex fe9141b9be3b..c8bd6770c2ff 100644\n--- a/src/server/engine_shard_set.cc\n+++ b/src/server/engine_shard_set.cc\n@@ -132,7 +132,9 @@ void EngineShard::DestroyThreadLocal() {\n // Is called by Transaction::ExecuteAsync in order to run transaction tasks.\n // Only runs in its own thread.\n void EngineShard::PollExecution(const char* context, Transaction* trans) {\n-  DVLOG(1) << \"PollExecution \" << context << \" \" << (trans ? trans->DebugId() : \"\");\n+  VLOG(2) << \"PollExecution \" << context << \" \" << (trans ? trans->DebugId() : \"\")\n+          << \" \" << txq_.size() << \" \" << continuation_trans_;\n+\n   ShardId sid = shard_id();\n \n   uint16_t trans_mask = trans ? trans->GetLocalMask(sid) : 0;\n@@ -170,7 +172,7 @@ void EngineShard::PollExecution(const char* context, Transaction* trans) {\n       // The fact that Tx is in the queue, already means that coordinator fiber will not progress,\n       // hence here it's enough to test for run_count and check local_mask.\n       bool is_armed = head->IsArmedInShard(sid);\n-      DVLOG(2) << \"Considering head \" << head->DebugId() << \" isarmed: \" << is_armed;\n+      VLOG(2) << \"Considering head \" << head->DebugId() << \" isarmed: \" << is_armed;\n \n       if (!is_armed)\n         break;\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex 11002fe765c7..07fc8e8ee42b 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -33,7 +33,7 @@ extern \"C\" {\n #include \"server/transaction.h\"\n #include \"server/version.h\"\n #include \"server/zset_family.h\"\n-#include \"util/metrics/metrics.h\"\n+#include \"util/html/sorted_table.h\"\n #include \"util/uring/uring_fiber_algo.h\"\n #include \"util/varz.h\"\n \n@@ -63,13 +63,13 @@ namespace this_fiber = ::boost::this_fiber;\n using absl::GetFlag;\n using absl::StrCat;\n using namespace facade;\n+namespace h2 = boost::beast::http;\n \n namespace {\n \n DEFINE_VARZ(VarzMapAverage, request_latency_usec);\n \n std::optional<VarzFunction> engine_varz;\n-metrics::CounterFamily cmd_req(\"requests_total\", \"Number of served redis requests\");\n \n constexpr size_t kMaxThreadSize = 1024;\n \n@@ -300,6 +300,48 @@ bool EvalValidator(CmdArgList args, ConnectionContext* cntx) {\n   return true;\n }\n \n+void TxTable(const http::QueryArgs& args, HttpContext* send) {\n+  using html::SortedTable;\n+\n+  http::StringResponse resp = http::MakeStringResponse(h2::status::ok);\n+  resp.body() = SortedTable::HtmlStart();\n+  SortedTable::StartTable({\"ShardId\", \"TID\", \"TxId\", \"Armed\"}, &resp.body());\n+\n+  if (shard_set) {\n+    vector<string> rows(shard_set->size());\n+\n+    shard_set->RunBriefInParallel([&](EngineShard* shard) {\n+      ShardId sid = shard->shard_id();\n+\n+      absl::AlphaNum tid(gettid());\n+      absl::AlphaNum sid_an(sid);\n+\n+      string& mine = rows[sid];\n+      TxQueue* queue = shard->txq();\n+\n+      if (!queue->Empty()) {\n+        auto cur = queue->Head();\n+        do {\n+          auto value = queue->At(cur);\n+          Transaction* trx = std::get<Transaction*>(value);\n+\n+          absl::AlphaNum an2(trx->txid());\n+          absl::AlphaNum an3(trx->IsArmedInShard(sid));\n+          SortedTable::Row({sid_an.Piece(), tid.Piece(), an2.Piece(), an3.Piece()}, &mine);\n+          cur = queue->Next(cur);\n+        } while (cur != queue->Head());\n+      }\n+    });\n+\n+    for (const auto& s : rows) {\n+      resp.body().append(s);\n+    }\n+  }\n+\n+  SortedTable::EndTable(&resp.body());\n+  send->Invoke(std::move(resp));\n+}\n+\n }  // namespace\n \n Service::Service(ProactorPool* pp) : pp_(*pp), server_family_(this) {\n@@ -334,7 +376,6 @@ void Service::Init(util::AcceptServer* acceptor, util::ListenerInterface* main_i\n   StringFamily::Init(&pp_);\n   GenericFamily::Init(&pp_);\n   server_family_.Init(acceptor, main_interface);\n-  cmd_req.Init(&pp_, {\"type\"});\n }\n \n void Service::Shutdown() {\n@@ -352,7 +393,6 @@ void Service::Shutdown() {\n   StringFamily::Shutdown();\n   GenericFamily::Shutdown();\n \n-  cmd_req.Shutdown();\n   shard_set->Shutdown();\n \n   // wait for all the pending callbacks to stop.\n@@ -503,7 +543,7 @@ void Service::DispatchCommand(CmdArgList args, facade::ConnectionContext* cntx)\n   }\n \n   dfly_cntx->cid = cid;\n-  cmd_req.Inc({cmd_name});\n+\n   cid->Invoke(args, dfly_cntx);\n   end_usec = ProactorBase::GetMonotonicTimeNs();\n \n@@ -856,6 +896,7 @@ void Service::Exec(CmdArgList args, ConnectionContext* cntx) {\n     return rb->SendError(\"-EXECABORT Transaction discarded because of previous errors\");\n   }\n \n+  VLOG(1) << \"StartExec \" << cntx->conn_state.exec_body.size();\n   rb->StartArray(cntx->conn_state.exec_body.size());\n   if (!cntx->conn_state.exec_body.empty()) {\n     CmdArgVec str_list;\n@@ -961,9 +1002,9 @@ void Service::Subscribe(CmdArgList args, ConnectionContext* cntx) {\n void Service::Unsubscribe(CmdArgList args, ConnectionContext* cntx) {\n   args.remove_prefix(1);\n \n-  if (args.size() == 0){\n+  if (args.size() == 0) {\n     cntx->UnsubscribeAll(true);\n-  }else{\n+  } else {\n     cntx->ChangeSubscription(false, true, std::move(args));\n   }\n }\n@@ -1026,6 +1067,7 @@ GlobalState Service::SwitchState(GlobalState from, GlobalState to) {\n \n void Service::ConfigureHttpHandlers(util::HttpListenerBase* base) {\n   server_family_.ConfigureMetrics(base);\n+  base->RegisterCb(\"/txz\", TxTable);\n }\n \n using ServiceFunc = void (Service::*)(CmdArgList, ConnectionContext* cntx);\ndiff --git a/src/server/stream_family.cc b/src/server/stream_family.cc\nindex 9d041b0adaaa..4cd2804b499d 100644\n--- a/src/server/stream_family.cc\n+++ b/src/server/stream_family.cc\n@@ -629,7 +629,8 @@ void StreamFamily::XDel(CmdArgList args, ConnectionContext* cntx) {\n   string_view key = ArgS(args, 1);\n   args.remove_prefix(2);\n \n-  absl::InlinedVector<streamID, 8> ids(ids.size());\n+  absl::InlinedVector<streamID, 8> ids(args.size());\n+\n   for (size_t i = 0; i < args.size(); ++i) {\n     ParsedStreamId parsed_id;\n     string_view str_id = ArgS(args, i);\ndiff --git a/src/server/transaction.cc b/src/server/transaction.cc\nindex 81a3c50b3132..786302a21a80 100644\n--- a/src/server/transaction.cc\n+++ b/src/server/transaction.cc\n@@ -299,7 +299,7 @@ bool Transaction::RunInShard(EngineShard* shard) {\n   // because Scheduling is done before multi-exec batch is executed. Therefore we\n   // lock keys right before the execution of each statement.\n \n-  DVLOG(1) << \"RunInShard: \" << DebugId() << \" sid:\" << shard->shard_id();\n+  VLOG(2) << \"RunInShard: \" << DebugId() << \" sid:\" << shard->shard_id();\n \n   unsigned idx = SidToId(shard->shard_id());\n   auto& sd = shard_data_[idx];\n@@ -462,20 +462,45 @@ void Transaction::ScheduleInternal() {\n         DCHECK(!span_all);\n         coordinator_state_ |= COORD_OOO;\n       }\n-      DVLOG(1) << \"Scheduled \" << DebugId()\n-               << \" OutOfOrder: \" << bool(coordinator_state_ & COORD_OOO);\n+      VLOG(2) << \"Scheduled \" << DebugId()\n+              << \" OutOfOrder: \" << bool(coordinator_state_ & COORD_OOO)\n+              << \" num_shards: \" << num_shards;\n       coordinator_state_ |= COORD_SCHED;\n       break;\n     }\n \n-    DVLOG(1) << \"Cancelling \" << DebugId();\n+    VLOG(2) << \"Cancelling \" << DebugId();\n+\n+    atomic_bool should_poll_execution{false};\n \n     auto cancel = [&](EngineShard* shard) {\n-      success.fetch_sub(CancelShardCb(shard), memory_order_relaxed);\n+      bool res = CancelShardCb(shard);\n+      if (res) {\n+        should_poll_execution.store(true, memory_order_relaxed);\n+      }\n     };\n \n     shard_set->RunBriefInParallel(std::move(cancel), is_active);\n-    CHECK_EQ(0u, success.load(memory_order_relaxed));\n+\n+    // We must follow up with PollExecution because in rare cases with multi-trans\n+    // that follows this one, we may find the next transaction in the queue that is never\n+    // trigerred. Which leads to deadlock. I could solve this by adding PollExecution to\n+    // CancelShardCb above but then we would need to use the shard_set queue since PollExecution\n+    // is blocking. I wanted to avoid the additional latency for the general case of running\n+    // CancelShardCb because of the very rate case below. Therefore, I decided to just fetch the\n+    // indication that we need to follow up with PollExecution and then send it to shard_set queue.\n+    // We do not need to wait for this callback to finish - just make sure it will eventually run.\n+    // See https://github.com/dragonflydb/dragonfly/issues/150 for more info.\n+    if (should_poll_execution.load(memory_order_relaxed)) {\n+      for (uint32_t i = 0; i < shard_set->size(); ++i) {\n+        if (!is_active(i))\n+          continue;\n+\n+        shard_set->Add(i, [] {\n+          EngineShard::tlocal()->PollExecution(\"cancel_cleanup\", nullptr);\n+        });\n+      }\n+    }\n   }\n \n   if (IsOOO()) {\n@@ -855,11 +880,12 @@ bool Transaction::CancelShardCb(EngineShard* shard) {\n \n   sd.pq_pos = TxQueue::kEnd;\n \n-  TxQueue* pq = shard->txq();\n-  auto val = pq->At(pos);\n+  TxQueue* txq = shard->txq();\n+  TxQueue::Iterator head = txq->Head();\n+  auto val = txq->At(pos);\n   Transaction* trans = absl::get<Transaction*>(val);\n-  DCHECK(trans == this) << \"Pos \" << pos << \", pq size \" << pq->size() << \", trans \" << trans;\n-  pq->Remove(pos);\n+  DCHECK(trans == this) << \"Pos \" << pos << \", txq size \" << txq->size() << \", trans \" << trans;\n+  txq->Remove(pos);\n \n   if (sd.local_mask & KEYLOCK_ACQUIRED) {\n     auto mode = Mode();\n@@ -867,7 +893,12 @@ bool Transaction::CancelShardCb(EngineShard* shard) {\n     shard->db_slice().Release(mode, lock_args);\n     sd.local_mask &= ~KEYLOCK_ACQUIRED;\n   }\n-  return true;\n+\n+  if (pos == head && !txq->Empty()) {\n+    return true;\n+  }\n+\n+  return false;\n }\n \n // runs in engine-shard thread.\ndiff --git a/src/server/transaction.h b/src/server/transaction.h\nindex b7c49eaaa180..8983586f5a70 100644\n--- a/src/server/transaction.h\n+++ b/src/server/transaction.h\n@@ -208,7 +208,7 @@ class Transaction {\n   /// Runs in the shard thread.\n   std::pair<bool, bool> ScheduleInShard(EngineShard* shard);\n \n-  // Returns true if operation was cancelled for this shard. Runs in the shard thread.\n+  // Returns true if we need to follow up with PollExecution on this shard.\n   bool CancelShardCb(EngineShard* shard);\n \n   // Shard callbacks used within Execute calls\n@@ -332,7 +332,7 @@ class Transaction {\n };\n \n inline uint16_t trans_id(const Transaction* ptr) {\n-  return intptr_t(ptr) & 0xFFFF;\n+  return (intptr_t(ptr) >> 8) & 0xFFFF;\n }\n \n OpResult<KeyIndex> DetermineKeys(const CommandId* cid, CmdArgList args);\n",
  "test_patch": "diff --git a/tests/async.py b/tests/async.py\nnew file mode 100755\nindex 000000000000..f4a31c89358f\n--- /dev/null\n+++ b/tests/async.py\n@@ -0,0 +1,58 @@\n+#!/usr/bin/env python3\n+\n+\"\"\"\n+This is the script that helped to reproduce https://github.com/dragonflydb/dragonfly/issues/150\n+The outcome - stalled code with all its connections deadlocked.\n+Reproduced only with dragonfly in release mode on multi-core machine.\n+\"\"\"\n+\n+import asyncio\n+import aioredis\n+\n+from loguru import logger as log\n+import sys\n+import random\n+\n+connection_pool = aioredis.ConnectionPool(host=\"localhost\", port=6379,\n+                                          db=1, decode_responses=True, max_connections=16)\n+\n+\n+key_index = 1\n+\n+async def post_to_redis(sem, db_name, index):\n+    global key_index\n+    async with sem:\n+        results = None\n+        try:\n+            redis_client = aioredis.Redis(connection_pool=connection_pool)\n+            async with redis_client.pipeline(transaction=True) as pipe:\n+                for i in range(1, 15):                    \n+                    pipe.hsetnx(name=f\"key_{key_index}\", key=\"name\", value=\"bla\")\n+                    key_index += 1\n+                #log.info(f\"after first half {key_index}\")\n+                for i in range(1, 15):\n+                    pipe.hsetnx(name=f\"bla_{key_index}\", key=\"name2\", value=\"bla\")\n+                    key_index += 1\n+                assert len(pipe.command_stack) > 0\n+                log.info(f\"before pipe.execute {key_index}\")\n+                results = await pipe.execute()\n+                log.info(f\"after pipe.execute {key_index}\")\n+        finally:\n+            # log.info(f\"before close {index}\")\n+            await redis_client.close()\n+            #log.info(f\"after close {index} {len(results)}\")\n+ \n+\n+async def do_concurrent(db_name):\n+    tasks = []\n+    sem = asyncio.Semaphore(10)\n+    for i in range(1, 3000):\n+        tasks.append(post_to_redis(sem, db_name, i))\n+    res = await asyncio.gather(*tasks)\n+   \n+\n+if __name__ == '__main__':\n+    log.remove()\n+    log.add(sys.stdout, enqueue=True, level='INFO')\n+    loop = asyncio.get_event_loop()\n+    loop.run_until_complete(do_concurrent(\"my_db\"))\n",
  "problem_statement": "Leaking connections when used with redis-py's asyncio.ConnectionPool\n### My setup:\r\nUbuntu 20.04.4 machine with Linux kernel 5.18\r\nDragonfly x86_64 version 0.3 alpha (bare metal)\r\n\r\n\r\n### Issue:\r\nWhen running an async redis client and using pipelining in order to batch transactions, after a short period of time Dragonfly stops responding and it just hangs.\r\nWhen observing the open connections, I found out that there are multiple ESTABLISHED connections to Dragonfly, all of them are just hanging.\r\n\r\nSame code works just fine with the latest redis-server and the connections are closed properly.\r\n\r\n### Python code to reproduce the problem:\r\n\r\n```python\r\nconnection_pools = {\r\n    \"my_db\" : None,\r\n    \"my_db2\" : None\r\n}\r\n\r\nconnection_pools[db_name] = aioredis.ConnectionPool(host=config[\"hostname\"], port=config[\"port\"], db=config[\"databases\"][db_name], password=config[\"password\"], decode_responses=True, max_connections=16)\r\n\r\nasync def post_to_redis(my_list, my_list2, db_name):\r\n        if connection_pools[db_name] is None:\r\n                await init_pool(db_name)\r\n            results = None\r\n        try:\r\n                redis_client = aioredis.Redis(connection_pool=connection_pools[db_name])\r\n                async with redis_client.pipeline(transaction=True) as pipe:\r\n                        for k,v in list(my_list):\r\n                                pipe.hsetnx(name=k, key=\"name\", value=v)\r\n                        for k,v in list(my_list2):\r\n                                pipe.hsetnx(name=k, key=\"name2\", value=v)\r\n                        if len(pipe.command_stack) > 0:\r\n                                results = await pipe.execute()\r\n        finally:\r\n                await redis_client.close()\r\n                return results\r\n\r\nasync def do_concurrent(db_name):\r\n        tasks = []\r\n        for i in range(1,100):\r\n                tasks.append(post_to_redis(my_list, my_list2, db_name))\r\n        res = await asyncio.gather(*tasks)```\r\n\r\n\n",
  "hints_text": "Thanks @thevaizman .\r\n\r\nI am not familiar with async functions in python. How do I call `do_concurrent` from the main function?\r\n\nHey, you will have to modify the provided code (especially the `ConnectionPool` bit) so that you can connect to your local Dragonfly DB instance.\r\nRegarding calling async functions, you can add the following code to the file:\r\n```python\r\nif __name__ == '__main__':\r\n    loop = asyncio.get_event_loop()\r\n    loop.run_until_complete(do_concurrent(\"my_db\"))\r\n```\n@thevaizman  I think that `init_pool` function is missing.  also my_list and my_list2\nIt is missing, sorry, I had to redact some stuff since this is some production code of ours which I'm not keen to share ;)\r\n\r\n```python\r\nasync def init_pool(db_name):\r\n    global connection_pools\r\n    config = await read_config(\"redis_config\")\r\n    if connection_pools[db_name] is None:\r\n        log.info(\"Will init ConnectionPool for {}\".format(db_name))\r\n        connection_pools[db_name] = aioredis.ConnectionPool(host=config[\"hostname\"], port=config[\"port\"], db=config[\"databases\"][db_name], password=config[\"password\"], decode_responses=True, max_connections=16)\r\n```\nAnd for creating the lists, you can use whatever values since the values don't affect the outcome:\r\n\r\n```python\r\n async def create_list():\r\n        l = []\r\n        for i in range(1,100):\r\n                l.append((i,\"val\"))\r\n        return l\r\n```\nHi Tal,\r\n\r\nmy final version of the script is below.\r\nCan you confirm that this script stalls DF in your environment? If not, then what's missing?\r\n\r\n```py\r\n#!/usr/bin/env python3\r\n\r\nimport asyncio\r\nimport aioredis\r\n \r\nconnection_pools = {\r\n    \"my_db\" : None,\r\n}\r\n\r\nconnection_pools[\"my_db\"] = aioredis.ConnectionPool(host=\"localhost\", port=6379, \r\n                                                    db=1, decode_responses=True, max_connections=16)\r\n\r\nmy_list = [(\"key1_\" + str(i), \"val1_\" + str(i)) for i in range(100) ]\r\nmy_list2 = [(\"list1_\" + str(i), \"lval1_\" + str(i)) for i in range(100) ]\r\n\r\nasync def post_to_redis(my_list, my_list2, db_name):\r\n    results = None\r\n    try:\r\n        redis_client = aioredis.Redis(connection_pool=connection_pools[db_name])\r\n        async with redis_client.pipeline(transaction=True) as pipe:\r\n                for k,v in list(my_list):\r\n                        pipe.hsetnx(name=k, key=\"name\", value=v)\r\n                for k,v in list(my_list2):\r\n                        pipe.hsetnx(name=k, key=\"name2\", value=v)\r\n                if len(pipe.command_stack) > 0:\r\n                        results = await pipe.execute()\r\n    finally:\r\n            await redis_client.close()\r\n            return results\r\n\r\n\r\nasync def do_concurrent(db_name):\r\n        tasks = []\r\n        for i in range(1,100):\r\n                tasks.append(post_to_redis(my_list, my_list2, db_name))\r\n        res = await asyncio.gather(*tasks)\r\n\r\n\r\nif __name__ == '__main__':\r\n    loop = asyncio.get_event_loop()\r\n    loop.run_until_complete(do_concurrent(\"my_db\"))\r\n```\nHey again, sorry, it looks like I forgot to mention that the keys need to actually be unique so that they can be inserted into the DB. Also, the Dragonfly DB machine I'm using is remote (not sure if that also matters).\r\nSorry for the confusion.\r\n\r\nAnyways, just confirmed that the following script indeed makes Dragonfly hang after ~20s:\r\n\r\n```python\r\nfrom redis import asyncio as aioredis\r\nimport asyncio \r\nfrom loguru import logger as log\r\nimport sys\r\nimport random\r\n\r\nconnection_pools = {\r\n    \"my_db\" : None,\r\n}\r\nconnection_pools[\"my_db\"] = aioredis.ConnectionPool(host=\"---------\", port=6380, \r\n                                                    db=1, password=\"-------\", decode_responses=True, max_connections=16)\r\n\r\nasync def post_to_redis(sem, db_name):\r\n    async with sem:\r\n        results = None\r\n        try:\r\n            redis_client = aioredis.Redis(connection_pool=connection_pools[db_name])\r\n            async with redis_client.pipeline(transaction=True) as pipe:\r\n                    for i in range(1,100):\r\n                            pipe.hsetnx(name=random.randint(1,999999999), key=\"name\", value=random.randint(1,999999999))\r\n                    for i in range(1,100):\r\n                            pipe.hsetnx(name=random.randint(1,999999999), key=\"name2\", value=random.randint(1,999999999))\r\n                    if len(pipe.command_stack) > 0:\r\n                            results = await pipe.execute()\r\n        finally:\r\n                await redis_client.close()\r\n                log.info(results)\r\n\r\nasync def do_concurrent(db_name):\r\n    tasks = []\r\n    sem = asyncio.Semaphore(10)\r\n    for i in range(1,100000):\r\n        log.info(\"Adding - {}\".format(i))\r\n        tasks.append(post_to_redis(sem, db_name))\r\n    res = await asyncio.gather(*tasks)\r\n    for r in res:\r\n        log.info(r)\r\n\r\nif __name__ == '__main__':\r\n    log.remove()\r\n    log.add(sys.stdout, enqueue=True, level='INFO')\r\n    loop = asyncio.get_event_loop()\r\n    loop.run_until_complete(do_concurrent(\"my_db\"))\r\n```\r\n\r\nYou can also observe how the connections are all ESTABLISHED and won't close until I forcefully kill the DF process:\r\n\r\n![A76BF2BC-46C8-47CA-9C4A-21437215079A](https://user-images.githubusercontent.com/101282057/173681673-fcb505fe-37e8-48be-8f2e-78d95475cbde.jpeg)\r\n\r\n\nThanks, I succeed to reproduce it locally. \nDude, thanks for helping me to reproduce this bug! \r\nIt's probably the toughest one I handled in the last 3 months (still did not solve it, btw).\nSure thing! Glad I could help. Thank you for the effort and in general for DF, can\u2019t wait to use it in our production! :)",
  "created_at": "2022-06-15T13:40:24Z",
  "modified_files": [
    "src/core/tx_queue.h",
    "src/facade/conn_context.h",
    "src/facade/dragonfly_connection.cc",
    "src/server/CMakeLists.txt",
    "src/server/conn_context.cc",
    "src/server/conn_context.h",
    "src/server/engine_shard_set.cc",
    "src/server/main_service.cc",
    "src/server/stream_family.cc",
    "src/server/transaction.cc",
    "src/server/transaction.h"
  ],
  "modified_test_files": [
    "b/tests/async.py"
  ]
}