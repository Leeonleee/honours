{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 1875,
  "instance_id": "dragonflydb__dragonfly-1875",
  "issue_numbers": [
    "1609"
  ],
  "base_commit": "09415c4f577887d744866a61b55af8f35612d66b",
  "patch": "diff --git a/src/facade/dragonfly_connection.cc b/src/facade/dragonfly_connection.cc\nindex 373733ff53e0..14bfac339e2f 100644\n--- a/src/facade/dragonfly_connection.cc\n+++ b/src/facade/dragonfly_connection.cc\n@@ -831,9 +831,10 @@ auto Connection::IoLoop(util::FiberSocketBase* peer, SinkReplyBuilder* orig_buil\n     io_buf_.CommitWrite(*recv_sz);\n     stats_->io_read_bytes += *recv_sz;\n     ++stats_->io_read_cnt;\n+\n     phase_ = PROCESS;\n     bool is_iobuf_full = io_buf_.AppendLen() == 0;\n-\n+    service_->AwaitOnPauseDispatch();\n     if (redis_parser_) {\n       parse_status = ParseRedis(orig_builder);\n     } else {\ndiff --git a/src/facade/ok_main.cc b/src/facade/ok_main.cc\nindex 0d4a5803b9b6..b00844591953 100644\n--- a/src/facade/ok_main.cc\n+++ b/src/facade/ok_main.cc\n@@ -44,6 +44,10 @@ class OkService : public ServiceInterface {\n   ConnectionStats* GetThreadLocalConnectionStats() final {\n     return &tl_stats;\n   }\n+\n+  void AwaitOnPauseDispatch() {\n+    return;\n+  }\n };\n \n void RunEngine(ProactorPool* pool, AcceptServer* acceptor) {\ndiff --git a/src/facade/service_interface.h b/src/facade/service_interface.h\nindex 7f02604cc552..da666e0c9bdb 100644\n--- a/src/facade/service_interface.h\n+++ b/src/facade/service_interface.h\n@@ -35,6 +35,7 @@ class ServiceInterface {\n   virtual ConnectionContext* CreateContext(util::FiberSocketBase* peer, Connection* owner) = 0;\n \n   virtual ConnectionStats* GetThreadLocalConnectionStats() = 0;\n+  virtual void AwaitOnPauseDispatch() = 0;\n \n   virtual void ConfigureHttpHandlers(util::HttpListenerBase* base, bool is_privileged) {\n   }\ndiff --git a/src/server/conn_context.cc b/src/server/conn_context.cc\nindex 78530893cf22..b5c9aee3416f 100644\n--- a/src/server/conn_context.cc\n+++ b/src/server/conn_context.cc\n@@ -235,6 +235,7 @@ void ConnectionContext::SendSubscriptionChangedResponse(string_view action,\n void ConnectionState::ExecInfo::Clear() {\n   state = EXEC_INACTIVE;\n   body.clear();\n+  is_write = false;\n   ClearWatched();\n }\n \ndiff --git a/src/server/conn_context.h b/src/server/conn_context.h\nindex f962864a6cc6..f0bde27b0653 100644\n--- a/src/server/conn_context.h\n+++ b/src/server/conn_context.h\n@@ -82,6 +82,7 @@ struct ConnectionState {\n \n     ExecState state = EXEC_INACTIVE;\n     std::vector<StoredCmd> body;\n+    bool is_write = false;\n \n     std::vector<std::pair<DbIndex, std::string>> watched_keys;  // List of keys registered by WATCH\n     std::atomic_bool watched_dirty = false;  // Set if a watched key was changed before EXEC\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex ba575e849870..b128fa70a53d 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -1067,6 +1067,14 @@ void Service::DispatchCommand(CmdArgList args, facade::ConnectionContext* cntx)\n               << \" in dbid=\" << dfly_cntx->conn_state.db_index;\n   }\n \n+  string_view cmd_name(cid->name());\n+  bool is_write = (cid->opt_mask() & CO::WRITE) || cmd_name == \"PUBLISH\" || cmd_name == \"EVAL\" ||\n+                  cmd_name == \"EVALSHA\";\n+  if (cmd_name == \"EXEC\" && dfly_cntx->conn_state.exec_info.is_write) {\n+    is_write = true;\n+  }\n+  etl.AwaitPauseState(is_write);\n+\n   etl.RecordCmd();\n \n   if (auto err = VerifyCommandState(cid, args_no_cmd, *dfly_cntx); err) {\n@@ -1082,7 +1090,9 @@ void Service::DispatchCommand(CmdArgList args, facade::ConnectionContext* cntx)\n     // TODO: protect against aggregating huge transactions.\n     StoredCmd stored_cmd{cid, args_no_cmd};\n     dfly_cntx->conn_state.exec_info.body.push_back(std::move(stored_cmd));\n-\n+    if (stored_cmd.Cid()->opt_mask() & CO::WRITE) {\n+      dfly_cntx->conn_state.exec_info.is_write = true;\n+    }\n     return cntx->SendSimpleString(\"QUEUED\");\n   }\n \n@@ -1254,8 +1264,9 @@ void Service::DispatchManyCommands(absl::Span<CmdArgList> args_list,\n     // invocations, we can potentially execute multiple eval in parallel, which is very powerful\n     // paired with shardlocal eval\n     const bool is_eval = CO::IsEvalKind(ArgS(args, 0));\n+    const bool is_pause = dfly::ServerState::tlocal()->IsPaused();\n \n-    if (!is_multi && !is_eval && cid != nullptr) {\n+    if (!is_multi && !is_eval && cid != nullptr && !is_pause) {\n       stored_cmds.reserve(args_list.size());\n       stored_cmds.emplace_back(cid, tail_args);\n       continue;\n@@ -1410,6 +1421,10 @@ facade::ConnectionStats* Service::GetThreadLocalConnectionStats() {\n   return ServerState::tl_connection_stats();\n }\n \n+void Service::AwaitOnPauseDispatch() {\n+  ServerState::tlocal()->AwaitOnPauseDispatch();\n+}\n+\n const CommandId* Service::FindCmd(std::string_view cmd) const {\n   return registry_.Find(cmd);\n }\ndiff --git a/src/server/main_service.h b/src/server/main_service.h\nindex 190cf40c7f69..84830cc1cee6 100644\n--- a/src/server/main_service.h\n+++ b/src/server/main_service.h\n@@ -73,6 +73,7 @@ class Service : public facade::ServiceInterface {\n                                            facade::Connection* owner) final;\n \n   facade::ConnectionStats* GetThreadLocalConnectionStats() final;\n+  void AwaitOnPauseDispatch() final;\n \n   std::pair<const CommandId*, CmdArgList> FindCmd(CmdArgList args) const;\n   const CommandId* FindCmd(std::string_view) const;\ndiff --git a/src/server/multi_command_squasher.h b/src/server/multi_command_squasher.h\nindex df08ed700796..61c1506283c7 100644\n--- a/src/server/multi_command_squasher.h\n+++ b/src/server/multi_command_squasher.h\n@@ -30,7 +30,7 @@ class MultiCommandSquasher {\n   }\n \n  private:\n-  // Per-shard exection info.\n+  // Per-shard execution info.\n   struct ShardExecInfo {\n     ShardExecInfo() : had_writes{false}, cmds{}, replies{}, local_tx{nullptr} {\n     }\n@@ -74,7 +74,7 @@ class MultiCommandSquasher {\n   ConnectionContext* cntx_;     // Underlying context\n   Service* service_;\n \n-  bool atomic_;                // Wheter working in any of the atomic modes\n+  bool atomic_;                // Whether working in any of the atomic modes\n   const CommandId* base_cid_;  // underlying cid (exec or eval) for executing batch hops\n \n   bool verify_commands_ = false;  // Whether commands need to be verified before execution\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex defc19f3b7f8..79d6e0f30214 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -27,6 +27,7 @@ extern \"C\" {\n \n #include \"base/flags.h\"\n #include \"base/logging.h\"\n+#include \"facade/cmd_arg_parser.h\"\n #include \"facade/dragonfly_connection.h\"\n #include \"facade/reply_builder.h\"\n #include \"io/file_util.h\"\n@@ -1213,49 +1214,139 @@ void ServerFamily::Auth(CmdArgList args, ConnectionContext* cntx) {\n void ServerFamily::Client(CmdArgList args, ConnectionContext* cntx) {\n   ToUpper(&args[0]);\n   string_view sub_cmd = ArgS(args, 0);\n+  CmdArgList sub_args = args.subspan(1);\n \n-  if (sub_cmd == \"SETNAME\" && args.size() == 2) {\n-    cntx->conn()->SetName(string{ArgS(args, 1)});\n+  if (sub_cmd == \"SETNAME\") {\n+    return ClientSetName(sub_args, cntx);\n+  } else if (sub_cmd == \"GETNAME\") {\n+    return ClientGetName(sub_args, cntx);\n+  } else if (sub_cmd == \"LIST\") {\n+    return ClientList(sub_args, cntx);\n+  } else if (sub_cmd == \"PAUSE\") {\n+    return ClientPause(sub_args, cntx);\n+  }\n+\n+  if (sub_cmd == \"SETINFO\") {\n     return (*cntx)->SendOk();\n   }\n \n-  if (sub_cmd == \"GETNAME\") {\n-    auto name = cntx->conn()->GetName();\n-    if (!name.empty()) {\n-      return (*cntx)->SendBulkString(name);\n-    } else {\n-      return (*cntx)->SendNull();\n-    }\n+  LOG_FIRST_N(ERROR, 10) << \"Subcommand \" << sub_cmd << \" not supported\";\n+  return (*cntx)->SendError(UnknownSubCmd(sub_cmd, \"CLIENT\"), kSyntaxErrType);\n+}\n+\n+void ServerFamily::ClientSetName(CmdArgList args, ConnectionContext* cntx) {\n+  if (args.size() == 1) {\n+    cntx->conn()->SetName(string{ArgS(args, 0)});\n+    return (*cntx)->SendOk();\n+  } else {\n+    return (*cntx)->SendError(facade::kSyntaxErr);\n   }\n+}\n \n-  if (sub_cmd == \"LIST\") {\n-    vector<string> client_info;\n-    absl::base_internal::SpinLock mu;\n+void ServerFamily::ClientGetName(CmdArgList args, ConnectionContext* cntx) {\n+  if (!args.empty()) {\n+    return (*cntx)->SendError(facade::kSyntaxErr);\n+  }\n+  auto name = cntx->conn()->GetName();\n+  if (!name.empty()) {\n+    return (*cntx)->SendBulkString(name);\n+  } else {\n+    return (*cntx)->SendNull();\n+  }\n+}\n \n-    // we can not preempt the connection traversal, so we need to use a spinlock.\n-    // alternatively we could lock when mutating the connection list, but it seems not important.\n-    auto cb = [&](unsigned thread_index, util::Connection* conn) {\n-      facade::Connection* dcon = static_cast<facade::Connection*>(conn);\n-      string info = dcon->GetClientInfo(thread_index);\n-      absl::base_internal::SpinLockHolder l(&mu);\n-      client_info.push_back(move(info));\n-    };\n+void ServerFamily::ClientList(CmdArgList args, ConnectionContext* cntx) {\n+  if (!args.empty()) {\n+    return (*cntx)->SendError(facade::kSyntaxErr);\n+  }\n \n-    for (auto* listener : listeners_) {\n-      listener->TraverseConnections(cb);\n-    }\n+  vector<string> client_info;\n+  absl::base_internal::SpinLock mu;\n+\n+  // we can not preempt the connection traversal, so we need to use a spinlock.\n+  // alternatively we could lock when mutating the connection list, but it seems not important.\n+  auto cb = [&](unsigned thread_index, util::Connection* conn) {\n+    facade::Connection* dcon = static_cast<facade::Connection*>(conn);\n+    string info = dcon->GetClientInfo(thread_index);\n+    absl::base_internal::SpinLockHolder l(&mu);\n+    client_info.push_back(std::move(info));\n+  };\n+\n+  for (auto* listener : listeners_) {\n+    listener->TraverseConnections(cb);\n+  }\n+\n+  string result = absl::StrJoin(client_info, \"\\n\");\n+  result.append(\"\\n\");\n+  return (*cntx)->SendBulkString(result);\n+}\n+\n+void ServerFamily::ClientPause(CmdArgList args, ConnectionContext* cntx) {\n+  CmdArgParser parser(args);\n \n-    string result = absl::StrJoin(move(client_info), \"\\n\");\n-    result.append(\"\\n\");\n-    return (*cntx)->SendBulkString(result);\n+  auto timeout = parser.Next().Int<uint64_t>();\n+  enum ClientPause pause_state = ClientPause::ALL;\n+  if (parser.HasNext()) {\n+    pause_state =\n+        parser.ToUpper().Next().Case(\"WRITE\", ClientPause::WRITE).Case(\"ALL\", ClientPause::ALL);\n+  }\n+  if (auto err = parser.Error(); err) {\n+    return (*cntx)->SendError(err->MakeReply());\n   }\n \n-  if (sub_cmd == \"SETINFO\") {\n-    return (*cntx)->SendOk();\n+  // Pause dispatch commands before updating client puase state, and enable dispatch after updating\n+  // pause state. This will unsure that when we after changing the state all running commands will\n+  // read the new pause state, and we will not pause client in the middle of a transaction.\n+  service_.proactor_pool().Await([](util::ProactorBase* pb) {\n+    ServerState& etl = *ServerState::tlocal();\n+    etl.SetPauseDispatch(true);\n+  });\n+\n+  // TODO handle blocking commands\n+  const absl::Duration kDispatchTimeout = absl::Seconds(1);\n+  if (!AwaitDispatches(kDispatchTimeout, [self = cntx->conn()](util::Connection* conn) {\n+        // Wait until the only command dispatching is the client pause command.\n+        return conn != self;\n+      })) {\n+    LOG(WARNING) << \"Couldn't wait for commands to finish dispatching. \" << kDispatchTimeout;\n+    service_.proactor_pool().Await([](util::ProactorBase* pb) {\n+      ServerState& etl = *ServerState::tlocal();\n+      etl.SetPauseDispatch(false);\n+    });\n+    return (*cntx)->SendError(\"Failed to pause all running clients\");\n   }\n \n-  LOG_FIRST_N(ERROR, 10) << \"Subcommand \" << sub_cmd << \" not supported\";\n-  return (*cntx)->SendError(UnknownSubCmd(sub_cmd, \"CLIENT\"), kSyntaxErrType);\n+  service_.proactor_pool().AwaitFiberOnAll([pause_state](util::ProactorBase* pb) {\n+    ServerState& etl = *ServerState::tlocal();\n+    etl.SetPauseState(pause_state, true);\n+    etl.SetPauseDispatch(false);\n+  });\n+\n+  // We should not expire/evict keys while clients are puased.\n+  shard_set->RunBriefInParallel(\n+      [](EngineShard* shard) { shard->db_slice().SetExpireAllowed(false); });\n+\n+  fb2::Fiber(\"client_pause\", [this, timeout, pause_state]() mutable {\n+    // On server shutdown we sleep 10ms to make sure all running task finish, therefore 10ms steps\n+    // ensure this fiber will not left hanging .\n+    auto step = 10ms;\n+    auto timeout_ms = timeout * 1ms;\n+    int64_t steps = timeout_ms.count() / step.count();\n+    ServerState& etl = *ServerState::tlocal();\n+    do {\n+      ThisFiber::SleepFor(step);\n+    } while (etl.gstate() != GlobalState::SHUTTING_DOWN && --steps > 0);\n+\n+    if (etl.gstate() != GlobalState::SHUTTING_DOWN) {\n+      service_.proactor_pool().AwaitFiberOnAll([pause_state](util::ProactorBase* pb) {\n+        ServerState::tlocal()->SetPauseState(pause_state, false);\n+      });\n+      shard_set->RunBriefInParallel(\n+          [](EngineShard* shard) { shard->db_slice().SetExpireAllowed(true); });\n+    }\n+  }).Detach();\n+\n+  (*cntx)->SendOk();\n }\n \n void ServerFamily::Config(CmdArgList args, ConnectionContext* cntx) {\ndiff --git a/src/server/server_family.h b/src/server/server_family.h\nindex a615a60e8e3b..e5b6ca6a7e4f 100644\n--- a/src/server/server_family.h\n+++ b/src/server/server_family.h\n@@ -211,6 +211,10 @@ class ServerFamily {\n \n   void Auth(CmdArgList args, ConnectionContext* cntx);\n   void Client(CmdArgList args, ConnectionContext* cntx);\n+  void ClientSetName(CmdArgList args, ConnectionContext* cntx);\n+  void ClientGetName(CmdArgList args, ConnectionContext* cntx);\n+  void ClientList(CmdArgList args, ConnectionContext* cntx);\n+  void ClientPause(CmdArgList args, ConnectionContext* cntx);\n   void Config(CmdArgList args, ConnectionContext* cntx);\n   void DbSize(CmdArgList args, ConnectionContext* cntx);\n   void Debug(CmdArgList args, ConnectionContext* cntx);\ndiff --git a/src/server/server_state.cc b/src/server/server_state.cc\nindex 7d820c5a4ecb..9419c3f30149 100644\n--- a/src/server/server_state.cc\n+++ b/src/server/server_state.cc\n@@ -112,6 +112,45 @@ bool ServerState::AllowInlineScheduling() const {\n   return true;\n }\n \n+void ServerState::SetPauseState(ClientPause state, bool start) {\n+  client_pauses_[int(state)] += (start ? 1 : -1);\n+  if (!client_pauses_[int(state)]) {\n+    client_pause_ec_.notifyAll();\n+  }\n+}\n+\n+bool ServerState::IsPaused() const {\n+  return client_pauses_[0] || client_pauses_[1];\n+}\n+\n+void ServerState::AwaitPauseState(bool is_write) {\n+  client_pause_ec_.await([is_write, this]() {\n+    if (client_pauses_[int(ClientPause::ALL)]) {\n+      return false;\n+    }\n+    if (is_write && client_pauses_[int(ClientPause::WRITE)]) {\n+      return false;\n+    }\n+    return true;\n+  });\n+}\n+\n+void ServerState::AwaitOnPauseDispatch() {\n+  pause_dispatch_ec_.await([this]() {\n+    if (pause_dispatch_) {\n+      return false;\n+    }\n+    return true;\n+  });\n+}\n+\n+void ServerState::SetPauseDispatch(bool pause) {\n+  pause_dispatch_ = pause;\n+  if (!pause_dispatch_) {\n+    pause_dispatch_ec_.notifyAll();\n+  }\n+}\n+\n Interpreter* ServerState::BorrowInterpreter() {\n   return interpreter_mgr_.Get();\n }\ndiff --git a/src/server/server_state.h b/src/server/server_state.h\nindex fcd015b0eef4..2a2478257c2b 100644\n--- a/src/server/server_state.h\n+++ b/src/server/server_state.h\n@@ -80,6 +80,8 @@ class MonitorsRepo {\n   unsigned int global_count_ = 0;  // by global its means that we count the monitor for all threads\n };\n \n+enum class ClientPause { WRITE, ALL };\n+\n // Present in every server thread. This class differs from EngineShard. The latter manages\n // state around engine shards while the former represents coordinator/connection state.\n // There may be threads that handle engine shards but not IO, there may be threads that handle IO\n@@ -220,6 +222,24 @@ class ServerState {  // public struct - to allow initialization.\n \n   acl::AclLog acl_log;\n \n+  // Starts or ends a `CLIENT PAUSE` command. @state controls whether\n+  // this is pausing only writes or every command, @start controls\n+  // whether this is starting or ending the pause.\n+  void SetPauseState(ClientPause state, bool start);\n+\n+  // Returns whether any type of commands is paused.\n+  bool IsPaused() const;\n+\n+  // Awaits until the pause is over and the command can execute.\n+  // @is_write controls whether the command is a write command or not.\n+  void AwaitPauseState(bool is_write);\n+\n+  // Toggle a boolean indicating whether the server should temporarily pause or allow dispatching\n+  // new commands.\n+  void SetPauseDispatch(bool pause);\n+  // Awaits until dispatching new commands is allowed as determinded by SetPauseDispatch function\n+  void AwaitOnPauseDispatch();\n+\n   SlowLogShard& GetSlowLog() {\n     return slow_log_shard_;\n   };\n@@ -237,6 +257,15 @@ class ServerState {  // public struct - to allow initialization.\n \n   GlobalState gstate_ = GlobalState::ACTIVE;\n \n+  // To support concurrent `CLIENT PAUSE commands` correctly, we store the amount\n+  // of current CLIENT PAUSE commands that are in effect. Blocked execution fibers\n+  // should subscribe to `client_pause_ec_` through `AwaitPauseState` to be\n+  // notified when the break is over.\n+  int client_pauses_[2] = {};\n+  EventCount client_pause_ec_;\n+  bool pause_dispatch_ = false;\n+  EventCount pause_dispatch_ec_;\n+\n   using Counter = util::SlidingCounter<7>;\n   Counter qps_;\n \n",
  "test_patch": "diff --git a/src/server/server_family_test.cc b/src/server/server_family_test.cc\nindex 07bdb956f408..e0a12c26956e 100644\n--- a/src/server/server_family_test.cc\n+++ b/src/server/server_family_test.cc\n@@ -4,6 +4,8 @@\n \n #include \"server/server_family.h\"\n \n+#include <absl/strings/match.h>\n+\n #include \"base/gtest.h\"\n #include \"base/logging.h\"\n #include \"facade/facade_test.h\"\n@@ -181,4 +183,21 @@ TEST_F(ServerFamilyTest, SlowLogMinusOneDisabled) {\n   EXPECT_THAT(resp.GetInt(), 0);\n }\n \n+TEST_F(ServerFamilyTest, ClientPause) {\n+  auto start = absl::Now();\n+  Run({\"CLIENT\", \"PAUSE\", \"50\"});\n+\n+  Run({\"get\", \"key\"});\n+  EXPECT_GT((absl::Now() - start), absl::Milliseconds(50));\n+\n+  start = absl::Now();\n+\n+  Run({\"CLIENT\", \"PAUSE\", \"50\", \"WRITE\"});\n+\n+  Run({\"get\", \"key\"});\n+  EXPECT_LT((absl::Now() - start), absl::Milliseconds(10));\n+  Run({\"set\", \"key\", \"value2\"});\n+  EXPECT_GT((absl::Now() - start), absl::Milliseconds(50));\n+}\n+\n }  // namespace dfly\ndiff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 2b68c511e9ee..309d9936fbe7 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -1761,3 +1761,49 @@ async def test_search(df_local_factory):\n     assert (await c_replica.ft(\"idx-m2\").search(Query(\"*\").sort_by(\"f2\").paging(0, 1))).docs[\n         0\n     ].id == \"k0\"\n+\n+\n+# @pytest.mark.slow\n+@pytest.mark.asyncio\n+async def test_client_pause_with_replica(df_local_factory, df_seeder_factory):\n+    master = df_local_factory.create(proactor_threads=4)\n+    replica = df_local_factory.create(proactor_threads=4)\n+    df_local_factory.start_all([master, replica])\n+\n+    seeder = df_seeder_factory.create(port=master.port)\n+\n+    c_master = master.client()\n+    c_replica = replica.client()\n+\n+    await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n+    await wait_available_async(c_replica)\n+\n+    fill_task = asyncio.create_task(seeder.run())\n+\n+    # Give the seeder a bit of time.\n+    await asyncio.sleep(1)\n+    # block the seeder for 4 seconds\n+    await c_master.execute_command(\"client pause 4000 write\")\n+    stats = await c_master.info(\"CommandStats\")\n+    info = await c_master.info(\"Stats\")\n+    await asyncio.sleep(0.5)\n+    stats_after_sleep = await c_master.info(\"CommandStats\")\n+    # Check no commands are executed except info and replconf called from replica\n+    for cmd, cmd_stats in stats_after_sleep.items():\n+        if \"cmdstat_INFO\" != cmd and \"cmdstat_REPLCONF\" != cmd_stats:\n+            assert stats[cmd] == cmd_stats\n+\n+    await asyncio.sleep(6)\n+    seeder.stop()\n+    await fill_task\n+    stats_after_pause_finish = await c_master.info(\"CommandStats\")\n+    more_exeuted = False\n+    for cmd, cmd_stats in stats_after_pause_finish.items():\n+        if \"cmdstat_INFO\" != cmd and \"cmdstat_REPLCONF\" != cmd_stats and stats[cmd] != cmd_stats:\n+            more_exeuted = True\n+    assert more_exeuted\n+\n+    capture = await seeder.capture(port=master.port)\n+    assert await seeder.compare(capture, port=replica.port)\n+\n+    await disconnect_clients(c_master, c_replica)\n",
  "problem_statement": "Add support for CLIENT PAUSE command\n\n",
  "hints_text": "",
  "created_at": "2023-09-18T13:21:42Z",
  "modified_files": [
    "src/facade/dragonfly_connection.cc",
    "src/facade/ok_main.cc",
    "src/facade/service_interface.h",
    "src/server/conn_context.cc",
    "src/server/conn_context.h",
    "src/server/main_service.cc",
    "src/server/main_service.h",
    "src/server/multi_command_squasher.h",
    "src/server/server_family.cc",
    "src/server/server_family.h",
    "src/server/server_state.cc",
    "src/server/server_state.h"
  ],
  "modified_test_files": [
    "src/server/server_family_test.cc",
    "tests/dragonfly/replication_test.py"
  ]
}