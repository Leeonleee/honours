{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 3457,
  "instance_id": "dragonflydb__dragonfly-3457",
  "issue_numbers": [
    "3456"
  ],
  "base_commit": "5db8d27363b588da84503d88317f9964c466c17c",
  "patch": "diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\nindex 40af15207b85..a1a3ed23314b 100644\n--- a/.github/workflows/ci.yml\n+++ b/.github/workflows/ci.yml\n@@ -118,6 +118,7 @@ jobs:\n           df -h\n           echo \"-----------------------------\"\n           ninja src/all\n+\n       - name: PostFail\n         if: failure()\n         run: |\n@@ -155,7 +156,7 @@ jobs:\n           timeout 5m ./multi_test --multi_exec_mode=1\n           timeout 5m ./multi_test --multi_exec_mode=3\n           timeout 5m ./json_family_test --jsonpathv2=false\n-\n+          timeout 5m ./tiered_storage_test --vmodule=db_slice=2 --logtostderr\n       - name: Upload unit logs on failure\n         if: failure()\n         uses: actions/upload-artifact@v4\ndiff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex 2992e6d2fc33..a70b776c6084 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -73,10 +73,12 @@ class PrimeEvictionPolicy {\n   static constexpr bool can_evict = true;  // we implement eviction functionality.\n   static constexpr bool can_gc = true;\n \n-  PrimeEvictionPolicy(const DbContext& cntx, bool can_evict, ssize_t mem_budget, ssize_t soft_limit,\n+  // mem_offset - memory_offset that we should account for in addition to DbSlice::memory_budget.\n+  // May be negative.\n+  PrimeEvictionPolicy(const DbContext& cntx, bool can_evict, ssize_t mem_offset, ssize_t soft_limit,\n                       DbSlice* db_slice, bool apply_memory_limit)\n       : db_slice_(db_slice),\n-        mem_budget_(mem_budget),\n+        mem_offset_(mem_offset),\n         soft_limit_(soft_limit),\n         cntx_(cntx),\n         can_evict_(can_evict),\n@@ -85,7 +87,6 @@ class PrimeEvictionPolicy {\n \n   // A hook function that is called every time a segment is full and requires splitting.\n   void RecordSplit(PrimeTable::Segment_t* segment) {\n-    mem_budget_ -= PrimeTable::kSegBytes;\n     DVLOG(2) << \"split: \" << segment->SlowSize() << \"/\" << segment->capacity();\n   }\n \n@@ -94,10 +95,6 @@ class PrimeEvictionPolicy {\n   unsigned GarbageCollect(const PrimeTable::HotspotBuckets& eb, PrimeTable* me);\n   unsigned Evict(const PrimeTable::HotspotBuckets& eb, PrimeTable* me);\n \n-  ssize_t mem_budget() const {\n-    return mem_budget_;\n-  }\n-\n   unsigned evicted() const {\n     return evicted_;\n   }\n@@ -108,7 +105,7 @@ class PrimeEvictionPolicy {\n \n  private:\n   DbSlice* db_slice_;\n-  ssize_t mem_budget_;\n+  ssize_t mem_offset_;\n   ssize_t soft_limit_ = 0;\n   const DbContext cntx_;\n \n@@ -136,7 +133,8 @@ class PrimeBumpPolicy {\n };\n \n bool PrimeEvictionPolicy::CanGrow(const PrimeTable& tbl) const {\n-  if (!apply_memory_limit_ || mem_budget_ > soft_limit_)\n+  ssize_t mem_available = db_slice_->memory_budget() + mem_offset_;\n+  if (!apply_memory_limit_ || mem_available > soft_limit_)\n     return true;\n \n   DCHECK_LE(tbl.size(), tbl.capacity());\n@@ -145,11 +143,11 @@ bool PrimeEvictionPolicy::CanGrow(const PrimeTable& tbl) const {\n   // we estimate how much memory we will take with the current capacity\n   // even though we may currently use less memory.\n   // see https://github.com/dragonflydb/dragonfly/issues/256#issuecomment-1227095503\n-  size_t new_available = (tbl.capacity() - tbl.size()) + PrimeTable::kSegCapacity;\n-  bool res =\n-      mem_budget_ > int64_t(PrimeTable::kSegBytes + db_slice_->bytes_per_object() * new_available *\n-                                                        GetFlag(FLAGS_table_growth_margin));\n-  VLOG(2) << \"available: \" << new_available << \", res: \" << res;\n+  size_t table_free_items = (tbl.capacity() - tbl.size()) + PrimeTable::kSegCapacity;\n+  size_t obj_bytes_estimation =\n+      db_slice_->bytes_per_object() * table_free_items * GetFlag(FLAGS_table_growth_margin);\n+  bool res = mem_available > int64_t(PrimeTable::kSegBytes + obj_bytes_estimation);\n+  VLOG(2) << \"available: \" << table_free_items << \", res: \" << res;\n \n   return res;\n }\n@@ -575,12 +573,21 @@ OpResult<DbSlice::AddOrFindResult> DbSlice::AddOrFindInternal(const Context& cnt\n   // It's a new entry.\n   CallChangeCallbacks(cntx.db_index, key, {key});\n \n+  ssize_t memory_offset = -key.size();\n+\n   // If we are low on memory due to cold storage, free some memory.\n-  if (memory_budget_ < ssize_t(key.size()) && owner_->tiered_storage()) {\n+  if (owner_->tiered_storage()) {\n     // At least 40KB bytes to cover potential segment split.\n-    size_t goal = std::max<size_t>(key.size() * 2 - memory_budget_, 40_KB);\n-    size_t reclaimed = owner_->tiered_storage()->ReclaimMemory(goal);\n-    memory_budget_ += reclaimed;\n+    ssize_t red_line = std::max<size_t>(key.size() * 2, 40_KB);\n+    if (memory_budget_ < red_line) {\n+      size_t goal = red_line - memory_budget_;\n+      size_t reclaimed = owner_->tiered_storage()->ReclaimMemory(goal);\n+      memory_budget_ += reclaimed;\n+    }\n+\n+    // CoolMemoryUsage is the memory that we can always reclaim, like in the block above,\n+    // therefore we include it for PrimeEvictionPolicy considerations.\n+    memory_offset += owner_->tiered_storage()->CoolMemoryUsage();\n   }\n \n   // In case we are loading from rdb file or replicating we want to disable conservative memory\n@@ -594,37 +601,36 @@ OpResult<DbSlice::AddOrFindResult> DbSlice::AddOrFindInternal(const Context& cnt\n   bool apply_memory_limit =\n       !owner_->IsReplica() && !(ServerState::tlocal()->gstate() == GlobalState::LOADING);\n \n-  PrimeEvictionPolicy evp{cntx,\n-                          (bool(caching_mode_) && !owner_->IsReplica()),\n-                          int64_t(memory_budget_ - key.size()),\n-                          ssize_t(soft_budget_limit_),\n-                          this,\n-                          apply_memory_limit};\n-\n   // If we are over limit in non-cache scenario, just be conservative and throw.\n-  if (apply_memory_limit && !caching_mode_ && evp.mem_budget() < 0) {\n-    VLOG(2) << \"AddOrFind: over limit, budget: \" << evp.mem_budget();\n+  if (apply_memory_limit && !caching_mode_ && memory_budget_ + memory_offset < 0) {\n+    VLOG(2) << \"AddOrFind: over limit, budget: \" << memory_budget_;\n     events_.insertion_rejections++;\n     return OpStatus::OUT_OF_MEMORY;\n   }\n \n+  PrimeEvictionPolicy evp{cntx,          (bool(caching_mode_) && !owner_->IsReplica()),\n+                          memory_offset, ssize_t(soft_budget_limit_),\n+                          this,          apply_memory_limit};\n+\n   // Fast-path if change_cb_ is empty so we Find or Add using\n   // the insert operation: twice more efficient.\n   CompactObj co_key{key};\n   PrimeIterator it;\n \n-  size_t table_before = db.table_memory();\n+  ssize_t table_before = db.prime.mem_usage();\n \n   try {\n     it = db.prime.InsertNew(std::move(co_key), PrimeValue{}, evp);\n   } catch (bad_alloc& e) {\n-    VLOG(2) << \"AddOrFind2: bad alloc exception, budget: \" << evp.mem_budget();\n+    VLOG(2) << \"AddOrFind: bad alloc exception, budget: \" << memory_budget_ + memory_offset;\n     events_.insertion_rejections++;\n     return OpStatus::OUT_OF_MEMORY;\n   }\n \n-  size_t evicted_obj_bytes = 0;\n-  if (evp.mem_budget() < 0 && apply_memory_limit) {\n+  ssize_t table_increase = db.prime.mem_usage() - table_before;\n+  memory_budget_ -= table_increase;\n+\n+  if (memory_budget_ < 0 && apply_memory_limit) {\n     // We may reach the state when our memory usage is below the limit even if we\n     // do not add new segments. For example, we have half full segments\n     // and we add new objects or update the existing ones and our memory usage grows.\n@@ -636,12 +642,11 @@ OpResult<DbSlice::AddOrFindResult> DbSlice::AddOrFindInternal(const Context& cnt\n #if 0\n     size_t evict_goal = std::max<size_t>(512, (-evp.mem_budget()) / 32);\n     auto [items, bytes] = FreeMemWithEvictionStep(cntx.db_index, it.segment_id(), evict_goal);\n-    evicted_obj_bytes = bytes;\n     events_.hard_evictions += items;\n #endif\n   }\n \n-  table_memory_ += (db.table_memory() - table_before);\n+  table_memory_ += table_increase;\n   entries_count_++;\n \n   db.stats.inline_keys += it->first.IsInline();\n@@ -654,8 +659,6 @@ OpResult<DbSlice::AddOrFindResult> DbSlice::AddOrFindInternal(const Context& cnt\n   events_.stash_unloaded = db.prime.stash_unloaded();\n   events_.evicted_keys += evp.evicted();\n   events_.garbage_checked += evp.checked();\n-\n-  memory_budget_ = evp.mem_budget() + evicted_obj_bytes;\n   if (cluster::IsClusterEnabled()) {\n     cluster::SlotId sid = cluster::KeySlot(key);\n     db.slots_stats[sid].key_count += 1;\n@@ -1470,9 +1473,9 @@ void DbSlice::PerformDeletion(Iterator del_it, ExpIterator exp_it, DbTable* tabl\n     shard_owner()->tiered_storage()->Delete(table->index, &del_it->second);\n   }\n \n-  size_t value_heap_size = pv.MallocUsed();\n+  ssize_t value_heap_size = pv.MallocUsed(), key_size_used = del_it->first.MallocUsed();\n   stats.inline_keys -= del_it->first.IsInline();\n-  AccountObjectMemory(del_it.key(), del_it->first.ObjType(), -del_it->first.MallocUsed(),\n+  AccountObjectMemory(del_it.key(), del_it->first.ObjType(), -key_size_used,\n                       table);                                                // Key\n   AccountObjectMemory(del_it.key(), pv.ObjType(), -value_heap_size, table);  // Value\n   if (pv.ObjType() == OBJ_HASH && pv.Encoding() == kEncodingListPack) {\n@@ -1487,8 +1490,14 @@ void DbSlice::PerformDeletion(Iterator del_it, ExpIterator exp_it, DbTable* tabl\n   }\n \n   table->prime.Erase(del_it.GetInnerIt());\n-  table_memory_ += (table->table_memory() - table_before);\n+\n+  // Note, currently we do not shrink our tables upon deletion.\n+  // This DCHECK ensures that if we decide to do so, we will have to update table_memory_\n+  // accordingly.\n+  DCHECK_EQ(table->table_memory(), table_before);\n+\n   --entries_count_;\n+  memory_budget_ += (value_heap_size + key_size_used);\n \n   SendInvalidationTrackingMessage(del_it.key());\n }\ndiff --git a/src/server/db_slice.h b/src/server/db_slice.h\nindex 034d76d0dcc4..e2719bfc913c 100644\n--- a/src/server/db_slice.h\n+++ b/src/server/db_slice.h\n@@ -575,7 +575,7 @@ class DbSlice {\n   bool expire_allowed_ = true;\n \n   uint64_t version_ = 1;  // Used to version entries in the PrimeTable.\n-  ssize_t memory_budget_ = SSIZE_MAX;\n+  ssize_t memory_budget_ = SSIZE_MAX / 2;\n   size_t bytes_per_object_ = 0;\n   size_t soft_budget_limit_ = 0;\n   size_t table_memory_ = 0;\n",
  "test_patch": "diff --git a/src/server/tiered_storage_test.cc b/src/server/tiered_storage_test.cc\nindex 4100b39f991f..82ebac000766 100644\n--- a/src/server/tiered_storage_test.cc\n+++ b/src/server/tiered_storage_test.cc\n@@ -310,8 +310,7 @@ TEST_F(TieredStorageTest, MemoryPressure) {\n       resp = Run({\"INFO\", \"ALL\"});\n       ASSERT_FALSE(true) << i << \"\\nInfo ALL:\\n\" << resp.GetString();\n     }\n-    // TODO: to remove it once used_mem_current is updated frequently.\n-    ThisFiber::SleepFor(300us);\n+    ThisFiber::SleepFor(100us);\n   }\n \n   EXPECT_LT(used_mem_peak.load(), 20_MB);\n",
  "problem_statement": "tiered_storage_test memory pressure fails\nhttps://github.com/dragonflydb/dragonfly/actions/runs/10261477794/job/28389293919?pr=3455#step:10:5287 \r\n\r\n```\r\n2024-08-06T06:55:27.8418416Z 49: ../src/server/tiered_storage_test.cc:311: Failure\r\n2024-08-06T06:55:27.8419232Z 49: Value of: true\r\n2024-08-06T06:55:27.8420000Z 49:   Actual: true\r\n2024-08-06T06:55:27.8420518Z 49: Expected: false\r\n```\n",
  "hints_text": "",
  "created_at": "2024-08-06T11:02:17Z",
  "modified_files": [
    ".github/workflows/ci.yml",
    "src/server/db_slice.cc",
    "src/server/db_slice.h"
  ],
  "modified_test_files": [
    "src/server/tiered_storage_test.cc"
  ]
}