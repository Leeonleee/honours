{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 1058,
  "instance_id": "dragonflydb__dragonfly-1058",
  "issue_numbers": [
    "1017"
  ],
  "base_commit": "0e7d137046d9aee20424311e4258639720d11f3c",
  "patch": "diff --git a/src/server/command_registry.h b/src/server/command_registry.h\nindex 25c32d7a35cc..6bc1ac7f7c0d 100644\n--- a/src/server/command_registry.h\n+++ b/src/server/command_registry.h\n@@ -22,7 +22,7 @@ enum CommandOpt : uint32_t {\n   READONLY = 1U << 0,\n   FAST = 1U << 1,\n   WRITE = 1U << 2,\n-  LOADING = 1U << 3,\n+  LOADING = 1U << 3,  // Command allowed during LOADING state.\n   DENYOOM = 1U << 4,  // use-memory in redis.\n   REVERSE_MAPPING = 1U << 5,\n \ndiff --git a/src/server/generic_family.cc b/src/server/generic_family.cc\nindex 501ca59942ce..a7dcc4d195a9 100644\n--- a/src/server/generic_family.cc\n+++ b/src/server/generic_family.cc\n@@ -1442,7 +1442,7 @@ void GenericFamily::Register(CommandRegistry* registry) {\n   constexpr auto kSelectOpts = CO::LOADING | CO::FAST | CO::NOSCRIPT;\n \n   *registry << CI{\"DEL\", CO::WRITE, -2, 1, -1, 1}.HFUNC(Del)\n-            /* Redis compaitibility:\n+            /* Redis compatibility:\n              * We don't allow PING during loading since in Redis PING is used as\n              * failure detection, and a loading server is considered to be\n              * not available. */\ndiff --git a/src/server/rdb_save.cc b/src/server/rdb_save.cc\nindex 43f7d61fb733..d2c89486a7c9 100644\n--- a/src/server/rdb_save.cc\n+++ b/src/server/rdb_save.cc\n@@ -1032,7 +1032,7 @@ error_code RdbSaver::Impl::ConsumeChannel(const Cancellation* cll) {\n     pushed_bytes += ptr->channel_bytes();\n   }\n \n-  DCHECK(!channel_.TryPop(*record));\n+  DCHECK(!record.has_value() || !channel_.TryPop(*record));\n \n   VLOG(1) << \"Channel pulled bytes: \" << channel_bytes << \" pushed bytes: \" << pushed_bytes;\n \ndiff --git a/src/server/replica.cc b/src/server/replica.cc\nindex 842f5ce66a2c..3765a44224f4 100644\n--- a/src/server/replica.cc\n+++ b/src/server/replica.cc\n@@ -150,55 +150,55 @@ Replica::~Replica() {\n \n static const char kConnErr[] = \"could not connect to master: \";\n \n-bool Replica::Start(ConnectionContext* cntx) {\n+error_code Replica::Start(ConnectionContext* cntx) {\n   VLOG(1) << \"Starting replication\";\n   ProactorBase* mythread = ProactorBase::me();\n   CHECK(mythread);\n \n+  RETURN_ON_ERR(cntx_.SwitchErrorHandler(absl::bind_front(&Replica::DefaultErrorHandler, this)));\n+\n+  auto check_connection_error = [this, &cntx](const error_code& ec, const char* msg) -> error_code {\n+    if (cntx_.IsCancelled()) {\n+      (*cntx)->SendError(\"replication cancelled\");\n+      return std::make_error_code(errc::operation_canceled);\n+    }\n+    if (ec) {\n+      (*cntx)->SendError(absl::StrCat(msg, ec.message()));\n+      cntx_.Cancel();\n+      return ec;\n+    }\n+    return {};\n+  };\n+\n   // 1. Resolve dns.\n   VLOG(1) << \"Resolving master DNS\";\n   error_code ec = ResolveMasterDns();\n-  if (ec) {\n-    (*cntx)->SendError(StrCat(\"could not resolve master dns\", ec.message()));\n-    return false;\n-  }\n+  RETURN_ON_ERR(check_connection_error(ec, \"could not resolve master dns\"));\n+\n   // 2. Connect socket.\n   VLOG(1) << \"Connecting to master\";\n   ec = ConnectAndAuth(absl::GetFlag(FLAGS_master_connect_timeout_ms) * 1ms);\n-  if (ec) {\n-    (*cntx)->SendError(StrCat(kConnErr, ec.message()));\n-    return false;\n-  }\n+  RETURN_ON_ERR(check_connection_error(ec, kConnErr));\n \n   // 3. Greet.\n   VLOG(1) << \"Greeting\";\n-  state_mask_ = R_ENABLED | R_TCP_CONNECTED;\n+  state_mask_.store(R_ENABLED | R_TCP_CONNECTED);\n   last_io_time_ = mythread->GetMonotonicTimeNs();\n   ec = Greet();\n-  if (ec) {\n-    (*cntx)->SendError(StrCat(\"could not greet master \", ec.message()));\n-    return false;\n-  }\n-\n-  // 4. Init basic context.\n-  cntx_.Reset(absl::bind_front(&Replica::DefaultErrorHandler, this));\n+  RETURN_ON_ERR(check_connection_error(ec, \"could not greet master \"));\n \n-  // 5. Spawn main coordination fiber.\n+  // 4. Spawn main coordination fiber.\n   sync_fb_ = MakeFiber(&Replica::MainReplicationFb, this);\n \n   (*cntx)->SendOk();\n-  return true;\n-}\n+  return {};\n+}  // namespace dfly\n \n void Replica::Stop() {\n   VLOG(1) << \"Stopping replication\";\n-  // Mark disabled, prevent from retrying.\n-  if (sock_) {\n-    sock_->proactor()->Await([this] {\n-      state_mask_ = 0;  // Specifically ~R_ENABLED.\n-      cntx_.Cancel();   // Context is fully resposible for cleanup.\n-    });\n-  }\n+  // Stops the loop in MainReplicationFb.\n+  state_mask_.store(0);  // Specifically ~R_ENABLED.\n+  cntx_.Cancel();        // Context is fully resposible for cleanup.\n \n   // Make sure the replica fully stopped and did all cleanup,\n   // so we can freely release resources (connections).\n@@ -217,11 +217,11 @@ void Replica::MainReplicationFb() {\n   SetShardStates(true);\n \n   error_code ec;\n-  while (state_mask_ & R_ENABLED) {\n+  while (state_mask_.load() & R_ENABLED) {\n     // Discard all previous errors and set default error handler.\n     cntx_.Reset(absl::bind_front(&Replica::DefaultErrorHandler, this));\n     // 1. Connect socket.\n-    if ((state_mask_ & R_TCP_CONNECTED) == 0) {\n+    if ((state_mask_.load() & R_TCP_CONNECTED) == 0) {\n       ThisFiber::SleepFor(500ms);\n       if (is_paused_)\n         continue;\n@@ -239,22 +239,25 @@ void Replica::MainReplicationFb() {\n         continue;\n       }\n       VLOG(1) << \"Replica socket connected\";\n-      state_mask_ |= R_TCP_CONNECTED;\n+      state_mask_.fetch_or(R_TCP_CONNECTED);\n+      continue;\n     }\n \n     // 2. Greet.\n-    if ((state_mask_ & R_GREETED) == 0) {\n+    if ((state_mask_.load() & R_GREETED) == 0) {\n       ec = Greet();\n       if (ec) {\n         LOG(INFO) << \"Error greeting \" << master_context_.Description() << \" \" << ec << \" \"\n                   << ec.message();\n-        state_mask_ &= R_ENABLED;\n+        state_mask_.fetch_and(R_ENABLED);\n         continue;\n       }\n+      state_mask_.fetch_or(R_GREETED);\n+      continue;\n     }\n \n     // 3. Initiate full sync\n-    if ((state_mask_ & R_SYNC_OK) == 0) {\n+    if ((state_mask_.load() & R_SYNC_OK) == 0) {\n       if (HasDflyMaster())\n         ec = InitiateDflySync();\n       else\n@@ -263,14 +266,15 @@ void Replica::MainReplicationFb() {\n       if (ec) {\n         LOG(WARNING) << \"Error syncing with \" << master_context_.Description() << \" \" << ec << \" \"\n                      << ec.message();\n-        state_mask_ &= R_ENABLED;  // reset all flags besides R_ENABLED\n+        state_mask_.fetch_and(R_ENABLED);  // reset all flags besides R_ENABLED\n         continue;\n       }\n-      state_mask_ |= R_SYNC_OK;\n+      state_mask_.fetch_or(R_SYNC_OK);\n+      continue;\n     }\n \n     // 4. Start stable state sync.\n-    DCHECK(state_mask_ & R_SYNC_OK);\n+    DCHECK(state_mask_.load() & R_SYNC_OK);\n \n     if (HasDflyMaster())\n       ec = ConsumeDflyStream();\n@@ -279,7 +283,7 @@ void Replica::MainReplicationFb() {\n \n     LOG(WARNING) << \"Error full sync with \" << master_context_.Description() << \" \" << ec << \" \"\n                  << ec.message();\n-    state_mask_ &= R_ENABLED;\n+    state_mask_.fetch_and(R_ENABLED);\n   }\n \n   // Wait for unblocking cleanup to finish.\n@@ -307,7 +311,16 @@ error_code Replica::ResolveMasterDns() {\n error_code Replica::ConnectAndAuth(std::chrono::milliseconds connect_timeout_ms) {\n   ProactorBase* mythread = ProactorBase::me();\n   CHECK(mythread);\n-  sock_.reset(mythread->CreateSocket());\n+  {\n+    unique_lock lk(sock_mu_);\n+    // The context closes sock_. So if the context error handler has already\n+    // run we must not create a new socket. sock_mu_ syncs between the two\n+    // functions.\n+    if (!cntx_.IsCancelled())\n+      sock_.reset(mythread->CreateSocket());\n+    else\n+      return cntx_.GetError();\n+  }\n \n   // We set this timeout because this call blocks other REPLICAOF commands. We don't need it for the\n   // rest of the sync.\n@@ -440,7 +453,7 @@ error_code Replica::Greet() {\n   }\n \n   io_buf.ConsumeInput(consumed);\n-  state_mask_ |= R_GREETED;\n+  state_mask_.fetch_or(R_GREETED);\n   return error_code{};\n }\n \n@@ -478,7 +491,7 @@ error_code Replica::InitiatePSync() {\n   // we get the snapshot size.\n   if (snapshot_size || token != nullptr) {  // full sync\n     // Start full sync\n-    state_mask_ |= R_SYNCING;\n+    state_mask_.fetch_or(R_SYNCING);\n \n     io::PrefixSource ps{io_buf.InputBuffer(), sock_.get()};\n \n@@ -519,8 +532,8 @@ error_code Replica::InitiatePSync() {\n     last_io_time_ = sock_thread->GetMonotonicTimeNs();\n   }\n \n-  state_mask_ &= ~R_SYNCING;\n-  state_mask_ |= R_SYNC_OK;\n+  state_mask_.fetch_and(~R_SYNCING);\n+  state_mask_.fetch_or(R_SYNC_OK);\n \n   // There is a data race condition in Redis-master code, where \"ACK 0\" handler may be\n   // triggered before Redis is ready to transition to the streaming state and it silenty ignores\n@@ -538,7 +551,7 @@ error_code Replica::InitiateDflySync() {\n     // We do the following operations regardless of outcome.\n     JoinAllFlows();\n     service_.SwitchState(GlobalState::LOADING, GlobalState::ACTIVE);\n-    state_mask_ &= ~R_SYNCING;\n+    state_mask_.fetch_and(~R_SYNCING);\n   };\n \n   // Initialize MultiShardExecution.\n@@ -575,7 +588,7 @@ error_code Replica::InitiateDflySync() {\n   JournalExecutor{&service_}.FlushAll();\n \n   // Start full sync flows.\n-  state_mask_ |= R_SYNCING;\n+  state_mask_.fetch_or(R_SYNCING);\n   {\n     auto partition = Partition(num_df_flows_);\n     auto shard_cb = [&](unsigned index, auto*) {\n@@ -722,6 +735,7 @@ error_code Replica::ConsumeDflyStream() {\n }\n \n void Replica::CloseSocket() {\n+  unique_lock lk(sock_mu_);\n   if (sock_) {\n     sock_->proactor()->Await([this] {\n       auto ec = sock_->Shutdown(SHUT_RDWR);\n@@ -804,7 +818,7 @@ error_code Replica::StartFullSyncFlow(BlockingCounter sb, Context* cntx) {\n   }\n   leftover_buf_->ConsumeInput(consumed);\n \n-  state_mask_ = R_ENABLED | R_TCP_CONNECTED;\n+  state_mask_.fetch_or(R_TCP_CONNECTED);\n \n   // We can not discard io_buf because it may contain data\n   // besides the response we parsed. Therefore we pass it further to ReplicateDFFb.\n@@ -1213,8 +1227,8 @@ Replica::Info Replica::GetInfo() const {\n     Info res;\n     res.host = master_context_.host;\n     res.port = master_context_.port;\n-    res.master_link_established = (state_mask_ & R_TCP_CONNECTED);\n-    res.sync_in_progress = (state_mask_ & R_SYNCING);\n+    res.master_link_established = (state_mask_.load() & R_TCP_CONNECTED);\n+    res.sync_in_progress = (state_mask_.load() & R_SYNCING);\n     res.master_last_io_sec = (ProactorBase::GetMonotonicTimeNs() - last_io_time) / 1000000000UL;\n     return res;\n   });\ndiff --git a/src/server/replica.h b/src/server/replica.h\nindex f2cc40c88635..019b2a73f823 100644\n--- a/src/server/replica.h\n+++ b/src/server/replica.h\n@@ -103,7 +103,7 @@ class Replica {\n   // Spawns a fiber that runs until link with master is broken or the replication is stopped.\n   // Returns true if initial link with master has been established or\n   // false if it has failed.\n-  bool Start(ConnectionContext* cntx);\n+  std::error_code Start(ConnectionContext* cntx);\n \n   void Stop();  // thread-safe\n \n@@ -221,6 +221,7 @@ class Replica {\n   Service& service_;\n   MasterContext master_context_;\n   std::unique_ptr<util::LinuxSocketBase> sock_;\n+  Mutex sock_mu_;\n \n   std::shared_ptr<MultiShardExecution> multi_shard_exe_;\n \n@@ -253,7 +254,7 @@ class Replica {\n   // ack_offs_ last acknowledged offset.\n   size_t repl_offs_ = 0, ack_offs_ = 0;\n   uint64_t last_io_time_ = 0;  // in ns, monotonic clock.\n-  unsigned state_mask_ = 0;\n+  std::atomic<unsigned> state_mask_ = 0;\n   unsigned num_df_flows_ = 0;\n \n   bool is_paused_ = false;\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 5d55fd8251cb..7e6a4239629e 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -1822,11 +1822,20 @@ void ServerFamily::ReplicaOf(CmdArgList args, ConnectionContext* cntx) {\n \n   LOG(INFO) << \"Replicating \" << host << \":\" << port_s;\n \n-  if (absl::EqualsIgnoreCase(host, \"no\") && absl::EqualsIgnoreCase(port_s, \"one\")) {\n-    // use this lock as critical section to prevent concurrent replicaof commands running.\n-    VLOG(1) << \"Acquire replica lock\";\n-    unique_lock lk(replicaof_mu_);\n+  // We lock to protect global state changes that we perform during the replication setup:\n+  // The replica_ pointer, GlobalState, and the DB itself (we do a flushall txn before syncing).\n+  // The lock is only released during replica_->Start because we want to allow cancellation during\n+  // the connection. If another replication command is received during Start() of an old\n+  // replication, it will acquire the lock, call Stop() on the old replica_ and wait for Stop() to\n+  // complete. So Replica::Stop() must\n+  // 1. Be very responsive, as it is called while holding the lock.\n+  // 2. Leave the DB in a consistent state after it is done.\n+  // We have a relatively involved state machine inside Replica itself which handels cancellation\n+  // with those requirements.\n+  VLOG(1) << \"Acquire replica lock\";\n+  unique_lock lk(replicaof_mu_);\n \n+  if (absl::EqualsIgnoreCase(host, \"no\") && absl::EqualsIgnoreCase(port_s, \"one\")) {\n     if (!ServerState::tlocal()->is_master) {\n       auto repl_ptr = replica_;\n       CHECK(repl_ptr);\n@@ -1849,8 +1858,6 @@ void ServerFamily::ReplicaOf(CmdArgList args, ConnectionContext* cntx) {\n \n   auto new_replica = make_shared<Replica>(string(host), port, &service_, master_id());\n \n-  VLOG(1) << \"Acquire replica lock\";\n-  unique_lock lk(replicaof_mu_);\n   if (replica_) {\n     replica_->Stop();  // NOTE: consider introducing update API flow.\n   } else {\n@@ -1858,8 +1865,7 @@ void ServerFamily::ReplicaOf(CmdArgList args, ConnectionContext* cntx) {\n \n     pool.AwaitFiberOnAll([&](util::ProactorBase* pb) { ServerState::tlocal()->is_master = false; });\n   }\n-\n-  replica_.swap(new_replica);\n+  replica_ = new_replica;\n \n   GlobalState new_state = service_.SwitchState(GlobalState::ACTIVE, GlobalState::LOADING);\n   if (new_state != GlobalState::LOADING) {\n@@ -1879,14 +1885,22 @@ void ServerFamily::ReplicaOf(CmdArgList args, ConnectionContext* cntx) {\n \n   // Replica sends response in either case. No need to send response in this function.\n   // It's a bit confusing but simpler.\n-  if (!replica_->Start(cntx)) {\n-    service_.SwitchState(GlobalState::LOADING, GlobalState::ACTIVE);\n-    replica_.reset();\n-  }\n+  lk.unlock();\n+  error_code ec = new_replica->Start(cntx);\n+  VLOG(1) << \"Acquire replica lock\";\n+  lk.lock();\n \n-  bool is_master = !replica_;\n-  pool.AwaitFiberOnAll(\n-      [&](util::ProactorBase* pb) { ServerState::tlocal()->is_master = is_master; });\n+  // Since we released the replication lock during Start(..), we need to check if this still the\n+  // last replicaof command we got. If it's not, then we were cancelled and just exit.\n+  if (replica_ == new_replica) {\n+    if (ec) {\n+      service_.SwitchState(GlobalState::LOADING, GlobalState::ACTIVE);\n+      replica_.reset();\n+    }\n+    bool is_master = !replica_;\n+    pool.AwaitFiberOnAll(\n+        [&](util::ProactorBase* pb) { ServerState::tlocal()->is_master = is_master; });\n+  }\n }\n \n void ServerFamily::ReplConf(CmdArgList args, ConnectionContext* cntx) {\n@@ -2041,7 +2055,7 @@ void ServerFamily::Dfly(CmdArgList args, ConnectionContext* cntx) {\n #define HFUNC(x) SetHandler(HandlerFunc(this, &ServerFamily::x))\n \n void ServerFamily::Register(CommandRegistry* registry) {\n-  constexpr auto kReplicaOpts = CO::ADMIN | CO::GLOBAL_TRANS;\n+  constexpr auto kReplicaOpts = CO::LOADING | CO::ADMIN | CO::GLOBAL_TRANS;\n   constexpr auto kMemOpts = CO::LOADING | CO::READONLY | CO::FAST | CO::NOSCRIPT;\n \n   *registry << CI{\"AUTH\", CO::NOSCRIPT | CO::FAST | CO::LOADING, -2, 0, 0, 0}.HFUNC(Auth)\ndiff --git a/src/server/snapshot.cc b/src/server/snapshot.cc\nindex 1c38df3ed7a1..c51bfbdfc5b6 100644\n--- a/src/server/snapshot.cc\n+++ b/src/server/snapshot.cc\n@@ -78,9 +78,12 @@ void SliceSnapshot::Cancel() {\n   VLOG(1) << \"SliceSnapshot::Cancel\";\n \n   CloseRecordChannel();\n-  if (journal_cb_id_) {\n-    db_slice_->shard_owner()->journal()->UnregisterOnChange(journal_cb_id_);\n+  // Cancel() might be called multiple times from different fibers of the same thread, but we\n+  // should unregister the callback only once.\n+  uint32_t cb_id = journal_cb_id_;\n+  if (cb_id) {\n     journal_cb_id_ = 0;\n+    db_slice_->shard_owner()->journal()->UnregisterOnChange(cb_id);\n   }\n }\n \n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 2943c24ccfe1..62cb5035ae5e 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -386,6 +386,54 @@ async def test_rotating_masters(df_local_factory, df_seeder_factory, t_replica,\n         fill_task.cancel()\n \n \n+@pytest.mark.asyncio\n+async def test_cancel_replication_immediately(df_local_factory, df_seeder_factory):\n+    \"\"\"\n+    Issue 40 replication commands randomally distributed over 10 seconds. This\n+    checks that the replication state machine can handle cancellation well.\n+    We assert that at least one command was cancelled during start and at least\n+    one command one successfull.\n+    After we finish the 'fuzzing' part, replicate the first master and check that\n+    all the data is correct.\n+    \"\"\"\n+    COMMANDS_TO_ISSUE = 40\n+\n+    replica = df_local_factory.create(port=BASE_PORT, v=1)\n+    masters = [df_local_factory.create(port=BASE_PORT+i+1) for i in range(4)]\n+    seeders = [df_seeder_factory.create(port=m.port) for m in masters]\n+\n+    df_local_factory.start_all([replica] + masters)\n+    c_replica = aioredis.Redis(port=replica.port)\n+    await asyncio.gather(*(seeder.run(target_deviation=0.1) for seeder in seeders))\n+\n+    replication_commands = []\n+    async def replicate(index):\n+        await asyncio.sleep(10.0 * random.random())\n+        try:\n+            start = time.time()\n+            await c_replica.execute_command(f\"REPLICAOF localhost {masters[index].port}\")\n+            # Giving replication commands shouldn't hang.\n+            assert time.time() - start < 2.0\n+            return True\n+        except aioredis.exceptions.ResponseError as e:\n+            assert e.args[0] == \"replication cancelled\"\n+            return False\n+\n+    for i in range(COMMANDS_TO_ISSUE):\n+        index = random.choice(range(len(masters)))\n+        replication_commands.append(replicate(index))\n+    results = await asyncio.gather(*replication_commands)\n+    num_successes = sum(results)\n+    assert COMMANDS_TO_ISSUE > num_successes, \"At least one REPLICAOF must be cancelled\"\n+    assert num_successes > 0, \"At least one REPLICAOF must be succeed\"\n+\n+\n+    await c_replica.execute_command(f\"REPLICAOF localhost {masters[0].port}\")\n+\n+    capture = await seeders[0].capture()\n+    assert await seeders[0].compare(capture, replica.port)\n+\n+\n \"\"\"\n Test flushall command. Set data to master send flashall and set more data.\n Check replica keys at the end.\n",
  "problem_statement": "Replication - replica should support setting up new master while its in loading state\n**Describe the bug**\r\nToday we return error on `replicaof <localhost> <port>` command if replica is in loading state, it returns \"ERR Can not execute during LOADING\"\r\n\r\n\r\n**Expected behavior**\r\non loading state if `replicaof <localhost> <port>` command is executed, start replication with new master\r\n\r\nQuestion:\r\nWhat should happen if `replicaof no one` is executed during loading state? should we return error? Should we return ok and flush all the data?\r\n\r\n.\r\n\n",
  "hints_text": "@Pothulapati FYI\n@romange @Pothulapati \r\n\r\nWhat should happen if `replicaof no one` interrupts the replication? My preference is to be conservative and keep returning the error. Second option I thought about was to return an error, stop the replication, and flush all data.\n'replicaof no one' is like, \"stop the replication, you are no more a slave\". I think it's a valid usecase and it should be respected. Why return an error?\n> Second option I thought about was to return an error, stop the replication, and flush all data.\r\n\r\nAnd Flushing the data would be counterproductive for our use-case, as we are marking the replica as master and expecting the previous data to continue to exist. if the data is flushed, there is no point of the failover.\nTo clarify - this is about the case where we're still in a LOADING state in the replication. So I don't think we could make the instance a master.\r\n\nI understand. But we should be able to make the instance master. The command in this case a descriptive command that describes the intent of the user - to switch to \"no slave\" state. It's different from an \"execution\" command like SET/GET. ping me offline if you do not understand the difference. Also see https://tslim.github.io/concepts/concepts/software-architecture/reconciliation-loops.html",
  "created_at": "2023-04-09T14:38:02Z",
  "modified_files": [
    "src/server/command_registry.h",
    "src/server/generic_family.cc",
    "src/server/rdb_save.cc",
    "src/server/replica.cc",
    "src/server/replica.h",
    "src/server/server_family.cc",
    "src/server/snapshot.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}