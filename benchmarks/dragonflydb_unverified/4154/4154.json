{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4154,
  "instance_id": "dragonflydb__dragonfly-4154",
  "issue_numbers": [
    "4143"
  ],
  "base_commit": "4e7800f94fba5581039a86eed3b163e64382ea44",
  "patch": "diff --git a/src/server/debugcmd.cc b/src/server/debugcmd.cc\nindex e8c264c74191..809f28100cc1 100644\n--- a/src/server/debugcmd.cc\n+++ b/src/server/debugcmd.cc\n@@ -401,7 +401,7 @@ void DebugCmd::Run(CmdArgList args, facade::SinkReplyBuilder* builder) {\n         \"    to meet value size.\",\n         \"    If RAND is specified then value will be set to random hex string in specified size.\",\n         \"    If SLOTS is specified then create keys only in given slots range.\",\n-        \"    TYPE specifies data type (must be STRING/LIST/SET/HSET/ZSET/JSON), default STRING.\",\n+        \"    TYPE specifies data type (must be STRING/LIST/SET/HASH/ZSET/JSON), default STRING.\",\n         \"    ELEMENTS specifies how many sub elements if relevant (like entries in a list / set).\",\n         \"OBJHIST\",\n         \"    Prints histogram of object sizes.\",\ndiff --git a/src/server/generic_family.cc b/src/server/generic_family.cc\nindex 0b3aa70bb5ee..a29b556675b3 100644\n--- a/src/server/generic_family.cc\n+++ b/src/server/generic_family.cc\n@@ -157,25 +157,32 @@ class RdbRestoreValue : protected RdbLoaderBase {\n                                            const RestoreArgs& args, EngineShard* shard);\n \n  private:\n-  std::optional<OpaqueObj> Parse(std::string_view payload);\n+  std::optional<OpaqueObj> Parse(io::Source* source);\n+  int rdb_type_ = -1;\n };\n \n-std::optional<RdbLoaderBase::OpaqueObj> RdbRestoreValue::Parse(std::string_view payload) {\n-  InMemSource source(payload);\n-  src_ = &source;\n-  if (io::Result<uint8_t> type_id = FetchType(); type_id && rdbIsObjectTypeDF(type_id.value())) {\n-    OpaqueObj obj;\n-    error_code ec = ReadObj(type_id.value(), &obj);  // load the type from the input stream\n-    if (ec) {\n-      LOG(ERROR) << \"failed to load data for type id \" << (unsigned int)type_id.value();\n-      return std::nullopt;\n+std::optional<RdbLoaderBase::OpaqueObj> RdbRestoreValue::Parse(io::Source* source) {\n+  src_ = source;\n+  if (pending_read_.remaining == 0) {\n+    io::Result<uint8_t> type_id = FetchType();\n+    if (type_id && rdbIsObjectTypeDF(type_id.value())) {\n+      rdb_type_ = *type_id;\n     }\n+  }\n \n-    return std::optional<OpaqueObj>(std::move(obj));\n-  } else {\n+  if (rdb_type_ == -1) {\n     LOG(ERROR) << \"failed to load type id from the input stream or type id is invalid\";\n     return std::nullopt;\n   }\n+\n+  OpaqueObj obj;\n+  error_code ec = ReadObj(rdb_type_, &obj);  // load the type from the input stream\n+  if (ec) {\n+    LOG(ERROR) << \"failed to load data for type id \" << rdb_type_;\n+    return std::nullopt;\n+  }\n+\n+  return std::optional<OpaqueObj>(std::move(obj));\n }\n \n std::optional<DbSlice::ItAndUpdater> RdbRestoreValue::Add(std::string_view data,\n@@ -183,17 +190,31 @@ std::optional<DbSlice::ItAndUpdater> RdbRestoreValue::Add(std::string_view data,\n                                                           const DbContext& cntx,\n                                                           const RestoreArgs& args,\n                                                           EngineShard* shard) {\n-  auto opaque_res = Parse(data);\n-  if (!opaque_res) {\n-    return std::nullopt;\n-  }\n-\n+  InMemSource data_src(data);\n   PrimeValue pv;\n-  if (auto ec = FromOpaque(*opaque_res, &pv); ec) {\n-    // we failed - report and exit\n-    LOG(WARNING) << \"error while trying to save data: \" << ec;\n-    return std::nullopt;\n-  }\n+  bool first_parse = true;\n+  do {\n+    auto opaque_res = Parse(&data_src);\n+    if (!opaque_res) {\n+      return std::nullopt;\n+    }\n+\n+    LoadConfig config;\n+    if (first_parse) {\n+      first_parse = false;\n+    } else {\n+      config.append = true;\n+    }\n+    if (pending_read_.remaining > 0) {\n+      config.streamed = true;\n+    }\n+\n+    if (auto ec = FromOpaque(*opaque_res, config, &pv); ec) {\n+      // we failed - report and exit\n+      LOG(WARNING) << \"error while trying to read data: \" << ec;\n+      return std::nullopt;\n+    }\n+  } while (pending_read_.remaining > 0);\n \n   if (auto res = db_slice.AddNew(cntx, key, std::move(pv), args.ExpirationTime()); res) {\n     res->it->first.SetSticky(args.Sticky());\ndiff --git a/src/server/rdb_load.cc b/src/server/rdb_load.cc\nindex 0b889fc4f57d..b2eb68aebbb8 100644\n--- a/src/server/rdb_load.cc\n+++ b/src/server/rdb_load.cc\n@@ -2667,10 +2667,6 @@ void RdbLoader::FlushAllShards() {\n     FlushShardAsync(i);\n }\n \n-std::error_code RdbLoaderBase::FromOpaque(const OpaqueObj& opaque, CompactObj* pv) {\n-  return RdbLoaderBase::FromOpaque(opaque, LoadConfig{}, pv);\n-}\n-\n std::error_code RdbLoaderBase::FromOpaque(const OpaqueObj& opaque, LoadConfig config,\n                                           CompactObj* pv) {\n   OpaqueObjLoader visitor(opaque.rdb_type, pv, config);\ndiff --git a/src/server/rdb_load.h b/src/server/rdb_load.h\nindex 830ee59cc6de..78733912b507 100644\n--- a/src/server/rdb_load.h\n+++ b/src/server/rdb_load.h\n@@ -139,7 +139,6 @@ class RdbLoaderBase {\n \n   template <typename T> io::Result<T> FetchInt();\n \n-  static std::error_code FromOpaque(const OpaqueObj& opaque, CompactObj* pv);\n   static std::error_code FromOpaque(const OpaqueObj& opaque, LoadConfig config, CompactObj* pv);\n \n   io::Result<uint64_t> LoadLen(bool* is_encoded);\n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex 66f70856e236..83acd11f71b3 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -13,7 +13,7 @@\n from redis.cluster import RedisCluster\n from redis.cluster import ClusterNode\n from .proxy import Proxy\n-from .seeder import SeederBase\n+from .seeder import StaticSeeder\n \n from . import dfly_args\n \n@@ -1773,6 +1773,45 @@ async def node1size0():\n         assert str(i) == await nodes[1].client.get(f\"{{key50}}:{i}\")\n \n \n+@dfly_args({\"proactor_threads\": 2, \"cluster_mode\": \"yes\"})\n+@pytest.mark.asyncio\n+async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory):\n+    instances = [\n+        df_factory.create(port=BASE_PORT + i, admin_port=BASE_PORT + i + 1000) for i in range(2)\n+    ]\n+    df_factory.start_all(instances)\n+\n+    nodes = [await create_node_info(instance) for instance in instances]\n+    nodes[0].slots = [(0, 16383)]\n+    nodes[1].slots = []\n+\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    logging.debug(\"Generating huge containers\")\n+    seeder = StaticSeeder(\n+        key_target=10,\n+        data_size=10_000_000,\n+        collection_size=10_000,\n+        variance=1,\n+        samples=1,\n+        types=[\"LIST\", \"HASH\", \"SET\", \"ZSET\", \"STRING\"],\n+    )\n+    await seeder.run(nodes[0].client)\n+    source_data = await StaticSeeder.capture(nodes[0].client)\n+\n+    nodes[0].migrations = [\n+        MigrationInfo(\"127.0.0.1\", instances[1].admin_port, [(0, 16383)], nodes[1].id)\n+    ]\n+    logging.debug(\"Migrating slots\")\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    logging.debug(\"Waiting for migration to finish\")\n+    await wait_for_status(nodes[0].admin_client, nodes[1].id, \"FINISHED\")\n+\n+    target_data = await StaticSeeder.capture(nodes[1].client)\n+    assert source_data == target_data\n+\n+\n def parse_lag(replication_info: str):\n     lags = re.findall(\"lag=([0-9]+)\\r\\n\", replication_info)\n     assert len(lags) == 1\ndiff --git a/tests/dragonfly/generic_test.py b/tests/dragonfly/generic_test.py\nindex 020d364a9c23..c4ceef3d6398 100644\n--- a/tests/dragonfly/generic_test.py\n+++ b/tests/dragonfly/generic_test.py\n@@ -1,4 +1,5 @@\n import os\n+import logging\n import pytest\n import redis\n import asyncio\n@@ -7,6 +8,7 @@\n from . import dfly_multi_test_args, dfly_args\n from .instance import DflyStartException\n from .utility import batch_fill_data, gen_test_data, EnvironCntx\n+from .seeder import StaticSeeder\n \n \n @dfly_multi_test_args({\"keys_output_limit\": 512}, {\"keys_output_limit\": 1024})\n@@ -168,3 +170,38 @@ async def test_denyoom_commands(df_factory):\n \n     # mget should not be rejected\n     await client.execute_command(\"mget x\")\n+\n+\n+@pytest.mark.parametrize(\"type\", [\"LIST\", \"HASH\", \"SET\", \"ZSET\", \"STRING\"])\n+@dfly_args({\"proactor_threads\": 4})\n+@pytest.mark.asyncio\n+async def test_rename_huge_values(df_factory, type):\n+    df_server = df_factory.create()\n+    df_server.start()\n+    client = df_server.client()\n+\n+    logging.debug(f\"Generating huge {type}\")\n+    seeder = StaticSeeder(\n+        key_target=1,\n+        data_size=10_000_000,\n+        collection_size=10_000,\n+        variance=1,\n+        samples=1,\n+        types=[type],\n+    )\n+    await seeder.run(client)\n+    source_data = await StaticSeeder.capture(client)\n+    logging.debug(f\"src {source_data}\")\n+\n+    # Rename multiple times to make sure the key moves between shards\n+    orig_name = (await client.execute_command(\"keys *\"))[0]\n+    old_name = orig_name\n+    new_name = \"\"\n+    for i in range(10):\n+        new_name = f\"new:{i}\"\n+        await client.execute_command(f\"rename {old_name} {new_name}\")\n+        old_name = new_name\n+    await client.execute_command(f\"rename {new_name} {orig_name}\")\n+    target_data = await StaticSeeder.capture(client)\n+\n+    assert source_data == target_data\n",
  "problem_statement": "Huge lists do not migrate fully between cluster nodes\n(this may apply to other data types as well)\r\n\r\nTo reproduce:\r\n\r\n* Run 2 cluster-mode Dragonfly processes (with with `--port=7001` and `--port=7002`, and pass `--cluster_mode=yes` to both)\r\n* Configure them to have all slots belong to the first:\r\n\r\n```\r\n./cluster_mgr.py --action=config_single_remote --target_port=7001\r\n./cluster_mgr.py --action=attach --target_port=7001 --attach_port=7002\r\n```\r\n\r\n* Debug populate a huge list on the node with the slots:\r\n\r\n```\r\n$ redis-cli -p 7001\r\nlocalhost:7001> debug populate 1 l: 1000 RAND TYPE list ELEMENTS 500000\r\nOK\r\nlocalhost:7001> llen l::0\r\n(integer) 500000\r\n```\r\n\r\n* Migrate all slots to second node:\r\n\r\n```\r\n./cluster_mgr.py --action=migrate --slot_start=0 --slot_end=16383 --target_port=7002\r\n```\r\n\r\n* Note the issue:\r\n\r\n```\r\nlocalhost:7002> llen l::0\r\n(integer) 4096\r\nlocalhost:7002> memory usage l::0\r\n(integer) 86064\r\n```\n",
  "hints_text": "We hame similar problem with RENAME command and big zset keys.\r\n```\r\nredis_version:6.2.11\r\ndragonfly_version:df-v1.24.0\r\nredis_mode:standalone\r\n```\r\n\r\n```\r\n127.0.0.1:6379[14]> type ipv4\r\nzset\r\n```\r\n\r\n```\r\n127.0.0.1:6379[14]> ZCARD ipv6_tmp\r\n(integer) 7391471\r\n127.0.0.1:6379[14]> RENAME ipv6_tmp ipv6\r\nOK\r\n(6.75s)\r\n127.0.0.1:6379[14]> ZCARD ipv6\r\n(integer) 4092\r\n```\r\n\nYes @anadion, that is due to the same root cause. Thanks for reporting!",
  "created_at": "2024-11-19T18:41:38Z",
  "modified_files": [
    "src/server/debugcmd.cc",
    "src/server/generic_family.cc",
    "src/server/rdb_load.cc",
    "src/server/rdb_load.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py",
    "tests/dragonfly/generic_test.py"
  ]
}