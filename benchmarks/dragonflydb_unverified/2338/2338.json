{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 2338,
  "instance_id": "dragonflydb__dragonfly-2338",
  "issue_numbers": [
    "2337"
  ],
  "base_commit": "1caa4ee0f1751f3e2b4c2e663af285bbaebf7c6e",
  "patch": "diff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 9c22416366bf..bd1f3863da3f 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -491,10 +491,26 @@ void ServerFamily::Init(util::AcceptServer* acceptor, std::vector<facade::Listen\n   if (ReplicaOfFlag flag = GetFlag(FLAGS_replicaof); flag.has_value()) {\n     service_.proactor_pool().GetNextProactor()->Await(\n         [this, &flag]() { this->Replicate(flag.host, flag.port); });\n-    return;  // DONT load any snapshots\n+  } else {  // load from snapshot only if --replicaof is empty\n+    LoadFromSnapshot();\n   }\n \n-  const auto load_path_result = snapshot_storage_->LoadPath(flag_dir, GetFlag(FLAGS_dbfilename));\n+  const auto create_snapshot_schedule_fb = [this] {\n+    snapshot_schedule_fb_ =\n+        service_.proactor_pool().GetNextProactor()->LaunchFiber([this] { SnapshotScheduling(); });\n+  };\n+  config_registry.RegisterMutable(\n+      \"snapshot_cron\", [this, create_snapshot_schedule_fb](const absl::CommandLineFlag& flag) {\n+        JoinSnapshotSchedule();\n+        create_snapshot_schedule_fb();\n+        return true;\n+      });\n+  create_snapshot_schedule_fb();\n+}\n+\n+void ServerFamily::LoadFromSnapshot() {\n+  const auto load_path_result =\n+      snapshot_storage_->LoadPath(GetFlag(FLAGS_dir), GetFlag(FLAGS_dbfilename));\n   if (load_path_result) {\n     const std::string load_path = *load_path_result;\n     if (!load_path.empty()) {\n@@ -507,19 +523,6 @@ void ServerFamily::Init(util::AcceptServer* acceptor, std::vector<facade::Listen\n       LOG(ERROR) << \"Failed to load snapshot: \" << load_path_result.error().Format();\n     }\n   }\n-\n-  const auto create_snapshot_schedule_fb = [this] {\n-    snapshot_schedule_fb_ =\n-        service_.proactor_pool().GetNextProactor()->LaunchFiber([this] { SnapshotScheduling(); });\n-  };\n-  config_registry.RegisterMutable(\n-      \"snapshot_cron\", [this, create_snapshot_schedule_fb](const absl::CommandLineFlag& flag) {\n-        JoinSnapshotSchedule();\n-        create_snapshot_schedule_fb();\n-        return true;\n-      });\n-\n-  create_snapshot_schedule_fb();\n }\n \n void ServerFamily::JoinSnapshotSchedule() {\n@@ -1937,9 +1940,14 @@ void ServerFamily::Hello(CmdArgList args, ConnectionContext* cntx) {\n void ServerFamily::ReplicaOfInternal(string_view host, string_view port_sv, ConnectionContext* cntx,\n                                      ActionOnConnectionFail on_err) {\n   LOG(INFO) << \"Replicating \" << host << \":\" << port_sv;\n-\n   unique_lock lk(replicaof_mu_);  // Only one REPLICAOF command can run at a time\n \n+  // We should not execute replica of command while loading from snapshot.\n+  if (ServerState::tlocal()->is_master && service_.GetGlobalState() == GlobalState::LOADING) {\n+    cntx->SendError(\"Can not execute during LOADING\");\n+    return;\n+  }\n+\n   // If NO ONE was supplied, just stop the current replica (if it exists)\n   if (IsReplicatingNoOne(host, port_sv)) {\n     if (!ServerState::tlocal()->is_master) {\ndiff --git a/src/server/server_family.h b/src/server/server_family.h\nindex 6ef954a13419..7222f3b73a19 100644\n--- a/src/server/server_family.h\n+++ b/src/server/server_family.h\n@@ -204,6 +204,7 @@ class ServerFamily {\n \n  private:\n   void JoinSnapshotSchedule();\n+  void LoadFromSnapshot();\n \n   uint32_t shard_count() const {\n     return shard_set->size();\n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex eedbc5c545ea..bc874f276e6e 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -1806,3 +1806,33 @@ async def test_client_pause_with_replica(df_local_factory, df_seeder_factory):\n     assert await seeder.compare(capture, port=replica.port)\n \n     await disconnect_clients(c_master, c_replica)\n+\n+\n+async def test_replicaof_reject_on_load(df_local_factory, df_seeder_factory):\n+    tmp_file_name = \"\".join(random.choices(string.ascii_letters, k=10))\n+    master = df_local_factory.create()\n+    replica = df_local_factory.create(dbfilename=f\"dump_{tmp_file_name}\")\n+    df_local_factory.start_all([master, replica])\n+\n+    seeder = df_seeder_factory.create(port=replica.port, keys=30000)\n+    await seeder.run(target_deviation=0.1)\n+    c_replica = replica.client()\n+    dbsize = await c_replica.dbsize()\n+    assert dbsize >= 9000\n+\n+    replica.stop()\n+    replica.start()\n+    c_replica = replica.client()\n+    # Check replica of not alowed while loading snapshot\n+    try:\n+        await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n+        assert False\n+    except aioredis.ResponseError as e:\n+        assert \"Can not execute during LOADING\" in str(e)\n+    # Check one we finish loading snapshot replicaof success\n+    await wait_available_async(c_replica)\n+    await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n+\n+    await c_replica.close()\n+    master.stop()\n+    replica.stop()\n",
  "problem_statement": "During the load snapshot process, should the REPLICAOF or SLAVEOF command be allowed to be executed?\n**Describe the bug**\r\n  When starting a stopped old instance as a replica of a master instance without data, after starting the replica instance, the most recent snapshot will be loaded first. When the load process is not completed, the replicateaof command is executed to establish the master-slave with the master without data. Relationship, since the main library does not have data, the `full sync ` process will be very fast. When the synchronization state reaches `stable sync`, a problem occurs. The process of replica loading the old snapshot has not ended yet, which ultimately leads to inconsistency between the master and slave data. question.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: [Debian GNU/Linux 11]\r\n - Kernel:  #1 SMP Debian 5.10.178-3 (2023-04-22) x86_64 GNU/Linux\r\n - Containerized?: [Physical machine environment]\r\n - Dragonfly Version: [v 1.3.0]\r\n\r\n**Expected behavior**\r\nAfter the slave executes the `replicaof` or `slaveof` command, the data in the master library and the slave library should be completely consistent.\r\n\r\n**Logs and Screenshots of process recurrence**\r\n\r\n- Background description\r\n  master\uff1a192.168.1.10:6380\r\n  replica:   192.168.1.10:6381\r\n  1.  The master has no data\r\n  2. The slave has many snapshots after bgsave of the previous instance.\r\n\r\n- Reproduction steps\r\n  1. Start the slave instance\r\n  2. Execute `replicaof 192.168.1.10 6380` or `slaveof 192.168.1.10 6380`\r\n  3. Since the master has no data, the slave synchronization state quickly changes from the full sync state to the stable sync state, and starts replicating the relationship.\r\n  4. There is a huge data inconsistency between master and slave\r\n  5. Execute REPLICAOF no one in the slave library to disconnect the master-slave replication\r\n  6. Execute `replicaof 192.168.1.10 6380` or `slaveof 192.168.1.10 6380` again \r\n  7. After reaching the stable sync state again, the data is consistent\r\n\r\n- Important log display\r\n```\r\nI20231226 15:32:25.121160 1434315 init.cc:70] dragonfly running in opt mode.\r\nI20231226 15:32:25.121472 1434315 dfly_main.cc:800] Starting dragonfly df-v1.13.0-f39eac5bcaf7c8ffe5c433a0e8e15747391199d9\r\nI20231226 15:32:25.121649 1434315 dfly_main.cc:863] Max memory limit is: 14.00GiB\r\nI20231226 15:32:25.199597 1434315 proactor_pool.cc:146] Running 56 io threads\r\nI20231226 15:32:25.200731 1434315 main_service.cc:2470] Multi-key commands are: \r\nI20231226 15:32:25.200749 1434315 main_service.cc:2478]     SINTERSTORE: with unlimited keys\r\n...\r\nI20231226 15:32:25.202522 1434315 main_service.cc:2485]     REPLICAOF\r\nI20231226 15:32:25.202529 1434315 main_service.cc:2485]     LTRIM\r\nI20231226 15:32:25.202536 1434315 main_service.cc:2485]     HSTRLEN\r\nI20231226 15:32:25.204681 1434315 dfly_main.cc:437] Listening on admin socket 192.168.1.11:16381\r\nI20231226 15:32:25.279189 1434315 snapshot_storage.cc:106] Load snapshot: Searching for snapshot in directory: \"/home/dba/dragonfly/dragonfly6381\"\r\nI20231226 15:32:25.280342 1434315 server_family.cc:673] Loading /home/dba/dragonfly/dragonfly6381/dump-2023-09-06T17:07:32-summary.dfs\r\nI20231226 15:32:25.280357 1434315 main_service.cc:2295] Switching state from ACTIVE to LOADING\r\nI20231226 15:32:25.296355 1434318 listener_interface.cc:101] sock[117] AcceptServer - listening on port 6381\r\nI20231226 15:32:25.296356 1434317 listener_interface.cc:101] sock[116] AcceptServer - listening on port 16381\r\nI20231226 15:32:52.904044 1434316 main_service.cc:1063] Got (1): [REPLICAOF,192.168.1.10,6380] in dbid=0\r\nI20231226 15:32:53.315989 1434316 server_family.cc:1937] Replicating 192.168.1.10:6380\r\nI20231226 15:32:53.417805 1434316 replica.cc:515] Started full sync with 192.168.1.10:6380\r\nI20231226 15:32:53.421008 1434316 replica.cc:535] full sync finished in 89 ms\r\nI20231226 15:32:53.425195 1434316 main_service.cc:2295] Switching state from LOADING to ACTIVE\r\nI20231226 15:32:53.425483 1434316 replica.cc:612] Transitioned into stable sync\r\nI20231226 15:32:57.308971 1434316 main_service.cc:1063] Got (1): [INFO] in dbid=0\r\n...\r\nI20231226 15:34:19.674978 1434317 main_service.cc:1063] Got (2): [INFO] in dbid=0\r\nI20231226 15:34:47.527698 1434320 server_family.cc:727] Load finished, num keys read: 174767542\r\nI20231226 15:34:52.395077 1434318 main_service.cc:1063] Got (3): [COMMAND] in dbid=0\r\nI20231226 15:34:53.693976 1434318 main_service.cc:1063] Got (3): [INFO] in dbid=0\r\n...\r\nI20231226 15:53:15.552983 1434320 main_service.cc:1063] Got (5): [REPLICAOF,no,one] in dbid=0\r\nI20231226 15:53:15.553061 1434320 server_family.cc:1937] Replicating no:one\r\nI20231226 15:53:15.557498 1434316 replica.cc:636] Exit stable sync\r\nW20231226 15:53:15.557523 1434316 replica.cc:237] Error stable sync with 192.168.1.10:6380 generic:125 Operation canceled\r\n...\r\nI20231226 15:53:25.385007 1434320 main_service.cc:1063] Got (5): [REPLICAOF,192.168.1.10,6380] in dbid=0\r\nI20231226 15:53:27.321966 1434320 server_family.cc:1937] Replicating 192.168.1.10:6380\r\nI20231226 15:53:27.322014 1434320 main_service.cc:2295] Switching state from ACTIVE to LOADING\r\nI20231226 15:53:27.386029 1434320 replica.cc:515] Started full sync with 192.168.1.10:6380\r\nI20231226 15:53:27.387676 1434320 replica.cc:535] full sync finished in 64 ms\r\nI20231226 15:53:27.387760 1434320 main_service.cc:2295] Switching state from LOADING to ACTIVE\r\nI20231226 15:53:27.387941 1434320 replica.cc:612] Transitioned into stable sync\r\n``` \r\n\r\n**Additional context**\r\nThis is how I think about it. This should not happen with redis, because the `replicaof` or `slaveof` commands cannot be executed during the load snapshot process. Due to the single-threaded design, they will be blocked. Due to its multi-threaded shared-nothing design mode, dragonfly will not block the `replicaof` or `slaveof` commands. However, the problem caused by this is that the master-slave instance data is inconsistent. The slave library has a lot more data from previous snapshots than the main library, thus resulting in subsequent use risks. For example, the memory usage of the main library is 50%, but the memory usage of the slave library is 90%, and the data is not easy to clean after a period of use.\r\n\r\n**My advice**\r\nSo my question is, how to solve this problem in dragonfly's architecture. I have two personal thoughts:\r\n1. Should the execution of `replicaof` and `slaveof` commands be prohibited during the load snapshot phase and a prompt given?\r\n2. If the execution of the `replicaof` and `slaveof` commands is not blocked, immediately after executing the command, all scheduling of the load snapshot process will be interrupted, and flushall async will be executed, and then go to the master to pull the snapshot, ultimately achieving consistency of the master-slave data.\r\n\n",
  "hints_text": "Hi @boomballa thank you for reporting this!\r\nYou are right we should not allow executing replicaof while we are loading from snapshot, we will fix this.\r\nToday we allow executing replicaof during loading state to enable running another replicaof comman if the first replicaof even if the first replicaof did not reach stable state. \r\nBut the scenario you are describing will be fixed ",
  "created_at": "2023-12-26T13:02:45Z",
  "modified_files": [
    "src/server/server_family.cc",
    "src/server/server_family.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}