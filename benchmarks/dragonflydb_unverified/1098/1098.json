{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 1098,
  "instance_id": "dragonflydb__dragonfly-1098",
  "issue_numbers": [
    "1039"
  ],
  "base_commit": "c952251381de2c0734fefde585755578dc3d7965",
  "patch": "diff --git a/src/facade/dragonfly_connection.cc b/src/facade/dragonfly_connection.cc\nindex 5643384fd28b..da6f1e4e44b3 100644\n--- a/src/facade/dragonfly_connection.cc\n+++ b/src/facade/dragonfly_connection.cc\n@@ -78,13 +78,6 @@ bool MatchHttp11Line(string_view line) {\n constexpr size_t kMinReadSize = 256;\n constexpr size_t kMaxReadSize = 32_KB;\n \n-struct PubMsgRecord {\n-  Connection::PubMessage pub_msg;\n-\n-  PubMsgRecord(Connection::PubMessage pmsg) : pub_msg(move(pmsg)) {\n-  }\n-};\n-\n #ifdef ABSL_HAVE_ADDRESS_SANITIZER\n constexpr size_t kReqStorageSize = 88;\n #else\n@@ -95,7 +88,7 @@ thread_local uint32_t free_req_release_weight = 0;\n \n }  // namespace\n \n-thread_local vector<Connection::RequestPtr> Connection::free_req_pool_;\n+thread_local vector<Connection::RequestPtr> Connection::pipeline_req_pool_;\n \n struct Connection::Shutdown {\n   absl::flat_hash_map<ShutdownHandle, ShutdownCb> map;\n@@ -116,19 +109,19 @@ struct Connection::RequestDeleter {\n   void operator()(Request* req) const;\n };\n \n+using PubMessage = Connection::PubMessage;\n+using MonitorMessage = std::string;\n+\n // Please note: The call to the Dtor is mandatory for this!!\n // This class contain types that don't have trivial destructed objects\n class Connection::Request {\n  public:\n-  using MonitorMessage = std::string;\n-\n-  struct PipelineMsg {\n-    // I do not use mi_heap_t explicitly but mi_stl_allocator at the end does the same job\n-    // of using the thread's heap.\n+  struct PipelineMessage {\n+    // mi_stl_allocator uses mi heap internally.\n     // The capacity is chosen so that we allocate a fully utilized (256 bytes) block.\n     using StorageType = absl::InlinedVector<char, kReqStorageSize, mi_stl_allocator<char>>;\n \n-    PipelineMsg(size_t nargs, size_t capacity) : args(nargs), storage(capacity) {\n+    PipelineMessage(size_t nargs, size_t capacity) : args(nargs), storage(capacity) {\n     }\n \n     void Reset(size_t nargs, size_t capacity);\n@@ -137,7 +130,7 @@ class Connection::Request {\n     StorageType storage;\n   };\n \n-  using MessagePayload = std::variant<PipelineMsg, PubMsgRecord, MonitorMessage>;\n+  using MessagePayload = std::variant<PipelineMessage, PubMessage, MonitorMessage>;\n \n   // Overload to create the a new pipeline message\n   static RequestPtr New(mi_heap_t* heap, const RespVec& args, size_t capacity);\n@@ -150,26 +143,29 @@ class Connection::Request {\n \n   void Emplace(const RespVec& args, size_t capacity);\n \n-  MessagePayload payload;\n-\n   size_t StorageCapacity() const;\n \n   bool IsPipelineMsg() const;\n \n  private:\n-  static constexpr size_t kSizeOfPipelineMsg = sizeof(PipelineMsg);\n+  static constexpr size_t kSizeOfPipelineMsg = sizeof(PipelineMessage);\n \n-  Request(size_t nargs, size_t capacity) : payload(PipelineMsg{nargs, capacity}) {\n+  Request(size_t nargs, size_t capacity) : payload(PipelineMessage{nargs, capacity}) {\n   }\n \n-  Request(PubMsgRecord msg) : payload(std::move(msg)) {\n+  Request(PubMessage msg) : payload(move(msg)) {\n   }\n \n-  Request(MonitorMessage msg) : payload(std::move(msg)) {\n+  Request(MonitorMessage msg) : payload(move(msg)) {\n   }\n \n   Request(const Request&) = delete;\n+\n+  // Store arguments for pipeline message.\n   void SetArgs(const RespVec& args);\n+\n+ public:\n+  MessagePayload payload;\n };\n \n Connection::PubMessage::PubMessage(string pattern, shared_ptr<string> channel,\n@@ -186,18 +182,18 @@ struct Connection::DispatchOperations {\n       : stats{me->service_->GetThreadLocalConnectionStats()}, builder{b}, self(me) {\n   }\n \n-  void operator()(const PubMsgRecord& msg);\n-  void operator()(Request::PipelineMsg& msg);\n-  void operator()(const Request::MonitorMessage& msg);\n+  void operator()(const PubMessage& msg);\n+  void operator()(Request::PipelineMessage& msg);\n+  void operator()(const MonitorMessage& msg);\n \n   ConnectionStats* stats = nullptr;\n   SinkReplyBuilder* builder = nullptr;\n   Connection* self = nullptr;\n };\n \n-Connection::RequestPtr Connection::Request::New(std::string msg) {\n+Connection::RequestPtr Connection::Request::New(MonitorMessage msg) {\n   void* ptr = mi_malloc(sizeof(Request));\n-  Request* req = new (ptr) Request(std::move(msg));\n+  Request* req = new (ptr) Request(move(msg));\n   return Connection::RequestPtr{req, Connection::RequestDeleter{}};\n }\n \n@@ -213,9 +209,20 @@ Connection::RequestPtr Connection::Request::New(mi_heap_t* heap, const RespVec&\n   return Connection::RequestPtr{req, Connection::RequestDeleter{}};\n }\n \n+Connection::RequestPtr Connection::Request::New(PubMessage pub_msg) {\n+  // This will generate a new request for pubsub message\n+  // Please note that unlike the above case, we don't need to \"protect\", the internals here\n+  // since we are currently using a borrow token for it - i.e. the BlockingCounter will\n+  // ensure that the message is not deleted until we are finish sending it at the other\n+  // side of the queue\n+  void* ptr = mi_malloc(sizeof(Request));\n+  Request* req = new (ptr) Request(move(pub_msg));\n+  return Connection::RequestPtr{req, Connection::RequestDeleter{}};\n+}\n+\n void Connection::Request::SetArgs(const RespVec& args) {\n-  // At this point we know that we have PipelineMsg in Request so next op is safe.\n-  PipelineMsg& pipeline_msg = std::get<PipelineMsg>(payload);\n+  // At this point we know that we have PipelineMessage in Request so next op is safe.\n+  PipelineMessage& pipeline_msg = std::get<PipelineMessage>(payload);\n   auto* next = pipeline_msg.storage.data();\n   for (size_t i = 0; i < args.size(); ++i) {\n     auto buf = args[i].GetBuf();\n@@ -226,34 +233,22 @@ void Connection::Request::SetArgs(const RespVec& args) {\n   }\n }\n \n-Connection::RequestPtr Connection::Request::New(PubMessage pub_msg) {\n-  // This will generate a new request for pubsub message\n-  // Please note that unlike the above case, we don't need to \"protect\", the internals here\n-  // since we are currently using a borrow token for it - i.e. the BlockingCounter will\n-  // ensure that the message is not deleted until we are finish sending it at the other\n-  // side of the queue\n-  PubMsgRecord new_msg{move(pub_msg)};\n-  void* ptr = mi_malloc(sizeof(Request));\n-  Request* req = new (ptr) Request(std::move(new_msg));\n-  return Connection::RequestPtr{req, Connection::RequestDeleter{}};\n-}\n-\n void Connection::RequestDeleter::operator()(Request* req) const {\n   req->~Request();\n   mi_free(req);\n }\n \n void Connection::Request::Emplace(const RespVec& args, size_t capacity) {\n-  PipelineMsg* msg = get_if<PipelineMsg>(&payload);\n+  PipelineMessage* msg = get_if<PipelineMessage>(&payload);\n   if (msg) {\n     msg->Reset(args.size(), capacity);\n   } else {\n-    payload = PipelineMsg{args.size(), capacity};\n+    payload = PipelineMessage{args.size(), capacity};\n   }\n   SetArgs(args);\n }\n \n-void Connection::Request::PipelineMsg::Reset(size_t nargs, size_t capacity) {\n+void Connection::Request::PipelineMessage::Reset(size_t nargs, size_t capacity) {\n   storage.resize(capacity);\n   args.resize(nargs);\n }\n@@ -262,8 +257,8 @@ template <class... Ts> struct Overloaded : Ts... { using Ts::operator()...; };\n template <class... Ts> Overloaded(Ts...) -> Overloaded<Ts...>;\n \n size_t Connection::Request::StorageCapacity() const {\n-  return std::visit(Overloaded{[](const PubMsgRecord& msg) -> size_t { return 0; },\n-                               [](const PipelineMsg& arg) -> size_t {\n+  return std::visit(Overloaded{[](const PubMessage& msg) -> size_t { return 0; },\n+                               [](const PipelineMessage& arg) -> size_t {\n                                  return arg.storage.capacity() + arg.args.capacity();\n                                },\n                                [](const MonitorMessage& arg) -> size_t { return arg.capacity(); }},\n@@ -271,18 +266,17 @@ size_t Connection::Request::StorageCapacity() const {\n }\n \n bool Connection::Request::IsPipelineMsg() const {\n-  return std::get_if<PipelineMsg>(&payload) != nullptr;\n+  return std::get_if<PipelineMessage>(&payload) != nullptr;\n }\n \n-void Connection::DispatchOperations::operator()(const Request::MonitorMessage& msg) {\n+void Connection::DispatchOperations::operator()(const MonitorMessage& msg) {\n   RedisReplyBuilder* rbuilder = (RedisReplyBuilder*)builder;\n   rbuilder->SendSimpleString(msg);\n }\n \n-void Connection::DispatchOperations::operator()(const PubMsgRecord& msg) {\n+void Connection::DispatchOperations::operator()(const PubMessage& pub_msg) {\n   RedisReplyBuilder* rbuilder = (RedisReplyBuilder*)builder;\n   ++stats->async_writes_cnt;\n-  const PubMessage& pub_msg = msg.pub_msg;\n   string_view arr[4];\n   if (pub_msg.type == PubMessage::kPublish) {\n     if (pub_msg.pattern.empty()) {\n@@ -309,7 +303,7 @@ void Connection::DispatchOperations::operator()(const PubMsgRecord& msg) {\n   }\n }\n \n-void Connection::DispatchOperations::operator()(Request::PipelineMsg& msg) {\n+void Connection::DispatchOperations::operator()(Request::PipelineMessage& msg) {\n   ++stats->pipelined_cmd_cnt;\n   self->pipeline_msg_cnt_--;\n \n@@ -325,7 +319,7 @@ void Connection::DispatchOperations::operator()(Request::PipelineMsg& msg) {\n \n Connection::Connection(Protocol protocol, util::HttpListenerBase* http_listener, SSL_CTX* ctx,\n                        ServiceInterface* service)\n-    : io_buf_(kMinReadSize), http_listener_(http_listener), ctx_(ctx), service_(service) {\n+    : io_buf_(kMinReadSize), http_listener_(http_listener), ctx_(ctx), service_(service), name_{} {\n   static atomic_uint32_t next_id{1};\n \n   protocol_ = protocol;\n@@ -344,8 +338,6 @@ Connection::Connection(Protocol protocol, util::HttpListenerBase* http_listener,\n \n   creation_time_ = time(nullptr);\n   last_interaction_ = creation_time_;\n-  memset(name_, 0, sizeof(name_));\n-  memset(phase_, 0, sizeof(phase_));\n   id_ = next_id.fetch_add(1, memory_order_relaxed);\n }\n \n@@ -356,8 +348,8 @@ Connection::~Connection() {\n void Connection::OnShutdown() {\n   VLOG(1) << \"Connection::OnShutdown\";\n \n-  if (shutdown_) {\n-    for (const auto& k_v : shutdown_->map) {\n+  if (shutdown_cb_) {\n+    for (const auto& k_v : shutdown_cb_->map) {\n       k_v.second();\n     }\n   }\n@@ -384,17 +376,17 @@ void Connection::OnPostMigrateThread() {\n }\n \n auto Connection::RegisterShutdownHook(ShutdownCb cb) -> ShutdownHandle {\n-  if (!shutdown_) {\n-    shutdown_ = make_unique<Shutdown>();\n+  if (!shutdown_cb_) {\n+    shutdown_cb_ = make_unique<Shutdown>();\n   }\n-  return shutdown_->Add(std::move(cb));\n+  return shutdown_cb_->Add(std::move(cb));\n }\n \n void Connection::UnregisterShutdownHook(ShutdownHandle id) {\n-  if (shutdown_) {\n-    shutdown_->Remove(id);\n-    if (shutdown_->map.empty())\n-      shutdown_.reset();\n+  if (shutdown_cb_) {\n+    shutdown_cb_->Remove(id);\n+    if (shutdown_cb_->map.empty())\n+      shutdown_cb_.reset();\n   }\n }\n \n@@ -465,11 +457,11 @@ void Connection::HandleRequests() {\n   VLOG(1) << \"Closed connection for peer \" << remote_ep;\n }\n \n-void Connection::RegisterOnBreak(BreakerCb breaker_cb) {\n+void Connection::RegisterBreakHook(BreakerCb breaker_cb) {\n   breaker_cb_ = breaker_cb;\n }\n \n-void Connection::SendMsgVecAsync(PubMessage pub_msg) {\n+void Connection::SendPubMessageAsync(PubMessage pub_msg) {\n   DCHECK(cc_);\n \n   if (cc_->conn_closing) {\n@@ -502,12 +494,15 @@ string Connection::GetClientInfo(unsigned thread_id) const {\n   getsockopt(lsb->native_handle(), SOL_SOCKET, SO_INCOMING_CPU, &cpu, &len);\n   int my_cpu_id = sched_getcpu();\n \n+  static constexpr string_view PHASE_NAMES[] = {\"readsock\", \"process\"};\n+  static_assert(PHASE_NAMES[PROCESS] == \"process\");\n+\n   absl::StrAppend(&res, \"id=\", id_, \" addr=\", re.address().to_string(), \":\", re.port());\n   absl::StrAppend(&res, \" laddr=\", le.address().to_string(), \":\", le.port());\n   absl::StrAppend(&res, \" fd=\", lsb->native_handle(), \" name=\", name_);\n   absl::StrAppend(&res, \" tid=\", thread_id, \" irqmatch=\", int(cpu == my_cpu_id));\n   absl::StrAppend(&res, \" age=\", now - creation_time_, \" idle=\", now - last_interaction_);\n-  absl::StrAppend(&res, \" phase=\", phase_);\n+  absl::StrAppend(&res, \" phase=\", PHASE_NAMES[phase_]);\n \n   if (cc_) {\n     string cc_info = service_->GetContextInfo(cc_.get());\n@@ -575,7 +570,7 @@ void Connection::ConnectionFlow(FiberSocketBase* peer) {\n   // At the start we read from the socket to determine the HTTP/Memstore protocol.\n   // Therefore we may already have some data in the buffer.\n   if (io_buf_.InputLen() > 0) {\n-    SetPhase(\"process\");\n+    phase_ = PROCESS;\n     if (redis_parser_) {\n       parse_status = ParseRedis();\n     } else {\n@@ -647,10 +642,10 @@ auto Connection::ParseRedis() -> ParserStatus {\n   mi_heap_t* tlh = mi_heap_get_backing();\n \n   do {\n-    result = redis_parser_->Parse(io_buf_.InputBuffer(), &consumed, &parse_args_);\n+    result = redis_parser_->Parse(io_buf_.InputBuffer(), &consumed, &tmp_parse_args_);\n \n-    if (result == RedisParser::OK && !parse_args_.empty()) {\n-      RespExpr& first = parse_args_.front();\n+    if (result == RedisParser::OK && !tmp_parse_args_.empty()) {\n+      RespExpr& first = tmp_parse_args_.front();\n       if (first.type == RespExpr::STRING) {\n         DVLOG(2) << \"Got Args with first token \" << ToSV(first.GetBuf());\n       }\n@@ -662,28 +657,18 @@ auto Connection::ParseRedis() -> ParserStatus {\n       bool is_sync_dispatch = !cc_->async_dispatch && !cc_->force_dispatch;\n       if (dispatch_q_.empty() && is_sync_dispatch && consumed >= io_buf_.InputLen()) {\n         // Gradually release the request pool.\n-        // The request pool is shared by all the connections in the thread so we do not want\n-        // to release it aggressively just because some connection is running in\n-        // non-pipelined mode. So we wait at least N times,\n-        // where N is the number of connections in the thread.\n-        if (!free_req_pool_.empty()) {\n-          ++free_req_release_weight;\n-          if (free_req_release_weight > stats_->num_conns) {\n-            free_req_release_weight = 0;\n-            stats_->pipeline_cache_capacity -= free_req_pool_.back()->StorageCapacity();\n-            free_req_pool_.pop_back();\n-          }\n-        }\n-        RespToArgList(parse_args_, &cmd_vec_);\n+        ShrinkPipelinePool();\n+\n+        RespToArgList(tmp_parse_args_, &tmp_cmd_vec_);\n \n-        DVLOG(2) << \"Sync dispatch \" << ToSV(cmd_vec_.front());\n+        DVLOG(2) << \"Sync dispatch \" << ToSV(tmp_cmd_vec_.front());\n \n-        CmdArgList cmd_list{cmd_vec_.data(), cmd_vec_.size()};\n+        CmdArgList cmd_list{tmp_cmd_vec_.data(), tmp_cmd_vec_.size()};\n         service_->DispatchCommand(cmd_list, cc_.get());\n         last_interaction_ = time(nullptr);\n       } else {\n         // Dispatch via queue to speedup input reading.\n-        RequestPtr req = FromArgs(std::move(parse_args_), tlh);\n+        RequestPtr req = FromArgs(std::move(tmp_parse_args_), tlh);\n         ++pipeline_msg_cnt_;\n         dispatch_q_.push_back(std::move(req));\n         if (dispatch_q_.size() == 1) {\n@@ -783,7 +768,7 @@ auto Connection::IoLoop(util::FiberSocketBase* peer) -> variant<error_code, Pars\n     FetchBuilderStats(stats_, builder);\n \n     io::MutableBytes append_buf = io_buf_.AppendBuffer();\n-    SetPhase(\"readsock\");\n+    phase_ = READ_SOCKET;\n \n     ::io::Result<size_t> recv_sz = peer->Recv(append_buf);\n     last_interaction_ = time(nullptr);\n@@ -797,7 +782,7 @@ auto Connection::IoLoop(util::FiberSocketBase* peer) -> variant<error_code, Pars\n     io_buf_.CommitWrite(*recv_sz);\n     stats_->io_read_bytes += *recv_sz;\n     ++stats_->io_read_cnt;\n-    SetPhase(\"process\");\n+    phase_ = PROCESS;\n \n     if (redis_parser_) {\n       parse_status = ParseRedis();\n@@ -864,7 +849,7 @@ void Connection::DispatchFiber(util::FiberSocketBase* peer) {\n \n     if (req->IsPipelineMsg() && stats_->pipeline_cache_capacity < request_cache_limit) {\n       stats_->pipeline_cache_capacity += req->StorageCapacity();\n-      free_req_pool_.push_back(std::move(req));\n+      pipeline_req_pool_.push_back(std::move(req));\n     }\n   }\n \n@@ -889,16 +874,39 @@ auto Connection::FromArgs(RespVec args, mi_heap_t* heap) -> RequestPtr {\n \n   RequestPtr req;\n \n-  if (free_req_pool_.empty()) {\n-    req = Request::New(heap, args, backed_sz);\n+  if (req = GetFromPipelinePool(); req) {\n+    req->Emplace(move(args), backed_sz);\n   } else {\n-    free_req_release_weight = 0;  // Reset the release weight.\n-    req = move(free_req_pool_.back());\n-    stats_->pipeline_cache_capacity -= req->StorageCapacity();\n+    req = Request::New(heap, args, backed_sz);\n+  }\n+  return req;\n+}\n \n-    free_req_pool_.pop_back();\n-    req->Emplace(move(args), backed_sz);\n+void Connection::ShrinkPipelinePool() {\n+  if (pipeline_req_pool_.empty())\n+    return;\n+\n+  // The request pool is shared by all the connections in the thread so we do not want\n+  // to release it aggressively just because some connection is running in\n+  // non-pipelined mode. So by using free_req_release_weight we wait at least N times,\n+  // where N is the number of connections in the thread.\n+  ++free_req_release_weight;\n+\n+  if (free_req_release_weight > stats_->num_conns) {\n+    free_req_release_weight = 0;\n+    stats_->pipeline_cache_capacity -= pipeline_req_pool_.back()->StorageCapacity();\n+    pipeline_req_pool_.pop_back();\n   }\n+}\n+\n+Connection::RequestPtr Connection::GetFromPipelinePool() {\n+  if (pipeline_req_pool_.empty())\n+    return {};\n+\n+  free_req_release_weight = 0;  // Reset the release weight.\n+  RequestPtr req = move(pipeline_req_pool_.back());\n+  stats_->pipeline_cache_capacity -= req->StorageCapacity();\n+  pipeline_req_pool_.pop_back();\n   return req;\n }\n \n@@ -907,7 +915,7 @@ void Connection::ShutdownSelf() {\n }\n \n void Connection::ShutdownThreadLocal() {\n-  free_req_pool_.clear();\n+  pipeline_req_pool_.clear();\n }\n \n void RespToArgList(const RespVec& src, CmdArgVec* dest) {\n@@ -919,7 +927,7 @@ void RespToArgList(const RespVec& src, CmdArgVec* dest) {\n   }\n }\n \n-void Connection::SendMonitorMsg(std::string monitor_msg) {\n+void Connection::SendMonitorMessageAsync(std::string monitor_msg) {\n   DCHECK(cc_);\n \n   if (!cc_->conn_closing) {\ndiff --git a/src/facade/dragonfly_connection.h b/src/facade/dragonfly_connection.h\nindex 0ff26f465547..d7e783d6060e 100644\n--- a/src/facade/dragonfly_connection.h\n+++ b/src/facade/dragonfly_connection.h\n@@ -38,40 +38,30 @@ class RedisParser;\n class ServiceInterface;\n class MemcacheParser;\n \n+// Connection represents an active connection for a client.\n+//\n+// It directly dispatches regular commands from the io-loop.\n+// For pipelined requests, monitor and pubsub messages it uses\n+// a separate dispatch queue that is processed on a separate fiber.\n class Connection : public util::Connection {\n  public:\n   Connection(Protocol protocol, util::HttpListenerBase* http_listener, SSL_CTX* ctx,\n              ServiceInterface* service);\n   ~Connection();\n \n-  using error_code = std::error_code;\n+  using BreakerCb = std::function<void(uint32_t)>;\n   using ShutdownCb = std::function<void()>;\n   using ShutdownHandle = unsigned;\n \n-  ShutdownHandle RegisterShutdownHook(ShutdownCb cb);\n-  void UnregisterShutdownHook(ShutdownHandle id);\n-\n-  Protocol protocol() const {\n-    return protocol_;\n-  }\n-\n-  using BreakerCb = std::function<void(uint32_t)>;\n-  void RegisterOnBreak(BreakerCb breaker_cb);\n-\n-  // This interface is used to pass a published message directly to the socket without\n-  // copying strings.\n-  // Once the msg is sent \"bc\" will be decreased so that caller could release the underlying\n-  // storage for the message.\n-  // virtual - to allow the testing code to override it.\n-\n+  // PubSub message, either incoming message for active subscription or reply for new subscription.\n   struct PubMessage {\n     enum Type { kSubscribe, kUnsubscribe, kPublish } type;\n \n-    std::string pattern{};  // if empty - means its a regular message, otherwise it's pmessage.\n+    std::string pattern{};  // non-empty for pattern subscriber\n     std::shared_ptr<std::string> channel{};\n-    std::shared_ptr<std::string> message{};  // ensure that this message would out live passing\n-                                             // between different threads/fibers\n-    uint32_t channel_cnt = 0;                // relevant only for kSubscribe and kUnsubscribe\n+    std::shared_ptr<std::string> message{};\n+\n+    uint32_t channel_cnt = 0;\n \n     PubMessage(bool add, std::shared_ptr<std::string> channel, uint32_t channel_cnt);\n     PubMessage(std::string pattern, std::shared_ptr<std::string> channel,\n@@ -82,23 +72,28 @@ class Connection : public util::Connection {\n     PubMessage(PubMessage&&) = default;\n   };\n \n-  // this function is overriden at test_utils TestConnection\n-  virtual void SendMsgVecAsync(PubMessage pub_msg);\n+  enum Phase { READ_SOCKET, PROCESS };\n \n-  // Note that this is accepted by value because the message is processed asynchronously.\n-  void SendMonitorMsg(std::string monitor_msg);\n+ public:\n+  // Add PubMessage to dispatch queue.\n+  // Virtual because behaviour is overwritten in test_utils.\n+  virtual void SendPubMessageAsync(PubMessage pub_msg);\n \n-  void SetName(std::string_view name) {\n-    CopyCharBuf(name, sizeof(name_), name_);\n-  }\n+  // Add monitor message to dispatch queue.\n+  void SendMonitorMessageAsync(std::string monitor_msg);\n \n-  const char* GetName() const {\n-    return name_;\n-  }\n+  // Register hook that is executed on connection shutdown.\n+  ShutdownHandle RegisterShutdownHook(ShutdownCb cb);\n \n-  void SetPhase(std::string_view phase) {\n-    CopyCharBuf(phase, sizeof(phase_), phase_);\n-  }\n+  void UnregisterShutdownHook(ShutdownHandle id);\n+\n+  // Register hook that is executen when the connection breaks.\n+  void RegisterBreakHook(BreakerCb breaker_cb);\n+\n+  // Manually shutdown self.\n+  void ShutdownSelf();\n+\n+  static void ShutdownThreadLocal();\n \n   std::string GetClientInfo(unsigned thread_id) const;\n   std::string RemoteEndpointStr() const;\n@@ -106,9 +101,17 @@ class Connection : public util::Connection {\n   std::string LocalBindAddress() const;\n   uint32_t GetClientId() const;\n \n-  void ShutdownSelf();\n+  Protocol protocol() const {\n+    return protocol_;\n+  }\n \n-  static void ShutdownThreadLocal();\n+  void SetName(std::string name) {\n+    name_ = name;\n+  }\n+\n+  std::string_view GetName() const {\n+    return name_;\n+  }\n \n  protected:\n   void OnShutdown() override;\n@@ -118,68 +121,84 @@ class Connection : public util::Connection {\n  private:\n   enum ParserStatus { OK, NEED_MORE, ERROR };\n \n-  void HandleRequests() final;\n+  class Request;\n+  struct DispatchOperations;\n+  struct DispatchCleanup;\n+  struct RequestDeleter;\n+  struct Shutdown;\n \n-  static void CopyCharBuf(std::string_view src, unsigned dest_len, char* dest) {\n-    src = src.substr(0, dest_len - 1);\n-    if (!src.empty())\n-      memcpy(dest, src.data(), src.size());\n-    dest[src.size()] = '\\0';\n-  }\n+  // Requests are allocated on the mimalloc heap and thus require a custom deleter.\n+  using RequestPtr = std::unique_ptr<Request, RequestDeleter>;\n \n-  //\n-  io::Result<bool> CheckForHttpProto(util::FiberSocketBase* peer);\n+ private:\n+  // Check protocol and handle connection.\n+  void HandleRequests() final;\n \n+  // Start dispatch fiber and run IoLoop.\n   void ConnectionFlow(util::FiberSocketBase* peer);\n+\n+  // Main loop reading client messages and passing requests to dispatch queue.\n   std::variant<std::error_code, ParserStatus> IoLoop(util::FiberSocketBase* peer);\n \n+  // Returns true if HTTP header is detected.\n+  io::Result<bool> CheckForHttpProto(util::FiberSocketBase* peer);\n+\n+  // Handles events from dispatch queue.\n   void DispatchFiber(util::FiberSocketBase* peer);\n \n+  // Create new pipeline request, re-use from pool when possible.\n+  RequestPtr FromArgs(RespVec args, mi_heap_t* heap);\n+\n   ParserStatus ParseRedis();\n   ParserStatus ParseMemcache();\n+\n   void OnBreakCb(int32_t mask);\n \n-  base::IoBuf io_buf_;\n+  // Shrink pipeline pool by a little while handling regular commands.\n+  void ShrinkPipelinePool();\n+\n+  // Returns non-null request ptr if pool has vacant entries.\n+  RequestPtr GetFromPipelinePool();\n+\n+ private:\n+  std::deque<RequestPtr> dispatch_q_;  // dispatch queue\n+  dfly::EventCount evc_;               // dispatch queue waker\n+\n+  base::IoBuf io_buf_;  // used in io loop and parsers\n   std::unique_ptr<RedisParser> redis_parser_;\n   std::unique_ptr<MemcacheParser> memcache_parser_;\n+\n+  uint32_t id_;\n+  Protocol protocol_;\n+  ConnectionStats* stats_ = nullptr;\n+\n   util::HttpListenerBase* http_listener_;\n   SSL_CTX* ctx_;\n-  ServiceInterface* service_;\n-  time_t creation_time_, last_interaction_;\n-  char name_[16];\n-  char phase_[16];\n \n-  std::unique_ptr<ConnectionContext> cc_;\n+  ServiceInterface* service_;\n \n-  class Request;\n-  struct DispatchOperations;\n-  struct DispatchCleanup;\n-  struct RequestDeleter;\n+  time_t creation_time_, last_interaction_;\n \n-  using RequestPtr = std::unique_ptr<Request, RequestDeleter>;\n+  Phase phase_;\n+  std::string name_;\n \n-  // args are passed deliberately by value - to pass the ownership.\n-  RequestPtr FromArgs(RespVec args, mi_heap_t* heap);\n+  std::unique_ptr<ConnectionContext> cc_;\n \n-  std::deque<RequestPtr> dispatch_q_;  // coordinated via evc_.\n+  unsigned parser_error_ = 0;\n   uint32_t pipeline_msg_cnt_ = 0;\n \n-  static thread_local std::vector<RequestPtr> free_req_pool_;\n-  dfly::EventCount evc_;\n-\n-  RespVec parse_args_;\n-  CmdArgVec cmd_vec_;\n-\n-  unsigned parser_error_ = 0;\n-  uint32_t id_;\n   uint32_t break_poll_id_ = UINT32_MAX;\n-  ConnectionStats* stats_ = nullptr;\n \n-  Protocol protocol_;\n-\n-  struct Shutdown;\n-  std::unique_ptr<Shutdown> shutdown_;\n   BreakerCb breaker_cb_;\n+  std::unique_ptr<Shutdown> shutdown_cb_;\n+\n+  RespVec tmp_parse_args_;\n+  CmdArgVec tmp_cmd_vec_;\n+\n+  // Pooled pipieline messages per-thread.\n+  // Aggregated while handling pipelines,\n+  // graudally released while handling regular commands.\n+  static thread_local std::vector<RequestPtr> pipeline_req_pool_;\n };\n \n void RespToArgList(const RespVec& src, CmdArgVec* dest);\ndiff --git a/src/server/conn_context.cc b/src/server/conn_context.cc\nindex 17daa81f9904..90b1fe7661f8 100644\n--- a/src/server/conn_context.cc\n+++ b/src/server/conn_context.cc\n@@ -127,7 +127,7 @@ void ConnectionContext::ChangeSubscription(bool to_add, bool to_reply, CmdArgLis\n \n   if (to_reply) {\n     for (size_t i = 0; i < result.size(); ++i) {\n-      owner()->SendMsgVecAsync({to_add, make_shared<string>(ArgS(args, i)), result[i]});\n+      owner()->SendPubMessageAsync({to_add, make_shared<string>(ArgS(args, i)), result[i]});\n     }\n   }\n }\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex 679c7b558e3c..bfec90d1358a 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -184,7 +184,7 @@ void SendMonitor(const std::string& msg) {\n \n     for (auto monitor_conn : monitors) {\n       // never preempts, so we can iterate safely.\n-      monitor_conn->SendMonitorMsg(msg);\n+      monitor_conn->SendMonitorMessageAsync(msg);\n     }\n   }\n }\n@@ -922,7 +922,7 @@ facade::ConnectionContext* Service::CreateContext(util::FiberSocketBase* peer,\n \n   // a bit of a hack. I set up breaker callback here for the owner.\n   // Should work though it's confusing to have it here.\n-  owner->RegisterOnBreak([res, this](uint32_t) {\n+  owner->RegisterBreakHook([res, this](uint32_t) {\n     if (res->transaction) {\n       res->transaction->BreakOnShutdown();\n     }\n@@ -1489,7 +1489,7 @@ void Service::Publish(CmdArgList args, ConnectionContext* cntx) {\n       while (it != subscribers_ptr->end() && it->thread_id == idx) {\n         facade::Connection* conn = it->conn_cntx->owner();\n         DCHECK(conn);\n-        conn->SendMsgVecAsync({move(it->pattern), move(channel_ptr), move(msg_ptr)});\n+        conn->SendPubMessageAsync({move(it->pattern), move(channel_ptr), move(msg_ptr)});\n         it->borrow_token.Dec();\n         it++;\n       }\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 1ebd36aa04af..c80b1db26dc4 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -1134,13 +1134,13 @@ void ServerFamily::Client(CmdArgList args, ConnectionContext* cntx) {\n   string_view sub_cmd = ArgS(args, 0);\n \n   if (sub_cmd == \"SETNAME\" && args.size() == 2) {\n-    cntx->owner()->SetName(ArgS(args, 1));\n+    cntx->owner()->SetName(string{ArgS(args, 1)});\n     return (*cntx)->SendOk();\n   }\n \n   if (sub_cmd == \"GETNAME\") {\n-    const char* name = cntx->owner()->GetName();\n-    if (*name != 0) {\n+    auto name = cntx->owner()->GetName();\n+    if (!name.empty()) {\n       return (*cntx)->SendBulkString(name);\n     } else {\n       return (*cntx)->SendNull();\n",
  "test_patch": "diff --git a/src/server/test_utils.cc b/src/server/test_utils.cc\nindex 815dc7e7030c..4805f2cb52f9 100644\n--- a/src/server/test_utils.cc\n+++ b/src/server/test_utils.cc\n@@ -61,7 +61,7 @@ TestConnection::TestConnection(Protocol protocol, io::StringSink* sink)\n     : facade::Connection(protocol, nullptr, nullptr, nullptr), sink_(sink) {\n }\n \n-void TestConnection::SendMsgVecAsync(PubMessage pmsg) {\n+void TestConnection::SendPubMessageAsync(PubMessage pmsg) {\n   if (pmsg.type == PubMessage::kPublish) {\n     messages.push_back(move(pmsg));\n   } else {\ndiff --git a/src/server/test_utils.h b/src/server/test_utils.h\nindex ea89c986decf..e36a8ab7151d 100644\n--- a/src/server/test_utils.h\n+++ b/src/server/test_utils.h\n@@ -21,7 +21,7 @@ class TestConnection : public facade::Connection {\n  public:\n   TestConnection(Protocol protocol, io::StringSink* sink);\n \n-  void SendMsgVecAsync(PubMessage pmsg) final;\n+  void SendPubMessageAsync(PubMessage pmsg) final;\n \n   std::vector<PubMessage> messages;\n \n",
  "problem_statement": "No backpressure on pubsub and monitor leads to piling up messages\nSame effect as #1034 (memory overflow), though not a leak\r\n\r\nWith pubsub, if the producer is much faster than the consumer, the messages pile up in `Connection::dispatch_q_` as `SendMsgVecAsync` is fully async and does not provide any backpressure\r\n\n",
  "hints_text": "",
  "created_at": "2023-04-15T21:15:08Z",
  "modified_files": [
    "src/facade/dragonfly_connection.cc",
    "src/facade/dragonfly_connection.h",
    "src/server/conn_context.cc",
    "src/server/main_service.cc",
    "src/server/server_family.cc"
  ],
  "modified_test_files": [
    "src/server/test_utils.cc",
    "src/server/test_utils.h"
  ]
}