{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4518,
  "instance_id": "dragonflydb__dragonfly-4518",
  "issue_numbers": [
    "3001"
  ],
  "base_commit": "52d88c2372783534fb2d598131978a102c95bf12",
  "patch": "diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml\nindex ba911ccc303f..0d81066493fa 100644\n--- a/.github/workflows/ci.yml\n+++ b/.github/workflows/ci.yml\n@@ -162,8 +162,12 @@ jobs:\n         run: |\n           cd ${GITHUB_WORKSPACE}/build\n           echo Run ctest -V -L DFLY\n+\n           GLOG_alsologtostderr=1 GLOG_vmodule=rdb_load=1,rdb_save=1,snapshot=1,op_manager=1,op_manager_test=1 \\\n-          FLAGS_fiber_safety_margin=4096 FLAGS_list_experimental_v2=true timeout 20m ctest -V -L DFLY\n+          FLAGS_fiber_safety_margin=4096 FLAGS_list_experimental_v2=true timeout 20m ctest -V -L DFLY -E allocation_tracker_test\n+\n+          # Run allocation tracker test separately without alsologtostderr because it generates a TON of logs.\n+          FLAGS_fiber_safety_margin=4096 timeout 5m ./allocation_tracker_test\n \n           echo \"Running tests with --force_epoll\"\n \n@@ -176,7 +180,9 @@ jobs:\n           EOF\n \n           gdb -ix ./init.gdb --batch -ex r --args ./dragonfly_test --force_epoll\n-          FLAGS_fiber_safety_margin=4096 FLAGS_force_epoll=true GLOG_vmodule=rdb_load=1,rdb_save=1,snapshot=1 timeout 20m ctest -V -L DFLY\n+          FLAGS_fiber_safety_margin=4096 FLAGS_force_epoll=true GLOG_vmodule=rdb_load=1,rdb_save=1,snapshot=1 timeout 20m ctest -V -L DFLY -E allocation_tracker_test\n+\n+          FLAGS_fiber_safety_margin=4096 FLAGS_force_epoll=true timeout 5m ./allocation_tracker_test\n \n           echo \"Finished running tests with --force_epoll\"\n \ndiff --git a/src/facade/command_id.h b/src/facade/command_id.h\nindex 9d4dc82563ed..51760a570289 100644\n--- a/src/facade/command_id.h\n+++ b/src/facade/command_id.h\n@@ -97,7 +97,12 @@ class CommandId {\n \n   // PSUBSCRIBE/PUNSUBSCRIBE variant\n   bool IsPSub() const {\n-    return is_p_sub_;\n+    return is_p_pub_sub_;\n+  }\n+\n+  // SSUBSCRIBE/SUNSUBSCRIBE variant\n+  bool IsShardedPSub() const {\n+    return is_sharded_pub_sub_;\n   }\n \n  protected:\n@@ -118,7 +123,8 @@ class CommandId {\n   bool restricted_ = false;\n \n   bool is_pub_sub_ = false;\n-  bool is_p_sub_ = false;\n+  bool is_sharded_pub_sub_ = false;\n+  bool is_p_pub_sub_ = false;\n };\n \n }  // namespace facade\ndiff --git a/src/facade/facade.cc b/src/facade/facade.cc\nindex cacd8e4bba2b..3eefce36a9f0 100644\n--- a/src/facade/facade.cc\n+++ b/src/facade/facade.cc\n@@ -138,7 +138,9 @@ CommandId::CommandId(const char* name, uint32_t mask, int8_t arity, int8_t first\n   if (name_ == \"PUBLISH\" || name_ == \"SUBSCRIBE\" || name_ == \"UNSUBSCRIBE\") {\n     is_pub_sub_ = true;\n   } else if (name_ == \"PSUBSCRIBE\" || name_ == \"PUNSUBSCRIBE\") {\n-    is_p_sub_ = true;\n+    is_p_pub_sub_ = true;\n+  } else if (name_ == \"SPUBLISH\" || name_ == \"SSUBSCRIBE\" || name_ == \"SUNSUBSCRIBE\") {\n+    is_sharded_pub_sub_ = true;\n   }\n }\n \ndiff --git a/src/server/acl/validator.cc b/src/server/acl/validator.cc\nindex b1f5e39f11be..4089ac1843db 100644\n--- a/src/server/acl/validator.cc\n+++ b/src/server/acl/validator.cc\n@@ -27,7 +27,7 @@ inline bool Matches(std::string_view pattern, std::string_view target) {\n \n   std::pair<bool, AclLog::Reason> auth_res;\n \n-  if (id.IsPubSub()) {\n+  if (id.IsPubSub() || id.IsShardedPSub()) {\n     auth_res = IsPubSubCommandAuthorized(false, cntx.acl_commands, cntx.pub_sub, tail_args, id);\n   } else if (id.IsPSub()) {\n     auth_res = IsPubSubCommandAuthorized(true, cntx.acl_commands, cntx.pub_sub, tail_args, id);\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex c2441d630534..1bc00ee47e18 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -920,6 +920,20 @@ void Service::Shutdown() {\n   facade::Connection::Shutdown();\n }\n \n+OpResult<KeyIndex> Service::FindKeys(const CommandId* cid, CmdArgList args) {\n+  if (!cid->IsShardedPSub()) {\n+    return DetermineKeys(cid, args);\n+  }\n+\n+  // Sharded pub sub\n+  // Command form: SPUBLISH shardchannel message\n+  if (cid->name() == registry_.RenamedOrOriginal(\"SPUBLISH\")) {\n+    return {KeyIndex(0, 1)};\n+  }\n+\n+  return {KeyIndex(0, args.size())};\n+}\n+\n optional<ErrorReply> Service::CheckKeysOwnership(const CommandId* cid, CmdArgList args,\n                                                  const ConnectionContext& dfly_cntx) {\n   if (dfly_cntx.is_replicating) {\n@@ -927,11 +941,12 @@ optional<ErrorReply> Service::CheckKeysOwnership(const CommandId* cid, CmdArgLis\n     return nullopt;\n   }\n \n-  if (cid->first_key_pos() == 0) {\n+  if (cid->first_key_pos() == 0 && !cid->IsShardedPSub()) {\n     return nullopt;  // No key command.\n   }\n \n-  OpResult<KeyIndex> key_index_res = DetermineKeys(cid, args);\n+  OpResult<KeyIndex> key_index_res = FindKeys(cid, args);\n+\n   if (!key_index_res) {\n     return ErrorReply{key_index_res.status()};\n   }\n@@ -2244,8 +2259,9 @@ void Service::Exec(CmdArgList args, const CommandContext& cmd_cntx) {\n   VLOG(2) << \"Exec completed\";\n }\n \n-void Service::Publish(CmdArgList args, const CommandContext& cmd_cntx) {\n-  if (IsClusterEnabled()) {\n+namespace {\n+void PublishImpl(bool reject_cluster, CmdArgList args, const CommandContext& cmd_cntx) {\n+  if (reject_cluster && IsClusterEnabled()) {\n     return cmd_cntx.rb->SendError(\"PUBLISH is not supported in cluster mode yet\");\n   }\n   string_view channel = ArgS(args, 0);\n@@ -2255,17 +2271,17 @@ void Service::Publish(CmdArgList args, const CommandContext& cmd_cntx) {\n   cmd_cntx.rb->SendLong(cs->SendMessages(channel, messages));\n }\n \n-void Service::Subscribe(CmdArgList args, const CommandContext& cmd_cntx) {\n-  if (IsClusterEnabled()) {\n+void SubscribeImpl(bool reject_cluster, CmdArgList args, const CommandContext& cmd_cntx) {\n+  if (reject_cluster && IsClusterEnabled()) {\n     return cmd_cntx.rb->SendError(\"SUBSCRIBE is not supported in cluster mode yet\");\n   }\n   cmd_cntx.conn_cntx->ChangeSubscription(true /*add*/, true /* reply*/, std::move(args),\n                                          static_cast<RedisReplyBuilder*>(cmd_cntx.rb));\n }\n \n-void Service::Unsubscribe(CmdArgList args, const CommandContext& cmd_cntx) {\n+void UnSubscribeImpl(bool reject_cluster, CmdArgList args, const CommandContext& cmd_cntx) {\n   auto* rb = static_cast<RedisReplyBuilder*>(cmd_cntx.rb);\n-  if (IsClusterEnabled()) {\n+  if (reject_cluster && IsClusterEnabled()) {\n     return cmd_cntx.rb->SendError(\"UNSUBSCRIBE is not supported in cluster mode yet\");\n   }\n \n@@ -2276,6 +2292,32 @@ void Service::Unsubscribe(CmdArgList args, const CommandContext& cmd_cntx) {\n   }\n }\n \n+}  // namespace\n+\n+void Service::Publish(CmdArgList args, const CommandContext& cmd_cntx) {\n+  PublishImpl(true, args, cmd_cntx);\n+}\n+\n+void Service::SPublish(CmdArgList args, const CommandContext& cmd_cntx) {\n+  PublishImpl(false, args, cmd_cntx);\n+}\n+\n+void Service::Subscribe(CmdArgList args, const CommandContext& cmd_cntx) {\n+  SubscribeImpl(true, args, cmd_cntx);\n+}\n+\n+void Service::SSubscribe(CmdArgList args, const CommandContext& cmd_cntx) {\n+  SubscribeImpl(false, args, cmd_cntx);\n+}\n+\n+void Service::Unsubscribe(CmdArgList args, const CommandContext& cmd_cntx) {\n+  UnSubscribeImpl(true, args, cmd_cntx);\n+}\n+\n+void Service::SUnsubscribe(CmdArgList args, const CommandContext& cmd_cntx) {\n+  UnSubscribeImpl(false, args, cmd_cntx);\n+}\n+\n void Service::PSubscribe(CmdArgList args, const CommandContext& cmd_cntx) {\n   auto* rb = static_cast<RedisReplyBuilder*>(cmd_cntx.rb);\n \n@@ -2641,9 +2683,13 @@ void Service::Register(CommandRegistry* registry) {\n              .SetValidator(&EvalValidator)\n       << CI{\"EXEC\", CO::LOADING | CO::NOSCRIPT, 1, 0, 0, acl::kExec}.MFUNC(Exec)\n       << CI{\"PUBLISH\", CO::LOADING | CO::FAST, 3, 0, 0, acl::kPublish}.MFUNC(Publish)\n+      << CI{\"SPUBLISH\", CO::LOADING | CO::FAST, 3, 0, 0, acl::kPublish}.MFUNC(SPublish)\n       << CI{\"SUBSCRIBE\", CO::NOSCRIPT | CO::LOADING, -2, 0, 0, acl::kSubscribe}.MFUNC(Subscribe)\n+      << CI{\"SSUBSCRIBE\", CO::NOSCRIPT | CO::LOADING, -2, 0, 0, acl::kSubscribe}.MFUNC(SSubscribe)\n       << CI{\"UNSUBSCRIBE\", CO::NOSCRIPT | CO::LOADING, -1, 0, 0, acl::kUnsubscribe}.MFUNC(\n              Unsubscribe)\n+      << CI{\"SUNSUBSCRIBE\", CO::NOSCRIPT | CO::LOADING, -1, 0, 0, acl::kUnsubscribe}.MFUNC(\n+             SUnsubscribe)\n       << CI{\"PSUBSCRIBE\", CO::NOSCRIPT | CO::LOADING, -2, 0, 0, acl::kPSubscribe}.MFUNC(PSubscribe)\n       << CI{\"PUNSUBSCRIBE\", CO::NOSCRIPT | CO::LOADING, -1, 0, 0, acl::kPUnsubsribe}.MFUNC(\n              PUnsubscribe)\ndiff --git a/src/server/main_service.h b/src/server/main_service.h\nindex 151f61c08518..32a7386deca8 100644\n--- a/src/server/main_service.h\n+++ b/src/server/main_service.h\n@@ -136,8 +136,11 @@ class Service : public facade::ServiceInterface {\n   void EvalShaRo(CmdArgList args, const CommandContext& cmd_cntx);\n   void Exec(CmdArgList args, const CommandContext& cmd_cntx);\n   void Publish(CmdArgList args, const CommandContext& cmd_cntx);\n+  void SPublish(CmdArgList args, const CommandContext& cmd_cntx);\n   void Subscribe(CmdArgList args, const CommandContext& cmd_cntx);\n+  void SSubscribe(CmdArgList args, const CommandContext& cmd_cntx);\n   void Unsubscribe(CmdArgList args, const CommandContext& cmd_cntx);\n+  void SUnsubscribe(CmdArgList args, const CommandContext& cmd_cntx);\n   void PSubscribe(CmdArgList args, const CommandContext& cmd_cntx);\n   void PUnsubscribe(CmdArgList args, const CommandContext& cmd_cntx);\n   void Function(CmdArgList args, const CommandContext& cmd_cntx);\n@@ -169,6 +172,8 @@ class Service : public facade::ServiceInterface {\n \n   void CallFromScript(ConnectionContext* cntx, Interpreter::CallArgs& args);\n \n+  OpResult<KeyIndex> FindKeys(const CommandId* cid, CmdArgList args);\n+\n   void RegisterCommands();\n   void Register(CommandRegistry* registry);\n \n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex 7839a7617f85..a6d8f3e92130 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -2940,3 +2940,43 @@ async def do_migration(index):\n     await seed\n     capture = await seeder.capture_fake_redis()\n     assert await seeder.compare(capture, nodes[1].instance.port)\n+\n+\n+@dfly_args({\"proactor_threads\": 2, \"cluster_mode\": \"yes\"})\n+async def test_cluster_sharded_pub_sub(df_factory: DflyInstanceFactory):\n+    nodes = [df_factory.create(port=next(next_port)) for i in range(2)]\n+    df_factory.start_all(nodes)\n+\n+    c_nodes = [node.client() for node in nodes]\n+\n+    nodes_info = [(await create_node_info(instance)) for instance in nodes]\n+    nodes_info[0].slots = [(0, 16383)]\n+    nodes_info[1].slots = []\n+\n+    await push_config(json.dumps(generate_config(nodes_info)), [node.client for node in nodes_info])\n+    # channel name kostas crc is at slot 2883 which is part of the first node.\n+    with pytest.raises(redis.exceptions.ResponseError) as moved_error:\n+        await c_nodes[1].execute_command(\"SSUBSCRIBE kostas\")\n+\n+    assert str(moved_error.value) == f\"MOVED 2833 127.0.0.1:{nodes[0].port}\"\n+\n+    node_a = ClusterNode(\"localhost\", nodes[0].port)\n+    node_b = ClusterNode(\"localhost\", nodes[1].port)\n+\n+    consumer_client = RedisCluster(startup_nodes=[node_a, node_b])\n+    consumer = consumer_client.pubsub()\n+    consumer.ssubscribe(\"kostas\")\n+\n+    await c_nodes[0].execute_command(\"SPUBLISH kostas hello\")\n+\n+    # Consume subscription message result from above\n+    message = consumer.get_sharded_message(target_node=node_a)\n+    assert message == {\"type\": \"subscribe\", \"pattern\": None, \"channel\": b\"kostas\", \"data\": 1}\n+\n+    message = consumer.get_sharded_message(target_node=node_a)\n+    assert message == {\"type\": \"message\", \"pattern\": None, \"channel\": b\"kostas\", \"data\": b\"hello\"}\n+\n+    consumer.sunsubscribe(\"kostas\")\n+    await c_nodes[0].execute_command(\"SPUBLISH kostas new_message\")\n+    message = consumer.get_sharded_message(target_node=node_a)\n+    assert message == {\"type\": \"unsubscribe\", \"pattern\": None, \"channel\": b\"kostas\", \"data\": 0}\n",
  "problem_statement": "Add support for sharded pub/sub SPUBLISH/SSUBSCRIBE etc\nRueidis tests use SPUBLISH. We should implement this.\n",
  "hints_text": "",
  "created_at": "2025-01-28T12:54:11Z",
  "modified_files": [
    ".github/workflows/ci.yml",
    "src/facade/command_id.h",
    "src/facade/facade.cc",
    "src/server/acl/validator.cc",
    "src/server/main_service.cc",
    "src/server/main_service.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py"
  ]
}