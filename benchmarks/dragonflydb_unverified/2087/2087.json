{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 2087,
  "instance_id": "dragonflydb__dragonfly-2087",
  "issue_numbers": [
    "2066"
  ],
  "base_commit": "502efd80b2bd9e6b3606f7b5a57d9cb814ce7342",
  "patch": "diff --git a/src/core/dense_set.cc b/src/core/dense_set.cc\nindex 860dc1b2e983..b0bd553cfca3 100644\n--- a/src/core/dense_set.cc\n+++ b/src/core/dense_set.cc\n@@ -159,12 +159,22 @@ void DenseSet::ClearInternal() {\n   for (auto it = entries_.begin(); it != entries_.end(); ++it) {\n     while (!it->IsEmpty()) {\n       bool has_ttl = it->HasTtl();\n+      bool is_displ = it->IsDisplaced();\n       void* obj = PopDataFront(it);\n+      int32_t delta = int32_t(BucketId(obj, 0)) - int32_t(it - entries_.begin());\n+      if (is_displ) {\n+        DCHECK(delta < 2 || delta > -2);\n+      } else {\n+        DCHECK_EQ(delta, 0);\n+      }\n       ObjDelete(obj, has_ttl);\n     }\n   }\n \n   entries_.clear();\n+  num_used_buckets_ = 0;\n+  num_chain_entries_ = 0;\n+  size_ = 0;\n }\n \n bool DenseSet::Equal(DensePtr dptr, const void* ptr, uint32_t cookie) const {\n@@ -234,16 +244,14 @@ void DenseSet::Reserve(size_t sz) {\n \n   sz = absl::bit_ceil(sz);\n   if (sz > entries_.size()) {\n+    size_t prev_size = entries_.size();\n     entries_.resize(sz);\n     capacity_log_ = absl::bit_width(sz) - 1;\n+    Grow(prev_size);\n   }\n }\n \n-void DenseSet::Grow() {\n-  size_t prev_size = entries_.size();\n-  entries_.resize(prev_size * 2);\n-  ++capacity_log_;\n-\n+void DenseSet::Grow(size_t prev_size) {\n   // perform rehashing of items in the set\n   for (long i = prev_size - 1; i >= 0; --i) {\n     DensePtr* curr = &entries_[i];\n@@ -299,6 +307,7 @@ void DenseSet::Grow() {\n           }\n \n           DVLOG(2) << \" Pushing to \" << bid << \" \" << dptr.GetObject();\n+          DCHECK_EQ(BucketId(dptr.GetObject(), 0), bid);\n           PushFront(dest, dptr);\n \n           dest->ClearDisplaced();\n@@ -371,7 +380,11 @@ void DenseSet::AddUnique(void* obj, bool has_ttl, uint64_t hashcode) {\n       break;\n     }\n \n-    Grow();\n+    size_t prev_size = entries_.size();\n+    entries_.resize(prev_size * 2);\n+    ++capacity_log_;\n+\n+    Grow(prev_size);\n     bucket_id = BucketId(hashcode);\n   }\n \n@@ -403,6 +416,7 @@ void DenseSet::AddUnique(void* obj, bool has_ttl, uint64_t hashcode) {\n     ++num_chain_entries_;\n   }\n \n+  DCHECK_EQ(BucketId(to_insert.GetObject(), 0), bucket_id);\n   ChainVectorIterator list = entries_.begin() + bucket_id;\n   PushFront(list, to_insert);\n   obj_malloc_used_ += ObjectAllocSize(obj);\ndiff --git a/src/core/dense_set.h b/src/core/dense_set.h\nindex 8a6790237376..2ad3c019c9f8 100644\n--- a/src/core/dense_set.h\n+++ b/src/core/dense_set.h\n@@ -336,7 +336,7 @@ class DenseSet {\n   // return if bucket has no item which is not displaced and right/left bucket has no displaced item\n   // belong to given bid\n   bool NoItemBelongsBucket(uint32_t bid) const;\n-  void Grow();\n+  void Grow(size_t prev_size);\n \n   // ============ Pseudo Linked List Functions for interacting with Chains ==================\n   size_t PushFront(ChainVectorIterator, void* obj, bool has_ttl);\ndiff --git a/src/core/string_map.cc b/src/core/string_map.cc\nindex 76cf10361d1e..2842b01199a3 100644\n--- a/src/core/string_map.cc\n+++ b/src/core/string_map.cc\n@@ -62,7 +62,7 @@ pair<sds, uint64_t> CreateEntry(string_view field, string_view value, uint32_t t\n }  // namespace\n \n StringMap::~StringMap() {\n-  Clear();\n+  ClearInternal();\n }\n \n bool StringMap::AddOrUpdate(string_view field, string_view value, uint32_t ttl_sec) {\n@@ -276,4 +276,16 @@ detail::SdsPair StringMap::iterator::BreakToPair(void* obj) {\n   return detail::SdsPair(f, GetValue(f));\n }\n \n+bool StringMap::iterator::ReallocIfNeeded(float ratio) {\n+  // Unwrap all links to correctly call SetObject()\n+  auto* ptr = curr_entry_;\n+  while (ptr->IsLink())\n+    ptr = ptr->AsLink();\n+\n+  auto* obj = ptr->GetObject();\n+  auto [new_obj, realloced] = static_cast<StringMap*>(owner_)->ReallocIfNeeded(obj, ratio);\n+  ptr->SetObject(new_obj);\n+  return realloced;\n+}\n+\n }  // namespace dfly\ndiff --git a/src/core/string_map.h b/src/core/string_map.h\nindex d4961ec6eeae..7388f70998fe 100644\n--- a/src/core/string_map.h\n+++ b/src/core/string_map.h\n@@ -67,17 +67,7 @@ class StringMap : public DenseSet {\n \n     // Try reducing memory fragmentation of the value by re-allocating. Returns true if\n     // re-allocation happened.\n-    bool ReallocIfNeeded(float ratio) {\n-      // Unwrap all links to correctly call SetObject()\n-      auto* ptr = curr_entry_;\n-      while (ptr->IsLink())\n-        ptr = ptr->AsLink();\n-\n-      auto* obj = ptr->GetObject();\n-      auto [new_obj, realloced] = static_cast<StringMap*>(owner_)->ReallocIfNeeded(obj, ratio);\n-      ptr->SetObject(new_obj);\n-      return realloced;\n-    }\n+    bool ReallocIfNeeded(float ratio);\n \n     iterator& operator++() {\n       Advance();\ndiff --git a/src/server/dfly_main.cc b/src/server/dfly_main.cc\nindex 897fe5ad8f36..b814d1339dd6 100644\n--- a/src/server/dfly_main.cc\n+++ b/src/server/dfly_main.cc\n@@ -463,6 +463,8 @@ bool RunEngine(ProactorPool* pool, AcceptServer* acceptor) {\n \n   // Start the acceptor loop and wait for the server to shutdown.\n   acceptor->Run();\n+  google::FlushLogFiles(google::INFO);  // Flush the header.\n+\n   acceptor->Wait();\n \n   version_monitor.Shutdown();\ndiff --git a/src/server/hset_family.cc b/src/server/hset_family.cc\nindex 6aeacaa15e12..531f9a2a32d6 100644\n--- a/src/server/hset_family.cc\n+++ b/src/server/hset_family.cc\n@@ -683,10 +683,12 @@ OpResult<uint32_t> OpSet(const OpArgs& op_args, string_view key, CmdArgList valu\n     bool added;\n \n     for (size_t i = 0; i < values.size(); i += 2) {\n+      string_view field = ToSV(values[i]);\n+      string_view value = ToSV(values[i + 1]);\n       if (op_sp.skip_if_exists)\n-        added = sm->AddOrSkip(ToSV(values[i]), ToSV(values[i + 1]), op_sp.ttl);\n+        added = sm->AddOrSkip(field, value, op_sp.ttl);\n       else\n-        added = sm->AddOrUpdate(ToSV(values[i]), ToSV(values[i + 1]), op_sp.ttl);\n+        added = sm->AddOrUpdate(field, value, op_sp.ttl);\n \n       created += unsigned(added);\n     }\ndiff --git a/src/server/rdb_load.cc b/src/server/rdb_load.cc\nindex 41abba5ce7df..b55169ce6bf9 100644\n--- a/src/server/rdb_load.cc\n+++ b/src/server/rdb_load.cc\n@@ -616,7 +616,7 @@ void RdbLoaderBase::OpaqueObjLoader::CreateHMap(const LoadTrace* ltrace) {\n           return;\n \n         if (!string_map->AddOrSkip(key, val)) {\n-          LOG(ERROR) << \"Duplicate hash fields detected\";\n+          LOG(ERROR) << \"Duplicate hash fields detected for field \" << key;\n           ec_ = RdbError(errc::rdb_file_corrupted);\n           return;\n         }\n@@ -2272,6 +2272,7 @@ void RdbLoader::LoadItemsBuffer(DbIndex db_ind, const ItemsBuf& ib) {\n   for (const auto* item : ib) {\n     PrimeValue pv;\n     if (ec_ = FromOpaque(item->val, &pv); ec_) {\n+      LOG(ERROR) << \"Could not load value for key '\" << item->key << \"' in DB \" << db_ind;\n       stop_early_ = true;\n       break;\n     }\ndiff --git a/src/server/rdb_save.cc b/src/server/rdb_save.cc\nindex 71b6210d03d2..bd1903bcff6f 100644\n--- a/src/server/rdb_save.cc\n+++ b/src/server/rdb_save.cc\n@@ -299,8 +299,10 @@ io::Result<uint8_t> RdbSerializer::SaveEntry(const PrimeKey& pk, const PrimeValu\n     return make_unexpected(ec);\n \n   ec = SaveValue(pv);\n-  if (ec)\n+  if (ec) {\n+    LOG(ERROR) << \"Problems saving value for key \" << key << \" in dbid=\" << dbid;\n     return make_unexpected(ec);\n+  }\n \n   return rdb_type;\n }\n",
  "test_patch": "diff --git a/src/core/string_set_test.cc b/src/core/string_set_test.cc\nindex 576fe0a830d7..1c7e27e8ee53 100644\n--- a/src/core/string_set_test.cc\n+++ b/src/core/string_set_test.cc\n@@ -418,4 +418,34 @@ TEST_F(StringSetTest, Ttl) {\n   }\n }\n \n+TEST_F(StringSetTest, Grow) {\n+  mt19937 generator(0);\n+\n+  for (size_t j = 0; j < 10; ++j) {\n+    for (size_t i = 0; i < 4098; ++i) {\n+      ss_->Reserve(generator() % 256);\n+      auto str = random_string(generator, 3);\n+      ss_->Add(str);\n+    }\n+    ss_->Clear();\n+  }\n+}\n+\n+TEST_F(StringSetTest, Reserve) {\n+  vector<string> strs;\n+  mt19937 generator(0);\n+\n+  for (size_t i = 0; i < 10; ++i) {\n+    strs.push_back(random_string(generator, 10));\n+    ss_->Add(strs.back());\n+  }\n+\n+  for (size_t j = 2; j < 20; j += 3) {\n+    ss_->Reserve(j * 20);\n+    for (size_t i = 0; i < 10; ++i) {\n+      ASSERT_TRUE(ss_->Contains(strs[i]));\n+    }\n+  }\n+}\n+\n }  // namespace dfly\n",
  "problem_statement": "Duplicate fields in hashmap\nI have a 3 nodes running dragonfly 1 master and 2 replicas.\r\nThe access to dragonfly instances is over haproxy with 2 backend: 1 for write (master) and 1 for read (master/replica)\r\n\r\nThe kernel version on servers are 5.15.0-73-generic on all servers.\r\n\r\nThe hardware is:\r\nMaster: 96 cores and 376 Gb ram\r\nReplica 1: 48 cores and 256 Gb ram\r\nReplica 1: 48 cores and 256 Gb ram\r\n\r\nThe load of clients is:\r\n\r\nOn Master:\r\n# Clients\r\nconnected_clients:402\r\nclient_read_buffer_bytes:14292480\r\nblocked_clients:0\r\ndispatch_queue_entries:0\r\n\r\nOn replica 1:\r\n# Clients\r\nconnected_clients:188\r\nclient_read_buffer_bytes:52480\r\nblocked_clients:0\r\ndispatch_queue_entries:108365\r\n\r\nOn replica 2:\r\nconnected_clients:202\r\nclient_read_buffer_bytes:54272\r\nblocked_clients:0\r\ndispatch_queue_entries:61381\r\n\r\nThe replication status is:\r\nOn Master:\r\nrole:master\r\nconnected_slaves:2\r\nslave0:ip=172.16.0.23,port=6392,state=stable_sync,lag=74\r\nslave1:ip=172.16.0.25,port=6392,state=stable_sync,lag=61\r\nmaster_replid:2c604ff0c72153701e8be30d707aa72d8f9d4ca9\r\n\r\nOn replica 1:\r\nrole:replica\r\nmaster_host:172.16.0.24\r\nmaster_port:6392\r\nmaster_link_status:up\r\nmaster_last_io_seconds_ago:0\r\nmaster_sync_in_progress:0\r\n\r\nOn replica 2:\r\nrole:replica\r\nmaster_host:172.16.0.24\r\nmaster_port:6392\r\nmaster_link_status:up\r\nmaster_last_io_seconds_ago:0\r\nmaster_sync_in_progress:0\r\n\r\nThe CPU on master consum is over 20%\r\nThe CPU on replica 1 consum is over 370%\r\nThe CPU on replica 1 consum is over 300%\r\n\r\nAny log on dragonfly log files.\r\n\r\nThe nodes run sentinel and show multiple:\r\n1303:X 24 Oct 2023 19:02:26.831 # +sdown slave 172.16.0.23:6392 172.16.0.23 6392 @ ovh-innova 172.16.0.24 6392\r\n\r\nI think these errors are due to the high CPU load of the replica nodes\r\n\r\nHow is it possible that for many more clients the CPU load of the master is much lower than that of the replicas??\r\n\r\nI attach info command on all nodes.\r\n\r\nThanks\r\n[dfly_info.log](https://github.com/dragonflydb/dragonfly/files/13121950/dfly_info.log)\r\n\n",
  "hints_text": "Thanks for reporting this. Can you please go to replica url: `http://replica:6379/profilez?profile=on`  this will start profiling the server, you should see in the INFO log the path to `.prof` file. Then, after 30s change the url to `http://replica:6379/profilez?profile=off` to flush the profiler file. We will need this file to see what the bottlenecks are. \nif you install `pprof` on your replica machine before starting the profiling, the browser will render the profiler graph that is also useful. \nHi I try to access this URL, on my case 172.16.0.25:6392/profilez?profile=on over browser and get beautiful image ;)\r\n\r\n<img width=\"568\" alt=\"image\" src=\"https://github.com/dragonflydb/dragonfly/assets/62182142/3bdafc31-2b40-4789-99b4-9dd443d86c83\">\r\n\r\nBut when go to 172.16.0.25:6392/profilez?profile=off redirect to http://172.16.0.25:6392/filez?file=/tmp/profile/dragonfly_24102023_183917.prof.svg and get this error\r\n<img width=\"900\" alt=\"image\" src=\"https://github.com/dragonflydb/dragonfly/assets/62182142/ae61770f-419b-447c-b559-6a6e291290ab\">\r\n\r\nOn /tmp directory not exist the file...\r\n\r\nDo you kown on that ubuntu package is pprof ?\r\n\r\n\r\n\nwhere dragonfly logs are located? did you install dragonfly from a debian package?\nThe logs is on /var/log/dragonfly this directory has nothing but dragonfly's log files.\r\n If you need me to install pprof, I can't find the ubuntu package where that \"utility\" is located\ncan you check if `/tmp/` has `profile` directory  and whether it has anything?\nYes I check /tmp directory and not contain profile directory\r\nI test profile on other replica and the result is the same no profile directory on /tmp\r\n\nI show on logs:\r\n\r\n`I20231024 20:58:14.724368 2454603 profilez_handler.cc:53] Starting profiling into /tmp/profile/dragonfly_24102023_185814.prof 1\r\n\r\nI20231024 20:58:48.925976 2456502 profilez_handler.cc:70] Symbols /usr/bin/dragonfly.debug, suffix: _24102023_185814.prof\r\n\r\nI20231024 20:58:48.926085 2456502 profilez_handler.cc:84] Running command: nice -n 15 pprof -noinlines -lines -unit ms --svg /usr/bin/dragonfly /tmp/profile/dragonfly_24102023_185814.prof > /tmp/profile/dragonfly_24102023_185814.prof.svg 2> /tmp/profile/dragonfly_24102023_185814.prof.err\r\n\r\nI20231024 20:58:48.929224 2456502 profilez_handler.cc:95] Redirecting to filez?file=/tmp/profile/dragonfly_24102023_185814.prof.svg\r\n\r\nI20231024 21:00:18.047973 4010978 save_stages_controller.cc:245] Saving \"/var/lib/dragonfly/dump-2023-10-24T21:00:10-summary.dfs\" finished after 7.71 s` \r\n\r\nBut /tmp/profile not exits\r\n\r\nI try to create manually /tmp/profile and change permissions to 777  an re run the profile. The result is the same nothing write on /tmp/profile\r\n\r\n\nI install pprof go command ;)\r\n\r\nWhen run profile show on log\r\n\r\n`I20231024 21:19:56.859346 2576396 profilez_handler.cc:53] Starting profiling into /tmp/profile/dragonfly_24102023_191956.prof 1`\r\n\r\nbut the file is not created\nI think the directory that you are checking as `/tmp` is not the directory that is `/tmp` for Dragonfly. In your example, you navigated to the svg file which gave an error that the file was empty (and thus an invalid SVG). This happens when pprof is not installed, because the command pipeline will create an empty file on error. Therefore /tmp/profile... exists. You still did not mention how you are running Dragonfly. Is it perhaps within a container?\nnop, I run dragonfly on bare metal\nour systemd has private tmp folder.\r\nplease do the following: \r\n```sh\r\ndfpid=$(pidof dragonfly)\r\nsudo nsenter -t $dfpid -m ls /tmp\r\n```\nok, I show the files, how copy this files?\r\n<img width=\"701\" alt=\"image\" src=\"https://github.com/dragonflydb/dragonfly/assets/62182142/e5bb3d20-652d-4b29-9d4d-6735d08d1b26\">\r\n\n```\r\nls /proc/$(pidof dragonfly)/root/tmp/profile\r\n```\r\nand then you can copy from the private tmp namespace to the regular /tmp dir\r\n\r\n```\r\ncp /proc/$(pidof dragonfly)/root/tmp/profile/something.prof /tmp/realtmp.prof\r\n```\n[Archive.zip](https://github.com/dragonflydb/dragonfly/files/13126803/Archive.zip)\r\n\nThank you @UkuLoskit  - I did not know this.\nUnfortunately the profiler data is not very useful, or at least looks very, very weird as if it shows wrong function names. \r\nwhat's your ubuntu version and the architecture `uname -a` and `cat /etc/os-release` ? \nAlso, can you please provide \"info all\" output on the master and on the replica?\nmaster\r\nLinux ovh-innova 5.15.0-73-generic #80~20.04.1-Ubuntu SMP Wed May 17 14:58:14 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.6 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n\r\n-----------------------\r\nreplica 1\r\nLinux innova2 5.15.0-73-generic #80-Ubuntu SMP Mon May 15 15:18:26 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nPRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n\r\n-------------------------\r\nreplica 2\r\nLinux innova2 5.15.0-73-generic #80-Ubuntu SMP Mon May 15 15:18:26 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nPRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n[dfly_info.log](https://github.com/dragonflydb/dragonfly/files/13127861/dfly_info.log)\r\n\nI try\r\n1.- Stop dragonfly on replica 1\r\n2.- rm /var/lib/dragonfly/*\r\n3.- start dragonfly on replica 1\r\n\r\nGet this error:\r\nE20231025 07:30:31.125586 866245 rdb_load.cc:619] Duplicate hash fields detected\r\nE20231025 07:30:31.135639 866244 rdb_load.cc:619] Duplicate hash fields detected\r\nE20231025 07:30:31.135838 866246 rdb_load.cc:619] Duplicate hash fields detected\r\nE20231025 07:30:31.142112 866248 rdb_load.cc:619] Duplicate hash fields detected\r\nE20231025 07:30:31.147995 866241 rdb_load.cc:619] Duplicate hash fields detected\r\nE20231025 07:30:31.149091 866247 rdb_load.cc:619] Duplicate hash fields detected\r\nE20231025 07:30:31.161890 866243 rdb_load.cc:619] Duplicate hash fields detected\r\nE20231025 07:30:31.177135 866242 rdb_load.cc:619] Duplicate hash fields detected\r\nW20231025 07:30:31.178701 866243 replica.cc:214] Error syncing with 172.16.0.24:6392 dragonfly.rdbload:5 Internal error when loading RDB file 5\r\n\r\n\ncan you please DM me on discord? will be easier to talk\n I would like to give you a dev version of dragonfly that will help with understanding both issues you are experiencing. Please tell me if you are interested.   \nHello, I think the problem comes from \"duplicity of sets\". I have stopped the three nodes and tried to start in order. When I tried to start the master I got the duplicate sets error and the only way to start was to delete the saved files. Are you interested in me sending you the database files?\nyes, very much so\nCan I send you the files privately?\nsure. Can you DM me on discord @fernandomacho  ?\nI just created an account on discord, I've never used this, from what I see I have to ask you for contact right? Your username on discord is romange?",
  "created_at": "2023-10-28T17:38:19Z",
  "modified_files": [
    "src/core/dense_set.cc",
    "src/core/dense_set.h",
    "src/core/string_map.cc",
    "src/core/string_map.h",
    "src/server/dfly_main.cc",
    "src/server/hset_family.cc",
    "src/server/rdb_load.cc",
    "src/server/rdb_save.cc"
  ],
  "modified_test_files": [
    "src/core/string_set_test.cc"
  ]
}