{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4142,
  "instance_id": "dragonflydb__dragonfly-4142",
  "issue_numbers": [
    "4139"
  ],
  "base_commit": "267d5ab370eab6e34d676308b83b0ddff0fa95c9",
  "patch": "diff --git a/src/server/common.cc b/src/server/common.cc\nindex 603455c82710..39d4acaff648 100644\n--- a/src/server/common.cc\n+++ b/src/server/common.cc\n@@ -122,6 +122,10 @@ size_t max_memory_limit = 0;\n size_t serialization_max_chunk_size = 0;\n Namespaces* namespaces = nullptr;\n \n+size_t FetchRssMemory(io::StatusData sdata) {\n+  return sdata.vm_rss + sdata.hugetlb_pages;\n+}\n+\n const char* GlobalStateName(GlobalState s) {\n   switch (s) {\n     case GlobalState::ACTIVE:\ndiff --git a/src/server/common.h b/src/server/common.h\nindex 28c41da4e12f..ac890b225e9b 100644\n--- a/src/server/common.h\n+++ b/src/server/common.h\n@@ -17,6 +17,7 @@\n #include \"core/compact_object.h\"\n #include \"facade/facade_types.h\"\n #include \"facade/op_status.h\"\n+#include \"helio/io/proc_reader.h\"\n #include \"util/fibers/fibers.h\"\n #include \"util/fibers/synchronization.h\"\n \n@@ -132,6 +133,8 @@ extern std::atomic_uint64_t rss_mem_peak;\n \n extern size_t max_memory_limit;\n \n+size_t FetchRssMemory(io::StatusData sdata);\n+\n extern Namespaces* namespaces;\n \n // version 5.11 maps to 511 etc.\ndiff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex ad7aed04eb5a..f9d65f447284 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -1214,6 +1214,7 @@ auto DbSlice::DeleteExpiredStep(const Context& cntx, unsigned count) -> DeleteEx\n     if (ttl <= 0) {\n       auto prime_it = db.prime.Find(it->first);\n       CHECK(!prime_it.is_done());\n+      result.deleted_bytes += prime_it->first.MallocUsed() + prime_it->second.MallocUsed();\n       ExpireIfNeeded(cntx, prime_it, false);\n       ++result.deleted;\n     } else {\n@@ -1290,9 +1291,6 @@ pair<uint64_t, size_t> DbSlice::FreeMemWithEvictionStep(DbIndex db_ind, size_t s\n \n     auto time_finish = absl::GetCurrentTimeNanos();\n     events_.evicted_keys += evicted_items;\n-    DVLOG(2) << \"Evicted: \" << evicted_bytes;\n-    DVLOG(2) << \"Number of keys evicted / max eviction per hb: \" << evicted_items << \"/\"\n-             << max_eviction_per_hb;\n     DVLOG(2) << \"Eviction time (us): \" << (time_finish - time_start) / 1000;\n     return pair<uint64_t, size_t>{evicted_items, evicted_bytes};\n   };\n@@ -1326,7 +1324,7 @@ pair<uint64_t, size_t> DbSlice::FreeMemWithEvictionStep(DbIndex db_ind, size_t s\n           if (record_keys)\n             keys_to_journal.emplace_back(key);\n \n-          evicted_bytes += evict_it->second.MallocUsed();\n+          evicted_bytes += evict_it->first.MallocUsed() + evict_it->second.MallocUsed();\n           ++evicted_items;\n           PerformDeletion(Iterator(evict_it, StringOrView::FromView(key)), db_table.get());\n \ndiff --git a/src/server/db_slice.h b/src/server/db_slice.h\nindex a5e97f04474b..47d179722639 100644\n--- a/src/server/db_slice.h\n+++ b/src/server/db_slice.h\n@@ -442,6 +442,7 @@ class DbSlice {\n \n   struct DeleteExpiredStats {\n     uint32_t deleted = 0;         // number of deleted items due to expiry (less than traversed).\n+    uint32_t deleted_bytes = 0;   // total bytes of deleted items.\n     uint32_t traversed = 0;       // number of traversed items that have ttl bit\n     size_t survivor_ttl_sum = 0;  // total sum of ttl of survivors (traversed - deleted).\n   };\ndiff --git a/src/server/engine_shard.cc b/src/server/engine_shard.cc\nindex 467cd32ac189..70d95ca58fe5 100644\n--- a/src/server/engine_shard.cc\n+++ b/src/server/engine_shard.cc\n@@ -60,6 +60,12 @@ ABSL_FLAG(float, tiered_offload_threshold, 0.5,\n ABSL_FLAG(bool, enable_heartbeat_eviction, true,\n           \"Enable eviction during heartbeat when memory is under pressure.\");\n \n+ABSL_FLAG(double, eviction_memory_budget_threshold, 0.1,\n+          \"Eviction starts when the free memory (including RSS memory) drops below \"\n+          \"eviction_memory_budget_threshold * max_memory_limit.\");\n+\n+ABSL_DECLARE_FLAG(uint32_t, max_eviction_per_heartbeat);\n+\n namespace dfly {\n \n using absl::GetFlag;\n@@ -198,6 +204,52 @@ optional<uint32_t> GetPeriodicCycleMs() {\n   return clock_cycle_ms;\n }\n \n+size_t CalculateHowManyBytesToEvictOnShard(size_t global_memory_limit, size_t global_used_memory,\n+                                           size_t shard_memory_threshold) {\n+  if (global_used_memory > global_memory_limit) {\n+    // Used memory is above the limit, we need to evict all bytes\n+    return (global_used_memory - global_memory_limit) / shard_set->size() + shard_memory_threshold;\n+  }\n+\n+  const size_t shard_budget = (global_memory_limit - global_used_memory) / shard_set->size();\n+  return shard_budget < shard_memory_threshold ? (shard_memory_threshold - shard_budget) : 0;\n+}\n+\n+/* Calculates the number of bytes to evict based on memory and rss memory usage. */\n+size_t CalculateEvictionBytes() {\n+  const size_t shards_count = shard_set->size();\n+  const double eviction_memory_budget_threshold = GetFlag(FLAGS_eviction_memory_budget_threshold);\n+\n+  const size_t shard_memory_budget_threshold =\n+      size_t(max_memory_limit * eviction_memory_budget_threshold) / shards_count;\n+\n+  const size_t global_used_memory = used_mem_current.load(memory_order_relaxed);\n+\n+  // Calculate how many bytes we need to evict on this shard\n+  size_t goal_bytes = CalculateHowManyBytesToEvictOnShard(max_memory_limit, global_used_memory,\n+                                                          shard_memory_budget_threshold);\n+\n+  const double rss_oom_deny_ratio = ServerState::tlocal()->rss_oom_deny_ratio;\n+\n+  /* If rss_oom_deny_ratio is set, we should evict depending on rss memory too */\n+  if (rss_oom_deny_ratio > 0.0) {\n+    const size_t max_rss_memory = size_t(rss_oom_deny_ratio * max_memory_limit);\n+    /* We start eviction when we have less than eviction_memory_budget_threshold * 100% of free rss\n+     * memory */\n+    const size_t shard_rss_memory_budget_threshold =\n+        size_t(max_rss_memory * eviction_memory_budget_threshold) / shards_count;\n+\n+    // Calculate how much rss memory is used by all shards\n+    const size_t global_used_rss_memory = rss_mem_current.load(memory_order_relaxed);\n+\n+    // Try to evict more bytes if we are close to the rss memory limit\n+    goal_bytes = std::max(\n+        goal_bytes, CalculateHowManyBytesToEvictOnShard(max_rss_memory, global_used_rss_memory,\n+                                                        shard_rss_memory_budget_threshold));\n+  }\n+  return goal_bytes;\n+}\n+\n }  // namespace\n \n __thread EngineShard* EngineShard::shard_ = nullptr;\n@@ -706,7 +758,6 @@ void EngineShard::RetireExpiredAndEvict() {\n     // TODO: iterate over all namespaces\n     DbSlice& db_slice = namespaces->GetDefaultNamespace().GetDbSlice(shard_id());\n     constexpr double kTtlDeleteLimit = 200;\n-    constexpr double kRedLimitFactor = 0.1;\n \n     uint32_t traversed = GetMovingSum6(TTL_TRAVERSE);\n     uint32_t deleted = GetMovingSum6(TTL_DELETE);\n@@ -720,11 +771,11 @@ void EngineShard::RetireExpiredAndEvict() {\n       ttl_delete_target = kTtlDeleteLimit * double(deleted) / (double(traversed) + 10);\n     }\n \n-    ssize_t eviction_redline = size_t(max_memory_limit * kRedLimitFactor) / shard_set->size();\n-\n     DbContext db_cntx;\n     db_cntx.time_now_ms = GetCurrentTimeMs();\n \n+    size_t eviction_goal = GetFlag(FLAGS_enable_heartbeat_eviction) ? CalculateEvictionBytes() : 0;\n+\n     for (unsigned i = 0; i < db_slice.db_array_size(); ++i) {\n       if (!db_slice.IsDbValid(i))\n         continue;\n@@ -734,15 +785,22 @@ void EngineShard::RetireExpiredAndEvict() {\n       if (expt->size() > pt->size() / 4) {\n         DbSlice::DeleteExpiredStats stats = db_slice.DeleteExpiredStep(db_cntx, ttl_delete_target);\n \n+        eviction_goal -= std::min(eviction_goal, size_t(stats.deleted_bytes));\n         counter_[TTL_TRAVERSE].IncBy(stats.traversed);\n         counter_[TTL_DELETE].IncBy(stats.deleted);\n       }\n \n-      // if our budget is below the limit\n-      if (db_slice.memory_budget() < eviction_redline && GetFlag(FLAGS_enable_heartbeat_eviction)) {\n+      if (eviction_goal) {\n         uint32_t starting_segment_id = rand() % pt->GetSegmentCount();\n-        db_slice.FreeMemWithEvictionStep(i, starting_segment_id,\n-                                         eviction_redline - db_slice.memory_budget());\n+        auto [evicted_items, evicted_bytes] =\n+            db_slice.FreeMemWithEvictionStep(i, starting_segment_id, eviction_goal);\n+\n+        DVLOG(2) << \"Heartbeat eviction: Expected to evict \" << eviction_goal\n+                 << \" bytes. Actually evicted \" << evicted_items << \" items, \" << evicted_bytes\n+                 << \" bytes. Max eviction per heartbeat: \"\n+                 << GetFlag(FLAGS_max_eviction_per_heartbeat);\n+\n+        eviction_goal -= std::min(eviction_goal, evicted_bytes);\n       }\n     }\n   }\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex d1cb338531a6..b7dd7a55c449 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -100,9 +100,10 @@ ABSL_FLAG(dfly::MemoryBytesFlag, maxmemory, dfly::MemoryBytesFlag{},\n           \"0 - means the program will automatically determine its maximum memory usage. \"\n           \"default: 0\");\n \n-ABSL_FLAG(double, oom_deny_ratio, 1.1,\n-          \"commands with flag denyoom will return OOM when the ratio between maxmemory and used \"\n-          \"memory is above this value\");\n+ABSL_RETIRED_FLAG(\n+    double, oom_deny_ratio, 1.1,\n+    \"commands with flag denyoom will return OOM when the ratio between maxmemory and used \"\n+    \"memory is above this value\");\n \n ABSL_FLAG(double, rss_oom_deny_ratio, 1.25,\n           \"When the ratio between maxmemory and RSS memory exceeds this value, commands marked as \"\n@@ -722,11 +723,6 @@ string FailedCommandToString(std::string_view command, facade::CmdArgList args,\n   return result;\n }\n \n-void SetOomDenyRatioOnAllThreads(double ratio) {\n-  auto cb = [ratio](unsigned, auto*) { ServerState::tlocal()->oom_deny_ratio = ratio; };\n-  shard_set->pool()->AwaitBrief(cb);\n-}\n-\n void SetRssOomDenyRatioOnAllThreads(double ratio) {\n   auto cb = [ratio](unsigned, auto*) { ServerState::tlocal()->rss_oom_deny_ratio = ratio; };\n   shard_set->pool()->AwaitBrief(cb);\n@@ -793,9 +789,6 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n   config_registry.RegisterMutable(\"max_eviction_per_heartbeat\");\n   config_registry.RegisterMutable(\"max_segment_to_consider\");\n \n-  config_registry.RegisterSetter<double>(\"oom_deny_ratio\",\n-                                         [](double val) { SetOomDenyRatioOnAllThreads(val); });\n-\n   config_registry.RegisterSetter<double>(\"rss_oom_deny_ratio\",\n                                          [](double val) { SetRssOomDenyRatioOnAllThreads(val); });\n \n@@ -873,7 +866,6 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n   });\n   Transaction::Init(shard_num);\n \n-  SetOomDenyRatioOnAllThreads(absl::GetFlag(FLAGS_oom_deny_ratio));\n   SetRssOomDenyRatioOnAllThreads(absl::GetFlag(FLAGS_rss_oom_deny_ratio));\n \n   // Requires that shard_set will be initialized before because server_family_.Init might\n@@ -1001,7 +993,7 @@ bool ShouldDenyOnOOM(const CommandId* cid) {\n     uint64_t start_ns = absl::GetCurrentTimeNanos();\n     auto memory_stats = etl.GetMemoryUsage(start_ns);\n \n-    if (memory_stats.used_mem > (max_memory_limit * etl.oom_deny_ratio) ||\n+    if (memory_stats.used_mem > max_memory_limit ||\n         (etl.rss_oom_deny_ratio > 0 &&\n          memory_stats.rss_mem > (max_memory_limit * etl.rss_oom_deny_ratio))) {\n       DLOG(WARNING) << \"Out of memory, used \" << memory_stats.used_mem << \" ,rss \"\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex b3ff9c1676ec..7e9638c10bd1 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -137,7 +137,6 @@ ABSL_DECLARE_FLAG(bool, tls);\n ABSL_DECLARE_FLAG(string, tls_ca_cert_file);\n ABSL_DECLARE_FLAG(string, tls_ca_cert_dir);\n ABSL_DECLARE_FLAG(int, replica_priority);\n-ABSL_DECLARE_FLAG(double, oom_deny_ratio);\n ABSL_DECLARE_FLAG(double, rss_oom_deny_ratio);\n \n bool AbslParseFlag(std::string_view in, ReplicaOfFlag* flag, std::string* err) {\n@@ -1014,7 +1013,7 @@ void ServerFamily::UpdateMemoryGlobalStats() {\n \n   io::Result<io::StatusData> sdata_res = io::ReadStatusInfo();\n   if (sdata_res) {\n-    size_t total_rss = sdata_res->vm_rss + sdata_res->hugetlb_pages;\n+    size_t total_rss = FetchRssMemory(sdata_res.value());\n     rss_mem_current.store(total_rss, memory_order_relaxed);\n     if (rss_mem_peak.load(memory_order_relaxed) < total_rss) {\n       rss_mem_peak.store(total_rss, memory_order_relaxed);\n@@ -1339,7 +1338,7 @@ void PrintPrometheusMetrics(uint64_t uptime, const Metrics& m, DflyCmd* dfly_cmd\n                       &resp->body());\n   }\n   if (sdata_res.has_value()) {\n-    size_t rss = sdata_res->vm_rss + sdata_res->hugetlb_pages;\n+    size_t rss = FetchRssMemory(sdata_res.value());\n     AppendMetricWithoutLabels(\"used_memory_rss_bytes\", \"\", rss, MetricType::GAUGE, &resp->body());\n     AppendMetricWithoutLabels(\"swap_memory_bytes\", \"\", sdata_res->vm_swap, MetricType::GAUGE,\n                               &resp->body());\ndiff --git a/src/server/server_state.h b/src/server/server_state.h\nindex 0cfc48be1634..9aed8901c5cc 100644\n--- a/src/server/server_state.h\n+++ b/src/server/server_state.h\n@@ -180,6 +180,7 @@ class ServerState {  // public struct - to allow initialization.\n     uint64_t used_mem = 0;\n     uint64_t rss_mem = 0;\n   };\n+\n   MemoryUsageStats GetMemoryUsage(uint64_t now_ns);\n \n   bool AllowInlineScheduling() const;\n@@ -296,7 +297,6 @@ class ServerState {  // public struct - to allow initialization.\n \n   // Exec descriptor frequency count for this thread.\n   absl::flat_hash_map<std::string, unsigned> exec_freq_count;\n-  double oom_deny_ratio;\n   double rss_oom_deny_ratio;\n \n  private:\n",
  "test_patch": "diff --git a/src/server/dragonfly_test.cc b/src/server/dragonfly_test.cc\nindex 276bcd87b992..83f8c3bdb375 100644\n--- a/src/server/dragonfly_test.cc\n+++ b/src/server/dragonfly_test.cc\n@@ -26,8 +26,8 @@ ABSL_DECLARE_FLAG(float, mem_defrag_threshold);\n ABSL_DECLARE_FLAG(float, mem_defrag_waste_threshold);\n ABSL_DECLARE_FLAG(uint32_t, mem_defrag_check_sec_interval);\n ABSL_DECLARE_FLAG(std::vector<std::string>, rename_command);\n-ABSL_DECLARE_FLAG(double, oom_deny_ratio);\n ABSL_DECLARE_FLAG(bool, lua_resp2_legacy_float);\n+ABSL_DECLARE_FLAG(double, eviction_memory_budget_threshold);\n \n namespace dfly {\n \n@@ -456,19 +456,22 @@ TEST_F(DflyEngineTest, OOM) {\n /// Reproduces the case where items with expiry data were evicted,\n /// and then written with the same key.\n TEST_F(DflyEngineTest, Bug207) {\n-  max_memory_limit = 300000;\n+  max_memory_limit = 300000 * 4;\n \n+  // The threshold is set to 0.3 to trigger eviction earlier and prevent OOM.\n   absl::FlagSaver fs;\n-  absl::SetFlag(&FLAGS_oom_deny_ratio, 4);\n-  ResetService();\n+  absl::SetFlag(&FLAGS_eviction_memory_budget_threshold, 0.3);\n \n   shard_set->TEST_EnableCacheMode();\n \n+  /* The value should be large enough to avoid being inlined. Heartbeat evicts only objects for\n+   * which HasAllocated() returns true. */\n+  std::string value(1000, '.');\n+\n   ssize_t i = 0;\n   RespExpr resp;\n-  for (; i < 10000; ++i) {\n-    resp = Run({\"setex\", StrCat(\"key\", i), \"30\", \"bar\"});\n-    // we evict some items because 5000 is too much when max_memory_limit is 300000.\n+  for (; i < 1000; ++i) {\n+    resp = Run({\"setex\", StrCat(\"key\", i), \"30\", value});\n     ASSERT_EQ(resp, \"OK\");\n   }\n \n@@ -489,10 +492,7 @@ TEST_F(DflyEngineTest, Bug207) {\n }\n \n TEST_F(DflyEngineTest, StickyEviction) {\n-  max_memory_limit = 300000;\n-  absl::FlagSaver fs;\n-  absl::SetFlag(&FLAGS_oom_deny_ratio, 4);\n-  ResetService();\n+  max_memory_limit = 600000;  // 0.6mb\n   shard_set->TEST_EnableCacheMode();\n \n   string tmp_val(100, '.');\ndiff --git a/tests/dragonfly/generic_test.py b/tests/dragonfly/generic_test.py\nindex c4ceef3d6398..25da08a42fc2 100644\n--- a/tests/dragonfly/generic_test.py\n+++ b/tests/dragonfly/generic_test.py\n@@ -148,14 +148,12 @@ async def test_reply_guard_oom(df_factory, df_seeder_factory):\n \n @pytest.mark.asyncio\n async def test_denyoom_commands(df_factory):\n-    df_server = df_factory.create(\n-        proactor_threads=1, maxmemory=\"256mb\", oom_deny_commands=\"get\", oom_deny_ratio=0.7\n-    )\n+    df_server = df_factory.create(proactor_threads=1, maxmemory=\"256mb\", oom_deny_commands=\"get\")\n     df_server.start()\n     client = df_server.client()\n     await client.execute_command(\"DEBUG POPULATE 7000 size 44000\")\n \n-    min_deny = 250 * 1024 * 1024  # 250mb\n+    min_deny = 256 * 1024 * 1024  # 256mb\n     info = await client.info(\"memory\")\n     print(f'Used memory {info[\"used_memory\"]}, rss {info[\"used_memory_rss\"]}')\n     assert info[\"used_memory\"] > min_deny, \"Weak testcase: too little used memory\"\ndiff --git a/tests/dragonfly/memory_test.py b/tests/dragonfly/memory_test.py\nindex 1105f1d83408..438f5296ffb7 100644\n--- a/tests/dragonfly/memory_test.py\n+++ b/tests/dragonfly/memory_test.py\n@@ -6,6 +6,31 @@\n from .instance import DflyInstance, DflyInstanceFactory\n \n \n+async def calculate_estimated_connection_memory(\n+    async_client: aioredis.Redis, df_server: DflyInstance\n+):\n+    memory_info = await async_client.info(\"memory\")\n+    already_used_rss_memory = memory_info[\"used_memory_rss\"]\n+\n+    connections_number = 100\n+    connections = []\n+    for _ in range(connections_number):\n+        conn = aioredis.Redis(port=df_server.port)\n+        await conn.ping()\n+        connections.append(conn)\n+\n+    await asyncio.sleep(1)  # Wait RSS update\n+\n+    memory_info = await async_client.info(\"memory\")\n+    estimated_connections_memory = memory_info[\"used_memory_rss\"] - already_used_rss_memory\n+\n+    # Close test connection\n+    for conn in connections:\n+        await conn.close()\n+\n+    return estimated_connections_memory // connections_number\n+\n+\n @pytest.mark.opt_only\n @pytest.mark.parametrize(\n     \"type, keys, val_size, elements\",\n@@ -160,3 +185,108 @@ async def test_eval_with_oom(df_factory: DflyInstanceFactory):\n     info = await client.info(\"memory\")\n     logging.debug(f'Used memory {info[\"used_memory\"]}, rss {info[\"used_memory_rss\"]}')\n     assert rss_before_eval * 1.01 > info[\"used_memory_rss\"]\n+\n+\n+@pytest.mark.asyncio\n+@dfly_args(\n+    {\n+        \"proactor_threads\": 1,\n+        \"cache_mode\": \"true\",\n+        \"maxmemory\": \"256mb\",\n+        \"rss_oom_deny_ratio\": 0.5,\n+        \"max_eviction_per_heartbeat\": 1000,\n+    }\n+)\n+async def test_cache_eviction_with_rss_deny_oom(\n+    async_client: aioredis.Redis,\n+    df_server: DflyInstance,\n+):\n+    \"\"\"\n+    Test to verify that cache eviction is triggered even if used memory is small but rss memory is above limit\n+    \"\"\"\n+\n+    max_memory = 256 * 1024 * 1024  # 256 MB\n+    rss_max_memory = int(max_memory * 0.5)  # 50% of max memory\n+\n+    data_fill_size = int(0.55 * rss_max_memory)  # 55% of rss_max_memory\n+    rss_increase_size = int(0.55 * rss_max_memory)  # 55% of max rss_max_memory\n+\n+    key_size = 1024 * 5  # 5 kb\n+    num_keys = data_fill_size // key_size\n+\n+    await asyncio.sleep(1)  # Wait for RSS update\n+\n+    estimated_connection_memory = await calculate_estimated_connection_memory(\n+        async_client, df_server\n+    )\n+    num_connections = rss_increase_size // estimated_connection_memory\n+\n+    logging.info(\n+        f\"Estimated connection memory: {estimated_connection_memory}. Number of connections: {num_connections}.\"\n+    )\n+\n+    # Fill data to 55% of rss max memory\n+    await async_client.execute_command(\"DEBUG\", \"POPULATE\", num_keys, \"key\", key_size)\n+\n+    await asyncio.sleep(1)  # Wait for RSS heartbeat update\n+\n+    # First test that eviction is not triggered without connection creation\n+    stats_info = await async_client.info(\"stats\")\n+    assert stats_info[\"evicted_keys\"] == 0, \"No eviction should start yet.\"\n+\n+    # Test that used memory is less than 90% of max memory\n+    memory_info = await async_client.info(\"memory\")\n+    assert (\n+        memory_info[\"used_memory\"] < max_memory * 0.9\n+    ), \"Used memory should be less than 90% of max memory.\"\n+    assert (\n+        memory_info[\"used_memory_rss\"] < rss_max_memory * 0.9\n+    ), \"RSS memory should be less than 90% of rss max memory (max_memory * rss_oom_deny_ratio).\"\n+\n+    # Disable heartbeat eviction\n+    await async_client.execute_command(\"CONFIG SET enable_heartbeat_eviction false\")\n+\n+    # Increase RSS memory by 55% of rss max memory\n+    # We can simulate RSS increase by creating new connections\n+    connections = []\n+    for _ in range(num_connections):\n+        conn = aioredis.Redis(port=df_server.port)\n+        await conn.ping()\n+        connections.append(conn)\n+\n+    await asyncio.sleep(1)\n+\n+    # Check that RSS memory is above rss limit\n+    memory_info = await async_client.info(\"memory\")\n+    assert (\n+        memory_info[\"used_memory_rss\"] >= rss_max_memory * 0.9\n+    ), \"RSS memory should exceed 90% of the maximum RSS memory limit (max_memory * rss_oom_deny_ratio).\"\n+\n+    # Enable heartbeat eviction\n+    await async_client.execute_command(\"CONFIG SET enable_heartbeat_eviction true\")\n+\n+    await asyncio.sleep(1)  # Wait for RSS heartbeat update\n+    await async_client.execute_command(\"MEMORY DECOMMIT\")\n+    await asyncio.sleep(1)  # Wait for RSS update\n+\n+    # Get RSS memory after creating new connections\n+    memory_info = await async_client.info(\"memory\")\n+    stats_info = await async_client.info(\"stats\")\n+\n+    logging.info(f'Evicted keys number: {stats_info[\"evicted_keys\"]}. Total keys: {num_keys}.')\n+\n+    assert (\n+        memory_info[\"used_memory\"] < data_fill_size\n+    ), \"Used memory should be less than initial fill size due to eviction.\"\n+\n+    assert (\n+        memory_info[\"used_memory_rss\"] < rss_max_memory * 0.9\n+    ), \"RSS memory should be less than 90% of rss max memory (max_memory * rss_oom_deny_ratio) after eviction.\"\n+\n+    # Check that eviction has occurred\n+    assert (\n+        stats_info[\"evicted_keys\"] > 0\n+    ), \"Eviction should have occurred due to rss memory pressure.\"\n+\n+    for conn in connections:\n+        await conn.close()\n",
  "problem_statement": "remove oom_deny_ratio flag\n1. flag should be deprecated use ABSL_RETIRED_FLAG\r\n2. server logic should still reject denyoom commands when server reaches max_memory_limit (as if we run with oom_deny_ratio=1.0)\r\n3. today in some test we use the oom_deny_ratio flag test eviction. we need to fix this test by tuning when eviction starts, today this is hardcoded at 95% usage, we can tune eviction with a new flag\n",
  "hints_text": "",
  "created_at": "2024-11-18T07:40:04Z",
  "modified_files": [
    "src/server/common.cc",
    "src/server/common.h",
    "src/server/db_slice.cc",
    "src/server/db_slice.h",
    "src/server/engine_shard.cc",
    "src/server/main_service.cc",
    "src/server/server_family.cc",
    "src/server/server_state.h"
  ],
  "modified_test_files": [
    "src/server/dragonfly_test.cc",
    "tests/dragonfly/generic_test.py",
    "tests/dragonfly/memory_test.py"
  ]
}