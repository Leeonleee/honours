{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4508,
  "instance_id": "dragonflydb__dragonfly-4508",
  "issue_numbers": [
    "4455"
  ],
  "base_commit": "bafb427a09a4bdf21b3bad6bfee04001074a20fb",
  "patch": "diff --git a/src/facade/reply_capture.cc b/src/facade/reply_capture.cc\nindex 02d00356e885..3638793125ce 100644\n--- a/src/facade/reply_capture.cc\n+++ b/src/facade/reply_capture.cc\n@@ -65,6 +65,7 @@ CapturingReplyBuilder::Payload CapturingReplyBuilder::Take() {\n   CHECK(stack_.empty());\n   Payload pl = std::move(current_);\n   current_ = monostate{};\n+  ConsumeLastError();\n   return pl;\n }\n \ndiff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex f67cf5cda3d3..249daee9b816 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -716,6 +716,7 @@ void DbSlice::FlushSlotsFb(const cluster::SlotSet& slot_ids) {\n     PrimeTable* table = GetTables(db_index).first;\n \n     auto iterate_bucket = [&](DbIndex db_index, PrimeTable::bucket_iterator it) {\n+      it.AdvanceIfNotOccupied();\n       while (!it.is_done()) {\n         del_entry_cb(it);\n         ++it;\n@@ -723,7 +724,7 @@ void DbSlice::FlushSlotsFb(const cluster::SlotSet& slot_ids) {\n     };\n \n     if (const PrimeTable::bucket_iterator* bit = req.update()) {\n-      if (bit->GetVersion() < next_version) {\n+      if (!bit->is_done() && bit->GetVersion() < next_version) {\n         iterate_bucket(db_index, *bit);\n       }\n     } else {\ndiff --git a/src/server/journal/streamer.cc b/src/server/journal/streamer.cc\nindex 9dd2d9472d6c..6cfacaa8051b 100644\n--- a/src/server/journal/streamer.cc\n+++ b/src/server/journal/streamer.cc\n@@ -297,7 +297,7 @@ bool RestoreStreamer::ShouldWrite(SlotId slot_id) const {\n bool RestoreStreamer::WriteBucket(PrimeTable::bucket_iterator it) {\n   bool written = false;\n \n-  if (it.GetVersion() < snapshot_version_) {\n+  if (!it.is_done() && it.GetVersion() < snapshot_version_) {\n     stats_.buckets_written++;\n \n     it.SetVersion(snapshot_version_);\ndiff --git a/src/server/snapshot.cc b/src/server/snapshot.cc\nindex c6c64261abb9..a7d8812c5c1f 100644\n--- a/src/server/snapshot.cc\n+++ b/src/server/snapshot.cc\n@@ -390,7 +390,7 @@ void SliceSnapshot::OnDbChange(DbIndex db_index, const DbSlice::ChangeReq& req)\n   const PrimeTable::bucket_iterator* bit = req.update();\n \n   if (bit) {\n-    if (bit->GetVersion() < snapshot_version_) {\n+    if (!bit->is_done() && bit->GetVersion() < snapshot_version_) {\n       stats_.side_saved += SerializeBucket(db_index, *bit);\n     }\n   } else {\n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex d2be391c6c0a..a207d3c2baa6 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -2763,3 +2763,101 @@ async def test_migration_one_after_another(df_factory: DflyInstanceFactory, df_s\n     dbsize_node2 = await nodes[2].client.dbsize()\n     assert dbsize_node1 + dbsize_node2 == dbsize_node0\n     assert dbsize_node2 > 0 and dbsize_node1 > 0\n+\n+\n+\"\"\"\n+Test cluster node distributing its slots into 3 other nodes.\n+In this test we randomize the slot ranges that are migrated to each node\n+For each migration we start migration, wait for it to finish and once it is finished we send migration finalization config\n+\"\"\"\n+\n+\n+@pytest.mark.slow\n+@pytest.mark.exclude_epoll\n+@pytest.mark.asyncio\n+@dfly_args({\"proactor_threads\": 4, \"cluster_mode\": \"yes\"})\n+async def test_migration_rebalance_node(df_factory: DflyInstanceFactory, df_seeder_factory):\n+    # 1. Create cluster of 3 nodes with all slots allocated to first node.\n+    instances = [\n+        df_factory.create(\n+            port=next(next_port),\n+            admin_port=next(next_port),\n+            vmodule=\"outgoing_slot_migration=2,cluster_family=2,incoming_slot_migration=2,streamer=2\",\n+        )\n+        for i in range(4)\n+    ]\n+    df_factory.start_all(instances)\n+\n+    def create_random_ranges():\n+        # Generate 2 random breakpoints within the range\n+        breakpoints = sorted(random.sample(range(1, 16382), 2))\n+        ranges = [\n+            (0, breakpoints[0] - 1),\n+            (breakpoints[0], breakpoints[1] - 1),\n+            (breakpoints[1], 16383),\n+        ]\n+        return ranges\n+\n+    # Create 3 random ranges from 0 to 16383\n+    random_ranges = create_random_ranges()\n+\n+    nodes = [(await create_node_info(instance)) for instance in instances]\n+    nodes[0].slots = random_ranges\n+    nodes[1].slots = []\n+    nodes[2].slots = []\n+    nodes[3].slots = []\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    key_num = 100000\n+    logging.debug(f\"DEBUG POPULATE first node with number of keys: {key_num}\")\n+    await StaticSeeder(key_target=key_num, data_size=100).run(nodes[0].client)\n+    dbsize_node0 = await nodes[0].client.dbsize()\n+    assert dbsize_node0 > (key_num * 0.95)\n+\n+    logging.debug(\"start seeding\")\n+    # Running seeder with pipeline mode when finalizing migrations leads to errors\n+    # TODO: I believe that changing the seeder to generate pipeline command only on specific slot will fix the problem\n+    seeder = df_seeder_factory.create(\n+        keys=50_000, port=instances[0].port, cluster_mode=True, pipeline=False\n+    )\n+    await seeder.run(target_deviation=0.1)\n+    seed = asyncio.create_task(seeder.run())\n+\n+    migration_info = [\n+        MigrationInfo(\"127.0.0.1\", nodes[1].instance.admin_port, [random_ranges[0]], nodes[1].id),\n+        MigrationInfo(\"127.0.0.1\", nodes[2].instance.admin_port, [random_ranges[1]], nodes[2].id),\n+        MigrationInfo(\"127.0.0.1\", nodes[3].instance.admin_port, [random_ranges[2]], nodes[3].id),\n+    ]\n+\n+    nodes_lock = asyncio.Lock()\n+\n+    async def do_migration(index):\n+        await asyncio.sleep(random.randint(1, 10) / 5)\n+        async with nodes_lock:\n+            logging.debug(f\"Start migration from node {index}\")\n+            nodes[0].migrations.append(migration_info[index - 1])\n+            await push_config(\n+                json.dumps(generate_config(nodes)), [node.admin_client for node in nodes]\n+            )\n+\n+        logging.debug(f\"wait migration from node {index}\")\n+        await wait_for_status(nodes[0].admin_client, nodes[index].id, \"FINISHED\", timeout=50)\n+        await wait_for_status(nodes[index].admin_client, nodes[0].id, \"FINISHED\", timeout=50)\n+        logging.debug(f\"finished migration from node {index}\")\n+        await asyncio.sleep(random.randint(1, 5) / 5)\n+        async with nodes_lock:\n+            logging.debug(f\"Finalize migration from node {index}\")\n+            nodes[index].slots = migration_info[index - 1].slots\n+            nodes[0].slots.remove(migration_info[index - 1].slots[0])\n+            nodes[0].migrations.remove(migration_info[index - 1])\n+            await push_config(\n+                json.dumps(generate_config(nodes)), [node.admin_client for node in nodes]\n+            )\n+\n+    all_migrations = [asyncio.create_task(do_migration(i)) for i in range(1, 4)]\n+    for migration in all_migrations:\n+        await migration\n+\n+    logging.debug(\"stop seeding\")\n+    seeder.stop()\n+    await seed\ndiff --git a/tests/dragonfly/utility.py b/tests/dragonfly/utility.py\nindex 8855d7bccbf5..fc661130d829 100644\n--- a/tests/dragonfly/utility.py\n+++ b/tests/dragonfly/utility.py\n@@ -425,6 +425,7 @@ def __init__(\n         stop_on_failure=True,\n         cluster_mode=False,\n         mirror_to_fake_redis=False,\n+        pipeline=True,\n     ):\n         if cluster_mode:\n             max_multikey = 1\n@@ -439,6 +440,7 @@ def __init__(\n         self.stop_flag = False\n         self.stop_on_failure = stop_on_failure\n         self.fake_redis = None\n+        self.use_pipeline = pipeline\n \n         self.log_file = log_file\n         if self.log_file is not None:\n@@ -447,6 +449,7 @@ def __init__(\n         if mirror_to_fake_redis:\n             logging.debug(\"Creating FakeRedis instance\")\n             self.fake_redis = fakeredis.FakeAsyncRedis()\n+            self.use_pipeline = False\n \n     async def run(self, target_ops=None, target_deviation=None):\n         \"\"\"\n@@ -604,18 +607,19 @@ async def _executor_task(self, db, queue):\n                 break\n \n             try:\n-                if self.fake_redis is None:\n+                if self.use_pipeline:\n                     pipe = client.pipeline(transaction=tx_data[1])\n                     for cmd in tx_data[0]:\n                         pipe.execute_command(*cmd)\n                     await pipe.execute()\n                 else:\n-                    # To mirror consistently to Fake Redis we must only send to it successful\n-                    # commands. We can't use pipes because they might succeed partially.\n                     for cmd in tx_data[0]:\n                         dfly_resp = await client.execute_command(*cmd)\n-                        fake_resp = await self.fake_redis.execute_command(*cmd)\n-                        assert dfly_resp == fake_resp\n+                        # To mirror consistently to Fake Redis we must only send to it successful\n+                        # commands. We can't use pipes because they might succeed partially.\n+                        if self.fake_redis is not None:\n+                            fake_resp = await self.fake_redis.execute_command(*cmd)\n+                            assert dfly_resp == fake_resp\n             except (redis.exceptions.ConnectionError, redis.exceptions.ResponseError) as e:\n                 if self.stop_on_failure:\n                     await self._close_client(client)\n",
  "problem_statement": "Dragonfly crashes during migrations\nDragonfly crashes during migrations, with stack trace:\n```\n#0  __pthread_kill_implementation (threadid=281474836474368, signo=signo@entry=11, no_tid=no_tid@entry=0) at ./nptl/pthread_kill.c:44\n#1  0x0000fffff7d97690 in __pthread_kill_internal (signo=11, threadid=<optimized out>) at ./nptl/pthread_kill.c:78\n#2  0x0000fffff7d4cb3c in __GI_raise (sig=11) at ../sysdeps/posix/raise.c:26\n#3  <signal handler called>\n#4  0x0000aaaaaaf2e4fc in dfly::DashTable<dfly::CompactObj, dfly::CompactObj, dfly::detail::PrimeTablePolicy>::Iterator<false, true>::GetVersion<true> (this=<optimized out>)\n    at /var/lib/dragonfly/dragonfly/dragonfly/src/core/dash.h:430\n#5  dfly::RestoreStreamer::WriteBucket (this=this@entry=0x3bd5e0c1598, it=...) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/journal/streamer.cc:300\n#6  0x0000aaaaaaf2f178 in dfly::RestoreStreamer::OnDbChange (this=0x3bd5e0c1598, db_index=<optimized out>, req=...) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/journal/streamer.cc:337\n#7  0x0000aaaaaaee31cc in std::function<void (unsigned short, dfly::DbSlice::ChangeReq const&)>::operator()(unsigned short, dfly::DbSlice::ChangeReq const&) const (__args#1=..., __args#0=<optimized out>, \n    this=0x3bd5e0e5558) at /usr/include/c++/13/bits/std_function.h:591\n#8  dfly::DbSlice::FlushChangeToEarlierCallbacks (this=this@entry=0x3bd5e0f0440, db_ind=db_ind@entry=0, it=..., upper_bound=3889649) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/db_slice.cc:1196\n#9  0x0000aaaaaaf2eaf8 in operator() (it=..., __closure=<optimized out>) at /usr/include/c++/13/bits/allocator.h:184\n#10 dfly::DashTable<dfly::CompactObj, dfly::CompactObj, dfly::detail::PrimeTablePolicy>::TraverseBuckets<dfly::RestoreStreamer::Run()::<lambda(dfly::DashTable<dfly::CompactObj, dfly::CompactObj, dfly::detail::PrimeTablePolicy>::bucket_iterator)> > (cb=..., cursor=..., this=<optimized out>) at /var/lib/dragonfly/dragonfly/dragonfly/src/core/dash.h:1003\n#11 dfly::RestoreStreamer::Run (this=0x3bd5e0c1e98) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/journal/streamer.cc:211\n#12 0x0000aaaaaae00224 in std::function<void (std::unique_ptr<dfly::cluster::OutgoingMigration::SliceSlotMigration, std::default_delete<dfly::cluster::OutgoingMigration::SliceSlotMigration> >&)>::operator()(std::unique_ptr<dfly::cluster::OutgoingMigration::SliceSlotMigration, std::default_delete<dfly::cluster::OutgoingMigration::SliceSlotMigration> >&) const (__args#0=..., this=<optimized out>)\n    at /usr/include/c++/13/bits/std_function.h:591\n#13 operator() (__closure=<synthetic pointer>, pb=<optimized out>) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/cluster/outgoing_slot_migration.cc:137\n#14 operator() (context=<optimized out>, __closure=<synthetic pointer>) at /var/lib/dragonfly/dragonfly/dragonfly/helio/util/proactor_pool.h:194\n#15 std::__invoke_impl<void, util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, util::fb2::ProactorBase*> (\n    __f=<synthetic pointer>) at /usr/include/c++/13/bits/invoke.h:61\n#16 std::__invoke<util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, util::fb2::ProactorBase*> (\n    __fn=<synthetic pointer>) at /usr/include/c++/13/bits/invoke.h:96\n#17 std::__apply_impl<util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, std::tuple<util::fb2::ProactorBase*>, 0> (__t=<synthetic pointer>, __f=<synthetic pointer>) at /usr/include/c++/13/tuple:2302\n#18 std::apply<util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, std::tuple<util::fb2::ProactorBase*> > (\n    __t=<synthetic pointer>, __f=<synthetic pointer>) at /usr/include/c++/13/tuple:2313\n#19 util::fb2::detail::WorkerFiberImpl<const util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, util::fb2::ProactorBase*&>::run_ (c=..., this=0xffffc4040e00) at /var/lib/dragonfly/dragonfly/dragonfly/helio/util/fibers/detail/fiber_interface.h:304\n\n```\n\nRunning Dragonfly v1.26.1\n\nThis is running 100x100GB shards, each populated to 75% memory. I'll add more information below...\n",
  "hints_text": "an 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaabe8d0b8         16  google::LogMessage::Fail()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaabe8cfc0        144  google::LogMessage::SendToLog()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaabe8c7c0         80  google::LogMessage::Flush()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaabe905a0         32  google::LogMessageFatal::~LogMessageFatal()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaaac69344        176  __assert_fail\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaaac93b34         16  dfly::detail::Segment<>::Key()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaaade71f0         80  dfly::DashTable<>::Iterator<>::operator->()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab50ff6c        176  dfly::RestoreStreamer::WriteBucket()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab50f11c        176  dfly::RestoreStreamer::Run()::{lambda()#1}::operator()()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab511278        112  dfly::DashTable<>::TraverseBuckets<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab50f384        240  dfly::RestoreStreamer::Run()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e8650         48  dfly::cluster::OutgoingMigration::SliceSlotMigration::R>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1de248         32  dfly::cluster::OutgoingMigration::SyncFb()::{lambda()#4>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e44c8         32  std::__invoke_impl<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e3494         64  std::__invoke_r<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e2390         48  std::_Function_handler<>::_M_invoke()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e8cc8         48  std::function<>::operator()()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1dd8ac         48  dfly::cluster::OutgoingMigration::OnAllShards()::{lambd>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e1208         64  util::ProactorPool::AwaitFiberOnAll<>()::{lambda()#1}::>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e6540         32  std::__invoke_impl<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e61d0         64  std::__invoke<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e5cac         48  std::__apply_impl<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e5cf4         64  std::apply<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e5d84        112  util::fb2::detail::WorkerFiberImpl<>::run_()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e5724         64  util::fb2::detail::WorkerFiberImpl<>::WorkerFiberImpl<>>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e7810         80  std::__invoke_impl<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @ ... and at least 4 more frames\nJan 20 15:01:06 ip-10-5-15-59 systemd[1]: dragonfly.service: Main process exited, code=dumped, status=6/ABRT\nJan 20 15:01:06 ip-10-5-15-59 systemd[1]: dragonfly.service: Failed with result 'core-dump'.\nAssumption for a flow which can lead to this crash\nA Cluster node gets new cluster config after migration was done leads to flushslots callback register, this callback deletes entries from table.\nAnother migration start from this cluster node starting RestoreStreamer::Run \nWhen calling the callback passed to TraverseBuckets we first call FlushChangeToEarlierCallbacks which will delete entries on this bucket and than will call the WriteBucket which will fail on the assert as the iterator to the entry in dash table points to an entry which is not longer occupied (GetBusy on the slot is false).\n\nHow to fix - fix the for loop in RestoreStreamer::WriteBucket to access only valid entries.\nTo test - create a test with 2 outgoing migrations from a cluster node. start the second migration after the first migration is finished\nI was able to reproduce this crash with my test\nReopenning this issue as we still see crashes, now from another flow\nJan 22 16:07:54 ip-10-5-32-145 dragonfly[1782]: *** SIGSEGV received at time=1737562074 on cpu 11 ***\nJan 22 16:07:54 ip-10-5-32-145 dragonfly[1782]: PC: @     0xaaaaaaf351fc  (unknown)  dfly::RestoreStreamer::WriteBucket()\nJan 22 16:07:54 ip-10-5-32-145 dragonfly[1782]:     @     0xaaaaab6f2158        224  absl::lts_20240722::AbslFailureSignalHandler()\nJan 22 16:07:54 ip-10-5-32-145 dragonfly[1782]:     @     0xfffff7ffb8f8       4912  (unknown)\nJan 22 16:07:54 ip-10-5-32-145 dragonfly[1782]:     @     0xaaaaaaf35f4c        144  dfly::RestoreStreamer::OnDbChange()\nJan 22 16:07:54 ip-10-5-32-145 dragonfly[1782]:     @     0xaaaaaaeecd2c        176  dfly::DbSlice::FlushChangeToEarlierCallbacks()\nJan 22 16:07:54 ip-10-5-32-145 dragonfly[1782]:     @     0xaaaaaaf35aec        528  dfly::RestoreStreamer::Run()\nJan 22 16:07:54 ip-10-5-32-145 dragonfly[1782]:     @     0xaaaaaae07d34        128  boost::context::detail::fiber_entry<>()",
  "created_at": "2025-01-26T19:23:45Z",
  "modified_files": [
    "src/facade/reply_capture.cc",
    "src/server/db_slice.cc",
    "src/server/journal/streamer.cc",
    "src/server/snapshot.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py",
    "tests/dragonfly/utility.py"
  ]
}