{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 5544,
  "instance_id": "dragonflydb__dragonfly-5544",
  "issue_numbers": [
    "5521"
  ],
  "base_commit": "eb0efff61d15954d170294f55b1959e96e4a07c5",
  "patch": "diff --git a/src/server/cluster/cluster_config.h b/src/server/cluster/cluster_config.h\nindex a225174897d6..2d8f0081ec46 100644\n--- a/src/server/cluster/cluster_config.h\n+++ b/src/server/cluster/cluster_config.h\n@@ -37,6 +37,12 @@ class ClusterConfig {\n \n   ClusterShardInfos GetConfig() const;\n \n+  // Use wisely, only after a deep copy of ClusterConfig and\n+  // to edit the config in place.\n+  ClusterShardInfos& GetMutableConfig() {\n+    return config_;\n+  }\n+\n   const SlotSet& GetOwnedSlots() const;\n \n   std::vector<MigrationInfo> GetNewOutgoingMigrations(\ndiff --git a/src/server/cluster/cluster_defs.h b/src/server/cluster/cluster_defs.h\nindex b130f20b0698..a00816969388 100644\n--- a/src/server/cluster/cluster_defs.h\n+++ b/src/server/cluster/cluster_defs.h\n@@ -147,6 +147,14 @@ class ClusterShardInfos {\n     return infos_.cend();\n   }\n \n+  auto begin() noexcept {\n+    return infos_.begin();\n+  }\n+\n+  auto end() noexcept {\n+    return infos_.end();\n+  }\n+\n   auto size() const noexcept {\n     return infos_.size();\n   }\ndiff --git a/src/server/cluster/cluster_family.cc b/src/server/cluster/cluster_family.cc\nindex 4c879359f601..404f1ffca938 100644\n--- a/src/server/cluster/cluster_family.cc\n+++ b/src/server/cluster/cluster_family.cc\n@@ -1088,6 +1088,70 @@ size_t ClusterFamily::MigrationsErrorsCount() const {\n   return error_num;\n }\n \n+void ClusterFamily::ReconcileMasterFlow() {\n+  auto config = cluster::ClusterConfig::Current();\n+\n+  for (auto& info : config->GetMutableConfig()) {\n+    if (info.master.id == id_) {\n+      if (!info.replicas.empty()) {\n+        LOG_IF(ERROR, info.replicas.size() > 1)\n+            << \"More than one replica found, slot redirection after takeover corrupted\";\n+\n+        // assumes there is one replica per master node\n+        // TODO figure a smart way to get the replica id_ so\n+        // we can find it here\n+        info.master = info.replicas.front();\n+        info.replicas.clear();\n+      }\n+      break;\n+    }\n+  }\n+}\n+\n+void ClusterFamily::ReconcileReplicaFlow() {\n+  auto new_config = ClusterConfig::Current()->CloneWithChanges({}, {});\n+  // Replace master with replica in shard config.\n+  bool found = false;\n+  for (ClusterShardInfo& info : new_config->GetMutableConfig()) {\n+    for (const auto& replica : info.replicas) {\n+      if (replica.id == id_) {\n+        info.master = replica;\n+        // New master has no replicas\n+        info.replicas.clear();\n+        found = true;\n+        break;\n+      }\n+    }\n+    if (found)\n+      break;\n+  }\n+\n+  LOG_IF(ERROR, !found) << \"Did not find replica in the cluster map\";\n+\n+  server_family_->service().proactor_pool().AwaitFiberOnAll(\n+      [&new_config](util::ProactorBase*) { ClusterConfig::SetCurrent(new_config); });\n+}\n+\n+void ClusterFamily::ReconcileMasterReplicaTakeoverSlots(bool was_master) {\n+  util::fb2::LockGuard gu(set_config_mu);\n+  util::fb2::LockGuard lk(migration_mu_);\n+\n+  auto config = ClusterConfig::Current();\n+\n+  // Sanity -- we should not reach there\n+  if (!config) {\n+    LOG(ERROR) << \"Cluster config after takeover is empty\";\n+    return;\n+  }\n+\n+  if (was_master) {\n+    ReconcileMasterFlow();\n+    return;\n+  }\n+\n+  ReconcileReplicaFlow();\n+}\n+\n using EngineFunc = void (ClusterFamily::*)(CmdArgList args, const CommandContext& cmd_cntx);\n \n inline CommandId::Handler3 HandlerFunc(ClusterFamily* se, EngineFunc f) {\ndiff --git a/src/server/cluster/cluster_family.h b/src/server/cluster/cluster_family.h\nindex f1bfe9083393..3ba2644f766d 100644\n--- a/src/server/cluster/cluster_family.h\n+++ b/src/server/cluster/cluster_family.h\n@@ -44,6 +44,14 @@ class ClusterFamily {\n \n   size_t MigrationsErrorsCount() const ABSL_LOCKS_EXCLUDED(migration_mu_);\n \n+  // Helper function to be used during takeover from both nodes (master and replica).\n+  // It reconciles the cluster configuration for both nodes to reflect the node\n+  // role changes after the takeover.\n+  // For the taking over node it's called at the end of the ReplTakeOver flow\n+  // and for the taken over node it's called at the end of the dflycmd::TakeOver\n+  void ReconcileMasterReplicaTakeoverSlots(bool was_master)\n+      ABSL_LOCKS_EXCLUDED(set_config_mu, migration_mu_);\n+\n  private:\n   using SinkReplyBuilder = facade::SinkReplyBuilder;\n \n@@ -69,7 +77,6 @@ class ClusterFamily {\n       ABSL_LOCKS_EXCLUDED(migration_mu_);\n   void DflyClusterFlushSlots(CmdArgList args, SinkReplyBuilder* builder);\n \n- private:  // Slots migration section\n   void DflySlotMigrationStatus(CmdArgList args, SinkReplyBuilder* builder)\n       ABSL_LOCKS_EXCLUDED(migration_mu_);\n \n@@ -114,11 +121,13 @@ class ClusterFamily {\n   std::vector<std::shared_ptr<OutgoingMigration>> outgoing_migration_jobs_\n       ABSL_GUARDED_BY(migration_mu_);\n \n- private:\n   std::optional<ClusterShardInfos> GetShardInfos(ConnectionContext* cntx) const;\n \n   ClusterShardInfo GetEmulatedShardInfo(ConnectionContext* cntx) const;\n \n+  void ReconcileMasterFlow() ABSL_EXCLUSIVE_LOCKS_REQUIRED(set_config_mu, migration_mu_);\n+  void ReconcileReplicaFlow() ABSL_EXCLUSIVE_LOCKS_REQUIRED(set_config_mu, migration_mu_);\n+\n   // Guards set configuration, so that we won't handle 2 in parallel.\n   mutable util::fb2::Mutex set_config_mu;\n \ndiff --git a/src/server/dflycmd.cc b/src/server/dflycmd.cc\nindex 1d348d6df7fb..940c9a39dfde 100644\n--- a/src/server/dflycmd.cc\n+++ b/src/server/dflycmd.cc\n@@ -20,6 +20,7 @@\n #include \"facade/cmd_arg_parser.h\"\n #include \"facade/dragonfly_connection.h\"\n #include \"facade/dragonfly_listener.h\"\n+#include \"server/cluster_support.h\"\n #include \"server/debugcmd.h\"\n #include \"server/engine_shard_set.h\"\n #include \"server/error.h\"\n@@ -511,8 +512,7 @@ void DflyCmd::TakeOver(CmdArgList args, RedisReplyBuilder* rb, ConnectionContext\n   atomic_bool catchup_success = true;\n   if (*status == OpStatus::OK) {\n     dfly::SharedLock lk{replica_ptr->shared_mu};\n-    auto cb = [replica_ptr = std::move(replica_ptr), end_time,\n-               &catchup_success](EngineShard* shard) {\n+    auto cb = [replica_ptr, end_time, &catchup_success](EngineShard* shard) {\n       if (!WaitReplicaFlowToCatchup(end_time, replica_ptr.get(), shard)) {\n         catchup_success.store(false);\n       }\n@@ -535,10 +535,16 @@ void DflyCmd::TakeOver(CmdArgList args, RedisReplyBuilder* rb, ConnectionContext\n       LOG(WARNING) << \"Failed to perform snapshot \" << ec.Format();\n     }\n   }\n-  VLOG(1) << \"Takeover accepted, shutting down.\";\n-  std::string save_arg = \"NOSAVE\";\n-  MutableSlice sargs(save_arg);\n-  return sf_->ShutdownCmd(CmdArgList(&sargs, 1), CommandContext{nullptr, rb, nullptr});\n+\n+  // For non-cluster mode we shutdown\n+  if (detail::cluster_mode == detail::ClusterMode::kNoCluster) {\n+    VLOG(1) << \"Takeover accepted, shutting down.\";\n+    std::string save_arg = \"NOSAVE\";\n+    MutableSlice sargs(save_arg);\n+    sf_->ShutdownCmd(CmdArgList(&sargs, 1), CommandContext{nullptr, rb, nullptr});\n+    return;\n+  }\n+  sf_->service().cluster_family().ReconcileMasterReplicaTakeoverSlots(true);\n }\n \n void DflyCmd::Expire(CmdArgList args, Transaction* tx, RedisReplyBuilder* rb) {\ndiff --git a/src/server/dflycmd.h b/src/server/dflycmd.h\nindex c4715bef637a..4bdb21ae1521 100644\n--- a/src/server/dflycmd.h\n+++ b/src/server/dflycmd.h\n@@ -217,7 +217,6 @@ class DflyCmd {\n   bool CheckReplicaStateOrReply(const ReplicaInfo& ri, SyncState expected,\n                                 facade::RedisReplyBuilder* rb);\n \n- private:\n   // Main entrypoint for stopping replication.\n   void StopReplication(uint32_t sync_id) ABSL_LOCKS_EXCLUDED(mu_);\n \ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex 96ef85814818..1144b30b3797 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -1039,6 +1039,49 @@ optional<ErrorReply> Service::CheckKeysOwnership(const CommandId* cid, CmdArgLis\n   return nullopt;\n }\n \n+// TODO(kostas) refactor. Almost 1-1 with CheckKeyOwnership() above.\n+std::optional<facade::ErrorReply> Service::TakenOverSlotError(const CommandId* cid, CmdArgList args,\n+                                                              const ConnectionContext& dfly_cntx) {\n+  if (cid->first_key_pos() == 0 && !cid->IsShardedPSub()) {\n+    return nullopt;  // No key command.\n+  }\n+\n+  OpResult<KeyIndex> key_index_res = FindKeys(cid, args);\n+\n+  if (!key_index_res) {\n+    return ErrorReply{key_index_res.status()};\n+  }\n+\n+  const auto& key_index = *key_index_res;\n+\n+  UniqueSlotChecker slot_checker;\n+  for (string_view key : key_index.Range(args)) {\n+    slot_checker.Add(key);\n+  }\n+\n+  if (slot_checker.IsCrossSlot()) {\n+    return ErrorReply{kCrossSlotError};\n+  }\n+\n+  optional<SlotId> keys_slot = slot_checker.GetUniqueSlotId();\n+  if (!keys_slot.has_value()) {\n+    return nullopt;\n+  }\n+\n+  if (auto error = cluster::SlotOwnershipError(*keys_slot);\n+      !error.status.has_value() || error.status.value() != facade::OpStatus::OK) {\n+    return ErrorReply{std::move(error)};\n+  }\n+  const auto cluster_config = cluster::ClusterConfig::Current();\n+  if (!cluster_config)\n+    return facade::ErrorReply{facade::kClusterNotConfigured};\n+\n+  // Moved regardless, we have been taken over\n+  cluster::ClusterNodeInfo redirect = cluster_config->GetMasterNodeForSlot(*keys_slot);\n+  return facade::ErrorReply{\n+      absl::StrCat(\"-MOVED \", *keys_slot, \" \", redirect.ip, \":\", redirect.port), \"MOVED\"};\n+}\n+\n // Return OK if all keys are allowed to be accessed: either declared in EVAL or\n // transaction is running in global or non-atomic mode.\n optional<ErrorReply> CheckKeysDeclared(const ConnectionState::ScriptInfo& eval_info,\n@@ -1158,6 +1201,15 @@ std::optional<ErrorReply> Service::VerifyCommandState(const CommandId* cid, CmdA\n       return ErrorReply(kLoadingErr);\n     }\n \n+    if (gstate == GlobalState::TAKEN_OVER) {\n+      if (IsClusterEnabled()) {\n+        if (auto err = TakenOverSlotError(cid, tail_args, dfly_cntx); err) {\n+          return err;\n+        }\n+      }\n+      return ErrorReply(kLoadingErr);\n+    }\n+\n     return ErrorReply{StrCat(\"Can not execute during \", GlobalStateName(gstate))};\n   }\n \ndiff --git a/src/server/main_service.h b/src/server/main_service.h\nindex b8f5160e6e25..a85439a23052 100644\n--- a/src/server/main_service.h\n+++ b/src/server/main_service.h\n@@ -160,6 +160,11 @@ class Service : public facade::ServiceInterface {\n   std::optional<facade::ErrorReply> CheckKeysOwnership(const CommandId* cid, CmdArgList args,\n                                                        const ConnectionContext& dfly_cntx);\n \n+  // Return moved error if we *own* the slot. This function is used from flows that assume our\n+  // state is TAKEN_OVER which happens after a replica takeover.\n+  std::optional<facade::ErrorReply> TakenOverSlotError(const CommandId* cid, CmdArgList args,\n+                                                       const ConnectionContext& dfly_cntx);\n+\n   void EvalInternal(CmdArgList args, const EvalArgs& eval_args, Interpreter* interpreter,\n                     SinkReplyBuilder* builder, ConnectionContext* cntx, bool read_only);\n   void CallSHA(CmdArgList args, std::string_view sha, Interpreter* interpreter,\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 8806383ace4c..9e0ea1a019b7 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -3304,6 +3304,8 @@ void ServerFamily::ReplTakeOver(CmdArgList args, const CommandContext& cmd_cntx)\n     return builder->SendError(\"Couldn't execute takeover\");\n \n   LOG(INFO) << \"Takeover successful, promoting this instance to master.\";\n+\n+  service().cluster_family().ReconcileMasterReplicaTakeoverSlots(false);\n   SetMasterFlagOnAllThreads(true);\n   last_master_data_ = replica_->Stop();\n   replica_.reset();\n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex b321a666b023..ac1d977f4a80 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -3357,3 +3357,59 @@ async def test_slot_migration_oom(df_factory):\n     assert status[0][0] == \"in\"\n     # Error message\n     assert status[0][4] == \"INCOMING_MIGRATION_OOM\"\n+\n+\n+@dfly_args({\"proactor_threads\": 4, \"cluster_mode\": \"yes\"})\n+async def test_replica_takeover_moved(\n+    df_factory: DflyInstanceFactory, df_seeder_factory: DflySeederFactory\n+):\n+    instances = [df_factory.create(port=next(next_port)) for i in range(4)]\n+    df_factory.start_all(instances)\n+\n+    nodes = [await create_node_info(n) for n in instances]\n+    m1, r1, m2, r2 = nodes\n+    master_nodes = [m1, m2]\n+\n+    m1.slots = [(0, 9000)]\n+    m2.slots = [(9001, 16383)]\n+\n+    m1.replicas = [r1]\n+    m2.replicas = [r2]\n+\n+    await push_config(json.dumps(generate_config(master_nodes)), [node.client for node in nodes])\n+\n+    logging.debug(\"create data\")\n+    await m1.client.execute_command(\"SET X 1\")\n+    # Slot number 16022\n+    await m2.client.execute_command(\"SET FOOX 1\")\n+\n+    logging.debug(\"start replication\")\n+    await r1.client.execute_command(f\"replicaof localhost {m1.instance.port}\")\n+    await r2.client.execute_command(f\"replicaof localhost {m2.instance.port}\")\n+\n+    await wait_available_async(r1.client)\n+\n+    assert await r1.client.execute_command(\"GET X\") == \"1\"\n+    assert await r1.client.execute_command(\"REPLTAKEOVER 20\") == \"OK\"\n+\n+    with pytest.raises(redis.exceptions.ResponseError) as moved_error:\n+        await m1.client.execute_command(\"GET X\")\n+\n+    assert str(moved_error.value) == f\"MOVED 7165 127.0.0.1:{r1.instance.port}\"\n+\n+    with pytest.raises(redis.exceptions.ResponseError) as moved_error:\n+        await m1.client.execute_command(\"GET FOOX\")\n+\n+    assert str(moved_error.value) == f\"MOVED 16022 127.0.0.1:{m2.instance.port}\"\n+\n+    # Try write command on the new master. It should succeed because during takeover,\n+    # we updated the config as well\n+    assert await r1.client.execute_command(\"SET X 2\") == \"OK\"\n+\n+    master_nodes = [r1, m2]\n+    r1.slots = [(0, 9000)]\n+    nodes.pop(0)\n+    await push_config(json.dumps(generate_config(master_nodes)), [node.client for node in nodes])\n+\n+    assert await r1.client.execute_command(\"GET X\") == \"2\"\n+    assert await m2.client.execute_command(\"GET FOOX\") == \"1\"\n",
  "problem_statement": "repltakeover in cluster mode should redirect to the replica\nCurrently `repltakeover` command for non-cluster mode shuts down the master.\n\nFor cluster mode it creates unnecessary disconnects and service unavailability for several seconds.\nIt's unnecessary as master knows exactly what is the last known location of its slots - his old replica.\n\nThe expected behavior should be -\nto return \"MOVED <replica_addr>\" for any transactional command , i.e. similarly to what we do with non-owned slots.\n",
  "hints_text": "",
  "created_at": "2025-07-22T10:27:03Z",
  "modified_files": [
    "src/server/cluster/cluster_config.h",
    "src/server/cluster/cluster_defs.h",
    "src/server/cluster/cluster_family.cc",
    "src/server/cluster/cluster_family.h",
    "src/server/dflycmd.cc",
    "src/server/dflycmd.h",
    "src/server/main_service.cc",
    "src/server/main_service.h",
    "src/server/server_family.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py"
  ]
}