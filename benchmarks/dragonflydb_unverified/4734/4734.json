{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4734,
  "instance_id": "dragonflydb__dragonfly-4734",
  "issue_numbers": [
    "4578"
  ],
  "base_commit": "57aa91fdb6381c99705e548fcca777ec4a3883c2",
  "patch": "diff --git a/src/server/journal/streamer.cc b/src/server/journal/streamer.cc\nindex 98c5efbb9783..c976ac52a4f3 100644\n--- a/src/server/journal/streamer.cc\n+++ b/src/server/journal/streamer.cc\n@@ -21,6 +21,9 @@ ABSL_FLAG(uint32_t, replication_timeout, 30000,\n ABSL_FLAG(uint32_t, replication_stream_output_limit, 64_KB,\n           \"Time to wait for the replication output buffer go below the throttle limit\");\n \n+ABSL_FLAG(uint32_t, migration_buckets_serialization_threshold, 100,\n+          \"The Number of buckets to serialize on each iteration before yielding\");\n+\n namespace dfly {\n using namespace util;\n using namespace journal;\n@@ -32,6 +35,7 @@ iovec IoVec(io::Bytes src) {\n }\n \n uint32_t replication_stream_output_limit_cached = 64_KB;\n+uint32_t migration_buckets_serialization_threshold_cached = 100;\n \n }  // namespace\n \n@@ -186,6 +190,8 @@ RestoreStreamer::RestoreStreamer(DbSlice* slice, cluster::SlotSet slots, journal\n                                  ExecutionState* cntx)\n     : JournalStreamer(journal, cntx), db_slice_(slice), my_slots_(std::move(slots)) {\n   DCHECK(slice != nullptr);\n+  migration_buckets_serialization_threshold_cached =\n+      absl::GetFlag(FLAGS_migration_buckets_serialization_threshold);\n   db_array_ = slice->databases();  // Inc ref to make sure DB isn't deleted while we use it\n }\n \n@@ -229,7 +235,7 @@ void RestoreStreamer::Run() {\n       stats_.buckets_loop += WriteBucket(it);\n     });\n \n-    if (++last_yield >= 100) {\n+    if (++last_yield >= migration_buckets_serialization_threshold_cached) {\n       ThisFiber::Yield();\n       last_yield = 0;\n     }\n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex 615208840bc0..46ac0af88dd1 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -2136,7 +2136,9 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)\n     assert extract_int_after_prefix(\"buckets on_db_update \", line) == 0\n \n \n-@dfly_args({\"proactor_threads\": 2, \"cluster_mode\": \"yes\"})\n+@dfly_args(\n+    {\"proactor_threads\": 2, \"cluster_mode\": \"yes\", \"migration_buckets_serialization_threshold\": 3}\n+)\n @pytest.mark.parametrize(\"chunk_size\", [1_000_000, 30])\n @pytest.mark.asyncio\n @pytest.mark.exclude_epoll\n",
  "problem_statement": "test_cluster_migration_while_seeding\nhttps://github.com/dragonflydb/dragonfly/actions/runs/13198374590/job/36844631873#step:12:890\n\n\n```\n        capture = await seeder.capture_fake_redis()\n        assert await seeder.compare(capture, instances[1].port)\n    \n        line = stop_and_get_restore_log(nodes[0].instance)\n        assert extract_int_after_prefix(\"Keys skipped \", line) == 0\n        assert extract_int_after_prefix(\"buckets skipped \", line) > 0\n        assert extract_int_after_prefix(\"keys written \", line) >= 9_000\n>       assert extract_int_after_prefix(\"buckets on_db_update \", line) > 0\nE       AssertionError: assert 0 > 0\nE        +  where 0 = extract_int_after_prefix('buckets on_db_update ', 'I20250207 11:07:42.041096 29736 streamer.cc:241] RestoreStreamer LSN of [0, 16383], shard 0 attempt 1 with 67485 commands. Buckets looped 897, buckets on_db_update 0, buckets skipped 1, buckets written 897. Keys skipped 0, keys written 9742\\n')\n\ndragonfly/cluster_test.py:2156: AssertionError\n\n```\n",
  "hints_text": "",
  "created_at": "2025-03-09T12:06:29Z",
  "modified_files": [
    "src/server/journal/streamer.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py"
  ]
}