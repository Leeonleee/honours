{
  "repo": "duckdb/duckdb",
  "pull_number": 14170,
  "instance_id": "duckdb__duckdb-14170",
  "issue_numbers": [
    "14129"
  ],
  "base_commit": "d58cc56f8554057f9646ea343d845e1b1ba6466e",
  "patch": "diff --git a/tools/pythonpkg/src/numpy/numpy_scan.cpp b/tools/pythonpkg/src/numpy/numpy_scan.cpp\nindex 54563204de5c..90f061416a83 100644\n--- a/tools/pythonpkg/src/numpy/numpy_scan.cpp\n+++ b/tools/pythonpkg/src/numpy/numpy_scan.cpp\n@@ -59,25 +59,31 @@ void ScanNumpyCategory(py::array &column, idx_t count, idx_t offset, Vector &out\n \t}\n }\n \n+static void ApplyMask(PandasColumnBindData &bind_data, ValidityMask &validity, idx_t count, idx_t offset) {\n+\tD_ASSERT(bind_data.mask);\n+\tauto mask = reinterpret_cast<const bool *>(bind_data.mask->numpy_array.data());\n+\tfor (idx_t i = 0; i < count; i++) {\n+\t\tauto is_null = mask[offset + i];\n+\t\tif (is_null) {\n+\t\t\tvalidity.SetInvalid(i);\n+\t\t}\n+\t}\n+}\n+\n template <class T>\n void ScanNumpyMasked(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {\n \tD_ASSERT(bind_data.pandas_col->Backend() == PandasColumnBackend::NUMPY);\n \tauto &numpy_col = reinterpret_cast<PandasNumpyColumn &>(*bind_data.pandas_col);\n \tScanNumpyColumn<T>(numpy_col.array, numpy_col.stride, offset, out, count);\n-\tauto &result_mask = FlatVector::Validity(out);\n \tif (bind_data.mask) {\n-\t\tauto mask = reinterpret_cast<const bool *>(bind_data.mask->numpy_array.data());\n-\t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tauto is_null = mask[offset + i];\n-\t\t\tif (is_null) {\n-\t\t\t\tresult_mask.SetInvalid(i);\n-\t\t\t}\n-\t\t}\n+\t\tauto &result_mask = FlatVector::Validity(out);\n+\t\tApplyMask(bind_data, result_mask, count, offset);\n \t}\n }\n \n template <class T>\n-void ScanNumpyFpColumn(const T *src_ptr, idx_t stride, idx_t count, idx_t offset, Vector &out) {\n+void ScanNumpyFpColumn(PandasColumnBindData &bind_data, const T *src_ptr, idx_t stride, idx_t count, idx_t offset,\n+                       Vector &out) {\n \tauto &mask = FlatVector::Validity(out);\n \tif (stride == sizeof(T)) {\n \t\tFlatVector::SetData(out, (data_ptr_t)(src_ptr + offset)); // NOLINT\n@@ -97,6 +103,10 @@ void ScanNumpyFpColumn(const T *src_ptr, idx_t stride, idx_t count, idx_t offset\n \t\t\t}\n \t\t}\n \t}\n+\tif (bind_data.mask) {\n+\t\tauto &result_mask = FlatVector::Validity(out);\n+\t\tApplyMask(bind_data, result_mask, count, offset);\n+\t}\n }\n \n template <class T>\n@@ -228,10 +238,12 @@ void NumpyScan::Scan(PandasColumnBindData &bind_data, idx_t count, idx_t offset,\n \t\tScanNumpyMasked<int64_t>(bind_data, count, offset, out);\n \t\tbreak;\n \tcase NumpyNullableType::FLOAT_32:\n-\t\tScanNumpyFpColumn<float>(reinterpret_cast<const float *>(array.data()), numpy_col.stride, count, offset, out);\n+\t\tScanNumpyFpColumn<float>(bind_data, reinterpret_cast<const float *>(array.data()), numpy_col.stride, count,\n+\t\t                         offset, out);\n \t\tbreak;\n \tcase NumpyNullableType::FLOAT_64:\n-\t\tScanNumpyFpColumn<double>(reinterpret_cast<const double *>(array.data()), numpy_col.stride, count, offset, out);\n+\t\tScanNumpyFpColumn<double>(bind_data, reinterpret_cast<const double *>(array.data()), numpy_col.stride, count,\n+\t\t                          offset, out);\n \t\tbreak;\n \tcase NumpyNullableType::DATETIME_NS:\n \tcase NumpyNullableType::DATETIME_MS:\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/pandas/test_pandas_types.py b/tools/pythonpkg/tests/fast/pandas/test_pandas_types.py\nindex 124b296eb33e..b5f5d6327bec 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_pandas_types.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_pandas_types.py\n@@ -1,4 +1,5 @@\n import duckdb\n+import pytest\n import pandas as pd\n import numpy\n import string\n@@ -74,6 +75,27 @@ def test_pandas_bool(self, duckdb_cursor):\n         data = numpy.array([True, False, False, True])\n         round_trip(data, 'bool')\n \n+    def test_pandas_masked_float64(self, duckdb_cursor, tmp_path):\n+        pa = pytest.importorskip(\"pyarrow\")\n+        pq = pytest.importorskip(\"pyarrow.parquet\")\n+\n+        # Create a sample DataFrame\n+        testdf = pd.DataFrame({\"value\": [26.0, 26.0, 26.0, pd.NA, 27.0, pd.NA, pd.NA, pd.NA, pd.NA, 29.0]})\n+\n+        # Set the correct dtype for the 'value' column\n+        testdf[\"value\"] = testdf[\"value\"].astype(pd.Float64Dtype())\n+\n+        # Write the DataFrame to a Parquet file using tmp_path fixture\n+        parquet_path = tmp_path / \"testdf.parquet\"\n+        pq.write_table(pa.Table.from_pandas(testdf), parquet_path)\n+\n+        # Read the Parquet file back into a DataFrame\n+        testdf2 = pd.read_parquet(parquet_path)\n+\n+        # Use duckdb_cursor to query the parquet data\n+        result = duckdb_cursor.execute(\"SELECT MIN(value) FROM testdf2\").fetchall()\n+        assert result[0][0] == 26\n+\n     def test_pandas_boolean(self, duckdb_cursor):\n         data = numpy.array([True, None, pd.NA, numpy.nan, True])\n         df_in = pd.DataFrame(\n",
  "problem_statement": "pd.NA values get coerced to 0.0 for a pd.Float64DType \n### What happens?\n\nI have a Pandas dataframe that I load from a Parquet file. If a column in that file is of pd.Float64DType, DuckDB does not convert pd.NA values to `NULL`, which would be an expected behaviour. \n\n### To Reproduce\n\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport duckdb\r\n```\r\n\r\n```\r\ntestdf = pd.DataFrame({\"value\": [26.0, 26.0, 26.0, pd.NA, 27.0, pd.NA, pd.NA, pd.NA, pd.NA, 29.0]})\r\ntestdf[\"value\"] = testdf[\"value\"].astype(pd.Float64Dtype())\r\n```\r\n\r\n```\r\npq.write_table(pa.Table.from_pandas(testdf), \"./testdf.parquet\")\r\n```\r\n\r\n```\r\ntestdf2 = pd.read_parquet(\"./testdf.parquet\")\r\n```\r\n\r\n```\r\ncon = duckdb.connect(database=\":memory:\")\r\ncon.register(\"testdf\", testdf)\r\ncon.register(\"testdf2\", testdf2)\r\n```\r\n\r\n```\r\ncon.sql(\"SELECT MIN(value) FROM testdf\")\r\n```\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 min(\"value\") \u2502\r\n\u2502    double    \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502         26.0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```\r\ntestdf2[\"value\"].min()\r\n```\r\n\r\n```\r\n26.0\r\n```\r\n\r\n```\r\ncon.sql(\"SELECT MIN(value) FROM testdf2\")\r\n```\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 min(\"value\") \u2502\r\n\u2502    double    \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502          0.0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n\n### OS:\n\nMacOS, x86_64\n\n### DuckDB Version:\n\n1.1.1.\n\n### DuckDB Client:\n\nPython\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nDmitry Kryuchkov\n\n### Affiliation:\n\nSHERLOK TECHNOLOGY PTY LTD\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n",
  "hints_text": "Float64DType is not a parquet data type. Given how bad pandas is around nullability, I wouldn't be surprised if it's a problem on their end (e.g. during the conversion to pyarrow). Did you try reading the parquet with a third party tool? Did you try writing from pandas to parquet directly without explicitly pyarrow conversion?",
  "created_at": "2024-09-30T08:30:06Z"
}