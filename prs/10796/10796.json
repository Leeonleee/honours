{
  "repo": "duckdb/duckdb",
  "pull_number": 10796,
  "instance_id": "duckdb__duckdb-10796",
  "issue_numbers": [
    "10795"
  ],
  "base_commit": "894710a6b8380e33385c2945c40bbcd8c9f485cc",
  "patch": "diff --git a/src/common/arrow/arrow_appender.cpp b/src/common/arrow/arrow_appender.cpp\nindex bbca77756832..faad7d2693f6 100644\n--- a/src/common/arrow/arrow_appender.cpp\n+++ b/src/common/arrow/arrow_appender.cpp\n@@ -236,11 +236,8 @@ static void InitializeFunctionPointers(ArrowAppendData &append_data, const Logic\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::MAP:\n-\t\tif (append_data.options.arrow_offset_size == ArrowOffsetSize::LARGE) {\n-\t\t\tInitializeAppenderForType<ArrowMapData<int64_t>>(append_data);\n-\t\t} else {\n-\t\t\tInitializeAppenderForType<ArrowMapData<int32_t>>(append_data);\n-\t\t}\n+\t\t// Arrow MapArray only supports 32-bit offsets. There is no LargeMapArray type in Arrow.\n+\t\tInitializeAppenderForType<ArrowMapData<int32_t>>(append_data);\n \t\tbreak;\n \tdefault:\n \t\tthrow NotImplementedException(\"Unsupported type in DuckDB -> Arrow Conversion: %s\\n\", type.ToString());\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_10795.py b/tools/pythonpkg/tests/fast/arrow/test_10795.py\nnew file mode 100644\nindex 000000000000..043bf4ff039d\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/arrow/test_10795.py\n@@ -0,0 +1,12 @@\n+import duckdb\n+import pytest\n+\n+pyarrow = pytest.importorskip('pyarrow')\n+\n+\n+@pytest.mark.parametrize('arrow_large_buffer_size', [True, False])\n+def test_10795(arrow_large_buffer_size):\n+    conn = duckdb.connect()\n+    conn.sql(f\"set arrow_large_buffer_size={arrow_large_buffer_size}\")\n+    arrow = conn.sql(\"select map(['non-inlined string', 'test', 'duckdb'], [42, 1337, 123]) as map\").to_arrow_table()\n+    assert arrow.to_pydict() == {'map': [[('non-inlined string', 42), ('test', 1337), ('duckdb', 123)]]}\n",
  "problem_statement": "Bug in map to Arrow conversion.\n### What happens?\n\nWhen `arrow_large_buffer_size` is enabled, conversion of map data type to Arrow results in incorrect data. It looks like DuckDB uses 64-bit offsets when generating Arrow MapArray, but MapArray only supports 32-bit offsets. There is no LargeMapArray type in Arrow.\n\n### To Reproduce\n\n```\r\nimport duckdb\r\n\r\n# correct\r\narrow = duckdb.sql(\"select map(['non-inlined string', 'test', 'duckdb'], [42, 1337, 123])\").to_arrow_table()\r\nprint(arrow)\r\n\r\n# incorrect\r\nduckdb.sql(\"set arrow_large_buffer_size=true\")\r\narrow = duckdb.sql(\"select map(['non-inlined string', 'test', 'duckdb'], [42, 1337, 123])\").to_arrow_table()\r\nprint(arrow)\r\n```\r\n\r\n```\r\npyarrow.Table\r\nmap(main.list_value('non-inlined string', 'test', 'duckdb'), main.list_value(42, 1337, 123)): map<string, int32>\r\n  child 0, entries: struct<key: string not null, value: int32> not null\r\n      child 0, key: string not null\r\n      child 1, value: int32\r\n----\r\nmap(main.list_value('non-inlined string', 'test', 'duckdb'), main.list_value(42, 1337, 123)): [[keys:[\"non-inlined string\",\"test\",\"duckdb\"]values:[42,1337,123]]\r\n```\r\n```\r\npyarrow.Table\r\nmap(main.list_value('non-inlined string', 'test', 'duckdb'), main.list_value(42, 1337, 123)): map<large_string, int32>\r\n  child 0, entries: struct<key: large_string not null, value: int32> not null\r\n      child 0, key: large_string not null\r\n      child 1, value: int32\r\n----\r\nmap(main.list_value('non-inlined string', 'test', 'duckdb'), main.list_value(42, 1337, 123)): [[keys:[]values:[]]]\r\n```\n\n### OS:\n\nubuntu 20.04\n\n### DuckDB Version:\n\nmain\n\n### DuckDB Client:\n\npython\n\n### Full Name:\n\nYiyuan Liu\n\n### Affiliation:\n\nHigh-Flyer AI\n\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\n\nI have tested with a nightly build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "",
  "created_at": "2024-02-22T06:19:18Z"
}