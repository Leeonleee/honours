{
  "repo": "duckdb/duckdb",
  "pull_number": 4724,
  "instance_id": "duckdb__duckdb-4724",
  "issue_numbers": [
    "4694",
    "4694"
  ],
  "base_commit": "27d89cabf527c5b533d691724692299886179224",
  "patch": "diff --git a/src/execution/operator/aggregate/physical_hash_aggregate.cpp b/src/execution/operator/aggregate/physical_hash_aggregate.cpp\nindex 454ee5931776..0f0f95ac3df0 100644\n--- a/src/execution/operator/aggregate/physical_hash_aggregate.cpp\n+++ b/src/execution/operator/aggregate/physical_hash_aggregate.cpp\n@@ -9,7 +9,7 @@\n #include \"duckdb/parallel/task_scheduler.hpp\"\n #include \"duckdb/planner/expression/bound_aggregate_expression.hpp\"\n #include \"duckdb/planner/expression/bound_constant_expression.hpp\"\n-#include \"duckdb/parallel/event.hpp\"\n+#include \"duckdb/parallel/base_pipeline_event.hpp\"\n #include \"duckdb/common/atomic.hpp\"\n \n namespace duckdb {\n@@ -168,16 +168,15 @@ void PhysicalHashAggregate::Combine(ExecutionContext &context, GlobalSinkState &\n \t}\n }\n \n-class HashAggregateFinalizeEvent : public Event {\n+class HashAggregateFinalizeEvent : public BasePipelineEvent {\n public:\n \tHashAggregateFinalizeEvent(const PhysicalHashAggregate &op_p, HashAggregateGlobalState &gstate_p,\n \t                           Pipeline *pipeline_p)\n-\t    : Event(pipeline_p->executor), op(op_p), gstate(gstate_p), pipeline(pipeline_p) {\n+\t    : BasePipelineEvent(*pipeline_p), op(op_p), gstate(gstate_p) {\n \t}\n \n \tconst PhysicalHashAggregate &op;\n \tHashAggregateGlobalState &gstate;\n-\tPipeline *pipeline;\n \n public:\n \tvoid Schedule() override {\ndiff --git a/src/execution/operator/aggregate/physical_ungrouped_aggregate.cpp b/src/execution/operator/aggregate/physical_ungrouped_aggregate.cpp\nindex 39fed3250222..600b59c75cb5 100644\n--- a/src/execution/operator/aggregate/physical_ungrouped_aggregate.cpp\n+++ b/src/execution/operator/aggregate/physical_ungrouped_aggregate.cpp\n@@ -8,7 +8,7 @@\n #include \"duckdb/parallel/thread_context.hpp\"\n #include \"duckdb/planner/expression/bound_aggregate_expression.hpp\"\n #include \"duckdb/execution/radix_partitioned_hashtable.hpp\"\n-#include \"duckdb/parallel/event.hpp\"\n+#include \"duckdb/parallel/base_pipeline_event.hpp\"\n #include \"duckdb/common/unordered_set.hpp\"\n #include \"duckdb/common/algorithm.hpp\"\n #include <functional>\n@@ -442,15 +442,14 @@ class DistinctAggregateFinalizeTask : public ExecutorTask {\n };\n \n // TODO: Create tasks and run these in parallel instead of doing this all in Schedule, single threaded\n-class DistinctAggregateFinalizeEvent : public Event {\n+class DistinctAggregateFinalizeEvent : public BasePipelineEvent {\n public:\n \tDistinctAggregateFinalizeEvent(const PhysicalUngroupedAggregate &op_p, UngroupedAggregateGlobalState &gstate_p,\n-\t                               Pipeline *pipeline_p, ClientContext &context)\n-\t    : Event(pipeline_p->executor), op(op_p), gstate(gstate_p), pipeline(pipeline_p), context(context) {\n+\t                               Pipeline &pipeline_p, ClientContext &context)\n+\t    : BasePipelineEvent(pipeline_p), op(op_p), gstate(gstate_p), context(context) {\n \t}\n \tconst PhysicalUngroupedAggregate &op;\n \tUngroupedAggregateGlobalState &gstate;\n-\tPipeline *pipeline;\n \tClientContext &context;\n \n public:\n@@ -463,16 +462,15 @@ class DistinctAggregateFinalizeEvent : public Event {\n \t}\n };\n \n-class DistinctCombineFinalizeEvent : public Event {\n+class DistinctCombineFinalizeEvent : public BasePipelineEvent {\n public:\n \tDistinctCombineFinalizeEvent(const PhysicalUngroupedAggregate &op_p, UngroupedAggregateGlobalState &gstate_p,\n-\t                             Pipeline *pipeline_p, ClientContext &client)\n-\t    : Event(pipeline_p->executor), op(op_p), gstate(gstate_p), pipeline(pipeline_p), client(client) {\n+\t                             Pipeline &pipeline_p, ClientContext &client)\n+\t    : BasePipelineEvent(pipeline_p), op(op_p), gstate(gstate_p), client(client) {\n \t}\n \n \tconst PhysicalUngroupedAggregate &op;\n \tUngroupedAggregateGlobalState &gstate;\n-\tPipeline *pipeline;\n \tClientContext &client;\n \n public:\n@@ -488,7 +486,7 @@ class DistinctCombineFinalizeEvent : public Event {\n \t\tSetTasks(move(tasks));\n \n \t\t//! Now that all tables are combined, it's time to do the distinct aggregations\n-\t\tauto new_event = make_shared<DistinctAggregateFinalizeEvent>(op, gstate, pipeline, client);\n+\t\tauto new_event = make_shared<DistinctAggregateFinalizeEvent>(op, gstate, *pipeline, client);\n \t\tthis->InsertEvent(move(new_event));\n \t}\n };\n@@ -517,12 +515,12 @@ SinkFinalizeType PhysicalUngroupedAggregate::FinalizeDistinct(Pipeline &pipeline\n \t\t}\n \t}\n \tif (any_partitioned) {\n-\t\tauto new_event = make_shared<DistinctCombineFinalizeEvent>(*this, gstate, &pipeline, context);\n+\t\tauto new_event = make_shared<DistinctCombineFinalizeEvent>(*this, gstate, pipeline, context);\n \t\tevent.InsertEvent(move(new_event));\n \t} else {\n \t\t//! Hashtables aren't partitioned, they dont need to be joined first\n \t\t//! So we can compute the aggregate already\n-\t\tauto new_event = make_shared<DistinctAggregateFinalizeEvent>(*this, gstate, &pipeline, context);\n+\t\tauto new_event = make_shared<DistinctAggregateFinalizeEvent>(*this, gstate, pipeline, context);\n \t\tevent.InsertEvent(move(new_event));\n \t}\n \treturn SinkFinalizeType::READY;\ndiff --git a/src/execution/operator/aggregate/physical_window.cpp b/src/execution/operator/aggregate/physical_window.cpp\nindex 318b8e43e632..ba6f2814fc26 100644\n--- a/src/execution/operator/aggregate/physical_window.cpp\n+++ b/src/execution/operator/aggregate/physical_window.cpp\n@@ -13,7 +13,7 @@\n #include \"duckdb/execution/window_segment_tree.hpp\"\n #include \"duckdb/main/client_config.hpp\"\n #include \"duckdb/main/config.hpp\"\n-#include \"duckdb/parallel/event.hpp\"\n+#include \"duckdb/parallel/base_pipeline_event.hpp\"\n #include \"duckdb/planner/expression/bound_reference_expression.hpp\"\n #include \"duckdb/planner/expression/bound_window_expression.hpp\"\n \n@@ -1502,19 +1502,18 @@ class WindowMergeTask : public ExecutorTask {\n \tWindowGlobalHashGroup &hash_group;\n };\n \n-class WindowMergeEvent : public Event {\n+class WindowMergeEvent : public BasePipelineEvent {\n public:\n \tWindowMergeEvent(WindowGlobalSinkState &gstate_p, Pipeline &pipeline_p, WindowGlobalHashGroup &hash_group_p)\n-\t    : Event(pipeline_p.executor), gstate(gstate_p), pipeline(pipeline_p), hash_group(hash_group_p) {\n+\t    : BasePipelineEvent(pipeline_p), gstate(gstate_p), hash_group(hash_group_p) {\n \t}\n \n \tWindowGlobalSinkState &gstate;\n-\tPipeline &pipeline;\n \tWindowGlobalHashGroup &hash_group;\n \n public:\n \tvoid Schedule() override {\n-\t\tauto &context = pipeline.GetClientContext();\n+\t\tauto &context = pipeline->GetClientContext();\n \n \t\t// Schedule tasks equal to the number of threads, which will each merge multiple partitions\n \t\tauto &ts = TaskScheduler::GetScheduler(context);\n@@ -1529,7 +1528,7 @@ class WindowMergeEvent : public Event {\n \n \tvoid FinishEvent() override {\n \t\thash_group.global_sort->CompleteMergeRound(true);\n-\t\tCreateMergeTasks(pipeline, *this, gstate, hash_group);\n+\t\tCreateMergeTasks(*pipeline, *this, gstate, hash_group);\n \t}\n \n \tstatic void CreateMergeTasks(Pipeline &pipeline, Event &event, WindowGlobalSinkState &state,\ndiff --git a/src/execution/operator/join/physical_hash_join.cpp b/src/execution/operator/join/physical_hash_join.cpp\nindex 3c98eea7614d..b30eb45afe81 100644\n--- a/src/execution/operator/join/physical_hash_join.cpp\n+++ b/src/execution/operator/join/physical_hash_join.cpp\n@@ -6,7 +6,7 @@\n #include \"duckdb/function/aggregate/distributive_functions.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/main/query_profiler.hpp\"\n-#include \"duckdb/parallel/event.hpp\"\n+#include \"duckdb/parallel/base_pipeline_event.hpp\"\n #include \"duckdb/parallel/pipeline.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n #include \"duckdb/storage/buffer_manager.hpp\"\n@@ -252,18 +252,17 @@ class HashJoinFinalizeTask : public ExecutorTask {\n \tbool parallel;\n };\n \n-class HashJoinFinalizeEvent : public Event {\n+class HashJoinFinalizeEvent : public BasePipelineEvent {\n public:\n \tHashJoinFinalizeEvent(Pipeline &pipeline_p, HashJoinGlobalSinkState &sink)\n-\t    : Event(pipeline_p.executor), pipeline(pipeline_p), sink(sink) {\n+\t    : BasePipelineEvent(pipeline_p), sink(sink) {\n \t}\n \n-\tPipeline &pipeline;\n \tHashJoinGlobalSinkState &sink;\n \n public:\n \tvoid Schedule() override {\n-\t\tauto &context = pipeline.GetClientContext();\n+\t\tauto &context = pipeline->GetClientContext();\n \t\tauto parallel_construct_count =\n \t\t    context.config.verify_parallelism ? STANDARD_VECTOR_SIZE : PARALLEL_CONSTRUCT_COUNT;\n \n@@ -330,20 +329,19 @@ class HashJoinPartitionTask : public ExecutorTask {\n \tJoinHashTable &local_ht;\n };\n \n-class HashJoinPartitionEvent : public Event {\n+class HashJoinPartitionEvent : public BasePipelineEvent {\n public:\n \tHashJoinPartitionEvent(Pipeline &pipeline_p, HashJoinGlobalSinkState &sink,\n \t                       vector<unique_ptr<JoinHashTable>> &local_hts)\n-\t    : Event(pipeline_p.executor), pipeline(pipeline_p), sink(sink), local_hts(local_hts) {\n+\t    : BasePipelineEvent(pipeline_p), sink(sink), local_hts(local_hts) {\n \t}\n \n-\tPipeline &pipeline;\n \tHashJoinGlobalSinkState &sink;\n \tvector<unique_ptr<JoinHashTable>> &local_hts;\n \n public:\n \tvoid Schedule() override {\n-\t\tauto &context = pipeline.GetClientContext();\n+\t\tauto &context = pipeline->GetClientContext();\n \t\tvector<unique_ptr<Task>> partition_tasks;\n \t\tpartition_tasks.reserve(local_hts.size());\n \t\tfor (auto &local_ht : local_hts) {\n@@ -356,7 +354,7 @@ class HashJoinPartitionEvent : public Event {\n \tvoid FinishEvent() override {\n \t\tlocal_hts.clear();\n \t\tsink.hash_table->PrepareExternalFinalize();\n-\t\tsink.ScheduleFinalize(pipeline, *this);\n+\t\tsink.ScheduleFinalize(*pipeline, *this);\n \t}\n };\n \ndiff --git a/src/execution/operator/join/physical_range_join.cpp b/src/execution/operator/join/physical_range_join.cpp\nindex a102e2105ae6..5bad1365693a 100644\n--- a/src/execution/operator/join/physical_range_join.cpp\n+++ b/src/execution/operator/join/physical_range_join.cpp\n@@ -8,7 +8,7 @@\n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n-#include \"duckdb/parallel/event.hpp\"\n+#include \"duckdb/parallel/base_pipeline_event.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n \n #include <thread>\n@@ -104,21 +104,20 @@ class RangeJoinMergeTask : public ExecutorTask {\n \tGlobalSortedTable &table;\n };\n \n-class RangeJoinMergeEvent : public Event {\n+class RangeJoinMergeEvent : public BasePipelineEvent {\n public:\n \tusing GlobalSortedTable = PhysicalRangeJoin::GlobalSortedTable;\n \n public:\n \tRangeJoinMergeEvent(GlobalSortedTable &table_p, Pipeline &pipeline_p)\n-\t    : Event(pipeline_p.executor), table(table_p), pipeline(pipeline_p) {\n+\t    : BasePipelineEvent(pipeline_p), table(table_p) {\n \t}\n \n \tGlobalSortedTable &table;\n-\tPipeline &pipeline;\n \n public:\n \tvoid Schedule() override {\n-\t\tauto &context = pipeline.GetClientContext();\n+\t\tauto &context = pipeline->GetClientContext();\n \n \t\t// Schedule tasks equal to the number of threads, which will each merge multiple partitions\n \t\tauto &ts = TaskScheduler::GetScheduler(context);\n@@ -137,7 +136,7 @@ class RangeJoinMergeEvent : public Event {\n \t\tglobal_sort_state.CompleteMergeRound(true);\n \t\tif (global_sort_state.sorted_blocks.size() > 1) {\n \t\t\t// Multiple blocks remaining: Schedule the next round\n-\t\t\ttable.ScheduleMergeTasks(pipeline, *this);\n+\t\t\ttable.ScheduleMergeTasks(*pipeline, *this);\n \t\t}\n \t}\n };\ndiff --git a/src/execution/operator/order/physical_order.cpp b/src/execution/operator/order/physical_order.cpp\nindex b3c53b40058a..df70c7ee1e46 100644\n--- a/src/execution/operator/order/physical_order.cpp\n+++ b/src/execution/operator/order/physical_order.cpp\n@@ -5,7 +5,7 @@\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/storage/buffer_manager.hpp\"\n \n-#include \"duckdb/parallel/event.hpp\"\n+#include \"duckdb/parallel/base_pipeline_event.hpp\"\n \n namespace duckdb {\n \n@@ -123,18 +123,17 @@ class PhysicalOrderMergeTask : public ExecutorTask {\n \tOrderGlobalState &state;\n };\n \n-class OrderMergeEvent : public Event {\n+class OrderMergeEvent : public BasePipelineEvent {\n public:\n \tOrderMergeEvent(OrderGlobalState &gstate_p, Pipeline &pipeline_p)\n-\t    : Event(pipeline_p.executor), gstate(gstate_p), pipeline(pipeline_p) {\n+\t    : BasePipelineEvent(pipeline_p), gstate(gstate_p) {\n \t}\n \n \tOrderGlobalState &gstate;\n-\tPipeline &pipeline;\n \n public:\n \tvoid Schedule() override {\n-\t\tauto &context = pipeline.GetClientContext();\n+\t\tauto &context = pipeline->GetClientContext();\n \n \t\t// Schedule tasks equal to the number of threads, which will each merge multiple partitions\n \t\tauto &ts = TaskScheduler::GetScheduler(context);\n@@ -153,7 +152,7 @@ class OrderMergeEvent : public Event {\n \t\tglobal_sort_state.CompleteMergeRound();\n \t\tif (global_sort_state.sorted_blocks.size() > 1) {\n \t\t\t// Multiple blocks remaining: Schedule the next round\n-\t\t\tPhysicalOrder::ScheduleMergeTasks(pipeline, *this, gstate);\n+\t\t\tPhysicalOrder::ScheduleMergeTasks(*pipeline, *this, gstate);\n \t\t}\n \t}\n };\ndiff --git a/src/include/duckdb/execution/executor.hpp b/src/include/duckdb/execution/executor.hpp\nindex 33c4f1abbdb1..c2b19bd1947c 100644\n--- a/src/include/duckdb/execution/executor.hpp\n+++ b/src/include/duckdb/execution/executor.hpp\n@@ -132,6 +132,8 @@ class Executor {\n \tatomic<idx_t> completed_pipelines;\n \t//! The total amount of pipelines in the query\n \tidx_t total_pipelines;\n+\t//! Whether or not execution is cancelled\n+\tbool cancelled;\n \n \t//! The adjacent union pipelines of each pipeline\n \t//! Union pipelines have the same sink, but can be run concurrently along with this pipeline\ndiff --git a/src/include/duckdb/parallel/base_pipeline_event.hpp b/src/include/duckdb/parallel/base_pipeline_event.hpp\nnew file mode 100644\nindex 000000000000..1878aaae0a1b\n--- /dev/null\n+++ b/src/include/duckdb/parallel/base_pipeline_event.hpp\n@@ -0,0 +1,26 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/parallel/base_pipeline_event.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/parallel/event.hpp\"\n+#include \"duckdb/parallel/pipeline.hpp\"\n+\n+namespace duckdb {\n+\n+//! A BasePipelineEvent is used as the basis of any event that belongs to a specific pipeline\n+class BasePipelineEvent : public Event {\n+public:\n+\tBasePipelineEvent(shared_ptr<Pipeline> pipeline);\n+\tBasePipelineEvent(Pipeline &pipeline);\n+\n+\t//! The pipeline that this event belongs to\n+\tshared_ptr<Pipeline> pipeline;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/parallel/pipeline_event.hpp b/src/include/duckdb/parallel/pipeline_event.hpp\nindex 3862322dd37d..7af51d4836f7 100644\n--- a/src/include/duckdb/parallel/pipeline_event.hpp\n+++ b/src/include/duckdb/parallel/pipeline_event.hpp\n@@ -8,18 +8,15 @@\n \n #pragma once\n \n-#include \"duckdb/parallel/event.hpp\"\n-#include \"duckdb/parallel/pipeline.hpp\"\n+#include \"duckdb/parallel/base_pipeline_event.hpp\"\n \n namespace duckdb {\n \n-class PipelineEvent : public Event {\n+//! A PipelineEvent is responsible for scheduling a pipeline\n+class PipelineEvent : public BasePipelineEvent {\n public:\n \tPipelineEvent(shared_ptr<Pipeline> pipeline);\n \n-\t//! The pipeline that this event belongs to\n-\tshared_ptr<Pipeline> pipeline;\n-\n public:\n \tvoid Schedule() override;\n \tvoid FinishEvent() override;\ndiff --git a/src/include/duckdb/parallel/pipeline_finish_event.hpp b/src/include/duckdb/parallel/pipeline_finish_event.hpp\nindex b01fdfdaf438..b5bdc750a605 100644\n--- a/src/include/duckdb/parallel/pipeline_finish_event.hpp\n+++ b/src/include/duckdb/parallel/pipeline_finish_event.hpp\n@@ -8,19 +8,15 @@\n \n #pragma once\n \n-#include \"duckdb/parallel/event.hpp\"\n-#include \"duckdb/parallel/pipeline.hpp\"\n+#include \"duckdb/parallel/base_pipeline_event.hpp\"\n \n namespace duckdb {\n class Executor;\n \n-class PipelineFinishEvent : public Event {\n+class PipelineFinishEvent : public BasePipelineEvent {\n public:\n \tPipelineFinishEvent(shared_ptr<Pipeline> pipeline);\n \n-\t//! The pipeline that this event belongs to\n-\tshared_ptr<Pipeline> pipeline;\n-\n public:\n \tvoid Schedule() override;\n \tvoid FinishEvent() override;\ndiff --git a/src/parallel/CMakeLists.txt b/src/parallel/CMakeLists.txt\nindex 04f738ecb9eb..440e654c3e59 100644\n--- a/src/parallel/CMakeLists.txt\n+++ b/src/parallel/CMakeLists.txt\n@@ -1,6 +1,7 @@\n add_library_unity(\n   duckdb_parallel\n   OBJECT\n+  base_pipeline_event.cpp\n   executor_task.cpp\n   executor.cpp\n   event.cpp\ndiff --git a/src/parallel/base_pipeline_event.cpp b/src/parallel/base_pipeline_event.cpp\nnew file mode 100644\nindex 000000000000..fa6c3079c4e1\n--- /dev/null\n+++ b/src/parallel/base_pipeline_event.cpp\n@@ -0,0 +1,13 @@\n+#include \"duckdb/parallel/base_pipeline_event.hpp\"\n+\n+namespace duckdb {\n+\n+BasePipelineEvent::BasePipelineEvent(shared_ptr<Pipeline> pipeline_p)\n+    : Event(pipeline_p->executor), pipeline(move(pipeline_p)) {\n+}\n+\n+BasePipelineEvent::BasePipelineEvent(Pipeline &pipeline_p)\n+    : Event(pipeline_p.executor), pipeline(pipeline_p.shared_from_this()) {\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/parallel/executor.cpp b/src/parallel/executor.cpp\nindex 265bc660df3a..9a3477603874 100644\n--- a/src/parallel/executor.cpp\n+++ b/src/parallel/executor.cpp\n@@ -28,6 +28,9 @@ Executor &Executor::Get(ClientContext &context) {\n \n void Executor::AddEvent(shared_ptr<Event> event) {\n \tlock_guard<mutex> elock(executor_lock);\n+\tif (cancelled) {\n+\t\treturn;\n+\t}\n \tevents.push_back(move(event));\n }\n \n@@ -331,6 +334,7 @@ void Executor::CancelTasks() {\n \tvector<weak_ptr<Pipeline>> weak_references;\n \t{\n \t\tlock_guard<mutex> elock(executor_lock);\n+\t\tcancelled = true;\n \t\tweak_references.reserve(pipelines.size());\n \t\tfor (auto &pipeline : pipelines) {\n \t\t\tweak_references.push_back(weak_ptr<Pipeline>(pipeline));\n@@ -419,6 +423,7 @@ PendingExecutionResult Executor::ExecuteTask() {\n void Executor::Reset() {\n \tlock_guard<mutex> elock(executor_lock);\n \tphysical_plan = nullptr;\n+\tcancelled = false;\n \towned_plan.reset();\n \troot_executor.reset();\n \troot_pipelines.clear();\ndiff --git a/src/parallel/pipeline_event.cpp b/src/parallel/pipeline_event.cpp\nindex ffc0e60232c7..b5591a643045 100644\n--- a/src/parallel/pipeline_event.cpp\n+++ b/src/parallel/pipeline_event.cpp\n@@ -2,8 +2,7 @@\n \n namespace duckdb {\n \n-PipelineEvent::PipelineEvent(shared_ptr<Pipeline> pipeline_p)\n-    : Event(pipeline_p->executor), pipeline(move(pipeline_p)) {\n+PipelineEvent::PipelineEvent(shared_ptr<Pipeline> pipeline_p) : BasePipelineEvent(move(pipeline_p)) {\n }\n \n void PipelineEvent::Schedule() {\ndiff --git a/src/parallel/pipeline_finish_event.cpp b/src/parallel/pipeline_finish_event.cpp\nindex 9ed5cafe9c53..d542b02750df 100644\n--- a/src/parallel/pipeline_finish_event.cpp\n+++ b/src/parallel/pipeline_finish_event.cpp\n@@ -3,8 +3,7 @@\n \n namespace duckdb {\n \n-PipelineFinishEvent::PipelineFinishEvent(shared_ptr<Pipeline> pipeline_p)\n-    : Event(pipeline_p->executor), pipeline(move(pipeline_p)) {\n+PipelineFinishEvent::PipelineFinishEvent(shared_ptr<Pipeline> pipeline_p) : BasePipelineEvent(move(pipeline_p)) {\n }\n \n void PipelineFinishEvent::Schedule() {\n",
  "test_patch": "diff --git a/test/fuzzer/pedro/hash_finalize_race.test b/test/fuzzer/pedro/hash_finalize_race.test\nnew file mode 100644\nindex 000000000000..bf6a01bd2058\n--- /dev/null\n+++ b/test/fuzzer/pedro/hash_finalize_race.test\n@@ -0,0 +1,19 @@\n+# name: test/fuzzer/pedro/hash_finalize_race.test\n+# description: Issue #4694: Race condition at HashJoinFinalizeEvent\n+# group: [pedro]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE SEQUENCE t4;\n+\n+statement ok\n+PRAGMA DEBUG_FORCE_EXTERNAL=1;\n+\n+loop i 0 10\n+\n+statement error\n+SELECT 1 FROM ((SELECT 2) INTERSECT (SELECT 2)) t2(c3) WHERE \"currval\"('t4') = EXISTS (SELECT 2);\n+\n+endloop\n",
  "problem_statement": "[Fuzzer] Race condition at HashJoinFinalizeEvent\n### What happens?\n\nRun the statements:\r\n```\r\nCREATE SEQUENCE t4;\r\nPRAGMA DEBUG_FORCE_EXTERNAL=1;\r\nSELECT 1 FROM ((SELECT 2) INTERSECT (SELECT 2)) t2(c3) WHERE \"currval\"('t4') = EXISTS (SELECT 2);\r\n```\r\nIt may trigger a heap-use-after-free with the address sanitizer at  duckdb::HashJoinFinalizeEvent::HashJoinFinalizeEvent\n\n### To Reproduce\n\nRun the statements above,\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\nlatest from sources\n\n### DuckDB Client:\n\nShell\n\n### Full Name:\n\nPedro Ferreira\n\n### Affiliation:\n\nHuawei\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n[Fuzzer] Race condition at HashJoinFinalizeEvent\n### What happens?\n\nRun the statements:\r\n```\r\nCREATE SEQUENCE t4;\r\nPRAGMA DEBUG_FORCE_EXTERNAL=1;\r\nSELECT 1 FROM ((SELECT 2) INTERSECT (SELECT 2)) t2(c3) WHERE \"currval\"('t4') = EXISTS (SELECT 2);\r\n```\r\nIt may trigger a heap-use-after-free with the address sanitizer at  duckdb::HashJoinFinalizeEvent::HashJoinFinalizeEvent\n\n### To Reproduce\n\nRun the statements above,\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\nlatest from sources\n\n### DuckDB Client:\n\nShell\n\n### Full Name:\n\nPedro Ferreira\n\n### Affiliation:\n\nHuawei\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "Nice find! I recently reworked the hash join a bit, so this seems to be happening in a new task that I've spawned.\r\n\r\nThe race condition is between the main thread calling `ClientContext::EndQueryInternal`, and another thread that calls `event->FinishTask()` at `physical_hash_join.cpp:322`. The object of the race condition is the `Executor` that is responsible for finishing the query. The main thread is ending the query while another thread is still working on it.\r\n\r\nThe query does not finish nicely, as it yields this error:\r\n```\r\nError: Serialization Error: currval: sequence is not yet defined in this session\r\n```\r\n\r\nSo I think the issue here is not with the hash join, but rather with the main thread ending the query early due to the error (and deleting the `Executor` while doing so), while other threads may still be working on tasks, which depend on the `Executor`.\nThanks for the explanation! So the solution would be for the main thread to wait for other threads to finish their current tasks and then cancel the whole plan?\nYes. Either that, or the tasks need to check whether the plan has been canceled before doing anything else with the `Executor`.\nNice find! I recently reworked the hash join a bit, so this seems to be happening in a new task that I've spawned.\r\n\r\nThe race condition is between the main thread calling `ClientContext::EndQueryInternal`, and another thread that calls `event->FinishTask()` at `physical_hash_join.cpp:322`. The object of the race condition is the `Executor` that is responsible for finishing the query. The main thread is ending the query while another thread is still working on it.\r\n\r\nThe query does not finish nicely, as it yields this error:\r\n```\r\nError: Serialization Error: currval: sequence is not yet defined in this session\r\n```\r\n\r\nSo I think the issue here is not with the hash join, but rather with the main thread ending the query early due to the error (and deleting the `Executor` while doing so), while other threads may still be working on tasks, which depend on the `Executor`.\nThanks for the explanation! So the solution would be for the main thread to wait for other threads to finish their current tasks and then cancel the whole plan?\nYes. Either that, or the tasks need to check whether the plan has been canceled before doing anything else with the `Executor`.",
  "created_at": "2022-09-15T08:03:47Z"
}