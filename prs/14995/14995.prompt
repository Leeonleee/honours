You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
WHERE clause ignores Decimal comparison when using Polars DataFrame
### What happens?

When filtering a Polars DataFrame containing Decimal values using `WHERE column > value`, DuckDB includes rows that fail the comparison, but this only happens with Polars DataFrames - not with Parquet files or DuckDB tables containing identical data.

### To Reproduce

```python
import duckdb
import polars as pl
import numpy as np
from pathlib import Path
import tempfile

np.random.seed(10)

with tempfile.TemporaryDirectory() as tmp_dir:
    path = Path(tmp_dir) / "test.parquet"
    pl.DataFrame({'x': pl.Series(np.random.uniform(-10, 10, 1000)).cast(pl.Decimal(18, 4))}).write_parquet(path, use_pyarrow=True)
    
    conn = duckdb.connect()
    conn.sql(f"CREATE TABLE x_table AS SELECT * FROM '{path}'")

    df_pl = pl.read_parquet(path)

    query = """
    SELECT x, x > 0.05 AS is_x_good, x::FLOAT > 0.05 AS is_float_x_good
    FROM {}
    WHERE is_x_good
    ORDER BY x ASC
    """
    print("DuckDB table:\n", conn.sql(query.format('x_table')), sep='')
    print("Parquet from DuckDB:\n", conn.sql(query.format(f"read_parquet('{path}')")), sep='')
    print("Polars df from DuckDB:\n", conn.sql(query.format("df_pl")), sep='')

    query_cte = """
    WITH base AS(
        SELECT x, x > 0.05 AS is_x_good, x::FLOAT > 0.05 AS is_float_x_good
        FROM df_pl
    )
    SELECT *
    FROM base
    WHERE is_x_good
    ORDER BY x ASC
    """
    print("Polars df from DuckDB cte:\n", conn.sql(query_cte), sep='')
```

#### Expected behavior
Querying the Polars DataFrame should return the same results as querying the Parquet file or DuckDB table directly - rows where `x > 0.05` evaluates to `true`.

#### Actual behavior

Full output showing the discrepancy:

```
DuckDB table:
┌───────────────┬───────────┬─────────────────┐
│       x       │ is_x_good │ is_float_x_good │
│ decimal(18,4) │  boolean  │     boolean     │
├───────────────┼───────────┼─────────────────┤
│        0.0533 │ true      │ true            │
│        0.0714 │ true      │ true            │
│        0.0901 │ true      │ true            │
│        0.0968 │ true      │ true            │
│        0.1011 │ true      │ true            │
│        0.1395 │ true      │ true            │
│        0.1520 │ true      │ true            │
│        0.1867 │ true      │ true            │
│        0.2066 │ true      │ true            │
│        0.2126 │ true      │ true            │
│           ·   │  ·        │  ·              │
│           ·   │  ·        │  ·              │
│           ·   │  ·        │  ·              │
│        9.7783 │ true      │ true            │
│        9.8024 │ true      │ true            │
│        9.8033 │ true      │ true            │
│        9.8187 │ true      │ true            │
│        9.8965 │ true      │ true            │
│        9.9051 │ true      │ true            │
│        9.9246 │ true      │ true            │
│        9.9409 │ true      │ true            │
│        9.9615 │ true      │ true            │
│        9.9727 │ true      │ true            │
├───────────────┴───────────┴─────────────────┤
│ 495 rows (20 shown)               3 columns │
└─────────────────────────────────────────────┘

Parquet from DuckDB:
┌───────────────┬───────────┬─────────────────┐
│       x       │ is_x_good │ is_float_x_good │
│ decimal(18,4) │  boolean  │     boolean     │
├───────────────┼───────────┼─────────────────┤
│        0.0533 │ true      │ true            │
│        0.0714 │ true      │ true            │
│        0.0901 │ true      │ true            │
│        0.0968 │ true      │ true            │
│        0.1011 │ true      │ true            │
│        0.1395 │ true      │ true            │
│        0.1520 │ true      │ true            │
│        0.1867 │ true      │ true            │
│        0.2066 │ true      │ true            │
│        0.2126 │ true      │ true            │
│           ·   │  ·        │  ·              │
│           ·   │  ·        │  ·              │
│           ·   │  ·        │  ·              │
│        9.7783 │ true      │ true            │
│        9.8024 │ true      │ true            │
│        9.8033 │ true      │ true            │
│        9.8187 │ true      │ true            │
│        9.8965 │ true      │ true            │
│        9.9051 │ true      │ true            │
│        9.9246 │ true      │ true            │
│        9.9409 │ true      │ true            │
│        9.9615 │ true      │ true            │
│        9.9727 │ true      │ true            │
├───────────────┴───────────┴─────────────────┤
│ 495 rows (20 shown)               3 columns │
└─────────────────────────────────────────────┘

Polars df from DuckDB:
┌───────────────┬───────────┬─────────────────┐
│       x       │ is_x_good │ is_float_x_good │
│ decimal(18,4) │  boolean  │     boolean     │
├───────────────┼───────────┼─────────────────┤
│        0.0349 │ false     │ false           │  <- Should not appear
│        0.0404 │ false     │ false           │  <- Should not appear
│        0.0533 │ true      │ true            │
│        0.0714 │ true      │ true            │
│        0.0901 │ true      │ true            │
│        0.0968 │ true      │ true            │
│        0.1011 │ true      │ true            │
│        0.1395 │ true      │ true            │
│        0.1520 │ true      │ true            │
│        0.1867 │ true      │ true            │
│           ·   │  ·        │  ·              │
│           ·   │  ·        │  ·              │
│           ·   │  ·        │  ·              │
│        9.7783 │ true      │ true            │
│        9.8024 │ true      │ true            │
│        9.8033 │ true      │ true            │
│        9.8187 │ true      │ true            │
│        9.8965 │ true      │ true            │
│        9.9051 │ true      │ true            │
│        9.9246 │ true      │ true            │
│        9.9409 │ true      │ true            │
│        9.9615 │ true      │ true            │
│        9.9727 │ true      │ true            │
├───────────────┴───────────┴─────────────────┤
│ 497 rows (20 shown)               3 columns │
└─────────────────────────────────────────────┘

Polars df from DuckDB cte:
┌───────────────┬───────────┬─────────────────┐
│       x       │ is_x_good │ is_float_x_good │
│ decimal(18,4) │  boolean  │     boolean     │
├───────────────┼───────────┼─────────────────┤
│        0.0349 │ false     │ false           │  <- Should not appear
│        0.0404 │ false     │ false           │  <- Should not appear
│        0.0533 │ true      │ true            │
│        0.0714 │ true      │ true            │
│        0.0901 │ true      │ true            │
│        0.0968 │ true      │ true            │
│        0.1011 │ true      │ true            │
│        0.1395 │ true      │ true            │
│        0.1520 │ true      │ true            │
│        0.1867 │ true      │ true            │
│           ·   │  ·        │  ·              │
│           ·   │  ·        │  ·              │
│           ·   │  ·        │  ·              │
│        9.7783 │ true      │ true            │
│        9.8024 │ true      │ true            │
│        9.8033 │ true      │ true            │
│        9.8187 │ true      │ true            │
│        9.8965 │ true      │ true            │
│        9.9051 │ true      │ true            │
│        9.9246 │ true      │ true            │
│        9.9409 │ true      │ true            │
│        9.9615 │ true      │ true            │
│        9.9727 │ true      │ true            │
├───────────────┴───────────┴─────────────────┤
│ 497 rows (20 shown)               3 columns │
└─────────────────────────────────────────────┘
```

When querying the Polars DataFrame, rows where both `is_x_good` and `is_float_x_good` are `false` appear in the results, despite the `WHERE is_x_good` clause. This doesn't happen when querying the Parquet file or DuckDB table directly.

Interestingly, filtering on `WHERE is_float_x_good` (casting to FLOAT first) produces the expected results.

DuckDB 1.1.1
Polars 1.7.1


### OS:

Linux

### DuckDB Version:

1.1.1

### DuckDB Client:

Python

### Hardware:

_No response_

### Full Name:

Pavel Khokhlov

### Affiliation:

personal

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of tools/pythonpkg/duckdb_extension_config.cmake]
1: ################################################################################
2: # Python DuckDB extension config
3: ################################################################################
4: #
5: # This is the default extension configuration for Python builds. Basically it means that all these extensions are
6: # "baked in" to the python binaries. Note that the configuration here is only when building Python using the main
7: # CMakeLists.txt file with the `BUILD_PYTHON` variable.
8: # TODO: unify this by making setup.py also use this configuration, making this the config for all python builds
9: duckdb_extension_load(json)
10: duckdb_extension_load(fts)
11: duckdb_extension_load(tpcds)
12: duckdb_extension_load(tpch)
13: duckdb_extension_load(parquet)
14: duckdb_extension_load(icu)
[end of tools/pythonpkg/duckdb_extension_config.cmake]
[start of tools/pythonpkg/src/arrow/arrow_array_stream.cpp]
1: #include "duckdb_python/arrow/arrow_array_stream.hpp"
2: 
3: #include "duckdb/common/assert.hpp"
4: #include "duckdb/common/common.hpp"
5: #include "duckdb/common/limits.hpp"
6: #include "duckdb/main/client_config.hpp"
7: #include "duckdb/planner/filter/conjunction_filter.hpp"
8: #include "duckdb/planner/filter/constant_filter.hpp"
9: #include "duckdb/planner/filter/struct_filter.hpp"
10: #include "duckdb/planner/table_filter.hpp"
11: 
12: #include "duckdb_python/pyconnection/pyconnection.hpp"
13: #include "duckdb_python/pyrelation.hpp"
14: #include "duckdb_python/pyresult.hpp"
15: #include "duckdb/function/table/arrow.hpp"
16: 
17: namespace duckdb {
18: 
19: void TransformDuckToArrowChunk(ArrowSchema &arrow_schema, ArrowArray &data, py::list &batches) {
20: 	py::gil_assert();
21: 	auto pyarrow_lib_module = py::module::import("pyarrow").attr("lib");
22: 	auto batch_import_func = pyarrow_lib_module.attr("RecordBatch").attr("_import_from_c");
23: 	batches.append(batch_import_func(reinterpret_cast<uint64_t>(&data), reinterpret_cast<uint64_t>(&arrow_schema)));
24: }
25: 
26: void VerifyArrowDatasetLoaded() {
27: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
28: 	if (!import_cache.pyarrow.dataset() || !ModuleIsLoaded<PyarrowDatasetCacheItem>()) {
29: 		throw InvalidInputException("Optional module 'pyarrow.dataset' is required to perform this action");
30: 	}
31: }
32: 
33: py::object PythonTableArrowArrayStreamFactory::ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,
34:                                                               ArrowStreamParameters &parameters,
35:                                                               const ClientProperties &client_properties) {
36: 	D_ASSERT(!py::isinstance<py::capsule>(arrow_obj_handle));
37: 	ArrowSchemaWrapper schema;
38: 	PythonTableArrowArrayStreamFactory::GetSchemaInternal(arrow_obj_handle, schema);
39: 	vector<string> unused_names;
40: 	vector<LogicalType> unused_types;
41: 	ArrowTableType arrow_table;
42: 	ArrowTableFunction::PopulateArrowTableType(arrow_table, schema, unused_names, unused_types);
43: 
44: 	auto filters = parameters.filters;
45: 	auto &column_list = parameters.projected_columns.columns;
46: 	auto &filter_to_col = parameters.projected_columns.filter_to_col;
47: 	py::list projection_list = py::cast(column_list);
48: 
49: 	bool has_filter = filters && !filters->filters.empty();
50: 
51: 	if (has_filter) {
52: 		auto filter = TransformFilter(*filters, parameters.projected_columns.projection_map, filter_to_col,
53: 		                              client_properties, arrow_table);
54: 		if (column_list.empty()) {
55: 			return arrow_scanner(arrow_obj_handle, py::arg("filter") = filter);
56: 		} else {
57: 			return arrow_scanner(arrow_obj_handle, py::arg("columns") = projection_list, py::arg("filter") = filter);
58: 		}
59: 	} else {
60: 		if (column_list.empty()) {
61: 			return arrow_scanner(arrow_obj_handle);
62: 		} else {
63: 			return arrow_scanner(arrow_obj_handle, py::arg("columns") = projection_list);
64: 		}
65: 	}
66: }
67: unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(uintptr_t factory_ptr,
68:                                                                                 ArrowStreamParameters &parameters) {
69: 	py::gil_scoped_acquire acquire;
70: 	auto factory = static_cast<PythonTableArrowArrayStreamFactory *>(reinterpret_cast<void *>(factory_ptr)); // NOLINT
71: 	D_ASSERT(factory->arrow_object);
72: 	py::handle arrow_obj_handle(factory->arrow_object);
73: 	auto arrow_object_type = DuckDBPyConnection::GetArrowType(arrow_obj_handle);
74: 
75: 	if (arrow_object_type == PyArrowObjectType::PyCapsule) {
76: 		auto res = make_uniq<ArrowArrayStreamWrapper>();
77: 		auto capsule = py::reinterpret_borrow<py::capsule>(arrow_obj_handle);
78: 		auto stream = capsule.get_pointer<struct ArrowArrayStream>();
79: 		if (!stream->release) {
80: 			throw InternalException("ArrowArrayStream was released by another thread/library");
81: 		}
82: 		res->arrow_array_stream = *stream;
83: 		stream->release = nullptr;
84: 		return res;
85: 	}
86: 
87: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
88: 	py::object scanner;
89: 	py::object arrow_batch_scanner = import_cache.pyarrow.dataset.Scanner().attr("from_batches");
90: 	switch (arrow_object_type) {
91: 	case PyArrowObjectType::Table: {
92: 		auto arrow_dataset = import_cache.pyarrow.dataset().attr("dataset");
93: 		auto dataset = arrow_dataset(arrow_obj_handle);
94: 		py::object arrow_scanner = dataset.attr("__class__").attr("scanner");
95: 		scanner = ProduceScanner(arrow_scanner, dataset, parameters, factory->client_properties);
96: 		break;
97: 	}
98: 	case PyArrowObjectType::RecordBatchReader: {
99: 		scanner = ProduceScanner(arrow_batch_scanner, arrow_obj_handle, parameters, factory->client_properties);
100: 		break;
101: 	}
102: 	case PyArrowObjectType::Scanner: {
103: 		// If it's a scanner we have to turn it to a record batch reader, and then a scanner again since we can't stack
104: 		// scanners on arrow Otherwise pushed-down projections and filters will disappear like tears in the rain
105: 		auto record_batches = arrow_obj_handle.attr("to_reader")();
106: 		scanner = ProduceScanner(arrow_batch_scanner, record_batches, parameters, factory->client_properties);
107: 		break;
108: 	}
109: 	case PyArrowObjectType::Dataset: {
110: 		py::object arrow_scanner = arrow_obj_handle.attr("__class__").attr("scanner");
111: 		scanner = ProduceScanner(arrow_scanner, arrow_obj_handle, parameters, factory->client_properties);
112: 		break;
113: 	}
114: 	default: {
115: 		auto py_object_type = string(py::str(arrow_obj_handle.get_type().attr("__name__")));
116: 		throw InvalidInputException("Object of type '%s' is not a recognized Arrow object", py_object_type);
117: 	}
118: 	}
119: 
120: 	auto record_batches = scanner.attr("to_reader")();
121: 	auto res = make_uniq<ArrowArrayStreamWrapper>();
122: 	auto export_to_c = record_batches.attr("_export_to_c");
123: 	export_to_c(reinterpret_cast<uint64_t>(&res->arrow_array_stream));
124: 	return res;
125: }
126: 
127: void PythonTableArrowArrayStreamFactory::GetSchemaInternal(py::handle arrow_obj_handle, ArrowSchemaWrapper &schema) {
128: 	if (py::isinstance<py::capsule>(arrow_obj_handle)) {
129: 		auto capsule = py::reinterpret_borrow<py::capsule>(arrow_obj_handle);
130: 		auto stream = capsule.get_pointer<struct ArrowArrayStream>();
131: 		if (!stream->release) {
132: 			throw InternalException("ArrowArrayStream was released by another thread/library");
133: 		}
134: 		stream->get_schema(stream, &schema.arrow_schema);
135: 		return;
136: 	}
137: 
138: 	auto table_class = py::module::import("pyarrow").attr("Table");
139: 	if (py::isinstance(arrow_obj_handle, table_class)) {
140: 		auto obj_schema = arrow_obj_handle.attr("schema");
141: 		auto export_to_c = obj_schema.attr("_export_to_c");
142: 		export_to_c(reinterpret_cast<uint64_t>(&schema.arrow_schema));
143: 		return;
144: 	}
145: 
146: 	VerifyArrowDatasetLoaded();
147: 
148: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
149: 	auto scanner_class = import_cache.pyarrow.dataset.Scanner();
150: 
151: 	if (py::isinstance(arrow_obj_handle, scanner_class)) {
152: 		auto obj_schema = arrow_obj_handle.attr("projected_schema");
153: 		auto export_to_c = obj_schema.attr("_export_to_c");
154: 		export_to_c(reinterpret_cast<uint64_t>(&schema));
155: 	} else {
156: 		auto obj_schema = arrow_obj_handle.attr("schema");
157: 		auto export_to_c = obj_schema.attr("_export_to_c");
158: 		export_to_c(reinterpret_cast<uint64_t>(&schema));
159: 	}
160: }
161: 
162: void PythonTableArrowArrayStreamFactory::GetSchema(uintptr_t factory_ptr, ArrowSchemaWrapper &schema) {
163: 	py::gil_scoped_acquire acquire;
164: 	auto factory = static_cast<PythonTableArrowArrayStreamFactory *>(reinterpret_cast<void *>(factory_ptr)); // NOLINT
165: 	D_ASSERT(factory->arrow_object);
166: 	py::handle arrow_obj_handle(factory->arrow_object);
167: 	GetSchemaInternal(arrow_obj_handle, schema);
168: }
169: 
170: string ConvertTimestampUnit(ArrowDateTimeType unit) {
171: 	switch (unit) {
172: 	case ArrowDateTimeType::MICROSECONDS:
173: 		return "us";
174: 	case ArrowDateTimeType::MILLISECONDS:
175: 		return "ms";
176: 	case ArrowDateTimeType::NANOSECONDS:
177: 		return "ns";
178: 	case ArrowDateTimeType::SECONDS:
179: 		return "s";
180: 	default:
181: 		throw NotImplementedException("DatetimeType not recognized in ConvertTimestampUnit: %d", (int)unit);
182: 	}
183: }
184: 
185: int64_t ConvertTimestampTZValue(int64_t base_value, ArrowDateTimeType datetime_type) {
186: 	auto input = timestamp_t(base_value);
187: 	if (!Timestamp::IsFinite(input)) {
188: 		return base_value;
189: 	}
190: 
191: 	switch (datetime_type) {
192: 	case ArrowDateTimeType::MICROSECONDS:
193: 		return Timestamp::GetEpochMicroSeconds(input);
194: 	case ArrowDateTimeType::MILLISECONDS:
195: 		return Timestamp::GetEpochMs(input);
196: 	case ArrowDateTimeType::NANOSECONDS:
197: 		return Timestamp::GetEpochNanoSeconds(input);
198: 	case ArrowDateTimeType::SECONDS:
199: 		return Timestamp::GetEpochSeconds(input);
200: 	default:
201: 		throw NotImplementedException("DatetimeType not recognized in ConvertTimestampTZValue");
202: 	}
203: }
204: 
205: py::object GetScalar(Value &constant, const string &timezone_config, const ArrowType &type) {
206: 	py::object scalar = py::module_::import("pyarrow").attr("scalar");
207: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
208: 	py::object dataset_scalar = import_cache.pyarrow.dataset().attr("scalar");
209: 	py::object scalar_value;
210: 	switch (constant.type().id()) {
211: 	case LogicalTypeId::BOOLEAN:
212: 		return dataset_scalar(constant.GetValue<bool>());
213: 	case LogicalTypeId::TINYINT:
214: 		return dataset_scalar(constant.GetValue<int8_t>());
215: 	case LogicalTypeId::SMALLINT:
216: 		return dataset_scalar(constant.GetValue<int16_t>());
217: 	case LogicalTypeId::INTEGER:
218: 		return dataset_scalar(constant.GetValue<int32_t>());
219: 	case LogicalTypeId::BIGINT:
220: 		return dataset_scalar(constant.GetValue<int64_t>());
221: 	case LogicalTypeId::DATE: {
222: 		py::object date_type = py::module_::import("pyarrow").attr("date32");
223: 		return dataset_scalar(scalar(constant.GetValue<int32_t>(), date_type()));
224: 	}
225: 	case LogicalTypeId::TIME: {
226: 		py::object date_type = py::module_::import("pyarrow").attr("time64");
227: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("us")));
228: 	}
229: 	case LogicalTypeId::TIMESTAMP: {
230: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
231: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("us")));
232: 	}
233: 	case LogicalTypeId::TIMESTAMP_MS: {
234: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
235: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("ms")));
236: 	}
237: 	case LogicalTypeId::TIMESTAMP_NS: {
238: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
239: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("ns")));
240: 	}
241: 	case LogicalTypeId::TIMESTAMP_SEC: {
242: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
243: 		return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type("s")));
244: 	}
245: 	case LogicalTypeId::TIMESTAMP_TZ: {
246: 		auto &datetime_info = type.GetTypeInfo<ArrowDateTimeInfo>();
247: 		auto base_value = constant.GetValue<int64_t>();
248: 		auto arrow_datetime_type = datetime_info.GetDateTimeType();
249: 		auto time_unit_string = ConvertTimestampUnit(arrow_datetime_type);
250: 		auto converted_value = ConvertTimestampTZValue(base_value, arrow_datetime_type);
251: 		py::object date_type = py::module_::import("pyarrow").attr("timestamp");
252: 		return dataset_scalar(scalar(converted_value, date_type(time_unit_string, py::arg("tz") = timezone_config)));
253: 	}
254: 	case LogicalTypeId::UTINYINT: {
255: 		py::object integer_type = py::module_::import("pyarrow").attr("uint8");
256: 		return dataset_scalar(scalar(constant.GetValue<uint8_t>(), integer_type()));
257: 	}
258: 	case LogicalTypeId::USMALLINT: {
259: 		py::object integer_type = py::module_::import("pyarrow").attr("uint16");
260: 		return dataset_scalar(scalar(constant.GetValue<uint16_t>(), integer_type()));
261: 	}
262: 	case LogicalTypeId::UINTEGER: {
263: 		py::object integer_type = py::module_::import("pyarrow").attr("uint32");
264: 		return dataset_scalar(scalar(constant.GetValue<uint32_t>(), integer_type()));
265: 	}
266: 	case LogicalTypeId::UBIGINT: {
267: 		py::object integer_type = py::module_::import("pyarrow").attr("uint64");
268: 		return dataset_scalar(scalar(constant.GetValue<uint64_t>(), integer_type()));
269: 	}
270: 	case LogicalTypeId::FLOAT:
271: 		return dataset_scalar(constant.GetValue<float>());
272: 	case LogicalTypeId::DOUBLE:
273: 		return dataset_scalar(constant.GetValue<double>());
274: 	case LogicalTypeId::VARCHAR:
275: 		return dataset_scalar(constant.ToString());
276: 	case LogicalTypeId::BLOB:
277: 		return dataset_scalar(py::bytes(constant.GetValueUnsafe<string>()));
278: 	case LogicalTypeId::DECIMAL: {
279: 		py::object date_type = py::module_::import("pyarrow").attr("decimal128");
280: 		uint8_t width;
281: 		uint8_t scale;
282: 		constant.type().GetDecimalProperties(width, scale);
283: 		switch (constant.type().InternalType()) {
284: 		case PhysicalType::INT16:
285: 			return dataset_scalar(scalar(constant.GetValue<int16_t>(), date_type(width, scale)));
286: 		case PhysicalType::INT32:
287: 			return dataset_scalar(scalar(constant.GetValue<int32_t>(), date_type(width, scale)));
288: 		case PhysicalType::INT64:
289: 			return dataset_scalar(scalar(constant.GetValue<int64_t>(), date_type(width, scale)));
290: 		default: {
291: 			auto hugeint_value = constant.GetValue<hugeint_t>();
292: 			auto hugeint_value_py = py::cast(hugeint_value.upper);
293: 			hugeint_value_py = hugeint_value_py.attr("__mul__")(NumericLimits<uint64_t>::Maximum());
294: 			hugeint_value_py = hugeint_value_py.attr("__add__")(hugeint_value.lower);
295: 			return dataset_scalar(scalar(hugeint_value_py, date_type(width, scale)));
296: 		}
297: 		}
298: 	}
299: 	default:
300: 		throw NotImplementedException("Unimplemented type \"%s\" for Arrow Filter Pushdown",
301: 		                              constant.type().ToString());
302: 	}
303: }
304: 
305: py::object TransformFilterRecursive(TableFilter &filter, vector<string> column_ref, const string &timezone_config,
306:                                     const ArrowType &type) {
307: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
308: 	py::object field = import_cache.pyarrow.dataset().attr("field");
309: 	switch (filter.filter_type) {
310: 	case TableFilterType::CONSTANT_COMPARISON: {
311: 		auto &constant_filter = filter.Cast<ConstantFilter>();
312: 		auto constant_field = field(py::tuple(py::cast(column_ref)));
313: 		auto constant_value = GetScalar(constant_filter.constant, timezone_config, type);
314: 		switch (constant_filter.comparison_type) {
315: 		case ExpressionType::COMPARE_EQUAL: {
316: 			return constant_field.attr("__eq__")(constant_value);
317: 		}
318: 		case ExpressionType::COMPARE_LESSTHAN: {
319: 			return constant_field.attr("__lt__")(constant_value);
320: 		}
321: 		case ExpressionType::COMPARE_GREATERTHAN: {
322: 			return constant_field.attr("__gt__")(constant_value);
323: 		}
324: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO: {
325: 			return constant_field.attr("__le__")(constant_value);
326: 		}
327: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO: {
328: 			return constant_field.attr("__ge__")(constant_value);
329: 		}
330: 		default:
331: 			throw NotImplementedException("Comparison Type can't be an Arrow Scan Pushdown Filter");
332: 		}
333: 	}
334: 	//! We do not pushdown is null yet
335: 	case TableFilterType::IS_NULL: {
336: 		auto constant_field = field(py::tuple(py::cast(column_ref)));
337: 		return constant_field.attr("is_null")();
338: 	}
339: 	case TableFilterType::IS_NOT_NULL: {
340: 		auto constant_field = field(py::tuple(py::cast(column_ref)));
341: 		return constant_field.attr("is_valid")();
342: 	}
343: 	//! We do not pushdown or conjunctions yet
344: 	case TableFilterType::CONJUNCTION_OR: {
345: 		auto &or_filter = filter.Cast<ConjunctionOrFilter>();
346: 		py::object expression = py::none();
347: 		for (idx_t i = 0; i < or_filter.child_filters.size(); i++) {
348: 			auto &child_filter = *or_filter.child_filters[i];
349: 			py::object child_expression = TransformFilterRecursive(child_filter, column_ref, timezone_config, type);
350: 			if (expression.is(py::none())) {
351: 				expression = std::move(child_expression);
352: 			} else {
353: 				expression = expression.attr("__or__")(child_expression);
354: 			}
355: 		}
356: 		return expression;
357: 	}
358: 	case TableFilterType::CONJUNCTION_AND: {
359: 		auto &and_filter = filter.Cast<ConjunctionAndFilter>();
360: 		py::object expression = py::none();
361: 		for (idx_t i = 0; i < and_filter.child_filters.size(); i++) {
362: 			auto &child_filter = *and_filter.child_filters[i];
363: 			py::object child_expression = TransformFilterRecursive(child_filter, column_ref, timezone_config, type);
364: 			if (expression.is(py::none())) {
365: 				expression = std::move(child_expression);
366: 			} else {
367: 				expression = expression.attr("__and__")(child_expression);
368: 			}
369: 		}
370: 		return expression;
371: 	}
372: 	case TableFilterType::STRUCT_EXTRACT: {
373: 		auto &struct_filter = filter.Cast<StructFilter>();
374: 		auto &child_name = struct_filter.child_name;
375: 		auto &struct_type_info = type.GetTypeInfo<ArrowStructInfo>();
376: 		auto &struct_child_type = struct_type_info.GetChild(struct_filter.child_idx);
377: 
378: 		column_ref.push_back(child_name);
379: 		auto child_expr = TransformFilterRecursive(*struct_filter.child_filter, std::move(column_ref), timezone_config,
380: 		                                           struct_child_type);
381: 		return child_expr;
382: 	}
383: 	case TableFilterType::OPTIONAL_FILTER:
384: 		return py::none();
385: 	default:
386: 		throw NotImplementedException("Pushdown Filter Type not supported in Arrow Scans");
387: 	}
388: }
389: 
390: py::object PythonTableArrowArrayStreamFactory::TransformFilter(TableFilterSet &filter_collection,
391:                                                                std::unordered_map<idx_t, string> &columns,
392:                                                                unordered_map<idx_t, idx_t> filter_to_col,
393:                                                                const ClientProperties &config,
394:                                                                const ArrowTableType &arrow_table) {
395: 	auto &filters_map = filter_collection.filters;
396: 
397: 	py::object expression = py::none();
398: 	for (auto &it : filters_map) {
399: 		auto column_idx = it.first;
400: 		auto &column_name = columns[column_idx];
401: 
402: 		vector<string> column_ref;
403: 		column_ref.push_back(column_name);
404: 
405: 		D_ASSERT(columns.find(column_idx) != columns.end());
406: 
407: 		auto &arrow_type = arrow_table.GetColumns().at(filter_to_col.at(column_idx));
408: 		py::object child_expression = TransformFilterRecursive(*it.second, column_ref, config.time_zone, *arrow_type);
409: 		if (child_expression.is(py::none())) {
410: 			continue;
411: 		} else if (expression.is(py::none())) {
412: 			expression = std::move(child_expression);
413: 		} else {
414: 			expression = expression.attr("__and__")(child_expression);
415: 		}
416: 	}
417: 	return expression;
418: }
419: 
420: } // namespace duckdb
[end of tools/pythonpkg/src/arrow/arrow_array_stream.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: