You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
suggest read the parquet file name case non sense
if I have a a.Parquet file. 
select * from 'a.Parquet' failed. 
I must rename it to a.parquet now. 

suggest read the parquet file name case non sense
if I have a a.Parquet file. 
select * from 'a.Parquet' failed. 
I must rename it to a.parquet now. 


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of extension/parquet/parquet-extension.cpp]
1: #include <string>
2: #include <vector>
3: #include <fstream>
4: #include <iostream>
5: 
6: #include "parquet-extension.hpp"
7: #include "parquet_reader.hpp"
8: #include "parquet_writer.hpp"
9: #include "parquet_metadata.hpp"
10: 
11: #include "duckdb.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb.hpp"
14: #include "duckdb/common/types/chunk_collection.hpp"
15: #include "duckdb/function/copy_function.hpp"
16: #include "duckdb/function/table_function.hpp"
17: #include "duckdb/common/file_system.hpp"
18: #include "duckdb/parallel/parallel_state.hpp"
19: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
20: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
21: 
22: #include "duckdb/main/config.hpp"
23: #include "duckdb/parser/expression/constant_expression.hpp"
24: #include "duckdb/parser/expression/function_expression.hpp"
25: #include "duckdb/parser/tableref/table_function_ref.hpp"
26: 
27: #include "duckdb/storage/statistics/base_statistics.hpp"
28: 
29: #include "duckdb/main/client_context.hpp"
30: #include "duckdb/catalog/catalog.hpp"
31: #endif
32: 
33: namespace duckdb {
34: 
35: struct ParquetReadBindData : public FunctionData {
36: 	shared_ptr<ParquetReader> initial_reader;
37: 	vector<string> files;
38: 	vector<column_t> column_ids;
39: 	atomic<idx_t> chunk_count;
40: 	atomic<idx_t> cur_file;
41: };
42: 
43: struct ParquetReadOperatorData : public FunctionOperatorData {
44: 	shared_ptr<ParquetReader> reader;
45: 	ParquetReaderScanState scan_state;
46: 	bool is_parallel;
47: 	idx_t file_index;
48: 	vector<column_t> column_ids;
49: 	TableFilterSet *table_filters;
50: };
51: 
52: struct ParquetReadParallelState : public ParallelState {
53: 	mutex lock;
54: 	shared_ptr<ParquetReader> current_reader;
55: 	idx_t file_index;
56: 	idx_t row_group_index;
57: };
58: 
59: class ParquetScanFunction {
60: public:
61: 	static TableFunctionSet GetFunctionSet() {
62: 		TableFunctionSet set("parquet_scan");
63: 		auto table_function =
64: 		    TableFunction({LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind, ParquetScanInit,
65: 		                  /* statistics */ ParquetScanStats, /* cleanup */ nullptr,
66: 		                  /* dependency */ nullptr, ParquetCardinality,
67: 		                  /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, ParquetScanMaxThreads,
68: 		                  ParquetInitParallelState, ParquetScanFuncParallel, ParquetScanParallelInit,
69: 		                  ParquetParallelStateNext, true, true, ParquetProgress);
70: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
71: 		set.AddFunction(table_function);
72: 		table_function = TableFunction({LogicalType::LIST(LogicalType::VARCHAR)}, ParquetScanImplementation,
73: 		                               ParquetScanBindList, ParquetScanInit, /* statistics */ ParquetScanStats,
74: 		                               /* cleanup */ nullptr,
75: 		                               /* dependency */ nullptr, ParquetCardinality,
76: 		                               /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr,
77: 		                               ParquetScanMaxThreads, ParquetInitParallelState, ParquetScanFuncParallel,
78: 		                               ParquetScanParallelInit, ParquetParallelStateNext, true, true, ParquetProgress);
79: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
80: 		set.AddFunction(table_function);
81: 		return set;
82: 	}
83: 
84: 	static unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,
85: 	                                                vector<string> &expected_names,
86: 	                                                vector<LogicalType> &expected_types) {
87: 		for (auto &option : info.options) {
88: 			auto loption = StringUtil::Lower(option.first);
89: 			if (loption == "compression" || loption == "codec") {
90: 				// CODEC option has no effect on parquet read: we determine codec from the file
91: 				continue;
92: 			} else {
93: 				throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
94: 			}
95: 		}
96: 		auto result = make_unique<ParquetReadBindData>();
97: 
98: 		FileSystem &fs = FileSystem::GetFileSystem(context);
99: 		result->files = fs.Glob(info.file_path);
100: 		if (result->files.empty()) {
101: 			throw IOException("No files found that match the pattern \"%s\"", info.file_path);
102: 		}
103: 		ParquetOptions parquet_options;
104: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types, parquet_options);
105: 		return move(result);
106: 	}
107: 
108: 	static unique_ptr<BaseStatistics> ParquetScanStats(ClientContext &context, const FunctionData *bind_data_p,
109: 	                                                   column_t column_index) {
110: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
111: 
112: 		if (column_index == COLUMN_IDENTIFIER_ROW_ID) {
113: 			return nullptr;
114: 		}
115: 
116: 		// we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
117: 
118: 		// We already parsed the metadata for the first file in a glob because we need some type info.
119: 		auto overall_stats = ParquetReader::ReadStatistics(
120: 		    *bind_data.initial_reader, bind_data.initial_reader->return_types[column_index], column_index,
121: 		    bind_data.initial_reader->metadata->metadata.get());
122: 
123: 		if (!overall_stats) {
124: 			return nullptr;
125: 		}
126: 
127: 		// if there is only one file in the glob (quite common case), we are done
128: 		auto &config = DBConfig::GetConfig(context);
129: 		if (bind_data.files.size() < 2) {
130: 			return overall_stats;
131: 		} else if (config.object_cache_enable) {
132: 			auto &cache = ObjectCache::GetObjectCache(context);
133: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
134: 			// enabled at all)
135: 			FileSystem &fs = FileSystem::GetFileSystem(context);
136: 			for (idx_t file_idx = 1; file_idx < bind_data.files.size(); file_idx++) {
137: 				auto &file_name = bind_data.files[file_idx];
138: 				auto metadata = std::dynamic_pointer_cast<ParquetFileMetadataCache>(cache.Get(file_name));
139: 				auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
140: 				                          FileSystem::DEFAULT_COMPRESSION, FileSystem::GetFileOpener(context));
141: 				// but we need to check if the metadata cache entries are current
142: 				if (!metadata || (fs.GetLastModifiedTime(*handle) >= metadata->read_time)) {
143: 					// missing or invalid metadata entry in cache, no usable stats overall
144: 					return nullptr;
145: 				}
146: 				// get and merge stats for file
147: 				auto file_stats = ParquetReader::ReadStatistics(*bind_data.initial_reader,
148: 				                                                bind_data.initial_reader->return_types[column_index],
149: 				                                                column_index, metadata->metadata.get());
150: 				if (!file_stats) {
151: 					return nullptr;
152: 				}
153: 				overall_stats->Merge(*file_stats);
154: 			}
155: 			// success!
156: 			return overall_stats;
157: 		}
158: 		// we have more than one file and no object cache so no statistics overall
159: 		return nullptr;
160: 	}
161: 
162: 	static void ParquetScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
163: 	                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
164: 	                                    ParallelState *parallel_state_p) {
165: 		//! FIXME: Have specialized parallel function from pandas scan here
166: 		ParquetScanImplementation(context, bind_data, operator_state, input, output);
167: 	}
168: 
169: 	static unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context, vector<string> files,
170: 	                                                        vector<LogicalType> &return_types, vector<string> &names,
171: 	                                                        ParquetOptions parquet_options) {
172: 		auto result = make_unique<ParquetReadBindData>();
173: 		result->files = move(files);
174: 
175: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], parquet_options);
176: 		return_types = result->initial_reader->return_types;
177: 
178: 		names = result->initial_reader->names;
179: 		return move(result);
180: 	}
181: 
182: 	static vector<string> ParquetGlob(FileSystem &fs, const string &glob) {
183: 		auto files = fs.Glob(glob);
184: 		if (files.empty()) {
185: 			throw IOException("No files found that match the pattern \"%s\"", glob);
186: 		}
187: 		return files;
188: 	}
189: 
190: 	static unique_ptr<FunctionData> ParquetScanBind(ClientContext &context, vector<Value> &inputs,
191: 	                                                unordered_map<string, Value> &named_parameters,
192: 	                                                vector<LogicalType> &input_table_types,
193: 	                                                vector<string> &input_table_names,
194: 	                                                vector<LogicalType> &return_types, vector<string> &names) {
195: 		auto file_name = inputs[0].GetValue<string>();
196: 		ParquetOptions parquet_options;
197: 		for (auto &kv : named_parameters) {
198: 			if (kv.first == "binary_as_string") {
199: 				parquet_options.binary_as_string = kv.second.value_.boolean;
200: 			}
201: 		}
202: 		FileSystem &fs = FileSystem::GetFileSystem(context);
203: 		auto files = ParquetGlob(fs, file_name);
204: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
205: 	}
206: 
207: 	static unique_ptr<FunctionData> ParquetScanBindList(ClientContext &context, vector<Value> &inputs,
208: 	                                                    unordered_map<string, Value> &named_parameters,
209: 	                                                    vector<LogicalType> &input_table_types,
210: 	                                                    vector<string> &input_table_names,
211: 	                                                    vector<LogicalType> &return_types, vector<string> &names) {
212: 		FileSystem &fs = FileSystem::GetFileSystem(context);
213: 		vector<string> files;
214: 		for (auto &val : inputs[0].list_value) {
215: 			auto glob_files = ParquetGlob(fs, val.ToString());
216: 			files.insert(files.end(), glob_files.begin(), glob_files.end());
217: 		}
218: 		if (files.empty()) {
219: 			throw IOException("Parquet reader needs at least one file to read");
220: 		}
221: 		ParquetOptions parquet_options;
222: 		for (auto &kv : named_parameters) {
223: 			if (kv.first == "binary_as_string") {
224: 				parquet_options.binary_as_string = kv.second.value_.boolean;
225: 			}
226: 		}
227: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
228: 	}
229: 
230: 	static unique_ptr<FunctionOperatorData> ParquetScanInit(ClientContext &context, const FunctionData *bind_data_p,
231: 	                                                        const vector<column_t> &column_ids,
232: 	                                                        TableFilterCollection *filters) {
233: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
234: 		bind_data.chunk_count = 0;
235: 		bind_data.cur_file = 0;
236: 		auto result = make_unique<ParquetReadOperatorData>();
237: 		result->column_ids = column_ids;
238: 
239: 		result->is_parallel = false;
240: 		result->file_index = 0;
241: 		result->table_filters = filters->table_filters;
242: 		// single-threaded: one thread has to read all groups
243: 		vector<idx_t> group_ids;
244: 		for (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {
245: 			group_ids.push_back(i);
246: 		}
247: 		result->reader = bind_data.initial_reader;
248: 		result->reader->InitializeScan(result->scan_state, column_ids, move(group_ids), filters->table_filters);
249: 		return move(result);
250: 	}
251: 
252: 	static int ParquetProgress(ClientContext &context, const FunctionData *bind_data_p) {
253: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
254: 		if (bind_data.initial_reader->NumRows() == 0) {
255: 			return (100 * (bind_data.cur_file + 1)) / bind_data.files.size();
256: 		}
257: 		auto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100 / bind_data.initial_reader->NumRows()) /
258: 		                  bind_data.files.size();
259: 		percentage += 100 * bind_data.cur_file / bind_data.files.size();
260: 		return percentage;
261: 	}
262: 
263: 	static unique_ptr<FunctionOperatorData>
264: 	ParquetScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *parallel_state_p,
265: 	                        const vector<column_t> &column_ids, TableFilterCollection *filters) {
266: 		auto result = make_unique<ParquetReadOperatorData>();
267: 		result->column_ids = column_ids;
268: 		result->is_parallel = true;
269: 		result->table_filters = filters->table_filters;
270: 		if (!ParquetParallelStateNext(context, bind_data_p, result.get(), parallel_state_p)) {
271: 			return nullptr;
272: 		}
273: 		return move(result);
274: 	}
275: 
276: 	static void ParquetScanImplementation(ClientContext &context, const FunctionData *bind_data_p,
277: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
278: 		if (!operator_state) {
279: 			return;
280: 		}
281: 		auto &data = (ParquetReadOperatorData &)*operator_state;
282: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
283: 
284: 		do {
285: 			data.reader->Scan(data.scan_state, output);
286: 			bind_data.chunk_count++;
287: 			if (output.size() == 0 && !data.is_parallel) {
288: 				auto &bind_data = (ParquetReadBindData &)*bind_data_p;
289: 				// check if there is another file
290: 				if (data.file_index + 1 < bind_data.files.size()) {
291: 					data.file_index++;
292: 					bind_data.cur_file++;
293: 					bind_data.chunk_count = 0;
294: 					string file = bind_data.files[data.file_index];
295: 					// move to the next file
296: 					data.reader = make_shared<ParquetReader>(context, file, data.reader->return_types,
297: 					                                         data.reader->parquet_options, bind_data.files[0]);
298: 					vector<idx_t> group_ids;
299: 					for (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {
300: 						group_ids.push_back(i);
301: 					}
302: 					data.reader->InitializeScan(data.scan_state, data.column_ids, move(group_ids), data.table_filters);
303: 				} else {
304: 					// exhausted all the files: done
305: 					break;
306: 				}
307: 			} else {
308: 				break;
309: 			}
310: 		} while (true);
311: 	}
312: 
313: 	static unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {
314: 		auto &data = (ParquetReadBindData &)*bind_data;
315: 		return make_unique<NodeStatistics>(data.initial_reader->NumRows() * data.files.size());
316: 	}
317: 
318: 	static idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {
319: 		auto &data = (ParquetReadBindData &)*bind_data;
320: 		return data.initial_reader->NumRowGroups() * data.files.size();
321: 	}
322: 
323: 	static unique_ptr<ParallelState> ParquetInitParallelState(ClientContext &context, const FunctionData *bind_data_p,
324: 	                                                          const vector<column_t> &column_ids,
325: 	                                                          TableFilterCollection *filters) {
326: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
327: 		auto result = make_unique<ParquetReadParallelState>();
328: 		result->current_reader = bind_data.initial_reader;
329: 		result->row_group_index = 0;
330: 		result->file_index = 0;
331: 		return move(result);
332: 	}
333: 
334: 	static bool ParquetParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
335: 	                                     FunctionOperatorData *state_p, ParallelState *parallel_state_p) {
336: 		if (!state_p) {
337: 			return false;
338: 		}
339: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
340: 		auto &parallel_state = (ParquetReadParallelState &)*parallel_state_p;
341: 		auto &scan_data = (ParquetReadOperatorData &)*state_p;
342: 
343: 		lock_guard<mutex> parallel_lock(parallel_state.lock);
344: 		if (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {
345: 			// groups remain in the current parquet file: read the next group
346: 			scan_data.reader = parallel_state.current_reader;
347: 			vector<idx_t> group_indexes {parallel_state.row_group_index};
348: 			scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
349: 			                                 scan_data.table_filters);
350: 			parallel_state.row_group_index++;
351: 			return true;
352: 		} else {
353: 			// no groups remain in the current parquet file: check if there are more files to read
354: 			while (parallel_state.file_index + 1 < bind_data.files.size()) {
355: 				// read the next file
356: 				string file = bind_data.files[++parallel_state.file_index];
357: 				parallel_state.current_reader =
358: 				    make_shared<ParquetReader>(context, file, parallel_state.current_reader->return_types,
359: 				                               parallel_state.current_reader->parquet_options);
360: 				if (parallel_state.current_reader->NumRowGroups() == 0) {
361: 					// empty parquet file, move to next file
362: 					continue;
363: 				}
364: 				// set up the scan state to read the first group
365: 				scan_data.reader = parallel_state.current_reader;
366: 				vector<idx_t> group_indexes {0};
367: 				scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
368: 				                                 scan_data.table_filters);
369: 				parallel_state.row_group_index = 1;
370: 				return true;
371: 			}
372: 		}
373: 		return false;
374: 	}
375: };
376: 
377: struct ParquetWriteBindData : public FunctionData {
378: 	vector<LogicalType> sql_types;
379: 	string file_name;
380: 	vector<string> column_names;
381: 	duckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
382: };
383: 
384: struct ParquetWriteGlobalState : public GlobalFunctionData {
385: 	unique_ptr<ParquetWriter> writer;
386: };
387: 
388: struct ParquetWriteLocalState : public LocalFunctionData {
389: 	ParquetWriteLocalState() {
390: 		buffer = make_unique<ChunkCollection>();
391: 	}
392: 
393: 	unique_ptr<ChunkCollection> buffer;
394: };
395: 
396: unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info, vector<string> &names,
397:                                           vector<LogicalType> &sql_types) {
398: 	auto bind_data = make_unique<ParquetWriteBindData>();
399: 	for (auto &option : info.options) {
400: 		auto loption = StringUtil::Lower(option.first);
401: 		if (loption == "compression" || loption == "codec") {
402: 			if (!option.second.empty()) {
403: 				auto roption = StringUtil::Lower(option.second[0].ToString());
404: 				if (roption == "uncompressed") {
405: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::UNCOMPRESSED;
406: 					continue;
407: 				} else if (roption == "snappy") {
408: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
409: 					continue;
410: 				} else if (roption == "gzip") {
411: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::GZIP;
412: 					continue;
413: 				} else if (roption == "zstd") {
414: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::ZSTD;
415: 					continue;
416: 				}
417: 			}
418: 			throw ParserException("Expected %s argument to be either [uncompressed, snappy, gzip or zstd]", loption);
419: 		} else {
420: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
421: 		}
422: 	}
423: 	bind_data->sql_types = sql_types;
424: 	bind_data->column_names = names;
425: 	bind_data->file_name = info.file_path;
426: 	return move(bind_data);
427: }
428: 
429: unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &context, FunctionData &bind_data) {
430: 	auto global_state = make_unique<ParquetWriteGlobalState>();
431: 	auto &parquet_bind = (ParquetWriteBindData &)bind_data;
432: 
433: 	auto &fs = FileSystem::GetFileSystem(context);
434: 	global_state->writer =
435: 	    make_unique<ParquetWriter>(fs, parquet_bind.file_name, FileSystem::GetFileOpener(context),
436: 	                               parquet_bind.sql_types, parquet_bind.column_names, parquet_bind.codec);
437: 	return move(global_state);
438: }
439: 
440: void ParquetWriteSink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
441:                       LocalFunctionData &lstate, DataChunk &input) {
442: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
443: 	auto &local_state = (ParquetWriteLocalState &)lstate;
444: 
445: 	// append data to the local (buffered) chunk collection
446: 	local_state.buffer->Append(input);
447: 	if (local_state.buffer->Count() > 100000) {
448: 		// if the chunk collection exceeds a certain size we flush it to the parquet file
449: 		global_state.writer->Flush(*local_state.buffer);
450: 		// and reset the buffer
451: 		local_state.buffer = make_unique<ChunkCollection>();
452: 	}
453: }
454: 
455: void ParquetWriteCombine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
456:                          LocalFunctionData &lstate) {
457: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
458: 	auto &local_state = (ParquetWriteLocalState &)lstate;
459: 	// flush any data left in the local state to the file
460: 	global_state.writer->Flush(*local_state.buffer);
461: }
462: 
463: void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
464: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
465: 	// finalize: write any additional metadata to the file here
466: 	global_state.writer->Finalize();
467: }
468: 
469: unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ClientContext &context, FunctionData &bind_data) {
470: 	return make_unique<ParquetWriteLocalState>();
471: }
472: 
473: unique_ptr<TableFunctionRef> ParquetScanReplacement(const string &table_name, void *data) {
474: 	if (!StringUtil::EndsWith(table_name, ".parquet")) {
475: 		return nullptr;
476: 	}
477: 	auto table_function = make_unique<TableFunctionRef>();
478: 	vector<unique_ptr<ParsedExpression>> children;
479: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
480: 	table_function->function = make_unique<FunctionExpression>("parquet_scan", move(children));
481: 	return table_function;
482: }
483: 
484: void ParquetExtension::Load(DuckDB &db) {
485: 	auto scan_fun = ParquetScanFunction::GetFunctionSet();
486: 	CreateTableFunctionInfo cinfo(scan_fun);
487: 	cinfo.name = "read_parquet";
488: 	CreateTableFunctionInfo pq_scan = cinfo;
489: 	pq_scan.name = "parquet_scan";
490: 
491: 	ParquetMetaDataFunction meta_fun;
492: 	CreateTableFunctionInfo meta_cinfo(meta_fun);
493: 
494: 	ParquetSchemaFunction schema_fun;
495: 	CreateTableFunctionInfo schema_cinfo(schema_fun);
496: 
497: 	CopyFunction function("parquet");
498: 	function.copy_to_bind = ParquetWriteBind;
499: 	function.copy_to_initialize_global = ParquetWriteInitializeGlobal;
500: 	function.copy_to_initialize_local = ParquetWriteInitializeLocal;
501: 	function.copy_to_sink = ParquetWriteSink;
502: 	function.copy_to_combine = ParquetWriteCombine;
503: 	function.copy_to_finalize = ParquetWriteFinalize;
504: 	function.copy_from_bind = ParquetScanFunction::ParquetReadBind;
505: 	function.copy_from_function = scan_fun.functions[0];
506: 
507: 	function.extension = "parquet";
508: 	CreateCopyFunctionInfo info(function);
509: 
510: 	Connection con(db);
511: 	con.BeginTransaction();
512: 	auto &context = *con.context;
513: 	auto &catalog = Catalog::GetCatalog(context);
514: 	catalog.CreateCopyFunction(context, &info);
515: 	catalog.CreateTableFunction(context, &cinfo);
516: 	catalog.CreateTableFunction(context, &pq_scan);
517: 	catalog.CreateTableFunction(context, &meta_cinfo);
518: 	catalog.CreateTableFunction(context, &schema_cinfo);
519: 	con.Commit();
520: 
521: 	auto &config = DBConfig::GetConfig(*db.instance);
522: 	config.replacement_scans.emplace_back(ParquetScanReplacement);
523: }
524: 
525: } // namespace duckdb
[end of extension/parquet/parquet-extension.cpp]
[start of src/function/table/read_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/main/database.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/main/config.hpp"
8: #include "duckdb/parser/expression/constant_expression.hpp"
9: #include "duckdb/parser/expression/function_expression.hpp"
10: #include "duckdb/parser/tableref/table_function_ref.hpp"
11: #include "duckdb/common/windows_undefs.hpp"
12: 
13: #include <limits>
14: 
15: namespace duckdb {
16: 
17: static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, vector<Value> &inputs,
18:                                             unordered_map<string, Value> &named_parameters,
19:                                             vector<LogicalType> &input_table_types, vector<string> &input_table_names,
20:                                             vector<LogicalType> &return_types, vector<string> &names) {
21: 	auto result = make_unique<ReadCSVData>();
22: 	auto &options = result->options;
23: 
24: 	string file_pattern = inputs[0].str_value;
25: 
26: 	auto &fs = FileSystem::GetFileSystem(context);
27: 	result->files = fs.Glob(file_pattern);
28: 	if (result->files.empty()) {
29: 		throw IOException("No files found that match the pattern \"%s\"", file_pattern);
30: 	}
31: 
32: 	for (auto &kv : named_parameters) {
33: 		if (kv.first == "auto_detect") {
34: 			options.auto_detect = kv.second.value_.boolean;
35: 		} else if (kv.first == "sep" || kv.first == "delim") {
36: 			options.delimiter = kv.second.str_value;
37: 			options.has_delimiter = true;
38: 		} else if (kv.first == "header") {
39: 			options.header = kv.second.value_.boolean;
40: 			options.has_header = true;
41: 		} else if (kv.first == "quote") {
42: 			options.quote = kv.second.str_value;
43: 			options.has_quote = true;
44: 		} else if (kv.first == "escape") {
45: 			options.escape = kv.second.str_value;
46: 			options.has_escape = true;
47: 		} else if (kv.first == "nullstr") {
48: 			options.null_str = kv.second.str_value;
49: 		} else if (kv.first == "sample_size") {
50: 			int64_t sample_size = kv.second.GetValue<int64_t>();
51: 			if (sample_size < 1 && sample_size != -1) {
52: 				throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
53: 			}
54: 			if (sample_size == -1) {
55: 				options.sample_chunks = std::numeric_limits<uint64_t>::max();
56: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
57: 			} else if (sample_size <= STANDARD_VECTOR_SIZE) {
58: 				options.sample_chunk_size = sample_size;
59: 				options.sample_chunks = 1;
60: 			} else {
61: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
62: 				options.sample_chunks = sample_size / STANDARD_VECTOR_SIZE;
63: 			}
64: 		} else if (kv.first == "sample_chunk_size") {
65: 			options.sample_chunk_size = kv.second.GetValue<int64_t>();
66: 			if (options.sample_chunk_size > STANDARD_VECTOR_SIZE) {
67: 				throw BinderException(
68: 				    "Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
69: 				    STANDARD_VECTOR_SIZE);
70: 			} else if (options.sample_chunk_size < 1) {
71: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be smaller than 1");
72: 			}
73: 		} else if (kv.first == "sample_chunks") {
74: 			options.sample_chunks = kv.second.GetValue<int64_t>();
75: 			if (options.sample_chunks < 1) {
76: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNKS: cannot be smaller than 1");
77: 			}
78: 		} else if (kv.first == "all_varchar") {
79: 			options.all_varchar = kv.second.value_.boolean;
80: 		} else if (kv.first == "dateformat") {
81: 			options.has_format[LogicalTypeId::DATE] = true;
82: 			auto &date_format = options.date_format[LogicalTypeId::DATE];
83: 			date_format.format_specifier = kv.second.str_value;
84: 			string error = StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
85: 			if (!error.empty()) {
86: 				throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
87: 			}
88: 		} else if (kv.first == "timestampformat") {
89: 			options.has_format[LogicalTypeId::TIMESTAMP] = true;
90: 			auto &timestamp_format = options.date_format[LogicalTypeId::TIMESTAMP];
91: 			timestamp_format.format_specifier = kv.second.str_value;
92: 			string error = StrTimeFormat::ParseFormatSpecifier(timestamp_format.format_specifier, timestamp_format);
93: 			if (!error.empty()) {
94: 				throw InvalidInputException("Could not parse TIMESTAMPFORMAT: %s", error.c_str());
95: 			}
96: 		} else if (kv.first == "normalize_names") {
97: 			options.normalize_names = kv.second.value_.boolean;
98: 		} else if (kv.first == "columns") {
99: 			auto &child_type = kv.second.type();
100: 			if (child_type.id() != LogicalTypeId::STRUCT) {
101: 				throw BinderException("read_csv columns requires a a struct as input");
102: 			}
103: 			D_ASSERT(StructType::GetChildCount(child_type) == kv.second.struct_value.size());
104: 			for (idx_t i = 0; i < kv.second.struct_value.size(); i++) {
105: 				auto &name = StructType::GetChildName(child_type, i);
106: 				auto &val = kv.second.struct_value[i];
107: 				names.push_back(name);
108: 				if (val.type().id() != LogicalTypeId::VARCHAR) {
109: 					throw BinderException("read_csv requires a type specification as string");
110: 				}
111: 				return_types.emplace_back(TransformStringToLogicalType(val.str_value.c_str()));
112: 			}
113: 			if (names.empty()) {
114: 				throw BinderException("read_csv requires at least a single column as input!");
115: 			}
116: 		} else if (kv.first == "compression") {
117: 			options.compression = kv.second.str_value;
118: 		} else if (kv.first == "filename") {
119: 			result->include_file_name = kv.second.value_.boolean;
120: 		} else if (kv.first == "skip") {
121: 			options.skip_rows = kv.second.GetValue<int64_t>();
122: 		}
123: 	}
124: 	if (!options.auto_detect && return_types.empty()) {
125: 		throw BinderException("read_csv requires columns to be specified. Use read_csv_auto or set read_csv(..., "
126: 		                      "AUTO_DETECT=TRUE) to automatically guess columns.");
127: 	}
128: 	if (!(options.compression == "infer" || options.compression == "gzip" || options.compression == "none" ||
129: 	      options.compression.empty())) {
130: 		throw BinderException("read_csv currently only supports 'gzip' compression.");
131: 	}
132: 	if (options.auto_detect) {
133: 		options.file_path = result->files[0];
134: 		auto initial_reader = make_unique<BufferedCSVReader>(context, options);
135: 
136: 		return_types.assign(initial_reader->sql_types.begin(), initial_reader->sql_types.end());
137: 		if (names.empty()) {
138: 			names.assign(initial_reader->col_names.begin(), initial_reader->col_names.end());
139: 		} else {
140: 			D_ASSERT(return_types.size() == names.size());
141: 		}
142: 		result->initial_reader = move(initial_reader);
143: 	} else {
144: 		result->sql_types = return_types;
145: 		D_ASSERT(return_types.size() == names.size());
146: 	}
147: 	if (result->include_file_name) {
148: 		return_types.push_back(LogicalType::VARCHAR);
149: 		names.emplace_back("filename");
150: 	}
151: 	return move(result);
152: }
153: 
154: struct ReadCSVOperatorData : public FunctionOperatorData {
155: 	//! The CSV reader
156: 	unique_ptr<BufferedCSVReader> csv_reader;
157: 	//! The index of the next file to read (i.e. current file + 1)
158: 	idx_t file_index;
159: };
160: 
161: static unique_ptr<FunctionOperatorData> ReadCSVInit(ClientContext &context, const FunctionData *bind_data_p,
162:                                                     const vector<column_t> &column_ids,
163:                                                     TableFilterCollection *filters) {
164: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
165: 	auto result = make_unique<ReadCSVOperatorData>();
166: 	if (bind_data.initial_reader) {
167: 		result->csv_reader = move(bind_data.initial_reader);
168: 	} else {
169: 		bind_data.options.file_path = bind_data.files[0];
170: 		result->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);
171: 	}
172: 	bind_data.bytes_read = 0;
173: 	bind_data.file_size = result->csv_reader->file_size;
174: 	result->file_index = 1;
175: 	return move(result);
176: }
177: 
178: static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, vector<Value> &inputs,
179:                                                 unordered_map<string, Value> &named_parameters,
180:                                                 vector<LogicalType> &input_table_types,
181:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
182:                                                 vector<string> &names) {
183: 	named_parameters["auto_detect"] = Value::BOOLEAN(true);
184: 	return ReadCSVBind(context, inputs, named_parameters, input_table_types, input_table_names, return_types, names);
185: }
186: 
187: static void ReadCSVFunction(ClientContext &context, const FunctionData *bind_data_p,
188:                             FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
189: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
190: 	auto &data = (ReadCSVOperatorData &)*operator_state;
191: 	do {
192: 		data.csv_reader->ParseCSV(output);
193: 		bind_data.bytes_read = data.csv_reader->bytes_in_chunk;
194: 		if (output.size() == 0 && data.file_index < bind_data.files.size()) {
195: 			// exhausted this file, but we have more files we can read
196: 			// open the next file and increment the counter
197: 			bind_data.options.file_path = bind_data.files[data.file_index];
198: 			data.csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, data.csv_reader->sql_types);
199: 			data.file_index++;
200: 		} else {
201: 			break;
202: 		}
203: 	} while (true);
204: 	if (bind_data.include_file_name) {
205: 		auto &col = output.data.back();
206: 		col.SetValue(0, Value(data.csv_reader->options.file_path));
207: 		col.SetVectorType(VectorType::CONSTANT_VECTOR);
208: 	}
209: }
210: 
211: static void ReadCSVAddNamedParameters(TableFunction &table_function) {
212: 	table_function.named_parameters["sep"] = LogicalType::VARCHAR;
213: 	table_function.named_parameters["delim"] = LogicalType::VARCHAR;
214: 	table_function.named_parameters["quote"] = LogicalType::VARCHAR;
215: 	table_function.named_parameters["escape"] = LogicalType::VARCHAR;
216: 	table_function.named_parameters["nullstr"] = LogicalType::VARCHAR;
217: 	table_function.named_parameters["columns"] = LogicalType::ANY;
218: 	table_function.named_parameters["header"] = LogicalType::BOOLEAN;
219: 	table_function.named_parameters["auto_detect"] = LogicalType::BOOLEAN;
220: 	table_function.named_parameters["sample_size"] = LogicalType::BIGINT;
221: 	table_function.named_parameters["sample_chunk_size"] = LogicalType::BIGINT;
222: 	table_function.named_parameters["sample_chunks"] = LogicalType::BIGINT;
223: 	table_function.named_parameters["all_varchar"] = LogicalType::BOOLEAN;
224: 	table_function.named_parameters["dateformat"] = LogicalType::VARCHAR;
225: 	table_function.named_parameters["timestampformat"] = LogicalType::VARCHAR;
226: 	table_function.named_parameters["normalize_names"] = LogicalType::BOOLEAN;
227: 	table_function.named_parameters["compression"] = LogicalType::VARCHAR;
228: 	table_function.named_parameters["filename"] = LogicalType::BOOLEAN;
229: 	table_function.named_parameters["skip"] = LogicalType::BIGINT;
230: }
231: 
232: int CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p) {
233: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
234: 	if (bind_data.file_size == 0) {
235: 		return 100;
236: 	}
237: 	auto percentage = bind_data.bytes_read * 100 / bind_data.file_size;
238: 	return percentage;
239: }
240: 
241: TableFunction ReadCSVTableFunction::GetFunction() {
242: 	TableFunction read_csv("read_csv", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVBind, ReadCSVInit);
243: 	read_csv.table_scan_progress = CSVReaderProgress;
244: 	ReadCSVAddNamedParameters(read_csv);
245: 	return read_csv;
246: }
247: 
248: void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {
249: 	set.AddFunction(ReadCSVTableFunction::GetFunction());
250: 
251: 	TableFunction read_csv_auto("read_csv_auto", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVAutoBind, ReadCSVInit);
252: 	read_csv_auto.table_scan_progress = CSVReaderProgress;
253: 	ReadCSVAddNamedParameters(read_csv_auto);
254: 	set.AddFunction(read_csv_auto);
255: }
256: 
257: unique_ptr<TableFunctionRef> ReadCSVReplacement(const string &table_name, void *data) {
258: 	if (!StringUtil::EndsWith(table_name, ".csv") && !StringUtil::EndsWith(table_name, ".tsv") &&
259: 	    !StringUtil::EndsWith(table_name, ".csv.gz")) {
260: 		return nullptr;
261: 	}
262: 	auto table_function = make_unique<TableFunctionRef>();
263: 	vector<unique_ptr<ParsedExpression>> children;
264: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
265: 	table_function->function = make_unique<FunctionExpression>("read_csv_auto", move(children));
266: 	return table_function;
267: }
268: 
269: void BuiltinFunctions::RegisterReadFunctions() {
270: 	CSVCopyFunction::RegisterFunction(*this);
271: 	ReadCSVTableFunction::RegisterFunction(*this);
272: 
273: 	auto &config = DBConfig::GetConfig(context);
274: 	config.replacement_scans.emplace_back(ReadCSVReplacement);
275: }
276: 
277: } // namespace duckdb
[end of src/function/table/read_csv.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: