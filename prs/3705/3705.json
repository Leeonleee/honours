{
  "repo": "duckdb/duckdb",
  "pull_number": 3705,
  "instance_id": "duckdb__duckdb-3705",
  "issue_numbers": [
    "3348",
    "3348"
  ],
  "base_commit": "620b44ad73ea76d3987c12872933ed8f571d3c3a",
  "patch": "diff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex 24a599493dcd..8147dc804599 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -174,6 +174,7 @@ LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, bool bi\n \t\t\t\t    \"DECIMAL converted type can only be set for value of Type::(FIXED_LEN_)BYTE_ARRAY/INT32/INT64\");\n \t\t\t}\n \t\tcase ConvertedType::UTF8:\n+\t\tcase ConvertedType::ENUM:\n \t\t\tswitch (s_ele.type) {\n \t\t\tcase Type::BYTE_ARRAY:\n \t\t\tcase Type::FIXED_LEN_BYTE_ARRAY:\n@@ -193,7 +194,6 @@ LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, bool bi\n \t\tcase ConvertedType::MAP:\n \t\tcase ConvertedType::MAP_KEY_VALUE:\n \t\tcase ConvertedType::LIST:\n-\t\tcase ConvertedType::ENUM:\n \t\tcase ConvertedType::JSON:\n \t\tcase ConvertedType::BSON:\n \t\tdefault:\ndiff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp\nindex 4017652b054f..8b8843524f0d 100644\n--- a/src/function/table/arrow.cpp\n+++ b/src/function/table/arrow.cpp\n@@ -863,6 +863,14 @@ void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanState &scan\n \t\tauto &struct_validity_mask = FlatVector::Validity(vector);\n \t\tfor (idx_t type_idx = 0; type_idx < (idx_t)array.n_children; type_idx++) {\n \t\t\tSetValidityMask(*child_entries[type_idx], *array.children[type_idx], scan_state, size, nested_offset);\n+\t\t\tif (!struct_validity_mask.AllValid()) {\n+\t\t\t\tauto &child_validity_mark = FlatVector::Validity(*child_entries[type_idx]);\n+\t\t\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\t\t\tif (!struct_validity_mask.RowIsValid(i)) {\n+\t\t\t\t\t\tchild_validity_mark.SetInvalid(i);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n \t\t\tColumnArrowToDuckDB(*child_entries[type_idx], *array.children[type_idx], scan_state, size,\n \t\t\t                    arrow_convert_data, col_idx, arrow_convert_idx, nested_offset, &struct_validity_mask);\n \t\t}\ndiff --git a/src/include/duckdb/common/field_writer.hpp b/src/include/duckdb/common/field_writer.hpp\nindex 50486f301a5c..df3b1b5a3b51 100644\n--- a/src/include/duckdb/common/field_writer.hpp\n+++ b/src/include/duckdb/common/field_writer.hpp\n@@ -199,7 +199,7 @@ class FieldReader {\n \t}\n \n \ttemplate <class T, class RETURN_TYPE = unique_ptr<T>, typename... ARGS>\n-\tRETURN_TYPE ReadSerializable(RETURN_TYPE default_value, ARGS &&...args) {\n+\tRETURN_TYPE ReadSerializable(RETURN_TYPE default_value, ARGS &&... args) {\n \t\tif (field_count >= max_field_count) {\n \t\t\t// field is not there, read the default value\n \t\t\treturn default_value;\n@@ -221,7 +221,7 @@ class FieldReader {\n \t}\n \n \ttemplate <class T, class RETURN_TYPE = unique_ptr<T>, typename... ARGS>\n-\tRETURN_TYPE ReadRequiredSerializable(ARGS &&...args) {\n+\tRETURN_TYPE ReadRequiredSerializable(ARGS &&... args) {\n \t\tif (field_count >= max_field_count) {\n \t\t\t// field is not there, read the default value\n \t\t\tthrow SerializationException(\"Attempting to read mandatory field, but field is missing\");\n",
  "test_patch": "diff --git a/data/parquet-testing/adam_genotypes.parquet b/data/parquet-testing/adam_genotypes.parquet\nnew file mode 100644\nindex 000000000000..5fea999cdb20\nBinary files /dev/null and b/data/parquet-testing/adam_genotypes.parquet differ\ndiff --git a/data/parquet-testing/enum.parquet b/data/parquet-testing/enum.parquet\nnew file mode 100644\nindex 000000000000..d0018c61e198\nBinary files /dev/null and b/data/parquet-testing/enum.parquet differ\ndiff --git a/test/sql/copy/parquet/enum_converted_type.test b/test/sql/copy/parquet/enum_converted_type.test\nnew file mode 100644\nindex 000000000000..4a41ed42d681\n--- /dev/null\n+++ b/test/sql/copy/parquet/enum_converted_type.test\n@@ -0,0 +1,14 @@\n+# name: test/sql/copy/parquet/enum_converted_type.test\n+# description: Test enum converted type\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query IIIIIII\n+select * from 'data/parquet-testing/enum.parquet';\n+----\n+1\t0\tt1\ttest_span\t1612550512340953\t500000\t[{'key': service_name, 'v_type': STRING, 'v_str': test_service, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }, {'key': http_method, 'v_type': STRING, 'v_str': POST, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }, {'key': method, 'v_type': STRING, 'v_str': callbacks.flannel, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }, {'key': boolean, 'v_type': BOOL, 'v_str': , 'v_bool': True, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }, {'key': int, 'v_type': INT64, 'v_str': , 'v_bool': False, 'v_int64': 1000, 'v_float64': 1001.200000, 'v_binary': }, {'key': float, 'v_type': FLOAT64, 'v_str': , 'v_bool': False, 'v_int64': 1000, 'v_float64': 1001.200000, 'v_binary': }, {'key': binary, 'v_type': BINARY, 'v_str': ignored, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': binaryTagValue}, {'key': type, 'v_type': STRING, 'v_str': msg_type, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }]\n+2\t1\tt1\ttest_span\t1612550512340954\t500001\t[{'key': service_name, 'v_type': STRING, 'v_str': test_service, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }, {'key': http_method, 'v_type': STRING, 'v_str': POST, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }, {'key': method, 'v_type': STRING, 'v_str': callbacks.flannel, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }, {'key': boolean, 'v_type': BOOL, 'v_str': , 'v_bool': True, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }, {'key': int, 'v_type': INT64, 'v_str': , 'v_bool': False, 'v_int64': 1000, 'v_float64': 1001.200000, 'v_binary': }, {'key': float, 'v_type': FLOAT64, 'v_str': , 'v_bool': False, 'v_int64': 1000, 'v_float64': 1001.200000, 'v_binary': }, {'key': binary, 'v_type': BINARY, 'v_str': ignored, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': binaryTagValue}, {'key': type, 'v_type': STRING, 'v_str': msg_type, 'v_bool': False, 'v_int64': 0, 'v_float64': 0.000000, 'v_binary': }]\ndiff --git a/test/sql/copy/parquet/parquet_enum_test.test b/test/sql/copy/parquet/parquet_enum_test.test\nnew file mode 100644\nindex 000000000000..ae298fba53c7\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_enum_test.test\n@@ -0,0 +1,13 @@\n+# name: test/sql/copy/parquet/parquet_enum_test.test\n+# description: Test parquet file with enum content\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query IIIIIIIIIIIIIIIIIIIIII\n+SELECT * FROM parquet_scan('data/parquet-testing/adam_genotypes.parquet')\n+----\n+{'referenceName': NULL, 'start': NULL, 'end': NULL, 'names': [name], 'splitFromMultiAllelic': False, 'referenceAllele': NULL, 'alternateAllele': NULL, 'quality': NULL, 'filtersApplied': NULL, 'filtersPassed': NULL, 'filtersFailed': [], 'annotation': NULL}\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\t[]\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\t[]\t[]\t[]\tFalse\tFalse\tNULL\tNULL\ndiff --git a/test/sql/copy/parquet/parquet_glob.test b/test/sql/copy/parquet/parquet_glob.test\nindex 2ada55168632..3ea1b162081f 100644\n--- a/test/sql/copy/parquet/parquet_glob.test\n+++ b/test/sql/copy/parquet/parquet_glob.test\n@@ -64,7 +64,7 @@ select count(*) from parquet_scan('')\n \n # schema mismatch in parquet glob\n statement error\n-select count(*) from parquet_scan('data/parquet-testing/*.parquet')\n+select * from parquet_scan('data/parquet-testing/*.parquet')\n \n # parquet glob with COPY FROM\n statement ok\n",
  "problem_statement": "Reading from Parquet file fails with error \"java.sql.SQLException: IO Error: Unsupported converted type\"\n#### What happens?\r\nI created a parquet file using protobuf parquet library.\r\n\r\nIt seems it is related to #1200\r\n\r\n#### To Reproduce\r\n`SELECT * from read_parquet('\" + tempLogFilePath.getLogFilePath() + \"')` fails on the parquet file when queried by the JDBC api.\r\n\r\nStack trace is below:\r\n```\r\njava.sql.SQLException: IO Error: Unsupported converted type\r\n\r\n\tat org.duckdb.DuckDBNative.duckdb_jdbc_prepare(Native Method)\r\n\tat org.duckdb.DuckDBPreparedStatement.prepare(DuckDBPreparedStatement.java:73)\r\n\tat org.duckdb.DuckDBPreparedStatement.executeQuery(DuckDBPreparedStatement.java:127)\r\n\tat com.slack.kaldb.logstore.columnar.io.impl.ProtobufParquetTest.readDuckDb(ProtobufParquetTest.java:153)\r\n\tat com.slack.kaldb.logstore.columnar.io.impl.ProtobufParquetTest.testSecorWriterDuckdbReader(ProtobufParquetTest.java:132)\r\n```\r\n\r\nLink to [unit test code](https://github.com/slackhq/kaldb/blob/playground_columnar/kaldb/src/test/java/com/slack/kaldb/logstore/columnar/io/impl/ProtobufParquetTest.java#L98)  and [probuf schema](https://github.com/slackhq/kaldb/blob/playground_columnar/kaldb/src/main/proto/trace.proto#L38). \r\n\r\nThe parquet file is valid since I can read it via the secor reader and `parquet-tools`. \r\n\r\nI am not sure what causes this exception. I tried replacing bytes type as string and I still the same exception.\r\n\r\n#### Environment (please complete the following information):\r\n - OS: macos\r\n - DuckDB Version: 0.3.2\r\n - DuckDB Client: Java \r\n\r\n#### Before Submitting\r\n\r\n- [*] **Have you tried this on the latest `master` branch?**\r\n* **Python**: `pip install duckdb --upgrade --pre`\r\n* **R**: `install.packages(\"https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz\", repos = NULL)`\r\n* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.\r\n\r\n- [ *] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\nReading from Parquet file fails with error \"java.sql.SQLException: IO Error: Unsupported converted type\"\n#### What happens?\r\nI created a parquet file using protobuf parquet library.\r\n\r\nIt seems it is related to #1200\r\n\r\n#### To Reproduce\r\n`SELECT * from read_parquet('\" + tempLogFilePath.getLogFilePath() + \"')` fails on the parquet file when queried by the JDBC api.\r\n\r\nStack trace is below:\r\n```\r\njava.sql.SQLException: IO Error: Unsupported converted type\r\n\r\n\tat org.duckdb.DuckDBNative.duckdb_jdbc_prepare(Native Method)\r\n\tat org.duckdb.DuckDBPreparedStatement.prepare(DuckDBPreparedStatement.java:73)\r\n\tat org.duckdb.DuckDBPreparedStatement.executeQuery(DuckDBPreparedStatement.java:127)\r\n\tat com.slack.kaldb.logstore.columnar.io.impl.ProtobufParquetTest.readDuckDb(ProtobufParquetTest.java:153)\r\n\tat com.slack.kaldb.logstore.columnar.io.impl.ProtobufParquetTest.testSecorWriterDuckdbReader(ProtobufParquetTest.java:132)\r\n```\r\n\r\nLink to [unit test code](https://github.com/slackhq/kaldb/blob/playground_columnar/kaldb/src/test/java/com/slack/kaldb/logstore/columnar/io/impl/ProtobufParquetTest.java#L98)  and [probuf schema](https://github.com/slackhq/kaldb/blob/playground_columnar/kaldb/src/main/proto/trace.proto#L38). \r\n\r\nThe parquet file is valid since I can read it via the secor reader and `parquet-tools`. \r\n\r\nI am not sure what causes this exception. I tried replacing bytes type as string and I still the same exception.\r\n\r\n#### Environment (please complete the following information):\r\n - OS: macos\r\n - DuckDB Version: 0.3.2\r\n - DuckDB Client: Java \r\n\r\n#### Before Submitting\r\n\r\n- [*] **Have you tried this on the latest `master` branch?**\r\n* **Python**: `pip install duckdb --upgrade --pre`\r\n* **R**: `install.packages(\"https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz\", repos = NULL)`\r\n* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.\r\n\r\n- [ *] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\n",
  "hints_text": "Generated parquet file is also attached.\r\n[0_1_00000000000000023232.log](https://github.com/duckdb/duckdb/files/8401790/0_1_00000000000000023232.log)\r\n\r\nI have also tried this query `SELECT decode(id) from parquet_scan('\" + tempLogFilePath.getLogFilePath() + \"')\"` looking at another issue and it fails with the same error.\nIt looks like this is due to `ConvertedType::ENUM` being used in the file, learned something today. This converted type is currently not supported. \nThanks for a quick response. \r\n\r\nIn this proto file, the [tags](https://github.com/slackhq/kaldb/blob/master/kaldb/src/main/proto/trace.proto#L68) field is a mechanism to add arbitrary number of typed fields to the Span. Do you have any recommendations on an alternative way to achieve the same goal in DuckDB?\nYou could use a `MAP` type to store key-value pairs, e.g.:\r\n\r\n```sql\r\nD create table keyvalue(kv map(varchar, varchar));\r\nD insert into keyvalue values (map(['key1', 'key2'], ['val1', 'val2']));\r\nD select * from keyvalue;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502           kv           \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 {key1=val1, key2=val2} \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD \r\n\r\n```\nAgree we can use the map type, but we didn't take that route since we would lose the benefits of columnar compression like dictionary encoding etc.. Further, joins would be very slow when joining across the MAP keys. \r\n\r\nIf there a way to do a MAP without losing the benefits of columnar compression etc?\nI think we can just decode that type as a string, cause its what the spec says it is. \nGenerated parquet file is also attached.\r\n[0_1_00000000000000023232.log](https://github.com/duckdb/duckdb/files/8401790/0_1_00000000000000023232.log)\r\n\r\nI have also tried this query `SELECT decode(id) from parquet_scan('\" + tempLogFilePath.getLogFilePath() + \"')\"` looking at another issue and it fails with the same error.\nIt looks like this is due to `ConvertedType::ENUM` being used in the file, learned something today. This converted type is currently not supported. \nThanks for a quick response. \r\n\r\nIn this proto file, the [tags](https://github.com/slackhq/kaldb/blob/master/kaldb/src/main/proto/trace.proto#L68) field is a mechanism to add arbitrary number of typed fields to the Span. Do you have any recommendations on an alternative way to achieve the same goal in DuckDB?\nYou could use a `MAP` type to store key-value pairs, e.g.:\r\n\r\n```sql\r\nD create table keyvalue(kv map(varchar, varchar));\r\nD insert into keyvalue values (map(['key1', 'key2'], ['val1', 'val2']));\r\nD select * from keyvalue;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502           kv           \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 {key1=val1, key2=val2} \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD \r\n\r\n```\nAgree we can use the map type, but we didn't take that route since we would lose the benefits of columnar compression like dictionary encoding etc.. Further, joins would be very slow when joining across the MAP keys. \r\n\r\nIf there a way to do a MAP without losing the benefits of columnar compression etc?\nI think we can just decode that type as a string, cause its what the spec says it is. ",
  "created_at": "2022-05-24T16:43:50Z"
}