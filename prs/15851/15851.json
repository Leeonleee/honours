{
  "repo": "duckdb/duckdb",
  "pull_number": 15851,
  "instance_id": "duckdb__duckdb-15851",
  "issue_numbers": [
    "15570",
    "15760"
  ],
  "base_commit": "8e68a3e34aa526a342ae91e1b14b764bb3075a12",
  "patch": "diff --git a/.github/config/out_of_tree_extensions.cmake b/.github/config/out_of_tree_extensions.cmake\nindex c02455026c25..2224acfd97a0 100644\n--- a/.github/config/out_of_tree_extensions.cmake\n+++ b/.github/config/out_of_tree_extensions.cmake\n@@ -56,10 +56,8 @@ endif()\n # for Delta\n if (NOT MINGW AND NOT \"${OS_NAME}\" STREQUAL \"linux\" AND NOT ${WASM_ENABLED})\n     duckdb_extension_load(delta\n-            LOAD_TESTS\n             GIT_URL https://github.com/duckdb/duckdb-delta\n-            GIT_TAG b7333c0143e101c720117d564651e693b317bb31\n-            APPLY_PATCHES\n+            GIT_TAG 846019edcc27000721ff9c4281e85a63d1aa10de\n     )\n endif()\n \n@@ -91,10 +89,9 @@ endif()\n duckdb_extension_load(inet\n     LOAD_TESTS\n     GIT_URL https://github.com/duckdb/duckdb-inet\n-    GIT_TAG 51d7ad789f34eecb36a2071bac5aef0e12747d70\n+    GIT_TAG a8b361ab5d43f6390d7cb48c9a9f0638e9581cf9\n     INCLUDE_DIR src/include\n     TEST_DIR test/sql\n-    APPLY_PATCHES\n     )\n \n ################# POSTGRES_SCANNER\ndiff --git a/.github/patches/extensions/delta/fixes.patch b/.github/patches/extensions/delta/fixes.patch\ndeleted file mode 100644\nindex 28b142ffd8b1..000000000000\n--- a/.github/patches/extensions/delta/fixes.patch\n+++ /dev/null\n@@ -1,142 +0,0 @@\n-diff --git a/src/functions/delta_scan.cpp b/src/functions/delta_scan.cpp\n-index 65eb34f..7210382 100644\n---- a/src/functions/delta_scan.cpp\n-+++ b/src/functions/delta_scan.cpp\n-@@ -464,7 +464,11 @@ unique_ptr<MultiFileList> DeltaSnapshot::ComplexFilterPushdown(ClientContext &co\n-     for (const auto &filter : filters) {\n-         combiner.AddFilter(filter->Copy());\n-     }\n--    auto filterstmp = combiner.GenerateTableScanFilters(info.column_ids);\n-+    vector<ColumnIndex> column_indexes;\n-+    for(auto column_id : info.column_ids) {\n-+    \tcolumn_indexes.emplace_back(column_id);\n-+    }\n-+    auto filterstmp = combiner.GenerateTableScanFilters(column_indexes);\n- \n-     // TODO: can/should we figure out if this filtered anything?\n-     auto filtered_list = make_uniq<DeltaSnapshot>(context, paths[0]);\n-@@ -529,7 +533,7 @@ unique_ptr<NodeStatistics> DeltaSnapshot::GetCardinality(ClientContext &context)\n-     return nullptr;\n- }\n- \n--unique_ptr<MultiFileReader> DeltaMultiFileReader::CreateInstance() {\n-+unique_ptr<MultiFileReader> DeltaMultiFileReader::CreateInstance(const TableFunction &table_function) {\n-     return std::move(make_uniq<DeltaMultiFileReader>());\n- }\n- \n-@@ -575,7 +579,7 @@ void DeltaMultiFileReader::BindOptions(MultiFileReaderOptions &options, MultiFil\n- void DeltaMultiFileReader::FinalizeBind(const MultiFileReaderOptions &file_options, const MultiFileReaderBindData &options,\n-                   const string &filename, const vector<string> &local_names,\n-                   const vector<LogicalType> &global_types, const vector<string> &global_names,\n--                  const vector<column_t> &global_column_ids, MultiFileReaderData &reader_data,\n-+                  const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,\n-                   ClientContext &context, optional_ptr<MultiFileReaderGlobalState> global_state) {\n-     MultiFileReader::FinalizeBind(file_options, options, filename, local_names, global_types, global_names, global_column_ids, reader_data, context, global_state);\n- \n-@@ -600,7 +604,7 @@ void DeltaMultiFileReader::FinalizeBind(const MultiFileReaderOptions &file_optio\n- \n-     if (!file_metadata->partition_map.empty()) {\n-         for (idx_t i = 0; i < global_column_ids.size(); i++) {\n--            column_t col_id = global_column_ids[i];\n-+            column_t col_id = global_column_ids[i].GetPrimaryIndex();\n-             if (IsRowIdColumnId(col_id)) {\n-                 continue;\n-             }\n-@@ -618,12 +622,12 @@ void DeltaMultiFileReader::FinalizeBind(const MultiFileReaderOptions &file_optio\n-     }\n- }\n- \n--unique_ptr<MultiFileList> DeltaMultiFileReader::CreateFileList(ClientContext &context, const vector<string>& paths, FileGlobOptions options) {\n-+shared_ptr<MultiFileList> DeltaMultiFileReader::CreateFileList(ClientContext &context, const vector<string>& paths, FileGlobOptions options) {\n-     if (paths.size() != 1) {\n-         throw BinderException(\"'delta_scan' only supports single path as input\");\n-     }\n- \n--    return make_uniq<DeltaSnapshot>(context, paths[0]);\n-+    return make_shared_ptr<DeltaSnapshot>(context, paths[0]);\n- }\n- \n- // Generate the correct Selection Vector Based on the Raw delta KernelBoolSlice dv and the row_id_column\n-@@ -670,14 +674,14 @@ unique_ptr<MultiFileReaderGlobalState> DeltaMultiFileReader::InitializeGlobalSta\n-                                                                                    const duckdb::MultiFileList &file_list,\n-                                                                                    const vector<duckdb::LogicalType> &global_types,\n-                                                                                    const vector<std::string> &global_names,\n--                                                                                   const vector<duckdb::column_t> &global_column_ids) {\n-+                                                                                   const vector<duckdb::ColumnIndex> &global_column_ids) {\n-     vector<LogicalType> extra_columns;\n-     vector<pair<string, idx_t>> mapped_columns;\n- \n-     // Create a map of the columns that are in the projection\n-     case_insensitive_map_t<idx_t> selected_columns;\n-     for (idx_t i = 0; i < global_column_ids.size(); i++) {\n--        auto global_id = global_column_ids[i];\n-+        auto global_id = global_column_ids[i].GetPrimaryIndex();\n-         if (IsRowIdColumnId(global_id)) {\n-             continue;\n-         }\n-@@ -736,7 +740,7 @@ unique_ptr<MultiFileReaderGlobalState> DeltaMultiFileReader::InitializeGlobalSta\n- // in the parquet files, we just add null constant columns\n- static void CustomMulfiFileNameMapping(const string &file_name, const vector<LogicalType> &local_types,\n-                                         const vector<string> &local_names, const vector<LogicalType> &global_types,\n--                                        const vector<string> &global_names, const vector<column_t> &global_column_ids,\n-+                                        const vector<string> &global_names, const vector<ColumnIndex> &global_column_ids,\n-                                         MultiFileReaderData &reader_data, const string &initial_file,\n-                                         optional_ptr<MultiFileReaderGlobalState> global_state) {\n-     D_ASSERT(global_types.size() == global_names.size());\n-@@ -760,7 +764,7 @@ static void CustomMulfiFileNameMapping(const string &file_name, const vector<Log\n- \t\t\tcontinue;\n- \t\t}\n- \t\t// not constant - look up the column in the name map\n--\t\tauto global_id = global_column_ids[i];\n-+\t\tauto global_id = global_column_ids[i].GetPrimaryIndex();\n- \t\tif (global_id >= global_types.size()) {\n- \t\t\tthrow InternalException(\n- \t\t\t    \"MultiFileReader::CreatePositionalMapping - global_id is out of range in global_types for this file\");\n-@@ -800,7 +804,7 @@ static void CustomMulfiFileNameMapping(const string &file_name, const vector<Log\n- \n- void DeltaMultiFileReader::CreateNameMapping(const string &file_name, const vector<LogicalType> &local_types,\n-                                         const vector<string> &local_names, const vector<LogicalType> &global_types,\n--                                        const vector<string> &global_names, const vector<column_t> &global_column_ids,\n-+                                        const vector<string> &global_names, const vector<ColumnIndex> &global_column_ids,\n-                                         MultiFileReaderData &reader_data, const string &initial_file,\n-                                         optional_ptr<MultiFileReaderGlobalState> global_state) {\n-     // First call the base implementation to do most mapping\n-diff --git a/src/include/functions/delta_scan.hpp b/src/include/functions/delta_scan.hpp\n-index aac35cc..d90968b 100644\n---- a/src/include/functions/delta_scan.hpp\n-+++ b/src/include/functions/delta_scan.hpp\n-@@ -103,9 +103,9 @@ struct DeltaMultiFileReaderGlobalState : public MultiFileReaderGlobalState {\n- };\n- \n- struct DeltaMultiFileReader : public MultiFileReader {\n--    static unique_ptr<MultiFileReader> CreateInstance();\n-+    static unique_ptr<MultiFileReader> CreateInstance(const TableFunction &table_function);\n-     //! Return a DeltaSnapshot\n--    unique_ptr<MultiFileList> CreateFileList(ClientContext &context, const vector<string> &paths,\n-+    shared_ptr<MultiFileList> CreateFileList(ClientContext &context, const vector<string> &paths,\n-                    FileGlobOptions options) override;\n- \n-     //! Override the regular parquet bind using the MultiFileReader Bind. The bind from these are what DuckDB's file\n-@@ -119,19 +119,19 @@ struct DeltaMultiFileReader : public MultiFileReader {\n- \n-     void CreateNameMapping(const string &file_name, const vector<LogicalType> &local_types,\n-                       const vector<string> &local_names, const vector<LogicalType> &global_types,\n--                      const vector<string> &global_names, const vector<column_t> &global_column_ids,\n-+                      const vector<string> &global_names, const vector<ColumnIndex> &global_column_ids,\n-                       MultiFileReaderData &reader_data, const string &initial_file,\n-                       optional_ptr<MultiFileReaderGlobalState> global_state) override;\n- \n-     unique_ptr<MultiFileReaderGlobalState> InitializeGlobalState(ClientContext &context, const MultiFileReaderOptions &file_options,\n-                           const MultiFileReaderBindData &bind_data, const MultiFileList &file_list,\n-                           const vector<LogicalType> &global_types, const vector<string> &global_names,\n--                          const vector<column_t> &global_column_ids) override;\n-+                          const vector<ColumnIndex> &global_column_ids) override;\n- \n-     void FinalizeBind(const MultiFileReaderOptions &file_options, const MultiFileReaderBindData &options,\n-                                        const string &filename, const vector<string> &local_names,\n-                                        const vector<LogicalType> &global_types, const vector<string> &global_names,\n--                                       const vector<column_t> &global_column_ids, MultiFileReaderData &reader_data,\n-+                                       const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,\n-                                        ClientContext &context, optional_ptr<MultiFileReaderGlobalState> global_state) override;\n- \n-     //! Override the FinalizeChunk method\ndiff --git a/.github/patches/extensions/delta/multi_file_reader_column_definition.patch b/.github/patches/extensions/delta/multi_file_reader_column_definition.patch\ndeleted file mode 100644\nindex 81855d22e231..000000000000\n--- a/.github/patches/extensions/delta/multi_file_reader_column_definition.patch\n+++ /dev/null\n@@ -1,207 +0,0 @@\n-diff --git a/src/functions/delta_scan.cpp b/src/functions/delta_scan.cpp\n-index 7210382..6eaf0b9 100644\n---- a/src/functions/delta_scan.cpp\n-+++ b/src/functions/delta_scan.cpp\n-@@ -576,12 +576,13 @@ void DeltaMultiFileReader::BindOptions(MultiFileReaderOptions &options, MultiFil\n-     }\n- }\n- \n--void DeltaMultiFileReader::FinalizeBind(const MultiFileReaderOptions &file_options, const MultiFileReaderBindData &options,\n--                  const string &filename, const vector<string> &local_names,\n--                  const vector<LogicalType> &global_types, const vector<string> &global_names,\n--                  const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,\n--                  ClientContext &context, optional_ptr<MultiFileReaderGlobalState> global_state) {\n--    MultiFileReader::FinalizeBind(file_options, options, filename, local_names, global_types, global_names, global_column_ids, reader_data, context, global_state);\n-+void DeltaMultiFileReader::FinalizeBind(const MultiFileReaderOptions &file_options,\n-+\t                                     const MultiFileReaderBindData &options, const string &filename,\n-+\t                                     const vector<MultiFileReaderColumnDefinition> &local_columns,\n-+\t                                     const vector<MultiFileReaderColumnDefinition> &global_columns,\n-+\t                                     const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,\n-+\t                                     ClientContext &context, optional_ptr<MultiFileReaderGlobalState> global_state) {\n-+    MultiFileReader::FinalizeBind(file_options, options, filename, local_columns, global_columns, global_column_ids, reader_data, context, global_state);\n- \n-     // Handle custom delta option set in MultiFileReaderOptions::custom_options\n-     auto file_number_opt = file_options.custom_options.find(\"delta_file_number\");\n-@@ -608,9 +609,9 @@ void DeltaMultiFileReader::FinalizeBind(const MultiFileReaderOptions &file_optio\n-             if (IsRowIdColumnId(col_id)) {\n-                 continue;\n-             }\n--            auto col_partition_entry = file_metadata->partition_map.find(global_names[col_id]);\n-+            auto col_partition_entry = file_metadata->partition_map.find(global_columns[col_id].name);\n-             if (col_partition_entry != file_metadata->partition_map.end()) {\n--                auto &current_type = global_types[col_id];\n-+                auto &current_type = global_columns[col_id].type;\n-                 if (current_type == LogicalType::BLOB) {\n-                     reader_data.constant_map.emplace_back(i, Value::BLOB_RAW(col_partition_entry->second));\n-                 } else {\n-@@ -668,13 +669,10 @@ void DeltaMultiFileReaderGlobalState::SetColumnIdx(const string &column, idx_t i\n-     throw IOException(\"Unknown column '%s' found as required by the DeltaMultiFileReader\");\n- }\n- \n--unique_ptr<MultiFileReaderGlobalState> DeltaMultiFileReader::InitializeGlobalState(duckdb::ClientContext &context,\n--                                                                                   const duckdb::MultiFileReaderOptions &file_options,\n--                                                                                   const duckdb::MultiFileReaderBindData &bind_data,\n--                                                                                   const duckdb::MultiFileList &file_list,\n--                                                                                   const vector<duckdb::LogicalType> &global_types,\n--                                                                                   const vector<std::string> &global_names,\n--                                                                                   const vector<duckdb::ColumnIndex> &global_column_ids) {\n-+unique_ptr<MultiFileReaderGlobalState> DeltaMultiFileReader::InitializeGlobalState(ClientContext &context, const MultiFileReaderOptions &file_options,\n-+\t                      const MultiFileReaderBindData &bind_data, const MultiFileList &file_list,\n-+\t                      const vector<MultiFileReaderColumnDefinition> &global_columns,\n-+\t                      const vector<ColumnIndex> &global_column_ids) {\n-     vector<LogicalType> extra_columns;\n-     vector<pair<string, idx_t>> mapped_columns;\n- \n-@@ -686,7 +684,7 @@ unique_ptr<MultiFileReaderGlobalState> DeltaMultiFileReader::InitializeGlobalSta\n-             continue;\n-         }\n- \n--        auto &global_name = global_names[global_id];\n-+        auto &global_name = global_columns[global_id].name;\n-         selected_columns.insert({global_name, i});\n-     }\n- \n-@@ -738,17 +736,16 @@ unique_ptr<MultiFileReaderGlobalState> DeltaMultiFileReader::InitializeGlobalSta\n- \n- // This code is duplicated from MultiFileReader::CreateNameMapping the difference is that for columns that are not found\n- // in the parquet files, we just add null constant columns\n--static void CustomMulfiFileNameMapping(const string &file_name, const vector<LogicalType> &local_types,\n--                                        const vector<string> &local_names, const vector<LogicalType> &global_types,\n--                                        const vector<string> &global_names, const vector<ColumnIndex> &global_column_ids,\n--                                        MultiFileReaderData &reader_data, const string &initial_file,\n--                                        optional_ptr<MultiFileReaderGlobalState> global_state) {\n--    D_ASSERT(global_types.size() == global_names.size());\n--\tD_ASSERT(local_types.size() == local_names.size());\n-+static void CustomMulfiFileNameMapping(const string &file_name,\n-+\t                                 const vector<MultiFileReaderColumnDefinition> &local_columns,\n-+\t                                 const vector<MultiFileReaderColumnDefinition> &global_columns,\n-+\t                                 const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,\n-+\t                                 const string &initial_file,\n-+\t                                 optional_ptr<MultiFileReaderGlobalState> global_state) {\n- \t// we have expected types: create a map of name -> column index\n- \tcase_insensitive_map_t<idx_t> name_map;\n--\tfor (idx_t col_idx = 0; col_idx < local_names.size(); col_idx++) {\n--\t\tname_map[local_names[col_idx]] = col_idx;\n-+\tfor (idx_t col_idx = 0; col_idx < local_columns.size(); col_idx++) {\n-+\t\tname_map[local_columns[col_idx].name] = col_idx;\n- \t}\n- \tfor (idx_t i = 0; i < global_column_ids.size(); i++) {\n- \t\t// check if this is a constant column\n-@@ -765,32 +762,32 @@ static void CustomMulfiFileNameMapping(const string &file_name, const vector<Log\n- \t\t}\n- \t\t// not constant - look up the column in the name map\n- \t\tauto global_id = global_column_ids[i].GetPrimaryIndex();\n--\t\tif (global_id >= global_types.size()) {\n-+\t\tif (global_id >= global_columns.size()) {\n- \t\t\tthrow InternalException(\n--\t\t\t    \"MultiFileReader::CreatePositionalMapping - global_id is out of range in global_types for this file\");\n-+\t\t\t    \"MultiFileReader::CreatePositionalMapping - global_id is out of range in global_columns for this file\");\n- \t\t}\n--\t\tauto &global_name = global_names[global_id];\n-+\t\tauto &global_name = global_columns[global_id].name;\n- \t\tauto entry = name_map.find(global_name);\n- \t\tif (entry == name_map.end()) {\n- \t\t\tstring candidate_names;\n--\t\t\tfor (auto &local_name : local_names) {\n-+\t\t\tfor (auto &column : local_columns) {\n- \t\t\t\tif (!candidate_names.empty()) {\n- \t\t\t\t\tcandidate_names += \", \";\n- \t\t\t\t}\n--\t\t\t\tcandidate_names += local_name;\n-+\t\t\t\tcandidate_names += column.name;\n- \t\t\t}\n- \t\t\t// FIXME: this override is pretty hacky: for missing columns we just insert NULL constants\n--\t\t    auto &global_type = global_types[global_id];\n-+\t\t    auto &global_type = global_columns[global_id].type;\n- \t\t    Value val (global_type);\n- \t\t    reader_data.constant_map.push_back({i, val});\n- \t\t    continue;\n- \t\t}\n- \t\t// we found the column in the local file - check if the types are the same\n- \t\tauto local_id = entry->second;\n--\t\tD_ASSERT(global_id < global_types.size());\n--\t\tD_ASSERT(local_id < local_types.size());\n--\t\tauto &global_type = global_types[global_id];\n--\t\tauto &local_type = local_types[local_id];\n-+\t\tD_ASSERT(global_id < global_columns.size());\n-+\t\tD_ASSERT(local_id < local_columns.size());\n-+\t\tauto &global_type = global_columns[global_id].type;\n-+\t\tauto &local_type = local_columns[local_id].type;\n- \t\tif (global_type != local_type) {\n- \t\t\treader_data.cast_map[local_id] = global_type;\n- \t\t}\n-@@ -802,13 +799,14 @@ static void CustomMulfiFileNameMapping(const string &file_name, const vector<Log\n- \treader_data.empty_columns = reader_data.column_ids.empty();\n- }\n- \n--void DeltaMultiFileReader::CreateNameMapping(const string &file_name, const vector<LogicalType> &local_types,\n--                                        const vector<string> &local_names, const vector<LogicalType> &global_types,\n--                                        const vector<string> &global_names, const vector<ColumnIndex> &global_column_ids,\n--                                        MultiFileReaderData &reader_data, const string &initial_file,\n--                                        optional_ptr<MultiFileReaderGlobalState> global_state) {\n-+void DeltaMultiFileReader::CreateColumnMapping(const string &file_name,\n-+\t                                 const vector<MultiFileReaderColumnDefinition> &local_columns,\n-+\t                                 const vector<MultiFileReaderColumnDefinition> &global_columns,\n-+\t                                 const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,\n-+\t                                 const MultiFileReaderBindData &bind_data, const string &initial_file,\n-+\t                                 optional_ptr<MultiFileReaderGlobalState> global_state) {\n-     // First call the base implementation to do most mapping\n--    CustomMulfiFileNameMapping(file_name, local_types, local_names, global_types, global_names, global_column_ids, reader_data, initial_file, global_state);\n-+    CustomMulfiFileNameMapping(file_name, local_columns, global_columns, global_column_ids, reader_data, initial_file, global_state);\n- \n-     // Then we handle delta specific mapping\n-     D_ASSERT(global_state);\n-@@ -820,8 +818,8 @@ void DeltaMultiFileReader::CreateNameMapping(const string &file_name, const vect\n- \n-         // Build the name map\n-         case_insensitive_map_t<idx_t> name_map;\n--        for (idx_t col_idx = 0; col_idx < local_names.size(); col_idx++) {\n--            name_map[local_names[col_idx]] = col_idx;\n-+        for (idx_t col_idx = 0; col_idx < local_columns.size(); col_idx++) {\n-+            name_map[local_columns[col_idx].name] = col_idx;\n-         }\n- \n-         // Lookup the required column in the local map\n-diff --git a/src/include/functions/delta_scan.hpp b/src/include/functions/delta_scan.hpp\n-index d90968b..c3e71f2 100644\n---- a/src/include/functions/delta_scan.hpp\n-+++ b/src/include/functions/delta_scan.hpp\n-@@ -117,22 +117,24 @@ struct DeltaMultiFileReader : public MultiFileReader {\n-     void BindOptions(MultiFileReaderOptions &options, MultiFileList &files,\n-                                         vector<LogicalType> &return_types, vector<string> &names, MultiFileReaderBindData& bind_data) override;\n- \n--    void CreateNameMapping(const string &file_name, const vector<LogicalType> &local_types,\n--                      const vector<string> &local_names, const vector<LogicalType> &global_types,\n--                      const vector<string> &global_names, const vector<ColumnIndex> &global_column_ids,\n--                      MultiFileReaderData &reader_data, const string &initial_file,\n--                      optional_ptr<MultiFileReaderGlobalState> global_state) override;\n-+    void CreateColumnMapping(const string &file_name,\n-+\t                                 const vector<MultiFileReaderColumnDefinition> &local_columns,\n-+\t                                 const vector<MultiFileReaderColumnDefinition> &global_columns,\n-+\t                                 const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,\n-+\t                                 const MultiFileReaderBindData &bind_data, const string &initial_file,\n-+\t                                 optional_ptr<MultiFileReaderGlobalState> global_state) override;\n- \n-     unique_ptr<MultiFileReaderGlobalState> InitializeGlobalState(ClientContext &context, const MultiFileReaderOptions &file_options,\n--                          const MultiFileReaderBindData &bind_data, const MultiFileList &file_list,\n--                          const vector<LogicalType> &global_types, const vector<string> &global_names,\n--                          const vector<ColumnIndex> &global_column_ids) override;\n--\n--    void FinalizeBind(const MultiFileReaderOptions &file_options, const MultiFileReaderBindData &options,\n--                                       const string &filename, const vector<string> &local_names,\n--                                       const vector<LogicalType> &global_types, const vector<string> &global_names,\n--                                       const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,\n--                                       ClientContext &context, optional_ptr<MultiFileReaderGlobalState> global_state) override;\n-+\t                      const MultiFileReaderBindData &bind_data, const MultiFileList &file_list,\n-+\t                      const vector<MultiFileReaderColumnDefinition> &global_columns,\n-+\t                      const vector<ColumnIndex> &global_column_ids) override;\n-+\n-+    void FinalizeBind(const MultiFileReaderOptions &file_options,\n-+\t                                     const MultiFileReaderBindData &options, const string &filename,\n-+\t                                     const vector<MultiFileReaderColumnDefinition> &local_columns,\n-+\t                                     const vector<MultiFileReaderColumnDefinition> &global_columns,\n-+\t                                     const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,\n-+\t                                     ClientContext &context, optional_ptr<MultiFileReaderGlobalState> global_state) override;\n- \n-     //! Override the FinalizeChunk method\n-     void FinalizeChunk(ClientContext &context, const MultiFileReaderBindData &bind_data,\ndiff --git a/.github/patches/extensions/inet/require_autoloading.patch b/.github/patches/extensions/inet/require_autoloading.patch\ndeleted file mode 100644\nindex 4611e2f6a893..000000000000\n--- a/.github/patches/extensions/inet/require_autoloading.patch\n+++ /dev/null\n@@ -1,26 +0,0 @@\n-diff --git a/test/sql/test_inet_storage.test b/test/sql/test_inet_storage.test\n-index 6679439..7de7620 100644\n---- a/test/sql/test_inet_storage.test\n-+++ b/test/sql/test_inet_storage.test\n-@@ -5,7 +5,7 @@\n- require inet\n- \n- # FIXME: Make inet properly autoloadable\n--require no_extension_autoloading\n-+require no_extension_autoloading \"FIXME: to be reviewed whether this can be lifted\"\n- \n- # load the DB from disk\n- load __TEST_DIR__/store_inet.db\n-diff --git a/test/sql/test_ipv6_inet_storage.test b/test/sql/test_ipv6_inet_storage.test\n-index 5be1ff2..7dc1930 100644\n---- a/test/sql/test_ipv6_inet_storage.test\n-+++ b/test/sql/test_ipv6_inet_storage.test\n-@@ -25,7 +25,7 @@ CREATE VIEW iview AS SELECT INET '::1'\n- restart\n- \n- #FIXME: INET needs to be explicitly autoloaded on restart\n--require no_extension_autoloading\n-+require no_extension_autoloading \"FIXME: INET needs to be explicitly autoloaded on restart\"\n- \n- query IIIIII\n- DESCRIBE tbl\ndiff --git a/.github/patches/extensions/substrait/or_filter_pushdown.patch b/.github/patches/extensions/substrait/or_filter_pushdown.patch\ndeleted file mode 100644\nindex c971d4f7fcda..000000000000\n--- a/.github/patches/extensions/substrait/or_filter_pushdown.patch\n+++ /dev/null\n@@ -1,31 +0,0 @@\n-diff --git a/src/to_substrait.cpp b/src/to_substrait.cpp\n-index a466fb2..0dd0766 100644\n---- a/src/to_substrait.cpp\n-+++ b/src/to_substrait.cpp\n-@@ -1296,6 +1296,17 @@ substrait::Rel *DuckDBToSubstrait::TransformGet(LogicalOperator &dop) {\n- \tauto bind_info = dget.function.get_bind_info(dget.bind_data.get());\n- \tauto sget = get_rel->mutable_read();\n- \n-+\tif (!dget.table_filters.filters.empty()) {\n-+\n-+\t\tfor (auto it = dget.table_filters.filters.begin(); it != dget.table_filters.filters.end();) {\n-+\t\t\tif (it->second->filter_type == TableFilterType::OPTIONAL_FILTER) {\n-+\t\t\t\tit = dget.table_filters.filters.erase(it);\n-+\t\t\t} else {\n-+\t\t\t\t++it;\n-+\t\t\t}\n-+\t\t}\n-+\t}\n-+\n- \tif (!dget.table_filters.filters.empty()) {\n- \t\t// Pushdown filter\n- \t\tauto filter = CreateConjunction(dget.table_filters.filters,\n-@@ -1317,7 +1328,7 @@ substrait::Rel *DuckDBToSubstrait::TransformGet(LogicalOperator &dop) {\n- \t\tauto &column_ids = dget.GetColumnIds();\n- \t\tfor (auto col_idx : dget.projection_ids) {\n- \t\t\tauto struct_item = select->add_struct_items();\n--\t\t\tstruct_item->set_field(static_cast<int32_t>(column_ids[col_idx]));\n-+\t\t\tstruct_item->set_field(static_cast<int32_t>(column_ids[col_idx].GetPrimaryIndex()));\n- \t\t\t// FIXME do we need to set the child? if yes, to what?\n- \t\t}\n- \t\tprojection->set_allocated_select(select);\ndiff --git a/.github/workflows/ExtensionTrigger.yml b/.github/workflows/ExtensionTrigger.yml\ndeleted file mode 100644\nindex c858fb5768fc..000000000000\n--- a/.github/workflows/ExtensionTrigger.yml\n+++ /dev/null\n@@ -1,15 +0,0 @@\n-name: Extension Trigger\n-on:\n-  workflow_dispatch:\n-  repository_dispatch:\n-\n-jobs:\n-  build-linux:\n-    runs-on: ubuntu-latest\n-\n-    steps:\n-    - uses: actions/checkout@v4\n-\n-    - name: Trigger Substrait Extension\n-      run: |\n-        curl -XPOST -u \"${{secrets.PAT_USERNAME}}:${{secrets.PAT_TOKEN}}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" https://api.github.com/repos/duckdb/substrait/dispatches --data '{\"event_type\": \"build_application\"}'\n\\ No newline at end of file\ndiff --git a/.github/workflows/LinuxRelease.yml b/.github/workflows/LinuxRelease.yml\nindex ab8a4cd44a34..3cb87603fd80 100644\n--- a/.github/workflows/LinuxRelease.yml\n+++ b/.github/workflows/LinuxRelease.yml\n@@ -88,6 +88,25 @@ jobs:\n       shell: bash\n       run: ./build/release/duckdb -c \"PRAGMA platform;\"\n \n+    - name: Deploy\n+      shell: bash\n+      env:\n+        AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}\n+        AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}\n+      run: |\n+        python3 scripts/amalgamation.py\n+        zip -j duckdb_cli-linux-amd64.zip build/release/duckdb\n+        zip -j libduckdb-linux-amd64.zip build/release/src/libduckdb*.* src/amalgamation/duckdb.hpp src/include/duckdb.h\n+        zip -j libduckdb-src.zip src/amalgamation/duckdb.hpp src/amalgamation/duckdb.cpp src/include/duckdb.h src/include/duckdb_extension.h\n+        ./scripts/upload-assets-to-staging.sh github_release libduckdb-src.zip libduckdb-linux-amd64.zip duckdb_cli-linux-amd64.zip\n+\n+    - uses: actions/upload-artifact@v4\n+      with:\n+        name: duckdb-binaries-linux\n+        path: |\n+          libduckdb-linux-amd64.zip\n+          duckdb_cli-linux-amd64.zip\n+\n     - name: Test\n       shell: bash\n       if: ${{ inputs.skip_tests != 'true' }}\n@@ -109,26 +128,6 @@ jobs:\n         build/release/benchmark/benchmark_runner benchmark/tpch/sf1/q01.benchmark\n         build/release/duckdb -c \"COPY (SELECT 42) TO '/dev/stdout' (FORMAT PARQUET)\" | cat\n \n-    - name: Deploy\n-      shell: bash\n-      env:\n-        AWS_ACCESS_KEY_ID: ${{ secrets.S3_DUCKDB_STAGING_ID }}\n-        AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_DUCKDB_STAGING_KEY }}\n-      run: |\n-        python3 scripts/amalgamation.py\n-        zip -j duckdb_cli-linux-amd64.zip build/release/duckdb\n-        zip -j libduckdb-linux-amd64.zip build/release/src/libduckdb*.* src/amalgamation/duckdb.hpp src/include/duckdb.h\n-        zip -j libduckdb-src.zip src/amalgamation/duckdb.hpp src/amalgamation/duckdb.cpp src/include/duckdb.h src/include/duckdb_extension.h\n-        ./scripts/upload-assets-to-staging.sh github_release libduckdb-src.zip libduckdb-linux-amd64.zip duckdb_cli-linux-amd64.zip\n-\n-    - uses: actions/upload-artifact@v4\n-      with:\n-        name: duckdb-binaries-linux\n-        path: |\n-          libduckdb-linux-amd64.zip\n-          duckdb_cli-linux-amd64.zip\n-\n-\n  linux-release-aarch64:\n    # Builds binaries for linux_arm64\n    name: Linux (aarch64)\ndiff --git a/Makefile b/Makefile\nindex 5f3a37ec4c97..227d582e0e98 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -316,7 +316,6 @@ clean-python:\n debug: ${EXTENSION_CONFIG_STEP}\n \tmkdir -p ./build/debug && \\\n \tcd build/debug && \\\n-\techo ${DUCKDB_EXTENSION_SUBSTRAIT_PATH} && \\\n \tcmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${FORCE_32_BIT_FLAG} ${DISABLE_UNITY_FLAG} ${DISABLE_SANITIZER_FLAG} ${STATIC_LIBCPP} ${CMAKE_VARS} ${CMAKE_VARS_BUILD} -DDEBUG_MOVE=1 -DCMAKE_BUILD_TYPE=Debug ../.. && \\\n \tcmake --build . --config Debug\n \ndiff --git a/benchmark/micro/index/insert/insert_pk_fk.benchmark b/benchmark/micro/index/insert/insert_pk_fk.benchmark\nindex fcb4709f781e..0f70944c4603 100644\n--- a/benchmark/micro/index/insert/insert_pk_fk.benchmark\n+++ b/benchmark/micro/index/insert/insert_pk_fk.benchmark\n@@ -25,7 +25,11 @@ COPY warehouse FROM '${BENCHMARK_DIR}/sf1data/warehouse.csv' (FORMAT 'csv', head\n COPY inventory FROM '${BENCHMARK_DIR}/sf1data/inventory.csv' (FORMAT 'csv', header 1, delimiter ',', quote '\"');\n \n cleanup\n-CREATE OR REPLACE TABLE date_dim (d_date_sk Integer Not Null PRIMARY KEY, d_date_id String Not Null, d_date Date Not Null, d_month_seq Integer, d_week_seq Integer, d_quarter_seq Integer, d_year Integer, d_dow Integer, d_moy Integer, d_dom Integer, d_qoy Integer, d_fy_year Integer, d_fy_quarter_seq Integer, d_fy_week_seq Integer, d_day_name String, d_quarter_name String, d_holiday String, d_weekend String, d_following_holiday String, d_first_dom Integer, d_last_dom Integer, d_same_day_ly Integer, d_same_day_lq Integer, d_current_day String, d_current_week String, d_current_month String, d_current_quarter String, d_current_year String);\n-CREATE OR REPLACE TABLE warehouse (w_warehouse_sk Integer Not Null PRIMARY KEY, w_warehouse_id String Not Null, w_warehouse_name String, w_warehouse_sq_ft Integer, w_street_number String, w_street_name String, w_street_type String, w_suite_number String, w_city String, w_county String, w_state String, w_zip String, w_country String, w_gmt_offset Decimal(5,2));\n-CREATE OR REPLACE TABLE item (i_item_sk Integer Not Null PRIMARY KEY, i_item_id String Not Null, i_rec_start_date Date, i_rec_end_date Date, i_item_desc String, i_current_price Decimal(7,2), i_wholesale_cost Decimal(7,2), i_brand_id Integer, i_brand String, i_class_id Integer, i_class String, i_category_id Integer, i_category String, i_manufact_id Integer, i_manufact String, i_size String, i_formulation String, i_color String, i_units String, i_container String, i_manager_id Integer, i_product_name String);\n-CREATE OR REPLACE TABLE inventory (inv_date_sk Integer Not Null REFERENCES date_dim (d_date_sk), inv_item_sk Integer Not Null REFERENCES item (i_item_sk), inv_warehouse_sk Integer Not Null REFERENCES warehouse (w_warehouse_sk), inv_quantity_on_hand Integer);\n\\ No newline at end of file\n+DROP TABLE inventory;\n+DROP TABLE item;\n+DROP TABLE warehouse;\n+DROP TABLE date_dim;\n+CREATE TABLE date_dim (d_date_sk Integer Not Null PRIMARY KEY, d_date_id String Not Null, d_date Date Not Null, d_month_seq Integer, d_week_seq Integer, d_quarter_seq Integer, d_year Integer, d_dow Integer, d_moy Integer, d_dom Integer, d_qoy Integer, d_fy_year Integer, d_fy_quarter_seq Integer, d_fy_week_seq Integer, d_day_name String, d_quarter_name String, d_holiday String, d_weekend String, d_following_holiday String, d_first_dom Integer, d_last_dom Integer, d_same_day_ly Integer, d_same_day_lq Integer, d_current_day String, d_current_week String, d_current_month String, d_current_quarter String, d_current_year String);\n+CREATE TABLE warehouse (w_warehouse_sk Integer Not Null PRIMARY KEY, w_warehouse_id String Not Null, w_warehouse_name String, w_warehouse_sq_ft Integer, w_street_number String, w_street_name String, w_street_type String, w_suite_number String, w_city String, w_county String, w_state String, w_zip String, w_country String, w_gmt_offset Decimal(5,2));\n+CREATE TABLE item (i_item_sk Integer Not Null PRIMARY KEY, i_item_id String Not Null, i_rec_start_date Date, i_rec_end_date Date, i_item_desc String, i_current_price Decimal(7,2), i_wholesale_cost Decimal(7,2), i_brand_id Integer, i_brand String, i_class_id Integer, i_class String, i_category_id Integer, i_category String, i_manufact_id Integer, i_manufact String, i_size String, i_formulation String, i_color String, i_units String, i_container String, i_manager_id Integer, i_product_name String);\n+CREATE TABLE inventory (inv_date_sk Integer Not Null REFERENCES date_dim (d_date_sk), inv_item_sk Integer Not Null REFERENCES item (i_item_sk), inv_warehouse_sk Integer Not Null REFERENCES warehouse (w_warehouse_sk), inv_quantity_on_hand Integer);\n\\ No newline at end of file\ndiff --git a/data/csv/afl/3977/case_1.csv b/data/csv/afl/3977/case_1.csv\nnew file mode 100644\nindex 000000000000..e69b13d4d99e\nBinary files /dev/null and b/data/csv/afl/3977/case_1.csv differ\ndiff --git a/data/csv/afl/3977/case_10.csv b/data/csv/afl/3977/case_10.csv\nnew file mode 100644\nindex 000000000000..3e3072b344af\nBinary files /dev/null and b/data/csv/afl/3977/case_10.csv differ\ndiff --git a/data/csv/afl/3977/case_11.csv b/data/csv/afl/3977/case_11.csv\nnew file mode 100644\nindex 000000000000..2bbbf9e3e979\nBinary files /dev/null and b/data/csv/afl/3977/case_11.csv differ\ndiff --git a/data/csv/afl/3977/case_12.csv b/data/csv/afl/3977/case_12.csv\nnew file mode 100644\nindex 000000000000..cb565bfdaec3\nBinary files /dev/null and b/data/csv/afl/3977/case_12.csv differ\ndiff --git a/data/csv/afl/3977/case_13.csv b/data/csv/afl/3977/case_13.csv\nnew file mode 100644\nindex 000000000000..5a7f3751c723\nBinary files /dev/null and b/data/csv/afl/3977/case_13.csv differ\ndiff --git a/data/csv/afl/3977/case_14.csv b/data/csv/afl/3977/case_14.csv\nnew file mode 100644\nindex 000000000000..929e48820fa1\nBinary files /dev/null and b/data/csv/afl/3977/case_14.csv differ\ndiff --git a/data/csv/afl/3977/case_15.csv b/data/csv/afl/3977/case_15.csv\nnew file mode 100644\nindex 000000000000..7d3129840c30\nBinary files /dev/null and b/data/csv/afl/3977/case_15.csv differ\ndiff --git a/data/csv/afl/3977/case_16.csv b/data/csv/afl/3977/case_16.csv\nnew file mode 100644\nindex 000000000000..8798828e1063\nBinary files /dev/null and b/data/csv/afl/3977/case_16.csv differ\ndiff --git a/data/csv/afl/3977/case_17.csv b/data/csv/afl/3977/case_17.csv\nnew file mode 100644\nindex 000000000000..34d5e21f3462\nBinary files /dev/null and b/data/csv/afl/3977/case_17.csv differ\ndiff --git a/data/csv/afl/3977/case_18.csv b/data/csv/afl/3977/case_18.csv\nnew file mode 100644\nindex 000000000000..e522f4b29436\nBinary files /dev/null and b/data/csv/afl/3977/case_18.csv differ\ndiff --git a/data/csv/afl/3977/case_19.csv b/data/csv/afl/3977/case_19.csv\nnew file mode 100644\nindex 000000000000..30ebb2778d25\nBinary files /dev/null and b/data/csv/afl/3977/case_19.csv differ\ndiff --git a/data/csv/afl/3977/case_2.csv b/data/csv/afl/3977/case_2.csv\nnew file mode 100644\nindex 000000000000..c9619da772c4\n--- /dev/null\n+++ b/data/csv/afl/3977/case_2.csv\n@@ -0,0 +1,3 @@\n+line1;line1_2;line1_3\n+line2;line2_2;line2_3\n+line3;line3_2;Rine3_3\n\\ No newline at end of file\ndiff --git a/data/csv/afl/3977/case_20.csv b/data/csv/afl/3977/case_20.csv\nnew file mode 100644\nindex 000000000000..f2204791ecb6\nBinary files /dev/null and b/data/csv/afl/3977/case_20.csv differ\ndiff --git a/data/csv/afl/3977/case_21.csv b/data/csv/afl/3977/case_21.csv\nnew file mode 100644\nindex 000000000000..65bb610e83ac\nBinary files /dev/null and b/data/csv/afl/3977/case_21.csv differ\ndiff --git a/data/csv/afl/3977/case_22.csv b/data/csv/afl/3977/case_22.csv\nnew file mode 100644\nindex 000000000000..37b7add295ec\nBinary files /dev/null and b/data/csv/afl/3977/case_22.csv differ\ndiff --git a/data/csv/afl/3977/case_23.csv b/data/csv/afl/3977/case_23.csv\nnew file mode 100644\nindex 000000000000..9b430d53e975\nBinary files /dev/null and b/data/csv/afl/3977/case_23.csv differ\ndiff --git a/data/csv/afl/3977/case_24.csv b/data/csv/afl/3977/case_24.csv\nnew file mode 100644\nindex 000000000000..babff5de5745\nBinary files /dev/null and b/data/csv/afl/3977/case_24.csv differ\ndiff --git a/data/csv/afl/3977/case_25.csv b/data/csv/afl/3977/case_25.csv\nnew file mode 100644\nindex 000000000000..82de320acb61\nBinary files /dev/null and b/data/csv/afl/3977/case_25.csv differ\ndiff --git a/data/csv/afl/3977/case_26.csv b/data/csv/afl/3977/case_26.csv\nnew file mode 100644\nindex 000000000000..1cc82568e70f\nBinary files /dev/null and b/data/csv/afl/3977/case_26.csv differ\ndiff --git a/data/csv/afl/3977/case_27.csv b/data/csv/afl/3977/case_27.csv\nnew file mode 100644\nindex 000000000000..1378f74ba85a\nBinary files /dev/null and b/data/csv/afl/3977/case_27.csv differ\ndiff --git a/data/csv/afl/3977/case_28.csv b/data/csv/afl/3977/case_28.csv\nnew file mode 100644\nindex 000000000000..43efeafc5e0c\nBinary files /dev/null and b/data/csv/afl/3977/case_28.csv differ\ndiff --git a/data/csv/afl/3977/case_29.csv b/data/csv/afl/3977/case_29.csv\nnew file mode 100644\nindex 000000000000..d1f60b6d942d\nBinary files /dev/null and b/data/csv/afl/3977/case_29.csv differ\ndiff --git a/data/csv/afl/3977/case_3.csv b/data/csv/afl/3977/case_3.csv\nnew file mode 100644\nindex 000000000000..861d472b9048\n--- /dev/null\n+++ b/data/csv/afl/3977/case_3.csv\n@@ -0,0 +1,1 @@\n+3;line3_2;line3_3\n\\ No newline at end of file\ndiff --git a/data/csv/afl/3977/case_30.csv b/data/csv/afl/3977/case_30.csv\nnew file mode 100644\nindex 000000000000..2adc417728fe\nBinary files /dev/null and b/data/csv/afl/3977/case_30.csv differ\ndiff --git a/data/csv/afl/3977/case_31.csv b/data/csv/afl/3977/case_31.csv\nnew file mode 100644\nindex 000000000000..c4ea43561ad8\nBinary files /dev/null and b/data/csv/afl/3977/case_31.csv differ\ndiff --git a/data/csv/afl/3977/case_32.csv b/data/csv/afl/3977/case_32.csv\nnew file mode 100644\nindex 000000000000..49f5a28bb267\nBinary files /dev/null and b/data/csv/afl/3977/case_32.csv differ\ndiff --git a/data/csv/afl/3977/case_33.csv b/data/csv/afl/3977/case_33.csv\nnew file mode 100644\nindex 000000000000..0322ccc54c03\nBinary files /dev/null and b/data/csv/afl/3977/case_33.csv differ\ndiff --git a/data/csv/afl/3977/case_34.csv b/data/csv/afl/3977/case_34.csv\nnew file mode 100644\nindex 000000000000..51ad255bac34\nBinary files /dev/null and b/data/csv/afl/3977/case_34.csv differ\ndiff --git a/data/csv/afl/3977/case_35.csv b/data/csv/afl/3977/case_35.csv\nnew file mode 100644\nindex 000000000000..954fc2e5a2c6\nBinary files /dev/null and b/data/csv/afl/3977/case_35.csv differ\ndiff --git a/data/csv/afl/3977/case_36.csv b/data/csv/afl/3977/case_36.csv\nnew file mode 100644\nindex 000000000000..3ad931bfcbf5\nBinary files /dev/null and b/data/csv/afl/3977/case_36.csv differ\ndiff --git a/data/csv/afl/3977/case_37.csv b/data/csv/afl/3977/case_37.csv\nnew file mode 100644\nindex 000000000000..8161bace41ba\nBinary files /dev/null and b/data/csv/afl/3977/case_37.csv differ\ndiff --git a/data/csv/afl/3977/case_38.csv b/data/csv/afl/3977/case_38.csv\nnew file mode 100644\nindex 000000000000..974367b0402c\nBinary files /dev/null and b/data/csv/afl/3977/case_38.csv differ\ndiff --git a/data/csv/afl/3977/case_39.csv b/data/csv/afl/3977/case_39.csv\nnew file mode 100644\nindex 000000000000..e26b19f89d5e\nBinary files /dev/null and b/data/csv/afl/3977/case_39.csv differ\ndiff --git a/data/csv/afl/3977/case_4.csv b/data/csv/afl/3977/case_4.csv\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/data/csv/afl/3977/case_40.csv b/data/csv/afl/3977/case_40.csv\nnew file mode 100644\nindex 000000000000..c9f9ac5880ea\nBinary files /dev/null and b/data/csv/afl/3977/case_40.csv differ\ndiff --git a/data/csv/afl/3977/case_41.csv b/data/csv/afl/3977/case_41.csv\nnew file mode 100644\nindex 000000000000..41deeed6880f\nBinary files /dev/null and b/data/csv/afl/3977/case_41.csv differ\ndiff --git a/data/csv/afl/3977/case_42.csv b/data/csv/afl/3977/case_42.csv\nnew file mode 100644\nindex 000000000000..4b96cce041b1\nBinary files /dev/null and b/data/csv/afl/3977/case_42.csv differ\ndiff --git a/data/csv/afl/3977/case_43.csv b/data/csv/afl/3977/case_43.csv\nnew file mode 100644\nindex 000000000000..7dfe369df9f3\nBinary files /dev/null and b/data/csv/afl/3977/case_43.csv differ\ndiff --git a/data/csv/afl/3977/case_44.csv b/data/csv/afl/3977/case_44.csv\nnew file mode 100644\nindex 000000000000..921253950cd9\nBinary files /dev/null and b/data/csv/afl/3977/case_44.csv differ\ndiff --git a/data/csv/afl/3977/case_45.csv b/data/csv/afl/3977/case_45.csv\nnew file mode 100644\nindex 000000000000..cb19a6e72abe\nBinary files /dev/null and b/data/csv/afl/3977/case_45.csv differ\ndiff --git a/data/csv/afl/3977/case_46.csv b/data/csv/afl/3977/case_46.csv\nnew file mode 100644\nindex 000000000000..600fbba9ed59\nBinary files /dev/null and b/data/csv/afl/3977/case_46.csv differ\ndiff --git a/data/csv/afl/3977/case_47.csv b/data/csv/afl/3977/case_47.csv\nnew file mode 100644\nindex 000000000000..c0f1b5e52264\nBinary files /dev/null and b/data/csv/afl/3977/case_47.csv differ\ndiff --git a/data/csv/afl/3977/case_48.csv b/data/csv/afl/3977/case_48.csv\nnew file mode 100644\nindex 000000000000..271b06559604\nBinary files /dev/null and b/data/csv/afl/3977/case_48.csv differ\ndiff --git a/data/csv/afl/3977/case_49.csv b/data/csv/afl/3977/case_49.csv\nnew file mode 100644\nindex 000000000000..c5e4b4ad79e7\nBinary files /dev/null and b/data/csv/afl/3977/case_49.csv differ\ndiff --git a/data/csv/afl/3977/case_5.csv b/data/csv/afl/3977/case_5.csv\nnew file mode 100644\nindex 000000000000..20491b527321\n--- /dev/null\n+++ b/data/csv/afl/3977/case_5.csv\n@@ -0,0 +1,6 @@\n+\"blaaaaaaaaaaaaaa\"\n+\"bla\"SON'}\u0010\u00011{\"col{\"co\u0010\u00011{\"co {\"col_a\":1,\"col_b\"n\n+l_a\":1,\"col_b\"n\n+\"bla\"\n+\"bla\"\n+\"bla\n\\ No newline at end of file\ndiff --git a/data/csv/afl/3977/case_50.csv b/data/csv/afl/3977/case_50.csv\nnew file mode 100644\nindex 000000000000..d6f76352fb15\nBinary files /dev/null and b/data/csv/afl/3977/case_50.csv differ\ndiff --git a/data/csv/afl/3977/case_51.csv b/data/csv/afl/3977/case_51.csv\nnew file mode 100644\nindex 000000000000..26381fb96cd5\nBinary files /dev/null and b/data/csv/afl/3977/case_51.csv differ\ndiff --git a/data/csv/afl/3977/case_52.csv b/data/csv/afl/3977/case_52.csv\nnew file mode 100644\nindex 000000000000..3a23e7f3e6f9\nBinary files /dev/null and b/data/csv/afl/3977/case_52.csv differ\ndiff --git a/data/csv/afl/3977/case_53.csv b/data/csv/afl/3977/case_53.csv\nnew file mode 100644\nindex 000000000000..6e4ff8649f92\nBinary files /dev/null and b/data/csv/afl/3977/case_53.csv differ\ndiff --git a/data/csv/afl/3977/case_54.csv b/data/csv/afl/3977/case_54.csv\nnew file mode 100644\nindex 000000000000..31fc262001e9\nBinary files /dev/null and b/data/csv/afl/3977/case_54.csv differ\ndiff --git a/data/csv/afl/3977/case_55.csv b/data/csv/afl/3977/case_55.csv\nnew file mode 100644\nindex 000000000000..6f1fc5e7267b\nBinary files /dev/null and b/data/csv/afl/3977/case_55.csv differ\ndiff --git a/data/csv/afl/3977/case_56.csv b/data/csv/afl/3977/case_56.csv\nnew file mode 100644\nindex 000000000000..204024993a21\nBinary files /dev/null and b/data/csv/afl/3977/case_56.csv differ\ndiff --git a/data/csv/afl/3977/case_57.csv b/data/csv/afl/3977/case_57.csv\nnew file mode 100644\nindex 000000000000..018368c81520\nBinary files /dev/null and b/data/csv/afl/3977/case_57.csv differ\ndiff --git a/data/csv/afl/3977/case_58.csv b/data/csv/afl/3977/case_58.csv\nnew file mode 100644\nindex 000000000000..16a841938d93\nBinary files /dev/null and b/data/csv/afl/3977/case_58.csv differ\ndiff --git a/data/csv/afl/3977/case_59.csv b/data/csv/afl/3977/case_59.csv\nnew file mode 100644\nindex 000000000000..13c07cf93aab\nBinary files /dev/null and b/data/csv/afl/3977/case_59.csv differ\ndiff --git a/data/csv/afl/3977/case_6.csv b/data/csv/afl/3977/case_6.csv\nnew file mode 100644\nindex 000000000000..6288a88f9bbb\nBinary files /dev/null and b/data/csv/afl/3977/case_6.csv differ\ndiff --git a/data/csv/afl/3977/case_60.csv b/data/csv/afl/3977/case_60.csv\nnew file mode 100644\nindex 000000000000..d6553426568f\nBinary files /dev/null and b/data/csv/afl/3977/case_60.csv differ\ndiff --git a/data/csv/afl/3977/case_61.csv b/data/csv/afl/3977/case_61.csv\nnew file mode 100644\nindex 000000000000..7b05567042f8\nBinary files /dev/null and b/data/csv/afl/3977/case_61.csv differ\ndiff --git a/data/csv/afl/3977/case_62.csv b/data/csv/afl/3977/case_62.csv\nnew file mode 100644\nindex 000000000000..10279dfd189a\nBinary files /dev/null and b/data/csv/afl/3977/case_62.csv differ\ndiff --git a/data/csv/afl/3977/case_63.csv b/data/csv/afl/3977/case_63.csv\nnew file mode 100644\nindex 000000000000..b0ccb52ea284\nBinary files /dev/null and b/data/csv/afl/3977/case_63.csv differ\ndiff --git a/data/csv/afl/3977/case_64.csv b/data/csv/afl/3977/case_64.csv\nnew file mode 100644\nindex 000000000000..e86846eff9ab\nBinary files /dev/null and b/data/csv/afl/3977/case_64.csv differ\ndiff --git a/data/csv/afl/3977/case_65.csv b/data/csv/afl/3977/case_65.csv\nnew file mode 100644\nindex 000000000000..079b9f20503e\nBinary files /dev/null and b/data/csv/afl/3977/case_65.csv differ\ndiff --git a/data/csv/afl/3977/case_66.csv b/data/csv/afl/3977/case_66.csv\nnew file mode 100644\nindex 000000000000..f9d6ea9cbdbc\nBinary files /dev/null and b/data/csv/afl/3977/case_66.csv differ\ndiff --git a/data/csv/afl/3977/case_67.csv b/data/csv/afl/3977/case_67.csv\nnew file mode 100644\nindex 000000000000..3ab64528302b\nBinary files /dev/null and b/data/csv/afl/3977/case_67.csv differ\ndiff --git a/data/csv/afl/3977/case_68.csv b/data/csv/afl/3977/case_68.csv\nnew file mode 100644\nindex 000000000000..5a6226db2fe7\nBinary files /dev/null and b/data/csv/afl/3977/case_68.csv differ\ndiff --git a/data/csv/afl/3977/case_69.csv b/data/csv/afl/3977/case_69.csv\nnew file mode 100644\nindex 000000000000..5bee9991494c\nBinary files /dev/null and b/data/csv/afl/3977/case_69.csv differ\ndiff --git a/data/csv/afl/3977/case_7.csv b/data/csv/afl/3977/case_7.csv\nnew file mode 100644\nindex 000000000000..d50d8eb80470\nBinary files /dev/null and b/data/csv/afl/3977/case_7.csv differ\ndiff --git a/data/csv/afl/3977/case_70.csv b/data/csv/afl/3977/case_70.csv\nnew file mode 100644\nindex 000000000000..fa6d9c61c758\nBinary files /dev/null and b/data/csv/afl/3977/case_70.csv differ\ndiff --git a/data/csv/afl/3977/case_71.csv b/data/csv/afl/3977/case_71.csv\nnew file mode 100644\nindex 000000000000..060bfb01e9d7\nBinary files /dev/null and b/data/csv/afl/3977/case_71.csv differ\ndiff --git a/data/csv/afl/3977/case_72.csv b/data/csv/afl/3977/case_72.csv\nnew file mode 100644\nindex 000000000000..30de195449fd\nBinary files /dev/null and b/data/csv/afl/3977/case_72.csv differ\ndiff --git a/data/csv/afl/3977/case_73.csv b/data/csv/afl/3977/case_73.csv\nnew file mode 100644\nindex 000000000000..8497c94c4fb7\nBinary files /dev/null and b/data/csv/afl/3977/case_73.csv differ\ndiff --git a/data/csv/afl/3977/case_74.csv b/data/csv/afl/3977/case_74.csv\nnew file mode 100644\nindex 000000000000..821523693e88\nBinary files /dev/null and b/data/csv/afl/3977/case_74.csv differ\ndiff --git a/data/csv/afl/3977/case_75.csv b/data/csv/afl/3977/case_75.csv\nnew file mode 100644\nindex 000000000000..85a82566e50b\nBinary files /dev/null and b/data/csv/afl/3977/case_75.csv differ\ndiff --git a/data/csv/afl/3977/case_76.csv b/data/csv/afl/3977/case_76.csv\nnew file mode 100644\nindex 000000000000..5003b82705ea\nBinary files /dev/null and b/data/csv/afl/3977/case_76.csv differ\ndiff --git a/data/csv/afl/3977/case_77.csv b/data/csv/afl/3977/case_77.csv\nnew file mode 100644\nindex 000000000000..5500c5157c37\nBinary files /dev/null and b/data/csv/afl/3977/case_77.csv differ\ndiff --git a/data/csv/afl/3977/case_78.csv b/data/csv/afl/3977/case_78.csv\nnew file mode 100644\nindex 000000000000..79d4b5866755\nBinary files /dev/null and b/data/csv/afl/3977/case_78.csv differ\ndiff --git a/data/csv/afl/3977/case_79.csv b/data/csv/afl/3977/case_79.csv\nnew file mode 100644\nindex 000000000000..662127e59e1d\nBinary files /dev/null and b/data/csv/afl/3977/case_79.csv differ\ndiff --git a/data/csv/afl/3977/case_8.csv b/data/csv/afl/3977/case_8.csv\nnew file mode 100644\nindex 000000000000..f7c7b1399007\nBinary files /dev/null and b/data/csv/afl/3977/case_8.csv differ\ndiff --git a/data/csv/afl/3977/case_80.csv b/data/csv/afl/3977/case_80.csv\nnew file mode 100644\nindex 000000000000..114ad5fa8fec\nBinary files /dev/null and b/data/csv/afl/3977/case_80.csv differ\ndiff --git a/data/csv/afl/3977/case_81.csv b/data/csv/afl/3977/case_81.csv\nnew file mode 100644\nindex 000000000000..f99002c5263d\nBinary files /dev/null and b/data/csv/afl/3977/case_81.csv differ\ndiff --git a/data/csv/afl/3977/case_82.csv b/data/csv/afl/3977/case_82.csv\nnew file mode 100644\nindex 000000000000..e4e1da27c222\nBinary files /dev/null and b/data/csv/afl/3977/case_82.csv differ\ndiff --git a/data/csv/afl/3977/case_83.csv b/data/csv/afl/3977/case_83.csv\nnew file mode 100644\nindex 000000000000..96e6ee3273ea\nBinary files /dev/null and b/data/csv/afl/3977/case_83.csv differ\ndiff --git a/data/csv/afl/3977/case_84.csv b/data/csv/afl/3977/case_84.csv\nnew file mode 100644\nindex 000000000000..bc8e6a8b6ce8\nBinary files /dev/null and b/data/csv/afl/3977/case_84.csv differ\ndiff --git a/data/csv/afl/3977/case_85.csv b/data/csv/afl/3977/case_85.csv\nnew file mode 100644\nindex 000000000000..4b37abac1b10\nBinary files /dev/null and b/data/csv/afl/3977/case_85.csv differ\ndiff --git a/data/csv/afl/3977/case_86.csv b/data/csv/afl/3977/case_86.csv\nnew file mode 100644\nindex 000000000000..631178cb3f47\nBinary files /dev/null and b/data/csv/afl/3977/case_86.csv differ\ndiff --git a/data/csv/afl/3977/case_87.csv b/data/csv/afl/3977/case_87.csv\nnew file mode 100644\nindex 000000000000..6c582bc4e474\nBinary files /dev/null and b/data/csv/afl/3977/case_87.csv differ\ndiff --git a/data/csv/afl/3977/case_88.csv b/data/csv/afl/3977/case_88.csv\nnew file mode 100644\nindex 000000000000..ede26b592233\nBinary files /dev/null and b/data/csv/afl/3977/case_88.csv differ\ndiff --git a/data/csv/afl/3977/case_9.csv b/data/csv/afl/3977/case_9.csv\nnew file mode 100644\nindex 000000000000..dcee2aab7c28\nBinary files /dev/null and b/data/csv/afl/3977/case_9.csv differ\ndiff --git a/data/csv/afl/3981/case_0.csv b/data/csv/afl/3981/case_0.csv\nnew file mode 100644\nindex 000000000000..59390ec49901\nBinary files /dev/null and b/data/csv/afl/3981/case_0.csv differ\ndiff --git a/data/csv/afl/3981/case_1.csv b/data/csv/afl/3981/case_1.csv\nnew file mode 100644\nindex 000000000000..a8919290cb72\nBinary files /dev/null and b/data/csv/afl/3981/case_1.csv differ\ndiff --git a/data/csv/afl/3981/case_2.csv b/data/csv/afl/3981/case_2.csv\nnew file mode 100644\nindex 000000000000..2154533db63a\nBinary files /dev/null and b/data/csv/afl/3981/case_2.csv differ\ndiff --git a/data/csv/afl/3981/case_3.csv b/data/csv/afl/3981/case_3.csv\nnew file mode 100644\nindex 000000000000..7fb006c47f72\nBinary files /dev/null and b/data/csv/afl/3981/case_3.csv differ\ndiff --git a/data/csv/afl/3981/case_4.csv b/data/csv/afl/3981/case_4.csv\nnew file mode 100644\nindex 000000000000..d73484ecd2ed\nBinary files /dev/null and b/data/csv/afl/3981/case_4.csv differ\ndiff --git a/data/csv/afl/3981/case_5.csv b/data/csv/afl/3981/case_5.csv\nnew file mode 100644\nindex 000000000000..7e5b80b63bdd\nBinary files /dev/null and b/data/csv/afl/3981/case_5.csv differ\ndiff --git a/data/csv/afl/3981/case_6.csv b/data/csv/afl/3981/case_6.csv\nnew file mode 100644\nindex 000000000000..3a200bab6ab5\nBinary files /dev/null and b/data/csv/afl/3981/case_6.csv differ\ndiff --git a/data/csv/extra_delimiters.csv b/data/csv/extra_delimiters.csv\nnew file mode 100644\nindex 000000000000..df5e498074c6\n--- /dev/null\n+++ b/data/csv/extra_delimiters.csv\n@@ -0,0 +1,5 @@\n+a,b,c\n+1,2,3\n+1,2,3\n+1,2,3,4,5\n+1,2,3\n\\ No newline at end of file\ndiff --git a/data/csv/pollock/file_field_delimiter_0x20.csv b/data/csv/pollock/file_field_delimiter_0x20.csv\nnew file mode 100644\nindex 000000000000..464e76bb632f\n--- /dev/null\n+++ b/data/csv/pollock/file_field_delimiter_0x20.csv\n@@ -0,0 +1,84 @@\n+DATE TIME Qty PRODUCTID Price ProductType \"ProductDescription\" \"URL\" Comments\n+28/01/2018 00:00 2 MG-8769 $74.69 Men's Waterproof Hiking Boots \"These waterproof hiking boots for men are rugged enough for peak performance yet light and quick enough to keep feet from feeling weighed down.\" \"https://www.example.com/product/MG_8769.html\"\n+29/01/2018 00:15 0 RI-3895 $29.81 Light-Up Running Jacket \"The next level of weather protection. This light-up jacket resists the elements and keeps you visible in low-light conditions. From running, biking or walking the dog, the durable construction and innovative safety features won't let you down.\" \"https://www.example.com/product/RI_3895.html\"\n+30/01/2018 00:30 1 RI-8070 $80.08 Men's Ventilated Trail Shoes \"Great grip and super extra breathability make these amazing ventilated hikers ideal for warm, dry conditions.\" \"https://www.example.com/product/RI_8070.html\"\n+31/01/2018 00:45 1 RI-9546 $25.55 Switch Fly Rods \"This lightweight fly rod delivers outstanding performance and can be used as either a traditional one-handed rod or as a two-handed spey rod. Two-handed technique is ideal for larger rivers and situations where there isn't space for a backcast.\" \"https://www.example.com/product/RI_9546.html\"\n+13/02/2018 01:00 9 CC-9259 $48.00 \"Throw Pillow, Wooden Paddles\" \"Add a pop of paddling fun to your bed, chair or sofa with this whimsical throw pillow, handhooked on front for a timeless style.\" \"https://www.example.com/product/CC_9259.html\"\n+14/02/2018 01:15 1 CC-1697 $34.22 Men's Heavy-Duty Suspenders \"These tough Men's Heavy-Duty Suspenders are made to hold up heavy wool pants without stretching in any way, shape or form.\" \"https://www.example.com/product/CC_1697.html\"\n+15/02/2018 01:30 2 RI-6052 $89.34 Organic Textured Cotton Towel \"All the softness and absorbency you've come to expect from our towels, in certified organic cotton for natural, ecofriendly comfort.\" \"https://www.example.com/product/RI_6052.html\"\n+16/02/2018 01:45 2 YY-3522 $19.34 \"Cycling Jersey, Short-Sleeve\" \"Designed with lots of performance features, plus a semi-form-fitting profile, this cycling jersey delivers all-day comfort and serious style.\" \"https://www.example.com/product/YY_3522.html\"\n+17/02/2018 02:00 0 YY-5315 $45.39 \"Men's Silk Underwear, Crewneck\" \"For strong, lightweight comfort without bulk, our men's silk crewneck makes for an ideal first layer against the cold from slope to lodge to shoveling snow.\" \"https://www.example.com/product/YY_5315.html\"\n+18/02/2018 02:15 2 BH-9827 $78.07 \"All-Weather Dining Table, Round 48\"\"\" \"Made in the USA to our exacting standards, this round patio table is durable enough to weather the elements year-round.\" \"https://www.example.com/product/BH_9827.html\"\n+19/02/2018 02:30 1 BH-7885 $52.45 Women's No-Show Socks \"The warmth and comfort of wool, these socks are designed in a minimal, no-show style that is of pure elegance and design.\" \"https://www.example.com/product/BH_7885.html\"\n+20/02/2018 02:45 1 BH-7531 $48.08 Women's  Fly Rod 8 Wt. \"Amazingly crisp action and a remarkably light feel in our 8\\'9\"\" length fly rod, impeccably designed for her.\" \"https://www.example.com/product/BH_7531.html\"\n+21/02/2018 03:00 3 BH-3190 $65.38 Men's Down Vest \"Our best-value down vest is packed with ultralight 650-fill for warmth and protection, even when wet.\" \"https://www.example.com/product/BH_3190.html\"\n+22/02/2018 03:15 3 HK-3372 $6.45 Tropic Cap \"Made with the same tropical fabric as our bestselling shirts, this wonderful cap provides UPF 50+ sun protection.\" \"https://www.example.com/product/HK_3372.html\"\n+23/02/2018 03:30 1 HK-5716 $46.01 Women's 3-in-1 Jacket \"Our women's 3-in-1 jacket offers three times the value and versatility for the solution to year-round weather conditions.\" \"https://www.example.com/product/HK_5716.html\"\n+24/02/2018 03:45 1 AN-5096 $33.83 \"Organic Cotton Oxford Shirt, Plaid\" \"Introducing our most easygoing oxford ever. This plaid oxford shirt is made from the purest cotton organically sourced and 100% certified and washed for a super laid-back look and feel.\" \"https://www.example.com/product/AN_5096.html\"\n+25/02/2018 04:00 3 AN-8641 $65.18 Kids' Good Mocs \"Just like Mom and Dad our shearling slippers wrap completely around little feet for soft, cozy comfort.\" \"https://www.example.com/product/AN_8641.html\"\n+26/02/2018 04:15 2 ZN-4777 $34.14 \"Kids' Sweater Fleece, Hooded\" \"With its fleece warmth, sweatshirt styling, and fun colorblocking, our updated kids' Sweater Fleece is the perfect layer for the classroom and the world outside of it.\" \"https://www.example.com/product/ZN_4777.html\"\n+27/02/2018 04:30 0 ZN-2049 $83.32 \"Women's Hikers, Low Ventilated\" \"Youll be light on your feet with these ultralightweight, breathable hikers. But dont let the comfortable, airy construction fool you: they deliver all the protection and stability you expect from a trail shoe.\" \"https://www.example.com/product/ZN_2049.html\"\n+28/02/2018 04:45 3 GN-5043 $69.07 \"Women's Comfort Cycling Jersey, Short-Sleeve\" \"This women's quarter-zip cycling shirt is our most popular cycling jersey, thanks in part to sun protection that's built right in. With a more relaxed look and fit than most cycling apparel.\" \"https://www.example.com/product/GN_5043.html\"\n+13/03/2018 05:00 0 GN-9860 $24.86 \"Men's Boxer, 5\"\" Inseam\" \"Perfect for a day on the bonefish flats, these boxers are breathable, quick drying and just may become your everyday underwear.\" \"https://www.example.com/product/GN_9860.html\"\n+14/03/2018 05:15 1 YY-2600 $90.99 \"Kids' Mountain Bike, 24\"\"\" \"An easy-to-ride mountain bike that not only proves to be great fun for all the kids, but also offers great durability and stability.\" \"https://www.example.com/product/YY_2600.html\"\n+15/03/2018 05:30 1 YY-4017 $69.52 Leather Sofa \"Our American-made leather sofa exhibits quality craftsmanship in every detail and delivers lasting comfort through and through.\" \"https://www.example.com/product/YY_4017.html\"\n+16/03/2018 05:45 1 RI-9156 $53.62 Girls' Waterproof 3-in-1 Jacket \"This 3-in-1 girls' jacket offers three great options for one great price, so she'll stay warm, dry and comfortable whatever the weather.\" \"https://www.example.com/product/RI_9156.html\"\n+17/03/2018 06:00 1 RI-9121 $9.81 Women's No-Show Socks \"These ankle socks provide all-day comfort and no-show style. In a naturally breathable merino-wool blend that wicks moisture and dries quickly.\" \"https://www.example.com/product/RI_9121.html\"\n+18/03/2018 06:15 1 RI-7338 $43.53 Kids' Hat \"Let your kids show their spirit for adventure with our fun trucker-style hat, ready to go from trailhead to bus stop and back.\" \"https://www.example.com/product/RI_7338.html\"\n+19/03/2018 06:30 0 ZN-6172 $30.79 Men's Snowshoes Set \"Take your favorite hikes all winter long with these easy-to-use snowshoes. Best of all, this boxed set helps you save money and includes everything you need to get started: snowshoes, hiking poles and a tough snowshoe storage bag.\" \"https://www.example.com/product/ZN_6172.html\"\n+20/03/2018 06:45 0 YY-2057 $43.4 \"Tippet Material, Pro Freshwater\" \"This tippet is field tested in a variety of situations, from spring creeks to tidal rips, and is guaranteed to perform under the worst weather conditions.\" \"https://www.example.com/product/YY_2057.html\"\n+21/03/2018 07:00 1 BH-5486 $0.91 Women's Wet Suit \"Ideal for kayaking, paddle boarding and surfing, this wetsuit covers your core in 3 mm neoprene for maximum warmth, while 2 mm thickness on your arms and legs ensures your range of motion is at its best.\" \"https://www.example.com/product/BH_5486.html\"\n+22/03/2018 07:15 3 BH-1732 $65.46 \"Waterproof Boots, Tall\" \"Industry-leading insulation and innovative construction allow us to make these super-warm winter boots 50% lighter than traditional boots.\" \"https://www.example.com/product/BH_1732.html\"\n+24/03/2018 07:45 0 BH-4480 $51.42 \"Heated Insoles\" \"This pair of water-resistant, remote-controlled heated insoles features thermal technology engineered to keep your feet warm, not hot, to avoid sweating and freezing.\" \"https://www.example.com/product/BH_4480.html\"\n+25/03/2018 08:00 0 BH-1061 $62.55 \"Base Layer, Pants\" \"Our exclusive wool performance fabric makes these pants an ideal base layer for cool-weather sports.\" \"https://www.example.com/product/BH_1061.html\"\n+26/03/2018 08:15 2 ON-7017 $0.9 \"Quarter-Zip Hoodie, Camo\" \"This camo quarter-zip hoodie is an extremely versatile mid layer thats great for all hunting seasons. Its made from warm, breathable fleece and features an innovative lightweight hood with a built-in face mask.\" \"https://www.example.com/product/ON_7017.html\"\n+27/03/2018 08:30 3 ON-1026 $20.65 \"Tee, Traditional Fit, Short-Sleeve\" \"Made of soft cotton that resists wrinkles, stains, shrinking, fading and pilling, our resilient tee keeps its shape wash after wash.\" \"https://www.example.com/product/ON_1026.html\"\n+28/03/2018 08:45 4 AN-6109 $18.58 Kids' Pullover \"Just as good today as it was back then, our kids' retro fleece pullover combines our soft, warm, stretchy fleece with cool colorblock designs and our logo.\" \"https://www.example.com/product/AN_6109.html\"\n+29/03/2018 09:00 3 AN-9272 $57.75 Women's Essential Running Vest \"This lightweight fitness vest allows excellent freedom of movement while resisting wind and light rain, making it perfect for running, hiking, cycling or walks around town.\" \"https://www.example.com/product/AN_9272.html\"\n+30/03/2018 09:15 0 AN-1646 $78.74 \"Rolling Duffle, Extra-Large\" \"We kept the heritage-inspired design of our spacious rolling duffle, then added thoughtful details like easy-rolling spinner wheels.\" \"https://www.example.com/product/AN_1646.html\"\n+31/03/2018 09:30 0 AN-9388 $64.02 \"Linen Shirt, Slightly Fitted Short-Sleeve Stripe\" \"Lightweight and breathable, this colorful shirt will be a warm-weather mainstay year after year. Light and comfortable, perfect for summer.\" \"https://www.example.com/product/AN_9388.html\"\n+13/04/2018 11:00 4 YY-9611 $3.79 \"Women's Hunting Shoes, 10\"\"\" \"The original boot, made since 1912. Now with even more protection from cold, wet weather, with the addition of a waterproof liner and warm insulation.\" \"https://www.example.com/product/YY_9611.html\"\n+14/04/2018 11:15 12 RI-1639 $25.14 \"Men's Loafers, Leather/Nubuck\" \"Our finest leather Men's slip-on penny loafers are handstitched from supple, fine-grain nubuck leather that molds to your foot for a custom-like feel.\" \"https://www.example.com/product/RI_1639.html\"\n+16/04/2018 11:45 15 RI-3655 $45.12 Kids' Sneaker \"Designed with FAST fun in mind, these super cool kids' sneakers feature a sleek, engineered mesh upper and award-winning cushioning for all-day, action-packed comfort.\" \"https://www.example.com/product/RI_3655.html\"\n+17/04/2018 12:00 16 CC-5271 $38.81 \"Kids' Shirt, Short Sleeve, Graphic\" \"With its great coverage and UPF 50+ sun protection, this quick-drying kids' rashguard blocks the sun, but not the summer fun.\" \"https://www.example.com/product/CC_5271.html\"\n+18/04/2018 12:15 9 CC-9916 $75.06 Men's Biking Shorts \"Designed specifically for mountain biking, the cycling shorts are built with performance features such as breathable, moisture-wicking fabric and chamois padding, with a casual, relaxed fit for comfort on and off the bike.\" \"https://www.example.com/product/CC_9916.html\"\n+19/04/2018 12:30 9 YY-7121 $67.64 Backpacking Stove Kit \"Our lightest, most compact 2-person cookset nests together to save space. Includes a stove, a two-liter hard anodized aluminum pot with clear strainer lid, 2 bowls, 2-12.5 oz. double-wall insulated mugs and 2 folding sporks.\" \"https://www.example.com/product/YY_7121.html\"\n+20/04/2018 12:45 15 YY-8954 $64.19 \"Travel Lock, Combination Cable\" \"This luggage lock helps keep your belongings secure while traveling. Allows your luggage to go through airport security and arrive at your destination fully locked.\" \"https://www.example.com/product/YY_8954.html\"\n+21/04/2018 13:00 8 YY-8239 $72 Men's Boots \"Here, when we call something good we mean it: just one touch of these soft men's boot slippers and you'll understand why.\" \"https://www.example.com/product/YY_8239.html\"\n+23/04/2018 13:30 26 BH-2268 $43.61 Men's Shorts \"We used performance-packed swim fabric to create board shorts that keep up during the most rigorous adventures, with enough style for heading into town afterward.\" \"https://www.example.com/product/BH_2268.html\"\n+25/04/2018 14:00 16 BH-4482 $53.95 Portable Game Center \"This portable game center will provide hours of fun at your barbecue, tailgate, beach trip and more. With easy set-up and take-down, you can take it anywhere.\" \"https://www.example.com/product/BH_4482.html\"\n+26/04/2018 14:15 22 HK-4191 $3.35 \"Women's Waterproof Hiking Boots, Leather Mesh\" \"The classic alpine hiker is back, with retro style and all-day comfort you'll want to wear beyond the trail.\" \"https://www.example.com/product/HK_4191.html\"\n+29/04/2018 15:00 30 AN-5829 $2.86 Women's Booties \"Inspired by the legendary comfort of our slippers, we designed these warm slipper booties to be so cozy you'll never want to take them off. The durable knit lambswool upper is lined with toasty fleece that wraps your feet in luxury.\" \"https://www.example.com/product/AN_5829.html\"\n+13/05/2018 18:45 24 BH-7941 $64.17 Cotton/Linen Ragg Sweater \"An ultracozy take on the classic ragg sweater. We created this swing sweater from beautifully marled cotton with linen for amazing texture.\" \"https://www.example.com/product/BH_7941.html\"\n+15/05/2018 19:15 18 HK-8027 $26.62 Camp Stove Bundle with Coffee Press \"This unique bundle includes the wood-burning camp Stove, Portable Grill, Kettle Pot with Coffee Press and a USB Lantern.\" \"https://www.example.com/product/HK_8027.html\"\n+18/05/2018 20:00 30 AN-2099 $64.14 Sandals \"An exclusive, these colorblocked sandals are an absolutely beautiful and incredibly comfortable addition to your warm-weather wardrobe.\" \"https://www.example.com/product/AN_2099.html\"\n+19/05/2018 20:15 33 AN-5746 $83.75 \"Flannel Tunic, Plaid\" \"Our most rugged flannel shirt is also one of the softest - double brushed for an exceptional feel. Made from heavyweight organic cotton in a flattering longer length.\" \"https://www.example.com/product/AN_5746.html\"\n+20/05/2018 20:30 16 AN-5136 $39.41 Ragg Wool Hat \"Our traditional ragg wool hat is thickly knit from soft lambswool yarns and keeps on insulating, even when wet.\" \"https://www.example.com/product/AN_5136.html\"\n+21/05/2018 20:45 21 ZN-5503 $88.45 Coffee Table \"Spend more time relaxing outside with our low-maintenance coffee table. Like all of our furniture, it won't chip, peel, warp or crack.\" \"https://www.example.com/product/ZN_5503.html\"\n+22/05/2018 21:00 19 ZN-4103 $68.74 Baseball Hat \"This sun-blocking baseball hat features a rear flap for extra coverage. It provides UPF 50+ rated sun protection, the highest possible.\" \"https://www.example.com/product/ZN_4103.html\"\n+23/05/2018 21:15 2 GN-1741 $48.05 \"Beach Chair, Print\" \"With four reclining positions and backpack straps for easy carrying, this is the most comfortable and convenient folding beach chair we've ever offered.\" \"https://www.example.com/product/GN_1741.html\"\n+24/05/2018 21:30 1 GN-5567 $26.87 Men's Reversible Jacket \"Designed to be the ultimate reversible jacket with two great options. We took the cozy comfort of plush fleece and combined it with the ultralight warmth of our insulation.\" \"https://www.example.com/product/GN_5567.html\"\n+25/05/2018 21:45 0 GN-3028 $12.98 \"Swimwear, Print\" \"With flattering coverage, amazing stay-put shape and quick-dry comfort, our best-value one-piece bathing suit is designed for sun and surf. Made from premium Italian swim fabric in fun wave print.\" \"https://www.example.com/product/GN_3028.html\"\n+26/05/2018 22:00 2 YY-4147 $11.06 Women's Sandals \"You wont find a sandal with better control or support than this. Its loaded with features like secure double-strap webbing, a podiatrist-certified footbed and a toe loop for extra forefoot stability.\" \"https://www.example.com/product/YY_4147.html\"\n+27/05/2018 22:15 0 WH-3118 $31.06 Men's Ear Warmer \"Designed to resist wind and water, these click-to-fit ear warmers are made with insulation for even more warmth during outdoor activities.\" \"https://www.example.com/product/WH_3118.html\"\n+28/05/2018 22:30 1 WH-2859 $72.91 \"Tee, Traditional Fit, Long-Sleeve\" \"Made of soft cotton that resists wrinkles, stains, shrinking, fading and pilling, our resilient tee keeps its shape wash after wash.\" \"https://www.example.com/product/WH_2859.html\"\n+30/05/2018 23:00 1 CC-2828 $79.36 \"Camp Light, Two-Pack\" \"Collapsed or extended, our lightweight camp lights take up minimal space, making them handy to bring on hikes and camping trips, and easy to keep on hand in case of a power outage.\" \"https://www.example.com/product/CC_2828.html\"\n+31/05/2018 15:30 3 HK-9973 $39.49 Women's Ventilated Hiking Shoes \"This lightweight, ventilated version of the best-selling trail hiker features less volume in the forefoot for a more athletic fit. It's also equipped with a grippier outsole that excels in both wet and dry conditions.\" \"https://www.example.com/product/HK_9973.html\"\n+21/06/2018 23:15 1 CC-5815 $13.57 Field Watch \"Inspired by watches worn by World War II infantrymen, this updated design makes it easy to tell time at a glance. New smaller size is perfect for men and women who want a classic field watch that's perfect for all occasions, both indoor and outdoor.\" \"https://www.example.com/product/CC_5815.html\"\n+22/06/2018 23:30 1 YY-2632 $27.84 Women's Capris \"These great cargo capris pants for women are designed for performance, with built-in UPF 50+ to keep you cool, dry, and protected from the sun while you're out enjoying it.\" \"https://www.example.com/product/YY_2632.html\"\n+23/06/2018 23:45 0 YY-1196 $90.15 Girls' Tee \"This girls' fitness tee controls moisture and odor: all at a great price, perfect for camp, sports and play.\" \"https://www.example.com/product/YY_1196.html\"\n+24/06/2018 00:00 0 YY-1894 $13.52 \"Men's Socks, Two-Pack\" \"Made of moisture-wicking merino wool with nylon for durability and spandex for shape-retaining stretch, these soft, form-fitting men's chino socks offer all-day comfort at home or work.\" \"https://www.example.com/product/YY_1894.html\"\n+26/06/2018 00:30 0 BH-1861 $82.12 Women's Socks \"These ankle socks provide all-day comfort and no-show style. In a naturally breathable merino-wool blend that wicks moisture and dries quickly.\" \"https://www.example.com/product/BH_1861.html\"\n+27/06/2018 17:15 1 RI-7731 $65.13 \"Ceramic Lamp, Stripe\" \"Bring light, texture and classic style to any space with this coast-inspired striped ceramic table lamp.\" \"https://www.example.com/product/RI_7731.html\"\n+29/06/2018 10:00 1 ZN-7072 $88.21 Kids' High Handles Boots \"Parents and little ones alike swear by this warm weatherproofing. Just pull these kids' handle boots on and seal the cold, wet weather out.\" \"https://www.example.com/product/ZN_7072.html\"\n+30/06/2018 10:15 2 GN-1306 $78.95 National Park Patch \"Were proud to offer the National Park collectible patch, featuring an embroidered scene of a breathtaking, rugged coastline.\" \"https://www.example.com/product/GN_1306.html\"\n+13/07/2018 10:30 1 GN-7767 $25.88 Men's Versatile Tote \"We combined details with your everyday needs to create this: an exceptional quality tote with a laptop compartment and enough room for all your other essentials.\" \"https://www.example.com/product/GN_7767.html\"\n+14/07/2018 10:45 0 GN-4624 $24.52 Summer Slip-On Sneakers \"Run a 5K, rock hop at the shore, window shop in town - these summer slip-on sneakers do it all. They're breathable, quick drying and comfortably cushioned for whatever your day has in store.\" \"https://www.example.com/product/GN_4624.html\"\n+15/07/2018 16:45 20 RI-7676 $7.49 Kids' Hydration Pack \"Designed after the most popular adult hydration pack, this hydration pack measures up to your kids' trail hydration needs and your highest expectations.\" \"https://www.example.com/product/RI_7676.html\"\n+16/07/2018 17:00 3 RI-5314 $76.26 Fishing Sunglasses \"These sunglasses are made for fishermen with large faces. The taller lens shape and thicker temples provide additional sun protection, great for fishermen who spend a lot of time on the water.\" \"https://www.example.com/product/RI_5314.html\"\n+17/07/2018 16:30 0 YY-7734 $73.42 Havana Hat \"The perfect complement to any summertime style: this fedora hat offers UPF 50+ protection and won't crunch or bend during your travels.\" \"https://www.example.com/product/YY_7734.html\"\n+18/07/2018 17:30 1 BH-7744 $56.76 Headlamp \"With proprietary construction, a front profile of only 9mm and weighing only 2.4 oz., this bright headlamp sits comfortably on your forehead without bouncing or slipping.\" \"https://www.example.com/product/BH_7744.html\"\n+19/07/2018 17:45 1 YY-8580 $6.31 \"Underwear, Print\" \"Our built-in fabric technology traps your body heat and keeps you up to 7 degrees warmer: for comfort as the days and months get colder.\" \"https://www.example.com/product/YY_8580.html\"\n+20/07/2018 18:00 1 BH-7918 $63.7 Kids' Jumper Hat \"With ski-inspired style and a toasty, fleece-lined earband, staying warm on the slopes has never looked cooler.\" \"https://www.example.com/product/BH_7918.html\"\n+22/07/2018 18:30 1 BH-1085 $85.07 Heated Insoles \"This pair of water-resistant, remote-controlled heated insoles features thermal technology engineered to keep your feet warm, not hot, to avoid sweating and freezing.\" \"https://www.example.com/product/BH_1085.html\"\n+24/07/2018 16:00 6 GN-2043 $23.25 \"Men's Boots, 10\"\" Shearling-Lined\" \"With waterproof leather outside and soft, plush shearling inside, our lined Boots are very possibly the coolest, warmest boots ever. Handcrafted right here.\" \"https://www.example.com/product/GN_2043.html\"\ndiff --git a/data/csv/pollock/file_quotation_char_0x27.csv b/data/csv/pollock/file_quotation_char_0x27.csv\nnew file mode 100644\nindex 000000000000..d8461eb582b4\n--- /dev/null\n+++ b/data/csv/pollock/file_quotation_char_0x27.csv\n@@ -0,0 +1,1 @@\n+31/05/2018,15:30,3,HK-9973,$39.49,Women's Ventilated Hiking Shoes,'This lightweight, ventilated version of the best-selling trail hiker features less volume in the forefoot for a more athletic fit. It's also equipped with a grippier outsole that excels in both wet and dry conditions.','https://www.example.com/product/HK_9973.html',\ndiff --git a/data/json/internal_4014.json b/data/json/internal_4014.json\nnew file mode 100644\nindex 000000000000..ad8744628ff8\n--- /dev/null\n+++ b/data/json/internal_4014.json\n@@ -0,0 +1,15 @@\n+{\n+   \"s4\": {\"1\": [1]},\n+ \"a\": {\n+    \"s1\": {\"1\": null},\n+    \"s2\": {\"1\": {}},\n+    \"s3\": {\"1\": \"1\"},\n+    \"s4\": {\"1\": [1]},\n+    \"s5\": {\"1\": 1},\n+    \"s6\": {\"1\": \"1\"},\n+    \"s7\": {\"1\": [1]},\n+    \"s8\": {\"1\": 1},\n+    \"s9\": {\"1\": \"1\"},\n+    \"s10\": {\"1\": [1]}\n+  }\n+}\n\\ No newline at end of file\ndiff --git a/extension/autocomplete/autocomplete_extension.cpp b/extension/autocomplete/autocomplete_extension.cpp\nindex 99087fe6a2f9..62db1f89ae63 100644\n--- a/extension/autocomplete/autocomplete_extension.cpp\n+++ b/extension/autocomplete/autocomplete_extension.cpp\n@@ -481,11 +481,7 @@ std::string AutocompleteExtension::Name() {\n }\n \n std::string AutocompleteExtension::Version() const {\n-#ifdef EXT_VERSION_AUTOCOMPLETE\n-\treturn EXT_VERSION_AUTOCOMPLETE;\n-#else\n-\treturn \"\";\n-#endif\n+\treturn DefaultVersion();\n }\n \n } // namespace duckdb\n@@ -496,7 +492,7 @@ DUCKDB_EXTENSION_API void autocomplete_init(duckdb::DatabaseInstance &db) {\n }\n \n DUCKDB_EXTENSION_API const char *autocomplete_version() {\n-\treturn duckdb::DuckDB::LibraryVersion();\n+\treturn duckdb::AutocompleteExtension::DefaultVersion();\n }\n }\n \ndiff --git a/extension/core_functions/function_list.cpp b/extension/core_functions/function_list.cpp\nindex fb8550c677ba..9051c60047c0 100644\n--- a/extension/core_functions/function_list.cpp\n+++ b/extension/core_functions/function_list.cpp\n@@ -136,6 +136,7 @@ static const StaticFunctionDefinition core_functions[] = {\n \tDUCKDB_AGGREGATE_FUNCTION(CovarPopFun),\n \tDUCKDB_AGGREGATE_FUNCTION(CovarSampFun),\n \tDUCKDB_SCALAR_FUNCTION(CurrentDatabaseFun),\n+\tDUCKDB_SCALAR_FUNCTION(CurrentDateFun),\n \tDUCKDB_SCALAR_FUNCTION(CurrentQueryFun),\n \tDUCKDB_SCALAR_FUNCTION(CurrentSchemaFun),\n \tDUCKDB_SCALAR_FUNCTION(CurrentSchemasFun),\n@@ -192,6 +193,7 @@ static const StaticFunctionDefinition core_functions[] = {\n \tDUCKDB_SCALAR_FUNCTION_ALIAS(GenRandomUuidFun),\n \tDUCKDB_SCALAR_FUNCTION_SET(GenerateSeriesFun),\n \tDUCKDB_SCALAR_FUNCTION(GetBitFun),\n+\tDUCKDB_SCALAR_FUNCTION(CurrentTimeFun),\n \tDUCKDB_SCALAR_FUNCTION(GetCurrentTimestampFun),\n \tDUCKDB_SCALAR_FUNCTION_SET_ALIAS(GradeUpFun),\n \tDUCKDB_SCALAR_FUNCTION_SET(GreatestFun),\n@@ -369,6 +371,7 @@ static const StaticFunctionDefinition core_functions[] = {\n \tDUCKDB_SCALAR_FUNCTION(ToTimestampFun),\n \tDUCKDB_SCALAR_FUNCTION(ToWeeksFun),\n \tDUCKDB_SCALAR_FUNCTION(ToYearsFun),\n+\tDUCKDB_SCALAR_FUNCTION_ALIAS(TodayFun),\n \tDUCKDB_SCALAR_FUNCTION_ALIAS(TransactionTimestampFun),\n \tDUCKDB_SCALAR_FUNCTION(TranslateFun),\n \tDUCKDB_SCALAR_FUNCTION_SET(TrimFun),\ndiff --git a/extension/core_functions/include/core_functions/scalar/date_functions.hpp b/extension/core_functions/include/core_functions/scalar/date_functions.hpp\nindex 7256502a9c50..efa9821d1d36 100644\n--- a/extension/core_functions/include/core_functions/scalar/date_functions.hpp\n+++ b/extension/core_functions/include/core_functions/scalar/date_functions.hpp\n@@ -33,6 +33,21 @@ struct CenturyFun {\n \tstatic ScalarFunctionSet GetFunctions();\n };\n \n+struct CurrentDateFun {\n+\tstatic constexpr const char *Name = \"current_date\";\n+\tstatic constexpr const char *Parameters = \"\";\n+\tstatic constexpr const char *Description = \"Returns the current date\";\n+\tstatic constexpr const char *Example = \"current_date()\";\n+\n+\tstatic ScalarFunction GetFunction();\n+};\n+\n+struct TodayFun {\n+\tusing ALIAS = CurrentDateFun;\n+\n+\tstatic constexpr const char *Name = \"today\";\n+};\n+\n struct DateDiffFun {\n \tstatic constexpr const char *Name = \"date_diff\";\n \tstatic constexpr const char *Parameters = \"part,startdate,enddate\";\n@@ -183,6 +198,15 @@ struct EpochNsFun {\n \tstatic ScalarFunctionSet GetFunctions();\n };\n \n+struct CurrentTimeFun {\n+\tstatic constexpr const char *Name = \"get_current_time\";\n+\tstatic constexpr const char *Parameters = \"\";\n+\tstatic constexpr const char *Description = \"Returns the current time\";\n+\tstatic constexpr const char *Example = \"get_current_time()\";\n+\n+\tstatic ScalarFunction GetFunction();\n+};\n+\n struct EraFun {\n \tstatic constexpr const char *Name = \"era\";\n \tstatic constexpr const char *Parameters = \"ts\";\ndiff --git a/extension/core_functions/scalar/date/current.cpp b/extension/core_functions/scalar/date/current.cpp\nindex 3d25ee80a0e3..61867f271d60 100644\n--- a/extension/core_functions/scalar/date/current.cpp\n+++ b/extension/core_functions/scalar/date/current.cpp\n@@ -6,6 +6,7 @@\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/planner/expression/bound_function_expression.hpp\"\n #include \"duckdb/transaction/meta_transaction.hpp\"\n+#include \"duckdb/planner/expression/bound_cast_expression.hpp\"\n \n namespace duckdb {\n \n@@ -26,4 +27,41 @@ ScalarFunction GetCurrentTimestampFun::GetFunction() {\n \treturn current_timestamp;\n }\n \n+static unique_ptr<Expression> CurrentTimeExpr(FunctionBindExpressionInput &input) {\n+\tauto timestamp = GetCurrentTimestampFun::GetFunction();\n+\ttimestamp.name = GetCurrentTimestampFun::Name;\n+\n+\tvector<unique_ptr<Expression>> args;\n+\n+\tauto func = make_uniq_base<Expression, BoundFunctionExpression>(LogicalType::TIMESTAMP_TZ, timestamp,\n+\t                                                                std::move(args), nullptr);\n+\n+\treturn BoundCastExpression::AddCastToType(input.context, std::move(func), LogicalType::TIME_TZ);\n+}\n+\n+static unique_ptr<Expression> CurrentDateExpr(FunctionBindExpressionInput &input) {\n+\tauto timestamp = GetCurrentTimestampFun::GetFunction();\n+\ttimestamp.name = GetCurrentTimestampFun::Name;\n+\n+\tvector<unique_ptr<Expression>> args;\n+\n+\tauto func = make_uniq_base<Expression, BoundFunctionExpression>(LogicalType::TIMESTAMP_TZ, timestamp,\n+\t                                                                std::move(args), nullptr);\n+\treturn BoundCastExpression::AddCastToType(input.context, std::move(func), LogicalType::DATE);\n+}\n+\n+ScalarFunction CurrentTimeFun::GetFunction() {\n+\tScalarFunction current_time({}, LogicalType::TIME_TZ, nullptr);\n+\tcurrent_time.bind_expression = CurrentTimeExpr;\n+\tcurrent_time.stability = FunctionStability::CONSISTENT_WITHIN_QUERY;\n+\treturn current_time;\n+}\n+\n+ScalarFunction CurrentDateFun::GetFunction() {\n+\tScalarFunction current_date({}, LogicalType::DATE, nullptr);\n+\tcurrent_date.bind_expression = CurrentDateExpr;\n+\tcurrent_date.stability = FunctionStability::CONSISTENT_WITHIN_QUERY;\n+\treturn current_date;\n+}\n+\n } // namespace duckdb\ndiff --git a/extension/core_functions/scalar/date/functions.json b/extension/core_functions/scalar/date/functions.json\nindex 6cc719b38651..4a1a36e8d2a3 100644\n--- a/extension/core_functions/scalar/date/functions.json\n+++ b/extension/core_functions/scalar/date/functions.json\n@@ -13,6 +13,13 @@\n         \"example\": \"century(timestamp '2021-08-03 11:59:44.123456')\",\n         \"type\": \"scalar_function_set\"\n     },\n+    {\n+        \"name\": \"current_date\",\n+        \"description\": \"Returns the current date\",\n+        \"example\": \"current_date()\",\n+        \"type\": \"scalar_function\",\n+        \"aliases\": [\"today\"]\n+    },\n     {\n         \"name\": \"date_diff\",\n         \"parameters\": \"part,startdate,enddate\",\n@@ -119,6 +126,13 @@\n         \"example\": \"epoch_ns(timestamp '2021-08-03 11:59:44.123456')\",\n         \"type\": \"scalar_function_set\"\n     },\n+    {\n+        \"struct\": \"CurrentTimeFun\",\n+        \"name\": \"get_current_time\",\n+        \"description\": \"Returns the current time\",\n+        \"example\": \"get_current_time()\",\n+        \"type\": \"scalar_function\"\n+    },\n     {\n         \"name\": \"era\",\n         \"parameters\": \"ts\",\ndiff --git a/extension/core_functions/scalar/generic/can_implicitly_cast.cpp b/extension/core_functions/scalar/generic/can_implicitly_cast.cpp\nindex 5db38d601905..5949dcc37c70 100644\n--- a/extension/core_functions/scalar/generic/can_implicitly_cast.cpp\n+++ b/extension/core_functions/scalar/generic/can_implicitly_cast.cpp\n@@ -18,8 +18,8 @@ static void CanCastImplicitlyFunction(DataChunk &args, ExpressionState &state, V\n }\n \n unique_ptr<Expression> BindCanCastImplicitlyExpression(FunctionBindExpressionInput &input) {\n-\tauto &source_type = input.function.children[0]->return_type;\n-\tauto &target_type = input.function.children[1]->return_type;\n+\tauto &source_type = input.children[0]->return_type;\n+\tauto &target_type = input.children[1]->return_type;\n \tif (source_type.id() == LogicalTypeId::UNKNOWN || source_type.id() == LogicalTypeId::SQLNULL ||\n \t    target_type.id() == LogicalTypeId::UNKNOWN || target_type.id() == LogicalTypeId::SQLNULL) {\n \t\t// parameter - unknown return type\ndiff --git a/extension/core_functions/scalar/generic/typeof.cpp b/extension/core_functions/scalar/generic/typeof.cpp\nindex 1f7caef848ce..895d474f429d 100644\n--- a/extension/core_functions/scalar/generic/typeof.cpp\n+++ b/extension/core_functions/scalar/generic/typeof.cpp\n@@ -10,7 +10,7 @@ static void TypeOfFunction(DataChunk &args, ExpressionState &state, Vector &resu\n }\n \n unique_ptr<Expression> BindTypeOfFunctionExpression(FunctionBindExpressionInput &input) {\n-\tauto &return_type = input.function.children[0]->return_type;\n+\tauto &return_type = input.children[0]->return_type;\n \tif (return_type.id() == LogicalTypeId::UNKNOWN || return_type.id() == LogicalTypeId::SQLNULL) {\n \t\t// parameter - unknown return type\n \t\treturn nullptr;\ndiff --git a/extension/json/json_functions/json_structure.cpp b/extension/json/json_functions/json_structure.cpp\nindex fbc71366ef92..260f7984a671 100644\n--- a/extension/json/json_functions/json_structure.cpp\n+++ b/extension/json/json_functions/json_structure.cpp\n@@ -662,7 +662,9 @@ static double CalculateTypeSimilarity(const LogicalType &merged, const LogicalTy\n \t\t}\n \n \t\t// Only maps and structs can be merged into a map\n-\t\tD_ASSERT(type.id() == LogicalTypeId::STRUCT);\n+\t\tif (type.id() != LogicalTypeId::STRUCT) {\n+\t\t\treturn -1;\n+\t\t}\n \t\treturn CalculateMapAndStructSimilarity(merged, type, false, max_depth, depth);\n \t}\n \tcase LogicalTypeId::LIST: {\ndiff --git a/extension/parquet/include/resizable_buffer.hpp b/extension/parquet/include/resizable_buffer.hpp\nindex 14658ecee85b..1e225d510320 100644\n--- a/extension/parquet/include/resizable_buffer.hpp\n+++ b/extension/parquet/include/resizable_buffer.hpp\n@@ -98,6 +98,7 @@ class ResizeableBuffer : public ByteBuffer {\n \t\t}\n \t\tif (new_size > alloc_len) {\n \t\t\talloc_len = NextPowerOfTwo(new_size);\n+\t\t\tallocated_data.Reset(); // Have to reset before allocating new buffer (otherwise we use ~2x the memory)\n \t\t\tallocated_data = allocator.Allocate(alloc_len);\n \t\t\tptr = allocated_data.get();\n \t\t}\ndiff --git a/extension/parquet/parquet_extension.cpp b/extension/parquet/parquet_extension.cpp\nindex 8ec9924a67bf..a9e8b20e4830 100644\n--- a/extension/parquet/parquet_extension.cpp\n+++ b/extension/parquet/parquet_extension.cpp\n@@ -1497,18 +1497,19 @@ static void ParquetCopySerialize(Serializer &serializer, const FunctionData &bin\n \t// We have to std::move them, otherwise MSVC will complain that it's not a \"const T &&\"\n \tconst auto compression_level = SerializeCompressionLevel(bind_data.compression_level);\n \tD_ASSERT(DeserializeCompressionLevel(compression_level) == bind_data.compression_level);\n+\tParquetWriteBindData default_value;\n \tserializer.WritePropertyWithDefault(109, \"compression_level\", compression_level);\n \tserializer.WritePropertyWithDefault(110, \"row_groups_per_file\", bind_data.row_groups_per_file,\n-\t                                    std::move(ParquetWriteBindData().row_groups_per_file));\n+\t                                    std::move(default_value.row_groups_per_file));\n \tserializer.WritePropertyWithDefault(111, \"debug_use_openssl\", bind_data.debug_use_openssl,\n-\t                                    std::move(ParquetWriteBindData().debug_use_openssl));\n+\t                                    std::move(default_value.debug_use_openssl));\n \tserializer.WritePropertyWithDefault(112, \"dictionary_size_limit\", bind_data.dictionary_size_limit,\n-\t                                    std::move(ParquetWriteBindData().dictionary_size_limit));\n+\t                                    std::move(default_value.dictionary_size_limit));\n \tserializer.WritePropertyWithDefault(113, \"bloom_filter_false_positive_ratio\",\n \t                                    bind_data.bloom_filter_false_positive_ratio,\n-\t                                    std::move(ParquetWriteBindData().bloom_filter_false_positive_ratio));\n+\t                                    std::move(default_value.bloom_filter_false_positive_ratio));\n \tserializer.WritePropertyWithDefault(114, \"parquet_version\", bind_data.parquet_version,\n-\t                                    std::move(ParquetWriteBindData().parquet_version));\n+\t                                    std::move(default_value.parquet_version));\n }\n \n static unique_ptr<FunctionData> ParquetCopyDeserialize(Deserializer &deserializer, CopyFunction &function) {\n@@ -1528,16 +1529,17 @@ static unique_ptr<FunctionData> ParquetCopyDeserialize(Deserializer &deserialize\n \tdeserializer.ReadPropertyWithDefault<optional_idx>(109, \"compression_level\", compression_level);\n \tdata->compression_level = DeserializeCompressionLevel(compression_level);\n \tD_ASSERT(SerializeCompressionLevel(data->compression_level) == compression_level);\n+\tParquetWriteBindData default_value;\n \tdata->row_groups_per_file = deserializer.ReadPropertyWithExplicitDefault<optional_idx>(\n-\t    110, \"row_groups_per_file\", std::move(ParquetWriteBindData().row_groups_per_file));\n+\t    110, \"row_groups_per_file\", std::move(default_value.row_groups_per_file));\n \tdata->debug_use_openssl = deserializer.ReadPropertyWithExplicitDefault<bool>(\n-\t    111, \"debug_use_openssl\", std::move(ParquetWriteBindData().debug_use_openssl));\n+\t    111, \"debug_use_openssl\", std::move(default_value.debug_use_openssl));\n \tdata->dictionary_size_limit = deserializer.ReadPropertyWithExplicitDefault<idx_t>(\n-\t    112, \"dictionary_size_limit\", std::move(ParquetWriteBindData().dictionary_size_limit));\n+\t    112, \"dictionary_size_limit\", std::move(default_value.dictionary_size_limit));\n \tdata->bloom_filter_false_positive_ratio = deserializer.ReadPropertyWithExplicitDefault<double>(\n-\t    113, \"bloom_filter_false_positive_ratio\", std::move(ParquetWriteBindData().bloom_filter_false_positive_ratio));\n-\tdata->parquet_version = deserializer.ReadPropertyWithExplicitDefault(\n-\t    114, \"parquet_version\", std::move(ParquetWriteBindData().parquet_version));\n+\t    113, \"bloom_filter_false_positive_ratio\", std::move(default_value.bloom_filter_false_positive_ratio));\n+\tdata->parquet_version =\n+\t    deserializer.ReadPropertyWithExplicitDefault(114, \"parquet_version\", std::move(default_value.parquet_version));\n \n \treturn std::move(data);\n }\ndiff --git a/extension/tpch/dbgen/dbgen.cpp b/extension/tpch/dbgen/dbgen.cpp\nindex 09a1aaac71e2..093b255a422b 100644\n--- a/extension/tpch/dbgen/dbgen.cpp\n+++ b/extension/tpch/dbgen/dbgen.cpp\n@@ -659,16 +659,24 @@ void DBGenWrapper::LoadTPCHData(ClientContext &context, double flt_scale, string\n \t\t\t\tthreads.emplace_back(ParallelTPCHAppend, &new_appenders[thr_idx], child_count, step);\n \t\t\t\tstep++;\n \t\t\t}\n-\t\t\t// flush the previous batch of appenders while waiting (if any are there)\n-\t\t\t// now flush the appenders in-order\n-\t\t\tfor(auto &appender : finished_appenders) {\n-\t\t\t\tappender.Flush();\n+\t\t\tErrorData error;\n+\t\t\ttry {\n+\t\t\t\t// flush the previous batch of appenders while waiting (if any are there)\n+\t\t\t\t// now flush the appenders in-order\n+\t\t\t\tfor(auto &appender : finished_appenders) {\n+\t\t\t\t\tappender.Flush();\n+\t\t\t\t}\n+\t\t\t} catch(std::exception &ex) {\n+\t\t\t\terror = ErrorData(ex);\n \t\t\t}\n \t\t\tfinished_appenders.clear();\n \t\t\t// wait for all threads to finish\n \t\t\tfor(auto &thread : threads) {\n \t\t\t\tthread.join();\n \t\t\t}\n+\t\t\tif (error.HasError()) {\n+\t\t\t\terror.Throw();\n+\t\t\t}\n \t\t\tfinished_appenders = std::move(new_appenders);\n \t\t}\n \t\t// flush the final batch of appenders\ndiff --git a/scripts/generate_benchmarks.py b/scripts/generate_benchmarks.py\ndeleted file mode 100644\nindex 089bd0adef03..000000000000\n--- a/scripts/generate_benchmarks.py\n+++ /dev/null\n@@ -1,37 +0,0 @@\n-import os\n-from python_helpers import open_utf8\n-\n-\n-def format_tpch_queries(target_dir, tpch_in, comment):\n-    with open_utf8(tpch_in, 'r') as f:\n-        text = f.read()\n-\n-    for i in range(1, 23):\n-        qnr = '%02d' % (i,)\n-        target_file = os.path.join(target_dir, 'q' + qnr + '.benchmark')\n-        new_text = '''# name: %s\n-# description: Run query %02d from the TPC-H benchmark (%s)\n-# group: [sf1]\n-\n-template %s\n-QUERY_NUMBER=%d\n-QUERY_NUMBER_PADDED=%02d''' % (\n-            target_file,\n-            i,\n-            comment,\n-            tpch_in,\n-            i,\n-            i,\n-        )\n-        with open_utf8(target_file, 'w+') as f:\n-            f.write(new_text)\n-\n-\n-# generate the TPC-H benchmark files\n-single_threaded_dir = os.path.join('benchmark', 'tpch', 'sf1')\n-single_threaded_in = os.path.join(single_threaded_dir, 'tpch_sf1.benchmark.in')\n-format_tpch_queries(single_threaded_dir, single_threaded_in, 'single-threaded')\n-\n-parallel_threaded_dir = os.path.join('benchmark', 'tpch', 'sf1-parallel')\n-parallel_threaded_in = os.path.join(parallel_threaded_dir, 'tpch_sf1_parallel.benchmark.in')\n-format_tpch_queries(parallel_threaded_dir, parallel_threaded_in, '4 threads')\ndiff --git a/scripts/generate_extensions_function.py b/scripts/generate_extensions_function.py\nindex 15417eb76c0a..eefd6dfffb06 100644\n--- a/scripts/generate_extensions_function.py\n+++ b/scripts/generate_extensions_function.py\n@@ -363,16 +363,10 @@ def __init__(self):\n         self.extensions: Dict[str, str] = get_extension_path_map()\n \n         self.stored_functions: Dict[str, List[Function]] = {\n-            'substrait': [\n-                Function(\"from_substrait\", CatalogType.TABLE),\n-                Function(\"get_substrait\", CatalogType.TABLE),\n-                Function(\"get_substrait_json\", CatalogType.TABLE),\n-                Function(\"from_substrait_json\", CatalogType.TABLE),\n-            ],\n             'arrow': [Function(\"scan_arrow_ipc\", CatalogType.TABLE), Function(\"to_arrow_ipc\", CatalogType.TABLE)],\n             'spatial': [],\n         }\n-        self.stored_settings: Dict[str, List[str]] = {'substrait': [], 'arrow': [], 'spatial': []}\n+        self.stored_settings: Dict[str, List[str]] = {'arrow': [], 'spatial': []}\n \n     def set_base(self):\n         (functions, function_overloads) = get_functions()\n@@ -718,6 +712,7 @@ def write_header(data: ExtensionData):\n     \"excel\",\n     \"fts\",\n     \"httpfs\",\n+    \"iceberg\",\n     \"inet\",\n     \"icu\",\n     \"json\",\n@@ -728,7 +723,8 @@ def write_header(data: ExtensionData):\n     \"sqlsmith\",\n     \"postgres_scanner\",\n     \"tpcds\",\n-    \"tpch\"\n+    \"tpch\",\n+    \"uc_catalog\"\n }; // END_OF_AUTOLOADABLE_EXTENSIONS\n \n } // namespace duckdb\"\"\"\ndiff --git a/src/catalog/default/default_functions.cpp b/src/catalog/default/default_functions.cpp\nindex 7702bc4e0fdf..480e05f81081 100644\n--- a/src/catalog/default/default_functions.cpp\n+++ b/src/catalog/default/default_functions.cpp\n@@ -163,9 +163,6 @@ static const DefaultMacro internal_macros[] = {\n \n \t// date functions\n \t{DEFAULT_SCHEMA, \"date_add\", {\"date\", \"interval\", nullptr}, {{nullptr, nullptr}}, \"date + interval\"},\n-\t{DEFAULT_SCHEMA, \"current_date\", {nullptr}, {{nullptr, nullptr}}, \"current_timestamp::DATE\"},\n-\t{DEFAULT_SCHEMA, \"today\", {nullptr}, {{nullptr, nullptr}}, \"current_timestamp::DATE\"},\n-\t{DEFAULT_SCHEMA, \"get_current_time\", {nullptr}, {{nullptr, nullptr}}, \"current_timestamp::TIMETZ\"},\n \n \t// regexp functions\n \t{DEFAULT_SCHEMA, \"regexp_split_to_table\", {\"text\", \"pattern\", nullptr}, {{nullptr, nullptr}}, \"unnest(string_split_regex(text, pattern))\"},\ndiff --git a/src/common/adbc/adbc.cpp b/src/common/adbc/adbc.cpp\nindex 7323f3b1f337..35ceb2f3406f 100644\n--- a/src/common/adbc/adbc.cpp\n+++ b/src/common/adbc/adbc.cpp\n@@ -53,7 +53,6 @@ AdbcStatusCode duckdb_adbc_init(int version, void *driver, struct AdbcError *err\n \tadbc_driver->ConnectionGetInfo = duckdb_adbc::ConnectionGetInfo;\n \tadbc_driver->StatementGetParameterSchema = duckdb_adbc::StatementGetParameterSchema;\n \tadbc_driver->ConnectionGetTableSchema = duckdb_adbc::ConnectionGetTableSchema;\n-\tadbc_driver->StatementSetSubstraitPlan = duckdb_adbc::StatementSetSubstraitPlan;\n \treturn ADBC_STATUS_OK;\n }\n \n@@ -70,7 +69,6 @@ struct DuckDBAdbcStatementWrapper {\n \tArrowArrayStream ingestion_stream;\n \tIngestionMode ingestion_mode = IngestionMode::CREATE;\n \tbool temporary_table = false;\n-\tuint8_t *substrait_plan;\n \tuint64_t plan_length;\n };\n \n@@ -157,36 +155,6 @@ AdbcStatusCode DatabaseNew(struct AdbcDatabase *database, struct AdbcError *erro\n \treturn CheckResult(res, error, \"Failed to allocate\");\n }\n \n-AdbcStatusCode StatementSetSubstraitPlan(struct AdbcStatement *statement, const uint8_t *plan, size_t length,\n-                                         struct AdbcError *error) {\n-\tif (!statement) {\n-\t\tSetError(error, \"Statement is not set\");\n-\t\treturn ADBC_STATUS_INVALID_ARGUMENT;\n-\t}\n-\tif (!plan) {\n-\t\tSetError(error, \"Substrait Plan is not set\");\n-\t\treturn ADBC_STATUS_INVALID_ARGUMENT;\n-\t}\n-\tif (length == 0) {\n-\t\tSetError(error, \"Can't execute plan with size = 0\");\n-\t\treturn ADBC_STATUS_INVALID_ARGUMENT;\n-\t}\n-\tauto wrapper = static_cast<DuckDBAdbcStatementWrapper *>(statement->private_data);\n-\tif (wrapper->ingestion_stream.release) {\n-\t\t// Release any resources currently held by the ingestion stream before we overwrite it\n-\t\twrapper->ingestion_stream.release(&wrapper->ingestion_stream);\n-\t\twrapper->ingestion_stream.release = nullptr;\n-\t}\n-\tif (wrapper->statement) {\n-\t\tduckdb_destroy_prepare(&wrapper->statement);\n-\t\twrapper->statement = nullptr;\n-\t}\n-\twrapper->substrait_plan = static_cast<uint8_t *>(malloc(sizeof(uint8_t) * length));\n-\twrapper->plan_length = length;\n-\tmemcpy(wrapper->substrait_plan, plan, length);\n-\treturn ADBC_STATUS_OK;\n-}\n-\n AdbcStatusCode DatabaseSetOption(struct AdbcDatabase *database, const char *key, const char *value,\n                                  struct AdbcError *error) {\n \tif (!database) {\n@@ -677,7 +645,6 @@ AdbcStatusCode StatementNew(struct AdbcConnection *connection, struct AdbcStatem\n \tstatement_wrapper->ingestion_stream.release = nullptr;\n \tstatement_wrapper->ingestion_table_name = nullptr;\n \tstatement_wrapper->db_schema = nullptr;\n-\tstatement_wrapper->substrait_plan = nullptr;\n \tstatement_wrapper->temporary_table = false;\n \n \tstatement_wrapper->ingestion_mode = IngestionMode::CREATE;\n@@ -709,10 +676,6 @@ AdbcStatusCode StatementRelease(struct AdbcStatement *statement, struct AdbcErro\n \t\tfree(wrapper->db_schema);\n \t\twrapper->db_schema = nullptr;\n \t}\n-\tif (wrapper->substrait_plan) {\n-\t\tfree(wrapper->substrait_plan);\n-\t\twrapper->substrait_plan = nullptr;\n-\t}\n \tfree(statement->private_data);\n \tstatement->private_data = nullptr;\n \treturn ADBC_STATUS_OK;\n@@ -805,25 +768,7 @@ AdbcStatusCode StatementExecuteQuery(struct AdbcStatement *statement, struct Arr\n \tif (has_stream && to_table) {\n \t\treturn IngestToTableFromBoundStream(wrapper, error);\n \t}\n-\tif (wrapper->substrait_plan != nullptr) {\n-\t\tauto plan_str = std::string(reinterpret_cast<const char *>(wrapper->substrait_plan), wrapper->plan_length);\n-\t\tduckdb::vector<duckdb::Value> params;\n-\t\tparams.emplace_back(duckdb::Value::BLOB_RAW(plan_str));\n-\t\tduckdb::unique_ptr<duckdb::QueryResult> query_result;\n-\t\ttry {\n-\t\t\tquery_result = reinterpret_cast<duckdb::Connection *>(wrapper->connection)\n-\t\t\t                   ->TableFunction(\"from_substrait\", params)\n-\t\t\t                   ->Execute();\n-\t\t} catch (duckdb::Exception &e) {\n-\t\t\tstd::string error_msg = \"It was not possible to execute substrait query. \" + std::string(e.what());\n-\t\t\tSetError(error, error_msg);\n-\t\t\treturn ADBC_STATUS_INVALID_ARGUMENT;\n-\t\t}\n-\t\tauto arrow_wrapper = new duckdb::ArrowResultWrapper();\n-\t\tarrow_wrapper->result =\n-\t\t    duckdb::unique_ptr_cast<duckdb::QueryResult, duckdb::MaterializedQueryResult>(std::move(query_result));\n-\t\twrapper->result = reinterpret_cast<duckdb_arrow>(arrow_wrapper);\n-\t} else if (has_stream) {\n+\tif (has_stream) {\n \t\t// A stream was bound to the statement, use that to bind parameters\n \t\tduckdb::unique_ptr<duckdb::QueryResult> result;\n \t\tArrowArrayStream stream = wrapper->ingestion_stream;\ndiff --git a/src/common/compressed_file_system.cpp b/src/common/compressed_file_system.cpp\nindex ddc325cf1156..7d2e2cfde82b 100644\n--- a/src/common/compressed_file_system.cpp\n+++ b/src/common/compressed_file_system.cpp\n@@ -44,7 +44,7 @@ int64_t CompressedFile::ReadData(void *buffer, int64_t remaining) {\n \t\t\tauto available =\n \t\t\t    MinValue<idx_t>(UnsafeNumericCast<idx_t>(remaining),\n \t\t\t                    UnsafeNumericCast<idx_t>(stream_data.out_buff_end - stream_data.out_buff_start));\n-\t\t\tmemcpy(data_ptr_t(buffer) + total_read, stream_data.out_buff_start, available);\n+\t\t\tmemcpy(static_cast<data_ptr_t>(buffer) + total_read, stream_data.out_buff_start, available);\n \n \t\t\t// increment the total read variables as required\n \t\t\tstream_data.out_buff_start += available;\ndiff --git a/src/common/gzip_file_system.cpp b/src/common/gzip_file_system.cpp\nindex ee0a215805ca..edb72bf911cd 100644\n--- a/src/common/gzip_file_system.cpp\n+++ b/src/common/gzip_file_system.cpp\n@@ -82,7 +82,7 @@ struct MiniZStreamWrapper : public StreamWrapper {\n \n \tvoid Close() override;\n \n-\tvoid FlushStream();\n+\tvoid FlushStream() const;\n };\n \n MiniZStreamWrapper::~MiniZStreamWrapper() {\n@@ -146,7 +146,7 @@ void MiniZStreamWrapper::Initialize(CompressedFile &file, bool write) {\n bool MiniZStreamWrapper::Read(StreamData &sd) {\n \t// Handling for the concatenated files\n \tif (sd.refresh) {\n-\t\tauto available = (uint32_t)(sd.in_buff_end - sd.in_buff_start);\n+\t\tauto available = static_cast<uint32_t>(sd.in_buff_end - sd.in_buff_start);\n \t\tif (available <= GZIP_FOOTER_SIZE) {\n \t\t\t// Only footer is available so we just close and return finished\n \t\t\tClose();\n@@ -173,7 +173,7 @@ bool MiniZStreamWrapper::Read(StreamData &sd) {\n \t\t\t\tc = UnsafeNumericCast<char>(*body_ptr);\n \t\t\t\tbody_ptr++;\n \t\t\t} while (c != '\\0' && body_ptr < sd.in_buff_end);\n-\t\t\tif ((idx_t)(body_ptr - sd.in_buff_start) >= GZIP_HEADER_MAXSIZE) {\n+\t\t\tif (static_cast<idx_t>(body_ptr - sd.in_buff_start) >= GZIP_HEADER_MAXSIZE) {\n \t\t\t\tthrow InternalException(\"Filename resulting in GZIP header larger than defined maximum (%d)\",\n \t\t\t\t                        GZIP_HEADER_MAXSIZE);\n \t\t\t}\n@@ -193,9 +193,9 @@ bool MiniZStreamWrapper::Read(StreamData &sd) {\n \t// actually decompress\n \tmz_stream_ptr->next_in = sd.in_buff_start;\n \tD_ASSERT(sd.in_buff_end - sd.in_buff_start < NumericLimits<int32_t>::Maximum());\n-\tmz_stream_ptr->avail_in = (uint32_t)(sd.in_buff_end - sd.in_buff_start);\n+\tmz_stream_ptr->avail_in = static_cast<uint32_t>(sd.in_buff_end - sd.in_buff_start);\n \tmz_stream_ptr->next_out = data_ptr_cast(sd.out_buff_end);\n-\tmz_stream_ptr->avail_out = (uint32_t)((sd.out_buff.get() + sd.out_buf_size) - sd.out_buff_end);\n+\tmz_stream_ptr->avail_out = static_cast<uint32_t>((sd.out_buff.get() + sd.out_buf_size) - sd.out_buff_end);\n \tauto ret = duckdb_miniz::mz_inflate(mz_stream_ptr.get(), duckdb_miniz::MZ_NO_FLUSH);\n \tif (ret != duckdb_miniz::MZ_OK && ret != duckdb_miniz::MZ_STREAM_END) {\n \t\tthrow IOException(\"Failed to decode gzip stream: %s\", duckdb_miniz::mz_error(ret));\n@@ -248,7 +248,7 @@ void MiniZStreamWrapper::Write(CompressedFile &file, StreamData &sd, data_ptr_t\n \t}\n }\n \n-void MiniZStreamWrapper::FlushStream() {\n+void MiniZStreamWrapper::FlushStream() const {\n \tauto &sd = file->stream_data;\n \tmz_stream_ptr->next_in = nullptr;\n \tmz_stream_ptr->avail_in = 0;\n@@ -371,7 +371,7 @@ string GZipFileSystem::UncompressGZIPString(const char *data, idx_t size) {\n \t\tdo {\n \t\t\tc = *body_ptr;\n \t\t\tbody_ptr++;\n-\t\t} while (c != '\\0' && (idx_t)(body_ptr - data) < size);\n+\t\t} while (c != '\\0' && static_cast<idx_t>(body_ptr - data) < size);\n \t}\n \n \t// stream is now set to beginning of payload data\n@@ -384,10 +384,10 @@ string GZipFileSystem::UncompressGZIPString(const char *data, idx_t size) {\n \tmz_stream_ptr->next_in = const_uchar_ptr_cast(body_ptr);\n \tmz_stream_ptr->avail_in = NumericCast<unsigned int>(bytes_remaining);\n \n-\tunsigned char decompress_buffer[BUFSIZ];\n \tstring decompressed;\n \n \twhile (status == duckdb_miniz::MZ_OK) {\n+\t\tunsigned char decompress_buffer[BUFSIZ];\n \t\tmz_stream_ptr->next_out = decompress_buffer;\n \t\tmz_stream_ptr->avail_out = sizeof(decompress_buffer);\n \t\tstatus = mz_inflate(mz_stream_ptr.get(), duckdb_miniz::MZ_NO_FLUSH);\ndiff --git a/src/execution/operator/aggregate/physical_streaming_window.cpp b/src/execution/operator/aggregate/physical_streaming_window.cpp\nindex 8d78e6653296..aa165678b133 100644\n--- a/src/execution/operator/aggregate/physical_streaming_window.cpp\n+++ b/src/execution/operator/aggregate/physical_streaming_window.cpp\n@@ -153,8 +153,6 @@ class StreamingWindowState : public OperatorState {\n \t\t\tComputeOffset(context, wexpr, offset);\n \t\t\tComputeDefault(context, wexpr, dflt);\n \n-\t\t\tcurr_chunk.Initialize(context, {wexpr.return_type});\n-\n \t\t\tbuffered = idx_t(std::abs(offset));\n \t\t\tprev.Reference(dflt);\n \t\t\tprev.Flatten(buffered);\n@@ -162,6 +160,10 @@ class StreamingWindowState : public OperatorState {\n \t\t}\n \n \t\tvoid Execute(ExecutionContext &context, DataChunk &input, DataChunk &delayed, Vector &result) {\n+\t\t\tif (!curr_chunk.ColumnCount()) {\n+\t\t\t\tcurr_chunk.Initialize(context.client, {result.GetType()}, delayed.GetCapacity());\n+\t\t\t}\n+\n \t\t\tif (offset >= 0) {\n \t\t\t\tExecuteLag(context, input, result);\n \t\t\t} else {\n@@ -212,7 +214,7 @@ class StreamingWindowState : public OperatorState {\n \t\t\tidx_t pos = 0;\n \t\t\tidx_t unified_offset = buffered;\n \t\t\tif (unified_offset < count) {\n-\t\t\t\tcurr_chunk.Reset();\n+\t\t\t\tReset(curr_chunk);\n \t\t\t\texecutor.Execute(input, curr_chunk);\n \t\t\t\tVectorOperations::Copy(curr, result, count, unified_offset, pos);\n \t\t\t\tpos += count - unified_offset;\n@@ -221,7 +223,7 @@ class StreamingWindowState : public OperatorState {\n \t\t\t// Copy unified[unified_offset:] => result[pos:]\n \t\t\tidx_t unified_count = count + delayed.size();\n \t\t\tif (unified_offset < unified_count) {\n-\t\t\t\tcurr_chunk.Reset();\n+\t\t\t\tReset(curr_chunk);\n \t\t\t\texecutor.Execute(delayed, curr_chunk);\n \t\t\t\tidx_t delayed_offset = unified_offset - count;\n \t\t\t\t// Only copy as many values as we need\n@@ -312,6 +314,13 @@ class StreamingWindowState : public OperatorState {\n \t\tinitialized = true;\n \t}\n \n+\tstatic inline void Reset(DataChunk &chunk) {\n+\t\t//\tReset trashes the capacity...\n+\t\tconst auto capacity = chunk.GetCapacity();\n+\t\tchunk.Reset();\n+\t\tchunk.SetCapacity(capacity);\n+\t}\n+\n public:\n \t//! We can't initialise until we have an input chunk\n \tbool initialized;\n@@ -470,34 +479,34 @@ void StreamingWindowState::AggregateState::Execute(ExecutionContext &context, Da\n \t}\n }\n \n-void PhysicalStreamingWindow::ExecuteFunctions(ExecutionContext &context, DataChunk &chunk, DataChunk &delayed,\n+void PhysicalStreamingWindow::ExecuteFunctions(ExecutionContext &context, DataChunk &output, DataChunk &delayed,\n                                                GlobalOperatorState &gstate_p, OperatorState &state_p) const {\n \tauto &gstate = gstate_p.Cast<StreamingWindowGlobalState>();\n \tauto &state = state_p.Cast<StreamingWindowState>();\n \n \t// Compute window functions\n-\tconst idx_t count = chunk.size();\n+\tconst idx_t count = output.size();\n \tconst column_t input_width = children[0]->GetTypes().size();\n \tfor (column_t expr_idx = 0; expr_idx < select_list.size(); expr_idx++) {\n \t\tcolumn_t col_idx = input_width + expr_idx;\n \t\tauto &expr = *select_list[expr_idx];\n-\t\tauto &result = chunk.data[col_idx];\n+\t\tauto &result = output.data[col_idx];\n \t\tswitch (expr.GetExpressionType()) {\n \t\tcase ExpressionType::WINDOW_AGGREGATE:\n-\t\t\tstate.aggregate_states[expr_idx]->Execute(context, chunk, result);\n+\t\t\tstate.aggregate_states[expr_idx]->Execute(context, output, result);\n \t\t\tbreak;\n \t\tcase ExpressionType::WINDOW_FIRST_VALUE:\n \t\tcase ExpressionType::WINDOW_PERCENT_RANK:\n \t\tcase ExpressionType::WINDOW_RANK:\n \t\tcase ExpressionType::WINDOW_RANK_DENSE: {\n \t\t\t// Reference constant vector\n-\t\t\tchunk.data[col_idx].Reference(*state.const_vectors[expr_idx]);\n+\t\t\toutput.data[col_idx].Reference(*state.const_vectors[expr_idx]);\n \t\t\tbreak;\n \t\t}\n \t\tcase ExpressionType::WINDOW_ROW_NUMBER: {\n \t\t\t// Set row numbers\n \t\t\tint64_t start_row = gstate.row_number;\n-\t\t\tauto rdata = FlatVector::GetData<int64_t>(chunk.data[col_idx]);\n+\t\t\tauto rdata = FlatVector::GetData<int64_t>(output.data[col_idx]);\n \t\t\tfor (idx_t i = 0; i < count; i++) {\n \t\t\t\trdata[i] = NumericCast<int64_t>(start_row + NumericCast<int64_t>(i));\n \t\t\t}\n@@ -505,7 +514,7 @@ void PhysicalStreamingWindow::ExecuteFunctions(ExecutionContext &context, DataCh\n \t\t}\n \t\tcase ExpressionType::WINDOW_LAG:\n \t\tcase ExpressionType::WINDOW_LEAD:\n-\t\t\tstate.lead_lag_states[expr_idx]->Execute(context, chunk, delayed, result);\n+\t\t\tstate.lead_lag_states[expr_idx]->Execute(context, output, delayed, result);\n \t\t\tbreak;\n \t\tdefault:\n \t\t\tthrow NotImplementedException(\"%s for StreamingWindow\", ExpressionTypeToString(expr.GetExpressionType()));\n@@ -515,13 +524,13 @@ void PhysicalStreamingWindow::ExecuteFunctions(ExecutionContext &context, DataCh\n }\n \n void PhysicalStreamingWindow::ExecuteInput(ExecutionContext &context, DataChunk &delayed, DataChunk &input,\n-                                           DataChunk &chunk, GlobalOperatorState &gstate_p,\n+                                           DataChunk &output, GlobalOperatorState &gstate_p,\n                                            OperatorState &state_p) const {\n \tauto &state = state_p.Cast<StreamingWindowState>();\n \n \t// Put payload columns in place\n \tfor (idx_t col_idx = 0; col_idx < input.data.size(); col_idx++) {\n-\t\tchunk.data[col_idx].Reference(input.data[col_idx]);\n+\t\toutput.data[col_idx].Reference(input.data[col_idx]);\n \t}\n \tidx_t count = input.size();\n \n@@ -531,51 +540,53 @@ void PhysicalStreamingWindow::ExecuteInput(ExecutionContext &context, DataChunk\n \t\tcount -= state.lead_count;\n \t\tinput.Copy(delayed, count);\n \t}\n-\tchunk.SetCardinality(count);\n+\toutput.SetCardinality(count);\n \n-\tExecuteFunctions(context, chunk, state.delayed, gstate_p, state_p);\n+\tExecuteFunctions(context, output, state.delayed, gstate_p, state_p);\n }\n \n void PhysicalStreamingWindow::ExecuteShifted(ExecutionContext &context, DataChunk &delayed, DataChunk &input,\n-                                             DataChunk &chunk, GlobalOperatorState &gstate_p,\n+                                             DataChunk &output, GlobalOperatorState &gstate_p,\n                                              OperatorState &state_p) const {\n \tauto &state = state_p.Cast<StreamingWindowState>();\n \tauto &shifted = state.shifted;\n \n-\tidx_t i = input.size();\n-\tidx_t d = delayed.size();\n-\tshifted.Reset();\n+\tidx_t out = output.size();\n+\tidx_t in = input.size();\n+\tidx_t delay = delayed.size();\n+\tD_ASSERT(out <= delay);\n+\n+\tstate.Reset(shifted);\n \t// shifted = delayed\n \tdelayed.Copy(shifted);\n-\tdelayed.Reset();\n+\tstate.Reset(delayed);\n \tfor (idx_t col_idx = 0; col_idx < delayed.data.size(); ++col_idx) {\n-\t\t// chunk[0:i] = shifted[0:i]\n-\t\tchunk.data[col_idx].Reference(shifted.data[col_idx]);\n-\t\t// delayed[0:i] = chunk[i:d-i]\n-\t\tVectorOperations::Copy(shifted.data[col_idx], delayed.data[col_idx], d, i, 0);\n-\t\t// delayed[d-i:d] = input[0:i]\n-\t\tVectorOperations::Copy(input.data[col_idx], delayed.data[col_idx], i, 0, d - i);\n+\t\t// output[0:out] = delayed[0:out]\n+\t\toutput.data[col_idx].Reference(shifted.data[col_idx]);\n+\t\t// delayed[0:out] = delayed[out:delay-out]\n+\t\tVectorOperations::Copy(shifted.data[col_idx], delayed.data[col_idx], delay, out, 0);\n+\t\t// delayed[delay-out:delay-out+in] = input[0:in]\n+\t\tVectorOperations::Copy(input.data[col_idx], delayed.data[col_idx], in, 0, delay - out);\n \t}\n-\tchunk.SetCardinality(i);\n-\tdelayed.SetCardinality(d);\n+\tdelayed.SetCardinality(delay - out + in);\n \n-\tExecuteFunctions(context, chunk, delayed, gstate_p, state_p);\n+\tExecuteFunctions(context, output, delayed, gstate_p, state_p);\n }\n \n void PhysicalStreamingWindow::ExecuteDelayed(ExecutionContext &context, DataChunk &delayed, DataChunk &input,\n-                                             DataChunk &chunk, GlobalOperatorState &gstate_p,\n+                                             DataChunk &output, GlobalOperatorState &gstate_p,\n                                              OperatorState &state_p) const {\n \t// Put payload columns in place\n \tfor (idx_t col_idx = 0; col_idx < delayed.data.size(); col_idx++) {\n-\t\tchunk.data[col_idx].Reference(delayed.data[col_idx]);\n+\t\toutput.data[col_idx].Reference(delayed.data[col_idx]);\n \t}\n \tidx_t count = delayed.size();\n-\tchunk.SetCardinality(count);\n+\toutput.SetCardinality(count);\n \n-\tExecuteFunctions(context, chunk, input, gstate_p, state_p);\n+\tExecuteFunctions(context, output, input, gstate_p, state_p);\n }\n \n-OperatorResultType PhysicalStreamingWindow::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n+OperatorResultType PhysicalStreamingWindow::Execute(ExecutionContext &context, DataChunk &input, DataChunk &output,\n                                                     GlobalOperatorState &gstate_p, OperatorState &state_p) const {\n \tauto &state = state_p.Cast<StreamingWindowState>();\n \tif (!state.initialized) {\n@@ -585,37 +596,37 @@ OperatorResultType PhysicalStreamingWindow::Execute(ExecutionContext &context, D\n \tauto &delayed = state.delayed;\n \t// We can Reset delayed now that no one can be referencing it.\n \tif (!delayed.size()) {\n-\t\tdelayed.Reset();\n+\t\tstate.Reset(delayed);\n \t}\n-\tconst idx_t available = delayed.size() + input.size();\n-\tif (available <= state.lead_count) {\n+\tif (delayed.size() < state.lead_count) {\n \t\t//\tIf we don't have enough to produce a single row,\n \t\t//\tthen just delay more rows, return nothing\n \t\t//\tand ask for more data.\n \t\tdelayed.Append(input);\n-\t\tchunk.SetCardinality(0);\n+\t\toutput.SetCardinality(0);\n \t\treturn OperatorResultType::NEED_MORE_INPUT;\n \t} else if (input.size() < delayed.size()) {\n \t\t// If we can't consume all of the delayed values,\n \t\t// we need to split them instead of referencing them all\n-\t\tExecuteShifted(context, delayed, input, chunk, gstate_p, state_p);\n+\t\toutput.SetCardinality(input.size());\n+\t\tExecuteShifted(context, delayed, input, output, gstate_p, state_p);\n \t\t// We delayed the unused input so ask for more\n \t\treturn OperatorResultType::NEED_MORE_INPUT;\n \t} else if (delayed.size()) {\n \t\t//\tWe have enough delayed rows so flush them\n-\t\tExecuteDelayed(context, delayed, input, chunk, gstate_p, state_p);\n+\t\tExecuteDelayed(context, delayed, input, output, gstate_p, state_p);\n \t\t// Defer resetting delayed as it may be referenced.\n \t\tdelayed.SetCardinality(0);\n \t\t// Come back to process the input\n \t\treturn OperatorResultType::HAVE_MORE_OUTPUT;\n \t} else {\n \t\t//\tNo delayed rows, so emit what we can and delay the rest.\n-\t\tExecuteInput(context, delayed, input, chunk, gstate_p, state_p);\n+\t\tExecuteInput(context, delayed, input, output, gstate_p, state_p);\n \t\treturn OperatorResultType::NEED_MORE_INPUT;\n \t}\n }\n \n-OperatorFinalizeResultType PhysicalStreamingWindow::FinalExecute(ExecutionContext &context, DataChunk &chunk,\n+OperatorFinalizeResultType PhysicalStreamingWindow::FinalExecute(ExecutionContext &context, DataChunk &output,\n                                                                  GlobalOperatorState &gstate_p,\n                                                                  OperatorState &state_p) const {\n \tauto &state = state_p.Cast<StreamingWindowState>();\n@@ -624,8 +635,15 @@ OperatorFinalizeResultType PhysicalStreamingWindow::FinalExecute(ExecutionContex\n \t\tauto &delayed = state.delayed;\n \t\t//\tThere are no more input rows\n \t\tauto &input = state.shifted;\n-\t\tinput.Reset();\n-\t\tExecuteDelayed(context, delayed, input, chunk, gstate_p, state_p);\n+\t\tstate.Reset(input);\n+\n+\t\tif (output.GetCapacity() < delayed.size()) {\n+\t\t\t//\tMore than one output buffer was delayed, so shift in what we can\n+\t\t\toutput.SetCardinality(output.GetCapacity());\n+\t\t\tExecuteShifted(context, delayed, input, output, gstate_p, state_p);\n+\t\t\treturn OperatorFinalizeResultType::HAVE_MORE_OUTPUT;\n+\t\t}\n+\t\tExecuteDelayed(context, delayed, input, output, gstate_p, state_p);\n \t}\n \n \treturn OperatorFinalizeResultType::FINISHED;\ndiff --git a/src/execution/operator/csv_scanner/scanner/column_count_scanner.cpp b/src/execution/operator/csv_scanner/scanner/column_count_scanner.cpp\nindex 906ace12b1e7..2b3361533267 100644\n--- a/src/execution/operator/csv_scanner/scanner/column_count_scanner.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/column_count_scanner.cpp\n@@ -39,7 +39,13 @@ idx_t ColumnCountResult::GetMostFrequentColumnCount() const {\n }\n \n bool ColumnCountResult::AddRow(ColumnCountResult &result, idx_t buffer_pos) {\n+\tconst LinePosition cur_position(result.cur_buffer_idx, buffer_pos + 1, result.current_buffer_size);\n+\tif (cur_position - result.last_position > result.state_machine.options.maximum_line_size.GetValue() &&\n+\t    buffer_pos != NumericLimits<idx_t>::Maximum()) {\n+\t\tresult.error = true;\n+\t}\n \tresult.InternalAddRow();\n+\tresult.last_position = cur_position;\n \tif (!result.states.EmptyLastValue()) {\n \t\tidx_t col_count_idx = result.result_position;\n \t\tfor (idx_t i = 0; i < result.result_position + 1; i++) {\n@@ -99,6 +105,7 @@ ColumnCountScanner::ColumnCountScanner(shared_ptr<CSVBufferManager> buffer_manag\n     : BaseScanner(std::move(buffer_manager), state_machine, std::move(error_handler), true, nullptr, iterator),\n       result(states, *state_machine, result_size_p), column_count(1), result_size(result_size_p) {\n \tsniffing = true;\n+\tresult.last_position = {0, 0, cur_buffer_handle->actual_size};\n }\n \n unique_ptr<StringValueScanner> ColumnCountScanner::UpgradeToStringValueScanner() {\n@@ -117,6 +124,7 @@ unique_ptr<StringValueScanner> ColumnCountScanner::UpgradeToStringValueScanner()\n ColumnCountResult &ColumnCountScanner::ParseChunk() {\n \tresult.result_position = 0;\n \tcolumn_count = 1;\n+\tresult.current_buffer_size = cur_buffer_handle->actual_size;\n \tParseChunkInternal(result);\n \treturn result;\n }\n@@ -139,6 +147,7 @@ void ColumnCountScanner::FinalizeChunkProcess() {\n \t\tif (iterator.pos.buffer_pos == cur_buffer_handle->actual_size) {\n \t\t\t// Move to next buffer\n \t\t\tcur_buffer_handle = buffer_manager->GetBuffer(++iterator.pos.buffer_idx);\n+\n \t\t\tif (!cur_buffer_handle) {\n \t\t\t\tbuffer_handle_ptr = nullptr;\n \t\t\t\tif (states.IsQuotedCurrent() && !states.IsUnquoted()) {\n@@ -158,6 +167,15 @@ void ColumnCountScanner::FinalizeChunkProcess() {\n \t\t\t\t\tresult.AddRow(result, NumericLimits<idx_t>::Maximum());\n \t\t\t\t}\n \t\t\t\treturn;\n+\t\t\t} else {\n+\t\t\t\tresult.cur_buffer_idx = iterator.pos.buffer_idx;\n+\t\t\t\tresult.current_buffer_size = cur_buffer_handle->actual_size;\n+\t\t\t\t// Do a quick check that the line is still sane\n+\t\t\t\tconst LinePosition cur_position(result.cur_buffer_idx, 0, result.current_buffer_size);\n+\t\t\t\tif (cur_position - result.last_position > result.state_machine.options.maximum_line_size.GetValue()) {\n+\t\t\t\t\tresult.error = true;\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n \t\t\t}\n \t\t\titerator.pos.buffer_pos = 0;\n \t\t\tbuffer_handle_ptr = cur_buffer_handle->Ptr();\ndiff --git a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\nindex 29838280ecf5..f48339a029aa 100644\n--- a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n@@ -148,7 +148,7 @@ inline bool IsValueNull(const char *null_str_ptr, const char *value_ptr, const i\n }\n \n bool StringValueResult::HandleTooManyColumnsError(const char *value_ptr, const idx_t size) {\n-\tif (cur_col_id >= number_of_columns) {\n+\tif (cur_col_id >= number_of_columns && state_machine.state_machine_options.rfc_4180.GetValue()) {\n \t\tbool error = true;\n \t\tif (cur_col_id == number_of_columns && ((quoted && state_machine.options.allow_quoted_nulls) || !quoted)) {\n \t\t\t// we make an exception if the first over-value is null\n@@ -220,6 +220,9 @@ void StringValueResult::AddValueToVector(const char *value_ptr, const idx_t size\n \t\treturn;\n \t}\n \tif (cur_col_id >= number_of_columns) {\n+\t\tif (!state_machine.state_machine_options.rfc_4180.GetValue()) {\n+\t\t\treturn;\n+\t\t}\n \t\tbool error = true;\n \t\tif (cur_col_id == number_of_columns && ((quoted && state_machine.options.allow_quoted_nulls) || !quoted)) {\n \t\t\t// we make an exception if the first over-value is null\n@@ -511,6 +514,10 @@ void StringValueResult::AddPossiblyEscapedValue(StringValueResult &result, const\n \t\t\t\treturn;\n \t\t\t}\n \t\t}\n+\t\tif (result.cur_col_id >= result.number_of_columns &&\n+\t\t    !result.state_machine.state_machine_options.rfc_4180.GetValue()) {\n+\t\t\treturn;\n+\t\t}\n \t\tif (!result.HandleTooManyColumnsError(value_ptr, length)) {\n \t\t\t// If it's an escaped value we have to remove all the escapes, this is not really great\n \t\t\t// If we are going to escape, this vector must be a varchar vector\n@@ -520,7 +527,6 @@ void StringValueResult::AddPossiblyEscapedValue(StringValueResult &result, const\n \t\t\t\t\t// We have to write the cast error message.\n \t\t\t\t\tstd::ostringstream error;\n \t\t\t\t\t// Casting Error Message\n-\n \t\t\t\t\terror << \"Could not convert string \\\"\" << std::string(value_ptr, length) << \"\\\" to \\'\"\n \t\t\t\t\t      << LogicalTypeIdToString(result.parse_types[result.chunk_col_id].type_id) << \"\\'\";\n \t\t\t\t\tauto error_string = error.str();\n@@ -533,6 +539,7 @@ void StringValueResult::AddPossiblyEscapedValue(StringValueResult &result, const\n \t\t\t\tauto value = StringValueScanner::RemoveEscape(\n \t\t\t\t    value_ptr, length, result.state_machine.dialect_options.state_machine_options.escape.GetValue(),\n \t\t\t\t    result.state_machine.dialect_options.state_machine_options.quote.GetValue(),\n+\t\t\t\t    result.state_machine.dialect_options.state_machine_options.rfc_4180.GetValue(),\n \t\t\t\t    result.parse_chunk.data[result.chunk_col_id]);\n \t\t\t\tresult.AddValueToVector(value.GetData(), value.GetSize());\n \t\t\t}\n@@ -806,7 +813,7 @@ bool StringValueResult::AddRowInternal() {\n \tquoted_new_line = false;\n \t// We need to check if we are getting the correct number of columns here.\n \t// If columns are correct, we add it, and that's it.\n-\tif (cur_col_id != number_of_columns) {\n+\tif (cur_col_id < number_of_columns) {\n \t\t// We have too few columns:\n \t\tif (null_padding) {\n \t\t\twhile (cur_col_id < number_of_columns) {\n@@ -1231,7 +1238,8 @@ void StringValueScanner::ProcessExtraRow() {\n \t}\n }\n \n-string_t StringValueScanner::RemoveEscape(const char *str_ptr, idx_t end, char escape, char quote, Vector &vector) {\n+string_t StringValueScanner::RemoveEscape(const char *str_ptr, idx_t end, char escape, char quote, bool rfc_4180,\n+                                          Vector &vector) {\n \t// Figure out the exact size\n \tidx_t str_pos = 0;\n \tbool just_escaped = false;\n@@ -1239,7 +1247,7 @@ string_t StringValueScanner::RemoveEscape(const char *str_ptr, idx_t end, char e\n \t\tif (str_ptr[cur_pos] == escape && !just_escaped) {\n \t\t\tjust_escaped = true;\n \t\t} else if (str_ptr[cur_pos] == quote) {\n-\t\t\tif (just_escaped) {\n+\t\t\tif (just_escaped || !rfc_4180) {\n \t\t\t\tstr_pos++;\n \t\t\t}\n \t\t\tjust_escaped = false;\n@@ -1259,7 +1267,7 @@ string_t StringValueScanner::RemoveEscape(const char *str_ptr, idx_t end, char e\n \t\tif (c == escape && !just_escaped) {\n \t\t\tjust_escaped = true;\n \t\t} else if (str_ptr[cur_pos] == quote) {\n-\t\t\tif (just_escaped) {\n+\t\t\tif (just_escaped || !rfc_4180) {\n \t\t\t\tremoved_escapes_ptr[str_pos++] = c;\n \t\t\t}\n \t\t\tjust_escaped = false;\n@@ -1289,10 +1297,8 @@ void StringValueScanner::ProcessOverBufferValue() {\n \t\t}\n \t\tif (states.NewRow() || states.NewValue()) {\n \t\t\tbreak;\n-\t\t} else {\n-\t\t\tif (!result.comment) {\n-\t\t\t\tover_buffer_string += previous_buffer[i];\n-\t\t\t}\n+\t\t} else if (!result.comment) {\n+\t\t\tover_buffer_string += previous_buffer[i];\n \t\t}\n \t\tif (states.IsQuoted()) {\n \t\t\tresult.SetQuoted(result, j);\n@@ -1323,16 +1329,13 @@ void StringValueScanner::ProcessOverBufferValue() {\n \t\tif (states.EmptyLine()) {\n \t\t\tif (state_machine->dialect_options.num_cols == 1) {\n \t\t\t\tbreak;\n-\t\t\t} else {\n-\t\t\t\tcontinue;\n \t\t\t}\n+\t\t\tcontinue;\n \t\t}\n \t\tif (states.NewRow() || states.NewValue()) {\n \t\t\tbreak;\n-\t\t} else {\n-\t\t\tif (!result.comment && !states.IsComment()) {\n-\t\t\t\tover_buffer_string += buffer_handle_ptr[iterator.pos.buffer_pos];\n-\t\t\t}\n+\t\t} else if (!result.comment && !states.IsComment()) {\n+\t\t\tover_buffer_string += buffer_handle_ptr[iterator.pos.buffer_pos];\n \t\t}\n \t\tif (states.IsQuoted()) {\n \t\t\tresult.SetQuoted(result, j);\n@@ -1357,7 +1360,7 @@ void StringValueScanner::ProcessOverBufferValue() {\n \t}\n \tif (!skip_value) {\n \t\tstring_t value;\n-\t\tif (result.quoted) {\n+\t\tif (result.quoted && !result.comment) {\n \t\t\tvalue = string_t(over_buffer_string.c_str() + result.quoted_position,\n \t\t\t                 UnsafeNumericCast<uint32_t>(over_buffer_string.size() - 1 - result.quoted_position));\n \t\t\tif (result.escaped) {\n@@ -1366,6 +1369,7 @@ void StringValueScanner::ProcessOverBufferValue() {\n \t\t\t\t\tvalue = RemoveEscape(str_ptr, over_buffer_string.size() - 2,\n \t\t\t\t\t                     state_machine->dialect_options.state_machine_options.escape.GetValue(),\n \t\t\t\t\t                     state_machine->dialect_options.state_machine_options.quote.GetValue(),\n+\t\t\t\t\t                     result.state_machine.dialect_options.state_machine_options.rfc_4180.GetValue(),\n \t\t\t\t\t                     result.parse_chunk.data[result.chunk_col_id]);\n \t\t\t\t}\n \t\t\t}\n@@ -1376,6 +1380,7 @@ void StringValueScanner::ProcessOverBufferValue() {\n \t\t\t\t\tvalue = RemoveEscape(over_buffer_string.c_str(), over_buffer_string.size(),\n \t\t\t\t\t                     state_machine->dialect_options.state_machine_options.escape.GetValue(),\n \t\t\t\t\t                     state_machine->dialect_options.state_machine_options.quote.GetValue(),\n+\t\t\t\t\t                     result.state_machine.dialect_options.state_machine_options.rfc_4180.GetValue(),\n \t\t\t\t\t                     result.parse_chunk.data[result.chunk_col_id]);\n \t\t\t\t}\n \t\t\t}\n@@ -1436,7 +1441,7 @@ bool StringValueScanner::MoveToNextBuffer() {\n \t\t\t// This means we reached the end of the file, we must add a last line if there is any to be added\n \t\t\tif (states.EmptyLine() || states.NewRow() || result.added_last_line || states.IsCurrentNewRow() ||\n \t\t\t    states.IsNotSet()) {\n-\t\t\t\tif (result.cur_col_id == result.number_of_columns) {\n+\t\t\t\tif (result.cur_col_id == result.number_of_columns && !result.IsStateCurrent(CSVState::INVALID)) {\n \t\t\t\t\tresult.number_of_rows++;\n \t\t\t\t}\n \t\t\t\tresult.cur_col_id = 0;\ndiff --git a/src/execution/operator/csv_scanner/sniffer/header_detection.cpp b/src/execution/operator/csv_scanner/sniffer/header_detection.cpp\nindex f973e8784850..fa48e5ef1758 100644\n--- a/src/execution/operator/csv_scanner/sniffer/header_detection.cpp\n+++ b/src/execution/operator/csv_scanner/sniffer/header_detection.cpp\n@@ -117,9 +117,7 @@ static void ReplaceNames(vector<string> &detected_names, CSVStateMachine &state_\n \t\t\t\t\tdetected_names.push_back(GenerateColumnName(options.name_list.size(), col++));\n \t\t\t\t\tbest_sql_types_candidates_per_column_idx[i] = {LogicalType::VARCHAR};\n \t\t\t\t}\n-\n \t\t\t\tdialect_options.num_cols = options.name_list.size();\n-\n \t\t\t} else {\n \t\t\t\t// we throw an error\n \t\t\t\tconst auto error = CSVError::HeaderSniffingError(\n@@ -128,8 +126,16 @@ static void ReplaceNames(vector<string> &detected_names, CSVStateMachine &state_\n \t\t\t\terror_handler.Error(error);\n \t\t\t}\n \t\t}\n-\t\tfor (idx_t i = 0; i < options.name_list.size(); i++) {\n-\t\t\tdetected_names[i] = options.name_list[i];\n+\t\tif (options.name_list.size() > detected_names.size()) {\n+\t\t\t// we throw an error\n+\t\t\tconst auto error =\n+\t\t\t    CSVError::HeaderSniffingError(options, best_header_row, options.name_list.size(),\n+\t\t\t                                  state_machine.dialect_options.state_machine_options.delimiter.GetValue());\n+\t\t\terror_handler.Error(error);\n+\t\t} else {\n+\t\t\tfor (idx_t i = 0; i < options.name_list.size(); i++) {\n+\t\t\t\tdetected_names[i] = options.name_list[i];\n+\t\t\t}\n \t\t}\n \t}\n }\ndiff --git a/src/execution/sample/reservoir_sample.cpp b/src/execution/sample/reservoir_sample.cpp\nindex ba777b609f02..402d84a80466 100644\n--- a/src/execution/sample/reservoir_sample.cpp\n+++ b/src/execution/sample/reservoir_sample.cpp\n@@ -225,10 +225,6 @@ vector<uint32_t> ReservoirSample::GetRandomizedVector(uint32_t range, uint32_t s\n \tfor (uint32_t i = 0; i < range; i++) {\n \t\tret.push_back(i);\n \t}\n-\tif (size == FIXED_SAMPLE_SIZE) {\n-\t\tstd::shuffle(ret.begin(), ret.end(), base_reservoir_sample->random);\n-\t\treturn ret;\n-\t}\n \tfor (uint32_t i = 0; i < size; i++) {\n \t\tuint32_t random_shuffle = base_reservoir_sample->random.NextRandomInteger32(i, range);\n \t\tif (random_shuffle == i) {\ndiff --git a/src/function/function_binder.cpp b/src/function/function_binder.cpp\nindex 67144182522e..b0e3bbc732e6 100644\n--- a/src/function/function_binder.cpp\n+++ b/src/function/function_binder.cpp\n@@ -457,7 +457,7 @@ unique_ptr<Expression> FunctionBinder::BindScalarFunction(ScalarFunction bound_f\n \t                                                      std::move(children), std::move(bind_info), is_operator);\n \tif (result_func->function.bind_expression) {\n \t\t// if a bind_expression callback is registered - call it and emit the resulting expression\n-\t\tFunctionBindExpressionInput input(context, result_func->bind_info.get(), *result_func);\n+\t\tFunctionBindExpressionInput input(context, result_func->bind_info.get(), result_func->children);\n \t\tresult = result_func->function.bind_expression(input);\n \t}\n \tif (!result) {\ndiff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp\nindex 706712f8193d..517c7a266cb0 100644\n--- a/src/function/table/read_csv.cpp\n+++ b/src/function/table/read_csv.cpp\n@@ -124,7 +124,7 @@ void SchemaDiscovery(ClientContext &context, ReadCSVData &result, CSVReaderOptio\n \t\tnames = best_schema.GetNames();\n \t\treturn_types = best_schema.GetTypes();\n \t}\n-\tif (only_header_or_empty_files == current_file) {\n+\tif (only_header_or_empty_files == current_file && !options.columns_set) {\n \t\tfor (auto &type : return_types) {\n \t\t\tD_ASSERT(type.id() == LogicalTypeId::BOOLEAN);\n \t\t\t// we default to varchar if all files are empty or only have a header after all the sniffing\ndiff --git a/src/function/table/system/duckdb_extensions.cpp b/src/function/table/system/duckdb_extensions.cpp\nindex 64d26dea9fb8..0edc2c2ff929 100644\n--- a/src/function/table/system/duckdb_extensions.cpp\n+++ b/src/function/table/system/duckdb_extensions.cpp\n@@ -149,11 +149,15 @@ unique_ptr<GlobalTableFunctionState> DuckDBExtensionsInit(ClientContext &context\n \t\t\tauto entry = installed_extensions.find(ext_name);\n \t\t\tif (entry == installed_extensions.end() || !entry->second.installed) {\n \t\t\t\tExtensionInformation &info = installed_extensions[ext_name];\n+\n \t\t\t\tinfo.name = ext_name;\n \t\t\t\tinfo.loaded = true;\n \t\t\t\tinfo.extension_version = ext_install_info->version;\n \t\t\t\tinfo.installed = ext_install_info->mode == ExtensionInstallMode::STATICALLY_LINKED;\n \t\t\t\tinfo.install_mode = ext_install_info->mode;\n+\t\t\t\tif (ext_data.install_info->mode == ExtensionInstallMode::STATICALLY_LINKED && info.file_path.empty()) {\n+\t\t\t\t\tinfo.file_path = \"(BUILT-IN)\";\n+\t\t\t\t}\n \t\t\t} else {\n \t\t\t\tentry->second.loaded = true;\n \t\t\t\tentry->second.extension_version = ext_install_info->version;\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/base_scanner.hpp b/src/include/duckdb/execution/operator/csv_scanner/base_scanner.hpp\nindex 93b8bf9ac1f3..b2d9dae68a39 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/base_scanner.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/base_scanner.hpp\n@@ -17,6 +17,35 @@\n namespace duckdb {\n \n class CSVFileScan;\n+\n+//! Class that keeps track of line starts, used for line size verification\n+class LinePosition {\n+public:\n+\tLinePosition() {\n+\t}\n+\tLinePosition(idx_t buffer_idx_p, idx_t buffer_pos_p, idx_t buffer_size_p)\n+\t    : buffer_pos(buffer_pos_p), buffer_size(buffer_size_p), buffer_idx(buffer_idx_p) {\n+\t}\n+\n+\tidx_t operator-(const LinePosition &other) const {\n+\t\tif (other.buffer_idx == buffer_idx) {\n+\t\t\treturn buffer_pos - other.buffer_pos;\n+\t\t}\n+\t\treturn other.buffer_size - other.buffer_pos + buffer_pos;\n+\t}\n+\n+\tbool operator==(const LinePosition &other) const {\n+\t\treturn buffer_pos == other.buffer_pos && buffer_idx == other.buffer_idx && buffer_size == other.buffer_size;\n+\t}\n+\n+\tidx_t GetGlobalPosition(idx_t requested_buffer_size, bool first_char_nl = false) const {\n+\t\treturn requested_buffer_size * buffer_idx + buffer_pos + first_char_nl;\n+\t}\n+\tidx_t buffer_pos = 0;\n+\tidx_t buffer_size = 0;\n+\tidx_t buffer_idx = 0;\n+};\n+\n class ScannerResult {\n public:\n \tScannerResult(CSVStates &states, CSVStateMachine &state_machine, idx_t result_size);\n@@ -52,6 +81,10 @@ class ScannerResult {\n \t\treturn result.comment == true;\n \t}\n \n+\tinline bool IsStateCurrent(CSVState state) const {\n+\t\treturn states.states[1] == state;\n+\t}\n+\n \t//! Variable to keep information regarding quoted and escaped values\n \tbool quoted = false;\n \t//! If the current quoted value is unquoted\n@@ -62,6 +95,8 @@ class ScannerResult {\n \tbool comment = false;\n \tidx_t quoted_position = 0;\n \n+\tLinePosition last_position;\n+\n \t//! Size of the result\n \tconst idx_t result_size;\n \n@@ -88,7 +123,7 @@ class BaseScanner {\n \t//! Returns true if the scanner is finished\n \tbool FinishedFile() const;\n \n-\t//! Parses data into a output_chunk\n+\t//! Parses data into an output_chunk\n \tvirtual ScannerResult &ParseChunk();\n \n \t//! Returns the result from the last Parse call. Shouts at you if you call it wrong\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/column_count_scanner.hpp b/src/include/duckdb/execution/operator/csv_scanner/column_count_scanner.hpp\nindex 8cecfe500d15..5da4d30376e7 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/column_count_scanner.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/column_count_scanner.hpp\n@@ -41,6 +41,9 @@ class ColumnCountResult : public ScannerResult {\n \tbool error = false;\n \tidx_t result_position = 0;\n \tbool cur_line_starts_as_comment = false;\n+\n+\tidx_t cur_buffer_idx = 0;\n+\tidx_t current_buffer_size = 0;\n \t//! How many rows fit a given column count\n \tmap<idx_t, idx_t> rows_per_column_count;\n \t//! Adds a Value to the result\ndiff --git a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\nindex a2a3d5372fe5..12fd0f42759d 100644\n--- a/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\n+++ b/src/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\n@@ -27,34 +27,6 @@ struct CSVBufferUsage {\n \tidx_t buffer_idx;\n };\n \n-//! Class that keeps track of line starts, used for line size verification\n-class LinePosition {\n-public:\n-\tLinePosition() {\n-\t}\n-\tLinePosition(idx_t buffer_idx_p, idx_t buffer_pos_p, idx_t buffer_size_p)\n-\t    : buffer_pos(buffer_pos_p), buffer_size(buffer_size_p), buffer_idx(buffer_idx_p) {\n-\t}\n-\n-\tidx_t operator-(const LinePosition &other) const {\n-\t\tif (other.buffer_idx == buffer_idx) {\n-\t\t\treturn buffer_pos - other.buffer_pos;\n-\t\t}\n-\t\treturn other.buffer_size - other.buffer_pos + buffer_pos;\n-\t}\n-\n-\tbool operator==(const LinePosition &other) const {\n-\t\treturn buffer_pos == other.buffer_pos && buffer_idx == other.buffer_idx && buffer_size == other.buffer_size;\n-\t}\n-\n-\tidx_t GetGlobalPosition(idx_t requested_buffer_size, bool first_char_nl = false) const {\n-\t\treturn requested_buffer_size * buffer_idx + buffer_pos + first_char_nl;\n-\t}\n-\tidx_t buffer_pos = 0;\n-\tidx_t buffer_size = 0;\n-\tidx_t buffer_idx = 0;\n-};\n-\n //! Keeps track of start and end of line positions in regard to the CSV file\n class FullLinePosition {\n public:\n@@ -181,7 +153,7 @@ class StringValueResult : public ScannerResult {\n \tunsafe_vector<ValidityMask *> validity_mask;\n \n \t//! Variables to iterate over the CSV buffers\n-\tLinePosition last_position;\n+\n \tchar *buffer_ptr;\n \tidx_t buffer_size;\n \tidx_t position_before_comment;\n@@ -322,7 +294,8 @@ class StringValueScanner : public BaseScanner {\n \tbool FinishedIterator() const;\n \n \t//! Creates a new string with all escaped values removed\n-\tstatic string_t RemoveEscape(const char *str_ptr, idx_t end, char escape, char quote, Vector &vector);\n+\tstatic string_t RemoveEscape(const char *str_ptr, idx_t end, char escape, char quote, bool rfc_4180,\n+\t                             Vector &vector);\n \n \t//! If we can directly cast the type when consuming the CSV file, or we have to do it later\n \tstatic bool CanDirectlyCast(const LogicalType &type, bool icu_loaded);\ndiff --git a/src/include/duckdb/function/scalar_function.hpp b/src/include/duckdb/function/scalar_function.hpp\nindex 236356b9af8e..f46bb60427df 100644\n--- a/src/include/duckdb/function/scalar_function.hpp\n+++ b/src/include/duckdb/function/scalar_function.hpp\n@@ -78,13 +78,13 @@ struct FunctionModifiedDatabasesInput {\n \n struct FunctionBindExpressionInput {\n \tFunctionBindExpressionInput(ClientContext &context_p, optional_ptr<FunctionData> bind_data_p,\n-\t                            BoundFunctionExpression &function_p)\n-\t    : context(context_p), bind_data(bind_data_p), function(function_p) {\n+\t                            vector<unique_ptr<Expression>> &children_p)\n+\t    : context(context_p), bind_data(bind_data_p), children(children_p) {\n \t}\n \n \tClientContext &context;\n \toptional_ptr<FunctionData> bind_data;\n-\tBoundFunctionExpression &function;\n+\tvector<unique_ptr<Expression>> &children;\n };\n \n struct ScalarFunctionBindInput {\ndiff --git a/src/include/duckdb/main/connection.hpp b/src/include/duckdb/main/connection.hpp\nindex d0935ca8e084..f5b46717c667 100644\n--- a/src/include/duckdb/main/connection.hpp\n+++ b/src/include/duckdb/main/connection.hpp\n@@ -166,14 +166,6 @@ class Connection {\n \tDUCKDB_API shared_ptr<Relation> RelationFromQuery(unique_ptr<SelectStatement> select_stmt,\n \t                                                  const string &alias = \"queryrelation\", const string &query = \"\");\n \n-\t//! Returns a substrait BLOB from a valid query\n-\tDUCKDB_API string GetSubstrait(const string &query);\n-\t//! Returns a Query Result from a substrait blob\n-\tDUCKDB_API unique_ptr<QueryResult> FromSubstrait(const string &proto);\n-\t//! Returns a substrait BLOB from a valid query\n-\tDUCKDB_API string GetSubstraitJSON(const string &query);\n-\t//! Returns a Query Result from a substrait JSON\n-\tDUCKDB_API unique_ptr<QueryResult> FromSubstraitJSON(const string &json);\n \tDUCKDB_API void BeginTransaction();\n \tDUCKDB_API void Commit();\n \tDUCKDB_API void Rollback();\ndiff --git a/src/include/duckdb/main/extension.hpp b/src/include/duckdb/main/extension.hpp\nindex 53de1481d946..b623daeff989 100644\n--- a/src/include/duckdb/main/extension.hpp\n+++ b/src/include/duckdb/main/extension.hpp\n@@ -24,6 +24,7 @@ class Extension {\n \tDUCKDB_API virtual std::string Version() const {\n \t\treturn \"\";\n \t}\n+\tDUCKDB_API static const char *DefaultVersion();\n };\n \n enum class ExtensionABIType : uint8_t {\ndiff --git a/src/include/duckdb/main/extension_entries.hpp b/src/include/duckdb/main/extension_entries.hpp\nindex c8ce3db3a533..530f388b2d57 100644\n--- a/src/include/duckdb/main/extension_entries.hpp\n+++ b/src/include/duckdb/main/extension_entries.hpp\n@@ -135,6 +135,7 @@ static constexpr ExtensionFunctionEntry EXTENSION_FUNCTIONS[] = {\n     {\"covar_samp\", \"core_functions\", CatalogType::AGGREGATE_FUNCTION_ENTRY},\n     {\"create_fts_index\", \"fts\", CatalogType::PRAGMA_FUNCTION_ENTRY},\n     {\"current_database\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n+    {\"current_date\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"current_localtime\", \"icu\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"current_localtimestamp\", \"icu\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"current_query\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n@@ -179,8 +180,6 @@ static constexpr ExtensionFunctionEntry EXTENSION_FUNCTIONS[] = {\n     {\"from_hex\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"from_json\", \"json\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"from_json_strict\", \"json\", CatalogType::SCALAR_FUNCTION_ENTRY},\n-    {\"from_substrait\", \"substrait\", CatalogType::TABLE_FUNCTION_ENTRY},\n-    {\"from_substrait_json\", \"substrait\", CatalogType::TABLE_FUNCTION_ENTRY},\n     {\"fsum\", \"core_functions\", CatalogType::AGGREGATE_FUNCTION_ENTRY},\n     {\"fuzz_all_functions\", \"sqlsmith\", CatalogType::TABLE_FUNCTION_ENTRY},\n     {\"fuzzyduck\", \"sqlsmith\", CatalogType::TABLE_FUNCTION_ENTRY},\n@@ -188,9 +187,8 @@ static constexpr ExtensionFunctionEntry EXTENSION_FUNCTIONS[] = {\n     {\"gcd\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"gen_random_uuid\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"get_bit\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n+    {\"get_current_time\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"get_current_timestamp\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n-    {\"get_substrait\", \"substrait\", CatalogType::TABLE_FUNCTION_ENTRY},\n-    {\"get_substrait_json\", \"substrait\", CatalogType::TABLE_FUNCTION_ENTRY},\n     {\"grade_up\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"greatest\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"greatest_common_divisor\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n@@ -683,6 +681,7 @@ static constexpr ExtensionFunctionEntry EXTENSION_FUNCTIONS[] = {\n     {\"to_timestamp\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"to_weeks\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"to_years\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n+    {\"today\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"tpcds\", \"tpcds\", CatalogType::PRAGMA_FUNCTION_ENTRY},\n     {\"tpcds_answers\", \"tpcds\", CatalogType::TABLE_FUNCTION_ENTRY},\n     {\"tpcds_queries\", \"tpcds\", CatalogType::TABLE_FUNCTION_ENTRY},\n@@ -1069,24 +1068,10 @@ static constexpr ExtensionEntry EXTENSION_SECRET_PROVIDERS[] = {\n     {\"mysql/config\", \"mysql_scanner\"},\n     {\"postgres/config\", \"postgres_scanner\"}}; // EXTENSION_SECRET_PROVIDERS\n \n-static constexpr const char *AUTOLOADABLE_EXTENSIONS[] = {\"aws\",\n-                                                          \"azure\",\n-                                                          \"autocomplete\",\n-                                                          \"core_functions\",\n-                                                          \"delta\",\n-                                                          \"excel\",\n-                                                          \"fts\",\n-                                                          \"httpfs\",\n-                                                          \"inet\",\n-                                                          \"icu\",\n-                                                          \"json\",\n-                                                          \"motherduck\",\n-                                                          \"mysql_scanner\",\n-                                                          \"parquet\",\n-                                                          \"sqlite_scanner\",\n-                                                          \"sqlsmith\",\n-                                                          \"postgres_scanner\",\n-                                                          \"tpcds\",\n-                                                          \"tpch\"}; // END_OF_AUTOLOADABLE_EXTENSIONS\n+static constexpr const char *AUTOLOADABLE_EXTENSIONS[] = {\n+    \"aws\",        \"azure\",         \"autocomplete\", \"core_functions\", \"delta\",    \"excel\",\n+    \"fts\",        \"httpfs\",        \"iceberg\",      \"inet\",           \"icu\",      \"json\",\n+    \"motherduck\", \"mysql_scanner\", \"parquet\",      \"sqlite_scanner\", \"sqlsmith\", \"postgres_scanner\",\n+    \"tpcds\",      \"tpch\",          \"uc_catalog\"}; // END_OF_AUTOLOADABLE_EXTENSIONS\n \n } // namespace duckdb\ndiff --git a/src/main/connection.cpp b/src/main/connection.cpp\nindex 119b39c892ad..a5742dbfdbfa 100644\n--- a/src/main/connection.cpp\n+++ b/src/main/connection.cpp\n@@ -28,9 +28,10 @@ Connection::Connection(DatabaseInstance &database)\n }\n \n Connection::Connection(DuckDB &database) : Connection(*database.instance) {\n+\t// Initialization of warning_cb happens in the other constructor\n }\n \n-Connection::Connection(Connection &&other) noexcept {\n+Connection::Connection(Connection &&other) noexcept : warning_cb(nullptr) {\n \tstd::swap(context, other.context);\n \tstd::swap(warning_cb, other.warning_cb);\n }\n@@ -98,34 +99,6 @@ unique_ptr<MaterializedQueryResult> Connection::Query(const string &query) {\n \treturn unique_ptr_cast<QueryResult, MaterializedQueryResult>(std::move(result));\n }\n \n-DUCKDB_API string Connection::GetSubstrait(const string &query) {\n-\tvector<Value> params;\n-\tparams.emplace_back(query);\n-\tauto result = TableFunction(\"get_substrait\", params)->Execute();\n-\tauto protobuf = result->FetchRaw()->GetValue(0, 0);\n-\treturn protobuf.GetValueUnsafe<string_t>().GetString();\n-}\n-\n-DUCKDB_API unique_ptr<QueryResult> Connection::FromSubstrait(const string &proto) {\n-\tvector<Value> params;\n-\tparams.emplace_back(Value::BLOB_RAW(proto));\n-\treturn TableFunction(\"from_substrait\", params)->Execute();\n-}\n-\n-DUCKDB_API string Connection::GetSubstraitJSON(const string &query) {\n-\tvector<Value> params;\n-\tparams.emplace_back(query);\n-\tauto result = TableFunction(\"get_substrait_json\", params)->Execute();\n-\tauto protobuf = result->FetchRaw()->GetValue(0, 0);\n-\treturn protobuf.GetValueUnsafe<string_t>().GetString();\n-}\n-\n-DUCKDB_API unique_ptr<QueryResult> Connection::FromSubstraitJSON(const string &json) {\n-\tvector<Value> params;\n-\tparams.emplace_back(json);\n-\treturn TableFunction(\"from_substrait_json\", params)->Execute();\n-}\n-\n unique_ptr<MaterializedQueryResult> Connection::Query(unique_ptr<SQLStatement> statement) {\n \tauto result = context->Query(std::move(statement), false);\n \tD_ASSERT(result->type == QueryResultType::MATERIALIZED_RESULT);\ndiff --git a/src/main/extension.cpp b/src/main/extension.cpp\nindex c13971b0c2ba..e07ce4c53a64 100644\n--- a/src/main/extension.cpp\n+++ b/src/main/extension.cpp\n@@ -50,18 +50,29 @@ string ParsedExtensionMetaData::GetInvalidMetadataError() {\n \t\tconst string engine_version = string(ExtensionHelper::GetVersionDirectoryName());\n \n \t\tif (engine_version != duckdb_version) {\n-\t\t\tresult += StringUtil::Format(\"The file was built for DuckDB version '%s', but we can only load extensions \"\n-\t\t\t                             \"built for DuckDB version '%s'.\",\n+\t\t\tresult += StringUtil::Format(\"The file was built specifically for DuckDB version '%s' and can only be \"\n+\t\t\t                             \"loaded with that version of DuckDB. (this version of DuckDB is '%s')\",\n \t\t\t                             PrettyPrintString(duckdb_version), engine_version);\n \t\t}\n-\t\t// C_STRUCT ABI versioning works when current duckdb version >= required version\n+\t\t// C_STRUCT ABI versioning\n \t} else if (abi_type == ExtensionABIType::C_STRUCT) {\n-\n-\t\tif (!VersioningUtils::IsSupportedCAPIVersion(duckdb_capi_version)) {\n-\t\t\tresult += StringUtil::Format(\"The file was built for DuckDB version '%s', but we can only load extensions \"\n-\t\t\t                             \"built for DuckDB C API 'v%lld.%lld.%lld' and lower.\",\n-\t\t\t                             duckdb_capi_version, DUCKDB_EXTENSION_API_VERSION_MAJOR,\n-\t\t\t                             DUCKDB_EXTENSION_API_VERSION_MINOR, DUCKDB_EXTENSION_API_VERSION_PATCH);\n+\t\tidx_t major, minor, patch;\n+\t\tif (!VersioningUtils::ParseSemver(duckdb_capi_version, major, minor, patch)) {\n+\t\t\tresult += StringUtil::Format(\"The file was built for DuckDB C API version '%s', which failed to parse as a \"\n+\t\t\t                             \"recognized version string\",\n+\t\t\t                             duckdb_capi_version, DUCKDB_EXTENSION_API_VERSION_MAJOR);\n+\t\t} else if (major != DUCKDB_EXTENSION_API_VERSION_MAJOR) {\n+\t\t\t// Special case where the extension is built for a completely unsupported API\n+\t\t\tresult +=\n+\t\t\t    StringUtil::Format(\"The file was built for DuckDB C API version '%s', but we can only load extensions \"\n+\t\t\t                       \"built for DuckDB C API 'v%lld.x.y'.\",\n+\t\t\t                       duckdb_capi_version, DUCKDB_EXTENSION_API_VERSION_MAJOR);\n+\t\t} else if (!VersioningUtils::IsSupportedCAPIVersion(major, minor, patch)) {\n+\t\t\tresult +=\n+\t\t\t    StringUtil::Format(\"The file was built for DuckDB C API version '%s', but we can only load extensions \"\n+\t\t\t                       \"built for DuckDB C API 'v%lld.%lld.%lld' and lower.\",\n+\t\t\t                       duckdb_capi_version, DUCKDB_EXTENSION_API_VERSION_MAJOR,\n+\t\t\t                       DUCKDB_EXTENSION_API_VERSION_MINOR, DUCKDB_EXTENSION_API_VERSION_PATCH);\n \t\t}\n \t} else {\n \t\tthrow InternalException(\"Unknown ABI type for extension: \" + extension_abi_metadata);\n@@ -137,4 +148,11 @@ bool VersioningUtils::ParseSemver(string &semver, idx_t &major_out, idx_t &minor\n \treturn true;\n }\n \n+const char *Extension::DefaultVersion() {\n+\tif (ExtensionHelper::IsRelease(DuckDB::LibraryVersion())) {\n+\t\treturn DuckDB::LibraryVersion();\n+\t}\n+\treturn DuckDB::SourceID();\n+}\n+\n } // namespace duckdb\ndiff --git a/src/main/extension/extension_helper.cpp b/src/main/extension/extension_helper.cpp\nindex 015d758820ac..c7b613226a10 100644\n--- a/src/main/extension/extension_helper.cpp\n+++ b/src/main/extension/extension_helper.cpp\n@@ -114,7 +114,6 @@ static const DefaultExtension internal_extensions[] = {\n     {\"postgres_scanner\", \"Adds support for connecting to a Postgres database\", false},\n     {\"inet\", \"Adds support for IP-related data types and functions\", false},\n     {\"spatial\", \"Geospatial extension that adds support for working with spatial data and functions\", false},\n-    {\"substrait\", \"Adds support for the Substrait integration\", false},\n     {\"aws\", \"Provides features that depend on the AWS SDK\", false},\n     {\"arrow\", \"A zero-copy data integration between Apache Arrow and DuckDB\", false},\n     {\"azure\", \"Adds a filesystem abstraction for Azure blob storage to DuckDB\", false},\n@@ -140,7 +139,7 @@ DefaultExtension ExtensionHelper::GetDefaultExtension(idx_t index) {\n // Allow Auto-Install Extensions\n //===--------------------------------------------------------------------===//\n static const char *const auto_install[] = {\"motherduck\", \"postgres_scanner\", \"mysql_scanner\", \"sqlite_scanner\",\n-                                           nullptr};\n+                                           \"delta\",      \"iceberg\",          \"uc_catalog\",    nullptr};\n \n // TODO: unify with new autoload mechanism\n bool ExtensionHelper::AllowAutoInstall(const string &extension) {\ndiff --git a/src/main/extension/extension_load.cpp b/src/main/extension/extension_load.cpp\nindex 23101f722f3d..84b28fef0939 100644\n--- a/src/main/extension/extension_load.cpp\n+++ b/src/main/extension/extension_load.cpp\n@@ -399,15 +399,13 @@ bool ExtensionHelper::TryInitialLoad(DatabaseInstance &db, FileSystem &fs, const\n \t\t\tsignature_valid = false;\n \t\t}\n \n-\t\tif (!signature_valid) {\n-\t\t\tthrow IOException(db.config.error_manager->FormatException(ErrorType::UNSIGNED_EXTENSION, filename) +\n-\t\t\t                  metadata_mismatch_error);\n-\t\t}\n-\n \t\tif (!metadata_mismatch_error.empty()) {\n-\t\t\t// Signed extensions perform the full check\n \t\t\tthrow InvalidInputException(metadata_mismatch_error);\n \t\t}\n+\n+\t\tif (!signature_valid) {\n+\t\t\tthrow IOException(db.config.error_manager->FormatException(ErrorType::UNSIGNED_EXTENSION, filename));\n+\t\t}\n \t} else if (!db.config.options.allow_extensions_metadata_mismatch) {\n \t\tif (!metadata_mismatch_error.empty()) {\n \t\t\t// Unsigned extensions AND configuration allowing n, loading allowed, mainly for\ndiff --git a/src/planner/binder/expression/bind_columnref_expression.cpp b/src/planner/binder/expression/bind_columnref_expression.cpp\nindex 1c9bc238ba8a..e5995d2915b9 100644\n--- a/src/planner/binder/expression/bind_columnref_expression.cpp\n+++ b/src/planner/binder/expression/bind_columnref_expression.cpp\n@@ -436,11 +436,13 @@ BindResult ExpressionBinder::BindExpression(ColumnRefExpression &col_ref_p, idx_\n \t\t\tif (found_alias) {\n \t\t\t\treturn alias_result;\n \t\t\t}\n-\n-\t\t\t// column was not found - check if it is a SQL value function\n-\t\t\tauto value_function = GetSQLValueFunction(col_ref_p.GetColumnName());\n-\t\t\tif (value_function) {\n-\t\t\t\treturn BindExpression(value_function, depth);\n+\t\t\tfound_alias = QualifyColumnAlias(col_ref_p);\n+\t\t\tif (!found_alias) {\n+\t\t\t\t// column was not found - check if it is a SQL value function\n+\t\t\t\tauto value_function = GetSQLValueFunction(col_ref_p.GetColumnName());\n+\t\t\t\tif (value_function) {\n+\t\t\t\t\treturn BindExpression(value_function, depth);\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \t\terror.AddQueryLocation(col_ref_p);\ndiff --git a/src/planner/binder/tableref/bind_table_function.cpp b/src/planner/binder/tableref/bind_table_function.cpp\nindex 3f4c249bda23..26dd86c9dfd1 100644\n--- a/src/planner/binder/tableref/bind_table_function.cpp\n+++ b/src/planner/binder/tableref/bind_table_function.cpp\n@@ -203,7 +203,9 @@ unique_ptr<LogicalOperator> Binder::BindTableFunctionInternal(TableFunction &tab\n \t\t                                  table_function.function_info.get(), this, table_function, ref);\n \t\tif (table_function.bind_replace) {\n \t\t\tauto new_plan = table_function.bind_replace(context, bind_input);\n-\t\t\tif (new_plan != nullptr) {\n+\t\t\tif (new_plan) {\n+\t\t\t\tnew_plan->alias = ref.alias;\n+\t\t\t\tnew_plan->column_name_alias = ref.column_name_alias;\n \t\t\t\treturn CreatePlan(*Bind(*new_plan));\n \t\t\t} else if (!table_function.bind) {\n \t\t\t\tthrow BinderException(\"Failed to bind \\\"%s\\\": nullptr returned from bind_replace without bind function\",\ndiff --git a/src/planner/expression/bound_function_expression.cpp b/src/planner/expression/bound_function_expression.cpp\nindex 1be27254099b..be146bd5090f 100644\n--- a/src/planner/expression/bound_function_expression.cpp\n+++ b/src/planner/expression/bound_function_expression.cpp\n@@ -109,12 +109,28 @@ void BoundFunctionExpression::Serialize(Serializer &serializer) const {\n unique_ptr<Expression> BoundFunctionExpression::Deserialize(Deserializer &deserializer) {\n \tauto return_type = deserializer.ReadProperty<LogicalType>(200, \"return_type\");\n \tauto children = deserializer.ReadProperty<vector<unique_ptr<Expression>>>(201, \"children\");\n+\n \tauto entry = FunctionSerializer::Deserialize<ScalarFunction, ScalarFunctionCatalogEntry>(\n \t    deserializer, CatalogType::SCALAR_FUNCTION_ENTRY, children, return_type);\n \tauto function_return_type = entry.first.return_type;\n+\n+\tauto is_operator = deserializer.ReadProperty<bool>(202, \"is_operator\");\n+\n+\tif (entry.first.bind_expression) {\n+\t\t// bind the function expression\n+\t\tauto &context = deserializer.Get<ClientContext &>();\n+\t\tauto bind_input = FunctionBindExpressionInput(context, entry.second, children);\n+\t\t// replace the function expression with the bound expression\n+\t\tauto bound_expression = entry.first.bind_expression(bind_input);\n+\t\tif (bound_expression) {\n+\t\t\treturn bound_expression;\n+\t\t}\n+\t\t// Otherwise, fall thorugh and continue on normally\n+\t}\n+\n \tauto result = make_uniq<BoundFunctionExpression>(std::move(function_return_type), std::move(entry.first),\n \t                                                 std::move(children), std::move(entry.second));\n-\tdeserializer.ReadProperty(202, \"is_operator\", result->is_operator);\n+\tresult->is_operator = is_operator;\n \tif (result->return_type != return_type) {\n \t\t// return type mismatch - push a cast\n \t\tauto &context = deserializer.Get<ClientContext &>();\ndiff --git a/tools/pythonpkg/duckdb-stubs/__init__.pyi b/tools/pythonpkg/duckdb-stubs/__init__.pyi\nindex b98763f7d3d9..8c57cce2aa8f 100644\n--- a/tools/pythonpkg/duckdb-stubs/__init__.pyi\n+++ b/tools/pythonpkg/duckdb-stubs/__init__.pyi\n@@ -343,10 +343,6 @@ class DuckDBPyConnection:\n     def from_arrow(self, arrow_object: object) -> DuckDBPyRelation: ...\n     def from_parquet(self, file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None) -> DuckDBPyRelation: ...\n     def read_parquet(self, file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None) -> DuckDBPyRelation: ...\n-    def from_substrait(self, proto: bytes) -> DuckDBPyRelation: ...\n-    def get_substrait(self, query: str, *, enable_optimizer: bool = True) -> str: ...\n-    def get_substrait_json(self, query: str, *, enable_optimizer: bool = True) -> str: ...\n-    def from_substrait_json(self, json: str) -> DuckDBPyRelation: ...\n     def get_table_names(self, query: str) -> Set[str]: ...\n     def install_extension(self, extension: str, *, force_install: bool = False, repository: Optional[str] = None, repository_url: Optional[str] = None, version: Optional[str] = None) -> None: ...\n     def load_extension(self, extension: str) -> None: ...\n@@ -694,10 +690,6 @@ def from_df(df: pandas.DataFrame, *, connection: DuckDBPyConnection = ...) -> Du\n def from_arrow(arrow_object: object, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n def from_parquet(file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n def read_parquet(file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n-def from_substrait(proto: bytes, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n-def get_substrait(query: str, *, enable_optimizer: bool = True, connection: DuckDBPyConnection = ...) -> str: ...\n-def get_substrait_json(query: str, *, enable_optimizer: bool = True, connection: DuckDBPyConnection = ...) -> str: ...\n-def from_substrait_json(json: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n def get_table_names(query: str, *, connection: DuckDBPyConnection = ...) -> Set[str]: ...\n def install_extension(extension: str, *, force_install: bool = False, repository: Optional[str] = None, repository_url: Optional[str] = None, version: Optional[str] = None, connection: DuckDBPyConnection = ...) -> None: ...\n def load_extension(extension: str, *, connection: DuckDBPyConnection = ...) -> None: ...\ndiff --git a/tools/pythonpkg/duckdb/__init__.py b/tools/pythonpkg/duckdb/__init__.py\nindex 04dfb7640dd2..4799270c3e31 100644\n--- a/tools/pythonpkg/duckdb/__init__.py\n+++ b/tools/pythonpkg/duckdb/__init__.py\n@@ -125,10 +125,6 @@\n \tread_parquet,\n \tfrom_parquet,\n \tread_parquet,\n-\tfrom_substrait,\n-\tget_substrait,\n-\tget_substrait_json,\n-\tfrom_substrait_json,\n \tget_table_names,\n \tinstall_extension,\n \tload_extension,\n@@ -208,10 +204,6 @@\n \t'read_parquet',\n \t'from_parquet',\n \t'read_parquet',\n-\t'from_substrait',\n-\t'get_substrait',\n-\t'get_substrait_json',\n-\t'from_substrait_json',\n \t'get_table_names',\n \t'install_extension',\n \t'load_extension',\ndiff --git a/tools/pythonpkg/duckdb_python.cpp b/tools/pythonpkg/duckdb_python.cpp\nindex 784269f4bd35..dcffa2bb6c7e 100644\n--- a/tools/pythonpkg/duckdb_python.cpp\n+++ b/tools/pythonpkg/duckdb_python.cpp\n@@ -754,46 +754,6 @@ static void InitializeConnectionMethods(py::module_ &m) {\n \t    py::arg(\"binary_as_string\") = false, py::kw_only(), py::arg(\"file_row_number\") = false,\n \t    py::arg(\"filename\") = false, py::arg(\"hive_partitioning\") = false, py::arg(\"union_by_name\") = false,\n \t    py::arg(\"compression\") = py::none(), py::arg(\"connection\") = py::none());\n-\tm.def(\n-\t    \"from_substrait\",\n-\t    [](py::bytes &proto, shared_ptr<DuckDBPyConnection> conn = nullptr) {\n-\t\t    if (!conn) {\n-\t\t\t    conn = DuckDBPyConnection::DefaultConnection();\n-\t\t    }\n-\t\t    return conn->FromSubstrait(proto);\n-\t    },\n-\t    \"Create a query object from protobuf plan\", py::arg(\"proto\"), py::kw_only(),\n-\t    py::arg(\"connection\") = py::none());\n-\tm.def(\n-\t    \"get_substrait\",\n-\t    [](const string &query, bool enable_optimizer = true, shared_ptr<DuckDBPyConnection> conn = nullptr) {\n-\t\t    if (!conn) {\n-\t\t\t    conn = DuckDBPyConnection::DefaultConnection();\n-\t\t    }\n-\t\t    return conn->GetSubstrait(query, enable_optimizer);\n-\t    },\n-\t    \"Serialize a query to protobuf\", py::arg(\"query\"), py::kw_only(), py::arg(\"enable_optimizer\") = true,\n-\t    py::arg(\"connection\") = py::none());\n-\tm.def(\n-\t    \"get_substrait_json\",\n-\t    [](const string &query, bool enable_optimizer = true, shared_ptr<DuckDBPyConnection> conn = nullptr) {\n-\t\t    if (!conn) {\n-\t\t\t    conn = DuckDBPyConnection::DefaultConnection();\n-\t\t    }\n-\t\t    return conn->GetSubstraitJSON(query, enable_optimizer);\n-\t    },\n-\t    \"Serialize a query to protobuf on the JSON format\", py::arg(\"query\"), py::kw_only(),\n-\t    py::arg(\"enable_optimizer\") = true, py::arg(\"connection\") = py::none());\n-\tm.def(\n-\t    \"from_substrait_json\",\n-\t    [](const string &json, shared_ptr<DuckDBPyConnection> conn = nullptr) {\n-\t\t    if (!conn) {\n-\t\t\t    conn = DuckDBPyConnection::DefaultConnection();\n-\t\t    }\n-\t\t    return conn->FromSubstraitJSON(json);\n-\t    },\n-\t    \"Create a query object from a JSON protobuf plan\", py::arg(\"json\"), py::kw_only(),\n-\t    py::arg(\"connection\") = py::none());\n \tm.def(\n \t    \"get_table_names\",\n \t    [](const string &query, shared_ptr<DuckDBPyConnection> conn = nullptr) {\ndiff --git a/tools/pythonpkg/scripts/connection_methods.json b/tools/pythonpkg/scripts/connection_methods.json\nindex e64d60f772fc..19f06c760309 100644\n--- a/tools/pythonpkg/scripts/connection_methods.json\n+++ b/tools/pythonpkg/scripts/connection_methods.json\n@@ -988,68 +988,6 @@\n \t\t],\n \t\t\"return\": \"DuckDBPyRelation\"\n \t},\n-\t{\n-\t\t\"name\": \"from_substrait\",\n-\t\t\"function\": \"FromSubstrait\",\n-\t\t\"docs\": \"Create a query object from protobuf plan\",\n-\t\t\"args\": [\n-\t\t\t{\n-\t\t\t\t\"name\": \"proto\",\n-\t\t\t\t\"type\": \"bytes\"\n-\t\t\t}\n-\t\t],\n-\t\t\"return\": \"DuckDBPyRelation\"\n-\t},\n-\t{\n-\t\t\"name\": \"get_substrait\",\n-\t\t\"function\": \"GetSubstrait\",\n-\t\t\"docs\": \"Serialize a query to protobuf\",\n-\t\t\"args\": [\n-\t\t\t{\n-\t\t\t\t\"name\": \"query\",\n-\t\t\t\t\"type\": \"str\"\n-\t\t\t}\n-\t\t],\n-\t\t\"kwargs\": [\n-\t\t\t{\n-\t\t\t\t\"name\": \"enable_optimizer\",\n-\t\t\t\t\"default\": \"True\",\n-\t\t\t\t\"type\": \"bool\"\n-\t\t\t}\n-\t\t],\n-\t\t\"return\": \"str\"\n-\t},\n-\t{\n-\t\t\"name\": \"get_substrait_json\",\n-\t\t\"function\": \"GetSubstraitJSON\",\n-\t\t\"docs\": \"Serialize a query to protobuf on the JSON format\",\n-\t\t\"args\": [\n-\t\t\t{\n-\t\t\t\t\"name\": \"query\",\n-\t\t\t\t\"type\": \"str\"\n-\t\t\t}\n-\t\t],\n-\t\t\"kwargs\": [\n-\t\t\t{\n-\t\t\t\t\"name\": \"enable_optimizer\",\n-\t\t\t\t\"default\": \"True\",\n-\t\t\t\t\"type\": \"bool\"\n-\t\t\t}\n-\t\t],\n-\t\t\"return\": \"str\"\n-\t},\n-\t{\n-\t\t\"name\": \"from_substrait_json\",\n-\t\t\"function\": \"FromSubstraitJSON\",\n-\t\t\"docs\": \"Create a query object from a JSON protobuf plan\",\n-\t\t\"args\": [\n-\t\t\t{\n-\t\t\t\t\"name\": \"json\",\n-\t\t\t\t\"type\": \"str\"\n-\t\t\t}\n-\t\t],\n-\t\t\"return\": \"DuckDBPyRelation\"\n-\t},\n \t{\n \t\t\"name\": \"get_table_names\",\n \t\t\"function\": \"GetTableNames\",\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\nindex aad8c44bc3ca..e108cbe5ff59 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\n@@ -271,14 +271,6 @@ struct DuckDBPyConnection : public enable_shared_from_this<DuckDBPyConnection> {\n \n \tunique_ptr<DuckDBPyRelation> FromArrow(py::object &arrow_object);\n \n-\tunique_ptr<DuckDBPyRelation> FromSubstrait(py::bytes &proto);\n-\n-\tunique_ptr<DuckDBPyRelation> GetSubstrait(const string &query, bool enable_optimizer = true);\n-\n-\tunique_ptr<DuckDBPyRelation> GetSubstraitJSON(const string &query, bool enable_optimizer = true);\n-\n-\tunique_ptr<DuckDBPyRelation> FromSubstraitJSON(const string &json);\n-\n \tunordered_set<string> GetTableNames(const string &query);\n \n \tshared_ptr<DuckDBPyConnection> UnregisterPythonObject(const string &name);\ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 13cd5e1512ba..b1131801ead7 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -266,15 +266,6 @@ static void InitializeConnectionMethods(py::class_<DuckDBPyConnection, shared_pt\n \t      py::arg(\"binary_as_string\") = false, py::kw_only(), py::arg(\"file_row_number\") = false,\n \t      py::arg(\"filename\") = false, py::arg(\"hive_partitioning\") = false, py::arg(\"union_by_name\") = false,\n \t      py::arg(\"compression\") = py::none());\n-\tm.def(\"from_substrait\", &DuckDBPyConnection::FromSubstrait, \"Create a query object from protobuf plan\",\n-\t      py::arg(\"proto\"));\n-\tm.def(\"get_substrait\", &DuckDBPyConnection::GetSubstrait, \"Serialize a query to protobuf\", py::arg(\"query\"),\n-\t      py::kw_only(), py::arg(\"enable_optimizer\") = true);\n-\tm.def(\"get_substrait_json\", &DuckDBPyConnection::GetSubstraitJSON,\n-\t      \"Serialize a query to protobuf on the JSON format\", py::arg(\"query\"), py::kw_only(),\n-\t      py::arg(\"enable_optimizer\") = true);\n-\tm.def(\"from_substrait_json\", &DuckDBPyConnection::FromSubstraitJSON,\n-\t      \"Create a query object from a JSON protobuf plan\", py::arg(\"json\"));\n \tm.def(\"get_table_names\", &DuckDBPyConnection::GetTableNames, \"Extract the required table names from a query\",\n \t      py::arg(\"query\"));\n \tm.def(\"install_extension\", &DuckDBPyConnection::InstallExtension,\n@@ -1731,40 +1722,6 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrow(py::object &arrow_obj\n \treturn make_uniq<DuckDBPyRelation>(std::move(rel));\n }\n \n-unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstrait(py::bytes &proto) {\n-\tauto &connection = con.GetConnection();\n-\tstring name = \"substrait_\" + StringUtil::GenerateRandomName();\n-\tvector<Value> params;\n-\tparams.emplace_back(Value::BLOB_RAW(proto));\n-\treturn make_uniq<DuckDBPyRelation>(connection.TableFunction(\"from_substrait\", params)->Alias(name));\n-}\n-\n-unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstrait(const string &query, bool enable_optimizer) {\n-\tauto &connection = con.GetConnection();\n-\tvector<Value> params;\n-\tparams.emplace_back(query);\n-\tnamed_parameter_map_t named_parameters({{\"enable_optimizer\", Value::BOOLEAN(enable_optimizer)}});\n-\treturn make_uniq<DuckDBPyRelation>(\n-\t    connection.TableFunction(\"get_substrait\", params, named_parameters)->Alias(query));\n-}\n-\n-unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstraitJSON(const string &query, bool enable_optimizer) {\n-\tauto &connection = con.GetConnection();\n-\tvector<Value> params;\n-\tparams.emplace_back(query);\n-\tnamed_parameter_map_t named_parameters({{\"enable_optimizer\", Value::BOOLEAN(enable_optimizer)}});\n-\treturn make_uniq<DuckDBPyRelation>(\n-\t    connection.TableFunction(\"get_substrait_json\", params, named_parameters)->Alias(query));\n-}\n-\n-unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstraitJSON(const string &json) {\n-\tauto &connection = con.GetConnection();\n-\tstring name = \"from_substrait_\" + StringUtil::GenerateRandomName();\n-\tvector<Value> params;\n-\tparams.emplace_back(json);\n-\treturn make_uniq<DuckDBPyRelation>(connection.TableFunction(\"from_substrait_json\", params)->Alias(name));\n-}\n-\n unordered_set<string> DuckDBPyConnection::GetTableNames(const string &query) {\n \tauto &connection = con.GetConnection();\n \treturn connection.GetTableNames(query);\ndiff --git a/tools/sqlite3_api_wrapper/include/shell_extension.hpp b/tools/sqlite3_api_wrapper/include/shell_extension.hpp\nindex 429adbfd5279..090d99cb44ec 100644\n--- a/tools/sqlite3_api_wrapper/include/shell_extension.hpp\n+++ b/tools/sqlite3_api_wrapper/include/shell_extension.hpp\n@@ -16,6 +16,7 @@ class ShellExtension : public Extension {\n public:\n \tvoid Load(DuckDB &db) override;\n \tstd::string Name() override;\n+\tstd::string Version() const override;\n };\n \n } // namespace duckdb\ndiff --git a/tools/sqlite3_api_wrapper/shell_extension.cpp b/tools/sqlite3_api_wrapper/shell_extension.cpp\nindex e2c074060d40..b553635ff534 100644\n--- a/tools/sqlite3_api_wrapper/shell_extension.cpp\n+++ b/tools/sqlite3_api_wrapper/shell_extension.cpp\n@@ -38,4 +38,8 @@ std::string ShellExtension::Name() {\n \treturn \"shell\";\n }\n \n+std::string ShellExtension::Version() const {\n+\treturn DefaultVersion();\n+}\n+\n } // namespace duckdb\n",
  "test_patch": "diff --git a/test/api/adbc/test_adbc.cpp b/test/api/adbc/test_adbc.cpp\nindex c61d2696fa68..afcbb596d073 100644\n--- a/test/api/adbc/test_adbc.cpp\n+++ b/test/api/adbc/test_adbc.cpp\n@@ -1017,96 +1017,6 @@ TEST_CASE(\"Test ADBC ConnectionGetTableSchema\", \"[adbc]\") {\n \tadbc_error.release(&adbc_error);\n }\n \n-TEST_CASE(\"Test ADBC Substrait\", \"[adbc]\") {\n-\tif (!duckdb_lib) {\n-\t\treturn;\n-\t}\n-\tAdbcDatabase adbc_database;\n-\tAdbcConnection adbc_connection;\n-\n-\tAdbcError adbc_error;\n-\tAdbcStatement adbc_statement;\n-\tInitializeADBCError(&adbc_error);\n-\n-\tArrowArrayStream arrow_stream;\n-\tArrowArray arrow_array;\n-\n-\tREQUIRE(SUCCESS(AdbcDatabaseNew(&adbc_database, &adbc_error)));\n-\tREQUIRE(SUCCESS(AdbcDatabaseSetOption(&adbc_database, \"driver\", duckdb_lib, &adbc_error)));\n-\tREQUIRE(SUCCESS(AdbcDatabaseSetOption(&adbc_database, \"entrypoint\", \"duckdb_adbc_init\", &adbc_error)));\n-\tREQUIRE(SUCCESS(AdbcDatabaseSetOption(&adbc_database, \"path\", \":memory:\", &adbc_error)));\n-\n-\tREQUIRE(SUCCESS(AdbcDatabaseInit(&adbc_database, &adbc_error)));\n-\n-\tREQUIRE(SUCCESS(AdbcConnectionNew(&adbc_connection, &adbc_error)));\n-\tREQUIRE(SUCCESS(AdbcConnectionInit(&adbc_connection, &adbc_database, &adbc_error)));\n-\n-\tauto conn = static_cast<Connection *>(adbc_connection.private_data);\n-\tif (!conn->context->db->ExtensionIsLoaded(\"substrait\")) {\n-\t\t// We need substrait to run this test\n-\t\tREQUIRE(SUCCESS(AdbcConnectionRelease(&adbc_connection, &adbc_error)));\n-\t\tREQUIRE(SUCCESS(AdbcDatabaseRelease(&adbc_database, &adbc_error)));\n-\t\treturn;\n-\t}\n-\t// Insert Data\n-\tADBCTestDatabase db;\n-\tauto &input_data = db.QueryArrow(\"SELECT 'Push Ups' as exercise, 3 as difficulty_level;\");\n-\tstring table_name = \"crossfit\";\n-\tREQUIRE(SUCCESS(AdbcStatementNew(&adbc_connection, &adbc_statement, &adbc_error)));\n-\n-\tREQUIRE(\n-\t    SUCCESS(StatementSetOption(&adbc_statement, ADBC_INGEST_OPTION_TARGET_TABLE, table_name.c_str(), &adbc_error)));\n-\n-\tREQUIRE(SUCCESS(StatementBindStream(&adbc_statement, &input_data, &adbc_error)));\n-\n-\tREQUIRE(SUCCESS(StatementExecuteQuery(&adbc_statement, nullptr, nullptr, &adbc_error)));\n-\n-\t// SELECT COUNT(*) FROM CROSSFIT\n-\tauto str_plan =\n-\t    \"\\\\x12\\\\x09\\\\x1A\\\\x07\\\\x10\\\\x01\\\\x1A\\\\x03lte\\\\x12\\\\x11\\\\x1A\\\\x0F\\\\x10\\\\x02\\\\x1A\\\\x0Bis_not_\"\n-\t    \"null\\\\x12\\\\x09\\\\x1A\\\\x07\\\\x10\\\\x03\\\\x1A\\\\x03and\\\\x12\\\\x0B\\\\x1A\\\\x09\\\\x10\\\\x04\\\\x1A\\\\x05count\\\\x1A\\\\xC7\\\\x01\\\\x\"\n-\t    \"12\\\\xC4\\\\x01\\\\x0A\\\\xB7\\\\x01:\\\\xB4\\\\x01\\\\x12\\\\xA7\\\\x01\\\\x22\\\\xA4\\\\x01\\\\x12\\\\x93\\\\x01\\\\x0A\\\\x90\\\\x01\\\\x12.\"\n-\t    \"\\\\x0A\\\\x08exercise\\\\x0A\\\\x0Fdificulty_level\\\\x12\\\\x11\\\\x0A\\\\x07\\\\xB2\\\\x01\\\\x04\\\\x08\\\\x0D\\\\x18\\\\x01\\\\x0A\\\\x04*\"\n-\t    \"\\\\x02\\\\x10\\\\x01\\\\x18\\\\x02\\\\x1AJ\\\\x1AH\\\\x08\\\\x03\\\\x1A\\\\x04\\\\x0A\\\\x02\\\\x10\\\\x01\\\\x22\\\\x22\\\\x1A \"\n-\t    \"\\\\x1A\\\\x1E\\\\x08\\\\x01\\\\x1A\\\\x04*\"\n-\t    \"\\\\x02\\\\x10\\\\x01\\\\x22\\\\x0C\\\\x1A\\\\x0A\\\\x12\\\\x08\\\\x0A\\\\x04\\\\x12\\\\x02\\\\x08\\\\x01\\\\x22\\\\x00\\\\x22\\\\x06\\\\x1A\\\\x04\\\\x0A\"\n-\t    \"\\\\x02(\\\\x05\\\\x22\\\\x1A\\\\x1A\\\\x18\\\\x1A\\\\x16\\\\x08\\\\x02\\\\x1A\\\\x04*\"\n-\t    \"\\\\x02\\\\x10\\\\x01\\\\x22\\\\x0C\\\\x1A\\\\x0A\\\\x12\\\\x08\\\\x0A\\\\x04\\\\x12\\\\x02\\\\x08\\\\x01\\\\x22\\\\x00\\\\x22\\\\x06\\\\x0A\\\\x02\\\\x0A\"\n-\t    \"\\\\x00\\\\x10\\\\x01:\\\\x0A\\\\x0A\\\\x08crossfit\\\\x1A\\\\x00\\\\x22\\\\x0A\\\\x0A\\\\x08\\\\x08\\\\x04*\\\\x04:\"\n-\t    \"\\\\x02\\\\x10\\\\x01\\\\x1A\\\\x08\\\\x12\\\\x06\\\\x0A\\\\x02\\\\x12\\\\x00\\\\x22\\\\x00\\\\x12\\\\x08exercise2\\\\x0A\\\\x10\\\\x18*\"\n-\t    \"\\\\x06DuckDB\";\n-\tauto plan = reinterpret_cast<const uint8_t *>(str_plan);\n-\tsize_t length = strlen(str_plan);\n-\tREQUIRE(SUCCESS(AdbcStatementRelease(&adbc_statement, &adbc_error)));\n-\n-\tREQUIRE(SUCCESS(AdbcStatementNew(&adbc_connection, &adbc_statement, &adbc_error)));\n-\tREQUIRE(SUCCESS(AdbcStatementSetSubstraitPlan(&adbc_statement, plan, length, &adbc_error)));\n-\tint64_t rows_affected;\n-\tREQUIRE(SUCCESS(AdbcStatementExecuteQuery(&adbc_statement, &arrow_stream, &rows_affected, &adbc_error)));\n-\tarrow_stream.get_next(&arrow_stream, &arrow_array);\n-\tREQUIRE(((reinterpret_cast<const int64_t *>(arrow_array.children[0]->buffers[1])[0] == 1)));\n-\tarrow_array.release(&arrow_array);\n-\tarrow_stream.release(&arrow_stream);\n-\n-\t// Try some errors\n-\tREQUIRE(!SUCCESS(AdbcStatementSetSubstraitPlan(&adbc_statement, nullptr, length, &adbc_error)));\n-\tREQUIRE((std::strcmp(adbc_error.message, \"Substrait Plan is not set\") == 0));\n-\tadbc_error.release(&adbc_error);\n-\n-\tREQUIRE(!SUCCESS(AdbcStatementSetSubstraitPlan(&adbc_statement, plan, 0, &adbc_error)));\n-\tREQUIRE((std::strcmp(adbc_error.message, \"Can't execute plan with size = 0\") == 0));\n-\tadbc_error.release(&adbc_error);\n-\n-\t// Broken Plan\n-\tREQUIRE(!SUCCESS(AdbcStatementSetSubstraitPlan(&adbc_statement, plan, 5, &adbc_error)));\n-\tREQUIRE(StringUtil::Contains(adbc_error.message, \"unterminated escape code at end of blob\"));\n-\n-\tREQUIRE(SUCCESS(AdbcStatementRelease(&adbc_statement, &adbc_error)));\n-\tREQUIRE(SUCCESS(AdbcConnectionRelease(&adbc_connection, &adbc_error)));\n-\tREQUIRE(SUCCESS(AdbcDatabaseRelease(&adbc_database, &adbc_error)));\n-\tadbc_error.release(&adbc_error);\n-}\n-\n TEST_CASE(\"Test ADBC Prepared Statement - Prepare nop\", \"[adbc]\") {\n \tif (!duckdb_lib) {\n \t\treturn;\ndiff --git a/test/api/capi/test_capi_data_chunk.cpp b/test/api/capi/test_capi_data_chunk.cpp\nindex 79012f375c39..fb8a6692374e 100644\n--- a/test/api/capi/test_capi_data_chunk.cpp\n+++ b/test/api/capi/test_capi_data_chunk.cpp\n@@ -513,12 +513,13 @@ TEST_CASE(\"Test DataChunk write BLOB\", \"[capi]\") {\n \tduckdb_destroy_logical_type(&column_type);\n \tuint8_t bytes[] = {0x80, 0x00, 0x01, 0x2a};\n \tduckdb_vector_assign_string_element_len(vector, 0, (const char *)bytes, 4);\n-\tauto string_data = (duckdb_string_t *)duckdb_vector_get_data(vector);\n-\tREQUIRE(string_data[0].value.inlined.length == 4);\n-\tREQUIRE(string_data[0].value.inlined.inlined[0] == (char)0x80);\n-\tREQUIRE(string_data[0].value.inlined.inlined[1] == (char)0x00);\n-\tREQUIRE(string_data[0].value.inlined.inlined[2] == (char)0x01);\n-\tREQUIRE(string_data[0].value.inlined.inlined[3] == (char)0x2a);\n+\tauto string_data = static_cast<duckdb_string_t *>(duckdb_vector_get_data(vector));\n+\tauto string_value = duckdb_string_t_data(string_data);\n+\tREQUIRE(duckdb_string_t_length(*string_data) == 4);\n+\tREQUIRE(string_value[0] == (char)0x80);\n+\tREQUIRE(string_value[1] == (char)0x00);\n+\tREQUIRE(string_value[2] == (char)0x01);\n+\tREQUIRE(string_value[3] == (char)0x2a);\n \tduckdb_destroy_data_chunk(&chunk);\n \tduckdb_destroy_logical_type(&type);\n }\n@@ -536,12 +537,13 @@ TEST_CASE(\"Test DataChunk write VARINT\", \"[capi]\") {\n \tduckdb_destroy_logical_type(&column_type);\n \tuint8_t bytes[] = {0x80, 0x00, 0x01, 0x2a}; // VARINT 42\n \tduckdb_vector_assign_string_element_len(vector, 0, (const char *)bytes, 4);\n-\tauto string_data = (duckdb_string_t *)duckdb_vector_get_data(vector);\n-\tREQUIRE(string_data[0].value.inlined.length == 4);\n-\tREQUIRE(string_data[0].value.inlined.inlined[0] == (char)0x80);\n-\tREQUIRE(string_data[0].value.inlined.inlined[1] == (char)0x00);\n-\tREQUIRE(string_data[0].value.inlined.inlined[2] == (char)0x01);\n-\tREQUIRE(string_data[0].value.inlined.inlined[3] == (char)0x2a);\n+\tauto string_data = static_cast<duckdb_string_t *>(duckdb_vector_get_data(vector));\n+\tauto string_value = duckdb_string_t_data(string_data);\n+\tREQUIRE(duckdb_string_t_length(*string_data) == 4);\n+\tREQUIRE(string_value[0] == (char)0x80);\n+\tREQUIRE(string_value[1] == (char)0x00);\n+\tREQUIRE(string_value[2] == (char)0x01);\n+\tREQUIRE(string_value[3] == (char)0x2a);\n \tduckdb_destroy_data_chunk(&chunk);\n \tduckdb_destroy_logical_type(&type);\n }\ndiff --git a/test/extension/update_extensions_ci.test b/test/extension/update_extensions_ci.test\nindex a8aceb8dae98..3171f0fe2be4 100644\n--- a/test/extension/update_extensions_ci.test\n+++ b/test/extension/update_extensions_ci.test\n@@ -202,13 +202,13 @@ statement error\n FORCE INSTALL '${DIRECT_INSTALL_DIR}/json_incorrect_version.duckdb_extension';\n ----\n Failed to install './build/extension_metadata_test_data/direct_install/json_incorrect_version.duckdb_extension'\n-The file was built for DuckDB version 'v1337', but we can only load extensions built for DuckDB version\n+The file was built specifically for DuckDB version 'v1337' and can only be loaded with that version of DuckDB. (this version of DuckDB is\n \n statement error\n FORCE INSTALL json_incorrect_version FROM '${LOCAL_EXTENSION_REPO_INCORRECT_DUCKDB_VERSION}';\n ----\n Failed to install 'json_incorrect_version'\n-The file was built for DuckDB version 'v1337', but we can only load extensions built for DuckDB version\n+The file was built specifically for DuckDB version 'v1337' and can only be loaded with that version of DuckDB. (this version of DuckDB is\n \n # These should print both errors\n statement error\n@@ -235,7 +235,7 @@ The file was built for the platform 'test_platform', but we can only load extens\n statement error\n LOAD '${DIRECT_INSTALL_DIR}/json_incorrect_version.duckdb_extension';\n ----\n-The file was built for DuckDB version 'v1337', but we can only load extensions built for DuckDB version\n+The file was built specifically for DuckDB version 'v1337' and can only be loaded with that version of DuckDB. (this version of DuckDB is\n \n # Note that this is the json extension with incorrect platform and version\n statement error\ndiff --git a/test/sql/aggregate/external/simple_external_aggregate.test_slow b/test/sql/aggregate/external/simple_external_aggregate.test_slow\nindex c0525f0b0fa2..317974e3f044 100644\n--- a/test/sql/aggregate/external/simple_external_aggregate.test_slow\n+++ b/test/sql/aggregate/external/simple_external_aggregate.test_slow\n@@ -27,7 +27,7 @@ statement ok\n set disabled_optimizers to 'compressed_materialization'\n \n statement ok\n-pragma memory_limit='600MB'\n+pragma memory_limit='800MB'\n \n query I\n select count(*) from (select distinct * from t1)\ndiff --git a/test/sql/catalog/function/query_function.test b/test/sql/catalog/function/query_function.test\nindex 0d65fa65b4ac..1ef00218e192 100644\n--- a/test/sql/catalog/function/query_function.test\n+++ b/test/sql/catalog/function/query_function.test\n@@ -249,6 +249,16 @@ FROM query_table('SELECT 4 + 2');\n ----\n Catalog Error: Table with name SELECT 4 + 2 does not exist!\n \n+query I\n+SELECT f.* FROM query_table('tbl_int') as f;\n+----\n+42\n+\n+query I\n+SELECT f.x FROM query_table('tbl_int') as f(x);\n+----\n+42\n+\n # test by_name argument\n query I\n FROM query_table(['tbl_int', 'tbl_varchar', 'tbl_empty', 'tbl2_varchar'], false);\ndiff --git a/test/sql/catalog/test_extension_suggestion.test b/test/sql/catalog/test_extension_suggestion.test\nindex 94f2307e15d0..9d88900f2680 100644\n--- a/test/sql/catalog/test_extension_suggestion.test\n+++ b/test/sql/catalog/test_extension_suggestion.test\n@@ -7,6 +7,6 @@ require skip_reload\n require no_extension_autoloading \"EXPECTED: This tests what happens when extension is not there\"\n \n statement error\n-SELECT get_substrait(\"select 1\");\n+SELECT from_json('data/json/array_of_empty_arrays.json');\n ----\n-Catalog Error: Scalar Function with name \"get_substrait\" is not in the catalog, a function by this name exists in the substrait extension, but it's of a different type, namely Table Function\n+Catalog Error: Scalar Function with name \"from_json\" is not in the catalog, but it exists in the json extension.\n\\ No newline at end of file\ndiff --git a/test/sql/copy/csv/14512.test b/test/sql/copy/csv/14512.test\nindex c970e05a47ca..c85603500d48 100644\n--- a/test/sql/copy/csv/14512.test\n+++ b/test/sql/copy/csv/14512.test\n@@ -6,7 +6,7 @@ statement ok\n PRAGMA enable_verification\n \n query II\n-FROM 'data/csv/14512.csv';\n+FROM read_csv('data/csv/14512.csv', RFC_4180=TRUE);\n ----\n onions \t,\n \ndiff --git a/test/sql/copy/csv/afl/test_fuzz_3977.test b/test/sql/copy/csv/afl/test_fuzz_3977.test\nnew file mode 100644\nindex 000000000000..16620e82a637\n--- /dev/null\n+++ b/test/sql/copy/csv/afl/test_fuzz_3977.test\n@@ -0,0 +1,363 @@\n+# name: test/sql/copy/csv/afl/test_fuzz_3977.test\n+# description: fuzzer generated csv files - should not raise internal exception (by failed assertion).\n+# group: [afl]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query I\n+select count(file) from glob('./data/csv/afl/3977/*');\n+----\n+88\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_1.csv', rejects_scan=0, buffer_size=655371, all_varchar=false, rejects_scan=0, buffer_size=42);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_2.csv', names=['a','b','c','d'], store_rejects=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_3.csv', names=['a','b','c','d'], store_rejects=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_4.csv', names=['a','b','c','d'], store_rejects=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_5.csv', auto_detect=false, columns={'a': 'VARCHAR'}, escape='\"', header=false, quote='\"', rfc_4180=true, store_rejects=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_6.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_7.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_8.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_9.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=false);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_10.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_11.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_12.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_13.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_14.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_15.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_16.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_17.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_18.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_19.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_20.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_21.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_22.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_23.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_24.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_25.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_26.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_27.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_28.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_29.csv', auto_detect=false, buffer_size=65536, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_30.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_31.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_32.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_33.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_34.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_35.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_36.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_37.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_38.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_39.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_40.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, comment=';', rejects_table='\"', rfc_4180=false);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_41.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_42.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_43.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_44.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_45.csv', auto_detect=false, buffer_size=810, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_46.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_47.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_48.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_49.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_50.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','\u0010':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_51.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', '|':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_52.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_53.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_54.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAr'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_55.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_56.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_57.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_58.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_59.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','\"':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_60.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_61.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_62.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_63.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_64.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_65.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_66.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_67.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_68.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_69.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_70.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_71.csv', auto_detect=false, buffer_size=16711722, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_72.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','F':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_73.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_74.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_75.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_76.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_77.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_78.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_79.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_80.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_81.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_82.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_83.csv', auto_detect=false, parallel=false, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_84.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_85.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_86.csv', auto_detect=false, buffer_size=720938, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_87.csv', auto_detect=false, buffer_size=42, columns={'a2.0-22222222222222222.0222->>':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3977/case_88.csv', auto_detect=false, buffer_size=42, columns={'a':'INTEGER','b':'INTEGER', 'c':'VARCHAR'}, delim=';', rejects_table='\"', rfc_4180=true);\n+----\ndiff --git a/test/sql/copy/csv/afl/test_fuzz_3981.test_slow b/test/sql/copy/csv/afl/test_fuzz_3981.test_slow\nnew file mode 100644\nindex 000000000000..19ad0bbcd0fe\n--- /dev/null\n+++ b/test/sql/copy/csv/afl/test_fuzz_3981.test_slow\n@@ -0,0 +1,40 @@\n+# name: test/sql/copy/csv/afl/test_fuzz_3981.test_slow\n+# description: fuzzer generated csv files - should not raise internal exception (by failed assertion).\n+# group: [afl]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query I\n+select count(file) from glob('data/csv/afl/3981/*');\n+----\n+7\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3981/case_0.csv', compression='gzip');\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3981/case_1.csv', compression='gzip');\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3981/case_2.csv', compression='gzip');\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3981/case_3.csv', compression='gzip');\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3981/case_4.csv', compression='gzip');\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3981/case_5.csv', compression='gzip');\n+----\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/3981/case_6.csv', compression='gzip');\n+----\n+\ndiff --git a/test/sql/copy/csv/auto/test_csv_auto.test b/test/sql/copy/csv/auto/test_csv_auto.test\nindex 54ad76526a46..9d689d0dd316 100644\n--- a/test/sql/copy/csv/auto/test_csv_auto.test\n+++ b/test/sql/copy/csv/auto/test_csv_auto.test\n@@ -19,7 +19,8 @@ FROM read_csv('data/csv/repromarket.csv',\n    header=false,\n    skip=0,\n    null_padding=true,\n-   ignore_errors=true\n+   ignore_errors=true,\n+   RFC_4180=true\n );\n ----\n nemanja.krpovic@gmail.com\tkrlleta\n@@ -53,7 +54,8 @@ select columns FROM sniff_csv('data/csv/auto/mock_duckdb_test_data.csv', ignore_\n [{'name': id, 'type': BIGINT}, {'name': name, 'type': VARCHAR}, {'name': age, 'type': BIGINT}, {'name': sex, 'type': VARCHAR}, {'name': state, 'type': VARCHAR}]\n \n query IIIII\n-FROM read_csv('data/csv/auto/mock_duckdb_test_data.csv', ignore_errors = true)\n+FROM read_csv('data/csv/auto/mock_duckdb_test_data.csv', ignore_errors = true,\n+   RFC_4180=true)\n ----\n 1\tJames\t30\tM\tAL\n 2\tJill\t32\tF\tCO\n@@ -64,7 +66,8 @@ FROM read_csv('data/csv/auto/mock_duckdb_test_data.csv', ignore_errors = true)\n 9\tTitus\t38\tM\tWY\n \n statement error\n-select * from read_csv_auto('data/csv/dates.csv', auto_detect=false, delim=',', quote='\"', columns={'a': 'VARCHAR'})\n+select * from read_csv_auto('data/csv/dates.csv', auto_detect=false, delim=',', quote='\"', columns={'a': 'VARCHAR'},\n+   RFC_4180=true)\n ----\n Expected Number of Columns: 1 Found: 2\n \ndiff --git a/test/sql/copy/csv/csv_line_too_long.test b/test/sql/copy/csv/csv_line_too_long.test\nindex 6563b660e6b9..e1a91d760da8 100644\n--- a/test/sql/copy/csv/csv_line_too_long.test\n+++ b/test/sql/copy/csv/csv_line_too_long.test\n@@ -8,17 +8,21 @@ PRAGMA enable_verification\n statement ok\n CREATE TABLE T1 (name VARCHAR);\n \n-\n-foreach path data/csv/line_too_long.csv.gz data/csv/line_too_long_with_newline.csv.gz data/csv/multiple_line_too_long.csv.gz\n-\n foreach header true false\n \n-\n statement error\n-COPY T1(name) from '${path}' (DELIMITER ',', HEADER ${header} , COMPRESSION gzip, ALLOW_QUOTED_NULLS false);\n+COPY T1(name) from 'data/csv/line_too_long.csv.gz' (DELIMITER ',', HEADER ${header} , COMPRESSION gzip, ALLOW_QUOTED_NULLS false);\n ----\n Maximum line size of 2000000 bytes exceeded\n \n-endloop\n+statement error\n+COPY T1(name) from 'data/csv/line_too_long_with_newline.csv.gz' (DELIMITER ',', HEADER ${header} , COMPRESSION gzip, ALLOW_QUOTED_NULLS false);\n+----\n+Be sure that the maximum line size is set to an appropriate value\n+\n+statement error\n+COPY T1(name) from 'data/csv/multiple_line_too_long.csv.gz' (DELIMITER ',', HEADER ${header} , COMPRESSION gzip, ALLOW_QUOTED_NULLS false);\n+----\n+Be sure that the maximum line size is set to an appropriate value\n \n endloop\ndiff --git a/test/sql/copy/csv/pollock/test_field_delimiter.test b/test/sql/copy/csv/pollock/test_field_delimiter.test\nnew file mode 100644\nindex 000000000000..b2b4304245f2\n--- /dev/null\n+++ b/test/sql/copy/csv/pollock/test_field_delimiter.test\n@@ -0,0 +1,11 @@\n+# name: test/sql/copy/csv/pollock/test_field_delimiter.test\n+# description: Test field delimiter from Pollock\n+# group: [pollock]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+FROM read_csv('data/csv/pollock/file_field_delimiter_0x20.csv', delim = ' ', escape = '\"', quote='\"', header = false, skip=1,\n+columns = {'Date':'VARCHAR','TIME':'VARCHAR','Qty':'VARCHAR','PRODUCTID':'VARCHAR','Price':'VARCHAR'\n+,'ProductType':'VARCHAR','ProductDescription':'VARCHAR','URL':'VARCHAR','Comments':'VARCHAR'}, auto_detect = false, RFC_4180=FALSE, null_padding = true)\ndiff --git a/test/sql/copy/csv/pollock/test_quotation_char.test b/test/sql/copy/csv/pollock/test_quotation_char.test\nnew file mode 100644\nindex 000000000000..1d259a2c6459\n--- /dev/null\n+++ b/test/sql/copy/csv/pollock/test_quotation_char.test\n@@ -0,0 +1,11 @@\n+# name: test/sql/copy/csv/pollock/test_quotation_char.test\n+# description: Test quotation from Pollock\n+# group: [pollock]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+FROM read_csv('data/csv/pollock/file_quotation_char_0x27.csv', delim = ',', escape = '\"', quote='''',\n+columns = {'Date':'VARCHAR','TIME':'VARCHAR','Qty':'VARCHAR','PRODUCTID':'VARCHAR','Price':'VARCHAR'\n+,'ProductType':'VARCHAR','ProductDescription':'VARCHAR','URL':'VARCHAR','Comments':'VARCHAR'}, auto_detect = false, RFC_4180=FALSE, null_padding = true)\ndiff --git a/test/sql/copy/csv/rejects/csv_incorrect_columns_amount_rejects.test b/test/sql/copy/csv/rejects/csv_incorrect_columns_amount_rejects.test\nindex 6fc829c263b9..ff418f8f8eb1 100644\n--- a/test/sql/copy/csv/rejects/csv_incorrect_columns_amount_rejects.test\n+++ b/test/sql/copy/csv/rejects/csv_incorrect_columns_amount_rejects.test\n@@ -36,7 +36,7 @@ statement ok\n SELECT * FROM read_csv(\n     'data/csv/rejects/incorrect_columns/many_columns.csv',\n     columns = {'a': 'INTEGER', 'b': 'INTEGER', 'c': 'INTEGER', 'd': 'INTEGER'},\n-    store_rejects=true, auto_detect=false, header = 1);\n+    store_rejects=true, auto_detect=false, header = 1, RFC_4180=True);\n \n query IIIIIIIII rowsort\n SELECT * EXCLUDE (scan_id) FROM reject_errors order by all;\n@@ -58,7 +58,7 @@ statement ok\n SELECT * FROM read_csv(\n     'data/csv/rejects/incorrect_columns/mix_columns.csv',\n     columns = {'a': 'INTEGER', 'b': 'INTEGER', 'c': 'INTEGER', 'd': 'INTEGER'},\n-    store_rejects=true, auto_detect=false, header = 1);\n+    store_rejects=true, auto_detect=false, header = 1, RFC_4180=True);\n \n query IIIIIIIII rowsort\n SELECT * EXCLUDE (scan_id) FROM reject_errors order by all;\n@@ -87,7 +87,7 @@ statement ok\n SELECT * FROM read_csv(\n     'data/csv/rejects/incorrect_columns/small_mix.csv',\n     columns = {'a': 'INTEGER', 'b': 'INTEGER', 'c': 'INTEGER', 'd': 'INTEGER'},\n-    store_rejects=true, auto_detect=false, header = 1);\n+    store_rejects=true, auto_detect=false, header = 1, RFC_4180=True);\n \n query IIIIIIIII rowsort\n SELECT * EXCLUDE (scan_id) FROM reject_errors order by all\n@@ -108,7 +108,7 @@ statement ok\n SELECT * FROM read_csv(\n     'data/csv/rejects/incorrect_columns/*.csv',\n     columns = {'a': 'INTEGER', 'b': 'INTEGER', 'c': 'INTEGER', 'd': 'INTEGER'},\n-   store_rejects=true, auto_detect=false, header = 1);\n+   store_rejects=true, auto_detect=false, header = 1, RFC_4180=True);\n \n query IIIIIIIII rowsort\n SELECT * EXCLUDE (scan_id) FROM reject_errors order by all\ndiff --git a/test/sql/copy/csv/rejects/csv_rejects_read.test b/test/sql/copy/csv/rejects/csv_rejects_read.test\nindex 2a33a2e779b4..bc0d8dda524c 100644\n--- a/test/sql/copy/csv/rejects/csv_rejects_read.test\n+++ b/test/sql/copy/csv/rejects/csv_rejects_read.test\n@@ -271,7 +271,7 @@ statement ok\n DROP TABLE reject_scans;\n \n query II\n-FROM read_csv('data/csv/error.csv', store_rejects=1);\n+FROM read_csv('data/csv/error.csv', store_rejects=1, RFC_4180=True);\n ----\n true\tfalse\n \ndiff --git a/test/sql/copy/csv/rejects/test_multiple_errors_same_line.test b/test/sql/copy/csv/rejects/test_multiple_errors_same_line.test\nindex e89a3bf13f97..fa63fffe9124 100644\n--- a/test/sql/copy/csv/rejects/test_multiple_errors_same_line.test\n+++ b/test/sql/copy/csv/rejects/test_multiple_errors_same_line.test\n@@ -10,7 +10,7 @@ require notwindows\n query IIII\n FROM read_csv('data/csv/rejects/multiple_errors/cast_and_more_col.csv',\n     columns = {'name': 'VARCHAR', 'age': 'INTEGER', 'current_day': 'DATE', 'barks': 'INTEGER'},\n-    store_rejects = true, auto_detect=false, header = 1);\n+    store_rejects = true, auto_detect=false, header = 1, RFC_4180=True);\n ----\n oogie boogie\t3\t2023-01-01\t2\n oogie boogie\t3\t2023-01-02\t5\n@@ -33,7 +33,7 @@ DROP TABLE reject_scans;\n query IIII\n FROM read_csv('data/csv/rejects/multiple_errors/multiple_cast_implicit.csv',\n     columns = {'name': 'VARCHAR', 'age': 'INTEGER', 'current_day': 'DATE', 'barks': 'INTEGER'},\n-    store_rejects = true, auto_detect=false, header = 1);\n+    store_rejects = true, auto_detect=false, header = 1, RFC_4180=True);\n ----\n oogie boogie\t3\t2023-01-01\t2\n oogie boogie\t3\t2023-01-02\t5\n@@ -158,7 +158,7 @@ DROP TABLE reject_scans;\n query IIII\n FROM read_csv('data/csv/rejects/multiple_errors/more_col_and_max_line.csv',\n     columns = {'name': 'VARCHAR', 'age': 'INTEGER', 'current_day': 'DATE', 'barks': 'INTEGER'},\n-    store_rejects = true, auto_detect=false, header = 1, max_line_size=40);\n+    store_rejects = true, auto_detect=false, header = 1, max_line_size=40, RFC_4180=True);\n ----\n oogie boogie\t3\t2023-01-01\t2\n oogie boogie\t3\t2023-01-02\t5\n@@ -300,7 +300,7 @@ DROP TABLE reject_scans;\n query IIII\n FROM read_csv('data/csv/rejects/multiple_errors/invalid_utf_max_line.csv',\n     columns = {'name': 'VARCHAR', 'age': 'INTEGER', 'current_day': 'DATE', 'barks': 'INTEGER'},\n-    store_rejects = true, auto_detect=false, header = 1, max_line_size=40);\n+    store_rejects = true, auto_detect=false, header = 1, max_line_size=40, RFC_4180=True);\n ----\n oogie boogie\t3\t2023-01-01\t2\n \n@@ -321,7 +321,7 @@ DROP TABLE reject_scans;\n query IIII\n FROM read_csv('data/csv/rejects/multiple_errors/invalid_utf_more.csv',\n     columns = {'name': 'VARCHAR', 'age': 'INTEGER', 'current_day': 'DATE', 'barks': 'INTEGER'},\n-    store_rejects = true, auto_detect=false, header = 1, max_line_size=40);\n+    store_rejects = true, auto_detect=false, header = 1, max_line_size=40, RFC_4180=True);\n ----\n oogie boogie\t3\t2023-01-01\t2\n \ndiff --git a/test/sql/copy/csv/relaxed_quotes.test b/test/sql/copy/csv/relaxed_quotes.test\nindex caaea768486b..ef3cd5bae806 100644\n--- a/test/sql/copy/csv/relaxed_quotes.test\n+++ b/test/sql/copy/csv/relaxed_quotes.test\n@@ -28,8 +28,8 @@ de:08115:4574:0:2\tWeil der Stadt Stadtgarten\t48.7523774577301\t8.869444093766\n de:08115:4575:0:1\tWeil der Stadt Merklinger Stra\u00dfe\t48.7554808692434\t8.86798882300553\n de:08115:4575:0:2\tWeil der Stadt Merklinger Stra\u00dfe\t48.7554808692434\t8.86798882300553\n de:08115:4577:0:1\tM\u00fcnklingen Neuhauser Str.\t48.7769389105929\t8.81035291436839\n-de:08317:12007:2:1\tLahr Schl\u00fcssel Vis-\u00e0-Vis Bus more text\t48.3411985847104\t7.87932997062448\n-de:08317:12007:2:1\tLahr Schl\u00fcssel Vis-\u00e0-Vis Bus\t48.3411985847104\t7.87932997062448\n+de:08317:12007:2:1\tLahr Schl\u00fcssel \"Vis-\u00e0-Vis Bus\" more text\t48.3411985847104\t7.87932997062448\n+de:08317:12007:2:1\tLahr Schl\u00fcssel \"Vis-\u00e0-Vis Bus\"\t48.3411985847104\t7.87932997062448\n \n query II\n from read_csv('data/csv/unescaped_quotes/unescaped_quote.csv', escape = '\"', rfc_4180=false);\ndiff --git a/test/sql/copy/csv/test_comment_midline.test b/test/sql/copy/csv/test_comment_midline.test\nindex 55cc29b35997..6d1ef940ef51 100644\n--- a/test/sql/copy/csv/test_comment_midline.test\n+++ b/test/sql/copy/csv/test_comment_midline.test\n@@ -102,7 +102,7 @@ FROM 'data/csv/comments/midline_empty_space.csv';\n 6\t7\n \n query II\n-FROM read_csv('data/csv/comments/mid_line_invalid.csv', ignore_errors = true, delim = ';', comment = '#', auto_detect = false, columns= {'a':'integer', 'b':'integer'});\n+FROM read_csv('data/csv/comments/mid_line_invalid.csv', ignore_errors = true, delim = ';', comment = '#', auto_detect = false, columns= {'a':'integer', 'b':'integer'}, RFC_4180=True);\n ----\n 1\t3\n 6\t7\ndiff --git a/test/sql/copy/csv/test_extra_delimiters_rfc.test b/test/sql/copy/csv/test_extra_delimiters_rfc.test\nnew file mode 100644\nindex 000000000000..eab1009f2175\n--- /dev/null\n+++ b/test/sql/copy/csv/test_extra_delimiters_rfc.test\n@@ -0,0 +1,14 @@\n+# name: test/sql/copy/csv/test_extra_delimiters_rfc.test\n+# description: Test Export function that is not null\n+# group: [csv]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query III\n+FROM read_csv('data/csv/extra_delimiters.csv', RFC_4180 = false, columns={'a':'VARCHAR','b':'VARCHAR','c':'VARCHAR'}, auto_detect = false, delim = ',', header = true)\n+----\n+1\t2\t3\n+1\t2\t3\n+1\t2\t3\n+1\t2\t3\n\\ No newline at end of file\ndiff --git a/test/sql/copy/csv/test_ignore_errors.test b/test/sql/copy/csv/test_ignore_errors.test\nindex a6c36c9e38f0..93c673f4a1ab 100644\n--- a/test/sql/copy/csv/test_ignore_errors.test\n+++ b/test/sql/copy/csv/test_ignore_errors.test\n@@ -68,7 +68,7 @@ statement ok\n DELETE FROM integers;\n \n statement ok\n-COPY integers FROM 'data/csv/test/error_too_many.csv' (HEADER, IGNORE_ERRORS, SAMPLE_SIZE -1)\n+COPY integers FROM 'data/csv/test/error_too_many.csv' (HEADER, IGNORE_ERRORS, SAMPLE_SIZE -1, RFC_4180 TRUE)\n \n statement error\n COPY integers FROM 'data/csv/test/error_too_many.csv' (HEADER)\ndiff --git a/test/sql/copy/csv/test_ignore_mid_null_line.test b/test/sql/copy/csv/test_ignore_mid_null_line.test\nindex 7ed5a1cbd0fe..6278dcc0bc9f 100644\n--- a/test/sql/copy/csv/test_ignore_mid_null_line.test\n+++ b/test/sql/copy/csv/test_ignore_mid_null_line.test\n@@ -7,7 +7,7 @@ PRAGMA enable_verification\n \n query III\n FROM read_csv('data/csv/error/mid_null.csv', delim = ';',\n-     columns = {'a':'integer','b':'integer','c':'integer'}, auto_detect = false, header = true, ignore_errors = true)\n+     columns = {'a':'integer','b':'integer','c':'integer'}, auto_detect = false, header = true, ignore_errors = true, RFC_4180=True)\n ----\n 1\t2\t3\n 1\t2\t3\n\\ No newline at end of file\ndiff --git a/test/sql/copy/csv/test_null_padding_projection.test b/test/sql/copy/csv/test_null_padding_projection.test\nindex 33ed335cf78e..3f1fd020162d 100644\n--- a/test/sql/copy/csv/test_null_padding_projection.test\n+++ b/test/sql/copy/csv/test_null_padding_projection.test\n@@ -8,14 +8,14 @@ PRAGMA enable_verification\n # Test simple null_padding, this will fail because we have a row with more columns that defined\n statement error\n from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n-'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect = false)\n+'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect = false, rfc_4180=True)\n ----\n Expected Number of Columns: 4 Found: 5\n \n # Create a view\n statement ok\n CREATE VIEW np AS  from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n-'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect = false, ignore_errors = true);\n+'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect = false, ignore_errors = true, rfc_4180=True);\n \n # With ignore errors this should work, with last row being ignored\n query IIII\n@@ -46,7 +46,7 @@ NULL\tNULL\n # Now let's try with options that give a const value\n query IIIII\n from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n-'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect => false, ignore_errors => true, filename => true);\n+'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect => false, ignore_errors => true, filename => true, rfc_4180=True);\n ----\n 10\t100\t1000\tNULL\tdata/csv/nullpadding.csv\n 10\t100\t1000\t10000\tdata/csv/nullpadding.csv\n@@ -55,7 +55,7 @@ from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n \n query II\n select a, filename from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n-'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect => false, ignore_errors => true, filename => true);\n+'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect => false, ignore_errors => true, filename => true, rfc_4180=True);\n ----\n 10\tdata/csv/nullpadding.csv\n 10\tdata/csv/nullpadding.csv\n@@ -64,7 +64,7 @@ select a, filename from read_csv('data/csv/nullpadding.csv',null_padding=true, c\n \n query I\n select filename from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n-'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect => false, ignore_errors => true, filename => true);\n+'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect => false, ignore_errors => true, filename => true, rfc_4180=True);\n ----\n data/csv/nullpadding.csv\n data/csv/nullpadding.csv\n@@ -73,7 +73,7 @@ data/csv/nullpadding.csv\n \n query I\n select a from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n-'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect = false, ignore_errors => true, filename => true);\n+'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect = false, ignore_errors => true, filename => true, rfc_4180=True);\n ----\n 10\n 10\n@@ -84,7 +84,7 @@ select a from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n \n query IIII\n select * from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n-'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect := false, ignore_errors := true)\n+'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect := false, ignore_errors := true, rfc_4180=True)\n where  b = 100;\n ----\n 10\t100\t1000\tNULL\n@@ -94,7 +94,7 @@ where  b = 100;\n \n query IIIII\n select * from read_csv('data/csv/nullpadding.csv',null_padding=true, columns={\n-'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect = false, ignore_errors = true, filename = true)\n+'a': 'INTEGER','b': 'INTEGER','c': 'INTEGER','d': 'INTEGER'}, auto_detect = false, ignore_errors = true, filename = true, rfc_4180=True)\n where a = 10 and d = 10000;\n ----\n 10\t100\t1000\t10000\tdata/csv/nullpadding.csv\ndiff --git a/test/sql/copy/csv/test_skip.test_slow b/test/sql/copy/csv/test_skip.test_slow\nindex b0fe6dd30cba..e6f71c6d7b4e 100644\n--- a/test/sql/copy/csv/test_skip.test_slow\n+++ b/test/sql/copy/csv/test_skip.test_slow\n@@ -5,8 +5,6 @@\n statement ok\n PRAGMA enable_verification\n \n-mode skip\n-\n statement ok\n copy (from range(10000)) to '__TEST_DIR__/skip.csv' (HEADER 0);\n \n@@ -67,10 +65,11 @@ SELECT * EXCLUDE (prompt) from sniff_csv('__TEST_DIR__/skip.csv',skip=3000)\n ----\n ,\t\\0\t\\0\t\\n\t\\0\t3000\t0\t[{'name': column0, 'type': BIGINT}]\tNULL\tNULL\tskip=3000\n \n+# If we don't encounter a value, we use skip\n query IIIIIIIIIII\n SELECT * EXCLUDE (prompt) from sniff_csv('__TEST_DIR__/skip.csv',skip=11000)\n ----\n-,\t\\0\t\\0\t\\n\t\\0\t11000\t0\t[{'name': column0, 'type': BIGINT}]\tNULL\tNULL\tskip=11000\n+,\t\\0\t\\0\t\\n\t\\0\t11000\t0\t[{'name': column0, 'type': VARCHAR}]\tNULL\tNULL\tskip=11000\n \n # Test with different buffer sizes\n \ndiff --git a/test/sql/json/issues/internal_issue4014.test b/test/sql/json/issues/internal_issue4014.test\nnew file mode 100644\nindex 000000000000..5e14739dc015\n--- /dev/null\n+++ b/test/sql/json/issues/internal_issue4014.test\n@@ -0,0 +1,8 @@\n+# name: test/sql/json/issues/internal_issue4014.test\n+# description: Test internal issue 4014 - AFL++ issue: segfault in json reader\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+FROM read_json('data/json/internal_4014.json', map_inference_threshold=0);\ndiff --git a/test/sql/parser/test_value_functions.test b/test/sql/parser/test_value_functions.test\nindex 32d6dd39fd41..ee507d265133 100644\n--- a/test/sql/parser/test_value_functions.test\n+++ b/test/sql/parser/test_value_functions.test\n@@ -71,3 +71,9 @@ select a as \"CURRENT_TIMESTAMP\" from (VALUES (84), (42)) t(a) order by \"CURRENT_\n ----\n 42\n 84\n+\n+# value function conflict in WHERE\n+query I\n+select a as localtime from (VALUES ('2018-01-01'), ('2022-01-01')) t(a) where localtime >= '2020-01-01'\n+----\n+2022-01-01\ndiff --git a/test/sql/sample/table_samples/basic_sample_tests.test b/test/sql/sample/table_samples/basic_sample_tests.test\nindex 2107062f32c3..1bb07364c9c3 100644\n--- a/test/sql/sample/table_samples/basic_sample_tests.test\n+++ b/test/sql/sample/table_samples/basic_sample_tests.test\n@@ -46,14 +46,14 @@ true\ttrue\n \n # about half the samples are below 102400 and half above\n query I\n-select count(*) < 1060 from duckdb_table_sample('t1') where a < 102400;\n+select count(*) from duckdb_table_sample('t1') where a < 102400;\n ----\n-true\n+1069\n \n query I\n-select count(*) < 1060 from duckdb_table_sample('t1') where a > 102400;\n+select count(*) from duckdb_table_sample('t1') where a > 102400;\n ----\n-true\n+979\n \n query I\n select count(*) from t1 using sample (200000);\ndiff --git a/test/sql/sample/table_samples/table_sample_is_stored.test_slow b/test/sql/sample/table_samples/table_sample_is_stored.test_slow\nindex aed4fbc8aae4..f8370557d2a5 100644\n--- a/test/sql/sample/table_samples/table_sample_is_stored.test_slow\n+++ b/test/sql/sample/table_samples/table_sample_is_stored.test_slow\n@@ -61,13 +61,13 @@ select count(*) from duckdb_table_sample('integers_2');\n \n \n query II\n-select floor(b / 1000000) as interval, count(*) > 350 as frequency from duckdb_table_sample('integers_1') group by interval order by all;\n+select floor(b / 1000000) as interval, count(*) as frequency from duckdb_table_sample('integers_1') group by interval order by all;\n ----\n-0.0\ttrue\n-1.0\ttrue\n-2.0\ttrue\n-3.0\ttrue\n-4.0\ttrue\n+0.0\t453\n+1.0\t408\n+2.0\t406\n+3.0\t404\n+4.0\t377\n \n \n # adding another interval should subtract an equal number from the rest of the intervals\n@@ -75,14 +75,14 @@ statement ok\n insert into integers_1 (select (range + 5) a, range b from range(5000000,6000000));\n \n query II\n-select floor(b / 1000000) as interval, count(*) > 300 as frequency from duckdb_table_sample('integers_1') group by interval order by all;\n+select floor(b / 1000000) as interval, count(*) as frequency from duckdb_table_sample('integers_1') group by interval order by all;\n ----\n-0.0\ttrue\n-1.0\ttrue\n-2.0\ttrue\n-3.0\ttrue\n-4.0\ttrue\n-5.0\ttrue\n+0.0\t374\n+1.0\t334\n+2.0\t332\n+3.0\t334\n+4.0\t311\n+5.0\t363\n \n # If double the table count is appended, around half the sample should account for the new values.\n statement ok\n@@ -97,17 +97,17 @@ select count(*) from integers_1;\n ## about half of the samples should have the pair '-1', 1.\n # on latest storage test its something like 997\n query I\n-select count(*) > 924 and count(*) < 1124 from duckdb_table_sample('integers_1') where a = -1 and b = -1;\n+select count(*) from duckdb_table_sample('integers_1') where a = -1 and b = -1;\n ----\n-true\n+914\n \n restart\n \n # updated sample is also newly serialized\n query I\n-select count(*) > 924 and count(*) < 1124 from duckdb_table_sample('integers_1') where a = -1 and b = -1;\n+select count(*) from duckdb_table_sample('integers_1') where a = -1 and b = -1;\n ----\n-true\n+914\n \n # create a view on top of the sample\n statement ok\n@@ -118,27 +118,18 @@ statement ok\n insert into integers_1 (select -2, -2 from range(6000000));\n \n \n-# 2048 / 3 = 682 (706 is good)\n-# on latest storage is 670\n+# 2048 / 3 = 682 (639 is good)\n query I\n-select count(*) > 600 and count(*) < 750 from sample_view where a = -2 and b = -2;\n+select count(*) from sample_view where a = -2 and b = -2;\n ----\n-true\n-\n-#query I nosort result_2\n-#select count(*) from sample_view where a = -2 and b = -2;\n-#----\n+639\n \n restart\n \n query I\n-select count(*) > 600 and count(*) < 750 from sample_view where a = -2 and b = -2;\n-----\n-true\n-\n-query I nosort result_2\n-select count(*) from sample_view where a = -2 and b = -2;\n+select count(*)  from sample_view where a = -2 and b = -2;\n ----\n+639\n \n # currently have 18_000_000 values in the table.\n # to try and get 1 value in the sample, we should add\ndiff --git a/test/sql/storage/parallel/insert_many_compressible_batches.test_slow b/test/sql/storage/parallel/insert_many_compressible_batches.test_slow\nindex 224460ef2dab..483e3e1b5626 100644\n--- a/test/sql/storage/parallel/insert_many_compressible_batches.test_slow\n+++ b/test/sql/storage/parallel/insert_many_compressible_batches.test_slow\n@@ -106,8 +106,10 @@ COMMIT\n # NULLs are RLE compressed (with Roaring)\n # So even with nulls we reach a similar compression ratio\n \n+mode skip\n+\n query I\n-SELECT COUNT(DISTINCT block_id) < 5 FROM pragma_storage_info('integers_batched_load_nulls');\n+SELECT COUNT(DISTINCT block_id) < 8 FROM pragma_storage_info('integers_batched_load_nulls');\n ----\n true\n \ndiff --git a/test/sql/tpch/dbgen_error.test b/test/sql/tpch/dbgen_error.test\nnew file mode 100644\nindex 000000000000..94600c66aa4f\n--- /dev/null\n+++ b/test/sql/tpch/dbgen_error.test\n@@ -0,0 +1,18 @@\n+# name: test/sql/tpch/dbgen_error.test\n+# description: Test error thrown during dbgen\n+# group: [tpch]\n+\n+require tpch\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+SET memory_limit = '100MB';\n+\n+statement ok\n+SET temp_directory = '.unrecognized_folder/folder2'\n+\n+statement error\n+CALL dbgen(sf=1);\n+----\ndiff --git a/test/sql/types/test_typeof.test b/test/sql/types/test_typeof.test\nnew file mode 100644\nindex 000000000000..1a2bd0231d80\n--- /dev/null\n+++ b/test/sql/types/test_typeof.test\n@@ -0,0 +1,10 @@\n+# name: test/sql/types/test_typeof.test\n+# group: [types]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query I\n+SELECT typeof(1)\n+----\n+INTEGER\n\\ No newline at end of file\ndiff --git a/test/sql/window/test_streaming_lead_lag.test b/test/sql/window/test_streaming_lead_lag.test\nindex 143813a6d8b9..de7be94f09e8 100644\n--- a/test/sql/window/test_streaming_lead_lag.test\n+++ b/test/sql/window/test_streaming_lead_lag.test\n@@ -222,3 +222,30 @@ FROM range(10) tbl(i)\n 7\t8\t9\n 8\t9\tNULL\n 9\tNULL\tNULL\n+\n+# Test incomplete buffering\n+query II\n+select * from (\n+\tselect \n+\t\tid, \n+\t\tlead(id, 2047, -1) over() l \n+\tfrom range(6144) tbl(id)\n+\twhere id != 1 \n+\t  and id != 2 \n+\t  and id != 2500 \n+\t  and id != 2501 \n+\t  and id != 2502\n+) \n+where id >= 2040 and id <= 2050;\n+----\n+2040\t4090\n+2041\t4091\n+2042\t4092\n+2043\t4093\n+2044\t4094\n+2045\t4095\n+2046\t4096\n+2047\t4097\n+2048\t4098\n+2049\t4099\n+2050\t4100\ndiff --git a/tools/pythonpkg/tests/fast/api/test_duckdb_connection.py b/tools/pythonpkg/tests/fast/api/test_duckdb_connection.py\nindex 560c8b320444..bd53c985fee4 100644\n--- a/tools/pythonpkg/tests/fast/api/test_duckdb_connection.py\n+++ b/tools/pythonpkg/tests/fast/api/test_duckdb_connection.py\n@@ -270,15 +270,6 @@ def test_from_parquet(self):\n     def test_from_query(self):\n         assert None != duckdb.from_query\n \n-    def test_from_substrait(self):\n-        assert None != duckdb.from_substrait\n-\n-    def test_get_substrait(self):\n-        assert None != duckdb.get_substrait\n-\n-    def test_get_substrait_json(self):\n-        assert None != duckdb.get_substrait_json\n-\n     def test_get_table_names(self):\n         assert None != duckdb.get_table_names\n \n",
  "problem_statement": "Binder error when using query_table with an alias\n### What happens?\n\nGiving `query_table` an alias and referencing columns via that throws a `BinderError`, e.g.:\r\n```\r\nBinder Error: Referenced table \"f\" not found!\r\nCandidate tables: \"unnamed_subquery\"\r\nLINE 1: SELECT bar, f.baz FROM query_table('foo') as f;\r\n                    ^\r\n```\r\nReferencing `baz` without the alias works as expected. My guess would be that the alias is either dropped or pulled into a subquery (like `SELECT bar, f.baz FROM (FROM foo as f)`) when `query_table` is transformed.\n\n### To Reproduce\n\nRun with DuckDB CLI client:\r\n```sql\r\nCREATE TABLE foo AS SELECT 1 as bar, 2 as baz;\r\nSELECT bar, f.baz FROM query_table('foo') as f;\r\n```\r\n\r\nOutput:\r\n```\r\nBinder Error: Referenced table \"f\" not found!\r\nCandidate tables: \"unnamed_subquery\"\r\nLINE 1: SELECT bar, f.baz FROM query_table('foo') as f;\r\n                    ^\r\n```\n\n### OS:\n\n6.6.10-1-MANJARO, x86_64 GNU/Linux\n\n### DuckDB Version:\n\nv1.1.3 19864453f7\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nLennart Hensler\n\n### Affiliation:\n\nPrivate usage\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNot applicable - the reproduction does not require a data set\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n",
  "hints_text": "",
  "created_at": "2025-01-22T21:35:59Z"
}