{
  "repo": "duckdb/duckdb",
  "pull_number": 10799,
  "instance_id": "duckdb__duckdb-10799",
  "issue_numbers": [
    "10790"
  ],
  "base_commit": "42b34697fd400d6f318dc1e964c4540aada0576f",
  "patch": "diff --git a/src/include/duckdb/transaction/meta_transaction.hpp b/src/include/duckdb/transaction/meta_transaction.hpp\nindex e858b5c8037e..5430ddd20d24 100644\n--- a/src/include/duckdb/transaction/meta_transaction.hpp\n+++ b/src/include/duckdb/transaction/meta_transaction.hpp\n@@ -60,6 +60,8 @@ class MetaTransaction {\n \t}\n \n private:\n+\t//! Lock to prevent all_transactions and transactions from getting out of sync\n+\tmutex lock;\n \t//! The set of active transactions for each database\n \treference_map_t<AttachedDatabase, reference<Transaction>> transactions;\n \t//! The set of transactions in order of when they were started\ndiff --git a/src/transaction/meta_transaction.cpp b/src/transaction/meta_transaction.cpp\nindex 739d59d5919a..7cd7fa450b81 100644\n--- a/src/transaction/meta_transaction.cpp\n+++ b/src/transaction/meta_transaction.cpp\n@@ -24,13 +24,28 @@ Transaction &Transaction::Get(ClientContext &context, AttachedDatabase &db) {\n \treturn meta_transaction.GetTransaction(db);\n }\n \n+#ifdef DEBUG\n+static void VerifyAllTransactionsUnique(AttachedDatabase &db, vector<reference<AttachedDatabase>> &all_transactions) {\n+\tfor (auto &tx : all_transactions) {\n+\t\tif (RefersToSameObject(db, tx.get())) {\n+\t\t\tthrow InternalException(\"Database is already present in all_transactions\");\n+\t\t}\n+\t}\n+}\n+#endif\n+\n Transaction &MetaTransaction::GetTransaction(AttachedDatabase &db) {\n+\tlock_guard<mutex> guard(lock);\n \tauto entry = transactions.find(db);\n \tif (entry == transactions.end()) {\n \t\tauto &new_transaction = db.GetTransactionManager().StartTransaction(context);\n \t\tnew_transaction.active_query = active_query;\n+#ifdef DEBUG\n+\t\tVerifyAllTransactionsUnique(db, all_transactions);\n+#endif\n \t\tall_transactions.push_back(db);\n \t\ttransactions.insert(make_pair(reference<AttachedDatabase>(db), reference<Transaction>(new_transaction)));\n+\n \t\treturn new_transaction;\n \t} else {\n \t\tD_ASSERT(entry->second.get().active_query == active_query);\n@@ -60,6 +75,9 @@ Transaction &Transaction::Get(ClientContext &context, Catalog &catalog) {\n \n ErrorData MetaTransaction::Commit() {\n \tErrorData error;\n+#ifdef DEBUG\n+\treference_set_t<AttachedDatabase> committed_tx;\n+#endif\n \t// commit transactions in reverse order\n \tfor (idx_t i = all_transactions.size(); i > 0; i--) {\n \t\tauto &db = all_transactions[i - 1].get();\n@@ -67,6 +85,12 @@ ErrorData MetaTransaction::Commit() {\n \t\tif (entry == transactions.end()) {\n \t\t\tthrow InternalException(\"Could not find transaction corresponding to database in MetaTransaction\");\n \t\t}\n+#ifdef DEBUG\n+\t\tauto already_committed = committed_tx.insert(db).second == false;\n+\t\tif (already_committed) {\n+\t\t\tthrow InternalException(\"All databases inside all_transactions should be unique, invariant broken!\");\n+\t\t}\n+#endif\n \t\tauto &transaction_manager = db.GetTransactionManager();\n \t\tauto &transaction = entry->second.get();\n \t\tif (!error.HasError()) {\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/test_metatransaction.py b/tools/pythonpkg/tests/fast/test_metatransaction.py\nnew file mode 100644\nindex 000000000000..158bb6a963b6\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/test_metatransaction.py\n@@ -0,0 +1,33 @@\n+import pytest\n+\n+pd = pytest.importorskip(\"pandas\")\n+np = pytest.importorskip(\"numpy\")\n+\n+NUMBER_OF_ROWS = 200000\n+NUMBER_OF_COLUMNS = 1\n+\n+\n+class TestMetaTransaction(object):\n+    def test_fetchmany(self, duckdb_cursor):\n+        duckdb_cursor.execute(\"CREATE SEQUENCE id_seq\")\n+        column_names = ',\\n'.join([f'column_{i} FLOAT' for i in range(1, NUMBER_OF_COLUMNS + 1)])\n+        create_table_query = f\"\"\"\n+        CREATE TABLE my_table (\n+            id INTEGER DEFAULT nextval('id_seq'),\n+            {column_names}\n+        )\n+        \"\"\"\n+        # Create a table containing a sequence\n+        duckdb_cursor.execute(create_table_query)\n+\n+        for i in range(20):\n+            # Then insert a large amount of tuples, triggering a parallel execution\n+            data = np.random.rand(NUMBER_OF_ROWS, NUMBER_OF_COLUMNS)\n+            columns = [f'Column_{i+1}' for i in range(NUMBER_OF_COLUMNS)]\n+            df = pd.DataFrame(data, columns=columns)\n+            df_columns = \", \".join(df.columns)\n+            # This gets executed in parallel, causing NextValFunction to be called in parallel\n+            # stressing the MetaTransaction::Get concurrency\n+            duckdb_cursor.execute(f\"INSERT INTO my_table ({df_columns}) SELECT * FROM df\")\n+            print(f\"inserted {i}\")\n+            duckdb_cursor.commit()\ndiff --git a/tools/shell/tests/test_meta_transaction.py b/tools/shell/tests/test_meta_transaction.py\nnew file mode 100644\nindex 000000000000..feff53397b81\n--- /dev/null\n+++ b/tools/shell/tests/test_meta_transaction.py\n@@ -0,0 +1,31 @@\n+# fmt: off\n+\n+from conftest import ShellTest\n+\n+def test_temp_directory(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\"CREATE SEQUENCE id_seq;\")\n+        .statement(\"\"\"\n+            CREATE TABLE my_table (\n+            id INTEGER DEFAULT nextval('id_seq'),\n+            a INTEGER\n+        );\"\"\")\n+        .statement(\"ATTACH ':memory:' AS s1;\")\n+        .statement(\"CREATE TABLE s1.tbl AS FROM range(2000000);\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+        .statement(\"INSERT INTO my_table (a) SELECT * FROM s1.tbl;\")\n+    )\n+    test = test.statement(\"select count(*) from my_table\")\n+    result = test.run()\n+    result.check_stdout(\"20000000\")\n+\n+# fmt: on\n",
  "problem_statement": "Importing large dataframes causes core dump\n### What happens?\n\nWhen writing large dataframes with a sequence for generating an autoincrement id, the following error occurs:\r\n\r\n`libc++abi: terminating due to uncaught exception of type duckdb::InternalException: INTERNAL Error: Attempted to access index 1 within vector of size 1`\r\n\r\nIf you remove the sequence column, this issue goes away\n\n### To Reproduce\n\n```python\r\nimport duckdb\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nnum_rows = 2000000\r\nnum_columns = 10\r\n\r\ncon = duckdb.connect()\r\ncon.execute(\"CREATE SEQUENCE id_seq\")\r\ncolumn_names = ',\\n'.join([f'column_{i} FLOAT' for i in range(1, num_columns + 1)])\r\ncreate_table_query = f\"\"\"\r\nCREATE TABLE my_table (\r\n    id INTEGER DEFAULT nextval('id_seq'),\r\n    {column_names}\r\n)\r\n\"\"\"\r\ncon.execute(create_table_query)\r\n\r\n\r\nfor i in range(20):\r\n    data = np.random.rand(num_rows, num_columns)\r\n    columns = [f'Column_{i+1}' for i in range(num_columns)]\r\n    df = pd.DataFrame(data, columns=columns)\r\n    df_columns = \", \".join(df.columns)\r\n    con.execute(f\"INSERT INTO my_table ({df_columns}) SELECT * FROM df\")\r\n    print(f\"inserted {i}\")\r\n    con.commit()\r\n```\n\n### OS:\n\nmacos sonoma 14.3\n\n### DuckDB Version:\n\n0.9.2, 0.10.1-dev240\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nRichmond Hollen\n\n### Affiliation:\n\nOuster, Inc\n\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\n\nI have tested with a nightly build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "Hi @rhollen, thanks, this is indeed reproducible! We'll take a look.\nbtw, we have a workaround in our use case that works. If you break the dataframe into chunks of 100k rows, it works with the sequence in place. I'm also considering removing the default and writing the sequence to the dataframe itself, which should also fix it, although I haven't tested that.",
  "created_at": "2024-02-22T09:39:02Z"
}