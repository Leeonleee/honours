{
  "repo": "duckdb/duckdb",
  "pull_number": 10300,
  "instance_id": "duckdb__duckdb-10300",
  "issue_numbers": [
    "10218"
  ],
  "base_commit": "2e3d7058de8d976edc52165a2ef6c5b1df6b29ba",
  "patch": "diff --git a/extension/json/buffered_json_reader.cpp b/extension/json/buffered_json_reader.cpp\nindex d6c3fb8c75a9..868e71ecea86 100644\n--- a/extension/json/buffered_json_reader.cpp\n+++ b/extension/json/buffered_json_reader.cpp\n@@ -14,7 +14,8 @@ JSONBufferHandle::JSONBufferHandle(idx_t buffer_index_p, idx_t readers_p, Alloca\n \n JSONFileHandle::JSONFileHandle(unique_ptr<FileHandle> file_handle_p, Allocator &allocator_p)\n     : file_handle(std::move(file_handle_p)), allocator(allocator_p), can_seek(file_handle->CanSeek()),\n-      file_size(file_handle->GetFileSize()), read_position(0), requested_reads(0), actual_reads(0), cached_size(0) {\n+      file_size(file_handle->GetFileSize()), read_position(0), requested_reads(0), actual_reads(0),\n+      last_read_requested(false), cached_size(0) {\n }\n \n bool JSONFileHandle::IsOpen() const {\n@@ -33,6 +34,7 @@ void JSONFileHandle::Reset() {\n \tread_position = 0;\n \trequested_reads = 0;\n \tactual_reads = 0;\n+\tlast_read_requested = false;\n \tif (IsOpen() && CanSeek()) {\n \t\tfile_handle->Reset();\n \t}\n@@ -58,73 +60,90 @@ FileHandle &JSONFileHandle::GetHandle() {\n \treturn *file_handle;\n }\n \n-idx_t JSONFileHandle::GetPositionAndSize(idx_t &position, idx_t requested_size) {\n+bool JSONFileHandle::GetPositionAndSize(idx_t &position, idx_t &size, idx_t requested_size) {\n \tD_ASSERT(requested_size != 0);\n+\tif (last_read_requested) {\n+\t\treturn false;\n+\t}\n \n \tposition = read_position;\n-\tauto actual_size = MinValue<idx_t>(requested_size, Remaining());\n-\tread_position += actual_size;\n-\tif (actual_size != 0) {\n-\t\trequested_reads++;\n+\tsize = MinValue<idx_t>(requested_size, Remaining());\n+\tread_position += size;\n+\n+\trequested_reads++;\n+\tif (size == 0) {\n+\t\tlast_read_requested = true;\n \t}\n \n-\treturn actual_size;\n+\treturn true;\n }\n \n-void JSONFileHandle::ReadAtPosition(char *pointer, idx_t size, idx_t position, bool sample_run,\n+void JSONFileHandle::ReadAtPosition(char *pointer, idx_t size, idx_t position, bool &file_done, bool sample_run,\n                                     optional_ptr<FileHandle> override_handle) {\n-\tD_ASSERT(size != 0);\n-\tauto &handle = override_handle ? *override_handle.get() : *file_handle.get();\n-\n-\tif (can_seek) {\n-\t\thandle.Read(pointer, size, position);\n-\t} else if (sample_run) { // Cache the buffer\n-\t\thandle.Read(pointer, size, position);\n+\tif (size != 0) {\n+\t\tauto &handle = override_handle ? *override_handle.get() : *file_handle.get();\n+\t\tif (can_seek) {\n+\t\t\thandle.Read(pointer, size, position);\n+\t\t} else if (sample_run) { // Cache the buffer\n+\t\t\thandle.Read(pointer, size, position);\n \n-\t\tcached_buffers.emplace_back(allocator.Allocate(size));\n-\t\tmemcpy(cached_buffers.back().get(), pointer, size);\n-\t\tcached_size += size;\n-\t} else {\n-\t\tif (!cached_buffers.empty() || position < cached_size) {\n-\t\t\tReadFromCache(pointer, size, position);\n-\t\t}\n+\t\t\tcached_buffers.emplace_back(allocator.Allocate(size));\n+\t\t\tmemcpy(cached_buffers.back().get(), pointer, size);\n+\t\t\tcached_size += size;\n+\t\t} else {\n+\t\t\tif (!cached_buffers.empty() || position < cached_size) {\n+\t\t\t\tReadFromCache(pointer, size, position);\n+\t\t\t}\n \n-\t\tif (size != 0) {\n-\t\t\thandle.Read(pointer, size, position);\n+\t\t\tif (size != 0) {\n+\t\t\t\thandle.Read(pointer, size, position);\n+\t\t\t}\n \t\t}\n \t}\n-\tif (++actual_reads > requested_reads) {\n+\n+\tconst auto incremented_actual_reads = ++actual_reads;\n+\tif (incremented_actual_reads > requested_reads) {\n \t\tthrow InternalException(\"JSONFileHandle performed more actual reads than requested reads\");\n \t}\n+\n+\tif (last_read_requested && incremented_actual_reads == requested_reads) {\n+\t\tfile_done = true;\n+\t}\n }\n \n-idx_t JSONFileHandle::Read(char *pointer, idx_t requested_size, bool sample_run) {\n+bool JSONFileHandle::Read(char *pointer, idx_t &read_size, idx_t requested_size, bool &file_done, bool sample_run) {\n \tD_ASSERT(requested_size != 0);\n-\tif (can_seek) {\n-\t\tauto actual_size = ReadInternal(pointer, requested_size);\n-\t\tread_position += actual_size;\n-\t\treturn actual_size;\n+\tif (last_read_requested) {\n+\t\treturn false;\n \t}\n \n-\tif (sample_run) { // Cache the buffer\n-\t\tauto actual_size = ReadInternal(pointer, requested_size);\n-\t\tif (actual_size > 0) {\n-\t\t\tcached_buffers.emplace_back(allocator.Allocate(actual_size));\n-\t\t\tmemcpy(cached_buffers.back().get(), pointer, actual_size);\n+\tif (can_seek) {\n+\t\tread_size = ReadInternal(pointer, requested_size);\n+\t\tread_position += read_size;\n+\t} else if (sample_run) { // Cache the buffer\n+\t\tread_size = ReadInternal(pointer, requested_size);\n+\t\tif (read_size > 0) {\n+\t\t\tcached_buffers.emplace_back(allocator.Allocate(read_size));\n+\t\t\tmemcpy(cached_buffers.back().get(), pointer, read_size);\n+\t\t}\n+\t\tcached_size += read_size;\n+\t\tread_position += read_size;\n+\t} else {\n+\t\tread_size = 0;\n+\t\tif (!cached_buffers.empty() || read_position < cached_size) {\n+\t\t\tread_size += ReadFromCache(pointer, requested_size, read_position);\n+\t\t}\n+\t\tif (requested_size != 0) {\n+\t\t\tread_size += ReadInternal(pointer, requested_size);\n \t\t}\n-\t\tcached_size += actual_size;\n-\t\tread_position += actual_size;\n-\t\treturn actual_size;\n \t}\n \n-\tidx_t actual_size = 0;\n-\tif (!cached_buffers.empty() || read_position < cached_size) {\n-\t\tactual_size += ReadFromCache(pointer, requested_size, read_position);\n-\t}\n-\tif (requested_size != 0) {\n-\t\tactual_size += ReadInternal(pointer, requested_size);\n+\tif (read_size == 0) {\n+\t\tlast_read_requested = true;\n+\t\tfile_done = true;\n \t}\n-\treturn actual_size;\n+\n+\treturn true;\n }\n \n idx_t JSONFileHandle::ReadInternal(char *pointer, const idx_t requested_size) {\n@@ -182,19 +201,6 @@ void BufferedJSONReader::OpenJSONFile() {\n \tReset();\n }\n \n-void BufferedJSONReader::CloseJSONFile() {\n-\twhile (true) {\n-\t\tlock_guard<mutex> guard(lock);\n-\t\tif (!file_handle->IsOpen()) {\n-\t\t\treturn; // Already closed\n-\t\t}\n-\t\tif (file_handle->RequestedReadsComplete()) {\n-\t\t\tfile_handle->Close();\n-\t\t\tbreak;\n-\t\t}\n-\t}\n-}\n-\n void BufferedJSONReader::Reset() {\n \tbuffer_index = 0;\n \tbuffer_map.clear();\n@@ -257,10 +263,11 @@ optional_ptr<JSONBufferHandle> BufferedJSONReader::GetBuffer(idx_t buffer_idx) {\n \treturn it == buffer_map.end() ? nullptr : it->second.get();\n }\n \n-AllocatedData BufferedJSONReader::RemoveBuffer(idx_t buffer_idx) {\n+AllocatedData BufferedJSONReader::RemoveBuffer(JSONBufferHandle &handle) {\n \tlock_guard<mutex> guard(lock);\n-\tauto it = buffer_map.find(buffer_idx);\n+\tauto it = buffer_map.find(handle.buffer_index);\n \tD_ASSERT(it != buffer_map.end());\n+\tD_ASSERT(RefersToSameObject(handle, *it->second));\n \tauto result = std::move(it->second->buffer);\n \tbuffer_map.erase(it);\n \treturn result;\n@@ -271,9 +278,12 @@ idx_t BufferedJSONReader::GetBufferIndex() {\n \treturn buffer_index++;\n }\n \n-void BufferedJSONReader::SetBufferLineOrObjectCount(idx_t index, idx_t count) {\n+void BufferedJSONReader::SetBufferLineOrObjectCount(JSONBufferHandle &handle, idx_t count) {\n \tlock_guard<mutex> guard(lock);\n-\tbuffer_line_or_object_counts[index] = count;\n+\tD_ASSERT(buffer_map.find(handle.buffer_index) != buffer_map.end());\n+\tD_ASSERT(RefersToSameObject(handle, *buffer_map.find(handle.buffer_index)->second));\n+\tD_ASSERT(buffer_line_or_object_counts[handle.buffer_index] == -1);\n+\tbuffer_line_or_object_counts[handle.buffer_index] = count;\n }\n \n idx_t BufferedJSONReader::GetLineNumber(idx_t buf_index, idx_t line_or_object_in_buf) {\n@@ -292,11 +302,11 @@ idx_t BufferedJSONReader::GetLineNumber(idx_t buf_index, idx_t line_or_object_in\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\tline += buffer_line_or_object_counts[b_idx];\n-\t\t\t\t\tthrown = true;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \t\tif (can_throw) {\n+\t\t\tthrown = true;\n \t\t\t// SQL uses 1-based indexing so I guess we will do that in our exception here as well\n \t\t\treturn line + 1;\n \t\t}\ndiff --git a/extension/json/include/buffered_json_reader.hpp b/extension/json/include/buffered_json_reader.hpp\nindex b5d453ff891d..c7e31cac5c37 100644\n--- a/extension/json/include/buffered_json_reader.hpp\n+++ b/extension/json/include/buffered_json_reader.hpp\n@@ -68,10 +68,11 @@ struct JSONFileHandle {\n \n \tFileHandle &GetHandle();\n \n-\tidx_t GetPositionAndSize(idx_t &position, idx_t requested_size);\n-\tidx_t Read(char *pointer, idx_t requested_size, bool sample_run);\n+\t//! The next two functions return whether the read was successful\n+\tbool GetPositionAndSize(idx_t &position, idx_t &size, idx_t requested_size);\n+\tbool Read(char *pointer, idx_t &read_size, idx_t requested_size, bool &file_done, bool sample_run);\n \t//! Read at position optionally allows passing a custom handle to read from, otherwise the default one is used\n-\tvoid ReadAtPosition(char *pointer, idx_t size, idx_t position, bool sample_run,\n+\tvoid ReadAtPosition(char *pointer, idx_t size, idx_t position, bool &file_done, bool sample_run,\n \t                    optional_ptr<FileHandle> override_handle = nullptr);\n \n private:\n@@ -91,6 +92,7 @@ struct JSONFileHandle {\n \tidx_t read_position;\n \tidx_t requested_reads;\n \tatomic<idx_t> actual_reads;\n+\tbool last_read_requested;\n \n \t//! Cached buffers for resetting when reading stream\n \tvector<AllocatedData> cached_buffers;\n@@ -102,7 +104,6 @@ class BufferedJSONReader {\n \tBufferedJSONReader(ClientContext &context, BufferedJSONReaderOptions options, string file_name);\n \n \tvoid OpenJSONFile();\n-\tvoid CloseJSONFile();\n \tvoid Reset();\n \n \tbool HasFileHandle() const;\n@@ -123,12 +124,12 @@ class BufferedJSONReader {\n \t//! Insert/get/remove buffer (grabs the lock)\n \tvoid InsertBuffer(idx_t buffer_idx, unique_ptr<JSONBufferHandle> &&buffer);\n \toptional_ptr<JSONBufferHandle> GetBuffer(idx_t buffer_idx);\n-\tAllocatedData RemoveBuffer(idx_t buffer_idx);\n+\tAllocatedData RemoveBuffer(JSONBufferHandle &handle);\n \n \t//! Get a new buffer index (must hold the lock)\n \tidx_t GetBufferIndex();\n \t//! Set line count for a buffer that is done (grabs the lock)\n-\tvoid SetBufferLineOrObjectCount(idx_t index, idx_t count);\n+\tvoid SetBufferLineOrObjectCount(JSONBufferHandle &handle, idx_t count);\n \t//! Throws a parse error that mentions the file name and line number\n \tvoid ThrowParseError(idx_t buf_index, idx_t line_or_object_in_buf, yyjson_read_err &err, const string &extra = \"\");\n \t//! Throws a transform error that mentions the file name and line number\ndiff --git a/extension/json/include/json.json b/extension/json/include/json.json\nindex 5c94804d0375..8b182dec927c 100644\n--- a/extension/json/include/json.json\n+++ b/extension/json/include/json.json\n@@ -147,6 +147,12 @@\n         \"name\": \"maximum_sample_files\",\n         \"type\": \"idx_t\",\n         \"default\": 32\n+      },\n+      {\n+        \"id\": 115,\n+        \"name\": \"convert_strings_to_integers\",\n+        \"type\": \"bool\",\n+        \"default\": false\n       }\n     ],\n     \"constructor\": [\"$ClientContext\", \"files\", \"date_format\", \"timestamp_format\"]\ndiff --git a/extension/json/include/json_scan.hpp b/extension/json/include/json_scan.hpp\nindex b1f7f00e253f..959c3e14a72a 100644\n--- a/extension/json/include/json_scan.hpp\n+++ b/extension/json/include/json_scan.hpp\n@@ -128,6 +128,8 @@ struct JSONScanData : public TableFunctionData {\n \tdouble field_appearance_threshold = 0.1;\n \t//! The maximum number of files we sample to sample sample_size rows\n \tidx_t maximum_sample_files = 32;\n+\t//! Whether we auto-detect and convert JSON strings to integers\n+\tbool convert_strings_to_integers = false;\n \n \t//! All column names (in order)\n \tvector<string> names;\n@@ -224,18 +226,17 @@ struct JSONScanLocalState {\n \n private:\n \tbool ReadNextBuffer(JSONScanGlobalState &gstate);\n-\tvoid ReadNextBufferInternal(JSONScanGlobalState &gstate, optional_idx &buffer_index);\n-\tvoid ReadNextBufferSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index);\n-\tvoid ReadNextBufferNoSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index);\n+\tbool ReadNextBufferInternal(JSONScanGlobalState &gstate, optional_idx &buffer_index, bool &file_done);\n+\tbool ReadNextBufferSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index, bool &file_done);\n+\tbool ReadNextBufferNoSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index, bool &file_done);\n \tvoid SkipOverArrayStart();\n \n \tvoid ReadAndAutoDetect(JSONScanGlobalState &gstate, optional_idx &buffer_index);\n-\tvoid ReconstructFirstObject();\n+\tbool ReconstructFirstObject();\n \tvoid ParseNextChunk();\n \n \tvoid ParseJSON(char *const json_start, const idx_t json_size, const idx_t remaining);\n \tvoid ThrowObjectSizeError(const idx_t object_size);\n-\tvoid ThrowInvalidAtEndError();\n \n \t//! Must hold the lock\n \tvoid TryIncrementFileIndex(JSONScanGlobalState &gstate) const;\ndiff --git a/extension/json/include/json_structure.hpp b/extension/json/include/json_structure.hpp\nindex 8c6e6d58b90b..7fcd43b35fe2 100644\n--- a/extension/json/include/json_structure.hpp\n+++ b/extension/json/include/json_structure.hpp\n@@ -31,19 +31,19 @@ struct JSONStructureNode {\n \tJSONStructureDescription &GetOrCreateDescription(LogicalTypeId type);\n \n \tbool ContainsVarchar() const;\n-\tvoid InitializeCandidateTypes(const idx_t max_depth, idx_t depth = 0);\n-\tvoid RefineCandidateTypes(yyjson_val *vals[], idx_t count, Vector &string_vector, ArenaAllocator &allocator,\n+\tvoid InitializeCandidateTypes(const idx_t max_depth, const bool convert_strings_to_integers, idx_t depth = 0);\n+\tvoid RefineCandidateTypes(yyjson_val *vals[], idx_t val_count, Vector &string_vector, ArenaAllocator &allocator,\n \t                          DateFormatMap &date_format_map);\n \n private:\n-\tvoid RefineCandidateTypesArray(yyjson_val *vals[], idx_t count, Vector &string_vector, ArenaAllocator &allocator,\n-\t                               DateFormatMap &date_format_map);\n-\tvoid RefineCandidateTypesObject(yyjson_val *vals[], idx_t count, Vector &string_vector, ArenaAllocator &allocator,\n+\tvoid RefineCandidateTypesArray(yyjson_val *vals[], idx_t val_count, Vector &string_vector,\n+\t                               ArenaAllocator &allocator, DateFormatMap &date_format_map);\n+\tvoid RefineCandidateTypesObject(yyjson_val *vals[], idx_t val_count, Vector &string_vector,\n+\t                                ArenaAllocator &allocator, DateFormatMap &date_format_map);\n+\tvoid RefineCandidateTypesString(yyjson_val *vals[], idx_t val_count, Vector &string_vector,\n \t                                DateFormatMap &date_format_map);\n-\tvoid RefineCandidateTypesString(yyjson_val *vals[], idx_t count, Vector &string_vector,\n-\t                                DateFormatMap &date_format_map);\n-\tvoid EliminateCandidateTypes(idx_t count, Vector &string_vector, DateFormatMap &date_format_map);\n-\tbool EliminateCandidateFormats(idx_t count, Vector &string_vector, Vector &result_vector,\n+\tvoid EliminateCandidateTypes(idx_t vec_count, Vector &string_vector, DateFormatMap &date_format_map);\n+\tbool EliminateCandidateFormats(idx_t vec_count, Vector &string_vector, Vector &result_vector,\n \t                               vector<StrpTimeFormat> &formats);\n \n public:\ndiff --git a/extension/json/json_functions/json_structure.cpp b/extension/json/json_functions/json_structure.cpp\nindex df0b87c55c3e..ad319c6a9469 100644\n--- a/extension/json/json_functions/json_structure.cpp\n+++ b/extension/json/json_functions/json_structure.cpp\n@@ -94,7 +94,8 @@ bool JSONStructureNode::ContainsVarchar() const {\n \treturn false;\n }\n \n-void JSONStructureNode::InitializeCandidateTypes(const idx_t max_depth, idx_t depth) {\n+void JSONStructureNode::InitializeCandidateTypes(const idx_t max_depth, const bool convert_strings_to_integers,\n+                                                 idx_t depth) {\n \tif (depth >= max_depth) {\n \t\treturn;\n \t}\n@@ -105,16 +106,21 @@ void JSONStructureNode::InitializeCandidateTypes(const idx_t max_depth, idx_t de\n \tauto &description = descriptions[0];\n \tif (description.type == LogicalTypeId::VARCHAR && !initialized) {\n \t\t// We loop through the candidate types and format templates from back to front\n-\t\tdescription.candidate_types = {LogicalTypeId::UUID, LogicalTypeId::BIGINT, LogicalTypeId::TIMESTAMP,\n-\t\t                               LogicalTypeId::DATE, LogicalTypeId::TIME};\n+\t\tif (convert_strings_to_integers) {\n+\t\t\tdescription.candidate_types = {LogicalTypeId::UUID, LogicalTypeId::BIGINT, LogicalTypeId::TIMESTAMP,\n+\t\t\t                               LogicalTypeId::DATE, LogicalTypeId::TIME};\n+\t\t} else {\n+\t\t\tdescription.candidate_types = {LogicalTypeId::UUID, LogicalTypeId::TIMESTAMP, LogicalTypeId::DATE,\n+\t\t\t                               LogicalTypeId::TIME};\n+\t\t}\n \t}\n \tinitialized = true;\n \tfor (auto &child : description.children) {\n-\t\tchild.InitializeCandidateTypes(max_depth, depth + 1);\n+\t\tchild.InitializeCandidateTypes(max_depth, convert_strings_to_integers, depth + 1);\n \t}\n }\n \n-void JSONStructureNode::RefineCandidateTypes(yyjson_val *vals[], idx_t count, Vector &string_vector,\n+void JSONStructureNode::RefineCandidateTypes(yyjson_val *vals[], idx_t val_count, Vector &string_vector,\n                                              ArenaAllocator &allocator, DateFormatMap &date_format_map) {\n \tif (descriptions.size() != 1) {\n \t\t// We can't refine types if we have more than 1 description (yet), defaults to JSON type for now\n@@ -126,17 +132,17 @@ void JSONStructureNode::RefineCandidateTypes(yyjson_val *vals[], idx_t count, Ve\n \tauto &description = descriptions[0];\n \tswitch (description.type) {\n \tcase LogicalTypeId::LIST:\n-\t\treturn RefineCandidateTypesArray(vals, count, string_vector, allocator, date_format_map);\n+\t\treturn RefineCandidateTypesArray(vals, val_count, string_vector, allocator, date_format_map);\n \tcase LogicalTypeId::STRUCT:\n-\t\treturn RefineCandidateTypesObject(vals, count, string_vector, allocator, date_format_map);\n+\t\treturn RefineCandidateTypesObject(vals, val_count, string_vector, allocator, date_format_map);\n \tcase LogicalTypeId::VARCHAR:\n-\t\treturn RefineCandidateTypesString(vals, count, string_vector, date_format_map);\n+\t\treturn RefineCandidateTypesString(vals, val_count, string_vector, date_format_map);\n \tdefault:\n \t\treturn;\n \t}\n }\n \n-void JSONStructureNode::RefineCandidateTypesArray(yyjson_val *vals[], idx_t count, Vector &string_vector,\n+void JSONStructureNode::RefineCandidateTypesArray(yyjson_val *vals[], idx_t val_count, Vector &string_vector,\n                                                   ArenaAllocator &allocator, DateFormatMap &date_format_map) {\n \tD_ASSERT(descriptions.size() == 1 && descriptions[0].type == LogicalTypeId::LIST);\n \tauto &desc = descriptions[0];\n@@ -144,7 +150,7 @@ void JSONStructureNode::RefineCandidateTypesArray(yyjson_val *vals[], idx_t coun\n \tauto &child = desc.children[0];\n \n \tidx_t total_list_size = 0;\n-\tfor (idx_t i = 0; i < count; i++) {\n+\tfor (idx_t i = 0; i < val_count; i++) {\n \t\tif (vals[i] && !unsafe_yyjson_is_null(vals[i])) {\n \t\t\tD_ASSERT(yyjson_is_arr(vals[i]));\n \t\t\ttotal_list_size += unsafe_yyjson_get_len(vals[i]);\n@@ -157,7 +163,7 @@ void JSONStructureNode::RefineCandidateTypesArray(yyjson_val *vals[], idx_t coun\n \n \tsize_t idx, max;\n \tyyjson_val *child_val;\n-\tfor (idx_t i = 0; i < count; i++) {\n+\tfor (idx_t i = 0; i < val_count; i++) {\n \t\tif (vals[i] && !unsafe_yyjson_is_null(vals[i])) {\n \t\t\tyyjson_arr_foreach(vals[i], idx, max, child_val) {\n \t\t\t\tchild_vals[offset++] = child_val;\n@@ -167,7 +173,7 @@ void JSONStructureNode::RefineCandidateTypesArray(yyjson_val *vals[], idx_t coun\n \tchild.RefineCandidateTypes(child_vals, total_list_size, string_vector, allocator, date_format_map);\n }\n \n-void JSONStructureNode::RefineCandidateTypesObject(yyjson_val *vals[], idx_t count, Vector &string_vector,\n+void JSONStructureNode::RefineCandidateTypesObject(yyjson_val *vals[], idx_t val_count, Vector &string_vector,\n                                                    ArenaAllocator &allocator, DateFormatMap &date_format_map) {\n \tD_ASSERT(descriptions.size() == 1 && descriptions[0].type == LogicalTypeId::STRUCT);\n \tauto &desc = descriptions[0];\n@@ -177,7 +183,7 @@ void JSONStructureNode::RefineCandidateTypesObject(yyjson_val *vals[], idx_t cou\n \tchild_vals.reserve(child_count);\n \tfor (idx_t child_idx = 0; child_idx < child_count; child_idx++) {\n \t\tchild_vals.emplace_back(\n-\t\t    reinterpret_cast<yyjson_val **>(allocator.AllocateAligned(count * sizeof(yyjson_val *))));\n+\t\t    reinterpret_cast<yyjson_val **>(allocator.AllocateAligned(val_count * sizeof(yyjson_val *))));\n \t}\n \n \tidx_t found_key_count;\n@@ -186,7 +192,7 @@ void JSONStructureNode::RefineCandidateTypesObject(yyjson_val *vals[], idx_t cou\n \tconst auto &key_map = desc.key_map;\n \tsize_t idx, max;\n \tyyjson_val *child_key, *child_val;\n-\tfor (idx_t i = 0; i < count; i++) {\n+\tfor (idx_t i = 0; i < val_count; i++) {\n \t\tif (vals[i] && !unsafe_yyjson_is_null(vals[i])) {\n \t\t\tfound_key_count = 0;\n \t\t\tmemset(found_keys, false, child_count);\n@@ -220,23 +226,24 @@ void JSONStructureNode::RefineCandidateTypesObject(yyjson_val *vals[], idx_t cou\n \t}\n \n \tfor (idx_t child_idx = 0; child_idx < child_count; child_idx++) {\n-\t\tdesc.children[child_idx].RefineCandidateTypes(child_vals[child_idx], count, string_vector, allocator,\n+\t\tdesc.children[child_idx].RefineCandidateTypes(child_vals[child_idx], val_count, string_vector, allocator,\n \t\t                                              date_format_map);\n \t}\n }\n \n-void JSONStructureNode::RefineCandidateTypesString(yyjson_val *vals[], idx_t count, Vector &string_vector,\n+void JSONStructureNode::RefineCandidateTypesString(yyjson_val *vals[], idx_t val_count, Vector &string_vector,\n                                                    DateFormatMap &date_format_map) {\n \tD_ASSERT(descriptions.size() == 1 && descriptions[0].type == LogicalTypeId::VARCHAR);\n \tif (descriptions[0].candidate_types.empty()) {\n \t\treturn;\n \t}\n \tstatic JSONTransformOptions OPTIONS;\n-\tJSONTransform::GetStringVector(vals, count, LogicalType::SQLNULL, string_vector, OPTIONS);\n-\tEliminateCandidateTypes(count, string_vector, date_format_map);\n+\tJSONTransform::GetStringVector(vals, val_count, LogicalType::SQLNULL, string_vector, OPTIONS);\n+\tEliminateCandidateTypes(val_count, string_vector, date_format_map);\n }\n \n-void JSONStructureNode::EliminateCandidateTypes(idx_t count, Vector &string_vector, DateFormatMap &date_format_map) {\n+void JSONStructureNode::EliminateCandidateTypes(idx_t vec_count, Vector &string_vector,\n+                                                DateFormatMap &date_format_map) {\n \tD_ASSERT(descriptions.size() == 1 && descriptions[0].type == LogicalTypeId::VARCHAR);\n \tauto &description = descriptions[0];\n \tauto &candidate_types = description.candidate_types;\n@@ -245,17 +252,17 @@ void JSONStructureNode::EliminateCandidateTypes(idx_t count, Vector &string_vect\n \t\t\treturn;\n \t\t}\n \t\tconst auto type = candidate_types.back();\n-\t\tVector result_vector(type, count);\n+\t\tVector result_vector(type, vec_count);\n \t\tif (date_format_map.HasFormats(type)) {\n \t\t\tauto &formats = date_format_map.GetCandidateFormats(type);\n-\t\t\tif (EliminateCandidateFormats(count, string_vector, result_vector, formats)) {\n+\t\t\tif (EliminateCandidateFormats(vec_count, string_vector, result_vector, formats)) {\n \t\t\t\treturn;\n \t\t\t} else {\n \t\t\t\tcandidate_types.pop_back();\n \t\t\t}\n \t\t} else {\n \t\t\tstring error_message;\n-\t\t\tif (!VectorOperations::DefaultTryCast(string_vector, result_vector, count, &error_message, true)) {\n+\t\t\tif (!VectorOperations::DefaultTryCast(string_vector, result_vector, vec_count, &error_message, true)) {\n \t\t\t\tcandidate_types.pop_back();\n \t\t\t} else {\n \t\t\t\treturn;\n@@ -289,7 +296,7 @@ bool TryParse(Vector &string_vector, StrpTimeFormat &format, const idx_t count)\n \treturn true;\n }\n \n-bool JSONStructureNode::EliminateCandidateFormats(idx_t count, Vector &string_vector, Vector &result_vector,\n+bool JSONStructureNode::EliminateCandidateFormats(idx_t vec_count, Vector &string_vector, Vector &result_vector,\n                                                   vector<StrpTimeFormat> &formats) {\n \tD_ASSERT(descriptions.size() == 1 && descriptions[0].type == LogicalTypeId::VARCHAR);\n \tconst auto type = result_vector.GetType().id();\n@@ -299,10 +306,10 @@ bool JSONStructureNode::EliminateCandidateFormats(idx_t count, Vector &string_ve\n \t\tbool success;\n \t\tswitch (type) {\n \t\tcase LogicalTypeId::DATE:\n-\t\t\tsuccess = TryParse<TryParseDate, date_t>(string_vector, format, count);\n+\t\t\tsuccess = TryParse<TryParseDate, date_t>(string_vector, format, vec_count);\n \t\t\tbreak;\n \t\tcase LogicalTypeId::TIMESTAMP:\n-\t\t\tsuccess = TryParse<TryParseTimeStamp, timestamp_t>(string_vector, format, count);\n+\t\t\tsuccess = TryParse<TryParseTimeStamp, timestamp_t>(string_vector, format, vec_count);\n \t\t\tbreak;\n \t\tdefault:\n \t\t\tthrow InternalException(\"No date/timestamp formats for %s\", EnumUtil::ToString(type));\ndiff --git a/extension/json/json_functions/read_json.cpp b/extension/json/json_functions/read_json.cpp\nindex a5d62ef5c845..8b331c8a7c26 100644\n--- a/extension/json/json_functions/read_json.cpp\n+++ b/extension/json/json_functions/read_json.cpp\n@@ -46,7 +46,7 @@ void JSONScan::AutoDetect(ClientContext &context, JSONScanData &bind_data, vecto\n \t\t\tif (!node.ContainsVarchar()) { // Can't refine non-VARCHAR types\n \t\t\t\tcontinue;\n \t\t\t}\n-\t\t\tnode.InitializeCandidateTypes(bind_data.max_depth);\n+\t\t\tnode.InitializeCandidateTypes(bind_data.max_depth, bind_data.convert_strings_to_integers);\n \t\t\tnode.RefineCandidateTypes(lstate.values, next, string_vector, allocator, bind_data.date_format_map);\n \t\t\tremaining -= next;\n \t\t}\n@@ -211,6 +211,8 @@ unique_ptr<FunctionData> ReadJSONBind(ClientContext &context, TableFunctionBindI\n \t\t\t\tthrow BinderException(\"read_json \\\"maximum_sample_files\\\" parameter must be positive, or -1 to remove \"\n \t\t\t\t                      \"the limit on the number of files used to sample \\\"sample_size\\\" rows.\");\n \t\t\t}\n+\t\t} else if (loption == \"convert_strings_to_integers\") {\n+\t\t\tbind_data->convert_strings_to_integers = BooleanValue::Get(kv.second);\n \t\t}\n \t}\n \n@@ -340,6 +342,7 @@ TableFunctionSet CreateJSONFunctionInfo(string name, shared_ptr<JSONScanInfo> in\n \ttable_function.name = std::move(name);\n \ttable_function.named_parameters[\"maximum_depth\"] = LogicalType::BIGINT;\n \ttable_function.named_parameters[\"field_appearance_threshold\"] = LogicalType::DOUBLE;\n+\ttable_function.named_parameters[\"convert_strings_to_integers\"] = LogicalType::BOOLEAN;\n \treturn MultiFileReader::CreateFunctionSet(table_function);\n }\n \ndiff --git a/extension/json/json_scan.cpp b/extension/json/json_scan.cpp\nindex 5d46af5255b9..f96d6c55eac6 100644\n--- a/extension/json/json_scan.cpp\n+++ b/extension/json/json_scan.cpp\n@@ -146,9 +146,8 @@ JSONScanGlobalState::JSONScanGlobalState(ClientContext &context, const JSONScanD\n \n JSONScanLocalState::JSONScanLocalState(ClientContext &context, JSONScanGlobalState &gstate)\n     : scan_count(0), batch_index(DConstants::INVALID_INDEX), total_read_size(0), total_tuple_count(0),\n-      bind_data(gstate.bind_data), allocator(BufferAllocator::Get(context)), current_reader(nullptr),\n-      current_buffer_handle(nullptr), is_last(false), fs(FileSystem::GetFileSystem(context)),\n-      thread_local_filehandle(nullptr), buffer_size(0), buffer_offset(0), prev_buffer_remainder(0) {\n+      bind_data(gstate.bind_data), allocator(BufferAllocator::Get(context)), is_last(false),\n+      fs(FileSystem::GetFileSystem(context)), buffer_size(0), buffer_offset(0), prev_buffer_remainder(0) {\n \n \t// Buffer to reconstruct JSON values when they cross a buffer boundary\n \treconstruct_buffer = gstate.allocator.Allocate(gstate.buffer_capacity);\n@@ -275,11 +274,11 @@ idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {\n \t\t\tif (!ReadNextBuffer(gstate)) {\n \t\t\t\tbreak;\n \t\t\t}\n-\t\t\tD_ASSERT(buffer_size != 0);\n \t\t\tif (current_buffer_handle->buffer_index != 0 &&\n \t\t\t    current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n-\t\t\t\tReconstructFirstObject();\n-\t\t\t\tscan_count++;\n+\t\t\t\tif (ReconstructFirstObject()) {\n+\t\t\t\t\tscan_count++;\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n@@ -289,14 +288,14 @@ idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {\n \treturn scan_count;\n }\n \n-static inline const char *NextNewline(char *ptr, idx_t size) {\n-\treturn char_ptr_cast(memchr(ptr, '\\n', size));\n+static inline const char *NextNewline(const char *ptr, const idx_t size) {\n+\treturn const_char_ptr_cast(memchr(ptr, '\\n', size));\n }\n \n-static inline const char *PreviousNewline(const char *ptr) {\n-\tfor (ptr--; true; ptr--) {\n-\t\tconst auto &c = *ptr;\n-\t\tif (c == '\\n') {\n+static inline const char *PreviousNewline(const char *ptr, const idx_t size) {\n+\tconst auto end = ptr - size;\n+\tfor (ptr--; ptr != end; ptr--) {\n+\t\tif (*ptr == '\\n') {\n \t\t\tbreak;\n \t\t}\n \t}\n@@ -432,13 +431,9 @@ void JSONScanLocalState::ThrowObjectSizeError(const idx_t object_size) {\n \t    bind_data.maximum_object_size, current_reader->GetFileName(), object_size);\n }\n \n-void JSONScanLocalState::ThrowInvalidAtEndError() {\n-\tthrow InvalidInputException(\"Invalid JSON detected at the end of file \\\"%s\\\".\", current_reader->GetFileName());\n-}\n-\n void JSONScanLocalState::TryIncrementFileIndex(JSONScanGlobalState &gstate) const {\n \tif (gstate.file_index < gstate.json_readers.size() &&\n-\t    current_reader.get() == gstate.json_readers[gstate.file_index].get()) {\n+\t    RefersToSameObject(*current_reader, *gstate.json_readers[gstate.file_index])) {\n \t\tgstate.file_index++;\n \t}\n }\n@@ -538,10 +533,10 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \tAllocatedData buffer;\n \n \t// Try to re-use a buffer that was used before\n-\tif (current_reader) {\n-\t\tcurrent_reader->SetBufferLineOrObjectCount(current_buffer_handle->buffer_index, lines_or_objects_in_buffer);\n-\t\tif (current_buffer_handle && --current_buffer_handle->readers == 0) {\n-\t\t\tbuffer = current_reader->RemoveBuffer(current_buffer_handle->buffer_index);\n+\tif (current_reader && current_buffer_handle) {\n+\t\tcurrent_reader->SetBufferLineOrObjectCount(*current_buffer_handle, lines_or_objects_in_buffer);\n+\t\tif (--current_buffer_handle->readers == 0) {\n+\t\t\tbuffer = current_reader->RemoveBuffer(*current_buffer_handle);\n \t\t}\n \t}\n \n@@ -559,39 +554,36 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \n \toptional_idx buffer_index;\n \twhile (true) {\n-\t\t// Now we finish the current reader\n+\t\t// Continue with the current reader\n \t\tif (current_reader) {\n-\t\t\t// If we performed the final read of this reader in the previous iteration, close it now\n-\t\t\tif (is_last) {\n-\t\t\t\tlock_guard<mutex> guard(gstate.lock);\n-\t\t\t\tTryIncrementFileIndex(gstate);\n-\t\t\t\tcurrent_reader->CloseJSONFile();\n-\t\t\t\tcurrent_reader = nullptr;\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\n-\t\t\t// Try to read\n-\t\t\tReadNextBufferInternal(gstate, buffer_index);\n-\t\t\tif (buffer_index.GetIndex() == 0 && current_reader->GetFormat() == JSONFormat::ARRAY) {\n-\t\t\t\tSkipOverArrayStart();\n+\t\t\t// Try to read (if we were not the last read in the previous iteration)\n+\t\t\tbool file_done = false;\n+\t\t\tbool read_success = ReadNextBufferInternal(gstate, buffer_index, file_done);\n+\t\t\tif (!is_last && read_success) {\n+\t\t\t\t// We read something\n+\t\t\t\tif (buffer_index.GetIndex() == 0 && current_reader->GetFormat() == JSONFormat::ARRAY) {\n+\t\t\t\t\tSkipOverArrayStart();\n+\t\t\t\t}\n \t\t\t}\n \n-\t\t\t// If this is the last read, end the parallel scan now so threads can move on\n-\t\t\tif (is_last && IsParallel(gstate)) {\n+\t\t\tif (file_done) {\n \t\t\t\tlock_guard<mutex> guard(gstate.lock);\n \t\t\t\tTryIncrementFileIndex(gstate);\n+\t\t\t\tcurrent_reader->GetFileHandle().Close();\n \t\t\t}\n \n-\t\t\tif (buffer_size == 0) {\n-\t\t\t\t// We didn't read anything, re-enter the loop\n-\t\t\t\tcontinue;\n+\t\t\tif (read_success) {\n+\t\t\t\tbreak;\n \t\t\t}\n-\t\t\t// We read something!\n-\t\t\tbreak;\n+\n+\t\t\t// We were the last reader last time, or we didn't read anything this time\n+\t\t\tcurrent_reader = nullptr;\n+\t\t\tcurrent_buffer_handle = nullptr;\n+\t\t\tis_last = false;\n \t\t}\n+\t\tD_ASSERT(!current_buffer_handle);\n \n \t\t// If we got here, we don't have a reader (anymore). Try to get one\n-\t\tis_last = false;\n \t\tunique_lock<mutex> guard(gstate.lock);\n \t\tif (gstate.file_index == gstate.json_readers.size()) {\n \t\t\treturn false; // No more files left\n@@ -610,6 +602,14 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \t\t// Open the file if it is not yet open\n \t\tif (!current_reader->IsOpen()) {\n \t\t\tcurrent_reader->OpenJSONFile();\n+\t\t\tif (current_reader->GetFileHandle().FileSize() == 0) {\n+\t\t\t\tcurrent_reader->GetFileHandle().Close();\n+\t\t\t\t// Skip over empty files\n+\t\t\t\tif (gstate.enable_parallel_scans) {\n+\t\t\t\t\tTryIncrementFileIndex(gstate);\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n \t\t}\n \n \t\t// Auto-detect if we haven't yet done this during the bind\n@@ -632,7 +632,6 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \n \t\tbreak;\n \t}\n-\tD_ASSERT(buffer_size != 0); // We should have read something if we got here\n \tD_ASSERT(buffer_index.IsValid());\n \n \tidx_t readers = 1;\n@@ -657,7 +656,10 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \n void JSONScanLocalState::ReadAndAutoDetect(JSONScanGlobalState &gstate, optional_idx &buffer_index) {\n \t// We have to detect the JSON format - hold the gstate lock while we do this\n-\tReadNextBufferInternal(gstate, buffer_index);\n+\tbool file_done = false;\n+\tif (!ReadNextBufferInternal(gstate, buffer_index, file_done)) {\n+\t\treturn;\n+\t}\n \tif (buffer_size == 0) {\n \t\treturn;\n \t}\n@@ -680,17 +682,24 @@ void JSONScanLocalState::ReadAndAutoDetect(JSONScanGlobalState &gstate, optional\n \t}\n }\n \n-void JSONScanLocalState::ReadNextBufferInternal(JSONScanGlobalState &gstate, optional_idx &buffer_index) {\n+bool JSONScanLocalState::ReadNextBufferInternal(JSONScanGlobalState &gstate, optional_idx &buffer_index,\n+                                                bool &file_done) {\n \tif (current_reader->GetFileHandle().CanSeek()) {\n-\t\tReadNextBufferSeek(gstate, buffer_index);\n+\t\tif (!ReadNextBufferSeek(gstate, buffer_index, file_done)) {\n+\t\t\treturn false;\n+\t\t}\n \t} else {\n-\t\tReadNextBufferNoSeek(gstate, buffer_index);\n+\t\tif (!ReadNextBufferNoSeek(gstate, buffer_index, file_done)) {\n+\t\t\treturn false;\n+\t\t}\n \t}\n \n \tbuffer_offset = 0;\n+\n+\treturn true;\n }\n \n-void JSONScanLocalState::ReadNextBufferSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index) {\n+bool JSONScanLocalState::ReadNextBufferSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index, bool &file_done) {\n \tauto &file_handle = current_reader->GetFileHandle();\n \n \tidx_t request_size = gstate.buffer_capacity - prev_buffer_remainder - YYJSON_PADDING_SIZE;\n@@ -699,71 +708,61 @@ void JSONScanLocalState::ReadNextBufferSeek(JSONScanGlobalState &gstate, optiona\n \n \t{\n \t\tlock_guard<mutex> reader_guard(current_reader->lock);\n-\t\tbuffer_index = current_reader->GetBufferIndex();\n-\n-\t\tread_size = file_handle.GetPositionAndSize(read_position, request_size);\n-\t\tis_last = read_size < request_size;\n-\n-\t\tif (!gstate.bind_data.ignore_errors && read_size == 0 && prev_buffer_remainder != 0) {\n-\t\t\tThrowInvalidAtEndError();\n+\t\tif (!file_handle.GetPositionAndSize(read_position, read_size, request_size)) {\n+\t\t\treturn false; // We weren't able to read\n \t\t}\n+\t\tbuffer_index = current_reader->GetBufferIndex();\n+\t\tis_last = read_size == 0;\n \n-\t\tif (read_size != 0 && current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n+\t\tif (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n \t\t\tbatch_index = gstate.batch_index++;\n \t\t}\n \t}\n \tbuffer_size = prev_buffer_remainder + read_size;\n-\tif (buffer_size == 0) {\n-\t\tcurrent_reader->SetBufferLineOrObjectCount(buffer_index.GetIndex(), 0);\n-\t\treturn;\n-\t}\n \n-\tauto &raw_handle = file_handle.GetHandle();\n-\t// For non-on-disk files, we create a handle per thread: this is faster for e.g. S3Filesystem where throttling\n-\t// per tcp connection can occur meaning that using multiple connections is faster.\n-\tif (!raw_handle.OnDiskFile() && raw_handle.CanSeek()) {\n-\t\tif (!thread_local_filehandle || thread_local_filehandle->GetPath() != raw_handle.GetPath()) {\n-\t\t\tthread_local_filehandle =\n-\t\t\t    fs.OpenFile(raw_handle.GetPath(), FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_DIRECT_IO);\n+\tif (read_size != 0) {\n+\t\tauto &raw_handle = file_handle.GetHandle();\n+\t\t// For non-on-disk files, we create a handle per thread: this is faster for e.g. S3Filesystem where throttling\n+\t\t// per tcp connection can occur meaning that using multiple connections is faster.\n+\t\tif (!raw_handle.OnDiskFile() && raw_handle.CanSeek()) {\n+\t\t\tif (!thread_local_filehandle || thread_local_filehandle->GetPath() != raw_handle.GetPath()) {\n+\t\t\t\tthread_local_filehandle =\n+\t\t\t\t    fs.OpenFile(raw_handle.GetPath(), FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_DIRECT_IO);\n+\t\t\t}\n+\t\t} else if (thread_local_filehandle) {\n+\t\t\tthread_local_filehandle = nullptr;\n \t\t}\n-\t} else if (thread_local_filehandle) {\n-\t\tthread_local_filehandle = nullptr;\n \t}\n \n \t// Now read the file lock-free!\n-\tfile_handle.ReadAtPosition(buffer_ptr + prev_buffer_remainder, read_size, read_position,\n+\tfile_handle.ReadAtPosition(buffer_ptr + prev_buffer_remainder, read_size, read_position, file_done,\n \t                           gstate.bind_data.type == JSONScanType::SAMPLE, thread_local_filehandle);\n+\n+\treturn true;\n }\n \n-void JSONScanLocalState::ReadNextBufferNoSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index) {\n+bool JSONScanLocalState::ReadNextBufferNoSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index,\n+                                              bool &file_done) {\n \tidx_t request_size = gstate.buffer_capacity - prev_buffer_remainder - YYJSON_PADDING_SIZE;\n \tidx_t read_size;\n+\n \t{\n \t\tlock_guard<mutex> reader_guard(current_reader->lock);\n-\t\tbuffer_index = current_reader->GetBufferIndex();\n-\n-\t\tif (current_reader->HasFileHandle() && current_reader->IsOpen()) {\n-\t\t\tread_size = current_reader->GetFileHandle().Read(buffer_ptr + prev_buffer_remainder, request_size,\n-\t\t\t                                                 gstate.bind_data.type == JSONScanType::SAMPLE);\n-\t\t\tis_last = read_size < request_size;\n-\t\t} else {\n-\t\t\tread_size = 0;\n-\t\t\tis_last = true;\n-\t\t}\n-\n-\t\tif (!gstate.bind_data.ignore_errors && read_size == 0 && prev_buffer_remainder != 0) {\n-\t\t\tThrowInvalidAtEndError();\n+\t\tif (!current_reader->HasFileHandle() || !current_reader->IsOpen() ||\n+\t\t    !current_reader->GetFileHandle().Read(buffer_ptr + prev_buffer_remainder, read_size, request_size,\n+\t\t                                          file_done, gstate.bind_data.type == JSONScanType::SAMPLE)) {\n+\t\t\treturn false; // Couldn't read anything\n \t\t}\n+\t\tbuffer_index = current_reader->GetBufferIndex();\n+\t\tis_last = read_size == 0;\n \n-\t\tif (read_size != 0 && current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n+\t\tif (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n \t\t\tbatch_index = gstate.batch_index++;\n \t\t}\n \t}\n \tbuffer_size = prev_buffer_remainder + read_size;\n-\tif (buffer_size == 0) {\n-\t\tcurrent_reader->SetBufferLineOrObjectCount(buffer_index.GetIndex(), 0);\n-\t\treturn;\n-\t}\n+\n+\treturn true;\n }\n \n void JSONScanLocalState::SkipOverArrayStart() {\n@@ -795,7 +794,7 @@ void JSONScanLocalState::SkipOverArrayStart() {\n \t}\n }\n \n-void JSONScanLocalState::ReconstructFirstObject() {\n+bool JSONScanLocalState::ReconstructFirstObject() {\n \tD_ASSERT(current_buffer_handle->buffer_index != 0);\n \tD_ASSERT(current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED);\n \n@@ -807,37 +806,48 @@ void JSONScanLocalState::ReconstructFirstObject() {\n \n \t// First we find the newline in the previous block\n \tauto prev_buffer_ptr = char_ptr_cast(previous_buffer_handle->buffer.get()) + previous_buffer_handle->buffer_size;\n-\tauto part1_ptr = PreviousNewline(prev_buffer_ptr);\n+\tauto part1_ptr = PreviousNewline(prev_buffer_ptr, previous_buffer_handle->buffer_size);\n \tauto part1_size = prev_buffer_ptr - part1_ptr;\n \n \t// Now copy the data to our reconstruct buffer\n \tconst auto reconstruct_ptr = reconstruct_buffer.get();\n \tmemcpy(reconstruct_ptr, part1_ptr, part1_size);\n-\t// Now find the newline in the current block\n-\tauto line_end = NextNewline(buffer_ptr, buffer_size);\n-\tif (line_end == nullptr) {\n-\t\tThrowObjectSizeError(buffer_size - buffer_offset);\n-\t} else {\n-\t\tline_end++;\n+\n+\t// We copied the object, so we are no longer reading the previous buffer\n+\tif (--previous_buffer_handle->readers == 0) {\n+\t\tcurrent_reader->RemoveBuffer(*previous_buffer_handle);\n \t}\n-\tidx_t part2_size = line_end - buffer_ptr;\n \n-\tidx_t line_size = part1_size + part2_size;\n-\tif (line_size > bind_data.maximum_object_size) {\n-\t\tThrowObjectSizeError(line_size);\n+\tif (part1_size == 1) {\n+\t\t// Just a newline\n+\t\treturn false;\n \t}\n \n-\t// And copy the remainder of the line to the reconstruct buffer\n-\tmemcpy(reconstruct_ptr + part1_size, buffer_ptr, part2_size);\n-\tmemset(reconstruct_ptr + line_size, 0, YYJSON_PADDING_SIZE);\n-\tbuffer_offset += part2_size;\n+\tidx_t line_size = part1_size;\n+\tif (buffer_size != 0) {\n+\t\t// Now find the newline in the current block\n+\t\tauto line_end = NextNewline(buffer_ptr, buffer_size);\n+\t\tif (line_end == nullptr) {\n+\t\t\tThrowObjectSizeError(buffer_size - buffer_offset);\n+\t\t} else {\n+\t\t\tline_end++;\n+\t\t}\n+\t\tidx_t part2_size = line_end - buffer_ptr;\n+\n+\t\tline_size += part2_size;\n+\t\tif (line_size > bind_data.maximum_object_size) {\n+\t\t\tThrowObjectSizeError(line_size);\n+\t\t}\n \n-\t// We copied the object, so we are no longer reading the previous buffer\n-\tif (--previous_buffer_handle->readers == 0) {\n-\t\tcurrent_reader->RemoveBuffer(current_buffer_handle->buffer_index - 1);\n+\t\t// And copy the remainder of the line to the reconstruct buffer\n+\t\tmemcpy(reconstruct_ptr + part1_size, buffer_ptr, part2_size);\n+\t\tmemset(reconstruct_ptr + line_size, 0, YYJSON_PADDING_SIZE);\n+\t\tbuffer_offset += part2_size;\n \t}\n \n \tParseJSON(char_ptr_cast(reconstruct_ptr), line_size, line_size);\n+\n+\treturn true;\n }\n \n void JSONScanLocalState::ParseNextChunk() {\n@@ -858,7 +868,7 @@ void JSONScanLocalState::ParseNextChunk() {\n \t\t\tif (!is_last) {\n \t\t\t\t// Last bit of data belongs to the next batch\n \t\t\t\tif (format != JSONFormat::NEWLINE_DELIMITED) {\n-\t\t\t\t\tif (scan_count == 0) {\n+\t\t\t\t\tif (remaining > bind_data.maximum_object_size) {\n \t\t\t\t\t\tThrowObjectSizeError(remaining);\n \t\t\t\t\t}\n \t\t\t\t\tmemcpy(reconstruct_buffer.get(), json_start, remaining);\ndiff --git a/extension/json/serialize_json.cpp b/extension/json/serialize_json.cpp\nindex 972e72878203..c45ffc2f51ae 100644\n--- a/extension/json/serialize_json.cpp\n+++ b/extension/json/serialize_json.cpp\n@@ -43,6 +43,7 @@ void JSONScanData::Serialize(Serializer &serializer) const {\n \tserializer.WritePropertyWithDefault<string>(112, \"timestamp_format\", GetTimestampFormat());\n \tserializer.WritePropertyWithDefault<double>(113, \"field_appearance_threshold\", field_appearance_threshold, 0.1);\n \tserializer.WritePropertyWithDefault<idx_t>(114, \"maximum_sample_files\", maximum_sample_files, 32);\n+\tserializer.WritePropertyWithDefault<bool>(115, \"convert_strings_to_integers\", convert_strings_to_integers, false);\n }\n \n unique_ptr<JSONScanData> JSONScanData::Deserialize(Deserializer &deserializer) {\n@@ -72,6 +73,7 @@ unique_ptr<JSONScanData> JSONScanData::Deserialize(Deserializer &deserializer) {\n \tresult->names = std::move(names);\n \tdeserializer.ReadPropertyWithDefault<double>(113, \"field_appearance_threshold\", result->field_appearance_threshold, 0.1);\n \tdeserializer.ReadPropertyWithDefault<idx_t>(114, \"maximum_sample_files\", result->maximum_sample_files, 32);\n+\tdeserializer.ReadPropertyWithDefault<bool>(115, \"convert_strings_to_integers\", result->convert_strings_to_integers, false);\n \treturn result;\n }\n \ndiff --git a/scripts/generate_serialization.py b/scripts/generate_serialization.py\nindex 4080bbd48b52..8d5a0fc60e6a 100644\n--- a/scripts/generate_serialization.py\n+++ b/scripts/generate_serialization.py\n@@ -141,11 +141,15 @@ def replace_pointer(type):\n     return re.sub('([a-zA-Z0-9]+)[*]', 'unique_ptr<\\\\1>', type)\n \n \n+def get_default_argument(default_value):\n+    return f'{default_value}'.lower() if type(default_value) == bool else f'{default_value}'\n+\n+\n def get_serialize_element(\n     property_name, property_id, property_key, property_type, has_default, default_value, is_deleted, pointer_type\n ):\n     assignment = '.' if pointer_type == 'none' else '->'\n-    default_argument = '' if default_value is None else f', {default_value}'\n+    default_argument = '' if default_value is None else f', {get_default_argument(default_value)}'\n     template = serialize_element\n     if is_deleted:\n         template = \"\\t/* [Deleted] (${PROPERTY_TYPE}) \\\"${PROPERTY_NAME}\\\" */\\n\"\n@@ -174,7 +178,7 @@ def get_deserialize_element_template(\n ):\n     # read_method = 'ReadProperty'\n     assignment = '.' if pointer_type == 'none' else '->'\n-    default_argument = '' if default_value is None else f', {default_value}'\n+    default_argument = '' if default_value is None else f', {get_default_argument(default_value)}'\n     if is_deleted:\n         template = template.replace(', result${ASSIGNMENT}${PROPERTY_NAME}', '').replace(\n             'ReadProperty', 'ReadDeletedProperty'\n",
  "test_patch": "diff --git a/test/sql/json/table/read_json_auto.test_slow b/test/sql/json/table/read_json_auto.test_slow\nindex f63a7050d010..09c0a8063f0c 100644\n--- a/test/sql/json/table/read_json_auto.test_slow\n+++ b/test/sql/json/table/read_json_auto.test_slow\n@@ -186,7 +186,7 @@ Binder Error: read_json_auto \"field_appearance_threshold\" parameter must be betw\n \n # inconsistent schema's - if we only sample 1 row, we get an error, because we only see a NULL value for the 2nd column\n statement error\n-select * from read_json_auto('data/json/inconsistent_schemas.ndjson', sample_size=1)\n+select * from read_json_auto('data/json/inconsistent_schemas.ndjson', sample_size=1, convert_strings_to_integers=true)\n ----\n Invalid Input Error: JSON transform error in file \"data/json/inconsistent_schemas.ndjson\", in line 3\n \n@@ -200,12 +200,13 @@ select * from read_json_auto('data/json/inconsistent_schemas.ndjson', sample_siz\n 4\tBroadcast News\n 5\tRaising Arizona\n \n-# we can also find bigint in strings (happens a lot in JSON for some reason ...\n+# we can also find bigint in strings (happens a lot in JSON for some reason ...)\n statement ok\n copy (select * from (values ('{\"id\": \"26941143801\"}'), ('{\"id\": \"26941143807\"}'))) to '__TEST_DIR__/my_file.json' (format csv, quote '', header 0)\n \n+# but only if we set the parameter to true\n query T\n-select typeof(id) from '__TEST_DIR__/my_file.json'\n+select typeof(id) from read_json('__TEST_DIR__/my_file.json', convert_strings_to_integers=true)\n ----\n BIGINT\n BIGINT\n",
  "problem_statement": "read_json_auto should not convert String to BigInt\n### What happens?\r\n\r\nWhen reading JSON files with read_json_auto, columns that are defined a String in JSON are converted to BigInt if they only contain number values.\r\n\r\nIMHO this should not happen, as JSON has numeric types.\r\n\r\n### To Reproduce\r\n\r\n```py\r\nimport duckdb\r\nimport tempfile\r\n\r\n\r\ndata = \"\"\"{\"name\": \"John Doe\",\"order_id\": \"30\"}\"\"\"\r\n\r\ntemp_file = tempfile.NamedTemporaryFile()\r\nwith open(temp_file.name, 'w') as file:\r\n    file.write(data)\r\n\r\n# con = duckdb.connect(database=\":memory:\")\r\nduckdb.sql(f\"\"\"\r\nCREATE TABLE orders AS SELECT * FROM read_json_auto('{temp_file.name}', format='newline_delimited');\r\n\"\"\")\r\nprint(duckdb.sql(\"describe orders\"))\r\n```\r\n\r\n`order_id` has the type `BIGINT`, but should be `VARCHAR`.\r\n\r\n### OS:\r\n\r\nmacOS, arm64\r\n\r\n### DuckDB Version:\r\n\r\n0.9.2\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nJochen Christ\r\n\r\n### Affiliation:\r\n\r\nINNOQ\r\n\r\n### Have you tried this on the latest `main` branch?\r\n\r\nI have tested with a main build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "Hi @jochenchrist, I'm the author of the JSON extension.\r\n\r\nI've added this [because people use string values in JSON to encode numbers larger than 2^53 (something to do with JavaScript)](https://stackoverflow.com/a/34989371). I thought the auto-conversion would be convenient. I ran benchmarks on some [GH Archive data](https://www.gharchive.org), and found that the size of the data when inserted into DuckDB was many GB's smaller when converting the integers-encoded-as-strings in that data to `BIGINT` using auto-detection, e.g.:\r\n```sql\r\ncreate table tbl as select * from 'gharchive_data.json.gz';\r\n```\r\nThis is because integers are much more space-efficient, and DuckDB can compress them better.\r\n\r\nHowever, I don't have much experience with JSON in practice, so I'd like some input. Perhaps this is not desirable from a user's point of view. What do you think?\nSure, here some use cases, they might feel odd:\r\n- One might perform conversions in a query, and expect a string / varchar\r\n- Many business numbers, such as order_ids or zip_codes simply are not integers, but strings\r\n- The extension behaves differently with leading zeros (`0123` vs `123`)\r\n- In data quality tests, like soda-core or great expectations schema checking on varchar fails\r\n\r\nWhat do you think about a configuration option, such as:  `convert_strings_to_integers=false` (or similar)\r\n\nI've been looking for a tool to use to help process API responses.  Its not been uncommon for me to encounter APIs that store identity columns as strings, which might currently happen to be all numbers, but explicitly state that such values should always be treated as strings and never converted.  The latest of which is: https://developer.todoist.com/sync/v9/\r\n\r\nI started to use DuckDB for the first time to help with that API  and was really excited to work with it initially as it seemed ideal in terms of schema generation, speed, and flexibility (good work).  But, once I realized the values were being stored as ints, I spent about a day trying to work around the issue.  I could have just specified the columns manually but part of this exercise was to find a tool that did the type/structure detection automatically.\r\n\r\nIMO, since JSON is a typed format, auto-generating a schema type as 'bigint' instead of 'str' when the format is clearly using a string is buggy behavior.  I understand that the whole point of `read_json_auto()` is to do helpful guessing but IMO this is \"wrong\", not helpful.  Strings with only integer characters _are not integers_, no exceptions,  at least for the default behavior IMO.\r\n\r\nI understand the use-case discussed above but I feel like that should be the opt-in special-case.  How many people using duckdb have integers that are that big encoded in strings compared to those who would want their strings to remain strings?  An option to opt into the int conversion behavior when needed, maybe even just for for specific columns, seems better to me. \r\n\r\nThanks for your effort and sharing the results.\r\n\r\n",
  "created_at": "2024-01-22T12:59:00Z"
}