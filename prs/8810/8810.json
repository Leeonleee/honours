{
  "repo": "duckdb/duckdb",
  "pull_number": 8810,
  "instance_id": "duckdb__duckdb-8810",
  "issue_numbers": [
    "8695",
    "8695",
    "8611"
  ],
  "base_commit": "6eddf2c74aa5053a6e87a7f8a07e3d6c5d429d3c",
  "patch": "diff --git a/data/json/filter_keystage.ndjson b/data/json/filter_keystage.ndjson\nnew file mode 100644\nindex 000000000000..aa05090ace7e\n--- /dev/null\n+++ b/data/json/filter_keystage.ndjson\n@@ -0,0 +1,3 @@\n+{\"filter_keystage\":null}\n+{\"filter_keystage\":[\"key-stage-1\"]}\n+{\"filter_keystage\":[\"key-stage-1\",\"key-stage-2\",\"key-stage-3\"]}\ndiff --git a/extension/json/buffered_json_reader.cpp b/extension/json/buffered_json_reader.cpp\nindex a21ba941ac64..946134556162 100644\n--- a/extension/json/buffered_json_reader.cpp\n+++ b/extension/json/buffered_json_reader.cpp\n@@ -2,9 +2,10 @@\n \n #include \"duckdb/common/field_writer.hpp\"\n #include \"duckdb/common/file_opener.hpp\"\n-#include \"duckdb/common/printer.hpp\"\n-#include \"duckdb/common/serializer/format_serializer.hpp\"\n #include \"duckdb/common/serializer/format_deserializer.hpp\"\n+#include \"duckdb/common/serializer/format_serializer.hpp\"\n+\n+#include <utility>\n \n namespace duckdb {\n \n@@ -37,11 +38,24 @@ bool JSONFileHandle::IsOpen() const {\n }\n \n void JSONFileHandle::Close() {\n-\tif (file_handle) {\n+\tif (IsOpen()) {\n \t\tfile_handle->Close();\n \t\tfile_handle = nullptr;\n \t}\n-\tcached_buffers.clear();\n+}\n+\n+void JSONFileHandle::Reset() {\n+\tD_ASSERT(RequestedReadsComplete());\n+\tread_position = 0;\n+\trequested_reads = 0;\n+\tactual_reads = 0;\n+\tif (IsOpen() && plain_file_source) {\n+\t\tfile_handle->Reset();\n+\t}\n+}\n+\n+bool JSONFileHandle::RequestedReadsComplete() {\n+\treturn requested_reads == actual_reads;\n }\n \n idx_t JSONFileHandle::FileSize() const {\n@@ -56,12 +70,9 @@ bool JSONFileHandle::CanSeek() const {\n \treturn can_seek;\n }\n \n-void JSONFileHandle::Seek(idx_t position) {\n-\tfile_handle->Seek(position);\n-}\n-\n idx_t JSONFileHandle::GetPositionAndSize(idx_t &position, idx_t requested_size) {\n \tD_ASSERT(requested_size != 0);\n+\n \tposition = read_position;\n \tauto actual_size = MinValue<idx_t>(requested_size, Remaining());\n \tread_position += actual_size;\n@@ -77,15 +88,18 @@ void JSONFileHandle::ReadAtPosition(char *pointer, idx_t size, idx_t position, b\n \tif (plain_file_source) {\n \t\tfile_handle->Read(pointer, size, position);\n \t\tactual_reads++;\n+\n \t\treturn;\n \t}\n \n \tif (sample_run) { // Cache the buffer\n \t\tfile_handle->Read(pointer, size, position);\n \t\tactual_reads++;\n+\n \t\tcached_buffers.emplace_back(allocator.Allocate(size));\n \t\tmemcpy(cached_buffers.back().get(), pointer, size);\n \t\tcached_size += size;\n+\n \t\treturn;\n \t}\n \n@@ -93,6 +107,7 @@ void JSONFileHandle::ReadAtPosition(char *pointer, idx_t size, idx_t position, b\n \t\tReadFromCache(pointer, size, position);\n \t\tactual_reads++;\n \t}\n+\n \tif (size != 0) {\n \t\tfile_handle->Read(pointer, size, position);\n \t\tactual_reads++;\n@@ -128,6 +143,19 @@ idx_t JSONFileHandle::Read(char *pointer, idx_t requested_size, bool sample_run)\n \treturn actual_size;\n }\n \n+idx_t JSONFileHandle::ReadInternal(char *pointer, const idx_t requested_size) {\n+\t// Deal with reading from pipes\n+\tidx_t total_read_size = 0;\n+\twhile (total_read_size < requested_size) {\n+\t\tauto read_size = file_handle->Read(pointer + total_read_size, requested_size - total_read_size);\n+\t\tif (read_size == 0) {\n+\t\t\tbreak;\n+\t\t}\n+\t\ttotal_read_size += read_size;\n+\t}\n+\treturn total_read_size;\n+}\n+\n idx_t JSONFileHandle::ReadFromCache(char *&pointer, idx_t &size, idx_t &position) {\n \tidx_t read_size = 0;\n \tidx_t total_offset = 0;\n@@ -154,35 +182,27 @@ idx_t JSONFileHandle::ReadFromCache(char *&pointer, idx_t &size, idx_t &position\n \treturn read_size;\n }\n \n-idx_t JSONFileHandle::ReadInternal(char *pointer, const idx_t requested_size) {\n-\t// Deal with reading from pipes\n-\tidx_t total_read_size = 0;\n-\twhile (total_read_size < requested_size) {\n-\t\tauto read_size = file_handle->Read(pointer + total_read_size, requested_size - total_read_size);\n-\t\tif (read_size == 0) {\n-\t\t\tbreak;\n-\t\t}\n-\t\ttotal_read_size += read_size;\n-\t}\n-\treturn total_read_size;\n-}\n-\n BufferedJSONReader::BufferedJSONReader(ClientContext &context, BufferedJSONReaderOptions options_p, string file_name_p)\n-    : context(context), options(options_p), file_name(std::move(file_name_p)), buffer_index(0) {\n+    : context(context), options(std::move(options_p)), file_name(std::move(file_name_p)), buffer_index(0),\n+      thrown(false) {\n }\n \n void BufferedJSONReader::OpenJSONFile() {\n-\tD_ASSERT(!IsDone());\n+\tD_ASSERT(!IsOpen());\n \tlock_guard<mutex> guard(lock);\n \tauto &file_system = FileSystem::GetFileSystem(context);\n \tauto regular_file_handle =\n \t    file_system.OpenFile(file_name.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK, options.compression);\n \tfile_handle = make_uniq<JSONFileHandle>(std::move(regular_file_handle), BufferAllocator::Get(context));\n+\tReset();\n }\n \n void BufferedJSONReader::CloseJSONFile() {\n \twhile (true) {\n \t\tlock_guard<mutex> guard(lock);\n+\t\tif (!file_handle->IsOpen()) {\n+\t\t\treturn; // Already closed\n+\t\t}\n \t\tif (file_handle->RequestedReadsComplete()) {\n \t\t\tfile_handle->Close();\n \t\t\tbreak;\n@@ -190,13 +210,22 @@ void BufferedJSONReader::CloseJSONFile() {\n \t}\n }\n \n-bool BufferedJSONReader::IsOpen() const {\n+void BufferedJSONReader::Reset() {\n+\tbuffer_index = 0;\n+\tbuffer_map.clear();\n+\tbuffer_line_or_object_counts.clear();\n+\tif (HasFileHandle()) {\n+\t\tfile_handle->Reset();\n+\t}\n+}\n+\n+bool BufferedJSONReader::HasFileHandle() const {\n \treturn file_handle != nullptr;\n }\n \n-bool BufferedJSONReader::IsDone() const {\n-\tif (file_handle) {\n-\t\treturn !file_handle->IsOpen();\n+bool BufferedJSONReader::IsOpen() const {\n+\tif (HasFileHandle()) {\n+\t\treturn file_handle->IsOpen();\n \t}\n \treturn false;\n }\n@@ -205,10 +234,6 @@ BufferedJSONReaderOptions &BufferedJSONReader::GetOptions() {\n \treturn options;\n }\n \n-const BufferedJSONReaderOptions &BufferedJSONReader::GetOptions() const {\n-\treturn options;\n-}\n-\n JSONFormat BufferedJSONReader::GetFormat() const {\n \treturn options.format;\n }\n@@ -232,6 +257,7 @@ const string &BufferedJSONReader::GetFileName() const {\n }\n \n JSONFileHandle &BufferedJSONReader::GetFileHandle() const {\n+\tD_ASSERT(HasFileHandle());\n \treturn *file_handle;\n }\n \n@@ -240,7 +266,7 @@ void BufferedJSONReader::InsertBuffer(idx_t buffer_idx, unique_ptr<JSONBufferHan\n \tbuffer_map.insert(make_pair(buffer_idx, std::move(buffer)));\n }\n \n-JSONBufferHandle *BufferedJSONReader::GetBuffer(idx_t buffer_idx) {\n+optional_ptr<JSONBufferHandle> BufferedJSONReader::GetBuffer(idx_t buffer_idx) {\n \tlock_guard<mutex> guard(lock);\n \tauto it = buffer_map.find(buffer_idx);\n \treturn it == buffer_map.end() ? nullptr : it->second.get();\n@@ -268,22 +294,28 @@ void BufferedJSONReader::SetBufferLineOrObjectCount(idx_t index, idx_t count) {\n idx_t BufferedJSONReader::GetLineNumber(idx_t buf_index, idx_t line_or_object_in_buf) {\n \tD_ASSERT(options.format != JSONFormat::AUTO_DETECT);\n \twhile (true) {\n-\t\tlock_guard<mutex> guard(lock);\n \t\tidx_t line = line_or_object_in_buf;\n \t\tbool can_throw = true;\n-\t\tfor (idx_t b_idx = 0; b_idx < buf_index; b_idx++) {\n-\t\t\tif (buffer_line_or_object_counts[b_idx] == -1) {\n-\t\t\t\tcan_throw = false;\n-\t\t\t\tbreak;\n-\t\t\t} else {\n-\t\t\t\tline += buffer_line_or_object_counts[b_idx];\n+\t\t{\n+\t\t\tlock_guard<mutex> guard(lock);\n+\t\t\tif (thrown) {\n+\t\t\t\treturn DConstants::INVALID_INDEX;\n+\t\t\t}\n+\t\t\tfor (idx_t b_idx = 0; b_idx < buf_index; b_idx++) {\n+\t\t\t\tif (buffer_line_or_object_counts[b_idx] == -1) {\n+\t\t\t\t\tcan_throw = false;\n+\t\t\t\t\tbreak;\n+\t\t\t\t} else {\n+\t\t\t\t\tline += buffer_line_or_object_counts[b_idx];\n+\t\t\t\t\tthrown = true;\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n-\t\tif (!can_throw) {\n-\t\t\tcontinue;\n+\t\tif (can_throw) {\n+\t\t\t// SQL uses 1-based indexing so I guess we will do that in our exception here as well\n+\t\t\treturn line + 1;\n \t\t}\n-\t\t// SQL uses 1-based indexing so I guess we will do that in our exception here as well\n-\t\treturn line + 1;\n+\t\tTaskScheduler::YieldThread();\n \t}\n }\n \n@@ -304,41 +336,11 @@ void BufferedJSONReader::ThrowTransformError(idx_t buf_index, idx_t line_or_obje\n }\n \n double BufferedJSONReader::GetProgress() const {\n-\tif (IsOpen()) {\n+\tif (HasFileHandle()) {\n \t\treturn 100.0 - 100.0 * double(file_handle->Remaining()) / double(file_handle->FileSize());\n \t} else {\n \t\treturn 0;\n \t}\n }\n \n-void BufferedJSONReader::Reset() {\n-\tbuffer_index = 0;\n-\tbuffer_map.clear();\n-\tbuffer_line_or_object_counts.clear();\n-\n-\tif (!file_handle) {\n-\t\treturn;\n-\t}\n-\n-\tif (file_handle->CanSeek()) {\n-\t\tfile_handle->Seek(0);\n-\t} else {\n-\t\tfile_handle->Reset();\n-\t}\n-\tfile_handle->Reset();\n-}\n-\n-void JSONFileHandle::Reset() {\n-\tread_position = 0;\n-\trequested_reads = 0;\n-\tactual_reads = 0;\n-\tif (plain_file_source) {\n-\t\tfile_handle->Reset();\n-\t}\n-}\n-\n-bool JSONFileHandle::RequestedReadsComplete() {\n-\treturn requested_reads == actual_reads;\n-}\n-\n } // namespace duckdb\ndiff --git a/extension/json/include/buffered_json_reader.hpp b/extension/json/include/buffered_json_reader.hpp\nindex 0ad3350a3ef5..e1f6c60e5f62 100644\n--- a/extension/json/include/buffered_json_reader.hpp\n+++ b/extension/json/include/buffered_json_reader.hpp\n@@ -9,13 +9,13 @@\n #pragma once\n \n #include \"duckdb/common/atomic.hpp\"\n+#include \"duckdb/common/enum_util.hpp\"\n #include \"duckdb/common/enums/file_compression_type.hpp\"\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/common/multi_file_reader.hpp\"\n #include \"duckdb/common/mutex.hpp\"\n #include \"json_common.hpp\"\n #include \"json_enums.hpp\"\n-#include \"duckdb/common/enum_util.hpp\"\n \n namespace duckdb {\n \n@@ -57,25 +57,25 @@ struct JSONBufferHandle {\n struct JSONFileHandle {\n public:\n \tJSONFileHandle(unique_ptr<FileHandle> file_handle, Allocator &allocator);\n+\n \tbool IsOpen() const;\n \tvoid Close();\n \n+\tvoid Reset();\n+\tbool RequestedReadsComplete();\n+\n \tidx_t FileSize() const;\n \tidx_t Remaining() const;\n \n \tbool CanSeek() const;\n-\tvoid Seek(idx_t position);\n \n \tidx_t GetPositionAndSize(idx_t &position, idx_t requested_size);\n \tvoid ReadAtPosition(char *pointer, idx_t size, idx_t position, bool sample_run);\n \tidx_t Read(char *pointer, idx_t requested_size, bool sample_run);\n \n-\tvoid Reset();\n-\tbool RequestedReadsComplete();\n-\n private:\n-\tidx_t ReadFromCache(char *&pointer, idx_t &size, idx_t &position);\n \tidx_t ReadInternal(char *pointer, const idx_t requested_size);\n+\tidx_t ReadFromCache(char *&pointer, idx_t &size, idx_t &position);\n \n private:\n \t//! The JSON file handle\n@@ -101,38 +101,18 @@ class BufferedJSONReader {\n public:\n \tBufferedJSONReader(ClientContext &context, BufferedJSONReaderOptions options, string file_name);\n \n-private:\n-\tClientContext &context;\n-\tBufferedJSONReaderOptions options;\n-\n-\t//! File name\n-\tconst string file_name;\n-\t//! File handle\n-\tunique_ptr<JSONFileHandle> file_handle;\n-\n-\t//! Next buffer index within the file\n-\tidx_t buffer_index;\n-\t//! Mapping from batch index to currently held buffers\n-\tunordered_map<idx_t, unique_ptr<JSONBufferHandle>> buffer_map;\n-\n-\t//! Line count per buffer\n-\tvector<int64_t> buffer_line_or_object_counts;\n-\n-public:\n-\tmutex lock;\n-\tMultiFileReaderData reader_data;\n-\n-public:\n \tvoid OpenJSONFile();\n \tvoid CloseJSONFile();\n+\tvoid Reset();\n+\n+\tbool HasFileHandle() const;\n \tbool IsOpen() const;\n-\tbool IsDone() const;\n \n \tBufferedJSONReaderOptions &GetOptions();\n-\tconst BufferedJSONReaderOptions &GetOptions() const;\n \n \tJSONFormat GetFormat() const;\n \tvoid SetFormat(JSONFormat format);\n+\n \tJSONRecordType GetRecordType() const;\n \tvoid SetRecordType(JSONRecordType type);\n \n@@ -142,7 +122,7 @@ class BufferedJSONReader {\n public:\n \t//! Insert/get/remove buffer (grabs the lock)\n \tvoid InsertBuffer(idx_t buffer_idx, unique_ptr<JSONBufferHandle> &&buffer);\n-\tJSONBufferHandle *GetBuffer(idx_t buffer_idx);\n+\toptional_ptr<JSONBufferHandle> GetBuffer(idx_t buffer_idx);\n \tAllocatedData RemoveBuffer(idx_t buffer_idx);\n \n \t//! Get a new buffer index (must hold the lock)\n@@ -154,11 +134,34 @@ class BufferedJSONReader {\n \t//! Throws a transform error that mentions the file name and line number\n \tvoid ThrowTransformError(idx_t buf_index, idx_t line_or_object_in_buf, const string &error_message);\n \n+\t//! Scan progress\n \tdouble GetProgress() const;\n-\tvoid Reset();\n \n private:\n \tidx_t GetLineNumber(idx_t buf_index, idx_t line_or_object_in_buf);\n+\n+private:\n+\tClientContext &context;\n+\tBufferedJSONReaderOptions options;\n+\n+\t//! File name\n+\tconst string file_name;\n+\t//! File handle\n+\tunique_ptr<JSONFileHandle> file_handle;\n+\n+\t//! Next buffer index within the file\n+\tidx_t buffer_index;\n+\t//! Mapping from batch index to currently held buffers\n+\tunordered_map<idx_t, unique_ptr<JSONBufferHandle>> buffer_map;\n+\n+\t//! Line count per buffer\n+\tvector<int64_t> buffer_line_or_object_counts;\n+\t//! Whether any of the reading threads has thrown an error\n+\tbool thrown;\n+\n+public:\n+\tmutex lock;\n+\tMultiFileReaderData reader_data;\n };\n \n } // namespace duckdb\ndiff --git a/extension/json/include/json_scan.hpp b/extension/json/include/json_scan.hpp\nindex a92ac76b93a7..35e0416cc7b3 100644\n--- a/extension/json/include/json_scan.hpp\n+++ b/extension/json/include/json_scan.hpp\n@@ -182,11 +182,13 @@ struct JSONScanGlobalState {\n \t//! One JSON reader per file\n \tvector<optional_ptr<BufferedJSONReader>> json_readers;\n \t//! Current file/batch index\n-\tidx_t file_index;\n+\tatomic<idx_t> file_index;\n \tatomic<idx_t> batch_index;\n \n \t//! Current number of threads active\n \tidx_t system_threads;\n+\t//! Whether we enable parallel scans (only if less files than threads)\n+\tbool enable_parallel_scans;\n };\n \n struct JSONScanLocalState {\n@@ -219,19 +221,20 @@ struct JSONScanLocalState {\n \n private:\n \tbool ReadNextBuffer(JSONScanGlobalState &gstate);\n-\tvoid ReadNextBufferInternal(JSONScanGlobalState &gstate, idx_t &buffer_index);\n-\tvoid ReadNextBufferSeek(JSONScanGlobalState &gstate, idx_t &buffer_index);\n-\tvoid ReadNextBufferNoSeek(JSONScanGlobalState &gstate, idx_t &buffer_index);\n+\tvoid ReadNextBufferInternal(JSONScanGlobalState &gstate, optional_idx &buffer_index);\n+\tvoid ReadNextBufferSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index);\n+\tvoid ReadNextBufferNoSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index);\n \tvoid SkipOverArrayStart();\n \n-\tbool ReadAndAutoDetect(JSONScanGlobalState &gstate, idx_t &buffer_index, const bool already_incremented_file_idx);\n-\tvoid ReconstructFirstObject(JSONScanGlobalState &gstate);\n+\tvoid ReadAndAutoDetect(JSONScanGlobalState &gstate, optional_idx &buffer_index);\n+\tvoid ReconstructFirstObject();\n \tvoid ParseNextChunk();\n \n \tvoid ParseJSON(char *const json_start, const idx_t json_size, const idx_t remaining);\n \tvoid ThrowObjectSizeError(const idx_t object_size);\n \tvoid ThrowInvalidAtEndError();\n \n+\tvoid TryIncrementFileIndex(JSONScanGlobalState &gstate) const;\n \tbool IsParallel(JSONScanGlobalState &gstate) const;\n \n private:\ndiff --git a/extension/json/json_scan.cpp b/extension/json/json_scan.cpp\nindex 0545a8ad4818..510c077ae6d7 100644\n--- a/extension/json/json_scan.cpp\n+++ b/extension/json/json_scan.cpp\n@@ -2,11 +2,11 @@\n \n #include \"duckdb/common/enum_util.hpp\"\n #include \"duckdb/common/multi_file_reader.hpp\"\n+#include \"duckdb/common/serializer/format_deserializer.hpp\"\n+#include \"duckdb/common/serializer/format_serializer.hpp\"\n #include \"duckdb/main/extension_helper.hpp\"\n #include \"duckdb/parallel/task_scheduler.hpp\"\n #include \"duckdb/storage/buffer_manager.hpp\"\n-#include \"duckdb/common/serializer/format_serializer.hpp\"\n-#include \"duckdb/common/serializer/format_deserializer.hpp\"\n \n namespace duckdb {\n \n@@ -39,16 +39,16 @@ void JSONScanData::Bind(ClientContext &context, TableFunctionBindInput &input) {\n \t\t\tmaximum_object_size = MaxValue<idx_t>(UIntegerValue::Get(kv.second), maximum_object_size);\n \t\t} else if (loption == \"format\") {\n \t\t\tauto arg = StringUtil::Lower(StringValue::Get(kv.second));\n-\t\t\tstatic const auto format_options =\n+\t\t\tstatic const auto FORMAT_OPTIONS =\n \t\t\t    case_insensitive_map_t<JSONFormat> {{\"auto\", JSONFormat::AUTO_DETECT},\n \t\t\t                                        {\"unstructured\", JSONFormat::UNSTRUCTURED},\n \t\t\t                                        {\"newline_delimited\", JSONFormat::NEWLINE_DELIMITED},\n \t\t\t                                        {\"nd\", JSONFormat::NEWLINE_DELIMITED},\n \t\t\t                                        {\"array\", JSONFormat::ARRAY}};\n-\t\t\tauto lookup = format_options.find(arg);\n-\t\t\tif (lookup == format_options.end()) {\n+\t\t\tauto lookup = FORMAT_OPTIONS.find(arg);\n+\t\t\tif (lookup == FORMAT_OPTIONS.end()) {\n \t\t\t\tvector<string> valid_options;\n-\t\t\t\tfor (auto &pair : format_options) {\n+\t\t\t\tfor (auto &pair : FORMAT_OPTIONS) {\n \t\t\t\t\tvalid_options.push_back(StringUtil::Format(\"'%s'\", pair.first));\n \t\t\t\t}\n \t\t\t\tthrow BinderException(\"format must be one of [%s], not '%s'\", StringUtil::Join(valid_options, \", \"),\n@@ -198,7 +198,8 @@ JSONScanGlobalState::JSONScanGlobalState(ClientContext &context, const JSONScanD\n     : bind_data(bind_data_p), transform_options(bind_data.transform_options),\n       allocator(BufferManager::GetBufferManager(context).GetBufferAllocator()),\n       buffer_capacity(bind_data.maximum_object_size * 2), file_index(0), batch_index(0),\n-      system_threads(TaskScheduler::GetScheduler(context).NumberOfThreads()) {\n+      system_threads(TaskScheduler::GetScheduler(context).NumberOfThreads()),\n+      enable_parallel_scans(bind_data.files.size() < system_threads) {\n }\n \n JSONScanLocalState::JSONScanLocalState(ClientContext &context, JSONScanGlobalState &gstate)\n@@ -275,7 +276,7 @@ idx_t JSONGlobalTableFunctionState::MaxThreads() const {\n \t\treturn state.system_threads;\n \t}\n \n-\tif (!state.json_readers.empty() && state.json_readers[0]->IsOpen()) {\n+\tif (!state.json_readers.empty() && state.json_readers[0]->HasFileHandle()) {\n \t\tauto &reader = *state.json_readers[0];\n \t\tif (reader.GetFormat() == JSONFormat::NEWLINE_DELIMITED) { // Auto-detected NDJSON\n \t\t\treturn state.system_threads;\n@@ -291,7 +292,7 @@ JSONLocalTableFunctionState::JSONLocalTableFunctionState(ClientContext &context,\n }\n \n unique_ptr<LocalTableFunctionState> JSONLocalTableFunctionState::Init(ExecutionContext &context,\n-                                                                      TableFunctionInitInput &input,\n+                                                                      TableFunctionInitInput &,\n                                                                       GlobalTableFunctionState *global_state) {\n \tauto &gstate = global_state->Cast<JSONGlobalTableFunctionState>();\n \tauto result = make_uniq<JSONLocalTableFunctionState>(context.client, gstate.state);\n@@ -318,19 +319,24 @@ static inline void SkipWhitespace(const char *buffer_ptr, idx_t &buffer_offset,\n \n idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {\n \tallocator.Reset();\n-\n \tscan_count = 0;\n-\tif (buffer_offset == buffer_size) {\n-\t\tif (!ReadNextBuffer(gstate)) {\n-\t\t\treturn scan_count;\n-\t\t}\n-\t\tD_ASSERT(buffer_size != 0);\n-\t\tif (current_buffer_handle->buffer_index != 0 && current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n-\t\t\tReconstructFirstObject(gstate);\n-\t\t\tscan_count++;\n+\n+\t// We have to wrap this in a loop otherwise we stop scanning too early when there's an empty JSON file\n+\twhile (scan_count == 0) {\n+\t\tif (buffer_offset == buffer_size) {\n+\t\t\tif (!ReadNextBuffer(gstate)) {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tD_ASSERT(buffer_size != 0);\n+\t\t\tif (current_buffer_handle->buffer_index != 0 &&\n+\t\t\t    current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n+\t\t\t\tReconstructFirstObject();\n+\t\t\t\tscan_count++;\n+\t\t\t}\n \t\t}\n+\n+\t\tParseNextChunk();\n \t}\n-\tParseNextChunk();\n \n \treturn scan_count;\n }\n@@ -349,7 +355,7 @@ static inline const char *PreviousNewline(const char *ptr) {\n \treturn ptr;\n }\n \n-static inline const char *NextJSONDefault(const char *ptr, const idx_t size, const char *const end) {\n+static inline const char *NextJSONDefault(const char *ptr, const char *const end) {\n \tidx_t parents = 0;\n \twhile (ptr != end) {\n \t\tswitch (*ptr++) {\n@@ -393,7 +399,7 @@ static inline const char *NextJSON(const char *ptr, const idx_t size) {\n \tcase '{':\n \tcase '[':\n \tcase '\"':\n-\t\tptr = NextJSONDefault(ptr, size, end);\n+\t\tptr = NextJSONDefault(ptr, end);\n \t\tbreak;\n \tdefault:\n \t\t// Special case: JSON array containing JSON without clear \"parents\", i.e., not obj/arr/str\n@@ -482,18 +488,21 @@ void JSONScanLocalState::ThrowInvalidAtEndError() {\n \tthrow InvalidInputException(\"Invalid JSON detected at the end of file \\\"%s\\\".\", current_reader->GetFileName());\n }\n \n-bool JSONScanLocalState::IsParallel(JSONScanGlobalState &gstate) const {\n-\tif (bind_data.files.size() >= gstate.system_threads) {\n-\t\t// More files than threads, just parallelize over the files\n-\t\treturn false;\n+void JSONScanLocalState::TryIncrementFileIndex(JSONScanGlobalState &gstate) const {\n+\tlock_guard<mutex> guard(gstate.lock);\n+\tif (gstate.file_index < gstate.json_readers.size() &&\n+\t    current_reader.get() == gstate.json_readers[gstate.file_index].get()) {\n+\t\tgstate.file_index++;\n \t}\n+}\n \n-\tif (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n-\t\t// NDJSON can be read in parallel\n-\t\treturn true;\n+bool JSONScanLocalState::IsParallel(JSONScanGlobalState &gstate) const {\n+\tif (bind_data.files.size() >= gstate.system_threads) {\n+\t\treturn false; // More files than threads, just parallelize over the files\n \t}\n \n-\treturn false;\n+\t// NDJSON can be read in parallel\n+\treturn current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED;\n }\n \n static pair<JSONFormat, JSONRecordType> DetectFormatAndRecordType(char *const buffer_ptr, const idx_t buffer_size,\n@@ -578,104 +587,107 @@ static pair<JSONFormat, JSONRecordType> DetectFormatAndRecordType(char *const bu\n }\n \n bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n+\t// First we make sure we have a buffer to read into\n \tAllocatedData buffer;\n+\n+\t// Try to re-use a buffer that was used before\n \tif (current_reader) {\n-\t\t// Keep track of this for accurate errors\n \t\tcurrent_reader->SetBufferLineOrObjectCount(current_buffer_handle->buffer_index, lines_or_objects_in_buffer);\n-\n-\t\t// Try to re-use existing buffer\n \t\tif (current_buffer_handle && --current_buffer_handle->readers == 0) {\n \t\t\tbuffer = current_reader->RemoveBuffer(current_buffer_handle->buffer_index);\n-\t\t} else {\n-\t\t\tbuffer = gstate.allocator.Allocate(gstate.buffer_capacity);\n \t\t}\n+\t}\n \n-\t\tif (!is_last) {\n-\t\t\tif (current_reader->GetFormat() != JSONFormat::NEWLINE_DELIMITED) {\n-\t\t\t\tmemcpy(buffer.get(), reconstruct_buffer.get(),\n-\t\t\t\t       prev_buffer_remainder); // Copy last bit of previous buffer\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (gstate.bind_data.type != JSONScanType::SAMPLE) {\n-\t\t\t\tcurrent_reader->CloseJSONFile(); // Close files that are done if we're not sampling\n-\t\t\t}\n-\t\t\tcurrent_reader = nullptr;\n-\t\t}\n-\t} else {\n+\t// If we cannot re-use a buffer we create a new one\n+\tif (!buffer.IsSet()) {\n \t\tbuffer = gstate.allocator.Allocate(gstate.buffer_capacity);\n \t}\n+\n \tbuffer_ptr = char_ptr_cast(buffer.get());\n \n-\tidx_t buffer_index;\n+\t// Copy last bit of previous buffer\n+\tif (current_reader && current_reader->GetFormat() != JSONFormat::NEWLINE_DELIMITED && !is_last) {\n+\t\tmemcpy(buffer_ptr, reconstruct_buffer.get(), prev_buffer_remainder);\n+\t}\n+\n+\toptional_idx buffer_index;\n \twhile (true) {\n+\t\t// Now we finish the current reader\n \t\tif (current_reader) {\n-\t\t\tReadNextBufferInternal(gstate, buffer_index);\n-\t\t\tif (buffer_size == 0) {\n-\t\t\t\tif (is_last && gstate.bind_data.type != JSONScanType::SAMPLE) {\n+\t\t\t// If we performed the final read of this reader in the previous iteration, close it now\n+\t\t\tif (is_last) {\n+\t\t\t\tif (gstate.bind_data.type != JSONScanType::SAMPLE) {\n+\t\t\t\t\tTryIncrementFileIndex(gstate);\n \t\t\t\t\tcurrent_reader->CloseJSONFile();\n \t\t\t\t}\n-\t\t\t\tif (IsParallel(gstate)) {\n-\t\t\t\t\t// If this threads' current reader is still the one at gstate.file_index,\n-\t\t\t\t\t// this thread can end the parallel scan\n-\t\t\t\t\tlock_guard<mutex> guard(gstate.lock);\n-\t\t\t\t\tif (gstate.file_index < gstate.json_readers.size() &&\n-\t\t\t\t\t    current_reader == gstate.json_readers[gstate.file_index].get()) {\n-\t\t\t\t\t\tgstate.file_index++; // End parallel scan\n-\t\t\t\t\t}\n-\t\t\t\t}\n \t\t\t\tcurrent_reader = nullptr;\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// Try to read\n+\t\t\tReadNextBufferInternal(gstate, buffer_index);\n+\n+\t\t\t// If this is the last read, end the parallel scan now so threads can move on\n+\t\t\tif (is_last && IsParallel(gstate)) {\n+\t\t\t\tTryIncrementFileIndex(gstate);\n+\t\t\t}\n+\n+\t\t\tif (buffer_size == 0) {\n+\t\t\t\t// We didn't read anything, re-enter the loop\n+\t\t\t\tcontinue;\n \t\t\t} else {\n-\t\t\t\tbreak; // We read something!\n+\t\t\t\t// We read something!\n+\t\t\t\tbreak;\n \t\t\t}\n \t\t}\n \n-\t\t// This thread needs a new reader\n+\t\t// If we got here, we don't have a reader (anymore). Try to get one\n+\t\tis_last = false;\n \t\t{\n \t\t\tlock_guard<mutex> guard(gstate.lock);\n \t\t\tif (gstate.file_index == gstate.json_readers.size()) {\n \t\t\t\treturn false; // No more files left\n \t\t\t}\n \n-\t\t\t// Try the next reader\n+\t\t\t// Assign the next reader to this thread\n \t\t\tcurrent_reader = gstate.json_readers[gstate.file_index].get();\n-\t\t\tif (current_reader->IsOpen()) {\n-\t\t\t\t// Can only be open from auto detection, so these should be known\n-\t\t\t\tif (!IsParallel(gstate)) {\n-\t\t\t\t\tbatch_index = gstate.batch_index++;\n-\t\t\t\t\tgstate.file_index++;\n-\t\t\t\t}\n-\t\t\t\tcontinue; // Re-enter the loop to start scanning the assigned file\n-\t\t\t}\n \n-\t\t\tcurrent_reader->OpenJSONFile();\n-\t\t\tbatch_index = gstate.batch_index++;\n-\t\t\tif (current_reader->GetFormat() != JSONFormat::AUTO_DETECT) {\n-\t\t\t\tif (!IsParallel(gstate)) {\n-\t\t\t\t\tgstate.file_index++;\n-\t\t\t\t}\n-\t\t\t\tcontinue;\n+\t\t\t// Open the file if it is not yet open\n+\t\t\tif (!current_reader->IsOpen()) {\n+\t\t\t\tcurrent_reader->OpenJSONFile();\n \t\t\t}\n+\t\t\tbatch_index = gstate.batch_index++;\n \n-\t\t\t// If we have less files than threads, we auto-detect within the lock,\n-\t\t\t// so other threads may join a parallel NDJSON scan\n-\t\t\tif (gstate.json_readers.size() < gstate.system_threads) {\n-\t\t\t\tif (ReadAndAutoDetect(gstate, buffer_index, false)) {\n-\t\t\t\t\tcontinue;\n+\t\t\t// Auto-detect format / record type\n+\t\t\tif (gstate.enable_parallel_scans) {\n+\t\t\t\t// Auto-detect within the lock, so threads may join a parallel NDJSON scan\n+\t\t\t\tif (current_reader->GetFormat() == JSONFormat::AUTO_DETECT) {\n+\t\t\t\t\tReadAndAutoDetect(gstate, buffer_index);\n \t\t\t\t}\n-\t\t\t\tbreak;\n+\t\t\t} else {\n+\t\t\t\tgstate.file_index++; // Increment the file index before dropping lock so other threads move on\n \t\t\t}\n+\t\t}\n \n-\t\t\t// Increment the file index within the lock, then read/auto-detect outside of the lock\n-\t\t\tgstate.file_index++;\n+\t\t// If we didn't auto-detect within the lock, do it now\n+\t\tif (current_reader->GetFormat() == JSONFormat::AUTO_DETECT) {\n+\t\t\tReadAndAutoDetect(gstate, buffer_index);\n \t\t}\n \n-\t\t// High amount of files, just do 1 thread per file\n-\t\tif (ReadAndAutoDetect(gstate, buffer_index, true)) {\n+\t\t// If we haven't already, increment the file index if non-parallel scan\n+\t\tif (gstate.enable_parallel_scans && !IsParallel(gstate)) {\n+\t\t\tTryIncrementFileIndex(gstate);\n+\t\t}\n+\n+\t\tif (!buffer_index.IsValid() || buffer_size == 0) {\n+\t\t\t// If we didn't get a buffer index (because not auto-detecting), or the file was empty, just re-enter loop\n \t\t\tcontinue;\n \t\t}\n+\n \t\tbreak;\n \t}\n \tD_ASSERT(buffer_size != 0); // We should have read something if we got here\n+\tD_ASSERT(buffer_index.IsValid());\n \n \tidx_t readers = 1;\n \tif (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n@@ -683,9 +695,10 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \t}\n \n \t// Create an entry and insert it into the map\n-\tauto json_buffer_handle = make_uniq<JSONBufferHandle>(buffer_index, readers, std::move(buffer), buffer_size);\n+\tauto json_buffer_handle =\n+\t    make_uniq<JSONBufferHandle>(buffer_index.GetIndex(), readers, std::move(buffer), buffer_size);\n \tcurrent_buffer_handle = json_buffer_handle.get();\n-\tcurrent_reader->InsertBuffer(buffer_index, std::move(json_buffer_handle));\n+\tcurrent_reader->InsertBuffer(buffer_index.GetIndex(), std::move(json_buffer_handle));\n \n \tprev_buffer_remainder = 0;\n \tlines_or_objects_in_buffer = 0;\n@@ -696,15 +709,11 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \treturn true;\n }\n \n-bool JSONScanLocalState::ReadAndAutoDetect(JSONScanGlobalState &gstate, idx_t &buffer_index,\n-                                           const bool already_incremented_file_idx) {\n+void JSONScanLocalState::ReadAndAutoDetect(JSONScanGlobalState &gstate, optional_idx &buffer_index) {\n \t// We have to detect the JSON format - hold the gstate lock while we do this\n \tReadNextBufferInternal(gstate, buffer_index);\n \tif (buffer_size == 0) {\n-\t\tif (!already_incremented_file_idx) {\n-\t\t\tgstate.file_index++; // Empty file, move to the next one\n-\t\t}\n-\t\treturn true;\n+\t\treturn;\n \t}\n \n \tauto format_and_record_type = DetectFormatAndRecordType(buffer_ptr, buffer_size, allocator.GetYYAlc());\n@@ -721,13 +730,9 @@ bool JSONScanLocalState::ReadAndAutoDetect(JSONScanGlobalState &gstate, idx_t &b\n \t\tthrow InvalidInputException(\"Expected file \\\"%s\\\" to contain records, detected non-record JSON instead.\",\n \t\t                            current_reader->GetFileName());\n \t}\n-\tif (!already_incremented_file_idx && !IsParallel(gstate)) {\n-\t\tgstate.file_index++;\n-\t}\n-\treturn false;\n }\n \n-void JSONScanLocalState::ReadNextBufferInternal(JSONScanGlobalState &gstate, idx_t &buffer_index) {\n+void JSONScanLocalState::ReadNextBufferInternal(JSONScanGlobalState &gstate, optional_idx &buffer_index) {\n \tif (current_reader->GetFileHandle().CanSeek()) {\n \t\tReadNextBufferSeek(gstate, buffer_index);\n \t} else {\n@@ -735,12 +740,12 @@ void JSONScanLocalState::ReadNextBufferInternal(JSONScanGlobalState &gstate, idx\n \t}\n \n \tbuffer_offset = 0;\n-\tif (buffer_index == 0 && current_reader->GetFormat() == JSONFormat::ARRAY) {\n+\tif (buffer_index.GetIndex() == 0 && current_reader->GetFormat() == JSONFormat::ARRAY) {\n \t\tSkipOverArrayStart();\n \t}\n }\n \n-void JSONScanLocalState::ReadNextBufferSeek(JSONScanGlobalState &gstate, idx_t &buffer_index) {\n+void JSONScanLocalState::ReadNextBufferSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index) {\n \tauto &file_handle = current_reader->GetFileHandle();\n \n \tidx_t request_size = gstate.buffer_capacity - prev_buffer_remainder - YYJSON_PADDING_SIZE;\n@@ -758,13 +763,13 @@ void JSONScanLocalState::ReadNextBufferSeek(JSONScanGlobalState &gstate, idx_t &\n \t\t\tThrowInvalidAtEndError();\n \t\t}\n \n-\t\tif (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n+\t\tif (read_size != 0 && current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n \t\t\tbatch_index = gstate.batch_index++;\n \t\t}\n \t}\n \tbuffer_size = prev_buffer_remainder + read_size;\n \tif (buffer_size == 0) {\n-\t\tcurrent_reader->SetBufferLineOrObjectCount(buffer_index, 0);\n+\t\tcurrent_reader->SetBufferLineOrObjectCount(buffer_index.GetIndex(), 0);\n \t\treturn;\n \t}\n \n@@ -773,33 +778,33 @@ void JSONScanLocalState::ReadNextBufferSeek(JSONScanGlobalState &gstate, idx_t &\n \t                           gstate.bind_data.type == JSONScanType::SAMPLE);\n }\n \n-void JSONScanLocalState::ReadNextBufferNoSeek(JSONScanGlobalState &gstate, idx_t &buffer_index) {\n+void JSONScanLocalState::ReadNextBufferNoSeek(JSONScanGlobalState &gstate, optional_idx &buffer_index) {\n \tidx_t request_size = gstate.buffer_capacity - prev_buffer_remainder - YYJSON_PADDING_SIZE;\n \tidx_t read_size;\n \t{\n \t\tlock_guard<mutex> reader_guard(current_reader->lock);\n \t\tbuffer_index = current_reader->GetBufferIndex();\n \n-\t\tif (current_reader->IsOpen() && !current_reader->IsDone()) {\n+\t\tif (current_reader->HasFileHandle() && current_reader->IsOpen()) {\n \t\t\tread_size = current_reader->GetFileHandle().Read(buffer_ptr + prev_buffer_remainder, request_size,\n \t\t\t                                                 gstate.bind_data.type == JSONScanType::SAMPLE);\n \t\t\tis_last = read_size < request_size;\n \t\t} else {\n \t\t\tread_size = 0;\n-\t\t\tis_last = false;\n+\t\t\tis_last = true;\n \t\t}\n \n \t\tif (!gstate.bind_data.ignore_errors && read_size == 0 && prev_buffer_remainder != 0) {\n \t\t\tThrowInvalidAtEndError();\n \t\t}\n \n-\t\tif (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n+\t\tif (read_size != 0 && current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n \t\t\tbatch_index = gstate.batch_index++;\n \t\t}\n \t}\n \tbuffer_size = prev_buffer_remainder + read_size;\n \tif (buffer_size == 0) {\n-\t\tcurrent_reader->SetBufferLineOrObjectCount(buffer_index, 0);\n+\t\tcurrent_reader->SetBufferLineOrObjectCount(buffer_index.GetIndex(), 0);\n \t\treturn;\n \t}\n }\n@@ -833,7 +838,7 @@ void JSONScanLocalState::SkipOverArrayStart() {\n \t}\n }\n \n-void JSONScanLocalState::ReconstructFirstObject(JSONScanGlobalState &gstate) {\n+void JSONScanLocalState::ReconstructFirstObject() {\n \tD_ASSERT(current_buffer_handle->buffer_index != 0);\n \tD_ASSERT(current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED);\n \n@@ -947,8 +952,7 @@ void JSONScanLocalState::ThrowTransformError(idx_t object_index, const string &e\n \tcurrent_reader->ThrowTransformError(current_buffer_handle->buffer_index, line_or_object_in_buffer, error_message);\n }\n \n-double JSONScan::ScanProgress(ClientContext &context, const FunctionData *bind_data_p,\n-                              const GlobalTableFunctionState *global_state) {\n+double JSONScan::ScanProgress(ClientContext &, const FunctionData *, const GlobalTableFunctionState *global_state) {\n \tauto &gstate = global_state->Cast<JSONGlobalTableFunctionState>().state;\n \tdouble progress = 0;\n \tfor (auto &reader : gstate.json_readers) {\n@@ -957,16 +961,16 @@ double JSONScan::ScanProgress(ClientContext &context, const FunctionData *bind_d\n \treturn progress / double(gstate.json_readers.size());\n }\n \n-idx_t JSONScan::GetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n-                              LocalTableFunctionState *local_state, GlobalTableFunctionState *global_state) {\n+idx_t JSONScan::GetBatchIndex(ClientContext &, const FunctionData *, LocalTableFunctionState *local_state,\n+                              GlobalTableFunctionState *) {\n \tauto &lstate = local_state->Cast<JSONLocalTableFunctionState>();\n \treturn lstate.GetBatchIndex();\n }\n \n-unique_ptr<NodeStatistics> JSONScan::Cardinality(ClientContext &context, const FunctionData *bind_data) {\n+unique_ptr<NodeStatistics> JSONScan::Cardinality(ClientContext &, const FunctionData *bind_data) {\n \tauto &data = bind_data->Cast<JSONScanData>();\n \tidx_t per_file_cardinality;\n-\tif (data.initial_reader && data.initial_reader->IsOpen()) {\n+\tif (data.initial_reader && data.initial_reader->HasFileHandle()) {\n \t\tper_file_cardinality = data.initial_reader->GetFileHandle().FileSize() / data.avg_tuple_size;\n \t} else {\n \t\tper_file_cardinality = 42; // The cardinality of an unknown JSON file is the almighty number 42\n@@ -984,25 +988,24 @@ void JSONScan::ComplexFilterPushdown(ClientContext &context, LogicalGet &get, Fu\n \t}\n }\n \n-void JSONScan::Serialize(FieldWriter &writer, const FunctionData *bind_data_p, const TableFunction &function) {\n+void JSONScan::Serialize(FieldWriter &writer, const FunctionData *bind_data_p, const TableFunction &) {\n \tauto &bind_data = bind_data_p->Cast<JSONScanData>();\n \tbind_data.Serialize(writer);\n }\n \n-unique_ptr<FunctionData> JSONScan::Deserialize(PlanDeserializationState &state, FieldReader &reader,\n-                                               TableFunction &function) {\n+unique_ptr<FunctionData> JSONScan::Deserialize(PlanDeserializationState &state, FieldReader &reader, TableFunction &) {\n \tauto result = make_uniq<JSONScanData>();\n \tresult->Deserialize(state.context, reader);\n \treturn std::move(result);\n }\n \n void JSONScan::FormatSerialize(FormatSerializer &serializer, const optional_ptr<FunctionData> bind_data_p,\n-                               const TableFunction &function) {\n+                               const TableFunction &) {\n \tauto &bind_data = bind_data_p->Cast<JSONScanData>();\n \tserializer.WriteProperty(100, \"scan_data\", &bind_data);\n }\n \n-unique_ptr<FunctionData> JSONScan::FormatDeserialize(FormatDeserializer &deserializer, TableFunction &function) {\n+unique_ptr<FunctionData> JSONScan::FormatDeserialize(FormatDeserializer &deserializer, TableFunction &) {\n \tunique_ptr<JSONScanData> result;\n \tdeserializer.ReadProperty(100, \"scan_data\", result);\n \treturn std::move(result);\ndiff --git a/src/execution/operator/aggregate/physical_window.cpp b/src/execution/operator/aggregate/physical_window.cpp\nindex cba3e5dcc234..18cef83165b5 100644\n--- a/src/execution/operator/aggregate/physical_window.cpp\n+++ b/src/execution/operator/aggregate/physical_window.cpp\n@@ -564,7 +564,7 @@ WindowGlobalSourceState::Task WindowGlobalSourceState::NextTask(idx_t hash_bin)\n \n \t\t//\tIf there is nothing to steal but there are unfinished partitions,\n \t\t//\tyield until any pending builds are done.\n-\t\tTaskScheduler::GetScheduler(context).YieldThread();\n+\t\tTaskScheduler::YieldThread();\n \t}\n \n \treturn Task();\ndiff --git a/src/include/duckdb/common/optional_idx.hpp b/src/include/duckdb/common/optional_idx.hpp\nindex 43e899f564ea..28c618f2ab73 100644\n--- a/src/include/duckdb/common/optional_idx.hpp\n+++ b/src/include/duckdb/common/optional_idx.hpp\n@@ -34,7 +34,7 @@ class optional_idx {\n \tvoid Invalidate() {\n \t\tindex = INVALID_INDEX;\n \t}\n-\tidx_t GetIndex() {\n+\tidx_t GetIndex() const {\n \t\tif (index == INVALID_INDEX) {\n \t\t\tthrow InternalException(\"Attempting to get the index of an optional_idx that is not set\");\n \t\t}\ndiff --git a/src/include/duckdb/parallel/task_scheduler.hpp b/src/include/duckdb/parallel/task_scheduler.hpp\nindex d3171c55ddcc..b54da51ceb2a 100644\n--- a/src/include/duckdb/parallel/task_scheduler.hpp\n+++ b/src/include/duckdb/parallel/task_scheduler.hpp\n@@ -68,7 +68,7 @@ class TaskScheduler {\n \tvoid Signal(idx_t n);\n \n \t//! Yield to other threads\n-\tvoid YieldThread();\n+\tstatic void YieldThread();\n \n \t//! Set the allocator flush threshold\n \tvoid SetAllocatorFlushTreshold(idx_t threshold);\ndiff --git a/src/planner/binder/expression/bind_function_expression.cpp b/src/planner/binder/expression/bind_function_expression.cpp\nindex 01e568b3f7d2..c2a6179ed290 100644\n--- a/src/planner/binder/expression/bind_function_expression.cpp\n+++ b/src/planner/binder/expression/bind_function_expression.cpp\n@@ -69,21 +69,30 @@ BindResult ExpressionBinder::BindExpression(FunctionExpression &function, idx_t\n \t}\n \n \tswitch (func->type) {\n-\tcase CatalogType::SCALAR_FUNCTION_ENTRY:\n+\tcase CatalogType::SCALAR_FUNCTION_ENTRY: {\n \t\t// scalar function\n \n \t\t// check for lambda parameters, ignore ->> operator (JSON extension)\n+\t\tbool try_bind_lambda = false;\n \t\tif (function.function_name != \"->>\") {\n \t\t\tfor (auto &child : function.children) {\n \t\t\t\tif (child->expression_class == ExpressionClass::LAMBDA) {\n-\t\t\t\t\treturn BindLambdaFunction(function, func->Cast<ScalarFunctionCatalogEntry>(), depth);\n+\t\t\t\t\ttry_bind_lambda = true;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n+\t\tif (try_bind_lambda) {\n+\t\t\tauto result = BindLambdaFunction(function, func->Cast<ScalarFunctionCatalogEntry>(), depth);\n+\t\t\tif (!result.HasError()) {\n+\t\t\t\t// Lambda bind successful\n+\t\t\t\treturn result;\n+\t\t\t}\n+\t\t}\n+\n \t\t// other scalar function\n \t\treturn BindFunction(function, func->Cast<ScalarFunctionCatalogEntry>(), depth);\n-\n+\t}\n \tcase CatalogType::MACRO_ENTRY:\n \t\t// macro function\n \t\treturn BindMacro(function, func->Cast<ScalarMacroCatalogEntry>(), depth, expr_ptr);\n@@ -134,7 +143,7 @@ BindResult ExpressionBinder::BindLambdaFunction(FunctionExpression &function, Sc\n \tstring error;\n \n \tif (function.children.size() != 2) {\n-\t\tthrow BinderException(\"Invalid function arguments!\");\n+\t\treturn BindResult(\"Invalid function arguments!\");\n \t}\n \tD_ASSERT(function.children[1]->GetExpressionClass() == ExpressionClass::LAMBDA);\n \n@@ -148,7 +157,7 @@ BindResult ExpressionBinder::BindLambdaFunction(FunctionExpression &function, Sc\n \tauto &list_child = BoundExpression::GetExpression(*function.children[0]);\n \tif (list_child->return_type.id() != LogicalTypeId::LIST && list_child->return_type.id() != LogicalTypeId::SQLNULL &&\n \t    list_child->return_type.id() != LogicalTypeId::UNKNOWN) {\n-\t\tthrow BinderException(\" Invalid LIST argument to \" + function.function_name + \"!\");\n+\t\treturn BindResult(\" Invalid LIST argument to \" + function.function_name + \"!\");\n \t}\n \n \tLogicalType list_child_type = list_child->return_type.id();\n",
  "test_patch": "diff --git a/test/sql/json/issues/issue8695.test b/test/sql/json/issues/issue8695.test\nnew file mode 100644\nindex 000000000000..6a6c67f3015b\n--- /dev/null\n+++ b/test/sql/json/issues/issue8695.test\n@@ -0,0 +1,28 @@\n+# name: test/sql/json/issues/issue8695.test\n+# description: Test issue 8695 - INTERNAL Error: Attempted to dereference unique_ptr that is NULL\n+# group: [issues]\n+\n+require json\n+\n+# these two succeeded\n+statement ok\n+SELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM read_json_auto('data/json/filter_keystage.ndjson');\n+\n+statement ok\n+WITH RECURSIVE nums AS (\n+    SELECT 0 AS n\n+    UNION ALL\n+    SELECT n + 1 FROM nums\n+    WHERE n < 5\n+)\n+SELECT * FROM nums;\n+\n+# but combining them fails\n+statement ok\n+WITH RECURSIVE nums AS (\n+    SELECT 0 AS n\n+    UNION ALL\n+    SELECT n + 1 FROM nums\n+    WHERE n < (SELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM read_json_auto('data/json/filter_keystage.ndjson'))\n+)\n+SELECT * FROM nums;\ndiff --git a/test/sql/json/large_quoted_string_constant.test b/test/sql/json/issues/large_quoted_string_constant.test\nsimilarity index 96%\nrename from test/sql/json/large_quoted_string_constant.test\nrename to test/sql/json/issues/large_quoted_string_constant.test\nindex b1d21279f2e9..38914661647c 100644\n--- a/test/sql/json/large_quoted_string_constant.test\n+++ b/test/sql/json/issues/large_quoted_string_constant.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/large_quoted_string_constant.test\n+# name: test/sql/json/issues/large_quoted_string_constant.test\n # description: Issue #2986: Large string constant with quotes\n-# group: [json]\n+# group: [issues]\n \n statement ok\n CREATE TABLE j2 (id INT, json VARCHAR, src VARCHAR);\ndiff --git a/test/sql/json/test_json_temp_8062.test b/test/sql/json/issues/test_json_temp_8062.test\nsimilarity index 79%\nrename from test/sql/json/test_json_temp_8062.test\nrename to test/sql/json/issues/test_json_temp_8062.test\nindex 89186548d221..1886f0b12e6e 100644\n--- a/test/sql/json/test_json_temp_8062.test\n+++ b/test/sql/json/issues/test_json_temp_8062.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_temp_8062.test\n+# name: test/sql/json/issues/test_json_temp_8062.test\n # description: Test JSON fields in temporary tables for issue 8062\n-# group: [json]\n+# group: [issues]\n \n # Can't do reload tests with temp tables\n require skip_reload\ndiff --git a/test/sql/json/json_nested_casts.test b/test/sql/json/scalar/json_nested_casts.test\nsimilarity index 99%\nrename from test/sql/json/json_nested_casts.test\nrename to test/sql/json/scalar/json_nested_casts.test\nindex 03614edf8c41..f6f12fe3f66e 100644\n--- a/test/sql/json/json_nested_casts.test\n+++ b/test/sql/json/scalar/json_nested_casts.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/json_nested_casts.test\n+# name: test/sql/json/scalar/json_nested_casts.test\n # description: Casts to and from nested types with JSON\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_array_length.test b/test/sql/json/scalar/test_json_array_length.test\nsimilarity index 90%\nrename from test/sql/json/test_json_array_length.test\nrename to test/sql/json/scalar/test_json_array_length.test\nindex c7fe8b40dbee..ea88f4119b23 100644\n--- a/test/sql/json/test_json_array_length.test\n+++ b/test/sql/json/scalar/test_json_array_length.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_array_length.test\n+# name: test/sql/json/scalar/test_json_array_length.test\n # description: Test JSON array length\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_contains.test b/test/sql/json/scalar/test_json_contains.test\nsimilarity index 96%\nrename from test/sql/json/test_json_contains.test\nrename to test/sql/json/scalar/test_json_contains.test\nindex f5b1c3a96969..f51cea3f2c15 100644\n--- a/test/sql/json/test_json_contains.test\n+++ b/test/sql/json/scalar/test_json_contains.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_contains.test\n+# name: test/sql/json/scalar/test_json_contains.test\n # description: Test JSON merge patch\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_create.test b/test/sql/json/scalar/test_json_create.test\nsimilarity index 99%\nrename from test/sql/json/test_json_create.test\nrename to test/sql/json/scalar/test_json_create.test\nindex c599bae255cd..e10ef42235f5 100644\n--- a/test/sql/json/test_json_create.test\n+++ b/test/sql/json/scalar/test_json_create.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_create.test\n+# name: test/sql/json/scalar/test_json_create.test\n # description: Test JSON create functions {json_object(), json_array(), json_quote()}\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_extract.test b/test/sql/json/scalar/test_json_extract.test\nsimilarity index 96%\nrename from test/sql/json/test_json_extract.test\nrename to test/sql/json/scalar/test_json_extract.test\nindex c08f6c6da23b..1e409868fcad 100644\n--- a/test/sql/json/test_json_extract.test\n+++ b/test/sql/json/scalar/test_json_extract.test\n@@ -1,12 +1,18 @@\n-# name: test/sql/json/test_json_extract.test\n+# name: test/sql/json/scalar/test_json_extract.test\n # description: Test JSON extract\n-# group: [json]\n+# group: [scalar]\n \n require json\n \n statement ok\n pragma enable_verification\n \n+# should work within other functions (no conflict with list Lambda functions)\n+query T\n+SELECT json_structure('{\"duck\":\"goose\"}'->'duck');\n+----\n+\"VARCHAR\"\n+\n # should go to our NULL\n query T\n select json_extract('{\"foo\": null}', '$.foo')\ndiff --git a/test/sql/json/test_json_keys.test b/test/sql/json/scalar/test_json_keys.test\nsimilarity index 93%\nrename from test/sql/json/test_json_keys.test\nrename to test/sql/json/scalar/test_json_keys.test\nindex 65d1b5595960..67fcd1345d4e 100644\n--- a/test/sql/json/test_json_keys.test\n+++ b/test/sql/json/scalar/test_json_keys.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_keys.test\n+# name: test/sql/json/scalar/test_json_keys.test\n # description: Test JSON keys function\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_merge_patch.test b/test/sql/json/scalar/test_json_merge_patch.test\nsimilarity index 95%\nrename from test/sql/json/test_json_merge_patch.test\nrename to test/sql/json/scalar/test_json_merge_patch.test\nindex 8a8905f30d1c..d9035297301a 100644\n--- a/test/sql/json/test_json_merge_patch.test\n+++ b/test/sql/json/scalar/test_json_merge_patch.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_merge_patch.test\n+# name: test/sql/json/scalar/test_json_merge_patch.test\n # description: Test JSON merge patch\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_path.test b/test/sql/json/scalar/test_json_path.test\nsimilarity index 98%\nrename from test/sql/json/test_json_path.test\nrename to test/sql/json/scalar/test_json_path.test\nindex dc3a82e5ddd4..850a72265f6e 100644\n--- a/test/sql/json/test_json_path.test\n+++ b/test/sql/json/scalar/test_json_path.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_path.test\n+# name: test/sql/json/scalar/test_json_path.test\n # description: Test JSON extract (path notation)\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_structure.test b/test/sql/json/scalar/test_json_structure.test\nsimilarity index 97%\nrename from test/sql/json/test_json_structure.test\nrename to test/sql/json/scalar/test_json_structure.test\nindex a5b1326727e9..48c8a5a2ba7d 100644\n--- a/test/sql/json/test_json_structure.test\n+++ b/test/sql/json/scalar/test_json_structure.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_structure.test\n+# name: test/sql/json/scalar/test_json_structure.test\n # description: Test json_structure() function\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_transform.test b/test/sql/json/scalar/test_json_transform.test\nsimilarity index 99%\nrename from test/sql/json/test_json_transform.test\nrename to test/sql/json/scalar/test_json_transform.test\nindex 771e8b3fe35f..5af447be65c3 100644\n--- a/test/sql/json/test_json_transform.test\n+++ b/test/sql/json/scalar/test_json_transform.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_transform.test\n+# name: test/sql/json/scalar/test_json_transform.test\n # description: Test json_transform function\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_type.test b/test/sql/json/scalar/test_json_type.test\nsimilarity index 98%\nrename from test/sql/json/test_json_type.test\nrename to test/sql/json/scalar/test_json_type.test\nindex dbca7d609fee..9f8920d745a0 100644\n--- a/test/sql/json/test_json_type.test\n+++ b/test/sql/json/scalar/test_json_type.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_type.test\n+# name: test/sql/json/scalar/test_json_type.test\n # description: Test JSON type\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/test_json_valid.test b/test/sql/json/scalar/test_json_valid.test\nsimilarity index 98%\nrename from test/sql/json/test_json_valid.test\nrename to test/sql/json/scalar/test_json_valid.test\nindex 1dc7c9638709..afa654004961 100644\n--- a/test/sql/json/test_json_valid.test\n+++ b/test/sql/json/scalar/test_json_valid.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/test_json_valid.test\n+# name: test/sql/json/scalar/test_json_valid.test\n # description: Test JSON valid\n-# group: [json]\n+# group: [scalar]\n \n require json\n \ndiff --git a/test/sql/json/json_empty_array.test b/test/sql/json/table/json_empty_array.test\nsimilarity index 93%\nrename from test/sql/json/json_empty_array.test\nrename to test/sql/json/table/json_empty_array.test\nindex f7987fc0149a..d0231dc57671 100644\n--- a/test/sql/json/json_empty_array.test\n+++ b/test/sql/json/table/json_empty_array.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/json_empty_array.test\n+# name: test/sql/json/table/json_empty_array.test\n # description: Read json files with empty arrays\n-# group: [json]\n+# group: [table]\n \n require json\n \ndiff --git a/test/sql/json/json_multi_file_reader.test b/test/sql/json/table/json_multi_file_reader.test\nsimilarity index 98%\nrename from test/sql/json/json_multi_file_reader.test\nrename to test/sql/json/table/json_multi_file_reader.test\nindex e065ebe24e6c..33c8cba8ecba 100644\n--- a/test/sql/json/json_multi_file_reader.test\n+++ b/test/sql/json/table/json_multi_file_reader.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/json_multi_file_reader.test\n+# name: test/sql/json/table/json_multi_file_reader.test\n # description: Test MultiFileReader integration in JSON reader\n-# group: [json]\n+# group: [table]\n \n require json\n \ndiff --git a/test/sql/json/read_json.test b/test/sql/json/table/read_json.test\nsimilarity index 99%\nrename from test/sql/json/read_json.test\nrename to test/sql/json/table/read_json.test\nindex 0c5ff9b6b3ec..00883a05985d 100644\n--- a/test/sql/json/read_json.test\n+++ b/test/sql/json/table/read_json.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/read_json.test\n+# name: test/sql/json/table/read_json.test\n # description: Read json files straight to columnar data\n-# group: [json]\n+# group: [table]\n \n require json\n \ndiff --git a/test/sql/json/read_json_auto.test_slow b/test/sql/json/table/read_json_auto.test_slow\nsimilarity index 98%\nrename from test/sql/json/read_json_auto.test_slow\nrename to test/sql/json/table/read_json_auto.test_slow\nindex 161ea2b194bd..766ffde71a5d 100644\n--- a/test/sql/json/read_json_auto.test_slow\n+++ b/test/sql/json/table/read_json_auto.test_slow\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/read_json_auto.test_slow\n+# name: test/sql/json/table/read_json_auto.test_slow\n # description: Read json files - schema detection\n-# group: [json]\n+# group: [table]\n \n require json\n \ndiff --git a/test/sql/json/read_json_dates.test b/test/sql/json/table/read_json_dates.test\nsimilarity index 98%\nrename from test/sql/json/read_json_dates.test\nrename to test/sql/json/table/read_json_dates.test\nindex 58de523b314f..aab483576f5a 100644\n--- a/test/sql/json/read_json_dates.test\n+++ b/test/sql/json/table/read_json_dates.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/read_json_dates.test\n+# name: test/sql/json/table/read_json_dates.test\n # description: Read json files - date detection\n-# group: [json]\n+# group: [table]\n \n require json\n \ndiff --git a/test/sql/json/read_json_many_files.test_slow b/test/sql/json/table/read_json_many_files.test_slow\nsimilarity index 87%\nrename from test/sql/json/read_json_many_files.test_slow\nrename to test/sql/json/table/read_json_many_files.test_slow\nindex 345a9f67f44f..885b71ab57c6 100644\n--- a/test/sql/json/read_json_many_files.test_slow\n+++ b/test/sql/json/table/read_json_many_files.test_slow\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/read_json_many_files.test_slow\n+# name: test/sql/json/table/read_json_many_files.test_slow\n # description: Read > 1000 json files (issue #6249)\n-# group: [json]\n+# group: [table]\n \n require json\n \ndiff --git a/test/sql/json/read_json_objects.test b/test/sql/json/table/read_json_objects.test\nsimilarity index 99%\nrename from test/sql/json/read_json_objects.test\nrename to test/sql/json/table/read_json_objects.test\nindex 48aa86ce80d1..2d749c9d3490 100644\n--- a/test/sql/json/read_json_objects.test\n+++ b/test/sql/json/table/read_json_objects.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/read_json_objects.test\n+# name: test/sql/json/table/read_json_objects.test\n # description: Read ndjson files\n-# group: [json]\n+# group: [table]\n \n require no_extension_autoloading\n \ndiff --git a/test/sql/json/read_json_union.test b/test/sql/json/table/read_json_union.test\nsimilarity index 94%\nrename from test/sql/json/read_json_union.test\nrename to test/sql/json/table/read_json_union.test\nindex 27d301f6a7dc..52933332d87a 100644\n--- a/test/sql/json/read_json_union.test\n+++ b/test/sql/json/table/read_json_union.test\n@@ -1,6 +1,6 @@\n-# name: test/sql/json/read_json_union.test\n+# name: test/sql/json/table/read_json_union.test\n # description: Read json files with unions straight to columnar data\n-# group: [json]\n+# group: [table]\n \n require json\n \n",
  "problem_statement": "INTERNAL Error: Attempted to dereference unique_ptr that is NULL\nI'm porting some queries from MySQL and ran into a problem with this one on DuckDB.\r\n\r\n### What happens?\r\n\r\nTwo queries work separately, but when combined result in a `unique_ptr` error.\r\n\r\nTested with 0.81 and main on **macOS/aarch64** and **Ubuntu 20.04/x64**.\r\n\r\n### To Reproduce\r\n\r\nFile **filter_keystage.ndjson**\r\n```json\r\n{\"filter_keystage\":null}\r\n{\"filter_keystage\":[\"key-stage-1\"]}\r\n{\"filter_keystage\":[\"key-stage-1\",\"key-stage-2\",\"key-stage-3\"]}\r\n```\r\nQueries\r\n```sql\r\n-- Succeeds\r\nSELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM  read_json_auto('filter_keystage.ndjson');\r\n\r\n-- Succeeds\r\nWITH RECURSIVE nums AS (\r\nSELECT 0 AS n\r\nUNION ALL\r\nSELECT n + 1 FROM nums\r\nWHERE n < 5\r\n)\r\nselect * from nums;\r\n\r\n-- Fails\r\nWITH RECURSIVE nums AS (\r\nSELECT 0 AS n\r\nUNION ALL\r\nSELECT n + 1 FROM nums\r\nWHERE n < (SELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM  read_json_auto('filter_keystage.ndjson'))\r\n)\r\nselect * from nums;\r\n```\r\n\r\n```\r\n> duckdb                                                                                                                                     (base)\r\nv0.8.1 6536a77232\r\nEnter \".help\" for usage hints.\r\nConnected to a transient in-memory database.\r\nUse \".open FILENAME\" to reopen on a persistent database.\r\nD SELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM  read_json_auto('filter_keystage.ndjson');\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 (CAST(max(json_array_length(filter_keystage)) AS INTEGER) - 1) \u2502\r\n\u2502                             int32                              \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                              2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD WITH RECURSIVE nums AS (\r\n> SELECT 0 AS n\r\n> UNION ALL\r\n> SELECT n + 1 FROM nums\r\n> WHERE n < 5\r\n> )\r\n> select * from nums;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   n   \u2502\r\n\u2502 int32 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502     0 \u2502\r\n\u2502     1 \u2502\r\n\u2502     2 \u2502\r\n\u2502     3 \u2502\r\n\u2502     4 \u2502\r\n\u2502     5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD WITH RECURSIVE nums AS (\r\n> SELECT 0 AS n\r\n> UNION ALL\r\n> SELECT n + 1 FROM nums\r\n> WHERE n < (SELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM  read_json_auto('filter_keystage.ndjson'))\r\n> )\r\n> select * from nums;\r\nError: INTERNAL Error: Attempted to dereference unique_ptr that is NULL!\r\nD\r\n```\r\n\r\n### OS:\r\n\r\nmacOS Ventura 13.4.1 aarch64\r\n\r\n### DuckDB Version:\r\n\r\n0.8.1\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nPeter Bowyer\r\n\r\n### Affiliation:\r\n\r\nN/A\r\n\r\n### Have you tried this on the latest `main` branch?\r\n\r\nI have tested with a main build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\nINTERNAL Error: Attempted to dereference unique_ptr that is NULL\nI'm porting some queries from MySQL and ran into a problem with this one on DuckDB.\r\n\r\n### What happens?\r\n\r\nTwo queries work separately, but when combined result in a `unique_ptr` error.\r\n\r\nTested with 0.81 and main on **macOS/aarch64** and **Ubuntu 20.04/x64**.\r\n\r\n### To Reproduce\r\n\r\nFile **filter_keystage.ndjson**\r\n```json\r\n{\"filter_keystage\":null}\r\n{\"filter_keystage\":[\"key-stage-1\"]}\r\n{\"filter_keystage\":[\"key-stage-1\",\"key-stage-2\",\"key-stage-3\"]}\r\n```\r\nQueries\r\n```sql\r\n-- Succeeds\r\nSELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM  read_json_auto('filter_keystage.ndjson');\r\n\r\n-- Succeeds\r\nWITH RECURSIVE nums AS (\r\nSELECT 0 AS n\r\nUNION ALL\r\nSELECT n + 1 FROM nums\r\nWHERE n < 5\r\n)\r\nselect * from nums;\r\n\r\n-- Fails\r\nWITH RECURSIVE nums AS (\r\nSELECT 0 AS n\r\nUNION ALL\r\nSELECT n + 1 FROM nums\r\nWHERE n < (SELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM  read_json_auto('filter_keystage.ndjson'))\r\n)\r\nselect * from nums;\r\n```\r\n\r\n```\r\n> duckdb                                                                                                                                     (base)\r\nv0.8.1 6536a77232\r\nEnter \".help\" for usage hints.\r\nConnected to a transient in-memory database.\r\nUse \".open FILENAME\" to reopen on a persistent database.\r\nD SELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM  read_json_auto('filter_keystage.ndjson');\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 (CAST(max(json_array_length(filter_keystage)) AS INTEGER) - 1) \u2502\r\n\u2502                             int32                              \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                              2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD WITH RECURSIVE nums AS (\r\n> SELECT 0 AS n\r\n> UNION ALL\r\n> SELECT n + 1 FROM nums\r\n> WHERE n < 5\r\n> )\r\n> select * from nums;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   n   \u2502\r\n\u2502 int32 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502     0 \u2502\r\n\u2502     1 \u2502\r\n\u2502     2 \u2502\r\n\u2502     3 \u2502\r\n\u2502     4 \u2502\r\n\u2502     5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD WITH RECURSIVE nums AS (\r\n> SELECT 0 AS n\r\n> UNION ALL\r\n> SELECT n + 1 FROM nums\r\n> WHERE n < (SELECT MAX(JSON_ARRAY_LENGTH(filter_keystage))::int - 1 FROM  read_json_auto('filter_keystage.ndjson'))\r\n> )\r\n> select * from nums;\r\nError: INTERNAL Error: Attempted to dereference unique_ptr that is NULL!\r\nD\r\n```\r\n\r\n### OS:\r\n\r\nmacOS Ventura 13.4.1 aarch64\r\n\r\n### DuckDB Version:\r\n\r\n0.8.1\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nPeter Bowyer\r\n\r\n### Affiliation:\r\n\r\nN/A\r\n\r\n### Have you tried this on the latest `main` branch?\r\n\r\nI have tested with a main build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\nGlobbing JSON with nested fields of varied shapes hangs indefinitely\n### What happens?\n\nGlobbing JSON appears to hang indefinitely when there are nested fields of varying shapes unless you use the pattern in Scenario 8. I tried a _bunch_ of different things and frankly confused the hell out of myself so I can't even attempt to guess at what's going on, but my theories would probably be wrong anyway. I can say definitively that hanging indefinitely with no error is probably not the intended behavior though so something seems to be up here. \n\n### To Reproduce\n\n# step 1: grab data\r\n\r\n```\r\nwget https://data.gharchive.org/2023-08-16-1.json.gz\r\nwget https://data.gharchive.org/2023-08-16-12.json.gz\r\n```\r\n\r\n# step 2: various attempts, the last one works\r\n\r\n```\r\nimport duckdb\r\n\r\ncon = duckdb.connect(database=\"github_archive.db\", read_only=False)\r\n\r\n# scenario 1\r\n# this complains that the shapes don't match once you do the insert\r\n# con.execute(\"CREATE OR REPLACE TABLE github_events AS SELECT * FROM './2023-08-16-1.json.gz'\")\r\n# con.execute(\"INSERT INTO github_events SELECT * FROM './2023-08-16-12.json.gz'\")\r\n\r\n# scenario 2 \r\n# this hangs forever\r\n# con.execute(\"CREATE OR REPLACE TABLE github_events AS SELECT * FROM '*.json.gz'\")\r\n\r\n# scenario 3\r\n# this works on a single file\r\n# con.execute(\"CREATE OR REPLACE TABLE github_events AS SELECT * FROM read_json_auto('2023-08-16-1.json.gz')\");\r\n\r\n# scenario 4\r\n# this hangs forever\r\n# con.execute(\"CREATE OR REPLACE TABLE github_events AS SELECT * FROM read_json_auto('*.json.gz')\")\r\n\r\n# scenario 5\r\n# this hangs forever\r\n# con.execute(\"CREATE OR REPLACE TABLE github_events(id STRING, type STRING, actor JSON, repo JSON, payload JSON, public BOOLEAN, created_at TIMESTAMP, org JSON); INSERT INTO github_events SELECT * FROM '*.json.gz';\")\r\n\r\n# scenario 6\r\n# this works on a single file\r\n# con.execute(\"CREATE OR REPLACE TABLE github_events(id STRING, type STRING, actor JSON, repo JSON, payload JSON, public BOOLEAN, created_at TIMESTAMP, org JSON); INSERT INTO github_events SELECT * FROM '2023-08-16-1.json.gz';\")\r\n\r\n# scenario 7\r\n# this says MAP is not the right type it sees it as a STRUCT\r\n# con.execute(\"CREATE OR REPLACE TABLE github_events(id STRING, type STRING, actor MAP(STRING, STRING), repo MAP(STRING, STRING), payload MAP(STRING, STRING), public BOOLEAN, created_at TIMESTAMP, org MAP(STRING, STRING)); INSERT INTO github_events SELECT * FROM '2023-08-16-1.json.gz';\")\r\n\r\n# scenario 8\r\n# this works for a glob!\r\n# con.execute(\"CREATE OR REPLACE TABLE github_events AS SELECT * FROM read_json('./*.json.gz', columns={'id': 'STRING', 'type': 'STRING', 'actor': 'JSON', 'repo': 'JSON', 'payload': 'JSON', 'public': 'BOOLEAN', 'created_at': 'TIMESTAMP', 'org': 'JSON'}, format='newline_delimited');\")\r\n```\n\n### OS:\n\nmacOS 13.4.1 arm64 \n\n### DuckDB Version:\n\n0.8.1\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nGwen Windflower\n\n### Affiliation:\n\ndbt Labs\n\n### Have you tried this on the latest `master` branch?\n\nI have tested with a master build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "\n\n",
  "created_at": "2023-09-06T07:10:32Z"
}