{
  "repo": "duckdb/duckdb",
  "pull_number": 5659,
  "instance_id": "duckdb__duckdb-5659",
  "issue_numbers": [
    "5655"
  ],
  "base_commit": "dc7639cd4b68d5005d4795aa4961132dd7b3d4ba",
  "patch": "diff --git a/src/function/table/version/pragma_version.cpp b/src/function/table/version/pragma_version.cpp\nindex a3f68515e855..dfa4424c5d64 100644\n--- a/src/function/table/version/pragma_version.cpp\n+++ b/src/function/table/version/pragma_version.cpp\n@@ -44,6 +44,10 @@ void PragmaVersion::RegisterFunction(BuiltinFunctions &set) {\n \tset.AddFunction(pragma_version);\n }\n \n+idx_t DuckDB::StandardVectorSize() {\n+\treturn STANDARD_VECTOR_SIZE;\n+}\n+\n const char *DuckDB::SourceID() {\n \treturn DUCKDB_SOURCE_ID;\n }\ndiff --git a/src/include/duckdb/main/database.hpp b/src/include/duckdb/main/database.hpp\nindex 3cf069413203..858b775f77b0 100644\n--- a/src/include/duckdb/main/database.hpp\n+++ b/src/include/duckdb/main/database.hpp\n@@ -95,6 +95,7 @@ class DuckDB {\n \tDUCKDB_API idx_t NumberOfThreads();\n \tDUCKDB_API static const char *SourceID();\n \tDUCKDB_API static const char *LibraryVersion();\n+\tDUCKDB_API static idx_t StandardVectorSize();\n \tDUCKDB_API static string Platform();\n \tDUCKDB_API bool ExtensionIsLoaded(const std::string &name);\n };\ndiff --git a/tools/pythonpkg/duckdb-stubs/__init__.pyi b/tools/pythonpkg/duckdb-stubs/__init__.pyi\nindex c0eeaede2b33..a98fd073550a 100644\n--- a/tools/pythonpkg/duckdb-stubs/__init__.pyi\n+++ b/tools/pythonpkg/duckdb-stubs/__init__.pyi\n@@ -23,6 +23,8 @@ operator: token_type\n paramstyle: str\n string_const: token_type\n threadsafety: int\n+__standard_vector_size__: int\n+\n __interactive__: bool\n __jupyter__: bool\n \ndiff --git a/tools/pythonpkg/duckdb_python.cpp b/tools/pythonpkg/duckdb_python.cpp\nindex d8afce69f46f..bec9dd5e7e9a 100644\n--- a/tools/pythonpkg/duckdb_python.cpp\n+++ b/tools/pythonpkg/duckdb_python.cpp\n@@ -73,6 +73,7 @@ PYBIND11_MODULE(DUCKDB_PYTHON_LIB_NAME, m) {\n \tm.doc() = \"DuckDB is an embeddable SQL OLAP Database Management System\";\n \tm.attr(\"__package__\") = \"duckdb\";\n \tm.attr(\"__version__\") = DuckDB::LibraryVersion();\n+\tm.attr(\"__standard_vector_size__\") = DuckDB::StandardVectorSize();\n \tm.attr(\"__git_revision__\") = DuckDB::SourceID();\n \tm.attr(\"__interactive__\") = DuckDBPyConnection::DetectAndGetEnvironment();\n \tm.attr(\"__jupyter__\") = DuckDBPyConnection::IsJupyter();\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp\nindex 6896f41c5464..3165141886f5 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp\n@@ -18,10 +18,12 @@ struct NumpyCacheItem : public PythonImportCacheItem {\n \t}\n \tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n \t\tndarray.LoadAttribute(\"ndarray\", cache, *this);\n+\t\tdatetime64.LoadAttribute(\"datetime64\", cache, *this);\n \t}\n \n public:\n \tPythonImportCacheItem ndarray;\n+\tPythonImportCacheItem datetime64;\n };\n \n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp b/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp\nindex 5c1fb05ee224..89d41e0b3ec3 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp\n@@ -38,6 +38,7 @@ enum class PythonObjectType {\n \tList,\n \tDict,\n \tNdArray,\n+\tNdDatetime,\n };\n \n PythonObjectType GetPythonObjectType(py::handle &ele);\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/python_import_cache.hpp b/tools/pythonpkg/src/include/duckdb_python/python_import_cache.hpp\ndeleted file mode 100644\nindex 3ffcbd127bd4..000000000000\n--- a/tools/pythonpkg/src/include/duckdb_python/python_import_cache.hpp\n+++ /dev/null\n@@ -1,260 +0,0 @@\n-//===----------------------------------------------------------------------===//\n-//                         DuckDB\n-//\n-// duckdb_python/python_object_container.hpp\n-//\n-//\n-//===----------------------------------------------------------------------===//\n-\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n-#include \"duckdb.hpp\"\n-#include \"duckdb/common/vector.hpp\"\n-#include \"duckdb_python/python_object_container.hpp\"\n-\n-#include \"datetime.h\" //From python\n-\n-namespace duckdb {\n-\n-struct PythonImportCache;\n-\n-struct PythonImportCacheItem {\n-public:\n-\tPythonImportCacheItem() : object(nullptr) {\n-\t}\n-\tvirtual ~PythonImportCacheItem() {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) {\n-\t}\n-\n-public:\n-\tbool IsLoaded() const;\n-\tbool IsInstance(py::handle object) const;\n-\tpy::handle operator()(void) const;\n-\tvoid LoadModule(const string &name, PythonImportCache &cache);\n-\tvoid LoadAttribute(const string &name, PythonImportCache &cache, PythonImportCacheItem &source);\n-\n-protected:\n-\tvirtual bool IsRequired() const {\n-\t\treturn true;\n-\t}\n-\n-private:\n-\tPyObject *AddCache(PythonImportCache &cache, py::object object);\n-\n-private:\n-\t//! The stored item\n-\tPyObject *object;\n-};\n-\n-//===--------------------------------------------------------------------===//\n-// Modules\n-//===--------------------------------------------------------------------===//\n-\n-struct IPythonDisplayCacheItem : public PythonImportCacheItem {\n-public:\n-\t~IPythonDisplayCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tdisplay.LoadAttribute(\"display\", cache, *this);\n-\t}\n-\n-public:\n-\tPythonImportCacheItem display;\n-};\n-\n-struct IPythonCacheItem : public PythonImportCacheItem {\n-public:\n-\t~IPythonCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tget_ipython.LoadAttribute(\"get_ipython\", cache, *this);\n-\t\tdisplay.LoadModule(\"IPython.display\", cache);\n-\t}\n-\n-public:\n-\tPythonImportCacheItem get_ipython;\n-\tIPythonDisplayCacheItem display;\n-\n-protected:\n-\tbool IsRequired() const override final {\n-\t\treturn false;\n-\t}\n-};\n-\n-struct PandasLibsCacheItem : public PythonImportCacheItem {\n-public:\n-\t~PandasLibsCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tNAType.LoadAttribute(\"NAType\", cache, *this);\n-\t}\n-\n-public:\n-\tPythonImportCacheItem NAType;\n-\n-protected:\n-\tbool IsRequired() const override final {\n-\t\treturn false;\n-\t}\n-};\n-\n-struct PandasCacheItem : public PythonImportCacheItem {\n-public:\n-\t~PandasCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tDataFrame.LoadAttribute(\"DataFrame\", cache, *this);\n-\t\tlibs.LoadModule(\"pandas._libs.missing\", cache);\n-\t\tisnull.LoadAttribute(\"isnull\", cache, *this);\n-\t}\n-\n-public:\n-\t//! pandas.DataFrame\n-\tPythonImportCacheItem DataFrame;\n-\tPandasLibsCacheItem libs;\n-\tPythonImportCacheItem isnull;\n-\n-protected:\n-\tbool IsRequired() const override final {\n-\t\treturn false;\n-\t}\n-};\n-\n-struct ArrowLibCacheItem : public PythonImportCacheItem {\n-public:\n-\t~ArrowLibCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tTable.LoadAttribute(\"Table\", cache, *this);\n-\t\tRecordBatchReader.LoadAttribute(\"RecordBatchReader\", cache, *this);\n-\t}\n-\n-public:\n-\tPythonImportCacheItem Table;\n-\tPythonImportCacheItem RecordBatchReader;\n-};\n-\n-struct ArrowDatasetCacheItem : public PythonImportCacheItem {\n-public:\n-\t~ArrowDatasetCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tDataset.LoadAttribute(\"Dataset\", cache, *this);\n-\t\tScanner.LoadAttribute(\"Scanner\", cache, *this);\n-\t}\n-\n-public:\n-\tPythonImportCacheItem Dataset;\n-\tPythonImportCacheItem Scanner;\n-};\n-\n-struct ArrowCacheItem : public PythonImportCacheItem {\n-public:\n-\t~ArrowCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tlib.LoadAttribute(\"lib\", cache, *this);\n-\t\tdataset.LoadModule(\"pyarrow.dataset\", cache);\n-\t}\n-\n-public:\n-\tArrowLibCacheItem lib;\n-\tArrowDatasetCacheItem dataset;\n-\n-protected:\n-\tbool IsRequired() const override final {\n-\t\treturn false;\n-\t}\n-};\n-\n-struct DecimalCacheItem : public PythonImportCacheItem {\n-public:\n-\t~DecimalCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tDecimal.LoadAttribute(\"Decimal\", cache, *this);\n-\t}\n-\n-public:\n-\t//! decimal.Decimal\n-\tPythonImportCacheItem Decimal;\n-};\n-\n-struct UUIDCacheItem : public PythonImportCacheItem {\n-public:\n-\t~UUIDCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tUUID.LoadAttribute(\"UUID\", cache, *this);\n-\t}\n-\n-public:\n-\t//! uuid.UUID\n-\tPythonImportCacheItem UUID;\n-};\n-\n-struct NumpyCacheItem : public PythonImportCacheItem {\n-public:\n-\t~NumpyCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tndarray.LoadAttribute(\"ndarray\", cache, *this);\n-\t}\n-\n-public:\n-\tPythonImportCacheItem ndarray;\n-};\n-\n-struct DatetimeCacheItem : public PythonImportCacheItem {\n-public:\n-\t~DatetimeCacheItem() override {\n-\t}\n-\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n-\t\tdatetime.LoadAttribute(\"datetime\", cache, *this);\n-\t\tdate.LoadAttribute(\"date\", cache, *this);\n-\t\ttime.LoadAttribute(\"time\", cache, *this);\n-\t\ttimedelta.LoadAttribute(\"timedelta\", cache, *this);\n-\t}\n-\n-public:\n-\tPythonImportCacheItem datetime;\n-\tPythonImportCacheItem date;\n-\tPythonImportCacheItem time;\n-\tPythonImportCacheItem timedelta;\n-};\n-\n-//===--------------------------------------------------------------------===//\n-// PythonImportCache\n-//===--------------------------------------------------------------------===//\n-\n-struct PythonImportCache {\n-public:\n-\texplicit PythonImportCache() {\n-\t\tpy::gil_scoped_acquire acquire;\n-\t\tnumpy.LoadModule(\"numpy\", *this);\n-\t\tdatetime.LoadModule(\"datetime\", *this);\n-\t\tdecimal.LoadModule(\"decimal\", *this);\n-\t\tuuid.LoadModule(\"uuid\", *this);\n-\t\tpandas.LoadModule(\"pandas\", *this);\n-\t\tarrow.LoadModule(\"pyarrow\", *this);\n-\t\tIPython.LoadModule(\"IPython\", *this);\n-\t}\n-\t~PythonImportCache();\n-\n-public:\n-\tNumpyCacheItem numpy;\n-\tDatetimeCacheItem datetime;\n-\tDecimalCacheItem decimal;\n-\tUUIDCacheItem uuid;\n-\tPandasCacheItem pandas;\n-\tArrowCacheItem arrow;\n-\tIPythonCacheItem IPython;\n-\n-public:\n-\tPyObject *AddCache(py::object item);\n-\n-private:\n-\tvector<py::object> owned_objects;\n-};\n-\n-} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/pandas/analyzer.cpp b/tools/pythonpkg/src/pandas/analyzer.cpp\nindex cb44900c9d13..7417cd12c2fc 100644\n--- a/tools/pythonpkg/src/pandas/analyzer.cpp\n+++ b/tools/pythonpkg/src/pandas/analyzer.cpp\n@@ -312,6 +312,9 @@ LogicalType PandasAnalyzer::GetItemType(py::handle ele, bool &can_convert) {\n \t\t}\n \t\treturn DictToStruct(dict, can_convert);\n \t}\n+\tcase PythonObjectType::NdDatetime: {\n+\t\treturn GetItemType(ele.attr(\"tolist\")(), can_convert);\n+\t}\n \tcase PythonObjectType::NdArray: {\n \t\tauto extended_type = ConvertPandasType(ele.attr(\"dtype\"));\n \t\tLogicalType ltype;\ndiff --git a/tools/pythonpkg/src/pandas/type.cpp b/tools/pythonpkg/src/pandas/type.cpp\nindex 405827050b20..732f86e90a80 100644\n--- a/tools/pythonpkg/src/pandas/type.cpp\n+++ b/tools/pythonpkg/src/pandas/type.cpp\n@@ -6,6 +6,16 @@\n \n namespace duckdb {\n \n+static bool IsDateTime(const string &col_type_str) {\n+\tif (StringUtil::StartsWith(col_type_str, \"datetime64[ns\")) {\n+\t\treturn true;\n+\t}\n+\tif (col_type_str == \"<M8[ns]\") {\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n PandasType ConvertPandasType(const py::object &col_type) {\n \tauto col_type_str = string(py::str(col_type));\n \n@@ -36,7 +46,7 @@ PandasType ConvertPandasType(const py::object &col_type) {\n \t\treturn PandasType::OBJECT;\n \t} else if (col_type_str == \"timedelta64[ns]\") {\n \t\treturn PandasType::TIMEDELTA;\n-\t} else if (StringUtil::StartsWith(col_type_str, \"datetime64[ns\") || col_type_str == \"<M8[ns]\") {\n+\t} else if (IsDateTime(col_type_str)) {\n \t\tif (hasattr(col_type, \"tz\")) {\n \t\t\t// The datetime has timezone information.\n \t\t\treturn PandasType::DATETIME_TZ;\ndiff --git a/tools/pythonpkg/src/python_conversion.cpp b/tools/pythonpkg/src/python_conversion.cpp\nindex 4206b1ea6c30..7ca57cf867cd 100644\n--- a/tools/pythonpkg/src/python_conversion.cpp\n+++ b/tools/pythonpkg/src/python_conversion.cpp\n@@ -240,6 +240,8 @@ PythonObjectType GetPythonObjectType(py::handle &ele) {\n \t\treturn PythonObjectType::Dict;\n \t} else if (py::isinstance(ele, import_cache.numpy.ndarray())) {\n \t\treturn PythonObjectType::NdArray;\n+\t} else if (py::isinstance(ele, import_cache.numpy.datetime64())) {\n+\t\treturn PythonObjectType::NdDatetime;\n \t} else {\n \t\treturn PythonObjectType::Other;\n \t}\n@@ -330,7 +332,8 @@ Value TransformPythonValue(py::handle ele, const LogicalType &target_type, bool\n \t\t}\n \t}\n \tcase PythonObjectType::NdArray:\n-\t\treturn TransformPythonValue(ele.attr(\"tolist\")());\n+\tcase PythonObjectType::NdDatetime:\n+\t\treturn TransformPythonValue(ele.attr(\"tolist\")(), target_type, nan_as_null);\n \tcase PythonObjectType::Other:\n \t\tthrow NotImplementedException(\"Unable to transform python value of type '%s' to DuckDB LogicalType\",\n \t\t                              py::str(ele.get_type()).cast<string>());\ndiff --git a/tools/pythonpkg/src/vector_conversion.cpp b/tools/pythonpkg/src/vector_conversion.cpp\nindex c33dafa92b1c..aaaa2d967600 100644\n--- a/tools/pythonpkg/src/vector_conversion.cpp\n+++ b/tools/pythonpkg/src/vector_conversion.cpp\n@@ -200,13 +200,13 @@ void VerifyTypeConstraints(Vector &vec, idx_t count) {\n void ScanPandasObjectColumn(PandasColumnBindData &bind_data, PyObject **col, idx_t count, idx_t offset, Vector &out) {\n \t// numpy_col is a sequential list of objects, that make up one \"column\" (Vector)\n \tout.SetVectorType(VectorType::FLAT_VECTOR);\n-\tauto gil = make_unique<PythonGILWrapper>(); // We're creating python objects here, so we need the GIL\n-\n-\tfor (idx_t i = 0; i < count; i++) {\n-\t\tidx_t source_idx = offset + i;\n-\t\tScanPandasObject(bind_data, col[source_idx], i, out);\n+\t{\n+\t\tPythonGILWrapper gil; // We're creating python objects here, so we need the GIL\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tidx_t source_idx = offset + i;\n+\t\t\tScanPandasObject(bind_data, col[source_idx], i, out);\n+\t\t}\n \t}\n-\tgil.reset();\n \tVerifyTypeConstraints(out, count);\n }\n \n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_arrow_list.py b/tools/pythonpkg/tests/fast/arrow/test_arrow_list.py\nindex e28264c3b394..6f12f6c2867b 100644\n--- a/tools/pythonpkg/tests/fast/arrow/test_arrow_list.py\n+++ b/tools/pythonpkg/tests/fast/arrow/test_arrow_list.py\n@@ -22,12 +22,6 @@ def create_and_register_arrow_table(column_list, duckdb_conn):\n     duck_from_arrow = duckdb_conn.from_arrow(res)\n     duck_from_arrow.create(\"testarrow\")\n \n-# If the value is a numpy array, turn it into a list (numpy.ndarray not currently supported by TransformPythonValue)\n-def transform(val):\n-    if (isinstance(val, np.ndarray)):\n-        val = list(val)\n-    return val\n-\n def create_and_register_comparison_result(column_list, duckdb_conn):\n     columns = \",\".join([f'{name} {dtype}' for (name, dtype, _) in column_list])\n     column_amount = len(column_list)\n@@ -36,7 +30,7 @@ def create_and_register_comparison_result(column_list, duckdb_conn):\n     inserted_values = []\n     for row in range(row_amount):\n         for col in range(column_amount):\n-            inserted_values.append(transform(column_list[col][2][row]))\n+            inserted_values.append(column_list[col][2][row])\n     inserted_values = tuple(inserted_values)\n \n     column_format = \",\".join(['?' for _ in range(column_amount)])\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\nindex 39c42d80af5c..5a9c216b06fc 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n@@ -6,6 +6,9 @@\n import decimal\n import math\n from decimal import Decimal\n+import re\n+\n+standard_vector_size = duckdb.__standard_vector_size__\n \n def create_generic_dataframe(data):\n     return pd.DataFrame({'0': pd.Series(data=data, dtype='object')})\n@@ -27,7 +30,7 @@ def ConvertStringToDecimal(data: list):\n \n class TestResolveObjectColumns(object):\n \n-    def test_integers(self, duckdb_cursor):\n+    def test_integers(self):\n         data = [5, 0, 3]\n         df_in = create_generic_dataframe(data)\n         # These are float64 because pandas would force these to be float64 even if we set them to int8, int16, int32, int64 respectively\n@@ -36,14 +39,14 @@ def test_integers(self, duckdb_cursor):\n         print(df_out)\n         pd.testing.assert_frame_equal(df_expected_res, df_out)\n \n-    def test_struct_correct(self, duckdb_cursor):\n+    def test_struct_correct(self):\n         data = [{'a': 1, 'b': 3, 'c': 3, 'd': 7}]\n         df = pd.DataFrame({'0': pd.Series(data=data)})\n         duckdb_col = duckdb.query(\"SELECT {a: 1, b: 3, c: 3, d: 7} as '0'\").df()\n         converted_col = duckdb.query_df(df, \"data\", \"SELECT * FROM data\").df()\n         pd.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_map_fallback_different_keys(self, duckdb_cursor):\n+    def test_map_fallback_different_keys(self):\n         x = pd.DataFrame(\n             [\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n@@ -67,7 +70,7 @@ def test_map_fallback_different_keys(self, duckdb_cursor):\n         equal_df = duckdb.query_df(y, \"y\", \"SELECT * FROM y\").df()\n         pd.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_map_fallback_incorrect_amount_of_keys(self, duckdb_cursor):\n+    def test_map_fallback_incorrect_amount_of_keys(self):\n         x = pd.DataFrame(\n             [\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n@@ -90,7 +93,7 @@ def test_map_fallback_incorrect_amount_of_keys(self, duckdb_cursor):\n         equal_df = duckdb.query_df(y, \"y\", \"SELECT * FROM y\").df()\n         pd.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_struct_value_upgrade(self, duckdb_cursor):\n+    def test_struct_value_upgrade(self):\n         x = pd.DataFrame(\n             [\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 'string'}],\n@@ -113,7 +116,7 @@ def test_struct_value_upgrade(self, duckdb_cursor):\n         equal_df = duckdb.query_df(y, \"y\", \"SELECT * FROM y\").df()\n         pd.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_struct_null(self, duckdb_cursor):\n+    def test_struct_null(self):\n         x = pd.DataFrame(\n             [\n                 [None],\n@@ -136,7 +139,7 @@ def test_struct_null(self, duckdb_cursor):\n         equal_df = duckdb.query_df(y, \"y\", \"SELECT * FROM y\").df()\n         pd.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_map_fallback_value_upgrade(self, duckdb_cursor):\n+    def test_map_fallback_value_upgrade(self):\n         x = pd.DataFrame(\n             [\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 'test'}],\n@@ -159,7 +162,7 @@ def test_map_fallback_value_upgrade(self, duckdb_cursor):\n         equal_df = duckdb.query_df(y, \"df\", \"SELECT * FROM df\").df()\n         pd.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_map_correct(self, duckdb_cursor):\n+    def test_map_correct(self):\n         x = pd.DataFrame(\n             [\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 7]}],\n@@ -185,7 +188,7 @@ def test_map_correct(self, duckdb_cursor):\n         print(converted_col.columns)\n         pd.testing.assert_frame_equal(converted_col, duckdb_col)\n \n-    def test_map_value_upgrade(self, duckdb_cursor):\n+    def test_map_value_upgrade(self):\n         x = pd.DataFrame(\n             [\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 'test']}],\n@@ -214,7 +217,7 @@ def test_map_value_upgrade(self, duckdb_cursor):\n         print(converted_col.columns)\n         pd.testing.assert_frame_equal(converted_col, duckdb_col)\n \n-    def test_map_duplicate(self, duckdb_cursor):\n+    def test_map_duplicate(self):\n         x = pd.DataFrame(\n             [\n                 [{'key': ['a', 'a', 'b'], 'value': [4, 0, 4]}]\n@@ -223,7 +226,7 @@ def test_map_duplicate(self, duckdb_cursor):\n         with pytest.raises(duckdb.InvalidInputException, match=\"Dict->Map conversion failed because 'key' list contains duplicates\"):\n             converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n \n-    def test_map_nullkey(self, duckdb_cursor):\n+    def test_map_nullkey(self):\n         x = pd.DataFrame(\n             [\n                 [{'key': [None, 'a', 'b'], 'value': [4, 0, 4]}]\n@@ -232,7 +235,7 @@ def test_map_nullkey(self, duckdb_cursor):\n         with pytest.raises(duckdb.InvalidInputException, match=\"Dict->Map conversion failed because 'key' list contains None\"):\n             converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n \n-    def test_map_nullkeylist(self, duckdb_cursor):\n+    def test_map_nullkeylist(self):\n         x = pd.DataFrame(\n             [\n                 [{'key': None, 'value': None}]\n@@ -243,7 +246,7 @@ def test_map_nullkeylist(self, duckdb_cursor):\n         duckdb_col = duckdb.query(\"SELECT {key: NULL, value: NULL} as '0'\").df()\n         pd.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_map_fallback_nullkey(self, duckdb_cursor):\n+    def test_map_fallback_nullkey(self):\n         x = pd.DataFrame(\n             [\n                 [{'a': 4, None: 0, 'c': 4}],\n@@ -253,7 +256,7 @@ def test_map_fallback_nullkey(self, duckdb_cursor):\n         with pytest.raises(duckdb.InvalidInputException, match=\"Dict->Map conversion failed because 'key' list contains None\"):\n             converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n \n-    def test_map_fallback_nullkey_coverage(self, duckdb_cursor):\n+    def test_map_fallback_nullkey_coverage(self):\n         x = pd.DataFrame(\n             [\n                 [{'key': None, 'value': None}],\n@@ -263,7 +266,7 @@ def test_map_fallback_nullkey_coverage(self, duckdb_cursor):\n         with pytest.raises(duckdb.InvalidInputException, match=\"Dict->Map conversion failed because 'key' list contains None\"):\n             converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n \n-    def test_struct_key_conversion(self, duckdb_cursor):\n+    def test_struct_key_conversion(self):\n         x = pd.DataFrame(\n             [\n                 [{\n@@ -279,7 +282,7 @@ def test_struct_key_conversion(self, duckdb_cursor):\n         duckdb.query(\"drop view if exists tbl\")\n         pd.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_correct(self, duckdb_cursor):\n+    def test_list_correct(self):\n         x = pd.DataFrame(\n             [\n                 {'0': [[5], [34], [-245]]}\n@@ -290,7 +293,7 @@ def test_list_correct(self, duckdb_cursor):\n         duckdb.query(\"drop view if exists tbl\")\n         pd.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_contains_null(self, duckdb_cursor):\n+    def test_list_contains_null(self):\n         x = pd.DataFrame(\n             [\n                 {'0': [[5], None, [-245]]}\n@@ -301,7 +304,7 @@ def test_list_contains_null(self, duckdb_cursor):\n         duckdb.query(\"drop view if exists tbl\")\n         pd.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_starts_with_null(self, duckdb_cursor):\n+    def test_list_starts_with_null(self):\n         x = pd.DataFrame(\n             [\n                 {'0': [None, [5], [-245]]}\n@@ -312,7 +315,7 @@ def test_list_starts_with_null(self, duckdb_cursor):\n         duckdb.query(\"drop view if exists tbl\")\n         pd.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_value_upgrade(self, duckdb_cursor):\n+    def test_list_value_upgrade(self):\n         x = pd.DataFrame(\n             [\n                 {'0': [['5'], [34], [-245]]}\n@@ -323,7 +326,7 @@ def test_list_value_upgrade(self, duckdb_cursor):\n         duckdb.query(\"drop view if exists tbl\")\n         pd.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_column_value_upgrade(self, duckdb_cursor):\n+    def test_list_column_value_upgrade(self):\n         x = pd.DataFrame(\n             [\n                 [ [1, 25, 300] ],\n@@ -352,7 +355,7 @@ def test_list_column_value_upgrade(self, duckdb_cursor):\n         print(converted_col.columns)\n         pd.testing.assert_frame_equal(converted_col, duckdb_col)\n \n-    def test_ubigint_object_conversion(self, duckdb_cursor):\n+    def test_ubigint_object_conversion(self):\n         # UBIGINT + TINYINT would result in HUGEINT, but conversion to HUGEINT is not supported yet from pandas->duckdb\n         # So this instead becomes a DOUBLE\n         data = [18446744073709551615, 0]\n@@ -361,14 +364,14 @@ def test_ubigint_object_conversion(self, duckdb_cursor):\n         float64 = np.dtype('float64')\n         assert isinstance(converted_col['0'].dtype, float64.__class__) == True\n \n-    def test_double_object_conversion(self, duckdb_cursor):\n+    def test_double_object_conversion(self):\n         data = [18446744073709551616, 0]\n         x = pd.DataFrame({'0': pd.Series(data=data, dtype='object')})\n         converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n         double_dtype = np.dtype('float64')\n         assert isinstance(converted_col['0'].dtype, double_dtype.__class__) == True\n \n-    def test_integer_conversion_fail(self, duckdb_cursor):\n+    def test_integer_conversion_fail(self):\n         data = [2**10000, 0]\n         x = pd.DataFrame({'0': pd.Series(data=data, dtype='object')})\n         converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n@@ -376,7 +379,30 @@ def test_integer_conversion_fail(self, duckdb_cursor):\n         double_dtype = np.dtype('object')\n         assert isinstance(converted_col['0'].dtype, double_dtype.__class__) == True\n \n-    def test_fallthrough_object_conversion(self, duckdb_cursor):\n+    # Most of the time numpy.datetime64 is just a wrapper around a datetime.datetime object\n+    # But to support arbitrary precision, it can fall back to using an `int` internally\n+    # Which we don't support yet\n+    def test_numpy_datetime(self):\n+        numpy = pytest.importorskip(\"numpy\")\n+\n+        data = []\n+        data += [numpy.datetime64('2022-12-10T21:38:24.578696')] * standard_vector_size\n+        data += [numpy.datetime64('2022-02-21T06:59:23.324812')] * standard_vector_size\n+        data += [numpy.datetime64('1974-06-05T13:12:01.000000')] * standard_vector_size\n+        data += [numpy.datetime64('2049-01-13T00:24:31.999999')] * standard_vector_size\n+        x = pd.DataFrame({'dates': pd.Series(data=data, dtype='object')})\n+        res = duckdb.query_df(x, \"x\", \"select distinct * from x\").df()\n+        assert(len(res['dates'].__array__()) == 4)\n+\n+    def test_numpy_datetime_int_internally(self):\n+        numpy = pytest.importorskip(\"numpy\")\n+\n+        data = [numpy.datetime64('2022-12-10T21:38:24.0000000000001')]\n+        x = pd.DataFrame({'dates': pd.Series(data=data, dtype='object')})\n+        with pytest.raises(duckdb.ConversionException, match=re.escape(\"Conversion Error: Unimplemented type for cast (BIGINT -> TIMESTAMP)\")):\n+            rel = duckdb.query_df(x, \"x\", \"create table dates as select dates::TIMESTAMP WITHOUT TIME ZONE from x\")\n+\n+    def test_fallthrough_object_conversion(self):\n         x = pd.DataFrame(\n             [\n                 [IntString(4)],\n@@ -434,10 +460,8 @@ def test_numeric_decimal_coverage(self):\n         assert(str(conversion) == '[(nan,), (nan,), (nan,), (inf,), (inf,), (inf,)]')\n \n     # Test that the column 'offset' is actually used when converting,\n-    # and that the same 1024 (STANDARD_VECTOR_SIZE) values are not being scanned over and over again\n+    # and that the same 2048 (STANDARD_VECTOR_SIZE) values are not being scanned over and over again\n     def test_multiple_chunks(self):\n-        standard_vector_size = 1024\n-\n         data = []\n         data += [datetime.date(2022, 9, 13) for x in range(standard_vector_size)]\n         data += [datetime.date(2022, 9, 14) for x in range(standard_vector_size)]\ndiff --git a/tools/pythonpkg/tests/fast/types/test_numpy.py b/tools/pythonpkg/tests/fast/types/test_numpy.py\nnew file mode 100644\nindex 000000000000..4f9e4efcdbec\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/types/test_numpy.py\n@@ -0,0 +1,13 @@\n+import duckdb\n+import numpy as np\n+import datetime\n+\n+class TestNumpyDatetime64(object):\n+    def test_numpy_datetime64(self, duckdb_cursor):\n+        duckdb_con = duckdb.connect()\n+\n+        duckdb_con.execute(\"create table tbl(col TIMESTAMP)\")\n+        duckdb_con.execute(\"insert into tbl VALUES (CAST(? AS TIMESTAMP WITHOUT TIME ZONE))\", parameters=[np.datetime64('2022-02-08T06:01:38.761310')])\n+        assert [(datetime.datetime(2022, 2, 8, 6, 1, 38, 761310),)] == duckdb_con.execute(\"select * from tbl\").fetchall()\n+\n+\n",
  "problem_statement": "Unable to transform python value of type '<class 'numpy.datetime64'>' to DuckDB LogicalType\n### What happens?\r\n\r\nI'm trying to add support for ibis+duckdb to hvplot/ HoloViews. See https://github.com/holoviz/holoviews/issues/5557#issuecomment-1345157033. When testing crossfiltering between plots I get `duckdb.NotImplementedException: Not implemented Error: Unable to transform python value of type '<class 'numpy.datetime64'>' to DuckDB LogicalType` error.\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n```python\r\nfrom pathlib import Path\r\n\r\nimport duckdb\r\nimport ibis\r\nimport numpy as np\r\nimport pandas as pd\r\nimport requests\r\n\r\nTRIP_DATA_PATH = \"yellow_tripdata_2022-02.parquet\"\r\n\r\nif not Path(TRIP_DATA_PATH).exists():\r\n    url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet\"\r\n    response = requests.get(url, verify=False)\r\n    with open(TRIP_DATA_PATH, \"wb\") as f:\r\n        f.write(response.content)\r\n\r\nDUCKDB_PATH = \"DuckDB.db\"\r\n\r\nif not Path(DUCKDB_PATH).exists():\r\n    duckdb_con = duckdb.connect(DUCKDB_PATH)\r\n    duckdb_con.execute(f\"CREATE TABLE big_df AS SELECT * FROM '{TRIP_DATA_PATH}'\")\r\nelse:\r\n    duckdb_con = duckdb.connect(DUCKDB_PATH)\r\n\r\nduckdb_con.execute(\"select * from big_df where big_df.tpep_pickup_datetime >= CAST(? AS TIMESTAMP WITHOUT TIME ZONE)\", parameters=[np.datetime64('2022-02-08T06:01:38.761310')]).execute()\r\n```\r\n\r\n### OS:\r\n\r\nwindows\r\n\r\n### DuckDB Version:\r\n\r\n0.6.0 (and  0.6.2.dev13)\r\n\r\n### DuckDB Client:\r\n\r\npython\r\n\r\n### Full Name:\r\n\r\nMarc Skov Madsen\r\n\r\n### Affiliation:\r\n\r\ndatamodelsanalytics.com\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\r\n\r\n### Additional Context\r\n\r\nAs I am originally using duckdb via Ibis, I've also added the issue to the Ibis issues https://github.com/ibis-project/ibis/issues/4997.\r\n\r\nI don't know if this is as expected (it is not for me), if Ibis should handle the numpy conversion or if DuckDb should.\n",
  "hints_text": "I'll look into this issue, sounds like the column is of type `object` and contains a numpy value, which was probably overlooked as a conversion option\n![image](https://user-images.githubusercontent.com/17162323/206872441-103a6adc-23ed-4319-9bf1-bec2a8e06d46.png)\r\n\nHmm if the precision of the numpy.datetime64 becomes too big it switches to an int as internal type, that's probably why they wrapped `datetime.datetime`, so they can scale out when it gets too big to be represented by a datetime.datetime",
  "created_at": "2022-12-10T20:58:21Z"
}