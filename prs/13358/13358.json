{
  "repo": "duckdb/duckdb",
  "pull_number": 13358,
  "instance_id": "duckdb__duckdb-13358",
  "issue_numbers": [
    "13138"
  ],
  "base_commit": "50bb607e2b2e5728664dd18da330eda354be3b96",
  "patch": "diff --git a/src/common/vector_operations/vector_copy.cpp b/src/common/vector_operations/vector_copy.cpp\nindex 7315a4e591dc..0880f23be632 100644\n--- a/src/common/vector_operations/vector_copy.cpp\n+++ b/src/common/vector_operations/vector_copy.cpp\n@@ -35,10 +35,7 @@ static const ValidityMask &ExtractValidityMask(const Vector &v) {\n }\n \n void VectorOperations::Copy(const Vector &source_p, Vector &target, const SelectionVector &sel_p, idx_t source_count,\n-                            idx_t source_offset, idx_t target_offset) {\n-\tD_ASSERT(source_offset <= source_count);\n-\tD_ASSERT(source_p.GetType() == target.GetType());\n-\tidx_t copy_count = source_count - source_offset;\n+                            idx_t source_offset, idx_t target_offset, idx_t copy_count) {\n \n \tSelectionVector owned_sel;\n \tconst SelectionVector *sel = &sel_p;\n@@ -173,7 +170,7 @@ void VectorOperations::Copy(const Vector &source_p, Vector &target, const Select\n \t\tD_ASSERT(source_children.size() == target_children.size());\n \t\tfor (idx_t i = 0; i < source_children.size(); i++) {\n \t\t\tVectorOperations::Copy(*source_children[i], *target_children[i], sel_p, source_count, source_offset,\n-\t\t\t                       target_offset);\n+\t\t\t                       target_offset, copy_count);\n \t\t}\n \t\tbreak;\n \t}\n@@ -267,6 +264,14 @@ void VectorOperations::Copy(const Vector &source_p, Vector &target, const Select\n \t}\n }\n \n+void VectorOperations::Copy(const Vector &source_p, Vector &target, const SelectionVector &sel_p, idx_t source_count,\n+                            idx_t source_offset, idx_t target_offset) {\n+\tD_ASSERT(source_offset <= source_count);\n+\tD_ASSERT(source_p.GetType() == target.GetType());\n+\tidx_t copy_count = source_count - source_offset;\n+\tVectorOperations::Copy(source_p, target, sel_p, source_count, source_offset, target_offset, copy_count);\n+}\n+\n void VectorOperations::Copy(const Vector &source, Vector &target, idx_t source_count, idx_t source_offset,\n                             idx_t target_offset) {\n \tVectorOperations::Copy(source, target, *FlatVector::IncrementalSelectionVector(), source_count, source_offset,\ndiff --git a/src/include/duckdb/common/vector_operations/vector_operations.hpp b/src/include/duckdb/common/vector_operations/vector_operations.hpp\nindex 514c703c9004..bd772fcad3ba 100644\n--- a/src/include/duckdb/common/vector_operations/vector_operations.hpp\n+++ b/src/include/duckdb/common/vector_operations/vector_operations.hpp\n@@ -192,6 +192,8 @@ struct VectorOperations {\n \t                 idx_t target_offset);\n \tstatic void Copy(const Vector &source, Vector &target, const SelectionVector &sel, idx_t source_count,\n \t                 idx_t source_offset, idx_t target_offset);\n+\tstatic void Copy(const Vector &source, Vector &target, const SelectionVector &sel, idx_t source_count,\n+\t                 idx_t source_offset, idx_t target_offset, idx_t copy_count);\n \n \t// Copy the data of <source> to the target location, setting null values to\n \t// NullValue<T>. Used to store data without separate NULL mask.\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp\nindex 4ab31a90ef23..6717648d0b03 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp\n@@ -79,7 +79,7 @@ template <class T>\n bool try_cast(const handle &object, T &result) {\n \ttry {\n \t\tresult = cast<T>(object);\n-\t} catch (cast_error &) {\n+\t} catch (pybind11::cast_error &) {\n \t\treturn false;\n \t}\n \treturn true;\ndiff --git a/tools/pythonpkg/src/python_udf.cpp b/tools/pythonpkg/src/python_udf.cpp\nindex 53e3eb72c6c9..bca316f09e36 100644\n--- a/tools/pythonpkg/src/python_udf.cpp\n+++ b/tools/pythonpkg/src/python_udf.cpp\n@@ -43,8 +43,7 @@ static py::object ConvertDataChunkToPyArrowTable(DataChunk &input, const ClientP\n \treturn pyarrow::ToArrowTable(types, names, ConvertToSingleBatch(types, names, input, options), options);\n }\n \n-static void ConvertPyArrowToDataChunk(const py::object &table, Vector &out, ClientContext &context, idx_t count) {\n-\n+static void ConvertArrowTableToVector(const py::object &table, Vector &out, ClientContext &context, idx_t count) {\n \t// Create the stream factory from the Table object\n \tauto stream_factory = make_uniq<PythonTableArrowArrayStreamFactory>(table.ptr(), context.GetClientProperties());\n \tauto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;\n@@ -68,7 +67,7 @@ static void ConvertPyArrowToDataChunk(const py::object &table, Vector &out, Clie\n \n \tTableFunctionRef empty;\n \tTableFunction dummy_table_function;\n-\tdummy_table_function.name = \"ConvertPyArrowToDataChunk\";\n+\tdummy_table_function.name = \"ConvertArrowTableToVector\";\n \tTableFunctionBindInput bind_input(children, named_params, input_types, input_names, nullptr, nullptr,\n \t                                  dummy_table_function, empty);\n \tvector<LogicalType> return_types;\n@@ -81,9 +80,6 @@ static void ConvertPyArrowToDataChunk(const py::object &table, Vector &out, Clie\n \t\t    \"The returned table from a pyarrow scalar udf should only contain one column, found %d\",\n \t\t    return_types.size());\n \t}\n-\t// if (return_types[0] != out.GetType()) {\n-\t//\tthrow InvalidInputException(\"The type of the returned array (%s) does not match the expected type: '%s'\", )\n-\t//}\n \n \tDataChunk result;\n \t// Reserve for STANDARD_VECTOR_SIZE instead of count, in case the returned table contains too many tuples\n@@ -101,14 +97,51 @@ static void ConvertPyArrowToDataChunk(const py::object &table, Vector &out, Clie\n \t}\n \n \tVectorOperations::Cast(context, result.data[0], out, count);\n+\tout.Flatten(count);\n }\n \n-static scalar_function_t CreateVectorizedFunction(PyObject *function, PythonExceptionHandling exception_handling) {\n+static string NullHandlingError() {\n+\treturn R\"(\n+The returned result contained NULL values, but the 'null_handling' was set to DEFAULT.\n+If you want more control over NULL values then 'null_handling' should be set to SPECIAL.\n+\n+With DEFAULT all rows containing NULL have been filtered from the UDFs input.\n+Those rows are automatically set to NULL in the final result.\n+The UDF is not expected to return NULL values.\n+\t)\";\n+}\n+\n+static ValidityMask &GetResultValidity(Vector &result) {\n+\tauto vector_type = result.GetVectorType();\n+\tif (vector_type == VectorType::CONSTANT_VECTOR) {\n+\t\treturn ConstantVector::Validity(result);\n+\t} else if (vector_type == VectorType::FLAT_VECTOR) {\n+\t\treturn FlatVector::Validity(result);\n+\t} else {\n+\t\tthrow InternalException(\"VectorType %s was not expected here (GetResultValidity)\",\n+\t\t                        EnumUtil::ToString(vector_type));\n+\t}\n+}\n+\n+static void VerifyVectorizedNullHandling(Vector &result, idx_t count) {\n+\tauto &validity = GetResultValidity(result);\n+\n+\tif (validity.AllValid()) {\n+\t\treturn;\n+\t}\n+\n+\tthrow InvalidInputException(NullHandlingError());\n+}\n+\n+static scalar_function_t CreateVectorizedFunction(PyObject *function, PythonExceptionHandling exception_handling,\n+                                                  FunctionNullHandling null_handling) {\n \t// Through the capture of the lambda, we have access to the function pointer\n \t// We just need to make sure that it doesn't get garbage collected\n \tscalar_function_t func = [=](DataChunk &input, ExpressionState &state, Vector &result) -> void {\n \t\tpy::gil_scoped_acquire gil;\n \n+\t\tconst bool default_null_handling = null_handling == FunctionNullHandling::DEFAULT_NULL_HANDLING;\n+\n \t\t// owning references\n \t\tpy::object python_object;\n \t\t// Convert the input datachunk to pyarrow\n@@ -119,6 +152,36 @@ static scalar_function_t CreateVectorizedFunction(PyObject *function, PythonExce\n \t\t\toptions = context.GetClientProperties();\n \t\t}\n \n+\t\tauto result_validity = FlatVector::Validity(result);\n+\t\tSelectionVector selvec(input.size());\n+\t\tidx_t input_size = input.size();\n+\t\tif (default_null_handling) {\n+\t\t\tvector<UnifiedVectorFormat> vec_data(input.ColumnCount());\n+\t\t\tfor (idx_t i = 0; i < input.ColumnCount(); i++) {\n+\t\t\t\tinput.data[i].ToUnifiedFormat(input.size(), vec_data[i]);\n+\t\t\t}\n+\n+\t\t\tidx_t index = 0;\n+\t\t\tfor (idx_t i = 0; i < input.size(); i++) {\n+\t\t\t\tbool any_null = false;\n+\t\t\t\tfor (idx_t col_idx = 0; col_idx < input.ColumnCount(); col_idx++) {\n+\t\t\t\t\tauto &vec = vec_data[col_idx];\n+\t\t\t\t\tif (!vec.validity.RowIsValid(vec.sel->get_index(i))) {\n+\t\t\t\t\t\tany_null = true;\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tif (any_null) {\n+\t\t\t\t\tresult_validity.SetInvalid(i);\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tselvec.set_index(index++, i);\n+\t\t\t}\n+\t\t\tif (index != input.size()) {\n+\t\t\t\tinput.Slice(selvec, index);\n+\t\t\t}\n+\t\t}\n+\n \t\tauto pyarrow_table = ConvertDataChunkToPyArrowTable(input, options);\n \t\tpy::tuple column_list = pyarrow_table.attr(\"columns\");\n \n@@ -126,7 +189,9 @@ static scalar_function_t CreateVectorizedFunction(PyObject *function, PythonExce\n \n \t\t// Call the function\n \t\tauto ret = PyObject_CallObject(function, column_list.ptr());\n+\t\tbool exception_occurred = false;\n \t\tif (ret == nullptr && PyErr_Occurred()) {\n+\t\t\texception_occurred = true;\n \t\t\tif (exception_handling == PythonExceptionHandling::FORWARD_ERROR) {\n \t\t\t\tauto exception = py::error_already_set();\n \t\t\t\tthrow InvalidInputException(\"Python exception occurred while executing the UDF: %s\", exception.what());\n@@ -154,9 +219,43 @@ static scalar_function_t CreateVectorizedFunction(PyObject *function, PythonExce\n \t\t\t}\n \t\t}\n \t\t// Convert the pyarrow result back to a DuckDB datachunk\n-\t\tConvertPyArrowToDataChunk(python_object, result, state.GetContext(), count);\n+\t\tif (count != input_size) {\n+\t\t\tD_ASSERT(default_null_handling);\n+\t\t\t// We filtered out some NULLs, now we need to reconstruct the final result by adding the nulls back\n+\t\t\tVector temp(result.GetType(), count);\n+\t\t\t// Convert the table into a temporary Vector\n+\t\t\tConvertArrowTableToVector(python_object, temp, state.GetContext(), count);\n+\t\t\tif (!exception_occurred) {\n+\t\t\t\tVerifyVectorizedNullHandling(temp, count);\n+\t\t\t}\n+\t\t\tif (count) {\n+\t\t\t\tSelectionVector inverted(input_size);\n+\t\t\t\t// Create a SelVec that inverts the filtering\n+\t\t\t\t// example: count: 6, null_indices: 1,3\n+\t\t\t\t// input selvec: [0, 2, 4, 5]\n+\t\t\t\t// inverted selvec: [0, 0, 1, 1, 2, 3]\n+\t\t\t\tidx_t src_index = 0;\n+\t\t\t\tfor (idx_t i = 0; i < input_size; i++) {\n+\t\t\t\t\t// Fill the gaps with the previous index\n+\t\t\t\t\tinverted.set_index(i, src_index);\n+\t\t\t\t\tif (src_index + 1 < count && selvec.get_index(src_index) == i) {\n+\t\t\t\t\t\tsrc_index++;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tVectorOperations::Copy(temp, result, inverted, count, 0, 0, input_size);\n+\t\t\t}\n+\t\t\tfor (idx_t i = 0; i < input_size; i++) {\n+\t\t\t\tFlatVector::SetNull(result, i, !result_validity.RowIsValid(i));\n+\t\t\t}\n+\t\t\tresult.Verify(input_size);\n+\t\t} else {\n+\t\t\tConvertArrowTableToVector(python_object, result, state.GetContext(), count);\n+\t\t\tif (default_null_handling && !exception_occurred) {\n+\t\t\t\tVerifyVectorizedNullHandling(result, count);\n+\t\t\t}\n+\t\t}\n \n-\t\tif (input.size() == 1) {\n+\t\tif (input_size == 1) {\n \t\t\tresult.SetVectorType(VectorType::CONSTANT_VECTOR);\n \t\t}\n \t};\n@@ -164,12 +263,15 @@ static scalar_function_t CreateVectorizedFunction(PyObject *function, PythonExce\n }\n \n static scalar_function_t CreateNativeFunction(PyObject *function, PythonExceptionHandling exception_handling,\n-                                              const ClientProperties &client_properties) {\n+                                              const ClientProperties &client_properties,\n+                                              FunctionNullHandling null_handling) {\n \t// Through the capture of the lambda, we have access to the function pointer\n \t// We just need to make sure that it doesn't get garbage collected\n \tscalar_function_t func = [=](DataChunk &input, ExpressionState &state, Vector &result) -> void { // NOLINT\n \t\tpy::gil_scoped_acquire gil;\n \n+\t\tconst bool default_null_handling = null_handling == FunctionNullHandling::DEFAULT_NULL_HANDLING;\n+\n \t\t// owning references\n \t\tvector<py::object> python_objects;\n \t\tvector<PyObject *> python_results;\n@@ -177,12 +279,23 @@ static scalar_function_t CreateNativeFunction(PyObject *function, PythonExceptio\n \t\tfor (idx_t row = 0; row < input.size(); row++) {\n \n \t\t\tauto bundled_parameters = py::tuple((int)input.ColumnCount());\n+\t\t\tbool contains_null = false;\n \t\t\tfor (idx_t i = 0; i < input.ColumnCount(); i++) {\n \t\t\t\t// Fill the tuple with the arguments for this row\n \t\t\t\tauto &column = input.data[i];\n \t\t\t\tauto value = column.GetValue(row);\n+\t\t\t\tif (value.IsNull() && default_null_handling) {\n+\t\t\t\t\tcontains_null = true;\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n \t\t\t\tbundled_parameters[i] = PythonObject::FromValue(value, column.GetType(), client_properties);\n \t\t\t}\n+\t\t\tif (contains_null) {\n+\t\t\t\t// Immediately insert None, no need to call the function\n+\t\t\t\tpython_objects.push_back(py::none());\n+\t\t\t\tpython_results[row] = py::none().ptr();\n+\t\t\t\tcontinue;\n+\t\t\t}\n \n \t\t\t// Call the function\n \t\t\tauto ret = PyObject_CallObject(function, bundled_parameters.ptr());\n@@ -197,6 +310,8 @@ static scalar_function_t CreateNativeFunction(PyObject *function, PythonExceptio\n \t\t\t\t} else {\n \t\t\t\t\tthrow NotImplementedException(\"Exception handling type not implemented\");\n \t\t\t\t}\n+\t\t\t} else if ((!ret || ret == Py_None) && default_null_handling) {\n+\t\t\t\tthrow InvalidInputException(NullHandlingError());\n \t\t\t}\n \t\t\tpython_objects.push_back(py::reinterpret_steal<py::object>(ret));\n \t\t\tpython_results[row] = ret;\n@@ -304,7 +419,8 @@ struct PythonUDFData {\n \t\tauto signature = GetSignature(udf);\n \t\tauto sig_params = signature.attr(\"parameters\");\n \t\tauto return_annotation = signature.attr(\"return_annotation\");\n-\t\tif (!py::none().is(return_annotation)) {\n+\t\tauto empty = py::module_::import(\"inspect\").attr(\"Signature\").attr(\"empty\");\n+\t\tif (!py::none().is(return_annotation) && !empty.is(return_annotation)) {\n \t\t\tshared_ptr<DuckDBPyType> pytype;\n \t\t\tif (py::try_cast<shared_ptr<DuckDBPyType>>(return_annotation, pytype)) {\n \t\t\t\treturn_type = pytype->Type();\n@@ -338,9 +454,9 @@ struct PythonUDFData {\n \n \t\tscalar_function_t func;\n \t\tif (vectorized) {\n-\t\t\tfunc = CreateVectorizedFunction(udf.ptr(), exception_handling);\n+\t\t\tfunc = CreateVectorizedFunction(udf.ptr(), exception_handling, null_handling);\n \t\t} else {\n-\t\t\tfunc = CreateNativeFunction(udf.ptr(), exception_handling, client_properties);\n+\t\t\tfunc = CreateNativeFunction(udf.ptr(), exception_handling, client_properties, null_handling);\n \t\t}\n \t\tFunctionStability function_side_effects =\n \t\t    side_effects ? FunctionStability::VOLATILE : FunctionStability::CONSISTENT;\ndiff --git a/tools/pythonpkg/src/typing/pytype.cpp b/tools/pythonpkg/src/typing/pytype.cpp\nindex 39172a12eb4a..7b03f167121c 100644\n--- a/tools/pythonpkg/src/typing/pytype.cpp\n+++ b/tools/pythonpkg/src/typing/pytype.cpp\n@@ -127,6 +127,9 @@ static bool FromNumpyType(const py::object &type, LogicalType &result) {\n \tauto obj = type();\n \t// We convert these to string because the underlying physical\n \t// types of a numpy type aren't consistent on every platform\n+\tif (!py::hasattr(obj, \"dtype\")) {\n+\t\treturn false;\n+\t}\n \tstring type_str = py::str(obj.attr(\"dtype\"));\n \tif (type_str == \"bool\") {\n \t\tresult = LogicalType::BOOLEAN;\n@@ -185,7 +188,7 @@ static LogicalType FromType(const py::type &obj) {\n \t\treturn result;\n \t}\n \n-\tthrow py::type_error(\"Could not convert from unknown 'type' to DuckDBPyType\");\n+\tthrow py::cast_error(\"Could not convert from unknown 'type' to DuckDBPyType\");\n }\n \n static bool IsMapType(const py::tuple &args) {\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/udf/test_null_filtering.py b/tools/pythonpkg/tests/fast/udf/test_null_filtering.py\nnew file mode 100644\nindex 000000000000..f7bdb828fd79\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/udf/test_null_filtering.py\n@@ -0,0 +1,237 @@\n+import duckdb\n+import pytest\n+\n+pd = pytest.importorskip(\"pandas\")\n+pa = pytest.importorskip(\"pyarrow\")\n+from typing import Union\n+import pyarrow.compute as pc\n+import uuid\n+import datetime\n+import numpy as np\n+import cmath\n+from typing import NamedTuple, Any, List\n+\n+from duckdb.typing import *\n+\n+\n+class Candidate(NamedTuple):\n+    type: duckdb.typing.DuckDBPyType\n+    variant_one: Any\n+    variant_two: Any\n+\n+\n+def layout(index: int):\n+    return [\n+        ['x', 'x', 'y'],\n+        ['x', None, 'y'],\n+        [None, 'y', None],\n+        ['x', None, None],\n+        [None, None, 'y'],\n+        [None, None, None],\n+    ][index]\n+\n+\n+def get_table_data():\n+    def add_variations(data, index: int):\n+        data.extend(\n+            [\n+                {\n+                    'a': layout(index),\n+                    'b': layout(0),\n+                    'c': layout(0),\n+                },\n+                {\n+                    'a': layout(0),\n+                    'b': layout(0),\n+                    'c': layout(index),\n+                },\n+            ]\n+        )\n+\n+    data = []\n+    add_variations(data, 1)\n+    add_variations(data, 2)\n+    add_variations(data, 3)\n+    add_variations(data, 4)\n+    add_variations(data, 5)\n+    return data\n+\n+\n+def get_types():\n+    return [\n+        Candidate(TINYINT, -42, -21),\n+        Candidate(SMALLINT, -512, -256),\n+        Candidate(INTEGER, -131072, -65536),\n+        Candidate(\n+            BIGINT,\n+            -17179869184,\n+            -8589934592,\n+        ),\n+        Candidate(\n+            UTINYINT,\n+            254,\n+            127,\n+        ),\n+        Candidate(\n+            USMALLINT,\n+            65535,\n+            32767,\n+        ),\n+        Candidate(\n+            UINTEGER,\n+            4294967295,\n+            2147483647,\n+        ),\n+        Candidate(UBIGINT, 18446744073709551615, 9223372036854776000),\n+        Candidate(VARCHAR, 'long_string_test', 'smallstring'),\n+        Candidate(\n+            UUID, uuid.UUID('ffffffff-ffff-ffff-ffff-ffffffffffff'), uuid.UUID('ffffffff-ffff-ffff-ffff-000000000000')\n+        ),\n+        Candidate(\n+            FLOAT,\n+            0.12246409803628922,\n+            0.24492819607257843,\n+        ),\n+        Candidate(\n+            DOUBLE,\n+            123142.12312416293784721232344,\n+            246284.2462483259,\n+        ),\n+        Candidate(DATE, datetime.date(2005, 3, 11), datetime.date(1989, 1, 7)),\n+        Candidate(TIMESTAMP, datetime.datetime(2009, 2, 13, 11, 5, 53), datetime.datetime(1989, 11, 20, 5, 3, 25)),\n+        Candidate(\n+            TIME,\n+            datetime.time(14, 1, 12),\n+            datetime.time(6, 3, 6),\n+        ),\n+        Candidate(\n+            BLOB,\n+            b'\\xF6\\x96\\xB0\\x85',\n+            b'\\x85\\xB0\\x96\\xF6',\n+        ),\n+        Candidate(\n+            INTERVAL,\n+            datetime.timedelta(days=30969, seconds=999, microseconds=999999),\n+            datetime.timedelta(days=786, seconds=53, microseconds=651),\n+        ),\n+        Candidate(\n+            BOOLEAN,\n+            True,\n+            False,\n+        ),\n+        Candidate(\n+            duckdb.struct_type(['BIGINT[]', 'VARCHAR[]']),\n+            {'v1': [1, 2, 3], 'v2': ['a', 'non-inlined string', 'duckdb']},\n+            {'v1': [5, 4, 3, 2, 1], 'v2': ['non-inlined-string', 'a', 'b', 'c', 'duckdb']},\n+        ),\n+        Candidate(duckdb.list_type('VARCHAR'), ['the', 'duck', 'non-inlined string'], ['non-inlined-string', 'test']),\n+    ]\n+\n+\n+def construct_query(tuples) -> str:\n+    def construct_values_list(row, start_param_idx):\n+        parameter_count = len(row)\n+        parameters = [f'${x+start_param_idx}' for x in range(parameter_count)]\n+        parameters = '(' + ', '.join(parameters) + ')'\n+        return parameters\n+\n+    row_size = len(tuples[0])\n+    values_list = [construct_values_list(x, 1 + (i * row_size)) for i, x in enumerate(tuples)]\n+    values_list = ', '.join(values_list)\n+\n+    query = f\"\"\"\n+        select * from (values {values_list})\n+    \"\"\"\n+    return query\n+\n+\n+def construct_parameters(tuples, dbtype):\n+    parameters = []\n+    for row in tuples:\n+        parameters.extend(list([duckdb.Value(x, dbtype) for x in row]))\n+    return parameters\n+\n+\n+class TestUDFNullFiltering(object):\n+    @pytest.mark.parametrize(\n+        'table_data',\n+        get_table_data(),\n+    )\n+    @pytest.mark.parametrize(\n+        'test_type',\n+        get_types(),\n+    )\n+    @pytest.mark.parametrize('udf_type', ['arrow', 'native'])\n+    def test_null_filtering(self, duckdb_cursor, table_data: dict, test_type: Candidate, udf_type):\n+        null_count = sum([1 for x in list(zip(*table_data.values())) if any([y == None for y in x])])\n+        row_count = len(table_data)\n+        table_data = {\n+            key: [None if not x else test_type.variant_one if x == 'x' else test_type.variant_two for x in value]\n+            for key, value in table_data.items()\n+        }\n+\n+        tuples = list(zip(*table_data.values()))\n+        query = construct_query(tuples)\n+        parameters = construct_parameters(tuples, test_type.type)\n+        rel = duckdb_cursor.sql(query + \" t(a, b, c)\", params=parameters)\n+        rel.to_table('tbl')\n+        rel.show()\n+\n+        def my_func(*args):\n+            if udf_type == 'arrow':\n+                my_func.count += len(args[0])\n+            else:\n+                my_func.count += 1\n+            return args[0]\n+\n+        def create_parameters(table_data, dbtype):\n+            return \", \".join(f'{key}::{dbtype}' for key in list(table_data.keys()))\n+\n+        my_func.count = 0\n+        duckdb_cursor.create_function('test', my_func, None, test_type.type, type=udf_type)\n+        query = f\"select test({create_parameters(table_data, test_type.type)}) from tbl\"\n+        result = duckdb_cursor.sql(query).fetchall()\n+\n+        expected_output = [\n+            (t[0],) if not any(x == None for x in t) else (None,) for t in list(zip(*table_data.values()))\n+        ]\n+        assert result == expected_output\n+        assert len(result) == row_count\n+        # Only the non-null tuples should have been seen by the UDF\n+        assert my_func.count == row_count - null_count\n+\n+    @pytest.mark.parametrize(\n+        'table_data',\n+        [\n+            [1, 2, 3, 4],\n+            [1, 2, None, 4],\n+        ],\n+    )\n+    def test_nulls_from_default_null_handling_native(self, duckdb_cursor, table_data):\n+        def returns_null(x):\n+            return None\n+\n+        df = pd.DataFrame({'a': table_data})\n+        duckdb_cursor.execute(\"create table tbl as select * from df\")\n+        duckdb_cursor.create_function('test', returns_null, [str], int, type='native')\n+        with pytest.raises(duckdb.InvalidInputException, match='The UDF is not expected to return NULL values'):\n+            result = duckdb_cursor.sql(\"select test(a::VARCHAR) from tbl\").fetchall()\n+\n+    @pytest.mark.parametrize(\n+        'table_data',\n+        [\n+            [1, 2, 3, 4],\n+            [1, 2, None, 4],\n+        ],\n+    )\n+    def test_nulls_from_default_null_handling_arrow(self, duckdb_cursor, table_data):\n+        def returns_null(x):\n+            l = x.to_pylist()\n+            return pa.array([None for _ in l], type=pa.int64())\n+\n+        df = pd.DataFrame({'a': table_data})\n+        duckdb_cursor.execute(\"create table tbl as select * from df\")\n+        duckdb_cursor.create_function('test', returns_null, [str], int, type='arrow')\n+        with pytest.raises(duckdb.InvalidInputException, match='The UDF is not expected to return NULL values'):\n+            result = duckdb_cursor.sql(\"select test(a::VARCHAR) from tbl\").fetchall()\n+            print(result)\n",
  "problem_statement": "python UDF: NULLs intercepted for scalars but not columnar args\n### What happens?\n\nAccording to https://duckdb.org/docs/api/python/function.html#null-handling, if you pass a NULL into a python UDF, duckdb will short-circuit and immediately resolve to NULL, and won't ever call the python with None.\r\n\r\nThis short-circuiting appears to happen when the input is a scalar, but doesn't when it is a column.\r\n\r\nPS I think this short-circuiting (with a configurable override) is fantastic UX\n\n### To Reproduce\n\n```python\r\nimport duckdb\r\nfrom duckdb.typing import INTEGER\r\n\r\n\r\ndef add_one(x):\r\n    return x + 1\r\n\r\n\r\nduckdb.create_function(\"add_one\", add_one, [INTEGER], INTEGER, type=\"native\")\r\nduckdb.sql(\"SELECT add_one(1), add_one(NULL)\").fetchall()\r\n# [(2, None)]\r\nduckdb.sql(\"SELECT add_one(x) FROM (SELECT UNNEST([1, NULL]) as x)\").fetchall()\r\n# Invalid Input Error: Python exception occurred while executing the UDF: TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\r\n```\n\n### OS:\n\nMacM1, MacOS 14.5\n\n### DuckDB Version:\n\n1.0.0\n\n### DuckDB Client:\n\npython\n\n### Full Name:\n\nNick Crews\n\n### Affiliation:\n\nShip Creek Group\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n",
  "hints_text": "I was testing the very same thing today and noticed the same thing for user defined scalar functions. It works if you explicitly pass NULL as one of the arguments but doesn't if the NULL value is passed as a column value.\nRepro with C API. I used 1.0.1-dev3196 as scalar functions aren't available in v1.\r\n\r\n```c\r\nstate = duckdb_query(connection, \"CREATE TABLE big_table_1 AS SELECT (greatest(random(), 0.1) * 1000)::BIGINT i FROM range(10) t(i);\", &result);\r\nstate = duckdb_query(connection, \"Insert into big_table_1 values (NULL);\", &result);\r\n\r\nauto function = duckdb_create_scalar_function();\r\nduckdb_scalar_function_set_name(function, \"my_null_count\");\r\n\r\n// set the variable arguments\r\nauto any_type = duckdb_create_logical_type(DUCKDB_TYPE_ANY);\r\nduckdb_scalar_function_set_varargs(function, any_type);\r\nduckdb_destroy_logical_type(&any_type);\r\n\r\n// set the return type uto bigint\r\nauto return_type = duckdb_create_logical_type(DUCKDB_TYPE_UBIGINT);\r\nduckdb_scalar_function_set_return_type(function, return_type);\r\nduckdb_destroy_logical_type(&return_type);\r\n\r\n// set up the function\r\nduckdb_scalar_function_set_function(function, CountNULLValues);\r\n\r\n// register and cleanup\r\nstate = duckdb_register_scalar_function(connection, function);\r\nduckdb_destroy_scalar_function(&function);\r\n\r\nstate = duckdb_query(connection, \"Select my_null_count(i, 42) from big_table_1\", &result);\r\n\r\nvoid CountNULLValues(duckdb_function_info, duckdb_data_chunk input, duckdb_vector output) {\r\n\r\n\t// Get the total number of rows and columns in this chunk.\r\n\tauto input_size = duckdb_data_chunk_get_size(input);\r\n\tauto column_count = duckdb_data_chunk_get_column_count(input);\r\n\r\n\t// Extract the validity masks.\r\n\tstd::vector<uint64_t*> validity_masks;\r\n\tfor (idx_t col_idx = 0; col_idx < column_count; col_idx++) {\r\n\t\tauto col = duckdb_data_chunk_get_vector(input, col_idx);\r\n\t\tauto validity_mask = duckdb_vector_get_validity(col);\r\n\t\tvalidity_masks.push_back(validity_mask);\r\n\t}\r\n\r\n\t// Execute the function.\r\n\tauto result_data = (uint64_t*)duckdb_vector_get_data(output);\r\n\tfor (idx_t row_idx = 0; row_idx < input_size; row_idx++) {\r\n\t\tidx_t null_count = 0;\r\n\t\tfor (idx_t col_idx = 0; col_idx < column_count; col_idx++) {\r\n\t\t\tif (!duckdb_validity_row_is_valid(validity_masks[col_idx], row_idx)) {\r\n\t\t\t\tnull_count++;\r\n\t\t\t}\r\n\t\t}\r\n\t\tresult_data[row_idx] = null_count;\r\n\t}\r\n}\r\n```\r\n\r\nWhen `CountNULLValues` is executed `input_size` is 11: 10 rows in the table without NULL and one row with NULL\nThis is expected behavior, constant NULLs can be short-circuited because we know the input is NULL, a column can contain anything, we won't know what it returns until runtime\nIf the custom UDF still has to handle NULL input, what's the point of short-circuiting?\nThe initial PR adding this: <https://github.com/duckdb/duckdb/pull/3866>\r\nThe changelist is quite small, on `main` the relevant code acting on this that I could find is this:\r\n```c++\r\n    if (bound_function.null_handling == FunctionNullHandling::DEFAULT_NULL_HANDLING) {\r\n        for (auto &child : children) {\r\n            if (child->return_type == LogicalTypeId::SQLNULL) {\r\n                return make_uniq<BoundConstantExpression>(Value(return_type_if_null));\r\n            }\r\n            if (!child->IsFoldable()) {\r\n                continue;\r\n            }\r\n            Value result;\r\n            if (!ExpressionExecutor::TryEvaluateScalar(context, *child, result)) {\r\n                continue;\r\n            }\r\n            if (result.IsNull()) {\r\n                return make_uniq<BoundConstantExpression>(Value(return_type_if_null));\r\n            }\r\n        }\r\n    }\r\n```\nI see two potential benefits of short-circuiting:\r\n1. UX, the python programmer doesn't have to worry about this case.\r\n2. performance, we can avoid slow python computation in some cases\r\n\r\nIf we only short-circuit on scalars, we lose the benefit of 1. We almost entirely lose the benefit of 2 as well, since I think it is reasonable to assume that calling a UDF with a column of N elements is O(N), whereas calling with a scalar is O(1), so we are only getting a performance boost in the O(1) case, which is going to only account for a tiny fraction of a queries runtime.\r\n\r\nSo, unless we can short-circuit in the columnar case too, I think we should remove the complexity and NEVER short-circuit.\r\n\r\nThe other thing confounding this is that short-circuiting a python UDF is probably much easier than short-circuiting a pyarrow UDF. But ideally these two flavors should have the same behavior. So unless we can get consistent behavior, I think we should err on the side of usability/simplicity and never short-circuit.",
  "created_at": "2024-08-08T13:23:18Z"
}