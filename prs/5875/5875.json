{
  "repo": "duckdb/duckdb",
  "pull_number": 5875,
  "instance_id": "duckdb__duckdb-5875",
  "issue_numbers": [
    "5779",
    "5779"
  ],
  "base_commit": "6fab1929615ff13f6fd23072d172096a00f2a1d1",
  "patch": "diff --git a/extension/parquet/column_writer.cpp b/extension/parquet/column_writer.cpp\nindex 340f31eaa505..630171e3a633 100644\n--- a/extension/parquet/column_writer.cpp\n+++ b/extension/parquet/column_writer.cpp\n@@ -1723,6 +1723,38 @@ void ListColumnWriter::FinalizeAnalyze(ColumnWriterState &state_p) {\n \tchild_writer->FinalizeAnalyze(*state.child_state);\n }\n \n+idx_t GetConsecutiveChildList(Vector &list, idx_t count, Vector &result) {\n+\tauto list_data = FlatVector::GetData<list_entry_t>(list);\n+\tauto &validity = FlatVector::Validity(list);\n+\tbool consecutive_flat_list = true;\n+\tidx_t child_count = 0;\n+\tfor (idx_t i = 0; i < count; i++) {\n+\t\tif (!validity.RowIsValid(i)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (list_data[i].offset != child_count) {\n+\t\t\tconsecutive_flat_list = false;\n+\t\t}\n+\t\tchild_count += list_data[i].length;\n+\t}\n+\tif (!consecutive_flat_list) {\n+\t\tSelectionVector child_sel(child_count);\n+\t\tidx_t entry = 0;\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tif (!validity.RowIsValid(i)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tfor (idx_t k = 0; k < list_data[i].length; k++) {\n+\t\t\t\tchild_sel.set_index(entry++, list_data[i].offset + k);\n+\t\t\t}\n+\t\t}\n+\n+\t\tresult.Slice(child_sel, child_count);\n+\t\tresult.Flatten(child_count);\n+\t}\n+\treturn child_count;\n+}\n+\n void ListColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {\n \tauto &state = (ListColumnWriterState &)state_p;\n \n@@ -1775,8 +1807,9 @@ void ListColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *pa\n \tstate.parent_index += vcount;\n \n \tauto &list_child = ListVector::GetEntry(vector);\n-\tauto list_count = ListVector::GetListSize(vector);\n-\tchild_writer->Prepare(*state.child_state, &state_p, list_child, list_count);\n+\tVector child_list(list_child);\n+\tidx_t child_count = GetConsecutiveChildList(vector, count, child_list);\n+\tchild_writer->Prepare(*state.child_state, &state_p, child_list, child_count);\n }\n \n void ListColumnWriter::BeginWrite(ColumnWriterState &state_p) {\n@@ -1788,8 +1821,9 @@ void ListColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t c\n \tauto &state = (ListColumnWriterState &)state_p;\n \n \tauto &list_child = ListVector::GetEntry(vector);\n-\tauto list_count = ListVector::GetListSize(vector);\n-\tchild_writer->Write(*state.child_state, list_child, list_count);\n+\tVector child_list(list_child);\n+\tidx_t child_count = GetConsecutiveChildList(vector, count, child_list);\n+\tchild_writer->Write(*state.child_state, child_list, child_count);\n }\n \n void ListColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {\n",
  "test_patch": "diff --git a/test/sql/copy/parquet/writer/parquet_write_issue_5779.test b/test/sql/copy/parquet/writer/parquet_write_issue_5779.test\nnew file mode 100644\nindex 000000000000..cd3c669907f8\n--- /dev/null\n+++ b/test/sql/copy/parquet/writer/parquet_write_issue_5779.test\n@@ -0,0 +1,74 @@\n+# name: test/sql/copy/parquet/writer/parquet_write_issue_5779.test\n+# description: Fix #5779: write subsection of list vector to Parquet\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+CREATE TABLE empty_lists(i INTEGER[]);\n+\n+statement ok\n+INSERT INTO empty_lists SELECT [] FROM range(10) UNION ALL SELECT [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n+\n+statement ok\n+COPY (SELECT * FROM empty_lists LIMIT 10) TO '__TEST_DIR__/emptylist_int.parquet';\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/emptylist_int.parquet'\n+----\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+\n+statement ok\n+CREATE TABLE empty_lists_varchar(i VARCHAR[]);\n+\n+statement ok\n+INSERT INTO empty_lists_varchar SELECT [] FROM range(10) UNION ALL SELECT ['hello', 'world', 'this', 'is', 'a', 'varchar', 'list']\n+\n+statement ok\n+COPY (SELECT * FROM empty_lists_varchar LIMIT 10) TO '__TEST_DIR__/emptylist_varchar.parquet';\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/emptylist_varchar.parquet'\n+----\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+\n+statement ok\n+CREATE TABLE empty_list_nested(i INT[][]);\n+\n+statement ok\n+INSERT INTO empty_list_nested SELECT [] FROM range(10) UNION ALL SELECT [[1, 2, 3], [4, 5], [6, 7, 8]]\n+\n+statement ok\n+COPY (SELECT * FROM empty_list_nested LIMIT 10) TO '__TEST_DIR__/empty_list_nested.parquet';\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/empty_list_nested.parquet'\n+----\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n+[]\n\\ No newline at end of file\n",
  "problem_statement": "\"INTERNAL Error: Writes are not correctly aligned!?\" when copying parquet\n### What happens?\n\nWhen running the following query in a `duckdb` shell, it exits with an error:\r\n``` sql\r\nCOPY (SELECT * FROM '/path/to/a.parquet') TO '/path/to/b.parquet' (FORMAT 'parquet', CODEC 'zstd')\r\n```\r\nError: `Error: INTERNAL Error: Writes are not correctly aligned!?`\n\n### To Reproduce\n\nThere is something about `/path/to/a.parquet` in particular that is causing this issue. It is a 1,800,000-row, 34-column dataset with a mixture of column types, that was itself generated as the result of a DuckDB query of the same general format (`COPY (...) TO '/path/to/a.parquet' (FORMAT 'parquet', CODEC 'zstd')`)\r\n\r\nI have been unable to find a short reproducer. Fortunately the file (~18MB) is only mildly sensitive (as in, I'd rather it not be floating around the internet, but it does not contain customer data), so I'm happy sharing it with folks who want to take a whack at debugging it. \n\n### OS:\n\nMac 13.1, M1\n\n### DuckDB Version:\n\n0.6.1\n\n### DuckDB Client:\n\nshell\n\n### Full Name:\n\nCarl Jackson\n\n### Affiliation:\n\nWatershed\n\n### Have you tried this on the latest `master` branch?\n\n- [ ] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n\"INTERNAL Error: Writes are not correctly aligned!?\" when copying parquet\n### What happens?\n\nWhen running the following query in a `duckdb` shell, it exits with an error:\r\n``` sql\r\nCOPY (SELECT * FROM '/path/to/a.parquet') TO '/path/to/b.parquet' (FORMAT 'parquet', CODEC 'zstd')\r\n```\r\nError: `Error: INTERNAL Error: Writes are not correctly aligned!?`\n\n### To Reproduce\n\nThere is something about `/path/to/a.parquet` in particular that is causing this issue. It is a 1,800,000-row, 34-column dataset with a mixture of column types, that was itself generated as the result of a DuckDB query of the same general format (`COPY (...) TO '/path/to/a.parquet' (FORMAT 'parquet', CODEC 'zstd')`)\r\n\r\nI have been unable to find a short reproducer. Fortunately the file (~18MB) is only mildly sensitive (as in, I'd rather it not be floating around the internet, but it does not contain customer data), so I'm happy sharing it with folks who want to take a whack at debugging it. \n\n### OS:\n\nMac 13.1, M1\n\n### DuckDB Version:\n\n0.6.1\n\n### DuckDB Client:\n\nshell\n\n### Full Name:\n\nCarl Jackson\n\n### Affiliation:\n\nWatershed\n\n### Have you tried this on the latest `master` branch?\n\n- [ ] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "FWIW, the error in the Node bindings was `Assertion failed: (write_count > 0), function Write, file parquet-amalgamation.cpp, line 5257.`\nThanks for the report! Could you send the file to my email (mark@duckdblabs.com)? Happy to take a stab at it.\nFWIW, the error in the Node bindings was `Assertion failed: (write_count > 0), function Write, file parquet-amalgamation.cpp, line 5257.`\nThanks for the report! Could you send the file to my email (mark@duckdblabs.com)? Happy to take a stab at it.",
  "created_at": "2023-01-10T12:02:27Z"
}