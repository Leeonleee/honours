You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Unit tests of built Python package fail on OSX (in conda-forge build)
### What happens?

Running the unit tests with a built `python-duckdb` on conda-forge fails with a segmentation fault in the `tools/pythonpkg/tests/fast/test_filesystem.py::TestPythonFilesystem::test_database_attach`

See the following stack trace:

```
tools/pythonpkg/tests/fast/test_filesystem.py::TestPythonFilesystem::test_database_attach Assertion failed: (!py::gil_check()), function Seek, file src/pyfilesystem.cpp, line 156.
Process 42452 stopped
* thread #1, queue = 'com.apple.main-thread', stop reason = hit program assert
    frame #4: 0x0000000106c640ec duckdb.cpython-311-darwin.so`duckdb::PythonFilesystem::Seek(duckdb::FileHandle&, unsigned long long) + 212
duckdb.cpython-311-darwin.so`duckdb::PythonFilesystem::Seek:
->  0x106c640ec <+212>: mov    w0, #0x1 ; =1
    0x106c640f0 <+216>: bl     0x106cc0fc0    ; symbol stub for: PyErr_PrintEx
    0x106c640f4 <+220>: mov    w0, #0x10 ; =16
    0x106c640f8 <+224>: bl     0x106cc1bd8    ; symbol stub for: __cxa_allocate_exception
Target 0: (python) stopped.
(lldb) bt all
* thread #1, queue = 'com.apple.main-thread', stop reason = hit program assert
    frame #0: 0x000000018a0655d0 libsystem_kernel.dylib`__pthread_kill + 8
    frame #1: 0x000000018a09dc20 libsystem_pthread.dylib`pthread_kill + 288
    frame #2: 0x0000000189faaa30 libsystem_c.dylib`abort + 180
    frame #3: 0x0000000189fa9d20 libsystem_c.dylib`__assert_rtn + 284
  * frame #4: 0x0000000106c640ec duckdb.cpython-311-darwin.so`duckdb::PythonFilesystem::Seek(duckdb::FileHandle&, unsigned long long) + 212
    frame #5: 0x0000000106c635cc duckdb.cpython-311-darwin.so`duckdb::PythonFilesystem::Write(duckdb::FileHandle&, void*, long long, unsigned long long) + 48
    frame #6: 0x0000000106a4f238 duckdb.cpython-311-darwin.so`duckdb::BlockManager::ConvertToPersistent(long long, duckdb::shared_ptr<duckdb::BlockHandle, true>) + 668
    frame #7: 0x0000000106b19c8c duckdb.cpython-311-darwin.so`duckdb::ColumnSegment::ConvertToPersistent(duckdb::optional_ptr<duckdb::BlockManager, true>, long long) + 124
    frame #8: 0x0000000106b1991c duckdb.cpython-311-darwin.so`duckdb::PartialBlockForCheckpoint::Flush(unsigned long long) + 360
    frame #9: 0x0000000106a11f04 duckdb.cpython-311-darwin.so`duckdb::SingleFileCheckpointWriter::WriteTable(duckdb::TableCatalogEntry&, duckdb::Serializer&) + 444
    frame #10: 0x0000000106a0f308 duckdb.cpython-311-darwin.so`duckdb::SingleFileCheckpointWriter::CreateCheckpoint() + 2656
    frame #11: 0x0000000106a27a24 duckdb.cpython-311-darwin.so`duckdb::SingleFileStorageManager::CreateCheckpoint(duckdb::CheckpointOptions) + 484
    frame #12: 0x00000001066ef59c duckdb.cpython-311-darwin.so`duckdb::AttachedDatabase::Close() + 304
    frame #13: 0x000000010670865c duckdb.cpython-311-darwin.so`duckdb::DatabaseManager::ResetDatabases(duckdb::unique_ptr<duckdb::TaskScheduler, std::__1::default_delete<duckdb::TaskScheduler>, true>&) + 156
    frame #14: 0x0000000106708370 duckdb.cpython-311-darwin.so`duckdb::DatabaseInstance::~DatabaseInstance() + 40
    frame #15: 0x00000001067741d4 duckdb.cpython-311-darwin.so`std::__1::__shared_ptr_emplace<duckdb::DuckDB, std::__1::allocator<duckdb::DuckDB>>::__on_zero_shared() + 64
    frame #16: 0x0000000106c1ca1c duckdb.cpython-311-darwin.so`duckdb::DuckDBPyConnection::Close() + 100
    frame #17: 0x0000000106c1c7f4 duckdb.cpython-311-darwin.so`duckdb::DuckDBPyConnection::Exit(duckdb::DuckDBPyConnection&, pybind11::object const&, pybind11::object const&, pybind11::object const&) + 24
    frame #18: 0x0000000106c4d92c duckdb.cpython-311-darwin.so`void pybind11::cpp_function::initialize<void (*&)(duckdb::DuckDBPyConnection&, pybind11::object const&, pybind11::object const&, pybind11::object const&), void, duckdb::DuckDBPyConnection&, pybind11::object const&, pybind11::object const&, pybind11::object const&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg, pybind11::arg>(void (*&)(duckdb::DuckDBPyConnection&, pybind11::object const&, pybind11::object const&, pybind11::object const&), void (*)(duckdb::DuckDBPyConnection&, pybind11::object const&, pybind11::object const&, pybind11::object const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg const&, pybind11::arg const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const + 140
    frame #19: 0x0000000106c4d878 duckdb.cpython-311-darwin.so`void pybind11::cpp_function::initialize<void (*&)(duckdb::DuckDBPyConnection&, pybind11::object const&, pybind11::object const&, pybind11::object const&), void, duckdb::DuckDBPyConnection&, pybind11::object const&, pybind11::object const&, pybind11::object const&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg, pybind11::arg>(void (*&)(duckdb::DuckDBPyConnection&, pybind11::object const&, pybind11::object const&, pybind11::object const&), void (*)(duckdb::DuckDBPyConnection&, pybind11::object const&, pybind11::object const&, pybind11::object const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg const&, pybind11::arg const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 40
    frame #20: 0x0000000106b755a8 duckdb.cpython-311-darwin.so`pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 4312
    frame #21: 0x00000001000b7f4c python`cfunction_call + 124
    frame #22: 0x00000001000606f0 python`_PyObject_MakeTpCall + 332
    frame #23: 0x0000000100064200 python`method_vectorcall + 276
    frame #24: 0x0000000100060ffc python`PyObject_Vectorcall + 76
    frame #25: 0x0000000100162e3c python`_PyEval_EvalFrameDefault + 46484
    frame #26: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #27: 0x0000000100064198 python`method_vectorcall + 172
    frame #28: 0x0000000100060e68 python`_PyVectorcall_Call + 132
    frame #29: 0x0000000100164dac python`_PyEval_EvalFrameDefault + 54532
    frame #30: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #31: 0x0000000100164dac python`_PyEval_EvalFrameDefault + 54532
    frame #32: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #33: 0x00000001000608b8 python`_PyObject_FastCallDictTstate + 156
    frame #34: 0x00000001000617f0 python`_PyObject_Call_Prepend + 176
    frame #35: 0x00000001000db278 python`slot_tp_call + 172
    frame #36: 0x00000001000606f0 python`_PyObject_MakeTpCall + 332
    frame #37: 0x0000000100162e3c python`_PyEval_EvalFrameDefault + 46484
    frame #38: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #39: 0x0000000100164dac python`_PyEval_EvalFrameDefault + 54532
    frame #40: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #41: 0x00000001000608b8 python`_PyObject_FastCallDictTstate + 156
    frame #42: 0x00000001000617f0 python`_PyObject_Call_Prepend + 176
    frame #43: 0x00000001000db278 python`slot_tp_call + 172
    frame #44: 0x000000010006126c python`_PyObject_Call + 236
    frame #45: 0x0000000100164dac python`_PyEval_EvalFrameDefault + 54532
    frame #46: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #47: 0x0000000100164dac python`_PyEval_EvalFrameDefault + 54532
    frame #48: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #49: 0x0000000100164dac python`_PyEval_EvalFrameDefault + 54532
    frame #50: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #51: 0x00000001000608b8 python`_PyObject_FastCallDictTstate + 156
    frame #52: 0x00000001000617f0 python`_PyObject_Call_Prepend + 176
    frame #53: 0x00000001000db278 python`slot_tp_call + 172
    frame #54: 0x00000001000606f0 python`_PyObject_MakeTpCall + 332
    frame #55: 0x0000000100162e3c python`_PyEval_EvalFrameDefault + 46484
    frame #56: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #57: 0x0000000100164dac python`_PyEval_EvalFrameDefault + 54532
    frame #58: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #59: 0x00000001000608b8 python`_PyObject_FastCallDictTstate + 156
    frame #60: 0x00000001000617f0 python`_PyObject_Call_Prepend + 176
    frame #61: 0x00000001000db278 python`slot_tp_call + 172
    frame #62: 0x00000001000606f0 python`_PyObject_MakeTpCall + 332
    frame #63: 0x0000000100162e3c python`_PyEval_EvalFrameDefault + 46484
    frame #64: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #65: 0x0000000100164dac python`_PyEval_EvalFrameDefault + 54532
    frame #66: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #67: 0x00000001000608b8 python`_PyObject_FastCallDictTstate + 156
    frame #68: 0x00000001000617f0 python`_PyObject_Call_Prepend + 176
    frame #69: 0x00000001000db278 python`slot_tp_call + 172
    frame #70: 0x00000001000606f0 python`_PyObject_MakeTpCall + 332
    frame #71: 0x0000000100162e3c python`_PyEval_EvalFrameDefault + 46484
    frame #72: 0x00000001001568f4 python`PyEval_EvalCode + 220
    frame #73: 0x00000001001526a4 python`builtin_exec + 1156
    frame #74: 0x00000001000b8c68 python`cfunction_vectorcall_FASTCALL_KEYWORDS + 168
    frame #75: 0x0000000100060ffc python`PyObject_Vectorcall + 76
    frame #76: 0x0000000100162e3c python`_PyEval_EvalFrameDefault + 46484
    frame #77: 0x00000001001674c8 python`_PyEval_Vector + 184
    frame #78: 0x00000001001e2588 python`pymain_run_module + 272
    frame #79: 0x00000001001e1f74 python`Py_RunMain + 2476
    frame #80: 0x00000001001e3050 python`pymain_main + 1252
    frame #81: 0x0000000100003398 python`main + 56
    frame #82: 0x0000000189d13154 dyld`start + 2476
  thread #2
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #3
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #4
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #5
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #6
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #7
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #8
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #9
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #10
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #11
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #12
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #13
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #14
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #15
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #16
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #17
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #18
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #19
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #20
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #21
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #22
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
  thread #23
    frame #0: 0x000000018a05cd50 libsystem_kernel.dylib`semaphore_wait_trap + 8
    frame #1: 0x0000000106898230 duckdb.cpython-311-darwin.so`duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 608
    frame #2: 0x000000010689ecec duckdb.cpython-311-darwin.so`void* std::__1::__thread_proxy[abi:ue170006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 72
    frame #3: 0x000000018a09df94 libsystem_pthread.dylib`_pthread_start + 136
(lldb)
```

### To Reproduce

To reproduce, you can build the conda-forge feedstock from the PR in https://github.com/conda-forge/python-duckdb-feedstock/pull/114 using `conda-build -m .ci_support/osx_arm64_python3.11.____cpython.yaml recipe`

### OS:

OSX

### DuckDB Version:

1.1.1

### DuckDB Client:

Python

### Hardware:

OSX (all architectures)

### Full Name:

Uwe Korn

### Affiliation:

conda-forge (in this case)

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Not applicable - the reproduction does not require a data set

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/pyconnection/pyconnection.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: #include "duckdb_python/arrow/arrow_array_stream.hpp"
11: #include "duckdb.hpp"
12: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
13: #include "duckdb/common/unordered_map.hpp"
14: #include "duckdb_python/import_cache/python_import_cache.hpp"
15: #include "duckdb_python/numpy/numpy_type.hpp"
16: #include "duckdb_python/pyrelation.hpp"
17: #include "duckdb_python/pytype.hpp"
18: #include "duckdb_python/path_like.hpp"
19: #include "duckdb/execution/operator/csv_scanner/csv_reader_options.hpp"
20: #include "duckdb_python/pyfilesystem.hpp"
21: #include "duckdb_python/pybind11/registered_py_object.hpp"
22: #include "duckdb_python/python_dependency.hpp"
23: #include "duckdb/function/scalar_function.hpp"
24: #include "duckdb_python/pybind11/conversions/exception_handling_enum.hpp"
25: #include "duckdb_python/pybind11/conversions/python_udf_type_enum.hpp"
26: #include "duckdb_python/pybind11/conversions/python_csv_line_terminator_enum.hpp"
27: #include "duckdb/common/shared_ptr.hpp"
28: 
29: namespace duckdb {
30: struct BoundParameterData;
31: 
32: enum class PythonEnvironmentType { NORMAL, INTERACTIVE, JUPYTER };
33: 
34: struct DuckDBPyRelation;
35: 
36: class RegisteredArrow : public RegisteredObject {
37: 
38: public:
39: 	RegisteredArrow(unique_ptr<PythonTableArrowArrayStreamFactory> arrow_factory_p, py::object obj_p)
40: 	    : RegisteredObject(std::move(obj_p)), arrow_factory(std::move(arrow_factory_p)) {};
41: 	unique_ptr<PythonTableArrowArrayStreamFactory> arrow_factory;
42: };
43: 
44: struct ConnectionGuard {
45: public:
46: 	ConnectionGuard() {
47: 	}
48: 	~ConnectionGuard() {
49: 	}
50: 
51: public:
52: 	DuckDB &GetDatabase() {
53: 		if (!database) {
54: 			ThrowConnectionException();
55: 		}
56: 		return *database;
57: 	}
58: 	const DuckDB &GetDatabase() const {
59: 		if (!database) {
60: 			ThrowConnectionException();
61: 		}
62: 		return *database;
63: 	}
64: 	Connection &GetConnection() {
65: 		if (!connection) {
66: 			ThrowConnectionException();
67: 		}
68: 		return *connection;
69: 	}
70: 	const Connection &GetConnection() const {
71: 		if (!connection) {
72: 			ThrowConnectionException();
73: 		}
74: 		return *connection;
75: 	}
76: 	DuckDBPyRelation &GetResult() {
77: 		if (!result) {
78: 			ThrowConnectionException();
79: 		}
80: 		return *result;
81: 	}
82: 	const DuckDBPyRelation &GetResult() const {
83: 		if (!result) {
84: 			ThrowConnectionException();
85: 		}
86: 		return *result;
87: 	}
88: 
89: public:
90: 	bool HasResult() const {
91: 		return result != nullptr;
92: 	}
93: 
94: public:
95: 	void SetDatabase(shared_ptr<DuckDB> db) {
96: 		database = std::move(db);
97: 	}
98: 	void SetDatabase(ConnectionGuard &con) {
99: 		if (!con.database) {
100: 			ThrowConnectionException();
101: 		}
102: 		database = con.database;
103: 	}
104: 	void SetConnection(unique_ptr<Connection> con) {
105: 		connection = std::move(con);
106: 	}
107: 	void SetResult(unique_ptr<DuckDBPyRelation> res) {
108: 		result = std::move(res);
109: 	}
110: 
111: private:
112: 	void ThrowConnectionException() const {
113: 		throw ConnectionException("Connection already closed!");
114: 	}
115: 
116: private:
117: 	shared_ptr<DuckDB> database;
118: 	unique_ptr<Connection> connection;
119: 	unique_ptr<DuckDBPyRelation> result;
120: };
121: 
122: struct DuckDBPyConnection : public enable_shared_from_this<DuckDBPyConnection> {
123: private:
124: 	class Cursors {
125: 	public:
126: 		Cursors() {
127: 		}
128: 
129: 	public:
130: 		void AddCursor(shared_ptr<DuckDBPyConnection> conn);
131: 		void ClearCursors();
132: 
133: 	private:
134: 		mutex lock;
135: 		vector<weak_ptr<DuckDBPyConnection>> cursors;
136: 	};
137: 
138: public:
139: 	ConnectionGuard con;
140: 	Cursors cursors;
141: 	std::mutex py_connection_lock;
142: 	//! MemoryFileSystem used to temporarily store file-like objects for reading
143: 	shared_ptr<ModifiedMemoryFileSystem> internal_object_filesystem;
144: 	case_insensitive_map_t<unique_ptr<ExternalDependency>> registered_functions;
145: 	case_insensitive_set_t registered_objects;
146: 
147: public:
148: 	explicit DuckDBPyConnection() {
149: 	}
150: 	~DuckDBPyConnection();
151: 
152: public:
153: 	static void Initialize(py::handle &m);
154: 	static void Cleanup();
155: 
156: 	shared_ptr<DuckDBPyConnection> Enter();
157: 
158: 	static void Exit(DuckDBPyConnection &self, const py::object &exc_type, const py::object &exc,
159: 	                 const py::object &traceback);
160: 
161: 	static bool DetectAndGetEnvironment();
162: 	static bool IsJupyter();
163: 	static shared_ptr<DuckDBPyConnection> DefaultConnection();
164: 	static PythonImportCache *ImportCache();
165: 	static bool IsInteractive();
166: 
167: 	unique_ptr<DuckDBPyRelation> ReadCSV(const py::object &name, py::kwargs &kwargs);
168: 
169: 	py::list ExtractStatements(const string &query);
170: 
171: 	unique_ptr<DuckDBPyRelation> ReadJSON(
172: 	    const py::object &name, const Optional<py::object> &columns = py::none(),
173: 	    const Optional<py::object> &sample_size = py::none(), const Optional<py::object> &maximum_depth = py::none(),
174: 	    const Optional<py::str> &records = py::none(), const Optional<py::str> &format = py::none(),
175: 	    const Optional<py::object> &date_format = py::none(), const Optional<py::object> &timestamp_format = py::none(),
176: 	    const Optional<py::object> &compression = py::none(),
177: 	    const Optional<py::object> &maximum_object_size = py::none(),
178: 	    const Optional<py::object> &ignore_errors = py::none(),
179: 	    const Optional<py::object> &convert_strings_to_integers = py::none(),
180: 	    const Optional<py::object> &field_appearance_threshold = py::none(),
181: 	    const Optional<py::object> &map_inference_threshold = py::none(),
182: 	    const Optional<py::object> &maximum_sample_files = py::none(),
183: 	    const Optional<py::object> &filename = py::none(), const Optional<py::object> &hive_partitioning = py::none(),
184: 	    const Optional<py::object> &union_by_name = py::none(), const Optional<py::object> &hive_types = py::none(),
185: 	    const Optional<py::object> &hive_types_autocast = py::none());
186: 
187: 	shared_ptr<DuckDBPyType> MapType(const shared_ptr<DuckDBPyType> &key_type,
188: 	                                 const shared_ptr<DuckDBPyType> &value_type);
189: 	shared_ptr<DuckDBPyType> StructType(const py::object &fields);
190: 	shared_ptr<DuckDBPyType> ListType(const shared_ptr<DuckDBPyType> &type);
191: 	shared_ptr<DuckDBPyType> ArrayType(const shared_ptr<DuckDBPyType> &type, idx_t size);
192: 	shared_ptr<DuckDBPyType> UnionType(const py::object &members);
193: 	shared_ptr<DuckDBPyType> EnumType(const string &name, const shared_ptr<DuckDBPyType> &type,
194: 	                                  const py::list &values_p);
195: 	shared_ptr<DuckDBPyType> DecimalType(int width, int scale);
196: 	shared_ptr<DuckDBPyType> StringType(const string &collation = string());
197: 	shared_ptr<DuckDBPyType> Type(const string &type_str);
198: 
199: 	shared_ptr<DuckDBPyConnection>
200: 	RegisterScalarUDF(const string &name, const py::function &udf, const py::object &arguments = py::none(),
201: 	                  const shared_ptr<DuckDBPyType> &return_type = nullptr, PythonUDFType type = PythonUDFType::NATIVE,
202: 	                  FunctionNullHandling null_handling = FunctionNullHandling::DEFAULT_NULL_HANDLING,
203: 	                  PythonExceptionHandling exception_handling = PythonExceptionHandling::FORWARD_ERROR,
204: 	                  bool side_effects = false);
205: 
206: 	shared_ptr<DuckDBPyConnection> UnregisterUDF(const string &name);
207: 
208: 	shared_ptr<DuckDBPyConnection> ExecuteMany(const py::object &query, py::object params = py::list());
209: 
210: 	void ExecuteImmediately(vector<unique_ptr<SQLStatement>> statements);
211: 	unique_ptr<PreparedStatement> PrepareQuery(unique_ptr<SQLStatement> statement);
212: 	unique_ptr<QueryResult> ExecuteInternal(PreparedStatement &prep, py::object params = py::list());
213: 
214: 	shared_ptr<DuckDBPyConnection> Execute(const py::object &query, py::object params = py::list());
215: 	shared_ptr<DuckDBPyConnection> ExecuteFromString(const string &query);
216: 
217: 	shared_ptr<DuckDBPyConnection> Append(const string &name, const PandasDataFrame &value, bool by_name);
218: 
219: 	shared_ptr<DuckDBPyConnection> RegisterPythonObject(const string &name, const py::object &python_object);
220: 
221: 	void InstallExtension(const string &extension, bool force_install = false,
222: 	                      const py::object &repository = py::none(), const py::object &repository_url = py::none(),
223: 	                      const py::object &version = py::none());
224: 
225: 	void LoadExtension(const string &extension);
226: 
227: 	unique_ptr<DuckDBPyRelation> RunQuery(const py::object &query, string alias = "", py::object params = py::list());
228: 
229: 	unique_ptr<DuckDBPyRelation> Table(const string &tname);
230: 
231: 	unique_ptr<DuckDBPyRelation> Values(py::object params = py::none());
232: 
233: 	unique_ptr<DuckDBPyRelation> View(const string &vname);
234: 
235: 	unique_ptr<DuckDBPyRelation> TableFunction(const string &fname, py::object params = py::list());
236: 
237: 	unique_ptr<DuckDBPyRelation> FromDF(const PandasDataFrame &value);
238: 
239: 	unique_ptr<DuckDBPyRelation> FromParquet(const string &file_glob, bool binary_as_string, bool file_row_number,
240: 	                                         bool filename, bool hive_partitioning, bool union_by_name,
241: 	                                         const py::object &compression = py::none());
242: 
243: 	unique_ptr<DuckDBPyRelation> FromParquets(const vector<string> &file_globs, bool binary_as_string,
244: 	                                          bool file_row_number, bool filename, bool hive_partitioning,
245: 	                                          bool union_by_name, const py::object &compression = py::none());
246: 
247: 	unique_ptr<DuckDBPyRelation> FromArrow(py::object &arrow_object);
248: 
249: 	unique_ptr<DuckDBPyRelation> FromSubstrait(py::bytes &proto);
250: 
251: 	unique_ptr<DuckDBPyRelation> GetSubstrait(const string &query, bool enable_optimizer = true);
252: 
253: 	unique_ptr<DuckDBPyRelation> GetSubstraitJSON(const string &query, bool enable_optimizer = true);
254: 
255: 	unique_ptr<DuckDBPyRelation> FromSubstraitJSON(const string &json);
256: 
257: 	unordered_set<string> GetTableNames(const string &query);
258: 
259: 	shared_ptr<DuckDBPyConnection> UnregisterPythonObject(const string &name);
260: 
261: 	shared_ptr<DuckDBPyConnection> Begin();
262: 
263: 	shared_ptr<DuckDBPyConnection> Commit();
264: 
265: 	shared_ptr<DuckDBPyConnection> Rollback();
266: 
267: 	shared_ptr<DuckDBPyConnection> Checkpoint();
268: 
269: 	void Close();
270: 
271: 	void Interrupt();
272: 
273: 	ModifiedMemoryFileSystem &GetObjectFileSystem();
274: 
275: 	// cursor() is stupid
276: 	shared_ptr<DuckDBPyConnection> Cursor();
277: 
278: 	Optional<py::list> GetDescription();
279: 
280: 	int GetRowcount();
281: 
282: 	// these should be functions on the result but well
283: 	Optional<py::tuple> FetchOne();
284: 
285: 	py::list FetchMany(idx_t size);
286: 
287: 	py::list FetchAll();
288: 
289: 	py::dict FetchNumpy();
290: 	PandasDataFrame FetchDF(bool date_as_object);
291: 	PandasDataFrame FetchDFChunk(const idx_t vectors_per_chunk = 1, bool date_as_object = false);
292: 
293: 	duckdb::pyarrow::Table FetchArrow(idx_t rows_per_batch);
294: 	PolarsDataFrame FetchPolars(idx_t rows_per_batch);
295: 
296: 	py::dict FetchPyTorch();
297: 
298: 	py::dict FetchTF();
299: 
300: 	duckdb::pyarrow::RecordBatchReader FetchRecordBatchReader(const idx_t rows_per_batch);
301: 
302: 	static shared_ptr<DuckDBPyConnection> Connect(const py::object &database, bool read_only, const py::dict &config);
303: 
304: 	static vector<Value> TransformPythonParamList(const py::handle &params);
305: 	static case_insensitive_map_t<BoundParameterData> TransformPythonParamDict(const py::dict &params);
306: 
307: 	void RegisterFilesystem(AbstractFileSystem filesystem);
308: 	void UnregisterFilesystem(const py::str &name);
309: 	py::list ListFilesystems();
310: 	bool FileSystemIsRegistered(const string &name);
311: 
312: 	//! Default connection to an in-memory database
313: 	static shared_ptr<DuckDBPyConnection> default_connection;
314: 	//! Caches and provides an interface to get frequently used modules+subtypes
315: 	static shared_ptr<PythonImportCache> import_cache;
316: 
317: 	static bool IsPandasDataframe(const py::object &object);
318: 	static bool IsPolarsDataframe(const py::object &object);
319: 	static PyArrowObjectType GetArrowType(const py::handle &obj);
320: 	static bool IsAcceptedArrowObject(const py::object &object);
321: 	static NumpyObjectType IsAcceptedNumpyObject(const py::object &object);
322: 
323: 	static unique_ptr<QueryResult> CompletePendingQuery(PendingQueryResult &pending_query);
324: 
325: private:
326: 	PathLike GetPathLike(const py::object &object);
327: 	unique_lock<std::mutex> AcquireConnectionLock();
328: 	ScalarFunction CreateScalarUDF(const string &name, const py::function &udf, const py::object &parameters,
329: 	                               const shared_ptr<DuckDBPyType> &return_type, bool vectorized,
330: 	                               FunctionNullHandling null_handling, PythonExceptionHandling exception_handling,
331: 	                               bool side_effects);
332: 	void RegisterArrowObject(const py::object &arrow_object, const string &name);
333: 	vector<unique_ptr<SQLStatement>> GetStatements(const py::object &query);
334: 
335: 	static PythonEnvironmentType environment;
336: 	static void DetectEnvironment();
337: };
338: 
339: template <typename T>
340: static bool ModuleIsLoaded() {
341: 	auto dict = pybind11::module_::import("sys").attr("modules");
342: 	return dict.contains(py::str(T::Name));
343: }
344: 
345: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp]
[start of tools/pythonpkg/src/pyconnection.cpp]
1: #include "duckdb_python/pyconnection/pyconnection.hpp"
2: 
3: #include "duckdb/catalog/default/default_types.hpp"
4: #include "duckdb/common/arrow/arrow.hpp"
5: #include "duckdb/common/enums/file_compression_type.hpp"
6: #include "duckdb/common/printer.hpp"
7: #include "duckdb/common/types.hpp"
8: #include "duckdb/common/types/vector.hpp"
9: #include "duckdb/function/table/read_csv.hpp"
10: #include "duckdb/main/client_config.hpp"
11: #include "duckdb/main/client_context.hpp"
12: #include "duckdb/main/config.hpp"
13: #include "duckdb/main/db_instance_cache.hpp"
14: #include "duckdb/main/extension_helper.hpp"
15: #include "duckdb/main/prepared_statement.hpp"
16: #include "duckdb/main/relation/read_csv_relation.hpp"
17: #include "duckdb/main/relation/read_json_relation.hpp"
18: #include "duckdb/main/relation/value_relation.hpp"
19: #include "duckdb/main/relation/view_relation.hpp"
20: #include "duckdb/parser/expression/constant_expression.hpp"
21: #include "duckdb/parser/expression/function_expression.hpp"
22: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
23: #include "duckdb/parser/parser.hpp"
24: #include "duckdb/parser/statement/select_statement.hpp"
25: #include "duckdb/parser/tableref/subqueryref.hpp"
26: #include "duckdb/parser/tableref/table_function_ref.hpp"
27: #include "duckdb_python/arrow/arrow_array_stream.hpp"
28: #include "duckdb_python/map.hpp"
29: #include "duckdb_python/pandas/pandas_scan.hpp"
30: #include "duckdb_python/pyrelation.hpp"
31: #include "duckdb_python/pystatement.hpp"
32: #include "duckdb_python/pyresult.hpp"
33: #include "duckdb_python/python_conversion.hpp"
34: #include "duckdb_python/numpy/numpy_type.hpp"
35: #include "duckdb/main/prepared_statement.hpp"
36: #include "duckdb_python/jupyter_progress_bar_display.hpp"
37: #include "duckdb_python/pyfilesystem.hpp"
38: #include "duckdb/main/client_config.hpp"
39: #include "duckdb/function/table/read_csv.hpp"
40: #include "duckdb/common/enums/file_compression_type.hpp"
41: #include "duckdb/catalog/default/default_types.hpp"
42: #include "duckdb/main/relation/value_relation.hpp"
43: #include "duckdb_python/filesystem_object.hpp"
44: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
45: #include "duckdb/function/scalar_function.hpp"
46: #include "duckdb_python/pandas/pandas_scan.hpp"
47: #include "duckdb_python/python_objects.hpp"
48: #include "duckdb/function/function.hpp"
49: #include "duckdb_python/pybind11/conversions/exception_handling_enum.hpp"
50: #include "duckdb/parser/parsed_data/drop_info.hpp"
51: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
52: #include "duckdb/main/pending_query_result.hpp"
53: #include "duckdb/parser/keyword_helper.hpp"
54: #include "duckdb_python/python_replacement_scan.hpp"
55: #include "duckdb/common/shared_ptr.hpp"
56: #include "duckdb/main/materialized_query_result.hpp"
57: #include "duckdb/main/stream_query_result.hpp"
58: #include "duckdb/main/relation/materialized_relation.hpp"
59: #include "duckdb/main/relation/query_relation.hpp"
60: #include "duckdb/main/extension_util.hpp"
61: #include "duckdb/parser/statement/load_statement.hpp"
62: 
63: #include <random>
64: 
65: #include "duckdb/common/printer.hpp"
66: 
67: namespace duckdb {
68: 
69: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::default_connection = nullptr;       // NOLINT: allow global
70: DBInstanceCache instance_cache;                                                        // NOLINT: allow global
71: shared_ptr<PythonImportCache> DuckDBPyConnection::import_cache = nullptr;              // NOLINT: allow global
72: PythonEnvironmentType DuckDBPyConnection::environment = PythonEnvironmentType::NORMAL; // NOLINT: allow global
73: 
74: DuckDBPyConnection::~DuckDBPyConnection() {
75: 	try {
76: 		py::gil_scoped_release gil;
77: 		// Release any structures that do not need to hold the GIL here
78: 		con.SetDatabase(nullptr);
79: 		con.SetConnection(nullptr);
80: 	} catch (...) { // NOLINT
81: 	}
82: }
83: 
84: void DuckDBPyConnection::DetectEnvironment() {
85: 	// If __main__ does not have a __file__ attribute, we are in interactive mode
86: 	auto main_module = py::module_::import("__main__");
87: 	if (py::hasattr(main_module, "__file__")) {
88: 		return;
89: 	}
90: 	DuckDBPyConnection::environment = PythonEnvironmentType::INTERACTIVE;
91: 	if (!ModuleIsLoaded<IpythonCacheItem>()) {
92: 		return;
93: 	}
94: 
95: 	// Check to see if we are in a Jupyter Notebook
96: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
97: 	auto get_ipython = import_cache_py.IPython.get_ipython();
98: 	if (get_ipython.ptr() == nullptr) {
99: 		// Could either not load the IPython module, or it has no 'get_ipython' attribute
100: 		return;
101: 	}
102: 	auto ipython = get_ipython();
103: 	if (!py::hasattr(ipython, "config")) {
104: 		return;
105: 	}
106: 	py::dict ipython_config = ipython.attr("config");
107: 	if (ipython_config.contains("IPKernelApp")) {
108: 		DuckDBPyConnection::environment = PythonEnvironmentType::JUPYTER;
109: 	}
110: 	return;
111: }
112: 
113: bool DuckDBPyConnection::DetectAndGetEnvironment() {
114: 	DuckDBPyConnection::DetectEnvironment();
115: 	return DuckDBPyConnection::IsInteractive();
116: }
117: 
118: bool DuckDBPyConnection::IsJupyter() {
119: 	return DuckDBPyConnection::environment == PythonEnvironmentType::JUPYTER;
120: }
121: 
122: // NOTE: this function is generated by tools/pythonpkg/scripts/generate_connection_methods.py.
123: // Do not edit this function manually, your changes will be overwritten!
124: 
125: static void InitializeConnectionMethods(py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>> &m) {
126: 	m.def("cursor", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection");
127: 	m.def("register_filesystem", &DuckDBPyConnection::RegisterFilesystem, "Register a fsspec compliant filesystem",
128: 	      py::arg("filesystem"));
129: 	m.def("unregister_filesystem", &DuckDBPyConnection::UnregisterFilesystem, "Unregister a filesystem",
130: 	      py::arg("name"));
131: 	m.def("list_filesystems", &DuckDBPyConnection::ListFilesystems,
132: 	      "List registered filesystems, including builtin ones");
133: 	m.def("filesystem_is_registered", &DuckDBPyConnection::FileSystemIsRegistered,
134: 	      "Check if a filesystem with the provided name is currently registered", py::arg("name"));
135: 	m.def("create_function", &DuckDBPyConnection::RegisterScalarUDF,
136: 	      "Create a DuckDB function out of the passing in Python function so it can be used in queries",
137: 	      py::arg("name"), py::arg("function"), py::arg("parameters") = py::none(), py::arg("return_type") = py::none(),
138: 	      py::kw_only(), py::arg("type") = PythonUDFType::NATIVE,
139: 	      py::arg("null_handling") = FunctionNullHandling::DEFAULT_NULL_HANDLING,
140: 	      py::arg("exception_handling") = PythonExceptionHandling::FORWARD_ERROR, py::arg("side_effects") = false);
141: 	m.def("remove_function", &DuckDBPyConnection::UnregisterUDF, "Remove a previously created function",
142: 	      py::arg("name"));
143: 	m.def("sqltype", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
144: 	      py::arg("type_str"));
145: 	m.def("dtype", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
146: 	      py::arg("type_str"));
147: 	m.def("type", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
148: 	      py::arg("type_str"));
149: 	m.def("array_type", &DuckDBPyConnection::ArrayType, "Create an array type object of 'type'",
150: 	      py::arg("type").none(false), py::arg("size"));
151: 	m.def("list_type", &DuckDBPyConnection::ListType, "Create a list type object of 'type'",
152: 	      py::arg("type").none(false));
153: 	m.def("union_type", &DuckDBPyConnection::UnionType, "Create a union type object from 'members'",
154: 	      py::arg("members").none(false));
155: 	m.def("string_type", &DuckDBPyConnection::StringType, "Create a string type with an optional collation",
156: 	      py::arg("collation") = "");
157: 	m.def("enum_type", &DuckDBPyConnection::EnumType,
158: 	      "Create an enum type of underlying 'type', consisting of the list of 'values'", py::arg("name"),
159: 	      py::arg("type"), py::arg("values"));
160: 	m.def("decimal_type", &DuckDBPyConnection::DecimalType, "Create a decimal type with 'width' and 'scale'",
161: 	      py::arg("width"), py::arg("scale"));
162: 	m.def("struct_type", &DuckDBPyConnection::StructType, "Create a struct type object from 'fields'",
163: 	      py::arg("fields"));
164: 	m.def("row_type", &DuckDBPyConnection::StructType, "Create a struct type object from 'fields'", py::arg("fields"));
165: 	m.def("map_type", &DuckDBPyConnection::MapType, "Create a map type object from 'key_type' and 'value_type'",
166: 	      py::arg("key").none(false), py::arg("value").none(false));
167: 	m.def("duplicate", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection");
168: 	m.def("execute", &DuckDBPyConnection::Execute,
169: 	      "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
170: 	      py::arg("parameters") = py::none());
171: 	m.def("executemany", &DuckDBPyConnection::ExecuteMany,
172: 	      "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
173: 	      py::arg("query"), py::arg("parameters") = py::none());
174: 	m.def("close", &DuckDBPyConnection::Close, "Close the connection");
175: 	m.def("interrupt", &DuckDBPyConnection::Interrupt, "Interrupt pending operations");
176: 	m.def("fetchone", &DuckDBPyConnection::FetchOne, "Fetch a single row from a result following execute");
177: 	m.def("fetchmany", &DuckDBPyConnection::FetchMany, "Fetch the next set of rows from a result following execute",
178: 	      py::arg("size") = 1);
179: 	m.def("fetchall", &DuckDBPyConnection::FetchAll, "Fetch all rows from a result following execute");
180: 	m.def("fetchnumpy", &DuckDBPyConnection::FetchNumpy, "Fetch a result as list of NumPy arrays following execute");
181: 	m.def("fetchdf", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
182: 	      py::arg("date_as_object") = false);
183: 	m.def("fetch_df", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
184: 	      py::arg("date_as_object") = false);
185: 	m.def("df", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
186: 	      py::arg("date_as_object") = false);
187: 	m.def("fetch_df_chunk", &DuckDBPyConnection::FetchDFChunk,
188: 	      "Fetch a chunk of the result as DataFrame following execute()", py::arg("vectors_per_chunk") = 1,
189: 	      py::kw_only(), py::arg("date_as_object") = false);
190: 	m.def("pl", &DuckDBPyConnection::FetchPolars, "Fetch a result as Polars DataFrame following execute()",
191: 	      py::arg("rows_per_batch") = 1000000);
192: 	m.def("fetch_arrow_table", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
193: 	      py::arg("rows_per_batch") = 1000000);
194: 	m.def("arrow", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
195: 	      py::arg("rows_per_batch") = 1000000);
196: 	m.def("fetch_record_batch", &DuckDBPyConnection::FetchRecordBatchReader,
197: 	      "Fetch an Arrow RecordBatchReader following execute()", py::arg("rows_per_batch") = 1000000);
198: 	m.def("torch", &DuckDBPyConnection::FetchPyTorch, "Fetch a result as dict of PyTorch Tensors following execute()");
199: 	m.def("tf", &DuckDBPyConnection::FetchTF, "Fetch a result as dict of TensorFlow Tensors following execute()");
200: 	m.def("begin", &DuckDBPyConnection::Begin, "Start a new transaction");
201: 	m.def("commit", &DuckDBPyConnection::Commit, "Commit changes performed within a transaction");
202: 	m.def("rollback", &DuckDBPyConnection::Rollback, "Roll back changes performed within a transaction");
203: 	m.def("checkpoint", &DuckDBPyConnection::Checkpoint,
204: 	      "Synchronizes data in the write-ahead log (WAL) to the database data file (no-op for in-memory connections)");
205: 	m.def("append", &DuckDBPyConnection::Append, "Append the passed DataFrame to the named table",
206: 	      py::arg("table_name"), py::arg("df"), py::kw_only(), py::arg("by_name") = false);
207: 	m.def("register", &DuckDBPyConnection::RegisterPythonObject,
208: 	      "Register the passed Python Object value for querying with a view", py::arg("view_name"),
209: 	      py::arg("python_object"));
210: 	m.def("unregister", &DuckDBPyConnection::UnregisterPythonObject, "Unregister the view name", py::arg("view_name"));
211: 	m.def("table", &DuckDBPyConnection::Table, "Create a relation object for the named table", py::arg("table_name"));
212: 	m.def("view", &DuckDBPyConnection::View, "Create a relation object for the named view", py::arg("view_name"));
213: 	m.def("values", &DuckDBPyConnection::Values, "Create a relation object from the passed values", py::arg("values"));
214: 	m.def("table_function", &DuckDBPyConnection::TableFunction,
215: 	      "Create a relation object from the named table function with given parameters", py::arg("name"),
216: 	      py::arg("parameters") = py::none());
217: 	m.def("read_json", &DuckDBPyConnection::ReadJSON, "Create a relation object from the JSON file in 'name'",
218: 	      py::arg("path_or_buffer"), py::kw_only(), py::arg("columns") = py::none(),
219: 	      py::arg("sample_size") = py::none(), py::arg("maximum_depth") = py::none(), py::arg("records") = py::none(),
220: 	      py::arg("format") = py::none(), py::arg("date_format") = py::none(), py::arg("timestamp_format") = py::none(),
221: 	      py::arg("compression") = py::none(), py::arg("maximum_object_size") = py::none(),
222: 	      py::arg("ignore_errors") = py::none(), py::arg("convert_strings_to_integers") = py::none(),
223: 	      py::arg("field_appearance_threshold") = py::none(), py::arg("map_inference_threshold") = py::none(),
224: 	      py::arg("maximum_sample_files") = py::none(), py::arg("filename") = py::none(),
225: 	      py::arg("hive_partitioning") = py::none(), py::arg("union_by_name") = py::none(),
226: 	      py::arg("hive_types") = py::none(), py::arg("hive_types_autocast") = py::none());
227: 	m.def("extract_statements", &DuckDBPyConnection::ExtractStatements,
228: 	      "Parse the query string and extract the Statement object(s) produced", py::arg("query"));
229: 	m.def("sql", &DuckDBPyConnection::RunQuery,
230: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
231: 	      "run the query as-is.",
232: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
233: 	m.def("query", &DuckDBPyConnection::RunQuery,
234: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
235: 	      "run the query as-is.",
236: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
237: 	m.def("from_query", &DuckDBPyConnection::RunQuery,
238: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
239: 	      "run the query as-is.",
240: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
241: 	m.def("read_csv", &DuckDBPyConnection::ReadCSV, "Create a relation object from the CSV file in 'name'",
242: 	      py::arg("path_or_buffer"), py::kw_only());
243: 	m.def("from_csv_auto", &DuckDBPyConnection::ReadCSV, "Create a relation object from the CSV file in 'name'",
244: 	      py::arg("path_or_buffer"), py::kw_only());
245: 	m.def("from_df", &DuckDBPyConnection::FromDF, "Create a relation object from the DataFrame in df", py::arg("df"));
246: 	m.def("from_arrow", &DuckDBPyConnection::FromArrow, "Create a relation object from an Arrow object",
247: 	      py::arg("arrow_object"));
248: 	m.def("from_parquet", &DuckDBPyConnection::FromParquet,
249: 	      "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
250: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
251: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
252: 	      py::arg("compression") = py::none());
253: 	m.def("read_parquet", &DuckDBPyConnection::FromParquet,
254: 	      "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
255: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
256: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
257: 	      py::arg("compression") = py::none());
258: 	m.def("from_parquet", &DuckDBPyConnection::FromParquets,
259: 	      "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
260: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
261: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
262: 	      py::arg("compression") = py::none());
263: 	m.def("read_parquet", &DuckDBPyConnection::FromParquets,
264: 	      "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
265: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
266: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
267: 	      py::arg("compression") = py::none());
268: 	m.def("from_substrait", &DuckDBPyConnection::FromSubstrait, "Create a query object from protobuf plan",
269: 	      py::arg("proto"));
270: 	m.def("get_substrait", &DuckDBPyConnection::GetSubstrait, "Serialize a query to protobuf", py::arg("query"),
271: 	      py::kw_only(), py::arg("enable_optimizer") = true);
272: 	m.def("get_substrait_json", &DuckDBPyConnection::GetSubstraitJSON,
273: 	      "Serialize a query to protobuf on the JSON format", py::arg("query"), py::kw_only(),
274: 	      py::arg("enable_optimizer") = true);
275: 	m.def("from_substrait_json", &DuckDBPyConnection::FromSubstraitJSON,
276: 	      "Create a query object from a JSON protobuf plan", py::arg("json"));
277: 	m.def("get_table_names", &DuckDBPyConnection::GetTableNames, "Extract the required table names from a query",
278: 	      py::arg("query"));
279: 	m.def("install_extension", &DuckDBPyConnection::InstallExtension,
280: 	      "Install an extension by name, with an optional version and/or repository to get the extension from",
281: 	      py::arg("extension"), py::kw_only(), py::arg("force_install") = false, py::arg("repository") = py::none(),
282: 	      py::arg("repository_url") = py::none(), py::arg("version") = py::none());
283: 	m.def("load_extension", &DuckDBPyConnection::LoadExtension, "Load an installed extension", py::arg("extension"));
284: } // END_OF_CONNECTION_METHODS
285: 
286: void DuckDBPyConnection::UnregisterFilesystem(const py::str &name) {
287: 	auto &database = con.GetDatabase();
288: 	auto &fs = database.GetFileSystem();
289: 
290: 	fs.UnregisterSubSystem(name);
291: }
292: 
293: void DuckDBPyConnection::RegisterFilesystem(AbstractFileSystem filesystem) {
294: 	PythonGILWrapper gil_wrapper;
295: 
296: 	auto &database = con.GetDatabase();
297: 	if (!py::isinstance<AbstractFileSystem>(filesystem)) {
298: 		throw InvalidInputException("Bad filesystem instance");
299: 	}
300: 
301: 	auto &fs = database.GetFileSystem();
302: 
303: 	auto protocol = filesystem.attr("protocol");
304: 	if (protocol.is_none() || py::str("abstract").equal(protocol)) {
305: 		throw InvalidInputException("Must provide concrete fsspec implementation");
306: 	}
307: 
308: 	vector<string> protocols;
309: 	if (py::isinstance<py::str>(protocol)) {
310: 		protocols.push_back(py::str(protocol));
311: 	} else {
312: 		for (const auto &sub_protocol : protocol) {
313: 			protocols.push_back(py::str(sub_protocol));
314: 		}
315: 	}
316: 
317: 	fs.RegisterSubSystem(make_uniq<PythonFilesystem>(std::move(protocols), std::move(filesystem)));
318: }
319: 
320: py::list DuckDBPyConnection::ListFilesystems() {
321: 	auto &database = con.GetDatabase();
322: 	auto subsystems = database.GetFileSystem().ListSubSystems();
323: 	py::list names;
324: 	for (auto &name : subsystems) {
325: 		names.append(py::str(name));
326: 	}
327: 	return names;
328: }
329: 
330: py::list DuckDBPyConnection::ExtractStatements(const string &query) {
331: 	py::list result;
332: 	auto &connection = con.GetConnection();
333: 	auto statements = connection.ExtractStatements(query);
334: 	for (auto &statement : statements) {
335: 		result.append(make_uniq<DuckDBPyStatement>(std::move(statement)));
336: 	}
337: 	return result;
338: }
339: 
340: bool DuckDBPyConnection::FileSystemIsRegistered(const string &name) {
341: 	auto &database = con.GetDatabase();
342: 	auto subsystems = database.GetFileSystem().ListSubSystems();
343: 	return std::find(subsystems.begin(), subsystems.end(), name) != subsystems.end();
344: }
345: 
346: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::UnregisterUDF(const string &name) {
347: 	auto entry = registered_functions.find(name);
348: 	if (entry == registered_functions.end()) {
349: 		// Not registered or already unregistered
350: 		throw InvalidInputException("No function by the name of '%s' was found in the list of registered functions",
351: 		                            name);
352: 	}
353: 
354: 	auto &connection = con.GetConnection();
355: 	auto &context = *connection.context;
356: 
357: 	context.RunFunctionInTransaction([&]() {
358: 		// create function
359: 		auto &catalog = Catalog::GetCatalog(context, SYSTEM_CATALOG);
360: 		DropInfo info;
361: 		info.type = CatalogType::SCALAR_FUNCTION_ENTRY;
362: 		info.name = name;
363: 		info.allow_drop_internal = true;
364: 		info.cascade = false;
365: 		info.if_not_found = OnEntryNotFound::THROW_EXCEPTION;
366: 		catalog.DropEntry(context, info);
367: 	});
368: 	registered_functions.erase(entry);
369: 
370: 	return shared_from_this();
371: }
372: 
373: shared_ptr<DuckDBPyConnection>
374: DuckDBPyConnection::RegisterScalarUDF(const string &name, const py::function &udf, const py::object &parameters_p,
375:                                       const shared_ptr<DuckDBPyType> &return_type_p, PythonUDFType type,
376:                                       FunctionNullHandling null_handling, PythonExceptionHandling exception_handling,
377:                                       bool side_effects) {
378: 	auto &connection = con.GetConnection();
379: 	auto &context = *connection.context;
380: 
381: 	if (context.transaction.HasActiveTransaction()) {
382: 		context.CancelTransaction();
383: 	}
384: 	if (registered_functions.find(name) != registered_functions.end()) {
385: 		throw NotImplementedException("A function by the name of '%s' is already created, creating multiple "
386: 		                              "functions with the same name is not supported yet, please remove it first",
387: 		                              name);
388: 	}
389: 	auto scalar_function = CreateScalarUDF(name, udf, parameters_p, return_type_p, type == PythonUDFType::ARROW,
390: 	                                       null_handling, exception_handling, side_effects);
391: 	CreateScalarFunctionInfo info(scalar_function);
392: 
393: 	context.RegisterFunction(info);
394: 
395: 	auto dependency = make_uniq<ExternalDependency>();
396: 	dependency->AddDependency("function", PythonDependencyItem::Create(udf));
397: 	registered_functions[name] = std::move(dependency);
398: 
399: 	return shared_from_this();
400: }
401: 
402: void DuckDBPyConnection::Initialize(py::handle &m) {
403: 	auto connection_module =
404: 	    py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection", py::module_local());
405: 
406: 	connection_module.def("__enter__", &DuckDBPyConnection::Enter)
407: 	    .def("__exit__", &DuckDBPyConnection::Exit, py::arg("exc_type"), py::arg("exc"), py::arg("traceback"));
408: 	connection_module.def("__del__", &DuckDBPyConnection::Close);
409: 
410: 	InitializeConnectionMethods(connection_module);
411: 	connection_module.def_property_readonly("description", &DuckDBPyConnection::GetDescription,
412: 	                                        "Get result set attributes, mainly column names");
413: 	connection_module.def_property_readonly("rowcount", &DuckDBPyConnection::GetRowcount, "Get result set row count");
414: 	PyDateTime_IMPORT; // NOLINT
415: 	DuckDBPyConnection::ImportCache();
416: }
417: 
418: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::ExecuteMany(const py::object &query, py::object params_p) {
419: 	con.SetResult(nullptr);
420: 	if (params_p.is_none()) {
421: 		params_p = py::list();
422: 	}
423: 
424: 	auto statements = GetStatements(query);
425: 	if (statements.empty()) {
426: 		// TODO: should we throw?
427: 		return nullptr;
428: 	}
429: 
430: 	auto last_statement = std::move(statements.back());
431: 	statements.pop_back();
432: 	// First immediately execute any preceding statements (if any)
433: 	// FIXME: DBAPI says to not accept an 'executemany' call with multiple statements
434: 	ExecuteImmediately(std::move(statements));
435: 
436: 	auto prep = PrepareQuery(std::move(last_statement));
437: 
438: 	if (!py::is_list_like(params_p)) {
439: 		throw InvalidInputException("executemany requires a list of parameter sets to be provided");
440: 	}
441: 	auto outer_list = py::list(params_p);
442: 	if (outer_list.empty()) {
443: 		throw InvalidInputException("executemany requires a non-empty list of parameter sets to be provided");
444: 	}
445: 
446: 	unique_ptr<QueryResult> query_result;
447: 	// Execute once for every set of parameters that are provided
448: 	for (auto &parameters : outer_list) {
449: 		auto params = py::reinterpret_borrow<py::object>(parameters);
450: 		query_result = ExecuteInternal(*prep, std::move(params));
451: 	}
452: 	// Set the internal 'result' object
453: 	if (query_result) {
454: 		auto py_result = make_uniq<DuckDBPyResult>(std::move(query_result));
455: 		con.SetResult(make_uniq<DuckDBPyRelation>(std::move(py_result)));
456: 	}
457: 
458: 	return shared_from_this();
459: }
460: 
461: unique_ptr<QueryResult> DuckDBPyConnection::CompletePendingQuery(PendingQueryResult &pending_query) {
462: 	PendingExecutionResult execution_result;
463: 	while (!PendingQueryResult::IsResultReady(execution_result = pending_query.ExecuteTask())) {
464: 		{
465: 			py::gil_scoped_acquire gil;
466: 			if (PyErr_CheckSignals() != 0) {
467: 				throw std::runtime_error("Query interrupted");
468: 			}
469: 		}
470: 		if (execution_result == PendingExecutionResult::BLOCKED) {
471: 			pending_query.WaitForTask();
472: 		}
473: 	}
474: 	if (execution_result == PendingExecutionResult::EXECUTION_ERROR) {
475: 		pending_query.ThrowError();
476: 	}
477: 	return pending_query.Execute();
478: }
479: 
480: py::list TransformNamedParameters(const case_insensitive_map_t<idx_t> &named_param_map, const py::dict &params) {
481: 	py::list new_params(params.size());
482: 
483: 	for (auto &item : params) {
484: 		const std::string &item_name = item.first.cast<std::string>();
485: 		auto entry = named_param_map.find(item_name);
486: 		if (entry == named_param_map.end()) {
487: 			throw InvalidInputException(
488: 			    "Named parameters could not be transformed, because query string is missing named parameter '%s'",
489: 			    item_name);
490: 		}
491: 		auto param_idx = entry->second;
492: 		// Add the value of the named parameter to the list
493: 		new_params[param_idx - 1] = item.second;
494: 	}
495: 
496: 	if (named_param_map.size() != params.size()) {
497: 		// One or more named parameters were expected, but not found
498: 		vector<string> missing_params;
499: 		missing_params.reserve(named_param_map.size());
500: 		for (auto &entry : named_param_map) {
501: 			auto &name = entry.first;
502: 			if (!params.contains(name)) {
503: 				missing_params.push_back(name);
504: 			}
505: 		}
506: 		auto message = StringUtil::Join(missing_params, ", ");
507: 		throw InvalidInputException("Not all named parameters have been located, missing: %s", message);
508: 	}
509: 
510: 	return new_params;
511: }
512: 
513: case_insensitive_map_t<BoundParameterData> TransformPreparedParameters(PreparedStatement &prep,
514:                                                                        const py::object &params) {
515: 	case_insensitive_map_t<BoundParameterData> named_values;
516: 	if (py::is_list_like(params)) {
517: 		if (prep.named_param_map.size() != py::len(params)) {
518: 			if (py::len(params) == 0) {
519: 				throw InvalidInputException("Expected %d parameters, but none were supplied",
520: 				                            prep.named_param_map.size());
521: 			}
522: 			throw InvalidInputException("Prepared statement needs %d parameters, %d given", prep.named_param_map.size(),
523: 			                            py::len(params));
524: 		}
525: 		auto unnamed_values = DuckDBPyConnection::TransformPythonParamList(params);
526: 		for (idx_t i = 0; i < unnamed_values.size(); i++) {
527: 			auto &value = unnamed_values[i];
528: 			auto identifier = std::to_string(i + 1);
529: 			named_values[identifier] = BoundParameterData(std::move(value));
530: 		}
531: 	} else if (py::is_dict_like(params)) {
532: 		auto dict = py::cast<py::dict>(params);
533: 		named_values = DuckDBPyConnection::TransformPythonParamDict(dict);
534: 	} else {
535: 		throw InvalidInputException("Prepared parameters can only be passed as a list or a dictionary");
536: 	}
537: 	return named_values;
538: }
539: 
540: unique_ptr<PreparedStatement> DuckDBPyConnection::PrepareQuery(unique_ptr<SQLStatement> statement) {
541: 	auto &connection = con.GetConnection();
542: 	unique_ptr<PreparedStatement> prep;
543: 	{
544: 		py::gil_scoped_release release;
545: 		unique_lock<mutex> lock(py_connection_lock);
546: 
547: 		prep = connection.Prepare(std::move(statement));
548: 		if (prep->HasError()) {
549: 			prep->error.Throw();
550: 		}
551: 	}
552: 	return prep;
553: }
554: 
555: unique_ptr<QueryResult> DuckDBPyConnection::ExecuteInternal(PreparedStatement &prep, py::object params) {
556: 	if (params.is_none()) {
557: 		params = py::list();
558: 	}
559: 
560: 	// Execute the prepared statement with the prepared parameters
561: 	auto named_values = TransformPreparedParameters(prep, params);
562: 	unique_ptr<QueryResult> res;
563: 	{
564: 		py::gil_scoped_release release;
565: 		unique_lock<std::mutex> lock(py_connection_lock);
566: 
567: 		auto pending_query = prep.PendingQuery(named_values);
568: 		if (pending_query->HasError()) {
569: 			pending_query->ThrowError();
570: 		}
571: 		res = CompletePendingQuery(*pending_query);
572: 
573: 		if (res->HasError()) {
574: 			res->ThrowError();
575: 		}
576: 	}
577: 	return res;
578: }
579: 
580: vector<unique_ptr<SQLStatement>> DuckDBPyConnection::GetStatements(const py::object &query) {
581: 	vector<unique_ptr<SQLStatement>> result;
582: 	auto &connection = con.GetConnection();
583: 
584: 	shared_ptr<DuckDBPyStatement> statement_obj;
585: 	if (py::try_cast(query, statement_obj)) {
586: 		result.push_back(statement_obj->GetStatement());
587: 		return result;
588: 	}
589: 	if (py::isinstance<py::str>(query)) {
590: 		auto sql_query = std::string(py::str(query));
591: 		return connection.ExtractStatements(sql_query);
592: 	}
593: 	throw InvalidInputException("Please provide either a DuckDBPyStatement or a string representing the query");
594: }
595: 
596: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::ExecuteFromString(const string &query) {
597: 	return Execute(py::str(query));
598: }
599: 
600: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Execute(const py::object &query, py::object params) {
601: 	con.SetResult(nullptr);
602: 
603: 	auto statements = GetStatements(query);
604: 	if (statements.empty()) {
605: 		// TODO: should we throw?
606: 		return nullptr;
607: 	}
608: 
609: 	auto last_statement = std::move(statements.back());
610: 	statements.pop_back();
611: 	// First immediately execute any preceding statements (if any)
612: 	// FIXME: SQLites implementation says to not accept an 'execute' call with multiple statements
613: 	ExecuteImmediately(std::move(statements));
614: 
615: 	auto prep = PrepareQuery(std::move(last_statement));
616: 	auto res = ExecuteInternal(*prep, std::move(params));
617: 
618: 	// Set the internal 'result' object
619: 	if (res) {
620: 		auto py_result = make_uniq<DuckDBPyResult>(std::move(res));
621: 		con.SetResult(make_uniq<DuckDBPyRelation>(std::move(py_result)));
622: 	}
623: 	return shared_from_this();
624: }
625: 
626: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Append(const string &name, const PandasDataFrame &value,
627:                                                           bool by_name) {
628: 	RegisterPythonObject("__append_df", value);
629: 	string columns = "";
630: 	if (by_name) {
631: 		auto df_columns = value.attr("columns");
632: 		vector<string> column_names;
633: 		for (auto &column : df_columns) {
634: 			column_names.push_back(std::string(py::str(column)));
635: 		}
636: 		columns += "(";
637: 		for (idx_t i = 0; i < column_names.size(); i++) {
638: 			auto &column = column_names[i];
639: 			if (i != 0) {
640: 				columns += ", ";
641: 			}
642: 			columns += StringUtil::Format("%s", SQLIdentifier(column));
643: 		}
644: 		columns += ")";
645: 	}
646: 
647: 	auto sql_query = StringUtil::Format("INSERT INTO %s %s SELECT * FROM __append_df", SQLIdentifier(name), columns);
648: 	return Execute(py::str(sql_query));
649: }
650: 
651: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::RegisterPythonObject(const string &name,
652:                                                                         const py::object &python_object) {
653: 	auto &connection = con.GetConnection();
654: 	auto &client = *connection.context;
655: 	auto object = PythonReplacementScan::ReplacementObject(python_object, name, client);
656: 	auto view_rel = make_shared_ptr<ViewRelation>(connection.context, std::move(object), name);
657: 	bool replace = registered_objects.count(name);
658: 	view_rel->CreateView(name, replace, true);
659: 	registered_objects.insert(name);
660: 	return shared_from_this();
661: }
662: 
663: static void ParseMultiFileReaderOptions(named_parameter_map_t &options, const Optional<py::object> &filename,
664:                                         const Optional<py::object> &hive_partitioning,
665:                                         const Optional<py::object> &union_by_name,
666:                                         const Optional<py::object> &hive_types,
667:                                         const Optional<py::object> &hive_types_autocast) {
668: 	if (!py::none().is(filename)) {
669: 		auto val = TransformPythonValue(filename);
670: 		options["filename"] = val;
671: 	}
672: 
673: 	if (!py::none().is(hive_types)) {
674: 		auto val = TransformPythonValue(hive_types);
675: 		options["hive_types"] = val;
676: 	}
677: 
678: 	if (!py::none().is(hive_partitioning)) {
679: 		if (!py::isinstance<py::bool_>(hive_partitioning)) {
680: 			string actual_type = py::str(hive_partitioning.get_type());
681: 			throw BinderException("read_json only accepts 'hive_partitioning' as a boolean, not '%s'", actual_type);
682: 		}
683: 		auto val = TransformPythonValue(hive_partitioning, LogicalTypeId::BOOLEAN);
684: 		options["hive_partitioning"] = val;
685: 	}
686: 
687: 	if (!py::none().is(union_by_name)) {
688: 		if (!py::isinstance<py::bool_>(union_by_name)) {
689: 			string actual_type = py::str(union_by_name.get_type());
690: 			throw BinderException("read_json only accepts 'union_by_name' as a boolean, not '%s'", actual_type);
691: 		}
692: 		auto val = TransformPythonValue(union_by_name, LogicalTypeId::BOOLEAN);
693: 		options["union_by_name"] = val;
694: 	}
695: 
696: 	if (!py::none().is(hive_types_autocast)) {
697: 		if (!py::isinstance<py::bool_>(hive_types_autocast)) {
698: 			string actual_type = py::str(hive_types_autocast.get_type());
699: 			throw BinderException("read_json only accepts 'hive_types_autocast' as a boolean, not '%s'", actual_type);
700: 		}
701: 		auto val = TransformPythonValue(hive_types_autocast, LogicalTypeId::BOOLEAN);
702: 		options["hive_types_autocast"] = val;
703: 	}
704: }
705: 
706: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(
707:     const py::object &name_p, const Optional<py::object> &columns, const Optional<py::object> &sample_size,
708:     const Optional<py::object> &maximum_depth, const Optional<py::str> &records, const Optional<py::str> &format,
709:     const Optional<py::object> &date_format, const Optional<py::object> &timestamp_format,
710:     const Optional<py::object> &compression, const Optional<py::object> &maximum_object_size,
711:     const Optional<py::object> &ignore_errors, const Optional<py::object> &convert_strings_to_integers,
712:     const Optional<py::object> &field_appearance_threshold, const Optional<py::object> &map_inference_threshold,
713:     const Optional<py::object> &maximum_sample_files, const Optional<py::object> &filename,
714:     const Optional<py::object> &hive_partitioning, const Optional<py::object> &union_by_name,
715:     const Optional<py::object> &hive_types, const Optional<py::object> &hive_types_autocast) {
716: 
717: 	named_parameter_map_t options;
718: 
719: 	auto &connection = con.GetConnection();
720: 	auto path_like = GetPathLike(name_p);
721: 	auto &name = path_like.files;
722: 	auto file_like_object_wrapper = std::move(path_like.dependency);
723: 
724: 	ParseMultiFileReaderOptions(options, filename, hive_partitioning, union_by_name, hive_types, hive_types_autocast);
725: 
726: 	if (!py::none().is(columns)) {
727: 		if (!py::is_dict_like(columns)) {
728: 			throw BinderException("read_json only accepts 'columns' as a dict[str, str]");
729: 		}
730: 		py::dict columns_dict = columns;
731: 		child_list_t<Value> struct_fields;
732: 
733: 		for (auto &kv : columns_dict) {
734: 			auto &column_name = kv.first;
735: 			auto &type = kv.second;
736: 			if (!py::isinstance<py::str>(column_name)) {
737: 				string actual_type = py::str(column_name.get_type());
738: 				throw BinderException("The provided column name must be a str, not of type '%s'", actual_type);
739: 			}
740: 			if (!py::isinstance<py::str>(type)) {
741: 				string actual_type = py::str(column_name.get_type());
742: 				throw BinderException("The provided column type must be a str, not of type '%s'", actual_type);
743: 			}
744: 			struct_fields.emplace_back(py::str(column_name), Value(py::str(type)));
745: 		}
746: 		auto dtype_struct = Value::STRUCT(std::move(struct_fields));
747: 		options["columns"] = std::move(dtype_struct);
748: 	}
749: 
750: 	if (!py::none().is(records)) {
751: 		if (!py::isinstance<py::str>(records)) {
752: 			string actual_type = py::str(records.get_type());
753: 			throw BinderException("read_json only accepts 'records' as a string, not '%s'", actual_type);
754: 		}
755: 		auto records_s = py::reinterpret_borrow<py::str>(records);
756: 		auto records_option = std::string(py::str(records_s));
757: 		options["records"] = Value(records_option);
758: 	}
759: 
760: 	if (!py::none().is(format)) {
761: 		if (!py::isinstance<py::str>(format)) {
762: 			string actual_type = py::str(format.get_type());
763: 			throw BinderException("read_json only accepts 'format' as a string, not '%s'", actual_type);
764: 		}
765: 		auto format_s = py::reinterpret_borrow<py::str>(format);
766: 		auto format_option = std::string(py::str(format_s));
767: 		options["format"] = Value(format_option);
768: 	}
769: 
770: 	if (!py::none().is(date_format)) {
771: 		if (!py::isinstance<py::str>(date_format)) {
772: 			string actual_type = py::str(date_format.get_type());
773: 			throw BinderException("read_json only accepts 'date_format' as a string, not '%s'", actual_type);
774: 		}
775: 		auto date_format_s = py::reinterpret_borrow<py::str>(date_format);
776: 		auto date_format_option = std::string(py::str(date_format_s));
777: 		options["date_format"] = Value(date_format_option);
778: 	}
779: 
780: 	if (!py::none().is(timestamp_format)) {
781: 		if (!py::isinstance<py::str>(timestamp_format)) {
782: 			string actual_type = py::str(timestamp_format.get_type());
783: 			throw BinderException("read_json only accepts 'timestamp_format' as a string, not '%s'", actual_type);
784: 		}
785: 		auto timestamp_format_s = py::reinterpret_borrow<py::str>(timestamp_format);
786: 		auto timestamp_format_option = std::string(py::str(timestamp_format_s));
787: 		options["timestamp_format"] = Value(timestamp_format_option);
788: 	}
789: 
790: 	if (!py::none().is(compression)) {
791: 		if (!py::isinstance<py::str>(compression)) {
792: 			string actual_type = py::str(compression.get_type());
793: 			throw BinderException("read_json only accepts 'compression' as a string, not '%s'", actual_type);
794: 		}
795: 		auto compression_s = py::reinterpret_borrow<py::str>(compression);
796: 		auto compression_option = std::string(py::str(compression_s));
797: 		options["compression"] = Value(compression_option);
798: 	}
799: 
800: 	if (!py::none().is(sample_size)) {
801: 		if (!py::isinstance<py::int_>(sample_size)) {
802: 			string actual_type = py::str(sample_size.get_type());
803: 			throw BinderException("read_json only accepts 'sample_size' as an integer, not '%s'", actual_type);
804: 		}
805: 		options["sample_size"] = Value::INTEGER(py::int_(sample_size));
806: 	}
807: 
808: 	if (!py::none().is(maximum_depth)) {
809: 		if (!py::isinstance<py::int_>(maximum_depth)) {
810: 			string actual_type = py::str(maximum_depth.get_type());
811: 			throw BinderException("read_json only accepts 'maximum_depth' as an integer, not '%s'", actual_type);
812: 		}
813: 		options["maximum_depth"] = Value::INTEGER(py::int_(maximum_depth));
814: 	}
815: 
816: 	if (!py::none().is(maximum_object_size)) {
817: 		if (!py::isinstance<py::int_>(maximum_object_size)) {
818: 			string actual_type = py::str(maximum_object_size.get_type());
819: 			throw BinderException("read_json only accepts 'maximum_object_size' as an unsigned integer, not '%s'",
820: 			                      actual_type);
821: 		}
822: 		auto val = TransformPythonValue(maximum_object_size, LogicalTypeId::UINTEGER);
823: 		options["maximum_object_size"] = val;
824: 	}
825: 
826: 	if (!py::none().is(ignore_errors)) {
827: 		if (!py::isinstance<py::bool_>(ignore_errors)) {
828: 			string actual_type = py::str(ignore_errors.get_type());
829: 			throw BinderException("read_json only accepts 'ignore_errors' as a boolean, not '%s'", actual_type);
830: 		}
831: 		auto val = TransformPythonValue(ignore_errors, LogicalTypeId::BOOLEAN);
832: 		options["ignore_errors"] = val;
833: 	}
834: 
835: 	if (!py::none().is(convert_strings_to_integers)) {
836: 		if (!py::isinstance<py::bool_>(convert_strings_to_integers)) {
837: 			string actual_type = py::str(convert_strings_to_integers.get_type());
838: 			throw BinderException("read_json only accepts 'convert_strings_to_integers' as a boolean, not '%s'",
839: 			                      actual_type);
840: 		}
841: 		auto val = TransformPythonValue(convert_strings_to_integers, LogicalTypeId::BOOLEAN);
842: 		options["convert_strings_to_integers"] = val;
843: 	}
844: 
845: 	if (!py::none().is(field_appearance_threshold)) {
846: 		if (!py::isinstance<py::float_>(field_appearance_threshold)) {
847: 			string actual_type = py::str(field_appearance_threshold.get_type());
848: 			throw BinderException("read_json only accepts 'field_appearance_threshold' as a float, not '%s'",
849: 			                      actual_type);
850: 		}
851: 		auto val = TransformPythonValue(field_appearance_threshold, LogicalTypeId::DOUBLE);
852: 		options["field_appearance_threshold"] = val;
853: 	}
854: 
855: 	if (!py::none().is(map_inference_threshold)) {
856: 		if (!py::isinstance<py::int_>(map_inference_threshold)) {
857: 			string actual_type = py::str(map_inference_threshold.get_type());
858: 			throw BinderException("read_json only accepts 'map_inference_threshold' as an integer, not '%s'",
859: 			                      actual_type);
860: 		}
861: 		auto val = TransformPythonValue(map_inference_threshold, LogicalTypeId::BIGINT);
862: 		options["map_inference_threshold"] = val;
863: 	}
864: 
865: 	if (!py::none().is(maximum_sample_files)) {
866: 		if (!py::isinstance<py::int_>(maximum_sample_files)) {
867: 			string actual_type = py::str(maximum_sample_files.get_type());
868: 			throw BinderException("read_json only accepts 'maximum_sample_files' as an integer, not '%s'", actual_type);
869: 		}
870: 		auto val = TransformPythonValue(maximum_sample_files, LogicalTypeId::BIGINT);
871: 		options["maximum_sample_files"] = val;
872: 	}
873: 
874: 	bool auto_detect = false;
875: 	if (!options.count("columns")) {
876: 		options["auto_detect"] = Value::BOOLEAN(true);
877: 		auto_detect = true;
878: 	}
879: 
880: 	py::gil_scoped_release gil;
881: 	auto read_json_relation =
882: 	    make_shared_ptr<ReadJSONRelation>(connection.context, name, std::move(options), auto_detect);
883: 	if (read_json_relation == nullptr) {
884: 		throw BinderException("read_json can only be used when the JSON extension is (statically) loaded");
885: 	}
886: 	if (file_like_object_wrapper) {
887: 		read_json_relation->AddExternalDependency(std::move(file_like_object_wrapper));
888: 	}
889: 	return make_uniq<DuckDBPyRelation>(std::move(read_json_relation));
890: }
891: 
892: PathLike DuckDBPyConnection::GetPathLike(const py::object &object) {
893: 	return PathLike::Create(object, *this);
894: }
895: 
896: static void AcceptableCSVOptions(const string &unkown_parameter) {
897: 	// List of strings to match against
898: 	const unordered_set<string> valid_parameters = {"header",
899: 	                                                "compression",
900: 	                                                "sep",
901: 	                                                "delimiter",
902: 	                                                "dtype",
903: 	                                                "na_values",
904: 	                                                "skiprows",
905: 	                                                "quotechar",
906: 	                                                "escapechar",
907: 	                                                "encoding",
908: 	                                                "parallel",
909: 	                                                "date_format",
910: 	                                                "timestamp_format",
911: 	                                                "sample_size",
912: 	                                                "all_varchar",
913: 	                                                "normalize_names",
914: 	                                                "null_padding",
915: 	                                                "names",
916: 	                                                "lineterminator",
917: 	                                                "columns",
918: 	                                                "auto_type_candidates",
919: 	                                                "max_line_size",
920: 	                                                "ignore_errors",
921: 	                                                "store_rejects",
922: 	                                                "rejects_table",
923: 	                                                "rejects_scan",
924: 	                                                "rejects_limit",
925: 	                                                "force_not_null",
926: 	                                                "buffer_size",
927: 	                                                "decimal",
928: 	                                                "allow_quoted_nulls",
929: 	                                                "filename",
930: 	                                                "hive_partitioning",
931: 	                                                "union_by_name",
932: 	                                                "hive_types",
933: 	                                                "hive_types_autocast"};
934: 
935: 	std::ostringstream error;
936: 	error << "The methods read_csv and read_csv_auto do not have the \"" << unkown_parameter << "\" argument." << '\n';
937: 	error << "Possible arguments as suggestions: " << '\n';
938: 	vector<string> parameters(valid_parameters.begin(), valid_parameters.end());
939: 	auto suggestions = StringUtil::TopNJaroWinkler(parameters, unkown_parameter, 3);
940: 	for (auto &suggestion : suggestions) {
941: 		error << "* " << suggestion << '\n';
942: 	}
943: 	throw InvalidInputException(error.str());
944: }
945: 
946: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadCSV(const py::object &name_p, py::kwargs &kwargs) {
947: 	py::object header = py::none();
948: 	py::object compression = py::none();
949: 	py::object sep = py::none();
950: 	py::object delimiter = py::none();
951: 	py::object dtype = py::none();
952: 	py::object na_values = py::none();
953: 	py::object skiprows = py::none();
954: 	py::object quotechar = py::none();
955: 	py::object escapechar = py::none();
956: 	py::object encoding = py::none();
957: 	py::object parallel = py::none();
958: 	py::object date_format = py::none();
959: 	py::object timestamp_format = py::none();
960: 	py::object sample_size = py::none();
961: 	py::object all_varchar = py::none();
962: 	py::object normalize_names = py::none();
963: 	py::object null_padding = py::none();
964: 	py::object names_p = py::none();
965: 	py::object lineterminator = py::none();
966: 	py::object columns = py::none();
967: 	py::object auto_type_candidates = py::none();
968: 	py::object max_line_size = py::none();
969: 	py::object ignore_errors = py::none();
970: 	py::object store_rejects = py::none();
971: 	py::object rejects_table = py::none();
972: 	py::object rejects_scan = py::none();
973: 	py::object rejects_limit = py::none();
974: 	py::object force_not_null = py::none();
975: 	py::object buffer_size = py::none();
976: 	py::object decimal = py::none();
977: 	py::object allow_quoted_nulls = py::none();
978: 	py::object filename = py::none();
979: 	py::object hive_partitioning = py::none();
980: 	py::object union_by_name = py::none();
981: 	py::object hive_types = py::none();
982: 	py::object hive_types_autocast = py::none();
983: 	for (auto &arg : kwargs) {
984: 		const auto &arg_name = py::str(arg.first).cast<std::string>();
985: 
986: 		if (arg_name == "header") {
987: 			header = kwargs[arg_name.c_str()];
988: 		} else if (arg_name == "compression") {
989: 			compression = kwargs[arg_name.c_str()];
990: 		} else if (arg_name == "sep") {
991: 			sep = kwargs[arg_name.c_str()];
992: 		} else if (arg_name == "delimiter") {
993: 			delimiter = kwargs[arg_name.c_str()];
994: 		} else if (arg_name == "dtype") {
995: 			dtype = kwargs[arg_name.c_str()];
996: 		} else if (arg_name == "na_values") {
997: 			na_values = kwargs[arg_name.c_str()];
998: 		} else if (arg_name == "skiprows") {
999: 			skiprows = kwargs[arg_name.c_str()];
1000: 		} else if (arg_name == "quotechar") {
1001: 			quotechar = kwargs[arg_name.c_str()];
1002: 		} else if (arg_name == "escapechar") {
1003: 			escapechar = kwargs[arg_name.c_str()];
1004: 		} else if (arg_name == "encoding") {
1005: 			encoding = kwargs[arg_name.c_str()];
1006: 		} else if (arg_name == "parallel") {
1007: 			parallel = kwargs[arg_name.c_str()];
1008: 		} else if (arg_name == "date_format") {
1009: 			date_format = kwargs[arg_name.c_str()];
1010: 		} else if (arg_name == "timestamp_format") {
1011: 			timestamp_format = kwargs[arg_name.c_str()];
1012: 		} else if (arg_name == "sample_size") {
1013: 			sample_size = kwargs[arg_name.c_str()];
1014: 		} else if (arg_name == "all_varchar") {
1015: 			all_varchar = kwargs[arg_name.c_str()];
1016: 		} else if (arg_name == "normalize_names") {
1017: 			normalize_names = kwargs[arg_name.c_str()];
1018: 		} else if (arg_name == "null_padding") {
1019: 			null_padding = kwargs[arg_name.c_str()];
1020: 		} else if (arg_name == "names") {
1021: 			names_p = kwargs[arg_name.c_str()];
1022: 		} else if (arg_name == "lineterminator") {
1023: 			lineterminator = kwargs[arg_name.c_str()];
1024: 		} else if (arg_name == "columns") {
1025: 			columns = kwargs[arg_name.c_str()];
1026: 		} else if (arg_name == "auto_type_candidates") {
1027: 			auto_type_candidates = kwargs[arg_name.c_str()];
1028: 		} else if (arg_name == "max_line_size") {
1029: 			max_line_size = kwargs[arg_name.c_str()];
1030: 		} else if (arg_name == "ignore_errors") {
1031: 			ignore_errors = kwargs[arg_name.c_str()];
1032: 		} else if (arg_name == "store_rejects") {
1033: 			store_rejects = kwargs[arg_name.c_str()];
1034: 		} else if (arg_name == "rejects_table") {
1035: 			rejects_table = kwargs[arg_name.c_str()];
1036: 		} else if (arg_name == "rejects_scan") {
1037: 			rejects_scan = kwargs[arg_name.c_str()];
1038: 		} else if (arg_name == "rejects_limit") {
1039: 			rejects_limit = kwargs[arg_name.c_str()];
1040: 		} else if (arg_name == "force_not_null") {
1041: 			force_not_null = kwargs[arg_name.c_str()];
1042: 		} else if (arg_name == "buffer_size") {
1043: 			buffer_size = kwargs[arg_name.c_str()];
1044: 		} else if (arg_name == "decimal") {
1045: 			decimal = kwargs[arg_name.c_str()];
1046: 		} else if (arg_name == "allow_quoted_nulls") {
1047: 			allow_quoted_nulls = kwargs[arg_name.c_str()];
1048: 		} else if (arg_name == "filename") {
1049: 			filename = kwargs[arg_name.c_str()];
1050: 		} else if (arg_name == "hive_partitioning") {
1051: 			hive_partitioning = kwargs[arg_name.c_str()];
1052: 		} else if (arg_name == "union_by_name") {
1053: 			union_by_name = kwargs[arg_name.c_str()];
1054: 		} else if (arg_name == "hive_types") {
1055: 			hive_types = kwargs[arg_name.c_str()];
1056: 		} else if (arg_name == "hive_types_autocast") {
1057: 			hive_types_autocast = kwargs[arg_name.c_str()];
1058: 		} else {
1059: 			AcceptableCSVOptions(arg_name);
1060: 		}
1061: 	}
1062: 
1063: 	auto &connection = con.GetConnection();
1064: 	CSVReaderOptions options;
1065: 	auto path_like = GetPathLike(name_p);
1066: 	auto &name = path_like.files;
1067: 	auto file_like_object_wrapper = std::move(path_like.dependency);
1068: 	named_parameter_map_t bind_parameters;
1069: 
1070: 	ParseMultiFileReaderOptions(bind_parameters, filename, hive_partitioning, union_by_name, hive_types,
1071: 	                            hive_types_autocast);
1072: 
1073: 	// First check if the header is explicitly set
1074: 	// when false this affects the returned types, so it needs to be known at initialization of the relation
1075: 	if (!py::none().is(header)) {
1076: 
1077: 		bool header_as_int = py::isinstance<py::int_>(header);
1078: 		bool header_as_bool = py::isinstance<py::bool_>(header);
1079: 
1080: 		bool header_value;
1081: 		if (header_as_bool) {
1082: 			header_value = py::bool_(header);
1083: 		} else if (header_as_int) {
1084: 			if ((int)py::int_(header) != 0) {
1085: 				throw InvalidInputException("read_csv only accepts 0 if 'header' is given as an integer");
1086: 			}
1087: 			header_value = true;
1088: 		} else {
1089: 			throw InvalidInputException("read_csv only accepts 'header' as an integer, or a boolean");
1090: 		}
1091: 		bind_parameters["header"] = Value::BOOLEAN(header_value);
1092: 	}
1093: 
1094: 	if (!py::none().is(compression)) {
1095: 		if (!py::isinstance<py::str>(compression)) {
1096: 			throw InvalidInputException("read_csv only accepts 'compression' as a string");
1097: 		}
1098: 		bind_parameters["compression"] = Value(py::str(compression));
1099: 	}
1100: 
1101: 	if (!py::none().is(dtype)) {
1102: 		if (py::is_dict_like(dtype)) {
1103: 			child_list_t<Value> struct_fields;
1104: 			py::dict dtype_dict = dtype;
1105: 			for (auto &kv : dtype_dict) {
1106: 				shared_ptr<DuckDBPyType> sql_type;
1107: 				if (!py::try_cast(kv.second, sql_type)) {
1108: 					throw py::value_error("The types provided to 'dtype' have to be DuckDBPyType");
1109: 				}
1110: 				struct_fields.emplace_back(py::str(kv.first), Value(sql_type->ToString()));
1111: 			}
1112: 			auto dtype_struct = Value::STRUCT(std::move(struct_fields));
1113: 			bind_parameters["dtypes"] = std::move(dtype_struct);
1114: 		} else if (py::is_list_like(dtype)) {
1115: 			vector<Value> list_values;
1116: 			py::list dtype_list = dtype;
1117: 			for (auto &child : dtype_list) {
1118: 				shared_ptr<DuckDBPyType> sql_type;
1119: 				if (!py::try_cast(child, sql_type)) {
1120: 					throw py::value_error("The types provided to 'dtype' have to be DuckDBPyType");
1121: 				}
1122: 				list_values.push_back(sql_type->ToString());
1123: 			}
1124: 			bind_parameters["dtypes"] = Value::LIST(LogicalType::VARCHAR, std::move(list_values));
1125: 		} else {
1126: 			throw InvalidInputException("read_csv only accepts 'dtype' as a dictionary or a list of strings");
1127: 		}
1128: 	}
1129: 
1130: 	bool has_sep = !py::none().is(sep);
1131: 	bool has_delimiter = !py::none().is(delimiter);
1132: 	if (has_sep && has_delimiter) {
1133: 		throw InvalidInputException("read_csv takes either 'delimiter' or 'sep', not both");
1134: 	}
1135: 	if (has_sep) {
1136: 		bind_parameters["delim"] = Value(py::str(sep));
1137: 	} else if (has_delimiter) {
1138: 		bind_parameters["delim"] = Value(py::str(delimiter));
1139: 	}
1140: 
1141: 	if (!py::none().is(names_p)) {
1142: 		if (!py::is_list_like(names_p)) {
1143: 			throw InvalidInputException("read_csv only accepts 'names' as a list of strings");
1144: 		}
1145: 		vector<Value> names;
1146: 		py::list names_list = names_p;
1147: 		for (auto &elem : names_list) {
1148: 			if (!py::isinstance<py::str>(elem)) {
1149: 				throw InvalidInputException("read_csv 'names' list has to consist of only strings");
1150: 			}
1151: 			names.push_back(Value(std::string(py::str(elem))));
1152: 		}
1153: 		bind_parameters["names"] = Value::LIST(LogicalType::VARCHAR, std::move(names));
1154: 	}
1155: 
1156: 	if (!py::none().is(na_values)) {
1157: 		vector<Value> null_values;
1158: 		if (!py::isinstance<py::str>(na_values) && !py::is_list_like(na_values)) {
1159: 			throw InvalidInputException("read_csv only accepts 'na_values' as a string or a list of strings");
1160: 		} else if (py::isinstance<py::str>(na_values)) {
1161: 			null_values.push_back(Value(py::str(na_values)));
1162: 		} else {
1163: 			py::list null_list = na_values;
1164: 			for (auto &elem : null_list) {
1165: 				if (!py::isinstance<py::str>(elem)) {
1166: 					throw InvalidInputException("read_csv 'na_values' list has to consist of only strings");
1167: 				}
1168: 				null_values.push_back(Value(std::string(py::str(elem))));
1169: 			}
1170: 		}
1171: 		bind_parameters["nullstr"] = Value::LIST(LogicalType::VARCHAR, std::move(null_values));
1172: 	}
1173: 
1174: 	if (!py::none().is(skiprows)) {
1175: 		if (!py::isinstance<py::int_>(skiprows)) {
1176: 			throw InvalidInputException("read_csv only accepts 'skiprows' as an integer");
1177: 		}
1178: 		bind_parameters["skip"] = Value::INTEGER(py::int_(skiprows));
1179: 	}
1180: 
1181: 	if (!py::none().is(parallel)) {
1182: 		if (!py::isinstance<py::bool_>(parallel)) {
1183: 			throw InvalidInputException("read_csv only accepts 'parallel' as a boolean");
1184: 		}
1185: 		bind_parameters["parallel"] = Value::BOOLEAN(py::bool_(parallel));
1186: 	}
1187: 
1188: 	if (!py::none().is(quotechar)) {
1189: 		if (!py::isinstance<py::str>(quotechar)) {
1190: 			throw InvalidInputException("read_csv only accepts 'quotechar' as a string");
1191: 		}
1192: 		bind_parameters["quote"] = Value(py::str(quotechar));
1193: 	}
1194: 
1195: 	if (!py::none().is(escapechar)) {
1196: 		if (!py::isinstance<py::str>(escapechar)) {
1197: 			throw InvalidInputException("read_csv only accepts 'escapechar' as a string");
1198: 		}
1199: 		bind_parameters["escape"] = Value(py::str(escapechar));
1200: 	}
1201: 
1202: 	if (!py::none().is(encoding)) {
1203: 		if (!py::isinstance<py::str>(encoding)) {
1204: 			throw InvalidInputException("read_csv only accepts 'encoding' as a string");
1205: 		}
1206: 		string encoding_str = StringUtil::Lower(py::str(encoding));
1207: 		if (encoding_str != "utf8" && encoding_str != "utf-8") {
1208: 			throw BinderException("Copy is only supported for UTF-8 encoded files, ENCODING 'UTF-8'");
1209: 		}
1210: 	}
1211: 
1212: 	if (!py::none().is(date_format)) {
1213: 		if (!py::isinstance<py::str>(date_format)) {
1214: 			throw InvalidInputException("read_csv only accepts 'date_format' as a string");
1215: 		}
1216: 		bind_parameters["dateformat"] = Value(py::str(date_format));
1217: 	}
1218: 
1219: 	if (!py::none().is(timestamp_format)) {
1220: 		if (!py::isinstance<py::str>(timestamp_format)) {
1221: 			throw InvalidInputException("read_csv only accepts 'timestamp_format' as a string");
1222: 		}
1223: 		bind_parameters["timestampformat"] = Value(py::str(timestamp_format));
1224: 	}
1225: 
1226: 	if (!py::none().is(sample_size)) {
1227: 		if (!py::isinstance<py::int_>(sample_size)) {
1228: 			throw InvalidInputException("read_csv only accepts 'sample_size' as an integer");
1229: 		}
1230: 		bind_parameters["sample_size"] = Value::INTEGER(py::int_(sample_size));
1231: 	}
1232: 
1233: 	if (!py::none().is(all_varchar)) {
1234: 		if (!py::isinstance<py::bool_>(all_varchar)) {
1235: 			throw InvalidInputException("read_csv only accepts 'all_varchar' as a boolean");
1236: 		}
1237: 		bind_parameters["all_varchar"] = Value::BOOLEAN(py::bool_(all_varchar));
1238: 	}
1239: 
1240: 	if (!py::none().is(normalize_names)) {
1241: 		if (!py::isinstance<py::bool_>(normalize_names)) {
1242: 			throw InvalidInputException("read_csv only accepts 'normalize_names' as a boolean");
1243: 		}
1244: 		bind_parameters["normalize_names"] = Value::BOOLEAN(py::bool_(normalize_names));
1245: 	}
1246: 
1247: 	if (!py::none().is(null_padding)) {
1248: 		if (!py::isinstance<py::bool_>(null_padding)) {
1249: 			throw InvalidInputException("read_csv only accepts 'null_padding' as a boolean");
1250: 		}
1251: 		bind_parameters["null_padding"] = Value::BOOLEAN(py::bool_(null_padding));
1252: 	}
1253: 
1254: 	if (!py::none().is(lineterminator)) {
1255: 		PythonCSVLineTerminator::Type new_line_type;
1256: 		if (!py::try_cast<PythonCSVLineTerminator::Type>(lineterminator, new_line_type)) {
1257: 			string actual_type = py::str(lineterminator.get_type());
1258: 			throw BinderException("read_csv only accepts 'lineterminator' as a string or CSVLineTerminator, not '%s'",
1259: 			                      actual_type);
1260: 		}
1261: 		bind_parameters["new_line"] = Value(PythonCSVLineTerminator::ToString(new_line_type));
1262: 	}
1263: 
1264: 	if (!py::none().is(max_line_size)) {
1265: 		if (!py::isinstance<py::str>(max_line_size) && !py::isinstance<py::int_>(max_line_size)) {
1266: 			string actual_type = py::str(max_line_size.get_type());
1267: 			throw BinderException("read_csv only accepts 'max_line_size' as a string or an integer, not '%s'",
1268: 			                      actual_type);
1269: 		}
1270: 		auto val = TransformPythonValue(max_line_size, LogicalTypeId::VARCHAR);
1271: 		bind_parameters["max_line_size"] = val;
1272: 	}
1273: 
1274: 	if (!py::none().is(auto_type_candidates)) {
1275: 		if (!py::isinstance<py::list>(auto_type_candidates)) {
1276: 			string actual_type = py::str(auto_type_candidates.get_type());
1277: 			throw BinderException("read_csv only accepts 'auto_type_candidates' as a list[str], not '%s'", actual_type);
1278: 		}
1279: 		auto val = TransformPythonValue(auto_type_candidates, LogicalType::LIST(LogicalTypeId::VARCHAR));
1280: 		bind_parameters["auto_type_candidates"] = val;
1281: 	}
1282: 
1283: 	if (!py::none().is(ignore_errors)) {
1284: 		if (!py::isinstance<py::bool_>(ignore_errors)) {
1285: 			string actual_type = py::str(ignore_errors.get_type());
1286: 			throw BinderException("read_csv only accepts 'ignore_errors' as a bool, not '%s'", actual_type);
1287: 		}
1288: 		auto val = TransformPythonValue(ignore_errors, LogicalTypeId::BOOLEAN);
1289: 		bind_parameters["ignore_errors"] = val;
1290: 	}
1291: 
1292: 	if (!py::none().is(store_rejects)) {
1293: 		if (!py::isinstance<py::bool_>(store_rejects)) {
1294: 			string actual_type = py::str(store_rejects.get_type());
1295: 			throw BinderException("read_csv only accepts 'store_rejects' as a bool, not '%s'", actual_type);
1296: 		}
1297: 		auto val = TransformPythonValue(store_rejects, LogicalTypeId::BOOLEAN);
1298: 		bind_parameters["store_rejects"] = val;
1299: 	}
1300: 
1301: 	if (!py::none().is(rejects_table)) {
1302: 		if (!py::isinstance<py::str>(rejects_table)) {
1303: 			string actual_type = py::str(rejects_table.get_type());
1304: 			throw BinderException("read_csv only accepts 'rejects_table' as a string, not '%s'", actual_type);
1305: 		}
1306: 		auto val = TransformPythonValue(rejects_table, LogicalTypeId::VARCHAR);
1307: 		bind_parameters["rejects_table"] = val;
1308: 	}
1309: 
1310: 	if (!py::none().is(rejects_scan)) {
1311: 		if (!py::isinstance<py::str>(rejects_scan)) {
1312: 			string actual_type = py::str(rejects_scan.get_type());
1313: 			throw BinderException("read_csv only accepts 'rejects_scan' as a string, not '%s'", actual_type);
1314: 		}
1315: 		auto val = TransformPythonValue(rejects_scan, LogicalTypeId::VARCHAR);
1316: 		bind_parameters["rejects_scan"] = val;
1317: 	}
1318: 
1319: 	if (!py::none().is(rejects_limit)) {
1320: 		if (!py::isinstance<py::int_>(rejects_limit)) {
1321: 			string actual_type = py::str(rejects_limit.get_type());
1322: 			throw BinderException("read_csv only accepts 'rejects_limit' as an int, not '%s'", actual_type);
1323: 		}
1324: 		auto val = TransformPythonValue(rejects_limit, LogicalTypeId::BIGINT);
1325: 		bind_parameters["rejects_limit"] = val;
1326: 	}
1327: 
1328: 	if (!py::none().is(force_not_null)) {
1329: 		if (!py::isinstance<py::list>(force_not_null)) {
1330: 			string actual_type = py::str(force_not_null.get_type());
1331: 			throw BinderException("read_csv only accepts 'force_not_null' as a list[str], not '%s'", actual_type);
1332: 		}
1333: 		auto val = TransformPythonValue(force_not_null, LogicalType::LIST(LogicalTypeId::VARCHAR));
1334: 		bind_parameters["force_not_null"] = val;
1335: 	}
1336: 
1337: 	if (!py::none().is(buffer_size)) {
1338: 		if (!py::isinstance<py::int_>(buffer_size)) {
1339: 			string actual_type = py::str(buffer_size.get_type());
1340: 			throw BinderException("read_csv only accepts 'buffer_size' as a list[str], not '%s'", actual_type);
1341: 		}
1342: 		auto val = TransformPythonValue(buffer_size, LogicalTypeId::UBIGINT);
1343: 		bind_parameters["buffer_size"] = val;
1344: 	}
1345: 
1346: 	if (!py::none().is(decimal)) {
1347: 		if (!py::isinstance<py::str>(decimal)) {
1348: 			string actual_type = py::str(decimal.get_type());
1349: 			throw BinderException("read_csv only accepts 'decimal' as a string, not '%s'", actual_type);
1350: 		}
1351: 		auto val = TransformPythonValue(decimal, LogicalTypeId::VARCHAR);
1352: 		bind_parameters["decimal_separator"] = val;
1353: 	}
1354: 
1355: 	if (!py::none().is(allow_quoted_nulls)) {
1356: 		if (!py::isinstance<py::bool_>(allow_quoted_nulls)) {
1357: 			string actual_type = py::str(allow_quoted_nulls.get_type());
1358: 			throw BinderException("read_csv only accepts 'allow_quoted_nulls' as a bool, not '%s'", actual_type);
1359: 		}
1360: 		auto val = TransformPythonValue(allow_quoted_nulls, LogicalTypeId::BOOLEAN);
1361: 		bind_parameters["allow_quoted_nulls"] = val;
1362: 	}
1363: 
1364: 	if (!py::none().is(columns)) {
1365: 		if (!py::is_dict_like(columns)) {
1366: 			throw BinderException("read_csv only accepts 'columns' as a dict[str, str]");
1367: 		}
1368: 		py::dict columns_dict = columns;
1369: 		child_list_t<Value> struct_fields;
1370: 
1371: 		for (auto &kv : columns_dict) {
1372: 			auto &column_name = kv.first;
1373: 			auto &type = kv.second;
1374: 			if (!py::isinstance<py::str>(column_name)) {
1375: 				string actual_type = py::str(column_name.get_type());
1376: 				throw BinderException("The provided column name must be a str, not of type '%s'", actual_type);
1377: 			}
1378: 			if (!py::isinstance<py::str>(type)) {
1379: 				string actual_type = py::str(column_name.get_type());
1380: 				throw BinderException("The provided column type must be a str, not of type '%s'", actual_type);
1381: 			}
1382: 			struct_fields.emplace_back(py::str(column_name), Value(py::str(type)));
1383: 		}
1384: 		auto dtype_struct = Value::STRUCT(std::move(struct_fields));
1385: 		bind_parameters["columns"] = std::move(dtype_struct);
1386: 	}
1387: 
1388: 	// Create the ReadCSV Relation using the 'options'
1389: 
1390: 	py::gil_scoped_release gil;
1391: 	auto read_csv_p = connection.ReadCSV(name, std::move(bind_parameters));
1392: 	auto &read_csv = read_csv_p->Cast<ReadCSVRelation>();
1393: 	if (file_like_object_wrapper) {
1394: 		read_csv.AddExternalDependency(std::move(file_like_object_wrapper));
1395: 	}
1396: 
1397: 	return make_uniq<DuckDBPyRelation>(read_csv_p->Alias(read_csv.alias));
1398: }
1399: 
1400: void DuckDBPyConnection::ExecuteImmediately(vector<unique_ptr<SQLStatement>> statements) {
1401: 	auto &connection = con.GetConnection();
1402: 	py::gil_scoped_release release;
1403: 	if (statements.empty()) {
1404: 		return;
1405: 	}
1406: 	for (auto &stmt : statements) {
1407: 		if (!stmt->named_param_map.empty()) {
1408: 			throw NotImplementedException(
1409: 			    "Prepared parameters are only supported for the last statement, please split your query up into "
1410: 			    "separate 'execute' calls if you want to use prepared parameters");
1411: 		}
1412: 		auto pending_query = connection.PendingQuery(std::move(stmt), false);
1413: 		if (pending_query->HasError()) {
1414: 			pending_query->ThrowError();
1415: 		}
1416: 		auto res = CompletePendingQuery(*pending_query);
1417: 
1418: 		if (res->HasError()) {
1419: 			res->ThrowError();
1420: 		}
1421: 	}
1422: }
1423: 
1424: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::RunQuery(const py::object &query, string alias, py::object params) {
1425: 	auto &connection = con.GetConnection();
1426: 	if (alias.empty()) {
1427: 		alias = "unnamed_relation_" + StringUtil::GenerateRandomName(16);
1428: 	}
1429: 
1430: 	auto statements = GetStatements(query);
1431: 	if (statements.empty()) {
1432: 		// TODO: should we throw?
1433: 		return nullptr;
1434: 	}
1435: 
1436: 	auto last_statement = std::move(statements.back());
1437: 	statements.pop_back();
1438: 	// First immediately execute any preceding statements (if any)
1439: 	ExecuteImmediately(std::move(statements));
1440: 
1441: 	// Attempt to create a Relation for lazy execution if possible
1442: 	shared_ptr<Relation> relation;
1443: 	if (py::none().is(params)) {
1444: 		// FIXME: currently we can't create relations with prepared parameters
1445: 		{
1446: 			py::gil_scoped_release gil;
1447: 			auto statement_type = last_statement->type;
1448: 			switch (statement_type) {
1449: 			case StatementType::SELECT_STATEMENT: {
1450: 				auto select_statement = unique_ptr_cast<SQLStatement, SelectStatement>(std::move(last_statement));
1451: 				relation = connection.RelationFromQuery(std::move(select_statement), alias);
1452: 				break;
1453: 			}
1454: 			default:
1455: 				break;
1456: 			}
1457: 		}
1458: 	}
1459: 
1460: 	if (!relation) {
1461: 		// Could not create a relation, resort to direct execution
1462: 		auto prep = PrepareQuery(std::move(last_statement));
1463: 		auto res = ExecuteInternal(*prep, std::move(params));
1464: 		if (!res) {
1465: 			return nullptr;
1466: 		}
1467: 		if (res->properties.return_type != StatementReturnType::QUERY_RESULT) {
1468: 			return nullptr;
1469: 		}
1470: 		if (res->type == QueryResultType::STREAM_RESULT) {
1471: 			auto &stream_result = res->Cast<StreamQueryResult>();
1472: 			res = stream_result.Materialize();
1473: 		}
1474: 		auto &materialized_result = res->Cast<MaterializedQueryResult>();
1475: 		relation = make_shared_ptr<MaterializedRelation>(connection.context, materialized_result.TakeCollection(),
1476: 		                                                 res->names, alias);
1477: 	}
1478: 	return make_uniq<DuckDBPyRelation>(std::move(relation));
1479: }
1480: 
1481: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Table(const string &tname) {
1482: 	auto &connection = con.GetConnection();
1483: 	auto qualified_name = QualifiedName::Parse(tname);
1484: 	if (qualified_name.schema.empty()) {
1485: 		qualified_name.schema = DEFAULT_SCHEMA;
1486: 	}
1487: 	try {
1488: 		return make_uniq<DuckDBPyRelation>(connection.Table(qualified_name.schema, qualified_name.name));
1489: 	} catch (const CatalogException &) {
1490: 		// CatalogException will be of the type '... is not a table'
1491: 		// Not a table in the database, make a query relation that can perform replacement scans
1492: 		auto sql_query = StringUtil::Format("from %s", KeywordHelper::WriteOptionallyQuoted(tname));
1493: 		return RunQuery(py::str(sql_query), tname);
1494: 	}
1495: }
1496: 
1497: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Values(py::object params) {
1498: 	auto &connection = con.GetConnection();
1499: 	if (params.is_none()) {
1500: 		params = py::list();
1501: 	}
1502: 	if (!py::hasattr(params, "__len__")) {
1503: 		throw InvalidInputException("Type of object passed to parameter 'values' must be iterable");
1504: 	}
1505: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(params)};
1506: 	return make_uniq<DuckDBPyRelation>(connection.Values(values));
1507: }
1508: 
1509: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::View(const string &vname) {
1510: 	auto &connection = con.GetConnection();
1511: 	return make_uniq<DuckDBPyRelation>(connection.View(vname));
1512: }
1513: 
1514: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fname, py::object params) {
1515: 	auto &connection = con.GetConnection();
1516: 	if (params.is_none()) {
1517: 		params = py::list();
1518: 	}
1519: 	if (!py::is_list_like(params)) {
1520: 		throw InvalidInputException("'params' has to be a list of parameters");
1521: 	}
1522: 
1523: 	return make_uniq<DuckDBPyRelation>(
1524: 	    connection.TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(params)));
1525: }
1526: 
1527: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(const PandasDataFrame &value) {
1528: 	auto &connection = con.GetConnection();
1529: 	string name = "df_" + StringUtil::GenerateRandomName();
1530: 	if (PandasDataFrame::IsPyArrowBacked(value)) {
1531: 		auto table = PandasDataFrame::ToArrowTable(value);
1532: 		return DuckDBPyConnection::FromArrow(table);
1533: 	}
1534: 	auto tableref = PythonReplacementScan::ReplacementObject(value, name, *connection.context);
1535: 	D_ASSERT(tableref);
1536: 	auto rel = make_shared_ptr<ViewRelation>(connection.context, std::move(tableref), name);
1537: 	return make_uniq<DuckDBPyRelation>(std::move(rel));
1538: }
1539: 
1540: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_glob, bool binary_as_string,
1541:                                                              bool file_row_number, bool filename,
1542:                                                              bool hive_partitioning, bool union_by_name,
1543:                                                              const py::object &compression) {
1544: 	auto &connection = con.GetConnection();
1545: 	string name = "parquet_" + StringUtil::GenerateRandomName();
1546: 	vector<Value> params;
1547: 	params.emplace_back(file_glob);
1548: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)},
1549: 	                                        {"file_row_number", Value::BOOLEAN(file_row_number)},
1550: 	                                        {"filename", Value::BOOLEAN(filename)},
1551: 	                                        {"hive_partitioning", Value::BOOLEAN(hive_partitioning)},
1552: 	                                        {"union_by_name", Value::BOOLEAN(union_by_name)}});
1553: 
1554: 	if (!py::none().is(compression)) {
1555: 		if (!py::isinstance<py::str>(compression)) {
1556: 			throw InvalidInputException("from_parquet only accepts 'compression' as a string");
1557: 		}
1558: 		named_parameters["compression"] = Value(py::str(compression));
1559: 	}
1560: 	py::gil_scoped_release gil;
1561: 	return make_uniq<DuckDBPyRelation>(connection.TableFunction("parquet_scan", params, named_parameters)->Alias(name));
1562: }
1563: 
1564: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquets(const vector<string> &file_globs, bool binary_as_string,
1565:                                                               bool file_row_number, bool filename,
1566:                                                               bool hive_partitioning, bool union_by_name,
1567:                                                               const py::object &compression) {
1568: 	auto &connection = con.GetConnection();
1569: 	string name = "parquet_" + StringUtil::GenerateRandomName();
1570: 	vector<Value> params;
1571: 	auto file_globs_as_value = vector<Value>();
1572: 	for (const auto &file : file_globs) {
1573: 		file_globs_as_value.emplace_back(file);
1574: 	}
1575: 	params.emplace_back(Value::LIST(file_globs_as_value));
1576: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)},
1577: 	                                        {"file_row_number", Value::BOOLEAN(file_row_number)},
1578: 	                                        {"filename", Value::BOOLEAN(filename)},
1579: 	                                        {"hive_partitioning", Value::BOOLEAN(hive_partitioning)},
1580: 	                                        {"union_by_name", Value::BOOLEAN(union_by_name)}});
1581: 
1582: 	if (!py::none().is(compression)) {
1583: 		if (!py::isinstance<py::str>(compression)) {
1584: 			throw InvalidInputException("from_parquet only accepts 'compression' as a string");
1585: 		}
1586: 		named_parameters["compression"] = Value(py::str(compression));
1587: 	}
1588: 
1589: 	return make_uniq<DuckDBPyRelation>(connection.TableFunction("parquet_scan", params, named_parameters)->Alias(name));
1590: }
1591: 
1592: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrow(py::object &arrow_object) {
1593: 	auto &connection = con.GetConnection();
1594: 	string name = "arrow_object_" + StringUtil::GenerateRandomName();
1595: 	if (!IsAcceptedArrowObject(arrow_object)) {
1596: 		auto py_object_type = string(py::str(arrow_object.get_type().attr("__name__")));
1597: 		throw InvalidInputException("Python Object Type %s is not an accepted Arrow Object.", py_object_type);
1598: 	}
1599: 	auto tableref = PythonReplacementScan::ReplacementObject(arrow_object, name, *connection.context);
1600: 	D_ASSERT(tableref);
1601: 	auto rel = make_shared_ptr<ViewRelation>(connection.context, std::move(tableref), name);
1602: 	return make_uniq<DuckDBPyRelation>(std::move(rel));
1603: }
1604: 
1605: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstrait(py::bytes &proto) {
1606: 	auto &connection = con.GetConnection();
1607: 	string name = "substrait_" + StringUtil::GenerateRandomName();
1608: 	vector<Value> params;
1609: 	params.emplace_back(Value::BLOB_RAW(proto));
1610: 	return make_uniq<DuckDBPyRelation>(connection.TableFunction("from_substrait", params)->Alias(name));
1611: }
1612: 
1613: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstrait(const string &query, bool enable_optimizer) {
1614: 	auto &connection = con.GetConnection();
1615: 	vector<Value> params;
1616: 	params.emplace_back(query);
1617: 	named_parameter_map_t named_parameters({{"enable_optimizer", Value::BOOLEAN(enable_optimizer)}});
1618: 	return make_uniq<DuckDBPyRelation>(
1619: 	    connection.TableFunction("get_substrait", params, named_parameters)->Alias(query));
1620: }
1621: 
1622: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstraitJSON(const string &query, bool enable_optimizer) {
1623: 	auto &connection = con.GetConnection();
1624: 	vector<Value> params;
1625: 	params.emplace_back(query);
1626: 	named_parameter_map_t named_parameters({{"enable_optimizer", Value::BOOLEAN(enable_optimizer)}});
1627: 	return make_uniq<DuckDBPyRelation>(
1628: 	    connection.TableFunction("get_substrait_json", params, named_parameters)->Alias(query));
1629: }
1630: 
1631: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstraitJSON(const string &json) {
1632: 	auto &connection = con.GetConnection();
1633: 	string name = "from_substrait_" + StringUtil::GenerateRandomName();
1634: 	vector<Value> params;
1635: 	params.emplace_back(json);
1636: 	return make_uniq<DuckDBPyRelation>(connection.TableFunction("from_substrait_json", params)->Alias(name));
1637: }
1638: 
1639: unordered_set<string> DuckDBPyConnection::GetTableNames(const string &query) {
1640: 	auto &connection = con.GetConnection();
1641: 	return connection.GetTableNames(query);
1642: }
1643: 
1644: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::UnregisterPythonObject(const string &name) {
1645: 	auto &connection = con.GetConnection();
1646: 	if (!registered_objects.count(name)) {
1647: 		return shared_from_this();
1648: 	}
1649: 	py::gil_scoped_release release;
1650: 	// FIXME: DROP TEMPORARY VIEW? doesn't exist?
1651: 	connection.Query("DROP VIEW \"" + name + "\"");
1652: 	registered_objects.erase(name);
1653: 	return shared_from_this();
1654: }
1655: 
1656: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Begin() {
1657: 	ExecuteFromString("BEGIN TRANSACTION");
1658: 	return shared_from_this();
1659: }
1660: 
1661: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Commit() {
1662: 	auto &connection = con.GetConnection();
1663: 	if (connection.context->transaction.IsAutoCommit()) {
1664: 		return shared_from_this();
1665: 	}
1666: 	ExecuteFromString("COMMIT");
1667: 	return shared_from_this();
1668: }
1669: 
1670: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Rollback() {
1671: 	ExecuteFromString("ROLLBACK");
1672: 	return shared_from_this();
1673: }
1674: 
1675: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Checkpoint() {
1676: 	ExecuteFromString("CHECKPOINT");
1677: 	return shared_from_this();
1678: }
1679: 
1680: Optional<py::list> DuckDBPyConnection::GetDescription() {
1681: 	if (!con.HasResult()) {
1682: 		return py::none();
1683: 	}
1684: 	auto &result = con.GetResult();
1685: 	return result.Description();
1686: }
1687: 
1688: int DuckDBPyConnection::GetRowcount() {
1689: 	return -1;
1690: }
1691: 
1692: void DuckDBPyConnection::Close() {
1693: 	con.SetResult(nullptr);
1694: 	con.SetConnection(nullptr);
1695: 	con.SetDatabase(nullptr);
1696: 	// https://peps.python.org/pep-0249/#Connection.close
1697: 	cursors.ClearCursors();
1698: 	registered_functions.clear();
1699: }
1700: 
1701: void DuckDBPyConnection::Interrupt() {
1702: 	auto &connection = con.GetConnection();
1703: 	connection.Interrupt();
1704: }
1705: 
1706: void DuckDBPyConnection::InstallExtension(const string &extension, bool force_install, const py::object &repository,
1707:                                           const py::object &repository_url, const py::object &version) {
1708: 	auto &connection = con.GetConnection();
1709: 
1710: 	auto install_statement = make_uniq<LoadStatement>();
1711: 	install_statement->info = make_uniq<LoadInfo>();
1712: 	auto &info = *install_statement->info;
1713: 
1714: 	info.filename = extension;
1715: 
1716: 	const bool has_repository = !py::none().is(repository);
1717: 	const bool has_repository_url = !py::none().is(repository_url);
1718: 	if (has_repository && has_repository_url) {
1719: 		throw InvalidInputException(
1720: 		    "Both 'repository' and 'repository_url' are set which is not allowed, please pick one or the other");
1721: 	}
1722: 	string repository_string;
1723: 	if (has_repository) {
1724: 		repository_string = py::str(repository);
1725: 	} else if (has_repository_url) {
1726: 		repository_string = py::str(repository_url);
1727: 	}
1728: 
1729: 	if ((has_repository || has_repository_url) && repository_string.empty()) {
1730: 		throw InvalidInputException("The provided 'repository' or 'repository_url' can not be empty!");
1731: 	}
1732: 
1733: 	string version_string;
1734: 	if (!py::none().is(version)) {
1735: 		version_string = py::str(version);
1736: 		if (version_string.empty()) {
1737: 			throw InvalidInputException("The provided 'version' can not be empty!");
1738: 		}
1739: 	}
1740: 
1741: 	info.repository = repository_string;
1742: 	info.repo_is_alias = repository_string.empty() ? false : has_repository;
1743: 	info.version = version_string;
1744: 	info.load_type = force_install ? LoadType::FORCE_INSTALL : LoadType::INSTALL;
1745: 	auto res = connection.Query(std::move(install_statement));
1746: 	if (res->HasError()) {
1747: 		res->ThrowError();
1748: 	}
1749: }
1750: 
1751: void DuckDBPyConnection::LoadExtension(const string &extension) {
1752: 	auto &connection = con.GetConnection();
1753: 	ExtensionHelper::LoadExternalExtension(*connection.context, extension);
1754: }
1755: 
1756: void DuckDBPyConnection::Cursors::AddCursor(shared_ptr<DuckDBPyConnection> conn) {
1757: 	lock_guard<mutex> l(lock);
1758: 
1759: 	// Clean up previously created cursors
1760: 	vector<weak_ptr<DuckDBPyConnection>> compacted_cursors;
1761: 	bool needs_compaction = false;
1762: 	for (auto &cur_p : cursors) {
1763: 		auto cur = cur_p.lock();
1764: 		if (!cur) {
1765: 			needs_compaction = true;
1766: 			continue;
1767: 		}
1768: 		compacted_cursors.push_back(cur_p);
1769: 	}
1770: 	if (needs_compaction) {
1771: 		cursors = std::move(compacted_cursors);
1772: 	}
1773: 
1774: 	cursors.push_back(conn);
1775: }
1776: 
1777: void DuckDBPyConnection::Cursors::ClearCursors() {
1778: 	lock_guard<mutex> l(lock);
1779: 
1780: 	for (auto &cur : cursors) {
1781: 		auto cursor = cur.lock();
1782: 		if (!cursor) {
1783: 			// The cursor has already been closed
1784: 			continue;
1785: 		}
1786: 		cursor->Close();
1787: 	}
1788: 
1789: 	cursors.clear();
1790: }
1791: 
1792: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Cursor() {
1793: 	auto res = make_shared_ptr<DuckDBPyConnection>();
1794: 	res->con.SetDatabase(con);
1795: 	res->con.SetConnection(make_uniq<Connection>(res->con.GetDatabase()));
1796: 	cursors.AddCursor(res);
1797: 	return res;
1798: }
1799: 
1800: // these should be functions on the result but well
1801: Optional<py::tuple> DuckDBPyConnection::FetchOne() {
1802: 	if (!con.HasResult()) {
1803: 		throw InvalidInputException("No open result set");
1804: 	}
1805: 	auto &result = con.GetResult();
1806: 	return result.FetchOne();
1807: }
1808: 
1809: py::list DuckDBPyConnection::FetchMany(idx_t size) {
1810: 	if (!con.HasResult()) {
1811: 		throw InvalidInputException("No open result set");
1812: 	}
1813: 	auto &result = con.GetResult();
1814: 	return result.FetchMany(size);
1815: }
1816: 
1817: py::list DuckDBPyConnection::FetchAll() {
1818: 	if (!con.HasResult()) {
1819: 		throw InvalidInputException("No open result set");
1820: 	}
1821: 	auto &result = con.GetResult();
1822: 	return result.FetchAll();
1823: }
1824: 
1825: py::dict DuckDBPyConnection::FetchNumpy() {
1826: 	if (!con.HasResult()) {
1827: 		throw InvalidInputException("No open result set");
1828: 	}
1829: 	auto &result = con.GetResult();
1830: 	return result.FetchNumpyInternal();
1831: }
1832: 
1833: PandasDataFrame DuckDBPyConnection::FetchDF(bool date_as_object) {
1834: 	if (!con.HasResult()) {
1835: 		throw InvalidInputException("No open result set");
1836: 	}
1837: 	auto &result = con.GetResult();
1838: 	return result.FetchDF(date_as_object);
1839: }
1840: 
1841: PandasDataFrame DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk, bool date_as_object) {
1842: 	if (!con.HasResult()) {
1843: 		throw InvalidInputException("No open result set");
1844: 	}
1845: 	auto &result = con.GetResult();
1846: 	return result.FetchDFChunk(vectors_per_chunk, date_as_object);
1847: }
1848: 
1849: duckdb::pyarrow::Table DuckDBPyConnection::FetchArrow(idx_t rows_per_batch) {
1850: 	if (!con.HasResult()) {
1851: 		throw InvalidInputException("No open result set");
1852: 	}
1853: 	auto &result = con.GetResult();
1854: 	return result.ToArrowTable(rows_per_batch);
1855: }
1856: 
1857: py::dict DuckDBPyConnection::FetchPyTorch() {
1858: 	if (!con.HasResult()) {
1859: 		throw InvalidInputException("No open result set");
1860: 	}
1861: 	auto &result = con.GetResult();
1862: 	return result.FetchPyTorch();
1863: }
1864: 
1865: py::dict DuckDBPyConnection::FetchTF() {
1866: 	if (!con.HasResult()) {
1867: 		throw InvalidInputException("No open result set");
1868: 	}
1869: 	auto &result = con.GetResult();
1870: 	return result.FetchTF();
1871: }
1872: 
1873: PolarsDataFrame DuckDBPyConnection::FetchPolars(idx_t rows_per_batch) {
1874: 	auto arrow = FetchArrow(rows_per_batch);
1875: 	return py::cast<PolarsDataFrame>(py::module::import("polars").attr("DataFrame")(arrow));
1876: }
1877: 
1878: duckdb::pyarrow::RecordBatchReader DuckDBPyConnection::FetchRecordBatchReader(const idx_t rows_per_batch) {
1879: 	if (!con.HasResult()) {
1880: 		throw InvalidInputException("No open result set");
1881: 	}
1882: 	auto &result = con.GetResult();
1883: 	return result.FetchRecordBatchReader(rows_per_batch);
1884: }
1885: 
1886: case_insensitive_map_t<Value> TransformPyConfigDict(const py::dict &py_config_dict) {
1887: 	case_insensitive_map_t<Value> config_dict;
1888: 	for (auto &kv : py_config_dict) {
1889: 		auto key = py::str(kv.first);
1890: 		auto val = py::str(kv.second);
1891: 		config_dict[key] = Value(val);
1892: 	}
1893: 	return config_dict;
1894: }
1895: 
1896: static bool HasJupyterProgressBarDependencies() {
1897: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1898: 	if (!import_cache.ipywidgets()) {
1899: 		// ipywidgets not installed, needed to support the progress bar
1900: 		return false;
1901: 	}
1902: 	return true;
1903: }
1904: 
1905: static void SetDefaultConfigArguments(ClientContext &context) {
1906: 	if (!DuckDBPyConnection::IsInteractive()) {
1907: 		// Don't need to set any special default arguments
1908: 		return;
1909: 	}
1910: 
1911: 	auto &config = ClientConfig::GetConfig(context);
1912: 	config.enable_progress_bar = true;
1913: 
1914: 	if (!DuckDBPyConnection::IsJupyter()) {
1915: 		return;
1916: 	}
1917: 	if (!HasJupyterProgressBarDependencies()) {
1918: 		// Disable progress bar altogether
1919: 		config.system_progress_bar_disable_reason =
1920: 		    "required package 'ipywidgets' is missing, which is needed to render progress bars in Jupyter";
1921: 		config.enable_progress_bar = false;
1922: 		return;
1923: 	}
1924: 
1925: 	// Set the function used to create the display for the progress bar
1926: 	context.config.display_create_func = JupyterProgressBarDisplay::Create;
1927: }
1928: 
1929: void InstantiateNewInstance(DuckDB &db) {
1930: 	auto &db_instance = *db.instance;
1931: 	PandasScanFunction scan_fun;
1932: 	MapFunction map_fun;
1933: 	ExtensionUtil::RegisterFunction(db_instance, scan_fun);
1934: 	ExtensionUtil::RegisterFunction(db_instance, map_fun);
1935: }
1936: 
1937: static shared_ptr<DuckDBPyConnection> FetchOrCreateInstance(const string &database_path, DBConfig &config) {
1938: 	auto res = make_shared_ptr<DuckDBPyConnection>();
1939: 	bool cache_instance = database_path != ":memory:" && !database_path.empty();
1940: 	config.replacement_scans.emplace_back(PythonReplacementScan::Replace);
1941: 	{
1942: 		py::gil_scoped_release release;
1943: 		unique_lock<mutex> lock(res->py_connection_lock);
1944: 		auto database =
1945: 		    instance_cache.GetOrCreateInstance(database_path, config, cache_instance, InstantiateNewInstance);
1946: 		res->con.SetDatabase(std::move(database));
1947: 		res->con.SetConnection(make_uniq<Connection>(res->con.GetDatabase()));
1948: 	}
1949: 	return res;
1950: }
1951: 
1952: bool IsDefaultConnectionString(const string &database, bool read_only, case_insensitive_map_t<Value> &config) {
1953: 	bool is_default = StringUtil::CIEquals(database, ":default:");
1954: 	if (!is_default) {
1955: 		return false;
1956: 	}
1957: 	// Only allow fetching the default connection when no options are passed
1958: 	if (read_only == true || !config.empty()) {
1959: 		throw InvalidInputException("Default connection fetching is only allowed without additional options");
1960: 	}
1961: 	return true;
1962: }
1963: 
1964: static string GetPathString(const py::object &path) {
1965: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1966: 	const bool is_path = py::isinstance(path, import_cache.pathlib.Path());
1967: 	if (is_path || py::isinstance<py::str>(path)) {
1968: 		return std::string(py::str(path));
1969: 	}
1970: 	string actual_type = py::str(path.get_type());
1971: 	throw InvalidInputException("Please provide either a str or a pathlib.Path, not %s", actual_type);
1972: }
1973: 
1974: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Connect(const py::object &database_p, bool read_only,
1975:                                                            const py::dict &config_options) {
1976: 	auto config_dict = TransformPyConfigDict(config_options);
1977: 	auto database = GetPathString(database_p);
1978: 	if (IsDefaultConnectionString(database, read_only, config_dict)) {
1979: 		return DuckDBPyConnection::DefaultConnection();
1980: 	}
1981: 
1982: 	DBConfig config(read_only);
1983: 	config.AddExtensionOption("pandas_analyze_sample",
1984: 	                          "The maximum number of rows to sample when analyzing a pandas object column.",
1985: 	                          LogicalType::UBIGINT, Value::UBIGINT(1000));
1986: 	config.AddExtensionOption("python_enable_replacements",
1987: 	                          "Whether variables visible to the current stack should be used for replacement scans.",
1988: 	                          LogicalType::BOOLEAN, Value::BOOLEAN(true));
1989: 	config.AddExtensionOption(
1990: 	    "python_scan_all_frames",
1991: 	    "If set, restores the old behavior of scanning all preceding frames to locate the referenced variable.",
1992: 	    LogicalType::BOOLEAN, Value::BOOLEAN(false));
1993: 	if (!DuckDBPyConnection::IsJupyter()) {
1994: 		config_dict["duckdb_api"] = Value("python");
1995: 	} else {
1996: 		config_dict["duckdb_api"] = Value("python jupyter");
1997: 	}
1998: 	config.SetOptionsByName(config_dict);
1999: 
2000: 	auto res = FetchOrCreateInstance(database, config);
2001: 	auto &client_context = *res->con.GetConnection().context;
2002: 	SetDefaultConfigArguments(client_context);
2003: 	return res;
2004: }
2005: 
2006: vector<Value> DuckDBPyConnection::TransformPythonParamList(const py::handle &params) {
2007: 	vector<Value> args;
2008: 	args.reserve(py::len(params));
2009: 
2010: 	for (auto param : params) {
2011: 		args.emplace_back(TransformPythonValue(param, LogicalType::UNKNOWN, false));
2012: 	}
2013: 	return args;
2014: }
2015: 
2016: case_insensitive_map_t<BoundParameterData> DuckDBPyConnection::TransformPythonParamDict(const py::dict &params) {
2017: 	case_insensitive_map_t<BoundParameterData> args;
2018: 
2019: 	for (auto pair : params) {
2020: 		auto &key = pair.first;
2021: 		auto &value = pair.second;
2022: 		args[std::string(py::str(key))] = BoundParameterData(TransformPythonValue(value, LogicalType::UNKNOWN, false));
2023: 	}
2024: 	return args;
2025: }
2026: 
2027: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::DefaultConnection() {
2028: 	if (!default_connection) {
2029: 		py::dict config_dict;
2030: 		default_connection = DuckDBPyConnection::Connect(py::str(":memory:"), false, config_dict);
2031: 	}
2032: 	return default_connection;
2033: }
2034: 
2035: PythonImportCache *DuckDBPyConnection::ImportCache() {
2036: 	if (!import_cache) {
2037: 		import_cache = make_shared_ptr<PythonImportCache>();
2038: 	}
2039: 	return import_cache.get();
2040: }
2041: 
2042: ModifiedMemoryFileSystem &DuckDBPyConnection::GetObjectFileSystem() {
2043: 	if (!internal_object_filesystem) {
2044: 		D_ASSERT(!FileSystemIsRegistered("DUCKDB_INTERNAL_OBJECTSTORE"));
2045: 		auto &import_cache_py = *ImportCache();
2046: 		auto modified_memory_fs = import_cache_py.duckdb.filesystem.ModifiedMemoryFileSystem();
2047: 		if (modified_memory_fs.ptr() == nullptr) {
2048: 			throw InvalidInputException(
2049: 			    "This operation could not be completed because required module 'fsspec' is not installed");
2050: 		}
2051: 		internal_object_filesystem = make_shared_ptr<ModifiedMemoryFileSystem>(modified_memory_fs());
2052: 		auto &abstract_fs = reinterpret_cast<AbstractFileSystem &>(*internal_object_filesystem);
2053: 		RegisterFilesystem(abstract_fs);
2054: 	}
2055: 	return *internal_object_filesystem;
2056: }
2057: 
2058: bool DuckDBPyConnection::IsInteractive() {
2059: 	return DuckDBPyConnection::environment != PythonEnvironmentType::NORMAL;
2060: }
2061: 
2062: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Enter() {
2063: 	return shared_from_this();
2064: }
2065: 
2066: void DuckDBPyConnection::Exit(DuckDBPyConnection &self, const py::object &exc_type, const py::object &exc,
2067:                               const py::object &traceback) {
2068: 	self.Close();
2069: 	if (exc_type.ptr() != Py_None) {
2070: 		// Propagate the exception if any occurred
2071: 		PyErr_SetObject(exc_type.ptr(), exc.ptr());
2072: 		throw py::error_already_set();
2073: 	}
2074: }
2075: 
2076: void DuckDBPyConnection::Cleanup() {
2077: 	default_connection.reset();
2078: 	import_cache.reset();
2079: }
2080: 
2081: bool DuckDBPyConnection::IsPandasDataframe(const py::object &object) {
2082: 	if (!ModuleIsLoaded<PandasCacheItem>()) {
2083: 		return false;
2084: 	}
2085: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
2086: 	return py::isinstance(object, import_cache_py.pandas.DataFrame());
2087: }
2088: 
2089: bool DuckDBPyConnection::IsPolarsDataframe(const py::object &object) {
2090: 	if (!ModuleIsLoaded<PolarsCacheItem>()) {
2091: 		return false;
2092: 	}
2093: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
2094: 	return py::isinstance(object, import_cache_py.polars.DataFrame()) ||
2095: 	       py::isinstance(object, import_cache_py.polars.LazyFrame());
2096: }
2097: 
2098: bool IsValidNumpyDimensions(const py::handle &object, int &dim) {
2099: 	// check the dimensions of numpy arrays
2100: 	// should only be called by IsAcceptedNumpyObject
2101: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
2102: 	if (!py::isinstance(object, import_cache.numpy.ndarray())) {
2103: 		return false;
2104: 	}
2105: 	auto shape = (py::cast<py::array>(object)).attr("shape");
2106: 	if (py::len(shape) != 1) {
2107: 		return false;
2108: 	}
2109: 	int cur_dim = (shape.attr("__getitem__")(0)).cast<int>();
2110: 	dim = dim == -1 ? cur_dim : dim;
2111: 	return dim == cur_dim;
2112: }
2113: NumpyObjectType DuckDBPyConnection::IsAcceptedNumpyObject(const py::object &object) {
2114: 	if (!ModuleIsLoaded<NumpyCacheItem>()) {
2115: 		return NumpyObjectType::INVALID;
2116: 	}
2117: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
2118: 	if (py::isinstance(object, import_cache.numpy.ndarray())) {
2119: 		auto len = py::len((py::cast<py::array>(object)).attr("shape"));
2120: 		switch (len) {
2121: 		case 1:
2122: 			return NumpyObjectType::NDARRAY1D;
2123: 		case 2:
2124: 			return NumpyObjectType::NDARRAY2D;
2125: 		default:
2126: 			return NumpyObjectType::INVALID;
2127: 		}
2128: 	} else if (py::is_dict_like(object)) {
2129: 		int dim = -1;
2130: 		for (auto item : py::cast<py::dict>(object)) {
2131: 			if (!IsValidNumpyDimensions(item.second, dim)) {
2132: 				return NumpyObjectType::INVALID;
2133: 			}
2134: 		}
2135: 		return NumpyObjectType::DICT;
2136: 	} else if (py::is_list_like(object)) {
2137: 		int dim = -1;
2138: 		for (auto item : py::cast<py::list>(object)) {
2139: 			if (!IsValidNumpyDimensions(item, dim)) {
2140: 				return NumpyObjectType::INVALID;
2141: 			}
2142: 		}
2143: 		return NumpyObjectType::LIST;
2144: 	}
2145: 	return NumpyObjectType::INVALID;
2146: }
2147: 
2148: PyArrowObjectType DuckDBPyConnection::GetArrowType(const py::handle &obj) {
2149: 	D_ASSERT(py::gil_check());
2150: 
2151: 	if (py::isinstance<py::capsule>(obj)) {
2152: 		auto capsule = py::reinterpret_borrow<py::capsule>(obj);
2153: 		if (string(capsule.name()) != "arrow_array_stream") {
2154: 			throw InvalidInputException("Expected a 'arrow_array_stream' PyCapsule, got: %s", string(capsule.name()));
2155: 		}
2156: 		auto stream = capsule.get_pointer<struct ArrowArrayStream>();
2157: 		if (!stream->release) {
2158: 			throw InvalidInputException("The ArrowArrayStream was already released");
2159: 		}
2160: 		return PyArrowObjectType::PyCapsule;
2161: 	}
2162: 
2163: 	if (ModuleIsLoaded<PyarrowCacheItem>()) {
2164: 		auto &import_cache = *DuckDBPyConnection::ImportCache();
2165: 		// First Verify Lib Types
2166: 		auto table_class = import_cache.pyarrow.Table();
2167: 		auto record_batch_reader_class = import_cache.pyarrow.RecordBatchReader();
2168: 		if (py::isinstance(obj, table_class)) {
2169: 			return PyArrowObjectType::Table;
2170: 		} else if (py::isinstance(obj, record_batch_reader_class)) {
2171: 			return PyArrowObjectType::RecordBatchReader;
2172: 		}
2173: 
2174: 		if (ModuleIsLoaded<PyarrowDatasetCacheItem>()) {
2175: 			// Then Verify dataset types
2176: 			auto dataset_class = import_cache.pyarrow.dataset.Dataset();
2177: 			auto scanner_class = import_cache.pyarrow.dataset.Scanner();
2178: 
2179: 			if (py::isinstance(obj, scanner_class)) {
2180: 				return PyArrowObjectType::Scanner;
2181: 			} else if (py::isinstance(obj, dataset_class)) {
2182: 				return PyArrowObjectType::Dataset;
2183: 			}
2184: 		}
2185: 	}
2186: 
2187: 	if (py::hasattr(obj, "__arrow_c_stream__")) {
2188: 		return PyArrowObjectType::PyCapsuleInterface;
2189: 	}
2190: 
2191: 	return PyArrowObjectType::Invalid;
2192: }
2193: 
2194: bool DuckDBPyConnection::IsAcceptedArrowObject(const py::object &object) {
2195: 	return DuckDBPyConnection::GetArrowType(object) != PyArrowObjectType::Invalid;
2196: }
2197: 
2198: unique_lock<std::mutex> DuckDBPyConnection::AcquireConnectionLock() {
2199: 	// we first release the gil and then acquire the connection lock
2200: 	unique_lock<std::mutex> lock(py_connection_lock, std::defer_lock);
2201: 	{
2202: 		py::gil_scoped_release release;
2203: 		lock.lock();
2204: 	}
2205: 	return lock;
2206: }
2207: 
2208: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection.cpp]
[start of tools/pythonpkg/src/pyrelation.cpp]
1: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
2: #include "duckdb_python/pyrelation.hpp"
3: #include "duckdb_python/pyconnection/pyconnection.hpp"
4: #include "duckdb_python/pytype.hpp"
5: #include "duckdb_python/pyresult.hpp"
6: #include "duckdb/parser/qualified_name.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb_python/numpy/numpy_type.hpp"
9: #include "duckdb/main/relation/query_relation.hpp"
10: #include "duckdb/parser/parser.hpp"
11: #include "duckdb/main/relation/view_relation.hpp"
12: #include "duckdb/function/pragma/pragma_functions.hpp"
13: #include "duckdb/parser/statement/pragma_statement.hpp"
14: #include "duckdb/common/box_renderer.hpp"
15: #include "duckdb/main/query_result.hpp"
16: #include "duckdb/main/materialized_query_result.hpp"
17: #include "duckdb/parser/statement/explain_statement.hpp"
18: #include "duckdb/catalog/default/default_types.hpp"
19: #include "duckdb/main/relation/value_relation.hpp"
20: #include "duckdb/main/relation/filter_relation.hpp"
21: #include "duckdb_python/expression/pyexpression.hpp"
22: 
23: namespace duckdb {
24: 
25: DuckDBPyRelation::DuckDBPyRelation(shared_ptr<Relation> rel_p) : rel(std::move(rel_p)) {
26: 	if (!rel) {
27: 		throw InternalException("DuckDBPyRelation created without a relation");
28: 	}
29: 	this->executed = false;
30: 	auto &columns = rel->Columns();
31: 	for (auto &col : columns) {
32: 		names.push_back(col.GetName());
33: 		types.push_back(col.GetType());
34: 	}
35: }
36: 
37: bool DuckDBPyRelation::CanBeRegisteredBy(Connection &con) {
38: 	return CanBeRegisteredBy(con.context);
39: }
40: 
41: bool DuckDBPyRelation::CanBeRegisteredBy(ClientContext &context) {
42: 	if (!rel) {
43: 		// PyRelation without an internal relation can not be registered
44: 		return false;
45: 	}
46: 	auto this_context = rel->context.TryGetContext();
47: 	if (!this_context) {
48: 		return false;
49: 	}
50: 	return &context == this_context.get();
51: }
52: 
53: bool DuckDBPyRelation::CanBeRegisteredBy(shared_ptr<ClientContext> &con) {
54: 	if (!con) {
55: 		return false;
56: 	}
57: 	return CanBeRegisteredBy(*con);
58: }
59: 
60: DuckDBPyRelation::~DuckDBPyRelation() {
61: 	// FIXME: It makes sense to release the GIL here, but it causes a crash
62: 	// because pybind11's gil_scoped_acquire and gil_scoped_release can not be nested
63: 	// The Relation will need to call the destructor of the ExternalDependency, which might need to hold the GIL
64: 	// py::gil_scoped_release gil;
65: 	rel.reset();
66: }
67: 
68: DuckDBPyRelation::DuckDBPyRelation(unique_ptr<DuckDBPyResult> result_p) : rel(nullptr), result(std::move(result_p)) {
69: 	if (!result) {
70: 		throw InternalException("DuckDBPyRelation created without a result");
71: 	}
72: 	this->executed = true;
73: 	this->types = result->GetTypes();
74: 	this->names = result->GetNames();
75: }
76: 
77: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::ProjectFromExpression(const string &expression) {
78: 	auto projected_relation = make_uniq<DuckDBPyRelation>(rel->Project(expression));
79: 	for (auto &dep : this->rel->external_dependencies) {
80: 		projected_relation->rel->AddExternalDependency(dep);
81: 	}
82: 	return projected_relation;
83: }
84: 
85: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Project(const py::args &args, const string &groups) {
86: 	if (!rel) {
87: 		return nullptr;
88: 	}
89: 	auto arg_count = args.size();
90: 	if (arg_count == 0) {
91: 		return nullptr;
92: 	}
93: 	py::handle first_arg = args[0];
94: 	if (arg_count == 1 && py::isinstance<py::str>(first_arg)) {
95: 		string expr_string = py::str(first_arg);
96: 		return ProjectFromExpression(expr_string);
97: 	} else {
98: 		vector<unique_ptr<ParsedExpression>> expressions;
99: 		for (auto arg : args) {
100: 			shared_ptr<DuckDBPyExpression> py_expr;
101: 			if (!py::try_cast<shared_ptr<DuckDBPyExpression>>(arg, py_expr)) {
102: 				throw InvalidInputException("Please provide arguments of type Expression!");
103: 			}
104: 			auto expr = py_expr->GetExpression().Copy();
105: 			expressions.push_back(std::move(expr));
106: 		}
107: 		vector<string> empty_aliases;
108: 		if (groups.empty()) {
109: 			// No groups provided
110: 			return make_uniq<DuckDBPyRelation>(rel->Project(std::move(expressions), empty_aliases));
111: 		}
112: 		return make_uniq<DuckDBPyRelation>(rel->Aggregate(std::move(expressions), groups));
113: 	}
114: }
115: 
116: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::ProjectFromTypes(const py::object &obj) {
117: 	if (!rel) {
118: 		return nullptr;
119: 	}
120: 	if (!py::isinstance<py::list>(obj)) {
121: 		throw InvalidInputException("'columns_by_type' expects a list containing types");
122: 	}
123: 	auto list = py::list(obj);
124: 	vector<LogicalType> types_filter;
125: 	// Collect the list of types specified that will be our filter
126: 	for (auto &item : list) {
127: 		LogicalType type;
128: 		if (py::isinstance<py::str>(item)) {
129: 			string type_str = py::str(item);
130: 			type = TransformStringToLogicalType(type_str, *rel->context.GetContext());
131: 		} else if (py::isinstance<DuckDBPyType>(item)) {
132: 			auto *type_p = item.cast<DuckDBPyType *>();
133: 			type = type_p->Type();
134: 		} else {
135: 			string actual_type = py::str(item.get_type());
136: 			throw InvalidInputException("Can only project on objects of type DuckDBPyType or str, not '%s'",
137: 			                            actual_type);
138: 		}
139: 		types_filter.push_back(std::move(type));
140: 	}
141: 
142: 	if (types_filter.empty()) {
143: 		throw InvalidInputException("List of types can not be empty!");
144: 	}
145: 
146: 	string projection = "";
147: 	for (idx_t i = 0; i < types.size(); i++) {
148: 		auto &type = types[i];
149: 		// Check if any of the types in the filter match the current type
150: 		if (std::find_if(types_filter.begin(), types_filter.end(),
151: 		                 [&](const LogicalType &filter) { return filter == type; }) != types_filter.end()) {
152: 			if (!projection.empty()) {
153: 				projection += ", ";
154: 			}
155: 			projection += names[i];
156: 		}
157: 	}
158: 	if (projection.empty()) {
159: 		throw InvalidInputException("None of the columns matched the provided type filter!");
160: 	}
161: 	return ProjectFromExpression(projection);
162: }
163: 
164: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::EmptyResult(const shared_ptr<ClientContext> &context,
165:                                                            const vector<LogicalType> &types, vector<string> names) {
166: 	vector<Value> dummy_values;
167: 	D_ASSERT(types.size() == names.size());
168: 	dummy_values.reserve(types.size());
169: 	D_ASSERT(!types.empty());
170: 	for (auto &type : types) {
171: 		dummy_values.emplace_back(type);
172: 	}
173: 	vector<vector<Value>> single_row(1, dummy_values);
174: 	auto values_relation =
175: 	    make_uniq<DuckDBPyRelation>(make_shared_ptr<ValueRelation>(context, single_row, std::move(names)));
176: 	// Add a filter on an impossible condition
177: 	return values_relation->FilterFromExpression("true = false");
178: }
179: 
180: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::SetAlias(const string &expr) {
181: 	return make_uniq<DuckDBPyRelation>(rel->Alias(expr));
182: }
183: 
184: py::str DuckDBPyRelation::GetAlias() {
185: 	return py::str(string(rel->GetAlias()));
186: }
187: 
188: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Filter(const py::object &expr) {
189: 	if (py::isinstance<py::str>(expr)) {
190: 		string expression = py::cast<py::str>(expr);
191: 		return FilterFromExpression(expression);
192: 	}
193: 	shared_ptr<DuckDBPyExpression> expression;
194: 	if (!py::try_cast(expr, expression)) {
195: 		throw InvalidInputException("Please provide either a string or a DuckDBPyExpression object to 'filter'");
196: 	}
197: 	auto expr_p = expression->GetExpression().Copy();
198: 	return make_uniq<DuckDBPyRelation>(rel->Filter(std::move(expr_p)));
199: }
200: 
201: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::FilterFromExpression(const string &expr) {
202: 	return make_uniq<DuckDBPyRelation>(rel->Filter(expr));
203: }
204: 
205: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Limit(int64_t n, int64_t offset) {
206: 	return make_uniq<DuckDBPyRelation>(rel->Limit(n, offset));
207: }
208: 
209: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Order(const string &expr) {
210: 	return make_uniq<DuckDBPyRelation>(rel->Order(expr));
211: }
212: 
213: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Sort(const py::args &args) {
214: 	vector<OrderByNode> order_nodes;
215: 	order_nodes.reserve(args.size());
216: 
217: 	for (auto arg : args) {
218: 		shared_ptr<DuckDBPyExpression> py_expr;
219: 		if (!py::try_cast<shared_ptr<DuckDBPyExpression>>(arg, py_expr)) {
220: 			string actual_type = py::str(arg.get_type());
221: 			throw InvalidInputException("Expected argument of type Expression, received '%s' instead", actual_type);
222: 		}
223: 		auto expr = py_expr->GetExpression().Copy();
224: 		order_nodes.emplace_back(py_expr->order_type, py_expr->null_order, std::move(expr));
225: 	}
226: 	if (order_nodes.empty()) {
227: 		throw InvalidInputException("Please provide at least one expression to sort on");
228: 	}
229: 	return make_uniq<DuckDBPyRelation>(rel->Order(std::move(order_nodes)));
230: }
231: 
232: vector<unique_ptr<ParsedExpression>> GetExpressions(ClientContext &context, const py::object &expr) {
233: 	if (py::is_list_like(expr)) {
234: 		vector<unique_ptr<ParsedExpression>> expressions;
235: 		auto aggregate_list = py::list(expr);
236: 		for (auto &item : aggregate_list) {
237: 			shared_ptr<DuckDBPyExpression> py_expr;
238: 			if (!py::try_cast<shared_ptr<DuckDBPyExpression>>(item, py_expr)) {
239: 				throw InvalidInputException("Please provide arguments of type Expression!");
240: 			}
241: 			auto expr = py_expr->GetExpression().Copy();
242: 			expressions.push_back(std::move(expr));
243: 		}
244: 		return expressions;
245: 	} else if (py::isinstance<py::str>(expr)) {
246: 		auto aggregate_list = std::string(py::str(expr));
247: 		return Parser::ParseExpressionList(aggregate_list, context.GetParserOptions());
248: 	} else {
249: 		string actual_type = py::str(expr.get_type());
250: 		throw InvalidInputException("Please provide either a string or list of Expression objects, not %s",
251: 		                            actual_type);
252: 	}
253: }
254: 
255: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Aggregate(const py::object &expr, const string &groups) {
256: 	AssertRelation();
257: 	auto expressions = GetExpressions(*rel->context.GetContext(), expr);
258: 	if (!groups.empty()) {
259: 		return make_uniq<DuckDBPyRelation>(rel->Aggregate(std::move(expressions), groups));
260: 	}
261: 	return make_uniq<DuckDBPyRelation>(rel->Aggregate(std::move(expressions)));
262: }
263: 
264: void DuckDBPyRelation::AssertResult() const {
265: 	if (!result) {
266: 		throw InvalidInputException("No open result set");
267: 	}
268: }
269: 
270: void DuckDBPyRelation::AssertRelation() const {
271: 	if (!rel) {
272: 		throw InvalidInputException("This relation was created from a result");
273: 	}
274: }
275: 
276: void DuckDBPyRelation::AssertResultOpen() const {
277: 	if (!result || result->IsClosed()) {
278: 		throw InvalidInputException("No open result set");
279: 	}
280: }
281: 
282: py::list DuckDBPyRelation::Description() {
283: 	return DuckDBPyResult::GetDescription(names, types);
284: }
285: 
286: Relation &DuckDBPyRelation::GetRel() {
287: 	if (!rel) {
288: 		throw InternalException("DuckDBPyRelation - calling GetRel, but no rel was present");
289: 	}
290: 	return *rel;
291: }
292: 
293: struct DescribeAggregateInfo {
294: 	explicit DescribeAggregateInfo(string name_p, bool numeric_only = false)
295: 	    : name(std::move(name_p)), numeric_only(numeric_only) {
296: 	}
297: 
298: 	string name;
299: 	bool numeric_only;
300: };
301: 
302: vector<string> CreateExpressionList(const vector<ColumnDefinition> &columns,
303:                                     const vector<DescribeAggregateInfo> &aggregates) {
304: 	vector<string> expressions;
305: 	expressions.reserve(columns.size());
306: 
307: 	string aggr_names = "UNNEST([";
308: 	for (idx_t i = 0; i < aggregates.size(); i++) {
309: 		if (i > 0) {
310: 			aggr_names += ", ";
311: 		}
312: 		aggr_names += "'";
313: 		aggr_names += aggregates[i].name;
314: 		aggr_names += "'";
315: 	}
316: 	aggr_names += "])";
317: 	aggr_names += " AS aggr";
318: 	expressions.push_back(aggr_names);
319: 	for (idx_t c = 0; c < columns.size(); c++) {
320: 		auto &col = columns[c];
321: 		string expr = "UNNEST([";
322: 		for (idx_t i = 0; i < aggregates.size(); i++) {
323: 			if (i > 0) {
324: 				expr += ", ";
325: 			}
326: 			if (aggregates[i].numeric_only && !col.GetType().IsNumeric()) {
327: 				expr += "NULL";
328: 				continue;
329: 			}
330: 			expr += aggregates[i].name;
331: 			expr += "(";
332: 			expr += KeywordHelper::WriteOptionallyQuoted(col.GetName());
333: 			expr += ")";
334: 			if (col.GetType().IsNumeric()) {
335: 				expr += "::DOUBLE";
336: 			} else {
337: 				expr += "::VARCHAR";
338: 			}
339: 		}
340: 		expr += "])";
341: 		expr += " AS " + KeywordHelper::WriteOptionallyQuoted(col.GetName());
342: 		expressions.push_back(expr);
343: 	}
344: 	return expressions;
345: }
346: 
347: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Describe() {
348: 	auto &columns = rel->Columns();
349: 	vector<DescribeAggregateInfo> aggregates;
350: 	aggregates = {DescribeAggregateInfo("count"),        DescribeAggregateInfo("mean", true),
351: 	              DescribeAggregateInfo("stddev", true), DescribeAggregateInfo("min"),
352: 	              DescribeAggregateInfo("max"),          DescribeAggregateInfo("median", true)};
353: 	auto expressions = CreateExpressionList(columns, aggregates);
354: 	return make_uniq<DuckDBPyRelation>(rel->Aggregate(expressions));
355: }
356: 
357: string DuckDBPyRelation::ToSQL() {
358: 	if (!rel) {
359: 		// This relation is just a wrapper around a result set, can't figure out what the SQL was
360: 		return "";
361: 	}
362: 	try {
363: 		return rel->GetQueryNode()->ToString();
364: 	} catch (const std::exception &) {
365: 		return "";
366: 	}
367: }
368: 
369: string DuckDBPyRelation::GenerateExpressionList(const string &function_name, const string &aggregated_columns,
370:                                                 const string &groups, const string &function_parameter,
371:                                                 bool ignore_nulls, const string &projected_columns,
372:                                                 const string &window_spec) {
373: 	auto input = StringUtil::Split(aggregated_columns, ',');
374: 	return GenerateExpressionList(function_name, std::move(input), groups, function_parameter, ignore_nulls,
375: 	                              projected_columns, window_spec);
376: }
377: 
378: string DuckDBPyRelation::GenerateExpressionList(const string &function_name, vector<string> input, const string &groups,
379:                                                 const string &function_parameter, bool ignore_nulls,
380:                                                 const string &projected_columns, const string &window_spec) {
381: 	string expr;
382: 
383: 	if (StringUtil::CIEquals("count", function_name) && input.empty()) {
384: 		// Insert an artificial '*'
385: 		input.push_back("*");
386: 	}
387: 
388: 	if (!projected_columns.empty()) {
389: 		expr = projected_columns + ", ";
390: 	}
391: 
392: 	if (input.empty() && !function_parameter.empty()) {
393: 		return expr +=
394: 		       function_name + "(" + function_parameter + ((ignore_nulls) ? " ignore nulls) " : ") ") + window_spec;
395: 	}
396: 	for (idx_t i = 0; i < input.size(); i++) {
397: 		if (function_parameter.empty()) {
398: 			expr += function_name + "(" + input[i] + ((ignore_nulls) ? " ignore nulls) " : ") ") + window_spec;
399: 		} else {
400: 			expr += function_name + "(" + input[i] + "," + function_parameter +
401: 			        ((ignore_nulls) ? " ignore nulls) " : ") ") + window_spec;
402: 		}
403: 
404: 		if (i < input.size() - 1) {
405: 			expr += ",";
406: 		}
407: 	}
408: 	return expr;
409: }
410: 
411: /* General aggregate functions */
412: 
413: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::GenericAggregator(const string &function_name,
414:                                                                  const string &aggregated_columns, const string &groups,
415:                                                                  const string &function_parameter,
416:                                                                  const string &projected_columns) {
417: 
418: 	//! Construct Aggregation Expression
419: 	auto expr = GenerateExpressionList(function_name, aggregated_columns, groups, function_parameter, false,
420: 	                                   projected_columns, "");
421: 	return Aggregate(py::str(expr), groups);
422: }
423: 
424: unique_ptr<DuckDBPyRelation>
425: DuckDBPyRelation::GenericWindowFunction(const string &function_name, const string &function_parameters,
426:                                         const string &aggr_columns, const string &window_spec, const bool &ignore_nulls,
427:                                         const string &projected_columns) {
428: 	auto expr = GenerateExpressionList(function_name, aggr_columns, "", function_parameters, ignore_nulls,
429: 	                                   projected_columns, window_spec);
430: 	return make_uniq<DuckDBPyRelation>(rel->Project(expr));
431: }
432: 
433: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::ApplyAggOrWin(const string &function_name, const string &agg_columns,
434:                                                              const string &function_parameters, const string &groups,
435:                                                              const string &window_spec, const string &projected_columns,
436:                                                              bool ignore_nulls) {
437: 	if (!groups.empty() && !window_spec.empty()) {
438: 		throw InvalidInputException("Either groups or window must be set (can't be both at the same time)");
439: 	}
440: 	if (!window_spec.empty()) {
441: 		return GenericWindowFunction(function_name, function_parameters, agg_columns, window_spec, ignore_nulls,
442: 		                             projected_columns);
443: 	} else {
444: 		return GenericAggregator(function_name, agg_columns, groups, function_parameters, projected_columns);
445: 	}
446: }
447: 
448: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::AnyValue(const std::string &column, const std::string &groups,
449:                                                         const std::string &window_spec,
450:                                                         const std::string &projected_columns) {
451: 	return ApplyAggOrWin("any_value", column, "", groups, window_spec, projected_columns);
452: }
453: 
454: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::ArgMax(const std::string &arg_column, const std::string &value_column,
455:                                                       const std::string &groups, const std::string &window_spec,
456:                                                       const std::string &projected_columns) {
457: 	return ApplyAggOrWin("arg_max", arg_column, value_column, groups, window_spec, projected_columns);
458: }
459: 
460: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::ArgMin(const std::string &arg_column, const std::string &value_column,
461:                                                       const std::string &groups, const std::string &window_spec,
462:                                                       const std::string &projected_columns) {
463: 	return ApplyAggOrWin("arg_min", arg_column, value_column, groups, window_spec, projected_columns);
464: }
465: 
466: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Avg(const std::string &column, const std::string &groups,
467:                                                    const std::string &window_spec,
468:                                                    const std::string &projected_columns) {
469: 	return ApplyAggOrWin("avg", column, "", groups, window_spec, projected_columns);
470: }
471: 
472: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::BitAnd(const std::string &column, const std::string &groups,
473:                                                       const std::string &window_spec,
474:                                                       const std::string &projected_columns) {
475: 	return ApplyAggOrWin("bit_and", column, "", groups, window_spec, projected_columns);
476: }
477: 
478: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::BitOr(const std::string &column, const std::string &groups,
479:                                                      const std::string &window_spec,
480:                                                      const std::string &projected_columns) {
481: 	return ApplyAggOrWin("bit_or", column, "", groups, window_spec, projected_columns);
482: }
483: 
484: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::BitXor(const std::string &column, const std::string &groups,
485:                                                       const std::string &window_spec,
486:                                                       const std::string &projected_columns) {
487: 	return ApplyAggOrWin("bit_xor", column, "", groups, window_spec, projected_columns);
488: }
489: 
490: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::BitStringAgg(const std::string &column, const Optional<py::object> &min,
491:                                                             const Optional<py::object> &max, const std::string &groups,
492:                                                             const std::string &window_spec,
493:                                                             const std::string &projected_columns) {
494: 	if ((min.is_none() && !max.is_none()) || (!min.is_none() && max.is_none())) {
495: 		throw InvalidInputException("Both min and max values must be set");
496: 	}
497: 	if (!min.is_none()) {
498: 		if (!py::isinstance<py::int_>(min) || !py::isinstance<py::int_>(max)) {
499: 			throw InvalidTypeException("min and max must be of type int");
500: 		}
501: 	}
502: 	auto bitstring_agg_params =
503: 	    min.is_none() ? "" : (std::to_string(min.cast<int>()) + "," + std::to_string(max.cast<int>()));
504: 	return ApplyAggOrWin("bitstring_agg", column, bitstring_agg_params, groups, window_spec, projected_columns);
505: }
506: 
507: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::BoolAnd(const std::string &column, const std::string &groups,
508:                                                        const std::string &window_spec,
509:                                                        const std::string &projected_columns) {
510: 	return ApplyAggOrWin("bool_and", column, "", groups, window_spec, projected_columns);
511: }
512: 
513: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::BoolOr(const std::string &column, const std::string &groups,
514:                                                       const std::string &window_spec,
515:                                                       const std::string &projected_columns) {
516: 	return ApplyAggOrWin("bool_or", column, "", groups, window_spec, projected_columns);
517: }
518: 
519: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::ValueCounts(const std::string &column, const std::string &groups) {
520: 	return Count(column, groups, "", column);
521: }
522: 
523: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Count(const std::string &column, const std::string &groups,
524:                                                      const std::string &window_spec,
525:                                                      const std::string &projected_columns) {
526: 	return ApplyAggOrWin("count", column, "", groups, window_spec, projected_columns);
527: }
528: 
529: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::FAvg(const std::string &column, const std::string &groups,
530:                                                     const std::string &window_spec,
531:                                                     const std::string &projected_columns) {
532: 	return ApplyAggOrWin("favg", column, "", groups, window_spec, projected_columns);
533: }
534: 
535: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::First(const string &column, const std::string &groups,
536:                                                      const string &projected_columns) {
537: 	return GenericAggregator("first", column, groups, "", projected_columns);
538: }
539: 
540: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::FSum(const std::string &column, const std::string &groups,
541:                                                     const std::string &window_spec,
542:                                                     const std::string &projected_columns) {
543: 	return ApplyAggOrWin("fsum", column, "", groups, window_spec, projected_columns);
544: }
545: 
546: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::GeoMean(const std::string &column, const std::string &groups,
547:                                                        const std::string &projected_columns) {
548: 	return GenericAggregator("geomean", column, groups, "", projected_columns);
549: }
550: 
551: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Histogram(const std::string &column, const std::string &groups,
552:                                                          const std::string &window_spec,
553:                                                          const std::string &projected_columns) {
554: 	return ApplyAggOrWin("histogram", column, "", groups, window_spec, projected_columns);
555: }
556: 
557: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::List(const std::string &column, const std::string &groups,
558:                                                     const std::string &window_spec,
559:                                                     const std::string &projected_columns) {
560: 	return ApplyAggOrWin("list", column, "", groups, window_spec, projected_columns);
561: }
562: 
563: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Last(const std::string &column, const std::string &groups,
564:                                                     const std::string &projected_columns) {
565: 	return GenericAggregator("last", column, groups, "", projected_columns);
566: }
567: 
568: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Max(const std::string &column, const std::string &groups,
569:                                                    const std::string &window_spec,
570:                                                    const std::string &projected_columns) {
571: 	return ApplyAggOrWin("max", column, "", groups, window_spec, projected_columns);
572: }
573: 
574: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Min(const std::string &column, const std::string &groups,
575:                                                    const std::string &window_spec,
576:                                                    const std::string &projected_columns) {
577: 	return ApplyAggOrWin("min", column, "", groups, window_spec, projected_columns);
578: }
579: 
580: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Product(const std::string &column, const std::string &groups,
581:                                                        const std::string &window_spec,
582:                                                        const std::string &projected_columns) {
583: 	return ApplyAggOrWin("product", column, "", groups, window_spec, projected_columns);
584: }
585: 
586: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::StringAgg(const std::string &column, const std::string &sep,
587:                                                          const std::string &groups, const std::string &window_spec,
588:                                                          const std::string &projected_columns) {
589: 	auto string_agg_params = "\'" + sep + "\'";
590: 	return ApplyAggOrWin("string_agg", column, string_agg_params, groups, window_spec, projected_columns);
591: }
592: 
593: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Sum(const std::string &column, const std::string &groups,
594:                                                    const std::string &window_spec,
595:                                                    const std::string &projected_columns) {
596: 	return ApplyAggOrWin("sum", column, "", groups, window_spec, projected_columns);
597: }
598: 
599: /* TODO: Approximate aggregate functions */
600: 
601: /* TODO: Statistical aggregate functions */
602: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Median(const std::string &column, const std::string &groups,
603:                                                       const std::string &window_spec,
604:                                                       const std::string &projected_columns) {
605: 	return ApplyAggOrWin("median", column, "", groups, window_spec, projected_columns);
606: }
607: 
608: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Mode(const std::string &column, const std::string &groups,
609:                                                     const std::string &window_spec,
610:                                                     const std::string &projected_columns) {
611: 	return ApplyAggOrWin("mode", column, "", groups, window_spec, projected_columns);
612: }
613: 
614: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::QuantileCont(const std::string &column, const py::object &q,
615:                                                             const std::string &groups, const std::string &window_spec,
616:                                                             const std::string &projected_columns) {
617: 	string quantile_params = "";
618: 	if (py::isinstance<py::float_>(q)) {
619: 		quantile_params = std::to_string(q.cast<float>());
620: 	} else if (py::isinstance<py::list>(q)) {
621: 		auto aux = q.cast<std::vector<double>>();
622: 		quantile_params += "[";
623: 		for (idx_t i = 0; i < aux.size(); i++) {
624: 			quantile_params += std::to_string(aux[i]);
625: 			if (i < aux.size() - 1) {
626: 				quantile_params += ",";
627: 			}
628: 		}
629: 		quantile_params += "]";
630: 	} else {
631: 		throw InvalidTypeException("Unsupported type for quantile");
632: 	}
633: 	return ApplyAggOrWin("quantile_cont", column, quantile_params, groups, window_spec, projected_columns);
634: }
635: 
636: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::QuantileDisc(const std::string &column, const py::object &q,
637:                                                             const std::string &groups, const std::string &window_spec,
638:                                                             const std::string &projected_columns) {
639: 	string quantile_params = "";
640: 	if (py::isinstance<py::float_>(q)) {
641: 		quantile_params = std::to_string(q.cast<float>());
642: 	} else if (py::isinstance<py::list>(q)) {
643: 		auto aux = q.cast<std::vector<double>>();
644: 		quantile_params += "[";
645: 		for (idx_t i = 0; i < aux.size(); i++) {
646: 			quantile_params += std::to_string(aux[i]);
647: 			if (i < aux.size() - 1) {
648: 				quantile_params += ",";
649: 			}
650: 		}
651: 		quantile_params += "]";
652: 	} else {
653: 		throw InvalidTypeException("Unsupported type for quantile");
654: 	}
655: 	return ApplyAggOrWin("quantile_disc", column, quantile_params, groups, window_spec, projected_columns);
656: }
657: 
658: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::StdPop(const std::string &column, const std::string &groups,
659:                                                       const std::string &window_spec,
660:                                                       const std::string &projected_columns) {
661: 	return ApplyAggOrWin("stddev_pop", column, "", groups, window_spec, projected_columns);
662: }
663: 
664: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::StdSamp(const std::string &column, const std::string &groups,
665:                                                        const std::string &window_spec,
666:                                                        const std::string &projected_columns) {
667: 	return ApplyAggOrWin("stddev_samp", column, "", groups, window_spec, projected_columns);
668: }
669: 
670: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::VarPop(const std::string &column, const std::string &groups,
671:                                                       const std::string &window_spec,
672:                                                       const std::string &projected_columns) {
673: 	return ApplyAggOrWin("var_pop", column, "", groups, window_spec, projected_columns);
674: }
675: 
676: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::VarSamp(const std::string &column, const std::string &groups,
677:                                                        const std::string &window_spec,
678:                                                        const std::string &projected_columns) {
679: 	return ApplyAggOrWin("var_samp", column, "", groups, window_spec, projected_columns);
680: }
681: 
682: idx_t DuckDBPyRelation::Length() {
683: 	auto aggregate_rel = GenericAggregator("count", "*");
684: 	aggregate_rel->Execute();
685: 	D_ASSERT(aggregate_rel->result);
686: 	auto tmp_res = std::move(aggregate_rel->result);
687: 	return tmp_res->FetchChunk()->GetValue(0, 0).GetValue<idx_t>();
688: }
689: 
690: py::tuple DuckDBPyRelation::Shape() {
691: 	auto length = Length();
692: 	return py::make_tuple(length, rel->Columns().size());
693: }
694: 
695: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Unique(const string &std_columns) {
696: 	return make_uniq<DuckDBPyRelation>(rel->Project(std_columns)->Distinct());
697: }
698: 
699: /* General-purpose window functions */
700: 
701: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::RowNumber(const string &window_spec, const string &projected_columns) {
702: 	return GenericWindowFunction("row_number", "", "*", window_spec, false, projected_columns);
703: }
704: 
705: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Rank(const string &window_spec, const string &projected_columns) {
706: 	return GenericWindowFunction("rank", "", "*", window_spec, false, projected_columns);
707: }
708: 
709: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::DenseRank(const string &window_spec, const string &projected_columns) {
710: 	return GenericWindowFunction("dense_rank", "", "*", window_spec, false, projected_columns);
711: }
712: 
713: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::PercentRank(const string &window_spec, const string &projected_columns) {
714: 	return GenericWindowFunction("percent_rank", "", "*", window_spec, false, projected_columns);
715: }
716: 
717: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::CumeDist(const string &window_spec, const string &projected_columns) {
718: 	return GenericWindowFunction("cume_dist", "", "*", window_spec, false, projected_columns);
719: }
720: 
721: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::FirstValue(const string &column, const string &window_spec,
722:                                                           const string &projected_columns) {
723: 	return GenericWindowFunction("first_value", "", column, window_spec, false, projected_columns);
724: }
725: 
726: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::NTile(const string &window_spec, const int &num_buckets,
727:                                                      const string &projected_columns) {
728: 	return GenericWindowFunction("ntile", std::to_string(num_buckets), "", window_spec, false, projected_columns);
729: }
730: 
731: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Lag(const string &column, const string &window_spec, const int &offset,
732:                                                    const string &default_value, const bool &ignore_nulls,
733:                                                    const string &projected_columns) {
734: 	string lag_params = "";
735: 	if (offset != 0) {
736: 		lag_params += std::to_string(offset);
737: 	}
738: 	if (!default_value.empty()) {
739: 		lag_params += "," + default_value;
740: 	}
741: 	return GenericWindowFunction("lag", lag_params, column, window_spec, ignore_nulls, projected_columns);
742: }
743: 
744: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::LastValue(const std::string &column, const std::string &window_spec,
745:                                                          const std::string &projected_columns) {
746: 	return GenericWindowFunction("last_value", "", column, window_spec, false, projected_columns);
747: }
748: 
749: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Lead(const string &column, const string &window_spec, const int &offset,
750:                                                     const string &default_value, const bool &ignore_nulls,
751:                                                     const string &projected_columns) {
752: 	string lead_params = "";
753: 	if (offset != 0) {
754: 		lead_params += std::to_string(offset);
755: 	}
756: 	if (!default_value.empty()) {
757: 		lead_params += "," + default_value;
758: 	}
759: 	return GenericWindowFunction("lead", lead_params, column, window_spec, ignore_nulls, projected_columns);
760: }
761: 
762: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::NthValue(const string &column, const string &window_spec,
763:                                                         const int &offset, const bool &ignore_nulls,
764:                                                         const string &projected_columns) {
765: 	return GenericWindowFunction("nth_value", std::to_string(offset), column, window_spec, ignore_nulls,
766: 	                             projected_columns);
767: }
768: 
769: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Distinct() {
770: 	return make_uniq<DuckDBPyRelation>(rel->Distinct());
771: }
772: 
773: duckdb::pyarrow::RecordBatchReader DuckDBPyRelation::FetchRecordBatchReader(idx_t rows_per_batch) {
774: 	AssertResult();
775: 	return result->FetchRecordBatchReader(rows_per_batch);
776: }
777: 
778: static unique_ptr<QueryResult> PyExecuteRelation(const shared_ptr<Relation> &rel, bool stream_result = false) {
779: 	if (!rel) {
780: 		return nullptr;
781: 	}
782: 	auto context = rel->context.GetContext();
783: 	py::gil_scoped_release release;
784: 	auto pending_query = context->PendingQuery(rel, stream_result);
785: 	return DuckDBPyConnection::CompletePendingQuery(*pending_query);
786: }
787: 
788: unique_ptr<QueryResult> DuckDBPyRelation::ExecuteInternal(bool stream_result) {
789: 	this->executed = true;
790: 	return PyExecuteRelation(rel, stream_result);
791: }
792: 
793: void DuckDBPyRelation::ExecuteOrThrow(bool stream_result) {
794: 	result.reset();
795: 	auto query_result = ExecuteInternal(stream_result);
796: 	if (!query_result) {
797: 		throw InternalException("ExecuteOrThrow - no query available to execute");
798: 	}
799: 	if (query_result->HasError()) {
800: 		query_result->ThrowError();
801: 	}
802: 	result = make_uniq<DuckDBPyResult>(std::move(query_result));
803: }
804: 
805: PandasDataFrame DuckDBPyRelation::FetchDF(bool date_as_object) {
806: 	if (!result) {
807: 		if (!rel) {
808: 			return py::none();
809: 		}
810: 		ExecuteOrThrow();
811: 	}
812: 	if (result->IsClosed()) {
813: 		return py::none();
814: 	}
815: 	auto df = result->FetchDF(date_as_object);
816: 	result = nullptr;
817: 	return df;
818: }
819: 
820: Optional<py::tuple> DuckDBPyRelation::FetchOne() {
821: 	if (!result) {
822: 		if (!rel) {
823: 			return py::none();
824: 		}
825: 		ExecuteOrThrow(true);
826: 	}
827: 	if (result->IsClosed()) {
828: 		return py::none();
829: 	}
830: 	return result->Fetchone();
831: }
832: 
833: py::list DuckDBPyRelation::FetchMany(idx_t size) {
834: 	if (!result) {
835: 		if (!rel) {
836: 			return py::list();
837: 		}
838: 		ExecuteOrThrow(true);
839: 		D_ASSERT(result);
840: 	}
841: 	if (result->IsClosed()) {
842: 		return py::list();
843: 	}
844: 	return result->Fetchmany(size);
845: }
846: 
847: py::list DuckDBPyRelation::FetchAll() {
848: 	if (!result) {
849: 		if (!rel) {
850: 			return py::list();
851: 		}
852: 		ExecuteOrThrow();
853: 	}
854: 	if (result->IsClosed()) {
855: 		return py::list();
856: 	}
857: 	auto res = result->Fetchall();
858: 	result = nullptr;
859: 	return res;
860: }
861: 
862: py::dict DuckDBPyRelation::FetchNumpy() {
863: 	if (!result) {
864: 		if (!rel) {
865: 			return py::none();
866: 		}
867: 		ExecuteOrThrow();
868: 	}
869: 	if (result->IsClosed()) {
870: 		return py::none();
871: 	}
872: 	auto res = result->FetchNumpy();
873: 	result = nullptr;
874: 	return res;
875: }
876: 
877: py::dict DuckDBPyRelation::FetchPyTorch() {
878: 	if (!result) {
879: 		if (!rel) {
880: 			return py::none();
881: 		}
882: 		ExecuteOrThrow();
883: 	}
884: 	if (result->IsClosed()) {
885: 		return py::none();
886: 	}
887: 	auto res = result->FetchPyTorch();
888: 	result = nullptr;
889: 	return res;
890: }
891: 
892: py::dict DuckDBPyRelation::FetchTF() {
893: 	if (!result) {
894: 		if (!rel) {
895: 			return py::none();
896: 		}
897: 		ExecuteOrThrow();
898: 	}
899: 	if (result->IsClosed()) {
900: 		return py::none();
901: 	}
902: 	auto res = result->FetchTF();
903: 	result = nullptr;
904: 	return res;
905: }
906: 
907: py::dict DuckDBPyRelation::FetchNumpyInternal(bool stream, idx_t vectors_per_chunk) {
908: 	if (!result) {
909: 		if (!rel) {
910: 			return py::none();
911: 		}
912: 		ExecuteOrThrow();
913: 	}
914: 	AssertResultOpen();
915: 	auto res = result->FetchNumpyInternal(stream, vectors_per_chunk);
916: 	result = nullptr;
917: 	return res;
918: }
919: 
920: //! Should this also keep track of when the result is empty and set result->result_closed accordingly?
921: PandasDataFrame DuckDBPyRelation::FetchDFChunk(idx_t vectors_per_chunk, bool date_as_object) {
922: 	if (!result) {
923: 		if (!rel) {
924: 			return py::none();
925: 		}
926: 		ExecuteOrThrow(true);
927: 	}
928: 	AssertResultOpen();
929: 	return result->FetchDFChunk(vectors_per_chunk, date_as_object);
930: }
931: 
932: duckdb::pyarrow::Table DuckDBPyRelation::ToArrowTableInternal(idx_t batch_size, bool to_polars) {
933: 	if (!result) {
934: 		if (!rel) {
935: 			return py::none();
936: 		}
937: 		ExecuteOrThrow();
938: 	}
939: 	AssertResultOpen();
940: 	auto res = result->FetchArrowTable(batch_size, to_polars);
941: 	result = nullptr;
942: 	return res;
943: }
944: 
945: duckdb::pyarrow::Table DuckDBPyRelation::ToArrowTable(idx_t batch_size) {
946: 	return ToArrowTableInternal(batch_size, false);
947: }
948: 
949: py::object DuckDBPyRelation::ToArrowCapsule(const py::object &requested_schema) {
950: 	if (!result) {
951: 		if (!rel) {
952: 			return py::none();
953: 		}
954: 		ExecuteOrThrow();
955: 	}
956: 	AssertResultOpen();
957: 	return result->FetchArrowCapsule();
958: }
959: 
960: PolarsDataFrame DuckDBPyRelation::ToPolars(idx_t batch_size) {
961: 	auto arrow = ToArrowTableInternal(batch_size, true);
962: 	return py::cast<PolarsDataFrame>(pybind11::module_::import("polars").attr("DataFrame")(arrow));
963: }
964: 
965: duckdb::pyarrow::RecordBatchReader DuckDBPyRelation::ToRecordBatch(idx_t batch_size) {
966: 	if (!result) {
967: 		if (!rel) {
968: 			return py::none();
969: 		}
970: 		ExecuteOrThrow(true);
971: 	}
972: 	AssertResultOpen();
973: 	return result->FetchRecordBatchReader(batch_size);
974: }
975: 
976: void DuckDBPyRelation::Close() {
977: 	// We always want to execute the query at least once, for side-effect purposes.
978: 	// if it has already been executed, we don't need to do it again.
979: 	if (!executed && !result) {
980: 		if (!rel) {
981: 			return;
982: 		}
983: 		ExecuteOrThrow();
984: 	}
985: 	if (result) {
986: 		result->Close();
987: 	}
988: }
989: 
990: bool DuckDBPyRelation::ContainsColumnByName(const string &name) const {
991: 	return std::find_if(names.begin(), names.end(),
992: 	                    [&](const string &item) { return StringUtil::CIEquals(name, item); }) != names.end();
993: }
994: 
995: static bool ContainsStructFieldByName(LogicalType &type, const string &name) {
996: 	if (type.id() != LogicalTypeId::STRUCT) {
997: 		return false;
998: 	}
999: 	auto count = StructType::GetChildCount(type);
1000: 	for (idx_t i = 0; i < count; i++) {
1001: 		auto &field_name = StructType::GetChildName(type, i);
1002: 		if (StringUtil::CIEquals(name, field_name)) {
1003: 			return true;
1004: 		}
1005: 	}
1006: 	return false;
1007: }
1008: 
1009: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::GetAttribute(const string &name) {
1010: 	// TODO: support fetching a result containing only column 'name' from a value_relation
1011: 	if (!rel) {
1012: 		throw py::attribute_error(
1013: 		    StringUtil::Format("This relation does not contain a column by the name of '%s'", name));
1014: 	}
1015: 	if (names.size() == 1 && ContainsStructFieldByName(types[0], name)) {
1016: 		return make_uniq<DuckDBPyRelation>(rel->Project({StringUtil::Format("%s.%s", names[0], name)}));
1017: 	}
1018: 	if (ContainsColumnByName(name)) {
1019: 		return make_uniq<DuckDBPyRelation>(rel->Project({StringUtil::Format("\"%s\"", name)}));
1020: 	}
1021: 	throw py::attribute_error(StringUtil::Format("This relation does not contain a column by the name of '%s'", name));
1022: }
1023: 
1024: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Union(DuckDBPyRelation *other) {
1025: 	return make_uniq<DuckDBPyRelation>(rel->Union(other->rel));
1026: }
1027: 
1028: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Except(DuckDBPyRelation *other) {
1029: 	return make_uniq<DuckDBPyRelation>(rel->Except(other->rel));
1030: }
1031: 
1032: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Intersect(DuckDBPyRelation *other) {
1033: 	return make_uniq<DuckDBPyRelation>(rel->Intersect(other->rel));
1034: }
1035: 
1036: namespace {
1037: struct SupportedPythonJoinType {
1038: 	string name;
1039: 	JoinType type;
1040: };
1041: } // namespace
1042: 
1043: static const SupportedPythonJoinType *GetSupportedJoinTypes(idx_t &length) {
1044: 	static const SupportedPythonJoinType SUPPORTED_TYPES[] = {{"left", JoinType::LEFT},   {"right", JoinType::RIGHT},
1045: 	                                                          {"outer", JoinType::OUTER}, {"semi", JoinType::SEMI},
1046: 	                                                          {"inner", JoinType::INNER}, {"anti", JoinType::ANTI}};
1047: 	static const auto SUPPORTED_TYPES_COUNT = sizeof(SUPPORTED_TYPES) / sizeof(SupportedPythonJoinType);
1048: 	length = SUPPORTED_TYPES_COUNT;
1049: 	return reinterpret_cast<const SupportedPythonJoinType *>(SUPPORTED_TYPES);
1050: }
1051: 
1052: static JoinType ParseJoinType(const string &type) {
1053: 	idx_t supported_types_count;
1054: 	auto supported_types = GetSupportedJoinTypes(supported_types_count);
1055: 	for (idx_t i = 0; i < supported_types_count; i++) {
1056: 		auto &supported_type = supported_types[i];
1057: 		if (supported_type.name == type) {
1058: 			return supported_type.type;
1059: 		}
1060: 	}
1061: 	return JoinType::INVALID;
1062: }
1063: 
1064: [[noreturn]] void ThrowUnsupportedJoinTypeError(const string &provided) {
1065: 	vector<string> supported_options;
1066: 	idx_t length;
1067: 	auto supported_types = GetSupportedJoinTypes(length);
1068: 	for (idx_t i = 0; i < length; i++) {
1069: 		supported_options.push_back(StringUtil::Format("'%s'", supported_types[i].name));
1070: 	}
1071: 	auto options = StringUtil::Join(supported_options, ", ");
1072: 	throw InvalidInputException("Unsupported join type %s, try one of: %s", provided, options);
1073: }
1074: 
1075: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Join(DuckDBPyRelation *other, const py::object &condition,
1076:                                                     const string &type) {
1077: 
1078: 	JoinType dtype;
1079: 	string type_string = StringUtil::Lower(type);
1080: 	StringUtil::Trim(type_string);
1081: 
1082: 	dtype = ParseJoinType(type_string);
1083: 	if (dtype == JoinType::INVALID) {
1084: 		ThrowUnsupportedJoinTypeError(type);
1085: 	}
1086: 	auto alias = GetAlias();
1087: 	auto other_alias = other->GetAlias();
1088: 	if (StringUtil::CIEquals(alias, other_alias)) {
1089: 		throw InvalidInputException("Both relations have the same alias, please change the alias of one or both "
1090: 		                            "relations using 'rel = rel.set_alias(<new alias>)'");
1091: 	}
1092: 	if (py::isinstance<py::str>(condition)) {
1093: 		auto condition_string = std::string(py::cast<py::str>(condition));
1094: 		return make_uniq<DuckDBPyRelation>(rel->Join(other->rel, condition_string, dtype));
1095: 	}
1096: 	shared_ptr<DuckDBPyExpression> condition_expr;
1097: 	if (!py::try_cast(condition, condition_expr)) {
1098: 		throw InvalidInputException(
1099: 		    "Please provide condition as an expression either in string form or as an Expression object");
1100: 	}
1101: 	vector<unique_ptr<ParsedExpression>> conditions;
1102: 	conditions.push_back(condition_expr->GetExpression().Copy());
1103: 	return make_uniq<DuckDBPyRelation>(rel->Join(other->rel, std::move(conditions), dtype));
1104: }
1105: 
1106: static Value NestedDictToStruct(const py::object &dictionary) {
1107: 	if (!py::isinstance<py::dict>(dictionary)) {
1108: 		throw InvalidInputException("NestedDictToStruct only accepts a dictionary as input");
1109: 	}
1110: 	py::dict dict_casted = py::dict(dictionary);
1111: 
1112: 	child_list_t<Value> children;
1113: 	for (auto item : dict_casted) {
1114: 		py::object item_key = item.first.cast<py::object>();
1115: 		py::object item_value = item.second.cast<py::object>();
1116: 
1117: 		if (!py::isinstance<py::str>(item_key)) {
1118: 			throw InvalidInputException("NestedDictToStruct only accepts a dictionary with string keys");
1119: 		}
1120: 
1121: 		if (py::isinstance<py::int_>(item_value)) {
1122: 			int32_t item_value_int = py::int_(item_value);
1123: 			children.push_back(std::make_pair(py::str(item_key), Value(item_value_int)));
1124: 		} else if (py::isinstance<py::dict>(item_value)) {
1125: 			children.push_back(std::make_pair(py::str(item_key), NestedDictToStruct(item_value)));
1126: 		} else {
1127: 			throw InvalidInputException(
1128: 			    "NestedDictToStruct only accepts a dictionary with integer values or nested dictionaries");
1129: 		}
1130: 	}
1131: 	return Value::STRUCT(std::move(children));
1132: }
1133: 
1134: void DuckDBPyRelation::ToParquet(const string &filename, const py::object &compression, const py::object &field_ids,
1135:                                  const py::object &row_group_size_bytes, const py::object &row_group_size) {
1136: 	case_insensitive_map_t<vector<Value>> options;
1137: 
1138: 	if (!py::none().is(compression)) {
1139: 		if (!py::isinstance<py::str>(compression)) {
1140: 			throw InvalidInputException("to_parquet only accepts 'compression' as a string");
1141: 		}
1142: 		options["compression"] = {Value(py::str(compression))};
1143: 	}
1144: 
1145: 	if (!py::none().is(field_ids)) {
1146: 		if (py::isinstance<py::dict>(field_ids)) {
1147: 			Value field_ids_value = NestedDictToStruct(field_ids);
1148: 			options["field_ids"] = {field_ids_value};
1149: 		} else if (py::isinstance<py::str>(field_ids)) {
1150: 			options["field_ids"] = {Value(py::str(field_ids))};
1151: 		} else {
1152: 			throw InvalidInputException("to_parquet only accepts 'field_ids' as a dictionary or 'auto'");
1153: 		}
1154: 	}
1155: 
1156: 	if (!py::none().is(row_group_size_bytes)) {
1157: 		if (py::isinstance<py::int_>(row_group_size_bytes)) {
1158: 			int64_t row_group_size_bytes_int = py::int_(row_group_size_bytes);
1159: 			options["row_group_size_bytes"] = {Value(row_group_size_bytes_int)};
1160: 		} else if (py::isinstance<py::str>(row_group_size_bytes)) {
1161: 			options["row_group_size_bytes"] = {Value(py::str(row_group_size_bytes))};
1162: 		} else {
1163: 			throw InvalidInputException(
1164: 			    "to_parquet only accepts 'row_group_size_bytes' as an integer or 'auto' string");
1165: 		}
1166: 	}
1167: 
1168: 	if (!py::none().is(row_group_size)) {
1169: 		if (!py::isinstance<py::int_>(row_group_size)) {
1170: 			throw InvalidInputException("to_parquet only accepts 'row_group_size' as an integer");
1171: 		}
1172: 		int64_t row_group_size_int = py::int_(row_group_size);
1173: 		options["row_group_size"] = {Value(row_group_size_int)};
1174: 	}
1175: 
1176: 	auto write_parquet = rel->WriteParquetRel(filename, std::move(options));
1177: 	PyExecuteRelation(write_parquet);
1178: }
1179: 
1180: void DuckDBPyRelation::ToCSV(const string &filename, const py::object &sep, const py::object &na_rep,
1181:                              const py::object &header, const py::object &quotechar, const py::object &escapechar,
1182:                              const py::object &date_format, const py::object &timestamp_format,
1183:                              const py::object &quoting, const py::object &encoding, const py::object &compression,
1184:                              const py::object &overwrite, const py::object &per_thread_output,
1185:                              const py::object &use_tmp_file, const py::object &partition_by,
1186:                              const py::object &write_partition_columns) {
1187: 	case_insensitive_map_t<vector<Value>> options;
1188: 
1189: 	if (!py::none().is(sep)) {
1190: 		if (!py::isinstance<py::str>(sep)) {
1191: 			throw InvalidInputException("to_csv only accepts 'sep' as a string");
1192: 		}
1193: 		options["delimiter"] = {Value(py::str(sep))};
1194: 	}
1195: 
1196: 	if (!py::none().is(na_rep)) {
1197: 		if (!py::isinstance<py::str>(na_rep)) {
1198: 			throw InvalidInputException("to_csv only accepts 'na_rep' as a string");
1199: 		}
1200: 		options["null"] = {Value(py::str(na_rep))};
1201: 	}
1202: 
1203: 	if (!py::none().is(header)) {
1204: 		if (!py::isinstance<py::bool_>(header)) {
1205: 			throw InvalidInputException("to_csv only accepts 'header' as a boolean");
1206: 		}
1207: 		options["header"] = {Value::BOOLEAN(py::bool_(header))};
1208: 	}
1209: 
1210: 	if (!py::none().is(quotechar)) {
1211: 		if (!py::isinstance<py::str>(quotechar)) {
1212: 			throw InvalidInputException("to_csv only accepts 'quotechar' as a string");
1213: 		}
1214: 		options["quote"] = {Value(py::str(quotechar))};
1215: 	}
1216: 
1217: 	if (!py::none().is(escapechar)) {
1218: 		if (!py::isinstance<py::str>(escapechar)) {
1219: 			throw InvalidInputException("to_csv only accepts 'escapechar' as a string");
1220: 		}
1221: 		options["escape"] = {Value(py::str(escapechar))};
1222: 	}
1223: 
1224: 	if (!py::none().is(date_format)) {
1225: 		if (!py::isinstance<py::str>(date_format)) {
1226: 			throw InvalidInputException("to_csv only accepts 'date_format' as a string");
1227: 		}
1228: 		options["dateformat"] = {Value(py::str(date_format))};
1229: 	}
1230: 
1231: 	if (!py::none().is(timestamp_format)) {
1232: 		if (!py::isinstance<py::str>(timestamp_format)) {
1233: 			throw InvalidInputException("to_csv only accepts 'timestamp_format' as a string");
1234: 		}
1235: 		options["timestampformat"] = {Value(py::str(timestamp_format))};
1236: 	}
1237: 
1238: 	if (!py::none().is(quoting)) {
1239: 		// TODO: add list of strings as valid option
1240: 		if (py::isinstance<py::str>(quoting)) {
1241: 			string quoting_option = StringUtil::Lower(py::str(quoting));
1242: 			if (quoting_option != "force" && quoting_option != "all") {
1243: 				throw InvalidInputException(
1244: 				    "to_csv 'quoting' supported options are ALL or FORCE (both set FORCE_QUOTE=True)");
1245: 			}
1246: 		} else if (py::isinstance<py::int_>(quoting)) {
1247: 			int64_t quoting_value = py::int_(quoting);
1248: 			// csv.QUOTE_ALL expands to 1
1249: 			static constexpr int64_t QUOTE_ALL = 1;
1250: 			if (quoting_value != QUOTE_ALL) {
1251: 				throw InvalidInputException("Only csv.QUOTE_ALL is a supported option for 'quoting' currently");
1252: 			}
1253: 		} else {
1254: 			throw InvalidInputException(
1255: 			    "to_csv only accepts 'quoting' as a string or a constant from the 'csv' package");
1256: 		}
1257: 		options["force_quote"] = {Value("*")};
1258: 	}
1259: 
1260: 	if (!py::none().is(encoding)) {
1261: 		if (!py::isinstance<py::str>(encoding)) {
1262: 			throw InvalidInputException("to_csv only accepts 'encoding' as a string");
1263: 		}
1264: 		string encoding_option = StringUtil::Lower(py::str(encoding));
1265: 		if (encoding_option != "utf-8" && encoding_option != "utf8") {
1266: 			throw InvalidInputException("The only supported encoding option is 'UTF8");
1267: 		}
1268: 	}
1269: 
1270: 	if (!py::none().is(compression)) {
1271: 		if (!py::isinstance<py::str>(compression)) {
1272: 			throw InvalidInputException("to_csv only accepts 'compression' as a string");
1273: 		}
1274: 		options["compression"] = {Value(py::str(compression))};
1275: 	}
1276: 
1277: 	if (!py::none().is(overwrite)) {
1278: 		if (!py::isinstance<py::bool_>(overwrite)) {
1279: 			throw InvalidInputException("to_csv only accepts 'overwrite' as a boolean");
1280: 		}
1281: 		options["overwrite_or_ignore"] = {Value::BOOLEAN(py::bool_(overwrite))};
1282: 	}
1283: 
1284: 	if (!py::none().is(per_thread_output)) {
1285: 		if (!py::isinstance<py::bool_>(per_thread_output)) {
1286: 			throw InvalidInputException("to_csv only accepts 'per_thread_output' as a boolean");
1287: 		}
1288: 		options["per_thread_output"] = {Value::BOOLEAN(py::bool_(per_thread_output))};
1289: 	}
1290: 
1291: 	if (!py::none().is(use_tmp_file)) {
1292: 		if (!py::isinstance<py::bool_>(use_tmp_file)) {
1293: 			throw InvalidInputException("to_csv only accepts 'use_tmp_file' as a boolean");
1294: 		}
1295: 		options["use_tmp_file"] = {Value::BOOLEAN(py::bool_(use_tmp_file))};
1296: 	}
1297: 
1298: 	if (!py::none().is(partition_by)) {
1299: 		if (!py::isinstance<py::list>(partition_by)) {
1300: 			throw InvalidInputException("to_csv only accepts 'partition_by' as a list of strings");
1301: 		}
1302: 		vector<Value> partition_by_values;
1303: 		const py::list &partition_fields = partition_by;
1304: 		for (auto &field : partition_fields) {
1305: 			if (!py::isinstance<py::str>(field)) {
1306: 				throw InvalidInputException("to_csv only accepts 'partition_by' as a list of strings");
1307: 			}
1308: 			partition_by_values.emplace_back(Value(py::str(field)));
1309: 		}
1310: 		options["partition_by"] = {partition_by_values};
1311: 	}
1312: 
1313: 	if (!py::none().is(write_partition_columns)) {
1314: 		if (!py::isinstance<py::bool_>(write_partition_columns)) {
1315: 			throw InvalidInputException("to_csv only accepts 'write_partition_columns' as a boolean");
1316: 		}
1317: 		options["write_partition_columns"] = {Value::BOOLEAN(py::bool_(write_partition_columns))};
1318: 	}
1319: 
1320: 	auto write_csv = rel->WriteCSVRel(filename, std::move(options));
1321: 	PyExecuteRelation(write_csv);
1322: }
1323: 
1324: // should this return a rel with the new view?
1325: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::CreateView(const string &view_name, bool replace) {
1326: 	rel->CreateView(view_name, replace);
1327: 	return make_uniq<DuckDBPyRelation>(rel);
1328: }
1329: 
1330: static bool IsDescribeStatement(SQLStatement &statement) {
1331: 	if (statement.type != StatementType::PRAGMA_STATEMENT) {
1332: 		return false;
1333: 	}
1334: 	auto &pragma_statement = statement.Cast<PragmaStatement>();
1335: 	if (pragma_statement.info->name != "show") {
1336: 		return false;
1337: 	}
1338: 	return true;
1339: }
1340: 
1341: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Query(const string &view_name, const string &sql_query) {
1342: 	auto view_relation = CreateView(view_name);
1343: 	auto all_dependencies = rel->GetAllDependencies();
1344: 
1345: 	Parser parser(rel->context.GetContext()->GetParserOptions());
1346: 	parser.ParseQuery(sql_query);
1347: 	if (parser.statements.size() != 1) {
1348: 		throw InvalidInputException("'DuckDBPyRelation.query' only accepts a single statement");
1349: 	}
1350: 	auto &statement = *parser.statements[0];
1351: 	if (statement.type == StatementType::SELECT_STATEMENT) {
1352: 		auto select_statement = unique_ptr_cast<SQLStatement, SelectStatement>(std::move(parser.statements[0]));
1353: 		auto query_relation = make_shared_ptr<QueryRelation>(rel->context.GetContext(), std::move(select_statement),
1354: 		                                                     sql_query, "query_relation");
1355: 		return make_uniq<DuckDBPyRelation>(std::move(query_relation));
1356: 	} else if (IsDescribeStatement(statement)) {
1357: 		auto query = PragmaShow(view_name);
1358: 		return Query(view_name, query);
1359: 	}
1360: 	{
1361: 		py::gil_scoped_release release;
1362: 		auto query_result = rel->context.GetContext()->Query(std::move(parser.statements[0]), false);
1363: 		// Execute it anyways, for creation/altering statements
1364: 		// We only care that it succeeds, we can't store the result
1365: 		D_ASSERT(query_result);
1366: 		if (query_result->HasError()) {
1367: 			query_result->ThrowError();
1368: 		}
1369: 	}
1370: 	return nullptr;
1371: }
1372: 
1373: DuckDBPyRelation &DuckDBPyRelation::Execute() {
1374: 	AssertRelation();
1375: 	ExecuteOrThrow();
1376: 	return *this;
1377: }
1378: 
1379: void DuckDBPyRelation::InsertInto(const string &table) {
1380: 	AssertRelation();
1381: 	auto parsed_info = QualifiedName::Parse(table);
1382: 	auto insert = rel->InsertRel(parsed_info.schema, parsed_info.name);
1383: 	PyExecuteRelation(insert);
1384: }
1385: 
1386: static bool IsAcceptedInsertRelationType(const Relation &relation) {
1387: 	return relation.type == RelationType::TABLE_RELATION;
1388: }
1389: 
1390: void DuckDBPyRelation::Insert(const py::object &params) {
1391: 	AssertRelation();
1392: 	if (!IsAcceptedInsertRelationType(*this->rel)) {
1393: 		throw InvalidInputException("'DuckDBPyRelation.insert' can only be used on a table relation");
1394: 	}
1395: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(params)};
1396: 
1397: 	py::gil_scoped_release release;
1398: 	rel->Insert(values);
1399: }
1400: 
1401: void DuckDBPyRelation::Create(const string &table) {
1402: 	AssertRelation();
1403: 	auto parsed_info = QualifiedName::Parse(table);
1404: 	auto create = rel->CreateRel(parsed_info.schema, parsed_info.name, false);
1405: 	PyExecuteRelation(create);
1406: }
1407: 
1408: unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Map(py::function fun, Optional<py::object> schema) {
1409: 	AssertRelation();
1410: 	vector<Value> params;
1411: 	params.emplace_back(Value::POINTER(CastPointerToValue(fun.ptr())));
1412: 	params.emplace_back(Value::POINTER(CastPointerToValue(schema.ptr())));
1413: 	auto relation = make_uniq<DuckDBPyRelation>(rel->TableFunction("python_map_function", params));
1414: 	auto rel_dependency = make_uniq<ExternalDependency>();
1415: 	rel_dependency->AddDependency("map", PythonDependencyItem::Create(std::move(fun)));
1416: 	rel_dependency->AddDependency("schema", PythonDependencyItem::Create(std::move(schema)));
1417: 	relation->rel->AddExternalDependency(std::move(rel_dependency));
1418: 	return relation;
1419: }
1420: 
1421: string DuckDBPyRelation::ToStringInternal(const BoxRendererConfig &config, bool invalidate_cache) {
1422: 	AssertRelation();
1423: 	if (rendered_result.empty() || invalidate_cache) {
1424: 		BoxRenderer renderer;
1425: 		auto limit = Limit(config.limit, 0);
1426: 		auto res = limit->ExecuteInternal();
1427: 
1428: 		auto context = rel->context.GetContext();
1429: 		rendered_result = res->ToBox(*context, config);
1430: 	}
1431: 	return rendered_result;
1432: }
1433: 
1434: string DuckDBPyRelation::ToString() {
1435: 	BoxRendererConfig config;
1436: 	config.limit = 10000;
1437: 	if (DuckDBPyConnection::IsJupyter()) {
1438: 		config.max_width = 10000;
1439: 	}
1440: 	return ToStringInternal(config);
1441: }
1442: 
1443: static idx_t IndexFromPyInt(const py::object &object) {
1444: 	auto index = py::cast<idx_t>(object);
1445: 	return index;
1446: }
1447: 
1448: void DuckDBPyRelation::Print(const Optional<py::int_> &max_width, const Optional<py::int_> &max_rows,
1449:                              const Optional<py::int_> &max_col_width, const Optional<py::str> &null_value,
1450:                              const py::object &render_mode) {
1451: 	BoxRendererConfig config;
1452: 	config.limit = 10000;
1453: 	if (DuckDBPyConnection::IsJupyter()) {
1454: 		config.max_width = 10000;
1455: 	}
1456: 
1457: 	bool invalidate_cache = false;
1458: 	if (!py::none().is(max_width)) {
1459: 		invalidate_cache = true;
1460: 		config.max_width = IndexFromPyInt(max_width);
1461: 	}
1462: 	if (!py::none().is(max_rows)) {
1463: 		invalidate_cache = true;
1464: 		config.max_rows = IndexFromPyInt(max_rows);
1465: 	}
1466: 	if (!py::none().is(max_col_width)) {
1467: 		invalidate_cache = true;
1468: 		config.max_col_width = IndexFromPyInt(max_col_width);
1469: 	}
1470: 	if (!py::none().is(null_value)) {
1471: 		invalidate_cache = true;
1472: 		config.null_value = py::cast<std::string>(null_value);
1473: 	}
1474: 	if (!py::none().is(render_mode)) {
1475: 		invalidate_cache = true;
1476: 		if (!py::try_cast(render_mode, config.render_mode)) {
1477: 			throw InvalidInputException("'render_mode' accepts either a string, RenderMode or int value");
1478: 		}
1479: 	}
1480: 
1481: 	py::print(py::str(ToStringInternal(config, invalidate_cache)));
1482: }
1483: 
1484: static ExplainFormat GetExplainFormat(ExplainType type) {
1485: 	if (DuckDBPyConnection::IsJupyter() && type != ExplainType::EXPLAIN_ANALYZE) {
1486: 		return ExplainFormat::HTML;
1487: 	} else {
1488: 		return ExplainFormat::DEFAULT;
1489: 	}
1490: }
1491: 
1492: static void DisplayHTML(const string &html) {
1493: 	py::gil_scoped_acquire gil;
1494: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1495: 	auto html_attr = import_cache.IPython.display.HTML();
1496: 	auto html_object = html_attr(py::str(html));
1497: 	auto display_attr = import_cache.IPython.display.display();
1498: 	display_attr(html_object);
1499: }
1500: 
1501: string DuckDBPyRelation::Explain(ExplainType type) {
1502: 	AssertRelation();
1503: 	py::gil_scoped_release release;
1504: 
1505: 	auto explain_format = GetExplainFormat(type);
1506: 	auto res = rel->Explain(type, explain_format);
1507: 	D_ASSERT(res->type == duckdb::QueryResultType::MATERIALIZED_RESULT);
1508: 	auto &materialized = res->Cast<MaterializedQueryResult>();
1509: 	auto &coll = materialized.Collection();
1510: 	if (explain_format != ExplainFormat::HTML || !DuckDBPyConnection::IsJupyter()) {
1511: 		string result;
1512: 		for (auto &row : coll.Rows()) {
1513: 			// Skip the first column because it just contains 'physical plan'
1514: 			for (idx_t col_idx = 1; col_idx < coll.ColumnCount(); col_idx++) {
1515: 				if (col_idx > 1) {
1516: 					result += "\t";
1517: 				}
1518: 				auto val = row.GetValue(col_idx);
1519: 				result += val.IsNull() ? "NULL" : StringUtil::Replace(val.ToString(), string("\0", 1), "\\0");
1520: 			}
1521: 			result += "\n";
1522: 		}
1523: 		return result;
1524: 	}
1525: 
1526: 	auto chunk = materialized.Fetch();
1527: 	for (idx_t i = 0; i < chunk->size(); i++) {
1528: 		auto plan = chunk->GetValue(1, i);
1529: 		auto plan_string = plan.GetValue<string>();
1530: 		DisplayHTML(plan_string);
1531: 	}
1532: 
1533: 	const string tree_resize_script = R"(
1534: <script>
1535: function toggleDisplay(button) {
1536:     const parentLi = button.closest('li');
1537:     const nestedUl = parentLi.querySelector('ul');
1538:     if (nestedUl) {
1539:         const currentDisplay = getComputedStyle(nestedUl).getPropertyValue('display');
1540:         if (currentDisplay === 'none') {
1541:             nestedUl.classList.toggle('hidden');
1542:             button.textContent = '-';
1543:         } else {
1544:             nestedUl.classList.toggle('hidden');
1545:             button.textContent = '+';
1546:         }
1547:     }
1548: }
1549: 
1550: function updateTreeHeight(tfTree) {
1551: 	if (!tfTree) {
1552: 		return;
1553: 	}
1554: 
1555: 	const closestElement = tfTree.closest('.lm-Widget.jp-OutputArea.jp-Cell-outputArea');
1556: 	if (!closestElement) {
1557: 		return;
1558: 	}
1559: 
1560: 	console.log(closestElement);
1561: 
1562: 	const height = getComputedStyle(closestElement).getPropertyValue('height');
1563: 	tfTree.style.height = height;
1564: }
1565: 
1566: function resizeTFTree() {
1567: 	const tfTrees = document.querySelectorAll('.tf-tree');
1568: 	tfTrees.forEach(tfTree => {
1569: 		console.log(tfTree);
1570: 		if (tfTree) {
1571: 			const jupyterViewPort = tfTree.closest('.lm-Widget.jp-OutputArea.jp-Cell-outputArea');
1572: 			console.log(jupyterViewPort);
1573: 			if (jupyterViewPort) {
1574: 				const resizeObserver = new ResizeObserver(() => {
1575: 					updateTreeHeight(tfTree);
1576: 				});
1577: 				resizeObserver.observe(jupyterViewPort);
1578: 			}
1579: 		}
1580: 	});
1581: }
1582: 
1583: resizeTFTree();
1584: 
1585: </script>
1586: 	)";
1587: 	DisplayHTML(tree_resize_script);
1588: 	return "";
1589: }
1590: 
1591: // TODO: RelationType to a python enum
1592: py::str DuckDBPyRelation::Type() {
1593: 	if (!rel) {
1594: 		return py::str("QUERY_RESULT");
1595: 	}
1596: 	return py::str(RelationTypeToString(rel->type));
1597: }
1598: 
1599: py::list DuckDBPyRelation::Columns() {
1600: 	AssertRelation();
1601: 	py::list res;
1602: 	for (auto &col : rel->Columns()) {
1603: 		res.append(col.Name());
1604: 	}
1605: 	return res;
1606: }
1607: 
1608: py::list DuckDBPyRelation::ColumnTypes() {
1609: 	AssertRelation();
1610: 	py::list res;
1611: 	for (auto &col : rel->Columns()) {
1612: 		res.append(DuckDBPyType(col.Type()));
1613: 	}
1614: 	return res;
1615: }
1616: 
1617: bool DuckDBPyRelation::IsRelation(const py::object &object) {
1618: 	return py::isinstance<DuckDBPyRelation>(object);
1619: }
1620: 
1621: } // namespace duckdb
[end of tools/pythonpkg/src/pyrelation.cpp]
[start of tools/pythonpkg/src/pyresult.cpp]
1: #include "duckdb_python/pyrelation.hpp"
2: #include "duckdb_python/pyconnection/pyconnection.hpp"
3: #include "duckdb_python/pyresult.hpp"
4: #include "duckdb_python/python_objects.hpp"
5: 
6: #include "duckdb_python/arrow/arrow_array_stream.hpp"
7: #include "duckdb/common/arrow/arrow.hpp"
8: #include "duckdb/common/arrow/arrow_util.hpp"
9: #include "duckdb/common/arrow/arrow_converter.hpp"
10: #include "duckdb/common/arrow/arrow_wrapper.hpp"
11: #include "duckdb/common/arrow/result_arrow_wrapper.hpp"
12: #include "duckdb/common/types/date.hpp"
13: #include "duckdb/common/types/hugeint.hpp"
14: #include "duckdb/common/types/uhugeint.hpp"
15: #include "duckdb/common/types/time.hpp"
16: #include "duckdb/common/types/timestamp.hpp"
17: #include "duckdb/common/types/uuid.hpp"
18: #include "duckdb_python/numpy/array_wrapper.hpp"
19: #include "duckdb/common/exception.hpp"
20: #include "duckdb/common/enums/stream_execution_result.hpp"
21: #include "duckdb_python/arrow/arrow_export_utils.hpp"
22: #include "duckdb/main/chunk_scan_state/query_result.hpp"
23: 
24: namespace duckdb {
25: 
26: DuckDBPyResult::DuckDBPyResult(unique_ptr<QueryResult> result_p) : result(std::move(result_p)) {
27: 	if (!result) {
28: 		throw InternalException("PyResult created without a result object");
29: 	}
30: }
31: 
32: DuckDBPyResult::~DuckDBPyResult() {
33: 	try {
34: 		py::gil_scoped_release gil;
35: 		result.reset();
36: 		current_chunk.reset();
37: 	} catch (...) { // NOLINT
38: 	}
39: }
40: 
41: const vector<string> &DuckDBPyResult::GetNames() {
42: 	if (!result) {
43: 		throw InternalException("Calling GetNames without a result object");
44: 	}
45: 	return result->names;
46: }
47: 
48: const vector<LogicalType> &DuckDBPyResult::GetTypes() {
49: 	if (!result) {
50: 		throw InternalException("Calling GetTypes without a result object");
51: 	}
52: 	return result->types;
53: }
54: 
55: unique_ptr<DataChunk> DuckDBPyResult::FetchChunk() {
56: 	if (!result) {
57: 		throw InternalException("FetchChunk called without a result object");
58: 	}
59: 	return FetchNext(*result);
60: }
61: 
62: unique_ptr<DataChunk> DuckDBPyResult::FetchNext(QueryResult &query_result) {
63: 	if (!result_closed && query_result.type == QueryResultType::STREAM_RESULT &&
64: 	    !query_result.Cast<StreamQueryResult>().IsOpen()) {
65: 		result_closed = true;
66: 		return nullptr;
67: 	}
68: 	if (query_result.type == QueryResultType::STREAM_RESULT) {
69: 		auto &stream_result = query_result.Cast<StreamQueryResult>();
70: 		StreamExecutionResult execution_result;
71: 		while (!StreamQueryResult::IsChunkReady(execution_result = stream_result.ExecuteTask())) {
72: 			{
73: 				py::gil_scoped_acquire gil;
74: 				if (PyErr_CheckSignals() != 0) {
75: 					throw std::runtime_error("Query interrupted");
76: 				}
77: 			}
78: 			if (execution_result == StreamExecutionResult::BLOCKED) {
79: 				stream_result.WaitForTask();
80: 			}
81: 		}
82: 		if (execution_result == StreamExecutionResult::EXECUTION_CANCELLED) {
83: 			throw InvalidInputException("The execution of the query was cancelled before it could finish, likely "
84: 			                            "caused by executing a different query");
85: 		}
86: 		if (execution_result == StreamExecutionResult::EXECUTION_ERROR) {
87: 			stream_result.ThrowError();
88: 		}
89: 	}
90: 	auto chunk = query_result.Fetch();
91: 	if (query_result.HasError()) {
92: 		query_result.ThrowError();
93: 	}
94: 	return chunk;
95: }
96: 
97: unique_ptr<DataChunk> DuckDBPyResult::FetchNextRaw(QueryResult &query_result) {
98: 	if (!result_closed && query_result.type == QueryResultType::STREAM_RESULT &&
99: 	    !query_result.Cast<StreamQueryResult>().IsOpen()) {
100: 		result_closed = true;
101: 		return nullptr;
102: 	}
103: 	auto chunk = query_result.FetchRaw();
104: 	if (query_result.HasError()) {
105: 		query_result.ThrowError();
106: 	}
107: 	return chunk;
108: }
109: 
110: Optional<py::tuple> DuckDBPyResult::Fetchone() {
111: 	{
112: 		py::gil_scoped_release release;
113: 		if (!result) {
114: 			throw InvalidInputException("result closed");
115: 		}
116: 		if (!current_chunk || chunk_offset >= current_chunk->size()) {
117: 			current_chunk = FetchNext(*result);
118: 			chunk_offset = 0;
119: 		}
120: 	}
121: 
122: 	if (!current_chunk || current_chunk->size() == 0) {
123: 		return py::none();
124: 	}
125: 	py::tuple res(result->types.size());
126: 
127: 	for (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {
128: 		auto &mask = FlatVector::Validity(current_chunk->data[col_idx]);
129: 		if (!mask.RowIsValid(chunk_offset)) {
130: 			res[col_idx] = py::none();
131: 			continue;
132: 		}
133: 		auto val = current_chunk->data[col_idx].GetValue(chunk_offset);
134: 		res[col_idx] = PythonObject::FromValue(val, result->types[col_idx], result->client_properties);
135: 	}
136: 	chunk_offset++;
137: 	return res;
138: }
139: 
140: py::list DuckDBPyResult::Fetchmany(idx_t size) {
141: 	py::list res;
142: 	for (idx_t i = 0; i < size; i++) {
143: 		auto fres = Fetchone();
144: 		if (fres.is_none()) {
145: 			break;
146: 		}
147: 		res.append(fres);
148: 	}
149: 	return res;
150: }
151: 
152: py::list DuckDBPyResult::Fetchall() {
153: 	py::list res;
154: 	while (true) {
155: 		auto fres = Fetchone();
156: 		if (fres.is_none()) {
157: 			break;
158: 		}
159: 		res.append(fres);
160: 	}
161: 	return res;
162: }
163: 
164: py::dict DuckDBPyResult::FetchNumpy() {
165: 	return FetchNumpyInternal();
166: }
167: 
168: void DuckDBPyResult::FillNumpy(py::dict &res, idx_t col_idx, NumpyResultConversion &conversion, const char *name) {
169: 	if (result->types[col_idx].id() == LogicalTypeId::ENUM) {
170: 		// first we (might) need to create the categorical type
171: 		if (categories_type.find(col_idx) == categories_type.end()) {
172: 			// Equivalent to: pandas.CategoricalDtype(['a', 'b'], ordered=True)
173: 			categories_type[col_idx] = py::module::import("pandas").attr("CategoricalDtype")(categories[col_idx], true);
174: 		}
175: 		// Equivalent to: pandas.Categorical.from_codes(codes=[0, 1, 0, 1], dtype=dtype)
176: 		res[name] = py::module::import("pandas")
177: 		                .attr("Categorical")
178: 		                .attr("from_codes")(conversion.ToArray(col_idx), py::arg("dtype") = categories_type[col_idx]);
179: 		if (!conversion.ToPandas()) {
180: 			res[name] = res[name].attr("to_numpy")();
181: 		}
182: 	} else {
183: 		res[name] = conversion.ToArray(col_idx);
184: 	}
185: }
186: 
187: void InsertCategory(QueryResult &result, unordered_map<idx_t, py::list> &categories) {
188: 	for (idx_t col_idx = 0; col_idx < result.types.size(); col_idx++) {
189: 		auto &type = result.types[col_idx];
190: 		if (type.id() == LogicalTypeId::ENUM) {
191: 			// It's an ENUM type, in addition to converting the codes we must convert the categories
192: 			if (categories.find(col_idx) == categories.end()) {
193: 				auto &categories_list = EnumType::GetValuesInsertOrder(type);
194: 				auto categories_size = EnumType::GetSize(type);
195: 				for (idx_t i = 0; i < categories_size; i++) {
196: 					categories[col_idx].append(py::cast(categories_list.GetValue(i).ToString()));
197: 				}
198: 			}
199: 		}
200: 	}
201: }
202: 
203: unique_ptr<NumpyResultConversion> DuckDBPyResult::InitializeNumpyConversion(bool pandas) {
204: 	if (!result) {
205: 		throw InvalidInputException("result closed");
206: 	}
207: 
208: 	idx_t initial_capacity = STANDARD_VECTOR_SIZE * 2ULL;
209: 	if (result->type == QueryResultType::MATERIALIZED_RESULT) {
210: 		// materialized query result: we know exactly how much space we need
211: 		auto &materialized = result->Cast<MaterializedQueryResult>();
212: 		initial_capacity = materialized.RowCount();
213: 	}
214: 
215: 	auto conversion =
216: 	    make_uniq<NumpyResultConversion>(result->types, initial_capacity, result->client_properties, pandas);
217: 	return conversion;
218: }
219: 
220: py::dict DuckDBPyResult::FetchNumpyInternal(bool stream, idx_t vectors_per_chunk,
221:                                             unique_ptr<NumpyResultConversion> conversion_p) {
222: 	if (!result) {
223: 		throw InvalidInputException("result closed");
224: 	}
225: 	if (!conversion_p) {
226: 		conversion_p = InitializeNumpyConversion();
227: 	}
228: 	auto &conversion = *conversion_p;
229: 
230: 	if (result->type == QueryResultType::MATERIALIZED_RESULT) {
231: 		auto &materialized = result->Cast<MaterializedQueryResult>();
232: 		for (auto &chunk : materialized.Collection().Chunks()) {
233: 			conversion.Append(chunk);
234: 		}
235: 		InsertCategory(materialized, categories);
236: 		materialized.Collection().Reset();
237: 	} else {
238: 		D_ASSERT(result->type == QueryResultType::STREAM_RESULT);
239: 		if (!stream) {
240: 			vectors_per_chunk = NumericLimits<idx_t>::Maximum();
241: 		}
242: 		auto &stream_result = result->Cast<StreamQueryResult>();
243: 		for (idx_t count_vec = 0; count_vec < vectors_per_chunk; count_vec++) {
244: 			if (!stream_result.IsOpen()) {
245: 				break;
246: 			}
247: 			unique_ptr<DataChunk> chunk;
248: 			{
249: 				py::gil_scoped_release release;
250: 				chunk = FetchNextRaw(stream_result);
251: 			}
252: 			if (!chunk || chunk->size() == 0) {
253: 				//! finished
254: 				break;
255: 			}
256: 			conversion.Append(*chunk);
257: 			InsertCategory(stream_result, categories);
258: 		}
259: 	}
260: 
261: 	// now that we have materialized the result in contiguous arrays, construct the actual NumPy arrays or categorical
262: 	// types
263: 	py::dict res;
264: 	auto names = result->names;
265: 	QueryResult::DeduplicateColumns(names);
266: 	for (idx_t col_idx = 0; col_idx < result->names.size(); col_idx++) {
267: 		auto &name = names[col_idx];
268: 		FillNumpy(res, col_idx, conversion, name.c_str());
269: 	}
270: 	return res;
271: }
272: 
273: // TODO: unify these with an enum/flag to indicate which conversions to do
274: void DuckDBPyResult::ChangeToTZType(PandasDataFrame &df) {
275: 	auto names = df.attr("columns").cast<vector<string>>();
276: 
277: 	for (idx_t i = 0; i < result->ColumnCount(); i++) {
278: 		if (result->types[i] == LogicalType::TIMESTAMP_TZ) {
279: 			// first localize to UTC then convert to timezone_config
280: 			auto utc_local = df[names[i].c_str()].attr("dt").attr("tz_localize")("UTC");
281: 			df.attr("__setitem__")(names[i].c_str(),
282: 			                       utc_local.attr("dt").attr("tz_convert")(result->client_properties.time_zone));
283: 		}
284: 	}
285: }
286: 
287: // TODO: unify these with an enum/flag to indicate which conversions to perform
288: void DuckDBPyResult::ChangeDateToDatetime(PandasDataFrame &df) {
289: 	auto names = df.attr("columns").cast<vector<string>>();
290: 
291: 	for (idx_t i = 0; i < result->ColumnCount(); i++) {
292: 		if (result->types[i] == LogicalType::DATE) {
293: 			df.attr("__setitem__")(names[i].c_str(), df[names[i].c_str()].attr("dt").attr("date"));
294: 		}
295: 	}
296: }
297: 
298: PandasDataFrame DuckDBPyResult::FrameFromNumpy(bool date_as_object, const py::handle &o) {
299: 	PandasDataFrame df = py::cast<PandasDataFrame>(py::module::import("pandas").attr("DataFrame").attr("from_dict")(o));
300: 	// Unfortunately we have to do a type change here for timezones since these types are not supported by numpy
301: 	ChangeToTZType(df);
302: 	if (date_as_object) {
303: 		ChangeDateToDatetime(df);
304: 	}
305: 	return df;
306: }
307: 
308: PandasDataFrame DuckDBPyResult::FetchDF(bool date_as_object) {
309: 	auto conversion = InitializeNumpyConversion(true);
310: 	return FrameFromNumpy(date_as_object, FetchNumpyInternal(false, 1, std::move(conversion)));
311: }
312: 
313: PandasDataFrame DuckDBPyResult::FetchDFChunk(idx_t num_of_vectors, bool date_as_object) {
314: 	auto conversion = InitializeNumpyConversion(true);
315: 	return FrameFromNumpy(date_as_object, FetchNumpyInternal(true, num_of_vectors, std::move(conversion)));
316: }
317: 
318: py::dict DuckDBPyResult::FetchPyTorch() {
319: 	auto result_dict = FetchNumpyInternal();
320: 	auto from_numpy = py::module::import("torch").attr("from_numpy");
321: 	for (auto &item : result_dict) {
322: 		result_dict[item.first] = from_numpy(item.second);
323: 	}
324: 	return result_dict;
325: }
326: 
327: py::dict DuckDBPyResult::FetchTF() {
328: 	auto result_dict = FetchNumpyInternal();
329: 	auto convert_to_tensor = py::module::import("tensorflow").attr("convert_to_tensor");
330: 	for (auto &item : result_dict) {
331: 		result_dict[item.first] = convert_to_tensor(item.second);
332: 	}
333: 	return result_dict;
334: }
335: 
336: bool DuckDBPyResult::FetchArrowChunk(ChunkScanState &scan_state, py::list &batches, idx_t rows_per_batch,
337:                                      bool to_polars) {
338: 	ArrowArray data;
339: 	idx_t count;
340: 	auto &query_result = *result.get();
341: 	{
342: 		py::gil_scoped_release release;
343: 		count = ArrowUtil::FetchChunk(scan_state, query_result.client_properties, rows_per_batch, &data);
344: 	}
345: 	if (count == 0) {
346: 		return false;
347: 	}
348: 	ArrowSchema arrow_schema;
349: 	auto names = query_result.names;
350: 	if (to_polars) {
351: 		QueryResult::DeduplicateColumns(names);
352: 	}
353: 	ArrowConverter::ToArrowSchema(&arrow_schema, query_result.types, names, query_result.client_properties);
354: 	TransformDuckToArrowChunk(arrow_schema, data, batches);
355: 	return true;
356: }
357: 
358: py::list DuckDBPyResult::FetchAllArrowChunks(idx_t rows_per_batch, bool to_polars) {
359: 	if (!result) {
360: 		throw InvalidInputException("result closed");
361: 	}
362: 	auto pyarrow_lib_module = py::module::import("pyarrow").attr("lib");
363: 
364: 	py::list batches;
365: 	QueryResultChunkScanState scan_state(*result.get());
366: 	while (FetchArrowChunk(scan_state, batches, rows_per_batch, to_polars)) {
367: 	}
368: 	return batches;
369: }
370: 
371: duckdb::pyarrow::Table DuckDBPyResult::FetchArrowTable(idx_t rows_per_batch, bool to_polars) {
372: 	if (!result) {
373: 		throw InvalidInputException("There is no query result");
374: 	}
375: 	auto names = result->names;
376: 	if (to_polars) {
377: 		QueryResult::DeduplicateColumns(names);
378: 	}
379: 	return pyarrow::ToArrowTable(result->types, names, FetchAllArrowChunks(rows_per_batch, to_polars),
380: 	                             result->client_properties);
381: }
382: 
383: ArrowArrayStream DuckDBPyResult::FetchArrowArrayStream(idx_t rows_per_batch) {
384: 	if (!result) {
385: 		throw InvalidInputException("There is no query result");
386: 	}
387: 	ResultArrowArrayStreamWrapper *result_stream = new ResultArrowArrayStreamWrapper(std::move(result), rows_per_batch);
388: 	// The 'result_stream' is part of the 'private_data' of the ArrowArrayStream and its lifetime is bound to that of
389: 	// the ArrowArrayStream.
390: 	return result_stream->stream;
391: }
392: 
393: duckdb::pyarrow::RecordBatchReader DuckDBPyResult::FetchRecordBatchReader(idx_t rows_per_batch) {
394: 	if (!result) {
395: 		throw InvalidInputException("There is no query result");
396: 	}
397: 	py::gil_scoped_acquire acquire;
398: 	auto pyarrow_lib_module = py::module::import("pyarrow").attr("lib");
399: 	auto record_batch_reader_func = pyarrow_lib_module.attr("RecordBatchReader").attr("_import_from_c");
400: 	auto stream = FetchArrowArrayStream(rows_per_batch);
401: 	py::object record_batch_reader = record_batch_reader_func((uint64_t)&stream); // NOLINT
402: 	return py::cast<duckdb::pyarrow::RecordBatchReader>(record_batch_reader);
403: }
404: 
405: static void ArrowArrayStreamPyCapsuleDestructor(PyObject *object) {
406: 	auto data = PyCapsule_GetPointer(object, "arrow_array_stream");
407: 	if (!data) {
408: 		return;
409: 	}
410: 	auto stream = reinterpret_cast<ArrowArrayStream *>(data);
411: 	if (stream->release) {
412: 		stream->release(stream);
413: 	}
414: 	delete stream;
415: }
416: 
417: py::object DuckDBPyResult::FetchArrowCapsule(idx_t rows_per_batch) {
418: 	auto stream_p = FetchArrowArrayStream(rows_per_batch);
419: 	auto stream = new ArrowArrayStream();
420: 	*stream = stream_p;
421: 	return py::capsule(stream, "arrow_array_stream", ArrowArrayStreamPyCapsuleDestructor);
422: }
423: 
424: py::str GetTypeToPython(const LogicalType &type) {
425: 	switch (type.id()) {
426: 	case LogicalTypeId::BOOLEAN:
427: 		return py::str("bool");
428: 	case LogicalTypeId::TINYINT:
429: 	case LogicalTypeId::SMALLINT:
430: 	case LogicalTypeId::INTEGER:
431: 	case LogicalTypeId::BIGINT:
432: 	case LogicalTypeId::UTINYINT:
433: 	case LogicalTypeId::USMALLINT:
434: 	case LogicalTypeId::UINTEGER:
435: 	case LogicalTypeId::UBIGINT:
436: 	case LogicalTypeId::HUGEINT:
437: 	case LogicalTypeId::UHUGEINT:
438: 	case LogicalTypeId::FLOAT:
439: 	case LogicalTypeId::DOUBLE:
440: 	case LogicalTypeId::DECIMAL: {
441: 		return py::str("NUMBER");
442: 	}
443: 	case LogicalTypeId::VARCHAR: {
444: 		if (type.HasAlias() && type.GetAlias() == "JSON") {
445: 			return py::str("JSON");
446: 		} else {
447: 			return py::str("STRING");
448: 		}
449: 	}
450: 	case LogicalTypeId::BLOB:
451: 	case LogicalTypeId::BIT:
452: 		return py::str("BINARY");
453: 	case LogicalTypeId::TIMESTAMP:
454: 	case LogicalTypeId::TIMESTAMP_TZ:
455: 	case LogicalTypeId::TIMESTAMP_MS:
456: 	case LogicalTypeId::TIMESTAMP_NS:
457: 	case LogicalTypeId::TIMESTAMP_SEC: {
458: 		return py::str("DATETIME");
459: 	}
460: 	case LogicalTypeId::TIME:
461: 	case LogicalTypeId::TIME_TZ: {
462: 		return py::str("Time");
463: 	}
464: 	case LogicalTypeId::DATE: {
465: 		return py::str("Date");
466: 	}
467: 	case LogicalTypeId::STRUCT:
468: 	case LogicalTypeId::MAP:
469: 		return py::str("dict");
470: 	case LogicalTypeId::LIST: {
471: 		return py::str("list");
472: 	}
473: 	case LogicalTypeId::INTERVAL: {
474: 		return py::str("TIMEDELTA");
475: 	}
476: 	case LogicalTypeId::UUID: {
477: 		return py::str("UUID");
478: 	}
479: 	default:
480: 		return py::str(type.ToString());
481: 	}
482: }
483: 
484: py::list DuckDBPyResult::GetDescription(const vector<string> &names, const vector<LogicalType> &types) {
485: 	py::list desc;
486: 
487: 	for (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {
488: 		auto py_name = py::str(names[col_idx]);
489: 		auto py_type = GetTypeToPython(types[col_idx]);
490: 		desc.append(py::make_tuple(py_name, py_type, py::none(), py::none(), py::none(), py::none(), py::none()));
491: 	}
492: 	return desc;
493: }
494: 
495: void DuckDBPyResult::Close() {
496: 	result = nullptr;
497: }
498: 
499: bool DuckDBPyResult::IsClosed() const {
500: 	return result_closed;
501: }
502: 
503: } // namespace duckdb
[end of tools/pythonpkg/src/pyresult.cpp]
[start of tools/pythonpkg/src/python_udf.cpp]
1: #include "duckdb/main/query_result.hpp"
2: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
3: #include "duckdb/function/scalar_function.hpp"
4: #include "duckdb_python/pytype.hpp"
5: #include "duckdb_python/pyconnection/pyconnection.hpp"
6: #include "duckdb_python/pandas/pandas_scan.hpp"
7: #include "duckdb/common/arrow/arrow.hpp"
8: #include "duckdb/common/arrow/arrow_converter.hpp"
9: #include "duckdb/common/arrow/arrow_wrapper.hpp"
10: #include "duckdb/common/arrow/arrow_appender.hpp"
11: #include "duckdb/common/arrow/result_arrow_wrapper.hpp"
12: #include "duckdb_python/arrow/arrow_array_stream.hpp"
13: #include "duckdb/function/table/arrow.hpp"
14: #include "duckdb/function/function.hpp"
15: #include "duckdb_python/numpy/numpy_scan.hpp"
16: #include "duckdb_python/arrow/arrow_export_utils.hpp"
17: #include "duckdb/common/types/arrow_aux_data.hpp"
18: #include "duckdb/parser/tableref/table_function_ref.hpp"
19: 
20: namespace duckdb {
21: 
22: static py::list ConvertToSingleBatch(vector<LogicalType> &types, vector<string> &names, DataChunk &input,
23:                                      const ClientProperties &options) {
24: 	ArrowSchema schema;
25: 	ArrowConverter::ToArrowSchema(&schema, types, names, options);
26: 
27: 	py::list single_batch;
28: 	ArrowAppender appender(types, STANDARD_VECTOR_SIZE, options);
29: 	appender.Append(input, 0, input.size(), input.size());
30: 	auto array = appender.Finalize();
31: 	TransformDuckToArrowChunk(schema, array, single_batch);
32: 	return single_batch;
33: }
34: 
35: static py::object ConvertDataChunkToPyArrowTable(DataChunk &input, const ClientProperties &options) {
36: 	auto types = input.GetTypes();
37: 	vector<string> names;
38: 	names.reserve(types.size());
39: 	for (idx_t i = 0; i < types.size(); i++) {
40: 		names.push_back(StringUtil::Format("c%d", i));
41: 	}
42: 
43: 	return pyarrow::ToArrowTable(types, names, ConvertToSingleBatch(types, names, input, options), options);
44: }
45: 
46: // If these types are arrow canonical extensions, we must check if they are registered.
47: // If not, we should error.
48: void AreExtensionsRegistered(const LogicalType &arrow_type, const LogicalType &duckdb_type) {
49: 	if (arrow_type != duckdb_type) {
50: 		// Is it a UUID Registration?
51: 		if (arrow_type.id() == LogicalTypeId::BLOB && duckdb_type.id() == LogicalTypeId::UUID) {
52: 			throw InvalidConfigurationException(
53: 			    "Mismatch on return type from Arrow object (%s) and DuckDB (%s). It seems that you are using the UUID "
54: 			    "arrow canonical extension, but the same is not yet registered. Make sure to register it first with "
55: 			    "e.g., pa.register_extension_type(UUIDType()). ",
56: 			    arrow_type.ToString(), duckdb_type.ToString());
57: 		}
58: 		// Is it a JSON Registration
59: 		if (!arrow_type.IsJSONType() && duckdb_type.IsJSONType()) {
60: 			throw InvalidConfigurationException(
61: 			    "Mismatch on return type from Arrow object (%s) and DuckDB (%s). It seems that you are using the JSON "
62: 			    "arrow canonical extension, but the same is not yet registered. Make sure to register it first with "
63: 			    "e.g., pa.register_extension_type(JSONType()). ",
64: 			    arrow_type.ToString(), duckdb_type.ToString());
65: 		}
66: 	}
67: }
68: static void ConvertArrowTableToVector(const py::object &table, Vector &out, ClientContext &context, idx_t count) {
69: 	// Create the stream factory from the Table object
70: 	auto ptr = table.ptr();
71: 	py::gil_scoped_release gil;
72: 
73: 	auto stream_factory = make_uniq<PythonTableArrowArrayStreamFactory>(ptr, context.GetClientProperties());
74: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
75: 	auto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;
76: 
77: 	// Get the functions we need
78: 	auto function = ArrowTableFunction::ArrowScanFunction;
79: 	auto bind = ArrowTableFunction::ArrowScanBind;
80: 	auto init_global = ArrowTableFunction::ArrowScanInitGlobal;
81: 	auto init_local = ArrowTableFunction::ArrowScanInitLocalInternal;
82: 
83: 	// Prepare the inputs for the bind
84: 	vector<Value> children;
85: 	children.reserve(3);
86: 	children.push_back(Value::POINTER(CastPointerToValue(stream_factory.get())));
87: 	children.push_back(Value::POINTER(CastPointerToValue(stream_factory_produce)));
88: 	children.push_back(Value::POINTER(CastPointerToValue(stream_factory_get_schema)));
89: 	named_parameter_map_t named_params;
90: 	vector<LogicalType> input_types;
91: 	vector<string> input_names;
92: 
93: 	TableFunctionRef empty;
94: 	TableFunction dummy_table_function;
95: 	dummy_table_function.name = "ConvertArrowTableToVector";
96: 	TableFunctionBindInput bind_input(children, named_params, input_types, input_names, nullptr, nullptr,
97: 	                                  dummy_table_function, empty);
98: 	vector<LogicalType> return_types;
99: 	vector<string> return_names;
100: 
101: 	auto bind_data = bind(context, bind_input, return_types, return_names);
102: 
103: 	if (return_types.size() != 1) {
104: 		throw InvalidInputException(
105: 		    "The returned table from a pyarrow scalar udf should only contain one column, found %d",
106: 		    return_types.size());
107: 	}
108: 
109: 	AreExtensionsRegistered(return_types[0], out.GetType());
110: 
111: 	DataChunk result;
112: 	// Reserve for STANDARD_VECTOR_SIZE instead of count, in case the returned table contains too many tuples
113: 	result.Initialize(context, return_types, STANDARD_VECTOR_SIZE);
114: 
115: 	vector<column_t> column_ids = {0};
116: 	TableFunctionInitInput input(bind_data.get(), column_ids, vector<idx_t>(), nullptr);
117: 	auto global_state = init_global(context, input);
118: 	auto local_state = init_local(context, input, global_state.get());
119: 
120: 	TableFunctionInput function_input(bind_data.get(), local_state.get(), global_state.get());
121: 	function(context, function_input, result);
122: 	if (result.size() != count) {
123: 		throw InvalidInputException("Returned pyarrow table should have %d tuples, found %d", count, result.size());
124: 	}
125: 
126: 	VectorOperations::Cast(context, result.data[0], out, count);
127: 	out.Flatten(count);
128: }
129: 
130: static string NullHandlingError() {
131: 	return R"(
132: The returned result contained NULL values, but the 'null_handling' was set to DEFAULT.
133: If you want more control over NULL values then 'null_handling' should be set to SPECIAL.
134: 
135: With DEFAULT all rows containing NULL have been filtered from the UDFs input.
136: Those rows are automatically set to NULL in the final result.
137: The UDF is not expected to return NULL values.
138: 	)";
139: }
140: 
141: static ValidityMask &GetResultValidity(Vector &result) {
142: 	auto vector_type = result.GetVectorType();
143: 	if (vector_type == VectorType::CONSTANT_VECTOR) {
144: 		return ConstantVector::Validity(result);
145: 	} else if (vector_type == VectorType::FLAT_VECTOR) {
146: 		return FlatVector::Validity(result);
147: 	} else {
148: 		throw InternalException("VectorType %s was not expected here (GetResultValidity)",
149: 		                        EnumUtil::ToString(vector_type));
150: 	}
151: }
152: 
153: static void VerifyVectorizedNullHandling(Vector &result, idx_t count) {
154: 	auto &validity = GetResultValidity(result);
155: 
156: 	if (validity.AllValid()) {
157: 		return;
158: 	}
159: 
160: 	throw InvalidInputException(NullHandlingError());
161: }
162: 
163: static scalar_function_t CreateVectorizedFunction(PyObject *function, PythonExceptionHandling exception_handling,
164:                                                   FunctionNullHandling null_handling) {
165: 	// Through the capture of the lambda, we have access to the function pointer
166: 	// We just need to make sure that it doesn't get garbage collected
167: 	scalar_function_t func = [=](DataChunk &input, ExpressionState &state, Vector &result) -> void {
168: 		py::gil_scoped_acquire gil;
169: 
170: 		const bool default_null_handling = null_handling == FunctionNullHandling::DEFAULT_NULL_HANDLING;
171: 
172: 		// owning references
173: 		py::object python_object;
174: 		// Convert the input datachunk to pyarrow
175: 		ClientProperties options;
176: 
177: 		if (state.HasContext()) {
178: 			auto &context = state.GetContext();
179: 			options = context.GetClientProperties();
180: 		}
181: 
182: 		auto result_validity = FlatVector::Validity(result);
183: 		SelectionVector selvec(input.size());
184: 		idx_t input_size = input.size();
185: 		if (default_null_handling) {
186: 			vector<UnifiedVectorFormat> vec_data(input.ColumnCount());
187: 			for (idx_t i = 0; i < input.ColumnCount(); i++) {
188: 				input.data[i].ToUnifiedFormat(input.size(), vec_data[i]);
189: 			}
190: 
191: 			idx_t index = 0;
192: 			for (idx_t i = 0; i < input.size(); i++) {
193: 				bool any_null = false;
194: 				for (idx_t col_idx = 0; col_idx < input.ColumnCount(); col_idx++) {
195: 					auto &vec = vec_data[col_idx];
196: 					if (!vec.validity.RowIsValid(vec.sel->get_index(i))) {
197: 						any_null = true;
198: 						break;
199: 					}
200: 				}
201: 				if (any_null) {
202: 					result_validity.SetInvalid(i);
203: 					continue;
204: 				}
205: 				selvec.set_index(index++, i);
206: 			}
207: 			if (index != input.size()) {
208: 				input.Slice(selvec, index);
209: 			}
210: 		}
211: 
212: 		auto pyarrow_table = ConvertDataChunkToPyArrowTable(input, options);
213: 		py::tuple column_list = pyarrow_table.attr("columns");
214: 
215: 		auto count = input.size();
216: 
217: 		// Call the function
218: 		auto ret = PyObject_CallObject(function, column_list.ptr());
219: 		bool exception_occurred = false;
220: 		if (ret == nullptr && PyErr_Occurred()) {
221: 			exception_occurred = true;
222: 			if (exception_handling == PythonExceptionHandling::FORWARD_ERROR) {
223: 				auto exception = py::error_already_set();
224: 				throw InvalidInputException("Python exception occurred while executing the UDF: %s", exception.what());
225: 			} else if (exception_handling == PythonExceptionHandling::RETURN_NULL) {
226: 				PyErr_Clear();
227: 				python_object = py::module_::import("pyarrow").attr("nulls")(count);
228: 			} else {
229: 				throw NotImplementedException("Exception handling type not implemented");
230: 			}
231: 		} else {
232: 			python_object = py::reinterpret_steal<py::object>(ret);
233: 		}
234: 		if (!py::isinstance(python_object, py::module_::import("pyarrow").attr("lib").attr("Table"))) {
235: 			// Try to convert into a table
236: 			py::list single_array(1);
237: 			py::list single_name(1);
238: 
239: 			single_array[0] = python_object;
240: 			single_name[0] = "c0";
241: 			try {
242: 				python_object = py::module_::import("pyarrow").attr("lib").attr("Table").attr("from_arrays")(
243: 				    single_array, py::arg("names") = single_name);
244: 			} catch (py::error_already_set &) {
245: 				throw InvalidInputException("Could not convert the result into an Arrow Table");
246: 			}
247: 		}
248: 		// Convert the pyarrow result back to a DuckDB datachunk
249: 		if (count != input_size) {
250: 			D_ASSERT(default_null_handling);
251: 			// We filtered out some NULLs, now we need to reconstruct the final result by adding the nulls back
252: 			Vector temp(result.GetType(), count);
253: 			// Convert the table into a temporary Vector
254: 			ConvertArrowTableToVector(python_object, temp, state.GetContext(), count);
255: 			if (!exception_occurred) {
256: 				VerifyVectorizedNullHandling(temp, count);
257: 			}
258: 			if (count) {
259: 				SelectionVector inverted(input_size);
260: 				// Create a SelVec that inverts the filtering
261: 				// example: count: 6, null_indices: 1,3
262: 				// input selvec: [0, 2, 4, 5]
263: 				// inverted selvec: [0, 0, 1, 1, 2, 3]
264: 				idx_t src_index = 0;
265: 				for (idx_t i = 0; i < input_size; i++) {
266: 					// Fill the gaps with the previous index
267: 					inverted.set_index(i, src_index);
268: 					if (src_index + 1 < count && selvec.get_index(src_index) == i) {
269: 						src_index++;
270: 					}
271: 				}
272: 				VectorOperations::Copy(temp, result, inverted, count, 0, 0, input_size);
273: 			}
274: 			for (idx_t i = 0; i < input_size; i++) {
275: 				FlatVector::SetNull(result, i, !result_validity.RowIsValid(i));
276: 			}
277: 			result.Verify(input_size);
278: 		} else {
279: 			ConvertArrowTableToVector(python_object, result, state.GetContext(), count);
280: 			if (default_null_handling && !exception_occurred) {
281: 				VerifyVectorizedNullHandling(result, count);
282: 			}
283: 		}
284: 
285: 		if (input_size == 1) {
286: 			result.SetVectorType(VectorType::CONSTANT_VECTOR);
287: 		}
288: 	};
289: 	return func;
290: }
291: 
292: static scalar_function_t CreateNativeFunction(PyObject *function, PythonExceptionHandling exception_handling,
293:                                               const ClientProperties &client_properties,
294:                                               FunctionNullHandling null_handling) {
295: 	// Through the capture of the lambda, we have access to the function pointer
296: 	// We just need to make sure that it doesn't get garbage collected
297: 	scalar_function_t func = [=](DataChunk &input, ExpressionState &state, Vector &result) -> void { // NOLINT
298: 		py::gil_scoped_acquire gil;
299: 
300: 		const bool default_null_handling = null_handling == FunctionNullHandling::DEFAULT_NULL_HANDLING;
301: 
302: 		// owning references
303: 		vector<py::object> python_objects;
304: 		vector<PyObject *> python_results;
305: 		python_results.resize(input.size());
306: 		for (idx_t row = 0; row < input.size(); row++) {
307: 
308: 			auto bundled_parameters = py::tuple((int)input.ColumnCount());
309: 			bool contains_null = false;
310: 			for (idx_t i = 0; i < input.ColumnCount(); i++) {
311: 				// Fill the tuple with the arguments for this row
312: 				auto &column = input.data[i];
313: 				auto value = column.GetValue(row);
314: 				if (value.IsNull() && default_null_handling) {
315: 					contains_null = true;
316: 					break;
317: 				}
318: 				bundled_parameters[i] = PythonObject::FromValue(value, column.GetType(), client_properties);
319: 			}
320: 			if (contains_null) {
321: 				// Immediately insert None, no need to call the function
322: 				python_objects.push_back(py::none());
323: 				python_results[row] = py::none().ptr();
324: 				continue;
325: 			}
326: 
327: 			// Call the function
328: 			auto ret = PyObject_CallObject(function, bundled_parameters.ptr());
329: 			if (ret == nullptr && PyErr_Occurred()) {
330: 				if (exception_handling == PythonExceptionHandling::FORWARD_ERROR) {
331: 					auto exception = py::error_already_set();
332: 					throw InvalidInputException("Python exception occurred while executing the UDF: %s",
333: 					                            exception.what());
334: 				} else if (exception_handling == PythonExceptionHandling::RETURN_NULL) {
335: 					PyErr_Clear();
336: 					ret = Py_None;
337: 				} else {
338: 					throw NotImplementedException("Exception handling type not implemented");
339: 				}
340: 			} else if ((!ret || ret == Py_None) && default_null_handling) {
341: 				throw InvalidInputException(NullHandlingError());
342: 			}
343: 			python_objects.push_back(py::reinterpret_steal<py::object>(ret));
344: 			python_results[row] = ret;
345: 		}
346: 
347: 		NumpyScan::ScanObjectColumn(python_results.data(), sizeof(PyObject *), input.size(), 0, result);
348: 		if (input.size() == 1) {
349: 			result.SetVectorType(VectorType::CONSTANT_VECTOR);
350: 		}
351: 	};
352: 	return func;
353: }
354: 
355: namespace {
356: 
357: struct ParameterKind {
358: 	enum class Type : uint8_t { POSITIONAL_ONLY, POSITIONAL_OR_KEYWORD, VAR_POSITIONAL, KEYWORD_ONLY, VAR_KEYWORD };
359: 	static ParameterKind::Type FromString(const string &type_str) {
360: 		if (type_str == "POSITIONAL_ONLY") {
361: 			return Type::POSITIONAL_ONLY;
362: 		} else if (type_str == "POSITIONAL_OR_KEYWORD") {
363: 			return Type::POSITIONAL_OR_KEYWORD;
364: 		} else if (type_str == "VAR_POSITIONAL") {
365: 			return Type::VAR_POSITIONAL;
366: 		} else if (type_str == "KEYWORD_ONLY") {
367: 			return Type::KEYWORD_ONLY;
368: 		} else if (type_str == "VAR_KEYWORD") {
369: 			return Type::VAR_KEYWORD;
370: 		} else {
371: 			throw NotImplementedException("ParameterKindType not implemented for '%s'", type_str);
372: 		}
373: 	}
374: };
375: 
376: struct PythonUDFData {
377: public:
378: 	PythonUDFData(const string &name, bool vectorized, FunctionNullHandling null_handling)
379: 	    : name(name), null_handling(null_handling), vectorized(vectorized) {
380: 		return_type = LogicalType::INVALID;
381: 		param_count = DConstants::INVALID_INDEX;
382: 	}
383: 
384: public:
385: 	string name;
386: 	vector<LogicalType> parameters;
387: 	LogicalType return_type;
388: 	LogicalType varargs = LogicalTypeId::INVALID;
389: 	FunctionNullHandling null_handling;
390: 	idx_t param_count;
391: 	bool vectorized;
392: 
393: public:
394: 	void Verify() {
395: 		if (return_type == LogicalType::INVALID) {
396: 			throw InvalidInputException("Could not infer the return type, please set it explicitly");
397: 		}
398: 	}
399: 
400: 	void OverrideReturnType(const shared_ptr<DuckDBPyType> &type) {
401: 		if (!type) {
402: 			return;
403: 		}
404: 		return_type = type->Type();
405: 	}
406: 
407: 	void OverrideParameters(const py::object &parameters_p) {
408: 		if (py::none().is(parameters_p)) {
409: 			return;
410: 		}
411: 		if (!py::isinstance<py::list>(parameters_p)) {
412: 			throw InvalidInputException("Either leave 'parameters' empty, or provide a list of DuckDBPyType objects");
413: 		}
414: 
415: 		auto params = py::list(parameters_p);
416: 		if (params.size() != param_count) {
417: 			throw InvalidInputException("%d types provided, but the provided function takes %d parameters",
418: 			                            params.size(), param_count);
419: 		}
420: 		D_ASSERT(parameters.empty() || parameters.size() == param_count);
421: 		if (parameters.empty()) {
422: 			for (idx_t i = 0; i < param_count; i++) {
423: 				parameters.push_back(LogicalType::ANY);
424: 			}
425: 		}
426: 		idx_t i = 0;
427: 		for (auto &param : params) {
428: 			auto type = py::cast<shared_ptr<DuckDBPyType>>(param);
429: 			parameters[i++] = type->Type();
430: 		}
431: 	}
432: 
433: 	py::object GetSignature(const py::object &udf) {
434: 		const int32_t PYTHON_3_10_HEX = 0x030a00f0;
435: 		auto python_version = PY_VERSION_HEX;
436: 
437: 		auto signature_func = py::module_::import("inspect").attr("signature");
438: 		if (python_version >= PYTHON_3_10_HEX) {
439: 			return signature_func(udf, py::arg("eval_str") = true);
440: 		} else {
441: 			return signature_func(udf);
442: 		}
443: 	}
444: 
445: 	void AnalyzeSignature(const py::object &udf) {
446: 		auto signature = GetSignature(udf);
447: 		auto sig_params = signature.attr("parameters");
448: 		auto return_annotation = signature.attr("return_annotation");
449: 		auto empty = py::module_::import("inspect").attr("Signature").attr("empty");
450: 		if (!py::none().is(return_annotation) && !empty.is(return_annotation)) {
451: 			shared_ptr<DuckDBPyType> pytype;
452: 			if (py::try_cast<shared_ptr<DuckDBPyType>>(return_annotation, pytype)) {
453: 				return_type = pytype->Type();
454: 			}
455: 		}
456: 		param_count = py::len(sig_params);
457: 		parameters.reserve(param_count);
458: 		auto params = py::dict(sig_params);
459: 		for (auto &item : params) {
460: 			auto &value = item.second;
461: 			shared_ptr<DuckDBPyType> pytype;
462: 			if (py::try_cast<shared_ptr<DuckDBPyType>>(value.attr("annotation"), pytype)) {
463: 				parameters.push_back(pytype->Type());
464: 			} else {
465: 				std::string kind = py::str(value.attr("kind"));
466: 				auto parameter_kind = ParameterKind::FromString(kind);
467: 				if (parameter_kind == ParameterKind::Type::VAR_POSITIONAL) {
468: 					varargs = LogicalType::ANY;
469: 				}
470: 				parameters.push_back(LogicalType::ANY);
471: 			}
472: 		}
473: 	}
474: 
475: 	ScalarFunction GetFunction(const py::function &udf, PythonExceptionHandling exception_handling, bool side_effects,
476: 	                           const ClientProperties &client_properties) {
477: 
478: 		auto &import_cache = *DuckDBPyConnection::ImportCache();
479: 		// Import this module, because importing this from a non-main thread causes a segfault
480: 		(void)import_cache.numpy.core.multiarray();
481: 
482: 		scalar_function_t func;
483: 		if (vectorized) {
484: 			func = CreateVectorizedFunction(udf.ptr(), exception_handling, null_handling);
485: 		} else {
486: 			func = CreateNativeFunction(udf.ptr(), exception_handling, client_properties, null_handling);
487: 		}
488: 		FunctionStability function_side_effects =
489: 		    side_effects ? FunctionStability::VOLATILE : FunctionStability::CONSISTENT;
490: 		ScalarFunction scalar_function(name, std::move(parameters), return_type, func, nullptr, nullptr, nullptr,
491: 		                               nullptr, varargs, function_side_effects, null_handling);
492: 		return scalar_function;
493: 	}
494: };
495: 
496: } // namespace
497: 
498: ScalarFunction DuckDBPyConnection::CreateScalarUDF(const string &name, const py::function &udf,
499:                                                    const py::object &parameters,
500:                                                    const shared_ptr<DuckDBPyType> &return_type, bool vectorized,
501:                                                    FunctionNullHandling null_handling,
502:                                                    PythonExceptionHandling exception_handling, bool side_effects) {
503: 	PythonUDFData data(name, vectorized, null_handling);
504: 	auto &connection = con.GetConnection();
505: 
506: 	data.AnalyzeSignature(udf);
507: 	data.OverrideParameters(parameters);
508: 	data.OverrideReturnType(return_type);
509: 	data.Verify();
510: 	return data.GetFunction(udf, exception_handling, side_effects, connection.context->GetClientProperties());
511: }
512: 
513: } // namespace duckdb
[end of tools/pythonpkg/src/python_udf.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: