You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
0- vs. 1-based indexing
#### What happens?
Arrays use a different index-base (0-based) than Postgres (1-based):

#### To Reproduce
DuckDB:
```sql
D select arr[1] as el from (select array[1,2,3] as arr) sub;
┌────┐
│ el │
├────┤
│ 2  │
└────┘
```

Postgres:
```sql
postgres=# select arr[1] as el from (select array[1,2,3] as arr) sub;
 el 
----
  1
(1 row)
```

We checked what other systems do:

* HyPer/Umbra: 1-based
* BigTable: 0-based
* MySQL: there are not arrays per se, the JSON extractor (`$[...]`) is [0-based](https://dev.mysql.com/doc/refman/5.7/en/json.html)
#### Environment (please complete the following information):
 - OS: n/a
 - DuckDB Version: 0.3.1
 - DuckDB Client: n/a

cc @lnkuiper 

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of src/catalog/default/default_functions.cpp]
1: #include "duckdb/catalog/default/default_functions.hpp"
2: #include "duckdb/parser/parser.hpp"
3: #include "duckdb/parser/parsed_data/create_macro_info.hpp"
4: #include "duckdb/parser/expression/columnref_expression.hpp"
5: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
6: 
7: namespace duckdb {
8: 
9: static DefaultMacro internal_macros[] = {
10: 	{DEFAULT_SCHEMA, "current_user", {nullptr}, "'duckdb'"},                       // user name of current execution context
11: 	{DEFAULT_SCHEMA, "current_catalog", {nullptr}, "'duckdb'"},                    // name of current database (called "catalog" in the SQL standard)
12: 	{DEFAULT_SCHEMA, "current_database", {nullptr}, "'duckdb'"},                   // name of current database
13: 	{DEFAULT_SCHEMA, "user", {nullptr}, "current_user"},                           // equivalent to current_user
14: 	{DEFAULT_SCHEMA, "session_user", {nullptr}, "'duckdb'"},                       // session user name
15: 	{"pg_catalog", "inet_client_addr", {nullptr}, "NULL"},                       // address of the remote connection
16: 	{"pg_catalog", "inet_client_port", {nullptr}, "NULL"},                       // port of the remote connection
17: 	{"pg_catalog", "inet_server_addr", {nullptr}, "NULL"},                       // address of the local connection
18: 	{"pg_catalog", "inet_server_port", {nullptr}, "NULL"},                       // port of the local connection
19: 	{"pg_catalog", "pg_my_temp_schema", {nullptr}, "0"},                         // OID of session's temporary schema, or 0 if none
20: 	{"pg_catalog", "pg_is_other_temp_schema", {"schema_id", nullptr}, "false"},  // is schema another session's temporary schema?
21: 
22: 	{"pg_catalog", "pg_conf_load_time", {nullptr}, "current_timestamp"},         // configuration load time
23: 	{"pg_catalog", "pg_postmaster_start_time", {nullptr}, "current_timestamp"},  // server start time
24: 
25: 	{"pg_catalog", "pg_typeof", {"expression", nullptr}, "lower(typeof(expression))"},  // get the data type of any value
26: 
27: 	// privilege functions
28: 	// {"has_any_column_privilege", {"user", "table", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for any column of table
29: 	{"pg_catalog", "has_any_column_privilege", {"table", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for any column of table
30: 	// {"has_column_privilege", {"user", "table", "column", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for column
31: 	{"pg_catalog", "has_column_privilege", {"table", "column", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for column
32: 	// {"has_database_privilege", {"user", "database", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for database
33: 	{"pg_catalog", "has_database_privilege", {"database", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for database
34: 	// {"has_foreign_data_wrapper_privilege", {"user", "fdw", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for foreign-data wrapper
35: 	{"pg_catalog", "has_foreign_data_wrapper_privilege", {"fdw", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for foreign-data wrapper
36: 	// {"has_function_privilege", {"user", "function", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for function
37: 	{"pg_catalog", "has_function_privilege", {"function", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for function
38: 	// {"has_language_privilege", {"user", "language", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for language
39: 	{"pg_catalog", "has_language_privilege", {"language", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for language
40: 	// {"has_schema_privilege", {"user", "schema, privilege", nullptr}, "true"},  //boolean  //does user have privilege for schema
41: 	{"pg_catalog", "has_schema_privilege", {"schema", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for schema
42: 	// {"has_sequence_privilege", {"user", "sequence", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for sequence
43: 	{"pg_catalog", "has_sequence_privilege", {"sequence", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for sequence
44: 	// {"has_server_privilege", {"user", "server", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for foreign server
45: 	{"pg_catalog", "has_server_privilege", {"server", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for foreign server
46: 	// {"has_table_privilege", {"user", "table", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for table
47: 	{"pg_catalog", "has_table_privilege", {"table", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for table
48: 	// {"has_tablespace_privilege", {"user", "tablespace", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for tablespace
49: 	{"pg_catalog", "has_tablespace_privilege", {"tablespace", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for tablespace
50: 
51: 	// various postgres system functions
52: 	{"pg_catalog", "pg_get_viewdef", {"oid", nullptr}, "(select sql from duckdb_views() v where v.view_oid=oid)"},
53: 	{"pg_catalog", "pg_get_constraintdef", {"constraint_oid", "pretty_bool", nullptr}, "(select constraint_text from duckdb_constraints() d_constraint where d_constraint.table_oid=constraint_oid/1000000 and d_constraint.constraint_index=constraint_oid%1000000)"},
54: 	{"pg_catalog", "pg_get_expr", {"pg_node_tree", "relation_oid", nullptr}, "pg_node_tree"},
55: 	{"pg_catalog", "format_pg_type", {"type_name", nullptr}, "case when type_name='FLOAT' then 'real' when type_name='DOUBLE' then 'double precision' when type_name='DECIMAL' then 'numeric' when type_name='VARCHAR' then 'character varying' when type_name='BLOB' then 'bytea' when type_name='TIMESTAMP' then 'timestamp without time zone' when type_name='TIME' then 'time without time zone' else lower(type_name) end"},
56: 	{"pg_catalog", "format_type", {"type_oid", "typemod", nullptr}, "(select format_pg_type(type_name) from duckdb_types() t where t.type_oid=type_oid) || case when typemod>0 then concat('(', typemod/1000, ',', typemod%1000, ')') else '' end"},
57: 
58: 	{"pg_catalog", "pg_has_role", {"user", "role", "privilege", nullptr}, "true"},  //boolean  //does user have privilege for role
59: 	{"pg_catalog", "pg_has_role", {"role", "privilege", nullptr}, "true"},  //boolean  //does current user have privilege for role
60: 
61: 	{"pg_catalog", "col_description", {"table_oid", "column_number", nullptr}, "NULL"},   // get comment for a table column
62: 	{"pg_catalog", "obj_description", {"object_oid", "catalog_name", nullptr}, "NULL"},   // get comment for a database object
63: 	{"pg_catalog", "shobj_description", {"object_oid", "catalog_name", nullptr}, "NULL"}, // get comment for a shared database object
64: 
65: 	// visibility functions
66: 	{"pg_catalog", "pg_collation_is_visible", {"collation_oid", nullptr}, "true"},
67: 	{"pg_catalog", "pg_conversion_is_visible", {"conversion_oid", nullptr}, "true"},
68: 	{"pg_catalog", "pg_function_is_visible", {"function_oid", nullptr}, "true"},
69: 	{"pg_catalog", "pg_opclass_is_visible", {"opclass_oid", nullptr}, "true"},
70: 	{"pg_catalog", "pg_operator_is_visible", {"operator_oid", nullptr}, "true"},
71: 	{"pg_catalog", "pg_opfamily_is_visible", {"opclass_oid", nullptr}, "true"},
72: 	{"pg_catalog", "pg_table_is_visible", {"table_oid", nullptr}, "true"},
73: 	{"pg_catalog", "pg_ts_config_is_visible", {"config_oid", nullptr}, "true"},
74: 	{"pg_catalog", "pg_ts_dict_is_visible", {"dict_oid", nullptr}, "true"},
75: 	{"pg_catalog", "pg_ts_parser_is_visible", {"parser_oid", nullptr}, "true"},
76: 	{"pg_catalog", "pg_ts_template_is_visible", {"template_oid", nullptr}, "true"},
77: 	{"pg_catalog", "pg_type_is_visible", {"type_oid", nullptr}, "true"},
78: 
79: 	{DEFAULT_SCHEMA, "round_even", {"x", "n", nullptr}, "CASE ((abs(x) * power(10, n+1)) % 10) WHEN 5 THEN round(x/2, n) * 2 ELSE round(x, n) END"},
80: 	{DEFAULT_SCHEMA, "roundbankers", {"x", "n", nullptr}, "round_even(x, n)"},
81: 	{DEFAULT_SCHEMA, "nullif", {"a", "b", nullptr}, "CASE WHEN a=b THEN NULL ELSE a END"},
82: 	{DEFAULT_SCHEMA, "list_append", {"l", "e", nullptr}, "list_concat(l, list_value(e))"},
83: 	{DEFAULT_SCHEMA, "array_append", {"arr", "el", nullptr}, "list_append(arr, el)"},
84: 	{DEFAULT_SCHEMA, "list_prepend", {"e", "l", nullptr}, "list_concat(list_value(e), l)"},
85: 	{DEFAULT_SCHEMA, "array_prepend", {"el", "arr", nullptr}, "list_prepend(el, arr)"},
86: 	{DEFAULT_SCHEMA, "array_pop_back", {"arr", nullptr}, "arr[:LEN(arr)-1]"},
87: 	{DEFAULT_SCHEMA, "array_pop_front", {"arr", nullptr}, "arr[1:]"},
88: 	{DEFAULT_SCHEMA, "array_push_back", {"arr", "e", nullptr}, "list_concat(arr, list_value(e))"},
89: 	{DEFAULT_SCHEMA, "array_push_front", {"arr", "e", nullptr}, "list_concat(list_value(e), arr)"},
90: 	{DEFAULT_SCHEMA, "generate_subscripts", {"arr", "dim", nullptr}, "unnest(generate_series(1, array_length(arr, dim)))"},
91: 	{DEFAULT_SCHEMA, "fdiv", {"x", "y", nullptr}, "floor(x/y)"},
92: 	{DEFAULT_SCHEMA, "fmod", {"x", "y", nullptr}, "(x-y*floor(x/y))"},
93: 	{nullptr, nullptr, {nullptr}, nullptr}};
94: 
95: unique_ptr<CreateMacroInfo> DefaultFunctionGenerator::CreateInternalMacroInfo(DefaultMacro &default_macro) {
96: 	// parse the expression
97: 	auto expressions = Parser::ParseExpressionList(default_macro.macro);
98: 	D_ASSERT(expressions.size() == 1);
99: 
100: 	auto result = make_unique<MacroFunction>(move(expressions[0]));
101: 	for (idx_t param_idx = 0; default_macro.parameters[param_idx] != nullptr; param_idx++) {
102: 		result->parameters.push_back(
103: 		    make_unique<ColumnRefExpression>(default_macro.parameters[param_idx]));
104: 	}
105: 
106: 	auto bind_info = make_unique<CreateMacroInfo>();
107: 	bind_info->schema = default_macro.schema;
108: 	bind_info->name = default_macro.name;
109: 	bind_info->temporary = true;
110: 	bind_info->internal = true;
111: 	bind_info->function = move(result);
112: 	return bind_info;
113: }
114: 
115: static unique_ptr<CreateFunctionInfo> GetDefaultFunction(const string &input_schema, const string &input_name) {
116: 	auto schema = StringUtil::Lower(input_schema);
117: 	auto name = StringUtil::Lower(input_name);
118: 	for (idx_t index = 0; internal_macros[index].name != nullptr; index++) {
119: 		if (internal_macros[index].schema == schema && internal_macros[index].name == name) {
120: 			return DefaultFunctionGenerator::CreateInternalMacroInfo(internal_macros[index]);
121: 		}
122: 	}
123: 	return nullptr;
124: }
125: 
126: DefaultFunctionGenerator::DefaultFunctionGenerator(Catalog &catalog, SchemaCatalogEntry *schema)
127:     : DefaultGenerator(catalog), schema(schema) {
128: }
129: 
130: unique_ptr<CatalogEntry> DefaultFunctionGenerator::CreateDefaultEntry(ClientContext &context,
131:                                                                       const string &entry_name) {
132: 	auto info = GetDefaultFunction(schema->name, entry_name);
133: 	if (info) {
134: 		return make_unique_base<CatalogEntry, MacroCatalogEntry>(&catalog, schema, (CreateMacroInfo *)info.get());
135: 	}
136: 	return nullptr;
137: }
138: 
139: vector<string> DefaultFunctionGenerator::GetDefaultEntries() {
140: 	vector<string> result;
141: 	for (idx_t index = 0; internal_macros[index].name != nullptr; index++) {
142: 		if (internal_macros[index].schema == schema->name) {
143: 			result.emplace_back(internal_macros[index].name);
144: 		}
145: 	}
146: 	return result;
147: }
148: 
149: } // namespace duckdb
[end of src/catalog/default/default_functions.cpp]
[start of src/function/scalar/list/array_slice.cpp]
1: #include "duckdb/planner/expression/bound_function_expression.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/parser/expression/bound_expression.hpp"
4: #include "duckdb/function/scalar/nested_functions.hpp"
5: #include "duckdb/common/types/chunk_collection.hpp"
6: #include "duckdb/common/types/data_chunk.hpp"
7: #include "duckdb/common/pair.hpp"
8: #include "duckdb/function/scalar/string_functions.hpp"
9: 
10: namespace duckdb {
11: 
12: template <typename INPUT_TYPE, typename INDEX_TYPE>
13: INDEX_TYPE ValueOffset(const INPUT_TYPE &value) {
14: 	return 0;
15: }
16: 
17: template <>
18: int64_t ValueOffset(const list_entry_t &value) {
19: 	return value.offset;
20: }
21: 
22: template <typename INPUT_TYPE, typename INDEX_TYPE>
23: INDEX_TYPE ValueLength(const INPUT_TYPE &value) {
24: 	return 0;
25: }
26: 
27: template <>
28: int64_t ValueLength(const list_entry_t &value) {
29: 	return value.length;
30: }
31: 
32: template <>
33: int32_t ValueLength(const string_t &value) {
34: 	return LengthFun::Length<string_t, int32_t>(value);
35: }
36: 
37: template <typename INPUT_TYPE, typename INDEX_TYPE>
38: bool ClampIndex(INDEX_TYPE &index, const INPUT_TYPE &value) {
39: 	const auto length = ValueLength<INPUT_TYPE, INDEX_TYPE>(value);
40: 	if (index < 0) {
41: 		if (-index > length) {
42: 			return false;
43: 		}
44: 		index = length + index;
45: 	} else if (index > length) {
46: 		index = length;
47: 	}
48: 	return true;
49: }
50: 
51: template <typename INPUT_TYPE, typename INDEX_TYPE>
52: static bool ClampSlice(const INPUT_TYPE &value, INDEX_TYPE &begin, INDEX_TYPE &end, bool begin_valid, bool end_valid) {
53: 	// Clamp offsets
54: 	begin = begin_valid ? begin : 0;
55: 	end = end_valid ? end : ValueLength<INPUT_TYPE, INDEX_TYPE>(value);
56: 	if (!ClampIndex(begin, value) || !ClampIndex(end, value)) {
57: 		return false;
58: 	}
59: 	end = MaxValue<INDEX_TYPE>(begin, end);
60: 
61: 	return true;
62: }
63: 
64: template <typename INPUT_TYPE, typename INDEX_TYPE>
65: INPUT_TYPE SliceValue(Vector &result, INPUT_TYPE input, INDEX_TYPE begin, INDEX_TYPE end) {
66: 	return input;
67: }
68: 
69: template <>
70: list_entry_t SliceValue(Vector &result, list_entry_t input, int64_t begin, int64_t end) {
71: 	input.offset += begin;
72: 	input.length = end - begin;
73: 	return input;
74: }
75: 
76: template <>
77: string_t SliceValue(Vector &result, string_t input, int32_t begin, int32_t end) {
78: 	// one-based - zero has strange semantics
79: 	return SubstringFun::SubstringScalarFunction(result, input, begin + 1, end - begin);
80: }
81: 
82: template <typename INPUT_TYPE, typename INDEX_TYPE>
83: static void ExecuteSlice(Vector &result, Vector &s, Vector &b, Vector &e, const idx_t count) {
84: 	if (result.GetVectorType() == VectorType::CONSTANT_VECTOR) {
85: 		auto rdata = ConstantVector::GetData<INPUT_TYPE>(result);
86: 		auto sdata = ConstantVector::GetData<INPUT_TYPE>(s);
87: 		auto bdata = ConstantVector::GetData<INDEX_TYPE>(b);
88: 		auto edata = ConstantVector::GetData<INDEX_TYPE>(e);
89: 
90: 		auto sliced = sdata[0];
91: 		auto begin = bdata[0];
92: 		auto end = edata[0];
93: 
94: 		auto svalid = !ConstantVector::IsNull(s);
95: 		auto bvalid = !ConstantVector::IsNull(b);
96: 		auto evalid = !ConstantVector::IsNull(e);
97: 
98: 		// Try to slice
99: 		if (!svalid || !ClampSlice(sliced, begin, end, bvalid, evalid)) {
100: 			ConstantVector::SetNull(result, true);
101: 		} else {
102: 			rdata[0] = SliceValue<INPUT_TYPE, INDEX_TYPE>(result, sliced, begin, end);
103: 		}
104: 	} else {
105: 		VectorData sdata, bdata, edata;
106: 
107: 		s.Orrify(count, sdata);
108: 		b.Orrify(count, bdata);
109: 		e.Orrify(count, edata);
110: 
111: 		auto rdata = FlatVector::GetData<INPUT_TYPE>(result);
112: 		auto &rmask = FlatVector::Validity(result);
113: 
114: 		for (idx_t i = 0; i < count; ++i) {
115: 			auto sidx = sdata.sel->get_index(i);
116: 			auto bidx = bdata.sel->get_index(i);
117: 			auto eidx = edata.sel->get_index(i);
118: 
119: 			auto sliced = ((INPUT_TYPE *)sdata.data)[sidx];
120: 			auto begin = ((INDEX_TYPE *)bdata.data)[bidx];
121: 			auto end = ((INDEX_TYPE *)edata.data)[eidx];
122: 
123: 			auto svalid = sdata.validity.RowIsValid(sidx);
124: 			auto bvalid = bdata.validity.RowIsValid(bidx);
125: 			auto evalid = edata.validity.RowIsValid(eidx);
126: 
127: 			// Try to slice
128: 			if (!svalid || !ClampSlice(sliced, begin, end, bvalid, evalid)) {
129: 				rmask.SetInvalid(i);
130: 			} else {
131: 				rdata[i] = SliceValue<INPUT_TYPE, INDEX_TYPE>(result, sliced, begin, end);
132: 			}
133: 		}
134: 	}
135: 
136: 	result.Verify(count);
137: }
138: 
139: static void ArraySliceFunction(DataChunk &args, ExpressionState &state, Vector &result) {
140: 	D_ASSERT(args.ColumnCount() == 3);
141: 	D_ASSERT(args.data.size() == 3);
142: 	auto count = args.size();
143: 
144: 	Vector &s = args.data[0];
145: 	Vector &b = args.data[1];
146: 	Vector &e = args.data[2];
147: 
148: 	s.Normalify(count);
149: 	switch (result.GetType().id()) {
150: 	case LogicalTypeId::LIST:
151: 		// Share the value dictionary as we are just going to slice it
152: 		ListVector::ReferenceEntry(result, s);
153: 		ExecuteSlice<list_entry_t, int64_t>(result, s, b, e, count);
154: 		break;
155: 	case LogicalTypeId::VARCHAR:
156: 		ExecuteSlice<string_t, int32_t>(result, s, b, e, count);
157: 		break;
158: 	default:
159: 		throw NotImplementedException("Specifier type not implemented");
160: 	}
161: 
162: 	result.SetVectorType(VectorType::CONSTANT_VECTOR);
163: 	for (idx_t i = 0; i < args.ColumnCount(); i++) {
164: 		if (args.data[i].GetVectorType() != VectorType::CONSTANT_VECTOR) {
165: 			result.SetVectorType(VectorType::FLAT_VECTOR);
166: 			break;
167: 		}
168: 	}
169: }
170: 
171: static unique_ptr<FunctionData> ArraySliceBind(ClientContext &context, ScalarFunction &bound_function,
172:                                                vector<unique_ptr<Expression>> &arguments) {
173: 	D_ASSERT(bound_function.arguments.size() == 3);
174: 	switch (arguments[0]->return_type.id()) {
175: 	case LogicalTypeId::LIST:
176: 		// The result is the same type
177: 		bound_function.return_type = arguments[0]->return_type;
178: 		break;
179: 	case LogicalTypeId::VARCHAR:
180: 		// string slice returns a string, but can only accept 32 bit integers
181: 		bound_function.return_type = arguments[0]->return_type;
182: 		bound_function.arguments[1] = LogicalType::INTEGER;
183: 		bound_function.arguments[2] = LogicalType::INTEGER;
184: 		break;
185: 	default:
186: 		throw BinderException("ARRAY_SLICE can only operate on LISTs and VARCHARs");
187: 	}
188: 
189: 	return make_unique<VariableReturnBindData>(bound_function.return_type);
190: }
191: 
192: void ArraySliceFun::RegisterFunction(BuiltinFunctions &set) {
193: 	// the arguments and return types are actually set in the binder function
194: 	ScalarFunction fun({LogicalType::ANY, LogicalType::BIGINT, LogicalType::BIGINT}, LogicalType::ANY,
195: 	                   ArraySliceFunction, false, ArraySliceBind);
196: 	fun.varargs = LogicalType::ANY;
197: 	set.AddFunction({"array_slice", "list_slice"}, fun);
198: }
199: 
200: } // namespace duckdb
[end of src/function/scalar/list/array_slice.cpp]
[start of src/function/scalar/list/list_extract.cpp]
1: #include "duckdb/planner/expression/bound_function_expression.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/common/vector_operations/binary_executor.hpp"
4: #include "duckdb/parser/expression/bound_expression.hpp"
5: #include "duckdb/function/scalar/nested_functions.hpp"
6: #include "duckdb/function/scalar/string_functions.hpp"
7: #include "duckdb/common/types/chunk_collection.hpp"
8: #include "duckdb/common/types/data_chunk.hpp"
9: #include "duckdb/common/pair.hpp"
10: #include "duckdb/storage/statistics/list_statistics.hpp"
11: #include "duckdb/storage/statistics/validity_statistics.hpp"
12: 
13: namespace duckdb {
14: 
15: template <class T, bool HEAP_REF = false, bool VALIDITY_ONLY = false>
16: void ListExtractTemplate(idx_t count, VectorData &list_data, VectorData &offsets_data, Vector &child_vector,
17:                          idx_t list_size, Vector &result) {
18: 	VectorData child_data;
19: 	child_vector.Orrify(list_size, child_data);
20: 
21: 	T *result_data;
22: 
23: 	result.SetVectorType(VectorType::FLAT_VECTOR);
24: 	if (!VALIDITY_ONLY) {
25: 		result_data = FlatVector::GetData<T>(result);
26: 	}
27: 	auto &result_mask = FlatVector::Validity(result);
28: 
29: 	// heap-ref once
30: 	if (HEAP_REF) {
31: 		StringVector::AddHeapReference(result, child_vector);
32: 	}
33: 
34: 	// this is lifted from ExecuteGenericLoop because we can't push the list child data into this otherwise
35: 	// should have gone with GetValue perhaps
36: 	for (idx_t i = 0; i < count; i++) {
37: 		auto list_index = list_data.sel->get_index(i);
38: 		auto offsets_index = offsets_data.sel->get_index(i);
39: 		if (list_data.validity.RowIsValid(list_index) && offsets_data.validity.RowIsValid(offsets_index)) {
40: 			auto list_entry = ((list_entry_t *)list_data.data)[list_index];
41: 			auto offsets_entry = ((int64_t *)offsets_data.data)[offsets_index];
42: 			idx_t child_offset;
43: 			if (offsets_entry < 0) {
44: 				if ((idx_t)-offsets_entry > list_entry.length) {
45: 					result_mask.SetInvalid(i);
46: 					continue;
47: 				}
48: 				child_offset = list_entry.offset + list_entry.length + offsets_entry;
49: 			} else {
50: 				if ((idx_t)offsets_entry >= list_entry.length) {
51: 					result_mask.SetInvalid(i);
52: 					continue;
53: 				}
54: 				child_offset = list_entry.offset + offsets_entry;
55: 			}
56: 			if (child_data.validity.RowIsValid(child_offset)) {
57: 				if (!VALIDITY_ONLY) {
58: 					result_data[i] = ((T *)child_data.data)[child_offset];
59: 				}
60: 			} else {
61: 				result_mask.SetInvalid(i);
62: 			}
63: 		} else {
64: 			result_mask.SetInvalid(i);
65: 		}
66: 	}
67: 	if (count == 1) {
68: 		result.SetVectorType(VectorType::CONSTANT_VECTOR);
69: 	}
70: }
71: static void ExecuteListExtractInternal(const idx_t count, VectorData &list, VectorData &offsets, Vector &child_vector,
72:                                        idx_t list_size, Vector &result) {
73: 	D_ASSERT(child_vector.GetType() == result.GetType());
74: 	switch (result.GetType().InternalType()) {
75: 	case PhysicalType::BOOL:
76: 	case PhysicalType::INT8:
77: 		ListExtractTemplate<int8_t>(count, list, offsets, child_vector, list_size, result);
78: 		break;
79: 	case PhysicalType::INT16:
80: 		ListExtractTemplate<int16_t>(count, list, offsets, child_vector, list_size, result);
81: 		break;
82: 	case PhysicalType::INT32:
83: 		ListExtractTemplate<int32_t>(count, list, offsets, child_vector, list_size, result);
84: 		break;
85: 	case PhysicalType::INT64:
86: 		ListExtractTemplate<int64_t>(count, list, offsets, child_vector, list_size, result);
87: 		break;
88: 	case PhysicalType::INT128:
89: 		ListExtractTemplate<hugeint_t>(count, list, offsets, child_vector, list_size, result);
90: 		break;
91: 	case PhysicalType::UINT8:
92: 		ListExtractTemplate<uint8_t>(count, list, offsets, child_vector, list_size, result);
93: 		break;
94: 	case PhysicalType::UINT16:
95: 		ListExtractTemplate<uint16_t>(count, list, offsets, child_vector, list_size, result);
96: 		break;
97: 	case PhysicalType::UINT32:
98: 		ListExtractTemplate<uint32_t>(count, list, offsets, child_vector, list_size, result);
99: 		break;
100: 	case PhysicalType::UINT64:
101: 		ListExtractTemplate<uint64_t>(count, list, offsets, child_vector, list_size, result);
102: 		break;
103: 	case PhysicalType::FLOAT:
104: 		ListExtractTemplate<float>(count, list, offsets, child_vector, list_size, result);
105: 		break;
106: 	case PhysicalType::DOUBLE:
107: 		ListExtractTemplate<double>(count, list, offsets, child_vector, list_size, result);
108: 		break;
109: 	case PhysicalType::VARCHAR:
110: 		ListExtractTemplate<string_t, true>(count, list, offsets, child_vector, list_size, result);
111: 		break;
112: 	case PhysicalType::INTERVAL:
113: 		ListExtractTemplate<interval_t>(count, list, offsets, child_vector, list_size, result);
114: 		break;
115: 	case PhysicalType::STRUCT: {
116: 		auto &entries = StructVector::GetEntries(child_vector);
117: 		auto &result_entries = StructVector::GetEntries(result);
118: 		D_ASSERT(entries.size() == result_entries.size());
119: 		// extract the child entries of the struct
120: 		for (idx_t i = 0; i < entries.size(); i++) {
121: 			ExecuteListExtractInternal(count, list, offsets, *entries[i], list_size, *result_entries[i]);
122: 		}
123: 		// extract the validity mask
124: 		ListExtractTemplate<bool, false, true>(count, list, offsets, child_vector, list_size, result);
125: 		break;
126: 	}
127: 	case PhysicalType::LIST: {
128: 		// nested list: we have to reference the child
129: 		auto &child_child_list = ListVector::GetEntry(child_vector);
130: 
131: 		ListVector::GetEntry(result).Reference(child_child_list);
132: 		ListVector::SetListSize(result, ListVector::GetListSize(child_vector));
133: 		ListExtractTemplate<list_entry_t>(count, list, offsets, child_vector, list_size, result);
134: 		break;
135: 	}
136: 	default:
137: 		throw NotImplementedException("Unimplemented type for LIST_EXTRACT");
138: 	}
139: }
140: 
141: static void ExecuteListExtract(Vector &result, Vector &list, Vector &offsets, const idx_t count) {
142: 	D_ASSERT(list.GetType().id() == LogicalTypeId::LIST);
143: 	VectorData list_data;
144: 	VectorData offsets_data;
145: 
146: 	list.Orrify(count, list_data);
147: 	offsets.Orrify(count, offsets_data);
148: 	ExecuteListExtractInternal(count, list_data, offsets_data, ListVector::GetEntry(list),
149: 	                           ListVector::GetListSize(list), result);
150: 	result.Verify(count);
151: }
152: 
153: static void ExecuteStringExtract(Vector &result, Vector &input_vector, Vector &subscript_vector, const idx_t count) {
154: 	BinaryExecutor::Execute<string_t, int32_t, string_t>(
155: 	    input_vector, subscript_vector, result, count, [&](string_t input_string, int32_t subscript) {
156: 		    return SubstringFun::SubstringScalarFunction(result, input_string, subscript + int32_t(subscript >= 0), 1);
157: 	    });
158: }
159: 
160: static void ListExtractFunction(DataChunk &args, ExpressionState &state, Vector &result) {
161: 	D_ASSERT(args.ColumnCount() == 2);
162: 	auto count = args.size();
163: 
164: 	result.SetVectorType(VectorType::CONSTANT_VECTOR);
165: 	for (idx_t i = 0; i < args.ColumnCount(); i++) {
166: 		if (args.data[i].GetVectorType() != VectorType::CONSTANT_VECTOR) {
167: 			result.SetVectorType(VectorType::FLAT_VECTOR);
168: 		}
169: 	}
170: 
171: 	Vector &base = args.data[0];
172: 	Vector &subscript = args.data[1];
173: 
174: 	switch (base.GetType().id()) {
175: 	case LogicalTypeId::LIST:
176: 		ExecuteListExtract(result, base, subscript, count);
177: 		break;
178: 	case LogicalTypeId::VARCHAR:
179: 		ExecuteStringExtract(result, base, subscript, count);
180: 		break;
181: 	case LogicalTypeId::SQLNULL:
182: 		result.SetVectorType(VectorType::CONSTANT_VECTOR);
183: 		ConstantVector::SetNull(result, true);
184: 		break;
185: 	default:
186: 		throw NotImplementedException("Specifier type not implemented");
187: 	}
188: }
189: 
190: static unique_ptr<FunctionData> ListExtractBind(ClientContext &context, ScalarFunction &bound_function,
191:                                                 vector<unique_ptr<Expression>> &arguments) {
192: 	D_ASSERT(bound_function.arguments.size() == 2);
193: 	if (arguments[0]->return_type.id() == LogicalTypeId::SQLNULL) {
194: 		bound_function.arguments[0] = LogicalType::SQLNULL;
195: 		bound_function.return_type = LogicalType::SQLNULL;
196: 	} else {
197: 		D_ASSERT(LogicalTypeId::LIST == arguments[0]->return_type.id());
198: 		// list extract returns the child type of the list as return type
199: 		bound_function.return_type = ListType::GetChildType(arguments[0]->return_type);
200: 	}
201: 	return make_unique<VariableReturnBindData>(bound_function.return_type);
202: }
203: 
204: static unique_ptr<BaseStatistics> ListExtractStats(ClientContext &context, BoundFunctionExpression &expr,
205:                                                    FunctionData *bind_data,
206:                                                    vector<unique_ptr<BaseStatistics>> &child_stats) {
207: 	if (!child_stats[0]) {
208: 		return nullptr;
209: 	}
210: 	auto &list_stats = (ListStatistics &)*child_stats[0];
211: 	if (!list_stats.child_stats) {
212: 		return nullptr;
213: 	}
214: 	auto child_copy = list_stats.child_stats->Copy();
215: 	// list_extract always pushes a NULL, since if the offset is out of range for a list it inserts a null
216: 	child_copy->validity_stats = make_unique<ValidityStatistics>(true);
217: 	return child_copy;
218: }
219: 
220: void ListExtractFun::RegisterFunction(BuiltinFunctions &set) {
221: 	// the arguments and return types are actually set in the binder function
222: 	ScalarFunction lfun({LogicalType::LIST(LogicalType::ANY), LogicalType::BIGINT}, LogicalType::ANY,
223: 	                    ListExtractFunction, false, ListExtractBind, nullptr, ListExtractStats);
224: 
225: 	ScalarFunction sfun({LogicalType::VARCHAR, LogicalType::INTEGER}, LogicalType::VARCHAR, ListExtractFunction, false,
226: 	                    nullptr);
227: 
228: 	ScalarFunctionSet list_extract("list_extract");
229: 	list_extract.AddFunction(lfun);
230: 	list_extract.AddFunction(sfun);
231: 	set.AddFunction(list_extract);
232: 
233: 	ScalarFunctionSet list_element("list_element");
234: 	list_element.AddFunction(lfun);
235: 	list_element.AddFunction(sfun);
236: 	set.AddFunction(list_element);
237: 
238: 	ScalarFunctionSet array_extract("array_extract");
239: 	array_extract.AddFunction(lfun);
240: 	array_extract.AddFunction(sfun);
241: 	array_extract.AddFunction(StructExtractFun::GetFunction());
242: 	set.AddFunction(array_extract);
243: }
244: 
245: } // namespace duckdb
[end of src/function/scalar/list/list_extract.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: