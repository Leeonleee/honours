{
  "repo": "duckdb/duckdb",
  "pull_number": 8710,
  "instance_id": "duckdb__duckdb-8710",
  "issue_numbers": [
    "8358"
  ],
  "base_commit": "fb10d8eedef99997ef4a3062e497480a4099eb0d",
  "patch": "diff --git a/.github/config/uncovered_files.csv b/.github/config/uncovered_files.csv\nindex 7e6eabe6e1c5..5064538f6c94 100644\n--- a/.github/config/uncovered_files.csv\n+++ b/.github/config/uncovered_files.csv\n@@ -323,7 +323,7 @@ extension/parquet/parquet_metadata.cpp\t29\n extension/parquet/parquet_reader.cpp\t113\n extension/parquet/parquet_statistics.cpp\t12\n extension/parquet/parquet_timestamp.cpp\t12\n-extension/parquet/parquet_writer.cpp\t7\n+extension/parquet/parquet_writer.cpp\t9\n extension/parquet/zstd_file_system.cpp\t6\n include/duckdb/catalog/catalog.hpp\t3\n include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\t3\ndiff --git a/extension/parquet/include/parquet_writer.hpp b/extension/parquet/include/parquet_writer.hpp\nindex 57005a65404d..5ff38ee16dd4 100644\n--- a/extension/parquet/include/parquet_writer.hpp\n+++ b/extension/parquet/include/parquet_writer.hpp\n@@ -75,7 +75,11 @@ class ParquetWriter {\n \t\treturn *writer;\n \t}\n \n+\tstatic bool TypeIsSupported(const LogicalType &type);\n+\n private:\n+\tstatic bool DuckDBTypeToParquetTypeInternal(const LogicalType &duckdb_type,\n+\t                                            duckdb_parquet::format::Type::type &type);\n \tstring file_name;\n \tvector<LogicalType> sql_types;\n \tvector<string> column_names;\ndiff --git a/extension/parquet/parquet_extension.cpp b/extension/parquet/parquet_extension.cpp\nindex b86df1f9f8a9..5aa1e6c881cf 100644\n--- a/extension/parquet/parquet_extension.cpp\n+++ b/extension/parquet/parquet_extension.cpp\n@@ -37,6 +37,7 @@\n #include \"duckdb/storage/table/row_group.hpp\"\n #include \"duckdb/common/serializer/format_serializer.hpp\"\n #include \"duckdb/common/serializer/format_deserializer.hpp\"\n+\n #endif\n \n namespace duckdb {\n@@ -826,6 +827,7 @@ unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info\n \tif (!row_group_size_bytes_set) {\n \t\tbind_data->row_group_size_bytes = bind_data->row_group_size * ParquetWriteBindData::BYTES_PER_ROW;\n \t}\n+\n \tbind_data->sql_types = sql_types;\n \tbind_data->column_names = names;\n \treturn std::move(bind_data);\n@@ -1003,6 +1005,7 @@ void ParquetExtension::Load(DuckDB &db) {\n \tfunction.desired_batch_size = ParquetWriteDesiredBatchSize;\n \tfunction.serialize = ParquetCopySerialize;\n \tfunction.deserialize = ParquetCopyDeserialize;\n+\tfunction.supports_type = ParquetWriter::TypeIsSupported;\n \n \tfunction.extension = \"parquet\";\n \tExtensionUtil::RegisterFunction(db_instance, function);\ndiff --git a/extension/parquet/parquet_writer.cpp b/extension/parquet/parquet_writer.cpp\nindex 8bbd52fcbd71..8d9d3fc119a2 100644\n--- a/extension/parquet/parquet_writer.cpp\n+++ b/extension/parquet/parquet_writer.cpp\n@@ -76,27 +76,32 @@ class MyTransport : public TTransport {\n \tSerializer &serializer;\n };\n \n-Type::type ParquetWriter::DuckDBTypeToParquetType(const LogicalType &duckdb_type) {\n+bool ParquetWriter::DuckDBTypeToParquetTypeInternal(const LogicalType &duckdb_type, Type::type &parquet_type) {\n \tswitch (duckdb_type.id()) {\n \tcase LogicalTypeId::BOOLEAN:\n-\t\treturn Type::BOOLEAN;\n+\t\tparquet_type = Type::BOOLEAN;\n+\t\tbreak;\n \tcase LogicalTypeId::TINYINT:\n \tcase LogicalTypeId::SMALLINT:\n \tcase LogicalTypeId::INTEGER:\n \tcase LogicalTypeId::DATE:\n-\t\treturn Type::INT32;\n+\t\tparquet_type = Type::INT32;\n+\t\tbreak;\n \tcase LogicalTypeId::BIGINT:\n-\t\treturn Type::INT64;\n+\t\tparquet_type = Type::INT64;\n+\t\tbreak;\n \tcase LogicalTypeId::FLOAT:\n-\t\treturn Type::FLOAT;\n+\t\tparquet_type = Type::FLOAT;\n+\t\tbreak;\n \tcase LogicalTypeId::DOUBLE:\n \tcase LogicalTypeId::HUGEINT:\n-\t\treturn Type::DOUBLE;\n+\t\tparquet_type = Type::DOUBLE;\n+\t\tbreak;\n \tcase LogicalTypeId::ENUM:\n \tcase LogicalTypeId::BLOB:\n \tcase LogicalTypeId::VARCHAR:\n-\tcase LogicalTypeId::BIT:\n-\t\treturn Type::BYTE_ARRAY;\n+\t\tparquet_type = Type::BYTE_ARRAY;\n+\t\tbreak;\n \tcase LogicalTypeId::TIME:\n \tcase LogicalTypeId::TIME_TZ:\n \tcase LogicalTypeId::TIMESTAMP:\n@@ -104,31 +109,80 @@ Type::type ParquetWriter::DuckDBTypeToParquetType(const LogicalType &duckdb_type\n \tcase LogicalTypeId::TIMESTAMP_MS:\n \tcase LogicalTypeId::TIMESTAMP_NS:\n \tcase LogicalTypeId::TIMESTAMP_SEC:\n-\t\treturn Type::INT64;\n+\t\tparquet_type = Type::INT64;\n+\t\tbreak;\n \tcase LogicalTypeId::UTINYINT:\n \tcase LogicalTypeId::USMALLINT:\n \tcase LogicalTypeId::UINTEGER:\n-\t\treturn Type::INT32;\n+\t\tparquet_type = Type::INT32;\n+\t\tbreak;\n \tcase LogicalTypeId::UBIGINT:\n-\t\treturn Type::INT64;\n+\t\tparquet_type = Type::INT64;\n+\t\tbreak;\n \tcase LogicalTypeId::INTERVAL:\n \tcase LogicalTypeId::UUID:\n-\t\treturn Type::FIXED_LEN_BYTE_ARRAY;\n+\t\tparquet_type = Type::FIXED_LEN_BYTE_ARRAY;\n+\t\tbreak;\n \tcase LogicalTypeId::DECIMAL:\n \t\tswitch (duckdb_type.InternalType()) {\n \t\tcase PhysicalType::INT16:\n \t\tcase PhysicalType::INT32:\n-\t\t\treturn Type::INT32;\n+\t\t\tparquet_type = Type::INT32;\n+\t\t\tbreak;\n \t\tcase PhysicalType::INT64:\n-\t\t\treturn Type::INT64;\n+\t\t\tparquet_type = Type::INT64;\n+\t\t\tbreak;\n \t\tcase PhysicalType::INT128:\n-\t\t\treturn Type::FIXED_LEN_BYTE_ARRAY;\n+\t\t\tparquet_type = Type::FIXED_LEN_BYTE_ARRAY;\n+\t\t\tbreak;\n \t\tdefault:\n \t\t\tthrow InternalException(\"Unsupported internal decimal type\");\n \t\t}\n+\t\tbreak;\n \tdefault:\n+\t\t// Anything that is not supported returns false\n+\t\treturn false;\n+\t}\n+\treturn true;\n+}\n+\n+Type::type ParquetWriter::DuckDBTypeToParquetType(const LogicalType &duckdb_type) {\n+\tType::type result;\n+\tif (!DuckDBTypeToParquetTypeInternal(duckdb_type, result)) {\n \t\tthrow NotImplementedException(\"Unimplemented type for Parquet \\\"%s\\\"\", duckdb_type.ToString());\n \t}\n+\treturn result;\n+}\n+\n+bool ParquetWriter::TypeIsSupported(const LogicalType &type) {\n+\tType::type unused;\n+\tauto id = type.id();\n+\tif (id == LogicalTypeId::LIST) {\n+\t\tauto &child_type = ListType::GetChildType(type);\n+\t\treturn TypeIsSupported(child_type);\n+\t}\n+\tif (id == LogicalTypeId::STRUCT) {\n+\t\tauto &children = StructType::GetChildTypes(type);\n+\t\tfor (auto &child : children) {\n+\t\t\tauto &child_type = child.second;\n+\t\t\tif (!TypeIsSupported(child_type)) {\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\tif (id == LogicalTypeId::MAP) {\n+\t\tauto &key_type = MapType::KeyType(type);\n+\t\tauto &value_type = MapType::ValueType(type);\n+\t\tif (!TypeIsSupported(key_type)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (!TypeIsSupported(value_type)) {\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn true;\n+\t}\n+\treturn DuckDBTypeToParquetTypeInternal(type, unused);\n }\n \n void ParquetWriter::SetSchemaProperties(const LogicalType &duckdb_type,\ndiff --git a/src/include/duckdb/function/copy_function.hpp b/src/include/duckdb/function/copy_function.hpp\nindex 3588724ae45c..19268e574d0b 100644\n--- a/src/include/duckdb/function/copy_function.hpp\n+++ b/src/include/duckdb/function/copy_function.hpp\n@@ -99,13 +99,16 @@ typedef void (*copy_flush_batch_t)(ClientContext &context, FunctionData &bind_da\n                                    PreparedBatchData &batch);\n typedef idx_t (*copy_desired_batch_size_t)(ClientContext &context, FunctionData &bind_data);\n \n+typedef bool (*copy_supports_type_t)(const LogicalType &type);\n+\n class CopyFunction : public Function {\n public:\n \texplicit CopyFunction(string name)\n \t    : Function(name), plan(nullptr), copy_to_bind(nullptr), copy_to_initialize_local(nullptr),\n \t      copy_to_initialize_global(nullptr), copy_to_sink(nullptr), copy_to_combine(nullptr),\n \t      copy_to_finalize(nullptr), execution_mode(nullptr), prepare_batch(nullptr), flush_batch(nullptr),\n-\t      desired_batch_size(nullptr), serialize(nullptr), deserialize(nullptr), copy_from_bind(nullptr) {\n+\t      desired_batch_size(nullptr), serialize(nullptr), deserialize(nullptr), supports_type(nullptr),\n+\t      copy_from_bind(nullptr) {\n \t}\n \n \t//! Plan rewrite copy function\n@@ -126,6 +129,8 @@ class CopyFunction : public Function {\n \tcopy_to_serialize_t serialize;\n \tcopy_to_deserialize_t deserialize;\n \n+\tcopy_supports_type_t supports_type;\n+\n \tcopy_from_bind_t copy_from_bind;\n \tTableFunction copy_from_function;\n \ndiff --git a/src/planner/binder/statement/bind_export.cpp b/src/planner/binder/statement/bind_export.cpp\nindex 1963ac77099e..eb466f2df92a 100644\n--- a/src/planner/binder/statement/bind_export.cpp\n+++ b/src/planner/binder/statement/bind_export.cpp\n@@ -11,6 +11,9 @@\n #include \"duckdb/planner/operator/logical_set_operation.hpp\"\n #include \"duckdb/parser/parsed_data/exported_table_data.hpp\"\n #include \"duckdb/parser/constraints/foreign_key_constraint.hpp\"\n+#include \"duckdb/parser/expression/cast_expression.hpp\"\n+#include \"duckdb/parser/tableref/basetableref.hpp\"\n+#include \"duckdb/parser/query_node/select_node.hpp\"\n \n #include \"duckdb/common/string_util.hpp\"\n #include <algorithm>\n@@ -96,6 +99,18 @@ string CreateFileName(const string &id_suffix, TableCatalogEntry &table, const s\n \treturn StringUtil::Format(\"%s_%s%s.%s\", schema, name, id_suffix, extension);\n }\n \n+unique_ptr<QueryNode> CreateSelectStatement(CopyStatement &stmt, vector<unique_ptr<ParsedExpression>> select_list) {\n+\tauto ref = make_uniq<BaseTableRef>();\n+\tref->catalog_name = stmt.info->catalog;\n+\tref->schema_name = stmt.info->schema;\n+\tref->table_name = stmt.info->table;\n+\n+\tauto statement = make_uniq<SelectNode>();\n+\tstatement->from_table = std::move(ref);\n+\tstatement->select_list = std::move(select_list);\n+\treturn std::move(statement);\n+}\n+\n BoundStatement Binder::Bind(ExportStatement &stmt) {\n \t// COPY TO a file\n \tauto &config = DBConfig::GetConfig(context);\n@@ -163,7 +178,15 @@ BoundStatement Binder::Bind(ExportStatement &stmt) {\n \t\tinfo->table = table.name;\n \n \t\t// We can not export generated columns\n+\t\tvector<unique_ptr<ParsedExpression>> expressions;\n \t\tfor (auto &col : table.GetColumns().Physical()) {\n+\t\t\tauto expression = make_uniq_base<ParsedExpression, ColumnRefExpression>(col.GetName());\n+\t\t\tauto is_supported = copy_function.function.supports_type;\n+\t\t\tif (is_supported && !is_supported(col.Type())) {\n+\t\t\t\texpression =\n+\t\t\t\t    make_uniq_base<ParsedExpression, CastExpression>(LogicalType::VARCHAR, std::move(expression));\n+\t\t\t}\n+\t\t\texpressions.push_back(std::move(expression));\n \t\t\tinfo->select_list.push_back(col.GetName());\n \t\t}\n \n@@ -181,17 +204,19 @@ BoundStatement Binder::Bind(ExportStatement &stmt) {\n \t\t// generate the copy statement and bind it\n \t\tCopyStatement copy_stmt;\n \t\tcopy_stmt.info = std::move(info);\n+\t\tcopy_stmt.select_statement = CreateSelectStatement(copy_stmt, std::move(expressions));\n \n \t\tauto copy_binder = Binder::CreateBinder(context, this);\n \t\tauto bound_statement = copy_binder->Bind(copy_stmt);\n+\t\tauto plan = std::move(bound_statement.plan);\n+\n \t\tif (child_operator) {\n \t\t\t// use UNION ALL to combine the individual copy statements into a single node\n-\t\t\tauto copy_union =\n-\t\t\t    make_uniq<LogicalSetOperation>(GenerateTableIndex(), 1, std::move(child_operator),\n-\t\t\t                                   std::move(bound_statement.plan), LogicalOperatorType::LOGICAL_UNION);\n+\t\t\tauto copy_union = make_uniq<LogicalSetOperation>(GenerateTableIndex(), 1, std::move(child_operator),\n+\t\t\t                                                 std::move(plan), LogicalOperatorType::LOGICAL_UNION);\n \t\t\tchild_operator = std::move(copy_union);\n \t\t} else {\n-\t\t\tchild_operator = std::move(bound_statement.plan);\n+\t\t\tchild_operator = std::move(plan);\n \t\t}\n \t}\n \n",
  "test_patch": "diff --git a/test/api/test_relation_api.cpp b/test/api/test_relation_api.cpp\nindex 945ff6d9df36..fa2fd5505b5a 100644\n--- a/test/api/test_relation_api.cpp\n+++ b/test/api/test_relation_api.cpp\n@@ -623,6 +623,9 @@ TEST_CASE(\"Test aggregates in relation API\", \"[relation_api]\") {\n \t// when using explicit groups, we cannot have non-explicit groups\n \tREQUIRE_THROWS(tbl->Aggregate(\"j, i+SUM(j)\", \"i\")->Order(\"1\")->Execute());\n \n+\t// Coverage: Groups expressions can not create multiple statements\n+\tREQUIRE_THROWS(tbl->Aggregate(\"i\", \"i; select 42\")->Execute());\n+\n \t// project -> aggregate -> project -> aggregate\n \t// SUM(j) = 18 -> 18 + 1 = 19 -> 19 * 2 = 38\n \tresult = tbl->Aggregate(\"SUM(j) AS k\")->Project(\"k+1 AS l\")->Aggregate(\"SUM(l) AS m\")->Project(\"m*2\")->Execute();\ndiff --git a/test/sql/export/parquet/export_parquet_bit.test b/test/sql/export/parquet/export_parquet_bit.test\nnew file mode 100644\nindex 000000000000..91b56c860254\n--- /dev/null\n+++ b/test/sql/export/parquet/export_parquet_bit.test\n@@ -0,0 +1,29 @@\n+# name: test/sql/export/parquet/export_parquet_bit.test\n+# description: Test EXPORT DATABASE with BIT columns\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+begin transaction;\n+\n+statement ok\n+create table tbl as select \"bit\" from test_all_types(), range(3);\n+\n+query I nosort result\n+select * from tbl;\n+----\n+\n+# now export the db\n+statement ok\n+EXPORT DATABASE '__TEST_DIR__/export_test' (FORMAT PARQUET)\n+\n+statement ok\n+ROLLBACK\n+\n+statement ok\n+IMPORT DATABASE '__TEST_DIR__/export_test'\n+\n+query I nosort result\n+select * from tbl;\n+----\ndiff --git a/test/sql/export/parquet/export_parquet_json.test b/test/sql/export/parquet/export_parquet_json.test\nnew file mode 100644\nindex 000000000000..6347d3e52738\n--- /dev/null\n+++ b/test/sql/export/parquet/export_parquet_json.test\n@@ -0,0 +1,31 @@\n+# name: test/sql/export/parquet/export_parquet_json.test\n+# description: Test EXPORT DATABASE with JSON columns\n+# group: [parquet]\n+\n+require parquet\n+\n+require json\n+\n+statement ok\n+begin transaction;\n+\n+statement ok\n+create table tbl as select val from ( select json_structure('{\"a\": 42}') val), range(3);\n+\n+query I nosort result\n+select * from tbl;\n+----\n+\n+# now export the db\n+statement ok\n+EXPORT DATABASE '__TEST_DIR__/export_test' (FORMAT PARQUET)\n+\n+statement ok\n+ROLLBACK\n+\n+statement ok\n+IMPORT DATABASE '__TEST_DIR__/export_test'\n+\n+query I nosort result\n+select * from tbl;\n+----\ndiff --git a/test/sql/export/parquet/export_parquet_list.test b/test/sql/export/parquet/export_parquet_list.test\nnew file mode 100644\nindex 000000000000..772870a5976d\n--- /dev/null\n+++ b/test/sql/export/parquet/export_parquet_list.test\n@@ -0,0 +1,31 @@\n+# name: test/sql/export/parquet/export_parquet_list.test\n+# description: Test EXPORT DATABASE with LIST columns\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+begin transaction;\n+\n+statement ok\n+create table tbl as select val from (\n+\tselect ['01010101'::BIT, '01011101001'::BIT] val\n+), range(3);\n+\n+query I nosort result\n+select * from tbl;\n+----\n+\n+# now export the db\n+statement ok\n+EXPORT DATABASE '__TEST_DIR__/export_test' (FORMAT PARQUET)\n+\n+statement ok\n+ROLLBACK\n+\n+statement ok\n+IMPORT DATABASE '__TEST_DIR__/export_test'\n+\n+query I nosort result\n+select * from tbl;\n+----\ndiff --git a/test/sql/export/parquet/export_parquet_map.test b/test/sql/export/parquet/export_parquet_map.test\nnew file mode 100644\nindex 000000000000..ac849901bb1b\n--- /dev/null\n+++ b/test/sql/export/parquet/export_parquet_map.test\n@@ -0,0 +1,34 @@\n+# name: test/sql/export/parquet/export_parquet_map.test\n+# description: Test EXPORT DATABASE with MAP columns\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+begin transaction;\n+\n+statement ok\n+create table tbl as select val from (\n+\tselect MAP {\n+\t\t'hello': '01010101000'::BIT,\n+\t\t'HELLO': NULL::BIT\n+\t} val\n+), range(3);\n+\n+query I nosort result\n+select * from tbl;\n+----\n+\n+# now export the db\n+statement ok\n+EXPORT DATABASE '__TEST_DIR__/export_test' (FORMAT PARQUET)\n+\n+statement ok\n+ROLLBACK\n+\n+statement ok\n+IMPORT DATABASE '__TEST_DIR__/export_test'\n+\n+query I nosort result\n+select * from tbl;\n+----\ndiff --git a/test/sql/export/parquet/export_parquet_struct.test b/test/sql/export/parquet/export_parquet_struct.test\nnew file mode 100644\nindex 000000000000..a68bcfa3671e\n--- /dev/null\n+++ b/test/sql/export/parquet/export_parquet_struct.test\n@@ -0,0 +1,35 @@\n+# name: test/sql/export/parquet/export_parquet_struct.test\n+# description: Test EXPORT DATABASE with MAP columns\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+begin transaction;\n+\n+statement ok\n+create table tbl as select val from (\n+\tselect {\n+\t\t'a': '01010101000'::BIT,\n+\t\t'b': true,\n+\t\t'c': NULL\n+\t} val\n+), range(3);\n+\n+query I nosort result\n+select * from tbl;\n+----\n+\n+# now export the db\n+statement ok\n+EXPORT DATABASE '__TEST_DIR__/export_test' (FORMAT PARQUET)\n+\n+statement ok\n+ROLLBACK\n+\n+statement ok\n+IMPORT DATABASE '__TEST_DIR__/export_test'\n+\n+query I nosort result\n+select * from tbl;\n+----\ndiff --git a/test/sql/export/parquet/export_parquet_union.test b/test/sql/export/parquet/export_parquet_union.test\nnew file mode 100644\nindex 000000000000..c35c3acc157a\n--- /dev/null\n+++ b/test/sql/export/parquet/export_parquet_union.test\n@@ -0,0 +1,29 @@\n+# name: test/sql/export/parquet/export_parquet_union.test\n+# description: Test EXPORT DATABASE with UNION columns\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+begin transaction;\n+\n+statement ok\n+create table tbl as select \"union\" from test_all_types(), range(3);\n+\n+query I nosort result\n+select * from tbl;\n+----\n+\n+# now export the db\n+statement ok\n+EXPORT DATABASE '__TEST_DIR__/export_test' (FORMAT PARQUET)\n+\n+statement ok\n+ROLLBACK\n+\n+statement ok\n+IMPORT DATABASE '__TEST_DIR__/export_test'\n+\n+query I nosort result\n+select * from tbl;\n+----\n",
  "problem_statement": "EXPORT DATABASE as Parquet does not work with all types\nWhen using `FORMAT parquet` EXPORT currently uses the underlying Parquet writers directly, which doesn't support all database types:\r\n\r\n```sql\r\nCREATE TABLE all_types AS FROM test_all_types();\r\nEXPORT DATABASE 'all_types_export' (FORMAT parquet);\r\n-- Error: INTERNAL Error: Unsupported type \"BIT\" in Parquet writer\r\n```\r\n\r\nThis could be made to work by, in the `EXPORT`, converting unsupported types to `VARCHAR` instead.\r\n\r\nCC @bleskes \r\n\n",
  "hints_text": "",
  "created_at": "2023-08-29T06:16:45Z"
}