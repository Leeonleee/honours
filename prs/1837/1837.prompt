You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Deleting with DELETE USING causes a segmentation fault
**What does happen?**
I am performing deletes with the `DELETE ... USING ...` (#1659) command. In some cases, it returns with a segmentation fault.

I have also seen a few `Conflict on tuple deletion!` but the root cause of the two seems to be the same (sometimes, running the same script multiple times returns either a segfault or a conflict).

**What should happen?**
The system should perform the delete operation on the tuples matching the condition.

**To Reproduce**
1. Grab the archive containing the data and the script  from https://surfdrive.surf.nl/files/index.php/s/VA92agRErNV2EJl
2. Run the SQL script:
   ```bash
   cat delete-segfault-mwe.sql | sed "s#PATHVAR#`pwd`/data#" | duckdb
   ```

    It returns a `Segmentation fault (core dumped)`

**Environment (please complete the following information):**
 - OS: Fedora 34
 - DuckDB Version: 0.2.6, master (979f14ed48b865e0f775b2898640757b6302f4e7)

The content of the SQL script is the following:
```sql
CREATE TABLE Person_likes_Comment (creationDate timestamp without time zone not null, id bigint not null, likes_Comment bigint not null);
CREATE TABLE Person_Delete_candidates (deletionDate timestamp without time zone not null, id bigint);

COPY Person_likes_Comment FROM 'PATHVAR/Person_likes_Comment.csv' (DELIMITER '|', TIMESTAMPFORMAT '%Y-%m-%dT%H:%M:%S.%g+00:00');
COPY Person_Delete_candidates FROM 'PATHVAR/Person_Delete_candidates.csv' (DELIMITER '|', HEADER, TIMESTAMPFORMAT '%Y-%m-%dT%H:%M:%S.%g+00:00');

DELETE FROM Person_likes_Comment USING Person_Delete_candidates WHERE Person_Delete_candidates.id = Person_likes_Comment.id;
```

The following query returns the tuples that should be deleted from the `Person_likes_Comment` (showing that there are no duplicates):
```sql
SELECT Person_likes_comment.* FROM Person_likes_Comment, Person_Delete_candidates WHERE Person_Delete_candidates.id = Person_likes_Comment.id;
```


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of src/include/duckdb/storage/table/row_group.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/row_group.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/vector_size.hpp"
12: #include "duckdb/storage/table/segment_base.hpp"
13: #include "duckdb/storage/table/chunk_info.hpp"
14: #include "duckdb/storage/table/append_state.hpp"
15: #include "duckdb/storage/table/scan_state.hpp"
16: #include "duckdb/storage/statistics/segment_statistics.hpp"
17: #include "duckdb/common/mutex.hpp"
18: 
19: namespace duckdb {
20: class ColumnData;
21: class DatabaseInstance;
22: class DataTable;
23: struct DataTableInfo;
24: class ExpressionExecutor;
25: class TableDataWriter;
26: class UpdateSegment;
27: class Vector;
28: struct RowGroupPointer;
29: struct VersionNode;
30: 
31: class RowGroup : public SegmentBase {
32: public:
33: 	friend class ColumnData;
34: 	friend class VersionDeleteState;
35: 
36: public:
37: 	static constexpr const idx_t ROW_GROUP_VECTOR_COUNT = 120;
38: 	static constexpr const idx_t ROW_GROUP_SIZE = STANDARD_VECTOR_SIZE * ROW_GROUP_VECTOR_COUNT;
39: 
40: public:
41: 	RowGroup(DatabaseInstance &db, DataTableInfo &table_info, idx_t start, idx_t count);
42: 	RowGroup(DatabaseInstance &db, DataTableInfo &table_info, const vector<LogicalType> &types,
43: 	         RowGroupPointer &pointer);
44: 	~RowGroup();
45: 
46: private:
47: 	//! The database instance
48: 	DatabaseInstance &db;
49: 	//! The table info of this row_group
50: 	DataTableInfo &table_info;
51: 	//! The version info of the row_group (inserted and deleted tuple info)
52: 	shared_ptr<VersionNode> version_info;
53: 	//! The column data of the row_group
54: 	vector<shared_ptr<ColumnData>> columns;
55: 	//! The segment statistics for each of the columns
56: 	vector<shared_ptr<SegmentStatistics>> stats;
57: 
58: public:
59: 	DatabaseInstance &GetDatabase() {
60: 		return db;
61: 	}
62: 	DataTableInfo &GetTableInfo() {
63: 		return table_info;
64: 	}
65: 	idx_t GetColumnIndex(ColumnData *data) {
66: 		for (idx_t i = 0; i < columns.size(); i++) {
67: 			if (columns[i].get() == data) {
68: 				return i;
69: 			}
70: 		}
71: 		return 0;
72: 	}
73: 
74: 	unique_ptr<RowGroup> AlterType(ClientContext &context, const LogicalType &target_type, idx_t changed_idx,
75: 	                               ExpressionExecutor &executor, TableScanState &scan_state, DataChunk &scan_chunk);
76: 	unique_ptr<RowGroup> AddColumn(ClientContext &context, ColumnDefinition &new_column, ExpressionExecutor &executor,
77: 	                               Expression *default_value, Vector &intermediate);
78: 	unique_ptr<RowGroup> RemoveColumn(idx_t removed_column);
79: 
80: 	void CommitDrop();
81: 	void CommitDropColumn(idx_t index);
82: 
83: 	void InitializeEmpty(const vector<LogicalType> &types);
84: 
85: 	//! Initialize a scan over this row_group
86: 	bool InitializeScan(RowGroupScanState &state);
87: 	bool InitializeScanWithOffset(RowGroupScanState &state, idx_t vector_offset);
88: 	//! Checks the given set of table filters against the row-group statistics. Returns false if the entire row group
89: 	//! can be skipped.
90: 	bool CheckZonemap(TableFilterSet &filters, const vector<column_t> &column_ids);
91: 	//! Checks the given set of table filters against the per-segment statistics. Returns false if any segments were
92: 	//! skipped.
93: 	bool CheckZonemapSegments(RowGroupScanState &state);
94: 	void Scan(Transaction &transaction, RowGroupScanState &state, DataChunk &result);
95: 	void IndexScan(RowGroupScanState &state, DataChunk &result, bool allow_pending_updates);
96: 
97: 	idx_t GetSelVector(Transaction &transaction, idx_t vector_idx, SelectionVector &sel_vector, idx_t max_count);
98: 
99: 	//! For a specific row, returns true if it should be used for the transaction and false otherwise.
100: 	bool Fetch(Transaction &transaction, idx_t row);
101: 	//! Fetch a specific row from the row_group and insert it into the result at the specified index
102: 	void FetchRow(Transaction &transaction, ColumnFetchState &state, const vector<column_t> &column_ids, row_t row_id,
103: 	              DataChunk &result, idx_t result_idx);
104: 
105: 	//! Append count rows to the version info
106: 	void AppendVersionInfo(Transaction &transaction, idx_t start, idx_t count, transaction_t commit_id);
107: 	//! Commit a previous append made by RowGroup::AppendVersionInfo
108: 	void CommitAppend(transaction_t commit_id, idx_t start, idx_t count);
109: 	//! Revert a previous append made by RowGroup::AppendVersionInfo
110: 	void RevertAppend(idx_t start);
111: 
112: 	//! Delete the given set of rows in the version manager
113: 	void Delete(Transaction &transaction, DataTable *table, Vector &row_ids, idx_t count);
114: 
115: 	RowGroupPointer Checkpoint(TableDataWriter &writer, vector<unique_ptr<BaseStatistics>> &global_stats);
116: 	static void Serialize(RowGroupPointer &pointer, Serializer &serializer);
117: 	static RowGroupPointer Deserialize(Deserializer &source, const vector<ColumnDefinition> &columns);
118: 
119: 	void InitializeAppend(Transaction &transaction, RowGroupAppendState &append_state, idx_t remaining_append_count);
120: 	void Append(RowGroupAppendState &append_state, DataChunk &chunk, idx_t append_count);
121: 
122: 	void Update(Transaction &transaction, DataChunk &updates, Vector &row_ids, const vector<column_t> &column_ids);
123: 	//! Update a single column; corresponds to DataTable::UpdateColumn
124: 	//! This method should only be called from the WAL
125: 	void UpdateColumn(Transaction &transaction, DataChunk &updates, Vector &row_ids,
126: 	                  const vector<column_t> &column_path);
127: 
128: 	void MergeStatistics(idx_t column_idx, BaseStatistics &other);
129: 	unique_ptr<BaseStatistics> GetStatistics(idx_t column_idx);
130: 
131: 	void GetStorageInfo(idx_t row_group_index, vector<vector<Value>> &result);
132: 
133: 	void Verify();
134: 
135: private:
136: 	ChunkInfo *GetChunkInfo(idx_t vector_idx);
137: 
138: 	template <bool SCAN_DELETES, bool SCAN_COMMITTED, bool ALLOW_UPDATES>
139: 	void TemplatedScan(Transaction *transaction, RowGroupScanState &state, DataChunk &result);
140: 
141: 	static void CheckpointDeletes(VersionNode *versions, Serializer &serializer);
142: 	static shared_ptr<VersionNode> DeserializeDeletes(Deserializer &source);
143: 
144: private:
145: 	mutex row_group_lock;
146: 	mutex stats_lock;
147: };
148: 
149: struct VersionNode {
150: 	unique_ptr<ChunkInfo> info[RowGroup::ROW_GROUP_VECTOR_COUNT];
151: };
152: 
153: } // namespace duckdb
[end of src/include/duckdb/storage/table/row_group.hpp]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/common/helper.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/planner/constraints/list.hpp"
10: #include "duckdb/planner/table_filter.hpp"
11: #include "duckdb/storage/storage_manager.hpp"
12: #include "duckdb/storage/table/row_group.hpp"
13: #include "duckdb/storage/table/persistent_table_data.hpp"
14: #include "duckdb/storage/table/transient_segment.hpp"
15: #include "duckdb/transaction/transaction.hpp"
16: #include "duckdb/transaction/transaction_manager.hpp"
17: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
18: #include "duckdb/storage/table/standard_column_data.hpp"
19: 
20: #include "duckdb/common/chrono.hpp"
21: 
22: namespace duckdb {
23: 
24: DataTable::DataTable(DatabaseInstance &db, const string &schema, const string &table, vector<LogicalType> types_p,
25:                      unique_ptr<PersistentTableData> data)
26:     : info(make_shared<DataTableInfo>(db, schema, table)), types(move(types_p)), db(db), total_rows(0), is_root(true) {
27: 	// initialize the table with the existing data from disk, if any
28: 	this->row_groups = make_shared<SegmentTree>();
29: 	if (data && !data->row_groups.empty()) {
30: 		for (auto &row_group_pointer : data->row_groups) {
31: 			auto new_row_group = make_unique<RowGroup>(db, *info, types, row_group_pointer);
32: 			auto row_group_count = new_row_group->start + new_row_group->count;
33: 			if (row_group_count > total_rows) {
34: 				total_rows = row_group_count;
35: 			}
36: 			row_groups->AppendSegment(move(new_row_group));
37: 		}
38: 		column_stats = move(data->column_stats);
39: 		if (column_stats.size() != types.size()) {
40: 			throw IOException("Table statistics column count is not aligned with table column count. Corrupt file?");
41: 		}
42: 	}
43: 	if (column_stats.empty()) {
44: 		D_ASSERT(total_rows == 0);
45: 
46: 		AppendRowGroup(0);
47: 		for (auto &type : types) {
48: 			column_stats.push_back(BaseStatistics::CreateEmpty(type));
49: 		}
50: 	} else {
51: 		D_ASSERT(column_stats.size() == types.size());
52: 		D_ASSERT(row_groups->GetRootSegment() != nullptr);
53: 	}
54: }
55: 
56: void DataTable::AppendRowGroup(idx_t start_row) {
57: 	auto new_row_group = make_unique<RowGroup>(db, *info, start_row, 0);
58: 	new_row_group->InitializeEmpty(types);
59: 	row_groups->AppendSegment(move(new_row_group));
60: }
61: 
62: DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value)
63:     : info(parent.info), types(parent.types), db(parent.db), total_rows(parent.total_rows.load()), is_root(true) {
64: 	// prevent any new tuples from being added to the parent
65: 	lock_guard<mutex> parent_lock(parent.append_lock);
66: 	// add the new column to this DataTable
67: 	auto new_column_type = new_column.type;
68: 	auto new_column_idx = parent.types.size();
69: 
70: 	types.push_back(new_column_type);
71: 
72: 	// set up the statistics
73: 	for (idx_t i = 0; i < parent.column_stats.size(); i++) {
74: 		column_stats.push_back(parent.column_stats[i]->Copy());
75: 	}
76: 	column_stats.push_back(BaseStatistics::CreateEmpty(new_column_type));
77: 
78: 	auto &transaction = Transaction::GetTransaction(context);
79: 
80: 	ExpressionExecutor executor;
81: 	DataChunk dummy_chunk;
82: 	Vector result(new_column_type);
83: 	if (!default_value) {
84: 		FlatVector::Validity(result).SetAllInvalid(STANDARD_VECTOR_SIZE);
85: 	} else {
86: 		executor.AddExpression(*default_value);
87: 	}
88: 
89: 	// fill the column with its DEFAULT value, or NULL if none is specified
90: 	auto new_stats = make_unique<SegmentStatistics>(new_column.type);
91: 	this->row_groups = make_shared<SegmentTree>();
92: 	auto current_row_group = (RowGroup *)parent.row_groups->GetRootSegment();
93: 	while (current_row_group) {
94: 		auto new_row_group = current_row_group->AddColumn(context, new_column, executor, default_value, result);
95: 		// merge in the statistics
96: 		column_stats[new_column_idx]->Merge(*new_row_group->GetStatistics(new_column_idx));
97: 
98: 		row_groups->AppendSegment(move(new_row_group));
99: 		current_row_group = (RowGroup *)current_row_group->next.get();
100: 	}
101: 
102: 	// also add this column to client local storage
103: 	transaction.storage.AddColumn(&parent, this, new_column, default_value);
104: 
105: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
106: 	parent.is_root = false;
107: }
108: 
109: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
110:     : info(parent.info), types(parent.types), db(parent.db), total_rows(parent.total_rows.load()), is_root(true) {
111: 	// prevent any new tuples from being added to the parent
112: 	lock_guard<mutex> parent_lock(parent.append_lock);
113: 	// first check if there are any indexes that exist that point to the removed column
114: 	info->indexes.Scan([&](Index &index) {
115: 		for (auto &column_id : index.column_ids) {
116: 			if (column_id == removed_column) {
117: 				throw CatalogException("Cannot drop this column: an index depends on it!");
118: 			} else if (column_id > removed_column) {
119: 				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
120: 			}
121: 		}
122: 		return false;
123: 	});
124: 
125: 	// erase the stats and type from this DataTable
126: 	D_ASSERT(removed_column < types.size());
127: 	types.erase(types.begin() + removed_column);
128: 	for (idx_t i = 0; i < parent.column_stats.size(); i++) {
129: 		if (i != removed_column) {
130: 			column_stats.push_back(parent.column_stats[i]->Copy());
131: 		}
132: 	}
133: 
134: 	// alter the row_groups and remove the column from each of them
135: 	this->row_groups = make_shared<SegmentTree>();
136: 	auto current_row_group = (RowGroup *)parent.row_groups->GetRootSegment();
137: 	while (current_row_group) {
138: 		auto new_row_group = current_row_group->RemoveColumn(removed_column);
139: 		row_groups->AppendSegment(move(new_row_group));
140: 		current_row_group = (RowGroup *)current_row_group->next.get();
141: 	}
142: 
143: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
144: 	parent.is_root = false;
145: }
146: 
147: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, const LogicalType &target_type,
148:                      vector<column_t> bound_columns, Expression &cast_expr)
149:     : info(parent.info), types(parent.types), db(parent.db), total_rows(parent.total_rows.load()), is_root(true) {
150: 	// prevent any tuples from being added to the parent
151: 	lock_guard<mutex> lock(append_lock);
152: 
153: 	// first check if there are any indexes that exist that point to the changed column
154: 	info->indexes.Scan([&](Index &index) {
155: 		for (auto &column_id : index.column_ids) {
156: 			if (column_id == changed_idx) {
157: 				throw CatalogException("Cannot change the type of this column: an index depends on it!");
158: 			}
159: 		}
160: 		return false;
161: 	});
162: 
163: 	// change the type in this DataTable
164: 	types[changed_idx] = target_type;
165: 
166: 	// set up the statistics for the table
167: 	// the column that had its type changed will have the new statistics computed during conversion
168: 	for (idx_t i = 0; i < types.size(); i++) {
169: 		if (i == changed_idx) {
170: 			column_stats.push_back(BaseStatistics::CreateEmpty(types[i]));
171: 		} else {
172: 			column_stats.push_back(parent.column_stats[i]->Copy());
173: 		}
174: 	}
175: 
176: 	// scan the original table, and fill the new column with the transformed value
177: 	auto &transaction = Transaction::GetTransaction(context);
178: 
179: 	vector<LogicalType> scan_types;
180: 	for (idx_t i = 0; i < bound_columns.size(); i++) {
181: 		if (bound_columns[i] == COLUMN_IDENTIFIER_ROW_ID) {
182: 			scan_types.push_back(LOGICAL_ROW_TYPE);
183: 		} else {
184: 			scan_types.push_back(parent.types[bound_columns[i]]);
185: 		}
186: 	}
187: 	DataChunk scan_chunk;
188: 	scan_chunk.Initialize(scan_types);
189: 
190: 	ExpressionExecutor executor;
191: 	executor.AddExpression(cast_expr);
192: 
193: 	TableScanState scan_state;
194: 	scan_state.column_ids = bound_columns;
195: 	scan_state.max_row = total_rows;
196: 
197: 	// now alter the type of the column within all of the row_groups individually
198: 	this->row_groups = make_shared<SegmentTree>();
199: 	auto current_row_group = (RowGroup *)parent.row_groups->GetRootSegment();
200: 	while (current_row_group) {
201: 		auto new_row_group =
202: 		    current_row_group->AlterType(context, target_type, changed_idx, executor, scan_state, scan_chunk);
203: 		column_stats[changed_idx]->Merge(*new_row_group->GetStatistics(changed_idx));
204: 		row_groups->AppendSegment(move(new_row_group));
205: 		current_row_group = (RowGroup *)current_row_group->next.get();
206: 	}
207: 
208: 	transaction.storage.ChangeType(&parent, this, changed_idx, target_type, bound_columns, cast_expr);
209: 
210: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
211: 	parent.is_root = false;
212: }
213: 
214: //===--------------------------------------------------------------------===//
215: // Scan
216: //===--------------------------------------------------------------------===//
217: void DataTable::InitializeScan(TableScanState &state, const vector<column_t> &column_ids,
218:                                TableFilterSet *table_filters) {
219: 	// initialize a column scan state for each column
220: 	// initialize the chunk scan state
221: 	auto row_group = (RowGroup *)row_groups->GetRootSegment();
222: 	state.column_ids = column_ids;
223: 	state.max_row = total_rows;
224: 	state.table_filters = table_filters;
225: 	if (table_filters) {
226: 		D_ASSERT(table_filters->filters.size() > 0);
227: 		state.adaptive_filter = make_unique<AdaptiveFilter>(table_filters);
228: 	}
229: 	while (row_group && !row_group->InitializeScan(state.row_group_scan_state)) {
230: 		row_group = (RowGroup *)row_group->next.get();
231: 	}
232: }
233: 
234: void DataTable::InitializeScan(Transaction &transaction, TableScanState &state, const vector<column_t> &column_ids,
235:                                TableFilterSet *table_filters) {
236: 	InitializeScan(state, column_ids, table_filters);
237: 	transaction.storage.InitializeScan(this, state.local_state, table_filters);
238: }
239: 
240: void DataTable::InitializeScanWithOffset(TableScanState &state, const vector<column_t> &column_ids, idx_t start_row,
241:                                          idx_t end_row) {
242: 
243: 	auto row_group = (RowGroup *)row_groups->GetSegment(start_row);
244: 	state.column_ids = column_ids;
245: 	state.max_row = end_row;
246: 	state.table_filters = nullptr;
247: 	idx_t start_vector = (start_row - row_group->start) / STANDARD_VECTOR_SIZE;
248: 	if (!row_group->InitializeScanWithOffset(state.row_group_scan_state, start_vector)) {
249: 		throw InternalException("Failed to initialize row group scan with offset");
250: 	}
251: }
252: 
253: bool DataTable::InitializeScanInRowGroup(TableScanState &state, const vector<column_t> &column_ids,
254:                                          TableFilterSet *table_filters, RowGroup *row_group, idx_t vector_index,
255:                                          idx_t max_row) {
256: 	state.column_ids = column_ids;
257: 	state.max_row = max_row;
258: 	state.table_filters = table_filters;
259: 	if (table_filters) {
260: 		D_ASSERT(table_filters->filters.size() > 0);
261: 		state.adaptive_filter = make_unique<AdaptiveFilter>(table_filters);
262: 	}
263: 	return row_group->InitializeScanWithOffset(state.row_group_scan_state, vector_index);
264: }
265: 
266: idx_t DataTable::MaxThreads(ClientContext &context) {
267: 	idx_t parallel_scan_vector_count = RowGroup::ROW_GROUP_VECTOR_COUNT;
268: 	if (context.force_parallelism) {
269: 		parallel_scan_vector_count = 1;
270: 	}
271: 	idx_t parallel_scan_tuple_count = STANDARD_VECTOR_SIZE * parallel_scan_vector_count;
272: 
273: 	return total_rows / parallel_scan_tuple_count + 1;
274: }
275: 
276: void DataTable::InitializeParallelScan(ParallelTableScanState &state) {
277: 	state.current_row_group = (RowGroup *)row_groups->GetRootSegment();
278: 	state.transaction_local_data = false;
279: }
280: 
281: bool DataTable::NextParallelScan(ClientContext &context, ParallelTableScanState &state, TableScanState &scan_state,
282:                                  const vector<column_t> &column_ids) {
283: 	while (state.current_row_group) {
284: 		idx_t vector_index;
285: 		idx_t max_row;
286: 		if (context.force_parallelism) {
287: 			vector_index = state.vector_index;
288: 			max_row = state.current_row_group->start +
289: 			          MinValue<idx_t>(state.current_row_group->count,
290: 			                          STANDARD_VECTOR_SIZE * state.vector_index + STANDARD_VECTOR_SIZE);
291: 		} else {
292: 			vector_index = 0;
293: 			max_row = state.current_row_group->start + state.current_row_group->count;
294: 		}
295: 		bool need_to_scan = InitializeScanInRowGroup(scan_state, column_ids, scan_state.table_filters,
296: 		                                             state.current_row_group, vector_index, max_row);
297: 		if (context.force_parallelism) {
298: 			state.vector_index++;
299: 			if (state.vector_index * STANDARD_VECTOR_SIZE >= state.current_row_group->count) {
300: 				state.current_row_group = (RowGroup *)state.current_row_group->next.get();
301: 				state.vector_index = 0;
302: 			}
303: 		} else {
304: 			state.current_row_group = (RowGroup *)state.current_row_group->next.get();
305: 		}
306: 		if (!need_to_scan) {
307: 			// filters allow us to skip this row group: move to the next row group
308: 			continue;
309: 		}
310: 		return true;
311: 	}
312: 	if (!state.transaction_local_data) {
313: 		auto &transaction = Transaction::GetTransaction(context);
314: 		// create a task for scanning the local data
315: 		scan_state.row_group_scan_state.max_row = 0;
316: 		scan_state.max_row = 0;
317: 		transaction.storage.InitializeScan(this, scan_state.local_state, scan_state.table_filters);
318: 		state.transaction_local_data = true;
319: 		return true;
320: 	} else {
321: 		// finished all scans: no more scans remaining
322: 		return false;
323: 	}
324: }
325: 
326: void DataTable::Scan(Transaction &transaction, DataChunk &result, TableScanState &state, vector<column_t> &column_ids) {
327: 	// scan the persistent segments
328: 	if (ScanBaseTable(transaction, result, state)) {
329: 		D_ASSERT(result.size() > 0);
330: 		return;
331: 	}
332: 
333: 	// scan the transaction-local segments
334: 	transaction.storage.Scan(state.local_state, column_ids, result);
335: }
336: 
337: bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, TableScanState &state) {
338: 	auto current_row_group = state.row_group_scan_state.row_group;
339: 	while (current_row_group) {
340: 		current_row_group->Scan(transaction, state.row_group_scan_state, result);
341: 		if (result.size() > 0) {
342: 			return true;
343: 		} else {
344: 			do {
345: 				current_row_group = state.row_group_scan_state.row_group = (RowGroup *)current_row_group->next.get();
346: 				if (current_row_group) {
347: 					bool scan_row_group = current_row_group->InitializeScan(state.row_group_scan_state);
348: 					if (scan_row_group) {
349: 						// skip this row group
350: 						break;
351: 					}
352: 				}
353: 			} while (current_row_group);
354: 		}
355: 	}
356: 	return false;
357: }
358: 
359: //===--------------------------------------------------------------------===//
360: // Fetch
361: //===--------------------------------------------------------------------===//
362: void DataTable::Fetch(Transaction &transaction, DataChunk &result, const vector<column_t> &column_ids,
363:                       Vector &row_identifiers, idx_t fetch_count, ColumnFetchState &state) {
364: 	// figure out which row_group to fetch from
365: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
366: 	idx_t count = 0;
367: 	for (idx_t i = 0; i < fetch_count; i++) {
368: 		auto row_id = row_ids[i];
369: 		auto row_group = (RowGroup *)row_groups->GetSegment(row_id);
370: 		if (!row_group->Fetch(transaction, row_id - row_group->start)) {
371: 			continue;
372: 		}
373: 		row_group->FetchRow(transaction, state, column_ids, row_id, result, count);
374: 		count++;
375: 	}
376: 	result.SetCardinality(count);
377: }
378: 
379: //===--------------------------------------------------------------------===//
380: // Append
381: //===--------------------------------------------------------------------===//
382: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, string &col_name) {
383: 	if (VectorOperations::HasNull(vector, count)) {
384: 		throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name, col_name);
385: 	}
386: }
387: 
388: static void VerifyCheckConstraint(TableCatalogEntry &table, Expression &expr, DataChunk &chunk) {
389: 	ExpressionExecutor executor(expr);
390: 	Vector result(LogicalType::INTEGER);
391: 	try {
392: 		executor.ExecuteExpression(chunk, result);
393: 	} catch (Exception &ex) {
394: 		throw ConstraintException("CHECK constraint failed: %s (Error: %s)", table.name, ex.what());
395: 	} catch (...) {
396: 		throw ConstraintException("CHECK constraint failed: %s (Unknown Error)", table.name);
397: 	}
398: 	VectorData vdata;
399: 	result.Orrify(chunk.size(), vdata);
400: 
401: 	auto dataptr = (int32_t *)vdata.data;
402: 	for (idx_t i = 0; i < chunk.size(); i++) {
403: 		auto idx = vdata.sel->get_index(i);
404: 		if (vdata.validity.RowIsValid(idx) && dataptr[idx] == 0) {
405: 			throw ConstraintException("CHECK constraint failed: %s", table.name);
406: 		}
407: 	}
408: }
409: 
410: void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, DataChunk &chunk) {
411: 	for (auto &constraint : table.bound_constraints) {
412: 		switch (constraint->type) {
413: 		case ConstraintType::NOT_NULL: {
414: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
415: 			VerifyNotNullConstraint(table, chunk.data[not_null.index], chunk.size(),
416: 			                        table.columns[not_null.index].name);
417: 			break;
418: 		}
419: 		case ConstraintType::CHECK: {
420: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
421: 			VerifyCheckConstraint(table, *check.expression, chunk);
422: 			break;
423: 		}
424: 		case ConstraintType::UNIQUE: {
425: 			//! check whether or not the chunk can be inserted into the indexes
426: 			info->indexes.Scan([&](Index &index) {
427: 				index.VerifyAppend(chunk);
428: 				return false;
429: 			});
430: 			break;
431: 		}
432: 		case ConstraintType::FOREIGN_KEY:
433: 		default:
434: 			throw NotImplementedException("Constraint type not implemented!");
435: 		}
436: 	}
437: }
438: 
439: void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
440: 	if (chunk.size() == 0) {
441: 		return;
442: 	}
443: 	if (chunk.ColumnCount() != table.columns.size()) {
444: 		throw CatalogException("Mismatch in column count for append");
445: 	}
446: 	if (!is_root) {
447: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
448: 	}
449: 
450: 	chunk.Verify();
451: 
452: 	// verify any constraints on the new chunk
453: 	VerifyAppendConstraints(table, chunk);
454: 
455: 	// append to the transaction local data
456: 	auto &transaction = Transaction::GetTransaction(context);
457: 	transaction.storage.Append(this, chunk);
458: }
459: 
460: void DataTable::InitializeAppend(Transaction &transaction, TableAppendState &state, idx_t append_count) {
461: 	// obtain the append lock for this table
462: 	state.append_lock = unique_lock<mutex>(append_lock);
463: 	if (!is_root) {
464: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
465: 	}
466: 	state.row_start = total_rows;
467: 	state.current_row = state.row_start;
468: 	state.remaining_append_count = append_count;
469: 
470: 	// start writing to the row_groups
471: 	lock_guard<mutex> row_group_lock(row_groups->node_lock);
472: 	auto last_row_group = (RowGroup *)row_groups->GetLastSegment();
473: 	D_ASSERT(total_rows == last_row_group->start + last_row_group->count);
474: 	last_row_group->InitializeAppend(transaction, state.row_group_append_state, state.remaining_append_count);
475: 	total_rows += append_count;
476: }
477: 
478: void DataTable::Append(Transaction &transaction, DataChunk &chunk, TableAppendState &state) {
479: 	D_ASSERT(is_root);
480: 	D_ASSERT(chunk.ColumnCount() == types.size());
481: 	chunk.Verify();
482: 
483: 	idx_t remaining = chunk.size();
484: 	while (true) {
485: 		auto current_row_group = state.row_group_append_state.row_group;
486: 		// check how much we can fit into the current row_group
487: 		idx_t append_count =
488: 		    MinValue<idx_t>(remaining, RowGroup::ROW_GROUP_SIZE - state.row_group_append_state.offset_in_row_group);
489: 		if (append_count > 0) {
490: 			current_row_group->Append(state.row_group_append_state, chunk, append_count);
491: 			// merge the stats
492: 			lock_guard<mutex> stats_guard(stats_lock);
493: 			for (idx_t i = 0; i < types.size(); i++) {
494: 				column_stats[i]->Merge(*current_row_group->GetStatistics(i));
495: 			}
496: 		}
497: 		state.remaining_append_count -= append_count;
498: 		remaining -= append_count;
499: 		if (remaining > 0) {
500: 			// we expect max 1 iteration of this loop (i.e. a single chunk should never overflow more than one
501: 			// row_group)
502: 			D_ASSERT(chunk.size() == remaining + append_count);
503: 			// slice the input chunk
504: 			if (remaining < chunk.size()) {
505: 				SelectionVector sel(STANDARD_VECTOR_SIZE);
506: 				for (idx_t i = 0; i < remaining; i++) {
507: 					sel.set_index(i, append_count + i);
508: 				}
509: 				chunk.Slice(sel, remaining);
510: 			}
511: 			// append a new row_group
512: 			AppendRowGroup(current_row_group->start + current_row_group->count);
513: 			// set up the append state for this row_group
514: 			lock_guard<mutex> row_group_lock(row_groups->node_lock);
515: 			auto last_row_group = (RowGroup *)row_groups->GetLastSegment();
516: 			last_row_group->InitializeAppend(transaction, state.row_group_append_state, state.remaining_append_count);
517: 			continue;
518: 		} else {
519: 			break;
520: 		}
521: 	}
522: 	state.current_row += chunk.size();
523: }
524: 
525: void DataTable::ScanTableSegment(idx_t row_start, idx_t count, const std::function<void(DataChunk &chunk)> &function) {
526: 	idx_t end = row_start + count;
527: 
528: 	vector<column_t> column_ids;
529: 	vector<LogicalType> types;
530: 	for (idx_t i = 0; i < this->types.size(); i++) {
531: 		column_ids.push_back(i);
532: 		types.push_back(this->types[i]);
533: 	}
534: 	DataChunk chunk;
535: 	chunk.Initialize(types);
536: 
537: 	CreateIndexScanState state;
538: 
539: 	idx_t row_start_aligned = row_start / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE;
540: 	InitializeScanWithOffset(state, column_ids, row_start_aligned, row_start + count);
541: 
542: 	idx_t current_row = row_start_aligned;
543: 	while (current_row < end) {
544: 		CreateIndexScan(state, column_ids, chunk, true);
545: 		if (chunk.size() == 0) {
546: 			break;
547: 		}
548: 		idx_t end_row = current_row + chunk.size();
549: 		// figure out if we need to write the entire chunk or just part of it
550: 		idx_t chunk_start = MaxValue<idx_t>(current_row, row_start);
551: 		idx_t chunk_end = MinValue<idx_t>(end_row, end);
552: 		D_ASSERT(chunk_start < chunk_end);
553: 		idx_t chunk_count = chunk_end - chunk_start;
554: 		if (chunk_count != chunk.size()) {
555: 			// need to slice the chunk before insert
556: 			auto start_in_chunk = chunk_start % STANDARD_VECTOR_SIZE;
557: 			SelectionVector sel(start_in_chunk, chunk_count);
558: 			chunk.Slice(sel, chunk_count);
559: 			chunk.Verify();
560: 		}
561: 		function(chunk);
562: 		chunk.Reset();
563: 		current_row = end_row;
564: 	}
565: }
566: 
567: void DataTable::WriteToLog(WriteAheadLog &log, idx_t row_start, idx_t count) {
568: 	log.WriteSetTable(info->schema, info->table);
569: 	ScanTableSegment(row_start, count, [&](DataChunk &chunk) { log.WriteInsert(chunk); });
570: }
571: 
572: void DataTable::CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count) {
573: 	lock_guard<mutex> lock(append_lock);
574: 
575: 	auto row_group = (RowGroup *)row_groups->GetSegment(row_start);
576: 	idx_t current_row = row_start;
577: 	idx_t remaining = count;
578: 	while (true) {
579: 		idx_t start_in_row_group = current_row - row_group->start;
580: 		idx_t append_count = MinValue<idx_t>(row_group->count - start_in_row_group, remaining);
581: 
582: 		row_group->CommitAppend(commit_id, start_in_row_group, append_count);
583: 
584: 		current_row += append_count;
585: 		remaining -= append_count;
586: 		if (remaining == 0) {
587: 			break;
588: 		}
589: 		row_group = (RowGroup *)row_group->next.get();
590: 	}
591: 	info->cardinality += count;
592: }
593: 
594: void DataTable::RevertAppendInternal(idx_t start_row, idx_t count) {
595: 	if (count == 0) {
596: 		// nothing to revert!
597: 		return;
598: 	}
599: 	if (total_rows != start_row + count) {
600: 		// interleaved append: don't do anything
601: 		// in this case the rows will stay as "inserted by transaction X", but will never be committed
602: 		// they will never be used by any other transaction and will essentially leave a gap
603: 		// this situation is rare, and as such we don't care about optimizing it (yet?)
604: 		// it only happens if C1 appends a lot of data -> C2 appends a lot of data -> C1 rolls back
605: 		return;
606: 	}
607: 	// adjust the cardinality
608: 	info->cardinality = start_row;
609: 	total_rows = start_row;
610: 	D_ASSERT(is_root);
611: 	// revert appends made to row_groups
612: 	lock_guard<mutex> tree_lock(row_groups->node_lock);
613: 	// find the segment index that the current row belongs to
614: 	idx_t segment_index = row_groups->GetSegmentIndex(start_row);
615: 	auto segment = row_groups->nodes[segment_index].node;
616: 	auto &info = (RowGroup &)*segment;
617: 
618: 	// remove any segments AFTER this segment: they should be deleted entirely
619: 	if (segment_index < row_groups->nodes.size() - 1) {
620: 		row_groups->nodes.erase(row_groups->nodes.begin() + segment_index + 1, row_groups->nodes.end());
621: 	}
622: 	info.next = nullptr;
623: 	info.RevertAppend(start_row);
624: }
625: 
626: void DataTable::RevertAppend(idx_t start_row, idx_t count) {
627: 	lock_guard<mutex> lock(append_lock);
628: 
629: 	if (!info->indexes.Empty()) {
630: 		idx_t current_row_base = start_row;
631: 		row_t row_data[STANDARD_VECTOR_SIZE];
632: 		Vector row_identifiers(LOGICAL_ROW_TYPE, (data_ptr_t)row_data);
633: 		ScanTableSegment(start_row, count, [&](DataChunk &chunk) {
634: 			for (idx_t i = 0; i < chunk.size(); i++) {
635: 				row_data[i] = current_row_base + i;
636: 			}
637: 			info->indexes.Scan([&](Index &index) {
638: 				index.Delete(chunk, row_identifiers);
639: 				return false;
640: 			});
641: 			current_row_base += chunk.size();
642: 		});
643: 	}
644: 	RevertAppendInternal(start_row, count);
645: }
646: 
647: //===--------------------------------------------------------------------===//
648: // Indexes
649: //===--------------------------------------------------------------------===//
650: bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
651: 	D_ASSERT(is_root);
652: 	if (info->indexes.Empty()) {
653: 		return true;
654: 	}
655: 	// first generate the vector of row identifiers
656: 	Vector row_identifiers(LOGICAL_ROW_TYPE);
657: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
658: 
659: 	vector<Index *> already_appended;
660: 	bool append_failed = false;
661: 	// now append the entries to the indices
662: 	info->indexes.Scan([&](Index &index) {
663: 		if (!index.Append(chunk, row_identifiers)) {
664: 			append_failed = true;
665: 			return true;
666: 		}
667: 		already_appended.push_back(&index);
668: 		return false;
669: 	});
670: 
671: 	if (append_failed) {
672: 		// constraint violation!
673: 		// remove any appended entries from previous indexes (if any)
674: 
675: 		for (auto *index : already_appended) {
676: 			index->Delete(chunk, row_identifiers);
677: 		}
678: 
679: 		return false;
680: 	}
681: 	return true;
682: }
683: 
684: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
685: 	D_ASSERT(is_root);
686: 	if (info->indexes.Empty()) {
687: 		return;
688: 	}
689: 	// first generate the vector of row identifiers
690: 	Vector row_identifiers(LOGICAL_ROW_TYPE);
691: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
692: 
693: 	// now remove the entries from the indices
694: 	RemoveFromIndexes(state, chunk, row_identifiers);
695: }
696: 
697: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
698: 	D_ASSERT(is_root);
699: 	info->indexes.Scan([&](Index &index) {
700: 		index.Delete(chunk, row_identifiers);
701: 		return false;
702: 	});
703: }
704: 
705: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
706: 	D_ASSERT(is_root);
707: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
708: 
709: 	// figure out which row_group to fetch from
710: 	auto row_group = (RowGroup *)row_groups->GetSegment(row_ids[0]);
711: 	auto row_group_vector_idx = (row_ids[0] - row_group->start) / STANDARD_VECTOR_SIZE;
712: 	auto base_row_id = row_group_vector_idx * STANDARD_VECTOR_SIZE;
713: 
714: 	// create a selection vector from the row_ids
715: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
716: 	for (idx_t i = 0; i < count; i++) {
717: 		auto row_in_vector = row_ids[i] - base_row_id;
718: 		D_ASSERT(row_in_vector < STANDARD_VECTOR_SIZE);
719: 		sel.set_index(i, row_in_vector);
720: 	}
721: 
722: 	// now fetch the columns from that row_group
723: 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
724: 	TableScanState state;
725: 	state.max_row = total_rows;
726: 	for (idx_t i = 0; i < types.size(); i++) {
727: 		state.column_ids.push_back(i);
728: 	}
729: 	DataChunk result;
730: 	result.Initialize(types);
731: 
732: 	row_group->InitializeScanWithOffset(state.row_group_scan_state, row_group_vector_idx);
733: 	row_group->IndexScan(state.row_group_scan_state, result, false);
734: 	result.Slice(sel, count);
735: 
736: 	info->indexes.Scan([&](Index &index) {
737: 		index.Delete(result, row_identifiers);
738: 		return false;
739: 	});
740: }
741: 
742: //===--------------------------------------------------------------------===//
743: // Delete
744: //===--------------------------------------------------------------------===//
745: void DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_identifiers, idx_t count) {
746: 	D_ASSERT(row_identifiers.GetType().InternalType() == ROW_TYPE);
747: 	if (count == 0) {
748: 		return;
749: 	}
750: 
751: 	auto &transaction = Transaction::GetTransaction(context);
752: 
753: 	row_identifiers.Normalify(count);
754: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
755: 	auto first_id = ids[0];
756: 
757: 	if (first_id >= MAX_ROW_ID) {
758: 		// deletion is in transaction-local storage: push delete into local chunk collection
759: 		transaction.storage.Delete(this, row_identifiers, count);
760: 	} else {
761: 		auto row_group = (RowGroup *)row_groups->GetSegment(first_id);
762: 		row_group->Delete(transaction, this, row_identifiers, count);
763: 	}
764: }
765: 
766: //===--------------------------------------------------------------------===//
767: // Update
768: //===--------------------------------------------------------------------===//
769: static void CreateMockChunk(vector<LogicalType> &types, const vector<column_t> &column_ids, DataChunk &chunk,
770:                             DataChunk &mock_chunk) {
771: 	// construct a mock DataChunk
772: 	mock_chunk.InitializeEmpty(types);
773: 	for (column_t i = 0; i < column_ids.size(); i++) {
774: 		mock_chunk.data[column_ids[i]].Reference(chunk.data[i]);
775: 	}
776: 	mock_chunk.SetCardinality(chunk.size());
777: }
778: 
779: static bool CreateMockChunk(TableCatalogEntry &table, const vector<column_t> &column_ids,
780:                             unordered_set<column_t> &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
781: 	idx_t found_columns = 0;
782: 	// check whether the desired columns are present in the UPDATE clause
783: 	for (column_t i = 0; i < column_ids.size(); i++) {
784: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
785: 			found_columns++;
786: 		}
787: 	}
788: 	if (found_columns == 0) {
789: 		// no columns were found: no need to check the constraint again
790: 		return false;
791: 	}
792: 	if (found_columns != desired_column_ids.size()) {
793: 		// FIXME: not all columns in UPDATE clause are present!
794: 		// this should not be triggered at all as the binder should add these columns
795: 		throw InternalException("Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
796: 	}
797: 	// construct a mock DataChunk
798: 	auto types = table.GetTypes();
799: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
800: 	return true;
801: }
802: 
803: void DataTable::VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chunk,
804:                                         const vector<column_t> &column_ids) {
805: 	for (auto &constraint : table.bound_constraints) {
806: 		switch (constraint->type) {
807: 		case ConstraintType::NOT_NULL: {
808: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
809: 			// check if the constraint is in the list of column_ids
810: 			for (idx_t i = 0; i < column_ids.size(); i++) {
811: 				if (column_ids[i] == not_null.index) {
812: 					// found the column id: check the data in
813: 					VerifyNotNullConstraint(table, chunk.data[i], chunk.size(), table.columns[not_null.index].name);
814: 					break;
815: 				}
816: 			}
817: 			break;
818: 		}
819: 		case ConstraintType::CHECK: {
820: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
821: 
822: 			DataChunk mock_chunk;
823: 			if (CreateMockChunk(table, column_ids, check.bound_columns, chunk, mock_chunk)) {
824: 				VerifyCheckConstraint(table, *check.expression, mock_chunk);
825: 			}
826: 			break;
827: 		}
828: 		case ConstraintType::UNIQUE:
829: 		case ConstraintType::FOREIGN_KEY:
830: 			break;
831: 		default:
832: 			throw NotImplementedException("Constraint type not implemented!");
833: 		}
834: 	}
835: 	// update should not be called for indexed columns!
836: 	// instead update should have been rewritten to delete + update on higher layer
837: #ifdef DEBUG
838: 	info->indexes.Scan([&](Index &index) {
839: 		D_ASSERT(!index.IndexIsUpdated(column_ids));
840: 		return false;
841: 	});
842: 
843: #endif
844: }
845: 
846: void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
847:                        const vector<column_t> &column_ids, DataChunk &updates) {
848: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
849: 
850: 	updates.Verify();
851: 	if (updates.size() == 0) {
852: 		return;
853: 	}
854: 
855: 	if (!is_root) {
856: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
857: 	}
858: 
859: 	// first verify that no constraints are violated
860: 	VerifyUpdateConstraints(table, updates, column_ids);
861: 
862: 	// now perform the actual update
863: 	auto &transaction = Transaction::GetTransaction(context);
864: 
865: 	updates.Normalify();
866: 	row_ids.Normalify(updates.size());
867: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
868: 	if (first_id >= MAX_ROW_ID) {
869: 		// update is in transaction-local storage: push update into local storage
870: 		transaction.storage.Update(this, row_ids, column_ids, updates);
871: 		return;
872: 	}
873: 	// find the row_group this id belongs to
874: 	auto row_group = (RowGroup *)row_groups->GetSegment(first_id);
875: 	row_group->Update(transaction, updates, row_ids, column_ids);
876: 
877: 	lock_guard<mutex> stats_guard(stats_lock);
878: 	for (idx_t i = 0; i < column_ids.size(); i++) {
879: 		auto column_id = column_ids[i];
880: 		column_stats[column_id]->Merge(*row_group->GetStatistics(column_id));
881: 	}
882: }
883: 
884: void DataTable::UpdateColumn(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
885:                              const vector<column_t> &column_path, DataChunk &updates) {
886: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
887: 	D_ASSERT(updates.ColumnCount() == 1);
888: 	updates.Verify();
889: 	if (updates.size() == 0) {
890: 		return;
891: 	}
892: 
893: 	if (!is_root) {
894: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
895: 	}
896: 
897: 	// now perform the actual update
898: 	auto &transaction = Transaction::GetTransaction(context);
899: 
900: 	updates.Normalify();
901: 	row_ids.Normalify(updates.size());
902: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
903: 	if (first_id >= MAX_ROW_ID) {
904: 		throw NotImplementedException("Cannot update a column-path on transaction local data");
905: 	}
906: 	// find the row_group this id belongs to
907: 	auto primary_column_idx = column_path[0];
908: 	auto row_group = (RowGroup *)row_groups->GetSegment(first_id);
909: 	row_group->UpdateColumn(transaction, updates, row_ids, column_path);
910: 
911: 	lock_guard<mutex> stats_guard(stats_lock);
912: 	column_stats[primary_column_idx]->Merge(*row_group->GetStatistics(primary_column_idx));
913: }
914: 
915: //===--------------------------------------------------------------------===//
916: // Create Index Scan
917: //===--------------------------------------------------------------------===//
918: void DataTable::InitializeCreateIndexScan(CreateIndexScanState &state, const vector<column_t> &column_ids) {
919: 	// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan
920: 	state.append_lock = std::unique_lock<mutex>(append_lock);
921: 	state.delete_lock = std::unique_lock<mutex>(row_groups->node_lock);
922: 
923: 	InitializeScan(state, column_ids);
924: }
925: 
926: void DataTable::CreateIndexScan(CreateIndexScanState &state, const vector<column_t> &column_ids, DataChunk &result,
927:                                 bool allow_pending_updates) {
928: 	// scan the persistent segments
929: 	if (ScanCreateIndex(state, result, allow_pending_updates)) {
930: 		return;
931: 	}
932: }
933: 
934: bool DataTable::ScanCreateIndex(CreateIndexScanState &state, DataChunk &result, bool allow_pending_updates) {
935: 	auto current_row_group = state.row_group_scan_state.row_group;
936: 	while (current_row_group) {
937: 		current_row_group->IndexScan(state.row_group_scan_state, result, allow_pending_updates);
938: 		if (result.size() > 0) {
939: 			return true;
940: 		} else {
941: 			current_row_group = state.row_group_scan_state.row_group = (RowGroup *)current_row_group->next.get();
942: 			if (current_row_group) {
943: 				current_row_group->InitializeScan(state.row_group_scan_state);
944: 			}
945: 		}
946: 	}
947: 	return false;
948: }
949: 
950: void DataTable::AddIndex(unique_ptr<Index> index, const vector<unique_ptr<Expression>> &expressions) {
951: 	DataChunk result;
952: 	result.Initialize(index->logical_types);
953: 
954: 	DataChunk intermediate;
955: 	vector<LogicalType> intermediate_types;
956: 	auto column_ids = index->column_ids;
957: 	column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
958: 	for (auto &id : index->column_ids) {
959: 		intermediate_types.push_back(types[id]);
960: 	}
961: 	intermediate_types.push_back(LOGICAL_ROW_TYPE);
962: 	intermediate.Initialize(intermediate_types);
963: 
964: 	// initialize an index scan
965: 	CreateIndexScanState state;
966: 	InitializeCreateIndexScan(state, column_ids);
967: 
968: 	if (!is_root) {
969: 		throw TransactionException("Transaction conflict: cannot add an index to a table that has been altered!");
970: 	}
971: 
972: 	// now start incrementally building the index
973: 	{
974: 		IndexLock lock;
975: 		index->InitializeLock(lock);
976: 		ExpressionExecutor executor(expressions);
977: 		while (true) {
978: 			intermediate.Reset();
979: 			// scan a new chunk from the table to index
980: 			CreateIndexScan(state, column_ids, intermediate);
981: 			if (intermediate.size() == 0) {
982: 				// finished scanning for index creation
983: 				// release all locks
984: 				break;
985: 			}
986: 			// resolve the expressions for this chunk
987: 			executor.Execute(intermediate, result);
988: 
989: 			// insert into the index
990: 			if (!index->Insert(lock, result, intermediate.data[intermediate.ColumnCount() - 1])) {
991: 				throw ConstraintException(
992: 				    "Cant create unique index, table contains duplicate data on indexed column(s)");
993: 			}
994: 		}
995: 	}
996: 	info->indexes.AddIndex(move(index));
997: }
998: 
999: unique_ptr<BaseStatistics> DataTable::GetStatistics(ClientContext &context, column_t column_id) {
1000: 	if (column_id == COLUMN_IDENTIFIER_ROW_ID) {
1001: 		return nullptr;
1002: 	}
1003: 	lock_guard<mutex> stats_guard(stats_lock);
1004: 	return column_stats[column_id]->Copy();
1005: }
1006: 
1007: //===--------------------------------------------------------------------===//
1008: // Checkpoint
1009: //===--------------------------------------------------------------------===//
1010: BlockPointer DataTable::Checkpoint(TableDataWriter &writer) {
1011: 	// checkpoint each individual row group
1012: 	// FIXME: we might want to combine adjacent row groups in case they have had deletions...
1013: 	vector<unique_ptr<BaseStatistics>> global_stats;
1014: 	for (idx_t i = 0; i < types.size(); i++) {
1015: 		global_stats.push_back(BaseStatistics::CreateEmpty(types[i]));
1016: 	}
1017: 
1018: 	auto row_group = (RowGroup *)row_groups->GetRootSegment();
1019: 	vector<RowGroupPointer> row_group_pointers;
1020: 	while (row_group) {
1021: 		auto pointer = row_group->Checkpoint(writer, global_stats);
1022: 		row_group_pointers.push_back(move(pointer));
1023: 		row_group = (RowGroup *)row_group->next.get();
1024: 	}
1025: 	// store the current position in the metadata writer
1026: 	// this is where the row groups for this table start
1027: 	auto &meta_writer = writer.GetMetaWriter();
1028: 	auto pointer = meta_writer.GetBlockPointer();
1029: 
1030: 	for (auto &stats : global_stats) {
1031: 		stats->Serialize(meta_writer);
1032: 	}
1033: 	// now start writing the row group pointers to disk
1034: 	meta_writer.Write<uint64_t>(row_group_pointers.size());
1035: 	for (auto &row_group_pointer : row_group_pointers) {
1036: 		RowGroup::Serialize(row_group_pointer, meta_writer);
1037: 	}
1038: 	return pointer;
1039: }
1040: 
1041: void DataTable::CommitDropColumn(idx_t index) {
1042: 	auto segment = (RowGroup *)row_groups->GetRootSegment();
1043: 	while (segment) {
1044: 		segment->CommitDropColumn(index);
1045: 		segment = (RowGroup *)segment->next.get();
1046: 	}
1047: }
1048: 
1049: idx_t DataTable::GetTotalRows() {
1050: 	return total_rows;
1051: }
1052: 
1053: void DataTable::CommitDropTable() {
1054: 	// commit a drop of this table: mark all blocks as modified so they can be reclaimed later on
1055: 	auto segment = (RowGroup *)row_groups->GetRootSegment();
1056: 	while (segment) {
1057: 		segment->CommitDrop();
1058: 		segment = (RowGroup *)segment->next.get();
1059: 	}
1060: }
1061: 
1062: //===--------------------------------------------------------------------===//
1063: // GetStorageInfo
1064: //===--------------------------------------------------------------------===//
1065: vector<vector<Value>> DataTable::GetStorageInfo() {
1066: 	vector<vector<Value>> result;
1067: 
1068: 	auto row_group = (RowGroup *)row_groups->GetRootSegment();
1069: 	idx_t row_group_index = 0;
1070: 	while (row_group) {
1071: 		row_group->GetStorageInfo(row_group_index, result);
1072: 		row_group_index++;
1073: 
1074: 		row_group = (RowGroup *)row_group->next.get();
1075: 	}
1076: 
1077: 	return result;
1078: }
1079: 
1080: } // namespace duckdb
[end of src/storage/data_table.cpp]
[start of src/storage/table/row_group.cpp]
1: #include "duckdb/storage/table/row_group.hpp"
2: #include "duckdb/common/types/vector.hpp"
3: #include "duckdb/transaction/transaction.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/storage/table/column_data.hpp"
6: #include "duckdb/storage/table/standard_column_data.hpp"
7: #include "duckdb/storage/table/update_segment.hpp"
8: #include "duckdb/common/chrono.hpp"
9: #include "duckdb/planner/table_filter.hpp"
10: #include "duckdb/execution/expression_executor.hpp"
11: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
12: #include "duckdb/storage/meta_block_reader.hpp"
13: 
14: namespace duckdb {
15: 
16: constexpr const idx_t RowGroup::ROW_GROUP_VECTOR_COUNT;
17: constexpr const idx_t RowGroup::ROW_GROUP_SIZE;
18: 
19: RowGroup::RowGroup(DatabaseInstance &db, DataTableInfo &table_info, idx_t start, idx_t count)
20:     : SegmentBase(start, count), db(db), table_info(table_info) {
21: 
22: 	Verify();
23: }
24: 
25: RowGroup::RowGroup(DatabaseInstance &db, DataTableInfo &table_info, const vector<LogicalType> &types,
26:                    RowGroupPointer &pointer)
27:     : SegmentBase(pointer.row_start, pointer.tuple_count), db(db), table_info(table_info) {
28: 	// deserialize the columns
29: 	if (pointer.data_pointers.size() != types.size()) {
30: 		throw IOException("Row group column count is unaligned with table column count. Corrupt file?");
31: 	}
32: 	for (idx_t i = 0; i < pointer.data_pointers.size(); i++) {
33: 		auto &block_pointer = pointer.data_pointers[i];
34: 		MetaBlockReader column_data_reader(db, block_pointer.block_id);
35: 		column_data_reader.offset = block_pointer.offset;
36: 		this->columns.push_back(ColumnData::Deserialize(table_info, i, start, column_data_reader, types[i]));
37: 	}
38: 
39: 	// set up the statistics
40: 	for (auto &stats : pointer.statistics) {
41: 		this->stats.push_back(make_shared<SegmentStatistics>(stats->type, move(stats)));
42: 	}
43: 	this->version_info = move(pointer.versions);
44: 
45: 	Verify();
46: }
47: 
48: RowGroup::~RowGroup() {
49: }
50: 
51: void RowGroup::InitializeEmpty(const vector<LogicalType> &types) {
52: 	// set up the segment trees for the column segments
53: 	for (idx_t i = 0; i < types.size(); i++) {
54: 		auto column_data = make_shared<StandardColumnData>(GetTableInfo(), i, start, types[i]);
55: 		stats.push_back(make_shared<SegmentStatistics>(types[i]));
56: 		columns.push_back(move(column_data));
57: 	}
58: }
59: 
60: bool RowGroup::InitializeScanWithOffset(RowGroupScanState &state, idx_t vector_offset) {
61: 	auto &column_ids = state.parent.column_ids;
62: 	if (state.parent.table_filters) {
63: 		if (!CheckZonemap(*state.parent.table_filters, column_ids)) {
64: 			return false;
65: 		}
66: 	}
67: 
68: 	state.row_group = this;
69: 	state.vector_index = vector_offset;
70: 	state.max_row =
71: 	    this->start > state.parent.max_row ? 0 : MinValue<idx_t>(this->count, state.parent.max_row - this->start);
72: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
73: 	for (idx_t i = 0; i < column_ids.size(); i++) {
74: 		auto column = column_ids[i];
75: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
76: 			columns[column]->InitializeScanWithOffset(state.column_scans[i],
77: 			                                          start + vector_offset * STANDARD_VECTOR_SIZE);
78: 		} else {
79: 			state.column_scans[i].current = nullptr;
80: 		}
81: 	}
82: 	return true;
83: }
84: 
85: bool RowGroup::InitializeScan(RowGroupScanState &state) {
86: 	auto &column_ids = state.parent.column_ids;
87: 	if (state.parent.table_filters) {
88: 		if (!CheckZonemap(*state.parent.table_filters, column_ids)) {
89: 			return false;
90: 		}
91: 	}
92: 	state.row_group = this;
93: 	state.vector_index = 0;
94: 	state.max_row =
95: 	    this->start > state.parent.max_row ? 0 : MinValue<idx_t>(this->count, state.parent.max_row - this->start);
96: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
97: 	for (idx_t i = 0; i < column_ids.size(); i++) {
98: 		auto column = column_ids[i];
99: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
100: 			columns[column]->InitializeScan(state.column_scans[i]);
101: 		} else {
102: 			state.column_scans[i].current = nullptr;
103: 		}
104: 	}
105: 	return true;
106: }
107: 
108: unique_ptr<RowGroup> RowGroup::AlterType(ClientContext &context, const LogicalType &target_type, idx_t changed_idx,
109:                                          ExpressionExecutor &executor, TableScanState &scan_state,
110:                                          DataChunk &scan_chunk) {
111: 	Verify();
112: 
113: 	// construct a new column data for this type
114: 	auto column_data = make_shared<StandardColumnData>(GetTableInfo(), changed_idx, start, target_type);
115: 
116: 	ColumnAppendState append_state;
117: 	column_data->InitializeAppend(append_state);
118: 
119: 	// scan the original table, and fill the new column with the transformed value
120: 	InitializeScan(scan_state.row_group_scan_state);
121: 
122: 	Vector append_vector(target_type);
123: 	auto altered_col_stats = make_shared<SegmentStatistics>(target_type);
124: 	while (true) {
125: 		// scan the table
126: 		scan_chunk.Reset();
127: 		IndexScan(scan_state.row_group_scan_state, scan_chunk, true);
128: 		if (scan_chunk.size() == 0) {
129: 			break;
130: 		}
131: 		// execute the expression
132: 		executor.ExecuteExpression(scan_chunk, append_vector);
133: 		column_data->Append(*altered_col_stats->statistics, append_state, append_vector, scan_chunk.size());
134: 	}
135: 
136: 	// set up the row_group based on this row_group
137: 	auto row_group = make_unique<RowGroup>(db, table_info, this->start, this->count);
138: 	row_group->version_info = version_info;
139: 	for (idx_t i = 0; i < columns.size(); i++) {
140: 		if (i == changed_idx) {
141: 			// this is the altered column: use the new column
142: 			row_group->columns.push_back(move(column_data));
143: 			row_group->stats.push_back(move(altered_col_stats));
144: 		} else {
145: 			// this column was not altered: use the data directly
146: 			row_group->columns.push_back(columns[i]);
147: 			row_group->stats.push_back(stats[i]);
148: 		}
149: 	}
150: 	row_group->Verify();
151: 	return row_group;
152: }
153: 
154: unique_ptr<RowGroup> RowGroup::AddColumn(ClientContext &context, ColumnDefinition &new_column,
155:                                          ExpressionExecutor &executor, Expression *default_value, Vector &result) {
156: 	Verify();
157: 
158: 	// construct a new column data for the new column
159: 	auto added_column = make_shared<StandardColumnData>(GetTableInfo(), columns.size(), start, new_column.type);
160: 
161: 	auto added_col_stats = make_shared<SegmentStatistics>(new_column.type);
162: 	idx_t rows_to_write = this->count;
163: 	if (rows_to_write > 0) {
164: 		DataChunk dummy_chunk;
165: 
166: 		ColumnAppendState state;
167: 		added_column->InitializeAppend(state);
168: 		for (idx_t i = 0; i < rows_to_write; i += STANDARD_VECTOR_SIZE) {
169: 			idx_t rows_in_this_vector = MinValue<idx_t>(rows_to_write - i, STANDARD_VECTOR_SIZE);
170: 			if (default_value) {
171: 				dummy_chunk.SetCardinality(rows_in_this_vector);
172: 				executor.ExecuteExpression(dummy_chunk, result);
173: 			}
174: 			added_column->Append(*added_col_stats->statistics, state, result, rows_in_this_vector);
175: 		}
176: 	}
177: 
178: 	// set up the row_group based on this row_group
179: 	auto row_group = make_unique<RowGroup>(db, table_info, this->start, this->count);
180: 	row_group->version_info = version_info;
181: 	row_group->columns = columns;
182: 	row_group->stats = stats;
183: 	// now add the new column
184: 	row_group->columns.push_back(move(added_column));
185: 	row_group->stats.push_back(move(added_col_stats));
186: 
187: 	row_group->Verify();
188: 	return row_group;
189: }
190: 
191: unique_ptr<RowGroup> RowGroup::RemoveColumn(idx_t removed_column) {
192: 	Verify();
193: 
194: 	D_ASSERT(removed_column < columns.size());
195: 
196: 	auto row_group = make_unique<RowGroup>(db, table_info, this->start, this->count);
197: 	row_group->version_info = version_info;
198: 	row_group->columns = columns;
199: 	row_group->stats = stats;
200: 	// now remove the column
201: 	row_group->columns.erase(row_group->columns.begin() + removed_column);
202: 	row_group->stats.erase(row_group->stats.begin() + removed_column);
203: 
204: 	row_group->Verify();
205: 	return row_group;
206: }
207: 
208: void RowGroup::CommitDrop() {
209: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
210: 		CommitDropColumn(column_idx);
211: 	}
212: }
213: 
214: void RowGroup::CommitDropColumn(idx_t column_idx) {
215: 	D_ASSERT(column_idx < columns.size());
216: 	columns[column_idx]->CommitDropColumn();
217: }
218: 
219: void RowGroupScanState::NextVector() {
220: 	vector_index++;
221: 	for (idx_t i = 0; i < parent.column_ids.size(); i++) {
222: 		column_scans[i].Next();
223: 	}
224: }
225: 
226: bool RowGroup::CheckZonemap(TableFilterSet &filters, const vector<column_t> &column_ids) {
227: 	for (auto &entry : filters.filters) {
228: 		auto column_index = entry.first;
229: 		auto &filter = entry.second;
230: 		auto base_column_index = column_ids[column_index];
231: 
232: 		auto propagate_result = filter->CheckStatistics(*stats[base_column_index]->statistics);
233: 		if (propagate_result == FilterPropagateResult::FILTER_ALWAYS_FALSE ||
234: 		    propagate_result == FilterPropagateResult::FILTER_FALSE_OR_NULL) {
235: 			return false;
236: 		}
237: 	}
238: 	return true;
239: }
240: 
241: bool RowGroup::CheckZonemapSegments(RowGroupScanState &state) {
242: 	if (!state.parent.table_filters) {
243: 		return true;
244: 	}
245: 	auto &column_ids = state.parent.column_ids;
246: 	for (auto &entry : state.parent.table_filters->filters) {
247: 		D_ASSERT(entry.first < column_ids.size());
248: 		auto column_idx = entry.first;
249: 		auto base_column_idx = column_ids[column_idx];
250: 		bool read_segment = columns[base_column_idx]->CheckZonemap(state.column_scans[column_idx], *entry.second);
251: 		if (!read_segment) {
252: 			idx_t target_row =
253: 			    state.column_scans[column_idx].current->start + state.column_scans[column_idx].current->count;
254: 			D_ASSERT(target_row >= this->start);
255: 			D_ASSERT(target_row <= this->start + this->count);
256: 			idx_t target_vector_index = (target_row - this->start) / STANDARD_VECTOR_SIZE;
257: 			if (state.vector_index == target_vector_index) {
258: 				// we can't skip any full vectors because this segment contains less than a full vector
259: 				// for now we just bail-out
260: 				// FIXME: we could check if we can ALSO skip the next segments, in which case skipping a full vector
261: 				// might be possible
262: 				// we don't care that much though, since a single segment that fits less than a full vector is
263: 				// exceedingly rare
264: 				return true;
265: 			}
266: 			while (state.vector_index < target_vector_index) {
267: 				state.NextVector();
268: 			}
269: 			return false;
270: 		}
271: 	}
272: 
273: 	return true;
274: }
275: 
276: template <bool SCAN_DELETES, bool SCAN_COMMITTED, bool ALLOW_UPDATES>
277: void RowGroup::TemplatedScan(Transaction *transaction, RowGroupScanState &state, DataChunk &result) {
278: 	auto &table_filters = state.parent.table_filters;
279: 	auto &column_ids = state.parent.column_ids;
280: 	auto &adaptive_filter = state.parent.adaptive_filter;
281: 	while (true) {
282: 		if (state.vector_index * STANDARD_VECTOR_SIZE >= state.max_row) {
283: 			// exceeded the amount of rows to scan
284: 			return;
285: 		}
286: 		idx_t current_row = state.vector_index * STANDARD_VECTOR_SIZE;
287: 		auto max_count = MinValue<idx_t>(STANDARD_VECTOR_SIZE, state.max_row - current_row);
288: 		// idx_t vector_offset = (current_row - state.base_row) / STANDARD_VECTOR_SIZE;
289: 		// //! first check the zonemap if we have to scan this partition
290: 		if (!CheckZonemapSegments(state)) {
291: 			continue;
292: 		}
293: 		// // second, scan the version chunk manager to figure out which tuples to load for this transaction
294: 		idx_t count;
295: 		SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
296: 		if (SCAN_DELETES) {
297: 			D_ASSERT(transaction);
298: 			count = state.row_group->GetSelVector(*transaction, state.vector_index, valid_sel, max_count);
299: 			if (count == 0) {
300: 				// nothing to scan for this vector, skip the entire vector
301: 				state.NextVector();
302: 				continue;
303: 			}
304: 		} else {
305: 			count = max_count;
306: 		}
307: 		idx_t approved_tuple_count = count;
308: 
309: 		for (idx_t i = 0; i < column_ids.size(); i++) {
310: 			auto column = column_ids[i];
311: 			if (column == COLUMN_IDENTIFIER_ROW_ID) {
312: 				// scan row id
313: 				D_ASSERT(result.data[i].GetType().InternalType() == ROW_TYPE);
314: 				result.data[i].Sequence(this->start + current_row, 1);
315: 			} else {
316: 				if (SCAN_COMMITTED) {
317: 					columns[column]->ScanCommitted(state.vector_index, state.column_scans[i], result.data[i],
318: 					                               ALLOW_UPDATES);
319: 				} else {
320: 					D_ASSERT(transaction);
321: 					D_ASSERT(ALLOW_UPDATES);
322: 					columns[column]->Scan(*transaction, state.vector_index, state.column_scans[i], result.data[i]);
323: 				}
324: 			}
325: 		}
326: 		if (table_filters) {
327: 			SelectionVector sel;
328: 			if (count != max_count) {
329: 				sel.Initialize(valid_sel);
330: 			} else {
331: 				sel.Initialize(FlatVector::INCREMENTAL_SELECTION_VECTOR);
332: 			}
333: 			//! First, we scan the columns with filters, fetch their data and generate a selection vector.
334: 			//! get runtime statistics
335: 			auto start_time = high_resolution_clock::now();
336: 			for (idx_t i = 0; i < adaptive_filter->permutation.size(); i++) {
337: 				auto tf_idx = adaptive_filter->permutation[i];
338: 				D_ASSERT(table_filters->filters.count(tf_idx) == 1);
339: 				auto &filter = table_filters->filters[tf_idx];
340: 				UncompressedSegment::FilterSelection(sel, result.data[tf_idx], *filter, approved_tuple_count,
341: 				                                     FlatVector::Validity(result.data[tf_idx]));
342: 			}
343: 			auto end_time = high_resolution_clock::now();
344: 			if (adaptive_filter && adaptive_filter->permutation.size() > 1) {
345: 				adaptive_filter->AdaptRuntimeStatistics(duration_cast<duration<double>>(end_time - start_time).count());
346: 			}
347: 
348: 			if (approved_tuple_count == 0) {
349: 				result.Reset();
350: 				state.vector_index++;
351: 				continue;
352: 			}
353: 			if (approved_tuple_count != max_count) {
354: 				result.Slice(sel, approved_tuple_count);
355: 			}
356: 		} else if (count != max_count) {
357: 			result.Slice(valid_sel, count);
358: 		}
359: 		D_ASSERT(approved_tuple_count > 0);
360: 		result.SetCardinality(approved_tuple_count);
361: 		state.vector_index++;
362: 		break;
363: 	}
364: }
365: 
366: void RowGroup::Scan(Transaction &transaction, RowGroupScanState &state, DataChunk &result) {
367: 	TemplatedScan<true, false, true>(&transaction, state, result);
368: }
369: 
370: void RowGroup::IndexScan(RowGroupScanState &state, DataChunk &result, bool allow_pending_updates) {
371: 	if (allow_pending_updates) {
372: 		TemplatedScan<false, true, true>(nullptr, state, result);
373: 	} else {
374: 		TemplatedScan<false, true, false>(nullptr, state, result);
375: 	}
376: }
377: 
378: ChunkInfo *RowGroup::GetChunkInfo(idx_t vector_idx) {
379: 	if (!version_info) {
380: 		return nullptr;
381: 	}
382: 	return version_info->info[vector_idx].get();
383: }
384: 
385: idx_t RowGroup::GetSelVector(Transaction &transaction, idx_t vector_idx, SelectionVector &sel_vector, idx_t max_count) {
386: 	lock_guard<mutex> lock(row_group_lock);
387: 
388: 	auto info = GetChunkInfo(vector_idx);
389: 	if (!info) {
390: 		return max_count;
391: 	}
392: 	return info->GetSelVector(transaction, sel_vector, max_count);
393: }
394: 
395: bool RowGroup::Fetch(Transaction &transaction, idx_t row) {
396: 	D_ASSERT(row < this->count);
397: 	lock_guard<mutex> lock(row_group_lock);
398: 
399: 	idx_t vector_index = row / STANDARD_VECTOR_SIZE;
400: 	auto info = GetChunkInfo(vector_index);
401: 	if (!info) {
402: 		return true;
403: 	}
404: 	return info->Fetch(transaction, row - vector_index * STANDARD_VECTOR_SIZE);
405: }
406: 
407: void RowGroup::FetchRow(Transaction &transaction, ColumnFetchState &state, const vector<column_t> &column_ids,
408:                         row_t row_id, DataChunk &result, idx_t result_idx) {
409: 	for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
410: 		auto column = column_ids[col_idx];
411: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
412: 			// row id column: fill in the row ids
413: 			D_ASSERT(result.data[col_idx].GetType().InternalType() == PhysicalType::INT64);
414: 			result.data[col_idx].SetVectorType(VectorType::FLAT_VECTOR);
415: 			auto data = FlatVector::GetData<row_t>(result.data[col_idx]);
416: 			data[result_idx] = row_id;
417: 		} else {
418: 			// regular column: fetch data from the base column
419: 			columns[column]->FetchRow(transaction, state, row_id, result.data[col_idx], result_idx);
420: 		}
421: 	}
422: }
423: 
424: void RowGroup::AppendVersionInfo(Transaction &transaction, idx_t row_group_start, idx_t count,
425:                                  transaction_t commit_id) {
426: 	idx_t row_group_end = row_group_start + count;
427: 	lock_guard<mutex> lock(row_group_lock);
428: 
429: 	this->count += count;
430: 	D_ASSERT(this->count <= RowGroup::ROW_GROUP_SIZE);
431: 
432: 	// create the version_info if it doesn't exist yet
433: 	if (!version_info) {
434: 		version_info = make_unique<VersionNode>();
435: 	}
436: 	idx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;
437: 	idx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;
438: 	for (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {
439: 		idx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;
440: 		idx_t end =
441: 		    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
442: 		if (start == 0 && end == STANDARD_VECTOR_SIZE) {
443: 			// entire vector is encapsulated by append: append a single constant
444: 			auto constant_info = make_unique<ChunkConstantInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);
445: 			constant_info->insert_id = commit_id;
446: 			constant_info->delete_id = NOT_DELETED_ID;
447: 			version_info->info[vector_idx] = move(constant_info);
448: 		} else {
449: 			// part of a vector is encapsulated: append to that part
450: 			ChunkVectorInfo *info;
451: 			if (!version_info->info[vector_idx]) {
452: 				// first time appending to this vector: create new info
453: 				auto insert_info = make_unique<ChunkVectorInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);
454: 				info = insert_info.get();
455: 				version_info->info[vector_idx] = move(insert_info);
456: 			} else {
457: 				D_ASSERT(version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);
458: 				// use existing vector
459: 				info = (ChunkVectorInfo *)version_info->info[vector_idx].get();
460: 			}
461: 			info->Append(start, end, commit_id);
462: 		}
463: 	}
464: }
465: 
466: void RowGroup::CommitAppend(transaction_t commit_id, idx_t row_group_start, idx_t count) {
467: 	D_ASSERT(version_info.get());
468: 	idx_t row_group_end = row_group_start + count;
469: 	lock_guard<mutex> lock(row_group_lock);
470: 
471: 	idx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;
472: 	idx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;
473: 	for (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {
474: 		idx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;
475: 		idx_t end =
476: 		    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
477: 
478: 		auto info = version_info->info[vector_idx].get();
479: 		info->CommitAppend(commit_id, start, end);
480: 	}
481: }
482: 
483: void RowGroup::RevertAppend(idx_t row_group_start) {
484: 	if (!version_info) {
485: 		return;
486: 	}
487: 	idx_t start_row = row_group_start - this->start;
488: 	idx_t start_vector_idx = (start_row + (STANDARD_VECTOR_SIZE - 1)) / STANDARD_VECTOR_SIZE;
489: 	for (idx_t vector_idx = start_vector_idx; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
490: 		version_info->info[vector_idx].reset();
491: 	}
492: 	for (auto &column : columns) {
493: 		column->RevertAppend(row_group_start);
494: 	}
495: 	this->count = MinValue<idx_t>(row_group_start - this->start, this->count);
496: 	Verify();
497: }
498: 
499: void RowGroup::InitializeAppend(Transaction &transaction, RowGroupAppendState &append_state,
500:                                 idx_t remaining_append_count) {
501: 	append_state.row_group = this;
502: 	append_state.offset_in_row_group = this->count;
503: 	// for each column, initialize the append state
504: 	append_state.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[columns.size()]);
505: 	for (idx_t i = 0; i < columns.size(); i++) {
506: 		columns[i]->InitializeAppend(append_state.states[i]);
507: 	}
508: 	// append the version info for this row_group
509: 	idx_t append_count = MinValue<idx_t>(remaining_append_count, RowGroup::ROW_GROUP_SIZE - this->count);
510: 	AppendVersionInfo(transaction, this->count, append_count, transaction.transaction_id);
511: }
512: 
513: void RowGroup::Append(RowGroupAppendState &state, DataChunk &chunk, idx_t append_count) {
514: 	// append to the current row_group
515: 	for (idx_t i = 0; i < columns.size(); i++) {
516: 		columns[i]->Append(*stats[i]->statistics, state.states[i], chunk.data[i], append_count);
517: 	}
518: 	state.offset_in_row_group += append_count;
519: }
520: 
521: void RowGroup::Update(Transaction &transaction, DataChunk &update_chunk, Vector &row_ids,
522:                       const vector<column_t> &column_ids) {
523: 	auto ids = FlatVector::GetData<row_t>(row_ids);
524: #ifdef DEBUG
525: 	for (size_t i = 0; i < update_chunk.size(); i++) {
526: 		D_ASSERT(ids[i] >= row_t(this->start) && ids[i] < row_t(this->start + this->count));
527: 	}
528: #endif
529: 	for (idx_t i = 0; i < column_ids.size(); i++) {
530: 		auto column = column_ids[i];
531: 		D_ASSERT(column != COLUMN_IDENTIFIER_ROW_ID);
532: 		D_ASSERT(columns[column]->type.id() == update_chunk.data[i].GetType().id());
533: 		columns[column]->Update(transaction, column, update_chunk.data[i], ids, update_chunk.size());
534: 		MergeStatistics(column, *columns[column]->GetUpdateStatistics());
535: 	}
536: }
537: 
538: void RowGroup::UpdateColumn(Transaction &transaction, DataChunk &updates, Vector &row_ids,
539:                             const vector<column_t> &column_path) {
540: 	D_ASSERT(updates.ColumnCount() == 1);
541: 	auto ids = FlatVector::GetData<row_t>(row_ids);
542: 
543: 	auto primary_column_idx = column_path[0];
544: 	D_ASSERT(primary_column_idx != COLUMN_IDENTIFIER_ROW_ID);
545: 	D_ASSERT(primary_column_idx < columns.size());
546: 	columns[primary_column_idx]->UpdateColumn(transaction, column_path, updates.data[0], ids, updates.size(), 1);
547: 	MergeStatistics(primary_column_idx, *columns[primary_column_idx]->GetUpdateStatistics());
548: }
549: 
550: unique_ptr<BaseStatistics> RowGroup::GetStatistics(idx_t column_idx) {
551: 	D_ASSERT(column_idx < stats.size());
552: 
553: 	lock_guard<mutex> slock(stats_lock);
554: 	return stats[column_idx]->statistics->Copy();
555: }
556: 
557: void RowGroup::MergeStatistics(idx_t column_idx, BaseStatistics &other) {
558: 	D_ASSERT(column_idx < stats.size());
559: 
560: 	lock_guard<mutex> slock(stats_lock);
561: 	stats[column_idx]->statistics->Merge(other);
562: }
563: 
564: RowGroupPointer RowGroup::Checkpoint(TableDataWriter &writer, vector<unique_ptr<BaseStatistics>> &global_stats) {
565: 	vector<unique_ptr<ColumnCheckpointState>> states;
566: 	states.reserve(columns.size());
567: 
568: 	// checkpoint the individual columns of the row group
569: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
570: 		auto &column = columns[column_idx];
571: 		auto checkpoint_state = column->Checkpoint(*this, writer, column_idx);
572: 		D_ASSERT(checkpoint_state);
573: 
574: 		auto stats = checkpoint_state->GetStatistics();
575: 		D_ASSERT(stats);
576: 
577: 		global_stats[column_idx]->Merge(*stats);
578: 		states.push_back(move(checkpoint_state));
579: 	}
580: 
581: 	// construct the row group pointer and write the column meta data to disk
582: 	D_ASSERT(states.size() == columns.size());
583: 	RowGroupPointer row_group_pointer;
584: 	row_group_pointer.row_start = start;
585: 	row_group_pointer.tuple_count = count;
586: 	for (auto &state : states) {
587: 		// get the current position of the meta data writer
588: 		auto &meta_writer = writer.GetMetaWriter();
589: 		auto pointer = meta_writer.GetBlockPointer();
590: 
591: 		// store the stats and the data pointers in the row group pointers
592: 		row_group_pointer.data_pointers.push_back(pointer);
593: 		row_group_pointer.statistics.push_back(move(state->global_stats));
594: 
595: 		// now flush the actual column data to disk
596: 		state->FlushToDisk();
597: 	}
598: 	row_group_pointer.versions = version_info;
599: 	Verify();
600: 	return row_group_pointer;
601: }
602: 
603: void RowGroup::CheckpointDeletes(VersionNode *versions, Serializer &serializer) {
604: 	if (!versions) {
605: 		// no version information: write nothing
606: 		serializer.Write<idx_t>(0);
607: 		return;
608: 	}
609: 	// first count how many ChunkInfo's we need to deserialize
610: 	idx_t chunk_info_count = 0;
611: 	for (idx_t vector_idx = 0; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
612: 		auto chunk_info = versions->info[vector_idx].get();
613: 		if (!chunk_info) {
614: 			continue;
615: 		}
616: 		chunk_info_count++;
617: 	}
618: 	// now serialize the actual version information
619: 	serializer.Write<idx_t>(chunk_info_count);
620: 	for (idx_t vector_idx = 0; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
621: 		auto chunk_info = versions->info[vector_idx].get();
622: 		if (!chunk_info) {
623: 			continue;
624: 		}
625: 		serializer.Write<idx_t>(vector_idx);
626: 		chunk_info->Serialize(serializer);
627: 	}
628: }
629: 
630: shared_ptr<VersionNode> RowGroup::DeserializeDeletes(Deserializer &source) {
631: 	auto chunk_count = source.Read<idx_t>();
632: 	if (chunk_count == 0) {
633: 		// no deletes
634: 		return nullptr;
635: 	}
636: 	auto version_info = make_shared<VersionNode>();
637: 	for (idx_t i = 0; i < chunk_count; i++) {
638: 		idx_t vector_index = source.Read<idx_t>();
639: 		version_info->info[vector_index] = ChunkInfo::Deserialize(source);
640: 	}
641: 	return version_info;
642: }
643: 
644: void RowGroup::Serialize(RowGroupPointer &pointer, Serializer &serializer) {
645: 	serializer.Write<uint64_t>(pointer.row_start);
646: 	serializer.Write<uint64_t>(pointer.tuple_count);
647: 	for (auto &stats : pointer.statistics) {
648: 		stats->Serialize(serializer);
649: 	}
650: 	for (auto &data_pointer : pointer.data_pointers) {
651: 		serializer.Write<block_id_t>(data_pointer.block_id);
652: 		serializer.Write<uint64_t>(data_pointer.offset);
653: 	}
654: 	CheckpointDeletes(pointer.versions.get(), serializer);
655: }
656: 
657: RowGroupPointer RowGroup::Deserialize(Deserializer &source, const vector<ColumnDefinition> &columns) {
658: 	RowGroupPointer result;
659: 	result.row_start = source.Read<uint64_t>();
660: 	result.tuple_count = source.Read<uint64_t>();
661: 
662: 	result.data_pointers.reserve(columns.size());
663: 	result.statistics.reserve(columns.size());
664: 
665: 	for (idx_t i = 0; i < columns.size(); i++) {
666: 		auto stats = BaseStatistics::Deserialize(source, columns[i].type);
667: 		result.statistics.push_back(move(stats));
668: 	}
669: 	for (idx_t i = 0; i < columns.size(); i++) {
670: 		BlockPointer pointer;
671: 		pointer.block_id = source.Read<block_id_t>();
672: 		pointer.offset = source.Read<uint64_t>();
673: 		result.data_pointers.push_back(pointer);
674: 	}
675: 	result.versions = DeserializeDeletes(source);
676: 	return result;
677: }
678: 
679: //===--------------------------------------------------------------------===//
680: // GetStorageInfo
681: //===--------------------------------------------------------------------===//
682: void RowGroup::GetStorageInfo(idx_t row_group_index, vector<vector<Value>> &result) {
683: 	for (idx_t col_idx = 0; col_idx < columns.size(); col_idx++) {
684: 		columns[col_idx]->GetStorageInfo(row_group_index, {col_idx}, result);
685: 	}
686: }
687: 
688: //===--------------------------------------------------------------------===//
689: // Version Delete Information
690: //===--------------------------------------------------------------------===//
691: class VersionDeleteState {
692: public:
693: 	VersionDeleteState(RowGroup &info, Transaction &transaction, DataTable *table, idx_t base_row)
694: 	    : info(info), transaction(transaction), table(table), current_info(nullptr), current_chunk(INVALID_INDEX),
695: 	      count(0), base_row(base_row) {
696: 	}
697: 
698: 	RowGroup &info;
699: 	Transaction &transaction;
700: 	DataTable *table;
701: 	ChunkVectorInfo *current_info;
702: 	idx_t current_chunk;
703: 	row_t rows[STANDARD_VECTOR_SIZE];
704: 	idx_t count;
705: 	idx_t base_row;
706: 	idx_t chunk_row;
707: 
708: public:
709: 	void Delete(row_t row_id);
710: 	void Flush();
711: };
712: 
713: void RowGroup::Delete(Transaction &transaction, DataTable *table, Vector &row_ids, idx_t count) {
714: 	lock_guard<mutex> lock(row_group_lock);
715: 	VersionDeleteState del_state(*this, transaction, table, this->start);
716: 
717: 	VectorData rdata;
718: 	row_ids.Orrify(count, rdata);
719: 	// obtain a write lock
720: 	auto ids = (row_t *)rdata.data;
721: 	for (idx_t i = 0; i < count; i++) {
722: 		auto ridx = rdata.sel->get_index(i);
723: 		D_ASSERT(ids[ridx] >= 0);
724: 		D_ASSERT(idx_t(ids[ridx]) >= this->start && idx_t(ids[ridx]) < this->start + this->count);
725: 		del_state.Delete(ids[ridx] - this->start);
726: 	}
727: 	del_state.Flush();
728: }
729: 
730: void RowGroup::Verify() {
731: #ifdef DEBUG
732: 	for (auto &column : columns) {
733: 		column->Verify(*this);
734: 	}
735: #endif
736: }
737: 
738: void VersionDeleteState::Delete(row_t row_id) {
739: 	D_ASSERT(row_id >= 0);
740: 	idx_t vector_idx = row_id / STANDARD_VECTOR_SIZE;
741: 	idx_t idx_in_vector = row_id - vector_idx * STANDARD_VECTOR_SIZE;
742: 	if (current_chunk != vector_idx) {
743: 		Flush();
744: 
745: 		if (!info.version_info) {
746: 			info.version_info = make_unique<VersionNode>();
747: 		}
748: 
749: 		if (!info.version_info->info[vector_idx]) {
750: 			// no info yet: create it
751: 			info.version_info->info[vector_idx] =
752: 			    make_unique<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);
753: 		} else if (info.version_info->info[vector_idx]->type == ChunkInfoType::CONSTANT_INFO) {
754: 			auto &constant = (ChunkConstantInfo &)*info.version_info->info[vector_idx];
755: 			// info exists but it's a constant info: convert to a vector info
756: 			auto new_info = make_unique<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);
757: 			new_info->insert_id = constant.insert_id.load();
758: 			for (idx_t i = 0; i < STANDARD_VECTOR_SIZE; i++) {
759: 				new_info->inserted[i] = constant.insert_id.load();
760: 			}
761: 			info.version_info->info[vector_idx] = move(new_info);
762: 		}
763: 		D_ASSERT(info.version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);
764: 		current_info = (ChunkVectorInfo *)info.version_info->info[vector_idx].get();
765: 		current_chunk = vector_idx;
766: 		chunk_row = vector_idx * STANDARD_VECTOR_SIZE;
767: 	}
768: 	rows[count++] = idx_in_vector;
769: }
770: 
771: void VersionDeleteState::Flush() {
772: 	if (count == 0) {
773: 		return;
774: 	}
775: 	// delete in the current info
776: 	current_info->Delete(transaction, rows, count);
777: 	// now push the delete into the undo buffer
778: 	transaction.PushDelete(table, current_info, rows, count, base_row + chunk_row);
779: 	count = 0;
780: }
781: 
782: } // namespace duckdb
[end of src/storage/table/row_group.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: