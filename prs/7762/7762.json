{
  "repo": "duckdb/duckdb",
  "pull_number": 7762,
  "instance_id": "duckdb__duckdb-7762",
  "issue_numbers": [
    "7727",
    "7729",
    "7727"
  ],
  "base_commit": "fc724f980f756b9867ab18fefc4690a7f76c220e",
  "patch": "diff --git a/extension/json/buffered_json_reader.cpp b/extension/json/buffered_json_reader.cpp\nindex d2bdfe55d969..5a408b9444d3 100644\n--- a/extension/json/buffered_json_reader.cpp\n+++ b/extension/json/buffered_json_reader.cpp\n@@ -30,6 +30,10 @@ JSONFileHandle::JSONFileHandle(unique_ptr<FileHandle> file_handle_p, Allocator &\n       requested_reads(0), actual_reads(0), cached_size(0) {\n }\n \n+bool JSONFileHandle::IsOpen() const {\n+\treturn file_handle != nullptr;\n+}\n+\n void JSONFileHandle::Close() {\n \tif (file_handle) {\n \t\tfile_handle->Close();\n@@ -62,19 +66,20 @@ idx_t JSONFileHandle::GetPositionAndSize(idx_t &position, idx_t requested_size)\n \tif (actual_size != 0) {\n \t\trequested_reads++;\n \t}\n+\n \treturn actual_size;\n }\n \n-void JSONFileHandle::ReadAtPosition(const char *pointer, idx_t size, idx_t position, bool sample_run) {\n+void JSONFileHandle::ReadAtPosition(char *pointer, idx_t size, idx_t position, bool sample_run) {\n \tD_ASSERT(size != 0);\n \tif (plain_file_source) {\n-\t\tfile_handle->Read((void *)pointer, size, position);\n+\t\tfile_handle->Read(pointer, size, position);\n \t\tactual_reads++;\n \t\treturn;\n \t}\n \n \tif (sample_run) { // Cache the buffer\n-\t\tfile_handle->Read((void *)pointer, size, position);\n+\t\tfile_handle->Read(pointer, size, position);\n \t\tactual_reads++;\n \t\tcached_buffers.emplace_back(allocator.Allocate(size));\n \t\tmemcpy(cached_buffers.back().get(), pointer, size);\n@@ -87,12 +92,12 @@ void JSONFileHandle::ReadAtPosition(const char *pointer, idx_t size, idx_t posit\n \t\tactual_reads++;\n \t}\n \tif (size != 0) {\n-\t\tfile_handle->Read((void *)pointer, size, position);\n+\t\tfile_handle->Read(pointer, size, position);\n \t\tactual_reads++;\n \t}\n }\n \n-idx_t JSONFileHandle::Read(const char *pointer, idx_t requested_size, bool sample_run) {\n+idx_t JSONFileHandle::Read(char *pointer, idx_t requested_size, bool sample_run) {\n \tD_ASSERT(requested_size != 0);\n \tif (plain_file_source) {\n \t\tauto actual_size = ReadInternal(pointer, requested_size);\n@@ -121,7 +126,7 @@ idx_t JSONFileHandle::Read(const char *pointer, idx_t requested_size, bool sampl\n \treturn actual_size;\n }\n \n-idx_t JSONFileHandle::ReadFromCache(const char *&pointer, idx_t &size, idx_t &position) {\n+idx_t JSONFileHandle::ReadFromCache(char *&pointer, idx_t &size, idx_t &position) {\n \tidx_t read_size = 0;\n \tidx_t total_offset = 0;\n \n@@ -134,7 +139,7 @@ idx_t JSONFileHandle::ReadFromCache(const char *&pointer, idx_t &size, idx_t &po\n \t\tif (position < total_offset + cached_buffer.GetSize()) {\n \t\t\tidx_t within_buffer_offset = position - total_offset;\n \t\t\tidx_t copy_size = MinValue<idx_t>(size, cached_buffer.GetSize() - within_buffer_offset);\n-\t\t\tmemcpy((void *)pointer, cached_buffer.get() + within_buffer_offset, copy_size);\n+\t\t\tmemcpy(pointer, cached_buffer.get() + within_buffer_offset, copy_size);\n \n \t\t\tread_size += copy_size;\n \t\t\tpointer += copy_size;\n@@ -147,11 +152,11 @@ idx_t JSONFileHandle::ReadFromCache(const char *&pointer, idx_t &size, idx_t &po\n \treturn read_size;\n }\n \n-idx_t JSONFileHandle::ReadInternal(const char *pointer, const idx_t requested_size) {\n+idx_t JSONFileHandle::ReadInternal(char *pointer, const idx_t requested_size) {\n \t// Deal with reading from pipes\n \tidx_t total_read_size = 0;\n \twhile (total_read_size < requested_size) {\n-\t\tauto read_size = file_handle->Read((void *)(pointer + total_read_size), requested_size - total_read_size);\n+\t\tauto read_size = file_handle->Read(pointer + total_read_size, requested_size - total_read_size);\n \t\tif (read_size == 0) {\n \t\t\tbreak;\n \t\t}\n@@ -165,6 +170,7 @@ BufferedJSONReader::BufferedJSONReader(ClientContext &context, BufferedJSONReade\n }\n \n void BufferedJSONReader::OpenJSONFile() {\n+\tD_ASSERT(!IsDone());\n \tlock_guard<mutex> guard(lock);\n \tauto &file_system = FileSystem::GetFileSystem(context);\n \tauto regular_file_handle =\n@@ -186,6 +192,13 @@ bool BufferedJSONReader::IsOpen() const {\n \treturn file_handle != nullptr;\n }\n \n+bool BufferedJSONReader::IsDone() const {\n+\tif (file_handle) {\n+\t\treturn !file_handle->IsOpen();\n+\t}\n+\treturn false;\n+}\n+\n BufferedJSONReaderOptions &BufferedJSONReader::GetOptions() {\n \treturn options;\n }\n@@ -212,10 +225,6 @@ void BufferedJSONReader::SetRecordType(duckdb::JSONRecordType type) {\n \toptions.record_type = type;\n }\n \n-bool BufferedJSONReader::IsParallel() const {\n-\treturn options.format == JSONFormat::NEWLINE_DELIMITED && file_handle->CanSeek();\n-}\n-\n const string &BufferedJSONReader::GetFileName() const {\n \treturn file_name;\n }\n@@ -288,7 +297,7 @@ void BufferedJSONReader::ThrowTransformError(idx_t buf_index, idx_t line_or_obje\n                                              const string &error_message) {\n \tstring unit = options.format == JSONFormat::NEWLINE_DELIMITED ? \"line\" : \"record/value\";\n \tauto line = GetLineNumber(buf_index, line_or_object_in_buf);\n-\tthrow InvalidInputException(\"JSON transform error in file \\\"%s\\\", in %s %llu: %s.\", file_name, unit, line,\n+\tthrow InvalidInputException(\"JSON transform error in file \\\"%s\\\", in %s %llu: %s\", file_name, unit, line,\n \t                            error_message);\n }\n \ndiff --git a/extension/json/include/buffered_json_reader.hpp b/extension/json/include/buffered_json_reader.hpp\nindex ac4d103a03c1..4918b29a056e 100644\n--- a/extension/json/include/buffered_json_reader.hpp\n+++ b/extension/json/include/buffered_json_reader.hpp\n@@ -71,6 +71,7 @@ struct JSONBufferHandle {\n struct JSONFileHandle {\n public:\n \tJSONFileHandle(unique_ptr<FileHandle> file_handle, Allocator &allocator);\n+\tbool IsOpen() const;\n \tvoid Close();\n \n \tidx_t FileSize() const;\n@@ -80,15 +81,15 @@ struct JSONFileHandle {\n \tvoid Seek(idx_t position);\n \n \tidx_t GetPositionAndSize(idx_t &position, idx_t requested_size);\n-\tvoid ReadAtPosition(const char *pointer, idx_t size, idx_t position, bool sample_run);\n-\tidx_t Read(const char *pointer, idx_t requested_size, bool sample_run);\n+\tvoid ReadAtPosition(char *pointer, idx_t size, idx_t position, bool sample_run);\n+\tidx_t Read(char *pointer, idx_t requested_size, bool sample_run);\n \n \tvoid Reset();\n \tbool RequestedReadsComplete();\n \n private:\n-\tidx_t ReadFromCache(const char *&pointer, idx_t &size, idx_t &position);\n-\tidx_t ReadInternal(const char *pointer, const idx_t requested_size);\n+\tidx_t ReadFromCache(char *&pointer, idx_t &size, idx_t &position);\n+\tidx_t ReadInternal(char *pointer, const idx_t requested_size);\n \n private:\n \t//! The JSON file handle\n@@ -139,6 +140,7 @@ class BufferedJSONReader {\n \tvoid OpenJSONFile();\n \tvoid CloseJSONFile();\n \tbool IsOpen() const;\n+\tbool IsDone() const;\n \n \tBufferedJSONReaderOptions &GetOptions();\n \tconst BufferedJSONReaderOptions &GetOptions() const;\n@@ -148,8 +150,6 @@ class BufferedJSONReader {\n \tJSONRecordType GetRecordType() const;\n \tvoid SetRecordType(JSONRecordType type);\n \n-\tbool IsParallel() const;\n-\n \tconst string &GetFileName() const;\n \tJSONFileHandle &GetFileHandle() const;\n \ndiff --git a/extension/json/include/json_common.hpp b/extension/json/include/json_common.hpp\nindex a715fb4e58a9..182908b00e5e 100644\n--- a/extension/json/include/json_common.hpp\n+++ b/extension/json/include/json_common.hpp\n@@ -170,6 +170,16 @@ struct JSONCommon {\n \t}\n \n public:\n+\ttemplate <class T>\n+\tstatic T *AllocateArray(yyjson_alc *alc, idx_t count) {\n+\t\treturn reinterpret_cast<T *>(alc->malloc(alc->ctx, sizeof(T) * count));\n+\t}\n+\n+\ttemplate <class T>\n+\tstatic T *AllocateArray(yyjson_mut_doc *doc, idx_t count) {\n+\t\treturn AllocateArray<T>(&doc->alc, count);\n+\t}\n+\n \tstatic inline yyjson_mut_doc *CreateDocument(yyjson_alc *alc) {\n \t\tD_ASSERT(alc);\n \t\treturn yyjson_mut_doc_new(alc);\n@@ -419,11 +429,11 @@ struct JSONCommon {\n \n template <>\n inline char *JSONCommon::WriteVal(yyjson_val *val, yyjson_alc *alc, idx_t &len) {\n-\treturn yyjson_val_write_opts(val, JSONCommon::WRITE_FLAG, alc, (size_t *)&len, nullptr);\n+\treturn yyjson_val_write_opts(val, JSONCommon::WRITE_FLAG, alc, reinterpret_cast<size_t *>(&len), nullptr);\n }\n template <>\n inline char *JSONCommon::WriteVal(yyjson_mut_val *val, yyjson_alc *alc, idx_t &len) {\n-\treturn yyjson_mut_val_write_opts(val, JSONCommon::WRITE_FLAG, alc, (size_t *)&len, nullptr);\n+\treturn yyjson_mut_val_write_opts(val, JSONCommon::WRITE_FLAG, alc, reinterpret_cast<size_t *>(&len), nullptr);\n }\n \n template <>\ndiff --git a/extension/json/include/json_scan.hpp b/extension/json/include/json_scan.hpp\nindex 82b09fa90196..b77ef5b80615 100644\n--- a/extension/json/include/json_scan.hpp\n+++ b/extension/json/include/json_scan.hpp\n@@ -232,6 +232,8 @@ struct JSONScanLocalState {\n \tvoid ThrowObjectSizeError(const idx_t object_size);\n \tvoid ThrowInvalidAtEndError();\n \n+\tbool IsParallel(JSONScanGlobalState &gstate) const;\n+\n private:\n \t//! Bind data\n \tconst JSONScanData &bind_data;\n@@ -245,7 +247,7 @@ struct JSONScanLocalState {\n \tbool is_last;\n \n \t//! Current buffer read info\n-\tconst char *buffer_ptr;\n+\tchar *buffer_ptr;\n \tidx_t buffer_size;\n \tidx_t buffer_offset;\n \tidx_t prev_buffer_remainder;\ndiff --git a/extension/json/json_functions.cpp b/extension/json/json_functions.cpp\nindex b30c8f832d52..5238f61695ea 100644\n--- a/extension/json/json_functions.cpp\n+++ b/extension/json/json_functions.cpp\n@@ -220,8 +220,8 @@ static bool CastVarcharToJSON(Vector &source, Vector &result, idx_t count, CastP\n \tbool success = true;\n \tUnaryExecutor::ExecuteWithNulls<string_t, string_t>(\n \t    source, result, count, [&](string_t input, ValidityMask &mask, idx_t idx) {\n-\t\t    auto data = (char *)(input.GetData());\n-\t\t    auto length = input.GetSize();\n+\t\t    auto data = input.GetDataWriteable();\n+\t\t    const auto length = input.GetSize();\n \n \t\t    yyjson_read_err error;\n \t\t    auto doc = JSONCommon::ReadDocumentUnsafe(data, length, JSONCommon::READ_FLAG, alc, &error);\n@@ -236,7 +236,7 @@ static bool CastVarcharToJSON(Vector &source, Vector &result, idx_t count, CastP\n \t\t    }\n \t\t    return input;\n \t    });\n-\tresult.Reinterpret(source);\n+\tStringVector::AddHeapReference(result, source);\n \treturn success;\n }\n \ndiff --git a/extension/json/json_functions/json_create.cpp b/extension/json/json_functions/json_create.cpp\nindex 9ed6bf1ad068..324338ea6c96 100644\n--- a/extension/json/json_functions/json_create.cpp\n+++ b/extension/json/json_functions/json_create.cpp\n@@ -276,7 +276,7 @@ static void CreateValuesStruct(const StructNames &names, yyjson_mut_doc *doc, yy\n \t\tvals[i] = yyjson_mut_obj(doc);\n \t}\n \t// Initialize re-usable array for the nested values\n-\tauto nested_vals = (yyjson_mut_val **)doc->alc.malloc(doc->alc.ctx, sizeof(yyjson_mut_val *) * count);\n+\tauto nested_vals = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, count);\n \n \t// Add the key/value pairs to the values\n \tauto &entries = StructVector::GetEntries(value_v);\n@@ -301,12 +301,12 @@ static void CreateValuesMap(const StructNames &names, yyjson_mut_doc *doc, yyjso\n \t// Create nested keys\n \tauto &map_key_v = MapVector::GetKeys(value_v);\n \tauto map_key_count = ListVector::GetListSize(value_v);\n-\tauto nested_keys = (yyjson_mut_val **)doc->alc.malloc(doc->alc.ctx, sizeof(yyjson_mut_val *) * map_key_count);\n+\tauto nested_keys = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, map_key_count);\n \tTemplatedCreateValues<string_t, string_t>(doc, nested_keys, map_key_v, map_key_count);\n \t// Create nested values\n \tauto &map_val_v = MapVector::GetValues(value_v);\n \tauto map_val_count = ListVector::GetListSize(value_v);\n-\tauto nested_vals = (yyjson_mut_val **)doc->alc.malloc(doc->alc.ctx, sizeof(yyjson_mut_val *) * map_val_count);\n+\tauto nested_vals = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, map_val_count);\n \tCreateValues(names, doc, nested_vals, map_val_v, map_val_count);\n \t// Add the key/value pairs to the values\n \tUnifiedVectorFormat map_data;\n@@ -338,7 +338,7 @@ static void CreateValuesUnion(const StructNames &names, yyjson_mut_doc *doc, yyj\n \t}\n \n \t// Initialize re-usable array for the nested values\n-\tauto nested_vals = (yyjson_mut_val **)doc->alc.malloc(doc->alc.ctx, sizeof(yyjson_mut_val *) * count);\n+\tauto nested_vals = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, count);\n \n \tauto &tag_v = UnionVector::GetTags(value_v);\n \tUnifiedVectorFormat tag_data;\n@@ -384,7 +384,7 @@ static void CreateValuesList(const StructNames &names, yyjson_mut_doc *doc, yyjs\n \t// Initialize array for the nested values\n \tauto &child_v = ListVector::GetEntry(value_v);\n \tauto child_count = ListVector::GetListSize(value_v);\n-\tauto nested_vals = (yyjson_mut_val **)doc->alc.malloc(doc->alc.ctx, sizeof(yyjson_mut_val *) * child_count);\n+\tauto nested_vals = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, child_count);\n \t// Fill nested_vals with list values\n \tCreateValues(names, doc, nested_vals, child_v, child_count);\n \t// Now we add the values to the appropriate JSON arrays\n@@ -501,12 +501,12 @@ static void ObjectFunction(DataChunk &args, ExpressionState &state, Vector &resu\n \t// Initialize values\n \tconst idx_t count = args.size();\n \tauto doc = JSONCommon::CreateDocument(alc);\n-\tauto objs = (yyjson_mut_val **)alc->malloc(alc->ctx, sizeof(yyjson_mut_val *) * count);\n+\tauto objs = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, count);\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tobjs[i] = yyjson_mut_obj(doc);\n \t}\n \t// Initialize a re-usable value array\n-\tauto vals = (yyjson_mut_val **)alc->malloc(alc->ctx, sizeof(yyjson_mut_val *) * count);\n+\tauto vals = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, count);\n \t// Loop through key/value pairs\n \tfor (idx_t pair_idx = 0; pair_idx < args.data.size() / 2; pair_idx++) {\n \t\tVector &key_v = args.data[pair_idx * 2];\n@@ -533,12 +533,12 @@ static void ArrayFunction(DataChunk &args, ExpressionState &state, Vector &resul\n \t// Initialize arrays\n \tconst idx_t count = args.size();\n \tauto doc = JSONCommon::CreateDocument(alc);\n-\tauto arrs = (yyjson_mut_val **)alc->malloc(alc->ctx, sizeof(yyjson_mut_val *) * count);\n+\tauto arrs = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, count);\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tarrs[i] = yyjson_mut_arr(doc);\n \t}\n \t// Initialize a re-usable value array\n-\tauto vals = (yyjson_mut_val **)alc->malloc(alc->ctx, sizeof(yyjson_mut_val *) * count);\n+\tauto vals = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, count);\n \t// Loop through args\n \tfor (auto &v : args.data) {\n \t\tCreateValues(info.const_struct_names, doc, vals, v, count);\n@@ -561,7 +561,7 @@ static void ToJSONFunctionInternal(const StructNames &names, Vector &input, cons\n                                    yyjson_alc *alc) {\n \t// Initialize array for values\n \tauto doc = JSONCommon::CreateDocument(alc);\n-\tauto vals = (yyjson_mut_val **)alc->malloc(alc->ctx, sizeof(yyjson_mut_val *) * count);\n+\tauto vals = JSONCommon::AllocateArray<yyjson_mut_val *>(doc, count);\n \tCreateValues(names, doc, vals, input, count);\n \n \t// Write JSON values to string\ndiff --git a/extension/json/json_functions/json_merge_patch.cpp b/extension/json/json_functions/json_merge_patch.cpp\nindex e2f50b6122de..e3d72ba77715 100644\n--- a/extension/json/json_functions/json_merge_patch.cpp\n+++ b/extension/json/json_functions/json_merge_patch.cpp\n@@ -59,11 +59,11 @@ static void MergePatchFunction(DataChunk &args, ExpressionState &state, Vector &\n \tconst auto count = args.size();\n \n \t// Read the first json arg\n-\tauto origs = (yyjson_mut_val **)alc->malloc(alc->ctx, sizeof(yyjson_mut_val *) * count);\n+\tauto origs = JSONCommon::AllocateArray<yyjson_mut_val *>(alc, count);\n \tReadObjects(doc, args.data[0], origs, count);\n \n \t// Read the next json args one by one and merge them into the first json arg\n-\tauto patches = (yyjson_mut_val **)alc->malloc(alc->ctx, sizeof(yyjson_mut_val *) * count);\n+\tauto patches = JSONCommon::AllocateArray<yyjson_mut_val *>(alc, count);\n \tfor (idx_t arg_idx = 1; arg_idx < args.data.size(); arg_idx++) {\n \t\tReadObjects(doc, args.data[arg_idx], patches, count);\n \t\tfor (idx_t i = 0; i < count; i++) {\ndiff --git a/extension/json/json_functions/json_serialize_sql.cpp b/extension/json/json_functions/json_serialize_sql.cpp\nindex 288833f19642..ff941f1bcf4f 100644\n--- a/extension/json/json_functions/json_serialize_sql.cpp\n+++ b/extension/json/json_functions/json_serialize_sql.cpp\n@@ -108,7 +108,7 @@ static void JsonSerializeFunction(DataChunk &args, ExpressionState &state, Vecto\n \t\t\tidx_t len;\n \t\t\tauto data = yyjson_mut_val_write_opts(result_obj,\n \t\t\t                                      info.format ? JSONCommon::WRITE_PRETTY_FLAG : JSONCommon::WRITE_FLAG,\n-\t\t\t                                      alc, (size_t *)&len, nullptr);\n+\t\t\t                                      alc, reinterpret_cast<size_t *>(&len), nullptr);\n \t\t\tif (data == nullptr) {\n \t\t\t\tthrow SerializationException(\n \t\t\t\t    \"Failed to serialize json, perhaps the query contains invalid utf8 characters?\");\n@@ -124,7 +124,7 @@ static void JsonSerializeFunction(DataChunk &args, ExpressionState &state, Vecto\n \t\t\tidx_t len;\n \t\t\tauto data = yyjson_mut_val_write_opts(result_obj,\n \t\t\t                                      info.format ? JSONCommon::WRITE_PRETTY_FLAG : JSONCommon::WRITE_FLAG,\n-\t\t\t                                      alc, (size_t *)&len, nullptr);\n+\t\t\t                                      alc, reinterpret_cast<size_t *>(&len), nullptr);\n \t\t\treturn StringVector::AddString(result, data, len);\n \t\t}\n \t});\ndiff --git a/extension/json/json_functions/json_structure.cpp b/extension/json/json_functions/json_structure.cpp\nindex b9b8df81fe73..d95024001f73 100644\n--- a/extension/json/json_functions/json_structure.cpp\n+++ b/extension/json/json_functions/json_structure.cpp\n@@ -150,7 +150,8 @@ void JSONStructureNode::RefineCandidateTypesArray(yyjson_val *vals[], idx_t coun\n \t}\n \n \tidx_t offset = 0;\n-\tauto child_vals = (yyjson_val **)allocator.AllocateAligned(total_list_size * sizeof(yyjson_val *));\n+\tauto child_vals =\n+\t    reinterpret_cast<yyjson_val **>(allocator.AllocateAligned(total_list_size * sizeof(yyjson_val *)));\n \n \tsize_t idx, max;\n \tyyjson_val *child_val;\n@@ -173,11 +174,12 @@ void JSONStructureNode::RefineCandidateTypesObject(yyjson_val *vals[], idx_t cou\n \tvector<yyjson_val **> child_vals;\n \tchild_vals.reserve(child_count);\n \tfor (idx_t child_idx = 0; child_idx < child_count; child_idx++) {\n-\t\tchild_vals.emplace_back((yyjson_val **)allocator.AllocateAligned(count * sizeof(yyjson_val *)));\n+\t\tchild_vals.emplace_back(\n+\t\t    reinterpret_cast<yyjson_val **>(allocator.AllocateAligned(count * sizeof(yyjson_val *))));\n \t}\n \n \tidx_t found_key_count;\n-\tauto found_keys = (bool *)allocator.AllocateAligned(sizeof(bool) * child_count);\n+\tauto found_keys = reinterpret_cast<bool *>(allocator.AllocateAligned(sizeof(bool) * child_count));\n \n \tconst auto &key_map = desc.key_map;\n \tsize_t idx, max;\ndiff --git a/extension/json/json_functions/json_transform.cpp b/extension/json/json_functions/json_transform.cpp\nindex 64fbb34d3eba..1225964796e4 100644\n--- a/extension/json/json_functions/json_transform.cpp\n+++ b/extension/json/json_functions/json_transform.cpp\n@@ -216,7 +216,7 @@ static inline bool GetValueString(yyjson_val *val, yyjson_alc *alc, string_t &re\n \n template <class T>\n static bool TransformNumerical(yyjson_val *vals[], Vector &result, const idx_t count, JSONTransformOptions &options) {\n-\tauto data = (T *)FlatVector::GetData(result);\n+\tauto data = FlatVector::GetData<T>(result);\n \tauto &validity = FlatVector::Validity(result);\n \n \tbool success = true;\n@@ -238,7 +238,7 @@ static bool TransformNumerical(yyjson_val *vals[], Vector &result, const idx_t c\n template <class T>\n static bool TransformDecimal(yyjson_val *vals[], Vector &result, const idx_t count, uint8_t width, uint8_t scale,\n                              JSONTransformOptions &options) {\n-\tauto data = (T *)FlatVector::GetData(result);\n+\tauto data = FlatVector::GetData<T>(result);\n \tauto &validity = FlatVector::Validity(result);\n \n \tbool success = true;\n@@ -373,7 +373,7 @@ static bool TransformFromStringWithFormat(yyjson_val *vals[], Vector &result, co\n }\n \n static bool TransformToString(yyjson_val *vals[], yyjson_alc *alc, Vector &result, const idx_t count) {\n-\tauto data = (string_t *)FlatVector::GetData(result);\n+\tauto data = FlatVector::GetData<string_t>(result);\n \tauto &validity = FlatVector::Validity(result);\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tconst auto &val = vals[i];\n@@ -400,11 +400,11 @@ bool JSONTransform::TransformObject(yyjson_val *objects[], yyjson_alc *alc, cons\n \tnested_vals.reserve(column_count);\n \tfor (idx_t col_idx = 0; col_idx < column_count; col_idx++) {\n \t\tkey_map.insert({{names[col_idx].c_str(), names[col_idx].length()}, col_idx});\n-\t\tnested_vals.push_back((yyjson_val **)alc->malloc(alc->ctx, sizeof(yyjson_val *) * count));\n+\t\tnested_vals.push_back(JSONCommon::AllocateArray<yyjson_val *>(alc, count));\n \t}\n \n \tidx_t found_key_count;\n-\tauto found_keys = (bool *)alc->malloc(alc->ctx, sizeof(bool) * column_count);\n+\tauto found_keys = JSONCommon::AllocateArray<bool>(alc, column_count);\n \n \tbool success = true;\n \n@@ -558,7 +558,7 @@ static bool TransformArray(yyjson_val *arrays[], yyjson_alc *alc, Vector &result\n \tListVector::Reserve(result, offset);\n \n \t// Initialize array for the nested values\n-\tauto nested_vals = (yyjson_val **)alc->malloc(alc->ctx, sizeof(yyjson_val *) * offset);\n+\tauto nested_vals = JSONCommon::AllocateArray<yyjson_val *>(alc, offset);\n \n \t// Get array values\n \tsize_t idx, max;\n@@ -617,8 +617,8 @@ static bool TransformObjectToMap(yyjson_val *objects[], yyjson_alc *alc, Vector\n \tauto list_entries = FlatVector::GetData<list_entry_t>(result);\n \tauto &list_validity = FlatVector::Validity(result);\n \n-\tauto keys = (yyjson_val **)alc->malloc(alc->ctx, sizeof(yyjson_val *) * list_size);\n-\tauto vals = (yyjson_val **)alc->malloc(alc->ctx, sizeof(yyjson_val *) * list_size);\n+\tauto keys = JSONCommon::AllocateArray<yyjson_val *>(alc, list_size);\n+\tauto vals = JSONCommon::AllocateArray<yyjson_val *>(alc, list_size);\n \n \tbool success = true;\n \tidx_t list_offset = 0;\n@@ -675,7 +675,7 @@ static bool TransformObjectToMap(yyjson_val *objects[], yyjson_alc *alc, Vector\n }\n \n bool TransformToJSON(yyjson_val *vals[], yyjson_alc *alc, Vector &result, const idx_t count) {\n-\tauto data = (string_t *)FlatVector::GetData(result);\n+\tauto data = FlatVector::GetData<string_t>(result);\n \tauto &validity = FlatVector::Validity(result);\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tconst auto &val = vals[i];\n@@ -779,8 +779,8 @@ static bool TransformFunctionInternal(Vector &input, const idx_t count, Vector &\n \tauto inputs = UnifiedVectorFormat::GetData<string_t>(input_data);\n \n \t// Read documents\n-\tauto docs = (yyjson_doc **)alc->malloc(alc->ctx, sizeof(yyjson_doc *) * count);\n-\tauto vals = (yyjson_val **)alc->malloc(alc->ctx, sizeof(yyjson_val *) * count);\n+\tauto docs = JSONCommon::AllocateArray<yyjson_doc *>(alc, count);\n+\tauto vals = JSONCommon::AllocateArray<yyjson_val *>(alc, count);\n \tauto &result_validity = FlatVector::Validity(result);\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tauto idx = input_data.sel->get_index(i);\ndiff --git a/extension/json/json_functions/read_json.cpp b/extension/json/json_functions/read_json.cpp\nindex 70914050c8ce..3e49c6462065 100644\n--- a/extension/json/json_functions/read_json.cpp\n+++ b/extension/json/json_functions/read_json.cpp\n@@ -258,7 +258,8 @@ static void ReadJSONFunction(ClientContext &context, TableFunctionInput &data_p,\n \t\t\tstring hint =\n \t\t\t    gstate.bind_data.auto_detect\n \t\t\t        ? \"\\nTry increasing 'sample_size', reducing 'maximum_depth', specifying 'columns', 'format' or \"\n-\t\t\t          \"'records' manually, or setting 'ignore_errors' to true.\"\n+\t\t\t          \"'records' manually, setting 'ignore_errors' to true, or setting 'union_by_name' to true when \"\n+\t\t\t          \"reading multiple files with a different structure.\"\n \t\t\t        : \"\\nTry setting 'auto_detect' to true, specifying 'format' or 'records' manually, or setting \"\n \t\t\t          \"'ignore_errors' to true.\";\n \t\t\tlstate.ThrowTransformError(lstate.transform_options.object_index,\ndiff --git a/extension/json/json_scan.cpp b/extension/json/json_scan.cpp\nindex bb6dc4973f89..fcf949a73951 100644\n--- a/extension/json/json_scan.cpp\n+++ b/extension/json/json_scan.cpp\n@@ -244,14 +244,13 @@ unique_ptr<GlobalTableFunctionState> JSONGlobalTableFunctionState::Init(ClientCo\n \n idx_t JSONGlobalTableFunctionState::MaxThreads() const {\n \tauto &bind_data = state.bind_data;\n-\tif (bind_data.options.format == JSONFormat::NEWLINE_DELIMITED &&\n-\t    bind_data.options.compression == FileCompressionType::UNCOMPRESSED) {\n+\tif (bind_data.options.format == JSONFormat::NEWLINE_DELIMITED) {\n \t\treturn state.system_threads;\n \t}\n \n \tif (!state.json_readers.empty() && state.json_readers[0]->IsOpen()) {\n \t\tauto &reader = *state.json_readers[0];\n-\t\tif (reader.IsParallel()) { // Auto-detected parallel scan\n+\t\tif (reader.GetFormat() == JSONFormat::NEWLINE_DELIMITED) { // Auto-detected NDJSON\n \t\t\treturn state.system_threads;\n \t\t}\n \t}\n@@ -298,6 +297,7 @@ idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {\n \t\tif (!ReadNextBuffer(gstate)) {\n \t\t\treturn scan_count;\n \t\t}\n+\t\tD_ASSERT(buffer_size != 0);\n \t\tif (current_buffer_handle->buffer_index != 0 && current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n \t\t\tReconstructFirstObject(gstate);\n \t\t\tscan_count++;\n@@ -308,8 +308,8 @@ idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {\n \treturn scan_count;\n }\n \n-static inline const char *NextNewline(const char *ptr, idx_t size) {\n-\treturn (const char *)memchr(ptr, '\\n', size);\n+static inline const char *NextNewline(char *ptr, idx_t size) {\n+\treturn char_ptr_cast(memchr(ptr, '\\n', size));\n }\n \n static inline const char *PreviousNewline(const char *ptr) {\n@@ -455,7 +455,21 @@ void JSONScanLocalState::ThrowInvalidAtEndError() {\n \tthrow InvalidInputException(\"Invalid JSON detected at the end of file \\\"%s\\\".\", current_reader->GetFileName());\n }\n \n-static pair<JSONFormat, JSONRecordType> DetectFormatAndRecordType(const char *const buffer_ptr, const idx_t buffer_size,\n+bool JSONScanLocalState::IsParallel(JSONScanGlobalState &gstate) const {\n+\tif (bind_data.files.size() >= gstate.system_threads) {\n+\t\t// More files than threads, just parallelize over the files\n+\t\treturn false;\n+\t}\n+\n+\tif (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n+\t\t// NDJSON can be read in parallel\n+\t\treturn true;\n+\t}\n+\n+\treturn false;\n+}\n+\n+static pair<JSONFormat, JSONRecordType> DetectFormatAndRecordType(char *const buffer_ptr, const idx_t buffer_size,\n                                                                   yyjson_alc *alc) {\n \t// First we do the easy check whether it's NEWLINE_DELIMITED\n \tauto line_end = NextNewline(buffer_ptr, buffer_size);\n@@ -464,7 +478,7 @@ static pair<JSONFormat, JSONRecordType> DetectFormatAndRecordType(const char *co\n \t\tSkipWhitespace(buffer_ptr, line_size, buffer_size);\n \n \t\tyyjson_read_err error;\n-\t\tauto doc = JSONCommon::ReadDocumentUnsafe((char *)buffer_ptr, line_size, JSONCommon::READ_FLAG, alc, &error);\n+\t\tauto doc = JSONCommon::ReadDocumentUnsafe(buffer_ptr, line_size, JSONCommon::READ_FLAG, alc, &error);\n \t\tif (error.code == YYJSON_READ_SUCCESS) { // We successfully read the line\n \t\t\tif (yyjson_is_arr(doc->root) && line_size == buffer_size) {\n \t\t\t\t// It's just one array, let's actually assume ARRAY, not NEWLINE_DELIMITED\n@@ -500,8 +514,8 @@ static pair<JSONFormat, JSONRecordType> DetectFormatAndRecordType(const char *co\n \n \t// It's definitely an ARRAY, but now we have to figure out if there's more than one top-level array\n \tyyjson_read_err error;\n-\tauto doc = JSONCommon::ReadDocumentUnsafe((char *)buffer_ptr + buffer_offset, remaining, JSONCommon::READ_STOP_FLAG,\n-\t                                          alc, &error);\n+\tauto doc =\n+\t    JSONCommon::ReadDocumentUnsafe(buffer_ptr + buffer_offset, remaining, JSONCommon::READ_STOP_FLAG, alc, &error);\n \tif (error.code == YYJSON_READ_SUCCESS) {\n \t\tD_ASSERT(yyjson_is_arr(doc->root));\n \n@@ -563,7 +577,7 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \t} else {\n \t\tbuffer = gstate.allocator.Allocate(gstate.buffer_capacity);\n \t}\n-\tbuffer_ptr = (const char *)buffer.get();\n+\tbuffer_ptr = char_ptr_cast(buffer.get());\n \n \tidx_t buffer_index;\n \twhile (true) {\n@@ -573,7 +587,7 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \t\t\t\tif (is_last && gstate.bind_data.type != JSONScanType::SAMPLE) {\n \t\t\t\t\tcurrent_reader->CloseJSONFile();\n \t\t\t\t}\n-\t\t\t\tif (current_reader->IsParallel()) {\n+\t\t\t\tif (IsParallel(gstate)) {\n \t\t\t\t\t// If this threads' current reader is still the one at gstate.file_index,\n \t\t\t\t\t// this thread can end the parallel scan\n \t\t\t\t\tlock_guard<mutex> guard(gstate.lock);\n@@ -599,7 +613,7 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \t\t\tcurrent_reader = gstate.json_readers[gstate.file_index].get();\n \t\t\tif (current_reader->IsOpen()) {\n \t\t\t\t// Can only be open from auto detection, so these should be known\n-\t\t\t\tif (!current_reader->IsParallel()) {\n+\t\t\t\tif (!IsParallel(gstate)) {\n \t\t\t\t\tbatch_index = gstate.batch_index++;\n \t\t\t\t\tgstate.file_index++;\n \t\t\t\t}\n@@ -609,15 +623,15 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \t\t\tcurrent_reader->OpenJSONFile();\n \t\t\tbatch_index = gstate.batch_index++;\n \t\t\tif (current_reader->GetFormat() != JSONFormat::AUTO_DETECT) {\n-\t\t\t\tif (!current_reader->IsParallel()) {\n+\t\t\t\tif (!IsParallel(gstate)) {\n \t\t\t\t\tgstate.file_index++;\n \t\t\t\t}\n \t\t\t\tcontinue;\n \t\t\t}\n \n-\t\t\t// If we have a low amount of files, we auto-detect within the lock,\n+\t\t\t// If we have less files than threads, we auto-detect within the lock,\n \t\t\t// so other threads may join a parallel NDJSON scan\n-\t\t\tif (gstate.json_readers.size() < 100) {\n+\t\t\tif (gstate.json_readers.size() < gstate.system_threads) {\n \t\t\t\tif (ReadAndAutoDetect(gstate, buffer_index, false)) {\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n@@ -637,7 +651,7 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \tD_ASSERT(buffer_size != 0); // We should have read something if we got here\n \n \tidx_t readers = 1;\n-\tif (current_reader->IsParallel()) {\n+\tif (current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED) {\n \t\treaders = is_last ? 1 : 2;\n \t}\n \n@@ -650,7 +664,7 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \tlines_or_objects_in_buffer = 0;\n \n \t// YYJSON needs this\n-\tmemset((void *)(buffer_ptr + buffer_size), 0, YYJSON_PADDING_SIZE);\n+\tmemset(buffer_ptr + buffer_size, 0, YYJSON_PADDING_SIZE);\n \n \treturn true;\n }\n@@ -680,7 +694,7 @@ bool JSONScanLocalState::ReadAndAutoDetect(JSONScanGlobalState &gstate, idx_t &b\n \t\tthrow InvalidInputException(\"Expected file \\\"%s\\\" to contain records, detected non-record JSON instead.\",\n \t\t                            current_reader->GetFileName());\n \t}\n-\tif (!already_incremented_file_idx && !current_reader->IsParallel()) {\n+\tif (!already_incremented_file_idx && !IsParallel(gstate)) {\n \t\tgstate.file_index++;\n \t}\n \treturn false;\n@@ -739,13 +753,14 @@ void JSONScanLocalState::ReadNextBufferNoSeek(JSONScanGlobalState &gstate, idx_t\n \t\tlock_guard<mutex> reader_guard(current_reader->lock);\n \t\tbuffer_index = current_reader->GetBufferIndex();\n \n-\t\tif (current_reader->IsOpen()) {\n+\t\tif (current_reader->IsOpen() && !current_reader->IsDone()) {\n \t\t\tread_size = current_reader->GetFileHandle().Read(buffer_ptr + prev_buffer_remainder, request_size,\n \t\t\t                                                 gstate.bind_data.type == JSONScanType::SAMPLE);\n+\t\t\tis_last = read_size < request_size;\n \t\t} else {\n \t\t\tread_size = 0;\n+\t\t\tis_last = false;\n \t\t}\n-\t\tis_last = read_size < request_size;\n \n \t\tif (!gstate.bind_data.ignore_errors && read_size == 0 && prev_buffer_remainder != 0) {\n \t\t\tThrowInvalidAtEndError();\n@@ -796,13 +811,13 @@ void JSONScanLocalState::ReconstructFirstObject(JSONScanGlobalState &gstate) {\n \tD_ASSERT(current_reader->GetFormat() == JSONFormat::NEWLINE_DELIMITED);\n \n \t// Spinlock until the previous batch index has also read its buffer\n-\tJSONBufferHandle *previous_buffer_handle = nullptr;\n+\toptional_ptr<JSONBufferHandle> previous_buffer_handle;\n \twhile (!previous_buffer_handle) {\n \t\tprevious_buffer_handle = current_reader->GetBuffer(current_buffer_handle->buffer_index - 1);\n \t}\n \n \t// First we find the newline in the previous block\n-\tauto prev_buffer_ptr = (const char *)previous_buffer_handle->buffer.get() + previous_buffer_handle->buffer_size;\n+\tauto prev_buffer_ptr = char_ptr_cast(previous_buffer_handle->buffer.get()) + previous_buffer_handle->buffer_size;\n \tauto part1_ptr = PreviousNewline(prev_buffer_ptr);\n \tauto part1_size = prev_buffer_ptr - part1_ptr;\n \n@@ -825,7 +840,7 @@ void JSONScanLocalState::ReconstructFirstObject(JSONScanGlobalState &gstate) {\n \n \t// And copy the remainder of the line to the reconstruct buffer\n \tmemcpy(reconstruct_ptr + part1_size, buffer_ptr, part2_size);\n-\tmemset((void *)(reconstruct_ptr + line_size), 0, YYJSON_PADDING_SIZE);\n+\tmemset(reconstruct_ptr + line_size, 0, YYJSON_PADDING_SIZE);\n \tbuffer_offset += part2_size;\n \n \t// We copied the object, so we are no longer reading the previous buffer\n@@ -833,7 +848,7 @@ void JSONScanLocalState::ReconstructFirstObject(JSONScanGlobalState &gstate) {\n \t\tcurrent_reader->RemoveBuffer(current_buffer_handle->buffer_index - 1);\n \t}\n \n-\tParseJSON((char *)reconstruct_ptr, line_size, line_size);\n+\tParseJSON(char_ptr_cast(reconstruct_ptr), line_size, line_size);\n }\n \n void JSONScanLocalState::ParseNextChunk() {\n@@ -867,7 +882,7 @@ void JSONScanLocalState::ParseNextChunk() {\n \t\t}\n \n \t\tidx_t json_size = json_end - json_start;\n-\t\tParseJSON((char *)json_start, json_size, remaining);\n+\t\tParseJSON(json_start, json_size, remaining);\n \t\tbuffer_offset += json_size;\n \n \t\tif (format == JSONFormat::ARRAY) {\n",
  "test_patch": "diff --git a/test/sql/json/test_json_copy.test_slow b/test/sql/json/test_json_copy.test_slow\nindex da6a34dfffdd..7149c54db8d0 100644\n--- a/test/sql/json/test_json_copy.test_slow\n+++ b/test/sql/json/test_json_copy.test_slow\n@@ -248,8 +248,9 @@ set memory_limit='1gb'\n statement ok\n COPY lineitem from '__TEST_DIR__/lineitem.json' (ARRAY)\n \n+# 4gb should be enough for the rest\n statement ok\n-reset memory_limit\n+set memory_limit='4gb'\n \n query I\n PRAGMA tpch(1)\ndiff --git a/test/sql/json/test_json_create.test b/test/sql/json/test_json_create.test\nindex dfe315f34a56..90446d1347a0 100644\n--- a/test/sql/json/test_json_create.test\n+++ b/test/sql/json/test_json_create.test\n@@ -7,6 +7,12 @@ require json\n statement ok\n pragma enable_verification\n \n+# issue #7727\n+query T\n+SELECT TRY_CAST('{{P{P{{{{ASD{AS{D{' AS JSON);\n+----\n+NULL\n+\n query T\n select json_quote({n: 42})\n ----\n",
  "problem_statement": "TRY_CAST doesn't work as expected with JSON\n### What happens?\n\nI'm not sure if this is expected behavior, but `TRY_CAST` has no effect on JSON types. When the string is not a valid json, it returns the string itself instead of NULL. \n\n### To Reproduce\n\n```sql\r\nD SELECT TRY_CAST('{{P{P{{{{ASD{AS{D{' AS JSON);\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 TRY_CAST('{{P{P{{{{ASD{AS{D{' AS JSON) \u2502\r\n\u2502                  json                  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 {{P{P{{{{ASD{AS{D{                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThis is a problem when parsing columns with potentially broken json fields, for example: \r\n```sql\r\nSELECT TRY_CAST(potentially_broken_json_col AS JSON)->>'some_field' FROM some_table\r\n```\r\nIt errors out instead of setting the result to NULL. `IF(json_valid(...)...` can be used as a workaround but it's a bit more verbose. \n\n### OS:\n\nx64\n\n### DuckDB Version:\n\n0.8\n\n### DuckDB Client:\n\ncli\n\n### Full Name:\n\nIgor Calabria\n\n### Affiliation:\n\nIncognia\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nRegression on `read_json_auto` performance between 0.7 and 0.8 for gzipped files\n### What happens?\n\nI am reading some nested JSON and found some discrepancies between version 0.7 (which used to work fine), and 0.8\r\n\r\nHere are different outputs for duckdb 0.7 (named `duckdb7`) and 0.8 (named `duckdb`) \r\n\r\n```\r\n# with 3000 entries in the sample\r\n\r\ntime duckdb7 :memory: \"select *  from read_json_auto('myfile.json.gz', sample_size=3000) limit 0;\"\r\nduckdb7 :memory:   0.18s user 0.01s system 100% cpu 0.196 total\r\n\r\ntime duckdb :memory: \"select *  from read_json_auto('myfile.json.gz', sample_size=3000) limit 0;\"\r\nduckdb :memory:   0.32s user 0.04s system 100% cpu 0.357 total\r\n\r\n# with 5000 entries in the sample\r\n\r\ntime duckdb7 :memory: \"select *  from read_json_auto('myfile.json.gz', sample_size=5000) limit 0;\"\r\nduckdb7 :memory:   0.27s user 0.02s system 99% cpu 0.297 total\r\n\r\ntime duckdb :memory: \"select *  from read_json_auto('myfile.json.gz', sample_size=5000) limit 0;\"\r\n# does not finish after several minutes, the CPU is at 100% and Memory usage is very low (2%)\r\n```\r\n\r\nAdding `maximum_depth=1` did not change the 0.8 behaviour\r\n\r\nI tried the non-gzipped version of my file:\r\n- duckdb 0.7 returned results in 0.097s\r\n- duckdb 0.8 successfully returned results this time, but in 0.398s (x4 compared to 0.7)\n\n### To Reproduce\n\nRun `read_json_auto` on a gzipped JSON file.\n\n### OS:\n\nx86_64\n\n### DuckDB Version:\n\nv0.8.0 e8e4cea5ec\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nBenoit Perigaud\n\n### Affiliation:\n\nNA\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nTRY_CAST doesn't work as expected with JSON\n### What happens?\n\nI'm not sure if this is expected behavior, but `TRY_CAST` has no effect on JSON types. When the string is not a valid json, it returns the string itself instead of NULL. \n\n### To Reproduce\n\n```sql\r\nD SELECT TRY_CAST('{{P{P{{{{ASD{AS{D{' AS JSON);\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 TRY_CAST('{{P{P{{{{ASD{AS{D{' AS JSON) \u2502\r\n\u2502                  json                  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 {{P{P{{{{ASD{AS{D{                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThis is a problem when parsing columns with potentially broken json fields, for example: \r\n```sql\r\nSELECT TRY_CAST(potentially_broken_json_col AS JSON)->>'some_field' FROM some_table\r\n```\r\nIt errors out instead of setting the result to NULL. `IF(json_valid(...)...` can be used as a workaround but it's a bit more verbose. \n\n### OS:\n\nx64\n\n### DuckDB Version:\n\n0.8\n\n### DuckDB Client:\n\ncli\n\n### Full Name:\n\nIgor Calabria\n\n### Affiliation:\n\nIncognia\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "\n\n",
  "created_at": "2023-05-31T14:51:35Z"
}