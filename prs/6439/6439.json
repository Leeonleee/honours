{
  "repo": "duckdb/duckdb",
  "pull_number": 6439,
  "instance_id": "duckdb__duckdb-6439",
  "issue_numbers": [
    "5685"
  ],
  "base_commit": "ae3510f06970e30f0078258e68d74d44a9b80cac",
  "patch": "diff --git a/extension/httpfs/httpfs-extension.cpp b/extension/httpfs/httpfs-extension.cpp\nindex 0d1784cc5a98..6de8745735c7 100644\n--- a/extension/httpfs/httpfs-extension.cpp\n+++ b/extension/httpfs/httpfs-extension.cpp\n@@ -39,6 +39,8 @@ static void LoadInternal(DatabaseInstance &instance) {\n \tconfig.AddExtensionOption(\"s3_url_style\", \"S3 url style ('vhost' (default) or 'path')\", LogicalType::VARCHAR,\n \t                          Value(\"vhost\"));\n \tconfig.AddExtensionOption(\"s3_use_ssl\", \"S3 use SSL (default true)\", LogicalType::BOOLEAN, Value(true));\n+\tconfig.AddExtensionOption(\"s3_url_compatibility_mode\", \"Disable Globs and Query Parameters on S3 urls\",\n+\t                          LogicalType::BOOLEAN, Value(false));\n \n \t// S3 Uploader config\n \tconfig.AddExtensionOption(\"s3_uploader_max_filesize\",\ndiff --git a/extension/httpfs/httpfs.cpp b/extension/httpfs/httpfs.cpp\nindex 068b6115f5ce..cbba4918f7be 100644\n--- a/extension/httpfs/httpfs.cpp\n+++ b/extension/httpfs/httpfs.cpp\n@@ -99,7 +99,7 @@ RunRequestWithRetry(const std::function<duckdb_httplib_openssl::Result(void)> &r\n \t\t\tcase 504: // Server has error\n \t\t\t\tbreak;\n \t\t\tdefault:\n-\t\t\t\treturn make_unique<ResponseWrapper>(response);\n+\t\t\t\treturn make_unique<ResponseWrapper>(response, url);\n \t\t\t}\n \t\t}\n \n@@ -128,7 +128,7 @@ RunRequestWithRetry(const std::function<duckdb_httplib_openssl::Result(void)> &r\n \n unique_ptr<ResponseWrapper> HTTPFileSystem::PostRequest(FileHandle &handle, string url, HeaderMap header_map,\n                                                         unique_ptr<char[]> &buffer_out, idx_t &buffer_out_len,\n-                                                        char *buffer_in, idx_t buffer_in_len) {\n+                                                        char *buffer_in, idx_t buffer_in_len, string params) {\n \tauto &hfs = (HTTPFileHandle &)handle;\n \tstring path, proto_host_port;\n \tParseUrl(url, path, proto_host_port);\n@@ -186,7 +186,7 @@ unique_ptr<duckdb_httplib_openssl::Client> HTTPFileSystem::GetClient(const HTTPP\n }\n \n unique_ptr<ResponseWrapper> HTTPFileSystem::PutRequest(FileHandle &handle, string url, HeaderMap header_map,\n-                                                       char *buffer_in, idx_t buffer_in_len) {\n+                                                       char *buffer_in, idx_t buffer_in_len, string params) {\n \tauto &hfs = (HTTPFileHandle &)handle;\n \tstring path, proto_host_port;\n \tParseUrl(url, path, proto_host_port);\n@@ -282,31 +282,17 @@ HTTPFileHandle::HTTPFileHandle(FileSystem &fs, string path, uint8_t flags, const\n       file_offset(0), buffer_start(0), buffer_end(0) {\n }\n \n-unique_ptr<HTTPFileHandle> HTTPFileSystem::CreateHandle(const string &path, const string &query_param, uint8_t flags,\n-                                                        FileLockType lock, FileCompressionType compression,\n-                                                        FileOpener *opener) {\n+unique_ptr<HTTPFileHandle> HTTPFileSystem::CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n+                                                        FileCompressionType compression, FileOpener *opener) {\n \tD_ASSERT(compression == FileCompressionType::UNCOMPRESSED);\n-\treturn duckdb::make_unique<HTTPFileHandle>(*this, query_param.empty() ? path : path + \"?\" + query_param, flags,\n-\t                                           HTTPParams::ReadFrom(opener));\n+\treturn duckdb::make_unique<HTTPFileHandle>(*this, path, flags, HTTPParams::ReadFrom(opener));\n }\n \n unique_ptr<FileHandle> HTTPFileSystem::OpenFile(const string &path, uint8_t flags, FileLockType lock,\n                                                 FileCompressionType compression, FileOpener *opener) {\n \tD_ASSERT(compression == FileCompressionType::UNCOMPRESSED);\n \n-\t// splitting query params from base path\n-\tstring stripped_path, query_param;\n-\n-\tauto question_pos = path.find_last_of('?');\n-\tif (question_pos == string::npos) {\n-\t\tstripped_path = path;\n-\t\tquery_param = \"\";\n-\t} else {\n-\t\tstripped_path = path.substr(0, question_pos);\n-\t\tquery_param = path.substr(question_pos + 1);\n-\t}\n-\n-\tauto handle = CreateHandle(stripped_path, query_param, flags, lock, compression, opener);\n+\tauto handle = CreateHandle(path, flags, lock, compression, opener);\n \thandle->Initialize(opener);\n \treturn std::move(handle);\n }\n@@ -499,8 +485,8 @@ void HTTPFileHandle::Initialize(FileOpener *opener) {\n \t\t\tlength = 0;\n \t\t\treturn;\n \t\t} else {\n-\t\t\tthrow IOException(\"Unable to connect to URL \\\"\" + path + \"\\\": \" + to_string(res->code) + \" (\" + res->error +\n-\t\t\t                  \")\");\n+\t\t\tthrow IOException(\"Unable to connect to URL \\\"\" + res->http_url + \"\\\": \" + to_string(res->code) + \" (\" +\n+\t\t\t                  res->error + \")\");\n \t\t}\n \t}\n \n@@ -547,12 +533,13 @@ void HTTPFileHandle::InitializeClient() {\n \thttp_client = HTTPFileSystem::GetClient(this->http_params, proto_host_port.c_str());\n }\n \n-ResponseWrapper::ResponseWrapper(duckdb_httplib_openssl::Response &res) {\n+ResponseWrapper::ResponseWrapper(duckdb_httplib_openssl::Response &res, string &original_url) {\n \tcode = res.status;\n \terror = res.reason;\n \tfor (auto &h : res.headers) {\n \t\theaders[h.first] = h.second;\n \t}\n+\thttp_url = original_url;\n }\n \n HTTPFileHandle::~HTTPFileHandle() = default;\ndiff --git a/extension/httpfs/include/httpfs.hpp b/extension/httpfs/include/httpfs.hpp\nindex 6a88fa2ae3e3..67577c163a6e 100644\n--- a/extension/httpfs/include/httpfs.hpp\n+++ b/extension/httpfs/include/httpfs.hpp\n@@ -19,10 +19,11 @@ using HeaderMap = case_insensitive_map_t<string>;\n // avoid including httplib in header\n struct ResponseWrapper {\n public:\n-\texplicit ResponseWrapper(duckdb_httplib_openssl::Response &res);\n+\texplicit ResponseWrapper(duckdb_httplib_openssl::Response &res, string &original_url);\n \tint code;\n \tstring error;\n \tHeaderMap headers;\n+\tstring http_url;\n };\n \n struct HTTPParams {\n@@ -91,8 +92,6 @@ class HTTPFileSystem : public FileSystem {\n \t}\n \n \t// HTTP Requests\n-\tvirtual unique_ptr<ResponseWrapper> PutRequest(FileHandle &handle, string url, HeaderMap header_map,\n-\t                                               char *buffer_in, idx_t buffer_in_len);\n \tvirtual unique_ptr<ResponseWrapper> HeadRequest(FileHandle &handle, string url, HeaderMap header_map);\n \t// Get Request with range parameter that GETs exactly buffer_out_len bytes from the url\n \tvirtual unique_ptr<ResponseWrapper> GetRangeRequest(FileHandle &handle, string url, HeaderMap header_map,\n@@ -100,7 +99,9 @@ class HTTPFileSystem : public FileSystem {\n \t// Post Request that can handle variable sized responses without a content-length header (needed for s3 multipart)\n \tvirtual unique_ptr<ResponseWrapper> PostRequest(FileHandle &handle, string url, HeaderMap header_map,\n \t                                                unique_ptr<char[]> &buffer_out, idx_t &buffer_out_len,\n-\t                                                char *buffer_in, idx_t buffer_in_len);\n+\t                                                char *buffer_in, idx_t buffer_in_len, string params = \"\");\n+\tvirtual unique_ptr<ResponseWrapper> PutRequest(FileHandle &handle, string url, HeaderMap header_map,\n+\t                                               char *buffer_in, idx_t buffer_in_len, string params = \"\");\n \n \t// FS methods\n \tvoid Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n@@ -129,9 +130,8 @@ class HTTPFileSystem : public FileSystem {\n \tunique_ptr<HTTPMetadataCache> global_metadata_cache;\n \n protected:\n-\tvirtual unique_ptr<HTTPFileHandle> CreateHandle(const string &path, const string &query_param, uint8_t flags,\n-\t                                                FileLockType lock, FileCompressionType compression,\n-\t                                                FileOpener *opener);\n+\tvirtual unique_ptr<HTTPFileHandle> CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n+\t                                                FileCompressionType compression, FileOpener *opener);\n };\n \n } // namespace duckdb\ndiff --git a/extension/httpfs/include/s3fs.hpp b/extension/httpfs/include/s3fs.hpp\nindex 35ed2958973b..768849b9b211 100644\n--- a/extension/httpfs/include/s3fs.hpp\n+++ b/extension/httpfs/include/s3fs.hpp\n@@ -41,6 +41,7 @@ struct S3AuthParams {\n \tstring endpoint;\n \tstring url_style;\n \tbool use_ssl;\n+\tbool s3_url_compatibility_mode;\n \n \tstatic S3AuthParams ReadFrom(FileOpener *opener);\n };\n@@ -51,6 +52,9 @@ struct ParsedS3Url {\n \tconst string bucket;\n \tconst string path;\n \tconst string query_param;\n+\tconst string trimmed_s3_url;\n+\n+\tstring GetHTTPUrl(S3AuthParams &auth_params, string http_query_string = \"\");\n };\n \n struct S3ConfigParams {\n@@ -95,11 +99,10 @@ class S3FileHandle : public HTTPFileHandle {\n \tfriend class S3FileSystem;\n \n public:\n-\tS3FileHandle(FileSystem &fs, string path_p, const string &stripped_path_p, uint8_t flags,\n-\t             const HTTPParams &http_params, const S3AuthParams &auth_params_p,\n-\t             const S3ConfigParams &config_params_p)\n+\tS3FileHandle(FileSystem &fs, string path_p, uint8_t flags, const HTTPParams &http_params,\n+\t             const S3AuthParams &auth_params_p, const S3ConfigParams &config_params_p)\n \t    : HTTPFileHandle(fs, std::move(path_p), flags, http_params), auth_params(auth_params_p),\n-\t      config_params(config_params_p), stripped_path(stripped_path_p) {\n+\t      config_params(config_params_p) {\n \n \t\tif (flags & FileFlags::FILE_FLAGS_WRITE && flags & FileFlags::FILE_FLAGS_READ) {\n \t\t\tthrow NotImplementedException(\"Cannot open an HTTP file for both reading and writing\");\n@@ -109,7 +112,6 @@ class S3FileHandle : public HTTPFileHandle {\n \t}\n \tS3AuthParams auth_params;\n \tconst S3ConfigParams config_params;\n-\tstring stripped_path;\n \n public:\n \tvoid Close() override;\n@@ -167,15 +169,14 @@ class S3FileSystem : public HTTPFileSystem {\n \tstring GetName() const override;\n \n public:\n-\t// HTTP Requests\n-\tunique_ptr<ResponseWrapper> PostRequest(FileHandle &handle, string url, HeaderMap header_map,\n+\tunique_ptr<ResponseWrapper> HeadRequest(FileHandle &handle, string s3_url, HeaderMap header_map) override;\n+\tunique_ptr<ResponseWrapper> GetRangeRequest(FileHandle &handle, string s3_url, HeaderMap header_map,\n+\t                                            idx_t file_offset, char *buffer_out, idx_t buffer_out_len) override;\n+\tunique_ptr<ResponseWrapper> PostRequest(FileHandle &handle, string s3_url, HeaderMap header_map,\n \t                                        unique_ptr<char[]> &buffer_out, idx_t &buffer_out_len, char *buffer_in,\n-\t                                        idx_t buffer_in_len) override;\n-\tunique_ptr<ResponseWrapper> PutRequest(FileHandle &handle, string url, HeaderMap header_map, char *buffer_in,\n-\t                                       idx_t buffer_in_len) override;\n-\tunique_ptr<ResponseWrapper> HeadRequest(FileHandle &handle, string url, HeaderMap header_map) override;\n-\tunique_ptr<ResponseWrapper> GetRangeRequest(FileHandle &handle, string url, HeaderMap header_map, idx_t file_offset,\n-\t                                            char *buffer_out, idx_t buffer_out_len) override;\n+\t                                        idx_t buffer_in_len, string http_params = \"\") override;\n+\tunique_ptr<ResponseWrapper> PutRequest(FileHandle &handle, string s3_url, HeaderMap header_map, char *buffer_in,\n+\t                                       idx_t buffer_in_len, string http_params = \"\") override;\n \n \tstatic void Verify();\n \n@@ -214,9 +215,8 @@ class S3FileSystem : public HTTPFileSystem {\n \t}\n \n protected:\n-\tunique_ptr<HTTPFileHandle> CreateHandle(const string &path, const string &query_param, uint8_t flags,\n-\t                                        FileLockType lock, FileCompressionType compression,\n-\t                                        FileOpener *opener) override;\n+\tunique_ptr<HTTPFileHandle> CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n+\t                                        FileCompressionType compression, FileOpener *opener) override;\n \n \tvoid FlushBuffer(S3FileHandle &handle, shared_ptr<S3WriteBuffer> write_buffer);\n \tstring GetPayloadHash(char *buffer, idx_t buffer_len);\ndiff --git a/extension/httpfs/s3fs.cpp b/extension/httpfs/s3fs.cpp\nindex 3cc4debd1a89..4106de5c072e 100644\n--- a/extension/httpfs/s3fs.cpp\n+++ b/extension/httpfs/s3fs.cpp\n@@ -169,6 +169,7 @@ S3AuthParams S3AuthParams::ReadFrom(FileOpener *opener) {\n \tstring session_token;\n \tstring endpoint;\n \tstring url_style;\n+\tbool s3_url_compatibility_mode;\n \tbool use_ssl;\n \tValue value;\n \n@@ -211,7 +212,14 @@ S3AuthParams S3AuthParams::ReadFrom(FileOpener *opener) {\n \t\tuse_ssl = true;\n \t}\n \n-\treturn {region, access_key_id, secret_access_key, session_token, endpoint, url_style, use_ssl};\n+\tif (FileOpener::TryGetCurrentSetting(opener, \"s3_url_compatibility_mode\", value)) {\n+\t\ts3_url_compatibility_mode = value.GetValue<bool>();\n+\t} else {\n+\t\ts3_url_compatibility_mode = true;\n+\t}\n+\n+\treturn {region,   access_key_id, secret_access_key, session_token,\n+\t        endpoint, url_style,     use_ssl,           s3_url_compatibility_mode};\n }\n \n S3ConfigParams S3ConfigParams::ReadFrom(FileOpener *opener) {\n@@ -264,9 +272,9 @@ string S3FileSystem::InitializeMultipartUpload(S3FileHandle &file_handle) {\n \tidx_t response_buffer_len = 1000;\n \tauto response_buffer = unique_ptr<char[]> {new char[response_buffer_len]};\n \n-\tstring query_param = \"?\" + UrlEncode(\"uploads\") + \"=\";\n-\tauto res = s3fs.PostRequest(file_handle, file_handle.stripped_path + query_param, {}, response_buffer,\n-\t                            response_buffer_len, nullptr, 0);\n+\tstring query_param = UrlEncode(\"uploads\") + \"=\";\n+\tauto res = s3fs.PostRequest(file_handle, file_handle.path, {}, response_buffer, response_buffer_len, nullptr, 0,\n+\t                            query_param);\n \tstring result(response_buffer.get(), response_buffer_len);\n \n \tauto open_tag_pos = result.find(\"<UploadId>\", 0);\n@@ -291,11 +299,11 @@ void S3FileSystem::UploadBuffer(S3FileHandle &file_handle, shared_ptr<S3WriteBuf\n \tcase_insensitive_map_t<string>::iterator etag_lookup;\n \n \ttry {\n-\t\tres = s3fs.PutRequest(file_handle, file_handle.stripped_path + \"?\" + query_param, {},\n-\t\t                      (char *)write_buffer->Ptr(), write_buffer->idx);\n+\t\tres = s3fs.PutRequest(file_handle, file_handle.path, {}, (char *)write_buffer->Ptr(), write_buffer->idx,\n+\t\t                      query_param);\n \n \t\tif (res->code != 200) {\n-\t\t\tthrow IOException(\"Unable to connect  to URL \" + file_handle.path + \" \" + res->error + \" (HTTP code \" +\n+\t\t\tthrow IOException(\"Unable to connect  to URL \" + res->http_url + \" \" + res->error + \" (HTTP code \" +\n \t\t\t                  to_string(res->code) + \")\");\n \t\t}\n \n@@ -423,9 +431,9 @@ void S3FileSystem::FinalizeMultipartUpload(S3FileHandle &file_handle) {\n \tidx_t response_buffer_len = 1000;\n \tauto response_buffer = unique_ptr<char[]> {new char[response_buffer_len]};\n \n-\tstring query_param = \"?\" + UrlEncode(\"uploadId\") + \"=\" + file_handle.multipart_upload_id;\n-\tauto res = s3fs.PostRequest(file_handle, file_handle.stripped_path + query_param, {}, response_buffer,\n-\t                            response_buffer_len, (char *)body.c_str(), body.length());\n+\tstring query_param = UrlEncode(\"uploadId\") + \"=\" + file_handle.multipart_upload_id;\n+\tauto res = s3fs.PostRequest(file_handle, file_handle.path, {}, response_buffer, response_buffer_len,\n+\t                            (char *)body.c_str(), body.length(), query_param);\n \tstring result(response_buffer.get(), response_buffer_len);\n \n \tauto open_tag_pos = result.find(\"<CompleteMultipartUploadResult\", 0);\n@@ -548,7 +556,7 @@ void S3FileSystem::ReadQueryParams(const string &url_query_param, S3AuthParams &\n }\n \n ParsedS3Url S3FileSystem::S3UrlParse(string url, S3AuthParams &params) {\n-\tstring http_proto, host, bucket, path, query_param;\n+\tstring http_proto, host, bucket, path, query_param, trimmed_s3_url;\n \n \tif (url.rfind(\"s3://\", 0) != 0) {\n \t\tthrow IOException(\"URL needs to start with s3://\");\n@@ -569,16 +577,25 @@ ParsedS3Url S3FileSystem::S3UrlParse(string url, S3AuthParams &params) {\n \t\tpath = \"\";\n \t}\n \n-\tauto question_pos = url.find_last_of('?');\n-\tif (question_pos != string::npos) {\n-\t\tquery_param = url.substr(question_pos + 1);\n-\t}\n-\n-\tif (!query_param.empty() && query_param.find('.') == string::npos) {\n-\t\tpath += url.substr(slash_pos, question_pos - slash_pos);\n-\t} else {\n+\tif (params.s3_url_compatibility_mode) {\n+\t\t// In url compatibility mode, we will ignore any special chars, so query param strings are disabled\n+\t\ttrimmed_s3_url = url;\n \t\tpath += url.substr(slash_pos);\n-\t\tquery_param = \"\";\n+\t} else {\n+\t\t// Parse query parameters\n+\t\tauto question_pos = url.find_first_of('?');\n+\t\tif (question_pos != string::npos) {\n+\t\t\tquery_param = url.substr(question_pos + 1);\n+\t\t\ttrimmed_s3_url = url.substr(0, question_pos);\n+\t\t} else {\n+\t\t\ttrimmed_s3_url = url;\n+\t\t}\n+\n+\t\tif (!query_param.empty()) {\n+\t\t\tpath += url.substr(slash_pos, question_pos - slash_pos);\n+\t\t} else {\n+\t\t\tpath += url.substr(slash_pos);\n+\t\t}\n \t}\n \n \tif (path.empty()) {\n@@ -593,7 +610,7 @@ ParsedS3Url S3FileSystem::S3UrlParse(string url, S3AuthParams &params) {\n \n \thttp_proto = params.use_ssl ? \"https://\" : \"http://\";\n \n-\treturn {http_proto, host, bucket, path, query_param};\n+\treturn {http_proto, host, bucket, path, query_param, trimmed_s3_url};\n }\n \n string S3FileSystem::GetPayloadHash(char *buffer, idx_t buffer_len) {\n@@ -608,73 +625,69 @@ string S3FileSystem::GetPayloadHash(char *buffer, idx_t buffer_len) {\n \t}\n }\n \n-static string get_full_s3_url(S3AuthParams &auth_params, ParsedS3Url parsed_url) {\n-\tstring full_url = parsed_url.http_proto + parsed_url.host + S3FileSystem::UrlEncode(parsed_url.path);\n+string ParsedS3Url::GetHTTPUrl(S3AuthParams &auth_params, string http_query_string) {\n+\tstring full_url = http_proto + host + S3FileSystem::UrlEncode(path);\n \n-\tif (!parsed_url.query_param.empty()) {\n-\t\tfull_url += \"?\" + parsed_url.query_param;\n+\tif (!http_query_string.empty()) {\n+\t\tfull_url += \"?\" + http_query_string;\n \t}\n \treturn full_url;\n }\n \n unique_ptr<ResponseWrapper> S3FileSystem::PostRequest(FileHandle &handle, string url, HeaderMap header_map,\n                                                       unique_ptr<char[]> &buffer_out, idx_t &buffer_out_len,\n-                                                      char *buffer_in, idx_t buffer_in_len) {\n+                                                      char *buffer_in, idx_t buffer_in_len, string http_params) {\n \tauto auth_params = static_cast<S3FileHandle &>(handle).auth_params;\n-\tauto parsed_url = S3UrlParse(url, auth_params);\n-\n-\tstring full_url = get_full_s3_url(auth_params, parsed_url);\n-\tstring post_url;\n+\tauto parsed_s3_url = S3UrlParse(url, auth_params);\n+\tstring http_url = parsed_s3_url.GetHTTPUrl(auth_params, http_params);\n \tauto payload_hash = GetPayloadHash(buffer_in, buffer_in_len);\n+\tauto headers = create_s3_header(parsed_s3_url.path, http_params, parsed_s3_url.host, \"s3\", \"POST\", auth_params, \"\",\n+\t                                \"\", payload_hash, \"application/octet-stream\");\n \n-\tauto headers = create_s3_header(parsed_url.path, parsed_url.query_param, parsed_url.host, \"s3\", \"POST\", auth_params,\n-\t                                \"\", \"\", payload_hash, \"application/octet-stream\");\n-\n-\treturn HTTPFileSystem::PostRequest(handle, full_url, headers, buffer_out, buffer_out_len, buffer_in, buffer_in_len);\n+\treturn HTTPFileSystem::PostRequest(handle, http_url, headers, buffer_out, buffer_out_len, buffer_in, buffer_in_len);\n }\n \n unique_ptr<ResponseWrapper> S3FileSystem::PutRequest(FileHandle &handle, string url, HeaderMap header_map,\n-                                                     char *buffer_in, idx_t buffer_in_len) {\n+                                                     char *buffer_in, idx_t buffer_in_len, string http_params) {\n \tauto auth_params = static_cast<S3FileHandle &>(handle).auth_params;\n-\tauto parsed_url = S3UrlParse(url, auth_params);\n-\tstring full_url = get_full_s3_url(auth_params, parsed_url);\n+\tauto parsed_s3_url = S3UrlParse(url, auth_params);\n+\tstring http_url = parsed_s3_url.GetHTTPUrl(auth_params, http_params);\n \tauto content_type = \"application/octet-stream\";\n \tauto payload_hash = GetPayloadHash(buffer_in, buffer_in_len);\n \n-\tauto headers = create_s3_header(parsed_url.path, parsed_url.query_param, parsed_url.host, \"s3\", \"PUT\", auth_params,\n-\t                                \"\", \"\", payload_hash, content_type);\n-\treturn HTTPFileSystem::PutRequest(handle, full_url, headers, buffer_in, buffer_in_len);\n+\tauto headers = create_s3_header(parsed_s3_url.path, http_params, parsed_s3_url.host, \"s3\", \"PUT\", auth_params, \"\",\n+\t                                \"\", payload_hash, content_type);\n+\treturn HTTPFileSystem::PutRequest(handle, http_url, headers, buffer_in, buffer_in_len);\n }\n \n-unique_ptr<ResponseWrapper> S3FileSystem::HeadRequest(FileHandle &handle, string url, HeaderMap header_map) {\n+unique_ptr<ResponseWrapper> S3FileSystem::HeadRequest(FileHandle &handle, string s3_url, HeaderMap header_map) {\n \tauto auth_params = static_cast<S3FileHandle &>(handle).auth_params;\n-\turl = url.substr(0, url.find_last_of('?'));\n-\tauto parsed_url = S3UrlParse(url, auth_params);\n-\tstring full_url = get_full_s3_url(auth_params, parsed_url);\n-\tauto headers = create_s3_header(parsed_url.path, parsed_url.query_param, parsed_url.host, \"s3\", \"HEAD\", auth_params,\n-\t                                \"\", \"\", \"\", \"\");\n-\treturn HTTPFileSystem::HeadRequest(handle, full_url, headers);\n+\tauto parsed_s3_url = S3UrlParse(s3_url, auth_params);\n+\tstring http_url = parsed_s3_url.GetHTTPUrl(auth_params);\n+\tauto headers =\n+\t    create_s3_header(parsed_s3_url.path, \"\", parsed_s3_url.host, \"s3\", \"HEAD\", auth_params, \"\", \"\", \"\", \"\");\n+\treturn HTTPFileSystem::HeadRequest(handle, http_url, headers);\n }\n \n-unique_ptr<ResponseWrapper> S3FileSystem::GetRangeRequest(FileHandle &handle, string url, HeaderMap header_map,\n+unique_ptr<ResponseWrapper> S3FileSystem::GetRangeRequest(FileHandle &handle, string s3_url, HeaderMap header_map,\n                                                           idx_t file_offset, char *buffer_out, idx_t buffer_out_len) {\n \tauto auth_params = static_cast<S3FileHandle &>(handle).auth_params;\n-\turl = url.substr(0, url.find_last_of('?'));\n-\tauto parsed_url = S3UrlParse(url, auth_params);\n-\tstring full_url = get_full_s3_url(auth_params, parsed_url);\n-\tauto headers = create_s3_header(parsed_url.path, parsed_url.query_param, parsed_url.host, \"s3\", \"GET\", auth_params,\n-\t                                \"\", \"\", \"\", \"\");\n-\treturn HTTPFileSystem::GetRangeRequest(handle, full_url, headers, file_offset, buffer_out, buffer_out_len);\n+\tauto parsed_s3_url = S3UrlParse(s3_url, auth_params);\n+\tstring http_url = parsed_s3_url.GetHTTPUrl(auth_params);\n+\tauto headers =\n+\t    create_s3_header(parsed_s3_url.path, \"\", parsed_s3_url.host, \"s3\", \"GET\", auth_params, \"\", \"\", \"\", \"\");\n+\treturn HTTPFileSystem::GetRangeRequest(handle, http_url, headers, file_offset, buffer_out, buffer_out_len);\n }\n \n-unique_ptr<HTTPFileHandle> S3FileSystem::CreateHandle(const string &path, const string &query_param, uint8_t flags,\n-                                                      FileLockType lock, FileCompressionType compression,\n-                                                      FileOpener *opener) {\n-\tauto s3authparams = S3AuthParams::ReadFrom(opener);\n-\tReadQueryParams(query_param, s3authparams);\n-\tstring full_path = query_param.empty() ? path : path + \"?\" + query_param;\n+unique_ptr<HTTPFileHandle> S3FileSystem::CreateHandle(const string &path, uint8_t flags, FileLockType lock,\n+                                                      FileCompressionType compression, FileOpener *opener) {\n+\tauto auth_params = S3AuthParams::ReadFrom(opener);\n+\n+\t// Scan the query string for any s3 authentication parameters\n+\tauto parsed_s3_url = S3UrlParse(path, auth_params);\n+\tReadQueryParams(parsed_s3_url.query_param, auth_params);\n \n-\treturn duckdb::make_unique<S3FileHandle>(*this, full_path, path, flags, HTTPParams::ReadFrom(opener), s3authparams,\n+\treturn duckdb::make_unique<S3FileHandle>(*this, path, flags, HTTPParams::ReadFrom(opener), auth_params,\n \t                                         S3ConfigParams::ReadFrom(opener));\n }\n \n@@ -838,27 +851,28 @@ vector<string> S3FileSystem::Glob(const string &glob_pattern, FileOpener *opener\n \tif (opener == nullptr) {\n \t\tthrow InternalException(\"Cannot S3 Glob without FileOpener\");\n \t}\n-\t// AWS matches on prefix, not glob pattern so we take a substring until the first wildcard char for the aws calls\n-\tauto first_wildcard_pos = glob_pattern.find_first_of(\"*?[\\\\\");\n-\tif (first_wildcard_pos == string::npos) {\n+\n+\t// Trim any query parameters from the string\n+\tauto s3_auth_params = S3AuthParams::ReadFrom(opener);\n+\n+\t// In url compatibility mode, we ignore globs allowing users to query files with the glob chars\n+\tif (s3_auth_params.s3_url_compatibility_mode) {\n \t\treturn {glob_pattern};\n \t}\n \n-\tauto dot_pos = glob_pattern.find('.');\n-\tif (glob_pattern[first_wildcard_pos] == '?' &&\n-\t    first_wildcard_pos > dot_pos) { // a '?' after the '.' so assuming query parameters\n+\tauto parsed_s3_url = S3UrlParse(glob_pattern, s3_auth_params);\n+\tauto parsed_glob_url = parsed_s3_url.trimmed_s3_url;\n+\n+\t// AWS matches on prefix, not glob pattern, so we take a substring until the first wildcard char for the aws calls\n+\tauto first_wildcard_pos = parsed_glob_url.find_first_of(\"*[\\\\\");\n+\tif (first_wildcard_pos == string::npos) {\n \t\treturn {glob_pattern};\n \t}\n \n-\tstring shared_path = glob_pattern.substr(0, first_wildcard_pos);\n-\n-\tauto s3_auth_params = S3AuthParams::ReadFrom(opener);\n+\tstring shared_path = parsed_glob_url.substr(0, first_wildcard_pos);\n \tauto http_params = HTTPParams::ReadFrom(opener);\n \n-\t// Parse pattern\n-\tauto parsed_url = S3UrlParse(glob_pattern, s3_auth_params);\n-\n-\tReadQueryParams(parsed_url.query_param, s3_auth_params);\n+\tReadQueryParams(parsed_s3_url.query_param, s3_auth_params);\n \n \t// Do main listobjectsv2 request\n \tvector<string> s3_keys;\n@@ -875,7 +889,7 @@ vector<string> S3FileSystem::Glob(const string &glob_pattern, FileOpener *opener\n \t\t// Repeat requests until the keys of all common prefixes are parsed.\n \t\tauto common_prefixes = AWSListObjectV2::ParseCommonPrefix(response_str);\n \t\twhile (!common_prefixes.empty()) {\n-\t\t\tauto prefix_path = \"s3://\" + parsed_url.bucket + '/' + common_prefixes.back();\n+\t\t\tauto prefix_path = \"s3://\" + parsed_s3_url.bucket + '/' + common_prefixes.back();\n \t\t\tcommon_prefixes.pop_back();\n \n \t\t\t// TODO we could optimize here by doing a match on the prefix, if it doesn't match we can skip this prefix\n@@ -893,23 +907,24 @@ vector<string> S3FileSystem::Glob(const string &glob_pattern, FileOpener *opener\n \t\t}\n \t} while (!main_continuation_token.empty());\n \n-\tauto pattern_trimmed = parsed_url.path.substr(1);\n+\tauto pattern_trimmed = parsed_s3_url.path.substr(1);\n \n \t// Trim the bucket prefix for path-style urls\n \tif (s3_auth_params.url_style == \"path\") {\n-\t\tpattern_trimmed = pattern_trimmed.substr(parsed_url.bucket.length() + 1);\n+\t\tpattern_trimmed = pattern_trimmed.substr(parsed_s3_url.bucket.length() + 1);\n \t}\n \n \tvector<string> result;\n \tfor (const auto &s3_key : s3_keys) {\n \n-\t\tauto is_match = LikeFun::Glob(s3_key.data(), s3_key.length(), pattern_trimmed.data(), pattern_trimmed.length());\n+\t\tauto is_match =\n+\t\t    LikeFun::Glob(s3_key.data(), s3_key.length(), pattern_trimmed.data(), pattern_trimmed.length(), false);\n \n \t\tif (is_match) {\n-\t\t\tauto result_full_url = \"s3://\" + parsed_url.bucket + \"/\" + s3_key;\n+\t\t\tauto result_full_url = \"s3://\" + parsed_s3_url.bucket + \"/\" + s3_key;\n \t\t\t// if a ? char was present, we re-add it here as the url parsing will have trimmed it.\n-\t\t\tif (parsed_url.query_param != \"\") {\n-\t\t\t\tresult_full_url += '?' + parsed_url.query_param;\n+\t\t\tif (!parsed_s3_url.query_param.empty()) {\n+\t\t\t\tresult_full_url += '?' + parsed_s3_url.query_param;\n \t\t\t}\n \t\t\tresult.push_back(result_full_url);\n \t\t}\ndiff --git a/src/function/scalar/string/like.cpp b/src/function/scalar/string/like.cpp\nindex 7a23ede547ad..d69c5a9c74d2 100644\n--- a/src/function/scalar/string/like.cpp\n+++ b/src/function/scalar/string/like.cpp\n@@ -220,7 +220,7 @@ bool LikeOperatorFunction(string_t &s, string_t &pat, char escape) {\n \treturn LikeOperatorFunction(s.GetDataUnsafe(), s.GetSize(), pat.GetDataUnsafe(), pat.GetSize(), escape);\n }\n \n-bool LikeFun::Glob(const char *string, idx_t slen, const char *pattern, idx_t plen) {\n+bool LikeFun::Glob(const char *string, idx_t slen, const char *pattern, idx_t plen, bool allow_question_mark) {\n \tidx_t sidx = 0;\n \tidx_t pidx = 0;\n main_loop : {\n@@ -249,8 +249,11 @@ main_loop : {\n \t\t\treturn false;\n \t\t}\n \t\tcase '?':\n-\t\t\t// wildcard: matches anything but null\n-\t\t\tbreak;\n+\t\t\t// when enabled: matches anything but null\n+\t\t\tif (allow_question_mark) {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tDUCKDB_EXPLICIT_FALLTHROUGH;\n \t\tcase '[':\n \t\t\tpidx++;\n \t\t\tgoto parse_bracket;\ndiff --git a/src/include/duckdb/function/scalar/string_functions.hpp b/src/include/duckdb/function/scalar/string_functions.hpp\nindex 7b66225ac6f7..1bdf93505912 100644\n--- a/src/include/duckdb/function/scalar/string_functions.hpp\n+++ b/src/include/duckdb/function/scalar/string_functions.hpp\n@@ -88,7 +88,8 @@ struct LengthFun {\n \n struct LikeFun {\n \tstatic void RegisterFunction(BuiltinFunctions &set);\n-\tDUCKDB_API static bool Glob(const char *s, idx_t slen, const char *pattern, idx_t plen);\n+\tDUCKDB_API static bool Glob(const char *s, idx_t slen, const char *pattern, idx_t plen,\n+\t                            bool allow_question_mark = true);\n };\n \n struct LikeEscapeFun {\ndiff --git a/src/include/duckdb/main/extension_entries.hpp b/src/include/duckdb/main/extension_entries.hpp\nindex 0ab460715223..9a69cb318bf4 100644\n--- a/src/include/duckdb/main/extension_entries.hpp\n+++ b/src/include/duckdb/main/extension_entries.hpp\n@@ -104,6 +104,7 @@ static constexpr ExtensionEntry EXTENSION_SETTINGS[] = {\n     {\"s3_uploader_max_filesize\", \"httpfs\"},\n     {\"s3_uploader_max_parts_per_file\", \"httpfs\"},\n     {\"s3_uploader_thread_limit\", \"httpfs\"},\n+    {\"s3_url_compatibility_mode\", \"httpfs\"},\n     {\"s3_url_style\", \"httpfs\"},\n     {\"s3_use_ssl\", \"httpfs\"},\n     {\"sqlite_all_varchar\", \"sqlite_scanner\"},\n",
  "test_patch": "diff --git a/test/sql/copy/csv/glob/copy_csv_glob_s3.test b/test/sql/copy/csv/glob/copy_csv_glob_s3.test\nindex 3d05803dae90..c6240db8a0a1 100644\n--- a/test/sql/copy/csv/glob/copy_csv_glob_s3.test\n+++ b/test/sql/copy/csv/glob/copy_csv_glob_s3.test\n@@ -33,7 +33,7 @@ statement ok\n CREATE TABLE dates(d DATE);\n \n statement ok\n-COPY dates FROM 's3://test-bucket/copy_csv_glob_s3/copy/a?/*.csv' (AUTO_DETECT 1);\n+COPY dates FROM 's3://test-bucket/copy_csv_glob_s3/copy/a[123]/*.csv' (AUTO_DETECT 1);\n \n # simple globbing for both url styles\n foreach urlstyle path vhost\ndiff --git a/test/sql/copy/csv/glob/read_csv_glob_s3.test b/test/sql/copy/csv/glob/read_csv_glob_s3.test\nindex a3cc5409aa96..c41dddbc9f71 100644\n--- a/test/sql/copy/csv/glob/read_csv_glob_s3.test\n+++ b/test/sql/copy/csv/glob/read_csv_glob_s3.test\n@@ -38,7 +38,7 @@ SET s3_url_style='${urlstyle}'\n \n # simple globbing\n query I\n-SELECT * FROM read_csv('s3://test-bucket/read_csv_glob_s3/glob/a?/*.csv', auto_detect=1) ORDER BY 1\n+SELECT * FROM read_csv('s3://test-bucket/read_csv_glob_s3/glob/a[123]/*.csv', auto_detect=1) ORDER BY 1\n ----\n 2019-06-05\n 2019-06-15\n@@ -51,7 +51,7 @@ SELECT * FROM read_csv('s3://test-bucket/read_csv_glob_s3/glob/a?/*.csv', auto_d\n 2019-08-25\n \n query I\n-SELECT * FROM read_csv('s3://test-bucket/read_csv_glob_s3/*/a?/a*.csv', auto_detect=1) ORDER BY 1\n+SELECT * FROM read_csv('s3://test-bucket/read_csv_glob_s3/*/a[123]/a*.csv', auto_detect=1) ORDER BY 1\n ----\n 2019-06-05\n 2019-06-15\n@@ -61,7 +61,7 @@ SELECT * FROM read_csv('s3://test-bucket/read_csv_glob_s3/*/a?/a*.csv', auto_det\n 2019-07-25\n \n query II\n-SELECT a, b LIKE '%a1.csv%' FROM read_csv('s3://test-bucket/read_csv_glob_s3/gl*/a?/a*.csv', auto_detect=1, filename=1) t1(a,b) ORDER BY 1\n+SELECT a, b LIKE '%a1.csv%' FROM read_csv('s3://test-bucket/read_csv_glob_s3/gl*/a[123]/a*.csv', auto_detect=1, filename=1) t1(a,b) ORDER BY 1\n ----\n 2019-06-05\t1\n 2019-06-15\t1\ndiff --git a/test/sql/copy/parquet/parquet_glob_s3.test b/test/sql/copy/parquet/parquet_glob_s3.test\nindex 15a014e24b03..373f004c6514 100644\n--- a/test/sql/copy/parquet/parquet_glob_s3.test\n+++ b/test/sql/copy/parquet/parquet_glob_s3.test\n@@ -36,7 +36,7 @@ statement ok\n CREATE TABLE vals (i INTEGER, j BLOB)\n \n statement ok\n-COPY vals FROM 's3://test-bucket/parquet_glob_s3/glob/t?.parquet' (FORMAT PARQUET);\n+COPY vals FROM 's3://test-bucket/parquet_glob_s3/glob/t[0-9].parquet' (FORMAT PARQUET);\n \n query II\n SELECT * FROM vals ORDER BY 1, 2\n@@ -58,11 +58,6 @@ statement ok\n SET s3_url_style='${urlstyle}'\n \n # Begin tests\n-query I\n-select count(*) from parquet_scan('s3://test-bucket/parquet_glob_s3/glob/t?.parquet')\n-----\n-2\n-\n query I\n select count(*) from parquet_scan('s3://test-bucket/parquet_glob_s3/glob/t[0-9].parquet')\n ----\n@@ -101,11 +96,6 @@ select count(*) from parquet_scan('s3://test-bucket/parquet_glob_s3/notglob/*.pa\n statement ok\n PRAGMA threads=4\n \n-query I\n-select count(*) from parquet_scan('s3://test-bucket/parquet_glob_s3/glob/t?.parquet')\n-----\n-2\n-\n query I\n select count(*) from parquet_scan('s3://test-bucket/parquet_glob_s3/glob/*')\n ----\n@@ -126,6 +116,33 @@ select count(*) from parquet_scan('s3://test-bucket/parquet_glob_s3/g*/t1.parque\n ----\n 2\n \n+# Question mark is not supported for S3 due to our use of query parameters\n+statement error\n+select count(*) from parquet_scan('s3://test-bucket/parquet_glob_s3/glob/t?.parquet')\n+----\n+IO Error: Invalid query parameters found.\n+\n+statement error\n+select count(*) from parquet_scan('s3://test-bucket/parquet_glob_s3/?lob/t?.parquet')\n+----\n+IO Error: Invalid query parameters found.\n+\n+# Finally, enabling url compatibility mode will disable globs allowing a user to query files with special chars\n+statement ok\n+SET s3_url_compatibility_mode=true;\n+\n+# Note that this is actually a file called '?.*[1-0]parquet??' which S3 should theoretically accept;\n+statement ok\n+COPY vals TO 's3://test-bucket/the_horror/?.*[1-0]parquetta??' (FORMAT parquet);\n+\n+query I\n+select count(*) from parquet_scan('s3://test-bucket/the_horror/?.*[1-0]parquetta??');\n+----\n+2\n+\n+statement ok\n+SET s3_url_compatibility_mode=false;\n+\n endloop\n \n # sanity check for request count\ndiff --git a/test/sql/copy/s3/download_config.test b/test/sql/copy/s3/download_config.test\nindex ec889baa8068..426bcd34c25b 100644\n--- a/test/sql/copy/s3/download_config.test\n+++ b/test/sql/copy/s3/download_config.test\n@@ -108,7 +108,17 @@ COPY test TO 's3://test-bucket-public/root-dir/test2.parquet';\n # 404\n statement error\n SELECT i FROM \"http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/root-dir/non-existent-file-ljaslkjdas.parquet\" LIMIT 3\n+----\n+IO Error: Unable to connect to URL \"http://test-bucket-public.\n \n # Connection error\n statement error\n-SELECT i FROM \"http://test-bucket-public.duckdb-minio-non-existent-host.com:9000/root-dir/non-existent-file-ljaslkjdas.parquet\" LIMIT 3\n\\ No newline at end of file\n+SELECT i FROM \"http://test-bucket-public.duckdb-minio-non-existent-host.com:9000/root-dir/non-existent-file-ljaslkjdas.parquet\" LIMIT 3\n+----\n+IO Error: Connection error for HTTP HEAD to 'http://test-bucket-public.\n+\n+# S3 errors should throw on\n+statement error\n+SELECT * FROM parquet_scan('s3://this-aint-no-bucket/no-path/no-file');\n+----\n+IO Error: Unable to connect to URL \"http://\n\\ No newline at end of file\ndiff --git a/test/sql/copy/s3/fully_qualified_s3_url.test b/test/sql/copy/s3/fully_qualified_s3_url.test\nindex 051fbac3103b..88d3eb156eb0 100644\n--- a/test/sql/copy/s3/fully_qualified_s3_url.test\n+++ b/test/sql/copy/s3/fully_qualified_s3_url.test\n@@ -36,7 +36,7 @@ SET s3_url_style='path';\n statement error\n COPY test TO 's3://test-bucket/s3_query_params/test.csv';\n ----\n-IO Error: Unable to connect to URL \"s3://test-bucket/s3_query_params/test.csv\": 403 (Forbidden)\n+IO Error: Unable to connect to URL\n \n #test with .csv file\n statement ok\n@@ -68,14 +68,6 @@ SELECT i FROM \"s3://test-bucket/s3_query_params/*.parquet?s3_access_key_id=${AWS\n 1\n 2\n \n-#test GLOB with .parquet file\n-query I\n-SELECT i FROM \"s3://test-bucket/s3_query_params/????.parquet?s3_access_key_id=${AWS_ACCESS_KEY_ID}&s3_secret_access_key=${AWS_SECRET_ACCESS_KEY}\" LIMIT 3\n-----\n-0\n-1\n-2\n-\n #global settings have not been modified by query parameters\n query I\n SELECT CURRENT_SETTING('s3_access_key_id');\ndiff --git a/test/sql/copy/s3/url_encode.test b/test/sql/copy/s3/url_encode.test\nindex 708d2c0ed8f4..f17f846eccf5 100644\n--- a/test/sql/copy/s3/url_encode.test\n+++ b/test/sql/copy/s3/url_encode.test\n@@ -27,6 +27,7 @@ statement ok\n CREATE TABLE test_1 as (SELECT 1 FROM range(0,5));\n CREATE TABLE test_2 as (SELECT 2 FROM range(0,5));\n CREATE TABLE test_3 as (SELECT 3 FROM range(0,5));\n+CREATE TABLE test_4 as (SELECT 4 FROM range(0,5));\n \n statement ok\n COPY test_1 TO 's3://test-bucket-public/url_encode/just because you can doesnt mean you should.parquet' (FORMAT 'parquet');\n@@ -79,3 +80,31 @@ query I\n SELECT * FROM \"http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/just+dont+use+plus+or+spaces+please.parquet\" LIMIT 1;\n ----\n 2\n+\n+# Due to our support for query parameters, this will fail\n+statement error\n+COPY test_4 TO 's3://test-bucket-public/url_encode/question?marks?are?even?worse.parquet' (FORMAT 'parquet');\n+----\n+Invalid query parameters found.\n+\n+# Enabling url compatibility mode will disable both Globs and query params\n+# allowing a user to query those hard-to-reach files\n+statement ok\n+SET s3_url_compatibility_mode=true;\n+\n+statement ok\n+COPY test_4 TO 's3://test-bucket-public/url_encode/question?marks?and*stars[and]brackets.parquet' (FORMAT 'parquet');\n+\n+query I\n+SELECT * FROM \"s3://test-bucket-public/url_encode/question?marks?and*stars[and]brackets.parquet\" LIMIT 1;\n+----\n+4\n+\n+# HTTP urls will be encoded here\n+query I\n+SELECT * FROM \"http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/question%3Fmarks%3Fand%2Astars%5Band%5Dbrackets.parquet\" LIMIT 1;\n+----\n+4\n+\n+statement ok\n+SET s3_url_compatibility_mode=false;\n\\ No newline at end of file\n",
  "problem_statement": "Error when coping data to Cloudflare R2\n### What happens?\n\nusing the S3 API, when I try to copy a file to R2, I get this error\r\n`duckdb.IOException: IO Error: Unexpected response during S3 multipart upload finalization`\r\n\r\nthe same file was fine using AWS Boto package\n\n### To Reproduce\n\n```\r\nimport streamlit as st\r\nimport duckdb\r\ncon=duckdb.connect()\r\n\r\ncon.execute(f'''\r\ninstall httpfs;\r\nLOAD httpfs;\r\nPRAGMA enable_object_cache ;\r\nset s3_region = 'auto';\r\nset s3_access_key_id = \"{st.secrets[\"aws_access_key_id_secret\"]}\" ;\r\nset s3_secret_access_key = '{st.secrets[\"aws_secret_access_key_secret\"] }';\r\nset s3_endpoint = '{st.secrets[\"endpoint_url_secret\"]}'  ;\r\nSET s3_url_style='path';\r\nCOPY (select * from 'Parquet/0.1/lineitem.parquet' ) to 's3://delta/1/lineitem.parquet'\r\n''')\r\n```\n\n### OS:\n\nwindows 10\n\n### DuckDB Version:\n\n0.6.2.dev480\n\n### DuckDB Client:\n\npython\n\n### Full Name:\n\nmimoune djouallah\n\n### Affiliation:\n\nPersonal project\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "@samansmink I am running into the exact same issue. Were you able to identify the root cause? In my case the target is minio. \n@ruchirj thanks for reporting this, I haven't had time yet to look into this due to the holidays, but its interesting to hear that you run into this in Minio, as we test all S3 code with Minio in our CI. I see however that we have a somewhat old version pinned. I'll try to find some time either this or next week to look into this\n@samansmink I use the latest `0.6.1` version of the Java DuckDB library and the latest version of Minio. Which versions are you using because if this error goes away by back pinning it would help unblock me while you look into this and by the way appreciate that you are! \n@ruchirj our minio config for CI is here: https://github.com/duckdb/duckdb/blob/master/scripts/minio_s3.yml, so we're using minio/minio:RELEASE.2021-11-03T03-36-36Z currently \nsame issue here, duckdb 0.7.0 and actual s3, was working with duckdb 0.6.1.\r\nAlso, we find a file in the bucket that has this name: `s3://BUCKET/duckdb-importer/hubs.parquet%3FpartNumber%3D1%26uploadId%3XXXX`\nWe've recently hit this issue and resolved it by setting the correct S3 permissions (i.e. allowed multipart operations).\n@complex64 we should have full permissions on the bucket already in place. Any quick way to check if we are missing some specific capability? (I checked through AWS console and aws cli, everything seemed fine)\r\n\nWe are also seeing this now after trying to upgrade from 0.6.1 to 0.7.0.\r\n\r\nThe IAM user has AmazonS3FullAccess.\nI'm working on adding improved http error reporting support, but in the meantime, if one of you could use Wireshark to look at the error you're experiencing, that would help us look into it\nI have the same issue with latest master and the correct permissions are set on the execution role according to https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#mpuAndPermissions\r\n\r\nI also verified that giving `s3:*` permission doesn't change the behavior unfortunately. There are files written to S3, in a fashion that https://github.com/duckdb/duckdb/issues/5685#issuecomment-1430024217 outlines...\r\n\r\nThe error seems to be raised here: https://github.com/duckdb/duckdb/blob/master/extension/httpfs/s3fs.cpp#L432-L434\r\n\r\ncc @samansmink \nI compared v0.6.1 and v0.7.0, there didn't change much in the `FinalizeMultipartUpload` method:  \r\nhttps://github.com/duckdb/duckdb/compare/v0.6.1...v0.7.0#diff-9ff786b314ba325d78b3dda594cb8d35bc53a57e8096febcc2c1c9bb13cd28efR405-R436\r\n\r\nI don't think that this is the reason for this bug (not a C++ expert at all though)...",
  "created_at": "2023-02-23T11:18:56Z"
}