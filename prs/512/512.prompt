You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
SIMILAR TO results in an incorrect result
Consider the following statements:

```sql
CREATE TABLE t0(c0 INT);
INSERT INTO t0(c0) VALUES (-10);
SELECT * FROM t0 WHERE t0.c0 NOT SIMILAR TO 0; -- expected: {-10}, actual: {}
```
Unexpectedly, the row in `t0` is not fetched. As expected, the negated predicate does not fetch any rows:
```sql
SELECT * FROM t0 WHERE t0.c0 SIMILAR TO 0; -- {}
```

I'm a bit confused about this case, since DuckDB seems to indicate that the negated predicate should evaluate to TRUE:
```sql
SELECT t0.c0 SIMILAR TO 0 FROM t0; -- true
```

I found this bug based on the latest master commit (340d867b232b56faf198594e40adba733ecb7bc8).

</issue>
<code>
[start of README.md]
1: <img align="left" src="logo/duckdb-logo.png" height="120">
2: 
3: # DuckDB, the SQLite for Analytics
4: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
5: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
6: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
7: 
8: <br>
9: 
10: 
11: # Requirements
12: DuckDB requires [CMake](https://cmake.org) to be installed and a `C++11` compliant compiler. GCC 4.9 and newer, Clang 3.9 and newer and VisualStudio 2017 are tested on each revision.
13: 
14: ## Compiling
15: Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You may run `make unit` and `make allunit` to verify that your version works properly after making changes.
16: 
17: # Usage
18: A command line utility based on `sqlite3` can be found in either `build/release/duckdb_cli` (release, the default) or `build/debug/duckdb_cli` (debug).
19: 
20: # Embedding
21: As DuckDB is an embedded database, there is no database server to launch or client to connect to a running server. However, the database server can be embedded directly into an application using the C or C++ bindings. The main build process creates the shared library `build/release/src/libduckdb.[so|dylib|dll]` that can be linked against. A static library is built as well.
22: 
23: For examples on how to embed DuckDB into your application, see the [examples](https://github.com/cwida/duckdb/tree/master/examples) folder.
24: 
25: ## Benchmarks
26: After compiling, benchmarks can be executed from the root directory by executing `./build/release/benchmark/benchmark_runner`.
27: 
28: ## Standing on the Shoulders of Giants
29: DuckDB is implemented in C++ 11, should compile with GCC and clang, uses CMake to build and [Catch2](https://github.com/catchorg/Catch2) for testing. DuckDB uses some components from various Open-Source databases and draws inspiration from scientific publications. Here is an overview:
30: 
31: * Parser: We use the PostgreSQL parser that was [repackaged as a stand-alone library](https://github.com/lfittl/libpg_query). The translation to our own parse tree is inspired by [Peloton](https://pelotondb.io).
32: * Shell: We have adapted the [SQLite shell](https://sqlite.org/cli.html) to work with DuckDB.
33: * Tests: We use the [SQL Logic Tests from SQLite](https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki) to test DuckDB.
34: * Query fuzzing: We use [SQLsmith](https://github.com/anse1/sqlsmith) to generate random queries for additional testing.
35: * Date Math: We use the date math component from [MonetDB](https://www.monetdb.org).
36: * SQL Window Functions: DuckDB's window functions implementation uses Segment Tree Aggregation as described in the paper "Efficient Processing of Window Functions in Analytical SQL Queries" by Viktor Leis, Kan Kundhikanjana, Alfons Kemper and Thomas Neumann.
37: * Execution engine: The vectorized execution engine is inspired by the paper "MonetDB/X100: Hyper-Pipelining Query Execution" by Peter Boncz, Marcin Zukowski and Niels Nes.
38: * Optimizer: DuckDB's optimizer draws inspiration from the papers "Dynamic programming strikes back" by Guido Moerkotte and Thomas Neumman as well as "Unnesting Arbitrary Queries" by Thomas Neumann and Alfons Kemper.
39: * Concurrency control: Our MVCC implementation is inspired by the paper "Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems" by Thomas Neumann, Tobias Mühlbauer and Alfons Kemper.
40: * Regular Expression: DuckDB uses Google's [RE2](https://github.com/google/re2) regular expression engine.
41: 
42: ## Other pages
43: * [Continuous Benchmarking (CB™)](https://www.duckdb.org/benchmarks/index.html), runs TPC-H, TPC-DS and some microbenchmarks on every commit
[end of README.md]
[start of src/execution/index/art/art.cpp]
1: #include "duckdb/execution/index/art/art.hpp"
2: #include "duckdb/execution/expression_executor.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include <algorithm>
5: #include <ctgmath>
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: ART::ART(DataTable &table, vector<column_t> column_ids, vector<unique_ptr<Expression>> unbound_expressions,
11:          bool is_unique)
12:     : Index(IndexType::ART, table, column_ids, move(unbound_expressions)), is_unique(is_unique) {
13: 	tree = nullptr;
14: 	expression_result.Initialize(types);
15: 	int n = 1;
16: 	//! little endian if true
17: 	if (*(char *)&n == 1) {
18: 		is_little_endian = true;
19: 	} else {
20: 		is_little_endian = false;
21: 	}
22: 	switch (types[0]) {
23: 	case TypeId::INT8:
24: 	case TypeId::INT16:
25: 	case TypeId::INT32:
26: 	case TypeId::INT64:
27: 	case TypeId::FLOAT:
28: 	case TypeId::DOUBLE:
29: 	case TypeId::VARCHAR:
30: 		break;
31: 	default:
32: 		throw InvalidTypeException(types[0], "Invalid type for index");
33: 	}
34: }
35: 
36: ART::~ART() {
37: }
38: 
39: bool ART::LeafMatches(Node *node, Key &key, unsigned depth) {
40: 	auto leaf = static_cast<Leaf *>(node);
41: 	Key &leaf_key = *leaf->value;
42: 	for (idx_t i = depth; i < leaf_key.len; i++) {
43: 		if (leaf_key[i] != key[i]) {
44: 			return false;
45: 		}
46: 	}
47: 
48: 	return true;
49: }
50: 
51: unique_ptr<IndexScanState> ART::InitializeScanSinglePredicate(Transaction &transaction, vector<column_t> column_ids,
52:                                                               Value value, ExpressionType expression_type) {
53: 	auto result = make_unique<ARTIndexScanState>(column_ids);
54: 	result->values[0] = value;
55: 	result->expressions[0] = expression_type;
56: 	return move(result);
57: }
58: 
59: unique_ptr<IndexScanState> ART::InitializeScanTwoPredicates(Transaction &transaction, vector<column_t> column_ids,
60:                                                             Value low_value, ExpressionType low_expression_type,
61:                                                             Value high_value, ExpressionType high_expression_type) {
62: 	auto result = make_unique<ARTIndexScanState>(column_ids);
63: 	result->values[0] = low_value;
64: 	result->expressions[0] = low_expression_type;
65: 	result->values[1] = high_value;
66: 	result->expressions[1] = high_expression_type;
67: 	return move(result);
68: }
69: 
70: //===--------------------------------------------------------------------===//
71: // Insert
72: //===--------------------------------------------------------------------===//
73: template <class T>
74: static void generate_keys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
75: 	VectorData idata;
76: 	input.Orrify(count, idata);
77: 
78: 	auto input_data = (T *)idata.data;
79: 	for (idx_t i = 0; i < count; i++) {
80: 		auto idx = idata.sel->get_index(i);
81: 		if ((*idata.nullmask)[idx]) {
82: 			keys.push_back(nullptr);
83: 		} else {
84: 			keys.push_back(Key::CreateKey<T>(input_data[idx], is_little_endian));
85: 		}
86: 	}
87: }
88: 
89: template <class T>
90: static void concatenate_keys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
91: 	VectorData idata;
92: 	input.Orrify(count, idata);
93: 
94: 	auto input_data = (T *)idata.data;
95: 	for (idx_t i = 0; i < count; i++) {
96: 		auto idx = idata.sel->get_index(i);
97: 		if ((*idata.nullmask)[idx] || !keys[i]) {
98: 			// either this column is NULL, or the previous column is NULL!
99: 			keys[i] = nullptr;
100: 		} else {
101: 			// concatenate the keys
102: 			auto old_key = move(keys[i]);
103: 			auto new_key = Key::CreateKey<T>(input_data[idx], is_little_endian);
104: 			auto keyLen = old_key->len + new_key->len;
105: 			auto compound_data = unique_ptr<data_t[]>(new data_t[keyLen]);
106: 			memcpy(compound_data.get(), old_key->data.get(), old_key->len);
107: 			memcpy(compound_data.get() + old_key->len, new_key->data.get(), new_key->len);
108: 			keys[i] = make_unique<Key>(move(compound_data), keyLen);
109: 		}
110: 	}
111: }
112: 
113: void ART::GenerateKeys(DataChunk &input, vector<unique_ptr<Key>> &keys) {
114: 	keys.reserve(STANDARD_VECTOR_SIZE);
115: 	// generate keys for the first input column
116: 	switch (input.data[0].type) {
117: 	case TypeId::BOOL:
118: 		generate_keys<bool>(input.data[0], input.size(), keys, is_little_endian);
119: 		break;
120: 	case TypeId::INT8:
121: 		generate_keys<int8_t>(input.data[0], input.size(), keys, is_little_endian);
122: 		break;
123: 	case TypeId::INT16:
124: 		generate_keys<int16_t>(input.data[0], input.size(), keys, is_little_endian);
125: 		break;
126: 	case TypeId::INT32:
127: 		generate_keys<int32_t>(input.data[0], input.size(), keys, is_little_endian);
128: 		break;
129: 	case TypeId::INT64:
130: 		generate_keys<int64_t>(input.data[0], input.size(), keys, is_little_endian);
131: 		break;
132: 	case TypeId::FLOAT:
133: 		generate_keys<float>(input.data[0], input.size(), keys, is_little_endian);
134: 		break;
135: 	case TypeId::DOUBLE:
136: 		generate_keys<double>(input.data[0], input.size(), keys, is_little_endian);
137: 		break;
138: 	case TypeId::VARCHAR:
139: 		generate_keys<string_t>(input.data[0], input.size(), keys, is_little_endian);
140: 		break;
141: 	default:
142: 		throw InvalidTypeException(input.data[0].type, "Invalid type for index");
143: 	}
144: 	for (idx_t i = 1; i < input.column_count(); i++) {
145: 		// for each of the remaining columns, concatenate
146: 		switch (input.data[i].type) {
147: 		case TypeId::BOOL:
148: 			concatenate_keys<bool>(input.data[i], input.size(), keys, is_little_endian);
149: 			break;
150: 		case TypeId::INT8:
151: 			concatenate_keys<int8_t>(input.data[i], input.size(), keys, is_little_endian);
152: 			break;
153: 		case TypeId::INT16:
154: 			concatenate_keys<int16_t>(input.data[i], input.size(), keys, is_little_endian);
155: 			break;
156: 		case TypeId::INT32:
157: 			concatenate_keys<int32_t>(input.data[i], input.size(), keys, is_little_endian);
158: 			break;
159: 		case TypeId::INT64:
160: 			concatenate_keys<int64_t>(input.data[i], input.size(), keys, is_little_endian);
161: 			break;
162: 		case TypeId::FLOAT:
163: 			concatenate_keys<float>(input.data[i], input.size(), keys, is_little_endian);
164: 			break;
165: 		case TypeId::DOUBLE:
166: 			concatenate_keys<double>(input.data[i], input.size(), keys, is_little_endian);
167: 			break;
168: 		case TypeId::VARCHAR:
169: 			concatenate_keys<string_t>(input.data[i], input.size(), keys, is_little_endian);
170: 			break;
171: 		default:
172: 			throw InvalidTypeException(input.data[0].type, "Invalid type for index");
173: 		}
174: 	}
175: }
176: 
177: bool ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {
178: 	assert(row_ids.type == ROW_TYPE);
179: 	assert(types[0] == input.data[0].type);
180: 
181: 	// generate the keys for the given input
182: 	vector<unique_ptr<Key>> keys;
183: 	GenerateKeys(input, keys);
184: 
185: 	// now insert the elements into the index
186: 	row_ids.Normalify(input.size());
187: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
188: 	idx_t failed_index = INVALID_INDEX;
189: 	for (idx_t i = 0; i < input.size(); i++) {
190: 		if (!keys[i]) {
191: 			continue;
192: 		}
193: 
194: 		row_t row_id = row_identifiers[i];
195: 		if (!Insert(tree, move(keys[i]), 0, row_id)) {
196: 			// failed to insert because of constraint violation
197: 			failed_index = i;
198: 			break;
199: 		}
200: 	}
201: 	if (failed_index != INVALID_INDEX) {
202: 		// failed to insert because of constraint violation: remove previously inserted entries
203: 		// generate keys again
204: 		keys.clear();
205: 		GenerateKeys(input, keys);
206: 		unique_ptr<Key> key;
207: 
208: 		// now erase the entries
209: 		for (idx_t i = 0; i < failed_index; i++) {
210: 			if (!keys[i]) {
211: 				continue;
212: 			}
213: 			row_t row_id = row_identifiers[i];
214: 			Erase(tree, *keys[i], 0, row_id);
215: 		}
216: 		return false;
217: 	}
218: 	return true;
219: }
220: 
221: bool ART::Append(IndexLock &lock, DataChunk &appended_data, Vector &row_identifiers) {
222: 	// first resolve the expressions for the index
223: 	ExecuteExpressions(appended_data, expression_result);
224: 
225: 	// now insert into the index
226: 	return Insert(lock, expression_result, row_identifiers);
227: }
228: 
229: void ART::VerifyAppend(DataChunk &chunk) {
230: 	if (!is_unique) {
231: 		return;
232: 	}
233: 	// unique index, check
234: 	lock_guard<mutex> l(lock);
235: 	// first resolve the expressions for the index
236: 	ExecuteExpressions(chunk, expression_result);
237: 
238: 	// generate the keys for the given input
239: 	vector<unique_ptr<Key>> keys;
240: 	GenerateKeys(expression_result, keys);
241: 
242: 	for (idx_t i = 0; i < chunk.size(); i++) {
243: 		if (!keys[i]) {
244: 			continue;
245: 		}
246: 		if (Lookup(tree, *keys[i], 0) != nullptr) {
247: 			// node already exists in tree
248: 			throw ConstraintException("duplicate key value violates primary key or unique constraint");
249: 		}
250: 	}
251: }
252: 
253: bool ART::InsertToLeaf(Leaf &leaf, row_t row_id) {
254: 	if (is_unique && leaf.num_elements != 0) {
255: 		return false;
256: 	}
257: 	leaf.Insert(row_id);
258: 	return true;
259: }
260: 
261: bool ART::Insert(unique_ptr<Node> &node, unique_ptr<Key> value, unsigned depth, row_t row_id) {
262: 	Key &key = *value;
263: 	if (!node) {
264: 		// node is currently empty, create a leaf here with the key
265: 		node = make_unique<Leaf>(*this, move(value), row_id);
266: 		return true;
267: 	}
268: 
269: 	if (node->type == NodeType::NLeaf) {
270: 		// Replace leaf with Node4 and store both leaves in it
271: 		auto leaf = static_cast<Leaf *>(node.get());
272: 
273: 		Key &existingKey = *leaf->value;
274: 		uint32_t newPrefixLength = 0;
275: 		// Leaf node is already there, update row_id vector
276: 		if (depth + newPrefixLength == existingKey.len && existingKey.len == key.len) {
277: 			return InsertToLeaf(*leaf, row_id);
278: 		}
279: 		while (existingKey[depth + newPrefixLength] == key[depth + newPrefixLength]) {
280: 			newPrefixLength++;
281: 			// Leaf node is already there, update row_id vector
282: 			if (depth + newPrefixLength == existingKey.len && existingKey.len == key.len) {
283: 				return InsertToLeaf(*leaf, row_id);
284: 			}
285: 		}
286: 
287: 		unique_ptr<Node> newNode = make_unique<Node4>(*this, newPrefixLength);
288: 		newNode->prefix_length = newPrefixLength;
289: 		memcpy(newNode->prefix.get(), &key[depth], newPrefixLength);
290: 		Node4::insert(*this, newNode, existingKey[depth + newPrefixLength], node);
291: 		unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
292: 		Node4::insert(*this, newNode, key[depth + newPrefixLength], leaf_node);
293: 		node = move(newNode);
294: 		return true;
295: 	}
296: 
297: 	// Handle prefix of inner node
298: 	if (node->prefix_length) {
299: 		uint32_t mismatchPos = Node::PrefixMismatch(*this, node.get(), key, depth);
300: 		if (mismatchPos != node->prefix_length) {
301: 			// Prefix differs, create new node
302: 			unique_ptr<Node> newNode = make_unique<Node4>(*this, mismatchPos);
303: 			newNode->prefix_length = mismatchPos;
304: 			memcpy(newNode->prefix.get(), node->prefix.get(), mismatchPos);
305: 			// Break up prefix
306: 			auto node_ptr = node.get();
307: 			Node4::insert(*this, newNode, node->prefix[mismatchPos], node);
308: 			node_ptr->prefix_length -= (mismatchPos + 1);
309: 			memmove(node_ptr->prefix.get(), node_ptr->prefix.get() + mismatchPos + 1, node_ptr->prefix_length);
310: 			unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
311: 			Node4::insert(*this, newNode, key[depth + mismatchPos], leaf_node);
312: 			node = move(newNode);
313: 			return true;
314: 		}
315: 		depth += node->prefix_length;
316: 	}
317: 
318: 	// Recurse
319: 	idx_t pos = node->GetChildPos(key[depth]);
320: 	if (pos != INVALID_INDEX) {
321: 		auto child = node->GetChild(pos);
322: 		return Insert(*child, move(value), depth + 1, row_id);
323: 	}
324: 	unique_ptr<Node> newNode = make_unique<Leaf>(*this, move(value), row_id);
325: 	Node::InsertLeaf(*this, node, key[depth], newNode);
326: 	return true;
327: }
328: 
329: //===--------------------------------------------------------------------===//
330: // Delete
331: //===--------------------------------------------------------------------===//
332: void ART::Delete(IndexLock &state, DataChunk &input, Vector &row_ids) {
333: 	// first resolve the expressions
334: 	ExecuteExpressions(input, expression_result);
335: 
336: 	// then generate the keys for the given input
337: 	vector<unique_ptr<Key>> keys;
338: 	GenerateKeys(expression_result, keys);
339: 
340: 	// now erase the elements from the database
341: 	row_ids.Normalify(input.size());
342: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
343: 
344: 	for (idx_t i = 0; i < input.size(); i++) {
345: 		if (!keys[i]) {
346: 			continue;
347: 		}
348: 		Erase(tree, *keys[i], 0, row_identifiers[i]);
349: 		// assert that the entry was erased properly
350: 		assert(!is_unique || Lookup(tree, *keys[i], 0) == nullptr);
351: 	}
352: }
353: 
354: void ART::Erase(unique_ptr<Node> &node, Key &key, unsigned depth, row_t row_id) {
355: 	if (!node) {
356: 		return;
357: 	}
358: 	// Delete a leaf from a tree
359: 	if (node->type == NodeType::NLeaf) {
360: 		// Make sure we have the right leaf
361: 		if (ART::LeafMatches(node.get(), key, depth)) {
362: 			node.reset();
363: 		}
364: 		return;
365: 	}
366: 
367: 	// Handle prefix
368: 	if (node->prefix_length) {
369: 		if (Node::PrefixMismatch(*this, node.get(), key, depth) != node->prefix_length) {
370: 			return;
371: 		}
372: 		depth += node->prefix_length;
373: 	}
374: 	idx_t pos = node->GetChildPos(key[depth]);
375: 	if (pos != INVALID_INDEX) {
376: 		auto child = node->GetChild(pos);
377: 		assert(child);
378: 
379: 		unique_ptr<Node> &child_ref = *child;
380: 		if (child_ref->type == NodeType::NLeaf && LeafMatches(child_ref.get(), key, depth)) {
381: 			// Leaf found, remove entry
382: 			auto leaf = static_cast<Leaf *>(child_ref.get());
383: 			if (leaf->num_elements > 1) {
384: 				// leaf has multiple rows: remove the row from the leaf
385: 				leaf->Remove(row_id);
386: 			} else {
387: 				// Leaf only has one element, delete leaf, decrement node counter and maybe shrink node
388: 				Node::Erase(*this, node, pos);
389: 			}
390: 		} else {
391: 			// Recurse
392: 			Erase(*child, key, depth + 1, row_id);
393: 		}
394: 	}
395: }
396: 
397: //===--------------------------------------------------------------------===//
398: // Point Query
399: //===--------------------------------------------------------------------===//
400: static unique_ptr<Key> CreateKey(ART &art, TypeId type, Value &value) {
401: 	assert(type == value.type);
402: 	switch (type) {
403: 	case TypeId::INT8:
404: 		return Key::CreateKey<int8_t>(value.value_.tinyint, art.is_little_endian);
405: 	case TypeId::INT16:
406: 		return Key::CreateKey<int16_t>(value.value_.smallint, art.is_little_endian);
407: 	case TypeId::INT32:
408: 		return Key::CreateKey<int32_t>(value.value_.integer, art.is_little_endian);
409: 	case TypeId::INT64:
410: 		return Key::CreateKey<int64_t>(value.value_.bigint, art.is_little_endian);
411: 	case TypeId::FLOAT:
412: 		return Key::CreateKey<float>(value.value_.float_, art.is_little_endian);
413: 	case TypeId::DOUBLE:
414: 		return Key::CreateKey<double>(value.value_.double_, art.is_little_endian);
415: 	case TypeId::VARCHAR:
416: 		return Key::CreateKey<string_t>(string_t(value.str_value.c_str(), value.str_value.size()),
417: 		                                art.is_little_endian);
418: 	default:
419: 		throw InvalidTypeException(type, "Invalid type for index");
420: 	}
421: }
422: 
423: void ART::SearchEqual(vector<row_t> &result_ids, ARTIndexScanState *state) {
424: 	unique_ptr<Key> key = CreateKey(*this, types[0], state->values[0]);
425: 	auto leaf = static_cast<Leaf *>(Lookup(tree, *key, 0));
426: 	if (!leaf) {
427: 		return;
428: 	}
429: 	for (idx_t i = 0; i < leaf->num_elements; i++) {
430: 		row_t row_id = leaf->GetRowId(i);
431: 		result_ids.push_back(row_id);
432: 	}
433: }
434: 
435: Node *ART::Lookup(unique_ptr<Node> &node, Key &key, unsigned depth) {
436: 	auto node_val = node.get();
437: 
438: 	while (node_val) {
439: 		if (node_val->type == NodeType::NLeaf) {
440: 			auto leaf = static_cast<Leaf *>(node_val);
441: 			Key &leafKey = *leaf->value;
442: 			//! Check leaf
443: 			for (idx_t i = depth; i < leafKey.len; i++) {
444: 				if (leafKey[i] != key[i]) {
445: 					return nullptr;
446: 				}
447: 			}
448: 			return node_val;
449: 		}
450: 		if (node_val->prefix_length) {
451: 			for (idx_t pos = 0; pos < node_val->prefix_length; pos++) {
452: 				if (key[depth + pos] != node_val->prefix[pos]) {
453: 					return nullptr;
454: 				}
455: 			}
456: 			depth += node_val->prefix_length;
457: 		}
458: 		idx_t pos = node_val->GetChildPos(key[depth]);
459: 		if (pos == INVALID_INDEX) {
460: 			return nullptr;
461: 		}
462: 		node_val = node_val->GetChild(pos)->get();
463: 		assert(node_val);
464: 
465: 		depth++;
466: 	}
467: 
468: 	return nullptr;
469: }
470: 
471: //===--------------------------------------------------------------------===//
472: // Iterator scans
473: //===--------------------------------------------------------------------===//
474: template <bool HAS_BOUND, bool INCLUSIVE>
475: void ART::IteratorScan(ARTIndexScanState *state, Iterator *it, vector<row_t> &result_ids, Key *bound) {
476: 	bool has_next;
477: 	do {
478: 		if (HAS_BOUND) {
479: 			assert(bound);
480: 			if (INCLUSIVE) {
481: 				if (*it->node->value > *bound) {
482: 					break;
483: 				}
484: 			} else {
485: 				if (*it->node->value >= *bound) {
486: 					break;
487: 				}
488: 			}
489: 		}
490: 		for (idx_t i = 0; i < it->node->num_elements; i++) {
491: 			row_t row_id = it->node->GetRowId(i);
492: 			result_ids.push_back(row_id);
493: 		}
494: 		has_next = ART::IteratorNext(*it);
495: 	} while (has_next);
496: }
497: 
498: bool ART::IteratorNext(Iterator &it) {
499: 	// Skip leaf
500: 	if ((it.depth) && ((it.stack[it.depth - 1].node)->type == NodeType::NLeaf)) {
501: 		it.depth--;
502: 	}
503: 
504: 	// Look for the next leaf
505: 	while (it.depth > 0) {
506: 		auto &top = it.stack[it.depth - 1];
507: 		Node *node = top.node;
508: 
509: 		if (node->type == NodeType::NLeaf) {
510: 			// found a leaf: move to next node
511: 			it.node = (Leaf *)node;
512: 			return true;
513: 		}
514: 
515: 		// Find next node
516: 		top.pos = node->GetNextPos(top.pos);
517: 		if (top.pos != INVALID_INDEX) {
518: 			// next node found: go there
519: 			it.stack[it.depth].node = node->GetChild(top.pos)->get();
520: 			it.stack[it.depth].pos = INVALID_INDEX;
521: 			it.depth++;
522: 		} else {
523: 			// no node found: move up the tree
524: 			it.depth--;
525: 		}
526: 	}
527: 	return false;
528: }
529: 
530: //===--------------------------------------------------------------------===//
531: // Greater Than
532: // Returns: True (If found leaf >= key)
533: //          False (Otherwise)
534: //===--------------------------------------------------------------------===//
535: bool ART::Bound(unique_ptr<Node> &n, Key &key, Iterator &it, bool inclusive) {
536: 	it.depth = 0;
537: 	if (!n) {
538: 		return false;
539: 	}
540: 	Node *node = n.get();
541: 
542: 	idx_t depth = 0;
543: 	while (true) {
544: 		auto &top = it.stack[it.depth];
545: 		top.node = node;
546: 		it.depth++;
547: 
548: 		if (node->type == NodeType::NLeaf) {
549: 			// found a leaf node: check if it is bigger or equal than the current key
550: 			auto leaf = static_cast<Leaf *>(node);
551: 			it.node = leaf;
552: 			// if the search is not inclusive the leaf node could still be equal to the current value
553: 			// check if leaf is equal to the current key
554: 			if (*leaf->value == key) {
555: 				// if its not inclusive check if there is a next leaf
556: 				if (!inclusive && !IteratorNext(it)) {
557: 					return false;
558: 				} else {
559: 					return true;
560: 				}
561: 			}
562: 
563: 			if (*leaf->value > key) {
564: 				return true;
565: 			}
566: 			// Leaf is lower than key
567: 			// Check if next leaf is still lower than key
568: 			while (IteratorNext(it)) {
569: 				if (*it.node->value == key) {
570: 					// if its not inclusive check if there is a next leaf
571: 					if (!inclusive && !IteratorNext(it)) {
572: 						return false;
573: 					} else {
574: 						return true;
575: 					}
576: 				} else if (*it.node->value > key) {
577: 					// if its not inclusive check if there is a next leaf
578: 					return true;
579: 				}
580: 			}
581: 			return false;
582: 		}
583: 		uint32_t mismatchPos = Node::PrefixMismatch(*this, node, key, depth);
584: 		if (mismatchPos != node->prefix_length) {
585: 			if (node->prefix[mismatchPos] < key[depth + mismatchPos]) {
586: 				// Less
587: 				it.depth--;
588: 				return IteratorNext(it);
589: 			} else {
590: 				// Greater
591: 				top.pos = INVALID_INDEX;
592: 				return IteratorNext(it);
593: 			}
594: 		}
595: 		// prefix matches, search inside the child for the key
596: 		depth += node->prefix_length;
597: 
598: 		top.pos = node->GetChildGreaterEqual(key[depth]);
599: 
600: 		if (top.pos == INVALID_INDEX) {
601: 			// Find min leaf
602: 			top.pos = node->GetMin();
603: 		}
604: 		node = node->GetChild(top.pos)->get();
605: 		depth++;
606: 	}
607: }
608: 
609: void ART::SearchGreater(vector<row_t> &result_ids, ARTIndexScanState *state, bool inclusive) {
610: 	Iterator *it = &state->iterator;
611: 	auto key = CreateKey(*this, types[0], state->values[0]);
612: 
613: 	// greater than scan: first set the iterator to the node at which we will start our scan by finding the lowest node
614: 	// that satisfies our requirement
615: 	if (!it->start) {
616: 		bool found = ART::Bound(tree, *key, *it, inclusive);
617: 		if (!found) {
618: 			return;
619: 		}
620: 		it->start = true;
621: 	}
622: 	// after that we continue the scan; we don't need to check the bounds as any value following this value is
623: 	// automatically bigger and hence satisfies our predicate
624: 	IteratorScan<false, false>(state, it, result_ids, nullptr);
625: }
626: 
627: //===--------------------------------------------------------------------===//
628: // Less Than
629: //===--------------------------------------------------------------------===//
630: static Leaf &FindMinimum(Iterator &it, Node &node) {
631: 	Node *next = nullptr;
632: 	idx_t pos = 0;
633: 	switch (node.type) {
634: 	case NodeType::NLeaf:
635: 		it.node = (Leaf *)&node;
636: 		return (Leaf &)node;
637: 	case NodeType::N4:
638: 		next = ((Node4 &)node).child[0].get();
639: 		break;
640: 	case NodeType::N16:
641: 		next = ((Node16 &)node).child[0].get();
642: 		break;
643: 	case NodeType::N48: {
644: 		auto &n48 = (Node48 &)node;
645: 		while (n48.childIndex[pos] == Node::EMPTY_MARKER) {
646: 			pos++;
647: 		}
648: 		next = n48.child[n48.childIndex[pos]].get();
649: 		break;
650: 	}
651: 	case NodeType::N256: {
652: 		auto &n256 = (Node256 &)node;
653: 		while (!n256.child[pos]) {
654: 			pos++;
655: 		}
656: 		next = n256.child[pos].get();
657: 		break;
658: 	}
659: 	}
660: 	it.stack[it.depth].node = &node;
661: 	it.stack[it.depth].pos = pos;
662: 	it.depth++;
663: 	return FindMinimum(it, *next);
664: }
665: 
666: void ART::SearchLess(vector<row_t> &result_ids, ARTIndexScanState *state, bool inclusive) {
667: 	if (!tree) {
668: 		return;
669: 	}
670: 
671: 	Iterator *it = &state->iterator;
672: 	auto upper_bound = CreateKey(*this, types[0], state->values[0]);
673: 
674: 	if (!it->start) {
675: 		// first find the minimum value in the ART: we start scanning from this value
676: 		auto &minimum = FindMinimum(state->iterator, *tree);
677: 		// early out min value higher than upper bound query
678: 		if (*minimum.value > *upper_bound) {
679: 			return;
680: 		}
681: 		it->start = true;
682: 	}
683: 	// now continue the scan until we reach the upper bound
684: 	if (inclusive) {
685: 		IteratorScan<true, true>(state, it, result_ids, upper_bound.get());
686: 	} else {
687: 		IteratorScan<true, false>(state, it, result_ids, upper_bound.get());
688: 	}
689: }
690: 
691: //===--------------------------------------------------------------------===//
692: // Closed Range Query
693: //===--------------------------------------------------------------------===//
694: void ART::SearchCloseRange(vector<row_t> &result_ids, ARTIndexScanState *state, bool left_inclusive,
695:                            bool right_inclusive) {
696: 	auto lower_bound = CreateKey(*this, types[0], state->values[0]);
697: 	auto upper_bound = CreateKey(*this, types[0], state->values[1]);
698: 	Iterator *it = &state->iterator;
699: 	// first find the first node that satisfies the left predicate
700: 	if (!it->start) {
701: 		bool found = ART::Bound(tree, *lower_bound, *it, left_inclusive);
702: 		if (!found) {
703: 			return;
704: 		}
705: 		it->start = true;
706: 	}
707: 	// now continue the scan until we reach the upper bound
708: 	if (right_inclusive) {
709: 		IteratorScan<true, true>(state, it, result_ids, upper_bound.get());
710: 	} else {
711: 		IteratorScan<true, false>(state, it, result_ids, upper_bound.get());
712: 	}
713: }
714: 
715: void ART::Scan(Transaction &transaction, TableIndexScanState &table_state, DataChunk &result) {
716: 	auto state = (ARTIndexScanState *)table_state.index_state.get();
717: 
718: 	// scan the index
719: 	if (!state->checked) {
720: 		vector<row_t> result_ids;
721: 		assert(state->values[0].type == types[0]);
722: 
723: 		if (state->values[1].is_null) {
724: 			lock_guard<mutex> l(lock);
725: 			// single predicate
726: 			switch (state->expressions[0]) {
727: 			case ExpressionType::COMPARE_EQUAL:
728: 				SearchEqual(result_ids, state);
729: 				break;
730: 			case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
731: 				SearchGreater(result_ids, state, true);
732: 				break;
733: 			case ExpressionType::COMPARE_GREATERTHAN:
734: 				SearchGreater(result_ids, state, false);
735: 				break;
736: 			case ExpressionType::COMPARE_LESSTHANOREQUALTO:
737: 				SearchLess(result_ids, state, true);
738: 				break;
739: 			case ExpressionType::COMPARE_LESSTHAN:
740: 				SearchLess(result_ids, state, false);
741: 				break;
742: 			default:
743: 				throw NotImplementedException("Operation not implemented");
744: 			}
745: 		} else {
746: 			lock_guard<mutex> l(lock);
747: 			// two predicates
748: 			assert(state->values[1].type == types[0]);
749: 			bool left_inclusive = state->expressions[0] == ExpressionType ::COMPARE_GREATERTHANOREQUALTO;
750: 			bool right_inclusive = state->expressions[1] == ExpressionType ::COMPARE_LESSTHANOREQUALTO;
751: 			SearchCloseRange(result_ids, state, left_inclusive, right_inclusive);
752: 		}
753: 		state->checked = true;
754: 
755: 		if (result_ids.size() == 0) {
756: 			return;
757: 		}
758: 
759: 		// sort the row ids
760: 		sort(result_ids.begin(), result_ids.end());
761: 		// duplicate eliminate the row ids and append them to the row ids of the state
762: 		state->result_ids.reserve(result_ids.size());
763: 
764: 		state->result_ids.push_back(result_ids[0]);
765: 		for (idx_t i = 1; i < result_ids.size(); i++) {
766: 			if (result_ids[i] != result_ids[i - 1]) {
767: 				state->result_ids.push_back(result_ids[i]);
768: 			}
769: 		}
770: 	}
771: 
772: 	if (state->result_index >= state->result_ids.size()) {
773: 		// exhausted all row ids
774: 		return;
775: 	}
776: 
777: 	// create a vector pointing to the current set of row ids
778: 	Vector row_identifiers(ROW_TYPE, (data_ptr_t)&state->result_ids[state->result_index]);
779: 	idx_t scan_count = std::min((idx_t)STANDARD_VECTOR_SIZE, (idx_t)state->result_ids.size() - state->result_index);
780: 
781: 	// fetch the actual values from the base table
782: 	table.Fetch(transaction, result, state->column_ids, row_identifiers, scan_count, table_state);
783: 
784: 	// move to the next set of row ids
785: 	state->result_index += scan_count;
786: }
[end of src/execution/index/art/art.cpp]
[start of src/execution/index/art/art_key.cpp]
1: #include <cfloat>
2: #include <limits.h>
3: #include <cstring> // strlen() on Solaris
4: 
5: #include "duckdb/execution/index/art/art_key.hpp"
6: #include "duckdb/execution/index/art/art.hpp"
7: 
8: using namespace duckdb;
9: 
10: //! these are optimized and assume a particular byte order
11: #define BSWAP16(x) ((uint16_t)((((uint16_t)(x)&0xff00) >> 8) | (((uint16_t)(x)&0x00ff) << 8)))
12: 
13: #define BSWAP32(x)                                                                                                     \
14: 	((uint32_t)((((uint32_t)(x)&0xff000000) >> 24) | (((uint32_t)(x)&0x00ff0000) >> 8) |                               \
15: 	            (((uint32_t)(x)&0x0000ff00) << 8) | (((uint32_t)(x)&0x000000ff) << 24)))
16: 
17: #define BSWAP64(x)                                                                                                     \
18: 	((uint64_t)((((uint64_t)(x)&0xff00000000000000ull) >> 56) | (((uint64_t)(x)&0x00ff000000000000ull) >> 40) |        \
19: 	            (((uint64_t)(x)&0x0000ff0000000000ull) >> 24) | (((uint64_t)(x)&0x000000ff00000000ull) >> 8) |         \
20: 	            (((uint64_t)(x)&0x00000000ff000000ull) << 8) | (((uint64_t)(x)&0x0000000000ff0000ull) << 24) |         \
21: 	            (((uint64_t)(x)&0x000000000000ff00ull) << 40) | (((uint64_t)(x)&0x00000000000000ffull) << 56)))
22: 
23: static uint8_t FlipSign(uint8_t key_byte) {
24: 	return key_byte ^ 128;
25: }
26: 
27: uint32_t Key::EncodeFloat(float x) {
28: 	unsigned long buff;
29: 
30: 	//! zero
31: 	if (x == 0) {
32: 		buff = 0;
33: 		buff |= (1u << 31);
34: 		return buff;
35: 	}
36: 	//! infinity
37: 	if (x > FLT_MAX) {
38: 		return UINT_MAX;
39: 	}
40: 	//! -infinity
41: 	if (x < -FLT_MAX) {
42: 		return 0;
43: 	}
44: 	buff = reinterpret_cast<uint32_t *>(&x)[0];
45: 	if ((buff & (1u << 31)) == 0) { //! +0 and positive numbers
46: 		buff |= (1u << 31);
47: 	} else {          //! negative numbers
48: 		buff = ~buff; //! complement 1
49: 	}
50: 
51: 	return buff;
52: }
53: 
54: uint64_t Key::EncodeDouble(double x) {
55: 	uint64_t buff;
56: 	//! zero
57: 	if (x == 0) {
58: 		buff = 0;
59: 		buff += (1ull << 63);
60: 		return buff;
61: 	}
62: 	//! infinity
63: 	if (x > DBL_MAX) {
64: 		return ULLONG_MAX;
65: 	}
66: 	//! -infinity
67: 	if (x < -DBL_MAX) {
68: 		return 0;
69: 	}
70: 	buff = reinterpret_cast<uint64_t *>(&x)[0];
71: 	if (buff < (1ull << 63)) { //! +0 and positive numbers
72: 		buff += (1ull << 63);
73: 	} else {          //! negative numbers
74: 		buff = ~buff; //! complement 1
75: 	}
76: 	return buff;
77: }
78: 
79: Key::Key(unique_ptr<data_t[]> data, idx_t len) : len(len), data(move(data)) {
80: }
81: 
82: template <> unique_ptr<data_t[]> Key::CreateData(int8_t value, bool is_little_endian) {
83: 	auto data = unique_ptr<data_t[]>(new data_t[sizeof(value)]);
84: 	reinterpret_cast<uint8_t *>(data.get())[0] = value;
85: 	data[0] = FlipSign(data[0]);
86: 	return data;
87: }
88: 
89: template <> unique_ptr<data_t[]> Key::CreateData(int16_t value, bool is_little_endian) {
90: 	auto data = unique_ptr<data_t[]>(new data_t[sizeof(value)]);
91: 	reinterpret_cast<uint16_t *>(data.get())[0] = is_little_endian ? BSWAP16(value) : value;
92: 	data[0] = FlipSign(data[0]);
93: 	return data;
94: }
95: 
96: template <> unique_ptr<data_t[]> Key::CreateData(int32_t value, bool is_little_endian) {
97: 	auto data = unique_ptr<data_t[]>(new data_t[sizeof(value)]);
98: 	reinterpret_cast<uint32_t *>(data.get())[0] = is_little_endian ? BSWAP32(value) : value;
99: 	data[0] = FlipSign(data[0]);
100: 	return data;
101: }
102: 
103: template <> unique_ptr<data_t[]> Key::CreateData(int64_t value, bool is_little_endian) {
104: 	auto data = unique_ptr<data_t[]>(new data_t[sizeof(value)]);
105: 	reinterpret_cast<uint64_t *>(data.get())[0] = is_little_endian ? BSWAP64(value) : value;
106: 	data[0] = FlipSign(data[0]);
107: 	return data;
108: }
109: 
110: template <> unique_ptr<data_t[]> Key::CreateData(float value, bool is_little_endian) {
111: 	uint32_t converted_value = EncodeFloat(value);
112: 	auto data = unique_ptr<data_t[]>(new data_t[sizeof(converted_value)]);
113: 	reinterpret_cast<uint32_t *>(data.get())[0] = is_little_endian ? BSWAP32(converted_value) : converted_value;
114: 	return data;
115: }
116: template <> unique_ptr<data_t[]> Key::CreateData(double value, bool is_little_endian) {
117: 	uint64_t converted_value = EncodeDouble(value);
118: 	auto data = unique_ptr<data_t[]>(new data_t[sizeof(converted_value)]);
119: 	reinterpret_cast<uint64_t *>(data.get())[0] = is_little_endian ? BSWAP64(converted_value) : converted_value;
120: 	return data;
121: }
122: 
123: template <> unique_ptr<Key> Key::CreateKey(string_t value, bool is_little_endian) {
124: 	idx_t len = value.GetSize() + 1;
125: 	auto data = unique_ptr<data_t[]>(new data_t[len]);
126: 	memcpy(data.get(), value.GetData(), len);
127: 	return make_unique<Key>(move(data), len);
128: }
129: 
130: template <> unique_ptr<Key> Key::CreateKey(const char *value, bool is_little_endian) {
131: 	return Key::CreateKey(string_t(value, strlen(value)), is_little_endian);
132: }
133: 
134: bool Key::operator>(const Key &k) const {
135: 	for (idx_t i = 0; i < std::min(len, k.len); i++) {
136: 		if (data[i] > k.data[i]) {
137: 			return true;
138: 		} else if (data[i] < k.data[i]) {
139: 			return false;
140: 		}
141: 	}
142: 	return len > k.len;
143: }
144: 
145: bool Key::operator<(const Key &k) const {
146: 	for (idx_t i = 0; i < std::min(len, k.len); i++) {
147: 		if (data[i] < k.data[i]) {
148: 			return true;
149: 		} else if (data[i] > k.data[i]) {
150: 			return false;
151: 		}
152: 	}
153: 	return len < k.len;
154: }
155: 
156: bool Key::operator>=(const Key &k) const {
157: 	for (idx_t i = 0; i < std::min(len, k.len); i++) {
158: 		if (data[i] > k.data[i]) {
159: 			return true;
160: 		} else if (data[i] < k.data[i]) {
161: 			return false;
162: 		}
163: 	}
164: 	return len >= k.len;
165: }
166: 
167: bool Key::operator==(const Key &k) const {
168: 	if (len != k.len) {
169: 		return false;
170: 	}
171: 	for (idx_t i = 0; i < len; i++) {
172: 		if (data[i] != k.data[i]) {
173: 			return false;
174: 		}
175: 	}
176: 	return true;
177: }
[end of src/execution/index/art/art_key.cpp]
[start of src/execution/join_hashtable.cpp]
1: #include "duckdb/execution/join_hashtable.hpp"
2: 
3: #include "duckdb/storage/buffer_manager.hpp"
4: 
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/types/null_value.hpp"
7: #include "duckdb/common/vector_operations/vector_operations.hpp"
8: #include "duckdb/common/vector_operations/unary_executor.hpp"
9: #include "duckdb/common/operator/comparison_operators.hpp"
10: 
11: using namespace std;
12: 
13: namespace duckdb {
14: 
15: using ScanStructure = JoinHashTable::ScanStructure;
16: 
17: JoinHashTable::JoinHashTable(BufferManager &buffer_manager, vector<JoinCondition> &conditions, vector<TypeId> btypes,
18:                              JoinType type)
19:     : buffer_manager(buffer_manager), build_types(move(btypes)), equality_size(0), condition_size(0), build_size(0),
20:       entry_size(0), tuple_size(0), join_type(type), finalized(false), has_null(false), count(0) {
21: 	for (auto &condition : conditions) {
22: 		assert(condition.left->return_type == condition.right->return_type);
23: 		auto type = condition.left->return_type;
24: 		auto type_size = GetTypeIdSize(type);
25: 		if (condition.comparison == ExpressionType::COMPARE_EQUAL) {
26: 			// all equality conditions should be at the front
27: 			// all other conditions at the back
28: 			// this assert checks that
29: 			assert(equality_types.size() == condition_types.size());
30: 			equality_types.push_back(type);
31: 			equality_size += type_size;
32: 		}
33: 		predicates.push_back(condition.comparison);
34: 		null_values_are_equal.push_back(condition.null_values_are_equal);
35: 		assert(!condition.null_values_are_equal ||
36: 		       (condition.null_values_are_equal && condition.comparison == ExpressionType::COMPARE_EQUAL));
37: 
38: 		condition_types.push_back(type);
39: 		condition_size += type_size;
40: 	}
41: 	// at least one equality is necessary
42: 	assert(equality_types.size() > 0);
43: 
44: 	if (type == JoinType::ANTI || type == JoinType::SEMI || type == JoinType::MARK) {
45: 		// for ANTI, SEMI and MARK join, we only need to store the keys
46: 		build_size = 0;
47: 		build_types.clear();
48: 	} else {
49: 		// otherwise we need to store the entire build side for reconstruction
50: 		// purposes
51: 		for (idx_t i = 0; i < build_types.size(); i++) {
52: 			build_size += GetTypeIdSize(build_types[i]);
53: 		}
54: 	}
55: 	tuple_size = condition_size + build_size;
56: 	// entry size is the tuple size and the size of the hash/next pointer
57: 	entry_size = tuple_size + std::max(sizeof(hash_t), sizeof(uintptr_t));
58: 	// compute the per-block capacity of this HT
59: 	block_capacity = std::max((idx_t)STANDARD_VECTOR_SIZE, (Storage::BLOCK_ALLOC_SIZE / entry_size) + 1);
60: }
61: 
62: JoinHashTable::~JoinHashTable() {
63: 	if (hash_map) {
64: 		auto hash_id = hash_map->block_id;
65: 		hash_map.reset();
66: 		buffer_manager.DestroyBuffer(hash_id);
67: 	}
68: 	pinned_handles.clear();
69: 	for (auto &block : blocks) {
70: 		buffer_manager.DestroyBuffer(block.block_id);
71: 	}
72: }
73: 
74: void JoinHashTable::ApplyBitmask(Vector &hashes, idx_t count) {
75: 	if (hashes.vector_type == VectorType::CONSTANT_VECTOR) {
76: 		assert(!ConstantVector::IsNull(hashes));
77: 		auto indices = ConstantVector::GetData<hash_t>(hashes);
78: 		*indices = *indices & bitmask;
79: 	} else {
80: 		hashes.Normalify(count);
81: 		auto indices = FlatVector::GetData<hash_t>(hashes);
82: 		for (idx_t i = 0; i < count; i++) {
83: 			indices[i] &= bitmask;
84: 		}
85: 	}
86: }
87: 
88: void JoinHashTable::ApplyBitmask(Vector &hashes, const SelectionVector &sel, idx_t count, Vector &pointers) {
89: 	VectorData hdata;
90: 	hashes.Orrify(count, hdata);
91: 
92: 	auto hash_data = (hash_t *)hdata.data;
93: 	auto result_data = FlatVector::GetData<data_ptr_t *>(pointers);
94: 	auto main_ht = (data_ptr_t *)hash_map->node->buffer;
95: 	for (idx_t i = 0; i < count; i++) {
96: 		auto rindex = sel.get_index(i);
97: 		auto hindex = hdata.sel->get_index(rindex);
98: 		auto hash = hash_data[hindex];
99: 		result_data[rindex] = main_ht + (hash & bitmask);
100: 	}
101: }
102: 
103: void JoinHashTable::Hash(DataChunk &keys, const SelectionVector &sel, idx_t count, Vector &hashes) {
104: 	if (count == keys.size()) {
105: 		// no null values are filtered: use regular hash functions
106: 		VectorOperations::Hash(keys.data[0], hashes, keys.size());
107: 		for (idx_t i = 1; i < equality_types.size(); i++) {
108: 			VectorOperations::CombineHash(hashes, keys.data[i], keys.size());
109: 		}
110: 	} else {
111: 		// null values were filtered: use selection vector
112: 		VectorOperations::Hash(keys.data[0], hashes, sel, count);
113: 		for (idx_t i = 1; i < equality_types.size(); i++) {
114: 			VectorOperations::CombineHash(hashes, keys.data[i], sel, count);
115: 		}
116: 	}
117: }
118: template <class T>
119: static void templated_serialize_vdata(VectorData &vdata, const SelectionVector &sel, idx_t count,
120:                                       data_ptr_t key_locations[]) {
121: 	auto source = (T *)vdata.data;
122: 	if (vdata.nullmask->any()) {
123: 		for (idx_t i = 0; i < count; i++) {
124: 			auto idx = sel.get_index(i);
125: 			auto source_idx = vdata.sel->get_index(idx);
126: 
127: 			auto target = (T *)key_locations[i];
128: 			if ((*vdata.nullmask)[source_idx]) {
129: 				*target = NullValue<T>();
130: 			} else {
131: 				*target = source[source_idx];
132: 			}
133: 			key_locations[i] += sizeof(T);
134: 		}
135: 	} else {
136: 		for (idx_t i = 0; i < count; i++) {
137: 			auto idx = sel.get_index(i);
138: 			auto source_idx = vdata.sel->get_index(idx);
139: 
140: 			auto target = (T *)key_locations[i];
141: 			*target = source[source_idx];
142: 			key_locations[i] += sizeof(T);
143: 		}
144: 	}
145: }
146: 
147: void JoinHashTable::SerializeVectorData(VectorData &vdata, TypeId type, const SelectionVector &sel, idx_t count,
148:                                         data_ptr_t key_locations[]) {
149: 	switch (type) {
150: 	case TypeId::BOOL:
151: 	case TypeId::INT8:
152: 		templated_serialize_vdata<int8_t>(vdata, sel, count, key_locations);
153: 		break;
154: 	case TypeId::INT16:
155: 		templated_serialize_vdata<int16_t>(vdata, sel, count, key_locations);
156: 		break;
157: 	case TypeId::INT32:
158: 		templated_serialize_vdata<int32_t>(vdata, sel, count, key_locations);
159: 		break;
160: 	case TypeId::INT64:
161: 		templated_serialize_vdata<int64_t>(vdata, sel, count, key_locations);
162: 		break;
163: 	case TypeId::FLOAT:
164: 		templated_serialize_vdata<float>(vdata, sel, count, key_locations);
165: 		break;
166: 	case TypeId::DOUBLE:
167: 		templated_serialize_vdata<double>(vdata, sel, count, key_locations);
168: 		break;
169: 	case TypeId::HASH:
170: 		templated_serialize_vdata<hash_t>(vdata, sel, count, key_locations);
171: 		break;
172: 	case TypeId::VARCHAR: {
173: 		auto source = (string_t *)vdata.data;
174: 		for (idx_t i = 0; i < count; i++) {
175: 			auto idx = sel.get_index(i);
176: 			auto source_idx = vdata.sel->get_index(idx);
177: 
178: 			auto target = (string_t *)key_locations[i];
179: 			if ((*vdata.nullmask)[source_idx]) {
180: 				*target = NullValue<string_t>();
181: 			} else if (source[source_idx].IsInlined()) {
182: 				*target = source[source_idx];
183: 			} else {
184: 				*target = string_heap.AddString(source[source_idx]);
185: 			}
186: 			key_locations[i] += sizeof(string_t);
187: 		}
188: 		break;
189: 	}
190: 	default:
191: 		throw NotImplementedException("FIXME: unimplemented serialize");
192: 	}
193: }
194: 
195: void JoinHashTable::SerializeVector(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t count,
196:                                     data_ptr_t key_locations[]) {
197: 	VectorData vdata;
198: 	v.Orrify(vcount, vdata);
199: 
200: 	SerializeVectorData(vdata, v.type, sel, count, key_locations);
201: }
202: 
203: idx_t JoinHashTable::AppendToBlock(HTDataBlock &block, BufferHandle &handle, idx_t count, data_ptr_t key_locations[],
204:                                    idx_t remaining) {
205: 	idx_t append_count = std::min(remaining, block.capacity - block.count);
206: 	auto dataptr = handle.node->buffer + block.count * entry_size;
207: 	idx_t offset = count - remaining;
208: 	for (idx_t i = 0; i < append_count; i++) {
209: 		key_locations[offset + i] = dataptr;
210: 		dataptr += entry_size;
211: 	}
212: 	block.count += append_count;
213: 	return append_count;
214: }
215: 
216: static idx_t FilterNullValues(VectorData &vdata, const SelectionVector &sel, idx_t count, SelectionVector &result) {
217: 	auto &nullmask = *vdata.nullmask;
218: 	idx_t result_count = 0;
219: 	for (idx_t i = 0; i < count; i++) {
220: 		auto idx = sel.get_index(i);
221: 		auto key_idx = vdata.sel->get_index(idx);
222: 		if (!nullmask[key_idx]) {
223: 			result.set_index(result_count++, idx);
224: 		}
225: 	}
226: 	return result_count;
227: }
228: 
229: idx_t JoinHashTable::PrepareKeys(DataChunk &keys, unique_ptr<VectorData[]> &key_data,
230:                                  const SelectionVector *&current_sel, SelectionVector &sel) {
231: 	key_data = keys.Orrify();
232: 
233: 	// figure out which keys are NULL, and create a selection vector out of them
234: 	current_sel = &FlatVector::IncrementalSelectionVector;
235: 	idx_t added_count = keys.size();
236: 	for (idx_t i = 0; i < keys.column_count(); i++) {
237: 		if (!null_values_are_equal[i]) {
238: 			if (!key_data[i].nullmask->any()) {
239: 				continue;
240: 			}
241: 			added_count = FilterNullValues(key_data[i], *current_sel, added_count, sel);
242: 			// null values are NOT equal for this column, filter them out
243: 			current_sel = &sel;
244: 		}
245: 	}
246: 	return added_count;
247: }
248: 
249: void JoinHashTable::Build(DataChunk &keys, DataChunk &payload) {
250: 	assert(!finalized);
251: 	assert(keys.size() == payload.size());
252: 	if (keys.size() == 0) {
253: 		return;
254: 	}
255: 	// special case: correlated mark join
256: 	if (join_type == JoinType::MARK && correlated_mark_join_info.correlated_types.size() > 0) {
257: 		auto &info = correlated_mark_join_info;
258: 		// Correlated MARK join
259: 		// for the correlated mark join we need to keep track of COUNT(*) and COUNT(COLUMN) for each of the correlated
260: 		// columns push into the aggregate hash table
261: 		assert(info.correlated_counts);
262: 		info.group_chunk.SetCardinality(keys);
263: 		for (idx_t i = 0; i < info.correlated_types.size(); i++) {
264: 			info.group_chunk.data[i].Reference(keys.data[i]);
265: 		}
266: 		info.payload_chunk.SetCardinality(keys);
267: 		for (idx_t i = 0; i < 2; i++) {
268: 			info.payload_chunk.data[i].Reference(keys.data[info.correlated_types.size()]);
269: 		}
270: 		info.correlated_counts->AddChunk(info.group_chunk, info.payload_chunk);
271: 	}
272: 
273: 	// prepare the keys for processing
274: 	unique_ptr<VectorData[]> key_data;
275: 	const SelectionVector *current_sel;
276: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
277: 	idx_t added_count = PrepareKeys(keys, key_data, current_sel, sel);
278: 	if (added_count < keys.size()) {
279: 		has_null = true;
280: 	}
281: 	if (added_count == 0) {
282: 		return;
283: 	}
284: 	count += added_count;
285: 
286: 	vector<unique_ptr<BufferHandle>> handles;
287: 	data_ptr_t key_locations[STANDARD_VECTOR_SIZE];
288: 	// first allocate space of where to serialize the keys and payload columns
289: 	idx_t remaining = added_count;
290: 	// first append to the last block (if any)
291: 	if (blocks.size() != 0) {
292: 		auto &last_block = blocks.back();
293: 		if (last_block.count < last_block.capacity) {
294: 			// last block has space: pin the buffer of this block
295: 			auto handle = buffer_manager.Pin(last_block.block_id);
296: 			// now append to the block
297: 			idx_t append_count = AppendToBlock(last_block, *handle, added_count, key_locations, remaining);
298: 			remaining -= append_count;
299: 			handles.push_back(move(handle));
300: 		}
301: 	}
302: 	while (remaining > 0) {
303: 		// now for the remaining data, allocate new buffers to store the data and append there
304: 		auto handle = buffer_manager.Allocate(block_capacity * entry_size);
305: 
306: 		HTDataBlock new_block;
307: 		new_block.count = 0;
308: 		new_block.capacity = block_capacity;
309: 		new_block.block_id = handle->block_id;
310: 
311: 		idx_t append_count = AppendToBlock(new_block, *handle, added_count, key_locations, remaining);
312: 		remaining -= append_count;
313: 		handles.push_back(move(handle));
314: 		blocks.push_back(new_block);
315: 	}
316: 
317: 	// hash the keys and obtain an entry in the list
318: 	// note that we only hash the keys used in the equality comparison
319: 	Vector hash_values(TypeId::HASH);
320: 	Hash(keys, *current_sel, added_count, hash_values);
321: 
322: 	// serialize the keys to the key locations
323: 	for (idx_t i = 0; i < keys.column_count(); i++) {
324: 		SerializeVectorData(key_data[i], keys.data[i].type, *current_sel, added_count, key_locations);
325: 	}
326: 	// now serialize the payload
327: 	if (build_types.size() > 0) {
328: 		for (idx_t i = 0; i < payload.column_count(); i++) {
329: 			SerializeVector(payload.data[i], payload.size(), *current_sel, added_count, key_locations);
330: 		}
331: 	}
332: 	SerializeVector(hash_values, payload.size(), *current_sel, added_count, key_locations);
333: }
334: 
335: void JoinHashTable::InsertHashes(Vector &hashes, idx_t count, data_ptr_t key_locations[]) {
336: 	assert(hashes.type == TypeId::HASH);
337: 
338: 	// use bitmask to get position in array
339: 	ApplyBitmask(hashes, count);
340: 
341: 	hashes.Normalify(count);
342: 
343: 	assert(hashes.vector_type == VectorType::FLAT_VECTOR);
344: 	auto pointers = (data_ptr_t *)hash_map->node->buffer;
345: 	auto indices = FlatVector::GetData<hash_t>(hashes);
346: 	for (idx_t i = 0; i < count; i++) {
347: 		auto index = indices[i];
348: 		// set prev in current key to the value (NOTE: this will be nullptr if
349: 		// there is none)
350: 		auto prev_pointer = (data_ptr_t *)(key_locations[i] + tuple_size);
351: 		*prev_pointer = pointers[index];
352: 
353: 		// set pointer to current tuple
354: 		pointers[index] = key_locations[i];
355: 	}
356: }
357: 
358: void JoinHashTable::Finalize() {
359: 	// the build has finished, now iterate over all the nodes and construct the final hash table
360: 	// select a HT that has at least 50% empty space
361: 	idx_t capacity = NextPowerOfTwo(std::max(count * 2, (idx_t)(Storage::BLOCK_ALLOC_SIZE / sizeof(data_ptr_t)) + 1));
362: 	// size needs to be a power of 2
363: 	assert((capacity & (capacity - 1)) == 0);
364: 	bitmask = capacity - 1;
365: 
366: 	// allocate the HT and initialize it with all-zero entries
367: 	hash_map = buffer_manager.Allocate(capacity * sizeof(data_ptr_t));
368: 	memset(hash_map->node->buffer, 0, capacity * sizeof(data_ptr_t));
369: 
370: 	Vector hashes(TypeId::HASH);
371: 	auto hash_data = FlatVector::GetData<hash_t>(hashes);
372: 	data_ptr_t key_locations[STANDARD_VECTOR_SIZE];
373: 	// now construct the actual hash table; scan the nodes
374: 	// as we can the nodes we pin all the blocks of the HT and keep them pinned until the HT is destroyed
375: 	// this is so that we can keep pointers around to the blocks
376: 	// FIXME: if we cannot keep everything pinned in memory, we could switch to an out-of-memory merge join or so
377: 	for (auto &block : blocks) {
378: 		auto handle = buffer_manager.Pin(block.block_id);
379: 		data_ptr_t dataptr = handle->node->buffer;
380: 		idx_t entry = 0;
381: 		while (entry < block.count) {
382: 			// fetch the next vector of entries from the blocks
383: 			idx_t next = std::min((idx_t)STANDARD_VECTOR_SIZE, block.count - entry);
384: 			for (idx_t i = 0; i < next; i++) {
385: 				hash_data[i] = *((hash_t *)(dataptr + tuple_size));
386: 				key_locations[i] = dataptr;
387: 				dataptr += entry_size;
388: 			}
389: 			// now insert into the hash table
390: 			InsertHashes(hashes, next, key_locations);
391: 
392: 			entry += next;
393: 		}
394: 		pinned_handles.push_back(move(handle));
395: 	}
396: 	finalized = true;
397: }
398: 
399: unique_ptr<ScanStructure> JoinHashTable::Probe(DataChunk &keys) {
400: 	assert(count > 0); // should be handled before
401: 	assert(finalized);
402: 
403: 	// set up the scan structure
404: 	auto ss = make_unique<ScanStructure>(*this);
405: 
406: 	// first prepare the keys for probing
407: 	const SelectionVector *current_sel;
408: 	ss->count = PrepareKeys(keys, ss->key_data, current_sel, ss->sel_vector);
409: 	if (ss->count == 0) {
410: 		return ss;
411: 	}
412: 
413: 	// hash all the keys
414: 	Vector hashes(TypeId::HASH);
415: 	Hash(keys, *current_sel, ss->count, hashes);
416: 
417: 	// now initialize the pointers of the scan structure based on the hashes
418: 	ApplyBitmask(hashes, *current_sel, ss->count, ss->pointers);
419: 
420: 	if (join_type != JoinType::INNER) {
421: 		ss->found_match = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);
422: 		memset(ss->found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
423: 	}
424: 
425: 	// create the selection vector linking to only non-empty entries
426: 	idx_t count = 0;
427: 	auto pointers = FlatVector::GetData<data_ptr_t>(ss->pointers);
428: 	for (idx_t i = 0; i < ss->count; i++) {
429: 		auto idx = current_sel->get_index(i);
430: 		auto chain_pointer = (data_ptr_t *)(pointers[idx]);
431: 		pointers[idx] = *chain_pointer;
432: 		if (pointers[idx]) {
433: 			ss->sel_vector.set_index(count++, idx);
434: 		}
435: 	}
436: 	ss->count = count;
437: 	return ss;
438: }
439: 
440: ScanStructure::ScanStructure(JoinHashTable &ht) : sel_vector(STANDARD_VECTOR_SIZE), ht(ht), finished(false) {
441: 	pointers.Initialize(TypeId::POINTER);
442: }
443: 
444: void ScanStructure::Next(DataChunk &keys, DataChunk &left, DataChunk &result) {
445: 	if (finished) {
446: 		return;
447: 	}
448: 
449: 	switch (ht.join_type) {
450: 	case JoinType::INNER:
451: 		NextInnerJoin(keys, left, result);
452: 		break;
453: 	case JoinType::SEMI:
454: 		NextSemiJoin(keys, left, result);
455: 		break;
456: 	case JoinType::MARK:
457: 		NextMarkJoin(keys, left, result);
458: 		break;
459: 	case JoinType::ANTI:
460: 		NextAntiJoin(keys, left, result);
461: 		break;
462: 	case JoinType::LEFT:
463: 		NextLeftJoin(keys, left, result);
464: 		break;
465: 	case JoinType::SINGLE:
466: 		NextSingleJoin(keys, left, result);
467: 		break;
468: 	default:
469: 		throw Exception("Unhandled join type in JoinHashTable");
470: 	}
471: }
472: 
473: template <bool NO_MATCH_SEL, class T, class OP>
474: static idx_t TemplatedGather(VectorData &vdata, Vector &pointers, const SelectionVector &current_sel, idx_t count,
475:                              idx_t offset, SelectionVector *match_sel, SelectionVector *no_match_sel,
476:                              idx_t &no_match_count) {
477: 	idx_t result_count = 0;
478: 	auto data = (T *)vdata.data;
479: 	auto ptrs = FlatVector::GetData<uintptr_t>(pointers);
480: 	for (idx_t i = 0; i < count; i++) {
481: 		auto idx = current_sel.get_index(i);
482: 		auto kidx = vdata.sel->get_index(idx);
483: 		auto gdata = (T *)(ptrs[idx] + offset);
484: 		if ((*vdata.nullmask)[kidx]) {
485: 			if (IsNullValue<T>(*gdata)) {
486: 				match_sel->set_index(result_count++, idx);
487: 			} else {
488: 				if (NO_MATCH_SEL) {
489: 					no_match_sel->set_index(no_match_count++, idx);
490: 				}
491: 			}
492: 		} else {
493: 			if (OP::template Operation<T>(data[kidx], *gdata)) {
494: 				match_sel->set_index(result_count++, idx);
495: 			} else {
496: 				if (NO_MATCH_SEL) {
497: 					no_match_sel->set_index(no_match_count++, idx);
498: 				}
499: 			}
500: 		}
501: 	}
502: 	return result_count;
503: }
504: 
505: template <bool NO_MATCH_SEL, class OP>
506: static idx_t GatherSwitch(VectorData &data, TypeId type, Vector &pointers, const SelectionVector &current_sel,
507:                           idx_t count, idx_t offset, SelectionVector *match_sel, SelectionVector *no_match_sel,
508:                           idx_t &no_match_count) {
509: 	switch (type) {
510: 	case TypeId::BOOL:
511: 	case TypeId::INT8:
512: 		return TemplatedGather<NO_MATCH_SEL, int8_t, OP>(data, pointers, current_sel, count, offset, match_sel,
513: 		                                                 no_match_sel, no_match_count);
514: 	case TypeId::INT16:
515: 		return TemplatedGather<NO_MATCH_SEL, int16_t, OP>(data, pointers, current_sel, count, offset, match_sel,
516: 		                                                  no_match_sel, no_match_count);
517: 	case TypeId::INT32:
518: 		return TemplatedGather<NO_MATCH_SEL, int32_t, OP>(data, pointers, current_sel, count, offset, match_sel,
519: 		                                                  no_match_sel, no_match_count);
520: 	case TypeId::INT64:
521: 		return TemplatedGather<NO_MATCH_SEL, int64_t, OP>(data, pointers, current_sel, count, offset, match_sel,
522: 		                                                  no_match_sel, no_match_count);
523: 	case TypeId::FLOAT:
524: 		return TemplatedGather<NO_MATCH_SEL, float, OP>(data, pointers, current_sel, count, offset, match_sel,
525: 		                                                no_match_sel, no_match_count);
526: 	case TypeId::DOUBLE:
527: 		return TemplatedGather<NO_MATCH_SEL, double, OP>(data, pointers, current_sel, count, offset, match_sel,
528: 		                                                 no_match_sel, no_match_count);
529: 	case TypeId::VARCHAR:
530: 		return TemplatedGather<NO_MATCH_SEL, string_t, OP>(data, pointers, current_sel, count, offset, match_sel,
531: 		                                                   no_match_sel, no_match_count);
532: 	default:
533: 		throw NotImplementedException("Unimplemented type for GatherSwitch");
534: 	}
535: }
536: 
537: template <bool NO_MATCH_SEL>
538: idx_t ScanStructure::ResolvePredicates(DataChunk &keys, SelectionVector *match_sel, SelectionVector *no_match_sel) {
539: 	SelectionVector *current_sel = &this->sel_vector;
540: 	idx_t remaining_count = this->count;
541: 	idx_t offset = 0;
542: 	idx_t no_match_count = 0;
543: 	for (idx_t i = 0; i < ht.predicates.size(); i++) {
544: 		switch (ht.predicates[i]) {
545: 		case ExpressionType::COMPARE_EQUAL:
546: 			remaining_count =
547: 			    GatherSwitch<NO_MATCH_SEL, Equals>(key_data[i], keys.data[i].type, this->pointers, *current_sel,
548: 			                                       remaining_count, offset, match_sel, no_match_sel, no_match_count);
549: 			break;
550: 		case ExpressionType::COMPARE_NOTEQUAL:
551: 			remaining_count =
552: 			    GatherSwitch<NO_MATCH_SEL, NotEquals>(key_data[i], keys.data[i].type, this->pointers, *current_sel,
553: 			                                          remaining_count, offset, match_sel, no_match_sel, no_match_count);
554: 			break;
555: 		case ExpressionType::COMPARE_GREATERTHAN:
556: 			remaining_count = GatherSwitch<NO_MATCH_SEL, GreaterThan>(key_data[i], keys.data[i].type, this->pointers,
557: 			                                                          *current_sel, remaining_count, offset, match_sel,
558: 			                                                          no_match_sel, no_match_count);
559: 			break;
560: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
561: 			remaining_count = GatherSwitch<NO_MATCH_SEL, GreaterThanEquals>(
562: 			    key_data[i], keys.data[i].type, this->pointers, *current_sel, remaining_count, offset, match_sel,
563: 			    no_match_sel, no_match_count);
564: 			break;
565: 		case ExpressionType::COMPARE_LESSTHAN:
566: 			remaining_count =
567: 			    GatherSwitch<NO_MATCH_SEL, LessThan>(key_data[i], keys.data[i].type, this->pointers, *current_sel,
568: 			                                         remaining_count, offset, match_sel, no_match_sel, no_match_count);
569: 			break;
570: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
571: 			remaining_count = GatherSwitch<NO_MATCH_SEL, LessThanEquals>(key_data[i], keys.data[i].type, this->pointers,
572: 			                                                             *current_sel, remaining_count, offset,
573: 			                                                             match_sel, no_match_sel, no_match_count);
574: 			break;
575: 		default:
576: 			throw NotImplementedException("Unimplemented comparison type for join");
577: 		}
578: 		if (remaining_count == 0) {
579: 			break;
580: 		}
581: 		current_sel = match_sel;
582: 		offset += GetTypeIdSize(keys.data[i].type);
583: 	}
584: 	return remaining_count;
585: }
586: 
587: idx_t ScanStructure::ResolvePredicates(DataChunk &keys, SelectionVector &match_sel, SelectionVector &no_match_sel) {
588: 	return ResolvePredicates<true>(keys, &match_sel, &no_match_sel);
589: }
590: 
591: idx_t ScanStructure::ResolvePredicates(DataChunk &keys, SelectionVector &match_sel) {
592: 	return ResolvePredicates<false>(keys, &match_sel, nullptr);
593: }
594: 
595: idx_t ScanStructure::ScanInnerJoin(DataChunk &keys, SelectionVector &result_vector) {
596: 	while (true) {
597: 		// resolve the predicates for this set of keys
598: 		idx_t result_count = ResolvePredicates(keys, result_vector);
599: 
600: 		// after doing all the comparisons set the found_match vector
601: 		if (found_match) {
602: 			for (idx_t i = 0; i < result_count; i++) {
603: 				auto idx = result_vector.get_index(i);
604: 				found_match[idx] = true;
605: 			}
606: 		}
607: 		if (result_count > 0) {
608: 			return result_count;
609: 		}
610: 		// no matches found: check the next set of pointers
611: 		AdvancePointers();
612: 		if (this->count == 0) {
613: 			return 0;
614: 		}
615: 	}
616: }
617: 
618: void ScanStructure::AdvancePointers(const SelectionVector &sel, idx_t sel_count) {
619: 	// now for all the pointers, we move on to the next set of pointers
620: 	idx_t new_count = 0;
621: 	auto ptrs = FlatVector::GetData<data_ptr_t>(this->pointers);
622: 	for (idx_t i = 0; i < sel_count; i++) {
623: 		auto idx = sel.get_index(i);
624: 		auto chain_pointer = (data_ptr_t *)(ptrs[idx] + ht.tuple_size);
625: 		ptrs[idx] = *chain_pointer;
626: 		if (ptrs[idx]) {
627: 			this->sel_vector.set_index(new_count++, idx);
628: 		}
629: 	}
630: 	this->count = new_count;
631: }
632: 
633: void ScanStructure::AdvancePointers() {
634: 	AdvancePointers(this->sel_vector, this->count);
635: }
636: 
637: template <class T>
638: static void TemplatedGatherResult(Vector &result, uintptr_t *pointers, const SelectionVector &result_vector,
639:                                   const SelectionVector &sel_vector, idx_t count, idx_t offset) {
640: 	auto rdata = FlatVector::GetData<T>(result);
641: 	auto &nullmask = FlatVector::Nullmask(result);
642: 	for (idx_t i = 0; i < count; i++) {
643: 		auto ridx = result_vector.get_index(i);
644: 		auto pidx = sel_vector.get_index(i);
645: 		auto hdata = (T *)(pointers[pidx] + offset);
646: 		if (IsNullValue<T>(*hdata)) {
647: 			nullmask[ridx] = true;
648: 		} else {
649: 			rdata[ridx] = *hdata;
650: 		}
651: 	}
652: }
653: 
654: void ScanStructure::GatherResult(Vector &result, const SelectionVector &result_vector,
655:                                  const SelectionVector &sel_vector, idx_t count, idx_t &offset) {
656: 	result.vector_type = VectorType::FLAT_VECTOR;
657: 	auto ptrs = FlatVector::GetData<uintptr_t>(pointers);
658: 	switch (result.type) {
659: 	case TypeId::BOOL:
660: 	case TypeId::INT8:
661: 		TemplatedGatherResult<int8_t>(result, ptrs, result_vector, sel_vector, count, offset);
662: 		break;
663: 	case TypeId::INT16:
664: 		TemplatedGatherResult<int16_t>(result, ptrs, result_vector, sel_vector, count, offset);
665: 		break;
666: 	case TypeId::INT32:
667: 		TemplatedGatherResult<int32_t>(result, ptrs, result_vector, sel_vector, count, offset);
668: 		break;
669: 	case TypeId::INT64:
670: 		TemplatedGatherResult<int64_t>(result, ptrs, result_vector, sel_vector, count, offset);
671: 		break;
672: 	case TypeId::FLOAT:
673: 		TemplatedGatherResult<float>(result, ptrs, result_vector, sel_vector, count, offset);
674: 		break;
675: 	case TypeId::DOUBLE:
676: 		TemplatedGatherResult<double>(result, ptrs, result_vector, sel_vector, count, offset);
677: 		break;
678: 	case TypeId::VARCHAR:
679: 		TemplatedGatherResult<string_t>(result, ptrs, result_vector, sel_vector, count, offset);
680: 		break;
681: 	default:
682: 		throw NotImplementedException("Unimplemented type for ScanStructure::GatherResult");
683: 	}
684: 	offset += GetTypeIdSize(result.type);
685: }
686: 
687: void ScanStructure::GatherResult(Vector &result, const SelectionVector &sel_vector, idx_t count, idx_t &offset) {
688: 	GatherResult(result, FlatVector::IncrementalSelectionVector, sel_vector, count, offset);
689: }
690: 
691: void ScanStructure::NextInnerJoin(DataChunk &keys, DataChunk &left, DataChunk &result) {
692: 	assert(result.column_count() == left.column_count() + ht.build_types.size());
693: 	if (this->count == 0) {
694: 		// no pointers left to chase
695: 		return;
696: 	}
697: 
698: 	SelectionVector result_vector(STANDARD_VECTOR_SIZE);
699: 
700: 	idx_t result_count = ScanInnerJoin(keys, result_vector);
701: 	if (result_count > 0) {
702: 		// matches were found
703: 		// construct the result
704: 		// on the LHS, we create a slice using the result vector
705: 		result.Slice(left, result_vector, result_count);
706: 
707: 		// on the RHS, we need to fetch the data from the hash table
708: 		idx_t offset = ht.condition_size;
709: 		for (idx_t i = 0; i < ht.build_types.size(); i++) {
710: 			auto &vector = result.data[left.column_count() + i];
711: 			assert(vector.type == ht.build_types[i]);
712: 			GatherResult(vector, result_vector, result_count, offset);
713: 		}
714: 		AdvancePointers();
715: 	}
716: }
717: 
718: void ScanStructure::ScanKeyMatches(DataChunk &keys) {
719: 	// the semi-join, anti-join and mark-join we handle a differently from the inner join
720: 	// since there can be at most STANDARD_VECTOR_SIZE results
721: 	// we handle the entire chunk in one call to Next().
722: 	// for every pointer, we keep chasing pointers and doing comparisons.
723: 	// this results in a boolean array indicating whether or not the tuple has a match
724: 	SelectionVector match_sel(STANDARD_VECTOR_SIZE), no_match_sel(STANDARD_VECTOR_SIZE);
725: 	while (this->count > 0) {
726: 		// resolve the predicates for the current set of pointers
727: 		idx_t match_count = ResolvePredicates(keys, match_sel, no_match_sel);
728: 		idx_t no_match_count = this->count - match_count;
729: 
730: 		// mark each of the matches as found
731: 		for (idx_t i = 0; i < match_count; i++) {
732: 			found_match[match_sel.get_index(i)] = true;
733: 		}
734: 		// continue searching for the ones where we did not find a match yet
735: 		AdvancePointers(no_match_sel, no_match_count);
736: 	}
737: }
738: 
739: template <bool MATCH> void ScanStructure::NextSemiOrAntiJoin(DataChunk &keys, DataChunk &left, DataChunk &result) {
740: 	assert(left.column_count() == result.column_count());
741: 	assert(keys.size() == left.size());
742: 	// create the selection vector from the matches that were found
743: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
744: 	idx_t result_count = 0;
745: 	for (idx_t i = 0; i < keys.size(); i++) {
746: 		if (found_match[i] == MATCH) {
747: 			// part of the result
748: 			sel.set_index(result_count++, i);
749: 		}
750: 	}
751: 	// construct the final result
752: 	if (result_count > 0) {
753: 		// we only return the columns on the left side
754: 		// reference the columns of the left side from the result
755: 		result.Slice(left, sel, result_count);
756: 	} else {
757: 		assert(result.size() == 0);
758: 	}
759: }
760: 
761: void ScanStructure::NextSemiJoin(DataChunk &keys, DataChunk &left, DataChunk &result) {
762: 	// first scan for key matches
763: 	ScanKeyMatches(keys);
764: 	// then construct the result from all tuples with a match
765: 	NextSemiOrAntiJoin<true>(keys, left, result);
766: 
767: 	finished = true;
768: }
769: 
770: void ScanStructure::NextAntiJoin(DataChunk &keys, DataChunk &left, DataChunk &result) {
771: 	// first scan for key matches
772: 	ScanKeyMatches(keys);
773: 	// then construct the result from all tuples that did not find a match
774: 	NextSemiOrAntiJoin<false>(keys, left, result);
775: 
776: 	finished = true;
777: }
778: 
779: void ScanStructure::ConstructMarkJoinResult(DataChunk &join_keys, DataChunk &child, DataChunk &result) {
780: 	// for the initial set of columns we just reference the left side
781: 	result.SetCardinality(child);
782: 	for (idx_t i = 0; i < child.column_count(); i++) {
783: 		result.data[i].Reference(child.data[i]);
784: 	}
785: 	auto &mark_vector = result.data.back();
786: 	mark_vector.vector_type = VectorType::FLAT_VECTOR;
787: 	// first we set the NULL values from the join keys
788: 	// if there is any NULL in the keys, the result is NULL
789: 	auto bool_result = FlatVector::GetData<bool>(mark_vector);
790: 	auto &nullmask = FlatVector::Nullmask(mark_vector);
791: 	for (idx_t col_idx = 0; col_idx < join_keys.column_count(); col_idx++) {
792: 		if (ht.null_values_are_equal[col_idx]) {
793: 			continue;
794: 		}
795: 		VectorData jdata;
796: 		join_keys.data[col_idx].Orrify(join_keys.size(), jdata);
797: 		if (jdata.nullmask->any()) {
798: 			for (idx_t i = 0; i < join_keys.size(); i++) {
799: 				auto jidx = jdata.sel->get_index(i);
800: 				nullmask[i] = (*jdata.nullmask)[jidx];
801: 			}
802: 		}
803: 	}
804: 	// now set the remaining entries to either true or false based on whether a match was found
805: 	if (found_match) {
806: 		for (idx_t i = 0; i < child.size(); i++) {
807: 			bool_result[i] = found_match[i];
808: 		}
809: 	} else {
810: 		memset(bool_result, 0, sizeof(bool) * child.size());
811: 	}
812: 	// if the right side contains NULL values, the result of any FALSE becomes NULL
813: 	if (ht.has_null) {
814: 		for (idx_t i = 0; i < child.size(); i++) {
815: 			if (!bool_result[i]) {
816: 				nullmask[i] = true;
817: 			}
818: 		}
819: 	}
820: }
821: 
822: void ScanStructure::NextMarkJoin(DataChunk &keys, DataChunk &input, DataChunk &result) {
823: 	assert(result.column_count() == input.column_count() + 1);
824: 	assert(result.data.back().type == TypeId::BOOL);
825: 	// this method should only be called for a non-empty HT
826: 	assert(ht.count > 0);
827: 
828: 	ScanKeyMatches(keys);
829: 	if (ht.correlated_mark_join_info.correlated_types.size() == 0) {
830: 		ConstructMarkJoinResult(keys, input, result);
831: 	} else {
832: 		auto &info = ht.correlated_mark_join_info;
833: 		// there are correlated columns
834: 		// first we fetch the counts from the aggregate hashtable corresponding to these entries
835: 		assert(keys.column_count() == info.group_chunk.column_count() + 1);
836: 		info.group_chunk.SetCardinality(keys);
837: 		for (idx_t i = 0; i < info.group_chunk.column_count(); i++) {
838: 			info.group_chunk.data[i].Reference(keys.data[i]);
839: 		}
840: 		info.correlated_counts->FetchAggregates(info.group_chunk, info.result_chunk);
841: 
842: 		// for the initial set of columns we just reference the left side
843: 		result.SetCardinality(input);
844: 		for (idx_t i = 0; i < input.column_count(); i++) {
845: 			result.data[i].Reference(input.data[i]);
846: 		}
847: 		// create the result matching vector
848: 		auto &last_key = keys.data.back();
849: 		auto &result_vector = result.data.back();
850: 		// first set the nullmask based on whether or not there were NULL values in the join key
851: 		result_vector.vector_type = VectorType::FLAT_VECTOR;
852: 		auto bool_result = FlatVector::GetData<bool>(result_vector);
853: 		auto &nullmask = FlatVector::Nullmask(result_vector);
854: 		switch (last_key.vector_type) {
855: 		case VectorType::CONSTANT_VECTOR:
856: 			if (ConstantVector::IsNull(last_key)) {
857: 				nullmask.set();
858: 			}
859: 			break;
860: 		case VectorType::FLAT_VECTOR:
861: 			nullmask = FlatVector::Nullmask(last_key);
862: 			break;
863: 		default: {
864: 			VectorData kdata;
865: 			last_key.Orrify(keys.size(), kdata);
866: 			for (idx_t i = 0; i < input.size(); i++) {
867: 				auto kidx = kdata.sel->get_index(i);
868: 				;
869: 				nullmask[i] = (*kdata.nullmask)[kidx];
870: 			}
871: 			break;
872: 		}
873: 		}
874: 
875: 		auto count_star = FlatVector::GetData<int64_t>(info.result_chunk.data[0]);
876: 		auto count = FlatVector::GetData<int64_t>(info.result_chunk.data[1]);
877: 		// set the entries to either true or false based on whether a match was found
878: 		for (idx_t i = 0; i < input.size(); i++) {
879: 			assert(count_star[i] >= count[i]);
880: 			bool_result[i] = found_match ? found_match[i] : false;
881: 			if (!bool_result[i] && count_star[i] > count[i]) {
882: 				// RHS has NULL value and result is false: set to null
883: 				nullmask[i] = true;
884: 			}
885: 			if (count_star[i] == 0) {
886: 				// count == 0, set nullmask to false (we know the result is false now)
887: 				nullmask[i] = false;
888: 			}
889: 		}
890: 	}
891: 	finished = true;
892: }
893: 
894: void ScanStructure::NextLeftJoin(DataChunk &keys, DataChunk &left, DataChunk &result) {
895: 	// a LEFT OUTER JOIN is identical to an INNER JOIN except all tuples that do
896: 	// not have a match must return at least one tuple (with the right side set
897: 	// to NULL in every column)
898: 	NextInnerJoin(keys, left, result);
899: 	if (result.size() == 0) {
900: 		// no entries left from the normal join
901: 		// fill in the result of the remaining left tuples
902: 		// together with NULL values on the right-hand side
903: 		idx_t remaining_count = 0;
904: 		SelectionVector sel(STANDARD_VECTOR_SIZE);
905: 		for (idx_t i = 0; i < left.size(); i++) {
906: 			if (!found_match[i]) {
907: 				sel.set_index(remaining_count++, i);
908: 			}
909: 		}
910: 		if (remaining_count > 0) {
911: 			// have remaining tuples
912: 			// slice the left side with tuples that did not find a match
913: 			result.Slice(left, sel, remaining_count);
914: 
915: 			// now set the right side to NULL
916: 			for (idx_t i = left.column_count(); i < result.column_count(); i++) {
917: 				result.data[i].vector_type = VectorType::CONSTANT_VECTOR;
918: 				ConstantVector::SetNull(result.data[i], true);
919: 			}
920: 		}
921: 		finished = true;
922: 	}
923: }
924: 
925: void ScanStructure::NextSingleJoin(DataChunk &keys, DataChunk &input, DataChunk &result) {
926: 	// single join
927: 	// this join is similar to the semi join except that
928: 	// (1) we actually return data from the RHS and
929: 	// (2) we return NULL for that data if there is no match
930: 	idx_t result_count = 0;
931: 	SelectionVector result_sel(STANDARD_VECTOR_SIZE);
932: 	SelectionVector match_sel(STANDARD_VECTOR_SIZE), no_match_sel(STANDARD_VECTOR_SIZE);
933: 	while (this->count > 0) {
934: 		// resolve the predicates for the current set of pointers
935: 		idx_t match_count = ResolvePredicates(keys, match_sel, no_match_sel);
936: 		idx_t no_match_count = this->count - match_count;
937: 
938: 		// mark each of the matches as found
939: 		for (idx_t i = 0; i < match_count; i++) {
940: 			// found a match for this index
941: 			auto index = match_sel.get_index(i);
942: 			found_match[index] = true;
943: 			result_sel.set_index(result_count++, index);
944: 		}
945: 		// continue searching for the ones where we did not find a match yet
946: 		AdvancePointers(no_match_sel, no_match_count);
947: 	}
948: 	// reference the columns of the left side from the result
949: 	assert(input.column_count() > 0);
950: 	for (idx_t i = 0; i < input.column_count(); i++) {
951: 		result.data[i].Reference(input.data[i]);
952: 	}
953: 	// now fetch the data from the RHS
954: 	idx_t offset = ht.condition_size;
955: 	for (idx_t i = 0; i < ht.build_types.size(); i++) {
956: 		auto &vector = result.data[input.column_count() + i];
957: 		// set NULL entries for every entry that was not found
958: 		auto &nullmask = FlatVector::Nullmask(vector);
959: 		nullmask.set();
960: 		for (idx_t j = 0; j < result_count; j++) {
961: 			nullmask[result_sel.get_index(j)] = false;
962: 		}
963: 		// for the remaining values we fetch the values
964: 		GatherResult(vector, result_sel, result_sel, result_count, offset);
965: 	}
966: 	result.SetCardinality(input.size());
967: 
968: 	// like the SEMI, ANTI and MARK join types, the SINGLE join only ever does one pass over the HT per input chunk
969: 	finished = true;
970: }
971: 
972: } // namespace duckdb
[end of src/execution/join_hashtable.cpp]
[start of src/function/scalar/string/regexp.cpp]
1: #include "duckdb/function/scalar/string_functions.hpp"
2: #include "duckdb/common/exception.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/planner/expression/bound_function_expression.hpp"
6: #include "duckdb/common/vector_operations/unary_executor.hpp"
7: #include "duckdb/common/vector_operations/binary_executor.hpp"
8: #include "duckdb/common/vector_operations/ternary_executor.hpp"
9: #include "utf8proc_wrapper.hpp"
10: 
11: #include "re2/re2.h"
12: 
13: using namespace std;
14: 
15: namespace duckdb {
16: 
17: RegexpMatchesBindData::RegexpMatchesBindData(unique_ptr<RE2> constant_pattern, string range_min, string range_max,
18:                                              bool range_success)
19:     : constant_pattern(std::move(constant_pattern)), range_min(range_min), range_max(range_max),
20:       range_success(range_success) {
21: }
22: 
23: RegexpMatchesBindData::~RegexpMatchesBindData() {
24: }
25: 
26: unique_ptr<FunctionData> RegexpMatchesBindData::Copy() {
27: 	return make_unique<RegexpMatchesBindData>(move(constant_pattern), range_min, range_max, range_success);
28: }
29: 
30: static inline re2::StringPiece CreateStringPiece(string_t &input) {
31: 	return re2::StringPiece(input.GetData(), input.GetSize());
32: }
33: 
34: static void regexp_matches_function(DataChunk &args, ExpressionState &state, Vector &result) {
35: 	auto &strings = args.data[0];
36: 	auto &patterns = args.data[1];
37: 
38: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
39: 	auto &info = (RegexpMatchesBindData &)*func_expr.bind_info;
40: 
41: 	RE2::Options options;
42: 	options.set_log_errors(false);
43: 
44: 	if (info.constant_pattern) {
45: 		// FIXME: this should be a unary loop
46: 		UnaryExecutor::Execute<string_t, bool, true>(strings, result, args.size(), [&](string_t input) {
47: 			return RE2::PartialMatch(CreateStringPiece(input), *info.constant_pattern);
48: 		});
49: 	} else {
50: 		BinaryExecutor::Execute<string_t, string_t, bool, true>(
51: 		    strings, patterns, result, args.size(), [&](string_t input, string_t pattern) {
52: 			    RE2 re(CreateStringPiece(pattern), options);
53: 			    if (!re.ok()) {
54: 				    throw Exception(re.error());
55: 			    }
56: 			    return RE2::PartialMatch(CreateStringPiece(input), re);
57: 		    });
58: 	}
59: }
60: 
61: static unique_ptr<FunctionData> regexp_matches_get_bind_function(BoundFunctionExpression &expr,
62:                                                                  ClientContext &context) {
63: 	// pattern is the second argument. If its constant, we can already prepare the pattern and store it for later.
64: 	assert(expr.children.size() == 2);
65: 	if (expr.children[1]->IsScalar()) {
66: 		Value pattern_str = ExpressionExecutor::EvaluateScalar(*expr.children[1]);
67: 		if (!pattern_str.is_null && pattern_str.type == TypeId::VARCHAR) {
68: 			RE2::Options options;
69: 			options.set_log_errors(false);
70: 			auto re = make_unique<RE2>(pattern_str.str_value, options);
71: 			if (!re->ok()) {
72: 				throw Exception(re->error());
73: 			}
74: 
75: 			string range_min, range_max;
76: 			auto range_success = re->PossibleMatchRange(&range_min, &range_max, 1000);
77: 			// range_min and range_max might produce non-valid UTF8 strings, e.g. in the case of 'a.*'
78: 			// in this case we don't push a range filter
79: 			if (range_success && (Utf8Proc::Analyze(range_min) == UnicodeType::INVALID ||
80: 			                      Utf8Proc::Analyze(range_max) == UnicodeType::INVALID)) {
81: 				range_success = false;
82: 			}
83: 
84: 			return make_unique<RegexpMatchesBindData>(move(re), range_min, range_max, range_success);
85: 		}
86: 	}
87: 	return make_unique<RegexpMatchesBindData>(nullptr, "", "", false);
88: }
89: 
90: static void regexp_replace_function(DataChunk &args, ExpressionState &state, Vector &result) {
91: 	auto &strings = args.data[0];
92: 	auto &patterns = args.data[1];
93: 	auto &replaces = args.data[2];
94: 
95: 	RE2::Options options;
96: 	options.set_log_errors(false);
97: 
98: 	TernaryExecutor::Execute<string_t, string_t, string_t, string_t>(
99: 	    strings, patterns, replaces, result, args.size(), [&](string_t input, string_t pattern, string_t replace) {
100: 		    RE2 re(CreateStringPiece(pattern), options);
101: 		    std::string sstring(input.GetData(), input.GetSize());
102: 		    RE2::Replace(&sstring, re, CreateStringPiece(replace));
103: 		    return StringVector::AddString(result, sstring);
104: 	    });
105: }
106: 
107: void RegexpFun::RegisterFunction(BuiltinFunctions &set) {
108: 	set.AddFunction(ScalarFunction("regexp_matches", {SQLType::VARCHAR, SQLType::VARCHAR}, SQLType::BOOLEAN,
109: 	                               regexp_matches_function, false, regexp_matches_get_bind_function));
110: 	set.AddFunction(ScalarFunction("regexp_replace", {SQLType::VARCHAR, SQLType::VARCHAR, SQLType::VARCHAR},
111: 	                               SQLType::VARCHAR, regexp_replace_function));
112: }
113: 
114: } // namespace duckdb
[end of src/function/scalar/string/regexp.cpp]
[start of src/include/duckdb/execution/index/art/art_key.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/index/art/art_key.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/exception.hpp"
13: 
14: namespace duckdb {
15: 
16: class Key {
17: public:
18: 	Key(unique_ptr<data_t[]> data, idx_t len);
19: 
20: 	idx_t len;
21: 	unique_ptr<data_t[]> data;
22: 
23: public:
24: 	template <class T> static unique_ptr<Key> CreateKey(T element, bool is_little_endian) {
25: 		auto data = Key::CreateData<T>(element, is_little_endian);
26: 		return make_unique<Key>(move(data), sizeof(element));
27: 	}
28: 
29: public:
30: 	data_t &operator[](std::size_t i) {
31: 		return data[i];
32: 	}
33: 	const data_t &operator[](std::size_t i) const {
34: 		return data[i];
35: 	}
36: 	bool operator>(const Key &k) const;
37: 	bool operator<(const Key &k) const;
38: 	bool operator>=(const Key &k) const;
39: 	bool operator==(const Key &k) const;
40: 
41: 	string ToString(bool is_little_endian, TypeId type);
42: 
43: 	static uint32_t EncodeFloat(float x);
44: 	static uint64_t EncodeDouble(double x);
45: 
46: private:
47: 	template <class T> static unique_ptr<data_t[]> CreateData(T value, bool is_little_endian) {
48: 		throw NotImplementedException("Cannot create data from this type");
49: 	}
50: };
51: 
52: template <> unique_ptr<data_t[]> Key::CreateData(int8_t value, bool is_little_endian);
53: template <> unique_ptr<data_t[]> Key::CreateData(int16_t value, bool is_little_endian);
54: template <> unique_ptr<data_t[]> Key::CreateData(int32_t value, bool is_little_endian);
55: template <> unique_ptr<data_t[]> Key::CreateData(int64_t value, bool is_little_endian);
56: template <> unique_ptr<data_t[]> Key::CreateData(double value, bool is_little_endian);
57: template <> unique_ptr<data_t[]> Key::CreateData(float value, bool is_little_endian);
58: 
59: template <> unique_ptr<Key> Key::CreateKey(string_t value, bool is_little_endian);
60: template <> unique_ptr<Key> Key::CreateKey(const char *value, bool is_little_endian);
61: 
62: } // namespace duckdb
[end of src/include/duckdb/execution/index/art/art_key.hpp]
[start of src/optimizer/join_order_optimizer.cpp]
1: #include "duckdb/optimizer/join_order_optimizer.hpp"
2: 
3: #include "duckdb/planner/expression/list.hpp"
4: #include "duckdb/planner/expression_iterator.hpp"
5: #include "duckdb/planner/operator/list.hpp"
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: using JoinNode = JoinOrderOptimizer::JoinNode;
11: 
12: //! Returns true if A and B are disjoint, false otherwise
13: template <class T> static bool Disjoint(unordered_set<T> &a, unordered_set<T> &b) {
14: 	for (auto &entry : a) {
15: 		if (b.find(entry) != b.end()) {
16: 			return false;
17: 		}
18: 	}
19: 	return true;
20: }
21: 
22: //! Extract the set of relations referred to inside an expression
23: bool JoinOrderOptimizer::ExtractBindings(Expression &expression, unordered_set<idx_t> &bindings) {
24: 	if (expression.type == ExpressionType::BOUND_COLUMN_REF) {
25: 		auto &colref = (BoundColumnRefExpression &)expression;
26: 		assert(colref.depth == 0);
27: 		assert(colref.binding.table_index != INVALID_INDEX);
28: 		// map the base table index to the relation index used by the JoinOrderOptimizer
29: 		assert(relation_mapping.find(colref.binding.table_index) != relation_mapping.end());
30: 		bindings.insert(relation_mapping[colref.binding.table_index]);
31: 	}
32: 	if (expression.type == ExpressionType::BOUND_REF) {
33: 		// bound expression
34: 		bindings.clear();
35: 		return false;
36: 	}
37: 	assert(expression.type != ExpressionType::SUBQUERY);
38: 	bool can_reorder = true;
39: 	ExpressionIterator::EnumerateChildren(expression, [&](Expression &expr) {
40: 		if (!ExtractBindings(expr, bindings)) {
41: 			can_reorder = false;
42: 			return;
43: 		}
44: 	});
45: 	return can_reorder;
46: }
47: 
48: static unique_ptr<LogicalOperator> PushFilter(unique_ptr<LogicalOperator> node, unique_ptr<Expression> expr) {
49: 	// push an expression into a filter
50: 	// first check if we have any filter to push it into
51: 	if (node->type != LogicalOperatorType::FILTER) {
52: 		// we don't, we need to create one
53: 		auto filter = make_unique<LogicalFilter>();
54: 		filter->children.push_back(move(node));
55: 		node = move(filter);
56: 	}
57: 	// push the filter into the LogicalFilter
58: 	assert(node->type == LogicalOperatorType::FILTER);
59: 	auto filter = (LogicalFilter *)node.get();
60: 	filter->expressions.push_back(move(expr));
61: 	return node;
62: }
63: 
64: bool JoinOrderOptimizer::ExtractJoinRelations(LogicalOperator &input_op, vector<LogicalOperator *> &filter_operators,
65:                                               LogicalOperator *parent) {
66: 	LogicalOperator *op = &input_op;
67: 	while (op->children.size() == 1 &&
68: 	       (op->type != LogicalOperatorType::PROJECTION && op->type != LogicalOperatorType::EXPRESSION_GET)) {
69: 		if (op->type == LogicalOperatorType::FILTER) {
70: 			// extract join conditions from filter
71: 			filter_operators.push_back(op);
72: 		}
73: 		if (op->type == LogicalOperatorType::AGGREGATE_AND_GROUP_BY || op->type == LogicalOperatorType::WINDOW) {
74: 			// don't push filters through projection or aggregate and group by
75: 			JoinOrderOptimizer optimizer;
76: 			op->children[0] = optimizer.Optimize(move(op->children[0]));
77: 			return false;
78: 		}
79: 		op = op->children[0].get();
80: 	}
81: 	bool non_reorderable_operation = false;
82: 	if (op->type == LogicalOperatorType::UNION || op->type == LogicalOperatorType::EXCEPT ||
83: 	    op->type == LogicalOperatorType::INTERSECT || op->type == LogicalOperatorType::DELIM_JOIN ||
84: 	    op->type == LogicalOperatorType::ANY_JOIN) {
85: 		// set operation, optimize separately in children
86: 		non_reorderable_operation = true;
87: 	}
88: 
89: 	if (op->type == LogicalOperatorType::COMPARISON_JOIN) {
90: 		LogicalJoin *join = (LogicalJoin *)op;
91: 		if (join->join_type == JoinType::INNER) {
92: 			// extract join conditions from inner join
93: 			filter_operators.push_back(op);
94: 		} else {
95: 			// non-inner join, not reordarable yet
96: 			non_reorderable_operation = true;
97: 		}
98: 	}
99: 	if (non_reorderable_operation) {
100: 		// we encountered a non-reordable operation (setop or non-inner join)
101: 		// we do not reorder non-inner joins yet, however we do want to expand the potential join graph around them
102: 		// non-inner joins are also tricky because we can't freely make conditions through them
103: 		// e.g. suppose we have (left LEFT OUTER JOIN right WHERE right IS NOT NULL), the join can generate
104: 		// new NULL values in the right side, so pushing this condition through the join leads to incorrect results
105: 		// for this reason, we just start a new JoinOptimizer pass in each of the children of the join
106: 		for (idx_t i = 0; i < op->children.size(); i++) {
107: 			JoinOrderOptimizer optimizer;
108: 			op->children[i] = optimizer.Optimize(move(op->children[i]));
109: 		}
110: 		// after this we want to treat this node as one  "end node" (like e.g. a base relation)
111: 		// however the join refers to multiple base relations
112: 		// enumerate all base relations obtained from this join and add them to the relation mapping
113: 		// also, we have to resolve the join conditions for the joins here
114: 		// get the left and right bindings
115: 		unordered_set<idx_t> bindings;
116: 		LogicalJoin::GetTableReferences(*op, bindings);
117: 		// now create the relation that refers to all these bindings
118: 		auto relation = make_unique<SingleJoinRelation>(&input_op, parent);
119: 		for (idx_t it : bindings) {
120: 			relation_mapping[it] = relations.size();
121: 		}
122: 		relations.push_back(move(relation));
123: 		return true;
124: 	}
125: 	if (op->type == LogicalOperatorType::COMPARISON_JOIN || op->type == LogicalOperatorType::CROSS_PRODUCT) {
126: 		// inner join or cross product
127: 		bool can_reorder_left = ExtractJoinRelations(*op->children[0], filter_operators, op);
128: 		bool can_reorder_right = ExtractJoinRelations(*op->children[1], filter_operators, op);
129: 		return can_reorder_left && can_reorder_right;
130: 	} else if (op->type == LogicalOperatorType::GET) {
131: 		// base table scan, add to set of relations
132: 		auto get = (LogicalGet *)op;
133: 		auto relation = make_unique<SingleJoinRelation>(&input_op, parent);
134: 		relation_mapping[get->table_index] = relations.size();
135: 		relations.push_back(move(relation));
136: 		return true;
137: 	} else if (op->type == LogicalOperatorType::EXPRESSION_GET) {
138: 		// base table scan, add to set of relations
139: 		auto get = (LogicalExpressionGet *)op;
140: 		auto relation = make_unique<SingleJoinRelation>(&input_op, parent);
141: 		relation_mapping[get->table_index] = relations.size();
142: 		relations.push_back(move(relation));
143: 		return true;
144: 	} else if (op->type == LogicalOperatorType::TABLE_FUNCTION) {
145: 		// table function call, add to set of relations
146: 		auto table_function = (LogicalTableFunction *)op;
147: 		auto relation = make_unique<SingleJoinRelation>(&input_op, parent);
148: 		relation_mapping[table_function->table_index] = relations.size();
149: 		relations.push_back(move(relation));
150: 		return true;
151: 	} else if (op->type == LogicalOperatorType::PROJECTION) {
152: 		auto proj = (LogicalProjection *)op;
153: 		// we run the join order optimizer witin the subquery as well
154: 		JoinOrderOptimizer optimizer;
155: 		op->children[0] = optimizer.Optimize(move(op->children[0]));
156: 		// projection, add to the set of relations
157: 		auto relation = make_unique<SingleJoinRelation>(&input_op, parent);
158: 		relation_mapping[proj->table_index] = relations.size();
159: 		relations.push_back(move(relation));
160: 		return true;
161: 	}
162: 	return false;
163: }
164: 
165: //! Update the exclusion set with all entries in the subgraph
166: static void UpdateExclusionSet(JoinRelationSet *node, unordered_set<idx_t> &exclusion_set) {
167: 	for (idx_t i = 0; i < node->count; i++) {
168: 		exclusion_set.insert(node->relations[i]);
169: 	}
170: }
171: 
172: //! Create a new JoinTree node by joining together two previous JoinTree nodes
173: static unique_ptr<JoinNode> CreateJoinTree(JoinRelationSet *set, NeighborInfo *info, JoinNode *left, JoinNode *right) {
174: 	// for the hash join we want the right side (build side) to have the smallest cardinality
175: 	// also just a heuristic but for now...
176: 	// FIXME: we should probably actually benchmark that as well
177: 	// FIXME: should consider different join algorithms, should we pick a join algorithm here as well? (probably)
178: 	if (left->cardinality < right->cardinality) {
179: 		return CreateJoinTree(set, info, right, left);
180: 	}
181: 	// the expected cardinality is the max of the child cardinalities
182: 	// FIXME: we should obviously use better cardinality estimation here
183: 	// but for now we just assume foreign key joins only
184: 	idx_t expected_cardinality;
185: 	if (info->filters.size() == 0) {
186: 		// cross product
187: 		expected_cardinality = left->cardinality * right->cardinality;
188: 	} else {
189: 		// normal join, expect foreign key join
190: 		expected_cardinality = std::max(left->cardinality, right->cardinality);
191: 	}
192: 	// cost is expected_cardinality plus the cost of the previous plans
193: 	idx_t cost = expected_cardinality;
194: 	return make_unique<JoinNode>(set, info, left, right, expected_cardinality, cost);
195: }
196: 
197: JoinNode *JoinOrderOptimizer::EmitPair(JoinRelationSet *left, JoinRelationSet *right, NeighborInfo *info) {
198: 	// get the left and right join plans
199: 	auto &left_plan = plans[left];
200: 	auto &right_plan = plans[right];
201: 	auto new_set = set_manager.Union(left, right);
202: 	// create the join tree based on combining the two plans
203: 	auto new_plan = CreateJoinTree(new_set, info, left_plan.get(), right_plan.get());
204: 	// check if this plan is the optimal plan we found for this set of relations
205: 	auto entry = plans.find(new_set);
206: 	if (entry == plans.end() || new_plan->cost < entry->second->cost) {
207: 		// the plan is the optimal plan, move it into the dynamic programming tree
208: 		auto result = new_plan.get();
209: 		plans[new_set] = move(new_plan);
210: 		return result;
211: 	}
212: 	return entry->second.get();
213: }
214: 
215: bool JoinOrderOptimizer::TryEmitPair(JoinRelationSet *left, JoinRelationSet *right, NeighborInfo *info) {
216: 	pairs++;
217: 	if (pairs >= 2000) {
218: 		// when the amount of pairs gets too large we exit the dynamic programming and resort to a greedy algorithm
219: 		// FIXME: simple heuristic currently
220: 		// at 10K pairs stop searching exactly and switch to heuristic
221: 		return false;
222: 	}
223: 	EmitPair(left, right, info);
224: 	return true;
225: }
226: 
227: bool JoinOrderOptimizer::EmitCSG(JoinRelationSet *node) {
228: 	// create the exclusion set as everything inside the subgraph AND anything with members BELOW it
229: 	unordered_set<idx_t> exclusion_set;
230: 	for (idx_t i = 0; i < node->relations[0]; i++) {
231: 		exclusion_set.insert(i);
232: 	}
233: 	UpdateExclusionSet(node, exclusion_set);
234: 	// find the neighbors given this exclusion set
235: 	auto neighbors = query_graph.GetNeighbors(node, exclusion_set);
236: 	if (neighbors.size() == 0) {
237: 		return true;
238: 	}
239: 	// we iterate over the neighbors ordered by their first node
240: 	sort(neighbors.begin(), neighbors.end());
241: 	for (auto neighbor : neighbors) {
242: 		// since the GetNeighbors only returns the smallest element in a list, the entry might not be connected to
243: 		// (only!) this neighbor,  hence we have to do a connectedness check before we can emit it
244: 		auto neighbor_relation = set_manager.GetJoinRelation(neighbor);
245: 		auto connection = query_graph.GetConnection(node, neighbor_relation);
246: 		if (connection) {
247: 			if (!TryEmitPair(node, neighbor_relation, connection)) {
248: 				return false;
249: 			}
250: 		}
251: 		if (!EnumerateCmpRecursive(node, neighbor_relation, exclusion_set)) {
252: 			return false;
253: 		}
254: 	}
255: 	return true;
256: }
257: 
258: bool JoinOrderOptimizer::EnumerateCmpRecursive(JoinRelationSet *left, JoinRelationSet *right,
259:                                                unordered_set<idx_t> exclusion_set) {
260: 	// get the neighbors of the second relation under the exclusion set
261: 	auto neighbors = query_graph.GetNeighbors(right, exclusion_set);
262: 	if (neighbors.size() == 0) {
263: 		return true;
264: 	}
265: 	vector<JoinRelationSet *> union_sets;
266: 	union_sets.resize(neighbors.size());
267: 	for (idx_t i = 0; i < neighbors.size(); i++) {
268: 		auto neighbor = set_manager.GetJoinRelation(neighbors[i]);
269: 		// emit the combinations of this node and its neighbors
270: 		auto combined_set = set_manager.Union(right, neighbor);
271: 		if (plans.find(combined_set) != plans.end()) {
272: 			auto connection = query_graph.GetConnection(left, combined_set);
273: 			if (connection) {
274: 				if (!TryEmitPair(left, combined_set, connection)) {
275: 					return false;
276: 				}
277: 			}
278: 		}
279: 		union_sets[i] = combined_set;
280: 	}
281: 	// recursively enumerate the sets
282: 	for (idx_t i = 0; i < neighbors.size(); i++) {
283: 		// updated the set of excluded entries with this neighbor
284: 		unordered_set<idx_t> new_exclusion_set = exclusion_set;
285: 		new_exclusion_set.insert(neighbors[i]);
286: 		if (!EnumerateCmpRecursive(left, union_sets[i], new_exclusion_set)) {
287: 			return false;
288: 		}
289: 	}
290: 	return true;
291: }
292: 
293: bool JoinOrderOptimizer::EnumerateCSGRecursive(JoinRelationSet *node, unordered_set<idx_t> &exclusion_set) {
294: 	// find neighbors of S under the exlusion set
295: 	auto neighbors = query_graph.GetNeighbors(node, exclusion_set);
296: 	if (neighbors.size() == 0) {
297: 		return true;
298: 	}
299: 	// now first emit the connected subgraphs of the neighbors
300: 	vector<JoinRelationSet *> union_sets;
301: 	union_sets.resize(neighbors.size());
302: 	for (idx_t i = 0; i < neighbors.size(); i++) {
303: 		auto neighbor = set_manager.GetJoinRelation(neighbors[i]);
304: 		// emit the combinations of this node and its neighbors
305: 		auto new_set = set_manager.Union(node, neighbor);
306: 		if (plans.find(new_set) != plans.end()) {
307: 			if (!EmitCSG(new_set)) {
308: 				return false;
309: 			}
310: 		}
311: 		union_sets[i] = new_set;
312: 	}
313: 	// recursively enumerate the sets
314: 	for (idx_t i = 0; i < neighbors.size(); i++) {
315: 		// updated the set of excluded entries with this neighbor
316: 		unordered_set<idx_t> new_exclusion_set = exclusion_set;
317: 		new_exclusion_set.insert(neighbors[i]);
318: 		if (!EnumerateCSGRecursive(union_sets[i], new_exclusion_set)) {
319: 			return false;
320: 		}
321: 	}
322: 	return true;
323: }
324: 
325: bool JoinOrderOptimizer::SolveJoinOrderExactly() {
326: 	// now we perform the actual dynamic programming to compute the final result
327: 	// we enumerate over all the possible pairs in the neighborhood
328: 	for (idx_t i = relations.size(); i > 0; i--) {
329: 		// for every node in the set, we consider it as the start node once
330: 		auto start_node = set_manager.GetJoinRelation(i - 1);
331: 		// emit the start node
332: 		if (!EmitCSG(start_node)) {
333: 			return false;
334: 		}
335: 		// initialize the set of exclusion_set as all the nodes with a number below this
336: 		unordered_set<idx_t> exclusion_set;
337: 		for (idx_t j = 0; j < i - 1; j++) {
338: 			exclusion_set.insert(j);
339: 		}
340: 		// then we recursively search for neighbors that do not belong to the banned entries
341: 		if (!EnumerateCSGRecursive(start_node, exclusion_set)) {
342: 			return false;
343: 		}
344: 	}
345: 	return true;
346: }
347: 
348: void JoinOrderOptimizer::SolveJoinOrderApproximately() {
349: 	// at this point, we exited the dynamic programming but did not compute the final join order because it took too
350: 	// long instead, we use a greedy heuristic to obtain a join ordering now we use Greedy Operator Ordering to
351: 	// construct the result tree first we start out with all the base relations (the to-be-joined relations)
352: 	vector<JoinRelationSet *> T;
353: 	for (idx_t i = 0; i < relations.size(); i++) {
354: 		T.push_back(set_manager.GetJoinRelation(i));
355: 	}
356: 	while (T.size() > 1) {
357: 		// now in every step of the algorithm, we greedily pick the join between the to-be-joined relations that has the
358: 		// smallest cost. This is O(r^2) per step, and every step will reduce the total amount of relations to-be-joined
359: 		// by 1, so the total cost is O(r^3) in the amount of relations
360: 		idx_t best_left = 0, best_right = 0;
361: 		JoinNode *best_connection = nullptr;
362: 		for (idx_t i = 0; i < T.size(); i++) {
363: 			auto left = T[i];
364: 			for (idx_t j = i + 1; j < T.size(); j++) {
365: 				auto right = T[j];
366: 				// check if we can connect these two relations
367: 				auto connection = query_graph.GetConnection(left, right);
368: 				if (connection) {
369: 					// we can! check the cost of this connection
370: 					auto node = EmitPair(left, right, connection);
371: 					if (!best_connection || node->cost < best_connection->cost) {
372: 						// best pair found so far
373: 						best_connection = node;
374: 						best_left = i;
375: 						best_right = j;
376: 					}
377: 				}
378: 			}
379: 		}
380: 		if (!best_connection) {
381: 			// could not find a connection, but we were not done with finding a completed plan
382: 			// we have to add a cross product; we add it between the two smallest relations
383: 			JoinNode *smallest_plans[2] = {nullptr};
384: 			idx_t smallest_index[2];
385: 			for (idx_t i = 0; i < T.size(); i++) {
386: 				// get the plan for this relation
387: 				auto current_plan = plans[T[i]].get();
388: 				// check if the cardinality is smaller than the smallest two found so far
389: 				for (idx_t j = 0; j < 2; j++) {
390: 					if (!smallest_plans[j] || smallest_plans[j]->cardinality > current_plan->cardinality) {
391: 						smallest_plans[j] = current_plan;
392: 						smallest_index[j] = i;
393: 						break;
394: 					}
395: 				}
396: 			}
397: 			assert(smallest_plans[0] && smallest_plans[1]);
398: 			assert(smallest_index[0] != smallest_index[1]);
399: 			auto left = smallest_plans[0]->set, right = smallest_plans[1]->set;
400: 			// create a cross product edge (i.e. edge with empty filter) between these two sets in the query graph
401: 			query_graph.CreateEdge(left, right, nullptr);
402: 			// now emit the pair and continue with the algorithm
403: 			auto connection = query_graph.GetConnection(left, right);
404: 			assert(connection);
405: 
406: 			best_connection = EmitPair(left, right, connection);
407: 			best_left = smallest_index[0];
408: 			best_right = smallest_index[1];
409: 			// the code below assumes best_right > best_left
410: 			if (best_left > best_right) {
411: 				swap(best_left, best_right);
412: 			}
413: 		}
414: 		// now update the to-be-checked pairs
415: 		// remove left and right, and add the combination
416: 
417: 		// important to erase the biggest element first
418: 		// if we erase the smallest element first the index of the biggest element changes
419: 		assert(best_right > best_left);
420: 		T.erase(T.begin() + best_right);
421: 		T.erase(T.begin() + best_left);
422: 		T.push_back(best_connection->set);
423: 	}
424: }
425: 
426: void JoinOrderOptimizer::SolveJoinOrder() {
427: 	// first try to solve the join order exactly
428: 	if (!SolveJoinOrderExactly()) {
429: 		// otherwise, if that times out we resort to a greedy algorithm
430: 		SolveJoinOrderApproximately();
431: 	}
432: }
433: 
434: void JoinOrderOptimizer::GenerateCrossProducts() {
435: 	// generate a set of cross products to combine the currently available plans into a full join plan
436: 	// we create edges between every relation with a high cost
437: 	for (idx_t i = 0; i < relations.size(); i++) {
438: 		auto left = set_manager.GetJoinRelation(i);
439: 		for (idx_t j = 0; j < relations.size(); j++) {
440: 			if (i != j) {
441: 				auto right = set_manager.GetJoinRelation(j);
442: 				query_graph.CreateEdge(left, right, nullptr);
443: 				query_graph.CreateEdge(right, left, nullptr);
444: 			}
445: 		}
446: 	}
447: }
448: 
449: static unique_ptr<LogicalOperator> ExtractJoinRelation(SingleJoinRelation &rel) {
450: 	auto &children = rel.parent->children;
451: 	for (idx_t i = 0; i < children.size(); i++) {
452: 		if (children[i].get() == rel.op) {
453: 			// found it! take ownership of it from the parent
454: 			auto result = move(children[i]);
455: 			children.erase(children.begin() + i);
456: 			return result;
457: 		}
458: 	}
459: 	throw Exception("Could not find relation in parent node (?)");
460: }
461: 
462: pair<JoinRelationSet *, unique_ptr<LogicalOperator>>
463: JoinOrderOptimizer::GenerateJoins(vector<unique_ptr<LogicalOperator>> &extracted_relations, JoinNode *node) {
464: 	JoinRelationSet *left_node = nullptr, *right_node = nullptr;
465: 	JoinRelationSet *result_relation;
466: 	unique_ptr<LogicalOperator> result_operator;
467: 	if (node->left && node->right) {
468: 		// generate the left and right children
469: 		auto left = GenerateJoins(extracted_relations, node->left);
470: 		auto right = GenerateJoins(extracted_relations, node->right);
471: 
472: 		if (node->info->filters.size() == 0) {
473: 			// no filters, create a cross product
474: 			auto join = make_unique<LogicalCrossProduct>();
475: 			join->children.push_back(move(left.second));
476: 			join->children.push_back(move(right.second));
477: 			result_operator = move(join);
478: 		} else {
479: 			// we have filters, create a join node
480: 			auto join = make_unique<LogicalComparisonJoin>(JoinType::INNER);
481: 			join->children.push_back(move(left.second));
482: 			join->children.push_back(move(right.second));
483: 			// set the join conditions from the join node
484: 			for (auto &f : node->info->filters) {
485: 				// extract the filter from the operator it originally belonged to
486: 				assert(filters[f->filter_index]);
487: 				auto condition = move(filters[f->filter_index]);
488: 				// now create the actual join condition
489: 				assert((JoinRelationSet::IsSubset(left.first, f->left_set) &&
490: 				        JoinRelationSet::IsSubset(right.first, f->right_set)) ||
491: 				       (JoinRelationSet::IsSubset(left.first, f->right_set) &&
492: 				        JoinRelationSet::IsSubset(right.first, f->left_set)));
493: 				JoinCondition cond;
494: 				assert(condition->GetExpressionClass() == ExpressionClass::BOUND_COMPARISON);
495: 				auto &comparison = (BoundComparisonExpression &)*condition;
496: 				// we need to figure out which side is which by looking at the relations available to us
497: 				bool invert = JoinRelationSet::IsSubset(left.first, f->left_set) ? false : true;
498: 				cond.left = !invert ? move(comparison.left) : move(comparison.right);
499: 				cond.right = !invert ? move(comparison.right) : move(comparison.left);
500: 				cond.comparison = condition->type;
501: 				if (invert) {
502: 					// reverse comparison expression if we reverse the order of the children
503: 					cond.comparison = FlipComparisionExpression(cond.comparison);
504: 				}
505: 				join->conditions.push_back(move(cond));
506: 			}
507: 			assert(join->conditions.size() > 0);
508: 			result_operator = move(join);
509: 		}
510: 		left_node = left.first;
511: 		right_node = right.first;
512: 		result_relation = set_manager.Union(left_node, right_node);
513: 	} else {
514: 		// base node, get the entry from the list of extracted relations
515: 		assert(node->set->count == 1);
516: 		assert(extracted_relations[node->set->relations[0]]);
517: 		result_relation = node->set;
518: 		result_operator = move(extracted_relations[node->set->relations[0]]);
519: 	}
520: 	// check if we should do a pushdown on this node
521: 	// basically, any remaining filter that is a subset of the current relation will no longer be used in joins
522: 	// hence we should push it here
523: 	for (idx_t i = 0; i < filter_infos.size(); i++) {
524: 		// check if the filter has already been extracted
525: 		auto info = filter_infos[i].get();
526: 		if (filters[info->filter_index]) {
527: 			// now check if the filter is a subset of the current relation
528: 			// note that infos with an empty relation set are a special case and we do not push them down
529: 			if (info->set->count > 0 && JoinRelationSet::IsSubset(result_relation, info->set)) {
530: 				auto filter = move(filters[info->filter_index]);
531: 				// if it is, we can push the filter
532: 				// we can push it either into a join or as a filter
533: 				// check if we are in a join or in a base table
534: 				if (!left_node || !info->left_set) {
535: 					// base table or non-comparison expression, push it as a filter
536: 					result_operator = PushFilter(move(result_operator), move(filter));
537: 					continue;
538: 				}
539: 				// the node below us is a join or cross product and the expression is a comparison
540: 				// check if the nodes can be split up into left/right
541: 				bool found_subset = false;
542: 				bool invert = false;
543: 				if (JoinRelationSet::IsSubset(left_node, info->left_set) &&
544: 				    JoinRelationSet::IsSubset(right_node, info->right_set)) {
545: 					found_subset = true;
546: 				} else if (JoinRelationSet::IsSubset(right_node, info->left_set) &&
547: 				           JoinRelationSet::IsSubset(left_node, info->right_set)) {
548: 					invert = true;
549: 					found_subset = true;
550: 				}
551: 				if (!found_subset) {
552: 					// could not be split up into left/right
553: 					result_operator = PushFilter(move(result_operator), move(filter));
554: 					continue;
555: 				}
556: 				// create the join condition
557: 				JoinCondition cond;
558: 				assert(filter->GetExpressionClass() == ExpressionClass::BOUND_COMPARISON);
559: 				auto &comparison = (BoundComparisonExpression &)*filter;
560: 				// we need to figure out which side is which by looking at the relations available to us
561: 				cond.left = !invert ? move(comparison.left) : move(comparison.right);
562: 				cond.right = !invert ? move(comparison.right) : move(comparison.left);
563: 				cond.comparison = comparison.type;
564: 				if (invert) {
565: 					// reverse comparison expression if we reverse the order of the children
566: 					cond.comparison = FlipComparisionExpression(comparison.type);
567: 				}
568: 				// now find the join to push it into
569: 				auto node = result_operator.get();
570: 				if (node->type == LogicalOperatorType::FILTER) {
571: 					node = node->children[0].get();
572: 				}
573: 				if (node->type == LogicalOperatorType::CROSS_PRODUCT) {
574: 					// turn into comparison join
575: 					auto comp_join = make_unique<LogicalComparisonJoin>(JoinType::INNER);
576: 					comp_join->children.push_back(move(node->children[0]));
577: 					comp_join->children.push_back(move(node->children[1]));
578: 					comp_join->conditions.push_back(move(cond));
579: 					if (node == result_operator.get()) {
580: 						result_operator = move(comp_join);
581: 					} else {
582: 						assert(result_operator->type == LogicalOperatorType::FILTER);
583: 						result_operator->children[0] = move(comp_join);
584: 					}
585: 				} else {
586: 					assert(node->type == LogicalOperatorType::COMPARISON_JOIN);
587: 					auto &comp_join = (LogicalComparisonJoin &)*node;
588: 					comp_join.conditions.push_back(move(cond));
589: 				}
590: 			}
591: 		}
592: 	}
593: 	return make_pair(result_relation, move(result_operator));
594: }
595: 
596: unique_ptr<LogicalOperator> JoinOrderOptimizer::RewritePlan(unique_ptr<LogicalOperator> plan, JoinNode *node) {
597: 	// now we have to rewrite the plan
598: 	bool root_is_join = plan->children.size() > 1;
599: 
600: 	// first we will extract all relations from the main plan
601: 	vector<unique_ptr<LogicalOperator>> extracted_relations;
602: 	for (idx_t i = 0; i < relations.size(); i++) {
603: 		extracted_relations.push_back(ExtractJoinRelation(*relations[i]));
604: 	}
605: 	// now we generate the actual joins
606: 	auto join_tree = GenerateJoins(extracted_relations, node);
607: 	// perform the final pushdown of remaining filters
608: 	for (idx_t i = 0; i < filters.size(); i++) {
609: 		// check if the filter has already been extracted
610: 		if (filters[i]) {
611: 			// if not we need to push it
612: 			join_tree.second = PushFilter(move(join_tree.second), move(filters[i]));
613: 		}
614: 	}
615: 
616: 	// find the first join in the relation to know where to place this node
617: 	if (root_is_join) {
618: 		// first node is the join, return it immediately
619: 		return move(join_tree.second);
620: 	}
621: 	assert(plan->children.size() == 1);
622: 	// have to move up through the relations
623: 	auto op = plan.get();
624: 	auto parent = plan.get();
625: 	while (op->type != LogicalOperatorType::CROSS_PRODUCT && op->type != LogicalOperatorType::COMPARISON_JOIN) {
626: 		assert(op->children.size() == 1);
627: 		parent = op;
628: 		op = op->children[0].get();
629: 	}
630: 	// have to replace at this node
631: 	parent->children[0] = move(join_tree.second);
632: 	return plan;
633: }
634: 
635: // the join ordering is pretty much a straight implementation of the paper "Dynamic Programming Strikes Back" by Guido
636: // Moerkotte and Thomas Neumannn, see that paper for additional info/documentation bonus slides:
637: // https://db.in.tum.de/teaching/ws1415/queryopt/chapter3.pdf?lang=de
638: // FIXME: incorporate cardinality estimation into the plans, possibly by pushing samples?
639: unique_ptr<LogicalOperator> JoinOrderOptimizer::Optimize(unique_ptr<LogicalOperator> plan) {
640: 	assert(filters.size() == 0 && relations.size() == 0); // assert that the JoinOrderOptimizer has not been used before
641: 	LogicalOperator *op = plan.get();
642: 	// now we optimize the current plan
643: 	// we skip past until we find the first projection, we do this because the HAVING clause inserts a Filter AFTER the
644: 	// group by and this filter cannot be reordered
645: 	// extract a list of all relations that have to be joined together
646: 	// and a list of all conditions that is applied to them
647: 	vector<LogicalOperator *> filter_operators;
648: 	if (!ExtractJoinRelations(*op, filter_operators)) {
649: 		// do not support reordering this type of plan
650: 		return plan;
651: 	}
652: 	if (relations.size() <= 1) {
653: 		// at most one relation, nothing to reorder
654: 		return plan;
655: 	}
656: 	// now that we know we are going to perform join ordering we actually extract the filters, eliminating duplicate
657: 	// filters in the process
658: 	expression_set_t filter_set;
659: 	for (auto &op : filter_operators) {
660: 		if (op->type == LogicalOperatorType::COMPARISON_JOIN) {
661: 			auto &join = (LogicalComparisonJoin &)*op;
662: 			assert(join.join_type == JoinType::INNER);
663: 			assert(join.expressions.size() == 0);
664: 			for (auto &cond : join.conditions) {
665: 				auto comparison =
666: 				    make_unique<BoundComparisonExpression>(cond.comparison, move(cond.left), move(cond.right));
667: 				if (filter_set.find(comparison.get()) == filter_set.end()) {
668: 					filter_set.insert(comparison.get());
669: 					filters.push_back(move(comparison));
670: 				}
671: 			}
672: 			join.conditions.clear();
673: 		} else {
674: 			for (idx_t i = 0; i < op->expressions.size(); i++) {
675: 				if (filter_set.find(op->expressions[i].get()) == filter_set.end()) {
676: 					filter_set.insert(op->expressions[i].get());
677: 					filters.push_back(move(op->expressions[i]));
678: 				}
679: 			}
680: 			op->expressions.clear();
681: 		}
682: 	}
683: 	// create potential edges from the comparisons
684: 	for (idx_t i = 0; i < filters.size(); i++) {
685: 		auto &filter = filters[i];
686: 		auto info = make_unique<FilterInfo>();
687: 		auto filter_info = info.get();
688: 		filter_infos.push_back(move(info));
689: 		// first extract the relation set for the entire filter
690: 		unordered_set<idx_t> bindings;
691: 		ExtractBindings(*filter, bindings);
692: 		filter_info->set = set_manager.GetJoinRelation(bindings);
693: 		filter_info->filter_index = i;
694: 		// now check if it can be used as a join predicate
695: 		if (filter->GetExpressionClass() == ExpressionClass::BOUND_COMPARISON) {
696: 			auto comparison = (BoundComparisonExpression *)filter.get();
697: 			// extract the bindings that are required for the left and right side of the comparison
698: 			unordered_set<idx_t> left_bindings, right_bindings;
699: 			ExtractBindings(*comparison->left, left_bindings);
700: 			ExtractBindings(*comparison->right, right_bindings);
701: 			if (left_bindings.size() > 0 && right_bindings.size() > 0) {
702: 				// both the left and the right side have bindings
703: 				// first create the relation sets, if they do not exist
704: 				filter_info->left_set = set_manager.GetJoinRelation(left_bindings);
705: 				filter_info->right_set = set_manager.GetJoinRelation(right_bindings);
706: 				// we can only create a meaningful edge if the sets are not exactly the same
707: 				if (filter_info->left_set != filter_info->right_set) {
708: 					// check if the sets are disjoint
709: 					if (Disjoint(left_bindings, right_bindings)) {
710: 						// they are disjoint, we only need to create one set of edges in the join graph
711: 						query_graph.CreateEdge(filter_info->left_set, filter_info->right_set, filter_info);
712: 						query_graph.CreateEdge(filter_info->right_set, filter_info->left_set, filter_info);
713: 					} else {
714: 						// the sets are not disjoint, we create two sets of edges
715: 						auto left_difference = set_manager.Difference(filter_info->left_set, filter_info->right_set);
716: 						auto right_difference = set_manager.Difference(filter_info->right_set, filter_info->left_set);
717: 						// -> LEFT <-> RIGHT \ LEFT
718: 						query_graph.CreateEdge(filter_info->left_set, right_difference, filter_info);
719: 						query_graph.CreateEdge(right_difference, filter_info->left_set, filter_info);
720: 						// -> RIGHT <-> LEFT \ RIGHT
721: 						query_graph.CreateEdge(left_difference, filter_info->right_set, filter_info);
722: 						query_graph.CreateEdge(filter_info->right_set, left_difference, filter_info);
723: 					}
724: 					continue;
725: 				}
726: 			}
727: 		}
728: 	}
729: 	// now use dynamic programming to figure out the optimal join order
730: 	// First we initialize each of the single-node plans with themselves and with their cardinalities these are the leaf
731: 	// nodes of the join tree NOTE: we can just use pointers to JoinRelationSet* here because the GetJoinRelation
732: 	// function ensures that a unique combination of relations will have a unique JoinRelationSet object.
733: 	for (idx_t i = 0; i < relations.size(); i++) {
734: 		auto &rel = *relations[i];
735: 		auto node = set_manager.GetJoinRelation(i);
736: 		plans[node] = make_unique<JoinNode>(node, rel.op->EstimateCardinality());
737: 	}
738: 	// now we perform the actual dynamic programming to compute the final result
739: 	SolveJoinOrder();
740: 	// now the optimal join path should have been found
741: 	// get it from the node
742: 	unordered_set<idx_t> bindings;
743: 	for (idx_t i = 0; i < relations.size(); i++) {
744: 		bindings.insert(i);
745: 	}
746: 	auto total_relation = set_manager.GetJoinRelation(bindings);
747: 	auto final_plan = plans.find(total_relation);
748: 	if (final_plan == plans.end()) {
749: 		// could not find the final plan
750: 		// this should only happen in case the sets are actually disjunct
751: 		// in this case we need to generate cross product to connect the disjoint sets
752: 		GenerateCrossProducts();
753: 		//! solve the join order again
754: 		SolveJoinOrder();
755: 		// now we can obtain the final plan!
756: 		final_plan = plans.find(total_relation);
757: 		assert(final_plan != plans.end());
758: 	}
759: 	// now perform the actual reordering
760: 	return RewritePlan(move(plan), final_plan->second.get());
761: }
[end of src/optimizer/join_order_optimizer.cpp]
[start of src/parser/transform/expression/transform_operator.cpp]
1: #include "duckdb/parser/expression/case_expression.hpp"
2: #include "duckdb/parser/expression/comparison_expression.hpp"
3: #include "duckdb/parser/expression/conjunction_expression.hpp"
4: #include "duckdb/parser/expression/constant_expression.hpp"
5: #include "duckdb/parser/expression/function_expression.hpp"
6: #include "duckdb/parser/expression/operator_expression.hpp"
7: #include "duckdb/parser/transformer.hpp"
8: 
9: using namespace duckdb;
10: using namespace std;
11: 
12: ExpressionType Transformer::OperatorToExpressionType(string &op) {
13: 	if (op == "=" || op == "==") {
14: 		return ExpressionType::COMPARE_EQUAL;
15: 	} else if (op == "!=" || op == "<>") {
16: 		return ExpressionType::COMPARE_NOTEQUAL;
17: 	} else if (op == "<") {
18: 		return ExpressionType::COMPARE_LESSTHAN;
19: 	} else if (op == ">") {
20: 		return ExpressionType::COMPARE_GREATERTHAN;
21: 	} else if (op == "<=") {
22: 		return ExpressionType::COMPARE_LESSTHANOREQUALTO;
23: 	} else if (op == ">=") {
24: 		return ExpressionType::COMPARE_GREATERTHANOREQUALTO;
25: 	}
26: 	return ExpressionType::INVALID;
27: }
28: 
29: unique_ptr<ParsedExpression> Transformer::TransformUnaryOperator(string op, unique_ptr<ParsedExpression> child) {
30: 	const auto schema = DEFAULT_SCHEMA;
31: 
32: 	vector<unique_ptr<ParsedExpression>> children;
33: 	children.push_back(move(child));
34: 
35: 	// built-in operator function
36: 	auto result = make_unique<FunctionExpression>(schema, op, children);
37: 	result->is_operator = true;
38: 	return move(result);
39: }
40: 
41: unique_ptr<ParsedExpression> Transformer::TransformBinaryOperator(string op, unique_ptr<ParsedExpression> left,
42:                                                                   unique_ptr<ParsedExpression> right) {
43: 	const auto schema = DEFAULT_SCHEMA;
44: 
45: 	vector<unique_ptr<ParsedExpression>> children;
46: 	children.push_back(move(left));
47: 	children.push_back(move(right));
48: 
49: 	if (op == "~" || op == "!~") {
50: 		// rewrite SIMILAR TO into regexp_matches('asdf', '.*sd.*')
51: 		bool invert_similar = op == "!~";
52: 
53: 		auto result = make_unique<FunctionExpression>(schema, "regexp_matches", children);
54: 		if (invert_similar) {
55: 			return make_unique<OperatorExpression>(ExpressionType::OPERATOR_NOT, move(result));
56: 		} else {
57: 			return move(result);
58: 		}
59: 	} else {
60: 		auto target_type = OperatorToExpressionType(op);
61: 		if (target_type != ExpressionType::INVALID) {
62: 			// built-in comparison operator
63: 			return make_unique<ComparisonExpression>(target_type, move(children[0]), move(children[1]));
64: 		} else {
65: 			// built-in operator function
66: 			auto result = make_unique<FunctionExpression>(schema, op, children);
67: 			result->is_operator = true;
68: 			return move(result);
69: 		}
70: 	}
71: }
72: 
73: unique_ptr<ParsedExpression> Transformer::TransformAExpr(PGAExpr *root) {
74: 	if (!root) {
75: 		return nullptr;
76: 	}
77: 	auto name = string((reinterpret_cast<PGValue *>(root->name->head->data.ptr_value))->val.str);
78: 
79: 	switch (root->kind) {
80: 	case PG_AEXPR_DISTINCT:
81: 		break;
82: 	case PG_AEXPR_IN: {
83: 		auto left_expr = TransformExpression(root->lexpr);
84: 		ExpressionType operator_type;
85: 		// this looks very odd, but seems to be the way to find out its NOT IN
86: 		if (name == "<>") {
87: 			// NOT IN
88: 			operator_type = ExpressionType::COMPARE_NOT_IN;
89: 		} else {
90: 			// IN
91: 			operator_type = ExpressionType::COMPARE_IN;
92: 		}
93: 		auto result = make_unique<OperatorExpression>(operator_type, move(left_expr));
94: 		TransformExpressionList((PGList *)root->rexpr, result->children);
95: 		return move(result);
96: 	} break;
97: 	// rewrite NULLIF(a, b) into CASE WHEN a=b THEN NULL ELSE a END
98: 	case PG_AEXPR_NULLIF: {
99: 		auto case_expr = make_unique<CaseExpression>();
100: 		auto value = TransformExpression(root->lexpr);
101: 		// the check (A = B)
102: 		case_expr->check = make_unique<ComparisonExpression>(ExpressionType::COMPARE_EQUAL, value->Copy(),
103: 		                                                     TransformExpression(root->rexpr));
104: 		// if A = B, then constant NULL
105: 		case_expr->result_if_true = make_unique<ConstantExpression>(SQLType::SQLNULL, Value());
106: 		// else A
107: 		case_expr->result_if_false = move(value);
108: 		return move(case_expr);
109: 	} break;
110: 	// rewrite (NOT) X BETWEEN A AND B into (NOT) AND(GREATERTHANOREQUALTO(X,
111: 	// A), LESSTHANOREQUALTO(X, B))
112: 	case PG_AEXPR_BETWEEN:
113: 	case PG_AEXPR_NOT_BETWEEN: {
114: 		auto between_args = reinterpret_cast<PGList *>(root->rexpr);
115: 
116: 		if (between_args->length != 2 || !between_args->head->data.ptr_value || !between_args->tail->data.ptr_value) {
117: 			throw Exception("(NOT) BETWEEN needs two args");
118: 		}
119: 
120: 		auto between_left = TransformExpression(reinterpret_cast<PGNode *>(between_args->head->data.ptr_value));
121: 		auto between_right = TransformExpression(reinterpret_cast<PGNode *>(between_args->tail->data.ptr_value));
122: 
123: 		auto compare_left = make_unique<ComparisonExpression>(ExpressionType::COMPARE_GREATERTHANOREQUALTO,
124: 		                                                      TransformExpression(root->lexpr), move(between_left));
125: 		auto compare_right = make_unique<ComparisonExpression>(ExpressionType::COMPARE_LESSTHANOREQUALTO,
126: 		                                                       TransformExpression(root->lexpr), move(between_right));
127: 		auto compare_between = make_unique<ConjunctionExpression>(ExpressionType::CONJUNCTION_AND, move(compare_left),
128: 		                                                          move(compare_right));
129: 		if (root->kind == PG_AEXPR_BETWEEN) {
130: 			return move(compare_between);
131: 		} else {
132: 			return make_unique<OperatorExpression>(ExpressionType::OPERATOR_NOT, move(compare_between));
133: 		}
134: 	} break;
135: 	// rewrite SIMILAR TO into regexp_matches('asdf', '.*sd.*')
136: 	case PG_AEXPR_SIMILAR: {
137: 		auto left_expr = TransformExpression(root->lexpr);
138: 		auto right_expr = TransformExpression(root->rexpr);
139: 
140: 		vector<unique_ptr<ParsedExpression>> children;
141: 		children.push_back(move(left_expr));
142: 
143: 		auto &similar_func = reinterpret_cast<FunctionExpression &>(*right_expr);
144: 		assert(similar_func.function_name == "similar_escape");
145: 		assert(similar_func.children.size() == 2);
146: 		if (similar_func.children[1]->type != ExpressionType::VALUE_CONSTANT) {
147: 			throw NotImplementedException("Custom escape in SIMILAR TO");
148: 		}
149: 		auto &constant = (ConstantExpression &)*similar_func.children[1];
150: 		if (!constant.value.is_null) {
151: 			throw NotImplementedException("Custom escape in SIMILAR TO");
152: 		}
153: 		// take the child of the similar_func
154: 		children.push_back(move(similar_func.children[0]));
155: 
156: 		// this looks very odd, but seems to be the way to find out its NOT IN
157: 		bool invert_similar = false;
158: 		if (name == "!~") {
159: 			// NOT SIMILAR TO
160: 			invert_similar = true;
161: 		}
162: 		const auto schema = DEFAULT_SCHEMA;
163: 		const auto regex_function = "regexp_matches";
164: 		auto result = make_unique<FunctionExpression>(schema, regex_function, children);
165: 
166: 		if (invert_similar) {
167: 			return make_unique<OperatorExpression>(ExpressionType::OPERATOR_NOT, move(result));
168: 		} else {
169: 			return move(result);
170: 		}
171: 	} break;
172: 	default:
173: 		break;
174: 	}
175: 
176: 	auto left_expr = TransformExpression(root->lexpr);
177: 	auto right_expr = TransformExpression(root->rexpr);
178: 
179: 	if (!left_expr) {
180: 		// prefix operator
181: 		return TransformUnaryOperator(name, move(right_expr));
182: 	} else if (!right_expr) {
183: 		throw NotImplementedException("Postfix operators not implemented!");
184: 	} else {
185: 		return TransformBinaryOperator(name, move(left_expr), move(right_expr));
186: 	}
187: }
[end of src/parser/transform/expression/transform_operator.cpp]
[start of src/parser/transform/statement/transform_create_table.cpp]
1: #include "duckdb/parser/statement/create_statement.hpp"
2: #include "duckdb/parser/parsed_data/create_table_info.hpp"
3: #include "duckdb/parser/transformer.hpp"
4: #include "duckdb/parser/constraint.hpp"
5: 
6: using namespace duckdb;
7: using namespace std;
8: 
9: unique_ptr<CreateStatement> Transformer::TransformCreateTable(PGNode *node) {
10: 	auto stmt = reinterpret_cast<PGCreateStmt *>(node);
11: 	assert(stmt);
12: 	auto result = make_unique<CreateStatement>();
13: 	auto info = make_unique<CreateTableInfo>();
14: 
15: 	if (stmt->inhRelations) {
16: 		throw NotImplementedException("inherited relations not implemented");
17: 	}
18: 	assert(stmt->relation);
19: 
20: 	info->schema = INVALID_SCHEMA;
21: 	if (stmt->relation->schemaname) {
22: 		info->schema = stmt->relation->schemaname;
23: 	}
24: 	info->table = stmt->relation->relname;
25: 	info->on_conflict = stmt->if_not_exists ? OnCreateConflict::IGNORE : OnCreateConflict::ERROR;
26: 	info->temporary = stmt->relation->relpersistence == PGPostgresRelPersistence::PG_RELPERSISTENCE_TEMP;
27: 
28: 	if (info->temporary && stmt->oncommit != PGOnCommitAction::PG_ONCOMMIT_PRESERVE_ROWS &&
29: 	    stmt->oncommit != PGOnCommitAction::PG_ONCOMMIT_NOOP) {
30: 		throw NotImplementedException("Only ON COMMIT PRESERVE ROWS is supported");
31: 	}
32: 
33: 	assert(stmt->tableElts);
34: 
35: 	for (auto c = stmt->tableElts->head; c != NULL; c = lnext(c)) {
36: 		auto node = reinterpret_cast<PGNode *>(c->data.ptr_value);
37: 		switch (node->type) {
38: 		case T_PGColumnDef: {
39: 			auto cdef = (PGColumnDef *)c->data.ptr_value;
40: 			SQLType target_type = TransformTypeName(cdef->typeName);
41: 			auto centry = ColumnDefinition(cdef->colname, target_type);
42: 
43: 			if (cdef->constraints) {
44: 				for (auto constr = cdef->constraints->head; constr != nullptr; constr = constr->next) {
45: 					auto constraint = TransformConstraint(constr, centry, info->columns.size());
46: 					if (constraint) {
47: 						info->constraints.push_back(move(constraint));
48: 					}
49: 				}
50: 			}
51: 			info->columns.push_back(move(centry));
52: 			break;
53: 		}
54: 		case T_PGConstraint: {
55: 			info->constraints.push_back(TransformConstraint(c));
56: 			break;
57: 		}
58: 		default:
59: 			throw NotImplementedException("ColumnDef type not handled yet");
60: 		}
61: 	}
62: 	result->info = move(info);
63: 	return result;
64: }
[end of src/parser/transform/statement/transform_create_table.cpp]
[start of src/parser/transform/tableref/transform_join.cpp]
1: #include "duckdb/common/exception.hpp"
2: #include "duckdb/parser/tableref/basetableref.hpp"
3: #include "duckdb/parser/tableref/crossproductref.hpp"
4: #include "duckdb/parser/tableref/joinref.hpp"
5: #include "duckdb/parser/transformer.hpp"
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: unique_ptr<TableRef> Transformer::TransformJoin(PGJoinExpr *root) {
11: 	auto result = make_unique<JoinRef>();
12: 	switch (root->jointype) {
13: 	case PG_JOIN_INNER: {
14: 		result->type = JoinType::INNER;
15: 		break;
16: 	}
17: 	case PG_JOIN_LEFT: {
18: 		result->type = JoinType::LEFT;
19: 		break;
20: 	}
21: 	case PG_JOIN_FULL: {
22: 		result->type = JoinType::OUTER;
23: 		break;
24: 	}
25: 	case PG_JOIN_SEMI: {
26: 		result->type = JoinType::SEMI;
27: 		break;
28: 	}
29: 	default: {
30: 		throw NotImplementedException("Join type %d not supported yet...\n", root->jointype);
31: 	}
32: 	}
33: 
34: 	// Check the type of left arg and right arg before transform
35: 	result->left = TransformTableRefNode(root->larg);
36: 	result->right = TransformTableRefNode(root->rarg);
37: 
38: 	if (root->usingClause && root->usingClause->length > 0) {
39: 		// usingClause is a list of strings
40: 		for (auto node = root->usingClause->head; node != nullptr; node = node->next) {
41: 			auto target = reinterpret_cast<PGNode *>(node->data.ptr_value);
42: 			assert(target->type == T_PGString);
43: 			auto column_name = string(reinterpret_cast<PGValue *>(target)->val.str);
44: 			result->using_columns.push_back(column_name);
45: 		}
46: 		return move(result);
47: 	}
48: 
49: 	if (!root->quals && result->using_columns.size() == 0) { // CROSS PRODUCT
50: 		auto cross = make_unique<CrossProductRef>();
51: 		cross->left = move(result->left);
52: 		cross->right = move(result->right);
53: 		return move(cross);
54: 	}
55: 
56: 	result->condition = TransformExpression(root->quals);
57: 	return move(result);
58: }
[end of src/parser/transform/tableref/transform_join.cpp]
[start of src/planner/binder/tableref/plan_joinref.cpp]
1: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
2: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
3: #include "duckdb/planner/expression/bound_conjunction_expression.hpp"
4: #include "duckdb/planner/expression/bound_constant_expression.hpp"
5: #include "duckdb/planner/expression/bound_operator_expression.hpp"
6: #include "duckdb/planner/expression/bound_subquery_expression.hpp"
7: #include "duckdb/planner/expression_iterator.hpp"
8: #include "duckdb/planner/binder.hpp"
9: #include "duckdb/planner/operator/logical_any_join.hpp"
10: #include "duckdb/planner/operator/logical_comparison_join.hpp"
11: #include "duckdb/planner/operator/logical_cross_product.hpp"
12: #include "duckdb/planner/operator/logical_filter.hpp"
13: #include "duckdb/planner/tableref/bound_joinref.hpp"
14: 
15: using namespace duckdb;
16: using namespace std;
17: 
18: //! Create a JoinCondition from a comparison
19: static bool CreateJoinCondition(Expression &expr, unordered_set<idx_t> &left_bindings,
20:                                 unordered_set<idx_t> &right_bindings, vector<JoinCondition> &conditions) {
21: 	// comparison
22: 	auto &comparison = (BoundComparisonExpression &)expr;
23: 	auto left_side = JoinSide::GetJoinSide(*comparison.left, left_bindings, right_bindings);
24: 	auto right_side = JoinSide::GetJoinSide(*comparison.right, left_bindings, right_bindings);
25: 	if (left_side != JoinSide::BOTH && right_side != JoinSide::BOTH) {
26: 		// join condition can be divided in a left/right side
27: 		JoinCondition condition;
28: 		condition.comparison = expr.type;
29: 		auto left = move(comparison.left);
30: 		auto right = move(comparison.right);
31: 		if (left_side == JoinSide::RIGHT) {
32: 			// left = right, right = left, flip the comparison symbol and reverse sides
33: 			swap(left, right);
34: 			condition.comparison = FlipComparisionExpression(expr.type);
35: 		}
36: 		condition.left = move(left);
37: 		condition.right = move(right);
38: 		conditions.push_back(move(condition));
39: 		return true;
40: 	}
41: 	return false;
42: }
43: 
44: unique_ptr<LogicalOperator> LogicalComparisonJoin::CreateJoin(JoinType type, unique_ptr<LogicalOperator> left_child,
45:                                                               unique_ptr<LogicalOperator> right_child,
46:                                                               unordered_set<idx_t> &left_bindings,
47:                                                               unordered_set<idx_t> &right_bindings,
48:                                                               vector<unique_ptr<Expression>> &expressions) {
49: 	vector<JoinCondition> conditions;
50: 	vector<unique_ptr<Expression>> arbitrary_expressions;
51: 	// first check if we can create
52: 	for (idx_t i = 0; i < expressions.size(); i++) {
53: 		auto &expr = expressions[i];
54: 		auto total_side = JoinSide::GetJoinSide(*expr, left_bindings, right_bindings);
55: 		if (total_side != JoinSide::BOTH) {
56: 			// join condition does not reference both sides, add it as filter under the join
57: 			if (type == JoinType::LEFT && total_side == JoinSide::RIGHT) {
58: 				// filter is on RHS and the join is a LEFT OUTER join, we can push it in the right child
59: 				if (right_child->type != LogicalOperatorType::FILTER) {
60: 					// not a filter yet, push a new empty filter
61: 					auto filter = make_unique<LogicalFilter>();
62: 					filter->AddChild(move(right_child));
63: 					right_child = move(filter);
64: 				}
65: 				// push the expression into the filter
66: 				auto &filter = (LogicalFilter &)*right_child;
67: 				filter.expressions.push_back(move(expr));
68: 				continue;
69: 			}
70: 		} else if (expr->type >= ExpressionType::COMPARE_EQUAL &&
71: 		           expr->type <= ExpressionType::COMPARE_GREATERTHANOREQUALTO) {
72: 			// comparison, check if we can create a comparison JoinCondition
73: 			if (CreateJoinCondition(*expr, left_bindings, right_bindings, conditions)) {
74: 				// successfully created the join condition
75: 				continue;
76: 			}
77: 		} else if (expr->type == ExpressionType::OPERATOR_NOT) {
78: 			auto &not_expr = (BoundOperatorExpression &)*expr;
79: 			assert(not_expr.children.size() == 1);
80: 			ExpressionType child_type = not_expr.children[0]->GetExpressionType();
81: 			// the condition is ON NOT (EXPRESSION)
82: 			// we can transform this to remove the NOT if the child is a Comparison
83: 			// e.g.:
84: 			// ON NOT (X = 3) can be turned into ON (X <> 3)
85: 			// ON NOT (X > 3) can be turned into ON (X <= 3)
86: 			// for non-comparison operators here we just push the filter
87: 			if (child_type >= ExpressionType::COMPARE_EQUAL &&
88: 			    child_type <= ExpressionType::COMPARE_GREATERTHANOREQUALTO) {
89: 				// switcheroo the child condition
90: 				// our join needs to compare explicit left and right sides. So we
91: 				// invert the condition to express NOT, this way we can still use
92: 				// equi-joins
93: 				not_expr.children[0]->type = NegateComparisionExpression(child_type);
94: 				if (CreateJoinCondition(*not_expr.children[0], left_bindings, right_bindings, conditions)) {
95: 					// successfully created the join condition
96: 					continue;
97: 				}
98: 			}
99: 		}
100: 		arbitrary_expressions.push_back(move(expr));
101: 	}
102: 	if (conditions.size() > 0) {
103: 		// we successfully convertedexpressions into JoinConditions
104: 		// create a LogicalComparisonJoin
105: 		auto comp_join = make_unique<LogicalComparisonJoin>(type);
106: 		comp_join->conditions = move(conditions);
107: 		comp_join->children.push_back(move(left_child));
108: 		comp_join->children.push_back(move(right_child));
109: 		if (arbitrary_expressions.size() > 0) {
110: 			// we have some arbitrary expressions as well
111: 			// add them to a filter
112: 			auto filter = make_unique<LogicalFilter>();
113: 			for (auto &expr : arbitrary_expressions) {
114: 				filter->expressions.push_back(move(expr));
115: 			}
116: 			LogicalFilter::SplitPredicates(filter->expressions);
117: 			filter->children.push_back(move(comp_join));
118: 			return move(filter);
119: 		}
120: 		return move(comp_join);
121: 	} else {
122: 		if (arbitrary_expressions.size() == 0) {
123: 			// all conditions were pushed down, add TRUE predicate
124: 			arbitrary_expressions.push_back(make_unique<BoundConstantExpression>(Value::BOOLEAN(true)));
125: 		}
126: 		// if we get here we could not create any JoinConditions
127: 		// turn this into an arbitrary expression join
128: 		auto any_join = make_unique<LogicalAnyJoin>(type);
129: 		// create the condition
130: 		any_join->children.push_back(move(left_child));
131: 		any_join->children.push_back(move(right_child));
132: 		// AND all the arbitrary expressions together
133: 		// do the same with any remaining conditions
134: 		any_join->condition = move(arbitrary_expressions[0]);
135: 		for (idx_t i = 1; i < arbitrary_expressions.size(); i++) {
136: 			any_join->condition = make_unique<BoundConjunctionExpression>(
137: 			    ExpressionType::CONJUNCTION_AND, move(any_join->condition), move(arbitrary_expressions[i]));
138: 		}
139: 		return move(any_join);
140: 	}
141: }
142: 
143: unique_ptr<LogicalOperator> Binder::CreatePlan(BoundJoinRef &ref) {
144: 	auto left = CreatePlan(*ref.left);
145: 	auto right = CreatePlan(*ref.right);
146: 
147: 	if (ref.type == JoinType::INNER) {
148: 		// inner join, generate a cross product + filter
149: 		// this will be later turned into a proper join by the join order optimizer
150: 		auto cross_product = make_unique<LogicalCrossProduct>();
151: 
152: 		cross_product->AddChild(move(left));
153: 		cross_product->AddChild(move(right));
154: 
155: 		unique_ptr<LogicalOperator> root = move(cross_product);
156: 
157: 		auto filter = make_unique<LogicalFilter>(move(ref.condition));
158: 		// visit the expressions in the filter
159: 		for (idx_t i = 0; i < filter->expressions.size(); i++) {
160: 			PlanSubqueries(&filter->expressions[i], &root);
161: 		}
162: 		filter->AddChild(move(root));
163: 		return move(filter);
164: 	}
165: 
166: 	// split the expressions by the AND clause
167: 	vector<unique_ptr<Expression>> expressions;
168: 	expressions.push_back(move(ref.condition));
169: 	LogicalFilter::SplitPredicates(expressions);
170: 
171: 	// find the table bindings on the LHS and RHS of the join
172: 	unordered_set<idx_t> left_bindings, right_bindings;
173: 	LogicalJoin::GetTableReferences(*left, left_bindings);
174: 	LogicalJoin::GetTableReferences(*right, right_bindings);
175: 	// now create the join operator from the set of join conditions
176: 	auto result = LogicalComparisonJoin::CreateJoin(ref.type, move(left), move(right), left_bindings, right_bindings,
177: 	                                                expressions);
178: 
179: 	LogicalOperator *join;
180: 	if (result->type == LogicalOperatorType::FILTER) {
181: 		join = result->children[0].get();
182: 	} else {
183: 		join = result.get();
184: 	}
185: 
186: 	// we visit the expressions depending on the type of join
187: 	if (join->type == LogicalOperatorType::COMPARISON_JOIN) {
188: 		// comparison join
189: 		// in this join we visit the expressions on the LHS with the LHS as root node
190: 		// and the expressions on the RHS with the RHS as root node
191: 		auto &comp_join = (LogicalComparisonJoin &)*join;
192: 		for (idx_t i = 0; i < comp_join.conditions.size(); i++) {
193: 			PlanSubqueries(&comp_join.conditions[i].left, &comp_join.children[0]);
194: 			PlanSubqueries(&comp_join.conditions[i].right, &comp_join.children[1]);
195: 		}
196: 	} else if (join->type == LogicalOperatorType::ANY_JOIN) {
197: 		auto &any_join = (LogicalAnyJoin &)*join;
198: 		// for the any join we just visit the condition
199: 		if (any_join.condition->HasSubquery()) {
200: 			throw NotImplementedException("Cannot perform non-inner join on subquery!");
201: 		}
202: 	}
203: 	return result;
204: }
[end of src/planner/binder/tableref/plan_joinref.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: