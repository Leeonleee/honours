You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Segmentation fault
### What happens?

Sometimes a segmentation fault when DuckDB is backed by a file, in-memory it does not issue a seg fault.

Out of 277 datasets, only 5 have this issue. With provided dataset, can always be reproduced.

```
(gdb) bt
#0  0x0000555555775ed6 in std::__shared_ptr<duckdb::DataTableInfo, (__gnu_cxx::_Lock_policy)2>::get (this=0x29c) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1667
#1  0x00005555557688de in duckdb::shared_ptr<duckdb::DataTableInfo, true>::operator* (this=0x29c) at /home/skinkie/Sources/duckdb/src/include/duckdb/common/shared_ptr_ipp.hpp:196
#2  0x000055555621bb98 in duckdb::RowGroupCollection::GetTableInfo (this=0x28c) at /home/skinkie/Sources/duckdb/src/include/duckdb/storage/table/row_group_collection.hpp:129
#3  0x00005555561e5cc6 in duckdb::RowGroup::GetTableInfo (this=0x55555a7159e0) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:135
#4  0x00005555561e6c1a in duckdb::RowGroup::AddColumn (this=0x55555a7159e0, new_collection=..., new_column=..., executor=..., result=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:305
#5  0x00005555561f019a in duckdb::RowGroupCollection::AddColumn (this=0x55555a1ee9c0, context=..., new_column=..., default_executor=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group_collection.cpp:1066
#6  0x0000555556279e05 in duckdb::DataTable::DataTable (this=0x55555a80eaf0, context=..., parent=..., new_column=..., default_value=...) at /home/skinkie/Sources/duckdb/src/storage/data_table.cpp:80
#7  0x00005555557a16e8 in std::_Construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/stl_construct.h:119
#8  0x000055555579c4e8 in std::allocator_traits<std::allocator<void> >::construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/alloc_traits.h:657
#9  std::_Sp_counted_ptr_inplace<duckdb::DataTable, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x55555a80eae0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:607
#10 0x00005555557974e8 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<duckdb::DataTable, std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x7fffffffa588, __p=@0x7fffffffa580: 0x0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:969
#11 0x00005555557916e2 in std::__shared_ptr<duckdb::DataTable, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x7fffffffa580, __tag=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1713
#12 0x0000555555788805 in std::shared_ptr<duckdb::DataTable>::shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (this=0x7fffffffa580, __tag=...)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:463
#13 0x000055555577cb90 in std::make_shared<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:1008
#14 0x000055555576f1f0 in duckdb::make_shared_ptr<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /home/skinkie/Sources/duckdb/src/include/duckdb/common/helper.hpp:73
#15 0x0000555555753e98 in duckdb::DuckTableEntry::AddColumn (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:391
#16 0x000055555575271e in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:231
#17 0x00005555557522bc in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:184
#18 0x00005555557b6bd8 in duckdb::CatalogSet::AlterEntry (this=0x55555a16be20, transaction=..., name="kv6_import", alter_info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_set.cpp:328
#19 0x000055555574fcf9 in duckdb::DuckSchemaEntry::Alter (this=0x55555a16bd40, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_schema_entry.cpp:291
#20 0x00005555557b156a in duckdb::Catalog::Alter (this=0x55555a1704e0, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:904
#21 0x00005555557b176b in duckdb::Catalog::Alter (this=0x55555a1704e0, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:912
#22 0x00005555570b1ff5 in duckdb::PhysicalAlter::GetData (this=0x7fffdc5f55a0, context=..., chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/execution/operator/schema/physical_alter.cpp:13
#23 0x0000555555ff23af in duckdb::PipelineExecutor::GetData (this=0x55555a31e250, chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:478
#24 0x0000555555ff24a0 in duckdb::PipelineExecutor::FetchFromSource (this=0x55555a31e250, result=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:504
#25 0x0000555555ff1268 in duckdb::PipelineExecutor::Execute (this=0x55555a31e250, max_chunks=50) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:204
#26 0x0000555555fed597 in duckdb::PipelineTask::ExecuteTask (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/pipeline.cpp:40
#27 0x0000555555fe7c38 in duckdb::ExecutorTask::Execute (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/executor_task.cpp:44
#28 0x0000555555feb739 in duckdb::Executor::ExecuteTask (this=0x55555a173350, dry_run=false) at /home/skinkie/Sources/duckdb/src/parallel/executor.cpp:580
#29 0x0000555555eab6d5 in duckdb::ClientContext::ExecuteTaskInternal (this=0x55555a16e960, lock=..., result=..., dry_run=false) at /home/skinkie/Sources/duckdb/src/main/client_context.cpp:557
#30 0x0000555555ec66bd in duckdb::PendingQueryResult::ExecuteTaskInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:68
#31 0x0000555555ec6751 in duckdb::PendingQueryResult::ExecuteInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:75
#32 0x0000555555ec6905 in duckdb::PendingQueryResult::Execute (this=0x55555a46b6d0) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:95
#33 0x0000555555ec72be in duckdb::PreparedStatement::Execute (this=0x55555a4dd5b0, values=..., allow_stream_result=false) at /home/skinkie/Sources/duckdb/src/main/prepared_statement.cpp:85
#34 0x00005555556b91da in duckdb_shell_sqlite3_print_duckbox (pStmt=0x7fffcc091870, max_rows=40, max_width=0, null_value=0x7fffffffc03c "", columnar=0)
    at /home/skinkie/Sources/duckdb/tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp:249
#35 0x000055555568be00 in exec_prepared_stmt (pArg=0x7fffffffbf20, pStmt=0x7fffcc091870) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:12752
#36 0x000055555568cc4c in shell_exec (pArg=0x7fffffffbf20, zSql=0x55555a1ef160 "alter table kv6_import add column trip_id varchar(16);", pzErrMsg=0x7fffffffbdb0) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:13087
#37 0x0000555555699c79 in runOneSqlLine (p=0x7fffffffbf20, zSql=0x55555a1ef160 "alter table kv6_import add column trip_id varchar(16);", in=0x55555a31e000, startline=31) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19273
#38 0x000055555569a16f in process_input (p=0x7fffffffbf20) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19384
#39 0x000055555569a49a in process_sqliterc (p=0x7fffffffbf20, sqliterc_override=0x7fffffffd720 "/tmp/script.sql") at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19505
#40 0x000055555569b174 in main (argc=4, argv=0x7fffffffd258) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19951
```

### To Reproduce

```sql
DROP TABLE IF EXISTS kv6_import;
DROP TABLE IF EXISTS tt_import;

CREATE TABLE "kv6_import" (
        "receive"                   TIMESTAMP     NOT NULL,
        "message"                   TIMESTAMP     NOT NULL,
        "vehicle"                   TIMESTAMP     NOT NULL,
        "messagetype"               VARCHAR(10)   NOT NULL,
        "operatingday"              DATE          NOT NULL,
        "dataownercode"             VARCHAR(10)   NOT NULL,
        "lineplanningnumber"        VARCHAR(10),
        "journeynumber"             UINTEGER       NOT NULL,
        "reinforcementnumber"       UTINYINT       NOT NULL,
        "userstopcode"              VARCHAR(10),
        "passagesequencenumber"     TINYINT,
        "distancesincelastuserstop" INTEGER,
        "punctuality"               INTEGER,
        "rd_x"                      VARCHAR(11),
        "rd_y"                      VARCHAR(11),
        "blockcode"                 INTEGER,
        "vehiclenumber"             SMALLINT,
        "wheelchairaccessible"      VARCHAR(5),
        "source"                    VARCHAR(10)   NOT NULL,
        "numberofcoaches"           UTINYINT
);

copy kv6_import from '/tmp/kv6-20240917.log' (DELIMITER ';');

delete from kv6_import where messagetype not in ('ARRIVAL', 'DEPARTURE');

alter table kv6_import add column trip_id varchar(16);

update kv6_import set trip_id = dataownercode || ':' || lineplanningnumber || ':' || journeynumber;

CREATE TABLE "tt_import" (
        "operatingday"          DATE          NOT NULL,
        "trip_id"               VARCHAR(16)   NOT NULL,
        "pointorder"            UTINYINT      NOT NULL,
        "passagesequencenumber" UTINYINT      NOT NULL,
        "userstopcode"          VARCHAR(10)   NOT NULL,
        "targetarrivaltime"     VARCHAR(8)    NOT NULL,
        "targetdeparturetime"   VARCHAR(8)    NOT NULL,
        "recordedarrivaltime"   VARCHAR(8),
        "recordeddeparturetime" VARCHAR(8)
);

copy tt_import from '/tmp/transittimes.csv' (DELIMITER ',');

delete from tt_import where trip_id like 'DOEKSEN:%' or trip_id like 'WSF:%' or trip_id like 'IFF%' or trip_id like 'WPD%' or trip_id like 'TESO%';

alter table tt_import drop column recordedarrivaltime;
alter table tt_import drop column recordeddeparturetime;

update kv6_import set trip_id = w.trip_id from (select tt_import.trip_id as trip_id, z.trip_id as kv6_trip_id, vehiclenumber from tt_import join (select trip_id, userstopcode, vehiclenumber from kv6_import join (select trip_id, min(receive) as receive from kv6_import where trip_id in (select trip_id from kv6_import where trip_id like 'ARR:%' and length(trip_id) = 13  except select trip_id from tt_import) group by trip_id) as y using (trip_id, receive)) as z using (userstopcode) where pointorder = 1 and tt_import.trip_id like z.trip_id[0:8] || '_' || z.trip_id[-5:]) as w where kv6_import.trip_id = kv6_trip_id and kv6_import.vehiclenumber = w.vehiclenumber;

copy (select tt_import.operatingday, tt_import.trip_id, x.reinforcementnumber, x.vehiclenumber, tt_import.userstopcode, tt_import.passagesequencenumber, tt_import.pointorder, tt_import.targetarrivaltime, tt_import.targetdeparturetime, x.arrival as recordedarrivaltime, x.departure as recordeddeparturetime from (select trip_id, reinforcementnumber, vehiclenumber, userstopcode, passagesequencenumber, max(a.receive) as arrival, max(b.receive) as departure from kv6_import as a full outer join kv6_import as b using (trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) where a.messagetype = 'ARRIVAL' and b.messagetype = 'DEPARTURE' group by trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) as x right join tt_import using (trip_id, userstopcode, passagesequencenumber) order by trip_id, reinforcementnumber, pointorder) to '/home/skinkie/arriva/volledig-20240917.csv.gz' header;
```

Data:
https://download.stefan.konink.de/duckdb/kv6-20240917.log.gz
https://download.stefan.konink.de/duckdb/transittimes.csv.gz


### OS:

Linux

### DuckDB Version:

main

### DuckDB Client:

cli

### Hardware:

amd64

### Full Name:

Stefan de Konink

### Affiliation:

Stichting OpenGeo

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
Segmentation fault
### What happens?

Sometimes a segmentation fault when DuckDB is backed by a file, in-memory it does not issue a seg fault.

Out of 277 datasets, only 5 have this issue. With provided dataset, can always be reproduced.

```
(gdb) bt
#0  0x0000555555775ed6 in std::__shared_ptr<duckdb::DataTableInfo, (__gnu_cxx::_Lock_policy)2>::get (this=0x29c) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1667
#1  0x00005555557688de in duckdb::shared_ptr<duckdb::DataTableInfo, true>::operator* (this=0x29c) at /home/skinkie/Sources/duckdb/src/include/duckdb/common/shared_ptr_ipp.hpp:196
#2  0x000055555621bb98 in duckdb::RowGroupCollection::GetTableInfo (this=0x28c) at /home/skinkie/Sources/duckdb/src/include/duckdb/storage/table/row_group_collection.hpp:129
#3  0x00005555561e5cc6 in duckdb::RowGroup::GetTableInfo (this=0x55555a7159e0) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:135
#4  0x00005555561e6c1a in duckdb::RowGroup::AddColumn (this=0x55555a7159e0, new_collection=..., new_column=..., executor=..., result=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:305
#5  0x00005555561f019a in duckdb::RowGroupCollection::AddColumn (this=0x55555a1ee9c0, context=..., new_column=..., default_executor=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group_collection.cpp:1066
#6  0x0000555556279e05 in duckdb::DataTable::DataTable (this=0x55555a80eaf0, context=..., parent=..., new_column=..., default_value=...) at /home/skinkie/Sources/duckdb/src/storage/data_table.cpp:80
#7  0x00005555557a16e8 in std::_Construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/stl_construct.h:119
#8  0x000055555579c4e8 in std::allocator_traits<std::allocator<void> >::construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/alloc_traits.h:657
#9  std::_Sp_counted_ptr_inplace<duckdb::DataTable, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x55555a80eae0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:607
#10 0x00005555557974e8 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<duckdb::DataTable, std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x7fffffffa588, __p=@0x7fffffffa580: 0x0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:969
#11 0x00005555557916e2 in std::__shared_ptr<duckdb::DataTable, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x7fffffffa580, __tag=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1713
#12 0x0000555555788805 in std::shared_ptr<duckdb::DataTable>::shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (this=0x7fffffffa580, __tag=...)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:463
#13 0x000055555577cb90 in std::make_shared<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:1008
#14 0x000055555576f1f0 in duckdb::make_shared_ptr<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /home/skinkie/Sources/duckdb/src/include/duckdb/common/helper.hpp:73
#15 0x0000555555753e98 in duckdb::DuckTableEntry::AddColumn (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:391
#16 0x000055555575271e in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:231
#17 0x00005555557522bc in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:184
#18 0x00005555557b6bd8 in duckdb::CatalogSet::AlterEntry (this=0x55555a16be20, transaction=..., name="kv6_import", alter_info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_set.cpp:328
#19 0x000055555574fcf9 in duckdb::DuckSchemaEntry::Alter (this=0x55555a16bd40, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_schema_entry.cpp:291
#20 0x00005555557b156a in duckdb::Catalog::Alter (this=0x55555a1704e0, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:904
#21 0x00005555557b176b in duckdb::Catalog::Alter (this=0x55555a1704e0, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:912
#22 0x00005555570b1ff5 in duckdb::PhysicalAlter::GetData (this=0x7fffdc5f55a0, context=..., chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/execution/operator/schema/physical_alter.cpp:13
#23 0x0000555555ff23af in duckdb::PipelineExecutor::GetData (this=0x55555a31e250, chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:478
#24 0x0000555555ff24a0 in duckdb::PipelineExecutor::FetchFromSource (this=0x55555a31e250, result=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:504
#25 0x0000555555ff1268 in duckdb::PipelineExecutor::Execute (this=0x55555a31e250, max_chunks=50) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:204
#26 0x0000555555fed597 in duckdb::PipelineTask::ExecuteTask (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/pipeline.cpp:40
#27 0x0000555555fe7c38 in duckdb::ExecutorTask::Execute (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/executor_task.cpp:44
#28 0x0000555555feb739 in duckdb::Executor::ExecuteTask (this=0x55555a173350, dry_run=false) at /home/skinkie/Sources/duckdb/src/parallel/executor.cpp:580
#29 0x0000555555eab6d5 in duckdb::ClientContext::ExecuteTaskInternal (this=0x55555a16e960, lock=..., result=..., dry_run=false) at /home/skinkie/Sources/duckdb/src/main/client_context.cpp:557
#30 0x0000555555ec66bd in duckdb::PendingQueryResult::ExecuteTaskInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:68
#31 0x0000555555ec6751 in duckdb::PendingQueryResult::ExecuteInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:75
#32 0x0000555555ec6905 in duckdb::PendingQueryResult::Execute (this=0x55555a46b6d0) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:95
#33 0x0000555555ec72be in duckdb::PreparedStatement::Execute (this=0x55555a4dd5b0, values=..., allow_stream_result=false) at /home/skinkie/Sources/duckdb/src/main/prepared_statement.cpp:85
#34 0x00005555556b91da in duckdb_shell_sqlite3_print_duckbox (pStmt=0x7fffcc091870, max_rows=40, max_width=0, null_value=0x7fffffffc03c "", columnar=0)
    at /home/skinkie/Sources/duckdb/tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp:249
#35 0x000055555568be00 in exec_prepared_stmt (pArg=0x7fffffffbf20, pStmt=0x7fffcc091870) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:12752
#36 0x000055555568cc4c in shell_exec (pArg=0x7fffffffbf20, zSql=0x55555a1ef160 "alter table kv6_import add column trip_id varchar(16);", pzErrMsg=0x7fffffffbdb0) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:13087
#37 0x0000555555699c79 in runOneSqlLine (p=0x7fffffffbf20, zSql=0x55555a1ef160 "alter table kv6_import add column trip_id varchar(16);", in=0x55555a31e000, startline=31) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19273
#38 0x000055555569a16f in process_input (p=0x7fffffffbf20) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19384
#39 0x000055555569a49a in process_sqliterc (p=0x7fffffffbf20, sqliterc_override=0x7fffffffd720 "/tmp/script.sql") at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19505
#40 0x000055555569b174 in main (argc=4, argv=0x7fffffffd258) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19951
```

### To Reproduce

```sql
DROP TABLE IF EXISTS kv6_import;
DROP TABLE IF EXISTS tt_import;

CREATE TABLE "kv6_import" (
        "receive"                   TIMESTAMP     NOT NULL,
        "message"                   TIMESTAMP     NOT NULL,
        "vehicle"                   TIMESTAMP     NOT NULL,
        "messagetype"               VARCHAR(10)   NOT NULL,
        "operatingday"              DATE          NOT NULL,
        "dataownercode"             VARCHAR(10)   NOT NULL,
        "lineplanningnumber"        VARCHAR(10),
        "journeynumber"             UINTEGER       NOT NULL,
        "reinforcementnumber"       UTINYINT       NOT NULL,
        "userstopcode"              VARCHAR(10),
        "passagesequencenumber"     TINYINT,
        "distancesincelastuserstop" INTEGER,
        "punctuality"               INTEGER,
        "rd_x"                      VARCHAR(11),
        "rd_y"                      VARCHAR(11),
        "blockcode"                 INTEGER,
        "vehiclenumber"             SMALLINT,
        "wheelchairaccessible"      VARCHAR(5),
        "source"                    VARCHAR(10)   NOT NULL,
        "numberofcoaches"           UTINYINT
);

copy kv6_import from '/tmp/kv6-20240917.log' (DELIMITER ';');

delete from kv6_import where messagetype not in ('ARRIVAL', 'DEPARTURE');

alter table kv6_import add column trip_id varchar(16);

update kv6_import set trip_id = dataownercode || ':' || lineplanningnumber || ':' || journeynumber;

CREATE TABLE "tt_import" (
        "operatingday"          DATE          NOT NULL,
        "trip_id"               VARCHAR(16)   NOT NULL,
        "pointorder"            UTINYINT      NOT NULL,
        "passagesequencenumber" UTINYINT      NOT NULL,
        "userstopcode"          VARCHAR(10)   NOT NULL,
        "targetarrivaltime"     VARCHAR(8)    NOT NULL,
        "targetdeparturetime"   VARCHAR(8)    NOT NULL,
        "recordedarrivaltime"   VARCHAR(8),
        "recordeddeparturetime" VARCHAR(8)
);

copy tt_import from '/tmp/transittimes.csv' (DELIMITER ',');

delete from tt_import where trip_id like 'DOEKSEN:%' or trip_id like 'WSF:%' or trip_id like 'IFF%' or trip_id like 'WPD%' or trip_id like 'TESO%';

alter table tt_import drop column recordedarrivaltime;
alter table tt_import drop column recordeddeparturetime;

update kv6_import set trip_id = w.trip_id from (select tt_import.trip_id as trip_id, z.trip_id as kv6_trip_id, vehiclenumber from tt_import join (select trip_id, userstopcode, vehiclenumber from kv6_import join (select trip_id, min(receive) as receive from kv6_import where trip_id in (select trip_id from kv6_import where trip_id like 'ARR:%' and length(trip_id) = 13  except select trip_id from tt_import) group by trip_id) as y using (trip_id, receive)) as z using (userstopcode) where pointorder = 1 and tt_import.trip_id like z.trip_id[0:8] || '_' || z.trip_id[-5:]) as w where kv6_import.trip_id = kv6_trip_id and kv6_import.vehiclenumber = w.vehiclenumber;

copy (select tt_import.operatingday, tt_import.trip_id, x.reinforcementnumber, x.vehiclenumber, tt_import.userstopcode, tt_import.passagesequencenumber, tt_import.pointorder, tt_import.targetarrivaltime, tt_import.targetdeparturetime, x.arrival as recordedarrivaltime, x.departure as recordeddeparturetime from (select trip_id, reinforcementnumber, vehiclenumber, userstopcode, passagesequencenumber, max(a.receive) as arrival, max(b.receive) as departure from kv6_import as a full outer join kv6_import as b using (trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) where a.messagetype = 'ARRIVAL' and b.messagetype = 'DEPARTURE' group by trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) as x right join tt_import using (trip_id, userstopcode, passagesequencenumber) order by trip_id, reinforcementnumber, pointorder) to '/home/skinkie/arriva/volledig-20240917.csv.gz' header;
```

Data:
https://download.stefan.konink.de/duckdb/kv6-20240917.log.gz
https://download.stefan.konink.de/duckdb/transittimes.csv.gz


### OS:

Linux

### DuckDB Version:

main

### DuckDB Client:

cli

### Hardware:

amd64

### Full Name:

Stefan de Konink

### Affiliation:

Stichting OpenGeo

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of .github/workflows/Regression.yml]
1: name: Regression
2: on:
3:   workflow_dispatch:
4:   repository_dispatch:
5:   push:
6:     branches:
7:       - '**'
8:       - '!main'
9:       - '!feature'
10:     paths-ignore:
11:       - '**.md'
12:       - 'tools/**'
13:       - '!tools/pythonpkg/**'
14:       - '.github/patches/duckdb-wasm/**'
15:       - '.github/workflows/**'
16:       - '!.github/workflows/Regression.yml'
17: 
18:   pull_request:
19:     types: [opened, reopened, ready_for_review]
20:     paths-ignore:
21:       - '**.md'
22:       - 'tools/**'
23:       - '!tools/pythonpkg/**'
24:       - '.github/patches/duckdb-wasm/**'
25:       - '.github/workflows/**'
26:       - '!.github/workflows/Regression.yml'
27: 
28: 
29: concurrency:
30:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/main' || github.sha }}
31:   cancel-in-progress: true
32: 
33: env:
34:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
35:   BASE_BRANCH: ${{ github.base_ref || (endsWith(github.ref, '_feature') && 'feature' || 'main') }}
36: 
37: jobs:
38:  regression-test-benchmark-runner:
39:   name: Regression Tests
40:   runs-on: ubuntu-20.04
41:   env:
42:     CC: gcc-10
43:     CXX: g++-10
44:     GEN: ninja
45:     BUILD_BENCHMARK: 1
46:     BUILD_TPCH: 1
47:     BUILD_TPCDS: 1
48:     BUILD_HTTPFS: 1
49:     BUILD_JEMALLOC: 1
50:     CORE_EXTENSIONS: "inet"
51: 
52:   steps:
53:     - uses: actions/checkout@v4
54:       with:
55:         fetch-depth: 0
56: 
57:     - uses: actions/setup-python@v5
58:       with:
59:         python-version: '3.12'
60: 
61:     - name: Install
62:       shell: bash
63:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install requests
64: 
65:     - name: Setup Ccache
66:       uses: hendrikmuhs/ccache-action@main
67:       with:
68:         key: ${{ github.job }}
69:         save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}
70: 
71:     - name: Build
72:       shell: bash
73:       run: |
74:         make
75:         git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1
76:         cd duckdb
77:         make
78:         cd ..
79: 
80:     - name: Set up benchmarks
81:       shell: bash
82:       run: |
83:         cp -r benchmark duckdb/
84: 
85:     - name: Regression Test Micro
86:       if: always()
87:       shell: bash
88:       run: |
89:         python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/micro.csv --verbose --threads=2
90: 
91:     - name: Regression Test Ingestion Perf
92:       if: always()
93:       shell: bash
94:       run: |
95:         python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/ingestion.csv --verbose --threads=2
96: 
97:     - name: Regression Test TPCH
98:       if: always()
99:       shell: bash
100:       run: |
101:         python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/tpch.csv --verbose --threads=2
102: 
103:     - name: Regression Test TPCH-PARQUET
104:       if: always()
105:       shell: bash
106:       run: |
107:         python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/tpch_parquet.csv --verbose --threads=2
108: 
109:     - name: Regression Test TPCDS
110:       if: always()
111:       shell: bash
112:       run: |
113:         python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/tpcds.csv --verbose --threads=2
114: 
115:     - name: Regression Test H2OAI
116:       if: always()
117:       shell: bash
118:       run: |
119:         python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/h2oai.csv --verbose --threads=2
120: 
121:     - name: Regression Test IMDB
122:       if: always()
123:       shell: bash
124:       run: |
125:         python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/imdb.csv --verbose --threads=2
126: 
127:     - name: Regression Test CSV
128:       if: always()
129:       shell: bash
130:       run: |
131:         python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/csv.csv --verbose --threads=2
132: 
133:  regression-test-storage:
134:   name: Storage Size Regression Test
135:   runs-on: ubuntu-20.04
136:   env:
137:     CC: gcc-10
138:     CXX: g++-10
139:     GEN: ninja
140:     BUILD_TPCH: 1
141:     BUILD_TPCDS: 1
142: 
143:   steps:
144:     - uses: actions/checkout@v4
145:       with:
146:         fetch-depth: 0
147: 
148:     - uses: actions/setup-python@v5
149:       with:
150:         python-version: '3.12'
151: 
152:     - name: Install
153:       shell: bash
154:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install requests
155: 
156:     - name: Setup Ccache
157:       uses: hendrikmuhs/ccache-action@main
158:       with:
159:         key: ${{ github.job }}
160:         save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}
161: 
162:     - name: Build
163:       shell: bash
164:       run: |
165:         make
166:         git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1
167:         cd duckdb
168:         make
169:         cd ..
170: 
171:     - name: Regression Test
172:       shell: bash
173:       run: |
174:         python scripts/regression_test_storage_size.py --old=duckdb/build/release/duckdb --new=build/release/duckdb
175: 
176:     - name: Test for incompatibility
177:       shell: bash
178:       run: |
179:         if (cmp test/sql/storage_version/storage_version.db duckdb/test/sql/storage_version/storage_version.db); then
180:           echo "storage_changed=false" >> $GITHUB_ENV
181:         else
182:           echo "storage_changed=true" >> $GITHUB_ENV
183:         fi
184: 
185:     - name: Regression Compatibility Test (testing bidirectional compatibility)
186:       shell: bash
187:       if: env.storage_changed == 'false'
188:       run: |
189:         # Regenerate test/sql/storage_version.db with newer version -> read with older version
190:         python3 scripts/generate_storage_version.py
191:         ./duckdb/build/release/duckdb test/sql/storage_version/storage_version.db
192:         # Regenerate test/sql/storage_version.db with older version -> read with newer version (already performed as part of test.slow)
193:         cd duckdb
194:         python3 ../scripts/generate_storage_version.py
195:         ../build/release/duckdb duckdb/test/sql/storage_version/storage_version.db
196:         cd ..
197: 
198:     - name: Regression Compatibility Test (testing storage version has been bumped)
199:       shell: bash
200:       if: env.storage_changed == 'true'
201:       run: |
202:         python3 scripts/generate_storage_version.py
203:         cd duckdb
204:         python3 scripts/generate_storage_version.py
205:         cd ..
206:         if (cmp -i 8 -n 12 test/sql/storage_version.db duckdb/test/sql/storage_version.db); then
207:            echo "Expected storage format to be bumped, but this is not the case"
208:            echo "This might fail spuriously if changes to content of test database / generation script happened"
209:            exit 1
210:         else
211:            echo "Storage bump detected, all good!"
212:         fi
213: 
214:  regression-test-python:
215:   name: Regression Test (Python Client)
216:   runs-on: ubuntu-20.04
217:   env:
218:     CC: gcc-10
219:     CXX: g++-10
220:     GEN: ninja
221: 
222:   steps:
223:     - uses: actions/checkout@v4
224:       with:
225:         fetch-depth: 0
226: 
227:     - uses: actions/setup-python@v5
228:       with:
229:         python-version: '3.12'
230: 
231:     - name: Install
232:       shell: bash
233:       run: |
234:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
235:         pip install numpy pytest pandas mypy psutil pyarrow
236: 
237:     - name: Setup Ccache
238:       uses: hendrikmuhs/ccache-action@main
239:       with:
240:         key: ${{ github.job }}
241:         save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}
242: 
243:     - name: Build Current Version
244:       shell: bash
245:       run: |
246:         cd tools/pythonpkg
247:         pip install --use-pep517 . --user
248:         cd ../..
249: 
250:     - name: Run New Version
251:       shell: bash
252:       run: |
253:         python scripts/regression_test_python.py --threads=2 --out-file=new.csv
254: 
255:     - name: Cleanup New Version
256:       shell: bash
257:       run: |
258:         cd tools/pythonpkg
259:         ./clean.sh
260:         cd ../..
261: 
262:     - name: Build Current
263:       shell: bash
264:       run: |
265:         git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1
266:         cd duckdb/tools/pythonpkg
267:         pip install --use-pep517 . --user
268:         cd ../../..
269: 
270:     - name: Run Current Version
271:       shell: bash
272:       run: |
273:         python scripts/regression_test_python.py --threads=2 --out-file=current.csv
274: 
275:     - name: Regression Test
276:       shell: bash
277:       run: |
278:         cp -r benchmark duckdb/
279:         python scripts/regression_check.py --old=current.csv --new=new.csv
280: 
281:  regression-test-plan-cost:
282:   name: Regression Test Join Order Plan Cost
283:   runs-on: ubuntu-20.04
284:   env:
285:     CC: gcc-10
286:     CXX: g++-10
287:     GEN: ninja
288:     BUILD_TPCH: 1
289:     BUILD_HTTPFS: 1
290: 
291:   steps:
292:     - uses: actions/checkout@v4
293:       with:
294:         fetch-depth: 0
295: 
296:     - uses: actions/setup-python@v5
297:       with:
298:         python-version: '3.12'
299: 
300:     - name: Install
301:       shell: bash
302:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install tqdm
303: 
304:     - name: Setup Ccache
305:       uses: hendrikmuhs/ccache-action@main
306:       with:
307:         key: ${{ github.job }}
308:         save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}
309: 
310:     - name: Build
311:       shell: bash
312:       run: |
313:         make
314:         git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1
315:         cd duckdb
316:         make
317:         cd ..
318: 
319:     - name: Set up benchmarks
320:       shell: bash
321:       run: |
322:         cp -r benchmark duckdb/
323: 
324:     - name: Regression Test IMDB
325:       if: always()
326:       shell: bash
327:       run: |
328:         python scripts/plan_cost_runner.py --old=duckdb/build/release/duckdb --new=build/release/duckdb --dir=benchmark/imdb_plan_cost
329: 
330:     - name: Regression Test TPCH
331:       if: always()
332:       shell: bash
333:       run: |
334:         python scripts/plan_cost_runner.py --old=duckdb/build/release/duckdb --new=build/release/duckdb --dir=benchmark/tpch_plan_cost
[end of .github/workflows/Regression.yml]
[start of extension/tpcds/dsdgen/answers/sf100/67.csv]
1: i_category|i_class|i_brandNULL|i_product_name|d_year|d_qoy|d_moy|s_store_idNULL|sumsales|rk
2: NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|32029263.56|5
3: NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|64897398.47|4
4: NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|130412669.10|3
5: NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|255726974.02|2
6: NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|101405424479.10|1
7: NULL|NULL|NULL|NULL|2000|NULL|NULL|NULL|32029263.56|5
8: NULL|NULL|NULL|NULL|2000|1|NULL|NULL|4638714.74|17
9: NULL|NULL|NULL|NULL|2000|1|1|NULL|1783708.73|96
10: NULL|NULL|NULL|NULL|2000|2|NULL|NULL|4645572.81|16
11: NULL|NULL|NULL|NULL|2000|2|4|NULL|1561458.02|98
12: NULL|NULL|NULL|NULL|2000|2|5|NULL|1590047.34|97
13: NULL|NULL|NULL|NULL|2000|2|6|NULL|1494067.45|100
14: NULL|NULL|NULL|NULL|2000|3|NULL|NULL|8690732.06|8
15: NULL|NULL|NULL|NULL|2000|3|7|NULL|1528725.34|99
16: NULL|NULL|NULL|NULL|2000|3|8|NULL|3598412.10|23
17: NULL|NULL|NULL|NULL|2000|3|9|NULL|3563594.62|24
18: NULL|NULL|NULL|NULL|2000|4|NULL|NULL|14054243.95|7
19: NULL|NULL|NULL|NULL|2000|4|10|NULL|3651929.46|22
20: NULL|NULL|NULL|NULL|2000|4|11|NULL|4950352.60|14
21: NULL|NULL|NULL|NULL|2000|4|12|NULL|5451961.89|11
22: NULL|NULL|amalgedu pack #2|NULL|NULL|NULL|NULL|NULL|1948460.41|84
23: NULL|NULL|amalgimporto #2|NULL|NULL|NULL|NULL|NULL|2060499.95|51
24: NULL|NULL|amalgscholar #2|NULL|NULL|NULL|NULL|NULL|2915350.58|33
25: NULL|NULL|edu packamalg #2|NULL|NULL|NULL|NULL|NULL|2337169.45|38
26: NULL|NULL|edu packamalgamalg #17|NULL|NULL|NULL|NULL|NULL|1983663.81|75
27: NULL|NULL|edu packamalgamalg #17|NULL|NULL|NULL|NULL|NULL|1983663.81|75
28: NULL|NULL|edu packamalgamalg #17|NULL|2000|NULL|NULL|NULL|1983663.81|75
29: NULL|NULL|exportiedu pack #1|NULL|NULL|NULL|NULL|NULL|2015173.47|63
30: NULL|NULL|exportiedu pack #1|NULL|NULL|NULL|NULL|NULL|2015173.47|63
31: NULL|NULL|exportiedu pack #1|NULL|2000|NULL|NULL|NULL|2015173.47|63
32: NULL|NULL|exportiedu pack #2|NULL|NULL|NULL|NULL|NULL|1980110.68|79
33: NULL|NULL|exportiedu pack #2|NULL|NULL|NULL|NULL|NULL|1980110.68|79
34: NULL|NULL|exportiedu pack #2|NULL|2000|NULL|NULL|NULL|1980110.68|79
35: NULL|NULL|exportiimporto #2|NULL|NULL|NULL|NULL|NULL|2054391.13|52
36: NULL|NULL|importoedu pack #1|NULL|NULL|NULL|NULL|NULL|2017011.96|59
37: NULL|NULL|importoedu pack #1|NULL|NULL|NULL|NULL|NULL|2017011.96|59
38: NULL|NULL|importoedu pack #1|NULL|2000|NULL|NULL|NULL|2017011.96|59
39: NULL|archery|NULL|NULL|NULL|NULL|NULL|NULL|1880685.97|94
40: NULL|athletic|NULL|NULL|NULL|NULL|NULL|NULL|3061542.94|28
41: NULL|athletic|NULL|NULL|NULL|NULL|NULL|NULL|5210675.89|12
42: NULL|baseball|NULL|NULL|NULL|NULL|NULL|NULL|1983963.50|74
43: NULL|basketball|NULL|NULL|NULL|NULL|NULL|NULL|1883886.93|93
44: NULL|business|NULL|NULL|NULL|NULL|NULL|NULL|2297093.85|39
45: NULL|classical|NULL|NULL|NULL|NULL|NULL|NULL|2917057.26|32
46: NULL|classical|edu packscholar #2|NULL|NULL|NULL|NULL|NULL|1916841.57|87
47: NULL|country|NULL|NULL|NULL|NULL|NULL|NULL|3163375.78|25
48: NULL|country|importoscholar #2|NULL|NULL|NULL|NULL|NULL|2148581.27|42
49: NULL|country|importoscholar #2|NULL|NULL|NULL|NULL|NULL|2148581.27|42
50: NULL|country|importoscholar #2|NULL|2000|NULL|NULL|NULL|2148581.27|42
51: NULL|customNULL|NULL|NULL|NULL|NULL|NULL|NULL|2075212.54|50
52: NULL|diamonds|NULL|NULL|NULL|NULL|NULL|NULL|1984577.91|73
53: NULL|dresses|NULL|NULL|NULL|NULL|NULL|NULL|2076617.17|48
54: NULL|dresses|NULL|NULL|NULL|NULL|NULL|NULL|3095482.08|26
55: NULL|dresses|NULL|NULL|NULL|NULL|NULL|NULL|6028869.45|9
56: NULL|dresses|NULL|NULL|2000|NULL|NULL|NULL|2076617.17|48
57: NULL|dresses|amalgamalg #2|NULL|NULL|NULL|NULL|NULL|2918521.76|31
58: NULL|earings|NULL|NULL|NULL|NULL|NULL|NULL|1988864.96|70
59: NULL|earings|NULL|NULL|NULL|NULL|NULL|NULL|1988864.96|70
60: NULL|earings|NULL|NULL|NULL|NULL|NULL|NULL|2003748.82|68
61: NULL|earings|NULL|NULL|2000|NULL|NULL|NULL|1988864.96|70
62: NULL|fragrances|NULL|NULL|NULL|NULL|NULL|NULL|1983149.33|78
63: NULL|fragrances|NULL|NULL|NULL|NULL|NULL|NULL|4894370.20|15
64: NULL|fragrances|importoamalg #1|NULL|NULL|NULL|NULL|NULL|1925135.95|86
65: NULL|history|NULL|NULL|NULL|NULL|NULL|NULL|1907089.25|89
66: NULL|history|NULL|NULL|NULL|NULL|NULL|NULL|1907089.25|89
67: NULL|history|NULL|NULL|NULL|NULL|NULL|NULL|2765482.40|37
68: NULL|history|NULL|NULL|2000|NULL|NULL|NULL|1907089.25|89
69: NULL|infants|NULL|NULL|NULL|NULL|NULL|NULL|2120615.81|46
70: NULL|loose stones|NULL|NULL|NULL|NULL|NULL|NULL|1886624.99|92
71: NULL|memory|NULL|NULL|NULL|NULL|NULL|NULL|2126286.51|45
72: NULL|mens|NULL|NULL|NULL|NULL|NULL|NULL|1966086.96|83
73: NULL|mens|NULL|NULL|NULL|NULL|NULL|NULL|2906981.21|35
74: NULL|newborn|NULL|NULL|NULL|NULL|NULL|NULL|2885464.55|36
75: NULL|newborn|NULL|NULL|NULL|NULL|NULL|NULL|3999371.31|19
76: NULL|pants|NULL|NULL|NULL|NULL|NULL|NULL|3899912.48|21
77: NULL|pants|exportiimporto #2|NULL|NULL|NULL|NULL|NULL|2024416.72|57
78: NULL|pants|exportiimporto #2|NULL|NULL|NULL|NULL|NULL|2960921.19|30
79: NULL|pants|exportiimporto #2|NULL|2000|NULL|NULL|NULL|2024416.72|57
80: NULL|pendants|NULL|NULL|NULL|NULL|NULL|NULL|1915760.89|88
81: NULL|popNULL|NULL|NULL|NULL|NULL|NULL|NULL|1866736.95|95
82: NULL|reference|NULL|NULL|NULL|NULL|NULL|NULL|1927519.76|85
83: NULL|rockNULL|NULL|NULL|NULL|NULL|NULL|NULL|2221752.90|41
84: NULL|rockNULL|NULL|NULL|NULL|NULL|NULL|NULL|2234031.88|40
85: NULL|shirts|NULL|NULL|NULL|NULL|NULL|NULL|2005748.24|66
86: NULL|shirts|NULL|NULL|NULL|NULL|NULL|NULL|2015374.72|62
87: NULL|shirts|NULL|NULL|NULL|NULL|NULL|NULL|3989095.69|20
88: NULL|shirts|NULL|NULL|2000|NULL|NULL|NULL|2005748.24|66
89: NULL|sports-apparel|NULL|NULL|NULL|NULL|NULL|NULL|2107528.93|47
90: NULL|sports-apparel|NULL|NULL|NULL|NULL|NULL|NULL|3055899.02|29
91: NULL|swimwear|NULL|NULL|NULL|NULL|NULL|NULL|2912705.57|34
92: NULL|swimwear|NULL|NULL|NULL|NULL|NULL|NULL|5986028.72|10
93: NULL|swimwear|edu packamalg #1|NULL|NULL|NULL|NULL|NULL|2027301.93|54
94: NULL|toddlers|NULL|NULL|NULL|NULL|NULL|NULL|2038611.36|53
95: NULL|toddlers|NULL|NULL|NULL|NULL|NULL|NULL|4013880.91|18
96: NULL|toddlers|exportiexporti #2|NULL|NULL|NULL|NULL|NULL|1975269.55|82
97: NULL|womens|NULL|NULL|NULL|NULL|NULL|NULL|1994814.32|69
98: NULL|womens|NULL|NULL|NULL|NULL|NULL|NULL|5074897.40|13
99: NULL|womens|amalgedu pack #2|NULL|NULL|NULL|NULL|NULL|2024659.79|55
100: NULL|womens|amalgedu pack #2|NULL|NULL|NULL|NULL|NULL|3080083.08|27
101: NULL|womens|amalgedu pack #2|NULL|2000|NULL|NULL|NULL|2024659.79|55
[end of extension/tpcds/dsdgen/answers/sf100/67.csv]
[start of src/common/extra_type_info.cpp]
1: #include "duckdb/common/extra_type_info.hpp"
2: #include "duckdb/common/serializer/deserializer.hpp"
3: #include "duckdb/common/enum_util.hpp"
4: #include "duckdb/common/numeric_utils.hpp"
5: #include "duckdb/common/serializer/serializer.hpp"
6: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
7: #include "duckdb/common/string_map_set.hpp"
8: 
9: namespace duckdb {
10: 
11: //===--------------------------------------------------------------------===//
12: // Extra Type Info
13: //===--------------------------------------------------------------------===//
14: ExtraTypeInfo::ExtraTypeInfo(ExtraTypeInfoType type) : type(type) {
15: }
16: ExtraTypeInfo::ExtraTypeInfo(ExtraTypeInfoType type, string alias) : type(type), alias(std::move(alias)) {
17: }
18: ExtraTypeInfo::~ExtraTypeInfo() {
19: }
20: shared_ptr<ExtraTypeInfo> ExtraTypeInfo::Copy() const {
21: 	return make_shared_ptr<ExtraTypeInfo>(*this);
22: }
23: 
24: static bool CompareModifiers(const vector<Value> &left, const vector<Value> &right) {
25: 	// Check if the common prefix of the properties is the same for both types
26: 	auto common_props = MinValue(left.size(), right.size());
27: 	for (idx_t i = 0; i < common_props; i++) {
28: 		if (left[i].type() != right[i].type()) {
29: 			return false;
30: 		}
31: 		// Special case for nulls:
32: 		// For type modifiers, NULL is equivalent to ANY
33: 		if (left[i].IsNull() || right[i].IsNull()) {
34: 			continue;
35: 		}
36: 		if (left[i] != right[i]) {
37: 			return false;
38: 		}
39: 	}
40: 	return true;
41: }
42: 
43: bool ExtraTypeInfo::Equals(ExtraTypeInfo *other_p) const {
44: 	if (type == ExtraTypeInfoType::INVALID_TYPE_INFO || type == ExtraTypeInfoType::STRING_TYPE_INFO ||
45: 	    type == ExtraTypeInfoType::GENERIC_TYPE_INFO) {
46: 		if (!other_p) {
47: 			if (!alias.empty()) {
48: 				return false;
49: 			}
50: 			//! We only need to compare aliases when both types have them in this case
51: 			return true;
52: 		}
53: 		if (alias != other_p->alias) {
54: 			return false;
55: 		}
56: 		if (!CompareModifiers(modifiers, other_p->modifiers)) {
57: 			return false;
58: 		}
59: 		return true;
60: 	}
61: 	if (!other_p) {
62: 		return false;
63: 	}
64: 	if (type != other_p->type) {
65: 		return false;
66: 	}
67: 	if (alias != other_p->alias) {
68: 		return false;
69: 	}
70: 	if (!CompareModifiers(modifiers, other_p->modifiers)) {
71: 		return false;
72: 	}
73: 	return EqualsInternal(other_p);
74: }
75: 
76: bool ExtraTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
77: 	// Do nothing
78: 	return true;
79: }
80: 
81: //===--------------------------------------------------------------------===//
82: // Decimal Type Info
83: //===--------------------------------------------------------------------===//
84: DecimalTypeInfo::DecimalTypeInfo() : ExtraTypeInfo(ExtraTypeInfoType::DECIMAL_TYPE_INFO) {
85: }
86: 
87: DecimalTypeInfo::DecimalTypeInfo(uint8_t width_p, uint8_t scale_p)
88:     : ExtraTypeInfo(ExtraTypeInfoType::DECIMAL_TYPE_INFO), width(width_p), scale(scale_p) {
89: 	D_ASSERT(width_p >= scale_p);
90: }
91: 
92: bool DecimalTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
93: 	auto &other = other_p->Cast<DecimalTypeInfo>();
94: 	return width == other.width && scale == other.scale;
95: }
96: 
97: shared_ptr<ExtraTypeInfo> DecimalTypeInfo::Copy() const {
98: 	return make_shared_ptr<DecimalTypeInfo>(*this);
99: }
100: 
101: //===--------------------------------------------------------------------===//
102: // String Type Info
103: //===--------------------------------------------------------------------===//
104: StringTypeInfo::StringTypeInfo() : ExtraTypeInfo(ExtraTypeInfoType::STRING_TYPE_INFO) {
105: }
106: 
107: StringTypeInfo::StringTypeInfo(string collation_p)
108:     : ExtraTypeInfo(ExtraTypeInfoType::STRING_TYPE_INFO), collation(std::move(collation_p)) {
109: }
110: 
111: bool StringTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
112: 	// collation info has no impact on equality
113: 	return true;
114: }
115: 
116: shared_ptr<ExtraTypeInfo> StringTypeInfo::Copy() const {
117: 	return make_shared_ptr<StringTypeInfo>(*this);
118: }
119: 
120: //===--------------------------------------------------------------------===//
121: // List Type Info
122: //===--------------------------------------------------------------------===//
123: ListTypeInfo::ListTypeInfo() : ExtraTypeInfo(ExtraTypeInfoType::LIST_TYPE_INFO) {
124: }
125: 
126: ListTypeInfo::ListTypeInfo(LogicalType child_type_p)
127:     : ExtraTypeInfo(ExtraTypeInfoType::LIST_TYPE_INFO), child_type(std::move(child_type_p)) {
128: }
129: 
130: bool ListTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
131: 	auto &other = other_p->Cast<ListTypeInfo>();
132: 	return child_type == other.child_type;
133: }
134: 
135: shared_ptr<ExtraTypeInfo> ListTypeInfo::Copy() const {
136: 	return make_shared_ptr<ListTypeInfo>(*this);
137: }
138: 
139: //===--------------------------------------------------------------------===//
140: // Struct Type Info
141: //===--------------------------------------------------------------------===//
142: StructTypeInfo::StructTypeInfo() : ExtraTypeInfo(ExtraTypeInfoType::STRUCT_TYPE_INFO) {
143: }
144: 
145: StructTypeInfo::StructTypeInfo(child_list_t<LogicalType> child_types_p)
146:     : ExtraTypeInfo(ExtraTypeInfoType::STRUCT_TYPE_INFO), child_types(std::move(child_types_p)) {
147: }
148: 
149: bool StructTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
150: 	auto &other = other_p->Cast<StructTypeInfo>();
151: 	return child_types == other.child_types;
152: }
153: 
154: shared_ptr<ExtraTypeInfo> StructTypeInfo::Copy() const {
155: 	return make_shared_ptr<StructTypeInfo>(*this);
156: }
157: 
158: //===--------------------------------------------------------------------===//
159: // Aggregate State Type Info
160: //===--------------------------------------------------------------------===//
161: AggregateStateTypeInfo::AggregateStateTypeInfo() : ExtraTypeInfo(ExtraTypeInfoType::AGGREGATE_STATE_TYPE_INFO) {
162: }
163: 
164: AggregateStateTypeInfo::AggregateStateTypeInfo(aggregate_state_t state_type_p)
165:     : ExtraTypeInfo(ExtraTypeInfoType::AGGREGATE_STATE_TYPE_INFO), state_type(std::move(state_type_p)) {
166: }
167: 
168: bool AggregateStateTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
169: 	auto &other = other_p->Cast<AggregateStateTypeInfo>();
170: 	return state_type.function_name == other.state_type.function_name &&
171: 	       state_type.return_type == other.state_type.return_type &&
172: 	       state_type.bound_argument_types == other.state_type.bound_argument_types;
173: }
174: 
175: shared_ptr<ExtraTypeInfo> AggregateStateTypeInfo::Copy() const {
176: 	return make_shared_ptr<AggregateStateTypeInfo>(*this);
177: }
178: 
179: //===--------------------------------------------------------------------===//
180: // User Type Info
181: //===--------------------------------------------------------------------===//
182: UserTypeInfo::UserTypeInfo() : ExtraTypeInfo(ExtraTypeInfoType::USER_TYPE_INFO) {
183: }
184: 
185: UserTypeInfo::UserTypeInfo(string name_p)
186:     : ExtraTypeInfo(ExtraTypeInfoType::USER_TYPE_INFO), user_type_name(std::move(name_p)) {
187: }
188: 
189: UserTypeInfo::UserTypeInfo(string name_p, vector<Value> modifiers_p)
190:     : ExtraTypeInfo(ExtraTypeInfoType::USER_TYPE_INFO), user_type_name(std::move(name_p)),
191:       user_type_modifiers(std::move(modifiers_p)) {
192: }
193: 
194: UserTypeInfo::UserTypeInfo(string catalog_p, string schema_p, string name_p, vector<Value> modifiers_p)
195:     : ExtraTypeInfo(ExtraTypeInfoType::USER_TYPE_INFO), catalog(std::move(catalog_p)), schema(std::move(schema_p)),
196:       user_type_name(std::move(name_p)), user_type_modifiers(std::move(modifiers_p)) {
197: }
198: 
199: bool UserTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
200: 	auto &other = other_p->Cast<UserTypeInfo>();
201: 	return other.user_type_name == user_type_name;
202: }
203: 
204: shared_ptr<ExtraTypeInfo> UserTypeInfo::Copy() const {
205: 	return make_shared_ptr<UserTypeInfo>(*this);
206: }
207: 
208: //===--------------------------------------------------------------------===//
209: // Enum Type Info
210: //===--------------------------------------------------------------------===//
211: PhysicalType EnumTypeInfo::DictType(idx_t size) {
212: 	if (size <= NumericLimits<uint8_t>::Maximum()) {
213: 		return PhysicalType::UINT8;
214: 	} else if (size <= NumericLimits<uint16_t>::Maximum()) {
215: 		return PhysicalType::UINT16;
216: 	} else if (size <= NumericLimits<uint32_t>::Maximum()) {
217: 		return PhysicalType::UINT32;
218: 	} else {
219: 		throw InternalException("Enum size must be lower than " + std::to_string(NumericLimits<uint32_t>::Maximum()));
220: 	}
221: }
222: 
223: template <class T>
224: struct EnumTypeInfoTemplated : public EnumTypeInfo {
225: 	explicit EnumTypeInfoTemplated(Vector &values_insert_order_p, idx_t size_p)
226: 	    : EnumTypeInfo(values_insert_order_p, size_p) {
227: 		D_ASSERT(values_insert_order_p.GetType().InternalType() == PhysicalType::VARCHAR);
228: 
229: 		UnifiedVectorFormat vdata;
230: 		values_insert_order.ToUnifiedFormat(size_p, vdata);
231: 
232: 		auto data = UnifiedVectorFormat::GetData<string_t>(vdata);
233: 		for (idx_t i = 0; i < size_p; i++) {
234: 			auto idx = vdata.sel->get_index(i);
235: 			if (!vdata.validity.RowIsValid(idx)) {
236: 				throw InternalException("Attempted to create ENUM type with NULL value");
237: 			}
238: 			if (values.count(data[idx]) > 0) {
239: 				throw InvalidInputException("Attempted to create ENUM type with duplicate value %s",
240: 				                            data[idx].GetString());
241: 			}
242: 			values[data[idx]] = UnsafeNumericCast<T>(i);
243: 		}
244: 	}
245: 
246: 	static shared_ptr<EnumTypeInfoTemplated> Deserialize(Deserializer &deserializer, uint32_t size) {
247: 		Vector values_insert_order(LogicalType::VARCHAR, size);
248: 		auto strings = FlatVector::GetData<string_t>(values_insert_order);
249: 
250: 		deserializer.ReadList(201, "values", [&](Deserializer::List &list, idx_t i) {
251: 			strings[i] = StringVector::AddStringOrBlob(values_insert_order, list.ReadElement<string>());
252: 		});
253: 		return make_shared_ptr<EnumTypeInfoTemplated>(values_insert_order, size);
254: 	}
255: 
256: 	const string_map_t<T> &GetValues() const {
257: 		return values;
258: 	}
259: 
260: 	EnumTypeInfoTemplated(const EnumTypeInfoTemplated &) = delete;
261: 	EnumTypeInfoTemplated &operator=(const EnumTypeInfoTemplated &) = delete;
262: 
263: private:
264: 	string_map_t<T> values;
265: };
266: 
267: EnumTypeInfo::EnumTypeInfo(Vector &values_insert_order_p, idx_t dict_size_p)
268:     : ExtraTypeInfo(ExtraTypeInfoType::ENUM_TYPE_INFO), values_insert_order(values_insert_order_p),
269:       dict_type(EnumDictType::VECTOR_DICT), dict_size(dict_size_p) {
270: }
271: 
272: const EnumDictType &EnumTypeInfo::GetEnumDictType() const {
273: 	return dict_type;
274: }
275: 
276: const Vector &EnumTypeInfo::GetValuesInsertOrder() const {
277: 	return values_insert_order;
278: }
279: 
280: const idx_t &EnumTypeInfo::GetDictSize() const {
281: 	return dict_size;
282: }
283: 
284: LogicalType EnumTypeInfo::CreateType(Vector &ordered_data, idx_t size) {
285: 	// Generate EnumTypeInfo
286: 	shared_ptr<ExtraTypeInfo> info;
287: 	auto enum_internal_type = EnumTypeInfo::DictType(size);
288: 	switch (enum_internal_type) {
289: 	case PhysicalType::UINT8:
290: 		info = make_shared_ptr<EnumTypeInfoTemplated<uint8_t>>(ordered_data, size);
291: 		break;
292: 	case PhysicalType::UINT16:
293: 		info = make_shared_ptr<EnumTypeInfoTemplated<uint16_t>>(ordered_data, size);
294: 		break;
295: 	case PhysicalType::UINT32:
296: 		info = make_shared_ptr<EnumTypeInfoTemplated<uint32_t>>(ordered_data, size);
297: 		break;
298: 	default:
299: 		throw InternalException("Invalid Physical Type for ENUMs");
300: 	}
301: 	// Generate Actual Enum Type
302: 	return LogicalType(LogicalTypeId::ENUM, info);
303: }
304: 
305: template <class T>
306: int64_t TemplatedGetPos(const string_map_t<T> &map, const string_t &key) {
307: 	auto it = map.find(key);
308: 	if (it == map.end()) {
309: 		return -1;
310: 	}
311: 	return it->second;
312: }
313: 
314: int64_t EnumType::GetPos(const LogicalType &type, const string_t &key) {
315: 	auto info = type.AuxInfo();
316: 	switch (type.InternalType()) {
317: 	case PhysicalType::UINT8:
318: 		return TemplatedGetPos(info->Cast<EnumTypeInfoTemplated<uint8_t>>().GetValues(), key);
319: 	case PhysicalType::UINT16:
320: 		return TemplatedGetPos(info->Cast<EnumTypeInfoTemplated<uint16_t>>().GetValues(), key);
321: 	case PhysicalType::UINT32:
322: 		return TemplatedGetPos(info->Cast<EnumTypeInfoTemplated<uint32_t>>().GetValues(), key);
323: 	default:
324: 		throw InternalException("ENUM can only have unsigned integers (except UINT64) as physical types");
325: 	}
326: }
327: 
328: string_t EnumType::GetString(const LogicalType &type, idx_t pos) {
329: 	D_ASSERT(pos < EnumType::GetSize(type));
330: 	return FlatVector::GetData<string_t>(EnumType::GetValuesInsertOrder(type))[pos];
331: }
332: 
333: shared_ptr<ExtraTypeInfo> EnumTypeInfo::Deserialize(Deserializer &deserializer) {
334: 	auto values_count = deserializer.ReadProperty<idx_t>(200, "values_count");
335: 	auto enum_internal_type = EnumTypeInfo::DictType(values_count);
336: 	switch (enum_internal_type) {
337: 	case PhysicalType::UINT8:
338: 		return EnumTypeInfoTemplated<uint8_t>::Deserialize(deserializer, NumericCast<uint32_t>(values_count));
339: 	case PhysicalType::UINT16:
340: 		return EnumTypeInfoTemplated<uint16_t>::Deserialize(deserializer, NumericCast<uint32_t>(values_count));
341: 	case PhysicalType::UINT32:
342: 		return EnumTypeInfoTemplated<uint32_t>::Deserialize(deserializer, NumericCast<uint32_t>(values_count));
343: 	default:
344: 		throw InternalException("Invalid Physical Type for ENUMs");
345: 	}
346: }
347: 
348: // Equalities are only used in enums with different catalog entries
349: bool EnumTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
350: 	auto &other = other_p->Cast<EnumTypeInfo>();
351: 	if (dict_type != other.dict_type) {
352: 		return false;
353: 	}
354: 	D_ASSERT(dict_type == EnumDictType::VECTOR_DICT);
355: 	// We must check if both enums have the same size
356: 	if (other.dict_size != dict_size) {
357: 		return false;
358: 	}
359: 	auto other_vector_ptr = FlatVector::GetData<string_t>(other.values_insert_order);
360: 	auto this_vector_ptr = FlatVector::GetData<string_t>(values_insert_order);
361: 
362: 	// Now we must check if all strings are the same
363: 	for (idx_t i = 0; i < dict_size; i++) {
364: 		if (!Equals::Operation(other_vector_ptr[i], this_vector_ptr[i])) {
365: 			return false;
366: 		}
367: 	}
368: 	return true;
369: }
370: 
371: void EnumTypeInfo::Serialize(Serializer &serializer) const {
372: 	ExtraTypeInfo::Serialize(serializer);
373: 
374: 	// Enums are special in that we serialize their values as a list instead of dumping the whole vector
375: 	auto strings = FlatVector::GetData<string_t>(values_insert_order);
376: 	serializer.WriteProperty(200, "values_count", dict_size);
377: 	serializer.WriteList(201, "values", dict_size,
378: 	                     [&](Serializer::List &list, idx_t i) { list.WriteElement(strings[i]); });
379: }
380: 
381: shared_ptr<ExtraTypeInfo> EnumTypeInfo::Copy() const {
382: 	Vector values_insert_order_copy(LogicalType::VARCHAR, false, false, 0);
383: 	values_insert_order_copy.Reference(values_insert_order);
384: 	return make_shared_ptr<EnumTypeInfo>(values_insert_order_copy, dict_size);
385: }
386: 
387: //===--------------------------------------------------------------------===//
388: // ArrayTypeInfo
389: //===--------------------------------------------------------------------===//
390: 
391: ArrayTypeInfo::ArrayTypeInfo(LogicalType child_type_p, uint32_t size_p)
392:     : ExtraTypeInfo(ExtraTypeInfoType::ARRAY_TYPE_INFO), child_type(std::move(child_type_p)), size(size_p) {
393: }
394: 
395: bool ArrayTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
396: 	auto &other = other_p->Cast<ArrayTypeInfo>();
397: 	return child_type == other.child_type && size == other.size;
398: }
399: 
400: shared_ptr<ExtraTypeInfo> ArrayTypeInfo::Copy() const {
401: 	return make_shared_ptr<ArrayTypeInfo>(*this);
402: }
403: 
404: //===--------------------------------------------------------------------===//
405: // Any Type Info
406: //===--------------------------------------------------------------------===//
407: AnyTypeInfo::AnyTypeInfo() : ExtraTypeInfo(ExtraTypeInfoType::ANY_TYPE_INFO) {
408: }
409: 
410: AnyTypeInfo::AnyTypeInfo(LogicalType target_type_p, idx_t cast_score_p)
411:     : ExtraTypeInfo(ExtraTypeInfoType::ANY_TYPE_INFO), target_type(std::move(target_type_p)), cast_score(cast_score_p) {
412: }
413: 
414: bool AnyTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
415: 	auto &other = other_p->Cast<AnyTypeInfo>();
416: 	return target_type == other.target_type && cast_score == other.cast_score;
417: }
418: 
419: shared_ptr<ExtraTypeInfo> AnyTypeInfo::Copy() const {
420: 	return make_shared_ptr<AnyTypeInfo>(*this);
421: }
422: 
423: //===--------------------------------------------------------------------===//
424: // Integer Literal Type Info
425: //===--------------------------------------------------------------------===//
426: IntegerLiteralTypeInfo::IntegerLiteralTypeInfo() : ExtraTypeInfo(ExtraTypeInfoType::INTEGER_LITERAL_TYPE_INFO) {
427: }
428: 
429: IntegerLiteralTypeInfo::IntegerLiteralTypeInfo(Value constant_value_p)
430:     : ExtraTypeInfo(ExtraTypeInfoType::INTEGER_LITERAL_TYPE_INFO), constant_value(std::move(constant_value_p)) {
431: 	if (constant_value.IsNull()) {
432: 		throw InternalException("Integer literal cannot be NULL");
433: 	}
434: }
435: 
436: bool IntegerLiteralTypeInfo::EqualsInternal(ExtraTypeInfo *other_p) const {
437: 	auto &other = other_p->Cast<IntegerLiteralTypeInfo>();
438: 	return constant_value == other.constant_value;
439: }
440: 
441: shared_ptr<ExtraTypeInfo> IntegerLiteralTypeInfo::Copy() const {
442: 	return make_shared_ptr<IntegerLiteralTypeInfo>(*this);
443: }
444: 
445: } // namespace duckdb
[end of src/common/extra_type_info.cpp]
[start of src/execution/operator/csv_scanner/sniffer/csv_sniffer.cpp]
1: #include "duckdb/execution/operator/csv_scanner/csv_sniffer.hpp"
2: #include "duckdb/common/types/value.hpp"
3: 
4: namespace duckdb {
5: 
6: CSVSniffer::CSVSniffer(CSVReaderOptions &options_p, shared_ptr<CSVBufferManager> buffer_manager_p,
7:                        CSVStateMachineCache &state_machine_cache_p, bool default_null_to_varchar_p)
8:     : state_machine_cache(state_machine_cache_p), options(options_p), buffer_manager(std::move(buffer_manager_p)),
9:       default_null_to_varchar(default_null_to_varchar_p) {
10: 	// Initialize Format Candidates
11: 	for (const auto &format_template : format_template_candidates) {
12: 		auto &logical_type = format_template.first;
13: 		best_format_candidates[logical_type].clear();
14: 	}
15: 	// Initialize max columns found to either 0 or however many were set
16: 	max_columns_found = set_columns.Size();
17: 	error_handler = make_shared_ptr<CSVErrorHandler>(options.ignore_errors.GetValue());
18: 	detection_error_handler = make_shared_ptr<CSVErrorHandler>(true);
19: 	if (options.columns_set) {
20: 		set_columns = SetColumns(&options.sql_type_list, &options.name_list);
21: 	}
22: }
23: 
24: bool SetColumns::IsSet() const {
25: 	if (!types) {
26: 		return false;
27: 	}
28: 	return !types->empty();
29: }
30: 
31: idx_t SetColumns::Size() const {
32: 	if (!types) {
33: 		return 0;
34: 	}
35: 	return types->size();
36: }
37: 
38: template <class T>
39: void MatchAndReplace(CSVOption<T> &original, CSVOption<T> &sniffed, const string &name, string &error) {
40: 	if (original.IsSetByUser()) {
41: 		// We verify that the user input matches the sniffed value
42: 		if (original != sniffed) {
43: 			error += "CSV Sniffer: Sniffer detected value different than the user input for the " + name;
44: 			error += " options \n Set: " + original.FormatValue() + " Sniffed: " + sniffed.FormatValue() + "\n";
45: 		}
46: 	} else {
47: 		// We replace the value of original with the sniffed value
48: 		original.Set(sniffed.GetValue(), false);
49: 	}
50: }
51: void MatchAndRepaceUserSetVariables(DialectOptions &original, DialectOptions &sniffed, string &error, bool found_date,
52:                                     bool found_timestamp) {
53: 	MatchAndReplace(original.header, sniffed.header, "Header", error);
54: 	if (sniffed.state_machine_options.new_line.GetValue() != NewLineIdentifier::NOT_SET) {
55: 		// Is sniffed line is not set (e.g., single-line file) , we don't try to replace and match.
56: 		MatchAndReplace(original.state_machine_options.new_line, sniffed.state_machine_options.new_line, "New Line",
57: 		                error);
58: 	}
59: 	MatchAndReplace(original.skip_rows, sniffed.skip_rows, "Skip Rows", error);
60: 	MatchAndReplace(original.state_machine_options.delimiter, sniffed.state_machine_options.delimiter, "Delimiter",
61: 	                error);
62: 	MatchAndReplace(original.state_machine_options.quote, sniffed.state_machine_options.quote, "Quote", error);
63: 	MatchAndReplace(original.state_machine_options.escape, sniffed.state_machine_options.escape, "Escape", error);
64: 	MatchAndReplace(original.state_machine_options.comment, sniffed.state_machine_options.comment, "Comment", error);
65: 	if (found_date) {
66: 		MatchAndReplace(original.date_format[LogicalTypeId::DATE], sniffed.date_format[LogicalTypeId::DATE],
67: 		                "Date Format", error);
68: 	}
69: 	if (found_timestamp) {
70: 		MatchAndReplace(original.date_format[LogicalTypeId::TIMESTAMP], sniffed.date_format[LogicalTypeId::TIMESTAMP],
71: 		                "Timestamp Format", error);
72: 	}
73: }
74: // Set the CSV Options in the reference
75: void CSVSniffer::SetResultOptions() {
76: 	bool found_date = false;
77: 	bool found_timestamp = false;
78: 	for (auto &type : detected_types) {
79: 		if (type == LogicalType::DATE) {
80: 			found_date = true;
81: 		} else if (type == LogicalType::TIMESTAMP) {
82: 			found_timestamp = true;
83: 		}
84: 	}
85: 	MatchAndRepaceUserSetVariables(options.dialect_options, best_candidate->GetStateMachine().dialect_options,
86: 	                               options.sniffer_user_mismatch_error, found_date, found_timestamp);
87: 	options.dialect_options.num_cols = best_candidate->GetStateMachine().dialect_options.num_cols;
88: 	options.dialect_options.rows_until_header = best_candidate->GetStateMachine().dialect_options.rows_until_header;
89: }
90: 
91: SnifferResult CSVSniffer::MinimalSniff() {
92: 	if (set_columns.IsSet()) {
93: 		// Nothing to see here
94: 		return SnifferResult(*set_columns.types, *set_columns.names);
95: 	}
96: 	// Return Types detected
97: 	vector<LogicalType> return_types;
98: 	// Column Names detected
99: 	vector<string> names;
100: 
101: 	buffer_manager->sniffing = true;
102: 	constexpr idx_t result_size = 2;
103: 
104: 	auto state_machine =
105: 	    make_shared_ptr<CSVStateMachine>(options, options.dialect_options.state_machine_options, state_machine_cache);
106: 	ColumnCountScanner count_scanner(buffer_manager, state_machine, error_handler, result_size);
107: 	auto &sniffed_column_counts = count_scanner.ParseChunk();
108: 	if (sniffed_column_counts.result_position == 0) {
109: 		return {{}, {}};
110: 	}
111: 
112: 	state_machine->dialect_options.num_cols = sniffed_column_counts[0].number_of_columns;
113: 	options.dialect_options.num_cols = sniffed_column_counts[0].number_of_columns;
114: 
115: 	// First figure out the number of columns on this configuration
116: 	auto scanner = count_scanner.UpgradeToStringValueScanner();
117: 	// Parse chunk and read csv with info candidate
118: 	auto &data_chunk = scanner->ParseChunk().ToChunk();
119: 	idx_t start_row = 0;
120: 	if (sniffed_column_counts.result_position == 2) {
121: 		// If equal to two, we will only use the second row for type checking
122: 		start_row = 1;
123: 	}
124: 
125: 	// Gather Types
126: 	for (idx_t i = 0; i < state_machine->dialect_options.num_cols; i++) {
127: 		best_sql_types_candidates_per_column_idx[i] = state_machine->options.auto_type_candidates;
128: 	}
129: 	SniffTypes(data_chunk, *state_machine, best_sql_types_candidates_per_column_idx, start_row);
130: 
131: 	// Possibly Gather Header
132: 	vector<HeaderValue> potential_header;
133: 	if (start_row != 0) {
134: 		for (idx_t col_idx = 0; col_idx < data_chunk.ColumnCount(); col_idx++) {
135: 			auto &cur_vector = data_chunk.data[col_idx];
136: 			auto vector_data = FlatVector::GetData<string_t>(cur_vector);
137: 			auto &validity = FlatVector::Validity(cur_vector);
138: 			HeaderValue val;
139: 			if (validity.RowIsValid(0)) {
140: 				val = HeaderValue(vector_data[0]);
141: 			}
142: 			potential_header.emplace_back(val);
143: 		}
144: 	}
145: 	names = DetectHeaderInternal(buffer_manager->context, potential_header, *state_machine, set_columns,
146: 	                             best_sql_types_candidates_per_column_idx, options, *error_handler);
147: 
148: 	for (idx_t column_idx = 0; column_idx < best_sql_types_candidates_per_column_idx.size(); column_idx++) {
149: 		LogicalType d_type = best_sql_types_candidates_per_column_idx[column_idx].back();
150: 		if (best_sql_types_candidates_per_column_idx[column_idx].size() == options.auto_type_candidates.size()) {
151: 			d_type = LogicalType::VARCHAR;
152: 		}
153: 		detected_types.push_back(d_type);
154: 	}
155: 
156: 	return {detected_types, names};
157: }
158: 
159: SnifferResult CSVSniffer::AdaptiveSniff(CSVSchema &file_schema) {
160: 	auto min_sniff_res = MinimalSniff();
161: 	bool run_full = error_handler->AnyErrors() || detection_error_handler->AnyErrors();
162: 	// Check if we are happy with the result or if we need to do more sniffing
163: 	if (!error_handler->AnyErrors() && !detection_error_handler->AnyErrors()) {
164: 		// If we got no errors, we also run full if schemas do not match.
165: 		if (!set_columns.IsSet() && !options.file_options.AnySet()) {
166: 			string error;
167: 			run_full =
168: 			    !file_schema.SchemasMatch(error, min_sniff_res.names, min_sniff_res.return_types, options.file_path);
169: 		}
170: 	}
171: 	if (run_full) {
172: 		// We run full sniffer
173: 		auto full_sniffer = SniffCSV();
174: 		if (!set_columns.IsSet() && !options.file_options.AnySet()) {
175: 			string error;
176: 			if (!file_schema.SchemasMatch(error, full_sniffer.names, full_sniffer.return_types, options.file_path) &&
177: 			    !options.ignore_errors.GetValue()) {
178: 				throw InvalidInputException(error);
179: 			}
180: 		}
181: 		return full_sniffer;
182: 	}
183: 	return min_sniff_res;
184: }
185: SnifferResult CSVSniffer::SniffCSV(bool force_match) {
186: 	buffer_manager->sniffing = true;
187: 	// 1. Dialect Detection
188: 	DetectDialect();
189: 	// 2. Type Detection
190: 	DetectTypes();
191: 	// 3. Type Refinement
192: 	RefineTypes();
193: 	// 4. Header Detection
194: 	DetectHeader();
195: 	// 5. Type Replacement
196: 	ReplaceTypes();
197: 
198: 	// We reset the buffer for compressed files
199: 	// This is done because we can't easily seek on compressed files, if a buffer goes out of scope we must read from
200: 	// the start
201: 	if (buffer_manager->file_handle->compression_type != FileCompressionType::UNCOMPRESSED) {
202: 		buffer_manager->ResetBufferManager();
203: 	}
204: 	buffer_manager->sniffing = false;
205: 	if (!best_candidate->error_handler->errors.empty() && !options.ignore_errors.GetValue()) {
206: 		for (auto &error_vector : best_candidate->error_handler->errors) {
207: 			for (auto &error : error_vector.second) {
208: 				if (error.type == MAXIMUM_LINE_SIZE) {
209: 					// If it's a maximum line size error, we can do it now.
210: 					error_handler->Error(error);
211: 				}
212: 			}
213: 		}
214: 	}
215: 	D_ASSERT(best_sql_types_candidates_per_column_idx.size() == names.size());
216: 	// We are done, Set the CSV Options in the reference. Construct and return the result.
217: 	SetResultOptions();
218: 	options.auto_detect = true;
219: 	// Check if everything matches
220: 	auto &error = options.sniffer_user_mismatch_error;
221: 	if (set_columns.IsSet()) {
222: 		bool match = true;
223: 		// Columns and their types were set, let's validate they match
224: 		if (options.dialect_options.header.GetValue()) {
225: 			// If the header exists it should match
226: 			string header_error = "The Column names set by the user do not match the ones found by the sniffer. \n";
227: 			auto &set_names = *set_columns.names;
228: 			if (set_names.size() == names.size()) {
229: 				for (idx_t i = 0; i < set_columns.Size(); i++) {
230: 					if (set_names[i] != names[i]) {
231: 						header_error += "Column at position: " + to_string(i) + " Set name: " + set_names[i] +
232: 						                " Sniffed Name: " + names[i] + "\n";
233: 						match = false;
234: 					}
235: 				}
236: 			}
237: 
238: 			if (!match) {
239: 				error += header_error;
240: 			}
241: 		}
242: 		match = true;
243: 		string type_error = "The Column types set by the user do not match the ones found by the sniffer. \n";
244: 		auto &set_types = *set_columns.types;
245: 		if (detected_types.size() == set_columns.Size()) {
246: 			for (idx_t i = 0; i < set_columns.Size(); i++) {
247: 				if (set_types[i] != detected_types[i]) {
248: 					type_error += "Column at position: " + to_string(i) + " Set type: " + set_types[i].ToString() +
249: 					              " Sniffed type: " + detected_types[i].ToString() + "\n";
250: 					detected_types[i] = set_types[i];
251: 					manually_set[i] = true;
252: 					match = false;
253: 				}
254: 			}
255: 		}
256: 
257: 		if (!match) {
258: 			error += type_error;
259: 		}
260: 
261: 		if (!error.empty() && force_match) {
262: 			throw InvalidInputException(error);
263: 		}
264: 		options.was_type_manually_set = manually_set;
265: 	}
266: 	if (!error.empty() && force_match) {
267: 		throw InvalidInputException(error);
268: 	}
269: 	options.was_type_manually_set = manually_set;
270: 	if (set_columns.IsSet()) {
271: 		return SnifferResult(*set_columns.types, *set_columns.names);
272: 	}
273: 	return SnifferResult(detected_types, names);
274: }
275: 
276: } // namespace duckdb
[end of src/execution/operator/csv_scanner/sniffer/csv_sniffer.cpp]
[start of src/execution/operator/csv_scanner/sniffer/header_detection.cpp]
1: #include "duckdb/common/types/cast_helpers.hpp"
2: #include "duckdb/execution/operator/csv_scanner/csv_sniffer.hpp"
3: #include "duckdb/execution/operator/csv_scanner/csv_reader_options.hpp"
4: 
5: #include "utf8proc.hpp"
6: 
7: namespace duckdb {
8: // Helper function to generate column names
9: static string GenerateColumnName(const idx_t total_cols, const idx_t col_number, const string &prefix = "column") {
10: 	auto max_digits = NumericHelper::UnsignedLength(total_cols - 1);
11: 	auto digits = NumericHelper::UnsignedLength(col_number);
12: 	string leading_zeros = string(NumericCast<idx_t>(max_digits - digits), '0');
13: 	string value = to_string(col_number);
14: 	return string(prefix + leading_zeros + value);
15: }
16: 
17: // Helper function for UTF-8 aware space trimming
18: static string TrimWhitespace(const string &col_name) {
19: 	utf8proc_int32_t codepoint;
20: 	auto str = reinterpret_cast<const utf8proc_uint8_t *>(col_name.c_str());
21: 	idx_t size = col_name.size();
22: 	// Find the first character that is not left trimmed
23: 	idx_t begin = 0;
24: 	while (begin < size) {
25: 		auto bytes = utf8proc_iterate(str + begin, NumericCast<utf8proc_ssize_t>(size - begin), &codepoint);
26: 		D_ASSERT(bytes > 0);
27: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
28: 			break;
29: 		}
30: 		begin += NumericCast<idx_t>(bytes);
31: 	}
32: 
33: 	// Find the last character that is not right trimmed
34: 	idx_t end = begin;
35: 	for (auto next = begin; next < col_name.size();) {
36: 		auto bytes = utf8proc_iterate(str + next, NumericCast<utf8proc_ssize_t>(size - next), &codepoint);
37: 		D_ASSERT(bytes > 0);
38: 		next += NumericCast<idx_t>(bytes);
39: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
40: 			end = next;
41: 		}
42: 	}
43: 
44: 	// return the trimmed string
45: 	return col_name.substr(begin, end - begin);
46: }
47: 
48: static string NormalizeColumnName(const string &col_name) {
49: 	// normalize UTF8 characters to NFKD
50: 	auto nfkd = utf8proc_NFKD(reinterpret_cast<const utf8proc_uint8_t *>(col_name.c_str()),
51: 	                          NumericCast<utf8proc_ssize_t>(col_name.size()));
52: 	const string col_name_nfkd = string(const_char_ptr_cast(nfkd), strlen(const_char_ptr_cast(nfkd)));
53: 	free(nfkd);
54: 
55: 	// only keep ASCII characters 0-9 a-z A-Z and replace spaces with regular whitespace
56: 	string col_name_ascii = "";
57: 	for (idx_t i = 0; i < col_name_nfkd.size(); i++) {
58: 		if (col_name_nfkd[i] == '_' || (col_name_nfkd[i] >= '0' && col_name_nfkd[i] <= '9') ||
59: 		    (col_name_nfkd[i] >= 'A' && col_name_nfkd[i] <= 'Z') ||
60: 		    (col_name_nfkd[i] >= 'a' && col_name_nfkd[i] <= 'z')) {
61: 			col_name_ascii += col_name_nfkd[i];
62: 		} else if (StringUtil::CharacterIsSpace(col_name_nfkd[i])) {
63: 			col_name_ascii += " ";
64: 		}
65: 	}
66: 
67: 	// trim whitespace and replace remaining whitespace by _
68: 	string col_name_trimmed = TrimWhitespace(col_name_ascii);
69: 	string col_name_cleaned = "";
70: 	bool in_whitespace = false;
71: 	for (idx_t i = 0; i < col_name_trimmed.size(); i++) {
72: 		if (col_name_trimmed[i] == ' ') {
73: 			if (!in_whitespace) {
74: 				col_name_cleaned += "_";
75: 				in_whitespace = true;
76: 			}
77: 		} else {
78: 			col_name_cleaned += col_name_trimmed[i];
79: 			in_whitespace = false;
80: 		}
81: 	}
82: 
83: 	// don't leave string empty; if not empty, make lowercase
84: 	if (col_name_cleaned.empty()) {
85: 		col_name_cleaned = "_";
86: 	} else {
87: 		col_name_cleaned = StringUtil::Lower(col_name_cleaned);
88: 	}
89: 
90: 	// prepend _ if name starts with a digit or is a reserved keyword
91: 	auto keyword = KeywordHelper::KeywordCategoryType(col_name_cleaned);
92: 	if (keyword == KeywordCategory::KEYWORD_TYPE_FUNC || keyword == KeywordCategory::KEYWORD_RESERVED ||
93: 	    (col_name_cleaned[0] >= '0' && col_name_cleaned[0] <= '9')) {
94: 		col_name_cleaned = "_" + col_name_cleaned;
95: 	}
96: 	return col_name_cleaned;
97: }
98: 
99: // If our columns were set by the user, we verify if their names match with the first row
100: bool CSVSniffer::DetectHeaderWithSetColumn(ClientContext &context, vector<HeaderValue> &best_header_row,
101:                                            const SetColumns &set_columns, CSVReaderOptions &options) {
102: 	bool has_header = true;
103: 
104: 	std::ostringstream error;
105: 	// User set the names, we must check if they match the first row
106: 	// We do a +1 to check for situations where the csv file has an extra all null column
107: 	if (set_columns.Size() != best_header_row.size() && set_columns.Size() + 1 != best_header_row.size()) {
108: 		return false;
109: 	}
110: 
111: 	// Let's do a match-aroo
112: 	for (idx_t i = 0; i < set_columns.Size(); i++) {
113: 		if (best_header_row[i].IsNull()) {
114: 			return false;
115: 		}
116: 		if (best_header_row[i].value != (*set_columns.names)[i]) {
117: 			error << "Header Mismatch at position:" << i << "\n";
118: 			error << "Expected Name: \"" << (*set_columns.names)[i] << "\".";
119: 			error << "Actual Name: \"" << best_header_row[i].value << "\"."
120: 			      << "\n";
121: 			has_header = false;
122: 			break;
123: 		}
124: 	}
125: 
126: 	if (!has_header) {
127: 		bool all_varchar = true;
128: 		bool first_row_consistent = true;
129: 		// We verify if the types are consistent
130: 		for (idx_t col = 0; col < set_columns.Size(); col++) {
131: 			// try cast to sql_type of column
132: 			const auto &sql_type = (*set_columns.types)[col];
133: 			if (sql_type != LogicalType::VARCHAR) {
134: 				all_varchar = false;
135: 				if (!CSVSniffer::CanYouCastIt(context, best_header_row[col].value, sql_type, options.dialect_options,
136: 				                              best_header_row[col].IsNull(), options.decimal_separator[0])) {
137: 					first_row_consistent = false;
138: 				}
139: 			}
140: 		}
141: 		if (!first_row_consistent) {
142: 			options.sniffer_user_mismatch_error += error.str();
143: 		}
144: 		if (all_varchar) {
145: 			return true;
146: 		}
147: 		return !first_row_consistent;
148: 	}
149: 	return has_header;
150: }
151: 
152: bool EmptyHeader(const string &col_name, bool is_null, bool normalize) {
153: 	if (col_name.empty() || is_null) {
154: 		return true;
155: 	}
156: 	if (normalize) {
157: 		// normalize has special logic to trim white spaces and generate names
158: 		return false;
159: 	}
160: 	// check if it's all white spaces
161: 	for (auto &c : col_name) {
162: 		if (!StringUtil::CharacterIsSpace(c)) {
163: 			return false;
164: 		}
165: 	}
166: 	// if we are not normalizing the name and is all white spaces, then we generate a name
167: 	return true;
168: }
169: 
170: vector<string>
171: CSVSniffer::DetectHeaderInternal(ClientContext &context, vector<HeaderValue> &best_header_row,
172:                                  CSVStateMachine &state_machine, const SetColumns &set_columns,
173:                                  unordered_map<idx_t, vector<LogicalType>> &best_sql_types_candidates_per_column_idx,
174:                                  CSVReaderOptions &options, CSVErrorHandler &error_handler) {
175: 	vector<string> detected_names;
176: 	auto &dialect_options = state_machine.dialect_options;
177: 	if (best_header_row.empty()) {
178: 		dialect_options.header = false;
179: 		for (idx_t col = 0; col < dialect_options.num_cols; col++) {
180: 			detected_names.push_back(GenerateColumnName(dialect_options.num_cols, col));
181: 		}
182: 		// If the user provided names, we must replace our header with the user provided names
183: 		if (!options.columns_set) {
184: 			for (idx_t i = 0; i < MinValue<idx_t>(best_header_row.size(), options.name_list.size()); i++) {
185: 				detected_names[i] = options.name_list[i];
186: 			}
187: 		}
188: 		return detected_names;
189: 	}
190: 	// information for header detection
191: 	// check if header row is all null and/or consistent with detected column data types
192: 	// If null-padding is not allowed and there is a mismatch between our header candidate and the number of columns
193: 	// We can't detect the dialect/type options properly
194: 	if (!options.null_padding && best_sql_types_candidates_per_column_idx.size() != best_header_row.size()) {
195: 		auto error =
196: 		    CSVError::HeaderSniffingError(options, best_header_row, best_sql_types_candidates_per_column_idx.size(),
197: 		                                  state_machine.dialect_options.state_machine_options.delimiter.GetValue());
198: 		error_handler.Error(error);
199: 	}
200: 	bool has_header;
201: 
202: 	if (set_columns.IsSet()) {
203: 		has_header = DetectHeaderWithSetColumn(context, best_header_row, set_columns, options);
204: 	} else {
205: 		bool first_row_consistent = true;
206: 		bool all_varchar = true;
207: 		bool first_row_nulls = true;
208: 		for (idx_t col = 0; col < best_header_row.size(); col++) {
209: 			if (!best_header_row[col].IsNull()) {
210: 				first_row_nulls = false;
211: 			}
212: 			// try cast to sql_type of column
213: 			const auto &sql_type = best_sql_types_candidates_per_column_idx[col].back();
214: 			if (sql_type != LogicalType::VARCHAR) {
215: 				all_varchar = false;
216: 				if (!CanYouCastIt(context, best_header_row[col].value, sql_type, dialect_options,
217: 				                  best_header_row[col].IsNull(), options.decimal_separator[0])) {
218: 					first_row_consistent = false;
219: 				}
220: 			}
221: 		}
222: 		// Our header is only false if types are not all varchar, and rows are consistent
223: 		if (all_varchar || first_row_nulls) {
224: 			has_header = true;
225: 		} else {
226: 			has_header = !first_row_consistent;
227: 		}
228: 	}
229: 
230: 	if (options.dialect_options.header.IsSetByUser()) {
231: 		// Header is defined by user, use that.
232: 		has_header = options.dialect_options.header.GetValue();
233: 	}
234: 	// update parser info, and read, generate & set col_names based on previous findings
235: 	if (has_header) {
236: 		dialect_options.header = true;
237: 		if (options.null_padding && !options.dialect_options.skip_rows.IsSetByUser()) {
238: 			if (dialect_options.skip_rows.GetValue() > 0) {
239: 				dialect_options.skip_rows = dialect_options.skip_rows.GetValue() - 1;
240: 			}
241: 		}
242: 		case_insensitive_map_t<idx_t> name_collision_count;
243: 
244: 		// get header names from CSV
245: 		for (idx_t col = 0; col < best_header_row.size(); col++) {
246: 			string &col_name = best_header_row[col].value;
247: 
248: 			// generate name if field is empty
249: 			if (EmptyHeader(col_name, best_header_row[col].is_null, options.normalize_names)) {
250: 				col_name = GenerateColumnName(dialect_options.num_cols, col);
251: 			}
252: 
253: 			// normalize names or at least trim whitespace
254: 			if (options.normalize_names) {
255: 				col_name = NormalizeColumnName(col_name);
256: 			} else {
257: 				col_name = TrimWhitespace(col_name);
258: 			}
259: 
260: 			// avoid duplicate header names
261: 			while (name_collision_count.find(col_name) != name_collision_count.end()) {
262: 				name_collision_count[col_name] += 1;
263: 				col_name = col_name + "_" + to_string(name_collision_count[col_name]);
264: 			}
265: 			detected_names.push_back(col_name);
266: 			name_collision_count[col_name] = 0;
267: 		}
268: 		if (best_header_row.size() < dialect_options.num_cols && options.null_padding) {
269: 			for (idx_t col = best_header_row.size(); col < dialect_options.num_cols; col++) {
270: 				detected_names.push_back(GenerateColumnName(dialect_options.num_cols, col));
271: 			}
272: 		} else if (best_header_row.size() < dialect_options.num_cols) {
273: 			throw InternalException("Detected header has number of columns inferior to dialect detection");
274: 		}
275: 
276: 	} else {
277: 		dialect_options.header = false;
278: 		for (idx_t col = 0; col < dialect_options.num_cols; col++) {
279: 			detected_names.push_back(GenerateColumnName(dialect_options.num_cols, col));
280: 		}
281: 	}
282: 
283: 	// If the user provided names, we must replace our header with the user provided names
284: 	if (!options.columns_set) {
285: 		for (idx_t i = 0; i < MinValue<idx_t>(detected_names.size(), options.name_list.size()); i++) {
286: 			detected_names[i] = options.name_list[i];
287: 		}
288: 	}
289: 	return detected_names;
290: }
291: void CSVSniffer::DetectHeader() {
292: 	auto &sniffer_state_machine = best_candidate->GetStateMachine();
293: 	names = DetectHeaderInternal(buffer_manager->context, best_header_row, sniffer_state_machine, set_columns,
294: 	                             best_sql_types_candidates_per_column_idx, options, *error_handler);
295: }
296: } // namespace duckdb
[end of src/execution/operator/csv_scanner/sniffer/header_detection.cpp]
[start of src/execution/operator/csv_scanner/util/csv_reader_options.cpp]
1: #include "duckdb/execution/operator/csv_scanner/csv_reader_options.hpp"
2: #include "duckdb/common/bind_helpers.hpp"
3: #include "duckdb/common/vector_size.hpp"
4: #include "duckdb/common/string_util.hpp"
5: #include "duckdb/common/enum_util.hpp"
6: #include "duckdb/common/multi_file_reader.hpp"
7: 
8: namespace duckdb {
9: 
10: static bool ParseBoolean(const Value &value, const string &loption);
11: 
12: static bool ParseBoolean(const vector<Value> &set, const string &loption) {
13: 	if (set.empty()) {
14: 		// no option specified: default to true
15: 		return true;
16: 	}
17: 	if (set.size() > 1) {
18: 		throw BinderException("\"%s\" expects a single argument as a boolean value (e.g. TRUE or 1)", loption);
19: 	}
20: 	return ParseBoolean(set[0], loption);
21: }
22: 
23: static bool ParseBoolean(const Value &value, const string &loption) {
24: 
25: 	if (value.type().id() == LogicalTypeId::LIST) {
26: 		auto &children = ListValue::GetChildren(value);
27: 		return ParseBoolean(children, loption);
28: 	}
29: 	if (value.type() == LogicalType::FLOAT || value.type() == LogicalType::DOUBLE ||
30: 	    value.type().id() == LogicalTypeId::DECIMAL) {
31: 		throw BinderException("\"%s\" expects a boolean value (e.g. TRUE or 1)", loption);
32: 	}
33: 	return BooleanValue::Get(value.DefaultCastAs(LogicalType::BOOLEAN));
34: }
35: 
36: static string ParseString(const Value &value, const string &loption) {
37: 	if (value.IsNull()) {
38: 		return string();
39: 	}
40: 	if (value.type().id() == LogicalTypeId::LIST) {
41: 		auto &children = ListValue::GetChildren(value);
42: 		if (children.size() != 1) {
43: 			throw BinderException("\"%s\" expects a single argument as a string value", loption);
44: 		}
45: 		return ParseString(children[0], loption);
46: 	}
47: 	if (value.type().id() != LogicalTypeId::VARCHAR) {
48: 		throw BinderException("\"%s\" expects a string argument!", loption);
49: 	}
50: 	return value.GetValue<string>();
51: }
52: 
53: static int64_t ParseInteger(const Value &value, const string &loption) {
54: 	if (value.type().id() == LogicalTypeId::LIST) {
55: 		auto &children = ListValue::GetChildren(value);
56: 		if (children.size() != 1) {
57: 			// no option specified or multiple options specified
58: 			throw BinderException("\"%s\" expects a single argument as an integer value", loption);
59: 		}
60: 		return ParseInteger(children[0], loption);
61: 	}
62: 	return value.GetValue<int64_t>();
63: }
64: 
65: bool CSVReaderOptions::GetHeader() const {
66: 	return this->dialect_options.header.GetValue();
67: }
68: 
69: void CSVReaderOptions::SetHeader(bool input) {
70: 	this->dialect_options.header.Set(input);
71: }
72: 
73: void CSVReaderOptions::SetCompression(const string &compression_p) {
74: 	this->compression = FileCompressionTypeFromString(compression_p);
75: }
76: 
77: string CSVReaderOptions::GetEscape() const {
78: 	return std::string(1, this->dialect_options.state_machine_options.escape.GetValue());
79: }
80: 
81: void CSVReaderOptions::SetEscape(const string &input) {
82: 	auto escape_str = input;
83: 	if (escape_str.size() > 1) {
84: 		throw InvalidInputException("The escape option cannot exceed a size of 1 byte.");
85: 	}
86: 	if (escape_str.empty()) {
87: 		escape_str = string("\0", 1);
88: 	}
89: 	this->dialect_options.state_machine_options.escape.Set(escape_str[0]);
90: }
91: 
92: idx_t CSVReaderOptions::GetSkipRows() const {
93: 	return NumericCast<idx_t>(this->dialect_options.skip_rows.GetValue());
94: }
95: 
96: void CSVReaderOptions::SetSkipRows(int64_t skip_rows) {
97: 	if (skip_rows < 0) {
98: 		throw InvalidInputException("skip_rows option from read_csv scanner, must be equal or higher than 0");
99: 	}
100: 	dialect_options.skip_rows.Set(NumericCast<idx_t>(skip_rows));
101: }
102: 
103: string CSVReaderOptions::GetDelimiter() const {
104: 	return std::string(1, this->dialect_options.state_machine_options.delimiter.GetValue());
105: }
106: 
107: void CSVReaderOptions::SetDelimiter(const string &input) {
108: 	auto delim_str = StringUtil::Replace(input, "\\t", "\t");
109: 	if (delim_str.size() > 1) {
110: 		throw InvalidInputException("The delimiter option cannot exceed a size of 1 byte.");
111: 	}
112: 	if (input.empty()) {
113: 		delim_str = string("\0", 1);
114: 	}
115: 	this->dialect_options.state_machine_options.delimiter.Set(delim_str[0]);
116: }
117: 
118: string CSVReaderOptions::GetQuote() const {
119: 	return std::string(1, this->dialect_options.state_machine_options.quote.GetValue());
120: }
121: 
122: void CSVReaderOptions::SetQuote(const string &quote_p) {
123: 	auto quote_str = quote_p;
124: 	if (quote_str.size() > 1) {
125: 		throw InvalidInputException("The quote option cannot exceed a size of 1 byte.");
126: 	}
127: 	if (quote_str.empty()) {
128: 		quote_str = string("\0", 1);
129: 	}
130: 	this->dialect_options.state_machine_options.quote.Set(quote_str[0]);
131: }
132: 
133: string CSVReaderOptions::GetComment() const {
134: 	return std::string(1, this->dialect_options.state_machine_options.comment.GetValue());
135: }
136: 
137: void CSVReaderOptions::SetComment(const string &comment_p) {
138: 	auto comment_str = comment_p;
139: 	if (comment_str.size() > 1) {
140: 		throw InvalidInputException("The comment option cannot exceed a size of 1 byte.");
141: 	}
142: 	if (comment_str.empty()) {
143: 		comment_str = string("\0", 1);
144: 	}
145: 	this->dialect_options.state_machine_options.comment.Set(comment_str[0]);
146: }
147: 
148: string CSVReaderOptions::GetNewline() const {
149: 	switch (dialect_options.state_machine_options.new_line.GetValue()) {
150: 	case NewLineIdentifier::CARRY_ON:
151: 		return "\\r\\n";
152: 	case NewLineIdentifier::SINGLE_R:
153: 		return "\\r";
154: 	case NewLineIdentifier::SINGLE_N:
155: 		return "\\n";
156: 	case NewLineIdentifier::NOT_SET:
157: 		return "";
158: 	default:
159: 		throw NotImplementedException("New line type not supported");
160: 	}
161: }
162: 
163: void CSVReaderOptions::SetNewline(const string &input) {
164: 	if (input == "\\n") {
165: 		dialect_options.state_machine_options.new_line.Set(NewLineIdentifier::SINGLE_N);
166: 	} else if (input == "\\r") {
167: 		dialect_options.state_machine_options.new_line.Set(NewLineIdentifier::SINGLE_R);
168: 	} else if (input == "\\r\\n") {
169: 		dialect_options.state_machine_options.new_line.Set(NewLineIdentifier::CARRY_ON);
170: 	} else {
171: 		throw InvalidInputException("This is not accepted as a newline: " + input);
172: 	}
173: }
174: 
175: bool CSVReaderOptions::IgnoreErrors() const {
176: 	return ignore_errors.GetValue() && !store_rejects.GetValue();
177: }
178: 
179: void CSVReaderOptions::SetDateFormat(LogicalTypeId type, const string &format, bool read_format) {
180: 	string error;
181: 	if (read_format) {
182: 		StrpTimeFormat strpformat;
183: 		error = StrTimeFormat::ParseFormatSpecifier(format, strpformat);
184: 		dialect_options.date_format[type].Set(strpformat);
185: 	} else {
186: 		write_date_format[type] = Value(format);
187: 	}
188: 	if (!error.empty()) {
189: 		throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
190: 	}
191: }
192: 
193: void CSVReaderOptions::SetReadOption(const string &loption, const Value &value, vector<string> &expected_names) {
194: 	if (SetBaseOption(loption, value)) {
195: 		return;
196: 	}
197: 	if (loption == "auto_detect") {
198: 		auto_detect = ParseBoolean(value, loption);
199: 	} else if (loption == "sample_size") {
200: 		auto sample_size_option = ParseInteger(value, loption);
201: 		if (sample_size_option < 1 && sample_size_option != -1) {
202: 			throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
203: 		}
204: 		if (sample_size_option == -1) {
205: 			// If -1, we basically read the whole thing
206: 			sample_size_chunks = NumericLimits<idx_t>().Maximum();
207: 		} else {
208: 			sample_size_chunks = NumericCast<idx_t>(sample_size_option / STANDARD_VECTOR_SIZE);
209: 			if (sample_size_option % STANDARD_VECTOR_SIZE != 0) {
210: 				sample_size_chunks++;
211: 			}
212: 		}
213: 
214: 	} else if (loption == "skip") {
215: 		SetSkipRows(ParseInteger(value, loption));
216: 	} else if (loption == "max_line_size" || loption == "maximum_line_size") {
217: 		maximum_line_size = NumericCast<idx_t>(ParseInteger(value, loption));
218: 	} else if (loption == "date_format" || loption == "dateformat") {
219: 		string format = ParseString(value, loption);
220: 		SetDateFormat(LogicalTypeId::DATE, format, true);
221: 	} else if (loption == "timestamp_format" || loption == "timestampformat") {
222: 		string format = ParseString(value, loption);
223: 		SetDateFormat(LogicalTypeId::TIMESTAMP, format, true);
224: 	} else if (loption == "ignore_errors") {
225: 		ignore_errors.Set(ParseBoolean(value, loption));
226: 	} else if (loption == "buffer_size") {
227: 		buffer_size = NumericCast<idx_t>(ParseInteger(value, loption));
228: 		if (buffer_size == 0) {
229: 			throw InvalidInputException("Buffer Size option must be higher than 0");
230: 		}
231: 	} else if (loption == "decimal_separator") {
232: 		decimal_separator = ParseString(value, loption);
233: 		if (decimal_separator != "." && decimal_separator != ",") {
234: 			throw BinderException("Unsupported parameter for DECIMAL_SEPARATOR: should be '.' or ','");
235: 		}
236: 	} else if (loption == "null_padding") {
237: 		null_padding = ParseBoolean(value, loption);
238: 	} else if (loption == "parallel") {
239: 		parallel = ParseBoolean(value, loption);
240: 	} else if (loption == "allow_quoted_nulls") {
241: 		allow_quoted_nulls = ParseBoolean(value, loption);
242: 	} else if (loption == "store_rejects") {
243: 		store_rejects.Set(ParseBoolean(value, loption));
244: 	} else if (loption == "force_not_null") {
245: 		if (!expected_names.empty()) {
246: 			force_not_null = ParseColumnList(value, expected_names, loption);
247: 		} else {
248: 			// Get the list of columns to use as a recovery key
249: 			auto &children = ListValue::GetChildren(value);
250: 			for (auto &child : children) {
251: 				auto col_name = child.GetValue<string>();
252: 				force_not_null_names.insert(col_name);
253: 			}
254: 		}
255: 
256: 	} else if (loption == "rejects_table") {
257: 		// skip, handled in SetRejectsOptions
258: 		auto table_name = ParseString(value, loption);
259: 		if (table_name.empty()) {
260: 			throw BinderException("REJECTS_TABLE option cannot be empty");
261: 		}
262: 		rejects_table_name.Set(table_name);
263: 	} else if (loption == "rejects_scan") {
264: 		// skip, handled in SetRejectsOptions
265: 		auto table_name = ParseString(value, loption);
266: 		if (table_name.empty()) {
267: 			throw BinderException("rejects_scan option cannot be empty");
268: 		}
269: 		rejects_scan_name.Set(table_name);
270: 	} else if (loption == "rejects_limit") {
271: 		auto limit = ParseInteger(value, loption);
272: 		if (limit < 0) {
273: 			throw BinderException("Unsupported parameter for REJECTS_LIMIT: cannot be negative");
274: 		}
275: 		rejects_limit = NumericCast<idx_t>(limit);
276: 	} else {
277: 		throw BinderException("Unrecognized option for CSV reader \"%s\"", loption);
278: 	}
279: }
280: 
281: void CSVReaderOptions::SetWriteOption(const string &loption, const Value &value) {
282: 	if (loption == "new_line") {
283: 		// Steal this from SetBaseOption so we can write different newlines (e.g., format JSON ARRAY)
284: 		write_newline = ParseString(value, loption);
285: 		return;
286: 	}
287: 
288: 	if (SetBaseOption(loption, value, true)) {
289: 		return;
290: 	}
291: 
292: 	if (loption == "force_quote") {
293: 		force_quote = ParseColumnList(value, name_list, loption);
294: 	} else if (loption == "date_format" || loption == "dateformat") {
295: 		string format = ParseString(value, loption);
296: 		SetDateFormat(LogicalTypeId::DATE, format, false);
297: 	} else if (loption == "timestamp_format" || loption == "timestampformat") {
298: 		string format = ParseString(value, loption);
299: 		if (StringUtil::Lower(format) == "iso") {
300: 			format = "%Y-%m-%dT%H:%M:%S.%fZ";
301: 		}
302: 		SetDateFormat(LogicalTypeId::TIMESTAMP, format, false);
303: 		SetDateFormat(LogicalTypeId::TIMESTAMP_TZ, format, false);
304: 	} else if (loption == "prefix") {
305: 		prefix = ParseString(value, loption);
306: 	} else if (loption == "suffix") {
307: 		suffix = ParseString(value, loption);
308: 	} else {
309: 		throw BinderException("Unrecognized option CSV writer \"%s\"", loption);
310: 	}
311: }
312: 
313: bool CSVReaderOptions::SetBaseOption(const string &loption, const Value &value, bool write_option) {
314: 	// Make sure this function was only called after the option was turned into lowercase
315: 	D_ASSERT(!std::any_of(loption.begin(), loption.end(), ::isupper));
316: 
317: 	if (StringUtil::StartsWith(loption, "delim") || StringUtil::StartsWith(loption, "sep")) {
318: 		SetDelimiter(ParseString(value, loption));
319: 	} else if (loption == "quote") {
320: 		SetQuote(ParseString(value, loption));
321: 	} else if (loption == "comment") {
322: 		SetComment(ParseString(value, loption));
323: 	} else if (loption == "new_line") {
324: 		SetNewline(ParseString(value, loption));
325: 	} else if (loption == "escape") {
326: 		SetEscape(ParseString(value, loption));
327: 	} else if (loption == "header") {
328: 		SetHeader(ParseBoolean(value, loption));
329: 	} else if (loption == "nullstr" || loption == "null") {
330: 		auto &child_type = value.type();
331: 		null_str.clear();
332: 		if (child_type.id() != LogicalTypeId::LIST && child_type.id() != LogicalTypeId::VARCHAR) {
333: 			throw BinderException("CSV Reader function option %s requires a string or a list as input", loption);
334: 		}
335: 		if (!null_str.empty()) {
336: 			throw BinderException("CSV Reader function option nullstr can only be supplied once");
337: 		}
338: 		if (child_type.id() == LogicalTypeId::LIST) {
339: 			auto &list_child = ListType::GetChildType(child_type);
340: 			const vector<Value> *children = nullptr;
341: 			if (list_child.id() == LogicalTypeId::LIST) {
342: 				// This can happen if it comes from a copy FROM/TO
343: 				auto &list_grandchild = ListType::GetChildType(list_child);
344: 				auto &children_ref = ListValue::GetChildren(value);
345: 				if (list_grandchild.id() != LogicalTypeId::VARCHAR || children_ref.size() != 1) {
346: 					throw BinderException("CSV Reader function option %s requires a non-empty list of possible null "
347: 					                      "strings (varchar) as input",
348: 					                      loption);
349: 				}
350: 				children = &ListValue::GetChildren(children_ref.back());
351: 			} else if (list_child.id() != LogicalTypeId::VARCHAR) {
352: 				throw BinderException("CSV Reader function option %s requires a non-empty list of possible null "
353: 				                      "strings (varchar) as input",
354: 				                      loption);
355: 			}
356: 			if (!children) {
357: 				children = &ListValue::GetChildren(value);
358: 			}
359: 			for (auto &child : *children) {
360: 				if (child.IsNull()) {
361: 					throw BinderException(
362: 					    "CSV Reader function option %s does not accept NULL values as a valid nullstr option", loption);
363: 				}
364: 				null_str.push_back(StringValue::Get(child));
365: 			}
366: 		} else {
367: 			null_str.push_back(StringValue::Get(ParseString(value, loption)));
368: 		}
369: 		if (null_str.size() > 1 && write_option) {
370: 			throw BinderException("CSV Writer function option %s only accepts one nullstr value.", loption);
371: 		}
372: 
373: 	} else if (loption == "encoding") {
374: 		auto encoding = StringUtil::Lower(ParseString(value, loption));
375: 		if (encoding != "utf8" && encoding != "utf-8") {
376: 			throw BinderException("Copy is only supported for UTF-8 encoded files, ENCODING 'UTF-8'");
377: 		}
378: 	} else if (loption == "compression") {
379: 		SetCompression(ParseString(value, loption));
380: 	} else {
381: 		// unrecognized option in base CSV
382: 		return false;
383: 	}
384: 	return true;
385: }
386: 
387: template <class T>
388: string FormatOptionLine(const string &name, const CSVOption<T> option) {
389: 	return name + " = " + option.FormatValue() + " " + option.FormatSet() + "\n  ";
390: }
391: bool CSVReaderOptions::WasTypeManuallySet(idx_t i) const {
392: 	if (i >= was_type_manually_set.size()) {
393: 		return false;
394: 	}
395: 	return was_type_manually_set[i];
396: }
397: 
398: string CSVReaderOptions::ToString(const string &current_file_path) const {
399: 	auto &delimiter = dialect_options.state_machine_options.delimiter;
400: 	auto &quote = dialect_options.state_machine_options.quote;
401: 	auto &escape = dialect_options.state_machine_options.escape;
402: 	auto &comment = dialect_options.state_machine_options.comment;
403: 	auto &new_line = dialect_options.state_machine_options.new_line;
404: 	auto &skip_rows = dialect_options.skip_rows;
405: 
406: 	auto &header = dialect_options.header;
407: 	string error = "  file=" + current_file_path + "\n  ";
408: 	// Let's first print options that can either be set by the user or by the sniffer
409: 	// delimiter
410: 	error += FormatOptionLine("delimiter", delimiter);
411: 	// quote
412: 	error += FormatOptionLine("quote", quote);
413: 	// escape
414: 	error += FormatOptionLine("escape", escape);
415: 	// newline
416: 	error += FormatOptionLine("new_line", new_line);
417: 	// has_header
418: 	error += FormatOptionLine("header", header);
419: 	// skip_rows
420: 	error += FormatOptionLine("skip_rows", skip_rows);
421: 	// comment
422: 	error += FormatOptionLine("comment", comment);
423: 	// date format
424: 	error += FormatOptionLine("date_format", dialect_options.date_format.at(LogicalType::DATE));
425: 	// timestamp format
426: 	error += FormatOptionLine("timestamp_format", dialect_options.date_format.at(LogicalType::TIMESTAMP));
427: 
428: 	// Now we do options that can only be set by the user, that might hold some general significance
429: 	// null padding
430: 	error += "null_padding=" + std::to_string(null_padding) + "\n  ";
431: 	// sample_size
432: 	error += "sample_size=" + std::to_string(sample_size_chunks * STANDARD_VECTOR_SIZE) + "\n  ";
433: 	// ignore_errors
434: 	error += "ignore_errors=" + ignore_errors.FormatValue() + "\n  ";
435: 	// all_varchar
436: 	error += "all_varchar=" + std::to_string(all_varchar) + "\n";
437: 
438: 	// Add information regarding sniffer mismatches (if any)
439: 	error += sniffer_user_mismatch_error;
440: 	return error;
441: }
442: 
443: static Value StringVectorToValue(const vector<string> &vec) {
444: 	vector<Value> content;
445: 	content.reserve(vec.size());
446: 	for (auto &item : vec) {
447: 		content.push_back(Value(item));
448: 	}
449: 	return Value::LIST(std::move(content));
450: }
451: 
452: static uint8_t GetCandidateSpecificity(const LogicalType &candidate_type) {
453: 	//! Const ht with accepted auto_types and their weights in specificity
454: 	const duckdb::unordered_map<uint8_t, uint8_t> auto_type_candidates_specificity {
455: 	    {(uint8_t)LogicalTypeId::VARCHAR, 0},   {(uint8_t)LogicalTypeId::DOUBLE, 1},
456: 	    {(uint8_t)LogicalTypeId::FLOAT, 2},     {(uint8_t)LogicalTypeId::DECIMAL, 3},
457: 	    {(uint8_t)LogicalTypeId::BIGINT, 4},    {(uint8_t)LogicalTypeId::INTEGER, 5},
458: 	    {(uint8_t)LogicalTypeId::SMALLINT, 6},  {(uint8_t)LogicalTypeId::TINYINT, 7},
459: 	    {(uint8_t)LogicalTypeId::TIMESTAMP, 8}, {(uint8_t)LogicalTypeId::DATE, 9},
460: 	    {(uint8_t)LogicalTypeId::TIME, 10},     {(uint8_t)LogicalTypeId::BOOLEAN, 11},
461: 	    {(uint8_t)LogicalTypeId::SQLNULL, 12}};
462: 
463: 	auto id = (uint8_t)candidate_type.id();
464: 	auto it = auto_type_candidates_specificity.find(id);
465: 	if (it == auto_type_candidates_specificity.end()) {
466: 		throw BinderException("Auto Type Candidate of type %s is not accepted as a valid input",
467: 		                      EnumUtil::ToString(candidate_type.id()));
468: 	}
469: 	return it->second;
470: }
471: bool StoreUserDefinedParameter(string &option) {
472: 	if (option == "column_types" || option == "types" || option == "dtypes" || option == "auto_detect" ||
473: 	    option == "auto_type_candidates" || option == "columns" || option == "names") {
474: 		// We don't store options related to types, names and auto-detection since these are either irrelevant to our
475: 		// prompt or are covered by the columns option.
476: 		return false;
477: 	}
478: 	return true;
479: }
480: void CSVReaderOptions::FromNamedParameters(named_parameter_map_t &in, ClientContext &context) {
481: 	map<string, string> ordered_user_defined_parameters;
482: 	for (auto &kv : in) {
483: 		if (MultiFileReader().ParseOption(kv.first, kv.second, file_options, context)) {
484: 			continue;
485: 		}
486: 		auto loption = StringUtil::Lower(kv.first);
487: 		// skip variables that are specific to auto detection
488: 		if (StoreUserDefinedParameter(loption)) {
489: 			ordered_user_defined_parameters[loption] = kv.second.ToSQLString();
490: 		}
491: 		if (loption == "columns") {
492: 			if (!name_list.empty()) {
493: 				throw BinderException("read_csv_auto column_names/names can only be supplied once");
494: 			}
495: 			columns_set = true;
496: 			auto &child_type = kv.second.type();
497: 			if (child_type.id() != LogicalTypeId::STRUCT) {
498: 				throw BinderException("read_csv columns requires a struct as input");
499: 			}
500: 			auto &struct_children = StructValue::GetChildren(kv.second);
501: 			D_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());
502: 			for (idx_t i = 0; i < struct_children.size(); i++) {
503: 				auto &name = StructType::GetChildName(child_type, i);
504: 				auto &val = struct_children[i];
505: 				name_list.push_back(name);
506: 				if (val.type().id() != LogicalTypeId::VARCHAR) {
507: 					throw BinderException("read_csv requires a type specification as string");
508: 				}
509: 				sql_types_per_column[name] = i;
510: 				sql_type_list.emplace_back(TransformStringToLogicalType(StringValue::Get(val), context));
511: 			}
512: 			if (name_list.empty()) {
513: 				throw BinderException("read_csv requires at least a single column as input!");
514: 			}
515: 		} else if (loption == "auto_type_candidates") {
516: 			auto_type_candidates.clear();
517: 			map<uint8_t, LogicalType> candidate_types;
518: 			// We always have the extremes of Null and Varchar, so we can default to varchar if the
519: 			// sniffer is not able to confidently detect that column type
520: 			candidate_types[GetCandidateSpecificity(LogicalType::VARCHAR)] = LogicalType::VARCHAR;
521: 			candidate_types[GetCandidateSpecificity(LogicalType::SQLNULL)] = LogicalType::SQLNULL;
522: 
523: 			auto &child_type = kv.second.type();
524: 			if (child_type.id() != LogicalTypeId::LIST) {
525: 				throw BinderException("read_csv auto_types requires a list as input");
526: 			}
527: 			auto &list_children = ListValue::GetChildren(kv.second);
528: 			if (list_children.empty()) {
529: 				throw BinderException("auto_type_candidates requires at least one type");
530: 			}
531: 			for (auto &child : list_children) {
532: 				if (child.type().id() != LogicalTypeId::VARCHAR) {
533: 					throw BinderException("auto_type_candidates requires a type specification as string");
534: 				}
535: 				auto candidate_type = TransformStringToLogicalType(StringValue::Get(child), context);
536: 				candidate_types[GetCandidateSpecificity(candidate_type)] = candidate_type;
537: 			}
538: 			for (auto &candidate_type : candidate_types) {
539: 				auto_type_candidates.emplace_back(candidate_type.second);
540: 			}
541: 		} else if (loption == "column_names" || loption == "names") {
542: 			if (!name_list.empty()) {
543: 				throw BinderException("read_csv_auto column_names/names can only be supplied once");
544: 			}
545: 			if (kv.second.IsNull()) {
546: 				throw BinderException("read_csv_auto %s cannot be NULL", kv.first);
547: 			}
548: 			auto &children = ListValue::GetChildren(kv.second);
549: 			for (auto &child : children) {
550: 				name_list.push_back(StringValue::Get(child));
551: 			}
552: 		} else if (loption == "column_types" || loption == "types" || loption == "dtypes") {
553: 			auto &child_type = kv.second.type();
554: 			if (child_type.id() != LogicalTypeId::STRUCT && child_type.id() != LogicalTypeId::LIST) {
555: 				throw BinderException("read_csv_auto %s requires a struct or list as input", kv.first);
556: 			}
557: 			if (!sql_type_list.empty()) {
558: 				throw BinderException("read_csv_auto column_types/types/dtypes can only be supplied once");
559: 			}
560: 			vector<string> sql_type_names;
561: 			if (child_type.id() == LogicalTypeId::STRUCT) {
562: 				auto &struct_children = StructValue::GetChildren(kv.second);
563: 				D_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());
564: 				for (idx_t i = 0; i < struct_children.size(); i++) {
565: 					auto &name = StructType::GetChildName(child_type, i);
566: 					auto &val = struct_children[i];
567: 					if (val.type().id() != LogicalTypeId::VARCHAR) {
568: 						throw BinderException("read_csv_auto %s requires a type specification as string", kv.first);
569: 					}
570: 					sql_type_names.push_back(StringValue::Get(val));
571: 					sql_types_per_column[name] = i;
572: 				}
573: 			} else {
574: 				auto &list_child = ListType::GetChildType(child_type);
575: 				if (list_child.id() != LogicalTypeId::VARCHAR) {
576: 					throw BinderException("read_csv_auto %s requires a list of types (varchar) as input", kv.first);
577: 				}
578: 				auto &children = ListValue::GetChildren(kv.second);
579: 				for (auto &child : children) {
580: 					sql_type_names.push_back(StringValue::Get(child));
581: 				}
582: 			}
583: 			sql_type_list.reserve(sql_type_names.size());
584: 			for (auto &sql_type : sql_type_names) {
585: 				auto def_type = TransformStringToLogicalType(sql_type, context);
586: 				if (def_type.id() == LogicalTypeId::USER) {
587: 					throw BinderException("Unrecognized type \"%s\" for read_csv_auto %s definition", sql_type,
588: 					                      kv.first);
589: 				}
590: 				sql_type_list.push_back(std::move(def_type));
591: 			}
592: 		} else if (loption == "all_varchar") {
593: 			all_varchar = BooleanValue::Get(kv.second);
594: 		} else if (loption == "normalize_names") {
595: 			normalize_names = BooleanValue::Get(kv.second);
596: 		} else {
597: 			SetReadOption(loption, kv.second, name_list);
598: 		}
599: 	}
600: 	for (auto &udf_parameter : ordered_user_defined_parameters) {
601: 		user_defined_parameters += udf_parameter.first + "=" + udf_parameter.second + ", ";
602: 	}
603: 	if (user_defined_parameters.size() >= 2) {
604: 		user_defined_parameters.erase(user_defined_parameters.size() - 2);
605: 	}
606: }
607: 
608: //! This function is used to remember options set by the sniffer, for use in ReadCSVRelation
609: void CSVReaderOptions::ToNamedParameters(named_parameter_map_t &named_params) {
610: 	auto &delimiter = dialect_options.state_machine_options.delimiter;
611: 	auto &quote = dialect_options.state_machine_options.quote;
612: 	auto &escape = dialect_options.state_machine_options.escape;
613: 	auto &comment = dialect_options.state_machine_options.comment;
614: 	auto &header = dialect_options.header;
615: 	if (delimiter.IsSetByUser()) {
616: 		named_params["delim"] = Value(GetDelimiter());
617: 	}
618: 	if (dialect_options.state_machine_options.new_line.IsSetByUser()) {
619: 		named_params["new_line"] = Value(GetNewline());
620: 	}
621: 	if (quote.IsSetByUser()) {
622: 		named_params["quote"] = Value(GetQuote());
623: 	}
624: 	if (escape.IsSetByUser()) {
625: 		named_params["escape"] = Value(GetEscape());
626: 	}
627: 	if (comment.IsSetByUser()) {
628: 		named_params["comment"] = Value(GetComment());
629: 	}
630: 	if (header.IsSetByUser()) {
631: 		named_params["header"] = Value(GetHeader());
632: 	}
633: 	named_params["max_line_size"] = Value::BIGINT(NumericCast<int64_t>(maximum_line_size));
634: 	if (dialect_options.skip_rows.IsSetByUser()) {
635: 		named_params["skip"] = Value::UBIGINT(GetSkipRows());
636: 	}
637: 	named_params["null_padding"] = Value::BOOLEAN(null_padding);
638: 	named_params["parallel"] = Value::BOOLEAN(parallel);
639: 	if (!dialect_options.date_format.at(LogicalType::DATE).GetValue().format_specifier.empty()) {
640: 		named_params["dateformat"] =
641: 		    Value(dialect_options.date_format.at(LogicalType::DATE).GetValue().format_specifier);
642: 	}
643: 	if (!dialect_options.date_format.at(LogicalType::TIMESTAMP).GetValue().format_specifier.empty()) {
644: 		named_params["timestampformat"] =
645: 		    Value(dialect_options.date_format.at(LogicalType::TIMESTAMP).GetValue().format_specifier);
646: 	}
647: 
648: 	named_params["normalize_names"] = Value::BOOLEAN(normalize_names);
649: 	if (!name_list.empty() && !named_params.count("columns") && !named_params.count("column_names") &&
650: 	    !named_params.count("names")) {
651: 		named_params["column_names"] = StringVectorToValue(name_list);
652: 	}
653: 	named_params["all_varchar"] = Value::BOOLEAN(all_varchar);
654: 	named_params["maximum_line_size"] = Value::BIGINT(NumericCast<int64_t>(maximum_line_size));
655: }
656: 
657: } // namespace duckdb
[end of src/execution/operator/csv_scanner/util/csv_reader_options.cpp]
[start of src/include/duckdb/storage/table/segment_tree.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/segment_tree.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include "duckdb/storage/storage_lock.hpp"
13: #include "duckdb/storage/table/segment_lock.hpp"
14: #include "duckdb/common/vector.hpp"
15: #include "duckdb/common/mutex.hpp"
16: #include "duckdb/common/string_util.hpp"
17: 
18: namespace duckdb {
19: 
20: template <class T>
21: struct SegmentNode {
22: 	idx_t row_start;
23: 	unique_ptr<T> node;
24: };
25: 
26: //! The SegmentTree maintains a list of all segments of a specific column in a table, and allows searching for a segment
27: //! by row number
28: template <class T, bool SUPPORTS_LAZY_LOADING = false>
29: class SegmentTree {
30: private:
31: 	class SegmentIterationHelper;
32: 
33: public:
34: 	explicit SegmentTree() : finished_loading(true) {
35: 	}
36: 	virtual ~SegmentTree() {
37: 	}
38: 
39: 	//! Locks the segment tree. All methods to the segment tree either lock the segment tree, or take an already
40: 	//! obtained lock.
41: 	SegmentLock Lock() {
42: 		return SegmentLock(node_lock);
43: 	}
44: 
45: 	bool IsEmpty(SegmentLock &l) {
46: 		return GetRootSegment(l) == nullptr;
47: 	}
48: 
49: 	//! Gets a pointer to the first segment. Useful for scans.
50: 	T *GetRootSegment() {
51: 		auto l = Lock();
52: 		return GetRootSegment(l);
53: 	}
54: 
55: 	T *GetRootSegment(SegmentLock &l) {
56: 		if (nodes.empty()) {
57: 			LoadNextSegment(l);
58: 		}
59: 		return GetRootSegmentInternal();
60: 	}
61: 	//! Obtains ownership of the data of the segment tree
62: 	vector<SegmentNode<T>> MoveSegments(SegmentLock &l) {
63: 		LoadAllSegments(l);
64: 		return std::move(nodes);
65: 	}
66: 	vector<SegmentNode<T>> MoveSegments() {
67: 		auto l = Lock();
68: 		return MoveSegments(l);
69: 	}
70: 	idx_t GetSegmentCount() {
71: 		auto l = Lock();
72: 		return GetSegmentCount(l);
73: 	}
74: 	idx_t GetSegmentCount(SegmentLock &l) {
75: 		return nodes.size();
76: 	}
77: 	//! Gets a pointer to the nth segment. Negative numbers start from the back.
78: 	T *GetSegmentByIndex(int64_t index) {
79: 		auto l = Lock();
80: 		return GetSegmentByIndex(l, index);
81: 	}
82: 	T *GetSegmentByIndex(SegmentLock &l, int64_t index) {
83: 		if (index < 0) {
84: 			// load all segments
85: 			LoadAllSegments(l);
86: 			index += nodes.size();
87: 			if (index < 0) {
88: 				return nullptr;
89: 			}
90: 			return nodes[UnsafeNumericCast<idx_t>(index)].node.get();
91: 		} else {
92: 			// lazily load segments until we reach the specific segment
93: 			while (idx_t(index) >= nodes.size() && LoadNextSegment(l)) {
94: 			}
95: 			if (idx_t(index) >= nodes.size()) {
96: 				return nullptr;
97: 			}
98: 			return nodes[UnsafeNumericCast<idx_t>(index)].node.get();
99: 		}
100: 	}
101: 	//! Gets the next segment
102: 	T *GetNextSegment(T *segment) {
103: 		if (!SUPPORTS_LAZY_LOADING) {
104: 			return segment->Next();
105: 		}
106: 		if (finished_loading) {
107: 			return segment->Next();
108: 		}
109: 		auto l = Lock();
110: 		return GetNextSegment(l, segment);
111: 	}
112: 	T *GetNextSegment(SegmentLock &l, T *segment) {
113: 		if (!segment) {
114: 			return nullptr;
115: 		}
116: #ifdef DEBUG
117: 		D_ASSERT(nodes[segment->index].node.get() == segment);
118: #endif
119: 		return GetSegmentByIndex(l, UnsafeNumericCast<int64_t>(segment->index + 1));
120: 	}
121: 
122: 	//! Gets a pointer to the last segment. Useful for appends.
123: 	T *GetLastSegment(SegmentLock &l) {
124: 		LoadAllSegments(l);
125: 		if (nodes.empty()) {
126: 			return nullptr;
127: 		}
128: 		return nodes.back().node.get();
129: 	}
130: 	//! Gets a pointer to a specific column segment for the given row
131: 	T *GetSegment(idx_t row_number) {
132: 		auto l = Lock();
133: 		return GetSegment(l, row_number);
134: 	}
135: 	T *GetSegment(SegmentLock &l, idx_t row_number) {
136: 		return nodes[GetSegmentIndex(l, row_number)].node.get();
137: 	}
138: 
139: 	//! Append a column segment to the tree
140: 	void AppendSegmentInternal(SegmentLock &l, unique_ptr<T> segment) {
141: 		D_ASSERT(segment);
142: 		// add the node to the list of nodes
143: 		if (!nodes.empty()) {
144: 			nodes.back().node->next = segment.get();
145: 		}
146: 		SegmentNode<T> node;
147: 		segment->index = nodes.size();
148: 		node.row_start = segment->start;
149: 		node.node = std::move(segment);
150: 		nodes.push_back(std::move(node));
151: 	}
152: 	void AppendSegment(unique_ptr<T> segment) {
153: 		auto l = Lock();
154: 		AppendSegment(l, std::move(segment));
155: 	}
156: 	void AppendSegment(SegmentLock &l, unique_ptr<T> segment) {
157: 		LoadAllSegments(l);
158: 		AppendSegmentInternal(l, std::move(segment));
159: 	}
160: 	//! Debug method, check whether the segment is in the segment tree
161: 	bool HasSegment(T *segment) {
162: 		auto l = Lock();
163: 		return HasSegment(l, segment);
164: 	}
165: 	bool HasSegment(SegmentLock &, T *segment) {
166: 		return segment->index < nodes.size() && nodes[segment->index].node.get() == segment;
167: 	}
168: 
169: 	//! Replace this tree with another tree, taking over its nodes in-place
170: 	void Replace(SegmentTree<T> &other) {
171: 		auto l = Lock();
172: 		Replace(l, other);
173: 	}
174: 	void Replace(SegmentLock &l, SegmentTree<T> &other) {
175: 		other.LoadAllSegments(l);
176: 		nodes = std::move(other.nodes);
177: 	}
178: 
179: 	//! Erase all segments after a specific segment
180: 	void EraseSegments(SegmentLock &l, idx_t segment_start) {
181: 		LoadAllSegments(l);
182: 		if (segment_start >= nodes.size() - 1) {
183: 			return;
184: 		}
185: 		nodes.erase(nodes.begin() + UnsafeNumericCast<int64_t>(segment_start) + 1, nodes.end());
186: 	}
187: 
188: 	//! Get the segment index of the column segment for the given row
189: 	idx_t GetSegmentIndex(SegmentLock &l, idx_t row_number) {
190: 		idx_t segment_index;
191: 		if (TryGetSegmentIndex(l, row_number, segment_index)) {
192: 			return segment_index;
193: 		}
194: 		string error;
195: 		error = StringUtil::Format("Attempting to find row number \"%lld\" in %lld nodes\n", row_number, nodes.size());
196: 		for (idx_t i = 0; i < nodes.size(); i++) {
197: 			error += StringUtil::Format("Node %lld: Start %lld, Count %lld", i, nodes[i].row_start,
198: 			                            nodes[i].node->count.load());
199: 		}
200: 		throw InternalException("Could not find node in column segment tree!\n%s%s", error, Exception::GetStackTrace());
201: 	}
202: 
203: 	bool TryGetSegmentIndex(SegmentLock &l, idx_t row_number, idx_t &result) {
204: 		// load segments until the row number is within bounds
205: 		while (nodes.empty() || (row_number >= (nodes.back().row_start + nodes.back().node->count))) {
206: 			if (!LoadNextSegment(l)) {
207: 				break;
208: 			}
209: 		}
210: 		if (nodes.empty()) {
211: 			return false;
212: 		}
213: 		idx_t lower = 0;
214: 		idx_t upper = nodes.size() - 1;
215: 		// binary search to find the node
216: 		while (lower <= upper) {
217: 			idx_t index = (lower + upper) / 2;
218: 			D_ASSERT(index < nodes.size());
219: 			auto &entry = nodes[index];
220: 			D_ASSERT(entry.row_start == entry.node->start);
221: 			if (row_number < entry.row_start) {
222: 				upper = index - 1;
223: 			} else if (row_number >= entry.row_start + entry.node->count) {
224: 				lower = index + 1;
225: 			} else {
226: 				result = index;
227: 				return true;
228: 			}
229: 		}
230: 		return false;
231: 	}
232: 
233: 	void Verify(SegmentLock &) {
234: #ifdef DEBUG
235: 		idx_t base_start = nodes.empty() ? 0 : nodes[0].node->start;
236: 		for (idx_t i = 0; i < nodes.size(); i++) {
237: 			D_ASSERT(nodes[i].row_start == nodes[i].node->start);
238: 			D_ASSERT(nodes[i].node->start == base_start);
239: 			base_start += nodes[i].node->count;
240: 		}
241: #endif
242: 	}
243: 	void Verify() {
244: #ifdef DEBUG
245: 		auto l = Lock();
246: 		Verify(l);
247: #endif
248: 	}
249: 
250: 	SegmentIterationHelper Segments() {
251: 		return SegmentIterationHelper(*this);
252: 	}
253: 
254: 	void Reinitialize() {
255: 		if (nodes.empty()) {
256: 			return;
257: 		}
258: 		idx_t offset = nodes[0].node->start;
259: 		for (auto &entry : nodes) {
260: 			if (entry.node->start != offset) {
261: 				throw InternalException("In SegmentTree::Reinitialize - gap found between nodes!");
262: 			}
263: 			entry.row_start = offset;
264: 			offset += entry.node->count;
265: 		}
266: 	}
267: 
268: protected:
269: 	atomic<bool> finished_loading;
270: 
271: 	//! Load the next segment - only used when lazily loading
272: 	virtual unique_ptr<T> LoadSegment() {
273: 		return nullptr;
274: 	}
275: 
276: private:
277: 	//! The nodes in the tree, can be binary searched
278: 	vector<SegmentNode<T>> nodes;
279: 	//! Lock to access or modify the nodes
280: 	mutex node_lock;
281: 
282: private:
283: 	T *GetRootSegmentInternal() {
284: 		return nodes.empty() ? nullptr : nodes[0].node.get();
285: 	}
286: 
287: 	class SegmentIterationHelper {
288: 	public:
289: 		explicit SegmentIterationHelper(SegmentTree &tree) : tree(tree) {
290: 		}
291: 
292: 	private:
293: 		SegmentTree &tree;
294: 
295: 	private:
296: 		class SegmentIterator {
297: 		public:
298: 			SegmentIterator(SegmentTree &tree_p, T *current_p) : tree(tree_p), current(current_p) {
299: 			}
300: 
301: 			SegmentTree &tree;
302: 			T *current;
303: 
304: 		public:
305: 			void Next() {
306: 				current = tree.GetNextSegment(current);
307: 			}
308: 
309: 			SegmentIterator &operator++() {
310: 				Next();
311: 				return *this;
312: 			}
313: 			bool operator!=(const SegmentIterator &other) const {
314: 				return current != other.current;
315: 			}
316: 			T &operator*() const {
317: 				D_ASSERT(current);
318: 				return *current;
319: 			}
320: 		};
321: 
322: 	public:
323: 		SegmentIterator begin() { // NOLINT: match stl API
324: 			return SegmentIterator(tree, tree.GetRootSegment());
325: 		}
326: 		SegmentIterator end() { // NOLINT: match stl API
327: 			return SegmentIterator(tree, nullptr);
328: 		}
329: 	};
330: 
331: 	//! Load the next segment, if there are any left to load
332: 	bool LoadNextSegment(SegmentLock &l) {
333: 		if (!SUPPORTS_LAZY_LOADING) {
334: 			return false;
335: 		}
336: 		if (finished_loading) {
337: 			return false;
338: 		}
339: 		auto result = LoadSegment();
340: 		if (result) {
341: 			AppendSegmentInternal(l, std::move(result));
342: 			return true;
343: 		}
344: 		return false;
345: 	}
346: 
347: 	//! Load all segments, if there are any left to load
348: 	void LoadAllSegments(SegmentLock &l) {
349: 		if (!SUPPORTS_LAZY_LOADING) {
350: 			return;
351: 		}
352: 		while (LoadNextSegment(l)) {
353: 		}
354: 	}
355: };
356: 
357: } // namespace duckdb
[end of src/include/duckdb/storage/table/segment_tree.hpp]
[start of src/storage/table/row_group_collection.cpp]
1: #include "duckdb/storage/table/row_group_collection.hpp"
2: 
3: #include "duckdb/common/serializer/binary_deserializer.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/execution/index/bound_index.hpp"
6: #include "duckdb/execution/task_error_manager.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/parallel/task_executor.hpp"
9: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
10: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
11: #include "duckdb/storage/data_table.hpp"
12: #include "duckdb/storage/metadata/metadata_reader.hpp"
13: #include "duckdb/storage/table/append_state.hpp"
14: #include "duckdb/storage/table/column_checkpoint_state.hpp"
15: #include "duckdb/storage/table/persistent_table_data.hpp"
16: #include "duckdb/storage/table/row_group_segment_tree.hpp"
17: #include "duckdb/storage/table/scan_state.hpp"
18: #include "duckdb/storage/table_storage_info.hpp"
19: 
20: namespace duckdb {
21: 
22: //===--------------------------------------------------------------------===//
23: // Row Group Segment Tree
24: //===--------------------------------------------------------------------===//
25: RowGroupSegmentTree::RowGroupSegmentTree(RowGroupCollection &collection)
26:     : SegmentTree<RowGroup, true>(), collection(collection), current_row_group(0), max_row_group(0) {
27: }
28: RowGroupSegmentTree::~RowGroupSegmentTree() {
29: }
30: 
31: void RowGroupSegmentTree::Initialize(PersistentTableData &data) {
32: 	D_ASSERT(data.row_group_count > 0);
33: 	current_row_group = 0;
34: 	max_row_group = data.row_group_count;
35: 	finished_loading = false;
36: 	reader = make_uniq<MetadataReader>(collection.GetMetadataManager(), data.block_pointer);
37: }
38: 
39: unique_ptr<RowGroup> RowGroupSegmentTree::LoadSegment() {
40: 	if (current_row_group >= max_row_group) {
41: 		reader.reset();
42: 		finished_loading = true;
43: 		return nullptr;
44: 	}
45: 	BinaryDeserializer deserializer(*reader);
46: 	deserializer.Begin();
47: 	auto row_group_pointer = RowGroup::Deserialize(deserializer);
48: 	deserializer.End();
49: 	current_row_group++;
50: 	return make_uniq<RowGroup>(collection, std::move(row_group_pointer));
51: }
52: 
53: //===--------------------------------------------------------------------===//
54: // Row Group Collection
55: //===--------------------------------------------------------------------===//
56: RowGroupCollection::RowGroupCollection(shared_ptr<DataTableInfo> info_p, BlockManager &block_manager,
57:                                        vector<LogicalType> types_p, idx_t row_start_p, idx_t total_rows_p)
58:     : block_manager(block_manager), total_rows(total_rows_p), info(std::move(info_p)), types(std::move(types_p)),
59:       row_start(row_start_p), allocation_size(0) {
60: 	row_groups = make_shared_ptr<RowGroupSegmentTree>(*this);
61: }
62: 
63: idx_t RowGroupCollection::GetTotalRows() const {
64: 	return total_rows.load();
65: }
66: 
67: const vector<LogicalType> &RowGroupCollection::GetTypes() const {
68: 	return types;
69: }
70: 
71: Allocator &RowGroupCollection::GetAllocator() const {
72: 	return Allocator::Get(info->GetDB());
73: }
74: 
75: AttachedDatabase &RowGroupCollection::GetAttached() {
76: 	return GetTableInfo().GetDB();
77: }
78: 
79: MetadataManager &RowGroupCollection::GetMetadataManager() {
80: 	return GetBlockManager().GetMetadataManager();
81: }
82: 
83: //===--------------------------------------------------------------------===//
84: // Initialize
85: //===--------------------------------------------------------------------===//
86: void RowGroupCollection::Initialize(PersistentTableData &data) {
87: 	D_ASSERT(this->row_start == 0);
88: 	auto l = row_groups->Lock();
89: 	this->total_rows = data.total_rows;
90: 	row_groups->Initialize(data);
91: 	stats.Initialize(types, data);
92: }
93: 
94: void RowGroupCollection::Initialize(PersistentCollectionData &data) {
95: 	stats.InitializeEmpty(types);
96: 	auto l = row_groups->Lock();
97: 	for (auto &row_group_data : data.row_group_data) {
98: 		auto row_group = make_uniq<RowGroup>(*this, row_group_data);
99: 		row_group->MergeIntoStatistics(stats);
100: 		total_rows += row_group->count;
101: 		row_groups->AppendSegment(l, std::move(row_group));
102: 	}
103: }
104: 
105: void RowGroupCollection::InitializeEmpty() {
106: 	stats.InitializeEmpty(types);
107: }
108: 
109: void RowGroupCollection::AppendRowGroup(SegmentLock &l, idx_t start_row) {
110: 	D_ASSERT(start_row >= row_start);
111: 	auto new_row_group = make_uniq<RowGroup>(*this, start_row, 0U);
112: 	new_row_group->InitializeEmpty(types);
113: 	row_groups->AppendSegment(l, std::move(new_row_group));
114: }
115: 
116: RowGroup *RowGroupCollection::GetRowGroup(int64_t index) {
117: 	return (RowGroup *)row_groups->GetSegmentByIndex(index);
118: }
119: 
120: void RowGroupCollection::Verify() {
121: #ifdef DEBUG
122: 	idx_t current_total_rows = 0;
123: 	row_groups->Verify();
124: 	for (auto &row_group : row_groups->Segments()) {
125: 		row_group.Verify();
126: 		D_ASSERT(&row_group.GetCollection() == this);
127: 		D_ASSERT(row_group.start == this->row_start + current_total_rows);
128: 		current_total_rows += row_group.count;
129: 	}
130: 	D_ASSERT(current_total_rows == total_rows.load());
131: #endif
132: }
133: 
134: //===--------------------------------------------------------------------===//
135: // Scan
136: //===--------------------------------------------------------------------===//
137: void RowGroupCollection::InitializeScan(CollectionScanState &state, const vector<column_t> &column_ids,
138:                                         TableFilterSet *table_filters) {
139: 	auto row_group = row_groups->GetRootSegment();
140: 	D_ASSERT(row_group);
141: 	state.row_groups = row_groups.get();
142: 	state.max_row = row_start + total_rows;
143: 	state.Initialize(GetTypes());
144: 	while (row_group && !row_group->InitializeScan(state)) {
145: 		row_group = row_groups->GetNextSegment(row_group);
146: 	}
147: }
148: 
149: void RowGroupCollection::InitializeCreateIndexScan(CreateIndexScanState &state) {
150: 	state.segment_lock = row_groups->Lock();
151: }
152: 
153: void RowGroupCollection::InitializeScanWithOffset(CollectionScanState &state, const vector<column_t> &column_ids,
154:                                                   idx_t start_row, idx_t end_row) {
155: 	auto row_group = row_groups->GetSegment(start_row);
156: 	D_ASSERT(row_group);
157: 	state.row_groups = row_groups.get();
158: 	state.max_row = end_row;
159: 	state.Initialize(GetTypes());
160: 	idx_t start_vector = (start_row - row_group->start) / STANDARD_VECTOR_SIZE;
161: 	if (!row_group->InitializeScanWithOffset(state, start_vector)) {
162: 		throw InternalException("Failed to initialize row group scan with offset");
163: 	}
164: }
165: 
166: bool RowGroupCollection::InitializeScanInRowGroup(CollectionScanState &state, RowGroupCollection &collection,
167:                                                   RowGroup &row_group, idx_t vector_index, idx_t max_row) {
168: 	state.max_row = max_row;
169: 	state.row_groups = collection.row_groups.get();
170: 	if (!state.column_scans) {
171: 		// initialize the scan state
172: 		state.Initialize(collection.GetTypes());
173: 	}
174: 	return row_group.InitializeScanWithOffset(state, vector_index);
175: }
176: 
177: void RowGroupCollection::InitializeParallelScan(ParallelCollectionScanState &state) {
178: 	state.collection = this;
179: 	state.current_row_group = row_groups->GetRootSegment();
180: 	state.vector_index = 0;
181: 	state.max_row = row_start + total_rows;
182: 	state.batch_index = 0;
183: 	state.processed_rows = 0;
184: }
185: 
186: bool RowGroupCollection::NextParallelScan(ClientContext &context, ParallelCollectionScanState &state,
187:                                           CollectionScanState &scan_state) {
188: 	while (true) {
189: 		idx_t vector_index;
190: 		idx_t max_row;
191: 		RowGroupCollection *collection;
192: 		RowGroup *row_group;
193: 		{
194: 			// select the next row group to scan from the parallel state
195: 			lock_guard<mutex> l(state.lock);
196: 			if (!state.current_row_group || state.current_row_group->count == 0) {
197: 				// no more data left to scan
198: 				break;
199: 			}
200: 			collection = state.collection;
201: 			row_group = state.current_row_group;
202: 			if (ClientConfig::GetConfig(context).verify_parallelism) {
203: 				vector_index = state.vector_index;
204: 				max_row = state.current_row_group->start +
205: 				          MinValue<idx_t>(state.current_row_group->count,
206: 				                          STANDARD_VECTOR_SIZE * state.vector_index + STANDARD_VECTOR_SIZE);
207: 				D_ASSERT(vector_index * STANDARD_VECTOR_SIZE < state.current_row_group->count);
208: 				state.vector_index++;
209: 				if (state.vector_index * STANDARD_VECTOR_SIZE >= state.current_row_group->count) {
210: 					state.current_row_group = row_groups->GetNextSegment(state.current_row_group);
211: 					state.vector_index = 0;
212: 				}
213: 			} else {
214: 				state.processed_rows += state.current_row_group->count;
215: 				vector_index = 0;
216: 				max_row = state.current_row_group->start + state.current_row_group->count;
217: 				state.current_row_group = row_groups->GetNextSegment(state.current_row_group);
218: 			}
219: 			max_row = MinValue<idx_t>(max_row, state.max_row);
220: 			scan_state.batch_index = ++state.batch_index;
221: 		}
222: 		D_ASSERT(collection);
223: 		D_ASSERT(row_group);
224: 
225: 		// initialize the scan for this row group
226: 		bool need_to_scan = InitializeScanInRowGroup(scan_state, *collection, *row_group, vector_index, max_row);
227: 		if (!need_to_scan) {
228: 			// skip this row group
229: 			continue;
230: 		}
231: 		return true;
232: 	}
233: 	lock_guard<mutex> l(state.lock);
234: 	scan_state.batch_index = state.batch_index;
235: 	return false;
236: }
237: 
238: bool RowGroupCollection::Scan(DuckTransaction &transaction, const vector<column_t> &column_ids,
239:                               const std::function<bool(DataChunk &chunk)> &fun) {
240: 	vector<LogicalType> scan_types;
241: 	for (idx_t i = 0; i < column_ids.size(); i++) {
242: 		scan_types.push_back(types[column_ids[i]]);
243: 	}
244: 	DataChunk chunk;
245: 	chunk.Initialize(GetAllocator(), scan_types);
246: 
247: 	// initialize the scan
248: 	TableScanState state;
249: 	state.Initialize(column_ids, nullptr);
250: 	InitializeScan(state.local_state, column_ids, nullptr);
251: 
252: 	while (true) {
253: 		chunk.Reset();
254: 		state.local_state.Scan(transaction, chunk);
255: 		if (chunk.size() == 0) {
256: 			return true;
257: 		}
258: 		if (!fun(chunk)) {
259: 			return false;
260: 		}
261: 	}
262: }
263: 
264: bool RowGroupCollection::Scan(DuckTransaction &transaction, const std::function<bool(DataChunk &chunk)> &fun) {
265: 	vector<column_t> column_ids;
266: 	column_ids.reserve(types.size());
267: 	for (idx_t i = 0; i < types.size(); i++) {
268: 		column_ids.push_back(i);
269: 	}
270: 	return Scan(transaction, column_ids, fun);
271: }
272: 
273: //===--------------------------------------------------------------------===//
274: // Fetch
275: //===--------------------------------------------------------------------===//
276: void RowGroupCollection::Fetch(TransactionData transaction, DataChunk &result, const vector<column_t> &column_ids,
277:                                const Vector &row_identifiers, idx_t fetch_count, ColumnFetchState &state) {
278: 	// figure out which row_group to fetch from
279: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
280: 	idx_t count = 0;
281: 	for (idx_t i = 0; i < fetch_count; i++) {
282: 		auto row_id = row_ids[i];
283: 		RowGroup *row_group;
284: 		{
285: 			idx_t segment_index;
286: 			auto l = row_groups->Lock();
287: 			if (!row_groups->TryGetSegmentIndex(l, UnsafeNumericCast<idx_t>(row_id), segment_index)) {
288: 				// in parallel append scenarios it is possible for the row_id
289: 				continue;
290: 			}
291: 			row_group = row_groups->GetSegmentByIndex(l, UnsafeNumericCast<int64_t>(segment_index));
292: 		}
293: 		if (!row_group->Fetch(transaction, UnsafeNumericCast<idx_t>(row_id) - row_group->start)) {
294: 			continue;
295: 		}
296: 		row_group->FetchRow(transaction, state, column_ids, row_id, result, count);
297: 		count++;
298: 	}
299: 	result.SetCardinality(count);
300: }
301: 
302: //===--------------------------------------------------------------------===//
303: // Append
304: //===--------------------------------------------------------------------===//
305: TableAppendState::TableAppendState()
306:     : row_group_append_state(*this), total_append_count(0), start_row_group(nullptr), transaction(0, 0) {
307: }
308: 
309: TableAppendState::~TableAppendState() {
310: }
311: 
312: bool RowGroupCollection::IsEmpty() const {
313: 	auto l = row_groups->Lock();
314: 	return IsEmpty(l);
315: }
316: 
317: bool RowGroupCollection::IsEmpty(SegmentLock &l) const {
318: 	return row_groups->IsEmpty(l);
319: }
320: 
321: void RowGroupCollection::InitializeAppend(TransactionData transaction, TableAppendState &state) {
322: 	state.row_start = UnsafeNumericCast<row_t>(total_rows.load());
323: 	state.current_row = state.row_start;
324: 	state.total_append_count = 0;
325: 
326: 	// start writing to the row_groups
327: 	auto l = row_groups->Lock();
328: 	if (IsEmpty(l)) {
329: 		// empty row group collection: empty first row group
330: 		AppendRowGroup(l, row_start);
331: 	}
332: 	state.start_row_group = row_groups->GetLastSegment(l);
333: 	D_ASSERT(this->row_start + total_rows == state.start_row_group->start + state.start_row_group->count);
334: 	state.start_row_group->InitializeAppend(state.row_group_append_state);
335: 	state.transaction = transaction;
336: 
337: 	// initialize thread-local stats so we have less lock contention when updating distinct statistics
338: 	state.stats = TableStatistics();
339: 	state.stats.InitializeEmpty(types);
340: }
341: 
342: void RowGroupCollection::InitializeAppend(TableAppendState &state) {
343: 	TransactionData tdata(0, 0);
344: 	InitializeAppend(tdata, state);
345: }
346: 
347: bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {
348: 	D_ASSERT(chunk.ColumnCount() == types.size());
349: 	chunk.Verify();
350: 
351: 	bool new_row_group = false;
352: 	idx_t total_append_count = chunk.size();
353: 	idx_t remaining = chunk.size();
354: 	state.total_append_count += total_append_count;
355: 	while (true) {
356: 		auto current_row_group = state.row_group_append_state.row_group;
357: 		// check how much we can fit into the current row_group
358: 		idx_t append_count =
359: 		    MinValue<idx_t>(remaining, Storage::ROW_GROUP_SIZE - state.row_group_append_state.offset_in_row_group);
360: 		if (append_count > 0) {
361: 			auto previous_allocation_size = current_row_group->GetAllocationSize();
362: 			current_row_group->Append(state.row_group_append_state, chunk, append_count);
363: 			allocation_size += current_row_group->GetAllocationSize() - previous_allocation_size;
364: 			// merge the stats
365: 			current_row_group->MergeIntoStatistics(stats);
366: 		}
367: 		remaining -= append_count;
368: 		if (remaining > 0) {
369: 			// we expect max 1 iteration of this loop (i.e. a single chunk should never overflow more than one
370: 			// row_group)
371: 			D_ASSERT(chunk.size() == remaining + append_count);
372: 			// slice the input chunk
373: 			if (remaining < chunk.size()) {
374: 				chunk.Slice(append_count, remaining);
375: 			}
376: 			// append a new row_group
377: 			new_row_group = true;
378: 			auto next_start = current_row_group->start + state.row_group_append_state.offset_in_row_group;
379: 
380: 			auto l = row_groups->Lock();
381: 			AppendRowGroup(l, next_start);
382: 			// set up the append state for this row_group
383: 			auto last_row_group = row_groups->GetLastSegment(l);
384: 			last_row_group->InitializeAppend(state.row_group_append_state);
385: 			continue;
386: 		} else {
387: 			break;
388: 		}
389: 	}
390: 	state.current_row += row_t(total_append_count);
391: 	auto local_stats_lock = state.stats.GetLock();
392: 	for (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {
393: 		state.stats.GetStats(*local_stats_lock, col_idx).UpdateDistinctStatistics(chunk.data[col_idx], chunk.size());
394: 	}
395: 	return new_row_group;
396: }
397: 
398: void RowGroupCollection::FinalizeAppend(TransactionData transaction, TableAppendState &state) {
399: 	auto remaining = state.total_append_count;
400: 	auto row_group = state.start_row_group;
401: 	while (remaining > 0) {
402: 		auto append_count = MinValue<idx_t>(remaining, Storage::ROW_GROUP_SIZE - row_group->count);
403: 		row_group->AppendVersionInfo(transaction, append_count);
404: 		remaining -= append_count;
405: 		row_group = row_groups->GetNextSegment(row_group);
406: 	}
407: 	total_rows += state.total_append_count;
408: 
409: 	state.total_append_count = 0;
410: 	state.start_row_group = nullptr;
411: 
412: 	auto global_stats_lock = stats.GetLock();
413: 	auto local_stats_lock = state.stats.GetLock();
414: 	for (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {
415: 		auto &global_stats = stats.GetStats(*global_stats_lock, col_idx);
416: 		if (!global_stats.HasDistinctStats()) {
417: 			continue;
418: 		}
419: 		auto &local_stats = state.stats.GetStats(*local_stats_lock, col_idx);
420: 		if (!local_stats.HasDistinctStats()) {
421: 			continue;
422: 		}
423: 		global_stats.DistinctStats().Merge(local_stats.DistinctStats());
424: 	}
425: 
426: 	Verify();
427: }
428: 
429: void RowGroupCollection::CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count) {
430: 	auto row_group = row_groups->GetSegment(row_start);
431: 	D_ASSERT(row_group);
432: 	idx_t current_row = row_start;
433: 	idx_t remaining = count;
434: 	while (true) {
435: 		idx_t start_in_row_group = current_row - row_group->start;
436: 		idx_t append_count = MinValue<idx_t>(row_group->count - start_in_row_group, remaining);
437: 
438: 		row_group->CommitAppend(commit_id, start_in_row_group, append_count);
439: 
440: 		current_row += append_count;
441: 		remaining -= append_count;
442: 		if (remaining == 0) {
443: 			break;
444: 		}
445: 		row_group = row_groups->GetNextSegment(row_group);
446: 	}
447: }
448: 
449: void RowGroupCollection::RevertAppendInternal(idx_t start_row) {
450: 	total_rows = start_row;
451: 
452: 	auto l = row_groups->Lock();
453: 	idx_t segment_count = row_groups->GetSegmentCount(l);
454: 	if (segment_count == 0) {
455: 		// we have no segments to revert
456: 		return;
457: 	}
458: 	idx_t segment_index;
459: 	// find the segment index that the start row belongs to
460: 	if (!row_groups->TryGetSegmentIndex(l, start_row, segment_index)) {
461: 		// revert from the last segment
462: 		segment_index = segment_count - 1;
463: 	}
464: 	auto &segment = *row_groups->GetSegmentByIndex(l, UnsafeNumericCast<int64_t>(segment_index));
465: 
466: 	// remove any segments AFTER this segment: they should be deleted entirely
467: 	row_groups->EraseSegments(l, segment_index);
468: 
469: 	segment.next = nullptr;
470: 	segment.RevertAppend(start_row);
471: }
472: 
473: void RowGroupCollection::CleanupAppend(transaction_t lowest_transaction, idx_t start, idx_t count) {
474: 	auto row_group = row_groups->GetSegment(start);
475: 	D_ASSERT(row_group);
476: 	idx_t current_row = start;
477: 	idx_t remaining = count;
478: 	while (true) {
479: 		idx_t start_in_row_group = current_row - row_group->start;
480: 		idx_t append_count = MinValue<idx_t>(row_group->count - start_in_row_group, remaining);
481: 
482: 		row_group->CleanupAppend(lowest_transaction, start_in_row_group, append_count);
483: 
484: 		current_row += append_count;
485: 		remaining -= append_count;
486: 		if (remaining == 0) {
487: 			break;
488: 		}
489: 		row_group = row_groups->GetNextSegment(row_group);
490: 	}
491: }
492: 
493: bool RowGroupCollection::IsPersistent() const {
494: 	for (auto &row_group : row_groups->Segments()) {
495: 		if (!row_group.IsPersistent()) {
496: 			return false;
497: 		}
498: 	}
499: 	return true;
500: }
501: 
502: void RowGroupCollection::MergeStorage(RowGroupCollection &data, optional_ptr<DataTable> table,
503:                                       optional_ptr<StorageCommitState> commit_state) {
504: 	D_ASSERT(data.types == types);
505: 	auto start_index = row_start + total_rows.load();
506: 	auto index = start_index;
507: 	auto segments = data.row_groups->MoveSegments();
508: 
509: 	// check if the row groups we are merging are optimistically written
510: 	// if all row groups are optimistically written we keep around the block pointers
511: 	unique_ptr<PersistentCollectionData> row_group_data;
512: 	idx_t optimistically_written_count = 0;
513: 	if (commit_state) {
514: 		for (auto &entry : segments) {
515: 			auto &row_group = *entry.node;
516: 			if (!row_group.IsPersistent()) {
517: 				break;
518: 			}
519: 			optimistically_written_count += row_group.count;
520: 		}
521: 		if (optimistically_written_count > 0) {
522: 			row_group_data = make_uniq<PersistentCollectionData>();
523: 		}
524: 	}
525: 	for (auto &entry : segments) {
526: 		auto &row_group = entry.node;
527: 		row_group->MoveToCollection(*this, index);
528: 
529: 		if (commit_state && (index - start_index) < optimistically_written_count) {
530: 			// serialize the block pointers of this row group
531: 			auto persistent_data = row_group->SerializeRowGroupInfo();
532: 			persistent_data.types = types;
533: 			row_group_data->row_group_data.push_back(std::move(persistent_data));
534: 		}
535: 		index += row_group->count;
536: 		row_groups->AppendSegment(std::move(row_group));
537: 	}
538: 	if (commit_state && optimistically_written_count > 0) {
539: 		// if we have serialized the row groups - push the serialized block pointers into the commit state
540: 		commit_state->AddRowGroupData(*table, start_index, optimistically_written_count, std::move(row_group_data));
541: 	}
542: 	stats.MergeStats(data.stats);
543: 	total_rows += data.total_rows.load();
544: }
545: 
546: //===--------------------------------------------------------------------===//
547: // Delete
548: //===--------------------------------------------------------------------===//
549: idx_t RowGroupCollection::Delete(TransactionData transaction, DataTable &table, row_t *ids, idx_t count) {
550: 	idx_t delete_count = 0;
551: 	// delete is in the row groups
552: 	// we need to figure out for each id to which row group it belongs
553: 	// usually all (or many) ids belong to the same row group
554: 	// we iterate over the ids and check for every id if it belongs to the same row group as their predecessor
555: 	idx_t pos = 0;
556: 	do {
557: 		idx_t start = pos;
558: 		auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(ids[start]));
559: 		for (pos++; pos < count; pos++) {
560: 			D_ASSERT(ids[pos] >= 0);
561: 			// check if this id still belongs to this row group
562: 			if (idx_t(ids[pos]) < row_group->start) {
563: 				// id is before row_group start -> it does not
564: 				break;
565: 			}
566: 			if (idx_t(ids[pos]) >= row_group->start + row_group->count) {
567: 				// id is after row group end -> it does not
568: 				break;
569: 			}
570: 		}
571: 		delete_count += row_group->Delete(transaction, table, ids + start, pos - start);
572: 	} while (pos < count);
573: 	return delete_count;
574: }
575: 
576: //===--------------------------------------------------------------------===//
577: // Update
578: //===--------------------------------------------------------------------===//
579: void RowGroupCollection::Update(TransactionData transaction, row_t *ids, const vector<PhysicalIndex> &column_ids,
580:                                 DataChunk &updates) {
581: 	idx_t pos = 0;
582: 	do {
583: 		idx_t start = pos;
584: 		auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(ids[pos]));
585: 		row_t base_id =
586: 		    UnsafeNumericCast<row_t>(row_group->start + ((UnsafeNumericCast<idx_t>(ids[pos]) - row_group->start) /
587: 		                                                 STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE));
588: 		auto max_id = MinValue<row_t>(base_id + STANDARD_VECTOR_SIZE,
589: 		                              UnsafeNumericCast<row_t>(row_group->start + row_group->count));
590: 		for (pos++; pos < updates.size(); pos++) {
591: 			D_ASSERT(ids[pos] >= 0);
592: 			// check if this id still belongs to this vector in this row group
593: 			if (ids[pos] < base_id) {
594: 				// id is before vector start -> it does not
595: 				break;
596: 			}
597: 			if (ids[pos] >= max_id) {
598: 				// id is after the maximum id in this vector -> it does not
599: 				break;
600: 			}
601: 		}
602: 		row_group->Update(transaction, updates, ids, start, pos - start, column_ids);
603: 
604: 		auto l = stats.GetLock();
605: 		for (idx_t i = 0; i < column_ids.size(); i++) {
606: 			auto column_id = column_ids[i];
607: 			stats.MergeStats(*l, column_id.index, *row_group->GetStatistics(column_id.index));
608: 		}
609: 	} while (pos < updates.size());
610: }
611: 
612: void RowGroupCollection::RemoveFromIndexes(TableIndexList &indexes, Vector &row_identifiers, idx_t count) {
613: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
614: 
615: 	// initialize the fetch state
616: 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
617: 	TableScanState state;
618: 	vector<column_t> column_ids;
619: 	column_ids.reserve(types.size());
620: 	for (idx_t i = 0; i < types.size(); i++) {
621: 		column_ids.push_back(i);
622: 	}
623: 	state.Initialize(std::move(column_ids));
624: 	state.table_state.max_row = row_start + total_rows;
625: 
626: 	// initialize the fetch chunk
627: 	DataChunk result;
628: 	result.Initialize(GetAllocator(), types);
629: 
630: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
631: 	// now iterate over the row ids
632: 	for (idx_t r = 0; r < count;) {
633: 		result.Reset();
634: 		// figure out which row_group to fetch from
635: 		auto row_id = row_ids[r];
636: 		auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(row_id));
637: 		auto row_group_vector_idx = (UnsafeNumericCast<idx_t>(row_id) - row_group->start) / STANDARD_VECTOR_SIZE;
638: 		auto base_row_id = row_group_vector_idx * STANDARD_VECTOR_SIZE + row_group->start;
639: 
640: 		// fetch the current vector
641: 		state.table_state.Initialize(GetTypes());
642: 		row_group->InitializeScanWithOffset(state.table_state, row_group_vector_idx);
643: 		row_group->ScanCommitted(state.table_state, result, TableScanType::TABLE_SCAN_COMMITTED_ROWS);
644: 		result.Verify();
645: 
646: 		// check for any remaining row ids if they also fall into this vector
647: 		// we try to fetch handle as many rows as possible at the same time
648: 		idx_t sel_count = 0;
649: 		for (; r < count; r++) {
650: 			idx_t current_row = idx_t(row_ids[r]);
651: 			if (current_row < base_row_id || current_row >= base_row_id + result.size()) {
652: 				// this row-id does not fall into the current chunk - break
653: 				break;
654: 			}
655: 			auto row_in_vector = current_row - base_row_id;
656: 			D_ASSERT(row_in_vector < result.size());
657: 			sel.set_index(sel_count++, row_in_vector);
658: 		}
659: 		D_ASSERT(sel_count > 0);
660: 		// slice the vector with all rows that are present in this vector and erase from the index
661: 		result.Slice(sel, sel_count);
662: 
663: 		indexes.Scan([&](Index &index) {
664: 			if (index.IsBound()) {
665: 				index.Cast<BoundIndex>().Delete(result, row_identifiers);
666: 			} else {
667: 				throw MissingExtensionException(
668: 				    "Cannot delete from index '%s', unknown index type '%s'. You need to load the "
669: 				    "extension that provides this index type before table '%s' can be modified.",
670: 				    index.GetIndexName(), index.GetIndexType(), info->GetTableName());
671: 			}
672: 			return false;
673: 		});
674: 	}
675: }
676: 
677: void RowGroupCollection::UpdateColumn(TransactionData transaction, Vector &row_ids, const vector<column_t> &column_path,
678:                                       DataChunk &updates) {
679: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
680: 	if (first_id >= MAX_ROW_ID) {
681: 		throw NotImplementedException("Cannot update a column-path on transaction local data");
682: 	}
683: 	// find the row_group this id belongs to
684: 	auto primary_column_idx = column_path[0];
685: 	auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(first_id));
686: 	row_group->UpdateColumn(transaction, updates, row_ids, column_path);
687: 
688: 	auto lock = stats.GetLock();
689: 	row_group->MergeIntoStatistics(primary_column_idx, stats.GetStats(*lock, primary_column_idx).Statistics());
690: }
691: 
692: //===--------------------------------------------------------------------===//
693: // Checkpoint State
694: //===--------------------------------------------------------------------===//
695: struct CollectionCheckpointState {
696: 	CollectionCheckpointState(RowGroupCollection &collection, TableDataWriter &writer,
697: 	                          vector<SegmentNode<RowGroup>> &segments, TableStatistics &global_stats)
698: 	    : collection(collection), writer(writer), executor(writer.GetScheduler()), segments(segments),
699: 	      global_stats(global_stats) {
700: 		writers.resize(segments.size());
701: 		write_data.resize(segments.size());
702: 	}
703: 
704: 	RowGroupCollection &collection;
705: 	TableDataWriter &writer;
706: 	TaskExecutor executor;
707: 	vector<SegmentNode<RowGroup>> &segments;
708: 	vector<unique_ptr<RowGroupWriter>> writers;
709: 	vector<RowGroupWriteData> write_data;
710: 	TableStatistics &global_stats;
711: 	mutex write_lock;
712: };
713: 
714: class BaseCheckpointTask : public BaseExecutorTask {
715: public:
716: 	explicit BaseCheckpointTask(CollectionCheckpointState &checkpoint_state)
717: 	    : BaseExecutorTask(checkpoint_state.executor), checkpoint_state(checkpoint_state) {
718: 	}
719: 
720: protected:
721: 	CollectionCheckpointState &checkpoint_state;
722: };
723: 
724: class CheckpointTask : public BaseCheckpointTask {
725: public:
726: 	CheckpointTask(CollectionCheckpointState &checkpoint_state, idx_t index)
727: 	    : BaseCheckpointTask(checkpoint_state), index(index) {
728: 	}
729: 
730: 	void ExecuteTask() override {
731: 		auto &entry = checkpoint_state.segments[index];
732: 		auto &row_group = *entry.node;
733: 		checkpoint_state.writers[index] = checkpoint_state.writer.GetRowGroupWriter(*entry.node);
734: 		checkpoint_state.write_data[index] = row_group.WriteToDisk(*checkpoint_state.writers[index]);
735: 	}
736: 
737: private:
738: 	idx_t index;
739: };
740: 
741: //===--------------------------------------------------------------------===//
742: // Vacuum
743: //===--------------------------------------------------------------------===//
744: struct VacuumState {
745: 	bool can_vacuum_deletes = false;
746: 	idx_t row_start = 0;
747: 	idx_t next_vacuum_idx = 0;
748: 	vector<idx_t> row_group_counts;
749: };
750: 
751: class VacuumTask : public BaseCheckpointTask {
752: public:
753: 	VacuumTask(CollectionCheckpointState &checkpoint_state, VacuumState &vacuum_state, idx_t segment_idx,
754: 	           idx_t merge_count, idx_t target_count, idx_t merge_rows, idx_t row_start)
755: 	    : BaseCheckpointTask(checkpoint_state), vacuum_state(vacuum_state), segment_idx(segment_idx),
756: 	      merge_count(merge_count), target_count(target_count), merge_rows(merge_rows), row_start(row_start) {
757: 	}
758: 
759: 	void ExecuteTask() override {
760: 		auto &collection = checkpoint_state.collection;
761: 		auto &types = collection.GetTypes();
762: 		// create the new set of target row groups (initially empty)
763: 		vector<unique_ptr<RowGroup>> new_row_groups;
764: 		vector<idx_t> append_counts;
765: 		idx_t row_group_rows = merge_rows;
766: 		idx_t start = row_start;
767: 		for (idx_t target_idx = 0; target_idx < target_count; target_idx++) {
768: 			idx_t current_row_group_rows = MinValue<idx_t>(row_group_rows, Storage::ROW_GROUP_SIZE);
769: 			auto new_row_group = make_uniq<RowGroup>(collection, start, current_row_group_rows);
770: 			new_row_group->InitializeEmpty(types);
771: 			new_row_groups.push_back(std::move(new_row_group));
772: 			append_counts.push_back(0);
773: 
774: 			row_group_rows -= current_row_group_rows;
775: 			start += current_row_group_rows;
776: 		}
777: 
778: 		DataChunk scan_chunk;
779: 		scan_chunk.Initialize(Allocator::DefaultAllocator(), types);
780: 
781: 		vector<column_t> column_ids;
782: 		for (idx_t c = 0; c < types.size(); c++) {
783: 			column_ids.push_back(c);
784: 		}
785: 
786: 		idx_t current_append_idx = 0;
787: 
788: 		// fill the new row group with the merged rows
789: 		TableAppendState append_state;
790: 		new_row_groups[current_append_idx]->InitializeAppend(append_state.row_group_append_state);
791: 
792: 		TableScanState scan_state;
793: 		scan_state.Initialize(column_ids);
794: 		scan_state.table_state.Initialize(types);
795: 		scan_state.table_state.max_row = idx_t(-1);
796: 		idx_t merged_groups = 0;
797: 		idx_t total_row_groups = vacuum_state.row_group_counts.size();
798: 		for (idx_t c_idx = segment_idx; merged_groups < merge_count && c_idx < total_row_groups; c_idx++) {
799: 			if (vacuum_state.row_group_counts[c_idx] == 0) {
800: 				continue;
801: 			}
802: 			merged_groups++;
803: 
804: 			auto &current_row_group = *checkpoint_state.segments[c_idx].node;
805: 
806: 			current_row_group.InitializeScan(scan_state.table_state);
807: 			while (true) {
808: 				scan_chunk.Reset();
809: 
810: 				current_row_group.ScanCommitted(scan_state.table_state, scan_chunk,
811: 				                                TableScanType::TABLE_SCAN_LATEST_COMMITTED_ROWS);
812: 				if (scan_chunk.size() == 0) {
813: 					break;
814: 				}
815: 				scan_chunk.Flatten();
816: 				idx_t remaining = scan_chunk.size();
817: 				while (remaining > 0) {
818: 					idx_t append_count =
819: 					    MinValue<idx_t>(remaining, Storage::ROW_GROUP_SIZE - append_counts[current_append_idx]);
820: 					new_row_groups[current_append_idx]->Append(append_state.row_group_append_state, scan_chunk,
821: 					                                           append_count);
822: 					append_counts[current_append_idx] += append_count;
823: 					remaining -= append_count;
824: 					const bool row_group_full = append_counts[current_append_idx] == Storage::ROW_GROUP_SIZE;
825: 					const bool last_row_group = current_append_idx + 1 >= new_row_groups.size();
826: 					if (remaining > 0 || (row_group_full && !last_row_group)) {
827: 						// move to the next row group
828: 						current_append_idx++;
829: 						new_row_groups[current_append_idx]->InitializeAppend(append_state.row_group_append_state);
830: 						// slice chunk for the next append
831: 						scan_chunk.Slice(append_count, remaining);
832: 					}
833: 				}
834: 			}
835: 			// drop the row group after merging
836: 			current_row_group.CommitDrop();
837: 			checkpoint_state.segments[c_idx].node.reset();
838: 		}
839: 		idx_t total_append_count = 0;
840: 		for (idx_t target_idx = 0; target_idx < target_count; target_idx++) {
841: 			auto &row_group = new_row_groups[target_idx];
842: 			row_group->Verify();
843: 
844: 			// assign the new row group to the current segment
845: 			checkpoint_state.segments[segment_idx + target_idx].node = std::move(row_group);
846: 			total_append_count += append_counts[target_idx];
847: 		}
848: 		if (total_append_count != merge_rows) {
849: 			throw InternalException("Mismatch in row group count vs verify count in RowGroupCollection::Checkpoint");
850: 		}
851: 		// merging is complete - execute checkpoint tasks of the target row groups
852: 		for (idx_t i = 0; i < target_count; i++) {
853: 			auto checkpoint_task = collection.GetCheckpointTask(checkpoint_state, segment_idx + i);
854: 			checkpoint_task->ExecuteTask();
855: 		}
856: 	}
857: 
858: private:
859: 	VacuumState &vacuum_state;
860: 	idx_t segment_idx;
861: 	idx_t merge_count;
862: 	idx_t target_count;
863: 	idx_t merge_rows;
864: 	idx_t row_start;
865: };
866: 
867: void RowGroupCollection::InitializeVacuumState(CollectionCheckpointState &checkpoint_state, VacuumState &state,
868:                                                vector<SegmentNode<RowGroup>> &segments) {
869: 	bool is_full_checkpoint = checkpoint_state.writer.GetCheckpointType() == CheckpointType::FULL_CHECKPOINT;
870: 	// currently we can only vacuum deletes if we are doing a full checkpoint and there are no indexes
871: 	state.can_vacuum_deletes = info->GetIndexes().Empty() && is_full_checkpoint;
872: 	if (!state.can_vacuum_deletes) {
873: 		return;
874: 	}
875: 	// obtain the set of committed row counts for each row group
876: 	state.row_group_counts.reserve(segments.size());
877: 	for (auto &entry : segments) {
878: 		auto &row_group = *entry.node;
879: 		auto row_group_count = row_group.GetCommittedRowCount();
880: 		if (row_group_count == 0) {
881: 			// empty row group - we can drop it entirely
882: 			row_group.CommitDrop();
883: 			entry.node.reset();
884: 		}
885: 		state.row_group_counts.push_back(row_group_count);
886: 	}
887: }
888: 
889: bool RowGroupCollection::ScheduleVacuumTasks(CollectionCheckpointState &checkpoint_state, VacuumState &state,
890:                                              idx_t segment_idx, bool schedule_vacuum) {
891: 	static constexpr const idx_t MAX_MERGE_COUNT = 3;
892: 
893: 	if (!state.can_vacuum_deletes) {
894: 		// we cannot vacuum deletes - cannot vacuum
895: 		return false;
896: 	}
897: 	if (segment_idx < state.next_vacuum_idx) {
898: 		// this segment is being vacuumed by a previously scheduled task
899: 		return true;
900: 	}
901: 	if (state.row_group_counts[segment_idx] == 0) {
902: 		// segment was already dropped - skip
903: 		D_ASSERT(!checkpoint_state.segments[segment_idx].node);
904: 		return false;
905: 	}
906: 	if (!schedule_vacuum) {
907: 		return false;
908: 	}
909: 	idx_t merge_rows;
910: 	idx_t next_idx = 0;
911: 	idx_t merge_count;
912: 	idx_t target_count;
913: 	bool perform_merge = false;
914: 	// check if we can merge row groups adjacent to the current segment_idx
915: 	// we try merging row groups into batches of 1-3 row groups
916: 	// our goal is to reduce the amount of row groups
917: 	// hence we target_count should be less than merge_count for a marge to be worth it
918: 	// we greedily prefer to merge to the lowest target_count
919: 	// i.e. we prefer to merge 2 row groups into 1, than 3 row groups into 2
920: 	for (target_count = 1; target_count <= MAX_MERGE_COUNT; target_count++) {
921: 		auto total_target_size = target_count * Storage::ROW_GROUP_SIZE;
922: 		merge_count = 0;
923: 		merge_rows = 0;
924: 		for (next_idx = segment_idx; next_idx < checkpoint_state.segments.size(); next_idx++) {
925: 			if (state.row_group_counts[next_idx] == 0) {
926: 				continue;
927: 			}
928: 			if (merge_rows + state.row_group_counts[next_idx] > total_target_size) {
929: 				// does not fit
930: 				break;
931: 			}
932: 			// we can merge this row group together with the other row group
933: 			merge_rows += state.row_group_counts[next_idx];
934: 			merge_count++;
935: 		}
936: 		if (target_count < merge_count) {
937: 			// we can reduce "merge_count" row groups to "target_count"
938: 			// perform the merge at this level
939: 			perform_merge = true;
940: 			break;
941: 		}
942: 	}
943: 	if (!perform_merge) {
944: 		return false;
945: 	}
946: 	// schedule the vacuum task
947: 	auto vacuum_task = make_uniq<VacuumTask>(checkpoint_state, state, segment_idx, merge_count, target_count,
948: 	                                         merge_rows, state.row_start);
949: 	checkpoint_state.executor.ScheduleTask(std::move(vacuum_task));
950: 	// skip vacuuming by the row groups we have merged
951: 	state.next_vacuum_idx = next_idx;
952: 	state.row_start += merge_rows;
953: 	return true;
954: }
955: 
956: //===--------------------------------------------------------------------===//
957: // Checkpoint
958: //===--------------------------------------------------------------------===//
959: unique_ptr<CheckpointTask> RowGroupCollection::GetCheckpointTask(CollectionCheckpointState &checkpoint_state,
960:                                                                  idx_t segment_idx) {
961: 	return make_uniq<CheckpointTask>(checkpoint_state, segment_idx);
962: }
963: 
964: void RowGroupCollection::Checkpoint(TableDataWriter &writer, TableStatistics &global_stats) {
965: 	auto segments = row_groups->MoveSegments();
966: 	auto l = row_groups->Lock();
967: 
968: 	CollectionCheckpointState checkpoint_state(*this, writer, segments, global_stats);
969: 
970: 	VacuumState vacuum_state;
971: 	InitializeVacuumState(checkpoint_state, vacuum_state, segments);
972: 	// schedule tasks
973: 	idx_t total_vacuum_tasks = 0;
974: 	auto &config = DBConfig::GetConfig(writer.GetDatabase());
975: 	for (idx_t segment_idx = 0; segment_idx < segments.size(); segment_idx++) {
976: 		auto &entry = segments[segment_idx];
977: 		auto vacuum_tasks = ScheduleVacuumTasks(checkpoint_state, vacuum_state, segment_idx,
978: 		                                        total_vacuum_tasks < config.options.max_vacuum_tasks);
979: 		if (vacuum_tasks) {
980: 			// vacuum tasks were scheduled - don't schedule a checkpoint task yet
981: 			total_vacuum_tasks++;
982: 			continue;
983: 		}
984: 		if (!entry.node) {
985: 			// row group was vacuumed/dropped - skip
986: 			continue;
987: 		}
988: 		// schedule a checkpoint task for this row group
989: 		entry.node->MoveToCollection(*this, vacuum_state.row_start);
990: 		auto checkpoint_task = GetCheckpointTask(checkpoint_state, segment_idx);
991: 		checkpoint_state.executor.ScheduleTask(std::move(checkpoint_task));
992: 		vacuum_state.row_start += entry.node->count;
993: 	}
994: 	// all tasks have been scheduled - execute tasks until we are done
995: 	checkpoint_state.executor.WorkOnTasks();
996: 
997: 	// no errors - finalize the row groups
998: 	idx_t new_total_rows = 0;
999: 	for (idx_t segment_idx = 0; segment_idx < segments.size(); segment_idx++) {
1000: 		auto &entry = segments[segment_idx];
1001: 		if (!entry.node) {
1002: 			// row group was vacuumed/dropped - skip
1003: 			continue;
1004: 		}
1005: 		auto &row_group = *entry.node;
1006: 		auto row_group_writer = std::move(checkpoint_state.writers[segment_idx]);
1007: 		if (!row_group_writer) {
1008: 			throw InternalException("Missing row group writer for index %llu", segment_idx);
1009: 		}
1010: 		auto pointer =
1011: 		    row_group.Checkpoint(std::move(checkpoint_state.write_data[segment_idx]), *row_group_writer, global_stats);
1012: 		writer.AddRowGroup(std::move(pointer), std::move(row_group_writer));
1013: 		row_groups->AppendSegment(l, std::move(entry.node));
1014: 		new_total_rows += row_group.count;
1015: 	}
1016: 	total_rows = new_total_rows;
1017: }
1018: 
1019: //===--------------------------------------------------------------------===//
1020: // CommitDrop
1021: //===--------------------------------------------------------------------===//
1022: void RowGroupCollection::CommitDropColumn(idx_t index) {
1023: 	for (auto &row_group : row_groups->Segments()) {
1024: 		row_group.CommitDropColumn(index);
1025: 	}
1026: }
1027: 
1028: void RowGroupCollection::CommitDropTable() {
1029: 	for (auto &row_group : row_groups->Segments()) {
1030: 		row_group.CommitDrop();
1031: 	}
1032: }
1033: 
1034: //===--------------------------------------------------------------------===//
1035: // GetColumnSegmentInfo
1036: //===--------------------------------------------------------------------===//
1037: vector<ColumnSegmentInfo> RowGroupCollection::GetColumnSegmentInfo() {
1038: 	vector<ColumnSegmentInfo> result;
1039: 	for (auto &row_group : row_groups->Segments()) {
1040: 		row_group.GetColumnSegmentInfo(row_group.index, result);
1041: 	}
1042: 	return result;
1043: }
1044: 
1045: //===--------------------------------------------------------------------===//
1046: // Alter
1047: //===--------------------------------------------------------------------===//
1048: shared_ptr<RowGroupCollection> RowGroupCollection::AddColumn(ClientContext &context, ColumnDefinition &new_column,
1049:                                                              ExpressionExecutor &default_executor) {
1050: 	idx_t new_column_idx = types.size();
1051: 	auto new_types = types;
1052: 	new_types.push_back(new_column.GetType());
1053: 	auto result =
1054: 	    make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), row_start, total_rows.load());
1055: 
1056: 	DataChunk dummy_chunk;
1057: 	Vector default_vector(new_column.GetType());
1058: 
1059: 	result->stats.InitializeAddColumn(stats, new_column.GetType());
1060: 	auto lock = result->stats.GetLock();
1061: 	auto &new_column_stats = result->stats.GetStats(*lock, new_column_idx);
1062: 
1063: 	// fill the column with its DEFAULT value, or NULL if none is specified
1064: 	auto new_stats = make_uniq<SegmentStatistics>(new_column.GetType());
1065: 	for (auto &current_row_group : row_groups->Segments()) {
1066: 		auto new_row_group = current_row_group.AddColumn(*result, new_column, default_executor, default_vector);
1067: 		// merge in the statistics
1068: 		new_row_group->MergeIntoStatistics(new_column_idx, new_column_stats.Statistics());
1069: 
1070: 		result->row_groups->AppendSegment(std::move(new_row_group));
1071: 	}
1072: 	return result;
1073: }
1074: 
1075: shared_ptr<RowGroupCollection> RowGroupCollection::RemoveColumn(idx_t col_idx) {
1076: 	D_ASSERT(col_idx < types.size());
1077: 	auto new_types = types;
1078: 	new_types.erase_at(col_idx);
1079: 
1080: 	auto result =
1081: 	    make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), row_start, total_rows.load());
1082: 	result->stats.InitializeRemoveColumn(stats, col_idx);
1083: 
1084: 	for (auto &current_row_group : row_groups->Segments()) {
1085: 		auto new_row_group = current_row_group.RemoveColumn(*result, col_idx);
1086: 		result->row_groups->AppendSegment(std::move(new_row_group));
1087: 	}
1088: 	return result;
1089: }
1090: 
1091: shared_ptr<RowGroupCollection> RowGroupCollection::AlterType(ClientContext &context, idx_t changed_idx,
1092:                                                              const LogicalType &target_type,
1093:                                                              vector<column_t> bound_columns, Expression &cast_expr) {
1094: 	D_ASSERT(changed_idx < types.size());
1095: 	auto new_types = types;
1096: 	new_types[changed_idx] = target_type;
1097: 
1098: 	auto result =
1099: 	    make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), row_start, total_rows.load());
1100: 	result->stats.InitializeAlterType(stats, changed_idx, target_type);
1101: 
1102: 	vector<LogicalType> scan_types;
1103: 	for (idx_t i = 0; i < bound_columns.size(); i++) {
1104: 		if (bound_columns[i] == COLUMN_IDENTIFIER_ROW_ID) {
1105: 			scan_types.emplace_back(LogicalType::ROW_TYPE);
1106: 		} else {
1107: 			scan_types.push_back(types[bound_columns[i]]);
1108: 		}
1109: 	}
1110: 	DataChunk scan_chunk;
1111: 	scan_chunk.Initialize(GetAllocator(), scan_types);
1112: 
1113: 	ExpressionExecutor executor(context);
1114: 	executor.AddExpression(cast_expr);
1115: 
1116: 	TableScanState scan_state;
1117: 	scan_state.Initialize(bound_columns);
1118: 	scan_state.table_state.max_row = row_start + total_rows;
1119: 
1120: 	// now alter the type of the column within all of the row_groups individually
1121: 	auto lock = result->stats.GetLock();
1122: 	auto &changed_stats = result->stats.GetStats(*lock, changed_idx);
1123: 	for (auto &current_row_group : row_groups->Segments()) {
1124: 		auto new_row_group = current_row_group.AlterType(*result, target_type, changed_idx, executor,
1125: 		                                                 scan_state.table_state, scan_chunk);
1126: 		new_row_group->MergeIntoStatistics(changed_idx, changed_stats.Statistics());
1127: 		result->row_groups->AppendSegment(std::move(new_row_group));
1128: 	}
1129: 
1130: 	return result;
1131: }
1132: 
1133: void RowGroupCollection::VerifyNewConstraint(DataTable &parent, const BoundConstraint &constraint) {
1134: 	if (total_rows == 0) {
1135: 		return;
1136: 	}
1137: 	// scan the original table, check if there's any null value
1138: 	auto &not_null_constraint = constraint.Cast<BoundNotNullConstraint>();
1139: 	vector<LogicalType> scan_types;
1140: 	auto physical_index = not_null_constraint.index.index;
1141: 	D_ASSERT(physical_index < types.size());
1142: 	scan_types.push_back(types[physical_index]);
1143: 	DataChunk scan_chunk;
1144: 	scan_chunk.Initialize(GetAllocator(), scan_types);
1145: 
1146: 	CreateIndexScanState state;
1147: 	vector<column_t> cids;
1148: 	cids.push_back(physical_index);
1149: 	// Use ScanCommitted to scan the latest committed data
1150: 	state.Initialize(cids, nullptr);
1151: 	InitializeScan(state.table_state, cids, nullptr);
1152: 	InitializeCreateIndexScan(state);
1153: 	while (true) {
1154: 		scan_chunk.Reset();
1155: 		state.table_state.ScanCommitted(scan_chunk, state.segment_lock,
1156: 		                                TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED);
1157: 		if (scan_chunk.size() == 0) {
1158: 			break;
1159: 		}
1160: 		// Check constraint
1161: 		if (VectorOperations::HasNull(scan_chunk.data[0], scan_chunk.size())) {
1162: 			throw ConstraintException("NOT NULL constraint failed: %s.%s", info->GetTableName(),
1163: 			                          parent.Columns()[physical_index].GetName());
1164: 		}
1165: 	}
1166: }
1167: 
1168: //===--------------------------------------------------------------------===//
1169: // Statistics
1170: //===--------------------------------------------------------------------===//
1171: void RowGroupCollection::CopyStats(TableStatistics &other_stats) {
1172: 	stats.CopyStats(other_stats);
1173: }
1174: 
1175: unique_ptr<BaseStatistics> RowGroupCollection::CopyStats(column_t column_id) {
1176: 	return stats.CopyStats(column_id);
1177: }
1178: 
1179: void RowGroupCollection::SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats) {
1180: 	D_ASSERT(column_id != COLUMN_IDENTIFIER_ROW_ID);
1181: 	auto stats_lock = stats.GetLock();
1182: 	stats.GetStats(*stats_lock, column_id).SetDistinct(std::move(distinct_stats));
1183: }
1184: 
1185: } // namespace duckdb
[end of src/storage/table/row_group_collection.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: