{
  "repo": "duckdb/duckdb",
  "pull_number": 13345,
  "instance_id": "duckdb__duckdb-13345",
  "issue_numbers": [
    "14077",
    "14077"
  ],
  "base_commit": "4d054051cdc4d9cdd7cc133c5236cf14782489e1",
  "patch": "diff --git a/.github/regression/realnest.csv b/.github/regression/realnest.csv\nnew file mode 100644\nindex 000000000000..e63a1d144dc9\n--- /dev/null\n+++ b/.github/regression/realnest.csv\n@@ -0,0 +1,16 @@\n+benchmark/realnest/01_aggregate-first-level-struct-members.benchmark\n+benchmark/realnest/02_list_sort.benchmark\n+benchmark/realnest/03_create_table_from_unnested_structs.benchmark\n+benchmark/realnest/04_list_transform_and_list_aggregate.benchmark\n+benchmark/realnest/05_list_filter.benchmark\n+benchmark/realnest/06_list_filter_on_unnested_structure.benchmark\n+benchmark/realnest/07_list_unique_on_transformed_and_aggregated_list.benchmark\n+benchmark/realnest/08_count_map_keys.benchmark\n+benchmark/realnest/09_array_agg.benchmark\n+benchmark/realnest/11_list_sort_reduce_transform.benchmark\n+benchmark/realnest/12_map_list_values.benchmark\n+benchmark/realnest/13_multi_join_nested_data_with_filtering.benchmark\n+benchmark/realnest/14_list_slice.benchmark\n+benchmark/realnest/15_list_sort.benchmark\n+benchmark/realnest/16_most_common_list_aggregates.benchmark\n+benchmark/realnest/17_list_aggregates_histogram_stddev_mode.benchmark\n\\ No newline at end of file\ndiff --git a/.github/workflows/Regression.yml b/.github/workflows/Regression.yml\nindex 25b5a470f77a..b1cf112b10a2 100644\n--- a/.github/workflows/Regression.yml\n+++ b/.github/workflows/Regression.yml\n@@ -35,300 +35,311 @@ env:\n   BASE_BRANCH: ${{ github.base_ref || (endsWith(github.ref, '_feature') && 'feature' || 'main') }}\n \n jobs:\n- regression-test-benchmark-runner:\n-  name: Regression Tests\n-  runs-on: ubuntu-20.04\n-  env:\n-    CC: gcc-10\n-    CXX: g++-10\n-    GEN: ninja\n-    BUILD_BENCHMARK: 1\n-    BUILD_TPCH: 1\n-    BUILD_TPCDS: 1\n-    BUILD_HTTPFS: 1\n-    BUILD_JEMALLOC: 1\n-    CORE_EXTENSIONS: \"inet\"\n-\n-  steps:\n-    - uses: actions/checkout@v4\n-      with:\n-        fetch-depth: 0\n-\n-    - uses: actions/setup-python@v5\n-      with:\n-        python-version: '3.12'\n-\n-    - name: Install\n-      shell: bash\n-      run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install requests\n-\n-    - name: Setup Ccache\n-      uses: hendrikmuhs/ccache-action@main\n-      with:\n-        key: ${{ github.job }}\n-        save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n-\n-    - name: Build\n-      shell: bash\n-      run: |\n-        make\n-        git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1\n-        cd duckdb\n-        make\n-        cd ..\n-\n-    - name: Set up benchmarks\n-      shell: bash\n-      run: |\n-        cp -r benchmark duckdb/\n-\n-    - name: Regression Test Micro\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/micro.csv --verbose --threads=2\n-\n-    - name: Regression Test Ingestion Perf\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/ingestion.csv --verbose --threads=2\n-\n-    - name: Regression Test TPCH\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/tpch.csv --verbose --threads=2\n-\n-    - name: Regression Test TPCH-PARQUET\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/tpch_parquet.csv --verbose --threads=2\n-\n-    - name: Regression Test TPCDS\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/tpcds.csv --verbose --threads=2\n-\n-    - name: Regression Test H2OAI\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/h2oai.csv --verbose --threads=2\n-\n-    - name: Regression Test IMDB\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/imdb.csv --verbose --threads=2\n-\n-    - name: Regression Test CSV\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/csv.csv --verbose --threads=2\n-\n- regression-test-storage:\n-  name: Storage Size Regression Test\n-  runs-on: ubuntu-20.04\n-  env:\n-    CC: gcc-10\n-    CXX: g++-10\n-    GEN: ninja\n-    BUILD_TPCH: 1\n-    BUILD_TPCDS: 1\n-\n-  steps:\n-    - uses: actions/checkout@v4\n-      with:\n-        fetch-depth: 0\n-\n-    - uses: actions/setup-python@v5\n-      with:\n-        python-version: '3.12'\n-\n-    - name: Install\n-      shell: bash\n-      run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install requests\n-\n-    - name: Setup Ccache\n-      uses: hendrikmuhs/ccache-action@main\n-      with:\n-        key: ${{ github.job }}\n-        save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n-\n-    - name: Build\n-      shell: bash\n-      run: |\n-        make\n-        git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1\n-        cd duckdb\n-        make\n-        cd ..\n-\n-    - name: Regression Test\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_storage_size.py --old=duckdb/build/release/duckdb --new=build/release/duckdb\n-\n-    - name: Test for incompatibility\n-      shell: bash\n-      run: |\n-        if (cmp test/sql/storage_version/storage_version.db duckdb/test/sql/storage_version/storage_version.db); then\n-          echo \"storage_changed=false\" >> $GITHUB_ENV\n-        else\n-          echo \"storage_changed=true\" >> $GITHUB_ENV\n-        fi\n-\n-    - name: Regression Compatibility Test (testing bidirectional compatibility)\n-      shell: bash\n-      if: env.storage_changed == 'false'\n-      run: |\n-        # Regenerate test/sql/storage_version.db with newer version -> read with older version\n-        python3 scripts/generate_storage_version.py\n-        ./duckdb/build/release/duckdb test/sql/storage_version/storage_version.db\n-        # Regenerate test/sql/storage_version.db with older version -> read with newer version (already performed as part of test.slow)\n-        cd duckdb\n-        python3 ../scripts/generate_storage_version.py\n-        ../build/release/duckdb duckdb/test/sql/storage_version/storage_version.db\n-        cd ..\n-\n-    - name: Regression Compatibility Test (testing storage version has been bumped)\n-      shell: bash\n-      if: env.storage_changed == 'true'\n-      run: |\n-        python3 scripts/generate_storage_version.py\n-        cd duckdb\n-        python3 scripts/generate_storage_version.py\n-        cd ..\n-        if (cmp -i 8 -n 12 test/sql/storage_version.db duckdb/test/sql/storage_version.db); then\n-           echo \"Expected storage format to be bumped, but this is not the case\"\n-           echo \"This might fail spuriously if changes to content of test database / generation script happened\"\n-           exit 1\n-        else\n-           echo \"Storage bump detected, all good!\"\n-        fi\n-\n- regression-test-python:\n-  name: Regression Test (Python Client)\n-  runs-on: ubuntu-20.04\n-  env:\n-    CC: gcc-10\n-    CXX: g++-10\n-    GEN: ninja\n-\n-  steps:\n-    - uses: actions/checkout@v4\n-      with:\n-        fetch-depth: 0\n-\n-    - uses: actions/setup-python@v5\n-      with:\n-        python-version: '3.12'\n-\n-    - name: Install\n-      shell: bash\n-      run: |\n-        sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build\n-        pip install numpy pytest pandas mypy psutil pyarrow\n-\n-    - name: Setup Ccache\n-      uses: hendrikmuhs/ccache-action@main\n-      with:\n-        key: ${{ github.job }}\n-        save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n-\n-    - name: Build Current Version\n-      shell: bash\n-      run: |\n-        cd tools/pythonpkg\n-        pip install --use-pep517 . --user\n-        cd ../..\n-\n-    - name: Run New Version\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_python.py --threads=2 --out-file=new.csv\n-\n-    - name: Cleanup New Version\n-      shell: bash\n-      run: |\n-        cd tools/pythonpkg\n-        ./clean.sh\n-        cd ../..\n-\n-    - name: Build Current\n-      shell: bash\n-      run: |\n-        git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1\n-        cd duckdb/tools/pythonpkg\n-        pip install --use-pep517 . --user\n-        cd ../../..\n-\n-    - name: Run Current Version\n-      shell: bash\n-      run: |\n-        python scripts/regression_test_python.py --threads=2 --out-file=current.csv\n-\n-    - name: Regression Test\n-      shell: bash\n-      run: |\n-        cp -r benchmark duckdb/\n-        python scripts/regression_check.py --old=current.csv --new=new.csv\n-\n- regression-test-plan-cost:\n-  name: Regression Test Join Order Plan Cost\n-  runs-on: ubuntu-20.04\n-  env:\n-    CC: gcc-10\n-    CXX: g++-10\n-    GEN: ninja\n-    BUILD_TPCH: 1\n-    BUILD_HTTPFS: 1\n-\n-  steps:\n-    - uses: actions/checkout@v4\n-      with:\n-        fetch-depth: 0\n-\n-    - uses: actions/setup-python@v5\n-      with:\n-        python-version: '3.12'\n-\n-    - name: Install\n-      shell: bash\n-      run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install tqdm\n-\n-    - name: Setup Ccache\n-      uses: hendrikmuhs/ccache-action@main\n-      with:\n-        key: ${{ github.job }}\n-        save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n-\n-    - name: Build\n-      shell: bash\n-      run: |\n-        make\n-        git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1\n-        cd duckdb\n-        make\n-        cd ..\n-\n-    - name: Set up benchmarks\n-      shell: bash\n-      run: |\n-        cp -r benchmark duckdb/\n-\n-    - name: Regression Test IMDB\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/plan_cost_runner.py --old=duckdb/build/release/duckdb --new=build/release/duckdb --dir=benchmark/imdb_plan_cost\n-\n-    - name: Regression Test TPCH\n-      if: always()\n-      shell: bash\n-      run: |\n-        python scripts/plan_cost_runner.py --old=duckdb/build/release/duckdb --new=build/release/duckdb --dir=benchmark/tpch_plan_cost\n+  regression-test-benchmark-runner:\n+    name: Regression Tests\n+    runs-on: ubuntu-20.04\n+    env:\n+      CC: gcc-10\n+      CXX: g++-10\n+      GEN: ninja\n+      BUILD_BENCHMARK: 1\n+      BUILD_TPCH: 1\n+      BUILD_TPCDS: 1\n+      BUILD_HTTPFS: 1\n+      BUILD_JEMALLOC: 1\n+      BUILD_JSON: 1\n+      CORE_EXTENSIONS: \"inet\"\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+\n+      - uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.12'\n+\n+      - name: Install\n+        shell: bash\n+        run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install requests\n+\n+      - name: Setup Ccache\n+        uses: hendrikmuhs/ccache-action@main\n+        with:\n+          key: ${{ github.job }}\n+          save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n+\n+      - name: Build\n+        shell: bash\n+        run: |\n+          make\n+          git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1\n+          cd duckdb\n+          make\n+          cd ..\n+\n+      - name: Set up benchmarks\n+        shell: bash\n+        run: |\n+          cp -r benchmark duckdb/\n+\n+      - name: Regression Test Micro\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/micro.csv --verbose --threads=2\n+\n+      - name: Regression Test Ingestion Perf\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/ingestion.csv --verbose --threads=2\n+\n+      - name: Regression Test TPCH\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/tpch.csv --verbose --threads=2\n+\n+      - name: Regression Test TPCH-PARQUET\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/tpch_parquet.csv --verbose --threads=2\n+\n+      - name: Regression Test TPCDS\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/tpcds.csv --verbose --threads=2\n+\n+      - name: Regression Test H2OAI\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/h2oai.csv --verbose --threads=2\n+\n+      - name: Regression Test IMDB\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/imdb.csv --verbose --threads=2\n+\n+      - name: Regression Test CSV\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/csv.csv --verbose --threads=2\n+\n+      - name: Regression Test RealNest \n+        if: always()\n+        shell: bash\n+        run: |\n+          mkdir -p duckdb_benchmark_data\n+          mkdir -p duckdb/duckdb_benchmark_data\n+          wget https://duckdb-blobs.s3.amazonaws.com/data/realnest/realnest.duckdb --output-document=duckdb_benchmark_data/real_nest.duckdb\n+          cp duckdb_benchmark_data/real_nest.duckdb duckdb/duckdb_benchmark_data/real_nest.duckdb\n+          python scripts/regression_test_runner.py --old=duckdb/build/release/benchmark/benchmark_runner --new=build/release/benchmark/benchmark_runner --benchmarks=.github/regression/realnest.csv --verbose --threads=2\n+         \n+  regression-test-storage:\n+    name: Storage Size Regression Test\n+    runs-on: ubuntu-20.04\n+    env:\n+      CC: gcc-10\n+      CXX: g++-10\n+      GEN: ninja\n+      BUILD_TPCH: 1\n+      BUILD_TPCDS: 1\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+\n+      - uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.12'\n+\n+      - name: Install\n+        shell: bash\n+        run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install requests\n+\n+      - name: Setup Ccache\n+        uses: hendrikmuhs/ccache-action@main\n+        with:\n+          key: ${{ github.job }}\n+          save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n+\n+      - name: Build\n+        shell: bash\n+        run: |\n+          make\n+          git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1\n+          cd duckdb\n+          make\n+          cd ..\n+\n+      - name: Regression Test\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_storage_size.py --old=duckdb/build/release/duckdb --new=build/release/duckdb\n+\n+      - name: Test for incompatibility\n+        shell: bash\n+        run: |\n+          if (cmp test/sql/storage_version/storage_version.db duckdb/test/sql/storage_version/storage_version.db); then\n+            echo \"storage_changed=false\" >> $GITHUB_ENV\n+          else\n+            echo \"storage_changed=true\" >> $GITHUB_ENV\n+          fi\n+\n+      - name: Regression Compatibility Test (testing bidirectional compatibility)\n+        shell: bash\n+        if: env.storage_changed == 'false'\n+        run: |\n+          # Regenerate test/sql/storage_version.db with newer version -> read with older version\n+          python3 scripts/generate_storage_version.py\n+          ./duckdb/build/release/duckdb test/sql/storage_version/storage_version.db\n+          # Regenerate test/sql/storage_version.db with older version -> read with newer version (already performed as part of test.slow)\n+          cd duckdb\n+          python3 ../scripts/generate_storage_version.py\n+          ../build/release/duckdb duckdb/test/sql/storage_version/storage_version.db\n+          cd ..\n+\n+      - name: Regression Compatibility Test (testing storage version has been bumped)\n+        shell: bash\n+        if: env.storage_changed == 'true'\n+        run: |\n+          python3 scripts/generate_storage_version.py\n+          cd duckdb\n+          python3 scripts/generate_storage_version.py\n+          cd ..\n+          if (cmp -i 8 -n 12 test/sql/storage_version.db duckdb/test/sql/storage_version.db); then\n+            echo \"Expected storage format to be bumped, but this is not the case\"\n+            echo \"This might fail spuriously if changes to content of test database / generation script happened\"\n+            exit 1\n+          else\n+            echo \"Storage bump detected, all good!\"\n+          fi\n+\n+  regression-test-python:\n+    name: Regression Test (Python Client)\n+    runs-on: ubuntu-20.04\n+    env:\n+      CC: gcc-10\n+      CXX: g++-10\n+      GEN: ninja\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+\n+      - uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.12'\n+\n+      - name: Install\n+        shell: bash\n+        run: |\n+          sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build\n+          pip install numpy pytest pandas mypy psutil pyarrow\n+\n+      - name: Setup Ccache\n+        uses: hendrikmuhs/ccache-action@main\n+        with:\n+          key: ${{ github.job }}\n+          save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n+\n+      - name: Build Current Version\n+        shell: bash\n+        run: |\n+          cd tools/pythonpkg\n+          pip install --use-pep517 . --user\n+          cd ../..\n+\n+      - name: Run New Version\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_python.py --threads=2 --out-file=new.csv\n+\n+      - name: Cleanup New Version\n+        shell: bash\n+        run: |\n+          cd tools/pythonpkg\n+          ./clean.sh\n+          cd ../..\n+\n+      - name: Build Current\n+        shell: bash\n+        run: |\n+          git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1\n+          cd duckdb/tools/pythonpkg\n+          pip install --use-pep517 . --user\n+          cd ../../..\n+\n+      - name: Run Current Version\n+        shell: bash\n+        run: |\n+          python scripts/regression_test_python.py --threads=2 --out-file=current.csv\n+\n+      - name: Regression Test\n+        shell: bash\n+        run: |\n+          cp -r benchmark duckdb/\n+          python scripts/regression_check.py --old=current.csv --new=new.csv\n+\n+  regression-test-plan-cost:\n+    name: Regression Test Join Order Plan Cost\n+    runs-on: ubuntu-20.04\n+    env:\n+      CC: gcc-10\n+      CXX: g++-10\n+      GEN: ninja\n+      BUILD_TPCH: 1\n+      BUILD_HTTPFS: 1\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+\n+      - uses: actions/setup-python@v5\n+        with:\n+          python-version: '3.12'\n+\n+      - name: Install\n+        shell: bash\n+        run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build && pip install tqdm\n+\n+      - name: Setup Ccache\n+        uses: hendrikmuhs/ccache-action@main\n+        with:\n+          key: ${{ github.job }}\n+          save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n+\n+      - name: Build\n+        shell: bash\n+        run: |\n+          make\n+          git clone --branch ${{ env.BASE_BRANCH }} https://github.com/duckdb/duckdb.git --depth=1\n+          cd duckdb\n+          make\n+          cd ..\n+\n+      - name: Set up benchmarks\n+        shell: bash\n+        run: |\n+          cp -r benchmark duckdb/\n+\n+      - name: Regression Test IMDB\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/plan_cost_runner.py --old=duckdb/build/release/duckdb --new=build/release/duckdb --dir=benchmark/imdb_plan_cost\n+\n+      - name: Regression Test TPCH\n+        if: always()\n+        shell: bash\n+        run: |\n+          python scripts/plan_cost_runner.py --old=duckdb/build/release/duckdb --new=build/release/duckdb --dir=benchmark/tpch_plan_cost\ndiff --git a/benchmark/realnest/01_aggregate-first-level-struct-members.benchmark b/benchmark/realnest/01_aggregate-first-level-struct-members.benchmark\nnew file mode 100644\nindex 000000000000..4bb47a74a7fa\n--- /dev/null\n+++ b/benchmark/realnest/01_aggregate-first-level-struct-members.benchmark\n@@ -0,0 +1,27 @@\n+# name: benchmark/realnest/01_aggregate-first-level-struct-members.benchmark\n+# description: Aggregate functions on the struct, group by one parameter\n+# group: [realnest]\n+\n+name aggregate-first-level-struct-members\n+\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT PV.npvs,\n+      sum(PV.x) AS sum_x,\n+      sum(PV.y) AS sum_y,\n+      sum(PV.z) AS sum_z,\n+      avg(MET.pt) AS avg_pt,\n+      min(MET.phi) AS min_phi,\n+      max(MET.sumet) AS max_sumet\n+FROM run2012B_singleMu\n+GROUP BY PV.npvs\n+HAVING sum_x > 1;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/02_list_sort.benchmark b/benchmark/realnest/02_list_sort.benchmark\nnew file mode 100644\nindex 000000000000..b97351fa5096\n--- /dev/null\n+++ b/benchmark/realnest/02_list_sort.benchmark\n@@ -0,0 +1,17 @@\n+# name: benchmark/realnest/02_list_sort.benchmark\n+# description: list_sort text entries\n+# group: [realnest]\n+\n+name list_sort\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT list_sort(body_text) FROM cord;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/03_create_table_from_unnested_structs.benchmark b/benchmark/realnest/03_create_table_from_unnested_structs.benchmark\nnew file mode 100644\nindex 000000000000..7bc78d59b7de\n--- /dev/null\n+++ b/benchmark/realnest/03_create_table_from_unnested_structs.benchmark\n@@ -0,0 +1,43 @@\n+# name: benchmark/realnest/03_create_table_from_unnested_structs.benchmark\n+# description: Create a table by unnesting and joining the structs in a JSON file\n+# group: [realnest]\n+\n+name create_table_from_unnested_structs\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+CREATE OR REPLACE TABLE combined AS \n+(SELECT unnested_hlt.*,\n+    unnested_pv.*,\n+    unnested_met.*,\n+    unnested_muon.*,\n+    unnested_electron.*,\n+    unnested_tau.*,\n+    unnested_photon.*,\n+    unnested_jet.*\n+FROM \n+    (SELECT rowid, UNNEST(HLT) AS hlt FROM run2012B_singleMu) AS unnested_hlt\n+    LEFT JOIN\n+    (SELECT rowid, UNNEST(PV) AS pv FROM run2012B_singleMu) AS unnested_pv ON unnested_hlt.rowid = unnested_pv.rowid\n+    LEFT JOIN\n+    (SELECT rowid, UNNEST(MET) AS met FROM run2012B_singleMu) AS unnested_met ON unnested_hlt.rowid = unnested_met.rowid\n+    LEFT JOIN\n+    (SELECT rowid, UNNEST(Muon, recursive:=true) AS muon FROM run2012B_singleMu) AS unnested_muon ON unnested_hlt.rowid = unnested_muon.rowid\n+    LEFT JOIN\n+    (SELECT rowid, UNNEST(Electron, recursive:=true) AS electron FROM run2012B_singleMu) AS unnested_electron ON unnested_hlt.rowid = unnested_electron.rowid\n+    LEFT JOIN\n+    (SELECT rowid, UNNEST(Tau, recursive:=true) AS tau FROM run2012B_singleMu) AS unnested_tau ON unnested_hlt.rowid = unnested_tau.rowid\n+    LEFT JOIN\n+    (SELECT rowid, UNNEST(Photon, recursive:=true) AS photon FROM run2012B_singleMu) AS unnested_photon ON unnested_hlt.rowid = unnested_photon.rowid\n+    LEFT JOIN\n+    (SELECT rowid, UNNEST(Jet, recursive:=true) AS jet FROM run2012B_singleMu) AS unnested_jet ON unnested_hlt.rowid = unnested_jet.rowid\n+LIMIT 100000\n+);\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/04_list_transform_and_list_aggregate.benchmark b/benchmark/realnest/04_list_transform_and_list_aggregate.benchmark\nnew file mode 100644\nindex 000000000000..8d9904bd4883\n--- /dev/null\n+++ b/benchmark/realnest/04_list_transform_and_list_aggregate.benchmark\n@@ -0,0 +1,61 @@\n+# name: benchmark/realnest/04_list_transform_and_list_aggregate.benchmark\n+# description: select average from transformed list and group by, having\n+# group: [realnest]\n+\n+name list_transform plus list_aggregate\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT\n+  list_aggregate(list_transform(Jet, x -> x.pt), 'avg') AS avg_pt,\n+  list_aggregate(list_transform(Jet, x -> x.eta), 'avg') AS avg_eta,\n+  list_aggregate(list_transform(Jet, x -> x.phi), 'avg') AS avg_phi,\n+  list_aggregate(list_transform(Jet, x -> x.mass), 'avg') AS avg_mass,\n+  list_aggregate(list_transform(Jet, x -> x.btag), 'avg') AS avg_btag,\n+  list_aggregate(list_transform(Photon, x -> x.pt), 'avg') AS ph_avg_pt,\n+  list_aggregate(list_transform(Photon, x -> x.eta), 'avg') AS ph_avg_eta,\n+  list_aggregate(list_transform(Photon, x -> x.phi), 'avg') AS ph_avg_phi,\n+  list_aggregate(list_transform(Photon, x -> x.mass), 'avg') AS ph_avg_mass,\n+  list_aggregate(list_transform(Photon, x -> x.pfreliso03_all), 'avg') AS ph_avg_pf,\n+  list_aggregate(list_transform(Photon, x -> x.jetidx), 'avg') AS ph_avg_jet,\n+  list_aggregate(list_transform(Photon, x -> x.genpartidx), 'avg') AS ph_avg_gen,\n+  list_aggregate(list_transform(Tau, x -> x.pt), 'avg') AS t_avg_pt,\n+  list_aggregate(list_transform(Tau, x -> x.eta), 'avg') AS t_avg_eta,\n+  list_aggregate(list_transform(Tau, x -> x.mass), 'avg') AS t_avg_mass,\n+  list_aggregate(list_transform(Tau, x -> x.decaymode), 'avg') AS t_avg_dec,\n+  list_aggregate(list_transform(Tau, x -> x.reliso_all), 'avg') AS t_avg_rel,\n+  list_aggregate(list_transform(Tau, x -> x.jetidx), 'avg') AS t_avg_jet,\n+  list_aggregate(list_transform(Tau, x -> x.genpartidx), 'avg') AS t_avg_gen,\n+  list_aggregate(list_transform(Electron, x -> x.pt), 'avg') AS el_avg_pt,\n+  list_aggregate(list_transform(Electron, x -> x.eta), 'avg') AS el_avg_eta,\n+  list_aggregate(list_transform(Electron, x -> x.phi), 'avg') AS el_avg_phi,\n+  list_aggregate(list_transform(Electron, x -> x.mass), 'avg') AS el_avg_mass,\n+  list_aggregate(list_transform(Electron, x -> x.pfreliso03_all), 'avg') AS el_avg_pf,\n+  list_aggregate(list_transform(Electron, x -> x.dxy), 'avg') AS el_avg_dxy,\n+  list_aggregate(list_transform(Electron, x -> x.dxyerr), 'avg') AS el_avg_dxyer,\n+  list_aggregate(list_transform(Electron, x -> x.dz), 'avg') AS el_avg_dz,\n+  list_aggregate(list_transform(Electron, x -> x.dzerr), 'avg') AS el_avg_dzer,\n+  list_aggregate(list_transform(Electron, x -> x.jetidx), 'avg') AS el_avg_jet,\n+  list_aggregate(list_transform(Electron, x -> x.genpartidx), 'avg') AS el_avg_gen, \n+  list_aggregate(list_transform(Muon, x -> x.pt), 'avg') AS mu_avg_pt, \n+  list_aggregate(list_transform(Muon, x -> x.eta), 'avg') AS mu_avg_eta, \n+  list_aggregate(list_transform(Muon, x -> x.phi), 'avg') AS mu_avg_phi, \n+  list_aggregate(list_transform(Muon, x -> x.mass), 'avg') AS mu_avg_mas, \n+  list_aggregate(list_transform(Muon, x -> x.pfreliso03_all), 'avg') AS mu_avg_pf3, \n+  list_aggregate(list_transform(Muon, x -> x.pfreliso04_all), 'avg') AS mu_avg_pf4, \n+  list_aggregate(list_transform(Muon, x -> x.dxy), 'avg') AS mu_avg_dxy, \n+  list_aggregate(list_transform(Muon, x -> x.dxyerr), 'avg') AS mu_avg_dxyer, \n+  list_aggregate(list_transform(Muon, x -> x.dz), 'avg') AS mu_avg_dz, \n+  list_aggregate(list_transform(Muon, x -> x.dzerr), 'avg') AS mu_avg_dzer, \n+  list_aggregate(list_transform(Muon, x -> x.jetidx), 'avg') AS mu_avg_jet, \n+  list_aggregate(list_transform(Muon, x -> x.genpartidx), 'avg') AS mu_avg_get\n+  FROM run2012B_singleMu\n+;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/05_list_filter.benchmark b/benchmark/realnest/05_list_filter.benchmark\nnew file mode 100644\nindex 000000000000..4663fb133587\n--- /dev/null\n+++ b/benchmark/realnest/05_list_filter.benchmark\n@@ -0,0 +1,28 @@\n+# name: benchmark/realnest/05_list_filter.benchmark\n+# description: Multiple list_filters\n+# group: [realnest]\n+\n+name list_filters\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT\n+    count(*) AS total_rows,\n+    sum(len(list_filter(Tau, x -> x.charge < 0))) AS negatives,\n+    sum(len(list_filter(Tau, x -> x.charge > 0))) AS positives,\n+    sum(len(list_filter(Tau, x -> x.charge = 0))) AS neutral,\n+    sum(len(list_filter(Tau, x -> (x.pt % 2) - 1 > 0))) AS odds,\n+    sum(len(list_filter(Tau, x -> x.idIsoVLoose != x.idIsoLoose))) AS idIsoMatch,\n+    sum(len(list_filter(Muon, x -> x.tightId == true))) AS muon,\n+    sum(len(list_filter(Electron, x -> x.mass > x.eta + x.phi))) AS elentron,\n+    sum(len(list_filter(Photon, x -> x.mass > 0))) AS photon,\n+    sum(len(list_filter(Jet, x -> x.puId != false))) AS jet,\n+FROM run2012B_singleMu;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/06_list_filter_on_unnested_structure.benchmark b/benchmark/realnest/06_list_filter_on_unnested_structure.benchmark\nnew file mode 100644\nindex 000000000000..4a059e55fcf4\n--- /dev/null\n+++ b/benchmark/realnest/06_list_filter_on_unnested_structure.benchmark\n@@ -0,0 +1,24 @@\n+# name: benchmark/realnest/06_list_filter_on_unnested_structure.benchmark\n+# description: list_filter on unnested_muon structure\n+# group: [realnest]\n+\n+name list_filter_on_unnested_structure\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT list_filter(\n+        [pt, eta, phi, mass, pfRelIso03_all, pfRelIso04_all, dxy, dxyErr, jetIdx, genPartIdx],\n+        x -> x > 0.01)\n+FROM (\n+    SELECT UNNEST(Muon, recursive:=true) AS unnested_muon\n+    FROM run2012B_singleMu\n+    )\n+;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/07_list_unique_on_transformed_and_aggregated_list.benchmark b/benchmark/realnest/07_list_unique_on_transformed_and_aggregated_list.benchmark\nnew file mode 100644\nindex 000000000000..a35a70d2aff9\n--- /dev/null\n+++ b/benchmark/realnest/07_list_unique_on_transformed_and_aggregated_list.benchmark\n@@ -0,0 +1,28 @@\n+# name: benchmark/realnest/07_list_unique_on_transformed_and_aggregated_list.benchmark\n+# description: Creates a list by unnesting and re-aggregating it, then performs list_transform and list_unique on that list\n+# group: [realnest]\n+\n+name list_operations_on_strings\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT list_unique(list_transform(lt, s -> length(s))) \n+FROM (\n+    SELECT list(text) AS lt \n+    FROM (\n+        SELECT bm.text \n+        FROM (\n+            SELECT UNNEST(back_matter) AS bm \n+            FROM cord\n+            )\n+        )\n+    )\n+;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/08_count_map_keys.benchmark b/benchmark/realnest/08_count_map_keys.benchmark\nnew file mode 100644\nindex 000000000000..0ddd40b19cde\n--- /dev/null\n+++ b/benchmark/realnest/08_count_map_keys.benchmark\n@@ -0,0 +1,23 @@\n+# name: benchmark/realnest/08_count_map_keys.benchmark\n+# description: Count map keys and aggregate them\n+# group: [realnest]\n+\n+name count_map_keys\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT keys, count(*) mentions \n+FROM (\n+    SELECT UNNEST(map_keys(tags)) AS keys \n+    FROM open_street_map\n+)\n+GROUP BY keys\n+ORDER BY mentions DESC;\ndiff --git a/benchmark/realnest/09_array_agg.benchmark b/benchmark/realnest/09_array_agg.benchmark\nnew file mode 100644\nindex 000000000000..e810cf40c04f\n--- /dev/null\n+++ b/benchmark/realnest/09_array_agg.benchmark\n@@ -0,0 +1,22 @@\n+# name: benchmark/realnest/09_array_agg.benchmark\n+# description: Aggregate nested structs\n+# group: [realnest]\n+\n+name aggregate_nested_structs\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT array_agg(data.entities.hashtags), \n+    array_agg(data.entities.mentions),\n+    array_agg(data.entities.urls),\n+    array_agg(data.entities.annotations),\n+    array_agg(data.entities.cashtags) \n+FROM twitter;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/11_list_sort_reduce_transform.benchmark b/benchmark/realnest/11_list_sort_reduce_transform.benchmark\nnew file mode 100644\nindex 000000000000..37a682144b54\n--- /dev/null\n+++ b/benchmark/realnest/11_list_sort_reduce_transform.benchmark\n@@ -0,0 +1,25 @@\n+# name: benchmark/realnest/11_list_sort_reduce_transform.benchmark\n+# description: Transform, aggregate, reduce and sort a list\n+# group: [realnest]\n+\n+name list_sort_reduce_transform\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT list_sort(\n+    array_agg(\n+        list_reduce(\n+            list_transform(Photon, x -> x.pt),\n+        (x, y, z) -> (x + y)^z)\n+        )\n+    ) AS List\n+FROM run2012B_singleMu\n+WHERE len(Photon) != 0;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/12_map_list_values.benchmark b/benchmark/realnest/12_map_list_values.benchmark\nnew file mode 100644\nindex 000000000000..f0d62b36461d\n--- /dev/null\n+++ b/benchmark/realnest/12_map_list_values.benchmark\n@@ -0,0 +1,22 @@\n+# name: benchmark/realnest/12_map_list_values.benchmark\n+# description: Map list values\n+# group: [realnest]\n+\n+name map_list_values\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT map(tau_pt, tau_eta),\n+    map(jet_pt, jet_eta),\n+    map(muon_pt, muon_eta),\n+    map(ph_pt, ph_eta)\n+FROM singleMu\n+ORDER BY ALL DESC;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/13_multi_join_nested_data_with_filtering.benchmark b/benchmark/realnest/13_multi_join_nested_data_with_filtering.benchmark\nnew file mode 100644\nindex 000000000000..a1b8037d36da\n--- /dev/null\n+++ b/benchmark/realnest/13_multi_join_nested_data_with_filtering.benchmark\n@@ -0,0 +1,28 @@\n+# name: benchmark/realnest/13_multi_join_nested_data_with_filtering.benchmark\n+# description: Multiple join conditions and filtering on merged and closed pull requests\n+# group: [realnest]\n+\n+name multi_join_nested_data_with_filtering\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT gh_pull.pull_request.base.repo.language AS language,\n+    gh_issue.issue.user.login AS login,\n+    gh_pull.pull_request.title AS title,\n+    gh_pull.pull_request.html_url AS url\n+FROM gh_issue, gh_pull \n+WHERE gh_pull.pull_request.base.repo.owner = gh_issue.issue.user\n+    AND gh_pull.pull_request.user = gh_pull.pull_request.base.repo.owner\n+    AND gh_issue.issue.assignee = gh_pull.pull_request.base.repo.owner\n+    AND gh_pull.pull_request.assignee = gh_pull.pull_request.base.repo.owner\n+    AND gh_pull.pull_request.merged = 'true'\n+    AND gh_pull.pull_request.state = 'closed'\n+ORDER BY language, title;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/14_list_slice.benchmark b/benchmark/realnest/14_list_slice.benchmark\nnew file mode 100644\nindex 000000000000..9c556778313d\n--- /dev/null\n+++ b/benchmark/realnest/14_list_slice.benchmark\n@@ -0,0 +1,25 @@\n+# name: benchmark/realnest/14_list_slice.benchmark\n+# description: Benchmark the list_slice function\n+# group: [realnest]\n+\n+name list_slice\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT list_slice(Jet, 2, 9),\n+    list_slice(Jet, 1, 6)[:3:-1],\n+    list_slice(Muon, 1, 5),\n+    list_slice(Muon, 2, 3)[:-4:-1],\n+    list_slice(Photon, 1, 3),\n+    list_slice(Photon, 1, 6)[:6:-1],\n+    list_slice(Tau, 5, 9),\n+    list_slice(Tau, 2, 9)[:-8:-1]\n+FROM single_mu_lists;\ndiff --git a/benchmark/realnest/15_list_sort.benchmark b/benchmark/realnest/15_list_sort.benchmark\nnew file mode 100644\nindex 000000000000..cc13199f85d5\n--- /dev/null\n+++ b/benchmark/realnest/15_list_sort.benchmark\n@@ -0,0 +1,27 @@\n+# name: benchmark/realnest/15_list_sort.benchmark\n+# description: Benchmarks list_sort function\n+# group: [realnest]\n+\n+name list_sort\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT list_sort(Jet, 'ASC'),\n+    list_sort(Muon, 'DESC'),\n+    list_sort(Muon, 'ASC', 'NULLS FIRST'),\n+    list_sort(Muon, 'ASC', 'NULLS LAST'),\n+    list_sort(Photon, 'ASC'),\n+    list_sort(Photon, 'DESC', 'NULLS FIRST'),\n+    list_sort(Photon, 'DESC', 'NULLS LAST'),\n+    list_sort(Tau, 'DESC'),\n+    list_sort(Tau, 'ASC', 'NULLS FIRST'),\n+    list_sort(Tau, 'ASC', 'NULLS LAST')\n+FROM single_mu_lists;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/16_most_common_list_aggregates.benchmark b/benchmark/realnest/16_most_common_list_aggregates.benchmark\nnew file mode 100644\nindex 000000000000..c7e5f1fb540c\n--- /dev/null\n+++ b/benchmark/realnest/16_most_common_list_aggregates.benchmark\n@@ -0,0 +1,82 @@\n+# name: benchmark/realnest/16_most_common_list_aggregates.benchmark\n+# description: Combination of the most common list_aggregate functions\n+# group: [realnest]\n+\n+name most_common_list_aggregates\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT \n+  list_aggregate(tau_pt, 'list'),\n+  list_aggregate(tau_eta, 'list'),\n+  list_aggregate(jet_pt, 'list'),\n+  list_aggregate(jet_eta, 'list'),\n+  list_aggregate(muon_pt, 'list'),\n+  list_aggregate(muon_eta, 'list'),\n+  list_aggregate(ph_pt, 'list'),\n+  list_aggregate(ph_eta, 'list'),\n+  list_aggregate(tau_pt, 'sum'),\n+  list_aggregate(tau_eta, 'sum'),\n+  list_aggregate(jet_pt, 'sum'),\n+  list_aggregate(jet_eta, 'sum'),\n+  list_aggregate(muon_pt, 'sum'),\n+  list_aggregate(muon_eta, 'sum'),\n+  list_aggregate(ph_pt, 'sum'),\n+  list_aggregate(ph_eta, 'sum'),\n+  list_aggregate(tau_pt, 'min'),\n+  list_aggregate(tau_eta, 'min'),\n+  list_aggregate(jet_pt, 'min'),\n+  list_aggregate(jet_eta, 'min'),\n+  list_aggregate(muon_pt, 'min'),\n+  list_aggregate(muon_eta, 'min'),\n+  list_aggregate(ph_pt, 'min'),\n+  list_aggregate(ph_eta, 'min'),\n+  list_aggregate(tau_pt, 'max'),\n+  list_aggregate(tau_eta, 'max'),\n+  list_aggregate(jet_pt, 'max'),\n+  list_aggregate(jet_eta, 'max'),\n+  list_aggregate(muon_pt, 'max'),\n+  list_aggregate(muon_eta, 'max'),\n+  list_aggregate(ph_pt, 'max'),\n+  list_aggregate(ph_eta, 'max'),\n+  list_aggregate(tau_pt, 'count'),\n+  list_aggregate(tau_eta, 'count'),\n+  list_aggregate(jet_pt, 'count'),\n+  list_aggregate(jet_eta, 'count'),\n+  list_aggregate(muon_pt, 'count'),\n+  list_aggregate(muon_eta, 'count'),\n+  list_aggregate(ph_pt, 'count'),\n+  list_aggregate(ph_eta, 'count'),\n+  list_aggregate(tau_pt, 'string_agg', '|'),\n+  list_aggregate(tau_eta, 'string_agg', '|'),\n+  list_aggregate(jet_pt, 'string_agg', '|'),\n+  list_aggregate(jet_eta, 'string_agg', '|'),\n+  list_aggregate(muon_pt, 'string_agg', '|'),\n+  list_aggregate(muon_eta, 'string_agg', '|'),\n+  list_aggregate(ph_pt, 'string_agg', '|'),\n+  list_aggregate(ph_eta, 'string_agg', '|'),\n+  list_aggregate(tau_pt, 'avg'),\n+  list_aggregate(tau_eta, 'avg'),\n+  list_aggregate(jet_pt, 'avg'),\n+  list_aggregate(jet_eta, 'avg'),\n+  list_aggregate(muon_pt, 'avg'),\n+  list_aggregate(muon_eta, 'avg'),\n+  list_aggregate(ph_pt, 'avg'),\n+  list_aggregate(ph_eta, 'avg'),\n+  list_aggregate(tau_pt, 'median'),\n+  list_aggregate(tau_eta, 'median'),\n+  list_aggregate(jet_pt, 'median'),\n+  list_aggregate(jet_eta, 'median'),\n+  list_aggregate(muon_pt, 'median'),\n+  list_aggregate(muon_eta, 'median'),\n+  list_aggregate(ph_pt, 'median'),\n+  list_aggregate(ph_eta, 'median') \n+FROM singleMu;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/17_list_aggregates_histogram_stddev_mode.benchmark b/benchmark/realnest/17_list_aggregates_histogram_stddev_mode.benchmark\nnew file mode 100644\nindex 000000000000..61d98b806d3e\n--- /dev/null\n+++ b/benchmark/realnest/17_list_aggregates_histogram_stddev_mode.benchmark\n@@ -0,0 +1,42 @@\n+# name: benchmark/realnest/17_list_aggregates_histogram_stddev_mode.benchmark\n+# description: Combination of list_aggregate functions histogram, stddev, mode\n+# group: [realnest]\n+\n+name list_aggregates_histogram_stddev_mode\n+group real_nest\n+\n+require json\n+\n+require httpfs\n+\n+cache real_nest.duckdb\n+\n+load benchmark/realnest/load.sql\n+\n+run\n+SELECT \n+  list_aggregate(tau_pt, 'stddev'),\n+  list_aggregate(tau_eta, 'stddev'),\n+  list_aggregate(jet_pt, 'stddev'),\n+  list_aggregate(jet_eta, 'stddev'),\n+  list_aggregate(muon_pt, 'stddev'),\n+  list_aggregate(muon_eta, 'stddev'),\n+  list_aggregate(ph_pt, 'stddev'),\n+  list_aggregate(ph_eta, 'stddev'),\n+  list_aggregate(tau_pt, 'mode'),\n+  list_aggregate(tau_eta, 'mode'),\n+  list_aggregate(jet_pt, 'mode'),\n+  list_aggregate(jet_eta, 'mode'),\n+  list_aggregate(muon_pt, 'mode'),\n+  list_aggregate(muon_eta, 'mode'),\n+  list_aggregate(ph_pt, 'mode'),\n+  list_aggregate(ph_eta, 'mode'),\n+  list_aggregate(tau_pt, 'histogram'),\n+  list_aggregate(tau_eta, 'histogram'),\n+  list_aggregate(jet_pt, 'histogram'),\n+  list_aggregate(jet_eta, 'histogram'),\n+  list_aggregate(muon_pt, 'histogram'),\n+  list_aggregate(muon_eta, 'histogram'),\n+  list_aggregate(ph_pt, 'histogram'),\n+  list_aggregate(ph_eta, 'histogram')  \n+FROM singleMu;\n\\ No newline at end of file\ndiff --git a/benchmark/realnest/load.sql b/benchmark/realnest/load.sql\nnew file mode 100644\nindex 000000000000..de06b949bd57\n--- /dev/null\n+++ b/benchmark/realnest/load.sql\n@@ -0,0 +1,27 @@\n+ATTACH 'https://duckdb-blobs.s3.amazonaws.com/data/realnest/cord_10k.duckdb' AS cord (READ_ONLY);\n+CREATE TABLE cord AS SELECT * FROM cord.cord;\n+ATTACH 'https://duckdb-blobs.s3.amazonaws.com/data/realnest/open_street_map_524k.duckdb' AS osm (READ_ONLY);\n+CREATE TABLE open_street_map AS SELECT * FROM osm.open_street_map;\n+ATTACH 'https://duckdb-blobs.s3.amazonaws.com/data/realnest/pull_131k.duckdb' AS gh_pull (READ_ONLY);\n+CREATE TABLE gh_pull AS SELECT * FROM gh_pull.gh_pull;\n+ATTACH 'https://duckdb-blobs.s3.amazonaws.com/data/realnest/issue_131k.duckdb' AS gh_issue (READ_ONLY);\n+CREATE TABLE gh_issue AS SELECT * FROM gh_issue.gh_issue;\n+ATTACH 'https://duckdb-blobs.s3.amazonaws.com/data/realnest/twitter_131k.duckdb' AS tw (READ_ONLY);\n+CREATE TABLE twitter AS SELECT * FROM tw.twitter;\n+ATTACH 'https://duckdb-blobs.s3.amazonaws.com/data/realnest/singleMu_524k.duckdb' AS rn_singleMu (READ_ONLY);\n+CREATE TABLE run2012B_singleMu AS SELECT * FROM rn_singleMu.run2012B_singleMu;\n+CREATE TABLE single_mu_lists AS SELECT * REPLACE(\n+    list_resize(Jet, 10, NULL) AS Jet, list_resize(Muon, 10, NULL) AS Muon, \n+    list_resize(Photon, 10, NULL) AS Photon, list_resize(Tau, 10, NULL) AS Tau) \n+FROM rn_singleMu.run2012B_singleMu;\n+CREATE OR REPLACE TABLE singleMu AS \n+SELECT \n+    list_distinct(list_transform(\"Tau\", x -> x.pt)) AS tau_pt, list_distinct(list_transform(\"Tau\", x -> x.eta)) AS tau_eta,\n+    list_distinct(list_transform(\"Jet\", x -> x.pt)) AS jet_pt, list_distinct(list_transform(\"Jet\", x -> x.eta)) AS jet_eta, \n+    list_distinct(list_transform(\"Muon\", x -> x.pt)) AS muon_pt, list_distinct(list_transform(\"Muon\", x -> x.eta)) AS muon_eta, \n+    list_distinct(list_transform(\"Photon\", x -> x.pt)) AS ph_pt, list_distinct(list_transform(\"Photon\", x -> x.eta)) AS ph_eta\n+FROM rn_singleMu.run2012B_singleMu ORDER BY all DESC;\n+UPDATE singleMu SET jet_eta = list_resize(jet_eta, len(jet_pt));\n+UPDATE singleMu SET muon_eta = list_resize(muon_eta, len(muon_pt));\n+UPDATE singleMu SET ph_eta = list_resize(ph_eta, len(ph_pt));\n+UPDATE singleMu SET tau_eta = list_resize(tau_eta, len(tau_pt));\n\\ No newline at end of file\ndiff --git a/extension/tpcds/dsdgen/answers/sf100/67.csv b/extension/tpcds/dsdgen/answers/sf100/67.csv\nindex bc31ccae5734..333d6bb0b43e 100644\n--- a/extension/tpcds/dsdgen/answers/sf100/67.csv\n+++ b/extension/tpcds/dsdgen/answers/sf100/67.csv\n@@ -1,101 +1,101 @@\n-i_category|i_class|i_brandNULL|i_product_name|d_year|d_qoy|d_moy|s_store_idNULL|sumsales|rk\r\n-NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|32029263.56|5\r\n-NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|64897398.47|4\r\n-NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|130412669.10|3\r\n-NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|255726974.02|2\r\n-NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|101405424479.10|1\r\n-NULL|NULL|NULL|NULL|2000|NULL|NULL|NULL|32029263.56|5\r\n-NULL|NULL|NULL|NULL|2000|1|NULL|NULL|4638714.74|17\r\n-NULL|NULL|NULL|NULL|2000|1|1|NULL|1783708.73|96\r\n-NULL|NULL|NULL|NULL|2000|2|NULL|NULL|4645572.81|16\r\n-NULL|NULL|NULL|NULL|2000|2|4|NULL|1561458.02|98\r\n-NULL|NULL|NULL|NULL|2000|2|5|NULL|1590047.34|97\r\n-NULL|NULL|NULL|NULL|2000|2|6|NULL|1494067.45|100\r\n-NULL|NULL|NULL|NULL|2000|3|NULL|NULL|8690732.06|8\r\n-NULL|NULL|NULL|NULL|2000|3|7|NULL|1528725.34|99\r\n-NULL|NULL|NULL|NULL|2000|3|8|NULL|3598412.10|23\r\n-NULL|NULL|NULL|NULL|2000|3|9|NULL|3563594.62|24\r\n-NULL|NULL|NULL|NULL|2000|4|NULL|NULL|14054243.95|7\r\n-NULL|NULL|NULL|NULL|2000|4|10|NULL|3651929.46|22\r\n-NULL|NULL|NULL|NULL|2000|4|11|NULL|4950352.60|14\r\n-NULL|NULL|NULL|NULL|2000|4|12|NULL|5451961.89|11\r\n-NULL|NULL|amalgedu pack #2|NULL|NULL|NULL|NULL|NULL|1948460.41|84\r\n-NULL|NULL|amalgimporto #2|NULL|NULL|NULL|NULL|NULL|2060499.95|51\r\n-NULL|NULL|amalgscholar #2|NULL|NULL|NULL|NULL|NULL|2915350.58|33\r\n-NULL|NULL|edu packamalg #2|NULL|NULL|NULL|NULL|NULL|2337169.45|38\r\n-NULL|NULL|edu packamalgamalg #17|NULL|NULL|NULL|NULL|NULL|1983663.81|75\r\n-NULL|NULL|edu packamalgamalg #17|NULL|NULL|NULL|NULL|NULL|1983663.81|75\r\n-NULL|NULL|edu packamalgamalg #17|NULL|2000|NULL|NULL|NULL|1983663.81|75\r\n-NULL|NULL|exportiedu pack #1|NULL|NULL|NULL|NULL|NULL|2015173.47|63\r\n-NULL|NULL|exportiedu pack #1|NULL|NULL|NULL|NULL|NULL|2015173.47|63\r\n-NULL|NULL|exportiedu pack #1|NULL|2000|NULL|NULL|NULL|2015173.47|63\r\n-NULL|NULL|exportiedu pack #2|NULL|NULL|NULL|NULL|NULL|1980110.68|79\r\n-NULL|NULL|exportiedu pack #2|NULL|NULL|NULL|NULL|NULL|1980110.68|79\r\n-NULL|NULL|exportiedu pack #2|NULL|2000|NULL|NULL|NULL|1980110.68|79\r\n-NULL|NULL|exportiimporto #2|NULL|NULL|NULL|NULL|NULL|2054391.13|52\r\n-NULL|NULL|importoedu pack #1|NULL|NULL|NULL|NULL|NULL|2017011.96|59\r\n-NULL|NULL|importoedu pack #1|NULL|NULL|NULL|NULL|NULL|2017011.96|59\r\n-NULL|NULL|importoedu pack #1|NULL|2000|NULL|NULL|NULL|2017011.96|59\r\n-NULL|archery|NULL|NULL|NULL|NULL|NULL|NULL|1880685.97|94\r\n-NULL|athletic|NULL|NULL|NULL|NULL|NULL|NULL|3061542.94|28\r\n-NULL|athletic|NULL|NULL|NULL|NULL|NULL|NULL|5210675.89|12\r\n-NULL|baseball|NULL|NULL|NULL|NULL|NULL|NULL|1983963.50|74\r\n-NULL|basketball|NULL|NULL|NULL|NULL|NULL|NULL|1883886.93|93\r\n-NULL|business|NULL|NULL|NULL|NULL|NULL|NULL|2297093.85|39\r\n-NULL|classical|NULL|NULL|NULL|NULL|NULL|NULL|2917057.26|32\r\n-NULL|classical|edu packscholar #2|NULL|NULL|NULL|NULL|NULL|1916841.57|87\r\n-NULL|country|NULL|NULL|NULL|NULL|NULL|NULL|3163375.78|25\r\n-NULL|country|importoscholar #2|NULL|NULL|NULL|NULL|NULL|2148581.27|42\r\n-NULL|country|importoscholar #2|NULL|NULL|NULL|NULL|NULL|2148581.27|42\r\n-NULL|country|importoscholar #2|NULL|2000|NULL|NULL|NULL|2148581.27|42\r\n-NULL|customNULL|NULL|NULL|NULL|NULL|NULL|NULL|2075212.54|50\r\n-NULL|diamonds|NULL|NULL|NULL|NULL|NULL|NULL|1984577.91|73\r\n-NULL|dresses|NULL|NULL|NULL|NULL|NULL|NULL|2076617.17|48\r\n-NULL|dresses|NULL|NULL|NULL|NULL|NULL|NULL|3095482.08|26\r\n-NULL|dresses|NULL|NULL|NULL|NULL|NULL|NULL|6028869.45|9\r\n-NULL|dresses|NULL|NULL|2000|NULL|NULL|NULL|2076617.17|48\r\n-NULL|dresses|amalgamalg #2|NULL|NULL|NULL|NULL|NULL|2918521.76|31\r\n-NULL|earings|NULL|NULL|NULL|NULL|NULL|NULL|1988864.96|70\r\n-NULL|earings|NULL|NULL|NULL|NULL|NULL|NULL|1988864.96|70\r\n-NULL|earings|NULL|NULL|NULL|NULL|NULL|NULL|2003748.82|68\r\n-NULL|earings|NULL|NULL|2000|NULL|NULL|NULL|1988864.96|70\r\n-NULL|fragrances|NULL|NULL|NULL|NULL|NULL|NULL|1983149.33|78\r\n-NULL|fragrances|NULL|NULL|NULL|NULL|NULL|NULL|4894370.20|15\r\n-NULL|fragrances|importoamalg #1|NULL|NULL|NULL|NULL|NULL|1925135.95|86\r\n-NULL|history|NULL|NULL|NULL|NULL|NULL|NULL|1907089.25|89\r\n-NULL|history|NULL|NULL|NULL|NULL|NULL|NULL|1907089.25|89\r\n-NULL|history|NULL|NULL|NULL|NULL|NULL|NULL|2765482.40|37\r\n-NULL|history|NULL|NULL|2000|NULL|NULL|NULL|1907089.25|89\r\n-NULL|infants|NULL|NULL|NULL|NULL|NULL|NULL|2120615.81|46\r\n-NULL|loose stones|NULL|NULL|NULL|NULL|NULL|NULL|1886624.99|92\r\n-NULL|memory|NULL|NULL|NULL|NULL|NULL|NULL|2126286.51|45\r\n-NULL|mens|NULL|NULL|NULL|NULL|NULL|NULL|1966086.96|83\r\n-NULL|mens|NULL|NULL|NULL|NULL|NULL|NULL|2906981.21|35\r\n-NULL|newborn|NULL|NULL|NULL|NULL|NULL|NULL|2885464.55|36\r\n-NULL|newborn|NULL|NULL|NULL|NULL|NULL|NULL|3999371.31|19\r\n-NULL|pants|NULL|NULL|NULL|NULL|NULL|NULL|3899912.48|21\r\n-NULL|pants|exportiimporto #2|NULL|NULL|NULL|NULL|NULL|2024416.72|57\r\n-NULL|pants|exportiimporto #2|NULL|NULL|NULL|NULL|NULL|2960921.19|30\r\n-NULL|pants|exportiimporto #2|NULL|2000|NULL|NULL|NULL|2024416.72|57\r\n-NULL|pendants|NULL|NULL|NULL|NULL|NULL|NULL|1915760.89|88\r\n-NULL|popNULL|NULL|NULL|NULL|NULL|NULL|NULL|1866736.95|95\r\n-NULL|reference|NULL|NULL|NULL|NULL|NULL|NULL|1927519.76|85\r\n-NULL|rockNULL|NULL|NULL|NULL|NULL|NULL|NULL|2221752.90|41\r\n-NULL|rockNULL|NULL|NULL|NULL|NULL|NULL|NULL|2234031.88|40\r\n-NULL|shirts|NULL|NULL|NULL|NULL|NULL|NULL|2005748.24|66\r\n-NULL|shirts|NULL|NULL|NULL|NULL|NULL|NULL|2015374.72|62\r\n-NULL|shirts|NULL|NULL|NULL|NULL|NULL|NULL|3989095.69|20\r\n-NULL|shirts|NULL|NULL|2000|NULL|NULL|NULL|2005748.24|66\r\n-NULL|sports-apparel|NULL|NULL|NULL|NULL|NULL|NULL|2107528.93|47\r\n-NULL|sports-apparel|NULL|NULL|NULL|NULL|NULL|NULL|3055899.02|29\r\n-NULL|swimwear|NULL|NULL|NULL|NULL|NULL|NULL|2912705.57|34\r\n-NULL|swimwear|NULL|NULL|NULL|NULL|NULL|NULL|5986028.72|10\r\n-NULL|swimwear|edu packamalg #1|NULL|NULL|NULL|NULL|NULL|2027301.93|54\r\n-NULL|toddlers|NULL|NULL|NULL|NULL|NULL|NULL|2038611.36|53\r\n-NULL|toddlers|NULL|NULL|NULL|NULL|NULL|NULL|4013880.91|18\r\n-NULL|toddlers|exportiexporti #2|NULL|NULL|NULL|NULL|NULL|1975269.55|82\r\n-NULL|womens|NULL|NULL|NULL|NULL|NULL|NULL|1994814.32|69\r\n-NULL|womens|NULL|NULL|NULL|NULL|NULL|NULL|5074897.40|13\r\n-NULL|womens|amalgedu pack #2|NULL|NULL|NULL|NULL|NULL|2024659.79|55\r\n-NULL|womens|amalgedu pack #2|NULL|NULL|NULL|NULL|NULL|3080083.08|27\r\n-NULL|womens|amalgedu pack #2|NULL|2000|NULL|NULL|NULL|2024659.79|55\r\n+i_category|i_class|i_brand|i_product_name|d_year|d_qoy|d_moy|s_store_id|sumsales|rk\n+NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|32029263.56|5\n+NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|64897398.47|4\n+NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|130412669.10|3\n+NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|255726974.02|2\n+NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|101405424479.10|1\n+NULL|NULL|NULL|NULL|2000|NULL|NULL|NULL|32029263.56|5\n+NULL|NULL|NULL|NULL|2000|1|NULL|NULL|4638714.74|17\n+NULL|NULL|NULL|NULL|2000|1|1|NULL|1783708.73|96\n+NULL|NULL|NULL|NULL|2000|2|NULL|NULL|4645572.81|16\n+NULL|NULL|NULL|NULL|2000|2|4|NULL|1561458.02|98\n+NULL|NULL|NULL|NULL|2000|2|5|NULL|1590047.34|97\n+NULL|NULL|NULL|NULL|2000|2|6|NULL|1494067.45|100\n+NULL|NULL|NULL|NULL|2000|3|NULL|NULL|8690732.06|8\n+NULL|NULL|NULL|NULL|2000|3|7|NULL|1528725.34|99\n+NULL|NULL|NULL|NULL|2000|3|8|NULL|3598412.10|23\n+NULL|NULL|NULL|NULL|2000|3|9|NULL|3563594.62|24\n+NULL|NULL|NULL|NULL|2000|4|NULL|NULL|14054243.95|7\n+NULL|NULL|NULL|NULL|2000|4|10|NULL|3651929.46|22\n+NULL|NULL|NULL|NULL|2000|4|11|NULL|4950352.60|14\n+NULL|NULL|NULL|NULL|2000|4|12|NULL|5451961.89|11\n+NULL|NULL|amalgedu pack #2|NULL|NULL|NULL|NULL|NULL|1948460.41|84\n+NULL|NULL|amalgimporto #2|NULL|NULL|NULL|NULL|NULL|2060499.95|51\n+NULL|NULL|amalgscholar #2|NULL|NULL|NULL|NULL|NULL|2915350.58|33\n+NULL|NULL|edu packamalg #2|NULL|NULL|NULL|NULL|NULL|2337169.45|38\n+NULL|NULL|edu packamalgamalg #17|NULL|NULL|NULL|NULL|NULL|1983663.81|75\n+NULL|NULL|edu packamalgamalg #17|NULL|NULL|NULL|NULL|NULL|1983663.81|75\n+NULL|NULL|edu packamalgamalg #17|NULL|2000|NULL|NULL|NULL|1983663.81|75\n+NULL|NULL|exportiedu pack #1|NULL|NULL|NULL|NULL|NULL|2015173.47|63\n+NULL|NULL|exportiedu pack #1|NULL|NULL|NULL|NULL|NULL|2015173.47|63\n+NULL|NULL|exportiedu pack #1|NULL|2000|NULL|NULL|NULL|2015173.47|63\n+NULL|NULL|exportiedu pack #2|NULL|NULL|NULL|NULL|NULL|1980110.68|79\n+NULL|NULL|exportiedu pack #2|NULL|NULL|NULL|NULL|NULL|1980110.68|79\n+NULL|NULL|exportiedu pack #2|NULL|2000|NULL|NULL|NULL|1980110.68|79\n+NULL|NULL|exportiimporto #2|NULL|NULL|NULL|NULL|NULL|2054391.13|52\n+NULL|NULL|importoedu pack #1|NULL|NULL|NULL|NULL|NULL|2017011.96|59\n+NULL|NULL|importoedu pack #1|NULL|NULL|NULL|NULL|NULL|2017011.96|59\n+NULL|NULL|importoedu pack #1|NULL|2000|NULL|NULL|NULL|2017011.96|59\n+NULL|archery|NULL|NULL|NULL|NULL|NULL|NULL|1880685.97|94\n+NULL|athletic|NULL|NULL|NULL|NULL|NULL|NULL|3061542.94|28\n+NULL|athletic|NULL|NULL|NULL|NULL|NULL|NULL|5210675.89|12\n+NULL|baseball|NULL|NULL|NULL|NULL|NULL|NULL|1983963.50|74\n+NULL|basketball|NULL|NULL|NULL|NULL|NULL|NULL|1883886.93|93\n+NULL|business|NULL|NULL|NULL|NULL|NULL|NULL|2297093.85|39\n+NULL|classical|NULL|NULL|NULL|NULL|NULL|NULL|2917057.26|32\n+NULL|classical|edu packscholar #2|NULL|NULL|NULL|NULL|NULL|1916841.57|87\n+NULL|country|NULL|NULL|NULL|NULL|NULL|NULL|3163375.78|25\n+NULL|country|importoscholar #2|NULL|NULL|NULL|NULL|NULL|2148581.27|42\n+NULL|country|importoscholar #2|NULL|NULL|NULL|NULL|NULL|2148581.27|42\n+NULL|country|importoscholar #2|NULL|2000|NULL|NULL|NULL|2148581.27|42\n+NULL|custom|NULL|NULL|NULL|NULL|NULL|NULL|2075212.54|50\n+NULL|diamonds|NULL|NULL|NULL|NULL|NULL|NULL|1984577.91|73\n+NULL|dresses|NULL|NULL|NULL|NULL|NULL|NULL|2076617.17|48\n+NULL|dresses|NULL|NULL|NULL|NULL|NULL|NULL|3095482.08|26\n+NULL|dresses|NULL|NULL|NULL|NULL|NULL|NULL|6028869.45|9\n+NULL|dresses|NULL|NULL|2000|NULL|NULL|NULL|2076617.17|48\n+NULL|dresses|amalgamalg #2|NULL|NULL|NULL|NULL|NULL|2918521.76|31\n+NULL|earings|NULL|NULL|NULL|NULL|NULL|NULL|1988864.96|70\n+NULL|earings|NULL|NULL|NULL|NULL|NULL|NULL|1988864.96|70\n+NULL|earings|NULL|NULL|NULL|NULL|NULL|NULL|2003748.82|68\n+NULL|earings|NULL|NULL|2000|NULL|NULL|NULL|1988864.96|70\n+NULL|fragrances|NULL|NULL|NULL|NULL|NULL|NULL|1983149.33|78\n+NULL|fragrances|NULL|NULL|NULL|NULL|NULL|NULL|4894370.20|15\n+NULL|fragrances|importoamalg #1|NULL|NULL|NULL|NULL|NULL|1925135.95|86\n+NULL|history|NULL|NULL|NULL|NULL|NULL|NULL|1907089.25|89\n+NULL|history|NULL|NULL|NULL|NULL|NULL|NULL|1907089.25|89\n+NULL|history|NULL|NULL|NULL|NULL|NULL|NULL|2765482.40|37\n+NULL|history|NULL|NULL|2000|NULL|NULL|NULL|1907089.25|89\n+NULL|infants|NULL|NULL|NULL|NULL|NULL|NULL|2120615.81|46\n+NULL|loose stones|NULL|NULL|NULL|NULL|NULL|NULL|1886624.99|92\n+NULL|memory|NULL|NULL|NULL|NULL|NULL|NULL|2126286.51|45\n+NULL|mens|NULL|NULL|NULL|NULL|NULL|NULL|1966086.96|83\n+NULL|mens|NULL|NULL|NULL|NULL|NULL|NULL|2906981.21|35\n+NULL|newborn|NULL|NULL|NULL|NULL|NULL|NULL|2885464.55|36\n+NULL|newborn|NULL|NULL|NULL|NULL|NULL|NULL|3999371.31|19\n+NULL|pants|NULL|NULL|NULL|NULL|NULL|NULL|3899912.48|21\n+NULL|pants|exportiimporto #2|NULL|NULL|NULL|NULL|NULL|2024416.72|57\n+NULL|pants|exportiimporto #2|NULL|NULL|NULL|NULL|NULL|2960921.19|30\n+NULL|pants|exportiimporto #2|NULL|2000|NULL|NULL|NULL|2024416.72|57\n+NULL|pendants|NULL|NULL|NULL|NULL|NULL|NULL|1915760.89|88\n+NULL|pop|NULL|NULL|NULL|NULL|NULL|NULL|1866736.95|95\n+NULL|reference|NULL|NULL|NULL|NULL|NULL|NULL|1927519.76|85\n+NULL|rock|NULL|NULL|NULL|NULL|NULL|NULL|2221752.90|41\n+NULL|rock|NULL|NULL|NULL|NULL|NULL|NULL|2234031.88|40\n+NULL|shirts|NULL|NULL|NULL|NULL|NULL|NULL|2005748.24|66\n+NULL|shirts|NULL|NULL|NULL|NULL|NULL|NULL|2015374.72|62\n+NULL|shirts|NULL|NULL|NULL|NULL|NULL|NULL|3989095.69|20\n+NULL|shirts|NULL|NULL|2000|NULL|NULL|NULL|2005748.24|66\n+NULL|sports-apparel|NULL|NULL|NULL|NULL|NULL|NULL|2107528.93|47\n+NULL|sports-apparel|NULL|NULL|NULL|NULL|NULL|NULL|3055899.02|29\n+NULL|swimwear|NULL|NULL|NULL|NULL|NULL|NULL|2912705.57|34\n+NULL|swimwear|NULL|NULL|NULL|NULL|NULL|NULL|5986028.72|10\n+NULL|swimwear|edu packamalg #1|NULL|NULL|NULL|NULL|NULL|2027301.93|54\n+NULL|toddlers|NULL|NULL|NULL|NULL|NULL|NULL|2038611.36|53\n+NULL|toddlers|NULL|NULL|NULL|NULL|NULL|NULL|4013880.91|18\n+NULL|toddlers|exportiexporti #2|NULL|NULL|NULL|NULL|NULL|1975269.55|82\n+NULL|womens|NULL|NULL|NULL|NULL|NULL|NULL|1994814.32|69\n+NULL|womens|NULL|NULL|NULL|NULL|NULL|NULL|5074897.40|13\n+NULL|womens|amalgedu pack #2|NULL|NULL|NULL|NULL|NULL|2024659.79|55\n+NULL|womens|amalgedu pack #2|NULL|NULL|NULL|NULL|NULL|3080083.08|27\n+NULL|womens|amalgedu pack #2|NULL|2000|NULL|NULL|NULL|2024659.79|55\n\\ No newline at end of file\ndiff --git a/src/common/extra_type_info.cpp b/src/common/extra_type_info.cpp\nindex 6c09480c1eb3..54f03447c8a5 100644\n--- a/src/common/extra_type_info.cpp\n+++ b/src/common/extra_type_info.cpp\n@@ -1,4 +1,5 @@\n #include \"duckdb/common/extra_type_info.hpp\"\n+#include \"duckdb/common/extra_type_info/enum_type_info.hpp\"\n #include \"duckdb/common/serializer/deserializer.hpp\"\n #include \"duckdb/common/enum_util.hpp\"\n #include \"duckdb/common/numeric_utils.hpp\"\n@@ -220,50 +221,6 @@ PhysicalType EnumTypeInfo::DictType(idx_t size) {\n \t}\n }\n \n-template <class T>\n-struct EnumTypeInfoTemplated : public EnumTypeInfo {\n-\texplicit EnumTypeInfoTemplated(Vector &values_insert_order_p, idx_t size_p)\n-\t    : EnumTypeInfo(values_insert_order_p, size_p) {\n-\t\tD_ASSERT(values_insert_order_p.GetType().InternalType() == PhysicalType::VARCHAR);\n-\n-\t\tUnifiedVectorFormat vdata;\n-\t\tvalues_insert_order.ToUnifiedFormat(size_p, vdata);\n-\n-\t\tauto data = UnifiedVectorFormat::GetData<string_t>(vdata);\n-\t\tfor (idx_t i = 0; i < size_p; i++) {\n-\t\t\tauto idx = vdata.sel->get_index(i);\n-\t\t\tif (!vdata.validity.RowIsValid(idx)) {\n-\t\t\t\tthrow InternalException(\"Attempted to create ENUM type with NULL value\");\n-\t\t\t}\n-\t\t\tif (values.count(data[idx]) > 0) {\n-\t\t\t\tthrow InvalidInputException(\"Attempted to create ENUM type with duplicate value %s\",\n-\t\t\t\t                            data[idx].GetString());\n-\t\t\t}\n-\t\t\tvalues[data[idx]] = UnsafeNumericCast<T>(i);\n-\t\t}\n-\t}\n-\n-\tstatic shared_ptr<EnumTypeInfoTemplated> Deserialize(Deserializer &deserializer, uint32_t size) {\n-\t\tVector values_insert_order(LogicalType::VARCHAR, size);\n-\t\tauto strings = FlatVector::GetData<string_t>(values_insert_order);\n-\n-\t\tdeserializer.ReadList(201, \"values\", [&](Deserializer::List &list, idx_t i) {\n-\t\t\tstrings[i] = StringVector::AddStringOrBlob(values_insert_order, list.ReadElement<string>());\n-\t\t});\n-\t\treturn make_shared_ptr<EnumTypeInfoTemplated>(values_insert_order, size);\n-\t}\n-\n-\tconst string_map_t<T> &GetValues() const {\n-\t\treturn values;\n-\t}\n-\n-\tEnumTypeInfoTemplated(const EnumTypeInfoTemplated &) = delete;\n-\tEnumTypeInfoTemplated &operator=(const EnumTypeInfoTemplated &) = delete;\n-\n-private:\n-\tstring_map_t<T> values;\n-};\n-\n EnumTypeInfo::EnumTypeInfo(Vector &values_insert_order_p, idx_t dict_size_p)\n     : ExtraTypeInfo(ExtraTypeInfoType::ENUM_TYPE_INFO), values_insert_order(values_insert_order_p),\n       dict_type(EnumDictType::VECTOR_DICT), dict_size(dict_size_p) {\ndiff --git a/src/execution/operator/csv_scanner/sniffer/csv_sniffer.cpp b/src/execution/operator/csv_scanner/sniffer/csv_sniffer.cpp\nindex bee31f880eb7..baefcff45fa4 100644\n--- a/src/execution/operator/csv_scanner/sniffer/csv_sniffer.cpp\n+++ b/src/execution/operator/csv_scanner/sniffer/csv_sniffer.cpp\n@@ -41,7 +41,7 @@ void MatchAndReplace(CSVOption<T> &original, CSVOption<T> &sniffed, const string\n \t\t// We verify that the user input matches the sniffed value\n \t\tif (original != sniffed) {\n \t\t\terror += \"CSV Sniffer: Sniffer detected value different than the user input for the \" + name;\n-\t\t\terror += \" options \\n Set: \" + original.FormatValue() + \" Sniffed: \" + sniffed.FormatValue() + \"\\n\";\n+\t\t\terror += \" options \\n Set: \" + original.FormatValue() + \", Sniffed: \" + sniffed.FormatValue() + \"\\n\";\n \t\t}\n \t} else {\n \t\t// We replace the value of original with the sniffed value\n@@ -228,8 +228,8 @@ SnifferResult CSVSniffer::SniffCSV(bool force_match) {\n \t\t\tif (set_names.size() == names.size()) {\n \t\t\t\tfor (idx_t i = 0; i < set_columns.Size(); i++) {\n \t\t\t\t\tif (set_names[i] != names[i]) {\n-\t\t\t\t\t\theader_error += \"Column at position: \" + to_string(i) + \" Set name: \" + set_names[i] +\n-\t\t\t\t\t\t                \" Sniffed Name: \" + names[i] + \"\\n\";\n+\t\t\t\t\t\theader_error += \"Column at position: \" + to_string(i) + \", Set name: \" + set_names[i] +\n+\t\t\t\t\t\t                \", Sniffed Name: \" + names[i] + \"\\n\";\n \t\t\t\t\t\tmatch = false;\n \t\t\t\t\t}\n \t\t\t\t}\ndiff --git a/src/execution/operator/csv_scanner/sniffer/header_detection.cpp b/src/execution/operator/csv_scanner/sniffer/header_detection.cpp\nindex fd050400ce0e..cf42f56dd3a1 100644\n--- a/src/execution/operator/csv_scanner/sniffer/header_detection.cpp\n+++ b/src/execution/operator/csv_scanner/sniffer/header_detection.cpp\n@@ -114,9 +114,9 @@ bool CSVSniffer::DetectHeaderWithSetColumn(ClientContext &context, vector<Header\n \t\t\treturn false;\n \t\t}\n \t\tif (best_header_row[i].value != (*set_columns.names)[i]) {\n-\t\t\terror << \"Header Mismatch at position:\" << i << \"\\n\";\n-\t\t\terror << \"Expected Name: \\\"\" << (*set_columns.names)[i] << \"\\\".\";\n-\t\t\terror << \"Actual Name: \\\"\" << best_header_row[i].value << \"\\\".\"\n+\t\t\terror << \"Header mismatch at position: \" << i << \"\\n\";\n+\t\t\terror << \"Expected name: \\\"\" << (*set_columns.names)[i] << \"\\\", \";\n+\t\t\terror << \"Actual name: \\\"\" << best_header_row[i].value << \"\\\".\"\n \t\t\t      << \"\\n\";\n \t\t\thas_header = false;\n \t\t\tbreak;\ndiff --git a/src/execution/operator/csv_scanner/util/csv_reader_options.cpp b/src/execution/operator/csv_scanner/util/csv_reader_options.cpp\nindex 21f910ece87a..4ec6541ae322 100644\n--- a/src/execution/operator/csv_scanner/util/csv_reader_options.cpp\n+++ b/src/execution/operator/csv_scanner/util/csv_reader_options.cpp\n@@ -404,7 +404,7 @@ string CSVReaderOptions::ToString(const string &current_file_path) const {\n \tauto &skip_rows = dialect_options.skip_rows;\n \n \tauto &header = dialect_options.header;\n-\tstring error = \"  file=\" + current_file_path + \"\\n  \";\n+\tstring error = \"  file = \" + current_file_path + \"\\n  \";\n \t// Let's first print options that can either be set by the user or by the sniffer\n \t// delimiter\n \terror += FormatOptionLine(\"delimiter\", delimiter);\n@@ -427,13 +427,13 @@ string CSVReaderOptions::ToString(const string &current_file_path) const {\n \n \t// Now we do options that can only be set by the user, that might hold some general significance\n \t// null padding\n-\terror += \"null_padding=\" + std::to_string(null_padding) + \"\\n  \";\n+\terror += \"null_padding = \" + std::to_string(null_padding) + \"\\n  \";\n \t// sample_size\n-\terror += \"sample_size=\" + std::to_string(sample_size_chunks * STANDARD_VECTOR_SIZE) + \"\\n  \";\n+\terror += \"sample_size = \" + std::to_string(sample_size_chunks * STANDARD_VECTOR_SIZE) + \"\\n  \";\n \t// ignore_errors\n-\terror += \"ignore_errors=\" + ignore_errors.FormatValue() + \"\\n  \";\n+\terror += \"ignore_errors = \" + ignore_errors.FormatValue() + \"\\n  \";\n \t// all_varchar\n-\terror += \"all_varchar=\" + std::to_string(all_varchar) + \"\\n\";\n+\terror += \"all_varchar = \" + std::to_string(all_varchar) + \"\\n\";\n \n \t// Add information regarding sniffer mismatches (if any)\n \terror += sniffer_user_mismatch_error;\ndiff --git a/src/include/duckdb/common/extra_type_info/enum_type_info.hpp b/src/include/duckdb/common/extra_type_info/enum_type_info.hpp\nnew file mode 100644\nindex 000000000000..3949caa8bf04\n--- /dev/null\n+++ b/src/include/duckdb/common/extra_type_info/enum_type_info.hpp\n@@ -0,0 +1,53 @@\n+#pragma once\n+\n+#include \"duckdb/common/extra_type_info.hpp\"\n+#include \"duckdb/common/serializer/deserializer.hpp\"\n+#include \"duckdb/common/string_map_set.hpp\"\n+\n+namespace duckdb {\n+\n+template <class T>\n+struct EnumTypeInfoTemplated : public EnumTypeInfo {\n+\texplicit EnumTypeInfoTemplated(Vector &values_insert_order_p, idx_t size_p)\n+\t    : EnumTypeInfo(values_insert_order_p, size_p) {\n+\t\tD_ASSERT(values_insert_order_p.GetType().InternalType() == PhysicalType::VARCHAR);\n+\n+\t\tUnifiedVectorFormat vdata;\n+\t\tvalues_insert_order.ToUnifiedFormat(size_p, vdata);\n+\n+\t\tauto data = UnifiedVectorFormat::GetData<string_t>(vdata);\n+\t\tfor (idx_t i = 0; i < size_p; i++) {\n+\t\t\tauto idx = vdata.sel->get_index(i);\n+\t\t\tif (!vdata.validity.RowIsValid(idx)) {\n+\t\t\t\tthrow InternalException(\"Attempted to create ENUM type with NULL value\");\n+\t\t\t}\n+\t\t\tif (values.count(data[idx]) > 0) {\n+\t\t\t\tthrow InvalidInputException(\"Attempted to create ENUM type with duplicate value %s\",\n+\t\t\t\t                            data[idx].GetString());\n+\t\t\t}\n+\t\t\tvalues[data[idx]] = UnsafeNumericCast<T>(i);\n+\t\t}\n+\t}\n+\n+\tstatic shared_ptr<EnumTypeInfoTemplated> Deserialize(Deserializer &deserializer, uint32_t size) {\n+\t\tVector values_insert_order(LogicalType::VARCHAR, size);\n+\t\tauto strings = FlatVector::GetData<string_t>(values_insert_order);\n+\n+\t\tdeserializer.ReadList(201, \"values\", [&](Deserializer::List &list, idx_t i) {\n+\t\t\tstrings[i] = StringVector::AddStringOrBlob(values_insert_order, list.ReadElement<string>());\n+\t\t});\n+\t\treturn make_shared_ptr<EnumTypeInfoTemplated>(values_insert_order, size);\n+\t}\n+\n+\tconst string_map_t<T> &GetValues() const {\n+\t\treturn values;\n+\t}\n+\n+\tEnumTypeInfoTemplated(const EnumTypeInfoTemplated &) = delete;\n+\tEnumTypeInfoTemplated &operator=(const EnumTypeInfoTemplated &) = delete;\n+\n+private:\n+\tstring_map_t<T> values;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/storage/table/segment_tree.hpp b/src/include/duckdb/storage/table/segment_tree.hpp\nindex e267d2e351eb..218cdac9b16f 100644\n--- a/src/include/duckdb/storage/table/segment_tree.hpp\n+++ b/src/include/duckdb/storage/table/segment_tree.hpp\n@@ -145,6 +145,7 @@ class SegmentTree {\n \t\t}\n \t\tSegmentNode<T> node;\n \t\tsegment->index = nodes.size();\n+\t\tsegment->next = nullptr;\n \t\tnode.row_start = segment->start;\n \t\tnode.node = std::move(segment);\n \t\tnodes.push_back(std::move(node));\ndiff --git a/src/storage/table/row_group_collection.cpp b/src/storage/table/row_group_collection.cpp\nindex b0eb5022f75b..4dfb194921b6 100644\n--- a/src/storage/table/row_group_collection.cpp\n+++ b/src/storage/table/row_group_collection.cpp\n@@ -962,8 +962,8 @@ unique_ptr<CheckpointTask> RowGroupCollection::GetCheckpointTask(CollectionCheck\n }\n \n void RowGroupCollection::Checkpoint(TableDataWriter &writer, TableStatistics &global_stats) {\n-\tauto segments = row_groups->MoveSegments();\n \tauto l = row_groups->Lock();\n+\tauto segments = row_groups->MoveSegments(l);\n \n \tCollectionCheckpointState checkpoint_state(*this, writer, segments, global_stats);\n \n",
  "test_patch": "diff --git a/test/sql/storage/delete/delete_final_row_group.test b/test/sql/storage/delete/delete_final_row_group.test\nnew file mode 100644\nindex 000000000000..6dba3aca5923\n--- /dev/null\n+++ b/test/sql/storage/delete/delete_final_row_group.test\n@@ -0,0 +1,42 @@\n+# name: test/sql/storage/delete/delete_final_row_group.test\n+# description: Test deleting the final row groups of a table\n+# group: [delete]\n+\n+load __TEST_DIR__/delete_final_row_group.db\n+\n+statement ok\n+CREATE TABLE integers AS SELECT * FROM range(0, 10000000) t(i);\n+\n+query I\n+DELETE FROM integers WHERE i>=5000000\n+----\n+5000000\n+\n+query II\n+SELECT COUNT(*), SUM(i) FROM integers\n+----\n+5000000\t12499997500000\n+\n+\n+statement ok\n+ALTER TABLE integers ADD COLUMN j INTEGER\n+\n+query III\n+SELECT COUNT(*), SUM(i), SUM(j) FROM integers\n+----\n+5000000\t12499997500000\tNULL\n+\n+statement ok\n+UPDATE integers SET j=i+1\n+\n+query III\n+SELECT COUNT(*), SUM(i), SUM(j) FROM integers\n+----\n+5000000\t12499997500000\t12500002500000\n+\n+restart\n+\n+query III\n+SELECT COUNT(*), SUM(i), SUM(j) FROM integers\n+----\n+5000000\t12499997500000\t12500002500000\ndiff --git a/test/sqlite/sqllogic_command.cpp b/test/sqlite/sqllogic_command.cpp\nindex eb7636362900..527b91c6d115 100644\n--- a/test/sqlite/sqllogic_command.cpp\n+++ b/test/sqlite/sqllogic_command.cpp\n@@ -492,14 +492,6 @@ void UnzipCommand::ExecuteInternal(ExecuteContext &context) const {\n \t\tthrow CatalogException(\"Cannot open the file \\\"%s\\\"\", input_path);\n \t}\n \n-\t// read the compressed data from the file\n-\tint64_t file_size = vfs.GetFileSize(*compressed_file_handle);\n-\tstd::unique_ptr<char[]> compressed_buffer(new char[BUFFER_SIZE]);\n-\tint64_t bytes_read = vfs.Read(*compressed_file_handle, compressed_buffer.get(), BUFFER_SIZE);\n-\tif (bytes_read < file_size) {\n-\t\tthrow CatalogException(\"Cannot read the file \\\"%s\\\"\", input_path);\n-\t}\n-\n \t// output\n \tFileOpenFlags out_flags(FileOpenFlags::FILE_FLAGS_FILE_CREATE | FileOpenFlags::FILE_FLAGS_WRITE);\n \tauto output_file = vfs.OpenFile(extraction_path, out_flags);\n@@ -507,9 +499,15 @@ void UnzipCommand::ExecuteInternal(ExecuteContext &context) const {\n \t\tthrow CatalogException(\"Cannot open the file \\\"%s\\\"\", extraction_path);\n \t}\n \n-\tint64_t bytes_written = vfs.Write(*output_file, compressed_buffer.get(), BUFFER_SIZE);\n-\tif (bytes_written < file_size) {\n-\t\tthrow CatalogException(\"Cannot write the file \\\"%s\\\"\", extraction_path);\n+\t// read the compressed data from the file\n+\twhile (true) {\n+\t\tstd::unique_ptr<char[]> compressed_buffer(new char[BUFFER_SIZE]);\n+\t\tint64_t bytes_read = vfs.Read(*compressed_file_handle, compressed_buffer.get(), BUFFER_SIZE);\n+\t\tif (bytes_read == 0) {\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\tvfs.Write(*output_file, compressed_buffer.get(), bytes_read);\n \t}\n }\n \n",
  "problem_statement": "Segmentation fault\n### What happens?\n\nSometimes a segmentation fault when DuckDB is backed by a file, in-memory it does not issue a seg fault.\r\n\r\nOut of 277 datasets, only 5 have this issue. With provided dataset, can always be reproduced.\r\n\r\n```\r\n(gdb) bt\r\n#0  0x0000555555775ed6 in std::__shared_ptr<duckdb::DataTableInfo, (__gnu_cxx::_Lock_policy)2>::get (this=0x29c) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1667\r\n#1  0x00005555557688de in duckdb::shared_ptr<duckdb::DataTableInfo, true>::operator* (this=0x29c) at /home/skinkie/Sources/duckdb/src/include/duckdb/common/shared_ptr_ipp.hpp:196\r\n#2  0x000055555621bb98 in duckdb::RowGroupCollection::GetTableInfo (this=0x28c) at /home/skinkie/Sources/duckdb/src/include/duckdb/storage/table/row_group_collection.hpp:129\r\n#3  0x00005555561e5cc6 in duckdb::RowGroup::GetTableInfo (this=0x55555a7159e0) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:135\r\n#4  0x00005555561e6c1a in duckdb::RowGroup::AddColumn (this=0x55555a7159e0, new_collection=..., new_column=..., executor=..., result=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:305\r\n#5  0x00005555561f019a in duckdb::RowGroupCollection::AddColumn (this=0x55555a1ee9c0, context=..., new_column=..., default_executor=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group_collection.cpp:1066\r\n#6  0x0000555556279e05 in duckdb::DataTable::DataTable (this=0x55555a80eaf0, context=..., parent=..., new_column=..., default_value=...) at /home/skinkie/Sources/duckdb/src/storage/data_table.cpp:80\r\n#7  0x00005555557a16e8 in std::_Construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)\r\n    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/stl_construct.h:119\r\n#8  0x000055555579c4e8 in std::allocator_traits<std::allocator<void> >::construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)\r\n    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/alloc_traits.h:657\r\n#9  std::_Sp_counted_ptr_inplace<duckdb::DataTable, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (\r\n    this=0x55555a80eae0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:607\r\n#10 0x00005555557974e8 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<duckdb::DataTable, std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (\r\n    this=0x7fffffffa588, __p=@0x7fffffffa580: 0x0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:969\r\n#11 0x00005555557916e2 in std::__shared_ptr<duckdb::DataTable, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (\r\n    this=0x7fffffffa580, __tag=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1713\r\n#12 0x0000555555788805 in std::shared_ptr<duckdb::DataTable>::shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (this=0x7fffffffa580, __tag=...)\r\n    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:463\r\n#13 0x000055555577cb90 in std::make_shared<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:1008\r\n#14 0x000055555576f1f0 in duckdb::make_shared_ptr<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /home/skinkie/Sources/duckdb/src/include/duckdb/common/helper.hpp:73\r\n#15 0x0000555555753e98 in duckdb::DuckTableEntry::AddColumn (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:391\r\n#16 0x000055555575271e in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:231\r\n#17 0x00005555557522bc in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:184\r\n#18 0x00005555557b6bd8 in duckdb::CatalogSet::AlterEntry (this=0x55555a16be20, transaction=..., name=\"kv6_import\", alter_info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_set.cpp:328\r\n#19 0x000055555574fcf9 in duckdb::DuckSchemaEntry::Alter (this=0x55555a16bd40, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_schema_entry.cpp:291\r\n#20 0x00005555557b156a in duckdb::Catalog::Alter (this=0x55555a1704e0, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:904\r\n#21 0x00005555557b176b in duckdb::Catalog::Alter (this=0x55555a1704e0, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:912\r\n#22 0x00005555570b1ff5 in duckdb::PhysicalAlter::GetData (this=0x7fffdc5f55a0, context=..., chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/execution/operator/schema/physical_alter.cpp:13\r\n#23 0x0000555555ff23af in duckdb::PipelineExecutor::GetData (this=0x55555a31e250, chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:478\r\n#24 0x0000555555ff24a0 in duckdb::PipelineExecutor::FetchFromSource (this=0x55555a31e250, result=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:504\r\n#25 0x0000555555ff1268 in duckdb::PipelineExecutor::Execute (this=0x55555a31e250, max_chunks=50) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:204\r\n#26 0x0000555555fed597 in duckdb::PipelineTask::ExecuteTask (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/pipeline.cpp:40\r\n#27 0x0000555555fe7c38 in duckdb::ExecutorTask::Execute (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/executor_task.cpp:44\r\n#28 0x0000555555feb739 in duckdb::Executor::ExecuteTask (this=0x55555a173350, dry_run=false) at /home/skinkie/Sources/duckdb/src/parallel/executor.cpp:580\r\n#29 0x0000555555eab6d5 in duckdb::ClientContext::ExecuteTaskInternal (this=0x55555a16e960, lock=..., result=..., dry_run=false) at /home/skinkie/Sources/duckdb/src/main/client_context.cpp:557\r\n#30 0x0000555555ec66bd in duckdb::PendingQueryResult::ExecuteTaskInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:68\r\n#31 0x0000555555ec6751 in duckdb::PendingQueryResult::ExecuteInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:75\r\n#32 0x0000555555ec6905 in duckdb::PendingQueryResult::Execute (this=0x55555a46b6d0) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:95\r\n#33 0x0000555555ec72be in duckdb::PreparedStatement::Execute (this=0x55555a4dd5b0, values=..., allow_stream_result=false) at /home/skinkie/Sources/duckdb/src/main/prepared_statement.cpp:85\r\n#34 0x00005555556b91da in duckdb_shell_sqlite3_print_duckbox (pStmt=0x7fffcc091870, max_rows=40, max_width=0, null_value=0x7fffffffc03c \"\", columnar=0)\r\n    at /home/skinkie/Sources/duckdb/tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp:249\r\n#35 0x000055555568be00 in exec_prepared_stmt (pArg=0x7fffffffbf20, pStmt=0x7fffcc091870) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:12752\r\n#36 0x000055555568cc4c in shell_exec (pArg=0x7fffffffbf20, zSql=0x55555a1ef160 \"alter table kv6_import add column trip_id varchar(16);\", pzErrMsg=0x7fffffffbdb0) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:13087\r\n#37 0x0000555555699c79 in runOneSqlLine (p=0x7fffffffbf20, zSql=0x55555a1ef160 \"alter table kv6_import add column trip_id varchar(16);\", in=0x55555a31e000, startline=31) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19273\r\n#38 0x000055555569a16f in process_input (p=0x7fffffffbf20) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19384\r\n#39 0x000055555569a49a in process_sqliterc (p=0x7fffffffbf20, sqliterc_override=0x7fffffffd720 \"/tmp/script.sql\") at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19505\r\n#40 0x000055555569b174 in main (argc=4, argv=0x7fffffffd258) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19951\r\n```\n\n### To Reproduce\n\n```sql\r\nDROP TABLE IF EXISTS kv6_import;\r\nDROP TABLE IF EXISTS tt_import;\r\n\r\nCREATE TABLE \"kv6_import\" (\r\n        \"receive\"                   TIMESTAMP     NOT NULL,\r\n        \"message\"                   TIMESTAMP     NOT NULL,\r\n        \"vehicle\"                   TIMESTAMP     NOT NULL,\r\n        \"messagetype\"               VARCHAR(10)   NOT NULL,\r\n        \"operatingday\"              DATE          NOT NULL,\r\n        \"dataownercode\"             VARCHAR(10)   NOT NULL,\r\n        \"lineplanningnumber\"        VARCHAR(10),\r\n        \"journeynumber\"             UINTEGER       NOT NULL,\r\n        \"reinforcementnumber\"       UTINYINT       NOT NULL,\r\n        \"userstopcode\"              VARCHAR(10),\r\n        \"passagesequencenumber\"     TINYINT,\r\n        \"distancesincelastuserstop\" INTEGER,\r\n        \"punctuality\"               INTEGER,\r\n        \"rd_x\"                      VARCHAR(11),\r\n        \"rd_y\"                      VARCHAR(11),\r\n        \"blockcode\"                 INTEGER,\r\n        \"vehiclenumber\"             SMALLINT,\r\n        \"wheelchairaccessible\"      VARCHAR(5),\r\n        \"source\"                    VARCHAR(10)   NOT NULL,\r\n        \"numberofcoaches\"           UTINYINT\r\n);\r\n\r\ncopy kv6_import from '/tmp/kv6-20240917.log' (DELIMITER ';');\r\n\r\ndelete from kv6_import where messagetype not in ('ARRIVAL', 'DEPARTURE');\r\n\r\nalter table kv6_import add column trip_id varchar(16);\r\n\r\nupdate kv6_import set trip_id = dataownercode || ':' || lineplanningnumber || ':' || journeynumber;\r\n\r\nCREATE TABLE \"tt_import\" (\r\n        \"operatingday\"          DATE          NOT NULL,\r\n        \"trip_id\"               VARCHAR(16)   NOT NULL,\r\n        \"pointorder\"            UTINYINT      NOT NULL,\r\n        \"passagesequencenumber\" UTINYINT      NOT NULL,\r\n        \"userstopcode\"          VARCHAR(10)   NOT NULL,\r\n        \"targetarrivaltime\"     VARCHAR(8)    NOT NULL,\r\n        \"targetdeparturetime\"   VARCHAR(8)    NOT NULL,\r\n        \"recordedarrivaltime\"   VARCHAR(8),\r\n        \"recordeddeparturetime\" VARCHAR(8)\r\n);\r\n\r\ncopy tt_import from '/tmp/transittimes.csv' (DELIMITER ',');\r\n\r\ndelete from tt_import where trip_id like 'DOEKSEN:%' or trip_id like 'WSF:%' or trip_id like 'IFF%' or trip_id like 'WPD%' or trip_id like 'TESO%';\r\n\r\nalter table tt_import drop column recordedarrivaltime;\r\nalter table tt_import drop column recordeddeparturetime;\r\n\r\nupdate kv6_import set trip_id = w.trip_id from (select tt_import.trip_id as trip_id, z.trip_id as kv6_trip_id, vehiclenumber from tt_import join (select trip_id, userstopcode, vehiclenumber from kv6_import join (select trip_id, min(receive) as receive from kv6_import where trip_id in (select trip_id from kv6_import where trip_id like 'ARR:%' and length(trip_id) = 13  except select trip_id from tt_import) group by trip_id) as y using (trip_id, receive)) as z using (userstopcode) where pointorder = 1 and tt_import.trip_id like z.trip_id[0:8] || '_' || z.trip_id[-5:]) as w where kv6_import.trip_id = kv6_trip_id and kv6_import.vehiclenumber = w.vehiclenumber;\r\n\r\ncopy (select tt_import.operatingday, tt_import.trip_id, x.reinforcementnumber, x.vehiclenumber, tt_import.userstopcode, tt_import.passagesequencenumber, tt_import.pointorder, tt_import.targetarrivaltime, tt_import.targetdeparturetime, x.arrival as recordedarrivaltime, x.departure as recordeddeparturetime from (select trip_id, reinforcementnumber, vehiclenumber, userstopcode, passagesequencenumber, max(a.receive) as arrival, max(b.receive) as departure from kv6_import as a full outer join kv6_import as b using (trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) where a.messagetype = 'ARRIVAL' and b.messagetype = 'DEPARTURE' group by trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) as x right join tt_import using (trip_id, userstopcode, passagesequencenumber) order by trip_id, reinforcementnumber, pointorder) to '/home/skinkie/arriva/volledig-20240917.csv.gz' header;\r\n```\r\n\r\nData:\r\nhttps://download.stefan.konink.de/duckdb/kv6-20240917.log.gz\r\nhttps://download.stefan.konink.de/duckdb/transittimes.csv.gz\r\n\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\nmain\n\n### DuckDB Client:\n\ncli\n\n### Hardware:\n\namd64\n\n### Full Name:\n\nStefan de Konink\n\n### Affiliation:\n\nStichting OpenGeo\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNo - Other reason (please specify in the issue body)\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\nSegmentation fault\n### What happens?\n\nSometimes a segmentation fault when DuckDB is backed by a file, in-memory it does not issue a seg fault.\r\n\r\nOut of 277 datasets, only 5 have this issue. With provided dataset, can always be reproduced.\r\n\r\n```\r\n(gdb) bt\r\n#0  0x0000555555775ed6 in std::__shared_ptr<duckdb::DataTableInfo, (__gnu_cxx::_Lock_policy)2>::get (this=0x29c) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1667\r\n#1  0x00005555557688de in duckdb::shared_ptr<duckdb::DataTableInfo, true>::operator* (this=0x29c) at /home/skinkie/Sources/duckdb/src/include/duckdb/common/shared_ptr_ipp.hpp:196\r\n#2  0x000055555621bb98 in duckdb::RowGroupCollection::GetTableInfo (this=0x28c) at /home/skinkie/Sources/duckdb/src/include/duckdb/storage/table/row_group_collection.hpp:129\r\n#3  0x00005555561e5cc6 in duckdb::RowGroup::GetTableInfo (this=0x55555a7159e0) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:135\r\n#4  0x00005555561e6c1a in duckdb::RowGroup::AddColumn (this=0x55555a7159e0, new_collection=..., new_column=..., executor=..., result=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:305\r\n#5  0x00005555561f019a in duckdb::RowGroupCollection::AddColumn (this=0x55555a1ee9c0, context=..., new_column=..., default_executor=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group_collection.cpp:1066\r\n#6  0x0000555556279e05 in duckdb::DataTable::DataTable (this=0x55555a80eaf0, context=..., parent=..., new_column=..., default_value=...) at /home/skinkie/Sources/duckdb/src/storage/data_table.cpp:80\r\n#7  0x00005555557a16e8 in std::_Construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)\r\n    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/stl_construct.h:119\r\n#8  0x000055555579c4e8 in std::allocator_traits<std::allocator<void> >::construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)\r\n    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/alloc_traits.h:657\r\n#9  std::_Sp_counted_ptr_inplace<duckdb::DataTable, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (\r\n    this=0x55555a80eae0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:607\r\n#10 0x00005555557974e8 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<duckdb::DataTable, std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (\r\n    this=0x7fffffffa588, __p=@0x7fffffffa580: 0x0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:969\r\n#11 0x00005555557916e2 in std::__shared_ptr<duckdb::DataTable, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (\r\n    this=0x7fffffffa580, __tag=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1713\r\n#12 0x0000555555788805 in std::shared_ptr<duckdb::DataTable>::shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (this=0x7fffffffa580, __tag=...)\r\n    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:463\r\n#13 0x000055555577cb90 in std::make_shared<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:1008\r\n#14 0x000055555576f1f0 in duckdb::make_shared_ptr<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /home/skinkie/Sources/duckdb/src/include/duckdb/common/helper.hpp:73\r\n#15 0x0000555555753e98 in duckdb::DuckTableEntry::AddColumn (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:391\r\n#16 0x000055555575271e in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:231\r\n#17 0x00005555557522bc in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:184\r\n#18 0x00005555557b6bd8 in duckdb::CatalogSet::AlterEntry (this=0x55555a16be20, transaction=..., name=\"kv6_import\", alter_info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_set.cpp:328\r\n#19 0x000055555574fcf9 in duckdb::DuckSchemaEntry::Alter (this=0x55555a16bd40, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_schema_entry.cpp:291\r\n#20 0x00005555557b156a in duckdb::Catalog::Alter (this=0x55555a1704e0, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:904\r\n#21 0x00005555557b176b in duckdb::Catalog::Alter (this=0x55555a1704e0, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:912\r\n#22 0x00005555570b1ff5 in duckdb::PhysicalAlter::GetData (this=0x7fffdc5f55a0, context=..., chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/execution/operator/schema/physical_alter.cpp:13\r\n#23 0x0000555555ff23af in duckdb::PipelineExecutor::GetData (this=0x55555a31e250, chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:478\r\n#24 0x0000555555ff24a0 in duckdb::PipelineExecutor::FetchFromSource (this=0x55555a31e250, result=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:504\r\n#25 0x0000555555ff1268 in duckdb::PipelineExecutor::Execute (this=0x55555a31e250, max_chunks=50) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:204\r\n#26 0x0000555555fed597 in duckdb::PipelineTask::ExecuteTask (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/pipeline.cpp:40\r\n#27 0x0000555555fe7c38 in duckdb::ExecutorTask::Execute (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/executor_task.cpp:44\r\n#28 0x0000555555feb739 in duckdb::Executor::ExecuteTask (this=0x55555a173350, dry_run=false) at /home/skinkie/Sources/duckdb/src/parallel/executor.cpp:580\r\n#29 0x0000555555eab6d5 in duckdb::ClientContext::ExecuteTaskInternal (this=0x55555a16e960, lock=..., result=..., dry_run=false) at /home/skinkie/Sources/duckdb/src/main/client_context.cpp:557\r\n#30 0x0000555555ec66bd in duckdb::PendingQueryResult::ExecuteTaskInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:68\r\n#31 0x0000555555ec6751 in duckdb::PendingQueryResult::ExecuteInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:75\r\n#32 0x0000555555ec6905 in duckdb::PendingQueryResult::Execute (this=0x55555a46b6d0) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:95\r\n#33 0x0000555555ec72be in duckdb::PreparedStatement::Execute (this=0x55555a4dd5b0, values=..., allow_stream_result=false) at /home/skinkie/Sources/duckdb/src/main/prepared_statement.cpp:85\r\n#34 0x00005555556b91da in duckdb_shell_sqlite3_print_duckbox (pStmt=0x7fffcc091870, max_rows=40, max_width=0, null_value=0x7fffffffc03c \"\", columnar=0)\r\n    at /home/skinkie/Sources/duckdb/tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp:249\r\n#35 0x000055555568be00 in exec_prepared_stmt (pArg=0x7fffffffbf20, pStmt=0x7fffcc091870) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:12752\r\n#36 0x000055555568cc4c in shell_exec (pArg=0x7fffffffbf20, zSql=0x55555a1ef160 \"alter table kv6_import add column trip_id varchar(16);\", pzErrMsg=0x7fffffffbdb0) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:13087\r\n#37 0x0000555555699c79 in runOneSqlLine (p=0x7fffffffbf20, zSql=0x55555a1ef160 \"alter table kv6_import add column trip_id varchar(16);\", in=0x55555a31e000, startline=31) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19273\r\n#38 0x000055555569a16f in process_input (p=0x7fffffffbf20) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19384\r\n#39 0x000055555569a49a in process_sqliterc (p=0x7fffffffbf20, sqliterc_override=0x7fffffffd720 \"/tmp/script.sql\") at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19505\r\n#40 0x000055555569b174 in main (argc=4, argv=0x7fffffffd258) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19951\r\n```\n\n### To Reproduce\n\n```sql\r\nDROP TABLE IF EXISTS kv6_import;\r\nDROP TABLE IF EXISTS tt_import;\r\n\r\nCREATE TABLE \"kv6_import\" (\r\n        \"receive\"                   TIMESTAMP     NOT NULL,\r\n        \"message\"                   TIMESTAMP     NOT NULL,\r\n        \"vehicle\"                   TIMESTAMP     NOT NULL,\r\n        \"messagetype\"               VARCHAR(10)   NOT NULL,\r\n        \"operatingday\"              DATE          NOT NULL,\r\n        \"dataownercode\"             VARCHAR(10)   NOT NULL,\r\n        \"lineplanningnumber\"        VARCHAR(10),\r\n        \"journeynumber\"             UINTEGER       NOT NULL,\r\n        \"reinforcementnumber\"       UTINYINT       NOT NULL,\r\n        \"userstopcode\"              VARCHAR(10),\r\n        \"passagesequencenumber\"     TINYINT,\r\n        \"distancesincelastuserstop\" INTEGER,\r\n        \"punctuality\"               INTEGER,\r\n        \"rd_x\"                      VARCHAR(11),\r\n        \"rd_y\"                      VARCHAR(11),\r\n        \"blockcode\"                 INTEGER,\r\n        \"vehiclenumber\"             SMALLINT,\r\n        \"wheelchairaccessible\"      VARCHAR(5),\r\n        \"source\"                    VARCHAR(10)   NOT NULL,\r\n        \"numberofcoaches\"           UTINYINT\r\n);\r\n\r\ncopy kv6_import from '/tmp/kv6-20240917.log' (DELIMITER ';');\r\n\r\ndelete from kv6_import where messagetype not in ('ARRIVAL', 'DEPARTURE');\r\n\r\nalter table kv6_import add column trip_id varchar(16);\r\n\r\nupdate kv6_import set trip_id = dataownercode || ':' || lineplanningnumber || ':' || journeynumber;\r\n\r\nCREATE TABLE \"tt_import\" (\r\n        \"operatingday\"          DATE          NOT NULL,\r\n        \"trip_id\"               VARCHAR(16)   NOT NULL,\r\n        \"pointorder\"            UTINYINT      NOT NULL,\r\n        \"passagesequencenumber\" UTINYINT      NOT NULL,\r\n        \"userstopcode\"          VARCHAR(10)   NOT NULL,\r\n        \"targetarrivaltime\"     VARCHAR(8)    NOT NULL,\r\n        \"targetdeparturetime\"   VARCHAR(8)    NOT NULL,\r\n        \"recordedarrivaltime\"   VARCHAR(8),\r\n        \"recordeddeparturetime\" VARCHAR(8)\r\n);\r\n\r\ncopy tt_import from '/tmp/transittimes.csv' (DELIMITER ',');\r\n\r\ndelete from tt_import where trip_id like 'DOEKSEN:%' or trip_id like 'WSF:%' or trip_id like 'IFF%' or trip_id like 'WPD%' or trip_id like 'TESO%';\r\n\r\nalter table tt_import drop column recordedarrivaltime;\r\nalter table tt_import drop column recordeddeparturetime;\r\n\r\nupdate kv6_import set trip_id = w.trip_id from (select tt_import.trip_id as trip_id, z.trip_id as kv6_trip_id, vehiclenumber from tt_import join (select trip_id, userstopcode, vehiclenumber from kv6_import join (select trip_id, min(receive) as receive from kv6_import where trip_id in (select trip_id from kv6_import where trip_id like 'ARR:%' and length(trip_id) = 13  except select trip_id from tt_import) group by trip_id) as y using (trip_id, receive)) as z using (userstopcode) where pointorder = 1 and tt_import.trip_id like z.trip_id[0:8] || '_' || z.trip_id[-5:]) as w where kv6_import.trip_id = kv6_trip_id and kv6_import.vehiclenumber = w.vehiclenumber;\r\n\r\ncopy (select tt_import.operatingday, tt_import.trip_id, x.reinforcementnumber, x.vehiclenumber, tt_import.userstopcode, tt_import.passagesequencenumber, tt_import.pointorder, tt_import.targetarrivaltime, tt_import.targetdeparturetime, x.arrival as recordedarrivaltime, x.departure as recordeddeparturetime from (select trip_id, reinforcementnumber, vehiclenumber, userstopcode, passagesequencenumber, max(a.receive) as arrival, max(b.receive) as departure from kv6_import as a full outer join kv6_import as b using (trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) where a.messagetype = 'ARRIVAL' and b.messagetype = 'DEPARTURE' group by trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) as x right join tt_import using (trip_id, userstopcode, passagesequencenumber) order by trip_id, reinforcementnumber, pointorder) to '/home/skinkie/arriva/volledig-20240917.csv.gz' header;\r\n```\r\n\r\nData:\r\nhttps://download.stefan.konink.de/duckdb/kv6-20240917.log.gz\r\nhttps://download.stefan.konink.de/duckdb/transittimes.csv.gz\r\n\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\nmain\n\n### DuckDB Client:\n\ncli\n\n### Hardware:\n\namd64\n\n### Full Name:\n\nStefan de Konink\n\n### Affiliation:\n\nStichting OpenGeo\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNo - Other reason (please specify in the issue body)\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n",
  "hints_text": "\n",
  "created_at": "2024-08-07T14:36:42Z"
}