{
  "repo": "duckdb/duckdb",
  "pull_number": 11468,
  "instance_id": "duckdb__duckdb-11468",
  "issue_numbers": [
    "4026"
  ],
  "base_commit": "2be8c43ca39a48545da675f6f1092ca38f8cf627",
  "patch": "diff --git a/tools/pythonpkg/src/numpy/array_wrapper.cpp b/tools/pythonpkg/src/numpy/array_wrapper.cpp\nindex 649b9548bdec..e82fb417447e 100644\n--- a/tools/pythonpkg/src/numpy/array_wrapper.cpp\n+++ b/tools/pythonpkg/src/numpy/array_wrapper.cpp\n@@ -17,7 +17,8 @@ namespace duckdb_py_convert {\n \n struct RegularConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic NUMPY_T ConvertValue(DUCKDB_T val) {\n+\tstatic NUMPY_T ConvertValue(DUCKDB_T val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\treturn (NUMPY_T)val;\n \t}\n \n@@ -30,7 +31,8 @@ struct RegularConvert {\n \n struct TimestampConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic int64_t ConvertValue(timestamp_t val) {\n+\tstatic int64_t ConvertValue(timestamp_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\tif (!Timestamp::IsFinite(val)) {\n \t\t\treturn val.value;\n \t\t}\n@@ -46,7 +48,8 @@ struct TimestampConvert {\n \n struct TimestampConvertSec {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic int64_t ConvertValue(timestamp_t val) {\n+\tstatic int64_t ConvertValue(timestamp_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\tif (!Timestamp::IsFinite(val)) {\n \t\t\treturn val.value;\n \t\t}\n@@ -62,7 +65,8 @@ struct TimestampConvertSec {\n \n struct TimestampConvertMilli {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic int64_t ConvertValue(timestamp_t val) {\n+\tstatic int64_t ConvertValue(timestamp_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\tif (!Timestamp::IsFinite(val)) {\n \t\t\treturn val.value;\n \t\t}\n@@ -78,7 +82,8 @@ struct TimestampConvertMilli {\n \n struct TimestampConvertNano {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic int64_t ConvertValue(timestamp_t val) {\n+\tstatic int64_t ConvertValue(timestamp_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\treturn val.value;\n \t}\n \n@@ -91,7 +96,8 @@ struct TimestampConvertNano {\n \n struct DateConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic int64_t ConvertValue(date_t val) {\n+\tstatic int64_t ConvertValue(date_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\treturn Date::EpochMicroseconds(val);\n \t}\n \n@@ -104,7 +110,8 @@ struct DateConvert {\n \n struct IntervalConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic int64_t ConvertValue(interval_t val) {\n+\tstatic int64_t ConvertValue(interval_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\treturn Interval::GetNanoseconds(val);\n \t}\n \n@@ -117,9 +124,13 @@ struct IntervalConvert {\n \n struct TimeConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic PyObject *ConvertValue(dtime_t val) {\n-\t\tauto str = duckdb::Time::ToString(val);\n-\t\treturn PyUnicode_FromStringAndSize(str.c_str(), str.size());\n+\tstatic PyObject *ConvertValue(dtime_t val, NumpyAppendData &append_data) {\n+\t\tauto &client_properties = append_data.client_properties;\n+\t\tauto value = Value::TIME(val);\n+\t\tauto py_obj = PythonObject::FromValue(value, LogicalType::TIME, client_properties);\n+\t\t// Release ownership of the PyObject* without decreasing refcount\n+\t\t// this returns a handle, of which we take the ptr to get the PyObject*\n+\t\treturn py_obj.release().ptr();\n \t}\n \n \ttemplate <class NUMPY_T, bool PANDAS>\n@@ -201,7 +212,8 @@ struct StringConvert {\n \t}\n \n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic PyObject *ConvertValue(string_t val) {\n+\tstatic PyObject *ConvertValue(string_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\t// we could use PyUnicode_FromStringAndSize here, but it does a lot of verification that we don't need\n \t\t// because of that it is a lot slower than it needs to be\n \t\tauto data = const_data_ptr_cast(val.GetData());\n@@ -234,7 +246,8 @@ struct StringConvert {\n \n struct BlobConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic PyObject *ConvertValue(string_t val) {\n+\tstatic PyObject *ConvertValue(string_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\treturn PyByteArray_FromStringAndSize(val.GetData(), val.GetSize());\n \t}\n \n@@ -247,7 +260,8 @@ struct BlobConvert {\n \n struct BitConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic PyObject *ConvertValue(string_t val) {\n+\tstatic PyObject *ConvertValue(string_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\treturn PyBytes_FromStringAndSize(val.GetData(), val.GetSize());\n \t}\n \n@@ -260,7 +274,8 @@ struct BitConvert {\n \n struct UUIDConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic PyObject *ConvertValue(hugeint_t val) {\n+\tstatic PyObject *ConvertValue(hugeint_t val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n \t\tpy::handle h = import_cache.uuid.UUID()(UUID::ToString(val)).release();\n \t\treturn h.ptr();\n@@ -380,7 +395,8 @@ struct MapConvert {\n \n struct IntegralConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T>\n-\tstatic NUMPY_T ConvertValue(DUCKDB_T val) {\n+\tstatic NUMPY_T ConvertValue(DUCKDB_T val, NumpyAppendData &append_data) {\n+\t\t(void)append_data;\n \t\treturn NUMPY_T(val);\n \t}\n \n@@ -392,14 +408,16 @@ struct IntegralConvert {\n };\n \n template <>\n-double IntegralConvert::ConvertValue(hugeint_t val) {\n+double IntegralConvert::ConvertValue(hugeint_t val, NumpyAppendData &append_data) {\n+\t(void)append_data;\n \tdouble result;\n \tHugeint::TryCast(val, result);\n \treturn result;\n }\n \n template <>\n-double IntegralConvert::ConvertValue(uhugeint_t val) {\n+double IntegralConvert::ConvertValue(uhugeint_t val, NumpyAppendData &append_data) {\n+\t(void)append_data;\n \tdouble result;\n \tUhugeint::TryCast(val, result);\n \treturn result;\n@@ -407,8 +425,8 @@ double IntegralConvert::ConvertValue(uhugeint_t val) {\n \n } // namespace duckdb_py_convert\n \n-template <class DUCKDB_T, class NUMPY_T, class CONVERT>\n-static bool ConvertColumn(NumpyAppendData &append_data) {\n+template <class DUCKDB_T, class NUMPY_T, class CONVERT, bool HAS_NULLS, bool PANDAS>\n+static bool ConvertColumnTemplated(NumpyAppendData &append_data) {\n \tauto target_offset = append_data.target_offset;\n \tauto target_data = append_data.target_data;\n \tauto target_mask = append_data.target_mask;\n@@ -416,34 +434,46 @@ static bool ConvertColumn(NumpyAppendData &append_data) {\n \tauto count = append_data.count;\n \tauto source_offset = append_data.source_offset;\n \n+\tauto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);\n+\tauto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);\n+\tbool mask_is_set = false;\n+\tfor (idx_t i = 0; i < count; i++) {\n+\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n+\t\tidx_t offset = target_offset + i;\n+\t\tif (HAS_NULLS && !idata.validity.RowIsValidUnsafe(src_idx)) {\n+\t\t\tout_ptr[offset] = CONVERT::template NullValue<NUMPY_T, PANDAS>(target_mask[offset]);\n+\t\t\tmask_is_set = mask_is_set || target_mask[offset];\n+\t\t} else {\n+\t\t\tout_ptr[offset] = CONVERT::template ConvertValue<DUCKDB_T, NUMPY_T>(src_ptr[src_idx], append_data);\n+\t\t\ttarget_mask[offset] = false;\n+\t\t}\n+\t}\n+\treturn mask_is_set;\n+}\n+\n+template <class DUCKDB_T, class NUMPY_T, class CONVERT>\n+static bool ConvertColumn(NumpyAppendData &append_data) {\n+\tauto target_offset = append_data.target_offset;\n+\tauto target_data = append_data.target_data;\n+\tauto &idata = append_data.idata;\n+\n \tauto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);\n \tauto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);\n \tif (!idata.validity.AllValid()) {\n-\t\tbool mask_is_set = false;\n-\t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n-\t\t\tidx_t offset = target_offset + i;\n-\t\t\tif (!idata.validity.RowIsValidUnsafe(src_idx)) {\n-\t\t\t\tif (append_data.pandas) {\n-\t\t\t\t\tout_ptr[offset] = CONVERT::template NullValue<NUMPY_T, true>(target_mask[offset]);\n-\t\t\t\t} else {\n-\t\t\t\t\tout_ptr[offset] = CONVERT::template NullValue<NUMPY_T, false>(target_mask[offset]);\n-\t\t\t\t}\n-\t\t\t\tmask_is_set = mask_is_set || target_mask[offset];\n-\t\t\t} else {\n-\t\t\t\tout_ptr[offset] = CONVERT::template ConvertValue<DUCKDB_T, NUMPY_T>(src_ptr[src_idx]);\n-\t\t\t\ttarget_mask[offset] = false;\n-\t\t\t}\n+\t\tif (append_data.pandas) {\n+\t\t\treturn ConvertColumnTemplated<DUCKDB_T, NUMPY_T, CONVERT, /*has_nulls=*/true, /*pandas=*/true>(append_data);\n+\t\t} else {\n+\t\t\treturn ConvertColumnTemplated<DUCKDB_T, NUMPY_T, CONVERT, /*has_nulls=*/true, /*pandas=*/false>(\n+\t\t\t    append_data);\n \t\t}\n-\t\treturn mask_is_set;\n \t} else {\n-\t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n-\t\t\tidx_t offset = target_offset + i;\n-\t\t\tout_ptr[offset] = CONVERT::template ConvertValue<DUCKDB_T, NUMPY_T>(src_ptr[src_idx]);\n-\t\t\ttarget_mask[offset] = false;\n+\t\tif (append_data.pandas) {\n+\t\t\treturn ConvertColumnTemplated<DUCKDB_T, NUMPY_T, CONVERT, /*has_nulls=*/false, /*pandas=*/true>(\n+\t\t\t    append_data);\n+\t\t} else {\n+\t\t\treturn ConvertColumnTemplated<DUCKDB_T, NUMPY_T, CONVERT, /*has_nulls=*/false, /*pandas=*/false>(\n+\t\t\t    append_data);\n \t\t}\n-\t\treturn false;\n \t}\n }\n \n@@ -464,16 +494,16 @@ static bool ConvertColumnCategoricalTemplate(NumpyAppendData &append_data) {\n \t\t\tif (!idata.validity.RowIsValidUnsafe(src_idx)) {\n \t\t\t\tout_ptr[offset] = static_cast<NUMPY_T>(-1);\n \t\t\t} else {\n-\t\t\t\tout_ptr[offset] =\n-\t\t\t\t    duckdb_py_convert::RegularConvert::template ConvertValue<DUCKDB_T, NUMPY_T>(src_ptr[src_idx]);\n+\t\t\t\tout_ptr[offset] = duckdb_py_convert::RegularConvert::template ConvertValue<DUCKDB_T, NUMPY_T>(\n+\t\t\t\t    src_ptr[src_idx], append_data);\n \t\t\t}\n \t\t}\n \t} else {\n \t\tfor (idx_t i = 0; i < count; i++) {\n \t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n \t\t\tidx_t offset = target_offset + i;\n-\t\t\tout_ptr[offset] =\n-\t\t\t    duckdb_py_convert::RegularConvert::template ConvertValue<DUCKDB_T, NUMPY_T>(src_ptr[src_idx]);\n+\t\t\tout_ptr[offset] = duckdb_py_convert::RegularConvert::template ConvertValue<DUCKDB_T, NUMPY_T>(\n+\t\t\t    src_ptr[src_idx], append_data);\n \t\t}\n \t}\n \t// Null values are encoded in the data itself\n@@ -560,7 +590,8 @@ static bool ConvertDecimalInternal(NumpyAppendData &append_data, double division\n \t\t\t\ttarget_mask[offset] = true;\n \t\t\t} else {\n \t\t\t\tout_ptr[offset] =\n-\t\t\t\t    duckdb_py_convert::IntegralConvert::ConvertValue<DUCKDB_T, double>(src_ptr[src_idx]) / division;\n+\t\t\t\t    duckdb_py_convert::IntegralConvert::ConvertValue<DUCKDB_T, double>(src_ptr[src_idx], append_data) /\n+\t\t\t\t    division;\n \t\t\t\ttarget_mask[offset] = false;\n \t\t\t}\n \t\t}\n@@ -570,7 +601,8 @@ static bool ConvertDecimalInternal(NumpyAppendData &append_data, double division\n \t\t\tidx_t src_idx = idata.sel->get_index(i + source_offset);\n \t\t\tidx_t offset = target_offset + i;\n \t\t\tout_ptr[offset] =\n-\t\t\t    duckdb_py_convert::IntegralConvert::ConvertValue<DUCKDB_T, double>(src_ptr[src_idx]) / division;\n+\t\t\t    duckdb_py_convert::IntegralConvert::ConvertValue<DUCKDB_T, double>(src_ptr[src_idx], append_data) /\n+\t\t\t    division;\n \t\t\ttarget_mask[offset] = false;\n \t\t}\n \t\treturn false;\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/api/test_dbapi13.py b/tools/pythonpkg/tests/fast/api/test_dbapi13.py\nindex 3271c59b5f6f..fb7fbaa8afc4 100644\n--- a/tools/pythonpkg/tests/fast/api/test_dbapi13.py\n+++ b/tools/pythonpkg/tests/fast/api/test_dbapi13.py\n@@ -6,17 +6,17 @@\n \n \n class TestNumpyTime(object):\n-    def test_fetchall_date(self, duckdb_cursor):\n+    def test_fetchall_time(self, duckdb_cursor):\n         res = duckdb_cursor.execute(\"SELECT TIME '13:06:40' as test_time\").fetchall()\n         assert res == [(datetime.time(13, 6, 40),)]\n \n-    def test_fetchnumpy_date(self, duckdb_cursor):\n+    def test_fetchnumpy_time(self, duckdb_cursor):\n         res = duckdb_cursor.execute(\"SELECT TIME '13:06:40' as test_time\").fetchnumpy()\n-        arr = numpy.array(['13:06:40'], dtype=\"object\")\n+        arr = numpy.array([datetime.time(13, 6, 40)], dtype=\"object\")\n         arr = numpy.ma.masked_array(arr)\n         numpy.testing.assert_array_equal(res['test_time'], arr)\n \n-    def test_fetchdf_date(self, duckdb_cursor):\n+    def test_fetchdf_time(self, duckdb_cursor):\n         res = duckdb_cursor.execute(\"SELECT TIME '13:06:40' as test_time\").fetchdf()\n-        ser = pandas.Series(numpy.array(['13:06:40'], dtype=\"object\"), name=\"test_time\")\n+        ser = pandas.Series(numpy.array([datetime.time(13, 6, 40)], dtype=\"object\"), name=\"test_time\")\n         pandas.testing.assert_series_equal(res['test_time'], ser)\n",
  "problem_statement": "TIME Value result -> Pandas Dataframe does not return datetime.time\n#### What happens?  \r\n\r\nWhen the result is of type TIME, exporting this result to a dataframe converts the value to a string, instead of an object of type `datetime.time`  \r\n```py\r\ntime = duckdb.query(\"SELECT make_time(23, 1, 34.234345) AS '0'\").df()\r\n----\r\n'23:01:34.234345'\r\n```\r\nWhereas exporting it as a tuple through `fetchone()` does create the expected result:\r\n```py\r\ntime = duckdb.query(\"SELECT make_time(23, 1, 34.234345) AS '0'\").fetchone()\r\n----\r\n(datetime.time(23, 1, 34, 234345),)\r\n```\r\n#### Environment (please complete the following information):\r\n - OS: (e.g. iOS)\r\n - DuckDB Version: [master:latest]\r\n - DuckDB Client: [Python]\r\n\r\n#### Identity Disclosure:\r\n - Full Name: [Thijs Bruineman]\r\n - Affiliation: [DuckDBLabs]\r\n\r\n#### Before Submitting\r\n\r\n- [x] **Have you tried this on the latest `master` branch?**\r\n* **Python**: `pip install duckdb --upgrade --pre`\r\n* **R**: `install.packages(\"https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz\", repos = NULL)`\r\n* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.\r\n\r\n- [x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\n",
  "hints_text": "Re-upping this with respect to dates out of bounds for pandas datetimens[64] format (i.e. https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-oob)\r\n\r\nShortly, one gets different output from queries depending on the output method, .fetchall() or .fetchdf()/df(). \r\nWith out of bounds values, e.g. '9999-12-31', .fetchdf()/df() return wrong values due to datetime conversion.\r\n\r\nTested on \r\nWindows\r\nduckdb-0.6.2.dev1934\r\npython client\r\n\r\n```\r\n>>> import duckdb\r\n>>> c = duckdb.connect(':memory:')\r\n>>> c.execute(\"create table test (date DATE)\")\r\n<duckdb.DuckDBPyConnection object at 0x000002293E4816B0>\r\n>>> c.execute(\"INSERT INTO TEST VALUES ('9999-12-31')\")\r\n<duckdb.DuckDBPyConnection object at 0x000002293E4816B0>\r\n\r\n>>> c.execute(\"select * from test\").fetchall()\r\n[(datetime.date(9999, 12, 31),)]\r\n\r\n>>> c.execute(\"select * from test\").df(date_as_object=True)\r\n         date\r\n0  1816-03-29\r\n\r\n>>> c.execute(\"select * from test\").df()\r\n                           date\r\n0 1816-03-29 05:56:08.066277376\r\n```\r\n\r\n\nI'm looking into this now, but I'm not sure what the best action to take is\r\n\r\nWe use the same logic for both NumPy and Pandas here currently.\r\nPandas has a way to deal with this with the Period type, though that would likely be an `object`, which requires sampling the column before conversion to detect which physical type in the array to use.\r\n\r\nBut Numpy has no way of dealing with this, so maybe we just want to throw here in the case of `fetchnumpy`\r\n\r\n\nEither throwing or returning a correct datetime seems fine to me. Supporting these date ranges is likely not particularly important but we should at the very least catch the overflow.\nHi both, \r\n\r\nI understand that at least the error is catched atm, but I'd still like to raise the issue of how to work on these date ranges - which are not uncommon in e.g. credit databases to signal annuities/credit in perpetuity.\r\n\r\nAs the issue now returns a `ConversionError`, it means even exploring tables with these date ranges is impossible through the python client, unless one works around it using `fetchall`.\r\n\r\nIs there no plan to support these date ranges, e.g. through pandas `Period` type?\r\n\r\nThanks\r\n\r\n \nIs this actually fixed? I still get `str` as the column type for time:\r\n\r\n```python\r\ntime = duckdb.query(\"SELECT make_time(23, 1, 34.234345) AS '0'\").df()\r\nprint(type(time.get('0')[0]))\r\n\r\n# outputs: <class 'str'>\r\n```\r\n\r\ntested on v0.10.1\n@suchanlee thanks for letting us know, we'll take a look.\nThanks for the quick response @szarnyasg ",
  "created_at": "2024-04-02T13:02:17Z"
}