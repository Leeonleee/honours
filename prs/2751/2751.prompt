You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Python enable_external_access config flag does not restrict csv/parquet access
#### What happens?
Hi Folks!

One of my coworkers was testing out the enable_external_access config parameter that you added for us (thank you!). Unfortunately, it does not work like we had expected. It does correctly restrict access to DataFrames in the global scope, but it does not restrict file system access for csv or parquet files. We are completely open to a separate flag that would restrict file system access, or for that capability to be added to this flag - whichever is easier! We have other protections in place to isolate users and prevent them from accessing things we want to protect so this is not urgent or gating a deployment, but this would be a really great security feature for us.

Related prior issue: #1773

Thanks!
-Alex

#### To Reproduce
```python
import duckdb
import pandas as pd

dftest = pd.DataFrame([{"This":1,"is":2,"a":3,"test":4},{"This":5,"is":6,"a":7,"test":8}])
dftest.to_csv('test.csv')
file_creation_conn = duckdb.connect(':memory:', config={'enable_external_access': True})
file_creation_conn.execute("COPY (select * from dftest) to 'test.parquet' WITH (FORMAT PARQUET)").fetchdf()
file_creation_conn.close()

# The goal is to disable dataframe as well as filesystem access for this connection
conn = duckdb.connect(':memory:', config={'enable_external_access': False})

#The following statements succeed when they should fail:
df = conn.execute("select * from parquet_scan('test.parquet')").fetchdf()
print('parquet_scan results:\n',df)
df = conn.execute('select * from "test.parquet"').fetchdf()
print('test.parquet results:\n',df)
df = conn.execute("select * from 'test.csv'").fetchdf()
print('test.csv results:\n',df)
df = conn.execute("select * from read_csv_auto('test.csv')").fetchdf()
print('read_csv_auto results:\n',df)

#This statement correctly fails 
df = conn.execute("select * from dftest").fetchdf()
print('select * from dftest results:\n',df)
```
#### Environment (please complete the following information):
 - OS: Windows
 - DuckDB Version: 0.3.1 and 0.3.2.dev521
 - DuckDB Client: Python

#### Before Submitting

- [X] **Have you tried this on the latest `master` branch?**

- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**


Python enable_external_access config flag does not restrict csv/parquet access
#### What happens?
Hi Folks!

One of my coworkers was testing out the enable_external_access config parameter that you added for us (thank you!). Unfortunately, it does not work like we had expected. It does correctly restrict access to DataFrames in the global scope, but it does not restrict file system access for csv or parquet files. We are completely open to a separate flag that would restrict file system access, or for that capability to be added to this flag - whichever is easier! We have other protections in place to isolate users and prevent them from accessing things we want to protect so this is not urgent or gating a deployment, but this would be a really great security feature for us.

Related prior issue: #1773

Thanks!
-Alex

#### To Reproduce
```python
import duckdb
import pandas as pd

dftest = pd.DataFrame([{"This":1,"is":2,"a":3,"test":4},{"This":5,"is":6,"a":7,"test":8}])
dftest.to_csv('test.csv')
file_creation_conn = duckdb.connect(':memory:', config={'enable_external_access': True})
file_creation_conn.execute("COPY (select * from dftest) to 'test.parquet' WITH (FORMAT PARQUET)").fetchdf()
file_creation_conn.close()

# The goal is to disable dataframe as well as filesystem access for this connection
conn = duckdb.connect(':memory:', config={'enable_external_access': False})

#The following statements succeed when they should fail:
df = conn.execute("select * from parquet_scan('test.parquet')").fetchdf()
print('parquet_scan results:\n',df)
df = conn.execute('select * from "test.parquet"').fetchdf()
print('test.parquet results:\n',df)
df = conn.execute("select * from 'test.csv'").fetchdf()
print('test.csv results:\n',df)
df = conn.execute("select * from read_csv_auto('test.csv')").fetchdf()
print('read_csv_auto results:\n',df)

#This statement correctly fails 
df = conn.execute("select * from dftest").fetchdf()
print('select * from dftest results:\n',df)
```
#### Environment (please complete the following information):
 - OS: Windows
 - DuckDB Version: 0.3.1 and 0.3.2.dev521
 - DuckDB Client: Python

#### Before Submitting

- [X] **Have you tried this on the latest `master` branch?**

- [X] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**



</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of extension/parquet/parquet-extension.cpp]
1: #include <string>
2: #include <vector>
3: #include <fstream>
4: #include <iostream>
5: 
6: #include "parquet-extension.hpp"
7: #include "parquet_reader.hpp"
8: #include "parquet_writer.hpp"
9: #include "parquet_metadata.hpp"
10: #include "zstd_file_system.hpp"
11: 
12: #include "duckdb.hpp"
13: #ifndef DUCKDB_AMALGAMATION
14: #include "duckdb/common/file_system.hpp"
15: #include "duckdb/common/types/chunk_collection.hpp"
16: #include "duckdb/function/copy_function.hpp"
17: #include "duckdb/function/table_function.hpp"
18: #include "duckdb/common/file_system.hpp"
19: #include "duckdb/parallel/parallel_state.hpp"
20: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
21: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
22: 
23: #include "duckdb/common/enums/file_compression_type.hpp"
24: #include "duckdb/main/config.hpp"
25: #include "duckdb/parser/expression/constant_expression.hpp"
26: #include "duckdb/parser/expression/function_expression.hpp"
27: #include "duckdb/parser/tableref/table_function_ref.hpp"
28: 
29: #include "duckdb/storage/statistics/base_statistics.hpp"
30: 
31: #include "duckdb/main/client_context.hpp"
32: #include "duckdb/catalog/catalog.hpp"
33: #endif
34: 
35: namespace duckdb {
36: 
37: struct ParquetReadBindData : public FunctionData {
38: 	shared_ptr<ParquetReader> initial_reader;
39: 	vector<string> files;
40: 	vector<column_t> column_ids;
41: 	atomic<idx_t> chunk_count;
42: 	atomic<idx_t> cur_file;
43: };
44: 
45: struct ParquetReadOperatorData : public FunctionOperatorData {
46: 	shared_ptr<ParquetReader> reader;
47: 	ParquetReaderScanState scan_state;
48: 	bool is_parallel;
49: 	idx_t file_index;
50: 	vector<column_t> column_ids;
51: 	TableFilterSet *table_filters;
52: };
53: 
54: struct ParquetReadParallelState : public ParallelState {
55: 	mutex lock;
56: 	shared_ptr<ParquetReader> current_reader;
57: 	idx_t file_index;
58: 	idx_t row_group_index;
59: };
60: 
61: class ParquetScanFunction {
62: public:
63: 	static TableFunctionSet GetFunctionSet() {
64: 		TableFunctionSet set("parquet_scan");
65: 		auto table_function =
66: 		    TableFunction({LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind, ParquetScanInit,
67: 		                  /* statistics */ ParquetScanStats, /* cleanup */ nullptr,
68: 		                  /* dependency */ nullptr, ParquetCardinality,
69: 		                  /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, ParquetScanMaxThreads,
70: 		                  ParquetInitParallelState, ParquetScanFuncParallel, ParquetScanParallelInit,
71: 		                  ParquetParallelStateNext, true, true, ParquetProgress);
72: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
73: 		set.AddFunction(table_function);
74: 		table_function = TableFunction({LogicalType::LIST(LogicalType::VARCHAR)}, ParquetScanImplementation,
75: 		                               ParquetScanBindList, ParquetScanInit, /* statistics */ ParquetScanStats,
76: 		                               /* cleanup */ nullptr,
77: 		                               /* dependency */ nullptr, ParquetCardinality,
78: 		                               /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr,
79: 		                               ParquetScanMaxThreads, ParquetInitParallelState, ParquetScanFuncParallel,
80: 		                               ParquetScanParallelInit, ParquetParallelStateNext, true, true, ParquetProgress);
81: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
82: 		set.AddFunction(table_function);
83: 		return set;
84: 	}
85: 
86: 	static unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,
87: 	                                                vector<string> &expected_names,
88: 	                                                vector<LogicalType> &expected_types) {
89: 		for (auto &option : info.options) {
90: 			auto loption = StringUtil::Lower(option.first);
91: 			if (loption == "compression" || loption == "codec") {
92: 				// CODEC option has no effect on parquet read: we determine codec from the file
93: 				continue;
94: 			} else {
95: 				throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
96: 			}
97: 		}
98: 		auto result = make_unique<ParquetReadBindData>();
99: 
100: 		FileSystem &fs = FileSystem::GetFileSystem(context);
101: 		result->files = fs.Glob(info.file_path);
102: 		if (result->files.empty()) {
103: 			throw IOException("No files found that match the pattern \"%s\"", info.file_path);
104: 		}
105: 		ParquetOptions parquet_options(context);
106: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types, parquet_options);
107: 		return move(result);
108: 	}
109: 
110: 	static unique_ptr<BaseStatistics> ParquetScanStats(ClientContext &context, const FunctionData *bind_data_p,
111: 	                                                   column_t column_index) {
112: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
113: 
114: 		if (column_index == COLUMN_IDENTIFIER_ROW_ID) {
115: 			return nullptr;
116: 		}
117: 
118: 		// we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
119: 
120: 		// We already parsed the metadata for the first file in a glob because we need some type info.
121: 		auto overall_stats = ParquetReader::ReadStatistics(
122: 		    *bind_data.initial_reader, bind_data.initial_reader->return_types[column_index], column_index,
123: 		    bind_data.initial_reader->metadata->metadata.get());
124: 
125: 		if (!overall_stats) {
126: 			return nullptr;
127: 		}
128: 
129: 		// if there is only one file in the glob (quite common case), we are done
130: 		auto &config = DBConfig::GetConfig(context);
131: 		if (bind_data.files.size() < 2) {
132: 			return overall_stats;
133: 		} else if (config.object_cache_enable) {
134: 			auto &cache = ObjectCache::GetObjectCache(context);
135: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
136: 			// enabled at all)
137: 			FileSystem &fs = FileSystem::GetFileSystem(context);
138: 			for (idx_t file_idx = 1; file_idx < bind_data.files.size(); file_idx++) {
139: 				auto &file_name = bind_data.files[file_idx];
140: 				auto metadata = std::dynamic_pointer_cast<ParquetFileMetadataCache>(cache.Get(file_name));
141: 				auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
142: 				                          FileSystem::DEFAULT_COMPRESSION, FileSystem::GetFileOpener(context));
143: 				// but we need to check if the metadata cache entries are current
144: 				if (!metadata || (fs.GetLastModifiedTime(*handle) >= metadata->read_time)) {
145: 					// missing or invalid metadata entry in cache, no usable stats overall
146: 					return nullptr;
147: 				}
148: 				// get and merge stats for file
149: 				auto file_stats = ParquetReader::ReadStatistics(*bind_data.initial_reader,
150: 				                                                bind_data.initial_reader->return_types[column_index],
151: 				                                                column_index, metadata->metadata.get());
152: 				if (!file_stats) {
153: 					return nullptr;
154: 				}
155: 				overall_stats->Merge(*file_stats);
156: 			}
157: 			// success!
158: 			return overall_stats;
159: 		}
160: 		// we have more than one file and no object cache so no statistics overall
161: 		return nullptr;
162: 	}
163: 
164: 	static void ParquetScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
165: 	                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
166: 	                                    ParallelState *parallel_state_p) {
167: 		//! FIXME: Have specialized parallel function from pandas scan here
168: 		ParquetScanImplementation(context, bind_data, operator_state, input, output);
169: 	}
170: 
171: 	static unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context, vector<string> files,
172: 	                                                        vector<LogicalType> &return_types, vector<string> &names,
173: 	                                                        ParquetOptions parquet_options) {
174: 		auto result = make_unique<ParquetReadBindData>();
175: 		result->files = move(files);
176: 
177: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], parquet_options);
178: 		return_types = result->initial_reader->return_types;
179: 
180: 		names = result->initial_reader->names;
181: 		return move(result);
182: 	}
183: 
184: 	static vector<string> ParquetGlob(FileSystem &fs, const string &glob) {
185: 		auto files = fs.Glob(glob);
186: 		if (files.empty()) {
187: 			throw IOException("No files found that match the pattern \"%s\"", glob);
188: 		}
189: 		return files;
190: 	}
191: 
192: 	static unique_ptr<FunctionData> ParquetScanBind(ClientContext &context, vector<Value> &inputs,
193: 	                                                unordered_map<string, Value> &named_parameters,
194: 	                                                vector<LogicalType> &input_table_types,
195: 	                                                vector<string> &input_table_names,
196: 	                                                vector<LogicalType> &return_types, vector<string> &names) {
197: 		auto file_name = inputs[0].GetValue<string>();
198: 		ParquetOptions parquet_options(context);
199: 		for (auto &kv : named_parameters) {
200: 			if (kv.first == "binary_as_string") {
201: 				parquet_options.binary_as_string = kv.second.value_.boolean;
202: 			}
203: 		}
204: 		FileSystem &fs = FileSystem::GetFileSystem(context);
205: 		auto files = ParquetGlob(fs, file_name);
206: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
207: 	}
208: 
209: 	static unique_ptr<FunctionData> ParquetScanBindList(ClientContext &context, vector<Value> &inputs,
210: 	                                                    unordered_map<string, Value> &named_parameters,
211: 	                                                    vector<LogicalType> &input_table_types,
212: 	                                                    vector<string> &input_table_names,
213: 	                                                    vector<LogicalType> &return_types, vector<string> &names) {
214: 		FileSystem &fs = FileSystem::GetFileSystem(context);
215: 		vector<string> files;
216: 		for (auto &val : inputs[0].list_value) {
217: 			auto glob_files = ParquetGlob(fs, val.ToString());
218: 			files.insert(files.end(), glob_files.begin(), glob_files.end());
219: 		}
220: 		if (files.empty()) {
221: 			throw IOException("Parquet reader needs at least one file to read");
222: 		}
223: 		ParquetOptions parquet_options(context);
224: 		for (auto &kv : named_parameters) {
225: 			if (kv.first == "binary_as_string") {
226: 				parquet_options.binary_as_string = kv.second.value_.boolean;
227: 			}
228: 		}
229: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
230: 	}
231: 
232: 	static unique_ptr<FunctionOperatorData> ParquetScanInit(ClientContext &context, const FunctionData *bind_data_p,
233: 	                                                        const vector<column_t> &column_ids,
234: 	                                                        TableFilterCollection *filters) {
235: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
236: 		bind_data.chunk_count = 0;
237: 		bind_data.cur_file = 0;
238: 		auto result = make_unique<ParquetReadOperatorData>();
239: 		result->column_ids = column_ids;
240: 
241: 		result->is_parallel = false;
242: 		result->file_index = 0;
243: 		result->table_filters = filters->table_filters;
244: 		// single-threaded: one thread has to read all groups
245: 		vector<idx_t> group_ids;
246: 		for (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {
247: 			group_ids.push_back(i);
248: 		}
249: 		result->reader = bind_data.initial_reader;
250: 		result->reader->InitializeScan(result->scan_state, column_ids, move(group_ids), filters->table_filters);
251: 		return move(result);
252: 	}
253: 
254: 	static int ParquetProgress(ClientContext &context, const FunctionData *bind_data_p) {
255: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
256: 		if (bind_data.initial_reader->NumRows() == 0) {
257: 			return (100 * (bind_data.cur_file + 1)) / bind_data.files.size();
258: 		}
259: 		auto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100 / bind_data.initial_reader->NumRows()) /
260: 		                  bind_data.files.size();
261: 		percentage += 100 * bind_data.cur_file / bind_data.files.size();
262: 		return percentage;
263: 	}
264: 
265: 	static unique_ptr<FunctionOperatorData>
266: 	ParquetScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *parallel_state_p,
267: 	                        const vector<column_t> &column_ids, TableFilterCollection *filters) {
268: 		auto result = make_unique<ParquetReadOperatorData>();
269: 		result->column_ids = column_ids;
270: 		result->is_parallel = true;
271: 		result->table_filters = filters->table_filters;
272: 		if (!ParquetParallelStateNext(context, bind_data_p, result.get(), parallel_state_p)) {
273: 			return nullptr;
274: 		}
275: 		return move(result);
276: 	}
277: 
278: 	static void ParquetScanImplementation(ClientContext &context, const FunctionData *bind_data_p,
279: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
280: 		if (!operator_state) {
281: 			return;
282: 		}
283: 		auto &data = (ParquetReadOperatorData &)*operator_state;
284: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
285: 
286: 		do {
287: 			data.reader->Scan(data.scan_state, output);
288: 			bind_data.chunk_count++;
289: 			if (output.size() == 0 && !data.is_parallel) {
290: 				auto &bind_data = (ParquetReadBindData &)*bind_data_p;
291: 				// check if there is another file
292: 				if (data.file_index + 1 < bind_data.files.size()) {
293: 					data.file_index++;
294: 					bind_data.cur_file++;
295: 					bind_data.chunk_count = 0;
296: 					string file = bind_data.files[data.file_index];
297: 					// move to the next file
298: 					data.reader = make_shared<ParquetReader>(context, file, data.reader->return_types,
299: 					                                         data.reader->parquet_options, bind_data.files[0]);
300: 					vector<idx_t> group_ids;
301: 					for (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {
302: 						group_ids.push_back(i);
303: 					}
304: 					data.reader->InitializeScan(data.scan_state, data.column_ids, move(group_ids), data.table_filters);
305: 				} else {
306: 					// exhausted all the files: done
307: 					break;
308: 				}
309: 			} else {
310: 				break;
311: 			}
312: 		} while (true);
313: 	}
314: 
315: 	static unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {
316: 		auto &data = (ParquetReadBindData &)*bind_data;
317: 		return make_unique<NodeStatistics>(data.initial_reader->NumRows() * data.files.size());
318: 	}
319: 
320: 	static idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {
321: 		auto &data = (ParquetReadBindData &)*bind_data;
322: 		return data.initial_reader->NumRowGroups() * data.files.size();
323: 	}
324: 
325: 	static unique_ptr<ParallelState> ParquetInitParallelState(ClientContext &context, const FunctionData *bind_data_p,
326: 	                                                          const vector<column_t> &column_ids,
327: 	                                                          TableFilterCollection *filters) {
328: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
329: 		auto result = make_unique<ParquetReadParallelState>();
330: 		result->current_reader = bind_data.initial_reader;
331: 		result->row_group_index = 0;
332: 		result->file_index = 0;
333: 		return move(result);
334: 	}
335: 
336: 	static bool ParquetParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
337: 	                                     FunctionOperatorData *state_p, ParallelState *parallel_state_p) {
338: 		if (!state_p) {
339: 			return false;
340: 		}
341: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
342: 		auto &parallel_state = (ParquetReadParallelState &)*parallel_state_p;
343: 		auto &scan_data = (ParquetReadOperatorData &)*state_p;
344: 
345: 		lock_guard<mutex> parallel_lock(parallel_state.lock);
346: 		if (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {
347: 			// groups remain in the current parquet file: read the next group
348: 			scan_data.reader = parallel_state.current_reader;
349: 			vector<idx_t> group_indexes {parallel_state.row_group_index};
350: 			scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
351: 			                                 scan_data.table_filters);
352: 			parallel_state.row_group_index++;
353: 			return true;
354: 		} else {
355: 			// no groups remain in the current parquet file: check if there are more files to read
356: 			while (parallel_state.file_index + 1 < bind_data.files.size()) {
357: 				// read the next file
358: 				string file = bind_data.files[++parallel_state.file_index];
359: 				parallel_state.current_reader =
360: 				    make_shared<ParquetReader>(context, file, parallel_state.current_reader->return_types,
361: 				                               parallel_state.current_reader->parquet_options);
362: 				if (parallel_state.current_reader->NumRowGroups() == 0) {
363: 					// empty parquet file, move to next file
364: 					continue;
365: 				}
366: 				// set up the scan state to read the first group
367: 				scan_data.reader = parallel_state.current_reader;
368: 				vector<idx_t> group_indexes {0};
369: 				scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
370: 				                                 scan_data.table_filters);
371: 				parallel_state.row_group_index = 1;
372: 				return true;
373: 			}
374: 		}
375: 		return false;
376: 	}
377: };
378: 
379: struct ParquetWriteBindData : public FunctionData {
380: 	vector<LogicalType> sql_types;
381: 	string file_name;
382: 	vector<string> column_names;
383: 	duckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
384: };
385: 
386: struct ParquetWriteGlobalState : public GlobalFunctionData {
387: 	unique_ptr<ParquetWriter> writer;
388: };
389: 
390: struct ParquetWriteLocalState : public LocalFunctionData {
391: 	ParquetWriteLocalState() {
392: 		buffer = make_unique<ChunkCollection>();
393: 	}
394: 
395: 	unique_ptr<ChunkCollection> buffer;
396: };
397: 
398: unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info, vector<string> &names,
399:                                           vector<LogicalType> &sql_types) {
400: 	auto bind_data = make_unique<ParquetWriteBindData>();
401: 	for (auto &option : info.options) {
402: 		auto loption = StringUtil::Lower(option.first);
403: 		if (loption == "compression" || loption == "codec") {
404: 			if (!option.second.empty()) {
405: 				auto roption = StringUtil::Lower(option.second[0].ToString());
406: 				if (roption == "uncompressed") {
407: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::UNCOMPRESSED;
408: 					continue;
409: 				} else if (roption == "snappy") {
410: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
411: 					continue;
412: 				} else if (roption == "gzip") {
413: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::GZIP;
414: 					continue;
415: 				} else if (roption == "zstd") {
416: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::ZSTD;
417: 					continue;
418: 				}
419: 			}
420: 			throw ParserException("Expected %s argument to be either [uncompressed, snappy, gzip or zstd]", loption);
421: 		} else {
422: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
423: 		}
424: 	}
425: 	bind_data->sql_types = sql_types;
426: 	bind_data->column_names = names;
427: 	bind_data->file_name = info.file_path;
428: 	return move(bind_data);
429: }
430: 
431: unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &context, FunctionData &bind_data) {
432: 	auto global_state = make_unique<ParquetWriteGlobalState>();
433: 	auto &parquet_bind = (ParquetWriteBindData &)bind_data;
434: 
435: 	auto &fs = FileSystem::GetFileSystem(context);
436: 	global_state->writer =
437: 	    make_unique<ParquetWriter>(fs, parquet_bind.file_name, FileSystem::GetFileOpener(context),
438: 	                               parquet_bind.sql_types, parquet_bind.column_names, parquet_bind.codec);
439: 	return move(global_state);
440: }
441: 
442: void ParquetWriteSink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
443:                       LocalFunctionData &lstate, DataChunk &input) {
444: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
445: 	auto &local_state = (ParquetWriteLocalState &)lstate;
446: 
447: 	// append data to the local (buffered) chunk collection
448: 	local_state.buffer->Append(input);
449: 	if (local_state.buffer->Count() > 100000) {
450: 		// if the chunk collection exceeds a certain size we flush it to the parquet file
451: 		global_state.writer->Flush(*local_state.buffer);
452: 		// and reset the buffer
453: 		local_state.buffer = make_unique<ChunkCollection>();
454: 	}
455: }
456: 
457: void ParquetWriteCombine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
458:                          LocalFunctionData &lstate) {
459: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
460: 	auto &local_state = (ParquetWriteLocalState &)lstate;
461: 	// flush any data left in the local state to the file
462: 	global_state.writer->Flush(*local_state.buffer);
463: }
464: 
465: void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
466: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
467: 	// finalize: write any additional metadata to the file here
468: 	global_state.writer->Finalize();
469: }
470: 
471: unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ClientContext &context, FunctionData &bind_data) {
472: 	return make_unique<ParquetWriteLocalState>();
473: }
474: 
475: unique_ptr<TableFunctionRef> ParquetScanReplacement(const string &table_name, void *data) {
476: 	if (!StringUtil::EndsWith(StringUtil::Lower(table_name), ".parquet")) {
477: 		return nullptr;
478: 	}
479: 	auto table_function = make_unique<TableFunctionRef>();
480: 	vector<unique_ptr<ParsedExpression>> children;
481: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
482: 	table_function->function = make_unique<FunctionExpression>("parquet_scan", move(children));
483: 	return table_function;
484: }
485: 
486: void ParquetExtension::Load(DuckDB &db) {
487: 	auto &fs = db.GetFileSystem();
488: 	fs.RegisterSubSystem(FileCompressionType::ZSTD, make_unique<ZStdFileSystem>());
489: 
490: 	auto scan_fun = ParquetScanFunction::GetFunctionSet();
491: 	CreateTableFunctionInfo cinfo(scan_fun);
492: 	cinfo.name = "read_parquet";
493: 	CreateTableFunctionInfo pq_scan = cinfo;
494: 	pq_scan.name = "parquet_scan";
495: 
496: 	ParquetMetaDataFunction meta_fun;
497: 	CreateTableFunctionInfo meta_cinfo(meta_fun);
498: 
499: 	ParquetSchemaFunction schema_fun;
500: 	CreateTableFunctionInfo schema_cinfo(schema_fun);
501: 
502: 	CopyFunction function("parquet");
503: 	function.copy_to_bind = ParquetWriteBind;
504: 	function.copy_to_initialize_global = ParquetWriteInitializeGlobal;
505: 	function.copy_to_initialize_local = ParquetWriteInitializeLocal;
506: 	function.copy_to_sink = ParquetWriteSink;
507: 	function.copy_to_combine = ParquetWriteCombine;
508: 	function.copy_to_finalize = ParquetWriteFinalize;
509: 	function.copy_from_bind = ParquetScanFunction::ParquetReadBind;
510: 	function.copy_from_function = scan_fun.functions[0];
511: 
512: 	function.extension = "parquet";
513: 	CreateCopyFunctionInfo info(function);
514: 
515: 	Connection con(db);
516: 	con.BeginTransaction();
517: 	auto &context = *con.context;
518: 	auto &catalog = Catalog::GetCatalog(context);
519: 	catalog.CreateCopyFunction(context, &info);
520: 	catalog.CreateTableFunction(context, &cinfo);
521: 	catalog.CreateTableFunction(context, &pq_scan);
522: 	catalog.CreateTableFunction(context, &meta_cinfo);
523: 	catalog.CreateTableFunction(context, &schema_cinfo);
524: 	con.Commit();
525: 
526: 	auto &config = DBConfig::GetConfig(*db.instance);
527: 	config.replacement_scans.emplace_back(ParquetScanReplacement);
528: 	config.AddExtensionOption("binary_as_string", "In Parquet files, interpret binary data as a string.",
529: 	                          LogicalType::BOOLEAN);
530: }
531: 
532: std::string ParquetExtension::Name() {
533: 	return "parquet";
534: }
535: 
536: } // namespace duckdb
[end of extension/parquet/parquet-extension.cpp]
[start of extension/parquet/parquet_metadata.cpp]
1: #include "parquet_metadata.hpp"
2: #include <sstream>
3: 
4: #ifndef DUCKDB_AMALGAMATION
5: #include "duckdb/common/types/blob.hpp"
6: #endif
7: 
8: namespace duckdb {
9: 
10: struct ParquetMetaDataBindData : public FunctionData {
11: 	vector<LogicalType> return_types;
12: 	vector<string> files;
13: };
14: 
15: struct ParquetMetaDataOperatorData : public FunctionOperatorData {
16: 	idx_t file_index;
17: 	ChunkCollection collection;
18: 
19: 	static void BindMetaData(vector<LogicalType> &return_types, vector<string> &names);
20: 	static void BindSchema(vector<LogicalType> &return_types, vector<string> &names);
21: 
22: 	void LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types, const string &file_path);
23: 	void LoadSchemaData(ClientContext &context, const vector<LogicalType> &return_types, const string &file_path);
24: };
25: 
26: template <class T>
27: string ConvertParquetElementToString(T &&entry) {
28: 	std::stringstream ss;
29: 	ss << entry;
30: 	return ss.str();
31: }
32: 
33: template <class T>
34: string PrintParquetElementToString(T &&entry) {
35: 	std::stringstream ss;
36: 	entry.printTo(ss);
37: 	return ss.str();
38: }
39: 
40: void ParquetMetaDataOperatorData::BindMetaData(vector<LogicalType> &return_types, vector<string> &names) {
41: 	names.emplace_back("file_name");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("row_group_id");
45: 	return_types.emplace_back(LogicalType::BIGINT);
46: 
47: 	names.emplace_back("row_group_num_rows");
48: 	return_types.emplace_back(LogicalType::BIGINT);
49: 
50: 	names.emplace_back("row_group_num_columns");
51: 	return_types.emplace_back(LogicalType::BIGINT);
52: 
53: 	names.emplace_back("row_group_bytes");
54: 	return_types.emplace_back(LogicalType::BIGINT);
55: 
56: 	names.emplace_back("column_id");
57: 	return_types.emplace_back(LogicalType::BIGINT);
58: 
59: 	names.emplace_back("file_offset");
60: 	return_types.emplace_back(LogicalType::BIGINT);
61: 
62: 	names.emplace_back("num_values");
63: 	return_types.emplace_back(LogicalType::BIGINT);
64: 
65: 	names.emplace_back("path_in_schema");
66: 	return_types.emplace_back(LogicalType::VARCHAR);
67: 
68: 	names.emplace_back("type");
69: 	return_types.emplace_back(LogicalType::VARCHAR);
70: 
71: 	names.emplace_back("stats_min");
72: 	return_types.emplace_back(LogicalType::VARCHAR);
73: 
74: 	names.emplace_back("stats_max");
75: 	return_types.emplace_back(LogicalType::VARCHAR);
76: 
77: 	names.emplace_back("stats_null_count");
78: 	return_types.emplace_back(LogicalType::BIGINT);
79: 
80: 	names.emplace_back("stats_distinct_count");
81: 	return_types.emplace_back(LogicalType::BIGINT);
82: 
83: 	names.emplace_back("stats_min_value");
84: 	return_types.emplace_back(LogicalType::VARCHAR);
85: 
86: 	names.emplace_back("stats_max_value");
87: 	return_types.emplace_back(LogicalType::VARCHAR);
88: 
89: 	names.emplace_back("compression");
90: 	return_types.emplace_back(LogicalType::VARCHAR);
91: 
92: 	names.emplace_back("encodings");
93: 	return_types.emplace_back(LogicalType::VARCHAR);
94: 
95: 	names.emplace_back("index_page_offset");
96: 	return_types.emplace_back(LogicalType::BIGINT);
97: 
98: 	names.emplace_back("dictionary_page_offset");
99: 	return_types.emplace_back(LogicalType::BIGINT);
100: 
101: 	names.emplace_back("data_page_offset");
102: 	return_types.emplace_back(LogicalType::BIGINT);
103: 
104: 	names.emplace_back("total_compressed_size");
105: 	return_types.emplace_back(LogicalType::BIGINT);
106: 
107: 	names.emplace_back("total_uncompressed_size");
108: 	return_types.emplace_back(LogicalType::BIGINT);
109: }
110: 
111: Value ConvertParquetStats(duckdb_parquet::format::Type::type type, bool stats_is_set, const std::string &stats) {
112: 	if (!stats_is_set) {
113: 		return Value(LogicalType::VARCHAR);
114: 	}
115: 	switch (type) {
116: 	case Type::BOOLEAN:
117: 		if (stats.size() == sizeof(bool)) {
118: 			return Value(Value::BOOLEAN(Load<bool>((data_ptr_t)stats.c_str())).ToString());
119: 		}
120: 		break;
121: 	case Type::INT32:
122: 		if (stats.size() == sizeof(int32_t)) {
123: 			return Value(Value::INTEGER(Load<int32_t>((data_ptr_t)stats.c_str())).ToString());
124: 		}
125: 		break;
126: 	case Type::INT64:
127: 		if (stats.size() == sizeof(int64_t)) {
128: 			return Value(Value::BIGINT(Load<int64_t>((data_ptr_t)stats.c_str())).ToString());
129: 		}
130: 		break;
131: 	case Type::FLOAT:
132: 		if (stats.size() == sizeof(float)) {
133: 			float val = Load<float>((data_ptr_t)stats.c_str());
134: 			if (Value::FloatIsValid(val)) {
135: 				return Value(Value::FLOAT(val).ToString());
136: 			}
137: 		}
138: 		break;
139: 	case Type::DOUBLE:
140: 		if (stats.size() == sizeof(double)) {
141: 			double val = Load<double>((data_ptr_t)stats.c_str());
142: 			if (Value::DoubleIsValid(val)) {
143: 				return Value(Value::DOUBLE(val).ToString());
144: 			}
145: 		}
146: 		break;
147: 	case Type::BYTE_ARRAY:
148: 	case Type::INT96:
149: 	case Type::FIXED_LEN_BYTE_ARRAY:
150: 	default:
151: 		break;
152: 	}
153: 	if (Value::StringIsValid(stats)) {
154: 		return Value(stats);
155: 	} else {
156: 		return Value(Blob::ToString(string_t(stats)));
157: 	}
158: }
159: 
160: void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types,
161:                                                    const string &file_path) {
162: 	collection.Reset();
163: 	ParquetOptions parquet_options(context);
164: 	auto reader = make_unique<ParquetReader>(context, file_path, parquet_options);
165: 	idx_t count = 0;
166: 	DataChunk current_chunk;
167: 	current_chunk.Initialize(return_types);
168: 	auto meta_data = reader->GetFileMetadata();
169: 	for (idx_t row_group_idx = 0; row_group_idx < meta_data->row_groups.size(); row_group_idx++) {
170: 		auto &row_group = meta_data->row_groups[row_group_idx];
171: 
172: 		for (idx_t col_idx = 0; col_idx < row_group.columns.size(); col_idx++) {
173: 			auto &column = row_group.columns[col_idx];
174: 			auto &col_meta = column.meta_data;
175: 			auto &stats = col_meta.statistics;
176: 
177: 			// file_name, LogicalType::VARCHAR
178: 			current_chunk.SetValue(0, count, file_path);
179: 
180: 			// row_group_id, LogicalType::BIGINT
181: 			current_chunk.SetValue(1, count, Value::BIGINT(row_group_idx));
182: 
183: 			// row_group_num_rows, LogicalType::BIGINT
184: 			current_chunk.SetValue(2, count, Value::BIGINT(row_group.num_rows));
185: 
186: 			// row_group_num_columns, LogicalType::BIGINT
187: 			current_chunk.SetValue(3, count, Value::BIGINT(row_group.columns.size()));
188: 
189: 			// row_group_bytes, LogicalType::BIGINT
190: 			current_chunk.SetValue(4, count, Value::BIGINT(row_group.total_byte_size));
191: 
192: 			// column_id, LogicalType::BIGINT
193: 			current_chunk.SetValue(5, count, Value::BIGINT(col_idx));
194: 
195: 			// file_offset, LogicalType::BIGINT
196: 			current_chunk.SetValue(6, count, Value::BIGINT(column.file_offset));
197: 
198: 			// num_values, LogicalType::BIGINT
199: 			current_chunk.SetValue(7, count, Value::BIGINT(col_meta.num_values));
200: 
201: 			// path_in_schema, LogicalType::VARCHAR
202: 			current_chunk.SetValue(8, count, StringUtil::Join(col_meta.path_in_schema, ", "));
203: 
204: 			// type, LogicalType::VARCHAR
205: 			current_chunk.SetValue(9, count, ConvertParquetElementToString(col_meta.type));
206: 
207: 			// stats_min, LogicalType::VARCHAR
208: 			current_chunk.SetValue(10, count, ConvertParquetStats(col_meta.type, stats.__isset.min, stats.min));
209: 
210: 			// stats_max, LogicalType::VARCHAR
211: 			current_chunk.SetValue(11, count, ConvertParquetStats(col_meta.type, stats.__isset.max, stats.max));
212: 
213: 			// stats_null_count, LogicalType::BIGINT
214: 			current_chunk.SetValue(
215: 			    12, count, stats.__isset.null_count ? Value::BIGINT(stats.null_count) : Value(LogicalType::BIGINT));
216: 
217: 			// stats_distinct_count, LogicalType::BIGINT
218: 			current_chunk.SetValue(13, count,
219: 			                       stats.__isset.distinct_count ? Value::BIGINT(stats.distinct_count)
220: 			                                                    : Value(LogicalType::BIGINT));
221: 
222: 			// stats_min_value, LogicalType::VARCHAR
223: 			current_chunk.SetValue(14, count,
224: 			                       ConvertParquetStats(col_meta.type, stats.__isset.min_value, stats.min_value));
225: 
226: 			// stats_max_value, LogicalType::VARCHAR
227: 			current_chunk.SetValue(15, count,
228: 			                       ConvertParquetStats(col_meta.type, stats.__isset.max_value, stats.max_value));
229: 
230: 			// compression, LogicalType::VARCHAR
231: 			current_chunk.SetValue(16, count, ConvertParquetElementToString(col_meta.codec));
232: 
233: 			// encodings, LogicalType::VARCHAR
234: 			vector<string> encoding_string;
235: 			for (auto &encoding : col_meta.encodings) {
236: 				encoding_string.push_back(ConvertParquetElementToString(encoding));
237: 			}
238: 			current_chunk.SetValue(17, count, Value(StringUtil::Join(encoding_string, ", ")));
239: 
240: 			// index_page_offset, LogicalType::BIGINT
241: 			current_chunk.SetValue(18, count, Value::BIGINT(col_meta.index_page_offset));
242: 
243: 			// dictionary_page_offset, LogicalType::BIGINT
244: 			current_chunk.SetValue(19, count, Value::BIGINT(col_meta.dictionary_page_offset));
245: 
246: 			// data_page_offset, LogicalType::BIGINT
247: 			current_chunk.SetValue(20, count, Value::BIGINT(col_meta.data_page_offset));
248: 
249: 			// total_compressed_size, LogicalType::BIGINT
250: 			current_chunk.SetValue(21, count, Value::BIGINT(col_meta.total_compressed_size));
251: 
252: 			// total_uncompressed_size, LogicalType::BIGINT
253: 			current_chunk.SetValue(22, count, Value::BIGINT(col_meta.total_uncompressed_size));
254: 
255: 			count++;
256: 			if (count >= STANDARD_VECTOR_SIZE) {
257: 				current_chunk.SetCardinality(count);
258: 				collection.Append(current_chunk);
259: 
260: 				count = 0;
261: 				current_chunk.Reset();
262: 			}
263: 		}
264: 	}
265: 	current_chunk.SetCardinality(count);
266: 	collection.Append(current_chunk);
267: }
268: 
269: void ParquetMetaDataOperatorData::BindSchema(vector<LogicalType> &return_types, vector<string> &names) {
270: 	names.emplace_back("file_name");
271: 	return_types.emplace_back(LogicalType::VARCHAR);
272: 
273: 	names.emplace_back("name");
274: 	return_types.emplace_back(LogicalType::VARCHAR);
275: 
276: 	names.emplace_back("type");
277: 	return_types.emplace_back(LogicalType::VARCHAR);
278: 
279: 	names.emplace_back("type_length");
280: 	return_types.emplace_back(LogicalType::VARCHAR);
281: 
282: 	names.emplace_back("repetition_type");
283: 	return_types.emplace_back(LogicalType::VARCHAR);
284: 
285: 	names.emplace_back("num_children");
286: 	return_types.emplace_back(LogicalType::BIGINT);
287: 
288: 	names.emplace_back("converted_type");
289: 	return_types.emplace_back(LogicalType::VARCHAR);
290: 
291: 	names.emplace_back("scale");
292: 	return_types.emplace_back(LogicalType::BIGINT);
293: 
294: 	names.emplace_back("precision");
295: 	return_types.emplace_back(LogicalType::BIGINT);
296: 
297: 	names.emplace_back("field_id");
298: 	return_types.emplace_back(LogicalType::BIGINT);
299: 
300: 	names.emplace_back("logical_type");
301: 	return_types.emplace_back(LogicalType::VARCHAR);
302: }
303: 
304: Value ParquetLogicalTypeToString(const duckdb_parquet::format::LogicalType &type) {
305: 
306: 	if (type.__isset.STRING) {
307: 		return Value(PrintParquetElementToString(type.STRING));
308: 	}
309: 	if (type.__isset.MAP) {
310: 		return Value(PrintParquetElementToString(type.MAP));
311: 	}
312: 	if (type.__isset.LIST) {
313: 		return Value(PrintParquetElementToString(type.LIST));
314: 	}
315: 	if (type.__isset.ENUM) {
316: 		return Value(PrintParquetElementToString(type.ENUM));
317: 	}
318: 	if (type.__isset.DECIMAL) {
319: 		return Value(PrintParquetElementToString(type.DECIMAL));
320: 	}
321: 	if (type.__isset.DATE) {
322: 		return Value(PrintParquetElementToString(type.DATE));
323: 	}
324: 	if (type.__isset.TIME) {
325: 		return Value(PrintParquetElementToString(type.TIME));
326: 	}
327: 	if (type.__isset.TIMESTAMP) {
328: 		return Value(PrintParquetElementToString(type.TIMESTAMP));
329: 	}
330: 	if (type.__isset.INTEGER) {
331: 		return Value(PrintParquetElementToString(type.INTEGER));
332: 	}
333: 	if (type.__isset.UNKNOWN) {
334: 		return Value(PrintParquetElementToString(type.UNKNOWN));
335: 	}
336: 	if (type.__isset.JSON) {
337: 		return Value(PrintParquetElementToString(type.JSON));
338: 	}
339: 	if (type.__isset.BSON) {
340: 		return Value(PrintParquetElementToString(type.BSON));
341: 	}
342: 	if (type.__isset.UUID) {
343: 		return Value(PrintParquetElementToString(type.UUID));
344: 	}
345: 	return Value();
346: }
347: 
348: void ParquetMetaDataOperatorData::LoadSchemaData(ClientContext &context, const vector<LogicalType> &return_types,
349:                                                  const string &file_path) {
350: 	collection.Reset();
351: 	ParquetOptions parquet_options(context);
352: 	auto reader = make_unique<ParquetReader>(context, file_path, parquet_options);
353: 	idx_t count = 0;
354: 	DataChunk current_chunk;
355: 	current_chunk.Initialize(return_types);
356: 	auto meta_data = reader->GetFileMetadata();
357: 	for (idx_t col_idx = 0; col_idx < meta_data->schema.size(); col_idx++) {
358: 		auto &column = meta_data->schema[col_idx];
359: 
360: 		// file_name, LogicalType::VARCHAR
361: 		current_chunk.SetValue(0, count, file_path);
362: 
363: 		// name, LogicalType::VARCHAR
364: 		current_chunk.SetValue(1, count, column.name);
365: 
366: 		// type, LogicalType::VARCHAR
367: 		current_chunk.SetValue(2, count, ConvertParquetElementToString(column.type));
368: 
369: 		// type_length, LogicalType::VARCHAR
370: 		current_chunk.SetValue(3, count, Value::INTEGER(column.type_length));
371: 
372: 		// repetition_type, LogicalType::VARCHAR
373: 		current_chunk.SetValue(4, count, ConvertParquetElementToString(column.repetition_type));
374: 
375: 		// num_children, LogicalType::BIGINT
376: 		current_chunk.SetValue(5, count, Value::BIGINT(column.num_children));
377: 
378: 		// converted_type, LogicalType::VARCHAR
379: 		current_chunk.SetValue(6, count, ConvertParquetElementToString(column.converted_type));
380: 
381: 		// scale, LogicalType::BIGINT
382: 		current_chunk.SetValue(7, count, Value::BIGINT(column.scale));
383: 
384: 		// precision, LogicalType::BIGINT
385: 		current_chunk.SetValue(8, count, Value::BIGINT(column.precision));
386: 
387: 		// field_id, LogicalType::BIGINT
388: 		current_chunk.SetValue(9, count, Value::BIGINT(column.field_id));
389: 
390: 		// logical_type, LogicalType::VARCHAR
391: 		current_chunk.SetValue(10, count, ParquetLogicalTypeToString(column.logicalType));
392: 
393: 		count++;
394: 		if (count >= STANDARD_VECTOR_SIZE) {
395: 			current_chunk.SetCardinality(count);
396: 			collection.Append(current_chunk);
397: 
398: 			count = 0;
399: 			current_chunk.Reset();
400: 		}
401: 	}
402: 	current_chunk.SetCardinality(count);
403: 	collection.Append(current_chunk);
404: }
405: 
406: template <bool SCHEMA>
407: unique_ptr<FunctionData> ParquetMetaDataBind(ClientContext &context, vector<Value> &inputs,
408:                                              unordered_map<string, Value> &named_parameters,
409:                                              vector<LogicalType> &input_table_types, vector<string> &input_table_names,
410:                                              vector<LogicalType> &return_types, vector<string> &names) {
411: 	if (SCHEMA) {
412: 		ParquetMetaDataOperatorData::BindSchema(return_types, names);
413: 	} else {
414: 		ParquetMetaDataOperatorData::BindMetaData(return_types, names);
415: 	}
416: 
417: 	auto file_name = inputs[0].GetValue<string>();
418: 	auto result = make_unique<ParquetMetaDataBindData>();
419: 
420: 	FileSystem &fs = FileSystem::GetFileSystem(context);
421: 	result->return_types = return_types;
422: 	result->files = fs.Glob(file_name);
423: 	if (result->files.empty()) {
424: 		throw IOException("No files found that match the pattern \"%s\"", file_name);
425: 	}
426: 	return move(result);
427: }
428: 
429: template <bool SCHEMA>
430: unique_ptr<FunctionOperatorData> ParquetMetaDataInit(ClientContext &context, const FunctionData *bind_data_p,
431:                                                      const vector<column_t> &column_ids,
432:                                                      TableFilterCollection *filters) {
433: 	auto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;
434: 	D_ASSERT(!bind_data.files.empty());
435: 
436: 	auto result = make_unique<ParquetMetaDataOperatorData>();
437: 	if (SCHEMA) {
438: 		result->LoadSchemaData(context, bind_data.return_types, bind_data.files[0]);
439: 	} else {
440: 		result->LoadFileMetaData(context, bind_data.return_types, bind_data.files[0]);
441: 	}
442: 	result->file_index = 0;
443: 	return move(result);
444: }
445: 
446: template <bool SCHEMA>
447: void ParquetMetaDataImplementation(ClientContext &context, const FunctionData *bind_data_p,
448:                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
449: 	auto &data = (ParquetMetaDataOperatorData &)*operator_state;
450: 	auto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;
451: 	while (true) {
452: 		auto chunk = data.collection.Fetch();
453: 		if (!chunk) {
454: 			if (data.file_index + 1 < bind_data.files.size()) {
455: 				// load the metadata for the next file
456: 				data.file_index++;
457: 				if (SCHEMA) {
458: 					data.LoadSchemaData(context, bind_data.return_types, bind_data.files[data.file_index]);
459: 				} else {
460: 					data.LoadFileMetaData(context, bind_data.return_types, bind_data.files[data.file_index]);
461: 				}
462: 				continue;
463: 			} else {
464: 				// no files remaining: done
465: 				return;
466: 			}
467: 		}
468: 		output.Move(*chunk);
469: 		if (output.size() != 0) {
470: 			return;
471: 		}
472: 	}
473: }
474: 
475: ParquetMetaDataFunction::ParquetMetaDataFunction()
476:     : TableFunction("parquet_metadata", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<false>,
477:                     ParquetMetaDataBind<false>, ParquetMetaDataInit<false>, /* statistics */ nullptr,
478:                     /* cleanup */ nullptr,
479:                     /* dependency */ nullptr, nullptr,
480:                     /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,
481:                     nullptr, false, false, nullptr) {
482: }
483: 
484: ParquetSchemaFunction::ParquetSchemaFunction()
485:     : TableFunction("parquet_schema", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<true>,
486:                     ParquetMetaDataBind<true>, ParquetMetaDataInit<true>, /* statistics */ nullptr,
487:                     /* cleanup */ nullptr,
488:                     /* dependency */ nullptr, nullptr,
489:                     /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,
490:                     nullptr, false, false, nullptr) {
491: }
492: 
493: } // namespace duckdb
[end of extension/parquet/parquet_metadata.cpp]
[start of src/common/exception.cpp]
1: #include "duckdb/common/exception.hpp"
2: 
3: #include "duckdb/common/string_util.hpp"
4: #include "duckdb/common/to_string.hpp"
5: #include "duckdb/common/types.hpp"
6: 
7: namespace duckdb {
8: 
9: Exception::Exception(const string &msg) : std::exception(), type(ExceptionType::INVALID) {
10: 	exception_message_ = msg;
11: }
12: 
13: Exception::Exception(ExceptionType exception_type, const string &message) : std::exception(), type(exception_type) {
14: 	exception_message_ = ExceptionTypeToString(exception_type) + " Error: " + message;
15: }
16: 
17: const char *Exception::what() const noexcept {
18: 	return exception_message_.c_str();
19: }
20: 
21: bool Exception::UncaughtException() {
22: #if __cplusplus >= 201703L
23: 	return std::uncaught_exceptions() > 0;
24: #else
25: 	return std::uncaught_exception();
26: #endif
27: }
28: 
29: string Exception::ConstructMessageRecursive(const string &msg, vector<ExceptionFormatValue> &values) {
30: 	return ExceptionFormatValue::Format(msg, values);
31: }
32: 
33: string Exception::ExceptionTypeToString(ExceptionType type) {
34: 	switch (type) {
35: 	case ExceptionType::INVALID:
36: 		return "Invalid";
37: 	case ExceptionType::OUT_OF_RANGE:
38: 		return "Out of Range";
39: 	case ExceptionType::CONVERSION:
40: 		return "Conversion";
41: 	case ExceptionType::UNKNOWN_TYPE:
42: 		return "Unknown Type";
43: 	case ExceptionType::DECIMAL:
44: 		return "Decimal";
45: 	case ExceptionType::MISMATCH_TYPE:
46: 		return "Mismatch Type";
47: 	case ExceptionType::DIVIDE_BY_ZERO:
48: 		return "Divide by Zero";
49: 	case ExceptionType::OBJECT_SIZE:
50: 		return "Object Size";
51: 	case ExceptionType::INVALID_TYPE:
52: 		return "Invalid type";
53: 	case ExceptionType::SERIALIZATION:
54: 		return "Serialization";
55: 	case ExceptionType::TRANSACTION:
56: 		return "TransactionContext";
57: 	case ExceptionType::NOT_IMPLEMENTED:
58: 		return "Not implemented";
59: 	case ExceptionType::EXPRESSION:
60: 		return "Expression";
61: 	case ExceptionType::CATALOG:
62: 		return "Catalog";
63: 	case ExceptionType::PARSER:
64: 		return "Parser";
65: 	case ExceptionType::BINDER:
66: 		return "Binder";
67: 	case ExceptionType::PLANNER:
68: 		return "Planner";
69: 	case ExceptionType::SCHEDULER:
70: 		return "Scheduler";
71: 	case ExceptionType::EXECUTOR:
72: 		return "Executor";
73: 	case ExceptionType::CONSTRAINT:
74: 		return "Constraint";
75: 	case ExceptionType::INDEX:
76: 		return "Index";
77: 	case ExceptionType::STAT:
78: 		return "Stat";
79: 	case ExceptionType::CONNECTION:
80: 		return "Connection";
81: 	case ExceptionType::SYNTAX:
82: 		return "Syntax";
83: 	case ExceptionType::SETTINGS:
84: 		return "Settings";
85: 	case ExceptionType::OPTIMIZER:
86: 		return "Optimizer";
87: 	case ExceptionType::NULL_POINTER:
88: 		return "NullPointer";
89: 	case ExceptionType::IO:
90: 		return "IO";
91: 	case ExceptionType::INTERRUPT:
92: 		return "INTERRUPT";
93: 	case ExceptionType::FATAL:
94: 		return "FATAL";
95: 	case ExceptionType::INTERNAL:
96: 		return "INTERNAL";
97: 	case ExceptionType::INVALID_INPUT:
98: 		return "Invalid Input";
99: 	case ExceptionType::OUT_OF_MEMORY:
100: 		return "Out of Memory";
101: 	default:
102: 		return "Unknown";
103: 	}
104: }
105: 
106: StandardException::StandardException(ExceptionType exception_type, const string &message)
107:     : Exception(exception_type, message) {
108: }
109: 
110: CastException::CastException(const PhysicalType orig_type, const PhysicalType new_type)
111:     : Exception(ExceptionType::CONVERSION,
112:                 "Type " + TypeIdToString(orig_type) + " can't be cast as " + TypeIdToString(new_type)) {
113: }
114: 
115: CastException::CastException(const LogicalType &orig_type, const LogicalType &new_type)
116:     : Exception(ExceptionType::CONVERSION,
117:                 "Type " + orig_type.ToString() + " can't be cast as " + new_type.ToString()) {
118: }
119: 
120: ValueOutOfRangeException::ValueOutOfRangeException(const int64_t value, const PhysicalType orig_type,
121:                                                    const PhysicalType new_type)
122:     : Exception(ExceptionType::CONVERSION, "Type " + TypeIdToString(orig_type) + " with value " +
123:                                                to_string((intmax_t)value) +
124:                                                " can't be cast because the value is out of range "
125:                                                "for the destination type " +
126:                                                TypeIdToString(new_type)) {
127: }
128: 
129: ValueOutOfRangeException::ValueOutOfRangeException(const double value, const PhysicalType orig_type,
130:                                                    const PhysicalType new_type)
131:     : Exception(ExceptionType::CONVERSION, "Type " + TypeIdToString(orig_type) + " with value " + to_string(value) +
132:                                                " can't be cast because the value is out of range "
133:                                                "for the destination type " +
134:                                                TypeIdToString(new_type)) {
135: }
136: 
137: ValueOutOfRangeException::ValueOutOfRangeException(const hugeint_t value, const PhysicalType orig_type,
138:                                                    const PhysicalType new_type)
139:     : Exception(ExceptionType::CONVERSION, "Type " + TypeIdToString(orig_type) + " with value " + value.ToString() +
140:                                                " can't be cast because the value is out of range "
141:                                                "for the destination type " +
142:                                                TypeIdToString(new_type)) {
143: }
144: 
145: ValueOutOfRangeException::ValueOutOfRangeException(const PhysicalType var_type, const idx_t length)
146:     : Exception(ExceptionType::OUT_OF_RANGE,
147:                 "The value is too long to fit into type " + TypeIdToString(var_type) + "(" + to_string(length) + ")") {
148: }
149: 
150: ConversionException::ConversionException(const string &msg) : Exception(ExceptionType::CONVERSION, msg) {
151: }
152: 
153: InvalidTypeException::InvalidTypeException(PhysicalType type, const string &msg)
154:     : Exception(ExceptionType::INVALID_TYPE, "Invalid Type [" + TypeIdToString(type) + "]: " + msg) {
155: }
156: 
157: InvalidTypeException::InvalidTypeException(const LogicalType &type, const string &msg)
158:     : Exception(ExceptionType::INVALID_TYPE, "Invalid Type [" + type.ToString() + "]: " + msg) {
159: }
160: 
161: TypeMismatchException::TypeMismatchException(const PhysicalType type_1, const PhysicalType type_2, const string &msg)
162:     : Exception(ExceptionType::MISMATCH_TYPE,
163:                 "Type " + TypeIdToString(type_1) + " does not match with " + TypeIdToString(type_2) + ". " + msg) {
164: }
165: 
166: TypeMismatchException::TypeMismatchException(const LogicalType &type_1, const LogicalType &type_2, const string &msg)
167:     : Exception(ExceptionType::MISMATCH_TYPE,
168:                 "Type " + type_1.ToString() + " does not match with " + type_2.ToString() + ". " + msg) {
169: }
170: 
171: TransactionException::TransactionException(const string &msg) : Exception(ExceptionType::TRANSACTION, msg) {
172: }
173: 
174: NotImplementedException::NotImplementedException(const string &msg) : Exception(ExceptionType::NOT_IMPLEMENTED, msg) {
175: }
176: 
177: OutOfRangeException::OutOfRangeException(const string &msg) : Exception(ExceptionType::OUT_OF_RANGE, msg) {
178: }
179: 
180: CatalogException::CatalogException(const string &msg) : StandardException(ExceptionType::CATALOG, msg) {
181: }
182: 
183: ParserException::ParserException(const string &msg) : StandardException(ExceptionType::PARSER, msg) {
184: }
185: 
186: SyntaxException::SyntaxException(const string &msg) : Exception(ExceptionType::SYNTAX, msg) {
187: }
188: 
189: ConstraintException::ConstraintException(const string &msg) : Exception(ExceptionType::CONSTRAINT, msg) {
190: }
191: 
192: BinderException::BinderException(const string &msg) : StandardException(ExceptionType::BINDER, msg) {
193: }
194: 
195: IOException::IOException(const string &msg) : Exception(ExceptionType::IO, msg) {
196: }
197: 
198: SerializationException::SerializationException(const string &msg) : Exception(ExceptionType::SERIALIZATION, msg) {
199: }
200: 
201: SequenceException::SequenceException(const string &msg) : Exception(ExceptionType::SERIALIZATION, msg) {
202: }
203: 
204: InterruptException::InterruptException() : Exception(ExceptionType::INTERRUPT, "Interrupted!") {
205: }
206: 
207: FatalException::FatalException(const string &msg) : Exception(ExceptionType::FATAL, msg) {
208: }
209: 
210: InternalException::InternalException(const string &msg) : Exception(ExceptionType::INTERNAL, msg) {
211: }
212: 
213: InvalidInputException::InvalidInputException(const string &msg) : Exception(ExceptionType::INVALID_INPUT, msg) {
214: }
215: 
216: OutOfMemoryException::OutOfMemoryException(const string &msg) : Exception(ExceptionType::OUT_OF_MEMORY, msg) {
217: }
218: 
219: } // namespace duckdb
[end of src/common/exception.cpp]
[start of src/execution/physical_plan/plan_export.cpp]
1: #include "duckdb/execution/physical_plan_generator.hpp"
2: #include "duckdb/execution/operator/persistent/physical_export.hpp"
3: #include "duckdb/planner/operator/logical_export.hpp"
4: 
5: namespace duckdb {
6: 
7: unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalExport &op) {
8: 	auto export_node = make_unique<PhysicalExport>(op.types, op.function, move(op.copy_info), op.estimated_cardinality,
9: 	                                               op.exported_tables);
10: 	// plan the underlying copy statements, if any
11: 	if (!op.children.empty()) {
12: 		auto plan = CreatePlan(*op.children[0]);
13: 		export_node->children.push_back(move(plan));
14: 	}
15: 	return move(export_node);
16: }
17: 
18: } // namespace duckdb
[end of src/execution/physical_plan/plan_export.cpp]
[start of src/function/pragma/pragma_queries.cpp]
1: #include "duckdb/function/pragma/pragma_functions.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/common/file_system.hpp"
4: 
5: namespace duckdb {
6: 
7: string PragmaTableInfo(ClientContext &context, const FunctionParameters &parameters) {
8: 	return StringUtil::Format("SELECT * FROM pragma_table_info('%s')", parameters.values[0].ToString());
9: }
10: 
11: string PragmaShowTables(ClientContext &context, const FunctionParameters &parameters) {
12: 	return "SELECT name FROM sqlite_master ORDER BY name";
13: }
14: 
15: string PragmaAllProfiling(ClientContext &context, const FunctionParameters &parameters) {
16: 	return "SELECT * FROM pragma_last_profiling_output() JOIN pragma_detailed_profiling_output() ON "
17: 	       "(pragma_last_profiling_output.operator_id);";
18: }
19: 
20: string PragmaDatabaseList(ClientContext &context, const FunctionParameters &parameters) {
21: 	return "SELECT * FROM pragma_database_list() ORDER BY 1";
22: }
23: 
24: string PragmaCollations(ClientContext &context, const FunctionParameters &parameters) {
25: 	return "SELECT * FROM pragma_collations() ORDER BY 1";
26: }
27: 
28: string PragmaFunctionsQuery(ClientContext &context, const FunctionParameters &parameters) {
29: 	return "SELECT * FROM pragma_functions() ORDER BY 1";
30: }
31: 
32: string PragmaShow(ClientContext &context, const FunctionParameters &parameters) {
33: 	// PRAGMA table_info but with some aliases
34: 	return StringUtil::Format(
35: 	    "SELECT name AS \"Field\", type as \"Type\", CASE WHEN \"notnull\" THEN 'NO' ELSE 'YES' END AS \"Null\", "
36: 	    "NULL AS \"Key\", dflt_value AS \"Default\", NULL AS \"Extra\" FROM pragma_table_info('%s')",
37: 	    parameters.values[0].ToString());
38: }
39: 
40: string PragmaVersion(ClientContext &context, const FunctionParameters &parameters) {
41: 	return "SELECT * FROM pragma_version()";
42: }
43: 
44: string PragmaImportDatabase(ClientContext &context, const FunctionParameters &parameters) {
45: 	auto &fs = FileSystem::GetFileSystem(context);
46: 	auto *opener = FileSystem::GetFileOpener(context);
47: 
48: 	string query;
49: 	// read the "shema.sql" and "load.sql" files
50: 	vector<string> files = {"schema.sql", "load.sql"};
51: 	for (auto &file : files) {
52: 		auto file_path = fs.JoinPath(parameters.values[0].ToString(), file);
53: 		auto handle = fs.OpenFile(file_path, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
54: 		                          FileSystem::DEFAULT_COMPRESSION, opener);
55: 		auto fsize = fs.GetFileSize(*handle);
56: 		auto buffer = unique_ptr<char[]>(new char[fsize]);
57: 		fs.Read(*handle, buffer.get(), fsize);
58: 
59: 		query += string(buffer.get(), fsize);
60: 	}
61: 	return query;
62: }
63: 
64: string PragmaDatabaseSize(ClientContext &context, const FunctionParameters &parameters) {
65: 	return "SELECT * FROM pragma_database_size()";
66: }
67: 
68: string PragmaStorageInfo(ClientContext &context, const FunctionParameters &parameters) {
69: 	return StringUtil::Format("SELECT * FROM pragma_storage_info('%s')", parameters.values[0].ToString());
70: }
71: 
72: void PragmaQueries::RegisterFunction(BuiltinFunctions &set) {
73: 	set.AddFunction(PragmaFunction::PragmaCall("table_info", PragmaTableInfo, {LogicalType::VARCHAR}));
74: 	set.AddFunction(PragmaFunction::PragmaCall("storage_info", PragmaStorageInfo, {LogicalType::VARCHAR}));
75: 	set.AddFunction(PragmaFunction::PragmaStatement("show_tables", PragmaShowTables));
76: 	set.AddFunction(PragmaFunction::PragmaStatement("database_list", PragmaDatabaseList));
77: 	set.AddFunction(PragmaFunction::PragmaStatement("collations", PragmaCollations));
78: 	set.AddFunction(PragmaFunction::PragmaCall("show", PragmaShow, {LogicalType::VARCHAR}));
79: 	set.AddFunction(PragmaFunction::PragmaStatement("version", PragmaVersion));
80: 	set.AddFunction(PragmaFunction::PragmaStatement("database_size", PragmaDatabaseSize));
81: 	set.AddFunction(PragmaFunction::PragmaStatement("functions", PragmaFunctionsQuery));
82: 	set.AddFunction(PragmaFunction::PragmaCall("import_database", PragmaImportDatabase, {LogicalType::VARCHAR}));
83: 	set.AddFunction(PragmaFunction::PragmaStatement("all_profiling_output", PragmaAllProfiling));
84: }
85: 
86: } // namespace duckdb
[end of src/function/pragma/pragma_queries.cpp]
[start of src/function/table/glob.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/function/table_function.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/common/file_system.hpp"
5: 
6: namespace duckdb {
7: 
8: struct GlobFunctionBindData : public TableFunctionData {
9: 	vector<string> files;
10: };
11: 
12: static unique_ptr<FunctionData> GlobFunctionBind(ClientContext &context, vector<Value> &inputs,
13:                                                  unordered_map<string, Value> &named_parameters,
14:                                                  vector<LogicalType> &input_table_types,
15:                                                  vector<string> &input_table_names, vector<LogicalType> &return_types,
16:                                                  vector<string> &names) {
17: 	auto result = make_unique<GlobFunctionBindData>();
18: 	auto &fs = FileSystem::GetFileSystem(context);
19: 	result->files = fs.Glob(inputs[0].str_value);
20: 	return_types.emplace_back(LogicalType::VARCHAR);
21: 	names.emplace_back("file");
22: 	return move(result);
23: }
24: 
25: struct GlobFunctionState : public FunctionOperatorData {
26: 	GlobFunctionState() : current_idx(0) {
27: 	}
28: 
29: 	idx_t current_idx;
30: };
31: 
32: static unique_ptr<FunctionOperatorData> GlobFunctionInit(ClientContext &context, const FunctionData *bind_data,
33:                                                          const vector<column_t> &column_ids,
34:                                                          TableFilterCollection *filters) {
35: 	return make_unique<GlobFunctionState>();
36: }
37: 
38: static void GlobFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *state_p,
39:                          DataChunk *input, DataChunk &output) {
40: 	auto &bind_data = (GlobFunctionBindData &)*bind_data_p;
41: 	auto &state = (GlobFunctionState &)*state_p;
42: 
43: 	idx_t count = 0;
44: 	idx_t next_idx = MinValue<idx_t>(state.current_idx + STANDARD_VECTOR_SIZE, bind_data.files.size());
45: 	for (; state.current_idx < next_idx; state.current_idx++) {
46: 		output.data[0].SetValue(count, bind_data.files[state.current_idx]);
47: 		count++;
48: 	}
49: 	output.SetCardinality(count);
50: }
51: 
52: void GlobTableFunction::RegisterFunction(BuiltinFunctions &set) {
53: 	TableFunctionSet glob("glob");
54: 	glob.AddFunction(TableFunction({LogicalType::VARCHAR}, GlobFunction, GlobFunctionBind, GlobFunctionInit));
55: 	set.AddFunction(glob);
56: }
57: 
58: } // namespace duckdb
[end of src/function/table/glob.cpp]
[start of src/function/table/read_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/main/database.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/main/config.hpp"
8: #include "duckdb/parser/expression/constant_expression.hpp"
9: #include "duckdb/parser/expression/function_expression.hpp"
10: #include "duckdb/parser/tableref/table_function_ref.hpp"
11: #include "duckdb/common/windows_undefs.hpp"
12: 
13: #include <limits>
14: 
15: namespace duckdb {
16: 
17: static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, vector<Value> &inputs,
18:                                             unordered_map<string, Value> &named_parameters,
19:                                             vector<LogicalType> &input_table_types, vector<string> &input_table_names,
20:                                             vector<LogicalType> &return_types, vector<string> &names) {
21: 	auto result = make_unique<ReadCSVData>();
22: 	auto &options = result->options;
23: 
24: 	string file_pattern = inputs[0].str_value;
25: 
26: 	auto &fs = FileSystem::GetFileSystem(context);
27: 	result->files = fs.Glob(file_pattern);
28: 	if (result->files.empty()) {
29: 		throw IOException("No files found that match the pattern \"%s\"", file_pattern);
30: 	}
31: 
32: 	for (auto &kv : named_parameters) {
33: 		if (kv.first == "auto_detect") {
34: 			options.auto_detect = kv.second.value_.boolean;
35: 		} else if (kv.first == "sep" || kv.first == "delim") {
36: 			options.SetDelimiter(kv.second.str_value);
37: 		} else if (kv.first == "header") {
38: 			options.header = kv.second.value_.boolean;
39: 			options.has_header = true;
40: 		} else if (kv.first == "quote") {
41: 			options.quote = kv.second.str_value;
42: 			options.has_quote = true;
43: 		} else if (kv.first == "escape") {
44: 			options.escape = kv.second.str_value;
45: 			options.has_escape = true;
46: 		} else if (kv.first == "nullstr") {
47: 			options.null_str = kv.second.str_value;
48: 		} else if (kv.first == "sample_size") {
49: 			int64_t sample_size = kv.second.GetValue<int64_t>();
50: 			if (sample_size < 1 && sample_size != -1) {
51: 				throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
52: 			}
53: 			if (sample_size == -1) {
54: 				options.sample_chunks = std::numeric_limits<uint64_t>::max();
55: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
56: 			} else if (sample_size <= STANDARD_VECTOR_SIZE) {
57: 				options.sample_chunk_size = sample_size;
58: 				options.sample_chunks = 1;
59: 			} else {
60: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
61: 				options.sample_chunks = sample_size / STANDARD_VECTOR_SIZE;
62: 			}
63: 		} else if (kv.first == "sample_chunk_size") {
64: 			options.sample_chunk_size = kv.second.GetValue<int64_t>();
65: 			if (options.sample_chunk_size > STANDARD_VECTOR_SIZE) {
66: 				throw BinderException(
67: 				    "Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
68: 				    STANDARD_VECTOR_SIZE);
69: 			} else if (options.sample_chunk_size < 1) {
70: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be smaller than 1");
71: 			}
72: 		} else if (kv.first == "sample_chunks") {
73: 			options.sample_chunks = kv.second.GetValue<int64_t>();
74: 			if (options.sample_chunks < 1) {
75: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNKS: cannot be smaller than 1");
76: 			}
77: 		} else if (kv.first == "all_varchar") {
78: 			options.all_varchar = kv.second.value_.boolean;
79: 		} else if (kv.first == "dateformat") {
80: 			options.has_format[LogicalTypeId::DATE] = true;
81: 			auto &date_format = options.date_format[LogicalTypeId::DATE];
82: 			date_format.format_specifier = kv.second.str_value;
83: 			string error = StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
84: 			if (!error.empty()) {
85: 				throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
86: 			}
87: 		} else if (kv.first == "timestampformat") {
88: 			options.has_format[LogicalTypeId::TIMESTAMP] = true;
89: 			auto &timestamp_format = options.date_format[LogicalTypeId::TIMESTAMP];
90: 			timestamp_format.format_specifier = kv.second.str_value;
91: 			string error = StrTimeFormat::ParseFormatSpecifier(timestamp_format.format_specifier, timestamp_format);
92: 			if (!error.empty()) {
93: 				throw InvalidInputException("Could not parse TIMESTAMPFORMAT: %s", error.c_str());
94: 			}
95: 		} else if (kv.first == "normalize_names") {
96: 			options.normalize_names = kv.second.value_.boolean;
97: 		} else if (kv.first == "columns") {
98: 			auto &child_type = kv.second.type();
99: 			if (child_type.id() != LogicalTypeId::STRUCT) {
100: 				throw BinderException("read_csv columns requires a a struct as input");
101: 			}
102: 			D_ASSERT(StructType::GetChildCount(child_type) == kv.second.struct_value.size());
103: 			for (idx_t i = 0; i < kv.second.struct_value.size(); i++) {
104: 				auto &name = StructType::GetChildName(child_type, i);
105: 				auto &val = kv.second.struct_value[i];
106: 				names.push_back(name);
107: 				if (val.type().id() != LogicalTypeId::VARCHAR) {
108: 					throw BinderException("read_csv requires a type specification as string");
109: 				}
110: 				return_types.emplace_back(TransformStringToLogicalType(val.str_value.c_str()));
111: 			}
112: 			if (names.empty()) {
113: 				throw BinderException("read_csv requires at least a single column as input!");
114: 			}
115: 		} else if (kv.first == "compression") {
116: 			options.compression = FileCompressionTypeFromString(kv.second.str_value);
117: 		} else if (kv.first == "filename") {
118: 			result->include_file_name = kv.second.value_.boolean;
119: 		} else if (kv.first == "skip") {
120: 			options.skip_rows = kv.second.GetValue<int64_t>();
121: 		}
122: 	}
123: 	if (!options.auto_detect && return_types.empty()) {
124: 		throw BinderException("read_csv requires columns to be specified. Use read_csv_auto or set read_csv(..., "
125: 		                      "AUTO_DETECT=TRUE) to automatically guess columns.");
126: 	}
127: 	if (options.auto_detect) {
128: 		options.file_path = result->files[0];
129: 		auto initial_reader = make_unique<BufferedCSVReader>(context, options);
130: 
131: 		return_types.assign(initial_reader->sql_types.begin(), initial_reader->sql_types.end());
132: 		if (names.empty()) {
133: 			names.assign(initial_reader->col_names.begin(), initial_reader->col_names.end());
134: 		} else {
135: 			D_ASSERT(return_types.size() == names.size());
136: 		}
137: 		result->initial_reader = move(initial_reader);
138: 	} else {
139: 		result->sql_types = return_types;
140: 		D_ASSERT(return_types.size() == names.size());
141: 	}
142: 	if (result->include_file_name) {
143: 		return_types.emplace_back(LogicalType::VARCHAR);
144: 		names.emplace_back("filename");
145: 	}
146: 	return move(result);
147: }
148: 
149: struct ReadCSVOperatorData : public FunctionOperatorData {
150: 	//! The CSV reader
151: 	unique_ptr<BufferedCSVReader> csv_reader;
152: 	//! The index of the next file to read (i.e. current file + 1)
153: 	idx_t file_index;
154: };
155: 
156: static unique_ptr<FunctionOperatorData> ReadCSVInit(ClientContext &context, const FunctionData *bind_data_p,
157:                                                     const vector<column_t> &column_ids,
158:                                                     TableFilterCollection *filters) {
159: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
160: 	auto result = make_unique<ReadCSVOperatorData>();
161: 	if (bind_data.initial_reader) {
162: 		result->csv_reader = move(bind_data.initial_reader);
163: 	} else {
164: 		bind_data.options.file_path = bind_data.files[0];
165: 		result->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);
166: 	}
167: 	bind_data.bytes_read = 0;
168: 	bind_data.file_size = result->csv_reader->file_size;
169: 	result->file_index = 1;
170: 	return move(result);
171: }
172: 
173: static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, vector<Value> &inputs,
174:                                                 unordered_map<string, Value> &named_parameters,
175:                                                 vector<LogicalType> &input_table_types,
176:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
177:                                                 vector<string> &names) {
178: 	named_parameters["auto_detect"] = Value::BOOLEAN(true);
179: 	return ReadCSVBind(context, inputs, named_parameters, input_table_types, input_table_names, return_types, names);
180: }
181: 
182: static void ReadCSVFunction(ClientContext &context, const FunctionData *bind_data_p,
183:                             FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
184: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
185: 	auto &data = (ReadCSVOperatorData &)*operator_state;
186: 	do {
187: 		data.csv_reader->ParseCSV(output);
188: 		bind_data.bytes_read = data.csv_reader->bytes_in_chunk;
189: 		if (output.size() == 0 && data.file_index < bind_data.files.size()) {
190: 			// exhausted this file, but we have more files we can read
191: 			// open the next file and increment the counter
192: 			bind_data.options.file_path = bind_data.files[data.file_index];
193: 			data.csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, data.csv_reader->sql_types);
194: 			data.file_index++;
195: 		} else {
196: 			break;
197: 		}
198: 	} while (true);
199: 	if (bind_data.include_file_name) {
200: 		auto &col = output.data.back();
201: 		col.SetValue(0, Value(data.csv_reader->options.file_path));
202: 		col.SetVectorType(VectorType::CONSTANT_VECTOR);
203: 	}
204: }
205: 
206: static void ReadCSVAddNamedParameters(TableFunction &table_function) {
207: 	table_function.named_parameters["sep"] = LogicalType::VARCHAR;
208: 	table_function.named_parameters["delim"] = LogicalType::VARCHAR;
209: 	table_function.named_parameters["quote"] = LogicalType::VARCHAR;
210: 	table_function.named_parameters["escape"] = LogicalType::VARCHAR;
211: 	table_function.named_parameters["nullstr"] = LogicalType::VARCHAR;
212: 	table_function.named_parameters["columns"] = LogicalType::ANY;
213: 	table_function.named_parameters["header"] = LogicalType::BOOLEAN;
214: 	table_function.named_parameters["auto_detect"] = LogicalType::BOOLEAN;
215: 	table_function.named_parameters["sample_size"] = LogicalType::BIGINT;
216: 	table_function.named_parameters["sample_chunk_size"] = LogicalType::BIGINT;
217: 	table_function.named_parameters["sample_chunks"] = LogicalType::BIGINT;
218: 	table_function.named_parameters["all_varchar"] = LogicalType::BOOLEAN;
219: 	table_function.named_parameters["dateformat"] = LogicalType::VARCHAR;
220: 	table_function.named_parameters["timestampformat"] = LogicalType::VARCHAR;
221: 	table_function.named_parameters["normalize_names"] = LogicalType::BOOLEAN;
222: 	table_function.named_parameters["compression"] = LogicalType::VARCHAR;
223: 	table_function.named_parameters["filename"] = LogicalType::BOOLEAN;
224: 	table_function.named_parameters["skip"] = LogicalType::BIGINT;
225: }
226: 
227: int CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p) {
228: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
229: 	if (bind_data.file_size == 0) {
230: 		return 100;
231: 	}
232: 	auto percentage = bind_data.bytes_read * 100 / bind_data.file_size;
233: 	return percentage;
234: }
235: 
236: TableFunction ReadCSVTableFunction::GetFunction() {
237: 	TableFunction read_csv("read_csv", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVBind, ReadCSVInit);
238: 	read_csv.table_scan_progress = CSVReaderProgress;
239: 	ReadCSVAddNamedParameters(read_csv);
240: 	return read_csv;
241: }
242: 
243: void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {
244: 	set.AddFunction(ReadCSVTableFunction::GetFunction());
245: 
246: 	TableFunction read_csv_auto("read_csv_auto", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVAutoBind, ReadCSVInit);
247: 	read_csv_auto.table_scan_progress = CSVReaderProgress;
248: 	ReadCSVAddNamedParameters(read_csv_auto);
249: 	set.AddFunction(read_csv_auto);
250: }
251: 
252: unique_ptr<TableFunctionRef> ReadCSVReplacement(const string &table_name, void *data) {
253: 	auto lower_name = StringUtil::Lower(table_name);
254: 	// remove any compression
255: 	if (StringUtil::EndsWith(lower_name, ".gz")) {
256: 		lower_name = lower_name.substr(0, lower_name.size() - 3);
257: 	} else if (StringUtil::EndsWith(lower_name, ".zst")) {
258: 		lower_name = lower_name.substr(0, lower_name.size() - 4);
259: 	}
260: 	if (!StringUtil::EndsWith(lower_name, ".csv") && !StringUtil::EndsWith(lower_name, ".tsv")) {
261: 		return nullptr;
262: 	}
263: 	auto table_function = make_unique<TableFunctionRef>();
264: 	vector<unique_ptr<ParsedExpression>> children;
265: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
266: 	table_function->function = make_unique<FunctionExpression>("read_csv_auto", move(children));
267: 	return table_function;
268: }
269: 
270: void BuiltinFunctions::RegisterReadFunctions() {
271: 	CSVCopyFunction::RegisterFunction(*this);
272: 	ReadCSVTableFunction::RegisterFunction(*this);
273: 
274: 	auto &config = DBConfig::GetConfig(context);
275: 	config.replacement_scans.emplace_back(ReadCSVReplacement);
276: }
277: 
278: } // namespace duckdb
[end of src/function/table/read_csv.cpp]
[start of src/include/duckdb/common/exception.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/exception.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/assert.hpp"
12: #include "duckdb/common/common.hpp"
13: #include "duckdb/common/exception_format_value.hpp"
14: #include "duckdb/common/vector.hpp"
15: 
16: #include <stdexcept>
17: 
18: namespace duckdb {
19: enum class PhysicalType : uint8_t;
20: struct LogicalType;
21: struct hugeint_t;
22: 
23: inline void assert_restrict_function(void *left_start, void *left_end, void *right_start, void *right_end,
24:                                      const char *fname, int linenr) {
25: 	// assert that the two pointers do not overlap
26: #ifdef DEBUG
27: 	if (!(left_end <= right_start || right_end <= left_start)) {
28: 		printf("ASSERT RESTRICT FAILED: %s:%d\n", fname, linenr);
29: 		D_ASSERT(0);
30: 	}
31: #endif
32: }
33: 
34: #define ASSERT_RESTRICT(left_start, left_end, right_start, right_end)                                                  \
35: 	assert_restrict_function(left_start, left_end, right_start, right_end, __FILE__, __LINE__)
36: 
37: //===--------------------------------------------------------------------===//
38: // Exception Types
39: //===--------------------------------------------------------------------===//
40: 
41: enum class ExceptionType {
42: 	INVALID = 0,          // invalid type
43: 	OUT_OF_RANGE = 1,     // value out of range error
44: 	CONVERSION = 2,       // conversion/casting error
45: 	UNKNOWN_TYPE = 3,     // unknown type
46: 	DECIMAL = 4,          // decimal related
47: 	MISMATCH_TYPE = 5,    // type mismatch
48: 	DIVIDE_BY_ZERO = 6,   // divide by 0
49: 	OBJECT_SIZE = 7,      // object size exceeded
50: 	INVALID_TYPE = 8,     // incompatible for operation
51: 	SERIALIZATION = 9,    // serialization
52: 	TRANSACTION = 10,     // transaction management
53: 	NOT_IMPLEMENTED = 11, // method not implemented
54: 	EXPRESSION = 12,      // expression parsing
55: 	CATALOG = 13,         // catalog related
56: 	PARSER = 14,          // parser related
57: 	PLANNER = 15,         // planner related
58: 	SCHEDULER = 16,       // scheduler related
59: 	EXECUTOR = 17,        // executor related
60: 	CONSTRAINT = 18,      // constraint related
61: 	INDEX = 19,           // index related
62: 	STAT = 20,            // stat related
63: 	CONNECTION = 21,      // connection related
64: 	SYNTAX = 22,          // syntax related
65: 	SETTINGS = 23,        // settings related
66: 	BINDER = 24,          // binder related
67: 	NETWORK = 25,         // network related
68: 	OPTIMIZER = 26,       // optimizer related
69: 	NULL_POINTER = 27,    // nullptr exception
70: 	IO = 28,              // IO exception
71: 	INTERRUPT = 29,       // interrupt
72: 	FATAL = 30, // Fatal exception: fatal exceptions are non-recoverable, and render the entire DB in an unusable state
73: 	INTERNAL =
74: 	    31, // Internal exception: exception that indicates something went wrong internally (i.e. bug in the code base)
75: 	INVALID_INPUT = 32, // Input or arguments error
76: 	OUT_OF_MEMORY = 33  // out of memory
77: };
78: 
79: class Exception : public std::exception {
80: public:
81: 	DUCKDB_API explicit Exception(const string &msg);
82: 	DUCKDB_API Exception(ExceptionType exception_type, const string &message);
83: 
84: 	ExceptionType type;
85: 
86: public:
87: 	DUCKDB_API const char *what() const noexcept override;
88: 
89: 	DUCKDB_API string ExceptionTypeToString(ExceptionType type);
90: 
91: 	template <typename... Args>
92: 	static string ConstructMessage(const string &msg, Args... params) {
93: 		vector<ExceptionFormatValue> values;
94: 		return ConstructMessageRecursive(msg, values, params...);
95: 	}
96: 
97: 	DUCKDB_API static string ConstructMessageRecursive(const string &msg, vector<ExceptionFormatValue> &values);
98: 
99: 	template <class T, typename... Args>
100: 	static string ConstructMessageRecursive(const string &msg, vector<ExceptionFormatValue> &values, T param,
101: 	                                        Args... params) {
102: 		values.push_back(ExceptionFormatValue::CreateFormatValue<T>(param));
103: 		return ConstructMessageRecursive(msg, values, params...);
104: 	}
105: 
106: 	DUCKDB_API static bool UncaughtException();
107: 
108: private:
109: 	string exception_message_;
110: };
111: 
112: //===--------------------------------------------------------------------===//
113: // Exception derived classes
114: //===--------------------------------------------------------------------===//
115: 
116: //! Exceptions that are StandardExceptions do NOT invalidate the current transaction when thrown
117: class StandardException : public Exception {
118: public:
119: 	DUCKDB_API StandardException(ExceptionType exception_type, const string &message);
120: };
121: 
122: class CatalogException : public StandardException {
123: public:
124: 	DUCKDB_API explicit CatalogException(const string &msg);
125: 
126: 	template <typename... Args>
127: 	explicit CatalogException(const string &msg, Args... params) : CatalogException(ConstructMessage(msg, params...)) {
128: 	}
129: };
130: 
131: class ParserException : public StandardException {
132: public:
133: 	DUCKDB_API explicit ParserException(const string &msg);
134: 
135: 	template <typename... Args>
136: 	explicit ParserException(const string &msg, Args... params) : ParserException(ConstructMessage(msg, params...)) {
137: 	}
138: };
139: 
140: class BinderException : public StandardException {
141: public:
142: 	DUCKDB_API explicit BinderException(const string &msg);
143: 
144: 	template <typename... Args>
145: 	explicit BinderException(const string &msg, Args... params) : BinderException(ConstructMessage(msg, params...)) {
146: 	}
147: };
148: 
149: class ConversionException : public Exception {
150: public:
151: 	DUCKDB_API explicit ConversionException(const string &msg);
152: 
153: 	template <typename... Args>
154: 	explicit ConversionException(const string &msg, Args... params)
155: 	    : ConversionException(ConstructMessage(msg, params...)) {
156: 	}
157: };
158: 
159: class TransactionException : public Exception {
160: public:
161: 	DUCKDB_API explicit TransactionException(const string &msg);
162: 
163: 	template <typename... Args>
164: 	explicit TransactionException(const string &msg, Args... params)
165: 	    : TransactionException(ConstructMessage(msg, params...)) {
166: 	}
167: };
168: 
169: class NotImplementedException : public Exception {
170: public:
171: 	DUCKDB_API explicit NotImplementedException(const string &msg);
172: 
173: 	template <typename... Args>
174: 	explicit NotImplementedException(const string &msg, Args... params)
175: 	    : NotImplementedException(ConstructMessage(msg, params...)) {
176: 	}
177: };
178: 
179: class OutOfRangeException : public Exception {
180: public:
181: 	DUCKDB_API explicit OutOfRangeException(const string &msg);
182: 
183: 	template <typename... Args>
184: 	explicit OutOfRangeException(const string &msg, Args... params)
185: 	    : OutOfRangeException(ConstructMessage(msg, params...)) {
186: 	}
187: };
188: 
189: class OutOfMemoryException : public Exception {
190: public:
191: 	DUCKDB_API explicit OutOfMemoryException(const string &msg);
192: 
193: 	template <typename... Args>
194: 	explicit OutOfMemoryException(const string &msg, Args... params)
195: 	    : OutOfMemoryException(ConstructMessage(msg, params...)) {
196: 	}
197: };
198: 
199: class SyntaxException : public Exception {
200: public:
201: 	DUCKDB_API explicit SyntaxException(const string &msg);
202: 
203: 	template <typename... Args>
204: 	explicit SyntaxException(const string &msg, Args... params) : SyntaxException(ConstructMessage(msg, params...)) {
205: 	}
206: };
207: 
208: class ConstraintException : public Exception {
209: public:
210: 	DUCKDB_API explicit ConstraintException(const string &msg);
211: 
212: 	template <typename... Args>
213: 	explicit ConstraintException(const string &msg, Args... params)
214: 	    : ConstraintException(ConstructMessage(msg, params...)) {
215: 	}
216: };
217: 
218: class IOException : public Exception {
219: public:
220: 	DUCKDB_API explicit IOException(const string &msg);
221: 
222: 	template <typename... Args>
223: 	explicit IOException(const string &msg, Args... params) : IOException(ConstructMessage(msg, params...)) {
224: 	}
225: };
226: 
227: class SerializationException : public Exception {
228: public:
229: 	DUCKDB_API explicit SerializationException(const string &msg);
230: 
231: 	template <typename... Args>
232: 	explicit SerializationException(const string &msg, Args... params)
233: 	    : SerializationException(ConstructMessage(msg, params...)) {
234: 	}
235: };
236: 
237: class SequenceException : public Exception {
238: public:
239: 	DUCKDB_API explicit SequenceException(const string &msg);
240: 
241: 	template <typename... Args>
242: 	explicit SequenceException(const string &msg, Args... params)
243: 	    : SequenceException(ConstructMessage(msg, params...)) {
244: 	}
245: };
246: 
247: class InterruptException : public Exception {
248: public:
249: 	DUCKDB_API InterruptException();
250: };
251: 
252: class FatalException : public Exception {
253: public:
254: 	DUCKDB_API explicit FatalException(const string &msg);
255: 
256: 	template <typename... Args>
257: 	explicit FatalException(const string &msg, Args... params) : FatalException(ConstructMessage(msg, params...)) {
258: 	}
259: };
260: 
261: class InternalException : public Exception {
262: public:
263: 	DUCKDB_API explicit InternalException(const string &msg);
264: 
265: 	template <typename... Args>
266: 	explicit InternalException(const string &msg, Args... params)
267: 	    : InternalException(ConstructMessage(msg, params...)) {
268: 	}
269: };
270: 
271: class InvalidInputException : public Exception {
272: public:
273: 	DUCKDB_API explicit InvalidInputException(const string &msg);
274: 
275: 	template <typename... Args>
276: 	explicit InvalidInputException(const string &msg, Args... params)
277: 	    : InvalidInputException(ConstructMessage(msg, params...)) {
278: 	}
279: };
280: 
281: class CastException : public Exception {
282: public:
283: 	DUCKDB_API CastException(const PhysicalType origType, const PhysicalType newType);
284: 	DUCKDB_API CastException(const LogicalType &origType, const LogicalType &newType);
285: };
286: 
287: class InvalidTypeException : public Exception {
288: public:
289: 	DUCKDB_API InvalidTypeException(PhysicalType type, const string &msg);
290: 	DUCKDB_API InvalidTypeException(const LogicalType &type, const string &msg);
291: };
292: 
293: class TypeMismatchException : public Exception {
294: public:
295: 	DUCKDB_API TypeMismatchException(const PhysicalType type_1, const PhysicalType type_2, const string &msg);
296: 	DUCKDB_API TypeMismatchException(const LogicalType &type_1, const LogicalType &type_2, const string &msg);
297: };
298: 
299: class ValueOutOfRangeException : public Exception {
300: public:
301: 	DUCKDB_API ValueOutOfRangeException(const int64_t value, const PhysicalType origType, const PhysicalType newType);
302: 	DUCKDB_API ValueOutOfRangeException(const hugeint_t value, const PhysicalType origType, const PhysicalType newType);
303: 	DUCKDB_API ValueOutOfRangeException(const double value, const PhysicalType origType, const PhysicalType newType);
304: 	DUCKDB_API ValueOutOfRangeException(const PhysicalType varType, const idx_t length);
305: };
306: 
307: } // namespace duckdb
[end of src/include/duckdb/common/exception.hpp]
[start of src/main/extension/extension_install.cpp]
1: #include "duckdb/main/extension_helper.hpp"
2: #include "duckdb/common/gzip_file_system.hpp"
3: 
4: #include "httplib.hpp"
5: #include "duckdb/common/windows_undefs.hpp"
6: 
7: #include <fstream>
8: 
9: namespace duckdb {
10: 
11: const vector<string> ExtensionHelper::PATH_COMPONENTS = {".duckdb", "extensions", DuckDB::SourceID(),
12:                                                          DuckDB::Platform()};
13: 
14: //===--------------------------------------------------------------------===//
15: // Install Extension
16: //===--------------------------------------------------------------------===//
17: void ExtensionHelper::InstallExtension(DatabaseInstance &db, const string &extension, bool force_install) {
18: 	auto &config = DBConfig::GetConfig(db);
19: 	if (!config.enable_external_access) {
20: 		throw Exception("Installing extensions is disabled");
21: 	}
22: 	auto &fs = FileSystem::GetFileSystem(db);
23: 
24: 	string local_path = fs.GetHomeDirectory();
25: 	if (!fs.DirectoryExists(local_path)) {
26: 		throw InternalException("Can't find the home directory at " + local_path);
27: 	}
28: 	for (auto &path_ele : PATH_COMPONENTS) {
29: 		local_path = fs.JoinPath(local_path, path_ele);
30: 		if (!fs.DirectoryExists(local_path)) {
31: 			fs.CreateDirectory(local_path);
32: 		}
33: 	}
34: 
35: 	auto extension_name = fs.ExtractBaseName(extension);
36: 
37: 	string local_extension_path = fs.JoinPath(local_path, extension_name + ".duckdb_extension");
38: 	if (fs.FileExists(local_extension_path) && !force_install) {
39: 		return;
40: 	}
41: 
42: 	auto is_http_url = StringUtil::Contains(extension, "http://");
43: 	if (fs.FileExists(extension)) {
44: 		std::ifstream in(extension, std::ios::binary);
45: 		if (in.bad()) {
46: 			throw IOException("Failed to read extension from \"%s\"", extension);
47: 		}
48: 		std::ofstream out(local_extension_path, std::ios::binary);
49: 		out << in.rdbuf();
50: 		if (out.bad()) {
51: 			throw IOException("Failed to write extension to \"%s\"", local_extension_path);
52: 		}
53: 		in.close();
54: 		out.close();
55: 		return;
56: 	} else if (StringUtil::Contains(extension, "/") && !is_http_url) {
57: 		throw IOException("Failed to read extension from \"%s\": no such file", extension);
58: 	}
59: 
60: 	string url_template = "http://extensions.duckdb.org/${REVISION}/${PLATFORM}/${NAME}.duckdb_extension.gz";
61: 
62: 	if (is_http_url) {
63: 		url_template = extension;
64: 		extension_name = "";
65: 	}
66: 
67: 	auto url = StringUtil::Replace(url_template, "${REVISION}", DuckDB::SourceID());
68: 	url = StringUtil::Replace(url, "${PLATFORM}", DuckDB::Platform());
69: 	url = StringUtil::Replace(url, "${NAME}", extension_name);
70: 
71: 	string no_http = StringUtil::Replace(url, "http://", "");
72: 
73: 	idx_t next = no_http.find('/', 0);
74: 	if (next == string::npos) {
75: 		throw IOException("No slash in URL template");
76: 	}
77: 
78: 	// Push the substring [last, next) on to splits
79: 	auto hostname_without_http = no_http.substr(0, next);
80: 	auto url_local_part = no_http.substr(next);
81: 
82: 	auto url_base = "http://" + hostname_without_http;
83: 	duckdb_httplib::Client cli(url_base.c_str());
84: 
85: 	duckdb_httplib::Headers headers = {{"User-Agent", StringUtil::Format("DuckDB %s %s %s", DuckDB::LibraryVersion(),
86: 	                                                                     DuckDB::SourceID(), DuckDB::Platform())}};
87: 
88: 	auto res = cli.Get(url_local_part.c_str(), headers);
89: 	if (!res || res->status != 200) {
90: 		throw IOException("Failed to download extension %s%s", url_base, url_local_part);
91: 	}
92: 	auto decompressed_body = GZipFileSystem::UncompressGZIPString(res->body);
93: 	std::ofstream out(local_extension_path, std::ios::binary);
94: 	out.write(decompressed_body.data(), decompressed_body.size());
95: 	if (out.bad()) {
96: 		throw IOException("Failed to write extension to %s", local_extension_path);
97: 	}
98: }
99: 
100: } // namespace duckdb
[end of src/main/extension/extension_install.cpp]
[start of src/main/extension/extension_load.cpp]
1: #include "duckdb/main/extension_helper.hpp"
2: #include "duckdb/common/dl.hpp"
3: 
4: namespace duckdb {
5: 
6: //===--------------------------------------------------------------------===//
7: // Load External Extension
8: //===--------------------------------------------------------------------===//
9: typedef void (*ext_init_fun_t)(DatabaseInstance &);
10: typedef const char *(*ext_version_fun_t)(void);
11: 
12: template <class T>
13: static T LoadFunctionFromDLL(void *dll, const string &function_name, const string &filename) {
14: 	auto function = dlsym(dll, function_name.c_str());
15: 	if (!function) {
16: 		throw IOException("File \"%s\" did not contain function \"%s\"", filename, function_name);
17: 	}
18: 	return (T)function;
19: }
20: 
21: void ExtensionHelper::LoadExternalExtension(DatabaseInstance &db, const string &extension) {
22: 	auto &config = DBConfig::GetConfig(db);
23: 	if (!config.enable_external_access) {
24: 		throw Exception("Loading external extensions is disabled");
25: 	}
26: 	auto &fs = FileSystem::GetFileSystem(db);
27: 	auto filename = fs.ConvertSeparators(extension);
28: 
29: 	// shorthand case
30: 	if (!StringUtil::Contains(extension, ".") && !StringUtil::Contains(extension, fs.PathSeparator())) {
31: 		string local_path = fs.GetHomeDirectory();
32: 		for (auto &path_ele : PATH_COMPONENTS) {
33: 			local_path = fs.JoinPath(local_path, path_ele);
34: 		}
35: 		filename = fs.JoinPath(local_path, extension + ".duckdb_extension");
36: 	}
37: 
38: 	if (!fs.FileExists(filename)) {
39: 		throw IOException("File \"%s\" not found", filename);
40: 	}
41: 	auto lib_hdl = dlopen(filename.c_str(), RTLD_LAZY | RTLD_LOCAL);
42: 	if (!lib_hdl) {
43: 		throw IOException("File \"%s\" could not be loaded", filename);
44: 	}
45: 
46: 	auto basename = fs.ExtractBaseName(filename);
47: 	auto init_fun_name = basename + "_init";
48: 	auto version_fun_name = basename + "_version";
49: 
50: 	ext_init_fun_t init_fun;
51: 	ext_version_fun_t version_fun;
52: 
53: 	init_fun = LoadFunctionFromDLL<ext_init_fun_t>(lib_hdl, init_fun_name, filename);
54: 	version_fun = LoadFunctionFromDLL<ext_version_fun_t>(lib_hdl, version_fun_name, filename);
55: 
56: 	auto extension_version = std::string((*version_fun)());
57: 	auto engine_version = DuckDB::LibraryVersion();
58: 	if (extension_version != engine_version) {
59: 		throw InvalidInputException("Extension \"%s\" version (%s) does not match DuckDB version (%s)", filename,
60: 		                            extension_version, engine_version);
61: 	}
62: 
63: 	try {
64: 		(*init_fun)(db);
65: 	} catch (std::exception &e) {
66: 		throw InvalidInputException("Initialization function \"%s\" from file \"%s\" threw an exception: \"%s\"",
67: 		                            init_fun_name, filename, e.what());
68: 	}
69: }
70: 
71: } // namespace duckdb
[end of src/main/extension/extension_load.cpp]
[start of src/main/settings/settings.cpp]
1: #include "duckdb/main/settings.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/main/config.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/catalog/catalog_search_path.hpp"
6: #include "duckdb/storage/buffer_manager.hpp"
7: #include "duckdb/parallel/task_scheduler.hpp"
8: #include "duckdb/planner/expression_binder.hpp"
9: #include "duckdb/main/query_profiler.hpp"
10: #include "duckdb/storage/storage_manager.hpp"
11: 
12: namespace duckdb {
13: 
14: //===--------------------------------------------------------------------===//
15: // Access Mode
16: //===--------------------------------------------------------------------===//
17: void AccessModeSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
18: 	auto parameter = StringUtil::Lower(input.ToString());
19: 	if (parameter == "automatic") {
20: 		config.access_mode = AccessMode::AUTOMATIC;
21: 	} else if (parameter == "read_only") {
22: 		config.access_mode = AccessMode::READ_ONLY;
23: 	} else if (parameter == "read_write") {
24: 		config.access_mode = AccessMode::READ_WRITE;
25: 	} else {
26: 		throw InvalidInputException(
27: 		    "Unrecognized parameter for option ACCESS_MODE \"%s\". Expected READ_ONLY or READ_WRITE.", parameter);
28: 	}
29: }
30: 
31: Value AccessModeSetting::GetSetting(ClientContext &context) {
32: 	auto &config = DBConfig::GetConfig(context);
33: 	switch (config.access_mode) {
34: 	case AccessMode::AUTOMATIC:
35: 		return "automatic";
36: 	case AccessMode::READ_ONLY:
37: 		return "read_only";
38: 	case AccessMode::READ_WRITE:
39: 		return "read_write";
40: 	default:
41: 		throw InternalException("Unknown access mode setting");
42: 	}
43: }
44: 
45: //===--------------------------------------------------------------------===//
46: // Checkpoint Threshold
47: //===--------------------------------------------------------------------===//
48: void CheckpointThresholdSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
49: 	idx_t new_limit = DBConfig::ParseMemoryLimit(input.ToString());
50: 	config.checkpoint_wal_size = new_limit;
51: }
52: 
53: Value CheckpointThresholdSetting::GetSetting(ClientContext &context) {
54: 	auto &config = DBConfig::GetConfig(context);
55: 	return Value(StringUtil::BytesToHumanReadableString(config.checkpoint_wal_size));
56: }
57: 
58: //===--------------------------------------------------------------------===//
59: // Debug Checkpoint Abort
60: //===--------------------------------------------------------------------===//
61: void DebugCheckpointAbort::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
62: 	auto checkpoint_abort = StringUtil::Lower(input.ToString());
63: 	if (checkpoint_abort == "none") {
64: 		config.checkpoint_abort = CheckpointAbort::NO_ABORT;
65: 	} else if (checkpoint_abort == "before_truncate") {
66: 		config.checkpoint_abort = CheckpointAbort::DEBUG_ABORT_BEFORE_TRUNCATE;
67: 	} else if (checkpoint_abort == "before_header") {
68: 		config.checkpoint_abort = CheckpointAbort::DEBUG_ABORT_BEFORE_HEADER;
69: 	} else if (checkpoint_abort == "after_free_list_write") {
70: 		config.checkpoint_abort = CheckpointAbort::DEBUG_ABORT_AFTER_FREE_LIST_WRITE;
71: 	} else {
72: 		throw ParserException(
73: 		    "Unrecognized option for PRAGMA debug_checkpoint_abort, expected none, before_truncate or before_header");
74: 	}
75: }
76: 
77: Value DebugCheckpointAbort::GetSetting(ClientContext &context) {
78: 	return Value();
79: }
80: 
81: //===--------------------------------------------------------------------===//
82: // Debug Force External
83: //===--------------------------------------------------------------------===//
84: void DebugForceExternal::SetLocal(ClientContext &context, const Value &input) {
85: 	ClientConfig::GetConfig(context).force_external = input.GetValue<bool>();
86: }
87: 
88: Value DebugForceExternal::GetSetting(ClientContext &context) {
89: 	return Value::BOOLEAN(ClientConfig::GetConfig(context).force_external);
90: }
91: 
92: //===--------------------------------------------------------------------===//
93: // Debug Many Free List blocks
94: //===--------------------------------------------------------------------===//
95: void DebugManyFreeListBlocks::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
96: 	config.debug_many_free_list_blocks = input.GetValue<bool>();
97: }
98: 
99: Value DebugManyFreeListBlocks::GetSetting(ClientContext &context) {
100: 	auto &config = DBConfig::GetConfig(context);
101: 	return Value::BOOLEAN(config.debug_many_free_list_blocks);
102: }
103: 
104: //===--------------------------------------------------------------------===//
105: // Debug Window Mode
106: //===--------------------------------------------------------------------===//
107: void DebugWindowMode::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
108: 	auto param = StringUtil::Lower(input.ToString());
109: 	if (param == "window") {
110: 		config.window_mode = WindowAggregationMode::WINDOW;
111: 	} else if (param == "combine") {
112: 		config.window_mode = WindowAggregationMode::COMBINE;
113: 	} else if (param == "separate") {
114: 		config.window_mode = WindowAggregationMode::SEPARATE;
115: 	} else {
116: 		throw ParserException("Unrecognized option for PRAGMA debug_window_mode, expected window, combine or separate");
117: 	}
118: }
119: 
120: Value DebugWindowMode::GetSetting(ClientContext &context) {
121: 	return Value();
122: }
123: 
124: //===--------------------------------------------------------------------===//
125: // Default Collation
126: //===--------------------------------------------------------------------===//
127: void DefaultCollationSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
128: 	auto parameter = StringUtil::Lower(input.ToString());
129: 	config.collation = parameter;
130: }
131: 
132: void DefaultCollationSetting::SetLocal(ClientContext &context, const Value &input) {
133: 	auto parameter = input.ToString();
134: 	// bind the collation to verify that it exists
135: 	ExpressionBinder::TestCollation(context, parameter);
136: 	auto &config = DBConfig::GetConfig(context);
137: 	config.collation = parameter;
138: }
139: 
140: Value DefaultCollationSetting::GetSetting(ClientContext &context) {
141: 	auto &config = DBConfig::GetConfig(context);
142: 	return Value(config.collation);
143: }
144: 
145: //===--------------------------------------------------------------------===//
146: // Default Order
147: //===--------------------------------------------------------------------===//
148: void DefaultOrderSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
149: 	auto parameter = StringUtil::Lower(input.ToString());
150: 	if (parameter == "ascending" || parameter == "asc") {
151: 		config.default_order_type = OrderType::ASCENDING;
152: 	} else if (parameter == "descending" || parameter == "desc") {
153: 		config.default_order_type = OrderType::DESCENDING;
154: 	} else {
155: 		throw InvalidInputException("Unrecognized parameter for option DEFAULT_ORDER \"%s\". Expected ASC or DESC.",
156: 		                            parameter);
157: 	}
158: }
159: 
160: Value DefaultOrderSetting::GetSetting(ClientContext &context) {
161: 	auto &config = DBConfig::GetConfig(context);
162: 	switch (config.default_order_type) {
163: 	case OrderType::ASCENDING:
164: 		return "asc";
165: 	case OrderType::DESCENDING:
166: 		return "desc";
167: 	default:
168: 		throw InternalException("Unknown order type setting");
169: 	}
170: }
171: 
172: //===--------------------------------------------------------------------===//
173: // Default Null Order
174: //===--------------------------------------------------------------------===//
175: void DefaultNullOrderSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
176: 	auto parameter = StringUtil::Lower(input.ToString());
177: 
178: 	if (parameter == "nulls_first" || parameter == "nulls first" || parameter == "null first" || parameter == "first") {
179: 		config.default_null_order = OrderByNullType::NULLS_FIRST;
180: 	} else if (parameter == "nulls_last" || parameter == "nulls last" || parameter == "null last" ||
181: 	           parameter == "last") {
182: 		config.default_null_order = OrderByNullType::NULLS_LAST;
183: 	} else {
184: 		throw ParserException(
185: 		    "Unrecognized parameter for option NULL_ORDER \"%s\", expected either NULLS FIRST or NULLS LAST",
186: 		    parameter);
187: 	}
188: }
189: 
190: Value DefaultNullOrderSetting::GetSetting(ClientContext &context) {
191: 	auto &config = DBConfig::GetConfig(context);
192: 	switch (config.default_null_order) {
193: 	case OrderByNullType::NULLS_FIRST:
194: 		return "nulls_first";
195: 	case OrderByNullType::NULLS_LAST:
196: 		return "nulls_last";
197: 	default:
198: 		throw InternalException("Unknown null order setting");
199: 	}
200: }
201: 
202: //===--------------------------------------------------------------------===//
203: // Disabled Optimizer
204: //===--------------------------------------------------------------------===//
205: void DisabledOptimizersSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
206: 	auto list = StringUtil::Split(input.ToString(), ",");
207: 	set<OptimizerType> disabled_optimizers;
208: 	for (auto &entry : list) {
209: 		auto param = StringUtil::Lower(entry);
210: 		StringUtil::Trim(param);
211: 		if (param.empty()) {
212: 			continue;
213: 		}
214: 		disabled_optimizers.insert(OptimizerTypeFromString(param));
215: 	}
216: 	config.disabled_optimizers = move(disabled_optimizers);
217: }
218: 
219: Value DisabledOptimizersSetting::GetSetting(ClientContext &context) {
220: 	auto &config = DBConfig::GetConfig(context);
221: 	string result;
222: 	for (auto &optimizer : config.disabled_optimizers) {
223: 		if (!result.empty()) {
224: 			result += ",";
225: 		}
226: 		result += OptimizerTypeToString(optimizer);
227: 	}
228: 	return Value(result);
229: }
230: 
231: //===--------------------------------------------------------------------===//
232: // Enable External Access
233: //===--------------------------------------------------------------------===//
234: void EnableExternalAccessSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
235: 	if (db) {
236: 		throw InvalidInputException("Cannot change enable_external_access setting while database is running");
237: 	}
238: 	config.enable_external_access = input.GetValue<bool>();
239: }
240: 
241: Value EnableExternalAccessSetting::GetSetting(ClientContext &context) {
242: 	auto &config = DBConfig::GetConfig(context);
243: 	return Value::BOOLEAN(config.enable_external_access);
244: }
245: 
246: //===--------------------------------------------------------------------===//
247: // Enable Object Cache
248: //===--------------------------------------------------------------------===//
249: void EnableObjectCacheSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
250: 	config.object_cache_enable = input.GetValue<bool>();
251: }
252: 
253: Value EnableObjectCacheSetting::GetSetting(ClientContext &context) {
254: 	auto &config = DBConfig::GetConfig(context);
255: 	return Value::BOOLEAN(config.object_cache_enable);
256: }
257: 
258: //===--------------------------------------------------------------------===//
259: // Enable Profiling
260: //===--------------------------------------------------------------------===//
261: void EnableProfilingSetting::SetLocal(ClientContext &context, const Value &input) {
262: 	auto parameter = StringUtil::Lower(input.ToString());
263: 	if (parameter == "json") {
264: 		context.profiler->automatic_print_format = ProfilerPrintFormat::JSON;
265: 	} else if (parameter == "query_tree") {
266: 		context.profiler->automatic_print_format = ProfilerPrintFormat::QUERY_TREE;
267: 	} else if (parameter == "query_tree_optimizer") {
268: 		context.profiler->automatic_print_format = ProfilerPrintFormat::QUERY_TREE_OPTIMIZER;
269: 	} else {
270: 		throw ParserException(
271: 		    "Unrecognized print format %s, supported formats: [json, query_tree, query_tree_optimizer]", parameter);
272: 	}
273: 	context.profiler->Enable();
274: }
275: 
276: Value EnableProfilingSetting::GetSetting(ClientContext &context) {
277: 	if (!context.profiler->IsEnabled()) {
278: 		return Value();
279: 	}
280: 	switch (context.profiler->automatic_print_format) {
281: 	case ProfilerPrintFormat::NONE:
282: 		return Value("none");
283: 	case ProfilerPrintFormat::JSON:
284: 		return Value("json");
285: 	case ProfilerPrintFormat::QUERY_TREE:
286: 		return Value("query_tree");
287: 	case ProfilerPrintFormat::QUERY_TREE_OPTIMIZER:
288: 		return Value("query_tree_optimizer");
289: 	default:
290: 		throw InternalException("Unsupported profiler print format");
291: 	}
292: }
293: 
294: //===--------------------------------------------------------------------===//
295: // Enable Progress Bar
296: //===--------------------------------------------------------------------===//
297: void EnableProgressBarSetting::SetLocal(ClientContext &context, const Value &input) {
298: 	ClientConfig::GetConfig(context).enable_progress_bar = input.GetValue<bool>();
299: }
300: 
301: Value EnableProgressBarSetting::GetSetting(ClientContext &context) {
302: 	return Value::BOOLEAN(ClientConfig::GetConfig(context).enable_progress_bar);
303: }
304: 
305: //===--------------------------------------------------------------------===//
306: // Explain Output
307: //===--------------------------------------------------------------------===//
308: void ExplainOutputSetting::SetLocal(ClientContext &context, const Value &input) {
309: 	auto parameter = StringUtil::Lower(input.ToString());
310: 	if (parameter == "all") {
311: 		ClientConfig::GetConfig(context).explain_output_type = ExplainOutputType::ALL;
312: 	} else if (parameter == "optimized_only") {
313: 		ClientConfig::GetConfig(context).explain_output_type = ExplainOutputType::OPTIMIZED_ONLY;
314: 	} else if (parameter == "physical_only") {
315: 		ClientConfig::GetConfig(context).explain_output_type = ExplainOutputType::PHYSICAL_ONLY;
316: 	} else {
317: 		throw ParserException("Unrecognized output type \"%s\", expected either ALL, OPTIMIZED_ONLY or PHYSICAL_ONLY",
318: 		                      parameter);
319: 	}
320: }
321: 
322: Value ExplainOutputSetting::GetSetting(ClientContext &context) {
323: 	switch (ClientConfig::GetConfig(context).explain_output_type) {
324: 	case ExplainOutputType::ALL:
325: 		return "all";
326: 	case ExplainOutputType::OPTIMIZED_ONLY:
327: 		return "optimized_only";
328: 	case ExplainOutputType::PHYSICAL_ONLY:
329: 		return "physical_only";
330: 	default:
331: 		throw InternalException("Unrecognized explain output type");
332: 	}
333: }
334: 
335: //===--------------------------------------------------------------------===//
336: // Force Compression
337: //===--------------------------------------------------------------------===//
338: void ForceCompressionSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
339: 	auto compression = StringUtil::Lower(input.ToString());
340: 	if (compression == "none") {
341: 		config.force_compression = CompressionType::COMPRESSION_AUTO;
342: 	} else {
343: 		auto compression_type = CompressionTypeFromString(compression);
344: 		if (compression_type == CompressionType::COMPRESSION_AUTO) {
345: 			throw ParserException("Unrecognized option for PRAGMA force_compression, expected none, uncompressed, rle, "
346: 			                      "dictionary, pfor, bitpacking or fsst");
347: 		}
348: 		config.force_compression = compression_type;
349: 	}
350: }
351: 
352: Value ForceCompressionSetting::GetSetting(ClientContext &context) {
353: 	return Value();
354: }
355: 
356: //===--------------------------------------------------------------------===//
357: // Log Query Path
358: //===--------------------------------------------------------------------===//
359: void LogQueryPathSetting::SetLocal(ClientContext &context, const Value &input) {
360: 	auto path = input.ToString();
361: 	if (path.empty()) {
362: 		// empty path: clean up query writer
363: 		context.log_query_writer = nullptr;
364: 	} else {
365: 		context.log_query_writer =
366: 		    make_unique<BufferedFileWriter>(FileSystem::GetFileSystem(context), path,
367: 		                                    BufferedFileWriter::DEFAULT_OPEN_FLAGS, context.file_opener.get());
368: 	}
369: }
370: 
371: Value LogQueryPathSetting::GetSetting(ClientContext &context) {
372: 	return context.log_query_writer ? Value(context.log_query_writer->path) : Value();
373: }
374: 
375: //===--------------------------------------------------------------------===//
376: // Maximum Memory
377: //===--------------------------------------------------------------------===//
378: void MaximumMemorySetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
379: 	config.maximum_memory = DBConfig::ParseMemoryLimit(input.ToString());
380: 	if (db) {
381: 		BufferManager::GetBufferManager(*db).SetLimit(config.maximum_memory);
382: 	}
383: }
384: 
385: Value MaximumMemorySetting::GetSetting(ClientContext &context) {
386: 	auto &config = DBConfig::GetConfig(context);
387: 	return Value(StringUtil::BytesToHumanReadableString(config.maximum_memory));
388: }
389: 
390: //===--------------------------------------------------------------------===//
391: // Perfect Hash Threshold
392: //===--------------------------------------------------------------------===//
393: void PerfectHashThresholdSetting::SetLocal(ClientContext &context, const Value &input) {
394: 	auto bits = input.GetValue<int32_t>();
395: 	if (bits < 0 || bits > 32) {
396: 		throw ParserException("Perfect HT threshold out of range: should be within range 0 - 32");
397: 	}
398: 	ClientConfig::GetConfig(context).perfect_ht_threshold = bits;
399: }
400: 
401: Value PerfectHashThresholdSetting::GetSetting(ClientContext &context) {
402: 	return Value::BIGINT(ClientConfig::GetConfig(context).perfect_ht_threshold);
403: }
404: 
405: //===--------------------------------------------------------------------===//
406: // Profiler History Size
407: //===--------------------------------------------------------------------===//
408: void ProfilerHistorySize::SetLocal(ClientContext &context, const Value &input) {
409: 	auto size = input.GetValue<int64_t>();
410: 	if (size <= 0) {
411: 		throw ParserException("Size should be >= 0");
412: 	}
413: 	context.query_profiler_history->SetProfilerHistorySize(size);
414: }
415: 
416: Value ProfilerHistorySize::GetSetting(ClientContext &context) {
417: 	return Value();
418: }
419: 
420: //===--------------------------------------------------------------------===//
421: // Profile Output
422: //===--------------------------------------------------------------------===//
423: void ProfileOutputSetting::SetLocal(ClientContext &context, const Value &input) {
424: 	auto parameter = input.ToString();
425: 	context.profiler->save_location = parameter;
426: }
427: 
428: Value ProfileOutputSetting::GetSetting(ClientContext &context) {
429: 	return Value(context.profiler->save_location);
430: }
431: 
432: //===--------------------------------------------------------------------===//
433: // Profiling Mode
434: //===--------------------------------------------------------------------===//
435: void ProfilingModeSetting::SetLocal(ClientContext &context, const Value &input) {
436: 	auto parameter = StringUtil::Lower(input.ToString());
437: 	if (parameter == "standard") {
438: 		context.profiler->Enable();
439: 	} else if (parameter == "detailed") {
440: 		context.profiler->DetailedEnable();
441: 	} else {
442: 		throw ParserException("Unrecognized profiling mode \"%s\", supported formats: [standard, detailed]", parameter);
443: 	}
444: }
445: 
446: Value ProfilingModeSetting::GetSetting(ClientContext &context) {
447: 	return Value(context.profiler->IsDetailedEnabled() ? "detailed" : "standard");
448: }
449: 
450: //===--------------------------------------------------------------------===//
451: // Progress Bar Time
452: //===--------------------------------------------------------------------===//
453: void ProgressBarTimeSetting::SetLocal(ClientContext &context, const Value &input) {
454: 	ClientConfig::GetConfig(context).wait_time = input.GetValue<int32_t>();
455: 	ClientConfig::GetConfig(context).enable_progress_bar = true;
456: }
457: 
458: Value ProgressBarTimeSetting::GetSetting(ClientContext &context) {
459: 	return Value::BIGINT(ClientConfig::GetConfig(context).wait_time);
460: }
461: 
462: //===--------------------------------------------------------------------===//
463: // Schema
464: //===--------------------------------------------------------------------===//
465: void SchemaSetting::SetLocal(ClientContext &context, const Value &input) {
466: 	auto parameter = input.ToString();
467: 	context.catalog_search_path->Set(parameter, true);
468: }
469: 
470: Value SchemaSetting::GetSetting(ClientContext &context) {
471: 	return SearchPathSetting::GetSetting(context);
472: }
473: 
474: //===--------------------------------------------------------------------===//
475: // Search Path
476: //===--------------------------------------------------------------------===//
477: void SearchPathSetting::SetLocal(ClientContext &context, const Value &input) {
478: 	auto parameter = input.ToString();
479: 	context.catalog_search_path->Set(parameter, false);
480: }
481: 
482: Value SearchPathSetting::GetSetting(ClientContext &context) {
483: 	return Value(StringUtil::Join(context.catalog_search_path->GetSetPaths(), ","));
484: }
485: 
486: //===--------------------------------------------------------------------===//
487: // Temp Directory
488: //===--------------------------------------------------------------------===//
489: void TempDirectorySetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
490: 	config.temporary_directory = input.ToString();
491: 	config.use_temporary_directory = !config.temporary_directory.empty();
492: 	if (db) {
493: 		auto &buffer_manager = BufferManager::GetBufferManager(*db);
494: 		buffer_manager.SetTemporaryDirectory(config.temporary_directory);
495: 	}
496: }
497: 
498: Value TempDirectorySetting::GetSetting(ClientContext &context) {
499: 	auto &buffer_manager = BufferManager::GetBufferManager(context);
500: 	return Value(buffer_manager.GetTemporaryDirectory());
501: }
502: 
503: //===--------------------------------------------------------------------===//
504: // Threads Setting
505: //===--------------------------------------------------------------------===//
506: void ThreadsSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
507: 	config.maximum_threads = input.GetValue<int64_t>();
508: 	if (db) {
509: 		TaskScheduler::GetScheduler(*db).SetThreads(config.maximum_threads);
510: 	}
511: }
512: 
513: Value ThreadsSetting::GetSetting(ClientContext &context) {
514: 	auto &config = DBConfig::GetConfig(context);
515: 	return Value::BIGINT(config.maximum_threads);
516: }
517: 
518: } // namespace duckdb
[end of src/main/settings/settings.cpp]
[start of src/planner/binder/statement/bind_copy.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/statement/copy_statement.hpp"
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/parser/statement/insert_statement.hpp"
5: #include "duckdb/planner/operator/logical_copy_to_file.hpp"
6: #include "duckdb/planner/operator/logical_get.hpp"
7: #include "duckdb/planner/operator/logical_insert.hpp"
8: #include "duckdb/catalog/catalog_entry/copy_function_catalog_entry.hpp"
9: #include "duckdb/main/client_context.hpp"
10: #include "duckdb/main/database.hpp"
11: 
12: #include "duckdb/parser/expression/columnref_expression.hpp"
13: #include "duckdb/parser/expression/star_expression.hpp"
14: #include "duckdb/parser/tableref/basetableref.hpp"
15: #include "duckdb/parser/query_node/select_node.hpp"
16: 
17: #include <algorithm>
18: 
19: namespace duckdb {
20: 
21: BoundStatement Binder::BindCopyTo(CopyStatement &stmt) {
22: 	// COPY TO a file
23: 	auto &config = DBConfig::GetConfig(context);
24: 	if (!config.enable_external_access) {
25: 		throw Exception("COPY TO is disabled by configuration");
26: 	}
27: 	BoundStatement result;
28: 	result.types = {LogicalType::BIGINT};
29: 	result.names = {"Count"};
30: 
31: 	// bind the select statement
32: 	auto select_node = Bind(*stmt.select_statement);
33: 
34: 	// lookup the format in the catalog
35: 	auto &catalog = Catalog::GetCatalog(context);
36: 	auto copy_function = catalog.GetEntry<CopyFunctionCatalogEntry>(context, DEFAULT_SCHEMA, stmt.info->format);
37: 	if (!copy_function->function.copy_to_bind) {
38: 		throw NotImplementedException("COPY TO is not supported for FORMAT \"%s\"", stmt.info->format);
39: 	}
40: 
41: 	auto function_data =
42: 	    copy_function->function.copy_to_bind(context, *stmt.info, select_node.names, select_node.types);
43: 	// now create the copy information
44: 	auto copy = make_unique<LogicalCopyToFile>(copy_function->function, move(function_data));
45: 	copy->AddChild(move(select_node.plan));
46: 
47: 	result.plan = move(copy);
48: 
49: 	return result;
50: }
51: 
52: BoundStatement Binder::BindCopyFrom(CopyStatement &stmt) {
53: 	auto &config = DBConfig::GetConfig(context);
54: 	if (!config.enable_external_access) {
55: 		throw Exception("COPY FROM is disabled by configuration");
56: 	}
57: 	BoundStatement result;
58: 	result.types = {LogicalType::BIGINT};
59: 	result.names = {"Count"};
60: 
61: 	D_ASSERT(!stmt.info->table.empty());
62: 	// COPY FROM a file
63: 	// generate an insert statement for the the to-be-inserted table
64: 	InsertStatement insert;
65: 	insert.table = stmt.info->table;
66: 	insert.schema = stmt.info->schema;
67: 	insert.columns = stmt.info->select_list;
68: 
69: 	// bind the insert statement to the base table
70: 	auto insert_statement = Bind(insert);
71: 	D_ASSERT(insert_statement.plan->type == LogicalOperatorType::LOGICAL_INSERT);
72: 
73: 	auto &bound_insert = (LogicalInsert &)*insert_statement.plan;
74: 
75: 	// lookup the format in the catalog
76: 	auto &catalog = Catalog::GetCatalog(context);
77: 	auto copy_function = catalog.GetEntry<CopyFunctionCatalogEntry>(context, DEFAULT_SCHEMA, stmt.info->format);
78: 	if (!copy_function->function.copy_from_bind) {
79: 		throw NotImplementedException("COPY FROM is not supported for FORMAT \"%s\"", stmt.info->format);
80: 	}
81: 	// lookup the table to copy into
82: 	auto table = Catalog::GetCatalog(context).GetEntry<TableCatalogEntry>(context, stmt.info->schema, stmt.info->table);
83: 	vector<string> expected_names;
84: 	if (!bound_insert.column_index_map.empty()) {
85: 		expected_names.resize(bound_insert.expected_types.size());
86: 		for (idx_t i = 0; i < table->columns.size(); i++) {
87: 			if (bound_insert.column_index_map[i] != DConstants::INVALID_INDEX) {
88: 				expected_names[bound_insert.column_index_map[i]] = table->columns[i].name;
89: 			}
90: 		}
91: 	} else {
92: 		expected_names.reserve(bound_insert.expected_types.size());
93: 		for (idx_t i = 0; i < table->columns.size(); i++) {
94: 			expected_names.push_back(table->columns[i].name);
95: 		}
96: 	}
97: 
98: 	auto function_data =
99: 	    copy_function->function.copy_from_bind(context, *stmt.info, expected_names, bound_insert.expected_types);
100: 	auto get = make_unique<LogicalGet>(0, copy_function->function.copy_from_function, move(function_data),
101: 	                                   bound_insert.expected_types, expected_names);
102: 	for (idx_t i = 0; i < bound_insert.expected_types.size(); i++) {
103: 		get->column_ids.push_back(i);
104: 	}
105: 	insert_statement.plan->children.push_back(move(get));
106: 	result.plan = move(insert_statement.plan);
107: 	return result;
108: }
109: 
110: BoundStatement Binder::Bind(CopyStatement &stmt) {
111: 	if (!stmt.info->is_from && !stmt.select_statement) {
112: 		// copy table into file without a query
113: 		// generate SELECT * FROM table;
114: 		auto ref = make_unique<BaseTableRef>();
115: 		ref->schema_name = stmt.info->schema;
116: 		ref->table_name = stmt.info->table;
117: 
118: 		auto statement = make_unique<SelectNode>();
119: 		statement->from_table = move(ref);
120: 		if (!stmt.info->select_list.empty()) {
121: 			for (auto &name : stmt.info->select_list) {
122: 				statement->select_list.push_back(make_unique<ColumnRefExpression>(name));
123: 			}
124: 		} else {
125: 			statement->select_list.push_back(make_unique<StarExpression>());
126: 		}
127: 		stmt.select_statement = move(statement);
128: 	}
129: 	this->allow_stream_result = false;
130: 	if (stmt.info->is_from) {
131: 		return BindCopyFrom(stmt);
132: 	} else {
133: 		return BindCopyTo(stmt);
134: 	}
135: }
136: 
137: } // namespace duckdb
[end of src/planner/binder/statement/bind_copy.cpp]
[start of src/planner/binder/statement/bind_export.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/statement/export_statement.hpp"
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/planner/operator/logical_export.hpp"
5: #include "duckdb/catalog/catalog_entry/copy_function_catalog_entry.hpp"
6: #include "duckdb/parser/statement/copy_statement.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/common/file_system.hpp"
10: #include "duckdb/planner/operator/logical_set_operation.hpp"
11: #include "duckdb/parser/parsed_data/exported_table_data.hpp"
12: 
13: #include "duckdb/common/string_util.hpp"
14: #include <algorithm>
15: 
16: namespace duckdb {
17: 
18: //! Sanitizes a string to have only low case chars and underscores
19: string SanitizeExportIdentifier(const string &str) {
20: 	// Copy the original string to result
21: 	string result(str);
22: 
23: 	for (idx_t i = 0; i < str.length(); ++i) {
24: 		auto c = str[i];
25: 		if (c >= 'a' && c <= 'z') {
26: 			// If it is lower case just continue
27: 			continue;
28: 		}
29: 
30: 		if (c >= 'A' && c <= 'Z') {
31: 			// To lowercase
32: 			result[i] = tolower(c);
33: 		} else {
34: 			// Substitute to underscore
35: 			result[i] = '_';
36: 		}
37: 	}
38: 
39: 	return result;
40: }
41: 
42: BoundStatement Binder::Bind(ExportStatement &stmt) {
43: 	// COPY TO a file
44: 	auto &config = DBConfig::GetConfig(context);
45: 	if (!config.enable_external_access) {
46: 		throw Exception("COPY TO is disabled by configuration");
47: 	}
48: 	BoundStatement result;
49: 	result.types = {LogicalType::BOOLEAN};
50: 	result.names = {"Success"};
51: 
52: 	// lookup the format in the catalog
53: 	auto &catalog = Catalog::GetCatalog(context);
54: 	auto copy_function = catalog.GetEntry<CopyFunctionCatalogEntry>(context, DEFAULT_SCHEMA, stmt.info->format);
55: 	if (!copy_function->function.copy_to_bind) {
56: 		throw NotImplementedException("COPY TO is not supported for FORMAT \"%s\"", stmt.info->format);
57: 	}
58: 
59: 	// gather a list of all the tables
60: 	vector<TableCatalogEntry *> tables;
61: 	auto schemas = catalog.schemas->GetEntries<SchemaCatalogEntry>(context);
62: 	for (auto &schema : schemas) {
63: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) {
64: 			if (entry->type == CatalogType::TABLE_ENTRY) {
65: 				tables.push_back((TableCatalogEntry *)entry);
66: 			}
67: 		});
68: 	}
69: 
70: 	// now generate the COPY statements for each of the tables
71: 	auto &fs = FileSystem::GetFileSystem(context);
72: 	unique_ptr<LogicalOperator> child_operator;
73: 
74: 	BoundExportData exported_tables;
75: 
76: 	idx_t id = 0; // Id for table
77: 	for (auto &table : tables) {
78: 		auto info = make_unique<CopyInfo>();
79: 		// we copy the options supplied to the EXPORT
80: 		info->format = stmt.info->format;
81: 		info->options = stmt.info->options;
82: 		// set up the file name for the COPY TO
83: 
84: 		auto exported_data = ExportedTableData();
85: 		if (table->schema->name == DEFAULT_SCHEMA) {
86: 			info->file_path =
87: 			    fs.JoinPath(stmt.info->file_path,
88: 			                StringUtil::Format("%s_%s.%s", to_string(id), SanitizeExportIdentifier(table->name),
89: 			                                   copy_function->function.extension));
90: 		} else {
91: 			info->file_path = fs.JoinPath(
92: 			    stmt.info->file_path,
93: 			    StringUtil::Format("%s_%s_%s.%s", SanitizeExportIdentifier(table->schema->name), to_string(id),
94: 			                       SanitizeExportIdentifier(table->name), copy_function->function.extension));
95: 		}
96: 		info->is_from = false;
97: 		info->schema = table->schema->name;
98: 		info->table = table->name;
99: 
100: 		exported_data.table_name = info->table;
101: 		exported_data.schema_name = info->schema;
102: 		exported_data.file_path = info->file_path;
103: 
104: 		exported_tables.data[table] = exported_data;
105: 		id++;
106: 
107: 		// generate the copy statement and bind it
108: 		CopyStatement copy_stmt;
109: 		copy_stmt.info = move(info);
110: 
111: 		auto copy_binder = Binder::CreateBinder(context);
112: 		auto bound_statement = copy_binder->Bind(copy_stmt);
113: 		if (child_operator) {
114: 			// use UNION ALL to combine the individual copy statements into a single node
115: 			auto copy_union =
116: 			    make_unique<LogicalSetOperation>(GenerateTableIndex(), 1, move(child_operator),
117: 			                                     move(bound_statement.plan), LogicalOperatorType::LOGICAL_UNION);
118: 			child_operator = move(copy_union);
119: 		} else {
120: 			child_operator = move(bound_statement.plan);
121: 		}
122: 	}
123: 
124: 	// try to create the directory, if it doesn't exist yet
125: 	// a bit hacky to do it here, but we need to create the directory BEFORE the copy statements run
126: 	if (!fs.DirectoryExists(stmt.info->file_path)) {
127: 		fs.CreateDirectory(stmt.info->file_path);
128: 	}
129: 
130: 	// create the export node
131: 	auto export_node = make_unique<LogicalExport>(copy_function->function, move(stmt.info), exported_tables);
132: 
133: 	if (child_operator) {
134: 		export_node->children.push_back(move(child_operator));
135: 	}
136: 
137: 	result.plan = move(export_node);
138: 	this->allow_stream_result = false;
139: 	return result;
140: }
141: 
142: } // namespace duckdb
[end of src/planner/binder/statement/bind_export.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: