You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
CSV reader on .zst and lag/lead: line order is not preserved 
### What happens?

When using `read_csv()` to read from a .zst-compressedfile and using `lead(line, 1) over ()`, the positions of the rows in the resultset do not match the positions of the corresponding lines in the file, even though the `preserve_insertion_order` is set to `TRUE` (default). (`lag(line, 1) over ()` shows similar behavior)

WIth a plain, uncompressed text file, the order is as expected.
Interestingly, adding `row_number() over ()` seems to remedy the issue.

### To Reproduce

```sql
SELECT  line
,       lead(line, 1) over () as next_line
FROM    read_csv(
          'https://database.lichess.org/standard/lichess_db_standard_rated_2013-01.pgn.zst'
        , columns = {'line': 'VARCHAR'}
        )
LIMIT 20;
```

output:

```
┌──────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│         line         │                                                                next_line                                                                 │
│       varchar        │                                                                 varchar                                                                  │
├──────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ [BlackElo "1855"]    │ [WhiteRatingDiff "+13"]                                                                                                                  │
│ [WhiteRatingDiff ".  │ [BlackRatingDiff "-15"]                                                                                                                  │
│ [BlackRatingDiff ".  │ [ECO "C00"]                                                                                                                              │
│ [ECO "C00"]          │ [Opening "Queen's Pawn Game: Franco-Sicilian Defense"]                                                                                   │
│ [Opening "Queen's .  │ [TimeControl "60+0"]                                                                                                                     │
│ [TimeControl "60+0"] │ [Termination "Normal"]                                                                                                                   │
│ [Termination "Norm.  │                                                                                                                                          │
│                      │ 1. e4 c5 2. d4 e6 3. dxc5 Bxc5 4. Nf3 Be7 5. c4 d5 6. cxd5 exd5 7. exd5 Nf6 8. Nc3 O-O 9. Bg5 h6 10. Bxf6 Bxf6 11. Be2 Re8 12. O-O Bg4.  │
│ 1. e4 c5 2. d4 e6 .  │                                                                                                                                          │
│                      │ [Event "Rated Bullet game"]                                                                                                              │
│ [Event "Rated Bull.  │ [Site "https://lichess.org/4op0p4eh"]                                                                                                    │
│ [Site "https://lic.  │ [White "LEGENDARY_ERFAN"]                                                                                                                │
│ [White "LEGENDARY_.  │ [Black "RookieRook"]                                                                                                                     │
│ [Black "RookieRook"] │ [Result "0-1"]                                                                                                                           │
│ [Result "0-1"]       │ [UTCDate "2013.01.01"]                                                                                                                   │
│ [UTCDate "2013.01..  │ [UTCTime "02:16:18"]                                                                                                                     │
│ [UTCTime "02:16:18"] │ [WhiteElo "1395"]                                                                                                                        │
│ [WhiteElo "1395"]    │ [BlackElo "1525"]                                                                                                                        │
│ [BlackElo "1525"]    │ [WhiteRatingDiff "-18"]                                                                                                                  │
│ [WhiteRatingDiff ".  │ [BlackRatingDiff "+8"]                                                                                                                   │
├──────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ 20 rows                                                                                                                                               2 columns │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

Query demonstrating line order as it appears in file:

```sql
SELECT  line
FROM    read_csv(
          'https://database.lichess.org/standard/lichess_db_standard_rated_2013-01.pgn.zst'
        , columns = {'line': 'VARCHAR'}
        )
LIMIT 20;
```

Output:
```
┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│                                                                        line                                                                        │
│                                                                      varchar                                                                       │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ [Event "Rated Classical game"]                                                                                                                     │
│ [Site "https://lichess.org/j1dkb5dw"]                                                                                                              │
│ [White "BFG9k"]                                                                                                                                    │
│ [Black "mamalak"]                                                                                                                                  │
│ [Result "1-0"]                                                                                                                                     │
│ [UTCDate "2012.12.31"]                                                                                                                             │
│ [UTCTime "23:01:03"]                                                                                                                               │
│ [WhiteElo "1639"]                                                                                                                                  │
│ [BlackElo "1403"]                                                                                                                                  │
│ [WhiteRatingDiff "+5"]                                                                                                                             │
│ [BlackRatingDiff "-8"]                                                                                                                             │
│ [ECO "C00"]                                                                                                                                        │
│ [Opening "French Defense: Normal Variation"]                                                                                                       │
│ [TimeControl "600+8"]                                                                                                                              │
│ [Termination "Normal"]                                                                                                                             │
│                                                                                                                                                    │
│ 1. e4 e6 2. d4 b6 3. a3 Bb7 4. Nc3 Nh6 5. Bxh6 gxh6 6. Be2 Qg5 7. Bg4 h5 8. Nf3 Qg6 9. Nh4 Qg5 10. Bxh5 Qxh4 11. Qf3 Kd8 12. Qxf7 Nc6 13. Qe8# 1-0 │
│                                                                                                                                                    │
│ [Event "Rated Classical game"]                                                                                                                     │
│ [Site "https://lichess.org/a9tcp02g"]                                                                                                              │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                      20 rows                                                                       │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

### OS:

windows 11 enterprise

### DuckDB Version:

v1.0.0 1f98600c2c

### DuckDB Client:

CLI

### Full Name:

Roland Bouman

### Affiliation:

EPAM Systems BV Netherlands

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/execution/operator/aggregate/physical_window.cpp]
1: #include "duckdb/execution/operator/aggregate/physical_window.hpp"
2: 
3: #include "duckdb/common/operator/add.hpp"
4: #include "duckdb/common/operator/cast_operators.hpp"
5: #include "duckdb/common/operator/comparison_operators.hpp"
6: #include "duckdb/common/operator/subtract.hpp"
7: #include "duckdb/common/optional_ptr.hpp"
8: #include "duckdb/common/radix_partitioning.hpp"
9: #include "duckdb/common/row_operations/row_operations.hpp"
10: #include "duckdb/common/sort/partition_state.hpp"
11: 
12: #include "duckdb/common/types/column/column_data_consumer.hpp"
13: #include "duckdb/common/types/row/row_data_collection_scanner.hpp"
14: #include "duckdb/common/uhugeint.hpp"
15: #include "duckdb/common/vector_operations/vector_operations.hpp"
16: #include "duckdb/common/windows_undefs.hpp"
17: #include "duckdb/execution/expression_executor.hpp"
18: #include "duckdb/execution/window_executor.hpp"
19: #include "duckdb/execution/window_segment_tree.hpp"
20: #include "duckdb/main/client_config.hpp"
21: #include "duckdb/main/config.hpp"
22: #include "duckdb/parallel/base_pipeline_event.hpp"
23: #include "duckdb/planner/expression/bound_reference_expression.hpp"
24: #include "duckdb/planner/expression/bound_window_expression.hpp"
25: 
26: #include <algorithm>
27: #include <cmath>
28: #include <numeric>
29: 
30: namespace duckdb {
31: 
32: //	Global sink state
33: class WindowGlobalSinkState;
34: 
35: enum WindowGroupStage : uint8_t { SINK, FINALIZE, GETDATA, DONE };
36: 
37: class WindowHashGroup {
38: public:
39: 	using HashGroupPtr = unique_ptr<PartitionGlobalHashGroup>;
40: 	using OrderMasks = PartitionGlobalHashGroup::OrderMasks;
41: 	using ExecutorGlobalStatePtr = unique_ptr<WindowExecutorGlobalState>;
42: 	using ExecutorGlobalStates = vector<ExecutorGlobalStatePtr>;
43: 
44: 	WindowHashGroup(WindowGlobalSinkState &gstate, const idx_t hash_bin_p);
45: 
46: 	ExecutorGlobalStates &Initialize(WindowGlobalSinkState &gstate);
47: 
48: 	// Scan all of the blocks during the build phase
49: 	unique_ptr<RowDataCollectionScanner> GetBuildScanner(idx_t block_idx) const {
50: 		if (!rows) {
51: 			return nullptr;
52: 		}
53: 		return make_uniq<RowDataCollectionScanner>(*rows, *heap, layout, external, block_idx, false);
54: 	}
55: 
56: 	// Scan a single block during the evaluate phase
57: 	unique_ptr<RowDataCollectionScanner> GetEvaluateScanner(idx_t block_idx) const {
58: 		//	Second pass can flush
59: 		D_ASSERT(rows);
60: 		return make_uniq<RowDataCollectionScanner>(*rows, *heap, layout, external, block_idx, true);
61: 	}
62: 
63: 	// The processing stage for this group
64: 	WindowGroupStage GetStage() const {
65: 		auto result = WindowGroupStage::SINK;
66: 
67: 		if (sunk == count) {
68: 			result = WindowGroupStage::FINALIZE;
69: 		}
70: 
71: 		if (finalized == blocks) {
72: 			result = WindowGroupStage::GETDATA;
73: 		}
74: 
75: 		return result;
76: 	}
77: 
78: 	//! The hash partition data
79: 	HashGroupPtr hash_group;
80: 	//! The size of the group
81: 	idx_t count = 0;
82: 	//! The number of blocks in the group
83: 	idx_t blocks = 0;
84: 	unique_ptr<RowDataCollection> rows;
85: 	unique_ptr<RowDataCollection> heap;
86: 	RowLayout layout;
87: 	//! The partition boundary mask
88: 	ValidityMask partition_mask;
89: 	//! The order boundary mask
90: 	OrderMasks order_masks;
91: 	//! External paging
92: 	bool external;
93: 	//! The function global states for this hash group
94: 	ExecutorGlobalStates gestates;
95: 
96: 	//! The bin number
97: 	idx_t hash_bin;
98: 	//! Single threading lock
99: 	mutex lock;
100: 	//! Count of sunk rows
101: 	std::atomic<idx_t> sunk;
102: 	//! Count of finalized blocks
103: 	std::atomic<idx_t> finalized;
104: 	//! The number of tasks left before we should be deleted
105: 	std::atomic<idx_t> tasks_remaining;
106: 	//! The output ordering batch index this hash group starts at
107: 	idx_t batch_base;
108: 
109: private:
110: 	void MaterializeSortedData();
111: };
112: 
113: class WindowPartitionGlobalSinkState;
114: 
115: class WindowGlobalSinkState : public GlobalSinkState {
116: public:
117: 	using ExecutorPtr = unique_ptr<WindowExecutor>;
118: 	using Executors = vector<ExecutorPtr>;
119: 
120: 	WindowGlobalSinkState(const PhysicalWindow &op, ClientContext &context);
121: 
122: 	//! Parent operator
123: 	const PhysicalWindow &op;
124: 	//! Execution context
125: 	ClientContext &context;
126: 	//! The partitioned sunk data
127: 	unique_ptr<WindowPartitionGlobalSinkState> global_partition;
128: 	//! The execution functions
129: 	Executors executors;
130: };
131: 
132: class WindowPartitionGlobalSinkState : public PartitionGlobalSinkState {
133: public:
134: 	using WindowHashGroupPtr = unique_ptr<WindowHashGroup>;
135: 
136: 	WindowPartitionGlobalSinkState(WindowGlobalSinkState &gsink, const BoundWindowExpression &wexpr)
137: 	    : PartitionGlobalSinkState(gsink.context, wexpr.partitions, wexpr.orders, gsink.op.children[0]->types,
138: 	                               wexpr.partitions_stats, gsink.op.estimated_cardinality),
139: 	      gsink(gsink) {
140: 	}
141: 	~WindowPartitionGlobalSinkState() override = default;
142: 
143: 	void OnBeginMerge() override {
144: 		PartitionGlobalSinkState::OnBeginMerge();
145: 		window_hash_groups.resize(hash_groups.size());
146: 	}
147: 
148: 	void OnSortedPartition(const idx_t group_idx) override {
149: 		PartitionGlobalSinkState::OnSortedPartition(group_idx);
150: 		window_hash_groups[group_idx] = make_uniq<WindowHashGroup>(gsink, group_idx);
151: 	}
152: 
153: 	//! Operator global sink state
154: 	WindowGlobalSinkState &gsink;
155: 	//! The sorted hash groups
156: 	vector<WindowHashGroupPtr> window_hash_groups;
157: };
158: 
159: //	Per-thread sink state
160: class WindowLocalSinkState : public LocalSinkState {
161: public:
162: 	WindowLocalSinkState(ClientContext &context, const WindowGlobalSinkState &gstate)
163: 	    : local_partition(context, *gstate.global_partition) {
164: 	}
165: 
166: 	void Sink(DataChunk &input_chunk) {
167: 		local_partition.Sink(input_chunk);
168: 	}
169: 
170: 	void Combine() {
171: 		local_partition.Combine();
172: 	}
173: 
174: 	PartitionLocalSinkState local_partition;
175: };
176: 
177: // this implements a sorted window functions variant
178: PhysicalWindow::PhysicalWindow(vector<LogicalType> types, vector<unique_ptr<Expression>> select_list_p,
179:                                idx_t estimated_cardinality, PhysicalOperatorType type)
180:     : PhysicalOperator(type, std::move(types), estimated_cardinality), select_list(std::move(select_list_p)),
181:       order_idx(0), is_order_dependent(false) {
182: 
183: 	idx_t max_orders = 0;
184: 	for (idx_t i = 0; i < select_list.size(); ++i) {
185: 		auto &expr = select_list[i];
186: 		D_ASSERT(expr->expression_class == ExpressionClass::BOUND_WINDOW);
187: 		auto &bound_window = expr->Cast<BoundWindowExpression>();
188: 		if (bound_window.partitions.empty() && bound_window.orders.empty()) {
189: 			is_order_dependent = true;
190: 		}
191: 
192: 		if (bound_window.orders.size() > max_orders) {
193: 			order_idx = i;
194: 			max_orders = bound_window.orders.size();
195: 		}
196: 	}
197: }
198: 
199: static unique_ptr<WindowExecutor> WindowExecutorFactory(BoundWindowExpression &wexpr, ClientContext &context,
200:                                                         WindowAggregationMode mode) {
201: 	switch (wexpr.type) {
202: 	case ExpressionType::WINDOW_AGGREGATE:
203: 		return make_uniq<WindowAggregateExecutor>(wexpr, context, mode);
204: 	case ExpressionType::WINDOW_ROW_NUMBER:
205: 		return make_uniq<WindowRowNumberExecutor>(wexpr, context);
206: 	case ExpressionType::WINDOW_RANK_DENSE:
207: 		return make_uniq<WindowDenseRankExecutor>(wexpr, context);
208: 	case ExpressionType::WINDOW_RANK:
209: 		return make_uniq<WindowRankExecutor>(wexpr, context);
210: 	case ExpressionType::WINDOW_PERCENT_RANK:
211: 		return make_uniq<WindowPercentRankExecutor>(wexpr, context);
212: 	case ExpressionType::WINDOW_CUME_DIST:
213: 		return make_uniq<WindowCumeDistExecutor>(wexpr, context);
214: 	case ExpressionType::WINDOW_NTILE:
215: 		return make_uniq<WindowNtileExecutor>(wexpr, context);
216: 	case ExpressionType::WINDOW_LEAD:
217: 	case ExpressionType::WINDOW_LAG:
218: 		return make_uniq<WindowLeadLagExecutor>(wexpr, context);
219: 	case ExpressionType::WINDOW_FIRST_VALUE:
220: 		return make_uniq<WindowFirstValueExecutor>(wexpr, context);
221: 	case ExpressionType::WINDOW_LAST_VALUE:
222: 		return make_uniq<WindowLastValueExecutor>(wexpr, context);
223: 	case ExpressionType::WINDOW_NTH_VALUE:
224: 		return make_uniq<WindowNthValueExecutor>(wexpr, context);
225: 		break;
226: 	default:
227: 		throw InternalException("Window aggregate type %s", ExpressionTypeToString(wexpr.type));
228: 	}
229: }
230: 
231: WindowGlobalSinkState::WindowGlobalSinkState(const PhysicalWindow &op, ClientContext &context)
232:     : op(op), context(context) {
233: 
234: 	D_ASSERT(op.select_list[op.order_idx]->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
235: 	auto &wexpr = op.select_list[op.order_idx]->Cast<BoundWindowExpression>();
236: 
237: 	const auto mode = DBConfig::GetConfig(context).options.window_mode;
238: 	for (idx_t expr_idx = 0; expr_idx < op.select_list.size(); ++expr_idx) {
239: 		D_ASSERT(op.select_list[expr_idx]->GetExpressionClass() == ExpressionClass::BOUND_WINDOW);
240: 		auto &wexpr = op.select_list[expr_idx]->Cast<BoundWindowExpression>();
241: 		auto wexec = WindowExecutorFactory(wexpr, context, mode);
242: 		executors.emplace_back(std::move(wexec));
243: 	}
244: 
245: 	global_partition = make_uniq<WindowPartitionGlobalSinkState>(*this, wexpr);
246: }
247: 
248: //===--------------------------------------------------------------------===//
249: // Sink
250: //===--------------------------------------------------------------------===//
251: SinkResultType PhysicalWindow::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {
252: 	auto &lstate = input.local_state.Cast<WindowLocalSinkState>();
253: 
254: 	lstate.Sink(chunk);
255: 
256: 	return SinkResultType::NEED_MORE_INPUT;
257: }
258: 
259: SinkCombineResultType PhysicalWindow::Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const {
260: 	auto &lstate = input.local_state.Cast<WindowLocalSinkState>();
261: 	lstate.Combine();
262: 
263: 	return SinkCombineResultType::FINISHED;
264: }
265: 
266: unique_ptr<LocalSinkState> PhysicalWindow::GetLocalSinkState(ExecutionContext &context) const {
267: 	auto &gstate = sink_state->Cast<WindowGlobalSinkState>();
268: 	return make_uniq<WindowLocalSinkState>(context.client, gstate);
269: }
270: 
271: unique_ptr<GlobalSinkState> PhysicalWindow::GetGlobalSinkState(ClientContext &context) const {
272: 	return make_uniq<WindowGlobalSinkState>(*this, context);
273: }
274: 
275: SinkFinalizeType PhysicalWindow::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
276:                                           OperatorSinkFinalizeInput &input) const {
277: 	auto &state = input.global_state.Cast<WindowGlobalSinkState>();
278: 
279: 	//	Did we get any data?
280: 	if (!state.global_partition->count) {
281: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
282: 	}
283: 
284: 	// Do we have any sorting to schedule?
285: 	if (state.global_partition->rows) {
286: 		D_ASSERT(!state.global_partition->grouping_data);
287: 		return state.global_partition->rows->count ? SinkFinalizeType::READY : SinkFinalizeType::NO_OUTPUT_POSSIBLE;
288: 	}
289: 
290: 	// Find the first group to sort
291: 	if (!state.global_partition->HasMergeTasks()) {
292: 		// Empty input!
293: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
294: 	}
295: 
296: 	// Schedule all the sorts for maximum thread utilisation
297: 	auto new_event = make_shared_ptr<PartitionMergeEvent>(*state.global_partition, pipeline);
298: 	event.InsertEvent(std::move(new_event));
299: 
300: 	return SinkFinalizeType::READY;
301: }
302: 
303: //===--------------------------------------------------------------------===//
304: // Source
305: //===--------------------------------------------------------------------===//
306: class WindowGlobalSourceState : public GlobalSourceState {
307: public:
308: 	using ScannerPtr = unique_ptr<RowDataCollectionScanner>;
309: 
310: 	struct Task {
311: 		Task(WindowGroupStage stage, idx_t group_idx, idx_t max_idx)
312: 		    : stage(stage), group_idx(group_idx), max_idx(max_idx) {
313: 		}
314: 		WindowGroupStage stage;
315: 		idx_t group_idx;
316: 		idx_t max_idx;
317: 		idx_t begin_idx = 0;
318: 		idx_t end_idx = 0;
319: 	};
320: 	using TaskPtr = optional_ptr<Task>;
321: 
322: 	WindowGlobalSourceState(ClientContext &context_p, WindowGlobalSinkState &gsink_p);
323: 
324: 	//! Get the next task given the current state
325: 	bool TryNextTask(TaskPtr &task);
326: 	//! Finish a task
327: 	void FinishTask(TaskPtr task);
328: 	//! Single-threaded manipulation of the interrupt queue
329: 	bool UpdateBlockedTasks(bool blocked, InterruptState &interrupt_state) const;
330: 
331: 	//! Context for executing computations
332: 	ClientContext &context;
333: 	//! All the sunk data
334: 	WindowGlobalSinkState &gsink;
335: 	//! State mutex
336: 	mutable mutex lock;
337: 	//! The list of tasks
338: 	vector<Task> tasks;
339: 	//! The the next task
340: 	idx_t next_task;
341: 	//! Stop producing tasks
342: 	atomic<bool> stopped;
343: 	//! The number of rows returned
344: 	atomic<idx_t> returned;
345: 	//! The set of blocked tasks
346: 	mutable vector<InterruptState> blocked_tasks;
347: 
348: public:
349: 	idx_t MaxThreads() override {
350: 		return tasks.size();
351: 	}
352: };
353: 
354: WindowGlobalSourceState::WindowGlobalSourceState(ClientContext &context_p, WindowGlobalSinkState &gsink_p)
355:     : context(context_p), gsink(gsink_p), next_task(0), stopped(false), returned(0) {
356: 	auto &gpart = gsink.global_partition;
357: 	auto &window_hash_groups = gsink.global_partition->window_hash_groups;
358: 
359: 	if (window_hash_groups.empty()) {
360: 		//	OVER()
361: 		if (gpart->rows && !gpart->rows->blocks.empty()) {
362: 			// We need to construct the single WindowHashGroup here because the sort tasks will not be run.
363: 			window_hash_groups.emplace_back(make_uniq<WindowHashGroup>(gsink, idx_t(0)));
364: 		}
365: 	} else {
366: 		idx_t batch_base = 0;
367: 		for (auto &window_hash_group : window_hash_groups) {
368: 			if (!window_hash_group) {
369: 				continue;
370: 			}
371: 			auto &rows = window_hash_group->rows;
372: 			if (!rows) {
373: 				continue;
374: 			}
375: 
376: 			const auto block_count = window_hash_group->rows->blocks.size();
377: 			window_hash_group->batch_base = batch_base;
378: 			batch_base += block_count;
379: 		}
380: 	}
381: 
382: 	//    Sort the groups from largest to smallest
383: 	if (window_hash_groups.empty()) {
384: 		return;
385: 	}
386: 
387: 	using PartitionBlock = std::pair<idx_t, idx_t>;
388: 	vector<PartitionBlock> partition_blocks;
389: 	for (idx_t group_idx = 0; group_idx < window_hash_groups.size(); ++group_idx) {
390: 		auto &window_hash_group = window_hash_groups[group_idx];
391: 		partition_blocks.emplace_back(window_hash_group->rows->blocks.size(), group_idx);
392: 	}
393: 	std::sort(partition_blocks.begin(), partition_blocks.end(), std::greater<PartitionBlock>());
394: 
395: 	//	Schedule the largest group on as many threads as possible
396: 	const auto threads = idx_t(TaskScheduler::GetScheduler(context).NumberOfThreads());
397: 	const auto &max_block = partition_blocks.front();
398: 	const auto per_thread = (max_block.first + threads - 1) / threads;
399: 
400: 	//	TODO: Generate dynamically instead of building a big list?
401: 	vector<WindowGroupStage> states {WindowGroupStage::SINK, WindowGroupStage::FINALIZE, WindowGroupStage::GETDATA};
402: 	for (const auto &b : partition_blocks) {
403: 		for (const auto &state : states) {
404: 			for (Task task(state, b.second, b.first); task.begin_idx < task.max_idx; task.begin_idx += per_thread) {
405: 				task.end_idx = MinValue<idx_t>(task.begin_idx + per_thread, task.max_idx);
406: 				tasks.emplace_back(task);
407: 				window_hash_groups[task.group_idx]->tasks_remaining++;
408: 			}
409: 		}
410: 	}
411: }
412: 
413: void WindowHashGroup::MaterializeSortedData() {
414: 	auto &global_sort_state = *hash_group->global_sort;
415: 	if (global_sort_state.sorted_blocks.empty()) {
416: 		return;
417: 	}
418: 
419: 	// scan the sorted row data
420: 	D_ASSERT(global_sort_state.sorted_blocks.size() == 1);
421: 	auto &sb = *global_sort_state.sorted_blocks[0];
422: 
423: 	// Free up some memory before allocating more
424: 	sb.radix_sorting_data.clear();
425: 	sb.blob_sorting_data = nullptr;
426: 
427: 	// Move the sorting row blocks into our RDCs
428: 	auto &buffer_manager = global_sort_state.buffer_manager;
429: 	auto &sd = *sb.payload_data;
430: 
431: 	// Data blocks are required
432: 	D_ASSERT(!sd.data_blocks.empty());
433: 	auto &block = sd.data_blocks[0];
434: 	rows = make_uniq<RowDataCollection>(buffer_manager, block->capacity, block->entry_size);
435: 	rows->blocks = std::move(sd.data_blocks);
436: 	rows->count = std::accumulate(rows->blocks.begin(), rows->blocks.end(), idx_t(0),
437: 	                              [&](idx_t c, const unique_ptr<RowDataBlock> &b) { return c + b->count; });
438: 
439: 	// Heap blocks are optional, but we want both for iteration.
440: 	if (!sd.heap_blocks.empty()) {
441: 		auto &block = sd.heap_blocks[0];
442: 		heap = make_uniq<RowDataCollection>(buffer_manager, block->capacity, block->entry_size);
443: 		heap->blocks = std::move(sd.heap_blocks);
444: 		hash_group.reset();
445: 	} else {
446: 		heap = make_uniq<RowDataCollection>(buffer_manager, buffer_manager.GetBlockSize(), 1U, true);
447: 	}
448: 	heap->count = std::accumulate(heap->blocks.begin(), heap->blocks.end(), idx_t(0),
449: 	                              [&](idx_t c, const unique_ptr<RowDataBlock> &b) { return c + b->count; });
450: }
451: 
452: WindowHashGroup::WindowHashGroup(WindowGlobalSinkState &gstate, const idx_t hash_bin_p)
453:     : count(0), blocks(0), hash_bin(hash_bin_p), sunk(0), finalized(0), tasks_remaining(0), batch_base(0) {
454: 	// There are three types of partitions:
455: 	// 1. No partition (no sorting)
456: 	// 2. One partition (sorting, but no hashing)
457: 	// 3. Multiple partitions (sorting and hashing)
458: 
459: 	//	How big is the partition?
460: 	auto &gpart = *gstate.global_partition;
461: 	layout.Initialize(gpart.payload_types);
462: 	if (hash_bin < gpart.hash_groups.size() && gpart.hash_groups[hash_bin]) {
463: 		count = gpart.hash_groups[hash_bin]->count;
464: 	} else if (gpart.rows && !hash_bin) {
465: 		count = gpart.count;
466: 	} else {
467: 		return;
468: 	}
469: 
470: 	//	Initialise masks to false
471: 	partition_mask.Initialize(count);
472: 	partition_mask.SetAllInvalid(count);
473: 
474: 	const auto &executors = gstate.executors;
475: 	for (auto &wexec : executors) {
476: 		auto &wexpr = wexec->wexpr;
477: 		auto &order_mask = order_masks[wexpr.partitions.size() + wexpr.orders.size()];
478: 		if (order_mask.IsMaskSet()) {
479: 			continue;
480: 		}
481: 		order_mask.Initialize(count);
482: 		order_mask.SetAllInvalid(count);
483: 	}
484: 
485: 	// Scan the sorted data into new Collections
486: 	external = gpart.external;
487: 	if (gpart.rows && !hash_bin) {
488: 		// Simple mask
489: 		partition_mask.SetValidUnsafe(0);
490: 		for (auto &order_mask : order_masks) {
491: 			order_mask.second.SetValidUnsafe(0);
492: 		}
493: 		//	No partition - align the heap blocks with the row blocks
494: 		rows = gpart.rows->CloneEmpty(gpart.rows->keep_pinned);
495: 		heap = gpart.strings->CloneEmpty(gpart.strings->keep_pinned);
496: 		RowDataCollectionScanner::AlignHeapBlocks(*rows, *heap, *gpart.rows, *gpart.strings, layout);
497: 		external = true;
498: 	} else if (hash_bin < gpart.hash_groups.size()) {
499: 		// Overwrite the collections with the sorted data
500: 		D_ASSERT(gpart.hash_groups[hash_bin].get());
501: 		hash_group = std::move(gpart.hash_groups[hash_bin]);
502: 		hash_group->ComputeMasks(partition_mask, order_masks);
503: 		external = hash_group->global_sort->external;
504: 		MaterializeSortedData();
505: 	}
506: 
507: 	if (rows) {
508: 		blocks = rows->blocks.size();
509: 	}
510: }
511: 
512: // Per-thread scan state
513: class WindowLocalSourceState : public LocalSourceState {
514: public:
515: 	using LocalStatePtr = unique_ptr<WindowExecutorLocalState>;
516: 	using LocalStates = vector<LocalStatePtr>;
517: 	using Task = WindowGlobalSourceState::Task;
518: 	using TaskPtr = optional_ptr<Task>;
519: 
520: 	explicit WindowLocalSourceState(WindowGlobalSourceState &gsource);
521: 	void BeginHashGroup();
522: 	void Sink();
523: 	void Finalize();
524: 	bool GetData(DataChunk &chunk);
525: 	void FinishHashGroup(TaskPtr prev_task);
526: 
527: 	//! The shared source state
528: 	WindowGlobalSourceState &gsource;
529: 	//! The current batch index (for output reordering)
530: 	idx_t batch_index;
531: 	//! The task this thread is working on
532: 	TaskPtr task;
533: 	//! The current source being processed
534: 	optional_ptr<WindowHashGroup> window_hash_group;
535: 	//! The scan cursor
536: 	unique_ptr<RowDataCollectionScanner> scanner;
537: 	//! Buffer for the inputs
538: 	DataChunk input_chunk;
539: 	//! Executor local states.
540: 	LocalStates local_states;
541: 	//! Buffer for window results
542: 	DataChunk output_chunk;
543: };
544: 
545: WindowHashGroup::ExecutorGlobalStates &WindowHashGroup::Initialize(WindowGlobalSinkState &gsink) {
546: 	//	Single-threaded building as this is mostly memory allocation
547: 	lock_guard<mutex> gestate_guard(lock);
548: 	const auto &executors = gsink.executors;
549: 	if (gestates.size() == executors.size()) {
550: 		return gestates;
551: 	}
552: 
553: 	// These can be large so we defer building them until we are ready.
554: 	for (auto &wexec : executors) {
555: 		auto &wexpr = wexec->wexpr;
556: 		auto &order_mask = order_masks[wexpr.partitions.size() + wexpr.orders.size()];
557: 		gestates.emplace_back(wexec->GetGlobalState(count, partition_mask, order_mask));
558: 	}
559: 
560: 	return gestates;
561: }
562: 
563: void WindowLocalSourceState::BeginHashGroup() {
564: 	if (!task) {
565: 		return;
566: 	}
567: 
568: 	auto &gsink = gsource.gsink;
569: 	auto &gpart = *gsink.global_partition;
570: 	window_hash_group = gpart.window_hash_groups[task->group_idx].get();
571: 
572: 	// Create the executor state for each function
573: 	// These can be large so we defer building them until we are ready.
574: 	auto &gestates = window_hash_group->Initialize(gsink);
575: 
576: 	//	Set up the local states
577: 	const auto &executors = gsink.executors;
578: 	for (idx_t w = 0; w < executors.size(); ++w) {
579: 		local_states.emplace_back(executors[w]->GetLocalState(*gestates[w]));
580: 	}
581: }
582: 
583: void WindowLocalSourceState::Sink() {
584: 	D_ASSERT(task->stage == WindowGroupStage::SINK);
585: 
586: 	auto &gsink = gsource.gsink;
587: 	const auto &executors = gsink.executors;
588: 	auto &gestates = window_hash_group->gestates;
589: 
590: 	//	First pass over the input without flushing
591: 	for (; task->begin_idx < task->end_idx; ++task->begin_idx) {
592: 		scanner = window_hash_group->GetBuildScanner(task->begin_idx);
593: 		if (!scanner) {
594: 			break;
595: 		}
596: 		while (true) {
597: 			//	TODO: Try to align on validity mask boundaries by starting ragged?
598: 			idx_t input_idx = scanner->Scanned();
599: 			input_chunk.Reset();
600: 			scanner->Scan(input_chunk);
601: 			if (input_chunk.size() == 0) {
602: 				break;
603: 			}
604: 
605: 			//	TODO: Stagger functions to reduce contention?
606: 			for (idx_t w = 0; w < executors.size(); ++w) {
607: 				executors[w]->Sink(input_chunk, input_idx, scanner->Count(), *gestates[w], *local_states[w]);
608: 			}
609: 
610: 			window_hash_group->sunk += input_chunk.size();
611: 		}
612: 
613: 		// External scanning assumes all blocks are swizzled.
614: 		scanner->SwizzleBlock(task->begin_idx);
615: 		scanner.reset();
616: 	}
617: }
618: 
619: void WindowLocalSourceState::Finalize() {
620: 	D_ASSERT(task->stage == WindowGroupStage::FINALIZE);
621: 
622: 	// Finalize all the executors.
623: 	// Parallel finalisation is handled internally by the executor,
624: 	// and should not return until all threads have completed work.
625: 	auto &gsink = gsource.gsink;
626: 	const auto &executors = gsink.executors;
627: 	auto &gestates = window_hash_group->gestates;
628: 	for (idx_t w = 0; w < executors.size(); ++w) {
629: 		executors[w]->Finalize(*gestates[w], *local_states[w]);
630: 	}
631: 
632: 	//	Mark this range as done
633: 	window_hash_group->finalized += (task->end_idx - task->begin_idx);
634: 	task->begin_idx = task->end_idx;
635: }
636: 
637: WindowLocalSourceState::WindowLocalSourceState(WindowGlobalSourceState &gsource) : gsource(gsource), batch_index(0) {
638: 	auto &gsink = gsource.gsink;
639: 	auto &global_partition = *gsink.global_partition;
640: 
641: 	input_chunk.Initialize(global_partition.allocator, global_partition.payload_types);
642: 
643: 	vector<LogicalType> output_types;
644: 	for (auto &wexec : gsink.executors) {
645: 		auto &wexpr = wexec->wexpr;
646: 		output_types.emplace_back(wexpr.return_type);
647: 	}
648: 	output_chunk.Initialize(global_partition.allocator, output_types);
649: }
650: 
651: bool WindowGlobalSourceState::TryNextTask(TaskPtr &task) {
652: 	lock_guard<mutex> task_guard(lock);
653: 	if (next_task >= tasks.size() || stopped) {
654: 		task = nullptr;
655: 		return true;
656: 	}
657: 
658: 	//	If the next task matches the current state of its group, then we can use it
659: 	//	Otherwise block.
660: 	task = &tasks[next_task];
661: 
662: 	auto &gpart = *gsink.global_partition;
663: 	auto &window_hash_group = gpart.window_hash_groups[task->group_idx];
664: 	auto group_stage = window_hash_group->GetStage();
665: 
666: 	if (task->stage == group_stage) {
667: 		++next_task;
668: 		return true;
669: 	}
670: 
671: 	task = nullptr;
672: 	return false;
673: }
674: 
675: void WindowGlobalSourceState::FinishTask(TaskPtr task) {
676: 	if (!task) {
677: 		return;
678: 	}
679: 
680: 	auto &gpart = *gsink.global_partition;
681: 	auto &finished_hash_group = gpart.window_hash_groups[task->group_idx];
682: 	D_ASSERT(finished_hash_group);
683: 
684: 	if (!--finished_hash_group->tasks_remaining) {
685: 		finished_hash_group.reset();
686: 	}
687: }
688: 
689: void WindowLocalSourceState::FinishHashGroup(TaskPtr prev_task) {
690: 	scanner.reset();
691: 	local_states.clear();
692: 
693: 	gsource.FinishTask(prev_task);
694: }
695: 
696: bool WindowLocalSourceState::GetData(DataChunk &result) {
697: 	// Are we done with this scanner?
698: 	if (scanner && !scanner->Remaining()) {
699: 		scanner.reset();
700: 		++task->begin_idx;
701: 	}
702: 
703: 	//	Are we done with this task?
704: 	while (!task || task->begin_idx >= task->end_idx || task->stage != WindowGroupStage::GETDATA) {
705: 		auto prev_task = task;
706: 		while (!gsource.TryNextTask(task)) {
707: 			FinishHashGroup(prev_task);
708: 			return false;
709: 		}
710: 
711: 		const auto new_group = (!task || !prev_task || task->group_idx != prev_task->group_idx);
712: 		// Release all the old group's data if we started a new group
713: 		if (new_group) {
714: 			FinishHashGroup(prev_task);
715: 			BeginHashGroup();
716: 		}
717: 
718: 		// Are we done?
719: 		if (!task) {
720: 			return true;
721: 		}
722: 
723: 		// Process the new state
724: 		switch (task->stage) {
725: 		case WindowGroupStage::SINK:
726: 			Sink();
727: 			D_ASSERT(task->begin_idx == task->end_idx);
728: 			continue;
729: 		case WindowGroupStage::FINALIZE:
730: 			Finalize();
731: 			D_ASSERT(task->begin_idx == task->end_idx);
732: 			continue;
733: 		case WindowGroupStage::GETDATA:
734: 			D_ASSERT(task->begin_idx < task->end_idx);
735: 			break;
736: 		default:
737: 			throw InternalException("Invalid window source state.");
738: 		}
739: 	}
740: 
741: 	D_ASSERT(window_hash_group->GetStage() == WindowGroupStage::GETDATA);
742: 
743: 	if (!scanner) {
744: 		scanner = window_hash_group->GetEvaluateScanner(task->begin_idx);
745: 		batch_index = window_hash_group->batch_base + task->begin_idx;
746: 	}
747: 
748: 	const auto position = scanner->Scanned();
749: 	input_chunk.Reset();
750: 	scanner->Scan(input_chunk);
751: 
752: 	const auto &executors = gsource.gsink.executors;
753: 	auto &gestates = window_hash_group->gestates;
754: 	output_chunk.Reset();
755: 	for (idx_t expr_idx = 0; expr_idx < executors.size(); ++expr_idx) {
756: 		auto &executor = *executors[expr_idx];
757: 		auto &gstate = *gestates[expr_idx];
758: 		auto &lstate = *local_states[expr_idx];
759: 		auto &result = output_chunk.data[expr_idx];
760: 		executor.Evaluate(position, input_chunk, result, lstate, gstate);
761: 	}
762: 	output_chunk.SetCardinality(input_chunk);
763: 	output_chunk.Verify();
764: 
765: 	idx_t out_idx = 0;
766: 	result.SetCardinality(input_chunk);
767: 	for (idx_t col_idx = 0; col_idx < input_chunk.ColumnCount(); col_idx++) {
768: 		result.data[out_idx++].Reference(input_chunk.data[col_idx]);
769: 	}
770: 	for (idx_t col_idx = 0; col_idx < output_chunk.ColumnCount(); col_idx++) {
771: 		result.data[out_idx++].Reference(output_chunk.data[col_idx]);
772: 	}
773: 	result.Verify();
774: 
775: 	return true;
776: }
777: 
778: unique_ptr<LocalSourceState> PhysicalWindow::GetLocalSourceState(ExecutionContext &context,
779:                                                                  GlobalSourceState &gsource_p) const {
780: 	auto &gsource = gsource_p.Cast<WindowGlobalSourceState>();
781: 	return make_uniq<WindowLocalSourceState>(gsource);
782: }
783: 
784: unique_ptr<GlobalSourceState> PhysicalWindow::GetGlobalSourceState(ClientContext &context) const {
785: 	auto &gsink = sink_state->Cast<WindowGlobalSinkState>();
786: 	return make_uniq<WindowGlobalSourceState>(context, gsink);
787: }
788: 
789: bool PhysicalWindow::SupportsBatchIndex() const {
790: 	//	We can only preserve order for single partitioning
791: 	//	or work stealing causes out of order batch numbers
792: 	auto &wexpr = select_list[order_idx]->Cast<BoundWindowExpression>();
793: 	return wexpr.partitions.empty() && !wexpr.orders.empty();
794: }
795: 
796: OrderPreservationType PhysicalWindow::SourceOrder() const {
797: 	return SupportsBatchIndex() ? OrderPreservationType::FIXED_ORDER : OrderPreservationType::NO_ORDER;
798: }
799: 
800: double PhysicalWindow::GetProgress(ClientContext &context, GlobalSourceState &gsource_p) const {
801: 	auto &gsource = gsource_p.Cast<WindowGlobalSourceState>();
802: 	const auto returned = gsource.returned.load();
803: 
804: 	auto &gsink = gsource.gsink;
805: 	const auto count = gsink.global_partition->count.load();
806: 	return count ? (double(returned) / double(count)) : -1;
807: }
808: 
809: idx_t PhysicalWindow::GetBatchIndex(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,
810:                                     LocalSourceState &lstate_p) const {
811: 	auto &lstate = lstate_p.Cast<WindowLocalSourceState>();
812: 	return lstate.batch_index;
813: }
814: 
815: bool WindowGlobalSourceState::UpdateBlockedTasks(bool blocked, InterruptState &interrupt_state) const {
816: 	lock_guard<mutex> guard(lock);
817: 	if (blocked) {
818: 		blocked_tasks.push_back(interrupt_state);
819: 	} else {
820: 		// The pipeline is unblocked, so flush tasks
821: 		for (auto &state : blocked_tasks) {
822: 			state.Callback();
823: 		}
824: 		blocked_tasks.clear();
825: 	}
826: 	return blocked;
827: }
828: 
829: SourceResultType PhysicalWindow::GetData(ExecutionContext &context, DataChunk &chunk,
830:                                          OperatorSourceInput &input) const {
831: 	auto &gsource = input.global_state.Cast<WindowGlobalSourceState>();
832: 	auto &lsource = input.local_state.Cast<WindowLocalSourceState>();
833: 
834: #if 1
835: 	//	Debugging variant
836: 	try {
837: 		while (!lsource.GetData(chunk)) {
838: 			TaskScheduler::GetScheduler(context.client).YieldThread();
839: 		}
840: 	} catch (...) {
841: 		gsource.stopped = true;
842: 		throw;
843: 	}
844: #else
845: 	try {
846: 		const auto blocked = !lsource.GetData(chunk);
847: 		if (gsource.UpdateBlockedTasks(blocked, input.interrupt_state)) {
848: 			return SourceResultType::BLOCKED;
849: 		}
850: 	} catch (...) {
851: 		gsource.stopped = true;
852: 		gsource.UpdateBlockedTasks(false, input.interrupt_state);
853: 		throw;
854: 	}
855: #endif
856: 	gsource.returned += chunk.size();
857: 
858: 	return chunk.size() == 0 ? SourceResultType::FINISHED : SourceResultType::HAVE_MORE_OUTPUT;
859: }
860: 
861: string PhysicalWindow::ParamsToString() const {
862: 	string result;
863: 	for (idx_t i = 0; i < select_list.size(); i++) {
864: 		if (i > 0) {
865: 			result += "\n";
866: 		}
867: 		result += select_list[i]->GetName();
868: 	}
869: 	return result;
870: }
871: 
872: } // namespace duckdb
[end of src/execution/operator/aggregate/physical_window.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: