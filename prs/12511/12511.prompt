You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Importing from Pandas has incorrect column type for empty string/object columns
### What happens?

Importing data from a Pandas dataframe result in incorrect column if the column in Pandas is object or string types and the column is completely empty.

Documentation is not clear on the implicit casting which occurs when importing a dataframe, but I would expect that the column/string pandas type becomes `VARCHAR`. This is the case when there are values in the column, but not when the column is empty, instead returning `INTEGER`. 

In my limited testing this does not occur with other column types.

Both 1.0.0 and 1.0.1-dev122 results in this bug.

### To Reproduce

The following code will create example dataframes and relations and compare types, failing as per bug description when checking relation data types. Changing `use_string_type` allows testing of both `object` and `string` Pandas types.

```python
import random

import pandas as pd
import numpy as np
import duckdb

def get_filled_df(rows: int = 100, use_string_type: bool = False) -> pd.DataFrame:
    """Return dataframe filled with random data of specific types."""
    index = np.arange(rows)
    data: dict = {
        "int_column": [random.randint(0, 100) for _ in range(rows)],
        "bool_column": [random.choice([True, False]) for _ in range(rows)],
        "float_column": [random.uniform(0, 1) for _ in range(rows)],
        "string_column": [random.choice(["foo", "bar", "baz", "qux"]) for _ in range(rows)]
    }

    return pd.DataFrame(data=data, index=index).convert_dtypes(infer_objects=False, convert_string=use_string_type)

def get_empty_df(rows: int = 100, use_string_type: bool = False) -> pd.DataFrame:
    """Return dataframe with empty columns of specific types."""
    df: pd.DataFrame = pd.DataFrame(index=np.arange(rows))

    df["int_column"] = pd.Series(dtype="Int64")
    df["bool_column"] = pd.Series(dtype="boolean")
    df["float_column"] = pd.Series(dtype="Float64")
    if use_string_type:
        df["string_column"] = pd.Series(dtype="string")
    else:
        df["string_column"] = pd.Series(dtype="object")

    return df

print(f"Pandas version: {pd.__version__}") # 2.2.2
print(f"Numpy version: {np.__version__}") # 1.26.4
print(f"DuckDB version: {duckdb.__version__}") # 1.0.0

use_string_type: bool = True
filled_df: pd.DataFrame = get_filled_df(use_string_type=use_string_type)
empty_df: pd.DataFrame = get_empty_df(use_string_type=use_string_type)

# check df correctly constructed
assert len(filled_df) == len(empty_df)

# check dtypes match
for fdf_type, edf_type in zip(filled_df.dtypes, empty_df.dtypes):
    assert fdf_type == edf_type

with duckdb.connect(database=":memory:") as con:
    f_df_rel: duckdb.DuckDBPyRelation = con.from_df(filled_df)
    e_df_rel: duckdb.DuckDBPyRelation = con.from_df(empty_df)

    for fdf_col, edf_col in zip(f_df_rel.columns, e_df_rel.columns):
        assert fdf_col == edf_col, f"Filled column {fdf_col} does not match empty col {fdf_col}"

    # fails with empty string column being INTEGER type instead of VARCHAR
    for (fdf_col, fdf_type), (edf_col, edf_type) in zip(zip(f_df_rel.columns, f_df_rel.dtypes), zip(e_df_rel.columns, e_df_rel.dtypes)):
        assert fdf_type == edf_type, f"Filled column {fdf_col} type {fdf_type} does not match empty column {edf_col} type {edf_type}"
```

This results in:

```console
AssertionError: Filled column string_column type VARCHAR does not match empty column string_column type INTEGER
```

### OS:

Ubuntu 20.04 WSL x64 (5.15.146.1-microsoft-standard-WSL2)

### DuckDB Version:

1.0.0, 1.0.1-dev122

### DuckDB Client:

Python

### Full Name:

Zander Horn

### Affiliation:

Stone Three

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Not applicable - the reproduction does not require a data set

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of tools/pythonpkg/src/include/duckdb_python/numpy/numpy_type.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/numpy/numpy_type.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types.hpp"
12: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
13: 
14: namespace duckdb {
15: 
16: // Pandas has two different sets of types
17: // NumPy dtypes (e.g., bool, int8,...)
18: // Pandas Specific Types (e.g., categorical, datetime_tz,...)
19: enum class NumpyNullableType : uint8_t {
20: 	//! NumPy dtypes
21: 	BOOL,        //! bool_, bool8
22: 	INT_8,       //! byte, int8
23: 	UINT_8,      //! ubyte, uint8
24: 	INT_16,      //! int16, short
25: 	UINT_16,     //! uint16, ushort
26: 	INT_32,      //! int32, intc
27: 	UINT_32,     //! uint32, uintc,
28: 	INT_64,      //! int64, int0, int_, intp, matrix
29: 	UINT_64,     //! uint64, uint, uint0, uintp
30: 	FLOAT_16,    //! float16, half
31: 	FLOAT_32,    //! float32, single
32: 	FLOAT_64,    //! float64, float_, double
33: 	OBJECT,      //! object
34: 	UNICODE,     //! <U1, unicode_, str_, str0
35: 	DATETIME_S,  //! datetime64[s], <M8[s]
36: 	DATETIME_MS, //! datetime64[ms], <M8[ms]
37: 	DATETIME_NS, //! datetime64[ns], <M8[ns]
38: 	DATETIME_US, //! datetime64[us], <M8[us]
39: 	TIMEDELTA,   //! timedelta64[D], timedelta64
40: 
41: 	//! ------------------------------------------------------------
42: 	//! Extension Types
43: 	//! ------------------------------------------------------------
44: 	CATEGORY, //! category
45: };
46: 
47: struct NumpyType {
48: 	NumpyNullableType type;
49: 	//! Optionally if the type is a DATETIME,
50: 	//! this indicates whether the type has timezone information
51: 	bool has_timezone = false;
52: };
53: 
54: enum class NumpyObjectType : uint8_t {
55: 	//! To identify supported Numpy objects for scaning
56: 	INVALID,   //! unsupported numpy object
57: 	NDARRAY1D, //! numpy array with shape (n, )
58: 	NDARRAY2D, //! numpy array with shape (n_rows, n_cols)
59: 	LIST,      //! list of numpy arrays of shape (n,)
60: 	DICT,      //! dict of numpy arrays of shape (n,)
61: };
62: 
63: NumpyType ConvertNumpyType(const py::handle &col_type);
64: LogicalType NumpyToLogicalType(const NumpyType &col_type);
65: 
66: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/numpy/numpy_type.hpp]
[start of tools/pythonpkg/src/numpy/numpy_bind.cpp]
1: #include "duckdb_python/numpy/numpy_bind.hpp"
2: #include "duckdb_python/numpy/array_wrapper.hpp"
3: #include "duckdb_python/pandas/pandas_analyzer.hpp"
4: #include "duckdb_python/pandas/column/pandas_numpy_column.hpp"
5: #include "duckdb_python/pandas/pandas_bind.hpp"
6: #include "duckdb_python/numpy/numpy_type.hpp"
7: 
8: namespace duckdb {
9: 
10: void NumpyBind::Bind(const ClientContext &context, py::handle df, vector<PandasColumnBindData> &bind_columns,
11:                      vector<LogicalType> &return_types, vector<string> &names) {
12: 
13: 	auto df_columns = py::list(df.attr("keys")());
14: 	auto df_types = py::list();
15: 	for (auto item : py::cast<py::dict>(df)) {
16: 		if (string(py::str(item.second.attr("dtype").attr("char"))) == "U") {
17: 			df_types.attr("append")(py::str("string"));
18: 			continue;
19: 		}
20: 		df_types.attr("append")(py::str(item.second.attr("dtype")));
21: 	}
22: 	auto get_fun = df.attr("__getitem__");
23: 	if (py::len(df_columns) == 0 || py::len(df_types) == 0 || py::len(df_columns) != py::len(df_types)) {
24: 		throw InvalidInputException("Need a DataFrame with at least one column");
25: 	}
26: 	for (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {
27: 		LogicalType duckdb_col_type;
28: 		PandasColumnBindData bind_data;
29: 
30: 		names.emplace_back(py::str(df_columns[col_idx]));
31: 		bind_data.numpy_type = ConvertNumpyType(df_types[col_idx]);
32: 
33: 		auto column = get_fun(df_columns[col_idx]);
34: 
35: 		if (bind_data.numpy_type.type == NumpyNullableType::FLOAT_16) {
36: 			bind_data.pandas_col = make_uniq<PandasNumpyColumn>(py::array(column.attr("astype")("float32")));
37: 			bind_data.numpy_type.type = NumpyNullableType::FLOAT_32;
38: 			duckdb_col_type = NumpyToLogicalType(bind_data.numpy_type);
39: 		} else if (bind_data.numpy_type.type == NumpyNullableType::OBJECT &&
40: 		           string(py::str(df_types[col_idx])) == "string") {
41: 			bind_data.numpy_type.type = NumpyNullableType::CATEGORY;
42: 			// here we call numpy.unique
43: 			// this function call will return the unique values of a given array
44: 			// together with the indices to reconstruct the given array
45: 			auto uniq = py::cast<py::tuple>(py::module_::import("numpy").attr("unique")(column, false, true));
46: 			vector<string> enum_entries = py::cast<vector<string>>(uniq.attr("__getitem__")(0));
47: 			idx_t size = enum_entries.size();
48: 			Vector enum_entries_vec(LogicalType::VARCHAR, size);
49: 			auto enum_entries_ptr = FlatVector::GetData<string_t>(enum_entries_vec);
50: 			for (idx_t i = 0; i < size; i++) {
51: 				enum_entries_ptr[i] = StringVector::AddStringOrBlob(enum_entries_vec, enum_entries[i]);
52: 			}
53: 			duckdb_col_type = LogicalType::ENUM(enum_entries_vec, size);
54: 			auto pandas_col = uniq.attr("__getitem__")(1);
55: 			bind_data.internal_categorical_type = string(py::str(pandas_col.attr("dtype")));
56: 			bind_data.pandas_col = make_uniq<PandasNumpyColumn>(pandas_col);
57: 		} else {
58: 			bind_data.pandas_col = make_uniq<PandasNumpyColumn>(column);
59: 			duckdb_col_type = NumpyToLogicalType(bind_data.numpy_type);
60: 		}
61: 
62: 		if (bind_data.numpy_type.type == NumpyNullableType::OBJECT) {
63: 			PandasAnalyzer analyzer(context);
64: 			if (analyzer.Analyze(get_fun(df_columns[col_idx]))) {
65: 				duckdb_col_type = analyzer.AnalyzedType();
66: 			}
67: 		}
68: 
69: 		return_types.push_back(duckdb_col_type);
70: 		bind_columns.push_back(std::move(bind_data));
71: 	}
72: }
73: 
74: } // namespace duckdb
[end of tools/pythonpkg/src/numpy/numpy_bind.cpp]
[start of tools/pythonpkg/src/numpy/numpy_scan.cpp]
1: #include "duckdb_python/pyrelation.hpp"
2: #include "duckdb_python/pyconnection/pyconnection.hpp"
3: #include "duckdb_python/pyresult.hpp"
4: #include "duckdb_python/python_conversion.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/common/types/timestamp.hpp"
7: #include "utf8proc_wrapper.hpp"
8: #include "duckdb/common/case_insensitive_map.hpp"
9: #include "duckdb_python/pandas/pandas_bind.hpp"
10: #include "duckdb_python/numpy/numpy_type.hpp"
11: #include "duckdb_python/pandas/pandas_analyzer.hpp"
12: #include "duckdb_python/numpy/numpy_type.hpp"
13: #include "duckdb/function/scalar/nested_functions.hpp"
14: #include "duckdb_python/numpy/numpy_scan.hpp"
15: #include "duckdb_python/pandas/column/pandas_numpy_column.hpp"
16: 
17: namespace duckdb {
18: 
19: template <class T>
20: void ScanNumpyColumn(py::array &numpy_col, idx_t stride, idx_t offset, Vector &out, idx_t count) {
21: 	auto src_ptr = (T *)numpy_col.data();
22: 	if (stride == sizeof(T)) {
23: 		FlatVector::SetData(out, data_ptr_cast(src_ptr + offset));
24: 	} else {
25: 		auto tgt_ptr = (T *)FlatVector::GetData(out);
26: 		for (idx_t i = 0; i < count; i++) {
27: 			tgt_ptr[i] = src_ptr[stride / sizeof(T) * (i + offset)];
28: 		}
29: 	}
30: }
31: 
32: template <class T, class V>
33: void ScanNumpyCategoryTemplated(py::array &column, idx_t offset, Vector &out, idx_t count) {
34: 	auto src_ptr = (T *)column.data();
35: 	auto tgt_ptr = (V *)FlatVector::GetData(out);
36: 	auto &tgt_mask = FlatVector::Validity(out);
37: 	for (idx_t i = 0; i < count; i++) {
38: 		if (src_ptr[i + offset] == -1) {
39: 			// Null value
40: 			tgt_mask.SetInvalid(i);
41: 		} else {
42: 			tgt_ptr[i] = src_ptr[i + offset];
43: 		}
44: 	}
45: }
46: 
47: template <class T>
48: void ScanNumpyCategory(py::array &column, idx_t count, idx_t offset, Vector &out, string &src_type) {
49: 	if (src_type == "int8") {
50: 		ScanNumpyCategoryTemplated<int8_t, T>(column, offset, out, count);
51: 	} else if (src_type == "int16") {
52: 		ScanNumpyCategoryTemplated<int16_t, T>(column, offset, out, count);
53: 	} else if (src_type == "int32") {
54: 		ScanNumpyCategoryTemplated<int32_t, T>(column, offset, out, count);
55: 	} else if (src_type == "int64") {
56: 		ScanNumpyCategoryTemplated<int64_t, T>(column, offset, out, count);
57: 	} else {
58: 		throw NotImplementedException("The Pandas type " + src_type + " for categorical types is not implemented yet");
59: 	}
60: }
61: 
62: template <class T>
63: void ScanNumpyMasked(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {
64: 	D_ASSERT(bind_data.pandas_col->Backend() == PandasColumnBackend::NUMPY);
65: 	auto &numpy_col = reinterpret_cast<PandasNumpyColumn &>(*bind_data.pandas_col);
66: 	ScanNumpyColumn<T>(numpy_col.array, numpy_col.stride, offset, out, count);
67: 	auto &result_mask = FlatVector::Validity(out);
68: 	if (bind_data.mask) {
69: 		auto mask = reinterpret_cast<const bool *>(bind_data.mask->numpy_array.data());
70: 		for (idx_t i = 0; i < count; i++) {
71: 			auto is_null = mask[offset + i];
72: 			if (is_null) {
73: 				result_mask.SetInvalid(i);
74: 			}
75: 		}
76: 	}
77: }
78: 
79: template <class T>
80: void ScanNumpyFpColumn(const T *src_ptr, idx_t stride, idx_t count, idx_t offset, Vector &out) {
81: 	auto &mask = FlatVector::Validity(out);
82: 	if (stride == sizeof(T)) {
83: 		FlatVector::SetData(out, (data_ptr_t)(src_ptr + offset)); // NOLINT
84: 		// Turn NaN values into NULL
85: 		auto tgt_ptr = FlatVector::GetData<T>(out);
86: 		for (idx_t i = 0; i < count; i++) {
87: 			if (Value::IsNan<T>(tgt_ptr[i])) {
88: 				mask.SetInvalid(i);
89: 			}
90: 		}
91: 	} else {
92: 		auto tgt_ptr = FlatVector::GetData<T>(out);
93: 		for (idx_t i = 0; i < count; i++) {
94: 			tgt_ptr[i] = src_ptr[stride / sizeof(T) * (i + offset)];
95: 			if (Value::IsNan<T>(tgt_ptr[i])) {
96: 				mask.SetInvalid(i);
97: 			}
98: 		}
99: 	}
100: }
101: 
102: template <class T>
103: static string_t DecodePythonUnicode(T *codepoints, idx_t codepoint_count, Vector &out) {
104: 	// first figure out how many bytes to allocate
105: 	idx_t utf8_length = 0;
106: 	for (idx_t i = 0; i < codepoint_count; i++) {
107: 		int len = Utf8Proc::CodepointLength(int(codepoints[i]));
108: 		D_ASSERT(len >= 1);
109: 		utf8_length += len;
110: 	}
111: 	int sz;
112: 	auto result = StringVector::EmptyString(out, utf8_length);
113: 	auto target = result.GetDataWriteable();
114: 	for (idx_t i = 0; i < codepoint_count; i++) {
115: 		Utf8Proc::CodepointToUtf8(int(codepoints[i]), sz, target);
116: 		D_ASSERT(sz >= 1);
117: 		target += sz;
118: 	}
119: 	result.Finalize();
120: 	return result;
121: }
122: 
123: static void SetInvalidRecursive(Vector &out, idx_t index) {
124: 	auto &validity = FlatVector::Validity(out);
125: 	validity.SetInvalid(index);
126: 	if (out.GetType().InternalType() == PhysicalType::STRUCT) {
127: 		auto &children = StructVector::GetEntries(out);
128: 		for (idx_t i = 0; i < children.size(); i++) {
129: 			SetInvalidRecursive(*children[i], index);
130: 		}
131: 	}
132: }
133: 
134: //! 'count' is the amount of rows in the 'out' vector
135: //! 'offset' is the current row number within this vector
136: void ScanNumpyObject(PyObject *object, idx_t offset, Vector &out) {
137: 
138: 	// handle None
139: 	if (object == Py_None) {
140: 		SetInvalidRecursive(out, offset);
141: 		return;
142: 	}
143: 
144: 	auto val = TransformPythonValue(object, out.GetType());
145: 	// Check if the Value type is accepted for the LogicalType of Vector
146: 	out.SetValue(offset, val);
147: }
148: 
149: static void VerifyMapConstraints(Vector &vec, idx_t count) {
150: 	auto invalid_reason = MapVector::CheckMapValidity(vec, count);
151: 	switch (invalid_reason) {
152: 	case MapInvalidReason::VALID:
153: 		return;
154: 	case MapInvalidReason::DUPLICATE_KEY:
155: 		throw InvalidInputException("Dict->Map conversion failed because 'key' list contains duplicates");
156: 	case MapInvalidReason::NULL_KEY:
157: 		throw InvalidInputException("Dict->Map conversion failed because 'key' list contains None");
158: 	default:
159: 		throw InvalidInputException("Option not implemented for MapInvalidReason");
160: 	}
161: }
162: 
163: void VerifyTypeConstraints(Vector &vec, idx_t count) {
164: 	switch (vec.GetType().id()) {
165: 	case LogicalTypeId::MAP: {
166: 		VerifyMapConstraints(vec, count);
167: 		break;
168: 	}
169: 	default:
170: 		return;
171: 	}
172: }
173: 
174: void NumpyScan::ScanObjectColumn(PyObject **col, idx_t stride, idx_t count, idx_t offset, Vector &out) {
175: 	// numpy_col is a sequential list of objects, that make up one "column" (Vector)
176: 	out.SetVectorType(VectorType::FLAT_VECTOR);
177: 	auto &mask = FlatVector::Validity(out);
178: 	PythonGILWrapper gil; // We're creating python objects here, so we need the GIL
179: 
180: 	if (stride == sizeof(PyObject *)) {
181: 		auto src_ptr = col + offset;
182: 		for (idx_t i = 0; i < count; i++) {
183: 			ScanNumpyObject(src_ptr[i], i, out);
184: 		}
185: 	} else {
186: 		for (idx_t i = 0; i < count; i++) {
187: 			auto src_ptr = col[stride / sizeof(PyObject *) * (i + offset)];
188: 			ScanNumpyObject(src_ptr, i, out);
189: 		}
190: 	}
191: 	VerifyTypeConstraints(out, count);
192: }
193: 
194: //! 'offset' is the offset within the column
195: //! 'count' is the amount of values we will convert in this batch
196: void NumpyScan::Scan(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {
197: 	D_ASSERT(bind_data.pandas_col->Backend() == PandasColumnBackend::NUMPY);
198: 	auto &numpy_col = reinterpret_cast<PandasNumpyColumn &>(*bind_data.pandas_col);
199: 	auto &array = numpy_col.array;
200: 
201: 	switch (bind_data.numpy_type.type) {
202: 	case NumpyNullableType::BOOL:
203: 		ScanNumpyMasked<bool>(bind_data, count, offset, out);
204: 		break;
205: 	case NumpyNullableType::UINT_8:
206: 		ScanNumpyMasked<uint8_t>(bind_data, count, offset, out);
207: 		break;
208: 	case NumpyNullableType::UINT_16:
209: 		ScanNumpyMasked<uint16_t>(bind_data, count, offset, out);
210: 		break;
211: 	case NumpyNullableType::UINT_32:
212: 		ScanNumpyMasked<uint32_t>(bind_data, count, offset, out);
213: 		break;
214: 	case NumpyNullableType::UINT_64:
215: 		ScanNumpyMasked<uint64_t>(bind_data, count, offset, out);
216: 		break;
217: 	case NumpyNullableType::INT_8:
218: 		ScanNumpyMasked<int8_t>(bind_data, count, offset, out);
219: 		break;
220: 	case NumpyNullableType::INT_16:
221: 		ScanNumpyMasked<int16_t>(bind_data, count, offset, out);
222: 		break;
223: 	case NumpyNullableType::INT_32:
224: 		ScanNumpyMasked<int32_t>(bind_data, count, offset, out);
225: 		break;
226: 	case NumpyNullableType::INT_64:
227: 		ScanNumpyMasked<int64_t>(bind_data, count, offset, out);
228: 		break;
229: 	case NumpyNullableType::FLOAT_32:
230: 		ScanNumpyFpColumn<float>(reinterpret_cast<const float *>(array.data()), numpy_col.stride, count, offset, out);
231: 		break;
232: 	case NumpyNullableType::FLOAT_64:
233: 		ScanNumpyFpColumn<double>(reinterpret_cast<const double *>(array.data()), numpy_col.stride, count, offset, out);
234: 		break;
235: 	case NumpyNullableType::DATETIME_NS:
236: 	case NumpyNullableType::DATETIME_MS:
237: 	case NumpyNullableType::DATETIME_US:
238: 	case NumpyNullableType::DATETIME_S: {
239: 		auto src_ptr = reinterpret_cast<const int64_t *>(array.data());
240: 		auto tgt_ptr = FlatVector::GetData<timestamp_t>(out);
241: 		auto &mask = FlatVector::Validity(out);
242: 
243: 		using timestamp_convert_func = std::function<timestamp_t(int64_t)>;
244: 		timestamp_convert_func convert_func;
245: 
246: 		switch (bind_data.numpy_type.type) {
247: 		case NumpyNullableType::DATETIME_NS:
248: 			if (bind_data.numpy_type.has_timezone) {
249: 				// Our timezone type is US, so we need to convert from NS to US
250: 				convert_func = Timestamp::FromEpochNanoSeconds;
251: 			} else {
252: 				convert_func = Timestamp::FromEpochMicroSeconds;
253: 			}
254: 			break;
255: 		case NumpyNullableType::DATETIME_MS:
256: 			if (bind_data.numpy_type.has_timezone) {
257: 				// Our timezone type is US, so we need to convert from MS to US
258: 				convert_func = Timestamp::FromEpochMs;
259: 			} else {
260: 				convert_func = Timestamp::FromEpochMicroSeconds;
261: 			}
262: 			break;
263: 		case NumpyNullableType::DATETIME_US:
264: 			convert_func = Timestamp::FromEpochMicroSeconds;
265: 			break;
266: 		case NumpyNullableType::DATETIME_S:
267: 			if (bind_data.numpy_type.has_timezone) {
268: 				// Our timezone type is US, so we need to convert from S to US
269: 				convert_func = Timestamp::FromEpochSeconds;
270: 			} else {
271: 				convert_func = Timestamp::FromEpochMicroSeconds;
272: 			}
273: 			break;
274: 		default:
275: 			throw NotImplementedException("Scan for datetime of this type is not supported yet");
276: 		};
277: 
278: 		for (idx_t row = 0; row < count; row++) {
279: 			auto source_idx = offset + row;
280: 			if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
281: 				// pandas Not a Time (NaT)
282: 				mask.SetInvalid(row);
283: 				continue;
284: 			}
285: 			// Direct conversion, we've already matched the numpy type with the equivalent duckdb type
286: 			auto input = timestamp_t(src_ptr[source_idx]);
287: 			if (Timestamp::IsFinite(input)) {
288: 				tgt_ptr[row] = convert_func(src_ptr[source_idx]);
289: 			} else {
290: 				tgt_ptr[row] = input;
291: 			}
292: 		}
293: 		break;
294: 	}
295: 	case NumpyNullableType::TIMEDELTA: {
296: 		auto src_ptr = reinterpret_cast<const int64_t *>(array.data());
297: 		auto tgt_ptr = FlatVector::GetData<interval_t>(out);
298: 		auto &mask = FlatVector::Validity(out);
299: 
300: 		for (idx_t row = 0; row < count; row++) {
301: 			auto source_idx = offset + row;
302: 			if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
303: 				// pandas Not a Time (NaT)
304: 				mask.SetInvalid(row);
305: 				continue;
306: 			}
307: 			int64_t micro = src_ptr[source_idx] / 1000;
308: 			int64_t days = micro / Interval::MICROS_PER_DAY;
309: 			micro = micro % Interval::MICROS_PER_DAY;
310: 			int64_t months = days / Interval::DAYS_PER_MONTH;
311: 			days = days % Interval::DAYS_PER_MONTH;
312: 			interval_t interval;
313: 			interval.months = months;
314: 			interval.days = days;
315: 			interval.micros = micro;
316: 			tgt_ptr[row] = interval;
317: 		}
318: 		break;
319: 	}
320: 	case NumpyNullableType::OBJECT: {
321: 		//! We have determined the underlying logical type of this object column
322: 		// Get the source pointer of the numpy array
323: 		auto src_ptr = (PyObject **)array.data(); // NOLINT
324: 		if (out.GetType().id() != LogicalTypeId::VARCHAR) {
325: 			return NumpyScan::ScanObjectColumn(src_ptr, numpy_col.stride, count, offset, out);
326: 		}
327: 
328: 		// Get the data pointer and the validity mask of the result vector
329: 		auto tgt_ptr = FlatVector::GetData<string_t>(out);
330: 		auto &out_mask = FlatVector::Validity(out);
331: 		unique_ptr<PythonGILWrapper> gil;
332: 		auto &import_cache = *DuckDBPyConnection::ImportCache();
333: 
334: 		// Loop over every row of the arrays contents
335: 		auto stride = numpy_col.stride;
336: 		for (idx_t row = 0; row < count; row++) {
337: 			auto source_idx = stride / sizeof(PyObject *) * (row + offset);
338: 
339: 			// Get the pointer to the object
340: 			PyObject *val = src_ptr[source_idx];
341: 			if (bind_data.numpy_type.type == NumpyNullableType::OBJECT && !py::isinstance<py::str>(val)) {
342: 				if (val == Py_None) {
343: 					out_mask.SetInvalid(row);
344: 					continue;
345: 				}
346: 				if (import_cache.pandas.NaT(false)) {
347: 					// If pandas is imported, check if this is pandas.NaT
348: 					py::handle value(val);
349: 					if (value.is(import_cache.pandas.NaT())) {
350: 						out_mask.SetInvalid(row);
351: 						continue;
352: 					}
353: 				}
354: 				if (import_cache.pandas.NA(false)) {
355: 					// If pandas is imported, check if this is pandas.NA
356: 					py::handle value(val);
357: 					if (value.is(import_cache.pandas.NA())) {
358: 						out_mask.SetInvalid(row);
359: 						continue;
360: 					}
361: 				}
362: 				if (py::isinstance<py::float_>(val) && std::isnan(PyFloat_AsDouble(val))) {
363: 					out_mask.SetInvalid(row);
364: 					continue;
365: 				}
366: 				if (!py::isinstance<py::str>(val)) {
367: 					if (!gil) {
368: 						gil = make_uniq<PythonGILWrapper>();
369: 					}
370: 					bind_data.object_str_val.Push(std::move(py::str(val)));
371: 					val = reinterpret_cast<PyObject *>(bind_data.object_str_val.LastAddedObject().ptr());
372: 				}
373: 			}
374: 			// Python 3 string representation:
375: 			// https://github.com/python/cpython/blob/3a8fdb28794b2f19f6c8464378fb8b46bce1f5f4/Include/cpython/unicodeobject.h#L79
376: 			py::handle val_handle(val);
377: 			if (!py::isinstance<py::str>(val_handle)) {
378: 				out_mask.SetInvalid(row);
379: 				continue;
380: 			}
381: 			if (PyUtil::PyUnicodeIsCompactASCII(val_handle)) {
382: 				// ascii string: we can zero copy
383: 				tgt_ptr[row] = string_t(PyUtil::PyUnicodeData(val_handle), PyUtil::PyUnicodeGetLength(val_handle));
384: 			} else {
385: 				// unicode gunk
386: 				auto ascii_obj = reinterpret_cast<PyASCIIObject *>(val);
387: 				auto unicode_obj = reinterpret_cast<PyCompactUnicodeObject *>(val);
388: 				// compact unicode string: is there utf8 data available?
389: 				if (unicode_obj->utf8) {
390: 					// there is! zero copy
391: 					tgt_ptr[row] = string_t(const_char_ptr_cast(unicode_obj->utf8), unicode_obj->utf8_length);
392: 				} else if (PyUtil::PyUnicodeIsCompact(unicode_obj) &&
393: 				           !PyUtil::PyUnicodeIsASCII(unicode_obj)) { // NOLINT
394: 					auto kind = PyUtil::PyUnicodeKind(val_handle);
395: 					switch (kind) {
396: 					case PyUnicode_1BYTE_KIND:
397: 						tgt_ptr[row] = DecodePythonUnicode<Py_UCS1>(PyUtil::PyUnicode1ByteData(val_handle),
398: 						                                            PyUtil::PyUnicodeGetLength(val_handle), out);
399: 						break;
400: 					case PyUnicode_2BYTE_KIND:
401: 						tgt_ptr[row] = DecodePythonUnicode<Py_UCS2>(PyUtil::PyUnicode2ByteData(val_handle),
402: 						                                            PyUtil::PyUnicodeGetLength(val_handle), out);
403: 						break;
404: 					case PyUnicode_4BYTE_KIND:
405: 						tgt_ptr[row] = DecodePythonUnicode<Py_UCS4>(PyUtil::PyUnicode4ByteData(val_handle),
406: 						                                            PyUtil::PyUnicodeGetLength(val_handle), out);
407: 						break;
408: 					default:
409: 						throw NotImplementedException(
410: 						    "Unsupported typekind constant %d for Python Unicode Compact decode", kind);
411: 					}
412: 				} else {
413: 					throw InvalidInputException("Unsupported string type: no clue what this string is");
414: 				}
415: 			}
416: 		}
417: 		break;
418: 	}
419: 	case NumpyNullableType::CATEGORY: {
420: 		switch (out.GetType().InternalType()) {
421: 		case PhysicalType::UINT8:
422: 			ScanNumpyCategory<uint8_t>(array, count, offset, out, bind_data.internal_categorical_type);
423: 			break;
424: 		case PhysicalType::UINT16:
425: 			ScanNumpyCategory<uint16_t>(array, count, offset, out, bind_data.internal_categorical_type);
426: 			break;
427: 		case PhysicalType::UINT32:
428: 			ScanNumpyCategory<uint32_t>(array, count, offset, out, bind_data.internal_categorical_type);
429: 			break;
430: 		default:
431: 			throw InternalException("Invalid Physical Type for ENUMs");
432: 		}
433: 		break;
434: 	}
435: 
436: 	default:
437: 		throw NotImplementedException("Unsupported pandas type");
438: 	}
439: }
440: 
441: } // namespace duckdb
[end of tools/pythonpkg/src/numpy/numpy_scan.cpp]
[start of tools/pythonpkg/src/numpy/type.cpp]
1: #include "duckdb_python/numpy/numpy_type.hpp"
2: #include "duckdb/common/string.hpp"
3: #include "duckdb/common/to_string.hpp"
4: #include "duckdb/common/string_util.hpp"
5: #include <exception>
6: 
7: namespace duckdb {
8: 
9: static bool IsDateTime(NumpyNullableType type) {
10: 	switch (type) {
11: 	case NumpyNullableType::DATETIME_NS:
12: 	case NumpyNullableType::DATETIME_S:
13: 	case NumpyNullableType::DATETIME_MS:
14: 	case NumpyNullableType::DATETIME_US:
15: 		return true;
16: 	default:
17: 		return false;
18: 	};
19: }
20: 
21: static NumpyNullableType ConvertNumpyTypeInternal(const string &col_type_str) {
22: 	if (col_type_str == "bool" || col_type_str == "boolean") {
23: 		return NumpyNullableType::BOOL;
24: 	}
25: 	if (col_type_str == "uint8" || col_type_str == "UInt8") {
26: 		return NumpyNullableType::UINT_8;
27: 	}
28: 	if (col_type_str == "uint16" || col_type_str == "UInt16") {
29: 		return NumpyNullableType::UINT_16;
30: 	}
31: 	if (col_type_str == "uint32" || col_type_str == "UInt32") {
32: 		return NumpyNullableType::UINT_32;
33: 	}
34: 	if (col_type_str == "uint64" || col_type_str == "UInt64") {
35: 		return NumpyNullableType::UINT_64;
36: 	}
37: 	if (col_type_str == "int8" || col_type_str == "Int8") {
38: 		return NumpyNullableType::INT_8;
39: 	}
40: 	if (col_type_str == "int16" || col_type_str == "Int16") {
41: 		return NumpyNullableType::INT_16;
42: 	}
43: 	if (col_type_str == "int32" || col_type_str == "Int32") {
44: 		return NumpyNullableType::INT_32;
45: 	}
46: 	if (col_type_str == "int64" || col_type_str == "Int64") {
47: 		return NumpyNullableType::INT_64;
48: 	}
49: 	if (col_type_str == "float16" || col_type_str == "Float16") {
50: 		return NumpyNullableType::FLOAT_16;
51: 	}
52: 	if (col_type_str == "float32" || col_type_str == "Float32") {
53: 		return NumpyNullableType::FLOAT_32;
54: 	}
55: 	if (col_type_str == "float64" || col_type_str == "Float64") {
56: 		return NumpyNullableType::FLOAT_64;
57: 	}
58: 	if (col_type_str == "object" || col_type_str == "string") {
59: 		//! this better be castable to strings
60: 		return NumpyNullableType::OBJECT;
61: 	}
62: 	if (col_type_str == "timedelta64[ns]") {
63: 		return NumpyNullableType::TIMEDELTA;
64: 	}
65: 	// We use 'StartsWith' because it might have ', tz' at the end, indicating timezone
66: 	if (StringUtil::StartsWith(col_type_str, "datetime64[ns")) {
67: 		return NumpyNullableType::DATETIME_NS;
68: 	}
69: 	if (StringUtil::StartsWith(col_type_str, "datetime64[us")) {
70: 		return NumpyNullableType::DATETIME_US;
71: 	}
72: 	if (StringUtil::StartsWith(col_type_str, "datetime64[ms")) {
73: 		return NumpyNullableType::DATETIME_MS;
74: 	}
75: 	if (StringUtil::StartsWith(col_type_str, "datetime64[s")) {
76: 		return NumpyNullableType::DATETIME_S;
77: 	}
78: 	// Legacy datetime type indicators
79: 	if (StringUtil::StartsWith(col_type_str, "<M8[ns")) {
80: 		return NumpyNullableType::DATETIME_NS;
81: 	}
82: 	if (StringUtil::StartsWith(col_type_str, "<M8[s")) {
83: 		return NumpyNullableType::DATETIME_S;
84: 	}
85: 	if (StringUtil::StartsWith(col_type_str, "<M8[us")) {
86: 		return NumpyNullableType::DATETIME_US;
87: 	}
88: 	if (StringUtil::StartsWith(col_type_str, "<M8[ms")) {
89: 		return NumpyNullableType::DATETIME_MS;
90: 	}
91: 	if (col_type_str == "category") {
92: 		return NumpyNullableType::CATEGORY;
93: 	}
94: 	throw NotImplementedException("Data type '%s' not recognized", col_type_str);
95: }
96: 
97: NumpyType ConvertNumpyType(const py::handle &col_type) {
98: 	auto col_type_str = string(py::str(col_type));
99: 	NumpyType numpy_type;
100: 
101: 	numpy_type.type = ConvertNumpyTypeInternal(col_type_str);
102: 	if (IsDateTime(numpy_type.type)) {
103: 		if (hasattr(col_type, "tz")) {
104: 			// The datetime has timezone information.
105: 			numpy_type.has_timezone = true;
106: 		}
107: 	}
108: 	return numpy_type;
109: }
110: 
111: LogicalType NumpyToLogicalType(const NumpyType &col_type) {
112: 	switch (col_type.type) {
113: 	case NumpyNullableType::BOOL:
114: 		return LogicalType::BOOLEAN;
115: 	case NumpyNullableType::INT_8:
116: 		return LogicalType::TINYINT;
117: 	case NumpyNullableType::UINT_8:
118: 		return LogicalType::UTINYINT;
119: 	case NumpyNullableType::INT_16:
120: 		return LogicalType::SMALLINT;
121: 	case NumpyNullableType::UINT_16:
122: 		return LogicalType::USMALLINT;
123: 	case NumpyNullableType::INT_32:
124: 		return LogicalType::INTEGER;
125: 	case NumpyNullableType::UINT_32:
126: 		return LogicalType::UINTEGER;
127: 	case NumpyNullableType::INT_64:
128: 		return LogicalType::BIGINT;
129: 	case NumpyNullableType::UINT_64:
130: 		return LogicalType::UBIGINT;
131: 	case NumpyNullableType::FLOAT_16:
132: 		return LogicalType::FLOAT;
133: 	case NumpyNullableType::FLOAT_32:
134: 		return LogicalType::FLOAT;
135: 	case NumpyNullableType::FLOAT_64:
136: 		return LogicalType::DOUBLE;
137: 	case NumpyNullableType::OBJECT:
138: 		return LogicalType::VARCHAR;
139: 	case NumpyNullableType::TIMEDELTA:
140: 		return LogicalType::INTERVAL;
141: 	case NumpyNullableType::DATETIME_MS: {
142: 		if (col_type.has_timezone) {
143: 			return LogicalType::TIMESTAMP_TZ;
144: 		}
145: 		return LogicalType::TIMESTAMP_MS;
146: 	}
147: 	case NumpyNullableType::DATETIME_NS: {
148: 		if (col_type.has_timezone) {
149: 			return LogicalType::TIMESTAMP_TZ;
150: 		}
151: 		return LogicalType::TIMESTAMP_NS;
152: 	}
153: 	case NumpyNullableType::DATETIME_S: {
154: 		if (col_type.has_timezone) {
155: 			return LogicalType::TIMESTAMP_TZ;
156: 		}
157: 		return LogicalType::TIMESTAMP_S;
158: 	}
159: 	case NumpyNullableType::DATETIME_US: {
160: 		if (col_type.has_timezone) {
161: 			return LogicalType::TIMESTAMP_TZ;
162: 		}
163: 		return LogicalType::TIMESTAMP;
164: 	}
165: 	default:
166: 		throw InternalException("No known conversion for NumpyNullableType '%d' to LogicalType");
167: 	}
168: }
169: 
170: } // namespace duckdb
[end of tools/pythonpkg/src/numpy/type.cpp]
[start of tools/pythonpkg/src/pandas/scan.cpp]
1: #include "duckdb_python/pandas/pandas_scan.hpp"
2: #include "duckdb_python/pandas/pandas_bind.hpp"
3: #include "duckdb_python/numpy/array_wrapper.hpp"
4: #include "utf8proc_wrapper.hpp"
5: #include "duckdb/common/types/timestamp.hpp"
6: #include "duckdb_python/numpy/numpy_scan.hpp"
7: #include "duckdb_python/numpy/numpy_bind.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb_python/pandas/column/pandas_numpy_column.hpp"
10: #include "duckdb/parser/tableref/table_function_ref.hpp"
11: 
12: #include "duckdb/common/atomic.hpp"
13: 
14: namespace duckdb {
15: 
16: struct PandasScanFunctionData : public TableFunctionData {
17: 	PandasScanFunctionData(py::handle df, idx_t row_count, vector<PandasColumnBindData> pandas_bind_data,
18: 	                       vector<LogicalType> sql_types, shared_ptr<DependencyItem> dependency)
19: 	    : df(df), row_count(row_count), lines_read(0), pandas_bind_data(std::move(pandas_bind_data)),
20: 	      sql_types(std::move(sql_types)), copied_df(std::move(dependency)) {
21: 	}
22: 	py::handle df;
23: 	idx_t row_count;
24: 	atomic<idx_t> lines_read;
25: 	vector<PandasColumnBindData> pandas_bind_data;
26: 	vector<LogicalType> sql_types;
27: 	shared_ptr<DependencyItem> copied_df;
28: 
29: 	~PandasScanFunctionData() override {
30: 		try {
31: 			py::gil_scoped_acquire acquire;
32: 			pandas_bind_data.clear();
33: 		} catch (...) { // NOLINT
34: 		}
35: 	}
36: };
37: 
38: struct PandasScanLocalState : public LocalTableFunctionState {
39: 	PandasScanLocalState(idx_t start, idx_t end) : start(start), end(end), batch_index(0) {
40: 	}
41: 
42: 	idx_t start;
43: 	idx_t end;
44: 	idx_t batch_index;
45: 	vector<column_t> column_ids;
46: };
47: 
48: struct PandasScanGlobalState : public GlobalTableFunctionState {
49: 	explicit PandasScanGlobalState(idx_t max_threads) : position(0), batch_index(0), max_threads(max_threads) {
50: 	}
51: 
52: 	std::mutex lock;
53: 	idx_t position;
54: 	idx_t batch_index;
55: 	idx_t max_threads;
56: 
57: 	idx_t MaxThreads() const override {
58: 		return max_threads;
59: 	}
60: };
61: 
62: PandasScanFunction::PandasScanFunction()
63:     : TableFunction("pandas_scan", {LogicalType::POINTER}, PandasScanFunc, PandasScanBind, PandasScanInitGlobal,
64:                     PandasScanInitLocal) {
65: 	get_batch_index = PandasScanGetBatchIndex;
66: 	cardinality = PandasScanCardinality;
67: 	table_scan_progress = PandasProgress;
68: 	serialize = PandasSerialize;
69: 	projection_pushdown = true;
70: }
71: 
72: idx_t PandasScanFunction::PandasScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,
73:                                                   LocalTableFunctionState *local_state,
74:                                                   GlobalTableFunctionState *global_state) {
75: 	auto &data = local_state->Cast<PandasScanLocalState>();
76: 	return data.batch_index;
77: }
78: 
79: unique_ptr<FunctionData> PandasScanFunction::PandasScanBind(ClientContext &context, TableFunctionBindInput &input,
80:                                                             vector<LogicalType> &return_types, vector<string> &names) {
81: 	py::gil_scoped_acquire acquire;
82: 	py::handle df(reinterpret_cast<PyObject *>(input.inputs[0].GetPointer()));
83: 
84: 	vector<PandasColumnBindData> pandas_bind_data;
85: 
86: 	auto is_py_dict = py::isinstance<py::dict>(df);
87: 	if (is_py_dict) {
88: 		NumpyBind::Bind(context, df, pandas_bind_data, return_types, names);
89: 	} else {
90: 		Pandas::Bind(context, df, pandas_bind_data, return_types, names);
91: 	}
92: 	auto df_columns = py::list(df.attr("keys")());
93: 
94: 	auto &ref = input.ref;
95: 
96: 	shared_ptr<DependencyItem> dependency_item;
97: 	if (ref.external_dependency) {
98: 		// This was created during the replacement scan (see python_replacement_scan.cpp)
99: 		dependency_item = ref.external_dependency->GetDependency("copy");
100: 		D_ASSERT(dependency_item);
101: 	}
102: 
103: 	auto get_fun = df.attr("__getitem__");
104: 	idx_t row_count = py::len(get_fun(df_columns[0]));
105: 	return make_uniq<PandasScanFunctionData>(df, row_count, std::move(pandas_bind_data), return_types, dependency_item);
106: }
107: 
108: unique_ptr<GlobalTableFunctionState> PandasScanFunction::PandasScanInitGlobal(ClientContext &context,
109:                                                                               TableFunctionInitInput &input) {
110: 	if (PyGILState_Check()) {
111: 		throw InvalidInputException("PandasScan called but GIL was already held!");
112: 	}
113: 	return make_uniq<PandasScanGlobalState>(PandasScanMaxThreads(context, input.bind_data.get()));
114: }
115: 
116: unique_ptr<LocalTableFunctionState> PandasScanFunction::PandasScanInitLocal(ExecutionContext &context,
117:                                                                             TableFunctionInitInput &input,
118:                                                                             GlobalTableFunctionState *gstate) {
119: 	auto result = make_uniq<PandasScanLocalState>(0, 0);
120: 	result->column_ids = input.column_ids;
121: 	PandasScanParallelStateNext(context.client, input.bind_data.get(), result.get(), gstate);
122: 	return std::move(result);
123: }
124: 
125: idx_t PandasScanFunction::PandasScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p) {
126: 	if (ClientConfig::GetConfig(context).verify_parallelism) {
127: 		return context.db->NumberOfThreads();
128: 	}
129: 	auto &bind_data = bind_data_p->Cast<PandasScanFunctionData>();
130: 	return bind_data.row_count / PANDAS_PARTITION_COUNT + 1;
131: }
132: 
133: bool PandasScanFunction::PandasScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
134:                                                      LocalTableFunctionState *lstate,
135:                                                      GlobalTableFunctionState *gstate) {
136: 	auto &bind_data = bind_data_p->Cast<PandasScanFunctionData>();
137: 	auto &parallel_state = gstate->Cast<PandasScanGlobalState>();
138: 	auto &state = lstate->Cast<PandasScanLocalState>();
139: 
140: 	lock_guard<mutex> parallel_lock(parallel_state.lock);
141: 	if (parallel_state.position >= bind_data.row_count) {
142: 		return false;
143: 	}
144: 	state.start = parallel_state.position;
145: 	parallel_state.position += PANDAS_PARTITION_COUNT;
146: 	if (parallel_state.position > bind_data.row_count) {
147: 		parallel_state.position = bind_data.row_count;
148: 	}
149: 	state.end = parallel_state.position;
150: 	state.batch_index = parallel_state.batch_index++;
151: 	return true;
152: }
153: 
154: double PandasScanFunction::PandasProgress(ClientContext &context, const FunctionData *bind_data_p,
155:                                           const GlobalTableFunctionState *gstate) {
156: 	auto &bind_data = bind_data_p->Cast<PandasScanFunctionData>();
157: 	if (bind_data.row_count == 0) {
158: 		return 100;
159: 	}
160: 	auto percentage = (bind_data.lines_read * 100.0) / bind_data.row_count;
161: 	return percentage;
162: }
163: 
164: void PandasScanFunction::PandasBackendScanSwitch(PandasColumnBindData &bind_data, idx_t count, idx_t offset,
165:                                                  Vector &out) {
166: 	auto backend = bind_data.pandas_col->Backend();
167: 	switch (backend) {
168: 	case PandasColumnBackend::NUMPY: {
169: 		NumpyScan::Scan(bind_data, count, offset, out);
170: 		break;
171: 	}
172: 	default: {
173: 		throw NotImplementedException("Type not implemented for PandasColumnBackend");
174: 	}
175: 	}
176: }
177: 
178: //! The main pandas scan function: note that this can be called in parallel without the GIL
179: //! hence this needs to be GIL-safe, i.e. no methods that create Python objects are allowed
180: void PandasScanFunction::PandasScanFunc(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {
181: 	auto &data = data_p.bind_data->CastNoConst<PandasScanFunctionData>();
182: 	auto &state = data_p.local_state->Cast<PandasScanLocalState>();
183: 
184: 	if (state.start >= state.end) {
185: 		if (!PandasScanParallelStateNext(context, data_p.bind_data.get(), data_p.local_state.get(),
186: 		                                 data_p.global_state.get())) {
187: 			return;
188: 		}
189: 	}
190: 	idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, state.end - state.start);
191: 	output.SetCardinality(this_count);
192: 	for (idx_t idx = 0; idx < state.column_ids.size(); idx++) {
193: 		auto col_idx = state.column_ids[idx];
194: 		if (col_idx == COLUMN_IDENTIFIER_ROW_ID) {
195: 			output.data[idx].Sequence(state.start, 1, this_count);
196: 		} else {
197: 			PandasBackendScanSwitch(data.pandas_bind_data[col_idx], this_count, state.start, output.data[idx]);
198: 		}
199: 	}
200: 	state.start += this_count;
201: 	data.lines_read += this_count;
202: }
203: 
204: unique_ptr<NodeStatistics> PandasScanFunction::PandasScanCardinality(ClientContext &context,
205:                                                                      const FunctionData *bind_data) {
206: 	auto &data = bind_data->Cast<PandasScanFunctionData>();
207: 	return make_uniq<NodeStatistics>(data.row_count, data.row_count);
208: }
209: 
210: py::object PandasScanFunction::PandasReplaceCopiedNames(const py::object &original_df) {
211: 	py::object copy_df = original_df.attr("copy")(false);
212: 	auto df_columns = py::list(original_df.attr("columns"));
213: 	vector<string> columns;
214: 	for (const auto &str : df_columns) {
215: 		columns.push_back(string(py::str(str)));
216: 	}
217: 	QueryResult::DeduplicateColumns(columns);
218: 
219: 	py::list new_columns(columns.size());
220: 	for (idx_t i = 0; i < columns.size(); i++) {
221: 		new_columns[i] = std::move(columns[i]);
222: 	}
223: 	copy_df.attr("columns") = std::move(new_columns);
224: 	columns.clear();
225: 	return copy_df;
226: }
227: 
228: void PandasScanFunction::PandasSerialize(Serializer &serializer, const optional_ptr<FunctionData> bind_data,
229:                                          const TableFunction &function) {
230: 	throw NotImplementedException("PandasScan function cannot be serialized");
231: }
232: 
233: } // namespace duckdb
[end of tools/pythonpkg/src/pandas/scan.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: