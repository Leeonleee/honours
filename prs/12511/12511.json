{
  "repo": "duckdb/duckdb",
  "pull_number": 12511,
  "instance_id": "duckdb__duckdb-12511",
  "issue_numbers": [
    "12507"
  ],
  "base_commit": "7d742ed325e286d92fdb97df49e909ebc22a3159",
  "patch": "diff --git a/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_type.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_type.hpp\nindex ab26b22b7a29..982f00ecf67d 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_type.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_type.hpp\n@@ -42,6 +42,7 @@ enum class NumpyNullableType : uint8_t {\n \t//! Extension Types\n \t//! ------------------------------------------------------------\n \tCATEGORY, //! category\n+\tSTRING,   //! string\n };\n \n struct NumpyType {\ndiff --git a/tools/pythonpkg/src/numpy/numpy_bind.cpp b/tools/pythonpkg/src/numpy/numpy_bind.cpp\nindex ae334badc727..0ffef850312f 100644\n--- a/tools/pythonpkg/src/numpy/numpy_bind.cpp\n+++ b/tools/pythonpkg/src/numpy/numpy_bind.cpp\n@@ -36,8 +36,7 @@ void NumpyBind::Bind(const ClientContext &context, py::handle df, vector<PandasC\n \t\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(py::array(column.attr(\"astype\")(\"float32\")));\n \t\t\tbind_data.numpy_type.type = NumpyNullableType::FLOAT_32;\n \t\t\tduckdb_col_type = NumpyToLogicalType(bind_data.numpy_type);\n-\t\t} else if (bind_data.numpy_type.type == NumpyNullableType::OBJECT &&\n-\t\t           string(py::str(df_types[col_idx])) == \"string\") {\n+\t\t} else if (bind_data.numpy_type.type == NumpyNullableType::STRING) {\n \t\t\tbind_data.numpy_type.type = NumpyNullableType::CATEGORY;\n \t\t\t// here we call numpy.unique\n \t\t\t// this function call will return the unique values of a given array\ndiff --git a/tools/pythonpkg/src/numpy/numpy_scan.cpp b/tools/pythonpkg/src/numpy/numpy_scan.cpp\nindex b4b1d3dbe276..ff8591ee0ff4 100644\n--- a/tools/pythonpkg/src/numpy/numpy_scan.cpp\n+++ b/tools/pythonpkg/src/numpy/numpy_scan.cpp\n@@ -317,11 +317,13 @@ void NumpyScan::Scan(PandasColumnBindData &bind_data, idx_t count, idx_t offset,\n \t\t}\n \t\tbreak;\n \t}\n+\tcase NumpyNullableType::STRING:\n \tcase NumpyNullableType::OBJECT: {\n-\t\t//! We have determined the underlying logical type of this object column\n \t\t// Get the source pointer of the numpy array\n \t\tauto src_ptr = (PyObject **)array.data(); // NOLINT\n-\t\tif (out.GetType().id() != LogicalTypeId::VARCHAR) {\n+\t\tconst bool is_object_col = bind_data.numpy_type.type == NumpyNullableType::OBJECT;\n+\t\tif (is_object_col && out.GetType().id() != LogicalTypeId::VARCHAR) {\n+\t\t\t//! We have determined the underlying logical type of this object column\n \t\t\treturn NumpyScan::ScanObjectColumn(src_ptr, numpy_col.stride, count, offset, out);\n \t\t}\n \n@@ -338,7 +340,7 @@ void NumpyScan::Scan(PandasColumnBindData &bind_data, idx_t count, idx_t offset,\n \n \t\t\t// Get the pointer to the object\n \t\t\tPyObject *val = src_ptr[source_idx];\n-\t\t\tif (bind_data.numpy_type.type == NumpyNullableType::OBJECT && !py::isinstance<py::str>(val)) {\n+\t\t\tif (!py::isinstance<py::str>(val)) {\n \t\t\t\tif (val == Py_None) {\n \t\t\t\t\tout_mask.SetInvalid(row);\n \t\t\t\t\tcontinue;\ndiff --git a/tools/pythonpkg/src/numpy/type.cpp b/tools/pythonpkg/src/numpy/type.cpp\nindex 00d86cab4df6..92ac478574c2 100644\n--- a/tools/pythonpkg/src/numpy/type.cpp\n+++ b/tools/pythonpkg/src/numpy/type.cpp\n@@ -55,8 +55,10 @@ static NumpyNullableType ConvertNumpyTypeInternal(const string &col_type_str) {\n \tif (col_type_str == \"float64\" || col_type_str == \"Float64\") {\n \t\treturn NumpyNullableType::FLOAT_64;\n \t}\n-\tif (col_type_str == \"object\" || col_type_str == \"string\") {\n-\t\t//! this better be castable to strings\n+\tif (col_type_str == \"string\") {\n+\t\treturn NumpyNullableType::STRING;\n+\t}\n+\tif (col_type_str == \"object\") {\n \t\treturn NumpyNullableType::OBJECT;\n \t}\n \tif (col_type_str == \"timedelta64[ns]\") {\n@@ -134,6 +136,8 @@ LogicalType NumpyToLogicalType(const NumpyType &col_type) {\n \t\treturn LogicalType::FLOAT;\n \tcase NumpyNullableType::FLOAT_64:\n \t\treturn LogicalType::DOUBLE;\n+\tcase NumpyNullableType::STRING:\n+\t\treturn LogicalType::VARCHAR;\n \tcase NumpyNullableType::OBJECT:\n \t\treturn LogicalType::VARCHAR;\n \tcase NumpyNullableType::TIMEDELTA:\ndiff --git a/tools/pythonpkg/src/pandas/scan.cpp b/tools/pythonpkg/src/pandas/scan.cpp\nindex 147d77f4a0a3..10424faab7ee 100644\n--- a/tools/pythonpkg/src/pandas/scan.cpp\n+++ b/tools/pythonpkg/src/pandas/scan.cpp\n@@ -95,9 +95,12 @@ unique_ptr<FunctionData> PandasScanFunction::PandasScanBind(ClientContext &conte\n \n \tshared_ptr<DependencyItem> dependency_item;\n \tif (ref.external_dependency) {\n-\t\t// This was created during the replacement scan (see python_replacement_scan.cpp)\n+\t\t// This was created during the replacement scan if this was a pandas DataFrame (see python_replacement_scan.cpp)\n \t\tdependency_item = ref.external_dependency->GetDependency(\"copy\");\n-\t\tD_ASSERT(dependency_item);\n+\t\tif (!dependency_item) {\n+\t\t\t// This was created during the replacement if this was a numpy scan\n+\t\t\tdependency_item = ref.external_dependency->GetDependency(\"data\");\n+\t\t}\n \t}\n \n \tauto get_fun = df.attr(\"__getitem__\");\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/pandas/test_pandas_na.py b/tools/pythonpkg/tests/fast/pandas/test_pandas_na.py\nindex fb102f9bd1a0..78c96915cf79 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_pandas_na.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_pandas_na.py\n@@ -2,6 +2,7 @@\n import datetime\n import duckdb\n import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n \n def assert_nullness(items, null_indices):\n@@ -13,6 +14,18 @@ def assert_nullness(items, null_indices):\n \n \n class TestPandasNA(object):\n+    @pytest.mark.parametrize('rows', [100, duckdb.__standard_vector_size__, 5000, 1000000])\n+    @pytest.mark.parametrize('pd', [NumpyPandas(), ArrowPandas()])\n+    def test_pandas_string_null(self, duckdb_cursor, rows, pd):\n+        df: pd.DataFrame = pd.DataFrame(index=np.arange(rows))\n+        df[\"string_column\"] = pd.Series(dtype=\"string\")\n+        e_df_rel = duckdb_cursor.from_df(df)\n+        assert e_df_rel.types == ['VARCHAR']\n+        roundtrip = e_df_rel.df()\n+        assert roundtrip['string_column'].dtype == 'object'\n+        expected = pd.DataFrame({'string_column': [None for _ in range(rows)]})\n+        pd.testing.assert_frame_equal(expected, roundtrip)\n+\n     def test_pandas_na(self, duckdb_cursor):\n         pd = pytest.importorskip('pandas', minversion='1.0.0', reason='Support for pandas.NA has not been added yet')\n         # DataFrame containing a single pd.NA\n",
  "problem_statement": "Importing from Pandas has incorrect column type for empty string/object columns\n### What happens?\r\n\r\nImporting data from a Pandas dataframe result in incorrect column if the column in Pandas is object or string types and the column is completely empty.\r\n\r\nDocumentation is not clear on the implicit casting which occurs when importing a dataframe, but I would expect that the column/string pandas type becomes `VARCHAR`. This is the case when there are values in the column, but not when the column is empty, instead returning `INTEGER`. \r\n\r\nIn my limited testing this does not occur with other column types.\r\n\r\nBoth 1.0.0 and 1.0.1-dev122 results in this bug.\r\n\r\n### To Reproduce\r\n\r\nThe following code will create example dataframes and relations and compare types, failing as per bug description when checking relation data types. Changing `use_string_type` allows testing of both `object` and `string` Pandas types.\r\n\r\n```python\r\nimport random\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport duckdb\r\n\r\ndef get_filled_df(rows: int = 100, use_string_type: bool = False) -> pd.DataFrame:\r\n    \"\"\"Return dataframe filled with random data of specific types.\"\"\"\r\n    index = np.arange(rows)\r\n    data: dict = {\r\n        \"int_column\": [random.randint(0, 100) for _ in range(rows)],\r\n        \"bool_column\": [random.choice([True, False]) for _ in range(rows)],\r\n        \"float_column\": [random.uniform(0, 1) for _ in range(rows)],\r\n        \"string_column\": [random.choice([\"foo\", \"bar\", \"baz\", \"qux\"]) for _ in range(rows)]\r\n    }\r\n\r\n    return pd.DataFrame(data=data, index=index).convert_dtypes(infer_objects=False, convert_string=use_string_type)\r\n\r\ndef get_empty_df(rows: int = 100, use_string_type: bool = False) -> pd.DataFrame:\r\n    \"\"\"Return dataframe with empty columns of specific types.\"\"\"\r\n    df: pd.DataFrame = pd.DataFrame(index=np.arange(rows))\r\n\r\n    df[\"int_column\"] = pd.Series(dtype=\"Int64\")\r\n    df[\"bool_column\"] = pd.Series(dtype=\"boolean\")\r\n    df[\"float_column\"] = pd.Series(dtype=\"Float64\")\r\n    if use_string_type:\r\n        df[\"string_column\"] = pd.Series(dtype=\"string\")\r\n    else:\r\n        df[\"string_column\"] = pd.Series(dtype=\"object\")\r\n\r\n    return df\r\n\r\nprint(f\"Pandas version: {pd.__version__}\") # 2.2.2\r\nprint(f\"Numpy version: {np.__version__}\") # 1.26.4\r\nprint(f\"DuckDB version: {duckdb.__version__}\") # 1.0.0\r\n\r\nuse_string_type: bool = True\r\nfilled_df: pd.DataFrame = get_filled_df(use_string_type=use_string_type)\r\nempty_df: pd.DataFrame = get_empty_df(use_string_type=use_string_type)\r\n\r\n# check df correctly constructed\r\nassert len(filled_df) == len(empty_df)\r\n\r\n# check dtypes match\r\nfor fdf_type, edf_type in zip(filled_df.dtypes, empty_df.dtypes):\r\n    assert fdf_type == edf_type\r\n\r\nwith duckdb.connect(database=\":memory:\") as con:\r\n    f_df_rel: duckdb.DuckDBPyRelation = con.from_df(filled_df)\r\n    e_df_rel: duckdb.DuckDBPyRelation = con.from_df(empty_df)\r\n\r\n    for fdf_col, edf_col in zip(f_df_rel.columns, e_df_rel.columns):\r\n        assert fdf_col == edf_col, f\"Filled column {fdf_col} does not match empty col {fdf_col}\"\r\n\r\n    # fails with empty string column being INTEGER type instead of VARCHAR\r\n    for (fdf_col, fdf_type), (edf_col, edf_type) in zip(zip(f_df_rel.columns, f_df_rel.dtypes), zip(e_df_rel.columns, e_df_rel.dtypes)):\r\n        assert fdf_type == edf_type, f\"Filled column {fdf_col} type {fdf_type} does not match empty column {edf_col} type {edf_type}\"\r\n```\r\n\r\nThis results in:\r\n\r\n```console\r\nAssertionError: Filled column string_column type VARCHAR does not match empty column string_column type INTEGER\r\n```\r\n\r\n### OS:\r\n\r\nUbuntu 20.04 WSL x64 (5.15.146.1-microsoft-standard-WSL2)\r\n\r\n### DuckDB Version:\r\n\r\n1.0.0, 1.0.1-dev122\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nZander Horn\r\n\r\n### Affiliation:\r\n\r\nStone Three\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a nightly build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nNot applicable - the reproduction does not require a data set\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "",
  "created_at": "2024-06-13T08:50:10Z"
}