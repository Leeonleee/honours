You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
ALTER TYPE with USING results in an assertion failure "types.size() > 0"
Consider the following statements:
```sql
CREATE TABLE t0(c0 INT);
ALTER TABLE t0 ALTER c0 TYPE VARCHAR USING ''; -- Assertion `types.size() > 0' failed.
```
Unexpectedly, the `ALTER TABLE` results in an assertion failure:
```
/duckdb/src/common/types/data_chunk.cpp:25: void duckdb::DataChunk::Initialize(std::vector<duckdb::TypeId>&): Assertion `types.size() > 0' failed.
```
I found this based on commit b6665e723c5f3d9cbd7b0018ce8c242260ac6dc3.
DROP column results in an assertion failure unique.index < base.columns.size() 2
Consider the following statements:
```sql
CREATE TABLE t0(c0 INT, c1 INT UNIQUE);
ALTER TABLE t0 DROP c1; -- Assertion `unique.index < base.columns.size()' failed.
```
Unexpectedly, the `ALTER TABLE` results in an assertion failure:
```
/duckdb/src/planner/binder/statement/bind_create_table.cpp:60: void BindConstraints(duckdb::Binder&, duckdb::BoundCreateTableInfo&): Assertion `unique.index < base.columns.size()' failed.
```
This seems to be a very similar underlying issue as https://github.com/cwida/duckdb/issues/625. However, the fix for that issue does not fix the one reported in this bug report. I found this based on commit b6665e723c5f3d9cbd7b0018ce8c242260ac6dc3.

</issue>
<code>
[start of README.md]
1: <img align="left" src="logo/duckdb-logo.png" height="120">
2: 
3: # DuckDB, the SQLite for Analytics
4: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
5: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
6: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
7: 
8: <br>
9: 
10: 
11: # Requirements
12: DuckDB requires [CMake](https://cmake.org) to be installed and a `C++11` compliant compiler. GCC 4.9 and newer, Clang 3.9 and newer and VisualStudio 2017 are tested on each revision.
13: 
14: ## Compiling
15: Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You may run `make unit` and `make allunit` to verify that your version works properly after making changes.
16: 
17: # Usage
18: A command line utility based on `sqlite3` can be found in either `build/release/duckdb_cli` (release, the default) or `build/debug/duckdb_cli` (debug).
19: 
20: # Embedding
21: As DuckDB is an embedded database, there is no database server to launch or client to connect to a running server. However, the database server can be embedded directly into an application using the C or C++ bindings. The main build process creates the shared library `build/release/src/libduckdb.[so|dylib|dll]` that can be linked against. A static library is built as well.
22: 
23: For examples on how to embed DuckDB into your application, see the [examples](https://github.com/cwida/duckdb/tree/master/examples) folder.
24: 
25: ## Benchmarks
26: After compiling, benchmarks can be executed from the root directory by executing `./build/release/benchmark/benchmark_runner`.
27: 
28: ## Standing on the Shoulders of Giants
29: DuckDB is implemented in C++ 11, should compile with GCC and clang, uses CMake to build and [Catch2](https://github.com/catchorg/Catch2) for testing. DuckDB uses some components from various Open-Source databases and draws inspiration from scientific publications. Here is an overview:
30: 
31: * Parser: We use the PostgreSQL parser that was [repackaged as a stand-alone library](https://github.com/lfittl/libpg_query). The translation to our own parse tree is inspired by [Peloton](https://pelotondb.io).
32: * Shell: We have adapted the [SQLite shell](https://sqlite.org/cli.html) to work with DuckDB.
33: * Tests: We use the [SQL Logic Tests from SQLite](https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki) to test DuckDB.
34: * Query fuzzing: We use [SQLsmith](https://github.com/anse1/sqlsmith) to generate random queries for additional testing.
35: * Date Math: We use the date math component from [MonetDB](https://www.monetdb.org).
36: * SQL Window Functions: DuckDB's window functions implementation uses Segment Tree Aggregation as described in the paper "Efficient Processing of Window Functions in Analytical SQL Queries" by Viktor Leis, Kan Kundhikanjana, Alfons Kemper and Thomas Neumann.
37: * Execution engine: The vectorized execution engine is inspired by the paper "MonetDB/X100: Hyper-Pipelining Query Execution" by Peter Boncz, Marcin Zukowski and Niels Nes.
38: * Optimizer: DuckDB's optimizer draws inspiration from the papers "Dynamic programming strikes back" by Guido Moerkotte and Thomas Neumman as well as "Unnesting Arbitrary Queries" by Thomas Neumann and Alfons Kemper.
39: * Concurrency control: Our MVCC implementation is inspired by the paper "Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems" by Thomas Neumann, Tobias Mühlbauer and Alfons Kemper.
40: * Regular Expression: DuckDB uses Google's [RE2](https://github.com/google/re2) regular expression engine.
41: 
42: ## Other pages
43: * [Continuous Benchmarking (CB™)](https://www.duckdb.org/benchmarks/index.html), runs TPC-H, TPC-DS and some microbenchmarks on every commit
[end of README.md]
[start of src/catalog/catalog_entry/table_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/serializer.hpp"
7: #include "duckdb/main/connection.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/parser/constraints/list.hpp"
10: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
11: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
12: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
13: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
14: #include "duckdb/planner/expression/bound_constant_expression.hpp"
15: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
16: #include "duckdb/storage/storage_manager.hpp"
17: #include "duckdb/planner/binder.hpp"
18: 
19: #include "duckdb/execution/index/art/art.hpp"
20: #include "duckdb/parser/expression/columnref_expression.hpp"
21: #include "duckdb/planner/expression/bound_reference_expression.hpp"
22: #include "duckdb/parser/parsed_expression_iterator.hpp"
23: #include "duckdb/planner/expression_binder/alter_binder.hpp"
24: 
25: #include <algorithm>
26: 
27: using namespace duckdb;
28: using namespace std;
29: 
30: TableCatalogEntry::TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, BoundCreateTableInfo *info,
31:                                      std::shared_ptr<DataTable> inherited_storage)
32:     : StandardEntry(CatalogType::TABLE, schema, catalog, info->Base().table), storage(inherited_storage),
33:       columns(move(info->Base().columns)), constraints(move(info->Base().constraints)),
34:       bound_constraints(move(info->bound_constraints)), name_map(info->name_map) {
35: 	this->temporary = info->Base().temporary;
36: 	// add the "rowid" alias, if there is no rowid column specified in the table
37: 	if (name_map.find("rowid") == name_map.end()) {
38: 		name_map["rowid"] = COLUMN_IDENTIFIER_ROW_ID;
39: 	}
40: 	if (!storage) {
41: 		// create the physical storage
42: 		storage = make_shared<DataTable>(catalog->storage, schema->name, name, GetTypes(), move(info->data));
43: 
44: 		// create the unique indexes for the UNIQUE and PRIMARY KEY constraints
45: 		for (idx_t i = 0; i < bound_constraints.size(); i++) {
46: 			auto &constraint = bound_constraints[i];
47: 			if (constraint->type == ConstraintType::UNIQUE) {
48: 				// unique constraint: create a unique index
49: 				auto &unique = (BoundUniqueConstraint &)*constraint;
50: 				// fetch types and create expressions for the index from the columns
51: 				vector<column_t> column_ids;
52: 				vector<unique_ptr<Expression>> unbound_expressions;
53: 				vector<unique_ptr<Expression>> bound_expressions;
54: 				idx_t key_nr = 0;
55: 				for (auto &key : unique.keys) {
56: 					TypeId column_type = GetInternalType(columns[key].type);
57: 					assert(key < columns.size());
58: 
59: 					unbound_expressions.push_back(
60: 					    make_unique<BoundColumnRefExpression>(column_type, ColumnBinding(0, column_ids.size())));
61: 					bound_expressions.push_back(make_unique<BoundReferenceExpression>(column_type, key_nr++));
62: 					column_ids.push_back(key);
63: 				}
64: 				// create an adaptive radix tree around the expressions
65: 				auto art = make_unique<ART>(*storage, column_ids, move(unbound_expressions), true);
66: 				storage->AddIndex(move(art), bound_expressions);
67: 			}
68: 		}
69: 	}
70: }
71: 
72: bool TableCatalogEntry::ColumnExists(const string &name) {
73: 	return name_map.find(name) != name_map.end();
74: }
75: 
76: unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, AlterInfo *info) {
77: 	if (info->type != AlterType::ALTER_TABLE) {
78: 		throw CatalogException("Can only modify table with ALTER TABLE statement");
79: 	}
80: 	auto table_info = (AlterTableInfo *)info;
81: 	switch (table_info->alter_table_type) {
82: 	case AlterTableType::RENAME_COLUMN: {
83: 		auto rename_info = (RenameColumnInfo *)table_info;
84: 		return RenameColumn(context, *rename_info);
85: 	}
86: 	case AlterTableType::RENAME_TABLE: {
87: 		auto rename_info = (RenameTableInfo *)table_info;
88: 		auto copied_table = Copy(context);
89: 		copied_table->name = rename_info->new_table_name;
90: 		return copied_table;
91: 	}
92: 	case AlterTableType::ADD_COLUMN: {
93: 		auto add_info = (AddColumnInfo *)table_info;
94: 		return AddColumn(context, *add_info);
95: 	}
96: 	case AlterTableType::REMOVE_COLUMN: {
97: 		auto remove_info = (RemoveColumnInfo *)table_info;
98: 		return RemoveColumn(context, *remove_info);
99: 	}
100: 	case AlterTableType::SET_DEFAULT: {
101: 		auto set_default_info = (SetDefaultInfo *)table_info;
102: 		return SetDefault(context, *set_default_info);
103: 	}
104: 	case AlterTableType::ALTER_COLUMN_TYPE: {
105: 		auto change_type_info = (ChangeColumnTypeInfo *)table_info;
106: 		return ChangeColumnType(context, *change_type_info);
107: 	}
108: 	default:
109: 		throw InternalException("Unrecognized alter table type!");
110: 	}
111: }
112: 
113: static void RenameExpression(ParsedExpression &expr, RenameColumnInfo &info) {
114: 	if (expr.type == ExpressionType::COLUMN_REF) {
115: 		auto &colref = (ColumnRefExpression &)expr;
116: 		if (colref.column_name == info.name) {
117: 			colref.column_name = info.new_name;
118: 		}
119: 	}
120: 	ParsedExpressionIterator::EnumerateChildren(
121: 	    expr, [&](const ParsedExpression &child) { RenameExpression((ParsedExpression &)child, info); });
122: }
123: 
124: unique_ptr<CatalogEntry> TableCatalogEntry::RenameColumn(ClientContext &context, RenameColumnInfo &info) {
125: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
126: 	create_info->temporary = temporary;
127: 	bool found = false;
128: 	for (idx_t i = 0; i < columns.size(); i++) {
129: 		ColumnDefinition copy = columns[i].Copy();
130: 
131: 		create_info->columns.push_back(move(copy));
132: 		if (info.name == columns[i].name) {
133: 			assert(!found);
134: 			create_info->columns[i].name = info.new_name;
135: 			found = true;
136: 		}
137: 	}
138: 	if (!found) {
139: 		throw CatalogException("Table does not have a column with name \"%s\"", info.name.c_str());
140: 	}
141: 	for (idx_t c_idx = 0; c_idx < constraints.size(); c_idx++) {
142: 		auto copy = constraints[c_idx]->Copy();
143: 		switch (copy->type) {
144: 		case ConstraintType::NOT_NULL:
145: 			// NOT NULL constraint: no adjustments necessary
146: 			break;
147: 		case ConstraintType::CHECK: {
148: 			// CHECK constraint: need to rename column references that refer to the renamed column
149: 			auto &check = (CheckConstraint &)*copy;
150: 			RenameExpression(*check.expression, info);
151: 			break;
152: 		}
153: 		case ConstraintType::UNIQUE: {
154: 			// UNIQUE constraint: possibly need to rename columns
155: 			auto &unique = (UniqueConstraint &)*copy;
156: 			for (idx_t i = 0; i < unique.columns.size(); i++) {
157: 				if (unique.columns[i] == info.name) {
158: 					unique.columns[i] = info.new_name;
159: 				}
160: 			}
161: 			break;
162: 		}
163: 		default:
164: 			throw CatalogException("Unsupported constraint for entry!");
165: 		}
166: 		create_info->constraints.push_back(move(copy));
167: 	}
168: 	Binder binder(context);
169: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
170: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
171: }
172: 
173: unique_ptr<CatalogEntry> TableCatalogEntry::AddColumn(ClientContext &context, AddColumnInfo &info) {
174: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
175: 	create_info->temporary = temporary;
176: 	for (idx_t i = 0; i < columns.size(); i++) {
177: 		create_info->columns.push_back(columns[i].Copy());
178: 	}
179: 	info.new_column.oid = columns.size();
180: 	create_info->columns.push_back(info.new_column.Copy());
181: 
182: 	Binder binder(context);
183: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
184: 	auto new_storage =
185: 	    make_shared<DataTable>(context, *storage, info.new_column, bound_create_info->bound_defaults.back().get());
186: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
187: 	                                      new_storage);
188: }
189: 
190: unique_ptr<CatalogEntry> TableCatalogEntry::RemoveColumn(ClientContext &context, RemoveColumnInfo &info) {
191: 	idx_t removed_index = INVALID_INDEX;
192: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
193: 	create_info->temporary = temporary;
194: 	for (idx_t i = 0; i < columns.size(); i++) {
195: 		if (columns[i].name == info.removed_column) {
196: 			assert(removed_index == INVALID_INDEX);
197: 			removed_index = i;
198: 			continue;
199: 		}
200: 		create_info->columns.push_back(columns[i].Copy());
201: 	}
202: 	if (removed_index == INVALID_INDEX) {
203: 		if (!info.if_exists) {
204: 			throw CatalogException("Table does not have a column with name \"%s\"", info.removed_column.c_str());
205: 		}
206: 		return nullptr;
207: 	}
208: 	if (create_info->columns.size() == 0) {
209: 		throw CatalogException("Cannot drop column: table only has one column remaining!");
210: 	}
211: 	// handle constraints for the new table
212: 	assert(constraints.size() == bound_constraints.size());
213: 	for (idx_t constr_idx = 0; constr_idx < constraints.size(); constr_idx++) {
214: 		auto &constraint = constraints[constr_idx];
215: 		auto &bound_constraint = bound_constraints[constr_idx];
216: 		switch (bound_constraint->type) {
217: 		case ConstraintType::NOT_NULL: {
218: 			auto &not_null_constraint = (BoundNotNullConstraint &)*bound_constraint;
219: 			if (not_null_constraint.index != removed_index) {
220: 				// the constraint is not about this column: we need to copy it
221: 				// we might need to shift the index back by one though, to account for the removed column
222: 				idx_t new_index = not_null_constraint.index;
223: 				if (not_null_constraint.index > removed_index) {
224: 					new_index -= 1;
225: 				}
226: 				create_info->constraints.push_back(make_unique<NotNullConstraint>(new_index));
227: 			}
228: 			break;
229: 		}
230: 		case ConstraintType::CHECK: {
231: 			// CHECK constraint
232: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraint;
233: 			// check if the removed column is part of the check constraint
234: 			if (bound_check.bound_columns.find(removed_index) != bound_check.bound_columns.end()) {
235: 				if (bound_check.bound_columns.size() > 1) {
236: 					// CHECK constraint that concerns mult
237: 					throw CatalogException(
238: 					    "Cannot drop column \"%s\" because there is a CHECK constraint that depends on it",
239: 					    info.removed_column.c_str());
240: 				} else {
241: 					// CHECK constraint that ONLY concerns this column, strip the constraint
242: 				}
243: 			} else {
244: 				// check constraint does not concern the removed column: simply re-add it
245: 				create_info->constraints.push_back(constraint->Copy());
246: 			}
247: 			break;
248: 		}
249: 		case ConstraintType::UNIQUE:
250: 			create_info->constraints.push_back(constraint->Copy());
251: 			break;
252: 		default:
253: 			throw InternalException("Unsupported constraint for entry!");
254: 		}
255: 	}
256: 
257: 	Binder binder(context);
258: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
259: 	auto new_storage = make_shared<DataTable>(context, *storage, removed_index);
260: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
261: 	                                      new_storage);
262: }
263: 
264: unique_ptr<CatalogEntry> TableCatalogEntry::SetDefault(ClientContext &context, SetDefaultInfo &info) {
265: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
266: 	bool found = false;
267: 	for (idx_t i = 0; i < columns.size(); i++) {
268: 		auto copy = columns[i].Copy();
269: 		if (info.column_name == copy.name) {
270: 			// set the default value of this column
271: 			copy.default_value = info.expression ? info.expression->Copy() : nullptr;
272: 			found = true;
273: 		}
274: 		create_info->columns.push_back(move(copy));
275: 	}
276: 	if (!found) {
277: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.table.c_str(),
278: 		                      info.column_name.c_str());
279: 	}
280: 
281: 	for (idx_t i = 0; i < constraints.size(); i++) {
282: 		auto constraint = constraints[i]->Copy();
283: 		create_info->constraints.push_back(move(constraint));
284: 	}
285: 
286: 	Binder binder(context);
287: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
288: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
289: }
290: 
291: unique_ptr<CatalogEntry> TableCatalogEntry::ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info) {
292: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
293: 	idx_t change_idx = INVALID_INDEX;
294: 	for (idx_t i = 0; i < columns.size(); i++) {
295: 		auto copy = columns[i].Copy();
296: 		if (info.column_name == copy.name) {
297: 			// set the default value of this column
298: 			change_idx = i;
299: 			copy.type = info.target_type;
300: 		}
301: 		create_info->columns.push_back(move(copy));
302: 	}
303: 	if (change_idx == INVALID_INDEX) {
304: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.table.c_str(),
305: 		                      info.column_name.c_str());
306: 	}
307: 
308: 	for (idx_t i = 0; i < constraints.size(); i++) {
309: 		auto constraint = constraints[i]->Copy();
310: 		switch (constraint->type) {
311: 		case ConstraintType::CHECK: {
312: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraints[i];
313: 			if (bound_check.bound_columns.find(change_idx) != bound_check.bound_columns.end()) {
314: 				throw BinderException("Cannot change the type of a column that has a CHECK constraint specified");
315: 			}
316: 			break;
317: 		}
318: 		case ConstraintType::NOT_NULL:
319: 			break;
320: 		case ConstraintType::UNIQUE: {
321: 			auto &bound_unique = (BoundUniqueConstraint &)*bound_constraints[i];
322: 			if (bound_unique.keys.find(change_idx) != bound_unique.keys.end()) {
323: 				throw BinderException(
324: 				    "Cannot change the type of a column that has a UNIQUE or PRIMARY KEY constraint specified");
325: 			}
326: 			break;
327: 		}
328: 		default:
329: 			throw InternalException("Unsupported constraint for entry!");
330: 		}
331: 		create_info->constraints.push_back(move(constraint));
332: 	}
333: 
334: 	Binder binder(context);
335: 	// bind the specified expression
336: 	vector<column_t> bound_columns;
337: 	AlterBinder expr_binder(binder, context, name, columns, bound_columns, info.target_type);
338: 	auto expression = info.expression->Copy();
339: 	auto bound_expression = expr_binder.Bind(expression);
340: 	auto new_storage =
341: 	    make_shared<DataTable>(context, *storage, change_idx, info.target_type, move(bound_columns), *bound_expression);
342: 
343: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
344: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
345: 	                                      new_storage);
346: }
347: 
348: ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {
349: 	auto entry = name_map.find(name);
350: 	if (entry == name_map.end() || entry->second == COLUMN_IDENTIFIER_ROW_ID) {
351: 		throw CatalogException("Column with name %s does not exist!", name.c_str());
352: 	}
353: 	return columns[entry->second];
354: }
355: 
356: vector<TypeId> TableCatalogEntry::GetTypes() {
357: 	vector<TypeId> types;
358: 	for (auto &it : columns) {
359: 		types.push_back(GetInternalType(it.type));
360: 	}
361: 	return types;
362: }
363: 
364: vector<TypeId> TableCatalogEntry::GetTypes(const vector<column_t> &column_ids) {
365: 	vector<TypeId> result;
366: 	for (auto &index : column_ids) {
367: 		if (index == COLUMN_IDENTIFIER_ROW_ID) {
368: 			result.push_back(TypeId::INT64);
369: 		} else {
370: 			result.push_back(GetInternalType(columns[index].type));
371: 		}
372: 	}
373: 	return result;
374: }
375: 
376: void TableCatalogEntry::Serialize(Serializer &serializer) {
377: 	serializer.WriteString(schema->name);
378: 	serializer.WriteString(name);
379: 	assert(columns.size() <= std::numeric_limits<uint32_t>::max());
380: 	serializer.Write<uint32_t>((uint32_t)columns.size());
381: 	for (auto &column : columns) {
382: 		column.Serialize(serializer);
383: 	}
384: 	assert(constraints.size() <= std::numeric_limits<uint32_t>::max());
385: 	serializer.Write<uint32_t>((uint32_t)constraints.size());
386: 	for (auto &constraint : constraints) {
387: 		constraint->Serialize(serializer);
388: 	}
389: }
390: 
391: unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source) {
392: 	auto info = make_unique<CreateTableInfo>();
393: 
394: 	info->schema = source.Read<string>();
395: 	info->table = source.Read<string>();
396: 	auto column_count = source.Read<uint32_t>();
397: 
398: 	for (uint32_t i = 0; i < column_count; i++) {
399: 		auto column = ColumnDefinition::Deserialize(source);
400: 		info->columns.push_back(move(column));
401: 	}
402: 	auto constraint_count = source.Read<uint32_t>();
403: 
404: 	for (uint32_t i = 0; i < constraint_count; i++) {
405: 		auto constraint = Constraint::Deserialize(source);
406: 		info->constraints.push_back(move(constraint));
407: 	}
408: 	return info;
409: }
410: 
411: unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {
412: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
413: 	for (idx_t i = 0; i < columns.size(); i++) {
414: 		create_info->columns.push_back(columns[i].Copy());
415: 	}
416: 
417: 	for (idx_t i = 0; i < constraints.size(); i++) {
418: 		auto constraint = constraints[i]->Copy();
419: 		create_info->constraints.push_back(move(constraint));
420: 	}
421: 
422: 	Binder binder(context);
423: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
424: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
425: }
426: 
427: void TableCatalogEntry::SetAsRoot() {
428: 	storage->SetAsRoot();
429: }
[end of src/catalog/catalog_entry/table_catalog_entry.cpp]
[start of src/execution/index/art/art.cpp]
1: #include "duckdb/execution/index/art/art.hpp"
2: #include "duckdb/execution/expression_executor.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include <algorithm>
5: #include <ctgmath>
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: ART::ART(DataTable &table, vector<column_t> column_ids, vector<unique_ptr<Expression>> unbound_expressions,
11:          bool is_unique)
12:     : Index(IndexType::ART, table, column_ids, move(unbound_expressions)), is_unique(is_unique) {
13: 	tree = nullptr;
14: 	expression_result.Initialize(types);
15: 	int n = 1;
16: 	//! little endian if true
17: 	if (*(char *)&n == 1) {
18: 		is_little_endian = true;
19: 	} else {
20: 		is_little_endian = false;
21: 	}
22: 	switch (types[0]) {
23: 	case TypeId::BOOL:
24: 	case TypeId::INT8:
25: 	case TypeId::INT16:
26: 	case TypeId::INT32:
27: 	case TypeId::INT64:
28: 	case TypeId::FLOAT:
29: 	case TypeId::DOUBLE:
30: 	case TypeId::VARCHAR:
31: 		break;
32: 	default:
33: 		throw InvalidTypeException(types[0], "Invalid type for index");
34: 	}
35: }
36: 
37: ART::~ART() {
38: }
39: 
40: bool ART::LeafMatches(Node *node, Key &key, unsigned depth) {
41: 	auto leaf = static_cast<Leaf *>(node);
42: 	Key &leaf_key = *leaf->value;
43: 	for (idx_t i = depth; i < leaf_key.len; i++) {
44: 		if (leaf_key[i] != key[i]) {
45: 			return false;
46: 		}
47: 	}
48: 
49: 	return true;
50: }
51: 
52: unique_ptr<IndexScanState> ART::InitializeScanSinglePredicate(Transaction &transaction, vector<column_t> column_ids,
53:                                                               Value value, ExpressionType expression_type) {
54: 	auto result = make_unique<ARTIndexScanState>(column_ids);
55: 	result->values[0] = value;
56: 	result->expressions[0] = expression_type;
57: 	return move(result);
58: }
59: 
60: unique_ptr<IndexScanState> ART::InitializeScanTwoPredicates(Transaction &transaction, vector<column_t> column_ids,
61:                                                             Value low_value, ExpressionType low_expression_type,
62:                                                             Value high_value, ExpressionType high_expression_type) {
63: 	auto result = make_unique<ARTIndexScanState>(column_ids);
64: 	result->values[0] = low_value;
65: 	result->expressions[0] = low_expression_type;
66: 	result->values[1] = high_value;
67: 	result->expressions[1] = high_expression_type;
68: 	return move(result);
69: }
70: 
71: //===--------------------------------------------------------------------===//
72: // Insert
73: //===--------------------------------------------------------------------===//
74: template <class T>
75: static void generate_keys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
76: 	VectorData idata;
77: 	input.Orrify(count, idata);
78: 
79: 	auto input_data = (T *)idata.data;
80: 	for (idx_t i = 0; i < count; i++) {
81: 		auto idx = idata.sel->get_index(i);
82: 		if ((*idata.nullmask)[idx]) {
83: 			keys.push_back(nullptr);
84: 		} else {
85: 			keys.push_back(Key::CreateKey<T>(input_data[idx], is_little_endian));
86: 		}
87: 	}
88: }
89: 
90: template <class T>
91: static void concatenate_keys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
92: 	VectorData idata;
93: 	input.Orrify(count, idata);
94: 
95: 	auto input_data = (T *)idata.data;
96: 	for (idx_t i = 0; i < count; i++) {
97: 		auto idx = idata.sel->get_index(i);
98: 		if ((*idata.nullmask)[idx] || !keys[i]) {
99: 			// either this column is NULL, or the previous column is NULL!
100: 			keys[i] = nullptr;
101: 		} else {
102: 			// concatenate the keys
103: 			auto old_key = move(keys[i]);
104: 			auto new_key = Key::CreateKey<T>(input_data[idx], is_little_endian);
105: 			auto keyLen = old_key->len + new_key->len;
106: 			auto compound_data = unique_ptr<data_t[]>(new data_t[keyLen]);
107: 			memcpy(compound_data.get(), old_key->data.get(), old_key->len);
108: 			memcpy(compound_data.get() + old_key->len, new_key->data.get(), new_key->len);
109: 			keys[i] = make_unique<Key>(move(compound_data), keyLen);
110: 		}
111: 	}
112: }
113: 
114: void ART::GenerateKeys(DataChunk &input, vector<unique_ptr<Key>> &keys) {
115: 	keys.reserve(STANDARD_VECTOR_SIZE);
116: 	// generate keys for the first input column
117: 	switch (input.data[0].type) {
118: 	case TypeId::BOOL:
119: 		generate_keys<bool>(input.data[0], input.size(), keys, is_little_endian);
120: 		break;
121: 	case TypeId::INT8:
122: 		generate_keys<int8_t>(input.data[0], input.size(), keys, is_little_endian);
123: 		break;
124: 	case TypeId::INT16:
125: 		generate_keys<int16_t>(input.data[0], input.size(), keys, is_little_endian);
126: 		break;
127: 	case TypeId::INT32:
128: 		generate_keys<int32_t>(input.data[0], input.size(), keys, is_little_endian);
129: 		break;
130: 	case TypeId::INT64:
131: 		generate_keys<int64_t>(input.data[0], input.size(), keys, is_little_endian);
132: 		break;
133: 	case TypeId::FLOAT:
134: 		generate_keys<float>(input.data[0], input.size(), keys, is_little_endian);
135: 		break;
136: 	case TypeId::DOUBLE:
137: 		generate_keys<double>(input.data[0], input.size(), keys, is_little_endian);
138: 		break;
139: 	case TypeId::VARCHAR:
140: 		generate_keys<string_t>(input.data[0], input.size(), keys, is_little_endian);
141: 		break;
142: 	default:
143: 		throw InvalidTypeException(input.data[0].type, "Invalid type for index");
144: 	}
145: 	for (idx_t i = 1; i < input.column_count(); i++) {
146: 		// for each of the remaining columns, concatenate
147: 		switch (input.data[i].type) {
148: 		case TypeId::BOOL:
149: 			concatenate_keys<bool>(input.data[i], input.size(), keys, is_little_endian);
150: 			break;
151: 		case TypeId::INT8:
152: 			concatenate_keys<int8_t>(input.data[i], input.size(), keys, is_little_endian);
153: 			break;
154: 		case TypeId::INT16:
155: 			concatenate_keys<int16_t>(input.data[i], input.size(), keys, is_little_endian);
156: 			break;
157: 		case TypeId::INT32:
158: 			concatenate_keys<int32_t>(input.data[i], input.size(), keys, is_little_endian);
159: 			break;
160: 		case TypeId::INT64:
161: 			concatenate_keys<int64_t>(input.data[i], input.size(), keys, is_little_endian);
162: 			break;
163: 		case TypeId::FLOAT:
164: 			concatenate_keys<float>(input.data[i], input.size(), keys, is_little_endian);
165: 			break;
166: 		case TypeId::DOUBLE:
167: 			concatenate_keys<double>(input.data[i], input.size(), keys, is_little_endian);
168: 			break;
169: 		case TypeId::VARCHAR:
170: 			concatenate_keys<string_t>(input.data[i], input.size(), keys, is_little_endian);
171: 			break;
172: 		default:
173: 			throw InvalidTypeException(input.data[0].type, "Invalid type for index");
174: 		}
175: 	}
176: }
177: 
178: bool ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {
179: 	assert(row_ids.type == ROW_TYPE);
180: 	assert(types[0] == input.data[0].type);
181: 
182: 	// generate the keys for the given input
183: 	vector<unique_ptr<Key>> keys;
184: 	GenerateKeys(input, keys);
185: 
186: 	// now insert the elements into the index
187: 	row_ids.Normalify(input.size());
188: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
189: 	idx_t failed_index = INVALID_INDEX;
190: 	for (idx_t i = 0; i < input.size(); i++) {
191: 		if (!keys[i]) {
192: 			continue;
193: 		}
194: 
195: 		row_t row_id = row_identifiers[i];
196: 		if (!Insert(tree, move(keys[i]), 0, row_id)) {
197: 			// failed to insert because of constraint violation
198: 			failed_index = i;
199: 			break;
200: 		}
201: 	}
202: 	if (failed_index != INVALID_INDEX) {
203: 		// failed to insert because of constraint violation: remove previously inserted entries
204: 		// generate keys again
205: 		keys.clear();
206: 		GenerateKeys(input, keys);
207: 		unique_ptr<Key> key;
208: 
209: 		// now erase the entries
210: 		for (idx_t i = 0; i < failed_index; i++) {
211: 			if (!keys[i]) {
212: 				continue;
213: 			}
214: 			row_t row_id = row_identifiers[i];
215: 			Erase(tree, *keys[i], 0, row_id);
216: 		}
217: 		return false;
218: 	}
219: 	return true;
220: }
221: 
222: bool ART::Append(IndexLock &lock, DataChunk &appended_data, Vector &row_identifiers) {
223: 	// first resolve the expressions for the index
224: 	ExecuteExpressions(appended_data, expression_result);
225: 
226: 	// now insert into the index
227: 	return Insert(lock, expression_result, row_identifiers);
228: }
229: 
230: void ART::VerifyAppend(DataChunk &chunk) {
231: 	if (!is_unique) {
232: 		return;
233: 	}
234: 	// unique index, check
235: 	lock_guard<mutex> l(lock);
236: 	// first resolve the expressions for the index
237: 	ExecuteExpressions(chunk, expression_result);
238: 
239: 	// generate the keys for the given input
240: 	vector<unique_ptr<Key>> keys;
241: 	GenerateKeys(expression_result, keys);
242: 
243: 	for (idx_t i = 0; i < chunk.size(); i++) {
244: 		if (!keys[i]) {
245: 			continue;
246: 		}
247: 		if (Lookup(tree, *keys[i], 0) != nullptr) {
248: 			// node already exists in tree
249: 			throw ConstraintException("duplicate key value violates primary key or unique constraint");
250: 		}
251: 	}
252: }
253: 
254: bool ART::InsertToLeaf(Leaf &leaf, row_t row_id) {
255: 	if (is_unique && leaf.num_elements != 0) {
256: 		return false;
257: 	}
258: 	leaf.Insert(row_id);
259: 	return true;
260: }
261: 
262: bool ART::Insert(unique_ptr<Node> &node, unique_ptr<Key> value, unsigned depth, row_t row_id) {
263: 	Key &key = *value;
264: 	if (!node) {
265: 		// node is currently empty, create a leaf here with the key
266: 		node = make_unique<Leaf>(*this, move(value), row_id);
267: 		return true;
268: 	}
269: 
270: 	if (node->type == NodeType::NLeaf) {
271: 		// Replace leaf with Node4 and store both leaves in it
272: 		auto leaf = static_cast<Leaf *>(node.get());
273: 
274: 		Key &existingKey = *leaf->value;
275: 		uint32_t newPrefixLength = 0;
276: 		// Leaf node is already there, update row_id vector
277: 		if (depth + newPrefixLength == existingKey.len && existingKey.len == key.len) {
278: 			return InsertToLeaf(*leaf, row_id);
279: 		}
280: 		while (existingKey[depth + newPrefixLength] == key[depth + newPrefixLength]) {
281: 			newPrefixLength++;
282: 			// Leaf node is already there, update row_id vector
283: 			if (depth + newPrefixLength == existingKey.len && existingKey.len == key.len) {
284: 				return InsertToLeaf(*leaf, row_id);
285: 			}
286: 		}
287: 
288: 		unique_ptr<Node> newNode = make_unique<Node4>(*this, newPrefixLength);
289: 		newNode->prefix_length = newPrefixLength;
290: 		memcpy(newNode->prefix.get(), &key[depth], newPrefixLength);
291: 		Node4::insert(*this, newNode, existingKey[depth + newPrefixLength], node);
292: 		unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
293: 		Node4::insert(*this, newNode, key[depth + newPrefixLength], leaf_node);
294: 		node = move(newNode);
295: 		return true;
296: 	}
297: 
298: 	// Handle prefix of inner node
299: 	if (node->prefix_length) {
300: 		uint32_t mismatchPos = Node::PrefixMismatch(*this, node.get(), key, depth);
301: 		if (mismatchPos != node->prefix_length) {
302: 			// Prefix differs, create new node
303: 			unique_ptr<Node> newNode = make_unique<Node4>(*this, mismatchPos);
304: 			newNode->prefix_length = mismatchPos;
305: 			memcpy(newNode->prefix.get(), node->prefix.get(), mismatchPos);
306: 			// Break up prefix
307: 			auto node_ptr = node.get();
308: 			Node4::insert(*this, newNode, node->prefix[mismatchPos], node);
309: 			node_ptr->prefix_length -= (mismatchPos + 1);
310: 			memmove(node_ptr->prefix.get(), node_ptr->prefix.get() + mismatchPos + 1, node_ptr->prefix_length);
311: 			unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
312: 			Node4::insert(*this, newNode, key[depth + mismatchPos], leaf_node);
313: 			node = move(newNode);
314: 			return true;
315: 		}
316: 		depth += node->prefix_length;
317: 	}
318: 
319: 	// Recurse
320: 	idx_t pos = node->GetChildPos(key[depth]);
321: 	if (pos != INVALID_INDEX) {
322: 		auto child = node->GetChild(pos);
323: 		return Insert(*child, move(value), depth + 1, row_id);
324: 	}
325: 	unique_ptr<Node> newNode = make_unique<Leaf>(*this, move(value), row_id);
326: 	Node::InsertLeaf(*this, node, key[depth], newNode);
327: 	return true;
328: }
329: 
330: //===--------------------------------------------------------------------===//
331: // Delete
332: //===--------------------------------------------------------------------===//
333: void ART::Delete(IndexLock &state, DataChunk &input, Vector &row_ids) {
334: 	// first resolve the expressions
335: 	ExecuteExpressions(input, expression_result);
336: 
337: 	// then generate the keys for the given input
338: 	vector<unique_ptr<Key>> keys;
339: 	GenerateKeys(expression_result, keys);
340: 
341: 	// now erase the elements from the database
342: 	row_ids.Normalify(input.size());
343: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
344: 
345: 	for (idx_t i = 0; i < input.size(); i++) {
346: 		if (!keys[i]) {
347: 			continue;
348: 		}
349: 		Erase(tree, *keys[i], 0, row_identifiers[i]);
350: 	}
351: }
352: 
353: void ART::Erase(unique_ptr<Node> &node, Key &key, unsigned depth, row_t row_id) {
354: 	if (!node) {
355: 		return;
356: 	}
357: 	// Delete a leaf from a tree
358: 	if (node->type == NodeType::NLeaf) {
359: 		// Make sure we have the right leaf
360: 		if (ART::LeafMatches(node.get(), key, depth)) {
361: 			auto leaf = static_cast<Leaf *>(node.get());
362: 			leaf->Remove(row_id);
363: 			if (leaf->num_elements == 0) {
364: 				node.reset();
365: 			}
366: 		}
367: 		return;
368: 	}
369: 
370: 	// Handle prefix
371: 	if (node->prefix_length) {
372: 		if (Node::PrefixMismatch(*this, node.get(), key, depth) != node->prefix_length) {
373: 			return;
374: 		}
375: 		depth += node->prefix_length;
376: 	}
377: 	idx_t pos = node->GetChildPos(key[depth]);
378: 	if (pos != INVALID_INDEX) {
379: 		auto child = node->GetChild(pos);
380: 		assert(child);
381: 
382: 		unique_ptr<Node> &child_ref = *child;
383: 		if (child_ref->type == NodeType::NLeaf && LeafMatches(child_ref.get(), key, depth)) {
384: 			// Leaf found, remove entry
385: 			auto leaf = static_cast<Leaf *>(child_ref.get());
386: 			leaf->Remove(row_id);
387: 			if (leaf->num_elements == 0) {
388: 				// Leaf is empty, delete leaf, decrement node counter and maybe shrink node
389: 				Node::Erase(*this, node, pos);
390: 			}
391: 		} else {
392: 			// Recurse
393: 			Erase(*child, key, depth + 1, row_id);
394: 		}
395: 	}
396: }
397: 
398: //===--------------------------------------------------------------------===//
399: // Point Query
400: //===--------------------------------------------------------------------===//
401: static unique_ptr<Key> CreateKey(ART &art, TypeId type, Value &value) {
402: 	assert(type == value.type);
403: 	switch (type) {
404: 	case TypeId::BOOL:
405: 		return Key::CreateKey<bool>(value.value_.boolean, art.is_little_endian);
406: 	case TypeId::INT8:
407: 		return Key::CreateKey<int8_t>(value.value_.tinyint, art.is_little_endian);
408: 	case TypeId::INT16:
409: 		return Key::CreateKey<int16_t>(value.value_.smallint, art.is_little_endian);
410: 	case TypeId::INT32:
411: 		return Key::CreateKey<int32_t>(value.value_.integer, art.is_little_endian);
412: 	case TypeId::INT64:
413: 		return Key::CreateKey<int64_t>(value.value_.bigint, art.is_little_endian);
414: 	case TypeId::FLOAT:
415: 		return Key::CreateKey<float>(value.value_.float_, art.is_little_endian);
416: 	case TypeId::DOUBLE:
417: 		return Key::CreateKey<double>(value.value_.double_, art.is_little_endian);
418: 	case TypeId::VARCHAR:
419: 		return Key::CreateKey<string_t>(string_t(value.str_value.c_str(), value.str_value.size()),
420: 		                                art.is_little_endian);
421: 	default:
422: 		throw InvalidTypeException(type, "Invalid type for index");
423: 	}
424: }
425: 
426: void ART::SearchEqual(vector<row_t> &result_ids, ARTIndexScanState *state) {
427: 	unique_ptr<Key> key = CreateKey(*this, types[0], state->values[0]);
428: 	auto leaf = static_cast<Leaf *>(Lookup(tree, *key, 0));
429: 	if (!leaf) {
430: 		return;
431: 	}
432: 	for (idx_t i = 0; i < leaf->num_elements; i++) {
433: 		row_t row_id = leaf->GetRowId(i);
434: 		result_ids.push_back(row_id);
435: 	}
436: }
437: 
438: Node *ART::Lookup(unique_ptr<Node> &node, Key &key, unsigned depth) {
439: 	auto node_val = node.get();
440: 
441: 	while (node_val) {
442: 		if (node_val->type == NodeType::NLeaf) {
443: 			auto leaf = static_cast<Leaf *>(node_val);
444: 			Key &leafKey = *leaf->value;
445: 			//! Check leaf
446: 			for (idx_t i = depth; i < leafKey.len; i++) {
447: 				if (leafKey[i] != key[i]) {
448: 					return nullptr;
449: 				}
450: 			}
451: 			return node_val;
452: 		}
453: 		if (node_val->prefix_length) {
454: 			for (idx_t pos = 0; pos < node_val->prefix_length; pos++) {
455: 				if (key[depth + pos] != node_val->prefix[pos]) {
456: 					return nullptr;
457: 				}
458: 			}
459: 			depth += node_val->prefix_length;
460: 		}
461: 		idx_t pos = node_val->GetChildPos(key[depth]);
462: 		if (pos == INVALID_INDEX) {
463: 			return nullptr;
464: 		}
465: 		node_val = node_val->GetChild(pos)->get();
466: 		assert(node_val);
467: 
468: 		depth++;
469: 	}
470: 
471: 	return nullptr;
472: }
473: 
474: //===--------------------------------------------------------------------===//
475: // Iterator scans
476: //===--------------------------------------------------------------------===//
477: template <bool HAS_BOUND, bool INCLUSIVE>
478: void ART::IteratorScan(ARTIndexScanState *state, Iterator *it, vector<row_t> &result_ids, Key *bound) {
479: 	bool has_next;
480: 	do {
481: 		if (HAS_BOUND) {
482: 			assert(bound);
483: 			if (INCLUSIVE) {
484: 				if (*it->node->value > *bound) {
485: 					break;
486: 				}
487: 			} else {
488: 				if (*it->node->value >= *bound) {
489: 					break;
490: 				}
491: 			}
492: 		}
493: 		for (idx_t i = 0; i < it->node->num_elements; i++) {
494: 			row_t row_id = it->node->GetRowId(i);
495: 			result_ids.push_back(row_id);
496: 		}
497: 		has_next = ART::IteratorNext(*it);
498: 	} while (has_next);
499: }
500: 
501: bool ART::IteratorNext(Iterator &it) {
502: 	// Skip leaf
503: 	if ((it.depth) && ((it.stack[it.depth - 1].node)->type == NodeType::NLeaf)) {
504: 		it.depth--;
505: 	}
506: 
507: 	// Look for the next leaf
508: 	while (it.depth > 0) {
509: 		auto &top = it.stack[it.depth - 1];
510: 		Node *node = top.node;
511: 
512: 		if (node->type == NodeType::NLeaf) {
513: 			// found a leaf: move to next node
514: 			it.node = (Leaf *)node;
515: 			return true;
516: 		}
517: 
518: 		// Find next node
519: 		top.pos = node->GetNextPos(top.pos);
520: 		if (top.pos != INVALID_INDEX) {
521: 			// next node found: go there
522: 			it.stack[it.depth].node = node->GetChild(top.pos)->get();
523: 			it.stack[it.depth].pos = INVALID_INDEX;
524: 			it.depth++;
525: 		} else {
526: 			// no node found: move up the tree
527: 			it.depth--;
528: 		}
529: 	}
530: 	return false;
531: }
532: 
533: //===--------------------------------------------------------------------===//
534: // Greater Than
535: // Returns: True (If found leaf >= key)
536: //          False (Otherwise)
537: //===--------------------------------------------------------------------===//
538: bool ART::Bound(unique_ptr<Node> &n, Key &key, Iterator &it, bool inclusive) {
539: 	it.depth = 0;
540: 	bool equal = false;
541: 	if (!n) {
542: 		return false;
543: 	}
544: 	Node *node = n.get();
545: 
546: 	idx_t depth = 0;
547: 	while (true) {
548: 		auto &top = it.stack[it.depth];
549: 		top.node = node;
550: 		it.depth++;
551: 		if (!equal) {
552: 			while (node->type != NodeType::NLeaf) {
553: 				node = node->GetChild(node->GetMin())->get();
554: 				auto &c_top = it.stack[it.depth];
555: 				c_top.node = node;
556: 				it.depth++;
557: 			}
558: 		}
559: 		if (node->type == NodeType::NLeaf) {
560: 			// found a leaf node: check if it is bigger or equal than the current key
561: 			auto leaf = static_cast<Leaf *>(node);
562: 			it.node = leaf;
563: 			// if the search is not inclusive the leaf node could still be equal to the current value
564: 			// check if leaf is equal to the current key
565: 			if (*leaf->value == key) {
566: 				// if its not inclusive check if there is a next leaf
567: 				if (!inclusive && !IteratorNext(it)) {
568: 					return false;
569: 				} else {
570: 					return true;
571: 				}
572: 			}
573: 
574: 			if (*leaf->value > key) {
575: 				return true;
576: 			}
577: 			// Leaf is lower than key
578: 			// Check if next leaf is still lower than key
579: 			while (IteratorNext(it)) {
580: 				if (*it.node->value == key) {
581: 					// if its not inclusive check if there is a next leaf
582: 					if (!inclusive && !IteratorNext(it)) {
583: 						return false;
584: 					} else {
585: 						return true;
586: 					}
587: 				} else if (*it.node->value > key) {
588: 					// if its not inclusive check if there is a next leaf
589: 					return true;
590: 				}
591: 			}
592: 			return false;
593: 		}
594: 		uint32_t mismatchPos = Node::PrefixMismatch(*this, node, key, depth);
595: 		if (mismatchPos != node->prefix_length) {
596: 			if (node->prefix[mismatchPos] < key[depth + mismatchPos]) {
597: 				// Less
598: 				it.depth--;
599: 				return IteratorNext(it);
600: 			} else {
601: 				// Greater
602: 				top.pos = INVALID_INDEX;
603: 				return IteratorNext(it);
604: 			}
605: 		}
606: 		// prefix matches, search inside the child for the key
607: 		depth += node->prefix_length;
608: 
609: 		top.pos = node->GetChildGreaterEqual(key[depth], equal);
610: 		if (top.pos == INVALID_INDEX) {
611: 			// Find min leaf
612: 			top.pos = node->GetMin();
613: 		}
614: 		node = node->GetChild(top.pos)->get();
615: 		//! This means all children of this node qualify as geq
616: 
617: 		depth++;
618: 	}
619: }
620: 
621: void ART::SearchGreater(vector<row_t> &result_ids, ARTIndexScanState *state, bool inclusive) {
622: 	Iterator *it = &state->iterator;
623: 	auto key = CreateKey(*this, types[0], state->values[0]);
624: 
625: 	// greater than scan: first set the iterator to the node at which we will start our scan by finding the lowest node
626: 	// that satisfies our requirement
627: 	if (!it->start) {
628: 		bool found = ART::Bound(tree, *key, *it, inclusive);
629: 		if (!found) {
630: 			return;
631: 		}
632: 		it->start = true;
633: 	}
634: 	// after that we continue the scan; we don't need to check the bounds as any value following this value is
635: 	// automatically bigger and hence satisfies our predicate
636: 	IteratorScan<false, false>(state, it, result_ids, nullptr);
637: }
638: 
639: //===--------------------------------------------------------------------===//
640: // Less Than
641: //===--------------------------------------------------------------------===//
642: static Leaf &FindMinimum(Iterator &it, Node &node) {
643: 	Node *next = nullptr;
644: 	idx_t pos = 0;
645: 	switch (node.type) {
646: 	case NodeType::NLeaf:
647: 		it.node = (Leaf *)&node;
648: 		return (Leaf &)node;
649: 	case NodeType::N4:
650: 		next = ((Node4 &)node).child[0].get();
651: 		break;
652: 	case NodeType::N16:
653: 		next = ((Node16 &)node).child[0].get();
654: 		break;
655: 	case NodeType::N48: {
656: 		auto &n48 = (Node48 &)node;
657: 		while (n48.childIndex[pos] == Node::EMPTY_MARKER) {
658: 			pos++;
659: 		}
660: 		next = n48.child[n48.childIndex[pos]].get();
661: 		break;
662: 	}
663: 	case NodeType::N256: {
664: 		auto &n256 = (Node256 &)node;
665: 		while (!n256.child[pos]) {
666: 			pos++;
667: 		}
668: 		next = n256.child[pos].get();
669: 		break;
670: 	}
671: 	}
672: 	it.stack[it.depth].node = &node;
673: 	it.stack[it.depth].pos = pos;
674: 	it.depth++;
675: 	return FindMinimum(it, *next);
676: }
677: 
678: void ART::SearchLess(vector<row_t> &result_ids, ARTIndexScanState *state, bool inclusive) {
679: 	if (!tree) {
680: 		return;
681: 	}
682: 
683: 	Iterator *it = &state->iterator;
684: 	auto upper_bound = CreateKey(*this, types[0], state->values[0]);
685: 
686: 	if (!it->start) {
687: 		// first find the minimum value in the ART: we start scanning from this value
688: 		auto &minimum = FindMinimum(state->iterator, *tree);
689: 		// early out min value higher than upper bound query
690: 		if (*minimum.value > *upper_bound) {
691: 			return;
692: 		}
693: 		it->start = true;
694: 	}
695: 	// now continue the scan until we reach the upper bound
696: 	if (inclusive) {
697: 		IteratorScan<true, true>(state, it, result_ids, upper_bound.get());
698: 	} else {
699: 		IteratorScan<true, false>(state, it, result_ids, upper_bound.get());
700: 	}
701: }
702: 
703: //===--------------------------------------------------------------------===//
704: // Closed Range Query
705: //===--------------------------------------------------------------------===//
706: void ART::SearchCloseRange(vector<row_t> &result_ids, ARTIndexScanState *state, bool left_inclusive,
707:                            bool right_inclusive) {
708: 	auto lower_bound = CreateKey(*this, types[0], state->values[0]);
709: 	auto upper_bound = CreateKey(*this, types[0], state->values[1]);
710: 	Iterator *it = &state->iterator;
711: 	// first find the first node that satisfies the left predicate
712: 	if (!it->start) {
713: 		bool found = ART::Bound(tree, *lower_bound, *it, left_inclusive);
714: 		if (!found) {
715: 			return;
716: 		}
717: 		it->start = true;
718: 	}
719: 	// now continue the scan until we reach the upper bound
720: 	if (right_inclusive) {
721: 		IteratorScan<true, true>(state, it, result_ids, upper_bound.get());
722: 	} else {
723: 		IteratorScan<true, false>(state, it, result_ids, upper_bound.get());
724: 	}
725: }
726: 
727: void ART::Scan(Transaction &transaction, TableIndexScanState &table_state, DataChunk &result) {
728: 	auto state = (ARTIndexScanState *)table_state.index_state.get();
729: 
730: 	// scan the index
731: 	if (!state->checked) {
732: 		vector<row_t> result_ids;
733: 		assert(state->values[0].type == types[0]);
734: 
735: 		if (state->values[1].is_null) {
736: 			lock_guard<mutex> l(lock);
737: 			// single predicate
738: 			switch (state->expressions[0]) {
739: 			case ExpressionType::COMPARE_EQUAL:
740: 				SearchEqual(result_ids, state);
741: 				break;
742: 			case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
743: 				SearchGreater(result_ids, state, true);
744: 				break;
745: 			case ExpressionType::COMPARE_GREATERTHAN:
746: 				SearchGreater(result_ids, state, false);
747: 				break;
748: 			case ExpressionType::COMPARE_LESSTHANOREQUALTO:
749: 				SearchLess(result_ids, state, true);
750: 				break;
751: 			case ExpressionType::COMPARE_LESSTHAN:
752: 				SearchLess(result_ids, state, false);
753: 				break;
754: 			default:
755: 				throw NotImplementedException("Operation not implemented");
756: 			}
757: 		} else {
758: 			lock_guard<mutex> l(lock);
759: 			// two predicates
760: 			assert(state->values[1].type == types[0]);
761: 			bool left_inclusive = state->expressions[0] == ExpressionType ::COMPARE_GREATERTHANOREQUALTO;
762: 			bool right_inclusive = state->expressions[1] == ExpressionType ::COMPARE_LESSTHANOREQUALTO;
763: 			SearchCloseRange(result_ids, state, left_inclusive, right_inclusive);
764: 		}
765: 		state->checked = true;
766: 
767: 		if (result_ids.size() == 0) {
768: 			return;
769: 		}
770: 
771: 		// sort the row ids
772: 		sort(result_ids.begin(), result_ids.end());
773: 		// duplicate eliminate the row ids and append them to the row ids of the state
774: 		state->result_ids.reserve(result_ids.size());
775: 
776: 		state->result_ids.push_back(result_ids[0]);
777: 		for (idx_t i = 1; i < result_ids.size(); i++) {
778: 			if (result_ids[i] != result_ids[i - 1]) {
779: 				state->result_ids.push_back(result_ids[i]);
780: 			}
781: 		}
782: 	}
783: 
784: 	if (state->result_index >= state->result_ids.size()) {
785: 		// exhausted all row ids
786: 		return;
787: 	}
788: 
789: 	// create a vector pointing to the current set of row ids
790: 	Vector row_identifiers(ROW_TYPE, (data_ptr_t)&state->result_ids[state->result_index]);
791: 	idx_t scan_count = std::min((idx_t)STANDARD_VECTOR_SIZE, (idx_t)state->result_ids.size() - state->result_index);
792: 
793: 	// fetch the actual values from the base table
794: 	table.Fetch(transaction, result, state->column_ids, row_identifiers, scan_count, table_state);
795: 
796: 	// move to the next set of row ids
797: 	state->result_index += scan_count;
798: }
[end of src/execution/index/art/art.cpp]
[start of src/execution/operator/schema/physical_create_index.cpp]
1: #include "duckdb/execution/operator/schema/physical_create_index.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/index_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/execution/expression_executor.hpp"
7: 
8: using namespace std;
9: 
10: namespace duckdb {
11: 
12: void PhysicalCreateIndex::GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) {
13: 	if (column_ids.size() == 0) {
14: 		throw NotImplementedException("CREATE INDEX does not refer to any columns in the base table!");
15: 	}
16: 
17: 	auto &schema = *table.schema;
18: 	auto index_entry = (IndexCatalogEntry *)schema.CreateIndex(context, info.get());
19: 	if (!index_entry) {
20: 		// index already exists, but error ignored because of IF NOT EXISTS
21: 		return;
22: 	}
23: 
24: 	unique_ptr<Index> index;
25: 	switch (info->index_type) {
26: 	case IndexType::ART: {
27: 		index = make_unique<ART>(*table.storage, column_ids, move(unbound_expressions), info->unique);
28: 		break;
29: 	}
30: 	default:
31: 		assert(0);
32: 		throw NotImplementedException("Unimplemented index type");
33: 	}
34: 	index_entry->index = index.get();
35: 	index_entry->info = table.storage->info;
36: 	table.storage->AddIndex(move(index), expressions);
37: 
38: 	chunk.SetCardinality(0);
39: 	state->finished = true;
40: }
41: 
42: } // namespace duckdb
[end of src/execution/operator/schema/physical_create_index.cpp]
[start of src/include/duckdb/execution/index/art/art.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/index/art/art.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/types/data_chunk.hpp"
13: #include "duckdb/common/types/vector.hpp"
14: #include "duckdb/parser/parsed_expression.hpp"
15: #include "duckdb/storage/data_table.hpp"
16: #include "duckdb/storage/index.hpp"
17: 
18: #include "duckdb/execution/index/art/art_key.hpp"
19: #include "duckdb/execution/index/art/leaf.hpp"
20: #include "duckdb/execution/index/art/node.hpp"
21: #include "duckdb/execution/index/art/node4.hpp"
22: #include "duckdb/execution/index/art/node16.hpp"
23: #include "duckdb/execution/index/art/node48.hpp"
24: #include "duckdb/execution/index/art/node256.hpp"
25: 
26: namespace duckdb {
27: struct IteratorEntry {
28: 	Node *node = nullptr;
29: 	idx_t pos = 0;
30: };
31: 
32: struct Iterator {
33: 	//! The current Leaf Node, valid if depth>0
34: 	Leaf *node = nullptr;
35: 	//! The current depth
36: 	int32_t depth = 0;
37: 	//! Stack, actually the size is determined at runtime
38: 	IteratorEntry stack[9];
39: 
40: 	bool start = false;
41: };
42: 
43: struct ARTIndexScanState : public IndexScanState {
44: 	ARTIndexScanState(vector<column_t> column_ids) : IndexScanState(column_ids), checked(false), result_index(0) {
45: 	}
46: 
47: 	Value values[2];
48: 	ExpressionType expressions[2];
49: 	bool checked;
50: 	idx_t result_index = 0;
51: 	vector<row_t> result_ids;
52: 	Iterator iterator;
53: };
54: 
55: class ART : public Index {
56: public:
57: 	ART(DataTable &table, vector<column_t> column_ids, vector<unique_ptr<Expression>> unbound_expressions,
58: 	    bool is_unique = false);
59: 	~ART();
60: 
61: 	//! Root of the tree
62: 	unique_ptr<Node> tree;
63: 	//! True if machine is little endian
64: 	bool is_little_endian;
65: 	//! Whether or not the ART is an index built to enforce a UNIQUE constraint
66: 	bool is_unique;
67: 
68: public:
69: 	//! Initialize a scan on the index with the given expression and column ids
70: 	//! to fetch from the base table for a single predicate
71: 	unique_ptr<IndexScanState> InitializeScanSinglePredicate(Transaction &transaction, vector<column_t> column_ids,
72: 	                                                         Value value, ExpressionType expressionType) override;
73: 
74: 	//! Initialize a scan on the index with the given expression and column ids
75: 	//! to fetch from the base table for two predicates
76: 	unique_ptr<IndexScanState> InitializeScanTwoPredicates(Transaction &transaction, vector<column_t> column_ids,
77: 	                                                       Value low_value, ExpressionType low_expression_type,
78: 	                                                       Value high_value,
79: 	                                                       ExpressionType high_expression_type) override;
80: 
81: 	//! Perform a lookup on the index
82: 	void Scan(Transaction &transaction, TableIndexScanState &state, DataChunk &result) override;
83: 	//! Append entries to the index
84: 	bool Append(IndexLock &lock, DataChunk &entries, Vector &row_identifiers) override;
85: 	//! Verify that data can be appended to the index
86: 	void VerifyAppend(DataChunk &chunk) override;
87: 	//! Delete entries in the index
88: 	void Delete(IndexLock &lock, DataChunk &entries, Vector &row_identifiers) override;
89: 
90: 	//! Insert data into the index.
91: 	bool Insert(IndexLock &lock, DataChunk &data, Vector &row_ids) override;
92: 
93: private:
94: 	DataChunk expression_result;
95: 
96: private:
97: 	//! Insert a row id into a leaf node
98: 	bool InsertToLeaf(Leaf &leaf, row_t row_id);
99: 	//! Insert the leaf value into the tree
100: 	bool Insert(unique_ptr<Node> &node, unique_ptr<Key> key, unsigned depth, row_t row_id);
101: 
102: 	//! Erase element from leaf (if leaf has more than one value) or eliminate the leaf itself
103: 	void Erase(unique_ptr<Node> &node, Key &key, unsigned depth, row_t row_id);
104: 
105: 	//! Check if the key of the leaf is equal to the searched key
106: 	bool LeafMatches(Node *node, Key &key, unsigned depth);
107: 
108: 	//! Find the node with a matching key, optimistic version
109: 	Node *Lookup(unique_ptr<Node> &node, Key &key, unsigned depth);
110: 
111: 	//! Find the first node that is bigger (or equal to) a specific key
112: 	bool Bound(unique_ptr<Node> &node, Key &key, Iterator &iterator, bool inclusive);
113: 
114: 	//! Gets next node for range queries
115: 	bool IteratorNext(Iterator &iter);
116: 
117: 	void SearchEqual(vector<row_t> &result_ids, ARTIndexScanState *state);
118: 	void SearchGreater(vector<row_t> &result_ids, ARTIndexScanState *state, bool inclusive);
119: 	void SearchLess(vector<row_t> &result_ids, ARTIndexScanState *state, bool inclusive);
120: 	void SearchCloseRange(vector<row_t> &result_ids, ARTIndexScanState *state, bool left_inclusive,
121: 	                      bool right_inclusive);
122: 
123: private:
124: 	template <bool HAS_BOUND, bool INCLUSIVE>
125: 	void IteratorScan(ARTIndexScanState *state, Iterator *it, vector<row_t> &result_ids, Key *upper_bound);
126: 
127: 	void GenerateKeys(DataChunk &input, vector<unique_ptr<Key>> &keys);
128: };
129: 
130: } // namespace duckdb
[end of src/include/duckdb/execution/index/art/art.hpp]
[start of src/include/duckdb/storage/index.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/index.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_set.hpp"
12: #include "duckdb/common/enums/index_type.hpp"
13: #include "duckdb/common/types/data_chunk.hpp"
14: #include "duckdb/parser/parsed_expression.hpp"
15: #include "duckdb/planner/expression.hpp"
16: #include "duckdb/storage/table/scan_state.hpp"
17: #include "duckdb/execution/expression_executor.hpp"
18: 
19: namespace duckdb {
20: 
21: class ClientContext;
22: class DataTable;
23: class Transaction;
24: 
25: struct IndexLock;
26: 
27: //! The index is an abstract base class that serves as the basis for indexes
28: class Index {
29: public:
30: 	Index(IndexType type, DataTable &table, vector<column_t> column_ids,
31: 	      vector<unique_ptr<Expression>> unbound_expressions);
32: 	virtual ~Index() = default;
33: 
34: 	//! Lock used for updating the index
35: 	std::mutex lock;
36: 	//! The type of the index
37: 	IndexType type;
38: 	//! The table
39: 	DataTable &table;
40: 	//! Column identifiers to extract from the base table
41: 	vector<column_t> column_ids;
42: 	//! unordered_set of column_ids used by the index
43: 	unordered_set<column_t> column_id_set;
44: 	//! Unbound expressions used by the index
45: 	vector<unique_ptr<Expression>> unbound_expressions;
46: 	//! The types of the expressions
47: 	vector<TypeId> types;
48: 
49: public:
50: 	//! Initialize a scan on the index with the given expression and column ids
51: 	//! to fetch from the base table when we only have one query predicate
52: 	virtual unique_ptr<IndexScanState> InitializeScanSinglePredicate(Transaction &transaction,
53: 	                                                                 vector<column_t> column_ids, Value value,
54: 	                                                                 ExpressionType expressionType) = 0;
55: 	//! Initialize a scan on the index with the given expression and column ids
56: 	//! to fetch from the base table for two query predicates
57: 	virtual unique_ptr<IndexScanState> InitializeScanTwoPredicates(Transaction &transaction,
58: 	                                                               vector<column_t> column_ids, Value low_value,
59: 	                                                               ExpressionType low_expression_type, Value high_value,
60: 	                                                               ExpressionType high_expression_type) = 0;
61: 	//! Perform a lookup on the index
62: 	virtual void Scan(Transaction &transaction, TableIndexScanState &state, DataChunk &result) = 0;
63: 
64: 	//! Obtain a lock on the index
65: 	virtual void InitializeLock(IndexLock &state);
66: 	//! Called when data is appended to the index. The lock obtained from InitializeAppend must be held
67: 	virtual bool Append(IndexLock &state, DataChunk &entries, Vector &row_identifiers) = 0;
68: 	bool Append(DataChunk &entries, Vector &row_identifiers);
69: 	//! Verify that data can be appended to the index
70: 	virtual void VerifyAppend(DataChunk &chunk) {
71: 	}
72: 
73: 	//! Called when data inside the index is Deleted
74: 	virtual void Delete(IndexLock &state, DataChunk &entries, Vector &row_identifiers) = 0;
75: 	void Delete(DataChunk &entries, Vector &row_identifiers);
76: 
77: 	//! Insert data into the index. Does not lock the index.
78: 	virtual bool Insert(IndexLock &lock, DataChunk &input, Vector &row_identifiers) = 0;
79: 
80: 	//! Returns true if the index is affected by updates on the specified column ids, and false otherwise
81: 	bool IndexIsUpdated(vector<column_t> &column_ids);
82: 
83: protected:
84: 	void ExecuteExpressions(DataChunk &input, DataChunk &result);
85: 
86: private:
87: 	//! Bound expressions used by the index
88: 	vector<unique_ptr<Expression>> bound_expressions;
89: 	//! Expression executor for the index expressions
90: 	ExpressionExecutor executor;
91: 
92: 	unique_ptr<Expression> BindExpression(unique_ptr<Expression> expr);
93: };
94: 
95: } // namespace duckdb
[end of src/include/duckdb/storage/index.hpp]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/common/helper.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/planner/constraints/list.hpp"
9: #include "duckdb/transaction/transaction.hpp"
10: #include "duckdb/transaction/transaction_manager.hpp"
11: #include "duckdb/storage/table/transient_segment.hpp"
12: #include "duckdb/storage/storage_manager.hpp"
13: 
14: using namespace duckdb;
15: using namespace std;
16: using namespace chrono;
17: 
18: DataTable::DataTable(StorageManager &storage, string schema, string table, vector<TypeId> types_,
19:                      unique_ptr<vector<unique_ptr<PersistentSegment>>[]> data)
20:     : info(make_shared<DataTableInfo>(schema, table)), types(types_), storage(storage),
21:       persistent_manager(make_shared<VersionManager>(*info)), transient_manager(make_shared<VersionManager>(*info)),
22:       is_root(true) {
23: 	// set up the segment trees for the column segments
24: 	for (idx_t i = 0; i < types.size(); i++) {
25: 		auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
26: 		column_data->type = types[i];
27: 		column_data->column_idx = i;
28: 		columns.push_back(move(column_data));
29: 	}
30: 
31: 	// initialize the table with the existing data from disk, if any
32: 	if (data && data[0].size() > 0) {
33: 		// first append all the segments to the set of column segments
34: 		for (idx_t i = 0; i < types.size(); i++) {
35: 			columns[i]->Initialize(data[i]);
36: 			if (columns[i]->persistent_rows != columns[0]->persistent_rows) {
37: 				throw Exception("Column length mismatch in table load!");
38: 			}
39: 		}
40: 		persistent_manager->max_row = columns[0]->persistent_rows;
41: 		transient_manager->base_row = persistent_manager->max_row;
42: 	}
43: }
44: 
45: DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value)
46:     : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
47:       transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
48: 	// prevent any new tuples from being added to the parent
49: 	lock_guard<mutex> parent_lock(parent.append_lock);
50: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
51: 	parent.is_root = false;
52: 	// add the new column to this DataTable
53: 	auto new_column_type = GetInternalType(new_column.type);
54: 	idx_t new_column_idx = columns.size();
55: 
56: 	types.push_back(new_column_type);
57: 	auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
58: 	column_data->type = new_column_type;
59: 	column_data->column_idx = new_column_idx;
60: 	columns.push_back(move(column_data));
61: 
62: 	// fill the column with its DEFAULT value, or NULL if none is specified
63: 	idx_t rows_to_write = persistent_manager->max_row + transient_manager->max_row;
64: 	if (rows_to_write > 0) {
65: 		ExpressionExecutor executor;
66: 		DataChunk dummy_chunk;
67: 		Vector result(new_column_type);
68: 		if (!default_value) {
69: 			FlatVector::Nullmask(result).set();
70: 		} else {
71: 			executor.AddExpression(*default_value);
72: 		}
73: 
74: 		ColumnAppendState state;
75: 		columns[new_column_idx]->InitializeAppend(state);
76: 		for (idx_t i = 0; i < rows_to_write; i += STANDARD_VECTOR_SIZE) {
77: 			idx_t rows_in_this_vector = std::min(rows_to_write - i, (idx_t)STANDARD_VECTOR_SIZE);
78: 			if (default_value) {
79: 				dummy_chunk.SetCardinality(rows_in_this_vector);
80: 				executor.ExecuteExpression(dummy_chunk, result);
81: 			}
82: 			columns[new_column_idx]->Append(state, result, rows_in_this_vector);
83: 		}
84: 	}
85: 	// also add this column to client local storage
86: 	Transaction::GetTransaction(context).storage.AddColumn(&parent, this, new_column, default_value);
87: }
88: 
89: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
90:     : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
91:       transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
92: 	// prevent any new tuples from being added to the parent
93: 	lock_guard<mutex> parent_lock(parent.append_lock);
94: 	// first check if there are any indexes that exist that point to the removed column
95: 	for (auto &index : info->indexes) {
96: 		for (auto &column_id : index->column_ids) {
97: 			if (column_id == removed_column) {
98: 				throw CatalogException("Cannot drop this column: an index depends on it!");
99: 			} else if (column_id > removed_column) {
100: 				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
101: 			}
102: 		}
103: 	}
104: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
105: 	parent.is_root = false;
106: 	// erase the column from this DataTable
107: 	assert(removed_column < types.size());
108: 	types.erase(types.begin() + removed_column);
109: 	columns.erase(columns.begin() + removed_column);
110: }
111: 
112: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, SQLType target_type,
113:                      vector<column_t> bound_columns, Expression &cast_expr)
114:     : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
115:       transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
116: 
117: 	// prevent any new tuples from being added to the parent
118: 	lock_guard<mutex> parent_lock(parent.append_lock);
119: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
120: 	parent.is_root = false;
121: 	// first check if there are any indexes that exist that point to the changed column
122: 	for (auto &index : info->indexes) {
123: 		for (auto &column_id : index->column_ids) {
124: 			if (column_id == changed_idx) {
125: 				throw CatalogException("Cannot change the type of this column: an index depends on it!");
126: 			}
127: 		}
128: 	}
129: 	// change the type in this DataTable
130: 	auto new_type = GetInternalType(target_type);
131: 	types[changed_idx] = new_type;
132: 
133: 	// construct a new column data for this type
134: 	auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
135: 	column_data->type = new_type;
136: 	column_data->column_idx = changed_idx;
137: 
138: 	ColumnAppendState append_state;
139: 	column_data->InitializeAppend(append_state);
140: 
141: 	// scan the original table, and fill the new column with the transformed value
142: 	auto &transaction = Transaction::GetTransaction(context);
143: 	TableScanState scan_state;
144: 
145: 	vector<TypeId> types;
146: 	for (idx_t i = 0; i < bound_columns.size(); i++) {
147: 		types.push_back(parent.types[i]);
148: 	}
149: 	parent.InitializeScan(transaction, scan_state, bound_columns, nullptr);
150: 
151: 	DataChunk scan_chunk;
152: 	scan_chunk.Initialize(types);
153: 	unordered_map<idx_t, vector<TableFilter>> dummy_filters;
154: 
155: 	ExpressionExecutor executor;
156: 	executor.AddExpression(cast_expr);
157: 
158: 	Vector append_vector(new_type);
159: 	while (true) {
160: 		// scan the table
161: 		parent.Scan(transaction, scan_chunk, scan_state, dummy_filters);
162: 		if (scan_chunk.size() == 0) {
163: 			break;
164: 		}
165: 		// execute the expression
166: 		executor.ExecuteExpression(scan_chunk, append_vector);
167: 		column_data->Append(append_state, append_vector, scan_chunk.size());
168: 	}
169: 	// also add this column to client local storage
170: 	transaction.storage.ChangeType(&parent, this, changed_idx, target_type, bound_columns, cast_expr);
171: 
172: 	columns[changed_idx] = move(column_data);
173: }
174: 
175: //===--------------------------------------------------------------------===//
176: // Scan
177: //===--------------------------------------------------------------------===//
178: void DataTable::InitializeScan(TableScanState &state, vector<column_t> column_ids,
179:                                unordered_map<idx_t, vector<TableFilter>> *table_filters) {
180: 	// initialize a column scan state for each column
181: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
182: 	for (idx_t i = 0; i < column_ids.size(); i++) {
183: 		auto column = column_ids[i];
184: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
185: 			columns[column]->InitializeScan(state.column_scans[i]);
186: 		}
187: 	}
188: 	state.column_ids = move(column_ids);
189: 	// initialize the chunk scan state
190: 	state.offset = 0;
191: 	state.current_persistent_row = 0;
192: 	state.max_persistent_row = persistent_manager->max_row;
193: 	state.current_transient_row = 0;
194: 	state.max_transient_row = transient_manager->max_row;
195: 	if (table_filters && table_filters->size() > 0) {
196: 		state.adaptive_filter = make_unique<AdaptiveFilter>(*table_filters);
197: 	}
198: }
199: 
200: void DataTable::InitializeScan(Transaction &transaction, TableScanState &state, vector<column_t> column_ids,
201:                                unordered_map<idx_t, vector<TableFilter>> *table_filters) {
202: 	InitializeScan(state, move(column_ids), table_filters);
203: 	transaction.storage.InitializeScan(this, state.local_state);
204: }
205: 
206: void DataTable::Scan(Transaction &transaction, DataChunk &result, TableScanState &state,
207:                      unordered_map<idx_t, vector<TableFilter>> &table_filters) {
208: 	// scan the persistent segments
209: 	while (ScanBaseTable(transaction, result, state, state.current_persistent_row, state.max_persistent_row, 0,
210: 	                     *persistent_manager, table_filters)) {
211: 		if (result.size() > 0) {
212: 			return;
213: 		}
214: 	}
215: 	// scan the transient segments
216: 	while (ScanBaseTable(transaction, result, state, state.current_transient_row, state.max_transient_row,
217: 	                     persistent_manager->max_row, *transient_manager, table_filters)) {
218: 		if (result.size() > 0) {
219: 			return;
220: 		}
221: 	}
222: 
223: 	// scan the transaction-local segments
224: 	transaction.storage.Scan(state.local_state, state.column_ids, result, &table_filters);
225: }
226: 
227: template <class T> bool checkZonemap(TableScanState &state, TableFilter &table_filter, T constant) {
228: 	T *min = (T *)state.column_scans[table_filter.column_index].current->stats.minimum.get();
229: 	T *max = (T *)state.column_scans[table_filter.column_index].current->stats.maximum.get();
230: 	switch (table_filter.comparison_type) {
231: 	case ExpressionType::COMPARE_EQUAL:
232: 		return constant >= *min && constant <= *max;
233: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
234: 		return constant <= *max;
235: 	case ExpressionType::COMPARE_GREATERTHAN:
236: 		return constant < *max;
237: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
238: 		return constant >= *min;
239: 	case ExpressionType::COMPARE_LESSTHAN:
240: 		return constant > *min;
241: 	default:
242: 		throw NotImplementedException("Operation not implemented");
243: 	}
244: }
245: 
246: bool checkZonemapString(TableScanState &state, TableFilter &table_filter, const char *constant) {
247: 	char *min = (char *)state.column_scans[table_filter.column_index].current->stats.minimum.get();
248: 	char *max = (char *)state.column_scans[table_filter.column_index].current->stats.maximum.get();
249: 	int min_comp = strcmp(min, constant);
250: 	int max_comp = strcmp(max, constant);
251: 	switch (table_filter.comparison_type) {
252: 	case ExpressionType::COMPARE_EQUAL:
253: 		return min_comp <= 0 && max_comp >= 0;
254: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
255: 	case ExpressionType::COMPARE_GREATERTHAN:
256: 		return max_comp >= 0;
257: 	case ExpressionType::COMPARE_LESSTHAN:
258: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
259: 		return min_comp <= 0;
260: 	default:
261: 		throw NotImplementedException("Operation not implemented");
262: 	}
263: }
264: 
265: bool DataTable::CheckZonemap(TableScanState &state, unordered_map<idx_t, vector<TableFilter>> &table_filters,
266:                              idx_t &current_row) {
267: 	bool readSegment = true;
268: 	for (auto &table_filter : table_filters) {
269: 		for (auto &predicate_constant : table_filter.second) {
270: 			if (!state.column_scans[predicate_constant.column_index].segment_checked) {
271: 				state.column_scans[predicate_constant.column_index].segment_checked = true;
272: 				if (!state.column_scans[predicate_constant.column_index].current) {
273: 					return true;
274: 				}
275: 				switch (state.column_scans[predicate_constant.column_index].current->type) {
276: 				case TypeId::INT8: {
277: 					int8_t constant = predicate_constant.constant.value_.tinyint;
278: 					readSegment &= checkZonemap<int8_t>(state, predicate_constant, constant);
279: 					break;
280: 				}
281: 				case TypeId::INT16: {
282: 					int16_t constant = predicate_constant.constant.value_.smallint;
283: 					readSegment &= checkZonemap<int16_t>(state, predicate_constant, constant);
284: 					break;
285: 				}
286: 				case TypeId::INT32: {
287: 					int32_t constant = predicate_constant.constant.value_.integer;
288: 					readSegment &= checkZonemap<int32_t>(state, predicate_constant, constant);
289: 					break;
290: 				}
291: 				case TypeId::INT64: {
292: 					int64_t constant = predicate_constant.constant.value_.bigint;
293: 					readSegment &= checkZonemap<int64_t>(state, predicate_constant, constant);
294: 					break;
295: 				}
296: 				case TypeId::FLOAT: {
297: 					float constant = predicate_constant.constant.value_.float_;
298: 					readSegment &= checkZonemap<float>(state, predicate_constant, constant);
299: 					break;
300: 				}
301: 				case TypeId::DOUBLE: {
302: 					double constant = predicate_constant.constant.value_.double_;
303: 					readSegment &= checkZonemap<double>(state, predicate_constant, constant);
304: 					break;
305: 				}
306: 				case TypeId::VARCHAR: {
307: 					//! we can only compare the first 7 bytes
308: 					size_t value_size = predicate_constant.constant.str_value.size() > 7
309: 					                        ? 7
310: 					                        : predicate_constant.constant.str_value.size();
311: 					string constant;
312: 					for (size_t i = 0; i < value_size; i++) {
313: 						constant += predicate_constant.constant.str_value[i];
314: 					}
315: 					readSegment &= checkZonemapString(state, predicate_constant, constant.c_str());
316: 					break;
317: 				}
318: 				default:
319: 					throw NotImplementedException("Unimplemented type for uncompressed segment");
320: 				}
321: 			}
322: 			if (!readSegment) {
323: 				//! We can skip this partition
324: 				idx_t vectorsToSkip =
325: 				    ceil((double)(state.column_scans[predicate_constant.column_index].current->count +
326: 				                  state.column_scans[predicate_constant.column_index].current->start - current_row) /
327: 				         STANDARD_VECTOR_SIZE);
328: 				for (idx_t i = 0; i < vectorsToSkip; ++i) {
329: 					state.NextVector();
330: 					current_row += STANDARD_VECTOR_SIZE;
331: 				}
332: 				return false;
333: 			}
334: 		}
335: 	}
336: 
337: 	return true;
338: }
339: 
340: bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, TableScanState &state, idx_t &current_row,
341:                               idx_t max_row, idx_t base_row, VersionManager &manager,
342:                               unordered_map<idx_t, vector<TableFilter>> &table_filters) {
343: 	if (current_row >= max_row) {
344: 		// exceeded the amount of rows to scan
345: 		return false;
346: 	}
347: 	idx_t max_count = std::min((idx_t)STANDARD_VECTOR_SIZE, max_row - current_row);
348: 	idx_t vector_offset = current_row / STANDARD_VECTOR_SIZE;
349: 	//! first check the zonemap if we have to scan this partition
350: 	if (!CheckZonemap(state, table_filters, current_row)) {
351: 		return true;
352: 	}
353: 	// second, scan the version chunk manager to figure out which tuples to load for this transaction
354: 	SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
355: 	idx_t count = manager.GetSelVector(transaction, vector_offset, valid_sel, max_count);
356: 	if (count == 0) {
357: 		// nothing to scan for this vector, skip the entire vector
358: 		state.NextVector();
359: 		current_row += STANDARD_VECTOR_SIZE;
360: 		return true;
361: 	}
362: 	idx_t approved_tuple_count = count;
363: 	if (count == max_count && table_filters.empty()) {
364: 		//! If we don't have any deleted tuples or filters we can just run a regular scan
365: 		for (idx_t i = 0; i < state.column_ids.size(); i++) {
366: 			auto column = state.column_ids[i];
367: 			if (column == COLUMN_IDENTIFIER_ROW_ID) {
368: 				// scan row id
369: 				assert(result.data[i].type == ROW_TYPE);
370: 				result.data[i].Sequence(base_row + current_row, 1);
371: 			} else {
372: 				columns[column]->Scan(transaction, state.column_scans[i], result.data[i]);
373: 			}
374: 		}
375: 	} else {
376: 		SelectionVector sel;
377: 
378: 		if (count != max_count) {
379: 			sel.Initialize(valid_sel);
380: 		} else {
381: 			sel.Initialize(FlatVector::IncrementalSelectionVector);
382: 		}
383: 		//! First, we scan the columns with filters, fetch their data and generate a selection vector.
384: 		//! get runtime statistics
385: 		auto start_time = high_resolution_clock::now();
386: 		for (idx_t i = 0; i < table_filters.size(); i++) {
387: 			auto tf_idx = state.adaptive_filter->permutation[i];
388: 			columns[tf_idx]->Select(transaction, state.column_scans[tf_idx], result.data[tf_idx], sel,
389: 			                        approved_tuple_count, table_filters[tf_idx]);
390: 		}
391: 		for (auto &table_filter : table_filters) {
392: 			result.data[table_filter.first].Slice(sel, approved_tuple_count);
393: 		}
394: 		//! Now we use the selection vector to fetch data for the other columns.
395: 		for (idx_t i = 0; i < state.column_ids.size(); i++) {
396: 			if (table_filters.find(i) == table_filters.end()) {
397: 				auto column = state.column_ids[i];
398: 				if (column == COLUMN_IDENTIFIER_ROW_ID) {
399: 					assert(result.data[i].type == TypeId::INT64);
400: 					result.data[i].vector_type = VectorType::FLAT_VECTOR;
401: 					auto result_data = (int64_t *)FlatVector::GetData(result.data[i]);
402: 					for (size_t sel_idx = 0; sel_idx < approved_tuple_count; sel_idx++) {
403: 						result_data[sel_idx] = base_row + current_row + sel.get_index(sel_idx);
404: 					}
405: 				} else {
406: 					columns[column]->FilterScan(transaction, state.column_scans[i], result.data[i], sel,
407: 					                            approved_tuple_count);
408: 				}
409: 			}
410: 		}
411: 		auto end_time = high_resolution_clock::now();
412: 		if (state.adaptive_filter && table_filters.size() > 1) {
413: 			state.adaptive_filter->AdaptRuntimeStatistics(
414: 			    duration_cast<duration<double>>(end_time - start_time).count());
415: 		}
416: 	}
417: 
418: 	result.SetCardinality(approved_tuple_count);
419: 	current_row += STANDARD_VECTOR_SIZE;
420: 	return true;
421: }
422: 
423: //===--------------------------------------------------------------------===//
424: // Index Scan
425: //===--------------------------------------------------------------------===//
426: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index,
427:                                     vector<column_t> column_ids) {
428: 	state.index = &index;
429: 	state.column_ids = move(column_ids);
430: 	transaction.storage.InitializeScan(this, state.local_state);
431: }
432: 
433: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value value,
434:                                     ExpressionType expr_type, vector<column_t> column_ids) {
435: 	InitializeIndexScan(transaction, state, index, move(column_ids));
436: 	state.index_state = index.InitializeScanSinglePredicate(transaction, state.column_ids, value, expr_type);
437: }
438: 
439: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value low_value,
440:                                     ExpressionType low_type, Value high_value, ExpressionType high_type,
441:                                     vector<column_t> column_ids) {
442: 	InitializeIndexScan(transaction, state, index, move(column_ids));
443: 	state.index_state =
444: 	    index.InitializeScanTwoPredicates(transaction, state.column_ids, low_value, low_type, high_value, high_type);
445: }
446: 
447: void DataTable::IndexScan(Transaction &transaction, DataChunk &result, TableIndexScanState &state) {
448: 	// clear any previously pinned blocks
449: 	state.fetch_state.handles.clear();
450: 	// scan the index
451: 	state.index->Scan(transaction, state, result);
452: 	if (result.size() > 0) {
453: 		return;
454: 	}
455: 	// scan the local structure
456: 	transaction.storage.Scan(state.local_state, state.column_ids, result);
457: }
458: 
459: //===--------------------------------------------------------------------===//
460: // Fetch
461: //===--------------------------------------------------------------------===//
462: void DataTable::Fetch(Transaction &transaction, DataChunk &result, vector<column_t> &column_ids,
463:                       Vector &row_identifiers, idx_t fetch_count, TableIndexScanState &state) {
464: 	// first figure out which row identifiers we should use for this transaction by looking at the VersionManagers
465: 	row_t rows[STANDARD_VECTOR_SIZE];
466: 	idx_t count = FetchRows(transaction, row_identifiers, fetch_count, rows);
467: 
468: 	if (count == 0) {
469: 		// no rows to use
470: 		return;
471: 	}
472: 	// for each of the remaining rows, now fetch the data
473: 	result.SetCardinality(count);
474: 	for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
475: 		auto column = column_ids[col_idx];
476: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
477: 			// row id column: fill in the row ids
478: 			assert(result.data[col_idx].type == TypeId::INT64);
479: 			result.data[col_idx].vector_type = VectorType::FLAT_VECTOR;
480: 			auto data = FlatVector::GetData<row_t>(result.data[col_idx]);
481: 			for (idx_t i = 0; i < count; i++) {
482: 				data[i] = rows[i];
483: 			}
484: 		} else {
485: 			// regular column: fetch data from the base column
486: 			for (idx_t i = 0; i < count; i++) {
487: 				auto row_id = rows[i];
488: 				columns[column]->FetchRow(state.fetch_state, transaction, row_id, result.data[col_idx], i);
489: 			}
490: 		}
491: 	}
492: }
493: 
494: idx_t DataTable::FetchRows(Transaction &transaction, Vector &row_identifiers, idx_t fetch_count, row_t result_rows[]) {
495: 	assert(row_identifiers.type == ROW_TYPE);
496: 
497: 	// obtain a read lock on the version managers
498: 	auto l1 = persistent_manager->lock.GetSharedLock();
499: 	auto l2 = transient_manager->lock.GetSharedLock();
500: 
501: 	// now iterate over the row ids and figure out which rows to use
502: 	idx_t count = 0;
503: 
504: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
505: 	for (idx_t i = 0; i < fetch_count; i++) {
506: 		auto row_id = row_ids[i];
507: 		bool use_row;
508: 		if ((idx_t)row_id < persistent_manager->max_row) {
509: 			// persistent row: use persistent manager
510: 			use_row = persistent_manager->Fetch(transaction, row_id);
511: 		} else {
512: 			// transient row: use transient manager
513: 			use_row = transient_manager->Fetch(transaction, row_id);
514: 		}
515: 		if (use_row) {
516: 			// row is not deleted; use the row
517: 			result_rows[count++] = row_id;
518: 		}
519: 	}
520: 	return count;
521: }
522: 
523: //===--------------------------------------------------------------------===//
524: // Append
525: //===--------------------------------------------------------------------===//
526: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, string &col_name) {
527: 	if (VectorOperations::HasNull(vector, count)) {
528: 		throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name.c_str(), col_name.c_str());
529: 	}
530: }
531: 
532: static void VerifyCheckConstraint(TableCatalogEntry &table, Expression &expr, DataChunk &chunk) {
533: 	ExpressionExecutor executor(expr);
534: 	Vector result(TypeId::INT32);
535: 	try {
536: 		executor.ExecuteExpression(chunk, result);
537: 	} catch (Exception &ex) {
538: 		throw ConstraintException("CHECK constraint failed: %s (Error: %s)", table.name.c_str(), ex.what());
539: 	} catch (...) {
540: 		throw ConstraintException("CHECK constraint failed: %s (Unknown Error)", table.name.c_str());
541: 	}
542: 	VectorData vdata;
543: 	result.Orrify(chunk.size(), vdata);
544: 
545: 	auto dataptr = (int32_t *)vdata.data;
546: 	for (idx_t i = 0; i < chunk.size(); i++) {
547: 		auto idx = vdata.sel->get_index(i);
548: 		if (!(*vdata.nullmask)[idx] && dataptr[idx] == 0) {
549: 			throw ConstraintException("CHECK constraint failed: %s", table.name.c_str());
550: 		}
551: 	}
552: }
553: 
554: void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, DataChunk &chunk) {
555: 	for (auto &constraint : table.bound_constraints) {
556: 		switch (constraint->type) {
557: 		case ConstraintType::NOT_NULL: {
558: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
559: 			VerifyNotNullConstraint(table, chunk.data[not_null.index], chunk.size(),
560: 			                        table.columns[not_null.index].name);
561: 			break;
562: 		}
563: 		case ConstraintType::CHECK: {
564: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
565: 			VerifyCheckConstraint(table, *check.expression, chunk);
566: 			break;
567: 		}
568: 		case ConstraintType::UNIQUE: {
569: 			//! check whether or not the chunk can be inserted into the indexes
570: 			for (auto &index : info->indexes) {
571: 				index->VerifyAppend(chunk);
572: 			}
573: 			break;
574: 		}
575: 		case ConstraintType::FOREIGN_KEY:
576: 		default:
577: 			throw NotImplementedException("Constraint type not implemented!");
578: 		}
579: 	}
580: }
581: 
582: void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
583: 	if (chunk.size() == 0) {
584: 		return;
585: 	}
586: 	if (chunk.column_count() != table.columns.size()) {
587: 		throw CatalogException("Mismatch in column count for append");
588: 	}
589: 	if (!is_root) {
590: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
591: 	}
592: 
593: 	chunk.Verify();
594: 
595: 	// verify any constraints on the new chunk
596: 	VerifyAppendConstraints(table, chunk);
597: 
598: 	// append to the transaction local data
599: 	auto &transaction = Transaction::GetTransaction(context);
600: 	transaction.storage.Append(this, chunk);
601: }
602: 
603: void DataTable::InitializeAppend(TableAppendState &state) {
604: 	// obtain the append lock for this table
605: 	state.append_lock = unique_lock<mutex>(append_lock);
606: 	if (!is_root) {
607: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
608: 	}
609: 	// obtain locks on all indexes for the table
610: 	state.index_locks = unique_ptr<IndexLock[]>(new IndexLock[info->indexes.size()]);
611: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
612: 		info->indexes[i]->InitializeLock(state.index_locks[i]);
613: 	}
614: 	// for each column, initialize the append state
615: 	state.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[types.size()]);
616: 	for (idx_t i = 0; i < types.size(); i++) {
617: 		columns[i]->InitializeAppend(state.states[i]);
618: 	}
619: 	state.row_start = transient_manager->max_row;
620: 	state.current_row = state.row_start;
621: }
622: 
623: void DataTable::Append(Transaction &transaction, transaction_t commit_id, DataChunk &chunk, TableAppendState &state) {
624: 	assert(is_root);
625: 	assert(chunk.column_count() == types.size());
626: 	chunk.Verify();
627: 
628: 	// set up the inserted info in the version manager
629: 	transient_manager->Append(transaction, state.current_row, chunk.size(), commit_id);
630: 
631: 	// append the physical data to each of the entries
632: 	for (idx_t i = 0; i < types.size(); i++) {
633: 		columns[i]->Append(state.states[i], chunk.data[i], chunk.size());
634: 	}
635: 	info->cardinality += chunk.size();
636: 	state.current_row += chunk.size();
637: }
638: 
639: void DataTable::RevertAppend(TableAppendState &state) {
640: 	if (state.row_start == state.current_row) {
641: 		// nothing to revert!
642: 		return;
643: 	}
644: 	assert(is_root);
645: 	// revert changes in the base columns
646: 	for (idx_t i = 0; i < types.size(); i++) {
647: 		columns[i]->RevertAppend(state.row_start);
648: 	}
649: 	// adjust the cardinality
650: 	info->cardinality -= state.current_row - state.row_start;
651: 	transient_manager->max_row = state.row_start;
652: 	// revert changes in the transient manager
653: 	transient_manager->RevertAppend(state.row_start, state.current_row);
654: }
655: 
656: //===--------------------------------------------------------------------===//
657: // Indexes
658: //===--------------------------------------------------------------------===//
659: bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
660: 	assert(is_root);
661: 	if (info->indexes.size() == 0) {
662: 		return true;
663: 	}
664: 	// first generate the vector of row identifiers
665: 	Vector row_identifiers(ROW_TYPE);
666: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
667: 
668: 	idx_t failed_index = INVALID_INDEX;
669: 	// now append the entries to the indices
670: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
671: 		if (!info->indexes[i]->Append(state.index_locks[i], chunk, row_identifiers)) {
672: 			failed_index = i;
673: 			break;
674: 		}
675: 	}
676: 	if (failed_index != INVALID_INDEX) {
677: 		// constraint violation!
678: 		// remove any appended entries from previous indexes (if any)
679: 		for (idx_t i = 0; i < failed_index; i++) {
680: 			info->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
681: 		}
682: 		return false;
683: 	}
684: 	return true;
685: }
686: 
687: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
688: 	assert(is_root);
689: 	if (info->indexes.size() == 0) {
690: 		return;
691: 	}
692: 	// first generate the vector of row identifiers
693: 	Vector row_identifiers(ROW_TYPE);
694: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
695: 
696: 	// now remove the entries from the indices
697: 	RemoveFromIndexes(state, chunk, row_identifiers);
698: }
699: 
700: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
701: 	assert(is_root);
702: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
703: 		info->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
704: 	}
705: }
706: 
707: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
708: 	assert(is_root);
709: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
710: 	// create a selection vector from the row_ids
711: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
712: 	for (idx_t i = 0; i < count; i++) {
713: 		sel.set_index(i, row_ids[i] % STANDARD_VECTOR_SIZE);
714: 	}
715: 
716: 	// fetch the data for these row identifiers
717: 	DataChunk result;
718: 	result.Initialize(types);
719: 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
720: 	auto states = unique_ptr<ColumnScanState[]>(new ColumnScanState[types.size()]);
721: 	for (idx_t i = 0; i < types.size(); i++) {
722: 		columns[i]->Fetch(states[i], row_ids[0], result.data[i]);
723: 	}
724: 	result.Slice(sel, count);
725: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
726: 		info->indexes[i]->Delete(result, row_identifiers);
727: 	}
728: }
729: 
730: //===--------------------------------------------------------------------===//
731: // Delete
732: //===--------------------------------------------------------------------===//
733: void DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_identifiers, idx_t count) {
734: 	assert(row_identifiers.type == ROW_TYPE);
735: 	if (count == 0) {
736: 		return;
737: 	}
738: 
739: 	auto &transaction = Transaction::GetTransaction(context);
740: 
741: 	row_identifiers.Normalify(count);
742: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
743: 	auto first_id = ids[0];
744: 
745: 	if (first_id >= MAX_ROW_ID) {
746: 		// deletion is in transaction-local storage: push delete into local chunk collection
747: 		transaction.storage.Delete(this, row_identifiers, count);
748: 	} else if ((idx_t)first_id < persistent_manager->max_row) {
749: 		// deletion is in persistent storage: delete in the persistent version manager
750: 		persistent_manager->Delete(transaction, this, row_identifiers, count);
751: 	} else {
752: 		// deletion is in transient storage: delete in the persistent version manager
753: 		transient_manager->Delete(transaction, this, row_identifiers, count);
754: 	}
755: }
756: 
757: //===--------------------------------------------------------------------===//
758: // Update
759: //===--------------------------------------------------------------------===//
760: static void CreateMockChunk(vector<TypeId> &types, vector<column_t> &column_ids, DataChunk &chunk,
761:                             DataChunk &mock_chunk) {
762: 	// construct a mock DataChunk
763: 	mock_chunk.InitializeEmpty(types);
764: 	for (column_t i = 0; i < column_ids.size(); i++) {
765: 		mock_chunk.data[column_ids[i]].Reference(chunk.data[i]);
766: 	}
767: 	mock_chunk.SetCardinality(chunk.size());
768: }
769: 
770: static bool CreateMockChunk(TableCatalogEntry &table, vector<column_t> &column_ids,
771:                             unordered_set<column_t> &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
772: 	idx_t found_columns = 0;
773: 	// check whether the desired columns are present in the UPDATE clause
774: 	for (column_t i = 0; i < column_ids.size(); i++) {
775: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
776: 			found_columns++;
777: 		}
778: 	}
779: 	if (found_columns == 0) {
780: 		// no columns were found: no need to check the constraint again
781: 		return false;
782: 	}
783: 	if (found_columns != desired_column_ids.size()) {
784: 		// FIXME: not all columns in UPDATE clause are present!
785: 		// this should not be triggered at all as the binder should add these columns
786: 		throw NotImplementedException(
787: 		    "Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
788: 	}
789: 	// construct a mock DataChunk
790: 	auto types = table.GetTypes();
791: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
792: 	return true;
793: }
794: 
795: void DataTable::VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chunk, vector<column_t> &column_ids) {
796: 	for (auto &constraint : table.bound_constraints) {
797: 		switch (constraint->type) {
798: 		case ConstraintType::NOT_NULL: {
799: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
800: 			// check if the constraint is in the list of column_ids
801: 			for (idx_t i = 0; i < column_ids.size(); i++) {
802: 				if (column_ids[i] == not_null.index) {
803: 					// found the column id: check the data in
804: 					VerifyNotNullConstraint(table, chunk.data[i], chunk.size(), table.columns[not_null.index].name);
805: 					break;
806: 				}
807: 			}
808: 			break;
809: 		}
810: 		case ConstraintType::CHECK: {
811: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
812: 
813: 			DataChunk mock_chunk;
814: 			if (CreateMockChunk(table, column_ids, check.bound_columns, chunk, mock_chunk)) {
815: 				VerifyCheckConstraint(table, *check.expression, mock_chunk);
816: 			}
817: 			break;
818: 		}
819: 		case ConstraintType::UNIQUE:
820: 		case ConstraintType::FOREIGN_KEY:
821: 			break;
822: 		default:
823: 			throw NotImplementedException("Constraint type not implemented!");
824: 		}
825: 	}
826: 	// update should not be called for indexed columns!
827: 	// instead update should have been rewritten to delete + update on higher layer
828: #ifdef DEBUG
829: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
830: 		assert(!info->indexes[i]->IndexIsUpdated(column_ids));
831: 	}
832: #endif
833: }
834: 
835: void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids, vector<column_t> &column_ids,
836:                        DataChunk &updates) {
837: 	assert(row_ids.type == ROW_TYPE);
838: 
839: 	updates.Verify();
840: 	if (updates.size() == 0) {
841: 		return;
842: 	}
843: 
844: 	// first verify that no constraints are violated
845: 	VerifyUpdateConstraints(table, updates, column_ids);
846: 
847: 	// now perform the actual update
848: 	auto &transaction = Transaction::GetTransaction(context);
849: 
850: 	updates.Normalify();
851: 	row_ids.Normalify(updates.size());
852: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
853: 	if (first_id >= MAX_ROW_ID) {
854: 		// update is in transaction-local storage: push update into local storage
855: 		transaction.storage.Update(this, row_ids, column_ids, updates);
856: 		return;
857: 	}
858: 
859: 	for (idx_t i = 0; i < column_ids.size(); i++) {
860: 		auto column = column_ids[i];
861: 		assert(column != COLUMN_IDENTIFIER_ROW_ID);
862: 
863: 		columns[column]->Update(transaction, updates.data[i], row_ids, updates.size());
864: 	}
865: }
866: 
867: //===--------------------------------------------------------------------===//
868: // Create Index Scan
869: //===--------------------------------------------------------------------===//
870: void DataTable::InitializeCreateIndexScan(CreateIndexScanState &state, vector<column_t> column_ids) {
871: 	// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan
872: 	state.append_lock = unique_lock<mutex>(append_lock);
873: 	// get a read lock on the VersionManagers to prevent any further deletions
874: 	state.locks.push_back(persistent_manager->lock.GetSharedLock());
875: 	state.locks.push_back(transient_manager->lock.GetSharedLock());
876: 
877: 	InitializeScan(state, column_ids);
878: }
879: 
880: void DataTable::CreateIndexScan(CreateIndexScanState &state, DataChunk &result) {
881: 	// scan the persistent segments
882: 	if (ScanCreateIndex(state, result, state.current_persistent_row, state.max_persistent_row, 0)) {
883: 		return;
884: 	}
885: 	// scan the transient segments
886: 	if (ScanCreateIndex(state, result, state.current_transient_row, state.max_transient_row,
887: 	                    state.max_persistent_row)) {
888: 		return;
889: 	}
890: }
891: 
892: bool DataTable::ScanCreateIndex(CreateIndexScanState &state, DataChunk &result, idx_t &current_row, idx_t max_row,
893:                                 idx_t base_row) {
894: 	if (current_row >= max_row) {
895: 		return false;
896: 	}
897: 	idx_t count = std::min((idx_t)STANDARD_VECTOR_SIZE, max_row - current_row);
898: 
899: 	// scan the base columns to fetch the actual data
900: 	// note that we insert all data into the index, even if it is marked as deleted
901: 	// FIXME: tuples that are already "cleaned up" do not need to be inserted into the index!
902: 	for (idx_t i = 0; i < state.column_ids.size(); i++) {
903: 		auto column = state.column_ids[i];
904: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
905: 			// scan row id
906: 			assert(result.data[i].type == ROW_TYPE);
907: 			result.data[i].Sequence(base_row + current_row, 1);
908: 		} else {
909: 			// scan actual base column
910: 			columns[column]->IndexScan(state.column_scans[i], result.data[i]);
911: 		}
912: 	}
913: 	result.SetCardinality(count);
914: 
915: 	current_row += STANDARD_VECTOR_SIZE;
916: 	return count > 0;
917: }
918: 
919: void DataTable::AddIndex(unique_ptr<Index> index, vector<unique_ptr<Expression>> &expressions) {
920: 	DataChunk result;
921: 	result.Initialize(index->types);
922: 
923: 	DataChunk intermediate;
924: 	vector<TypeId> intermediate_types;
925: 	auto column_ids = index->column_ids;
926: 	column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
927: 	for (auto &id : index->column_ids) {
928: 		intermediate_types.push_back(types[id]);
929: 	}
930: 	intermediate_types.push_back(ROW_TYPE);
931: 	intermediate.Initialize(intermediate_types);
932: 
933: 	// initialize an index scan
934: 	CreateIndexScanState state;
935: 	InitializeCreateIndexScan(state, column_ids);
936: 
937: 	if (!is_root) {
938: 		throw TransactionException("Transaction conflict: cannot add an index to a table that has been altered!");
939: 	}
940: 
941: 	// now start incrementally building the index
942: 	IndexLock lock;
943: 	index->InitializeLock(lock);
944: 	ExpressionExecutor executor(expressions);
945: 	while (true) {
946: 		intermediate.Reset();
947: 		// scan a new chunk from the table to index
948: 		CreateIndexScan(state, intermediate);
949: 		if (intermediate.size() == 0) {
950: 			// finished scanning for index creation
951: 			// release all locks
952: 			break;
953: 		}
954: 		// resolve the expressions for this chunk
955: 		executor.Execute(intermediate, result);
956: 
957: 		// insert into the index
958: 		if (!index->Insert(lock, result, intermediate.data[intermediate.column_count() - 1])) {
959: 			throw ConstraintException("Cant create unique index, table contains duplicate data on indexed column(s)");
960: 		}
961: 	}
962: 	info->indexes.push_back(move(index));
963: }
[end of src/storage/data_table.cpp]
[start of src/storage/index.cpp]
1: #include "duckdb/storage/index.hpp"
2: #include "duckdb/execution/expression_executor.hpp"
3: #include "duckdb/planner/expression_iterator.hpp"
4: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
5: #include "duckdb/planner/expression/bound_reference_expression.hpp"
6: #include "duckdb/storage/table/append_state.hpp"
7: 
8: using namespace duckdb;
9: using namespace std;
10: 
11: Index::Index(IndexType type, DataTable &table, vector<column_t> column_ids,
12:              vector<unique_ptr<Expression>> unbound_expressions)
13:     : type(type), table(table), column_ids(column_ids), unbound_expressions(move(unbound_expressions)) {
14: 	for (auto &expr : this->unbound_expressions) {
15: 		types.push_back(expr->return_type);
16: 		bound_expressions.push_back(BindExpression(expr->Copy()));
17: 	}
18: 	for (auto &bound_expr : bound_expressions) {
19: 		executor.AddExpression(*bound_expr);
20: 	}
21: 	for (auto column_id : column_ids) {
22: 		column_id_set.insert(column_id);
23: 	}
24: }
25: 
26: void Index::InitializeLock(IndexLock &state) {
27: 	state.index_lock = unique_lock<mutex>(lock);
28: }
29: 
30: bool Index::Append(DataChunk &entries, Vector &row_identifiers) {
31: 	IndexLock state;
32: 	InitializeLock(state);
33: 	return Append(state, entries, row_identifiers);
34: }
35: 
36: void Index::Delete(DataChunk &entries, Vector &row_identifiers) {
37: 	IndexLock state;
38: 	InitializeLock(state);
39: 	Delete(state, entries, row_identifiers);
40: }
41: 
42: void Index::ExecuteExpressions(DataChunk &input, DataChunk &result) {
43: 	executor.Execute(input, result);
44: }
45: 
46: unique_ptr<Expression> Index::BindExpression(unique_ptr<Expression> expr) {
47: 	if (expr->type == ExpressionType::BOUND_COLUMN_REF) {
48: 		auto &bound_colref = (BoundColumnRefExpression &)*expr;
49: 		return make_unique<BoundReferenceExpression>(expr->return_type, column_ids[bound_colref.binding.column_index]);
50: 	}
51: 	ExpressionIterator::EnumerateChildren(*expr,
52: 	                                      [&](unique_ptr<Expression> expr) { return BindExpression(move(expr)); });
53: 	return expr;
54: }
55: 
56: bool Index::IndexIsUpdated(vector<column_t> &column_ids) {
57: 	for (auto &column : column_ids) {
58: 		if (column_id_set.find(column) != column_id_set.end()) {
59: 			return true;
60: 		}
61: 	}
62: 	return false;
63: }
[end of src/storage/index.cpp]
[start of src/storage/local_storage.cpp]
1: #include "duckdb/transaction/local_storage.hpp"
2: #include "duckdb/execution/index/art/art.hpp"
3: #include "duckdb/storage/table/append_state.hpp"
4: #include "duckdb/storage/write_ahead_log.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/storage/uncompressed_segment.hpp"
7: 
8: using namespace duckdb;
9: using namespace std;
10: 
11: LocalTableStorage::LocalTableStorage(DataTable &table) : max_row(0) {
12: 	for (auto &index : table.info->indexes) {
13: 		assert(index->type == IndexType::ART);
14: 		auto &art = (ART &)*index;
15: 		if (art.is_unique) {
16: 			// unique index: create a local ART index that maintains the same unique constraint
17: 			vector<unique_ptr<Expression>> unbound_expressions;
18: 			for (auto &expr : art.unbound_expressions) {
19: 				unbound_expressions.push_back(expr->Copy());
20: 			}
21: 			indexes.push_back(make_unique<ART>(table, art.column_ids, move(unbound_expressions), true));
22: 		}
23: 	}
24: }
25: 
26: LocalTableStorage::~LocalTableStorage() {
27: }
28: 
29: void LocalTableStorage::InitializeScan(LocalScanState &state) {
30: 	state.storage = this;
31: 
32: 	state.chunk_index = 0;
33: 	state.max_index = collection.chunks.size() - 1;
34: 	state.last_chunk_count = collection.chunks.back()->size();
35: }
36: 
37: void LocalTableStorage::Clear() {
38: 	collection.chunks.clear();
39: 	indexes.clear();
40: 	deleted_entries.clear();
41: }
42: 
43: void LocalStorage::InitializeScan(DataTable *table, LocalScanState &state) {
44: 	auto entry = table_storage.find(table);
45: 	if (entry == table_storage.end()) {
46: 		// no local storage for table: set scan to nullptr
47: 		state.storage = nullptr;
48: 		return;
49: 	}
50: 	state.storage = entry->second.get();
51: 	state.storage->InitializeScan(state);
52: }
53: 
54: void LocalStorage::Scan(LocalScanState &state, const vector<column_t> &column_ids, DataChunk &result,
55:                         unordered_map<idx_t, vector<TableFilter>> *table_filters) {
56: 	if (!state.storage || state.chunk_index > state.max_index) {
57: 		// nothing left to scan
58: 		result.Reset();
59: 		return;
60: 	}
61: 	auto &chunk = *state.storage->collection.chunks[state.chunk_index];
62: 	idx_t chunk_count = state.chunk_index == state.max_index ? state.last_chunk_count : chunk.size();
63: 	idx_t count = chunk_count;
64: 
65: 	// first create a selection vector from the deleted entries (if any)
66: 	SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
67: 	auto entry = state.storage->deleted_entries.find(state.chunk_index);
68: 	if (entry != state.storage->deleted_entries.end()) {
69: 		// deleted entries! create a selection vector
70: 		auto deleted = entry->second.get();
71: 		idx_t new_count = 0;
72: 		for (idx_t i = 0; i < count; i++) {
73: 			if (!deleted[i]) {
74: 				valid_sel.set_index(new_count++, i);
75: 			}
76: 		}
77: 		if (new_count == 0 && count > 0) {
78: 			// all entries in this chunk were deleted: continue to next chunk
79: 			state.chunk_index++;
80: 			Scan(state, column_ids, result, table_filters);
81: 			return;
82: 		}
83: 		count = new_count;
84: 	}
85: 
86: 	SelectionVector sel;
87: 	if (count != chunk_count) {
88: 		sel.Initialize(valid_sel);
89: 	} else {
90: 		sel.Initialize(FlatVector::IncrementalSelectionVector);
91: 	}
92: 	// now scan the vectors of the chunk
93: 	for (idx_t i = 0; i < column_ids.size(); i++) {
94: 		auto id = column_ids[i];
95: 		if (id == COLUMN_IDENTIFIER_ROW_ID) {
96: 			// row identifier: return a sequence of rowids starting from MAX_ROW_ID plus the row offset in the chunk
97: 			result.data[i].Sequence(MAX_ROW_ID + state.chunk_index * STANDARD_VECTOR_SIZE, 1);
98: 		} else {
99: 			result.data[i].Reference(chunk.data[id]);
100: 		}
101: 		idx_t approved_tuple_count = count;
102: 		if (table_filters) {
103: 			auto column_filters = table_filters->find(i);
104: 			if (column_filters != table_filters->end()) {
105: 				//! We have filters to apply here
106: 				for (auto &column_filter : column_filters->second) {
107: 					nullmask_t nullmask = FlatVector::Nullmask(result.data[i]);
108: 					UncompressedSegment::filterSelection(sel, result.data[i], column_filter, approved_tuple_count,
109: 					                                     nullmask);
110: 				}
111: 				count = approved_tuple_count;
112: 			}
113: 		}
114: 	}
115: 	if (count == 0) {
116: 		// all entries in this chunk were filtered:: Continue on next chunk
117: 		state.chunk_index++;
118: 		Scan(state, column_ids, result, table_filters);
119: 		return;
120: 	}
121: 	if (count == chunk_count) {
122: 		result.SetCardinality(count);
123: 	} else {
124: 		result.Slice(sel, count);
125: 	}
126: 	state.chunk_index++;
127: }
128: 
129: void LocalStorage::Append(DataTable *table, DataChunk &chunk) {
130: 	auto entry = table_storage.find(table);
131: 	LocalTableStorage *storage;
132: 	if (entry == table_storage.end()) {
133: 		auto new_storage = make_unique<LocalTableStorage>(*table);
134: 		storage = new_storage.get();
135: 		table_storage.insert(make_pair(table, move(new_storage)));
136: 	} else {
137: 		storage = entry->second.get();
138: 	}
139: 	// append to unique indices (if any)
140: 	if (storage->indexes.size() > 0) {
141: 		idx_t base_id = MAX_ROW_ID + storage->collection.count;
142: 
143: 		// first generate the vector of row identifiers
144: 		Vector row_ids(ROW_TYPE);
145: 		VectorOperations::GenerateSequence(row_ids, chunk.size(), base_id, 1);
146: 
147: 		// now append the entries to the indices
148: 		for (auto &index : storage->indexes) {
149: 			if (!index->Append(chunk, row_ids)) {
150: 				throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
151: 			}
152: 		}
153: 	}
154: 
155: 	//! Append to the chunk
156: 	storage->collection.Append(chunk);
157: }
158: 
159: LocalTableStorage *LocalStorage::GetStorage(DataTable *table) {
160: 	auto entry = table_storage.find(table);
161: 	assert(entry != table_storage.end());
162: 	return entry->second.get();
163: }
164: 
165: static idx_t GetChunk(Vector &row_ids) {
166: 	auto ids = FlatVector::GetData<row_t>(row_ids);
167: 	auto first_id = ids[0] - MAX_ROW_ID;
168: 
169: 	return first_id / STANDARD_VECTOR_SIZE;
170: }
171: 
172: void LocalStorage::Delete(DataTable *table, Vector &row_ids, idx_t count) {
173: 	auto storage = GetStorage(table);
174: 	// figure out the chunk from which these row ids came
175: 	idx_t chunk_idx = GetChunk(row_ids);
176: 	assert(chunk_idx < storage->collection.chunks.size());
177: 
178: 	// get a pointer to the deleted entries for this chunk
179: 	bool *deleted;
180: 	auto entry = storage->deleted_entries.find(chunk_idx);
181: 	if (entry == storage->deleted_entries.end()) {
182: 		// nothing deleted yet, add the deleted entries
183: 		auto del_entries = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);
184: 		memset(del_entries.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
185: 		deleted = del_entries.get();
186: 		storage->deleted_entries.insert(make_pair(chunk_idx, move(del_entries)));
187: 	} else {
188: 		deleted = entry->second.get();
189: 	}
190: 
191: 	// now actually mark the entries as deleted in the deleted vector
192: 	idx_t base_index = MAX_ROW_ID + chunk_idx * STANDARD_VECTOR_SIZE;
193: 
194: 	auto ids = FlatVector::GetData<row_t>(row_ids);
195: 	for (idx_t i = 0; i < count; i++) {
196: 		auto id = ids[i] - base_index;
197: 		deleted[id] = true;
198: 	}
199: }
200: 
201: template <class T>
202: static void update_data(Vector &data_vector, Vector &update_vector, Vector &row_ids, idx_t count, idx_t base_index) {
203: 	VectorData udata;
204: 	update_vector.Orrify(count, udata);
205: 
206: 	auto target = FlatVector::GetData<T>(data_vector);
207: 	auto &nullmask = FlatVector::Nullmask(data_vector);
208: 	auto ids = FlatVector::GetData<row_t>(row_ids);
209: 	auto updates = (T *)udata.data;
210: 
211: 	for (idx_t i = 0; i < count; i++) {
212: 		auto uidx = udata.sel->get_index(i);
213: 
214: 		auto id = ids[i] - base_index;
215: 		target[id] = updates[uidx];
216: 		nullmask[id] = (*udata.nullmask)[uidx];
217: 	}
218: }
219: 
220: static void update_chunk(Vector &data, Vector &updates, Vector &row_ids, idx_t count, idx_t base_index) {
221: 	assert(data.type == updates.type);
222: 	assert(row_ids.type == ROW_TYPE);
223: 
224: 	switch (data.type) {
225: 	case TypeId::INT8:
226: 		update_data<int8_t>(data, updates, row_ids, count, base_index);
227: 		break;
228: 	case TypeId::INT16:
229: 		update_data<int16_t>(data, updates, row_ids, count, base_index);
230: 		break;
231: 	case TypeId::INT32:
232: 		update_data<int32_t>(data, updates, row_ids, count, base_index);
233: 		break;
234: 	case TypeId::INT64:
235: 		update_data<int64_t>(data, updates, row_ids, count, base_index);
236: 		break;
237: 	case TypeId::FLOAT:
238: 		update_data<float>(data, updates, row_ids, count, base_index);
239: 		break;
240: 	case TypeId::DOUBLE:
241: 		update_data<double>(data, updates, row_ids, count, base_index);
242: 		break;
243: 	default:
244: 		throw Exception("Unsupported type for in-place update");
245: 	}
246: }
247: 
248: void LocalStorage::Update(DataTable *table, Vector &row_ids, vector<column_t> &column_ids, DataChunk &data) {
249: 	auto storage = GetStorage(table);
250: 	// figure out the chunk from which these row ids came
251: 	idx_t chunk_idx = GetChunk(row_ids);
252: 	assert(chunk_idx < storage->collection.chunks.size());
253: 
254: 	idx_t base_index = MAX_ROW_ID + chunk_idx * STANDARD_VECTOR_SIZE;
255: 
256: 	// now perform the actual update
257: 	auto &chunk = *storage->collection.chunks[chunk_idx];
258: 	for (idx_t i = 0; i < column_ids.size(); i++) {
259: 		auto col_idx = column_ids[i];
260: 		update_chunk(chunk.data[col_idx], data.data[i], row_ids, data.size(), base_index);
261: 	}
262: }
263: 
264: template <class T> bool LocalStorage::ScanTableStorage(DataTable *table, LocalTableStorage *storage, T &&fun) {
265: 	vector<column_t> column_ids;
266: 	for (idx_t i = 0; i < table->types.size(); i++) {
267: 		column_ids.push_back(i);
268: 	}
269: 
270: 	DataChunk chunk;
271: 	chunk.Initialize(table->types);
272: 
273: 	// initialize the scan
274: 	LocalScanState state;
275: 	storage->InitializeScan(state);
276: 
277: 	while (true) {
278: 		Scan(state, column_ids, chunk);
279: 		if (chunk.size() == 0) {
280: 			return true;
281: 		}
282: 		if (!fun(chunk)) {
283: 			return false;
284: 		}
285: 	}
286: }
287: 
288: void LocalStorage::Commit(LocalStorage::CommitState &commit_state, Transaction &transaction, WriteAheadLog *log,
289:                           transaction_t commit_id) {
290: 	// commit local storage, iterate over all entries in the table storage map
291: 	for (auto &entry : table_storage) {
292: 		auto table = entry.first;
293: 		auto storage = entry.second.get();
294: 
295: 		// initialize the append state
296: 		auto append_state_ptr = make_unique<TableAppendState>();
297: 		auto &append_state = *append_state_ptr;
298: 		// add it to the set of append states
299: 		commit_state.append_states[table] = move(append_state_ptr);
300: 		table->InitializeAppend(append_state);
301: 
302: 		if (log && !table->info->IsTemporary()) {
303: 			log->WriteSetTable(table->info->schema, table->info->table);
304: 		}
305: 
306: 		// scan all chunks in this storage
307: 		ScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {
308: 			// append this chunk to the indexes of the table
309: 			if (!table->AppendToIndexes(append_state, chunk, append_state.current_row)) {
310: 				throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
311: 			}
312: 
313: 			// append to base table
314: 			table->Append(transaction, commit_id, chunk, append_state);
315: 			// if there is a WAL, write the chunk to there as well
316: 			if (log && !table->info->IsTemporary()) {
317: 				log->WriteInsert(chunk);
318: 			}
319: 			return true;
320: 		});
321: 	}
322: 	// finished commit: clear local storage
323: 	for (auto &entry : table_storage) {
324: 		entry.second->Clear();
325: 	}
326: 	table_storage.clear();
327: }
328: 
329: void LocalStorage::RevertCommit(LocalStorage::CommitState &commit_state) {
330: 	for (auto &entry : commit_state.append_states) {
331: 		auto table = entry.first;
332: 		auto storage = table_storage[table].get();
333: 		auto &append_state = *entry.second;
334: 		if (table->info->indexes.size() > 0 && !table->info->IsTemporary()) {
335: 			row_t current_row = append_state.row_start;
336: 			// remove the data from the indexes, if there are any indexes
337: 			ScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {
338: 				// append this chunk to the indexes of the table
339: 				table->RemoveFromIndexes(append_state, chunk, current_row);
340: 
341: 				current_row += chunk.size();
342: 				if (current_row >= append_state.current_row) {
343: 					// finished deleting all rows from the index: abort now
344: 					return false;
345: 				}
346: 				return true;
347: 			});
348: 		}
349: 
350: 		table->RevertAppend(*entry.second);
351: 	}
352: }
353: 
354: void LocalStorage::AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column,
355:                              Expression *default_value) {
356: 	// check if there are any pending appends for the old version of the table
357: 	auto entry = table_storage.find(old_dt);
358: 	if (entry == table_storage.end()) {
359: 		return;
360: 	}
361: 	// take over the storage from the old entry
362: 	auto new_storage = move(entry->second);
363: 
364: 	// now add the new column filled with the default value to all chunks
365: 	auto new_column_type = GetInternalType(new_column.type);
366: 	ExpressionExecutor executor;
367: 	DataChunk dummy_chunk;
368: 	if (default_value) {
369: 		executor.AddExpression(*default_value);
370: 	}
371: 
372: 	new_storage->collection.types.push_back(new_column_type);
373: 	for (idx_t chunk_idx = 0; chunk_idx < new_storage->collection.chunks.size(); chunk_idx++) {
374: 		auto &chunk = new_storage->collection.chunks[chunk_idx];
375: 		Vector result(new_column_type);
376: 		if (default_value) {
377: 			dummy_chunk.SetCardinality(chunk->size());
378: 			executor.ExecuteExpression(dummy_chunk, result);
379: 		} else {
380: 			FlatVector::Nullmask(result).set();
381: 		}
382: 		chunk->data.push_back(move(result));
383: 	}
384: 
385: 	table_storage.erase(entry);
386: 	table_storage[new_dt] = move(new_storage);
387: }
388: 
389: void LocalStorage::ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, SQLType target_type,
390:                               vector<column_t> bound_columns, Expression &cast_expr) {
391: 	// check if there are any pending appends for the old version of the table
392: 	auto entry = table_storage.find(old_dt);
393: 	if (entry == table_storage.end()) {
394: 		return;
395: 	}
396: 	throw NotImplementedException("FIXME: ALTER TYPE with transaction local data not currently supported");
397: }
[end of src/storage/local_storage.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: