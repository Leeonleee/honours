{
  "repo": "duckdb/duckdb",
  "pull_number": 9273,
  "instance_id": "duckdb__duckdb-9273",
  "issue_numbers": [
    "8989"
  ],
  "base_commit": "67d3e1452832f9aa0c560d6f48bc3a0a8cae24a0",
  "patch": "diff --git a/extension/json/json_functions.cpp b/extension/json/json_functions.cpp\nindex 97a2cb4d3214..72246ab8d69d 100644\n--- a/extension/json/json_functions.cpp\n+++ b/extension/json/json_functions.cpp\n@@ -189,16 +189,7 @@ vector<TableFunctionSet> JSONFunctions::GetTableFunctions() {\n \n unique_ptr<TableRef> JSONFunctions::ReadJSONReplacement(ClientContext &context, const string &table_name,\n                                                         ReplacementScanData *data) {\n-\tauto lower_name = StringUtil::Lower(table_name);\n-\t// remove any compression\n-\tif (StringUtil::EndsWith(lower_name, \".gz\")) {\n-\t\tlower_name = lower_name.substr(0, lower_name.size() - 3);\n-\t} else if (StringUtil::EndsWith(lower_name, \".zst\")) {\n-\t\tlower_name = lower_name.substr(0, lower_name.size() - 4);\n-\t}\n-\tif (!StringUtil::EndsWith(lower_name, \".json\") && !StringUtil::Contains(lower_name, \".json?\") &&\n-\t    !StringUtil::EndsWith(lower_name, \".jsonl\") && !StringUtil::Contains(lower_name, \".jsonl?\") &&\n-\t    !StringUtil::EndsWith(lower_name, \".ndjson\") && !StringUtil::Contains(lower_name, \".ndjson?\")) {\n+\tif (!ReplacementScan::CanReplace(table_name, {\"json\", \"jsonl\", \"ndjson\"})) {\n \t\treturn nullptr;\n \t}\n \tauto table_function = make_uniq<TableFunctionRef>();\ndiff --git a/extension/parquet/parquet_extension.cpp b/extension/parquet/parquet_extension.cpp\nindex f2192bf16055..e897645e0c4e 100644\n--- a/extension/parquet/parquet_extension.cpp\n+++ b/extension/parquet/parquet_extension.cpp\n@@ -20,6 +20,8 @@\n #include \"duckdb/common/enums/file_compression_type.hpp\"\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/common/multi_file_reader.hpp\"\n+#include \"duckdb/common/serializer/deserializer.hpp\"\n+#include \"duckdb/common/serializer/serializer.hpp\"\n #include \"duckdb/common/types/chunk_collection.hpp\"\n #include \"duckdb/function/copy_function.hpp\"\n #include \"duckdb/function/table_function.hpp\"\n@@ -34,8 +36,6 @@\n #include \"duckdb/planner/operator/logical_get.hpp\"\n #include \"duckdb/storage/statistics/base_statistics.hpp\"\n #include \"duckdb/storage/table/row_group.hpp\"\n-#include \"duckdb/common/serializer/serializer.hpp\"\n-#include \"duckdb/common/serializer/deserializer.hpp\"\n #endif\n \n namespace duckdb {\n@@ -983,8 +983,7 @@ idx_t ParquetWriteDesiredBatchSize(ClientContext &context, FunctionData &bind_da\n //===--------------------------------------------------------------------===//\n unique_ptr<TableRef> ParquetScanReplacement(ClientContext &context, const string &table_name,\n                                             ReplacementScanData *data) {\n-\tauto lower_name = StringUtil::Lower(table_name);\n-\tif (!StringUtil::EndsWith(lower_name, \".parquet\") && !StringUtil::Contains(lower_name, \".parquet?\")) {\n+\tif (!ReplacementScan::CanReplace(table_name, {\"parquet\"})) {\n \t\treturn nullptr;\n \t}\n \tauto table_function = make_uniq<TableFunctionRef>();\ndiff --git a/src/include/duckdb/function/replacement_scan.hpp b/src/include/duckdb/function/replacement_scan.hpp\nindex 3447b96d468a..455f8ae19ebc 100644\n--- a/src/include/duckdb/function/replacement_scan.hpp\n+++ b/src/include/duckdb/function/replacement_scan.hpp\n@@ -9,6 +9,7 @@\n #pragma once\n \n #include \"duckdb/common/common.hpp\"\n+#include \"duckdb/common/string_util.hpp\"\n \n namespace duckdb {\n \n@@ -30,6 +31,25 @@ struct ReplacementScan {\n \t    : function(function), data(std::move(data_p)) {\n \t}\n \n+\tstatic bool CanReplace(const string &table_name, const vector<string> &extensions) {\n+\t\tauto lower_name = StringUtil::Lower(table_name);\n+\n+\t\tif (StringUtil::EndsWith(lower_name, \".gz\")) {\n+\t\t\tlower_name = lower_name.substr(0, lower_name.size() - 3);\n+\t\t} else if (StringUtil::EndsWith(lower_name, \".zst\")) {\n+\t\t\tlower_name = lower_name.substr(0, lower_name.size() - 4);\n+\t\t}\n+\n+\t\tfor (auto &extension : extensions) {\n+\t\t\tif (StringUtil::EndsWith(lower_name, \".\" + extension) ||\n+\t\t\t    StringUtil::Contains(lower_name, \".\" + extension + \"?\")) {\n+\t\t\t\treturn true;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn false;\n+\t}\n+\n \treplacement_scan_t function;\n \tunique_ptr<ReplacementScanData> data;\n };\ndiff --git a/src/parser/transform/statement/transform_copy.cpp b/src/parser/transform/statement/transform_copy.cpp\nindex 41c8bea840cb..d509480247e0 100644\n--- a/src/parser/transform/statement/transform_copy.cpp\n+++ b/src/parser/transform/statement/transform_copy.cpp\n@@ -1,6 +1,7 @@\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/common/types/value.hpp\"\n #include \"duckdb/core_functions/scalar/struct_functions.hpp\"\n+#include \"duckdb/function/replacement_scan.hpp\"\n #include \"duckdb/parser/expression/constant_expression.hpp\"\n #include \"duckdb/parser/expression/function_expression.hpp\"\n #include \"duckdb/parser/statement/copy_statement.hpp\"\n@@ -82,9 +83,10 @@ unique_ptr<CopyStatement> Transformer::TransformCopy(duckdb_libpgquery::PGCopySt\n \t\t// copy to a file\n \t\tinfo.file_path = stmt.filename;\n \t}\n-\tif (StringUtil::EndsWith(info.file_path, \".parquet\")) {\n+\n+\tif (ReplacementScan::CanReplace(info.file_path, {\"parquet\"})) {\n \t\tinfo.format = \"parquet\";\n-\t} else if (StringUtil::EndsWith(info.file_path, \".json\") || StringUtil::EndsWith(info.file_path, \".ndjson\")) {\n+\t} else if (ReplacementScan::CanReplace(info.file_path, {\"json\", \"jsonl\", \"ndjson\"})) {\n \t\tinfo.format = \"json\";\n \t} else {\n \t\tinfo.format = \"csv\";\n",
  "test_patch": "diff --git a/test/sql/json/table/read_json_many_files.test_slow b/test/sql/json/table/read_json_many_files.test_slow\nindex 3da4f199b0c1..6b11d0f7a3a2 100644\n--- a/test/sql/json/table/read_json_many_files.test_slow\n+++ b/test/sql/json/table/read_json_many_files.test_slow\n@@ -23,11 +23,11 @@ select count(*) from read_json_auto('__TEST_DIR__/input*.json');\n loop i 0 2000\n \n statement ok\n-copy input to '__TEST_DIR__/input${i}.json.gz' (HEADER 0);\n+copy input to '__TEST_DIR__/input${i}.json.gz' (COMPRESSION GZIP);\n \n endloop\n \n query T\n select count(*) from read_json_auto('__TEST_DIR__/input*.json.gz');\n ----\n-6000\n\\ No newline at end of file\n+6000\ndiff --git a/test/sql/json/test_json_copy.test_slow b/test/sql/json/test_json_copy.test_slow\nindex 0453a583886a..e7fca3838be1 100644\n--- a/test/sql/json/test_json_copy.test_slow\n+++ b/test/sql/json/test_json_copy.test_slow\n@@ -4,6 +4,24 @@\n \n require json\n \n+# test automatic detection even with .gz\n+statement ok\n+create table integers as select 42 i\n+\n+statement ok\n+copy integers to '__TEST_DIR__/integers.json.gz' (FORMAT JSON, COMPRESSION GZIP)\n+\n+statement ok\n+delete from integers\n+\n+statement ok\n+copy integers from '__TEST_DIR__/integers.json.gz'\n+\n+query T\n+select i from integers\n+----\n+42\n+\n # test writing all types to file\n statement ok\n create type small_enum as enum ('DUCK_DUCK_ENUM', 'GOOSE');\n@@ -192,7 +210,7 @@ copy (select range as i from range(10)) to '__TEST_DIR__/my.json' (COMPRESSION)\n Binder Error\n \n statement ok\n-copy (select range as i from range(10)) to '__TEST_DIR__/my.json.gz' (COMPRESSION GZIP, HEADER 0)\n+copy (select range as i from range(10)) to '__TEST_DIR__/my.json.gz' (COMPRESSION GZIP)\n \n statement ok\n create table my_range (i bigint)\n@@ -209,7 +227,7 @@ select * from '__TEST_DIR__/my.json.gz'\n mode skip\n \n statement ok\n-copy (select range as i from range(10)) to '__TEST_DIR__/my.json.zst' (COMPRESSION ZSTD, HEADER 0)\n+copy (select range as i from range(10)) to '__TEST_DIR__/my.json.zst' (COMPRESSION ZSTD)\n \n statement ok\n select * from '__TEST_DIR__/my.json.zst'\n",
  "problem_statement": "COPY FROM does not recognize json.gz files and treats it as CSV \n### What happens?\n\nDuckdb does not recognize files with a .json.gz extension and treats it as CSV. \r\nThis happens when using the COPY FROM syntax. \n\n### To Reproduce\n\necho '{\"id\": 234, \"name\": \"Test\"}' > test.json\r\ngzip test.json\r\n\r\nduckdb\r\ncreate table test (id integer, name varchar);\r\ncopy test from 'test.json.gz';\r\n\r\nError: Invalid Input Error: Could not convert string '{\"id\": 234' to INT32 at line 1 in column \"id\". Parser options:\r\n  file=test.json.gz\r\n  delimiter=',' (default)\r\n  quote='\"' (default)\r\n  escape='\"' (default)\r\n  header=0' (default)\r\n  sample_size=20480\r\n  ignore_errors=0\r\n  all_varchar=0 \r\n\r\n\n\n### OS:\n\nx86_64\n\n### DuckDB Version:\n\nv0.8.1 6536a77232\n\n### DuckDB Client:\n\ncli\n\n### Full Name:\n\nJasper Op de Coul\n\n### Affiliation:\n\nErasmus University Rotterdam\n\n### Have you tried this on the latest `main` branch?\n\nI have tested with a release build (and could not test with a main build)\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "",
  "created_at": "2023-10-09T11:08:48Z"
}