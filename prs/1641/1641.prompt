You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Support for parquet fixed_len_byte_array columns
Attempting to read a `fixed_len_byte_array` column from a parquet file yields the very unhelpful error:

```
Error: Not implemented Error: INVALID
```

Reproducer:
```
$ curl -O https://nelhage.com/files/fixed.parquet
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   135  100   135    0     0    480      0 --:--:-- --:--:-- --:--:--   480
$ duckdb test.duckdb "select * from parquet_scan('fixed.parquet');"
Error: Not implemented Error: INVALID
```


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "utf8proc_wrapper.hpp"
4: 
5: #include "snappy.h"
6: #include "miniz_wrapper.hpp"
7: #include "zstd.h"
8: #include <iostream>
9: 
10: #include "duckdb.hpp"
11: #ifndef DUCKDB_AMALGAMATION
12: #include "duckdb/common/types/chunk_collection.hpp"
13: #endif
14: 
15: namespace duckdb {
16: 
17: using parquet::format::CompressionCodec;
18: using parquet::format::ConvertedType;
19: using parquet::format::Encoding;
20: using parquet::format::PageType;
21: using parquet::format::Type;
22: 
23: const uint32_t RleBpDecoder::BITPACK_MASKS[] = {
24:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
25:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
26:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
27: 
28: const uint8_t RleBpDecoder::BITPACK_DLEN = 8;
29: 
30: ColumnReader::~ColumnReader() {
31: }
32: 
33: unique_ptr<ColumnReader> ColumnReader::CreateReader(const LogicalType &type_p, const SchemaElement &schema_p,
34:                                                     idx_t file_idx_p, idx_t max_define, idx_t max_repeat) {
35: 	switch (type_p.id()) {
36: 	case LogicalTypeId::BOOLEAN:
37: 		return make_unique<BooleanColumnReader>(type_p, schema_p, file_idx_p, max_define, max_repeat);
38: 	case LogicalTypeId::UTINYINT:
39: 		return make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
40: 		    type_p, schema_p, file_idx_p, max_define, max_repeat);
41: 	case LogicalTypeId::USMALLINT:
42: 		return make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
43: 		    type_p, schema_p, file_idx_p, max_define, max_repeat);
44: 	case LogicalTypeId::UINTEGER:
45: 		return make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
46: 		    type_p, schema_p, file_idx_p, max_define, max_repeat);
47: 	case LogicalTypeId::UBIGINT:
48: 		return make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
49: 		    type_p, schema_p, file_idx_p, max_define, max_repeat);
50: 	case LogicalTypeId::INTEGER:
51: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
52: 		    type_p, schema_p, file_idx_p, max_define, max_repeat);
53: 	case LogicalTypeId::BIGINT:
54: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
55: 		    type_p, schema_p, file_idx_p, max_define, max_repeat);
56: 	case LogicalTypeId::FLOAT:
57: 		return make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
58: 		    type_p, schema_p, file_idx_p, max_define, max_repeat);
59: 	case LogicalTypeId::DOUBLE:
60: 		return make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
61: 		    type_p, schema_p, file_idx_p, max_define, max_repeat);
62: 	case LogicalTypeId::TIMESTAMP:
63: 		switch (schema_p.type) {
64: 		case Type::INT96:
65: 			return make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
66: 			    type_p, schema_p, file_idx_p, max_define, max_repeat);
67: 		case Type::INT64:
68: 			switch (schema_p.converted_type) {
69: 			case ConvertedType::TIMESTAMP_MICROS:
70: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
71: 				    type_p, schema_p, file_idx_p, max_define, max_repeat);
72: 			case ConvertedType::TIMESTAMP_MILLIS:
73: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
74: 				    type_p, schema_p, file_idx_p, max_define, max_repeat);
75: 			default:
76: 				break;
77: 			}
78: 		default:
79: 			break;
80: 		}
81: 		break;
82: 	case LogicalTypeId::DATE:
83: 		return make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(type_p, schema_p, file_idx_p,
84: 		                                                                            max_define, max_repeat);
85: 	case LogicalTypeId::BLOB:
86: 	case LogicalTypeId::VARCHAR:
87: 		return make_unique<StringColumnReader>(type_p, schema_p, file_idx_p, max_define, max_repeat);
88: 	case LogicalTypeId::DECIMAL:
89: 		// we have to figure out what kind of int we need
90: 		switch (type_p.InternalType()) {
91: 		case PhysicalType::INT16:
92: 			return make_unique<DecimalColumnReader<int16_t>>(type_p, schema_p, file_idx_p, max_define, max_repeat);
93: 		case PhysicalType::INT32:
94: 			return make_unique<DecimalColumnReader<int32_t>>(type_p, schema_p, file_idx_p, max_define, max_repeat);
95: 		case PhysicalType::INT64:
96: 			return make_unique<DecimalColumnReader<int64_t>>(type_p, schema_p, file_idx_p, max_define, max_repeat);
97: 		case PhysicalType::INT128:
98: 			return make_unique<DecimalColumnReader<hugeint_t>>(type_p, schema_p, file_idx_p, max_define, max_repeat);
99: 
100: 		default:
101: 			break;
102: 		}
103: 		break;
104: 	default:
105: 		break;
106: 	}
107: 	throw NotImplementedException(type_p.ToString());
108: }
109: 
110: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
111: 	dict_decoder.reset();
112: 	defined_decoder.reset();
113: 	block.reset();
114: 
115: 	PageHeader page_hdr;
116: 	page_hdr.read(protocol);
117: 
118: 	//	page_hdr.printTo(std::cout);
119: 	//	std::cout << '\n';
120: 
121: 	PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
122: 
123: 	switch (page_hdr.type) {
124: 	case PageType::DATA_PAGE_V2:
125: 	case PageType::DATA_PAGE:
126: 		PrepareDataPage(page_hdr);
127: 		break;
128: 	case PageType::DICTIONARY_PAGE:
129: 		Dictionary(move(block), page_hdr.dictionary_page_header.num_values);
130: 		break;
131: 	default:
132: 		break; // ignore INDEX page type and any other custom extensions
133: 	}
134: }
135: 
136: void ColumnReader::PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size) {
137: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
138: 
139: 	block = make_shared<ResizeableBuffer>(compressed_page_size + 1);
140: 	trans.read((uint8_t *)block->ptr, compressed_page_size);
141: 
142: 	shared_ptr<ResizeableBuffer> unpacked_block;
143: 	if (chunk->meta_data.codec != CompressionCodec::UNCOMPRESSED) {
144: 		unpacked_block = make_shared<ResizeableBuffer>(uncompressed_page_size + 1);
145: 	}
146: 
147: 	switch (chunk->meta_data.codec) {
148: 	case CompressionCodec::UNCOMPRESSED:
149: 		break;
150: 	case CompressionCodec::GZIP: {
151: 		MiniZStream s;
152: 
153: 		s.Decompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr,
154: 		             uncompressed_page_size);
155: 		block = move(unpacked_block);
156: 
157: 		break;
158: 	}
159: 	case CompressionCodec::SNAPPY: {
160: 		auto res = snappy::RawUncompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr);
161: 		if (!res) {
162: 			throw std::runtime_error("Decompression failure");
163: 		}
164: 		block = move(unpacked_block);
165: 		break;
166: 	}
167: 	case CompressionCodec::ZSTD: {
168: 		auto res = duckdb_zstd::ZSTD_decompress((char *)unpacked_block->ptr, uncompressed_page_size,
169: 		                                        (const char *)block->ptr, compressed_page_size);
170: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)uncompressed_page_size) {
171: 			throw std::runtime_error("ZSTD Decompression failure");
172: 		}
173: 		block = move(unpacked_block);
174: 		break;
175: 	}
176: 
177: 	default: {
178: 		std::stringstream codec_name;
179: 		codec_name << chunk->meta_data.codec;
180: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
181: 		                         "\". Supported options are uncompressed, gzip or snappy");
182: 		break;
183: 	}
184: 	}
185: }
186: 
187: static uint8_t ComputeBitWidth(idx_t val) {
188: 	if (val == 0) {
189: 		return 0;
190: 	}
191: 	uint8_t ret = 1;
192: 	while (((idx_t)(1 << ret) - 1) < val) {
193: 		ret++;
194: 	}
195: 	return ret;
196: }
197: 
198: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
199: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
200: 		throw std::runtime_error("Missing data page header from data page");
201: 	}
202: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
203: 		throw std::runtime_error("Missing data page header from data page v2");
204: 	}
205: 
206: 	page_rows_available = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
207: 	                                                           : page_hdr.data_page_header_v2.num_values;
208: 	auto page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
209: 	                                                          : page_hdr.data_page_header_v2.encoding;
210: 
211: 	if (HasRepeats()) {
212: 		uint32_t rep_length = page_hdr.type == PageType::DATA_PAGE
213: 		                          ? block->read<uint32_t>()
214: 		                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;
215: 		block->available(rep_length);
216: 		repeated_decoder =
217: 		    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length, ComputeBitWidth(max_repeat));
218: 		block->inc(rep_length);
219: 	}
220: 
221: 	if (HasDefines()) {
222: 		uint32_t def_length = page_hdr.type == PageType::DATA_PAGE
223: 		                          ? block->read<uint32_t>()
224: 		                          : page_hdr.data_page_header_v2.definition_levels_byte_length;
225: 		block->available(def_length);
226: 		defined_decoder =
227: 		    make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length, ComputeBitWidth(max_define));
228: 		block->inc(def_length);
229: 	}
230: 
231: 	switch (page_encoding) {
232: 	case Encoding::RLE_DICTIONARY:
233: 	case Encoding::PLAIN_DICTIONARY: {
234: 		// TODO there seems to be some confusion whether this is in the bytes for v2
235: 		// where is it otherwise??
236: 		auto dict_width = block->read<uint8_t>();
237: 		// TODO somehow dict_width can be 0 ?
238: 		dict_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, block->len, dict_width);
239: 		block->inc(block->len);
240: 		break;
241: 	}
242: 	case Encoding::PLAIN:
243: 		// nothing to do here, will be read directly below
244: 		break;
245: 
246: 	default:
247: 		throw std::runtime_error("Unsupported page encoding");
248: 	}
249: }
250: 
251: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
252:                          Vector &result) {
253: 	// we need to reset the location because multiple column readers share the same protocol
254: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
255: 	trans.SetLocation(chunk_read_offset);
256: 
257: 	idx_t result_offset = 0;
258: 	auto to_read = num_values;
259: 
260: 	while (to_read > 0) {
261: 		while (page_rows_available == 0) {
262: 			PrepareRead(filter);
263: 		}
264: 
265: 		D_ASSERT(block);
266: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
267: 
268: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
269: 
270: 		if (HasRepeats()) {
271: 			D_ASSERT(repeated_decoder);
272: 			repeated_decoder->GetBatch<uint8_t>((char *)repeat_out + result_offset, read_now);
273: 		}
274: 
275: 		if (HasDefines()) {
276: 			D_ASSERT(defined_decoder);
277: 			defined_decoder->GetBatch<uint8_t>((char *)define_out + result_offset, read_now);
278: 		}
279: 
280: 		if (dict_decoder) {
281: 			// we need the null count because the offsets and plain values have no entries for nulls
282: 			idx_t null_count = 0;
283: 			if (HasDefines()) {
284: 				for (idx_t i = 0; i < read_now; i++) {
285: 					if (define_out[i + result_offset] != max_define) {
286: 						null_count++;
287: 					}
288: 				}
289: 			}
290: 
291: 			offset_buffer.resize(sizeof(uint32_t) * (read_now - null_count));
292: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
293: 			DictReference(result);
294: 			Offsets((uint32_t *)offset_buffer.ptr, define_out, read_now, filter, result_offset, result);
295: 		} else {
296: 			PlainReference(block, result);
297: 			Plain(block, define_out, read_now, filter, result_offset, result);
298: 		}
299: 
300: 		result_offset += read_now;
301: 		page_rows_available -= read_now;
302: 		to_read -= read_now;
303: 	}
304: 	group_rows_available -= num_values;
305: 	chunk_read_offset = trans.GetLocation();
306: 
307: 	return num_values;
308: }
309: 
310: void ColumnReader::Skip(idx_t num_values) {
311: 	dummy_define.zero();
312: 	dummy_repeat.zero();
313: 
314: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
315: 	auto values_read =
316: 	    Read(num_values, none_filter, (uint8_t *)dummy_define.ptr, (uint8_t *)dummy_repeat.ptr, dummy_result);
317: 	if (values_read != num_values) {
318: 		throw std::runtime_error("Row count mismatch when skipping rows");
319: 	}
320: }
321: 
322: void StringColumnReader::VerifyString(const char *str_data, idx_t str_len) {
323: 	if (Type() != LogicalTypeId::VARCHAR) {
324: 		return;
325: 	}
326: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
327: 	// technically Parquet should guarantee this, but reality is often disappointing
328: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len);
329: 	if (utf_type == UnicodeType::INVALID) {
330: 		throw InternalException("Invalid string encoding found in Parquet file: value is not valid UTF8!");
331: 	}
332: }
333: 
334: void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {
335: 	dict = move(data);
336: 	dict_strings = unique_ptr<string_t[]>(new string_t[num_entries]);
337: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
338: 		uint32_t str_len = dict->read<uint32_t>();
339: 		dict->available(str_len);
340: 
341: 		VerifyString(dict->ptr, str_len);
342: 		dict_strings[dict_idx] = string_t(dict->ptr, str_len);
343: 		dict->inc(str_len);
344: 	}
345: }
346: 
347: class ParquetStringVectorBuffer : public VectorBuffer {
348: public:
349: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
350: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(move(buffer_p)) {
351: 	}
352: 
353: private:
354: 	shared_ptr<ByteBuffer> buffer;
355: };
356: 
357: void StringColumnReader::DictReference(Vector &result) {
358: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
359: }
360: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
361: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(move(plain_data)));
362: }
363: 
364: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
365: 	auto &dict_strings = ((StringColumnReader &)reader).dict_strings;
366: 	return dict_strings[offset];
367: }
368: 
369: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
370: 	uint32_t str_len = plain_data.read<uint32_t>();
371: 	plain_data.available(str_len);
372: 	((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);
373: 	auto ret_str = string_t(plain_data.ptr, str_len);
374: 	plain_data.inc(str_len);
375: 	return ret_str;
376: }
377: 
378: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
379: 	uint32_t str_len = plain_data.read<uint32_t>();
380: 	plain_data.available(str_len);
381: 	plain_data.inc(str_len);
382: }
383: 
384: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
385:                              Vector &result_out) {
386: 	if (!ListVector::HasEntry(result_out)) {
387: 		auto list_child = make_unique<Vector>(result_out.GetType().child_types()[0].second);
388: 		ListVector::SetEntry(result_out, move(list_child));
389: 	}
390: 
391: 	idx_t result_offset = 0;
392: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
393: 
394: 	while (result_offset < num_values) {
395: 		auto child_req_num_values = MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
396: 
397: 		if (child_req_num_values == 0) {
398: 			break;
399: 		}
400: 
401: 		child_defines.zero();
402: 		child_repeats.zero();
403: 
404: 		idx_t child_actual_num_values = 0;
405: 
406: 		if (overflow_child_count == 0) {
407: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
408: 			                                                    child_repeats_ptr, child_result);
409: 		} else {
410: 			child_actual_num_values = overflow_child_count;
411: 			overflow_child_count = 0;
412: 			child_result.Reference(overflow_child_vector);
413: 		}
414: 
415: 		append_chunk.data[0].Reference(child_result);
416: 		append_chunk.SetCardinality(child_actual_num_values);
417: 		append_chunk.Verify();
418: 
419: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
420: 		ListVector::Append(result_out, append_chunk.data[0], append_chunk.size());
421: 		// hard-won piece of code this, modify at your own risk
422: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
423: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
424: 		idx_t child_idx;
425: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
426: 			if (child_repeats_ptr[child_idx] == max_repeat) { // value repeats on this level, append
427: 				D_ASSERT(result_offset > 0);
428: 				result_ptr[result_offset - 1].length++;
429: 				continue;
430: 			}
431: 			if (result_offset >= num_values) { // we ran out of output space
432: 				break;
433: 			}
434: 			if (child_defines_ptr[child_idx] >= max_define) {
435: 				// value has been defined down the stack, hence its NOT NULL
436: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
437: 				result_ptr[result_offset].length = 1;
438: 			} else {
439: 				// value is NULL somewhere up the stack
440: 				FlatVector::SetNull(result_out, result_offset, true);
441: 				result_ptr[result_offset].offset = 0;
442: 				result_ptr[result_offset].length = 0;
443: 			}
444: 
445: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
446: 			define_out[result_offset] = child_defines_ptr[child_idx];
447: 
448: 			result_offset++;
449: 		}
450: 
451: 		// we have read more values from the child reader than we can fit into the result for this read
452: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
453: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
454: 			overflow_child_vector.Slice(child_result, child_idx);
455: 			overflow_child_count = child_actual_num_values - child_idx;
456: 			overflow_child_vector.Verify(overflow_child_count);
457: 
458: 			// move values in the child repeats and defines *backward* by child_idx
459: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
460: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
461: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
462: 			}
463: 		}
464: 	}
465: 	result_out.Verify(result_offset);
466: 	return result_offset;
467: }
468: 
469: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of extension/parquet/include/column_reader.hpp]
1: #pragma once
2: 
3: #include "parquet_types.h"
4: #include "thrift_tools.hpp"
5: #include "resizable_buffer.hpp"
6: 
7: #include "parquet_rle_bp_decoder.hpp"
8: #include "parquet_statistics.hpp"
9: 
10: #include "duckdb.hpp"
11: #ifndef DUCKDB_AMALGAMATION
12: #include "duckdb/storage/statistics/string_statistics.hpp"
13: #include "duckdb/storage/statistics/numeric_statistics.hpp"
14: #include "duckdb/common/types/vector.hpp"
15: #include "duckdb/common/types/string_type.hpp"
16: #include "duckdb/common/types/chunk_collection.hpp"
17: #include "duckdb/common/operator/cast_operators.hpp"
18: #endif
19: 
20: namespace duckdb {
21: 
22: using apache::thrift::protocol::TProtocol;
23: 
24: using parquet::format::ColumnChunk;
25: using parquet::format::FieldRepetitionType;
26: using parquet::format::PageHeader;
27: using parquet::format::SchemaElement;
28: 
29: typedef std::bitset<STANDARD_VECTOR_SIZE> parquet_filter_t;
30: 
31: class ColumnReader {
32: 
33: public:
34: 	static unique_ptr<ColumnReader> CreateReader(const LogicalType &type_p, const SchemaElement &schema_p,
35: 	                                             idx_t schema_idx_p, idx_t max_define, idx_t max_repeat);
36: 
37: 	ColumnReader(LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define_p,
38: 	             idx_t max_repeat_p)
39: 	    : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), type(type_p),
40: 	      page_rows_available(0) {
41: 
42: 		// dummies for Skip()
43: 		dummy_result.Initialize(Type());
44: 		none_filter.none();
45: 		dummy_define.resize(STANDARD_VECTOR_SIZE);
46: 		dummy_repeat.resize(STANDARD_VECTOR_SIZE);
47: 	};
48: 
49: 	virtual void IntializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
50: 		D_ASSERT(file_idx < columns.size());
51: 		chunk = &columns[file_idx];
52: 		protocol = &protocol_p;
53: 		D_ASSERT(chunk);
54: 		D_ASSERT(chunk->__isset.meta_data);
55: 
56: 		if (chunk->__isset.file_path) {
57: 			throw std::runtime_error("Only inlined data files are supported (no references)");
58: 		}
59: 
60: 		// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.
61: 		chunk_read_offset = chunk->meta_data.data_page_offset;
62: 		if (chunk->meta_data.__isset.dictionary_page_offset && chunk->meta_data.dictionary_page_offset >= 4) {
63: 			// this assumes the data pages follow the dict pages directly.
64: 			chunk_read_offset = chunk->meta_data.dictionary_page_offset;
65: 		}
66: 		group_rows_available = chunk->meta_data.num_values;
67: 	}
68: 
69: 	virtual ~ColumnReader();
70: 
71: 	virtual idx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
72: 	                   Vector &result_out);
73: 
74: 	virtual void Skip(idx_t num_values);
75: 
76: 	const LogicalType &Type() {
77: 		return type;
78: 	}
79: 
80: 	const SchemaElement &Schema() {
81: 		return schema;
82: 	}
83: 
84: 	virtual idx_t GroupRowsAvailable() {
85: 		return group_rows_available;
86: 	}
87: 
88: 	unique_ptr<BaseStatistics> Stats(const std::vector<ColumnChunk> &columns) {
89: 		if (Type().id() == LogicalTypeId::LIST || Type().id() == LogicalTypeId::STRUCT) {
90: 			return nullptr;
91: 		}
92: 		return ParquetTransformColumnStatistics(Schema(), Type(), columns[file_idx]);
93: 	}
94: 
95: protected:
96: 	// readers that use the default Read() need to implement those
97: 	virtual void Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
98: 	                   idx_t result_offset, Vector &result) {
99: 		throw NotImplementedException("Plain");
100: 	}
101: 
102: 	virtual void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) {
103: 		throw NotImplementedException("Dictionary");
104: 	}
105: 
106: 	virtual void Offsets(uint32_t *offsets, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
107: 	                     idx_t result_offset, Vector &result) {
108: 		throw NotImplementedException("Offsets");
109: 	}
110: 
111: 	// these are nops for most types, but not for strings
112: 	virtual void DictReference(Vector &result) {
113: 	}
114: 	virtual void PlainReference(shared_ptr<ByteBuffer>, Vector &result) {
115: 	}
116: 
117: 	bool HasDefines() {
118: 		return max_define > 0;
119: 	}
120: 
121: 	bool HasRepeats() {
122: 		return max_repeat > 0;
123: 	}
124: 
125: 	const SchemaElement &schema;
126: 
127: 	idx_t file_idx;
128: 	idx_t max_define;
129: 	idx_t max_repeat;
130: 
131: private:
132: 	void PrepareRead(parquet_filter_t &filter);
133: 	void PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size);
134: 	void PrepareDataPage(PageHeader &page_hdr);
135: 
136: 	LogicalType type;
137: 	const parquet::format::ColumnChunk *chunk;
138: 
139: 	apache::thrift::protocol::TProtocol *protocol;
140: 	idx_t page_rows_available;
141: 	idx_t group_rows_available;
142: 	idx_t chunk_read_offset;
143: 
144: 	shared_ptr<ResizeableBuffer> block;
145: 
146: 	ResizeableBuffer offset_buffer;
147: 
148: 	unique_ptr<RleBpDecoder> dict_decoder;
149: 	unique_ptr<RleBpDecoder> defined_decoder;
150: 	unique_ptr<RleBpDecoder> repeated_decoder;
151: 
152: 	// dummies for Skip()
153: 	Vector dummy_result;
154: 	parquet_filter_t none_filter;
155: 	ResizeableBuffer dummy_define;
156: 	ResizeableBuffer dummy_repeat;
157: };
158: 
159: template <class VALUE_TYPE>
160: struct TemplatedParquetValueConversion {
161: 	static VALUE_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
162: 		D_ASSERT(offset < dict.len / sizeof(VALUE_TYPE));
163: 		return ((VALUE_TYPE *)dict.ptr)[offset];
164: 	}
165: 
166: 	static VALUE_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
167: 		return plain_data.read<VALUE_TYPE>();
168: 	}
169: 
170: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
171: 		plain_data.inc(sizeof(VALUE_TYPE));
172: 	}
173: };
174: 
175: template <class VALUE_TYPE, class VALUE_CONVERSION>
176: class TemplatedColumnReader : public ColumnReader {
177: 
178: public:
179: 	TemplatedColumnReader(LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p, idx_t max_define_p,
180: 	                      idx_t max_repeat_p)
181: 	    : ColumnReader(type_p, schema_p, schema_idx_p, max_define_p, max_repeat_p) {};
182: 
183: 	void Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) override {
184: 		dict = move(data);
185: 	}
186: 
187: 	void Offsets(uint32_t *offsets, uint8_t *defines, uint64_t num_values, parquet_filter_t &filter,
188: 	             idx_t result_offset, Vector &result) override {
189: 		auto result_ptr = FlatVector::GetData<VALUE_TYPE>(result);
190: 
191: 		idx_t offset_idx = 0;
192: 		for (idx_t row_idx = 0; row_idx < num_values; row_idx++) {
193: 			if (HasDefines() && defines[row_idx + result_offset] != max_define) {
194: 				FlatVector::SetNull(result, row_idx + result_offset, true);
195: 				continue;
196: 			}
197: 			if (filter[row_idx + result_offset]) {
198: 				VALUE_TYPE val = VALUE_CONVERSION::DictRead(*dict, offsets[offset_idx++], *this);
199: 				if (!Value::IsValid(val)) {
200: 					FlatVector::SetNull(result, row_idx + result_offset, true);
201: 					continue;
202: 				}
203: 				result_ptr[row_idx + result_offset] = val;
204: 			} else {
205: 				offset_idx++;
206: 			}
207: 		}
208: 	}
209: 
210: 	void Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, uint64_t num_values, parquet_filter_t &filter,
211: 	           idx_t result_offset, Vector &result) override {
212: 		auto result_ptr = FlatVector::GetData<VALUE_TYPE>(result);
213: 		for (idx_t row_idx = 0; row_idx < num_values; row_idx++) {
214: 			if (HasDefines() && defines[row_idx + result_offset] != max_define) {
215: 				FlatVector::SetNull(result, row_idx + result_offset, true);
216: 				continue;
217: 			}
218: 			if (filter[row_idx + result_offset]) {
219: 				VALUE_TYPE val = VALUE_CONVERSION::PlainRead(*plain_data, *this);
220: 				if (!Value::IsValid(val)) {
221: 					FlatVector::SetNull(result, row_idx + result_offset, true);
222: 					continue;
223: 				}
224: 				result_ptr[row_idx + result_offset] = val;
225: 			} else { // there is still some data there that we have to skip over
226: 				VALUE_CONVERSION::PlainSkip(*plain_data, *this);
227: 			}
228: 		}
229: 	}
230: 
231: 	shared_ptr<ByteBuffer> dict;
232: };
233: 
234: struct StringParquetValueConversion {
235: 	static string_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader);
236: 
237: 	static string_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader);
238: 
239: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader);
240: };
241: 
242: class StringColumnReader : public TemplatedColumnReader<string_t, StringParquetValueConversion> {
243: 
244: public:
245: 	StringColumnReader(LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p, idx_t max_define_p,
246: 	                   idx_t max_repeat_p)
247: 	    : TemplatedColumnReader<string_t, StringParquetValueConversion>(type_p, schema_p, schema_idx_p, max_define_p,
248: 	                                                                    max_repeat_p) {};
249: 
250: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) override;
251: 
252: 	unique_ptr<string_t[]> dict_strings;
253: 	void VerifyString(const char *str_data, idx_t str_len);
254: 
255: protected:
256: 	void DictReference(Vector &result) override;
257: 	void PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) override;
258: };
259: 
260: template <class DUCKDB_PHYSICAL_TYPE>
261: struct DecimalParquetValueConversion {
262: 
263: 	static DUCKDB_PHYSICAL_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
264: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)dict.ptr;
265: 		return dict_ptr[offset];
266: 	}
267: 
268: 	static DUCKDB_PHYSICAL_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
269: 		DUCKDB_PHYSICAL_TYPE res = 0;
270: 		auto byte_len = (idx_t)reader.Schema().type_length; /* sure, type length needs to be a signed int */
271: 		D_ASSERT(byte_len <= sizeof(DUCKDB_PHYSICAL_TYPE));
272: 		plain_data.available(byte_len);
273: 		auto res_ptr = (uint8_t *)&res;
274: 
275: 		// numbers are stored as two's complement so some muckery is required
276: 		bool positive = (*plain_data.ptr & 0x80) == 0;
277: 
278: 		for (idx_t i = 0; i < byte_len; i++) {
279: 			auto byte = *(plain_data.ptr + (byte_len - i - 1));
280: 			res_ptr[i] = positive ? byte : byte ^ 0xFF;
281: 		}
282: 		plain_data.inc(byte_len);
283: 		if (!positive) {
284: 			res += 1;
285: 			return -res;
286: 		}
287: 		return res;
288: 	}
289: 
290: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
291: 		plain_data.inc(reader.Schema().type_length);
292: 	}
293: };
294: 
295: template <class DUCKDB_PHYSICAL_TYPE>
296: class DecimalColumnReader
297:     : public TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE, DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE>> {
298: 
299: public:
300: 	DecimalColumnReader(LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define_p,
301: 	                    idx_t max_repeat_p)
302: 	    : TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE, DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE>>(
303: 	          type_p, schema_p, file_idx_p, max_define_p, max_repeat_p) {};
304: 
305: protected:
306: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) {
307: 		this->dict = make_shared<ResizeableBuffer>(num_entries * sizeof(DUCKDB_PHYSICAL_TYPE));
308: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)this->dict->ptr;
309: 		for (idx_t i = 0; i < num_entries; i++) {
310: 			dict_ptr[i] = DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE>::PlainRead(*dictionary_data, *this);
311: 		}
312: 	}
313: };
314: 
315: template <class PARQUET_PHYSICAL_TYPE, class DUCKDB_PHYSICAL_TYPE,
316:           DUCKDB_PHYSICAL_TYPE (*FUNC)(const PARQUET_PHYSICAL_TYPE &input)>
317: struct CallbackParquetValueConversion {
318: 	static DUCKDB_PHYSICAL_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
319: 		return TemplatedParquetValueConversion<DUCKDB_PHYSICAL_TYPE>::DictRead(dict, offset, reader);
320: 	}
321: 
322: 	static DUCKDB_PHYSICAL_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
323: 		return FUNC(plain_data.read<PARQUET_PHYSICAL_TYPE>());
324: 	}
325: 
326: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
327: 		plain_data.inc(sizeof(PARQUET_PHYSICAL_TYPE));
328: 	}
329: };
330: 
331: template <class PARQUET_PHYSICAL_TYPE, class DUCKDB_PHYSICAL_TYPE,
332:           DUCKDB_PHYSICAL_TYPE (*FUNC)(const PARQUET_PHYSICAL_TYPE &input)>
333: class CallbackColumnReader
334:     : public TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
335:                                    CallbackParquetValueConversion<PARQUET_PHYSICAL_TYPE, DUCKDB_PHYSICAL_TYPE, FUNC>> {
336: 
337: public:
338: 	CallbackColumnReader(LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define_p,
339: 	                     idx_t max_repeat_p)
340: 	    : TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
341: 	                            CallbackParquetValueConversion<PARQUET_PHYSICAL_TYPE, DUCKDB_PHYSICAL_TYPE, FUNC>>(
342: 	          type_p, schema_p, file_idx_p, max_define_p, max_repeat_p) {};
343: 
344: protected:
345: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) {
346: 		this->dict = make_shared<ResizeableBuffer>(num_entries * sizeof(DUCKDB_PHYSICAL_TYPE));
347: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)this->dict->ptr;
348: 		for (idx_t i = 0; i < num_entries; i++) {
349: 			dict_ptr[i] = FUNC(dictionary_data->read<PARQUET_PHYSICAL_TYPE>());
350: 		}
351: 	}
352: };
353: 
354: struct BooleanParquetValueConversion;
355: 
356: class BooleanColumnReader : public TemplatedColumnReader<bool, BooleanParquetValueConversion> {
357: public:
358: 	BooleanColumnReader(LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p, idx_t max_define_p,
359: 	                    idx_t max_repeat_p)
360: 	    : TemplatedColumnReader<bool, BooleanParquetValueConversion>(type_p, schema_p, schema_idx_p, max_define_p,
361: 	                                                                 max_repeat_p),
362: 	      byte_pos(0) {};
363: 
364: 	uint8_t byte_pos;
365: 
366: 	void IntializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) override {
367: 		byte_pos = 0;
368: 		TemplatedColumnReader<bool, BooleanParquetValueConversion>::IntializeRead(columns, protocol_p);
369: 	}
370: };
371: 
372: struct BooleanParquetValueConversion {
373: 	static bool DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
374: 		throw std::runtime_error("Dicts for booleans make no sense");
375: 	}
376: 
377: 	static bool PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
378: 		plain_data.available(1);
379: 		auto &byte_pos = ((BooleanColumnReader &)reader).byte_pos;
380: 		bool ret = (*plain_data.ptr >> byte_pos) & 1;
381: 		byte_pos++;
382: 		if (byte_pos == 8) {
383: 			byte_pos = 0;
384: 			plain_data.inc(1);
385: 		}
386: 		return ret;
387: 	}
388: 
389: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
390: 		PlainRead(plain_data, reader);
391: 	}
392: };
393: 
394: class StructColumnReader : public ColumnReader {
395: 
396: public:
397: 	StructColumnReader(LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p, idx_t max_define_p,
398: 	                   idx_t max_repeat_p, vector<unique_ptr<ColumnReader>> child_readers_p)
399: 	    : ColumnReader(type_p, schema_p, schema_idx_p, max_define_p, max_repeat_p),
400: 	      child_readers(move(child_readers_p)) {
401: 		D_ASSERT(type_p.id() == LogicalTypeId::STRUCT);
402: 		D_ASSERT(!type_p.child_types().empty());
403: 	};
404: 
405: 	ColumnReader *GetChildReader(idx_t child_idx) {
406: 		return child_readers[child_idx].get();
407: 	}
408: 
409: 	void IntializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) override {
410: 		for (auto &child : child_readers) {
411: 			child->IntializeRead(columns, protocol_p);
412: 		}
413: 	}
414: 
415: 	idx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
416: 	           Vector &result) override {
417: 		result.Initialize(Type());
418: 
419: 		for (idx_t i = 0; i < Type().child_types().size(); i++) {
420: 			auto child_read = make_unique<Vector>();
421: 			child_read->Initialize(Type().child_types()[i].second);
422: 			auto child_num_values = child_readers[i]->Read(num_values, filter, define_out, repeat_out, *child_read);
423: 			if (child_num_values != num_values) {
424: 				throw std::runtime_error("Struct child row count mismatch");
425: 			}
426: 			StructVector::AddEntry(result, Type().child_types()[i].first, move(child_read));
427: 		}
428: 
429: 		return num_values;
430: 	}
431: 
432: 	virtual void Skip(idx_t num_values) override {
433: 		D_ASSERT(0);
434: 	}
435: 
436: 	idx_t GroupRowsAvailable() override {
437: 		return child_readers[0]->GroupRowsAvailable();
438: 	}
439: 
440: 	vector<unique_ptr<ColumnReader>> child_readers;
441: };
442: 
443: class ListColumnReader : public ColumnReader {
444: public:
445: 	ListColumnReader(LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p, idx_t max_define_p,
446: 	                 idx_t max_repeat_p, unique_ptr<ColumnReader> child_column_reader_p)
447: 	    : ColumnReader(type_p, schema_p, schema_idx_p, max_define_p, max_repeat_p),
448: 	      child_column_reader(move(child_column_reader_p)), overflow_child_count(0) {
449: 
450: 		child_defines.resize(STANDARD_VECTOR_SIZE);
451: 		child_repeats.resize(STANDARD_VECTOR_SIZE);
452: 		child_defines_ptr = (uint8_t *)child_defines.ptr;
453: 		child_repeats_ptr = (uint8_t *)child_repeats.ptr;
454: 
455: 		auto child_type = Type().child_types()[0].second;
456: 		child_result.Initialize(child_type);
457: 
458: 		vector<LogicalType> append_chunk_types;
459: 		append_chunk_types.push_back(child_type);
460: 		append_chunk.Initialize(append_chunk_types);
461: 
462: 		child_filter.set();
463: 	};
464: 
465: 	idx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
466: 	           Vector &result_out) override;
467: 
468: 	virtual void Skip(idx_t num_values) override {
469: 		D_ASSERT(0);
470: 	}
471: 
472: 	void IntializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) override {
473: 		child_column_reader->IntializeRead(columns, protocol_p);
474: 	}
475: 
476: 	idx_t GroupRowsAvailable() override {
477: 		return child_column_reader->GroupRowsAvailable();
478: 	}
479: 
480: private:
481: 	unique_ptr<ColumnReader> child_column_reader;
482: 	ResizeableBuffer child_defines;
483: 	ResizeableBuffer child_repeats;
484: 	uint8_t *child_defines_ptr;
485: 	uint8_t *child_repeats_ptr;
486: 
487: 	Vector child_result;
488: 	parquet_filter_t child_filter;
489: 	DataChunk append_chunk;
490: 
491: 	Vector overflow_child_vector;
492: 	idx_t overflow_child_count;
493: };
494: 
495: } // namespace duckdb
[end of extension/parquet/include/column_reader.hpp]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_statistics.hpp"
4: #include "column_reader.hpp"
5: 
6: #include "thrift_tools.hpp"
7: 
8: #include "parquet_file_metadata_cache.hpp"
9: 
10: #include "duckdb.hpp"
11: #ifndef DUCKDB_AMALGAMATION
12: #include "duckdb/planner/table_filter.hpp"
13: #include "duckdb/common/file_system.hpp"
14: #include "duckdb/common/string_util.hpp"
15: #include "duckdb/common/types/date.hpp"
16: #include "duckdb/common/pair.hpp"
17: 
18: #include "duckdb/storage/object_cache.hpp"
19: #endif
20: 
21: #include <sstream>
22: #include <cassert>
23: #include <chrono>
24: #include <cstring>
25: #include <iostream>
26: 
27: namespace duckdb {
28: 
29: using parquet::format::ColumnChunk;
30: using parquet::format::ConvertedType;
31: using parquet::format::FieldRepetitionType;
32: using parquet::format::FileMetaData;
33: using parquet::format::RowGroup;
34: using parquet::format::SchemaElement;
35: using parquet::format::Statistics;
36: using parquet::format::Type;
37: 
38: static unique_ptr<apache::thrift::protocol::TProtocol> CreateThriftProtocol(FileHandle &file_handle) {
39: 	shared_ptr<ThriftFileTransport> trans(new ThriftFileTransport(file_handle));
40: 	return make_unique<apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(trans);
41: }
42: 
43: static shared_ptr<ParquetFileMetadataCache> LoadMetadata(FileHandle &file_handle) {
44: 	auto current_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());
45: 
46: 	auto proto = CreateThriftProtocol(file_handle);
47: 	auto &transport = ((ThriftFileTransport &)*proto->getTransport());
48: 	auto file_size = transport.GetSize();
49: 	if (file_size < 12) {
50: 		throw InvalidInputException("File '%s' too small to be a Parquet file", file_handle.path);
51: 	}
52: 
53: 	ResizeableBuffer buf;
54: 	buf.resize(8);
55: 	buf.zero();
56: 
57: 	transport.SetLocation(file_size - 8);
58: 	transport.read((uint8_t *)buf.ptr, 8);
59: 
60: 	if (strncmp(buf.ptr + 4, "PAR1", 4) != 0) {
61: 		throw InvalidInputException("No magic bytes found at end of file '%s'", file_handle.path);
62: 	}
63: 	// read four-byte footer length from just before the end magic bytes
64: 	auto footer_len = *(uint32_t *)buf.ptr;
65: 	if (footer_len <= 0 || file_size < 12 + footer_len) {
66: 		throw InvalidInputException("Footer length error in file '%s'", file_handle.path);
67: 	}
68: 	auto metadata_pos = file_size - (footer_len + 8);
69: 	transport.SetLocation(metadata_pos);
70: 
71: 	auto metadata = make_unique<FileMetaData>();
72: 	metadata->read(proto.get());
73: 	return make_shared<ParquetFileMetadataCache>(move(metadata), current_time);
74: }
75: 
76: static LogicalType DeriveLogicalType(const SchemaElement &s_ele) {
77: 	// inner node
78: 	D_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);
79: 	switch (s_ele.type) {
80: 	case Type::BOOLEAN:
81: 		return LogicalType::BOOLEAN;
82: 	case Type::INT32:
83: 		if (s_ele.__isset.converted_type) {
84: 			switch (s_ele.converted_type) {
85: 			case ConvertedType::DATE:
86: 				return LogicalType::DATE;
87: 			case ConvertedType::UINT_8:
88: 				return LogicalType::UTINYINT;
89: 			case ConvertedType::UINT_16:
90: 				return LogicalType::USMALLINT;
91: 			default:
92: 				return LogicalType::INTEGER;
93: 			}
94: 		}
95: 		return LogicalType::INTEGER;
96: 	case Type::INT64:
97: 		if (s_ele.__isset.converted_type) {
98: 			switch (s_ele.converted_type) {
99: 			case ConvertedType::TIMESTAMP_MICROS:
100: 			case ConvertedType::TIMESTAMP_MILLIS:
101: 				return LogicalType::TIMESTAMP;
102: 			case ConvertedType::UINT_32:
103: 				return LogicalType::UINTEGER;
104: 			case ConvertedType::UINT_64:
105: 				return LogicalType::UBIGINT;
106: 			default:
107: 				return LogicalType::BIGINT;
108: 			}
109: 		}
110: 		return LogicalType::BIGINT;
111: 
112: 	case Type::INT96: // always a timestamp it would seem
113: 		return LogicalType::TIMESTAMP;
114: 	case Type::FLOAT:
115: 		return LogicalType::FLOAT;
116: 	case Type::DOUBLE:
117: 		return LogicalType::DOUBLE;
118: 		//			case parquet::format::Type::FIXED_LEN_BYTE_ARRAY: {
119: 		// TODO some decimals yuck
120: 	case Type::BYTE_ARRAY:
121: 		if (s_ele.__isset.converted_type) {
122: 			switch (s_ele.converted_type) {
123: 			case ConvertedType::UTF8:
124: 				return LogicalType::VARCHAR;
125: 			default:
126: 				return LogicalType::BLOB;
127: 			}
128: 		}
129: 		return LogicalType::BLOB;
130: 	case Type::FIXED_LEN_BYTE_ARRAY:
131: 		if (s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::DECIMAL && s_ele.__isset.scale &&
132: 		    s_ele.__isset.scale && s_ele.__isset.type_length) {
133: 			// habemus decimal
134: 			return LogicalType(LogicalTypeId::DECIMAL, s_ele.precision, s_ele.scale);
135: 		}
136: 	default:
137: 		return LogicalType::INVALID;
138: 	}
139: }
140: 
141: static unique_ptr<ColumnReader> CreateReaderRecursive(const FileMetaData *file_meta_data, idx_t depth, idx_t max_define,
142:                                                       idx_t max_repeat, idx_t &next_schema_idx, idx_t &next_file_idx) {
143: 	D_ASSERT(file_meta_data);
144: 	D_ASSERT(next_schema_idx < file_meta_data->schema.size());
145: 	auto &s_ele = file_meta_data->schema[next_schema_idx];
146: 	auto this_idx = next_schema_idx;
147: 
148: 	if (s_ele.__isset.repetition_type) {
149: 		if (s_ele.repetition_type != FieldRepetitionType::REQUIRED) {
150: 			max_define = depth;
151: 		}
152: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
153: 			max_repeat++;
154: 		}
155: 	}
156: 
157: 	if (!s_ele.__isset.type) { // inner node
158: 		if (s_ele.num_children == 0) {
159: 			throw std::runtime_error("Node has no children but should");
160: 		}
161: 		child_list_t<LogicalType> child_types;
162: 		vector<unique_ptr<ColumnReader>> child_readers;
163: 
164: 		idx_t c_idx = 0;
165: 		while (c_idx < (idx_t)s_ele.num_children) {
166: 			next_schema_idx++;
167: 
168: 			auto &child_ele = file_meta_data->schema[next_schema_idx];
169: 
170: 			auto child_reader = CreateReaderRecursive(file_meta_data, depth + 1, max_define, max_repeat,
171: 			                                          next_schema_idx, next_file_idx);
172: 			child_types.push_back(make_pair(child_ele.name, child_reader->Type()));
173: 			child_readers.push_back(move(child_reader));
174: 
175: 			c_idx++;
176: 		}
177: 		D_ASSERT(!child_types.empty());
178: 		unique_ptr<ColumnReader> result;
179: 		LogicalType result_type;
180: 		// if we only have a single child no reason to create a struct ay
181: 		if (child_types.size() > 1 || depth == 0) {
182: 			result_type = LogicalType(LogicalTypeId::STRUCT, child_types);
183: 			result = make_unique<StructColumnReader>(result_type, s_ele, this_idx, max_define, max_repeat,
184: 			                                         move(child_readers));
185: 		} else {
186: 			// if we have a struct with only a single type, pull up
187: 			result_type = child_types[0].second;
188: 			result = move(child_readers[0]);
189: 		}
190: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
191: 			result_type = LogicalType(LogicalTypeId::LIST, {make_pair("", result_type)});
192: 			return make_unique<ListColumnReader>(result_type, s_ele, this_idx, max_define, max_repeat, move(result));
193: 		}
194: 		return result;
195: 	} else { // leaf node
196: 		// TODO check return value of derive type or should we only do this on read()
197: 		return ColumnReader::CreateReader(DeriveLogicalType(s_ele), s_ele, next_file_idx++, max_define, max_repeat);
198: 	}
199: }
200: 
201: // TODO we don't need readers for columns we are not going to read ay
202: static unique_ptr<ColumnReader> CreateReader(const FileMetaData *file_meta_data) {
203: 	idx_t next_schema_idx = 0;
204: 	idx_t next_file_idx = 0;
205: 
206: 	auto ret = CreateReaderRecursive(file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);
207: 	D_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);
208: 	D_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());
209: 	return ret;
210: }
211: 
212: void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p) {
213: 	auto file_meta_data = GetFileMetadata();
214: 
215: 	if (file_meta_data->__isset.encryption_algorithm) {
216: 		throw FormatException("Encrypted Parquet files are not supported");
217: 	}
218: 	// check if we like this schema
219: 	if (file_meta_data->schema.size() < 2) {
220: 		throw FormatException("Need at least one non-root column in the file");
221: 	}
222: 
223: 	bool has_expected_types = !expected_types_p.empty();
224: 	auto root_reader = CreateReader(file_meta_data);
225: 
226: 	auto root_type = root_reader->Type();
227: 	D_ASSERT(root_type.id() == LogicalTypeId::STRUCT);
228: 	idx_t col_idx = 0;
229: 	for (auto &type_pair : root_type.child_types()) {
230: 		if (has_expected_types && expected_types_p[col_idx] != type_pair.second) {
231: 			if (initial_filename_p.empty()) {
232: 				throw FormatException("column \"%d\" in parquet file is of type %s, could not auto cast to "
233: 				                      "expected type %s for this column",
234: 				                      col_idx, type_pair.second, expected_types_p[col_idx].ToString());
235: 			} else {
236: 				throw FormatException("schema mismatch in Parquet glob: column \"%d\" in parquet file is of type "
237: 				                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
238: 				                      col_idx, type_pair.second, initial_filename_p,
239: 				                      expected_types_p[col_idx].ToString());
240: 			}
241: 		} else {
242: 			names.push_back(type_pair.first);
243: 			return_types.push_back(type_pair.second);
244: 		}
245: 		col_idx++;
246: 	}
247: 	D_ASSERT(!names.empty());
248: 	D_ASSERT(!return_types.empty());
249: }
250: 
251: ParquetReader::ParquetReader(unique_ptr<FileHandle> file_handle_p, const vector<LogicalType> &expected_types_p,
252:                              const string &initial_filename_p) {
253: 	file_name = file_handle_p->path;
254: 	file_handle = move(file_handle_p);
255: 	metadata = LoadMetadata(*file_handle);
256: 	InitializeSchema(expected_types_p, initial_filename_p);
257: }
258: 
259: ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<LogicalType> &expected_types_p,
260:                              const string &initial_filename_p) {
261: 	auto &fs = FileSystem::GetFileSystem(context_p);
262: 	file_name = move(file_name_p);
263: 	file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
264: 	// If object cached is disabled
265: 	// or if this file has cached metadata
266: 	// or if the cached version already expired
267: 
268: 	auto last_modify_time = fs.GetLastModifiedTime(*file_handle);
269: 	if (!ObjectCache::ObjectCacheEnabled(context_p)) {
270: 		metadata = LoadMetadata(*file_handle);
271: 	} else {
272: 		metadata =
273: 		    std::dynamic_pointer_cast<ParquetFileMetadataCache>(ObjectCache::GetObjectCache(context_p).Get(file_name));
274: 		if (!metadata || (last_modify_time + 10 >= metadata->read_time)) {
275: 			metadata = LoadMetadata(*file_handle);
276: 			ObjectCache::GetObjectCache(context_p).Put(file_name, metadata);
277: 		}
278: 	}
279: 
280: 	InitializeSchema(expected_types_p, initial_filename_p);
281: }
282: 
283: ParquetReader::~ParquetReader() {
284: }
285: 
286: const FileMetaData *ParquetReader::GetFileMetadata() {
287: 	D_ASSERT(metadata);
288: 	D_ASSERT(metadata->metadata);
289: 	return metadata->metadata.get();
290: }
291: 
292: // TODO also somewhat ugly, perhaps this can be moved to the column reader too
293: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(LogicalType &type, column_t file_col_idx,
294:                                                          const FileMetaData *file_meta_data) {
295: 	unique_ptr<BaseStatistics> column_stats;
296: 	auto root_reader = CreateReader(file_meta_data);
297: 	auto column_reader = ((StructColumnReader *)root_reader.get())->GetChildReader(file_col_idx);
298: 
299: 	for (auto &row_group : file_meta_data->row_groups) {
300: 		auto chunk_stats = column_reader->Stats(row_group.columns);
301: 		if (!chunk_stats) {
302: 			return nullptr;
303: 		}
304: 		if (!column_stats) {
305: 			column_stats = move(chunk_stats);
306: 		} else {
307: 			column_stats->Merge(*chunk_stats);
308: 		}
309: 	}
310: 	return column_stats;
311: }
312: 
313: const RowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
314: 	auto file_meta_data = GetFileMetadata();
315: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
316: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
317: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
318: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
319: }
320: 
321: void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx) {
322: 	auto &group = GetGroup(state);
323: 
324: 	auto column_reader = ((StructColumnReader *)state.root_reader.get())->GetChildReader(state.column_ids[out_col_idx]);
325: 
326: 	// TODO move this to columnreader too
327: 	if (state.filters) {
328: 		auto stats = column_reader->Stats(group.columns);
329: 		// filters contain output chunk index, not file col idx!
330: 		auto filter_entry = state.filters->filters.find(out_col_idx);
331: 		if (stats && filter_entry != state.filters->filters.end()) {
332: 			bool skip_chunk = false;
333: 			switch (column_reader->Type().id()) {
334: 			case LogicalTypeId::UTINYINT:
335: 			case LogicalTypeId::USMALLINT:
336: 			case LogicalTypeId::UINTEGER:
337: 			case LogicalTypeId::UBIGINT:
338: 			case LogicalTypeId::INTEGER:
339: 			case LogicalTypeId::BIGINT:
340: 			case LogicalTypeId::FLOAT:
341: 			case LogicalTypeId::TIMESTAMP:
342: 			case LogicalTypeId::DOUBLE: {
343: 				auto &num_stats = (NumericStatistics &)*stats;
344: 				for (auto &filter : filter_entry->second) {
345: 					skip_chunk = !num_stats.CheckZonemap(filter.comparison_type, filter.constant);
346: 					if (skip_chunk) {
347: 						break;
348: 					}
349: 				}
350: 				break;
351: 			}
352: 			case LogicalTypeId::BLOB:
353: 			case LogicalTypeId::VARCHAR: {
354: 				auto &str_stats = (StringStatistics &)*stats;
355: 				for (auto &filter : filter_entry->second) {
356: 					skip_chunk = !str_stats.CheckZonemap(filter.comparison_type, filter.constant.str_value);
357: 					if (skip_chunk) {
358: 						break;
359: 					}
360: 				}
361: 				break;
362: 			}
363: 			default:
364: 				break;
365: 			}
366: 			if (skip_chunk) {
367: 				state.group_offset = group.num_rows;
368: 				return;
369: 				// this effectively will skip this chunk
370: 			}
371: 		}
372: 	}
373: 
374: 	state.root_reader->IntializeRead(group.columns, *state.thrift_file_proto);
375: }
376: 
377: idx_t ParquetReader::NumRows() {
378: 	return GetFileMetadata()->num_rows;
379: }
380: 
381: idx_t ParquetReader::NumRowGroups() {
382: 	return GetFileMetadata()->row_groups.size();
383: }
384: 
385: void ParquetReader::InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids,
386:                                    vector<idx_t> groups_to_read, TableFilterSet *filters) {
387: 	state.current_group = -1;
388: 	state.finished = false;
389: 	state.column_ids = move(column_ids);
390: 	state.group_offset = 0;
391: 	state.group_idx_list = move(groups_to_read);
392: 	state.filters = filters;
393: 	state.sel.Initialize(STANDARD_VECTOR_SIZE);
394: 	state.file_handle = file_handle->file_system.OpenFile(file_handle->path, FileFlags::FILE_FLAGS_READ);
395: 	state.thrift_file_proto = CreateThriftProtocol(*state.file_handle);
396: 	state.root_reader = CreateReader(GetFileMetadata());
397: 
398: 	state.define_buf.resize(STANDARD_VECTOR_SIZE);
399: 	state.repeat_buf.resize(STANDARD_VECTOR_SIZE);
400: }
401: 
402: template <class T, class OP>
403: void TemplatedFilterOperation(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {
404: 	D_ASSERT(v.GetVectorType() == VectorType::FLAT_VECTOR); // we just created the damn thing it better be
405: 
406: 	auto v_ptr = FlatVector::GetData<T>(v);
407: 	auto &mask = FlatVector::Validity(v);
408: 
409: 	if (!mask.AllValid()) {
410: 		for (idx_t i = 0; i < count; i++) {
411: 			filter_mask[i] = filter_mask[i] && mask.RowIsValid(i) && OP::Operation(v_ptr[i], constant);
412: 		}
413: 	} else {
414: 		for (idx_t i = 0; i < count; i++) {
415: 			filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
416: 		}
417: 	}
418: }
419: 
420: template <class OP>
421: static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {
422: 	if (filter_mask.none() || count == 0) {
423: 		return;
424: 	}
425: 	switch (v.GetType().id()) {
426: 	case LogicalTypeId::BOOLEAN:
427: 		TemplatedFilterOperation<bool, OP>(v, constant.value_.boolean, filter_mask, count);
428: 		break;
429: 
430: 	case LogicalTypeId::UTINYINT:
431: 		TemplatedFilterOperation<uint8_t, OP>(v, constant.value_.utinyint, filter_mask, count);
432: 		break;
433: 
434: 	case LogicalTypeId::USMALLINT:
435: 		TemplatedFilterOperation<uint16_t, OP>(v, constant.value_.usmallint, filter_mask, count);
436: 		break;
437: 
438: 	case LogicalTypeId::UINTEGER:
439: 		TemplatedFilterOperation<uint32_t, OP>(v, constant.value_.uinteger, filter_mask, count);
440: 		break;
441: 
442: 	case LogicalTypeId::UBIGINT:
443: 		TemplatedFilterOperation<uint64_t, OP>(v, constant.value_.ubigint, filter_mask, count);
444: 		break;
445: 
446: 	case LogicalTypeId::INTEGER:
447: 		TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
448: 		break;
449: 
450: 	case LogicalTypeId::BIGINT:
451: 		TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
452: 		break;
453: 
454: 	case LogicalTypeId::FLOAT:
455: 		TemplatedFilterOperation<float, OP>(v, constant.value_.float_, filter_mask, count);
456: 		break;
457: 
458: 	case LogicalTypeId::DOUBLE:
459: 		TemplatedFilterOperation<double, OP>(v, constant.value_.double_, filter_mask, count);
460: 		break;
461: 
462: 	case LogicalTypeId::TIMESTAMP:
463: 		TemplatedFilterOperation<timestamp_t, OP>(v, constant.value_.bigint, filter_mask, count);
464: 		break;
465: 
466: 	case LogicalTypeId::BLOB:
467: 	case LogicalTypeId::VARCHAR:
468: 		TemplatedFilterOperation<string_t, OP>(v, string_t(constant.str_value), filter_mask, count);
469: 		break;
470: 
471: 	default:
472: 		throw NotImplementedException("Unsupported type for filter %s", v.ToString());
473: 	}
474: }
475: 
476: void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {
477: 	while (ScanInternal(state, result)) {
478: 		if (result.size() > 0) {
479: 			break;
480: 		}
481: 		result.Reset();
482: 	}
483: }
484: 
485: bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {
486: 	if (state.finished) {
487: 		return false;
488: 	}
489: 
490: 	// see if we have to switch to the next row group in the parquet file
491: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
492: 		state.current_group++;
493: 		state.group_offset = 0;
494: 
495: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
496: 			state.finished = true;
497: 			return false;
498: 		}
499: 
500: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
501: 			// this is a special case where we are not interested in the actual contents of the file
502: 			if (state.column_ids[out_col_idx] == COLUMN_IDENTIFIER_ROW_ID) {
503: 				continue;
504: 			}
505: 
506: 			PrepareRowGroupBuffer(state, out_col_idx);
507: 		}
508: 		return true;
509: 	}
510: 
511: 	auto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);
512: 	result.SetCardinality(this_output_chunk_rows);
513: 
514: 	if (this_output_chunk_rows == 0) {
515: 		state.finished = true;
516: 		return false; // end of last group, we are done
517: 	}
518: 
519: 	// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to
520: 	// be relevant
521: 	parquet_filter_t filter_mask;
522: 	filter_mask.set();
523: 
524: 	state.define_buf.zero();
525: 	state.repeat_buf.zero();
526: 
527: 	auto define_ptr = (uint8_t *)state.define_buf.ptr;
528: 	auto repeat_ptr = (uint8_t *)state.repeat_buf.ptr;
529: 
530: 	auto root_reader = ((StructColumnReader *)state.root_reader.get());
531: 
532: 	if (state.filters) {
533: 		vector<bool> need_to_read(result.ColumnCount(), true);
534: 
535: 		// first load the columns that are used in filters
536: 		for (auto &filter_col : state.filters->filters) {
537: 			auto file_col_idx = state.column_ids[filter_col.first];
538: 
539: 			if (filter_mask.none()) { // if no rows are left we can stop checking filters
540: 				break;
541: 			}
542: 
543: 			root_reader->GetChildReader(file_col_idx)
544: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[filter_col.first]);
545: 
546: 			need_to_read[filter_col.first] = false;
547: 
548: 			for (auto &filter : filter_col.second) {
549: 				switch (filter.comparison_type) {
550: 				case ExpressionType::COMPARE_EQUAL:
551: 					FilterOperationSwitch<Equals>(result.data[filter_col.first], filter.constant, filter_mask,
552: 					                              this_output_chunk_rows);
553: 					break;
554: 				case ExpressionType::COMPARE_LESSTHAN:
555: 					FilterOperationSwitch<LessThan>(result.data[filter_col.first], filter.constant, filter_mask,
556: 					                                this_output_chunk_rows);
557: 					break;
558: 				case ExpressionType::COMPARE_LESSTHANOREQUALTO:
559: 					FilterOperationSwitch<LessThanEquals>(result.data[filter_col.first], filter.constant, filter_mask,
560: 					                                      this_output_chunk_rows);
561: 					break;
562: 				case ExpressionType::COMPARE_GREATERTHAN:
563: 					FilterOperationSwitch<GreaterThan>(result.data[filter_col.first], filter.constant, filter_mask,
564: 					                                   this_output_chunk_rows);
565: 					break;
566: 				case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
567: 					FilterOperationSwitch<GreaterThanEquals>(result.data[filter_col.first], filter.constant,
568: 					                                         filter_mask, this_output_chunk_rows);
569: 					break;
570: 				default:
571: 					D_ASSERT(0);
572: 				}
573: 			}
574: 		}
575: 
576: 		// we still may have to read some cols
577: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
578: 			if (!need_to_read[out_col_idx]) {
579: 				continue;
580: 			}
581: 			auto file_col_idx = state.column_ids[out_col_idx];
582: 
583: 			if (filter_mask.none()) {
584: 				root_reader->GetChildReader(file_col_idx)->Skip(result.size());
585: 				continue;
586: 			}
587: 			// TODO handle ROWID here, too
588: 			root_reader->GetChildReader(file_col_idx)
589: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
590: 		}
591: 
592: 		idx_t sel_size = 0;
593: 		for (idx_t i = 0; i < this_output_chunk_rows; i++) {
594: 			if (filter_mask[i]) {
595: 				state.sel.set_index(sel_size++, i);
596: 			}
597: 		}
598: 
599: 		result.Slice(state.sel, sel_size);
600: 		result.Verify();
601: 
602: 	} else { // #nofilter, just fricking load the data
603: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
604: 			auto file_col_idx = state.column_ids[out_col_idx];
605: 
606: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
607: 				Value constant_42 = Value::BIGINT(42);
608: 				result.data[out_col_idx].Reference(constant_42);
609: 				continue;
610: 			}
611: 
612: 			root_reader->GetChildReader(file_col_idx)
613: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
614: 		}
615: 	}
616: 
617: 	state.group_offset += this_output_chunk_rows;
618: 	return true;
619: }
620: 
621: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: