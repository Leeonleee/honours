{
  "repo": "duckdb/duckdb",
  "pull_number": 7197,
  "instance_id": "duckdb__duckdb-7197",
  "issue_numbers": [
    "6532"
  ],
  "base_commit": "c5737e4a948a72b20c39cee054a65a524542d832",
  "patch": "diff --git a/tools/pythonpkg/duckdb-stubs/__init__.pyi b/tools/pythonpkg/duckdb-stubs/__init__.pyi\nindex e7226a5e4d54..2b9140cd28a3 100644\n--- a/tools/pythonpkg/duckdb-stubs/__init__.pyi\n+++ b/tools/pythonpkg/duckdb-stubs/__init__.pyi\n@@ -225,7 +225,7 @@ class DuckDBPyRelation:\n     def kurt(self, aggregation_columns: str, group_columns: str = ...) -> DuckDBPyRelation: ...\n     def limit(self, n: int, offset: int = ...) -> DuckDBPyRelation: ...\n     def mad(self, aggregation_columns: str, group_columns: str = ...) -> DuckDBPyRelation: ...\n-    def map(self, map_function: function) -> DuckDBPyRelation: ...\n+    def map(self, map_function: function, schema: Optional[Dict[str, DuckDBPyType]]) -> DuckDBPyRelation: ...\n     def max(self, max_aggr: str, group_expr: str = ...) -> DuckDBPyRelation: ...\n     def mean(self, mean_aggr: str, group_expr: str = ...) -> DuckDBPyRelation: ...\n     def median(self, median_aggr: str, group_expr: str = ...) -> DuckDBPyRelation: ...\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyrelation.hpp b/tools/pythonpkg/src/include/duckdb_python/pyrelation.hpp\nindex b967773cc2f6..fc942e2c1128 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyrelation.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyrelation.hpp\n@@ -25,6 +25,8 @@ struct DuckDBPyConnection;\n \n class PythonDependencies : public ExternalDependency {\n public:\n+\texplicit PythonDependencies() : ExternalDependency(ExternalDependenciesType::PYTHON_DEPENDENCY) {\n+\t}\n \texplicit PythonDependencies(py::function map_function)\n \t    : ExternalDependency(ExternalDependenciesType::PYTHON_DEPENDENCY), map_function(std::move(map_function)) {};\n \texplicit PythonDependencies(unique_ptr<RegisteredObject> py_object)\n@@ -156,7 +158,7 @@ struct DuckDBPyRelation {\n \n \tunique_ptr<DuckDBPyRelation> Intersect(DuckDBPyRelation *other);\n \n-\tunique_ptr<DuckDBPyRelation> Map(py::function fun);\n+\tunique_ptr<DuckDBPyRelation> Map(py::function fun, Optional<py::object> schema);\n \n \tunique_ptr<DuckDBPyRelation> Join(DuckDBPyRelation *other, const string &condition, const string &type);\n \ndiff --git a/tools/pythonpkg/src/map.cpp b/tools/pythonpkg/src/map.cpp\nindex 33ec7cc9e452..0aebac4cc138 100644\n--- a/tools/pythonpkg/src/map.cpp\n+++ b/tools/pythonpkg/src/map.cpp\n@@ -6,11 +6,14 @@\n #include \"duckdb_python/pandas/column/pandas_numpy_column.hpp\"\n #include \"duckdb_python/pandas/pandas_scan.hpp\"\n #include \"duckdb_python/pybind11/dataframe.hpp\"\n+#include \"duckdb_python/pytype.hpp\"\n+#include \"duckdb_python/pybind11/dataframe.hpp\"\n \n namespace duckdb {\n \n MapFunction::MapFunction()\n-    : TableFunction(\"python_map_function\", {LogicalType::TABLE, LogicalType::POINTER}, nullptr, MapFunctionBind) {\n+    : TableFunction(\"python_map_function\", {LogicalType::TABLE, LogicalType::POINTER, LogicalType::POINTER}, nullptr,\n+                    MapFunctionBind) {\n \tin_out_function = MapFunctionExec;\n }\n \n@@ -42,6 +45,11 @@ static py::handle FunctionCall(NumpyResultConversion &conversion, vector<string>\n \t\tthrow InvalidInputException(\"No return value from Python function\");\n \t}\n \n+\tif (!py::isinstance<PandasDataFrame>(df)) {\n+\t\tthrow InvalidInputException(\n+\t\t    \"Expected the UDF to return an object of type 'pandas.DataFrame', found '%s' instead\",\n+\t\t    std::string(py::str(df.attr(\"__class__\"))));\n+\t}\n \tif (PandasDataFrame::IsPyArrowBacked(df)) {\n \t\tthrow InvalidInputException(\n \t\t    \"Produced DataFrame has columns that are backed by PyArrow, which is not supported yet in 'map'\");\n@@ -86,6 +94,35 @@ static void OverrideNullType(vector<LogicalType> &return_types, const vector<str\n \t}\n }\n \n+unique_ptr<FunctionData> BindExplicitSchema(unique_ptr<MapFunctionData> function_data, PyObject *schema_p,\n+                                            vector<LogicalType> &types, vector<string> &names) {\n+\tD_ASSERT(schema_p != Py_None);\n+\n+\tauto schema_object = py::reinterpret_borrow<py::dict>(schema_p);\n+\tif (!py::isinstance<py::dict>(schema_object)) {\n+\t\tthrow InvalidInputException(\"'schema' should be given as a Dict[str, DuckDBType]\");\n+\t}\n+\tauto schema = py::dict(schema_object);\n+\n+\tauto column_count = schema.size();\n+\n+\ttypes.reserve(column_count);\n+\tnames.reserve(column_count);\n+\tfor (auto &item : schema) {\n+\t\tauto name = item.first;\n+\t\tauto type_p = item.second;\n+\t\tnames.push_back(std::string(py::str(name)));\n+\t\t// TODO: replace with py::try_cast so we can catch the error and throw a better exception\n+\t\tauto type = py::cast<shared_ptr<DuckDBPyType>>(type_p);\n+\t\ttypes.push_back(type->Type());\n+\t}\n+\n+\tfunction_data->out_names = names;\n+\tfunction_data->out_types = types;\n+\n+\treturn std::move(function_data);\n+}\n+\n // we call the passed function with a zero-row data frame to infer the output columns and their names.\n // they better not change in the actual execution ^^\n unique_ptr<FunctionData> MapFunction::MapFunctionBind(ClientContext &context, TableFunctionBindInput &input,\n@@ -95,9 +132,15 @@ unique_ptr<FunctionData> MapFunction::MapFunctionBind(ClientContext &context, Ta\n \tauto data_uptr = make_uniq<MapFunctionData>();\n \tauto &data = *data_uptr;\n \tdata.function = (PyObject *)input.inputs[0].GetPointer();\n+\tauto explicit_schema = (PyObject *)input.inputs[1].GetPointer();\n+\n \tdata.in_names = input.input_table_names;\n \tdata.in_types = input.input_table_types;\n \n+\tif (explicit_schema != Py_None) {\n+\t\treturn BindExplicitSchema(std::move(data_uptr), explicit_schema, return_types, names);\n+\t}\n+\n \tNumpyResultConversion conversion(data.in_types, 0);\n \tauto df = FunctionCall(conversion, data.in_names, data.function);\n \tvector<PandasColumnBindData> pandas_bind_data; // unused\ndiff --git a/tools/pythonpkg/src/pyrelation.cpp b/tools/pythonpkg/src/pyrelation.cpp\nindex d2523946e673..e10a75300a15 100644\n--- a/tools/pythonpkg/src/pyrelation.cpp\n+++ b/tools/pythonpkg/src/pyrelation.cpp\n@@ -844,12 +844,16 @@ void DuckDBPyRelation::Create(const string &table) {\n \tPyExecuteRelation(create);\n }\n \n-unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Map(py::function fun) {\n+unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Map(py::function fun, Optional<py::object> schema) {\n \tAssertRelation();\n \tvector<Value> params;\n \tparams.emplace_back(Value::POINTER((uintptr_t)fun.ptr()));\n+\tparams.emplace_back(Value::POINTER((uintptr_t)schema.ptr()));\n \tauto relation = make_uniq<DuckDBPyRelation>(rel->TableFunction(\"python_map_function\", params));\n-\trelation->rel->extra_dependencies = make_uniq<PythonDependencies>(fun);\n+\tauto rel_dependency = make_uniq<PythonDependencies>();\n+\trel_dependency->map_function = std::move(fun);\n+\trel_dependency->py_object_list.push_back(std::move(make_uniq<RegisteredObject>(std::move(schema))));\n+\trelation->rel->extra_dependencies = std::move(rel_dependency);\n \treturn relation;\n }\n \ndiff --git a/tools/pythonpkg/src/pyrelation/initialize.cpp b/tools/pythonpkg/src/pyrelation/initialize.cpp\nindex cdf7d35687d5..415d211bf1d0 100644\n--- a/tools/pythonpkg/src/pyrelation/initialize.cpp\n+++ b/tools/pythonpkg/src/pyrelation/initialize.cpp\n@@ -210,7 +210,8 @@ void DuckDBPyRelation::Initialize(py::handle &m) {\n \t             py::arg(\"replace\") = true);\n \n \trelation_module\n-\t    .def(\"map\", &DuckDBPyRelation::Map, py::arg(\"map_function\"), \"Calls the passed function on the relation\")\n+\t    .def(\"map\", &DuckDBPyRelation::Map, py::arg(\"map_function\"), py::kw_only(), py::arg(\"schema\") = py::none(),\n+\t         \"Calls the passed function on the relation\")\n \t    .def(\"show\", &DuckDBPyRelation::Print, \"Display a summary of the data\")\n \t    .def(\"__str__\", &DuckDBPyRelation::ToString)\n \t    .def(\"__repr__\", &DuckDBPyRelation::ToString);\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/test_map.py b/tools/pythonpkg/tests/fast/test_map.py\nindex b75db20aed1c..edeece2a61af 100644\n--- a/tools/pythonpkg/tests/fast/test_map.py\n+++ b/tools/pythonpkg/tests/fast/test_map.py\n@@ -2,6 +2,7 @@\n import numpy\n import pytest\n from datetime import date, timedelta\n+import re\n from conftest import NumpyPandas, ArrowPandas\n \n class TestMap(object):\n@@ -64,7 +65,7 @@ def return_empty_df(df):\n         with pytest.raises(duckdb.InvalidInputException, match='UDF column name mismatch'):\n             print(testrel.map(evil3).df())\n \n-        with pytest.raises(AttributeError):\n+        with pytest.raises(duckdb.InvalidInputException, match=\"Expected the UDF to return an object of type 'pandas.DataFrame'\"):\n             print(testrel.map(evil4).df())\n \n         with pytest.raises(duckdb.InvalidInputException):\n@@ -123,7 +124,71 @@ def mapper(x):\n         x = rel.fetchdf()\n         assert x['days_to_add'].to_numpy()[0] == 1\n \n-    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_explicit_schema(self):\n+        def cast_to_string(df):\n+            df['i'] = df['i'].astype(str)\n+            return df\n+\n+        con = duckdb.connect()\n+        rel = con.sql('select i from range (10) tbl(i)')\n+        assert rel.types[0] == int\n+        mapped_rel = rel.map(cast_to_string, schema={'i': str})\n+        assert mapped_rel.types[0] == str\n+\n+    def test_explicit_schema_returntype_mismatch(self):\n+        def does_nothing(df):\n+            return df\n+\n+        con = duckdb.connect()\n+        rel = con.sql('select i from range(10) tbl(i)')\n+        # expects the mapper to return a string column\n+        rel = rel.map(does_nothing, schema={'i': str})\n+        with pytest.raises(duckdb.InvalidInputException, match=re.escape(\"UDF column type mismatch, expected [VARCHAR], got [BIGINT]\")):\n+            rel.fetchall()\n+\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_explicit_schema_name_mismatch(self, pandas):\n+        def renames_column(df):\n+            return pandas.DataFrame({'a': df['i']})\n+\n+        con = duckdb.connect()\n+        rel = con.sql('select i from range(10) tbl(i)')\n+        rel = rel.map(renames_column, schema={'i': int})\n+        with pytest.raises(duckdb.InvalidInputException, match=re.escape('UDF column name mismatch')):\n+            rel.fetchall()\n+\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_explicit_schema_error(self, pandas):\n+        def no_op(df):\n+            return df\n+\n+        con = duckdb.connect()\n+        rel = con.sql('select 42')\n+        with pytest.raises(duckdb.InvalidInputException, match=re.escape(\"Invalid Input Error: 'schema' should be given as a Dict[str, DuckDBType]\")):\n+            rel.map(no_op, schema=[int])\n+\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_returns_non_dataframe(self, pandas):\n+        def returns_series(df):\n+            return df.loc[:,'i']\n+\n+        con = duckdb.connect()\n+        rel = con.sql('select i, i as j from range(10) tbl(i)')\n+        with pytest.raises(duckdb.InvalidInputException, match=re.escape(\"Expected the UDF to return an object of type 'pandas.DataFrame', found '<class 'pandas.core.series.Series'>' instead\")):\n+            rel = rel.map(returns_series)\n+\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_explicit_schema_columncount_mismatch(self, pandas):\n+        def returns_subset(df):\n+            return pandas.DataFrame({'i': df.loc[:,'i']})\n+\n+        con = duckdb.connect()\n+        rel = con.sql('select i, i as j from range(10) tbl(i)')\n+        rel = rel.map(returns_subset, schema={'i': int, 'j': int})\n+        with pytest.raises(duckdb.InvalidInputException, match='Invalid Input Error: Expected 2 columns from UDF, got 1'):\n+            rel.fetchall()\n+\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n     def test_pyarrow_df(self, pandas):\n         # PyArrow backed dataframes only exist on pandas >= 2.0.0\n         _ = pytest.importorskip(\"pandas\", \"2.0.0\")\n",
  "problem_statement": "UDF column type mismatch, expected [NULL], got [BLOB]\n### What happens?\n\nWhen trying to apply a `map` to a BLOB column an error is returned:\r\n```\r\nInvalidInputException: Invalid Input Error: UDF column type mismatch, expected [NULL], got [BLOB]\r\n```\n\n### To Reproduce\n\n```\r\nimport duckdb\r\n\r\ndef nop(df):\r\n    return df\r\n\r\nduckdb.values([b'1234']).map(nop)\r\n\r\n---------------------------------------------------------------------------\r\nInvalidInputException                     Traceback (most recent call last)\r\nFile /data/data/Code/orc/python/venv/lib/python3.8/site-packages/IPython/core/formatters.py:706, in PlainTextFormatter.__call__(self, obj)\r\n    699 stream = StringIO()\r\n    700 printer = pretty.RepresentationPrinter(stream, self.verbose,\r\n    701     self.max_width, self.newline,\r\n    702     max_seq_length=self.max_seq_length,\r\n    703     singleton_pprinters=self.singleton_printers,\r\n    704     type_pprinters=self.type_printers,\r\n    705     deferred_pprinters=self.deferred_printers)\r\n--> 706 printer.pretty(obj)\r\n    707 printer.flush()\r\n    708 return stream.getvalue()\r\n\r\nFile /data/data/Code/orc/python/venv/lib/python3.8/site-packages/IPython/lib/pretty.py:410, in RepresentationPrinter.pretty(self, obj)\r\n    407                         return meth(obj, self, cycle)\r\n    408                 if cls is not object \\\r\n    409                         and callable(cls.__dict__.get('__repr__')):\r\n--> 410                     return _repr_pprint(obj, self, cycle)\r\n    412     return _default_pprint(obj, self, cycle)\r\n    413 finally:\r\n\r\nFile /data/data/Code/orc/python/venv/lib/python3.8/site-packages/IPython/lib/pretty.py:778, in _repr_pprint(obj, p, cycle)\r\n    776 \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\r\n    777 # Find newlines and replace them with p.break_()\r\n...\r\n--> 778 output = repr(obj)\r\n    779 lines = output.splitlines()\r\n    780 with p.group():\r\n\r\nInvalidInputException: Invalid Input Error: UDF column type mismatch, expected [NULL], got [BLOB]\r\n```\n\n### OS:\n\nlinux\n\n### DuckDB Version:\n\n0.7.1\n\n### DuckDB Client:\n\npython\n\n### Full Name:\n\nHinko Kocevar\n\n### Affiliation:\n\nESS ERIC\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "I don't think that's caused by the BLOB column, the current implementation of `map` will call the UDF first with an empty dataframe during bind time.\r\nWe use the return value of this to figure out the schema of the result of the function.\r\n\r\nAfter altering the udf to print the dtypes of the dataframe, I think I know what's going on:\r\n```py\r\ndef nop(df):\r\n\tprint(df.dtypes)\r\n\treturn df\r\n\r\nduckdb.values([b'1234']).map(nop).fetchall()\r\n```\r\n```\r\ncol0    object\r\ndtype: object\r\ncol0    object\r\ndtype: object\r\ncol0    object\r\ndtype: object\r\n```\r\n\r\nHmm actually, it is a problem on our side, but I don't see a way to fix this.\r\nA little insight:\r\nWhen we create a numpy array, it goes through a switch to figure out the type:\r\n```c++\r\n\tcase LogicalTypeId::TIME:\r\n\tcase LogicalTypeId::TIME_TZ:\r\n\tcase LogicalTypeId::VARCHAR:\r\n\tcase LogicalTypeId::BIT:\r\n\tcase LogicalTypeId::BLOB:\r\n\tcase LogicalTypeId::ENUM:\r\n\tcase LogicalTypeId::LIST:\r\n\tcase LogicalTypeId::MAP:\r\n\tcase LogicalTypeId::STRUCT:\r\n\tcase LogicalTypeId::UUID:\r\n\t\ttype_width = sizeof(PyObject *);\r\n\t\tbreak;\r\n```\r\nAs you can see BLOB maps to PyObject* (becomes an 'object' column in pandas)\r\nWhen we receive a pandas object column, we have to figure out what DuckDB type it maps to, this is done by sampling the column.\r\nAt first the type starts off as NULL, and for every non-null value we find, we upgrade the type\r\nBut since we call the UDF the first time with an empty dataframe (to essentially ask you what the return type is), and you return that dataframe with an object column, the analyzer has nothing to go off of, so it returns NULL as the type.\r\n\nActually, since we know the original types, we can check if the original type was a type that maps to 'object', and also check if the analyzed type is NULL - and then override it to the original type\r\n\r\nThat might work\nThanks guys! The PR solves the issue for me!\nI might have spoken too soon..\r\n\r\nAfter testing my original use case (I posted the minimal example that failed at the start of this issue) of converting the data type in the `map()` function I get this problem:\r\n\r\n```\r\n>>> duckdb.values([msgpack.packb(10000)])\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502     col0     \u2502\r\n\u2502     blob     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 \\xCD\\x27\\x10 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n>>> def db_conv(v):\r\n...  def g(vv):\r\n...   return msgpack.unpackb(vv)\r\n...  v['col0'] = v['col0'].apply(g)\r\n...  return v\r\n\r\n>>> duckdb.values([msgpack.packb(10000)]).map(db_conv)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nduckdb.InvalidInputException: Invalid Input Error: UDF column type mismatch, expected [BLOB], got [BIGINT]\r\n\r\n```\r\n\r\n\r\nFor example, same happens if I want to map an `integer` to `float`  (not what I'm looking for):\r\n\r\n```\r\n>>> def db_conv2(v):\r\n...  def g(vv):\r\n...   return float(vv)\r\n...  v['col0'] = v['col0'].apply(g)\r\n...  return v\r\n\r\n>>> duckdb.values([10000])\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 col0  \u2502\r\n\u2502 int16 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 10000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n>>> duckdb.values([10000]).map(db_conv2)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nduckdb.InvalidInputException: Invalid Input Error: UDF column type mismatch, expected [SMALLINT], got [DOUBLE]\r\n\r\n```\r\n\r\nbut, then this conversion works fine (not what I'm looking for):\r\n\r\n```\r\n\r\n>>> def db_conv3(v):\r\n...  v['col0'] = v['col0'].astype(float)\r\n...  return v\r\n... \r\n>>> duckdb.values([10000]).map(db_conv3)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  col0   \u2502\r\n\u2502 double  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 10000.0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\n```\r\n\r\nI guess it has to do with the way that the output type is inferred, but I'm not familiar with the internals. Would it be possible for `map()` to specify the output format explicitly? In the original use case with the `msgpack.unpackb()` the resulting data type is known only after the blob is unpacked.",
  "created_at": "2023-04-23T13:29:31Z"
}