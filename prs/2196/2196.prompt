You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
EXPORT database using 'codec zstd' writes a broken load.sql file
**What does happen?**

I create a database 'main' with a single table 'data' loaded from a parquet file.  I store the database to disk:

EXPORT database 'datastore' (format parquet, codec zstd);

This successfully writes out the the parquet file and two sql files to the 'datastore' subfolder.  Here are the contents of the load.sql file:

load.sql:
COPY "data" FROM 'datastore\0_data.parquet' (FORMAT 'parquet', codec 'zstd');

Now I exit duckdb and start a new session.  I attempt to import the saved database:

IMPORT DATABASE 'datastore';
Error: Not implemented Error: Unsupported option for COPY FROM parquet: codec

The 'data' table is created but empty, due to the error.

I edit the load.sql to remove the Codec part:
COPY "data" FROM 'datastore\0_data.parquet' (FORMAT 'parquet');

I drop the empty 'data' table then try again importing the database:

IMPORT DATABASE 'datastore';
OK

Everything's fine.  It creates the 'data' table and populates it.


**What should happen?**
Apparently for the IMPORT function, when it comes to loading parquet databases, duckdb is smart enough to know what kind of compression is in use and does not like it when you specify the Codec option.  Therefore, I suppose when creating the load.sql file it should not add the Codec option??

**To Reproduce**
Steps to reproduce the behavior. Bonus points if those are only SQL queries.
1. EXPORT your in-memory database in parquet format using zstd compression, as described above
2. restart your duckdb session
3. run: IMPORT DATABASE 'your_database';
4. see if you get the same error: "Error: Not implemented Error: Unsupported option for COPY FROM parquet: codec". If so...
5. restart your duckdb session
6. edit your database load.sql file as described above
7. run: IMPORT DATABASE 'your_database';
8. see if your database loaded fine this with the modified load.sql file.

**Environment (please complete the following information):**
 - OS: Windows 10 1909
 - DuckDB Version: CLI 0.2.7



</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![codecov](https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN)](https://codecov.io/gh/duckdb/duckdb)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of extension/parquet/parquet-extension.cpp]
1: #include <string>
2: #include <vector>
3: #include <fstream>
4: #include <iostream>
5: 
6: #include "parquet-extension.hpp"
7: #include "parquet_reader.hpp"
8: #include "parquet_writer.hpp"
9: #include "parquet_metadata.hpp"
10: 
11: #include "duckdb.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb.hpp"
14: #include "duckdb/common/types/chunk_collection.hpp"
15: #include "duckdb/function/copy_function.hpp"
16: #include "duckdb/function/table_function.hpp"
17: #include "duckdb/common/file_system.hpp"
18: #include "duckdb/parallel/parallel_state.hpp"
19: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
20: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
21: 
22: #include "duckdb/main/config.hpp"
23: #include "duckdb/parser/expression/constant_expression.hpp"
24: #include "duckdb/parser/expression/function_expression.hpp"
25: #include "duckdb/parser/tableref/table_function_ref.hpp"
26: 
27: #include "duckdb/storage/statistics/base_statistics.hpp"
28: 
29: #include "duckdb/main/client_context.hpp"
30: #include "duckdb/catalog/catalog.hpp"
31: #endif
32: 
33: namespace duckdb {
34: 
35: struct ParquetReadBindData : public FunctionData {
36: 	shared_ptr<ParquetReader> initial_reader;
37: 	vector<string> files;
38: 	vector<column_t> column_ids;
39: 	atomic<idx_t> chunk_count;
40: 	atomic<idx_t> cur_file;
41: };
42: 
43: struct ParquetReadOperatorData : public FunctionOperatorData {
44: 	shared_ptr<ParquetReader> reader;
45: 	ParquetReaderScanState scan_state;
46: 	bool is_parallel;
47: 	idx_t file_index;
48: 	vector<column_t> column_ids;
49: 	TableFilterSet *table_filters;
50: };
51: 
52: struct ParquetReadParallelState : public ParallelState {
53: 	mutex lock;
54: 	shared_ptr<ParquetReader> current_reader;
55: 	idx_t file_index;
56: 	idx_t row_group_index;
57: };
58: 
59: class ParquetScanFunction {
60: public:
61: 	static TableFunctionSet GetFunctionSet() {
62: 		TableFunctionSet set("parquet_scan");
63: 		set.AddFunction(TableFunction({LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind,
64: 		                              ParquetScanInit, /* statistics */ ParquetScanStats, /* cleanup */ nullptr,
65: 		                              /* dependency */ nullptr, ParquetCardinality,
66: 		                              /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr,
67: 		                              ParquetScanMaxThreads, ParquetInitParallelState, ParquetScanFuncParallel,
68: 		                              ParquetScanParallelInit, ParquetParallelStateNext, true, true, ParquetProgress));
69: 		set.AddFunction(TableFunction({LogicalType::LIST(LogicalType::VARCHAR)}, ParquetScanImplementation,
70: 		                              ParquetScanBindList, ParquetScanInit, /* statistics */ ParquetScanStats,
71: 		                              /* cleanup */ nullptr,
72: 		                              /* dependency */ nullptr, ParquetCardinality,
73: 		                              /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr,
74: 		                              ParquetScanMaxThreads, ParquetInitParallelState, ParquetScanFuncParallel,
75: 		                              ParquetScanParallelInit, ParquetParallelStateNext, true, true, ParquetProgress));
76: 		return set;
77: 	}
78: 
79: 	static unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,
80: 	                                                vector<string> &expected_names,
81: 	                                                vector<LogicalType> &expected_types) {
82: 		for (auto &option : info.options) {
83: 			throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
84: 		}
85: 		auto result = make_unique<ParquetReadBindData>();
86: 
87: 		FileSystem &fs = FileSystem::GetFileSystem(context);
88: 		result->files = fs.Glob(info.file_path);
89: 		if (result->files.empty()) {
90: 			throw IOException("No files found that match the pattern \"%s\"", info.file_path);
91: 		}
92: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types);
93: 		return move(result);
94: 	}
95: 
96: 	static unique_ptr<BaseStatistics> ParquetScanStats(ClientContext &context, const FunctionData *bind_data_p,
97: 	                                                   column_t column_index) {
98: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
99: 
100: 		if (column_index == COLUMN_IDENTIFIER_ROW_ID) {
101: 			return nullptr;
102: 		}
103: 
104: 		// we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
105: 
106: 		// We already parsed the metadata for the first file in a glob because we need some type info.
107: 		auto overall_stats = ParquetReader::ReadStatistics(
108: 		    *bind_data.initial_reader, bind_data.initial_reader->return_types[column_index], column_index,
109: 		    bind_data.initial_reader->metadata->metadata.get());
110: 
111: 		if (!overall_stats) {
112: 			return nullptr;
113: 		}
114: 
115: 		// if there is only one file in the glob (quite common case), we are done
116: 		auto &config = DBConfig::GetConfig(context);
117: 		if (bind_data.files.size() < 2) {
118: 			return overall_stats;
119: 		} else if (config.object_cache_enable) {
120: 			auto &cache = ObjectCache::GetObjectCache(context);
121: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
122: 			// enabled at all)
123: 			FileSystem &fs = FileSystem::GetFileSystem(context);
124: 			for (idx_t file_idx = 1; file_idx < bind_data.files.size(); file_idx++) {
125: 				auto &file_name = bind_data.files[file_idx];
126: 				auto metadata = std::dynamic_pointer_cast<ParquetFileMetadataCache>(cache.Get(file_name));
127: 				auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
128: 				// but we need to check if the metadata cache entries are current
129: 				if (!metadata || (fs.GetLastModifiedTime(*handle) >= metadata->read_time)) {
130: 					// missing or invalid metadata entry in cache, no usable stats overall
131: 					return nullptr;
132: 				}
133: 				// get and merge stats for file
134: 				auto file_stats = ParquetReader::ReadStatistics(*bind_data.initial_reader,
135: 				                                                bind_data.initial_reader->return_types[column_index],
136: 				                                                column_index, metadata->metadata.get());
137: 				if (!file_stats) {
138: 					return nullptr;
139: 				}
140: 				overall_stats->Merge(*file_stats);
141: 			}
142: 			// success!
143: 			return overall_stats;
144: 		}
145: 		// we have more than one file and no object cache so no statistics overall
146: 		return nullptr;
147: 	}
148: 
149: 	static void ParquetScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
150: 	                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
151: 	                                    ParallelState *parallel_state_p) {
152: 		//! FIXME: Have specialized parallel function from pandas scan here
153: 		ParquetScanImplementation(context, bind_data, operator_state, input, output);
154: 	}
155: 
156: 	static unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context, vector<string> files,
157: 	                                                        vector<LogicalType> &return_types, vector<string> &names) {
158: 		auto result = make_unique<ParquetReadBindData>();
159: 		result->files = move(files);
160: 
161: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0]);
162: 		return_types = result->initial_reader->return_types;
163: 
164: 		names = result->initial_reader->names;
165: 		return move(result);
166: 	}
167: 
168: 	static vector<string> ParquetGlob(FileSystem &fs, const string &glob) {
169: 		auto files = fs.Glob(glob);
170: 		if (files.empty()) {
171: 			throw IOException("No files found that match the pattern \"%s\"", glob);
172: 		}
173: 		return files;
174: 	}
175: 
176: 	static unique_ptr<FunctionData> ParquetScanBind(ClientContext &context, vector<Value> &inputs,
177: 	                                                unordered_map<string, Value> &named_parameters,
178: 	                                                vector<LogicalType> &input_table_types,
179: 	                                                vector<string> &input_table_names,
180: 	                                                vector<LogicalType> &return_types, vector<string> &names) {
181: 		auto file_name = inputs[0].GetValue<string>();
182: 
183: 		FileSystem &fs = FileSystem::GetFileSystem(context);
184: 		auto files = ParquetGlob(fs, file_name);
185: 		return ParquetScanBindInternal(context, move(files), return_types, names);
186: 	}
187: 
188: 	static unique_ptr<FunctionData> ParquetScanBindList(ClientContext &context, vector<Value> &inputs,
189: 	                                                    unordered_map<string, Value> &named_parameters,
190: 	                                                    vector<LogicalType> &input_table_types,
191: 	                                                    vector<string> &input_table_names,
192: 	                                                    vector<LogicalType> &return_types, vector<string> &names) {
193: 		FileSystem &fs = FileSystem::GetFileSystem(context);
194: 		vector<string> files;
195: 		for (auto &val : inputs[0].list_value) {
196: 			auto glob_files = ParquetGlob(fs, val.ToString());
197: 			files.insert(files.end(), glob_files.begin(), glob_files.end());
198: 		}
199: 		if (files.empty()) {
200: 			throw IOException("Parquet reader needs at least one file to read");
201: 		}
202: 		return ParquetScanBindInternal(context, move(files), return_types, names);
203: 	}
204: 
205: 	static unique_ptr<FunctionOperatorData> ParquetScanInit(ClientContext &context, const FunctionData *bind_data_p,
206: 	                                                        const vector<column_t> &column_ids,
207: 	                                                        TableFilterCollection *filters) {
208: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
209: 		bind_data.chunk_count = 0;
210: 		bind_data.cur_file = 0;
211: 		auto result = make_unique<ParquetReadOperatorData>();
212: 		result->column_ids = column_ids;
213: 
214: 		result->is_parallel = false;
215: 		result->file_index = 0;
216: 		result->table_filters = filters->table_filters;
217: 		// single-threaded: one thread has to read all groups
218: 		vector<idx_t> group_ids;
219: 		for (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {
220: 			group_ids.push_back(i);
221: 		}
222: 		result->reader = bind_data.initial_reader;
223: 		result->reader->InitializeScan(result->scan_state, column_ids, move(group_ids), filters->table_filters);
224: 		return move(result);
225: 	}
226: 
227: 	static int ParquetProgress(ClientContext &context, const FunctionData *bind_data_p) {
228: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
229: 		if (bind_data.initial_reader->NumRows() == 0) {
230: 			return (100 * (bind_data.cur_file + 1)) / bind_data.files.size();
231: 		}
232: 		auto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100 / bind_data.initial_reader->NumRows()) /
233: 		                  bind_data.files.size();
234: 		percentage += 100 * bind_data.cur_file / bind_data.files.size();
235: 		return percentage;
236: 	}
237: 
238: 	static unique_ptr<FunctionOperatorData>
239: 	ParquetScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *parallel_state_p,
240: 	                        const vector<column_t> &column_ids, TableFilterCollection *filters) {
241: 		auto result = make_unique<ParquetReadOperatorData>();
242: 		result->column_ids = column_ids;
243: 		result->is_parallel = true;
244: 		result->table_filters = filters->table_filters;
245: 		if (!ParquetParallelStateNext(context, bind_data_p, result.get(), parallel_state_p)) {
246: 			return nullptr;
247: 		}
248: 		return move(result);
249: 	}
250: 
251: 	static void ParquetScanImplementation(ClientContext &context, const FunctionData *bind_data_p,
252: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
253: 		auto &data = (ParquetReadOperatorData &)*operator_state;
254: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
255: 
256: 		do {
257: 			data.reader->Scan(data.scan_state, output);
258: 			bind_data.chunk_count++;
259: 			if (output.size() == 0 && !data.is_parallel) {
260: 				auto &bind_data = (ParquetReadBindData &)*bind_data_p;
261: 				// check if there is another file
262: 				if (data.file_index + 1 < bind_data.files.size()) {
263: 					data.file_index++;
264: 					bind_data.cur_file++;
265: 					bind_data.chunk_count = 0;
266: 					string file = bind_data.files[data.file_index];
267: 					// move to the next file
268: 					data.reader =
269: 					    make_shared<ParquetReader>(context, file, data.reader->return_types, bind_data.files[0]);
270: 					vector<idx_t> group_ids;
271: 					for (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {
272: 						group_ids.push_back(i);
273: 					}
274: 					data.reader->InitializeScan(data.scan_state, data.column_ids, move(group_ids), data.table_filters);
275: 				} else {
276: 					// exhausted all the files: done
277: 					break;
278: 				}
279: 			} else {
280: 				break;
281: 			}
282: 		} while (true);
283: 	}
284: 
285: 	static unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {
286: 		auto &data = (ParquetReadBindData &)*bind_data;
287: 		return make_unique<NodeStatistics>(data.initial_reader->NumRows() * data.files.size());
288: 	}
289: 
290: 	static idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {
291: 		auto &data = (ParquetReadBindData &)*bind_data;
292: 		return data.initial_reader->NumRowGroups() * data.files.size();
293: 	}
294: 
295: 	static unique_ptr<ParallelState> ParquetInitParallelState(ClientContext &context, const FunctionData *bind_data_p) {
296: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
297: 		auto result = make_unique<ParquetReadParallelState>();
298: 		result->current_reader = bind_data.initial_reader;
299: 		result->row_group_index = 0;
300: 		result->file_index = 0;
301: 		return move(result);
302: 	}
303: 
304: 	static bool ParquetParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
305: 	                                     FunctionOperatorData *state_p, ParallelState *parallel_state_p) {
306: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
307: 		auto &parallel_state = (ParquetReadParallelState &)*parallel_state_p;
308: 		auto &scan_data = (ParquetReadOperatorData &)*state_p;
309: 
310: 		lock_guard<mutex> parallel_lock(parallel_state.lock);
311: 		if (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {
312: 			// groups remain in the current parquet file: read the next group
313: 			scan_data.reader = parallel_state.current_reader;
314: 			vector<idx_t> group_indexes {parallel_state.row_group_index};
315: 			scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
316: 			                                 scan_data.table_filters);
317: 			parallel_state.row_group_index++;
318: 			return true;
319: 		} else {
320: 			// no groups remain in the current parquet file: check if there are more files to read
321: 			while (parallel_state.file_index + 1 < bind_data.files.size()) {
322: 				// read the next file
323: 				string file = bind_data.files[++parallel_state.file_index];
324: 				parallel_state.current_reader =
325: 				    make_shared<ParquetReader>(context, file, parallel_state.current_reader->return_types);
326: 				if (parallel_state.current_reader->NumRowGroups() == 0) {
327: 					// empty parquet file, move to next file
328: 					continue;
329: 				}
330: 				// set up the scan state to read the first group
331: 				scan_data.reader = parallel_state.current_reader;
332: 				vector<idx_t> group_indexes {0};
333: 				scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
334: 				                                 scan_data.table_filters);
335: 				parallel_state.row_group_index = 1;
336: 				return true;
337: 			}
338: 		}
339: 		return false;
340: 	}
341: };
342: 
343: struct ParquetWriteBindData : public FunctionData {
344: 	vector<LogicalType> sql_types;
345: 	string file_name;
346: 	vector<string> column_names;
347: 	duckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
348: };
349: 
350: struct ParquetWriteGlobalState : public GlobalFunctionData {
351: 	unique_ptr<ParquetWriter> writer;
352: };
353: 
354: struct ParquetWriteLocalState : public LocalFunctionData {
355: 	ParquetWriteLocalState() {
356: 		buffer = make_unique<ChunkCollection>();
357: 	}
358: 
359: 	unique_ptr<ChunkCollection> buffer;
360: };
361: 
362: unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info, vector<string> &names,
363:                                           vector<LogicalType> &sql_types) {
364: 	auto bind_data = make_unique<ParquetWriteBindData>();
365: 	for (auto &option : info.options) {
366: 		auto loption = StringUtil::Lower(option.first);
367: 		if (loption == "compression" || loption == "codec") {
368: 			if (!option.second.empty()) {
369: 				auto roption = StringUtil::Lower(option.second[0].ToString());
370: 				if (roption == "uncompressed") {
371: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::UNCOMPRESSED;
372: 					continue;
373: 				} else if (roption == "snappy") {
374: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
375: 					continue;
376: 				} else if (roption == "gzip") {
377: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::GZIP;
378: 					continue;
379: 				} else if (roption == "zstd") {
380: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::ZSTD;
381: 					continue;
382: 				}
383: 			}
384: 			throw ParserException("Expected %s argument to be either [uncompressed, snappy, gzip or zstd]", loption);
385: 		} else {
386: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
387: 		}
388: 	}
389: 	bind_data->sql_types = sql_types;
390: 	bind_data->column_names = names;
391: 	bind_data->file_name = info.file_path;
392: 	return move(bind_data);
393: }
394: 
395: unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &context, FunctionData &bind_data) {
396: 	auto global_state = make_unique<ParquetWriteGlobalState>();
397: 	auto &parquet_bind = (ParquetWriteBindData &)bind_data;
398: 
399: 	auto &fs = FileSystem::GetFileSystem(context);
400: 	global_state->writer = make_unique<ParquetWriter>(fs, parquet_bind.file_name, parquet_bind.sql_types,
401: 	                                                  parquet_bind.column_names, parquet_bind.codec);
402: 	return move(global_state);
403: }
404: 
405: void ParquetWriteSink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
406:                       LocalFunctionData &lstate, DataChunk &input) {
407: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
408: 	auto &local_state = (ParquetWriteLocalState &)lstate;
409: 
410: 	// append data to the local (buffered) chunk collection
411: 	local_state.buffer->Append(input);
412: 	if (local_state.buffer->Count() > 100000) {
413: 		// if the chunk collection exceeds a certain size we flush it to the parquet file
414: 		global_state.writer->Flush(*local_state.buffer);
415: 		// and reset the buffer
416: 		local_state.buffer = make_unique<ChunkCollection>();
417: 	}
418: }
419: 
420: void ParquetWriteCombine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
421:                          LocalFunctionData &lstate) {
422: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
423: 	auto &local_state = (ParquetWriteLocalState &)lstate;
424: 	// flush any data left in the local state to the file
425: 	global_state.writer->Flush(*local_state.buffer);
426: }
427: 
428: void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
429: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
430: 	// finalize: write any additional metadata to the file here
431: 	global_state.writer->Finalize();
432: }
433: 
434: unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ClientContext &context, FunctionData &bind_data) {
435: 	return make_unique<ParquetWriteLocalState>();
436: }
437: 
438: unique_ptr<TableFunctionRef> ParquetScanReplacement(const string &table_name, void *data) {
439: 	if (!StringUtil::EndsWith(table_name, ".parquet")) {
440: 		return nullptr;
441: 	}
442: 	auto table_function = make_unique<TableFunctionRef>();
443: 	vector<unique_ptr<ParsedExpression>> children;
444: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
445: 	table_function->function = make_unique<FunctionExpression>("parquet_scan", move(children));
446: 	return table_function;
447: }
448: 
449: void ParquetExtension::Load(DuckDB &db) {
450: 	auto scan_fun = ParquetScanFunction::GetFunctionSet();
451: 	CreateTableFunctionInfo cinfo(scan_fun);
452: 	cinfo.name = "read_parquet";
453: 	CreateTableFunctionInfo pq_scan = cinfo;
454: 	pq_scan.name = "parquet_scan";
455: 
456: 	ParquetMetaDataFunction meta_fun;
457: 	CreateTableFunctionInfo meta_cinfo(meta_fun);
458: 
459: 	ParquetSchemaFunction schema_fun;
460: 	CreateTableFunctionInfo schema_cinfo(schema_fun);
461: 
462: 	CopyFunction function("parquet");
463: 	function.copy_to_bind = ParquetWriteBind;
464: 	function.copy_to_initialize_global = ParquetWriteInitializeGlobal;
465: 	function.copy_to_initialize_local = ParquetWriteInitializeLocal;
466: 	function.copy_to_sink = ParquetWriteSink;
467: 	function.copy_to_combine = ParquetWriteCombine;
468: 	function.copy_to_finalize = ParquetWriteFinalize;
469: 	function.copy_from_bind = ParquetScanFunction::ParquetReadBind;
470: 	function.copy_from_function = scan_fun.functions[0];
471: 
472: 	function.extension = "parquet";
473: 	CreateCopyFunctionInfo info(function);
474: 
475: 	Connection con(db);
476: 	con.BeginTransaction();
477: 	auto &context = *con.context;
478: 	auto &catalog = Catalog::GetCatalog(context);
479: 	catalog.CreateCopyFunction(context, &info);
480: 	catalog.CreateTableFunction(context, &cinfo);
481: 	catalog.CreateTableFunction(context, &pq_scan);
482: 	catalog.CreateTableFunction(context, &meta_cinfo);
483: 	catalog.CreateTableFunction(context, &schema_cinfo);
484: 	con.Commit();
485: 
486: 	auto &config = DBConfig::GetConfig(*db.instance);
487: 	config.replacement_scans.emplace_back(ParquetScanReplacement);
488: }
489: 
490: } // namespace duckdb
[end of extension/parquet/parquet-extension.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: