{
  "repo": "duckdb/duckdb",
  "pull_number": 2912,
  "instance_id": "duckdb__duckdb-2912",
  "issue_numbers": [
    "2911"
  ],
  "base_commit": "cd8ea601b5a92ee7d94f71294362c48c9435f5f7",
  "patch": "diff --git a/src/common/arrow_wrapper.cpp b/src/common/arrow_wrapper.cpp\nindex 1cb6da5f0ccb..0fc0eedbca64 100644\n--- a/src/common/arrow_wrapper.cpp\n+++ b/src/common/arrow_wrapper.cpp\n@@ -101,8 +101,9 @@ int ResultArrowArrayStreamWrapper::MyStreamGetNext(struct ArrowArrayStream *stre\n \tif (result.type == QueryResultType::STREAM_RESULT) {\n \t\tauto &stream_result = (StreamQueryResult &)result;\n \t\tif (!stream_result.IsOpen()) {\n-\t\t\tmy_stream->last_error = \"Query Stream is closed\";\n-\t\t\treturn -1;\n+\t\t\t// Nothing to output\n+\t\t\tout->release = nullptr;\n+\t\t\treturn 0;\n \t\t}\n \t}\n \tunique_ptr<DataChunk> chunk_result = result.Fetch();\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_dataset.py b/tools/pythonpkg/tests/fast/arrow/test_dataset.py\nindex e91d29c5e159..8c24783b0052 100644\n--- a/tools/pythonpkg/tests/fast/arrow/test_dataset.py\n+++ b/tools/pythonpkg/tests/fast/arrow/test_dataset.py\n@@ -51,4 +51,34 @@ def test_parallel_dataset_register(self,duckdb_cursor):\n \n         rel = duckdb_conn.register(\"dataset\",userdata_parquet_dataset)\n \n-        assert duckdb_conn.execute(\"Select count(*) from dataset where first_name = 'Jose' and salary > 134708.82\").fetchone()[0] == 12\n\\ No newline at end of file\n+        assert duckdb_conn.execute(\"Select count(*) from dataset where first_name = 'Jose' and salary > 134708.82\").fetchone()[0] == 12\n+\n+    def test_parallel_dataset_roundtrip(self,duckdb_cursor):\n+        if not can_run:\n+            return\n+\n+        duckdb_conn = duckdb.connect()\n+        duckdb_conn.execute(\"PRAGMA threads=4\")\n+        duckdb_conn.execute(\"PRAGMA verify_parallelism\")\n+\n+        parquet_filename = os.path.join(os.path.dirname(os.path.realpath(__file__)),'data','userdata1.parquet')\n+\n+        userdata_parquet_dataset= pyarrow.dataset.dataset([\n+            parquet_filename,\n+            parquet_filename,\n+            parquet_filename,\n+        ]\n+        , format=\"parquet\")\n+\n+        rel = duckdb_conn.register(\"dataset\",userdata_parquet_dataset)\n+\n+        query = duckdb_conn.execute(\"SELECT * FROM dataset order by id\" )\n+        record_batch_reader = query.fetch_record_batch(2048)\n+\n+        arrow_table = record_batch_reader.read_all()\n+        # reorder since order of rows isn't deterministic\n+        df = userdata_parquet_dataset.to_table().to_pandas().sort_values('id').reset_index(drop=True)\n+        # turn it into an arrow table\n+        arrow_table_2 = pyarrow.Table.from_pandas(df)\n+\n+        assert arrow_table.equals(arrow_table_2)\n\\ No newline at end of file\ndiff --git a/tools/rpkg/tests/testthat/test_fetch_arrow.R b/tools/rpkg/tests/testthat/test_fetch_arrow.R\nindex 90ccc3b446cc..f419ea11e2af 100644\n--- a/tools/rpkg/tests/testthat/test_fetch_arrow.R\n+++ b/tools/rpkg/tests/testthat/test_fetch_arrow.R\n@@ -208,7 +208,7 @@ test_that(\"duckdb_fetch_arrow() record_batch_reader multiple vectors per chunk\",\n     cur_batch <- record_batch_reader$read_next_batch()\n     expect_equal(904,cur_batch$num_rows)\n \n-    expect_error(record_batch_reader$read_next_batch())\n+    record_batch_reader$read_next_batch()\n     \n     dbDisconnect(con, shutdown = T)\n })\n",
  "problem_statement": "Tests for dataset roundtrip with Arrow\nWe've been working on enabling true streaming with DuckDB <-> Arrow in R and haven't been able to get it quite right (you can see some of the stuff we're running into at https://github.com/apache/arrow/pull/11730).\r\n\r\nAs part of that process I tried to write some python tests to see if I can confirm it's an (Arrow or DuckDB) R package issue, or if it is something more widespread. \r\n\r\nI've seen the two following two errors in alternation on this test when running it locally. The IOError: Query Stream is closed is something we've seen on the R side as well.\r\n\r\nAny thoughts on what might be causing this?\r\n\r\n<details>\r\n```\r\ntests/fast/arrow/test_dataset.py:79: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\npyarrow/ipc.pxi:524: in pyarrow.lib._ReadPandasMixin.read_pandas\r\n    ???\r\npyarrow/ipc.pxi:587: in pyarrow.lib.RecordBatchReader.read_all\r\n    ???\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n>   ???\r\nE   OSError: Query Stream is closed\r\n\r\npyarrow/error.pxi:114: OSError\r\n```\r\n\r\n\r\n```\r\ntests/fast/arrow/test_dataset.py:80: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\npyarrow/ipc.pxi:525: in pyarrow.lib._ReadPandasMixin.read_pandas\r\n    ???\r\npyarrow/array.pxi:766: in pyarrow.lib._PandasConvertible.to_pandas\r\n    ???\r\npyarrow/table.pxi:1815: in pyarrow.lib.Table._to_pandas\r\n    ???\r\n../../../../envs/duckdb/lib/python3.9/site-packages/pyarrow/pandas_compat.py:789: in table_to_blockmanager\r\n    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)\r\n../../../../envs/duckdb/lib/python3.9/site-packages/pyarrow/pandas_compat.py:1128: in _table_to_blocks\r\n    result = pa.lib.table_to_blocks(options, block_table, categories,\r\npyarrow/table.pxi:1225: in pyarrow.lib.table_to_blocks\r\n    ???\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n>   ???\r\nE   pyarrow.lib.ArrowException: Unknown error: Wrapping P\ufffdid@com.com failed\r\n\r\npyarrow/error.pxi:137: ArrowException\r\n```\r\n</details>\n",
  "hints_text": "",
  "created_at": "2022-01-12T10:53:36Z"
}