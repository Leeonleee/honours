{
  "repo": "duckdb/duckdb",
  "pull_number": 16284,
  "instance_id": "duckdb__duckdb-16284",
  "issue_numbers": [
    "16257",
    "16257",
    "16260"
  ],
  "base_commit": "8605f802de38c8a6e63dfe3e757ccc6845c36784",
  "patch": "diff --git a/data/csv/afl/20250211_csv_fuzz_crash/case_53.csv b/data/csv/afl/20250211_csv_fuzz_crash/case_53.csv\nnew file mode 100644\nindex 000000000000..869b03087ab2\nBinary files /dev/null and b/data/csv/afl/20250211_csv_fuzz_crash/case_53.csv differ\ndiff --git a/data/csv/afl/4172/case_4.csv b/data/csv/afl/4172/case_4.csv\nnew file mode 100644\nindex 000000000000..c3c65cce9923\nBinary files /dev/null and b/data/csv/afl/4172/case_4.csv differ\ndiff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex 9e135e8b43ef..06f3b7a827a8 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -304,7 +304,8 @@ void ColumnReader::PreparePageV2(PageHeader &page_hdr) {\n \n \tauto compressed_bytes = page_hdr.compressed_page_size - uncompressed_bytes;\n \n-\tAllocateCompressed(compressed_bytes);\n+\tResizeableBuffer compressed_buffer;\n+\tcompressed_buffer.resize(GetAllocator(), compressed_bytes);\n \treader.ReadData(*protocol, compressed_buffer.ptr, compressed_bytes);\n \n \tDecompressInternal(chunk->meta_data.codec, compressed_buffer.ptr, compressed_bytes, block->ptr + uncompressed_bytes,\n@@ -319,10 +320,6 @@ void ColumnReader::AllocateBlock(idx_t size) {\n \t}\n }\n \n-void ColumnReader::AllocateCompressed(idx_t size) {\n-\tcompressed_buffer.resize(GetAllocator(), size);\n-}\n-\n void ColumnReader::PreparePage(PageHeader &page_hdr) {\n \tAllocateBlock(page_hdr.uncompressed_page_size + 1);\n \tif (chunk->meta_data.codec == CompressionCodec::UNCOMPRESSED) {\n@@ -333,7 +330,8 @@ void ColumnReader::PreparePage(PageHeader &page_hdr) {\n \t\treturn;\n \t}\n \n-\tAllocateCompressed(page_hdr.compressed_page_size + 1);\n+\tResizeableBuffer compressed_buffer;\n+\tcompressed_buffer.resize(GetAllocator(), page_hdr.compressed_page_size + 1);\n \treader.ReadData(*protocol, compressed_buffer.ptr, page_hdr.compressed_page_size);\n \n \tDecompressInternal(chunk->meta_data.codec, compressed_buffer.ptr, page_hdr.compressed_page_size, block->ptr,\ndiff --git a/extension/parquet/include/column_reader.hpp b/extension/parquet/include/column_reader.hpp\nindex 0e77e2a51a84..35544de8e900 100644\n--- a/extension/parquet/include/column_reader.hpp\n+++ b/extension/parquet/include/column_reader.hpp\n@@ -289,7 +289,6 @@ class ColumnReader {\n \n private:\n \tvoid AllocateBlock(idx_t size);\n-\tvoid AllocateCompressed(idx_t size);\n \tvoid PrepareRead(optional_ptr<const TableFilter> filter);\n \tvoid PreparePage(PageHeader &page_hdr);\n \tvoid PrepareDataPage(PageHeader &page_hdr);\n@@ -305,8 +304,6 @@ class ColumnReader {\n \n \tshared_ptr<ResizeableBuffer> block;\n \n-\tResizeableBuffer compressed_buffer;\n-\n \tColumnEncoding encoding = ColumnEncoding::INVALID;\n \tunique_ptr<RleBpDecoder> defined_decoder;\n \tunique_ptr<RleBpDecoder> repeated_decoder;\ndiff --git a/extension/parquet/include/parquet_bss_encoder.hpp b/extension/parquet/include/parquet_bss_encoder.hpp\nindex 49b1ab05b4ae..78f75a0c7572 100644\n--- a/extension/parquet/include/parquet_bss_encoder.hpp\n+++ b/extension/parquet/include/parquet_bss_encoder.hpp\n@@ -30,7 +30,6 @@ class BssEncoder {\n \t}\n \n \tvoid FinishWrite(WriteStream &writer) {\n-\t\tD_ASSERT(count == total_value_count);\n \t\twriter.WriteData(buffer.get(), total_value_count * bit_width);\n \t}\n \ndiff --git a/extension/parquet/include/parquet_dlba_encoder.hpp b/extension/parquet/include/parquet_dlba_encoder.hpp\nindex ef7d19f0cfcb..897dc9c5fead 100644\n--- a/extension/parquet/include/parquet_dlba_encoder.hpp\n+++ b/extension/parquet/include/parquet_dlba_encoder.hpp\n@@ -33,9 +33,8 @@ class DlbaEncoder {\n \t}\n \n \tvoid FinishWrite(WriteStream &writer) {\n-\t\tD_ASSERT(stream->GetPosition() == total_string_size);\n \t\tdbp_encoder.FinishWrite(writer);\n-\t\twriter.WriteData(buffer.get(), total_string_size);\n+\t\twriter.WriteData(buffer.get(), stream->GetPosition());\n \t}\n \n private:\ndiff --git a/extension/parquet/include/writer/boolean_column_writer.hpp b/extension/parquet/include/writer/boolean_column_writer.hpp\nindex f513f15a578f..6c650cb0308f 100644\n--- a/extension/parquet/include/writer/boolean_column_writer.hpp\n+++ b/extension/parquet/include/writer/boolean_column_writer.hpp\n@@ -24,7 +24,7 @@ class BooleanColumnWriter : public PrimitiveColumnWriter {\n \tvoid WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *state_p,\n \t                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override;\n \n-\tunique_ptr<ColumnWriterPageState> InitializePageState(PrimitiveColumnWriterState &state) override;\n+\tunique_ptr<ColumnWriterPageState> InitializePageState(PrimitiveColumnWriterState &state, idx_t page_idx) override;\n \tvoid FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state_p) override;\n \n \tidx_t GetRowSize(const Vector &vector, const idx_t index, const PrimitiveColumnWriterState &state) const override;\ndiff --git a/extension/parquet/include/writer/enum_column_writer.hpp b/extension/parquet/include/writer/enum_column_writer.hpp\nindex 724bfab6def6..ab4772eb23a1 100644\n--- a/extension/parquet/include/writer/enum_column_writer.hpp\n+++ b/extension/parquet/include/writer/enum_column_writer.hpp\n@@ -28,7 +28,7 @@ class EnumColumnWriter : public PrimitiveColumnWriter {\n \tvoid WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state_p,\n \t                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override;\n \n-\tunique_ptr<ColumnWriterPageState> InitializePageState(PrimitiveColumnWriterState &state) override;\n+\tunique_ptr<ColumnWriterPageState> InitializePageState(PrimitiveColumnWriterState &state, idx_t page_idx) override;\n \n \tvoid FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state_p) override;\n \ndiff --git a/extension/parquet/include/writer/primitive_column_writer.hpp b/extension/parquet/include/writer/primitive_column_writer.hpp\nindex 6315efbd7452..f3bea0323b1f 100644\n--- a/extension/parquet/include/writer/primitive_column_writer.hpp\n+++ b/extension/parquet/include/writer/primitive_column_writer.hpp\n@@ -93,7 +93,7 @@ class PrimitiveColumnWriter : public ColumnWriter {\n \tvirtual unique_ptr<ColumnWriterStatistics> InitializeStatsState();\n \n \t//! Initialize the writer for a specific page. Only used for scalar types.\n-\tvirtual unique_ptr<ColumnWriterPageState> InitializePageState(PrimitiveColumnWriterState &state);\n+\tvirtual unique_ptr<ColumnWriterPageState> InitializePageState(PrimitiveColumnWriterState &state, idx_t page_idx);\n \n \t//! Flushes the writer for a specific page. Only used for scalar types.\n \tvirtual void FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state);\ndiff --git a/extension/parquet/include/writer/templated_column_writer.hpp b/extension/parquet/include/writer/templated_column_writer.hpp\nindex 027af57fe6c5..c5aae83bc502 100644\n--- a/extension/parquet/include/writer/templated_column_writer.hpp\n+++ b/extension/parquet/include/writer/templated_column_writer.hpp\n@@ -126,11 +126,12 @@ class StandardColumnWriter : public PrimitiveColumnWriter {\n \t\treturn std::move(result);\n \t}\n \n-\tunique_ptr<ColumnWriterPageState> InitializePageState(PrimitiveColumnWriterState &state_p) override {\n+\tunique_ptr<ColumnWriterPageState> InitializePageState(PrimitiveColumnWriterState &state_p,\n+\t                                                      idx_t page_idx) override {\n \t\tauto &state = state_p.Cast<StandardColumnWriterState<SRC, TGT, OP>>();\n-\n-\t\tauto result = make_uniq<StandardWriterPageState<SRC, TGT, OP>>(state.total_value_count, state.total_string_size,\n-\t\t                                                               state.encoding, state.dictionary);\n+\t\tconst auto &page_info = state_p.page_info[page_idx];\n+\t\tauto result = make_uniq<StandardWriterPageState<SRC, TGT, OP>>(\n+\t\t    page_info.row_count - page_info.empty_count, state.total_string_size, state.encoding, state.dictionary);\n \t\treturn std::move(result);\n \t}\n \ndiff --git a/extension/parquet/writer/boolean_column_writer.cpp b/extension/parquet/writer/boolean_column_writer.cpp\nindex a8b2f9add185..c9b0e96618a9 100644\n--- a/extension/parquet/writer/boolean_column_writer.cpp\n+++ b/extension/parquet/writer/boolean_column_writer.cpp\n@@ -83,7 +83,8 @@ void BooleanColumnWriter::WriteVector(WriteStream &temp_writer, ColumnWriterStat\n \t}\n }\n \n-unique_ptr<ColumnWriterPageState> BooleanColumnWriter::InitializePageState(PrimitiveColumnWriterState &state) {\n+unique_ptr<ColumnWriterPageState> BooleanColumnWriter::InitializePageState(PrimitiveColumnWriterState &state,\n+                                                                           idx_t page_idx) {\n \treturn make_uniq<BooleanWriterPageState>();\n }\n \ndiff --git a/extension/parquet/writer/enum_column_writer.cpp b/extension/parquet/writer/enum_column_writer.cpp\nindex 8518019efedd..c6ae42827ada 100644\n--- a/extension/parquet/writer/enum_column_writer.cpp\n+++ b/extension/parquet/writer/enum_column_writer.cpp\n@@ -65,7 +65,8 @@ void EnumColumnWriter::WriteVector(WriteStream &temp_writer, ColumnWriterStatist\n \t}\n }\n \n-unique_ptr<ColumnWriterPageState> EnumColumnWriter::InitializePageState(PrimitiveColumnWriterState &state) {\n+unique_ptr<ColumnWriterPageState> EnumColumnWriter::InitializePageState(PrimitiveColumnWriterState &state,\n+                                                                        idx_t page_idx) {\n \treturn make_uniq<EnumWriterPageState>(bit_width);\n }\n \ndiff --git a/extension/parquet/writer/primitive_column_writer.cpp b/extension/parquet/writer/primitive_column_writer.cpp\nindex 9e3515de9d78..e1f54ca88002 100644\n--- a/extension/parquet/writer/primitive_column_writer.cpp\n+++ b/extension/parquet/writer/primitive_column_writer.cpp\n@@ -28,7 +28,8 @@ void PrimitiveColumnWriter::RegisterToRowGroup(duckdb_parquet::RowGroup &row_gro\n \trow_group.columns.push_back(std::move(column_chunk));\n }\n \n-unique_ptr<ColumnWriterPageState> PrimitiveColumnWriter::InitializePageState(PrimitiveColumnWriterState &state) {\n+unique_ptr<ColumnWriterPageState> PrimitiveColumnWriter::InitializePageState(PrimitiveColumnWriterState &state,\n+                                                                             idx_t page_idx) {\n \treturn nullptr;\n }\n \n@@ -114,7 +115,7 @@ void PrimitiveColumnWriter::BeginWrite(ColumnWriterState &state_p) {\n \t\t    MaxValue<idx_t>(NextPowerOfTwo(page_info.estimated_page_size), MemoryStream::DEFAULT_INITIAL_CAPACITY));\n \t\twrite_info.write_count = page_info.empty_count;\n \t\twrite_info.max_write_count = page_info.row_count;\n-\t\twrite_info.page_state = InitializePageState(state);\n+\t\twrite_info.page_state = InitializePageState(state, page_idx);\n \n \t\twrite_info.compressed_size = 0;\n \t\twrite_info.compressed_data = nullptr;\ndiff --git a/src/execution/index/art/iterator.cpp b/src/execution/index/art/iterator.cpp\nindex 689029a02e40..1c138e1d3e34 100644\n--- a/src/execution/index/art/iterator.cpp\n+++ b/src/execution/index/art/iterator.cpp\n@@ -46,9 +46,11 @@ bool Iterator::Scan(const ARTKey &upper_bound, const idx_t max_count, unsafe_vec\n \tbool has_next;\n \tdo {\n \t\t// An empty upper bound indicates that no upper bound exists.\n-\t\tif (!upper_bound.Empty() && status == GateStatus::GATE_NOT_SET) {\n-\t\t\tif (current_key.GreaterThan(upper_bound, equal, nested_depth)) {\n-\t\t\t\treturn true;\n+\t\tif (!upper_bound.Empty()) {\n+\t\t\tif (status == GateStatus::GATE_NOT_SET || entered_nested_leaf) {\n+\t\t\t\tif (current_key.GreaterThan(upper_bound, equal, nested_depth)) {\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n@@ -86,6 +88,7 @@ bool Iterator::Scan(const ARTKey &upper_bound, const idx_t max_count, unsafe_vec\n \t\t\tthrow InternalException(\"Invalid leaf type for index scan.\");\n \t\t}\n \n+\t\tentered_nested_leaf = false;\n \t\thas_next = Next();\n \t} while (has_next);\n \treturn true;\n@@ -104,6 +107,7 @@ void Iterator::FindMinimum(const Node &node) {\n \tif (node.GetGateStatus() == GateStatus::GATE_SET) {\n \t\tD_ASSERT(status == GateStatus::GATE_NOT_SET);\n \t\tstatus = GateStatus::GATE_SET;\n+\t\tentered_nested_leaf = true;\n \t\tnested_depth = 0;\n \t}\n \ndiff --git a/src/execution/operator/csv_scanner/encode/csv_encoder.cpp b/src/execution/operator/csv_scanner/encode/csv_encoder.cpp\nindex 89fc5df040bd..8a6c08032597 100644\n--- a/src/execution/operator/csv_scanner/encode/csv_encoder.cpp\n+++ b/src/execution/operator/csv_scanner/encode/csv_encoder.cpp\n@@ -51,6 +51,10 @@ CSVEncoder::CSVEncoder(DBConfig &config, const string &encoding_name_to_find, id\n \t}\n \t// We ensure that the encoded buffer size is an even number to make the two byte lookup on utf-16 work\n \tidx_t encoded_buffer_size = buffer_size % 2 != 0 ? buffer_size - 1 : buffer_size;\n+\tif (encoded_buffer_size == 0) {\n+\t\t// This might happen if buffer size = 1\n+\t\tencoded_buffer_size = 2;\n+\t}\n \tD_ASSERT(encoded_buffer_size > 0);\n \tencoded_buffer.Initialize(encoded_buffer_size);\n \tremaining_bytes_buffer.Initialize(function->GetBytesPerIteration());\ndiff --git a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\nindex 94ef37399510..266f35196af5 100644\n--- a/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n+++ b/src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n@@ -688,23 +688,29 @@ bool LineError::HandleErrors(StringValueResult &result) {\n \t\t\t\t    line_pos.GetGlobalPosition(result.requested_size), result.path);\n \t\t\t}\n \t\t\tbreak;\n-\t\tcase CAST_ERROR:\n+\t\tcase CAST_ERROR: {\n+\t\t\tstring column_name;\n+\t\t\tLogicalTypeId type_id;\n+\t\t\tif (cur_error.col_idx < result.names.size()) {\n+\t\t\t\tcolumn_name = result.names[cur_error.col_idx];\n+\t\t\t}\n+\t\t\tif (cur_error.col_idx < result.number_of_columns) {\n+\t\t\t\ttype_id = result.parse_types[cur_error.chunk_idx].type_id;\n+\t\t\t}\n \t\t\tif (result.current_line_position.begin == line_pos) {\n \t\t\t\tcsv_error = CSVError::CastError(\n-\t\t\t\t    result.state_machine.options, result.names[cur_error.col_idx], cur_error.error_message,\n-\t\t\t\t    cur_error.col_idx, borked_line, lines_per_batch,\n+\t\t\t\t    result.state_machine.options, column_name, cur_error.error_message, cur_error.col_idx, borked_line,\n+\t\t\t\t    lines_per_batch,\n \t\t\t\t    result.current_line_position.begin.GetGlobalPosition(result.requested_size, first_nl),\n-\t\t\t\t    line_pos.GetGlobalPosition(result.requested_size, first_nl),\n-\t\t\t\t    result.parse_types[cur_error.chunk_idx].type_id, result.path);\n+\t\t\t\t    line_pos.GetGlobalPosition(result.requested_size, first_nl), type_id, result.path);\n \t\t\t} else {\n \t\t\t\tcsv_error = CSVError::CastError(\n-\t\t\t\t    result.state_machine.options, result.names[cur_error.col_idx], cur_error.error_message,\n-\t\t\t\t    cur_error.col_idx, borked_line, lines_per_batch,\n+\t\t\t\t    result.state_machine.options, column_name, cur_error.error_message, cur_error.col_idx, borked_line,\n+\t\t\t\t    lines_per_batch,\n \t\t\t\t    result.current_line_position.begin.GetGlobalPosition(result.requested_size, first_nl),\n-\t\t\t\t    line_pos.GetGlobalPosition(result.requested_size), result.parse_types[cur_error.chunk_idx].type_id,\n-\t\t\t\t    result.path);\n+\t\t\t\t    line_pos.GetGlobalPosition(result.requested_size), type_id, result.path);\n \t\t\t}\n-\t\t\tbreak;\n+\t\t} break;\n \t\tcase MAXIMUM_LINE_SIZE:\n \t\t\tcsv_error = CSVError::LineSizeError(\n \t\t\t    result.state_machine.options, lines_per_batch, borked_line,\ndiff --git a/src/execution/operator/csv_scanner/util/csv_reader_options.cpp b/src/execution/operator/csv_scanner/util/csv_reader_options.cpp\nindex 9f7391eaa7a8..a4e0c82440cc 100644\n--- a/src/execution/operator/csv_scanner/util/csv_reader_options.cpp\n+++ b/src/execution/operator/csv_scanner/util/csv_reader_options.cpp\n@@ -254,6 +254,10 @@ void CSVReaderOptions::SetReadOption(const string &loption, const Value &value,\n \t\t\tthrow BinderException(\"Invalid value for MAX_LINE_SIZE parameter: it cannot be smaller than 0\");\n \t\t}\n \t\tmaximum_line_size.Set(NumericCast<idx_t>(line_size));\n+\t\tif (buffer_size_option.IsSetByUser() && maximum_line_size.GetValue() > buffer_size_option.GetValue()) {\n+\t\t\tthrow InvalidInputException(\"Buffer Size of %d must be a higher value than the maximum line size %d\",\n+\t\t\t                            buffer_size_option.GetValue(), maximum_line_size.GetValue());\n+\t\t}\n \t} else if (loption == \"date_format\" || loption == \"dateformat\") {\n \t\tstring format = ParseString(value, loption);\n \t\tSetDateFormat(LogicalTypeId::DATE, format, true);\n@@ -267,6 +271,12 @@ void CSVReaderOptions::SetReadOption(const string &loption, const Value &value,\n \t\tif (buffer_size_option == 0) {\n \t\t\tthrow InvalidInputException(\"Buffer Size option must be higher than 0\");\n \t\t}\n+\t\tif (maximum_line_size.IsSetByUser() && maximum_line_size.GetValue() > buffer_size_option.GetValue()) {\n+\t\t\tthrow InvalidInputException(\"Buffer Size of %d must be a higher value than the maximum line size %d\",\n+\t\t\t                            buffer_size_option.GetValue(), maximum_line_size.GetValue());\n+\t\t} else {\n+\t\t\tmaximum_line_size.Set(buffer_size_option.GetValue(), false);\n+\t\t}\n \t} else if (loption == \"decimal_separator\") {\n \t\tdecimal_separator = ParseString(value, loption);\n \t\tif (decimal_separator != \".\" && decimal_separator != \",\") {\n@@ -301,6 +311,9 @@ void CSVReaderOptions::SetReadOption(const string &loption, const Value &value,\n \t\tif (table_name.empty()) {\n \t\t\tthrow BinderException(\"REJECTS_TABLE option cannot be empty\");\n \t\t}\n+\t\tif (KeywordHelper::RequiresQuotes(table_name)) {\n+\t\t\tthrow BinderException(\"rejects_scan option: %s requires quotes to be used as an identifier\", table_name);\n+\t\t}\n \t\trejects_table_name.Set(table_name);\n \t} else if (loption == \"rejects_scan\") {\n \t\t// skip, handled in SetRejectsOptions\n@@ -308,6 +321,9 @@ void CSVReaderOptions::SetReadOption(const string &loption, const Value &value,\n \t\tif (table_name.empty()) {\n \t\t\tthrow BinderException(\"rejects_scan option cannot be empty\");\n \t\t}\n+\t\tif (KeywordHelper::RequiresQuotes(table_name)) {\n+\t\t\tthrow BinderException(\"rejects_scan option: %s requires quotes to be used as an identifier\", table_name);\n+\t\t}\n \t\trejects_scan_name.Set(table_name);\n \t} else if (loption == \"rejects_limit\") {\n \t\tauto limit = ParseInteger(value, loption);\ndiff --git a/src/execution/operator/helper/physical_streaming_sample.cpp b/src/execution/operator/helper/physical_streaming_sample.cpp\nindex 309256244927..ed9e21f35195 100644\n--- a/src/execution/operator/helper/physical_streaming_sample.cpp\n+++ b/src/execution/operator/helper/physical_streaming_sample.cpp\n@@ -5,10 +5,11 @@\n \n namespace duckdb {\n \n-PhysicalStreamingSample::PhysicalStreamingSample(vector<LogicalType> types, SampleMethod method, double percentage,\n-                                                 int64_t seed, idx_t estimated_cardinality)\n-    : PhysicalOperator(PhysicalOperatorType::STREAMING_SAMPLE, std::move(types), estimated_cardinality), method(method),\n-      percentage(percentage / 100), seed(seed) {\n+PhysicalStreamingSample::PhysicalStreamingSample(vector<LogicalType> types, unique_ptr<SampleOptions> options,\n+                                                 idx_t estimated_cardinality)\n+    : PhysicalOperator(PhysicalOperatorType::STREAMING_SAMPLE, std::move(types), estimated_cardinality),\n+      sample_options(std::move(options)) {\n+\tpercentage = sample_options->sample_size.GetValue<double>() / 100;\n }\n \n //===--------------------------------------------------------------------===//\n@@ -49,13 +50,21 @@ void PhysicalStreamingSample::BernoulliSample(DataChunk &input, DataChunk &resul\n \t}\n }\n \n+bool PhysicalStreamingSample::ParallelOperator() const {\n+\treturn !(sample_options->repeatable || sample_options->seed.IsValid());\n+}\n+\n unique_ptr<OperatorState> PhysicalStreamingSample::GetOperatorState(ExecutionContext &context) const {\n-\treturn make_uniq<StreamingSampleOperatorState>(seed);\n+\tif (!ParallelOperator()) {\n+\t\treturn make_uniq<StreamingSampleOperatorState>(static_cast<int64_t>(sample_options->seed.GetIndex()));\n+\t}\n+\tRandomEngine random;\n+\treturn make_uniq<StreamingSampleOperatorState>(static_cast<int64_t>(random.NextRandomInteger64()));\n }\n \n OperatorResultType PhysicalStreamingSample::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n                                                     GlobalOperatorState &gstate, OperatorState &state) const {\n-\tswitch (method) {\n+\tswitch (sample_options->method) {\n \tcase SampleMethod::BERNOULLI_SAMPLE:\n \t\tBernoulliSample(input, chunk, state);\n \t\tbreak;\n@@ -70,7 +79,7 @@ OperatorResultType PhysicalStreamingSample::Execute(ExecutionContext &context, D\n \n InsertionOrderPreservingMap<string> PhysicalStreamingSample::ParamsToString() const {\n \tInsertionOrderPreservingMap<string> result;\n-\tresult[\"Sample Method\"] = EnumUtil::ToString(method) + \": \" + to_string(100 * percentage) + \"%\";\n+\tresult[\"Sample Method\"] = EnumUtil::ToString(sample_options->method) + \": \" + to_string(100 * percentage) + \"%\";\n \treturn result;\n }\n \ndiff --git a/src/execution/operator/persistent/physical_batch_insert.cpp b/src/execution/operator/persistent/physical_batch_insert.cpp\nindex 2e546c477282..f898af2c7014 100644\n--- a/src/execution/operator/persistent/physical_batch_insert.cpp\n+++ b/src/execution/operator/persistent/physical_batch_insert.cpp\n@@ -215,7 +215,9 @@ class MergeCollectionTask : public BatchInsertTask {\n \t\tauto &gstate = gstate_p.Cast<BatchInsertGlobalState>();\n \t\tauto &lstate = lstate_p.Cast<BatchInsertLocalState>();\n \t\t// merge together the collections\n-\t\tD_ASSERT(lstate.writer);\n+\t\tif (!lstate.writer) {\n+\t\t\tlstate.writer = &gstate.table.GetStorage().CreateOptimisticWriter(context);\n+\t\t}\n \t\tauto final_collection = gstate.MergeCollections(context, std::move(merge_collections), *lstate.writer);\n \t\t// add the merged-together collection to the set of batch indexes\n \t\tlock_guard<mutex> l(gstate.lock);\ndiff --git a/src/execution/physical_plan/plan_sample.cpp b/src/execution/physical_plan/plan_sample.cpp\nindex be55784779fb..883c7055d46f 100644\n--- a/src/execution/physical_plan/plan_sample.cpp\n+++ b/src/execution/physical_plan/plan_sample.cpp\n@@ -28,9 +28,7 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalSample &op\n \t\t\t                      \"reservoir sampling or use a sample_size\",\n \t\t\t                      EnumUtil::ToString(op.sample_options->method));\n \t\t}\n-\t\tsample = make_uniq<PhysicalStreamingSample>(\n-\t\t    op.types, op.sample_options->method, op.sample_options->sample_size.GetValue<double>(),\n-\t\t    static_cast<int64_t>(op.sample_options->seed.GetIndex()), op.estimated_cardinality);\n+\t\tsample = make_uniq<PhysicalStreamingSample>(op.types, std::move(op.sample_options), op.estimated_cardinality);\n \t\tbreak;\n \tdefault:\n \t\tthrow InternalException(\"Unimplemented sample method\");\ndiff --git a/src/function/scalar/generic/getvariable.cpp b/src/function/scalar/generic/getvariable.cpp\nindex 14d32954d1cf..0181c07523bc 100644\n--- a/src/function/scalar/generic/getvariable.cpp\n+++ b/src/function/scalar/generic/getvariable.cpp\n@@ -24,12 +24,12 @@ struct GetVariableBindData : FunctionData {\n \n static unique_ptr<FunctionData> GetVariableBind(ClientContext &context, ScalarFunction &function,\n                                                 vector<unique_ptr<Expression>> &arguments) {\n+\tif (arguments[0]->HasParameter() || arguments[0]->return_type.id() == LogicalTypeId::UNKNOWN) {\n+\t\tthrow ParameterNotResolvedException();\n+\t}\n \tif (!arguments[0]->IsFoldable()) {\n \t\tthrow NotImplementedException(\"getvariable requires a constant input\");\n \t}\n-\tif (arguments[0]->HasParameter()) {\n-\t\tthrow ParameterNotResolvedException();\n-\t}\n \tValue value;\n \tauto variable_name = ExpressionExecutor::EvaluateScalar(context, *arguments[0]);\n \tif (!variable_name.IsNull()) {\ndiff --git a/src/function/window/window_boundaries_state.cpp b/src/function/window/window_boundaries_state.cpp\nindex ce3ba3bbeb85..a4b034441d38 100644\n--- a/src/function/window/window_boundaries_state.cpp\n+++ b/src/function/window/window_boundaries_state.cpp\n@@ -180,9 +180,9 @@ struct OperationCompare : public std::function<bool(T, T)> {\n };\n \n template <typename T, typename OP, bool FROM>\n-static idx_t FindTypedRangeBound(WindowCursor &over, const idx_t order_begin, const idx_t order_end,\n-                                 const WindowBoundary range, WindowInputExpression &boundary, const idx_t chunk_idx,\n-                                 const FrameBounds &prev) {\n+static idx_t FindTypedRangeBound(WindowCursor &range_lo, WindowCursor &range_hi, const idx_t order_begin,\n+                                 const idx_t order_end, const WindowBoundary range, WindowInputExpression &boundary,\n+                                 const idx_t chunk_idx, const FrameBounds &prev) {\n \tD_ASSERT(!boundary.CellIsNull(chunk_idx));\n \tconst auto val = boundary.GetCell<T>(chunk_idx);\n \n@@ -191,14 +191,14 @@ static idx_t FindTypedRangeBound(WindowCursor &over, const idx_t order_begin, co\n \t// Check that the value we are searching for is in range.\n \tif (range == WindowBoundary::EXPR_PRECEDING_RANGE) {\n \t\t//\tPreceding but value past the current value\n-\t\tconst auto cur_val = over.GetCell<T>(0, order_end - 1);\n+\t\tconst auto cur_val = range_hi.GetCell<T>(0, order_end - 1);\n \t\tif (comp(cur_val, val)) {\n \t\t\tthrow OutOfRangeException(\"Invalid RANGE PRECEDING value\");\n \t\t}\n \t} else {\n \t\t//\tFollowing but value before the current value\n \t\tD_ASSERT(range == WindowBoundary::EXPR_FOLLOWING_RANGE);\n-\t\tconst auto cur_val = over.GetCell<T>(0, order_begin);\n+\t\tconst auto cur_val = range_lo.GetCell<T>(0, order_begin);\n \t\tif (comp(val, cur_val)) {\n \t\t\tthrow OutOfRangeException(\"Invalid RANGE FOLLOWING value\");\n \t\t}\n@@ -206,20 +206,28 @@ static idx_t FindTypedRangeBound(WindowCursor &over, const idx_t order_begin, co\n \t//\tTry to reuse the previous bounds to restrict the search.\n \t//\tThis is only valid if the previous bounds were non-empty\n \t//\tOnly inject the comparisons if the previous bounds are a strict subset.\n-\tWindowColumnIterator<T> begin(over, order_begin);\n-\tWindowColumnIterator<T> end(over, order_end);\n+\tWindowColumnIterator<T> begin(range_lo, order_begin);\n+\tWindowColumnIterator<T> end(range_hi, order_end);\n \tif (prev.start < prev.end) {\n \t\tif (order_begin < prev.start && prev.start < order_end) {\n-\t\t\tconst auto first = over.GetCell<T>(0, prev.start);\n-\t\t\tif (!comp(val, first)) {\n-\t\t\t\t//\tprev.first <= val, so we can start further forward\n+\t\t\tconst auto first = range_lo.GetCell<T>(0, prev.start);\n+\t\t\tif (FROM && !comp(val, first)) {\n+\t\t\t\t// If prev.start == val and we are looking for a lower bound, then we are done\n+\t\t\t\tif (!comp(first, val)) {\n+\t\t\t\t\treturn prev.start;\n+\t\t\t\t}\n+\t\t\t\t//\tprev.start <= val, so we can start further forward\n \t\t\t\tbegin += UnsafeNumericCast<int64_t>(prev.start - order_begin);\n \t\t\t}\n \t\t}\n \t\tif (order_begin < prev.end && prev.end < order_end) {\n-\t\t\tconst auto second = over.GetCell<T>(0, prev.end - 1);\n+\t\t\tconst auto second = range_hi.GetCell<T>(0, prev.end - 1);\n \t\t\tif (!comp(second, val)) {\n-\t\t\t\t//\tval <= prev.second, so we can end further back\n+\t\t\t\t//  If val == prev.end and we are looking for an upper bound, then we are done\n+\t\t\t\tif (!FROM && !comp(val, second)) {\n+\t\t\t\t\treturn prev.end;\n+\t\t\t\t}\n+\t\t\t\t//\tval <= prev.end, so we can end further back\n \t\t\t\t// (prev.second is the largest peer)\n \t\t\t\tend -= UnsafeNumericCast<int64_t>(order_end - prev.end - 1);\n \t\t\t}\n@@ -234,52 +242,65 @@ static idx_t FindTypedRangeBound(WindowCursor &over, const idx_t order_begin, co\n }\n \n template <typename OP, bool FROM>\n-static idx_t FindRangeBound(WindowCursor &over, const idx_t order_begin, const idx_t order_end,\n-                            const WindowBoundary range, WindowInputExpression &boundary, const idx_t chunk_idx,\n-                            const FrameBounds &prev) {\n+static idx_t FindRangeBound(WindowCursor &range_lo, WindowCursor &range_hi, const idx_t order_begin,\n+                            const idx_t order_end, const WindowBoundary range, WindowInputExpression &boundary,\n+                            const idx_t chunk_idx, const FrameBounds &prev) {\n \tswitch (boundary.InternalType()) {\n \tcase PhysicalType::INT8:\n-\t\treturn FindTypedRangeBound<int8_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<int8_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                             chunk_idx, prev);\n \tcase PhysicalType::INT16:\n-\t\treturn FindTypedRangeBound<int16_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<int16_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                              chunk_idx, prev);\n \tcase PhysicalType::INT32:\n-\t\treturn FindTypedRangeBound<int32_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<int32_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                              chunk_idx, prev);\n \tcase PhysicalType::INT64:\n-\t\treturn FindTypedRangeBound<int64_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<int64_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                              chunk_idx, prev);\n \tcase PhysicalType::UINT8:\n-\t\treturn FindTypedRangeBound<uint8_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<uint8_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                              chunk_idx, prev);\n \tcase PhysicalType::UINT16:\n-\t\treturn FindTypedRangeBound<uint16_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<uint16_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                               chunk_idx, prev);\n \tcase PhysicalType::UINT32:\n-\t\treturn FindTypedRangeBound<uint32_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<uint32_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                               chunk_idx, prev);\n \tcase PhysicalType::UINT64:\n-\t\treturn FindTypedRangeBound<uint64_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<uint64_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                               chunk_idx, prev);\n \tcase PhysicalType::INT128:\n-\t\treturn FindTypedRangeBound<hugeint_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<hugeint_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                                chunk_idx, prev);\n \tcase PhysicalType::UINT128:\n-\t\treturn FindTypedRangeBound<uhugeint_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx,\n-\t\t                                                 prev);\n+\t\treturn FindTypedRangeBound<uhugeint_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                                 chunk_idx, prev);\n \tcase PhysicalType::FLOAT:\n-\t\treturn FindTypedRangeBound<float, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<float, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                            chunk_idx, prev);\n \tcase PhysicalType::DOUBLE:\n-\t\treturn FindTypedRangeBound<double, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindTypedRangeBound<double, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                             chunk_idx, prev);\n \tcase PhysicalType::INTERVAL:\n-\t\treturn FindTypedRangeBound<interval_t, OP, FROM>(over, order_begin, order_end, range, boundary, chunk_idx,\n-\t\t                                                 prev);\n+\t\treturn FindTypedRangeBound<interval_t, OP, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary,\n+\t\t                                                 chunk_idx, prev);\n \tdefault:\n \t\tthrow InternalException(\"Unsupported column type for RANGE\");\n \t}\n }\n \n template <bool FROM>\n-static idx_t FindOrderedRangeBound(WindowCursor &over, const OrderType range_sense, const idx_t order_begin,\n-                                   const idx_t order_end, const WindowBoundary range, WindowInputExpression &boundary,\n-                                   const idx_t chunk_idx, const FrameBounds &prev) {\n+static idx_t FindOrderedRangeBound(WindowCursor &range_lo, WindowCursor &range_hi, const OrderType range_sense,\n+                                   const idx_t order_begin, const idx_t order_end, const WindowBoundary range,\n+                                   WindowInputExpression &boundary, const idx_t chunk_idx, const FrameBounds &prev) {\n \tswitch (range_sense) {\n \tcase OrderType::ASCENDING:\n-\t\treturn FindRangeBound<LessThan, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindRangeBound<LessThan, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary, chunk_idx,\n+\t\t                                      prev);\n \tcase OrderType::DESCENDING:\n-\t\treturn FindRangeBound<GreaterThan, FROM>(over, order_begin, order_end, range, boundary, chunk_idx, prev);\n+\t\treturn FindRangeBound<GreaterThan, FROM>(range_lo, range_hi, order_begin, order_end, range, boundary, chunk_idx,\n+\t\t                                         prev);\n \tdefault:\n \t\tthrow InternalException(\"Unsupported ORDER BY sense for RANGE\");\n \t}\n@@ -718,6 +739,13 @@ void WindowBoundariesState::FrameBegin(DataChunk &bounds, idx_t row_idx, const i\n \tprev.start = valid_begin_data[0];\n \tprev.end = valid_end_data[0];\n \n+\tif (has_preceding_range || has_following_range) {\n+\t\tif (range_lo.get() != range.get()) {\n+\t\t\trange_lo = range.get();\n+\t\t\trange_hi = range_lo->Copy();\n+\t\t}\n+\t}\n+\n \tswitch (start_boundary) {\n \tcase WindowBoundary::UNBOUNDED_PRECEDING:\n \t\tbounds.data[FRAME_BEGIN].Reference(bounds.data[PARTITION_BEGIN]);\n@@ -766,7 +794,12 @@ void WindowBoundariesState::FrameBegin(DataChunk &bounds, idx_t row_idx, const i\n \t\t\t} else {\n \t\t\t\tconst auto valid_start = valid_begin_data[chunk_idx];\n \t\t\t\tprev.end = valid_end_data[chunk_idx];\n-\t\t\t\twindow_start = FindOrderedRangeBound<true>(*range, range_sense, valid_start, row_idx + 1,\n+\t\t\t\tconst auto cur_partition = partition_begin_data[chunk_idx];\n+\t\t\t\tif (cur_partition != prev_partition) {\n+\t\t\t\t\tprev.start = valid_start;\n+\t\t\t\t\tprev_partition = cur_partition;\n+\t\t\t\t}\n+\t\t\t\twindow_start = FindOrderedRangeBound<true>(*range_lo, *range_hi, range_sense, valid_start, row_idx + 1,\n \t\t\t\t                                           start_boundary, boundary_begin, chunk_idx, prev);\n \t\t\t\tprev.start = window_start;\n \t\t\t}\n@@ -785,8 +818,8 @@ void WindowBoundariesState::FrameBegin(DataChunk &bounds, idx_t row_idx, const i\n \t\t\t\t\tprev.start = valid_begin_data[chunk_idx];\n \t\t\t\t\tprev_partition = cur_partition;\n \t\t\t\t}\n-\t\t\t\twindow_start = FindOrderedRangeBound<true>(*range, range_sense, row_idx, valid_end, start_boundary,\n-\t\t\t\t                                           boundary_begin, chunk_idx, prev);\n+\t\t\t\twindow_start = FindOrderedRangeBound<true>(*range_lo, *range_hi, range_sense, row_idx, valid_end,\n+\t\t\t\t                                           start_boundary, boundary_begin, chunk_idx, prev);\n \t\t\t\tprev.start = window_start;\n \t\t\t}\n \t\t\tframe_begin_data[chunk_idx] = window_start;\n@@ -862,6 +895,13 @@ void WindowBoundariesState::FrameEnd(DataChunk &bounds, idx_t row_idx, const idx\n \tprev.start = valid_begin_data[0];\n \tprev.end = valid_end_data[0];\n \n+\tif (has_preceding_range || has_following_range) {\n+\t\tif (range_lo.get() != range.get()) {\n+\t\t\trange_lo = range.get();\n+\t\t\trange_hi = range_lo->Copy();\n+\t\t}\n+\t}\n+\n \tswitch (end_boundary) {\n \tcase WindowBoundary::CURRENT_ROW_ROWS:\n \t\tfor (idx_t chunk_idx = 0; chunk_idx < count; ++chunk_idx, ++row_idx) {\n@@ -911,8 +951,13 @@ void WindowBoundariesState::FrameEnd(DataChunk &bounds, idx_t row_idx, const idx\n \t\t\t} else {\n \t\t\t\tconst auto valid_start = valid_begin_data[chunk_idx];\n \t\t\t\tprev.start = valid_start;\n-\t\t\t\twindow_end = FindOrderedRangeBound<false>(*range, range_sense, valid_start, row_idx + 1, end_boundary,\n-\t\t\t\t                                          boundary_end, chunk_idx, prev);\n+\t\t\t\tconst auto cur_partition = partition_begin_data[chunk_idx];\n+\t\t\t\tif (cur_partition != prev_partition) {\n+\t\t\t\t\tprev.end = valid_end;\n+\t\t\t\t\tprev_partition = cur_partition;\n+\t\t\t\t}\n+\t\t\t\twindow_end = FindOrderedRangeBound<false>(*range_lo, *range_hi, range_sense, valid_start, row_idx + 1,\n+\t\t\t\t                                          end_boundary, boundary_end, chunk_idx, prev);\n \t\t\t\tprev.end = window_end;\n \t\t\t}\n \t\t\tframe_end_data[chunk_idx] = window_end;\n@@ -930,8 +975,8 @@ void WindowBoundariesState::FrameEnd(DataChunk &bounds, idx_t row_idx, const idx\n \t\t\t\t\tprev.end = valid_end;\n \t\t\t\t\tprev_partition = cur_partition;\n \t\t\t\t}\n-\t\t\t\twindow_end = FindOrderedRangeBound<false>(*range, range_sense, row_idx, valid_end, end_boundary,\n-\t\t\t\t                                          boundary_end, chunk_idx, prev);\n+\t\t\t\twindow_end = FindOrderedRangeBound<false>(*range_lo, *range_hi, range_sense, row_idx, valid_end,\n+\t\t\t\t                                          end_boundary, boundary_end, chunk_idx, prev);\n \t\t\t\tprev.end = window_end;\n \t\t\t}\n \t\t\tframe_end_data[chunk_idx] = window_end;\ndiff --git a/src/include/duckdb/execution/index/art/iterator.hpp b/src/include/duckdb/execution/index/art/iterator.hpp\nindex 58a0f106d54d..977cc7791081 100644\n--- a/src/include/duckdb/execution/index/art/iterator.hpp\n+++ b/src/include/duckdb/execution/index/art/iterator.hpp\n@@ -90,6 +90,8 @@ class Iterator {\n \tGateStatus status;\n \t//! Depth in a nested leaf.\n \tuint8_t nested_depth = 0;\n+\t//! True, if we entered a nested leaf to retrieve the next node.\n+\tbool entered_nested_leaf = false;\n \n private:\n \t//! Goes to the next leaf in the ART and sets it as last_leaf,\ndiff --git a/src/include/duckdb/execution/operator/helper/physical_streaming_sample.hpp b/src/include/duckdb/execution/operator/helper/physical_streaming_sample.hpp\nindex dafaf849f556..6f75b2cf1964 100644\n--- a/src/include/duckdb/execution/operator/helper/physical_streaming_sample.hpp\n+++ b/src/include/duckdb/execution/operator/helper/physical_streaming_sample.hpp\n@@ -19,12 +19,10 @@ class PhysicalStreamingSample : public PhysicalOperator {\n \tstatic constexpr const PhysicalOperatorType TYPE = PhysicalOperatorType::STREAMING_SAMPLE;\n \n public:\n-\tPhysicalStreamingSample(vector<LogicalType> types, SampleMethod method, double percentage, int64_t seed,\n-\t                        idx_t estimated_cardinality);\n+\tPhysicalStreamingSample(vector<LogicalType> types, unique_ptr<SampleOptions> options, idx_t estimated_cardinality);\n \n-\tSampleMethod method;\n+\tunique_ptr<SampleOptions> sample_options;\n \tdouble percentage;\n-\tint64_t seed;\n \n public:\n \t// Operator interface\n@@ -32,9 +30,7 @@ class PhysicalStreamingSample : public PhysicalOperator {\n \tOperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n \t                           GlobalOperatorState &gstate, OperatorState &state) const override;\n \n-\tbool ParallelOperator() const override {\n-\t\treturn true;\n-\t}\n+\tbool ParallelOperator() const override;\n \n \tInsertionOrderPreservingMap<string> ParamsToString() const override;\n \ndiff --git a/src/include/duckdb/function/window/window_boundaries_state.hpp b/src/include/duckdb/function/window/window_boundaries_state.hpp\nindex 2748bc7a0600..11c724d9b638 100644\n--- a/src/include/duckdb/function/window/window_boundaries_state.hpp\n+++ b/src/include/duckdb/function/window/window_boundaries_state.hpp\n@@ -148,6 +148,10 @@ struct WindowBoundariesState {\n \tidx_t valid_end = 0;\n \n \tFrameBounds prev;\n+\n+\t// Extra range cursor\n+\toptional_ptr<WindowCursor> range_lo;\n+\tunique_ptr<WindowCursor> range_hi;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/main/pending_query_result.hpp b/src/include/duckdb/main/pending_query_result.hpp\nindex 72fe9405fef8..cf0268712cba 100644\n--- a/src/include/duckdb/main/pending_query_result.hpp\n+++ b/src/include/duckdb/main/pending_query_result.hpp\n@@ -29,6 +29,8 @@ class PendingQueryResult : public BaseQueryResult {\n \tDUCKDB_API explicit PendingQueryResult(ErrorData error_message);\n \tDUCKDB_API ~PendingQueryResult() override;\n \tDUCKDB_API bool AllowStreamResult() const;\n+\tPendingQueryResult(const PendingQueryResult &) = delete;\n+\tPendingQueryResult &operator=(const PendingQueryResult &) = delete;\n \n public:\n \t//! Executes a single task within the query, returning whether or not the query is ready.\ndiff --git a/src/main/extension/extension_load.cpp b/src/main/extension/extension_load.cpp\nindex 4be4588371cf..5499665f1a27 100644\n--- a/src/main/extension/extension_load.cpp\n+++ b/src/main/extension/extension_load.cpp\n@@ -71,15 +71,11 @@ struct ExtensionAccess {\n \tstatic void SetError(duckdb_extension_info info, const char *error) {\n \t\tauto &load_state = DuckDBExtensionLoadState::Get(info);\n \n-\t\tif (error) {\n-\t\t\tload_state.has_error = true;\n-\t\t\tload_state.error_data = ErrorData(error);\n-\t\t} else {\n-\t\t\tload_state.has_error = true;\n-\t\t\tload_state.error_data = ErrorData(\n-\t\t\t    ExceptionType::UNKNOWN_TYPE,\n-\t\t\t    \"Extension has indicated an error occured during initialization, but did not set an error message.\");\n-\t\t}\n+\t\tload_state.has_error = true;\n+\t\tload_state.error_data =\n+\t\t    error ? ErrorData(error)\n+\t\t          : ErrorData(ExceptionType::UNKNOWN_TYPE, \"Extension has indicated an error occured during \"\n+\t\t                                                   \"initialization, but did not set an error message.\");\n \t}\n \n \t//! Called by the extension get a pointer to the database that is loading it\n@@ -92,9 +88,11 @@ struct ExtensionAccess {\n \t\t\tload_state.database_data->database = make_shared_ptr<DuckDB>(load_state.db);\n \t\t\treturn reinterpret_cast<duckdb_database *>(load_state.database_data.get());\n \t\t} catch (std::exception &ex) {\n+\t\t\tload_state.has_error = true;\n \t\t\tload_state.error_data = ErrorData(ex);\n \t\t\treturn nullptr;\n \t\t} catch (...) {\n+\t\t\tload_state.has_error = true;\n \t\t\tload_state.error_data =\n \t\t\t    ErrorData(ExceptionType::UNKNOWN_TYPE, \"Unknown error in GetDatabase when trying to load extension!\");\n \t\t\treturn nullptr;\ndiff --git a/src/parser/transform/expression/transform_subquery.cpp b/src/parser/transform/expression/transform_subquery.cpp\nindex 6f6d742073ba..0403d24bc5dc 100644\n--- a/src/parser/transform/expression/transform_subquery.cpp\n+++ b/src/parser/transform/expression/transform_subquery.cpp\n@@ -107,6 +107,7 @@ unique_ptr<ParsedExpression> Transformer::TransformSubquery(duckdb_libpgquery::P\n \t\t\t}\n \t\t}\n \t\t// transform constants (e.g. ORDER BY 1) into positional references (ORDER BY #1)\n+\t\tidx_t array_idx = 0;\n \t\tif (aggr->order_bys) {\n \t\t\tfor (auto &order : aggr->order_bys->orders) {\n \t\t\t\tif (order.expression->GetExpressionType() == ExpressionType::VALUE_CONSTANT) {\n@@ -120,8 +121,10 @@ unique_ptr<ParsedExpression> Transformer::TransformSubquery(duckdb_libpgquery::P\n \t\t\t\t\t}\n \t\t\t\t} else if (sub_select) {\n \t\t\t\t\t// if we have a SELECT we can push the ORDER BY clause into the SELECT list and reference it\n+\t\t\t\t\tauto alias = \"__array_internal_idx_\" + to_string(++array_idx);\n+\t\t\t\t\torder.expression->alias = alias;\n \t\t\t\t\tsub_select->select_list.push_back(std::move(order.expression));\n-\t\t\t\t\torder.expression = make_uniq<PositionalReferenceExpression>(sub_select->select_list.size() - 1);\n+\t\t\t\t\torder.expression = make_uniq<ColumnRefExpression>(alias);\n \t\t\t\t} else {\n \t\t\t\t\t// otherwise we remove order qualifications\n \t\t\t\t\tRemoveOrderQualificationRecursive(order.expression);\ndiff --git a/src/storage/table/row_group.cpp b/src/storage/table/row_group.cpp\nindex d5250387362b..55c6e064f4e5 100644\n--- a/src/storage/table/row_group.cpp\n+++ b/src/storage/table/row_group.cpp\n@@ -430,14 +430,13 @@ bool RowGroup::CheckZonemap(ScanFilterInfo &filters) {\n \t\tif (prune_result == FilterPropagateResult::FILTER_ALWAYS_FALSE) {\n \t\t\treturn false;\n \t\t}\n-\t\tif (prune_result == FilterPropagateResult::FILTER_ALWAYS_TRUE) {\n-\t\t\t// filter is always true - no need to check it\n-\t\t\t// label the filter as always true so we don't need to check it anymore\n-\t\t\tfilters.SetFilterAlwaysTrue(i);\n-\t\t}\n \t\tif (filter.filter_type == TableFilterType::OPTIONAL_FILTER) {\n \t\t\t// these are only for row group checking, set as always true so we don't check it\n \t\t\tfilters.SetFilterAlwaysTrue(i);\n+\t\t} else if (prune_result == FilterPropagateResult::FILTER_ALWAYS_TRUE) {\n+\t\t\t// filter is always true - no need to check it\n+\t\t\t// label the filter as always true so we don't need to check it anymore\n+\t\t\tfilters.SetFilterAlwaysTrue(i);\n \t\t}\n \t}\n \treturn true;\n@@ -619,7 +618,7 @@ void RowGroup::TemplatedScan(TransactionData transaction, CollectionScanState &s\n \t\t\t\t\t\tif (prune_result == FilterPropagateResult::FILTER_ALWAYS_FALSE) {\n \t\t\t\t\t\t\t// We can just break out of the loop here.\n \t\t\t\t\t\t\tapproved_tuple_count = 0;\n-\t\t\t\t\t\t\tbreak;\n+\t\t\t\t\t\t\tcontinue;\n \t\t\t\t\t\t}\n \n \t\t\t\t\t\t// Generate row ids\ndiff --git a/src/storage/table/scan_state.cpp b/src/storage/table/scan_state.cpp\nindex adeccde91b03..fdfa76433059 100644\n--- a/src/storage/table/scan_state.cpp\n+++ b/src/storage/table/scan_state.cpp\n@@ -96,6 +96,9 @@ void ScanFilterInfo::CheckAllFilters() {\n \n void ScanFilterInfo::SetFilterAlwaysTrue(idx_t filter_idx) {\n \tauto &filter = filter_list[filter_idx];\n+\tif (filter.always_true) {\n+\t\treturn;\n+\t}\n \tfilter.always_true = true;\n \tcolumn_has_filter[filter.scan_column_index] = false;\n \talways_true_filters++;\n",
  "test_patch": "diff --git a/test/issues/general/test_16257.test_slow b/test/issues/general/test_16257.test_slow\nnew file mode 100644\nindex 000000000000..6b3faf9a7ba4\n--- /dev/null\n+++ b/test/issues/general/test_16257.test_slow\n@@ -0,0 +1,25 @@\n+# name: test/issues/general/test_16257.test_slow\n+# description: Issue 16257 - value count mismatch when writing DELTA_BINARY_PACKED\n+# group: [general]\n+\n+require parquet\n+\n+# Some macros to generate lorem ipsum\n+statement ok\n+CREATE OR REPLACE MACRO deterministic_random(rand) AS hash(rand) / 18446744073709551615;\n+\n+statement ok\n+CREATE OR REPLACE MACRO lorem_word(rand) AS ['voluptatem', 'quaerat', 'quiquia', 'non', 'dolore', 'dolorem', 'labore', 'consectetur', 'porro', 'sed', 'numquam', 'aliquam', 'sit', 'eius', 'modi', 'est', 'amet', 'magnam', 'dolor', 'etincidunt', 'velit', 'neque', 'ipsum', 'adipisci', 'quisquam', 'ut', 'tempora'][1 + floor(rand * 27 % 27)::BIGINT];\n+\n+statement ok\n+CREATE OR REPLACE MACRO lorem_sentence_util(s) AS upper(s[1]) || s[2:] || '.';\n+\n+statement ok\n+CREATE OR REPLACE MACRO lorem_sentence(rand, words) AS lorem_sentence_util(list_aggr([lorem_word(deterministic_random(rand + i)) for i in range(words)], 'string_agg', ' '));\n+\n+\n+statement ok\n+SET preserve_insertion_order=false;\n+\n+statement ok\n+COPY (SELECT lorem_sentence(random(), 20) FROM range(1_000_000)) TO '__TEST_DIR__/16257.parquet' (PARQUET_VERSION V2, ROW_GROUP_SIZE 2_000_000);\ndiff --git a/test/sql/copy/csv/afl/fuzz_20250211_crash.test b/test/sql/copy/csv/afl/fuzz_20250211_crash.test\nnew file mode 100644\nindex 000000000000..7a10d16a002d\n--- /dev/null\n+++ b/test/sql/copy/csv/afl/fuzz_20250211_crash.test\n@@ -0,0 +1,10 @@\n+# name: test/sql/copy/csv/afl/fuzz_20250211_crash.test\n+# description: fuzzer generated csv files - should not raise internal exception (by failed assertion).\n+# group: [afl]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/20250211_csv_fuzz_crash/case_53.csv', buffer_size=42);\n+----\ndiff --git a/test/sql/copy/csv/afl/test_fuzz_4172.test b/test/sql/copy/csv/afl/test_fuzz_4172.test\nnew file mode 100644\nindex 000000000000..e22e66604e57\n--- /dev/null\n+++ b/test/sql/copy/csv/afl/test_fuzz_4172.test\n@@ -0,0 +1,10 @@\n+# name: test/sql/copy/csv/afl/test_fuzz_4172.test\n+# description: fuzzer generated csv files - should not raise internal exception (by failed assertion).\n+# group: [afl]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement maybe\n+FROM read_csv('data/csv/afl/4172/case_4.csv', ignore_errors=true, buffer_size=1, store_rejects=false);\n+----\ndiff --git a/test/sql/copy/csv/maximum_line_size.test_slow b/test/sql/copy/csv/maximum_line_size.test_slow\nindex db5672073fbf..1ea62d7e6513 100644\n--- a/test/sql/copy/csv/maximum_line_size.test_slow\n+++ b/test/sql/copy/csv/maximum_line_size.test_slow\n@@ -39,4 +39,4 @@ Be sure that the maximum line size is set to an appropriate value\n statement error\n select * from read_csv_auto('data/csv/issue_8320_3.csv.gz', max_line_size = 2097152, buffer_size = 10);\n ----\n-BUFFER_SIZE option was set to 10, while MAX_LINE_SIZE was set to 2097152. BUFFER_SIZE must have always be set to value bigger than MAX_LINE_SIZE\n\\ No newline at end of file\n+Buffer Size of 10 must be a higher value than the maximum line size 2097152\n\\ No newline at end of file\ndiff --git a/test/sql/copy/csv/parallel/csv_parallel_buffer_size.test b/test/sql/copy/csv/parallel/csv_parallel_buffer_size.test\nindex 466758becc6f..04f78983c3cb 100644\n--- a/test/sql/copy/csv/parallel/csv_parallel_buffer_size.test\n+++ b/test/sql/copy/csv/parallel/csv_parallel_buffer_size.test\n@@ -22,7 +22,7 @@ SELECT sum(a) FROM read_csv('data/csv/test/multi_column_integer_rn.csv',  COLUMN\n 111111111\n \n query IIII\n-select * from read_csv('data/csv/test/multi_column_string.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), auto_detect='true', delim = '|', buffer_size=25)\n+select * from read_csv('data/csv/test/multi_column_string.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), auto_detect='true', delim = '|', buffer_size=30)\n ----\n 1\t6370\t371\tp1\n 10\t214\t465\tp2\n@@ -35,7 +35,7 @@ select * from read_csv('data/csv/test/multi_column_string.csv',  COLUMNS=STRUCT_\n 100000000\t15519\t785\tp9\n \n query IIII\n-select * from read_csv('data/csv/test/multi_column_string_rn.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), auto_detect='true', delim = '|', buffer_size=25)\n+select * from read_csv('data/csv/test/multi_column_string_rn.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), auto_detect='true', delim = '|', buffer_size=27)\n ----\n 1\t6370\t371\tp1\n 10\t214\t465\tp2\n@@ -53,7 +53,7 @@ SELECT sum(a) FROM read_csv('data/csv/test/new_line_string_rn.csv',  COLUMNS=STR\n 111\n \n query I\n-SELECT sum(a) FROM read_csv('data/csv/test/new_line_string_rn.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), auto_detect='true', delim = '|', buffer_size=80)\n+SELECT sum(a) FROM read_csv('data/csv/test/new_line_string_rn.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), auto_detect='true', delim = '|', buffer_size=100)\n ----\n 111\n \n@@ -64,7 +64,7 @@ SELECT sum(a) FROM read_csv('data/csv/test/new_line_string_rn_exc.csv',  COLUMNS\n 111\n \n query I\n-SELECT sum(a) FROM read_csv('data/csv/test/new_line_string_rn_exc.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), auto_detect='true', delim = '|', buffer_size=60)\n+SELECT sum(a) FROM read_csv('data/csv/test/new_line_string_rn_exc.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), auto_detect='true', delim = '|', buffer_size=80)\n ----\n 111\n \n@@ -75,6 +75,6 @@ SELECT sum(a) FROM read_csv('data/csv/test/new_line_string.csv',  COLUMNS=STRUCT\n 111\n \n query I\n-SELECT sum(a) FROM read_csv('data/csv/test/new_line_string.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), auto_detect='true', delim = '|', buffer_size=60)\n+SELECT sum(a) FROM read_csv('data/csv/test/new_line_string.csv',  COLUMNS=STRUCT_PACK(a := 'INTEGER', b := 'INTEGER', c := 'INTEGER', d := 'VARCHAR'), quote ='\"', escape ='\"', comment = '', auto_detect='true', delim = '|', buffer_size=100, new_line = '\\r\\n')\n ----\n 111\ndiff --git a/test/sql/copy/csv/parallel/csv_parallel_new_line.test_slow b/test/sql/copy/csv/parallel/csv_parallel_new_line.test_slow\nindex b9654da5d729..f9fb5c45385f 100644\n--- a/test/sql/copy/csv/parallel/csv_parallel_new_line.test_slow\n+++ b/test/sql/copy/csv/parallel/csv_parallel_new_line.test_slow\n@@ -9,7 +9,7 @@ PRAGMA verify_parallelism\n statement ok\n PRAGMA enable_verification\n \n-loop i 25 100\n+loop i 27 100\n \n \n # Test read_csv auto with \\n\ndiff --git a/test/sql/copy/csv/relaxed_quotes.test b/test/sql/copy/csv/relaxed_quotes.test\nindex f8c6e8012c88..6bdfa8ede56b 100644\n--- a/test/sql/copy/csv/relaxed_quotes.test\n+++ b/test/sql/copy/csv/relaxed_quotes.test\n@@ -78,12 +78,7 @@ statement ok\n drop table t;\n \n statement error\n-create table t as from read_csv('data/csv/unescaped_quotes/unescaped_quote_new_line_rn.csv', strict_mode=false, buffer_size = 20, header = 0)\n-----\n-\n-\n-statement error\n-create table t as from read_csv('data/csv/unescaped_quotes/unescaped_quote_new_line_rn.csv', strict_mode=false, buffer_size = 20, header = 0)\n+create table t as from read_csv('data/csv/unescaped_quotes/unescaped_quote_new_line_rn.csv', strict_mode=false, buffer_size = 20, header = 0, delim = ';')\n ----\n \n statement ok\ndiff --git a/test/sql/copy/csv/test_validator.test b/test/sql/copy/csv/test_validator.test\nindex 66a444809520..e44d97b23014 100644\n--- a/test/sql/copy/csv/test_validator.test\n+++ b/test/sql/copy/csv/test_validator.test\n@@ -52,7 +52,7 @@ statement ok\n FROM read_csv('data/csv/validator/quoted_new_value.csv', columns = {'band': 'varchar', 'album': 'varchar', 'release': 'varchar'}, quote = '''', delim = ';', header = 0)\n \n statement ok\n-FROM read_csv('data/csv/validator/quoted_new_value.csv', columns = {'band': 'varchar', 'album': 'varchar', 'release': 'varchar'}, quote = '''', delim = ';', header = 0, buffer_size = 46)\n+FROM read_csv('data/csv/validator/quoted_new_value.csv', columns = {'band': 'varchar', 'album': 'varchar', 'release': 'varchar'}, quote = '''', delim = ';', header = 0, buffer_size = 48)\n \n statement ok\n FROM read_csv('data/csv/validator/single_column_quoted_newline.csv', columns = {'Raffaella Carr\u00e0': 'varchar'}, quote = '\"',  buffer_size = 24)\ndiff --git a/test/sql/index/art/scan/test_art_scan_normal_to_nested.test b/test/sql/index/art/scan/test_art_scan_normal_to_nested.test\nnew file mode 100644\nindex 000000000000..0cd8cf886fe5\n--- /dev/null\n+++ b/test/sql/index/art/scan/test_art_scan_normal_to_nested.test\n@@ -0,0 +1,38 @@\n+# name: test/sql/index/art/scan/test_art_scan_normal_to_nested.test\n+# description: Test range scanning with an iterator moving from a normal leaf to a nested leaf.\n+# group: [scan]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE integers (i BIGINT);\n+\n+statement ok\n+CREATE INDEX idx_integers ON integers (i);\n+\n+statement ok\n+INSERT INTO integers (i) VALUES ('1'), ('-1'), ('1');\n+\n+# The border is exactly when moving from a non-nested leaf to a nested leaf.\n+\n+query I\n+SELECT i FROM integers WHERE i <= 0;\n+----\n+-1\n+\n+# Issue 16074.\n+\n+statement ok\n+CREATE TABLE t0(c1 TIMESTAMP);\n+\n+statement ok\n+INSERT INTO t0(c1) VALUES ('2020-02-29 12:00:00'), ('1969-12-09 09:26:38'), ('2020-02-29 12:00:00');\n+\n+statement ok\n+CREATE INDEX i0 ON t0(c1);\n+\n+query I\n+SELECT c1 FROM t0 WHERE c1 <= '2007-07-07 07:07:07';\n+----\n+1969-12-09 09:26:38\n\\ No newline at end of file\ndiff --git a/test/sql/sample/bernoulli_sampling.test b/test/sql/sample/bernoulli_sampling.test\nnew file mode 100644\nindex 000000000000..95b3e3796c8f\n--- /dev/null\n+++ b/test/sql/sample/bernoulli_sampling.test\n@@ -0,0 +1,54 @@\n+# name: test/sql/sample/bernoulli_sampling.test\n+# description: Test reservoir sample crash on large data sets\n+# group: [sample]\n+\n+statement ok\n+create table output (num_rows INT);\n+\n+statement ok\n+select setseed(0.3);\n+\n+loop i 0 500\n+\n+statement ok\n+WITH some_tab AS (\n+    SELECT UNNEST(range(1000)) AS id\n+),\n+some_tab_unq AS (\n+    SELECT distinct(id) AS id FROM some_tab\n+),\n+sampled AS (\n+    select id from some_tab_unq\n+    USING SAMPLE 1% (bernoulli)\n+)\n+INSERT INTO output select count(*) as n_rows FROM sampled;\n+\n+endloop\n+\n+\n+query II\n+select min(num_rows) > 0, count(*) FILTER (num_rows = 0) = 0 from output;\n+----\n+true\ttrue\n+\n+query III\n+select avg(rowid), min(rowid), max(rowid) from output where num_rows = 0;\n+----\n+NULL\tNULL\tNULL\n+\n+statement ok\n+create table t1 as select range id from range(1000);\n+\n+statement ok\n+select setseed(0.6);\n+\n+query I nosort result_1\n+select id from t1 USING SAMPLE 1% (bernoulli, 5);\n+----\n+\n+query I nosort result_1\n+select id from t1 USING SAMPLE 1% (bernoulli, 5);\n+----\n+\n+\n+\ndiff --git a/test/sql/subquery/scalar/array_order_subquery.test b/test/sql/subquery/scalar/array_order_subquery.test\nindex a0ca2fb4c7d0..94abd308009a 100644\n--- a/test/sql/subquery/scalar/array_order_subquery.test\n+++ b/test/sql/subquery/scalar/array_order_subquery.test\n@@ -86,6 +86,16 @@ SELECT ARRAY\n ----\n [3, 2, 1]\n \n+query I\n+select array(select * from unnest(['a', 'b']) as _t(u) order by if(u='a',100, 1)) as out;\n+----\n+[b, a]\n+\n+query I\n+select array(select * from unnest(['a', 'b']) as _t(u) order by if(u='a',100, 1) desc) as out;\n+----\n+[a, b]\n+\n statement error\n SELECT ARRAY\n   (SELECT 1 UNION ALL\ndiff --git a/test/sql/variables/test_variables.test b/test/sql/variables/test_variables.test\nindex ad3c15d43f57..b67d81686222 100644\n--- a/test/sql/variables/test_variables.test\n+++ b/test/sql/variables/test_variables.test\n@@ -13,6 +13,22 @@ SELECT GETVARIABLE('animal')\n ----\n duck\n \n+statement ok\n+PREPARE v1 AS SELECT GETVARIABLE($1);\n+\n+query I\n+EXECUTE v1('animal');\n+----\n+duck\n+\n+statement ok\n+CREATE MACRO _(x) AS getvariable(x);\n+\n+query I\n+SELECT _('animal')\n+----\n+duck\n+\n # overwriting\n statement ok\n SET VARIABLE animal='bird'\n",
  "problem_statement": "InternalException: INTERNAL Error: value count mismatch when writing DELTA_BINARY_PACKED\n### What happens?\n\nwriting parquet file crash duckdb, notice, it works fine for sf =1 , but crash with 5 and above\n\n### To Reproduce\n\n```python\nimport duckdb\ncon=duckdb.connect()\ncon.sql(f\"\"\" ATTACH './db.duckdb' AS db (STORAGE_VERSION 'v1.2.0') \"\"\")\ncon.sql(f\" use db\")\ncon.sql(f\"CALL dbgen(sf=5)\")\ncon.sql(f\"\"\" COPY (SELECT * FROM partsupp) TO './partsupp' (FORMAT PARQUET,PARQUET_VERSION V2,PER_THREAD_OUTPUT TRUE,ROW_GROUP_SIZE 2_000_000 , APPEND) \"\"\")\ncon.close()\n```\n\n### OS:\n\nlinux\n\n### DuckDB Version:\n\nduckdb-1.2.1.dev321\n\n### DuckDB Client:\n\npython\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nmim\n\n### Affiliation:\n\npersonal\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a nightly build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\nInternalException: INTERNAL Error: value count mismatch when writing DELTA_BINARY_PACKED\n### What happens?\n\nwriting parquet file crash duckdb, notice, it works fine for sf =1 , but crash with 5 and above\n\n### To Reproduce\n\n```python\nimport duckdb\ncon=duckdb.connect()\ncon.sql(f\"\"\" ATTACH './db.duckdb' AS db (STORAGE_VERSION 'v1.2.0') \"\"\")\ncon.sql(f\" use db\")\ncon.sql(f\"CALL dbgen(sf=5)\")\ncon.sql(f\"\"\" COPY (SELECT * FROM partsupp) TO './partsupp' (FORMAT PARQUET,PARQUET_VERSION V2,PER_THREAD_OUTPUT TRUE,ROW_GROUP_SIZE 2_000_000 , APPEND) \"\"\")\ncon.close()\n```\n\n### OS:\n\nlinux\n\n### DuckDB Version:\n\nduckdb-1.2.1.dev321\n\n### DuckDB Client:\n\npython\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nmim\n\n### Affiliation:\n\npersonal\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a nightly build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\nChanged behaviour of getvariable\n### What happens?\n\nwith duckdb versions prior to version 1.1.3, i would run this command for ease of use:\n`CREATE MACRO _(x) AS getvariable(x);`\nso that i didn't have to write it each time,\nbut now on duckdb 1.2.0 i get back this error:\n`Not implemented Error: getvariable requires a constant input`\n\n1. is this expected?\n2. is there a way to rename it, and in general, to rename functions such as `getvariable` ?\n\n\n\n### To Reproduce\n\nfor reproducing, try this command on duckdb 1.1.3 and duckdb 1.2.0:\n`CREATE MACRO _(x) AS getvariable(x);`\n\n### OS:\n\nany\n\n### DuckDB Version:\n\n1.2.0\n\n### DuckDB Client:\n\nany \n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nMassimiliano Pizzotti\n\n### Affiliation:\n\nEssilorLuxottica\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "\n\n",
  "created_at": "2025-02-18T10:34:40Z"
}