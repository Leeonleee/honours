{
  "repo": "duckdb/duckdb",
  "pull_number": 3700,
  "instance_id": "duckdb__duckdb-3700",
  "issue_numbers": [
    "2260"
  ],
  "base_commit": "620b44ad73ea76d3987c12872933ed8f571d3c3a",
  "patch": "diff --git a/.github/regression/micro.csv b/.github/regression/micro.csv\nindex fe4ec909e72b..f717fd6bad9c 100644\n--- a/.github/regression/micro.csv\n+++ b/.github/regression/micro.csv\n@@ -3,8 +3,8 @@ Append100KIntegersPREPARED\n benchmark/micro/cast/cast_date_string.benchmark\n benchmark/micro/cast/cast_int_string.benchmark\n benchmark/micro/cast/cast_double_string.benchmark\n-benchmark/micro/cast/cast_int32_int64.benchmark\n-benchmark/micro/cast/cast_int64_int32.benchmark\n benchmark/micro/cast/cast_string_double.benchmark\n benchmark/micro/cast/cast_string_int.benchmark\n benchmark/micro/cast/cast_timestamp_string.benchmark\n+benchmark/micro/limit/parallel_limit.benchmark\n+benchmark/micro/filter/parallel_complex_filter.benchmark\ndiff --git a/benchmark/benchmark_runner.cpp b/benchmark/benchmark_runner.cpp\nindex 6c2059b982fa..15bc48caad0a 100644\n--- a/benchmark/benchmark_runner.cpp\n+++ b/benchmark/benchmark_runner.cpp\n@@ -47,12 +47,18 @@ static bool endsWith(const string &mainStr, const string &toMatch) {\n BenchmarkRunner::BenchmarkRunner() {\n }\n \n-void BenchmarkRunner::SaveDatabase(DuckDB &db, string name) {\n-\tauto &fs = db.GetFileSystem();\n+void BenchmarkRunner::InitializeBenchmarkDirectory() {\n+\tauto fs = FileSystem::CreateLocal();\n \t// check if the database directory exists; if not create it\n-\tif (!fs.DirectoryExists(DUCKDB_BENCHMARK_DIRECTORY)) {\n-\t\tfs.CreateDirectory(DUCKDB_BENCHMARK_DIRECTORY);\n+\tif (!fs->DirectoryExists(DUCKDB_BENCHMARK_DIRECTORY)) {\n+\t\tfs->CreateDirectory(DUCKDB_BENCHMARK_DIRECTORY);\n \t}\n+}\n+\n+void BenchmarkRunner::SaveDatabase(DuckDB &db, string name) {\n+\tInitializeBenchmarkDirectory();\n+\n+\tauto &fs = db.GetFileSystem();\n \tConnection con(db);\n \tauto result = con.Query(\n \t    StringUtil::Format(\"EXPORT DATABASE '%s' (FORMAT CSV)\", fs.JoinPath(DUCKDB_BENCHMARK_DIRECTORY, name)));\n@@ -266,6 +272,8 @@ void parse_arguments(const int arg_counter, char const *const *arg_values) {\n  * Returns an configuration error code.\n  */\n ConfigurationError run_benchmarks() {\n+\tBenchmarkRunner::InitializeBenchmarkDirectory();\n+\n \tauto &instance = BenchmarkRunner::GetInstance();\n \tauto &benchmarks = instance.benchmarks;\n \tif (!instance.configuration.name_pattern.empty()) {\ndiff --git a/benchmark/include/benchmark_runner.hpp b/benchmark/include/benchmark_runner.hpp\nindex 0990bbbfcfda..43725b585afd 100644\n--- a/benchmark/include/benchmark_runner.hpp\n+++ b/benchmark/include/benchmark_runner.hpp\n@@ -32,6 +32,8 @@ class BenchmarkRunner {\n \t\treturn instance;\n \t}\n \n+\tstatic void InitializeBenchmarkDirectory();\n+\n \t//! Save the current database state, exporting it to a set of CSVs in the DUCKDB_BENCHMARK_DIRECTORY directory\n \tstatic void SaveDatabase(DuckDB &db, string name);\n \t//! Try to initialize the database from the DUCKDB_BENCHMARK_DIRECTORY\ndiff --git a/benchmark/micro/filter/parallel_complex_filter.benchmark b/benchmark/micro/filter/parallel_complex_filter.benchmark\nnew file mode 100644\nindex 000000000000..8aa94c477efd\n--- /dev/null\n+++ b/benchmark/micro/filter/parallel_complex_filter.benchmark\n@@ -0,0 +1,19 @@\n+# name: benchmark/micro/filter/parallel_complex_filter.benchmark\n+# description: Benchmark of parallel complex filter limit computation\n+# group: [filter]\n+\n+name Parallel Complex Filter\n+group micro\n+subgroup filter\n+\n+load\n+CREATE TABLE integers AS SELECT * FROM range(100000000) tbl(i);\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 17797934;\n+\n+run\n+SELECT * FROM integers WHERE i IN (SELECT * FROM other_table)\n+\n+result I\n+337\n+948247\n+17797934\ndiff --git a/benchmark/micro/limit/parallel_limit.benchmark b/benchmark/micro/limit/parallel_limit.benchmark\nnew file mode 100644\nindex 000000000000..a52a46f01350\n--- /dev/null\n+++ b/benchmark/micro/limit/parallel_limit.benchmark\n@@ -0,0 +1,20 @@\n+# name: benchmark/micro/limit/parallel_limit.benchmark\n+# description: Benchmark of parallel limit computation\n+# group: [limit]\n+\n+name Parallel Limit\n+group micro\n+subgroup limit\n+\n+load\n+CREATE TABLE integers AS SELECT * FROM range(100000000) tbl(i);\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 17797934 UNION ALL SELECT 99999998 UNION ALL SELECT 99999999\n+\n+run\n+SELECT * FROM integers WHERE i IN (SELECT * FROM other_table) LIMIT 4\n+\n+result I\n+337\n+948247\n+17797934\n+99999998\ndiff --git a/benchmark/micro/limit/parallel_streaming_limit.benchmark b/benchmark/micro/limit/parallel_streaming_limit.benchmark\nnew file mode 100644\nindex 000000000000..f7a3e675166b\n--- /dev/null\n+++ b/benchmark/micro/limit/parallel_streaming_limit.benchmark\n@@ -0,0 +1,22 @@\n+# name: benchmark/micro/limit/parallel_streaming_limit.benchmark\n+# description: Benchmark of parallel streaming limit computation\n+# group: [limit]\n+\n+name Parallel Streaming Limit\n+group micro\n+subgroup limit\n+\n+load\n+SET preserve_insertion_order=false;\n+CREATE TABLE integers AS SELECT i, 1 AS j FROM range(100000000) tbl(i);\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 17797934 UNION ALL SELECT 99999998 UNION ALL SELECT 99999999\n+\n+\n+run\n+SELECT j FROM integers WHERE i IN (SELECT * FROM other_table) LIMIT 4\n+\n+result I\n+1\n+1\n+1\n+1\ndiff --git a/benchmark/micro/limit/parquet_parallel_limit.benchmark b/benchmark/micro/limit/parquet_parallel_limit.benchmark\nnew file mode 100644\nindex 000000000000..eae0f748c76b\n--- /dev/null\n+++ b/benchmark/micro/limit/parquet_parallel_limit.benchmark\n@@ -0,0 +1,24 @@\n+# name: benchmark/micro/limit/parquet_parallel_limit.benchmark\n+# description: Benchmark of parallel limit computation\n+# group: [limit]\n+\n+name Parallel Limit (Parquet)\n+group micro\n+subgroup limit\n+\n+require parquet\n+\n+load\n+CREATE TABLE tmp_integers AS SELECT * FROM range(100000000) tbl(i);\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 17797934 UNION ALL SELECT 99999998 UNION ALL SELECT 99999999;\n+COPY tmp_integers TO '${BENCHMARK_DIR}/integers.parquet';\n+CREATE VIEW integers AS SELECT * FROM '${BENCHMARK_DIR}/integers.parquet';\n+\n+run\n+SELECT * FROM integers WHERE i IN (SELECT * FROM other_table) LIMIT 4\n+\n+result I\n+337\n+948247\n+17797934\n+99999998\ndiff --git a/benchmark/micro/limit/parquet_parallel_limit_glob.benchmark b/benchmark/micro/limit/parquet_parallel_limit_glob.benchmark\nnew file mode 100644\nindex 000000000000..a63c8548ed4b\n--- /dev/null\n+++ b/benchmark/micro/limit/parquet_parallel_limit_glob.benchmark\n@@ -0,0 +1,24 @@\n+# name: benchmark/micro/limit/parquet_parallel_limit_glob.benchmark\n+# description: Benchmark of parallel limit computation\n+# group: [limit]\n+\n+name Parallel Limit (Parquet Glob)\n+group micro\n+subgroup limit\n+\n+require parquet\n+\n+load\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 17797934 UNION ALL SELECT 99999998 UNION ALL SELECT 99999999;\n+COPY (SELECT * FROM range(50000000) t(i)) TO '${BENCHMARK_DIR}/integers1.parquet';\n+COPY (SELECT * FROM range(50000000, 100000000) t(i)) TO '${BENCHMARK_DIR}/integers2.parquet';\n+CREATE VIEW integers AS SELECT * FROM parquet_scan(['${BENCHMARK_DIR}/integers1.parquet', '${BENCHMARK_DIR}/integers2.parquet']);\n+\n+run\n+SELECT * FROM integers WHERE i IN (SELECT * FROM other_table) LIMIT 4\n+\n+result I\n+337\n+948247\n+17797934\n+99999998\ndiff --git a/extension/parquet/parquet-extension.cpp b/extension/parquet/parquet-extension.cpp\nindex dfe73e8d5c09..14a3221570eb 100644\n--- a/extension/parquet/parquet-extension.cpp\n+++ b/extension/parquet/parquet-extension.cpp\n@@ -50,6 +50,7 @@ struct ParquetReadOperatorData : public FunctionOperatorData {\n \tshared_ptr<ParquetReader> reader;\n \tParquetReaderScanState scan_state;\n \tbool is_parallel;\n+\tidx_t batch_index;\n \tidx_t file_index;\n \tvector<column_t> column_ids;\n \tTableFilterSet *table_filters;\n@@ -58,6 +59,7 @@ struct ParquetReadOperatorData : public FunctionOperatorData {\n struct ParquetReadParallelState : public ParallelState {\n \tmutex lock;\n \tshared_ptr<ParquetReader> current_reader;\n+\tidx_t batch_index;\n \tidx_t file_index;\n \tidx_t row_group_index;\n };\n@@ -74,14 +76,11 @@ class ParquetScanFunction {\n \t\t                  ParquetInitParallelState, ParquetScanFuncParallel, ParquetScanParallelInit,\n \t\t                  ParquetParallelStateNext, true, true, ParquetProgress);\n \t\ttable_function.named_parameters[\"binary_as_string\"] = LogicalType::BOOLEAN;\n+\t\ttable_function.get_batch_index = ParquetScanGetBatchIndex;\n+\t\ttable_function.supports_batch_index = true;\n \t\tset.AddFunction(table_function);\n-\t\ttable_function = TableFunction({LogicalType::LIST(LogicalType::VARCHAR)}, ParquetScanImplementation,\n-\t\t                               ParquetScanBindList, ParquetScanInit, /* statistics */ ParquetScanStats,\n-\t\t                               /* cleanup */ nullptr,\n-\t\t                               /* dependency */ nullptr, ParquetCardinality,\n-\t\t                               /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr,\n-\t\t                               ParquetScanMaxThreads, ParquetInitParallelState, ParquetScanFuncParallel,\n-\t\t                               ParquetScanParallelInit, ParquetParallelStateNext, true, true, ParquetProgress);\n+\t\ttable_function.arguments = {LogicalType::LIST(LogicalType::VARCHAR)};\n+\t\ttable_function.bind = ParquetScanBindList;\n \t\ttable_function.named_parameters[\"binary_as_string\"] = LogicalType::BOOLEAN;\n \t\tset.AddFunction(table_function);\n \t\treturn set;\n@@ -280,6 +279,7 @@ class ParquetScanFunction {\n \t\tauto result = make_unique<ParquetReadOperatorData>();\n \t\tresult->column_ids = column_ids;\n \t\tresult->is_parallel = true;\n+\t\tresult->batch_index = 0;\n \t\tresult->table_filters = filters->table_filters;\n \t\tif (!ParquetParallelStateNext(context, bind_data_p, result.get(), parallel_state_p)) {\n \t\t\treturn nullptr;\n@@ -287,6 +287,12 @@ class ParquetScanFunction {\n \t\treturn move(result);\n \t}\n \n+\tstatic idx_t ParquetScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n+\t                                      FunctionOperatorData *operator_state, ParallelState *parallel_state_p) {\n+\t\tauto &data = (ParquetReadOperatorData &)*operator_state;\n+\t\treturn data.batch_index;\n+\t}\n+\n \tstatic void ParquetScanImplementation(ClientContext &context, const FunctionData *bind_data_p,\n \t                                      FunctionOperatorData *operator_state, DataChunk &output) {\n \t\tif (!operator_state) {\n@@ -343,6 +349,7 @@ class ParquetScanFunction {\n \t\tresult->current_reader = bind_data.initial_reader;\n \t\tresult->row_group_index = 0;\n \t\tresult->file_index = 0;\n+\t\tresult->batch_index = 0;\n \t\treturn move(result);\n \t}\n \n@@ -362,6 +369,8 @@ class ParquetScanFunction {\n \t\t\tvector<idx_t> group_indexes {parallel_state.row_group_index};\n \t\t\tscan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,\n \t\t\t                                 scan_data.table_filters);\n+\t\t\tscan_data.batch_index = parallel_state.batch_index++;\n+\t\t\tscan_data.file_index = parallel_state.file_index;\n \t\t\tparallel_state.row_group_index++;\n \t\t\treturn true;\n \t\t} else {\n@@ -381,6 +390,8 @@ class ParquetScanFunction {\n \t\t\t\tvector<idx_t> group_indexes {0};\n \t\t\t\tscan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,\n \t\t\t\t                                 scan_data.table_filters);\n+\t\t\t\tscan_data.batch_index = parallel_state.batch_index++;\n+\t\t\t\tscan_data.file_index = parallel_state.file_index;\n \t\t\t\tparallel_state.row_group_index = 1;\n \t\t\t\treturn true;\n \t\t\t}\ndiff --git a/src/common/enums/physical_operator_type.cpp b/src/common/enums/physical_operator_type.cpp\nindex 50fce25e4c1c..dd3c06350923 100644\n--- a/src/common/enums/physical_operator_type.cpp\n+++ b/src/common/enums/physical_operator_type.cpp\n@@ -19,6 +19,8 @@ string PhysicalOperatorToString(PhysicalOperatorType type) {\n \t\treturn \"LIMIT\";\n \tcase PhysicalOperatorType::LIMIT_PERCENT:\n \t\treturn \"LIMIT_PERCENT\";\n+\tcase PhysicalOperatorType::STREAMING_LIMIT:\n+\t\treturn \"STREAMING_LIMIT\";\n \tcase PhysicalOperatorType::RESERVOIR_SAMPLE:\n \t\treturn \"RESERVOIR_SAMPLE\";\n \tcase PhysicalOperatorType::STREAMING_SAMPLE:\n@@ -117,6 +119,8 @@ string PhysicalOperatorToString(PhysicalOperatorType type) {\n \t\treturn \"INOUT_FUNCTION\";\n \tcase PhysicalOperatorType::CREATE_TYPE:\n \t\treturn \"CREATE_TYPE\";\n+\tcase PhysicalOperatorType::RESULT_COLLECTOR:\n+\t\treturn \"RESULT_COLLECTOR\";\n \tcase PhysicalOperatorType::INVALID:\n \t\tbreak;\n \t}\ndiff --git a/src/common/types/CMakeLists.txt b/src/common/types/CMakeLists.txt\nindex 00f2139a22ef..cb5e8c267cfd 100644\n--- a/src/common/types/CMakeLists.txt\n+++ b/src/common/types/CMakeLists.txt\n@@ -1,6 +1,7 @@\n add_library_unity(\n   duckdb_common_types\n   OBJECT\n+  batched_chunk_collection.cpp\n   blob.cpp\n   cast_helpers.cpp\n   chunk_collection.cpp\ndiff --git a/src/common/types/batched_chunk_collection.cpp b/src/common/types/batched_chunk_collection.cpp\nnew file mode 100644\nindex 000000000000..681d9c0df85e\n--- /dev/null\n+++ b/src/common/types/batched_chunk_collection.cpp\n@@ -0,0 +1,71 @@\n+#include \"duckdb/common/types/batched_chunk_collection.hpp\"\n+#include \"duckdb/common/printer.hpp\"\n+\n+namespace duckdb {\n+\n+BatchedChunkCollection::BatchedChunkCollection() {\n+}\n+\n+void BatchedChunkCollection::Append(DataChunk &input, idx_t batch_index) {\n+\tD_ASSERT(batch_index != DConstants::INVALID_INDEX);\n+\tauto entry = data.find(batch_index);\n+\tChunkCollection *collection;\n+\tif (entry == data.end()) {\n+\t\tauto new_collection = make_unique<ChunkCollection>();\n+\t\tcollection = new_collection.get();\n+\t\tdata.insert(make_pair(batch_index, move(new_collection)));\n+\t} else {\n+\t\tcollection = entry->second.get();\n+\t}\n+\tcollection->Append(input);\n+}\n+\n+void BatchedChunkCollection::Merge(BatchedChunkCollection &other) {\n+\tfor (auto &entry : other.data) {\n+\t\tif (data.find(entry.first) != data.end()) {\n+\t\t\tthrow InternalException(\n+\t\t\t    \"BatchChunkCollection::Merge error - batch index %d is present in both collections. This occurs when \"\n+\t\t\t    \"batch indexes are not uniquely distributed over threads\",\n+\t\t\t    entry.first);\n+\t\t}\n+\t\tdata[entry.first] = move(entry.second);\n+\t}\n+\tother.data.clear();\n+}\n+\n+void BatchedChunkCollection::InitializeScan(BatchedChunkScanState &state) {\n+\tstate.iterator = data.begin();\n+\tstate.chunk_index = 0;\n+}\n+\n+void BatchedChunkCollection::Scan(BatchedChunkScanState &state, DataChunk &output) {\n+\twhile (state.iterator != data.end()) {\n+\t\t// check if there is a chunk remaining in this collection\n+\t\tauto collection = state.iterator->second.get();\n+\t\tif (state.chunk_index < collection->ChunkCount()) {\n+\t\t\t// there is! increment the chunk count\n+\t\t\toutput.Reference(collection->GetChunk(state.chunk_index));\n+\t\t\tstate.chunk_index++;\n+\t\t\treturn;\n+\t\t}\n+\t\t// there isn't! move to the next collection\n+\t\tstate.iterator++;\n+\t\tstate.chunk_index = 0;\n+\t}\n+}\n+\n+string BatchedChunkCollection::ToString() const {\n+\tstring result;\n+\tresult += \"Batched Chunk Collection\\n\";\n+\tfor (auto &entry : data) {\n+\t\tresult += \"Batch Index - \" + to_string(entry.first) + \"\\n\";\n+\t\tresult += entry.second->ToString() + \"\\n\\n\";\n+\t}\n+\treturn result;\n+}\n+\n+void BatchedChunkCollection::Print() const {\n+\tPrinter::Print(ToString());\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/common/types/chunk_collection.cpp b/src/common/types/chunk_collection.cpp\nindex c44a2390268c..c97e6675305d 100644\n--- a/src/common/types/chunk_collection.cpp\n+++ b/src/common/types/chunk_collection.cpp\n@@ -420,7 +420,12 @@ void ChunkCollection::CopyCell(idx_t column, idx_t index, Vector &target, idx_t\n \tVectorOperations::Copy(source, target, source_offset + 1, source_offset, target_offset);\n }\n \n-void ChunkCollection::Print() {\n+string ChunkCollection::ToString() const {\n+\treturn chunks.empty() ? \"ChunkCollection [ 0 ]\"\n+\t                      : \"ChunkCollection [ \" + std::to_string(count) + \" ]: \\n\" + chunks[0]->ToString();\n+}\n+\n+void ChunkCollection::Print() const {\n \tPrinter::Print(ToString());\n }\n \ndiff --git a/src/common/vector_operations/comparison_operators.cpp b/src/common/vector_operations/comparison_operators.cpp\nindex c58813e0b0be..5f2d931a6980 100644\n--- a/src/common/vector_operations/comparison_operators.cpp\n+++ b/src/common/vector_operations/comparison_operators.cpp\n@@ -137,17 +137,14 @@ inline idx_t ComparisonSelector::Select<duckdb::LessThanEquals>(Vector &left, Ve\n \treturn VectorOperations::LessThanEquals(left, right, sel, count, true_sel, false_sel);\n }\n \n-static idx_t ComparesNotNull(ValidityMask &vleft, ValidityMask &vright, ValidityMask &vresult, idx_t count,\n-                             SelectionVector &not_null) {\n-\tidx_t valid = 0;\n+static void ComparesNotNull(VectorData &ldata, VectorData &rdata, ValidityMask &vresult, idx_t count) {\n \tfor (idx_t i = 0; i < count; ++i) {\n-\t\tif (vleft.RowIsValid(i) && vright.RowIsValid(i)) {\n-\t\t\tnot_null.set_index(valid++, i);\n-\t\t} else {\n+\t\tauto lidx = ldata.sel->get_index(i);\n+\t\tauto ridx = rdata.sel->get_index(i);\n+\t\tif (!ldata.validity.RowIsValid(lidx) || !rdata.validity.RowIsValid(ridx)) {\n \t\t\tvresult.SetInvalid(i);\n \t\t}\n \t}\n-\treturn valid;\n }\n \n template <typename OP>\n@@ -174,23 +171,17 @@ static void NestedComparisonExecutor(Vector &left, Vector &right, Vector &result\n \n \tresult.SetVectorType(VectorType::FLAT_VECTOR);\n \tauto result_data = FlatVector::GetData<bool>(result);\n-\tauto &validity = FlatVector::Validity(result);\n+\tauto &result_validity = FlatVector::Validity(result);\n \n \tVectorData leftv, rightv;\n \tleft.Orrify(count, leftv);\n \tright.Orrify(count, rightv);\n-\n+\tif (!leftv.validity.AllValid() || !rightv.validity.AllValid()) {\n+\t\tComparesNotNull(leftv, rightv, result_validity, count);\n+\t}\n \tSelectionVector true_sel(count);\n \tSelectionVector false_sel(count);\n-\n-\tidx_t match_count = 0;\n-\tif (leftv.validity.AllValid() && rightv.validity.AllValid()) {\n-\t\tmatch_count = ComparisonSelector::Select<OP>(left, right, nullptr, count, &true_sel, &false_sel);\n-\t} else {\n-\t\tSelectionVector not_null(count);\n-\t\tcount = ComparesNotNull(leftv.validity, rightv.validity, validity, count, not_null);\n-\t\tmatch_count = ComparisonSelector::Select<OP>(left, right, &not_null, count, &true_sel, &false_sel);\n-\t}\n+\tidx_t match_count = ComparisonSelector::Select<OP>(left, right, nullptr, count, &true_sel, &false_sel);\n \n \tfor (idx_t i = 0; i < match_count; ++i) {\n \t\tconst auto idx = true_sel.get_index(i);\ndiff --git a/src/execution/operator/helper/CMakeLists.txt b/src/execution/operator/helper/CMakeLists.txt\nindex 5a59a2278be8..38986766854d 100644\n--- a/src/execution/operator/helper/CMakeLists.txt\n+++ b/src/execution/operator/helper/CMakeLists.txt\n@@ -1,15 +1,19 @@\n add_library_unity(\n   duckdb_operator_helper\n   OBJECT\n+  physical_batch_collector.cpp\n   physical_execute.cpp\n   physical_explain_analyze.cpp\n   physical_limit.cpp\n   physical_limit_percent.cpp\n   physical_load.cpp\n+  physical_materialized_collector.cpp\n   physical_pragma.cpp\n   physical_prepare.cpp\n   physical_reservoir_sample.cpp\n+  physical_result_collector.cpp\n   physical_set.cpp\n+  physical_streaming_limit.cpp\n   physical_streaming_sample.cpp\n   physical_transaction.cpp\n   physical_vacuum.cpp)\ndiff --git a/src/execution/operator/helper/physical_batch_collector.cpp b/src/execution/operator/helper/physical_batch_collector.cpp\nnew file mode 100644\nindex 000000000000..9a8044207802\n--- /dev/null\n+++ b/src/execution/operator/helper/physical_batch_collector.cpp\n@@ -0,0 +1,79 @@\n+#include \"duckdb/execution/operator/helper/physical_batch_collector.hpp\"\n+#include \"duckdb/common/types/batched_chunk_collection.hpp\"\n+#include \"duckdb/main/materialized_query_result.hpp\"\n+#include \"duckdb/main/client_context.hpp\"\n+\n+namespace duckdb {\n+\n+PhysicalBatchCollector::PhysicalBatchCollector(PreparedStatementData &data) : PhysicalResultCollector(data) {\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Sink\n+//===--------------------------------------------------------------------===//\n+class BatchCollectorGlobalState : public GlobalSinkState {\n+public:\n+\tmutex glock;\n+\tBatchedChunkCollection data;\n+\tunique_ptr<MaterializedQueryResult> result;\n+};\n+\n+class BatchCollectorLocalState : public LocalSinkState {\n+public:\n+\tBatchedChunkCollection data;\n+};\n+\n+SinkResultType PhysicalBatchCollector::Sink(ExecutionContext &context, GlobalSinkState &gstate,\n+                                            LocalSinkState &lstate_p, DataChunk &input) const {\n+\tauto &state = (BatchCollectorLocalState &)lstate_p;\n+\tstate.data.Append(input, state.batch_index);\n+\treturn SinkResultType::NEED_MORE_INPUT;\n+}\n+\n+void PhysicalBatchCollector::Combine(ExecutionContext &context, GlobalSinkState &gstate_p,\n+                                     LocalSinkState &lstate_p) const {\n+\tauto &gstate = (BatchCollectorGlobalState &)gstate_p;\n+\tauto &state = (BatchCollectorLocalState &)lstate_p;\n+\n+\tlock_guard<mutex> lock(gstate.glock);\n+\tgstate.data.Merge(state.data);\n+}\n+\n+SinkFinalizeType PhysicalBatchCollector::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,\n+                                                  GlobalSinkState &gstate_p) const {\n+\tauto &gstate = (BatchCollectorGlobalState &)gstate_p;\n+\tauto result =\n+\t    make_unique<MaterializedQueryResult>(statement_type, properties, types, names, context.shared_from_this());\n+\tDataChunk output;\n+\toutput.Initialize(types);\n+\n+\tBatchedChunkScanState state;\n+\tgstate.data.InitializeScan(state);\n+\twhile (true) {\n+\t\toutput.Reset();\n+\t\tgstate.data.Scan(state, output);\n+\t\tif (output.size() == 0) {\n+\t\t\tbreak;\n+\t\t}\n+\t\tresult->collection.Append(output);\n+\t}\n+\n+\tgstate.result = move(result);\n+\treturn SinkFinalizeType::READY;\n+}\n+\n+unique_ptr<LocalSinkState> PhysicalBatchCollector::GetLocalSinkState(ExecutionContext &context) const {\n+\treturn make_unique<BatchCollectorLocalState>();\n+}\n+\n+unique_ptr<GlobalSinkState> PhysicalBatchCollector::GetGlobalSinkState(ClientContext &context) const {\n+\treturn make_unique<BatchCollectorGlobalState>();\n+}\n+\n+unique_ptr<QueryResult> PhysicalBatchCollector::GetResult(GlobalSinkState &state) {\n+\tauto &gstate = (BatchCollectorGlobalState &)state;\n+\tD_ASSERT(gstate.result);\n+\treturn move(gstate.result);\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/execution/operator/helper/physical_execute.cpp b/src/execution/operator/helper/physical_execute.cpp\nindex 715bc149587e..68658f97b958 100644\n--- a/src/execution/operator/helper/physical_execute.cpp\n+++ b/src/execution/operator/helper/physical_execute.cpp\n@@ -6,4 +6,13 @@ PhysicalExecute::PhysicalExecute(PhysicalOperator *plan)\n     : PhysicalOperator(PhysicalOperatorType::EXECUTE, plan->types, -1), plan(plan) {\n }\n \n+vector<PhysicalOperator *> PhysicalExecute::GetChildren() const {\n+\treturn {plan};\n+}\n+\n+void PhysicalExecute::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\t// EXECUTE statement: build pipeline on child\n+\tplan->BuildPipelines(executor, current, state);\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/helper/physical_limit.cpp b/src/execution/operator/helper/physical_limit.cpp\nindex 79936d6e0886..c82548cd36f5 100644\n--- a/src/execution/operator/helper/physical_limit.cpp\n+++ b/src/execution/operator/helper/physical_limit.cpp\n@@ -1,18 +1,40 @@\n #include \"duckdb/execution/operator/helper/physical_limit.hpp\"\n \n #include \"duckdb/common/algorithm.hpp\"\n+#include \"duckdb/main/config.hpp\"\n \n #include \"duckdb/execution/expression_executor.hpp\"\n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/batched_chunk_collection.hpp\"\n+#include \"duckdb/execution/operator/helper/physical_streaming_limit.hpp\"\n \n namespace duckdb {\n \n+PhysicalLimit::PhysicalLimit(vector<LogicalType> types, idx_t limit, idx_t offset,\n+                             unique_ptr<Expression> limit_expression, unique_ptr<Expression> offset_expression,\n+                             idx_t estimated_cardinality)\n+    : PhysicalOperator(PhysicalOperatorType::LIMIT, move(types), estimated_cardinality), limit_value(limit),\n+      offset_value(offset), limit_expression(move(limit_expression)), offset_expression(move(offset_expression)) {\n+}\n+\n //===--------------------------------------------------------------------===//\n // Sink\n //===--------------------------------------------------------------------===//\n class LimitGlobalState : public GlobalSinkState {\n public:\n-\texplicit LimitGlobalState(const PhysicalLimit &op) : current_offset(0) {\n+\texplicit LimitGlobalState(const PhysicalLimit &op) {\n+\t\tlimit = 0;\n+\t\toffset = 0;\n+\t}\n+\n+\tmutex glock;\n+\tidx_t limit;\n+\tidx_t offset;\n+\tBatchedChunkCollection data;\n+};\n+\n+class LimitLocalState : public LocalSinkState {\n+public:\n+\texplicit LimitLocalState(const PhysicalLimit &op) : current_offset(0) {\n \t\tthis->limit = op.limit_expression ? DConstants::INVALID_INDEX : op.limit_value;\n \t\tthis->offset = op.offset_expression ? DConstants::INVALID_INDEX : op.offset_value;\n \t}\n@@ -20,31 +42,30 @@ class LimitGlobalState : public GlobalSinkState {\n \tidx_t current_offset;\n \tidx_t limit;\n \tidx_t offset;\n-\tChunkCollection data;\n+\tBatchedChunkCollection data;\n };\n \n unique_ptr<GlobalSinkState> PhysicalLimit::GetGlobalSinkState(ClientContext &context) const {\n \treturn make_unique<LimitGlobalState>(*this);\n }\n \n-SinkResultType PhysicalLimit::Sink(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate,\n-                                   DataChunk &input) const {\n-\tD_ASSERT(input.size() > 0);\n-\tauto &state = (LimitGlobalState &)gstate;\n-\tauto &limit = state.limit;\n-\tauto &offset = state.offset;\n+unique_ptr<LocalSinkState> PhysicalLimit::GetLocalSinkState(ExecutionContext &context) const {\n+\treturn make_unique<LimitLocalState>(*this);\n+}\n \n+bool PhysicalLimit::ComputeOffset(DataChunk &input, idx_t &limit, idx_t &offset, idx_t current_offset,\n+                                  idx_t &max_element, Expression *limit_expression, Expression *offset_expression) {\n \tif (limit != DConstants::INVALID_INDEX && offset != DConstants::INVALID_INDEX) {\n-\t\tidx_t max_element = limit + offset;\n-\t\tif ((limit == 0 || state.current_offset >= max_element) && !(limit_expression || offset_expression)) {\n-\t\t\treturn SinkResultType::FINISHED;\n+\t\tmax_element = limit + offset;\n+\t\tif ((limit == 0 || current_offset >= max_element) && !(limit_expression || offset_expression)) {\n+\t\t\treturn false;\n \t\t}\n \t}\n \n \t// get the next chunk from the child\n \tif (limit == DConstants::INVALID_INDEX) {\n \t\tlimit = 1ULL << 62ULL;\n-\t\tValue val = GetDelimiter(input, limit_expression.get());\n+\t\tValue val = GetDelimiter(input, limit_expression);\n \t\tif (!val.IsNull()) {\n \t\t\tlimit = val.GetValue<idx_t>();\n \t\t}\n@@ -54,7 +75,7 @@ SinkResultType PhysicalLimit::Sink(ExecutionContext &context, GlobalSinkState &g\n \t}\n \tif (offset == DConstants::INVALID_INDEX) {\n \t\toffset = 0;\n-\t\tValue val = GetDelimiter(input, offset_expression.get());\n+\t\tValue val = GetDelimiter(input, offset_expression);\n \t\tif (!val.IsNull()) {\n \t\t\toffset = val.GetValue<idx_t>();\n \t\t}\n@@ -62,42 +83,77 @@ SinkResultType PhysicalLimit::Sink(ExecutionContext &context, GlobalSinkState &g\n \t\t\tthrow BinderException(\"Max value %lld for LIMIT/OFFSET is %lld\", offset, 1ULL << 62ULL);\n \t\t}\n \t}\n-\tidx_t max_element = limit + offset;\n-\tif (limit == 0 || state.current_offset >= max_element) {\n-\t\treturn SinkResultType::FINISHED;\n-\t}\n-\tif (!HandleOffset(input, state.current_offset, offset, limit)) {\n-\t\treturn SinkResultType::NEED_MORE_INPUT;\n+\tmax_element = limit + offset;\n+\tif (limit == 0 || current_offset >= max_element) {\n+\t\treturn false;\n \t}\n+\treturn true;\n+}\n+\n+SinkResultType PhysicalLimit::Sink(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate,\n+                                   DataChunk &input) const {\n+\n+\tD_ASSERT(input.size() > 0);\n+\tauto &state = (LimitLocalState &)lstate;\n+\tauto &limit = state.limit;\n+\tauto &offset = state.offset;\n \n-\tstate.data.Append(input);\n+\tidx_t max_element;\n+\tif (!ComputeOffset(input, limit, offset, state.current_offset, max_element, limit_expression.get(),\n+\t                   offset_expression.get())) {\n+\t\treturn SinkResultType::FINISHED;\n+\t}\n+\tstate.data.Append(input, lstate.batch_index);\n+\tstate.current_offset += input.size();\n \treturn SinkResultType::NEED_MORE_INPUT;\n }\n \n+void PhysicalLimit::Combine(ExecutionContext &context, GlobalSinkState &gstate_p, LocalSinkState &lstate_p) const {\n+\tauto &gstate = (LimitGlobalState &)gstate_p;\n+\tauto &state = (LimitLocalState &)lstate_p;\n+\n+\tlock_guard<mutex> lock(gstate.glock);\n+\tgstate.limit = state.limit;\n+\tgstate.offset = state.offset;\n+\tgstate.data.Merge(state.data);\n+}\n+\n //===--------------------------------------------------------------------===//\n // Source\n //===--------------------------------------------------------------------===//\n-class LimitOperatorState : public GlobalSourceState {\n+class LimitSourceState : public GlobalSourceState {\n public:\n-\tLimitOperatorState() : chunk_idx(0) {\n+\tLimitSourceState() {\n+\t\tinitialized = false;\n+\t\tcurrent_offset = 0;\n \t}\n \n-\tidx_t chunk_idx;\n+\tbool initialized;\n+\tidx_t current_offset;\n+\tBatchedChunkScanState scan_state;\n };\n \n unique_ptr<GlobalSourceState> PhysicalLimit::GetGlobalSourceState(ClientContext &context) const {\n-\treturn make_unique<LimitOperatorState>();\n+\treturn make_unique<LimitSourceState>();\n }\n \n void PhysicalLimit::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,\n                             LocalSourceState &lstate) const {\n \tauto &gstate = (LimitGlobalState &)*sink_state;\n-\tauto &state = (LimitOperatorState &)gstate_p;\n-\tif (state.chunk_idx >= gstate.data.ChunkCount()) {\n-\t\treturn;\n+\tauto &state = (LimitSourceState &)gstate_p;\n+\twhile (state.current_offset < gstate.limit + gstate.offset) {\n+\t\tif (!state.initialized) {\n+\t\t\tgstate.data.InitializeScan(state.scan_state);\n+\t\t\tstate.initialized = true;\n+\t\t}\n+\t\tgstate.data.Scan(state.scan_state, chunk);\n+\t\tif (chunk.size() == 0) {\n+\t\t\tbreak;\n+\t\t}\n+\t\tif (HandleOffset(chunk, state.current_offset, gstate.offset, gstate.limit)) {\n+\t\t\tbreak;\n+\t\t}\n \t}\n-\tchunk.Reference(gstate.data.GetChunk(state.chunk_idx));\n-\tstate.chunk_idx++;\n }\n \n bool PhysicalLimit::HandleOffset(DataChunk &input, idx_t &current_offset, idx_t offset, idx_t limit) {\ndiff --git a/src/execution/operator/helper/physical_materialized_collector.cpp b/src/execution/operator/helper/physical_materialized_collector.cpp\nnew file mode 100644\nindex 000000000000..e06ea66d0e8a\n--- /dev/null\n+++ b/src/execution/operator/helper/physical_materialized_collector.cpp\n@@ -0,0 +1,46 @@\n+#include \"duckdb/execution/operator/helper/physical_materialized_collector.hpp\"\n+#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/main/materialized_query_result.hpp\"\n+#include \"duckdb/main/client_context.hpp\"\n+\n+namespace duckdb {\n+\n+PhysicalMaterializedCollector::PhysicalMaterializedCollector(PreparedStatementData &data, bool parallel)\n+    : PhysicalResultCollector(data), parallel(parallel) {\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Sink\n+//===--------------------------------------------------------------------===//\n+class MaterializedCollectorGlobalState : public GlobalSinkState {\n+public:\n+\tmutex glock;\n+\tunique_ptr<MaterializedQueryResult> result;\n+};\n+\n+SinkResultType PhysicalMaterializedCollector::Sink(ExecutionContext &context, GlobalSinkState &gstate_p,\n+                                                   LocalSinkState &lstate, DataChunk &input) const {\n+\tauto &gstate = (MaterializedCollectorGlobalState &)gstate_p;\n+\tlock_guard<mutex> lock(gstate.glock);\n+\tgstate.result->collection.Append(input);\n+\treturn SinkResultType::NEED_MORE_INPUT;\n+}\n+\n+unique_ptr<GlobalSinkState> PhysicalMaterializedCollector::GetGlobalSinkState(ClientContext &context) const {\n+\tauto state = make_unique<MaterializedCollectorGlobalState>();\n+\tstate->result =\n+\t    make_unique<MaterializedQueryResult>(statement_type, properties, types, names, context.shared_from_this());\n+\treturn move(state);\n+}\n+\n+unique_ptr<QueryResult> PhysicalMaterializedCollector::GetResult(GlobalSinkState &state) {\n+\tauto &gstate = (MaterializedCollectorGlobalState &)state;\n+\tD_ASSERT(gstate.result);\n+\treturn move(gstate.result);\n+}\n+\n+bool PhysicalMaterializedCollector::ParallelSink() const {\n+\treturn parallel;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/execution/operator/helper/physical_result_collector.cpp b/src/execution/operator/helper/physical_result_collector.cpp\nnew file mode 100644\nindex 000000000000..760c0cdddd08\n--- /dev/null\n+++ b/src/execution/operator/helper/physical_result_collector.cpp\n@@ -0,0 +1,49 @@\n+#include \"duckdb/execution/operator/helper/physical_result_collector.hpp\"\n+#include \"duckdb/execution/operator/helper/physical_materialized_collector.hpp\"\n+#include \"duckdb/execution/operator/helper/physical_batch_collector.hpp\"\n+#include \"duckdb/main/prepared_statement_data.hpp\"\n+#include \"duckdb/main/config.hpp\"\n+#include \"duckdb/parallel/pipeline.hpp\"\n+\n+namespace duckdb {\n+\n+PhysicalResultCollector::PhysicalResultCollector(PreparedStatementData &data)\n+    : PhysicalOperator(PhysicalOperatorType::RESULT_COLLECTOR, {LogicalType::BOOLEAN}, 0),\n+      statement_type(data.statement_type), properties(data.properties), plan(data.plan.get()), names(data.names) {\n+\tthis->types = data.types;\n+}\n+\n+unique_ptr<PhysicalResultCollector> PhysicalResultCollector::GetResultCollector(ClientContext &context,\n+                                                                                PreparedStatementData &data) {\n+\tauto &config = DBConfig::GetConfig(context);\n+\tbool use_materialized_collector = !config.preserve_insertion_order || !data.plan->AllSourcesSupportBatchIndex();\n+\tif (use_materialized_collector) {\n+\t\t// parallel materialized collector only if we don't care about maintaining insertion order\n+\t\treturn make_unique_base<PhysicalResultCollector, PhysicalMaterializedCollector>(\n+\t\t    data, !config.preserve_insertion_order);\n+\t} else {\n+\t\t// we care about maintaining insertion order and the sources all support batch indexes\n+\t\t// use a batch collector\n+\t\treturn make_unique_base<PhysicalResultCollector, PhysicalBatchCollector>(data);\n+\t}\n+}\n+\n+vector<PhysicalOperator *> PhysicalResultCollector::GetChildren() const {\n+\treturn {plan};\n+}\n+\n+void PhysicalResultCollector::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\t// operator is a sink, build a pipeline\n+\tsink_state.reset();\n+\n+\t// single operator:\n+\t// the operator becomes the data source of the current pipeline\n+\tstate.SetPipelineSource(current, this);\n+\t// we create a new pipeline starting from the child\n+\tD_ASSERT(children.size() == 0);\n+\tD_ASSERT(plan);\n+\n+\tBuildChildPipeline(executor, current, state, plan);\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/execution/operator/helper/physical_streaming_limit.cpp b/src/execution/operator/helper/physical_streaming_limit.cpp\nnew file mode 100644\nindex 000000000000..13c8931e6f18\n--- /dev/null\n+++ b/src/execution/operator/helper/physical_streaming_limit.cpp\n@@ -0,0 +1,71 @@\n+#include \"duckdb/execution/operator/helper/physical_streaming_limit.hpp\"\n+#include \"duckdb/execution/operator/helper/physical_limit.hpp\"\n+\n+namespace duckdb {\n+\n+PhysicalStreamingLimit::PhysicalStreamingLimit(vector<LogicalType> types, idx_t limit, idx_t offset,\n+                                               unique_ptr<Expression> limit_expression,\n+                                               unique_ptr<Expression> offset_expression, idx_t estimated_cardinality,\n+                                               bool parallel)\n+    : PhysicalOperator(PhysicalOperatorType::STREAMING_LIMIT, move(types), estimated_cardinality), limit_value(limit),\n+      offset_value(offset), limit_expression(move(limit_expression)), offset_expression(move(offset_expression)),\n+      parallel(parallel) {\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Operator\n+//===--------------------------------------------------------------------===//\n+class StreamingLimitOperatorState : public OperatorState {\n+public:\n+\texplicit StreamingLimitOperatorState(const PhysicalStreamingLimit &op) {\n+\t\tthis->limit = op.limit_expression ? DConstants::INVALID_INDEX : op.limit_value;\n+\t\tthis->offset = op.offset_expression ? DConstants::INVALID_INDEX : op.offset_value;\n+\t}\n+\n+\tidx_t limit;\n+\tidx_t offset;\n+};\n+\n+class StreamingLimitGlobalState : public GlobalOperatorState {\n+public:\n+\tStreamingLimitGlobalState() : current_offset(0) {\n+\t}\n+\n+\tstd::atomic<idx_t> current_offset;\n+};\n+\n+unique_ptr<OperatorState> PhysicalStreamingLimit::GetOperatorState(ClientContext &context) const {\n+\treturn make_unique<StreamingLimitOperatorState>(*this);\n+}\n+\n+unique_ptr<GlobalOperatorState> PhysicalStreamingLimit::GetGlobalOperatorState(ClientContext &context) const {\n+\treturn make_unique<StreamingLimitGlobalState>();\n+}\n+\n+OperatorResultType PhysicalStreamingLimit::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n+                                                   GlobalOperatorState &gstate_p, OperatorState &state_p) const {\n+\tauto &gstate = (StreamingLimitGlobalState &)gstate_p;\n+\tauto &state = (StreamingLimitOperatorState &)state_p;\n+\tauto &limit = state.limit;\n+\tauto &offset = state.offset;\n+\tidx_t current_offset = gstate.current_offset.fetch_add(input.size());\n+\tidx_t max_element;\n+\tif (!PhysicalLimit::ComputeOffset(input, limit, offset, current_offset, max_element, limit_expression.get(),\n+\t                                  offset_expression.get())) {\n+\t\treturn OperatorResultType::FINISHED;\n+\t}\n+\tif (PhysicalLimit::HandleOffset(input, current_offset, offset, limit)) {\n+\t\tchunk.Reference(input);\n+\t}\n+\treturn OperatorResultType::NEED_MORE_INPUT;\n+}\n+\n+bool PhysicalStreamingLimit::IsOrderDependent() const {\n+\treturn !parallel;\n+}\n+\n+bool PhysicalStreamingLimit::ParallelOperator() const {\n+\treturn parallel;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/execution/operator/join/physical_cross_product.cpp b/src/execution/operator/join/physical_cross_product.cpp\nindex 43a4d4ddc917..5442aac3cc06 100644\n--- a/src/execution/operator/join/physical_cross_product.cpp\n+++ b/src/execution/operator/join/physical_cross_product.cpp\n@@ -1,6 +1,7 @@\n #include \"duckdb/execution/operator/join/physical_cross_product.hpp\"\n \n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n+#include \"duckdb/execution/operator/join/physical_join.hpp\"\n \n namespace duckdb {\n \n@@ -88,4 +89,15 @@ OperatorResultType PhysicalCrossProduct::Execute(ExecutionContext &context, Data\n \treturn OperatorResultType::HAVE_MORE_OUTPUT;\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalCrossProduct::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\tPhysicalJoin::BuildJoinPipelines(executor, current, state, *this);\n+}\n+\n+vector<const PhysicalOperator *> PhysicalCrossProduct::GetSources() const {\n+\treturn children[0]->GetSources();\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/join/physical_delim_join.cpp b/src/execution/operator/join/physical_delim_join.cpp\nindex d7adb7b976e1..9e0616175f47 100644\n--- a/src/execution/operator/join/physical_delim_join.cpp\n+++ b/src/execution/operator/join/physical_delim_join.cpp\n@@ -3,7 +3,9 @@\n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n #include \"duckdb/execution/operator/aggregate/physical_hash_aggregate.hpp\"\n+#include \"duckdb/execution/operator/set/physical_recursive_cte.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n+#include \"duckdb/parallel/pipeline.hpp\"\n \n namespace duckdb {\n \n@@ -23,6 +25,16 @@ PhysicalDelimJoin::PhysicalDelimJoin(vector<LogicalType> types, unique_ptr<Physi\n \tjoin->children[0] = move(cached_chunk_scan);\n }\n \n+vector<PhysicalOperator *> PhysicalDelimJoin::GetChildren() const {\n+\tvector<PhysicalOperator *> result;\n+\tfor (auto &child : children) {\n+\t\tresult.push_back(child.get());\n+\t}\n+\tresult.push_back(join.get());\n+\tresult.push_back(distinct.get());\n+\treturn result;\n+}\n+\n //===--------------------------------------------------------------------===//\n // Sink\n //===--------------------------------------------------------------------===//\n@@ -96,4 +108,38 @@ string PhysicalDelimJoin::ParamsToString() const {\n \treturn join->ParamsToString();\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalDelimJoin::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\top_state.reset();\n+\tsink_state.reset();\n+\n+\t// duplicate eliminated join\n+\tauto pipeline = make_shared<Pipeline>(executor);\n+\tstate.SetPipelineSink(*pipeline, this);\n+\tcurrent.AddDependency(pipeline);\n+\n+\t// recurse into the pipeline child\n+\tchildren[0]->BuildPipelines(executor, *pipeline, state);\n+\tif (type == PhysicalOperatorType::DELIM_JOIN) {\n+\t\t// recurse into the actual join\n+\t\t// any pipelines in there depend on the main pipeline\n+\t\t// any scan of the duplicate eliminated data on the RHS depends on this pipeline\n+\t\t// we add an entry to the mapping of (PhysicalOperator*) -> (Pipeline*)\n+\t\tfor (auto &delim_scan : delim_scans) {\n+\t\t\tstate.delim_join_dependencies[delim_scan] = pipeline.get();\n+\t\t}\n+\t\tjoin->BuildPipelines(executor, current, state);\n+\t}\n+\tif (!state.recursive_cte) {\n+\t\t// regular pipeline: schedule it\n+\t\tstate.AddPipeline(executor, move(pipeline));\n+\t} else {\n+\t\t// CTE pipeline! add it to the CTE pipelines\n+\t\tauto &cte = (PhysicalRecursiveCTE &)*state.recursive_cte;\n+\t\tcte.pipelines.push_back(move(pipeline));\n+\t}\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/join/physical_iejoin.cpp b/src/execution/operator/join/physical_iejoin.cpp\nindex a9e8b10c2c4f..3e01dc26303d 100644\n--- a/src/execution/operator/join/physical_iejoin.cpp\n+++ b/src/execution/operator/join/physical_iejoin.cpp\n@@ -1101,4 +1101,36 @@ void PhysicalIEJoin::GetData(ExecutionContext &context, DataChunk &result, Globa\n \t}\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalIEJoin::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\tD_ASSERT(children.size() == 2);\n+\tif (state.recursive_cte) {\n+\t\tthrow NotImplementedException(\"IEJoins are not supported in recursive CTEs yet\");\n+\t}\n+\n+\t// Build the LHS\n+\tauto lhs_pipeline = make_shared<Pipeline>(executor);\n+\tstate.SetPipelineSink(*lhs_pipeline, this);\n+\tD_ASSERT(children[0].get());\n+\tchildren[0]->BuildPipelines(executor, *lhs_pipeline, state);\n+\n+\t// Build the RHS\n+\tauto rhs_pipeline = make_shared<Pipeline>(executor);\n+\tstate.SetPipelineSink(*rhs_pipeline, this);\n+\tD_ASSERT(children[1].get());\n+\tchildren[1]->BuildPipelines(executor, *rhs_pipeline, state);\n+\n+\t// RHS => LHS => current\n+\tcurrent.AddDependency(rhs_pipeline);\n+\trhs_pipeline->AddDependency(lhs_pipeline);\n+\n+\tstate.AddPipeline(executor, move(lhs_pipeline));\n+\tstate.AddPipeline(executor, move(rhs_pipeline));\n+\n+\t// Now build both and scan\n+\tstate.SetPipelineSource(current, this);\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/join/physical_index_join.cpp b/src/execution/operator/join/physical_index_join.cpp\nindex 804f709c3d11..5a7091477ec9 100644\n--- a/src/execution/operator/join/physical_index_join.cpp\n+++ b/src/execution/operator/join/physical_index_join.cpp\n@@ -199,4 +199,19 @@ OperatorResultType PhysicalIndexJoin::Execute(ExecutionContext &context, DataChu\n \treturn OperatorResultType::HAVE_MORE_OUTPUT;\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalIndexJoin::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\t// index join: we only continue into the LHS\n+\t// the right side is probed by the index join\n+\t// so we don't need to do anything in the pipeline with this child\n+\tstate.AddPipelineOperator(current, this);\n+\tchildren[0]->BuildPipelines(executor, current, state);\n+}\n+\n+vector<const PhysicalOperator *> PhysicalIndexJoin::GetSources() const {\n+\treturn children[0]->GetSources();\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/join/physical_join.cpp b/src/execution/operator/join/physical_join.cpp\nindex b36c2bef6828..71bd0583c995 100644\n--- a/src/execution/operator/join/physical_join.cpp\n+++ b/src/execution/operator/join/physical_join.cpp\n@@ -1,4 +1,5 @@\n #include \"duckdb/execution/operator/join/physical_join.hpp\"\n+#include \"duckdb/parallel/pipeline.hpp\"\n \n namespace duckdb {\n \n@@ -19,4 +20,42 @@ bool PhysicalJoin::EmptyResultIfRHSIsEmpty() const {\n \t}\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalJoin::BuildJoinPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state,\n+                                      PhysicalOperator &op) {\n+\top.op_state.reset();\n+\top.sink_state.reset();\n+\n+\t// on the LHS (probe child), the operator becomes a regular operator\n+\tstate.AddPipelineOperator(current, &op);\n+\tif (op.IsSource()) {\n+\t\t// FULL or RIGHT outer join\n+\t\t// schedule a scan of the node as a child pipeline\n+\t\t// this scan has to be performed AFTER all the probing has happened\n+\t\tif (state.recursive_cte) {\n+\t\t\tthrow NotImplementedException(\"FULL and RIGHT outer joins are not supported in recursive CTEs yet\");\n+\t\t}\n+\t\tstate.AddChildPipeline(executor, current);\n+\t}\n+\t// continue building the pipeline on this child\n+\top.children[0]->BuildPipelines(executor, current, state);\n+\n+\t// on the RHS (build side), we construct a new child pipeline with this pipeline as its source\n+\top.BuildChildPipeline(executor, current, state, op.children[1].get());\n+}\n+\n+void PhysicalJoin::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\tPhysicalJoin::BuildJoinPipelines(executor, current, state, *this);\n+}\n+\n+vector<const PhysicalOperator *> PhysicalJoin::GetSources() const {\n+\tauto result = children[0]->GetSources();\n+\tif (IsSource()) {\n+\t\tresult.push_back(this);\n+\t}\n+\treturn result;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/persistent/physical_export.cpp b/src/execution/operator/persistent/physical_export.cpp\nindex cd5bd671d0df..23ab136366d7 100644\n--- a/src/execution/operator/persistent/physical_export.cpp\n+++ b/src/execution/operator/persistent/physical_export.cpp\n@@ -5,6 +5,7 @@\n #include \"duckdb/catalog/catalog_entry/schema_catalog_entry.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/parser/keyword_helper.hpp\"\n+#include \"duckdb/parallel/pipeline.hpp\"\n \n #include <algorithm>\n #include <sstream>\n@@ -170,4 +171,21 @@ SinkResultType PhysicalExport::Sink(ExecutionContext &context, GlobalSinkState &\n \treturn SinkResultType::NEED_MORE_INPUT;\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalExport::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\t// EXPORT has an optional child\n+\t// we only need to schedule child pipelines if there is a child\n+\tstate.SetPipelineSource(current, this);\n+\tif (children.empty()) {\n+\t\treturn;\n+\t}\n+\tPhysicalOperator::BuildPipelines(executor, current, state);\n+}\n+\n+vector<const PhysicalOperator *> PhysicalExport::GetSources() const {\n+\treturn {this};\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/scan/physical_chunk_scan.cpp b/src/execution/operator/scan/physical_chunk_scan.cpp\nindex f2bfddc6c452..3bf2289b2d0b 100644\n--- a/src/execution/operator/scan/physical_chunk_scan.cpp\n+++ b/src/execution/operator/scan/physical_chunk_scan.cpp\n@@ -1,4 +1,6 @@\n #include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n+#include \"duckdb/parallel/pipeline.hpp\"\n+#include \"duckdb/execution/operator/join/physical_delim_join.hpp\"\n \n namespace duckdb {\n \n@@ -31,4 +33,36 @@ void PhysicalChunkScan::GetData(ExecutionContext &context, DataChunk &chunk, Glo\n \tstate.chunk_index++;\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalChunkScan::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\t// check if there is any additional action we need to do depending on the type\n+\tswitch (type) {\n+\tcase PhysicalOperatorType::DELIM_SCAN: {\n+\t\tauto entry = state.delim_join_dependencies.find(this);\n+\t\tD_ASSERT(entry != state.delim_join_dependencies.end());\n+\t\t// this chunk scan introduces a dependency to the current pipeline\n+\t\t// namely a dependency on the duplicate elimination pipeline to finish\n+\t\tauto delim_dependency = entry->second->shared_from_this();\n+\t\tauto delim_sink = state.GetPipelineSink(*delim_dependency);\n+\t\tD_ASSERT(delim_sink);\n+\t\tD_ASSERT(delim_sink->type == PhysicalOperatorType::DELIM_JOIN);\n+\t\tauto &delim_join = (PhysicalDelimJoin &)*delim_sink;\n+\t\tcurrent.AddDependency(delim_dependency);\n+\t\tstate.SetPipelineSource(current, (PhysicalOperator *)delim_join.distinct.get());\n+\t\treturn;\n+\t}\n+\tcase PhysicalOperatorType::RECURSIVE_CTE_SCAN:\n+\t\tif (!state.recursive_cte) {\n+\t\t\tthrow InternalException(\"Recursive CTE scan found without recursive CTE node\");\n+\t\t}\n+\t\tbreak;\n+\tdefault:\n+\t\tbreak;\n+\t}\n+\tD_ASSERT(children.empty());\n+\tstate.SetPipelineSource(current, this);\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/scan/physical_table_scan.cpp b/src/execution/operator/scan/physical_table_scan.cpp\nindex 3e69a79ff3fd..e97ffb7d225b 100644\n--- a/src/execution/operator/scan/physical_table_scan.cpp\n+++ b/src/execution/operator/scan/physical_table_scan.cpp\n@@ -113,6 +113,16 @@ void PhysicalTableScan::GetData(ExecutionContext &context, DataChunk &chunk, Glo\n \t}\n }\n \n+idx_t PhysicalTableScan::GetBatchIndex(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,\n+                                       LocalSourceState &lstate) const {\n+\tD_ASSERT(SupportsBatchIndex());\n+\tD_ASSERT(function.get_batch_index);\n+\tauto &gstate = (TableScanGlobalState &)gstate_p;\n+\tauto &state = (TableScanLocalState &)lstate;\n+\treturn function.get_batch_index(context.client, bind_data.get(), state.operator_data.get(),\n+\t                                gstate.parallel_state.get());\n+}\n+\n string PhysicalTableScan::GetName() const {\n \treturn StringUtil::Upper(function.name);\n }\ndiff --git a/src/execution/operator/set/physical_recursive_cte.cpp b/src/execution/operator/set/physical_recursive_cte.cpp\nindex a7809b99d206..cca5081b78d5 100644\n--- a/src/execution/operator/set/physical_recursive_cte.cpp\n+++ b/src/execution/operator/set/physical_recursive_cte.cpp\n@@ -9,6 +9,7 @@\n #include \"duckdb/parallel/task_scheduler.hpp\"\n #include \"duckdb/execution/executor.hpp\"\n #include \"duckdb/parallel/event.hpp\"\n+#include \"duckdb/parallel/pipeline.hpp\"\n \n namespace duckdb {\n \n@@ -146,4 +147,38 @@ void PhysicalRecursiveCTE::ExecuteRecursivePipelines(ExecutionContext &context)\n \t}\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalRecursiveCTE::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\top_state.reset();\n+\tsink_state.reset();\n+\n+\t// recursive CTE\n+\tstate.SetPipelineSource(current, this);\n+\t// the LHS of the recursive CTE is our initial state\n+\t// we build this pipeline as normal\n+\tauto pipeline_child = children[0].get();\n+\t// for the RHS, we gather all pipelines that depend on the recursive cte\n+\t// these pipelines need to be rerun\n+\tif (state.recursive_cte) {\n+\t\tthrow InternalException(\"Recursive CTE detected WITHIN a recursive CTE node\");\n+\t}\n+\tstate.recursive_cte = this;\n+\n+\tauto recursive_pipeline = make_shared<Pipeline>(executor);\n+\tstate.SetPipelineSink(*recursive_pipeline, this);\n+\tchildren[1]->BuildPipelines(executor, *recursive_pipeline, state);\n+\n+\tpipelines.push_back(move(recursive_pipeline));\n+\n+\tstate.recursive_cte = nullptr;\n+\n+\tBuildChildPipeline(executor, current, state, pipeline_child);\n+}\n+\n+vector<const PhysicalOperator *> PhysicalRecursiveCTE::GetSources() const {\n+\treturn {this};\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/operator/set/physical_union.cpp b/src/execution/operator/set/physical_union.cpp\nindex 395174435ca6..b6858755b4e7 100644\n--- a/src/execution/operator/set/physical_union.cpp\n+++ b/src/execution/operator/set/physical_union.cpp\n@@ -1,5 +1,6 @@\n #include \"duckdb/execution/operator/set/physical_union.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n+#include \"duckdb/parallel/pipeline.hpp\"\n \n namespace duckdb {\n \n@@ -10,4 +11,47 @@ PhysicalUnion::PhysicalUnion(vector<LogicalType> types, unique_ptr<PhysicalOpera\n \tchildren.push_back(move(bottom));\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalUnion::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\tif (state.recursive_cte) {\n+\t\tthrow NotImplementedException(\"UNIONS are not supported in recursive CTEs yet\");\n+\t}\n+\top_state.reset();\n+\tsink_state.reset();\n+\n+\tauto union_pipeline = make_shared<Pipeline>(executor);\n+\tauto pipeline_ptr = union_pipeline.get();\n+\tauto &child_pipelines = state.GetChildPipelines(executor);\n+\tauto &child_dependencies = state.GetChildDependencies(executor);\n+\tauto &union_pipelines = state.GetUnionPipelines(executor);\n+\t// set up dependencies for any child pipelines to this union pipeline\n+\tauto child_entry = child_pipelines.find(&current);\n+\tif (child_entry != child_pipelines.end()) {\n+\t\tfor (auto &current_child : child_entry->second) {\n+\t\t\tD_ASSERT(child_dependencies.find(current_child.get()) != child_dependencies.end());\n+\t\t\tchild_dependencies[current_child.get()].push_back(pipeline_ptr);\n+\t\t}\n+\t}\n+\t// for the current pipeline, continue building on the LHS\n+\tstate.SetPipelineOperators(*union_pipeline, state.GetPipelineOperators(current));\n+\tchildren[0]->BuildPipelines(executor, current, state);\n+\t// insert the union pipeline as a union pipeline of the current node\n+\tunion_pipelines[&current].push_back(move(union_pipeline));\n+\n+\t// for the union pipeline, build on the RHS\n+\tstate.SetPipelineSink(*pipeline_ptr, state.GetPipelineSink(current));\n+\tchildren[1]->BuildPipelines(executor, *pipeline_ptr, state);\n+}\n+\n+vector<const PhysicalOperator *> PhysicalUnion::GetSources() const {\n+\tvector<const PhysicalOperator *> result;\n+\tfor (auto &child : children) {\n+\t\tauto child_sources = child->GetSources();\n+\t\tresult.insert(result.end(), child_sources.begin(), child_sources.end());\n+\t}\n+\treturn result;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/physical_operator.cpp b/src/execution/physical_operator.cpp\nindex 43b7579839a8..47860f5046e0 100644\n--- a/src/execution/physical_operator.cpp\n+++ b/src/execution/physical_operator.cpp\n@@ -6,7 +6,8 @@\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n #include \"duckdb/common/tree_renderer.hpp\"\n-#include \"duckdb/common/enums/operator_result_type.hpp\"\n+#include \"duckdb/parallel/pipeline.hpp\"\n+#include \"duckdb/execution/operator/set/physical_recursive_cte.hpp\"\n \n namespace duckdb {\n \n@@ -25,6 +26,14 @@ void PhysicalOperator::Print() const {\n }\n // LCOV_EXCL_STOP\n \n+vector<PhysicalOperator *> PhysicalOperator::GetChildren() const {\n+\tvector<PhysicalOperator *> result;\n+\tfor (auto &child : children) {\n+\t\tresult.push_back(child.get());\n+\t}\n+\treturn result;\n+}\n+\n //===--------------------------------------------------------------------===//\n // Operator\n //===--------------------------------------------------------------------===//\n@@ -60,6 +69,11 @@ void PhysicalOperator::GetData(ExecutionContext &context, DataChunk &chunk, Glob\n                                LocalSourceState &lstate) const {\n \tthrow InternalException(\"Calling GetData on a node that is not a source!\");\n }\n+\n+idx_t PhysicalOperator::GetBatchIndex(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n+                                      LocalSourceState &lstate) const {\n+\tthrow InternalException(\"Calling GetBatchIndex on a node that does not support it\");\n+}\n // LCOV_EXCL_STOP\n \n //===--------------------------------------------------------------------===//\n@@ -88,4 +102,97 @@ unique_ptr<GlobalSinkState> PhysicalOperator::GetGlobalSinkState(ClientContext &\n \treturn make_unique<GlobalSinkState>();\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Construction\n+//===--------------------------------------------------------------------===//\n+void PhysicalOperator::AddPipeline(Executor &executor, shared_ptr<Pipeline> pipeline, PipelineBuildState &state) {\n+\tif (!state.recursive_cte) {\n+\t\t// regular pipeline: schedule it\n+\t\tstate.AddPipeline(executor, move(pipeline));\n+\t} else {\n+\t\t// CTE pipeline! add it to the CTE pipelines\n+\t\tauto &cte = (PhysicalRecursiveCTE &)*state.recursive_cte;\n+\t\tcte.pipelines.push_back(move(pipeline));\n+\t}\n+}\n+\n+void PhysicalOperator::BuildChildPipeline(Executor &executor, Pipeline &current, PipelineBuildState &state,\n+                                          PhysicalOperator *pipeline_child) {\n+\tauto pipeline = make_shared<Pipeline>(executor);\n+\tstate.SetPipelineSink(*pipeline, this);\n+\t// the current is dependent on this pipeline to complete\n+\tcurrent.AddDependency(pipeline);\n+\t// recurse into the pipeline child\n+\tpipeline_child->BuildPipelines(executor, *pipeline, state);\n+\tAddPipeline(executor, move(pipeline), state);\n+}\n+\n+void PhysicalOperator::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+\top_state.reset();\n+\tif (IsSink()) {\n+\t\t// operator is a sink, build a pipeline\n+\t\tsink_state.reset();\n+\n+\t\t// single operator:\n+\t\t// the operator becomes the data source of the current pipeline\n+\t\tstate.SetPipelineSource(current, this);\n+\t\t// we create a new pipeline starting from the child\n+\t\tD_ASSERT(children.size() == 1);\n+\n+\t\tBuildChildPipeline(executor, current, state, children[0].get());\n+\t} else {\n+\t\t// operator is not a sink! recurse in children\n+\t\tif (children.empty()) {\n+\t\t\t// source\n+\t\t\tstate.SetPipelineSource(current, this);\n+\t\t} else {\n+\t\t\tif (children.size() != 1) {\n+\t\t\t\tthrow InternalException(\"Operator not supported in BuildPipelines\");\n+\t\t\t}\n+\t\t\tstate.AddPipelineOperator(current, this);\n+\t\t\tchildren[0]->BuildPipelines(executor, current, state);\n+\t\t}\n+\t}\n+}\n+\n+vector<const PhysicalOperator *> PhysicalOperator::GetSources() const {\n+\tvector<const PhysicalOperator *> result;\n+\tif (IsSink()) {\n+\t\tD_ASSERT(children.size() == 1);\n+\t\tresult.push_back(this);\n+\t\treturn result;\n+\t} else {\n+\t\tif (children.empty()) {\n+\t\t\t// source\n+\t\t\tresult.push_back(this);\n+\t\t\treturn result;\n+\t\t} else {\n+\t\t\tif (children.size() != 1) {\n+\t\t\t\tthrow InternalException(\"Operator not supported in GetSource\");\n+\t\t\t}\n+\t\t\treturn children[0]->GetSources();\n+\t\t}\n+\t}\n+}\n+\n+bool PhysicalOperator::AllSourcesSupportBatchIndex() const {\n+\tauto sources = GetSources();\n+\tfor (auto &source : sources) {\n+\t\tif (!source->SupportsBatchIndex()) {\n+\t\t\treturn false;\n+\t\t}\n+\t}\n+\treturn true;\n+}\n+\n+void PhysicalOperator::Verify() {\n+#ifdef DEBUG\n+\tauto sources = GetSources();\n+\tD_ASSERT(!sources.empty());\n+\tfor (auto &child : children) {\n+\t\tchild->Verify();\n+\t}\n+#endif\n+}\n+\n } // namespace duckdb\ndiff --git a/src/execution/physical_plan/plan_limit.cpp b/src/execution/physical_plan/plan_limit.cpp\nindex 3cbe365e4ccd..93170bbf7502 100644\n--- a/src/execution/physical_plan/plan_limit.cpp\n+++ b/src/execution/physical_plan/plan_limit.cpp\n@@ -1,6 +1,8 @@\n #include \"duckdb/execution/operator/helper/physical_limit.hpp\"\n+#include \"duckdb/execution/operator/helper/physical_streaming_limit.hpp\"\n #include \"duckdb/execution/physical_plan_generator.hpp\"\n #include \"duckdb/planner/operator/logical_limit.hpp\"\n+#include \"duckdb/main/config.hpp\"\n \n namespace duckdb {\n \n@@ -8,11 +10,29 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalLimit &op)\n \tD_ASSERT(op.children.size() == 1);\n \n \tauto plan = CreatePlan(*op.children[0]);\n+\tauto &config = DBConfig::GetConfig(context);\n+\tunique_ptr<PhysicalOperator> limit;\n+\tif (!config.preserve_insertion_order) {\n+\t\t// use parallel streaming limit if insertion order is not important\n+\t\tlimit = make_unique<PhysicalStreamingLimit>(op.types, (idx_t)op.limit_val, op.offset_val, move(op.limit),\n+\t\t                                            move(op.offset), op.estimated_cardinality, true);\n+\t} else {\n+\t\t// maintaining insertion order is important\n+\t\tbool all_sources_support_batch_index = plan->AllSourcesSupportBatchIndex();\n+\n+\t\tif (all_sources_support_batch_index) {\n+\t\t\t// source supports batch index: use parallel batch limit\n+\t\t\tlimit = make_unique<PhysicalLimit>(op.types, (idx_t)op.limit_val, op.offset_val, move(op.limit),\n+\t\t\t                                   move(op.offset), op.estimated_cardinality);\n+\t\t} else {\n+\t\t\t// source does not support batch index: use a non-parallel streaming limit\n+\t\t\tlimit = make_unique<PhysicalStreamingLimit>(op.types, (idx_t)op.limit_val, op.offset_val, move(op.limit),\n+\t\t\t                                            move(op.offset), op.estimated_cardinality, false);\n+\t\t}\n+\t}\n \n-\tauto limit = make_unique<PhysicalLimit>(op.types, (idx_t)op.limit_val, op.offset_val, move(op.limit),\n-\t                                        move(op.offset), op.estimated_cardinality);\n \tlimit->children.push_back(move(plan));\n-\treturn move(limit);\n+\treturn limit;\n }\n \n } // namespace duckdb\ndiff --git a/src/execution/physical_plan_generator.cpp b/src/execution/physical_plan_generator.cpp\nindex 531fc3a69590..a843e62066a0 100644\n--- a/src/execution/physical_plan_generator.cpp\n+++ b/src/execution/physical_plan_generator.cpp\n@@ -47,6 +47,8 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(unique_ptr<Logica\n \tprofiler.StartPhase(\"create_plan\");\n \tauto plan = CreatePlan(*op);\n \tprofiler.EndPhase();\n+\n+\tplan->Verify();\n \treturn plan;\n }\n \ndiff --git a/src/function/table/table_scan.cpp b/src/function/table/table_scan.cpp\nindex 7198b6aa08a4..34f002718076 100644\n--- a/src/function/table/table_scan.cpp\n+++ b/src/function/table/table_scan.cpp\n@@ -124,6 +124,19 @@ double TableScanProgress(ClientContext &context, const FunctionData *bind_data_p\n \treturn percentage;\n }\n \n+idx_t TableScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n+                             FunctionOperatorData *operator_state, ParallelState *parallel_state_p) {\n+\tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n+\tauto &state = (TableScanOperatorData &)*operator_state;\n+\tif (state.scan_state.row_group_scan_state.row_group) {\n+\t\treturn state.scan_state.row_group_scan_state.row_group->start;\n+\t}\n+\tif (state.scan_state.local_state.max_index > 0) {\n+\t\treturn bind_data.table->storage->GetTotalRows() + state.scan_state.local_state.chunk_index;\n+\t}\n+\treturn 0;\n+}\n+\n void TableScanDependency(unordered_set<CatalogEntry *> &entries, const FunctionData *bind_data_p) {\n \tauto &bind_data = (const TableScanBindData &)*bind_data_p;\n \tentries.insert(bind_data.table);\n@@ -328,7 +341,9 @@ void TableScanPushdownComplexFilter(ClientContext &context, LogicalGet &get, Fun\n \t\t\t\tget.function.init_parallel_state = nullptr;\n \t\t\t\tget.function.parallel_state_next = nullptr;\n \t\t\t\tget.function.table_scan_progress = nullptr;\n+\t\t\t\tget.function.get_batch_index = nullptr;\n \t\t\t\tget.function.filter_pushdown = false;\n+\t\t\t\tget.function.supports_batch_index = false;\n \t\t\t} else {\n \t\t\t\tbind_data.result_ids.clear();\n \t\t\t}\n@@ -357,8 +372,10 @@ TableFunction TableScanFunction::GetFunction() {\n \tscan_function.parallel_init = TableScanParallelInit;\n \tscan_function.parallel_state_next = TableScanParallelStateNext;\n \tscan_function.table_scan_progress = TableScanProgress;\n+\tscan_function.get_batch_index = TableScanGetBatchIndex;\n \tscan_function.projection_pushdown = true;\n \tscan_function.filter_pushdown = true;\n+\tscan_function.supports_batch_index = true;\n \treturn scan_function;\n }\n \ndiff --git a/src/function/table_function.cpp b/src/function/table_function.cpp\nindex 663a05fe8c0a..dccdf257cd40 100644\n--- a/src/function/table_function.cpp\n+++ b/src/function/table_function.cpp\n@@ -27,7 +27,7 @@ TableFunction::TableFunction(string name, vector<LogicalType> arguments, table_f\n       cardinality(cardinality), pushdown_complex_filter(pushdown_complex_filter), to_string(to_string),\n       max_threads(max_threads), init_parallel_state(init_parallel_state), parallel_function(parallel_function),\n       parallel_init(parallel_init), parallel_state_next(parallel_state_next), table_scan_progress(query_progress),\n-      projection_pushdown(projection_pushdown), filter_pushdown(filter_pushdown) {\n+      projection_pushdown(projection_pushdown), filter_pushdown(filter_pushdown), supports_batch_index(false) {\n }\n \n TableFunction::TableFunction(const vector<LogicalType> &arguments, table_function_t function,\ndiff --git a/src/include/duckdb/common/enums/physical_operator_type.hpp b/src/include/duckdb/common/enums/physical_operator_type.hpp\nindex 2eb5ba0f4b62..e10216b9604a 100644\n--- a/src/include/duckdb/common/enums/physical_operator_type.hpp\n+++ b/src/include/duckdb/common/enums/physical_operator_type.hpp\n@@ -19,6 +19,7 @@ enum class PhysicalOperatorType : uint8_t {\n \tINVALID,\n \tORDER_BY,\n \tLIMIT,\n+\tSTREAMING_LIMIT,\n \tLIMIT_PERCENT,\n \tTOP_N,\n \tWINDOW,\n@@ -93,7 +94,8 @@ enum class PhysicalOperatorType : uint8_t {\n \tEXPORT,\n \tSET,\n \tLOAD,\n-\tINOUT_FUNCTION\n+\tINOUT_FUNCTION,\n+\tRESULT_COLLECTOR\n };\n \n string PhysicalOperatorToString(PhysicalOperatorType type);\ndiff --git a/src/include/duckdb/common/types/batched_chunk_collection.hpp b/src/include/duckdb/common/types/batched_chunk_collection.hpp\nnew file mode 100644\nindex 000000000000..3e6aff2099de\n--- /dev/null\n+++ b/src/include/duckdb/common/types/batched_chunk_collection.hpp\n@@ -0,0 +1,46 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/types/batched_chunk_collection.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/map.hpp\"\n+#include \"duckdb/common/types/chunk_collection.hpp\"\n+\n+namespace duckdb {\n+\n+struct BatchedChunkScanState {\n+\tmap<idx_t, unique_ptr<ChunkCollection>>::iterator iterator;\n+\tidx_t chunk_index;\n+};\n+\n+//!  A BatchedChunkCollection holds a number of data entries that are partitioned by batch index\n+//! Scans over a BatchedChunkCollection are ordered by batch index\n+class BatchedChunkCollection {\n+public:\n+\tDUCKDB_API BatchedChunkCollection();\n+\n+\t//! Appends a datachunk with the given batch index to the batched collection\n+\tDUCKDB_API void Append(DataChunk &input, idx_t batch_index);\n+\n+\t//! Merge the other batched chunk collection into this batched collection\n+\tDUCKDB_API void Merge(BatchedChunkCollection &other);\n+\n+\t//! Initialize a scan over the batched chunk collection\n+\tDUCKDB_API void InitializeScan(BatchedChunkScanState &state);\n+\n+\t//! Scan a chunk from the batched chunk collection, in-order of batch index\n+\tDUCKDB_API void Scan(BatchedChunkScanState &state, DataChunk &output);\n+\n+\tDUCKDB_API string ToString() const;\n+\tDUCKDB_API void Print() const;\n+\n+private:\n+\t//! The data of the batched chunk collection - a set of batch_index -> ChunkCollection pointers\n+\tmap<idx_t, unique_ptr<ChunkCollection>> data;\n+};\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/types/chunk_collection.hpp b/src/include/duckdb/common/types/chunk_collection.hpp\nindex 4e44cb9a4599..ee8e4d7c4e81 100644\n--- a/src/include/duckdb/common/types/chunk_collection.hpp\n+++ b/src/include/duckdb/common/types/chunk_collection.hpp\n@@ -70,11 +70,8 @@ class ChunkCollection {\n \t//! Copy a single cell to a target vector\n \tDUCKDB_API void CopyCell(idx_t column, idx_t index, Vector &target, idx_t target_offset);\n \n-\tDUCKDB_API string ToString() const {\n-\t\treturn chunks.size() == 0 ? \"ChunkCollection [ 0 ]\"\n-\t\t                          : \"ChunkCollection [ \" + std::to_string(count) + \" ]: \\n\" + chunks[0]->ToString();\n-\t}\n-\tDUCKDB_API void Print();\n+\tDUCKDB_API string ToString() const;\n+\tDUCKDB_API void Print() const;\n \n \t//! Gets a reference to the chunk at the given index\n \tDUCKDB_API DataChunk &GetChunkForRow(idx_t row_index) {\ndiff --git a/src/include/duckdb/execution/executor.hpp b/src/include/duckdb/execution/executor.hpp\nindex c78aebaef7a7..7a3e20f415b5 100644\n--- a/src/include/duckdb/execution/executor.hpp\n+++ b/src/include/duckdb/execution/executor.hpp\n@@ -33,6 +33,7 @@ using event_map_t = unordered_map<const Pipeline *, PipelineEventStack>;\n class Executor {\n \tfriend class Pipeline;\n \tfriend class PipelineTask;\n+\tfriend class PipelineBuildState;\n \n public:\n \texplicit Executor(ClientContext &context);\n@@ -44,7 +45,7 @@ class Executor {\n \tstatic Executor &Get(ClientContext &context);\n \n \tvoid Initialize(PhysicalOperator *physical_plan);\n-\tvoid BuildPipelines(PhysicalOperator *op, Pipeline *current);\n+\tvoid Initialize(unique_ptr<PhysicalOperator> physical_plan);\n \n \tvoid CancelTasks();\n \tPendingExecutionResult ExecuteTask();\n@@ -82,7 +83,14 @@ class Executor {\n \n \tvoid ReschedulePipelines(const vector<shared_ptr<Pipeline>> &pipelines, vector<shared_ptr<Event>> &events);\n \n+\t//! Whether or not the root of the pipeline is a result collector object\n+\tbool HasResultCollector();\n+\t//! Returns the query result - can only be used if `HasResultCollector` returns true\n+\tunique_ptr<QueryResult> GetResult();\n+\n private:\n+\tvoid InitializeInternal(PhysicalOperator *physical_plan);\n+\n \tvoid ScheduleEvents();\n \tvoid ScheduleEventsInternal(const vector<shared_ptr<Pipeline>> &pipelines,\n \t                            unordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> &child_pipelines,\n@@ -105,6 +113,7 @@ class Executor {\n \n private:\n \tPhysicalOperator *physical_plan;\n+\tunique_ptr<PhysicalOperator> owned_plan;\n \n \tmutex executor_lock;\n \t//! The pipelines of the current query\n@@ -140,12 +149,6 @@ class Executor {\n \t//! Dependencies of child pipelines\n \tunordered_map<Pipeline *, vector<Pipeline *>> child_dependencies;\n \n-\t//! Duplicate eliminated join scan dependencies\n-\tunordered_map<PhysicalOperator *, Pipeline *> delim_join_dependencies;\n-\n-\t//! Active recursive CTE node (if any)\n-\tPhysicalOperator *recursive_cte;\n-\n \t//! The last pending execution result (if any)\n \tPendingExecutionResult execution_result;\n \t//! The current task in process (if any)\ndiff --git a/src/include/duckdb/execution/operator/helper/physical_batch_collector.hpp b/src/include/duckdb/execution/operator/helper/physical_batch_collector.hpp\nnew file mode 100644\nindex 000000000000..b7976b7b995b\n--- /dev/null\n+++ b/src/include/duckdb/execution/operator/helper/physical_batch_collector.hpp\n@@ -0,0 +1,42 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/execution/operator/helper/physical_batch_collector.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/execution/operator/helper/physical_result_collector.hpp\"\n+\n+namespace duckdb {\n+\n+class PhysicalBatchCollector : public PhysicalResultCollector {\n+public:\n+\tPhysicalBatchCollector(PreparedStatementData &data);\n+\n+public:\n+\tunique_ptr<QueryResult> GetResult(GlobalSinkState &state) override;\n+\n+public:\n+\t// Sink interface\n+\tSinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,\n+\t                    DataChunk &input) const override;\n+\tvoid Combine(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate) const override;\n+\tSinkFinalizeType Finalize(Pipeline &pipeline, Event &event, ClientContext &context,\n+\t                          GlobalSinkState &gstate) const override;\n+\n+\tunique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;\n+\tunique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;\n+\n+\tbool RequiresBatchIndex() const override {\n+\t\treturn true;\n+\t}\n+\n+\tbool ParallelSink() const override {\n+\t\treturn true;\n+\t}\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/helper/physical_execute.hpp b/src/include/duckdb/execution/operator/helper/physical_execute.hpp\nindex 971d0f827149..c673e7d2735a 100644\n--- a/src/include/duckdb/execution/operator/helper/physical_execute.hpp\n+++ b/src/include/duckdb/execution/operator/helper/physical_execute.hpp\n@@ -20,6 +20,12 @@ class PhysicalExecute : public PhysicalOperator {\n \tPhysicalOperator *plan;\n \tunique_ptr<PhysicalOperator> owned_plan;\n \tshared_ptr<PreparedStatementData> prepared;\n+\n+public:\n+\tvector<PhysicalOperator *> GetChildren() const override;\n+\n+public:\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/helper/physical_limit.hpp b/src/include/duckdb/execution/operator/helper/physical_limit.hpp\nindex fc1b2c3ad8ad..60370af9532d 100644\n--- a/src/include/duckdb/execution/operator/helper/physical_limit.hpp\n+++ b/src/include/duckdb/execution/operator/helper/physical_limit.hpp\n@@ -17,16 +17,18 @@ namespace duckdb {\n class PhysicalLimit : public PhysicalOperator {\n public:\n \tPhysicalLimit(vector<LogicalType> types, idx_t limit, idx_t offset, unique_ptr<Expression> limit_expression,\n-\t              unique_ptr<Expression> offset_expression, idx_t estimated_cardinality)\n-\t    : PhysicalOperator(PhysicalOperatorType::LIMIT, move(types), estimated_cardinality), limit_value(limit),\n-\t      offset_value(offset), limit_expression(move(limit_expression)), offset_expression(move(offset_expression)) {\n-\t}\n+\t              unique_ptr<Expression> offset_expression, idx_t estimated_cardinality);\n \n \tidx_t limit_value;\n \tidx_t offset_value;\n \tunique_ptr<Expression> limit_expression;\n \tunique_ptr<Expression> offset_expression;\n \n+public:\n+\tbool IsOrderDependent() const override {\n+\t\treturn true;\n+\t}\n+\n public:\n \t// Source interface\n \tunique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;\n@@ -35,18 +37,27 @@ class PhysicalLimit : public PhysicalOperator {\n \n public:\n \t// Sink Interface\n-\tunique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;\n \tSinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,\n \t                    DataChunk &input) const override;\n+\tvoid Combine(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate) const override;\n+\tunique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;\n+\tunique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;\n \n \tbool IsSink() const override {\n \t\treturn true;\n \t}\n \n-\tbool SinkOrderMatters() const override {\n+\tbool ParallelSink() const override {\n \t\treturn true;\n \t}\n \n+\tbool RequiresBatchIndex() const override {\n+\t\treturn true;\n+\t}\n+\n+public:\n+\tstatic bool ComputeOffset(DataChunk &input, idx_t &limit, idx_t &offset, idx_t current_offset, idx_t &max_element,\n+\t                          Expression *limit_expression, Expression *offset_expression);\n \tstatic bool HandleOffset(DataChunk &input, idx_t &current_offset, idx_t offset, idx_t limit);\n \tstatic Value GetDelimiter(DataChunk &input, Expression *expr);\n };\ndiff --git a/src/include/duckdb/execution/operator/helper/physical_limit_percent.hpp b/src/include/duckdb/execution/operator/helper/physical_limit_percent.hpp\nindex cc1f4c2492cb..6164a5bc58e5 100644\n--- a/src/include/duckdb/execution/operator/helper/physical_limit_percent.hpp\n+++ b/src/include/duckdb/execution/operator/helper/physical_limit_percent.hpp\n@@ -29,6 +29,11 @@ class PhysicalLimitPercent : public PhysicalOperator {\n \tunique_ptr<Expression> limit_expression;\n \tunique_ptr<Expression> offset_expression;\n \n+public:\n+\tbool IsOrderDependent() const override {\n+\t\treturn true;\n+\t}\n+\n public:\n \t// Source interface\n \tunique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;\n@@ -44,10 +49,6 @@ class PhysicalLimitPercent : public PhysicalOperator {\n \tbool IsSink() const override {\n \t\treturn true;\n \t}\n-\n-\tbool SinkOrderMatters() const override {\n-\t\treturn true;\n-\t}\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/helper/physical_materialized_collector.hpp b/src/include/duckdb/execution/operator/helper/physical_materialized_collector.hpp\nnew file mode 100644\nindex 000000000000..468aeaa7335e\n--- /dev/null\n+++ b/src/include/duckdb/execution/operator/helper/physical_materialized_collector.hpp\n@@ -0,0 +1,34 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/execution/operator/helper/physical_materialized_collector.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/execution/operator/helper/physical_result_collector.hpp\"\n+\n+namespace duckdb {\n+\n+class PhysicalMaterializedCollector : public PhysicalResultCollector {\n+public:\n+\tPhysicalMaterializedCollector(PreparedStatementData &data, bool parallel);\n+\n+\tbool parallel;\n+\n+public:\n+\tunique_ptr<QueryResult> GetResult(GlobalSinkState &state) override;\n+\n+public:\n+\t// Sink interface\n+\tSinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,\n+\t                    DataChunk &input) const override;\n+\n+\tunique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;\n+\n+\tbool ParallelSink() const override;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/helper/physical_result_collector.hpp b/src/include/duckdb/execution/operator/helper/physical_result_collector.hpp\nnew file mode 100644\nindex 000000000000..fe1c630bb052\n--- /dev/null\n+++ b/src/include/duckdb/execution/operator/helper/physical_result_collector.hpp\n@@ -0,0 +1,44 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/execution/operator/helper/physical_result_collector.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/execution/physical_operator.hpp\"\n+#include \"duckdb/common/enums/statement_type.hpp\"\n+\n+namespace duckdb {\n+class PreparedStatementData;\n+\n+//! PhysicalResultCollector is an abstract class that is used to generate the final result of a query\n+class PhysicalResultCollector : public PhysicalOperator {\n+public:\n+\tPhysicalResultCollector(PreparedStatementData &data);\n+\n+\tStatementType statement_type;\n+\tStatementProperties properties;\n+\tPhysicalOperator *plan;\n+\tvector<string> names;\n+\n+public:\n+\tstatic unique_ptr<PhysicalResultCollector> GetResultCollector(ClientContext &context, PreparedStatementData &data);\n+\n+public:\n+\t//! The final method used to fetch the query result from this operator\n+\tvirtual unique_ptr<QueryResult> GetResult(GlobalSinkState &state) = 0;\n+\n+\tbool IsSink() const override {\n+\t\treturn true;\n+\t}\n+\n+public:\n+\tvector<PhysicalOperator *> GetChildren() const override;\n+\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/helper/physical_streaming_limit.hpp b/src/include/duckdb/execution/operator/helper/physical_streaming_limit.hpp\nnew file mode 100644\nindex 000000000000..2def2d4c0af0\n--- /dev/null\n+++ b/src/include/duckdb/execution/operator/helper/physical_streaming_limit.hpp\n@@ -0,0 +1,39 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/execution/operator/helper/physical_streaming_limit.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/execution/physical_operator.hpp\"\n+#include \"duckdb/planner/expression.hpp\"\n+\n+namespace duckdb {\n+\n+class PhysicalStreamingLimit : public PhysicalOperator {\n+public:\n+\tPhysicalStreamingLimit(vector<LogicalType> types, idx_t limit, idx_t offset,\n+\t                       unique_ptr<Expression> limit_expression, unique_ptr<Expression> offset_expression,\n+\t                       idx_t estimated_cardinality, bool parallel);\n+\n+\tidx_t limit_value;\n+\tidx_t offset_value;\n+\tunique_ptr<Expression> limit_expression;\n+\tunique_ptr<Expression> offset_expression;\n+\tbool parallel;\n+\n+public:\n+\t// Operator interface\n+\tunique_ptr<OperatorState> GetOperatorState(ClientContext &context) const override;\n+\tunique_ptr<GlobalOperatorState> GetGlobalOperatorState(ClientContext &context) const override;\n+\tOperatorResultType Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n+\t                           GlobalOperatorState &gstate, OperatorState &state) const override;\n+\n+\tbool IsOrderDependent() const override;\n+\tbool ParallelOperator() const override;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/join/physical_cross_product.hpp b/src/include/duckdb/execution/operator/join/physical_cross_product.hpp\nindex 434abdc9e26b..6ce9dd78d11b 100644\n--- a/src/include/duckdb/execution/operator/join/physical_cross_product.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_cross_product.hpp\n@@ -44,6 +44,10 @@ class PhysicalCrossProduct : public PhysicalOperator {\n \tbool ParallelSink() const override {\n \t\treturn true;\n \t}\n+\n+public:\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n+\tvector<const PhysicalOperator *> GetSources() const override;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/join/physical_delim_join.hpp b/src/include/duckdb/execution/operator/join/physical_delim_join.hpp\nindex d990af2ce151..73523c29f896 100644\n--- a/src/include/duckdb/execution/operator/join/physical_delim_join.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_delim_join.hpp\n@@ -25,6 +25,9 @@ class PhysicalDelimJoin : public PhysicalOperator {\n \tunique_ptr<PhysicalHashAggregate> distinct;\n \tvector<PhysicalOperator *> delim_scans;\n \n+public:\n+\tvector<PhysicalOperator *> GetChildren() const override;\n+\n public:\n \tunique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;\n \tunique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;\n@@ -42,6 +45,9 @@ class PhysicalDelimJoin : public PhysicalOperator {\n \t}\n \n \tstring ParamsToString() const override;\n+\n+public:\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/join/physical_iejoin.hpp b/src/include/duckdb/execution/operator/join/physical_iejoin.hpp\nindex ceac82064bc4..a811c8850dd9 100644\n--- a/src/include/duckdb/execution/operator/join/physical_iejoin.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_iejoin.hpp\n@@ -61,6 +61,9 @@ class PhysicalIEJoin : public PhysicalRangeJoin {\n \t\treturn true;\n \t}\n \n+public:\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n+\n private:\n \t// resolve joins that can potentially output N*M elements (INNER, LEFT, FULL)\n \tvoid ResolveComplexJoin(ExecutionContext &context, DataChunk &result, LocalSourceState &state) const;\ndiff --git a/src/include/duckdb/execution/operator/join/physical_index_join.hpp b/src/include/duckdb/execution/operator/join/physical_index_join.hpp\nindex f93b33e0f7fe..402d7104ce3e 100644\n--- a/src/include/duckdb/execution/operator/join/physical_index_join.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_index_join.hpp\n@@ -62,6 +62,10 @@ class PhysicalIndexJoin : public PhysicalOperator {\n \t\treturn true;\n \t}\n \n+public:\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n+\tvector<const PhysicalOperator *> GetSources() const override;\n+\n private:\n \tvoid GetRHSMatches(ExecutionContext &context, DataChunk &input, OperatorState &state_p) const;\n \t//! Fills result chunk\ndiff --git a/src/include/duckdb/execution/operator/join/physical_join.hpp b/src/include/duckdb/execution/operator/join/physical_join.hpp\nindex a74ad949c5d9..520c5698213d 100644\n--- a/src/include/duckdb/execution/operator/join/physical_join.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_join.hpp\n@@ -28,6 +28,12 @@ class PhysicalJoin : public PhysicalOperator {\n \tstatic void ConstructMarkJoinResult(DataChunk &join_keys, DataChunk &left, DataChunk &result, bool found_match[],\n \t                                    bool has_null);\n \tstatic void ConstructLeftJoinResult(DataChunk &left, DataChunk &result, bool found_match[]);\n+\n+public:\n+\tstatic void BuildJoinPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state,\n+\t                               PhysicalOperator &op);\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n+\tvector<const PhysicalOperator *> GetSources() const override;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/persistent/physical_export.hpp b/src/include/duckdb/execution/operator/persistent/physical_export.hpp\nindex 3b813d7bea85..14c156a030e7 100644\n--- a/src/include/duckdb/execution/operator/persistent/physical_export.hpp\n+++ b/src/include/duckdb/execution/operator/persistent/physical_export.hpp\n@@ -49,6 +49,10 @@ class PhysicalExport : public PhysicalOperator {\n \tbool IsSink() const override {\n \t\treturn true;\n \t}\n+\n+public:\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n+\tvector<const PhysicalOperator *> GetSources() const override;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/scan/physical_chunk_scan.hpp b/src/include/duckdb/execution/operator/scan/physical_chunk_scan.hpp\nindex eda51fcb824e..7700e8f7f389 100644\n--- a/src/include/duckdb/execution/operator/scan/physical_chunk_scan.hpp\n+++ b/src/include/duckdb/execution/operator/scan/physical_chunk_scan.hpp\n@@ -29,6 +29,9 @@ class PhysicalChunkScan : public PhysicalOperator {\n \tunique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;\n \tvoid GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n \t             LocalSourceState &lstate) const override;\n+\n+public:\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/scan/physical_table_scan.hpp b/src/include/duckdb/execution/operator/scan/physical_table_scan.hpp\nindex b4efa894547e..e79b6408661c 100644\n--- a/src/include/duckdb/execution/operator/scan/physical_table_scan.hpp\n+++ b/src/include/duckdb/execution/operator/scan/physical_table_scan.hpp\n@@ -45,10 +45,16 @@ class PhysicalTableScan : public PhysicalOperator {\n \tunique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;\n \tvoid GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n \t             LocalSourceState &lstate) const override;\n+\tidx_t GetBatchIndex(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n+\t                    LocalSourceState &lstate) const override;\n \n \tbool ParallelSource() const override {\n \t\treturn true;\n \t}\n+\n+\tbool SupportsBatchIndex() const override {\n+\t\treturn function.supports_batch_index;\n+\t}\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/set/physical_recursive_cte.hpp b/src/include/duckdb/execution/operator/set/physical_recursive_cte.hpp\nindex c5a983c61391..e2f9f05f804c 100644\n--- a/src/include/duckdb/execution/operator/set/physical_recursive_cte.hpp\n+++ b/src/include/duckdb/execution/operator/set/physical_recursive_cte.hpp\n@@ -40,6 +40,11 @@ class PhysicalRecursiveCTE : public PhysicalOperator {\n \t\treturn true;\n \t}\n \n+public:\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n+\n+\tvector<const PhysicalOperator *> GetSources() const override;\n+\n private:\n \t//! Probe Hash Table and eliminate duplicate rows\n \tidx_t ProbeHT(DataChunk &chunk, RecursiveCTEState &state) const;\ndiff --git a/src/include/duckdb/execution/operator/set/physical_union.hpp b/src/include/duckdb/execution/operator/set/physical_union.hpp\nindex 1af92dd3456d..3c0fd72615bb 100644\n--- a/src/include/duckdb/execution/operator/set/physical_union.hpp\n+++ b/src/include/duckdb/execution/operator/set/physical_union.hpp\n@@ -15,6 +15,10 @@ class PhysicalUnion : public PhysicalOperator {\n public:\n \tPhysicalUnion(vector<LogicalType> types, unique_ptr<PhysicalOperator> top, unique_ptr<PhysicalOperator> bottom,\n \t              idx_t estimated_cardinality);\n+\n+public:\n+\tvoid BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) override;\n+\tvector<const PhysicalOperator *> GetSources() const override;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/physical_operator.hpp b/src/include/duckdb/execution/physical_operator.hpp\nindex ca7dec6b225f..4542659ad8e0 100644\n--- a/src/include/duckdb/execution/physical_operator.hpp\n+++ b/src/include/duckdb/execution/physical_operator.hpp\n@@ -17,8 +17,10 @@\n \n namespace duckdb {\n class Event;\n+class Executor;\n class PhysicalOperator;\n class Pipeline;\n+class PipelineBuildState;\n \n // LCOV_EXCL_START\n class OperatorState {\n@@ -50,6 +52,13 @@ class LocalSinkState {\n public:\n \tvirtual ~LocalSinkState() {\n \t}\n+\n+\t//! The current batch index\n+\t//! This is only set in case RequiresBatchIndex() is true, and the source has support for it (SupportsBatchIndex())\n+\t//! Otherwise this is left on INVALID_INDEX\n+\t//! The batch index is a globally unique, increasing index that should be used to maintain insertion order\n+\t//! //! in conjunction with parallelism\n+\tidx_t batch_index = DConstants::INVALID_INDEX;\n };\n \n class GlobalSourceState {\n@@ -67,6 +76,7 @@ class LocalSourceState {\n \tvirtual ~LocalSourceState() {\n \t}\n };\n+\n // LCOV_EXCL_STOP\n \n //! PhysicalOperator is the base class of the physical operators present in the\n@@ -99,6 +109,7 @@ class PhysicalOperator {\n \t}\n \tvirtual string ToString() const;\n \tvoid Print() const;\n+\tvirtual vector<PhysicalOperator *> GetChildren() const;\n \n \t//! Return a vector of the types that will be returned by this operator\n \tconst vector<LogicalType> &GetTypes() const {\n@@ -109,6 +120,14 @@ class PhysicalOperator {\n \t\treturn false;\n \t}\n \n+\tvirtual void Verify();\n+\n+\t//! Whether or not the operator depends on the order of the input chunks\n+\t//! If this is set to true, we cannot do things like caching intermediate vectors\n+\tvirtual bool IsOrderDependent() const {\n+\t\treturn false;\n+\t}\n+\n public:\n \t// Operator interface\n \tvirtual unique_ptr<OperatorState> GetOperatorState(ClientContext &context) const;\n@@ -131,6 +150,8 @@ class PhysicalOperator {\n \tvirtual unique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const;\n \tvirtual void GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n \t                     LocalSourceState &lstate) const;\n+\tvirtual idx_t GetBatchIndex(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n+\t                            LocalSourceState &lstate) const;\n \n \tvirtual bool IsSource() const {\n \t\treturn false;\n@@ -140,6 +161,10 @@ class PhysicalOperator {\n \t\treturn false;\n \t}\n \n+\tvirtual bool SupportsBatchIndex() const {\n+\t\treturn false;\n+\t}\n+\n public:\n \t// Sink interface\n \n@@ -168,9 +193,19 @@ class PhysicalOperator {\n \t\treturn false;\n \t}\n \n-\tvirtual bool SinkOrderMatters() const {\n+\tvirtual bool RequiresBatchIndex() const {\n \t\treturn false;\n \t}\n+\n+public:\n+\t// Pipeline construction\n+\tvirtual vector<const PhysicalOperator *> GetSources() const;\n+\tbool AllSourcesSupportBatchIndex() const;\n+\n+\tvoid AddPipeline(Executor &executor, shared_ptr<Pipeline> current, PipelineBuildState &state);\n+\tvirtual void BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state);\n+\tvoid BuildChildPipeline(Executor &executor, Pipeline &current, PipelineBuildState &state,\n+\t                        PhysicalOperator *pipeline_child);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/function/table_function.hpp b/src/include/duckdb/function/table_function.hpp\nindex 8631edd73389..0d7816f7467f 100644\n--- a/src/include/duckdb/function/table_function.hpp\n+++ b/src/include/duckdb/function/table_function.hpp\n@@ -67,7 +67,8 @@ typedef OperatorResultType (*table_in_out_function_t)(ClientContext &context, co\n typedef void (*table_function_parallel_t)(ClientContext &context, const FunctionData *bind_data,\n                                           FunctionOperatorData *operator_state, DataChunk &output,\n                                           ParallelState *parallel_state);\n-\n+typedef idx_t (*table_function_get_batch_index_t)(ClientContext &context, const FunctionData *bind_data,\n+                                                  FunctionOperatorData *operator_state, ParallelState *parallel_state);\n typedef void (*table_function_cleanup_t)(ClientContext &context, const FunctionData *bind_data,\n                                          FunctionOperatorData *operator_state);\n typedef idx_t (*table_function_max_threads_t)(ClientContext &context, const FunctionData *bind_data);\n@@ -163,12 +164,16 @@ class TableFunction : public SimpleNamedParameterFunction {\n \ttable_function_parallel_state_next_t parallel_state_next;\n \t//! (Optional) return how much of the table we have scanned up to this point (% of the data)\n \ttable_function_progress_t table_scan_progress;\n+\t//! (Optional) returns the current batch index of the current scan operator\n+\ttable_function_get_batch_index_t get_batch_index;\n \t//! Whether or not the table function supports projection pushdown. If not supported a projection will be added\n \t//! that filters out unused columns.\n \tbool projection_pushdown;\n \t//! Whether or not the table function supports filter pushdown. If not supported a filter will be added\n \t//! that applies the table filter directly.\n \tbool filter_pushdown;\n+\t//! Whether or not the table function supports fetching of a batch index\n+\tbool supports_batch_index;\n \t//! Additional function info, passed to the bind\n \tshared_ptr<TableFunctionInfo> function_info;\n };\ndiff --git a/src/include/duckdb/main/client_config.hpp b/src/include/duckdb/main/client_config.hpp\nindex 37f3976e3876..351cc9dc29e1 100644\n--- a/src/include/duckdb/main/client_config.hpp\n+++ b/src/include/duckdb/main/client_config.hpp\n@@ -16,6 +16,11 @@\n \n namespace duckdb {\n class ClientContext;\n+class PhysicalResultCollector;\n+class PreparedStatementData;\n+\n+typedef std::function<unique_ptr<PhysicalResultCollector>(ClientContext &context, PreparedStatementData &data)>\n+    get_result_collector_t;\n \n struct ClientConfig {\n \t//! If the query profiler is enabled or not.\n@@ -63,6 +68,10 @@ struct ClientConfig {\n \t//! Generic options\n \tcase_insensitive_map_t<Value> set_variables;\n \n+\t//! Function that is used to create the result collector for a materialized result\n+\t//! Defaults to PhysicalMaterializedCollector\n+\tget_result_collector_t result_collector = nullptr;\n+\n public:\n \tstatic ClientConfig &GetConfig(ClientContext &context);\n \ndiff --git a/src/include/duckdb/main/client_context.hpp b/src/include/duckdb/main/client_context.hpp\nindex 4afdb472ded0..a586a5b256d5 100644\n--- a/src/include/duckdb/main/client_context.hpp\n+++ b/src/include/duckdb/main/client_context.hpp\n@@ -44,6 +44,13 @@ struct ActiveQueryContext;\n struct ParserOptions;\n struct ClientData;\n \n+struct PendingQueryParameters {\n+\t//! Prepared statement parameters (if any)\n+\tvector<Value> *parameters = nullptr;\n+\t//! Whether or not a stream result should be allowed\n+\tbool allow_stream_result = false;\n+};\n+\n //! The ClientContext holds information relevant to the current client session\n //! during execution\n class ClientContext : public std::enable_shared_from_this<ClientContext> {\n@@ -89,9 +96,10 @@ class ClientContext : public std::enable_shared_from_this<ClientContext> {\n \n \t//! Issues a query to the database and returns a Pending Query Result. Note that \"query\" may only contain\n \t//! a single statement.\n-\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(const string &query);\n+\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(const string &query, bool allow_stream_result);\n \t//! Issues a query to the database and returns a Pending Query Result\n-\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(unique_ptr<SQLStatement> statement);\n+\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(unique_ptr<SQLStatement> statement,\n+\t                                                       bool allow_stream_result);\n \n \t//! Destroy the client context\n \tDUCKDB_API void Destroy();\n@@ -116,13 +124,15 @@ class ClientContext : public std::enable_shared_from_this<ClientContext> {\n \t//! It is possible that the prepared statement will be re-bound. This will generally happen if the catalog is\n \t//! modified in between the prepared statement being bound and the prepared statement being run.\n \tDUCKDB_API unique_ptr<PendingQueryResult>\n-\tPendingQuery(const string &query, shared_ptr<PreparedStatementData> &prepared, vector<Value> &values);\n+\tPendingQuery(const string &query, shared_ptr<PreparedStatementData> &prepared, PendingQueryParameters parameters);\n \n \t//! Execute a prepared statement with the given name and set of parameters\n \t//! It is possible that the prepared statement will be re-bound. This will generally happen if the catalog is\n \t//! modified in between the prepared statement being bound and the prepared statement being run.\n \tDUCKDB_API unique_ptr<QueryResult> Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,\n \t                                           vector<Value> &values, bool allow_stream_result = true);\n+\tDUCKDB_API unique_ptr<QueryResult> Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,\n+\t                                           PendingQueryParameters parameters);\n \n \t//! Gets current percentage of the query's progress, returns 0 in case the progress bar is disabled.\n \tDUCKDB_API double GetProgress();\n@@ -171,9 +181,8 @@ class ClientContext : public std::enable_shared_from_this<ClientContext> {\n \t                     string &error);\n \t//! Issues a query to the database and returns a Pending Query Result\n \tunique_ptr<PendingQueryResult> PendingQueryInternal(ClientContextLock &lock, unique_ptr<SQLStatement> statement,\n-\t                                                    bool verify = true);\n-\tunique_ptr<QueryResult> ExecutePendingQueryInternal(ClientContextLock &lock, PendingQueryResult &query,\n-\t                                                    bool allow_stream_result);\n+\t                                                    PendingQueryParameters parameters, bool verify = true);\n+\tunique_ptr<QueryResult> ExecutePendingQueryInternal(ClientContextLock &lock, PendingQueryResult &query);\n \n \t//! Parse statements from a query\n \tvector<unique_ptr<SQLStatement>> ParseStatementsInternal(ClientContextLock &lock, const string &query);\n@@ -185,29 +194,28 @@ class ClientContext : public std::enable_shared_from_this<ClientContext> {\n \t//! Internal clean up, does not lock. Caller must hold the context_lock.\n \tvoid CleanupInternal(ClientContextLock &lock, BaseQueryResult *result = nullptr,\n \t                     bool invalidate_transaction = false);\n-\tstring FinalizeQuery(ClientContextLock &lock, bool success);\n \tunique_ptr<PendingQueryResult> PendingStatementOrPreparedStatement(ClientContextLock &lock, const string &query,\n \t                                                                   unique_ptr<SQLStatement> statement,\n \t                                                                   shared_ptr<PreparedStatementData> &prepared,\n-\t                                                                   vector<Value> *values);\n+\t                                                                   PendingQueryParameters parameters);\n \tunique_ptr<PendingQueryResult> PendingPreparedStatement(ClientContextLock &lock,\n \t                                                        shared_ptr<PreparedStatementData> statement_p,\n-\t                                                        vector<Value> bound_values);\n+\t                                                        PendingQueryParameters parameters);\n \n \t//! Internally prepare a SQL statement. Caller must hold the context_lock.\n \tshared_ptr<PreparedStatementData> CreatePreparedStatement(ClientContextLock &lock, const string &query,\n \t                                                          unique_ptr<SQLStatement> statement,\n \t                                                          vector<Value> *values = nullptr);\n \tunique_ptr<PendingQueryResult> PendingStatementInternal(ClientContextLock &lock, const string &query,\n-\t                                                        unique_ptr<SQLStatement> statement);\n+\t                                                        unique_ptr<SQLStatement> statement,\n+\t                                                        PendingQueryParameters parameters);\n \tunique_ptr<QueryResult> RunStatementInternal(ClientContextLock &lock, const string &query,\n \t                                             unique_ptr<SQLStatement> statement, bool allow_stream_result,\n \t                                             bool verify = true);\n \tunique_ptr<PreparedStatement> PrepareInternal(ClientContextLock &lock, unique_ptr<SQLStatement> statement);\n \tvoid LogQueryInternal(ClientContextLock &lock, const string &query);\n \n-\tunique_ptr<QueryResult> FetchResultInternal(ClientContextLock &lock, PendingQueryResult &pending,\n-\t                                            bool allow_stream_result);\n+\tunique_ptr<QueryResult> FetchResultInternal(ClientContextLock &lock, PendingQueryResult &pending);\n \tunique_ptr<DataChunk> FetchInternal(ClientContextLock &lock, Executor &executor, BaseQueryResult &result);\n \n \tunique_ptr<ClientContextLock> LockContext();\n@@ -220,14 +228,13 @@ class ClientContext : public std::enable_shared_from_this<ClientContext> {\n \n \tPendingExecutionResult ExecuteTaskInternal(ClientContextLock &lock, PendingQueryResult &result);\n \n-\tunique_ptr<PendingQueryResult>\n-\tPendingStatementOrPreparedStatementInternal(ClientContextLock &lock, const string &query,\n-\t                                            unique_ptr<SQLStatement> statement,\n-\t                                            shared_ptr<PreparedStatementData> &prepared, vector<Value> *values);\n+\tunique_ptr<PendingQueryResult> PendingStatementOrPreparedStatementInternal(\n+\t    ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,\n+\t    shared_ptr<PreparedStatementData> &prepared, PendingQueryParameters parameters);\n \n \tunique_ptr<PendingQueryResult> PendingQueryPreparedInternal(ClientContextLock &lock, const string &query,\n \t                                                            shared_ptr<PreparedStatementData> &prepared,\n-\t                                                            vector<Value> &values);\n+\t                                                            PendingQueryParameters parameters);\n \n private:\n \t//! Lock on using the ClientContext in parallel\ndiff --git a/src/include/duckdb/main/config.hpp b/src/include/duckdb/main/config.hpp\nindex c4576e3795cc..a71694afe844 100644\n--- a/src/include/duckdb/main/config.hpp\n+++ b/src/include/duckdb/main/config.hpp\n@@ -125,6 +125,8 @@ struct DBConfig {\n \tbool debug_many_free_list_blocks = false;\n \t//! Debug setting for window aggregation mode: (window, combine, separate)\n \tWindowAggregationMode window_mode = WindowAggregationMode::WINDOW;\n+\t//! Whether or not preserving insertion order should be preserved\n+\tbool preserve_insertion_order = true;\n \n \t//! Extra parameters that can be SET for loaded extensions\n \tcase_insensitive_map_t<ExtensionOption> extension_parameters;\ndiff --git a/src/include/duckdb/main/connection.hpp b/src/include/duckdb/main/connection.hpp\nindex 87bc475ed9bb..b815e7d5b8e8 100644\n--- a/src/include/duckdb/main/connection.hpp\n+++ b/src/include/duckdb/main/connection.hpp\n@@ -84,9 +84,10 @@ class Connection {\n \n \t//! Issues a query to the database and returns a Pending Query Result. Note that \"query\" may only contain\n \t//! a single statement.\n-\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(const string &query);\n+\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(const string &query, bool allow_stream_result = false);\n \t//! Issues a query to the database and returns a Pending Query Result\n-\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(unique_ptr<SQLStatement> statement);\n+\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(unique_ptr<SQLStatement> statement,\n+\t                                                       bool allow_stream_result = false);\n \n \t//! Prepare the specified query, returning a prepared statement object\n \tDUCKDB_API unique_ptr<PreparedStatement> Prepare(const string &query);\ndiff --git a/src/include/duckdb/main/materialized_query_result.hpp b/src/include/duckdb/main/materialized_query_result.hpp\nindex 42a85a467895..24682d264dec 100644\n--- a/src/include/duckdb/main/materialized_query_result.hpp\n+++ b/src/include/duckdb/main/materialized_query_result.hpp\n@@ -19,8 +19,6 @@ class ClientContext;\n class MaterializedQueryResult : public QueryResult {\n public:\n \tfriend class ClientContext;\n-\t//! Creates an empty successful query result\n-\tDUCKDB_API explicit MaterializedQueryResult(StatementType statement_type);\n \t//! Creates a successful query result with the specified names and types\n \tDUCKDB_API MaterializedQueryResult(StatementType statement_type, StatementProperties properties,\n \t                                   vector<LogicalType> types, vector<string> names,\ndiff --git a/src/include/duckdb/main/pending_query_result.hpp b/src/include/duckdb/main/pending_query_result.hpp\nindex 6fb24e66dc85..168a9cb56144 100644\n--- a/src/include/duckdb/main/pending_query_result.hpp\n+++ b/src/include/duckdb/main/pending_query_result.hpp\n@@ -22,7 +22,7 @@ class PendingQueryResult : public BaseQueryResult {\n \n public:\n \tDUCKDB_API PendingQueryResult(shared_ptr<ClientContext> context, PreparedStatementData &statement,\n-\t                              vector<LogicalType> types);\n+\t                              vector<LogicalType> types, bool allow_stream_result);\n \tDUCKDB_API explicit PendingQueryResult(string error_message);\n \tDUCKDB_API ~PendingQueryResult();\n \n@@ -36,18 +36,19 @@ class PendingQueryResult : public BaseQueryResult {\n \n \t//! Returns the result of the query as an actual query result.\n \t//! This returns (mostly) instantly if ExecuteTask has been called until RESULT_READY was returned.\n-\tDUCKDB_API unique_ptr<QueryResult> Execute(bool allow_streaming_result = false);\n+\tDUCKDB_API unique_ptr<QueryResult> Execute();\n \n \tDUCKDB_API void Close();\n \n private:\n \tshared_ptr<ClientContext> context;\n+\tbool allow_stream_result;\n \n private:\n \tvoid CheckExecutableInternal(ClientContextLock &lock);\n \n \tPendingExecutionResult ExecuteTaskInternal(ClientContextLock &lock);\n-\tunique_ptr<QueryResult> ExecuteInternal(ClientContextLock &lock, bool allow_streaming_result = false);\n+\tunique_ptr<QueryResult> ExecuteInternal(ClientContextLock &lock);\n \tunique_ptr<ClientContextLock> LockContext();\n };\n \ndiff --git a/src/include/duckdb/main/prepared_statement.hpp b/src/include/duckdb/main/prepared_statement.hpp\nindex c3a1cf639d88..4c81d7d65230 100644\n--- a/src/include/duckdb/main/prepared_statement.hpp\n+++ b/src/include/duckdb/main/prepared_statement.hpp\n@@ -68,7 +68,7 @@ class PreparedStatement {\n \t}\n \n \t//! Create a pending query result of the prepared statement with the given set of arguments\n-\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(vector<Value> &values);\n+\tDUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(vector<Value> &values, bool allow_stream_result = true);\n \n \t//! Execute the prepared statement with the given set of values\n \tDUCKDB_API unique_ptr<QueryResult> Execute(vector<Value> &values, bool allow_stream_result = true);\ndiff --git a/src/include/duckdb/main/settings.hpp b/src/include/duckdb/main/settings.hpp\nindex b5069993ae7a..d605bba1eb03 100644\n--- a/src/include/duckdb/main/settings.hpp\n+++ b/src/include/duckdb/main/settings.hpp\n@@ -213,6 +213,16 @@ struct PreserveIdentifierCase {\n \tstatic Value GetSetting(ClientContext &context);\n };\n \n+struct PreserveInsertionOrder {\n+\tstatic constexpr const char *Name = \"preserve_insertion_order\";\n+\tstatic constexpr const char *Description =\n+\t    \"Whether or not to preserve insertion order. If set to false the system is allowed to re-order any results \"\n+\t    \"that do not contain ORDER BY clauses.\";\n+\tstatic constexpr const LogicalTypeId InputType = LogicalTypeId::BOOLEAN;\n+\tstatic void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);\n+\tstatic Value GetSetting(ClientContext &context);\n+};\n+\n struct ProfilerHistorySize {\n \tstatic constexpr const char *Name = \"profiler_history_size\";\n \tstatic constexpr const char *Description = \"Sets the profiler history size\";\ndiff --git a/src/include/duckdb/parallel/pipeline.hpp b/src/include/duckdb/parallel/pipeline.hpp\nindex 710e8c7ba685..b92eab4431f2 100644\n--- a/src/include/duckdb/parallel/pipeline.hpp\n+++ b/src/include/duckdb/parallel/pipeline.hpp\n@@ -19,12 +19,45 @@ namespace duckdb {\n class Executor;\n class Event;\n \n+class PipelineBuildState {\n+public:\n+\t//! How much to increment batch indexes when multiple pipelines share the same source\n+\tconstexpr static idx_t BATCH_INCREMENT = 10000000000000;\n+\n+public:\n+\t//! The current recursive CTE node (if any)\n+\tPhysicalOperator *recursive_cte = nullptr;\n+\n+\t//! Duplicate eliminated join scan dependencies\n+\tunordered_map<PhysicalOperator *, Pipeline *> delim_join_dependencies;\n+\n+\t//! The number of pipelines that have each specific sink as their sink\n+\tunordered_map<PhysicalOperator *, idx_t> sink_pipeline_count;\n+\n+public:\n+\tvoid SetPipelineSource(Pipeline &pipeline, PhysicalOperator *op);\n+\tvoid SetPipelineSink(Pipeline &pipeline, PhysicalOperator *op);\n+\tvoid SetPipelineOperators(Pipeline &pipeline, vector<PhysicalOperator *> operators);\n+\tvoid AddPipelineOperator(Pipeline &pipeline, PhysicalOperator *op);\n+\tvoid AddPipeline(Executor &executor, shared_ptr<Pipeline> pipeline);\n+\tvoid AddChildPipeline(Executor &executor, Pipeline &pipeline);\n+\n+\tunordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> &GetUnionPipelines(Executor &executor);\n+\tunordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> &GetChildPipelines(Executor &executor);\n+\tunordered_map<Pipeline *, vector<Pipeline *>> &GetChildDependencies(Executor &executor);\n+\n+\tPhysicalOperator *GetPipelineSource(Pipeline &pipeline);\n+\tPhysicalOperator *GetPipelineSink(Pipeline &pipeline);\n+\tvector<PhysicalOperator *> GetPipelineOperators(Pipeline &pipeline);\n+};\n+\n //! The Pipeline class represents an execution pipeline\n class Pipeline : public std::enable_shared_from_this<Pipeline> {\n \tfriend class Executor;\n \tfriend class PipelineExecutor;\n \tfriend class PipelineEvent;\n \tfriend class PipelineFinishEvent;\n+\tfriend class PipelineBuildState;\n \n public:\n \texplicit Pipeline(Executor &execution_context);\n@@ -57,6 +90,9 @@ class Pipeline : public std::enable_shared_from_this<Pipeline> {\n \t\treturn sink;\n \t}\n \n+\t//! Returns whether any of the operators in the pipeline care about preserving insertion order\n+\tbool IsOrderDependent() const;\n+\n private:\n \t//! Whether or not the pipeline has been readied\n \tbool ready;\n@@ -75,6 +111,9 @@ class Pipeline : public std::enable_shared_from_this<Pipeline> {\n \t//! The dependencies of this pipeline\n \tvector<weak_ptr<Pipeline>> dependencies;\n \n+\t//! The base batch index of this pipeline\n+\tidx_t base_batch_index = 0;\n+\n private:\n \tbool GetProgressInternal(ClientContext &context, PhysicalOperator *op, double &current_percentage);\n \tvoid ScheduleSequentialTask(shared_ptr<Event> &event);\ndiff --git a/src/include/duckdb/parallel/pipeline_executor.hpp b/src/include/duckdb/parallel/pipeline_executor.hpp\nindex 87f107ac1077..2b67bc39a6f4 100644\n--- a/src/include/duckdb/parallel/pipeline_executor.hpp\n+++ b/src/include/duckdb/parallel/pipeline_executor.hpp\n@@ -77,7 +77,9 @@ class PipelineExecutor {\n \t//! Whether or not the pipeline has been finalized (used for verification only)\n \tbool finalized = false;\n \t//! Whether or not the pipeline has finished processing\n-\tbool finished_processing = false;\n+\tint32_t finished_processing_idx = -1;\n+\t//! Whether or not this pipeline requires keeping track of the batch index of the source\n+\tbool requires_batch_index = false;\n \n \t//! Cached chunks for any operators that require caching\n \tvector<unique_ptr<DataChunk>> cached_chunks;\n@@ -90,6 +92,9 @@ class PipelineExecutor {\n \tvoid GoToSource(idx_t &current_idx, idx_t initial_idx);\n \tvoid FetchFromSource(DataChunk &result);\n \n+\tvoid FinishProcessing(int32_t operator_idx = -1);\n+\tbool IsFinished();\n+\n \tOperatorResultType ExecutePushInternal(DataChunk &input, idx_t initial_idx = 0);\n \t//! Pushes a chunk through the pipeline and returns a single result chunk\n \t//! Returns whether or not a new input chunk is needed, or whether or not we are finished\ndiff --git a/src/include/duckdb/planner/parsed_data/bound_create_table_info.hpp b/src/include/duckdb/planner/parsed_data/bound_create_table_info.hpp\nindex 4c353a353c8c..ac12e9405705 100644\n--- a/src/include/duckdb/planner/parsed_data/bound_create_table_info.hpp\n+++ b/src/include/duckdb/planner/parsed_data/bound_create_table_info.hpp\n@@ -19,7 +19,8 @@ namespace duckdb {\n class CatalogEntry;\n \n struct BoundCreateTableInfo {\n-\texplicit BoundCreateTableInfo(unique_ptr<CreateInfo> base) : base(move(base)) {\n+\texplicit BoundCreateTableInfo(unique_ptr<CreateInfo> base_p) : base(move(base_p)) {\n+\t\tD_ASSERT(base);\n \t}\n \n \t//! The schema to create the table in\n@@ -42,6 +43,7 @@ struct BoundCreateTableInfo {\n \tunique_ptr<LogicalOperator> query;\n \n \tCreateTableInfo &Base() {\n+\t\tD_ASSERT(base);\n \t\treturn (CreateTableInfo &)*base;\n \t}\n };\ndiff --git a/src/main/client_context.cpp b/src/main/client_context.cpp\nindex f71c1834ea43..fc38cbeb2b1f 100644\n--- a/src/main/client_context.cpp\n+++ b/src/main/client_context.cpp\n@@ -33,6 +33,7 @@\n #include \"duckdb/planner/pragma_handler.hpp\"\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/execution/column_binding_resolver.hpp\"\n+#include \"duckdb/execution/operator/helper/physical_result_collector.hpp\"\n #include \"duckdb/parser/query_node/select_node.hpp\"\n \n namespace duckdb {\n@@ -200,14 +201,15 @@ const string &ClientContext::GetCurrentQuery() {\n \treturn active_query->query;\n }\n \n-unique_ptr<QueryResult> ClientContext::FetchResultInternal(ClientContextLock &lock, PendingQueryResult &pending,\n-                                                           bool allow_stream_result) {\n+unique_ptr<QueryResult> ClientContext::FetchResultInternal(ClientContextLock &lock, PendingQueryResult &pending) {\n \tD_ASSERT(active_query);\n \tD_ASSERT(active_query->open_result == &pending);\n \tD_ASSERT(active_query->prepared);\n+\tauto &executor = GetExecutor();\n \tauto &prepared = *active_query->prepared;\n-\tbool create_stream_result = prepared.properties.allow_stream_result && allow_stream_result;\n+\tbool create_stream_result = prepared.properties.allow_stream_result && pending.allow_stream_result;\n \tif (create_stream_result) {\n+\t\tD_ASSERT(!executor.HasResultCollector());\n \t\tactive_query->progress_bar.reset();\n \t\tquery_progress = -1;\n \n@@ -218,25 +220,32 @@ unique_ptr<QueryResult> ClientContext::FetchResultInternal(ClientContextLock &lo\n \t\tactive_query->open_result = stream_result.get();\n \t\treturn move(stream_result);\n \t}\n-\t// create a materialized result by continuously fetching\n-\tauto result = make_unique<MaterializedQueryResult>(pending.statement_type, pending.properties, pending.types,\n-\t                                                   pending.names, shared_from_this());\n-\tresult->properties = pending.properties;\n-\twhile (true) {\n-\t\tauto chunk = FetchInternal(lock, GetExecutor(), *result);\n-\t\tif (!chunk || chunk->size() == 0) {\n-\t\t\tbreak;\n-\t\t}\n+\tunique_ptr<QueryResult> result;\n+\tif (executor.HasResultCollector()) {\n+\t\t// we have a result collector - fetch the result directly from the result collector\n+\t\tresult = executor.GetResult();\n+\t\tCleanupInternal(lock, result.get(), false);\n+\t} else {\n+\t\t// no result collector - create a materialized result by continuously fetching\n+\t\tauto materialized_result = make_unique<MaterializedQueryResult>(\n+\t\t    pending.statement_type, pending.properties, pending.types, pending.names, shared_from_this());\n+\t\twhile (true) {\n+\t\t\tauto chunk = FetchInternal(lock, GetExecutor(), *materialized_result);\n+\t\t\tif (!chunk || chunk->size() == 0) {\n+\t\t\t\tbreak;\n+\t\t\t}\n #ifdef DEBUG\n-\t\tfor (idx_t i = 0; i < chunk->ColumnCount(); i++) {\n-\t\t\tif (pending.types[i].id() == LogicalTypeId::VARCHAR) {\n-\t\t\t\tchunk->data[i].UTFVerify(chunk->size());\n+\t\t\tfor (idx_t i = 0; i < chunk->ColumnCount(); i++) {\n+\t\t\t\tif (pending.types[i].id() == LogicalTypeId::VARCHAR) {\n+\t\t\t\t\tchunk->data[i].UTFVerify(chunk->size());\n+\t\t\t\t}\n \t\t\t}\n-\t\t}\n #endif\n-\t\tresult->collection.Append(*chunk);\n+\t\t\tmaterialized_result->collection.Append(*chunk);\n+\t\t}\n+\t\tresult = move(materialized_result);\n \t}\n-\treturn move(result);\n+\treturn result;\n }\n \n shared_ptr<PreparedStatementData> ClientContext::CreatePreparedStatement(ClientContextLock &lock, const string &query,\n@@ -299,7 +308,7 @@ double ClientContext::GetProgress() {\n \n unique_ptr<PendingQueryResult> ClientContext::PendingPreparedStatement(ClientContextLock &lock,\n                                                                        shared_ptr<PreparedStatementData> statement_p,\n-                                                                       vector<Value> bound_values) {\n+                                                                       PendingQueryParameters parameters) {\n \tD_ASSERT(active_query);\n \tauto &statement = *statement_p;\n \tif (ActiveTransaction().IsInvalidated() && statement.properties.requires_valid_transaction) {\n@@ -312,7 +321,7 @@ unique_ptr<PendingQueryResult> ClientContext::PendingPreparedStatement(ClientCon\n \t}\n \n \t// bind the bound values before execution\n-\tstatement.Bind(move(bound_values));\n+\tstatement.Bind(parameters.parameters ? *parameters.parameters : vector<Value>());\n \n \tactive_query->executor = make_unique<Executor>(*this);\n \tauto &executor = *active_query->executor;\n@@ -321,12 +330,23 @@ unique_ptr<PendingQueryResult> ClientContext::PendingPreparedStatement(ClientCon\n \t\tactive_query->progress_bar->Start();\n \t\tquery_progress = 0;\n \t}\n-\texecutor.Initialize(statement.plan.get());\n+\tauto stream_result = parameters.allow_stream_result && statement.properties.allow_stream_result;\n+\tif (!stream_result && statement.properties.return_type == StatementReturnType::QUERY_RESULT) {\n+\t\tunique_ptr<PhysicalResultCollector> collector;\n+\t\tauto &config = ClientConfig::GetConfig(*this);\n+\t\tauto get_method =\n+\t\t    config.result_collector ? config.result_collector : PhysicalResultCollector::GetResultCollector;\n+\t\tcollector = get_method(*this, statement);\n+\t\tD_ASSERT(collector->type == PhysicalOperatorType::RESULT_COLLECTOR);\n+\t\texecutor.Initialize(move(collector));\n+\t} else {\n+\t\texecutor.Initialize(statement.plan.get());\n+\t}\n \tauto types = executor.GetTypes();\n \tD_ASSERT(types == statement.types);\n \tD_ASSERT(!active_query->open_result);\n \n-\tauto pending_result = make_unique<PendingQueryResult>(shared_from_this(), *statement_p, move(types));\n+\tauto pending_result = make_unique<PendingQueryResult>(shared_from_this(), *statement_p, move(types), stream_result);\n \tactive_query->prepared = move(statement_p);\n \tactive_query->open_result = pending_result.get();\n \treturn pending_result;\n@@ -454,49 +474,59 @@ unique_ptr<PreparedStatement> ClientContext::Prepare(const string &query) {\n \n unique_ptr<PendingQueryResult> ClientContext::PendingQueryPreparedInternal(ClientContextLock &lock, const string &query,\n                                                                            shared_ptr<PreparedStatementData> &prepared,\n-                                                                           vector<Value> &values) {\n+                                                                           PendingQueryParameters parameters) {\n \ttry {\n \t\tInitialCleanup(lock);\n \t} catch (std::exception &ex) {\n \t\treturn make_unique<PendingQueryResult>(ex.what());\n \t}\n-\treturn PendingStatementOrPreparedStatementInternal(lock, query, nullptr, prepared, &values);\n+\treturn PendingStatementOrPreparedStatementInternal(lock, query, nullptr, prepared, parameters);\n }\n \n-unique_ptr<PendingQueryResult>\n-ClientContext::PendingQuery(const string &query, shared_ptr<PreparedStatementData> &prepared, vector<Value> &values) {\n+unique_ptr<PendingQueryResult> ClientContext::PendingQuery(const string &query,\n+                                                           shared_ptr<PreparedStatementData> &prepared,\n+                                                           PendingQueryParameters parameters) {\n \tauto lock = LockContext();\n-\treturn PendingQueryPreparedInternal(*lock, query, prepared, values);\n+\treturn PendingQueryPreparedInternal(*lock, query, prepared, parameters);\n }\n \n unique_ptr<QueryResult> ClientContext::Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,\n-                                               vector<Value> &values, bool allow_stream_result) {\n+                                               PendingQueryParameters parameters) {\n \tauto lock = LockContext();\n-\tauto pending = PendingQueryPreparedInternal(*lock, query, prepared, values);\n+\tauto pending = PendingQueryPreparedInternal(*lock, query, prepared, parameters);\n \tif (!pending->success) {\n \t\treturn make_unique<MaterializedQueryResult>(pending->error);\n \t}\n-\treturn pending->ExecuteInternal(*lock, allow_stream_result);\n+\treturn pending->ExecuteInternal(*lock);\n+}\n+\n+unique_ptr<QueryResult> ClientContext::Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,\n+                                               vector<Value> &values, bool allow_stream_result) {\n+\tPendingQueryParameters parameters;\n+\tparameters.parameters = &values;\n+\tparameters.allow_stream_result = allow_stream_result;\n+\treturn Execute(query, prepared, parameters);\n }\n \n unique_ptr<PendingQueryResult> ClientContext::PendingStatementInternal(ClientContextLock &lock, const string &query,\n-                                                                       unique_ptr<SQLStatement> statement) {\n+                                                                       unique_ptr<SQLStatement> statement,\n+                                                                       PendingQueryParameters parameters) {\n \t// prepare the query for execution\n \tauto prepared = CreatePreparedStatement(lock, query, move(statement));\n-\t// by default, no values are bound\n-\tvector<Value> bound_values;\n \t// execute the prepared statement\n-\treturn PendingPreparedStatement(lock, move(prepared), move(bound_values));\n+\treturn PendingPreparedStatement(lock, move(prepared), parameters);\n }\n \n unique_ptr<QueryResult> ClientContext::RunStatementInternal(ClientContextLock &lock, const string &query,\n                                                             unique_ptr<SQLStatement> statement,\n                                                             bool allow_stream_result, bool verify) {\n-\tauto pending = PendingQueryInternal(lock, move(statement), verify);\n+\tPendingQueryParameters parameters;\n+\tparameters.allow_stream_result = allow_stream_result;\n+\tauto pending = PendingQueryInternal(lock, move(statement), parameters, verify);\n \tif (!pending->success) {\n \t\treturn make_unique<MaterializedQueryResult>(move(pending->error));\n \t}\n-\treturn ExecutePendingQueryInternal(lock, *pending, allow_stream_result);\n+\treturn ExecutePendingQueryInternal(lock, *pending);\n }\n \n bool ClientContext::IsActiveResult(ClientContextLock &lock, BaseQueryResult *result) {\n@@ -519,7 +549,7 @@ static bool IsExplainAnalyze(SQLStatement *statement) {\n \n unique_ptr<PendingQueryResult> ClientContext::PendingStatementOrPreparedStatementInternal(\n     ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,\n-    shared_ptr<PreparedStatementData> &prepared, vector<Value> *values) {\n+    shared_ptr<PreparedStatementData> &prepared, PendingQueryParameters parameters) {\n \t// check if we are on AutoCommit. In this case we should start a transaction.\n \tif (statement && config.query_verification_enabled) {\n \t\t// query verification is enabled\n@@ -556,13 +586,12 @@ unique_ptr<PendingQueryResult> ClientContext::PendingStatementOrPreparedStatemen\n \t\t\tbreak;\n \t\t}\n \t}\n-\treturn PendingStatementOrPreparedStatement(lock, query, move(statement), prepared, values);\n+\treturn PendingStatementOrPreparedStatement(lock, query, move(statement), prepared, parameters);\n }\n \n-unique_ptr<PendingQueryResult>\n-ClientContext::PendingStatementOrPreparedStatement(ClientContextLock &lock, const string &query,\n-                                                   unique_ptr<SQLStatement> statement,\n-                                                   shared_ptr<PreparedStatementData> &prepared, vector<Value> *values) {\n+unique_ptr<PendingQueryResult> ClientContext::PendingStatementOrPreparedStatement(\n+    ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,\n+    shared_ptr<PreparedStatementData> &prepared, PendingQueryParameters parameters) {\n \tunique_ptr<PendingQueryResult> result;\n \n \tBeginQueryInternal(lock, query);\n@@ -572,13 +601,14 @@ ClientContext::PendingStatementOrPreparedStatement(ClientContextLock &lock, cons\n \tbool invalidate_query = true;\n \ttry {\n \t\tif (statement) {\n-\t\t\tresult = PendingStatementInternal(lock, query, move(statement));\n+\t\t\tresult = PendingStatementInternal(lock, query, move(statement), parameters);\n \t\t} else {\n \t\t\tauto &catalog = Catalog::GetCatalog(*this);\n \t\t\tif (prepared->unbound_statement && (catalog.GetCatalogVersion() != prepared->catalog_version ||\n \t\t\t                                    !prepared->properties.bound_all_parameters)) {\n \t\t\t\t// catalog was modified: rebind the statement before execution\n-\t\t\t\tauto new_prepared = CreatePreparedStatement(lock, query, prepared->unbound_statement->Copy(), values);\n+\t\t\t\tauto new_prepared =\n+\t\t\t\t    CreatePreparedStatement(lock, query, prepared->unbound_statement->Copy(), parameters.parameters);\n \t\t\t\tif (prepared->types != new_prepared->types && prepared->properties.bound_all_parameters) {\n \t\t\t\t\tthrow BinderException(\"Rebinding statement after catalog change resulted in change of types\");\n \t\t\t\t}\n@@ -587,7 +617,7 @@ ClientContext::PendingStatementOrPreparedStatement(ClientContextLock &lock, cons\n \t\t\t\tprepared = move(new_prepared);\n \t\t\t\tprepared->properties.bound_all_parameters = false;\n \t\t\t}\n-\t\t\tresult = PendingPreparedStatement(lock, prepared, *values);\n+\t\t\tresult = PendingPreparedStatement(lock, prepared, parameters);\n \t\t}\n \t} catch (StandardException &ex) {\n \t\t// standard exceptions do not invalidate the current transaction\n@@ -629,8 +659,8 @@ void ClientContext::LogQueryInternal(ClientContextLock &, const string &query) {\n }\n \n unique_ptr<QueryResult> ClientContext::Query(unique_ptr<SQLStatement> statement, bool allow_stream_result) {\n-\tauto pending_query = PendingQuery(move(statement));\n-\treturn pending_query->Execute(allow_stream_result);\n+\tauto pending_query = PendingQuery(move(statement), allow_stream_result);\n+\treturn pending_query->Execute();\n }\n \n unique_ptr<QueryResult> ClientContext::Query(const string &query, bool allow_stream_result) {\n@@ -655,13 +685,14 @@ unique_ptr<QueryResult> ClientContext::Query(const string &query, bool allow_str\n \tfor (idx_t i = 0; i < statements.size(); i++) {\n \t\tauto &statement = statements[i];\n \t\tbool is_last_statement = i + 1 == statements.size();\n-\t\tbool stream_result = allow_stream_result && is_last_statement;\n-\t\tauto pending_query = PendingQueryInternal(*lock, move(statement));\n+\t\tPendingQueryParameters parameters;\n+\t\tparameters.allow_stream_result = allow_stream_result && is_last_statement;\n+\t\tauto pending_query = PendingQueryInternal(*lock, move(statement), parameters);\n \t\tunique_ptr<QueryResult> current_result;\n \t\tif (!pending_query->success) {\n \t\t\tcurrent_result = make_unique<MaterializedQueryResult>(pending_query->error);\n \t\t} else {\n-\t\t\tcurrent_result = ExecutePendingQueryInternal(*lock, *pending_query, stream_result);\n+\t\t\tcurrent_result = ExecutePendingQueryInternal(*lock, *pending_query);\n \t\t}\n \t\t// now append the result to the list of results\n \t\tif (!last_result) {\n@@ -690,7 +721,7 @@ bool ClientContext::ParseStatements(ClientContextLock &lock, const string &query\n \t}\n }\n \n-unique_ptr<PendingQueryResult> ClientContext::PendingQuery(const string &query) {\n+unique_ptr<PendingQueryResult> ClientContext::PendingQuery(const string &query, bool allow_stream_result) {\n \tauto lock = LockContext();\n \n \tstring error;\n@@ -701,28 +732,33 @@ unique_ptr<PendingQueryResult> ClientContext::PendingQuery(const string &query)\n \tif (statements.size() != 1) {\n \t\treturn make_unique<PendingQueryResult>(\"PendingQuery can only take a single statement\");\n \t}\n-\treturn PendingQueryInternal(*lock, move(statements[0]));\n+\tPendingQueryParameters parameters;\n+\tparameters.allow_stream_result = allow_stream_result;\n+\treturn PendingQueryInternal(*lock, move(statements[0]), parameters);\n }\n \n-unique_ptr<PendingQueryResult> ClientContext::PendingQuery(unique_ptr<SQLStatement> statement) {\n+unique_ptr<PendingQueryResult> ClientContext::PendingQuery(unique_ptr<SQLStatement> statement,\n+                                                           bool allow_stream_result) {\n \tauto lock = LockContext();\n-\treturn PendingQueryInternal(*lock, move(statement));\n+\tPendingQueryParameters parameters;\n+\tparameters.allow_stream_result = allow_stream_result;\n+\treturn PendingQueryInternal(*lock, move(statement), parameters);\n }\n \n unique_ptr<PendingQueryResult> ClientContext::PendingQueryInternal(ClientContextLock &lock,\n-                                                                   unique_ptr<SQLStatement> statement, bool verify) {\n+                                                                   unique_ptr<SQLStatement> statement,\n+                                                                   PendingQueryParameters parameters, bool verify) {\n \tauto query = statement->query;\n \tshared_ptr<PreparedStatementData> prepared;\n \tif (verify) {\n-\t\treturn PendingStatementOrPreparedStatementInternal(lock, query, move(statement), prepared, nullptr);\n+\t\treturn PendingStatementOrPreparedStatementInternal(lock, query, move(statement), prepared, parameters);\n \t} else {\n-\t\treturn PendingStatementOrPreparedStatement(lock, query, move(statement), prepared, nullptr);\n+\t\treturn PendingStatementOrPreparedStatement(lock, query, move(statement), prepared, parameters);\n \t}\n }\n \n-unique_ptr<QueryResult> ClientContext::ExecutePendingQueryInternal(ClientContextLock &lock, PendingQueryResult &query,\n-                                                                   bool allow_stream_result) {\n-\treturn query.ExecuteInternal(lock, allow_stream_result);\n+unique_ptr<QueryResult> ClientContext::ExecutePendingQueryInternal(ClientContextLock &lock, PendingQueryResult &query) {\n+\treturn query.ExecuteInternal(lock);\n }\n \n void ClientContext::Interrupt() {\ndiff --git a/src/main/config.cpp b/src/main/config.cpp\nindex 64b582ba0eca..b23d0c633b35 100644\n--- a/src/main/config.cpp\n+++ b/src/main/config.cpp\n@@ -47,6 +47,7 @@ static ConfigurationOption internal_options[] = {DUCKDB_GLOBAL(AccessModeSetting\n                                                  DUCKDB_GLOBAL_ALIAS(\"null_order\", DefaultNullOrderSetting),\n                                                  DUCKDB_LOCAL(PerfectHashThresholdSetting),\n                                                  DUCKDB_LOCAL(PreserveIdentifierCase),\n+                                                 DUCKDB_GLOBAL(PreserveInsertionOrder),\n                                                  DUCKDB_LOCAL(ProfilerHistorySize),\n                                                  DUCKDB_LOCAL(ProfileOutputSetting),\n                                                  DUCKDB_LOCAL(ProfilingModeSetting),\ndiff --git a/src/main/connection.cpp b/src/main/connection.cpp\nindex f2a1b8ebe368..23819f9c404b 100644\n--- a/src/main/connection.cpp\n+++ b/src/main/connection.cpp\n@@ -79,12 +79,12 @@ unique_ptr<MaterializedQueryResult> Connection::Query(unique_ptr<SQLStatement> s\n \treturn unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));\n }\n \n-unique_ptr<PendingQueryResult> Connection::PendingQuery(const string &query) {\n-\treturn context->PendingQuery(query);\n+unique_ptr<PendingQueryResult> Connection::PendingQuery(const string &query, bool allow_stream_result) {\n+\treturn context->PendingQuery(query, allow_stream_result);\n }\n \n-unique_ptr<PendingQueryResult> Connection::PendingQuery(unique_ptr<SQLStatement> statement) {\n-\treturn context->PendingQuery(move(statement));\n+unique_ptr<PendingQueryResult> Connection::PendingQuery(unique_ptr<SQLStatement> statement, bool allow_stream_result) {\n+\treturn context->PendingQuery(move(statement), allow_stream_result);\n }\n \n unique_ptr<PreparedStatement> Connection::Prepare(const string &query) {\ndiff --git a/src/main/pending_query_result.cpp b/src/main/pending_query_result.cpp\nindex 37365e3368fc..7d39cd92e46f 100644\n--- a/src/main/pending_query_result.cpp\n+++ b/src/main/pending_query_result.cpp\n@@ -5,10 +5,10 @@\n namespace duckdb {\n \n PendingQueryResult::PendingQueryResult(shared_ptr<ClientContext> context_p, PreparedStatementData &statement,\n-                                       vector<LogicalType> types_p)\n+                                       vector<LogicalType> types_p, bool allow_stream_result)\n     : BaseQueryResult(QueryResultType::PENDING_RESULT, statement.statement_type, statement.properties, move(types_p),\n                       statement.names),\n-      context(move(context_p)) {\n+      context(move(context_p)), allow_stream_result(allow_stream_result) {\n }\n \n PendingQueryResult::PendingQueryResult(string error) : BaseQueryResult(QueryResultType::PENDING_RESULT, move(error)) {\n@@ -46,21 +46,21 @@ PendingExecutionResult PendingQueryResult::ExecuteTaskInternal(ClientContextLock\n \treturn context->ExecuteTaskInternal(lock, *this);\n }\n \n-unique_ptr<QueryResult> PendingQueryResult::ExecuteInternal(ClientContextLock &lock, bool allow_streaming_result) {\n+unique_ptr<QueryResult> PendingQueryResult::ExecuteInternal(ClientContextLock &lock) {\n \tCheckExecutableInternal(lock);\n \twhile (ExecuteTaskInternal(lock) == PendingExecutionResult::RESULT_NOT_READY) {\n \t}\n \tif (!success) {\n \t\treturn make_unique<MaterializedQueryResult>(error);\n \t}\n-\tauto result = context->FetchResultInternal(lock, *this, allow_streaming_result);\n+\tauto result = context->FetchResultInternal(lock, *this);\n \tClose();\n \treturn result;\n }\n \n-unique_ptr<QueryResult> PendingQueryResult::Execute(bool allow_streaming_result) {\n+unique_ptr<QueryResult> PendingQueryResult::Execute() {\n \tauto lock = LockContext();\n-\treturn ExecuteInternal(*lock, allow_streaming_result);\n+\treturn ExecuteInternal(*lock);\n }\n \n void PendingQueryResult::Close() {\ndiff --git a/src/main/prepared_statement.cpp b/src/main/prepared_statement.cpp\nindex 8ea3dd5aca8b..c07ed5ece2c7 100644\n--- a/src/main/prepared_statement.cpp\n+++ b/src/main/prepared_statement.cpp\n@@ -43,19 +43,22 @@ const vector<string> &PreparedStatement::GetNames() {\n }\n \n unique_ptr<QueryResult> PreparedStatement::Execute(vector<Value> &values, bool allow_stream_result) {\n-\tauto pending = PendingQuery(values);\n+\tauto pending = PendingQuery(values, allow_stream_result);\n \tif (!pending->success) {\n \t\treturn make_unique<MaterializedQueryResult>(pending->error);\n \t}\n-\treturn pending->Execute(allow_stream_result && data->properties.allow_stream_result);\n+\treturn pending->Execute();\n }\n \n-unique_ptr<PendingQueryResult> PreparedStatement::PendingQuery(vector<Value> &values) {\n+unique_ptr<PendingQueryResult> PreparedStatement::PendingQuery(vector<Value> &values, bool allow_stream_result) {\n \tif (!success) {\n \t\tthrow InvalidInputException(\"Attempting to execute an unsuccessfully prepared statement!\");\n \t}\n \tD_ASSERT(data);\n-\tauto result = context->PendingQuery(query, data, values);\n+\tPendingQueryParameters parameters;\n+\tparameters.parameters = &values;\n+\tparameters.allow_stream_result = allow_stream_result && data->properties.allow_stream_result;\n+\tauto result = context->PendingQuery(query, data, parameters);\n \treturn result;\n }\n \ndiff --git a/src/main/query_profiler.cpp b/src/main/query_profiler.cpp\nindex 7747fc9daf2b..65f5c1a03f2d 100644\n--- a/src/main/query_profiler.cpp\n+++ b/src/main/query_profiler.cpp\n@@ -66,6 +66,7 @@ bool QueryProfiler::OperatorRequiresProfiling(PhysicalOperatorType op_type) {\n \tcase PhysicalOperatorType::STREAMING_SAMPLE:\n \tcase PhysicalOperatorType::LIMIT:\n \tcase PhysicalOperatorType::LIMIT_PERCENT:\n+\tcase PhysicalOperatorType::STREAMING_LIMIT:\n \tcase PhysicalOperatorType::TOP_N:\n \tcase PhysicalOperatorType::WINDOW:\n \tcase PhysicalOperatorType::UNNEST:\n@@ -569,28 +570,11 @@ unique_ptr<QueryProfiler::TreeNode> QueryProfiler::CreateTree(PhysicalOperator *\n \tnode->extra_info = root->ParamsToString();\n \tnode->depth = depth;\n \ttree_map[root] = node.get();\n-\tfor (auto &child : root->children) {\n-\t\tauto child_node = CreateTree(child.get(), depth + 1);\n+\tauto children = root->GetChildren();\n+\tfor (auto &child : children) {\n+\t\tauto child_node = CreateTree(child, depth + 1);\n \t\tnode->children.push_back(move(child_node));\n \t}\n-\tswitch (root->type) {\n-\tcase PhysicalOperatorType::DELIM_JOIN: {\n-\t\tauto &delim_join = (PhysicalDelimJoin &)*root;\n-\t\tauto child_node = CreateTree((PhysicalOperator *)delim_join.join.get(), depth + 1);\n-\t\tnode->children.push_back(move(child_node));\n-\t\tchild_node = CreateTree((PhysicalOperator *)delim_join.distinct.get(), depth + 1);\n-\t\tnode->children.push_back(move(child_node));\n-\t\tbreak;\n-\t}\n-\tcase PhysicalOperatorType::EXECUTE: {\n-\t\tauto &execute = (PhysicalExecute &)*root;\n-\t\tauto child_node = CreateTree((PhysicalOperator *)execute.plan, depth + 1);\n-\t\tnode->children.push_back(move(child_node));\n-\t\tbreak;\n-\t}\n-\tdefault:\n-\t\tbreak;\n-\t}\n \treturn node;\n }\n \ndiff --git a/src/main/settings/settings.cpp b/src/main/settings/settings.cpp\nindex 96000076024f..f4c463ac247f 100644\n--- a/src/main/settings/settings.cpp\n+++ b/src/main/settings/settings.cpp\n@@ -456,6 +456,18 @@ Value PreserveIdentifierCase::GetSetting(ClientContext &context) {\n \treturn Value::BOOLEAN(ClientConfig::GetConfig(context).preserve_identifier_case);\n }\n \n+//===--------------------------------------------------------------------===//\n+// PreserveInsertionOrder\n+//===--------------------------------------------------------------------===//\n+void PreserveInsertionOrder::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {\n+\tconfig.preserve_insertion_order = input.GetValue<bool>();\n+}\n+\n+Value PreserveInsertionOrder::GetSetting(ClientContext &context) {\n+\tauto &config = DBConfig::GetConfig(context);\n+\treturn Value::BOOLEAN(config.preserve_insertion_order);\n+}\n+\n //===--------------------------------------------------------------------===//\n // Profiler History Size\n //===--------------------------------------------------------------------===//\ndiff --git a/src/parallel/executor.cpp b/src/parallel/executor.cpp\nindex cb15e078e856..4cd626a7820c 100644\n--- a/src/parallel/executor.cpp\n+++ b/src/parallel/executor.cpp\n@@ -1,10 +1,5 @@\n #include \"duckdb/execution/executor.hpp\"\n \n-#include \"duckdb/execution/operator/helper/physical_execute.hpp\"\n-#include \"duckdb/execution/operator/join/physical_delim_join.hpp\"\n-#include \"duckdb/execution/operator/join/physical_iejoin.hpp\"\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n-#include \"duckdb/execution/operator/set/physical_recursive_cte.hpp\"\n #include \"duckdb/execution/physical_operator.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/main/client_data.hpp\"\n@@ -17,6 +12,8 @@\n #include \"duckdb/parallel/pipeline_finish_event.hpp\"\n #include \"duckdb/parallel/pipeline_complete_event.hpp\"\n \n+#include \"duckdb/execution/operator/helper/physical_result_collector.hpp\"\n+\n #include <algorithm>\n \n namespace duckdb {\n@@ -257,8 +254,18 @@ void Executor::VerifyPipelines() {\n #endif\n }\n \n+void Executor::Initialize(unique_ptr<PhysicalOperator> physical_plan) {\n+\tReset();\n+\towned_plan = move(physical_plan);\n+\tInitializeInternal(owned_plan.get());\n+}\n+\n void Executor::Initialize(PhysicalOperator *plan) {\n \tReset();\n+\tInitializeInternal(plan);\n+}\n+\n+void Executor::InitializeInternal(PhysicalOperator *plan) {\n \n \tauto &scheduler = TaskScheduler::GetScheduler(context);\n \t{\n@@ -271,7 +278,9 @@ void Executor::Initialize(PhysicalOperator *plan) {\n \n \t\tauto root_pipeline = make_shared<Pipeline>(*this);\n \t\troot_pipeline->sink = nullptr;\n-\t\tBuildPipelines(physical_plan, root_pipeline.get());\n+\n+\t\tPipelineBuildState state;\n+\t\tphysical_plan->BuildPipelines(*this, *root_pipeline, state);\n \n \t\tthis->total_pipelines = pipelines.size();\n \n@@ -382,9 +391,8 @@ PendingExecutionResult Executor::ExecuteTask() {\n \n void Executor::Reset() {\n \tlock_guard<mutex> elock(executor_lock);\n-\tdelim_join_dependencies.clear();\n-\trecursive_cte = nullptr;\n \tphysical_plan = nullptr;\n+\towned_plan.reset();\n \troot_executor.reset();\n \troot_pipelines.clear();\n \troot_pipeline_idx = 0;\n@@ -425,235 +433,6 @@ void Executor::AddChildPipeline(Pipeline *current) {\n \tchild_pipelines[current].push_back(move(child_pipeline));\n }\n \n-void Executor::BuildPipelines(PhysicalOperator *op, Pipeline *current) {\n-\tD_ASSERT(current);\n-\top->op_state.reset();\n-\tif (op->IsSink()) {\n-\t\t// operator is a sink, build a pipeline\n-\t\top->sink_state.reset();\n-\n-\t\tPhysicalOperator *pipeline_child = nullptr;\n-\t\tswitch (op->type) {\n-\t\tcase PhysicalOperatorType::CREATE_TABLE_AS:\n-\t\tcase PhysicalOperatorType::INSERT:\n-\t\tcase PhysicalOperatorType::DELETE_OPERATOR:\n-\t\tcase PhysicalOperatorType::UPDATE:\n-\t\tcase PhysicalOperatorType::HASH_GROUP_BY:\n-\t\tcase PhysicalOperatorType::SIMPLE_AGGREGATE:\n-\t\tcase PhysicalOperatorType::PERFECT_HASH_GROUP_BY:\n-\t\tcase PhysicalOperatorType::WINDOW:\n-\t\tcase PhysicalOperatorType::ORDER_BY:\n-\t\tcase PhysicalOperatorType::RESERVOIR_SAMPLE:\n-\t\tcase PhysicalOperatorType::TOP_N:\n-\t\tcase PhysicalOperatorType::COPY_TO_FILE:\n-\t\tcase PhysicalOperatorType::LIMIT:\n-\t\tcase PhysicalOperatorType::LIMIT_PERCENT:\n-\t\tcase PhysicalOperatorType::EXPLAIN_ANALYZE:\n-\t\t\tD_ASSERT(op->children.size() == 1);\n-\t\t\t// single operator:\n-\t\t\t// the operator becomes the data source of the current pipeline\n-\t\t\tcurrent->source = op;\n-\t\t\t// we create a new pipeline starting from the child\n-\t\t\tpipeline_child = op->children[0].get();\n-\t\t\tbreak;\n-\t\tcase PhysicalOperatorType::EXPORT:\n-\t\t\t// EXPORT has an optional child\n-\t\t\t// we only need to schedule child pipelines if there is a child\n-\t\t\tcurrent->source = op;\n-\t\t\tif (op->children.empty()) {\n-\t\t\t\treturn;\n-\t\t\t}\n-\t\t\tD_ASSERT(op->children.size() == 1);\n-\t\t\tpipeline_child = op->children[0].get();\n-\t\t\tbreak;\n-\t\tcase PhysicalOperatorType::NESTED_LOOP_JOIN:\n-\t\tcase PhysicalOperatorType::BLOCKWISE_NL_JOIN:\n-\t\tcase PhysicalOperatorType::HASH_JOIN:\n-\t\tcase PhysicalOperatorType::PIECEWISE_MERGE_JOIN:\n-\t\tcase PhysicalOperatorType::CROSS_PRODUCT:\n-\t\t\t// regular join, create a pipeline with RHS source that sinks into this pipeline\n-\t\t\tpipeline_child = op->children[1].get();\n-\t\t\t// on the LHS (probe child), the operator becomes a regular operator\n-\t\t\tcurrent->operators.push_back(op);\n-\t\t\tif (op->IsSource()) {\n-\t\t\t\t// FULL or RIGHT outer join\n-\t\t\t\t// schedule a scan of the node as a child pipeline\n-\t\t\t\t// this scan has to be performed AFTER all the probing has happened\n-\t\t\t\tif (recursive_cte) {\n-\t\t\t\t\tthrow NotImplementedException(\"FULL and RIGHT outer joins are not supported in recursive CTEs yet\");\n-\t\t\t\t}\n-\t\t\t\tAddChildPipeline(current);\n-\t\t\t}\n-\t\t\tBuildPipelines(op->children[0].get(), current);\n-\t\t\tbreak;\n-\t\tcase PhysicalOperatorType::IE_JOIN: {\n-\t\t\tD_ASSERT(op->children.size() == 2);\n-\t\t\tif (recursive_cte) {\n-\t\t\t\tthrow NotImplementedException(\"IEJoins are not supported in recursive CTEs yet\");\n-\t\t\t}\n-\n-\t\t\t// Build the LHS\n-\t\t\tauto lhs_pipeline = make_shared<Pipeline>(*this);\n-\t\t\tlhs_pipeline->sink = op;\n-\t\t\tD_ASSERT(op->children[0].get());\n-\t\t\tBuildPipelines(op->children[0].get(), lhs_pipeline.get());\n-\n-\t\t\t// Build the RHS\n-\t\t\tauto rhs_pipeline = make_shared<Pipeline>(*this);\n-\t\t\trhs_pipeline->sink = op;\n-\t\t\tD_ASSERT(op->children[1].get());\n-\t\t\tBuildPipelines(op->children[1].get(), rhs_pipeline.get());\n-\n-\t\t\t// RHS => LHS => current\n-\t\t\tcurrent->AddDependency(rhs_pipeline);\n-\t\t\trhs_pipeline->AddDependency(lhs_pipeline);\n-\n-\t\t\tpipelines.emplace_back(move(lhs_pipeline));\n-\t\t\tpipelines.emplace_back(move(rhs_pipeline));\n-\n-\t\t\t// Now build both and scan\n-\t\t\tcurrent->source = op;\n-\t\t\treturn;\n-\t\t}\n-\t\tcase PhysicalOperatorType::DELIM_JOIN: {\n-\t\t\t// duplicate eliminated join\n-\t\t\t// for delim joins, recurse into the actual join\n-\t\t\tpipeline_child = op->children[0].get();\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase PhysicalOperatorType::RECURSIVE_CTE: {\n-\t\t\tauto &cte_node = (PhysicalRecursiveCTE &)*op;\n-\n-\t\t\t// recursive CTE\n-\t\t\tcurrent->source = op;\n-\t\t\t// the LHS of the recursive CTE is our initial state\n-\t\t\t// we build this pipeline as normal\n-\t\t\tpipeline_child = op->children[0].get();\n-\t\t\t// for the RHS, we gather all pipelines that depend on the recursive cte\n-\t\t\t// these pipelines need to be rerun\n-\t\t\tif (recursive_cte) {\n-\t\t\t\tthrow InternalException(\"Recursive CTE detected WITHIN a recursive CTE node\");\n-\t\t\t}\n-\t\t\trecursive_cte = op;\n-\n-\t\t\tauto recursive_pipeline = make_shared<Pipeline>(*this);\n-\t\t\trecursive_pipeline->sink = op;\n-\t\t\top->sink_state.reset();\n-\t\t\tBuildPipelines(op->children[1].get(), recursive_pipeline.get());\n-\n-\t\t\tcte_node.pipelines.push_back(move(recursive_pipeline));\n-\n-\t\t\trecursive_cte = nullptr;\n-\t\t\tbreak;\n-\t\t}\n-\t\tdefault:\n-\t\t\tthrow InternalException(\"Unimplemented sink type!\");\n-\t\t}\n-\t\t// the current is dependent on this pipeline to complete\n-\t\tauto pipeline = make_shared<Pipeline>(*this);\n-\t\tpipeline->sink = op;\n-\t\tcurrent->AddDependency(pipeline);\n-\t\tD_ASSERT(pipeline_child);\n-\t\t// recurse into the pipeline child\n-\t\tBuildPipelines(pipeline_child, pipeline.get());\n-\t\tif (op->type == PhysicalOperatorType::DELIM_JOIN) {\n-\t\t\t// for delim joins, recurse into the actual join\n-\t\t\t// any pipelines in there depend on the main pipeline\n-\t\t\tauto &delim_join = (PhysicalDelimJoin &)*op;\n-\t\t\t// any scan of the duplicate eliminated data on the RHS depends on this pipeline\n-\t\t\t// we add an entry to the mapping of (PhysicalOperator*) -> (Pipeline*)\n-\t\t\tfor (auto &delim_scan : delim_join.delim_scans) {\n-\t\t\t\tdelim_join_dependencies[delim_scan] = pipeline.get();\n-\t\t\t}\n-\t\t\tBuildPipelines(delim_join.join.get(), current);\n-\t\t}\n-\t\tif (!recursive_cte) {\n-\t\t\t// regular pipeline: schedule it\n-\t\t\tpipelines.push_back(move(pipeline));\n-\t\t} else {\n-\t\t\t// CTE pipeline! add it to the CTE pipelines\n-\t\t\tD_ASSERT(recursive_cte);\n-\t\t\tauto &cte = (PhysicalRecursiveCTE &)*recursive_cte;\n-\t\t\tcte.pipelines.push_back(move(pipeline));\n-\t\t}\n-\t} else {\n-\t\t// operator is not a sink! recurse in children\n-\t\t// first check if there is any additional action we need to do depending on the type\n-\t\tswitch (op->type) {\n-\t\tcase PhysicalOperatorType::DELIM_SCAN: {\n-\t\t\tD_ASSERT(op->children.empty());\n-\t\t\tauto entry = delim_join_dependencies.find(op);\n-\t\t\tD_ASSERT(entry != delim_join_dependencies.end());\n-\t\t\t// this chunk scan introduces a dependency to the current pipeline\n-\t\t\t// namely a dependency on the duplicate elimination pipeline to finish\n-\t\t\tauto delim_dependency = entry->second->shared_from_this();\n-\t\t\tD_ASSERT(delim_dependency->sink->type == PhysicalOperatorType::DELIM_JOIN);\n-\t\t\tauto &delim_join = (PhysicalDelimJoin &)*delim_dependency->sink;\n-\t\t\tcurrent->AddDependency(delim_dependency);\n-\t\t\tcurrent->source = (PhysicalOperator *)delim_join.distinct.get();\n-\t\t\treturn;\n-\t\t}\n-\t\tcase PhysicalOperatorType::EXECUTE: {\n-\t\t\t// EXECUTE statement: build pipeline on child\n-\t\t\tauto &execute = (PhysicalExecute &)*op;\n-\t\t\tBuildPipelines(execute.plan, current);\n-\t\t\treturn;\n-\t\t}\n-\t\tcase PhysicalOperatorType::RECURSIVE_CTE_SCAN: {\n-\t\t\tif (!recursive_cte) {\n-\t\t\t\tthrow InternalException(\"Recursive CTE scan found without recursive CTE node\");\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase PhysicalOperatorType::INDEX_JOIN: {\n-\t\t\t// index join: we only continue into the LHS\n-\t\t\t// the right side is probed by the index join\n-\t\t\t// so we don't need to do anything in the pipeline with this child\n-\t\t\tcurrent->operators.push_back(op);\n-\t\t\tBuildPipelines(op->children[0].get(), current);\n-\t\t\treturn;\n-\t\t}\n-\t\tcase PhysicalOperatorType::UNION: {\n-\t\t\tif (recursive_cte) {\n-\t\t\t\tthrow NotImplementedException(\"UNIONS are not supported in recursive CTEs yet\");\n-\t\t\t}\n-\t\t\tauto union_pipeline = make_shared<Pipeline>(*this);\n-\t\t\tauto pipeline_ptr = union_pipeline.get();\n-\t\t\t// set up dependencies for any child pipelines to this union pipeline\n-\t\t\tauto child_entry = child_pipelines.find(current);\n-\t\t\tif (child_entry != child_pipelines.end()) {\n-\t\t\t\tfor (auto &current_child : child_entry->second) {\n-\t\t\t\t\tD_ASSERT(child_dependencies.find(current_child.get()) != child_dependencies.end());\n-\t\t\t\t\tchild_dependencies[current_child.get()].push_back(pipeline_ptr);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\t// for the current pipeline, continue building on the LHS\n-\t\t\tunion_pipeline->operators = current->operators;\n-\t\t\tBuildPipelines(op->children[0].get(), current);\n-\t\t\t// insert the union pipeline as a union pipeline of the current node\n-\t\t\tunion_pipelines[current].push_back(move(union_pipeline));\n-\n-\t\t\t// for the union pipeline, build on the RHS\n-\t\t\tpipeline_ptr->sink = current->sink;\n-\t\t\tBuildPipelines(op->children[1].get(), pipeline_ptr);\n-\t\t\treturn;\n-\t\t}\n-\t\tdefault:\n-\t\t\tbreak;\n-\t\t}\n-\t\tif (op->children.empty()) {\n-\t\t\t// source\n-\t\t\tcurrent->source = op;\n-\t\t} else {\n-\t\t\tif (op->children.size() != 1) {\n-\t\t\t\tthrow InternalException(\"Operator not supported yet\");\n-\t\t\t}\n-\t\t\tcurrent->operators.push_back(op);\n-\t\t\tBuildPipelines(op->children[0].get(), current);\n-\t\t}\n-\t}\n-}\n-\n vector<LogicalType> Executor::GetTypes() {\n \tD_ASSERT(physical_plan);\n \treturn physical_plan->GetTypes();\n@@ -721,6 +500,17 @@ bool Executor::GetPipelinesProgress(double &current_progress) { // LCOV_EXCL_STA\n \t}\n } // LCOV_EXCL_STOP\n \n+bool Executor::HasResultCollector() {\n+\treturn physical_plan->type == PhysicalOperatorType::RESULT_COLLECTOR;\n+}\n+\n+unique_ptr<QueryResult> Executor::GetResult() {\n+\tD_ASSERT(HasResultCollector());\n+\tauto &result_collector = (PhysicalResultCollector &)*physical_plan;\n+\tD_ASSERT(result_collector.sink_state);\n+\treturn result_collector.GetResult(*result_collector.sink_state);\n+}\n+\n unique_ptr<DataChunk> Executor::FetchChunk() {\n \tD_ASSERT(physical_plan);\n \ndiff --git a/src/parallel/pipeline.cpp b/src/parallel/pipeline.cpp\nindex 55cade87ba16..0945f13c9cba 100644\n--- a/src/parallel/pipeline.cpp\n+++ b/src/parallel/pipeline.cpp\n@@ -8,11 +8,7 @@\n #include \"duckdb/main/database.hpp\"\n \n #include \"duckdb/execution/operator/aggregate/physical_simple_aggregate.hpp\"\n-#include \"duckdb/execution/operator/aggregate/physical_window.hpp\"\n #include \"duckdb/execution/operator/scan/physical_table_scan.hpp\"\n-#include \"duckdb/execution/operator/order/physical_order.hpp\"\n-#include \"duckdb/execution/operator/aggregate/physical_hash_aggregate.hpp\"\n-#include \"duckdb/execution/operator/join/physical_hash_join.hpp\"\n #include \"duckdb/parallel/pipeline_executor.hpp\"\n #include \"duckdb/parallel/pipeline_event.hpp\"\n \n@@ -112,6 +108,7 @@ void Pipeline::ScheduleSequentialTask(shared_ptr<Event> &event) {\n }\n \n bool Pipeline::ScheduleParallel(shared_ptr<Event> &event) {\n+\t// check if the sink, source and all intermediate operators support parallelism\n \tif (!sink->ParallelSink()) {\n \t\treturn false;\n \t}\n@@ -123,10 +120,35 @@ bool Pipeline::ScheduleParallel(shared_ptr<Event> &event) {\n \t\t\treturn false;\n \t\t}\n \t}\n+\tif (sink->RequiresBatchIndex()) {\n+\t\tif (!source->SupportsBatchIndex()) {\n+\t\t\tthrow InternalException(\n+\t\t\t    \"Attempting to schedule a pipeline where the sink requires batch index but source does not support it\");\n+\t\t}\n+\t}\n \tidx_t max_threads = source_state->MaxThreads();\n \treturn LaunchScanTasks(event, max_threads);\n }\n \n+bool Pipeline::IsOrderDependent() const {\n+\tauto &config = DBConfig::GetConfig(executor.context);\n+\tif (!config.preserve_insertion_order) {\n+\t\treturn false;\n+\t}\n+\tif (sink && sink->IsOrderDependent()) {\n+\t\treturn true;\n+\t}\n+\tif (source->IsOrderDependent()) {\n+\t\treturn true;\n+\t}\n+\tfor (auto &op : operators) {\n+\t\tif (op->IsOrderDependent()) {\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n void Pipeline::Schedule(shared_ptr<Event> &event) {\n \tD_ASSERT(ready);\n \tD_ASSERT(sink);\n@@ -224,4 +246,57 @@ vector<PhysicalOperator *> Pipeline::GetOperators() const {\n \treturn result;\n }\n \n+//===--------------------------------------------------------------------===//\n+// Pipeline Build State\n+//===--------------------------------------------------------------------===//\n+void PipelineBuildState::SetPipelineSource(Pipeline &pipeline, PhysicalOperator *op) {\n+\tpipeline.source = op;\n+}\n+\n+void PipelineBuildState::SetPipelineSink(Pipeline &pipeline, PhysicalOperator *op) {\n+\tpipeline.sink = op;\n+\t// set the base batch index of this pipeline based on how many other pipelines have this node as their sink\n+\tpipeline.base_batch_index = BATCH_INCREMENT * sink_pipeline_count[op];\n+\t// increment the number of nodes that have this pipeline as their sink\n+\tsink_pipeline_count[op]++;\n+}\n+\n+void PipelineBuildState::AddPipelineOperator(Pipeline &pipeline, PhysicalOperator *op) {\n+\tpipeline.operators.push_back(op);\n+}\n+\n+void PipelineBuildState::AddPipeline(Executor &executor, shared_ptr<Pipeline> pipeline) {\n+\texecutor.pipelines.push_back(move(pipeline));\n+}\n+\n+PhysicalOperator *PipelineBuildState::GetPipelineSource(Pipeline &pipeline) {\n+\treturn pipeline.source;\n+}\n+\n+PhysicalOperator *PipelineBuildState::GetPipelineSink(Pipeline &pipeline) {\n+\treturn pipeline.sink;\n+}\n+\n+void PipelineBuildState::SetPipelineOperators(Pipeline &pipeline, vector<PhysicalOperator *> operators) {\n+\tpipeline.operators = move(operators);\n+}\n+\n+void PipelineBuildState::AddChildPipeline(Executor &executor, Pipeline &pipeline) {\n+\texecutor.AddChildPipeline(&pipeline);\n+}\n+\n+unordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> &PipelineBuildState::GetUnionPipelines(Executor &executor) {\n+\treturn executor.union_pipelines;\n+}\n+unordered_map<Pipeline *, vector<shared_ptr<Pipeline>>> &PipelineBuildState::GetChildPipelines(Executor &executor) {\n+\treturn executor.child_pipelines;\n+}\n+unordered_map<Pipeline *, vector<Pipeline *>> &PipelineBuildState::GetChildDependencies(Executor &executor) {\n+\treturn executor.child_dependencies;\n+}\n+\n+vector<PhysicalOperator *> PipelineBuildState::GetPipelineOperators(Pipeline &pipeline) {\n+\treturn pipeline.operators;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/parallel/pipeline_executor.cpp b/src/parallel/pipeline_executor.cpp\nindex ce4809a1ada4..e62fc64e12d7 100644\n--- a/src/parallel/pipeline_executor.cpp\n+++ b/src/parallel/pipeline_executor.cpp\n@@ -1,5 +1,6 @@\n #include \"duckdb/parallel/pipeline_executor.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n+#include \"duckdb/common/limits.hpp\"\n \n namespace duckdb {\n \n@@ -9,7 +10,9 @@ PipelineExecutor::PipelineExecutor(ClientContext &context_p, Pipeline &pipeline_\n \tlocal_source_state = pipeline.source->GetLocalSourceState(context, *pipeline.source_state);\n \tif (pipeline.sink) {\n \t\tlocal_sink_state = pipeline.sink->GetLocalSinkState(context);\n+\t\trequires_batch_index = pipeline.sink->RequiresBatchIndex() && pipeline.source->SupportsBatchIndex();\n \t}\n+\tbool can_cache_in_pipeline = pipeline.sink && !pipeline.IsOrderDependent() && !requires_batch_index;\n \tintermediate_chunks.reserve(pipeline.operators.size());\n \tintermediate_states.reserve(pipeline.operators.size());\n \tcached_chunks.resize(pipeline.operators.size());\n@@ -20,7 +23,7 @@ PipelineExecutor::PipelineExecutor(ClientContext &context_p, Pipeline &pipeline_\n \t\tchunk->Initialize(prev_operator->GetTypes());\n \t\tintermediate_chunks.push_back(move(chunk));\n \t\tintermediate_states.push_back(current_operator->GetOperatorState(context.client));\n-\t\tif (pipeline.sink && !pipeline.sink->SinkOrderMatters() && current_operator->RequiresCache()) {\n+\t\tif (can_cache_in_pipeline && current_operator->RequiresCache()) {\n \t\t\tauto &cache_types = current_operator->GetTypes();\n \t\t\tbool can_cache = true;\n \t\t\tfor (auto &type : cache_types) {\n@@ -38,7 +41,7 @@ PipelineExecutor::PipelineExecutor(ClientContext &context_p, Pipeline &pipeline_\n \t\tif (current_operator->IsSink() && current_operator->sink_state->state == SinkFinalizeType::NO_OUTPUT_POSSIBLE) {\n \t\t\t// one of the operators has already figured out no output is possible\n \t\t\t// we can skip executing the pipeline\n-\t\t\tfinished_processing = true;\n+\t\t\tFinishProcessing();\n \t\t}\n \t}\n \tInitializeChunk(final_chunk);\n@@ -49,7 +52,7 @@ bool PipelineExecutor::Execute(idx_t max_chunks) {\n \tbool exhausted_source = false;\n \tauto &source_chunk = pipeline.operators.empty() ? final_chunk : *intermediate_chunks[0];\n \tfor (idx_t i = 0; i < max_chunks; i++) {\n-\t\tif (finished_processing) {\n+\t\tif (IsFinished()) {\n \t\t\tbreak;\n \t\t}\n \t\tsource_chunk.Reset();\n@@ -60,11 +63,11 @@ bool PipelineExecutor::Execute(idx_t max_chunks) {\n \t\t}\n \t\tauto result = ExecutePushInternal(source_chunk);\n \t\tif (result == OperatorResultType::FINISHED) {\n-\t\t\tfinished_processing = true;\n+\t\t\tD_ASSERT(IsFinished());\n \t\t\tbreak;\n \t\t}\n \t}\n-\tif (!exhausted_source && !finished_processing) {\n+\tif (!exhausted_source && !IsFinished()) {\n \t\treturn false;\n \t}\n \tPushFinalize();\n@@ -79,6 +82,15 @@ OperatorResultType PipelineExecutor::ExecutePush(DataChunk &input) { // LCOV_EXC\n \treturn ExecutePushInternal(input);\n } // LCOV_EXCL_STOP\n \n+void PipelineExecutor::FinishProcessing(int32_t operator_idx) {\n+\tfinished_processing_idx = operator_idx < 0 ? NumericLimits<int32_t>::Maximum() : operator_idx;\n+\tin_process_operators = stack<idx_t>();\n+}\n+\n+bool PipelineExecutor::IsFinished() {\n+\treturn finished_processing_idx >= 0;\n+}\n+\n OperatorResultType PipelineExecutor::ExecutePushInternal(DataChunk &input, idx_t initial_idx) {\n \tD_ASSERT(pipeline.sink);\n \tif (input.size() == 0) { // LCOV_EXCL_START\n@@ -103,6 +115,7 @@ OperatorResultType PipelineExecutor::ExecutePushInternal(DataChunk &input, idx_t\n \t\t\tauto sink_result = pipeline.sink->Sink(context, *pipeline.sink->sink_state, *local_sink_state, sink_chunk);\n \t\t\tEndOperator(pipeline.sink, nullptr);\n \t\t\tif (sink_result == SinkResultType::FINISHED) {\n+\t\t\t\tFinishProcessing();\n \t\t\t\treturn OperatorResultType::FINISHED;\n \t\t\t}\n \t\t}\n@@ -110,7 +123,6 @@ OperatorResultType PipelineExecutor::ExecutePushInternal(DataChunk &input, idx_t\n \t\t\treturn OperatorResultType::NEED_MORE_INPUT;\n \t\t}\n \t}\n-\treturn OperatorResultType::FINISHED;\n }\n \n void PipelineExecutor::PushFinalize() {\n@@ -119,13 +131,15 @@ void PipelineExecutor::PushFinalize() {\n \t}\n \tfinalized = true;\n \t// flush all caches\n-\tif (!finished_processing) {\n-\t\tD_ASSERT(in_process_operators.empty());\n-\t\tfor (idx_t i = 0; i < cached_chunks.size(); i++) {\n-\t\t\tif (cached_chunks[i] && cached_chunks[i]->size() > 0) {\n-\t\t\t\tExecutePushInternal(*cached_chunks[i], i + 1);\n-\t\t\t\tcached_chunks[i].reset();\n-\t\t\t}\n+\t// note that even if an operator has finished, we might still need to flush caches AFTER that operator\n+\t// e.g. if we have SOURCE -> LIMIT -> CROSS_PRODUCT -> SINK, if the LIMIT reports no more rows will be passed on\n+\t// we still need to flush caches from the CROSS_PRODUCT\n+\tD_ASSERT(in_process_operators.empty());\n+\tidx_t start_idx = IsFinished() ? idx_t(finished_processing_idx) : 0;\n+\tfor (idx_t i = start_idx; i < cached_chunks.size(); i++) {\n+\t\tif (cached_chunks[i] && cached_chunks[i]->size() > 0) {\n+\t\t\tExecutePushInternal(*cached_chunks[i], i + 1);\n+\t\t\tcached_chunks[i].reset();\n \t\t}\n \t}\n \tD_ASSERT(local_sink_state);\n@@ -181,7 +195,7 @@ void PipelineExecutor::CacheChunk(DataChunk &current_chunk, idx_t operator_idx)\n }\n \n void PipelineExecutor::ExecutePull(DataChunk &result) {\n-\tif (finished_processing) {\n+\tif (IsFinished()) {\n \t\treturn;\n \t}\n \tauto &executor = pipeline.executor;\n@@ -197,7 +211,10 @@ void PipelineExecutor::ExecutePull(DataChunk &result) {\n \t\t\t\t}\n \t\t\t}\n \t\t\tif (!pipeline.operators.empty()) {\n-\t\t\t\tExecute(source_chunk, result);\n+\t\t\t\tauto state = Execute(source_chunk, result);\n+\t\t\t\tif (state == OperatorResultType::FINISHED) {\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \t} catch (std::exception &ex) { // LCOV_EXCL_START\n@@ -281,6 +298,7 @@ OperatorResultType PipelineExecutor::Execute(DataChunk &input, DataChunk &result\n \t\t\t\tin_process_operators.push(current_idx);\n \t\t\t} else if (result == OperatorResultType::FINISHED) {\n \t\t\t\tD_ASSERT(current_chunk.size() == 0);\n+\t\t\t\tFinishProcessing(current_idx);\n \t\t\t\treturn OperatorResultType::FINISHED;\n \t\t\t}\n \t\t\tcurrent_chunk.Verify();\n@@ -314,6 +332,14 @@ OperatorResultType PipelineExecutor::Execute(DataChunk &input, DataChunk &result\n void PipelineExecutor::FetchFromSource(DataChunk &result) {\n \tStartOperator(pipeline.source);\n \tpipeline.source->GetData(context, result, *pipeline.source_state, *local_source_state);\n+\tif (result.size() != 0 && requires_batch_index) {\n+\t\tauto next_batch_index =\n+\t\t    pipeline.source->GetBatchIndex(context, result, *pipeline.source_state, *local_source_state);\n+\t\tnext_batch_index += pipeline.base_batch_index;\n+\t\tD_ASSERT(local_sink_state->batch_index <= next_batch_index ||\n+\t\t         local_sink_state->batch_index == DConstants::INVALID_INDEX);\n+\t\tlocal_sink_state->batch_index = next_batch_index;\n+\t}\n \tEndOperator(pipeline.source, &result);\n }\n \ndiff --git a/tools/pythonpkg/duckdb_python.cpp b/tools/pythonpkg/duckdb_python.cpp\nindex ab97295847c2..d3b69bf861a2 100644\n--- a/tools/pythonpkg/duckdb_python.cpp\n+++ b/tools/pythonpkg/duckdb_python.cpp\n@@ -5,7 +5,6 @@\n #include \"duckdb/common/vector.hpp\"\n \n #include \"duckdb_python/array_wrapper.hpp\"\n-#include \"duckdb_python/pandas_scan.hpp\"\n #include \"duckdb_python/pyconnection.hpp\"\n #include \"duckdb_python/pyrelation.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp b/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp\nindex cb1b69be8193..bc2480b4e9cc 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp\n@@ -56,6 +56,9 @@ struct PandasScanFunction : public TableFunction {\n \n \tstatic unique_ptr<NodeStatistics> PandasScanCardinality(ClientContext &context, const FunctionData *bind_data);\n \n+\tstatic idx_t PandasScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n+\t                                     FunctionOperatorData *operator_state, ParallelState *parallel_state_p);\n+\n \t// Helper function that transform pandas df names to make them work with our binder\n \tstatic py::object PandasReplaceCopiedNames(const py::object &original_df);\n };\ndiff --git a/tools/pythonpkg/src/pandas_scan.cpp b/tools/pythonpkg/src/pandas_scan.cpp\nindex 671d47ece76b..6aff3067f3bd 100644\n--- a/tools/pythonpkg/src/pandas_scan.cpp\n+++ b/tools/pythonpkg/src/pandas_scan.cpp\n@@ -29,20 +29,22 @@ struct PandasScanFunctionData : public TableFunctionData {\n };\n \n struct PandasScanState : public FunctionOperatorData {\n-\tPandasScanState(idx_t start, idx_t end) : start(start), end(end) {\n+\tPandasScanState(idx_t start, idx_t end) : start(start), end(end), batch_index(0) {\n \t}\n \n \tidx_t start;\n \tidx_t end;\n+\tidx_t batch_index;\n \tvector<column_t> column_ids;\n };\n \n struct ParallelPandasScanState : public ParallelState {\n-\tParallelPandasScanState() : position(0) {\n+\tParallelPandasScanState() : position(0), batch_index(0) {\n \t}\n \n \tstd::mutex lock;\n \tidx_t position;\n+\tidx_t batch_index;\n };\n \n PandasScanFunction::PandasScanFunction()\n@@ -50,6 +52,15 @@ PandasScanFunction::PandasScanFunction()\n                     nullptr, nullptr, PandasScanCardinality, nullptr, nullptr, PandasScanMaxThreads,\n                     PandasScanInitParallelState, PandasScanFuncParallel, PandasScanParallelInit,\n                     PandasScanParallelStateNext, true, false, PandasProgress) {\n+\tget_batch_index = PandasScanGetBatchIndex;\n+\tsupports_batch_index = true;\n+}\n+\n+idx_t PandasScanFunction::PandasScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n+                                                  FunctionOperatorData *operator_state,\n+                                                  ParallelState *parallel_state_p) {\n+\tauto &data = (PandasScanState &)*operator_state;\n+\treturn data.batch_index;\n }\n \n unique_ptr<FunctionData> PandasScanFunction::PandasScanBind(ClientContext &context, TableFunctionBindInput &input,\n@@ -122,6 +133,7 @@ bool PandasScanFunction::PandasScanParallelStateNext(ClientContext &context, con\n \t\tparallel_state.position = bind_data.row_count;\n \t}\n \tstate.end = parallel_state.position;\n+\tstate.batch_index = parallel_state.batch_index++;\n \treturn true;\n }\n \n",
  "test_patch": "diff --git a/test/api/test_pending_query.cpp b/test/api/test_pending_query.cpp\nindex 9c62faed6de7..e2c7f6f151c3 100644\n--- a/test/api/test_pending_query.cpp\n+++ b/test/api/test_pending_query.cpp\n@@ -18,20 +18,20 @@ TEST_CASE(\"Test Pending Query API\", \"[api]\") {\n \n \t\t// cannot fetch twice from the same pending query\n \t\tREQUIRE_THROWS(pending_query->Execute());\n-\t\tREQUIRE_THROWS(pending_query->Execute(true));\n+\t\tREQUIRE_THROWS(pending_query->Execute());\n \n \t\t// query the connection as normal after\n \t\tresult = con.Query(\"SELECT 42\");\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {42}));\n \t}\n \tSECTION(\"Streaming result\") {\n-\t\tauto pending_query = con.PendingQuery(\"SELECT SUM(i) FROM range(1000000) tbl(i)\");\n+\t\tauto pending_query = con.PendingQuery(\"SELECT SUM(i) FROM range(1000000) tbl(i)\", true);\n \t\tREQUIRE(pending_query->success);\n-\t\tauto result = pending_query->Execute(true);\n+\t\tauto result = pending_query->Execute();\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {Value::BIGINT(499999500000)}));\n \n \t\t// cannot fetch twice from the same pending query\n-\t\tREQUIRE_THROWS(pending_query->Execute(true));\n+\t\tREQUIRE_THROWS(pending_query->Execute());\n \t\tREQUIRE_THROWS(pending_query->Execute());\n \n \t\t// query the connection as normal after\n@@ -39,15 +39,15 @@ TEST_CASE(\"Test Pending Query API\", \"[api]\") {\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {42}));\n \t}\n \tSECTION(\"Execute tasks\") {\n-\t\tauto pending_query = con.PendingQuery(\"SELECT SUM(i) FROM range(1000000) tbl(i)\");\n+\t\tauto pending_query = con.PendingQuery(\"SELECT SUM(i) FROM range(1000000) tbl(i)\", true);\n \t\twhile (pending_query->ExecuteTask() == PendingExecutionResult::RESULT_NOT_READY)\n \t\t\t;\n \t\tREQUIRE(pending_query->success);\n-\t\tauto result = pending_query->Execute(true);\n+\t\tauto result = pending_query->Execute();\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {Value::BIGINT(499999500000)}));\n \n \t\t// cannot fetch twice from the same pending query\n-\t\tREQUIRE_THROWS(pending_query->Execute(true));\n+\t\tREQUIRE_THROWS(pending_query->Execute());\n \n \t\t// query the connection as normal after\n \t\tresult = con.Query(\"SELECT 42\");\n@@ -55,14 +55,14 @@ TEST_CASE(\"Test Pending Query API\", \"[api]\") {\n \t}\n \tSECTION(\"Create pending query while another pending query exists\") {\n \t\tauto pending_query = con.PendingQuery(\"SELECT SUM(i) FROM range(1000000) tbl(i)\");\n-\t\tauto pending_query2 = con.PendingQuery(\"SELECT SUM(i) FROM range(1000000) tbl(i)\");\n+\t\tauto pending_query2 = con.PendingQuery(\"SELECT SUM(i) FROM range(1000000) tbl(i)\", true);\n \n \t\t// first pending query is now closed\n \t\tREQUIRE_THROWS(pending_query->ExecuteTask());\n \t\tREQUIRE_THROWS(pending_query->Execute());\n \n \t\t// we can execute the second one\n-\t\tauto result = pending_query2->Execute(true);\n+\t\tauto result = pending_query2->Execute();\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {Value::BIGINT(499999500000)}));\n \n \t\t// query the connection as normal after\n@@ -95,10 +95,10 @@ TEST_CASE(\"Test Pending Query API\", \"[api]\") {\n \tSECTION(\"Runtime error in pending query (streaming)\") {\n \t\t// this succeeds initially\n \t\tauto pending_query =\n-\t\t    con.PendingQuery(\"SELECT concat(SUM(i)::varchar, 'hello')::INT FROM range(1000000) tbl(i)\");\n+\t\t    con.PendingQuery(\"SELECT concat(SUM(i)::varchar, 'hello')::INT FROM range(1000000) tbl(i)\", true);\n \t\tREQUIRE(pending_query->success);\n \t\t// still succeeds...\n-\t\tauto result = pending_query->Execute(true);\n+\t\tauto result = pending_query->Execute();\n \t\tREQUIRE(result->success);\n \t\tauto chunk = result->Fetch();\n \t\tREQUIRE(!chunk);\n@@ -161,7 +161,7 @@ TEST_CASE(\"Test Pending Query Prepared Statements API\", \"[api]\") {\n \n \t\t// cannot fetch twice from the same pending query\n \t\tREQUIRE_THROWS(pending_query->Execute());\n-\t\tREQUIRE_THROWS(pending_query->Execute(true));\n+\t\tREQUIRE_THROWS(pending_query->Execute());\n \n \t\t// we can use the prepared query again, however\n \t\tpending_query = prepare->PendingQuery(500000);\n@@ -172,7 +172,7 @@ TEST_CASE(\"Test Pending Query Prepared Statements API\", \"[api]\") {\n \n \t\t// cannot fetch twice from the same pending query\n \t\tREQUIRE_THROWS(pending_query->Execute());\n-\t\tREQUIRE_THROWS(pending_query->Execute(true));\n+\t\tREQUIRE_THROWS(pending_query->Execute());\n \t}\n \tSECTION(\"Error during prepare\") {\n \t\tauto prepare = con.Prepare(\"SELECT SUM(i+X) FROM range(1000000) tbl(i) WHERE i>=$1\");\n@@ -181,13 +181,15 @@ TEST_CASE(\"Test Pending Query Prepared Statements API\", \"[api]\") {\n \t\tREQUIRE_THROWS(prepare->PendingQuery(0));\n \t}\n \tSECTION(\"Error during execution\") {\n+\t\tvector<Value> parameters;\n \t\tauto prepared = con.Prepare(\"SELECT concat(SUM(i)::varchar, CASE WHEN SUM(i) IS NULL THEN 0 ELSE 'hello' \"\n \t\t                            \"END)::INT FROM range(1000000) tbl(i) WHERE i>$1\");\n \t\t// this succeeds initially\n-\t\tauto pending_query = prepared->PendingQuery(0);\n+\t\tparameters = {Value::INTEGER(0)};\n+\t\tauto pending_query = prepared->PendingQuery(parameters, true);\n \t\tREQUIRE(pending_query->success);\n \t\t// still succeeds...\n-\t\tauto result = pending_query->Execute(true);\n+\t\tauto result = pending_query->Execute();\n \t\tREQUIRE(result->success);\n \t\t//! fail!\n \t\tauto chunk = result->Fetch();\n@@ -199,10 +201,11 @@ TEST_CASE(\"Test Pending Query Prepared Statements API\", \"[api]\") {\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {42}));\n \n \t\t// if we change the parameter this works\n-\t\tpending_query = prepared->PendingQuery(2000000);\n+\t\tparameters = {Value::INTEGER(2000000)};\n+\t\tpending_query = prepared->PendingQuery(parameters, true);\n \t\tREQUIRE(pending_query->success);\n \t\t// still succeeds...\n-\t\tresult = pending_query->Execute(true);\n+\t\tresult = pending_query->Execute();\n \t\tREQUIRE(result->success);\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {Value::BIGINT(0)}));\n \t}\ndiff --git a/test/issues/general/test_1248.test b/test/issues/general/test_1248.test\nindex 22fddd925012..a7f122544c2e 100644\n--- a/test/issues/general/test_1248.test\n+++ b/test/issues/general/test_1248.test\n@@ -13,7 +13,7 @@ with test(round) as (\n )\n select test.round\n from\n-    (select round from test limit 1) as subselect, \n+    (select round from test limit 1) as subselect,\n     test;\n \n query I\n@@ -22,7 +22,7 @@ with test(round) as (\n )\n select test.round\n from\n-    (select round from test limit 1) as subselect, \n+    (select round from test limit 1) as subselect,\n     test;\n ----\n 0\n@@ -71,6 +71,19 @@ from\n 20\n 21\n \n+query I\n+with recursive test(round) as (\n+    select 0\n+    union all\n+    select round+1 from test where round <= 20\n+)\n+select count(*)\n+from\n+    (select round from test limit 1) as subselect,\n+    test;\n+----\n+22\n+\n query II\n with recursive test(round) as (\n     select 0\ndiff --git a/test/sql/aggregate/aggregates/test_state_export.test b/test/sql/aggregate/aggregates/test_state_export.test\nindex a2f2fad0e7e9..6396310706f7 100644\n--- a/test/sql/aggregate/aggregates/test_state_export.test\n+++ b/test/sql/aggregate/aggregates/test_state_export.test\n@@ -57,12 +57,12 @@ select g, finalize(combine(sum(d) EXPORT_STATE, sum_state)) combined_sum from du\n statement ok\n CREATE TABLE state2 AS SELECT g, sum(d) EXPORT_STATE sum_state FROM dummy WHERE g < 5 GROUP BY g ORDER BY g;\n \n-query II nosort res3\n+query II rowsort res3\n select g, finalize(sum_state) * 2 combined_sum from (select g, sum(d) EXPORT_STATE sum_state from dummy where g >= 5 GROUP BY g union all SELECT * FROM state2) ORDER BY g;\n ----\n \n # combine aggregate states in JOINs with NULLs\n-query II nosort res3\n+query II rowsort res3\n with groups as (select distinct g from dummy)\n select g, FINALIZE(COMBINE(sum_state, sum_state2)) * 2 from groups left join state2 using(g) left join (select g, sum(d) EXPORT_STATE sum_state2 from dummy where g >= 5 GROUP BY g) using (g)\n ----\ndiff --git a/test/sql/copy/parquet/parquet_parallel_limit.test_slow b/test/sql/copy/parquet/parquet_parallel_limit.test_slow\nnew file mode 100644\nindex 000000000000..ee86fb0dddf0\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_parallel_limit.test_slow\n@@ -0,0 +1,77 @@\n+# name: test/sql/copy/parquet/parquet_parallel_limit.test_slow\n+# description: Test reading parquet files with parallel LIMIT execution\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 1779793 UNION ALL SELECT 4779793;\n+\n+statement ok\n+COPY (SELECT * FROM range(10000000) tbl(i)) TO '__TEST_DIR__/integers.parquet' (FORMAT PARQUET);\n+\n+statement ok\n+CREATE VIEW integers AS SELECT * FROM '__TEST_DIR__/integers.parquet'\n+\n+query I\n+SELECT * FROM integers LIMIT 5\n+----\n+0\n+1\n+2\n+3\n+4\n+\n+query I\n+SELECT * FROM integers WHERE i>1978321 OR i=334 LIMIT 5\n+----\n+334\n+1978322\n+1978323\n+1978324\n+1978325\n+\n+query I\n+SELECT * FROM integers WHERE i>1978321 LIMIT 5\n+----\n+1978322\n+1978323\n+1978324\n+1978325\n+1978326\n+\n+query I\n+SELECT * FROM integers WHERE i>4978321 LIMIT 5\n+----\n+4978322\n+4978323\n+4978324\n+4978325\n+4978326\n+\n+# IN-clause (semi join)\n+query I\n+SELECT * FROM integers WHERE i IN (SELECT * FROM other_table)\n+----\n+337\n+948247\n+1779793\n+4779793\n+\n+# UNION\n+query I\n+(SELECT * FROM integers WHERE i>1978321 LIMIT 5) UNION ALL (SELECT * FROM integers WHERE i>4978321 LIMIT 5)\n+----\n+1978322\n+1978323\n+1978324\n+1978325\n+1978326\n+4978322\n+4978323\n+4978324\n+4978325\n+4978326\ndiff --git a/test/sql/copy/parquet/parquet_parallel_limit_glob.test_slow b/test/sql/copy/parquet/parquet_parallel_limit_glob.test_slow\nnew file mode 100644\nindex 000000000000..151cecacbec9\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_parallel_limit_glob.test_slow\n@@ -0,0 +1,78 @@\n+# name: test/sql/copy/parquet/parquet_parallel_limit_glob.test_slow\n+# description: Test reading parquet files with parallel LIMIT execution with multiple input files\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 1779793 UNION ALL SELECT 4779793;\n+\n+statement ok\n+COPY (SELECT * FROM range(5000000) tbl(i)) TO '__TEST_DIR__/integers1.parquet' (FORMAT PARQUET);\n+COPY (SELECT * FROM range(5000000, 10000000) tbl(i)) TO '__TEST_DIR__/integers2.parquet' (FORMAT PARQUET);\n+\n+statement ok\n+CREATE VIEW integers AS SELECT * FROM parquet_scan(['__TEST_DIR__/integers1.parquet', '__TEST_DIR__/integers2.parquet'])\n+\n+query I\n+SELECT * FROM integers LIMIT 5\n+----\n+0\n+1\n+2\n+3\n+4\n+\n+query I\n+SELECT * FROM integers WHERE i>1978321 OR i=334 LIMIT 5\n+----\n+334\n+1978322\n+1978323\n+1978324\n+1978325\n+\n+query I\n+SELECT * FROM integers WHERE i>1978321 LIMIT 5\n+----\n+1978322\n+1978323\n+1978324\n+1978325\n+1978326\n+\n+query I\n+SELECT * FROM integers WHERE i>4978321 LIMIT 5\n+----\n+4978322\n+4978323\n+4978324\n+4978325\n+4978326\n+\n+# IN-clause (semi join)\n+query I\n+SELECT * FROM integers WHERE i IN (SELECT * FROM other_table)\n+----\n+337\n+948247\n+1779793\n+4779793\n+\n+# UNION\n+query I\n+(SELECT * FROM integers WHERE i>1978321 LIMIT 5) UNION ALL (SELECT * FROM integers WHERE i>4978321 LIMIT 5)\n+----\n+1978322\n+1978323\n+1978324\n+1978325\n+1978326\n+4978322\n+4978323\n+4978324\n+4978325\n+4978326\ndiff --git a/test/sql/limit/parallel_limit_transaction_local.test_slow b/test/sql/limit/parallel_limit_transaction_local.test_slow\nnew file mode 100644\nindex 000000000000..73642ab6f85c\n--- /dev/null\n+++ b/test/sql/limit/parallel_limit_transaction_local.test_slow\n@@ -0,0 +1,87 @@\n+# name: test/sql/limit/parallel_limit_transaction_local.test_slow\n+# description: Test correct behavior of parallel limit in the presence of transaction-local data\n+# group: [limit]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+PRAGMA threads=8\n+\n+statement ok\n+CREATE TABLE integers AS SELECT * FROM range(5000000) tbl(i);\n+\n+statement ok\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 1779793 UNION ALL SELECT 8779793;\n+\n+statement ok\n+BEGIN TRANSACTION\n+\n+statement ok\n+INSERT INTO integers SELECT * FROM range(5000000, 10000000) tbl(i);\n+\n+query I\n+SELECT * FROM integers LIMIT 5\n+----\n+0\n+1\n+2\n+3\n+4\n+\n+query I\n+SELECT * FROM integers WHERE i>5978321 OR i=334 LIMIT 5\n+----\n+334\n+5978322\n+5978323\n+5978324\n+5978325\n+\n+query I\n+SELECT * FROM integers WHERE i>5978321 LIMIT 5\n+----\n+5978322\n+5978323\n+5978324\n+5978325\n+5978326\n+\n+query I\n+SELECT * FROM integers WHERE i>8978321 LIMIT 5\n+----\n+8978322\n+8978323\n+8978324\n+8978325\n+8978326\n+\n+# IN-clause (semi join)\n+query I\n+SELECT * FROM integers WHERE i IN (SELECT * FROM other_table)\n+----\n+337\n+948247\n+1779793\n+8779793\n+\n+\n+query I\n+(SELECT * FROM integers WHERE i>1978321 LIMIT 5) UNION ALL (SELECT * FROM integers WHERE i>8978321 LIMIT 5)\n+----\n+1978322\n+1978323\n+1978324\n+1978325\n+1978326\n+8978322\n+8978323\n+8978324\n+8978325\n+8978326\n+\n+statement ok\n+DROP TABLE integers\n+\n+statement ok\n+ROLLBACK\ndiff --git a/test/sql/limit/test_parallel_limit.test_slow b/test/sql/limit/test_parallel_limit.test_slow\nnew file mode 100644\nindex 000000000000..db525fb57882\n--- /dev/null\n+++ b/test/sql/limit/test_parallel_limit.test_slow\n@@ -0,0 +1,81 @@\n+# name: test/sql/limit/test_parallel_limit.test_slow\n+# description: Test parallel limit execution\n+# group: [limit]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+PRAGMA threads=8\n+\n+statement ok\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 1779793 UNION ALL SELECT 4779793;\n+\n+foreach table_type TABLE VIEW\n+\n+statement ok\n+CREATE ${table_type} integers AS SELECT * FROM range(10000000) tbl(i);\n+\n+query I\n+SELECT * FROM integers LIMIT 5\n+----\n+0\n+1\n+2\n+3\n+4\n+\n+query I\n+SELECT * FROM integers WHERE i>1978321 OR i=334 LIMIT 5\n+----\n+334\n+1978322\n+1978323\n+1978324\n+1978325\n+\n+query I\n+SELECT * FROM integers WHERE i>1978321 LIMIT 5\n+----\n+1978322\n+1978323\n+1978324\n+1978325\n+1978326\n+\n+query I\n+SELECT * FROM integers WHERE i>4978321 LIMIT 5\n+----\n+4978322\n+4978323\n+4978324\n+4978325\n+4978326\n+\n+# IN-clause (semi join)\n+query I\n+SELECT * FROM integers WHERE i IN (SELECT * FROM other_table)\n+----\n+337\n+948247\n+1779793\n+4779793\n+\n+query I\n+(SELECT * FROM integers WHERE i>1978321 LIMIT 5) UNION ALL (SELECT * FROM integers WHERE i>4978321 LIMIT 5)\n+----\n+1978322\n+1978323\n+1978324\n+1978325\n+1978326\n+4978322\n+4978323\n+4978324\n+4978325\n+4978326\n+\n+statement ok\n+DROP ${table_type} integers\n+\n+endloop\ndiff --git a/test/sql/limit/test_preserve_insertion_order.test b/test/sql/limit/test_preserve_insertion_order.test\nnew file mode 100644\nindex 000000000000..2d9bfaec32fb\n--- /dev/null\n+++ b/test/sql/limit/test_preserve_insertion_order.test\n@@ -0,0 +1,53 @@\n+# name: test/sql/limit/test_preserve_insertion_order.test\n+# description: Test limit with preserve insertion order disabled\n+# group: [limit]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+SET preserve_insertion_order=false\n+\n+statement ok\n+CREATE TABLE integers AS SELECT 1 FROM range(1000000)\n+\n+query I\n+SELECT * FROM integers LIMIT 5\n+----\n+1\n+1\n+1\n+1\n+1\n+\n+query I\n+SELECT * FROM integers LIMIT 5 OFFSET 500000\n+----\n+1\n+1\n+1\n+1\n+1\n+\n+statement ok\n+CREATE TABLE integers2 AS SELECT * FROM range(1000000) tbl (i)\n+\n+# we need a rowsort here because we don't preserve insertion order\n+# note that rowsort is a string-based sort\n+query I rowsort\n+SELECT * FROM integers2 WHERE i IN (337, 195723, 442578, 994375)\n+----\n+195723\n+337\n+442578\n+994375\n+\n+# when insertion order is disabled, there is no guarantee on which tuples come out of a LIMIT clause\n+# for the LIMIT test select everything\n+query I rowsort\n+SELECT * FROM integers2 WHERE i IN (337, 195723, 442578, 994375) LIMIT 4\n+----\n+195723\n+337\n+442578\n+994375\ndiff --git a/test/sql/parallelism/intraquery/parallel_materialization.test_slow b/test/sql/parallelism/intraquery/parallel_materialization.test_slow\nnew file mode 100644\nindex 000000000000..5569b27b2031\n--- /dev/null\n+++ b/test/sql/parallelism/intraquery/parallel_materialization.test_slow\n@@ -0,0 +1,81 @@\n+# name: test/sql/parallelism/intraquery/parallel_materialization.test_slow\n+# description: Test parallel materialization of results\n+# group: [intraquery]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+PRAGMA threads=4\n+\n+statement ok\n+PRAGMA verify_parallelism\n+\n+statement ok\n+CREATE TABLE integers(i INTEGER)\n+\n+statement ok\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 1779793 UNION ALL SELECT 4779793;\n+\n+statement ok\n+INSERT INTO integers SELECT * FROM range(2500000)\n+\n+statement ok\n+BEGIN TRANSACTION\n+\n+statement ok\n+INSERT INTO integers SELECT * FROM range(2500000, 5000000) tbl(i);\n+\n+# run these tests twice - once with transaction local data and once without\n+loop i 0 2\n+\n+# IN-clause (semi join)\n+query I\n+SELECT * FROM integers WHERE i IN (SELECT * FROM other_table)\n+----\n+337\n+948247\n+1779793\n+4779793\n+\n+# explicit join\n+query I\n+SELECT * FROM integers JOIN other_table USING(i)\n+----\n+337\n+948247\n+1779793\n+4779793\n+\n+# simple WHERE clause\n+query I\n+SELECT * FROM integers WHERE i > 337 AND i < 340\n+----\n+338\n+339\n+\n+# IN-clause\n+query I\n+SELECT * FROM integers WHERE i IN (337, 948247, 1779793, 4779793, 99999999999999)\n+----\n+337\n+948247\n+1779793\n+4779793\n+\n+# more complex where clause\n+query I\n+SELECT * FROM integers WHERE i=337 OR (i+i>1896494 AND i+i<= 1896498) OR (i*2=9559586)\n+----\n+337\n+948248\n+948249\n+4779793\n+\n+statement ok\n+COMMIT\n+\n+statement ok\n+BEGIN TRANSACTION\n+\n+endloop\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_pandas_limit.py b/tools/pythonpkg/tests/fast/pandas/test_pandas_limit.py\nnew file mode 100644\nindex 000000000000..fcb2d91f0db5\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/pandas/test_pandas_limit.py\n@@ -0,0 +1,13 @@\n+import duckdb\n+import pandas as pd\n+import pytest\n+\n+class TestPandasLimit(object):\n+    def test_pandas_limit(self, duckdb_cursor):\n+        con = duckdb.connect()\n+        df = con.execute('select * from range(10000000) tbl(i)').df()\n+\n+        con.execute('SET threads=8')\n+\n+        limit_df = con.execute('SELECT * FROM df WHERE i=334 OR i>9967864 LIMIT 5').df()\n+        assert list(limit_df['i']) == [334, 9967865, 9967866, 9967867, 9967868]\n",
  "problem_statement": "Parallel result set materialization\nFollow up from #2245 \r\n\r\nMy current idea is that for result sets that are fully materialized we can quite nicely maintain the original order by having the scans also output the current row group id that they are scanning. In the result set construction we can then materialize the result sets aligned along these block ids (e.g. in a map<block_id_t, ChunkCollection>). This could be done fully in parallel as long as the individual threads have distinct block ids (which is the case for scanning our internal data format and Parquet files). All we need to do then is to emit the chunks in order of block ids.\r\n\r\nThis would require some re-engineering of the table scans, but as I am currently working on the push-based model (which requires heavy re-engineering anyway!) I will try to work that in there, unless there are other/better suggestions for solving this problem.\n",
  "hints_text": "",
  "created_at": "2022-05-24T12:46:34Z"
}