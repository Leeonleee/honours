You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Threading issue: Multiple left joins to same table and where clause kills Python process
Hey Folks,

I'm seeing some failures when attempting to use multithreading. The failure mode is that the Python process completely dies. 

I've isolated at least one basic query that is not working for some reason (It's not 100% of the time that it fails, but close to it. Maybe some kind of race condition?). It runs correctly if threads are not enabled (when pragma threads line of code is comment out).  It also runs if the where clause is not present. It also runs if the left join table is only used once.

I'm using the Python client (Python 3.7.7 on Windows, DuckDB 0.2.3). 

Thanks for your help with this! We were upgrading from 0.1.9 to 0.2.2 to get multithreading and some of the other syntax goodness you've added lately and had to revert the multithreading piece.

Thanks,
Alex

```python
#This is broken:
import duckdb
import pandas as pd

main_table = pd.DataFrame([{"join_column":"text"}])
left_join_table = pd.DataFrame([{"join_column":"text","other_column":"more text"}])

sql = """
select
    main_table.*
    ,t1.*
    ,t2.*
from main_table 
left join left_join_table t1
    on main_table.join_column = t1.join_column
left join left_join_table t2
    on main_table.join_column = t2.join_column
where
    t1.other_column = 'more text'
"""

try:
    duckdb_conn = duckdb.connect()
    duckdb_conn.execute("PRAGMA threads=4")
    duckdb_conn.register('main_table',main_table)
    duckdb_conn.register('left_join_table',left_join_table)
    output_df = duckdb_conn.execute(sql).fetchdf()
except Exception as err:
    print(err)
finally:
    duckdb_conn.close()
    
output_df
````

```python
#This works:
import duckdb
import pandas as pd

main_table = pd.DataFrame([{"join_column":"text"}])
left_join_table = pd.DataFrame([{"join_column":"text","other_column":"more text"}])

sql = """
select
    main_table.*
    ,t1.*
    ,t2.*
from main_table 
left join left_join_table t1
    on main_table.join_column = t1.join_column
left join left_join_table t2
    on main_table.join_column = t2.join_column
"""

try:
    duckdb_conn = duckdb.connect()
    duckdb_conn.execute("PRAGMA threads=4")
    duckdb_conn.register('main_table',main_table)
    duckdb_conn.register('left_join_table',left_join_table)
    output_df = duckdb_conn.execute(sql).fetchdf()
except Exception as err:
    print(err)
finally:
    duckdb_conn.close()
    
output_df
```

```python
#This works:
import duckdb
import pandas as pd

main_table = pd.DataFrame([{"join_column":"text"}])
left_join_table = pd.DataFrame([{"join_column":"text","other_column":"more text"}])

sql = """
select
    main_table.*
    ,t1.*
    ,t2.*
from main_table 
left join left_join_table t1
    on main_table.join_column = t1.join_column
left join left_join_table t2
    on main_table.join_column = t2.join_column
where
    t1.other_column = 'more text'
"""

try:
    duckdb_conn = duckdb.connect()

    duckdb_conn.register('main_table',main_table)
    duckdb_conn.register('left_join_table',left_join_table)
    output_df = duckdb_conn.execute(sql).fetchdf()
except Exception as err:
    print(err)
finally:
    duckdb_conn.close()
    
output_df
```
Threading issue: Multiple left joins to same table and where clause kills Python process
Hey Folks,

I'm seeing some failures when attempting to use multithreading. The failure mode is that the Python process completely dies. 

I've isolated at least one basic query that is not working for some reason (It's not 100% of the time that it fails, but close to it. Maybe some kind of race condition?). It runs correctly if threads are not enabled (when pragma threads line of code is comment out).  It also runs if the where clause is not present. It also runs if the left join table is only used once.

I'm using the Python client (Python 3.7.7 on Windows, DuckDB 0.2.3). 

Thanks for your help with this! We were upgrading from 0.1.9 to 0.2.2 to get multithreading and some of the other syntax goodness you've added lately and had to revert the multithreading piece.

Thanks,
Alex

```python
#This is broken:
import duckdb
import pandas as pd

main_table = pd.DataFrame([{"join_column":"text"}])
left_join_table = pd.DataFrame([{"join_column":"text","other_column":"more text"}])

sql = """
select
    main_table.*
    ,t1.*
    ,t2.*
from main_table 
left join left_join_table t1
    on main_table.join_column = t1.join_column
left join left_join_table t2
    on main_table.join_column = t2.join_column
where
    t1.other_column = 'more text'
"""

try:
    duckdb_conn = duckdb.connect()
    duckdb_conn.execute("PRAGMA threads=4")
    duckdb_conn.register('main_table',main_table)
    duckdb_conn.register('left_join_table',left_join_table)
    output_df = duckdb_conn.execute(sql).fetchdf()
except Exception as err:
    print(err)
finally:
    duckdb_conn.close()
    
output_df
````

```python
#This works:
import duckdb
import pandas as pd

main_table = pd.DataFrame([{"join_column":"text"}])
left_join_table = pd.DataFrame([{"join_column":"text","other_column":"more text"}])

sql = """
select
    main_table.*
    ,t1.*
    ,t2.*
from main_table 
left join left_join_table t1
    on main_table.join_column = t1.join_column
left join left_join_table t2
    on main_table.join_column = t2.join_column
"""

try:
    duckdb_conn = duckdb.connect()
    duckdb_conn.execute("PRAGMA threads=4")
    duckdb_conn.register('main_table',main_table)
    duckdb_conn.register('left_join_table',left_join_table)
    output_df = duckdb_conn.execute(sql).fetchdf()
except Exception as err:
    print(err)
finally:
    duckdb_conn.close()
    
output_df
```

```python
#This works:
import duckdb
import pandas as pd

main_table = pd.DataFrame([{"join_column":"text"}])
left_join_table = pd.DataFrame([{"join_column":"text","other_column":"more text"}])

sql = """
select
    main_table.*
    ,t1.*
    ,t2.*
from main_table 
left join left_join_table t1
    on main_table.join_column = t1.join_column
left join left_join_table t2
    on main_table.join_column = t2.join_column
where
    t1.other_column = 'more text'
"""

try:
    duckdb_conn = duckdb.connect()

    duckdb_conn.register('main_table',main_table)
    duckdb_conn.register('left_join_table',left_join_table)
    output_df = duckdb_conn.execute(sql).fetchdf()
except Exception as err:
    print(err)
finally:
    duckdb_conn.close()
    
output_df
```

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of scripts/package_build.py]
1: import os
2: import sys
3: import shutil
4: import subprocess
5: from python_helpers import open_utf8
6: 
7: excluded_objects = ['utf8proc_data.cpp']
8: 
9: def get_libraries(binary_dir, libraries, extensions):
10:     result_libs = []
11:     def find_library_recursive(search_dir, potential_libnames):
12:         flist = os.listdir(search_dir)
13:         for fname in flist:
14:             fpath = os.path.join(search_dir, fname)
15:             if os.path.isdir(fpath):
16:                 entry = find_library_recursive(fpath, potential_libnames)
17:                 if entry != None:
18:                     return entry
19:             elif os.path.isfile(fpath) and fname in potential_libnames:
20:                 return search_dir
21:         return None
22: 
23:     def find_library(search_dir, libname, result_libs):
24:         if libname == 'Threads::Threads':
25:             result_libs += [(None, 'pthread')]
26:             return
27:         libextensions = ['.a', '.lib']
28:         libprefixes = ['', 'lib']
29:         potential_libnames = []
30:         for ext in libextensions:
31:             for prefix in libprefixes:
32:                 potential_libnames.append(prefix + libname + ext)
33:         libdir = find_library_recursive(binary_dir, potential_libnames)
34: 
35:         result_libs += [(libdir, libname)]
36: 
37:     result_libs += [(os.path.join(binary_dir, 'src'), 'duckdb_static')]
38:     for ext in extensions:
39:         result_libs += [(os.path.join(binary_dir, 'extension', ext), ext + '_extension')]
40: 
41:     for libname in libraries:
42:         find_library(binary_dir, libname, result_libs)
43: 
44:     return result_libs
45: 
46: def includes(extensions):
47:     scripts_dir = os.path.dirname(os.path.abspath(__file__))
48:     # add includes for duckdb and extensions
49:     includes = []
50:     includes.append(os.path.join(scripts_dir, '..', 'src', 'include'))
51:     includes.append(os.path.join(scripts_dir, '..'))
52:     for ext in extensions:
53:         includes.append(os.path.join(scripts_dir, '..', 'extension', ext, 'include'))
54:     return includes
55: 
56: def include_flags(extensions):
57:     return ' ' + ' '.join(['-I' + x for x in includes(extensions)])
58: 
59: def convert_backslashes(x):
60:     return '/'.join(x.split(os.path.sep))
61: 
62: def get_relative_path(source_dir, target_file):
63:     source_dir = convert_backslashes(source_dir)
64:     target_file = convert_backslashes(target_file)
65: 
66:     # absolute path: try to convert
67:     if source_dir in target_file:
68:         target_file = target_file.replace(source_dir, "").lstrip('/')
69:     return target_file
70: 
71: def git_commit_hash():
72:     try:
73:         return subprocess.check_output(['git','log','-1','--format=%h']).strip().decode('utf8')
74:     except:
75:         if 'SETUPTOOLS_SCM_PRETEND_HASH' in os.environ:
76:             return os.environ['SETUPTOOLS_SCM_PRETEND_HASH']
77:         else:
78:             return "deadbeeff"
79: 
80: def git_dev_version():
81:     try:
82:         version = subprocess.check_output(['git','describe','--tags','--abbrev=0']).strip().decode('utf8')
83:         long_version = subprocess.check_output(['git','describe','--tags','--long']).strip().decode('utf8')
84:         version_splits = version.lstrip('v').split('.')
85:         dev_version = long_version.split('-')[1]
86:         if int(dev_version) == 0:
87:             # directly on a tag: emit the regular version
88:             return '.'.join(version_splits)
89:         else:
90:             # not on a tag: increment the version by one and add a -devX suffix
91:             version_splits[2] = str(int(version_splits[2]) + 1)
92:             return '.'.join(version_splits) + "-dev" + dev_version
93:     except:
94:         if 'SETUPTOOLS_SCM_PRETEND_VERSION' in os.environ:
95:             return os.environ['SETUPTOOLS_SCM_PRETEND_VERSION']
96:         else:
97:             return "0.0.0"
98: 
99: def include_package(pkg_name, pkg_dir, include_files, include_list, source_list):
100:     import amalgamation
101:     original_path = sys.path
102:     # append the directory
103:     sys.path.append(pkg_dir)
104:     ext_pkg = __import__(pkg_name + '_config')
105: 
106:     ext_include_dirs = ext_pkg.include_directories
107:     ext_source_files = ext_pkg.source_files
108: 
109:     include_files += amalgamation.list_includes_files(ext_include_dirs)
110:     include_list += ext_include_dirs
111:     source_list += ext_source_files
112: 
113:     sys.path = original_path
114: 
115: def build_package(target_dir, extensions, linenumbers = False):
116:     if not os.path.isdir(target_dir):
117:         os.mkdir(target_dir)
118: 
119:     scripts_dir = os.path.dirname(os.path.abspath(__file__))
120:     sys.path.append(scripts_dir)
121:     import amalgamation
122: 
123:     prev_wd = os.getcwd()
124:     os.chdir(os.path.join(scripts_dir, '..'))
125: 
126:     # obtain the list of source files from the amalgamation
127:     source_list = amalgamation.list_sources()
128:     include_list = amalgamation.list_include_dirs()
129:     include_files = amalgamation.list_includes()
130: 
131:     def copy_file(src, target_dir):
132:         # get the path
133:         full_path = src.split(os.path.sep)
134:         current_path = target_dir
135:         for i in range(len(full_path) - 1):
136:             current_path = os.path.join(current_path, full_path[i])
137:             if not os.path.isdir(current_path):
138:                 os.mkdir(current_path)
139:         target_name = full_path[-1]
140:         target_file = os.path.join(current_path, target_name)
141:         amalgamation.copy_if_different(src, target_file)
142: 
143:     # include the main extension helper
144:     include_files += [os.path.join('extension', 'extension_helper.hpp')]
145:     # include the separate extensions
146:     for ext in extensions:
147:         ext_path = os.path.join(scripts_dir, '..', 'extension', ext)
148:         include_package(ext, ext_path, include_files, include_list, source_list)
149: 
150:     for src in source_list:
151:         copy_file(src, target_dir)
152: 
153:     for inc in include_files:
154:         copy_file(inc, target_dir)
155: 
156:     # handle pragma_version.cpp: paste #define DUCKDB_SOURCE_ID and DUCKDB_VERSION there
157:     curdir = os.getcwd()
158:     os.chdir(os.path.join(scripts_dir, '..'))
159:     githash = git_commit_hash()
160:     dev_version = git_dev_version()
161:     os.chdir(curdir)
162:     # open the file and read the current contents
163:     fpath = os.path.join(target_dir, 'src', 'function', 'table', 'version', 'pragma_version.cpp')
164:     with open_utf8(fpath, 'r') as f:
165:         text = f.read()
166:     # now add the DUCKDB_SOURCE_ID define, if it is not there already
167:     found_hash = False
168:     found_dev = False
169:     lines = text.split('\n')
170:     for i in range(len(lines)):
171:         if '#define DUCKDB_SOURCE_ID ' in lines[i]:
172:             lines[i] = '#define DUCKDB_SOURCE_ID "{}"'.format(githash)
173:             found_hash = True
174:             break
175:         if '#define DUCKDB_VERSION ' in lines[i]:
176:             lines[i] = '#define DUCKDB_VERSION "{}"'.format(dev_version)
177:             found_dev = True
178:             break
179:     if not found_hash:
180:         lines = ['#ifndef DUCKDB_SOURCE_ID', '#define DUCKDB_SOURCE_ID "{}"'.format(githash), '#endif'] + lines
181:     if not found_dev:
182:         lines = ['#ifndef DUCKDB_VERSION', '#define DUCKDB_VERSION "{}"'.format(dev_version), '#endif'] + lines
183:     text = '\n'.join(lines)
184:     with open_utf8(fpath, 'w+') as f:
185:         f.write(text)
186: 
187:     def file_is_excluded(fname):
188:         for entry in excluded_objects:
189:             if entry in fname:
190:                 return True
191:         return False
192: 
193:     def generate_unity_build(entries, idx, linenumbers):
194:         ub_file = os.path.join(target_dir, 'amalgamation-{}.cpp'.format(str(idx)))
195:         with open_utf8(ub_file, 'w+') as f:
196:             for entry in entries:
197:                 if linenumbers:
198:                     f.write('#line 0 "{}"\n'.format(convert_backslashes(entry)))
199:                 f.write('#include "{}"\n\n'.format(convert_backslashes(entry)))
200:         return ub_file
201: 
202:     def generate_unity_builds(source_list, nsplits, linenumbers):
203:         source_list.sort()
204: 
205:         files_per_split = len(source_list) / nsplits
206:         new_source_files = []
207:         current_files = []
208:         idx = 1
209:         for entry in source_list:
210:             if not entry.startswith('src'):
211:                 new_source_files.append(os.path.join('duckdb', entry))
212:                 continue
213: 
214:             current_files.append(entry)
215:             if len(current_files) > files_per_split:
216:                 new_source_files.append(generate_unity_build(current_files, idx, linenumbers))
217:                 current_files = []
218:                 idx += 1
219:         if len(current_files) > 0:
220:             new_source_files.append(generate_unity_build(current_files, idx, linenumbers))
221:             current_files = []
222:             idx += 1
223: 
224:         return new_source_files
225: 
226:     original_sources = source_list
227:     source_list = generate_unity_builds(source_list, 8, linenumbers)
228: 
229:     os.chdir(prev_wd)
230:     return ([convert_backslashes(x) for x in source_list if not file_is_excluded(x)],
231:             [convert_backslashes(x) for x in include_list],
232:             [convert_backslashes(x) for x in original_sources])
[end of scripts/package_build.py]
[start of src/include/duckdb/common/types/string_type.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/string_type.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include "duckdb/common/assert.hpp"
13: #include <cstring>
14: 
15: namespace duckdb {
16: 
17: struct string_t {
18: 	friend struct StringComparisonOperators;
19: 	friend class StringSegment;
20: 
21: public:
22: 	static constexpr idx_t PREFIX_LENGTH = 4 * sizeof(char);
23: 	static constexpr idx_t INLINE_LENGTH = 12;
24: 
25: 	string_t() = default;
26: 	string_t(uint32_t len) {
27: 		value.inlined.length = len;
28: 		memset(value.inlined.inlined, 0, INLINE_LENGTH);
29: 	}
30: 	string_t(const char *data, uint32_t len) {
31: 		value.inlined.length = len;
32: 		D_ASSERT(data || GetSize() == 0);
33: 		if (IsInlined()) {
34: 			// zero initialize the prefix first
35: 			// this makes sure that strings with length smaller than 4 still have an equal prefix
36: 			memset(value.inlined.inlined, 0, PREFIX_LENGTH);
37: 			if (GetSize() == 0) {
38: 				return;
39: 			}
40: 			// small string: inlined
41: 			/* Note: this appears to write out-of bounds on `prefix` if `length` > `PREFIX_LENGTH`
42: 			 but this is not the case because the `value_` union `inlined` char array directly
43: 			 follows it with 8 more chars to use for the string value.
44: 			 */
45: 			memcpy(value.inlined.inlined, data, GetSize());
46: 		} else {
47: 			// large string: store pointer
48: 			memcpy(value.pointer.prefix, data, PREFIX_LENGTH);
49: 			value.pointer.ptr = (char *)data;
50: 		}
51: 	}
52: 	string_t(const char *data) : string_t(data, strlen(data)) {
53: 	}
54: 	string_t(const string &value) : string_t(value.c_str(), value.size()) {
55: 	}
56: 
57: 	bool IsInlined() const {
58: 		return GetSize() <= INLINE_LENGTH;
59: 	}
60: 
61: 	//! this is unsafe since the string will not be terminated at the end
62: 	const char *GetDataUnsafe() const {
63: 		return IsInlined() ? (const char *)value.inlined.inlined : value.pointer.ptr;
64: 	}
65: 
66: 	char *GetDataWriteable() const {
67: 		return IsInlined() ? (char *)value.inlined.inlined : value.pointer.ptr;
68: 	}
69: 
70: 	const char *GetPrefix() const {
71: 		return value.pointer.prefix;
72: 	}
73: 
74: 	idx_t GetSize() const {
75: 		return value.inlined.length;
76: 	}
77: 
78: 	string GetString() const {
79: 		return string(GetDataUnsafe(), GetSize());
80: 	}
81: 
82: 	void Finalize() {
83: 		// set trailing NULL byte
84: 		auto dataptr = (char *)GetDataUnsafe();
85: 		if (GetSize() <= INLINE_LENGTH) {
86: 			// fill prefix with zeros if the length is smaller than the prefix length
87: 			for (idx_t i = GetSize(); i < PREFIX_LENGTH; i++) {
88: 				value.inlined.inlined[i] = '\0';
89: 			}
90: 		} else {
91: 			// copy the data into the prefix
92: 			memcpy(value.pointer.prefix, dataptr, PREFIX_LENGTH);
93: 		}
94: 	}
95: 
96: 	void Verify();
97: 	void VerifyNull();
98: 
99: private:
100: 	union {
101: 		struct {
102: 			uint32_t length;
103: 			char prefix[4];
104: 			char *ptr;
105: 		} pointer;
106: 		struct {
107: 			uint32_t length;
108: 			char inlined[12];
109: 		} inlined;
110: 	} value;
111: };
112: 
113: } // namespace duckdb
[end of src/include/duckdb/common/types/string_type.hpp]
[start of third_party/utf8proc/include/utf8proc_wrapper.hpp]
1: #pragma once
2: 
3: #include <string>
4: #include <cassert>
5: #include <cstring>
6: 
7: namespace duckdb {
8: 
9: enum class UnicodeType {INVALID, ASCII, UNICODE};
10: 
11: 
12: class Utf8Proc {
13: public:
14: 	//! Distinguishes ASCII, Valid UTF8 and Invalid UTF8 strings
15: 	static UnicodeType Analyze(const char *s, size_t len);
16: 	//! Performs UTF NFC normalization of string, return value needs to be free'd
17: 	static char* Normalize(const char* s, size_t len);
18: 	//! Returns whether or not the UTF8 string is valid
19: 	static bool IsValid(const char *s, size_t len);
20: 	//! Returns the position (in bytes) of the next grapheme cluster
21: 	static size_t NextGraphemeCluster(const char *s, size_t len, size_t pos);
22: 	//! Returns the position (in bytes) of the previous grapheme cluster
23: 	static size_t PreviousGraphemeCluster(const char *s, size_t len, size_t pos);
24: };
25: 
26: }
[end of third_party/utf8proc/include/utf8proc_wrapper.hpp]
[start of third_party/utf8proc/utf8proc_wrapper.cpp]
1: #include "utf8proc_wrapper.hpp"
2: #include "utf8proc_wrapper.h"
3: #include "utf8proc.hpp"
4: 
5: using namespace std;
6: 
7: namespace duckdb {
8: 
9: // This function efficiently checks if a string is valid UTF8.
10: // It was originally written by Sjoerd Mullender.
11: 
12: // Here is the table that makes it work:
13: 
14: // B 		= Number of Bytes in UTF8 encoding
15: // C_MIN 	= First Unicode code point
16: // C_MAX 	= Last Unicode code point
17: // B1 		= First Byte Prefix
18: 
19: // 	B	C_MIN		C_MAX		B1
20: //	1	U+000000	U+00007F		0xxxxxxx
21: //	2	U+000080	U+0007FF		110xxxxx
22: //	3	U+000800	U+00FFFF		1110xxxx
23: //	4	U+010000	U+10FFFF		11110xxx
24: 
25: UnicodeType Utf8Proc::Analyze(const char *s, size_t len) {
26: 	UnicodeType type = UnicodeType::ASCII;
27: 	char c;
28: 	for (size_t i = 0; i < len; i++) {
29: 		c = s[i];
30: 		if (c == '\0') {
31: 			return UnicodeType::INVALID;
32: 		}
33: 		// 1 Byte / ASCII
34: 		if ((c & 0x80) == 0)
35: 			continue;
36: 		type = UnicodeType::UNICODE;
37: 		if ((s[++i] & 0xC0) != 0x80)
38: 			return UnicodeType::INVALID;
39: 		if ((c & 0xE0) == 0xC0)
40: 			continue;
41: 		if ((s[++i] & 0xC0) != 0x80)
42: 			return UnicodeType::INVALID;
43: 		if ((c & 0xF0) == 0xE0)
44: 			continue;
45: 		if ((s[++i] & 0xC0) != 0x80)
46: 			return UnicodeType::INVALID;
47: 		if ((c & 0xF8) == 0xF0)
48: 			continue;
49: 		return UnicodeType::INVALID;
50: 	}
51: 
52: 	return type;
53: }
54: 
55: 
56: char* Utf8Proc::Normalize(const char *s, size_t len) {
57: 	assert(s);
58: 	assert(Utf8Proc::Analyze(s, len) != UnicodeType::INVALID);
59: 	return (char*) utf8proc_NFC((const utf8proc_uint8_t*) s, len);
60: }
61: 
62: bool Utf8Proc::IsValid(const char *s, size_t len) {
63: 	return Utf8Proc::Analyze(s, len) != UnicodeType::INVALID;
64: }
65: 
66: size_t Utf8Proc::NextGraphemeCluster(const char *s, size_t len, size_t cpos) {
67: 	return utf8proc_next_grapheme(s, len, cpos);
68: }
69: 
70: size_t Utf8Proc::PreviousGraphemeCluster(const char *s, size_t len, size_t cpos) {
71: 	if (!Utf8Proc::IsValid(s, len)) {
72: 		return cpos - 1;
73: 	}
74: 	size_t current_pos = 0;
75: 	while(true) {
76: 		size_t new_pos = NextGraphemeCluster(s, len, current_pos);
77: 		if (new_pos <= current_pos || new_pos >= cpos) {
78: 			return current_pos;
79: 		}
80: 		current_pos = new_pos;
81: 	}
82: }
83: 
84: }
85: 
86: size_t utf8proc_next_grapheme_cluster(const char *s, size_t len, size_t pos) {
87: 	return duckdb::Utf8Proc::NextGraphemeCluster(s, len, pos);
88: }
89: 
90: size_t utf8proc_prev_grapheme_cluster(const char *s, size_t len, size_t pos) {
91: 	return duckdb::Utf8Proc::PreviousGraphemeCluster(s, len, pos);
92: }
93: 
94: size_t utf8proc_render_width(const char *s, size_t len, size_t pos) {
95: 	int sz;
96: 	auto codepoint = duckdb::utf8proc_codepoint(s + pos, sz);
97: 	auto properties = duckdb::utf8proc_get_property(codepoint);
98: 	return properties->charwidth;
99: }
100: 
101: int utf8proc_is_valid(const char *s, size_t len) {
102: 	return duckdb::Utf8Proc::IsValid(s, len) ? 1 : 0;
103: }
[end of third_party/utf8proc/utf8proc_wrapper.cpp]
[start of tools/pythonpkg/duckdb_python.cpp]
1: #include <pybind11/pybind11.h>
2: #include <pybind11/numpy.h>
3: 
4: #include <unordered_map>
5: #include <vector>
6: 
7: #include "datetime.h" // from Python
8: 
9: #include "duckdb.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/common/arrow.hpp"
12: #include "duckdb/common/types/date.hpp"
13: #include "duckdb/common/types/hugeint.hpp"
14: #include "duckdb/common/types/time.hpp"
15: #include "duckdb/common/types/timestamp.hpp"
16: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
17: #include "duckdb/parser/parser.hpp"
18: #include "extension/extension_helper.hpp"
19: 
20: #include <random>
21: 
22: namespace py = pybind11;
23: 
24: using namespace duckdb;
25: using namespace std;
26: 
27: namespace duckdb_py_convert {
28: 
29: struct RegularConvert {
30: 	template <class DUCKDB_T, class NUMPY_T> static NUMPY_T convert_value(DUCKDB_T val) {
31: 		return (NUMPY_T)val;
32: 	}
33: };
34: 
35: struct TimestampConvert {
36: 	template <class DUCKDB_T, class NUMPY_T> static int64_t convert_value(timestamp_t val) {
37: 		return val / 1000.0;
38: 	}
39: };
40: 
41: struct DateConvert {
42: 	template <class DUCKDB_T, class NUMPY_T> static int64_t convert_value(date_t val) {
43: 		return Date::Epoch(val);
44: 	}
45: };
46: 
47: struct TimeConvert {
48: 	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(dtime_t val) {
49: 		return py::str(duckdb::Time::ToString(val).c_str());
50: 	}
51: };
52: 
53: struct StringConvert {
54: 	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(string_t val) {
55: 		return py::str(val.GetString());
56: 	}
57: };
58: 
59: struct BlobConvert {
60: 	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(string_t val) {
61: 		return py::bytes(val.GetString());
62: 	}
63: };
64: 
65: struct IntegralConvert {
66: 	template <class DUCKDB_T, class NUMPY_T> static NUMPY_T convert_value(DUCKDB_T val) {
67: 		return NUMPY_T(val);
68: 	}
69: };
70: 
71: template <> double IntegralConvert::convert_value(hugeint_t val) {
72: 	double result;
73: 	Hugeint::TryCast(val, result);
74: 	return result;
75: }
76: 
77: template <class DUCKDB_T, class NUMPY_T, class CONVERT>
78: static py::array fetch_column(string numpy_type, ChunkCollection &collection, idx_t column) {
79: 	auto out = py::array(py::dtype(numpy_type), collection.Count());
80: 	auto out_ptr = (NUMPY_T *)out.mutable_data();
81: 
82: 	idx_t out_offset = 0;
83: 	for (auto &data_chunk : collection.Chunks()) {
84: 		auto &src = data_chunk->data[column];
85: 		auto src_ptr = FlatVector::GetData<DUCKDB_T>(src);
86: 		auto &nullmask = FlatVector::Nullmask(src);
87: 		for (idx_t i = 0; i < data_chunk->size(); i++) {
88: 			if (nullmask[i]) {
89: 				continue;
90: 			}
91: 			out_ptr[i + out_offset] = CONVERT::template convert_value<DUCKDB_T, NUMPY_T>(src_ptr[i]);
92: 		}
93: 		out_offset += data_chunk->size();
94: 	}
95: 	return out;
96: }
97: 
98: template <class T> static py::array fetch_column_regular(string numpy_type, ChunkCollection &collection, idx_t column) {
99: 	return fetch_column<T, T, RegularConvert>(numpy_type, collection, column);
100: }
101: 
102: template <class DUCKDB_T>
103: static void decimal_convert_internal(ChunkCollection &collection, idx_t column, double *out_ptr, double division) {
104: 	idx_t out_offset = 0;
105: 	for (auto &data_chunk : collection.Chunks()) {
106: 		auto &src = data_chunk->data[column];
107: 		auto src_ptr = FlatVector::GetData<DUCKDB_T>(src);
108: 		auto &nullmask = FlatVector::Nullmask(src);
109: 		for (idx_t i = 0; i < data_chunk->size(); i++) {
110: 			if (nullmask[i]) {
111: 				continue;
112: 			}
113: 			out_ptr[i + out_offset] = IntegralConvert::convert_value<DUCKDB_T, double>(src_ptr[i]) / division;
114: 		}
115: 		out_offset += data_chunk->size();
116: 	}
117: }
118: 
119: static py::array fetch_column_decimal(string numpy_type, ChunkCollection &collection, idx_t column,
120:                                       LogicalType &decimal_type) {
121: 	auto out = py::array(py::dtype(numpy_type), collection.Count());
122: 	auto out_ptr = (double *)out.mutable_data();
123: 
124: 	auto dec_scale = decimal_type.scale();
125: 	double division = pow(10, dec_scale);
126: 	switch (decimal_type.InternalType()) {
127: 	case PhysicalType::INT16:
128: 		decimal_convert_internal<int16_t>(collection, column, out_ptr, division);
129: 		break;
130: 	case PhysicalType::INT32:
131: 		decimal_convert_internal<int32_t>(collection, column, out_ptr, division);
132: 		break;
133: 	case PhysicalType::INT64:
134: 		decimal_convert_internal<int64_t>(collection, column, out_ptr, division);
135: 		break;
136: 	case PhysicalType::INT128:
137: 		decimal_convert_internal<hugeint_t>(collection, column, out_ptr, division);
138: 		break;
139: 	default:
140: 		throw NotImplementedException("Unimplemented internal type for DECIMAL");
141: 	}
142: 	return out;
143: }
144: 
145: } // namespace duckdb_py_convert
146: 
147: namespace random_string {
148: static std::random_device rd;
149: static std::mt19937 gen(rd());
150: static std::uniform_int_distribution<> dis(0, 15);
151: 
152: std::string generate() {
153: 	std::stringstream ss;
154: 	int i;
155: 	ss << std::hex;
156: 	for (i = 0; i < 16; i++) {
157: 		ss << dis(gen);
158: 	}
159: 	return ss.str();
160: }
161: } // namespace random_string
162: 
163: enum class PandasType : uint8_t {
164: 	BOOLEAN,
165: 	TINYINT_NATIVE,
166: 	TINYINT_OBJECT,
167: 	SMALLINT_NATIVE,
168: 	SMALLINT_OBJECT,
169: 	INTEGER_NATIVE,
170: 	INTEGER_OBJECT,
171: 	BIGINT_NATIVE,
172: 	BIGINT_OBJECT,
173: 	FLOAT,
174: 	DOUBLE,
175: 	TIMESTAMP_NATIVE,
176: 	TIMESTAMP_OBJECT,
177: 	VARCHAR
178: };
179: 
180: struct PandasColumnBindData {
181: 	PandasType pandas_type;
182: 	py::array numpy_col;
183: };
184: 
185: struct PandasScanFunctionData : public TableFunctionData {
186: 	PandasScanFunctionData(py::handle df, idx_t row_count, vector<PandasColumnBindData> pandas_bind_data_,
187: 	                       vector<LogicalType> sql_types_)
188: 	    : df(df), row_count(row_count), pandas_bind_data(move(pandas_bind_data_)), sql_types(move(sql_types_)) {
189: 	}
190: 	py::handle df;
191: 	idx_t row_count;
192: 	vector<PandasColumnBindData> pandas_bind_data;
193: 	vector<LogicalType> sql_types;
194: };
195: 
196: struct PandasScanState : public FunctionOperatorData {
197: 	PandasScanState() : position(0) {
198: 	}
199: 
200: 	idx_t position;
201: };
202: 
203: struct PandasScanFunction : public TableFunction {
204: 	PandasScanFunction()
205: 	    : TableFunction("pandas_scan", {LogicalType::VARCHAR}, pandas_scan_function, pandas_scan_bind, pandas_scan_init,
206: 	                    nullptr, nullptr, nullptr, pandas_scan_cardinality){};
207: 
208: 	static void ConvertPandasType(const string &col_type, LogicalType &duckdb_col_type, PandasType &pandas_type) {
209: 		if (col_type == "bool") {
210: 			duckdb_col_type = LogicalType::BOOLEAN;
211: 			pandas_type = PandasType::BOOLEAN;
212: 		} else if (col_type == "int8") {
213: 			duckdb_col_type = LogicalType::TINYINT;
214: 			pandas_type = PandasType::TINYINT_NATIVE;
215: 		} else if (col_type == "Int8") {
216: 			duckdb_col_type = LogicalType::TINYINT;
217: 			pandas_type = PandasType::TINYINT_OBJECT;
218: 		} else if (col_type == "int16") {
219: 			duckdb_col_type = LogicalType::SMALLINT;
220: 			pandas_type = PandasType::SMALLINT_NATIVE;
221: 		} else if (col_type == "Int16") {
222: 			duckdb_col_type = LogicalType::SMALLINT;
223: 			pandas_type = PandasType::SMALLINT_OBJECT;
224: 		} else if (col_type == "int32") {
225: 			duckdb_col_type = LogicalType::INTEGER;
226: 			pandas_type = PandasType::INTEGER_NATIVE;
227: 		} else if (col_type == "Int32") {
228: 			duckdb_col_type = LogicalType::INTEGER;
229: 			pandas_type = PandasType::INTEGER_OBJECT;
230: 		} else if (col_type == "int64") {
231: 			duckdb_col_type = LogicalType::BIGINT;
232: 			pandas_type = PandasType::BIGINT_NATIVE;
233: 		} else if (col_type == "Int64") {
234: 			duckdb_col_type = LogicalType::BIGINT;
235: 			pandas_type = PandasType::BIGINT_OBJECT;
236: 		} else if (col_type == "float32") {
237: 			duckdb_col_type = LogicalType::FLOAT;
238: 			pandas_type = PandasType::FLOAT;
239: 		} else if (col_type == "float64") {
240: 			duckdb_col_type = LogicalType::DOUBLE;
241: 			pandas_type = PandasType::DOUBLE;
242: 		} else if (col_type == "datetime64[ns]") {
243: 			duckdb_col_type = LogicalType::TIMESTAMP;
244: 			pandas_type = PandasType::TIMESTAMP_NATIVE;
245: 		} else if (StringUtil::StartsWith(col_type, "datetime64[ns")) {
246: 			duckdb_col_type = LogicalType::TIMESTAMP;
247: 			pandas_type = PandasType::TIMESTAMP_OBJECT;
248: 		} else if (col_type == "object") {
249: 			// this better be strings
250: 			duckdb_col_type = LogicalType::VARCHAR;
251: 			pandas_type = PandasType::VARCHAR;
252: 		} else {
253: 			throw runtime_error("unsupported python type " + col_type);
254: 		}
255: 	}
256: 
257: 	static unique_ptr<FunctionData> pandas_scan_bind(ClientContext &context, vector<Value> &inputs,
258: 	                                                 unordered_map<string, Value> &named_parameters,
259: 	                                                 vector<LogicalType> &return_types, vector<string> &names) {
260: 		// Hey, it works (TM)
261: 		py::handle df((PyObject *)std::stoull(inputs[0].GetValue<string>(), nullptr, 16));
262: 
263: 		/* TODO this fails on Python2 for some reason
264: 		auto pandas_mod = py::module::import("pandas.core.frame");
265: 		auto df_class = pandas_mod.attr("DataFrame");
266: 
267: 		if (!df.get_type().is(df_class)) {
268: 		    throw Exception("parameter is not a DataFrame");
269: 		} */
270: 
271: 		auto df_columns = py::list(df.attr("columns"));
272: 		auto df_types = py::list(df.attr("dtypes"));
273: 		auto get_fun = df.attr("__getitem__");
274: 		// TODO support masked arrays as well
275: 		// TODO support dicts of numpy arrays as well
276: 		if (py::len(df_columns) == 0 || py::len(df_types) == 0 || py::len(df_columns) != py::len(df_types)) {
277: 			throw runtime_error("Need a DataFrame with at least one column");
278: 		}
279: 		vector<PandasColumnBindData> pandas_bind_data;
280: 		for (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {
281: 			LogicalType duckdb_col_type;
282: 			PandasColumnBindData bind_data;
283: 			bind_data.numpy_col = py::array(get_fun(df_columns[col_idx]).attr("to_numpy")());
284: 
285: 			auto col_type = string(py::str(df_types[col_idx]));
286: 			if (col_type == "category") {
287: 				// for category types, we use the converted numpy type
288: 				auto numpy_type = bind_data.numpy_col.attr("dtype");
289: 				auto category_type = string(py::str(numpy_type));
290: 				ConvertPandasType(category_type, duckdb_col_type, bind_data.pandas_type);
291: 			} else {
292: 				// regular type
293: 				ConvertPandasType(col_type, duckdb_col_type, bind_data.pandas_type);
294: 			}
295: 			names.push_back(string(py::str(df_columns[col_idx])));
296: 			return_types.push_back(duckdb_col_type);
297: 			pandas_bind_data.push_back(move(bind_data));
298: 		}
299: 		idx_t row_count = py::len(get_fun(df_columns[0]));
300: 		return make_unique<PandasScanFunctionData>(df, row_count, move(pandas_bind_data), return_types);
301: 	}
302: 
303: 	static unique_ptr<FunctionOperatorData> pandas_scan_init(ClientContext &context, const FunctionData *bind_data,
304: 	                                                         vector<column_t> &column_ids,
305: 	                                                         TableFilterSet *table_filters) {
306: 		return make_unique<PandasScanState>();
307: 	}
308: 
309: 	template <class T> static void scan_pandas_column(py::array &numpy_col, idx_t count, idx_t offset, Vector &out) {
310: 		auto src_ptr = (T *)numpy_col.data();
311: 		FlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));
312: 	}
313: 
314: 	template <class T>
315: 	static void scan_pandas_numeric_object(py::array &numpy_col, idx_t count, idx_t offset, Vector &out) {
316: 		auto src_ptr = (PyObject **)numpy_col.data();
317: 		auto tgt_ptr = FlatVector::GetData<T>(out);
318: 		auto &nullmask = FlatVector::Nullmask(out);
319: 		for (idx_t i = 0; i < count; i++) {
320: 			auto obj = src_ptr[offset + i];
321: 			auto &py_obj = *((py::object *)&obj);
322: 			if (!py::isinstance<py::int_>(py_obj)) {
323: 				nullmask[i] = true;
324: 				continue;
325: 			}
326: 			tgt_ptr[i] = py_obj.cast<T>();
327: 		}
328: 	}
329: 
330: 	template <class T> static bool ValueIsNull(T value) {
331: 		throw runtime_error("unsupported type for ValueIsNull");
332: 	}
333: 
334: 	template <class T> static void scan_pandas_fp_column(T *src_ptr, idx_t count, idx_t offset, Vector &out) {
335: 		FlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));
336: 		auto tgt_ptr = FlatVector::GetData<T>(out);
337: 		auto &nullmask = FlatVector::Nullmask(out);
338: 		for (idx_t i = 0; i < count; i++) {
339: 			if (ValueIsNull(tgt_ptr[i])) {
340: 				nullmask[i] = true;
341: 			}
342: 		}
343: 	}
344: 
345: 	static void ConvertVector(PandasColumnBindData &bind_data, py::array &numpy_col, idx_t count, idx_t offset, Vector &out) {
346: 		switch (bind_data.pandas_type) {
347: 		case PandasType::BOOLEAN:
348: 			scan_pandas_column<bool>(numpy_col, count, offset, out);
349: 			break;
350: 		case PandasType::TINYINT_NATIVE:
351: 			scan_pandas_column<int8_t>(numpy_col, count, offset, out);
352: 			break;
353: 		case PandasType::SMALLINT_NATIVE:
354: 			scan_pandas_column<int16_t>(numpy_col, count, offset, out);
355: 			break;
356: 		case PandasType::INTEGER_NATIVE:
357: 			scan_pandas_column<int32_t>(numpy_col, count, offset, out);
358: 			break;
359: 		case PandasType::BIGINT_NATIVE:
360: 			scan_pandas_column<int64_t>(numpy_col, count, offset, out);
361: 			break;
362: 		case PandasType::TINYINT_OBJECT:
363: 			scan_pandas_numeric_object<int8_t>(numpy_col, count, offset, out);
364: 			break;
365: 		case PandasType::SMALLINT_OBJECT:
366: 			scan_pandas_numeric_object<int16_t>(numpy_col, count, offset, out);
367: 			break;
368: 		case PandasType::INTEGER_OBJECT:
369: 			scan_pandas_numeric_object<int32_t>(numpy_col, count, offset, out);
370: 			break;
371: 		case PandasType::BIGINT_OBJECT:
372: 			scan_pandas_numeric_object<int64_t>(numpy_col, count, offset, out);
373: 			break;
374: 		case PandasType::FLOAT:
375: 			scan_pandas_fp_column<float>((float *)numpy_col.data(), count, offset,
376: 											out);
377: 			break;
378: 		case PandasType::DOUBLE:
379: 			scan_pandas_fp_column<double>((double *)numpy_col.data(), count, offset, out);
380: 			break;
381: 		case PandasType::TIMESTAMP_NATIVE: {
382: 			auto src_ptr = (int64_t *)numpy_col.data();
383: 			auto tgt_ptr = FlatVector::GetData<timestamp_t>(out);
384: 			auto &nullmask = FlatVector::Nullmask(out);
385: 
386: 			for (idx_t row = 0; row < count; row++) {
387: 				auto source_idx = offset + row;
388: 				if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
389: 					// pandas Not a Time (NaT)
390: 					nullmask[row] = true;
391: 					continue;
392: 				}
393: 				tgt_ptr[row] = Timestamp::FromEpochNanoSeconds(src_ptr[source_idx]);
394: 			}
395: 			break;
396: 		}
397: 		case PandasType::TIMESTAMP_OBJECT: {
398: 			auto src_ptr = (PyObject **)numpy_col.data();
399: 			auto tgt_ptr = FlatVector::GetData<timestamp_t>(out);
400: 			auto &nullmask = FlatVector::Nullmask(out);
401: 			auto pandas_mod = py::module::import("pandas");
402: 			auto pandas_datetime = pandas_mod.attr("Timestamp");
403: 			for (idx_t row = 0; row < count; row++) {
404: 				auto source_idx = offset + row;
405: 				auto val = src_ptr[source_idx];
406: 				auto &py_obj = *((py::object *)&val);
407: 				if (!py::isinstance(py_obj, pandas_datetime)) {
408: 					nullmask[row] = true;
409: 					continue;
410: 				}
411: 				// FIXME: consider timezone
412: 				auto epoch = py_obj.attr("timestamp")();
413: 				auto seconds = int64_t(epoch.cast<double>());
414: 				tgt_ptr[row] = Timestamp::FromEpochSeconds(seconds);
415: 			}
416: 			break;
417: 		}
418: 		case PandasType::VARCHAR: {
419: 			auto src_ptr = (PyObject **)numpy_col.data();
420: 			auto tgt_ptr = FlatVector::GetData<string_t>(out);
421: 			for (idx_t row = 0; row < count; row++) {
422: 				auto source_idx = offset + row;
423: 				auto val = src_ptr[source_idx];
424: #if PY_MAJOR_VERSION >= 3
425: 				if (!PyUnicode_Check(val)) {
426: 					FlatVector::SetNull(out, row, true);
427: 					continue;
428: 				}
429: 				if (PyUnicode_READY(val) != 0) {
430: 					throw runtime_error("failure in PyUnicode_READY");
431: 				}
432: 				tgt_ptr[row] = StringVector::AddString(out, ((py::object *)&val)->cast<string>());
433: #else
434: 				if (!py::isinstance<py::str>(*((py::object *)&val))) {
435: 					FlatVector::SetNull(out, row, true);
436: 					continue;
437: 				}
438: 
439: 				tgt_ptr[row] = StringVector::AddString(out, ((py::object *)&val)->cast<string>());
440: #endif
441: 			}
442: 			break;
443: 		}
444: 		default:
445: 			throw runtime_error("Unsupported type " + out.type.ToString());
446: 		}
447: 
448: 	}
449: 
450: 	static void pandas_scan_function(ClientContext &context, const FunctionData *bind_data,
451: 	                                 FunctionOperatorData *operator_state, DataChunk &output) {
452: 		auto &data = (PandasScanFunctionData &)*bind_data;
453: 		auto &state = (PandasScanState &)*operator_state;
454: 
455: 		if (state.position >= data.row_count) {
456: 			return;
457: 		}
458: 		idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, data.row_count - state.position);
459: 
460: 		auto df_names = py::list(data.df.attr("columns"));
461: 
462: 		output.SetCardinality(this_count);
463: 		for (idx_t col_idx = 0; col_idx < output.ColumnCount(); col_idx++) {
464: 			ConvertVector(data.pandas_bind_data[col_idx], data.pandas_bind_data[col_idx].numpy_col, this_count, state.position, output.data[col_idx]);
465: 		}
466: 		state.position += this_count;
467: 	}
468: 
469: 	static unique_ptr<NodeStatistics> pandas_scan_cardinality(ClientContext &context, const FunctionData *bind_data) {
470: 		auto &data = (PandasScanFunctionData &)*bind_data;
471: 		return make_unique<NodeStatistics>(data.row_count, data.row_count);
472: 	}
473: };
474: 
475: template <> bool PandasScanFunction::ValueIsNull(float value);
476: template <> bool PandasScanFunction::ValueIsNull(double value);
477: 
478: template <> bool PandasScanFunction::ValueIsNull(float value) {
479: 	return !Value::FloatIsValid(value);
480: }
481: 
482: template <> bool PandasScanFunction::ValueIsNull(double value) {
483: 	return !Value::DoubleIsValid(value);
484: }
485: 
486: struct DuckDBPyResult {
487: 
488: 	template <class SRC> static SRC fetch_scalar(Vector &src_vec, idx_t offset) {
489: 		auto src_ptr = FlatVector::GetData<SRC>(src_vec);
490: 		return src_ptr[offset];
491: 	}
492: 
493: 	py::object fetchone() {
494: 		if (!result) {
495: 			throw runtime_error("result closed");
496: 		}
497: 		if (!current_chunk || chunk_offset >= current_chunk->size()) {
498: 			current_chunk = result->Fetch();
499: 			chunk_offset = 0;
500: 		}
501: 		if (current_chunk->size() == 0) {
502: 			return py::none();
503: 		}
504: 		py::tuple res(result->types.size());
505: 
506: 		for (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {
507: 			auto &nullmask = FlatVector::Nullmask(current_chunk->data[col_idx]);
508: 			if (nullmask[chunk_offset]) {
509: 				res[col_idx] = py::none();
510: 				continue;
511: 			}
512: 			auto val = current_chunk->data[col_idx].GetValue(chunk_offset);
513: 			switch (result->types[col_idx].id()) {
514: 			case LogicalTypeId::BOOLEAN:
515: 				res[col_idx] = val.GetValue<bool>();
516: 				break;
517: 			case LogicalTypeId::TINYINT:
518: 				res[col_idx] = val.GetValue<int8_t>();
519: 				break;
520: 			case LogicalTypeId::SMALLINT:
521: 				res[col_idx] = val.GetValue<int16_t>();
522: 				break;
523: 			case LogicalTypeId::INTEGER:
524: 				res[col_idx] = val.GetValue<int32_t>();
525: 				break;
526: 			case LogicalTypeId::BIGINT:
527: 				res[col_idx] = val.GetValue<int64_t>();
528: 				break;
529: 			case LogicalTypeId::HUGEINT: {
530: 				auto hugeint_str = val.GetValue<string>();
531: 				res[col_idx] = PyLong_FromString((char *)hugeint_str.c_str(), nullptr, 10);
532: 				break;
533: 			}
534: 			case LogicalTypeId::FLOAT:
535: 				res[col_idx] = val.GetValue<float>();
536: 				break;
537: 			case LogicalTypeId::DOUBLE:
538: 				res[col_idx] = val.GetValue<double>();
539: 				break;
540: 			case LogicalTypeId::DECIMAL:
541: 				res[col_idx] = val.CastAs(LogicalType::DOUBLE).GetValue<double>();
542: 				break;
543: 			case LogicalTypeId::VARCHAR:
544: 				res[col_idx] = val.GetValue<string>();
545: 				break;
546: 			case LogicalTypeId::BLOB:
547: 				res[col_idx] = py::bytes(val.GetValue<string>());
548: 				break;
549: 			case LogicalTypeId::TIMESTAMP: {
550: 				D_ASSERT(result->types[col_idx].InternalType() == PhysicalType::INT64);
551: 
552: 				auto timestamp = val.GetValueUnsafe<int64_t>();
553: 				int32_t year, month, day, hour, min, sec, micros;
554: 				date_t date;
555: 				dtime_t time;
556: 				Timestamp::Convert(timestamp, date, time);
557: 				Date::Convert(date, year, month, day);
558: 				Time::Convert(time, hour, min, sec, micros);
559: 				res[col_idx] = PyDateTime_FromDateAndTime(year, month, day, hour, min, sec, micros);
560: 				break;
561: 			}
562: 			case LogicalTypeId::TIME: {
563: 				D_ASSERT(result->types[col_idx].InternalType() == PhysicalType::INT64);
564: 
565: 				int32_t hour, min, sec, microsec;
566: 				auto time = val.GetValueUnsafe<int64_t>();
567: 				duckdb::Time::Convert(time, hour, min, sec, microsec);
568: 				res[col_idx] = PyTime_FromTime(hour, min, sec, microsec);
569: 				break;
570: 			}
571: 			case LogicalTypeId::DATE: {
572: 				D_ASSERT(result->types[col_idx].InternalType() == PhysicalType::INT32);
573: 
574: 				auto date = val.GetValueUnsafe<int32_t>();
575: 				int32_t year, month, day;
576: 				duckdb::Date::Convert(date, year, month, day);
577: 				res[col_idx] = PyDate_FromDate(year, month, day);
578: 				break;
579: 			}
580: 
581: 			default:
582: 				throw runtime_error("unsupported type: " + result->types[col_idx].ToString());
583: 			}
584: 		}
585: 		chunk_offset++;
586: 		return move(res);
587: 	}
588: 
589: 	py::list fetchall() {
590: 		py::list res;
591: 		while (true) {
592: 			auto fres = fetchone();
593: 			if (fres.is_none()) {
594: 				break;
595: 			}
596: 			res.append(fres);
597: 		}
598: 		return res;
599: 	}
600: 
601: 	py::dict fetchnumpy() {
602: 		if (!result) {
603: 			throw runtime_error("result closed");
604: 		}
605: 		// need to materialize the result if it was streamed because we need the count :/
606: 		MaterializedQueryResult *mres = nullptr;
607: 		unique_ptr<QueryResult> mat_res_holder;
608: 		if (result->type == QueryResultType::STREAM_RESULT) {
609: 			mat_res_holder = ((StreamQueryResult *)result.get())->Materialize();
610: 			mres = (MaterializedQueryResult *)mat_res_holder.get();
611: 		} else {
612: 			mres = (MaterializedQueryResult *)result.get();
613: 		}
614: 		D_ASSERT(mres);
615: 
616: 		py::dict res;
617: 		for (idx_t col_idx = 0; col_idx < mres->types.size(); col_idx++) {
618: 			// convert the actual payload
619: 			py::array col_res;
620: 			switch (mres->types[col_idx].id()) {
621: 			case LogicalTypeId::BOOLEAN:
622: 				col_res = duckdb_py_convert::fetch_column_regular<bool>("bool", mres->collection, col_idx);
623: 				break;
624: 			case LogicalTypeId::TINYINT:
625: 				col_res = duckdb_py_convert::fetch_column_regular<int8_t>("int8", mres->collection, col_idx);
626: 				break;
627: 			case LogicalTypeId::SMALLINT:
628: 				col_res = duckdb_py_convert::fetch_column_regular<int16_t>("int16", mres->collection, col_idx);
629: 				break;
630: 			case LogicalTypeId::INTEGER:
631: 				col_res = duckdb_py_convert::fetch_column_regular<int32_t>("int32", mres->collection, col_idx);
632: 				break;
633: 			case LogicalTypeId::BIGINT:
634: 				col_res = duckdb_py_convert::fetch_column_regular<int64_t>("int64", mres->collection, col_idx);
635: 				break;
636: 			case LogicalTypeId::HUGEINT:
637: 				col_res = duckdb_py_convert::fetch_column<hugeint_t, double, duckdb_py_convert::IntegralConvert>(
638: 				    "float64", mres->collection, col_idx);
639: 				break;
640: 			case LogicalTypeId::FLOAT:
641: 				col_res = duckdb_py_convert::fetch_column_regular<float>("float32", mres->collection, col_idx);
642: 				break;
643: 			case LogicalTypeId::DOUBLE:
644: 				col_res = duckdb_py_convert::fetch_column_regular<double>("float64", mres->collection, col_idx);
645: 				break;
646: 			case LogicalTypeId::DECIMAL:
647: 				col_res =
648: 				    duckdb_py_convert::fetch_column_decimal("float64", mres->collection, col_idx, mres->types[col_idx]);
649: 				break;
650: 			case LogicalTypeId::TIMESTAMP:
651: 				col_res = duckdb_py_convert::fetch_column<timestamp_t, int64_t, duckdb_py_convert::TimestampConvert>(
652: 				    "datetime64[ms]", mres->collection, col_idx);
653: 				break;
654: 			case LogicalTypeId::DATE:
655: 				col_res = duckdb_py_convert::fetch_column<date_t, int64_t, duckdb_py_convert::DateConvert>(
656: 				    "datetime64[s]", mres->collection, col_idx);
657: 				break;
658: 			case LogicalTypeId::TIME:
659: 				col_res = duckdb_py_convert::fetch_column<dtime_t, py::str, duckdb_py_convert::TimeConvert>(
660: 				    "object", mres->collection, col_idx);
661: 				break;
662: 			case LogicalTypeId::VARCHAR:
663: 				col_res = duckdb_py_convert::fetch_column<string_t, py::str, duckdb_py_convert::StringConvert>(
664: 				    "object", mres->collection, col_idx);
665: 				break;
666: 			case LogicalTypeId::BLOB:
667: 				col_res = duckdb_py_convert::fetch_column<string_t, py::bytes, duckdb_py_convert::BlobConvert>(
668: 				    "object", mres->collection, col_idx);
669: 				break;
670: 			default:
671: 				throw runtime_error("unsupported type " + mres->types[col_idx].ToString());
672: 			}
673: 
674: 			// convert the nullmask
675: 			auto nullmask = py::array(py::dtype("bool"), mres->collection.Count());
676: 			auto nullmask_ptr = (bool *)nullmask.mutable_data();
677: 			idx_t out_offset = 0;
678: 			for (auto &data_chunk : mres->collection.Chunks()) {
679: 				auto &src_nm = FlatVector::Nullmask(data_chunk->data[col_idx]);
680: 				for (idx_t i = 0; i < data_chunk->size(); i++) {
681: 					nullmask_ptr[i + out_offset] = src_nm[i];
682: 				}
683: 				out_offset += data_chunk->size();
684: 			}
685: 
686: 			// create masked array and assign to output
687: 			auto masked_array = py::module::import("numpy.ma").attr("masked_array")(col_res, nullmask);
688: 			res[mres->names[col_idx].c_str()] = masked_array;
689: 		}
690: 		return res;
691: 	}
692: 
693: 	py::object fetchdf() {
694: 		return py::module::import("pandas").attr("DataFrame").attr("from_dict")(fetchnumpy());
695: 	}
696: 
697: 	py::object fetch_arrow_table() {
698: 		if (!result) {
699: 			throw runtime_error("result closed");
700: 		}
701: 
702: 		auto pyarrow_lib_module = py::module::import("pyarrow").attr("lib");
703: 
704: 		auto batch_import_func = pyarrow_lib_module.attr("RecordBatch").attr("_import_from_c");
705: 		auto from_batches_func = pyarrow_lib_module.attr("Table").attr("from_batches");
706: 		auto schema_import_func = pyarrow_lib_module.attr("Schema").attr("_import_from_c");
707: 		ArrowSchema schema;
708: 		result->ToArrowSchema(&schema);
709: 		auto schema_obj = schema_import_func((uint64_t)&schema);
710: 
711: 		py::list batches;
712: 		while (true) {
713: 			auto data_chunk = result->Fetch();
714: 			if (data_chunk->size() == 0) {
715: 				break;
716: 			}
717: 			ArrowArray data;
718: 			data_chunk->ToArrowArray(&data);
719: 			ArrowSchema schema;
720: 			result->ToArrowSchema(&schema);
721: 			batches.append(batch_import_func((uint64_t)&data, (uint64_t)&schema));
722: 		}
723: 		return from_batches_func(batches, schema_obj);
724: 	}
725: 
726: 	py::list description() {
727: 		py::list desc(result->names.size());
728: 		for (idx_t col_idx = 0; col_idx < result->names.size(); col_idx++) {
729: 			py::tuple col_desc(7);
730: 			col_desc[0] = py::str(result->names[col_idx]);
731: 			col_desc[1] = py::none();
732: 			col_desc[2] = py::none();
733: 			col_desc[3] = py::none();
734: 			col_desc[4] = py::none();
735: 			col_desc[5] = py::none();
736: 			col_desc[6] = py::none();
737: 			desc[col_idx] = col_desc;
738: 		}
739: 		return desc;
740: 	}
741: 
742: 	void close() {
743: 		result = nullptr;
744: 	}
745: 	idx_t chunk_offset = 0;
746: 
747: 	unique_ptr<QueryResult> result;
748: 	unique_ptr<DataChunk> current_chunk;
749: };
750: 
751: struct DuckDBPyRelation;
752: 
753: struct DuckDBPyConnection {
754: 	DuckDBPyConnection *executemany(string query, py::object params = py::list()) {
755: 		execute(query, params, true);
756: 		return this;
757: 	}
758: 
759: 	~DuckDBPyConnection() {
760: 		for (auto &element : registered_dfs) {
761: 			unregister_df(element.first);
762: 		}
763: 	}
764: 
765: 	DuckDBPyConnection *execute(string query, py::object params = py::list(), bool many = false) {
766: 		if (!connection) {
767: 			throw runtime_error("connection closed");
768: 		}
769: 		result = nullptr;
770: 
771: 		auto statements = connection->ExtractStatements(query);
772: 		if (statements.size() == 0) {
773: 			// no statements to execute
774: 			return this;
775: 		}
776: 		// if there are multiple statements, we directly execute the statements besides the last one
777: 		// we only return the result of the last statement to the user, unless one of the previous statements fails
778: 		for (idx_t i = 0; i + 1 < statements.size(); i++) {
779: 			auto res = connection->Query(move(statements[i]));
780: 			if (!res->success) {
781: 				throw runtime_error(res->error);
782: 			}
783: 		}
784: 
785: 		auto prep = connection->Prepare(move(statements.back()));
786: 		if (!prep->success) {
787: 			throw runtime_error(prep->error);
788: 		}
789: 
790: 		// this is a list of a list of parameters in executemany
791: 		py::list params_set;
792: 		if (!many) {
793: 			params_set = py::list(1);
794: 			params_set[0] = params;
795: 		} else {
796: 			params_set = params;
797: 		}
798: 
799: 		for (pybind11::handle single_query_params : params_set) {
800: 			if (prep->n_param != py::len(single_query_params)) {
801: 				throw runtime_error("Prepared statments needs " + to_string(prep->n_param) + " parameters, " +
802: 				                    to_string(py::len(single_query_params)) + " given");
803: 			}
804: 			auto args = DuckDBPyConnection::transform_python_param_list(single_query_params);
805: 			auto res = make_unique<DuckDBPyResult>();
806: 			res->result = prep->Execute(args);
807: 			if (!res->result->success) {
808: 				throw runtime_error(res->result->error);
809: 			}
810: 			if (!many) {
811: 				result = move(res);
812: 			}
813: 		}
814: 		return this;
815: 	}
816: 
817: 	DuckDBPyConnection *append(string name, py::object value) {
818: 		register_df("__append_df", value);
819: 		return execute("INSERT INTO \"" + name + "\" SELECT * FROM __append_df");
820: 	}
821: 
822: 	static string ptr_to_string(void const *ptr) {
823: 		std::ostringstream address;
824: 		address << ptr;
825: 		return address.str();
826: 	}
827: 
828: 	DuckDBPyConnection *register_df(string name, py::object value) {
829: 		// hack alert: put the pointer address into the function call as a string
830: 		execute("CREATE OR REPLACE VIEW \"" + name + "\" AS SELECT * FROM pandas_scan('" + ptr_to_string(value.ptr()) +
831: 		        "')");
832: 
833: 		// try to bind
834: 		execute("SELECT * FROM \"" + name + "\" WHERE FALSE");
835: 
836: 		// keep a reference
837: 		registered_dfs[name] = value;
838: 		return this;
839: 	}
840: 
841: 	unique_ptr<DuckDBPyRelation> table(string tname) {
842: 		if (!connection) {
843: 			throw runtime_error("connection closed");
844: 		}
845: 		return make_unique<DuckDBPyRelation>(connection->Table(tname));
846: 	}
847: 
848: 	unique_ptr<DuckDBPyRelation> values(py::object params = py::list()) {
849: 		if (!connection) {
850: 			throw runtime_error("connection closed");
851: 		}
852: 		vector<vector<Value>> values{DuckDBPyConnection::transform_python_param_list(params)};
853: 		return make_unique<DuckDBPyRelation>(connection->Values(values));
854: 	}
855: 
856: 	unique_ptr<DuckDBPyRelation> view(string vname) {
857: 		if (!connection) {
858: 			throw runtime_error("connection closed");
859: 		}
860: 		return make_unique<DuckDBPyRelation>(connection->View(vname));
861: 	}
862: 
863: 	unique_ptr<DuckDBPyRelation> table_function(string fname, py::object params = py::list()) {
864: 		if (!connection) {
865: 			throw runtime_error("connection closed");
866: 		}
867: 
868: 		return make_unique<DuckDBPyRelation>(
869: 		    connection->TableFunction(fname, DuckDBPyConnection::transform_python_param_list(params)));
870: 	}
871: 
872: 	unique_ptr<DuckDBPyRelation> from_df(py::object value) {
873: 		if (!connection) {
874: 			throw runtime_error("connection closed");
875: 		};
876: 		string name = "df_" + random_string::generate();
877: 		registered_dfs[name] = value;
878: 		vector<Value> params;
879: 		params.push_back(Value(ptr_to_string(value.ptr())));
880: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("pandas_scan", params)->Alias(name));
881: 	}
882: 
883: 	unique_ptr<DuckDBPyRelation> from_csv_auto(string filename) {
884: 		if (!connection) {
885: 			throw runtime_error("connection closed");
886: 		};
887: 		vector<Value> params;
888: 		params.push_back(Value(filename));
889: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("read_csv_auto", params)->Alias(filename));
890: 	}
891: 
892: 	unique_ptr<DuckDBPyRelation> from_parquet(string filename) {
893: 		if (!connection) {
894: 			throw runtime_error("connection closed");
895: 		};
896: 		vector<Value> params;
897: 		params.push_back(Value(filename));
898: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("parquet_scan", params)->Alias(filename));
899: 	}
900: 
901: 	struct PythonTableArrowArrayStream {
902: 		PythonTableArrowArrayStream(py::object arrow_table) : arrow_table(arrow_table) {
903: 			stream.get_schema = PythonTableArrowArrayStream::my_stream_getschema;
904: 			stream.get_next = PythonTableArrowArrayStream::my_stream_getnext;
905: 			stream.release = PythonTableArrowArrayStream::my_stream_release;
906: 			stream.get_last_error = PythonTableArrowArrayStream::my_stream_getlasterror;
907: 			stream.private_data = this;
908: 
909: 			batches = arrow_table.attr("to_batches")();
910: 		}
911: 
912: 		static int my_stream_getschema(struct ArrowArrayStream *stream, struct ArrowSchema *out) {
913: 			D_ASSERT(stream->private_data);
914: 			auto my_stream = (PythonTableArrowArrayStream *)stream->private_data;
915: 			if (!stream->release) {
916: 				my_stream->last_error = "stream was released";
917: 				return -1;
918: 			}
919: 			my_stream->arrow_table.attr("schema").attr("_export_to_c")((uint64_t)out);
920: 			return 0;
921: 		}
922: 
923: 		static int my_stream_getnext(struct ArrowArrayStream *stream, struct ArrowArray *out) {
924: 			D_ASSERT(stream->private_data);
925: 			auto my_stream = (PythonTableArrowArrayStream *)stream->private_data;
926: 			if (!stream->release) {
927: 				my_stream->last_error = "stream was released";
928: 				return -1;
929: 			}
930: 			if (my_stream->batch_idx >= py::len(my_stream->batches)) {
931: 				out->release = nullptr;
932: 				return 0;
933: 			}
934: 			my_stream->batches[my_stream->batch_idx++].attr("_export_to_c")((uint64_t)out);
935: 			return 0;
936: 		}
937: 
938: 		static void my_stream_release(struct ArrowArrayStream *stream) {
939: 			if (!stream->release) {
940: 				return;
941: 			}
942: 			stream->release = nullptr;
943: 			delete (PythonTableArrowArrayStream *)stream->private_data;
944: 		}
945: 
946: 		static const char *my_stream_getlasterror(struct ArrowArrayStream *stream) {
947: 			if (!stream->release) {
948: 				return "stream was released";
949: 			}
950: 			D_ASSERT(stream->private_data);
951: 			auto my_stream = (PythonTableArrowArrayStream *)stream->private_data;
952: 			return my_stream->last_error.c_str();
953: 		}
954: 
955: 		ArrowArrayStream stream;
956: 		string last_error;
957: 		py::object arrow_table;
958: 		py::list batches;
959: 		idx_t batch_idx = 0;
960: 	};
961: 
962: 	unique_ptr<DuckDBPyRelation> from_arrow_table(py::object table) {
963: 		if (!connection) {
964: 			throw runtime_error("connection closed");
965: 		};
966: 
967: 		// the following is a careful dance around having to depend on pyarrow
968: 		if (table.is_none() || string(py::str(table.get_type().attr("__name__"))) != "Table") {
969: 			throw runtime_error("Only arrow tables supported");
970: 		}
971: 
972: 		auto my_arrow_table = new PythonTableArrowArrayStream(table);
973: 		string name = "arrow_table_" + ptr_to_string((void *)my_arrow_table);
974: 		return make_unique<DuckDBPyRelation>(
975: 		    connection->TableFunction("arrow_scan", {Value::POINTER((uintptr_t)my_arrow_table)})->Alias(name));
976: 	}
977: 
978: 	DuckDBPyConnection *unregister_df(string name) {
979: 		registered_dfs[name] = py::none();
980: 		return this;
981: 	}
982: 
983: 	DuckDBPyConnection *begin() {
984: 		execute("BEGIN TRANSACTION");
985: 		return this;
986: 	}
987: 
988: 	DuckDBPyConnection *commit() {
989: 		if (connection->context->transaction.IsAutoCommit()) {
990: 			return this;
991: 		}
992: 		execute("COMMIT");
993: 		return this;
994: 	}
995: 
996: 	DuckDBPyConnection *rollback() {
997: 		execute("ROLLBACK");
998: 		return this;
999: 	}
1000: 
1001: 	py::object getattr(py::str key) {
1002: 		if (key.cast<string>() == "description") {
1003: 			if (!result) {
1004: 				throw runtime_error("no open result set");
1005: 			}
1006: 			return result->description();
1007: 		}
1008: 		return py::none();
1009: 	}
1010: 
1011: 	void close() {
1012: 		connection = nullptr;
1013: 		database = nullptr;
1014: 		for (auto &cur : cursors) {
1015: 			cur->close();
1016: 		}
1017: 		cursors.clear();
1018: 	}
1019: 
1020: 	// cursor() is stupid
1021: 	shared_ptr<DuckDBPyConnection> cursor() {
1022: 		auto res = make_shared<DuckDBPyConnection>();
1023: 		res->database = database;
1024: 		res->connection = make_unique<Connection>(*res->database);
1025: 		cursors.push_back(res);
1026: 		return res;
1027: 	}
1028: 
1029: 	// these should be functions on the result but well
1030: 	py::tuple fetchone() {
1031: 		if (!result) {
1032: 			throw runtime_error("no open result set");
1033: 		}
1034: 		return result->fetchone();
1035: 	}
1036: 
1037: 	py::list fetchall() {
1038: 		if (!result) {
1039: 			throw runtime_error("no open result set");
1040: 		}
1041: 		return result->fetchall();
1042: 	}
1043: 
1044: 	py::dict fetchnumpy() {
1045: 		if (!result) {
1046: 			throw runtime_error("no open result set");
1047: 		}
1048: 		return result->fetchnumpy();
1049: 	}
1050: 	py::object fetchdf() {
1051: 		if (!result) {
1052: 			throw runtime_error("no open result set");
1053: 		}
1054: 		return result->fetchdf();
1055: 	}
1056: 	py::object fetcharrow() {
1057: 		if (!result) {
1058: 			throw runtime_error("no open result set");
1059: 		}
1060: 		return result->fetch_arrow_table();
1061: 	}
1062: 
1063: 	static shared_ptr<DuckDBPyConnection> connect(string database, bool read_only) {
1064: 		auto res = make_shared<DuckDBPyConnection>();
1065: 		DBConfig config;
1066: 		if (read_only) {
1067: 			config.access_mode = AccessMode::READ_ONLY;
1068: 		}
1069: 		res->database = make_unique<DuckDB>(database, &config);
1070: 		ExtensionHelper::LoadAllExtensions(*res->database);
1071: 		res->connection = make_unique<Connection>(*res->database);
1072: 
1073: 		PandasScanFunction scan_fun;
1074: 		CreateTableFunctionInfo info(scan_fun);
1075: 
1076: 		auto &context = *res->connection->context;
1077: 		context.transaction.BeginTransaction();
1078: 		context.catalog.CreateTableFunction(context, &info);
1079: 		context.transaction.Commit();
1080: 
1081: 		return res;
1082: 	}
1083: 
1084: 	shared_ptr<DuckDB> database;
1085: 	unique_ptr<Connection> connection;
1086: 	unordered_map<string, py::object> registered_dfs;
1087: 	unique_ptr<DuckDBPyResult> result;
1088: 	vector<shared_ptr<DuckDBPyConnection>> cursors;
1089: 
1090: 	static vector<Value> transform_python_param_list(py::handle params) {
1091: 		vector<Value> args;
1092: 
1093: 		auto datetime_mod = py::module::import("datetime");
1094: 		auto datetime_date = datetime_mod.attr("date");
1095: 		auto datetime_datetime = datetime_mod.attr("datetime");
1096: 		auto datetime_time = datetime_mod.attr("time");
1097: 		auto decimal_mod = py::module::import("decimal");
1098: 		auto decimal_decimal = decimal_mod.attr("Decimal");
1099: 
1100: 		for (pybind11::handle ele : params) {
1101: 			if (ele.is_none()) {
1102: 				args.push_back(Value());
1103: 			} else if (py::isinstance<py::bool_>(ele)) {
1104: 				args.push_back(Value::BOOLEAN(ele.cast<bool>()));
1105: 			} else if (py::isinstance<py::int_>(ele)) {
1106: 				args.push_back(Value::BIGINT(ele.cast<int64_t>()));
1107: 			} else if (py::isinstance<py::float_>(ele)) {
1108: 				args.push_back(Value::DOUBLE(ele.cast<double>()));
1109: 			} else if (py::isinstance<py::str>(ele)) {
1110: 				args.push_back(Value(ele.cast<string>()));
1111: 			} else if (py::isinstance(ele, decimal_decimal)) {
1112: 				args.push_back(Value(py::str(ele).cast<string>()));
1113: 			} else if (py::isinstance(ele, datetime_datetime)) {
1114: 				auto year = PyDateTime_GET_YEAR(ele.ptr());
1115: 				auto month = PyDateTime_GET_MONTH(ele.ptr());
1116: 				auto day = PyDateTime_GET_DAY(ele.ptr());
1117: 				auto hour = PyDateTime_DATE_GET_HOUR(ele.ptr());
1118: 				auto minute = PyDateTime_DATE_GET_MINUTE(ele.ptr());
1119: 				auto second = PyDateTime_DATE_GET_SECOND(ele.ptr());
1120: 				auto micros = PyDateTime_DATE_GET_MICROSECOND(ele.ptr());
1121: 				args.push_back(Value::TIMESTAMP(year, month, day, hour, minute, second, micros));
1122: 			} else if (py::isinstance(ele, datetime_time)) {
1123: 				auto hour = PyDateTime_TIME_GET_HOUR(ele.ptr());
1124: 				auto minute = PyDateTime_TIME_GET_MINUTE(ele.ptr());
1125: 				auto second = PyDateTime_TIME_GET_SECOND(ele.ptr());
1126: 				auto micros = PyDateTime_TIME_GET_MICROSECOND(ele.ptr());
1127: 				args.push_back(Value::TIME(hour, minute, second, micros));
1128: 			} else if (py::isinstance(ele, datetime_date)) {
1129: 				auto year = PyDateTime_GET_YEAR(ele.ptr());
1130: 				auto month = PyDateTime_GET_MONTH(ele.ptr());
1131: 				auto day = PyDateTime_GET_DAY(ele.ptr());
1132: 				args.push_back(Value::DATE(year, month, day));
1133: 			} else {
1134: 				throw runtime_error("unknown param type " + py::str(ele.get_type()).cast<string>());
1135: 			}
1136: 		}
1137: 		return args;
1138: 	}
1139: };
1140: 
1141: static shared_ptr<DuckDBPyConnection> default_connection_ = nullptr;
1142: 
1143: static DuckDBPyConnection *default_connection() {
1144: 	if (!default_connection_) {
1145: 		default_connection_ = DuckDBPyConnection::connect(":memory:", false);
1146: 	}
1147: 	return default_connection_.get();
1148: }
1149: 
1150: struct DuckDBPyRelation {
1151: 
1152: 	DuckDBPyRelation(shared_ptr<Relation> rel) : rel(rel) {
1153: 	}
1154: 
1155: 	static unique_ptr<DuckDBPyRelation> from_df(py::object df) {
1156: 		return default_connection()->from_df(df);
1157: 	}
1158: 
1159: 	static unique_ptr<DuckDBPyRelation> values(py::object values = py::list()) {
1160: 		return default_connection()->values(values);
1161: 	}
1162: 
1163: 	static unique_ptr<DuckDBPyRelation> from_csv_auto(string filename) {
1164: 		return default_connection()->from_csv_auto(filename);
1165: 	}
1166: 
1167: 	static unique_ptr<DuckDBPyRelation> from_parquet(string filename) {
1168: 		return default_connection()->from_parquet(filename);
1169: 	}
1170: 
1171: 	static unique_ptr<DuckDBPyRelation> from_arrow_table(py::object table) {
1172: 		return default_connection()->from_arrow_table(table);
1173: 	}
1174: 
1175: 	unique_ptr<DuckDBPyRelation> project(string expr) {
1176: 		return make_unique<DuckDBPyRelation>(rel->Project(expr));
1177: 	}
1178: 
1179: 	static unique_ptr<DuckDBPyRelation> project_df(py::object df, string expr) {
1180: 		return default_connection()->from_df(df)->project(expr);
1181: 	}
1182: 
1183: 	unique_ptr<DuckDBPyRelation> alias(string expr) {
1184: 		return make_unique<DuckDBPyRelation>(rel->Alias(expr));
1185: 	}
1186: 
1187: 	static unique_ptr<DuckDBPyRelation> alias_df(py::object df, string expr) {
1188: 		return default_connection()->from_df(df)->alias(expr);
1189: 	}
1190: 
1191: 	unique_ptr<DuckDBPyRelation> filter(string expr) {
1192: 		return make_unique<DuckDBPyRelation>(rel->Filter(expr));
1193: 	}
1194: 
1195: 	static unique_ptr<DuckDBPyRelation> filter_df(py::object df, string expr) {
1196: 		return default_connection()->from_df(df)->filter(expr);
1197: 	}
1198: 
1199: 	unique_ptr<DuckDBPyRelation> limit(int64_t n) {
1200: 		return make_unique<DuckDBPyRelation>(rel->Limit(n));
1201: 	}
1202: 
1203: 	static unique_ptr<DuckDBPyRelation> limit_df(py::object df, int64_t n) {
1204: 		return default_connection()->from_df(df)->limit(n);
1205: 	}
1206: 
1207: 	unique_ptr<DuckDBPyRelation> order(string expr) {
1208: 		return make_unique<DuckDBPyRelation>(rel->Order(expr));
1209: 	}
1210: 
1211: 	static unique_ptr<DuckDBPyRelation> order_df(py::object df, string expr) {
1212: 		return default_connection()->from_df(df)->order(expr);
1213: 	}
1214: 
1215: 	unique_ptr<DuckDBPyRelation> aggregate(string expr, string groups = "") {
1216: 		if (groups.size() > 0) {
1217: 			return make_unique<DuckDBPyRelation>(rel->Aggregate(expr, groups));
1218: 		}
1219: 		return make_unique<DuckDBPyRelation>(rel->Aggregate(expr));
1220: 	}
1221: 
1222: 	static unique_ptr<DuckDBPyRelation> aggregate_df(py::object df, string expr, string groups = "") {
1223: 		return default_connection()->from_df(df)->aggregate(expr, groups);
1224: 	}
1225: 
1226: 	unique_ptr<DuckDBPyRelation> distinct() {
1227: 		return make_unique<DuckDBPyRelation>(rel->Distinct());
1228: 	}
1229: 
1230: 	static unique_ptr<DuckDBPyRelation> distinct_df(py::object df) {
1231: 		return default_connection()->from_df(df)->distinct();
1232: 	}
1233: 
1234: 	py::object to_df() {
1235: 		auto res = make_unique<DuckDBPyResult>();
1236: 		res->result = rel->Execute();
1237: 		if (!res->result->success) {
1238: 			throw runtime_error(res->result->error);
1239: 		}
1240: 		return res->fetchdf();
1241: 	}
1242: 
1243: 	py::object to_arrow_table() {
1244: 		auto res = make_unique<DuckDBPyResult>();
1245: 		res->result = rel->Execute();
1246: 		if (!res->result->success) {
1247: 			throw runtime_error(res->result->error);
1248: 		}
1249: 		return res->fetch_arrow_table();
1250: 	}
1251: 
1252: 	unique_ptr<DuckDBPyRelation> union_(DuckDBPyRelation *other) {
1253: 		return make_unique<DuckDBPyRelation>(rel->Union(other->rel));
1254: 	}
1255: 
1256: 	unique_ptr<DuckDBPyRelation> except(DuckDBPyRelation *other) {
1257: 		return make_unique<DuckDBPyRelation>(rel->Except(other->rel));
1258: 	}
1259: 
1260: 	unique_ptr<DuckDBPyRelation> intersect(DuckDBPyRelation *other) {
1261: 		return make_unique<DuckDBPyRelation>(rel->Intersect(other->rel));
1262: 	}
1263: 
1264: 	unique_ptr<DuckDBPyRelation> join(DuckDBPyRelation *other, string condition) {
1265: 		return make_unique<DuckDBPyRelation>(rel->Join(other->rel, condition));
1266: 	}
1267: 
1268: 	void write_csv(string file) {
1269: 		rel->WriteCSV(file);
1270: 	}
1271: 
1272: 	static void write_csv_df(py::object df, string file) {
1273: 		return default_connection()->from_df(df)->write_csv(file);
1274: 	}
1275: 
1276: 	// should this return a rel with the new view?
1277: 	unique_ptr<DuckDBPyRelation> create_view(string view_name, bool replace = true) {
1278: 		rel->CreateView(view_name, replace);
1279: 		return make_unique<DuckDBPyRelation>(rel);
1280: 	}
1281: 
1282: 	static unique_ptr<DuckDBPyRelation> create_view_df(py::object df, string view_name, bool replace = true) {
1283: 		return default_connection()->from_df(df)->create_view(view_name, replace);
1284: 	}
1285: 
1286: 	unique_ptr<DuckDBPyResult> query(string view_name, string sql_query) {
1287: 		auto res = make_unique<DuckDBPyResult>();
1288: 		res->result = rel->Query(view_name, sql_query);
1289: 		if (!res->result->success) {
1290: 			throw runtime_error(res->result->error);
1291: 		}
1292: 		return res;
1293: 	}
1294: 
1295: 	unique_ptr<DuckDBPyResult> execute() {
1296: 		auto res = make_unique<DuckDBPyResult>();
1297: 		res->result = rel->Execute();
1298: 		if (!res->result->success) {
1299: 			throw runtime_error(res->result->error);
1300: 		}
1301: 		return res;
1302: 	}
1303: 
1304: 	static unique_ptr<DuckDBPyResult> query_df(py::object df, string view_name, string sql_query) {
1305: 		return default_connection()->from_df(df)->query(view_name, sql_query);
1306: 	}
1307: 
1308: 	void insert_into(string table) {
1309: 		rel->Insert(table);
1310: 	}
1311: 
1312: 	void insert(py::object params = py::list()) {
1313: 		vector<vector<Value>> values{DuckDBPyConnection::transform_python_param_list(params)};
1314: 		rel->Insert(values);
1315: 	}
1316: 
1317: 	void create(string table) {
1318: 		rel->Create(table);
1319: 	}
1320: 
1321: 	string print() {
1322: 		return rel->ToString() + "\n---------------------\n-- Result Preview  --\n---------------------\n" +
1323: 		       rel->Limit(10)->Execute()->ToString() + "\n";
1324: 	}
1325: 
1326: 	py::object getattr(py::str key) {
1327: 		auto key_s = key.cast<string>();
1328: 		if (key_s == "alias") {
1329: 			return py::str(string(rel->GetAlias()));
1330: 		} else if (key_s == "type") {
1331: 			return py::str(RelationTypeToString(rel->type));
1332: 		} else if (key_s == "columns") {
1333: 			py::list res;
1334: 			for (auto &col : rel->Columns()) {
1335: 				res.append(col.name);
1336: 			}
1337: 			return move(res);
1338: 		} else if (key_s == "types" || key_s == "dtypes") {
1339: 			py::list res;
1340: 			for (auto &col : rel->Columns()) {
1341: 				res.append(col.type.ToString());
1342: 			}
1343: 			return move(res);
1344: 		}
1345: 		return py::none();
1346: 	}
1347: 
1348: 	shared_ptr<Relation> rel;
1349: };
1350: 
1351: enum PySQLTokenType {
1352: 	PySQLTokenIdentifier = 0,
1353: 	PySQLTokenNumericConstant,
1354: 	PySQLTokenStringConstant,
1355: 	PySQLTokenOperator,
1356: 	PySQLTokenKeyword,
1357: 	PySQLTokenComment
1358: };
1359: 
1360: static py::object py_tokenize(string query) {
1361: 	auto tokens = Parser::Tokenize(query);
1362: 	py::list result;
1363: 	for (auto &token : tokens) {
1364: 		auto tuple = py::tuple(2);
1365: 		tuple[0] = token.start;
1366: 		switch (token.type) {
1367: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_IDENTIFIER:
1368: 			tuple[1] = PySQLTokenIdentifier;
1369: 			break;
1370: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_NUMERIC_CONSTANT:
1371: 			tuple[1] = PySQLTokenNumericConstant;
1372: 			break;
1373: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_STRING_CONSTANT:
1374: 			tuple[1] = PySQLTokenStringConstant;
1375: 			break;
1376: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_OPERATOR:
1377: 			tuple[1] = PySQLTokenOperator;
1378: 			break;
1379: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_KEYWORD:
1380: 			tuple[1] = PySQLTokenKeyword;
1381: 			break;
1382: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_COMMENT:
1383: 			tuple[1] = PySQLTokenComment;
1384: 			break;
1385: 		}
1386: 		result.append(tuple);
1387: 	}
1388: 	return move(result);
1389: }
1390: 
1391: PYBIND11_MODULE(duckdb, m) {
1392: 	m.doc() = "DuckDB is an embeddable SQL OLAP Database Management System";
1393: 	m.attr("__package__") = "duckdb";
1394: 	m.attr("__version__") = DuckDB::LibraryVersion();
1395: 	m.attr("__git_revision__") = DuckDB::SourceID();
1396: 
1397: 	m.def("connect", &DuckDBPyConnection::connect,
1398: 	      "Create a DuckDB database instance. Can take a database file name to read/write persistent data and a "
1399: 	      "read_only flag if no changes are desired",
1400: 	      py::arg("database") = ":memory:", py::arg("read_only") = false);
1401: 	m.def("tokenize", py_tokenize,
1402: 	      "Tokenizes a SQL string, returning a list of (position, type) tuples that can be "
1403: 	      "used for e.g. syntax highlighting",
1404: 	      py::arg("query"));
1405: 	py::enum_<PySQLTokenType>(m, "token_type")
1406: 	    .value("identifier", PySQLTokenType::PySQLTokenIdentifier)
1407: 	    .value("numeric_const", PySQLTokenType::PySQLTokenNumericConstant)
1408: 	    .value("string_const", PySQLTokenType::PySQLTokenStringConstant)
1409: 	    .value("operator", PySQLTokenType::PySQLTokenOperator)
1410: 	    .value("keyword", PySQLTokenType::PySQLTokenKeyword)
1411: 	    .value("comment", PySQLTokenType::PySQLTokenComment)
1412: 	    .export_values();
1413: 
1414: 	auto conn_class =
1415: 	    py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection")
1416: 	        .def("cursor", &DuckDBPyConnection::cursor, "Create a duplicate of the current connection")
1417: 	        .def("duplicate", &DuckDBPyConnection::cursor, "Create a duplicate of the current connection")
1418: 	        .def("execute", &DuckDBPyConnection::execute,
1419: 	             "Execute the given SQL query, optionally using prepared statements with parameters set",
1420: 	             py::arg("query"), py::arg("parameters") = py::list(), py::arg("multiple_parameter_sets") = false)
1421: 	        .def("executemany", &DuckDBPyConnection::executemany,
1422: 	             "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
1423: 	             py::arg("query"), py::arg("parameters") = py::list())
1424: 	        .def("close", &DuckDBPyConnection::close, "Close the connection")
1425: 	        .def("fetchone", &DuckDBPyConnection::fetchone, "Fetch a single row from a result following execute")
1426: 	        .def("fetchall", &DuckDBPyConnection::fetchall, "Fetch all rows from a result following execute")
1427: 	        .def("fetchnumpy", &DuckDBPyConnection::fetchnumpy,
1428: 	             "Fetch a result as list of NumPy arrays following execute")
1429: 	        .def("fetchdf", &DuckDBPyConnection::fetchdf, "Fetch a result as Data.Frame following execute()")
1430: 	        .def("df", &DuckDBPyConnection::fetchdf, "Fetch a result as Data.Frame following execute()")
1431: 	        .def("fetch_arrow_table", &DuckDBPyConnection::fetcharrow,
1432: 	             "Fetch a result as Arrow table following execute()")
1433: 	        .def("arrow", &DuckDBPyConnection::fetcharrow, "Fetch a result as Arrow table following execute()")
1434: 	        .def("begin", &DuckDBPyConnection::begin, "Start a new transaction")
1435: 	        .def("commit", &DuckDBPyConnection::commit, "Commit changes performed within a transaction")
1436: 	        .def("rollback", &DuckDBPyConnection::rollback, "Roll back changes performed within a transaction")
1437: 	        .def("append", &DuckDBPyConnection::append, "Append the passed Data.Frame to the named table",
1438: 	             py::arg("table_name"), py::arg("df"))
1439: 	        .def("register", &DuckDBPyConnection::register_df,
1440: 	             "Register the passed Data.Frame value for querying with a view", py::arg("view_name"), py::arg("df"))
1441: 	        .def("unregister", &DuckDBPyConnection::unregister_df, "Unregister the view name", py::arg("view_name"))
1442: 	        .def("table", &DuckDBPyConnection::table, "Create a relation object for the name'd table",
1443: 	             py::arg("table_name"))
1444: 	        .def("view", &DuckDBPyConnection::view, "Create a relation object for the name'd view",
1445: 	             py::arg("view_name"))
1446: 	        .def("values", &DuckDBPyConnection::values, "Create a relation object from the passed values",
1447: 	             py::arg("values"))
1448: 	        .def("table_function", &DuckDBPyConnection::table_function,
1449: 	             "Create a relation object from the name'd table function with given parameters", py::arg("name"),
1450: 	             py::arg("parameters") = py::list())
1451: 	        .def("from_df", &DuckDBPyConnection::from_df, "Create a relation object from the Data.Frame in df",
1452: 	             py::arg("df"))
1453: 	        .def("from_arrow_table", &DuckDBPyConnection::from_arrow_table,
1454: 	             "Create a relation object from an Arrow table", py::arg("table"))
1455: 	        .def("df", &DuckDBPyConnection::from_df,
1456: 	             "Create a relation object from the Data.Frame in df (alias of from_df)", py::arg("df"))
1457: 	        .def("from_csv_auto", &DuckDBPyConnection::from_csv_auto,
1458: 	             "Create a relation object from the CSV file in file_name", py::arg("file_name"))
1459: 	        .def("from_parquet", &DuckDBPyConnection::from_parquet,
1460: 	             "Create a relation object from the Parquet file in file_name", py::arg("file_name"))
1461: 	        .def("__getattr__", &DuckDBPyConnection::getattr, "Get result set attributes, mainly column names");
1462: 
1463: 	py::class_<DuckDBPyResult>(m, "DuckDBPyResult")
1464: 	    .def("close", &DuckDBPyResult::close)
1465: 	    .def("fetchone", &DuckDBPyResult::fetchone)
1466: 	    .def("fetchall", &DuckDBPyResult::fetchall)
1467: 	    .def("fetchnumpy", &DuckDBPyResult::fetchnumpy)
1468: 	    .def("fetchdf", &DuckDBPyResult::fetchdf)
1469: 	    .def("fetch_df", &DuckDBPyResult::fetchdf)
1470: 	    .def("fetch_arrow_table", &DuckDBPyResult::fetch_arrow_table)
1471: 	    .def("arrow", &DuckDBPyResult::fetch_arrow_table)
1472: 	    .def("df", &DuckDBPyResult::fetchdf);
1473: 
1474: 	py::class_<DuckDBPyRelation>(m, "DuckDBPyRelation")
1475: 	    .def("filter", &DuckDBPyRelation::filter, "Filter the relation object by the filter in filter_expr",
1476: 	         py::arg("filter_expr"))
1477: 	    .def("project", &DuckDBPyRelation::project, "Project the relation object by the projection in project_expr",
1478: 	         py::arg("project_expr"))
1479: 	    .def("set_alias", &DuckDBPyRelation::alias, "Rename the relation object to new alias", py::arg("alias"))
1480: 	    .def("order", &DuckDBPyRelation::order, "Reorder the relation object by order_expr", py::arg("order_expr"))
1481: 	    .def("aggregate", &DuckDBPyRelation::aggregate,
1482: 	         "Compute the aggregate aggr_expr by the optional groups group_expr on the relation", py::arg("aggr_expr"),
1483: 	         py::arg("group_expr") = "")
1484: 	    .def("union", &DuckDBPyRelation::union_,
1485: 	         "Create the set union of this relation object with another relation object in other_rel")
1486: 	    .def("except_", &DuckDBPyRelation::except,
1487: 	         "Create the set except of this relation object with another relation object in other_rel",
1488: 	         py::arg("other_rel"))
1489: 	    .def("intersect", &DuckDBPyRelation::intersect,
1490: 	         "Create the set intersection of this relation object with another relation object in other_rel",
1491: 	         py::arg("other_rel"))
1492: 	    .def("join", &DuckDBPyRelation::join,
1493: 	         "Join the relation object with another relation object in other_rel using the join condition expression "
1494: 	         "in join_condition",
1495: 	         py::arg("other_rel"), py::arg("join_condition"))
1496: 	    .def("distinct", &DuckDBPyRelation::distinct, "Retrieve distinct rows from this relation object")
1497: 	    .def("limit", &DuckDBPyRelation::limit, "Only retrieve the first n rows from this relation object",
1498: 	         py::arg("n"))
1499: 	    .def("query", &DuckDBPyRelation::query,
1500: 	         "Run the given SQL query in sql_query on the view named virtual_table_name that refers to the relation "
1501: 	         "object",
1502: 	         py::arg("virtual_table_name"), py::arg("sql_query"))
1503: 	    .def("execute", &DuckDBPyRelation::execute, "Transform the relation into a result set")
1504: 	    .def("write_csv", &DuckDBPyRelation::write_csv, "Write the relation object to a CSV file in file_name",
1505: 	         py::arg("file_name"))
1506: 	    .def("insert_into", &DuckDBPyRelation::insert_into,
1507: 	         "Inserts the relation object into an existing table named table_name", py::arg("table_name"))
1508: 	    .def("insert", &DuckDBPyRelation::insert, "Inserts the given values into the relation", py::arg("values"))
1509: 	    .def("create", &DuckDBPyRelation::create,
1510: 	         "Creates a new table named table_name with the contents of the relation object", py::arg("table_name"))
1511: 	    .def("create_view", &DuckDBPyRelation::create_view,
1512: 	         "Creates a view named view_name that refers to the relation object", py::arg("view_name"),
1513: 	         py::arg("replace") = true)
1514: 	    .def("to_arrow_table", &DuckDBPyRelation::to_arrow_table, "Transforms the relation object into a Arrow table")
1515: 	    .def("arrow", &DuckDBPyRelation::to_arrow_table, "Transforms the relation object into a Arrow table")
1516: 	    .def("to_df", &DuckDBPyRelation::to_df, "Transforms the relation object into a Data.Frame")
1517: 	    .def("df", &DuckDBPyRelation::to_df, "Transforms the relation object into a Data.Frame")
1518: 	    .def("__str__", &DuckDBPyRelation::print)
1519: 	    .def("__repr__", &DuckDBPyRelation::print)
1520: 	    .def("__getattr__", &DuckDBPyRelation::getattr);
1521: 
1522: 	m.def("values", &DuckDBPyRelation::values, "Create a relation object from the passed values", py::arg("values"));
1523: 	m.def("from_csv_auto", &DuckDBPyRelation::from_csv_auto, "Creates a relation object from the CSV file in file_name",
1524: 	      py::arg("file_name"));
1525: 	m.def("from_parquet", &DuckDBPyRelation::from_parquet,
1526: 	      "Creates a relation object from the Parquet file in file_name", py::arg("file_name"));
1527: 	m.def("df", &DuckDBPyRelation::from_df, "Create a relation object from the Data.Frame df", py::arg("df"));
1528: 	m.def("from_df", &DuckDBPyRelation::from_df, "Create a relation object from the Data.Frame df", py::arg("df"));
1529: 	m.def("from_arrow_table", &DuckDBPyRelation::from_arrow_table, "Create a relation object from an Arrow table",
1530: 	      py::arg("table"));
1531: 	m.def("arrow", &DuckDBPyRelation::from_arrow_table, "Create a relation object from an Arrow table",
1532: 	      py::arg("table"));
1533: 	m.def("filter", &DuckDBPyRelation::filter_df, "Filter the Data.Frame df by the filter in filter_expr",
1534: 	      py::arg("df"), py::arg("filter_expr"));
1535: 	m.def("project", &DuckDBPyRelation::project_df, "Project the Data.Frame df by the projection in project_expr",
1536: 	      py::arg("df"), py::arg("project_expr"));
1537: 	m.def("alias", &DuckDBPyRelation::alias_df, "Create a relation from Data.Frame df with the passed alias",
1538: 	      py::arg("df"), py::arg("alias"));
1539: 	m.def("order", &DuckDBPyRelation::order_df, "Reorder the Data.Frame df by order_expr", py::arg("df"),
1540: 	      py::arg("order_expr"));
1541: 	m.def("aggregate", &DuckDBPyRelation::aggregate_df,
1542: 	      "Compute the aggregate aggr_expr by the optional groups group_expr on Data.frame df", py::arg("df"),
1543: 	      py::arg("aggr_expr"), py::arg("group_expr") = "");
1544: 	m.def("distinct", &DuckDBPyRelation::distinct_df, "Compute the distinct rows from Data.Frame df ", py::arg("df"));
1545: 	m.def("limit", &DuckDBPyRelation::limit_df, "Retrieve the first n rows from the Data.Frame df", py::arg("df"),
1546: 	      py::arg("n"));
1547: 	m.def("query", &DuckDBPyRelation::query_df,
1548: 	      "Run the given SQL query in sql_query on the view named virtual_table_name that contains the content of "
1549: 	      "Data.Frame df",
1550: 	      py::arg("df"), py::arg("virtual_table_name"), py::arg("sql_query"));
1551: 	m.def("write_csv", &DuckDBPyRelation::write_csv_df, "Write the Data.Frame df to a CSV file in file_name",
1552: 	      py::arg("df"), py::arg("file_name"));
1553: 
1554: 	// we need this because otherwise we try to remove registered_dfs on shutdown when python is already dead
1555: 	auto clean_default_connection = []() { default_connection_.reset(); };
1556: 	m.add_object("_clean_default_connection", py::capsule(clean_default_connection));
1557: 	PyDateTime_IMPORT;
1558: }
[end of tools/pythonpkg/duckdb_python.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: