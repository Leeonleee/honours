{
  "repo": "duckdb/duckdb",
  "pull_number": 1226,
  "instance_id": "duckdb__duckdb-1226",
  "issue_numbers": [
    "1220",
    "1220"
  ],
  "base_commit": "d27dd50d3e785845065bbf9ba29ad7984ec96b0a",
  "patch": "diff --git a/scripts/package_build.py b/scripts/package_build.py\nindex dac2ed816247..7a792d0f2bbc 100644\n--- a/scripts/package_build.py\n+++ b/scripts/package_build.py\n@@ -49,6 +49,7 @@ def includes(extensions):\n     includes = []\n     includes.append(os.path.join(scripts_dir, '..', 'src', 'include'))\n     includes.append(os.path.join(scripts_dir, '..'))\n+    includes.append(os.path.join(scripts_dir, '..', 'third_party', 'utf8proc', 'include'))\n     for ext in extensions:\n         includes.append(os.path.join(scripts_dir, '..', 'extension', ext, 'include'))\n     return includes\ndiff --git a/src/include/duckdb/common/types/string_type.hpp b/src/include/duckdb/common/types/string_type.hpp\nindex 8d7b19f60ef1..e9da89fd9372 100644\n--- a/src/include/duckdb/common/types/string_type.hpp\n+++ b/src/include/duckdb/common/types/string_type.hpp\n@@ -25,7 +25,6 @@ struct string_t {\n \tstring_t() = default;\n \tstring_t(uint32_t len) {\n \t\tvalue.inlined.length = len;\n-\t\tmemset(value.inlined.inlined, 0, INLINE_LENGTH);\n \t}\n \tstring_t(const char *data, uint32_t len) {\n \t\tvalue.inlined.length = len;\ndiff --git a/third_party/utf8proc/include/utf8proc_wrapper.hpp b/third_party/utf8proc/include/utf8proc_wrapper.hpp\nindex 1986736332d9..9135b90d2e88 100644\n--- a/third_party/utf8proc/include/utf8proc_wrapper.hpp\n+++ b/third_party/utf8proc/include/utf8proc_wrapper.hpp\n@@ -21,6 +21,11 @@ class Utf8Proc {\n \tstatic size_t NextGraphemeCluster(const char *s, size_t len, size_t pos);\n \t//! Returns the position (in bytes) of the previous grapheme cluster\n \tstatic size_t PreviousGraphemeCluster(const char *s, size_t len, size_t pos);\n+\n+\t//! Transform a codepoint to utf8 and writes it to \"c\", sets \"sz\" to the size of the codepoint\n+\tstatic bool CodepointToUtf8(int cp, int &sz, char *c);\n+\t//! Returns the codepoint length in bytes when encoded in UTF8\n+\tstatic int CodepointLength(int cp);\n };\n \n }\ndiff --git a/third_party/utf8proc/utf8proc_wrapper.cpp b/third_party/utf8proc/utf8proc_wrapper.cpp\nindex 87e5e2bb4b28..15c1e4cacae8 100644\n--- a/third_party/utf8proc/utf8proc_wrapper.cpp\n+++ b/third_party/utf8proc/utf8proc_wrapper.cpp\n@@ -81,6 +81,14 @@ size_t Utf8Proc::PreviousGraphemeCluster(const char *s, size_t len, size_t cpos)\n \t}\n }\n \n+bool Utf8Proc::CodepointToUtf8(int cp, int &sz, char *c) {\n+\treturn utf8proc_codepoint_to_utf8(cp, sz, c);\n+}\n+\n+int Utf8Proc::CodepointLength(int cp) {\n+\treturn utf8proc_codepoint_length(cp);\n+}\n+\n }\n \n size_t utf8proc_next_grapheme_cluster(const char *s, size_t len, size_t pos) {\ndiff --git a/tools/pythonpkg/duckdb_python.cpp b/tools/pythonpkg/duckdb_python.cpp\nindex fcac87c667d4..c34e5cf517d8 100644\n--- a/tools/pythonpkg/duckdb_python.cpp\n+++ b/tools/pythonpkg/duckdb_python.cpp\n@@ -16,6 +16,7 @@\n #include \"duckdb/parser/parsed_data/create_table_function_info.hpp\"\n #include \"duckdb/parser/parser.hpp\"\n #include \"extension/extension_helper.hpp\"\n+#include \"utf8proc_wrapper.hpp\"\n \n #include <random>\n \n@@ -162,24 +163,26 @@ std::string generate() {\n \n enum class PandasType : uint8_t {\n \tBOOLEAN,\n-\tTINYINT_NATIVE,\n-\tTINYINT_OBJECT,\n-\tSMALLINT_NATIVE,\n-\tSMALLINT_OBJECT,\n-\tINTEGER_NATIVE,\n-\tINTEGER_OBJECT,\n-\tBIGINT_NATIVE,\n-\tBIGINT_OBJECT,\n+\tTINYINT,\n+\tSMALLINT,\n+\tINTEGER,\n+\tBIGINT,\n \tFLOAT,\n \tDOUBLE,\n-\tTIMESTAMP_NATIVE,\n-\tTIMESTAMP_OBJECT,\n+\tTIMESTAMP,\n \tVARCHAR\n };\n \n+struct NumPyArrayWrapper {\n+\tNumPyArrayWrapper(py::array numpy_array) : numpy_array(move(numpy_array)) {}\n+\n+\tpy::array numpy_array;\n+};\n+\n struct PandasColumnBindData {\n \tPandasType pandas_type;\n \tpy::array numpy_col;\n+\tunique_ptr<NumPyArrayWrapper> mask;\n };\n \n struct PandasScanFunctionData : public TableFunctionData {\n@@ -209,42 +212,24 @@ struct PandasScanFunction : public TableFunction {\n \t\tif (col_type == \"bool\") {\n \t\t\tduckdb_col_type = LogicalType::BOOLEAN;\n \t\t\tpandas_type = PandasType::BOOLEAN;\n-\t\t} else if (col_type == \"int8\") {\n+\t\t} else if (col_type == \"int8\" || col_type == \"Int8\") {\n \t\t\tduckdb_col_type = LogicalType::TINYINT;\n-\t\t\tpandas_type = PandasType::TINYINT_NATIVE;\n-\t\t} else if (col_type == \"Int8\") {\n-\t\t\tduckdb_col_type = LogicalType::TINYINT;\n-\t\t\tpandas_type = PandasType::TINYINT_OBJECT;\n-\t\t} else if (col_type == \"int16\") {\n-\t\t\tduckdb_col_type = LogicalType::SMALLINT;\n-\t\t\tpandas_type = PandasType::SMALLINT_NATIVE;\n-\t\t} else if (col_type == \"Int16\") {\n+\t\t\tpandas_type = PandasType::TINYINT;\n+\t\t} else if (col_type == \"int16\" || col_type == \"Int16\") {\n \t\t\tduckdb_col_type = LogicalType::SMALLINT;\n-\t\t\tpandas_type = PandasType::SMALLINT_OBJECT;\n-\t\t} else if (col_type == \"int32\") {\n+\t\t\tpandas_type = PandasType::SMALLINT;\n+\t\t} else if (col_type == \"int32\" || col_type == \"Int32\") {\n \t\t\tduckdb_col_type = LogicalType::INTEGER;\n-\t\t\tpandas_type = PandasType::INTEGER_NATIVE;\n-\t\t} else if (col_type == \"Int32\") {\n-\t\t\tduckdb_col_type = LogicalType::INTEGER;\n-\t\t\tpandas_type = PandasType::INTEGER_OBJECT;\n-\t\t} else if (col_type == \"int64\") {\n-\t\t\tduckdb_col_type = LogicalType::BIGINT;\n-\t\t\tpandas_type = PandasType::BIGINT_NATIVE;\n-\t\t} else if (col_type == \"Int64\") {\n+\t\t\tpandas_type = PandasType::INTEGER;\n+\t\t} else if (col_type == \"int64\" || col_type == \"Int64\") {\n \t\t\tduckdb_col_type = LogicalType::BIGINT;\n-\t\t\tpandas_type = PandasType::BIGINT_OBJECT;\n+\t\t\tpandas_type = PandasType::BIGINT;\n \t\t} else if (col_type == \"float32\") {\n \t\t\tduckdb_col_type = LogicalType::FLOAT;\n \t\t\tpandas_type = PandasType::FLOAT;\n \t\t} else if (col_type == \"float64\") {\n \t\t\tduckdb_col_type = LogicalType::DOUBLE;\n \t\t\tpandas_type = PandasType::DOUBLE;\n-\t\t} else if (col_type == \"datetime64[ns]\") {\n-\t\t\tduckdb_col_type = LogicalType::TIMESTAMP;\n-\t\t\tpandas_type = PandasType::TIMESTAMP_NATIVE;\n-\t\t} else if (StringUtil::StartsWith(col_type, \"datetime64[ns\")) {\n-\t\t\tduckdb_col_type = LogicalType::TIMESTAMP;\n-\t\t\tpandas_type = PandasType::TIMESTAMP_OBJECT;\n \t\t} else if (col_type == \"object\") {\n \t\t\t// this better be strings\n \t\t\tduckdb_col_type = LogicalType::VARCHAR;\n@@ -280,17 +265,33 @@ struct PandasScanFunction : public TableFunction {\n \t\tfor (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {\n \t\t\tLogicalType duckdb_col_type;\n \t\t\tPandasColumnBindData bind_data;\n-\t\t\tbind_data.numpy_col = py::array(get_fun(df_columns[col_idx]).attr(\"to_numpy\")());\n \n \t\t\tauto col_type = string(py::str(df_types[col_idx]));\n-\t\t\tif (col_type == \"category\") {\n-\t\t\t\t// for category types, we use the converted numpy type\n-\t\t\t\tauto numpy_type = bind_data.numpy_col.attr(\"dtype\");\n-\t\t\t\tauto category_type = string(py::str(numpy_type));\n-\t\t\t\tConvertPandasType(category_type, duckdb_col_type, bind_data.pandas_type);\n+\t\t\tif (col_type == \"Int8\" || col_type == \"Int16\" || col_type == \"Int32\" || col_type == \"Int64\") {\n+\t\t\t\t// numeric object\n+\t\t\t\t// fetch the internal data and mask array\n+\t\t\t\tbind_data.numpy_col = get_fun(df_columns[col_idx]).attr(\"array\").attr(\"_data\");\n+\t\t\t\tbind_data.mask = make_unique<NumPyArrayWrapper>(get_fun(df_columns[col_idx]).attr(\"array\").attr(\"_mask\"));\n+\t\t\t\tConvertPandasType(col_type, duckdb_col_type, bind_data.pandas_type);\n+\t\t\t} else if (StringUtil::StartsWith(col_type, \"datetime64[ns\") || col_type == \"<M8[ns]\") {\n+\t\t\t\t// timestamp type\n+\t\t\t\tbind_data.numpy_col = get_fun(df_columns[col_idx]).attr(\"array\").attr(\"_data\");\n+\t\t\t\tbind_data.mask = nullptr;\n+\t\t\t\tduckdb_col_type = LogicalType::TIMESTAMP;\n+\t\t\t\tbind_data.pandas_type = PandasType::TIMESTAMP;\n \t\t\t} else {\n \t\t\t\t// regular type\n-\t\t\t\tConvertPandasType(col_type, duckdb_col_type, bind_data.pandas_type);\n+\t\t\t\tauto column = get_fun(df_columns[col_idx]);\n+\t\t\t\tbind_data.numpy_col = py::array(column.attr(\"to_numpy\")());\n+\t\t\t\tbind_data.mask = nullptr;\n+\t\t\t\tif (col_type == \"category\") {\n+\t\t\t\t\t// for category types, we use the converted numpy type\n+\t\t\t\t\tauto numpy_type = bind_data.numpy_col.attr(\"dtype\");\n+\t\t\t\t\tauto category_type = string(py::str(numpy_type));\n+\t\t\t\t\tConvertPandasType(category_type, duckdb_col_type, bind_data.pandas_type);\n+\t\t\t\t} else {\n+\t\t\t\t\tConvertPandasType(col_type, duckdb_col_type, bind_data.pandas_type);\n+\t\t\t\t}\n \t\t\t}\n \t\t\tnames.push_back(string(py::str(df_columns[col_idx])));\n \t\t\treturn_types.push_back(duckdb_col_type);\n@@ -312,18 +313,16 @@ struct PandasScanFunction : public TableFunction {\n \t}\n \n \ttemplate <class T>\n-\tstatic void scan_pandas_numeric_object(py::array &numpy_col, idx_t count, idx_t offset, Vector &out) {\n-\t\tauto src_ptr = (PyObject **)numpy_col.data();\n-\t\tauto tgt_ptr = FlatVector::GetData<T>(out);\n-\t\tauto &nullmask = FlatVector::Nullmask(out);\n-\t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tauto obj = src_ptr[offset + i];\n-\t\t\tauto &py_obj = *((py::object *)&obj);\n-\t\t\tif (!py::isinstance<py::int_>(py_obj)) {\n-\t\t\t\tnullmask[i] = true;\n-\t\t\t\tcontinue;\n+\tstatic void scan_pandas_numeric(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {\n+\t\tscan_pandas_column<T>(bind_data.numpy_col, count, offset, out);\n+\t\tif (bind_data.mask) {\n+\t\t\tauto mask = (bool *) bind_data.mask->numpy_array.data();\n+\t\t\tfor(idx_t i = 0; i < count; i++) {\n+\t\t\t\tauto is_null = mask[offset + i];\n+\t\t\t\tif (is_null) {\n+\t\t\t\t\tFlatVector::SetNull(out, i, true);\n+\t\t\t\t}\n \t\t\t}\n-\t\t\ttgt_ptr[i] = py_obj.cast<T>();\n \t\t}\n \t}\n \n@@ -342,34 +341,42 @@ struct PandasScanFunction : public TableFunction {\n \t\t}\n \t}\n \n+\ttemplate<class T>\n+\tstatic string_t DecodePythonUnicode(T *codepoints, idx_t codepoint_count, Vector &out) {\n+\t\t// first figure out how many bytes to allocate\n+\t\tidx_t utf8_length = 0;\n+\t\tfor(idx_t i = 0; i < codepoint_count; i++) {\n+\t\t\tint len = Utf8Proc::CodepointLength(int(codepoints[i]));\n+\t\t\tD_ASSERT(len >= 1);\n+\t\t\tutf8_length += len;\n+\t\t}\n+\t\tint sz;\n+\t\tauto result = StringVector::EmptyString(out, utf8_length);\n+\t\tauto target = result.GetDataWriteable();\n+\t\tfor(idx_t i = 0; i < codepoint_count; i++) {\n+\t\t\tUtf8Proc::CodepointToUtf8(int(codepoints[i]), sz, target);\n+\t\t\tD_ASSERT(sz >= 1);\n+\t\t\ttarget += sz;\n+\t\t}\n+\t\treturn result;\n+\t}\n+\n \tstatic void ConvertVector(PandasColumnBindData &bind_data, py::array &numpy_col, idx_t count, idx_t offset, Vector &out) {\n \t\tswitch (bind_data.pandas_type) {\n \t\tcase PandasType::BOOLEAN:\n \t\t\tscan_pandas_column<bool>(numpy_col, count, offset, out);\n \t\t\tbreak;\n-\t\tcase PandasType::TINYINT_NATIVE:\n-\t\t\tscan_pandas_column<int8_t>(numpy_col, count, offset, out);\n+\t\tcase PandasType::TINYINT:\n+\t\t\tscan_pandas_numeric<int8_t>(bind_data, count, offset, out);\n \t\t\tbreak;\n-\t\tcase PandasType::SMALLINT_NATIVE:\n-\t\t\tscan_pandas_column<int16_t>(numpy_col, count, offset, out);\n+\t\tcase PandasType::SMALLINT:\n+\t\t\tscan_pandas_numeric<int16_t>(bind_data, count, offset, out);\n \t\t\tbreak;\n-\t\tcase PandasType::INTEGER_NATIVE:\n-\t\t\tscan_pandas_column<int32_t>(numpy_col, count, offset, out);\n+\t\tcase PandasType::INTEGER:\n+\t\t\tscan_pandas_numeric<int32_t>(bind_data, count, offset, out);\n \t\t\tbreak;\n-\t\tcase PandasType::BIGINT_NATIVE:\n-\t\t\tscan_pandas_column<int64_t>(numpy_col, count, offset, out);\n-\t\t\tbreak;\n-\t\tcase PandasType::TINYINT_OBJECT:\n-\t\t\tscan_pandas_numeric_object<int8_t>(numpy_col, count, offset, out);\n-\t\t\tbreak;\n-\t\tcase PandasType::SMALLINT_OBJECT:\n-\t\t\tscan_pandas_numeric_object<int16_t>(numpy_col, count, offset, out);\n-\t\t\tbreak;\n-\t\tcase PandasType::INTEGER_OBJECT:\n-\t\t\tscan_pandas_numeric_object<int32_t>(numpy_col, count, offset, out);\n-\t\t\tbreak;\n-\t\tcase PandasType::BIGINT_OBJECT:\n-\t\t\tscan_pandas_numeric_object<int64_t>(numpy_col, count, offset, out);\n+\t\tcase PandasType::BIGINT:\n+\t\t\tscan_pandas_numeric<int64_t>(bind_data, count, offset, out);\n \t\t\tbreak;\n \t\tcase PandasType::FLOAT:\n \t\t\tscan_pandas_fp_column<float>((float *)numpy_col.data(), count, offset,\n@@ -378,7 +385,7 @@ struct PandasScanFunction : public TableFunction {\n \t\tcase PandasType::DOUBLE:\n \t\t\tscan_pandas_fp_column<double>((double *)numpy_col.data(), count, offset, out);\n \t\t\tbreak;\n-\t\tcase PandasType::TIMESTAMP_NATIVE: {\n+\t\tcase PandasType::TIMESTAMP: {\n \t\t\tauto src_ptr = (int64_t *)numpy_col.data();\n \t\t\tauto tgt_ptr = FlatVector::GetData<timestamp_t>(out);\n \t\t\tauto &nullmask = FlatVector::Nullmask(out);\n@@ -394,27 +401,6 @@ struct PandasScanFunction : public TableFunction {\n \t\t\t}\n \t\t\tbreak;\n \t\t}\n-\t\tcase PandasType::TIMESTAMP_OBJECT: {\n-\t\t\tauto src_ptr = (PyObject **)numpy_col.data();\n-\t\t\tauto tgt_ptr = FlatVector::GetData<timestamp_t>(out);\n-\t\t\tauto &nullmask = FlatVector::Nullmask(out);\n-\t\t\tauto pandas_mod = py::module::import(\"pandas\");\n-\t\t\tauto pandas_datetime = pandas_mod.attr(\"Timestamp\");\n-\t\t\tfor (idx_t row = 0; row < count; row++) {\n-\t\t\t\tauto source_idx = offset + row;\n-\t\t\t\tauto val = src_ptr[source_idx];\n-\t\t\t\tauto &py_obj = *((py::object *)&val);\n-\t\t\t\tif (!py::isinstance(py_obj, pandas_datetime)) {\n-\t\t\t\t\tnullmask[row] = true;\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n-\t\t\t\t// FIXME: consider timezone\n-\t\t\t\tauto epoch = py_obj.attr(\"timestamp\")();\n-\t\t\t\tauto seconds = int64_t(epoch.cast<double>());\n-\t\t\t\ttgt_ptr[row] = Timestamp::FromEpochSeconds(seconds);\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n \t\tcase PandasType::VARCHAR: {\n \t\t\tauto src_ptr = (PyObject **)numpy_col.data();\n \t\t\tauto tgt_ptr = FlatVector::GetData<string_t>(out);\n@@ -422,21 +408,61 @@ struct PandasScanFunction : public TableFunction {\n \t\t\t\tauto source_idx = offset + row;\n \t\t\t\tauto val = src_ptr[source_idx];\n #if PY_MAJOR_VERSION >= 3\n-\t\t\t\tif (!PyUnicode_Check(val)) {\n+\t\t\t\t// Python 3 string representation:\n+\t\t\t\t// https://github.com/python/cpython/blob/3a8fdb28794b2f19f6c8464378fb8b46bce1f5f4/Include/cpython/unicodeobject.h#L79\n+\t\t\t\tif (!PyUnicode_CheckExact(val)) {\n \t\t\t\t\tFlatVector::SetNull(out, row, true);\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n-\t\t\t\tif (PyUnicode_READY(val) != 0) {\n-\t\t\t\t\tthrow runtime_error(\"failure in PyUnicode_READY\");\n+\t\t\t\tif (PyUnicode_IS_COMPACT_ASCII(val)) {\n+\t\t\t\t\t// ascii string: we can zero copy\n+\t\t\t\t\ttgt_ptr[row] = string_t((const char*) PyUnicode_DATA(val), PyUnicode_GET_LENGTH(val));\n+\t\t\t\t} else {\n+\t\t\t\t\t// unicode gunk\n+\t\t\t\t\tauto ascii_obj = (PyASCIIObject *) val;\n+\t\t\t\t\tauto unicode_obj = (PyCompactUnicodeObject *) val;\n+\t\t\t\t\t// compact unicode string: is there utf8 data available?\n+\t\t\t\t\tif (unicode_obj->utf8) {\n+\t\t\t\t\t\t// there is! zero copy\n+\t\t\t\t\t\ttgt_ptr[row] = string_t((const char*) unicode_obj->utf8, unicode_obj->utf8_length);\n+\t\t\t\t\t} else if (PyUnicode_IS_COMPACT(unicode_obj) && !PyUnicode_IS_ASCII(unicode_obj)) {\n+\t\t\t\t\t\tauto kind = PyUnicode_KIND(val);\n+\t\t\t\t\t\tswitch(kind) {\n+\t\t\t\t\t\tcase PyUnicode_1BYTE_KIND:\n+\t\t\t\t\t\t\ttgt_ptr[row] = DecodePythonUnicode<Py_UCS1>(PyUnicode_1BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);\n+\t\t\t\t\t\t\tbreak;\n+\t\t\t\t\t\tcase PyUnicode_2BYTE_KIND:\n+\t\t\t\t\t\t\ttgt_ptr[row] = DecodePythonUnicode<Py_UCS2>(PyUnicode_2BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);\n+\t\t\t\t\t\t\tbreak;\n+\t\t\t\t\t\tcase PyUnicode_4BYTE_KIND:\n+\t\t\t\t\t\t\ttgt_ptr[row] = DecodePythonUnicode<Py_UCS4>(PyUnicode_4BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);\n+\t\t\t\t\t\t\tbreak;\n+\t\t\t\t\t\tdefault:\n+\t\t\t\t\t\t\tthrow runtime_error(\"Unsupported typekind for Python Unicode Compact decode\");\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else if (ascii_obj->state.kind == PyUnicode_WCHAR_KIND) {\n+\t\t\t\t\t\tthrow runtime_error(\"Unsupported: decode not ready legacy string\");\n+\t\t\t\t\t} else if (!PyUnicode_IS_COMPACT(unicode_obj) && ascii_obj->state.kind != PyUnicode_WCHAR_KIND) {\n+\t\t\t\t\t\tthrow runtime_error(\"Unsupported: decode ready legacy string\");\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tthrow runtime_error(\"Unsupported string type: no clue what this string is\");\n+\t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\ttgt_ptr[row] = StringVector::AddString(out, ((py::object *)&val)->cast<string>());\n #else\n-\t\t\t\tif (!py::isinstance<py::str>(*((py::object *)&val))) {\n+\t\t\t\tif (PyString_CheckExact(val)) {\n+\t\t\t\t\tauto dataptr = PyString_AS_STRING(val);\n+\t\t\t\t\tauto size = PyString_GET_SIZE(val);\n+\t\t\t\t\t// string object: directly pass the data\n+\t\t\t\t\tif (Utf8Proc::Analyze(dataptr, size) == UnicodeType::INVALID) {\n+\t\t\t\t\t\tthrow runtime_error(\"String does contains invalid UTF8! Please encode as UTF8 first\");\n+\t\t\t\t\t}\n+\t\t\t\t\ttgt_ptr[row] = string_t(dataptr, size);\n+\t\t\t\t} else if (PyUnicode_CheckExact(val)) {\n+\t\t\t\t\tthrow std::runtime_error(\"Unicode is only supported in Python 3 and up.\");\n+\t\t\t\t} else {\n \t\t\t\t\tFlatVector::SetNull(out, row, true);\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n-\n-\t\t\t\ttgt_ptr[row] = StringVector::AddString(out, ((py::object *)&val)->cast<string>());\n #endif\n \t\t\t}\n \t\t\tbreak;\n@@ -444,21 +470,21 @@ struct PandasScanFunction : public TableFunction {\n \t\tdefault:\n \t\t\tthrow runtime_error(\"Unsupported type \" + out.type.ToString());\n \t\t}\n-\n \t}\n \n+\t//! The main pandas scan function: note that this can be called in parallel without the GIL\n+\t//! hence this needs to be GIL-safe, i.e. no methods that create Python objects are allowed\n \tstatic void pandas_scan_function(ClientContext &context, const FunctionData *bind_data,\n \t                                 FunctionOperatorData *operator_state, DataChunk &output) {\n \t\tauto &data = (PandasScanFunctionData &)*bind_data;\n \t\tauto &state = (PandasScanState &)*operator_state;\n \n+\n \t\tif (state.position >= data.row_count) {\n \t\t\treturn;\n \t\t}\n \t\tidx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, data.row_count - state.position);\n \n-\t\tauto df_names = py::list(data.df.attr(\"columns\"));\n-\n \t\toutput.SetCardinality(this_count);\n \t\tfor (idx_t col_idx = 0; col_idx < output.ColumnCount(); col_idx++) {\n \t\t\tConvertVector(data.pandas_bind_data[col_idx], data.pandas_bind_data[col_idx].numpy_col, this_count, state.position, output.data[col_idx]);\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/test_dbapi12.py b/tools/pythonpkg/tests/test_dbapi12.py\nindex 7c37f6c27b25..911926c894c1 100644\n--- a/tools/pythonpkg/tests/test_dbapi12.py\n+++ b/tools/pythonpkg/tests/test_dbapi12.py\n@@ -2,52 +2,55 @@\n import tempfile\n import os\n import pandas as pd\n+import sys\n \n class TestRelationApi(object):\n-\tdef test_readonly(self, duckdb_cursor):\n+    def test_readonly(self, duckdb_cursor):\n+        if sys.version_info.major < 3:\n+            return\n \n-\t\ttest_df = pd.DataFrame.from_dict({\"i\":[1, 2, 3], \"j\":[\"one\", \"two\", \"three\"]})\n+        test_df = pd.DataFrame.from_dict({\"i\":[1, 2, 3], \"j\":[\"one\", \"two\", \"three\"]})\n \n-\t\tdef test_rel(rel, duckdb_cursor):\n-\t\t\tres = rel.filter('i < 3').order('j').project('i').union(rel.filter('i > 2').project('i')).join(rel.set_alias('a1'), 'i').project('CAST(i as BIGINT) i, j').order('i')\n-\t\t\tpd.testing.assert_frame_equal(res.to_df(), test_df)\n-\t\t\tres3 = duckdb_cursor.from_df(res.to_df()).to_df()\n-\t\t\tpd.testing.assert_frame_equal(res3, test_df)\n+        def test_rel(rel, duckdb_cursor):\n+            res = rel.filter('i < 3').order('j').project('i').union(rel.filter('i > 2').project('i')).join(rel.set_alias('a1'), 'i').project('CAST(i as BIGINT) i, j').order('i')\n+            pd.testing.assert_frame_equal(res.to_df(), test_df)\n+            res3 = duckdb_cursor.from_df(res.to_df()).to_df()\n+            pd.testing.assert_frame_equal(res3, test_df)\n \n-\t\t\tdf_sql = res.query('x', 'select CAST(i as BIGINT) i, j from x')\n-\t\t\tpd.testing.assert_frame_equal(df_sql.fetchdf(), test_df)\n+            df_sql = res.query('x', 'select CAST(i as BIGINT) i, j from x')\n+            pd.testing.assert_frame_equal(df_sql.fetchdf(), test_df)\n \n-\t\t\tres2 = res.aggregate('i, count(j) as cj', 'i').order('i')\n-\t\t\tcmp_df = pd.DataFrame.from_dict({\"i\":[1, 2, 3], \"cj\":[1, 1, 1]})\n-\t\t\tpd.testing.assert_frame_equal(res2.to_df(), cmp_df)\n+            res2 = res.aggregate('i, count(j) as cj', 'i').order('i')\n+            cmp_df = pd.DataFrame.from_dict({\"i\":[1, 2, 3], \"cj\":[1, 1, 1]})\n+            pd.testing.assert_frame_equal(res2.to_df(), cmp_df)\n \n-\t\t\trel.create('a2')\n-\t\t\trel_a2 = duckdb_cursor.table('a2').project('CAST(i as BIGINT) i, j').to_df()\n-\t\t\tpd.testing.assert_frame_equal(rel_a2, test_df)\n+            rel.create('a2')\n+            rel_a2 = duckdb_cursor.table('a2').project('CAST(i as BIGINT) i, j').to_df()\n+            pd.testing.assert_frame_equal(rel_a2, test_df)\n \n-\t\t\tduckdb_cursor.execute('DROP TABLE IF EXISTS a3')\n-\t\t\tduckdb_cursor.execute('CREATE TABLE a3 (i INTEGER, j STRING)')\n-\t\t\trel.insert_into('a3')\n-\t\t\trel_a3 = duckdb_cursor.table('a3').project('CAST(i as BIGINT) i, j').to_df()\n-\t\t\tpd.testing.assert_frame_equal(rel_a3, test_df)\n+            duckdb_cursor.execute('DROP TABLE IF EXISTS a3')\n+            duckdb_cursor.execute('CREATE TABLE a3 (i INTEGER, j STRING)')\n+            rel.insert_into('a3')\n+            rel_a3 = duckdb_cursor.table('a3').project('CAST(i as BIGINT) i, j').to_df()\n+            pd.testing.assert_frame_equal(rel_a3, test_df)\n \n-\t\tduckdb_cursor.execute('CREATE TABLE a (i INTEGER, j STRING)')\n-\t\tduckdb_cursor.execute(\"INSERT INTO a VALUES (1, 'one'), (2, 'two'), (3, 'three')\")\n-\t\tduckdb_cursor.execute('CREATE VIEW v AS SELECT * FROM a')\n+        duckdb_cursor.execute('CREATE TABLE a (i INTEGER, j STRING)')\n+        duckdb_cursor.execute(\"INSERT INTO a VALUES (1, 'one'), (2, 'two'), (3, 'three')\")\n+        duckdb_cursor.execute('CREATE VIEW v AS SELECT * FROM a')\n \n-\t\tduckdb_cursor.execute('CREATE TEMPORARY TABLE at (i INTEGER)')\n-\t\tduckdb_cursor.execute('CREATE TEMPORARY VIEW vt AS SELECT * FROM at')\n+        duckdb_cursor.execute('CREATE TEMPORARY TABLE at (i INTEGER)')\n+        duckdb_cursor.execute('CREATE TEMPORARY VIEW vt AS SELECT * FROM at')\n \n-\t\trel_a = duckdb_cursor.table('a')\n-\t\trel_v = duckdb_cursor.view('v')\n-\t\t#rel_at = duckdb_cursor.table('at')\n-\t\t#rel_vt = duckdb_cursor.view('vt')\n+        rel_a = duckdb_cursor.table('a')\n+        rel_v = duckdb_cursor.view('v')\n+        #rel_at = duckdb_cursor.table('at')\n+        #rel_vt = duckdb_cursor.view('vt')\n \n-\t\trel_df = duckdb_cursor.from_df(test_df)\n+        rel_df = duckdb_cursor.from_df(test_df)\n \n-\t\ttest_rel(rel_a, duckdb_cursor)\n-\t\ttest_rel(rel_v, duckdb_cursor)\n-\t\ttest_rel(rel_df, duckdb_cursor)\n+        test_rel(rel_a, duckdb_cursor)\n+        test_rel(rel_v, duckdb_cursor)\n+        test_rel(rel_df, duckdb_cursor)\n \n # cursor = duckdb.connect().cursor()\n # TestRelationApi().test_readonly(cursor)\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/tests/test_parallel_pandas_scan.py b/tools/pythonpkg/tests/test_parallel_pandas_scan.py\nnew file mode 100644\nindex 000000000000..41427487a655\n--- /dev/null\n+++ b/tools/pythonpkg/tests/test_parallel_pandas_scan.py\n@@ -0,0 +1,76 @@\n+#!/usr/bin/env python\n+# -*- coding: utf-8 -*-\n+import duckdb\n+import pandas as pd\n+import numpy\n+import datetime\n+import sys\n+\n+def run_parallel_queries(main_table, left_join_table, expected_df, iteration_count = 5):\n+    for i in range(0, iteration_count):\n+        output_df = None\n+        sql = \"\"\"\n+        select\n+            main_table.*\n+            ,t1.*\n+            ,t2.*\n+        from main_table\n+        left join left_join_table t1\n+            on main_table.join_column = t1.join_column\n+        left join left_join_table t2\n+            on main_table.join_column = t2.join_column\n+        \"\"\"\n+        try:\n+            duckdb_conn = duckdb.connect()\n+            duckdb_conn.execute(\"PRAGMA threads=4\")\n+            duckdb_conn.register('main_table', main_table)\n+            duckdb_conn.register('left_join_table', left_join_table)\n+            output_df = duckdb_conn.execute(sql).fetchdf()\n+            pd.testing.assert_frame_equal(expected_df, output_df)\n+            print(output_df)\n+        except Exception as err:\n+            print(err)\n+        finally:\n+            duckdb_conn.close()\n+\n+class TestParallelPandasScan(object):\n+    def test_parallel_numeric_scan(self, duckdb_cursor):\n+        main_table = pd.DataFrame([{\"join_column\": 3}])\n+        left_join_table = pd.DataFrame([{\"join_column\": 3,\"other_column\": 4}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table)\n+\n+    def test_parallel_ascii_text(self, duckdb_cursor):\n+        main_table = pd.DataFrame([{\"join_column\":\"text\"}])\n+        left_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table)\n+\n+    def test_parallel_unicode_text(self, duckdb_cursor):\n+        main_table = pd.DataFrame([{\"join_column\":u\"m\u00fchleisen\"}])\n+        left_join_table = pd.DataFrame([{\"join_column\": u\"m\u00fchleisen\",\"other_column\":u\"h\u00f6h\u00f6h\u00f6\"}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table)\n+\n+    def test_parallel_complex_unicode_text(self, duckdb_cursor):\n+        if sys.version_info.major < 3:\n+            return\n+        main_table = pd.DataFrame([{\"join_column\":u\"\u9d28\"}])\n+        left_join_table = pd.DataFrame([{\"join_column\": u\"\u9d28\",\"other_column\":u\"\u6578\u64da\u5eab\"}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table)\n+\n+    def test_parallel_emojis(self, duckdb_cursor):\n+        if sys.version_info.major < 3:\n+            return\n+        main_table = pd.DataFrame([{\"join_column\":u\"\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f L\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0fR \ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f\"}])\n+        left_join_table = pd.DataFrame([{\"join_column\": u\"\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f L\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0fR \ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f\",\"other_column\":u\"\ud83e\udd86\ud83c\udf5e\ud83e\udd86\"}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table)\n+\n+    def test_parallel_numeric_object(self, duckdb_cursor):\n+        main_table = pd.DataFrame({ 'join_column': pd.Series([3], dtype=\"Int8\") })\n+        left_join_table = pd.DataFrame({ 'join_column': pd.Series([3], dtype=\"Int8\"), 'other_column': pd.Series([4], dtype=\"Int8\") })\n+        expected_df = pd.DataFrame({ \"join_column\": numpy.array([3], dtype=numpy.int8), \"other_column\": numpy.array([4], dtype=numpy.int8)})\n+        run_parallel_queries(main_table, left_join_table, expected_df)\n+\n+    def test_parallel_timestamp(self, duckdb_cursor):\n+        main_table = pd.DataFrame({ 'join_column': [pd.Timestamp('20180310T11:17:54Z')] })\n+        left_join_table = pd.DataFrame({ 'join_column': [pd.Timestamp('20180310T11:17:54Z')], 'other_column': [pd.Timestamp('20190310T11:17:54Z')] })\n+        expected_df = pd.DataFrame({ \"join_column\": numpy.array([datetime.datetime(2018, 3, 10, 11, 17, 54)], dtype='datetime64[ns]'), \"other_column\": numpy.array([datetime.datetime(2019, 3, 10, 11, 17, 54)], dtype='datetime64[ns]')})\n+        run_parallel_queries(main_table, left_join_table, expected_df)\ndiff --git a/tools/pythonpkg/tests/test_unicode.py b/tools/pythonpkg/tests/test_unicode.py\nindex f88e4a092bec..e807b575611a 100644\n--- a/tools/pythonpkg/tests/test_unicode.py\n+++ b/tools/pythonpkg/tests/test_unicode.py\n@@ -3,10 +3,12 @@\n \n import duckdb\n import pandas as pd\n-\n+import sys\n \n class TestUnicode(object):\n     def test_unicode_pandas_scan(self, duckdb_cursor):\n+        if sys.version_info.major < 3:\n+            return\n         con = duckdb.connect(database=':memory:', read_only=False)\n         test_df = pd.DataFrame.from_dict({\"i\":[1, 2, 3], \"j\":[\"a\", \"c\", u\"\u00eb\"]})\n         con.register('test_df_view', test_df)\n",
  "problem_statement": "Threading issue: Multiple left joins to same table and where clause kills Python process\nHey Folks,\r\n\r\nI'm seeing some failures when attempting to use multithreading. The failure mode is that the Python process completely dies. \r\n\r\nI've isolated at least one basic query that is not working for some reason (It's not 100% of the time that it fails, but close to it. Maybe some kind of race condition?). It runs correctly if threads are not enabled (when pragma threads line of code is comment out).  It also runs if the where clause is not present. It also runs if the left join table is only used once.\r\n\r\nI'm using the Python client (Python 3.7.7 on Windows, DuckDB 0.2.3). \r\n\r\nThanks for your help with this! We were upgrading from 0.1.9 to 0.2.2 to get multithreading and some of the other syntax goodness you've added lately and had to revert the multithreading piece.\r\n\r\nThanks,\r\nAlex\r\n\r\n```python\r\n#This is broken:\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nmain_table = pd.DataFrame([{\"join_column\":\"text\"}])\r\nleft_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\r\n\r\nsql = \"\"\"\r\nselect\r\n    main_table.*\r\n    ,t1.*\r\n    ,t2.*\r\nfrom main_table \r\nleft join left_join_table t1\r\n    on main_table.join_column = t1.join_column\r\nleft join left_join_table t2\r\n    on main_table.join_column = t2.join_column\r\nwhere\r\n    t1.other_column = 'more text'\r\n\"\"\"\r\n\r\ntry:\r\n    duckdb_conn = duckdb.connect()\r\n    duckdb_conn.execute(\"PRAGMA threads=4\")\r\n    duckdb_conn.register('main_table',main_table)\r\n    duckdb_conn.register('left_join_table',left_join_table)\r\n    output_df = duckdb_conn.execute(sql).fetchdf()\r\nexcept Exception as err:\r\n    print(err)\r\nfinally:\r\n    duckdb_conn.close()\r\n    \r\noutput_df\r\n````\r\n\r\n```python\r\n#This works:\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nmain_table = pd.DataFrame([{\"join_column\":\"text\"}])\r\nleft_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\r\n\r\nsql = \"\"\"\r\nselect\r\n    main_table.*\r\n    ,t1.*\r\n    ,t2.*\r\nfrom main_table \r\nleft join left_join_table t1\r\n    on main_table.join_column = t1.join_column\r\nleft join left_join_table t2\r\n    on main_table.join_column = t2.join_column\r\n\"\"\"\r\n\r\ntry:\r\n    duckdb_conn = duckdb.connect()\r\n    duckdb_conn.execute(\"PRAGMA threads=4\")\r\n    duckdb_conn.register('main_table',main_table)\r\n    duckdb_conn.register('left_join_table',left_join_table)\r\n    output_df = duckdb_conn.execute(sql).fetchdf()\r\nexcept Exception as err:\r\n    print(err)\r\nfinally:\r\n    duckdb_conn.close()\r\n    \r\noutput_df\r\n```\r\n\r\n```python\r\n#This works:\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nmain_table = pd.DataFrame([{\"join_column\":\"text\"}])\r\nleft_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\r\n\r\nsql = \"\"\"\r\nselect\r\n    main_table.*\r\n    ,t1.*\r\n    ,t2.*\r\nfrom main_table \r\nleft join left_join_table t1\r\n    on main_table.join_column = t1.join_column\r\nleft join left_join_table t2\r\n    on main_table.join_column = t2.join_column\r\nwhere\r\n    t1.other_column = 'more text'\r\n\"\"\"\r\n\r\ntry:\r\n    duckdb_conn = duckdb.connect()\r\n\r\n    duckdb_conn.register('main_table',main_table)\r\n    duckdb_conn.register('left_join_table',left_join_table)\r\n    output_df = duckdb_conn.execute(sql).fetchdf()\r\nexcept Exception as err:\r\n    print(err)\r\nfinally:\r\n    duckdb_conn.close()\r\n    \r\noutput_df\r\n```\nThreading issue: Multiple left joins to same table and where clause kills Python process\nHey Folks,\r\n\r\nI'm seeing some failures when attempting to use multithreading. The failure mode is that the Python process completely dies. \r\n\r\nI've isolated at least one basic query that is not working for some reason (It's not 100% of the time that it fails, but close to it. Maybe some kind of race condition?). It runs correctly if threads are not enabled (when pragma threads line of code is comment out).  It also runs if the where clause is not present. It also runs if the left join table is only used once.\r\n\r\nI'm using the Python client (Python 3.7.7 on Windows, DuckDB 0.2.3). \r\n\r\nThanks for your help with this! We were upgrading from 0.1.9 to 0.2.2 to get multithreading and some of the other syntax goodness you've added lately and had to revert the multithreading piece.\r\n\r\nThanks,\r\nAlex\r\n\r\n```python\r\n#This is broken:\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nmain_table = pd.DataFrame([{\"join_column\":\"text\"}])\r\nleft_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\r\n\r\nsql = \"\"\"\r\nselect\r\n    main_table.*\r\n    ,t1.*\r\n    ,t2.*\r\nfrom main_table \r\nleft join left_join_table t1\r\n    on main_table.join_column = t1.join_column\r\nleft join left_join_table t2\r\n    on main_table.join_column = t2.join_column\r\nwhere\r\n    t1.other_column = 'more text'\r\n\"\"\"\r\n\r\ntry:\r\n    duckdb_conn = duckdb.connect()\r\n    duckdb_conn.execute(\"PRAGMA threads=4\")\r\n    duckdb_conn.register('main_table',main_table)\r\n    duckdb_conn.register('left_join_table',left_join_table)\r\n    output_df = duckdb_conn.execute(sql).fetchdf()\r\nexcept Exception as err:\r\n    print(err)\r\nfinally:\r\n    duckdb_conn.close()\r\n    \r\noutput_df\r\n````\r\n\r\n```python\r\n#This works:\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nmain_table = pd.DataFrame([{\"join_column\":\"text\"}])\r\nleft_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\r\n\r\nsql = \"\"\"\r\nselect\r\n    main_table.*\r\n    ,t1.*\r\n    ,t2.*\r\nfrom main_table \r\nleft join left_join_table t1\r\n    on main_table.join_column = t1.join_column\r\nleft join left_join_table t2\r\n    on main_table.join_column = t2.join_column\r\n\"\"\"\r\n\r\ntry:\r\n    duckdb_conn = duckdb.connect()\r\n    duckdb_conn.execute(\"PRAGMA threads=4\")\r\n    duckdb_conn.register('main_table',main_table)\r\n    duckdb_conn.register('left_join_table',left_join_table)\r\n    output_df = duckdb_conn.execute(sql).fetchdf()\r\nexcept Exception as err:\r\n    print(err)\r\nfinally:\r\n    duckdb_conn.close()\r\n    \r\noutput_df\r\n```\r\n\r\n```python\r\n#This works:\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nmain_table = pd.DataFrame([{\"join_column\":\"text\"}])\r\nleft_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\r\n\r\nsql = \"\"\"\r\nselect\r\n    main_table.*\r\n    ,t1.*\r\n    ,t2.*\r\nfrom main_table \r\nleft join left_join_table t1\r\n    on main_table.join_column = t1.join_column\r\nleft join left_join_table t2\r\n    on main_table.join_column = t2.join_column\r\nwhere\r\n    t1.other_column = 'more text'\r\n\"\"\"\r\n\r\ntry:\r\n    duckdb_conn = duckdb.connect()\r\n\r\n    duckdb_conn.register('main_table',main_table)\r\n    duckdb_conn.register('left_join_table',left_join_table)\r\n    output_df = duckdb_conn.execute(sql).fetchdf()\r\nexcept Exception as err:\r\n    print(err)\r\nfinally:\r\n    duckdb_conn.close()\r\n    \r\noutput_df\r\n```\n",
  "hints_text": "And I did see the issue on both 0.2.2 and 0.2.3 - sorry that it wasn't very clear above! We were deploying 0.2.2 to our servers and saw the issue, and I reproduced it on my local machine on 0.2.3. \nThanks for the report! I can confirm that this indeed breaks. The crash doesn't seem to be related to the core engine itself, but rather to the Pandas scan function. If I alter the program such that it creates a table out of the Pandas DataFrames, then I don't encounter any crashes:\r\n\r\n```python\r\n#This is broken:\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nmain_table = pd.DataFrame([{\"join_column\":\"text\"}])\r\nleft_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\r\n\r\nwhile True:\r\n\toutput_df = None\r\n\tsql = \"\"\"\r\n\tselect\r\n\t\tmain_table.*\r\n\t\t,t1.*\r\n\t\t,t2.*\r\n\tfrom main_table\r\n\tleft join left_join_table t1\r\n\t\ton main_table.join_column = t1.join_column\r\n\tleft join left_join_table t2\r\n\t\ton main_table.join_column = t2.join_column\r\n\twhere\r\n\t\tt1.other_column = 'more text'\r\n\t\"\"\"\r\n\ttry:\r\n\t\tduckdb_conn = duckdb.connect()\r\n\t\tduckdb_conn.execute(\"PRAGMA threads=4\")\r\n\t\tduckdb_conn.df(main_table).create('main_table')\r\n\t\tduckdb_conn.df(left_join_table).create('left_join_table')\r\n\t\toutput_df = duckdb_conn.execute(sql).fetchdf()\r\n\texcept Exception as err:\r\n\t\tprint(err)\r\n\tfinally:\r\n\t\tduckdb_conn.close()\r\n\tprint(output_df)\r\n```\r\n\r\nIt seems to me that the query creates two separate pipelines for the two separate joins, each scanning the same Pandas DataFrame. There is likely a race condition happening in the Python package, perhaps because of reference counting. Removing the WHERE clause actually did not fix the problem for me (i.e. I also encountered the crash without the WHERE clause). However, adding more operators makes the scan take longer, which might make it more likely for the race condition to trigger. More investigation to follow.\nAnd I did see the issue on both 0.2.2 and 0.2.3 - sorry that it wasn't very clear above! We were deploying 0.2.2 to our servers and saw the issue, and I reproduced it on my local machine on 0.2.3. \nThanks for the report! I can confirm that this indeed breaks. The crash doesn't seem to be related to the core engine itself, but rather to the Pandas scan function. If I alter the program such that it creates a table out of the Pandas DataFrames, then I don't encounter any crashes:\r\n\r\n```python\r\n#This is broken:\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nmain_table = pd.DataFrame([{\"join_column\":\"text\"}])\r\nleft_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\r\n\r\nwhile True:\r\n\toutput_df = None\r\n\tsql = \"\"\"\r\n\tselect\r\n\t\tmain_table.*\r\n\t\t,t1.*\r\n\t\t,t2.*\r\n\tfrom main_table\r\n\tleft join left_join_table t1\r\n\t\ton main_table.join_column = t1.join_column\r\n\tleft join left_join_table t2\r\n\t\ton main_table.join_column = t2.join_column\r\n\twhere\r\n\t\tt1.other_column = 'more text'\r\n\t\"\"\"\r\n\ttry:\r\n\t\tduckdb_conn = duckdb.connect()\r\n\t\tduckdb_conn.execute(\"PRAGMA threads=4\")\r\n\t\tduckdb_conn.df(main_table).create('main_table')\r\n\t\tduckdb_conn.df(left_join_table).create('left_join_table')\r\n\t\toutput_df = duckdb_conn.execute(sql).fetchdf()\r\n\texcept Exception as err:\r\n\t\tprint(err)\r\n\tfinally:\r\n\t\tduckdb_conn.close()\r\n\tprint(output_df)\r\n```\r\n\r\nIt seems to me that the query creates two separate pipelines for the two separate joins, each scanning the same Pandas DataFrame. There is likely a race condition happening in the Python package, perhaps because of reference counting. Removing the WHERE clause actually did not fix the problem for me (i.e. I also encountered the crash without the WHERE clause). However, adding more operators makes the scan take longer, which might make it more likely for the race condition to trigger. More investigation to follow.",
  "created_at": "2020-12-14T12:33:27Z"
}