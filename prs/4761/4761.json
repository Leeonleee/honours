{
  "repo": "duckdb/duckdb",
  "pull_number": 4761,
  "instance_id": "duckdb__duckdb-4761",
  "issue_numbers": [
    "3595",
    "3595"
  ],
  "base_commit": "1d39c0c2eed223eeca232b9a976bb6488569fea7",
  "patch": "diff --git a/benchmark/tpch/join/floating_point_join.benchmark b/benchmark/tpch/join/floating_point_join.benchmark\nnew file mode 100644\nindex 000000000000..82f3e17e31c7\n--- /dev/null\n+++ b/benchmark/tpch/join/floating_point_join.benchmark\n@@ -0,0 +1,20 @@\n+# name: benchmark/tpch/join/floating_point_join.benchmark\n+# description: COUNT aggregate over join on double keys\n+# group: [join]\n+\n+name Join Double Keys\n+group join\n+subgroup tpch\n+\n+require tpch\n+\n+load\n+CALL dbgen(sf=1, suffix='_normal');\n+CREATE TABLE lineitem AS SELECT * REPLACE (l_orderkey::DOUBLE AS l_orderkey) FROM lineitem_normal;\n+CREATE TABLE orders AS SELECT * REPLACE (o_orderkey::DOUBLE AS o_orderkey) FROM orders_normal;\n+\n+run\n+SELECT COUNT(*) from lineitem join orders on (l_orderkey=o_orderkey);\n+\n+result I\n+6001215\ndiff --git a/benchmark/tpch/join/integer_join.benchmark b/benchmark/tpch/join/integer_join.benchmark\nnew file mode 100644\nindex 000000000000..a7cd13934dbc\n--- /dev/null\n+++ b/benchmark/tpch/join/integer_join.benchmark\n@@ -0,0 +1,20 @@\n+# name: benchmark/tpch/join/integer_join.benchmark\n+# description: COUNT aggregate over join on integer keys\n+# group: [join]\n+\n+name Join Integer Keys\n+group join\n+subgroup tpch\n+\n+require tpch\n+\n+cache tpch_sf1\n+\n+load\n+CALL dbgen(sf=1);\n+\n+run\n+SELECT COUNT(*) from lineitem join orders on (l_orderkey=o_orderkey);\n+\n+result I\n+6001215\ndiff --git a/src/common/types/hash.cpp b/src/common/types/hash.cpp\nindex c648e1812c28..27b846893308 100644\n--- a/src/common/types/hash.cpp\n+++ b/src/common/types/hash.cpp\n@@ -24,12 +24,16 @@ hash_t Hash(hugeint_t val) {\n \n template <>\n hash_t Hash(float val) {\n-\treturn std::hash<float> {}(val);\n+\tstatic_assert(sizeof(float) == sizeof(uint32_t), \"\");\n+\tuint32_t uval = *((uint32_t *)&val);\n+\treturn murmurhash64(uval);\n }\n \n template <>\n hash_t Hash(double val) {\n-\treturn std::hash<double> {}(val);\n+\tstatic_assert(sizeof(double) == sizeof(uint64_t), \"\");\n+\tuint64_t uval = *((uint64_t *)&val);\n+\treturn murmurhash64(uval);\n }\n \n template <>\n",
  "test_patch": "diff --git a/test/sql/tpch/tpch_floating_point_join.test_slow b/test/sql/tpch/tpch_floating_point_join.test_slow\nnew file mode 100644\nindex 000000000000..29d2ecaa4015\n--- /dev/null\n+++ b/test/sql/tpch/tpch_floating_point_join.test_slow\n@@ -0,0 +1,35 @@\n+# name: test/sql/tpch/tpch_floating_point_join.test_slow\n+# description: Join using floating point keys\n+# group: [tpch]\n+\n+require tpch\n+\n+statement ok\n+CALL dbgen(sf=0.1, suffix='_normal');\n+\n+statement ok\n+CREATE TABLE lineitem_flt AS SELECT * REPLACE (l_orderkey::DOUBLE AS l_orderkey) FROM lineitem_normal;\n+\n+statement ok\n+CREATE TABLE orders_flt AS SELECT * REPLACE (o_orderkey::DOUBLE AS o_orderkey) FROM orders_normal;\n+\n+statement ok\n+CREATE TABLE lineitem_dbl AS SELECT * REPLACE (l_orderkey::DOUBLE AS l_orderkey) FROM lineitem_normal;\n+\n+statement ok\n+CREATE TABLE orders_dbl AS SELECT * REPLACE (o_orderkey::DOUBLE AS o_orderkey) FROM orders_normal;\n+\n+query I\n+SELECT COUNT(*) from lineitem_normal join orders_normal on (l_orderkey=o_orderkey);\n+----\n+600572\n+\n+query I\n+SELECT COUNT(*) from lineitem_dbl join orders_dbl on (l_orderkey=o_orderkey);\n+----\n+600572\n+\n+query I\n+SELECT COUNT(*) from lineitem_flt join orders_flt on (l_orderkey=o_orderkey);\n+----\n+600572\n",
  "problem_statement": "Joining on arrow tables on `float64` types is ~5x slower on M1 `arm64`\n#### What happens?\r\nJoining on arrow tables on `float64` types is ~5x slower on M1 `arm64` vs `x86`. The code below produced nearly identical times when run on an `x86` machine. \r\n\r\n#### To Reproduce\r\nSteps to reproduce the behavior. Bonus points if those are only SQL queries.\r\n### Joining on `float64`\r\n```python\r\nimport pyarrow as pa\r\nimport duckdb\r\nimport numpy as np\r\n\r\nconn = duckdb.connect()\r\narr1 = pa.array(np.arange(1000), type=pa.float64())\r\narr2 = pa.array(np.arange(1000), type=pa.float64())\r\nt1_f = pa.table([arr1], names=[\"id\"])\r\nt2_f = pa.table([arr2], names=[\"id\"])\r\n```\r\n```py\r\n %timeit -n 10 -r 3 conn.execute(\"select * from t1_f, t2_f where t1_f.id = t2_f.id\").fetchall()\r\n4.99 ms \u00b1 1.51 ms per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\r\n```\r\n\r\n### Joining on `int64`\r\n```py\r\narr1 = pa.array(np.arange(1000), type=pa.int64())\r\narr2 = pa.array(np.arange(1000), type=pa.int64())\r\nt1_int = pa.table([arr1], names=[\"id\"])\r\nt2_int = pa.table([arr2], names=[\"id\"])\r\n```\r\n\r\n```py\r\nIn [20]: %timeit -n 10 -r 3 conn.execute(\"select * from t1_int, t2_int where t1_int.id = t2_int.id\").fetchall()\r\n869 \u00b5s \u00b1 131 \u00b5s per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\r\n```\r\n### Platform Specs\r\n```json\r\n{\"machine\": \"arm64\", \"platform\": \"macOS-12.1-arm64-arm-64bit\", \"system\": \"Darwin\", \"cpu_count\": 8}\r\n```\r\n#### Environment (please complete the following information):\r\n - OS: iOS (arm64)\r\n - DuckDB Version: 0.3.4\r\n - DuckDB Client:  Python\r\n\r\n\r\n#### Before Submitting\r\n\r\n- [x ] **Have you tried this on the latest `master` branch?**\r\n* **Python**: `pip install duckdb --upgrade --pre`\r\n* **R**: `install.packages(\"https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz\", repos = NULL)`\r\n* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.\r\n\r\n- [ x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\nJoining on arrow tables on `float64` types is ~5x slower on M1 `arm64`\n#### What happens?\r\nJoining on arrow tables on `float64` types is ~5x slower on M1 `arm64` vs `x86`. The code below produced nearly identical times when run on an `x86` machine. \r\n\r\n#### To Reproduce\r\nSteps to reproduce the behavior. Bonus points if those are only SQL queries.\r\n### Joining on `float64`\r\n```python\r\nimport pyarrow as pa\r\nimport duckdb\r\nimport numpy as np\r\n\r\nconn = duckdb.connect()\r\narr1 = pa.array(np.arange(1000), type=pa.float64())\r\narr2 = pa.array(np.arange(1000), type=pa.float64())\r\nt1_f = pa.table([arr1], names=[\"id\"])\r\nt2_f = pa.table([arr2], names=[\"id\"])\r\n```\r\n```py\r\n %timeit -n 10 -r 3 conn.execute(\"select * from t1_f, t2_f where t1_f.id = t2_f.id\").fetchall()\r\n4.99 ms \u00b1 1.51 ms per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\r\n```\r\n\r\n### Joining on `int64`\r\n```py\r\narr1 = pa.array(np.arange(1000), type=pa.int64())\r\narr2 = pa.array(np.arange(1000), type=pa.int64())\r\nt1_int = pa.table([arr1], names=[\"id\"])\r\nt2_int = pa.table([arr2], names=[\"id\"])\r\n```\r\n\r\n```py\r\nIn [20]: %timeit -n 10 -r 3 conn.execute(\"select * from t1_int, t2_int where t1_int.id = t2_int.id\").fetchall()\r\n869 \u00b5s \u00b1 131 \u00b5s per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\r\n```\r\n### Platform Specs\r\n```json\r\n{\"machine\": \"arm64\", \"platform\": \"macOS-12.1-arm64-arm-64bit\", \"system\": \"Darwin\", \"cpu_count\": 8}\r\n```\r\n#### Environment (please complete the following information):\r\n - OS: iOS (arm64)\r\n - DuckDB Version: 0.3.4\r\n - DuckDB Client:  Python\r\n\r\n\r\n#### Before Submitting\r\n\r\n- [x ] **Have you tried this on the latest `master` branch?**\r\n* **Python**: `pip install duckdb --upgrade --pre`\r\n* **R**: `install.packages(\"https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz\", repos = NULL)`\r\n* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.\r\n\r\n- [ x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\n",
  "hints_text": "cc: @cpcloud \nCan you also share the specs of the x86 machine?\nHere you go:\r\n```\r\n{\"machine\": \"x86_64\",  \"platform\": \"Linux-5.17.5-76051705-generic-x86_64-with-glibc2.35\", \"system\": \"Linux\", \"cpu_count\": 48}\r\n```\r\n\r\nand here are the %timit:\r\n```python\r\nIn [22]: %timeit -n 10 -r 3 conn.execute(\"select * from t1_f, t2_f where t1_f.id = t2_f.id\").fetchdf()\r\n1.64 ms \u00b1 96.3 \u00b5s per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\r\nIn [23]: %timeit -n 10 -r 3 conn.execute(\"select * from t1_int, t2_int where t1_int.id = t2_int.id\").fetchdf()\r\n1.62 ms \u00b1 217 \u00b5s per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\r\n```\nI wonder if this is related to a bug I filed awhile ago.. https://github.com/duckdb/duckdb/issues/2909\nUpon further testing, I think they are related. In my query on the M1 only, I join on a double and it takes forever. \r\n\r\nHowever, if I write a parquet file storing it as integer (or if I just cast the double as an INTEGER in realtime) it is much faster.\r\n\r\nThe first query finishes in 5 seconds, the last one hasn't finished after several minutes.\r\n\r\n```\r\n\r\n system.time(m<-dbGetQuery(con,\"select * from parquet_scan('crsp.msf.parquet') as m inner join parquet_scan('crsp.dsenames') as d on cast(m.permno as INTEGER)=d.permno where date between namedt and nameendt\"))\r\n   user  system elapsed \r\n  4.408   0.712   5.457 \r\n> system.time(m<-dbGetQuery(con,\"select * from parquet_scan('crsp.msf.parquet') as m inner join parquet_scan('crsp.dsenames') as d on cast(m.permno as DOUBLE)=d.permno where date between namedt and nameendt\"))\r\n```\r\n\r\nEDIT: Interestingly, if I casted the float as an INTEGER then as a VARCHAR the join is approximately as fast as if I did it via INTEGER and far faster than float. It is curious because (my CS is really rusty) I thought FLOATs were sometimes in some contexts just evaluated as strings.\nSince we use the same code on both architectures, I think this may be compiler/hardware related.\nI'm really glad that joining on integer /  casting to string  are sufficiently fast.  It is bad practice in my context to join on integers anyway. It is just a bit annoying that R tends to turn integers into numeric but that is easy to work around.\r\n\r\nAbsent discovering this, it makes DuckDB unusable on my M1 Mac. \r\n\nUsing a M1 Max \r\nI had a terrible experience joining on float (which took 10min) vs casting to integer and then joining (which took 240ms)\r\n2500x difference\ncc: @cpcloud \nCan you also share the specs of the x86 machine?\nHere you go:\r\n```\r\n{\"machine\": \"x86_64\",  \"platform\": \"Linux-5.17.5-76051705-generic-x86_64-with-glibc2.35\", \"system\": \"Linux\", \"cpu_count\": 48}\r\n```\r\n\r\nand here are the %timit:\r\n```python\r\nIn [22]: %timeit -n 10 -r 3 conn.execute(\"select * from t1_f, t2_f where t1_f.id = t2_f.id\").fetchdf()\r\n1.64 ms \u00b1 96.3 \u00b5s per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\r\nIn [23]: %timeit -n 10 -r 3 conn.execute(\"select * from t1_int, t2_int where t1_int.id = t2_int.id\").fetchdf()\r\n1.62 ms \u00b1 217 \u00b5s per loop (mean \u00b1 std. dev. of 3 runs, 10 loops each)\r\n```\nI wonder if this is related to a bug I filed awhile ago.. https://github.com/duckdb/duckdb/issues/2909\nUpon further testing, I think they are related. In my query on the M1 only, I join on a double and it takes forever. \r\n\r\nHowever, if I write a parquet file storing it as integer (or if I just cast the double as an INTEGER in realtime) it is much faster.\r\n\r\nThe first query finishes in 5 seconds, the last one hasn't finished after several minutes.\r\n\r\n```\r\n\r\n system.time(m<-dbGetQuery(con,\"select * from parquet_scan('crsp.msf.parquet') as m inner join parquet_scan('crsp.dsenames') as d on cast(m.permno as INTEGER)=d.permno where date between namedt and nameendt\"))\r\n   user  system elapsed \r\n  4.408   0.712   5.457 \r\n> system.time(m<-dbGetQuery(con,\"select * from parquet_scan('crsp.msf.parquet') as m inner join parquet_scan('crsp.dsenames') as d on cast(m.permno as DOUBLE)=d.permno where date between namedt and nameendt\"))\r\n```\r\n\r\nEDIT: Interestingly, if I casted the float as an INTEGER then as a VARCHAR the join is approximately as fast as if I did it via INTEGER and far faster than float. It is curious because (my CS is really rusty) I thought FLOATs were sometimes in some contexts just evaluated as strings.\nSince we use the same code on both architectures, I think this may be compiler/hardware related.\nI'm really glad that joining on integer /  casting to string  are sufficiently fast.  It is bad practice in my context to join on integers anyway. It is just a bit annoying that R tends to turn integers into numeric but that is easy to work around.\r\n\r\nAbsent discovering this, it makes DuckDB unusable on my M1 Mac. \r\n\nUsing a M1 Max \r\nI had a terrible experience joining on float (which took 10min) vs casting to integer and then joining (which took 240ms)\r\n2500x difference",
  "created_at": "2022-09-20T09:02:38Z"
}