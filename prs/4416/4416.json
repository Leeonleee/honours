{
  "repo": "duckdb/duckdb",
  "pull_number": 4416,
  "instance_id": "duckdb__duckdb-4416",
  "issue_numbers": [
    "4401",
    "4406"
  ],
  "base_commit": "df955903e7e5a3e7407b1fd0a854d7ad61f4f665",
  "patch": "diff --git a/src/storage/compression/numeric_constant.cpp b/src/storage/compression/numeric_constant.cpp\nindex aeac087e5477..2d246291b584 100644\n--- a/src/storage/compression/numeric_constant.cpp\n+++ b/src/storage/compression/numeric_constant.cpp\n@@ -16,26 +16,6 @@ unique_ptr<SegmentScanState> ConstantInitScan(ColumnSegment &segment) {\n \treturn nullptr;\n }\n \n-//===--------------------------------------------------------------------===//\n-// Scan base data\n-//===--------------------------------------------------------------------===//\n-void ConstantScanFunctionValidity(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result) {\n-\tauto &validity = (ValidityStatistics &)*segment.stats.statistics;\n-\tif (validity.has_null) {\n-\t\tresult.SetVectorType(VectorType::CONSTANT_VECTOR);\n-\t\tConstantVector::SetNull(result, true);\n-\t}\n-}\n-\n-template <class T>\n-void ConstantScanFunction(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result) {\n-\tauto &nstats = (NumericStatistics &)*segment.stats.statistics;\n-\n-\tauto data = FlatVector::GetData<T>(result);\n-\tdata[0] = nstats.min.GetValueUnsafe<T>();\n-\tresult.SetVectorType(VectorType::CONSTANT_VECTOR);\n-}\n-\n //===--------------------------------------------------------------------===//\n // Scan Partial\n //===--------------------------------------------------------------------===//\n@@ -71,6 +51,31 @@ void ConstantScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t s\n \tConstantFillFunction<T>(segment, result, result_offset, scan_count);\n }\n \n+//===--------------------------------------------------------------------===//\n+// Scan base data\n+//===--------------------------------------------------------------------===//\n+void ConstantScanFunctionValidity(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result) {\n+\tauto &validity = (ValidityStatistics &)*segment.stats.statistics;\n+\tif (validity.has_null) {\n+\t\tif (result.GetVectorType() == VectorType::CONSTANT_VECTOR) {\n+\t\t\tresult.SetVectorType(VectorType::CONSTANT_VECTOR);\n+\t\t\tConstantVector::SetNull(result, true);\n+\t\t} else {\n+\t\t\tresult.Flatten(scan_count);\n+\t\t\tConstantFillFunctionValidity(segment, result, 0, scan_count);\n+\t\t}\n+\t}\n+}\n+\n+template <class T>\n+void ConstantScanFunction(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result) {\n+\tauto &nstats = (NumericStatistics &)*segment.stats.statistics;\n+\n+\tauto data = FlatVector::GetData<T>(result);\n+\tdata[0] = nstats.min.GetValueUnsafe<T>();\n+\tresult.SetVectorType(VectorType::CONSTANT_VECTOR);\n+}\n+\n //===--------------------------------------------------------------------===//\n // Fetch\n //===--------------------------------------------------------------------===//\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex 684a27a037fb..22af9e6c8433 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -1038,8 +1038,7 @@ void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {\n \tresult.Initialize(Allocator::Get(db), types);\n \n \trow_group->InitializeScanWithOffset(state.row_group_scan_state, row_group_vector_idx);\n-\trow_group->ScanCommitted(state.row_group_scan_state, result,\n-\t                         TableScanType::TABLE_SCAN_COMMITTED_ROWS_DISALLOW_UPDATES);\n+\trow_group->ScanCommitted(state.row_group_scan_state, result, TableScanType::TABLE_SCAN_COMMITTED_ROWS);\n \tresult.Slice(sel, count);\n \n \tinfo->indexes.Scan([&](Index &index) {\ndiff --git a/src/storage/table/update_segment.cpp b/src/storage/table/update_segment.cpp\nindex 31570af66e00..94cbe6370cbc 100644\n--- a/src/storage/table/update_segment.cpp\n+++ b/src/storage/table/update_segment.cpp\n@@ -630,10 +630,14 @@ static void InitializeUpdateData(UpdateInfo *base_info, Vector &base_data, Updat\n \t}\n \n \tauto base_array_data = FlatVector::GetData<T>(base_data);\n+\tauto &base_validity = FlatVector::Validity(base_data);\n \tauto base_tuple_data = (T *)base_info->tuple_data;\n \tfor (idx_t i = 0; i < base_info->N; i++) {\n-\t\tbase_tuple_data[i] =\n-\t\t    UpdateSelectElement::Operation<T>(base_info->segment, base_array_data[base_info->tuples[i]]);\n+\t\tauto base_idx = base_info->tuples[i];\n+\t\tif (!base_validity.RowIsValid(base_idx)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tbase_tuple_data[i] = UpdateSelectElement::Operation<T>(base_info->segment, base_array_data[base_idx]);\n \t}\n }\n \n",
  "test_patch": "diff --git a/test/sql/update/large_string_null_update.test b/test/sql/update/large_string_null_update.test\nnew file mode 100644\nindex 000000000000..64e4d74f09a0\n--- /dev/null\n+++ b/test/sql/update/large_string_null_update.test\n@@ -0,0 +1,33 @@\n+# name: test/sql/update/large_string_null_update.test\n+# description: Test back and forth update of string NULL values with many strings\n+# group: [update]\n+\n+load __TEST_DIR__/string_null_update.db\n+\n+statement ok\n+create table t (id int, col varchar);\n+\n+statement ok\n+insert into t (id) select range as id from range(0, 1000000);\n+\n+\n+query III\n+SELECT COUNT(*), COUNT(id), COUNT(col) FROM t\n+----\n+1000000\t1000000\t0\n+\n+statement ok\n+update t set col = 'x';\n+\n+query III\n+SELECT COUNT(*), COUNT(id), COUNT(col) FROM t\n+----\n+1000000\t1000000\t1000000\n+\n+statement ok\n+update t set col = NULL;\n+\n+query III\n+SELECT COUNT(*), COUNT(id), COUNT(col) FROM t\n+----\n+1000000\t1000000\t0\ndiff --git a/test/sql/update/update_delete_wal.test b/test/sql/update/update_delete_wal.test\nnew file mode 100644\nindex 000000000000..7fd736bcef5f\n--- /dev/null\n+++ b/test/sql/update/update_delete_wal.test\n@@ -0,0 +1,41 @@\n+# name: test/sql/update/update_delete_wal.test\n+# description: Integer null update checkpoint\n+# group: [update]\n+\n+load __TEST_DIR__/update_delete_wal.db\n+\n+statement ok\n+SET wal_autocheckpoint='1TB';\n+\n+statement ok\n+create table test (id bigint primary key, c1 text);\n+\n+statement ok\n+insert into test (id, c1) values (1, 'foo');\n+\n+statement ok\n+insert into test (id, c1) values (2, 'bar');\n+\n+statement ok\n+begin transaction;\n+\n+statement ok\n+delete from test where id = 1;\n+\n+statement ok\n+update test set c1='baz' where id=2;\n+\n+statement ok\n+commit;\n+\n+query II\n+SELECT * FROM test ORDER BY id\n+----\n+2\tbaz\n+\n+restart\n+\n+query II\n+SELECT * FROM test ORDER BY id\n+----\n+2\tbaz\ndiff --git a/test/sql/update/update_null_integers.test b/test/sql/update/update_null_integers.test\nnew file mode 100644\nindex 000000000000..d90147489842\n--- /dev/null\n+++ b/test/sql/update/update_null_integers.test\n@@ -0,0 +1,37 @@\n+# name: test/sql/update/update_null_integers.test\n+# description: Integer null update checkpoint\n+# group: [update]\n+\n+load __TEST_DIR__/update_null_integers.db\n+\n+statement ok\n+SET wal_autocheckpoint='1TB';\n+\n+statement ok\n+CREATE TABLE t(i int, j int);\n+\n+# Insert >= 1024 NULLs\n+statement ok\n+INSERT INTO t SELECT ii, NULL FROM range(1024) tbl(ii);\n+\n+query III\n+select COUNT(j), MIN(j), MAX(j) from t;\n+----\n+0\tNULL\tNULL\n+\n+# checkpoint to trigger the issue\n+statement ok\n+CHECKPOINT;\n+\n+query III\n+select COUNT(j), MIN(j), MAX(j) from t;\n+----\n+0\tNULL\tNULL\n+\n+statement ok\n+UPDATE t SET j = 1;\n+\n+query III\n+select COUNT(j), MIN(j), MAX(j) from t;\n+----\n+1024\t1\t1\n",
  "problem_statement": "Segmentation fault on UPDATE statement\n### What happens?\r\n\r\nOn a large table (1M+ rows), an UPDATE statement causes the duckdb CLI to segfault.\r\n\r\n### To Reproduce\r\n\r\n#### 1. Table Schema\r\n\r\n```\r\nCREATE TABLE t (\r\n  id INTEGER,\r\n  date DATE,\r\n  amount DOUBLE,\r\n  category VARCHAR,\r\n  subcategory VARCHAR,\r\n  sku VARCHAR,\r\n  order_id VARCHAR,\r\n  \"order\" VARCHAR,\r\n  invoice VARCHAR\r\n);\r\n```\r\n\r\n#### 2. Insert Data\r\n\r\nIn my case, I can consistently reproduce the error with 1M rows. I also see this error with fewer rows (in the 100k range) but the bug is not as reliable.\r\n\r\nThus, by using the attached CSV file, you may import data like so:\r\n\r\n[segfault.csv.gz](https://github.com/duckdb/duckdb/files/9338823/segfault.csv.gz)\r\n\r\n```\r\n COPY t from 'segfault.csv.gz' (AUTO_DETECT TRUE);\r\n```\r\n\r\n#### 3. UPDATE query\r\n\r\n```\r\nUPDATE t SET sku = 'x';\r\n```\r\n\r\nThis will result in a `Segmentation fault`.\r\n\r\n### OS:\r\n\r\nLinux\r\n\r\n### DuckDB Version:\r\n\r\nv0.4.1-dev1556 540eab3cc\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nJay Goel\r\n\r\n### Affiliation:\r\n\r\nJournalize.io\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\nDuckDB crash: uncaught `TransactionException: Cannot create index with outstanding updates`\n### What happens?\r\n\r\nReopening the following issue because I found a consistent repro on the latest DuckDB version: https://github.com/duckdb/duckdb/issues/3666\r\n\r\nI found a series of SQL statements which consistently causes DuckDB files to go into a corrupted state. Committing the transaction crashes the process. Any attempt to open a DB file after running these commands also crashes the process.\r\n\r\n### To Reproduce\r\n\r\n```\r\nD create table test (id bigint primary key, c1 text);\r\nD insert into test (id, c1) values (1, 'foo');\r\nD insert into test (id, c1) values (2, 'bar');\r\nD begin transaction;\r\nD delete from test where id = 1;\r\nD update test set c1='baz' where id=2;\r\nD commit;\r\nlibc++abi: terminating with uncaught exception of type duckdb::TransactionException: TransactionContext Error: Cannot create index with outstanding updates\r\nAbort trap: 6\r\nlogout\r\n```\r\n\r\n### OS:\r\n\r\niOS / docker\r\n\r\n### DuckDB Version:\r\n\r\n0.4.0\r\n\r\n### DuckDB Client:\r\n\r\nRust + CLI\r\n\r\n### Full Name:\r\n\r\nAarash Heydari\r\n\r\n### Affiliation:\r\n\r\nDataland\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "After more experiments, I think I have narrowed the scope of the bug and simplified the test case.\r\n\r\nThis bug seems to occur when a column has NULL values. Here is a simpler example to demonstrate the issue:\r\n\r\n```\r\n[ec2-user@ip-1.1.1.1 ~]$ ./duckdb bug.duckdb\r\nv0.4.1-dev1556 540eab3cc\r\nEnter \".help\" for usage hints.\r\nD create table t (id int, col varchar);\r\nD insert into t (id) select range as id from range(0, 10000000);\r\nD update t set col = 'x';\r\nSegmentation fault\r\n```\r\n\r\nHowever, if you create a table with a default value for the strings, then I do not see this issue. The following create table does _not_ trigger this issue: `create table t (id int, col varchar default '');`\nThe error is clearly coming from this line:\r\nhttps://github.com/duckdb/duckdb/blob/master/src/storage/table/column_data.cpp#L96-L103 \r\n\r\nThere seems to be two questions: \r\n1. What upstream code path would be calling it with `ALLOW_UPDATES = false` at the moment that the connection is opened without catching the exception? \r\n2. What is unique about my data (specifically it seems like the the state of my DB + the staged edits in my WAL file) that is triggering this? \nIt would be very helpful if you could try and make a small reproducible example using SQL queries. \r\n\r\n> What is unique about my data (specifically it seems like the the state of my DB + the staged edits in my WAL file) that is triggering this?\r\n\r\nIt seems to me like you are using DuckDB in a rather non-traditional manner - namely doing a lot of table modifications (updates, adding columns, dropping columns) with indexes present. While that should definitely work it is not a very common use case and hence not as well tested.\nHi Mark, thanks for the response. I'll work on making a reproducible example with just SQL queries. For now, I sent my database files to your email (`mark.raasveldt@gmail.com` and `m.raasveldt@cwi.nl`). I wonder if you have debugger tools that could help us understand what's happening when we try to open the file.",
  "created_at": "2022-08-16T21:19:12Z"
}