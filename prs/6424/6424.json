{
  "repo": "duckdb/duckdb",
  "pull_number": 6424,
  "instance_id": "duckdb__duckdb-6424",
  "issue_numbers": [
    "6420",
    "6420",
    "6420"
  ],
  "base_commit": "666d2a612508bd033261113b0af32c62b6ac2b64",
  "patch": "diff --git a/src/function/table/system/CMakeLists.txt b/src/function/table/system/CMakeLists.txt\nindex 5d9a0ebee145..52bed19ffb83 100644\n--- a/src/function/table/system/CMakeLists.txt\n+++ b/src/function/table/system/CMakeLists.txt\n@@ -13,6 +13,7 @@ add_library_unity(\n   duckdb_sequences.cpp\n   duckdb_settings.cpp\n   duckdb_tables.cpp\n+  duckdb_temporary_files.cpp\n   duckdb_types.cpp\n   duckdb_views.cpp\n   pragma_collations.cpp\ndiff --git a/src/function/table/system/duckdb_temporary_files.cpp b/src/function/table/system/duckdb_temporary_files.cpp\nnew file mode 100644\nindex 000000000000..f6de22ce2e2f\n--- /dev/null\n+++ b/src/function/table/system/duckdb_temporary_files.cpp\n@@ -0,0 +1,59 @@\n+#include \"duckdb/function/table/system_functions.hpp\"\n+#include \"duckdb/storage/buffer_manager.hpp\"\n+\n+namespace duckdb {\n+\n+struct DuckDBTemporaryFilesData : public GlobalTableFunctionState {\n+\tDuckDBTemporaryFilesData() : offset(0) {\n+\t}\n+\n+\tvector<TemporaryFileInformation> entries;\n+\tidx_t offset;\n+};\n+\n+static unique_ptr<FunctionData> DuckDBTemporaryFilesBind(ClientContext &context, TableFunctionBindInput &input,\n+                                                         vector<LogicalType> &return_types, vector<string> &names) {\n+\tnames.emplace_back(\"path\");\n+\treturn_types.emplace_back(LogicalType::VARCHAR);\n+\n+\tnames.emplace_back(\"size\");\n+\treturn_types.emplace_back(LogicalType::BIGINT);\n+\n+\treturn nullptr;\n+}\n+\n+unique_ptr<GlobalTableFunctionState> DuckDBTemporaryFilesInit(ClientContext &context, TableFunctionInitInput &input) {\n+\tauto result = make_unique<DuckDBTemporaryFilesData>();\n+\n+\tresult->entries = BufferManager::GetBufferManager(context).GetTemporaryFiles();\n+\treturn std::move(result);\n+}\n+\n+void DuckDBTemporaryFilesFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n+\tauto &data = (DuckDBTemporaryFilesData &)*data_p.global_state;\n+\tif (data.offset >= data.entries.size()) {\n+\t\t// finished returning values\n+\t\treturn;\n+\t}\n+\t// start returning values\n+\t// either fill up the chunk or return all the remaining columns\n+\tidx_t count = 0;\n+\twhile (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {\n+\t\tauto &entry = data.entries[data.offset++];\n+\t\t// return values:\n+\t\tidx_t col = 0;\n+\t\t// database_name, VARCHAR\n+\t\toutput.SetValue(col++, count, entry.path);\n+\t\t// database_oid, BIGINT\n+\t\toutput.SetValue(col++, count, Value::BIGINT(entry.size));\n+\t\tcount++;\n+\t}\n+\toutput.SetCardinality(count);\n+}\n+\n+void DuckDBTemporaryFilesFun::RegisterFunction(BuiltinFunctions &set) {\n+\tset.AddFunction(TableFunction(\"duckdb_temporary_files\", {}, DuckDBTemporaryFilesFunction, DuckDBTemporaryFilesBind,\n+\t                              DuckDBTemporaryFilesInit));\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/function/table/system_functions.cpp b/src/function/table/system_functions.cpp\nindex 0830067745ef..edc16ce7dfa0 100644\n--- a/src/function/table/system_functions.cpp\n+++ b/src/function/table/system_functions.cpp\n@@ -29,6 +29,7 @@ void BuiltinFunctions::RegisterSQLiteFunctions() {\n \tDuckDBSequencesFun::RegisterFunction(*this);\n \tDuckDBSettingsFun::RegisterFunction(*this);\n \tDuckDBTablesFun::RegisterFunction(*this);\n+\tDuckDBTemporaryFilesFun::RegisterFunction(*this);\n \tDuckDBTypesFun::RegisterFunction(*this);\n \tDuckDBViewsFun::RegisterFunction(*this);\n \tTestAllTypesFun::RegisterFunction(*this);\ndiff --git a/src/include/duckdb/function/table/system_functions.hpp b/src/include/duckdb/function/table/system_functions.hpp\nindex 77bcb2b0c6b7..a810a250b541 100644\n--- a/src/include/duckdb/function/table/system_functions.hpp\n+++ b/src/include/duckdb/function/table/system_functions.hpp\n@@ -89,6 +89,10 @@ struct DuckDBTablesFun {\n \tstatic void RegisterFunction(BuiltinFunctions &set);\n };\n \n+struct DuckDBTemporaryFilesFun {\n+\tstatic void RegisterFunction(BuiltinFunctions &set);\n+};\n+\n struct DuckDBTypesFun {\n \tstatic void RegisterFunction(BuiltinFunctions &set);\n };\ndiff --git a/src/include/duckdb/storage/buffer_manager.hpp b/src/include/duckdb/storage/buffer_manager.hpp\nindex 385ae9727fac..44a9dd3c15ff 100644\n--- a/src/include/duckdb/storage/buffer_manager.hpp\n+++ b/src/include/duckdb/storage/buffer_manager.hpp\n@@ -23,6 +23,11 @@ class DatabaseInstance;\n class TemporaryDirectoryHandle;\n struct EvictionQueue;\n \n+struct TemporaryFileInformation {\n+\tstring path;\n+\tidx_t size;\n+};\n+\n //! The buffer manager is in charge of handling memory management for the database. It hands out memory buffers that can\n //! be used by the database internally.\n //\n@@ -98,6 +103,9 @@ class BufferManager {\n \tDUCKDB_API void ReserveMemory(idx_t size);\n \tDUCKDB_API void FreeReservedMemory(idx_t size);\n \n+\t//! Returns a list of all temporary files\n+\tvector<TemporaryFileInformation> GetTemporaryFiles();\n+\n private:\n \t//! Register an in-memory buffer of arbitrary size, as long as it is >= BLOCK_SIZE. can_destroy signifies whether or\n \t//! not the buffer can be destroyed when unpinned, or whether or not it needs to be written to a temporary file so\ndiff --git a/src/storage/buffer_manager.cpp b/src/storage/buffer_manager.cpp\nindex 59701d0cbe51..c543ee756e3a 100644\n--- a/src/storage/buffer_manager.cpp\n+++ b/src/storage/buffer_manager.cpp\n@@ -62,7 +62,7 @@ BlockHandle::BlockHandle(BlockManager &block_manager, block_id_t block_id_p, uni\n \tmemory_charge = std::move(reservation);\n }\n \n-BlockHandle::~BlockHandle() {\n+BlockHandle::~BlockHandle() { // NOLINT: allow internal exceptions\n \t// being destroyed, so any unswizzled pointers are just binary junk now.\n \tunswizzled = nullptr;\n \tauto &buffer_manager = block_manager.buffer_manager;\n@@ -521,11 +521,8 @@ void BufferManager::PurgeQueue() {\n \n void BlockManager::UnregisterBlock(block_id_t block_id, bool can_destroy) {\n \tif (block_id >= MAXIMUM_BLOCK) {\n-\t\t// in-memory buffer: destroy the buffer\n-\t\tif (!can_destroy) {\n-\t\t\t// buffer could have been offloaded to disk: remove the file\n-\t\t\tbuffer_manager.DeleteTemporaryFile(block_id);\n-\t\t}\n+\t\t// in-memory buffer: buffer could have been offloaded to disk: remove the file\n+\t\tbuffer_manager.DeleteTemporaryFile(block_id);\n \t} else {\n \t\tlock_guard<mutex> lock(blocks_lock);\n \t\t// on-disk block: erase from list of blocks in manager\n@@ -607,7 +604,11 @@ struct BlockIndexManager {\n \t//! Returns true if the max_index has been altered\n \tbool RemoveIndex(idx_t index) {\n \t\t// remove this block from the set of blocks\n-\t\tindexes_in_use.erase(index);\n+\t\tauto entry = indexes_in_use.find(index);\n+\t\tif (entry == indexes_in_use.end()) {\n+\t\t\tthrow InternalException(\"RemoveIndex - index %llu not found in indexes_in_use\", index);\n+\t\t}\n+\t\tindexes_in_use.erase(entry);\n \t\tfree_indexes.insert(index);\n \t\t// check if we can truncate the file\n \n@@ -616,7 +617,7 @@ struct BlockIndexManager {\n \t\tif (max_index_in_use < max_index) {\n \t\t\t// max index in use is lower than the max_index\n \t\t\t// reduce the max_index\n-\t\t\tmax_index = max_index_in_use + 1;\n+\t\t\tmax_index = indexes_in_use.empty() ? 0 : max_index_in_use + 1;\n \t\t\t// we can remove any free_indexes that are larger than the current max_index\n \t\t\twhile (!free_indexes.empty()) {\n \t\t\t\tauto max_entry = *free_indexes.rbegin();\n@@ -692,16 +693,15 @@ class TemporaryFileHandle {\n \n \tunique_ptr<FileBuffer> ReadTemporaryBuffer(block_id_t id, idx_t block_index,\n \t                                           unique_ptr<FileBuffer> reusable_buffer) {\n-\t\tauto buffer =\n-\t\t    ReadTemporaryBufferInternal(BufferManager::GetBufferManager(db), *handle, GetPositionInFile(block_index),\n-\t\t                                Storage::BLOCK_SIZE, id, std::move(reusable_buffer));\n-\t\t{\n-\t\t\t// remove the block (and potentially truncate the temp file)\n-\t\t\tTemporaryFileLock lock(file_lock);\n-\t\t\tD_ASSERT(handle);\n-\t\t\tRemoveTempBlockIndex(lock, block_index);\n-\t\t}\n-\t\treturn buffer;\n+\t\treturn ReadTemporaryBufferInternal(BufferManager::GetBufferManager(db), *handle, GetPositionInFile(block_index),\n+\t\t                                   Storage::BLOCK_SIZE, id, std::move(reusable_buffer));\n+\t}\n+\n+\tvoid EraseBlockIndex(block_id_t block_index) {\n+\t\t// remove the block (and potentially truncate the temp file)\n+\t\tTemporaryFileLock lock(file_lock);\n+\t\tD_ASSERT(handle);\n+\t\tRemoveTempBlockIndex(lock, block_index);\n \t}\n \n \tbool DeleteIfEmpty() {\n@@ -717,6 +717,14 @@ class TemporaryFileHandle {\n \t\treturn true;\n \t}\n \n+\tTemporaryFileInformation GetTemporaryFile() {\n+\t\tTemporaryFileLock lock(file_lock);\n+\t\tTemporaryFileInformation info;\n+\t\tinfo.path = path;\n+\t\tinfo.size = GetPositionInFile(index_manager.GetMaxIndex());\n+\t\treturn info;\n+\t}\n+\n private:\n \tvoid CreateFileIfNotExists(TemporaryFileLock &) {\n \t\tif (handle) {\n@@ -817,7 +825,7 @@ class TemporaryFileManager {\n \t\t{\n \t\t\t// remove the block (and potentially erase the temp file)\n \t\t\tTemporaryManagerLock lock(manager_lock);\n-\t\t\tEraseUsedBlock(lock, id, handle, index.file_index);\n+\t\t\tEraseUsedBlock(lock, id, handle, index);\n \t\t}\n \t\treturn buffer;\n \t}\n@@ -826,14 +834,29 @@ class TemporaryFileManager {\n \t\tTemporaryManagerLock lock(manager_lock);\n \t\tauto index = GetTempBlockIndex(lock, id);\n \t\tauto handle = GetFileHandle(lock, index.file_index);\n-\t\tEraseUsedBlock(lock, id, handle, index.file_index);\n+\t\tEraseUsedBlock(lock, id, handle, index);\n+\t}\n+\n+\tvector<TemporaryFileInformation> GetTemporaryFiles() {\n+\t\tlock_guard<mutex> lock(manager_lock);\n+\t\tvector<TemporaryFileInformation> result;\n+\t\tfor (auto &file : files) {\n+\t\t\tresult.push_back(file.second->GetTemporaryFile());\n+\t\t}\n+\t\treturn result;\n \t}\n \n private:\n-\tvoid EraseUsedBlock(TemporaryManagerLock &lock, block_id_t id, TemporaryFileHandle *handle, idx_t file_index) {\n-\t\tused_blocks.erase(id);\n+\tvoid EraseUsedBlock(TemporaryManagerLock &lock, block_id_t id, TemporaryFileHandle *handle,\n+\t                    TemporaryFileIndex index) {\n+\t\tauto entry = used_blocks.find(id);\n+\t\tif (entry == used_blocks.end()) {\n+\t\t\tthrow InternalException(\"EraseUsedBlock - Block %llu not found in used blocks\", id);\n+\t\t}\n+\t\tused_blocks.erase(entry);\n+\t\thandle->EraseBlockIndex(index.block_index);\n \t\tif (handle->DeleteIfEmpty()) {\n-\t\t\tEraseFileHandle(lock, file_index);\n+\t\t\tEraseFileHandle(lock, index.file_index);\n \t\t}\n \t}\n \n@@ -965,6 +988,35 @@ void BufferManager::DeleteTemporaryFile(block_id_t id) {\n \t}\n }\n \n+vector<TemporaryFileInformation> BufferManager::GetTemporaryFiles() {\n+\tvector<TemporaryFileInformation> result;\n+\tif (temp_directory.empty()) {\n+\t\treturn result;\n+\t}\n+\t{\n+\t\tlock_guard<mutex> temp_handle_guard(temp_handle_lock);\n+\t\tif (temp_directory_handle) {\n+\t\t\tresult = temp_directory_handle->GetTempFile().GetTemporaryFiles();\n+\t\t}\n+\t}\n+\tauto &fs = FileSystem::GetFileSystem(db);\n+\tfs.ListFiles(temp_directory, [&](const string &name, bool is_dir) {\n+\t\tif (is_dir) {\n+\t\t\treturn;\n+\t\t}\n+\t\tif (!StringUtil::EndsWith(name, \".block\")) {\n+\t\t\treturn;\n+\t\t}\n+\t\tTemporaryFileInformation info;\n+\t\tinfo.path = name;\n+\t\tauto handle = fs.OpenFile(name, FileFlags::FILE_FLAGS_READ);\n+\t\tinfo.size = fs.GetFileSize(*handle);\n+\t\thandle.reset();\n+\t\tresult.push_back(info);\n+\t});\n+\treturn result;\n+}\n+\n string BufferManager::InMemoryWarning() {\n \tif (!temp_directory.empty()) {\n \t\treturn \"\";\n",
  "test_patch": "diff --git a/test/sql/outofcore/leftover_temp_files_issue_6420.test_slow b/test/sql/outofcore/leftover_temp_files_issue_6420.test_slow\nnew file mode 100644\nindex 000000000000..2e22a2eac9c8\n--- /dev/null\n+++ b/test/sql/outofcore/leftover_temp_files_issue_6420.test_slow\n@@ -0,0 +1,99 @@\n+# name: test/sql/outofcore/leftover_temp_files_issue_6420.test_slow\n+# description: Issue #6420: Large joins in persistent databases have left over temporary directory\n+# group: [outofcore]\n+\n+load __TEST_DIR__/leftover_temp_files.db\n+\n+require tpch\n+\n+statement ok\n+SET memory_limit='1GB';\n+\n+statement ok\n+CALL dbgen(sf=1);\n+\n+statement ok\n+ALTER TABLE lineitem RENAME TO lineitem1\n+\n+statement ok\n+CREATE TABLE lineitem2 AS FROM lineitem1\n+\n+# creating and dropping a temp table should not leave temp files\n+statement ok\n+CREATE OR REPLACE TEMPORARY TABLE ans as select l1.* from lineitem1 l1;\n+\n+query I\n+SELECT COUNT(*) > 0 FROM duckdb_temporary_files()\n+----\n+true\n+\n+statement ok\n+DROP TABLE ans;\n+\n+query I\n+SELECT COUNT(*) > 0 FROM duckdb_temporary_files()\n+----\n+false\n+\n+# creating and dropping a table with an ORDER BY should not leave temp files\n+statement ok\n+CREATE OR REPLACE TEMPORARY TABLE ans as select l1.*, l1.* from lineitem1 l1 ORDER BY l_orderkey, l_returnflag\n+\n+query I\n+SELECT COUNT(*) > 0 FROM duckdb_temporary_files()\n+----\n+true\n+\n+statement ok\n+DROP TABLE ans;\n+\n+query I\n+SELECT COUNT(*) > 0 FROM duckdb_temporary_files()\n+----\n+false\n+\n+# performing a small hash join should not leave temp files\n+statement ok\n+CREATE OR REPLACE TEMPORARY TABLE ans as select l1.*, l2.* from lineitem1 l1 JOIN (FROM lineitem2 l2 WHERE l_orderkey<10000) AS l2 USING (l_orderkey, l_linenumber)\n+\n+statement ok\n+DROP TABLE ans;\n+\n+query I\n+SELECT COUNT(*) > 0 FROM duckdb_temporary_files()\n+----\n+false\n+\n+# performing a large window function\n+statement ok\n+CREATE OR REPLACE TEMPORARY TABLE ans as select l1.*, row_number() OVER (PARTITION BY l_orderkey, l_linenumber ORDER BY l_orderkey) from lineitem1 l1\n+\n+query I\n+SELECT COUNT(*) > 0 FROM duckdb_temporary_files()\n+----\n+true\n+\n+statement ok\n+DROP TABLE ans;\n+\n+query I\n+SELECT COUNT(*) > 0 FROM duckdb_temporary_files()\n+----\n+false\n+\n+# performing a large hash join should not leave temp files\n+statement ok\n+CREATE OR REPLACE TEMPORARY TABLE ans as select l1.*, l2.* from lineitem1 l1 JOIN lineitem2 l2 USING (l_orderkey, l_linenumber)\n+\n+query I\n+SELECT COUNT(*) > 0 FROM duckdb_temporary_files()\n+----\n+true\n+\n+statement ok\n+DROP TABLE ans;\n+\n+query I\n+SELECT COUNT(*) > 0 FROM duckdb_temporary_files()\n+----\n+false\n",
  "problem_statement": "Large joins in persistent databases have left over temporary directory\n### What happens?\n\nWhen joining two large tables on primary keys (or very unique keys), the temporary directory doesn't get cleaned up when the query finishes. A number of `.tmp` files are left over, each about 1gb large.\n\n### To Reproduce\n\nMake sure to start a persistent database\r\n```\r\n$ ./build/release/duckdb join-storage.duckdb\r\n```\r\nSetup\r\n```SQL\r\ncall dbgen(sf=10);\r\nalter table lineitem rename to lineitem1;\r\n-- remove tables not needed for the test\r\ndrop table customer; drop table nation; drop table orders; drop table part; drop table partsupp; drop table region;\r\ncall dbgen(sf=10);\r\nalter table lineitem rename to lineitem2;\r\n-- remove tables not needed for the test\r\ndrop table customer; drop table nation; drop table orders; drop table part; drop table partsupp; drop table region;\r\n-- copy the line item table and create a nice pk for joining.\r\nCREATE TABLE lineitem_1_with_pk\r\n(\r\n    l_orderkey    BIGINT not null,\r\n    l_partkey     BIGINT not null,\r\n    l_suppkey     BIGINT not null,\r\n    l_linenumber  BIGINT not null,\r\n    l_quantity    DOUBLE PRECISION not null,\r\n    l_extendedprice  DOUBLE PRECISION not null,\r\n    l_discount    DOUBLE PRECISION not null,\r\n    l_tax         DOUBLE PRECISION not null,\r\n    l_returnflag  CHAR(1) not null,\r\n    l_linestatus  CHAR(1) not null,\r\n    l_shipdate    DATE not null,\r\n    l_commitdate  DATE not null,\r\n    l_receiptdate DATE not null,\r\n    l_shipinstruct CHAR(25) not null,\r\n    l_shipmode     CHAR(10) not null,\r\n    l_comment      VARCHAR(44) not null,\r\n    l_tmp_primary_key BIGINT\r\n);\r\nCREATE SEQUENCE l1_pk START 1;\r\nINSERT INTO lineitem_1_with_pk SELECT l1.*, nextval('l1_pk') FROM lineitem1 l1;\r\nCREATE TABLE lineitem_2_with_pk\r\n(\r\n    l_orderkey    BIGINT not null,\r\n    l_partkey     BIGINT not null,\r\n    l_suppkey     BIGINT not null,\r\n    l_linenumber  BIGINT not null,\r\n    l_quantity    DOUBLE PRECISION not null,\r\n    l_extendedprice  DOUBLE PRECISION not null,\r\n    l_discount    DOUBLE PRECISION not null,\r\n    l_tax         DOUBLE PRECISION not null,\r\n    l_returnflag  CHAR(1) not null,\r\n    l_linestatus  CHAR(1) not null,\r\n    l_shipdate    DATE not null,\r\n    l_commitdate  DATE not null,\r\n    l_receiptdate DATE not null,\r\n    l_shipinstruct CHAR(25) not null,\r\n    l_shipmode     CHAR(10) not null,\r\n    l_comment      VARCHAR(44) not null,\r\n    l_tmp_primary_key BIGINT,\r\n);\r\nCREATE SEQUENCE l2_pk START 1;\r\nINSERT INTO lineitem_2_with_pk SELECT l2.*, nextval('l2_pk') FROM lineitem2 l2;\r\nDROP TABLE lineitem1;\r\nDROP TABLE lineitem2;\r\n```\r\nThe following join query should trigger the bug. The query will succeed, but the data won't be cleaned up.\r\n```\r\nCREATE TABLE ans as select l1.*, l2.* from lineitem1 l1, lineitem2 l2 WHERE l1.l_tmp_primary_key = l2.l_tmp_primary_key;\r\n```\r\nInspect the tmp directory\r\n```\r\n$ ls -lah join-storage.duckdb.tmp\r\n```\r\n\r\n\r\n\n\n### OS:\n\nMacOS M1\n\n### DuckDB Version:\n\nv0.7.1-dev229\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nTom Ebergen\n\n### Affiliation:\n\nDuckDB Labs\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nLarge joins in persistent databases have left over temporary directory\n### What happens?\n\nWhen joining two large tables on primary keys (or very unique keys), the temporary directory doesn't get cleaned up when the query finishes. A number of `.tmp` files are left over, each about 1gb large.\n\n### To Reproduce\n\nMake sure to start a persistent database\r\n```\r\n$ ./build/release/duckdb join-storage.duckdb\r\n```\r\nSetup\r\n```SQL\r\ncall dbgen(sf=10);\r\nalter table lineitem rename to lineitem1;\r\n-- remove tables not needed for the test\r\ndrop table customer; drop table nation; drop table orders; drop table part; drop table partsupp; drop table region;\r\ncall dbgen(sf=10);\r\nalter table lineitem rename to lineitem2;\r\n-- remove tables not needed for the test\r\ndrop table customer; drop table nation; drop table orders; drop table part; drop table partsupp; drop table region;\r\n-- copy the line item table and create a nice pk for joining.\r\nCREATE TABLE lineitem_1_with_pk\r\n(\r\n    l_orderkey    BIGINT not null,\r\n    l_partkey     BIGINT not null,\r\n    l_suppkey     BIGINT not null,\r\n    l_linenumber  BIGINT not null,\r\n    l_quantity    DOUBLE PRECISION not null,\r\n    l_extendedprice  DOUBLE PRECISION not null,\r\n    l_discount    DOUBLE PRECISION not null,\r\n    l_tax         DOUBLE PRECISION not null,\r\n    l_returnflag  CHAR(1) not null,\r\n    l_linestatus  CHAR(1) not null,\r\n    l_shipdate    DATE not null,\r\n    l_commitdate  DATE not null,\r\n    l_receiptdate DATE not null,\r\n    l_shipinstruct CHAR(25) not null,\r\n    l_shipmode     CHAR(10) not null,\r\n    l_comment      VARCHAR(44) not null,\r\n    l_tmp_primary_key BIGINT\r\n);\r\nCREATE SEQUENCE l1_pk START 1;\r\nINSERT INTO lineitem_1_with_pk SELECT l1.*, nextval('l1_pk') FROM lineitem1 l1;\r\nCREATE TABLE lineitem_2_with_pk\r\n(\r\n    l_orderkey    BIGINT not null,\r\n    l_partkey     BIGINT not null,\r\n    l_suppkey     BIGINT not null,\r\n    l_linenumber  BIGINT not null,\r\n    l_quantity    DOUBLE PRECISION not null,\r\n    l_extendedprice  DOUBLE PRECISION not null,\r\n    l_discount    DOUBLE PRECISION not null,\r\n    l_tax         DOUBLE PRECISION not null,\r\n    l_returnflag  CHAR(1) not null,\r\n    l_linestatus  CHAR(1) not null,\r\n    l_shipdate    DATE not null,\r\n    l_commitdate  DATE not null,\r\n    l_receiptdate DATE not null,\r\n    l_shipinstruct CHAR(25) not null,\r\n    l_shipmode     CHAR(10) not null,\r\n    l_comment      VARCHAR(44) not null,\r\n    l_tmp_primary_key BIGINT,\r\n);\r\nCREATE SEQUENCE l2_pk START 1;\r\nINSERT INTO lineitem_2_with_pk SELECT l2.*, nextval('l2_pk') FROM lineitem2 l2;\r\nDROP TABLE lineitem1;\r\nDROP TABLE lineitem2;\r\n```\r\nThe following join query should trigger the bug. The query will succeed, but the data won't be cleaned up.\r\n```\r\nCREATE TABLE ans as select l1.*, l2.* from lineitem1 l1, lineitem2 l2 WHERE l1.l_tmp_primary_key = l2.l_tmp_primary_key;\r\n```\r\nInspect the tmp directory\r\n```\r\n$ ls -lah join-storage.duckdb.tmp\r\n```\r\n\r\n\r\n\n\n### OS:\n\nMacOS M1\n\n### DuckDB Version:\n\nv0.7.1-dev229\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nTom Ebergen\n\n### Affiliation:\n\nDuckDB Labs\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nLarge joins in persistent databases have left over temporary directory\n### What happens?\n\nWhen joining two large tables on primary keys (or very unique keys), the temporary directory doesn't get cleaned up when the query finishes. A number of `.tmp` files are left over, each about 1gb large.\n\n### To Reproduce\n\nMake sure to start a persistent database\r\n```\r\n$ ./build/release/duckdb join-storage.duckdb\r\n```\r\nSetup\r\n```SQL\r\ncall dbgen(sf=10);\r\nalter table lineitem rename to lineitem1;\r\n-- remove tables not needed for the test\r\ndrop table customer; drop table nation; drop table orders; drop table part; drop table partsupp; drop table region;\r\ncall dbgen(sf=10);\r\nalter table lineitem rename to lineitem2;\r\n-- remove tables not needed for the test\r\ndrop table customer; drop table nation; drop table orders; drop table part; drop table partsupp; drop table region;\r\n-- copy the line item table and create a nice pk for joining.\r\nCREATE TABLE lineitem_1_with_pk\r\n(\r\n    l_orderkey    BIGINT not null,\r\n    l_partkey     BIGINT not null,\r\n    l_suppkey     BIGINT not null,\r\n    l_linenumber  BIGINT not null,\r\n    l_quantity    DOUBLE PRECISION not null,\r\n    l_extendedprice  DOUBLE PRECISION not null,\r\n    l_discount    DOUBLE PRECISION not null,\r\n    l_tax         DOUBLE PRECISION not null,\r\n    l_returnflag  CHAR(1) not null,\r\n    l_linestatus  CHAR(1) not null,\r\n    l_shipdate    DATE not null,\r\n    l_commitdate  DATE not null,\r\n    l_receiptdate DATE not null,\r\n    l_shipinstruct CHAR(25) not null,\r\n    l_shipmode     CHAR(10) not null,\r\n    l_comment      VARCHAR(44) not null,\r\n    l_tmp_primary_key BIGINT\r\n);\r\nCREATE SEQUENCE l1_pk START 1;\r\nINSERT INTO lineitem_1_with_pk SELECT l1.*, nextval('l1_pk') FROM lineitem1 l1;\r\nCREATE TABLE lineitem_2_with_pk\r\n(\r\n    l_orderkey    BIGINT not null,\r\n    l_partkey     BIGINT not null,\r\n    l_suppkey     BIGINT not null,\r\n    l_linenumber  BIGINT not null,\r\n    l_quantity    DOUBLE PRECISION not null,\r\n    l_extendedprice  DOUBLE PRECISION not null,\r\n    l_discount    DOUBLE PRECISION not null,\r\n    l_tax         DOUBLE PRECISION not null,\r\n    l_returnflag  CHAR(1) not null,\r\n    l_linestatus  CHAR(1) not null,\r\n    l_shipdate    DATE not null,\r\n    l_commitdate  DATE not null,\r\n    l_receiptdate DATE not null,\r\n    l_shipinstruct CHAR(25) not null,\r\n    l_shipmode     CHAR(10) not null,\r\n    l_comment      VARCHAR(44) not null,\r\n    l_tmp_primary_key BIGINT,\r\n);\r\nCREATE SEQUENCE l2_pk START 1;\r\nINSERT INTO lineitem_2_with_pk SELECT l2.*, nextval('l2_pk') FROM lineitem2 l2;\r\nDROP TABLE lineitem1;\r\nDROP TABLE lineitem2;\r\n```\r\nThe following join query should trigger the bug. The query will succeed, but the data won't be cleaned up.\r\n```\r\nCREATE TABLE ans as select l1.*, l2.* from lineitem1 l1, lineitem2 l2 WHERE l1.l_tmp_primary_key = l2.l_tmp_primary_key;\r\n```\r\nInspect the tmp directory\r\n```\r\n$ ls -lah join-storage.duckdb.tmp\r\n```\r\n\r\n\r\n\n\n### OS:\n\nMacOS M1\n\n### DuckDB Version:\n\nv0.7.1-dev229\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nTom Ebergen\n\n### Affiliation:\n\nDuckDB Labs\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "Just to clarify, the `.block` files are cleaned up, but the `.tmp` files created by `TemporaryFileHandle` aren't. This is definitely a bug, but the bug is probably not in `PhysicalHashJoin`, because the hash join just creates blocks through the `BufferManager` API.\n> Just to clarify, the `.block` files are cleaned up, but the `.tmp` files created by `TemporaryFileHandle` aren't. This is definitely a bug, but the bug is probably not in `PhysicalHashJoin`, because the hash join just creates blocks through the `BufferManager` API.\r\n\r\nYes this is true. Just `.tmp` files are leftover. I also just noticed that if I exit the duckdb shell after the query completes, the `.tmp` files are cleaned up.\nJust to clarify, the `.block` files are cleaned up, but the `.tmp` files created by `TemporaryFileHandle` aren't. This is definitely a bug, but the bug is probably not in `PhysicalHashJoin`, because the hash join just creates blocks through the `BufferManager` API.\n> Just to clarify, the `.block` files are cleaned up, but the `.tmp` files created by `TemporaryFileHandle` aren't. This is definitely a bug, but the bug is probably not in `PhysicalHashJoin`, because the hash join just creates blocks through the `BufferManager` API.\r\n\r\nYes this is true. Just `.tmp` files are leftover. I also just noticed that if I exit the duckdb shell after the query completes, the `.tmp` files are cleaned up.\nJust to clarify, the `.block` files are cleaned up, but the `.tmp` files created by `TemporaryFileHandle` aren't. This is definitely a bug, but the bug is probably not in `PhysicalHashJoin`, because the hash join just creates blocks through the `BufferManager` API.\n> Just to clarify, the `.block` files are cleaned up, but the `.tmp` files created by `TemporaryFileHandle` aren't. This is definitely a bug, but the bug is probably not in `PhysicalHashJoin`, because the hash join just creates blocks through the `BufferManager` API.\r\n\r\nYes this is true. Just `.tmp` files are leftover. I also just noticed that if I exit the duckdb shell after the query completes, the `.tmp` files are cleaned up.",
  "created_at": "2023-02-22T14:17:06Z"
}