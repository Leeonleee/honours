{
  "repo": "duckdb/duckdb",
  "pull_number": 9931,
  "instance_id": "duckdb__duckdb-9931",
  "issue_numbers": [
    "109"
  ],
  "base_commit": "e5716b925d6f987c1a0dcb9d8657a61be9fe8d2a",
  "patch": "diff --git a/benchmark/tpch/delete/delete_lineitem_half.benchmark b/benchmark/tpch/delete/delete_lineitem_half.benchmark\nnew file mode 100644\nindex 000000000000..050791fe4985\n--- /dev/null\n+++ b/benchmark/tpch/delete/delete_lineitem_half.benchmark\n@@ -0,0 +1,21 @@\n+# name: benchmark/tpch/delete/delete_lineitem_half.benchmark\n+# description: Delete half of the lineitem table\n+# group: [delete]\n+\n+require tpch\n+\n+cache tpch_sf1.duckdb\n+\n+load\n+CALL dbgen(sf=1);\n+CREATE TABLE lineitem_deletes AS FROM lineitem;\n+\n+run\n+DELETE FROM lineitem_deletes WHERE l_orderkey%2=0;\n+\n+result I\n+3000586\n+\n+cleanup\n+DROP TABLE lineitem_deletes;\n+CREATE TABLE lineitem_deletes AS FROM lineitem;\ndiff --git a/src/common/types/data_chunk.cpp b/src/common/types/data_chunk.cpp\nindex a734f08629ef..aa1e1deb8cab 100644\n--- a/src/common/types/data_chunk.cpp\n+++ b/src/common/types/data_chunk.cpp\n@@ -305,6 +305,15 @@ void DataChunk::Slice(DataChunk &other, const SelectionVector &sel, idx_t count_\n \t}\n }\n \n+void DataChunk::Slice(idx_t offset, idx_t slice_count) {\n+\tD_ASSERT(offset + slice_count <= size());\n+\tSelectionVector sel(slice_count);\n+\tfor (idx_t i = 0; i < slice_count; i++) {\n+\t\tsel.set_index(i, offset + i);\n+\t}\n+\tSlice(sel, slice_count);\n+}\n+\n unsafe_unique_array<UnifiedVectorFormat> DataChunk::ToUnifiedFormat() {\n \tauto unified_data = make_unsafe_uniq_array<UnifiedVectorFormat>(ColumnCount());\n \tfor (idx_t col_idx = 0; col_idx < ColumnCount(); col_idx++) {\ndiff --git a/src/include/duckdb/common/types/data_chunk.hpp b/src/include/duckdb/common/types/data_chunk.hpp\nindex fdb44cddd98a..141dee911a18 100644\n--- a/src/include/duckdb/common/types/data_chunk.hpp\n+++ b/src/include/duckdb/common/types/data_chunk.hpp\n@@ -133,6 +133,9 @@ class DataChunk {\n \t//! Turning all Vectors into Dictionary Vectors, using 'sel'\n \tDUCKDB_API void Slice(DataChunk &other, const SelectionVector &sel, idx_t count, idx_t col_offset = 0);\n \n+\t//! Slice a DataChunk from \"offset\" to \"offset + count\"\n+\tDUCKDB_API void Slice(idx_t offset, idx_t count);\n+\n \t//! Resets the DataChunk to its state right after the DataChunk::Initialize\n \t//! function was called. This sets the count to 0, and resets each member\n \t//! Vector to point back to the data owned by this DataChunk.\ndiff --git a/src/include/duckdb/storage/table/row_group.hpp b/src/include/duckdb/storage/table/row_group.hpp\nindex 5b416ba1179e..ec702cb21cfe 100644\n--- a/src/include/duckdb/storage/table/row_group.hpp\n+++ b/src/include/duckdb/storage/table/row_group.hpp\n@@ -118,7 +118,8 @@ class RowGroup : public SegmentBase<RowGroup> {\n \tidx_t Delete(TransactionData transaction, DataTable &table, row_t *row_ids, idx_t count);\n \n \tRowGroupWriteData WriteToDisk(PartialBlockManager &manager, const vector<CompressionType> &compression_types);\n-\tbool AllDeleted();\n+\t//! Returns the number of committed rows (count - committed deletes)\n+\tidx_t GetCommittedRowCount();\n \tRowGroupPointer Checkpoint(RowGroupWriter &writer, TableStatistics &global_stats);\n \n \tvoid InitializeAppend(RowGroupAppendState &append_state);\ndiff --git a/src/include/duckdb/storage/table/row_group_collection.hpp b/src/include/duckdb/storage/table/row_group_collection.hpp\nindex f2ea8fff6f4c..8e754dd25903 100644\n--- a/src/include/duckdb/storage/table/row_group_collection.hpp\n+++ b/src/include/duckdb/storage/table/row_group_collection.hpp\n@@ -29,6 +29,7 @@ class BoundConstraint;\n class RowGroupSegmentTree;\n struct ColumnSegmentInfo;\n class MetadataManager;\n+struct VacuumState;\n \n class RowGroupCollection {\n public:\n@@ -87,6 +88,9 @@ class RowGroupCollection {\n \n \tvoid Checkpoint(TableDataWriter &writer, TableStatistics &global_stats);\n \n+\tvoid InitializeVacuumState(VacuumState &state, vector<SegmentNode<RowGroup>> &segments);\n+\tvoid VacuumDeletes(VacuumState &state, vector<SegmentNode<RowGroup>> &segments, idx_t segment_idx);\n+\n \tvoid CommitDropColumn(idx_t index);\n \tvoid CommitDropTable();\n \ndiff --git a/src/storage/table/chunk_info.cpp b/src/storage/table/chunk_info.cpp\nindex fa19b5021070..344b6c4d6548 100644\n--- a/src/storage/table/chunk_info.cpp\n+++ b/src/storage/table/chunk_info.cpp\n@@ -22,7 +22,7 @@ struct CommittedVersionOperator {\n \t}\n \n \tstatic bool UseDeletedVersion(transaction_t min_start_time, transaction_t min_transaction_id, transaction_t id) {\n-\t\treturn (id >= min_start_time && id < TRANSACTION_ID_START) || (id >= min_transaction_id);\n+\t\treturn (id >= min_start_time && id < TRANSACTION_ID_START) || id == NOT_DELETED_ID;\n \t}\n };\n \ndiff --git a/src/storage/table/row_group.cpp b/src/storage/table/row_group.cpp\nindex 73fe4e6a373b..cf5dbe450b34 100644\n--- a/src/storage/table/row_group.cpp\n+++ b/src/storage/table/row_group.cpp\n@@ -668,6 +668,7 @@ void RowGroup::InitializeAppend(RowGroupAppendState &append_state) {\n \n void RowGroup::Append(RowGroupAppendState &state, DataChunk &chunk, idx_t append_count) {\n \t// append to the current row_group\n+\tD_ASSERT(chunk.ColumnCount() == GetColumnCount());\n \tfor (idx_t i = 0; i < GetColumnCount(); i++) {\n \t\tauto &col_data = GetColumn(i);\n \t\tcol_data.Append(state.states[i], chunk.data[i], append_count);\n@@ -759,16 +760,12 @@ RowGroupWriteData RowGroup::WriteToDisk(PartialBlockManager &manager,\n \treturn result;\n }\n \n-bool RowGroup::AllDeleted() {\n-\tif (HasUnloadedDeletes()) {\n-\t\t// deletes aren't loaded yet - we know not everything is deleted\n-\t\treturn false;\n-\t}\n+idx_t RowGroup::GetCommittedRowCount() {\n \tauto &vinfo = GetVersionInfo();\n \tif (!vinfo) {\n-\t\treturn false;\n+\t\treturn count;\n \t}\n-\treturn vinfo->GetCommittedDeletedCount(count) == count;\n+\treturn count - vinfo->GetCommittedDeletedCount(count);\n }\n \n bool RowGroup::HasUnloadedDeletes() const {\ndiff --git a/src/storage/table/row_group_collection.cpp b/src/storage/table/row_group_collection.cpp\nindex c8342dfdd97b..e9a456ef8ee5 100644\n--- a/src/storage/table/row_group_collection.cpp\n+++ b/src/storage/table/row_group_collection.cpp\n@@ -359,11 +359,7 @@ bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {\n \t\t\tD_ASSERT(chunk.size() == remaining + append_count);\n \t\t\t// slice the input chunk\n \t\t\tif (remaining < chunk.size()) {\n-\t\t\t\tSelectionVector sel(remaining);\n-\t\t\t\tfor (idx_t i = 0; i < remaining; i++) {\n-\t\t\t\t\tsel.set_index(i, append_count + i);\n-\t\t\t\t}\n-\t\t\t\tchunk.Slice(sel, remaining);\n+\t\t\t\tchunk.Slice(append_count, remaining);\n \t\t\t}\n \t\t\t// append a new row_group\n \t\t\tnew_row_group = true;\n@@ -599,17 +595,183 @@ void RowGroupCollection::UpdateColumn(TransactionData transaction, Vector &row_i\n //===--------------------------------------------------------------------===//\n // Checkpoint\n //===--------------------------------------------------------------------===//\n-void RowGroupCollection::Checkpoint(TableDataWriter &writer, TableStatistics &global_stats) {\n-\tbool can_vacuum_deletes = info->indexes.Empty();\n-\tidx_t start = this->row_start;\n-\tauto segments = row_groups->MoveSegments();\n-\tauto l = row_groups->Lock();\n+struct VacuumState {\n+\tbool can_vacuum_deletes = false;\n+\tidx_t next_vacuum_idx = 0;\n+\tvector<idx_t> row_group_counts;\n+};\n+\n+void RowGroupCollection::InitializeVacuumState(VacuumState &state, vector<SegmentNode<RowGroup>> &segments) {\n+\tstate.can_vacuum_deletes = info->indexes.Empty();\n+\tif (!state.can_vacuum_deletes) {\n+\t\treturn;\n+\t}\n+\t// obtain the set of committed row counts for each row group\n+\tstate.row_group_counts.reserve(segments.size());\n \tfor (auto &entry : segments) {\n \t\tauto &row_group = *entry.node;\n-\t\tif (can_vacuum_deletes && row_group.AllDeleted()) {\n+\t\tauto row_group_count = row_group.GetCommittedRowCount();\n+\t\tif (row_group_count == 0) {\n+\t\t\t// empty row group - we can drop it entirely\n \t\t\trow_group.CommitDrop();\n+\t\t\tentry.node.reset();\n+\t\t}\n+\t\tstate.row_group_counts.push_back(row_group_count);\n+\t}\n+}\n+void RowGroupCollection::VacuumDeletes(VacuumState &state, vector<SegmentNode<RowGroup>> &segments, idx_t segment_idx) {\n+\tstatic constexpr const idx_t MAX_MERGE_COUNT = 3;\n+\n+\tif (!state.can_vacuum_deletes || state.next_vacuum_idx >= segment_idx) {\n+\t\t// don't vacuum this segment index\n+\t\treturn;\n+\t}\n+\tif (state.row_group_counts[segment_idx] == 0) {\n+\t\t// segment was already dropped - skip\n+\t\treturn;\n+\t}\n+\tidx_t merge_rows;\n+\tidx_t next_idx;\n+\tidx_t merge_count;\n+\tidx_t target_count;\n+\tbool perform_merge = false;\n+\t// check if we can merge row groups adjacent to the current segment_idx\n+\t// we try merging row groups into batches of 1-3 row groups\n+\t// our goal is to reduce the amount of row groups\n+\t// hence we target_count should be less than merge_count for a marge to be worth it\n+\t// we greedily prefer to merge to the lowest target_count\n+\t// i.e. we prefer to merge 2 row groups into 1, than 3 row groups into 2\n+\tfor (target_count = 1; target_count <= MAX_MERGE_COUNT; target_count++) {\n+\t\tauto total_target_size = target_count * Storage::ROW_GROUP_SIZE;\n+\t\tmerge_count = 0;\n+\t\tmerge_rows = 0;\n+\t\tfor (next_idx = segment_idx; next_idx < segments.size(); next_idx++) {\n+\t\t\tif (state.row_group_counts[next_idx] == 0) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tif (merge_rows + state.row_group_counts[next_idx] > total_target_size) {\n+\t\t\t\t// does not fit\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\t// we can merge this row group together with the other row group\n+\t\t\tmerge_rows += state.row_group_counts[next_idx];\n+\t\t\tmerge_count++;\n+\t\t}\n+\t\tif (target_count < merge_count) {\n+\t\t\t// we can reduce \"merge_count\" row groups to \"target_count\"\n+\t\t\t// perform the merge at this level\n+\t\t\tperform_merge = true;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (!perform_merge) {\n+\t\treturn;\n+\t}\n+\t// create the new set of target row groups (initially empty)\n+\tvector<unique_ptr<RowGroup>> new_row_groups;\n+\tvector<idx_t> append_counts;\n+\tidx_t row_group_rows = merge_rows;\n+\tidx_t start = segments[segment_idx].node->start;\n+\tfor (idx_t target_idx = 0; target_idx < target_count; target_idx++) {\n+\t\tidx_t current_row_group_rows = MinValue<idx_t>(row_group_rows, Storage::ROW_GROUP_SIZE);\n+\t\tauto new_row_group = make_uniq<RowGroup>(*this, start, current_row_group_rows);\n+\t\tnew_row_group->InitializeEmpty(types);\n+\t\tnew_row_groups.push_back(std::move(new_row_group));\n+\t\tappend_counts.push_back(0);\n+\n+\t\trow_group_rows -= current_row_group_rows;\n+\t\tstart += current_row_group_rows;\n+\t}\n+\n+\tDataChunk scan_chunk;\n+\tscan_chunk.Initialize(Allocator::DefaultAllocator(), types);\n+\n+\tvector<column_t> column_ids;\n+\tfor (idx_t c = 0; c < types.size(); c++) {\n+\t\tcolumn_ids.push_back(c);\n+\t}\n+\n+\tidx_t current_append_idx = 0;\n+\n+\t// fill the new row group with the merged rows\n+\tTableAppendState append_state;\n+\tnew_row_groups[current_append_idx]->InitializeAppend(append_state.row_group_append_state);\n+\n+\tTableScanState scan_state;\n+\tscan_state.Initialize(column_ids);\n+\tscan_state.table_state.Initialize(types);\n+\tscan_state.table_state.max_row = idx_t(-1);\n+\tfor (idx_t c_idx = segment_idx; c_idx < next_idx; c_idx++) {\n+\t\tif (state.row_group_counts[c_idx] == 0) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tauto &current_row_group = *segments[c_idx].node;\n+\n+\t\tcurrent_row_group.InitializeScan(scan_state.table_state);\n+\t\twhile (true) {\n+\t\t\tscan_chunk.Reset();\n+\n+\t\t\tcurrent_row_group.ScanCommitted(scan_state.table_state, scan_chunk,\n+\t\t\t                                TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED);\n+\t\t\tif (scan_chunk.size() == 0) {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tidx_t remaining = scan_chunk.size();\n+\t\t\twhile (remaining > 0) {\n+\t\t\t\tidx_t append_count =\n+\t\t\t\t    MinValue<idx_t>(remaining, Storage::ROW_GROUP_SIZE - append_counts[current_append_idx]);\n+\t\t\t\tnew_row_groups[current_append_idx]->Append(append_state.row_group_append_state, scan_chunk,\n+\t\t\t\t                                           append_count);\n+\t\t\t\tappend_counts[current_append_idx] += append_count;\n+\t\t\t\tremaining -= append_count;\n+\t\t\t\tif (remaining > 0) {\n+\t\t\t\t\t// move to the next row group\n+\t\t\t\t\tcurrent_append_idx++;\n+\t\t\t\t\tnew_row_groups[current_append_idx]->InitializeAppend(append_state.row_group_append_state);\n+\t\t\t\t\t// slice chunk for the next append\n+\t\t\t\t\tscan_chunk.Slice(append_count, remaining);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\t// drop the row group after merging\n+\t\tstate.row_group_counts[c_idx] = 0;\n+\t\tcurrent_row_group.CommitDrop();\n+\t\tsegments[c_idx].node.reset();\n+\t}\n+\tidx_t total_append_count = 0;\n+\tfor (idx_t target_idx = 0; target_idx < target_count; target_idx++) {\n+\t\tauto &row_group = new_row_groups[target_idx];\n+\t\trow_group->Verify();\n+\n+\t\t// assign the new row group to the current segment\n+\t\tstate.row_group_counts[segment_idx + target_idx] = row_group->count;\n+\t\tsegments[segment_idx + target_idx].node = std::move(row_group);\n+\t\ttotal_append_count += append_counts[target_idx];\n+\t}\n+\tif (total_append_count != merge_rows) {\n+\t\tthrow InternalException(\"Mismatch in row group count vs verify count in RowGroupCollection::Checkpoint\");\n+\t}\n+\t// skip vacuuming by the row groups we have merged\n+\tstate.next_vacuum_idx = segment_idx + merge_count;\n+}\n+\n+void RowGroupCollection::Checkpoint(TableDataWriter &writer, TableStatistics &global_stats) {\n+\tauto segments = row_groups->MoveSegments();\n+\tauto l = row_groups->Lock();\n+\n+\tVacuumState vacuum_state;\n+\tInitializeVacuumState(vacuum_state, segments);\n+\tidx_t start = this->row_start;\n+\tfor (idx_t segment_idx = 0; segment_idx < segments.size(); segment_idx++) {\n+\t\tauto &entry = segments[segment_idx];\n+\t\t// prior to checkpointing check if we can vacuum any rows at this segment\n+\t\tVacuumDeletes(vacuum_state, segments, segment_idx);\n+\t\tif (!entry.node) {\n+\t\t\t// row group was vacuumed/dropped - skip\n \t\t\tcontinue;\n \t\t}\n+\t\tauto &row_group = *entry.node;\n+\n \t\trow_group.MoveToCollection(*this, start);\n \t\tauto row_group_writer = writer.GetRowGroupWriter(row_group);\n \t\tauto pointer = row_group.Checkpoint(*row_group_writer, global_stats);\n",
  "test_patch": "diff --git a/test/sql/storage/delete/drop_many_deletes.test_slow b/test/sql/storage/delete/drop_many_deletes.test_slow\nindex 9e572b61609d..ef040352bec2 100644\n--- a/test/sql/storage/delete/drop_many_deletes.test_slow\n+++ b/test/sql/storage/delete/drop_many_deletes.test_slow\n@@ -24,7 +24,7 @@ SELECT COUNT(*) FROM integers\n query I\n SELECT COUNT(*) FROM pragma_metadata_info()\n ----\n-7\n+3\n \n statement ok\n DROP TABLE integers\ndiff --git a/test/sql/storage/vacuum/vacuum_deletes_cleanup.test_slow b/test/sql/storage/vacuum/vacuum_deletes_cleanup.test_slow\nindex 59341af0ac2f..cf625353c8ca 100644\n--- a/test/sql/storage/vacuum/vacuum_deletes_cleanup.test_slow\n+++ b/test/sql/storage/vacuum/vacuum_deletes_cleanup.test_slow\n@@ -27,7 +27,7 @@ SELECT SUM(i) FROM integers\n NULL\n \n query I\n-SELECT total_blocks * block_size < 15 * 262144 FROM pragma_database_size()\n+SELECT total_blocks * block_size < 8 * 262144 FROM pragma_database_size()\n ----\n true\n \n@@ -85,7 +85,7 @@ SELECT SUM(i) FROM integers\n NULL\n \n query I\n-SELECT total_blocks * block_size < 15 * 262144 FROM pragma_database_size()\n+SELECT total_blocks * block_size < 8 * 262144 FROM pragma_database_size()\n ----\n true\n \ndiff --git a/test/sql/storage/vacuum/vacuum_partial_deletes.test_slow b/test/sql/storage/vacuum/vacuum_partial_deletes.test_slow\nnew file mode 100644\nindex 000000000000..2aac9f813cf7\n--- /dev/null\n+++ b/test/sql/storage/vacuum/vacuum_partial_deletes.test_slow\n@@ -0,0 +1,42 @@\n+# name: test/sql/storage/vacuum/vacuum_partial_deletes.test_slow\n+# description: Verify that deletes get vacuumed correctly through merging of adjacent row groups\n+# group: [vacuum]\n+\n+load __TEST_DIR__/vacuum_partial_deletes.db\n+\n+statement ok\n+CREATE TABLE integers(i INTEGER);\n+\n+statement ok\n+INSERT INTO integers SELECT * FROM range(1000000);\n+\n+query I\n+SELECT SUM(i) FROM integers WHERE i%2<>0\n+----\n+250000000000\n+\n+statement ok\n+CHECKPOINT\n+\n+# 1M rows, 128K each is around ~9 row groups\n+query I\n+SELECT COUNT(DISTINCT row_group_id) > 6 AND COUNT(DISTINCT row_group_id) <= 10 FROM pragma_storage_info('integers')\n+----\n+true\n+\n+statement ok\n+DELETE FROM integers WHERE i%2=0\n+\n+statement ok\n+CHECKPOINT\n+\n+query I\n+SELECT SUM(i) FROM integers\n+----\n+250000000000\n+\n+# after deleting we have 500K rows left, which should be 4~5 row groups\n+query I\n+SELECT COUNT(DISTINCT row_group_id) > 3 AND COUNT(DISTINCT row_group_id) <= 6 FROM pragma_storage_info('integers')\n+----\n+true\ndiff --git a/test/sql/storage/vacuum/vacuum_partial_deletes_cleanup.test_slow b/test/sql/storage/vacuum/vacuum_partial_deletes_cleanup.test_slow\nnew file mode 100644\nindex 000000000000..9c8b3a6763b2\n--- /dev/null\n+++ b/test/sql/storage/vacuum/vacuum_partial_deletes_cleanup.test_slow\n@@ -0,0 +1,45 @@\n+# name: test/sql/storage/vacuum/vacuum_partial_deletes_cleanup.test_slow\n+# description: Verify that deleting rows and re-appending does not increase storage size\n+# group: [vacuum]\n+\n+load __TEST_DIR__/vacuum_deletes_partial_cleanup.db\n+\n+statement ok\n+CREATE TABLE integers(i INTEGER);\n+\n+# verify that deleting an entire table in a loop doesn't increase database size (i.e. deletes are vacuumed correctly)\n+loop i 0 10\n+\n+statement ok\n+INSERT INTO integers SELECT * FROM range(1000000);\n+\n+query I\n+SELECT SUM(i) FROM integers\n+----\n+499999500000\n+\n+query I\n+DELETE FROM integers WHERE i%2=0\n+----\n+500000\n+\n+statement ok\n+CHECKPOINT\n+\n+query I\n+DELETE FROM integers WHERE i%2<>0\n+----\n+500000\n+\n+\n+query I\n+SELECT SUM(i) FROM integers\n+----\n+NULL\n+\n+query I\n+SELECT total_blocks * block_size < 8 * 262144 FROM pragma_database_size()\n+----\n+true\n+\n+endloop\ndiff --git a/test/sql/storage/vacuum/vacuum_partial_deletes_complex.test_slow b/test/sql/storage/vacuum/vacuum_partial_deletes_complex.test_slow\nnew file mode 100644\nindex 000000000000..1ebb0698a0c5\n--- /dev/null\n+++ b/test/sql/storage/vacuum/vacuum_partial_deletes_complex.test_slow\n@@ -0,0 +1,44 @@\n+# name: test/sql/storage/vacuum/vacuum_partial_deletes_complex.test_slow\n+# description: Verify that deletes get vacuumed correctly through merging of adjacent row groups\n+# group: [vacuum]\n+\n+load __TEST_DIR__/vacuum_partial_deletes_complex.db\n+\n+statement ok\n+CREATE TABLE integers(i INTEGER);\n+\n+statement ok\n+INSERT INTO integers SELECT * FROM range(1000000);\n+\n+query I\n+SELECT SUM(i) FROM integers WHERE i%3<>0\n+----\n+333332666667\n+\n+statement ok\n+CHECKPOINT\n+\n+# 1M rows, 128K each is around ~9 row groups\n+query I\n+SELECT COUNT(DISTINCT row_group_id) > 6 AND COUNT(DISTINCT row_group_id) <= 10 FROM pragma_storage_info('integers')\n+----\n+true\n+\n+statement ok\n+DELETE FROM integers WHERE i%3=0\n+\n+statement ok\n+CHECKPOINT\n+\n+query I\n+SELECT SUM(i) FROM integers\n+----\n+333332666667\n+\n+# after deleting we have 666K rows left, which should be 6~7 row groups\n+# note that this is more difficult, since after deleting each row group has ~80K rows\n+# this means that we need to merge 3 row groups into 2 row groups\n+query I\n+SELECT COUNT(DISTINCT row_group_id) > 4 AND COUNT(DISTINCT row_group_id) <= 7 FROM pragma_storage_info('integers')\n+----\n+true\ndiff --git a/test/sql/storage/vacuum/vacuum_partial_deletes_mixed.test_slow b/test/sql/storage/vacuum/vacuum_partial_deletes_mixed.test_slow\nnew file mode 100644\nindex 000000000000..6437544a77e1\n--- /dev/null\n+++ b/test/sql/storage/vacuum/vacuum_partial_deletes_mixed.test_slow\n@@ -0,0 +1,100 @@\n+# name: test/sql/storage/vacuum/vacuum_partial_deletes_mixed.test_slow\n+# description: Verify that deletes get vacuumed correctly through merging of adjacent row groups\n+# group: [vacuum]\n+\n+load __TEST_DIR__/vacuum_partial_deletes_mixed.db\n+\n+statement ok\n+CREATE TABLE integers(i INTEGER);\n+\n+statement ok\n+INSERT INTO integers SELECT * FROM range(1000000);\n+\n+# 1M rows, 128K each is around ~9 row groups\n+query I\n+SELECT COUNT(DISTINCT row_group_id) > 6 AND COUNT(DISTINCT row_group_id) <= 10 FROM pragma_storage_info('integers')\n+----\n+true\n+\n+# mix of deletions\n+# we use weird/odd numbers here as well for testing purposes\n+# 0..157K - delete every other entry\n+query I\n+DELETE FROM integers WHERE i%2 AND i<157353;\n+----\n+78676\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM integers\n+----\n+921324\t493809587024\t0\t999999\n+\n+# 157K..433K - delete ALL entries\n+query I\n+DELETE FROM integers WHERE i>=157353 AND i<433427;\n+----\n+276074\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM integers\n+----\n+645250\t412260226201\t0\t999999\n+\n+# 433K..512K - delete every odd 5K entries\n+query I\n+DELETE FROM integers WHERE (i//4973)%2=0 AND i>=433427 AND i<512933;\n+----\n+39784\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM integers\n+----\n+605466\t393365969137\t0\t999999\n+\n+# 512K..732K - delete every 7 entries\n+query I\n+DELETE FROM integers WHERE i%7=0 AND i>=512933 AND i<721377\n+----\n+29777\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM integers\n+----\n+575689\t374988944702\t0\t999999\n+\n+# 732K..910K - delete every 3 entries but based on the hash to make it more random\n+query I\n+DELETE FROM integers WHERE hash(i)%3=0 AND i>=721377 AND i<909999\n+----\n+62853\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM integers\n+----\n+512836\t323738360425\t0\t999999\n+\n+# 732K..910K - delete every 2 entries but based on the hash to make it more random\n+query I\n+DELETE FROM integers WHERE hash(i)%2=0 AND i>=909999\n+----\n+89983\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM integers\n+----\n+422853\t237804579942\t0\t987388\n+\n+statement ok\n+CHECKPOINT\n+\n+query IIII\n+SELECT COUNT(*), SUM(i), MIN(i), MAX(i) FROM integers\n+----\n+422853\t237804579942\t0\t987388\n+\n+# after deleting we have 422K rows left, which should be 4 row groups\n+# note that achieving exactly 4 row groups is difficult because of the mixed nature of the deletes\n+query I\n+SELECT COUNT(DISTINCT row_group_id) >= 4 AND COUNT(DISTINCT row_group_id) <= 7 FROM pragma_storage_info('integers')\n+----\n+true\n",
  "problem_statement": "Vacuum Deletes\nWhile updates are in-place, deletes leave holes in the table right now. We need to figure out how to deal with that, either by vacuuming or in some other way. Deletes are not mentioned at all in the MVCC paper as far as I remember. We can reuse the space for inserts later on if we know where the gaps are, or move stuff around.\n",
  "hints_text": "Any updates?\nThis is preventing DuckDB to be used in my architecture: every day I have a batch which updates some tables based on external data feeds, by DELETE-ing some records and INSERT-ing new ones. As the space used by the DELETEd records is never reclaimed, this database is growing indefinitely, adding 5 gigabytes every day.\nFixing this (and several other issues related to the storage) are on the roadmap for the next release.\n@Mytherin any idea on when this will be? or when there will be a release candidate? Been having to do exports to parquet and load back and its very painful to manage without some degree of vacuum command.\n> @Mytherin any idea on when this will be? or when there will be a release candidate? Been having to do exports to parquet and load back and its very painful to manage without some degree of vacuum command.\r\n\r\n@tsriharsha  We've found the following process to be much faster/ more efficient than exporting to parquet and reloading:\r\n\r\n1. `duckdb temp.db`\r\n2. `ATTACH '/my/bloated/duck.db' as source (READ_ONLY);`\r\n3. `CREATE TABLE mytable AS SELECT * from source.main.mytable;`\r\n4. `DETACH source;`\r\n5.  bash: `mv temp.db /my/bloated/duck.db` \r\n\r\nHope this helps\nI believe I've noticed a similar problem with a bloated DuckDB file after dropping tables (via `DROP` and `CREATE OR REPLACE`). Will the upcoming fix address this situation as well?\nIs there new progress on this? my db file size in disk keep increasing, delete rows/truncate table can not reduce the size. \nWould also like to know if there are any updates here, this is crucial for us to be able to scale with DuckDB.\nDon't know if this is the same or a different issue, but it also happens with `:memory:` DB. When I delete or recreate a table, memory consumption of the process is not affected and keeps increasing. I can open a new issue with the details and script to reproduce if you think it's not related to this one.\n> I believe I've noticed a similar problem with a bloated DuckDB file after dropping tables (via `DROP` and `CREATE OR REPLACE`). Will the upcoming fix address this situation as well?\r\n\r\nThe upcoming release has support for truncating database files, so data can actually be freed on disk (instead of keeping the blocks labeled as free for future usage). However, it is limited in the sense it does not move around blocks, so if your file looks like this:\r\n\r\n```bash\r\n[HEADER][Table 1][Table 2]\r\n```\r\n\r\nAnd you drop `Table 1`, then no space will be freed (since `Table 2` occupies space later on in the file). We aim to add support for an explicit `VACUUM` in the future that will address this but it will not be part of the next release.\r\n\r\n> Don't know if this is the same or a different issue, but it also happens with :memory: DB. When I delete or recreate a table, memory consumption of the process is not affected and keeps increasing. I can open a new issue with the details and script to reproduce if you think it's not related to this one.\r\n\r\nThat's likely unrelated. \n> > I believe I've noticed a similar problem with a bloated DuckDB file after dropping tables (via `DROP` and `CREATE OR REPLACE`). Will the upcoming fix address this situation as well?\r\n> \r\n> The upcoming release has support for truncating database files, so data can actually be freed on disk (instead of keeping the blocks labeled as free for future usage). However, it is limited in the sense it does not move around blocks, so if your file looks like this:\r\n> \r\n> ```shell\r\n> [HEADER][Table 1][Table 2]\r\n> ```\r\n> \r\n> And you drop `Table 1`, then no space will be freed (since `Table 2` occupies space later on in the file). We aim to add support for an explicit `VACUUM` in the future that will address this but it will not be part of the next release.\r\n> \r\n> > Don't know if this is the same or a different issue, but it also happens with :memory: DB. When I delete or recreate a table, memory consumption of the process is not affected and keeps increasing. I can open a new issue with the details and script to reproduce if you think it's not related to this one.\r\n> \r\n> That's likely unrelated.\r\n\r\n@Mytherin were any improvements made in the most recent release for freeing up space while using a `memory` DB or does it only address database files?\nI have a similar issue where disk space is not freed after dropping all tables.\r\n```\r\n./duckdb db3.db\r\nv0.9.1 401c8061c6\r\nEnter \".help\" for usage hints.\r\nD show tables;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  name   \u2502\r\n\u2502 varchar \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0 rows  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD CREATE TABLE t1 AS SELECT * FROM read_parquet('output-zstd.parquet');\r\n100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f \r\nD PRAGMA database_size;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 database_name \u2502 database_size \u2502 block_size \u2502 total_blocks \u2502 used_blocks \u2502 free_blocks \u2502 wal_size \u2502 memory_usage \u2502 memory_limit \u2502\r\n\u2502    varchar    \u2502    varchar    \u2502   int64    \u2502    int64     \u2502    int64    \u2502    int64    \u2502 varchar  \u2502   varchar    \u2502   varchar    \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 db3           \u2502 1.0GB         \u2502     262144 \u2502         3826 \u2502        3826 \u2502           0 \u2502 0 bytes  \u2502 1.0GB        \u2502 13.3GB       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD DROP TABLE t1;\r\nD show tables;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  name   \u2502\r\n\u2502 varchar \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0 rows  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD PRAGMA database_size;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 database_name \u2502 database_size \u2502 block_size \u2502 total_blocks \u2502 used_blocks \u2502 free_blocks \u2502 wal_size \u2502 memory_usage \u2502 memory_limit \u2502\r\n\u2502    varchar    \u2502    varchar    \u2502   int64    \u2502    int64     \u2502    int64    \u2502    int64    \u2502 varchar  \u2502   varchar    \u2502   varchar    \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 db3           \u2502 1.0GB         \u2502     262144 \u2502         3826 \u2502        3826 \u2502           0 \u2502 22 bytes \u2502 786KB        \u2502 13.3GB       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nWould be nice to have some \"optimize database;\", \"optimize table <table_name>;\", \"vacuum rebuilt ...\", etc. even though it's a slow and blocking operation.\n@pablodcar opened similar.",
  "created_at": "2023-12-08T13:28:19Z"
}