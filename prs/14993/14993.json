{
  "repo": "duckdb/duckdb",
  "pull_number": 14993,
  "instance_id": "duckdb__duckdb-14993",
  "issue_numbers": [
    "14980"
  ],
  "base_commit": "aa2fe677d6b8ec4bb4b35d47b5a521cd8e1a76bf",
  "patch": "diff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp\nindex f5bea5a3abbe..a42dfc6a703e 100644\n--- a/src/function/table/arrow.cpp\n+++ b/src/function/table/arrow.cpp\n@@ -370,6 +370,15 @@ void ArrowTableFunction::PopulateArrowTableType(ArrowTableType &arrow_table, Arr\n \t}\n }\n \n+unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBindDumb(ClientContext &context, TableFunctionBindInput &input,\n+                                                               vector<LogicalType> &return_types,\n+                                                               vector<string> &names) {\n+\tauto bind_data = ArrowScanBind(context, input, return_types, names);\n+\tauto &arrow_bind_data = bind_data->Cast<ArrowScanFunctionData>();\n+\tarrow_bind_data.projection_pushdown_enabled = false;\n+\treturn bind_data;\n+}\n+\n unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &context, TableFunctionBindInput &input,\n                                                            vector<LogicalType> &return_types, vector<string> &names) {\n \tif (input.inputs[0].IsNull() || input.inputs[1].IsNull() || input.inputs[2].IsNull()) {\n@@ -475,7 +484,10 @@ ArrowTableFunction::ArrowScanInitLocalInternal(ClientContext &context, TableFunc\n \tauto result = make_uniq<ArrowScanLocalState>(std::move(current_chunk));\n \tresult->column_ids = input.column_ids;\n \tresult->filters = input.filters.get();\n-\tif (!input.projection_ids.empty()) {\n+\tauto &bind_data = input.bind_data->Cast<ArrowScanFunctionData>();\n+\tif (!bind_data.projection_pushdown_enabled) {\n+\t\tresult->column_ids.clear();\n+\t} else if (!input.projection_ids.empty()) {\n \t\tauto &asgs = global_state_p->Cast<ArrowScanGlobalState>();\n \t\tresult->all_columns.Initialize(context, asgs.scanned_types);\n \t}\n@@ -594,7 +606,7 @@ void ArrowTableFunction::RegisterFunction(BuiltinFunctions &set) {\n \tset.AddFunction(arrow);\n \n \tTableFunction arrow_dumb(\"arrow_scan_dumb\", {LogicalType::POINTER, LogicalType::POINTER, LogicalType::POINTER},\n-\t                         ArrowScanFunction, ArrowScanBind, ArrowScanInitGlobal, ArrowScanInitLocal);\n+\t                         ArrowScanFunction, ArrowScanBindDumb, ArrowScanInitGlobal, ArrowScanInitLocal);\n \tarrow_dumb.cardinality = ArrowScanCardinality;\n \tarrow_dumb.get_partition_data = ArrowGetPartitionData;\n \tarrow_dumb.projection_pushdown = false;\ndiff --git a/src/function/table/arrow_conversion.cpp b/src/function/table/arrow_conversion.cpp\nindex 77ba98746898..8a19c1b7dc37 100644\n--- a/src/function/table/arrow_conversion.cpp\n+++ b/src/function/table/arrow_conversion.cpp\n@@ -1340,7 +1340,7 @@ static void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, Arr\n void ArrowTableFunction::ArrowToDuckDB(ArrowScanLocalState &scan_state, const arrow_column_map_t &arrow_convert_data,\n                                        DataChunk &output, idx_t start, bool arrow_scan_is_projected) {\n \tfor (idx_t idx = 0; idx < output.ColumnCount(); idx++) {\n-\t\tauto col_idx = scan_state.column_ids[idx];\n+\t\tauto col_idx = scan_state.column_ids.empty() ? idx : scan_state.column_ids[idx];\n \n \t\t// If projection was not pushed down into the arrow scanner, but projection pushdown is enabled on the\n \t\t// table function, we need to use original column ids here.\ndiff --git a/src/include/duckdb/function/table/arrow.hpp b/src/include/duckdb/function/table/arrow.hpp\nindex d9930f466a74..4f8f24f3d02a 100644\n--- a/src/include/duckdb/function/table/arrow.hpp\n+++ b/src/include/duckdb/function/table/arrow.hpp\n@@ -65,6 +65,8 @@ struct ArrowScanFunctionData : public TableFunctionData {\n \tshared_ptr<DependencyItem> dependency;\n \t//! Arrow table data\n \tArrowTableType arrow_table;\n+\t//! Whether projection pushdown is enabled on the scan\n+\tbool projection_pushdown_enabled = true;\n };\n \n struct ArrowRunEndEncodingState {\n@@ -184,6 +186,8 @@ struct ArrowTableFunction {\n \t//! Binds an arrow table\n \tstatic unique_ptr<FunctionData> ArrowScanBind(ClientContext &context, TableFunctionBindInput &input,\n \t                                              vector<LogicalType> &return_types, vector<string> &names);\n+\tstatic unique_ptr<FunctionData> ArrowScanBindDumb(ClientContext &context, TableFunctionBindInput &input,\n+\t                                                  vector<LogicalType> &return_types, vector<string> &names);\n \t//! Actual conversion from Arrow to DuckDB\n \tstatic void ArrowToDuckDB(ArrowScanLocalState &scan_state, const arrow_column_map_t &arrow_convert_data,\n \t                          DataChunk &output, idx_t start, bool arrow_scan_is_projected = true);\ndiff --git a/tools/pythonpkg/duckdb_extension_config.cmake b/tools/pythonpkg/duckdb_extension_config.cmake\nindex 38622df8cc84..0c978737d879 100644\n--- a/tools/pythonpkg/duckdb_extension_config.cmake\n+++ b/tools/pythonpkg/duckdb_extension_config.cmake\n@@ -7,7 +7,6 @@\n # CMakeLists.txt file with the `BUILD_PYTHON` variable.\n # TODO: unify this by making setup.py also use this configuration, making this the config for all python builds\n duckdb_extension_load(json)\n-duckdb_extension_load(fts)\n duckdb_extension_load(tpcds)\n duckdb_extension_load(tpch)\n duckdb_extension_load(parquet)\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_projection_pushdown.py b/tools/pythonpkg/tests/fast/arrow/test_projection_pushdown.py\nindex 069c4a10112a..39022ab4d752 100644\n--- a/tools/pythonpkg/tests/fast/arrow/test_projection_pushdown.py\n+++ b/tools/pythonpkg/tests/fast/arrow/test_projection_pushdown.py\n@@ -2,27 +2,33 @@\n import os\n import pytest\n \n-try:\n-    import pyarrow as pa\n-    import pyarrow.dataset as ds\n-\n-    can_run = True\n-except:\n-    can_run = False\n-\n \n class TestArrowProjectionPushdown(object):\n     def test_projection_pushdown_no_filter(self, duckdb_cursor):\n-        if not can_run:\n-            return\n-        duckdb_conn = duckdb.connect()\n-        duckdb_conn.execute(\"CREATE TABLE test (a  INTEGER, b INTEGER, c INTEGER)\")\n-        duckdb_conn.execute(\"INSERT INTO  test VALUES (1,1,1),(10,10,10),(100,10,100),(NULL,NULL,NULL)\")\n-        duck_tbl = duckdb_conn.table(\"test\")\n+        pa = pytest.importorskip(\"pyarrow\")\n+        ds = pytest.importorskip(\"pyarrow.dataset\")\n+\n+        duckdb_cursor.execute(\n+            \"\"\"\n+            CREATE TABLE test (a  INTEGER, b INTEGER, c INTEGER)\n+        \"\"\"\n+        )\n+        duckdb_cursor.execute(\n+            \"\"\"\n+            INSERT INTO test VALUES\n+                (1,2,3),\n+                (10,20,30),\n+                (100,200,300),\n+                (NULL,NULL,NULL)\n+        \"\"\"\n+        )\n+        duck_tbl = duckdb_cursor.table(\"test\")\n         arrow_table = duck_tbl.arrow()\n-        duckdb_conn.register(\"testarrowtable\", arrow_table)\n-        assert duckdb_conn.execute(\"SELECT sum(a) FROM  testarrowtable\").fetchall() == [(111,)]\n+        assert duckdb_cursor.execute(\"SELECT sum(c) FROM arrow_table\").fetchall() == [(333,)]\n+\n+        # RecordBatch does not use projection pushdown, test that this also still works\n+        record_batch = arrow_table.to_batches()[0]\n+        assert duckdb_cursor.execute(\"SELECT sum(c) FROM record_batch\").fetchall() == [(333,)]\n \n         arrow_dataset = ds.dataset(arrow_table)\n-        duckdb_conn.register(\"testarrowdataset\", arrow_dataset)\n-        assert duckdb_conn.execute(\"SELECT sum(a) FROM  testarrowdataset\").fetchall() == [(111,)]\n+        assert duckdb_cursor.execute(\"SELECT sum(c) FROM arrow_dataset\").fetchall() == [(333,)]\n",
  "problem_statement": "NULL pointer dereference scanning from arrow with lists of structs\n### What happens?\r\n\r\nThere's a segfault when trying to select a column from an arrow batch that has two lists and one of them is a list of structs. Maybe there's a simpler explanation, but this is what I've been able to trivially reproduce. I first filed this under https://github.com/marcboeker/go-duckdb/issues/321, but it's not very go specific, and the Python support seems more first-class so this repro seemed more likely to get some attention.\r\n\r\n### To Reproduce\r\n\r\nCreate an arrow batch with lists of structs and then try to scan one of them. Below is an example in Python, but it also happens using the C-API from go. \r\n\r\n```python\r\n# /// script\r\n# requires-python = \">=3.12\"\r\n# dependencies = [\r\n#     \"duckdb\",\r\n#     \"pyarrow\",\r\n# ]\r\n# ///\r\n\r\nimport pyarrow as pa\r\n\r\nschema = pa.schema([\r\n    pa.field(\"m\", pa.struct([\r\n        pa.field(\"array\", pa.list_(\r\n            pa.struct([pa.field(\"a\", pa.int64())])\r\n        ))\r\n    ])),\r\n    pa.field(\"a\", pa.struct([\r\n        pa.field(\"array\", pa.list_(pa.int64()))\r\n    ]))\r\n])\r\nm_a_values = pa.array([1], type=pa.int64())  # Values for \"a\"\r\nm_array = pa.ListArray.from_arrays(\r\n    offsets=pa.array([0, 1], type=pa.int32()),  # One list with one element\r\n    values=pa.StructArray.from_arrays([m_a_values], names=[\"a\"])\r\n)\r\nm = pa.StructArray.from_arrays([m_array], names=[\"array\"])\r\na_array_values = pa.array([1], type=pa.int64())  # Values for the list\r\na_array = pa.ListArray.from_arrays(\r\n    offsets=pa.array([0, 1], type=pa.int32()),  # One list with one element\r\n    values=a_array_values\r\n)\r\na = pa.StructArray.from_arrays([a_array], names=[\"array\"])\r\nrecord_batch = pa.RecordBatch.from_arrays([m, a], schema.names)\r\n\r\nimport duckdb\r\ncon = duckdb.connect()\r\nresults = con.execute(\"SELECT a FROM record_batch\").arrow()\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nduckdb.duckdb.InternalException: INTERNAL Error: Attempted to dereference unique_ptr that is NULL!\r\nThis error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.\r\nFor more information, see https://duckdb.org/docs/dev/internal_errors\r\n```\r\n\r\n### OS:\r\n\r\nlinux x86_64\r\n\r\n### DuckDB Version:\r\n\r\n1.1.3\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nAndrew Werner\r\n\r\n### Affiliation:\r\n\r\nData Ex Machina, Inc\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "I'll add that `select *` works, but many other subselects such as `select a` , `select m`, `select a.*`, `select a.array`, etc lead to the crash. ",
  "created_at": "2024-11-26T14:45:06Z"
}