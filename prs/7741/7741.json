{
  "repo": "duckdb/duckdb",
  "pull_number": 7741,
  "instance_id": "duckdb__duckdb-7741",
  "issue_numbers": [
    "7667"
  ],
  "base_commit": "54a1955565ad8c6c0d9e3633b18a877ad9ed88be",
  "patch": "diff --git a/src/common/types/row/tuple_data_scatter_gather.cpp b/src/common/types/row/tuple_data_scatter_gather.cpp\nindex 3dcecd079d44..b235928c8aaf 100644\n--- a/src/common/types/row/tuple_data_scatter_gather.cpp\n+++ b/src/common/types/row/tuple_data_scatter_gather.cpp\n@@ -62,8 +62,19 @@ inline string_t TupleDataWithinListValueLoad(const data_ptr_t &location, data_pt\n \treturn result;\n }\n \n+static void ResetCombinedListData(vector<TupleDataVectorFormat> &vector_data) {\n+\tfor (auto &vd : vector_data) {\n+\t\tvd.combined_list_data = nullptr;\n+\t\tResetCombinedListData(vd.child_formats);\n+\t}\n+}\n+\n void TupleDataCollection::ComputeHeapSizes(TupleDataChunkState &chunk_state, const DataChunk &new_chunk,\n                                            const SelectionVector &append_sel, const idx_t append_count) {\n+#ifdef DEBUG\n+\tResetCombinedListData(chunk_state.vector_data);\n+#endif\n+\n \tauto heap_sizes = FlatVector::GetData<idx_t>(chunk_state.heap_sizes);\n \tstd::fill_n(heap_sizes, new_chunk.size(), 0);\n \n@@ -293,7 +304,12 @@ static void ApplySliceRecursive(const Vector &source_v, TupleDataVectorFormat &s\n \t\tfor (idx_t struct_col_idx = 0; struct_col_idx < struct_sources.size(); struct_col_idx++) {\n \t\t\tauto &struct_source = *struct_sources[struct_col_idx];\n \t\t\tauto &struct_format = source_format.child_formats[struct_col_idx];\n-\t\t\tstruct_format.combined_list_data = make_uniq<CombinedListData>();\n+#ifdef DEBUG\n+\t\t\tD_ASSERT(!struct_format.combined_list_data);\n+#endif\n+\t\t\tif (!struct_format.combined_list_data) {\n+\t\t\t\tstruct_format.combined_list_data = make_uniq<CombinedListData>();\n+\t\t\t}\n \t\t\tApplySliceRecursive(struct_source, struct_format, *source_format.data.sel, count);\n \t\t}\n \t}\n@@ -308,14 +324,15 @@ void TupleDataCollection::ListWithinListComputeHeapSizes(Vector &heap_sizes_v, c\n \tconst auto list_entries = UnifiedVectorFormat::GetData<list_entry_t>(list_data);\n \tconst auto &list_validity = list_data.validity;\n \n-\t// Child list\n+\t// Child list (\"source_v\")\n \tconst auto &child_list_data = source_format.data;\n \tconst auto child_list_sel = *child_list_data.sel;\n \tconst auto child_list_entries = UnifiedVectorFormat::GetData<list_entry_t>(child_list_data);\n \tconst auto &child_list_validity = child_list_data.validity;\n \n-\t// Figure out actual child list size (differs from ListVector::GetListSize if dict/const vector)\n-\tidx_t child_list_child_count = ListVector::GetListSize(source_v);\n+\t// Figure out actual child list size (can differ from ListVector::GetListSize if dict/const vector),\n+\t// and we cannot use ConstantVector::ZeroSelectionVector because it may need to be longer than STANDARD_VECTOR_SIZE\n+\tidx_t sum_of_sizes = 0;\n \tfor (idx_t i = 0; i < append_count; i++) {\n \t\tconst auto list_idx = list_sel.get_index(append_sel.get_index(i));\n \t\tif (!list_validity.RowIsValid(list_idx)) {\n@@ -324,15 +341,33 @@ void TupleDataCollection::ListWithinListComputeHeapSizes(Vector &heap_sizes_v, c\n \t\tconst auto &list_entry = list_entries[list_idx];\n \t\tconst auto &list_offset = list_entry.offset;\n \t\tconst auto &list_length = list_entry.length;\n-\t\tchild_list_child_count = MaxValue<idx_t>(child_list_child_count, list_offset + list_length);\n+\n+\t\tfor (idx_t child_i = 0; child_i < list_length; child_i++) {\n+\t\t\tconst auto child_list_idx = child_list_sel.get_index(list_offset + child_i);\n+\t\t\tif (!child_list_validity.RowIsValid(child_list_idx)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tconst auto &child_list_entry = child_list_entries[child_list_idx];\n+\t\t\tconst auto &child_list_length = child_list_entry.length;\n+\n+\t\t\tsum_of_sizes += child_list_length;\n+\t\t}\n \t}\n+\tconst auto child_list_child_count = MaxValue<idx_t>(sum_of_sizes, ListVector::GetListSize(source_v));\n \n \t// Target\n \tauto heap_sizes = FlatVector::GetData<idx_t>(heap_sizes_v);\n \n \t// Construct combined list entries and a selection vector for the child list child\n \tauto &child_format = source_format.child_formats[0];\n-\tchild_format.combined_list_data = make_uniq<CombinedListData>();\n+#ifdef DEBUG\n+\t// In debug mode this should be deleted by ResetCombinedListData\n+\tD_ASSERT(!child_format.combined_list_data);\n+#endif\n+\tif (!child_format.combined_list_data) {\n+\t\tchild_format.combined_list_data = make_uniq<CombinedListData>();\n+\t}\n \tauto &combined_list_data = *child_format.combined_list_data;\n \tauto &combined_list_entries = combined_list_data.combined_list_entries;\n \tSelectionVector combined_sel(child_list_child_count);\n@@ -365,7 +400,7 @@ void TupleDataCollection::ListWithinListComputeHeapSizes(Vector &heap_sizes_v, c\n \t\t\t\tconst auto &child_list_offset = child_list_entry.offset;\n \t\t\t\tconst auto &child_list_length = child_list_entry.length;\n \n-\t\t\t\t// Add this child's list entry's to the combined selection vector\n+\t\t\t\t// Add this child's list entries to the combined selection vector\n \t\t\t\tfor (idx_t child_value_i = 0; child_value_i < child_list_length; child_value_i++) {\n \t\t\t\t\tauto idx = combined_list_offset + child_list_size + child_value_i;\n \t\t\t\t\tauto loc = child_list_offset + child_value_i;\n",
  "test_patch": "diff --git a/data/parquet-testing/malloy-smaller/load.sql b/data/parquet-testing/malloy-smaller/load.sql\nnew file mode 100644\nindex 000000000000..d1161b8b4fe7\n--- /dev/null\n+++ b/data/parquet-testing/malloy-smaller/load.sql\n@@ -0,0 +1,1 @@\n+COPY tbl2 FROM 'malloy-small/tbl_.parquet' (FORMAT 'parquet');\ndiff --git a/data/parquet-testing/malloy-smaller/schema.sql b/data/parquet-testing/malloy-smaller/schema.sql\nnew file mode 100644\nindex 000000000000..70313691cdba\n--- /dev/null\n+++ b/data/parquet-testing/malloy-smaller/schema.sql\n@@ -0,0 +1,9 @@\n+CREATE SCHEMA malloytest;\n+\n+\n+\n+CREATE TABLE tbl2(visitorId BIGINT, visitNumber BIGINT, hits STRUCT(hitNumber BIGINT, \"time\" BIGINT, \"hour\" BIGINT, \"minute\" BIGINT, isSecure BOOLEAN, isInteraction BOOLEAN, isEntrance BOOLEAN, isExit BOOLEAN, referer VARCHAR, page STRUCT(pagePath VARCHAR, hostname VARCHAR, pageTitle VARCHAR, searchKeyword VARCHAR, searchCategory VARCHAR, pagePathLevel1 VARCHAR, pagePathLevel2 VARCHAR, pagePathLevel3 VARCHAR, pagePathLevel4 VARCHAR), \"transaction\" STRUCT(transactionId VARCHAR, transactionRevenue BIGINT, transactionTax BIGINT, transactionShipping BIGINT, affiliation VARCHAR, currencyCode VARCHAR, localTransactionRevenue BIGINT, localTransactionTax BIGINT, localTransactionShipping BIGINT, transactionCoupon VARCHAR), item STRUCT(transactionId VARCHAR, productName VARCHAR, productCategory VARCHAR, productSku VARCHAR, itemQuantity BIGINT, itemRevenue BIGINT, currencyCode VARCHAR, localItemRevenue BIGINT), contentInfo STRUCT(contentDescription VARCHAR), appInfo STRUCT(\"name\" VARCHAR, \"version\" VARCHAR, id VARCHAR, installerId VARCHAR, appInstallerId VARCHAR, appName VARCHAR, appVersion VARCHAR, appId VARCHAR, screenName VARCHAR, landingScreenName VARCHAR, exitScreenName VARCHAR, screenDepth VARCHAR), exceptionInfo STRUCT(description VARCHAR, isFatal BOOLEAN, exceptions BIGINT, fatalExceptions BIGINT), eventInfo STRUCT(eventCategory VARCHAR, eventAction VARCHAR, eventLabel VARCHAR, eventValue BIGINT), product STRUCT(productSKU VARCHAR, v2ProductName VARCHAR, v2ProductCategory VARCHAR, productVariant VARCHAR, productBrand VARCHAR, productRevenue BIGINT, localProductRevenue BIGINT, productPrice BIGINT, localProductPrice BIGINT, productQuantity BIGINT, productRefundAmount BIGINT, localProductRefundAmount BIGINT, isImpression BOOLEAN, isClick BOOLEAN, customDimensions STRUCT(\"index\" BIGINT, \"value\" VARCHAR)[], customMetrics STRUCT(\"index\" BIGINT, \"value\" BIGINT)[], productListName VARCHAR, productListPosition BIGINT, productCouponCode VARCHAR)[], promotion STRUCT(promoId VARCHAR, promoName VARCHAR, promoCreative VARCHAR, promoPosition VARCHAR)[], promotionActionInfo STRUCT(promoIsView BOOLEAN, promoIsClick BOOLEAN), refund STRUCT(refundAmount BIGINT, localRefundAmount BIGINT), eCommerceAction STRUCT(action_type VARCHAR, step BIGINT, \"option\" VARCHAR), experiment STRUCT(experimentId VARCHAR, experimentVariant VARCHAR)[], publisher STRUCT(dfpClicks BIGINT, dfpImpressions BIGINT, dfpMatchedQueries BIGINT, dfpMeasurableImpressions BIGINT, dfpQueries BIGINT, dfpRevenueCpm BIGINT, dfpRevenueCpc BIGINT, dfpViewableImpressions BIGINT, dfpPagesViewed BIGINT, adsenseBackfillDfpClicks BIGINT, adsenseBackfillDfpImpressions BIGINT, adsenseBackfillDfpMatchedQueries BIGINT, adsenseBackfillDfpMeasurableImpressions BIGINT, adsenseBackfillDfpQueries BIGINT, adsenseBackfillDfpRevenueCpm BIGINT, adsenseBackfillDfpRevenueCpc BIGINT, adsenseBackfillDfpViewableImpressions BIGINT, adsenseBackfillDfpPagesViewed BIGINT, adxBackfillDfpClicks BIGINT, adxBackfillDfpImpressions BIGINT, adxBackfillDfpMatchedQueries BIGINT, adxBackfillDfpMeasurableImpressions BIGINT, adxBackfillDfpQueries BIGINT, adxBackfillDfpRevenueCpm BIGINT, adxBackfillDfpRevenueCpc BIGINT, adxBackfillDfpViewableImpressions BIGINT, adxBackfillDfpPagesViewed BIGINT, adxClicks BIGINT, adxImpressions BIGINT, adxMatchedQueries BIGINT, adxMeasurableImpressions BIGINT, adxQueries BIGINT, adxRevenue BIGINT, adxViewableImpressions BIGINT, adxPagesViewed BIGINT, adsViewed BIGINT, adsUnitsViewed BIGINT, adsUnitsMatched BIGINT, viewableAdsViewed BIGINT, measurableAdsViewed BIGINT, adsPagesViewed BIGINT, adsClicked BIGINT, adsRevenue BIGINT, dfpAdGroup VARCHAR, dfpAdUnits VARCHAR, dfpNetworkId VARCHAR), customVariables STRUCT(\"index\" BIGINT, customVarName VARCHAR, customVarValue VARCHAR)[], customDimensions STRUCT(\"index\" BIGINT, \"value\" VARCHAR)[], customMetrics STRUCT(\"index\" BIGINT, \"value\" BIGINT)[], \"type\" VARCHAR, social STRUCT(socialInteractionNetwork VARCHAR, socialInteractionAction VARCHAR, socialInteractions BIGINT, socialInteractionTarget VARCHAR, socialNetwork VARCHAR, uniqueSocialInteractions BIGINT, hasSocialSourceReferral VARCHAR, socialInteractionNetworkAction VARCHAR), latencyTracking STRUCT(pageLoadSample BIGINT, pageLoadTime BIGINT, pageDownloadTime BIGINT, redirectionTime BIGINT, speedMetricsSample BIGINT, domainLookupTime BIGINT, serverConnectionTime BIGINT, serverResponseTime BIGINT, domLatencyMetricsSample BIGINT, domInteractiveTime BIGINT, domContentLoadedTime BIGINT, userTimingValue BIGINT, userTimingSample BIGINT, userTimingVariable VARCHAR, userTimingCategory VARCHAR, userTimingLabel VARCHAR), sourcePropertyInfo STRUCT(sourcePropertyDisplayName VARCHAR, sourcePropertyTrackingId VARCHAR), contentGroup STRUCT(contentGroup1 VARCHAR, contentGroup2 VARCHAR, contentGroup3 VARCHAR, contentGroup4 VARCHAR, contentGroup5 VARCHAR, previousContentGroup1 VARCHAR, previousContentGroup2 VARCHAR, previousContentGroup3 VARCHAR, previousContentGroup4 VARCHAR, previousContentGroup5 VARCHAR, contentGroupUniqueViews1 BIGINT, contentGroupUniqueViews2 BIGINT, contentGroupUniqueViews3 BIGINT, contentGroupUniqueViews4 BIGINT, contentGroupUniqueViews5 BIGINT), dataSource VARCHAR, publisher_infos STRUCT(dfpClicks BIGINT, dfpImpressions BIGINT, dfpMatchedQueries BIGINT, dfpMeasurableImpressions BIGINT, dfpQueries BIGINT, dfpRevenueCpm BIGINT, dfpRevenueCpc BIGINT, dfpViewableImpressions BIGINT, dfpPagesViewed BIGINT, adsenseBackfillDfpClicks BIGINT, adsenseBackfillDfpImpressions BIGINT, adsenseBackfillDfpMatchedQueries BIGINT, adsenseBackfillDfpMeasurableImpressions BIGINT, adsenseBackfillDfpQueries BIGINT, adsenseBackfillDfpRevenueCpm BIGINT, adsenseBackfillDfpRevenueCpc BIGINT, adsenseBackfillDfpViewableImpressions BIGINT, adsenseBackfillDfpPagesViewed BIGINT, adxBackfillDfpClicks BIGINT, adxBackfillDfpImpressions BIGINT, adxBackfillDfpMatchedQueries BIGINT, adxBackfillDfpMeasurableImpressions BIGINT, adxBackfillDfpQueries BIGINT, adxBackfillDfpRevenueCpm BIGINT, adxBackfillDfpRevenueCpc BIGINT, adxBackfillDfpViewableImpressions BIGINT, adxBackfillDfpPagesViewed BIGINT, adxClicks BIGINT, adxImpressions BIGINT, adxMatchedQueries BIGINT, adxMeasurableImpressions BIGINT, adxQueries BIGINT, adxRevenue BIGINT, adxViewableImpressions BIGINT, adxPagesViewed BIGINT, adsViewed BIGINT, adsUnitsViewed BIGINT, adsUnitsMatched BIGINT, viewableAdsViewed BIGINT, measurableAdsViewed BIGINT, adsPagesViewed BIGINT, adsClicked BIGINT, adsRevenue BIGINT, dfpAdGroup VARCHAR, dfpAdUnits VARCHAR, dfpNetworkId VARCHAR)[])[], totals STRUCT(visits BIGINT, hits BIGINT, pageviews BIGINT, timeOnSite BIGINT, bounces BIGINT, transactions BIGINT, transactionRevenue BIGINT, newVisits BIGINT, screenviews BIGINT, uniqueScreenviews BIGINT, timeOnScreen BIGINT, totalTransactionRevenue BIGINT, sessionQualityDim BIGINT));\n+\n+\n+\n+\ndiff --git a/data/parquet-testing/malloy-smaller/tbl_.parquet b/data/parquet-testing/malloy-smaller/tbl_.parquet\nnew file mode 100644\nindex 000000000000..a63745f6c5b8\nBinary files /dev/null and b/data/parquet-testing/malloy-smaller/tbl_.parquet differ\ndiff --git a/test/sql/join/test_huge_nested_payloads.test_slow b/test/sql/join/test_huge_nested_payloads.test_slow\nnew file mode 100644\nindex 000000000000..18aabf08d903\n--- /dev/null\n+++ b/test/sql/join/test_huge_nested_payloads.test_slow\n@@ -0,0 +1,23 @@\n+# name: test/sql/join/test_huge_nested_payloads.test_slow\n+# description: Test join with nested types in a HUGE payload\n+# group: [join]\n+\n+require parquet\n+\n+statement ok\n+IMPORT DATABASE 'data/parquet-testing/malloy-smaller'\n+\n+statement ok\n+    SELECT\n+       hits_0.page.\"pageTitle\" as \"pageTitle\",\n+       COUNT(DISTINCT CONCAT(ga_sessions.\"__distinct_key\", 'x', hits_0_outer.__row_id)) as \"hits_count\",\n+       COUNT(DISTINCT CASE WHEN product_0.\"productQuantity\">0\n+       THEN CONCAT(ga_sessions.\"__distinct_key\", 'x', hits_0_outer.__row_id) END) as \"sold_count\"\n+    FROM (SELECT GEN_RANDOM_UUID() as __distinct_key, * FROM tbl2 as x) as ga_sessions\n+    LEFT JOIN LATERAL (SELECT UNNEST(GENERATE_SERIES(1, length(ga_sessions.\"hits\"),1)) as __row_id,\n+        UNNEST(ga_sessions.\"hits\"), 1 as ignoreme) as hits_0_outer(__row_id, hits_0,ignoreme) ON  hits_0_outer.ignoreme=1\n+    LEFT JOIN LATERAL (SELECT UNNEST(hits_0.\"product\"), 1 as ignoreme) as product_0_outer(product_0,ignoreme)\n+        ON product_0_outer.ignoreme=1\n+    WHERE ga_sessions.totals.\"transactionRevenue\">0\n+    GROUP BY 1\n+    ORDER BY 2 desc NULLS LAST;\ndiff --git a/test/sql/join/test_nested_payloads.test b/test/sql/join/test_nested_payloads.test\nindex 5922aba200dd..6a0c0ebe3ca1 100644\n--- a/test/sql/join/test_nested_payloads.test\n+++ b/test/sql/join/test_nested_payloads.test\n@@ -8,6 +8,9 @@ SET default_null_order='nulls_first';\n statement ok\n PRAGMA enable_verification\n \n+statement ok\n+PRAGMA verify_external\n+\n statement ok\n CREATE TABLE integers(fk INTEGER)\n \n@@ -383,13 +386,13 @@ SELECT fk, pk, p FROM integers FULL OUTER JOIN longlists ON integers.fk=longlist\n ----\n 21 values hashing to 397312e04f9a44a70a2672865240afb0\n \n+# duckdb-fuzzer issue #132\n statement ok\n CREATE TABLE all_types(\"varchar\" VARCHAR, nested_int_array INTEGER[][]);;\n \n statement ok\n INSERT INTO all_types VALUES('b',[[], NULL, [], [NULL]]);\n \n-# duckdb-fuzzer issue #132\n query T\n SELECT ref_1.nested_int_array AS c0\n FROM all_types AS ref_1\n@@ -397,3 +400,14 @@ INNER JOIN (SELECT NULL AS c8 FROM range(3)) AS subq_1 ON (ref_1.\"varchar\" = ref\n INNER JOIN range(3) AS ref_4(time_tz) ON (subq_1.c8 = ref_4.time_tz);\n ----\n \n+\n+# duckdb-fuzzer issue #135\n+statement ok\n+CREATE TABLE nested(nested_int_array INTEGER[][]);\n+\n+statement ok\n+INSERT INTO nested VALUES([[42, 999]]);\n+SELECT (\n+\tSELECT ref_1.nested_int_array\n+) AS c0\n+FROM range(3), nested AS ref_1;\n",
  "problem_statement": "Segfault when querying on nested data\n### What happens?\n\nA query from the Malloy \"Nested Source Table\" test suite results in a segfault.\n\n### To Reproduce\n\nRunning the following query:\r\n```\r\n    SELECT\r\n       hits_0.page.\"pageTitle\" as \"pageTitle\",\r\n       COUNT(DISTINCT CONCAT(ga_sessions.\"__distinct_key\", 'x', hits_0_outer.__row_id)) as \"hits_count\",\r\n       COUNT(DISTINCT CASE WHEN product_0.\"productQuantity\">0 THEN CONCAT(ga_sessions.\"__distinct_key\", 'x', hits_0_outer.__row_id) END) as \"sold_count\"\r\n    FROM (SELECT GEN_RANDOM_UUID() as __distinct_key, * FROM malloytest.ga_sample as x) as ga_sessions\r\n    LEFT JOIN LATERAL (SELECT UNNEST(GENERATE_SERIES(1, length(ga_sessions.\"hits\"),1)) as __row_id, UNNEST(ga_sessions.\"hits\"), 1 as ignoreme) as hits_0_outer(__row_id, hits_0,ignoreme) ON  hits_0_outer.ignoreme=1\r\n    LEFT JOIN LATERAL (SELECT UNNEST(hits_0.\"product\"), 1 as ignoreme) as product_0_outer(product_0,ignoreme) ON product_0_outer.ignoreme=1\r\n    WHERE ga_sessions.totals.\"transactionRevenue\">0\r\n    GROUP BY 1\r\n    ORDER BY 2 desc NULLS LAST\r\n```\r\nagainst the database here: https://www.scullinsteel.com/data/duckdb_test.db results in:\r\n```\r\nzsh: segmentation fault  ./duckdb\r\n```\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\n0.8.0\n\n### DuckDB Client:\n\ncli\n\n### Full Name:\n\nWill Scullin\n\n### Affiliation:\n\nGoogle\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "",
  "created_at": "2023-05-30T15:14:18Z"
}