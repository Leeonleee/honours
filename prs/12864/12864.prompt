You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
InternalException: INTERNAL Error: Attempted to dereference unique_ptr that is NULL!
### What happens?

After closing my connection on my Python client, connecting and writing to it causes this error. If I am understanding correctly, this shouldn't happen since I am reopening the connection and therefore it should allow a write.

### To Reproduce

```
import duckdb
schema = {
            "doc_id": "VARCHAR",
            "embeddings": "FLOAT[384]",
            "properties": "MAP(VARCHAR, VARCHAR)",
            "text_representation": "VARCHAR",
            "bbox": "DOUBLE[]",
            "shingles": "BIGINT[]",
            "type": "VARCHAR",
        }
in_memory_db = duckdb.connect(":default:")
in_memory_db.execute(f"""CREATE TABLE in_memory_table (doc_id {schema.get('doc_id')},
                      embeddings {schema.get('embeddings')}, properties {schema.get('properties')}, 
                      text_representation {schema.get('text_representation')}, bbox {schema.get('bbox')}, 
                      shingles {schema.get('shingles')}, type {schema.get('type')})"""
in_memory_db.close()
in_memory_db = duckdb.connect(":default:")
in_memory_db.execute(f"""CREATE TABLE in_memory_table (doc_id {schema.get('doc_id')},
                      embeddings {schema.get('embeddings')}, properties {schema.get('properties')}, 
                      text_representation {schema.get('text_representation')}, bbox {schema.get('bbox')}, 
                      shingles {schema.get('shingles')}, type {schema.get('type')})"""
```

### OS:

arm64

### DuckDB Version:

1.0.0

### DuckDB Client:

Python

### Full Name:

Karan Sampath

### Affiliation:

Aryn AI

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot share the data sets because they are confidential

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/pyconnection/pyconnection.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: #include "duckdb_python/arrow/arrow_array_stream.hpp"
11: #include "duckdb.hpp"
12: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
13: #include "duckdb/common/unordered_map.hpp"
14: #include "duckdb_python/import_cache/python_import_cache.hpp"
15: #include "duckdb_python/numpy/numpy_type.hpp"
16: #include "duckdb_python/pyrelation.hpp"
17: #include "duckdb_python/pytype.hpp"
18: #include "duckdb_python/path_like.hpp"
19: #include "duckdb/execution/operator/csv_scanner/csv_reader_options.hpp"
20: #include "duckdb_python/pyfilesystem.hpp"
21: #include "duckdb_python/pybind11/registered_py_object.hpp"
22: #include "duckdb_python/python_dependency.hpp"
23: #include "duckdb/function/scalar_function.hpp"
24: #include "duckdb_python/pybind11/conversions/exception_handling_enum.hpp"
25: #include "duckdb_python/pybind11/conversions/python_udf_type_enum.hpp"
26: #include "duckdb/common/shared_ptr.hpp"
27: 
28: namespace duckdb {
29: struct BoundParameterData;
30: 
31: enum class PythonEnvironmentType { NORMAL, INTERACTIVE, JUPYTER };
32: 
33: struct DuckDBPyRelation;
34: 
35: class RegisteredArrow : public RegisteredObject {
36: 
37: public:
38: 	RegisteredArrow(unique_ptr<PythonTableArrowArrayStreamFactory> arrow_factory_p, py::object obj_p)
39: 	    : RegisteredObject(std::move(obj_p)), arrow_factory(std::move(arrow_factory_p)) {};
40: 	unique_ptr<PythonTableArrowArrayStreamFactory> arrow_factory;
41: };
42: 
43: struct DuckDBPyConnection : public enable_shared_from_this<DuckDBPyConnection> {
44: public:
45: 	shared_ptr<DuckDB> database;
46: 	unique_ptr<Connection> connection;
47: 	unique_ptr<DuckDBPyRelation> result;
48: 	vector<weak_ptr<DuckDBPyConnection>> cursors;
49: 	unordered_map<string, shared_ptr<Relation>> temporary_views;
50: 	std::mutex py_connection_lock;
51: 	//! MemoryFileSystem used to temporarily store file-like objects for reading
52: 	shared_ptr<ModifiedMemoryFileSystem> internal_object_filesystem;
53: 	case_insensitive_map_t<unique_ptr<ExternalDependency>> registered_functions;
54: 
55: public:
56: 	explicit DuckDBPyConnection() {
57: 	}
58: 	~DuckDBPyConnection();
59: 
60: public:
61: 	static void Initialize(py::handle &m);
62: 	static void Cleanup();
63: 
64: 	shared_ptr<DuckDBPyConnection> Enter();
65: 
66: 	static void Exit(DuckDBPyConnection &self, const py::object &exc_type, const py::object &exc,
67: 	                 const py::object &traceback);
68: 
69: 	static bool DetectAndGetEnvironment();
70: 	static bool IsJupyter();
71: 	static shared_ptr<DuckDBPyConnection> DefaultConnection();
72: 	static PythonImportCache *ImportCache();
73: 	static bool IsInteractive();
74: 
75: 	unique_ptr<DuckDBPyRelation>
76: 	ReadCSV(const py::object &name, const py::object &header = py::none(), const py::object &compression = py::none(),
77: 	        const py::object &sep = py::none(), const py::object &delimiter = py::none(),
78: 	        const py::object &dtype = py::none(), const py::object &na_values = py::none(),
79: 	        const py::object &skiprows = py::none(), const py::object &quotechar = py::none(),
80: 	        const py::object &escapechar = py::none(), const py::object &encoding = py::none(),
81: 	        const py::object &parallel = py::none(), const py::object &date_format = py::none(),
82: 	        const py::object &timestamp_format = py::none(), const py::object &sample_size = py::none(),
83: 	        const py::object &all_varchar = py::none(), const py::object &normalize_names = py::none(),
84: 	        const py::object &filename = py::none(), const py::object &null_padding = py::none(),
85: 	        const py::object &names = py::none());
86: 
87: 	py::list ExtractStatements(const string &query);
88: 
89: 	unique_ptr<DuckDBPyRelation> ReadJSON(
90: 	    const string &name, const Optional<py::object> &columns = py::none(),
91: 	    const Optional<py::object> &sample_size = py::none(), const Optional<py::object> &maximum_depth = py::none(),
92: 	    const Optional<py::str> &records = py::none(), const Optional<py::str> &format = py::none(),
93: 	    const Optional<py::object> &date_format = py::none(), const Optional<py::object> &timestamp_format = py::none(),
94: 	    const Optional<py::object> &compression = py::none(),
95: 	    const Optional<py::object> &maximum_object_size = py::none(),
96: 	    const Optional<py::object> &ignore_errors = py::none(),
97: 	    const Optional<py::object> &convert_strings_to_integers = py::none(),
98: 	    const Optional<py::object> &field_appearance_threshold = py::none(),
99: 	    const Optional<py::object> &map_inference_threshold = py::none(),
100: 	    const Optional<py::object> &maximum_sample_files = py::none(),
101: 	    const Optional<py::object> &filename = py::none(), const Optional<py::object> &hive_partitioning = py::none(),
102: 	    const Optional<py::object> &union_by_name = py::none(), const Optional<py::object> &hive_types = py::none(),
103: 	    const Optional<py::object> &hive_types_autocast = py::none());
104: 
105: 	shared_ptr<DuckDBPyType> MapType(const shared_ptr<DuckDBPyType> &key_type,
106: 	                                 const shared_ptr<DuckDBPyType> &value_type);
107: 	shared_ptr<DuckDBPyType> StructType(const py::object &fields);
108: 	shared_ptr<DuckDBPyType> ListType(const shared_ptr<DuckDBPyType> &type);
109: 	shared_ptr<DuckDBPyType> ArrayType(const shared_ptr<DuckDBPyType> &type, idx_t size);
110: 	shared_ptr<DuckDBPyType> UnionType(const py::object &members);
111: 	shared_ptr<DuckDBPyType> EnumType(const string &name, const shared_ptr<DuckDBPyType> &type,
112: 	                                  const py::list &values_p);
113: 	shared_ptr<DuckDBPyType> DecimalType(int width, int scale);
114: 	shared_ptr<DuckDBPyType> StringType(const string &collation = string());
115: 	shared_ptr<DuckDBPyType> Type(const string &type_str);
116: 
117: 	shared_ptr<DuckDBPyConnection>
118: 	RegisterScalarUDF(const string &name, const py::function &udf, const py::object &arguments = py::none(),
119: 	                  const shared_ptr<DuckDBPyType> &return_type = nullptr, PythonUDFType type = PythonUDFType::NATIVE,
120: 	                  FunctionNullHandling null_handling = FunctionNullHandling::DEFAULT_NULL_HANDLING,
121: 	                  PythonExceptionHandling exception_handling = PythonExceptionHandling::FORWARD_ERROR,
122: 	                  bool side_effects = false);
123: 
124: 	shared_ptr<DuckDBPyConnection> UnregisterUDF(const string &name);
125: 
126: 	shared_ptr<DuckDBPyConnection> ExecuteMany(const py::object &query, py::object params = py::list());
127: 
128: 	void ExecuteImmediately(vector<unique_ptr<SQLStatement>> statements);
129: 	unique_ptr<PreparedStatement> PrepareQuery(unique_ptr<SQLStatement> statement);
130: 	unique_ptr<QueryResult> ExecuteInternal(PreparedStatement &prep, py::object params = py::list());
131: 
132: 	shared_ptr<DuckDBPyConnection> Execute(const py::object &query, py::object params = py::list());
133: 	shared_ptr<DuckDBPyConnection> ExecuteFromString(const string &query);
134: 
135: 	shared_ptr<DuckDBPyConnection> Append(const string &name, const PandasDataFrame &value, bool by_name);
136: 
137: 	shared_ptr<DuckDBPyConnection> RegisterPythonObject(const string &name, const py::object &python_object);
138: 
139: 	void InstallExtension(const string &extension, bool force_install = false);
140: 
141: 	void LoadExtension(const string &extension);
142: 
143: 	unique_ptr<DuckDBPyRelation> RunQuery(const py::object &query, string alias = "", py::object params = py::list());
144: 
145: 	unique_ptr<DuckDBPyRelation> Table(const string &tname);
146: 
147: 	unique_ptr<DuckDBPyRelation> Values(py::object params = py::none());
148: 
149: 	unique_ptr<DuckDBPyRelation> View(const string &vname);
150: 
151: 	unique_ptr<DuckDBPyRelation> TableFunction(const string &fname, py::object params = py::list());
152: 
153: 	unique_ptr<DuckDBPyRelation> FromDF(const PandasDataFrame &value);
154: 
155: 	unique_ptr<DuckDBPyRelation> FromParquet(const string &file_glob, bool binary_as_string, bool file_row_number,
156: 	                                         bool filename, bool hive_partitioning, bool union_by_name,
157: 	                                         const py::object &compression = py::none());
158: 
159: 	unique_ptr<DuckDBPyRelation> FromParquets(const vector<string> &file_globs, bool binary_as_string,
160: 	                                          bool file_row_number, bool filename, bool hive_partitioning,
161: 	                                          bool union_by_name, const py::object &compression = py::none());
162: 
163: 	unique_ptr<DuckDBPyRelation> FromArrow(py::object &arrow_object);
164: 
165: 	unique_ptr<DuckDBPyRelation> FromSubstrait(py::bytes &proto);
166: 
167: 	unique_ptr<DuckDBPyRelation> GetSubstrait(const string &query, bool enable_optimizer = true);
168: 
169: 	unique_ptr<DuckDBPyRelation> GetSubstraitJSON(const string &query, bool enable_optimizer = true);
170: 
171: 	unique_ptr<DuckDBPyRelation> FromSubstraitJSON(const string &json);
172: 
173: 	unordered_set<string> GetTableNames(const string &query);
174: 
175: 	shared_ptr<DuckDBPyConnection> UnregisterPythonObject(const string &name);
176: 
177: 	shared_ptr<DuckDBPyConnection> Begin();
178: 
179: 	shared_ptr<DuckDBPyConnection> Commit();
180: 
181: 	shared_ptr<DuckDBPyConnection> Rollback();
182: 
183: 	shared_ptr<DuckDBPyConnection> Checkpoint();
184: 
185: 	void Close();
186: 
187: 	void Interrupt();
188: 
189: 	ModifiedMemoryFileSystem &GetObjectFileSystem();
190: 
191: 	// cursor() is stupid
192: 	shared_ptr<DuckDBPyConnection> Cursor();
193: 
194: 	Optional<py::list> GetDescription();
195: 
196: 	int GetRowcount();
197: 
198: 	// these should be functions on the result but well
199: 	Optional<py::tuple> FetchOne();
200: 
201: 	py::list FetchMany(idx_t size);
202: 
203: 	py::list FetchAll();
204: 
205: 	py::dict FetchNumpy();
206: 	PandasDataFrame FetchDF(bool date_as_object);
207: 	PandasDataFrame FetchDFChunk(const idx_t vectors_per_chunk = 1, bool date_as_object = false) const;
208: 
209: 	duckdb::pyarrow::Table FetchArrow(idx_t rows_per_batch);
210: 	PolarsDataFrame FetchPolars(idx_t rows_per_batch);
211: 
212: 	py::dict FetchPyTorch();
213: 
214: 	py::dict FetchTF();
215: 
216: 	duckdb::pyarrow::RecordBatchReader FetchRecordBatchReader(const idx_t rows_per_batch) const;
217: 
218: 	static shared_ptr<DuckDBPyConnection> Connect(const py::object &database, bool read_only, const py::dict &config);
219: 
220: 	static vector<Value> TransformPythonParamList(const py::handle &params);
221: 	static case_insensitive_map_t<BoundParameterData> TransformPythonParamDict(const py::dict &params);
222: 
223: 	void RegisterFilesystem(AbstractFileSystem filesystem);
224: 	void UnregisterFilesystem(const py::str &name);
225: 	py::list ListFilesystems();
226: 	bool FileSystemIsRegistered(const string &name);
227: 
228: 	//! Default connection to an in-memory database
229: 	static shared_ptr<DuckDBPyConnection> default_connection;
230: 	//! Caches and provides an interface to get frequently used modules+subtypes
231: 	static shared_ptr<PythonImportCache> import_cache;
232: 
233: 	static bool IsPandasDataframe(const py::object &object);
234: 	static bool IsPolarsDataframe(const py::object &object);
235: 	static bool IsAcceptedArrowObject(const py::object &object);
236: 	static NumpyObjectType IsAcceptedNumpyObject(const py::object &object);
237: 
238: 	static unique_ptr<QueryResult> CompletePendingQuery(PendingQueryResult &pending_query);
239: 
240: private:
241: 	PathLike GetPathLike(const py::object &object);
242: 	unique_lock<std::mutex> AcquireConnectionLock();
243: 	ScalarFunction CreateScalarUDF(const string &name, const py::function &udf, const py::object &parameters,
244: 	                               const shared_ptr<DuckDBPyType> &return_type, bool vectorized,
245: 	                               FunctionNullHandling null_handling, PythonExceptionHandling exception_handling,
246: 	                               bool side_effects);
247: 	void RegisterArrowObject(const py::object &arrow_object, const string &name);
248: 	vector<unique_ptr<SQLStatement>> GetStatements(const py::object &query);
249: 
250: 	static PythonEnvironmentType environment;
251: 	static void DetectEnvironment();
252: };
253: 
254: template <typename T>
255: static bool ModuleIsLoaded() {
256: 	auto dict = pybind11::module_::import("sys").attr("modules");
257: 	return dict.contains(py::str(T::Name));
258: }
259: 
260: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp]
[start of tools/pythonpkg/src/pyconnection.cpp]
1: #include "duckdb_python/pyconnection/pyconnection.hpp"
2: 
3: #include "duckdb/catalog/default/default_types.hpp"
4: #include "duckdb/common/arrow/arrow.hpp"
5: #include "duckdb/common/enums/file_compression_type.hpp"
6: #include "duckdb/common/printer.hpp"
7: #include "duckdb/common/types.hpp"
8: #include "duckdb/common/types/vector.hpp"
9: #include "duckdb/function/table/read_csv.hpp"
10: #include "duckdb/main/client_config.hpp"
11: #include "duckdb/main/client_context.hpp"
12: #include "duckdb/main/config.hpp"
13: #include "duckdb/main/db_instance_cache.hpp"
14: #include "duckdb/main/extension_helper.hpp"
15: #include "duckdb/main/prepared_statement.hpp"
16: #include "duckdb/main/relation/read_csv_relation.hpp"
17: #include "duckdb/main/relation/read_json_relation.hpp"
18: #include "duckdb/main/relation/value_relation.hpp"
19: #include "duckdb/parser/expression/constant_expression.hpp"
20: #include "duckdb/parser/expression/function_expression.hpp"
21: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
22: #include "duckdb/parser/parser.hpp"
23: #include "duckdb/parser/statement/select_statement.hpp"
24: #include "duckdb/parser/tableref/subqueryref.hpp"
25: #include "duckdb/parser/tableref/table_function_ref.hpp"
26: #include "duckdb_python/arrow/arrow_array_stream.hpp"
27: #include "duckdb_python/map.hpp"
28: #include "duckdb_python/pandas/pandas_scan.hpp"
29: #include "duckdb_python/pyrelation.hpp"
30: #include "duckdb_python/pystatement.hpp"
31: #include "duckdb_python/pyresult.hpp"
32: #include "duckdb_python/python_conversion.hpp"
33: #include "duckdb_python/numpy/numpy_type.hpp"
34: #include "duckdb/main/prepared_statement.hpp"
35: #include "duckdb_python/jupyter_progress_bar_display.hpp"
36: #include "duckdb_python/pyfilesystem.hpp"
37: #include "duckdb/main/client_config.hpp"
38: #include "duckdb/function/table/read_csv.hpp"
39: #include "duckdb/common/enums/file_compression_type.hpp"
40: #include "duckdb/catalog/default/default_types.hpp"
41: #include "duckdb/main/relation/value_relation.hpp"
42: #include "duckdb_python/filesystem_object.hpp"
43: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
44: #include "duckdb/function/scalar_function.hpp"
45: #include "duckdb_python/pandas/pandas_scan.hpp"
46: #include "duckdb_python/python_objects.hpp"
47: #include "duckdb/function/function.hpp"
48: #include "duckdb_python/pybind11/conversions/exception_handling_enum.hpp"
49: #include "duckdb/parser/parsed_data/drop_info.hpp"
50: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
51: #include "duckdb/main/pending_query_result.hpp"
52: #include "duckdb/parser/keyword_helper.hpp"
53: #include "duckdb_python/python_replacement_scan.hpp"
54: #include "duckdb/common/shared_ptr.hpp"
55: #include "duckdb/main/materialized_query_result.hpp"
56: #include "duckdb/main/stream_query_result.hpp"
57: #include "duckdb/main/relation/materialized_relation.hpp"
58: #include "duckdb/main/relation/query_relation.hpp"
59: 
60: #include <random>
61: 
62: #include "duckdb/common/printer.hpp"
63: 
64: namespace duckdb {
65: 
66: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::default_connection = nullptr;       // NOLINT: allow global
67: DBInstanceCache instance_cache;                                                        // NOLINT: allow global
68: shared_ptr<PythonImportCache> DuckDBPyConnection::import_cache = nullptr;              // NOLINT: allow global
69: PythonEnvironmentType DuckDBPyConnection::environment = PythonEnvironmentType::NORMAL; // NOLINT: allow global
70: 
71: DuckDBPyConnection::~DuckDBPyConnection() {
72: 	try {
73: 		py::gil_scoped_release gil;
74: 		// Release any structures that do not need to hold the GIL here
75: 		database.reset();
76: 		connection.reset();
77: 		temporary_views.clear();
78: 	} catch (...) { // NOLINT
79: 	}
80: }
81: 
82: void DuckDBPyConnection::DetectEnvironment() {
83: 	// If __main__ does not have a __file__ attribute, we are in interactive mode
84: 	auto main_module = py::module_::import("__main__");
85: 	if (py::hasattr(main_module, "__file__")) {
86: 		return;
87: 	}
88: 	DuckDBPyConnection::environment = PythonEnvironmentType::INTERACTIVE;
89: 	if (!ModuleIsLoaded<IpythonCacheItem>()) {
90: 		return;
91: 	}
92: 
93: 	// Check to see if we are in a Jupyter Notebook
94: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
95: 	auto get_ipython = import_cache_py.IPython.get_ipython();
96: 	if (get_ipython.ptr() == nullptr) {
97: 		// Could either not load the IPython module, or it has no 'get_ipython' attribute
98: 		return;
99: 	}
100: 	auto ipython = get_ipython();
101: 	if (!py::hasattr(ipython, "config")) {
102: 		return;
103: 	}
104: 	py::dict ipython_config = ipython.attr("config");
105: 	if (ipython_config.contains("IPKernelApp")) {
106: 		DuckDBPyConnection::environment = PythonEnvironmentType::JUPYTER;
107: 	}
108: 	return;
109: }
110: 
111: bool DuckDBPyConnection::DetectAndGetEnvironment() {
112: 	DuckDBPyConnection::DetectEnvironment();
113: 	return DuckDBPyConnection::IsInteractive();
114: }
115: 
116: bool DuckDBPyConnection::IsJupyter() {
117: 	return DuckDBPyConnection::environment == PythonEnvironmentType::JUPYTER;
118: }
119: 
120: // NOTE: this function is generated by tools/pythonpkg/scripts/generate_connection_methods.py.
121: // Do not edit this function manually, your changes will be overwritten!
122: 
123: static void InitializeConnectionMethods(py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>> &m) {
124: 	m.def("cursor", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection");
125: 	m.def("register_filesystem", &DuckDBPyConnection::RegisterFilesystem, "Register a fsspec compliant filesystem",
126: 	      py::arg("filesystem"));
127: 	m.def("unregister_filesystem", &DuckDBPyConnection::UnregisterFilesystem, "Unregister a filesystem",
128: 	      py::arg("name"));
129: 	m.def("list_filesystems", &DuckDBPyConnection::ListFilesystems,
130: 	      "List registered filesystems, including builtin ones");
131: 	m.def("filesystem_is_registered", &DuckDBPyConnection::FileSystemIsRegistered,
132: 	      "Check if a filesystem with the provided name is currently registered", py::arg("name"));
133: 	m.def("create_function", &DuckDBPyConnection::RegisterScalarUDF,
134: 	      "Create a DuckDB function out of the passing in Python function so it can be used in queries",
135: 	      py::arg("name"), py::arg("function"), py::arg("parameters") = py::none(), py::arg("return_type") = py::none(),
136: 	      py::kw_only(), py::arg("type") = PythonUDFType::NATIVE,
137: 	      py::arg("null_handling") = FunctionNullHandling::DEFAULT_NULL_HANDLING,
138: 	      py::arg("exception_handling") = PythonExceptionHandling::FORWARD_ERROR, py::arg("side_effects") = false);
139: 	m.def("remove_function", &DuckDBPyConnection::UnregisterUDF, "Remove a previously created function",
140: 	      py::arg("name"));
141: 	m.def("sqltype", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
142: 	      py::arg("type_str"));
143: 	m.def("dtype", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
144: 	      py::arg("type_str"));
145: 	m.def("type", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
146: 	      py::arg("type_str"));
147: 	m.def("array_type", &DuckDBPyConnection::ArrayType, "Create an array type object of 'type'",
148: 	      py::arg("type").none(false), py::arg("size"));
149: 	m.def("list_type", &DuckDBPyConnection::ListType, "Create a list type object of 'type'",
150: 	      py::arg("type").none(false));
151: 	m.def("union_type", &DuckDBPyConnection::UnionType, "Create a union type object from 'members'",
152: 	      py::arg("members").none(false));
153: 	m.def("string_type", &DuckDBPyConnection::StringType, "Create a string type with an optional collation",
154: 	      py::arg("collation") = "");
155: 	m.def("enum_type", &DuckDBPyConnection::EnumType,
156: 	      "Create an enum type of underlying 'type', consisting of the list of 'values'", py::arg("name"),
157: 	      py::arg("type"), py::arg("values"));
158: 	m.def("decimal_type", &DuckDBPyConnection::DecimalType, "Create a decimal type with 'width' and 'scale'",
159: 	      py::arg("width"), py::arg("scale"));
160: 	m.def("struct_type", &DuckDBPyConnection::StructType, "Create a struct type object from 'fields'",
161: 	      py::arg("fields"));
162: 	m.def("row_type", &DuckDBPyConnection::StructType, "Create a struct type object from 'fields'", py::arg("fields"));
163: 	m.def("map_type", &DuckDBPyConnection::MapType, "Create a map type object from 'key_type' and 'value_type'",
164: 	      py::arg("key").none(false), py::arg("value").none(false));
165: 	m.def("duplicate", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection");
166: 	m.def("execute", &DuckDBPyConnection::Execute,
167: 	      "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
168: 	      py::arg("parameters") = py::none());
169: 	m.def("executemany", &DuckDBPyConnection::ExecuteMany,
170: 	      "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
171: 	      py::arg("query"), py::arg("parameters") = py::none());
172: 	m.def("close", &DuckDBPyConnection::Close, "Close the connection");
173: 	m.def("interrupt", &DuckDBPyConnection::Interrupt, "Interrupt pending operations");
174: 	m.def("fetchone", &DuckDBPyConnection::FetchOne, "Fetch a single row from a result following execute");
175: 	m.def("fetchmany", &DuckDBPyConnection::FetchMany, "Fetch the next set of rows from a result following execute",
176: 	      py::arg("size") = 1);
177: 	m.def("fetchall", &DuckDBPyConnection::FetchAll, "Fetch all rows from a result following execute");
178: 	m.def("fetchnumpy", &DuckDBPyConnection::FetchNumpy, "Fetch a result as list of NumPy arrays following execute");
179: 	m.def("fetchdf", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
180: 	      py::arg("date_as_object") = false);
181: 	m.def("fetch_df", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
182: 	      py::arg("date_as_object") = false);
183: 	m.def("df", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
184: 	      py::arg("date_as_object") = false);
185: 	m.def("fetch_df_chunk", &DuckDBPyConnection::FetchDFChunk,
186: 	      "Fetch a chunk of the result as DataFrame following execute()", py::arg("vectors_per_chunk") = 1,
187: 	      py::kw_only(), py::arg("date_as_object") = false);
188: 	m.def("pl", &DuckDBPyConnection::FetchPolars, "Fetch a result as Polars DataFrame following execute()",
189: 	      py::arg("rows_per_batch") = 1000000);
190: 	m.def("fetch_arrow_table", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
191: 	      py::arg("rows_per_batch") = 1000000);
192: 	m.def("arrow", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
193: 	      py::arg("rows_per_batch") = 1000000);
194: 	m.def("fetch_record_batch", &DuckDBPyConnection::FetchRecordBatchReader,
195: 	      "Fetch an Arrow RecordBatchReader following execute()", py::arg("rows_per_batch") = 1000000);
196: 	m.def("torch", &DuckDBPyConnection::FetchPyTorch, "Fetch a result as dict of PyTorch Tensors following execute()");
197: 	m.def("tf", &DuckDBPyConnection::FetchTF, "Fetch a result as dict of TensorFlow Tensors following execute()");
198: 	m.def("begin", &DuckDBPyConnection::Begin, "Start a new transaction");
199: 	m.def("commit", &DuckDBPyConnection::Commit, "Commit changes performed within a transaction");
200: 	m.def("rollback", &DuckDBPyConnection::Rollback, "Roll back changes performed within a transaction");
201: 	m.def("checkpoint", &DuckDBPyConnection::Checkpoint,
202: 	      "Synchronizes data in the write-ahead log (WAL) to the database data file (no-op for in-memory connections)");
203: 	m.def("append", &DuckDBPyConnection::Append, "Append the passed DataFrame to the named table",
204: 	      py::arg("table_name"), py::arg("df"), py::kw_only(), py::arg("by_name") = false);
205: 	m.def("register", &DuckDBPyConnection::RegisterPythonObject,
206: 	      "Register the passed Python Object value for querying with a view", py::arg("view_name"),
207: 	      py::arg("python_object"));
208: 	m.def("unregister", &DuckDBPyConnection::UnregisterPythonObject, "Unregister the view name", py::arg("view_name"));
209: 	m.def("table", &DuckDBPyConnection::Table, "Create a relation object for the named table", py::arg("table_name"));
210: 	m.def("view", &DuckDBPyConnection::View, "Create a relation object for the named view", py::arg("view_name"));
211: 	m.def("values", &DuckDBPyConnection::Values, "Create a relation object from the passed values", py::arg("values"));
212: 	m.def("table_function", &DuckDBPyConnection::TableFunction,
213: 	      "Create a relation object from the named table function with given parameters", py::arg("name"),
214: 	      py::arg("parameters") = py::none());
215: 	m.def("read_json", &DuckDBPyConnection::ReadJSON, "Create a relation object from the JSON file in 'name'",
216: 	      py::arg("name"), py::kw_only(), py::arg("columns") = py::none(), py::arg("sample_size") = py::none(),
217: 	      py::arg("maximum_depth") = py::none(), py::arg("records") = py::none(), py::arg("format") = py::none(),
218: 	      py::arg("date_format") = py::none(), py::arg("timestamp_format") = py::none(),
219: 	      py::arg("compression") = py::none(), py::arg("maximum_object_size") = py::none(),
220: 	      py::arg("ignore_errors") = py::none(), py::arg("convert_strings_to_integers") = py::none(),
221: 	      py::arg("field_appearance_threshold") = py::none(), py::arg("map_inference_threshold") = py::none(),
222: 	      py::arg("maximum_sample_files") = py::none(), py::arg("filename") = py::none(),
223: 	      py::arg("hive_partitioning") = py::none(), py::arg("union_by_name") = py::none(),
224: 	      py::arg("hive_types") = py::none(), py::arg("hive_types_autocast") = py::none());
225: 	m.def("extract_statements", &DuckDBPyConnection::ExtractStatements,
226: 	      "Parse the query string and extract the Statement object(s) produced", py::arg("query"));
227: 	m.def("sql", &DuckDBPyConnection::RunQuery,
228: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
229: 	      "run the query as-is.",
230: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
231: 	m.def("query", &DuckDBPyConnection::RunQuery,
232: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
233: 	      "run the query as-is.",
234: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
235: 	m.def("from_query", &DuckDBPyConnection::RunQuery,
236: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
237: 	      "run the query as-is.",
238: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
239: 	m.def("read_csv", &DuckDBPyConnection::ReadCSV, "Create a relation object from the CSV file in 'name'",
240: 	      py::arg("path_or_buffer"), py::kw_only(), py::arg("header") = py::none(), py::arg("compression") = py::none(),
241: 	      py::arg("sep") = py::none(), py::arg("delimiter") = py::none(), py::arg("dtype") = py::none(),
242: 	      py::arg("na_values") = py::none(), py::arg("skiprows") = py::none(), py::arg("quotechar") = py::none(),
243: 	      py::arg("escapechar") = py::none(), py::arg("encoding") = py::none(), py::arg("parallel") = py::none(),
244: 	      py::arg("date_format") = py::none(), py::arg("timestamp_format") = py::none(),
245: 	      py::arg("sample_size") = py::none(), py::arg("all_varchar") = py::none(),
246: 	      py::arg("normalize_names") = py::none(), py::arg("filename") = py::none(),
247: 	      py::arg("null_padding") = py::none(), py::arg("names") = py::none());
248: 	m.def("from_csv_auto", &DuckDBPyConnection::ReadCSV, "Create a relation object from the CSV file in 'name'",
249: 	      py::arg("path_or_buffer"), py::kw_only(), py::arg("header") = py::none(), py::arg("compression") = py::none(),
250: 	      py::arg("sep") = py::none(), py::arg("delimiter") = py::none(), py::arg("dtype") = py::none(),
251: 	      py::arg("na_values") = py::none(), py::arg("skiprows") = py::none(), py::arg("quotechar") = py::none(),
252: 	      py::arg("escapechar") = py::none(), py::arg("encoding") = py::none(), py::arg("parallel") = py::none(),
253: 	      py::arg("date_format") = py::none(), py::arg("timestamp_format") = py::none(),
254: 	      py::arg("sample_size") = py::none(), py::arg("all_varchar") = py::none(),
255: 	      py::arg("normalize_names") = py::none(), py::arg("filename") = py::none(),
256: 	      py::arg("null_padding") = py::none(), py::arg("names") = py::none());
257: 	m.def("from_df", &DuckDBPyConnection::FromDF, "Create a relation object from the DataFrame in df", py::arg("df"));
258: 	m.def("from_arrow", &DuckDBPyConnection::FromArrow, "Create a relation object from an Arrow object",
259: 	      py::arg("arrow_object"));
260: 	m.def("from_parquet", &DuckDBPyConnection::FromParquet,
261: 	      "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
262: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
263: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
264: 	      py::arg("compression") = py::none());
265: 	m.def("read_parquet", &DuckDBPyConnection::FromParquet,
266: 	      "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
267: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
268: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
269: 	      py::arg("compression") = py::none());
270: 	m.def("from_parquet", &DuckDBPyConnection::FromParquets,
271: 	      "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
272: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
273: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
274: 	      py::arg("compression") = py::none());
275: 	m.def("read_parquet", &DuckDBPyConnection::FromParquets,
276: 	      "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
277: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
278: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
279: 	      py::arg("compression") = py::none());
280: 	m.def("from_substrait", &DuckDBPyConnection::FromSubstrait, "Create a query object from protobuf plan",
281: 	      py::arg("proto"));
282: 	m.def("get_substrait", &DuckDBPyConnection::GetSubstrait, "Serialize a query to protobuf", py::arg("query"),
283: 	      py::kw_only(), py::arg("enable_optimizer") = true);
284: 	m.def("get_substrait_json", &DuckDBPyConnection::GetSubstraitJSON,
285: 	      "Serialize a query to protobuf on the JSON format", py::arg("query"), py::kw_only(),
286: 	      py::arg("enable_optimizer") = true);
287: 	m.def("from_substrait_json", &DuckDBPyConnection::FromSubstraitJSON,
288: 	      "Create a query object from a JSON protobuf plan", py::arg("json"));
289: 	m.def("get_table_names", &DuckDBPyConnection::GetTableNames, "Extract the required table names from a query",
290: 	      py::arg("query"));
291: 	m.def("install_extension", &DuckDBPyConnection::InstallExtension, "Install an extension by name",
292: 	      py::arg("extension"), py::kw_only(), py::arg("force_install") = false);
293: 	m.def("load_extension", &DuckDBPyConnection::LoadExtension, "Load an installed extension", py::arg("extension"));
294: } // END_OF_CONNECTION_METHODS
295: 
296: void DuckDBPyConnection::UnregisterFilesystem(const py::str &name) {
297: 	auto &fs = database->GetFileSystem();
298: 
299: 	fs.UnregisterSubSystem(name);
300: }
301: 
302: void DuckDBPyConnection::RegisterFilesystem(AbstractFileSystem filesystem) {
303: 	PythonGILWrapper gil_wrapper;
304: 
305: 	if (!py::isinstance<AbstractFileSystem>(filesystem)) {
306: 		throw InvalidInputException("Bad filesystem instance");
307: 	}
308: 
309: 	auto &fs = database->GetFileSystem();
310: 
311: 	auto protocol = filesystem.attr("protocol");
312: 	if (protocol.is_none() || py::str("abstract").equal(protocol)) {
313: 		throw InvalidInputException("Must provide concrete fsspec implementation");
314: 	}
315: 
316: 	vector<string> protocols;
317: 	if (py::isinstance<py::str>(protocol)) {
318: 		protocols.push_back(py::str(protocol));
319: 	} else {
320: 		for (const auto &sub_protocol : protocol) {
321: 			protocols.push_back(py::str(sub_protocol));
322: 		}
323: 	}
324: 
325: 	fs.RegisterSubSystem(make_uniq<PythonFilesystem>(std::move(protocols), std::move(filesystem)));
326: }
327: 
328: py::list DuckDBPyConnection::ListFilesystems() {
329: 	auto subsystems = database->GetFileSystem().ListSubSystems();
330: 	py::list names;
331: 	for (auto &name : subsystems) {
332: 		names.append(py::str(name));
333: 	}
334: 	return names;
335: }
336: 
337: py::list DuckDBPyConnection::ExtractStatements(const string &query) {
338: 	if (!connection) {
339: 		throw ConnectionException("Connection already closed!");
340: 	}
341: 	py::list result;
342: 	auto statements = connection->ExtractStatements(query);
343: 	for (auto &statement : statements) {
344: 		result.append(make_uniq<DuckDBPyStatement>(std::move(statement)));
345: 	}
346: 	return result;
347: }
348: 
349: bool DuckDBPyConnection::FileSystemIsRegistered(const string &name) {
350: 	auto subsystems = database->GetFileSystem().ListSubSystems();
351: 	return std::find(subsystems.begin(), subsystems.end(), name) != subsystems.end();
352: }
353: 
354: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::UnregisterUDF(const string &name) {
355: 	if (!connection) {
356: 		throw ConnectionException("Connection already closed!");
357: 	}
358: 	auto entry = registered_functions.find(name);
359: 	if (entry == registered_functions.end()) {
360: 		// Not registered or already unregistered
361: 		throw InvalidInputException("No function by the name of '%s' was found in the list of registered functions",
362: 		                            name);
363: 	}
364: 
365: 	auto &context = *connection->context;
366: 
367: 	context.RunFunctionInTransaction([&]() {
368: 		// create function
369: 		auto &catalog = Catalog::GetCatalog(context, SYSTEM_CATALOG);
370: 		DropInfo info;
371: 		info.type = CatalogType::SCALAR_FUNCTION_ENTRY;
372: 		info.name = name;
373: 		info.allow_drop_internal = true;
374: 		info.cascade = false;
375: 		info.if_not_found = OnEntryNotFound::THROW_EXCEPTION;
376: 		catalog.DropEntry(context, info);
377: 	});
378: 	registered_functions.erase(entry);
379: 
380: 	return shared_from_this();
381: }
382: 
383: shared_ptr<DuckDBPyConnection>
384: DuckDBPyConnection::RegisterScalarUDF(const string &name, const py::function &udf, const py::object &parameters_p,
385:                                       const shared_ptr<DuckDBPyType> &return_type_p, PythonUDFType type,
386:                                       FunctionNullHandling null_handling, PythonExceptionHandling exception_handling,
387:                                       bool side_effects) {
388: 	if (!connection) {
389: 		throw ConnectionException("Connection already closed!");
390: 	}
391: 	auto &context = *connection->context;
392: 
393: 	if (context.transaction.HasActiveTransaction()) {
394: 		throw InvalidInputException(
395: 		    "This function can not be called with an active transaction!, commit or abort the existing one first");
396: 	}
397: 	if (registered_functions.find(name) != registered_functions.end()) {
398: 		throw NotImplementedException("A function by the name of '%s' is already created, creating multiple "
399: 		                              "functions with the same name is not supported yet, please remove it first",
400: 		                              name);
401: 	}
402: 	auto scalar_function = CreateScalarUDF(name, udf, parameters_p, return_type_p, type == PythonUDFType::ARROW,
403: 	                                       null_handling, exception_handling, side_effects);
404: 	CreateScalarFunctionInfo info(scalar_function);
405: 
406: 	context.RegisterFunction(info);
407: 
408: 	auto dependency = make_uniq<ExternalDependency>();
409: 	dependency->AddDependency("function", PythonDependencyItem::Create(udf));
410: 	registered_functions[name] = std::move(dependency);
411: 
412: 	return shared_from_this();
413: }
414: 
415: void DuckDBPyConnection::Initialize(py::handle &m) {
416: 	auto connection_module =
417: 	    py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection", py::module_local());
418: 
419: 	connection_module.def("__enter__", &DuckDBPyConnection::Enter)
420: 	    .def("__exit__", &DuckDBPyConnection::Exit, py::arg("exc_type"), py::arg("exc"), py::arg("traceback"));
421: 	connection_module.def("__del__", &DuckDBPyConnection::Close);
422: 
423: 	InitializeConnectionMethods(connection_module);
424: 	connection_module.def_property_readonly("description", &DuckDBPyConnection::GetDescription,
425: 	                                        "Get result set attributes, mainly column names");
426: 	connection_module.def_property_readonly("rowcount", &DuckDBPyConnection::GetRowcount, "Get result set row count");
427: 	PyDateTime_IMPORT; // NOLINT
428: 	DuckDBPyConnection::ImportCache();
429: }
430: 
431: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::ExecuteMany(const py::object &query, py::object params_p) {
432: 	result.reset();
433: 	if (params_p.is_none()) {
434: 		params_p = py::list();
435: 	}
436: 
437: 	auto statements = GetStatements(query);
438: 	if (statements.empty()) {
439: 		// TODO: should we throw?
440: 		return nullptr;
441: 	}
442: 
443: 	auto last_statement = std::move(statements.back());
444: 	statements.pop_back();
445: 	// First immediately execute any preceding statements (if any)
446: 	// FIXME: DBAPI says to not accept an 'executemany' call with multiple statements
447: 	ExecuteImmediately(std::move(statements));
448: 
449: 	auto prep = PrepareQuery(std::move(last_statement));
450: 
451: 	if (!py::is_list_like(params_p)) {
452: 		throw InvalidInputException("executemany requires a list of parameter sets to be provided");
453: 	}
454: 	auto outer_list = py::list(params_p);
455: 	if (outer_list.empty()) {
456: 		throw InvalidInputException("executemany requires a non-empty list of parameter sets to be provided");
457: 	}
458: 
459: 	unique_ptr<QueryResult> query_result;
460: 	// Execute once for every set of parameters that are provided
461: 	for (auto &parameters : outer_list) {
462: 		auto params = py::reinterpret_borrow<py::object>(parameters);
463: 		query_result = ExecuteInternal(*prep, std::move(params));
464: 	}
465: 	// Set the internal 'result' object
466: 	if (query_result) {
467: 		auto py_result = make_uniq<DuckDBPyResult>(std::move(query_result));
468: 		result = make_uniq<DuckDBPyRelation>(std::move(py_result));
469: 	}
470: 
471: 	return shared_from_this();
472: }
473: 
474: unique_ptr<QueryResult> DuckDBPyConnection::CompletePendingQuery(PendingQueryResult &pending_query) {
475: 	PendingExecutionResult execution_result;
476: 	while (!PendingQueryResult::IsResultReady(execution_result = pending_query.ExecuteTask())) {
477: 		{
478: 			py::gil_scoped_acquire gil;
479: 			if (PyErr_CheckSignals() != 0) {
480: 				throw std::runtime_error("Query interrupted");
481: 			}
482: 		}
483: 		if (execution_result == PendingExecutionResult::BLOCKED) {
484: 			pending_query.WaitForTask();
485: 		}
486: 	}
487: 	if (execution_result == PendingExecutionResult::EXECUTION_ERROR) {
488: 		pending_query.ThrowError();
489: 	}
490: 	return pending_query.Execute();
491: }
492: 
493: py::list TransformNamedParameters(const case_insensitive_map_t<idx_t> &named_param_map, const py::dict &params) {
494: 	py::list new_params(params.size());
495: 
496: 	for (auto &item : params) {
497: 		const std::string &item_name = item.first.cast<std::string>();
498: 		auto entry = named_param_map.find(item_name);
499: 		if (entry == named_param_map.end()) {
500: 			throw InvalidInputException(
501: 			    "Named parameters could not be transformed, because query string is missing named parameter '%s'",
502: 			    item_name);
503: 		}
504: 		auto param_idx = entry->second;
505: 		// Add the value of the named parameter to the list
506: 		new_params[param_idx - 1] = item.second;
507: 	}
508: 
509: 	if (named_param_map.size() != params.size()) {
510: 		// One or more named parameters were expected, but not found
511: 		vector<string> missing_params;
512: 		missing_params.reserve(named_param_map.size());
513: 		for (auto &entry : named_param_map) {
514: 			auto &name = entry.first;
515: 			if (!params.contains(name)) {
516: 				missing_params.push_back(name);
517: 			}
518: 		}
519: 		auto message = StringUtil::Join(missing_params, ", ");
520: 		throw InvalidInputException("Not all named parameters have been located, missing: %s", message);
521: 	}
522: 
523: 	return new_params;
524: }
525: 
526: case_insensitive_map_t<BoundParameterData> TransformPreparedParameters(PreparedStatement &prep,
527:                                                                        const py::object &params) {
528: 	case_insensitive_map_t<BoundParameterData> named_values;
529: 	if (py::is_list_like(params)) {
530: 		if (prep.n_param != py::len(params)) {
531: 			throw InvalidInputException("Prepared statement needs %d parameters, %d given", prep.n_param,
532: 			                            py::len(params));
533: 		}
534: 		auto unnamed_values = DuckDBPyConnection::TransformPythonParamList(params);
535: 		for (idx_t i = 0; i < unnamed_values.size(); i++) {
536: 			auto &value = unnamed_values[i];
537: 			auto identifier = std::to_string(i + 1);
538: 			named_values[identifier] = BoundParameterData(std::move(value));
539: 		}
540: 	} else if (py::is_dict_like(params)) {
541: 		auto dict = py::cast<py::dict>(params);
542: 		named_values = DuckDBPyConnection::TransformPythonParamDict(dict);
543: 	} else {
544: 		throw InvalidInputException("Prepared parameters can only be passed as a list or a dictionary");
545: 	}
546: 	return named_values;
547: }
548: 
549: unique_ptr<PreparedStatement> DuckDBPyConnection::PrepareQuery(unique_ptr<SQLStatement> statement) {
550: 	unique_ptr<PreparedStatement> prep;
551: 	{
552: 		py::gil_scoped_release release;
553: 		unique_lock<mutex> lock(py_connection_lock);
554: 
555: 		prep = connection->Prepare(std::move(statement));
556: 		if (prep->HasError()) {
557: 			prep->error.Throw();
558: 		}
559: 	}
560: 	return prep;
561: }
562: 
563: unique_ptr<QueryResult> DuckDBPyConnection::ExecuteInternal(PreparedStatement &prep, py::object params) {
564: 	if (!connection) {
565: 		throw ConnectionException("Connection has already been closed");
566: 	}
567: 	if (params.is_none()) {
568: 		params = py::list();
569: 	}
570: 
571: 	// Execute the prepared statement with the prepared parameters
572: 	auto named_values = TransformPreparedParameters(prep, params);
573: 	unique_ptr<QueryResult> res;
574: 	{
575: 		py::gil_scoped_release release;
576: 		unique_lock<std::mutex> lock(py_connection_lock);
577: 
578: 		auto pending_query = prep.PendingQuery(named_values);
579: 		res = CompletePendingQuery(*pending_query);
580: 
581: 		if (res->HasError()) {
582: 			res->ThrowError();
583: 		}
584: 	}
585: 	return res;
586: }
587: 
588: vector<unique_ptr<SQLStatement>> DuckDBPyConnection::GetStatements(const py::object &query) {
589: 	vector<unique_ptr<SQLStatement>> result;
590: 	if (!connection) {
591: 		throw ConnectionException("Connection has already been closed");
592: 	}
593: 
594: 	shared_ptr<DuckDBPyStatement> statement_obj;
595: 	if (py::try_cast(query, statement_obj)) {
596: 		result.push_back(statement_obj->GetStatement());
597: 		return result;
598: 	}
599: 	if (py::isinstance<py::str>(query)) {
600: 		auto sql_query = std::string(py::str(query));
601: 		return connection->ExtractStatements(sql_query);
602: 	}
603: 	throw InvalidInputException("Please provide either a DuckDBPyStatement or a string representing the query");
604: }
605: 
606: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::ExecuteFromString(const string &query) {
607: 	return Execute(py::str(query));
608: }
609: 
610: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Execute(const py::object &query, py::object params) {
611: 	result.reset();
612: 
613: 	auto statements = GetStatements(query);
614: 	if (statements.empty()) {
615: 		// TODO: should we throw?
616: 		return nullptr;
617: 	}
618: 
619: 	auto last_statement = std::move(statements.back());
620: 	statements.pop_back();
621: 	// First immediately execute any preceding statements (if any)
622: 	// FIXME: SQLites implementation says to not accept an 'execute' call with multiple statements
623: 	ExecuteImmediately(std::move(statements));
624: 
625: 	auto prep = PrepareQuery(std::move(last_statement));
626: 	auto res = ExecuteInternal(*prep, std::move(params));
627: 
628: 	// Set the internal 'result' object
629: 	if (res) {
630: 		auto py_result = make_uniq<DuckDBPyResult>(std::move(res));
631: 		result = make_uniq<DuckDBPyRelation>(std::move(py_result));
632: 	}
633: 	return shared_from_this();
634: }
635: 
636: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Append(const string &name, const PandasDataFrame &value,
637:                                                           bool by_name) {
638: 	RegisterPythonObject("__append_df", value);
639: 	string columns = "";
640: 	if (by_name) {
641: 		auto df_columns = value.attr("columns");
642: 		vector<string> column_names;
643: 		for (auto &column : df_columns) {
644: 			column_names.push_back(std::string(py::str(column)));
645: 		}
646: 		columns += "(";
647: 		for (idx_t i = 0; i < column_names.size(); i++) {
648: 			auto &column = column_names[i];
649: 			if (i != 0) {
650: 				columns += ", ";
651: 			}
652: 			columns += StringUtil::Format("%s", SQLIdentifier(column));
653: 		}
654: 		columns += ")";
655: 	}
656: 
657: 	auto sql_query = StringUtil::Format("INSERT INTO %s %s SELECT * FROM __append_df", SQLIdentifier(name), columns);
658: 	return Execute(py::str(sql_query));
659: }
660: 
661: void DuckDBPyConnection::RegisterArrowObject(const py::object &arrow_object, const string &name) {
662: 	auto stream_factory =
663: 	    make_uniq<PythonTableArrowArrayStreamFactory>(arrow_object.ptr(), connection->context->GetClientProperties());
664: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
665: 	auto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;
666: 	{
667: 		py::gil_scoped_release release;
668: 		temporary_views[name] =
669: 		    connection
670: 		        ->TableFunction("arrow_scan", {Value::POINTER(CastPointerToValue(stream_factory.get())),
671: 		                                       Value::POINTER(CastPointerToValue(stream_factory_produce)),
672: 		                                       Value::POINTER(CastPointerToValue(stream_factory_get_schema))})
673: 		        ->CreateView(name, true, true);
674: 	}
675: 	vector<shared_ptr<ExternalDependency>> dependencies;
676: 	auto dependency = make_shared_ptr<ExternalDependency>();
677: 	auto dependency_item =
678: 	    PythonDependencyItem::Create(make_uniq<RegisteredArrow>(std::move(stream_factory), arrow_object));
679: 	dependency->AddDependency("object", std::move(dependency_item));
680: 	dependencies.push_back(std::move(dependency));
681: 	connection->context->external_dependencies[name] = std::move(dependencies);
682: }
683: 
684: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::RegisterPythonObject(const string &name,
685:                                                                         const py::object &python_object) {
686: 	if (!connection) {
687: 		throw ConnectionException("Connection has already been closed");
688: 	}
689: 
690: 	if (DuckDBPyConnection::IsPandasDataframe(python_object)) {
691: 		if (PandasDataFrame::IsPyArrowBacked(python_object)) {
692: 			auto arrow_table = PandasDataFrame::ToArrowTable(python_object);
693: 			RegisterArrowObject(arrow_table, name);
694: 		} else {
695: 			auto new_df = PandasScanFunction::PandasReplaceCopiedNames(python_object);
696: 			{
697: 				py::gil_scoped_release release;
698: 				temporary_views[name] =
699: 				    connection->TableFunction("pandas_scan", {Value::POINTER(CastPointerToValue(new_df.ptr()))})
700: 				        ->CreateView(name, true, true);
701: 			}
702: 
703: 			auto dependency = make_shared_ptr<ExternalDependency>();
704: 			dependency->AddDependency("original", PythonDependencyItem::Create(python_object));
705: 			dependency->AddDependency("copy", PythonDependencyItem::Create(std::move(new_df)));
706: 
707: 			vector<shared_ptr<ExternalDependency>> dependencies;
708: 			dependencies.push_back(std::move(dependency));
709: 			connection->context->external_dependencies[name] = std::move(dependencies);
710: 		}
711: 	} else if (IsAcceptedArrowObject(python_object) || IsPolarsDataframe(python_object)) {
712: 		py::object arrow_object;
713: 		if (IsPolarsDataframe(python_object)) {
714: 			if (PolarsDataFrame::IsDataFrame(python_object)) {
715: 				arrow_object = python_object.attr("to_arrow")();
716: 			} else if (PolarsDataFrame::IsLazyFrame(python_object)) {
717: 				py::object materialized = python_object.attr("collect")();
718: 				arrow_object = materialized.attr("to_arrow")();
719: 			} else {
720: 				throw NotImplementedException("Unsupported Polars DF Type");
721: 			}
722: 		} else {
723: 			arrow_object = python_object;
724: 		}
725: 		RegisterArrowObject(arrow_object, name);
726: 	} else if (DuckDBPyRelation::IsRelation(python_object)) {
727: 		auto pyrel = py::cast<DuckDBPyRelation *>(python_object);
728: 		if (!pyrel->CanBeRegisteredBy(*connection)) {
729: 			throw InvalidInputException(
730: 			    "The relation you are attempting to register was not made from this connection");
731: 		}
732: 		pyrel->CreateView(name, true);
733: 	} else {
734: 		auto py_object_type = string(py::str(python_object.get_type().attr("__name__")));
735: 		throw InvalidInputException("Python Object %s not suitable to be registered as a view", py_object_type);
736: 	}
737: 	return shared_from_this();
738: }
739: 
740: static void ParseMultiFileReaderOptions(named_parameter_map_t &options, const Optional<py::object> &filename,
741:                                         const Optional<py::object> &hive_partitioning,
742:                                         const Optional<py::object> &union_by_name,
743:                                         const Optional<py::object> &hive_types,
744:                                         const Optional<py::object> &hive_types_autocast) {
745: 	if (!py::none().is(filename)) {
746: 		auto val = TransformPythonValue(filename);
747: 		options["filename"] = val;
748: 	}
749: 
750: 	if (!py::none().is(hive_types)) {
751: 		auto val = TransformPythonValue(hive_types);
752: 		options["hive_types"] = val;
753: 	}
754: 
755: 	if (!py::none().is(hive_partitioning)) {
756: 		if (!py::isinstance<py::bool_>(hive_partitioning)) {
757: 			string actual_type = py::str(hive_partitioning.get_type());
758: 			throw BinderException("read_json only accepts 'hive_partitioning' as a boolean, not '%s'", actual_type);
759: 		}
760: 		auto val = TransformPythonValue(hive_partitioning, LogicalTypeId::BOOLEAN);
761: 		options["hive_partitioning"] = val;
762: 	}
763: 
764: 	if (!py::none().is(union_by_name)) {
765: 		if (!py::isinstance<py::bool_>(union_by_name)) {
766: 			string actual_type = py::str(union_by_name.get_type());
767: 			throw BinderException("read_json only accepts 'union_by_name' as a boolean, not '%s'", actual_type);
768: 		}
769: 		auto val = TransformPythonValue(union_by_name, LogicalTypeId::BOOLEAN);
770: 		options["union_by_name"] = val;
771: 	}
772: 
773: 	if (!py::none().is(hive_types_autocast)) {
774: 		if (!py::isinstance<py::bool_>(hive_types_autocast)) {
775: 			string actual_type = py::str(hive_types_autocast.get_type());
776: 			throw BinderException("read_json only accepts 'hive_types_autocast' as a boolean, not '%s'", actual_type);
777: 		}
778: 		auto val = TransformPythonValue(hive_types_autocast, LogicalTypeId::BOOLEAN);
779: 		options["hive_types_autocast"] = val;
780: 	}
781: }
782: 
783: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(
784:     const string &name, const Optional<py::object> &columns, const Optional<py::object> &sample_size,
785:     const Optional<py::object> &maximum_depth, const Optional<py::str> &records, const Optional<py::str> &format,
786:     const Optional<py::object> &date_format, const Optional<py::object> &timestamp_format,
787:     const Optional<py::object> &compression, const Optional<py::object> &maximum_object_size,
788:     const Optional<py::object> &ignore_errors, const Optional<py::object> &convert_strings_to_integers,
789:     const Optional<py::object> &field_appearance_threshold, const Optional<py::object> &map_inference_threshold,
790:     const Optional<py::object> &maximum_sample_files, const Optional<py::object> &filename,
791:     const Optional<py::object> &hive_partitioning, const Optional<py::object> &union_by_name,
792:     const Optional<py::object> &hive_types, const Optional<py::object> &hive_types_autocast) {
793: 	if (!connection) {
794: 		throw ConnectionException("Connection has already been closed");
795: 	}
796: 
797: 	named_parameter_map_t options;
798: 
799: 	ParseMultiFileReaderOptions(options, filename, hive_partitioning, union_by_name, hive_types, hive_types_autocast);
800: 
801: 	if (!py::none().is(columns)) {
802: 		if (!py::is_dict_like(columns)) {
803: 			throw BinderException("read_json only accepts 'columns' as a dict[str, str]");
804: 		}
805: 		py::dict columns_dict = columns;
806: 		child_list_t<Value> struct_fields;
807: 
808: 		for (auto &kv : columns_dict) {
809: 			auto &column_name = kv.first;
810: 			auto &type = kv.second;
811: 			if (!py::isinstance<py::str>(column_name)) {
812: 				string actual_type = py::str(column_name.get_type());
813: 				throw BinderException("The provided column name must be a str, not of type '%s'", actual_type);
814: 			}
815: 			if (!py::isinstance<py::str>(type)) {
816: 				string actual_type = py::str(column_name.get_type());
817: 				throw BinderException("The provided column type must be a str, not of type '%s'", actual_type);
818: 			}
819: 			struct_fields.emplace_back(py::str(column_name), Value(py::str(type)));
820: 		}
821: 		auto dtype_struct = Value::STRUCT(std::move(struct_fields));
822: 		options["columns"] = std::move(dtype_struct);
823: 	}
824: 
825: 	if (!py::none().is(records)) {
826: 		if (!py::isinstance<py::str>(records)) {
827: 			string actual_type = py::str(records.get_type());
828: 			throw BinderException("read_json only accepts 'records' as a string, not '%s'", actual_type);
829: 		}
830: 		auto records_s = py::reinterpret_borrow<py::str>(records);
831: 		auto records_option = std::string(py::str(records_s));
832: 		options["records"] = Value(records_option);
833: 	}
834: 
835: 	if (!py::none().is(format)) {
836: 		if (!py::isinstance<py::str>(format)) {
837: 			string actual_type = py::str(format.get_type());
838: 			throw BinderException("read_json only accepts 'format' as a string, not '%s'", actual_type);
839: 		}
840: 		auto format_s = py::reinterpret_borrow<py::str>(format);
841: 		auto format_option = std::string(py::str(format_s));
842: 		options["format"] = Value(format_option);
843: 	}
844: 
845: 	if (!py::none().is(date_format)) {
846: 		if (!py::isinstance<py::str>(date_format)) {
847: 			string actual_type = py::str(date_format.get_type());
848: 			throw BinderException("read_json only accepts 'date_format' as a string, not '%s'", actual_type);
849: 		}
850: 		auto date_format_s = py::reinterpret_borrow<py::str>(date_format);
851: 		auto date_format_option = std::string(py::str(date_format_s));
852: 		options["date_format"] = Value(date_format_option);
853: 	}
854: 
855: 	if (!py::none().is(timestamp_format)) {
856: 		if (!py::isinstance<py::str>(timestamp_format)) {
857: 			string actual_type = py::str(timestamp_format.get_type());
858: 			throw BinderException("read_json only accepts 'timestamp_format' as a string, not '%s'", actual_type);
859: 		}
860: 		auto timestamp_format_s = py::reinterpret_borrow<py::str>(timestamp_format);
861: 		auto timestamp_format_option = std::string(py::str(timestamp_format_s));
862: 		options["timestamp_format"] = Value(timestamp_format_option);
863: 	}
864: 
865: 	if (!py::none().is(compression)) {
866: 		if (!py::isinstance<py::str>(compression)) {
867: 			string actual_type = py::str(compression.get_type());
868: 			throw BinderException("read_json only accepts 'compression' as a string, not '%s'", actual_type);
869: 		}
870: 		auto compression_s = py::reinterpret_borrow<py::str>(compression);
871: 		auto compression_option = std::string(py::str(compression_s));
872: 		options["compression"] = Value(compression_option);
873: 	}
874: 
875: 	if (!py::none().is(sample_size)) {
876: 		if (!py::isinstance<py::int_>(sample_size)) {
877: 			string actual_type = py::str(sample_size.get_type());
878: 			throw BinderException("read_json only accepts 'sample_size' as an integer, not '%s'", actual_type);
879: 		}
880: 		options["sample_size"] = Value::INTEGER(py::int_(sample_size));
881: 	}
882: 
883: 	if (!py::none().is(maximum_depth)) {
884: 		if (!py::isinstance<py::int_>(maximum_depth)) {
885: 			string actual_type = py::str(maximum_depth.get_type());
886: 			throw BinderException("read_json only accepts 'maximum_depth' as an integer, not '%s'", actual_type);
887: 		}
888: 		options["maximum_depth"] = Value::INTEGER(py::int_(maximum_depth));
889: 	}
890: 
891: 	if (!py::none().is(maximum_object_size)) {
892: 		if (!py::isinstance<py::int_>(maximum_object_size)) {
893: 			string actual_type = py::str(maximum_object_size.get_type());
894: 			throw BinderException("read_json only accepts 'maximum_object_size' as an unsigned integer, not '%s'",
895: 			                      actual_type);
896: 		}
897: 		auto val = TransformPythonValue(maximum_object_size, LogicalTypeId::UINTEGER);
898: 		options["maximum_object_size"] = val;
899: 	}
900: 
901: 	if (!py::none().is(ignore_errors)) {
902: 		if (!py::isinstance<py::bool_>(ignore_errors)) {
903: 			string actual_type = py::str(ignore_errors.get_type());
904: 			throw BinderException("read_json only accepts 'ignore_errors' as a boolean, not '%s'", actual_type);
905: 		}
906: 		auto val = TransformPythonValue(ignore_errors, LogicalTypeId::BOOLEAN);
907: 		options["ignore_errors"] = val;
908: 	}
909: 
910: 	if (!py::none().is(convert_strings_to_integers)) {
911: 		if (!py::isinstance<py::bool_>(convert_strings_to_integers)) {
912: 			string actual_type = py::str(convert_strings_to_integers.get_type());
913: 			throw BinderException("read_json only accepts 'convert_strings_to_integers' as a boolean, not '%s'",
914: 			                      actual_type);
915: 		}
916: 		auto val = TransformPythonValue(convert_strings_to_integers, LogicalTypeId::BOOLEAN);
917: 		options["convert_strings_to_integers"] = val;
918: 	}
919: 
920: 	if (!py::none().is(field_appearance_threshold)) {
921: 		if (!py::isinstance<py::float_>(field_appearance_threshold)) {
922: 			string actual_type = py::str(field_appearance_threshold.get_type());
923: 			throw BinderException("read_json only accepts 'field_appearance_threshold' as a float, not '%s'",
924: 			                      actual_type);
925: 		}
926: 		auto val = TransformPythonValue(field_appearance_threshold, LogicalTypeId::DOUBLE);
927: 		options["field_appearance_threshold"] = val;
928: 	}
929: 
930: 	if (!py::none().is(map_inference_threshold)) {
931: 		if (!py::isinstance<py::int_>(map_inference_threshold)) {
932: 			string actual_type = py::str(map_inference_threshold.get_type());
933: 			throw BinderException("read_json only accepts 'map_inference_threshold' as an integer, not '%s'",
934: 			                      actual_type);
935: 		}
936: 		auto val = TransformPythonValue(map_inference_threshold, LogicalTypeId::BIGINT);
937: 		options["map_inference_threshold"] = val;
938: 	}
939: 
940: 	if (!py::none().is(maximum_sample_files)) {
941: 		if (!py::isinstance<py::int_>(maximum_sample_files)) {
942: 			string actual_type = py::str(maximum_sample_files.get_type());
943: 			throw BinderException("read_json only accepts 'maximum_sample_files' as an integer, not '%s'", actual_type);
944: 		}
945: 		auto val = TransformPythonValue(maximum_sample_files, LogicalTypeId::BIGINT);
946: 		options["maximum_sample_files"] = val;
947: 	}
948: 
949: 	bool auto_detect = false;
950: 	if (!options.count("columns")) {
951: 		options["auto_detect"] = Value::BOOLEAN(true);
952: 		auto_detect = true;
953: 	}
954: 
955: 	auto read_json_relation =
956: 	    make_shared_ptr<ReadJSONRelation>(connection->context, name, std::move(options), auto_detect);
957: 	if (read_json_relation == nullptr) {
958: 		throw BinderException("read_json can only be used when the JSON extension is (statically) loaded");
959: 	}
960: 	return make_uniq<DuckDBPyRelation>(std::move(read_json_relation));
961: }
962: 
963: PathLike DuckDBPyConnection::GetPathLike(const py::object &object) {
964: 	return PathLike::Create(object, *this);
965: }
966: 
967: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadCSV(
968:     const py::object &name_p, const py::object &header, const py::object &compression, const py::object &sep,
969:     const py::object &delimiter, const py::object &dtype, const py::object &na_values, const py::object &skiprows,
970:     const py::object &quotechar, const py::object &escapechar, const py::object &encoding, const py::object &parallel,
971:     const py::object &date_format, const py::object &timestamp_format, const py::object &sample_size,
972:     const py::object &all_varchar, const py::object &normalize_names, const py::object &filename,
973:     const py::object &null_padding, const py::object &names_p) {
974: 	if (!connection) {
975: 		throw ConnectionException("Connection has already been closed");
976: 	}
977: 	CSVReaderOptions options;
978: 	auto path_like = GetPathLike(name_p);
979: 	auto &name = path_like.files;
980: 	auto file_like_object_wrapper = std::move(path_like.dependency);
981: 	named_parameter_map_t bind_parameters;
982: 
983: 	// First check if the header is explicitly set
984: 	// when false this affects the returned types, so it needs to be known at initialization of the relation
985: 	if (!py::none().is(header)) {
986: 
987: 		bool header_as_int = py::isinstance<py::int_>(header);
988: 		bool header_as_bool = py::isinstance<py::bool_>(header);
989: 
990: 		bool header_value;
991: 		if (header_as_bool) {
992: 			header_value = py::bool_(header);
993: 		} else if (header_as_int) {
994: 			if ((int)py::int_(header) != 0) {
995: 				throw InvalidInputException("read_csv only accepts 0 if 'header' is given as an integer");
996: 			}
997: 			header_value = true;
998: 		} else {
999: 			throw InvalidInputException("read_csv only accepts 'header' as an integer, or a boolean");
1000: 		}
1001: 		bind_parameters["header"] = Value::BOOLEAN(header_value);
1002: 	}
1003: 
1004: 	if (!py::none().is(compression)) {
1005: 		if (!py::isinstance<py::str>(compression)) {
1006: 			throw InvalidInputException("read_csv only accepts 'compression' as a string");
1007: 		}
1008: 		bind_parameters["compression"] = Value(py::str(compression));
1009: 	}
1010: 
1011: 	if (!py::none().is(dtype)) {
1012: 		if (py::is_dict_like(dtype)) {
1013: 			child_list_t<Value> struct_fields;
1014: 			py::dict dtype_dict = dtype;
1015: 			for (auto &kv : dtype_dict) {
1016: 				shared_ptr<DuckDBPyType> sql_type;
1017: 				if (!py::try_cast(kv.second, sql_type)) {
1018: 					throw py::value_error("The types provided to 'dtype' have to be DuckDBPyType");
1019: 				}
1020: 				struct_fields.emplace_back(py::str(kv.first), Value(sql_type->ToString()));
1021: 			}
1022: 			auto dtype_struct = Value::STRUCT(std::move(struct_fields));
1023: 			bind_parameters["dtypes"] = std::move(dtype_struct);
1024: 		} else if (py::is_list_like(dtype)) {
1025: 			vector<Value> list_values;
1026: 			py::list dtype_list = dtype;
1027: 			for (auto &child : dtype_list) {
1028: 				shared_ptr<DuckDBPyType> sql_type;
1029: 				if (!py::try_cast(child, sql_type)) {
1030: 					throw py::value_error("The types provided to 'dtype' have to be DuckDBPyType");
1031: 				}
1032: 				list_values.push_back(sql_type->ToString());
1033: 			}
1034: 			bind_parameters["dtypes"] = Value::LIST(LogicalType::VARCHAR, std::move(list_values));
1035: 		} else {
1036: 			throw InvalidInputException("read_csv only accepts 'dtype' as a dictionary or a list of strings");
1037: 		}
1038: 	}
1039: 
1040: 	bool has_sep = !py::none().is(sep);
1041: 	bool has_delimiter = !py::none().is(delimiter);
1042: 	if (has_sep && has_delimiter) {
1043: 		throw InvalidInputException("read_csv takes either 'delimiter' or 'sep', not both");
1044: 	}
1045: 	if (has_sep) {
1046: 		bind_parameters["delim"] = Value(py::str(sep));
1047: 	} else if (has_delimiter) {
1048: 		bind_parameters["delim"] = Value(py::str(delimiter));
1049: 	}
1050: 
1051: 	if (!py::none().is(names_p)) {
1052: 		if (!py::is_list_like(names_p)) {
1053: 			throw InvalidInputException("read_csv only accepts 'names' as a list of strings");
1054: 		}
1055: 		vector<Value> names;
1056: 		py::list names_list = names_p;
1057: 		for (auto &elem : names_list) {
1058: 			if (!py::isinstance<py::str>(elem)) {
1059: 				throw InvalidInputException("read_csv 'names' list has to consist of only strings");
1060: 			}
1061: 			names.push_back(Value(std::string(py::str(elem))));
1062: 		}
1063: 		bind_parameters["names"] = Value::LIST(LogicalType::VARCHAR, std::move(names));
1064: 	}
1065: 
1066: 	if (!py::none().is(na_values)) {
1067: 		vector<Value> null_values;
1068: 		if (!py::isinstance<py::str>(na_values) && !py::is_list_like(na_values)) {
1069: 			throw InvalidInputException("read_csv only accepts 'na_values' as a string or a list of strings");
1070: 		} else if (py::isinstance<py::str>(na_values)) {
1071: 			null_values.push_back(Value(py::str(na_values)));
1072: 		} else {
1073: 			py::list null_list = na_values;
1074: 			for (auto &elem : null_list) {
1075: 				if (!py::isinstance<py::str>(elem)) {
1076: 					throw InvalidInputException("read_csv 'na_values' list has to consist of only strings");
1077: 				}
1078: 				null_values.push_back(Value(std::string(py::str(elem))));
1079: 			}
1080: 		}
1081: 		bind_parameters["nullstr"] = Value::LIST(LogicalType::VARCHAR, std::move(null_values));
1082: 	}
1083: 
1084: 	if (!py::none().is(skiprows)) {
1085: 		if (!py::isinstance<py::int_>(skiprows)) {
1086: 			throw InvalidInputException("read_csv only accepts 'skiprows' as an integer");
1087: 		}
1088: 		bind_parameters["skip"] = Value::INTEGER(py::int_(skiprows));
1089: 	}
1090: 
1091: 	if (!py::none().is(parallel)) {
1092: 		if (!py::isinstance<py::bool_>(parallel)) {
1093: 			throw InvalidInputException("read_csv only accepts 'parallel' as a boolean");
1094: 		}
1095: 		bind_parameters["parallel"] = Value::BOOLEAN(py::bool_(parallel));
1096: 	}
1097: 
1098: 	if (!py::none().is(quotechar)) {
1099: 		if (!py::isinstance<py::str>(quotechar)) {
1100: 			throw InvalidInputException("read_csv only accepts 'quotechar' as a string");
1101: 		}
1102: 		bind_parameters["quote"] = Value(py::str(quotechar));
1103: 	}
1104: 
1105: 	if (!py::none().is(escapechar)) {
1106: 		if (!py::isinstance<py::str>(escapechar)) {
1107: 			throw InvalidInputException("read_csv only accepts 'escapechar' as a string");
1108: 		}
1109: 		bind_parameters["escape"] = Value(py::str(escapechar));
1110: 	}
1111: 
1112: 	if (!py::none().is(encoding)) {
1113: 		if (!py::isinstance<py::str>(encoding)) {
1114: 			throw InvalidInputException("read_csv only accepts 'encoding' as a string");
1115: 		}
1116: 		string encoding_str = StringUtil::Lower(py::str(encoding));
1117: 		if (encoding_str != "utf8" && encoding_str != "utf-8") {
1118: 			throw BinderException("Copy is only supported for UTF-8 encoded files, ENCODING 'UTF-8'");
1119: 		}
1120: 	}
1121: 
1122: 	if (!py::none().is(date_format)) {
1123: 		if (!py::isinstance<py::str>(date_format)) {
1124: 			throw InvalidInputException("read_csv only accepts 'date_format' as a string");
1125: 		}
1126: 		bind_parameters["dateformat"] = Value(py::str(date_format));
1127: 	}
1128: 
1129: 	if (!py::none().is(timestamp_format)) {
1130: 		if (!py::isinstance<py::str>(timestamp_format)) {
1131: 			throw InvalidInputException("read_csv only accepts 'timestamp_format' as a string");
1132: 		}
1133: 		bind_parameters["timestampformat"] = Value(py::str(timestamp_format));
1134: 	}
1135: 
1136: 	if (!py::none().is(sample_size)) {
1137: 		if (!py::isinstance<py::int_>(sample_size)) {
1138: 			throw InvalidInputException("read_csv only accepts 'sample_size' as an integer");
1139: 		}
1140: 		bind_parameters["sample_size"] = Value::INTEGER(py::int_(sample_size));
1141: 	}
1142: 
1143: 	if (!py::none().is(all_varchar)) {
1144: 		if (!py::isinstance<py::bool_>(all_varchar)) {
1145: 			throw InvalidInputException("read_csv only accepts 'all_varchar' as a boolean");
1146: 		}
1147: 		bind_parameters["all_varchar"] = Value::BOOLEAN(py::bool_(all_varchar));
1148: 	}
1149: 
1150: 	if (!py::none().is(normalize_names)) {
1151: 		if (!py::isinstance<py::bool_>(normalize_names)) {
1152: 			throw InvalidInputException("read_csv only accepts 'normalize_names' as a boolean");
1153: 		}
1154: 		bind_parameters["normalize_names"] = Value::BOOLEAN(py::bool_(normalize_names));
1155: 	}
1156: 
1157: 	if (!py::none().is(filename)) {
1158: 		if (!py::isinstance<py::bool_>(filename)) {
1159: 			throw InvalidInputException("read_csv only accepts 'filename' as a boolean");
1160: 		}
1161: 		bind_parameters["filename"] = Value::BOOLEAN(py::bool_(filename));
1162: 	}
1163: 
1164: 	if (!py::none().is(null_padding)) {
1165: 		if (!py::isinstance<py::bool_>(null_padding)) {
1166: 			throw InvalidInputException("read_csv only accepts 'null_padding' as a boolean");
1167: 		}
1168: 		bind_parameters["null_padding"] = Value::BOOLEAN(py::bool_(null_padding));
1169: 	}
1170: 
1171: 	// Create the ReadCSV Relation using the 'options'
1172: 
1173: 	auto read_csv_p = connection->ReadCSV(name, std::move(bind_parameters));
1174: 	auto &read_csv = read_csv_p->Cast<ReadCSVRelation>();
1175: 	if (file_like_object_wrapper) {
1176: 		read_csv.AddExternalDependency(std::move(file_like_object_wrapper));
1177: 	}
1178: 
1179: 	return make_uniq<DuckDBPyRelation>(read_csv_p->Alias(read_csv.alias));
1180: }
1181: 
1182: void DuckDBPyConnection::ExecuteImmediately(vector<unique_ptr<SQLStatement>> statements) {
1183: 	if (statements.empty()) {
1184: 		return;
1185: 	}
1186: 	for (auto &stmt : statements) {
1187: 		if (stmt->n_param != 0) {
1188: 			throw NotImplementedException(
1189: 			    "Prepared parameters are only supported for the last statement, please split your query up into "
1190: 			    "separate 'execute' calls if you want to use prepared parameters");
1191: 		}
1192: 		auto pending_query = connection->PendingQuery(std::move(stmt), false);
1193: 		auto res = CompletePendingQuery(*pending_query);
1194: 
1195: 		if (res->HasError()) {
1196: 			res->ThrowError();
1197: 		}
1198: 	}
1199: }
1200: 
1201: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::RunQuery(const py::object &query, string alias, py::object params) {
1202: 	if (!connection) {
1203: 		throw ConnectionException("Connection has already been closed");
1204: 	}
1205: 	if (alias.empty()) {
1206: 		alias = "unnamed_relation_" + StringUtil::GenerateRandomName(16);
1207: 	}
1208: 
1209: 	auto statements = GetStatements(query);
1210: 	if (statements.empty()) {
1211: 		// TODO: should we throw?
1212: 		return nullptr;
1213: 	}
1214: 
1215: 	auto last_statement = std::move(statements.back());
1216: 	statements.pop_back();
1217: 	// First immediately execute any preceding statements (if any)
1218: 	ExecuteImmediately(std::move(statements));
1219: 
1220: 	// Attempt to create a Relation for lazy execution if possible
1221: 	shared_ptr<Relation> relation;
1222: 	if (py::none().is(params)) {
1223: 		// FIXME: currently we can't create relations with prepared parameters
1224: 		auto statement_type = last_statement->type;
1225: 		switch (statement_type) {
1226: 		case StatementType::SELECT_STATEMENT: {
1227: 			auto select_statement = unique_ptr_cast<SQLStatement, SelectStatement>(std::move(last_statement));
1228: 			relation = connection->RelationFromQuery(std::move(select_statement), alias);
1229: 			break;
1230: 		}
1231: 		default:
1232: 			break;
1233: 		}
1234: 	}
1235: 
1236: 	if (!relation) {
1237: 		// Could not create a relation, resort to direct execution
1238: 		auto prep = PrepareQuery(std::move(last_statement));
1239: 		auto res = ExecuteInternal(*prep, std::move(params));
1240: 		if (!res) {
1241: 			return nullptr;
1242: 		}
1243: 		if (res->properties.return_type != StatementReturnType::QUERY_RESULT) {
1244: 			return nullptr;
1245: 		}
1246: 		if (res->type == QueryResultType::STREAM_RESULT) {
1247: 			auto &stream_result = res->Cast<StreamQueryResult>();
1248: 			res = stream_result.Materialize();
1249: 		}
1250: 		auto &materialized_result = res->Cast<MaterializedQueryResult>();
1251: 		relation = make_shared_ptr<MaterializedRelation>(connection->context, materialized_result.TakeCollection(),
1252: 		                                                 res->names, alias);
1253: 	}
1254: 	return make_uniq<DuckDBPyRelation>(std::move(relation));
1255: }
1256: 
1257: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Table(const string &tname) {
1258: 	if (!connection) {
1259: 		throw ConnectionException("Connection has already been closed");
1260: 	}
1261: 	auto qualified_name = QualifiedName::Parse(tname);
1262: 	if (qualified_name.schema.empty()) {
1263: 		qualified_name.schema = DEFAULT_SCHEMA;
1264: 	}
1265: 	try {
1266: 		return make_uniq<DuckDBPyRelation>(connection->Table(qualified_name.schema, qualified_name.name));
1267: 	} catch (const CatalogException &) {
1268: 		// CatalogException will be of the type '... is not a table'
1269: 		// Not a table in the database, make a query relation that can perform replacement scans
1270: 		auto sql_query = StringUtil::Format("from %s", KeywordHelper::WriteOptionallyQuoted(tname));
1271: 		return RunQuery(py::str(sql_query), tname);
1272: 	}
1273: }
1274: 
1275: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Values(py::object params) {
1276: 	if (!connection) {
1277: 		throw ConnectionException("Connection has already been closed");
1278: 	}
1279: 	if (params.is_none()) {
1280: 		params = py::list();
1281: 	}
1282: 	if (!py::hasattr(params, "__len__")) {
1283: 		throw InvalidInputException("Type of object passed to parameter 'values' must be iterable");
1284: 	}
1285: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(params)};
1286: 	return make_uniq<DuckDBPyRelation>(connection->Values(values));
1287: }
1288: 
1289: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::View(const string &vname) {
1290: 	if (!connection) {
1291: 		throw ConnectionException("Connection has already been closed");
1292: 	}
1293: 	// First check our temporary view
1294: 	if (temporary_views.find(vname) != temporary_views.end()) {
1295: 		return make_uniq<DuckDBPyRelation>(temporary_views[vname]);
1296: 	}
1297: 	return make_uniq<DuckDBPyRelation>(connection->View(vname));
1298: }
1299: 
1300: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fname, py::object params) {
1301: 	if (params.is_none()) {
1302: 		params = py::list();
1303: 	}
1304: 	if (!py::is_list_like(params)) {
1305: 		throw InvalidInputException("'params' has to be a list of parameters");
1306: 	}
1307: 	if (!connection) {
1308: 		throw ConnectionException("Connection has already been closed");
1309: 	}
1310: 
1311: 	return make_uniq<DuckDBPyRelation>(
1312: 	    connection->TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(params)));
1313: }
1314: 
1315: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(const PandasDataFrame &value) {
1316: 	if (!connection) {
1317: 		throw ConnectionException("Connection has already been closed");
1318: 	}
1319: 	string name = "df_" + StringUtil::GenerateRandomName();
1320: 	if (PandasDataFrame::IsPyArrowBacked(value)) {
1321: 		auto table = PandasDataFrame::ToArrowTable(value);
1322: 		return DuckDBPyConnection::FromArrow(table);
1323: 	}
1324: 	auto new_df = PandasScanFunction::PandasReplaceCopiedNames(value);
1325: 	vector<Value> params;
1326: 	params.emplace_back(Value::POINTER(CastPointerToValue(new_df.ptr())));
1327: 	auto rel = connection->TableFunction("pandas_scan", params)->Alias(name);
1328: 	auto dependency = make_shared_ptr<ExternalDependency>();
1329: 	dependency->AddDependency("original", PythonDependencyItem::Create(value));
1330: 	dependency->AddDependency("copy", PythonDependencyItem::Create(new_df));
1331: 	rel->AddExternalDependency(std::move(dependency));
1332: 	return make_uniq<DuckDBPyRelation>(std::move(rel));
1333: }
1334: 
1335: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_glob, bool binary_as_string,
1336:                                                              bool file_row_number, bool filename,
1337:                                                              bool hive_partitioning, bool union_by_name,
1338:                                                              const py::object &compression) {
1339: 	if (!connection) {
1340: 		throw ConnectionException("Connection has already been closed");
1341: 	}
1342: 	string name = "parquet_" + StringUtil::GenerateRandomName();
1343: 	vector<Value> params;
1344: 	params.emplace_back(file_glob);
1345: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)},
1346: 	                                        {"file_row_number", Value::BOOLEAN(file_row_number)},
1347: 	                                        {"filename", Value::BOOLEAN(filename)},
1348: 	                                        {"hive_partitioning", Value::BOOLEAN(hive_partitioning)},
1349: 	                                        {"union_by_name", Value::BOOLEAN(union_by_name)}});
1350: 
1351: 	if (!py::none().is(compression)) {
1352: 		if (!py::isinstance<py::str>(compression)) {
1353: 			throw InvalidInputException("from_parquet only accepts 'compression' as a string");
1354: 		}
1355: 		named_parameters["compression"] = Value(py::str(compression));
1356: 	}
1357: 	return make_uniq<DuckDBPyRelation>(
1358: 	    connection->TableFunction("parquet_scan", params, named_parameters)->Alias(name));
1359: }
1360: 
1361: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquets(const vector<string> &file_globs, bool binary_as_string,
1362:                                                               bool file_row_number, bool filename,
1363:                                                               bool hive_partitioning, bool union_by_name,
1364:                                                               const py::object &compression) {
1365: 	if (!connection) {
1366: 		throw ConnectionException("Connection has already been closed");
1367: 	}
1368: 	string name = "parquet_" + StringUtil::GenerateRandomName();
1369: 	vector<Value> params;
1370: 	auto file_globs_as_value = vector<Value>();
1371: 	for (const auto &file : file_globs) {
1372: 		file_globs_as_value.emplace_back(file);
1373: 	}
1374: 	params.emplace_back(Value::LIST(file_globs_as_value));
1375: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)},
1376: 	                                        {"file_row_number", Value::BOOLEAN(file_row_number)},
1377: 	                                        {"filename", Value::BOOLEAN(filename)},
1378: 	                                        {"hive_partitioning", Value::BOOLEAN(hive_partitioning)},
1379: 	                                        {"union_by_name", Value::BOOLEAN(union_by_name)}});
1380: 
1381: 	if (!py::none().is(compression)) {
1382: 		if (!py::isinstance<py::str>(compression)) {
1383: 			throw InvalidInputException("from_parquet only accepts 'compression' as a string");
1384: 		}
1385: 		named_parameters["compression"] = Value(py::str(compression));
1386: 	}
1387: 
1388: 	return make_uniq<DuckDBPyRelation>(
1389: 	    connection->TableFunction("parquet_scan", params, named_parameters)->Alias(name));
1390: }
1391: 
1392: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrow(py::object &arrow_object) {
1393: 	if (!connection) {
1394: 		throw ConnectionException("Connection has already been closed");
1395: 	}
1396: 	py::gil_scoped_acquire acquire;
1397: 	string name = "arrow_object_" + StringUtil::GenerateRandomName();
1398: 	if (!IsAcceptedArrowObject(arrow_object)) {
1399: 		auto py_object_type = string(py::str(arrow_object.get_type().attr("__name__")));
1400: 		throw InvalidInputException("Python Object Type %s is not an accepted Arrow Object.", py_object_type);
1401: 	}
1402: 	auto stream_factory =
1403: 	    make_uniq<PythonTableArrowArrayStreamFactory>(arrow_object.ptr(), connection->context->GetClientProperties());
1404: 
1405: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
1406: 	auto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;
1407: 
1408: 	auto rel = connection
1409: 	               ->TableFunction("arrow_scan", {Value::POINTER(CastPointerToValue(stream_factory.get())),
1410: 	                                              Value::POINTER(CastPointerToValue(stream_factory_produce)),
1411: 	                                              Value::POINTER(CastPointerToValue(stream_factory_get_schema))})
1412: 	               ->Alias(name);
1413: 	auto dependency = make_shared_ptr<ExternalDependency>();
1414: 	auto dependency_item =
1415: 	    PythonDependencyItem::Create(make_uniq<RegisteredArrow>(std::move(stream_factory), arrow_object));
1416: 	dependency->AddDependency("object", std::move(dependency_item));
1417: 	rel->AddExternalDependency(std::move(dependency));
1418: 	return make_uniq<DuckDBPyRelation>(std::move(rel));
1419: }
1420: 
1421: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstrait(py::bytes &proto) {
1422: 	if (!connection) {
1423: 		throw ConnectionException("Connection has already been closed");
1424: 	}
1425: 	string name = "substrait_" + StringUtil::GenerateRandomName();
1426: 	vector<Value> params;
1427: 	params.emplace_back(Value::BLOB_RAW(proto));
1428: 	return make_uniq<DuckDBPyRelation>(connection->TableFunction("from_substrait", params)->Alias(name));
1429: }
1430: 
1431: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstrait(const string &query, bool enable_optimizer) {
1432: 	if (!connection) {
1433: 		throw ConnectionException("Connection has already been closed");
1434: 	}
1435: 	vector<Value> params;
1436: 	params.emplace_back(query);
1437: 	named_parameter_map_t named_parameters({{"enable_optimizer", Value::BOOLEAN(enable_optimizer)}});
1438: 	return make_uniq<DuckDBPyRelation>(
1439: 	    connection->TableFunction("get_substrait", params, named_parameters)->Alias(query));
1440: }
1441: 
1442: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstraitJSON(const string &query, bool enable_optimizer) {
1443: 	if (!connection) {
1444: 		throw ConnectionException("Connection has already been closed");
1445: 	}
1446: 	vector<Value> params;
1447: 	params.emplace_back(query);
1448: 	named_parameter_map_t named_parameters({{"enable_optimizer", Value::BOOLEAN(enable_optimizer)}});
1449: 	return make_uniq<DuckDBPyRelation>(
1450: 	    connection->TableFunction("get_substrait_json", params, named_parameters)->Alias(query));
1451: }
1452: 
1453: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstraitJSON(const string &json) {
1454: 	if (!connection) {
1455: 		throw ConnectionException("Connection has already been closed");
1456: 	}
1457: 	string name = "from_substrait_" + StringUtil::GenerateRandomName();
1458: 	vector<Value> params;
1459: 	params.emplace_back(json);
1460: 	return make_uniq<DuckDBPyRelation>(connection->TableFunction("from_substrait_json", params)->Alias(name));
1461: }
1462: 
1463: unordered_set<string> DuckDBPyConnection::GetTableNames(const string &query) {
1464: 	if (!connection) {
1465: 		throw ConnectionException("Connection has already been closed");
1466: 	}
1467: 	return connection->GetTableNames(query);
1468: }
1469: 
1470: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::UnregisterPythonObject(const string &name) {
1471: 	connection->context->external_dependencies.erase(name);
1472: 	temporary_views.erase(name);
1473: 	py::gil_scoped_release release;
1474: 	if (connection) {
1475: 		connection->Query("DROP VIEW \"" + name + "\"");
1476: 	}
1477: 	return shared_from_this();
1478: }
1479: 
1480: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Begin() {
1481: 	ExecuteFromString("BEGIN TRANSACTION");
1482: 	return shared_from_this();
1483: }
1484: 
1485: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Commit() {
1486: 	if (connection->context->transaction.IsAutoCommit()) {
1487: 		return shared_from_this();
1488: 	}
1489: 	ExecuteFromString("COMMIT");
1490: 	return shared_from_this();
1491: }
1492: 
1493: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Rollback() {
1494: 	ExecuteFromString("ROLLBACK");
1495: 	return shared_from_this();
1496: }
1497: 
1498: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Checkpoint() {
1499: 	ExecuteFromString("CHECKPOINT");
1500: 	return shared_from_this();
1501: }
1502: 
1503: Optional<py::list> DuckDBPyConnection::GetDescription() {
1504: 	if (!result) {
1505: 		return py::none();
1506: 	}
1507: 	return result->Description();
1508: }
1509: 
1510: int DuckDBPyConnection::GetRowcount() {
1511: 	return -1;
1512: }
1513: 
1514: void DuckDBPyConnection::Close() {
1515: 	result = nullptr;
1516: 	connection = nullptr;
1517: 	database = nullptr;
1518: 	temporary_views.clear();
1519: 	// https://peps.python.org/pep-0249/#Connection.close
1520: 	for (auto &cur : cursors) {
1521: 		auto cursor = cur.lock();
1522: 		if (!cursor) {
1523: 			// The cursor has already been closed
1524: 			continue;
1525: 		}
1526: 		cursor->Close();
1527: 	}
1528: 	registered_functions.clear();
1529: 	cursors.clear();
1530: }
1531: 
1532: void DuckDBPyConnection::Interrupt() {
1533: 	if (!connection) {
1534: 		throw ConnectionException("Connection has already been closed");
1535: 	}
1536: 	connection->Interrupt();
1537: }
1538: 
1539: void DuckDBPyConnection::InstallExtension(const string &extension, bool force_install) {
1540: 	ExtensionHelper::InstallExtension(*connection->context, extension, force_install);
1541: }
1542: 
1543: void DuckDBPyConnection::LoadExtension(const string &extension) {
1544: 	ExtensionHelper::LoadExternalExtension(*connection->context, extension);
1545: }
1546: 
1547: // cursor() is stupid
1548: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Cursor() {
1549: 	if (!connection) {
1550: 		throw ConnectionException("Connection has already been closed");
1551: 	}
1552: 	auto res = make_shared_ptr<DuckDBPyConnection>();
1553: 	res->database = database;
1554: 	res->connection = make_uniq<Connection>(*res->database);
1555: 	cursors.push_back(res);
1556: 	return res;
1557: }
1558: 
1559: // these should be functions on the result but well
1560: Optional<py::tuple> DuckDBPyConnection::FetchOne() {
1561: 	if (!result) {
1562: 		throw InvalidInputException("No open result set");
1563: 	}
1564: 	return result->FetchOne();
1565: }
1566: 
1567: py::list DuckDBPyConnection::FetchMany(idx_t size) {
1568: 	if (!result) {
1569: 		throw InvalidInputException("No open result set");
1570: 	}
1571: 	return result->FetchMany(size);
1572: }
1573: 
1574: py::list DuckDBPyConnection::FetchAll() {
1575: 	if (!result) {
1576: 		throw InvalidInputException("No open result set");
1577: 	}
1578: 	return result->FetchAll();
1579: }
1580: 
1581: py::dict DuckDBPyConnection::FetchNumpy() {
1582: 	if (!result) {
1583: 		throw InvalidInputException("No open result set");
1584: 	}
1585: 	return result->FetchNumpyInternal();
1586: }
1587: 
1588: PandasDataFrame DuckDBPyConnection::FetchDF(bool date_as_object) {
1589: 	if (!result) {
1590: 		throw InvalidInputException("No open result set");
1591: 	}
1592: 	return result->FetchDF(date_as_object);
1593: }
1594: 
1595: PandasDataFrame DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk, bool date_as_object) const {
1596: 	if (!result) {
1597: 		throw InvalidInputException("No open result set");
1598: 	}
1599: 	return result->FetchDFChunk(vectors_per_chunk, date_as_object);
1600: }
1601: 
1602: duckdb::pyarrow::Table DuckDBPyConnection::FetchArrow(idx_t rows_per_batch) {
1603: 	if (!result) {
1604: 		throw InvalidInputException("No open result set");
1605: 	}
1606: 	return result->ToArrowTable(rows_per_batch);
1607: }
1608: 
1609: py::dict DuckDBPyConnection::FetchPyTorch() {
1610: 	if (!result) {
1611: 		throw InvalidInputException("No open result set");
1612: 	}
1613: 	return result->FetchPyTorch();
1614: }
1615: 
1616: py::dict DuckDBPyConnection::FetchTF() {
1617: 	if (!result) {
1618: 		throw InvalidInputException("No open result set");
1619: 	}
1620: 	return result->FetchTF();
1621: }
1622: 
1623: PolarsDataFrame DuckDBPyConnection::FetchPolars(idx_t rows_per_batch) {
1624: 	auto arrow = FetchArrow(rows_per_batch);
1625: 	return py::cast<PolarsDataFrame>(py::module::import("polars").attr("DataFrame")(arrow));
1626: }
1627: 
1628: duckdb::pyarrow::RecordBatchReader DuckDBPyConnection::FetchRecordBatchReader(const idx_t rows_per_batch) const {
1629: 	if (!result) {
1630: 		throw InvalidInputException("No open result set");
1631: 	}
1632: 	return result->FetchRecordBatchReader(rows_per_batch);
1633: }
1634: 
1635: case_insensitive_map_t<Value> TransformPyConfigDict(const py::dict &py_config_dict) {
1636: 	case_insensitive_map_t<Value> config_dict;
1637: 	for (auto &kv : py_config_dict) {
1638: 		auto key = py::str(kv.first);
1639: 		auto val = py::str(kv.second);
1640: 		config_dict[key] = Value(val);
1641: 	}
1642: 	return config_dict;
1643: }
1644: 
1645: void CreateNewInstance(DuckDBPyConnection &res, const string &database, DBConfig &config) {
1646: 	// We don't cache unnamed memory instances (i.e., :memory:)
1647: 	bool cache_instance = database != ":memory:" && !database.empty();
1648: 	config.replacement_scans.emplace_back(PythonReplacementScan::Replace);
1649: 	res.database = instance_cache.CreateInstance(database, config, cache_instance);
1650: 	res.connection = make_uniq<Connection>(*res.database);
1651: 	auto &context = *res.connection->context;
1652: 	PandasScanFunction scan_fun;
1653: 	CreateTableFunctionInfo scan_info(scan_fun);
1654: 	MapFunction map_fun;
1655: 	CreateTableFunctionInfo map_info(map_fun);
1656: 	auto &catalog = Catalog::GetSystemCatalog(context);
1657: 	context.transaction.BeginTransaction();
1658: 	catalog.CreateTableFunction(context, &scan_info);
1659: 	catalog.CreateTableFunction(context, &map_info);
1660: 	context.transaction.Commit();
1661: }
1662: 
1663: static bool HasJupyterProgressBarDependencies() {
1664: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1665: 	if (!import_cache.ipywidgets()) {
1666: 		// ipywidgets not installed, needed to support the progress bar
1667: 		return false;
1668: 	}
1669: 	return true;
1670: }
1671: 
1672: static void SetDefaultConfigArguments(ClientContext &context) {
1673: 	if (!DuckDBPyConnection::IsInteractive()) {
1674: 		// Don't need to set any special default arguments
1675: 		return;
1676: 	}
1677: 
1678: 	auto &config = ClientConfig::GetConfig(context);
1679: 	config.enable_progress_bar = true;
1680: 
1681: 	if (!DuckDBPyConnection::IsJupyter()) {
1682: 		return;
1683: 	}
1684: 	if (!HasJupyterProgressBarDependencies()) {
1685: 		// Disable progress bar altogether
1686: 		config.system_progress_bar_disable_reason =
1687: 		    "required package 'ipywidgets' is missing, which is needed to render progress bars in Jupyter";
1688: 		config.enable_progress_bar = false;
1689: 		return;
1690: 	}
1691: 
1692: 	// Set the function used to create the display for the progress bar
1693: 	context.config.display_create_func = JupyterProgressBarDisplay::Create;
1694: }
1695: 
1696: static shared_ptr<DuckDBPyConnection> FetchOrCreateInstance(const string &database, DBConfig &config) {
1697: 	auto res = make_shared_ptr<DuckDBPyConnection>();
1698: 	res->database = instance_cache.GetInstance(database, config);
1699: 	if (!res->database) {
1700: 		//! No cached database, we must create a new instance
1701: 		CreateNewInstance(*res, database, config);
1702: 		return res;
1703: 	}
1704: 	res->connection = make_uniq<Connection>(*res->database);
1705: 	return res;
1706: }
1707: 
1708: bool IsDefaultConnectionString(const string &database, bool read_only, case_insensitive_map_t<Value> &config) {
1709: 	bool is_default = StringUtil::CIEquals(database, ":default:");
1710: 	if (!is_default) {
1711: 		return false;
1712: 	}
1713: 	// Only allow fetching the default connection when no options are passed
1714: 	if (read_only == true || !config.empty()) {
1715: 		throw InvalidInputException("Default connection fetching is only allowed without additional options");
1716: 	}
1717: 	return true;
1718: }
1719: 
1720: static string GetPathString(const py::object &path) {
1721: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1722: 	const bool is_path = py::isinstance(path, import_cache.pathlib.Path());
1723: 	if (is_path || py::isinstance<py::str>(path)) {
1724: 		return std::string(py::str(path));
1725: 	}
1726: 	string actual_type = py::str(path.get_type());
1727: 	throw InvalidInputException("Please provide either a str or a pathlib.Path, not %s", actual_type);
1728: }
1729: 
1730: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Connect(const py::object &database_p, bool read_only,
1731:                                                            const py::dict &config_options) {
1732: 	auto config_dict = TransformPyConfigDict(config_options);
1733: 	auto database = GetPathString(database_p);
1734: 	if (IsDefaultConnectionString(database, read_only, config_dict)) {
1735: 		return DuckDBPyConnection::DefaultConnection();
1736: 	}
1737: 
1738: 	DBConfig config(read_only);
1739: 	config.AddExtensionOption("pandas_analyze_sample",
1740: 	                          "The maximum number of rows to sample when analyzing a pandas object column.",
1741: 	                          LogicalType::UBIGINT, Value::UBIGINT(1000));
1742: 	config.AddExtensionOption("python_enable_replacements",
1743: 	                          "Whether variables visible to the current stack should be used for replacement scans.",
1744: 	                          LogicalType::BOOLEAN, Value::BOOLEAN(true));
1745: 	if (!DuckDBPyConnection::IsJupyter()) {
1746: 		config_dict["duckdb_api"] = Value("python");
1747: 	} else {
1748: 		config_dict["duckdb_api"] = Value("python jupyter");
1749: 	}
1750: 	config.SetOptionsByName(config_dict);
1751: 
1752: 	auto res = FetchOrCreateInstance(database, config);
1753: 	auto &client_context = *res->connection->context;
1754: 	SetDefaultConfigArguments(client_context);
1755: 	return res;
1756: }
1757: 
1758: vector<Value> DuckDBPyConnection::TransformPythonParamList(const py::handle &params) {
1759: 	vector<Value> args;
1760: 	args.reserve(py::len(params));
1761: 
1762: 	for (auto param : params) {
1763: 		args.emplace_back(TransformPythonValue(param, LogicalType::UNKNOWN, false));
1764: 	}
1765: 	return args;
1766: }
1767: 
1768: case_insensitive_map_t<BoundParameterData> DuckDBPyConnection::TransformPythonParamDict(const py::dict &params) {
1769: 	case_insensitive_map_t<BoundParameterData> args;
1770: 
1771: 	for (auto pair : params) {
1772: 		auto &key = pair.first;
1773: 		auto &value = pair.second;
1774: 		args[std::string(py::str(key))] = BoundParameterData(TransformPythonValue(value, LogicalType::UNKNOWN, false));
1775: 	}
1776: 	return args;
1777: }
1778: 
1779: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::DefaultConnection() {
1780: 	if (!default_connection) {
1781: 		py::dict config_dict;
1782: 		default_connection = DuckDBPyConnection::Connect(py::str(":memory:"), false, config_dict);
1783: 	}
1784: 	return default_connection;
1785: }
1786: 
1787: PythonImportCache *DuckDBPyConnection::ImportCache() {
1788: 	if (!import_cache) {
1789: 		import_cache = make_shared_ptr<PythonImportCache>();
1790: 	}
1791: 	return import_cache.get();
1792: }
1793: 
1794: ModifiedMemoryFileSystem &DuckDBPyConnection::GetObjectFileSystem() {
1795: 	if (!internal_object_filesystem) {
1796: 		D_ASSERT(!FileSystemIsRegistered("DUCKDB_INTERNAL_OBJECTSTORE"));
1797: 		auto &import_cache_py = *ImportCache();
1798: 		auto modified_memory_fs = import_cache_py.duckdb.filesystem.ModifiedMemoryFileSystem();
1799: 		if (modified_memory_fs.ptr() == nullptr) {
1800: 			throw InvalidInputException(
1801: 			    "This operation could not be completed because required module 'fsspec' is not installed");
1802: 		}
1803: 		internal_object_filesystem = make_shared_ptr<ModifiedMemoryFileSystem>(modified_memory_fs());
1804: 		auto &abstract_fs = reinterpret_cast<AbstractFileSystem &>(*internal_object_filesystem);
1805: 		RegisterFilesystem(abstract_fs);
1806: 	}
1807: 	return *internal_object_filesystem;
1808: }
1809: 
1810: bool DuckDBPyConnection::IsInteractive() {
1811: 	return DuckDBPyConnection::environment != PythonEnvironmentType::NORMAL;
1812: }
1813: 
1814: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Enter() {
1815: 	return shared_from_this();
1816: }
1817: 
1818: void DuckDBPyConnection::Exit(DuckDBPyConnection &self, const py::object &exc_type, const py::object &exc,
1819:                               const py::object &traceback) {
1820: 	self.Close();
1821: 	if (exc_type.ptr() != Py_None) {
1822: 		// Propagate the exception if any occurred
1823: 		PyErr_SetObject(exc_type.ptr(), exc.ptr());
1824: 		throw py::error_already_set();
1825: 	}
1826: }
1827: 
1828: void DuckDBPyConnection::Cleanup() {
1829: 	default_connection.reset();
1830: 	import_cache.reset();
1831: }
1832: 
1833: bool DuckDBPyConnection::IsPandasDataframe(const py::object &object) {
1834: 	if (!ModuleIsLoaded<PandasCacheItem>()) {
1835: 		return false;
1836: 	}
1837: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
1838: 	return py::isinstance(object, import_cache_py.pandas.DataFrame());
1839: }
1840: 
1841: bool DuckDBPyConnection::IsPolarsDataframe(const py::object &object) {
1842: 	if (!ModuleIsLoaded<PolarsCacheItem>()) {
1843: 		return false;
1844: 	}
1845: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
1846: 	return py::isinstance(object, import_cache_py.polars.DataFrame()) ||
1847: 	       py::isinstance(object, import_cache_py.polars.LazyFrame());
1848: }
1849: 
1850: bool IsValidNumpyDimensions(const py::handle &object, int &dim) {
1851: 	// check the dimensions of numpy arrays
1852: 	// should only be called by IsAcceptedNumpyObject
1853: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1854: 	if (!py::isinstance(object, import_cache.numpy.ndarray())) {
1855: 		return false;
1856: 	}
1857: 	auto shape = (py::cast<py::array>(object)).attr("shape");
1858: 	if (py::len(shape) != 1) {
1859: 		return false;
1860: 	}
1861: 	int cur_dim = (shape.attr("__getitem__")(0)).cast<int>();
1862: 	dim = dim == -1 ? cur_dim : dim;
1863: 	return dim == cur_dim;
1864: }
1865: NumpyObjectType DuckDBPyConnection::IsAcceptedNumpyObject(const py::object &object) {
1866: 	if (!ModuleIsLoaded<NumpyCacheItem>()) {
1867: 		return NumpyObjectType::INVALID;
1868: 	}
1869: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1870: 	if (py::isinstance(object, import_cache.numpy.ndarray())) {
1871: 		auto len = py::len((py::cast<py::array>(object)).attr("shape"));
1872: 		switch (len) {
1873: 		case 1:
1874: 			return NumpyObjectType::NDARRAY1D;
1875: 		case 2:
1876: 			return NumpyObjectType::NDARRAY2D;
1877: 		default:
1878: 			return NumpyObjectType::INVALID;
1879: 		}
1880: 	} else if (py::is_dict_like(object)) {
1881: 		int dim = -1;
1882: 		for (auto item : py::cast<py::dict>(object)) {
1883: 			if (!IsValidNumpyDimensions(item.second, dim)) {
1884: 				return NumpyObjectType::INVALID;
1885: 			}
1886: 		}
1887: 		return NumpyObjectType::DICT;
1888: 	} else if (py::is_list_like(object)) {
1889: 		int dim = -1;
1890: 		for (auto item : py::cast<py::list>(object)) {
1891: 			if (!IsValidNumpyDimensions(item, dim)) {
1892: 				return NumpyObjectType::INVALID;
1893: 			}
1894: 		}
1895: 		return NumpyObjectType::LIST;
1896: 	}
1897: 	return NumpyObjectType::INVALID;
1898: }
1899: 
1900: bool DuckDBPyConnection::IsAcceptedArrowObject(const py::object &object) {
1901: 	if (!ModuleIsLoaded<PyarrowCacheItem>()) {
1902: 		return false;
1903: 	}
1904: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
1905: 	if (py::isinstance(object, import_cache_py.pyarrow.Table()) ||
1906: 	    py::isinstance(object, import_cache_py.pyarrow.RecordBatchReader())) {
1907: 		return true;
1908: 	}
1909: 	if (!ModuleIsLoaded<PyarrowDatasetCacheItem>()) {
1910: 		return false;
1911: 	}
1912: 	return (py::isinstance(object, import_cache_py.pyarrow.dataset.Dataset()) ||
1913: 	        py::isinstance(object, import_cache_py.pyarrow.dataset.Scanner()));
1914: }
1915: 
1916: unique_lock<std::mutex> DuckDBPyConnection::AcquireConnectionLock() {
1917: 	// we first release the gil and then acquire the connection lock
1918: 	unique_lock<std::mutex> lock(py_connection_lock, std::defer_lock);
1919: 	{
1920: 		py::gil_scoped_release release;
1921: 		lock.lock();
1922: 	}
1923: 	return lock;
1924: }
1925: 
1926: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection.cpp]
[start of tools/pythonpkg/src/pyconnection/type_creation.cpp]
1: #include "duckdb_python/pyconnection/pyconnection.hpp"
2: 
3: namespace duckdb {
4: 
5: shared_ptr<DuckDBPyType> DuckDBPyConnection::MapType(const shared_ptr<DuckDBPyType> &key_type,
6:                                                      const shared_ptr<DuckDBPyType> &value_type) {
7: 	auto map_type = LogicalType::MAP(key_type->Type(), value_type->Type());
8: 	return make_shared_ptr<DuckDBPyType>(map_type);
9: }
10: 
11: shared_ptr<DuckDBPyType> DuckDBPyConnection::ListType(const shared_ptr<DuckDBPyType> &type) {
12: 	auto array_type = LogicalType::LIST(type->Type());
13: 	return make_shared_ptr<DuckDBPyType>(array_type);
14: }
15: 
16: shared_ptr<DuckDBPyType> DuckDBPyConnection::ArrayType(const shared_ptr<DuckDBPyType> &type, idx_t size) {
17: 	auto array_type = LogicalType::ARRAY(type->Type(), size);
18: 	return make_shared_ptr<DuckDBPyType>(array_type);
19: }
20: 
21: static child_list_t<LogicalType> GetChildList(const py::object &container) {
22: 	child_list_t<LogicalType> types;
23: 	if (py::isinstance<py::list>(container)) {
24: 		const py::list &fields = container;
25: 		idx_t i = 1;
26: 		for (auto &item : fields) {
27: 			shared_ptr<DuckDBPyType> pytype;
28: 			if (!py::try_cast<shared_ptr<DuckDBPyType>>(item, pytype)) {
29: 				string actual_type = py::str(item.get_type());
30: 				throw InvalidInputException("object has to be a list of DuckDBPyType's, not '%s'", actual_type);
31: 			}
32: 			types.push_back(std::make_pair(StringUtil::Format("v%d", i++), pytype->Type()));
33: 		}
34: 		return types;
35: 	} else if (py::isinstance<py::dict>(container)) {
36: 		const py::dict &fields = container;
37: 		for (auto &item : fields) {
38: 			auto &name_p = item.first;
39: 			auto &type_p = item.second;
40: 			string name = py::str(name_p);
41: 			shared_ptr<DuckDBPyType> pytype;
42: 			if (!py::try_cast<shared_ptr<DuckDBPyType>>(type_p, pytype)) {
43: 				string actual_type = py::str(type_p.get_type());
44: 				throw InvalidInputException("object has to be a list of DuckDBPyType's, not '%s'", actual_type);
45: 			}
46: 			types.push_back(std::make_pair(name, pytype->Type()));
47: 		}
48: 		return types;
49: 	} else {
50: 		string actual_type = py::str(container.get_type());
51: 		throw InvalidInputException(
52: 		    "Can not construct a child list from object of type '%s', only dict/list is supported", actual_type);
53: 	}
54: }
55: 
56: shared_ptr<DuckDBPyType> DuckDBPyConnection::StructType(const py::object &fields) {
57: 	child_list_t<LogicalType> types = GetChildList(fields);
58: 	if (types.empty()) {
59: 		throw InvalidInputException("Can not create an empty struct type!");
60: 	}
61: 	auto struct_type = LogicalType::STRUCT(std::move(types));
62: 	return make_shared_ptr<DuckDBPyType>(struct_type);
63: }
64: 
65: shared_ptr<DuckDBPyType> DuckDBPyConnection::UnionType(const py::object &members) {
66: 	child_list_t<LogicalType> types = GetChildList(members);
67: 
68: 	if (types.empty()) {
69: 		throw InvalidInputException("Can not create an empty union type!");
70: 	}
71: 	auto union_type = LogicalType::UNION(std::move(types));
72: 	return make_shared_ptr<DuckDBPyType>(union_type);
73: }
74: 
75: shared_ptr<DuckDBPyType> DuckDBPyConnection::EnumType(const string &name, const shared_ptr<DuckDBPyType> &type,
76:                                                       const py::list &values_p) {
77: 	throw NotImplementedException("enum_type creation method is not implemented yet");
78: }
79: 
80: shared_ptr<DuckDBPyType> DuckDBPyConnection::DecimalType(int width, int scale) {
81: 	auto decimal_type = LogicalType::DECIMAL(width, scale);
82: 	return make_shared_ptr<DuckDBPyType>(decimal_type);
83: }
84: 
85: shared_ptr<DuckDBPyType> DuckDBPyConnection::StringType(const string &collation) {
86: 	LogicalType type;
87: 	if (collation.empty()) {
88: 		type = LogicalType::VARCHAR;
89: 	} else {
90: 		type = LogicalType::VARCHAR_COLLATION(collation);
91: 	}
92: 	return make_shared_ptr<DuckDBPyType>(type);
93: }
94: 
95: shared_ptr<DuckDBPyType> DuckDBPyConnection::Type(const string &type_str) {
96: 	if (!connection) {
97: 		throw ConnectionException("Connection already closed!");
98: 	}
99: 	auto &context = *connection->context;
100: 	shared_ptr<DuckDBPyType> result;
101: 	context.RunFunctionInTransaction([&result, &type_str, &context]() {
102: 		result = make_shared_ptr<DuckDBPyType>(TransformStringToLogicalType(type_str, context));
103: 	});
104: 	return result;
105: }
106: 
107: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection/type_creation.cpp]
[start of tools/pythonpkg/src/python_udf.cpp]
1: #include "duckdb/main/query_result.hpp"
2: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
3: #include "duckdb/function/scalar_function.hpp"
4: #include "duckdb_python/pytype.hpp"
5: #include "duckdb_python/pyconnection/pyconnection.hpp"
6: #include "duckdb_python/pandas/pandas_scan.hpp"
7: #include "duckdb/common/arrow/arrow.hpp"
8: #include "duckdb/common/arrow/arrow_converter.hpp"
9: #include "duckdb/common/arrow/arrow_wrapper.hpp"
10: #include "duckdb/common/arrow/arrow_appender.hpp"
11: #include "duckdb/common/arrow/result_arrow_wrapper.hpp"
12: #include "duckdb_python/arrow/arrow_array_stream.hpp"
13: #include "duckdb/function/table/arrow.hpp"
14: #include "duckdb/function/function.hpp"
15: #include "duckdb_python/numpy/numpy_scan.hpp"
16: #include "duckdb_python/arrow/arrow_export_utils.hpp"
17: #include "duckdb/common/types/arrow_aux_data.hpp"
18: #include "duckdb/parser/tableref/table_function_ref.hpp"
19: 
20: namespace duckdb {
21: 
22: static py::list ConvertToSingleBatch(vector<LogicalType> &types, vector<string> &names, DataChunk &input,
23:                                      const ClientProperties &options) {
24: 	ArrowSchema schema;
25: 	ArrowConverter::ToArrowSchema(&schema, types, names, options);
26: 
27: 	py::list single_batch;
28: 	ArrowAppender appender(types, STANDARD_VECTOR_SIZE, options);
29: 	appender.Append(input, 0, input.size(), input.size());
30: 	auto array = appender.Finalize();
31: 	TransformDuckToArrowChunk(schema, array, single_batch);
32: 	return single_batch;
33: }
34: 
35: static py::object ConvertDataChunkToPyArrowTable(DataChunk &input, const ClientProperties &options) {
36: 	auto types = input.GetTypes();
37: 	vector<string> names;
38: 	names.reserve(types.size());
39: 	for (idx_t i = 0; i < types.size(); i++) {
40: 		names.push_back(StringUtil::Format("c%d", i));
41: 	}
42: 
43: 	return pyarrow::ToArrowTable(types, names, ConvertToSingleBatch(types, names, input, options), options);
44: }
45: 
46: static void ConvertPyArrowToDataChunk(const py::object &table, Vector &out, ClientContext &context, idx_t count) {
47: 
48: 	// Create the stream factory from the Table object
49: 	auto stream_factory = make_uniq<PythonTableArrowArrayStreamFactory>(table.ptr(), context.GetClientProperties());
50: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
51: 	auto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;
52: 
53: 	// Get the functions we need
54: 	auto function = ArrowTableFunction::ArrowScanFunction;
55: 	auto bind = ArrowTableFunction::ArrowScanBind;
56: 	auto init_global = ArrowTableFunction::ArrowScanInitGlobal;
57: 	auto init_local = ArrowTableFunction::ArrowScanInitLocalInternal;
58: 
59: 	// Prepare the inputs for the bind
60: 	vector<Value> children;
61: 	children.reserve(3);
62: 	children.push_back(Value::POINTER(CastPointerToValue(stream_factory.get())));
63: 	children.push_back(Value::POINTER(CastPointerToValue(stream_factory_produce)));
64: 	children.push_back(Value::POINTER(CastPointerToValue(stream_factory_get_schema)));
65: 	named_parameter_map_t named_params;
66: 	vector<LogicalType> input_types;
67: 	vector<string> input_names;
68: 
69: 	TableFunctionRef empty;
70: 	TableFunction dummy_table_function;
71: 	dummy_table_function.name = "ConvertPyArrowToDataChunk";
72: 	TableFunctionBindInput bind_input(children, named_params, input_types, input_names, nullptr, nullptr,
73: 	                                  dummy_table_function, empty);
74: 	vector<LogicalType> return_types;
75: 	vector<string> return_names;
76: 
77: 	auto bind_data = bind(context, bind_input, return_types, return_names);
78: 
79: 	if (return_types.size() != 1) {
80: 		throw InvalidInputException(
81: 		    "The returned table from a pyarrow scalar udf should only contain one column, found %d",
82: 		    return_types.size());
83: 	}
84: 	// if (return_types[0] != out.GetType()) {
85: 	//	throw InvalidInputException("The type of the returned array (%s) does not match the expected type: '%s'", )
86: 	//}
87: 
88: 	DataChunk result;
89: 	// Reserve for STANDARD_VECTOR_SIZE instead of count, in case the returned table contains too many tuples
90: 	result.Initialize(context, return_types, STANDARD_VECTOR_SIZE);
91: 
92: 	vector<column_t> column_ids = {0};
93: 	TableFunctionInitInput input(bind_data.get(), column_ids, vector<idx_t>(), nullptr);
94: 	auto global_state = init_global(context, input);
95: 	auto local_state = init_local(context, input, global_state.get());
96: 
97: 	TableFunctionInput function_input(bind_data.get(), local_state.get(), global_state.get());
98: 	function(context, function_input, result);
99: 	if (result.size() != count) {
100: 		throw InvalidInputException("Returned pyarrow table should have %d tuples, found %d", count, result.size());
101: 	}
102: 
103: 	VectorOperations::Cast(context, result.data[0], out, count);
104: }
105: 
106: static scalar_function_t CreateVectorizedFunction(PyObject *function, PythonExceptionHandling exception_handling) {
107: 	// Through the capture of the lambda, we have access to the function pointer
108: 	// We just need to make sure that it doesn't get garbage collected
109: 	scalar_function_t func = [=](DataChunk &input, ExpressionState &state, Vector &result) -> void {
110: 		py::gil_scoped_acquire gil;
111: 
112: 		// owning references
113: 		py::object python_object;
114: 		// Convert the input datachunk to pyarrow
115: 		ClientProperties options;
116: 
117: 		if (state.HasContext()) {
118: 			auto &context = state.GetContext();
119: 			options = context.GetClientProperties();
120: 		}
121: 
122: 		auto pyarrow_table = ConvertDataChunkToPyArrowTable(input, options);
123: 		py::tuple column_list = pyarrow_table.attr("columns");
124: 
125: 		auto count = input.size();
126: 
127: 		// Call the function
128: 		auto ret = PyObject_CallObject(function, column_list.ptr());
129: 		if (ret == nullptr && PyErr_Occurred()) {
130: 			if (exception_handling == PythonExceptionHandling::FORWARD_ERROR) {
131: 				auto exception = py::error_already_set();
132: 				throw InvalidInputException("Python exception occurred while executing the UDF: %s", exception.what());
133: 			} else if (exception_handling == PythonExceptionHandling::RETURN_NULL) {
134: 				PyErr_Clear();
135: 				python_object = py::module_::import("pyarrow").attr("nulls")(count);
136: 			} else {
137: 				throw NotImplementedException("Exception handling type not implemented");
138: 			}
139: 		} else {
140: 			python_object = py::reinterpret_steal<py::object>(ret);
141: 		}
142: 		if (!py::isinstance(python_object, py::module_::import("pyarrow").attr("lib").attr("Table"))) {
143: 			// Try to convert into a table
144: 			py::list single_array(1);
145: 			py::list single_name(1);
146: 
147: 			single_array[0] = python_object;
148: 			single_name[0] = "c0";
149: 			try {
150: 				python_object = py::module_::import("pyarrow").attr("lib").attr("Table").attr("from_arrays")(
151: 				    single_array, py::arg("names") = single_name);
152: 			} catch (py::error_already_set &) {
153: 				throw InvalidInputException("Could not convert the result into an Arrow Table");
154: 			}
155: 		}
156: 		// Convert the pyarrow result back to a DuckDB datachunk
157: 		ConvertPyArrowToDataChunk(python_object, result, state.GetContext(), count);
158: 
159: 		if (input.size() == 1) {
160: 			result.SetVectorType(VectorType::CONSTANT_VECTOR);
161: 		}
162: 	};
163: 	return func;
164: }
165: 
166: static scalar_function_t CreateNativeFunction(PyObject *function, PythonExceptionHandling exception_handling,
167:                                               const ClientProperties &client_properties) {
168: 	// Through the capture of the lambda, we have access to the function pointer
169: 	// We just need to make sure that it doesn't get garbage collected
170: 	scalar_function_t func = [=](DataChunk &input, ExpressionState &state, Vector &result) -> void { // NOLINT
171: 		py::gil_scoped_acquire gil;
172: 
173: 		// owning references
174: 		vector<py::object> python_objects;
175: 		vector<PyObject *> python_results;
176: 		python_results.resize(input.size());
177: 		for (idx_t row = 0; row < input.size(); row++) {
178: 
179: 			auto bundled_parameters = py::tuple((int)input.ColumnCount());
180: 			for (idx_t i = 0; i < input.ColumnCount(); i++) {
181: 				// Fill the tuple with the arguments for this row
182: 				auto &column = input.data[i];
183: 				auto value = column.GetValue(row);
184: 				bundled_parameters[i] = PythonObject::FromValue(value, column.GetType(), client_properties);
185: 			}
186: 
187: 			// Call the function
188: 			auto ret = PyObject_CallObject(function, bundled_parameters.ptr());
189: 			if (ret == nullptr && PyErr_Occurred()) {
190: 				if (exception_handling == PythonExceptionHandling::FORWARD_ERROR) {
191: 					auto exception = py::error_already_set();
192: 					throw InvalidInputException("Python exception occurred while executing the UDF: %s",
193: 					                            exception.what());
194: 				} else if (exception_handling == PythonExceptionHandling::RETURN_NULL) {
195: 					PyErr_Clear();
196: 					ret = Py_None;
197: 				} else {
198: 					throw NotImplementedException("Exception handling type not implemented");
199: 				}
200: 			}
201: 			python_objects.push_back(py::reinterpret_steal<py::object>(ret));
202: 			python_results[row] = ret;
203: 		}
204: 
205: 		NumpyScan::ScanObjectColumn(python_results.data(), sizeof(PyObject *), input.size(), 0, result);
206: 		if (input.size() == 1) {
207: 			result.SetVectorType(VectorType::CONSTANT_VECTOR);
208: 		}
209: 	};
210: 	return func;
211: }
212: 
213: namespace {
214: 
215: struct ParameterKind {
216: 	enum class Type : uint8_t { POSITIONAL_ONLY, POSITIONAL_OR_KEYWORD, VAR_POSITIONAL, KEYWORD_ONLY, VAR_KEYWORD };
217: 	static ParameterKind::Type FromString(const string &type_str) {
218: 		if (type_str == "POSITIONAL_ONLY") {
219: 			return Type::POSITIONAL_ONLY;
220: 		} else if (type_str == "POSITIONAL_OR_KEYWORD") {
221: 			return Type::POSITIONAL_OR_KEYWORD;
222: 		} else if (type_str == "VAR_POSITIONAL") {
223: 			return Type::VAR_POSITIONAL;
224: 		} else if (type_str == "KEYWORD_ONLY") {
225: 			return Type::KEYWORD_ONLY;
226: 		} else if (type_str == "VAR_KEYWORD") {
227: 			return Type::VAR_KEYWORD;
228: 		} else {
229: 			throw NotImplementedException("ParameterKindType not implemented for '%s'", type_str);
230: 		}
231: 	}
232: };
233: 
234: struct PythonUDFData {
235: public:
236: 	PythonUDFData(const string &name, bool vectorized, FunctionNullHandling null_handling)
237: 	    : name(name), null_handling(null_handling), vectorized(vectorized) {
238: 		return_type = LogicalType::INVALID;
239: 		param_count = DConstants::INVALID_INDEX;
240: 	}
241: 
242: public:
243: 	string name;
244: 	vector<LogicalType> parameters;
245: 	LogicalType return_type;
246: 	LogicalType varargs = LogicalTypeId::INVALID;
247: 	FunctionNullHandling null_handling;
248: 	idx_t param_count;
249: 	bool vectorized;
250: 
251: public:
252: 	void Verify() {
253: 		if (return_type == LogicalType::INVALID) {
254: 			throw InvalidInputException("Could not infer the return type, please set it explicitly");
255: 		}
256: 	}
257: 
258: 	void OverrideReturnType(const shared_ptr<DuckDBPyType> &type) {
259: 		if (!type) {
260: 			return;
261: 		}
262: 		return_type = type->Type();
263: 	}
264: 
265: 	void OverrideParameters(const py::object &parameters_p) {
266: 		if (py::none().is(parameters_p)) {
267: 			return;
268: 		}
269: 		if (!py::isinstance<py::list>(parameters_p)) {
270: 			throw InvalidInputException("Either leave 'parameters' empty, or provide a list of DuckDBPyType objects");
271: 		}
272: 
273: 		auto params = py::list(parameters_p);
274: 		if (params.size() != param_count) {
275: 			throw InvalidInputException("%d types provided, but the provided function takes %d parameters",
276: 			                            params.size(), param_count);
277: 		}
278: 		D_ASSERT(parameters.empty() || parameters.size() == param_count);
279: 		if (parameters.empty()) {
280: 			for (idx_t i = 0; i < param_count; i++) {
281: 				parameters.push_back(LogicalType::ANY);
282: 			}
283: 		}
284: 		idx_t i = 0;
285: 		for (auto &param : params) {
286: 			auto type = py::cast<shared_ptr<DuckDBPyType>>(param);
287: 			parameters[i++] = type->Type();
288: 		}
289: 	}
290: 
291: 	py::object GetSignature(const py::object &udf) {
292: 		const int32_t PYTHON_3_10_HEX = 0x030a00f0;
293: 		auto python_version = PY_VERSION_HEX;
294: 
295: 		auto signature_func = py::module_::import("inspect").attr("signature");
296: 		if (python_version >= PYTHON_3_10_HEX) {
297: 			return signature_func(udf, py::arg("eval_str") = true);
298: 		} else {
299: 			return signature_func(udf);
300: 		}
301: 	}
302: 
303: 	void AnalyzeSignature(const py::object &udf) {
304: 		auto signature = GetSignature(udf);
305: 		auto sig_params = signature.attr("parameters");
306: 		auto return_annotation = signature.attr("return_annotation");
307: 		if (!py::none().is(return_annotation)) {
308: 			shared_ptr<DuckDBPyType> pytype;
309: 			if (py::try_cast<shared_ptr<DuckDBPyType>>(return_annotation, pytype)) {
310: 				return_type = pytype->Type();
311: 			}
312: 		}
313: 		param_count = py::len(sig_params);
314: 		parameters.reserve(param_count);
315: 		auto params = py::dict(sig_params);
316: 		for (auto &item : params) {
317: 			auto &value = item.second;
318: 			shared_ptr<DuckDBPyType> pytype;
319: 			if (py::try_cast<shared_ptr<DuckDBPyType>>(value.attr("annotation"), pytype)) {
320: 				parameters.push_back(pytype->Type());
321: 			} else {
322: 				std::string kind = py::str(value.attr("kind"));
323: 				auto parameter_kind = ParameterKind::FromString(kind);
324: 				if (parameter_kind == ParameterKind::Type::VAR_POSITIONAL) {
325: 					varargs = LogicalType::ANY;
326: 				}
327: 				parameters.push_back(LogicalType::ANY);
328: 			}
329: 		}
330: 	}
331: 
332: 	ScalarFunction GetFunction(const py::function &udf, PythonExceptionHandling exception_handling, bool side_effects,
333: 	                           const ClientProperties &client_properties) {
334: 
335: 		auto &import_cache = *DuckDBPyConnection::ImportCache();
336: 		// Import this module, because importing this from a non-main thread causes a segfault
337: 		(void)import_cache.numpy.core.multiarray();
338: 
339: 		scalar_function_t func;
340: 		if (vectorized) {
341: 			func = CreateVectorizedFunction(udf.ptr(), exception_handling);
342: 		} else {
343: 			func = CreateNativeFunction(udf.ptr(), exception_handling, client_properties);
344: 		}
345: 		FunctionStability function_side_effects =
346: 		    side_effects ? FunctionStability::VOLATILE : FunctionStability::CONSISTENT;
347: 		ScalarFunction scalar_function(name, std::move(parameters), return_type, func, nullptr, nullptr, nullptr,
348: 		                               nullptr, varargs, function_side_effects, null_handling);
349: 		return scalar_function;
350: 	}
351: };
352: 
353: } // namespace
354: 
355: ScalarFunction DuckDBPyConnection::CreateScalarUDF(const string &name, const py::function &udf,
356:                                                    const py::object &parameters,
357:                                                    const shared_ptr<DuckDBPyType> &return_type, bool vectorized,
358:                                                    FunctionNullHandling null_handling,
359:                                                    PythonExceptionHandling exception_handling, bool side_effects) {
360: 	PythonUDFData data(name, vectorized, null_handling);
361: 
362: 	data.AnalyzeSignature(udf);
363: 	data.OverrideParameters(parameters);
364: 	data.OverrideReturnType(return_type);
365: 	data.Verify();
366: 	return data.GetFunction(udf, exception_handling, side_effects, connection->context->GetClientProperties());
367: }
368: 
369: } // namespace duckdb
[end of tools/pythonpkg/src/python_udf.cpp]
[start of tools/pythonpkg/src/typing/pytype.cpp]
1: #include "duckdb_python/pytype.hpp"
2: #include "duckdb/common/types.hpp"
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/string_util.hpp"
5: #include "duckdb_python/pyconnection/pyconnection.hpp"
6: #include "duckdb/main/connection.hpp"
7: #include "duckdb/common/vector.hpp"
8: 
9: namespace duckdb {
10: 
11: // NOLINTNEXTLINE(readability-identifier-naming)
12: bool PyGenericAlias::check_(const py::handle &object) {
13: 	if (!ModuleIsLoaded<TypesCacheItem>()) {
14: 		return false;
15: 	}
16: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
17: 	return py::isinstance(object, import_cache.types.GenericAlias());
18: }
19: 
20: // NOLINTNEXTLINE(readability-identifier-naming)
21: bool PyUnionType::check_(const py::handle &object) {
22: 	auto types_loaded = ModuleIsLoaded<TypesCacheItem>();
23: 	auto typing_loaded = ModuleIsLoaded<TypingCacheItem>();
24: 
25: 	if (!types_loaded && !typing_loaded) {
26: 		return false;
27: 	}
28: 
29: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
30: 	if (types_loaded && py::isinstance(object, import_cache.types.UnionType())) {
31: 		return true;
32: 	}
33: 	if (typing_loaded && py::isinstance(object, import_cache.typing._UnionGenericAlias())) {
34: 		return true;
35: 	}
36: 	return false;
37: }
38: 
39: DuckDBPyType::DuckDBPyType(LogicalType type) : type(std::move(type)) {
40: }
41: 
42: bool DuckDBPyType::Equals(const shared_ptr<DuckDBPyType> &other) const {
43: 	if (!other) {
44: 		return false;
45: 	}
46: 	return type == other->type;
47: }
48: 
49: bool DuckDBPyType::EqualsString(const string &type_str) const {
50: 	return StringUtil::CIEquals(type.ToString(), type_str);
51: }
52: 
53: shared_ptr<DuckDBPyType> DuckDBPyType::GetAttribute(const string &name) const {
54: 	if (type.id() == LogicalTypeId::STRUCT || type.id() == LogicalTypeId::UNION) {
55: 		auto &children = StructType::GetChildTypes(type);
56: 		for (idx_t i = 0; i < children.size(); i++) {
57: 			auto &child = children[i];
58: 			if (StringUtil::CIEquals(child.first, name)) {
59: 				return make_shared_ptr<DuckDBPyType>(StructType::GetChildType(type, i));
60: 			}
61: 		}
62: 	}
63: 	if (type.id() == LogicalTypeId::LIST && StringUtil::CIEquals(name, "child")) {
64: 		return make_shared_ptr<DuckDBPyType>(ListType::GetChildType(type));
65: 	}
66: 	if (type.id() == LogicalTypeId::MAP) {
67: 		auto is_key = StringUtil::CIEquals(name, "key");
68: 		auto is_value = StringUtil::CIEquals(name, "value");
69: 		if (is_key) {
70: 			return make_shared_ptr<DuckDBPyType>(MapType::KeyType(type));
71: 		} else if (is_value) {
72: 			return make_shared_ptr<DuckDBPyType>(MapType::ValueType(type));
73: 		} else {
74: 			throw py::attribute_error(StringUtil::Format("Tried to get a child from a map by the name of '%s', but "
75: 			                                             "this type only has 'key' and 'value' children",
76: 			                                             name));
77: 		}
78: 	}
79: 	throw py::attribute_error(
80: 	    StringUtil::Format("Tried to get child type by the name of '%s', but this type either isn't nested, "
81: 	                       "or it doesn't have a child by that name",
82: 	                       name));
83: }
84: 
85: static LogicalType FromObject(const py::object &object);
86: 
87: namespace {
88: enum class PythonTypeObject : uint8_t {
89: 	INVALID,   // not convertible to our type
90: 	BASE,      // 'builtin' type objects
91: 	UNION,     // typing.UnionType
92: 	COMPOSITE, // list|dict types
93: 	STRUCT,    // dictionary
94: 	STRING,    // string value
95: };
96: }
97: 
98: static PythonTypeObject GetTypeObjectType(const py::handle &type_object) {
99: 	if (py::isinstance<py::type>(type_object)) {
100: 		return PythonTypeObject::BASE;
101: 	}
102: 	if (py::isinstance<py::str>(type_object)) {
103: 		return PythonTypeObject::STRING;
104: 	}
105: 	if (py::isinstance<PyGenericAlias>(type_object)) {
106: 		return PythonTypeObject::COMPOSITE;
107: 	}
108: 	if (py::isinstance<py::dict>(type_object)) {
109: 		return PythonTypeObject::STRUCT;
110: 	}
111: 	if (py::isinstance<PyUnionType>(type_object)) {
112: 		return PythonTypeObject::UNION;
113: 	}
114: 	return PythonTypeObject::INVALID;
115: }
116: 
117: static LogicalType FromString(const string &type_str, shared_ptr<DuckDBPyConnection> connection) {
118: 	if (!connection) {
119: 		connection = DuckDBPyConnection::DefaultConnection();
120: 	}
121: 	return TransformStringToLogicalType(type_str, *connection->connection->context);
122: }
123: 
124: static bool FromNumpyType(const py::object &type, LogicalType &result) {
125: 	// Since this is a type, we have to create an instance from it first.
126: 	auto obj = type();
127: 	// We convert these to string because the underlying physical
128: 	// types of a numpy type aren't consistent on every platform
129: 	string type_str = py::str(obj.attr("dtype"));
130: 	if (type_str == "bool") {
131: 		result = LogicalType::BOOLEAN;
132: 	} else if (type_str == "int8") {
133: 		result = LogicalType::TINYINT;
134: 	} else if (type_str == "uint8") {
135: 		result = LogicalType::UTINYINT;
136: 	} else if (type_str == "int16") {
137: 		result = LogicalType::SMALLINT;
138: 	} else if (type_str == "uint16") {
139: 		result = LogicalType::USMALLINT;
140: 	} else if (type_str == "int32") {
141: 		result = LogicalType::INTEGER;
142: 	} else if (type_str == "uint32") {
143: 		result = LogicalType::UINTEGER;
144: 	} else if (type_str == "int64") {
145: 		result = LogicalType::BIGINT;
146: 	} else if (type_str == "uint64") {
147: 		result = LogicalType::UBIGINT;
148: 	} else if (type_str == "float16") {
149: 		// FIXME: should we even support this?
150: 		result = LogicalType::FLOAT;
151: 	} else if (type_str == "float32") {
152: 		result = LogicalType::FLOAT;
153: 	} else if (type_str == "float64") {
154: 		result = LogicalType::DOUBLE;
155: 	} else {
156: 		return false;
157: 	}
158: 	return true;
159: }
160: 
161: static LogicalType FromType(const py::type &obj) {
162: 	py::module_ builtins = py::module_::import("builtins");
163: 	if (obj.is(builtins.attr("str"))) {
164: 		return LogicalType::VARCHAR;
165: 	}
166: 	if (obj.is(builtins.attr("int"))) {
167: 		return LogicalType::BIGINT;
168: 	}
169: 	if (obj.is(builtins.attr("bytearray"))) {
170: 		return LogicalType::BLOB;
171: 	}
172: 	if (obj.is(builtins.attr("bytes"))) {
173: 		return LogicalType::BLOB;
174: 	}
175: 	if (obj.is(builtins.attr("float"))) {
176: 		return LogicalType::DOUBLE;
177: 	}
178: 	if (obj.is(builtins.attr("bool"))) {
179: 		return LogicalType::BOOLEAN;
180: 	}
181: 
182: 	LogicalType result;
183: 	if (FromNumpyType(obj, result)) {
184: 		return result;
185: 	}
186: 
187: 	throw py::type_error("Could not convert from unknown 'type' to DuckDBPyType");
188: }
189: 
190: static bool IsMapType(const py::tuple &args) {
191: 	if (args.size() != 2) {
192: 		return false;
193: 	}
194: 	for (auto &arg : args) {
195: 		if (GetTypeObjectType(arg) == PythonTypeObject::INVALID) {
196: 			return false;
197: 		}
198: 	}
199: 	return true;
200: }
201: 
202: static py::tuple FilterNones(const py::tuple &args) {
203: 	py::list result;
204: 
205: 	for (const auto &arg : args) {
206: 		py::object object = py::reinterpret_borrow<py::object>(arg);
207: 		if (object.is(py::none().get_type())) {
208: 			continue;
209: 		}
210: 		result.append(object);
211: 	}
212: 	return py::tuple(result);
213: }
214: 
215: static LogicalType FromUnionTypeInternal(const py::tuple &args) {
216: 	idx_t index = 1;
217: 	child_list_t<LogicalType> members;
218: 
219: 	for (const auto &arg : args) {
220: 		auto name = StringUtil::Format("u%d", index++);
221: 		py::object object = py::reinterpret_borrow<py::object>(arg);
222: 		members.push_back(make_pair(name, FromObject(object)));
223: 	}
224: 
225: 	return LogicalType::UNION(std::move(members));
226: }
227: 
228: static LogicalType FromUnionType(const py::object &obj) {
229: 	py::tuple args = obj.attr("__args__");
230: 
231: 	// Optional inserts NoneType into the Union
232: 	// all types are nullable in DuckDB so we just filter the Nones
233: 	auto filtered_args = FilterNones(args);
234: 	if (filtered_args.size() == 1) {
235: 		// If only a single type is left, dont construct a UNION
236: 		return FromObject(filtered_args[0]);
237: 	}
238: 	return FromUnionTypeInternal(filtered_args);
239: };
240: 
241: static LogicalType FromGenericAlias(const py::object &obj) {
242: 	py::module_ builtins = py::module_::import("builtins");
243: 	py::module_ types = py::module_::import("types");
244: 	auto generic_alias = types.attr("GenericAlias");
245: 	D_ASSERT(py::isinstance(obj, generic_alias));
246: 	auto origin = obj.attr("__origin__");
247: 	py::tuple args = obj.attr("__args__");
248: 
249: 	if (origin.is(builtins.attr("list"))) {
250: 		if (args.size() != 1) {
251: 			throw NotImplementedException("Can only create a LIST from a single type");
252: 		}
253: 		return LogicalType::LIST(FromObject(args[0]));
254: 	}
255: 	if (origin.is(builtins.attr("dict"))) {
256: 		if (IsMapType(args)) {
257: 			return LogicalType::MAP(FromObject(args[0]), FromObject(args[1]));
258: 		} else {
259: 			throw NotImplementedException("Can only create a MAP from a dict if args is formed correctly");
260: 		}
261: 	}
262: 	string origin_type = py::str(origin);
263: 	throw InvalidInputException("Could not convert from '%s' to DuckDBPyType", origin_type);
264: }
265: 
266: static LogicalType FromDictionary(const py::object &obj) {
267: 	auto dict = py::reinterpret_steal<py::dict>(obj);
268: 	child_list_t<LogicalType> children;
269: 	children.reserve(dict.size());
270: 	for (auto &item : dict) {
271: 		auto &name_p = item.first;
272: 		auto type_p = py::reinterpret_borrow<py::object>(item.second);
273: 		string name = py::str(name_p);
274: 		auto type = FromObject(type_p);
275: 		children.push_back(std::make_pair(name, std::move(type)));
276: 	}
277: 	return LogicalType::STRUCT(std::move(children));
278: }
279: 
280: static LogicalType FromObject(const py::object &object) {
281: 	auto object_type = GetTypeObjectType(object);
282: 	switch (object_type) {
283: 	case PythonTypeObject::BASE: {
284: 		return FromType(object);
285: 	}
286: 	case PythonTypeObject::COMPOSITE: {
287: 		return FromGenericAlias(object);
288: 	}
289: 	case PythonTypeObject::STRUCT: {
290: 		return FromDictionary(object);
291: 	}
292: 	case PythonTypeObject::UNION: {
293: 		return FromUnionType(object);
294: 	}
295: 	case PythonTypeObject::STRING: {
296: 		auto string_value = std::string(py::str(object));
297: 		return FromString(string_value, nullptr);
298: 	}
299: 	default: {
300: 		string actual_type = py::str(object.get_type());
301: 		throw NotImplementedException("Could not convert from object of type '%s' to DuckDBPyType", actual_type);
302: 	}
303: 	}
304: }
305: 
306: void DuckDBPyType::Initialize(py::handle &m) {
307: 	auto type_module = py::class_<DuckDBPyType, shared_ptr<DuckDBPyType>>(m, "DuckDBPyType", py::module_local());
308: 
309: 	type_module.def("__repr__", &DuckDBPyType::ToString, "Stringified representation of the type object");
310: 	type_module.def("__eq__", &DuckDBPyType::Equals, "Compare two types for equality", py::arg("other"));
311: 	type_module.def("__eq__", &DuckDBPyType::EqualsString, "Compare two types for equality", py::arg("other"));
312: 	type_module.def_property_readonly("id", &DuckDBPyType::GetId);
313: 	type_module.def_property_readonly("children", &DuckDBPyType::Children);
314: 	type_module.def(py::init<>([](const string &type_str, shared_ptr<DuckDBPyConnection> connection = nullptr) {
315: 		auto ltype = FromString(type_str, std::move(connection));
316: 		return make_shared_ptr<DuckDBPyType>(ltype);
317: 	}));
318: 	type_module.def(py::init<>([](const PyGenericAlias &obj) {
319: 		auto ltype = FromGenericAlias(obj);
320: 		return make_shared_ptr<DuckDBPyType>(ltype);
321: 	}));
322: 	type_module.def(py::init<>([](const PyUnionType &obj) {
323: 		auto ltype = FromUnionType(obj);
324: 		return make_shared_ptr<DuckDBPyType>(ltype);
325: 	}));
326: 	type_module.def(py::init<>([](const py::object &obj) {
327: 		auto ltype = FromObject(obj);
328: 		return make_shared_ptr<DuckDBPyType>(ltype);
329: 	}));
330: 	type_module.def("__getattr__", &DuckDBPyType::GetAttribute, "Get the child type by 'name'", py::arg("name"));
331: 	type_module.def("__getitem__", &DuckDBPyType::GetAttribute, "Get the child type by 'name'", py::arg("name"));
332: 
333: 	py::implicitly_convertible<py::object, DuckDBPyType>();
334: 	py::implicitly_convertible<py::str, DuckDBPyType>();
335: 	py::implicitly_convertible<PyGenericAlias, DuckDBPyType>();
336: 	py::implicitly_convertible<PyUnionType, DuckDBPyType>();
337: }
338: 
339: string DuckDBPyType::ToString() const {
340: 	return type.ToString();
341: }
342: 
343: py::list DuckDBPyType::Children() const {
344: 
345: 	switch (type.id()) {
346: 	case LogicalTypeId::LIST:
347: 	case LogicalTypeId::STRUCT:
348: 	case LogicalTypeId::UNION:
349: 	case LogicalTypeId::MAP:
350: 	case LogicalTypeId::ARRAY:
351: 	case LogicalTypeId::ENUM:
352: 	case LogicalTypeId::DECIMAL:
353: 		break;
354: 	default:
355: 		throw InvalidInputException("This type is not nested so it doesn't have children");
356: 	}
357: 
358: 	py::list children;
359: 	auto id = type.id();
360: 	if (id == LogicalTypeId::LIST) {
361: 		children.append(py::make_tuple("child", make_shared_ptr<DuckDBPyType>(ListType::GetChildType(type))));
362: 		return children;
363: 	}
364: 	if (id == LogicalTypeId::ARRAY) {
365: 		children.append(py::make_tuple("child", make_shared_ptr<DuckDBPyType>(ArrayType::GetChildType(type))));
366: 		children.append(py::make_tuple("size", ArrayType::GetSize(type)));
367: 		return children;
368: 	}
369: 	if (id == LogicalTypeId::ENUM) {
370: 		auto &values_insert_order = EnumType::GetValuesInsertOrder(type);
371: 		auto strings = FlatVector::GetData<string_t>(values_insert_order);
372: 		py::list strings_list;
373: 		for (size_t i = 0; i < EnumType::GetSize(type); i++) {
374: 			strings_list.append(py::str(strings[i].GetString()));
375: 		}
376: 		children.append(py::make_tuple("values", strings_list));
377: 		return children;
378: 	}
379: 	if (id == LogicalTypeId::STRUCT || id == LogicalTypeId::UNION) {
380: 		auto &struct_children = StructType::GetChildTypes(type);
381: 		for (idx_t i = 0; i < struct_children.size(); i++) {
382: 			auto &child = struct_children[i];
383: 			children.append(
384: 			    py::make_tuple(child.first, make_shared_ptr<DuckDBPyType>(StructType::GetChildType(type, i))));
385: 		}
386: 		return children;
387: 	}
388: 	if (id == LogicalTypeId::MAP) {
389: 		children.append(py::make_tuple("key", make_shared_ptr<DuckDBPyType>(MapType::KeyType(type))));
390: 		children.append(py::make_tuple("value", make_shared_ptr<DuckDBPyType>(MapType::ValueType(type))));
391: 		return children;
392: 	}
393: 	if (id == LogicalTypeId::DECIMAL) {
394: 		children.append(py::make_tuple("precision", DecimalType::GetWidth(type)));
395: 		children.append(py::make_tuple("scale", DecimalType::GetScale(type)));
396: 		return children;
397: 	}
398: 	throw InternalException("Children is not implemented for this type");
399: }
400: 
401: string DuckDBPyType::GetId() const {
402: 	return StringUtil::Lower(LogicalTypeIdToString(type.id()));
403: }
404: 
405: const LogicalType &DuckDBPyType::Type() const {
406: 	return type;
407: }
408: 
409: } // namespace duckdb
[end of tools/pythonpkg/src/typing/pytype.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: