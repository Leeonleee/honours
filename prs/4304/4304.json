{
  "repo": "duckdb/duckdb",
  "pull_number": 4304,
  "instance_id": "duckdb__duckdb-4304",
  "issue_numbers": [
    "4208"
  ],
  "base_commit": "93af1c0582399dc301b1058e7066a2bae667ef27",
  "patch": "diff --git a/.github/regression/micro.csv b/.github/regression/micro.csv\nindex 23633335c8e3..a665044fc762 100644\n--- a/.github/regression/micro.csv\n+++ b/.github/regression/micro.csv\n@@ -9,3 +9,4 @@ benchmark/micro/cast/cast_timestamp_string.benchmark\n benchmark/micro/limit/parallel_limit.benchmark\n benchmark/micro/filter/parallel_complex_filter.benchmark\n benchmark/micro/catalog/add_column_empty.benchmark\n+benchmark/micro/groupby-parallel/many_groups_large_values_small.benchmark\ndiff --git a/benchmark/micro/groupby-parallel/many_groups_large_values.benchmark b/benchmark/micro/groupby-parallel/many_groups_large_values.benchmark\nnew file mode 100644\nindex 000000000000..db4e5907b041\n--- /dev/null\n+++ b/benchmark/micro/groupby-parallel/many_groups_large_values.benchmark\n@@ -0,0 +1,16 @@\n+# name: benchmark/micro/groupby-parallel/many_groups_large_values.benchmark\n+# description: Aggregation with many groups and large values\n+# group: [groupby-parallel]\n+\n+name Grouped Aggregate (S, 100M)\n+group aggregate\n+subgroup parallel\n+\n+load\n+create temporary table df as\n+select (i/2 * 5000000000)::bigint AS g1,\n+       (case when i%2=0 then 0 else 5000000000 end)::BIGINT g2,\n+       ((random() * 100000000) * 5000000000)::BIGINT AS d from range(100000000) t(i);\n+\n+run\n+select sum(d), count(*) from df group by g1, g2 limit 1;\ndiff --git a/benchmark/micro/groupby-parallel/many_groups_large_values_small.benchmark b/benchmark/micro/groupby-parallel/many_groups_large_values_small.benchmark\nnew file mode 100644\nindex 000000000000..91e3f8903ecb\n--- /dev/null\n+++ b/benchmark/micro/groupby-parallel/many_groups_large_values_small.benchmark\n@@ -0,0 +1,16 @@\n+# name: benchmark/micro/groupby-parallel/many_groups_large_values_small.benchmark\n+# description: Aggregation with many groups and large values\n+# group: [groupby-parallel]\n+\n+name Grouped Aggregate (S, 10M)\n+group aggregate\n+subgroup parallel\n+\n+load\n+create temporary table df as\n+select (i/2 * 5000000000)::bigint AS g1,\n+       (case when i%2=0 then 0 else 5000000000 end)::BIGINT g2,\n+       ((random() * 100000000) * 5000000000)::BIGINT AS d from range(10000000) t(i);\n+\n+run\n+select sum(d), count(*) from df group by g1, g2 limit 1;\ndiff --git a/src/common/checksum.cpp b/src/common/checksum.cpp\nindex f7ebf95ba93c..c1fab841f547 100644\n--- a/src/common/checksum.cpp\n+++ b/src/common/checksum.cpp\n@@ -3,13 +3,17 @@\n \n namespace duckdb {\n \n+hash_t Checksum(uint64_t x) {\n+\treturn x * UINT64_C(0xbf58476d1ce4e5b9);\n+}\n+\n uint64_t Checksum(uint8_t *buffer, size_t size) {\n \tuint64_t result = 5381;\n \tuint64_t *ptr = (uint64_t *)buffer;\n \tsize_t i;\n-\t// for efficiency, we first hash uint64_t values\n+\t// for efficiency, we first checksum uint64_t values\n \tfor (i = 0; i < size / 8; i++) {\n-\t\tresult ^= Hash(ptr[i]);\n+\t\tresult ^= Checksum(ptr[i]);\n \t}\n \tif (size - i * 8 > 0) {\n \t\t// the remaining 0-7 bytes we hash using a string hash\ndiff --git a/src/include/duckdb/common/types/hash.hpp b/src/include/duckdb/common/types/hash.hpp\nindex a3fe08bef2f0..1663fdae1480 100644\n--- a/src/include/duckdb/common/types/hash.hpp\n+++ b/src/include/duckdb/common/types/hash.hpp\n@@ -20,7 +20,12 @@ struct string_t;\n // see: https://nullprogram.com/blog/2018/07/31/\n \n inline hash_t murmurhash64(uint64_t x) {\n-\treturn x * UINT64_C(0xbf58476d1ce4e5b9);\n+\tx ^= x >> 32;\n+\tx *= 0xd6e8feb86659fd93U;\n+\tx ^= x >> 32;\n+\tx *= 0xd6e8feb86659fd93U;\n+\tx ^= x >> 32;\n+\treturn x;\n }\n \n inline hash_t murmurhash32(uint32_t x) {\n",
  "test_patch": "diff --git a/test/sql/function/generic/hash_func.test b/test/sql/function/generic/hash_func.test\nindex 17c01c02e2aa..64c2670e26e7 100644\n--- a/test/sql/function/generic/hash_func.test\n+++ b/test/sql/function/generic/hash_func.test\n@@ -43,8 +43,8 @@ CREATE TABLE structs AS\n query II\n SELECT s, HASH(s) FROM structs\n ----\n-{'i': 5, 's': string}\t740871714876126105\n-{'i': -2, 's': NULL}\t17099606643559359783\n+{'i': 5, 's': string}\t14281775631523469912\n+{'i': -2, 's': NULL}\t10548482790516923454\n {'i': NULL, 's': not null}\t3003122281486162523\n {'i': NULL, 's': NULL}\t3284068718015453704\n NULL\t3284068718015453704\n@@ -64,11 +64,11 @@ CREATE TABLE lists AS\n query II\n SELECT li, HASH(li) FROM lists\n ----\n-[1]\t13787848793156543929\n-[1, 2]\t17040374892423656643\n+[1]\t4717996019076358352\n+[1, 2]\t9505922914455077890\n []\t13787848793156543929\n-[1, 2, 3]\t7476886050421391040\n-[1, 2, 3, 4, 5]\t14066138495847501977\n+[1, 2, 3]\t17668602753667955124\n+[1, 2, 3, 4, 5]\t15108644377358177276\n NULL\t13787848793156543929\n \n # These should all be different\n@@ -97,11 +97,11 @@ CREATE TABLE maps AS\n query II\n SELECT m, HASH(m) FROM maps\n ----\n-{1=TGTA}\t17453441119065739993\n-{1=CGGT, 2=CCTC}\t17559095502144960275\n+{1=TGTA}\t18414860414590184248\n+{1=CGGT, 2=CCTC}\t5600317661921613618\n {}\t13787848793156543929\n-{1=TCTA, 2=NULL, 3=CGGT}\t2792451321112537359\n-{1=TGTA, 2=CGGT, 3=CCTC, 4=TCTA, 5=AGGG}\t4809790237298330474\n+{1=TCTA, 2=NULL, 3=CGGT}\t6544980779747424731\n+{1=TGTA, 2=CGGT, 3=CCTC, 4=TCTA, 5=AGGG}\t16668895355375581271\n NULL\t13787848793156543929\n \n # Enums\n@@ -141,15 +141,15 @@ query II\n SELECT r, HASH(r) FROM enums;\n ----\n black\t0\n-brown\t13787848793156543929\n-red\t9128953512603536242\n-orange\t4470058232050528555\n-yellow\t18257907025207072484\n-green\t13599011744654064797\n-blue\t8940116464101057110\n-violet\t4281221183548049423\n-grey\t18069069976704593352\n-white\t13410174696151585665\n+brown\t4717996019076358352\n+red\t2060787363917578834\n+orange\t8131803788478518982\n+yellow\t8535942711051191036\n+green\t4244145009296420692\n+blue\t8888402906861678137\n+violet\t8736873150706563146\n+grey\t14111048738911615569\n+white\t17319221087726947361\n NULL\t13787848793156543929\n \n #\n@@ -168,45 +168,45 @@ query II\n SELECT r, HASH(r, 'capacitor') FROM enums;\n ----\n black\t7369304742611425093\n-brown\t17622053087933425908\n-red\t4887319798539380775\n-orange\t16007009976945076310\n-yellow\t3272276687517477249\n-green\t13525024195354487088\n-blue\t1623456562146077027\n-violet\t7334323862924159634\n-grey\t17302236295104785101\n-white\t5719861276895931004\n+brown\t17958022323911528725\n+red\t15049238273627934727\n+orange\t2215118097084515411\n+yellow\t10230008431092204377\n+green\t8631602209256161521\n+blue\t9137195300565087092\n+violet\t15840250322969774143\n+grey\t10002041810586422988\n+white\t7490901544426217372\n NULL\t17622053087933425908\n \n query II\n SELECT r, HASH('2022-02-12'::DATE, r) FROM enums;\n ----\n-black\t16459750633599831531\n-brown\t6572019127167117394\n-red\t11158874397277898393\n-orange\t15736722485037776064\n-yellow\t1805339037354498831\n-green\t6401201528186794358\n-blue\t10988056798030196669\n-violet\t16070308044331337188\n-grey\t2210982190410211363\n-white\t6806703926582615658\n-NULL\t6572019127167117394\n+black\t3894059433588258326\n+brown\t8607445473131337414\n+red\t3067823654208451652\n+orange\t5103565852972245200\n+yellow\t4647636508562683114\n+green\t931190734685821250\n+blue\t5571948026949490735\n+violet\t5707716297828748892\n+grey\t17716877796313500743\n+white\t14289993898341317175\n+NULL\t9895038406959666095\n \n query II\n SELECT r, HASH(r, r) FROM enums;\n ----\n black\t0\n-brown\t3284068718015453704\n-red\t6568137436030907408\n-orange\t9685416622162693688\n-yellow\t13136274872061814816\n-green\t7011800500800979688\n-blue\t924089170615835760\n-violet\t4099459648366917080\n-grey\t7825805670414078016\n-white\t10610061594230288056\n+brown\t15999884124319671936\n+red\t12251416989675881744\n+orange\t585839332880254416\n+yellow\t11369494775129347808\n+green\t3128489375186843872\n+blue\t7192567945819736584\n+violet\t14170932434502608688\n+grey\t3413701607955866584\n+white\t17431810229651948792\n NULL\t3284068718015453704\n \n #\n@@ -223,5 +223,5 @@ CREATE TABLE issue2498 AS SELECT * FROM (VALUES\n query II\n SELECT k, HASH(k) FROM issue2498\n ----\n-{'x': [{'l4': [52, 53]}, {'l4': [54, 55]}]}\t9803607596746594136\n-{'x': [{'l4': [52, 53]}, {'l4': [54, 55]}]}\t9803607596746594136\n+{'x': [{'l4': [52, 53]}, {'l4': [54, 55]}]}\t10598558919874546101\n+{'x': [{'l4': [52, 53]}, {'l4': [54, 55]}]}\t10598558919874546101\ndiff --git a/tools/pythonpkg/tests/coverage/test_pandas_categorical_coverage.py b/tools/pythonpkg/tests/coverage/test_pandas_categorical_coverage.py\nindex 7610dc0ab2c7..5289f1c8f79b 100644\n--- a/tools/pythonpkg/tests/coverage/test_pandas_categorical_coverage.py\n+++ b/tools/pythonpkg/tests/coverage/test_pandas_categorical_coverage.py\n@@ -39,8 +39,8 @@ def check_create_table(category):\n     res = conn.execute(\"SELECT x FROM t1 where x = '1'\").fetchall()\n     assert res == [('1',)]\n \n-    res =  conn.execute(\"SELECT t1.x FROM t1 inner join t2 on (t1.x = t2.x)\").fetchall()\n-    assert res == conn.execute(\"SELECT x FROM t1\").fetchall()\n+    res =  conn.execute(\"SELECT t1.x FROM t1 inner join t2 on (t1.x = t2.x) order by t1.x\").fetchall()\n+    assert res == conn.execute(\"SELECT x FROM t1 order by t1.x\").fetchall()\n     \n     res = conn.execute(\"SELECT t1.x FROM t1 inner join t2 on (t1.x = t2.y) order by t1.x\").fetchall()\n     correct_res = conn.execute(\"SELECT x FROM t1 order by x\").fetchall()\n",
  "problem_statement": "Unexpected performance drop-off (~10x) aggregating 64bit int vs 32bit int groups\n### What happens?\r\n\r\nI was looking at the [grouped aggregation benchmark](https://gist.github.com/hannes/e2599ae338d275c241c567934a13d422?permalink_comment_id=4244863#gistcomment-4244863) earlier, as referenced in [Parallel Grouped Aggregation in DuckDB](https://duckdb.org/2022/03/07/aggregate-hashtable.html), and found a fairly dramatic performance issue (which is so large I think it is likely to be a bug), just by using 64bit values (while keeping everything else about the benchmark the same).\r\n\r\nPerformance drops by a full order of magnitude (~10x) across the board.\r\n\r\n### To Reproduce\r\n\r\nRun the [grouped aggregation benchmark](https://gist.github.com/hannes/e2599ae338d275c241c567934a13d422) with the following minor change to the `experiment()` function:\r\n\r\n```python\r\n# scale by a value that will force the underlying column dtypes to 64bit\r\nscaling_factor = 5_000_000_000\r\n    \r\ng1 = np.repeat( np.arange( 0, int( n_groups / 2 ) ), int( n_rows / (n_groups / 2) ) ) * scaling_factor\r\ng2 = np.tile( np.arange( 0, 2 ), int( n_rows / 2 ) ) * scaling_factor\r\nd = np.random.randint( 0, n_rows, n_rows ) * scaling_factor\r\n```\r\nThis changes nothing about how much data is generated, it just scales the test values to guarantee that 64bit column dtypes are applied. The result is an unexpected performance falloff.\r\n\r\nResults\r\n---\r\n\r\nCharacterizing the slowdown under 32bit/64bit regimes, with `polars` as a reference implementation:\r\n\r\nrows | groups | dd:32 | dd:64 | dd:slowdown | pl:32 | pl:64 | pl:slowdown\r\n-- | -- | -- | -- | -- | -- | -- | --\r\n10,000,000 | 1,000 | 0.0140 | 0.588 | 42.0x | 0.032 | 0.088 | 2.8x\r\n10,000,000 | 10,000 | 0.0520 | 0.664 | 12.8x | 0.031 | 0.089 | 2.9x\r\n10,000,000 | 100,000 | 0.0680 | 0.599 | 8.8x | 0.038 | 0.098 | 2.6x\r\n10,000,000 | 1,000,000 | 0.1280 | 1.481 | 11.6x | 0.090 | 0.183 | 2.0x\r\n10,000,000 | 10,000,000 | 0.1980 | 2.830 | 14.3x | 0.228 | 0.307 | 1.3x\r\n100,000,000 | 1,000 | 0.2130 | 4.145 | 19.5x | 0.319 | 0.841 | 2.6x\r\n100,000,000 | 100,000 | 0.5490 | 4.530 | 8.3x | 0.415 | 0.978 | 2.4x\r\n100,000,000 | 1,000,000 | 0.8060 | 7.525 | 9.3x | 0.841 | 1.893 | 2.3x\r\n100,000,000 | 10,000,000 | 1.5920 | 16.932 | 10.6x | 1.609 | 2.984 | 1.9x\r\n100,000,000 | 100,000,000 | 2.7520 | 31.819 | 11.6x | 3.729 | 5.861 | 1.6x\r\n\r\nMoving to 64bit...\r\n\r\n* DuckDB (`dd` in the table above) slows down  by ~10x. \r\n  _this degree of drop-off is not proportional to the extra work being done_\r\n\r\n* Polars (`pl` in the table above) slows down by ~2x.\r\n  _this is broadly what you'd expect, given that 64bit values occupy twice the memory of 32bit values_\r\n\r\n<img width=\"448\" alt=\"image\" src=\"https://user-images.githubusercontent.com/2613171/180982253-b21c4657-c806-4989-aabe-f80d4dcf8490.png\">\r\n\r\n```\r\ny-axis: time in seconds (lower is better) \r\nx-axis: rows/groups bucket in the above table\r\n```\r\n\r\n### OS:\r\n\r\nmacOS (12.4)\r\n\r\n### DuckDB Version:\r\n\r\n0.4.0\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nAlexander Beedie\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [ ] I agree _(no - tested on latest officially released package, v0.4.0)_\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "Thanks for the report! This data set seems to trigger a worst-case behavior in our hash function, causing all the data to be handled in the same hash partition. Modifying the hash function fixes the issue. This is likely because the data all consists of very large numbers that are evenly spaced out. We should definitely have a look at fixing this, although I doubt this is a common data distribution in practice.",
  "created_at": "2022-08-05T20:58:28Z"
}