{
  "repo": "duckdb/duckdb",
  "pull_number": 6319,
  "instance_id": "duckdb__duckdb-6319",
  "issue_numbers": [
    "6305",
    "6305"
  ],
  "base_commit": "88290f2c1c4fdd109c70a83353a157b89b2e0b4b",
  "patch": "diff --git a/extension/json/buffered_json_reader.cpp b/extension/json/buffered_json_reader.cpp\nindex 3e4e7b196d1f..82dc06479e71 100644\n--- a/extension/json/buffered_json_reader.cpp\n+++ b/extension/json/buffered_json_reader.cpp\n@@ -90,13 +90,13 @@ void JSONFileHandle::ReadAtPosition(const char *pointer, idx_t size, idx_t posit\n idx_t JSONFileHandle::Read(const char *pointer, idx_t requested_size, bool sample_run) {\n \tD_ASSERT(requested_size != 0);\n \tif (plain_file_source) {\n-\t\tauto actual_size = file_handle->Read((void *)pointer, requested_size);\n+\t\tauto actual_size = ReadInternal(pointer, requested_size);\n \t\tread_position += actual_size;\n \t\treturn actual_size;\n \t}\n \n \tif (sample_run) { // Cache the buffer\n-\t\tauto actual_size = file_handle->Read((void *)pointer, requested_size);\n+\t\tauto actual_size = ReadInternal(pointer, requested_size);\n \t\tif (actual_size > 0) {\n \t\t\tcached_buffers.emplace_back(allocator.Allocate(actual_size));\n \t\t\tmemcpy(cached_buffers.back().get(), pointer, actual_size);\n@@ -111,7 +111,7 @@ idx_t JSONFileHandle::Read(const char *pointer, idx_t requested_size, bool sampl\n \t\tactual_size += ReadFromCache(pointer, requested_size, read_position);\n \t}\n \tif (requested_size != 0) {\n-\t\tactual_size += file_handle->Read((void *)pointer, requested_size);\n+\t\tactual_size += ReadInternal(pointer, requested_size);\n \t}\n \treturn actual_size;\n }\n@@ -119,7 +119,10 @@ idx_t JSONFileHandle::Read(const char *pointer, idx_t requested_size, bool sampl\n idx_t JSONFileHandle::ReadFromCache(const char *&pointer, idx_t &size, idx_t &position) {\n \tidx_t read_size = 0;\n \tidx_t total_offset = 0;\n-\tfor (auto &cached_buffer : cached_buffers) {\n+\n+\tidx_t cached_buffer_idx;\n+\tfor (cached_buffer_idx = 0; cached_buffer_idx < cached_buffers.size(); cached_buffer_idx++) {\n+\t\tauto &cached_buffer = cached_buffers[cached_buffer_idx];\n \t\tif (size == 0) {\n \t\t\tbreak;\n \t\t}\n@@ -135,9 +138,23 @@ idx_t JSONFileHandle::ReadFromCache(const char *&pointer, idx_t &size, idx_t &po\n \t\t}\n \t\ttotal_offset += cached_buffer.GetSize();\n \t}\n+\n \treturn read_size;\n }\n \n+idx_t JSONFileHandle::ReadInternal(const char *pointer, const idx_t requested_size) {\n+\t// Deal with reading from pipes\n+\tidx_t total_read_size = 0;\n+\twhile (total_read_size < requested_size) {\n+\t\tauto read_size = file_handle->Read((void *)(pointer + total_read_size), requested_size - total_read_size);\n+\t\tif (read_size == 0) {\n+\t\t\tbreak;\n+\t\t}\n+\t\ttotal_read_size += read_size;\n+\t}\n+\treturn total_read_size;\n+}\n+\n BufferedJSONReader::BufferedJSONReader(ClientContext &context, BufferedJSONReaderOptions options_p, string file_path_p)\n     : file_path(std::move(file_path_p)), context(context), options(std::move(options_p)), buffer_index(0) {\n }\ndiff --git a/extension/json/include/buffered_json_reader.hpp b/extension/json/include/buffered_json_reader.hpp\nindex f98e3feddc9a..64373799a1e9 100644\n--- a/extension/json/include/buffered_json_reader.hpp\n+++ b/extension/json/include/buffered_json_reader.hpp\n@@ -21,7 +21,7 @@ enum class JSONFormat : uint8_t {\n \tAUTO_DETECT = 0,\n \t//! One object after another, newlines can be anywhere\n \tUNSTRUCTURED = 1,\n-\t//! Objects are separated by newlines, newlines do not occur within objects (NDJSON)\n+\t//! Objects are separated by newlines, newlines do not occur within values (NDJSON)\n \tNEWLINE_DELIMITED = 2,\n };\n \n@@ -75,6 +75,7 @@ struct JSONFileHandle {\n \n private:\n \tidx_t ReadFromCache(const char *&pointer, idx_t &size, idx_t &position);\n+\tidx_t ReadInternal(const char *pointer, const idx_t requested_size);\n \n private:\n \t//! The JSON file handle\ndiff --git a/extension/json/include/json_scan.hpp b/extension/json/include/json_scan.hpp\nindex e1a1c9c5dd98..d662a7d4344a 100644\n--- a/extension/json/include/json_scan.hpp\n+++ b/extension/json/include/json_scan.hpp\n@@ -20,20 +20,23 @@ enum class JSONScanType : uint8_t {\n \tINVALID = 0,\n \t//! Read JSON straight to columnar data\n \tREAD_JSON = 1,\n-\t//! Read JSON objects as strings\n+\t//! Read JSON values as strings\n \tREAD_JSON_OBJECTS = 2,\n \t//! Sample run for schema detection\n \tSAMPLE = 3,\n };\n \n-enum class JSONScanTopLevelType : uint8_t {\n-\tINVALID = 0,\n-\t//! Sequential objects, e.g., NDJSON\n-\tOBJECTS = 1,\n-\t//! Top-level array containing objects\n-\tARRAY_OF_OBJECTS = 2,\n-\t//! Other, e.g., array of integer, or just strings\n-\tOTHER = 3\n+enum class JSONRecordType : uint8_t {\n+\t//! Sequential values\n+\tRECORDS = 0,\n+\t//! Array of values\n+\tARRAY_OF_RECORDS = 1,\n+\t//! Sequential non-object JSON\n+\tJSON = 2,\n+\t//! Array of non-object JSON\n+\tARRAY_OF_JSON = 3,\n+\t//! Auto-detect\n+\tAUTO = 4,\n };\n \n //! Even though LogicalTypeId is just a uint8_t, this is still needed ...\n@@ -114,8 +117,8 @@ struct JSONScanData : public TableFunctionData {\n \tvector<idx_t> valid_cols;\n \t//! Max depth we go to detect nested JSON schema (defaults to unlimited)\n \tidx_t max_depth = NumericLimits<idx_t>::Maximum();\n-\t//! Whether we're parsing objects (usually), or something else like arrays\n-\tJSONScanTopLevelType top_level_type = JSONScanTopLevelType::OBJECTS;\n+\t//! Whether we're parsing values (usually), or something else\n+\tJSONRecordType record_type = JSONRecordType::RECORDS;\n \t//! Forced date/timestamp formats\n \tstring date_format;\n \tstring timestamp_format;\n@@ -129,12 +132,13 @@ struct JSONScanData : public TableFunctionData {\n struct JSONScanInfo : public TableFunctionInfo {\n public:\n \texplicit JSONScanInfo(JSONScanType type_p = JSONScanType::INVALID, JSONFormat format_p = JSONFormat::AUTO_DETECT,\n-\t                      bool auto_detect_p = false)\n-\t    : type(type_p), format(format_p), auto_detect(auto_detect_p) {\n+\t                      JSONRecordType record_type_p = JSONRecordType::AUTO, bool auto_detect_p = false)\n+\t    : type(type_p), format(format_p), record_type(record_type_p), auto_detect(auto_detect_p) {\n \t}\n \n \tJSONScanType type;\n \tJSONFormat format;\n+\tJSONRecordType record_type;\n \tbool auto_detect;\n };\n \n@@ -189,15 +193,15 @@ struct JSONScanLocalState {\n public:\n \tidx_t ReadNext(JSONScanGlobalState &gstate);\n \tyyjson_alc *GetAllocator();\n-\tvoid ThrowTransformError(idx_t count, idx_t object_index, const string &error_message);\n+\tvoid ThrowTransformError(idx_t object_index, const string &error_message);\n \n \tidx_t scan_count;\n \tJSONLine lines[STANDARD_VECTOR_SIZE];\n-\tyyjson_val *objects[STANDARD_VECTOR_SIZE];\n+\tyyjson_val *values[STANDARD_VECTOR_SIZE];\n \n \tidx_t array_idx;\n \tidx_t array_offset;\n-\tyyjson_val *array_objects[STANDARD_VECTOR_SIZE];\n+\tyyjson_val *array_values[STANDARD_VECTOR_SIZE];\n \n \tidx_t batch_index;\n \n@@ -207,7 +211,7 @@ struct JSONScanLocalState {\n \n private:\n \tyyjson_val *ParseLine(char *line_start, idx_t line_size, idx_t remaining, JSONLine &line);\n-\tidx_t GetObjectsFromArray();\n+\tidx_t GetObjectsFromArray(JSONScanGlobalState &gstate);\n \n private:\n \t//! Bind data\n@@ -228,7 +232,7 @@ struct JSONScanLocalState {\n \tidx_t prev_buffer_remainder;\n \tidx_t lines_or_objects_in_buffer;\n \n-\t//! Buffer to reconstruct split objects\n+\t//! Buffer to reconstruct split values\n \tAllocatedData reconstruct_buffer;\n \t//! Copy of current buffer for YYJSON_READ_INSITU\n \tAllocatedData current_buffer_copy;\n@@ -307,7 +311,7 @@ struct JSONScan {\n \tstatic void TableFunctionDefaults(TableFunction &table_function) {\n \t\ttable_function.named_parameters[\"maximum_object_size\"] = LogicalType::UINTEGER;\n \t\ttable_function.named_parameters[\"ignore_errors\"] = LogicalType::BOOLEAN;\n-\t\ttable_function.named_parameters[\"format\"] = LogicalType::VARCHAR;\n+\t\ttable_function.named_parameters[\"lines\"] = LogicalType::VARCHAR;\n \t\ttable_function.named_parameters[\"compression\"] = LogicalType::VARCHAR;\n \n \t\ttable_function.table_scan_progress = JSONScanProgress;\ndiff --git a/extension/json/json_functions/copy_json.cpp b/extension/json/json_functions/copy_json.cpp\nindex 6e93d27a023c..06d042db3afb 100644\n--- a/extension/json/json_functions/copy_json.cpp\n+++ b/extension/json/json_functions/copy_json.cpp\n@@ -1,7 +1,9 @@\n #include \"duckdb/function/copy_function.hpp\"\n #include \"duckdb/parser/expression/constant_expression.hpp\"\n #include \"duckdb/parser/expression/function_expression.hpp\"\n+#include \"duckdb/parser/expression/positional_reference_expression.hpp\"\n #include \"duckdb/parser/query_node/select_node.hpp\"\n+#include \"duckdb/parser/tableref/subqueryref.hpp\"\n #include \"duckdb/planner/binder.hpp\"\n #include \"json_functions.hpp\"\n #include \"json_scan.hpp\"\n@@ -12,42 +14,52 @@ namespace duckdb {\n static BoundStatement CopyToJSONPlan(Binder &binder, CopyStatement &stmt) {\n \tauto stmt_copy = stmt.Copy();\n \tauto &copy = (CopyStatement &)*stmt_copy;\n-\tauto &select_stmt = (SelectNode &)*copy.select_statement;\n \tauto &info = *copy.info;\n \n-\t// strftime if the user specified a format TODO: deal with date/timestamp within nested types\n-\tauto date_it = info.options.find(\"dateformat\");\n-\tauto timestamp_it = info.options.find(\"timestampformat\");\n-\n \t// Bind the select statement of the original to resolve the types\n \tauto dummy_binder = Binder::CreateBinder(binder.context, &binder, true);\n \tauto bound_original = dummy_binder->Bind(*stmt.select_statement);\n-\tD_ASSERT(bound_original.types.size() == select_stmt.select_list.size());\n-\tconst idx_t num_cols = bound_original.types.size();\n \n-\t// This loop also makes sure the columns have an alias (needed for struct_pack)\n+\t// Create new SelectNode with the original SelectNode as a subquery in the FROM clause\n+\tauto select_stmt = make_unique<SelectStatement>();\n+\tselect_stmt->node = std::move(copy.select_statement);\n+\tauto subquery_ref = make_unique<SubqueryRef>(std::move(select_stmt));\n+\tcopy.select_statement = make_unique_base<QueryNode, SelectNode>();\n+\tauto &new_select_node = (SelectNode &)*copy.select_statement;\n+\tnew_select_node.from_table = std::move(subquery_ref);\n+\n+\t// Create new select list\n+\tvector<unique_ptr<ParsedExpression>> select_list;\n+\tselect_list.reserve(bound_original.types.size());\n+\n+\t// strftime if the user specified a format (loop also gives columns a name, needed for struct_pack)\n+\t// TODO: deal with date/timestamp within nested types\n+\tconst auto date_it = info.options.find(\"dateformat\");\n+\tconst auto timestamp_it = info.options.find(\"timestampformat\");\n \tvector<unique_ptr<ParsedExpression>> strftime_children;\n-\tfor (idx_t i = 0; i < num_cols; i++) {\n+\tfor (idx_t col_idx = 0; col_idx < bound_original.types.size(); col_idx++) {\n+\t\tauto column = make_unique_base<ParsedExpression, PositionalReferenceExpression>(col_idx + 1);\n \t\tstrftime_children.clear();\n-\t\tauto &col = select_stmt.select_list[i];\n-\t\tauto name = col->GetName();\n-\t\tif (bound_original.types[i] == LogicalTypeId::DATE && date_it != info.options.end()) {\n-\t\t\tstrftime_children.emplace_back(std::move(col));\n+\t\tconst auto &type = bound_original.types[col_idx];\n+\t\tconst auto &name = bound_original.names[col_idx];\n+\t\tif (date_it != info.options.end() && type == LogicalTypeId::DATE) {\n+\t\t\tstrftime_children.emplace_back(std::move(column));\n \t\t\tstrftime_children.emplace_back(make_unique<ConstantExpression>(date_it->second.back()));\n-\t\t\tcol = make_unique<FunctionExpression>(\"strftime\", std::move(strftime_children));\n-\t\t} else if (bound_original.types[i] == LogicalTypeId::TIMESTAMP && timestamp_it != info.options.end()) {\n-\t\t\tstrftime_children.emplace_back(std::move(col));\n+\t\t\tcolumn = make_unique<FunctionExpression>(\"strftime\", std::move(strftime_children));\n+\t\t} else if (timestamp_it != info.options.end() && type == LogicalTypeId::TIMESTAMP) {\n+\t\t\tstrftime_children.emplace_back(std::move(column));\n \t\t\tstrftime_children.emplace_back(make_unique<ConstantExpression>(timestamp_it->second.back()));\n-\t\t\tcol = make_unique<FunctionExpression>(\"strftime\", std::move(strftime_children));\n+\t\t\tcolumn = make_unique<FunctionExpression>(\"strftime\", std::move(strftime_children));\n \t\t}\n-\t\tcol->alias = name;\n+\t\tcolumn->alias = name;\n+\t\tselect_list.emplace_back(std::move(column));\n \t}\n \n \t// Now create the struct_pack/to_json to create a JSON object per row\n+\tauto &select_node = (SelectNode &)*copy.select_statement;\n \tvector<unique_ptr<ParsedExpression>> struct_pack_child;\n-\tstruct_pack_child.emplace_back(make_unique<FunctionExpression>(\"struct_pack\", std::move(select_stmt.select_list)));\n-\tselect_stmt.select_list.clear();\n-\tselect_stmt.select_list.emplace_back(make_unique<FunctionExpression>(\"to_json\", std::move(struct_pack_child)));\n+\tstruct_pack_child.emplace_back(make_unique<FunctionExpression>(\"struct_pack\", std::move(select_list)));\n+\tselect_node.select_list.emplace_back(make_unique<FunctionExpression>(\"to_json\", std::move(struct_pack_child)));\n \n \t// Now we can just use the CSV writer\n \tinfo.format = \"csv\";\n@@ -101,7 +113,8 @@ CreateCopyFunctionInfo JSONFunctions::GetJSONCopyFunction() {\n \n \tfunction.copy_from_bind = CopyFromJSONBind;\n \tfunction.copy_from_function = JSONFunctions::GetReadJSONTableFunction(\n-\t    false, make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::AUTO_DETECT, false));\n+\t    false,\n+\t    make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::AUTO_DETECT, JSONRecordType::RECORDS, false));\n \n \treturn CreateCopyFunctionInfo(function);\n }\ndiff --git a/extension/json/json_functions/json_create.cpp b/extension/json/json_functions/json_create.cpp\nindex ace88e06fdd5..19ff2968d0ae 100644\n--- a/extension/json/json_functions/json_create.cpp\n+++ b/extension/json/json_functions/json_create.cpp\n@@ -56,7 +56,7 @@ static LogicalType GetJSONType(unordered_map<string, unique_ptr<Vector>> &const_\n \t// The nested types need to conform as well\n \tcase LogicalTypeId::LIST:\n \t\treturn LogicalType::LIST(GetJSONType(const_struct_names, ListType::GetChildType(type)));\n-\t// Struct and MAP are treated as JSON objects\n+\t// Struct and MAP are treated as JSON values\n \tcase LogicalTypeId::STRUCT: {\n \t\tchild_list_t<LogicalType> child_types;\n \t\tfor (const auto &child_type : StructType::GetChildTypes(type)) {\n@@ -247,14 +247,14 @@ static void TemplatedCreateValues(yyjson_mut_doc *doc, yyjson_mut_val *vals[], V\n \n static void CreateValuesStruct(const JSONCreateFunctionData &info, yyjson_mut_doc *doc, yyjson_mut_val *vals[],\n                                Vector &value_v, idx_t count) {\n-\t// Structs become objects, therefore we initialize vals to JSON objects\n+\t// Structs become values, therefore we initialize vals to JSON values\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tvals[i] = yyjson_mut_obj(doc);\n \t}\n \t// Initialize re-usable array for the nested values\n \tauto nested_vals = (yyjson_mut_val **)doc->alc.malloc(doc->alc.ctx, sizeof(yyjson_mut_val *) * count);\n \n-\t// Add the key/value pairs to the objects\n+\t// Add the key/value pairs to the values\n \tauto &entries = StructVector::GetEntries(value_v);\n \tfor (idx_t entry_i = 0; entry_i < entries.size(); entry_i++) {\n \t\tauto &struct_key_v = *info.const_struct_names.at(StructType::GetChildName(value_v.GetType(), entry_i));\n@@ -284,7 +284,7 @@ static void CreateValuesMap(const JSONCreateFunctionData &info, yyjson_mut_doc *\n \tauto map_val_count = ListVector::GetListSize(value_v);\n \tauto nested_vals = (yyjson_mut_val **)doc->alc.malloc(doc->alc.ctx, sizeof(yyjson_mut_val *) * map_val_count);\n \tCreateValues(info, doc, nested_vals, map_val_v, map_val_count);\n-\t// Add the key/value pairs to the objects\n+\t// Add the key/value pairs to the values\n \tUnifiedVectorFormat map_data;\n \tvalue_v.ToUnifiedFormat(count, map_data);\n \tauto map_key_list_entries = (list_entry_t *)map_data.data;\n@@ -308,7 +308,7 @@ static void CreateValuesMap(const JSONCreateFunctionData &info, yyjson_mut_doc *\n \n static void CreateValuesUnion(const JSONCreateFunctionData &info, yyjson_mut_doc *doc, yyjson_mut_val *vals[],\n                               Vector &value_v, idx_t count) {\n-\t// Structs become objects, therefore we initialize vals to JSON objects\n+\t// Structs become values, therefore we initialize vals to JSON values\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tvals[i] = yyjson_mut_obj(doc);\n \t}\n@@ -320,7 +320,7 @@ static void CreateValuesUnion(const JSONCreateFunctionData &info, yyjson_mut_doc\n \tUnifiedVectorFormat tag_data;\n \ttag_v.ToUnifiedFormat(count, tag_data);\n \n-\t// Add the key/value pairs to the objects\n+\t// Add the key/value pairs to the values\n \tfor (idx_t member_idx = 0; member_idx < UnionType::GetMemberCount(value_v.GetType()); member_idx++) {\n \t\tauto &member_val_v = UnionVector::GetMember(value_v, member_idx);\n \t\tauto &member_key_v = *info.const_struct_names.at(UnionType::GetMemberName(value_v.GetType(), member_idx));\n@@ -425,7 +425,7 @@ static void ObjectFunction(DataChunk &args, ExpressionState &state, Vector &resu\n \tauto &lstate = JSONFunctionLocalState::ResetAndGet(state);\n \tauto alc = lstate.json_allocator.GetYYJSONAllocator();\n \n-\t// Initialize objects\n+\t// Initialize values\n \tconst idx_t count = args.size();\n \tauto doc = JSONCommon::CreateDocument(alc);\n \tyyjson_mut_val *objs[STANDARD_VECTOR_SIZE];\n@@ -440,7 +440,7 @@ static void ObjectFunction(DataChunk &args, ExpressionState &state, Vector &resu\n \t\tVector &value_v = args.data[pair_idx * 2 + 1];\n \t\tCreateKeyValuePairs(info, doc, objs, vals, key_v, value_v, count);\n \t}\n-\t// Write JSON objects to string\n+\t// Write JSON values to string\n \tauto objects = FlatVector::GetData<string_t>(result);\n \tfor (idx_t i = 0; i < count; i++) {\n \t\tobjects[i] = JSONCommon::WriteVal<yyjson_mut_val>(objs[i], alc);\ndiff --git a/extension/json/json_functions/json_transform.cpp b/extension/json/json_functions/json_transform.cpp\nindex f4bef879ec46..7d5d4561c3bc 100644\n--- a/extension/json/json_functions/json_transform.cpp\n+++ b/extension/json/json_functions/json_transform.cpp\n@@ -380,12 +380,20 @@ bool JSONTransform::TransformObject(yyjson_val *objects[], yyjson_alc *alc, cons\n \tsize_t idx, max;\n \tyyjson_val *key, *val;\n \tfor (idx_t i = 0; i < count; i++) {\n-\t\tif (objects[i]) {\n+\t\tif (objects[i] && !unsafe_yyjson_is_null(objects[i])) {\n+\t\t\tif (!unsafe_yyjson_is_obj(objects[i]) && options.strict_cast) {\n+\t\t\t\toptions.error_message =\n+\t\t\t\t    StringUtil::Format(\"Expected OBJECT, but got %s: %s\", JSONCommon::ValTypeToString(objects[i]),\n+\t\t\t\t                       JSONCommon::ValToString(objects[i], 50));\n+\t\t\t\toptions.object_index = i;\n+\t\t\t\tsuccess = false;\n+\t\t\t\tbreak;\n+\t\t\t}\n \t\t\tfound_key_count = 0;\n \t\t\tmemset(found_keys, false, column_count);\n \t\t\tyyjson_obj_foreach(objects[i], idx, max, key, val) {\n-\t\t\t\tauto key_ptr = yyjson_get_str(key);\n-\t\t\t\tauto key_len = yyjson_get_len(key);\n+\t\t\t\tauto key_ptr = unsafe_yyjson_get_str(key);\n+\t\t\t\tauto key_len = unsafe_yyjson_get_len(key);\n \t\t\t\tauto it = key_map.find({key_ptr, key_len});\n \t\t\t\tif (it != key_map.end()) {\n \t\t\t\t\tconst auto &col_idx = it->second;\n@@ -476,13 +484,24 @@ static bool TransformArray(yyjson_val *arrays[], yyjson_alc *alc, Vector &result\n \tauto &list_validity = FlatVector::Validity(result);\n \tidx_t offset = 0;\n \tfor (idx_t i = 0; i < count; i++) {\n-\t\tif (!arrays[i] || yyjson_is_null(arrays[i])) {\n+\t\tif (!arrays[i] || unsafe_yyjson_is_null(arrays[i])) {\n \t\t\tlist_validity.SetInvalid(i);\n+\t\t} else if (!unsafe_yyjson_is_arr(arrays[i])) {\n+\t\t\tif (options.strict_cast) {\n+\t\t\t\toptions.error_message =\n+\t\t\t\t    StringUtil::Format(\"Expected ARRAY, but got %s: %s\", JSONCommon::ValTypeToString(arrays[i]),\n+\t\t\t\t                       JSONCommon::ValToString(arrays[i], 50));\n+\t\t\t\toptions.object_index = i;\n+\t\t\t\treturn false;\n+\t\t\t} else {\n+\t\t\t\tlist_validity.SetInvalid(i);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tauto &entry = list_entries[i];\n+\t\t\tentry.offset = offset;\n+\t\t\tentry.length = unsafe_yyjson_get_len(arrays[i]);\n+\t\t\toffset += entry.length;\n \t\t}\n-\t\tauto &entry = list_entries[i];\n-\t\tentry.offset = offset;\n-\t\tentry.length = yyjson_arr_size(arrays[i]);\n-\t\toffset += entry.length;\n \t}\n \tListVector::SetListSize(result, offset);\n \tListVector::Reserve(result, offset);\ndiff --git a/extension/json/json_functions/read_json.cpp b/extension/json/json_functions/read_json.cpp\nindex 585973a5530a..5ee3538c8669 100644\n--- a/extension/json/json_functions/read_json.cpp\n+++ b/extension/json/json_functions/read_json.cpp\n@@ -21,23 +21,30 @@ void JSONScan::AutoDetect(ClientContext &context, JSONScanData &bind_data, vecto\n \twhile (remaining != 0) {\n \t\tallocator.Reset();\n \t\tauto read_count = lstate.ReadNext(gstate);\n-\t\tif (read_count > 1) {\n+\t\tif (lstate.scan_count > 1) {\n \t\t\tmore_than_one = true;\n \t\t}\n \t\tif (read_count == 0) {\n \t\t\tbreak;\n \t\t}\n \t\tidx_t next = MinValue<idx_t>(read_count, remaining);\n+\t\tyyjson_val **values;\n+\t\tif (bind_data.record_type == JSONRecordType::ARRAY_OF_RECORDS ||\n+\t\t    bind_data.record_type == JSONRecordType::ARRAY_OF_JSON) {\n+\t\t\tvalues = lstate.array_values;\n+\t\t} else {\n+\t\t\tvalues = lstate.values;\n+\t\t}\n \t\tfor (idx_t i = 0; i < next; i++) {\n-\t\t\tif (lstate.objects[i]) {\n-\t\t\t\tJSONStructure::ExtractStructure(lstate.objects[i], node);\n+\t\t\tif (values[i]) {\n+\t\t\t\tJSONStructure::ExtractStructure(values[i], node);\n \t\t\t}\n \t\t}\n \t\tif (!node.ContainsVarchar()) { // Can't refine non-VARCHAR types\n \t\t\tcontinue;\n \t\t}\n \t\tnode.InitializeCandidateTypes(bind_data.max_depth);\n-\t\tnode.RefineCandidateTypes(lstate.objects, next, string_vector, allocator, bind_data.date_format_map);\n+\t\tnode.RefineCandidateTypes(values, next, string_vector, allocator, bind_data.date_format_map);\n \t\tremaining -= next;\n \n \t\tif (gstate.file_index == 10) {\n@@ -46,29 +53,48 @@ void JSONScan::AutoDetect(ClientContext &context, JSONScanData &bind_data, vecto\n \t\t}\n \t}\n \tbind_data.type = original_scan_type;\n-\tbind_data.transform_options.date_format_map = &bind_data.date_format_map;\n \n+\t// Convert structure to logical type\n \tauto type = JSONStructure::StructureToType(context, node, bind_data.max_depth);\n-\tif (type.id() == LogicalTypeId::STRUCT) {\n-\t\tbind_data.top_level_type = JSONScanTopLevelType::OBJECTS;\n-\t} else if (!more_than_one && type.id() == LogicalTypeId::LIST &&\n-\t           ListType::GetChildType(type).id() == LogicalTypeId::STRUCT) {\n-\t\tbind_data.top_level_type = JSONScanTopLevelType::ARRAY_OF_OBJECTS;\n-\t\tbind_data.options.format = JSONFormat::UNSTRUCTURED;\n-\t\ttype = ListType::GetChildType(type);\n+\n+\t// Detect record type\n+\tif (bind_data.record_type == JSONRecordType::AUTO) {\n+\t\tswitch (type.id()) {\n+\t\tcase LogicalTypeId::STRUCT:\n+\t\t\tbind_data.record_type = JSONRecordType::RECORDS;\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::LIST: {\n+\t\t\tif (more_than_one) {\n+\t\t\t\tbind_data.record_type = JSONRecordType::JSON;\n+\t\t\t} else {\n+\t\t\t\ttype = ListType::GetChildType(type);\n+\t\t\t\tif (type.id() == LogicalTypeId::STRUCT) {\n+\t\t\t\t\tbind_data.record_type = JSONRecordType::ARRAY_OF_RECORDS;\n+\t\t\t\t} else {\n+\t\t\t\t\tbind_data.record_type = JSONRecordType::ARRAY_OF_JSON;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\tbind_data.record_type = JSONRecordType::JSON;\n+\t\t}\n \t}\n \n-\tif (type.id() != LogicalTypeId::STRUCT) {\n-\t\treturn_types.emplace_back(type);\n-\t\tnames.emplace_back(\"json\");\n-\t\tbind_data.top_level_type = JSONScanTopLevelType::OTHER;\n-\t} else {\n-\t\tconst auto &child_types = StructType::GetChildTypes(type);\n-\t\treturn_types.reserve(child_types.size());\n-\t\tnames.reserve(child_types.size());\n-\t\tfor (auto &child_type : child_types) {\n-\t\t\treturn_types.emplace_back(child_type.second);\n-\t\t\tnames.emplace_back(child_type.first);\n+\t// Detect return type\n+\tif (bind_data.auto_detect) {\n+\t\tbind_data.transform_options.date_format_map = &bind_data.date_format_map;\n+\t\tif (type.id() != LogicalTypeId::STRUCT) {\n+\t\t\treturn_types.emplace_back(type);\n+\t\t\tnames.emplace_back(\"json\");\n+\t\t} else {\n+\t\t\tconst auto &child_types = StructType::GetChildTypes(type);\n+\t\t\treturn_types.reserve(child_types.size());\n+\t\t\tnames.reserve(child_types.size());\n+\t\t\tfor (auto &child_type : child_types) {\n+\t\t\t\treturn_types.emplace_back(child_type.second);\n+\t\t\t\tnames.emplace_back(child_type.first);\n+\t\t\t}\n \t\t}\n \t}\n \n@@ -149,6 +175,22 @@ void JSONScan::InitializeBindData(ClientContext &context, JSONScanData &bind_dat\n \t\t\tif (!error.empty()) {\n \t\t\t\tthrow InvalidInputException(\"Could not parse TIMESTAMPFORMAT: %s\", error.c_str());\n \t\t\t}\n+\t\t} else if (loption == \"json_format\") {\n+\t\t\tauto arg = StringValue::Get(kv.second);\n+\t\t\tif (arg == \"records\") {\n+\t\t\t\tbind_data.record_type = JSONRecordType::RECORDS;\n+\t\t\t} else if (arg == \"array_of_records\") {\n+\t\t\t\tbind_data.record_type = JSONRecordType::ARRAY_OF_RECORDS;\n+\t\t\t} else if (arg == \"values\") {\n+\t\t\t\tbind_data.record_type = JSONRecordType::JSON;\n+\t\t\t} else if (arg == \"array_of_values\") {\n+\t\t\t\tbind_data.record_type = JSONRecordType::ARRAY_OF_JSON;\n+\t\t\t} else if (arg == \"auto\") {\n+\t\t\t\tbind_data.record_type = JSONRecordType::AUTO;\n+\t\t\t} else {\n+\t\t\t\tthrow InvalidInputException(\"\\\"json_format\\\" must be one of ['records', 'array_of_records', 'json', \"\n+\t\t\t\t                            \"'array_of_json', 'auto']\");\n+\t\t\t}\n \t\t}\n \t}\n }\n@@ -169,7 +211,7 @@ unique_ptr<FunctionData> ReadJSONBind(ClientContext &context, TableFunctionBindI\n \n \tbind_data.InitializeFormats();\n \n-\tif (bind_data.auto_detect) {\n+\tif (bind_data.auto_detect || bind_data.record_type == JSONRecordType::AUTO) {\n \t\tJSONScan::AutoDetect(context, bind_data, return_types, names);\n \t\tbind_data.names = names;\n \t}\n@@ -189,9 +231,14 @@ static void ReadJSONFunction(ClientContext &context, TableFunctionInput &data_p,\n \tauto &lstate = ((JSONLocalTableFunctionState &)*data_p.local_state).state;\n \n \tconst auto count = lstate.ReadNext(gstate);\n-\tconst auto objects = gstate.bind_data.top_level_type == JSONScanTopLevelType::ARRAY_OF_OBJECTS\n-\t                         ? lstate.array_objects\n-\t                         : lstate.objects;\n+\tyyjson_val **values;\n+\tif (gstate.bind_data.record_type == JSONRecordType::ARRAY_OF_RECORDS ||\n+\t    gstate.bind_data.record_type == JSONRecordType::ARRAY_OF_JSON) {\n+\t\tvalues = lstate.array_values;\n+\t} else {\n+\t\tD_ASSERT(gstate.bind_data.record_type != JSONRecordType::AUTO);\n+\t\tvalues = lstate.values;\n+\t}\n \toutput.SetCardinality(count);\n \n \tvector<Vector *> result_vectors;\n@@ -203,20 +250,21 @@ static void ReadJSONFunction(ClientContext &context, TableFunctionInput &data_p,\n \n \t// Pass current reader to transform options so we can get line number information if an error occurs\n \tbool success;\n-\tif (gstate.bind_data.top_level_type == JSONScanTopLevelType::OTHER) {\n-\t\tsuccess = JSONTransform::Transform(objects, lstate.GetAllocator(), *result_vectors[0], count,\n-\t\t                                   lstate.transform_options);\n-\t} else {\n-\t\tsuccess = JSONTransform::TransformObject(objects, lstate.GetAllocator(), count, gstate.bind_data.names,\n+\tif (gstate.bind_data.record_type == JSONRecordType::RECORDS ||\n+\t    gstate.bind_data.record_type == JSONRecordType::ARRAY_OF_RECORDS) {\n+\t\tsuccess = JSONTransform::TransformObject(values, lstate.GetAllocator(), count, gstate.bind_data.names,\n \t\t                                         result_vectors, lstate.transform_options);\n+\t} else {\n+\t\tsuccess = JSONTransform::Transform(values, lstate.GetAllocator(), *result_vectors[0], count,\n+\t\t                                   lstate.transform_options);\n \t}\n \n \tif (!success) {\n \t\tstring hint = gstate.bind_data.auto_detect\n \t\t                  ? \"\\nTry increasing 'sample_size', reducing 'maximum_depth', specifying 'columns' manually, \"\n-\t\t                    \"or setting 'ignore_errors' to true.\"\n-\t\t                  : \"\";\n-\t\tlstate.ThrowTransformError(count, lstate.transform_options.object_index,\n+\t\t                    \"specifying 'lines' or 'json_format' manually, or setting 'ignore_errors' to true.\"\n+\t\t                  : \"\\n Try specifying 'lines' or 'json_format' manually, or setting 'ignore_errors' to true.\";\n+\t\tlstate.ThrowTransformError(lstate.transform_options.object_index,\n \t\t                           lstate.transform_options.error_message + hint);\n \t}\n }\n@@ -234,6 +282,7 @@ TableFunction JSONFunctions::GetReadJSONTableFunction(bool list_parameter, share\n \ttable_function.named_parameters[\"date_format\"] = LogicalType::VARCHAR;\n \ttable_function.named_parameters[\"timestampformat\"] = LogicalType::VARCHAR;\n \ttable_function.named_parameters[\"timestamp_format\"] = LogicalType::VARCHAR;\n+\ttable_function.named_parameters[\"json_format\"] = LogicalType::VARCHAR;\n \n \ttable_function.projection_pushdown = true;\n \t// TODO: might be able to do filter pushdown/prune too\n@@ -251,7 +300,8 @@ TableFunction GetReadJSONAutoTableFunction(bool list_parameter, shared_ptr<JSONS\n \n CreateTableFunctionInfo JSONFunctions::GetReadJSONFunction() {\n \tTableFunctionSet function_set(\"read_json\");\n-\tauto function_info = make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::UNSTRUCTURED, false);\n+\tauto function_info =\n+\t    make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::UNSTRUCTURED, JSONRecordType::RECORDS, false);\n \tfunction_set.AddFunction(JSONFunctions::GetReadJSONTableFunction(false, function_info));\n \tfunction_set.AddFunction(JSONFunctions::GetReadJSONTableFunction(true, function_info));\n \treturn CreateTableFunctionInfo(function_set);\n@@ -259,7 +309,8 @@ CreateTableFunctionInfo JSONFunctions::GetReadJSONFunction() {\n \n CreateTableFunctionInfo JSONFunctions::GetReadNDJSONFunction() {\n \tTableFunctionSet function_set(\"read_ndjson\");\n-\tauto function_info = make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::NEWLINE_DELIMITED, false);\n+\tauto function_info = make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::NEWLINE_DELIMITED,\n+\t                                               JSONRecordType::RECORDS, false);\n \tfunction_set.AddFunction(JSONFunctions::GetReadJSONTableFunction(false, function_info));\n \tfunction_set.AddFunction(JSONFunctions::GetReadJSONTableFunction(true, function_info));\n \treturn CreateTableFunctionInfo(function_set);\n@@ -267,7 +318,8 @@ CreateTableFunctionInfo JSONFunctions::GetReadNDJSONFunction() {\n \n CreateTableFunctionInfo JSONFunctions::GetReadJSONAutoFunction() {\n \tTableFunctionSet function_set(\"read_json_auto\");\n-\tauto function_info = make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::AUTO_DETECT, true);\n+\tauto function_info =\n+\t    make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::AUTO_DETECT, JSONRecordType::AUTO, true);\n \tfunction_set.AddFunction(GetReadJSONAutoTableFunction(false, function_info));\n \tfunction_set.AddFunction(GetReadJSONAutoTableFunction(true, function_info));\n \treturn CreateTableFunctionInfo(function_set);\n@@ -275,7 +327,8 @@ CreateTableFunctionInfo JSONFunctions::GetReadJSONAutoFunction() {\n \n CreateTableFunctionInfo JSONFunctions::GetReadNDJSONAutoFunction() {\n \tTableFunctionSet function_set(\"read_ndjson_auto\");\n-\tauto function_info = make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::NEWLINE_DELIMITED, true);\n+\tauto function_info =\n+\t    make_shared<JSONScanInfo>(JSONScanType::READ_JSON, JSONFormat::NEWLINE_DELIMITED, JSONRecordType::AUTO, true);\n \tfunction_set.AddFunction(GetReadJSONAutoTableFunction(false, function_info));\n \tfunction_set.AddFunction(GetReadJSONAutoTableFunction(true, function_info));\n \treturn CreateTableFunctionInfo(function_set);\ndiff --git a/extension/json/json_functions/read_json_objects.cpp b/extension/json/json_functions/read_json_objects.cpp\nindex 6455d1fc74b4..38308b1c7069 100644\n--- a/extension/json/json_functions/read_json_objects.cpp\n+++ b/extension/json/json_functions/read_json_objects.cpp\n@@ -20,7 +20,7 @@ static void ReadJSONObjectsFunction(ClientContext &context, TableFunctionInput &\n \t// Fetch next lines\n \tconst auto count = lstate.ReadNext(gstate);\n \tconst auto lines = lstate.lines;\n-\tconst auto objects = lstate.objects;\n+\tconst auto objects = lstate.values;\n \n \t// Create the strings without copying them\n \tauto strings = FlatVector::GetData<string_t>(output.data[0]);\n@@ -48,7 +48,8 @@ TableFunction GetReadJSONObjectsTableFunction(bool list_parameter, shared_ptr<JS\n \n CreateTableFunctionInfo JSONFunctions::GetReadJSONObjectsFunction() {\n \tTableFunctionSet function_set(\"read_json_objects\");\n-\tauto function_info = make_shared<JSONScanInfo>(JSONScanType::READ_JSON_OBJECTS, JSONFormat::UNSTRUCTURED);\n+\tauto function_info =\n+\t    make_shared<JSONScanInfo>(JSONScanType::READ_JSON_OBJECTS, JSONFormat::UNSTRUCTURED, JSONRecordType::JSON);\n \tfunction_set.AddFunction(GetReadJSONObjectsTableFunction(false, function_info));\n \tfunction_set.AddFunction(GetReadJSONObjectsTableFunction(true, function_info));\n \treturn CreateTableFunctionInfo(function_set);\n@@ -56,7 +57,8 @@ CreateTableFunctionInfo JSONFunctions::GetReadJSONObjectsFunction() {\n \n CreateTableFunctionInfo JSONFunctions::GetReadNDJSONObjectsFunction() {\n \tTableFunctionSet function_set(\"read_ndjson_objects\");\n-\tauto function_info = make_shared<JSONScanInfo>(JSONScanType::READ_JSON_OBJECTS, JSONFormat::NEWLINE_DELIMITED);\n+\tauto function_info =\n+\t    make_shared<JSONScanInfo>(JSONScanType::READ_JSON_OBJECTS, JSONFormat::NEWLINE_DELIMITED, JSONRecordType::JSON);\n \tfunction_set.AddFunction(GetReadJSONObjectsTableFunction(false, function_info));\n \tfunction_set.AddFunction(GetReadJSONObjectsTableFunction(true, function_info));\n \treturn CreateTableFunctionInfo(function_set);\ndiff --git a/extension/json/json_scan.cpp b/extension/json/json_scan.cpp\nindex 30df5c35e83b..b0a40f94c3e1 100644\n--- a/extension/json/json_scan.cpp\n+++ b/extension/json/json_scan.cpp\n@@ -1,9 +1,9 @@\n #include \"json_scan.hpp\"\n \n #include \"duckdb/main/database.hpp\"\n+#include \"duckdb/main/extension_helper.hpp\"\n #include \"duckdb/parallel/task_scheduler.hpp\"\n #include \"duckdb/storage/buffer_manager.hpp\"\n-#include \"duckdb/main/extension_helper.hpp\"\n \n namespace duckdb {\n \n@@ -20,8 +20,9 @@ unique_ptr<FunctionData> JSONScanData::Bind(ClientContext &context, TableFunctio\n \tauto &options = result->options;\n \n \tauto &info = (JSONScanInfo &)*input.info;\n-\toptions.format = info.format;\n \tresult->type = info.type;\n+\toptions.format = info.format;\n+\tresult->record_type = info.record_type;\n \tresult->auto_detect = info.auto_detect;\n \n \tvector<string> patterns;\n@@ -40,19 +41,16 @@ unique_ptr<FunctionData> JSONScanData::Bind(ClientContext &context, TableFunctio\n \t\t\tresult->ignore_errors = BooleanValue::Get(kv.second);\n \t\t} else if (loption == \"maximum_object_size\") {\n \t\t\tresult->maximum_object_size = MaxValue<idx_t>(UIntegerValue::Get(kv.second), result->maximum_object_size);\n-\t\t} else if (loption == \"format\") {\n+\t\t} else if (loption == \"lines\") {\n \t\t\tauto format = StringUtil::Lower(StringValue::Get(kv.second));\n \t\t\tif (format == \"auto\") {\n \t\t\t\toptions.format = JSONFormat::AUTO_DETECT;\n-\t\t\t} else if (format == \"unstructured\") {\n+\t\t\t} else if (format == \"false\") {\n \t\t\t\toptions.format = JSONFormat::UNSTRUCTURED;\n-\t\t\t} else if (format == \"newline_delimited\") {\n+\t\t\t} else if (format == \"true\") {\n \t\t\t\toptions.format = JSONFormat::NEWLINE_DELIMITED;\n-\t\t\t} else if (format == \"array_of_objects\") {\n-\t\t\t\tresult->top_level_type = JSONScanTopLevelType::ARRAY_OF_OBJECTS;\n \t\t\t} else {\n-\t\t\t\tthrow BinderException(\n-\t\t\t\t    \"format must be one of ['auto', 'unstructured', 'newline_delimited', 'array_of_objects']\");\n+\t\t\t\tthrow BinderException(\"\\\"lines\\\" must be one of ['auto', 'true', 'false']\");\n \t\t\t}\n \t\t} else if (loption == \"compression\") {\n \t\t\tauto compression = StringUtil::Lower(StringValue::Get(kv.second));\n@@ -70,10 +68,6 @@ unique_ptr<FunctionData> JSONScanData::Bind(ClientContext &context, TableFunctio\n \t\t}\n \t}\n \n-\tif (result->top_level_type == JSONScanTopLevelType::ARRAY_OF_OBJECTS) {\n-\t\tresult->options.format = JSONFormat::UNSTRUCTURED;\n-\t}\n-\n \treturn std::move(result);\n }\n \n@@ -140,7 +134,7 @@ void JSONScanData::Serialize(FieldWriter &writer) {\n \twriter.WriteList<string>(names);\n \twriter.WriteList<idx_t>(valid_cols);\n \twriter.WriteField<idx_t>(max_depth);\n-\twriter.WriteField<JSONScanTopLevelType>(top_level_type);\n+\twriter.WriteField<JSONRecordType>(record_type);\n \tif (!date_format.empty()) {\n \t\twriter.WriteString(date_format);\n \t} else {\n@@ -165,7 +159,7 @@ void JSONScanData::Deserialize(FieldReader &reader) {\n \tnames = reader.ReadRequiredList<string>();\n \tvalid_cols = reader.ReadRequiredList<idx_t>();\n \tmax_depth = reader.ReadRequired<idx_t>();\n-\ttop_level_type = reader.ReadRequired<JSONScanTopLevelType>();\n+\trecord_type = reader.ReadRequired<JSONRecordType>();\n \tdate_format = reader.ReadRequired<string>();\n \ttimestamp_format = reader.ReadRequired<string>();\n \n@@ -193,7 +187,7 @@ JSONScanLocalState::JSONScanLocalState(ClientContext &context, JSONScanGlobalSta\n       json_allocator(BufferAllocator::Get(context)), current_reader(nullptr), current_buffer_handle(nullptr),\n       is_last(false), buffer_size(0), buffer_offset(0), prev_buffer_remainder(0) {\n \n-\t// Buffer to reconstruct JSON objects when they cross a buffer boundary\n+\t// Buffer to reconstruct JSON values when they cross a buffer boundary\n \treconstruct_buffer = gstate.allocator.Allocate(gstate.bind_data.maximum_object_size + YYJSON_PADDING_SIZE);\n \n \t// This is needed for JSONFormat::UNSTRUCTURED, to make use of YYJSON_READ_INSITU\n@@ -234,7 +228,26 @@ unique_ptr<GlobalTableFunctionState> JSONGlobalTableFunctionState::Init(ClientCo\n }\n \n idx_t JSONGlobalTableFunctionState::MaxThreads() const {\n-\treturn state.system_threads;\n+\tauto &bind_data = state.bind_data;\n+\n+\tauto num_files = bind_data.file_paths.size();\n+\tidx_t readers_per_file;\n+\tif (bind_data.options.format == JSONFormat::UNSTRUCTURED) {\n+\t\t// Unstructured necessitates single thread\n+\t\treaders_per_file = 1;\n+\t} else if (!state.json_readers.empty() && state.json_readers[0]->IsOpen()) {\n+\t\tauto &reader = *state.json_readers[0];\n+\t\tconst auto &options = reader.GetOptions();\n+\t\tif (options.format == JSONFormat::UNSTRUCTURED || options.compression != FileCompressionType::UNCOMPRESSED) {\n+\t\t\t// Auto-detected unstructured - same story, compression also really limits parallelism\n+\t\t\treaders_per_file = 1;\n+\t\t} else {\n+\t\t\treturn state.system_threads;\n+\t\t}\n+\t} else {\n+\t\treturn state.system_threads;\n+\t}\n+\treturn num_files * readers_per_file;\n }\n \n JSONLocalTableFunctionState::JSONLocalTableFunctionState(ClientContext &context, JSONScanGlobalState &gstate)\n@@ -270,8 +283,10 @@ static inline void SkipWhitespace(const char *buffer_ptr, idx_t &buffer_offset,\n idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {\n \tjson_allocator.Reset();\n \n-\tif (gstate.bind_data.top_level_type == JSONScanTopLevelType::ARRAY_OF_OBJECTS && array_idx < scan_count) {\n-\t\treturn GetObjectsFromArray();\n+\tif ((gstate.bind_data.record_type == JSONRecordType::ARRAY_OF_RECORDS ||\n+\t     gstate.bind_data.record_type == JSONRecordType::ARRAY_OF_JSON) &&\n+\t    array_idx < scan_count) {\n+\t\treturn GetObjectsFromArray(gstate);\n \t}\n \n \tidx_t count = 0;\n@@ -302,13 +317,11 @@ idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {\n \t// Skip over any remaining whitespace for the next scan\n \tSkipWhitespace(buffer_ptr, buffer_offset, buffer_size);\n \n-\tif (gstate.bind_data.top_level_type == JSONScanTopLevelType::ARRAY_OF_OBJECTS) {\n-\t\tif (scan_count > 1) {\n-\t\t\tthrow InvalidInputException(\"File must have exactly one array of objects when format='array_of_objects'\");\n-\t\t}\n+\tif (gstate.bind_data.record_type == JSONRecordType::ARRAY_OF_RECORDS ||\n+\t    gstate.bind_data.record_type == JSONRecordType::ARRAY_OF_JSON) {\n \t\tarray_idx = 0;\n \t\tarray_offset = 0;\n-\t\treturn GetObjectsFromArray();\n+\t\treturn GetObjectsFromArray(gstate);\n \t}\n \n \treturn count;\n@@ -385,18 +398,22 @@ yyjson_val *JSONScanLocalState::ParseLine(char *line_start, idx_t line_size, idx\n \t}\n }\n \n-idx_t JSONScanLocalState::GetObjectsFromArray() {\n+idx_t JSONScanLocalState::GetObjectsFromArray(JSONScanGlobalState &gstate) {\n \tidx_t arr_count = 0;\n \n \tsize_t idx, max;\n \tyyjson_val *val;\n \tfor (; array_idx < scan_count; array_idx++, array_offset = 0) {\n-\t\tif (objects[array_idx]) {\n-\t\t\tyyjson_arr_foreach(objects[array_idx], idx, max, val) {\n+\t\tauto &value = values[array_idx];\n+\t\tif (!value) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (unsafe_yyjson_is_arr(value)) {\n+\t\t\tyyjson_arr_foreach(value, idx, max, val) {\n \t\t\t\tif (idx < array_offset) {\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n-\t\t\t\tarray_objects[arr_count++] = val;\n+\t\t\t\tarray_values[arr_count++] = val;\n \t\t\t\tif (arr_count == STANDARD_VECTOR_SIZE) {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n@@ -405,6 +422,11 @@ idx_t JSONScanLocalState::GetObjectsFromArray() {\n \t\t\tif (arr_count == STANDARD_VECTOR_SIZE) {\n \t\t\t\tbreak;\n \t\t\t}\n+\t\t} else if (!gstate.bind_data.ignore_errors) {\n+\t\t\tThrowTransformError(\n+\t\t\t    array_idx,\n+\t\t\t    StringUtil::Format(\"Expected JSON ARRAY but got %s: %s\\nTry setting json_format to 'records'\",\n+\t\t\t                       JSONCommon::ValTypeToString(value), JSONCommon::ValToString(value, 50)));\n \t\t}\n \t}\n \treturn arr_count;\n@@ -662,7 +684,7 @@ void JSONScanLocalState::ReconstructFirstObject(JSONScanGlobalState &gstate) {\n \t\tcurrent_reader->RemoveBuffer(current_buffer_handle->buffer_index - 1);\n \t}\n \n-\tobjects[0] = ParseLine((char *)reconstruct_ptr, line_size, line_size, lines[0]);\n+\tvalues[0] = ParseLine((char *)reconstruct_ptr, line_size, line_size, lines[0]);\n }\n \n void JSONScanLocalState::ReadUnstructured(idx_t &count) {\n@@ -706,7 +728,7 @@ void JSONScanLocalState::ReadUnstructured(idx_t &count) {\n \t\t} else {\n \t\t\tcurrent_reader->ThrowParseError(current_buffer_handle->buffer_index, lines_or_objects_in_buffer, error);\n \t\t}\n-\t\tobjects[count] = read_doc->root;\n+\t\tvalues[count] = read_doc->root;\n \t}\n }\n \n@@ -732,7 +754,7 @@ void JSONScanLocalState::ReadNewlineDelimited(idx_t &count) {\n \t\t}\n \t\tidx_t line_size = line_end - line_start;\n \n-\t\tobjects[count] = ParseLine((char *)line_start, line_size, remaining, lines[count]);\n+\t\tvalues[count] = ParseLine((char *)line_start, line_size, remaining, lines[count]);\n \n \t\tbuffer_offset += line_size;\n \t\tSkipWhitespace(buffer_ptr, buffer_offset, buffer_size);\n@@ -743,11 +765,11 @@ yyjson_alc *JSONScanLocalState::GetAllocator() {\n \treturn json_allocator.GetYYJSONAllocator();\n }\n \n-void JSONScanLocalState::ThrowTransformError(idx_t count, idx_t object_index, const string &error_message) {\n+void JSONScanLocalState::ThrowTransformError(idx_t object_index, const string &error_message) {\n \tD_ASSERT(current_reader);\n \tD_ASSERT(current_buffer_handle);\n \tD_ASSERT(object_index != DConstants::INVALID_INDEX);\n-\tauto line_or_object_in_buffer = lines_or_objects_in_buffer - count + object_index;\n+\tauto line_or_object_in_buffer = lines_or_objects_in_buffer - scan_count + object_index;\n \tcurrent_reader->ThrowTransformError(current_buffer_handle->buffer_index, line_or_object_in_buffer, error_message);\n }\n \ndiff --git a/src/include/duckdb/common/enums/statement_type.hpp b/src/include/duckdb/common/enums/statement_type.hpp\nindex ed1d4ee28fbe..ce24d65b4541 100644\n--- a/src/include/duckdb/common/enums/statement_type.hpp\n+++ b/src/include/duckdb/common/enums/statement_type.hpp\n@@ -48,7 +48,7 @@ enum class StatementType : uint8_t {\n \n };\n \n-string StatementTypeToString(StatementType type);\n+DUCKDB_API string StatementTypeToString(StatementType type);\n \n enum class StatementReturnType : uint8_t {\n \tQUERY_RESULT, // the statement returns a query result (e.g. for display to the user)\ndiff --git a/src/include/duckdb/parser/query_node/select_node.hpp b/src/include/duckdb/parser/query_node/select_node.hpp\nindex 0b77599c8855..d48dfcb09ad7 100644\n--- a/src/include/duckdb/parser/query_node/select_node.hpp\n+++ b/src/include/duckdb/parser/query_node/select_node.hpp\n@@ -21,7 +21,7 @@ namespace duckdb {\n //! SelectNode represents a standard SELECT statement\n class SelectNode : public QueryNode {\n public:\n-\tSelectNode();\n+\tDUCKDB_API SelectNode();\n \n \t//! The projection list\n \tvector<unique_ptr<ParsedExpression>> select_list;\ndiff --git a/src/include/duckdb/parser/sql_statement.hpp b/src/include/duckdb/parser/sql_statement.hpp\nindex eab3cf99070d..8f3852a92bd7 100644\n--- a/src/include/duckdb/parser/sql_statement.hpp\n+++ b/src/include/duckdb/parser/sql_statement.hpp\n@@ -40,11 +40,11 @@ class SQLStatement {\n \tSQLStatement(const SQLStatement &other) = default;\n \n public:\n-\tvirtual string ToString() const {\n+\tDUCKDB_API virtual string ToString() const {\n \t\tthrow InternalException(\"ToString not supported for this type of SQLStatement: '%s'\",\n \t\t                        StatementTypeToString(type));\n \t}\n \t//! Create a copy of this SelectStatement\n-\tvirtual unique_ptr<SQLStatement> Copy() const = 0;\n+\tDUCKDB_API virtual unique_ptr<SQLStatement> Copy() const = 0;\n };\n } // namespace duckdb\ndiff --git a/src/include/duckdb/parser/statement/copy_statement.hpp b/src/include/duckdb/parser/statement/copy_statement.hpp\nindex 98892d0d41ac..e632eff7dfd8 100644\n--- a/src/include/duckdb/parser/statement/copy_statement.hpp\n+++ b/src/include/duckdb/parser/statement/copy_statement.hpp\n@@ -28,7 +28,7 @@ class CopyStatement : public SQLStatement {\n \tCopyStatement(const CopyStatement &other);\n \n public:\n-\tunique_ptr<SQLStatement> Copy() const override;\n+\tDUCKDB_API unique_ptr<SQLStatement> Copy() const override;\n \n private:\n };\ndiff --git a/src/include/duckdb/parser/statement/select_statement.hpp b/src/include/duckdb/parser/statement/select_statement.hpp\nindex 78b61e6f5339..277c0191d16f 100644\n--- a/src/include/duckdb/parser/statement/select_statement.hpp\n+++ b/src/include/duckdb/parser/statement/select_statement.hpp\n@@ -21,7 +21,7 @@ class QueryNode;\n //! SelectStatement is a typical SELECT clause\n class SelectStatement : public SQLStatement {\n public:\n-\tSelectStatement() : SQLStatement(StatementType::SELECT_STATEMENT) {\n+\tDUCKDB_API SelectStatement() : SQLStatement(StatementType::SELECT_STATEMENT) {\n \t}\n \n \t//! The main query node\n@@ -32,9 +32,9 @@ class SelectStatement : public SQLStatement {\n \n public:\n \t//! Convert the SELECT statement to a string\n-\tstring ToString() const override;\n+\tDUCKDB_API string ToString() const override;\n \t//! Create a copy of this SelectStatement\n-\tunique_ptr<SQLStatement> Copy() const override;\n+\tDUCKDB_API unique_ptr<SQLStatement> Copy() const override;\n \t//! Serializes a SelectStatement to a stand-alone binary blob\n \tvoid Serialize(Serializer &serializer) const;\n \t//! Deserializes a blob back into a SelectStatement, returns nullptr if\ndiff --git a/src/include/duckdb/parser/tableref/subqueryref.hpp b/src/include/duckdb/parser/tableref/subqueryref.hpp\nindex d712ea2a2376..847b9226003b 100644\n--- a/src/include/duckdb/parser/tableref/subqueryref.hpp\n+++ b/src/include/duckdb/parser/tableref/subqueryref.hpp\n@@ -15,7 +15,7 @@ namespace duckdb {\n //! Represents a subquery\n class SubqueryRef : public TableRef {\n public:\n-\texplicit SubqueryRef(unique_ptr<SelectStatement> subquery, string alias = string());\n+\tDUCKDB_API explicit SubqueryRef(unique_ptr<SelectStatement> subquery, string alias = string());\n \n \t//! The subquery\n \tunique_ptr<SelectStatement> subquery;\n",
  "test_patch": "diff --git a/test/sql/json/read_json.test b/test/sql/json/read_json.test\nindex ca2e235768cc..74853ee16531 100644\n--- a/test/sql/json/read_json.test\n+++ b/test/sql/json/read_json.test\n@@ -50,7 +50,7 @@ NULL\tNULL\n 4\tBroadcast News\n 5\tRaising Arizona\n \n-# some of these objects don't have \"name\"\n+# some of these values don't have \"name\"\n query II\n SELECT * FROM read_json('data/json/different_schemas.ndjson', columns={id: 'INTEGER', name: 'VARCHAR'})\n ----\n@@ -186,28 +186,46 @@ d5c52052-5f8e-473f-bc8d-176342643ef5\tUUID\n ae24e69e-e0bf-4e85-9848-27d35df85b8b\tUUID\n 63928b16-1814-436f-8b30-b3c40cc31d51\tUUID\n \n-# top-level array of objects\n+# top-level array of values\n query I\n-select * from read_json('data/json/top_level_array.json', columns={conclusion: 'VARCHAR'}, format='array_of_objects')\n+select * from read_json('data/json/top_level_array.json', columns={conclusion: 'VARCHAR'}, json_format='array_of_records')\n ----\n cancelled\n cancelled\n \n+query I\n+select * from read_json('data/json/top_level_array.json', auto_detect=true, json_format='array_of_records')\n+----\n+cancelled\n+cancelled\n+\n+# if we try to read it with 'unstructured', we get an error\n+statement error\n+select * from read_json('data/json/top_level_array.json', columns={conclusion: 'VARCHAR'}, lines='false')\n+----\n+Invalid Input Error: JSON transform error in file \"data/json/top_level_array.json\", in object 1: Expected OBJECT, but got ARRAY\n+\n+# if we try to read an ndjson file as if it is an array of values, we get an error\n+statement error\n+select * from read_json_auto('data/json/example_n.ndjson', json_format='array_of_records')\n+----\n+Invalid Input Error: JSON transform error in file \"data/json/example_n.ndjson\", in line 1: Expected JSON ARRAY but got OBJECT\n+\n # test that we can read a list of longer than STANDARD_VECTOR_SIZE properly\n statement ok\n copy (select list(to_json({duck: 42})) from range(10000)) to '__TEST_DIR__/my_file.json' (format csv, quote '')\n \n query T\n-select count(*) from read_json('__TEST_DIR__/my_file.json', columns={duck: 'INTEGER'}, format='array_of_objects')\n+select count(*) from read_json('__TEST_DIR__/my_file.json', columns={duck: 'INTEGER'}, json_format='array_of_records')\n ----\n 10000\n \n query T\n-select sum(duck) = 42*10000 from read_json('__TEST_DIR__/my_file.json', columns={duck: 'INTEGER'}, format='array_of_objects')\n+select sum(duck) = 42*10000 from read_json('__TEST_DIR__/my_file.json', columns={duck: 'INTEGER'}, json_format='array_of_records')\n ----\n true\n \n-# read_json_auto also understands array of objects\n+# read_json_auto also understands array of values\n query T\n select count(*) from '__TEST_DIR__/my_file.json'\n ----\n@@ -218,6 +236,42 @@ select sum(duck) = 42*10000 from '__TEST_DIR__/my_file.json'\n ----\n true\n \n+# what if we do an array of non-values?\n+statement ok\n+copy (select list(range) from range(10)) to '__TEST_DIR__/my_file.json' (format csv, quote '')\n+\n+query T\n+select * from '__TEST_DIR__/my_file.json'\n+----\n+0\n+1\n+2\n+3\n+4\n+5\n+6\n+7\n+8\n+9\n+\n+query T\n+select * from read_json('__TEST_DIR__/my_file.json', json_format='array_of_values', columns={range: 'INTEGER'})\n+----\n+0\n+1\n+2\n+3\n+4\n+5\n+6\n+7\n+8\n+9\n+\n+# fails because it's not an array of records\n+statement error\n+select * from read_json_auto('__TEST_DIR__/my_file.json', json_format='array_of_records')\n+\n statement ok\n pragma disable_verification\n \ndiff --git a/test/sql/json/read_json_auto.test_slow b/test/sql/json/read_json_auto.test_slow\nindex 916050245341..16cc5c4f5fd0 100644\n--- a/test/sql/json/read_json_auto.test_slow\n+++ b/test/sql/json/read_json_auto.test_slow\n@@ -84,7 +84,7 @@ select * from '__TEST_DIR__/my_file.json'\n bar\t0\n baz\t1\n \n-# we can read objects from a top-level list\n+# we can read values from a top-level list\n query I\n select * from 'data/json/top_level_array.json'\n ----\n@@ -96,11 +96,12 @@ select count(*) from 'data/json/top_level_array.json'\n ----\n 2\n \n-# for maximum_depth=0 this is an array of objects\n+# for maximum_depth=0 this is two records of JSON\n query I\n select * from read_json_auto('data/json/top_level_array.json', maximum_depth=0)\n ----\n-[{\"conclusion\":\"cancelled\"}, {\"conclusion\":\"cancelled\"}]\n+{\"conclusion\":\"cancelled\"}\n+{\"conclusion\":\"cancelled\"}\n \n # for 1 it's 1 column of JSON\n query I\ndiff --git a/test/sql/json/read_json_objects.test b/test/sql/json/read_json_objects.test\nindex 18e9f3594bbe..4ccd98e0f8d5 100644\n--- a/test/sql/json/read_json_objects.test\n+++ b/test/sql/json/read_json_objects.test\n@@ -46,7 +46,7 @@ SELECT * FROM read_ndjson_objects('data/json/example_n.ndjson')\n \n # we can auto-detect that it's newline-delimited\n query I\n-SELECT * FROM read_json_objects('data/json/example_n.ndjson', format='auto')\n+SELECT * FROM read_json_objects('data/json/example_n.ndjson', lines='auto')\n ----\n {\"id\":1,\"name\":\"O Brother, Where Art Thou?\"}\n {\"id\":2,\"name\":\"Home for the Holidays\"}\n@@ -66,7 +66,7 @@ SELECT * FROM read_json_objects('data/json/example_r.ndjson')\n \n # we can detect that it's not newline-delimited\n query I\n-SELECT * FROM read_json_objects('data/json/example_r.ndjson', format='auto')\n+SELECT * FROM read_json_objects('data/json/example_r.ndjson', lines='auto')\n ----\n {\"id\":1,\"name\":\"O Brother, Where Art Thou?\"}\n {\"id\":2,\"name\":\"Home for the Holidays\"}\n@@ -183,7 +183,7 @@ Invalid Input Error: Malformed JSON in file \"data/json/unterminated_quotes.ndjso\n \n # we can auto-detect and ignore the error (becomes NULL)\n query I\n-select * from read_ndjson_objects('data/json/unterminated_quotes.ndjson', format='auto', ignore_errors=true)\n+select * from read_ndjson_objects('data/json/unterminated_quotes.ndjson', lines='auto', ignore_errors=true)\n ----\n {\"id\":1,\"name\":\"O Brother, Where Art Thou?\"}\n {\"id\":2,\"name\":\"Home for the Holidays\"}\n@@ -191,7 +191,7 @@ NULL\n {\"id\":4,\"name\":\"Broadcast News\"}\n {\"id\":5,\"name\":\"Raising Arizona\"}\n \n-# multiple objects per line (works for read_json_objects)\n+# multiple values per line (works for read_json_objects)\n query I\n select * from read_json_objects('data/json/multiple_objects_per_line.ndjson')\n ----\ndiff --git a/test/sql/json/test_json_copy.test_slow b/test/sql/json/test_json_copy.test_slow\nindex 8614ae6b2bbe..a0f01d979206 100644\n--- a/test/sql/json/test_json_copy.test_slow\n+++ b/test/sql/json/test_json_copy.test_slow\n@@ -4,6 +4,90 @@\n \n require json\n \n+# test issue #6305\n+statement ok\n+copy (\n+    select * from values\n+    (uuid(), 10),\n+    (uuid(), 10),\n+    (uuid(), 15),\n+    (uuid(), 5)\n+    v (order_id, revenue)\n+) to '__TEST_DIR__/query.json' (format json)\n+\n+query II\n+select typeof(order_id), revenue from '__TEST_DIR__/query.json'\n+----\n+UUID\t10\n+UUID\t10\n+UUID\t15\n+UUID\t5\n+\n+# struct star expression should work too\n+statement ok\n+copy (\n+    select v.* from values\n+    ({order_id: uuid(), revenue: 10}),\n+    ({order_id: uuid(), revenue: 10}),\n+    ({order_id: uuid(), revenue: 15}),\n+    ({order_id: uuid(), revenue: 5}),\n+    t (v)\n+) to '__TEST_DIR__/query.json' (format json)\n+\n+query II\n+select typeof(order_id), revenue from '__TEST_DIR__/query.json'\n+----\n+UUID\t10\n+UUID\t10\n+UUID\t15\n+UUID\t5\n+\n+# exclude\n+statement ok\n+copy (\n+    select order_id, * exclude (order_id) from values\n+    (uuid(), 10),\n+    (uuid(), 10),\n+    (uuid(), 15),\n+    (uuid(), 5)\n+    v (order_id, revenue)\n+) to '__TEST_DIR__/query.json' (format json)\n+\n+query II\n+select typeof(order_id), revenue from '__TEST_DIR__/query.json'\n+----\n+UUID\t10\n+UUID\t10\n+UUID\t15\n+UUID\t5\n+\n+# and finally, replace\n+statement ok\n+copy (\n+    select * replace (revenue + 1 as revenue) from values\n+    (uuid(), 10),\n+    (uuid(), 10),\n+    (uuid(), 15),\n+    (uuid(), 5)\n+    v (order_id, revenue)\n+) to '__TEST_DIR__/query.json' (format json)\n+\n+query II\n+select typeof(order_id), revenue from '__TEST_DIR__/query.json'\n+----\n+UUID\t11\n+UUID\t11\n+UUID\t16\n+UUID\t6\n+\n+statement ok\n+copy (select 42 as a, a + 1) to '__TEST_DIR__/out.json' (format json);\n+\n+query II\n+select * from '__TEST_DIR__/out.json'\n+----\n+42\t43\n+\n require tpch\n \n statement ok\ndiff --git a/test/sql/json/test_json_structure.test b/test/sql/json/test_json_structure.test\nindex 92affc50f8c0..a5b1326727e9 100644\n--- a/test/sql/json/test_json_structure.test\n+++ b/test/sql/json/test_json_structure.test\n@@ -33,7 +33,7 @@ select json_structure('\"duck\"')\n ----\n \"VARCHAR\"\n \n-# simple objects\n+# simple values\n query T\n select json_structure('{\"a\": 42}')\n ----\n",
  "problem_statement": "Segmentation error with json extension and values clause\n### What happens?\n\nThe python client crashes out with a segmentation fault when combining the `values` clause and the json extension.\r\n\r\n\n\n### To Reproduce\n\n/// python\r\n\r\n```python\r\nimport duckdb\r\n\r\ncon = duckdb.connect()\r\n\r\ncon.execute(\"install json\")\r\ncon.execute(\"load json\")\r\n\r\ncon.execute(\"\"\"\r\n    copy (\r\n        select * from values\r\n        (uuid(), 10),\r\n        (uuid(), 10),\r\n        (uuid(), 15),\r\n        (uuid(), 5)\r\n        v (order_id, revenue)\r\n    ) to 'query.json' (format json)\r\n\"\"\")\r\n```\r\n\r\nIn the above query, swapping the format to CSV / parquet yields a query that executes without a problem.\r\n\r\nI'm not able to validate this on the prerelease (master branch) version because the `json` extension.\r\n\r\nI also ran into a buffer error a while ago related to this that I was unable to find again.\n\n### OS:\n\nMac OS\n\n### DuckDB Version:\n\n0.7.0\n\n### DuckDB Client:\n\nPython (3.11)\n\n### Full Name:\n\nJames McNeill\n\n### Affiliation:\n\nConjura\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nSegmentation error with json extension and values clause\n### What happens?\n\nThe python client crashes out with a segmentation fault when combining the `values` clause and the json extension.\r\n\r\n\n\n### To Reproduce\n\n/// python\r\n\r\n```python\r\nimport duckdb\r\n\r\ncon = duckdb.connect()\r\n\r\ncon.execute(\"install json\")\r\ncon.execute(\"load json\")\r\n\r\ncon.execute(\"\"\"\r\n    copy (\r\n        select * from values\r\n        (uuid(), 10),\r\n        (uuid(), 10),\r\n        (uuid(), 15),\r\n        (uuid(), 5)\r\n        v (order_id, revenue)\r\n    ) to 'query.json' (format json)\r\n\"\"\")\r\n```\r\n\r\nIn the above query, swapping the format to CSV / parquet yields a query that executes without a problem.\r\n\r\nI'm not able to validate this on the prerelease (master branch) version because the `json` extension.\r\n\r\nI also ran into a buffer error a while ago related to this that I was unable to find again.\n\n### OS:\n\nMac OS\n\n### DuckDB Version:\n\n0.7.0\n\n### DuckDB Client:\n\nPython (3.11)\n\n### Full Name:\n\nJames McNeill\n\n### Affiliation:\n\nConjura\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "I'm definitely happy to have a go at fixing this but I am a cpp noob :(\n@Tishj on your comment - unfortunately `con.sql` returns the same error \ud83d\ude22\nIn fact, basically nothing seems to export into `format json` for me. it's possible that it's just something weird on my machine \ud83d\ude05.\r\n\r\nI've tried all permutations of:\r\n1. exporting the result of a table\r\n2. both in memory and \"with path\" db connections\r\n\nI'm definitely happy to have a go at fixing this but I am a cpp noob :(\n@Tishj on your comment - unfortunately `con.sql` returns the same error \ud83d\ude22\nIn fact, basically nothing seems to export into `format json` for me. it's possible that it's just something weird on my machine \ud83d\ude05.\r\n\r\nI've tried all permutations of:\r\n1. exporting the result of a table\r\n2. both in memory and \"with path\" db connections\r\n",
  "created_at": "2023-02-16T09:33:44Z"
}