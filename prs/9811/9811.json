{
  "repo": "duckdb/duckdb",
  "pull_number": 9811,
  "instance_id": "duckdb__duckdb-9811",
  "issue_numbers": [
    "6669"
  ],
  "base_commit": "275f4a7e9564db08cc3cda211e1f63a7967308cf",
  "patch": "diff --git a/tools/pythonpkg/src/pandas/analyzer.cpp b/tools/pythonpkg/src/pandas/analyzer.cpp\nindex dafb5d05eaf8..dd7992515654 100644\n--- a/tools/pythonpkg/src/pandas/analyzer.cpp\n+++ b/tools/pythonpkg/src/pandas/analyzer.cpp\n@@ -363,6 +363,16 @@ uint64_t PandasAnalyzer::GetSampleIncrement(idx_t rows) {\n \treturn rows / sample;\n }\n \n+static py::object FindFirstNonNull(const py::handle &row, idx_t offset, idx_t range) {\n+\tfor (idx_t i = 0; i < range; i++) {\n+\t\tauto obj = row(offset + i);\n+\t\tif (!obj.is_none()) {\n+\t\t\treturn obj;\n+\t\t}\n+\t}\n+\treturn py::none();\n+}\n+\n LogicalType PandasAnalyzer::InnerAnalyze(py::object column, bool &can_convert, bool sample, idx_t increment) {\n \tidx_t rows = py::len(column);\n \n@@ -380,18 +390,14 @@ LogicalType PandasAnalyzer::InnerAnalyze(py::object column, bool &can_convert, b\n \t}\n \tauto row = column.attr(\"__getitem__\");\n \n-\tvector<LogicalType> types;\n-\tauto item_type = GetItemType(row(0), can_convert);\n-\tif (!can_convert) {\n-\t\treturn item_type;\n-\t}\n-\ttypes.push_back(item_type);\n-\n \tif (sample) {\n \t\tincrement = GetSampleIncrement(rows);\n \t}\n-\tfor (idx_t i = increment; i < rows; i += increment) {\n-\t\tauto next_item_type = GetItemType(row(i), can_convert);\n+\tLogicalType item_type = LogicalType::SQLNULL;\n+\tvector<LogicalType> types;\n+\tfor (idx_t i = 0; i < rows; i += increment) {\n+\t\tauto obj = FindFirstNonNull(row, i, increment);\n+\t\tauto next_item_type = GetItemType(obj, can_convert);\n \t\ttypes.push_back(next_item_type);\n \n \t\tif (!can_convert || !UpgradeType(item_type, next_item_type)) {\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\nindex 79c8b1f7dacb..88e43c2cb0d6 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n@@ -15,6 +15,19 @@ def create_generic_dataframe(data, pandas):\n     return pandas.DataFrame({'0': pandas.Series(data=data, dtype='object')})\n \n \n+def create_repeated_nulls(size):\n+    data = [None, \"a\"]\n+    n = size\n+    data = data * n\n+    return data\n+\n+\n+def create_trailing_non_null(size):\n+    data = [None for _ in range(size - 1)]\n+    data.append('this is a long string')\n+    return data\n+\n+\n class IntString:\n     def __init__(self, value: int):\n         self.value = value\n@@ -206,6 +219,18 @@ def test_map_correct(self, pandas):\n         print(converted_col.columns)\n         pandas.testing.assert_frame_equal(converted_col, duckdb_col)\n \n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    @pytest.mark.parametrize('sample_size', [1, 10])\n+    @pytest.mark.parametrize('fill', [1000, 10000])\n+    @pytest.mark.parametrize('get_data', [create_repeated_nulls, create_trailing_non_null])\n+    def test_analyzing_nulls(self, pandas, duckdb_cursor, fill, sample_size, get_data):\n+        data = get_data(fill)\n+        df1 = pandas.DataFrame(data={\"col1\": data})\n+        duckdb_cursor.execute(f\"SET GLOBAL pandas_analyze_sample={sample_size}\")\n+        df = duckdb_cursor.execute(\"select * from df1\").df()\n+\n+        pandas.testing.assert_frame_equal(df1, df)\n+\n     @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n     def test_map_value_upgrade(self, pandas):\n         con = duckdb.connect()\n",
  "problem_statement": "Unimplemented type for cast (VARCHAR -> NULL) error for aggregation on large string column\n### What happens?\r\n\r\nFor a query involving a string column with NULLs, on a relatively large DataFrame (3.2 million rows), I receive the following error:\r\n\r\n```\r\nInvalidInputException: Invalid Input Error: Failed to cast value: Unimplemented type for cast (VARCHAR -> NULL)\r\n```\r\nThe query completes successfully when the dataset is slightly smaller.\r\n\r\n### To Reproduce\r\n\r\nLoad an initial 3201 row dataset and remove all but one column (`Major_Genre`). This is a VARCHAR column that contains NULL values.\r\n\r\n```python\r\nimport pandas as pd\r\nimport duckdb\r\n\r\n# Load movies dataset using pandas and remove all but one string column\r\nmovies = pd.read_json('https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/movies.json')\r\nmovies = movies[[\"Major_Genre\"]]\r\nduckdb.query(\"SELECT * from movies\")\r\n```\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    Major_Genre    \u2502\r\n\u2502      varchar      \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 NULL              \u2502\r\n\u2502 Drama             \u2502\r\n\u2502 Comedy            \u2502\r\n\u2502 Comedy            \u2502\r\n\u2502 Drama             \u2502\r\n\u2502 NULL              \u2502\r\n\u2502 NULL              \u2502\r\n\u2502 Comedy            \u2502\r\n\u2502 NULL              \u2502\r\n\u2502 NULL              \u2502\r\n\u2502  \u00b7                \u2502\r\n\u2502  \u00b7                \u2502\r\n\u2502  \u00b7                \u2502\r\n\u2502 Drama             \u2502\r\n\u2502 Adventure         \u2502\r\n\u2502 Comedy            \u2502\r\n\u2502 Comedy            \u2502\r\n\u2502 Comedy            \u2502\r\n\u2502 Comedy            \u2502\r\n\u2502 Thriller/Suspense \u2502\r\n\u2502 Adventure         \u2502\r\n\u2502 Adventure         \u2502\r\n\u2502 Adventure         \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502     3201 rows     \u2502\r\n\u2502    (20 shown)     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nDuplicate and concat the original dataset `n` times to create a larger dataset, then count the number of each unique value of the Major_Genera column.\r\n\r\nWhen n is 999, the total number of rows of the dataset is 3197799 and the query completes as expected.\r\n```python\r\nn = 999\r\nmovies_medium = pd.concat([movies] * n, axis=0).reset_index()\r\nprint(len(movies_medium))\r\nduckdb.query(\"SELECT Major_Genre, COUNT(*) from movies_medium GROUP BY Major_Genre\")\r\n```\r\n```\r\n3197799\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502     Major_Genre     \u2502 count_star() \u2502\r\n\u2502       varchar       \u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Action              \u2502       419580 \u2502\r\n\u2502 Drama               \u2502       788211 \u2502\r\n\u2502 NULL                \u2502       274725 \u2502\r\n\u2502 Comedy              \u2502       674325 \u2502\r\n\u2502 Black Comedy        \u2502        35964 \u2502\r\n\u2502 Adventure           \u2502       273726 \u2502\r\n\u2502 Thriller/Suspense   \u2502       238761 \u2502\r\n\u2502 Musical             \u2502        52947 \u2502\r\n\u2502 Romantic Comedy     \u2502       136863 \u2502\r\n\u2502 Horror              \u2502       218781 \u2502\r\n\u2502 Western             \u2502        35964 \u2502\r\n\u2502 Documentary         \u2502        42957 \u2502\r\n\u2502 Concert/Performance \u2502         4995 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 13 rows                  2 columns \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nWhen n is 1000, the total number of rows of the dataset is 3201000 and the query raises an error:\r\n\r\n```python\r\nn = 1000\r\nmovies_medium = pd.concat([movies] * n, axis=0).reset_index()\r\nprint(len(movies_medium))\r\nduckdb.query(\"SELECT Major_Genre, COUNT(*) from movies_medium GROUP BY Major_Genre\")\r\n```\r\n```\r\n3201000\r\n---------------------------------------------------------------------------\r\nInvalidInputException                     Traceback (most recent call last)\r\n...\r\nInvalidInputException: Invalid Input Error: Failed to cast value: Unimplemented type for cast (VARCHAR -> NULL)\r\n```\r\n\r\nIf nulls are removed, the query works for even much larger datasets\r\n```\r\nn = 2000\r\nmovies_medium = pd.concat([movies] * n, axis=0).reset_index().dropna()\r\nprint(len(movies_medium))\r\nduckdb.query(\"SELECT Major_Genre, COUNT(*) from movies_medium GROUP BY Major_Genre\")\r\n```\r\n```\r\n5852000\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502     Major_Genre     \u2502 count_star() \u2502\r\n\u2502       varchar       \u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Thriller/Suspense   \u2502       478000 \u2502\r\n\u2502 Drama               \u2502      1578000 \u2502\r\n\u2502 Horror              \u2502       438000 \u2502\r\n\u2502 Adventure           \u2502       548000 \u2502\r\n\u2502 Action              \u2502       840000 \u2502\r\n\u2502 Romantic Comedy     \u2502       274000 \u2502\r\n\u2502 Comedy              \u2502      1350000 \u2502\r\n\u2502 Documentary         \u2502        86000 \u2502\r\n\u2502 Black Comedy        \u2502        72000 \u2502\r\n\u2502 Musical             \u2502       106000 \u2502\r\n\u2502 Western             \u2502        72000 \u2502\r\n\u2502 Concert/Performance \u2502        10000 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 12 rows                  2 columns \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\n### OS:\r\n\r\nmacOS Ventura 13.2.1\r\n\r\n### DuckDB Version:\r\n\r\n0.7.2-dev586. The error is present in 0.7.0 and beyond, but there is no error in 0.6.0.\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nJon Mease\r\n\r\n### Affiliation:\r\n\r\nVegaFusion / Hex\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "Is this just because of the pandas_analyze_sample limit? \r\n\r\nhttps://duckdb.org/docs/api/python/data_ingestion#pandas-dataframes---object-columns\r\n\r\n\nThanks @paul-iqmo,\r\n\r\nI wouldn't have though so. The dataset itself is 3201 rows, and when I increase the size, it's concatenated with itself, so both string add null values are present all throughout the final DataFrame.  It's also an error that wasn't present in version 0.6.0.\r\n\r\nThat said, I just tried it and setting `pandas_analyze_sample` to 10000 does eliminate the error.\r\n\r\n```python\r\nduckdb.default_connection.execute(\"SET GLOBAL pandas_analyze_sample=10000\")\r\nn = 1000\r\nmovies_medium = pd.concat([movies] * n, axis=0).reset_index()\r\nprint(len(movies_medium))\r\nduckdb.query(\"SELECT Major_Genre, COUNT(*) from movies_medium GROUP BY Major_Genre\")\r\n```\r\n```\r\n3201000\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502     Major_Genre     \u2502 count_star() \u2502\r\n\u2502       varchar       \u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Action              \u2502       420000 \u2502\r\n\u2502 Black Comedy        \u2502        36000 \u2502\r\n\u2502 Adventure           \u2502       274000 \u2502\r\n\u2502 Musical             \u2502        53000 \u2502\r\n\u2502 Horror              \u2502       219000 \u2502\r\n\u2502 Drama               \u2502       789000 \u2502\r\n\u2502 Thriller/Suspense   \u2502       239000 \u2502\r\n\u2502 Comedy              \u2502       675000 \u2502\r\n\u2502 NULL                \u2502       275000 \u2502\r\n\u2502 Romantic Comedy     \u2502       137000 \u2502\r\n\u2502 Western             \u2502        36000 \u2502\r\n\u2502 Concert/Performance \u2502         5000 \u2502\r\n\u2502 Documentary         \u2502        43000 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 13 rows                  2 columns \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nI'm happy to make this change in my own code, but does it make sense that this would be needed in this case?\r\n\nI did a little testing, and it seems to occur whenever your dataset is a multiple of pandas_analyze_sample.\r\n\r\nFor example, with pandas_analyze_sample = 1000, the error occurs when \"n\" = 1000, 2000, 3000. And, when pandas_analyze_sample = 999, the error occurs when n = 999, 999*2, etc\r\n\r\nWithout looking at the sampling code, my guess is when the data repetition is aligned with pandas_analyze_sample, duckdb is always sampling the same element N times.\r\n\r\nThis is surprising to me, as I had assumed that pandas_analyze_sample was actually reading the first N items... not actually sampling. \r\n\r\n---\r\nEdit: \r\n\r\nI think this explains it: \r\n```\r\nuint64_t PandasAnalyzer::GetSampleIncrement(idx_t rows) {\r\n\tD_ASSERT(sample_size != 0);\r\n\t//! Apply the maximum\r\n\tauto sample = sample_size;\r\n\tif (sample > rows) {\r\n\t\tsample = rows;\r\n\t}\r\n\treturn rows / sample;\r\n}\r\n```\r\nhttps://github.com/duckdb/duckdb/blob/79f67b86260c76cd54175b9dd91d7924761a9f44/tools/pythonpkg/src/pandas_analyzer.cpp#L336]\r\n\r\nIf you have 3201 * 1000 rows, with pandas_analyze_sample = 1000, then it'll increment every 3201 rows... in other words: it's sampling the same element repeatedly. \n> I did a little testing, and it seems to occur whenever your dataset is a multiple of pandas_analyze_sample.\r\n> \r\n> For example, with pandas_analyze_sample = 1000, the error occurs when \"n\" = 1000, 2000, 3000. And, when pandas_analyze_sample = 999, the error occurs when n = 999, 999*2, etc\r\n> \r\n> Without looking at the sampling code, my guess is when the data repetition is aligned with pandas_analyze_sample, duckdb is always sampling the same element N times.\r\n> \r\n> This is surprising to me, as I had assumed that pandas_analyze_sample was actually reading the first N items... not actually sampling.\r\n> \r\n> Edit:\r\n> \r\n> I think this explains it:\r\n> \r\n> ```\r\n> uint64_t PandasAnalyzer::GetSampleIncrement(idx_t rows) {\r\n> \tD_ASSERT(sample_size != 0);\r\n> \t//! Apply the maximum\r\n> \tauto sample = sample_size;\r\n> \tif (sample > rows) {\r\n> \t\tsample = rows;\r\n> \t}\r\n> \treturn rows / sample;\r\n> }\r\n> ```\r\n> \r\n> [https://github.com/duckdb/duckdb/blob/79f67b86260c76cd54175b9dd91d7924761a9f44/tools/pythonpkg/src/pandas_analyzer.cpp#L336]](https://github.com/duckdb/duckdb/blob/79f67b86260c76cd54175b9dd91d7924761a9f44/tools/pythonpkg/src/pandas_analyzer.cpp#L336%5D)\r\n> \r\n> If you have 3201 * 1000 rows, with pandas_analyze_sample = 1000, then it'll increment every 3201 rows... in other words: it's sampling the same element repeatedly.\r\n\r\nI completely missed your comment at the time, but reading it back that doesn't seem correct?\r\nEither way I think it's worth verifying so I'll definitely have a look at some point\nIt's been a while since I looked at this.. but I agree it doesn't sound right (and I only did a cursory look at the code), but this seemed consistent with the repro below: \r\n\r\nThis code will always throw an InvalidInputException for any n (data is repeated n times) when offset=0. Change offset=1, and it'll pass. \r\n\r\n```\r\nimport pandas as pd \r\nimport duckdb \r\ndata = [None, \"a\"]\r\nn = 100\r\noffset = 0\r\n\r\ndata = data * n\r\ndf1 = pd.DataFrame(data={\"col1\": data})\r\n\r\nwith duckdb.connect() as con:\r\n    con.execute(f\"SET global pandas_analyze_sample={n + offset}\")\r\n    df = con.execute(\"select * from df1\").df()\r\n    print(df)\r\n```\r\n\r\nFor any n > 0, you'll get: \r\n> InvalidInputException: Invalid Input Error: Failed to cast value: Unimplemented type for cast (VARCHAR -> NULL)\r\n\r\n     \nI'm closing this because this is expected behavior.\r\n\r\nTo fix this we would need to change the sampling strategy, and this issue only occurs when you have highly repetitive data, and the sample size is equal to the rate of repetition (or a multiple of it).\r\n\r\nAs was already said this can be fixed by changing the sampling size.\nIt is *wild* that you can consider this to be expected behaviour.\r\nGiven that most applications would not know in advance the sizes of datasets they have to work with, leaving this issue in my data stack is a game of Russian Roulette. \r\nThere are a number of similar threads that have been closed off.\r\nIf this isn't taken seriously, I would actively advise against using duckdb.\r\n  \n> If this isn't taken seriously, I would actively advise against using duckdb.\r\n\r\nI think that is maybe getting a bit carried away but I agree that this behaviour is super annoying.  I've faced this error many times myself and the frustrating thing is that the error message itself is quite misleading.  Would be very helpful if more was done to address this.\n> I think that is maybe getting a bit carried away but I agree that this behaviour is super annoying. \r\n\r\nThe bug is annoying, but the bigger issue is the way this has repeatedly been considered a feature rather than a bug! Perhaps a flag to turn the sampling feature off, rather than having to set a number and hope a dataset never matches a multiple of it in the future. ",
  "created_at": "2023-11-27T12:55:56Z"
}