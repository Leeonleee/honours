{
  "repo": "duckdb/duckdb",
  "pull_number": 4909,
  "instance_id": "duckdb__duckdb-4909",
  "issue_numbers": [
    "4903"
  ],
  "base_commit": "c2e70c43dfeb083aa6d00d676dc51634ec9dd87d",
  "patch": "diff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex 123d100c3239..0b697de5f200 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -258,7 +258,7 @@ unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(const FileMetaData\n \n \tif (!s_ele.__isset.type) { // inner node\n \t\tif (s_ele.num_children == 0) {\n-\t\t\tthrow std::runtime_error(\"Node has no children but should\");\n+\t\t\tthrow InvalidInputException(\"Node has no children but should\");\n \t\t}\n \t\tchild_list_t<LogicalType> child_types;\n \t\tvector<unique_ptr<ColumnReader>> child_readers;\n@@ -575,7 +575,7 @@ uint64_t ParquetReader::GetGroupCompressedSize(ParquetReaderScanState &state) {\n \n \tif (total_compressed_size != 0 && calc_compressed_size != 0 &&\n \t    (idx_t)total_compressed_size != calc_compressed_size) {\n-\t\tthrow std::runtime_error(\"mismatch between calculated compressed size and reported compressed size\");\n+\t\tthrow InvalidInputException(\"mismatch between calculated compressed size and reported compressed size\");\n \t}\n \n \treturn total_compressed_size ? total_compressed_size : calc_compressed_size;\n@@ -918,7 +918,7 @@ bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &resul\n \t\t\tdouble scan_percentage = (double)(to_scan_compressed_bytes) / total_row_group_span;\n \n \t\t\tif (to_scan_compressed_bytes > total_row_group_span) {\n-\t\t\t\tthrow std::runtime_error(\n+\t\t\t\tthrow InvalidInputException(\n \t\t\t\t    \"Malformed parquet file: sum of total compressed bytes of columns seems incorrect\");\n \t\t\t}\n \n@@ -1042,11 +1042,12 @@ bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &resul\n \t\t\t\tcontinue;\n \t\t\t}\n \n-\t\t\t//\t\t\tstd::cout << \"Reading nofilter for col \" <<\n-\t\t\t// root_reader->GetChildReader(file_col_idx)->Schema().name\n-\t\t\t//<< \"\\n\";\n-\t\t\troot_reader->GetChildReader(file_col_idx)\n-\t\t\t    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);\n+\t\t\tauto rows_read = root_reader->GetChildReader(file_col_idx)\n+\t\t\t                     ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);\n+\t\t\tif (rows_read != result.size()) {\n+\t\t\t\tthrow InvalidInputException(\"Mismatch in parquet read for column %llu, expected %llu rows, got %llu\",\n+\t\t\t\t                            file_col_idx, result.size(), rows_read);\n+\t\t\t}\n \t\t}\n \t}\n \n",
  "test_patch": "diff --git a/data/parquet-testing/bug4903.parquet b/data/parquet-testing/bug4903.parquet\nnew file mode 100644\nindex 000000000000..2979df699383\nBinary files /dev/null and b/data/parquet-testing/bug4903.parquet differ\ndiff --git a/test/sql/copy/parquet/parquet_4903.test b/test/sql/copy/parquet/parquet_4903.test\nnew file mode 100644\nindex 000000000000..778576be11ed\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_4903.test\n@@ -0,0 +1,9 @@\n+# name: test/sql/copy/parquet/parquet_4903.test\n+# description: Issue #4442: Parquet reader converts timestamp to i64 *sometimes*\n+# group: [parquet]\n+\n+require parquet\n+\n+# file is corrupt\n+statement error\n+SELECT type_param_constraints FROM 'data/parquet-testing/bug4903.parquet' limit 10\n",
  "problem_statement": "Segfault on reading parquet file\n### What happens?\n\nI read a parquet file I created using parquet-dotnet and DuckDB segfaults\r\n\r\n```\r\nD select * from 'symbol_signature.parquet' limit 10 ;\r\nfish: Job 1, 'duckdb' terminated by signal SIGSEGV (Address boundary error)\r\n```\r\n\r\nMaybe the file is somehow invalid, but I guess it shouldn't segfault anyway. [pqrs](https://github.com/manojkarthick/pqrs) opens the file just fine. It only fails about 50% of the time.\n\n### To Reproduce\n\n[symbol_signature.parquet.gz](https://github.com/duckdb/duckdb/files/9728412/symbol_signature.parquet.gz) (gzipped because github does not like parquet. Contains only 2 rows, 5KB uncompressed. For some reason, it doesn't crash with only one of the rows)\r\n\r\n```\r\nexyi@NaN ...> duckdb\r\nD select * from 'symbol_signature.parquet' limit 10 ;\r\nfish: Job 1, 'duckdb' terminated by signal SIGSEGV (Address boundary error)\r\n```\n\n### OS:\n\nArch Linux | 5.19.13-arch1-1 #1 SMP PREEMPT_DYNAMIC Tue, 04 Oct 2022 14:36:58 +0000 x86_64 GNU/Linux\n\n### DuckDB Version:\n\nv0.5.1 7c111322de\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nStanislav Luke\u0161\n\n### Affiliation:\n\nDefinitely no oracle\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "Indeed that's a bug, it triggers an internal assert \r\n`INTERNAL Error: Assertion triggered in file \"/Users/hannes/source/duckdb/src/common/types/vector.cpp\" on line 1193: le.offset + le.length <= child_size`\nThe file is definitely corrupted, pyarrow says `pyarrow.lib.ArrowInvalid: Length spanned by list offsets (1) larger than values array (length 0)`",
  "created_at": "2022-10-07T09:10:40Z"
}