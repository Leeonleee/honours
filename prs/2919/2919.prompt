You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
No Longer Convert Identifiers to Lower Case?
A remnant from the Postgres parser is that unquoted identifiers will be automatically converted into lower case, e.g. the following create statements are equivalent

```sql
CREATE TABLE MyTable(MyColumn INTEGER);
CREATE TABLE "mytable"("mycolumn" INTEGER);
```

We have added case insensitivity to many areas in the binder (and are adding more #2071), which makes this behavior a lot less annoying to deal with. However, when interfacing with other APIs this case folding might lead to surprising behavior.

For example, if we have a query such as this:

```sql
SELECT MyColumn + 1 AS MyAddition FROM MyTable
```

And then convert the result into e.g. a Pandas DataFrame, or write it to a Parquet file, the column name will be lowercased (to "myaddition"). This makes subsequent operations on these external structures rather confusing.

The work-around for this is to quote the values, which preserves the case. However, that is not obvious to beginners and probably not what we want to have in the first place.

Perhaps we should remove this automatic lowercasing of identifiers, as we already handle case-insensitivity at the binder level we don't need to handle it in the parser/transformer.

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of examples/standalone-plan/main.cpp]
1: #include "duckdb.hpp"
2: #ifndef DUCKDB_AMALGAMATION
3: #include "duckdb/planner/logical_operator.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/planner/operator/logical_aggregate.hpp"
6: #include "duckdb/planner/operator/logical_get.hpp"
7: #include "duckdb/function/table/table_scan.hpp"
8: #include "duckdb/planner/expression.hpp"
9: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
10: #include "duckdb/planner/expression/bound_cast_expression.hpp"
11: #include "duckdb/planner/expression/bound_conjunction_expression.hpp"
12: #include "duckdb/planner/expression/bound_reference_expression.hpp"
13: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
14: #include "duckdb/planner/expression/bound_constant_expression.hpp"
15: #include "duckdb/planner/expression/bound_function_expression.hpp"
16: #include "duckdb/function/function_set.hpp"
17: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
18: #include "duckdb/parser/parsed_data/create_aggregate_function_info.hpp"
19: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
20: #include "duckdb/parser/tableref/table_function_ref.hpp"
21: #include "duckdb/parser/expression/constant_expression.hpp"
22: #include "duckdb/parser/expression/function_expression.hpp"
23: #include "duckdb/storage/statistics/numeric_statistics.hpp"
24: #endif
25: 
26: using namespace duckdb;
27: 
28: // in this example we build a simple volcano model executor on top of the DuckDB logical plans
29: // for simplicity, the executor only handles integer values and doesn't handle null values
30: 
31: void ExecuteQuery(Connection &con, const string &query);
32: void CreateFunction(Connection &con, string name, vector<LogicalType> arguments, LogicalType return_type);
33: void CreateAggregateFunction(Connection &con, string name, vector<LogicalType> arguments, LogicalType return_type);
34: 
35: //===--------------------------------------------------------------------===//
36: // Example Using DuckDB Catalog/Tables
37: //===--------------------------------------------------------------------===//
38: void RunExampleDuckDBCatalog() {
39: 	// in this example we use the DuckDB CREATE TABLE syntax to create tables to link against
40: 	// this works and the tables are easy to define, but since the tables are empty there are no statistics available
41: 	// we can use our own table functions (see RunExampleTableScan), but this is slightly more involved
42: 
43: 	DBConfig config;
44: 	config.initialize_default_database = false;
45: 
46: 	// disable the statistics propagator optimizer
47: 	// this is required since the statistics propagator will truncate our plan
48: 	// (e.g. it will recognize the table is empty that satisfy the predicate i=3
49: 	//       and then prune the entire plan)
50: 	config.disabled_optimizers.insert(OptimizerType::STATISTICS_PROPAGATION);
51: 	// we don't support filter pushdown yet in our toy example
52: 	config.disabled_optimizers.insert(OptimizerType::FILTER_PUSHDOWN);
53: 
54: 	DuckDB db(nullptr, &config);
55: 	Connection con(db);
56: 
57: 	// we perform an explicit BEGIN TRANSACTION here
58: 	// since "CreateFunction" will directly poke around in the catalog
59: 	// which requires an active transaction
60: 	con.Query("BEGIN TRANSACTION");
61: 
62: 	// register dummy tables (for our binding purposes)
63: 	con.Query("CREATE TABLE mytable(i INTEGER, j INTEGER)");
64: 	con.Query("CREATE TABLE myothertable(k INTEGER)");
65: 	// contents of the tables
66: 	// mytable:
67: 	// i: 1, 2, 3, 4, 5
68: 	// j: 2, 3, 4, 5, 6
69: 	// myothertable
70: 	// k: 1, 10, 20
71: 	// (see MyScanNode)
72: 
73: 	// register functions and aggregates (for our binding purposes)
74: 	CreateFunction(con, "+", {LogicalType::INTEGER, LogicalType::INTEGER}, LogicalType::INTEGER);
75: 	CreateAggregateFunction(con, "count_star", {}, LogicalType::BIGINT);
76: 	CreateAggregateFunction(con, "sum", {LogicalType::INTEGER}, LogicalType::INTEGER);
77: 
78: 	con.Query("COMMIT");
79: 
80: 	// standard projections
81: 	ExecuteQuery(con, "SELECT * FROM mytable");
82: 	ExecuteQuery(con, "SELECT i FROM mytable");
83: 	ExecuteQuery(con, "SELECT j FROM mytable");
84: 	ExecuteQuery(con, "SELECT k FROM myothertable");
85: 	// some simple filter + projection
86: 	ExecuteQuery(con, "SELECT i+1 FROM mytable WHERE i=3 OR i=4");
87: 	// more complex filters
88: 	ExecuteQuery(con, "SELECT i+1 FROM mytable WHERE (i<=2 AND j<=3) OR (i=4 AND j=5)");
89: 	// aggregate
90: 	ExecuteQuery(con, "SELECT COUNT(*), SUM(i) + 1, SUM(j) + 2 FROM mytable WHERE i>2");
91: 	// with a subquery
92: 	ExecuteQuery(con,
93: 	             "SELECT a, b + 1, c + 2 FROM (SELECT COUNT(*), SUM(i), SUM(j) FROM mytable WHERE i > 2) tbl(a, b, c)");
94: }
95: 
96: //===--------------------------------------------------------------------===//
97: // Example Using Custom Scan Function
98: //===--------------------------------------------------------------------===//
99: void CreateMyScanFunction(Connection &con);
100: 
101: unique_ptr<TableFunctionRef> MyReplacementScan(const string &table_name, void *data) {
102: 	auto table_function = make_unique<TableFunctionRef>();
103: 	vector<unique_ptr<ParsedExpression>> children;
104: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
105: 	table_function->function = make_unique<FunctionExpression>("my_scan", move(children));
106: 	return table_function;
107: }
108: 
109: void RunExampleTableScan() {
110: 	// in this example we use our own TableFunction instead of the built-in "seq_scan"
111: 	// this allows us to emit our own statistics without needing to insert them into the DuckDB tables
112: 	// it also allows us to define ourselves what we do/do not support
113: 	// (e.g. we can disable projection or filter pushdown in the table function if desired)
114: 	// this means we don't need to disable optimizers anymore
115: 
116: 	DBConfig config;
117: 	config.initialize_default_database = false;
118: 	config.replacement_scans.push_back(ReplacementScan(MyReplacementScan));
119: 
120: 	DuckDB db(nullptr, &config);
121: 	Connection con(db);
122: 
123: 	// we perform an explicit BEGIN TRANSACTION here
124: 	// since "CreateFunction" will directly poke around in the catalog
125: 	// which requires an active transaction
126: 	con.Query("BEGIN TRANSACTION");
127: 
128: 	// register functions and aggregates (for our binding purposes)
129: 	CreateFunction(con, "+", {LogicalType::INTEGER, LogicalType::INTEGER}, LogicalType::INTEGER);
130: 	CreateAggregateFunction(con, "count_star", {}, LogicalType::BIGINT);
131: 	CreateAggregateFunction(con, "sum", {LogicalType::INTEGER}, LogicalType::INTEGER);
132: 
133: 	CreateMyScanFunction(con);
134: 
135: 	con.Query("COMMIT");
136: 
137: 	// standard projections
138: 	ExecuteQuery(con, "SELECT * FROM mytable");
139: 	ExecuteQuery(con, "SELECT i FROM mytable");
140: 	ExecuteQuery(con, "SELECT j FROM mytable");
141: 	ExecuteQuery(con, "SELECT k FROM myothertable");
142: 	// some simple filter + projection
143: 	ExecuteQuery(con, "SELECT i+1 FROM mytable WHERE i=3 OR i=4");
144: 	// more complex filters
145: 	ExecuteQuery(con, "SELECT i+1 FROM mytable WHERE (i<=2 AND j<=3) OR (i=4 AND j=5)");
146: 	// aggregate
147: 	ExecuteQuery(con, "SELECT COUNT(*), SUM(i) + 1, SUM(j) + 2 FROM mytable WHERE i>2");
148: 	// with a subquery
149: 	ExecuteQuery(con,
150: 	             "SELECT a, b + 1, c + 2 FROM (SELECT COUNT(*), SUM(i), SUM(j) FROM mytable WHERE i > 2) tbl(a, b, c)");
151: }
152: 
153: int main() {
154: 	RunExampleDuckDBCatalog();
155: 	RunExampleTableScan();
156: }
157: 
158: //===--------------------------------------------------------------------===//
159: // Create Dummy Scalar/Aggregate Functions in the Catalog
160: //===--------------------------------------------------------------------===//
161: void CreateFunction(Connection &con, string name, vector<LogicalType> arguments, LogicalType return_type) {
162: 	auto &context = *con.context;
163: 	auto &catalog = Catalog::GetCatalog(context);
164: 
165: 	// we can register multiple functions here if we want overloads
166: 	// you may also want to set has_side_effects or varargs in the ScalarFunction (if required)
167: 	ScalarFunctionSet set(name);
168: 	set.AddFunction(ScalarFunction(move(arguments), move(return_type), nullptr));
169: 
170: 	CreateScalarFunctionInfo info(move(set));
171: 	catalog.CreateFunction(context, &info);
172: }
173: 
174: void CreateAggregateFunction(Connection &con, string name, vector<LogicalType> arguments, LogicalType return_type) {
175: 	auto &context = *con.context;
176: 	auto &catalog = Catalog::GetCatalog(context);
177: 
178: 	// we can register multiple functions here if we want overloads
179: 	AggregateFunctionSet set(name);
180: 	set.AddFunction(AggregateFunction(move(arguments), move(return_type), nullptr, nullptr, nullptr, nullptr, nullptr));
181: 
182: 	CreateAggregateFunctionInfo info(move(set));
183: 	catalog.CreateFunction(context, &info);
184: }
185: 
186: //===--------------------------------------------------------------------===//
187: // Custom Table Scan Function
188: //===--------------------------------------------------------------------===//
189: struct MyBindData : public FunctionData {
190: 	MyBindData(string name_p) : table_name(move(name_p)) {
191: 	}
192: 
193: 	string table_name;
194: };
195: 
196: // contents of the tables
197: // mytable:
198: // i: 1, 2, 3, 4, 5
199: // j: 2, 3, 4, 5, 6
200: // myothertable
201: // k: 1, 10, 20
202: // (see MyScanNode)
203: static unique_ptr<FunctionData> MyScanBind(ClientContext &context, vector<Value> &inputs,
204:                                            unordered_map<string, Value> &named_parameters,
205:                                            vector<LogicalType> &input_table_types, vector<string> &input_table_names,
206:                                            vector<LogicalType> &return_types, vector<string> &names) {
207: 	auto table_name = inputs[0].ToString();
208: 	if (table_name == "mytable") {
209: 		names.emplace_back("i");
210: 		return_types.emplace_back(LogicalType::INTEGER);
211: 
212: 		names.emplace_back("j");
213: 		return_types.emplace_back(LogicalType::INTEGER);
214: 	} else if (table_name == "myothertable") {
215: 		names.emplace_back("k");
216: 		return_types.emplace_back(LogicalType::INTEGER);
217: 	} else {
218: 		throw std::runtime_error("Unknown table " + table_name);
219: 	}
220: 	auto result = make_unique<MyBindData>(table_name);
221: 	return move(result);
222: }
223: 
224: static unique_ptr<BaseStatistics> MyScanStatistics(ClientContext &context, const FunctionData *bind_data_p,
225:                                                    column_t column_id) {
226: 	auto &bind_data = (MyBindData &)*bind_data_p;
227: 	if (bind_data.table_name == "mytable") {
228: 		if (column_id == 0) {
229: 			// i: 1, 2, 3, 4, 5
230: 			return make_unique<NumericStatistics>(LogicalType::INTEGER, Value::INTEGER(1), Value::INTEGER(5));
231: 		} else if (column_id == 1) {
232: 			// j: 2, 3, 4, 5, 6
233: 			return make_unique<NumericStatistics>(LogicalType::INTEGER, Value::INTEGER(2), Value::INTEGER(6));
234: 		}
235: 	} else if (bind_data.table_name == "myothertable") {
236: 		// k: 1, 10, 20
237: 		return make_unique<NumericStatistics>(LogicalType::INTEGER, Value::INTEGER(1), Value::INTEGER(20));
238: 	}
239: 	return nullptr;
240: }
241: 
242: unique_ptr<NodeStatistics> MyScanCardinality(ClientContext &context, const FunctionData *bind_data_p) {
243: 	auto &bind_data = (MyBindData &)*bind_data_p;
244: 	if (bind_data.table_name == "mytable") {
245: 		// 5 tuples
246: 		return make_unique<NodeStatistics>(5, 5);
247: 	} else if (bind_data.table_name == "myothertable") {
248: 		return make_unique<NodeStatistics>(3, 3);
249: 	}
250: 	return nullptr;
251: }
252: 
253: void CreateMyScanFunction(Connection &con) {
254: 	auto &context = *con.context;
255: 	auto &catalog = Catalog::GetCatalog(context);
256: 
257: 	TableFunction my_scan("my_scan", {LogicalType::VARCHAR}, nullptr, MyScanBind, nullptr, MyScanStatistics, nullptr,
258: 	                      nullptr, MyScanCardinality);
259: 	my_scan.projection_pushdown = true;
260: 	my_scan.filter_pushdown = false;
261: 
262: 	CreateTableFunctionInfo info(move(my_scan));
263: 	catalog.CreateTableFunction(context, &info);
264: }
265: 
266: //===--------------------------------------------------------------------===//
267: // Example Execution Engine: Row-based volcano style that only supports int32
268: //===--------------------------------------------------------------------===//
269: class MyNode {
270: public:
271: 	virtual ~MyNode() {
272: 	}
273: 	virtual vector<int> GetNextRow() = 0;
274: 
275: 	unique_ptr<MyNode> child;
276: };
277: 
278: class MyPlanGenerator {
279: public:
280: 	unique_ptr<MyNode> TransformPlan(LogicalOperator &op);
281: };
282: 
283: void ExecuteQuery(Connection &con, const string &query) {
284: 	// create the logical plan
285: 	auto plan = con.ExtractPlan(query);
286: 	plan->Print();
287: 
288: 	// transform the logical plan into our own plan
289: 	MyPlanGenerator generator;
290: 	auto my_plan = generator.TransformPlan(*plan);
291: 
292: 	// execute the plan and print the result
293: 	printf("Executing query: %s\n", query.c_str());
294: 	printf("----------------------\n");
295: 	vector<int> result;
296: 	while (true) {
297: 		result = my_plan->GetNextRow();
298: 		if (result.empty()) {
299: 			break;
300: 		}
301: 		string str;
302: 		for (size_t i = 0; i < result.size(); i++) {
303: 			if (i > 0) {
304: 				str += ", ";
305: 			}
306: 			str += std::to_string(result[i]);
307: 		}
308: 		printf("%s\n", str.c_str());
309: 	}
310: 	printf("----------------------\n");
311: }
312: 
313: //===--------------------------------------------------------------------===//
314: // Table Scan Node
315: //===--------------------------------------------------------------------===//
316: class MyScanNode : public MyNode {
317: public:
318: 	MyScanNode(string name_p, vector<column_t> column_ids_p) : name(move(name_p)), column_ids(move(column_ids_p)) {
319: 		// fill up the data based on which table we are scanning
320: 		if (name == "mytable") {
321: 			// i
322: 			data.push_back({1, 2, 3, 4, 5});
323: 			// j
324: 			data.push_back({2, 3, 4, 5, 6});
325: 		} else if (name == "myothertable") {
326: 			// k
327: 			data.push_back({1, 10, 20});
328: 		} else {
329: 			throw std::runtime_error("Unsupported table!");
330: 		}
331: 	}
332: 
333: 	string name;
334: 	vector<column_t> column_ids;
335: 	vector<vector<int>> data;
336: 	int index = 0;
337: 
338: 	vector<int> GetNextRow() override {
339: 		vector<int> result;
340: 		if (index >= data[0].size()) {
341: 			return result;
342: 		}
343: 		// fill the result based on the projection list (column_ids)
344: 		for (size_t i = 0; i < column_ids.size(); i++) {
345: 			result.push_back(data[column_ids[i]][index]);
346: 		}
347: 		index++;
348: 		return result;
349: 	};
350: };
351: 
352: //===--------------------------------------------------------------------===//
353: // Expression Execution
354: //===--------------------------------------------------------------------===//
355: 
356: // note that we run expression execution directly on top of DuckDB expressions
357: // it is also possible to transform the expressions into our own expressions (MyExpression)
358: class MyExpressionExecutor {
359: public:
360: 	MyExpressionExecutor(vector<int> current_row_p) : current_row(move(current_row_p)) {
361: 	}
362: 
363: 	vector<int> current_row;
364: 
365: 	int Execute(Expression &expression);
366: 
367: protected:
368: 	int Execute(BoundReferenceExpression &expr);
369: 	int Execute(BoundCastExpression &expr);
370: 	int Execute(BoundComparisonExpression &expr);
371: 	int Execute(BoundConjunctionExpression &expr);
372: 	int Execute(BoundConstantExpression &expr);
373: 	int Execute(BoundFunctionExpression &expr);
374: };
375: 
376: //===--------------------------------------------------------------------===//
377: // Filter
378: //===--------------------------------------------------------------------===//
379: class MyFilterNode : public MyNode {
380: public:
381: 	MyFilterNode(unique_ptr<Expression> filter_node) : filter(move(filter_node)) {
382: 	}
383: 
384: 	unique_ptr<Expression> filter;
385: 
386: 	bool ExecuteFilter(Expression &expr, const vector<int> &current_row) {
387: 		MyExpressionExecutor executor(current_row);
388: 		auto val = executor.Execute(expr);
389: 		return val != 0;
390: 	}
391: 
392: 	vector<int> GetNextRow() override {
393: 		D_ASSERT(child);
394: 		while (true) {
395: 			auto next = child->GetNextRow();
396: 			if (next.empty()) {
397: 				return next;
398: 			}
399: 			// check if the filter passes, if it does we return the row
400: 			// if not we return the next row
401: 			if (ExecuteFilter(*filter, next)) {
402: 				return next;
403: 			}
404: 		}
405: 	};
406: };
407: 
408: //===--------------------------------------------------------------------===//
409: // Projection
410: //===--------------------------------------------------------------------===//
411: class MyProjectionNode : public MyNode {
412: public:
413: 	MyProjectionNode(vector<unique_ptr<Expression>> projections_p) : projections(move(projections_p)) {
414: 	}
415: 
416: 	vector<unique_ptr<Expression>> projections;
417: 
418: 	vector<int> GetNextRow() override {
419: 		auto next = child->GetNextRow();
420: 		if (next.empty()) {
421: 			return next;
422: 		}
423: 		MyExpressionExecutor executor(next);
424: 		vector<int> result;
425: 		for (size_t i = 0; i < projections.size(); i++) {
426: 			result.push_back(executor.Execute(*projections[i]));
427: 		}
428: 		return result;
429: 	};
430: };
431: 
432: //===--------------------------------------------------------------------===//
433: // Aggregate
434: //===--------------------------------------------------------------------===//
435: class MyAggregateNode : public MyNode {
436: public:
437: 	MyAggregateNode(vector<unique_ptr<Expression>> aggregates_p) : aggregates(move(aggregates_p)) {
438: 		// initialize aggregate states to 0
439: 		aggregate_states.resize(aggregates.size(), 0);
440: 	}
441: 
442: 	vector<unique_ptr<Expression>> aggregates;
443: 	vector<int> aggregate_states;
444: 
445: 	void ExecuteAggregate(MyExpressionExecutor &executor, int index, BoundAggregateExpression &expr) {
446: 		if (expr.function.name == "sum") {
447: 			int child = executor.Execute(*expr.children[0]);
448: 			aggregate_states[index] += child;
449: 		} else if (expr.function.name == "count_star") {
450: 			aggregate_states[index]++;
451: 		} else {
452: 			throw std::runtime_error("Unsupported aggregate function " + expr.function.name);
453: 		}
454: 	}
455: 
456: 	vector<int> GetNextRow() override {
457: 		if (aggregate_states.empty()) {
458: 			// finished aggregating
459: 			return aggregate_states;
460: 		}
461: 		while (true) {
462: 			auto next = child->GetNextRow();
463: 			if (next.empty()) {
464: 				return move(aggregate_states);
465: 			}
466: 			MyExpressionExecutor executor(next);
467: 			for (size_t i = 0; i < aggregates.size(); i++) {
468: 				ExecuteAggregate(executor, i, (BoundAggregateExpression &)*aggregates[i]);
469: 			}
470: 		}
471: 	};
472: };
473: 
474: //===--------------------------------------------------------------------===//
475: // Plan Transformer - Transform a DuckDB logical plan into a custom plan (MyNode)
476: //===--------------------------------------------------------------------===//
477: unique_ptr<MyNode> MyPlanGenerator::TransformPlan(LogicalOperator &op) {
478: 	switch (op.type) {
479: 	case LogicalOperatorType::LOGICAL_PROJECTION: {
480: 		// projection
481: 		auto child = TransformPlan(*op.children[0]);
482: 		auto node = make_unique<MyProjectionNode>(move(op.expressions));
483: 		node->child = move(child);
484: 		return move(node);
485: 	}
486: 	case LogicalOperatorType::LOGICAL_FILTER: {
487: 		// filter
488: 		auto child = TransformPlan(*op.children[0]);
489: 		auto node = make_unique<MyFilterNode>(move(op.expressions[0]));
490: 		node->child = move(child);
491: 		return move(node);
492: 	}
493: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY: {
494: 		auto &aggr = (LogicalAggregate &)op;
495: 		if (!aggr.groups.empty()) {
496: 			throw std::runtime_error("Grouped aggregate not supported");
497: 		}
498: 		auto child = TransformPlan(*op.children[0]);
499: 		auto node = make_unique<MyAggregateNode>(move(op.expressions));
500: 		node->child = move(child);
501: 		return move(node);
502: 	}
503: 	case LogicalOperatorType::LOGICAL_GET: {
504: 		auto &get = (LogicalGet &)op;
505: 		// table scan or table function
506: 
507: 		// get nodes have two properties: table_filters (filter pushdown) and column_ids (projection pushdown)
508: 		// table_filters are only generated if optimizers are enabled (through the filter pushdown optimizer)
509: 		// column_ids are always generated
510: 		// the column_ids specify which columns should be emitted and in which order
511: 		// e.g. if we have a table "i, j, k" and the column_ids are {0, 2} we should emit ONLY "i, k" and in that order
512: 		if (get.function.name == "seq_scan") {
513: 			// built-in table scan
514: 			auto &table = (TableScanBindData &)*get.bind_data;
515: 			if (!get.table_filters.filters.empty()) {
516: 				// note: filter pushdown will only be triggered if optimizers are enabled
517: 				throw std::runtime_error("Filter pushdown unsupported");
518: 			}
519: 			return make_unique<MyScanNode>(table.table->name, get.column_ids);
520: 		} else if (get.function.name == "my_scan") {
521: 			// our own scan
522: 			auto &my_bind_data = (MyBindData &)*get.bind_data;
523: 			return make_unique<MyScanNode>(my_bind_data.table_name, get.column_ids);
524: 		} else {
525: 			throw std::runtime_error("Unsupported table function");
526: 		}
527: 	}
528: 	default:
529: 		throw std::runtime_error("Unsupported logical operator for transformation");
530: 	}
531: }
532: 
533: //===--------------------------------------------------------------------===//
534: // Expression Execution for various built-in expressions
535: //===--------------------------------------------------------------------===//
536: int MyExpressionExecutor::Execute(BoundReferenceExpression &expr) {
537: 	// column references (e.g. "SELECT a FROM tbl") are turned into BoundReferences
538: 	// these refer to an index within the row they come from
539: 	// because of that it is important to correctly handle the get.column_ids
540: 	return current_row[expr.index];
541: }
542: 
543: int MyExpressionExecutor::Execute(BoundCastExpression &expr) {
544: 	return Execute(*expr.child);
545: }
546: 
547: int MyExpressionExecutor::Execute(BoundConjunctionExpression &expr) {
548: 	int result;
549: 	if (expr.GetExpressionType() == ExpressionType::CONJUNCTION_AND) {
550: 		result = 1;
551: 		for (size_t i = 0; i < expr.children.size(); i++) {
552: 			result = result && Execute(*expr.children[i]);
553: 		}
554: 	} else if (expr.GetExpressionType() == ExpressionType::CONJUNCTION_OR) {
555: 		result = 0;
556: 		for (size_t i = 0; i < expr.children.size(); i++) {
557: 			result = result || Execute(*expr.children[i]);
558: 		}
559: 	} else {
560: 		throw std::runtime_error("Unrecognized conjunction (this shouldn't be possible)");
561: 	}
562: 	return result;
563: }
564: 
565: int MyExpressionExecutor::Execute(BoundConstantExpression &expr) {
566: 	return expr.value.GetValue<int32_t>();
567: }
568: 
569: int MyExpressionExecutor::Execute(BoundComparisonExpression &expr) {
570: 	auto lchild = Execute(*expr.left);
571: 	auto rchild = Execute(*expr.right);
572: 	bool cmp;
573: 	switch (expr.GetExpressionType()) {
574: 	case ExpressionType::COMPARE_EQUAL:
575: 		cmp = lchild == rchild;
576: 		break;
577: 	case ExpressionType::COMPARE_NOTEQUAL:
578: 		cmp = lchild != rchild;
579: 		break;
580: 	case ExpressionType::COMPARE_LESSTHAN:
581: 		cmp = lchild < rchild;
582: 		break;
583: 	case ExpressionType::COMPARE_GREATERTHAN:
584: 		cmp = lchild > rchild;
585: 		break;
586: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
587: 		cmp = lchild <= rchild;
588: 		break;
589: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
590: 		cmp = lchild >= rchild;
591: 		break;
592: 	default:
593: 		throw std::runtime_error("Unsupported comparison");
594: 	}
595: 	return cmp ? 1 : 0;
596: }
597: 
598: //===--------------------------------------------------------------------===//
599: // Expression Execution for built-in functions
600: //===--------------------------------------------------------------------===//
601: int MyExpressionExecutor::Execute(BoundFunctionExpression &expr) {
602: 	if (expr.function.name == "+") {
603: 		auto lchild = Execute(*expr.children[0]);
604: 		auto rchild = Execute(*expr.children[1]);
605: 		return lchild + rchild;
606: 	}
607: 	throw std::runtime_error("Unsupported function " + expr.function.name);
608: }
609: 
610: int MyExpressionExecutor::Execute(Expression &expression) {
611: 	switch (expression.GetExpressionClass()) {
612: 	case ExpressionClass::BOUND_REF:
613: 		return Execute((BoundReferenceExpression &)expression);
614: 	case ExpressionClass::BOUND_CAST:
615: 		return Execute((BoundCastExpression &)expression);
616: 	case ExpressionClass::BOUND_COMPARISON:
617: 		return Execute((BoundComparisonExpression &)expression);
618: 	case ExpressionClass::BOUND_CONJUNCTION:
619: 		return Execute((BoundConjunctionExpression &)expression);
620: 	case ExpressionClass::BOUND_CONSTANT:
621: 		return Execute((BoundConstantExpression &)expression);
622: 	case ExpressionClass::BOUND_FUNCTION:
623: 		return Execute((BoundFunctionExpression &)expression);
624: 	default:
625: 		throw std::runtime_error("Unsupported expression for expression executor " + expression.ToString());
626: 	}
627: }
[end of examples/standalone-plan/main.cpp]
[start of extension/icu/icu-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include "include/icu-extension.hpp"
4: #include "include/icu-collate.hpp"
5: #include "include/icu-dateadd.hpp"
6: #include "include/icu-datepart.hpp"
7: #include "include/icu-datesub.hpp"
8: #include "include/icu-datetrunc.hpp"
9: #include "include/icu-makedate.hpp"
10: 
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/main/connection.hpp"
13: #include "duckdb/main/config.hpp"
14: 
15: #include "duckdb/common/string_util.hpp"
16: #include "duckdb/planner/expression/bound_function_expression.hpp"
17: #include "duckdb/function/scalar_function.hpp"
18: #include "duckdb/common/vector_operations/unary_executor.hpp"
19: #include "duckdb/parser/parsed_data/create_collation_info.hpp"
20: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
21: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
22: #include "duckdb/execution/expression_executor.hpp"
23: #include "duckdb/catalog/catalog.hpp"
24: 
25: #include <cassert>
26: 
27: namespace duckdb {
28: 
29: struct IcuBindData : public FunctionData {
30: 	std::unique_ptr<icu::Collator> collator;
31: 	string language;
32: 	string country;
33: 
34: 	IcuBindData(string language_p, string country_p) : language(move(language_p)), country(move(country_p)) {
35: 		UErrorCode status = U_ZERO_ERROR;
36: 		auto locale = icu::Locale(language.c_str(), country.c_str());
37: 		if (locale.isBogus()) {
38: 			throw InternalException("Locale is bogus!?");
39: 		}
40: 		this->collator = std::unique_ptr<icu::Collator>(icu::Collator::createInstance(locale, status));
41: 		if (U_FAILURE(status)) {
42: 			auto error_name = u_errorName(status);
43: 			throw InternalException("Failed to create ICU collator: %s (language: %s, country: %s)", error_name,
44: 			                        language, country);
45: 		}
46: 	}
47: 
48: 	unique_ptr<FunctionData> Copy() override {
49: 		return make_unique<IcuBindData>(language, country);
50: 	}
51: };
52: 
53: static int32_t ICUGetSortKey(icu::Collator &collator, string_t input, unique_ptr<char[]> &buffer,
54:                              int32_t &buffer_size) {
55: 	int32_t string_size =
56: 	    collator.getSortKey(icu::UnicodeString::fromUTF8(icu::StringPiece(input.GetDataUnsafe(), input.GetSize())),
57: 	                        (uint8_t *)buffer.get(), buffer_size);
58: 	if (string_size > buffer_size) {
59: 		// have to resize the buffer
60: 		buffer_size = string_size;
61: 		buffer = unique_ptr<char[]>(new char[buffer_size]);
62: 
63: 		string_size =
64: 		    collator.getSortKey(icu::UnicodeString::fromUTF8(icu::StringPiece(input.GetDataUnsafe(), input.GetSize())),
65: 		                        (uint8_t *)buffer.get(), buffer_size);
66: 	}
67: 	return string_size;
68: }
69: 
70: static void ICUCollateFunction(DataChunk &args, ExpressionState &state, Vector &result) {
71: 	const char HEX_TABLE[] = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F'};
72: 
73: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
74: 	auto &info = (IcuBindData &)*func_expr.bind_info;
75: 	auto &collator = *info.collator;
76: 
77: 	unique_ptr<char[]> buffer;
78: 	int32_t buffer_size = 0;
79: 	UnaryExecutor::Execute<string_t, string_t>(args.data[0], result, args.size(), [&](string_t input) {
80: 		// create a sort key from the string
81: 		const auto string_size = idx_t(ICUGetSortKey(collator, input, buffer, buffer_size));
82: 		// convert the sort key to hexadecimal
83: 		auto str_result = StringVector::EmptyString(result, (string_size - 1) * 2);
84: 		auto str_data = str_result.GetDataWriteable();
85: 		for (idx_t i = 0; i < string_size - 1; i++) {
86: 			uint8_t byte = uint8_t(buffer[i]);
87: 			D_ASSERT(byte != 0);
88: 			str_data[i * 2] = HEX_TABLE[byte / 16];
89: 			str_data[i * 2 + 1] = HEX_TABLE[byte % 16];
90: 		}
91: 		// printf("%s: %s\n", input.GetString().c_str(), str_result.GetString().c_str());
92: 		return str_result;
93: 	});
94: }
95: 
96: static unique_ptr<FunctionData> ICUCollateBind(ClientContext &context, ScalarFunction &bound_function,
97:                                                vector<unique_ptr<Expression>> &arguments) {
98: 	auto splits = StringUtil::Split(bound_function.name, "_");
99: 	if (splits.size() == 1) {
100: 		return make_unique<IcuBindData>(splits[0], "");
101: 	} else if (splits.size() == 2) {
102: 		return make_unique<IcuBindData>(splits[0], splits[1]);
103: 	} else {
104: 		throw InternalException("Expected one or two splits");
105: 	}
106: }
107: 
108: static unique_ptr<FunctionData> ICUSortKeyBind(ClientContext &context, ScalarFunction &bound_function,
109:                                                vector<unique_ptr<Expression>> &arguments) {
110: 	if (!arguments[1]->IsFoldable()) {
111: 		throw NotImplementedException("ICU_SORT_KEY(VARCHAR, VARCHAR) with non-constant collation is not supported");
112: 	}
113: 	Value val = ExpressionExecutor::EvaluateScalar(*arguments[1]).CastAs(LogicalType::VARCHAR);
114: 	if (val.IsNull()) {
115: 		throw NotImplementedException("ICU_SORT_KEY(VARCHAR, VARCHAR) expected a non-null collation");
116: 	}
117: 	auto splits = StringUtil::Split(StringValue::Get(val), "_");
118: 	if (splits.size() == 1) {
119: 		return make_unique<IcuBindData>(splits[0], "");
120: 	} else if (splits.size() == 2) {
121: 		return make_unique<IcuBindData>(splits[0], splits[1]);
122: 	} else {
123: 		throw InternalException("Expected one or two splits");
124: 	}
125: }
126: 
127: static ScalarFunction GetICUFunction(const string &collation) {
128: 	return ScalarFunction(collation, {LogicalType::VARCHAR}, LogicalType::VARCHAR, ICUCollateFunction, false,
129: 	                      ICUCollateBind);
130: }
131: 
132: static void SetICUTimeZone(ClientContext &context, SetScope scope, Value &parameter) {
133: 	icu::StringPiece utf8(StringValue::Get(parameter));
134: 	const auto uid = icu::UnicodeString::fromUTF8(utf8);
135: 	std::unique_ptr<icu::TimeZone> tz(icu::TimeZone::createTimeZone(uid));
136: 	if (*tz == icu::TimeZone::getUnknown()) {
137: 		throw NotImplementedException("Unknown TimeZone setting");
138: 	}
139: }
140: 
141: struct ICUTimeZoneData : public FunctionOperatorData {
142: 	ICUTimeZoneData() : tzs(icu::TimeZone::createEnumeration()) {
143: 		UErrorCode status = U_ZERO_ERROR;
144: 		std::unique_ptr<icu::Calendar> calendar(icu::Calendar::createInstance(status));
145: 		now = calendar->getNow();
146: 	}
147: 
148: 	std::unique_ptr<icu::StringEnumeration> tzs;
149: 	UDate now;
150: };
151: 
152: static unique_ptr<FunctionData> ICUTimeZoneBind(ClientContext &context, vector<Value> &inputs,
153:                                                 unordered_map<string, Value> &named_parameters,
154:                                                 vector<LogicalType> &input_table_types,
155:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
156:                                                 vector<string> &names) {
157: 	names.emplace_back("name");
158: 	return_types.emplace_back(LogicalType::VARCHAR);
159: 	names.emplace_back("abbrev");
160: 	return_types.emplace_back(LogicalType::VARCHAR);
161: 	names.emplace_back("utc_offset");
162: 	return_types.emplace_back(LogicalType::INTERVAL);
163: 	names.emplace_back("is_dst");
164: 	return_types.emplace_back(LogicalType::BOOLEAN);
165: 
166: 	return nullptr;
167: }
168: 
169: static unique_ptr<FunctionOperatorData> ICUTimeZoneInit(ClientContext &context, const FunctionData *bind_data,
170:                                                         const vector<column_t> &column_ids,
171:                                                         TableFilterCollection *filters) {
172: 	return make_unique<ICUTimeZoneData>();
173: }
174: 
175: static void ICUTimeZoneCleanup(ClientContext &context, const FunctionData *bind_data,
176:                                FunctionOperatorData *operator_state) {
177: 	auto &data = (ICUTimeZoneData &)*operator_state;
178: 	(void)data.tzs.release();
179: }
180: 
181: static void ICUTimeZoneFunction(ClientContext &context, const FunctionData *bind_data,
182:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
183: 	auto &data = (ICUTimeZoneData &)*operator_state;
184: 	idx_t index = 0;
185: 	while (index < STANDARD_VECTOR_SIZE) {
186: 		UErrorCode status = U_ZERO_ERROR;
187: 		auto long_id = data.tzs->snext(status);
188: 		if (U_FAILURE(status) || !long_id) {
189: 			break;
190: 		}
191: 
192: 		//	The LONG name is the one we looked up
193: 		std::string utf8;
194: 		long_id->toUTF8String(utf8);
195: 		output.SetValue(0, index, Value(utf8));
196: 
197: 		//	We don't have the zone tree for determining abbreviated names,
198: 		//	so the SHORT name is the first equivalent TZ without a slash.
199: 		icu::UnicodeString short_id = *long_id;
200: 		const auto nIDs = icu::TimeZone::countEquivalentIDs(*long_id);
201: 		for (int32_t idx = 0; idx < nIDs; ++idx) {
202: 			const auto eid = icu::TimeZone::getEquivalentID(*long_id, idx);
203: 			if (eid.indexOf(char16_t('/')) < 0) {
204: 				short_id = eid;
205: 				break;
206: 			}
207: 		}
208: 
209: 		utf8.clear();
210: 		short_id.toUTF8String(utf8);
211: 		output.SetValue(1, index, Value(utf8));
212: 
213: 		std::unique_ptr<icu::TimeZone> tz(icu::TimeZone::createTimeZone(*long_id));
214: 		int32_t raw_offset_ms;
215: 		int32_t dst_offset_ms;
216: 		tz->getOffset(data.now, false, raw_offset_ms, dst_offset_ms, status);
217: 		if (U_FAILURE(status)) {
218: 			break;
219: 		}
220: 
221: 		output.SetValue(2, index, Value::INTERVAL(Interval::FromMicro(raw_offset_ms * Interval::MICROS_PER_MSEC)));
222: 		output.SetValue(3, index, Value(dst_offset_ms != 0));
223: 		++index;
224: 	}
225: 	output.SetCardinality(index);
226: }
227: 
228: struct ICUCalendarData : public FunctionOperatorData {
229: 	ICUCalendarData() {
230: 		// All calendars are available in all locales
231: 		UErrorCode status = U_ZERO_ERROR;
232: 		calendars.reset(icu::Calendar::getKeywordValuesForLocale("calendar", icu::Locale::getDefault(), false, status));
233: 	}
234: 
235: 	std::unique_ptr<icu::StringEnumeration> calendars;
236: };
237: 
238: static unique_ptr<FunctionData> ICUCalendarBind(ClientContext &context, vector<Value> &inputs,
239:                                                 unordered_map<string, Value> &named_parameters,
240:                                                 vector<LogicalType> &input_table_types,
241:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
242:                                                 vector<string> &names) {
243: 	names.emplace_back("name");
244: 	return_types.emplace_back(LogicalType::VARCHAR);
245: 
246: 	return nullptr;
247: }
248: 
249: static unique_ptr<FunctionOperatorData> ICUCalendarInit(ClientContext &context, const FunctionData *bind_data,
250:                                                         const vector<column_t> &column_ids,
251:                                                         TableFilterCollection *filters) {
252: 	return make_unique<ICUCalendarData>();
253: }
254: 
255: static void ICUCalendarFunction(ClientContext &context, const FunctionData *bind_data,
256:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
257: 	auto &data = (ICUCalendarData &)*operator_state;
258: 	idx_t index = 0;
259: 	while (index < STANDARD_VECTOR_SIZE) {
260: 		if (!data.calendars) {
261: 			break;
262: 		}
263: 
264: 		UErrorCode status = U_ZERO_ERROR;
265: 		auto calendar = data.calendars->snext(status);
266: 		if (U_FAILURE(status) || !calendar) {
267: 			break;
268: 		}
269: 
270: 		//	The calendar name is all we have
271: 		std::string utf8;
272: 		calendar->toUTF8String(utf8);
273: 		output.SetValue(0, index, Value(utf8));
274: 
275: 		++index;
276: 	}
277: 	output.SetCardinality(index);
278: }
279: 
280: static void ICUCalendarCleanup(ClientContext &context, const FunctionData *bind_data,
281:                                FunctionOperatorData *operator_state) {
282: 	auto &data = (ICUCalendarData &)*operator_state;
283: 	(void)data.calendars.release();
284: }
285: 
286: static void SetICUCalendar(ClientContext &context, SetScope scope, Value &parameter) {
287: 	const auto name = parameter.Value::GetValueUnsafe<string>();
288: 	string locale_key = "@calendar=" + name;
289: 	icu::Locale locale(locale_key.c_str());
290: 
291: 	UErrorCode status = U_ZERO_ERROR;
292: 	std::unique_ptr<icu::Calendar> cal(icu::Calendar::createInstance(locale, status));
293: 	if (U_FAILURE(status) || name != cal->getType()) {
294: 		throw NotImplementedException("Unknown Calendar setting");
295: 	}
296: }
297: 
298: void ICUExtension::Load(DuckDB &db) {
299: 	Connection con(db);
300: 	con.BeginTransaction();
301: 
302: 	auto &catalog = Catalog::GetCatalog(*con.context);
303: 
304: 	// iterate over all the collations
305: 	int32_t count;
306: 	auto locales = icu::Collator::getAvailableLocales(count);
307: 	for (int32_t i = 0; i < count; i++) {
308: 		string collation;
309: 		if (string(locales[i].getCountry()).empty()) {
310: 			// language only
311: 			collation = locales[i].getLanguage();
312: 		} else {
313: 			// language + country
314: 			collation = locales[i].getLanguage() + string("_") + locales[i].getCountry();
315: 		}
316: 		collation = StringUtil::Lower(collation);
317: 
318: 		CreateCollationInfo info(collation, GetICUFunction(collation), false, true);
319: 		info.on_conflict = OnCreateConflict::IGNORE_ON_CONFLICT;
320: 		catalog.CreateCollation(*con.context, &info);
321: 	}
322: 	ScalarFunction sort_key("icu_sort_key", {LogicalType::VARCHAR, LogicalType::VARCHAR}, LogicalType::VARCHAR,
323: 	                        ICUCollateFunction, false, ICUSortKeyBind);
324: 
325: 	CreateScalarFunctionInfo sort_key_info(move(sort_key));
326: 	catalog.CreateFunction(*con.context, &sort_key_info);
327: 
328: 	// Time Zones
329: 	auto &config = DBConfig::GetConfig(*db.instance);
330: 	config.AddExtensionOption("TimeZone", "The current time zone", LogicalType::VARCHAR, SetICUTimeZone);
331: 	std::unique_ptr<icu::TimeZone> tz(icu::TimeZone::createDefault());
332: 	icu::UnicodeString tz_id;
333: 	std::string tz_string;
334: 	tz->getID(tz_id).toUTF8String(tz_string);
335: 	config.set_variables["TimeZone"] = Value(tz_string);
336: 
337: 	TableFunction tz_names("pg_timezone_names", {}, ICUTimeZoneFunction, ICUTimeZoneBind, ICUTimeZoneInit, nullptr,
338: 	                       ICUTimeZoneCleanup);
339: 	CreateTableFunctionInfo tz_names_info(move(tz_names));
340: 	catalog.CreateTableFunction(*con.context, &tz_names_info);
341: 
342: 	RegisterICUDateAddFunctions(*con.context);
343: 	RegisterICUDatePartFunctions(*con.context);
344: 	RegisterICUDateSubFunctions(*con.context);
345: 	RegisterICUDateTruncFunctions(*con.context);
346: 	RegisterICUMakeDateFunctions(*con.context);
347: 
348: 	// Calendars
349: 	config.AddExtensionOption("Calendar", "The current calendar", LogicalType::VARCHAR, SetICUCalendar);
350: 	UErrorCode status = U_ZERO_ERROR;
351: 	std::unique_ptr<icu::Calendar> cal(icu::Calendar::createInstance(status));
352: 	config.set_variables["Calendar"] = Value(cal->getType());
353: 
354: 	TableFunction cal_names("icu_calendar_names", {}, ICUCalendarFunction, ICUCalendarBind, ICUCalendarInit, nullptr,
355: 	                        ICUCalendarCleanup);
356: 	CreateTableFunctionInfo cal_names_info(move(cal_names));
357: 	catalog.CreateTableFunction(*con.context, &cal_names_info);
358: 
359: 	con.Commit();
360: }
361: 
362: std::string ICUExtension::Name() {
363: 	return "icu";
364: }
365: 
366: } // namespace duckdb
367: 
368: extern "C" {
369: 
370: DUCKDB_EXTENSION_API void icu_init(duckdb::DatabaseInstance &db) { // NOLINT
371: 	duckdb::DuckDB db_wrapper(db);
372: 	db_wrapper.LoadExtension<duckdb::ICUExtension>();
373: }
374: 
375: DUCKDB_EXTENSION_API const char *icu_version() { // NOLINT
376: 	return duckdb::DuckDB::LibraryVersion();
377: }
378: }
[end of extension/icu/icu-extension.cpp]
[start of extension/parquet/parquet-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include <string>
4: #include <vector>
5: #include <fstream>
6: #include <iostream>
7: 
8: #include "parquet-extension.hpp"
9: #include "parquet_reader.hpp"
10: #include "parquet_writer.hpp"
11: #include "parquet_metadata.hpp"
12: #include "zstd_file_system.hpp"
13: 
14: #include "duckdb.hpp"
15: #ifndef DUCKDB_AMALGAMATION
16: #include "duckdb/common/file_system.hpp"
17: #include "duckdb/common/types/chunk_collection.hpp"
18: #include "duckdb/function/copy_function.hpp"
19: #include "duckdb/function/table_function.hpp"
20: #include "duckdb/common/file_system.hpp"
21: #include "duckdb/parallel/parallel_state.hpp"
22: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
23: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
24: 
25: #include "duckdb/common/enums/file_compression_type.hpp"
26: #include "duckdb/main/config.hpp"
27: #include "duckdb/parser/expression/constant_expression.hpp"
28: #include "duckdb/parser/expression/function_expression.hpp"
29: #include "duckdb/parser/tableref/table_function_ref.hpp"
30: 
31: #include "duckdb/storage/statistics/base_statistics.hpp"
32: 
33: #include "duckdb/main/client_context.hpp"
34: #include "duckdb/catalog/catalog.hpp"
35: #endif
36: 
37: namespace duckdb {
38: 
39: struct ParquetReadBindData : public FunctionData {
40: 	shared_ptr<ParquetReader> initial_reader;
41: 	vector<string> files;
42: 	vector<column_t> column_ids;
43: 	atomic<idx_t> chunk_count;
44: 	atomic<idx_t> cur_file;
45: };
46: 
47: struct ParquetReadOperatorData : public FunctionOperatorData {
48: 	shared_ptr<ParquetReader> reader;
49: 	ParquetReaderScanState scan_state;
50: 	bool is_parallel;
51: 	idx_t file_index;
52: 	vector<column_t> column_ids;
53: 	TableFilterSet *table_filters;
54: };
55: 
56: struct ParquetReadParallelState : public ParallelState {
57: 	mutex lock;
58: 	shared_ptr<ParquetReader> current_reader;
59: 	idx_t file_index;
60: 	idx_t row_group_index;
61: };
62: 
63: class ParquetScanFunction {
64: public:
65: 	static TableFunctionSet GetFunctionSet() {
66: 		TableFunctionSet set("parquet_scan");
67: 		auto table_function =
68: 		    TableFunction({LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind, ParquetScanInit,
69: 		                  /* statistics */ ParquetScanStats, /* cleanup */ nullptr,
70: 		                  /* dependency */ nullptr, ParquetCardinality,
71: 		                  /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, ParquetScanMaxThreads,
72: 		                  ParquetInitParallelState, ParquetScanFuncParallel, ParquetScanParallelInit,
73: 		                  ParquetParallelStateNext, true, true, ParquetProgress);
74: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
75: 		set.AddFunction(table_function);
76: 		table_function = TableFunction({LogicalType::LIST(LogicalType::VARCHAR)}, ParquetScanImplementation,
77: 		                               ParquetScanBindList, ParquetScanInit, /* statistics */ ParquetScanStats,
78: 		                               /* cleanup */ nullptr,
79: 		                               /* dependency */ nullptr, ParquetCardinality,
80: 		                               /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr,
81: 		                               ParquetScanMaxThreads, ParquetInitParallelState, ParquetScanFuncParallel,
82: 		                               ParquetScanParallelInit, ParquetParallelStateNext, true, true, ParquetProgress);
83: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
84: 		set.AddFunction(table_function);
85: 		return set;
86: 	}
87: 
88: 	static unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,
89: 	                                                vector<string> &expected_names,
90: 	                                                vector<LogicalType> &expected_types) {
91: 		for (auto &option : info.options) {
92: 			auto loption = StringUtil::Lower(option.first);
93: 			if (loption == "compression" || loption == "codec") {
94: 				// CODEC option has no effect on parquet read: we determine codec from the file
95: 				continue;
96: 			} else {
97: 				throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
98: 			}
99: 		}
100: 		auto result = make_unique<ParquetReadBindData>();
101: 
102: 		FileSystem &fs = FileSystem::GetFileSystem(context);
103: 		result->files = fs.Glob(info.file_path);
104: 		if (result->files.empty()) {
105: 			throw IOException("No files found that match the pattern \"%s\"", info.file_path);
106: 		}
107: 		ParquetOptions parquet_options(context);
108: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types, parquet_options);
109: 		return move(result);
110: 	}
111: 
112: 	static unique_ptr<BaseStatistics> ParquetScanStats(ClientContext &context, const FunctionData *bind_data_p,
113: 	                                                   column_t column_index) {
114: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
115: 
116: 		if (column_index == COLUMN_IDENTIFIER_ROW_ID) {
117: 			return nullptr;
118: 		}
119: 
120: 		// we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
121: 
122: 		// We already parsed the metadata for the first file in a glob because we need some type info.
123: 		auto overall_stats = ParquetReader::ReadStatistics(
124: 		    *bind_data.initial_reader, bind_data.initial_reader->return_types[column_index], column_index,
125: 		    bind_data.initial_reader->metadata->metadata.get());
126: 
127: 		if (!overall_stats) {
128: 			return nullptr;
129: 		}
130: 
131: 		// if there is only one file in the glob (quite common case), we are done
132: 		auto &config = DBConfig::GetConfig(context);
133: 		if (bind_data.files.size() < 2) {
134: 			return overall_stats;
135: 		} else if (config.object_cache_enable) {
136: 			auto &cache = ObjectCache::GetObjectCache(context);
137: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
138: 			// enabled at all)
139: 			FileSystem &fs = FileSystem::GetFileSystem(context);
140: 			for (idx_t file_idx = 1; file_idx < bind_data.files.size(); file_idx++) {
141: 				auto &file_name = bind_data.files[file_idx];
142: 				auto metadata = cache.Get<ParquetFileMetadataCache>(file_name);
143: 				if (!metadata) {
144: 					// missing metadata entry in cache, no usable stats
145: 					return nullptr;
146: 				}
147: 				auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
148: 				                          FileSystem::DEFAULT_COMPRESSION, FileSystem::GetFileOpener(context));
149: 				// we need to check if the metadata cache entries are current
150: 				if (fs.GetLastModifiedTime(*handle) >= metadata->read_time) {
151: 					// missing or invalid metadata entry in cache, no usable stats overall
152: 					return nullptr;
153: 				}
154: 				// get and merge stats for file
155: 				auto file_stats = ParquetReader::ReadStatistics(*bind_data.initial_reader,
156: 				                                                bind_data.initial_reader->return_types[column_index],
157: 				                                                column_index, metadata->metadata.get());
158: 				if (!file_stats) {
159: 					return nullptr;
160: 				}
161: 				overall_stats->Merge(*file_stats);
162: 			}
163: 			// success!
164: 			return overall_stats;
165: 		}
166: 		// we have more than one file and no object cache so no statistics overall
167: 		return nullptr;
168: 	}
169: 
170: 	static void ParquetScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
171: 	                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
172: 	                                    ParallelState *parallel_state_p) {
173: 		//! FIXME: Have specialized parallel function from pandas scan here
174: 		ParquetScanImplementation(context, bind_data, operator_state, input, output);
175: 	}
176: 
177: 	static unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context, vector<string> files,
178: 	                                                        vector<LogicalType> &return_types, vector<string> &names,
179: 	                                                        ParquetOptions parquet_options) {
180: 		auto result = make_unique<ParquetReadBindData>();
181: 		result->files = move(files);
182: 
183: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], parquet_options);
184: 		return_types = result->initial_reader->return_types;
185: 
186: 		names = result->initial_reader->names;
187: 		return move(result);
188: 	}
189: 
190: 	static vector<string> ParquetGlob(FileSystem &fs, const string &glob) {
191: 		auto files = fs.Glob(glob);
192: 		if (files.empty()) {
193: 			throw IOException("No files found that match the pattern \"%s\"", glob);
194: 		}
195: 		return files;
196: 	}
197: 
198: 	static unique_ptr<FunctionData> ParquetScanBind(ClientContext &context, vector<Value> &inputs,
199: 	                                                unordered_map<string, Value> &named_parameters,
200: 	                                                vector<LogicalType> &input_table_types,
201: 	                                                vector<string> &input_table_names,
202: 	                                                vector<LogicalType> &return_types, vector<string> &names) {
203: 		auto &config = DBConfig::GetConfig(context);
204: 		if (!config.enable_external_access) {
205: 			throw PermissionException("Scanning Parquet files is disabled through configuration");
206: 		}
207: 		auto file_name = inputs[0].GetValue<string>();
208: 		ParquetOptions parquet_options(context);
209: 		for (auto &kv : named_parameters) {
210: 			if (kv.first == "binary_as_string") {
211: 				parquet_options.binary_as_string = BooleanValue::Get(kv.second);
212: 			}
213: 		}
214: 		FileSystem &fs = FileSystem::GetFileSystem(context);
215: 		auto files = ParquetGlob(fs, file_name);
216: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
217: 	}
218: 
219: 	static unique_ptr<FunctionData> ParquetScanBindList(ClientContext &context, vector<Value> &inputs,
220: 	                                                    unordered_map<string, Value> &named_parameters,
221: 	                                                    vector<LogicalType> &input_table_types,
222: 	                                                    vector<string> &input_table_names,
223: 	                                                    vector<LogicalType> &return_types, vector<string> &names) {
224: 		auto &config = DBConfig::GetConfig(context);
225: 		if (!config.enable_external_access) {
226: 			throw PermissionException("Scanning Parquet files is disabled through configuration");
227: 		}
228: 		FileSystem &fs = FileSystem::GetFileSystem(context);
229: 		vector<string> files;
230: 		for (auto &val : ListValue::GetChildren(inputs[0])) {
231: 			auto glob_files = ParquetGlob(fs, val.ToString());
232: 			files.insert(files.end(), glob_files.begin(), glob_files.end());
233: 		}
234: 		if (files.empty()) {
235: 			throw IOException("Parquet reader needs at least one file to read");
236: 		}
237: 		ParquetOptions parquet_options(context);
238: 		for (auto &kv : named_parameters) {
239: 			if (kv.first == "binary_as_string") {
240: 				parquet_options.binary_as_string = BooleanValue::Get(kv.second);
241: 			}
242: 		}
243: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
244: 	}
245: 
246: 	static unique_ptr<FunctionOperatorData> ParquetScanInit(ClientContext &context, const FunctionData *bind_data_p,
247: 	                                                        const vector<column_t> &column_ids,
248: 	                                                        TableFilterCollection *filters) {
249: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
250: 		bind_data.chunk_count = 0;
251: 		bind_data.cur_file = 0;
252: 		auto result = make_unique<ParquetReadOperatorData>();
253: 		result->column_ids = column_ids;
254: 
255: 		result->is_parallel = false;
256: 		result->file_index = 0;
257: 		result->table_filters = filters->table_filters;
258: 		// single-threaded: one thread has to read all groups
259: 		vector<idx_t> group_ids;
260: 		for (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {
261: 			group_ids.push_back(i);
262: 		}
263: 		result->reader = bind_data.initial_reader;
264: 		result->reader->InitializeScan(result->scan_state, column_ids, move(group_ids), filters->table_filters);
265: 		return move(result);
266: 	}
267: 
268: 	static double ParquetProgress(ClientContext &context, const FunctionData *bind_data_p) {
269: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
270: 		if (bind_data.initial_reader->NumRows() == 0) {
271: 			return (100.0 * (bind_data.cur_file + 1)) / bind_data.files.size();
272: 		}
273: 		auto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100.0 / bind_data.initial_reader->NumRows()) /
274: 		                  bind_data.files.size();
275: 		percentage += 100.0 * bind_data.cur_file / bind_data.files.size();
276: 		return percentage;
277: 	}
278: 
279: 	static unique_ptr<FunctionOperatorData>
280: 	ParquetScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *parallel_state_p,
281: 	                        const vector<column_t> &column_ids, TableFilterCollection *filters) {
282: 		auto result = make_unique<ParquetReadOperatorData>();
283: 		result->column_ids = column_ids;
284: 		result->is_parallel = true;
285: 		result->table_filters = filters->table_filters;
286: 		if (!ParquetParallelStateNext(context, bind_data_p, result.get(), parallel_state_p)) {
287: 			return nullptr;
288: 		}
289: 		return move(result);
290: 	}
291: 
292: 	static void ParquetScanImplementation(ClientContext &context, const FunctionData *bind_data_p,
293: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
294: 		if (!operator_state) {
295: 			return;
296: 		}
297: 		auto &data = (ParquetReadOperatorData &)*operator_state;
298: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
299: 
300: 		do {
301: 			data.reader->Scan(data.scan_state, output);
302: 			bind_data.chunk_count++;
303: 			if (output.size() == 0 && !data.is_parallel) {
304: 				auto &bind_data = (ParquetReadBindData &)*bind_data_p;
305: 				// check if there is another file
306: 				if (data.file_index + 1 < bind_data.files.size()) {
307: 					data.file_index++;
308: 					bind_data.cur_file++;
309: 					bind_data.chunk_count = 0;
310: 					string file = bind_data.files[data.file_index];
311: 					// move to the next file
312: 					data.reader = make_shared<ParquetReader>(context, file, data.reader->return_types,
313: 					                                         data.reader->parquet_options, bind_data.files[0]);
314: 					vector<idx_t> group_ids;
315: 					for (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {
316: 						group_ids.push_back(i);
317: 					}
318: 					data.reader->InitializeScan(data.scan_state, data.column_ids, move(group_ids), data.table_filters);
319: 				} else {
320: 					// exhausted all the files: done
321: 					break;
322: 				}
323: 			} else {
324: 				break;
325: 			}
326: 		} while (true);
327: 	}
328: 
329: 	static unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {
330: 		auto &data = (ParquetReadBindData &)*bind_data;
331: 		return make_unique<NodeStatistics>(data.initial_reader->NumRows() * data.files.size());
332: 	}
333: 
334: 	static idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {
335: 		auto &data = (ParquetReadBindData &)*bind_data;
336: 		return data.initial_reader->NumRowGroups() * data.files.size();
337: 	}
338: 
339: 	static unique_ptr<ParallelState> ParquetInitParallelState(ClientContext &context, const FunctionData *bind_data_p,
340: 	                                                          const vector<column_t> &column_ids,
341: 	                                                          TableFilterCollection *filters) {
342: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
343: 		auto result = make_unique<ParquetReadParallelState>();
344: 		result->current_reader = bind_data.initial_reader;
345: 		result->row_group_index = 0;
346: 		result->file_index = 0;
347: 		return move(result);
348: 	}
349: 
350: 	static bool ParquetParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
351: 	                                     FunctionOperatorData *state_p, ParallelState *parallel_state_p) {
352: 		if (!state_p) {
353: 			return false;
354: 		}
355: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
356: 		auto &parallel_state = (ParquetReadParallelState &)*parallel_state_p;
357: 		auto &scan_data = (ParquetReadOperatorData &)*state_p;
358: 
359: 		lock_guard<mutex> parallel_lock(parallel_state.lock);
360: 		if (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {
361: 			// groups remain in the current parquet file: read the next group
362: 			scan_data.reader = parallel_state.current_reader;
363: 			vector<idx_t> group_indexes {parallel_state.row_group_index};
364: 			scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
365: 			                                 scan_data.table_filters);
366: 			parallel_state.row_group_index++;
367: 			return true;
368: 		} else {
369: 			// no groups remain in the current parquet file: check if there are more files to read
370: 			while (parallel_state.file_index + 1 < bind_data.files.size()) {
371: 				// read the next file
372: 				string file = bind_data.files[++parallel_state.file_index];
373: 				parallel_state.current_reader =
374: 				    make_shared<ParquetReader>(context, file, parallel_state.current_reader->return_types,
375: 				                               parallel_state.current_reader->parquet_options);
376: 				if (parallel_state.current_reader->NumRowGroups() == 0) {
377: 					// empty parquet file, move to next file
378: 					continue;
379: 				}
380: 				// set up the scan state to read the first group
381: 				scan_data.reader = parallel_state.current_reader;
382: 				vector<idx_t> group_indexes {0};
383: 				scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
384: 				                                 scan_data.table_filters);
385: 				parallel_state.row_group_index = 1;
386: 				return true;
387: 			}
388: 		}
389: 		return false;
390: 	}
391: };
392: 
393: struct ParquetWriteBindData : public FunctionData {
394: 	vector<LogicalType> sql_types;
395: 	string file_name;
396: 	vector<string> column_names;
397: 	duckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
398: 	idx_t row_group_size = 100000;
399: };
400: 
401: struct ParquetWriteGlobalState : public GlobalFunctionData {
402: 	unique_ptr<ParquetWriter> writer;
403: };
404: 
405: struct ParquetWriteLocalState : public LocalFunctionData {
406: 	ParquetWriteLocalState() {
407: 		buffer = make_unique<ChunkCollection>();
408: 	}
409: 
410: 	unique_ptr<ChunkCollection> buffer;
411: };
412: 
413: unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info, vector<string> &names,
414:                                           vector<LogicalType> &sql_types) {
415: 	auto bind_data = make_unique<ParquetWriteBindData>();
416: 	for (auto &option : info.options) {
417: 		auto loption = StringUtil::Lower(option.first);
418: 		if (loption == "row_group_size" || loption == "chunk_size") {
419: 			bind_data->row_group_size = option.second[0].GetValue<uint64_t>();
420: 		} else if (loption == "compression" || loption == "codec") {
421: 			if (!option.second.empty()) {
422: 				auto roption = StringUtil::Lower(option.second[0].ToString());
423: 				if (roption == "uncompressed") {
424: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::UNCOMPRESSED;
425: 					continue;
426: 				} else if (roption == "snappy") {
427: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
428: 					continue;
429: 				} else if (roption == "gzip") {
430: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::GZIP;
431: 					continue;
432: 				} else if (roption == "zstd") {
433: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::ZSTD;
434: 					continue;
435: 				}
436: 			}
437: 			throw ParserException("Expected %s argument to be either [uncompressed, snappy, gzip or zstd]", loption);
438: 		} else {
439: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
440: 		}
441: 	}
442: 	bind_data->sql_types = sql_types;
443: 	bind_data->column_names = names;
444: 	bind_data->file_name = info.file_path;
445: 	return move(bind_data);
446: }
447: 
448: unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &context, FunctionData &bind_data) {
449: 	auto global_state = make_unique<ParquetWriteGlobalState>();
450: 	auto &parquet_bind = (ParquetWriteBindData &)bind_data;
451: 
452: 	auto &fs = FileSystem::GetFileSystem(context);
453: 	global_state->writer =
454: 	    make_unique<ParquetWriter>(fs, parquet_bind.file_name, FileSystem::GetFileOpener(context),
455: 	                               parquet_bind.sql_types, parquet_bind.column_names, parquet_bind.codec);
456: 	return move(global_state);
457: }
458: 
459: void ParquetWriteSink(ClientContext &context, FunctionData &bind_data_p, GlobalFunctionData &gstate,
460:                       LocalFunctionData &lstate, DataChunk &input) {
461: 	auto &bind_data = (ParquetWriteBindData &)bind_data_p;
462: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
463: 	auto &local_state = (ParquetWriteLocalState &)lstate;
464: 
465: 	// append data to the local (buffered) chunk collection
466: 	local_state.buffer->Append(input);
467: 	if (local_state.buffer->Count() > bind_data.row_group_size) {
468: 		// if the chunk collection exceeds a certain size we flush it to the parquet file
469: 		global_state.writer->Flush(*local_state.buffer);
470: 		// and reset the buffer
471: 		local_state.buffer = make_unique<ChunkCollection>();
472: 	}
473: }
474: 
475: void ParquetWriteCombine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
476:                          LocalFunctionData &lstate) {
477: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
478: 	auto &local_state = (ParquetWriteLocalState &)lstate;
479: 	// flush any data left in the local state to the file
480: 	global_state.writer->Flush(*local_state.buffer);
481: }
482: 
483: void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
484: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
485: 	// finalize: write any additional metadata to the file here
486: 	global_state.writer->Finalize();
487: }
488: 
489: unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ClientContext &context, FunctionData &bind_data) {
490: 	return make_unique<ParquetWriteLocalState>();
491: }
492: 
493: unique_ptr<TableFunctionRef> ParquetScanReplacement(const string &table_name, void *data) {
494: 	if (!StringUtil::EndsWith(StringUtil::Lower(table_name), ".parquet")) {
495: 		return nullptr;
496: 	}
497: 	auto table_function = make_unique<TableFunctionRef>();
498: 	vector<unique_ptr<ParsedExpression>> children;
499: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
500: 	table_function->function = make_unique<FunctionExpression>("parquet_scan", move(children));
501: 	return table_function;
502: }
503: 
504: void ParquetExtension::Load(DuckDB &db) {
505: 	auto &fs = db.GetFileSystem();
506: 	fs.RegisterSubSystem(FileCompressionType::ZSTD, make_unique<ZStdFileSystem>());
507: 
508: 	auto scan_fun = ParquetScanFunction::GetFunctionSet();
509: 	CreateTableFunctionInfo cinfo(scan_fun);
510: 	cinfo.name = "read_parquet";
511: 	CreateTableFunctionInfo pq_scan = cinfo;
512: 	pq_scan.name = "parquet_scan";
513: 
514: 	ParquetMetaDataFunction meta_fun;
515: 	CreateTableFunctionInfo meta_cinfo(meta_fun);
516: 
517: 	ParquetSchemaFunction schema_fun;
518: 	CreateTableFunctionInfo schema_cinfo(schema_fun);
519: 
520: 	CopyFunction function("parquet");
521: 	function.copy_to_bind = ParquetWriteBind;
522: 	function.copy_to_initialize_global = ParquetWriteInitializeGlobal;
523: 	function.copy_to_initialize_local = ParquetWriteInitializeLocal;
524: 	function.copy_to_sink = ParquetWriteSink;
525: 	function.copy_to_combine = ParquetWriteCombine;
526: 	function.copy_to_finalize = ParquetWriteFinalize;
527: 	function.copy_from_bind = ParquetScanFunction::ParquetReadBind;
528: 	function.copy_from_function = scan_fun.functions[0];
529: 
530: 	function.extension = "parquet";
531: 	CreateCopyFunctionInfo info(function);
532: 
533: 	Connection con(db);
534: 	con.BeginTransaction();
535: 	auto &context = *con.context;
536: 	auto &catalog = Catalog::GetCatalog(context);
537: 	catalog.CreateCopyFunction(context, &info);
538: 	catalog.CreateTableFunction(context, &cinfo);
539: 	catalog.CreateTableFunction(context, &pq_scan);
540: 	catalog.CreateTableFunction(context, &meta_cinfo);
541: 	catalog.CreateTableFunction(context, &schema_cinfo);
542: 	con.Commit();
543: 
544: 	auto &config = DBConfig::GetConfig(*db.instance);
545: 	config.replacement_scans.emplace_back(ParquetScanReplacement);
546: 	config.AddExtensionOption("binary_as_string", "In Parquet files, interpret binary data as a string.",
547: 	                          LogicalType::BOOLEAN);
548: }
549: 
550: std::string ParquetExtension::Name() {
551: 	return "parquet";
552: }
553: 
554: } // namespace duckdb
[end of extension/parquet/parquet-extension.cpp]
[start of extension/parquet/parquet_metadata.cpp]
1: #include "parquet_metadata.hpp"
2: #include "parquet_statistics.hpp"
3: 
4: #include <sstream>
5: 
6: #ifndef DUCKDB_AMALGAMATION
7: #include "duckdb/common/types/blob.hpp"
8: #include "duckdb/main/config.hpp"
9: #endif
10: 
11: namespace duckdb {
12: 
13: struct ParquetMetaDataBindData : public FunctionData {
14: 	vector<LogicalType> return_types;
15: 	vector<string> files;
16: };
17: 
18: struct ParquetMetaDataOperatorData : public FunctionOperatorData {
19: 	idx_t file_index;
20: 	ChunkCollection collection;
21: 
22: 	static void BindMetaData(vector<LogicalType> &return_types, vector<string> &names);
23: 	static void BindSchema(vector<LogicalType> &return_types, vector<string> &names);
24: 
25: 	void LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types, const string &file_path);
26: 	void LoadSchemaData(ClientContext &context, const vector<LogicalType> &return_types, const string &file_path);
27: };
28: 
29: template <class T>
30: string ConvertParquetElementToString(T &&entry) {
31: 	std::stringstream ss;
32: 	ss << entry;
33: 	return ss.str();
34: }
35: 
36: template <class T>
37: string PrintParquetElementToString(T &&entry) {
38: 	std::stringstream ss;
39: 	entry.printTo(ss);
40: 	return ss.str();
41: }
42: 
43: void ParquetMetaDataOperatorData::BindMetaData(vector<LogicalType> &return_types, vector<string> &names) {
44: 	names.emplace_back("file_name");
45: 	return_types.emplace_back(LogicalType::VARCHAR);
46: 
47: 	names.emplace_back("row_group_id");
48: 	return_types.emplace_back(LogicalType::BIGINT);
49: 
50: 	names.emplace_back("row_group_num_rows");
51: 	return_types.emplace_back(LogicalType::BIGINT);
52: 
53: 	names.emplace_back("row_group_num_columns");
54: 	return_types.emplace_back(LogicalType::BIGINT);
55: 
56: 	names.emplace_back("row_group_bytes");
57: 	return_types.emplace_back(LogicalType::BIGINT);
58: 
59: 	names.emplace_back("column_id");
60: 	return_types.emplace_back(LogicalType::BIGINT);
61: 
62: 	names.emplace_back("file_offset");
63: 	return_types.emplace_back(LogicalType::BIGINT);
64: 
65: 	names.emplace_back("num_values");
66: 	return_types.emplace_back(LogicalType::BIGINT);
67: 
68: 	names.emplace_back("path_in_schema");
69: 	return_types.emplace_back(LogicalType::VARCHAR);
70: 
71: 	names.emplace_back("type");
72: 	return_types.emplace_back(LogicalType::VARCHAR);
73: 
74: 	names.emplace_back("stats_min");
75: 	return_types.emplace_back(LogicalType::VARCHAR);
76: 
77: 	names.emplace_back("stats_max");
78: 	return_types.emplace_back(LogicalType::VARCHAR);
79: 
80: 	names.emplace_back("stats_null_count");
81: 	return_types.emplace_back(LogicalType::BIGINT);
82: 
83: 	names.emplace_back("stats_distinct_count");
84: 	return_types.emplace_back(LogicalType::BIGINT);
85: 
86: 	names.emplace_back("stats_min_value");
87: 	return_types.emplace_back(LogicalType::VARCHAR);
88: 
89: 	names.emplace_back("stats_max_value");
90: 	return_types.emplace_back(LogicalType::VARCHAR);
91: 
92: 	names.emplace_back("compression");
93: 	return_types.emplace_back(LogicalType::VARCHAR);
94: 
95: 	names.emplace_back("encodings");
96: 	return_types.emplace_back(LogicalType::VARCHAR);
97: 
98: 	names.emplace_back("index_page_offset");
99: 	return_types.emplace_back(LogicalType::BIGINT);
100: 
101: 	names.emplace_back("dictionary_page_offset");
102: 	return_types.emplace_back(LogicalType::BIGINT);
103: 
104: 	names.emplace_back("data_page_offset");
105: 	return_types.emplace_back(LogicalType::BIGINT);
106: 
107: 	names.emplace_back("total_compressed_size");
108: 	return_types.emplace_back(LogicalType::BIGINT);
109: 
110: 	names.emplace_back("total_uncompressed_size");
111: 	return_types.emplace_back(LogicalType::BIGINT);
112: }
113: 
114: Value ConvertParquetStats(const LogicalType &type, const duckdb_parquet::format::SchemaElement &schema_ele,
115:                           bool stats_is_set, const std::string &stats) {
116: 	if (!stats_is_set) {
117: 		return Value(LogicalType::VARCHAR);
118: 	}
119: 	return ParquetStatisticsUtils::ConvertValue(type, schema_ele, stats).CastAs(LogicalType::VARCHAR);
120: }
121: 
122: void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types,
123:                                                    const string &file_path) {
124: 	collection.Reset();
125: 	ParquetOptions parquet_options(context);
126: 	auto reader = make_unique<ParquetReader>(context, file_path, parquet_options);
127: 	idx_t count = 0;
128: 	DataChunk current_chunk;
129: 	current_chunk.Initialize(return_types);
130: 	auto meta_data = reader->GetFileMetadata();
131: 	vector<LogicalType> column_types;
132: 	vector<idx_t> schema_indexes;
133: 	for (idx_t schema_idx = 0; schema_idx < meta_data->schema.size(); schema_idx++) {
134: 		auto &schema_element = meta_data->schema[schema_idx];
135: 		if (schema_element.num_children > 0) {
136: 			continue;
137: 		}
138: 		column_types.push_back(ParquetReader::DeriveLogicalType(schema_element, false));
139: 		schema_indexes.push_back(schema_idx);
140: 	}
141: 
142: 	for (idx_t row_group_idx = 0; row_group_idx < meta_data->row_groups.size(); row_group_idx++) {
143: 		auto &row_group = meta_data->row_groups[row_group_idx];
144: 
145: 		if (row_group.columns.size() > column_types.size()) {
146: 			throw InternalException("Too many column in row group: corrupt file?");
147: 		}
148: 		for (idx_t col_idx = 0; col_idx < row_group.columns.size(); col_idx++) {
149: 			auto &column = row_group.columns[col_idx];
150: 			auto &col_meta = column.meta_data;
151: 			auto &stats = col_meta.statistics;
152: 			auto &schema_element = meta_data->schema[schema_indexes[col_idx]];
153: 			auto &column_type = column_types[col_idx];
154: 
155: 			// file_name, LogicalType::VARCHAR
156: 			current_chunk.SetValue(0, count, file_path);
157: 
158: 			// row_group_id, LogicalType::BIGINT
159: 			current_chunk.SetValue(1, count, Value::BIGINT(row_group_idx));
160: 
161: 			// row_group_num_rows, LogicalType::BIGINT
162: 			current_chunk.SetValue(2, count, Value::BIGINT(row_group.num_rows));
163: 
164: 			// row_group_num_columns, LogicalType::BIGINT
165: 			current_chunk.SetValue(3, count, Value::BIGINT(row_group.columns.size()));
166: 
167: 			// row_group_bytes, LogicalType::BIGINT
168: 			current_chunk.SetValue(4, count, Value::BIGINT(row_group.total_byte_size));
169: 
170: 			// column_id, LogicalType::BIGINT
171: 			current_chunk.SetValue(5, count, Value::BIGINT(col_idx));
172: 
173: 			// file_offset, LogicalType::BIGINT
174: 			current_chunk.SetValue(6, count, Value::BIGINT(column.file_offset));
175: 
176: 			// num_values, LogicalType::BIGINT
177: 			current_chunk.SetValue(7, count, Value::BIGINT(col_meta.num_values));
178: 
179: 			// path_in_schema, LogicalType::VARCHAR
180: 			current_chunk.SetValue(8, count, StringUtil::Join(col_meta.path_in_schema, ", "));
181: 
182: 			// type, LogicalType::VARCHAR
183: 			current_chunk.SetValue(9, count, ConvertParquetElementToString(col_meta.type));
184: 
185: 			// stats_min, LogicalType::VARCHAR
186: 			current_chunk.SetValue(10, count,
187: 			                       ConvertParquetStats(column_type, schema_element, stats.__isset.min, stats.min));
188: 
189: 			// stats_max, LogicalType::VARCHAR
190: 			current_chunk.SetValue(11, count,
191: 			                       ConvertParquetStats(column_type, schema_element, stats.__isset.max, stats.max));
192: 
193: 			// stats_null_count, LogicalType::BIGINT
194: 			current_chunk.SetValue(
195: 			    12, count, stats.__isset.null_count ? Value::BIGINT(stats.null_count) : Value(LogicalType::BIGINT));
196: 
197: 			// stats_distinct_count, LogicalType::BIGINT
198: 			current_chunk.SetValue(13, count,
199: 			                       stats.__isset.distinct_count ? Value::BIGINT(stats.distinct_count)
200: 			                                                    : Value(LogicalType::BIGINT));
201: 
202: 			// stats_min_value, LogicalType::VARCHAR
203: 			current_chunk.SetValue(
204: 			    14, count, ConvertParquetStats(column_type, schema_element, stats.__isset.min_value, stats.min_value));
205: 
206: 			// stats_max_value, LogicalType::VARCHAR
207: 			current_chunk.SetValue(
208: 			    15, count, ConvertParquetStats(column_type, schema_element, stats.__isset.max_value, stats.max_value));
209: 
210: 			// compression, LogicalType::VARCHAR
211: 			current_chunk.SetValue(16, count, ConvertParquetElementToString(col_meta.codec));
212: 
213: 			// encodings, LogicalType::VARCHAR
214: 			vector<string> encoding_string;
215: 			for (auto &encoding : col_meta.encodings) {
216: 				encoding_string.push_back(ConvertParquetElementToString(encoding));
217: 			}
218: 			current_chunk.SetValue(17, count, Value(StringUtil::Join(encoding_string, ", ")));
219: 
220: 			// index_page_offset, LogicalType::BIGINT
221: 			current_chunk.SetValue(18, count, Value::BIGINT(col_meta.index_page_offset));
222: 
223: 			// dictionary_page_offset, LogicalType::BIGINT
224: 			current_chunk.SetValue(19, count, Value::BIGINT(col_meta.dictionary_page_offset));
225: 
226: 			// data_page_offset, LogicalType::BIGINT
227: 			current_chunk.SetValue(20, count, Value::BIGINT(col_meta.data_page_offset));
228: 
229: 			// total_compressed_size, LogicalType::BIGINT
230: 			current_chunk.SetValue(21, count, Value::BIGINT(col_meta.total_compressed_size));
231: 
232: 			// total_uncompressed_size, LogicalType::BIGINT
233: 			current_chunk.SetValue(22, count, Value::BIGINT(col_meta.total_uncompressed_size));
234: 
235: 			count++;
236: 			if (count >= STANDARD_VECTOR_SIZE) {
237: 				current_chunk.SetCardinality(count);
238: 				collection.Append(current_chunk);
239: 
240: 				count = 0;
241: 				current_chunk.Reset();
242: 			}
243: 		}
244: 	}
245: 	current_chunk.SetCardinality(count);
246: 	collection.Append(current_chunk);
247: }
248: 
249: void ParquetMetaDataOperatorData::BindSchema(vector<LogicalType> &return_types, vector<string> &names) {
250: 	names.emplace_back("file_name");
251: 	return_types.emplace_back(LogicalType::VARCHAR);
252: 
253: 	names.emplace_back("name");
254: 	return_types.emplace_back(LogicalType::VARCHAR);
255: 
256: 	names.emplace_back("type");
257: 	return_types.emplace_back(LogicalType::VARCHAR);
258: 
259: 	names.emplace_back("type_length");
260: 	return_types.emplace_back(LogicalType::VARCHAR);
261: 
262: 	names.emplace_back("repetition_type");
263: 	return_types.emplace_back(LogicalType::VARCHAR);
264: 
265: 	names.emplace_back("num_children");
266: 	return_types.emplace_back(LogicalType::BIGINT);
267: 
268: 	names.emplace_back("converted_type");
269: 	return_types.emplace_back(LogicalType::VARCHAR);
270: 
271: 	names.emplace_back("scale");
272: 	return_types.emplace_back(LogicalType::BIGINT);
273: 
274: 	names.emplace_back("precision");
275: 	return_types.emplace_back(LogicalType::BIGINT);
276: 
277: 	names.emplace_back("field_id");
278: 	return_types.emplace_back(LogicalType::BIGINT);
279: 
280: 	names.emplace_back("logical_type");
281: 	return_types.emplace_back(LogicalType::VARCHAR);
282: }
283: 
284: Value ParquetLogicalTypeToString(const duckdb_parquet::format::LogicalType &type) {
285: 
286: 	if (type.__isset.STRING) {
287: 		return Value(PrintParquetElementToString(type.STRING));
288: 	}
289: 	if (type.__isset.MAP) {
290: 		return Value(PrintParquetElementToString(type.MAP));
291: 	}
292: 	if (type.__isset.LIST) {
293: 		return Value(PrintParquetElementToString(type.LIST));
294: 	}
295: 	if (type.__isset.ENUM) {
296: 		return Value(PrintParquetElementToString(type.ENUM));
297: 	}
298: 	if (type.__isset.DECIMAL) {
299: 		return Value(PrintParquetElementToString(type.DECIMAL));
300: 	}
301: 	if (type.__isset.DATE) {
302: 		return Value(PrintParquetElementToString(type.DATE));
303: 	}
304: 	if (type.__isset.TIME) {
305: 		return Value(PrintParquetElementToString(type.TIME));
306: 	}
307: 	if (type.__isset.TIMESTAMP) {
308: 		return Value(PrintParquetElementToString(type.TIMESTAMP));
309: 	}
310: 	if (type.__isset.INTEGER) {
311: 		return Value(PrintParquetElementToString(type.INTEGER));
312: 	}
313: 	if (type.__isset.UNKNOWN) {
314: 		return Value(PrintParquetElementToString(type.UNKNOWN));
315: 	}
316: 	if (type.__isset.JSON) {
317: 		return Value(PrintParquetElementToString(type.JSON));
318: 	}
319: 	if (type.__isset.BSON) {
320: 		return Value(PrintParquetElementToString(type.BSON));
321: 	}
322: 	if (type.__isset.UUID) {
323: 		return Value(PrintParquetElementToString(type.UUID));
324: 	}
325: 	return Value();
326: }
327: 
328: void ParquetMetaDataOperatorData::LoadSchemaData(ClientContext &context, const vector<LogicalType> &return_types,
329:                                                  const string &file_path) {
330: 	collection.Reset();
331: 	ParquetOptions parquet_options(context);
332: 	auto reader = make_unique<ParquetReader>(context, file_path, parquet_options);
333: 	idx_t count = 0;
334: 	DataChunk current_chunk;
335: 	current_chunk.Initialize(return_types);
336: 	auto meta_data = reader->GetFileMetadata();
337: 	for (idx_t col_idx = 0; col_idx < meta_data->schema.size(); col_idx++) {
338: 		auto &column = meta_data->schema[col_idx];
339: 
340: 		// file_name, LogicalType::VARCHAR
341: 		current_chunk.SetValue(0, count, file_path);
342: 
343: 		// name, LogicalType::VARCHAR
344: 		current_chunk.SetValue(1, count, column.name);
345: 
346: 		// type, LogicalType::VARCHAR
347: 		current_chunk.SetValue(2, count, ConvertParquetElementToString(column.type));
348: 
349: 		// type_length, LogicalType::VARCHAR
350: 		current_chunk.SetValue(3, count, Value::INTEGER(column.type_length));
351: 
352: 		// repetition_type, LogicalType::VARCHAR
353: 		current_chunk.SetValue(4, count, ConvertParquetElementToString(column.repetition_type));
354: 
355: 		// num_children, LogicalType::BIGINT
356: 		current_chunk.SetValue(5, count, Value::BIGINT(column.num_children));
357: 
358: 		// converted_type, LogicalType::VARCHAR
359: 		current_chunk.SetValue(6, count, ConvertParquetElementToString(column.converted_type));
360: 
361: 		// scale, LogicalType::BIGINT
362: 		current_chunk.SetValue(7, count, Value::BIGINT(column.scale));
363: 
364: 		// precision, LogicalType::BIGINT
365: 		current_chunk.SetValue(8, count, Value::BIGINT(column.precision));
366: 
367: 		// field_id, LogicalType::BIGINT
368: 		current_chunk.SetValue(9, count, Value::BIGINT(column.field_id));
369: 
370: 		// logical_type, LogicalType::VARCHAR
371: 		current_chunk.SetValue(10, count, ParquetLogicalTypeToString(column.logicalType));
372: 
373: 		count++;
374: 		if (count >= STANDARD_VECTOR_SIZE) {
375: 			current_chunk.SetCardinality(count);
376: 			collection.Append(current_chunk);
377: 
378: 			count = 0;
379: 			current_chunk.Reset();
380: 		}
381: 	}
382: 	current_chunk.SetCardinality(count);
383: 	collection.Append(current_chunk);
384: }
385: 
386: template <bool SCHEMA>
387: unique_ptr<FunctionData> ParquetMetaDataBind(ClientContext &context, vector<Value> &inputs,
388:                                              unordered_map<string, Value> &named_parameters,
389:                                              vector<LogicalType> &input_table_types, vector<string> &input_table_names,
390:                                              vector<LogicalType> &return_types, vector<string> &names) {
391: 	auto &config = DBConfig::GetConfig(context);
392: 	if (!config.enable_external_access) {
393: 		throw PermissionException("Scanning Parquet files is disabled through configuration");
394: 	}
395: 	if (SCHEMA) {
396: 		ParquetMetaDataOperatorData::BindSchema(return_types, names);
397: 	} else {
398: 		ParquetMetaDataOperatorData::BindMetaData(return_types, names);
399: 	}
400: 
401: 	auto file_name = inputs[0].GetValue<string>();
402: 	auto result = make_unique<ParquetMetaDataBindData>();
403: 
404: 	FileSystem &fs = FileSystem::GetFileSystem(context);
405: 	result->return_types = return_types;
406: 	result->files = fs.Glob(file_name);
407: 	if (result->files.empty()) {
408: 		throw IOException("No files found that match the pattern \"%s\"", file_name);
409: 	}
410: 	return move(result);
411: }
412: 
413: template <bool SCHEMA>
414: unique_ptr<FunctionOperatorData> ParquetMetaDataInit(ClientContext &context, const FunctionData *bind_data_p,
415:                                                      const vector<column_t> &column_ids,
416:                                                      TableFilterCollection *filters) {
417: 	auto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;
418: 	D_ASSERT(!bind_data.files.empty());
419: 
420: 	auto result = make_unique<ParquetMetaDataOperatorData>();
421: 	if (SCHEMA) {
422: 		result->LoadSchemaData(context, bind_data.return_types, bind_data.files[0]);
423: 	} else {
424: 		result->LoadFileMetaData(context, bind_data.return_types, bind_data.files[0]);
425: 	}
426: 	result->file_index = 0;
427: 	return move(result);
428: }
429: 
430: template <bool SCHEMA>
431: void ParquetMetaDataImplementation(ClientContext &context, const FunctionData *bind_data_p,
432:                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
433: 	auto &data = (ParquetMetaDataOperatorData &)*operator_state;
434: 	auto &bind_data = (ParquetMetaDataBindData &)*bind_data_p;
435: 	while (true) {
436: 		auto chunk = data.collection.Fetch();
437: 		if (!chunk) {
438: 			if (data.file_index + 1 < bind_data.files.size()) {
439: 				// load the metadata for the next file
440: 				data.file_index++;
441: 				if (SCHEMA) {
442: 					data.LoadSchemaData(context, bind_data.return_types, bind_data.files[data.file_index]);
443: 				} else {
444: 					data.LoadFileMetaData(context, bind_data.return_types, bind_data.files[data.file_index]);
445: 				}
446: 				continue;
447: 			} else {
448: 				// no files remaining: done
449: 				return;
450: 			}
451: 		}
452: 		output.Move(*chunk);
453: 		if (output.size() != 0) {
454: 			return;
455: 		}
456: 	}
457: }
458: 
459: ParquetMetaDataFunction::ParquetMetaDataFunction()
460:     : TableFunction("parquet_metadata", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<false>,
461:                     ParquetMetaDataBind<false>, ParquetMetaDataInit<false>, /* statistics */ nullptr,
462:                     /* cleanup */ nullptr,
463:                     /* dependency */ nullptr, nullptr,
464:                     /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,
465:                     nullptr, false, false, nullptr) {
466: }
467: 
468: ParquetSchemaFunction::ParquetSchemaFunction()
469:     : TableFunction("parquet_schema", {LogicalType::VARCHAR}, ParquetMetaDataImplementation<true>,
470:                     ParquetMetaDataBind<true>, ParquetMetaDataInit<true>, /* statistics */ nullptr,
471:                     /* cleanup */ nullptr,
472:                     /* dependency */ nullptr, nullptr,
473:                     /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, nullptr, nullptr, nullptr, nullptr,
474:                     nullptr, false, false, nullptr) {
475: }
476: 
477: } // namespace duckdb
[end of extension/parquet/parquet_metadata.cpp]
[start of extension/tpcds/tpcds-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include "tpcds-extension.hpp"
4: 
5: #include "dsdgen.hpp"
6: 
7: #ifndef DUCKDB_AMALGAMATION
8: #include "duckdb/function/table_function.hpp"
9: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
10: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
11: #include "duckdb/parser/parsed_data/create_view_info.hpp"
12: #include "duckdb/parser/parser.hpp"
13: #include "duckdb/parser/statement/select_statement.hpp"
14: #endif
15: 
16: namespace duckdb {
17: 
18: struct DSDGenFunctionData : public TableFunctionData {
19: 	DSDGenFunctionData() {
20: 	}
21: 
22: 	bool finished = false;
23: 	double sf = 0;
24: 	string schema = DEFAULT_SCHEMA;
25: 	string suffix;
26: 	bool overwrite = false;
27: 	bool keys = false;
28: };
29: 
30: static unique_ptr<FunctionData> DsdgenBind(ClientContext &context, vector<Value> &inputs,
31:                                            unordered_map<string, Value> &named_parameters,
32:                                            vector<LogicalType> &input_table_types, vector<string> &input_table_names,
33:                                            vector<LogicalType> &return_types, vector<string> &names) {
34: 	auto result = make_unique<DSDGenFunctionData>();
35: 	for (auto &kv : named_parameters) {
36: 		if (kv.first == "sf") {
37: 			result->sf = kv.second.GetValue<double>();
38: 		} else if (kv.first == "schema") {
39: 			result->schema = StringValue::Get(kv.second);
40: 		} else if (kv.first == "suffix") {
41: 			result->suffix = StringValue::Get(kv.second);
42: 		} else if (kv.first == "overwrite") {
43: 			result->overwrite = kv.second.GetValue<bool>();
44: 		} else if (kv.first == "keys") {
45: 			result->keys = kv.second.GetValue<bool>();
46: 		}
47: 	}
48: 	return_types.emplace_back(LogicalType::BOOLEAN);
49: 	names.emplace_back("Success");
50: 	return move(result);
51: }
52: 
53: static void DsdgenFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
54:                            DataChunk *input, DataChunk &output) {
55: 	auto &data = (DSDGenFunctionData &)*bind_data;
56: 	if (data.finished) {
57: 		return;
58: 	}
59: 	tpcds::DSDGenWrapper::CreateTPCDSSchema(context, data.schema, data.suffix, data.keys, data.overwrite);
60: 	tpcds::DSDGenWrapper::DSDGen(data.sf, context, data.schema, data.suffix);
61: 
62: 	data.finished = true;
63: }
64: 
65: struct TPCDSData : public FunctionOperatorData {
66: 	TPCDSData() : offset(0) {
67: 	}
68: 	idx_t offset;
69: };
70: 
71: unique_ptr<FunctionOperatorData> TPCDSInit(ClientContext &context, const FunctionData *bind_data,
72:                                            const vector<column_t> &column_ids, TableFilterCollection *filters) {
73: 	auto result = make_unique<TPCDSData>();
74: 	return move(result);
75: }
76: 
77: static unique_ptr<FunctionData> TPCDSQueryBind(ClientContext &context, vector<Value> &inputs,
78:                                                unordered_map<string, Value> &named_parameters,
79:                                                vector<LogicalType> &input_table_types,
80:                                                vector<string> &input_table_names, vector<LogicalType> &return_types,
81:                                                vector<string> &names) {
82: 	names.emplace_back("query_nr");
83: 	return_types.emplace_back(LogicalType::INTEGER);
84: 
85: 	names.emplace_back("query");
86: 	return_types.emplace_back(LogicalType::VARCHAR);
87: 
88: 	return nullptr;
89: }
90: 
91: static void TPCDSQueryFunction(ClientContext &context, const FunctionData *bind_data,
92:                                FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
93: 	auto &data = (TPCDSData &)*operator_state;
94: 	idx_t tpcds_queries = tpcds::DSDGenWrapper::QueriesCount();
95: 	if (data.offset >= tpcds_queries) {
96: 		// finished returning values
97: 		return;
98: 	}
99: 	idx_t chunk_count = 0;
100: 	while (data.offset < tpcds_queries && chunk_count < STANDARD_VECTOR_SIZE) {
101: 		auto query = TPCDSExtension::GetQuery(data.offset + 1);
102: 		// "query_nr", PhysicalType::INT32
103: 		output.SetValue(0, chunk_count, Value::INTEGER((int32_t)data.offset + 1));
104: 		// "query", PhysicalType::VARCHAR
105: 		output.SetValue(1, chunk_count, Value(query));
106: 		data.offset++;
107: 		chunk_count++;
108: 	}
109: 	output.SetCardinality(chunk_count);
110: }
111: 
112: static unique_ptr<FunctionData> TPCDSQueryAnswerBind(ClientContext &context, vector<Value> &inputs,
113:                                                      unordered_map<string, Value> &named_parameters,
114:                                                      vector<LogicalType> &input_table_types,
115:                                                      vector<string> &input_table_names,
116:                                                      vector<LogicalType> &return_types, vector<string> &names) {
117: 	names.emplace_back("query_nr");
118: 	return_types.emplace_back(LogicalType::INTEGER);
119: 
120: 	names.emplace_back("scale_factor");
121: 	return_types.emplace_back(LogicalType::DOUBLE);
122: 
123: 	names.emplace_back("answer");
124: 	return_types.emplace_back(LogicalType::VARCHAR);
125: 
126: 	return nullptr;
127: }
128: 
129: static void TPCDSQueryAnswerFunction(ClientContext &context, const FunctionData *bind_data,
130:                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
131: 	auto &data = (TPCDSData &)*operator_state;
132: 	idx_t tpcds_queries = tpcds::DSDGenWrapper::QueriesCount();
133: 	vector<double> scale_factors {1, 10};
134: 	idx_t total_answers = tpcds_queries * scale_factors.size();
135: 	if (data.offset >= total_answers) {
136: 		// finished returning values
137: 		return;
138: 	}
139: 	idx_t chunk_count = 0;
140: 	while (data.offset < total_answers && chunk_count < STANDARD_VECTOR_SIZE) {
141: 		idx_t cur_query = data.offset % tpcds_queries;
142: 		idx_t cur_sf = data.offset / tpcds_queries;
143: 		auto answer = TPCDSExtension::GetAnswer(scale_factors[cur_sf], cur_query + 1);
144: 		// "query_nr", PhysicalType::INT32
145: 		output.SetValue(0, chunk_count, Value::INTEGER((int32_t)cur_query + 1));
146: 		// "scale_factor", PhysicalType::DOUBLE
147: 		output.SetValue(1, chunk_count, Value::DOUBLE(scale_factors[cur_sf]));
148: 		// "query", PhysicalType::VARCHAR
149: 		output.SetValue(2, chunk_count, Value(answer));
150: 		data.offset++;
151: 		chunk_count++;
152: 	}
153: 	output.SetCardinality(chunk_count);
154: }
155: 
156: static string PragmaTpcdsQuery(ClientContext &context, const FunctionParameters &parameters) {
157: 	auto index = parameters.values[0].GetValue<int32_t>();
158: 	return tpcds::DSDGenWrapper::GetQuery(index);
159: }
160: 
161: void TPCDSExtension::Load(DuckDB &db) {
162: 	Connection con(db);
163: 	con.BeginTransaction();
164: 
165: 	TableFunction dsdgen_func("dsdgen", {}, DsdgenFunction, DsdgenBind);
166: 	dsdgen_func.named_parameters["sf"] = LogicalType::DOUBLE;
167: 	dsdgen_func.named_parameters["overwrite"] = LogicalType::BOOLEAN;
168: 	dsdgen_func.named_parameters["keys"] = LogicalType::BOOLEAN;
169: 	dsdgen_func.named_parameters["schema"] = LogicalType::VARCHAR;
170: 	dsdgen_func.named_parameters["suffix"] = LogicalType::VARCHAR;
171: 	CreateTableFunctionInfo dsdgen_info(dsdgen_func);
172: 
173: 	// create the dsdgen function
174: 	auto &catalog = Catalog::GetCatalog(*con.context);
175: 	catalog.CreateTableFunction(*con.context, &dsdgen_info);
176: 
177: 	// create the TPCDS pragma that allows us to run the query
178: 	auto tpcds_func = PragmaFunction::PragmaCall("tpcds", PragmaTpcdsQuery, {LogicalType::BIGINT});
179: 	CreatePragmaFunctionInfo info(tpcds_func);
180: 	catalog.CreatePragmaFunction(*con.context, &info);
181: 
182: 	// create the TPCDS_QUERIES function that returns the query
183: 	TableFunction tpcds_query_func("tpcds_queries", {}, TPCDSQueryFunction, TPCDSQueryBind, TPCDSInit);
184: 	CreateTableFunctionInfo tpcds_query_info(tpcds_query_func);
185: 	catalog.CreateTableFunction(*con.context, &tpcds_query_info);
186: 
187: 	// create the TPCDS_ANSWERS that returns the query result
188: 	TableFunction tpcds_query_answer_func("tpcds_answers", {}, TPCDSQueryAnswerFunction, TPCDSQueryAnswerBind,
189: 	                                      TPCDSInit);
190: 	CreateTableFunctionInfo tpcds_query_asnwer_info(tpcds_query_answer_func);
191: 	catalog.CreateTableFunction(*con.context, &tpcds_query_asnwer_info);
192: 
193: 	con.Commit();
194: }
195: 
196: std::string TPCDSExtension::GetQuery(int query) {
197: 	return tpcds::DSDGenWrapper::GetQuery(query);
198: }
199: 
200: std::string TPCDSExtension::GetAnswer(double sf, int query) {
201: 	return tpcds::DSDGenWrapper::GetAnswer(sf, query);
202: }
203: 
204: std::string TPCDSExtension::Name() {
205: 	return "tpcds";
206: }
207: 
208: } // namespace duckdb
209: 
210: extern "C" {
211: DUCKDB_EXTENSION_API void tpcds_init(duckdb::DatabaseInstance &db) {
212: 	duckdb::DuckDB db_wrapper(db);
213: 	db_wrapper.LoadExtension<duckdb::TPCDSExtension>();
214: }
215: 
216: DUCKDB_EXTENSION_API const char *tpcds_version() {
217: 	return duckdb::DuckDB::LibraryVersion();
218: }
219: }
[end of extension/tpcds/tpcds-extension.cpp]
[start of extension/tpch/tpch-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include "tpch-extension.hpp"
4: 
5: #ifndef DUCKDB_AMALGAMATION
6: #include "duckdb/function/table_function.hpp"
7: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
8: #include "duckdb/parser/parsed_data/create_view_info.hpp"
9: #include "duckdb/parser/parser.hpp"
10: #include "duckdb/parser/statement/select_statement.hpp"
11: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
12: #endif
13: 
14: #include "dbgen/dbgen.hpp"
15: 
16: namespace duckdb {
17: 
18: struct DBGenFunctionData : public TableFunctionData {
19: 	DBGenFunctionData() {
20: 	}
21: 
22: 	bool finished = false;
23: 	double sf = 0;
24: 	string schema = DEFAULT_SCHEMA;
25: 	string suffix;
26: 	bool overwrite = false;
27: };
28: 
29: static unique_ptr<FunctionData> DbgenBind(ClientContext &context, vector<Value> &inputs,
30:                                           unordered_map<string, Value> &named_parameters,
31:                                           vector<LogicalType> &input_table_types, vector<string> &input_table_names,
32:                                           vector<LogicalType> &return_types, vector<string> &names) {
33: 	auto result = make_unique<DBGenFunctionData>();
34: 	for (auto &kv : named_parameters) {
35: 		if (kv.first == "sf") {
36: 			result->sf = DoubleValue::Get(kv.second);
37: 		} else if (kv.first == "schema") {
38: 			result->schema = StringValue::Get(kv.second);
39: 		} else if (kv.first == "suffix") {
40: 			result->suffix = StringValue::Get(kv.second);
41: 		} else if (kv.first == "overwrite") {
42: 			result->overwrite = BooleanValue::Get(kv.second);
43: 		}
44: 	}
45: 	return_types.emplace_back(LogicalType::BOOLEAN);
46: 	names.emplace_back("Success");
47: 	return move(result);
48: }
49: 
50: static void DbgenFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
51:                           DataChunk *input, DataChunk &output) {
52: 	auto &data = (DBGenFunctionData &)*bind_data;
53: 	if (data.finished) {
54: 		return;
55: 	}
56: 	tpch::DBGenWrapper::CreateTPCHSchema(context, data.schema, data.suffix);
57: 	tpch::DBGenWrapper::LoadTPCHData(context, data.sf, data.schema, data.suffix);
58: 
59: 	data.finished = true;
60: }
61: 
62: struct TPCHData : public FunctionOperatorData {
63: 	TPCHData() : offset(0) {
64: 	}
65: 	idx_t offset;
66: };
67: 
68: unique_ptr<FunctionOperatorData> TPCHInit(ClientContext &context, const FunctionData *bind_data,
69:                                           const vector<column_t> &column_ids, TableFilterCollection *filters) {
70: 	auto result = make_unique<TPCHData>();
71: 	return move(result);
72: }
73: 
74: static unique_ptr<FunctionData> TPCHQueryBind(ClientContext &context, vector<Value> &inputs,
75:                                               unordered_map<string, Value> &named_parameters,
76:                                               vector<LogicalType> &input_table_types, vector<string> &input_table_names,
77:                                               vector<LogicalType> &return_types, vector<string> &names) {
78: 	names.emplace_back("query_nr");
79: 	return_types.emplace_back(LogicalType::INTEGER);
80: 
81: 	names.emplace_back("query");
82: 	return_types.emplace_back(LogicalType::VARCHAR);
83: 
84: 	return nullptr;
85: }
86: 
87: static void TPCHQueryFunction(ClientContext &context, const FunctionData *bind_data,
88:                               FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
89: 	auto &data = (TPCHData &)*operator_state;
90: 	idx_t tpch_queries = 22;
91: 	if (data.offset >= tpch_queries) {
92: 		// finished returning values
93: 		return;
94: 	}
95: 	idx_t chunk_count = 0;
96: 	while (data.offset < tpch_queries && chunk_count < STANDARD_VECTOR_SIZE) {
97: 		auto query = tpch::DBGenWrapper::GetQuery(data.offset + 1);
98: 		// "query_nr", PhysicalType::INT32
99: 		output.SetValue(0, chunk_count, Value::INTEGER((int32_t)data.offset + 1));
100: 		// "query", PhysicalType::VARCHAR
101: 		output.SetValue(1, chunk_count, Value(query));
102: 		data.offset++;
103: 		chunk_count++;
104: 	}
105: 	output.SetCardinality(chunk_count);
106: }
107: 
108: static unique_ptr<FunctionData> TPCHQueryAnswerBind(ClientContext &context, vector<Value> &inputs,
109:                                                     unordered_map<string, Value> &named_parameters,
110:                                                     vector<LogicalType> &input_table_types,
111:                                                     vector<string> &input_table_names,
112:                                                     vector<LogicalType> &return_types, vector<string> &names) {
113: 	names.emplace_back("query_nr");
114: 	return_types.emplace_back(LogicalType::INTEGER);
115: 
116: 	names.emplace_back("scale_factor");
117: 	return_types.emplace_back(LogicalType::DOUBLE);
118: 
119: 	names.emplace_back("answer");
120: 	return_types.emplace_back(LogicalType::VARCHAR);
121: 
122: 	return nullptr;
123: }
124: 
125: static void TPCHQueryAnswerFunction(ClientContext &context, const FunctionData *bind_data,
126:                                     FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
127: 	auto &data = (TPCHData &)*operator_state;
128: 	idx_t tpch_queries = 22;
129: 	vector<double> scale_factors {0.01, 0.1, 1};
130: 	idx_t total_answers = tpch_queries * scale_factors.size();
131: 	if (data.offset >= total_answers) {
132: 		// finished returning values
133: 		return;
134: 	}
135: 	idx_t chunk_count = 0;
136: 	while (data.offset < total_answers && chunk_count < STANDARD_VECTOR_SIZE) {
137: 		idx_t cur_query = data.offset % tpch_queries;
138: 		idx_t cur_sf = data.offset / tpch_queries;
139: 		auto answer = tpch::DBGenWrapper::GetAnswer(scale_factors[cur_sf], cur_query + 1);
140: 		// "query_nr", PhysicalType::INT32
141: 		output.SetValue(0, chunk_count, Value::INTEGER((int32_t)cur_query + 1));
142: 		// "scale_factor", PhysicalType::INT32
143: 		output.SetValue(1, chunk_count, Value::DOUBLE(scale_factors[cur_sf]));
144: 		// "query", PhysicalType::VARCHAR
145: 		output.SetValue(2, chunk_count, Value(answer));
146: 		data.offset++;
147: 		chunk_count++;
148: 	}
149: 	output.SetCardinality(chunk_count);
150: }
151: 
152: static string PragmaTpchQuery(ClientContext &context, const FunctionParameters &parameters) {
153: 	auto index = parameters.values[0].GetValue<int32_t>();
154: 	return tpch::DBGenWrapper::GetQuery(index);
155: }
156: 
157: void TPCHExtension::Load(DuckDB &db) {
158: 	Connection con(db);
159: 	con.BeginTransaction();
160: 	auto &catalog = Catalog::GetCatalog(*con.context);
161: 
162: 	TableFunction dbgen_func("dbgen", {}, DbgenFunction, DbgenBind);
163: 	dbgen_func.named_parameters["sf"] = LogicalType::DOUBLE;
164: 	dbgen_func.named_parameters["overwrite"] = LogicalType::BOOLEAN;
165: 	dbgen_func.named_parameters["schema"] = LogicalType::VARCHAR;
166: 	dbgen_func.named_parameters["suffix"] = LogicalType::VARCHAR;
167: 	CreateTableFunctionInfo dbgen_info(dbgen_func);
168: 
169: 	// create the dbgen function
170: 	catalog.CreateTableFunction(*con.context, &dbgen_info);
171: 
172: 	// create the TPCH pragma that allows us to run the query
173: 	auto tpch_func = PragmaFunction::PragmaCall("tpch", PragmaTpchQuery, {LogicalType::BIGINT});
174: 	CreatePragmaFunctionInfo info(tpch_func);
175: 	catalog.CreatePragmaFunction(*con.context, &info);
176: 
177: 	// create the TPCH_QUERIES function that returns the query
178: 	TableFunction tpch_query_func("tpch_queries", {}, TPCHQueryFunction, TPCHQueryBind, TPCHInit);
179: 	CreateTableFunctionInfo tpch_query_info(tpch_query_func);
180: 	catalog.CreateTableFunction(*con.context, &tpch_query_info);
181: 
182: 	// create the TPCH_ANSWERS that returns the query result
183: 	TableFunction tpch_query_answer_func("tpch_answers", {}, TPCHQueryAnswerFunction, TPCHQueryAnswerBind, TPCHInit);
184: 	CreateTableFunctionInfo tpch_query_asnwer_info(tpch_query_answer_func);
185: 	catalog.CreateTableFunction(*con.context, &tpch_query_asnwer_info);
186: 
187: 	con.Commit();
188: }
189: 
190: std::string TPCHExtension::GetQuery(int query) {
191: 	return tpch::DBGenWrapper::GetQuery(query);
192: }
193: 
194: std::string TPCHExtension::GetAnswer(double sf, int query) {
195: 	return tpch::DBGenWrapper::GetAnswer(sf, query);
196: }
197: 
198: std::string TPCHExtension::Name() {
199: 	return "tpch";
200: }
201: 
202: } // namespace duckdb
203: 
204: extern "C" {
205: 
206: DUCKDB_EXTENSION_API void tpch_init(duckdb::DatabaseInstance &db) {
207: 	duckdb::DuckDB db_wrapper(db);
208: 	db_wrapper.LoadExtension<duckdb::TPCHExtension>();
209: }
210: 
211: DUCKDB_EXTENSION_API const char *tpch_version() {
212: 	return duckdb::DuckDB::LibraryVersion();
213: }
214: }
[end of extension/tpch/tpch-extension.cpp]
[start of src/execution/operator/persistent/buffered_csv_reader.cpp]
1: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/file_system.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/types/cast_helpers.hpp"
8: #include "duckdb/common/vector_operations/unary_executor.hpp"
9: #include "duckdb/common/vector_operations/vector_operations.hpp"
10: #include "duckdb/function/scalar/strftime.hpp"
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/parser/column_definition.hpp"
13: #include "duckdb/storage/data_table.hpp"
14: #include "utf8proc_wrapper.hpp"
15: #include "utf8proc.hpp"
16: #include "duckdb/parser/keyword_helper.hpp"
17: 
18: #include <algorithm>
19: #include <cctype>
20: #include <cstring>
21: #include <fstream>
22: 
23: namespace duckdb {
24: 
25: struct CSVFileHandle {
26: public:
27: 	explicit CSVFileHandle(unique_ptr<FileHandle> file_handle_p) : file_handle(move(file_handle_p)) {
28: 		can_seek = file_handle->CanSeek();
29: 		plain_file_source = file_handle->OnDiskFile() && can_seek;
30: 		file_size = file_handle->GetFileSize();
31: 	}
32: 
33: 	bool CanSeek() {
34: 		return can_seek;
35: 	}
36: 	void Seek(idx_t position) {
37: 		if (!can_seek) {
38: 			throw InternalException("Cannot seek in this file");
39: 		}
40: 		file_handle->Seek(position);
41: 	}
42: 	idx_t SeekPosition() {
43: 		if (!can_seek) {
44: 			throw InternalException("Cannot seek in this file");
45: 		}
46: 		return file_handle->SeekPosition();
47: 	}
48: 	void Reset() {
49: 		if (plain_file_source) {
50: 			file_handle->Reset();
51: 		} else {
52: 			if (!reset_enabled) {
53: 				throw InternalException("Reset called but reset is not enabled for this CSV Handle");
54: 			}
55: 			read_position = 0;
56: 		}
57: 	}
58: 	bool PlainFileSource() {
59: 		return plain_file_source;
60: 	}
61: 
62: 	idx_t FileSize() {
63: 		return file_size;
64: 	}
65: 
66: 	idx_t Read(void *buffer, idx_t nr_bytes) {
67: 		if (!plain_file_source) {
68: 			// not a plain file source: we need to do some bookkeeping around the reset functionality
69: 			idx_t result_offset = 0;
70: 			if (read_position < buffer_size) {
71: 				// we need to read from our cached buffer
72: 				auto buffer_read_count = MinValue<idx_t>(nr_bytes, buffer_size - read_position);
73: 				memcpy(buffer, cached_buffer.get() + read_position, buffer_read_count);
74: 				result_offset += buffer_read_count;
75: 				read_position += buffer_read_count;
76: 				if (result_offset == nr_bytes) {
77: 					return nr_bytes;
78: 				}
79: 			} else if (!reset_enabled && cached_buffer) {
80: 				// reset is disabled but we still have cached data
81: 				// we can remove any cached data
82: 				cached_buffer.reset();
83: 				buffer_size = 0;
84: 				buffer_capacity = 0;
85: 				read_position = 0;
86: 			}
87: 			// we have data left to read from the file
88: 			// read directly into the buffer
89: 			auto bytes_read = file_handle->Read((char *)buffer + result_offset, nr_bytes - result_offset);
90: 			read_position += bytes_read;
91: 			if (reset_enabled) {
92: 				// if reset caching is enabled, we need to cache the bytes that we have read
93: 				if (buffer_size + bytes_read >= buffer_capacity) {
94: 					// no space; first enlarge the buffer
95: 					buffer_capacity = MaxValue<idx_t>(NextPowerOfTwo(buffer_size + bytes_read), buffer_capacity * 2);
96: 
97: 					auto new_buffer = unique_ptr<data_t[]>(new data_t[buffer_capacity]);
98: 					if (buffer_size > 0) {
99: 						memcpy(new_buffer.get(), cached_buffer.get(), buffer_size);
100: 					}
101: 					cached_buffer = move(new_buffer);
102: 				}
103: 				memcpy(cached_buffer.get() + buffer_size, (char *)buffer + result_offset, bytes_read);
104: 				buffer_size += bytes_read;
105: 			}
106: 
107: 			return result_offset + bytes_read;
108: 		} else {
109: 			return file_handle->Read(buffer, nr_bytes);
110: 		}
111: 	}
112: 
113: 	string ReadLine() {
114: 		string result;
115: 		char buffer[1];
116: 		while (true) {
117: 			idx_t tuples_read = Read(buffer, 1);
118: 			if (tuples_read == 0 || buffer[0] == '\n') {
119: 				return result;
120: 			}
121: 			if (buffer[0] != '\r') {
122: 				result += buffer[0];
123: 			}
124: 		}
125: 	}
126: 
127: 	void DisableReset() {
128: 		this->reset_enabled = false;
129: 	}
130: 
131: private:
132: 	unique_ptr<FileHandle> file_handle;
133: 	bool reset_enabled = true;
134: 	bool can_seek = false;
135: 	bool plain_file_source = false;
136: 	idx_t file_size = 0;
137: 	// reset support
138: 	unique_ptr<data_t[]> cached_buffer;
139: 	idx_t read_position = 0;
140: 	idx_t buffer_size = 0;
141: 	idx_t buffer_capacity = 0;
142: };
143: 
144: void BufferedCSVReaderOptions::SetDelimiter(const string &input) {
145: 	this->delimiter = StringUtil::Replace(input, "\\t", "\t");
146: 	this->has_delimiter = true;
147: 	if (input.empty()) {
148: 		throw BinderException("DELIM or SEP must not be empty");
149: 	}
150: }
151: 
152: std::string BufferedCSVReaderOptions::ToString() const {
153: 	return "DELIMITER='" + delimiter + (has_delimiter ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
154: 	       ", QUOTE='" + quote + (has_quote ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
155: 	       ", ESCAPE='" + escape + (has_escape ? "'" : (auto_detect ? "' (auto detected)" : "' (default)")) +
156: 	       ", HEADER=" + std::to_string(header) +
157: 	       (has_header ? "" : (auto_detect ? " (auto detected)" : "' (default)")) +
158: 	       ", SAMPLE_SIZE=" + std::to_string(sample_chunk_size * sample_chunks) +
159: 	       ", ALL_VARCHAR=" + std::to_string(all_varchar);
160: }
161: 
162: static string GetLineNumberStr(idx_t linenr, bool linenr_estimated) {
163: 	string estimated = (linenr_estimated ? string(" (estimated)") : string(""));
164: 	return to_string(linenr + 1) + estimated;
165: }
166: 
167: static bool StartsWithNumericDate(string &separator, const string &value) {
168: 	auto begin = value.c_str();
169: 	auto end = begin + value.size();
170: 
171: 	//	StrpTimeFormat::Parse will skip whitespace, so we can too
172: 	auto field1 = std::find_if_not(begin, end, StringUtil::CharacterIsSpace);
173: 	if (field1 == end) {
174: 		return false;
175: 	}
176: 
177: 	//	first numeric field must start immediately
178: 	if (!StringUtil::CharacterIsDigit(*field1)) {
179: 		return false;
180: 	}
181: 	auto literal1 = std::find_if_not(field1, end, StringUtil::CharacterIsDigit);
182: 	if (literal1 == end) {
183: 		return false;
184: 	}
185: 
186: 	//	second numeric field must exist
187: 	auto field2 = std::find_if(literal1, end, StringUtil::CharacterIsDigit);
188: 	if (field2 == end) {
189: 		return false;
190: 	}
191: 	auto literal2 = std::find_if_not(field2, end, StringUtil::CharacterIsDigit);
192: 	if (literal2 == end) {
193: 		return false;
194: 	}
195: 
196: 	//	third numeric field must exist
197: 	auto field3 = std::find_if(literal2, end, StringUtil::CharacterIsDigit);
198: 	if (field3 == end) {
199: 		return false;
200: 	}
201: 
202: 	//	second literal must match first
203: 	if (((field3 - literal2) != (field2 - literal1)) || strncmp(literal1, literal2, (field2 - literal1)) != 0) {
204: 		return false;
205: 	}
206: 
207: 	//	copy the literal as the separator, escaping percent signs
208: 	separator.clear();
209: 	while (literal1 < field2) {
210: 		const auto literal_char = *literal1++;
211: 		if (literal_char == '%') {
212: 			separator.push_back(literal_char);
213: 		}
214: 		separator.push_back(literal_char);
215: 	}
216: 
217: 	return true;
218: }
219: 
220: string GenerateDateFormat(const string &separator, const char *format_template) {
221: 	string format_specifier = format_template;
222: 
223: 	//	replace all dashes with the separator
224: 	for (auto pos = std::find(format_specifier.begin(), format_specifier.end(), '-'); pos != format_specifier.end();
225: 	     pos = std::find(pos + separator.size(), format_specifier.end(), '-')) {
226: 		format_specifier.replace(pos, pos + 1, separator);
227: 	}
228: 
229: 	return format_specifier;
230: }
231: 
232: TextSearchShiftArray::TextSearchShiftArray() {
233: }
234: 
235: TextSearchShiftArray::TextSearchShiftArray(string search_term) : length(search_term.size()) {
236: 	if (length > 255) {
237: 		throw Exception("Size of delimiter/quote/escape in CSV reader is limited to 255 bytes");
238: 	}
239: 	// initialize the shifts array
240: 	shifts = unique_ptr<uint8_t[]>(new uint8_t[length * 255]);
241: 	memset(shifts.get(), 0, length * 255 * sizeof(uint8_t));
242: 	// iterate over each of the characters in the array
243: 	for (idx_t main_idx = 0; main_idx < length; main_idx++) {
244: 		uint8_t current_char = (uint8_t)search_term[main_idx];
245: 		// now move over all the remaining positions
246: 		for (idx_t i = main_idx; i < length; i++) {
247: 			bool is_match = true;
248: 			// check if the prefix matches at this position
249: 			// if it does, we move to this position after encountering the current character
250: 			for (idx_t j = 0; j < main_idx; j++) {
251: 				if (search_term[i - main_idx + j] != search_term[j]) {
252: 					is_match = false;
253: 				}
254: 			}
255: 			if (!is_match) {
256: 				continue;
257: 			}
258: 			shifts[i * 255 + current_char] = main_idx + 1;
259: 		}
260: 	}
261: }
262: 
263: BufferedCSVReader::BufferedCSVReader(FileSystem &fs_p, FileOpener *opener_p, BufferedCSVReaderOptions options_p,
264:                                      const vector<LogicalType> &requested_types)
265:     : fs(fs_p), opener(opener_p), options(move(options_p)), buffer_size(0), position(0), start(0) {
266: 	file_handle = OpenCSV(options);
267: 	Initialize(requested_types);
268: }
269: 
270: BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options_p,
271:                                      const vector<LogicalType> &requested_types)
272:     : BufferedCSVReader(FileSystem::GetFileSystem(context), FileSystem::GetFileOpener(context), move(options_p),
273:                         requested_types) {
274: }
275: 
276: BufferedCSVReader::~BufferedCSVReader() {
277: }
278: 
279: idx_t BufferedCSVReader::GetFileSize() {
280: 	return file_handle ? file_handle->FileSize() : 0;
281: }
282: 
283: void BufferedCSVReader::Initialize(const vector<LogicalType> &requested_types) {
284: 	PrepareComplexParser();
285: 	if (options.auto_detect) {
286: 		sql_types = SniffCSV(requested_types);
287: 		if (sql_types.empty()) {
288: 			throw Exception("Failed to detect column types from CSV: is the file a valid CSV file?");
289: 		}
290: 		if (cached_chunks.empty()) {
291: 			JumpToBeginning(options.skip_rows, options.header);
292: 		}
293: 	} else {
294: 		sql_types = requested_types;
295: 		ResetBuffer();
296: 		SkipRowsAndReadHeader(options.skip_rows, options.header);
297: 	}
298: 	InitParseChunk(sql_types.size());
299: 	// we only need reset support during the automatic CSV type detection
300: 	// since reset support might require caching (in the case of streams), we disable it for the remainder
301: 	file_handle->DisableReset();
302: }
303: 
304: void BufferedCSVReader::PrepareComplexParser() {
305: 	delimiter_search = TextSearchShiftArray(options.delimiter);
306: 	escape_search = TextSearchShiftArray(options.escape);
307: 	quote_search = TextSearchShiftArray(options.quote);
308: }
309: 
310: unique_ptr<CSVFileHandle> BufferedCSVReader::OpenCSV(const BufferedCSVReaderOptions &options) {
311: 	auto file_handle = fs.OpenFile(options.file_path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK,
312: 	                               options.compression, this->opener);
313: 	return make_unique<CSVFileHandle>(move(file_handle));
314: }
315: 
316: // Helper function to generate column names
317: static string GenerateColumnName(const idx_t total_cols, const idx_t col_number, const string &prefix = "column") {
318: 	int max_digits = NumericHelper::UnsignedLength(total_cols - 1);
319: 	int digits = NumericHelper::UnsignedLength(col_number);
320: 	string leading_zeros = string(max_digits - digits, '0');
321: 	string value = to_string(col_number);
322: 	return string(prefix + leading_zeros + value);
323: }
324: 
325: // Helper function for UTF-8 aware space trimming
326: static string TrimWhitespace(const string &col_name) {
327: 	utf8proc_int32_t codepoint;
328: 	auto str = reinterpret_cast<const utf8proc_uint8_t *>(col_name.c_str());
329: 	idx_t size = col_name.size();
330: 	// Find the first character that is not left trimmed
331: 	idx_t begin = 0;
332: 	while (begin < size) {
333: 		auto bytes = utf8proc_iterate(str + begin, size - begin, &codepoint);
334: 		D_ASSERT(bytes > 0);
335: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
336: 			break;
337: 		}
338: 		begin += bytes;
339: 	}
340: 
341: 	// Find the last character that is not right trimmed
342: 	idx_t end;
343: 	end = begin;
344: 	for (auto next = begin; next < col_name.size();) {
345: 		auto bytes = utf8proc_iterate(str + next, size - next, &codepoint);
346: 		D_ASSERT(bytes > 0);
347: 		next += bytes;
348: 		if (utf8proc_category(codepoint) != UTF8PROC_CATEGORY_ZS) {
349: 			end = next;
350: 		}
351: 	}
352: 
353: 	// return the trimmed string
354: 	return col_name.substr(begin, end - begin);
355: }
356: 
357: static string NormalizeColumnName(const string &col_name) {
358: 	// normalize UTF8 characters to NFKD
359: 	auto nfkd = utf8proc_NFKD((const utf8proc_uint8_t *)col_name.c_str(), col_name.size());
360: 	const string col_name_nfkd = string((const char *)nfkd, strlen((const char *)nfkd));
361: 	free(nfkd);
362: 
363: 	// only keep ASCII characters 0-9 a-z A-Z and replace spaces with regular whitespace
364: 	string col_name_ascii = "";
365: 	for (idx_t i = 0; i < col_name_nfkd.size(); i++) {
366: 		if (col_name_nfkd[i] == '_' || (col_name_nfkd[i] >= '0' && col_name_nfkd[i] <= '9') ||
367: 		    (col_name_nfkd[i] >= 'A' && col_name_nfkd[i] <= 'Z') ||
368: 		    (col_name_nfkd[i] >= 'a' && col_name_nfkd[i] <= 'z')) {
369: 			col_name_ascii += col_name_nfkd[i];
370: 		} else if (StringUtil::CharacterIsSpace(col_name_nfkd[i])) {
371: 			col_name_ascii += " ";
372: 		}
373: 	}
374: 
375: 	// trim whitespace and replace remaining whitespace by _
376: 	string col_name_trimmed = TrimWhitespace(col_name_ascii);
377: 	string col_name_cleaned = "";
378: 	bool in_whitespace = false;
379: 	for (idx_t i = 0; i < col_name_trimmed.size(); i++) {
380: 		if (col_name_trimmed[i] == ' ') {
381: 			if (!in_whitespace) {
382: 				col_name_cleaned += "_";
383: 				in_whitespace = true;
384: 			}
385: 		} else {
386: 			col_name_cleaned += col_name_trimmed[i];
387: 			in_whitespace = false;
388: 		}
389: 	}
390: 
391: 	// don't leave string empty; if not empty, make lowercase
392: 	if (col_name_cleaned.empty()) {
393: 		col_name_cleaned = "_";
394: 	} else {
395: 		col_name_cleaned = StringUtil::Lower(col_name_cleaned);
396: 	}
397: 
398: 	// prepend _ if name starts with a digit or is a reserved keyword
399: 	if (KeywordHelper::IsKeyword(col_name_cleaned) || (col_name_cleaned[0] >= '0' && col_name_cleaned[0] <= '9')) {
400: 		col_name_cleaned = "_" + col_name_cleaned;
401: 	}
402: 	return col_name_cleaned;
403: }
404: 
405: void BufferedCSVReader::ResetBuffer() {
406: 	buffer.reset();
407: 	buffer_size = 0;
408: 	position = 0;
409: 	start = 0;
410: 	cached_buffers.clear();
411: }
412: 
413: void BufferedCSVReader::ResetStream() {
414: 	if (!file_handle->CanSeek()) {
415: 		// seeking to the beginning appears to not be supported in all compiler/os-scenarios,
416: 		// so we have to create a new stream source here for now
417: 		file_handle->Reset();
418: 	} else {
419: 		file_handle->Seek(0);
420: 	}
421: 	linenr = 0;
422: 	linenr_estimated = false;
423: 	bytes_per_line_avg = 0;
424: 	sample_chunk_idx = 0;
425: 	jumping_samples = false;
426: }
427: 
428: void BufferedCSVReader::InitParseChunk(idx_t num_cols) {
429: 	// adapt not null info
430: 	if (options.force_not_null.size() != num_cols) {
431: 		options.force_not_null.resize(num_cols, false);
432: 	}
433: 	if (num_cols == parse_chunk.ColumnCount()) {
434: 		parse_chunk.Reset();
435: 	} else {
436: 		parse_chunk.Destroy();
437: 
438: 		// initialize the parse_chunk with a set of VARCHAR types
439: 		vector<LogicalType> varchar_types(num_cols, LogicalType::VARCHAR);
440: 		parse_chunk.Initialize(varchar_types);
441: 	}
442: }
443: 
444: void BufferedCSVReader::JumpToBeginning(idx_t skip_rows = 0, bool skip_header = false) {
445: 	ResetBuffer();
446: 	ResetStream();
447: 	SkipRowsAndReadHeader(skip_rows, skip_header);
448: 	sample_chunk_idx = 0;
449: 	bytes_in_chunk = 0;
450: 	end_of_file_reached = false;
451: 	bom_checked = false;
452: }
453: 
454: void BufferedCSVReader::SkipRowsAndReadHeader(idx_t skip_rows, bool skip_header) {
455: 	for (idx_t i = 0; i < skip_rows; i++) {
456: 		// ignore skip rows
457: 		string read_line = file_handle->ReadLine();
458: 		linenr++;
459: 	}
460: 
461: 	if (skip_header) {
462: 		// ignore the first line as a header line
463: 		InitParseChunk(sql_types.size());
464: 		ParseCSV(ParserMode::PARSING_HEADER);
465: 	}
466: }
467: 
468: bool BufferedCSVReader::JumpToNextSample() {
469: 	// get bytes contained in the previously read chunk
470: 	idx_t remaining_bytes_in_buffer = buffer_size - start;
471: 	bytes_in_chunk -= remaining_bytes_in_buffer;
472: 	if (remaining_bytes_in_buffer == 0) {
473: 		return false;
474: 	}
475: 
476: 	// assess if it makes sense to jump, based on size of the first chunk relative to size of the entire file
477: 	if (sample_chunk_idx == 0) {
478: 		idx_t bytes_first_chunk = bytes_in_chunk;
479: 		double chunks_fit = (file_handle->FileSize() / (double)bytes_first_chunk);
480: 		jumping_samples = chunks_fit >= options.sample_chunks;
481: 
482: 		// jump back to the beginning
483: 		JumpToBeginning(options.skip_rows, options.header);
484: 		sample_chunk_idx++;
485: 		return true;
486: 	}
487: 
488: 	if (end_of_file_reached || sample_chunk_idx >= options.sample_chunks) {
489: 		return false;
490: 	}
491: 
492: 	// if we deal with any other sources than plaintext files, jumping_samples can be tricky. In that case
493: 	// we just read x continuous chunks from the stream TODO: make jumps possible for zipfiles.
494: 	if (!file_handle->PlainFileSource() || !jumping_samples) {
495: 		sample_chunk_idx++;
496: 		return true;
497: 	}
498: 
499: 	// update average bytes per line
500: 	double bytes_per_line = bytes_in_chunk / (double)options.sample_chunk_size;
501: 	bytes_per_line_avg = ((bytes_per_line_avg * (sample_chunk_idx)) + bytes_per_line) / (sample_chunk_idx + 1);
502: 
503: 	// if none of the previous conditions were met, we can jump
504: 	idx_t partition_size = (idx_t)round(file_handle->FileSize() / (double)options.sample_chunks);
505: 
506: 	// calculate offset to end of the current partition
507: 	int64_t offset = partition_size - bytes_in_chunk - remaining_bytes_in_buffer;
508: 	auto current_pos = file_handle->SeekPosition();
509: 
510: 	if (current_pos + offset < file_handle->FileSize()) {
511: 		// set position in stream and clear failure bits
512: 		file_handle->Seek(current_pos + offset);
513: 
514: 		// estimate linenr
515: 		linenr += (idx_t)round((offset + remaining_bytes_in_buffer) / bytes_per_line_avg);
516: 		linenr_estimated = true;
517: 	} else {
518: 		// seek backwards from the end in last chunk and hope to catch the end of the file
519: 		// TODO: actually it would be good to make sure that the end of file is being reached, because
520: 		// messy end-lines are quite common. For this case, however, we first need a skip_end detection anyways.
521: 		file_handle->Seek(file_handle->FileSize() - bytes_in_chunk);
522: 
523: 		// estimate linenr
524: 		linenr = (idx_t)round((file_handle->FileSize() - bytes_in_chunk) / bytes_per_line_avg);
525: 		linenr_estimated = true;
526: 	}
527: 
528: 	// reset buffers and parse chunk
529: 	ResetBuffer();
530: 
531: 	// seek beginning of next line
532: 	// FIXME: if this jump ends up in a quoted linebreak, we will have a problem
533: 	string read_line = file_handle->ReadLine();
534: 	linenr++;
535: 
536: 	sample_chunk_idx++;
537: 
538: 	return true;
539: }
540: 
541: void BufferedCSVReader::SetDateFormat(const string &format_specifier, const LogicalTypeId &sql_type) {
542: 	options.has_format[sql_type] = true;
543: 	auto &date_format = options.date_format[sql_type];
544: 	date_format.format_specifier = format_specifier;
545: 	StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
546: }
547: 
548: bool BufferedCSVReader::TryCastValue(const Value &value, const LogicalType &sql_type) {
549: 	if (options.has_format[LogicalTypeId::DATE] && sql_type.id() == LogicalTypeId::DATE) {
550: 		date_t result;
551: 		string error_message;
552: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(string_t(StringValue::Get(value)), result,
553: 		                                                             error_message);
554: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type.id() == LogicalTypeId::TIMESTAMP) {
555: 		timestamp_t result;
556: 		string error_message;
557: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(string_t(StringValue::Get(value)),
558: 		                                                                       result, error_message);
559: 	} else {
560: 		Value new_value;
561: 		string error_message;
562: 		return value.TryCastAs(sql_type, new_value, &error_message, true);
563: 	}
564: }
565: 
566: struct TryCastDateOperator {
567: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, date_t &result, string &error_message) {
568: 		return options.date_format[LogicalTypeId::DATE].TryParseDate(input, result, error_message);
569: 	}
570: };
571: 
572: struct TryCastTimestampOperator {
573: 	static bool Operation(BufferedCSVReaderOptions &options, string_t input, timestamp_t &result,
574: 	                      string &error_message) {
575: 		return options.date_format[LogicalTypeId::TIMESTAMP].TryParseTimestamp(input, result, error_message);
576: 	}
577: };
578: 
579: template <class OP, class T>
580: static bool TemplatedTryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector,
581:                                        idx_t count, string &error_message) {
582: 	D_ASSERT(input_vector.GetType().id() == LogicalTypeId::VARCHAR);
583: 	bool all_converted = true;
584: 	UnaryExecutor::Execute<string_t, T>(input_vector, result_vector, count, [&](string_t input) {
585: 		T result;
586: 		if (!OP::Operation(options, input, result, error_message)) {
587: 			all_converted = false;
588: 		}
589: 		return result;
590: 	});
591: 	return all_converted;
592: }
593: 
594: bool TryCastDateVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
595:                        string &error_message) {
596: 	return TemplatedTryCastDateVector<TryCastDateOperator, date_t>(options, input_vector, result_vector, count,
597: 	                                                               error_message);
598: }
599: 
600: bool TryCastTimestampVector(BufferedCSVReaderOptions &options, Vector &input_vector, Vector &result_vector, idx_t count,
601:                             string &error_message) {
602: 	return TemplatedTryCastDateVector<TryCastTimestampOperator, timestamp_t>(options, input_vector, result_vector,
603: 	                                                                         count, error_message);
604: }
605: 
606: bool BufferedCSVReader::TryCastVector(Vector &parse_chunk_col, idx_t size, const LogicalType &sql_type) {
607: 	// try vector-cast from string to sql_type
608: 	Vector dummy_result(sql_type);
609: 	if (options.has_format[LogicalTypeId::DATE] && sql_type == LogicalTypeId::DATE) {
610: 		// use the date format to cast the chunk
611: 		string error_message;
612: 		return TryCastDateVector(options, parse_chunk_col, dummy_result, size, error_message);
613: 	} else if (options.has_format[LogicalTypeId::TIMESTAMP] && sql_type == LogicalTypeId::TIMESTAMP) {
614: 		// use the timestamp format to cast the chunk
615: 		string error_message;
616: 		return TryCastTimestampVector(options, parse_chunk_col, dummy_result, size, error_message);
617: 	} else {
618: 		// target type is not varchar: perform a cast
619: 		string error_message;
620: 		return VectorOperations::TryCast(parse_chunk_col, dummy_result, size, &error_message, true);
621: 	}
622: }
623: 
624: enum class QuoteRule : uint8_t { QUOTES_RFC = 0, QUOTES_OTHER = 1, NO_QUOTES = 2 };
625: 
626: void BufferedCSVReader::DetectDialect(const vector<LogicalType> &requested_types,
627:                                       BufferedCSVReaderOptions &original_options,
628:                                       vector<BufferedCSVReaderOptions> &info_candidates, idx_t &best_num_cols) {
629: 	// set up the candidates we consider for delimiter and quote rules based on user input
630: 	vector<string> delim_candidates;
631: 	vector<QuoteRule> quoterule_candidates;
632: 	vector<vector<string>> quote_candidates_map;
633: 	vector<vector<string>> escape_candidates_map = {{""}, {"\\"}, {""}};
634: 
635: 	if (options.has_delimiter) {
636: 		// user provided a delimiter: use that delimiter
637: 		delim_candidates = {options.delimiter};
638: 	} else {
639: 		// no delimiter provided: try standard/common delimiters
640: 		delim_candidates = {",", "|", ";", "\t"};
641: 	}
642: 	if (options.has_quote) {
643: 		// user provided quote: use that quote rule
644: 		quote_candidates_map = {{options.quote}, {options.quote}, {options.quote}};
645: 	} else {
646: 		// no quote rule provided: use standard/common quotes
647: 		quote_candidates_map = {{"\""}, {"\"", "'"}, {""}};
648: 	}
649: 	if (options.has_escape) {
650: 		// user provided escape: use that escape rule
651: 		if (options.escape.empty()) {
652: 			quoterule_candidates = {QuoteRule::QUOTES_RFC};
653: 		} else {
654: 			quoterule_candidates = {QuoteRule::QUOTES_OTHER};
655: 		}
656: 		escape_candidates_map[static_cast<uint8_t>(quoterule_candidates[0])] = {options.escape};
657: 	} else {
658: 		// no escape provided: try standard/common escapes
659: 		quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER, QuoteRule::NO_QUOTES};
660: 	}
661: 
662: 	idx_t best_consistent_rows = 0;
663: 	for (auto quoterule : quoterule_candidates) {
664: 		const auto &quote_candidates = quote_candidates_map[static_cast<uint8_t>(quoterule)];
665: 		for (const auto &quote : quote_candidates) {
666: 			for (const auto &delim : delim_candidates) {
667: 				const auto &escape_candidates = escape_candidates_map[static_cast<uint8_t>(quoterule)];
668: 				for (const auto &escape : escape_candidates) {
669: 					BufferedCSVReaderOptions sniff_info = original_options;
670: 					sniff_info.delimiter = delim;
671: 					sniff_info.quote = quote;
672: 					sniff_info.escape = escape;
673: 
674: 					options = sniff_info;
675: 					PrepareComplexParser();
676: 
677: 					JumpToBeginning(original_options.skip_rows);
678: 					sniffed_column_counts.clear();
679: 
680: 					if (!TryParseCSV(ParserMode::SNIFFING_DIALECT)) {
681: 						continue;
682: 					}
683: 
684: 					idx_t start_row = original_options.skip_rows;
685: 					idx_t consistent_rows = 0;
686: 					idx_t num_cols = 0;
687: 
688: 					for (idx_t row = 0; row < sniffed_column_counts.size(); row++) {
689: 						if (sniffed_column_counts[row] == num_cols) {
690: 							consistent_rows++;
691: 						} else {
692: 							num_cols = sniffed_column_counts[row];
693: 							start_row = row + original_options.skip_rows;
694: 							consistent_rows = 1;
695: 						}
696: 					}
697: 
698: 					// some logic
699: 					bool more_values = (consistent_rows > best_consistent_rows && num_cols >= best_num_cols);
700: 					bool single_column_before = best_num_cols < 2 && num_cols > best_num_cols;
701: 					bool rows_consistent =
702: 					    start_row + consistent_rows - original_options.skip_rows == sniffed_column_counts.size();
703: 					bool more_than_one_row = (consistent_rows > 1);
704: 					bool more_than_one_column = (num_cols > 1);
705: 					bool start_good = !info_candidates.empty() && (start_row <= info_candidates.front().skip_rows);
706: 
707: 					if (!requested_types.empty() && requested_types.size() != num_cols) {
708: 						continue;
709: 					} else if ((more_values || single_column_before) && rows_consistent) {
710: 						sniff_info.skip_rows = start_row;
711: 						sniff_info.num_cols = num_cols;
712: 						best_consistent_rows = consistent_rows;
713: 						best_num_cols = num_cols;
714: 
715: 						info_candidates.clear();
716: 						info_candidates.push_back(sniff_info);
717: 					} else if (more_than_one_row && more_than_one_column && start_good && rows_consistent) {
718: 						bool same_quote_is_candidate = false;
719: 						for (auto &info_candidate : info_candidates) {
720: 							if (quote.compare(info_candidate.quote) == 0) {
721: 								same_quote_is_candidate = true;
722: 							}
723: 						}
724: 						if (!same_quote_is_candidate) {
725: 							sniff_info.skip_rows = start_row;
726: 							sniff_info.num_cols = num_cols;
727: 							info_candidates.push_back(sniff_info);
728: 						}
729: 					}
730: 				}
731: 			}
732: 		}
733: 	}
734: }
735: 
736: void BufferedCSVReader::DetectCandidateTypes(const vector<LogicalType> &type_candidates,
737:                                              const map<LogicalTypeId, vector<const char *>> &format_template_candidates,
738:                                              const vector<BufferedCSVReaderOptions> &info_candidates,
739:                                              BufferedCSVReaderOptions &original_options, idx_t best_num_cols,
740:                                              vector<vector<LogicalType>> &best_sql_types_candidates,
741:                                              std::map<LogicalTypeId, vector<string>> &best_format_candidates,
742:                                              DataChunk &best_header_row) {
743: 	BufferedCSVReaderOptions best_options;
744: 	idx_t min_varchar_cols = best_num_cols + 1;
745: 
746: 	// check which info candidate leads to minimum amount of non-varchar columns...
747: 	for (const auto &t : format_template_candidates) {
748: 		best_format_candidates[t.first].clear();
749: 	}
750: 	for (auto &info_candidate : info_candidates) {
751: 		options = info_candidate;
752: 		vector<vector<LogicalType>> info_sql_types_candidates(options.num_cols, type_candidates);
753: 		std::map<LogicalTypeId, bool> has_format_candidates;
754: 		std::map<LogicalTypeId, vector<string>> format_candidates;
755: 		for (const auto &t : format_template_candidates) {
756: 			has_format_candidates[t.first] = false;
757: 			format_candidates[t.first].clear();
758: 		}
759: 
760: 		// set all sql_types to VARCHAR so we can do datatype detection based on VARCHAR values
761: 		sql_types.clear();
762: 		sql_types.assign(options.num_cols, LogicalType::VARCHAR);
763: 
764: 		// jump to beginning and skip potential header
765: 		JumpToBeginning(options.skip_rows, true);
766: 		DataChunk header_row;
767: 		header_row.Initialize(sql_types);
768: 		parse_chunk.Copy(header_row);
769: 
770: 		if (header_row.size() == 0) {
771: 			continue;
772: 		}
773: 
774: 		// init parse chunk and read csv with info candidate
775: 		InitParseChunk(sql_types.size());
776: 		ParseCSV(ParserMode::SNIFFING_DATATYPES);
777: 		for (idx_t row_idx = 0; row_idx <= parse_chunk.size(); row_idx++) {
778: 			bool is_header_row = row_idx == 0;
779: 			idx_t row = row_idx - 1;
780: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
781: 				auto &col_type_candidates = info_sql_types_candidates[col];
782: 				while (col_type_candidates.size() > 1) {
783: 					const auto &sql_type = col_type_candidates.back();
784: 					// try cast from string to sql_type
785: 					Value dummy_val;
786: 					if (is_header_row) {
787: 						dummy_val = header_row.GetValue(col, 0);
788: 					} else {
789: 						dummy_val = parse_chunk.GetValue(col, row);
790: 					}
791: 					// try formatting for date types if the user did not specify one and it starts with numeric values.
792: 					string separator;
793: 					if (has_format_candidates.count(sql_type.id()) && !original_options.has_format[sql_type.id()] &&
794: 					    StartsWithNumericDate(separator, StringValue::Get(dummy_val))) {
795: 						// generate date format candidates the first time through
796: 						auto &type_format_candidates = format_candidates[sql_type.id()];
797: 						const auto had_format_candidates = has_format_candidates[sql_type.id()];
798: 						if (!has_format_candidates[sql_type.id()]) {
799: 							has_format_candidates[sql_type.id()] = true;
800: 							// order by preference
801: 							auto entry = format_template_candidates.find(sql_type.id());
802: 							if (entry != format_template_candidates.end()) {
803: 								const auto &format_template_list = entry->second;
804: 								for (const auto &t : format_template_list) {
805: 									const auto format_string = GenerateDateFormat(separator, t);
806: 									// don't parse ISO 8601
807: 									if (format_string.find("%Y-%m-%d") == string::npos) {
808: 										type_format_candidates.emplace_back(format_string);
809: 									}
810: 								}
811: 							}
812: 							//	initialise the first candidate
813: 							options.has_format[sql_type.id()] = true;
814: 							//	all formats are constructed to be valid
815: 							SetDateFormat(type_format_candidates.back(), sql_type.id());
816: 						}
817: 						// check all formats and keep the first one that works
818: 						StrpTimeFormat::ParseResult result;
819: 						auto save_format_candidates = type_format_candidates;
820: 						while (!type_format_candidates.empty()) {
821: 							//	avoid using exceptions for flow control...
822: 							auto &current_format = options.date_format[sql_type.id()];
823: 							if (current_format.Parse(StringValue::Get(dummy_val), result)) {
824: 								break;
825: 							}
826: 							//	doesn't work - move to the next one
827: 							type_format_candidates.pop_back();
828: 							options.has_format[sql_type.id()] = (!type_format_candidates.empty());
829: 							if (!type_format_candidates.empty()) {
830: 								SetDateFormat(type_format_candidates.back(), sql_type.id());
831: 							}
832: 						}
833: 						//	if none match, then this is not a value of type sql_type,
834: 						if (type_format_candidates.empty()) {
835: 							//	so restore the candidates that did work.
836: 							//	or throw them out if they were generated by this value.
837: 							if (had_format_candidates) {
838: 								type_format_candidates.swap(save_format_candidates);
839: 								if (!type_format_candidates.empty()) {
840: 									SetDateFormat(type_format_candidates.back(), sql_type.id());
841: 								}
842: 							} else {
843: 								has_format_candidates[sql_type.id()] = false;
844: 							}
845: 						}
846: 					}
847: 					// try cast from string to sql_type
848: 					if (TryCastValue(dummy_val, sql_type)) {
849: 						break;
850: 					} else {
851: 						col_type_candidates.pop_back();
852: 					}
853: 				}
854: 			}
855: 			// reset type detection, because first row could be header,
856: 			// but only do it if csv has more than one line (including header)
857: 			if (parse_chunk.size() > 0 && is_header_row) {
858: 				info_sql_types_candidates = vector<vector<LogicalType>>(options.num_cols, type_candidates);
859: 				for (auto &f : format_candidates) {
860: 					f.second.clear();
861: 				}
862: 				for (auto &h : has_format_candidates) {
863: 					h.second = false;
864: 				}
865: 			}
866: 		}
867: 
868: 		idx_t varchar_cols = 0;
869: 		for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
870: 			auto &col_type_candidates = info_sql_types_candidates[col];
871: 			// check number of varchar columns
872: 			const auto &col_type = col_type_candidates.back();
873: 			if (col_type == LogicalType::VARCHAR) {
874: 				varchar_cols++;
875: 			}
876: 		}
877: 
878: 		// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 30% of best_num_cols.
879: 		if (varchar_cols < min_varchar_cols && parse_chunk.ColumnCount() > (best_num_cols * 0.7)) {
880: 			// we have a new best_options candidate
881: 			best_options = info_candidate;
882: 			min_varchar_cols = varchar_cols;
883: 			best_sql_types_candidates = info_sql_types_candidates;
884: 			best_format_candidates = format_candidates;
885: 			best_header_row.Destroy();
886: 			auto header_row_types = header_row.GetTypes();
887: 			best_header_row.Initialize(header_row_types);
888: 			header_row.Copy(best_header_row);
889: 		}
890: 	}
891: 
892: 	options = best_options;
893: 	for (const auto &best : best_format_candidates) {
894: 		if (!best.second.empty()) {
895: 			SetDateFormat(best.second.back(), best.first);
896: 		}
897: 	}
898: }
899: 
900: void BufferedCSVReader::DetectHeader(const vector<vector<LogicalType>> &best_sql_types_candidates,
901:                                      const DataChunk &best_header_row) {
902: 	// information for header detection
903: 	bool first_row_consistent = true;
904: 	bool first_row_nulls = false;
905: 
906: 	// check if header row is all null and/or consistent with detected column data types
907: 	first_row_nulls = true;
908: 	for (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {
909: 		auto dummy_val = best_header_row.GetValue(col, 0);
910: 		if (!dummy_val.IsNull()) {
911: 			first_row_nulls = false;
912: 		}
913: 
914: 		// try cast to sql_type of column
915: 		const auto &sql_type = best_sql_types_candidates[col].back();
916: 		if (!TryCastValue(dummy_val, sql_type)) {
917: 			first_row_consistent = false;
918: 		}
919: 	}
920: 
921: 	// update parser info, and read, generate & set col_names based on previous findings
922: 	if (((!first_row_consistent || first_row_nulls) && !options.has_header) || (options.has_header && options.header)) {
923: 		options.header = true;
924: 		unordered_map<string, idx_t> name_collision_count;
925: 		// get header names from CSV
926: 		for (idx_t col = 0; col < options.num_cols; col++) {
927: 			const auto &val = best_header_row.GetValue(col, 0);
928: 			string col_name = val.ToString();
929: 
930: 			// generate name if field is empty
931: 			if (col_name.empty() || val.IsNull()) {
932: 				col_name = GenerateColumnName(options.num_cols, col);
933: 			}
934: 
935: 			// normalize names or at least trim whitespace
936: 			if (options.normalize_names) {
937: 				col_name = NormalizeColumnName(col_name);
938: 			} else {
939: 				col_name = TrimWhitespace(col_name);
940: 			}
941: 
942: 			// avoid duplicate header names
943: 			const string col_name_raw = col_name;
944: 			while (name_collision_count.find(col_name) != name_collision_count.end()) {
945: 				name_collision_count[col_name] += 1;
946: 				col_name = col_name + "_" + to_string(name_collision_count[col_name]);
947: 			}
948: 
949: 			col_names.push_back(col_name);
950: 			name_collision_count[col_name] = 0;
951: 		}
952: 
953: 	} else {
954: 		options.header = false;
955: 		for (idx_t col = 0; col < options.num_cols; col++) {
956: 			string column_name = GenerateColumnName(options.num_cols, col);
957: 			col_names.push_back(column_name);
958: 		}
959: 	}
960: }
961: 
962: vector<LogicalType> BufferedCSVReader::RefineTypeDetection(const vector<LogicalType> &type_candidates,
963:                                                            const vector<LogicalType> &requested_types,
964:                                                            vector<vector<LogicalType>> &best_sql_types_candidates,
965:                                                            map<LogicalTypeId, vector<string>> &best_format_candidates) {
966: 	// for the type refine we set the SQL types to VARCHAR for all columns
967: 	sql_types.clear();
968: 	sql_types.assign(options.num_cols, LogicalType::VARCHAR);
969: 
970: 	vector<LogicalType> detected_types;
971: 
972: 	// if data types were provided, exit here if number of columns does not match
973: 	if (!requested_types.empty()) {
974: 		if (requested_types.size() != options.num_cols) {
975: 			throw InvalidInputException(
976: 			    "Error while determining column types: found %lld columns but expected %d. (%s)", options.num_cols,
977: 			    requested_types.size(), options.ToString());
978: 		} else {
979: 			detected_types = requested_types;
980: 		}
981: 	} else if (options.all_varchar) {
982: 		// return all types varchar
983: 		detected_types = sql_types;
984: 	} else {
985: 		// jump through the rest of the file and continue to refine the sql type guess
986: 		while (JumpToNextSample()) {
987: 			InitParseChunk(sql_types.size());
988: 			// if jump ends up a bad line, we just skip this chunk
989: 			if (!TryParseCSV(ParserMode::SNIFFING_DATATYPES)) {
990: 				continue;
991: 			}
992: 			for (idx_t col = 0; col < parse_chunk.ColumnCount(); col++) {
993: 				vector<LogicalType> &col_type_candidates = best_sql_types_candidates[col];
994: 				while (col_type_candidates.size() > 1) {
995: 					const auto &sql_type = col_type_candidates.back();
996: 					//	narrow down the date formats
997: 					if (best_format_candidates.count(sql_type.id())) {
998: 						auto &best_type_format_candidates = best_format_candidates[sql_type.id()];
999: 						auto save_format_candidates = best_type_format_candidates;
1000: 						while (!best_type_format_candidates.empty()) {
1001: 							if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
1002: 								break;
1003: 							}
1004: 							//	doesn't work - move to the next one
1005: 							best_type_format_candidates.pop_back();
1006: 							options.has_format[sql_type.id()] = (!best_type_format_candidates.empty());
1007: 							if (!best_type_format_candidates.empty()) {
1008: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
1009: 							}
1010: 						}
1011: 						//	if none match, then this is not a column of type sql_type,
1012: 						if (best_type_format_candidates.empty()) {
1013: 							//	so restore the candidates that did work.
1014: 							best_type_format_candidates.swap(save_format_candidates);
1015: 							if (!best_type_format_candidates.empty()) {
1016: 								SetDateFormat(best_type_format_candidates.back(), sql_type.id());
1017: 							}
1018: 						}
1019: 					}
1020: 
1021: 					if (TryCastVector(parse_chunk.data[col], parse_chunk.size(), sql_type)) {
1022: 						break;
1023: 					} else {
1024: 						col_type_candidates.pop_back();
1025: 					}
1026: 				}
1027: 			}
1028: 
1029: 			if (!jumping_samples) {
1030: 				if ((sample_chunk_idx)*options.sample_chunk_size <= options.buffer_size) {
1031: 					// cache parse chunk
1032: 					// create a new chunk and fill it with the remainder
1033: 					auto chunk = make_unique<DataChunk>();
1034: 					auto parse_chunk_types = parse_chunk.GetTypes();
1035: 					chunk->Move(parse_chunk);
1036: 					cached_chunks.push(move(chunk));
1037: 				} else {
1038: 					while (!cached_chunks.empty()) {
1039: 						cached_chunks.pop();
1040: 					}
1041: 				}
1042: 			}
1043: 		}
1044: 
1045: 		// set sql types
1046: 		for (auto &best_sql_types_candidate : best_sql_types_candidates) {
1047: 			LogicalType d_type = best_sql_types_candidate.back();
1048: 			if (best_sql_types_candidate.size() == type_candidates.size()) {
1049: 				d_type = LogicalType::VARCHAR;
1050: 			}
1051: 			detected_types.push_back(d_type);
1052: 		}
1053: 	}
1054: 
1055: 	return detected_types;
1056: }
1057: 
1058: vector<LogicalType> BufferedCSVReader::SniffCSV(const vector<LogicalType> &requested_types) {
1059: 	for (auto &type : requested_types) {
1060: 		// auto detect for blobs not supported: there may be invalid UTF-8 in the file
1061: 		if (type.id() == LogicalTypeId::BLOB) {
1062: 			return requested_types;
1063: 		}
1064: 	}
1065: 
1066: 	// #######
1067: 	// ### dialect detection
1068: 	// #######
1069: 	BufferedCSVReaderOptions original_options = options;
1070: 	vector<BufferedCSVReaderOptions> info_candidates;
1071: 	idx_t best_num_cols = 0;
1072: 
1073: 	DetectDialect(requested_types, original_options, info_candidates, best_num_cols);
1074: 
1075: 	// if no dialect candidate was found, then file was most likely empty and we throw an exception
1076: 	if (info_candidates.empty()) {
1077: 		throw InvalidInputException(
1078: 		    "Error in file \"%s\": CSV options could not be auto-detected. Consider setting parser options manually.",
1079: 		    options.file_path);
1080: 	}
1081: 
1082: 	// #######
1083: 	// ### type detection (initial)
1084: 	// #######
1085: 	// type candidates, ordered by descending specificity (~ from high to low)
1086: 	vector<LogicalType> type_candidates = {
1087: 	    LogicalType::VARCHAR, LogicalType::TIMESTAMP,
1088: 	    LogicalType::DATE,    LogicalType::TIME,
1089: 	    LogicalType::DOUBLE,  /* LogicalType::FLOAT,*/ LogicalType::BIGINT,
1090: 	    LogicalType::INTEGER, /*LogicalType::SMALLINT, LogicalType::TINYINT,*/ LogicalType::BOOLEAN,
1091: 	    LogicalType::SQLNULL};
1092: 	// format template candidates, ordered by descending specificity (~ from high to low)
1093: 	std::map<LogicalTypeId, vector<const char *>> format_template_candidates = {
1094: 	    {LogicalTypeId::DATE, {"%m-%d-%Y", "%m-%d-%y", "%d-%m-%Y", "%d-%m-%y", "%Y-%m-%d", "%y-%m-%d"}},
1095: 	    {LogicalTypeId::TIMESTAMP,
1096: 	     {"%Y-%m-%d %H:%M:%S.%f", "%m-%d-%Y %I:%M:%S %p", "%m-%d-%y %I:%M:%S %p", "%d-%m-%Y %H:%M:%S",
1097: 	      "%d-%m-%y %H:%M:%S", "%Y-%m-%d %H:%M:%S", "%y-%m-%d %H:%M:%S"}},
1098: 	};
1099: 	vector<vector<LogicalType>> best_sql_types_candidates;
1100: 	map<LogicalTypeId, vector<string>> best_format_candidates;
1101: 	DataChunk best_header_row;
1102: 	DetectCandidateTypes(type_candidates, format_template_candidates, info_candidates, original_options, best_num_cols,
1103: 	                     best_sql_types_candidates, best_format_candidates, best_header_row);
1104: 
1105: 	// #######
1106: 	// ### header detection
1107: 	// #######
1108: 	options.num_cols = best_num_cols;
1109: 	DetectHeader(best_sql_types_candidates, best_header_row);
1110: 
1111: 	// #######
1112: 	// ### type detection (refining)
1113: 	// #######
1114: 	return RefineTypeDetection(type_candidates, requested_types, best_sql_types_candidates, best_format_candidates);
1115: }
1116: 
1117: bool BufferedCSVReader::TryParseComplexCSV(DataChunk &insert_chunk, string &error_message) {
1118: 	// used for parsing algorithm
1119: 	bool finished_chunk = false;
1120: 	idx_t column = 0;
1121: 	vector<idx_t> escape_positions;
1122: 	uint8_t delimiter_pos = 0, escape_pos = 0, quote_pos = 0;
1123: 	idx_t offset = 0;
1124: 
1125: 	// read values into the buffer (if any)
1126: 	if (position >= buffer_size) {
1127: 		if (!ReadBuffer(start)) {
1128: 			return true;
1129: 		}
1130: 	}
1131: 	// start parsing the first value
1132: 	start = position;
1133: 	goto value_start;
1134: value_start:
1135: 	/* state: value_start */
1136: 	// this state parses the first characters of a value
1137: 	offset = 0;
1138: 	delimiter_pos = 0;
1139: 	quote_pos = 0;
1140: 	do {
1141: 		idx_t count = 0;
1142: 		for (; position < buffer_size; position++) {
1143: 			quote_search.Match(quote_pos, buffer[position]);
1144: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1145: 			count++;
1146: 			if (delimiter_pos == options.delimiter.size()) {
1147: 				// found a delimiter, add the value
1148: 				offset = options.delimiter.size() - 1;
1149: 				goto add_value;
1150: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1151: 				// found a newline, add the row
1152: 				goto add_row;
1153: 			}
1154: 			if (count > quote_pos) {
1155: 				// did not find a quote directly at the start of the value, stop looking for the quote now
1156: 				goto normal;
1157: 			}
1158: 			if (quote_pos == options.quote.size()) {
1159: 				// found a quote, go to quoted loop and skip the initial quote
1160: 				start += options.quote.size();
1161: 				goto in_quotes;
1162: 			}
1163: 		}
1164: 	} while (ReadBuffer(start));
1165: 	// file ends while scanning for quote/delimiter, go to final state
1166: 	goto final_state;
1167: normal:
1168: 	/* state: normal parsing state */
1169: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1170: 	position++;
1171: 	do {
1172: 		for (; position < buffer_size; position++) {
1173: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1174: 			if (delimiter_pos == options.delimiter.size()) {
1175: 				offset = options.delimiter.size() - 1;
1176: 				goto add_value;
1177: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1178: 				goto add_row;
1179: 			}
1180: 		}
1181: 	} while (ReadBuffer(start));
1182: 	goto final_state;
1183: add_value:
1184: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1185: 	// increase position by 1 and move start to the new position
1186: 	offset = 0;
1187: 	start = ++position;
1188: 	if (position >= buffer_size && !ReadBuffer(start)) {
1189: 		// file ends right after delimiter, go to final state
1190: 		goto final_state;
1191: 	}
1192: 	goto value_start;
1193: add_row : {
1194: 	// check type of newline (\r or \n)
1195: 	bool carriage_return = buffer[position] == '\r';
1196: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1197: 	finished_chunk = AddRow(insert_chunk, column);
1198: 	// increase position by 1 and move start to the new position
1199: 	offset = 0;
1200: 	start = ++position;
1201: 	if (position >= buffer_size && !ReadBuffer(start)) {
1202: 		// file ends right after newline, go to final state
1203: 		goto final_state;
1204: 	}
1205: 	if (carriage_return) {
1206: 		// \r newline, go to special state that parses an optional \n afterwards
1207: 		goto carriage_return;
1208: 	} else {
1209: 		// \n newline, move to value start
1210: 		if (finished_chunk) {
1211: 			return true;
1212: 		}
1213: 		goto value_start;
1214: 	}
1215: }
1216: in_quotes:
1217: 	/* state: in_quotes */
1218: 	// this state parses the remainder of a quoted value
1219: 	quote_pos = 0;
1220: 	escape_pos = 0;
1221: 	position++;
1222: 	do {
1223: 		for (; position < buffer_size; position++) {
1224: 			quote_search.Match(quote_pos, buffer[position]);
1225: 			escape_search.Match(escape_pos, buffer[position]);
1226: 			if (quote_pos == options.quote.size()) {
1227: 				goto unquote;
1228: 			} else if (escape_pos == options.escape.size()) {
1229: 				escape_positions.push_back(position - start - (options.escape.size() - 1));
1230: 				goto handle_escape;
1231: 			}
1232: 		}
1233: 	} while (ReadBuffer(start));
1234: 	// still in quoted state at the end of the file, error:
1235: 	error_message = StringUtil::Format("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1236: 	                                   GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1237: 	return false;
1238: unquote:
1239: 	/* state: unquote */
1240: 	// this state handles the state directly after we unquote
1241: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1242: 	// or a delimiter/newline, ending the current value and moving on to the next value
1243: 	delimiter_pos = 0;
1244: 	quote_pos = 0;
1245: 	position++;
1246: 	if (position >= buffer_size && !ReadBuffer(start)) {
1247: 		// file ends right after unquote, go to final state
1248: 		offset = options.quote.size();
1249: 		goto final_state;
1250: 	}
1251: 	if (StringUtil::CharacterIsNewline(buffer[position])) {
1252: 		// quote followed by newline, add row
1253: 		offset = options.quote.size();
1254: 		goto add_row;
1255: 	}
1256: 	do {
1257: 		idx_t count = 0;
1258: 		for (; position < buffer_size; position++) {
1259: 			quote_search.Match(quote_pos, buffer[position]);
1260: 			delimiter_search.Match(delimiter_pos, buffer[position]);
1261: 			count++;
1262: 			if (count > delimiter_pos && count > quote_pos) {
1263: 				error_message = StringUtil::Format(
1264: 				    "Error in file \"%s\" on line %s: quote should be followed by end of value, end "
1265: 				    "of row or another quote. (%s)",
1266: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1267: 				return false;
1268: 			}
1269: 			if (delimiter_pos == options.delimiter.size()) {
1270: 				// quote followed by delimiter, add value
1271: 				offset = options.quote.size() + options.delimiter.size() - 1;
1272: 				goto add_value;
1273: 			} else if (quote_pos == options.quote.size() &&
1274: 			           (options.escape.empty() || options.escape == options.quote)) {
1275: 				// quote followed by quote, go back to quoted state and add to escape
1276: 				escape_positions.push_back(position - start - (options.quote.size() - 1));
1277: 				goto in_quotes;
1278: 			}
1279: 		}
1280: 	} while (ReadBuffer(start));
1281: 	error_message = StringUtil::Format(
1282: 	    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of row or another quote. (%s)",
1283: 	    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1284: 	return false;
1285: handle_escape:
1286: 	escape_pos = 0;
1287: 	quote_pos = 0;
1288: 	position++;
1289: 	do {
1290: 		idx_t count = 0;
1291: 		for (; position < buffer_size; position++) {
1292: 			quote_search.Match(quote_pos, buffer[position]);
1293: 			escape_search.Match(escape_pos, buffer[position]);
1294: 			count++;
1295: 			if (count > escape_pos && count > quote_pos) {
1296: 				error_message = StringUtil::Format(
1297: 				    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1298: 				    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1299: 				return false;
1300: 			}
1301: 			if (quote_pos == options.quote.size() || escape_pos == options.escape.size()) {
1302: 				// found quote or escape: move back to quoted state
1303: 				goto in_quotes;
1304: 			}
1305: 		}
1306: 	} while (ReadBuffer(start));
1307: 	error_message =
1308: 	    StringUtil::Format("Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)",
1309: 	                       options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1310: 	return false;
1311: carriage_return:
1312: 	/* state: carriage_return */
1313: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1314: 	if (buffer[position] == '\n') {
1315: 		// newline after carriage return: skip
1316: 		start = ++position;
1317: 		if (position >= buffer_size && !ReadBuffer(start)) {
1318: 			// file ends right after newline, go to final state
1319: 			goto final_state;
1320: 		}
1321: 	}
1322: 	if (finished_chunk) {
1323: 		return true;
1324: 	}
1325: 	goto value_start;
1326: final_state:
1327: 	if (finished_chunk) {
1328: 		return true;
1329: 	}
1330: 	if (column > 0 || position > start) {
1331: 		// remaining values to be added to the chunk
1332: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1333: 		finished_chunk = AddRow(insert_chunk, column);
1334: 	}
1335: 	// final stage, only reached after parsing the file is finished
1336: 	// flush the parsed chunk and finalize parsing
1337: 	if (mode == ParserMode::PARSING) {
1338: 		Flush(insert_chunk);
1339: 	}
1340: 
1341: 	end_of_file_reached = true;
1342: 	return true;
1343: }
1344: 
1345: bool BufferedCSVReader::TryParseSimpleCSV(DataChunk &insert_chunk, string &error_message) {
1346: 	// used for parsing algorithm
1347: 	bool finished_chunk = false;
1348: 	idx_t column = 0;
1349: 	idx_t offset = 0;
1350: 	vector<idx_t> escape_positions;
1351: 
1352: 	// read values into the buffer (if any)
1353: 	if (position >= buffer_size) {
1354: 		if (!ReadBuffer(start)) {
1355: 			return true;
1356: 		}
1357: 	}
1358: 	// start parsing the first value
1359: 	goto value_start;
1360: value_start:
1361: 	offset = 0;
1362: 	/* state: value_start */
1363: 	// this state parses the first character of a value
1364: 	if (buffer[position] == options.quote[0]) {
1365: 		// quote: actual value starts in the next position
1366: 		// move to in_quotes state
1367: 		start = position + 1;
1368: 		goto in_quotes;
1369: 	} else {
1370: 		// no quote, move to normal parsing state
1371: 		start = position;
1372: 		goto normal;
1373: 	}
1374: normal:
1375: 	/* state: normal parsing state */
1376: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
1377: 	do {
1378: 		for (; position < buffer_size; position++) {
1379: 			if (buffer[position] == options.delimiter[0]) {
1380: 				// delimiter: end the value and add it to the chunk
1381: 				goto add_value;
1382: 			} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1383: 				// newline: add row
1384: 				goto add_row;
1385: 			}
1386: 		}
1387: 	} while (ReadBuffer(start));
1388: 	// file ends during normal scan: go to end state
1389: 	goto final_state;
1390: add_value:
1391: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1392: 	// increase position by 1 and move start to the new position
1393: 	offset = 0;
1394: 	start = ++position;
1395: 	if (position >= buffer_size && !ReadBuffer(start)) {
1396: 		// file ends right after delimiter, go to final state
1397: 		goto final_state;
1398: 	}
1399: 	goto value_start;
1400: add_row : {
1401: 	// check type of newline (\r or \n)
1402: 	bool carriage_return = buffer[position] == '\r';
1403: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1404: 	finished_chunk = AddRow(insert_chunk, column);
1405: 	// increase position by 1 and move start to the new position
1406: 	offset = 0;
1407: 	start = ++position;
1408: 	if (position >= buffer_size && !ReadBuffer(start)) {
1409: 		// file ends right after delimiter, go to final state
1410: 		goto final_state;
1411: 	}
1412: 	if (carriage_return) {
1413: 		// \r newline, go to special state that parses an optional \n afterwards
1414: 		goto carriage_return;
1415: 	} else {
1416: 		// \n newline, move to value start
1417: 		if (finished_chunk) {
1418: 			return true;
1419: 		}
1420: 		goto value_start;
1421: 	}
1422: }
1423: in_quotes:
1424: 	/* state: in_quotes */
1425: 	// this state parses the remainder of a quoted value
1426: 	position++;
1427: 	do {
1428: 		for (; position < buffer_size; position++) {
1429: 			if (buffer[position] == options.quote[0]) {
1430: 				// quote: move to unquoted state
1431: 				goto unquote;
1432: 			} else if (buffer[position] == options.escape[0]) {
1433: 				// escape: store the escaped position and move to handle_escape state
1434: 				escape_positions.push_back(position - start);
1435: 				goto handle_escape;
1436: 			}
1437: 		}
1438: 	} while (ReadBuffer(start));
1439: 	// still in quoted state at the end of the file, error:
1440: 	throw InvalidInputException("Error in file \"%s\" on line %s: unterminated quotes. (%s)", options.file_path,
1441: 	                            GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1442: unquote:
1443: 	/* state: unquote */
1444: 	// this state handles the state directly after we unquote
1445: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
1446: 	// or a delimiter/newline, ending the current value and moving on to the next value
1447: 	position++;
1448: 	if (position >= buffer_size && !ReadBuffer(start)) {
1449: 		// file ends right after unquote, go to final state
1450: 		offset = 1;
1451: 		goto final_state;
1452: 	}
1453: 	if (buffer[position] == options.quote[0] && (options.escape.empty() || options.escape[0] == options.quote[0])) {
1454: 		// escaped quote, return to quoted state and store escape position
1455: 		escape_positions.push_back(position - start);
1456: 		goto in_quotes;
1457: 	} else if (buffer[position] == options.delimiter[0]) {
1458: 		// delimiter, add value
1459: 		offset = 1;
1460: 		goto add_value;
1461: 	} else if (StringUtil::CharacterIsNewline(buffer[position])) {
1462: 		offset = 1;
1463: 		goto add_row;
1464: 	} else {
1465: 		error_message = StringUtil::Format(
1466: 		    "Error in file \"%s\" on line %s: quote should be followed by end of value, end of "
1467: 		    "row or another quote. (%s)",
1468: 		    options.file_path, GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1469: 		return false;
1470: 	}
1471: handle_escape:
1472: 	/* state: handle_escape */
1473: 	// escape should be followed by a quote or another escape character
1474: 	position++;
1475: 	if (position >= buffer_size && !ReadBuffer(start)) {
1476: 		error_message = StringUtil::Format(
1477: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1478: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1479: 		return false;
1480: 	}
1481: 	if (buffer[position] != options.quote[0] && buffer[position] != options.escape[0]) {
1482: 		error_message = StringUtil::Format(
1483: 		    "Error in file \"%s\" on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE. (%s)", options.file_path,
1484: 		    GetLineNumberStr(linenr, linenr_estimated).c_str(), options.ToString());
1485: 		return false;
1486: 	}
1487: 	// escape was followed by quote or escape, go back to quoted state
1488: 	goto in_quotes;
1489: carriage_return:
1490: 	/* state: carriage_return */
1491: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
1492: 	if (buffer[position] == '\n') {
1493: 		// newline after carriage return: skip
1494: 		// increase position by 1 and move start to the new position
1495: 		start = ++position;
1496: 		if (position >= buffer_size && !ReadBuffer(start)) {
1497: 			// file ends right after delimiter, go to final state
1498: 			goto final_state;
1499: 		}
1500: 	}
1501: 	if (finished_chunk) {
1502: 		return true;
1503: 	}
1504: 	goto value_start;
1505: final_state:
1506: 	if (finished_chunk) {
1507: 		return true;
1508: 	}
1509: 
1510: 	if (column > 0 || position > start) {
1511: 		// remaining values to be added to the chunk
1512: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
1513: 		finished_chunk = AddRow(insert_chunk, column);
1514: 	}
1515: 
1516: 	// final stage, only reached after parsing the file is finished
1517: 	// flush the parsed chunk and finalize parsing
1518: 	if (mode == ParserMode::PARSING) {
1519: 		Flush(insert_chunk);
1520: 	}
1521: 
1522: 	end_of_file_reached = true;
1523: 	return true;
1524: }
1525: 
1526: bool BufferedCSVReader::ReadBuffer(idx_t &start) {
1527: 	auto old_buffer = move(buffer);
1528: 
1529: 	// the remaining part of the last buffer
1530: 	idx_t remaining = buffer_size - start;
1531: 	idx_t buffer_read_size = INITIAL_BUFFER_SIZE;
1532: 	while (remaining > buffer_read_size) {
1533: 		buffer_read_size *= 2;
1534: 	}
1535: 	if (remaining + buffer_read_size > MAXIMUM_CSV_LINE_SIZE) {
1536: 		throw InvalidInputException("Maximum line size of %llu bytes exceeded!", MAXIMUM_CSV_LINE_SIZE);
1537: 	}
1538: 	buffer = unique_ptr<char[]>(new char[buffer_read_size + remaining + 1]);
1539: 	buffer_size = remaining + buffer_read_size;
1540: 	if (remaining > 0) {
1541: 		// remaining from last buffer: copy it here
1542: 		memcpy(buffer.get(), old_buffer.get() + start, remaining);
1543: 	}
1544: 	idx_t read_count = file_handle->Read(buffer.get() + remaining, buffer_read_size);
1545: 
1546: 	bytes_in_chunk += read_count;
1547: 	buffer_size = remaining + read_count;
1548: 	buffer[buffer_size] = '\0';
1549: 	if (old_buffer) {
1550: 		cached_buffers.push_back(move(old_buffer));
1551: 	}
1552: 	start = 0;
1553: 	position = remaining;
1554: 	if (!bom_checked) {
1555: 		bom_checked = true;
1556: 		if (read_count >= 3 && buffer[0] == '\xEF' && buffer[1] == '\xBB' && buffer[2] == '\xBF') {
1557: 			position += 3;
1558: 		}
1559: 	}
1560: 
1561: 	return read_count > 0;
1562: }
1563: 
1564: void BufferedCSVReader::ParseCSV(DataChunk &insert_chunk) {
1565: 	// if no auto-detect or auto-detect with jumping samples, we have nothing cached and start from the beginning
1566: 	if (cached_chunks.empty()) {
1567: 		cached_buffers.clear();
1568: 	} else {
1569: 		auto &chunk = cached_chunks.front();
1570: 		parse_chunk.Move(*chunk);
1571: 		cached_chunks.pop();
1572: 		Flush(insert_chunk);
1573: 		return;
1574: 	}
1575: 
1576: 	string error_message;
1577: 	if (!TryParseCSV(ParserMode::PARSING, insert_chunk, error_message)) {
1578: 		throw InvalidInputException(error_message);
1579: 	}
1580: }
1581: 
1582: bool BufferedCSVReader::TryParseCSV(ParserMode mode) {
1583: 	DataChunk dummy_chunk;
1584: 	string error_message;
1585: 	return TryParseCSV(mode, dummy_chunk, error_message);
1586: }
1587: 
1588: void BufferedCSVReader::ParseCSV(ParserMode mode) {
1589: 	DataChunk dummy_chunk;
1590: 	string error_message;
1591: 	if (!TryParseCSV(mode, dummy_chunk, error_message)) {
1592: 		throw InvalidInputException(error_message);
1593: 	}
1594: }
1595: 
1596: bool BufferedCSVReader::TryParseCSV(ParserMode parser_mode, DataChunk &insert_chunk, string &error_message) {
1597: 	mode = parser_mode;
1598: 
1599: 	if (options.quote.size() <= 1 && options.escape.size() <= 1 && options.delimiter.size() == 1) {
1600: 		return TryParseSimpleCSV(insert_chunk, error_message);
1601: 	} else {
1602: 		return TryParseComplexCSV(insert_chunk, error_message);
1603: 	}
1604: }
1605: 
1606: void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vector<idx_t> &escape_positions) {
1607: 	if (length == 0 && column == 0) {
1608: 		row_empty = true;
1609: 	} else {
1610: 		row_empty = false;
1611: 	}
1612: 
1613: 	if (!sql_types.empty() && column == sql_types.size() && length == 0) {
1614: 		// skip a single trailing delimiter in last column
1615: 		return;
1616: 	}
1617: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1618: 		column++;
1619: 		return;
1620: 	}
1621: 	if (column >= sql_types.size()) {
1622: 		throw InvalidInputException("Error on line %s: expected %lld values per row, but got more. (%s)",
1623: 		                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(),
1624: 		                            options.ToString());
1625: 	}
1626: 
1627: 	// insert the line number into the chunk
1628: 	idx_t row_entry = parse_chunk.size();
1629: 
1630: 	str_val[length] = '\0';
1631: 
1632: 	// test against null string
1633: 	if (!options.force_not_null[column] && strcmp(options.null_str.c_str(), str_val) == 0) {
1634: 		FlatVector::SetNull(parse_chunk.data[column], row_entry, true);
1635: 	} else {
1636: 		auto &v = parse_chunk.data[column];
1637: 		auto parse_data = FlatVector::GetData<string_t>(v);
1638: 		if (!escape_positions.empty()) {
1639: 			// remove escape characters (if any)
1640: 			string old_val = str_val;
1641: 			string new_val = "";
1642: 			idx_t prev_pos = 0;
1643: 			for (idx_t i = 0; i < escape_positions.size(); i++) {
1644: 				idx_t next_pos = escape_positions[i];
1645: 				new_val += old_val.substr(prev_pos, next_pos - prev_pos);
1646: 
1647: 				if (options.escape.empty() || options.escape == options.quote) {
1648: 					prev_pos = next_pos + options.quote.size();
1649: 				} else {
1650: 					prev_pos = next_pos + options.escape.size();
1651: 				}
1652: 			}
1653: 			new_val += old_val.substr(prev_pos, old_val.size() - prev_pos);
1654: 			escape_positions.clear();
1655: 			parse_data[row_entry] = StringVector::AddStringOrBlob(v, string_t(new_val));
1656: 		} else {
1657: 			parse_data[row_entry] = string_t(str_val, length);
1658: 		}
1659: 	}
1660: 
1661: 	// move to the next column
1662: 	column++;
1663: }
1664: 
1665: bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {
1666: 	linenr++;
1667: 
1668: 	if (row_empty) {
1669: 		row_empty = false;
1670: 		if (sql_types.size() != 1) {
1671: 			column = 0;
1672: 			return false;
1673: 		}
1674: 	}
1675: 
1676: 	if (column < sql_types.size() && mode != ParserMode::SNIFFING_DIALECT) {
1677: 		throw InvalidInputException("Error on line %s: expected %lld values per row, but got %d. (%s)",
1678: 		                            GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column,
1679: 		                            options.ToString());
1680: 	}
1681: 
1682: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1683: 		sniffed_column_counts.push_back(column);
1684: 
1685: 		if (sniffed_column_counts.size() == options.sample_chunk_size) {
1686: 			return true;
1687: 		}
1688: 	} else {
1689: 		parse_chunk.SetCardinality(parse_chunk.size() + 1);
1690: 	}
1691: 
1692: 	if (mode == ParserMode::PARSING_HEADER) {
1693: 		return true;
1694: 	}
1695: 
1696: 	if (mode == ParserMode::SNIFFING_DATATYPES && parse_chunk.size() == options.sample_chunk_size) {
1697: 		return true;
1698: 	}
1699: 
1700: 	if (mode == ParserMode::PARSING && parse_chunk.size() == STANDARD_VECTOR_SIZE) {
1701: 		Flush(insert_chunk);
1702: 		return true;
1703: 	}
1704: 
1705: 	column = 0;
1706: 	return false;
1707: }
1708: 
1709: void BufferedCSVReader::Flush(DataChunk &insert_chunk) {
1710: 	if (parse_chunk.size() == 0) {
1711: 		return;
1712: 	}
1713: 	// convert the columns in the parsed chunk to the types of the table
1714: 	insert_chunk.SetCardinality(parse_chunk);
1715: 	for (idx_t col_idx = 0; col_idx < sql_types.size(); col_idx++) {
1716: 		if (sql_types[col_idx].id() == LogicalTypeId::VARCHAR) {
1717: 			// target type is varchar: no need to convert
1718: 			// just test that all strings are valid utf-8 strings
1719: 			auto parse_data = FlatVector::GetData<string_t>(parse_chunk.data[col_idx]);
1720: 			for (idx_t i = 0; i < parse_chunk.size(); i++) {
1721: 				if (!FlatVector::IsNull(parse_chunk.data[col_idx], i)) {
1722: 					auto s = parse_data[i];
1723: 					auto utf_type = Utf8Proc::Analyze(s.GetDataUnsafe(), s.GetSize());
1724: 					if (utf_type == UnicodeType::INVALID) {
1725: 						string col_name = to_string(col_idx);
1726: 						if (col_idx < col_names.size()) {
1727: 							col_name = "\"" + col_names[col_idx] + "\"";
1728: 						}
1729: 						throw InvalidInputException("Error in file \"%s\" between line %llu and %llu in column \"%s\": "
1730: 						                            "file is not valid UTF8. Parser options: %s",
1731: 						                            options.file_path, linenr - parse_chunk.size(), linenr, col_name,
1732: 						                            options.ToString());
1733: 					}
1734: 				}
1735: 			}
1736: 			insert_chunk.data[col_idx].Reference(parse_chunk.data[col_idx]);
1737: 		} else {
1738: 			string error_message;
1739: 			bool success;
1740: 			if (options.has_format[LogicalTypeId::DATE] && sql_types[col_idx].id() == LogicalTypeId::DATE) {
1741: 				// use the date format to cast the chunk
1742: 				success = TryCastDateVector(options, parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1743: 				                            parse_chunk.size(), error_message);
1744: 			} else if (options.has_format[LogicalTypeId::TIMESTAMP] &&
1745: 			           sql_types[col_idx].id() == LogicalTypeId::TIMESTAMP) {
1746: 				// use the date format to cast the chunk
1747: 				success = TryCastTimestampVector(options, parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1748: 				                                 parse_chunk.size(), error_message);
1749: 			} else {
1750: 				// target type is not varchar: perform a cast
1751: 				success = VectorOperations::TryCast(parse_chunk.data[col_idx], insert_chunk.data[col_idx],
1752: 				                                    parse_chunk.size(), &error_message);
1753: 			}
1754: 			if (!success) {
1755: 				string col_name = to_string(col_idx);
1756: 				if (col_idx < col_names.size()) {
1757: 					col_name = "\"" + col_names[col_idx] + "\"";
1758: 				}
1759: 
1760: 				if (options.auto_detect) {
1761: 					throw InvalidInputException("%s in column %s, between line %llu and %llu. Parser "
1762: 					                            "options: %s. Consider either increasing the sample size "
1763: 					                            "(SAMPLE_SIZE=X [X rows] or SAMPLE_SIZE=-1 [all rows]), "
1764: 					                            "or skipping column conversion (ALL_VARCHAR=1)",
1765: 					                            error_message, col_name, linenr - parse_chunk.size() + 1, linenr,
1766: 					                            options.ToString());
1767: 				} else {
1768: 					throw InvalidInputException("%s between line %llu and %llu in column %s. Parser options: %s ",
1769: 					                            error_message, linenr - parse_chunk.size(), linenr, col_name,
1770: 					                            options.ToString());
1771: 				}
1772: 			}
1773: 		}
1774: 	}
1775: 	parse_chunk.Reset();
1776: }
1777: } // namespace duckdb
[end of src/execution/operator/persistent/buffered_csv_reader.cpp]
[start of src/function/function.cpp]
1: #include "duckdb/function/function.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
5: #include "duckdb/common/types/hash.hpp"
6: #include "duckdb/common/limits.hpp"
7: #include "duckdb/common/string_util.hpp"
8: #include "duckdb/function/aggregate_function.hpp"
9: #include "duckdb/function/cast_rules.hpp"
10: #include "duckdb/function/scalar/string_functions.hpp"
11: #include "duckdb/function/scalar_function.hpp"
12: #include "duckdb/parser/parsed_data/create_aggregate_function_info.hpp"
13: #include "duckdb/parser/parsed_data/create_collation_info.hpp"
14: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
15: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
16: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
17: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
18: #include "duckdb/parser/parsed_data/pragma_info.hpp"
19: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
20: #include "duckdb/planner/expression/bound_cast_expression.hpp"
21: #include "duckdb/planner/expression/bound_function_expression.hpp"
22: #include "duckdb/planner/expression_binder.hpp"
23: 
24: namespace duckdb {
25: 
26: FunctionData::~FunctionData() {
27: }
28: 
29: unique_ptr<FunctionData> FunctionData::Copy() {
30: 	throw InternalException("Unimplemented copy for FunctionData");
31: }
32: 
33: bool FunctionData::Equals(FunctionData &other) {
34: 	return true;
35: }
36: 
37: bool FunctionData::Equals(FunctionData *left, FunctionData *right) {
38: 	if (left == right) {
39: 		return true;
40: 	}
41: 	if (!left || !right) {
42: 		return false;
43: 	}
44: 	return left->Equals(*right);
45: }
46: 
47: Function::Function(string name_p) : name(move(name_p)) {
48: }
49: Function::~Function() {
50: }
51: 
52: SimpleFunction::SimpleFunction(string name_p, vector<LogicalType> arguments_p, LogicalType varargs_p)
53:     : Function(move(name_p)), arguments(move(arguments_p)), varargs(move(varargs_p)) {
54: }
55: 
56: SimpleFunction::~SimpleFunction() {
57: }
58: 
59: string SimpleFunction::ToString() {
60: 	return Function::CallToString(name, arguments);
61: }
62: 
63: bool SimpleFunction::HasVarArgs() const {
64: 	return varargs.id() != LogicalTypeId::INVALID;
65: }
66: 
67: SimpleNamedParameterFunction::SimpleNamedParameterFunction(string name_p, vector<LogicalType> arguments_p,
68:                                                            LogicalType varargs_p)
69:     : SimpleFunction(move(name_p), move(arguments_p), move(varargs_p)) {
70: }
71: 
72: SimpleNamedParameterFunction::~SimpleNamedParameterFunction() {
73: }
74: 
75: string SimpleNamedParameterFunction::ToString() {
76: 	return Function::CallToString(name, arguments, named_parameters);
77: }
78: 
79: bool SimpleNamedParameterFunction::HasNamedParameters() {
80: 	return !named_parameters.empty();
81: }
82: 
83: BaseScalarFunction::BaseScalarFunction(string name_p, vector<LogicalType> arguments_p, LogicalType return_type_p,
84:                                        bool has_side_effects, LogicalType varargs_p)
85:     : SimpleFunction(move(name_p), move(arguments_p), move(varargs_p)), return_type(move(return_type_p)),
86:       has_side_effects(has_side_effects) {
87: }
88: 
89: BaseScalarFunction::~BaseScalarFunction() {
90: }
91: 
92: string BaseScalarFunction::ToString() {
93: 	return Function::CallToString(name, arguments, return_type);
94: }
95: 
96: // add your initializer for new functions here
97: void BuiltinFunctions::Initialize() {
98: 	RegisterSQLiteFunctions();
99: 	RegisterReadFunctions();
100: 	RegisterTableFunctions();
101: 	RegisterArrowFunctions();
102: 
103: 	RegisterAlgebraicAggregates();
104: 	RegisterDistributiveAggregates();
105: 	RegisterNestedAggregates();
106: 	RegisterHolisticAggregates();
107: 	RegisterRegressiveAggregates();
108: 
109: 	RegisterDateFunctions();
110: 	RegisterEnumFunctions();
111: 	RegisterGenericFunctions();
112: 	RegisterMathFunctions();
113: 	RegisterOperators();
114: 	RegisterSequenceFunctions();
115: 	RegisterStringFunctions();
116: 	RegisterNestedFunctions();
117: 	RegisterTrigonometricsFunctions();
118: 
119: 	RegisterPragmaFunctions();
120: 
121: 	// initialize collations
122: 	AddCollation("nocase", LowerFun::GetFunction(), true);
123: 	AddCollation("noaccent", StripAccentsFun::GetFunction());
124: 	AddCollation("nfc", NFCNormalizeFun::GetFunction());
125: }
126: 
127: BuiltinFunctions::BuiltinFunctions(ClientContext &context, Catalog &catalog) : context(context), catalog(catalog) {
128: }
129: 
130: void BuiltinFunctions::AddCollation(string name, ScalarFunction function, bool combinable,
131:                                     bool not_required_for_equality) {
132: 	CreateCollationInfo info(move(name), move(function), combinable, not_required_for_equality);
133: 	catalog.CreateCollation(context, &info);
134: }
135: 
136: void BuiltinFunctions::AddFunction(AggregateFunctionSet set) {
137: 	CreateAggregateFunctionInfo info(move(set));
138: 	catalog.CreateFunction(context, &info);
139: }
140: 
141: void BuiltinFunctions::AddFunction(AggregateFunction function) {
142: 	CreateAggregateFunctionInfo info(move(function));
143: 	catalog.CreateFunction(context, &info);
144: }
145: 
146: void BuiltinFunctions::AddFunction(PragmaFunction function) {
147: 	CreatePragmaFunctionInfo info(move(function));
148: 	catalog.CreatePragmaFunction(context, &info);
149: }
150: 
151: void BuiltinFunctions::AddFunction(const string &name, vector<PragmaFunction> functions) {
152: 	CreatePragmaFunctionInfo info(name, move(functions));
153: 	catalog.CreatePragmaFunction(context, &info);
154: }
155: 
156: void BuiltinFunctions::AddFunction(ScalarFunction function) {
157: 	CreateScalarFunctionInfo info(move(function));
158: 	catalog.CreateFunction(context, &info);
159: }
160: 
161: void BuiltinFunctions::AddFunction(const vector<string> &names, ScalarFunction function) { // NOLINT: false positive
162: 	for (auto &name : names) {
163: 		function.name = name;
164: 		AddFunction(function);
165: 	}
166: }
167: 
168: void BuiltinFunctions::AddFunction(ScalarFunctionSet set) {
169: 	CreateScalarFunctionInfo info(move(set));
170: 	catalog.CreateFunction(context, &info);
171: }
172: 
173: void BuiltinFunctions::AddFunction(TableFunction function) {
174: 	CreateTableFunctionInfo info(move(function));
175: 	catalog.CreateTableFunction(context, &info);
176: }
177: 
178: void BuiltinFunctions::AddFunction(TableFunctionSet set) {
179: 	CreateTableFunctionInfo info(move(set));
180: 	catalog.CreateTableFunction(context, &info);
181: }
182: 
183: void BuiltinFunctions::AddFunction(CopyFunction function) {
184: 	CreateCopyFunctionInfo info(move(function));
185: 	catalog.CreateCopyFunction(context, &info);
186: }
187: 
188: hash_t BaseScalarFunction::Hash() const {
189: 	hash_t hash = return_type.Hash();
190: 	for (auto &arg : arguments) {
191: 		duckdb::CombineHash(hash, arg.Hash());
192: 	}
193: 	return hash;
194: }
195: 
196: string Function::CallToString(const string &name, const vector<LogicalType> &arguments) {
197: 	string result = name + "(";
198: 	result += StringUtil::Join(arguments, arguments.size(), ", ",
199: 	                           [](const LogicalType &argument) { return argument.ToString(); });
200: 	return result + ")";
201: }
202: 
203: string Function::CallToString(const string &name, const vector<LogicalType> &arguments,
204:                               const LogicalType &return_type) {
205: 	string result = CallToString(name, arguments);
206: 	result += " -> " + return_type.ToString();
207: 	return result;
208: }
209: 
210: string Function::CallToString(const string &name, const vector<LogicalType> &arguments,
211:                               const unordered_map<string, LogicalType> &named_parameters) {
212: 	vector<string> input_arguments;
213: 	input_arguments.reserve(arguments.size() + named_parameters.size());
214: 	for (auto &arg : arguments) {
215: 		input_arguments.push_back(arg.ToString());
216: 	}
217: 	for (auto &kv : named_parameters) {
218: 		input_arguments.push_back(StringUtil::Format("%s : %s", kv.first, kv.second.ToString()));
219: 	}
220: 	return StringUtil::Format("%s(%s)", name, StringUtil::Join(input_arguments, ", "));
221: }
222: 
223: static int64_t BindVarArgsFunctionCost(SimpleFunction &func, vector<LogicalType> &arguments) {
224: 	if (arguments.size() < func.arguments.size()) {
225: 		// not enough arguments to fulfill the non-vararg part of the function
226: 		return -1;
227: 	}
228: 	int64_t cost = 0;
229: 	for (idx_t i = 0; i < arguments.size(); i++) {
230: 		LogicalType arg_type = i < func.arguments.size() ? func.arguments[i] : func.varargs;
231: 		if (arguments[i] == arg_type) {
232: 			// arguments match: do nothing
233: 			continue;
234: 		}
235: 		int64_t cast_cost = CastRules::ImplicitCast(arguments[i], arg_type);
236: 		if (cast_cost >= 0) {
237: 			// we can implicitly cast, add the cost to the total cost
238: 			cost += cast_cost;
239: 		} else {
240: 			// we can't implicitly cast: throw an error
241: 			return -1;
242: 		}
243: 	}
244: 	return cost;
245: }
246: 
247: static int64_t BindFunctionCost(SimpleFunction &func, vector<LogicalType> &arguments) {
248: 	if (func.HasVarArgs()) {
249: 		// special case varargs function
250: 		return BindVarArgsFunctionCost(func, arguments);
251: 	}
252: 	if (func.arguments.size() != arguments.size()) {
253: 		// invalid argument count: check the next function
254: 		return -1;
255: 	}
256: 	int64_t cost = 0;
257: 	for (idx_t i = 0; i < arguments.size(); i++) {
258: 		if (arguments[i].id() == func.arguments[i].id()) {
259: 			// arguments match: do nothing
260: 			continue;
261: 		}
262: 		int64_t cast_cost = CastRules::ImplicitCast(arguments[i], func.arguments[i]);
263: 		if (cast_cost >= 0) {
264: 			// we can implicitly cast, add the cost to the total cost
265: 			cost += cast_cost;
266: 		} else {
267: 			// we can't implicitly cast: throw an error
268: 			return -1;
269: 		}
270: 	}
271: 	return cost;
272: }
273: 
274: template <class T>
275: static idx_t BindFunctionFromArguments(const string &name, vector<T> &functions, vector<LogicalType> &arguments,
276:                                        string &error) {
277: 	idx_t best_function = DConstants::INVALID_INDEX;
278: 	int64_t lowest_cost = NumericLimits<int64_t>::Maximum();
279: 	vector<idx_t> conflicting_functions;
280: 	for (idx_t f_idx = 0; f_idx < functions.size(); f_idx++) {
281: 		auto &func = functions[f_idx];
282: 		// check the arguments of the function
283: 		int64_t cost = BindFunctionCost(func, arguments);
284: 		if (cost < 0) {
285: 			// auto casting was not possible
286: 			continue;
287: 		}
288: 		if (cost == lowest_cost) {
289: 			conflicting_functions.push_back(f_idx);
290: 			continue;
291: 		}
292: 		if (cost > lowest_cost) {
293: 			continue;
294: 		}
295: 		conflicting_functions.clear();
296: 		lowest_cost = cost;
297: 		best_function = f_idx;
298: 	}
299: 	if (!conflicting_functions.empty()) {
300: 		// there are multiple possible function definitions
301: 		// throw an exception explaining which overloads are there
302: 		conflicting_functions.push_back(best_function);
303: 		string call_str = Function::CallToString(name, arguments);
304: 		string candidate_str = "";
305: 		for (auto &conf : conflicting_functions) {
306: 			auto &f = functions[conf];
307: 			candidate_str += "\t" + f.ToString() + "\n";
308: 		}
309: 		error =
310: 		    StringUtil::Format("Could not choose a best candidate function for the function call \"%s\". In order to "
311: 		                       "select one, please add explicit type casts.\n\tCandidate functions:\n%s",
312: 		                       call_str, candidate_str);
313: 		return DConstants::INVALID_INDEX;
314: 	}
315: 	if (best_function == DConstants::INVALID_INDEX) {
316: 		// no matching function was found, throw an error
317: 		string call_str = Function::CallToString(name, arguments);
318: 		string candidate_str = "";
319: 		for (auto &f : functions) {
320: 			candidate_str += "\t" + f.ToString() + "\n";
321: 		}
322: 		error = StringUtil::Format("No function matches the given name and argument types '%s'. You might need to add "
323: 		                           "explicit type casts.\n\tCandidate functions:\n%s",
324: 		                           call_str, candidate_str);
325: 		return DConstants::INVALID_INDEX;
326: 	}
327: 	return best_function;
328: }
329: 
330: idx_t Function::BindFunction(const string &name, vector<ScalarFunction> &functions, vector<LogicalType> &arguments,
331:                              string &error) {
332: 	return BindFunctionFromArguments(name, functions, arguments, error);
333: }
334: 
335: idx_t Function::BindFunction(const string &name, vector<AggregateFunction> &functions, vector<LogicalType> &arguments,
336:                              string &error) {
337: 	return BindFunctionFromArguments(name, functions, arguments, error);
338: }
339: 
340: idx_t Function::BindFunction(const string &name, vector<TableFunction> &functions, vector<LogicalType> &arguments,
341:                              string &error) {
342: 	return BindFunctionFromArguments(name, functions, arguments, error);
343: }
344: 
345: idx_t Function::BindFunction(const string &name, vector<PragmaFunction> &functions, PragmaInfo &info, string &error) {
346: 	vector<LogicalType> types;
347: 	for (auto &value : info.parameters) {
348: 		types.push_back(value.type());
349: 	}
350: 	idx_t entry = BindFunctionFromArguments(name, functions, types, error);
351: 	if (entry == DConstants::INVALID_INDEX) {
352: 		throw BinderException(error);
353: 	}
354: 	auto &candidate_function = functions[entry];
355: 	// cast the input parameters
356: 	for (idx_t i = 0; i < info.parameters.size(); i++) {
357: 		auto target_type =
358: 		    i < candidate_function.arguments.size() ? candidate_function.arguments[i] : candidate_function.varargs;
359: 		info.parameters[i] = info.parameters[i].CastAs(target_type);
360: 	}
361: 	return entry;
362: }
363: 
364: vector<LogicalType> GetLogicalTypesFromExpressions(vector<unique_ptr<Expression>> &arguments) {
365: 	vector<LogicalType> types;
366: 	types.reserve(arguments.size());
367: 	for (auto &argument : arguments) {
368: 		types.push_back(argument->return_type);
369: 	}
370: 	return types;
371: }
372: 
373: idx_t Function::BindFunction(const string &name, vector<ScalarFunction> &functions,
374:                              vector<unique_ptr<Expression>> &arguments, string &error) {
375: 	auto types = GetLogicalTypesFromExpressions(arguments);
376: 	return Function::BindFunction(name, functions, types, error);
377: }
378: 
379: idx_t Function::BindFunction(const string &name, vector<AggregateFunction> &functions,
380:                              vector<unique_ptr<Expression>> &arguments, string &error) {
381: 	auto types = GetLogicalTypesFromExpressions(arguments);
382: 	return Function::BindFunction(name, functions, types, error);
383: }
384: 
385: idx_t Function::BindFunction(const string &name, vector<TableFunction> &functions,
386:                              vector<unique_ptr<Expression>> &arguments, string &error) {
387: 	auto types = GetLogicalTypesFromExpressions(arguments);
388: 	return Function::BindFunction(name, functions, types, error);
389: }
390: 
391: enum class LogicalTypeComparisonResult { IDENTICAL_TYPE, TARGET_IS_ANY, DIFFERENT_TYPES };
392: 
393: LogicalTypeComparisonResult RequiresCast(const LogicalType &source_type, const LogicalType &target_type) {
394: 	if (target_type.id() == LogicalTypeId::ANY) {
395: 		return LogicalTypeComparisonResult::TARGET_IS_ANY;
396: 	}
397: 	if (source_type == target_type) {
398: 		return LogicalTypeComparisonResult::IDENTICAL_TYPE;
399: 	}
400: 	if (source_type.id() == LogicalTypeId::LIST && target_type.id() == LogicalTypeId::LIST) {
401: 		return RequiresCast(ListType::GetChildType(source_type), ListType::GetChildType(target_type));
402: 	}
403: 	return LogicalTypeComparisonResult::DIFFERENT_TYPES;
404: }
405: 
406: void BaseScalarFunction::CastToFunctionArguments(vector<unique_ptr<Expression>> &children) {
407: 	for (idx_t i = 0; i < children.size(); i++) {
408: 		auto target_type = i < this->arguments.size() ? this->arguments[i] : this->varargs;
409: 		target_type.Verify();
410: 		// check if the type of child matches the type of function argument
411: 		// if not we need to add a cast
412: 		auto cast_result = RequiresCast(children[i]->return_type, target_type);
413: 		// except for one special case: if the function accepts ANY argument
414: 		// in that case we don't add a cast
415: 		if (cast_result == LogicalTypeComparisonResult::TARGET_IS_ANY) {
416: 			if (children[i]->return_type.id() == LogicalTypeId::UNKNOWN) {
417: 				// UNLESS the child is a prepared statement parameter
418: 				// in that case we default the prepared statement parameter to VARCHAR
419: 				children[i]->return_type =
420: 				    ExpressionBinder::ExchangeType(target_type, LogicalTypeId::ANY, LogicalType::VARCHAR);
421: 			}
422: 		} else if (cast_result == LogicalTypeComparisonResult::DIFFERENT_TYPES) {
423: 			children[i] = BoundCastExpression::AddCastToType(move(children[i]), target_type);
424: 		}
425: 	}
426: }
427: 
428: unique_ptr<BoundFunctionExpression> ScalarFunction::BindScalarFunction(ClientContext &context, const string &schema,
429:                                                                        const string &name,
430:                                                                        vector<unique_ptr<Expression>> children,
431:                                                                        string &error, bool is_operator) {
432: 	// bind the function
433: 	auto function = Catalog::GetCatalog(context).GetEntry(context, CatalogType::SCALAR_FUNCTION_ENTRY, schema, name);
434: 	D_ASSERT(function && function->type == CatalogType::SCALAR_FUNCTION_ENTRY);
435: 	return ScalarFunction::BindScalarFunction(context, (ScalarFunctionCatalogEntry &)*function, move(children), error,
436: 	                                          is_operator);
437: }
438: 
439: unique_ptr<BoundFunctionExpression> ScalarFunction::BindScalarFunction(ClientContext &context,
440:                                                                        ScalarFunctionCatalogEntry &func,
441:                                                                        vector<unique_ptr<Expression>> children,
442:                                                                        string &error, bool is_operator) {
443: 	// bind the function
444: 	idx_t best_function = Function::BindFunction(func.name, func.functions, children, error);
445: 	if (best_function == DConstants::INVALID_INDEX) {
446: 		return nullptr;
447: 	}
448: 	// found a matching function!
449: 	auto &bound_function = func.functions[best_function];
450: 	return ScalarFunction::BindScalarFunction(context, bound_function, move(children), is_operator);
451: }
452: 
453: unique_ptr<BoundFunctionExpression> ScalarFunction::BindScalarFunction(ClientContext &context,
454:                                                                        ScalarFunction bound_function,
455:                                                                        vector<unique_ptr<Expression>> children,
456:                                                                        bool is_operator) {
457: 	unique_ptr<FunctionData> bind_info;
458: 	if (bound_function.bind) {
459: 		bind_info = bound_function.bind(context, bound_function, children);
460: 	}
461: 	// check if we need to add casts to the children
462: 	bound_function.CastToFunctionArguments(children);
463: 
464: 	// now create the function
465: 	auto return_type = bound_function.return_type;
466: 	return make_unique<BoundFunctionExpression>(move(return_type), move(bound_function), move(children),
467: 	                                            move(bind_info), is_operator);
468: }
469: 
470: unique_ptr<BoundAggregateExpression>
471: AggregateFunction::BindAggregateFunction(ClientContext &context, AggregateFunction bound_function,
472:                                          vector<unique_ptr<Expression>> children, unique_ptr<Expression> filter,
473:                                          bool is_distinct, unique_ptr<BoundOrderModifier> order_bys) {
474: 	unique_ptr<FunctionData> bind_info;
475: 	if (bound_function.bind) {
476: 		bind_info = bound_function.bind(context, bound_function, children);
477: 		// we may have lost some arguments in the bind
478: 		children.resize(MinValue(bound_function.arguments.size(), children.size()));
479: 	}
480: 
481: 	// check if we need to add casts to the children
482: 	bound_function.CastToFunctionArguments(children);
483: 
484: 	// Special case: for ORDER BY aggregates, we wrap the aggregate function in a SortedAggregateFunction
485: 	// The children are the sort clauses and the binding contains the ordering data.
486: 	if (order_bys && !order_bys->orders.empty()) {
487: 		bind_info = BindSortedAggregate(bound_function, children, move(bind_info), move(order_bys));
488: 	}
489: 
490: 	return make_unique<BoundAggregateExpression>(move(bound_function), move(children), move(filter), move(bind_info),
491: 	                                             is_distinct);
492: }
493: 
494: } // namespace duckdb
[end of src/function/function.cpp]
[start of src/function/table/arrow.cpp]
1: #include "duckdb/common/arrow.hpp"
2: 
3: #include "duckdb.hpp"
4: #include "duckdb/common/arrow_wrapper.hpp"
5: #include "duckdb/common/limits.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/types/date.hpp"
8: #include "duckdb/common/types/hugeint.hpp"
9: #include "duckdb/common/types/time.hpp"
10: #include "duckdb/common/types/timestamp.hpp"
11: #include "duckdb/function/table/arrow.hpp"
12: #include "duckdb/function/table_function.hpp"
13: #include "duckdb/main/client_context.hpp"
14: #include "duckdb/main/connection.hpp"
15: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
16: #include "utf8proc_wrapper.hpp"
17: 
18: #include "duckdb/common/operator/multiply.hpp"
19: #include "duckdb/common/mutex.hpp"
20: #include <map>
21: 
22: namespace duckdb {
23: 
24: LogicalType GetArrowLogicalType(ArrowSchema &schema,
25:                                 std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
26:                                 idx_t col_idx) {
27: 	auto format = string(schema.format);
28: 	if (arrow_convert_data.find(col_idx) == arrow_convert_data.end()) {
29: 		arrow_convert_data[col_idx] = make_unique<ArrowConvertData>();
30: 	}
31: 	if (format == "n") {
32: 		return LogicalType::SQLNULL;
33: 	} else if (format == "b") {
34: 		return LogicalType::BOOLEAN;
35: 	} else if (format == "c") {
36: 		return LogicalType::TINYINT;
37: 	} else if (format == "s") {
38: 		return LogicalType::SMALLINT;
39: 	} else if (format == "i") {
40: 		return LogicalType::INTEGER;
41: 	} else if (format == "l") {
42: 		return LogicalType::BIGINT;
43: 	} else if (format == "C") {
44: 		return LogicalType::UTINYINT;
45: 	} else if (format == "S") {
46: 		return LogicalType::USMALLINT;
47: 	} else if (format == "I") {
48: 		return LogicalType::UINTEGER;
49: 	} else if (format == "L") {
50: 		return LogicalType::UBIGINT;
51: 	} else if (format == "f") {
52: 		return LogicalType::FLOAT;
53: 	} else if (format == "g") {
54: 		return LogicalType::DOUBLE;
55: 	} else if (format[0] == 'd') { //! this can be either decimal128 or decimal 256 (e.g., d:38,0)
56: 		std::string parameters = format.substr(format.find(':'));
57: 		uint8_t width = std::stoi(parameters.substr(1, parameters.find(',')));
58: 		uint8_t scale = std::stoi(parameters.substr(parameters.find(',') + 1));
59: 		if (width > 38) {
60: 			throw NotImplementedException("Unsupported Internal Arrow Type for Decimal %s", format);
61: 		}
62: 		return LogicalType::DECIMAL(width, scale);
63: 	} else if (format == "u") {
64: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);
65: 		return LogicalType::VARCHAR;
66: 	} else if (format == "U") {
67: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);
68: 		return LogicalType::VARCHAR;
69: 	} else if (format == "tsn:") {
70: 		return LogicalTypeId::TIMESTAMP_NS;
71: 	} else if (format == "tsu:") {
72: 		return LogicalTypeId::TIMESTAMP;
73: 	} else if (format == "tsm:") {
74: 		return LogicalTypeId::TIMESTAMP_MS;
75: 	} else if (format == "tss:") {
76: 		return LogicalTypeId::TIMESTAMP_SEC;
77: 	} else if (format == "tdD") {
78: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::DAYS);
79: 		return LogicalType::DATE;
80: 	} else if (format == "tdm") {
81: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);
82: 		return LogicalType::DATE;
83: 	} else if (format == "tts") {
84: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);
85: 		return LogicalType::TIME;
86: 	} else if (format == "ttm") {
87: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);
88: 		return LogicalType::TIME;
89: 	} else if (format == "ttu") {
90: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);
91: 		return LogicalType::TIME;
92: 	} else if (format == "ttn") {
93: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);
94: 		return LogicalType::TIME;
95: 	} else if (format == "tDs") {
96: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);
97: 		return LogicalType::INTERVAL;
98: 	} else if (format == "tDm") {
99: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);
100: 		return LogicalType::INTERVAL;
101: 	} else if (format == "tDu") {
102: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);
103: 		return LogicalType::INTERVAL;
104: 	} else if (format == "tDn") {
105: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);
106: 		return LogicalType::INTERVAL;
107: 	} else if (format == "tiD") {
108: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::DAYS);
109: 		return LogicalType::INTERVAL;
110: 	} else if (format == "tiM") {
111: 		arrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MONTHS);
112: 		return LogicalType::INTERVAL;
113: 	} else if (format == "+l") {
114: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);
115: 		auto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);
116: 		return LogicalType::LIST(child_type);
117: 	} else if (format == "+L") {
118: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);
119: 		auto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);
120: 		return LogicalType::LIST(child_type);
121: 	} else if (format[0] == '+' && format[1] == 'w') {
122: 		std::string parameters = format.substr(format.find(':') + 1);
123: 		idx_t fixed_size = std::stoi(parameters);
124: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);
125: 		auto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);
126: 		return LogicalType::LIST(move(child_type));
127: 	} else if (format == "+s") {
128: 		child_list_t<LogicalType> child_types;
129: 		for (idx_t type_idx = 0; type_idx < (idx_t)schema.n_children; type_idx++) {
130: 			auto child_type = GetArrowLogicalType(*schema.children[type_idx], arrow_convert_data, col_idx);
131: 			child_types.push_back({schema.children[type_idx]->name, child_type});
132: 		}
133: 		return LogicalType::STRUCT(move(child_types));
134: 
135: 	} else if (format == "+m") {
136: 		child_list_t<LogicalType> child_types;
137: 		//! First type will be struct, so we skip it
138: 		auto &struct_schema = *schema.children[0];
139: 		for (idx_t type_idx = 0; type_idx < (idx_t)struct_schema.n_children; type_idx++) {
140: 			//! The other types must be added on lists
141: 			auto child_type = GetArrowLogicalType(*struct_schema.children[type_idx], arrow_convert_data, col_idx);
142: 
143: 			auto list_type = LogicalType::LIST(child_type);
144: 			child_types.push_back({struct_schema.children[type_idx]->name, list_type});
145: 		}
146: 		return LogicalType::MAP(move(child_types));
147: 	} else if (format == "z") {
148: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);
149: 		return LogicalType::BLOB;
150: 	} else if (format == "Z") {
151: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);
152: 		return LogicalType::BLOB;
153: 	} else if (format[0] == 'w') {
154: 		std::string parameters = format.substr(format.find(':') + 1);
155: 		idx_t fixed_size = std::stoi(parameters);
156: 		arrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);
157: 		return LogicalType::BLOB;
158: 	} else {
159: 		throw NotImplementedException("Unsupported Internal Arrow Type %s", format);
160: 	}
161: }
162: 
163: unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &context, vector<Value> &inputs,
164:                                                            unordered_map<string, Value> &named_parameters,
165:                                                            vector<LogicalType> &input_table_types,
166:                                                            vector<string> &input_table_names,
167:                                                            vector<LogicalType> &return_types, vector<string> &names) {
168: 	typedef unique_ptr<ArrowArrayStreamWrapper> (*stream_factory_produce_t)(
169: 	    uintptr_t stream_factory_ptr,
170: 	    std::pair<std::unordered_map<idx_t, string>, std::vector<string>> & project_columns,
171: 	    TableFilterCollection * filters);
172: 	auto stream_factory_ptr = inputs[0].GetPointer();
173: 	auto stream_factory_produce = (stream_factory_produce_t)inputs[1].GetPointer();
174: 	auto rows_per_thread = inputs[2].GetValue<uint64_t>();
175: 	std::pair<std::unordered_map<idx_t, string>, std::vector<string>> project_columns;
176: #ifndef DUCKDB_NO_THREADS
177: 
178: 	auto res = make_unique<ArrowScanFunctionData>(rows_per_thread, stream_factory_produce, stream_factory_ptr,
179: 	                                              std::this_thread::get_id());
180: #else
181: 	auto res = make_unique<ArrowScanFunctionData>(rows_per_thread, stream_factory_produce, stream_factory_ptr);
182: #endif
183: 	auto &data = *res;
184: 	auto stream = stream_factory_produce(stream_factory_ptr, project_columns, nullptr);
185: 
186: 	data.number_of_rows = stream->number_of_rows;
187: 	if (!stream) {
188: 		throw InvalidInputException("arrow_scan: NULL pointer passed");
189: 	}
190: 
191: 	stream->GetSchema(data.schema_root);
192: 
193: 	for (idx_t col_idx = 0; col_idx < (idx_t)data.schema_root.arrow_schema.n_children; col_idx++) {
194: 		auto &schema = *data.schema_root.arrow_schema.children[col_idx];
195: 		if (!schema.release) {
196: 			throw InvalidInputException("arrow_scan: released schema passed");
197: 		}
198: 		if (schema.dictionary) {
199: 			res->arrow_convert_data[col_idx] =
200: 			    make_unique<ArrowConvertData>(GetArrowLogicalType(schema, res->arrow_convert_data, col_idx));
201: 			return_types.emplace_back(GetArrowLogicalType(*schema.dictionary, res->arrow_convert_data, col_idx));
202: 		} else {
203: 			return_types.emplace_back(GetArrowLogicalType(schema, res->arrow_convert_data, col_idx));
204: 		}
205: 		auto format = string(schema.format);
206: 		auto name = string(schema.name);
207: 		if (name.empty()) {
208: 			name = string("v") + to_string(col_idx);
209: 		}
210: 		names.push_back(name);
211: 	}
212: 	return move(res);
213: }
214: 
215: unique_ptr<ArrowArrayStreamWrapper> ProduceArrowScan(const ArrowScanFunctionData &function,
216:                                                      const vector<column_t> &column_ids,
217:                                                      TableFilterCollection *filters) {
218: 	//! Generate Projection Pushdown Vector
219: 	pair<unordered_map<idx_t, string>, vector<string>> project_columns;
220: 	D_ASSERT(!column_ids.empty());
221: 	for (idx_t idx = 0; idx < column_ids.size(); idx++) {
222: 		auto col_idx = column_ids[idx];
223: 		if (col_idx != COLUMN_IDENTIFIER_ROW_ID) {
224: 			auto &schema = *function.schema_root.arrow_schema.children[col_idx];
225: 			project_columns.first[idx] = schema.name;
226: 			project_columns.second.emplace_back(schema.name);
227: 		}
228: 	}
229: 	return function.scanner_producer(function.stream_factory_ptr, project_columns, filters);
230: }
231: 
232: unique_ptr<FunctionOperatorData> ArrowTableFunction::ArrowScanInit(ClientContext &context,
233:                                                                    const FunctionData *bind_data,
234:                                                                    const vector<column_t> &column_ids,
235:                                                                    TableFilterCollection *filters) {
236: 	auto current_chunk = make_unique<ArrowArrayWrapper>();
237: 	auto result = make_unique<ArrowScanState>(move(current_chunk));
238: 	result->column_ids = column_ids;
239: 	auto &data = (const ArrowScanFunctionData &)*bind_data;
240: 	result->stream = ProduceArrowScan(data, column_ids, filters);
241: 	return move(result);
242: }
243: 
244: void ShiftRight(unsigned char *ar, int size, int shift) {
245: 	int carry = 0;
246: 	while (shift--) {
247: 		for (int i = size - 1; i >= 0; --i) {
248: 			int next = (ar[i] & 1) ? 0x80 : 0;
249: 			ar[i] = carry | (ar[i] >> 1);
250: 			carry = next;
251: 		}
252: 	}
253: }
254: 
255: void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size, int64_t nested_offset,
256:                      bool add_null = false) {
257: 	auto &mask = FlatVector::Validity(vector);
258: 	if (array.null_count != 0 && array.buffers[0]) {
259: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
260: 		auto bit_offset = scan_state.chunk_offset + array.offset;
261: 		if (nested_offset != -1) {
262: 			bit_offset = nested_offset;
263: 		}
264: 		auto n_bitmask_bytes = (size + 8 - 1) / 8;
265: 		mask.EnsureWritable();
266: 		if (bit_offset % 8 == 0) {
267: 			//! just memcpy nullmask
268: 			memcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);
269: 		} else {
270: 			//! need to re-align nullmask
271: 			std::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);
272: 			memcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);
273: 			ShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,
274: 			           bit_offset % 8); //! why this has to be a right shift is a mystery to me
275: 			memcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);
276: 		}
277: 	}
278: 	if (add_null) {
279: 		//! We are setting a validity mask of the data part of dictionary vector
280: 		//! For some reason, Nulls are allowed to be indexes, hence we need to set the last element here to be null
281: 		//! We might have to resize the mask
282: 		mask.Resize(size, size + 1);
283: 		mask.SetInvalid(size);
284: 	}
285: }
286: 
287: void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanState &scan_state, idx_t size) {
288: 	if (array.null_count != 0 && array.buffers[0]) {
289: 		auto bit_offset = scan_state.chunk_offset + array.offset;
290: 		auto n_bitmask_bytes = (size + 8 - 1) / 8;
291: 		mask.EnsureWritable();
292: 		if (bit_offset % 8 == 0) {
293: 			//! just memcpy nullmask
294: 			memcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);
295: 		} else {
296: 			//! need to re-align nullmask
297: 			std::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);
298: 			memcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);
299: 			ShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,
300: 			           bit_offset % 8); //! why this has to be a right shift is a mystery to me
301: 			memcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);
302: 		}
303: 	}
304: }
305: 
306: void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
307:                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
308:                          std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset = -1,
309:                          ValidityMask *parent_mask = nullptr);
310: 
311: void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
312:                        std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
313:                        std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {
314: 	auto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];
315: 	idx_t list_size = 0;
316: 	SetValidityMask(vector, array, scan_state, size, nested_offset);
317: 	idx_t start_offset = 0;
318: 	idx_t cur_offset = 0;
319: 	if (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {
320: 		//! Have to check validity mask before setting this up
321: 		idx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;
322: 		if (nested_offset != -1) {
323: 			offset = original_type.second * nested_offset;
324: 		}
325: 		start_offset = offset;
326: 		auto list_data = FlatVector::GetData<list_entry_t>(vector);
327: 		for (idx_t i = 0; i < size; i++) {
328: 			auto &le = list_data[i];
329: 			le.offset = cur_offset;
330: 			le.length = original_type.second;
331: 			cur_offset += original_type.second;
332: 		}
333: 		list_size = cur_offset;
334: 	} else if (original_type.first == ArrowVariableSizeType::NORMAL) {
335: 		auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
336: 		if (nested_offset != -1) {
337: 			offsets = (uint32_t *)array.buffers[1] + nested_offset;
338: 		}
339: 		start_offset = offsets[0];
340: 		auto list_data = FlatVector::GetData<list_entry_t>(vector);
341: 		for (idx_t i = 0; i < size; i++) {
342: 			auto &le = list_data[i];
343: 			le.offset = cur_offset;
344: 			le.length = offsets[i + 1] - offsets[i];
345: 			cur_offset += le.length;
346: 		}
347: 		list_size = offsets[size];
348: 	} else {
349: 		auto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
350: 		if (nested_offset != -1) {
351: 			offsets = (uint64_t *)array.buffers[1] + nested_offset;
352: 		}
353: 		start_offset = offsets[0];
354: 		auto list_data = FlatVector::GetData<list_entry_t>(vector);
355: 		for (idx_t i = 0; i < size; i++) {
356: 			auto &le = list_data[i];
357: 			le.offset = cur_offset;
358: 			le.length = offsets[i + 1] - offsets[i];
359: 			cur_offset += le.length;
360: 		}
361: 		list_size = offsets[size];
362: 	}
363: 	list_size -= start_offset;
364: 	ListVector::Reserve(vector, list_size);
365: 	ListVector::SetListSize(vector, list_size);
366: 	auto &child_vector = ListVector::GetEntry(vector);
367: 	SetValidityMask(child_vector, *array.children[0], scan_state, list_size, start_offset);
368: 	auto &list_mask = FlatVector::Validity(vector);
369: 	if (parent_mask) {
370: 		//! Since this List is owned by a struct we must guarantee their validity map matches on Null
371: 		if (!parent_mask->AllValid()) {
372: 			for (idx_t i = 0; i < size; i++) {
373: 				if (!parent_mask->RowIsValid(i)) {
374: 					list_mask.SetInvalid(i);
375: 				}
376: 			}
377: 		}
378: 	}
379: 	if (list_size == 0 && start_offset == 0) {
380: 		ColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,
381: 		                    arrow_convert_idx, -1);
382: 	} else {
383: 		ColumnArrowToDuckDB(child_vector, *array.children[0], scan_state, list_size, arrow_convert_data, col_idx,
384: 		                    arrow_convert_idx, start_offset);
385: 	}
386: }
387: 
388: void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
389:                        std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
390:                        std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset) {
391: 	auto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];
392: 	SetValidityMask(vector, array, scan_state, size, nested_offset);
393: 	if (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {
394: 		//! Have to check validity mask before setting this up
395: 		idx_t offset = (scan_state.chunk_offset + array.offset) * original_type.second;
396: 		if (nested_offset != -1) {
397: 			offset = original_type.second * nested_offset;
398: 		}
399: 		auto cdata = (char *)array.buffers[1];
400: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
401: 			if (FlatVector::IsNull(vector, row_idx)) {
402: 				continue;
403: 			}
404: 			auto bptr = cdata + offset;
405: 			auto blob_len = original_type.second;
406: 			FlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);
407: 			offset += blob_len;
408: 		}
409: 	} else if (original_type.first == ArrowVariableSizeType::NORMAL) {
410: 		auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
411: 		if (nested_offset != -1) {
412: 			offsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;
413: 		}
414: 		auto cdata = (char *)array.buffers[2];
415: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
416: 			if (FlatVector::IsNull(vector, row_idx)) {
417: 				continue;
418: 			}
419: 			auto bptr = cdata + offsets[row_idx];
420: 			auto blob_len = offsets[row_idx + 1] - offsets[row_idx];
421: 			FlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);
422: 		}
423: 	} else {
424: 		//! Check if last offset is higher than max uint32
425: 		if (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START
426: 			throw std::runtime_error("DuckDB does not support Blobs over 4GB");
427: 		} // LCOV_EXCL_STOP
428: 		auto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
429: 		if (nested_offset != -1) {
430: 			offsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;
431: 		}
432: 		auto cdata = (char *)array.buffers[2];
433: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
434: 			if (FlatVector::IsNull(vector, row_idx)) {
435: 				continue;
436: 			}
437: 			auto bptr = cdata + offsets[row_idx];
438: 			auto blob_len = offsets[row_idx + 1] - offsets[row_idx];
439: 			FlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);
440: 		}
441: 	}
442: }
443: 
444: void ArrowToDuckDBMapList(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
445:                           std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
446:                           std::pair<idx_t, idx_t> &arrow_convert_idx, uint32_t *offsets, ValidityMask *parent_mask) {
447: 	idx_t list_size = offsets[size] - offsets[0];
448: 	ListVector::Reserve(vector, list_size);
449: 
450: 	auto &child_vector = ListVector::GetEntry(vector);
451: 	auto list_data = FlatVector::GetData<list_entry_t>(vector);
452: 	auto cur_offset = 0;
453: 	for (idx_t i = 0; i < size; i++) {
454: 		auto &le = list_data[i];
455: 		le.offset = cur_offset;
456: 		le.length = offsets[i + 1] - offsets[i];
457: 		cur_offset += le.length;
458: 	}
459: 	ListVector::SetListSize(vector, list_size);
460: 	if (list_size == 0 && offsets[0] == 0) {
461: 		SetValidityMask(child_vector, array, scan_state, list_size, -1);
462: 	} else {
463: 		SetValidityMask(child_vector, array, scan_state, list_size, offsets[0]);
464: 	}
465: 
466: 	auto &list_mask = FlatVector::Validity(vector);
467: 	if (parent_mask) {
468: 		//! Since this List is owned by a struct we must guarantee their validity map matches on Null
469: 		if (!parent_mask->AllValid()) {
470: 			for (idx_t i = 0; i < size; i++) {
471: 				if (!parent_mask->RowIsValid(i)) {
472: 					list_mask.SetInvalid(i);
473: 				}
474: 			}
475: 		}
476: 	}
477: 	if (list_size == 0 && offsets[0] == 0) {
478: 		ColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,
479: 		                    -1);
480: 	} else {
481: 		ColumnArrowToDuckDB(child_vector, array, scan_state, list_size, arrow_convert_data, col_idx, arrow_convert_idx,
482: 		                    offsets[0]);
483: 	}
484: }
485: template <class T>
486: static void SetVectorString(Vector &vector, idx_t size, char *cdata, T *offsets) {
487: 	auto strings = FlatVector::GetData<string_t>(vector);
488: 	for (idx_t row_idx = 0; row_idx < size; row_idx++) {
489: 		if (FlatVector::IsNull(vector, row_idx)) {
490: 			continue;
491: 		}
492: 		auto cptr = cdata + offsets[row_idx];
493: 		auto str_len = offsets[row_idx + 1] - offsets[row_idx];
494: 		strings[row_idx] = string_t(cptr, str_len);
495: 	}
496: }
497: 
498: void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset) {
499: 	auto internal_type = GetTypeIdSize(vector.GetType().InternalType());
500: 	auto data_ptr = (data_ptr_t)array.buffers[1] + internal_type * (scan_state.chunk_offset + array.offset);
501: 	if (nested_offset != -1) {
502: 		data_ptr = (data_ptr_t)array.buffers[1] + internal_type * (array.offset + nested_offset);
503: 	}
504: 	FlatVector::SetData(vector, data_ptr);
505: }
506: 
507: template <class T>
508: void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset, idx_t size,
509:                     int64_t conversion) {
510: 	auto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);
511: 	auto &validity_mask = FlatVector::Validity(vector);
512: 	auto src_ptr = (T *)array.buffers[1] + scan_state.chunk_offset + array.offset;
513: 	if (nested_offset != -1) {
514: 		src_ptr = (T *)array.buffers[1] + nested_offset + array.offset;
515: 	}
516: 	for (idx_t row = 0; row < size; row++) {
517: 		if (!validity_mask.RowIsValid(row)) {
518: 			continue;
519: 		}
520: 		if (!TryMultiplyOperator::Operation((int64_t)src_ptr[row], conversion, tgt_ptr[row].micros)) {
521: 			throw ConversionException("Could not convert Interval to Microsecond");
522: 		}
523: 	}
524: }
525: 
526: void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset,
527:                           idx_t size, int64_t conversion) {
528: 	auto tgt_ptr = (interval_t *)FlatVector::GetData(vector);
529: 	auto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
530: 	if (nested_offset != -1) {
531: 		src_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;
532: 	}
533: 	for (idx_t row = 0; row < size; row++) {
534: 		tgt_ptr[row].days = 0;
535: 		tgt_ptr[row].months = 0;
536: 		if (!TryMultiplyOperator::Operation(src_ptr[row], conversion, tgt_ptr[row].micros)) {
537: 			throw ConversionException("Could not convert Interval to Microsecond");
538: 		}
539: 	}
540: }
541: 
542: void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, int64_t nested_offset,
543:                               idx_t size) {
544: 	auto tgt_ptr = (interval_t *)FlatVector::GetData(vector);
545: 	auto src_ptr = (int32_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
546: 	if (nested_offset != -1) {
547: 		src_ptr = (int32_t *)array.buffers[1] + nested_offset + array.offset;
548: 	}
549: 	for (idx_t row = 0; row < size; row++) {
550: 		tgt_ptr[row].days = 0;
551: 		tgt_ptr[row].micros = 0;
552: 		tgt_ptr[row].months = src_ptr[row];
553: 	}
554: }
555: 
556: void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
557:                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,
558:                          std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {
559: 	switch (vector.GetType().id()) {
560: 	case LogicalTypeId::SQLNULL:
561: 		vector.Reference(Value());
562: 		break;
563: 	case LogicalTypeId::BOOLEAN: {
564: 		//! Arrow bit-packs boolean values
565: 		//! Lets first figure out where we are in the source array
566: 		auto src_ptr = (uint8_t *)array.buffers[1] + (scan_state.chunk_offset + array.offset) / 8;
567: 
568: 		if (nested_offset != -1) {
569: 			src_ptr = (uint8_t *)array.buffers[1] + (nested_offset + array.offset) / 8;
570: 		}
571: 		auto tgt_ptr = (uint8_t *)FlatVector::GetData(vector);
572: 		int src_pos = 0;
573: 		idx_t cur_bit = scan_state.chunk_offset % 8;
574: 		if (nested_offset != -1) {
575: 			cur_bit = nested_offset % 8;
576: 		}
577: 		for (idx_t row = 0; row < size; row++) {
578: 			if ((src_ptr[src_pos] & (1 << cur_bit)) == 0) {
579: 				tgt_ptr[row] = 0;
580: 			} else {
581: 				tgt_ptr[row] = 1;
582: 			}
583: 			cur_bit++;
584: 			if (cur_bit == 8) {
585: 				src_pos++;
586: 				cur_bit = 0;
587: 			}
588: 		}
589: 		break;
590: 	}
591: 	case LogicalTypeId::TINYINT:
592: 	case LogicalTypeId::SMALLINT:
593: 	case LogicalTypeId::INTEGER:
594: 	case LogicalTypeId::FLOAT:
595: 	case LogicalTypeId::UTINYINT:
596: 	case LogicalTypeId::USMALLINT:
597: 	case LogicalTypeId::UINTEGER:
598: 	case LogicalTypeId::UBIGINT:
599: 	case LogicalTypeId::BIGINT:
600: 	case LogicalTypeId::HUGEINT:
601: 	case LogicalTypeId::TIMESTAMP:
602: 	case LogicalTypeId::TIMESTAMP_SEC:
603: 	case LogicalTypeId::TIMESTAMP_MS:
604: 	case LogicalTypeId::TIMESTAMP_NS: {
605: 		DirectConversion(vector, array, scan_state, nested_offset);
606: 		break;
607: 	}
608: 	case LogicalTypeId::DOUBLE: {
609: 		DirectConversion(vector, array, scan_state, nested_offset);
610: 		//! Need to check if there are NaNs, if yes, must turn that to null
611: 		auto data = (double *)vector.GetData();
612: 		auto &mask = FlatVector::Validity(vector);
613: 		for (idx_t row_idx = 0; row_idx < size; row_idx++) {
614: 			if (!Value::DoubleIsValid(data[row_idx])) {
615: 				mask.SetInvalid(row_idx);
616: 			}
617: 		}
618: 		break;
619: 	}
620: 	case LogicalTypeId::VARCHAR: {
621: 		auto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];
622: 		auto cdata = (char *)array.buffers[2];
623: 		if (original_type.first == ArrowVariableSizeType::SUPER_SIZE) {
624: 			if (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START
625: 				throw std::runtime_error("DuckDB does not support Strings over 4GB");
626: 			} // LCOV_EXCL_STOP
627: 			auto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
628: 			if (nested_offset != -1) {
629: 				offsets = (uint64_t *)array.buffers[1] + array.offset + nested_offset;
630: 			}
631: 			SetVectorString(vector, size, cdata, offsets);
632: 
633: 		} else {
634: 			auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
635: 			if (nested_offset != -1) {
636: 				offsets = (uint32_t *)array.buffers[1] + array.offset + nested_offset;
637: 			}
638: 			SetVectorString(vector, size, cdata, offsets);
639: 		}
640: 
641: 		break;
642: 	}
643: 	case LogicalTypeId::DATE: {
644: 		auto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];
645: 		switch (precision) {
646: 		case ArrowDateTimeType::DAYS: {
647: 			DirectConversion(vector, array, scan_state, nested_offset);
648: 			break;
649: 		}
650: 		case ArrowDateTimeType::MILLISECONDS: {
651: 			//! convert date from nanoseconds to days
652: 			auto src_ptr = (uint64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
653: 			if (nested_offset != -1) {
654: 				src_ptr = (uint64_t *)array.buffers[1] + nested_offset + array.offset;
655: 			}
656: 			auto tgt_ptr = (date_t *)FlatVector::GetData(vector);
657: 			for (idx_t row = 0; row < size; row++) {
658: 				tgt_ptr[row] = date_t(int64_t(src_ptr[row]) / (1000 * 60 * 60 * 24));
659: 			}
660: 			break;
661: 		}
662: 		default:
663: 			throw std::runtime_error("Unsupported precision for Date Type ");
664: 		}
665: 		break;
666: 	}
667: 	case LogicalTypeId::TIME: {
668: 		auto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];
669: 		switch (precision) {
670: 		case ArrowDateTimeType::SECONDS: {
671: 			TimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000000);
672: 			break;
673: 		}
674: 		case ArrowDateTimeType::MILLISECONDS: {
675: 			TimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000);
676: 			break;
677: 		}
678: 		case ArrowDateTimeType::MICROSECONDS: {
679: 			TimeConversion<int64_t>(vector, array, scan_state, nested_offset, size, 1);
680: 			break;
681: 		}
682: 		case ArrowDateTimeType::NANOSECONDS: {
683: 			auto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);
684: 			auto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
685: 			if (nested_offset != -1) {
686: 				src_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;
687: 			}
688: 			for (idx_t row = 0; row < size; row++) {
689: 				tgt_ptr[row].micros = src_ptr[row] / 1000;
690: 			}
691: 			break;
692: 		}
693: 		default:
694: 			throw std::runtime_error("Unsupported precision for Time Type ");
695: 		}
696: 		break;
697: 	}
698: 	case LogicalTypeId::INTERVAL: {
699: 		auto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];
700: 		switch (precision) {
701: 		case ArrowDateTimeType::SECONDS: {
702: 			IntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000000);
703: 			break;
704: 		}
705: 		case ArrowDateTimeType::DAYS:
706: 		case ArrowDateTimeType::MILLISECONDS: {
707: 			IntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000);
708: 			break;
709: 		}
710: 		case ArrowDateTimeType::MICROSECONDS: {
711: 			IntervalConversionUs(vector, array, scan_state, nested_offset, size, 1);
712: 			break;
713: 		}
714: 		case ArrowDateTimeType::NANOSECONDS: {
715: 			auto tgt_ptr = (interval_t *)FlatVector::GetData(vector);
716: 			auto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
717: 			if (nested_offset != -1) {
718: 				src_ptr = (int64_t *)array.buffers[1] + nested_offset + array.offset;
719: 			}
720: 			for (idx_t row = 0; row < size; row++) {
721: 				tgt_ptr[row].micros = src_ptr[row] / 1000;
722: 				tgt_ptr[row].days = 0;
723: 				tgt_ptr[row].months = 0;
724: 			}
725: 			break;
726: 		}
727: 		case ArrowDateTimeType::MONTHS: {
728: 			IntervalConversionMonths(vector, array, scan_state, nested_offset, size);
729: 			break;
730: 		}
731: 		default:
732: 			throw std::runtime_error("Unsupported precision for Interval/Duration Type ");
733: 		}
734: 		break;
735: 	}
736: 	case LogicalTypeId::DECIMAL: {
737: 		auto val_mask = FlatVector::Validity(vector);
738: 		//! We have to convert from INT128
739: 		auto src_ptr = (hugeint_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;
740: 		if (nested_offset != -1) {
741: 			src_ptr = (hugeint_t *)array.buffers[1] + nested_offset + array.offset;
742: 		}
743: 		switch (vector.GetType().InternalType()) {
744: 		case PhysicalType::INT16: {
745: 			auto tgt_ptr = (int16_t *)FlatVector::GetData(vector);
746: 			for (idx_t row = 0; row < size; row++) {
747: 				if (val_mask.RowIsValid(row)) {
748: 					auto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);
749: 					D_ASSERT(result);
750: 					(void)result;
751: 				}
752: 			}
753: 			break;
754: 		}
755: 		case PhysicalType::INT32: {
756: 			auto tgt_ptr = (int32_t *)FlatVector::GetData(vector);
757: 			for (idx_t row = 0; row < size; row++) {
758: 				if (val_mask.RowIsValid(row)) {
759: 					auto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);
760: 					D_ASSERT(result);
761: 					(void)result;
762: 				}
763: 			}
764: 			break;
765: 		}
766: 		case PhysicalType::INT64: {
767: 			auto tgt_ptr = (int64_t *)FlatVector::GetData(vector);
768: 			for (idx_t row = 0; row < size; row++) {
769: 				if (val_mask.RowIsValid(row)) {
770: 					auto result = Hugeint::TryCast(src_ptr[row], tgt_ptr[row]);
771: 					D_ASSERT(result);
772: 					(void)result;
773: 				}
774: 			}
775: 			break;
776: 		}
777: 		case PhysicalType::INT128: {
778: 			FlatVector::SetData(vector, (data_ptr_t)array.buffers[1] + GetTypeIdSize(vector.GetType().InternalType()) *
779: 			                                                               (scan_state.chunk_offset + array.offset));
780: 			break;
781: 		}
782: 		default:
783: 			throw std::runtime_error("Unsupported physical type for Decimal: " +
784: 			                         TypeIdToString(vector.GetType().InternalType()));
785: 		}
786: 		break;
787: 	}
788: 	case LogicalTypeId::BLOB: {
789: 		ArrowToDuckDBBlob(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,
790: 		                  nested_offset);
791: 		break;
792: 	}
793: 	case LogicalTypeId::LIST: {
794: 		ArrowToDuckDBList(vector, array, scan_state, size, arrow_convert_data, col_idx, arrow_convert_idx,
795: 		                  nested_offset, parent_mask);
796: 		break;
797: 	}
798: 	case LogicalTypeId::MAP: {
799: 		//! Since this is a map we skip first child, because its a struct
800: 		auto &struct_arrow = *array.children[0];
801: 		auto &child_entries = StructVector::GetEntries(vector);
802: 		D_ASSERT(child_entries.size() == 2);
803: 		auto offsets = (uint32_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;
804: 		if (nested_offset != -1) {
805: 			offsets = (uint32_t *)array.buffers[1] + nested_offset;
806: 		}
807: 		auto &struct_validity_mask = FlatVector::Validity(vector);
808: 		//! Fill the children
809: 		for (idx_t type_idx = 0; type_idx < (idx_t)struct_arrow.n_children; type_idx++) {
810: 			ArrowToDuckDBMapList(*child_entries[type_idx], *struct_arrow.children[type_idx], scan_state, size,
811: 			                     arrow_convert_data, col_idx, arrow_convert_idx, offsets, &struct_validity_mask);
812: 		}
813: 		break;
814: 	}
815: 	case LogicalTypeId::STRUCT: {
816: 		//! Fill the children
817: 		auto &child_entries = StructVector::GetEntries(vector);
818: 		auto &struct_validity_mask = FlatVector::Validity(vector);
819: 		for (idx_t type_idx = 0; type_idx < (idx_t)array.n_children; type_idx++) {
820: 			SetValidityMask(*child_entries[type_idx], *array.children[type_idx], scan_state, size, nested_offset);
821: 			ColumnArrowToDuckDB(*child_entries[type_idx], *array.children[type_idx], scan_state, size,
822: 			                    arrow_convert_data, col_idx, arrow_convert_idx, nested_offset, &struct_validity_mask);
823: 		}
824: 		break;
825: 	}
826: 	default:
827: 		throw std::runtime_error("Unsupported type " + vector.GetType().ToString());
828: 	}
829: }
830: 
831: template <class T>
832: static void SetSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {
833: 	auto indices = (T *)indices_p;
834: 	for (idx_t row = 0; row < size; row++) {
835: 		sel.set_index(row, indices[row]);
836: 	}
837: }
838: 
839: template <class T>
840: static void SetSelectionVectorLoopWithChecks(SelectionVector &sel, data_ptr_t indices_p, idx_t size) {
841: 
842: 	auto indices = (T *)indices_p;
843: 	for (idx_t row = 0; row < size; row++) {
844: 		if (indices[row] > NumericLimits<uint32_t>::Maximum()) {
845: 			throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
846: 		}
847: 		sel.set_index(row, indices[row]);
848: 	}
849: }
850: 
851: template <class T>
852: static void SetMaskedSelectionVectorLoop(SelectionVector &sel, data_ptr_t indices_p, idx_t size, ValidityMask &mask,
853:                                          idx_t last_element_pos) {
854: 	auto indices = (T *)indices_p;
855: 	for (idx_t row = 0; row < size; row++) {
856: 		if (mask.RowIsValid(row)) {
857: 			sel.set_index(row, indices[row]);
858: 		} else {
859: 			//! Need to point out to last element
860: 			sel.set_index(row, last_element_pos);
861: 		}
862: 	}
863: }
864: 
865: void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType &logical_type, idx_t size,
866:                         ValidityMask *mask = nullptr, idx_t last_element_pos = 0) {
867: 	sel.Initialize(size);
868: 
869: 	if (mask) {
870: 		switch (logical_type.id()) {
871: 		case LogicalTypeId::UTINYINT:
872: 			SetMaskedSelectionVectorLoop<uint8_t>(sel, indices_p, size, *mask, last_element_pos);
873: 			break;
874: 		case LogicalTypeId::TINYINT:
875: 			SetMaskedSelectionVectorLoop<int8_t>(sel, indices_p, size, *mask, last_element_pos);
876: 			break;
877: 		case LogicalTypeId::USMALLINT:
878: 			SetMaskedSelectionVectorLoop<uint16_t>(sel, indices_p, size, *mask, last_element_pos);
879: 			break;
880: 		case LogicalTypeId::SMALLINT:
881: 			SetMaskedSelectionVectorLoop<int16_t>(sel, indices_p, size, *mask, last_element_pos);
882: 			break;
883: 		case LogicalTypeId::UINTEGER:
884: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
885: 				//! Its guaranteed that our indices will point to the last element, so just throw an error
886: 				throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
887: 			}
888: 			SetMaskedSelectionVectorLoop<uint32_t>(sel, indices_p, size, *mask, last_element_pos);
889: 			break;
890: 		case LogicalTypeId::INTEGER:
891: 			SetMaskedSelectionVectorLoop<int32_t>(sel, indices_p, size, *mask, last_element_pos);
892: 			break;
893: 		case LogicalTypeId::UBIGINT:
894: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
895: 				//! Its guaranteed that our indices will point to the last element, so just throw an error
896: 				throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
897: 			}
898: 			SetMaskedSelectionVectorLoop<uint64_t>(sel, indices_p, size, *mask, last_element_pos);
899: 			break;
900: 		case LogicalTypeId::BIGINT:
901: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
902: 				//! Its guaranteed that our indices will point to the last element, so just throw an error
903: 				throw std::runtime_error("DuckDB only supports indices that fit on an uint32");
904: 			}
905: 			SetMaskedSelectionVectorLoop<int64_t>(sel, indices_p, size, *mask, last_element_pos);
906: 			break;
907: 
908: 		default:
909: 			throw std::runtime_error("(Arrow) Unsupported type for selection vectors " + logical_type.ToString());
910: 		}
911: 
912: 	} else {
913: 		switch (logical_type.id()) {
914: 		case LogicalTypeId::UTINYINT:
915: 			SetSelectionVectorLoop<uint8_t>(sel, indices_p, size);
916: 			break;
917: 		case LogicalTypeId::TINYINT:
918: 			SetSelectionVectorLoop<int8_t>(sel, indices_p, size);
919: 			break;
920: 		case LogicalTypeId::USMALLINT:
921: 			SetSelectionVectorLoop<uint16_t>(sel, indices_p, size);
922: 			break;
923: 		case LogicalTypeId::SMALLINT:
924: 			SetSelectionVectorLoop<int16_t>(sel, indices_p, size);
925: 			break;
926: 		case LogicalTypeId::UINTEGER:
927: 			SetSelectionVectorLoop<uint32_t>(sel, indices_p, size);
928: 			break;
929: 		case LogicalTypeId::INTEGER:
930: 			SetSelectionVectorLoop<int32_t>(sel, indices_p, size);
931: 			break;
932: 		case LogicalTypeId::UBIGINT:
933: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
934: 				//! We need to check if our indexes fit in a uint32_t
935: 				SetSelectionVectorLoopWithChecks<uint64_t>(sel, indices_p, size);
936: 			} else {
937: 				SetSelectionVectorLoop<uint64_t>(sel, indices_p, size);
938: 			}
939: 			break;
940: 		case LogicalTypeId::BIGINT:
941: 			if (last_element_pos > NumericLimits<uint32_t>::Maximum()) {
942: 				//! We need to check if our indexes fit in a uint32_t
943: 				SetSelectionVectorLoopWithChecks<int64_t>(sel, indices_p, size);
944: 			} else {
945: 				SetSelectionVectorLoop<int64_t>(sel, indices_p, size);
946: 			}
947: 			break;
948: 		default:
949: 			throw std::runtime_error("(Arrow) Unsupported type for selection vectors " + logical_type.ToString());
950: 		}
951: 	}
952: }
953: 
954: void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, ArrowScanState &scan_state, idx_t size,
955:                                    std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
956:                                    idx_t col_idx, std::pair<idx_t, idx_t> &arrow_convert_idx) {
957: 	SelectionVector sel;
958: 	auto &dict_vectors = scan_state.arrow_dictionary_vectors;
959: 	if (dict_vectors.find(col_idx) == dict_vectors.end()) {
960: 		//! We need to set the dictionary data for this column
961: 		auto base_vector = make_unique<Vector>(vector.GetType(), array.dictionary->length);
962: 		SetValidityMask(*base_vector, *array.dictionary, scan_state, array.dictionary->length, 0, array.null_count > 0);
963: 		ColumnArrowToDuckDB(*base_vector, *array.dictionary, scan_state, array.dictionary->length, arrow_convert_data,
964: 		                    col_idx, arrow_convert_idx);
965: 		dict_vectors[col_idx] = move(base_vector);
966: 	}
967: 	auto dictionary_type = arrow_convert_data[col_idx]->dictionary_type;
968: 	//! Get Pointer to Indices of Dictionary
969: 	auto indices = (data_ptr_t)array.buffers[1] +
970: 	               GetTypeIdSize(dictionary_type.InternalType()) * (scan_state.chunk_offset + array.offset);
971: 	if (array.null_count > 0) {
972: 		ValidityMask indices_validity;
973: 		GetValidityMask(indices_validity, array, scan_state, size);
974: 		SetSelectionVector(sel, indices, dictionary_type, size, &indices_validity, array.dictionary->length);
975: 	} else {
976: 		SetSelectionVector(sel, indices, dictionary_type, size);
977: 	}
978: 	vector.Slice(*dict_vectors[col_idx], sel, size);
979: }
980: 
981: void ArrowTableFunction::ArrowToDuckDB(ArrowScanState &scan_state,
982:                                        std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
983:                                        DataChunk &output, idx_t start) {
984: 	for (idx_t idx = 0; idx < output.ColumnCount(); idx++) {
985: 		auto col_idx = scan_state.column_ids[idx];
986: 		std::pair<idx_t, idx_t> arrow_convert_idx {0, 0};
987: 		auto &array = *scan_state.chunk->arrow_array.children[idx];
988: 		if (!array.release) {
989: 			throw InvalidInputException("arrow_scan: released array passed");
990: 		}
991: 		if (array.length != scan_state.chunk->arrow_array.length) {
992: 			throw InvalidInputException("arrow_scan: array length mismatch");
993: 		}
994: 		if (array.dictionary) {
995: 			ColumnArrowToDuckDBDictionary(output.data[idx], array, scan_state, output.size(), arrow_convert_data,
996: 			                              col_idx, arrow_convert_idx);
997: 		} else {
998: 			SetValidityMask(output.data[idx], array, scan_state, output.size(), -1);
999: 			ColumnArrowToDuckDB(output.data[idx], array, scan_state, output.size(), arrow_convert_data, col_idx,
1000: 			                    arrow_convert_idx);
1001: 		}
1002: 	}
1003: }
1004: 
1005: void ArrowTableFunction::ArrowScanFunction(ClientContext &context, const FunctionData *bind_data,
1006:                                            FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
1007: 
1008: 	auto &data = (ArrowScanFunctionData &)*bind_data;
1009: 	auto &state = (ArrowScanState &)*operator_state;
1010: 
1011: 	//! have we run out of data on the current chunk? move to next one
1012: 	while (state.chunk_offset >= (idx_t)state.chunk->arrow_array.length) {
1013: 		state.chunk_offset = 0;
1014: 		state.arrow_dictionary_vectors.clear();
1015: 		state.chunk = state.stream->GetNextChunk();
1016: 		//! have we run out of chunks? we are done
1017: 		if (!state.chunk->arrow_array.release) {
1018: 			return;
1019: 		}
1020: 	}
1021: 
1022: 	int64_t output_size = MinValue<int64_t>(STANDARD_VECTOR_SIZE, state.chunk->arrow_array.length - state.chunk_offset);
1023: 	data.lines_read += output_size;
1024: 	output.SetCardinality(output_size);
1025: 	ArrowToDuckDB(state, data.arrow_convert_data, output, data.lines_read - output_size);
1026: 	output.Verify();
1027: 	state.chunk_offset += output.size();
1028: }
1029: 
1030: void ArrowTableFunction::ArrowScanFunctionParallel(ClientContext &context, const FunctionData *bind_data,
1031:                                                    FunctionOperatorData *operator_state, DataChunk *input,
1032:                                                    DataChunk &output, ParallelState *parallel_state_p) {
1033: 	auto &data = (ArrowScanFunctionData &)*bind_data;
1034: 	auto &state = (ArrowScanState &)*operator_state;
1035: 	//! Out of tuples in this chunk
1036: 	if (state.chunk_offset >= (idx_t)state.chunk->arrow_array.length) {
1037: 		return;
1038: 	}
1039: 	int64_t output_size = MinValue<int64_t>(STANDARD_VECTOR_SIZE, state.chunk->arrow_array.length - state.chunk_offset);
1040: 	data.lines_read += output_size;
1041: 	output.SetCardinality(output_size);
1042: 	ArrowToDuckDB(state, data.arrow_convert_data, output, data.lines_read - output_size);
1043: 	output.Verify();
1044: 	state.chunk_offset += output.size();
1045: }
1046: 
1047: idx_t ArrowTableFunction::ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p) {
1048: 	auto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;
1049: 	if (bind_data.number_of_rows <= 0 || ClientConfig::GetConfig(context).verify_parallelism) {
1050: 		return context.db->NumberOfThreads();
1051: 	}
1052: 	return ((bind_data.number_of_rows + bind_data.rows_per_thread - 1) / bind_data.rows_per_thread) + 1;
1053: }
1054: 
1055: unique_ptr<ParallelState> ArrowTableFunction::ArrowScanInitParallelState(ClientContext &context,
1056:                                                                          const FunctionData *bind_data_p,
1057:                                                                          const vector<column_t> &column_ids,
1058:                                                                          TableFilterCollection *filters) {
1059: 	auto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;
1060: 	auto result = make_unique<ParallelArrowScanState>();
1061: 	result->stream = ProduceArrowScan(bind_data, column_ids, filters);
1062: 	return move(result);
1063: }
1064: 
1065: bool ArrowTableFunction::ArrowScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
1066:                                                     FunctionOperatorData *operator_state,
1067:                                                     ParallelState *parallel_state_p) {
1068: 	auto &state = (ArrowScanState &)*operator_state;
1069: 	auto &parallel_state = (ParallelArrowScanState &)*parallel_state_p;
1070: 
1071: 	lock_guard<mutex> parallel_lock(parallel_state.main_mutex);
1072: 	state.chunk_offset = 0;
1073: 
1074: 	auto current_chunk = parallel_state.stream->GetNextChunk();
1075: 	while (current_chunk->arrow_array.length == 0 && current_chunk->arrow_array.release) {
1076: 		current_chunk = parallel_state.stream->GetNextChunk();
1077: 	}
1078: 	state.chunk = move(current_chunk);
1079: 	//! have we run out of chunks? we are done
1080: 	if (!state.chunk->arrow_array.release) {
1081: 		return false;
1082: 	}
1083: 	return true;
1084: }
1085: 
1086: unique_ptr<FunctionOperatorData>
1087: ArrowTableFunction::ArrowScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *state,
1088:                                           const vector<column_t> &column_ids, TableFilterCollection *filters) {
1089: 	auto current_chunk = make_unique<ArrowArrayWrapper>();
1090: 	auto result = make_unique<ArrowScanState>(move(current_chunk));
1091: 	result->column_ids = column_ids;
1092: 	result->filters = filters;
1093: 	ArrowScanParallelStateNext(context, bind_data_p, result.get(), state);
1094: 	return move(result);
1095: }
1096: 
1097: unique_ptr<NodeStatistics> ArrowTableFunction::ArrowScanCardinality(ClientContext &context, const FunctionData *data) {
1098: 	auto &bind_data = (ArrowScanFunctionData &)*data;
1099: 	return make_unique<NodeStatistics>(bind_data.number_of_rows, bind_data.number_of_rows);
1100: }
1101: 
1102: double ArrowTableFunction::ArrowProgress(ClientContext &context, const FunctionData *bind_data_p) {
1103: 	auto &bind_data = (const ArrowScanFunctionData &)*bind_data_p;
1104: 	if (bind_data.number_of_rows == 0) {
1105: 		return 100;
1106: 	}
1107: 	auto percentage = bind_data.lines_read * 100.0 / bind_data.number_of_rows;
1108: 	return percentage;
1109: }
1110: 
1111: void ArrowTableFunction::RegisterFunction(BuiltinFunctions &set) {
1112: 	TableFunctionSet arrow("arrow_scan");
1113: 	arrow.AddFunction(TableFunction({LogicalType::POINTER, LogicalType::POINTER, LogicalType::UBIGINT},
1114: 	                                ArrowScanFunction, ArrowScanBind, ArrowScanInit, nullptr, nullptr, nullptr,
1115: 	                                ArrowScanCardinality, nullptr, nullptr, ArrowScanMaxThreads,
1116: 	                                ArrowScanInitParallelState, ArrowScanFunctionParallel, ArrowScanParallelInit,
1117: 	                                ArrowScanParallelStateNext, true, true, ArrowProgress));
1118: 	set.AddFunction(arrow);
1119: }
1120: 
1121: void BuiltinFunctions::RegisterArrowFunctions() {
1122: 	ArrowTableFunction::RegisterFunction(*this);
1123: }
1124: } // namespace duckdb
[end of src/function/table/arrow.cpp]
[start of src/function/table/checkpoint.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/main/client_context.hpp"
3: #include "duckdb/storage/storage_manager.hpp"
4: #include "duckdb/transaction/transaction_manager.hpp"
5: 
6: namespace duckdb {
7: 
8: static unique_ptr<FunctionData> CheckpointBind(ClientContext &context, vector<Value> &inputs,
9:                                                unordered_map<string, Value> &named_parameters,
10:                                                vector<LogicalType> &input_table_types,
11:                                                vector<string> &input_table_names, vector<LogicalType> &return_types,
12:                                                vector<string> &names) {
13: 	return_types.emplace_back(LogicalType::BOOLEAN);
14: 	names.emplace_back("Success");
15: 	return nullptr;
16: }
17: 
18: template <bool FORCE>
19: static void TemplatedCheckpointFunction(ClientContext &context, const FunctionData *bind_data_p,
20:                                         FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
21: 	auto &transaction_manager = TransactionManager::Get(context);
22: 	transaction_manager.Checkpoint(context, FORCE);
23: }
24: 
25: void CheckpointFunction::RegisterFunction(BuiltinFunctions &set) {
26: 	TableFunction checkpoint("checkpoint", {}, TemplatedCheckpointFunction<false>, CheckpointBind);
27: 	set.AddFunction(checkpoint);
28: 	TableFunction force_checkpoint("force_checkpoint", {}, TemplatedCheckpointFunction<true>, CheckpointBind);
29: 	set.AddFunction(force_checkpoint);
30: }
31: 
32: } // namespace duckdb
[end of src/function/table/checkpoint.cpp]
[start of src/function/table/glob.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/function/table_function.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/common/file_system.hpp"
5: #include "duckdb/main/config.hpp"
6: 
7: namespace duckdb {
8: 
9: struct GlobFunctionBindData : public TableFunctionData {
10: 	vector<string> files;
11: };
12: 
13: static unique_ptr<FunctionData> GlobFunctionBind(ClientContext &context, vector<Value> &inputs,
14:                                                  unordered_map<string, Value> &named_parameters,
15:                                                  vector<LogicalType> &input_table_types,
16:                                                  vector<string> &input_table_names, vector<LogicalType> &return_types,
17:                                                  vector<string> &names) {
18: 	auto &config = DBConfig::GetConfig(context);
19: 	if (!config.enable_external_access) {
20: 		throw PermissionException("Globbing is disabled through configuration");
21: 	}
22: 	auto result = make_unique<GlobFunctionBindData>();
23: 	auto &fs = FileSystem::GetFileSystem(context);
24: 	result->files = fs.Glob(StringValue::Get(inputs[0]));
25: 	return_types.emplace_back(LogicalType::VARCHAR);
26: 	names.emplace_back("file");
27: 	return move(result);
28: }
29: 
30: struct GlobFunctionState : public FunctionOperatorData {
31: 	GlobFunctionState() : current_idx(0) {
32: 	}
33: 
34: 	idx_t current_idx;
35: };
36: 
37: static unique_ptr<FunctionOperatorData> GlobFunctionInit(ClientContext &context, const FunctionData *bind_data,
38:                                                          const vector<column_t> &column_ids,
39:                                                          TableFilterCollection *filters) {
40: 	return make_unique<GlobFunctionState>();
41: }
42: 
43: static void GlobFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *state_p,
44:                          DataChunk *input, DataChunk &output) {
45: 	auto &bind_data = (GlobFunctionBindData &)*bind_data_p;
46: 	auto &state = (GlobFunctionState &)*state_p;
47: 
48: 	idx_t count = 0;
49: 	idx_t next_idx = MinValue<idx_t>(state.current_idx + STANDARD_VECTOR_SIZE, bind_data.files.size());
50: 	for (; state.current_idx < next_idx; state.current_idx++) {
51: 		output.data[0].SetValue(count, bind_data.files[state.current_idx]);
52: 		count++;
53: 	}
54: 	output.SetCardinality(count);
55: }
56: 
57: void GlobTableFunction::RegisterFunction(BuiltinFunctions &set) {
58: 	TableFunctionSet glob("glob");
59: 	glob.AddFunction(TableFunction({LogicalType::VARCHAR}, GlobFunction, GlobFunctionBind, GlobFunctionInit));
60: 	set.AddFunction(glob);
61: }
62: 
63: } // namespace duckdb
[end of src/function/table/glob.cpp]
[start of src/function/table/pragma_detailed_profiling_output.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
3: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
4: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
5: #include "duckdb/main/query_profiler.hpp"
6: #include "duckdb/main/client_context.hpp"
7: #include "duckdb/common/limits.hpp"
8: namespace duckdb {
9: 
10: struct PragmaDetailedProfilingOutputOperatorData : public FunctionOperatorData {
11: 	explicit PragmaDetailedProfilingOutputOperatorData() : chunk_index(0), initialized(false) {
12: 	}
13: 	idx_t chunk_index;
14: 	bool initialized;
15: };
16: 
17: struct PragmaDetailedProfilingOutputData : public TableFunctionData {
18: 	explicit PragmaDetailedProfilingOutputData(vector<LogicalType> &types) : types(types) {
19: 	}
20: 	unique_ptr<ChunkCollection> collection;
21: 	vector<LogicalType> types;
22: };
23: 
24: static unique_ptr<FunctionData> PragmaDetailedProfilingOutputBind(ClientContext &context, vector<Value> &inputs,
25:                                                                   unordered_map<string, Value> &named_parameters,
26:                                                                   vector<LogicalType> &input_table_types,
27:                                                                   vector<string> &input_table_names,
28:                                                                   vector<LogicalType> &return_types,
29:                                                                   vector<string> &names) {
30: 	names.emplace_back("OPERATOR_ID");
31: 	return_types.emplace_back(LogicalType::INTEGER);
32: 
33: 	names.emplace_back("ANNOTATION");
34: 	return_types.emplace_back(LogicalType::VARCHAR);
35: 
36: 	names.emplace_back("ID");
37: 	return_types.emplace_back(LogicalType::INTEGER);
38: 
39: 	names.emplace_back("NAME");
40: 	return_types.emplace_back(LogicalType::VARCHAR);
41: 
42: 	names.emplace_back("TIME");
43: 	return_types.emplace_back(LogicalType::DOUBLE);
44: 
45: 	names.emplace_back("CYCLES_PER_TUPLE");
46: 	return_types.emplace_back(LogicalType::DOUBLE);
47: 
48: 	names.emplace_back("SAMPLE_SIZE");
49: 	return_types.emplace_back(LogicalType::INTEGER);
50: 
51: 	names.emplace_back("INPUT_SIZE");
52: 	return_types.emplace_back(LogicalType::INTEGER);
53: 
54: 	names.emplace_back("EXTRA_INFO");
55: 	return_types.emplace_back(LogicalType::VARCHAR);
56: 
57: 	return make_unique<PragmaDetailedProfilingOutputData>(return_types);
58: }
59: 
60: unique_ptr<FunctionOperatorData> PragmaDetailedProfilingOutputInit(ClientContext &context,
61:                                                                    const FunctionData *bind_data,
62:                                                                    const vector<column_t> &column_ids,
63:                                                                    TableFilterCollection *filters) {
64: 	return make_unique<PragmaDetailedProfilingOutputOperatorData>();
65: }
66: 
67: // Insert a row into the given datachunk
68: static void SetValue(DataChunk &output, int index, int op_id, string annotation, int id, string name, double time,
69:                      int sample_counter, int tuple_counter, string extra_info) {
70: 	output.SetValue(0, index, op_id);
71: 	output.SetValue(1, index, move(annotation));
72: 	output.SetValue(2, index, id);
73: 	output.SetValue(3, index, move(name));
74: #if defined(RDTSC)
75: 	output.SetValue(4, index, Value(nullptr));
76: 	output.SetValue(5, index, time);
77: #else
78: 	output.SetValue(4, index, time);
79: 	output.SetValue(5, index, Value(nullptr));
80: 
81: #endif
82: 	output.SetValue(6, index, sample_counter);
83: 	output.SetValue(7, index, tuple_counter);
84: 	output.SetValue(8, index, move(extra_info));
85: }
86: 
87: static void ExtractFunctions(ChunkCollection &collection, ExpressionInfo &info, DataChunk &chunk, int op_id,
88:                              int &fun_id) {
89: 	if (info.hasfunction) {
90: 		D_ASSERT(info.sample_tuples_count != 0);
91: 		SetValue(chunk, chunk.size(), op_id, "Function", fun_id++, info.function_name,
92: 		         int(info.function_time) / double(info.sample_tuples_count), info.sample_tuples_count,
93: 		         info.tuples_count, "");
94: 
95: 		chunk.SetCardinality(chunk.size() + 1);
96: 		if (chunk.size() == STANDARD_VECTOR_SIZE) {
97: 			collection.Append(chunk);
98: 			chunk.Reset();
99: 		}
100: 	}
101: 	if (info.children.empty()) {
102: 		return;
103: 	}
104: 	// extract the children of this node
105: 	for (auto &child : info.children) {
106: 		ExtractFunctions(collection, *child, chunk, op_id, fun_id);
107: 	}
108: }
109: 
110: static void PragmaDetailedProfilingOutputFunction(ClientContext &context, const FunctionData *bind_data_p,
111:                                                   FunctionOperatorData *operator_state, DataChunk *input,
112:                                                   DataChunk &output) {
113: 	auto &state = (PragmaDetailedProfilingOutputOperatorData &)*operator_state;
114: 	auto &data = (PragmaDetailedProfilingOutputData &)*bind_data_p;
115: 
116: 	if (!state.initialized) {
117: 		// create a ChunkCollection
118: 		auto collection = make_unique<ChunkCollection>();
119: 
120: 		// create a chunk
121: 		DataChunk chunk;
122: 		chunk.Initialize(data.types);
123: 
124: 		// Initialize ids
125: 		int operator_counter = 1;
126: 		int function_counter = 1;
127: 		int expression_counter = 1;
128: 		if (context.query_profiler_history->GetPrevProfilers().empty()) {
129: 			return;
130: 		}
131: 		// For each Operator
132: 		for (auto op : context.query_profiler_history->GetPrevProfilers().back().second->GetTreeMap()) {
133: 			// For each Expression Executor
134: 			for (auto &expr_executor : op.second->info.executors_info) {
135: 				// For each Expression tree
136: 				if (!expr_executor) {
137: 					continue;
138: 				}
139: 				for (auto &expr_timer : expr_executor->roots) {
140: 					D_ASSERT(expr_timer->sample_tuples_count != 0);
141: 					SetValue(chunk, chunk.size(), operator_counter, "ExpressionRoot", expression_counter++,
142: 					         // Sometimes, cycle counter is not accurate, too big or too small. return 0 for
143: 					         // those cases
144: 					         expr_timer->name, int(expr_timer->time) / double(expr_timer->sample_tuples_count),
145: 					         expr_timer->sample_tuples_count, expr_timer->tuples_count, expr_timer->extra_info);
146: 					// Increment cardinality
147: 					chunk.SetCardinality(chunk.size() + 1);
148: 					// Check whether data chunk is full or not
149: 					if (chunk.size() == STANDARD_VECTOR_SIZE) {
150: 						collection->Append(chunk);
151: 						chunk.Reset();
152: 					}
153: 					// Extract all functions inside the tree
154: 					ExtractFunctions(*collection, *expr_timer->root, chunk, operator_counter, function_counter);
155: 				}
156: 			}
157: 			operator_counter++;
158: 		}
159: 		collection->Append(chunk);
160: 		data.collection = move(collection);
161: 		state.initialized = true;
162: 	}
163: 
164: 	if (state.chunk_index >= data.collection->ChunkCount()) {
165: 		output.SetCardinality(0);
166: 		return;
167: 	}
168: 	output.Reference(data.collection->GetChunk(state.chunk_index++));
169: }
170: 
171: void PragmaDetailedProfilingOutput::RegisterFunction(BuiltinFunctions &set) {
172: 	set.AddFunction(TableFunction("pragma_detailed_profiling_output", {}, PragmaDetailedProfilingOutputFunction,
173: 	                              PragmaDetailedProfilingOutputBind, PragmaDetailedProfilingOutputInit));
174: }
175: 
176: } // namespace duckdb
[end of src/function/table/pragma_detailed_profiling_output.cpp]
[start of src/function/table/pragma_last_profiling_output.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
3: #include "duckdb/common/limits.hpp"
4: #include "duckdb/function/table/system_functions.hpp"
5: #include "duckdb/main/client_context.hpp"
6: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
7: #include "duckdb/main/query_profiler.hpp"
8: 
9: namespace duckdb {
10: 
11: struct PragmaLastProfilingOutputOperatorData : public FunctionOperatorData {
12: 	PragmaLastProfilingOutputOperatorData() : chunk_index(0), initialized(false) {
13: 	}
14: 	idx_t chunk_index;
15: 	bool initialized;
16: };
17: 
18: struct PragmaLastProfilingOutputData : public TableFunctionData {
19: 	explicit PragmaLastProfilingOutputData(vector<LogicalType> &types) : types(types) {
20: 	}
21: 	unique_ptr<ChunkCollection> collection;
22: 	vector<LogicalType> types;
23: };
24: 
25: static unique_ptr<FunctionData> PragmaLastProfilingOutputBind(ClientContext &context, vector<Value> &inputs,
26:                                                               unordered_map<string, Value> &named_parameters,
27:                                                               vector<LogicalType> &input_table_types,
28:                                                               vector<string> &input_table_names,
29:                                                               vector<LogicalType> &return_types,
30:                                                               vector<string> &names) {
31: 	names.emplace_back("OPERATOR_ID");
32: 	return_types.emplace_back(LogicalType::INTEGER);
33: 
34: 	names.emplace_back("NAME");
35: 	return_types.emplace_back(LogicalType::VARCHAR);
36: 
37: 	names.emplace_back("TIME");
38: 	return_types.emplace_back(LogicalType::DOUBLE);
39: 
40: 	names.emplace_back("CARDINALITY");
41: 	return_types.emplace_back(LogicalType::BIGINT);
42: 
43: 	names.emplace_back("DESCRIPTION");
44: 	return_types.emplace_back(LogicalType::VARCHAR);
45: 
46: 	return make_unique<PragmaLastProfilingOutputData>(return_types);
47: }
48: 
49: static void SetValue(DataChunk &output, int index, int op_id, string name, double time, int64_t car,
50:                      string description) {
51: 	output.SetValue(0, index, op_id);
52: 	output.SetValue(1, index, move(name));
53: 	output.SetValue(2, index, time);
54: 	output.SetValue(3, index, car);
55: 	output.SetValue(4, index, move(description));
56: }
57: 
58: unique_ptr<FunctionOperatorData> PragmaLastProfilingOutputInit(ClientContext &context, const FunctionData *bind_data,
59:                                                                const vector<column_t> &column_ids,
60:                                                                TableFilterCollection *filters) {
61: 	return make_unique<PragmaLastProfilingOutputOperatorData>();
62: }
63: 
64: static void PragmaLastProfilingOutputFunction(ClientContext &context, const FunctionData *bind_data_p,
65:                                               FunctionOperatorData *operator_state, DataChunk *input,
66:                                               DataChunk &output) {
67: 	auto &state = (PragmaLastProfilingOutputOperatorData &)*operator_state;
68: 	auto &data = (PragmaLastProfilingOutputData &)*bind_data_p;
69: 	if (!state.initialized) {
70: 		// create a ChunkCollection
71: 		auto collection = make_unique<ChunkCollection>();
72: 
73: 		DataChunk chunk;
74: 		chunk.Initialize(data.types);
75: 		int operator_counter = 1;
76: 		if (!context.query_profiler_history->GetPrevProfilers().empty()) {
77: 			for (auto op : context.query_profiler_history->GetPrevProfilers().back().second->GetTreeMap()) {
78: 				SetValue(chunk, chunk.size(), operator_counter++, op.second->name, op.second->info.time,
79: 				         op.second->info.elements, " ");
80: 				chunk.SetCardinality(chunk.size() + 1);
81: 				if (chunk.size() == STANDARD_VECTOR_SIZE) {
82: 					collection->Append(chunk);
83: 					chunk.Reset();
84: 				}
85: 			}
86: 		}
87: 		collection->Append(chunk);
88: 		data.collection = move(collection);
89: 		state.initialized = true;
90: 	}
91: 
92: 	if (state.chunk_index >= data.collection->ChunkCount()) {
93: 		output.SetCardinality(0);
94: 		return;
95: 	}
96: 	output.Reference(data.collection->GetChunk(state.chunk_index++));
97: }
98: 
99: void PragmaLastProfilingOutput::RegisterFunction(BuiltinFunctions &set) {
100: 	set.AddFunction(TableFunction("pragma_last_profiling_output", {}, PragmaLastProfilingOutputFunction,
101: 	                              PragmaLastProfilingOutputBind, PragmaLastProfilingOutputInit));
102: }
103: 
104: } // namespace duckdb
[end of src/function/table/pragma_last_profiling_output.cpp]
[start of src/function/table/range.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/function/table/summary.hpp"
3: #include "duckdb/function/table_function.hpp"
4: #include "duckdb/function/function_set.hpp"
5: #include "duckdb/common/algorithm.hpp"
6: #include "duckdb/common/operator/add.hpp"
7: 
8: namespace duckdb {
9: 
10: //===--------------------------------------------------------------------===//
11: // Range (integers)
12: //===--------------------------------------------------------------------===//
13: struct RangeFunctionBindData : public TableFunctionData {
14: 	hugeint_t start;
15: 	hugeint_t end;
16: 	hugeint_t increment;
17: };
18: 
19: template <bool GENERATE_SERIES>
20: static unique_ptr<FunctionData>
21: RangeFunctionBind(ClientContext &context, vector<Value> &inputs, unordered_map<string, Value> &named_parameters,
22:                   vector<LogicalType> &input_table_types, vector<string> &input_table_names,
23:                   vector<LogicalType> &return_types, vector<string> &names) {
24: 	auto result = make_unique<RangeFunctionBindData>();
25: 	if (inputs.size() < 2) {
26: 		// single argument: only the end is specified
27: 		result->start = 0;
28: 		result->end = inputs[0].GetValue<int64_t>();
29: 	} else {
30: 		// two arguments: first two arguments are start and end
31: 		result->start = inputs[0].GetValue<int64_t>();
32: 		result->end = inputs[1].GetValue<int64_t>();
33: 	}
34: 	if (inputs.size() < 3) {
35: 		result->increment = 1;
36: 	} else {
37: 		result->increment = inputs[2].GetValue<int64_t>();
38: 	}
39: 	if (result->increment == 0) {
40: 		throw BinderException("interval cannot be 0!");
41: 	}
42: 	if (result->start > result->end && result->increment > 0) {
43: 		throw BinderException("start is bigger than end, but increment is positive: cannot generate infinite series");
44: 	} else if (result->start < result->end && result->increment < 0) {
45: 		throw BinderException("start is smaller than end, but increment is negative: cannot generate infinite series");
46: 	}
47: 	return_types.emplace_back(LogicalType::BIGINT);
48: 	if (GENERATE_SERIES) {
49: 		// generate_series has inclusive bounds on the RHS
50: 		if (result->increment < 0) {
51: 			result->end = result->end - 1;
52: 		} else {
53: 			result->end = result->end + 1;
54: 		}
55: 		names.emplace_back("generate_series");
56: 	} else {
57: 		names.emplace_back("range");
58: 	}
59: 	return move(result);
60: }
61: 
62: struct RangeFunctionState : public FunctionOperatorData {
63: 	RangeFunctionState() : current_idx(0) {
64: 	}
65: 
66: 	int64_t current_idx;
67: };
68: 
69: static unique_ptr<FunctionOperatorData> RangeFunctionInit(ClientContext &context, const FunctionData *bind_data,
70:                                                           const vector<column_t> &column_ids,
71:                                                           TableFilterCollection *filters) {
72: 	return make_unique<RangeFunctionState>();
73: }
74: 
75: static void RangeFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *state_p,
76:                           DataChunk *input, DataChunk &output) {
77: 	auto &bind_data = (RangeFunctionBindData &)*bind_data_p;
78: 	auto &state = (RangeFunctionState &)*state_p;
79: 
80: 	auto increment = bind_data.increment;
81: 	auto end = bind_data.end;
82: 	hugeint_t current_value = bind_data.start + increment * state.current_idx;
83: 	int64_t current_value_i64;
84: 	if (!Hugeint::TryCast<int64_t>(current_value, current_value_i64)) {
85: 		return;
86: 	}
87: 	// set the result vector as a sequence vector
88: 	output.data[0].Sequence(current_value_i64, Hugeint::Cast<int64_t>(increment));
89: 	int64_t offset = increment < 0 ? 1 : -1;
90: 	idx_t remaining = MinValue<idx_t>(Hugeint::Cast<idx_t>((end - current_value + (increment + offset)) / increment),
91: 	                                  STANDARD_VECTOR_SIZE);
92: 	// increment the index pointer by the remaining count
93: 	state.current_idx += remaining;
94: 	output.SetCardinality(remaining);
95: }
96: 
97: unique_ptr<NodeStatistics> RangeCardinality(ClientContext &context, const FunctionData *bind_data_p) {
98: 	auto &bind_data = (RangeFunctionBindData &)*bind_data_p;
99: 	idx_t cardinality = Hugeint::Cast<idx_t>((bind_data.end - bind_data.start) / bind_data.increment);
100: 	return make_unique<NodeStatistics>(cardinality, cardinality);
101: }
102: 
103: //===--------------------------------------------------------------------===//
104: // Range (timestamp)
105: //===--------------------------------------------------------------------===//
106: struct RangeDateTimeBindData : public TableFunctionData {
107: 	timestamp_t start;
108: 	timestamp_t end;
109: 	interval_t increment;
110: 	bool inclusive_bound;
111: 	bool greater_than_check;
112: 
113: 	bool Finished(timestamp_t current_value) {
114: 		if (greater_than_check) {
115: 			if (inclusive_bound) {
116: 				return current_value > end;
117: 			} else {
118: 				return current_value >= end;
119: 			}
120: 		} else {
121: 			if (inclusive_bound) {
122: 				return current_value < end;
123: 			} else {
124: 				return current_value <= end;
125: 			}
126: 		}
127: 	}
128: };
129: 
130: template <bool GENERATE_SERIES>
131: static unique_ptr<FunctionData>
132: RangeDateTimeBind(ClientContext &context, vector<Value> &inputs, unordered_map<string, Value> &named_parameters,
133:                   vector<LogicalType> &input_table_types, vector<string> &input_table_names,
134:                   vector<LogicalType> &return_types, vector<string> &names) {
135: 	auto result = make_unique<RangeDateTimeBindData>();
136: 	D_ASSERT(inputs.size() == 3);
137: 	result->start = inputs[0].GetValue<timestamp_t>();
138: 	result->end = inputs[1].GetValue<timestamp_t>();
139: 	result->increment = inputs[2].GetValue<interval_t>();
140: 
141: 	if (result->increment.months == 0 && result->increment.days == 0 && result->increment.micros == 0) {
142: 		throw BinderException("interval cannot be 0!");
143: 	}
144: 	// all elements should point in the same direction
145: 	if (result->increment.months > 0 || result->increment.days > 0 || result->increment.micros > 0) {
146: 		if (result->increment.months < 0 || result->increment.days < 0 || result->increment.micros < 0) {
147: 			throw BinderException("RANGE with composite interval that has mixed signs is not supported");
148: 		}
149: 		result->greater_than_check = true;
150: 		if (result->start > result->end) {
151: 			throw BinderException(
152: 			    "start is bigger than end, but increment is positive: cannot generate infinite series");
153: 		}
154: 	} else {
155: 		result->greater_than_check = false;
156: 		if (result->start < result->end) {
157: 			throw BinderException(
158: 			    "start is smaller than end, but increment is negative: cannot generate infinite series");
159: 		}
160: 	}
161: 	return_types.push_back(inputs[0].type());
162: 	if (GENERATE_SERIES) {
163: 		// generate_series has inclusive bounds on the RHS
164: 		result->inclusive_bound = true;
165: 		names.emplace_back("generate_series");
166: 	} else {
167: 		result->inclusive_bound = false;
168: 		names.emplace_back("range");
169: 	}
170: 	return move(result);
171: }
172: 
173: struct RangeDateTimeState : public FunctionOperatorData {
174: 	explicit RangeDateTimeState(timestamp_t start_p) : current_state(start_p) {
175: 	}
176: 
177: 	timestamp_t current_state;
178: 	bool finished = false;
179: };
180: 
181: static unique_ptr<FunctionOperatorData> RangeDateTimeInit(ClientContext &context, const FunctionData *bind_data_p,
182:                                                           const vector<column_t> &column_ids,
183:                                                           TableFilterCollection *filters) {
184: 	auto &bind_data = (RangeDateTimeBindData &)*bind_data_p;
185: 	return make_unique<RangeDateTimeState>(bind_data.start);
186: }
187: 
188: static void RangeDateTimeFunction(ClientContext &context, const FunctionData *bind_data_p,
189:                                   FunctionOperatorData *state_p, DataChunk *input, DataChunk &output) {
190: 	auto &bind_data = (RangeDateTimeBindData &)*bind_data_p;
191: 	auto &state = (RangeDateTimeState &)*state_p;
192: 	if (state.finished) {
193: 		return;
194: 	}
195: 
196: 	idx_t size = 0;
197: 	auto data = FlatVector::GetData<timestamp_t>(output.data[0]);
198: 	while (true) {
199: 		data[size++] = state.current_state;
200: 		state.current_state =
201: 		    AddOperator::Operation<timestamp_t, interval_t, timestamp_t>(state.current_state, bind_data.increment);
202: 		if (bind_data.Finished(state.current_state)) {
203: 			state.finished = true;
204: 			break;
205: 		}
206: 		if (size >= STANDARD_VECTOR_SIZE) {
207: 			break;
208: 		}
209: 	}
210: 	output.SetCardinality(size);
211: }
212: 
213: void RangeTableFunction::RegisterFunction(BuiltinFunctions &set) {
214: 	TableFunctionSet range("range");
215: 
216: 	// single argument range: (end) - implicit start = 0 and increment = 1
217: 	range.AddFunction(TableFunction({LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<false>, RangeFunctionInit,
218: 	                                nullptr, nullptr, nullptr, RangeCardinality));
219: 	// two arguments range: (start, end) - implicit increment = 1
220: 	range.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<false>,
221: 	                                RangeFunctionInit, nullptr, nullptr, nullptr, RangeCardinality));
222: 	// three arguments range: (start, end, increment)
223: 	range.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT, LogicalType::BIGINT}, RangeFunction,
224: 	                                RangeFunctionBind<false>, RangeFunctionInit, nullptr, nullptr, nullptr,
225: 	                                RangeCardinality));
226: 	range.AddFunction(TableFunction({LogicalType::TIMESTAMP, LogicalType::TIMESTAMP, LogicalType::INTERVAL},
227: 	                                RangeDateTimeFunction, RangeDateTimeBind<false>, RangeDateTimeInit));
228: 	set.AddFunction(range);
229: 	// generate_series: similar to range, but inclusive instead of exclusive bounds on the RHS
230: 	TableFunctionSet generate_series("generate_series");
231: 	generate_series.AddFunction(TableFunction({LogicalType::BIGINT}, RangeFunction, RangeFunctionBind<true>,
232: 	                                          RangeFunctionInit, nullptr, nullptr, nullptr, RangeCardinality));
233: 	generate_series.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT}, RangeFunction,
234: 	                                          RangeFunctionBind<true>, RangeFunctionInit, nullptr, nullptr, nullptr,
235: 	                                          RangeCardinality));
236: 	generate_series.AddFunction(TableFunction({LogicalType::BIGINT, LogicalType::BIGINT, LogicalType::BIGINT},
237: 	                                          RangeFunction, RangeFunctionBind<true>, RangeFunctionInit, nullptr,
238: 	                                          nullptr, nullptr, RangeCardinality));
239: 	generate_series.AddFunction(TableFunction({LogicalType::TIMESTAMP, LogicalType::TIMESTAMP, LogicalType::INTERVAL},
240: 	                                          RangeDateTimeFunction, RangeDateTimeBind<true>, RangeDateTimeInit));
241: 	set.AddFunction(generate_series);
242: }
243: 
244: void BuiltinFunctions::RegisterTableFunctions() {
245: 	CheckpointFunction::RegisterFunction(*this);
246: 	GlobTableFunction::RegisterFunction(*this);
247: 	RangeTableFunction::RegisterFunction(*this);
248: 	RepeatTableFunction::RegisterFunction(*this);
249: 	SummaryTableFunction::RegisterFunction(*this);
250: 	UnnestTableFunction::RegisterFunction(*this);
251: }
252: 
253: } // namespace duckdb
[end of src/function/table/range.cpp]
[start of src/function/table/read_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/main/database.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/main/config.hpp"
8: #include "duckdb/parser/expression/constant_expression.hpp"
9: #include "duckdb/parser/expression/function_expression.hpp"
10: #include "duckdb/parser/tableref/table_function_ref.hpp"
11: #include "duckdb/common/windows_undefs.hpp"
12: 
13: #include <limits>
14: 
15: namespace duckdb {
16: 
17: static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, vector<Value> &inputs,
18:                                             unordered_map<string, Value> &named_parameters,
19:                                             vector<LogicalType> &input_table_types, vector<string> &input_table_names,
20:                                             vector<LogicalType> &return_types, vector<string> &names) {
21: 	auto &config = DBConfig::GetConfig(context);
22: 	if (!config.enable_external_access) {
23: 		throw PermissionException("Scanning CSV files is disabled through configuration");
24: 	}
25: 	auto result = make_unique<ReadCSVData>();
26: 	auto &options = result->options;
27: 
28: 	auto &file_pattern = StringValue::Get(inputs[0]);
29: 
30: 	auto &fs = FileSystem::GetFileSystem(context);
31: 	result->files = fs.Glob(file_pattern);
32: 	if (result->files.empty()) {
33: 		throw IOException("No files found that match the pattern \"%s\"", file_pattern);
34: 	}
35: 
36: 	for (auto &kv : named_parameters) {
37: 		if (kv.first == "auto_detect") {
38: 			options.auto_detect = BooleanValue::Get(kv.second);
39: 		} else if (kv.first == "sep" || kv.first == "delim") {
40: 			options.SetDelimiter(StringValue::Get(kv.second));
41: 		} else if (kv.first == "header") {
42: 			options.header = BooleanValue::Get(kv.second);
43: 			options.has_header = true;
44: 		} else if (kv.first == "quote") {
45: 			options.quote = StringValue::Get(kv.second);
46: 			options.has_quote = true;
47: 		} else if (kv.first == "escape") {
48: 			options.escape = StringValue::Get(kv.second);
49: 			options.has_escape = true;
50: 		} else if (kv.first == "nullstr") {
51: 			options.null_str = StringValue::Get(kv.second);
52: 		} else if (kv.first == "sample_size") {
53: 			int64_t sample_size = kv.second.GetValue<int64_t>();
54: 			if (sample_size < 1 && sample_size != -1) {
55: 				throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
56: 			}
57: 			if (sample_size == -1) {
58: 				options.sample_chunks = std::numeric_limits<uint64_t>::max();
59: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
60: 			} else if (sample_size <= STANDARD_VECTOR_SIZE) {
61: 				options.sample_chunk_size = sample_size;
62: 				options.sample_chunks = 1;
63: 			} else {
64: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
65: 				options.sample_chunks = sample_size / STANDARD_VECTOR_SIZE;
66: 			}
67: 		} else if (kv.first == "sample_chunk_size") {
68: 			options.sample_chunk_size = kv.second.GetValue<int64_t>();
69: 			if (options.sample_chunk_size > STANDARD_VECTOR_SIZE) {
70: 				throw BinderException(
71: 				    "Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
72: 				    STANDARD_VECTOR_SIZE);
73: 			} else if (options.sample_chunk_size < 1) {
74: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be smaller than 1");
75: 			}
76: 		} else if (kv.first == "sample_chunks") {
77: 			options.sample_chunks = kv.second.GetValue<int64_t>();
78: 			if (options.sample_chunks < 1) {
79: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNKS: cannot be smaller than 1");
80: 			}
81: 		} else if (kv.first == "all_varchar") {
82: 			options.all_varchar = BooleanValue::Get(kv.second);
83: 		} else if (kv.first == "dateformat") {
84: 			options.has_format[LogicalTypeId::DATE] = true;
85: 			auto &date_format = options.date_format[LogicalTypeId::DATE];
86: 			date_format.format_specifier = StringValue::Get(kv.second);
87: 			string error = StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
88: 			if (!error.empty()) {
89: 				throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
90: 			}
91: 		} else if (kv.first == "timestampformat") {
92: 			options.has_format[LogicalTypeId::TIMESTAMP] = true;
93: 			auto &timestamp_format = options.date_format[LogicalTypeId::TIMESTAMP];
94: 			timestamp_format.format_specifier = StringValue::Get(kv.second);
95: 			string error = StrTimeFormat::ParseFormatSpecifier(timestamp_format.format_specifier, timestamp_format);
96: 			if (!error.empty()) {
97: 				throw InvalidInputException("Could not parse TIMESTAMPFORMAT: %s", error.c_str());
98: 			}
99: 		} else if (kv.first == "normalize_names") {
100: 			options.normalize_names = BooleanValue::Get(kv.second);
101: 		} else if (kv.first == "columns") {
102: 			auto &child_type = kv.second.type();
103: 			if (child_type.id() != LogicalTypeId::STRUCT) {
104: 				throw BinderException("read_csv columns requires a a struct as input");
105: 			}
106: 			auto &struct_children = StructValue::GetChildren(kv.second);
107: 			D_ASSERT(StructType::GetChildCount(child_type) == struct_children.size());
108: 			for (idx_t i = 0; i < struct_children.size(); i++) {
109: 				auto &name = StructType::GetChildName(child_type, i);
110: 				auto &val = struct_children[i];
111: 				names.push_back(name);
112: 				if (val.type().id() != LogicalTypeId::VARCHAR) {
113: 					throw BinderException("read_csv requires a type specification as string");
114: 				}
115: 				return_types.emplace_back(TransformStringToLogicalType(StringValue::Get(val)));
116: 			}
117: 			if (names.empty()) {
118: 				throw BinderException("read_csv requires at least a single column as input!");
119: 			}
120: 		} else if (kv.first == "compression") {
121: 			options.compression = FileCompressionTypeFromString(StringValue::Get(kv.second));
122: 		} else if (kv.first == "filename") {
123: 			result->include_file_name = BooleanValue::Get(kv.second);
124: 		} else if (kv.first == "skip") {
125: 			options.skip_rows = kv.second.GetValue<int64_t>();
126: 		}
127: 	}
128: 	if (!options.auto_detect && return_types.empty()) {
129: 		throw BinderException("read_csv requires columns to be specified. Use read_csv_auto or set read_csv(..., "
130: 		                      "AUTO_DETECT=TRUE) to automatically guess columns.");
131: 	}
132: 	if (options.auto_detect) {
133: 		options.file_path = result->files[0];
134: 		auto initial_reader = make_unique<BufferedCSVReader>(context, options);
135: 
136: 		return_types.assign(initial_reader->sql_types.begin(), initial_reader->sql_types.end());
137: 		if (names.empty()) {
138: 			names.assign(initial_reader->col_names.begin(), initial_reader->col_names.end());
139: 		} else {
140: 			D_ASSERT(return_types.size() == names.size());
141: 		}
142: 		result->initial_reader = move(initial_reader);
143: 	} else {
144: 		result->sql_types = return_types;
145: 		D_ASSERT(return_types.size() == names.size());
146: 	}
147: 	if (result->include_file_name) {
148: 		return_types.emplace_back(LogicalType::VARCHAR);
149: 		names.emplace_back("filename");
150: 	}
151: 	return move(result);
152: }
153: 
154: struct ReadCSVOperatorData : public FunctionOperatorData {
155: 	//! The CSV reader
156: 	unique_ptr<BufferedCSVReader> csv_reader;
157: 	//! The index of the next file to read (i.e. current file + 1)
158: 	idx_t file_index;
159: };
160: 
161: static unique_ptr<FunctionOperatorData> ReadCSVInit(ClientContext &context, const FunctionData *bind_data_p,
162:                                                     const vector<column_t> &column_ids,
163:                                                     TableFilterCollection *filters) {
164: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
165: 	auto result = make_unique<ReadCSVOperatorData>();
166: 	if (bind_data.initial_reader) {
167: 		result->csv_reader = move(bind_data.initial_reader);
168: 	} else {
169: 		bind_data.options.file_path = bind_data.files[0];
170: 		result->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);
171: 	}
172: 	bind_data.bytes_read = 0;
173: 	bind_data.file_size = result->csv_reader->GetFileSize();
174: 	result->file_index = 1;
175: 	return move(result);
176: }
177: 
178: static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, vector<Value> &inputs,
179:                                                 unordered_map<string, Value> &named_parameters,
180:                                                 vector<LogicalType> &input_table_types,
181:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
182:                                                 vector<string> &names) {
183: 	named_parameters["auto_detect"] = Value::BOOLEAN(true);
184: 	return ReadCSVBind(context, inputs, named_parameters, input_table_types, input_table_names, return_types, names);
185: }
186: 
187: static void ReadCSVFunction(ClientContext &context, const FunctionData *bind_data_p,
188:                             FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
189: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
190: 	auto &data = (ReadCSVOperatorData &)*operator_state;
191: 	do {
192: 		data.csv_reader->ParseCSV(output);
193: 		bind_data.bytes_read = data.csv_reader->bytes_in_chunk;
194: 		if (output.size() == 0 && data.file_index < bind_data.files.size()) {
195: 			// exhausted this file, but we have more files we can read
196: 			// open the next file and increment the counter
197: 			bind_data.options.file_path = bind_data.files[data.file_index];
198: 			data.csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, data.csv_reader->sql_types);
199: 			data.file_index++;
200: 		} else {
201: 			break;
202: 		}
203: 	} while (true);
204: 	if (bind_data.include_file_name) {
205: 		auto &col = output.data.back();
206: 		col.SetValue(0, Value(data.csv_reader->options.file_path));
207: 		col.SetVectorType(VectorType::CONSTANT_VECTOR);
208: 	}
209: }
210: 
211: static void ReadCSVAddNamedParameters(TableFunction &table_function) {
212: 	table_function.named_parameters["sep"] = LogicalType::VARCHAR;
213: 	table_function.named_parameters["delim"] = LogicalType::VARCHAR;
214: 	table_function.named_parameters["quote"] = LogicalType::VARCHAR;
215: 	table_function.named_parameters["escape"] = LogicalType::VARCHAR;
216: 	table_function.named_parameters["nullstr"] = LogicalType::VARCHAR;
217: 	table_function.named_parameters["columns"] = LogicalType::ANY;
218: 	table_function.named_parameters["header"] = LogicalType::BOOLEAN;
219: 	table_function.named_parameters["auto_detect"] = LogicalType::BOOLEAN;
220: 	table_function.named_parameters["sample_size"] = LogicalType::BIGINT;
221: 	table_function.named_parameters["sample_chunk_size"] = LogicalType::BIGINT;
222: 	table_function.named_parameters["sample_chunks"] = LogicalType::BIGINT;
223: 	table_function.named_parameters["all_varchar"] = LogicalType::BOOLEAN;
224: 	table_function.named_parameters["dateformat"] = LogicalType::VARCHAR;
225: 	table_function.named_parameters["timestampformat"] = LogicalType::VARCHAR;
226: 	table_function.named_parameters["normalize_names"] = LogicalType::BOOLEAN;
227: 	table_function.named_parameters["compression"] = LogicalType::VARCHAR;
228: 	table_function.named_parameters["filename"] = LogicalType::BOOLEAN;
229: 	table_function.named_parameters["skip"] = LogicalType::BIGINT;
230: }
231: 
232: double CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p) {
233: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
234: 	if (bind_data.file_size == 0) {
235: 		return 100;
236: 	}
237: 	auto percentage = (bind_data.bytes_read * 100.0) / bind_data.file_size;
238: 	return percentage;
239: }
240: 
241: TableFunction ReadCSVTableFunction::GetFunction() {
242: 	TableFunction read_csv("read_csv", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVBind, ReadCSVInit);
243: 	read_csv.table_scan_progress = CSVReaderProgress;
244: 	ReadCSVAddNamedParameters(read_csv);
245: 	return read_csv;
246: }
247: 
248: void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {
249: 	set.AddFunction(ReadCSVTableFunction::GetFunction());
250: 
251: 	TableFunction read_csv_auto("read_csv_auto", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVAutoBind, ReadCSVInit);
252: 	read_csv_auto.table_scan_progress = CSVReaderProgress;
253: 	ReadCSVAddNamedParameters(read_csv_auto);
254: 	set.AddFunction(read_csv_auto);
255: }
256: 
257: unique_ptr<TableFunctionRef> ReadCSVReplacement(const string &table_name, void *data) {
258: 	auto lower_name = StringUtil::Lower(table_name);
259: 	// remove any compression
260: 	if (StringUtil::EndsWith(lower_name, ".gz")) {
261: 		lower_name = lower_name.substr(0, lower_name.size() - 3);
262: 	} else if (StringUtil::EndsWith(lower_name, ".zst")) {
263: 		lower_name = lower_name.substr(0, lower_name.size() - 4);
264: 	}
265: 	if (!StringUtil::EndsWith(lower_name, ".csv") && !StringUtil::EndsWith(lower_name, ".tsv")) {
266: 		return nullptr;
267: 	}
268: 	auto table_function = make_unique<TableFunctionRef>();
269: 	vector<unique_ptr<ParsedExpression>> children;
270: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
271: 	table_function->function = make_unique<FunctionExpression>("read_csv_auto", move(children));
272: 	return table_function;
273: }
274: 
275: void BuiltinFunctions::RegisterReadFunctions() {
276: 	CSVCopyFunction::RegisterFunction(*this);
277: 	ReadCSVTableFunction::RegisterFunction(*this);
278: 
279: 	auto &config = DBConfig::GetConfig(context);
280: 	config.replacement_scans.emplace_back(ReadCSVReplacement);
281: }
282: 
283: } // namespace duckdb
[end of src/function/table/read_csv.cpp]
[start of src/function/table/repeat.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/common/algorithm.hpp"
3: 
4: namespace duckdb {
5: 
6: struct RepeatFunctionData : public TableFunctionData {
7: 	RepeatFunctionData(Value value, idx_t target_count) : value(move(value)), target_count(target_count) {
8: 	}
9: 
10: 	Value value;
11: 	idx_t target_count;
12: };
13: 
14: struct RepeatOperatorData : public FunctionOperatorData {
15: 	RepeatOperatorData() : current_count(0) {
16: 	}
17: 	idx_t current_count;
18: };
19: 
20: static unique_ptr<FunctionData> RepeatBind(ClientContext &context, vector<Value> &inputs,
21:                                            unordered_map<string, Value> &named_parameters,
22:                                            vector<LogicalType> &input_table_types, vector<string> &input_table_names,
23:                                            vector<LogicalType> &return_types, vector<string> &names) {
24: 	// the repeat function returns the type of the first argument
25: 	return_types.push_back(inputs[0].type());
26: 	names.push_back(inputs[0].ToString());
27: 	return make_unique<RepeatFunctionData>(inputs[0], inputs[1].GetValue<int64_t>());
28: }
29: 
30: static unique_ptr<FunctionOperatorData> RepeatInit(ClientContext &context, const FunctionData *bind_data,
31:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
32: 	return make_unique<RepeatOperatorData>();
33: }
34: 
35: static void RepeatFunction(ClientContext &context, const FunctionData *bind_data_p,
36:                            FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
37: 	auto &bind_data = (RepeatFunctionData &)*bind_data_p;
38: 	auto &state = (RepeatOperatorData &)*operator_state;
39: 
40: 	idx_t remaining = MinValue<idx_t>(bind_data.target_count - state.current_count, STANDARD_VECTOR_SIZE);
41: 	output.data[0].Reference(bind_data.value);
42: 	output.SetCardinality(remaining);
43: 	state.current_count += remaining;
44: }
45: 
46: static unique_ptr<NodeStatistics> RepeatCardinality(ClientContext &context, const FunctionData *bind_data_p) {
47: 	auto &bind_data = (RepeatFunctionData &)*bind_data_p;
48: 	return make_unique<NodeStatistics>(bind_data.target_count, bind_data.target_count);
49: }
50: 
51: void RepeatTableFunction::RegisterFunction(BuiltinFunctions &set) {
52: 	TableFunction repeat("repeat", {LogicalType::ANY, LogicalType::BIGINT}, RepeatFunction, RepeatBind, RepeatInit,
53: 	                     nullptr, nullptr, nullptr, RepeatCardinality);
54: 	set.AddFunction(repeat);
55: }
56: 
57: } // namespace duckdb
[end of src/function/table/repeat.cpp]
[start of src/function/table/summary.cpp]
1: #include "duckdb/function/table/summary.hpp"
2: #include "duckdb/function/table_function.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/common/file_system.hpp"
5: 
6: // this function makes not that much sense on its own but is a demo for table-parameter table-producing functions
7: 
8: namespace duckdb {
9: 
10: static unique_ptr<FunctionData> SummaryFunctionBind(ClientContext &context, vector<Value> &inputs,
11:                                                     unordered_map<string, Value> &named_parameters,
12:                                                     vector<LogicalType> &input_table_types,
13:                                                     vector<string> &input_table_names,
14:                                                     vector<LogicalType> &return_types, vector<string> &names) {
15: 
16: 	return_types.emplace_back(LogicalType::VARCHAR);
17: 	names.emplace_back("summary");
18: 
19: 	for (idx_t i = 0; i < input_table_types.size(); i++) {
20: 		return_types.push_back(input_table_types[i]);
21: 		names.emplace_back(input_table_names[i]);
22: 	}
23: 
24: 	return make_unique<TableFunctionData>();
25: }
26: 
27: static void SummaryFunction(ClientContext &context, const FunctionData *bind_data_p, FunctionOperatorData *state_p,
28:                             DataChunk *input, DataChunk &output) {
29: 	D_ASSERT(input);
30: 	output.SetCardinality(input->size());
31: 
32: 	for (idx_t row_idx = 0; row_idx < input->size(); row_idx++) {
33: 		string summary_val = "[";
34: 
35: 		for (idx_t col_idx = 0; col_idx < input->ColumnCount(); col_idx++) {
36: 			summary_val += input->GetValue(col_idx, row_idx).ToString();
37: 			if (col_idx < input->ColumnCount() - 1) {
38: 				summary_val += ", ";
39: 			}
40: 		}
41: 		summary_val += "]";
42: 		output.SetValue(0, row_idx, Value(summary_val));
43: 	}
44: 	for (idx_t col_idx = 0; col_idx < input->ColumnCount(); col_idx++) {
45: 		output.data[col_idx + 1].Reference(input->data[col_idx]);
46: 	}
47: }
48: 
49: void SummaryTableFunction::RegisterFunction(BuiltinFunctions &set) {
50: 	TableFunctionSet summary("summary");
51: 	summary.AddFunction(TableFunction({LogicalType::TABLE}, SummaryFunction, SummaryFunctionBind));
52: 	set.AddFunction(summary);
53: }
54: 
55: } // namespace duckdb
[end of src/function/table/summary.cpp]
[start of src/function/table/system/duckdb_columns.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/parser/constraints/not_null_constraint.hpp"
9: 
10: #include <set>
11: 
12: namespace duckdb {
13: 
14: struct DuckDBColumnsData : public FunctionOperatorData {
15: 	DuckDBColumnsData() : offset(0), column_offset(0) {
16: 	}
17: 
18: 	vector<CatalogEntry *> entries;
19: 	idx_t offset;
20: 	idx_t column_offset;
21: };
22: 
23: static unique_ptr<FunctionData> DuckDBColumnsBind(ClientContext &context, vector<Value> &inputs,
24:                                                   unordered_map<string, Value> &named_parameters,
25:                                                   vector<LogicalType> &input_table_types,
26:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
27:                                                   vector<string> &names) {
28: 	names.emplace_back("schema_oid");
29: 	return_types.emplace_back(LogicalType::BIGINT);
30: 
31: 	names.emplace_back("schema_name");
32: 	return_types.emplace_back(LogicalType::VARCHAR);
33: 
34: 	names.emplace_back("table_oid");
35: 	return_types.emplace_back(LogicalType::BIGINT);
36: 
37: 	names.emplace_back("table_name");
38: 	return_types.emplace_back(LogicalType::VARCHAR);
39: 
40: 	names.emplace_back("column_name");
41: 	return_types.emplace_back(LogicalType::VARCHAR);
42: 
43: 	names.emplace_back("column_index");
44: 	return_types.emplace_back(LogicalType::INTEGER);
45: 
46: 	names.emplace_back("internal");
47: 	return_types.emplace_back(LogicalType::BOOLEAN);
48: 
49: 	names.emplace_back("column_default");
50: 	return_types.emplace_back(LogicalType::VARCHAR);
51: 
52: 	names.emplace_back("is_nullable");
53: 	return_types.emplace_back(LogicalType::BOOLEAN);
54: 
55: 	names.emplace_back("data_type");
56: 	return_types.emplace_back(LogicalType::VARCHAR);
57: 
58: 	names.emplace_back("data_type_id");
59: 	return_types.emplace_back(LogicalType::BIGINT);
60: 
61: 	names.emplace_back("character_maximum_length");
62: 	return_types.emplace_back(LogicalType::INTEGER);
63: 
64: 	names.emplace_back("numeric_precision");
65: 	return_types.emplace_back(LogicalType::INTEGER);
66: 
67: 	names.emplace_back("numeric_precision_radix");
68: 	return_types.emplace_back(LogicalType::INTEGER);
69: 
70: 	names.emplace_back("numeric_scale");
71: 	return_types.emplace_back(LogicalType::INTEGER);
72: 
73: 	return nullptr;
74: }
75: 
76: unique_ptr<FunctionOperatorData> DuckDBColumnsInit(ClientContext &context, const FunctionData *bind_data,
77:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
78: 	auto result = make_unique<DuckDBColumnsData>();
79: 
80: 	// scan all the schemas for tables and views and collect them
81: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
82: 	for (auto &schema : schemas) {
83: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
84: 	}
85: 
86: 	// check the temp schema as well
87: 	context.temporary_objects->Scan(context, CatalogType::TABLE_ENTRY,
88: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
89: 	return move(result);
90: }
91: 
92: namespace { // anonymous namespace for the ColumnHelper classes for working with tables/views
93: 
94: class ColumnHelper {
95: public:
96: 	static unique_ptr<ColumnHelper> Create(CatalogEntry *entry);
97: 
98: 	virtual ~ColumnHelper() {
99: 	}
100: 
101: 	virtual StandardEntry *Entry() = 0;
102: 	virtual idx_t NumColumns() = 0;
103: 	virtual const string &ColumnName(idx_t col) = 0;
104: 	virtual const LogicalType &ColumnType(idx_t col) = 0;
105: 	virtual const Value ColumnDefault(idx_t col) = 0;
106: 	virtual bool IsNullable(idx_t col) = 0;
107: 
108: 	void WriteColumns(idx_t index, idx_t start_col, idx_t end_col, DataChunk &output);
109: };
110: 
111: class TableColumnHelper : public ColumnHelper {
112: public:
113: 	explicit TableColumnHelper(TableCatalogEntry *entry) : entry(entry) {
114: 		for (auto &constraint : entry->constraints) {
115: 			if (constraint->type == ConstraintType::NOT_NULL) {
116: 				auto &not_null = *reinterpret_cast<NotNullConstraint *>(constraint.get());
117: 				not_null_cols.insert(not_null.index);
118: 			}
119: 		}
120: 	}
121: 
122: 	StandardEntry *Entry() override {
123: 		return entry;
124: 	}
125: 	idx_t NumColumns() override {
126: 		return entry->columns.size();
127: 	}
128: 	const string &ColumnName(idx_t col) override {
129: 		return entry->columns[col].name;
130: 	}
131: 	const LogicalType &ColumnType(idx_t col) override {
132: 		return entry->columns[col].type;
133: 	}
134: 	const Value ColumnDefault(idx_t col) override {
135: 		if (entry->columns[col].default_value) {
136: 			return Value(entry->columns[col].default_value->ToString());
137: 		}
138: 		return Value();
139: 	}
140: 	bool IsNullable(idx_t col) override {
141: 		return not_null_cols.find(col) == not_null_cols.end();
142: 	}
143: 
144: private:
145: 	TableCatalogEntry *entry;
146: 	std::set<idx_t> not_null_cols;
147: };
148: 
149: class ViewColumnHelper : public ColumnHelper {
150: public:
151: 	explicit ViewColumnHelper(ViewCatalogEntry *entry) : entry(entry) {
152: 	}
153: 
154: 	StandardEntry *Entry() override {
155: 		return entry;
156: 	}
157: 	idx_t NumColumns() override {
158: 		return entry->types.size();
159: 	}
160: 	const string &ColumnName(idx_t col) override {
161: 		return entry->aliases[col];
162: 	}
163: 	const LogicalType &ColumnType(idx_t col) override {
164: 		return entry->types[col];
165: 	}
166: 	const Value ColumnDefault(idx_t col) override {
167: 		return Value();
168: 	}
169: 	bool IsNullable(idx_t col) override {
170: 		return true;
171: 	}
172: 
173: private:
174: 	ViewCatalogEntry *entry;
175: };
176: 
177: unique_ptr<ColumnHelper> ColumnHelper::Create(CatalogEntry *entry) {
178: 	switch (entry->type) {
179: 	case CatalogType::TABLE_ENTRY:
180: 		return make_unique<TableColumnHelper>((TableCatalogEntry *)entry);
181: 	case CatalogType::VIEW_ENTRY:
182: 		return make_unique<ViewColumnHelper>((ViewCatalogEntry *)entry);
183: 	default:
184: 		throw NotImplementedException("Unsupported catalog type for duckdb_columns");
185: 	}
186: }
187: 
188: void ColumnHelper::WriteColumns(idx_t start_index, idx_t start_col, idx_t end_col, DataChunk &output) {
189: 	for (idx_t i = start_col; i < end_col; i++) {
190: 		auto index = start_index + (i - start_col);
191: 		auto &entry = *Entry();
192: 
193: 		// schema_oid, BIGINT
194: 		output.SetValue(0, index, Value::BIGINT(entry.schema->oid));
195: 		// schema_name, VARCHAR
196: 		output.SetValue(1, index, entry.schema->name);
197: 		// table_oid, BIGINT
198: 		output.SetValue(2, index, Value::BIGINT(entry.oid));
199: 		// table_name, VARCHAR
200: 		output.SetValue(3, index, entry.name);
201: 		// column_name, VARCHAR
202: 		output.SetValue(4, index, Value(ColumnName(i)));
203: 		// column_index, INTEGER
204: 		output.SetValue(5, index, Value::INTEGER(i + 1));
205: 		// internal, BOOLEAN
206: 		output.SetValue(6, index, Value::BOOLEAN(entry.internal));
207: 		// column_default, VARCHAR
208: 		output.SetValue(7, index, Value(ColumnDefault(i)));
209: 		// is_nullable, BOOLEAN
210: 		output.SetValue(8, index, Value::BOOLEAN(IsNullable(i)));
211: 		// data_type, VARCHAR
212: 		const LogicalType &type = ColumnType(i);
213: 		output.SetValue(9, index, Value(type.ToString()));
214: 		// data_type_id, BIGINT
215: 		output.SetValue(10, index, Value::BIGINT(int(type.id())));
216: 		if (type == LogicalType::VARCHAR) {
217: 			// FIXME: need check constraints in place to set this correctly
218: 			// character_maximum_length, INTEGER
219: 			output.SetValue(11, index, Value());
220: 		} else {
221: 			// "character_maximum_length", PhysicalType::INTEGER
222: 			output.SetValue(11, index, Value());
223: 		}
224: 
225: 		Value numeric_precision, numeric_scale, numeric_precision_radix;
226: 		switch (type.id()) {
227: 		case LogicalTypeId::DECIMAL:
228: 			numeric_precision = Value::INTEGER(DecimalType::GetWidth(type));
229: 			numeric_scale = Value::INTEGER(DecimalType::GetScale(type));
230: 			numeric_precision_radix = Value::INTEGER(10);
231: 			break;
232: 		case LogicalTypeId::HUGEINT:
233: 			numeric_precision = Value::INTEGER(128);
234: 			numeric_scale = Value::INTEGER(0);
235: 			numeric_precision_radix = Value::INTEGER(2);
236: 			break;
237: 		case LogicalTypeId::BIGINT:
238: 			numeric_precision = Value::INTEGER(64);
239: 			numeric_scale = Value::INTEGER(0);
240: 			numeric_precision_radix = Value::INTEGER(2);
241: 			break;
242: 		case LogicalTypeId::INTEGER:
243: 			numeric_precision = Value::INTEGER(32);
244: 			numeric_scale = Value::INTEGER(0);
245: 			numeric_precision_radix = Value::INTEGER(2);
246: 			break;
247: 		case LogicalTypeId::SMALLINT:
248: 			numeric_precision = Value::INTEGER(16);
249: 			numeric_scale = Value::INTEGER(0);
250: 			numeric_precision_radix = Value::INTEGER(2);
251: 			break;
252: 		case LogicalTypeId::TINYINT:
253: 			numeric_precision = Value::INTEGER(8);
254: 			numeric_scale = Value::INTEGER(0);
255: 			numeric_precision_radix = Value::INTEGER(2);
256: 			break;
257: 		case LogicalTypeId::FLOAT:
258: 			numeric_precision = Value::INTEGER(24);
259: 			numeric_scale = Value::INTEGER(0);
260: 			numeric_precision_radix = Value::INTEGER(2);
261: 			break;
262: 		case LogicalTypeId::DOUBLE:
263: 			numeric_precision = Value::INTEGER(53);
264: 			numeric_scale = Value::INTEGER(0);
265: 			numeric_precision_radix = Value::INTEGER(2);
266: 			break;
267: 		default:
268: 			numeric_precision = Value();
269: 			numeric_scale = Value();
270: 			numeric_precision_radix = Value();
271: 			break;
272: 		}
273: 
274: 		// numeric_precision, INTEGER
275: 		output.SetValue(12, index, numeric_precision);
276: 		// numeric_precision_radix, INTEGER
277: 		output.SetValue(13, index, numeric_precision_radix);
278: 		// numeric_scale, INTEGER
279: 		output.SetValue(14, index, numeric_scale);
280: 	}
281: }
282: 
283: } // anonymous namespace
284: 
285: void DuckDBColumnsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
286:                            DataChunk *input, DataChunk &output) {
287: 	auto &data = (DuckDBColumnsData &)*operator_state;
288: 	if (data.offset >= data.entries.size()) {
289: 		// finished returning values
290: 		return;
291: 	}
292: 
293: 	// We need to track the offset of the relation we're writing as well as the last column
294: 	// we wrote from that relation (if any); it's possible that we can fill up the output
295: 	// with a partial list of columns from a relation and will need to pick up processing the
296: 	// next chunk at the same spot.
297: 	idx_t next = data.offset;
298: 	idx_t column_offset = data.column_offset;
299: 	idx_t index = 0;
300: 	while (next < data.entries.size() && index < STANDARD_VECTOR_SIZE) {
301: 		auto column_helper = ColumnHelper::Create(data.entries[next]);
302: 		idx_t columns = column_helper->NumColumns();
303: 
304: 		// Check to see if we are going to exceed the maximum index for a DataChunk
305: 		if (index + (columns - column_offset) > STANDARD_VECTOR_SIZE) {
306: 			idx_t column_limit = column_offset + (STANDARD_VECTOR_SIZE - index);
307: 			output.SetCardinality(STANDARD_VECTOR_SIZE);
308: 			column_helper->WriteColumns(index, column_offset, column_limit, output);
309: 
310: 			// Make the current column limit the column offset when we process the next chunk
311: 			column_offset = column_limit;
312: 			break;
313: 		} else {
314: 			// Otherwise, write all of the columns from the current relation and
315: 			// then move on to the next one.
316: 			output.SetCardinality(index + (columns - column_offset));
317: 			column_helper->WriteColumns(index, column_offset, columns, output);
318: 			index += columns - column_offset;
319: 			next++;
320: 			column_offset = 0;
321: 		}
322: 	}
323: 	data.offset = next;
324: 	data.column_offset = column_offset;
325: }
326: 
327: void DuckDBColumnsFun::RegisterFunction(BuiltinFunctions &set) {
328: 	set.AddFunction(TableFunction("duckdb_columns", {}, DuckDBColumnsFunction, DuckDBColumnsBind, DuckDBColumnsInit));
329: }
330: 
331: } // namespace duckdb
[end of src/function/table/system/duckdb_columns.cpp]
[start of src/function/table/system/duckdb_constraints.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/parser/constraint.hpp"
9: #include "duckdb/parser/constraints/check_constraint.hpp"
10: #include "duckdb/parser/constraints/unique_constraint.hpp"
11: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
12: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
13: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
14: #include "duckdb/storage/data_table.hpp"
15: 
16: namespace duckdb {
17: 
18: struct DuckDBConstraintsData : public FunctionOperatorData {
19: 	DuckDBConstraintsData() : offset(0), constraint_offset(0) {
20: 	}
21: 
22: 	vector<CatalogEntry *> entries;
23: 	idx_t offset;
24: 	idx_t constraint_offset;
25: };
26: 
27: static unique_ptr<FunctionData> DuckDBConstraintsBind(ClientContext &context, vector<Value> &inputs,
28:                                                       unordered_map<string, Value> &named_parameters,
29:                                                       vector<LogicalType> &input_table_types,
30:                                                       vector<string> &input_table_names,
31:                                                       vector<LogicalType> &return_types, vector<string> &names) {
32: 	names.emplace_back("schema_name");
33: 	return_types.emplace_back(LogicalType::VARCHAR);
34: 
35: 	names.emplace_back("schema_oid");
36: 	return_types.emplace_back(LogicalType::BIGINT);
37: 
38: 	names.emplace_back("table_name");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("table_oid");
42: 	return_types.emplace_back(LogicalType::BIGINT);
43: 
44: 	names.emplace_back("constraint_index");
45: 	return_types.emplace_back(LogicalType::BIGINT);
46: 
47: 	// CHECK, PRIMARY KEY or UNIQUE
48: 	names.emplace_back("constraint_type");
49: 	return_types.emplace_back(LogicalType::VARCHAR);
50: 
51: 	names.emplace_back("constraint_text");
52: 	return_types.emplace_back(LogicalType::VARCHAR);
53: 
54: 	names.emplace_back("expression");
55: 	return_types.emplace_back(LogicalType::VARCHAR);
56: 
57: 	names.emplace_back("constraint_column_indexes");
58: 	;
59: 	return_types.push_back(LogicalType::LIST(LogicalType::BIGINT));
60: 
61: 	names.emplace_back("constraint_column_names");
62: 	return_types.push_back(LogicalType::LIST(LogicalType::VARCHAR));
63: 
64: 	return nullptr;
65: }
66: 
67: unique_ptr<FunctionOperatorData> DuckDBConstraintsInit(ClientContext &context, const FunctionData *bind_data,
68:                                                        const vector<column_t> &column_ids,
69:                                                        TableFilterCollection *filters) {
70: 	auto result = make_unique<DuckDBConstraintsData>();
71: 
72: 	// scan all the schemas for tables and collect themand collect them
73: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
74: 	for (auto &schema : schemas) {
75: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
76: 	};
77: 
78: 	// check the temp schema as well
79: 	context.temporary_objects->Scan(context, CatalogType::TABLE_ENTRY,
80: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
81: 	return move(result);
82: }
83: 
84: void DuckDBConstraintsFunction(ClientContext &context, const FunctionData *bind_data,
85:                                FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
86: 	auto &data = (DuckDBConstraintsData &)*operator_state;
87: 	if (data.offset >= data.entries.size()) {
88: 		// finished returning values
89: 		return;
90: 	}
91: 	// start returning values
92: 	// either fill up the chunk or return all the remaining columns
93: 	idx_t count = 0;
94: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
95: 		auto &entry = data.entries[data.offset];
96: 
97: 		if (entry->type != CatalogType::TABLE_ENTRY) {
98: 			data.offset++;
99: 			continue;
100: 		}
101: 
102: 		auto &table = (TableCatalogEntry &)*entry;
103: 		for (; data.constraint_offset < table.constraints.size() && count < STANDARD_VECTOR_SIZE;
104: 		     data.constraint_offset++) {
105: 			auto &constraint = table.constraints[data.constraint_offset];
106: 			// return values:
107: 			// schema_name, LogicalType::VARCHAR
108: 			output.SetValue(0, count, Value(table.schema->name));
109: 			// schema_oid, LogicalType::BIGINT
110: 			output.SetValue(1, count, Value::BIGINT(table.schema->oid));
111: 			// table_name, LogicalType::VARCHAR
112: 			output.SetValue(2, count, Value(table.name));
113: 			// table_oid, LogicalType::BIGINT
114: 			output.SetValue(3, count, Value::BIGINT(table.oid));
115: 
116: 			// constraint_index, BIGINT
117: 			output.SetValue(4, count, Value::BIGINT(data.constraint_offset));
118: 
119: 			// constraint_type, VARCHAR
120: 			string constraint_type;
121: 			switch (constraint->type) {
122: 			case ConstraintType::CHECK:
123: 				constraint_type = "CHECK";
124: 				break;
125: 			case ConstraintType::UNIQUE: {
126: 				auto &unique = (UniqueConstraint &)*constraint;
127: 				constraint_type = unique.is_primary_key ? "PRIMARY KEY" : "UNIQUE";
128: 				break;
129: 			}
130: 			case ConstraintType::NOT_NULL:
131: 				constraint_type = "NOT NULL";
132: 				break;
133: 			case ConstraintType::FOREIGN_KEY:
134: 				constraint_type = "FOREIGN KEY";
135: 				break;
136: 			default:
137: 				throw NotImplementedException("Unimplemented constraint for duckdb_constraints");
138: 			}
139: 			output.SetValue(5, count, Value(constraint_type));
140: 
141: 			// constraint_text, VARCHAR
142: 			output.SetValue(6, count, Value(constraint->ToString()));
143: 
144: 			// expression, VARCHAR
145: 			Value expression_text;
146: 			if (constraint->type == ConstraintType::CHECK) {
147: 				auto &check = (CheckConstraint &)*constraint;
148: 				expression_text = Value(check.expression->ToString());
149: 			}
150: 			output.SetValue(7, count, expression_text);
151: 
152: 			auto &bound_constraint = (BoundConstraint &)*table.bound_constraints[data.constraint_offset];
153: 			vector<column_t> column_index_list;
154: 			switch (bound_constraint.type) {
155: 			case ConstraintType::CHECK: {
156: 				auto &bound_check = (BoundCheckConstraint &)bound_constraint;
157: 				for (auto &col_idx : bound_check.bound_columns) {
158: 					column_index_list.push_back(col_idx);
159: 				}
160: 				break;
161: 			}
162: 			case ConstraintType::UNIQUE: {
163: 				auto &bound_unique = (BoundUniqueConstraint &)bound_constraint;
164: 				for (auto &col_idx : bound_unique.keys) {
165: 					column_index_list.push_back(column_t(col_idx));
166: 				}
167: 				break;
168: 			}
169: 			case ConstraintType::NOT_NULL: {
170: 				auto &bound_not_null = (BoundNotNullConstraint &)bound_constraint;
171: 				column_index_list.push_back(bound_not_null.index);
172: 				break;
173: 			}
174: 			case ConstraintType::FOREIGN_KEY:
175: 			default:
176: 				throw NotImplementedException("Unimplemented constraint for duckdb_constraints");
177: 			}
178: 
179: 			vector<Value> index_list;
180: 			vector<Value> column_name_list;
181: 			for (auto column_index : column_index_list) {
182: 				index_list.push_back(Value::BIGINT(column_index));
183: 				column_name_list.emplace_back(table.columns[column_index].name);
184: 			}
185: 
186: 			// constraint_column_indexes, LIST
187: 			output.SetValue(8, count, Value::LIST(move(index_list)));
188: 
189: 			// constraint_column_names, LIST
190: 			output.SetValue(9, count, Value::LIST(move(column_name_list)));
191: 
192: 			count++;
193: 		}
194: 		if (data.constraint_offset >= table.constraints.size()) {
195: 			data.constraint_offset = 0;
196: 			data.offset++;
197: 		}
198: 	}
199: 	output.SetCardinality(count);
200: }
201: 
202: void DuckDBConstraintsFun::RegisterFunction(BuiltinFunctions &set) {
203: 	set.AddFunction(TableFunction("duckdb_constraints", {}, DuckDBConstraintsFunction, DuckDBConstraintsBind,
204: 	                              DuckDBConstraintsInit));
205: }
206: 
207: } // namespace duckdb
[end of src/function/table/system/duckdb_constraints.cpp]
[start of src/function/table/system/duckdb_dependencies.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/dependency_manager.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/main/client_context.hpp"
7: 
8: namespace duckdb {
9: 
10: struct DependencyInformation {
11: 	CatalogEntry *object;
12: 	CatalogEntry *dependent;
13: 	DependencyType type;
14: };
15: 
16: struct DuckDBDependenciesData : public FunctionOperatorData {
17: 	DuckDBDependenciesData() : offset(0) {
18: 	}
19: 
20: 	vector<DependencyInformation> entries;
21: 	idx_t offset;
22: };
23: 
24: static unique_ptr<FunctionData> DuckDBDependenciesBind(ClientContext &context, vector<Value> &inputs,
25:                                                        unordered_map<string, Value> &named_parameters,
26:                                                        vector<LogicalType> &input_table_types,
27:                                                        vector<string> &input_table_names,
28:                                                        vector<LogicalType> &return_types, vector<string> &names) {
29: 	names.emplace_back("classid");
30: 	return_types.emplace_back(LogicalType::BIGINT);
31: 
32: 	names.emplace_back("objid");
33: 	return_types.emplace_back(LogicalType::BIGINT);
34: 
35: 	names.emplace_back("objsubid");
36: 	return_types.emplace_back(LogicalType::INTEGER);
37: 
38: 	names.emplace_back("refclassid");
39: 	return_types.emplace_back(LogicalType::BIGINT);
40: 
41: 	names.emplace_back("refobjid");
42: 	return_types.emplace_back(LogicalType::BIGINT);
43: 
44: 	names.emplace_back("refobjsubid");
45: 	return_types.emplace_back(LogicalType::INTEGER);
46: 
47: 	names.emplace_back("deptype");
48: 	return_types.emplace_back(LogicalType::VARCHAR);
49: 
50: 	return nullptr;
51: }
52: 
53: unique_ptr<FunctionOperatorData> DuckDBDependenciesInit(ClientContext &context, const FunctionData *bind_data,
54:                                                         const vector<column_t> &column_ids,
55:                                                         TableFilterCollection *filters) {
56: 	auto result = make_unique<DuckDBDependenciesData>();
57: 
58: 	// scan all the schemas and collect them
59: 	auto &catalog = Catalog::GetCatalog(context);
60: 	auto &dependency_manager = catalog.GetDependencyManager();
61: 	dependency_manager.Scan([&](CatalogEntry *obj, CatalogEntry *dependent, DependencyType type) {
62: 		DependencyInformation info;
63: 		info.object = obj;
64: 		info.dependent = dependent;
65: 		info.type = type;
66: 		result->entries.push_back(info);
67: 	});
68: 
69: 	return move(result);
70: }
71: 
72: void DuckDBDependenciesFunction(ClientContext &context, const FunctionData *bind_data,
73:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
74: 	auto &data = (DuckDBDependenciesData &)*operator_state;
75: 	if (data.offset >= data.entries.size()) {
76: 		// finished returning values
77: 		return;
78: 	}
79: 	// start returning values
80: 	// either fill up the chunk or return all the remaining columns
81: 	idx_t count = 0;
82: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
83: 		auto &entry = data.entries[data.offset];
84: 
85: 		// return values:
86: 		// classid, LogicalType::BIGINT
87: 		output.SetValue(0, count, Value::BIGINT(0));
88: 		// objid, LogicalType::BIGINT
89: 		output.SetValue(1, count, Value::BIGINT(entry.object->oid));
90: 		// objsubid, LogicalType::INTEGER
91: 		output.SetValue(2, count, Value::INTEGER(0));
92: 		// refclassid, LogicalType::BIGINT
93: 		output.SetValue(3, count, Value::BIGINT(0));
94: 		// refobjid, LogicalType::BIGINT
95: 		output.SetValue(4, count, Value::BIGINT(entry.dependent->oid));
96: 		// refobjsubid, LogicalType::INTEGER
97: 		output.SetValue(5, count, Value::INTEGER(0));
98: 		// deptype, LogicalType::VARCHAR
99: 		string dependency_type_str;
100: 		switch (entry.type) {
101: 		case DependencyType::DEPENDENCY_REGULAR:
102: 			dependency_type_str = "n";
103: 			break;
104: 		case DependencyType::DEPENDENCY_AUTOMATIC:
105: 			dependency_type_str = "a";
106: 			break;
107: 		default:
108: 			throw NotImplementedException("Unimplemented dependency type");
109: 		}
110: 		output.SetValue(6, count, Value(dependency_type_str));
111: 
112: 		data.offset++;
113: 		count++;
114: 	}
115: 	output.SetCardinality(count);
116: }
117: 
118: void DuckDBDependenciesFun::RegisterFunction(BuiltinFunctions &set) {
119: 	set.AddFunction(TableFunction("duckdb_dependencies", {}, DuckDBDependenciesFunction, DuckDBDependenciesBind,
120: 	                              DuckDBDependenciesInit));
121: }
122: 
123: } // namespace duckdb
[end of src/function/table/system/duckdb_dependencies.cpp]
[start of src/function/table/system/duckdb_functions.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
7: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
8: #include "duckdb/catalog/catalog_entry/table_function_catalog_entry.hpp"
9: #include "duckdb/catalog/catalog_entry/pragma_function_catalog_entry.hpp"
10: #include "duckdb/parser/expression/columnref_expression.hpp"
11: #include "duckdb/common/algorithm.hpp"
12: 
13: namespace duckdb {
14: 
15: struct DuckDBFunctionsData : public FunctionOperatorData {
16: 	DuckDBFunctionsData() : offset(0), offset_in_entry(0) {
17: 	}
18: 
19: 	vector<CatalogEntry *> entries;
20: 	idx_t offset;
21: 	idx_t offset_in_entry;
22: };
23: 
24: static unique_ptr<FunctionData> DuckDBFunctionsBind(ClientContext &context, vector<Value> &inputs,
25:                                                     unordered_map<string, Value> &named_parameters,
26:                                                     vector<LogicalType> &input_table_types,
27:                                                     vector<string> &input_table_names,
28:                                                     vector<LogicalType> &return_types, vector<string> &names) {
29: 	names.emplace_back("schema_name");
30: 	return_types.emplace_back(LogicalType::VARCHAR);
31: 
32: 	names.emplace_back("function_name");
33: 	return_types.emplace_back(LogicalType::VARCHAR);
34: 
35: 	names.emplace_back("function_type");
36: 	return_types.emplace_back(LogicalType::VARCHAR);
37: 
38: 	names.emplace_back("description");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("return_type");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("parameters");
45: 	return_types.push_back(LogicalType::LIST(LogicalType::VARCHAR));
46: 
47: 	names.emplace_back("parameter_types");
48: 	return_types.push_back(LogicalType::LIST(LogicalType::VARCHAR));
49: 
50: 	names.emplace_back("varargs");
51: 	return_types.emplace_back(LogicalType::VARCHAR);
52: 
53: 	names.emplace_back("macro_definition");
54: 	return_types.emplace_back(LogicalType::VARCHAR);
55: 
56: 	return nullptr;
57: }
58: 
59: static void ExtractFunctionsFromSchema(ClientContext &context, SchemaCatalogEntry &schema,
60:                                        DuckDBFunctionsData &result) {
61: 	schema.Scan(context, CatalogType::SCALAR_FUNCTION_ENTRY,
62: 	            [&](CatalogEntry *entry) { result.entries.push_back(entry); });
63: 	schema.Scan(context, CatalogType::TABLE_FUNCTION_ENTRY,
64: 	            [&](CatalogEntry *entry) { result.entries.push_back(entry); });
65: 	schema.Scan(context, CatalogType::PRAGMA_FUNCTION_ENTRY,
66: 	            [&](CatalogEntry *entry) { result.entries.push_back(entry); });
67: }
68: 
69: unique_ptr<FunctionOperatorData> DuckDBFunctionsInit(ClientContext &context, const FunctionData *bind_data,
70:                                                      const vector<column_t> &column_ids,
71:                                                      TableFilterCollection *filters) {
72: 	auto result = make_unique<DuckDBFunctionsData>();
73: 
74: 	// scan all the schemas for tables and collect themand collect them
75: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
76: 	for (auto &schema : schemas) {
77: 		ExtractFunctionsFromSchema(context, *schema, *result);
78: 	};
79: 	ExtractFunctionsFromSchema(context, *context.temporary_objects, *result);
80: 
81: 	std::sort(result->entries.begin(), result->entries.end(),
82: 	          [&](CatalogEntry *a, CatalogEntry *b) { return (int)a->type < (int)b->type; });
83: 	return move(result);
84: }
85: 
86: struct ScalarFunctionExtractor {
87: 	static idx_t FunctionCount(ScalarFunctionCatalogEntry &entry) {
88: 		return entry.functions.size();
89: 	}
90: 
91: 	static Value GetFunctionType() {
92: 		return Value("scalar");
93: 	}
94: 
95: 	static Value GetFunctionDescription(ScalarFunctionCatalogEntry &entry, idx_t offset) {
96: 		return Value();
97: 	}
98: 
99: 	static Value GetReturnType(ScalarFunctionCatalogEntry &entry, idx_t offset) {
100: 		return Value(entry.functions[offset].return_type.ToString());
101: 	}
102: 
103: 	static Value GetParameters(ScalarFunctionCatalogEntry &entry, idx_t offset) {
104: 		vector<Value> results;
105: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
106: 			results.emplace_back("col" + to_string(i));
107: 		}
108: 		return Value::LIST(LogicalType::VARCHAR, move(results));
109: 	}
110: 
111: 	static Value GetParameterTypes(ScalarFunctionCatalogEntry &entry, idx_t offset) {
112: 		vector<Value> results;
113: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
114: 			results.emplace_back(entry.functions[offset].arguments[i].ToString());
115: 		}
116: 		return Value::LIST(LogicalType::VARCHAR, move(results));
117: 	}
118: 
119: 	static Value GetVarArgs(ScalarFunctionCatalogEntry &entry, idx_t offset) {
120: 		return entry.functions[offset].varargs.id() == LogicalTypeId::INVALID
121: 		           ? Value()
122: 		           : Value(entry.functions[offset].varargs.ToString());
123: 	}
124: 
125: 	static Value GetMacroDefinition(ScalarFunctionCatalogEntry &entry, idx_t offset) {
126: 		return Value();
127: 	}
128: };
129: 
130: struct AggregateFunctionExtractor {
131: 	static idx_t FunctionCount(AggregateFunctionCatalogEntry &entry) {
132: 		return entry.functions.size();
133: 	}
134: 
135: 	static Value GetFunctionType() {
136: 		return Value("aggregate");
137: 	}
138: 
139: 	static Value GetFunctionDescription(AggregateFunctionCatalogEntry &entry, idx_t offset) {
140: 		return Value();
141: 	}
142: 
143: 	static Value GetReturnType(AggregateFunctionCatalogEntry &entry, idx_t offset) {
144: 		return Value(entry.functions[offset].return_type.ToString());
145: 	}
146: 
147: 	static Value GetParameters(AggregateFunctionCatalogEntry &entry, idx_t offset) {
148: 		vector<Value> results;
149: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
150: 			results.emplace_back("col" + to_string(i));
151: 		}
152: 		return Value::LIST(LogicalType::VARCHAR, move(results));
153: 	}
154: 
155: 	static Value GetParameterTypes(AggregateFunctionCatalogEntry &entry, idx_t offset) {
156: 		vector<Value> results;
157: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
158: 			results.emplace_back(entry.functions[offset].arguments[i].ToString());
159: 		}
160: 		return Value::LIST(LogicalType::VARCHAR, move(results));
161: 	}
162: 
163: 	static Value GetVarArgs(AggregateFunctionCatalogEntry &entry, idx_t offset) {
164: 		return entry.functions[offset].varargs.id() == LogicalTypeId::INVALID
165: 		           ? Value()
166: 		           : Value(entry.functions[offset].varargs.ToString());
167: 	}
168: 
169: 	static Value GetMacroDefinition(AggregateFunctionCatalogEntry &entry, idx_t offset) {
170: 		return Value();
171: 	}
172: };
173: 
174: struct MacroExtractor {
175: 	static idx_t FunctionCount(MacroCatalogEntry &entry) {
176: 		return 1;
177: 	}
178: 
179: 	static Value GetFunctionType() {
180: 		return Value("macro");
181: 	}
182: 
183: 	static Value GetFunctionDescription(MacroCatalogEntry &entry, idx_t offset) {
184: 		return Value();
185: 	}
186: 
187: 	static Value GetReturnType(MacroCatalogEntry &entry, idx_t offset) {
188: 		return Value();
189: 	}
190: 
191: 	static Value GetParameters(MacroCatalogEntry &entry, idx_t offset) {
192: 		vector<Value> results;
193: 		for (auto &param : entry.function->parameters) {
194: 			D_ASSERT(param->type == ExpressionType::COLUMN_REF);
195: 			auto &colref = (ColumnRefExpression &)*param;
196: 			results.emplace_back(colref.GetColumnName());
197: 		}
198: 		for (auto &param_entry : entry.function->default_parameters) {
199: 			results.emplace_back(param_entry.first);
200: 		}
201: 		return Value::LIST(LogicalType::VARCHAR, move(results));
202: 	}
203: 
204: 	static Value GetParameterTypes(MacroCatalogEntry &entry, idx_t offset) {
205: 		vector<Value> results;
206: 		for (idx_t i = 0; i < entry.function->parameters.size(); i++) {
207: 			results.emplace_back(LogicalType::VARCHAR);
208: 		}
209: 		for (idx_t i = 0; i < entry.function->default_parameters.size(); i++) {
210: 			results.emplace_back(LogicalType::VARCHAR);
211: 		}
212: 		return Value::LIST(LogicalType::VARCHAR, move(results));
213: 	}
214: 
215: 	static Value GetVarArgs(MacroCatalogEntry &entry, idx_t offset) {
216: 		return Value();
217: 	}
218: 
219: 	static Value GetMacroDefinition(MacroCatalogEntry &entry, idx_t offset) {
220: 		return entry.function->expression->ToString();
221: 	}
222: };
223: 
224: struct TableFunctionExtractor {
225: 	static idx_t FunctionCount(TableFunctionCatalogEntry &entry) {
226: 		return entry.functions.size();
227: 	}
228: 
229: 	static Value GetFunctionType() {
230: 		return Value("table");
231: 	}
232: 
233: 	static Value GetFunctionDescription(TableFunctionCatalogEntry &entry, idx_t offset) {
234: 		return Value();
235: 	}
236: 
237: 	static Value GetReturnType(TableFunctionCatalogEntry &entry, idx_t offset) {
238: 		return Value();
239: 	}
240: 
241: 	static Value GetParameters(TableFunctionCatalogEntry &entry, idx_t offset) {
242: 		vector<Value> results;
243: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
244: 			results.emplace_back("col" + to_string(i));
245: 		}
246: 		for (auto &param : entry.functions[offset].named_parameters) {
247: 			results.emplace_back(param.first);
248: 		}
249: 		return Value::LIST(LogicalType::VARCHAR, move(results));
250: 	}
251: 
252: 	static Value GetParameterTypes(TableFunctionCatalogEntry &entry, idx_t offset) {
253: 		vector<Value> results;
254: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
255: 			results.emplace_back(entry.functions[offset].arguments[i].ToString());
256: 		}
257: 		for (auto &param : entry.functions[offset].named_parameters) {
258: 			results.emplace_back(param.second.ToString());
259: 		}
260: 		return Value::LIST(LogicalType::VARCHAR, move(results));
261: 	}
262: 
263: 	static Value GetVarArgs(TableFunctionCatalogEntry &entry, idx_t offset) {
264: 		return entry.functions[offset].varargs.id() == LogicalTypeId::INVALID
265: 		           ? Value()
266: 		           : Value(entry.functions[offset].varargs.ToString());
267: 	}
268: 
269: 	static Value GetMacroDefinition(TableFunctionCatalogEntry &entry, idx_t offset) {
270: 		return Value();
271: 	}
272: };
273: 
274: struct PragmaFunctionExtractor {
275: 	static idx_t FunctionCount(PragmaFunctionCatalogEntry &entry) {
276: 		return entry.functions.size();
277: 	}
278: 
279: 	static Value GetFunctionType() {
280: 		return Value("pragma");
281: 	}
282: 
283: 	static Value GetFunctionDescription(PragmaFunctionCatalogEntry &entry, idx_t offset) {
284: 		return Value();
285: 	}
286: 
287: 	static Value GetReturnType(PragmaFunctionCatalogEntry &entry, idx_t offset) {
288: 		return Value();
289: 	}
290: 
291: 	static Value GetParameters(PragmaFunctionCatalogEntry &entry, idx_t offset) {
292: 		vector<Value> results;
293: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
294: 			results.emplace_back("col" + to_string(i));
295: 		}
296: 		for (auto &param : entry.functions[offset].named_parameters) {
297: 			results.emplace_back(param.first);
298: 		}
299: 		return Value::LIST(LogicalType::VARCHAR, move(results));
300: 	}
301: 
302: 	static Value GetParameterTypes(PragmaFunctionCatalogEntry &entry, idx_t offset) {
303: 		vector<Value> results;
304: 		for (idx_t i = 0; i < entry.functions[offset].arguments.size(); i++) {
305: 			results.emplace_back(entry.functions[offset].arguments[i].ToString());
306: 		}
307: 		for (auto &param : entry.functions[offset].named_parameters) {
308: 			results.emplace_back(param.second.ToString());
309: 		}
310: 		return Value::LIST(LogicalType::VARCHAR, move(results));
311: 	}
312: 
313: 	static Value GetVarArgs(PragmaFunctionCatalogEntry &entry, idx_t offset) {
314: 		return entry.functions[offset].varargs.id() == LogicalTypeId::INVALID
315: 		           ? Value()
316: 		           : Value(entry.functions[offset].varargs.ToString());
317: 	}
318: 
319: 	static Value GetMacroDefinition(PragmaFunctionCatalogEntry &entry, idx_t offset) {
320: 		return Value();
321: 	}
322: };
323: 
324: template <class T, class OP>
325: bool ExtractFunctionData(StandardEntry *entry, idx_t function_idx, DataChunk &output, idx_t output_offset) {
326: 	auto &function = (T &)*entry;
327: 	// schema_name, LogicalType::VARCHAR
328: 	output.SetValue(0, output_offset, Value(entry->schema->name));
329: 
330: 	// function_name, LogicalType::VARCHAR
331: 	output.SetValue(1, output_offset, Value(entry->name));
332: 
333: 	// function_type, LogicalType::VARCHAR
334: 	output.SetValue(2, output_offset, Value(OP::GetFunctionType()));
335: 
336: 	// function_description, LogicalType::VARCHAR
337: 	output.SetValue(3, output_offset, OP::GetFunctionDescription(function, function_idx));
338: 
339: 	// return_type, LogicalType::VARCHAR
340: 	output.SetValue(4, output_offset, OP::GetReturnType(function, function_idx));
341: 
342: 	// parameters, LogicalType::LIST(LogicalType::VARCHAR)
343: 	output.SetValue(5, output_offset, OP::GetParameters(function, function_idx));
344: 
345: 	// parameter_types, LogicalType::LIST(LogicalType::VARCHAR)
346: 	output.SetValue(6, output_offset, OP::GetParameterTypes(function, function_idx));
347: 
348: 	// varargs, LogicalType::VARCHAR
349: 	output.SetValue(7, output_offset, OP::GetVarArgs(function, function_idx));
350: 
351: 	// macro_definition, LogicalType::VARCHAR
352: 	output.SetValue(8, output_offset, OP::GetMacroDefinition(function, function_idx));
353: 
354: 	return function_idx + 1 == OP::FunctionCount(function);
355: }
356: 
357: void DuckDBFunctionsFunction(ClientContext &context, const FunctionData *bind_data,
358:                              FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
359: 	auto &data = (DuckDBFunctionsData &)*operator_state;
360: 	if (data.offset >= data.entries.size()) {
361: 		// finished returning values
362: 		return;
363: 	}
364: 	// start returning values
365: 	// either fill up the chunk or return all the remaining columns
366: 	idx_t count = 0;
367: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
368: 		auto &entry = data.entries[data.offset];
369: 		auto standard_entry = (StandardEntry *)entry;
370: 		bool finished = false;
371: 
372: 		switch (entry->type) {
373: 		case CatalogType::SCALAR_FUNCTION_ENTRY:
374: 			finished = ExtractFunctionData<ScalarFunctionCatalogEntry, ScalarFunctionExtractor>(
375: 			    standard_entry, data.offset_in_entry, output, count);
376: 			break;
377: 		case CatalogType::AGGREGATE_FUNCTION_ENTRY:
378: 			finished = ExtractFunctionData<AggregateFunctionCatalogEntry, AggregateFunctionExtractor>(
379: 			    standard_entry, data.offset_in_entry, output, count);
380: 			break;
381: 		case CatalogType::MACRO_ENTRY:
382: 			finished = ExtractFunctionData<MacroCatalogEntry, MacroExtractor>(standard_entry, data.offset_in_entry,
383: 			                                                                  output, count);
384: 			break;
385: 		case CatalogType::TABLE_FUNCTION_ENTRY:
386: 			finished = ExtractFunctionData<TableFunctionCatalogEntry, TableFunctionExtractor>(
387: 			    standard_entry, data.offset_in_entry, output, count);
388: 			break;
389: 		case CatalogType::PRAGMA_FUNCTION_ENTRY:
390: 			finished = ExtractFunctionData<PragmaFunctionCatalogEntry, PragmaFunctionExtractor>(
391: 			    standard_entry, data.offset_in_entry, output, count);
392: 			break;
393: 		default:
394: 			throw InternalException("FIXME: unrecognized function type in duckdb_functions");
395: 		}
396: 		if (finished) {
397: 			// finished with this function, move to the next function
398: 			data.offset++;
399: 			data.offset_in_entry = 0;
400: 		} else {
401: 			// more functions remain
402: 			data.offset_in_entry++;
403: 		}
404: 		count++;
405: 	}
406: 	output.SetCardinality(count);
407: }
408: 
409: void DuckDBFunctionsFun::RegisterFunction(BuiltinFunctions &set) {
410: 	set.AddFunction(
411: 	    TableFunction("duckdb_functions", {}, DuckDBFunctionsFunction, DuckDBFunctionsBind, DuckDBFunctionsInit));
412: }
413: 
414: } // namespace duckdb
[end of src/function/table/system/duckdb_functions.cpp]
[start of src/function/table/system/duckdb_indexes.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/index_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/storage/data_table.hpp"
9: #include "duckdb/storage/index.hpp"
10: 
11: namespace duckdb {
12: 
13: struct DuckDBIndexesData : public FunctionOperatorData {
14: 	DuckDBIndexesData() : offset(0) {
15: 	}
16: 
17: 	vector<CatalogEntry *> entries;
18: 	idx_t offset;
19: };
20: 
21: static unique_ptr<FunctionData> DuckDBIndexesBind(ClientContext &context, vector<Value> &inputs,
22:                                                   unordered_map<string, Value> &named_parameters,
23:                                                   vector<LogicalType> &input_table_types,
24:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
25:                                                   vector<string> &names) {
26: 	names.emplace_back("schema_name");
27: 	return_types.emplace_back(LogicalType::VARCHAR);
28: 
29: 	names.emplace_back("schema_oid");
30: 	return_types.emplace_back(LogicalType::BIGINT);
31: 
32: 	names.emplace_back("index_name");
33: 	return_types.emplace_back(LogicalType::VARCHAR);
34: 
35: 	names.emplace_back("index_oid");
36: 	return_types.emplace_back(LogicalType::BIGINT);
37: 
38: 	names.emplace_back("table_name");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("table_oid");
42: 	return_types.emplace_back(LogicalType::BIGINT);
43: 
44: 	names.emplace_back("is_unique");
45: 	return_types.emplace_back(LogicalType::BOOLEAN);
46: 
47: 	names.emplace_back("is_primary");
48: 	return_types.emplace_back(LogicalType::BOOLEAN);
49: 
50: 	names.emplace_back("expressions");
51: 	return_types.emplace_back(LogicalType::VARCHAR);
52: 
53: 	names.emplace_back("sql");
54: 	return_types.emplace_back(LogicalType::VARCHAR);
55: 
56: 	return nullptr;
57: }
58: 
59: unique_ptr<FunctionOperatorData> DuckDBIndexesInit(ClientContext &context, const FunctionData *bind_data,
60:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
61: 	auto result = make_unique<DuckDBIndexesData>();
62: 
63: 	// scan all the schemas for tables and collect themand collect them
64: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
65: 	for (auto &schema : schemas) {
66: 		schema->Scan(context, CatalogType::INDEX_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
67: 	};
68: 
69: 	// check the temp schema as well
70: 	context.temporary_objects->Scan(context, CatalogType::INDEX_ENTRY,
71: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
72: 	return move(result);
73: }
74: 
75: void DuckDBIndexesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
76:                            DataChunk *input, DataChunk &output) {
77: 	auto &data = (DuckDBIndexesData &)*operator_state;
78: 	if (data.offset >= data.entries.size()) {
79: 		// finished returning values
80: 		return;
81: 	}
82: 	// start returning values
83: 	// either fill up the chunk or return all the remaining columns
84: 	idx_t count = 0;
85: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
86: 		auto &entry = data.entries[data.offset++];
87: 
88: 		auto &index = (IndexCatalogEntry &)*entry;
89: 		// return values:
90: 
91: 		// schema_name, VARCHAR
92: 		output.SetValue(0, count, Value(index.schema->name));
93: 		// schema_oid, BIGINT
94: 		output.SetValue(1, count, Value::BIGINT(index.schema->oid));
95: 		// index_name, VARCHAR
96: 		output.SetValue(2, count, Value(index.name));
97: 		// index_oid, BIGINT
98: 		output.SetValue(3, count, Value::BIGINT(index.oid));
99: 		// table_name, VARCHAR
100: 		output.SetValue(4, count, Value(index.info->table));
101: 		// table_oid, BIGINT
102: 		// find the table in the catalog
103: 		auto &catalog = Catalog::GetCatalog(context);
104: 		auto table_entry = catalog.GetEntry(context, CatalogType::TABLE_ENTRY, index.info->schema, index.info->table);
105: 		output.SetValue(5, count, Value::BIGINT(table_entry->oid));
106: 		// is_unique, BOOLEAN
107: 		output.SetValue(6, count, Value::BOOLEAN(index.index->is_unique));
108: 		// is_primary, BOOLEAN
109: 		output.SetValue(7, count, Value::BOOLEAN(index.index->is_primary));
110: 		// expressions, VARCHAR
111: 		output.SetValue(8, count, Value());
112: 		// sql, VARCHAR
113: 		output.SetValue(9, count, Value(index.ToSQL()));
114: 
115: 		count++;
116: 	}
117: 	output.SetCardinality(count);
118: }
119: 
120: void DuckDBIndexesFun::RegisterFunction(BuiltinFunctions &set) {
121: 	set.AddFunction(TableFunction("duckdb_indexes", {}, DuckDBIndexesFunction, DuckDBIndexesBind, DuckDBIndexesInit));
122: }
123: 
124: } // namespace duckdb
[end of src/function/table/system/duckdb_indexes.cpp]
[start of src/function/table/system/duckdb_keywords.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/parser/parser.hpp"
6: 
7: namespace duckdb {
8: 
9: struct DuckDBKeywordsData : public FunctionOperatorData {
10: 	DuckDBKeywordsData() : offset(0) {
11: 	}
12: 
13: 	vector<ParserKeyword> entries;
14: 	idx_t offset;
15: };
16: 
17: static unique_ptr<FunctionData> DuckDBKeywordsBind(ClientContext &context, vector<Value> &inputs,
18:                                                    unordered_map<string, Value> &named_parameters,
19:                                                    vector<LogicalType> &input_table_types,
20:                                                    vector<string> &input_table_names, vector<LogicalType> &return_types,
21:                                                    vector<string> &names) {
22: 	names.emplace_back("keyword_name");
23: 	return_types.emplace_back(LogicalType::VARCHAR);
24: 
25: 	names.emplace_back("keyword_category");
26: 	return_types.emplace_back(LogicalType::VARCHAR);
27: 
28: 	return nullptr;
29: }
30: 
31: unique_ptr<FunctionOperatorData> DuckDBKeywordsInit(ClientContext &context, const FunctionData *bind_data,
32:                                                     const vector<column_t> &column_ids,
33:                                                     TableFilterCollection *filters) {
34: 	auto result = make_unique<DuckDBKeywordsData>();
35: 	result->entries = Parser::KeywordList();
36: 	return move(result);
37: }
38: 
39: void DuckDBKeywordsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
40:                             DataChunk *input, DataChunk &output) {
41: 	auto &data = (DuckDBKeywordsData &)*operator_state;
42: 	if (data.offset >= data.entries.size()) {
43: 		// finished returning values
44: 		return;
45: 	}
46: 	// start returning values
47: 	// either fill up the chunk or return all the remaining columns
48: 	idx_t count = 0;
49: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
50: 		auto &entry = data.entries[data.offset++];
51: 
52: 		// keyword_name, VARCHAR
53: 		output.SetValue(0, count, Value(entry.name));
54: 		// keyword_category, VARCHAR
55: 		string category_name;
56: 		switch (entry.category) {
57: 		case KeywordCategory::KEYWORD_RESERVED:
58: 			category_name = "reserved";
59: 			break;
60: 		case KeywordCategory::KEYWORD_UNRESERVED:
61: 			category_name = "unreserved";
62: 			break;
63: 		case KeywordCategory::KEYWORD_TYPE_FUNC:
64: 			category_name = "type_function";
65: 			break;
66: 		case KeywordCategory::KEYWORD_COL_NAME:
67: 			category_name = "column_name";
68: 			break;
69: 		default:
70: 			throw InternalException("Unrecognized keyword category");
71: 		}
72: 		output.SetValue(1, count, Value(move(category_name)));
73: 
74: 		count++;
75: 	}
76: 	output.SetCardinality(count);
77: }
78: 
79: void DuckDBKeywordsFun::RegisterFunction(BuiltinFunctions &set) {
80: 	set.AddFunction(
81: 	    TableFunction("duckdb_keywords", {}, DuckDBKeywordsFunction, DuckDBKeywordsBind, DuckDBKeywordsInit));
82: }
83: 
84: } // namespace duckdb
[end of src/function/table/system/duckdb_keywords.cpp]
[start of src/function/table/system/duckdb_schemas.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/main/client_context.hpp"
7: 
8: namespace duckdb {
9: 
10: struct DuckDBSchemasData : public FunctionOperatorData {
11: 	DuckDBSchemasData() : offset(0) {
12: 	}
13: 
14: 	vector<SchemaCatalogEntry *> entries;
15: 	idx_t offset;
16: };
17: 
18: static unique_ptr<FunctionData> DuckDBSchemasBind(ClientContext &context, vector<Value> &inputs,
19:                                                   unordered_map<string, Value> &named_parameters,
20:                                                   vector<LogicalType> &input_table_types,
21:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
22:                                                   vector<string> &names) {
23: 	names.emplace_back("oid");
24: 	return_types.emplace_back(LogicalType::BIGINT);
25: 
26: 	names.emplace_back("schema_name");
27: 	return_types.emplace_back(LogicalType::VARCHAR);
28: 
29: 	names.emplace_back("internal");
30: 	return_types.emplace_back(LogicalType::BOOLEAN);
31: 
32: 	names.emplace_back("sql");
33: 	return_types.emplace_back(LogicalType::VARCHAR);
34: 
35: 	return nullptr;
36: }
37: 
38: unique_ptr<FunctionOperatorData> DuckDBSchemasInit(ClientContext &context, const FunctionData *bind_data,
39:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
40: 	auto result = make_unique<DuckDBSchemasData>();
41: 
42: 	// scan all the schemas and collect them
43: 	Catalog::GetCatalog(context).ScanSchemas(
44: 	    context, [&](CatalogEntry *entry) { result->entries.push_back((SchemaCatalogEntry *)entry); });
45: 	// get the temp schema as well
46: 	result->entries.push_back(context.temporary_objects.get());
47: 
48: 	return move(result);
49: }
50: 
51: void DuckDBSchemasFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
52:                            DataChunk *input, DataChunk &output) {
53: 	auto &data = (DuckDBSchemasData &)*operator_state;
54: 	if (data.offset >= data.entries.size()) {
55: 		// finished returning values
56: 		return;
57: 	}
58: 	// start returning values
59: 	// either fill up the chunk or return all the remaining columns
60: 	idx_t count = 0;
61: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
62: 		auto &entry = data.entries[data.offset];
63: 
64: 		// return values:
65: 		// "oid", PhysicalType::BIGINT
66: 		output.SetValue(0, count, Value::BIGINT(entry->oid));
67: 		// "schema_name", PhysicalType::VARCHAR
68: 		output.SetValue(1, count, Value(entry->name));
69: 		// "internal", PhysicalType::BOOLEAN
70: 		output.SetValue(2, count, Value::BOOLEAN(entry->internal));
71: 		// "sql", PhysicalType::VARCHAR
72: 		output.SetValue(3, count, Value());
73: 
74: 		data.offset++;
75: 		count++;
76: 	}
77: 	output.SetCardinality(count);
78: }
79: 
80: void DuckDBSchemasFun::RegisterFunction(BuiltinFunctions &set) {
81: 	set.AddFunction(TableFunction("duckdb_schemas", {}, DuckDBSchemasFunction, DuckDBSchemasBind, DuckDBSchemasInit));
82: }
83: 
84: } // namespace duckdb
[end of src/function/table/system/duckdb_schemas.cpp]
[start of src/function/table/system/duckdb_sequences.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: namespace duckdb {
10: 
11: struct DuckDBSequencesData : public FunctionOperatorData {
12: 	DuckDBSequencesData() : offset(0) {
13: 	}
14: 
15: 	vector<CatalogEntry *> entries;
16: 	idx_t offset;
17: };
18: 
19: static unique_ptr<FunctionData> DuckDBSequencesBind(ClientContext &context, vector<Value> &inputs,
20:                                                     unordered_map<string, Value> &named_parameters,
21:                                                     vector<LogicalType> &input_table_types,
22:                                                     vector<string> &input_table_names,
23:                                                     vector<LogicalType> &return_types, vector<string> &names) {
24: 	names.emplace_back("schema_name");
25: 	return_types.emplace_back(LogicalType::VARCHAR);
26: 
27: 	names.emplace_back("schema_oid");
28: 	return_types.emplace_back(LogicalType::BIGINT);
29: 
30: 	names.emplace_back("sequence_name");
31: 	return_types.emplace_back(LogicalType::VARCHAR);
32: 
33: 	names.emplace_back("sequence_oid");
34: 	return_types.emplace_back(LogicalType::BIGINT);
35: 
36: 	names.emplace_back("temporary");
37: 	return_types.emplace_back(LogicalType::BOOLEAN);
38: 
39: 	names.emplace_back("start_value");
40: 	return_types.emplace_back(LogicalType::BIGINT);
41: 
42: 	names.emplace_back("min_value");
43: 	return_types.emplace_back(LogicalType::BIGINT);
44: 
45: 	names.emplace_back("max_value");
46: 	return_types.emplace_back(LogicalType::BIGINT);
47: 
48: 	names.emplace_back("increment_by");
49: 	return_types.emplace_back(LogicalType::BIGINT);
50: 
51: 	names.emplace_back("cycle");
52: 	return_types.emplace_back(LogicalType::BOOLEAN);
53: 
54: 	names.emplace_back("last_value");
55: 	return_types.emplace_back(LogicalType::BIGINT);
56: 
57: 	names.emplace_back("sql");
58: 	return_types.emplace_back(LogicalType::VARCHAR);
59: 
60: 	return nullptr;
61: }
62: 
63: unique_ptr<FunctionOperatorData> DuckDBSequencesInit(ClientContext &context, const FunctionData *bind_data,
64:                                                      const vector<column_t> &column_ids,
65:                                                      TableFilterCollection *filters) {
66: 	auto result = make_unique<DuckDBSequencesData>();
67: 
68: 	// scan all the schemas for tables and collect themand collect them
69: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
70: 	for (auto &schema : schemas) {
71: 		schema->Scan(context, CatalogType::SEQUENCE_ENTRY,
72: 		             [&](CatalogEntry *entry) { result->entries.push_back(entry); });
73: 	};
74: 
75: 	// check the temp schema as well
76: 	context.temporary_objects->Scan(context, CatalogType::SEQUENCE_ENTRY,
77: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
78: 	return move(result);
79: }
80: 
81: void DuckDBSequencesFunction(ClientContext &context, const FunctionData *bind_data,
82:                              FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
83: 	auto &data = (DuckDBSequencesData &)*operator_state;
84: 	if (data.offset >= data.entries.size()) {
85: 		// finished returning values
86: 		return;
87: 	}
88: 	// start returning values
89: 	// either fill up the chunk or return all the remaining columns
90: 	idx_t count = 0;
91: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
92: 		auto &entry = data.entries[data.offset++];
93: 
94: 		auto &seq = (SequenceCatalogEntry &)*entry;
95: 		// return values:
96: 		// schema_name, VARCHAR
97: 		output.SetValue(0, count, Value(seq.schema->name));
98: 		// schema_oid, BIGINT
99: 		output.SetValue(1, count, Value::BIGINT(seq.schema->oid));
100: 		// sequence_name, VARCHAR
101: 		output.SetValue(2, count, Value(seq.name));
102: 		// sequence_oid, BIGINT
103: 		output.SetValue(3, count, Value::BIGINT(seq.oid));
104: 		// temporary, BOOLEAN
105: 		output.SetValue(4, count, Value::BOOLEAN(seq.temporary));
106: 		// start_value, BIGINT
107: 		output.SetValue(5, count, Value::BIGINT(seq.start_value));
108: 		// min_value, BIGINT
109: 		output.SetValue(6, count, Value::BIGINT(seq.min_value));
110: 		// max_value, BIGINT
111: 		output.SetValue(7, count, Value::BIGINT(seq.max_value));
112: 		// increment_by, BIGINT
113: 		output.SetValue(8, count, Value::BIGINT(seq.increment));
114: 		// cycle, BOOLEAN
115: 		output.SetValue(9, count, Value::BOOLEAN(seq.cycle));
116: 		// last_value, BIGINT
117: 		output.SetValue(10, count, seq.usage_count == 0 ? Value() : Value::BOOLEAN(seq.last_value));
118: 		// sql, LogicalType::VARCHAR
119: 		output.SetValue(11, count, Value(seq.ToSQL()));
120: 
121: 		count++;
122: 	}
123: 	output.SetCardinality(count);
124: }
125: 
126: void DuckDBSequencesFun::RegisterFunction(BuiltinFunctions &set) {
127: 	set.AddFunction(
128: 	    TableFunction("duckdb_sequences", {}, DuckDBSequencesFunction, DuckDBSequencesBind, DuckDBSequencesInit));
129: }
130: 
131: } // namespace duckdb
[end of src/function/table/system/duckdb_sequences.cpp]
[start of src/function/table/system/duckdb_settings.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: #include "duckdb/common/types/chunk_collection.hpp"
3: #include "duckdb/main/config.hpp"
4: #include "duckdb/main/client_context.hpp"
5: 
6: namespace duckdb {
7: 
8: struct DuckDBSettingValue {
9: 	string name;
10: 	string value;
11: 	string description;
12: 	string input_type;
13: };
14: 
15: struct DuckDBSettingsData : public FunctionOperatorData {
16: 	DuckDBSettingsData() : offset(0) {
17: 	}
18: 
19: 	vector<DuckDBSettingValue> settings;
20: 	idx_t offset;
21: };
22: 
23: static unique_ptr<FunctionData> DuckDBSettingsBind(ClientContext &context, vector<Value> &inputs,
24:                                                    unordered_map<string, Value> &named_parameters,
25:                                                    vector<LogicalType> &input_table_types,
26:                                                    vector<string> &input_table_names, vector<LogicalType> &return_types,
27:                                                    vector<string> &names) {
28: 	names.emplace_back("name");
29: 	return_types.emplace_back(LogicalType::VARCHAR);
30: 
31: 	names.emplace_back("value");
32: 	return_types.emplace_back(LogicalType::VARCHAR);
33: 
34: 	names.emplace_back("description");
35: 	return_types.emplace_back(LogicalType::VARCHAR);
36: 
37: 	names.emplace_back("input_type");
38: 	return_types.emplace_back(LogicalType::VARCHAR);
39: 
40: 	return nullptr;
41: }
42: 
43: unique_ptr<FunctionOperatorData> DuckDBSettingsInit(ClientContext &context, const FunctionData *bind_data,
44:                                                     const vector<column_t> &column_ids,
45:                                                     TableFilterCollection *filters) {
46: 	auto result = make_unique<DuckDBSettingsData>();
47: 
48: 	auto &config = DBConfig::GetConfig(context);
49: 	auto options_count = DBConfig::GetOptionCount();
50: 	for (idx_t i = 0; i < options_count; i++) {
51: 		auto option = DBConfig::GetOptionByIndex(i);
52: 		D_ASSERT(option);
53: 		DuckDBSettingValue value;
54: 		value.name = option->name;
55: 		value.value = option->get_setting(context).ToString();
56: 		value.description = option->description;
57: 		value.input_type = LogicalTypeIdToString(option->parameter_type);
58: 
59: 		result->settings.push_back(move(value));
60: 	}
61: 	for (auto &ext_param : config.extension_parameters) {
62: 		Value setting_val;
63: 		string setting_str_val;
64: 		if (context.TryGetCurrentSetting(ext_param.first, setting_val)) {
65: 			setting_str_val = setting_val.ToString();
66: 		}
67: 		DuckDBSettingValue value;
68: 		value.name = ext_param.first;
69: 		value.value = move(setting_str_val);
70: 		value.description = ext_param.second.description;
71: 		value.input_type = ext_param.second.type.ToString();
72: 
73: 		result->settings.push_back(move(value));
74: 	}
75: 	return move(result);
76: }
77: 
78: void DuckDBSettingsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
79:                             DataChunk *input, DataChunk &output) {
80: 	auto &data = (DuckDBSettingsData &)*operator_state;
81: 	if (data.offset >= data.settings.size()) {
82: 		// finished returning values
83: 		return;
84: 	}
85: 	// start returning values
86: 	// either fill up the chunk or return all the remaining columns
87: 	idx_t count = 0;
88: 	while (data.offset < data.settings.size() && count < STANDARD_VECTOR_SIZE) {
89: 		auto &entry = data.settings[data.offset++];
90: 
91: 		// return values:
92: 		// name, LogicalType::VARCHAR
93: 		output.SetValue(0, count, Value(entry.name));
94: 		// value, LogicalType::VARCHAR
95: 		output.SetValue(1, count, Value(entry.value));
96: 		// description, LogicalType::VARCHAR
97: 		output.SetValue(2, count, Value(entry.description));
98: 		// input_type, LogicalType::VARCHAR
99: 		output.SetValue(3, count, Value(entry.input_type));
100: 		count++;
101: 	}
102: 	output.SetCardinality(count);
103: }
104: 
105: void DuckDBSettingsFun::RegisterFunction(BuiltinFunctions &set) {
106: 	set.AddFunction(
107: 	    TableFunction("duckdb_settings", {}, DuckDBSettingsFunction, DuckDBSettingsBind, DuckDBSettingsInit));
108: }
109: 
110: } // namespace duckdb
[end of src/function/table/system/duckdb_settings.cpp]
[start of src/function/table/system/duckdb_tables.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/parser/constraint.hpp"
9: #include "duckdb/parser/constraints/unique_constraint.hpp"
10: #include "duckdb/storage/data_table.hpp"
11: 
12: namespace duckdb {
13: 
14: struct DuckDBTablesData : public FunctionOperatorData {
15: 	DuckDBTablesData() : offset(0) {
16: 	}
17: 
18: 	vector<CatalogEntry *> entries;
19: 	idx_t offset;
20: };
21: 
22: static unique_ptr<FunctionData> DuckDBTablesBind(ClientContext &context, vector<Value> &inputs,
23:                                                  unordered_map<string, Value> &named_parameters,
24:                                                  vector<LogicalType> &input_table_types,
25:                                                  vector<string> &input_table_names, vector<LogicalType> &return_types,
26:                                                  vector<string> &names) {
27: 	names.emplace_back("schema_name");
28: 	return_types.emplace_back(LogicalType::VARCHAR);
29: 
30: 	names.emplace_back("schema_oid");
31: 	return_types.emplace_back(LogicalType::BIGINT);
32: 
33: 	names.emplace_back("table_name");
34: 	return_types.emplace_back(LogicalType::VARCHAR);
35: 
36: 	names.emplace_back("table_oid");
37: 	return_types.emplace_back(LogicalType::BIGINT);
38: 
39: 	names.emplace_back("internal");
40: 	return_types.emplace_back(LogicalType::BOOLEAN);
41: 
42: 	names.emplace_back("temporary");
43: 	return_types.emplace_back(LogicalType::BOOLEAN);
44: 
45: 	names.emplace_back("has_primary_key");
46: 	return_types.emplace_back(LogicalType::BOOLEAN);
47: 
48: 	names.emplace_back("estimated_size");
49: 	return_types.emplace_back(LogicalType::BIGINT);
50: 
51: 	names.emplace_back("column_count");
52: 	return_types.emplace_back(LogicalType::BIGINT);
53: 
54: 	names.emplace_back("index_count");
55: 	return_types.emplace_back(LogicalType::BIGINT);
56: 
57: 	names.emplace_back("check_constraint_count");
58: 	return_types.emplace_back(LogicalType::BIGINT);
59: 
60: 	names.emplace_back("sql");
61: 	return_types.emplace_back(LogicalType::VARCHAR);
62: 
63: 	return nullptr;
64: }
65: 
66: unique_ptr<FunctionOperatorData> DuckDBTablesInit(ClientContext &context, const FunctionData *bind_data,
67:                                                   const vector<column_t> &column_ids, TableFilterCollection *filters) {
68: 	auto result = make_unique<DuckDBTablesData>();
69: 
70: 	// scan all the schemas for tables and collect themand collect them
71: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
72: 	for (auto &schema : schemas) {
73: 		schema->Scan(context, CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
74: 	};
75: 
76: 	// check the temp schema as well
77: 	context.temporary_objects->Scan(context, CatalogType::TABLE_ENTRY,
78: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
79: 	return move(result);
80: }
81: 
82: static bool TableHasPrimaryKey(TableCatalogEntry &table) {
83: 	for (auto &constraint : table.constraints) {
84: 		if (constraint->type == ConstraintType::UNIQUE) {
85: 			auto &unique = (UniqueConstraint &)*constraint;
86: 			if (unique.is_primary_key) {
87: 				return true;
88: 			}
89: 		}
90: 	}
91: 	return false;
92: }
93: 
94: static idx_t CheckConstraintCount(TableCatalogEntry &table) {
95: 	idx_t check_count = 0;
96: 	for (auto &constraint : table.constraints) {
97: 		if (constraint->type == ConstraintType::CHECK) {
98: 			check_count++;
99: 		}
100: 	}
101: 	return check_count;
102: }
103: 
104: void DuckDBTablesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
105:                           DataChunk *input, DataChunk &output) {
106: 	auto &data = (DuckDBTablesData &)*operator_state;
107: 	if (data.offset >= data.entries.size()) {
108: 		// finished returning values
109: 		return;
110: 	}
111: 	// start returning values
112: 	// either fill up the chunk or return all the remaining columns
113: 	idx_t count = 0;
114: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
115: 		auto &entry = data.entries[data.offset++];
116: 
117: 		if (entry->type != CatalogType::TABLE_ENTRY) {
118: 			continue;
119: 		}
120: 		auto &table = (TableCatalogEntry &)*entry;
121: 		// return values:
122: 		// schema_name, LogicalType::VARCHAR
123: 		output.SetValue(0, count, Value(table.schema->name));
124: 		// schema_oid, LogicalType::BIGINT
125: 		output.SetValue(1, count, Value::BIGINT(table.schema->oid));
126: 		// table_name, LogicalType::VARCHAR
127: 		output.SetValue(2, count, Value(table.name));
128: 		// table_oid, LogicalType::BIGINT
129: 		output.SetValue(3, count, Value::BIGINT(table.oid));
130: 		// internal, LogicalType::BOOLEAN
131: 		output.SetValue(4, count, Value::BOOLEAN(table.internal));
132: 		// temporary, LogicalType::BOOLEAN
133: 		output.SetValue(5, count, Value::BOOLEAN(table.temporary));
134: 		// has_primary_key, LogicalType::BOOLEAN
135: 		output.SetValue(6, count, Value::BOOLEAN(TableHasPrimaryKey(table)));
136: 		// estimated_size, LogicalType::BIGINT
137: 		output.SetValue(7, count, Value::BIGINT(table.storage->info->cardinality.load()));
138: 		// column_count, LogicalType::BIGINT
139: 		output.SetValue(8, count, Value::BIGINT(table.columns.size()));
140: 		// index_count, LogicalType::BIGINT
141: 		output.SetValue(9, count, Value::BIGINT(table.storage->info->indexes.Count()));
142: 		// check_constraint_count, LogicalType::BIGINT
143: 		output.SetValue(10, count, Value::BIGINT(CheckConstraintCount(table)));
144: 		// sql, LogicalType::VARCHAR
145: 		output.SetValue(11, count, Value(table.ToSQL()));
146: 
147: 		count++;
148: 	}
149: 	output.SetCardinality(count);
150: }
151: 
152: void DuckDBTablesFun::RegisterFunction(BuiltinFunctions &set) {
153: 	set.AddFunction(TableFunction("duckdb_tables", {}, DuckDBTablesFunction, DuckDBTablesBind, DuckDBTablesInit));
154: }
155: 
156: } // namespace duckdb
[end of src/function/table/system/duckdb_tables.cpp]
[start of src/function/table/system/duckdb_types.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: namespace duckdb {
10: 
11: struct DuckDBTypesData : public FunctionOperatorData {
12: 	DuckDBTypesData() : offset(0) {
13: 	}
14: 
15: 	vector<LogicalType> types;
16: 	idx_t offset;
17: };
18: 
19: static unique_ptr<FunctionData> DuckDBTypesBind(ClientContext &context, vector<Value> &inputs,
20:                                                 unordered_map<string, Value> &named_parameters,
21:                                                 vector<LogicalType> &input_table_types,
22:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
23:                                                 vector<string> &names) {
24: 	names.emplace_back("schema_name");
25: 	return_types.emplace_back(LogicalType::VARCHAR);
26: 
27: 	names.emplace_back("schema_oid");
28: 	return_types.emplace_back(LogicalType::BIGINT);
29: 
30: 	names.emplace_back("type_oid");
31: 	return_types.emplace_back(LogicalType::BIGINT);
32: 
33: 	names.emplace_back("type_name");
34: 	return_types.emplace_back(LogicalType::VARCHAR);
35: 
36: 	names.emplace_back("type_size");
37: 	return_types.emplace_back(LogicalType::BIGINT);
38: 
39: 	// NUMERIC, STRING, DATETIME, BOOLEAN, COMPOSITE, USER
40: 	names.emplace_back("type_category");
41: 	return_types.emplace_back(LogicalType::VARCHAR);
42: 
43: 	names.emplace_back("internal");
44: 	return_types.emplace_back(LogicalType::BOOLEAN);
45: 
46: 	return nullptr;
47: }
48: 
49: unique_ptr<FunctionOperatorData> DuckDBTypesInit(ClientContext &context, const FunctionData *bind_data,
50:                                                  const vector<column_t> &column_ids, TableFilterCollection *filters) {
51: 	auto result = make_unique<DuckDBTypesData>();
52: 	result->types = LogicalType::AllTypes();
53: 	// FIXME: add user-defined types here (when we have them)
54: 	return move(result);
55: }
56: 
57: void DuckDBTypesFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
58:                          DataChunk *input, DataChunk &output) {
59: 	auto &data = (DuckDBTypesData &)*operator_state;
60: 	if (data.offset >= data.types.size()) {
61: 		// finished returning values
62: 		return;
63: 	}
64: 	// start returning values
65: 	// either fill up the chunk or return all the remaining columns
66: 	idx_t count = 0;
67: 	while (data.offset < data.types.size() && count < STANDARD_VECTOR_SIZE) {
68: 		auto &type = data.types[data.offset++];
69: 
70: 		// return values:
71: 		// schema_name, VARCHAR
72: 		output.SetValue(0, count, Value());
73: 		// schema_oid, BIGINT
74: 		output.SetValue(1, count, Value());
75: 		// type_oid, BIGINT
76: 		output.SetValue(2, count, Value::BIGINT(int(type.id())));
77: 		// type_name, VARCHAR
78: 		output.SetValue(3, count, Value(type.ToString()));
79: 		// type_size, BIGINT
80: 		auto internal_type = type.InternalType();
81: 		output.SetValue(4, count,
82: 		                internal_type == PhysicalType::INVALID ? Value() : Value::BIGINT(GetTypeIdSize(internal_type)));
83: 		// type_category, VARCHAR
84: 		string category;
85: 		switch (type.id()) {
86: 		case LogicalTypeId::TINYINT:
87: 		case LogicalTypeId::SMALLINT:
88: 		case LogicalTypeId::INTEGER:
89: 		case LogicalTypeId::BIGINT:
90: 		case LogicalTypeId::DECIMAL:
91: 		case LogicalTypeId::FLOAT:
92: 		case LogicalTypeId::DOUBLE:
93: 		case LogicalTypeId::UTINYINT:
94: 		case LogicalTypeId::USMALLINT:
95: 		case LogicalTypeId::UINTEGER:
96: 		case LogicalTypeId::UBIGINT:
97: 		case LogicalTypeId::HUGEINT:
98: 			category = "NUMERIC";
99: 			break;
100: 		case LogicalTypeId::DATE:
101: 		case LogicalTypeId::TIME:
102: 		case LogicalTypeId::TIMESTAMP_SEC:
103: 		case LogicalTypeId::TIMESTAMP_MS:
104: 		case LogicalTypeId::TIMESTAMP:
105: 		case LogicalTypeId::TIMESTAMP_NS:
106: 		case LogicalTypeId::INTERVAL:
107: 		case LogicalTypeId::TIME_TZ:
108: 		case LogicalTypeId::TIMESTAMP_TZ:
109: 			category = "DATETIME";
110: 			break;
111: 		case LogicalTypeId::CHAR:
112: 		case LogicalTypeId::VARCHAR:
113: 			category = "STRING";
114: 			break;
115: 		case LogicalTypeId::BOOLEAN:
116: 			category = "BOOLEAN";
117: 			break;
118: 		case LogicalTypeId::STRUCT:
119: 		case LogicalTypeId::LIST:
120: 		case LogicalTypeId::MAP:
121: 			category = "COMPOSITE";
122: 			break;
123: 		default:
124: 			break;
125: 		}
126: 		output.SetValue(5, count, category.empty() ? Value() : Value(category));
127: 		// internal, BOOLEAN
128: 		output.SetValue(6, count, Value::BOOLEAN(true));
129: 
130: 		count++;
131: 	}
132: 	output.SetCardinality(count);
133: }
134: 
135: void DuckDBTypesFun::RegisterFunction(BuiltinFunctions &set) {
136: 	set.AddFunction(TableFunction("duckdb_types", {}, DuckDBTypesFunction, DuckDBTypesBind, DuckDBTypesInit));
137: }
138: 
139: } // namespace duckdb
[end of src/function/table/system/duckdb_types.cpp]
[start of src/function/table/system/duckdb_views.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: namespace duckdb {
10: 
11: struct DuckDBViewsData : public FunctionOperatorData {
12: 	DuckDBViewsData() : offset(0) {
13: 	}
14: 
15: 	vector<CatalogEntry *> entries;
16: 	idx_t offset;
17: };
18: 
19: static unique_ptr<FunctionData> DuckDBViewsBind(ClientContext &context, vector<Value> &inputs,
20:                                                 unordered_map<string, Value> &named_parameters,
21:                                                 vector<LogicalType> &input_table_types,
22:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
23:                                                 vector<string> &names) {
24: 	names.emplace_back("schema_name");
25: 	return_types.emplace_back(LogicalType::VARCHAR);
26: 
27: 	names.emplace_back("schema_oid");
28: 	return_types.emplace_back(LogicalType::BIGINT);
29: 
30: 	names.emplace_back("view_name");
31: 	return_types.emplace_back(LogicalType::VARCHAR);
32: 
33: 	names.emplace_back("view_oid");
34: 	return_types.emplace_back(LogicalType::BIGINT);
35: 
36: 	names.emplace_back("internal");
37: 	return_types.emplace_back(LogicalType::BOOLEAN);
38: 
39: 	names.emplace_back("temporary");
40: 	return_types.emplace_back(LogicalType::BOOLEAN);
41: 
42: 	names.emplace_back("column_count");
43: 	return_types.emplace_back(LogicalType::BIGINT);
44: 
45: 	names.emplace_back("sql");
46: 	return_types.emplace_back(LogicalType::VARCHAR);
47: 
48: 	return nullptr;
49: }
50: 
51: unique_ptr<FunctionOperatorData> DuckDBViewsInit(ClientContext &context, const FunctionData *bind_data,
52:                                                  const vector<column_t> &column_ids, TableFilterCollection *filters) {
53: 	auto result = make_unique<DuckDBViewsData>();
54: 
55: 	// scan all the schemas for tables and collect themand collect them
56: 	auto schemas = Catalog::GetCatalog(context).schemas->GetEntries<SchemaCatalogEntry>(context);
57: 	for (auto &schema : schemas) {
58: 		schema->Scan(context, CatalogType::VIEW_ENTRY, [&](CatalogEntry *entry) { result->entries.push_back(entry); });
59: 	};
60: 
61: 	// check the temp schema as well
62: 	context.temporary_objects->Scan(context, CatalogType::VIEW_ENTRY,
63: 	                                [&](CatalogEntry *entry) { result->entries.push_back(entry); });
64: 	return move(result);
65: }
66: 
67: void DuckDBViewsFunction(ClientContext &context, const FunctionData *bind_data, FunctionOperatorData *operator_state,
68:                          DataChunk *input, DataChunk &output) {
69: 	auto &data = (DuckDBViewsData &)*operator_state;
70: 	if (data.offset >= data.entries.size()) {
71: 		// finished returning values
72: 		return;
73: 	}
74: 	// start returning values
75: 	// either fill up the chunk or return all the remaining columns
76: 	idx_t count = 0;
77: 	while (data.offset < data.entries.size() && count < STANDARD_VECTOR_SIZE) {
78: 		auto &entry = data.entries[data.offset++];
79: 
80: 		if (entry->type != CatalogType::VIEW_ENTRY) {
81: 			continue;
82: 		}
83: 		auto &view = (ViewCatalogEntry &)*entry;
84: 
85: 		// return values:
86: 		// schema_name, LogicalType::VARCHAR
87: 		output.SetValue(0, count, Value(view.schema->name));
88: 		// schema_oid, LogicalType::BIGINT
89: 		output.SetValue(1, count, Value::BIGINT(view.schema->oid));
90: 		// view_name, LogicalType::VARCHAR
91: 		output.SetValue(2, count, Value(view.name));
92: 		// view_oid, LogicalType::BIGINT
93: 		output.SetValue(3, count, Value::BIGINT(view.oid));
94: 		// internal, LogicalType::BOOLEAN
95: 		output.SetValue(4, count, Value::BOOLEAN(view.internal));
96: 		// temporary, LogicalType::BOOLEAN
97: 		output.SetValue(5, count, Value::BOOLEAN(view.temporary));
98: 		// column_count, LogicalType::BIGINT
99: 		output.SetValue(6, count, Value::BIGINT(view.types.size()));
100: 		// sql, LogicalType::VARCHAR
101: 		output.SetValue(7, count, Value(view.ToSQL()));
102: 
103: 		count++;
104: 	}
105: 	output.SetCardinality(count);
106: }
107: 
108: void DuckDBViewsFun::RegisterFunction(BuiltinFunctions &set) {
109: 	set.AddFunction(TableFunction("duckdb_views", {}, DuckDBViewsFunction, DuckDBViewsBind, DuckDBViewsInit));
110: }
111: 
112: } // namespace duckdb
[end of src/function/table/system/duckdb_views.cpp]
[start of src/function/table/system/pragma_collations.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/collate_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: 
8: namespace duckdb {
9: 
10: struct PragmaCollateData : public FunctionOperatorData {
11: 	PragmaCollateData() : offset(0) {
12: 	}
13: 
14: 	vector<string> entries;
15: 	idx_t offset;
16: };
17: 
18: static unique_ptr<FunctionData> PragmaCollateBind(ClientContext &context, vector<Value> &inputs,
19:                                                   unordered_map<string, Value> &named_parameters,
20:                                                   vector<LogicalType> &input_table_types,
21:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
22:                                                   vector<string> &names) {
23: 	names.emplace_back("collname");
24: 	return_types.emplace_back(LogicalType::VARCHAR);
25: 
26: 	return nullptr;
27: }
28: 
29: unique_ptr<FunctionOperatorData> PragmaCollateInit(ClientContext &context, const FunctionData *bind_data,
30:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
31: 	auto result = make_unique<PragmaCollateData>();
32: 
33: 	Catalog::GetCatalog(context).schemas->Scan(context, [&](CatalogEntry *entry) {
34: 		auto schema = (SchemaCatalogEntry *)entry;
35: 		schema->Scan(context, CatalogType::COLLATION_ENTRY,
36: 		             [&](CatalogEntry *entry) { result->entries.push_back(entry->name); });
37: 	});
38: 
39: 	return move(result);
40: }
41: 
42: static void PragmaCollateFunction(ClientContext &context, const FunctionData *bind_data,
43:                                   FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
44: 	auto &data = (PragmaCollateData &)*operator_state;
45: 	if (data.offset >= data.entries.size()) {
46: 		// finished returning values
47: 		return;
48: 	}
49: 	idx_t next = MinValue<idx_t>(data.offset + STANDARD_VECTOR_SIZE, data.entries.size());
50: 	output.SetCardinality(next - data.offset);
51: 	for (idx_t i = data.offset; i < next; i++) {
52: 		auto index = i - data.offset;
53: 		output.SetValue(0, index, Value(data.entries[i]));
54: 	}
55: 
56: 	data.offset = next;
57: }
58: 
59: void PragmaCollations::RegisterFunction(BuiltinFunctions &set) {
60: 	set.AddFunction(
61: 	    TableFunction("pragma_collations", {}, PragmaCollateFunction, PragmaCollateBind, PragmaCollateInit));
62: }
63: 
64: } // namespace duckdb
[end of src/function/table/system/pragma_collations.cpp]
[start of src/function/table/system/pragma_database_list.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/storage/storage_manager.hpp"
4: 
5: namespace duckdb {
6: 
7: struct PragmaDatabaseListData : public FunctionOperatorData {
8: 	PragmaDatabaseListData() : finished(false) {
9: 	}
10: 
11: 	bool finished;
12: };
13: 
14: static unique_ptr<FunctionData> PragmaDatabaseListBind(ClientContext &context, vector<Value> &inputs,
15:                                                        unordered_map<string, Value> &named_parameters,
16:                                                        vector<LogicalType> &input_table_types,
17:                                                        vector<string> &input_table_names,
18:                                                        vector<LogicalType> &return_types, vector<string> &names) {
19: 	names.emplace_back("seq");
20: 	return_types.emplace_back(LogicalType::INTEGER);
21: 
22: 	names.emplace_back("name");
23: 	return_types.emplace_back(LogicalType::VARCHAR);
24: 
25: 	names.emplace_back("file");
26: 	return_types.emplace_back(LogicalType::VARCHAR);
27: 
28: 	return nullptr;
29: }
30: 
31: unique_ptr<FunctionOperatorData> PragmaDatabaseListInit(ClientContext &context, const FunctionData *bind_data,
32:                                                         const vector<column_t> &column_ids,
33:                                                         TableFilterCollection *filters) {
34: 	return make_unique<PragmaDatabaseListData>();
35: }
36: 
37: void PragmaDatabaseListFunction(ClientContext &context, const FunctionData *bind_data,
38:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
39: 	auto &data = (PragmaDatabaseListData &)*operator_state;
40: 	if (data.finished) {
41: 		return;
42: 	}
43: 
44: 	output.SetCardinality(1);
45: 	output.data[0].SetValue(0, Value::INTEGER(0));
46: 	output.data[1].SetValue(0, Value("main"));
47: 	output.data[2].SetValue(0, Value(StorageManager::GetStorageManager(context).GetDBPath()));
48: 
49: 	data.finished = true;
50: }
51: 
52: void PragmaDatabaseList::RegisterFunction(BuiltinFunctions &set) {
53: 	set.AddFunction(TableFunction("pragma_database_list", {}, PragmaDatabaseListFunction, PragmaDatabaseListBind,
54: 	                              PragmaDatabaseListInit));
55: }
56: 
57: } // namespace duckdb
[end of src/function/table/system/pragma_database_list.cpp]
[start of src/function/table/system/pragma_database_size.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/storage/storage_manager.hpp"
4: #include "duckdb/storage/block_manager.hpp"
5: #include "duckdb/storage/storage_info.hpp"
6: #include "duckdb/common/to_string.hpp"
7: #include "duckdb/common/string_util.hpp"
8: 
9: namespace duckdb {
10: 
11: struct PragmaDatabaseSizeData : public FunctionOperatorData {
12: 	PragmaDatabaseSizeData() : finished(false) {
13: 	}
14: 
15: 	bool finished;
16: };
17: 
18: static unique_ptr<FunctionData> PragmaDatabaseSizeBind(ClientContext &context, vector<Value> &inputs,
19:                                                        unordered_map<string, Value> &named_parameters,
20:                                                        vector<LogicalType> &input_table_types,
21:                                                        vector<string> &input_table_names,
22:                                                        vector<LogicalType> &return_types, vector<string> &names) {
23: 	names.emplace_back("database_size");
24: 	return_types.emplace_back(LogicalType::VARCHAR);
25: 
26: 	names.emplace_back("block_size");
27: 	return_types.emplace_back(LogicalType::BIGINT);
28: 
29: 	names.emplace_back("total_blocks");
30: 	return_types.emplace_back(LogicalType::BIGINT);
31: 
32: 	names.emplace_back("used_blocks");
33: 	return_types.emplace_back(LogicalType::BIGINT);
34: 
35: 	names.emplace_back("free_blocks");
36: 	return_types.emplace_back(LogicalType::BIGINT);
37: 
38: 	names.emplace_back("wal_size");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("memory_usage");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("memory_limit");
45: 	return_types.emplace_back(LogicalType::VARCHAR);
46: 
47: 	return nullptr;
48: }
49: 
50: unique_ptr<FunctionOperatorData> PragmaDatabaseSizeInit(ClientContext &context, const FunctionData *bind_data,
51:                                                         const vector<column_t> &column_ids,
52:                                                         TableFilterCollection *filters) {
53: 	return make_unique<PragmaDatabaseSizeData>();
54: }
55: 
56: void PragmaDatabaseSizeFunction(ClientContext &context, const FunctionData *bind_data,
57:                                 FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
58: 	auto &data = (PragmaDatabaseSizeData &)*operator_state;
59: 	if (data.finished) {
60: 		return;
61: 	}
62: 	auto &storage = StorageManager::GetStorageManager(context);
63: 	auto &block_manager = BlockManager::GetBlockManager(context);
64: 	auto &buffer_manager = BufferManager::GetBufferManager(context);
65: 
66: 	output.SetCardinality(1);
67: 	if (!storage.InMemory()) {
68: 		auto total_blocks = block_manager.TotalBlocks();
69: 		auto block_size = Storage::BLOCK_ALLOC_SIZE;
70: 		auto free_blocks = block_manager.FreeBlocks();
71: 		auto used_blocks = total_blocks - free_blocks;
72: 		auto bytes = (total_blocks * block_size);
73: 		auto wal_size = storage.GetWriteAheadLog()->GetWALSize();
74: 		output.data[0].SetValue(0, Value(StringUtil::BytesToHumanReadableString(bytes)));
75: 		output.data[1].SetValue(0, Value::BIGINT(block_size));
76: 		output.data[2].SetValue(0, Value::BIGINT(total_blocks));
77: 		output.data[3].SetValue(0, Value::BIGINT(used_blocks));
78: 		output.data[4].SetValue(0, Value::BIGINT(free_blocks));
79: 		output.data[5].SetValue(0, Value(StringUtil::BytesToHumanReadableString(wal_size)));
80: 	} else {
81: 		output.data[0].SetValue(0, Value());
82: 		output.data[1].SetValue(0, Value());
83: 		output.data[2].SetValue(0, Value());
84: 		output.data[3].SetValue(0, Value());
85: 		output.data[4].SetValue(0, Value());
86: 		output.data[5].SetValue(0, Value());
87: 	}
88: 	output.data[6].SetValue(0, Value(StringUtil::BytesToHumanReadableString(buffer_manager.GetUsedMemory())));
89: 	auto max_memory = buffer_manager.GetMaxMemory();
90: 	output.data[7].SetValue(0, max_memory == (idx_t)-1 ? Value("Unlimited")
91: 	                                                   : Value(StringUtil::BytesToHumanReadableString(max_memory)));
92: 
93: 	data.finished = true;
94: }
95: 
96: void PragmaDatabaseSize::RegisterFunction(BuiltinFunctions &set) {
97: 	set.AddFunction(TableFunction("pragma_database_size", {}, PragmaDatabaseSizeFunction, PragmaDatabaseSizeBind,
98: 	                              PragmaDatabaseSizeInit));
99: }
100: 
101: } // namespace duckdb
[end of src/function/table/system/pragma_database_size.cpp]
[start of src/function/table/system/pragma_functions.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
7: #include "duckdb/common/types/chunk_collection.hpp"
8: #include "duckdb/common/exception.hpp"
9: 
10: namespace duckdb {
11: 
12: struct PragmaFunctionsData : public FunctionOperatorData {
13: 	PragmaFunctionsData() : offset(0), offset_in_entry(0) {
14: 	}
15: 
16: 	vector<CatalogEntry *> entries;
17: 	idx_t offset;
18: 	idx_t offset_in_entry;
19: };
20: 
21: static unique_ptr<FunctionData> PragmaFunctionsBind(ClientContext &context, vector<Value> &inputs,
22:                                                     unordered_map<string, Value> &named_parameters,
23:                                                     vector<LogicalType> &input_table_types,
24:                                                     vector<string> &input_table_names,
25:                                                     vector<LogicalType> &return_types, vector<string> &names) {
26: 	names.emplace_back("name");
27: 	return_types.emplace_back(LogicalType::VARCHAR);
28: 
29: 	names.emplace_back("type");
30: 	return_types.emplace_back(LogicalType::VARCHAR);
31: 
32: 	names.emplace_back("parameters");
33: 	return_types.push_back(LogicalType::LIST(LogicalType::VARCHAR));
34: 
35: 	names.emplace_back("varargs");
36: 	return_types.emplace_back(LogicalType::VARCHAR);
37: 
38: 	names.emplace_back("return_type");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("side_effects");
42: 	return_types.emplace_back(LogicalType::BOOLEAN);
43: 
44: 	return nullptr;
45: }
46: 
47: unique_ptr<FunctionOperatorData> PragmaFunctionsInit(ClientContext &context, const FunctionData *bind_data,
48:                                                      const vector<column_t> &column_ids,
49:                                                      TableFilterCollection *filters) {
50: 	auto result = make_unique<PragmaFunctionsData>();
51: 
52: 	Catalog::GetCatalog(context).schemas->Scan(context, [&](CatalogEntry *entry) {
53: 		auto schema = (SchemaCatalogEntry *)entry;
54: 		schema->Scan(context, CatalogType::SCALAR_FUNCTION_ENTRY,
55: 		             [&](CatalogEntry *entry) { result->entries.push_back(entry); });
56: 	});
57: 
58: 	return move(result);
59: }
60: 
61: void AddFunction(BaseScalarFunction &f, idx_t &count, DataChunk &output, bool is_aggregate) {
62: 	output.SetValue(0, count, Value(f.name));
63: 	output.SetValue(1, count, Value(is_aggregate ? "AGGREGATE" : "SCALAR"));
64: 	auto result_data = FlatVector::GetData<list_entry_t>(output.data[2]);
65: 	result_data[count].offset = ListVector::GetListSize(output.data[2]);
66: 	result_data[count].length = f.arguments.size();
67: 	string parameters;
68: 	for (idx_t i = 0; i < f.arguments.size(); i++) {
69: 		auto val = Value(f.arguments[i].ToString());
70: 		ListVector::PushBack(output.data[2], val);
71: 	}
72: 
73: 	output.SetValue(3, count, f.varargs.id() != LogicalTypeId::INVALID ? Value(f.varargs.ToString()) : Value());
74: 	output.SetValue(4, count, f.return_type.ToString());
75: 	output.SetValue(5, count, Value::BOOLEAN(f.has_side_effects));
76: 
77: 	count++;
78: }
79: 
80: static void PragmaFunctionsFunction(ClientContext &context, const FunctionData *bind_data,
81:                                     FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
82: 	auto &data = (PragmaFunctionsData &)*operator_state;
83: 	if (data.offset >= data.entries.size()) {
84: 		// finished returning values
85: 		return;
86: 	}
87: 	idx_t count = 0;
88: 	while (count < STANDARD_VECTOR_SIZE && data.offset < data.entries.size()) {
89: 		auto &entry = data.entries[data.offset];
90: 		switch (entry->type) {
91: 		case CatalogType::SCALAR_FUNCTION_ENTRY: {
92: 			auto &func = (ScalarFunctionCatalogEntry &)*entry;
93: 			if (data.offset_in_entry >= func.functions.size()) {
94: 				data.offset++;
95: 				data.offset_in_entry = 0;
96: 				break;
97: 			}
98: 			AddFunction(func.functions[data.offset_in_entry++], count, output, false);
99: 			break;
100: 		}
101: 		case CatalogType::AGGREGATE_FUNCTION_ENTRY: {
102: 			auto &aggr = (AggregateFunctionCatalogEntry &)*entry;
103: 			if (data.offset_in_entry >= aggr.functions.size()) {
104: 				data.offset++;
105: 				data.offset_in_entry = 0;
106: 				break;
107: 			}
108: 			AddFunction(aggr.functions[data.offset_in_entry++], count, output, true);
109: 			break;
110: 		}
111: 		default:
112: 			data.offset++;
113: 			break;
114: 		}
115: 	}
116: 	output.SetCardinality(count);
117: }
118: 
119: void PragmaFunctionPragma::RegisterFunction(BuiltinFunctions &set) {
120: 	set.AddFunction(
121: 	    TableFunction("pragma_functions", {}, PragmaFunctionsFunction, PragmaFunctionsBind, PragmaFunctionsInit));
122: }
123: 
124: } // namespace duckdb
[end of src/function/table/system/pragma_functions.cpp]
[start of src/function/table/system/pragma_storage_info.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/parser/qualified_name.hpp"
7: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
8: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
9: 
10: #include "duckdb/common/exception.hpp"
11: #include "duckdb/common/limits.hpp"
12: #include "duckdb/storage/data_table.hpp"
13: 
14: #include <algorithm>
15: 
16: namespace duckdb {
17: 
18: struct PragmaStorageFunctionData : public TableFunctionData {
19: 	explicit PragmaStorageFunctionData(TableCatalogEntry *table_entry) : table_entry(table_entry) {
20: 	}
21: 
22: 	TableCatalogEntry *table_entry;
23: 	vector<vector<Value>> storage_info;
24: };
25: 
26: struct PragmaStorageOperatorData : public FunctionOperatorData {
27: 	PragmaStorageOperatorData() : offset(0) {
28: 	}
29: 
30: 	idx_t offset;
31: };
32: 
33: static unique_ptr<FunctionData> PragmaStorageInfoBind(ClientContext &context, vector<Value> &inputs,
34:                                                       unordered_map<string, Value> &named_parameters,
35:                                                       vector<LogicalType> &input_table_types,
36:                                                       vector<string> &input_table_names,
37:                                                       vector<LogicalType> &return_types, vector<string> &names) {
38: 	names.emplace_back("row_group_id");
39: 	return_types.emplace_back(LogicalType::BIGINT);
40: 
41: 	names.emplace_back("column_name");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("column_id");
45: 	return_types.emplace_back(LogicalType::BIGINT);
46: 
47: 	names.emplace_back("column_path");
48: 	return_types.emplace_back(LogicalType::VARCHAR);
49: 
50: 	names.emplace_back("segment_id");
51: 	return_types.emplace_back(LogicalType::BIGINT);
52: 
53: 	names.emplace_back("segment_type");
54: 	return_types.emplace_back(LogicalType::VARCHAR);
55: 
56: 	names.emplace_back("start");
57: 	return_types.emplace_back(LogicalType::BIGINT);
58: 
59: 	names.emplace_back("count");
60: 	return_types.emplace_back(LogicalType::BIGINT);
61: 
62: 	names.emplace_back("compression");
63: 	return_types.emplace_back(LogicalType::VARCHAR);
64: 
65: 	names.emplace_back("stats");
66: 	return_types.emplace_back(LogicalType::VARCHAR);
67: 
68: 	names.emplace_back("has_updates");
69: 	return_types.emplace_back(LogicalType::BOOLEAN);
70: 
71: 	names.emplace_back("persistent");
72: 	return_types.emplace_back(LogicalType::BOOLEAN);
73: 
74: 	names.emplace_back("block_id");
75: 	return_types.emplace_back(LogicalType::BIGINT);
76: 
77: 	names.emplace_back("block_offset");
78: 	return_types.emplace_back(LogicalType::BIGINT);
79: 
80: 	auto qname = QualifiedName::Parse(inputs[0].GetValue<string>());
81: 
82: 	// look up the table name in the catalog
83: 	auto &catalog = Catalog::GetCatalog(context);
84: 	auto entry = catalog.GetEntry(context, CatalogType::TABLE_ENTRY, qname.schema, qname.name);
85: 	if (entry->type != CatalogType::TABLE_ENTRY) {
86: 		throw Exception("storage_info requires a table as parameter");
87: 	}
88: 	auto table_entry = (TableCatalogEntry *)entry;
89: 
90: 	auto result = make_unique<PragmaStorageFunctionData>(table_entry);
91: 	result->storage_info = table_entry->storage->GetStorageInfo();
92: 	return move(result);
93: }
94: 
95: unique_ptr<FunctionOperatorData> PragmaStorageInfoInit(ClientContext &context, const FunctionData *bind_data,
96:                                                        const vector<column_t> &column_ids,
97:                                                        TableFilterCollection *filters) {
98: 	return make_unique<PragmaStorageOperatorData>();
99: }
100: 
101: static void PragmaStorageInfoFunction(ClientContext &context, const FunctionData *bind_data_p,
102:                                       FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
103: 	auto &bind_data = (PragmaStorageFunctionData &)*bind_data_p;
104: 	auto &data = (PragmaStorageOperatorData &)*operator_state;
105: 	idx_t count = 0;
106: 	while (data.offset < bind_data.storage_info.size() && count < STANDARD_VECTOR_SIZE) {
107: 		auto &entry = bind_data.storage_info[data.offset++];
108: 		D_ASSERT(entry.size() + 1 == output.ColumnCount());
109: 		idx_t result_idx = 0;
110: 		for (idx_t col_idx = 0; col_idx < entry.size(); col_idx++, result_idx++) {
111: 			if (col_idx == 1) {
112: 				// write the column name
113: 				auto column_index = entry[col_idx].GetValue<int64_t>();
114: 				output.SetValue(result_idx, count, Value(bind_data.table_entry->columns[column_index].name));
115: 				result_idx++;
116: 			}
117: 			output.SetValue(result_idx, count, entry[col_idx]);
118: 		}
119: 
120: 		count++;
121: 	}
122: 	output.SetCardinality(count);
123: }
124: 
125: void PragmaStorageInfo::RegisterFunction(BuiltinFunctions &set) {
126: 	set.AddFunction(TableFunction("pragma_storage_info", {LogicalType::VARCHAR}, PragmaStorageInfoFunction,
127: 	                              PragmaStorageInfoBind, PragmaStorageInfoInit));
128: }
129: 
130: } // namespace duckdb
[end of src/function/table/system/pragma_storage_info.cpp]
[start of src/function/table/system/pragma_table_info.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/parser/qualified_name.hpp"
7: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
8: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
9: 
10: #include "duckdb/common/exception.hpp"
11: #include "duckdb/common/limits.hpp"
12: 
13: #include <algorithm>
14: 
15: namespace duckdb {
16: 
17: struct PragmaTableFunctionData : public TableFunctionData {
18: 	explicit PragmaTableFunctionData(CatalogEntry *entry_p) : entry(entry_p) {
19: 	}
20: 
21: 	CatalogEntry *entry;
22: };
23: 
24: struct PragmaTableOperatorData : public FunctionOperatorData {
25: 	PragmaTableOperatorData() : offset(0) {
26: 	}
27: 	idx_t offset;
28: };
29: 
30: static unique_ptr<FunctionData> PragmaTableInfoBind(ClientContext &context, vector<Value> &inputs,
31:                                                     unordered_map<string, Value> &named_parameters,
32:                                                     vector<LogicalType> &input_table_types,
33:                                                     vector<string> &input_table_names,
34:                                                     vector<LogicalType> &return_types, vector<string> &names) {
35: 	names.emplace_back("cid");
36: 	return_types.emplace_back(LogicalType::INTEGER);
37: 
38: 	names.emplace_back("name");
39: 	return_types.emplace_back(LogicalType::VARCHAR);
40: 
41: 	names.emplace_back("type");
42: 	return_types.emplace_back(LogicalType::VARCHAR);
43: 
44: 	names.emplace_back("notnull");
45: 	return_types.emplace_back(LogicalType::BOOLEAN);
46: 
47: 	names.emplace_back("dflt_value");
48: 	return_types.emplace_back(LogicalType::VARCHAR);
49: 
50: 	names.emplace_back("pk");
51: 	return_types.emplace_back(LogicalType::BOOLEAN);
52: 
53: 	auto qname = QualifiedName::Parse(inputs[0].GetValue<string>());
54: 
55: 	// look up the table name in the catalog
56: 	auto &catalog = Catalog::GetCatalog(context);
57: 	auto entry = catalog.GetEntry(context, CatalogType::TABLE_ENTRY, qname.schema, qname.name);
58: 	return make_unique<PragmaTableFunctionData>(entry);
59: }
60: 
61: unique_ptr<FunctionOperatorData> PragmaTableInfoInit(ClientContext &context, const FunctionData *bind_data,
62:                                                      const vector<column_t> &column_ids,
63:                                                      TableFilterCollection *filters) {
64: 	return make_unique<PragmaTableOperatorData>();
65: }
66: 
67: static void CheckConstraints(TableCatalogEntry *table, idx_t oid, bool &out_not_null, bool &out_pk) {
68: 	out_not_null = false;
69: 	out_pk = false;
70: 	// check all constraints
71: 	// FIXME: this is pretty inefficient, it probably doesn't matter
72: 	for (auto &constraint : table->bound_constraints) {
73: 		switch (constraint->type) {
74: 		case ConstraintType::NOT_NULL: {
75: 			auto &not_null = (BoundNotNullConstraint &)*constraint;
76: 			if (not_null.index == oid) {
77: 				out_not_null = true;
78: 			}
79: 			break;
80: 		}
81: 		case ConstraintType::UNIQUE: {
82: 			auto &unique = (BoundUniqueConstraint &)*constraint;
83: 			if (unique.is_primary_key && unique.key_set.find(oid) != unique.key_set.end()) {
84: 				out_pk = true;
85: 			}
86: 			break;
87: 		}
88: 		default:
89: 			break;
90: 		}
91: 	}
92: }
93: 
94: static void PragmaTableInfoTable(PragmaTableOperatorData &data, TableCatalogEntry *table, DataChunk &output) {
95: 	if (data.offset >= table->columns.size()) {
96: 		// finished returning values
97: 		return;
98: 	}
99: 	// start returning values
100: 	// either fill up the chunk or return all the remaining columns
101: 	idx_t next = MinValue<idx_t>(data.offset + STANDARD_VECTOR_SIZE, table->columns.size());
102: 	output.SetCardinality(next - data.offset);
103: 
104: 	for (idx_t i = data.offset; i < next; i++) {
105: 		bool not_null, pk;
106: 		auto index = i - data.offset;
107: 		auto &column = table->columns[i];
108: 		D_ASSERT(column.oid < (idx_t)NumericLimits<int32_t>::Maximum());
109: 		CheckConstraints(table, column.oid, not_null, pk);
110: 
111: 		// return values:
112: 		// "cid", PhysicalType::INT32
113: 		output.SetValue(0, index, Value::INTEGER((int32_t)column.oid));
114: 		// "name", PhysicalType::VARCHAR
115: 		output.SetValue(1, index, Value(column.name));
116: 		// "type", PhysicalType::VARCHAR
117: 		output.SetValue(2, index, Value(column.type.ToString()));
118: 		// "notnull", PhysicalType::BOOL
119: 		output.SetValue(3, index, Value::BOOLEAN(not_null));
120: 		// "dflt_value", PhysicalType::VARCHAR
121: 		Value def_value = column.default_value ? Value(column.default_value->ToString()) : Value();
122: 		output.SetValue(4, index, def_value);
123: 		// "pk", PhysicalType::BOOL
124: 		output.SetValue(5, index, Value::BOOLEAN(pk));
125: 	}
126: 	data.offset = next;
127: }
128: 
129: static void PragmaTableInfoView(PragmaTableOperatorData &data, ViewCatalogEntry *view, DataChunk &output) {
130: 	if (data.offset >= view->types.size()) {
131: 		// finished returning values
132: 		return;
133: 	}
134: 	// start returning values
135: 	// either fill up the chunk or return all the remaining columns
136: 	idx_t next = MinValue<idx_t>(data.offset + STANDARD_VECTOR_SIZE, view->types.size());
137: 	output.SetCardinality(next - data.offset);
138: 
139: 	for (idx_t i = data.offset; i < next; i++) {
140: 		auto index = i - data.offset;
141: 		auto type = view->types[index];
142: 		auto &name = view->aliases[index];
143: 		// return values:
144: 		// "cid", PhysicalType::INT32
145: 
146: 		output.SetValue(0, index, Value::INTEGER((int32_t)index));
147: 		// "name", PhysicalType::VARCHAR
148: 		output.SetValue(1, index, Value(name));
149: 		// "type", PhysicalType::VARCHAR
150: 		output.SetValue(2, index, Value(type.ToString()));
151: 		// "notnull", PhysicalType::BOOL
152: 		output.SetValue(3, index, Value::BOOLEAN(false));
153: 		// "dflt_value", PhysicalType::VARCHAR
154: 		output.SetValue(4, index, Value());
155: 		// "pk", PhysicalType::BOOL
156: 		output.SetValue(5, index, Value::BOOLEAN(false));
157: 	}
158: 	data.offset = next;
159: }
160: 
161: static void PragmaTableInfoFunction(ClientContext &context, const FunctionData *bind_data_p,
162:                                     FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
163: 	auto &bind_data = (PragmaTableFunctionData &)*bind_data_p;
164: 	auto &state = (PragmaTableOperatorData &)*operator_state;
165: 	switch (bind_data.entry->type) {
166: 	case CatalogType::TABLE_ENTRY:
167: 		PragmaTableInfoTable(state, (TableCatalogEntry *)bind_data.entry, output);
168: 		break;
169: 	case CatalogType::VIEW_ENTRY:
170: 		PragmaTableInfoView(state, (ViewCatalogEntry *)bind_data.entry, output);
171: 		break;
172: 	default:
173: 		throw NotImplementedException("Unimplemented catalog type for pragma_table_info");
174: 	}
175: }
176: 
177: void PragmaTableInfo::RegisterFunction(BuiltinFunctions &set) {
178: 	set.AddFunction(TableFunction("pragma_table_info", {LogicalType::VARCHAR}, PragmaTableInfoFunction,
179: 	                              PragmaTableInfoBind, PragmaTableInfoInit));
180: }
181: 
182: } // namespace duckdb
[end of src/function/table/system/pragma_table_info.cpp]
[start of src/function/table/unnest.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/common/algorithm.hpp"
3: 
4: namespace duckdb {
5: 
6: struct UnnestFunctionData : public TableFunctionData {
7: 	explicit UnnestFunctionData(Value value) : value(move(value)) {
8: 	}
9: 
10: 	Value value;
11: };
12: 
13: struct UnnestOperatorData : public FunctionOperatorData {
14: 	UnnestOperatorData() : current_count(0) {
15: 	}
16: 
17: 	idx_t current_count;
18: };
19: 
20: static unique_ptr<FunctionData> UnnestBind(ClientContext &context, vector<Value> &inputs,
21:                                            unordered_map<string, Value> &named_parameters,
22:                                            vector<LogicalType> &input_table_types, vector<string> &input_table_names,
23:                                            vector<LogicalType> &return_types, vector<string> &names) {
24: 	return_types.push_back(ListType::GetChildType(inputs[0].type()));
25: 	names.push_back(inputs[0].ToString());
26: 	return make_unique<UnnestFunctionData>(inputs[0]);
27: }
28: 
29: static unique_ptr<FunctionOperatorData> UnnestInit(ClientContext &context, const FunctionData *bind_data,
30:                                                    const vector<column_t> &column_ids, TableFilterCollection *filters) {
31: 	return make_unique<UnnestOperatorData>();
32: }
33: 
34: static void UnnestFunction(ClientContext &context, const FunctionData *bind_data_p,
35:                            FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
36: 	auto &bind_data = (UnnestFunctionData &)*bind_data_p;
37: 	auto &state = (UnnestOperatorData &)*operator_state;
38: 
39: 	auto &list_value = ListValue::GetChildren(bind_data.value);
40: 	idx_t count = 0;
41: 	for (; state.current_count < list_value.size() && count < STANDARD_VECTOR_SIZE; state.current_count++) {
42: 		output.data[0].SetValue(count, list_value[state.current_count]);
43: 		count++;
44: 	}
45: 	output.SetCardinality(count);
46: }
47: 
48: void UnnestTableFunction::RegisterFunction(BuiltinFunctions &set) {
49: 	TableFunction unnest_function("unnest", {LogicalTypeId::LIST}, UnnestFunction, UnnestBind, UnnestInit);
50: 	set.AddFunction(unnest_function);
51: }
52: 
53: } // namespace duckdb
[end of src/function/table/unnest.cpp]
[start of src/function/table/version/pragma_version.cpp]
1: #include "duckdb/function/table/system_functions.hpp"
2: #include "duckdb/main/database.hpp"
3: 
4: namespace duckdb {
5: 
6: struct PragmaVersionData : public FunctionOperatorData {
7: 	PragmaVersionData() : finished(false) {
8: 	}
9: 	bool finished;
10: };
11: 
12: static unique_ptr<FunctionData> PragmaVersionBind(ClientContext &context, vector<Value> &inputs,
13:                                                   unordered_map<string, Value> &named_parameters,
14:                                                   vector<LogicalType> &input_table_types,
15:                                                   vector<string> &input_table_names, vector<LogicalType> &return_types,
16:                                                   vector<string> &names) {
17: 	names.emplace_back("library_version");
18: 	return_types.emplace_back(LogicalType::VARCHAR);
19: 	names.emplace_back("source_id");
20: 	return_types.emplace_back(LogicalType::VARCHAR);
21: 	return nullptr;
22: }
23: 
24: static unique_ptr<FunctionOperatorData> PragmaVersionInit(ClientContext &context, const FunctionData *bind_data,
25:                                                           const vector<column_t> &column_ids,
26:                                                           TableFilterCollection *filters) {
27: 	return make_unique<PragmaVersionData>();
28: }
29: 
30: static void PragmaVersionFunction(ClientContext &context, const FunctionData *bind_data,
31:                                   FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
32: 	auto &data = (PragmaVersionData &)*operator_state;
33: 	if (data.finished) {
34: 		// finished returning values
35: 		return;
36: 	}
37: 	output.SetCardinality(1);
38: 	output.SetValue(0, 0, DuckDB::LibraryVersion());
39: 	output.SetValue(1, 0, DuckDB::SourceID());
40: 	data.finished = true;
41: }
42: 
43: void PragmaVersion::RegisterFunction(BuiltinFunctions &set) {
44: 	set.AddFunction(TableFunction("pragma_version", {}, PragmaVersionFunction, PragmaVersionBind, PragmaVersionInit));
45: }
46: 
47: const char *DuckDB::SourceID() {
48: 	return DUCKDB_SOURCE_ID;
49: }
50: 
51: const char *DuckDB::LibraryVersion() {
52: 	return DUCKDB_VERSION;
53: }
54: 
55: string DuckDB::Platform() {
56: 	string os = "linux";
57: 	string arch = "amd64";
58: #ifdef _WIN32
59: 	os = "windows";
60: #elif defined(__APPLE__)
61: 	os = "osx";
62: #endif
63: #if defined(__aarch64__) || defined(__ARM_ARCH_ISA_A64)
64: 	arch = "arm64";
65: #endif
66: 	return os + "_" + arch;
67: }
68: 
69: } // namespace duckdb
[end of src/function/table/version/pragma_version.cpp]
[start of src/include/duckdb/function/function.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/function.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/data_chunk.hpp"
12: #include "duckdb/common/unordered_map.hpp"
13: #include "duckdb/common/unordered_set.hpp"
14: #include "duckdb/parser/column_definition.hpp"
15: 
16: namespace duckdb {
17: class CatalogEntry;
18: class Catalog;
19: class ClientContext;
20: class Expression;
21: class ExpressionExecutor;
22: class Transaction;
23: 
24: class AggregateFunction;
25: class AggregateFunctionSet;
26: class CopyFunction;
27: class PragmaFunction;
28: class ScalarFunctionSet;
29: class ScalarFunction;
30: class TableFunctionSet;
31: class TableFunction;
32: 
33: struct PragmaInfo;
34: 
35: struct FunctionData {
36: 	DUCKDB_API virtual ~FunctionData();
37: 
38: 	DUCKDB_API virtual unique_ptr<FunctionData> Copy();
39: 	DUCKDB_API virtual bool Equals(FunctionData &other);
40: 	DUCKDB_API static bool Equals(FunctionData *left, FunctionData *right);
41: };
42: 
43: struct TableFunctionData : public FunctionData {
44: 	// used to pass on projections to table functions that support them. NB, can contain COLUMN_IDENTIFIER_ROW_ID
45: 	vector<idx_t> column_ids;
46: };
47: 
48: struct FunctionParameters {
49: 	vector<Value> values;
50: 	unordered_map<string, Value> named_parameters;
51: };
52: 
53: //! Function is the base class used for any type of function (scalar, aggregate or simple function)
54: class Function {
55: public:
56: 	DUCKDB_API explicit Function(string name);
57: 	DUCKDB_API virtual ~Function();
58: 
59: 	//! The name of the function
60: 	string name;
61: 
62: public:
63: 	//! Returns the formatted string name(arg1, arg2, ...)
64: 	DUCKDB_API static string CallToString(const string &name, const vector<LogicalType> &arguments);
65: 	//! Returns the formatted string name(arg1, arg2..) -> return_type
66: 	DUCKDB_API static string CallToString(const string &name, const vector<LogicalType> &arguments,
67: 	                                      const LogicalType &return_type);
68: 	//! Returns the formatted string name(arg1, arg2.., np1=a, np2=b, ...)
69: 	DUCKDB_API static string CallToString(const string &name, const vector<LogicalType> &arguments,
70: 	                                      const unordered_map<string, LogicalType> &named_parameters);
71: 
72: 	//! Bind a scalar function from the set of functions and input arguments. Returns the index of the chosen function,
73: 	//! returns DConstants::INVALID_INDEX and sets error if none could be found
74: 	DUCKDB_API static idx_t BindFunction(const string &name, vector<ScalarFunction> &functions,
75: 	                                     vector<LogicalType> &arguments, string &error);
76: 	DUCKDB_API static idx_t BindFunction(const string &name, vector<ScalarFunction> &functions,
77: 	                                     vector<unique_ptr<Expression>> &arguments, string &error);
78: 	//! Bind an aggregate function from the set of functions and input arguments. Returns the index of the chosen
79: 	//! function, returns DConstants::INVALID_INDEX and sets error if none could be found
80: 	DUCKDB_API static idx_t BindFunction(const string &name, vector<AggregateFunction> &functions,
81: 	                                     vector<LogicalType> &arguments, string &error);
82: 	DUCKDB_API static idx_t BindFunction(const string &name, vector<AggregateFunction> &functions,
83: 	                                     vector<unique_ptr<Expression>> &arguments, string &error);
84: 	//! Bind a table function from the set of functions and input arguments. Returns the index of the chosen
85: 	//! function, returns DConstants::INVALID_INDEX and sets error if none could be found
86: 	DUCKDB_API static idx_t BindFunction(const string &name, vector<TableFunction> &functions,
87: 	                                     vector<LogicalType> &arguments, string &error);
88: 	DUCKDB_API static idx_t BindFunction(const string &name, vector<TableFunction> &functions,
89: 	                                     vector<unique_ptr<Expression>> &arguments, string &error);
90: 	//! Bind a pragma function from the set of functions and input arguments
91: 	DUCKDB_API static idx_t BindFunction(const string &name, vector<PragmaFunction> &functions, PragmaInfo &info,
92: 	                                     string &error);
93: };
94: 
95: class SimpleFunction : public Function {
96: public:
97: 	DUCKDB_API SimpleFunction(string name, vector<LogicalType> arguments,
98: 	                          LogicalType varargs = LogicalType(LogicalTypeId::INVALID));
99: 	DUCKDB_API ~SimpleFunction() override;
100: 
101: 	//! The set of arguments of the function
102: 	vector<LogicalType> arguments;
103: 	//! The type of varargs to support, or LogicalTypeId::INVALID if the function does not accept variable length
104: 	//! arguments
105: 	LogicalType varargs;
106: 
107: public:
108: 	DUCKDB_API virtual string ToString();
109: 
110: 	DUCKDB_API bool HasVarArgs() const;
111: };
112: 
113: class SimpleNamedParameterFunction : public SimpleFunction {
114: public:
115: 	DUCKDB_API SimpleNamedParameterFunction(string name, vector<LogicalType> arguments,
116: 	                                        LogicalType varargs = LogicalType(LogicalTypeId::INVALID));
117: 	DUCKDB_API ~SimpleNamedParameterFunction() override;
118: 
119: 	//! The named parameters of the function
120: 	unordered_map<string, LogicalType> named_parameters;
121: 
122: public:
123: 	DUCKDB_API string ToString() override;
124: 	DUCKDB_API bool HasNamedParameters();
125: 
126: 	DUCKDB_API void EvaluateInputParameters(vector<LogicalType> &arguments, vector<Value> &parameters,
127: 	                                        unordered_map<string, Value> &named_parameters,
128: 	                                        vector<unique_ptr<ParsedExpression>> &children);
129: };
130: 
131: class BaseScalarFunction : public SimpleFunction {
132: public:
133: 	DUCKDB_API BaseScalarFunction(string name, vector<LogicalType> arguments, LogicalType return_type,
134: 	                              bool has_side_effects, LogicalType varargs = LogicalType(LogicalTypeId::INVALID));
135: 	DUCKDB_API ~BaseScalarFunction() override;
136: 
137: 	//! Return type of the function
138: 	LogicalType return_type;
139: 	//! Whether or not the function has side effects (e.g. sequence increments, random() functions, NOW()). Functions
140: 	//! with side-effects cannot be constant-folded.
141: 	bool has_side_effects;
142: 
143: public:
144: 	DUCKDB_API hash_t Hash() const;
145: 
146: 	//! Cast a set of expressions to the arguments of this function
147: 	DUCKDB_API void CastToFunctionArguments(vector<unique_ptr<Expression>> &children);
148: 
149: 	DUCKDB_API string ToString() override;
150: };
151: 
152: class BuiltinFunctions {
153: public:
154: 	BuiltinFunctions(ClientContext &transaction, Catalog &catalog);
155: 
156: 	//! Initialize a catalog with all built-in functions
157: 	void Initialize();
158: 
159: public:
160: 	void AddFunction(AggregateFunctionSet set);
161: 	void AddFunction(AggregateFunction function);
162: 	void AddFunction(ScalarFunctionSet set);
163: 	void AddFunction(PragmaFunction function);
164: 	void AddFunction(const string &name, vector<PragmaFunction> functions);
165: 	void AddFunction(ScalarFunction function);
166: 	void AddFunction(const vector<string> &names, ScalarFunction function);
167: 	void AddFunction(TableFunctionSet set);
168: 	void AddFunction(TableFunction function);
169: 	void AddFunction(CopyFunction function);
170: 
171: 	void AddCollation(string name, ScalarFunction function, bool combinable = false,
172: 	                  bool not_required_for_equality = false);
173: 
174: private:
175: 	ClientContext &context;
176: 	Catalog &catalog;
177: 
178: private:
179: 	template <class T>
180: 	void Register() {
181: 		T::RegisterFunction(*this);
182: 	}
183: 
184: 	// table-producing functions
185: 	void RegisterSQLiteFunctions();
186: 	void RegisterReadFunctions();
187: 	void RegisterTableFunctions();
188: 	void RegisterArrowFunctions();
189: 
190: 	// aggregates
191: 	void RegisterAlgebraicAggregates();
192: 	void RegisterDistributiveAggregates();
193: 	void RegisterNestedAggregates();
194: 	void RegisterHolisticAggregates();
195: 	void RegisterRegressiveAggregates();
196: 
197: 	// scalar functions
198: 	void RegisterDateFunctions();
199: 	void RegisterEnumFunctions();
200: 	void RegisterGenericFunctions();
201: 	void RegisterMathFunctions();
202: 	void RegisterOperators();
203: 	void RegisterStringFunctions();
204: 	void RegisterNestedFunctions();
205: 	void RegisterSequenceFunctions();
206: 	void RegisterTrigonometricsFunctions();
207: 
208: 	// pragmas
209: 	void RegisterPragmaFunctions();
210: };
211: 
212: } // namespace duckdb
[end of src/include/duckdb/function/function.hpp]
[start of src/include/duckdb/function/pragma_function.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/pragma_function.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/function/function.hpp"
12: #include "duckdb/parser/parsed_data/pragma_info.hpp"
13: #include "duckdb/common/unordered_map.hpp"
14: 
15: namespace duckdb {
16: class ClientContext;
17: 
18: //! Return a substitute query to execute instead of this pragma statement
19: typedef string (*pragma_query_t)(ClientContext &context, const FunctionParameters &parameters);
20: //! Execute the main pragma function
21: typedef void (*pragma_function_t)(ClientContext &context, const FunctionParameters &parameters);
22: 
23: //! Pragma functions are invoked by calling PRAGMA x
24: //! Pragma functions come in three types:
25: //! * Call: function call, e.g. PRAGMA table_info('tbl')
26: //!   -> call statements can take multiple parameters
27: //! * Statement: statement without parameters, e.g. PRAGMA show_tables
28: //!   -> this is similar to a call pragma but without parameters
29: //! Pragma functions can either return a new query to execute (pragma_query_t)
30: //! or they can
31: class PragmaFunction : public SimpleNamedParameterFunction {
32: public:
33: 	// Call
34: 	DUCKDB_API static PragmaFunction PragmaCall(const string &name, pragma_query_t query, vector<LogicalType> arguments,
35: 	                                            LogicalType varargs = LogicalType::INVALID);
36: 	DUCKDB_API static PragmaFunction PragmaCall(const string &name, pragma_function_t function,
37: 	                                            vector<LogicalType> arguments,
38: 	                                            LogicalType varargs = LogicalType::INVALID);
39: 	// Statement
40: 	DUCKDB_API static PragmaFunction PragmaStatement(const string &name, pragma_query_t query);
41: 	DUCKDB_API static PragmaFunction PragmaStatement(const string &name, pragma_function_t function);
42: 
43: 	DUCKDB_API string ToString() override;
44: 
45: public:
46: 	PragmaType type;
47: 
48: 	pragma_query_t query;
49: 	pragma_function_t function;
50: 	unordered_map<string, LogicalType> named_parameters;
51: 
52: private:
53: 	PragmaFunction(string name, PragmaType pragma_type, pragma_query_t query, pragma_function_t function,
54: 	               vector<LogicalType> arguments, LogicalType varargs);
55: };
56: 
57: } // namespace duckdb
[end of src/include/duckdb/function/pragma_function.hpp]
[start of src/include/duckdb/function/table/arrow.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/table/arrow.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/function/table_function.hpp"
12: #include "duckdb/parallel/parallel_state.hpp"
13: #include "duckdb/common/arrow_wrapper.hpp"
14: #include "duckdb/common/atomic.hpp"
15: #include "duckdb/common/mutex.hpp"
16: #include "duckdb/common/thread.hpp"
17: #include <map>
18: #include <condition_variable>
19: 
20: namespace duckdb {
21: //===--------------------------------------------------------------------===//
22: // Arrow Variable Size Types
23: //===--------------------------------------------------------------------===//
24: enum class ArrowVariableSizeType : uint8_t { FIXED_SIZE = 0, NORMAL = 1, SUPER_SIZE = 2 };
25: 
26: //===--------------------------------------------------------------------===//
27: // Arrow Time/Date Types
28: //===--------------------------------------------------------------------===//
29: enum class ArrowDateTimeType : uint8_t {
30: 	MILLISECONDS = 0,
31: 	MICROSECONDS = 1,
32: 	NANOSECONDS = 2,
33: 	SECONDS = 3,
34: 	DAYS = 4,
35: 	MONTHS = 5
36: };
37: struct ArrowConvertData {
38: 	ArrowConvertData(LogicalType type) : dictionary_type(type) {};
39: 	ArrowConvertData() {};
40: 	//! Hold type of dictionary
41: 	LogicalType dictionary_type;
42: 	//! If its a variable size type (e.g., strings, blobs, lists) holds which type it is
43: 	vector<std::pair<ArrowVariableSizeType, idx_t>> variable_sz_type;
44: 	//! If this is a date/time holds its precision
45: 	vector<ArrowDateTimeType> date_time_precision;
46: };
47: 
48: struct ArrowScanFunctionData : public TableFunctionData {
49: #ifndef DUCKDB_NO_THREADS
50: 
51: 	ArrowScanFunctionData(idx_t rows_per_thread_p,
52: 	                      unique_ptr<ArrowArrayStreamWrapper> (*scanner_producer_p)(
53: 	                          uintptr_t stream_factory_ptr,
54: 	                          std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,
55: 	                          TableFilterCollection *filters),
56: 	                      uintptr_t stream_factory_ptr_p, std::thread::id thread_id_p)
57: 	    : lines_read(0), rows_per_thread(rows_per_thread_p), stream_factory_ptr(stream_factory_ptr_p),
58: 	      scanner_producer(scanner_producer_p), number_of_rows(0), thread_id(thread_id_p) {
59: 	}
60: #endif
61: 
62: 	ArrowScanFunctionData(idx_t rows_per_thread_p,
63: 	                      unique_ptr<ArrowArrayStreamWrapper> (*scanner_producer_p)(
64: 	                          uintptr_t stream_factory_ptr,
65: 	                          std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,
66: 	                          TableFilterCollection *filters),
67: 	                      uintptr_t stream_factory_ptr_p)
68: 	    : lines_read(0), rows_per_thread(rows_per_thread_p), stream_factory_ptr(stream_factory_ptr_p),
69: 	      scanner_producer(scanner_producer_p), number_of_rows(0) {
70: 	}
71: 	//! This holds the original list type (col_idx, [ArrowListType,size])
72: 	std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> arrow_convert_data;
73: 	std::atomic<idx_t> lines_read;
74: 	ArrowSchemaWrapper schema_root;
75: 	idx_t rows_per_thread;
76: 	//! Pointer to the scanner factory
77: 	uintptr_t stream_factory_ptr;
78: 	//! Pointer to the scanner factory produce
79: 	unique_ptr<ArrowArrayStreamWrapper> (*scanner_producer)(
80: 	    uintptr_t stream_factory_ptr,
81: 	    std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,
82: 	    TableFilterCollection *filters);
83: 	//! Number of rows (Used in cardinality and progress bar)
84: 	int64_t number_of_rows;
85: #ifndef DUCKDB_NO_THREADS
86: 	// Thread that made first call in the binder
87: 	std::thread::id thread_id;
88: #endif
89: };
90: 
91: struct ArrowScanState : public FunctionOperatorData {
92: 	explicit ArrowScanState(unique_ptr<ArrowArrayWrapper> current_chunk) : chunk(move(current_chunk)) {
93: 	}
94: 	unique_ptr<ArrowArrayStreamWrapper> stream;
95: 	unique_ptr<ArrowArrayWrapper> chunk;
96: 	idx_t chunk_offset = 0;
97: 	vector<column_t> column_ids;
98: 	//! Store child vectors for Arrow Dictionary Vectors (col-idx,vector)
99: 	unordered_map<idx_t, unique_ptr<Vector>> arrow_dictionary_vectors;
100: 	TableFilterCollection *filters = nullptr;
101: };
102: 
103: struct ParallelArrowScanState : public ParallelState {
104: 	ParallelArrowScanState() {
105: 	}
106: 	unique_ptr<ArrowArrayStreamWrapper> stream;
107: 	std::mutex main_mutex;
108: 	bool ready = false;
109: };
110: 
111: struct ArrowTableFunction {
112: public:
113: 	static void RegisterFunction(BuiltinFunctions &set);
114: 
115: private:
116: 	//! Binds an arrow table
117: 	static unique_ptr<FunctionData> ArrowScanBind(ClientContext &context, vector<Value> &inputs,
118: 	                                              unordered_map<string, Value> &named_parameters,
119: 	                                              vector<LogicalType> &input_table_types,
120: 	                                              vector<string> &input_table_names, vector<LogicalType> &return_types,
121: 	                                              vector<string> &names);
122: 	//! Actual conversion from Arrow to DuckDB
123: 	static void ArrowToDuckDB(ArrowScanState &scan_state,
124: 	                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,
125: 	                          DataChunk &output, idx_t start);
126: 
127: 	//! -----Single Thread Functions:-----
128: 	//! Initialize Single Thread Scan
129: 	static unique_ptr<FunctionOperatorData> ArrowScanInit(ClientContext &context, const FunctionData *bind_data,
130: 	                                                      const vector<column_t> &column_ids,
131: 	                                                      TableFilterCollection *filters);
132: 
133: 	//! Scan Function for Single Thread Execution
134: 	static void ArrowScanFunction(ClientContext &context, const FunctionData *bind_data,
135: 	                              FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output);
136: 
137: 	//! -----Multi Thread Functions:-----
138: 	//! Initialize Parallel State
139: 	static unique_ptr<ParallelState> ArrowScanInitParallelState(ClientContext &context, const FunctionData *bind_data_p,
140: 	                                                            const vector<column_t> &column_ids,
141: 	                                                            TableFilterCollection *filters);
142: 	//! Initialize Parallel Scans
143: 	static unique_ptr<FunctionOperatorData> ArrowScanParallelInit(ClientContext &context,
144: 	                                                              const FunctionData *bind_data_p, ParallelState *state,
145: 	                                                              const vector<column_t> &column_ids,
146: 	                                                              TableFilterCollection *filters);
147: 	//! Defines Maximum Number of Threads
148: 	static idx_t ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p);
149: 	//! Scan Function for Parallel Execution
150: 	static void ArrowScanFunctionParallel(ClientContext &context, const FunctionData *bind_data,
151: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
152: 	                                      ParallelState *parallel_state_p);
153: 	//! Get next chunk for the running thread
154: 	static bool ArrowScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
155: 	                                       FunctionOperatorData *operator_state, ParallelState *parallel_state_p);
156: 
157: 	//! -----Utility Functions:-----
158: 	//! Gets Arrow Table's Cardinality
159: 	static unique_ptr<NodeStatistics> ArrowScanCardinality(ClientContext &context, const FunctionData *bind_data);
160: 	//! Gets the progress on the table scan, used for Progress Bars
161: 	static double ArrowProgress(ClientContext &context, const FunctionData *bind_data_p);
162: };
163: 
164: } // namespace duckdb
[end of src/include/duckdb/function/table/arrow.hpp]
[start of src/include/duckdb/function/table_function.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/table_function.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_map.hpp"
12: #include "duckdb/function/function.hpp"
13: #include "duckdb/storage/statistics/node_statistics.hpp"
14: 
15: #include <functional>
16: 
17: namespace duckdb {
18: class BaseStatistics;
19: class LogicalGet;
20: struct ParallelState;
21: class TableFilterSet;
22: 
23: struct FunctionOperatorData {
24: 	DUCKDB_API virtual ~FunctionOperatorData();
25: };
26: 
27: struct TableFilterCollection {
28: 	DUCKDB_API explicit TableFilterCollection(TableFilterSet *table_filters);
29: 
30: 	TableFilterSet *table_filters;
31: };
32: 
33: typedef unique_ptr<FunctionData> (*table_function_bind_t)(ClientContext &context, vector<Value> &inputs,
34:                                                           unordered_map<string, Value> &named_parameters,
35:                                                           vector<LogicalType> &input_table_types,
36:                                                           vector<string> &input_table_names,
37:                                                           vector<LogicalType> &return_types, vector<string> &names);
38: typedef unique_ptr<FunctionOperatorData> (*table_function_init_t)(ClientContext &context, const FunctionData *bind_data,
39:                                                                   const vector<column_t> &column_ids,
40:                                                                   TableFilterCollection *filters);
41: typedef unique_ptr<BaseStatistics> (*table_statistics_t)(ClientContext &context, const FunctionData *bind_data,
42:                                                          column_t column_index);
43: typedef void (*table_function_t)(ClientContext &context, const FunctionData *bind_data,
44:                                  FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output);
45: 
46: typedef void (*table_function_parallel_t)(ClientContext &context, const FunctionData *bind_data,
47:                                           FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
48:                                           ParallelState *parallel_state);
49: 
50: typedef void (*table_function_cleanup_t)(ClientContext &context, const FunctionData *bind_data,
51:                                          FunctionOperatorData *operator_state);
52: typedef idx_t (*table_function_max_threads_t)(ClientContext &context, const FunctionData *bind_data);
53: typedef unique_ptr<ParallelState> (*table_function_init_parallel_state_t)(ClientContext &context,
54:                                                                           const FunctionData *bind_data,
55:                                                                           const vector<column_t> &column_ids,
56:                                                                           TableFilterCollection *filters);
57: typedef unique_ptr<FunctionOperatorData> (*table_function_init_parallel_t)(ClientContext &context,
58:                                                                            const FunctionData *bind_data,
59:                                                                            ParallelState *state,
60:                                                                            const vector<column_t> &column_ids,
61:                                                                            TableFilterCollection *filters);
62: typedef bool (*table_function_parallel_state_next_t)(ClientContext &context, const FunctionData *bind_data,
63:                                                      FunctionOperatorData *state, ParallelState *parallel_state);
64: typedef double (*table_function_progress_t)(ClientContext &context, const FunctionData *bind_data);
65: typedef void (*table_function_dependency_t)(unordered_set<CatalogEntry *> &dependencies, const FunctionData *bind_data);
66: typedef unique_ptr<NodeStatistics> (*table_function_cardinality_t)(ClientContext &context,
67:                                                                    const FunctionData *bind_data);
68: typedef void (*table_function_pushdown_complex_filter_t)(ClientContext &context, LogicalGet &get,
69:                                                          FunctionData *bind_data,
70:                                                          vector<unique_ptr<Expression>> &filters);
71: typedef string (*table_function_to_string_t)(const FunctionData *bind_data);
72: 
73: class TableFunction : public SimpleNamedParameterFunction {
74: public:
75: 	DUCKDB_API
76: 	TableFunction(string name, vector<LogicalType> arguments, table_function_t function,
77: 	              table_function_bind_t bind = nullptr, table_function_init_t init = nullptr,
78: 	              table_statistics_t statistics = nullptr, table_function_cleanup_t cleanup = nullptr,
79: 	              table_function_dependency_t dependency = nullptr, table_function_cardinality_t cardinality = nullptr,
80: 	              table_function_pushdown_complex_filter_t pushdown_complex_filter = nullptr,
81: 	              table_function_to_string_t to_string = nullptr, table_function_max_threads_t max_threads = nullptr,
82: 	              table_function_init_parallel_state_t init_parallel_state = nullptr,
83: 	              table_function_parallel_t parallel_function = nullptr,
84: 	              table_function_init_parallel_t parallel_init = nullptr,
85: 	              table_function_parallel_state_next_t parallel_state_next = nullptr, bool projection_pushdown = false,
86: 	              bool filter_pushdown = false, table_function_progress_t query_progress = nullptr);
87: 	DUCKDB_API
88: 	TableFunction(const vector<LogicalType> &arguments, table_function_t function, table_function_bind_t bind = nullptr,
89: 	              table_function_init_t init = nullptr, table_statistics_t statistics = nullptr,
90: 	              table_function_cleanup_t cleanup = nullptr, table_function_dependency_t dependency = nullptr,
91: 	              table_function_cardinality_t cardinality = nullptr,
92: 	              table_function_pushdown_complex_filter_t pushdown_complex_filter = nullptr,
93: 	              table_function_to_string_t to_string = nullptr, table_function_max_threads_t max_threads = nullptr,
94: 	              table_function_init_parallel_state_t init_parallel_state = nullptr,
95: 	              table_function_parallel_t parallel_function = nullptr,
96: 	              table_function_init_parallel_t parallel_init = nullptr,
97: 	              table_function_parallel_state_next_t parallel_state_next = nullptr, bool projection_pushdown = false,
98: 	              bool filter_pushdown = false, table_function_progress_t query_progress = nullptr);
99: 	DUCKDB_API TableFunction();
100: 
101: 	//! Bind function
102: 	//! This function is used for determining the return type of a table producing function and returning bind data
103: 	//! The returned FunctionData object should be constant and should not be changed during execution.
104: 	table_function_bind_t bind;
105: 	//! (Optional) init function
106: 	//! Initialize the operator state of the function. The operator state is used to keep track of the progress in the
107: 	//! table function.
108: 	table_function_init_t init;
109: 	//! The main function
110: 	table_function_t function;
111: 	//! (Optional) statistics function
112: 	//! Returns the statistics of a specified column
113: 	table_statistics_t statistics;
114: 	//! (Optional) cleanup function
115: 	//! The final cleanup function, called after all data is exhausted from the main function
116: 	table_function_cleanup_t cleanup;
117: 	//! (Optional) dependency function
118: 	//! Sets up which catalog entries this table function depend on
119: 	table_function_dependency_t dependency;
120: 	//! (Optional) cardinality function
121: 	//! Returns the expected cardinality of this scan
122: 	table_function_cardinality_t cardinality;
123: 	//! (Optional) pushdown a set of arbitrary filter expressions, rather than only simple comparisons with a constant
124: 	//! Any functions remaining in the expression list will be pushed as a regular filter after the scan
125: 	table_function_pushdown_complex_filter_t pushdown_complex_filter;
126: 	//! (Optional) function for rendering the operator to a string in profiling output
127: 	table_function_to_string_t to_string;
128: 	//! (Optional) function that returns the maximum amount of threads that can work on this task
129: 	table_function_max_threads_t max_threads;
130: 	//! (Optional) initialize the parallel scan state, called once in total.
131: 	table_function_init_parallel_state_t init_parallel_state;
132: 	//! (Optional) Parallel version of the main function
133: 	table_function_parallel_t parallel_function;
134: 	//! (Optional) initialize the parallel scan given the parallel state. Called once per task. Return nullptr if there
135: 	//! is nothing left to scan.
136: 	table_function_init_parallel_t parallel_init;
137: 	//! (Optional) return the next chunk to process in the parallel scan, or return nullptr if there is none
138: 	table_function_parallel_state_next_t parallel_state_next;
139: 	//! (Optional) return how much of the table we have scanned up to this point (% of the data)
140: 	table_function_progress_t table_scan_progress;
141: 	//! Whether or not the table function supports projection pushdown. If not supported a projection will be added
142: 	//! that filters out unused columns.
143: 	bool projection_pushdown;
144: 	//! Whether or not the table function supports filter pushdown. If not supported a filter will be added
145: 	//! that applies the table filter directly.
146: 	bool filter_pushdown;
147: };
148: 
149: } // namespace duckdb
[end of src/include/duckdb/function/table_function.hpp]
[start of src/include/duckdb/main/client_config.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/client_config.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/types/value.hpp"
13: #include "duckdb/common/enums/output_type.hpp"
14: #include "duckdb/common/case_insensitive_map.hpp"
15: #include "duckdb/common/enums/profiler_format.hpp"
16: 
17: namespace duckdb {
18: class ClientContext;
19: 
20: struct ClientConfig {
21: 	//! If the query profiler is enabled or not.
22: 	bool enable_profiler = false;
23: 	//! If detailed query profiling is enabled
24: 	bool enable_detailed_profiling = false;
25: 	//! The format to automatically print query profiling information in (default: disabled)
26: 	ProfilerPrintFormat profiler_print_format = ProfilerPrintFormat::NONE;
27: 	//! The file to save query profiling information to, instead of printing it to the console
28: 	//! (empty = print to console)
29: 	string profiler_save_location;
30: 
31: 	//! If the progress bar is enabled or not.
32: 	bool enable_progress_bar = false;
33: 	//! If the print of the progress bar is enabled
34: 	bool print_progress_bar = true;
35: 	//! The wait time before showing the progress bar
36: 	int wait_time = 2000;
37: 
38: 	// Whether or not aggressive query verification is enabled
39: 	bool query_verification_enabled = false;
40: 	//! Enable the running of optimizers
41: 	bool enable_optimizer = true;
42: 	//! Force parallelism of small tables, used for testing
43: 	bool verify_parallelism = false;
44: 	//! Force index join independent of table cardinality, used for testing
45: 	bool force_index_join = false;
46: 	//! Force out-of-core computation for operators that support it, used for testing
47: 	bool force_external = false;
48: 	//! Maximum bits allowed for using a perfect hash table (i.e. the perfect HT can hold up to 2^perfect_ht_threshold
49: 	//! elements)
50: 	idx_t perfect_ht_threshold = 12;
51: 
52: 	//! The explain output type used when none is specified (default: PHYSICAL_ONLY)
53: 	ExplainOutputType explain_output_type = ExplainOutputType::PHYSICAL_ONLY;
54: 
55: 	//! Generic options
56: 	case_insensitive_map_t<Value> set_variables;
57: 
58: public:
59: 	static ClientConfig &GetConfig(ClientContext &context);
60: };
61: 
62: } // namespace duckdb
[end of src/include/duckdb/main/client_config.hpp]
[start of src/include/duckdb/main/client_context.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/client_context.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
12: #include "duckdb/catalog/catalog_set.hpp"
13: #include "duckdb/common/enums/pending_execution_result.hpp"
14: #include "duckdb/common/deque.hpp"
15: #include "duckdb/common/pair.hpp"
16: #include "duckdb/common/progress_bar.hpp"
17: #include "duckdb/common/unordered_set.hpp"
18: #include "duckdb/common/winapi.hpp"
19: #include "duckdb/main/prepared_statement.hpp"
20: #include "duckdb/main/stream_query_result.hpp"
21: #include "duckdb/main/table_description.hpp"
22: #include "duckdb/transaction/transaction_context.hpp"
23: #include "duckdb/main/pending_query_result.hpp"
24: #include <random>
25: #include "duckdb/common/atomic.hpp"
26: #include "duckdb/main/client_config.hpp"
27: 
28: namespace duckdb {
29: class Appender;
30: class Catalog;
31: class CatalogSearchPath;
32: class ChunkCollection;
33: class DatabaseInstance;
34: class FileOpener;
35: class LogicalOperator;
36: class PreparedStatementData;
37: class Relation;
38: class BufferedFileWriter;
39: class QueryProfiler;
40: class QueryProfilerHistory;
41: class ClientContextLock;
42: struct CreateScalarFunctionInfo;
43: class ScalarFunctionCatalogEntry;
44: struct ActiveQueryContext;
45: 
46: //! The ClientContext holds information relevant to the current client session
47: //! during execution
48: class ClientContext : public std::enable_shared_from_this<ClientContext> {
49: 	friend class PendingQueryResult;
50: 	friend class StreamQueryResult;
51: 	friend class TransactionManager;
52: 
53: public:
54: 	DUCKDB_API explicit ClientContext(shared_ptr<DatabaseInstance> db);
55: 	DUCKDB_API ~ClientContext();
56: 
57: 	//! Query profiler
58: 	shared_ptr<QueryProfiler> profiler;
59: 	//! QueryProfiler History
60: 	unique_ptr<QueryProfilerHistory> query_profiler_history;
61: 	//! The database that this client is connected to
62: 	shared_ptr<DatabaseInstance> db;
63: 	//! Data for the currently running transaction
64: 	TransactionContext transaction;
65: 	//! Whether or not the query is interrupted
66: 	atomic<bool> interrupted;
67: 
68: 	unique_ptr<SchemaCatalogEntry> temporary_objects;
69: 	unordered_map<string, shared_ptr<PreparedStatementData>> prepared_statements;
70: 
71: 	//! The writer used to log queries (if logging is enabled)
72: 	unique_ptr<BufferedFileWriter> log_query_writer;
73: 	//! The random generator used by random(). Its seed value can be set by setseed().
74: 	std::mt19937 random_engine;
75: 
76: 	const unique_ptr<CatalogSearchPath> catalog_search_path;
77: 
78: 	unique_ptr<FileOpener> file_opener;
79: 
80: 	//! The client configuration
81: 	ClientConfig config;
82: 
83: public:
84: 	DUCKDB_API Transaction &ActiveTransaction() {
85: 		return transaction.ActiveTransaction();
86: 	}
87: 
88: 	//! Interrupt execution of a query
89: 	DUCKDB_API void Interrupt();
90: 	//! Enable query profiling
91: 	DUCKDB_API void EnableProfiling();
92: 	//! Disable query profiling
93: 	DUCKDB_API void DisableProfiling();
94: 
95: 	//! Issue a query, returning a QueryResult. The QueryResult can be either a StreamQueryResult or a
96: 	//! MaterializedQueryResult. The StreamQueryResult will only be returned in the case of a successful SELECT
97: 	//! statement.
98: 	DUCKDB_API unique_ptr<QueryResult> Query(const string &query, bool allow_stream_result);
99: 	DUCKDB_API unique_ptr<QueryResult> Query(unique_ptr<SQLStatement> statement, bool allow_stream_result);
100: 
101: 	//! Issues a query to the database and returns a Pending Query Result. Note that "query" may only contain
102: 	//! a single statement.
103: 	DUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(const string &query);
104: 	//! Issues a query to the database and returns a Pending Query Result
105: 	DUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(unique_ptr<SQLStatement> statement);
106: 
107: 	//! Destroy the client context
108: 	DUCKDB_API void Destroy();
109: 
110: 	//! Get the table info of a specific table, or nullptr if it cannot be found
111: 	DUCKDB_API unique_ptr<TableDescription> TableInfo(const string &schema_name, const string &table_name);
112: 	//! Appends a DataChunk to the specified table. Returns whether or not the append was successful.
113: 	DUCKDB_API void Append(TableDescription &description, ChunkCollection &collection);
114: 	//! Try to bind a relation in the current client context; either throws an exception or fills the result_columns
115: 	//! list with the set of returned columns
116: 	DUCKDB_API void TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns);
117: 
118: 	//! Execute a relation
119: 	DUCKDB_API unique_ptr<QueryResult> Execute(const shared_ptr<Relation> &relation);
120: 
121: 	//! Prepare a query
122: 	DUCKDB_API unique_ptr<PreparedStatement> Prepare(const string &query);
123: 	//! Directly prepare a SQL statement
124: 	DUCKDB_API unique_ptr<PreparedStatement> Prepare(unique_ptr<SQLStatement> statement);
125: 
126: 	//! Create a pending query result from a prepared statement with the given name and set of parameters
127: 	//! It is possible that the prepared statement will be re-bound. This will generally happen if the catalog is
128: 	//! modified in between the prepared statement being bound and the prepared statement being run.
129: 	DUCKDB_API unique_ptr<PendingQueryResult>
130: 	PendingQuery(const string &query, shared_ptr<PreparedStatementData> &prepared, vector<Value> &values);
131: 
132: 	//! Execute a prepared statement with the given name and set of parameters
133: 	//! It is possible that the prepared statement will be re-bound. This will generally happen if the catalog is
134: 	//! modified in between the prepared statement being bound and the prepared statement being run.
135: 	DUCKDB_API unique_ptr<QueryResult> Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
136: 	                                           vector<Value> &values, bool allow_stream_result = true);
137: 
138: 	//! Gets current percentage of the query's progress, returns 0 in case the progress bar is disabled.
139: 	DUCKDB_API double GetProgress();
140: 
141: 	//! Register function in the temporary schema
142: 	DUCKDB_API void RegisterFunction(CreateFunctionInfo *info);
143: 
144: 	//! Parse statements from a query
145: 	DUCKDB_API vector<unique_ptr<SQLStatement>> ParseStatements(const string &query);
146: 
147: 	//! Extract the logical plan of a query
148: 	DUCKDB_API unique_ptr<LogicalOperator> ExtractPlan(const string &query);
149: 	DUCKDB_API void HandlePragmaStatements(vector<unique_ptr<SQLStatement>> &statements);
150: 
151: 	//! Runs a function with a valid transaction context, potentially starting a transaction if the context is in auto
152: 	//! commit mode.
153: 	DUCKDB_API void RunFunctionInTransaction(const std::function<void(void)> &fun,
154: 	                                         bool requires_valid_transaction = true);
155: 	//! Same as RunFunctionInTransaction, but does not obtain a lock on the client context or check for validation
156: 	DUCKDB_API void RunFunctionInTransactionInternal(ClientContextLock &lock, const std::function<void(void)> &fun,
157: 	                                                 bool requires_valid_transaction = true);
158: 
159: 	//! Equivalent to CURRENT_SETTING(key) SQL function.
160: 	DUCKDB_API bool TryGetCurrentSetting(const std::string &key, Value &result);
161: 
162: 	DUCKDB_API unique_ptr<DataChunk> Fetch(ClientContextLock &lock, StreamQueryResult &result);
163: 
164: 	//! Whether or not the given result object (streaming query result or pending query result) is active
165: 	DUCKDB_API bool IsActiveResult(ClientContextLock &lock, BaseQueryResult *result);
166: 
167: 	//! Returns the current executor
168: 	Executor &GetExecutor();
169: 
170: 	//! Returns the current query string (if any)
171: 	const string &GetCurrentQuery();
172: 
173: 	//! Fetch a list of table names that are required for a given query
174: 	DUCKDB_API unordered_set<string> GetTableNames(const string &query);
175: 
176: private:
177: 	//! Parse statements and resolve pragmas from a query
178: 	bool ParseStatements(ClientContextLock &lock, const string &query, vector<unique_ptr<SQLStatement>> &result,
179: 	                     string &error);
180: 	//! Issues a query to the database and returns a Pending Query Result
181: 	unique_ptr<PendingQueryResult> PendingQueryInternal(ClientContextLock &lock, unique_ptr<SQLStatement> statement,
182: 	                                                    bool verify = true);
183: 	unique_ptr<QueryResult> ExecutePendingQueryInternal(ClientContextLock &lock, PendingQueryResult &query,
184: 	                                                    bool allow_stream_result);
185: 
186: 	//! Parse statements from a query
187: 	vector<unique_ptr<SQLStatement>> ParseStatementsInternal(ClientContextLock &lock, const string &query);
188: 	//! Perform aggressive query verification of a SELECT statement. Only called when query_verification_enabled is
189: 	//! true.
190: 	string VerifyQuery(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement);
191: 
192: 	void InitialCleanup(ClientContextLock &lock);
193: 	//! Internal clean up, does not lock. Caller must hold the context_lock.
194: 	void CleanupInternal(ClientContextLock &lock, BaseQueryResult *result = nullptr,
195: 	                     bool invalidate_transaction = false);
196: 	string FinalizeQuery(ClientContextLock &lock, bool success);
197: 	unique_ptr<PendingQueryResult> PendingStatementOrPreparedStatement(ClientContextLock &lock, const string &query,
198: 	                                                                   unique_ptr<SQLStatement> statement,
199: 	                                                                   shared_ptr<PreparedStatementData> &prepared,
200: 	                                                                   vector<Value> *values);
201: 	unique_ptr<PendingQueryResult> PendingPreparedStatement(ClientContextLock &lock,
202: 	                                                        shared_ptr<PreparedStatementData> statement_p,
203: 	                                                        vector<Value> bound_values);
204: 
205: 	//! Internally prepare a SQL statement. Caller must hold the context_lock.
206: 	shared_ptr<PreparedStatementData> CreatePreparedStatement(ClientContextLock &lock, const string &query,
207: 	                                                          unique_ptr<SQLStatement> statement);
208: 	unique_ptr<PendingQueryResult> PendingStatementInternal(ClientContextLock &lock, const string &query,
209: 	                                                        unique_ptr<SQLStatement> statement);
210: 	unique_ptr<QueryResult> RunStatementInternal(ClientContextLock &lock, const string &query,
211: 	                                             unique_ptr<SQLStatement> statement, bool allow_stream_result,
212: 	                                             bool verify = true);
213: 	unique_ptr<PreparedStatement> PrepareInternal(ClientContextLock &lock, unique_ptr<SQLStatement> statement);
214: 	void LogQueryInternal(ClientContextLock &lock, const string &query);
215: 
216: 	unique_ptr<QueryResult> FetchResultInternal(ClientContextLock &lock, PendingQueryResult &pending,
217: 	                                            bool allow_stream_result);
218: 	unique_ptr<DataChunk> FetchInternal(ClientContextLock &lock, Executor &executor, BaseQueryResult &result);
219: 
220: 	unique_ptr<ClientContextLock> LockContext();
221: 
222: 	bool UpdateFunctionInfoFromEntry(ScalarFunctionCatalogEntry *existing_function, CreateScalarFunctionInfo *new_info);
223: 
224: 	void BeginTransactionInternal(ClientContextLock &lock, bool requires_valid_transaction);
225: 	void BeginQueryInternal(ClientContextLock &lock, const string &query);
226: 	string EndQueryInternal(ClientContextLock &lock, bool success, bool invalidate_transaction);
227: 
228: 	PendingExecutionResult ExecuteTaskInternal(ClientContextLock &lock, PendingQueryResult &result);
229: 
230: 	unique_ptr<PendingQueryResult>
231: 	PendingStatementOrPreparedStatementInternal(ClientContextLock &lock, const string &query,
232: 	                                            unique_ptr<SQLStatement> statement,
233: 	                                            shared_ptr<PreparedStatementData> &prepared, vector<Value> *values);
234: 
235: 	unique_ptr<PendingQueryResult> PendingQueryPreparedInternal(ClientContextLock &lock, const string &query,
236: 	                                                            shared_ptr<PreparedStatementData> &prepared,
237: 	                                                            vector<Value> &values);
238: 
239: private:
240: 	//! Lock on using the ClientContext in parallel
241: 	mutex context_lock;
242: 	//! The currently active query context
243: 	unique_ptr<ActiveQueryContext> active_query;
244: 	//! The current query progress
245: 	atomic<double> query_progress;
246: };
247: 
248: class ClientContextLock {
249: public:
250: 	explicit ClientContextLock(mutex &context_lock) : client_guard(context_lock) {
251: 	}
252: 
253: 	~ClientContextLock() {
254: 	}
255: 
256: private:
257: 	lock_guard<mutex> client_guard;
258: };
259: 
260: } // namespace duckdb
[end of src/include/duckdb/main/client_context.hpp]
[start of src/include/duckdb/main/connection.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/connection.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/enums/profiler_format.hpp"
12: #include "duckdb/common/serializer/buffered_file_writer.hpp"
13: #include "duckdb/common/winapi.hpp"
14: #include "duckdb/function/udf_function.hpp"
15: #include "duckdb/main/materialized_query_result.hpp"
16: #include "duckdb/main/pending_query_result.hpp"
17: #include "duckdb/main/prepared_statement.hpp"
18: #include "duckdb/main/query_result.hpp"
19: #include "duckdb/main/relation.hpp"
20: #include "duckdb/main/stream_query_result.hpp"
21: #include "duckdb/main/table_description.hpp"
22: #include "duckdb/parser/sql_statement.hpp"
23: 
24: namespace duckdb {
25: 
26: class ChunkCollection;
27: class ClientContext;
28: class DatabaseInstance;
29: class DuckDB;
30: class LogicalOperator;
31: 
32: typedef void (*warning_callback)(std::string);
33: 
34: //! A connection to a database. This represents a (client) connection that can
35: //! be used to query the database.
36: class Connection {
37: public:
38: 	DUCKDB_API explicit Connection(DuckDB &database);
39: 	DUCKDB_API explicit Connection(DatabaseInstance &database);
40: 
41: 	shared_ptr<ClientContext> context;
42: 	warning_callback warning_cb;
43: 
44: public:
45: 	//! Returns query profiling information for the current query
46: 	DUCKDB_API string GetProfilingInformation(ProfilerPrintFormat format = ProfilerPrintFormat::QUERY_TREE);
47: 
48: 	//! Interrupt execution of the current query
49: 	DUCKDB_API void Interrupt();
50: 
51: 	//! Enable query profiling
52: 	DUCKDB_API void EnableProfiling();
53: 	//! Disable query profiling
54: 	DUCKDB_API void DisableProfiling();
55: 
56: 	DUCKDB_API void SetWarningCallback(warning_callback);
57: 
58: 	//! Enable aggressive verification/testing of queries, should only be used in testing
59: 	DUCKDB_API void EnableQueryVerification();
60: 	DUCKDB_API void DisableQueryVerification();
61: 	//! Force parallel execution, even for smaller tables. Should only be used in testing.
62: 	DUCKDB_API void ForceParallelism();
63: 
64: 	//! Issues a query to the database and returns a QueryResult. This result can be either a StreamQueryResult or a
65: 	//! MaterializedQueryResult. The result can be stepped through with calls to Fetch(). Note that there can only be
66: 	//! one active StreamQueryResult per Connection object. Calling SendQuery() will invalidate any previously existing
67: 	//! StreamQueryResult.
68: 	DUCKDB_API unique_ptr<QueryResult> SendQuery(const string &query);
69: 	//! Issues a query to the database and materializes the result (if necessary). Always returns a
70: 	//! MaterializedQueryResult.
71: 	DUCKDB_API unique_ptr<MaterializedQueryResult> Query(const string &query);
72: 	//! Issues a query to the database and materializes the result (if necessary). Always returns a
73: 	//! MaterializedQueryResult.
74: 	DUCKDB_API unique_ptr<MaterializedQueryResult> Query(unique_ptr<SQLStatement> statement);
75: 	// prepared statements
76: 	template <typename... Args>
77: 	unique_ptr<QueryResult> Query(const string &query, Args... args) {
78: 		vector<Value> values;
79: 		return QueryParamsRecursive(query, values, args...);
80: 	}
81: 
82: 	//! Issues a query to the database and returns a Pending Query Result. Note that "query" may only contain
83: 	//! a single statement.
84: 	DUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(const string &query);
85: 	//! Issues a query to the database and returns a Pending Query Result
86: 	DUCKDB_API unique_ptr<PendingQueryResult> PendingQuery(unique_ptr<SQLStatement> statement);
87: 
88: 	//! Prepare the specified query, returning a prepared statement object
89: 	DUCKDB_API unique_ptr<PreparedStatement> Prepare(const string &query);
90: 	//! Prepare the specified statement, returning a prepared statement object
91: 	DUCKDB_API unique_ptr<PreparedStatement> Prepare(unique_ptr<SQLStatement> statement);
92: 
93: 	//! Get the table info of a specific table (in the default schema), or nullptr if it cannot be found
94: 	DUCKDB_API unique_ptr<TableDescription> TableInfo(const string &table_name);
95: 	//! Get the table info of a specific table, or nullptr if it cannot be found
96: 	DUCKDB_API unique_ptr<TableDescription> TableInfo(const string &schema_name, const string &table_name);
97: 
98: 	//! Extract a set of SQL statements from a specific query
99: 	DUCKDB_API vector<unique_ptr<SQLStatement>> ExtractStatements(const string &query);
100: 	//! Extract the logical plan that corresponds to a query
101: 	DUCKDB_API unique_ptr<LogicalOperator> ExtractPlan(const string &query);
102: 
103: 	//! Appends a DataChunk to the specified table
104: 	DUCKDB_API void Append(TableDescription &description, DataChunk &chunk);
105: 	//! Appends a ChunkCollection to the specified table
106: 	DUCKDB_API void Append(TableDescription &description, ChunkCollection &collection);
107: 
108: 	//! Returns a relation that produces a table from this connection
109: 	DUCKDB_API shared_ptr<Relation> Table(const string &tname);
110: 	DUCKDB_API shared_ptr<Relation> Table(const string &schema_name, const string &table_name);
111: 	//! Returns a relation that produces a view from this connection
112: 	DUCKDB_API shared_ptr<Relation> View(const string &tname);
113: 	DUCKDB_API shared_ptr<Relation> View(const string &schema_name, const string &table_name);
114: 	//! Returns a relation that calls a specified table function
115: 	DUCKDB_API shared_ptr<Relation> TableFunction(const string &tname);
116: 	DUCKDB_API shared_ptr<Relation> TableFunction(const string &tname, const vector<Value> &values,
117: 	                                              const unordered_map<string, Value> &named_parameters);
118: 	DUCKDB_API shared_ptr<Relation> TableFunction(const string &tname, const vector<Value> &values);
119: 	//! Returns a relation that produces values
120: 	DUCKDB_API shared_ptr<Relation> Values(const vector<vector<Value>> &values);
121: 	DUCKDB_API shared_ptr<Relation> Values(const vector<vector<Value>> &values, const vector<string> &column_names,
122: 	                                       const string &alias = "values");
123: 	DUCKDB_API shared_ptr<Relation> Values(const string &values);
124: 	DUCKDB_API shared_ptr<Relation> Values(const string &values, const vector<string> &column_names,
125: 	                                       const string &alias = "values");
126: 	//! Reads CSV file
127: 	DUCKDB_API shared_ptr<Relation> ReadCSV(const string &csv_file);
128: 	DUCKDB_API shared_ptr<Relation> ReadCSV(const string &csv_file, const vector<string> &columns);
129: 	//! Returns a relation from a query
130: 	DUCKDB_API shared_ptr<Relation> RelationFromQuery(const string &query, const string &alias = "queryrelation");
131: 
132: 	DUCKDB_API void BeginTransaction();
133: 	DUCKDB_API void Commit();
134: 	DUCKDB_API void Rollback();
135: 	DUCKDB_API void SetAutoCommit(bool auto_commit);
136: 	DUCKDB_API bool IsAutoCommit();
137: 
138: 	//! Fetch a list of table names that are required for a given query
139: 	DUCKDB_API unordered_set<string> GetTableNames(const string &query);
140: 
141: 	template <typename TR, typename... Args>
142: 	void CreateScalarFunction(const string &name, TR (*udf_func)(Args...)) {
143: 		scalar_function_t function = UDFWrapper::CreateScalarFunction<TR, Args...>(name, udf_func);
144: 		UDFWrapper::RegisterFunction<TR, Args...>(name, function, *context);
145: 	}
146: 
147: 	template <typename TR, typename... Args>
148: 	void CreateScalarFunction(const string &name, vector<LogicalType> args, LogicalType ret_type,
149: 	                          TR (*udf_func)(Args...)) {
150: 		scalar_function_t function =
151: 		    UDFWrapper::CreateScalarFunction<TR, Args...>(name, args, move(ret_type), udf_func);
152: 		UDFWrapper::RegisterFunction(name, args, ret_type, function, *context);
153: 	}
154: 
155: 	template <typename TR, typename... Args>
156: 	void CreateVectorizedFunction(const string &name, scalar_function_t udf_func,
157: 	                              LogicalType varargs = LogicalType::INVALID) {
158: 		UDFWrapper::RegisterFunction<TR, Args...>(name, udf_func, *context, move(varargs));
159: 	}
160: 
161: 	DUCKDB_API void CreateVectorizedFunction(const string &name, vector<LogicalType> args, LogicalType ret_type,
162: 	                                         scalar_function_t udf_func, LogicalType varargs = LogicalType::INVALID) {
163: 		UDFWrapper::RegisterFunction(name, move(args), move(ret_type), udf_func, *context, move(varargs));
164: 	}
165: 
166: 	//------------------------------------- Aggreate Functions ----------------------------------------//
167: 	template <typename UDF_OP, typename STATE, typename TR, typename TA>
168: 	void CreateAggregateFunction(const string &name) {
169: 		AggregateFunction function = UDFWrapper::CreateAggregateFunction<UDF_OP, STATE, TR, TA>(name);
170: 		UDFWrapper::RegisterAggrFunction(function, *context);
171: 	}
172: 
173: 	template <typename UDF_OP, typename STATE, typename TR, typename TA, typename TB>
174: 	void CreateAggregateFunction(const string &name) {
175: 		AggregateFunction function = UDFWrapper::CreateAggregateFunction<UDF_OP, STATE, TR, TA, TB>(name);
176: 		UDFWrapper::RegisterAggrFunction(function, *context);
177: 	}
178: 
179: 	template <typename UDF_OP, typename STATE, typename TR, typename TA>
180: 	void CreateAggregateFunction(const string &name, LogicalType ret_type, LogicalType input_typeA) {
181: 		AggregateFunction function =
182: 		    UDFWrapper::CreateAggregateFunction<UDF_OP, STATE, TR, TA>(name, ret_type, input_typeA);
183: 		UDFWrapper::RegisterAggrFunction(function, *context);
184: 	}
185: 
186: 	template <typename UDF_OP, typename STATE, typename TR, typename TA, typename TB>
187: 	void CreateAggregateFunction(const string &name, LogicalType ret_type, LogicalType input_typeA,
188: 	                             LogicalType input_typeB) {
189: 		AggregateFunction function =
190: 		    UDFWrapper::CreateAggregateFunction<UDF_OP, STATE, TR, TA, TB>(name, ret_type, input_typeA, input_typeB);
191: 		UDFWrapper::RegisterAggrFunction(function, *context);
192: 	}
193: 
194: 	DUCKDB_API void CreateAggregateFunction(const string &name, vector<LogicalType> arguments, LogicalType return_type,
195: 	                                        aggregate_size_t state_size, aggregate_initialize_t initialize,
196: 	                                        aggregate_update_t update, aggregate_combine_t combine,
197: 	                                        aggregate_finalize_t finalize,
198: 	                                        aggregate_simple_update_t simple_update = nullptr,
199: 	                                        bind_aggregate_function_t bind = nullptr,
200: 	                                        aggregate_destructor_t destructor = nullptr) {
201: 		AggregateFunction function =
202: 		    UDFWrapper::CreateAggregateFunction(name, arguments, return_type, state_size, initialize, update, combine,
203: 		                                        finalize, simple_update, bind, destructor);
204: 		UDFWrapper::RegisterAggrFunction(function, *context);
205: 	}
206: 
207: private:
208: 	unique_ptr<QueryResult> QueryParamsRecursive(const string &query, vector<Value> &values);
209: 
210: 	template <typename T, typename... Args>
211: 	unique_ptr<QueryResult> QueryParamsRecursive(const string &query, vector<Value> &values, T value, Args... args) {
212: 		values.push_back(Value::CreateValue<T>(value));
213: 		return QueryParamsRecursive(query, values, args...);
214: 	}
215: };
216: 
217: } // namespace duckdb
[end of src/include/duckdb/main/connection.hpp]
[start of src/include/duckdb/main/relation.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/relation.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/enums/join_type.hpp"
13: #include "duckdb/common/enums/relation_type.hpp"
14: #include "duckdb/common/winapi.hpp"
15: #include "duckdb/main/query_result.hpp"
16: #include "duckdb/parser/column_definition.hpp"
17: #include "duckdb/common/unordered_map.hpp"
18: 
19: #include <memory>
20: 
21: namespace duckdb {
22: struct BoundStatement;
23: 
24: class ClientContext;
25: class Binder;
26: class LogicalOperator;
27: class QueryNode;
28: class TableRef;
29: 
30: class Relation : public std::enable_shared_from_this<Relation> {
31: public:
32: 	DUCKDB_API Relation(ClientContext &context, RelationType type) : context(context), type(type) {
33: 	}
34: 	DUCKDB_API virtual ~Relation() {
35: 	}
36: 
37: 	ClientContext &context;
38: 	RelationType type;
39: 
40: public:
41: 	DUCKDB_API virtual const vector<ColumnDefinition> &Columns() = 0;
42: 	DUCKDB_API virtual unique_ptr<QueryNode> GetQueryNode();
43: 	DUCKDB_API virtual BoundStatement Bind(Binder &binder);
44: 	DUCKDB_API virtual string GetAlias();
45: 
46: 	DUCKDB_API unique_ptr<QueryResult> Execute();
47: 	DUCKDB_API string ToString();
48: 	DUCKDB_API virtual string ToString(idx_t depth) = 0;
49: 
50: 	DUCKDB_API void Print();
51: 	DUCKDB_API void Head(idx_t limit = 10);
52: 
53: 	DUCKDB_API shared_ptr<Relation> CreateView(const string &name, bool replace = true, bool temporary = false);
54: 	DUCKDB_API unique_ptr<QueryResult> Query(const string &sql);
55: 	DUCKDB_API unique_ptr<QueryResult> Query(const string &name, const string &sql);
56: 
57: 	//! Explain the query plan of this relation
58: 	DUCKDB_API unique_ptr<QueryResult> Explain();
59: 
60: 	DUCKDB_API virtual unique_ptr<TableRef> GetTableRef();
61: 	DUCKDB_API virtual bool IsReadOnly() {
62: 		return true;
63: 	}
64: 
65: public:
66: 	// PROJECT
67: 	DUCKDB_API shared_ptr<Relation> Project(const string &select_list);
68: 	DUCKDB_API shared_ptr<Relation> Project(const string &expression, const string &alias);
69: 	DUCKDB_API shared_ptr<Relation> Project(const string &select_list, const vector<string> &aliases);
70: 	DUCKDB_API shared_ptr<Relation> Project(const vector<string> &expressions);
71: 	DUCKDB_API shared_ptr<Relation> Project(const vector<string> &expressions, const vector<string> &aliases);
72: 
73: 	// FILTER
74: 	DUCKDB_API shared_ptr<Relation> Filter(const string &expression);
75: 	DUCKDB_API shared_ptr<Relation> Filter(const vector<string> &expressions);
76: 
77: 	// LIMIT
78: 	DUCKDB_API shared_ptr<Relation> Limit(int64_t n, int64_t offset = 0);
79: 
80: 	// ORDER
81: 	DUCKDB_API shared_ptr<Relation> Order(const string &expression);
82: 	DUCKDB_API shared_ptr<Relation> Order(const vector<string> &expressions);
83: 
84: 	// JOIN operation
85: 	DUCKDB_API shared_ptr<Relation> Join(const shared_ptr<Relation> &other, const string &condition,
86: 	                                     JoinType type = JoinType::INNER);
87: 
88: 	// SET operations
89: 	DUCKDB_API shared_ptr<Relation> Union(const shared_ptr<Relation> &other);
90: 	DUCKDB_API shared_ptr<Relation> Except(const shared_ptr<Relation> &other);
91: 	DUCKDB_API shared_ptr<Relation> Intersect(const shared_ptr<Relation> &other);
92: 
93: 	// DISTINCT operation
94: 	DUCKDB_API shared_ptr<Relation> Distinct();
95: 
96: 	// AGGREGATES
97: 	DUCKDB_API shared_ptr<Relation> Aggregate(const string &aggregate_list);
98: 	DUCKDB_API shared_ptr<Relation> Aggregate(const vector<string> &aggregates);
99: 	DUCKDB_API shared_ptr<Relation> Aggregate(const string &aggregate_list, const string &group_list);
100: 	DUCKDB_API shared_ptr<Relation> Aggregate(const vector<string> &aggregates, const vector<string> &groups);
101: 
102: 	// ALIAS
103: 	DUCKDB_API shared_ptr<Relation> Alias(const string &alias);
104: 
105: 	//! Insert the data from this relation into a table
106: 	DUCKDB_API void Insert(const string &table_name);
107: 	DUCKDB_API void Insert(const string &schema_name, const string &table_name);
108: 	//! Insert a row (i.e.,list of values) into a table
109: 	DUCKDB_API void Insert(const vector<vector<Value>> &values);
110: 	//! Create a table and insert the data from this relation into that table
111: 	DUCKDB_API void Create(const string &table_name);
112: 	DUCKDB_API void Create(const string &schema_name, const string &table_name);
113: 
114: 	//! Write a relation to a CSV file
115: 	DUCKDB_API void WriteCSV(const string &csv_file);
116: 
117: 	//! Update a table, can only be used on a TableRelation
118: 	DUCKDB_API virtual void Update(const string &update, const string &condition = string());
119: 	//! Delete from a table, can only be used on a TableRelation
120: 	DUCKDB_API virtual void Delete(const string &condition = string());
121: 	//! Create a relation from calling a table in/out function on the input relation
122: 	//! Create a relation from calling a table in/out function on the input relation
123: 	DUCKDB_API shared_ptr<Relation> TableFunction(const std::string &fname, const vector<Value> &values);
124: 	DUCKDB_API shared_ptr<Relation> TableFunction(const std::string &fname, const vector<Value> &values,
125: 	                                              const unordered_map<string, Value> &named_parameters);
126: 
127: public:
128: 	//! Whether or not the relation inherits column bindings from its child or not, only relevant for binding
129: 	DUCKDB_API virtual bool InheritsColumnBindings() {
130: 		return false;
131: 	}
132: 	DUCKDB_API virtual Relation *ChildRelation() {
133: 		return nullptr;
134: 	}
135: 
136: protected:
137: 	DUCKDB_API string RenderWhitespace(idx_t depth);
138: };
139: 
140: } // namespace duckdb
[end of src/include/duckdb/main/relation.hpp]
[start of src/include/duckdb/main/relation/table_function_relation.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/relation/table_function_relation.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/main/relation.hpp"
12: 
13: namespace duckdb {
14: 
15: class TableFunctionRelation : public Relation {
16: public:
17: 	TableFunctionRelation(ClientContext &context, string name, vector<Value> parameters,
18: 	                      unordered_map<string, Value> named_parameters,
19: 	                      shared_ptr<Relation> input_relation_p = nullptr);
20: 
21: 	TableFunctionRelation(ClientContext &context, string name, vector<Value> parameters,
22: 	                      shared_ptr<Relation> input_relation_p = nullptr);
23: 
24: 	string name;
25: 	vector<Value> parameters;
26: 	unordered_map<string, Value> named_parameters;
27: 	vector<ColumnDefinition> columns;
28: 	shared_ptr<Relation> input_relation;
29: 
30: public:
31: 	unique_ptr<QueryNode> GetQueryNode() override;
32: 	unique_ptr<TableRef> GetTableRef() override;
33: 
34: 	const vector<ColumnDefinition> &Columns() override;
35: 	string ToString(idx_t depth) override;
36: 	string GetAlias() override;
37: };
38: 
39: } // namespace duckdb
[end of src/include/duckdb/main/relation/table_function_relation.hpp]
[start of src/include/duckdb/main/settings.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/settings.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/types/value.hpp"
13: 
14: namespace duckdb {
15: class ClientContext;
16: class DatabaseInstance;
17: struct DBConfig;
18: 
19: struct AccessModeSetting {
20: 	static constexpr const char *Name = "access_mode";
21: 	static constexpr const char *Description = "Access mode of the database (AUTOMATIC, READ_ONLY or READ_WRITE)";
22: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
23: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
24: 	static Value GetSetting(ClientContext &context);
25: };
26: 
27: struct CheckpointThresholdSetting {
28: 	static constexpr const char *Name = "checkpoint_threshold";
29: 	static constexpr const char *Description =
30: 	    "The WAL size threshold at which to automatically trigger a checkpoint (e.g. 1GB)";
31: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
32: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
33: 	static Value GetSetting(ClientContext &context);
34: };
35: 
36: struct DebugCheckpointAbort {
37: 	static constexpr const char *Name = "debug_checkpoint_abort";
38: 	static constexpr const char *Description =
39: 	    "DEBUG SETTING: trigger an abort while checkpointing for testing purposes";
40: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
41: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
42: 	static Value GetSetting(ClientContext &context);
43: };
44: 
45: struct DebugForceExternal {
46: 	static constexpr const char *Name = "debug_force_external";
47: 	static constexpr const char *Description =
48: 	    "DEBUG SETTING: force out-of-core computation for operators that support it, used for testing";
49: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::BOOLEAN;
50: 	static void SetLocal(ClientContext &context, const Value &parameter);
51: 	static Value GetSetting(ClientContext &context);
52: };
53: 
54: struct DebugManyFreeListBlocks {
55: 	static constexpr const char *Name = "debug_many_free_list_blocks";
56: 	static constexpr const char *Description = "DEBUG SETTING: add additional blocks to the free list";
57: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::BOOLEAN;
58: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
59: 	static Value GetSetting(ClientContext &context);
60: };
61: 
62: struct DebugWindowMode {
63: 	static constexpr const char *Name = "debug_window_mode";
64: 	static constexpr const char *Description = "DEBUG SETTING: switch window mode to use";
65: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
66: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
67: 	static Value GetSetting(ClientContext &context);
68: };
69: 
70: struct DefaultCollationSetting {
71: 	static constexpr const char *Name = "default_collation";
72: 	static constexpr const char *Description = "The collation setting used when none is specified";
73: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
74: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
75: 	static void SetLocal(ClientContext &context, const Value &parameter);
76: 	static Value GetSetting(ClientContext &context);
77: };
78: 
79: struct DefaultOrderSetting {
80: 	static constexpr const char *Name = "default_order";
81: 	static constexpr const char *Description = "The order type used when none is specified (ASC or DESC)";
82: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
83: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
84: 	static Value GetSetting(ClientContext &context);
85: };
86: 
87: struct DefaultNullOrderSetting {
88: 	static constexpr const char *Name = "default_null_order";
89: 	static constexpr const char *Description = "Null ordering used when none is specified (NULLS_FIRST or NULLS_LAST)";
90: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
91: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
92: 	static Value GetSetting(ClientContext &context);
93: };
94: 
95: struct DisabledOptimizersSetting {
96: 	static constexpr const char *Name = "disabled_optimizers";
97: 	static constexpr const char *Description = "DEBUG SETTING: disable a specific set of optimizers (comma separated)";
98: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
99: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
100: 	static Value GetSetting(ClientContext &context);
101: };
102: 
103: struct EnableExternalAccessSetting {
104: 	static constexpr const char *Name = "enable_external_access";
105: 	static constexpr const char *Description =
106: 	    "Allow the database to access external state (through e.g. loading/installing modules, COPY TO/FROM, CSV "
107: 	    "readers, pandas replacement scans, etc)";
108: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::BOOLEAN;
109: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
110: 	static Value GetSetting(ClientContext &context);
111: };
112: 
113: struct EnableObjectCacheSetting {
114: 	static constexpr const char *Name = "enable_object_cache";
115: 	static constexpr const char *Description = "Whether or not object cache is used to cache e.g. Parquet metadata";
116: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::BOOLEAN;
117: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
118: 	static Value GetSetting(ClientContext &context);
119: };
120: 
121: struct EnableProfilingSetting {
122: 	static constexpr const char *Name = "enable_profiling";
123: 	static constexpr const char *Description =
124: 	    "Enables profiling, and sets the output format (JSON, QUERY_TREE, QUERY_TREE_OPTIMIZER)";
125: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
126: 	static void SetLocal(ClientContext &context, const Value &parameter);
127: 	static Value GetSetting(ClientContext &context);
128: };
129: 
130: struct EnableProgressBarSetting {
131: 	static constexpr const char *Name = "enable_progress_bar";
132: 	static constexpr const char *Description =
133: 	    "Enables the progress bar, printing progress to the terminal for long queries";
134: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::BOOLEAN;
135: 	static void SetLocal(ClientContext &context, const Value &parameter);
136: 	static Value GetSetting(ClientContext &context);
137: };
138: 
139: struct ExplainOutputSetting {
140: 	static constexpr const char *Name = "explain_output";
141: 	static constexpr const char *Description = "Output of EXPLAIN statements (ALL, OPTIMIZED_ONLY, PHYSICAL_ONLY)";
142: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
143: 	static void SetLocal(ClientContext &context, const Value &parameter);
144: 	static Value GetSetting(ClientContext &context);
145: };
146: 
147: struct ForceCompressionSetting {
148: 	static constexpr const char *Name = "force_compression";
149: 	static constexpr const char *Description = "DEBUG SETTING: forces a specific compression method to be used";
150: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
151: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
152: 	static Value GetSetting(ClientContext &context);
153: };
154: 
155: struct LogQueryPathSetting {
156: 	static constexpr const char *Name = "log_query_path";
157: 	static constexpr const char *Description =
158: 	    "Specifies the path to which queries should be logged (default: empty string, queries are not logged)";
159: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
160: 	static void SetLocal(ClientContext &context, const Value &parameter);
161: 	static Value GetSetting(ClientContext &context);
162: };
163: 
164: struct MaximumMemorySetting {
165: 	static constexpr const char *Name = "max_memory";
166: 	static constexpr const char *Description = "The maximum memory of the system (e.g. 1GB)";
167: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
168: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
169: 	static Value GetSetting(ClientContext &context);
170: };
171: 
172: struct PerfectHashThresholdSetting {
173: 	static constexpr const char *Name = "perfect_ht_threshold";
174: 	static constexpr const char *Description = "Threshold in bytes for when to use a perfect hash table (default: 12)";
175: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::BIGINT;
176: 	static void SetLocal(ClientContext &context, const Value &parameter);
177: 	static Value GetSetting(ClientContext &context);
178: };
179: 
180: struct ProfilerHistorySize {
181: 	static constexpr const char *Name = "profiler_history_size";
182: 	static constexpr const char *Description = "Sets the profiler history size";
183: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::BIGINT;
184: 	static void SetLocal(ClientContext &context, const Value &parameter);
185: 	static Value GetSetting(ClientContext &context);
186: };
187: 
188: struct ProfileOutputSetting {
189: 	static constexpr const char *Name = "profile_output";
190: 	static constexpr const char *Description =
191: 	    "The file to which profile output should be saved, or empty to print to the terminal";
192: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
193: 	static void SetLocal(ClientContext &context, const Value &parameter);
194: 	static Value GetSetting(ClientContext &context);
195: };
196: 
197: struct ProfilingModeSetting {
198: 	static constexpr const char *Name = "profiling_mode";
199: 	static constexpr const char *Description = "The profiling mode (STANDARD or DETAILED)";
200: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
201: 	static void SetLocal(ClientContext &context, const Value &parameter);
202: 	static Value GetSetting(ClientContext &context);
203: };
204: 
205: struct ProgressBarTimeSetting {
206: 	static constexpr const char *Name = "progress_bar_time";
207: 	static constexpr const char *Description =
208: 	    "Sets the time (in milliseconds) how long a query needs to take before we start printing a progress bar";
209: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::BIGINT;
210: 	static void SetLocal(ClientContext &context, const Value &parameter);
211: 	static Value GetSetting(ClientContext &context);
212: };
213: 
214: struct SchemaSetting {
215: 	static constexpr const char *Name = "schema";
216: 	static constexpr const char *Description =
217: 	    "Sets the default search schema. Equivalent to setting search_path to a single value.";
218: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
219: 	static void SetLocal(ClientContext &context, const Value &parameter);
220: 	static Value GetSetting(ClientContext &context);
221: };
222: 
223: struct SearchPathSetting {
224: 	static constexpr const char *Name = "search_path";
225: 	static constexpr const char *Description =
226: 	    "Sets the default search search path as a comma-separated list of values";
227: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
228: 	static void SetLocal(ClientContext &context, const Value &parameter);
229: 	static Value GetSetting(ClientContext &context);
230: };
231: 
232: struct TempDirectorySetting {
233: 	static constexpr const char *Name = "temp_directory";
234: 	static constexpr const char *Description = "Set the directory to which to write temp files";
235: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::VARCHAR;
236: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
237: 	static Value GetSetting(ClientContext &context);
238: };
239: 
240: struct ThreadsSetting {
241: 	static constexpr const char *Name = "threads";
242: 	static constexpr const char *Description = "The number of total threads used by the system.";
243: 	static constexpr const LogicalTypeId InputType = LogicalTypeId::BIGINT;
244: 	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
245: 	static Value GetSetting(ClientContext &context);
246: };
247: 
248: } // namespace duckdb
[end of src/include/duckdb/main/settings.hpp]
[start of src/include/duckdb/parser/parsed_data/pragma_info.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/parsed_data/pragma_info.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/parsed_data/parse_info.hpp"
12: #include "duckdb/common/types/value.hpp"
13: #include "duckdb/common/unordered_map.hpp"
14: #include "duckdb/parser/parsed_expression.hpp"
15: 
16: namespace duckdb {
17: 
18: enum class PragmaType : uint8_t { PRAGMA_STATEMENT, PRAGMA_CALL };
19: 
20: struct PragmaInfo : public ParseInfo {
21: 	//! Name of the PRAGMA statement
22: 	string name;
23: 	//! Parameter list (if any)
24: 	vector<Value> parameters;
25: 	//! Named parameter list (if any)
26: 	unordered_map<string, Value> named_parameters;
27: 
28: public:
29: 	unique_ptr<PragmaInfo> Copy() const {
30: 		auto result = make_unique<PragmaInfo>();
31: 		result->name = name;
32: 		result->parameters = parameters;
33: 		result->named_parameters = named_parameters;
34: 		return result;
35: 	}
36: };
37: 
38: } // namespace duckdb
[end of src/include/duckdb/parser/parsed_data/pragma_info.hpp]
[start of src/include/duckdb/parser/parser.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/parser.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/sql_statement.hpp"
12: #include "duckdb/parser/parsed_expression.hpp"
13: #include "duckdb/parser/query_node.hpp"
14: #include "duckdb/parser/column_definition.hpp"
15: #include "duckdb/parser/simplified_token.hpp"
16: 
17: namespace duckdb_libpgquery {
18: struct PGNode;
19: struct PGList;
20: } // namespace duckdb_libpgquery
21: 
22: namespace duckdb {
23: 
24: //! The parser is responsible for parsing the query and converting it into a set
25: //! of parsed statements. The parsed statements can then be converted into a
26: //! plan and executed.
27: class Parser {
28: public:
29: 	Parser();
30: 
31: 	//! The parsed SQL statements from an invocation to ParseQuery.
32: 	vector<unique_ptr<SQLStatement>> statements;
33: 
34: public:
35: 	//! Attempts to parse a query into a series of SQL statements. Returns
36: 	//! whether or not the parsing was successful. If the parsing was
37: 	//! successful, the parsed statements will be stored in the statements
38: 	//! variable.
39: 	void ParseQuery(const string &query);
40: 
41: 	//! Tokenize a query, returning the raw tokens together with their locations
42: 	static vector<SimplifiedToken> Tokenize(const string &query);
43: 
44: 	//! Returns true if the given text matches a keyword of the parser
45: 	static bool IsKeyword(const string &text);
46: 	//! Returns a list of all keywords in the parser
47: 	static vector<ParserKeyword> KeywordList();
48: 
49: 	//! Parses a list of expressions (i.e. the list found in a SELECT clause)
50: 	static vector<unique_ptr<ParsedExpression>> ParseExpressionList(const string &select_list);
51: 	//! Parses a list as found in an ORDER BY expression (i.e. including optional ASCENDING/DESCENDING modifiers)
52: 	static vector<OrderByNode> ParseOrderList(const string &select_list);
53: 	//! Parses an update list (i.e. the list found in the SET clause of an UPDATE statement)
54: 	static void ParseUpdateList(const string &update_list, vector<string> &update_columns,
55: 	                            vector<unique_ptr<ParsedExpression>> &expressions);
56: 	//! Parses a VALUES list (i.e. the list of expressions after a VALUES clause)
57: 	static vector<vector<unique_ptr<ParsedExpression>>> ParseValuesList(const string &value_list);
58: 	//! Parses a column list (i.e. as found in a CREATE TABLE statement)
59: 	static vector<ColumnDefinition> ParseColumnList(const string &column_list);
60: };
61: } // namespace duckdb
[end of src/include/duckdb/parser/parser.hpp]
[start of src/include/duckdb/planner/binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_map.hpp"
12: #include "duckdb/parser/column_definition.hpp"
13: #include "duckdb/parser/tokens.hpp"
14: #include "duckdb/planner/bind_context.hpp"
15: #include "duckdb/planner/bound_tokens.hpp"
16: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
17: #include "duckdb/planner/logical_operator.hpp"
18: #include "duckdb/planner/bound_statement.hpp"
19: #include "duckdb/common/case_insensitive_map.hpp"
20: #include "duckdb/parser/result_modifier.hpp"
21: 
22: namespace duckdb {
23: class BoundResultModifier;
24: class BoundSelectNode;
25: class ClientContext;
26: class ExpressionBinder;
27: class LimitModifier;
28: class OrderBinder;
29: class TableCatalogEntry;
30: class ViewCatalogEntry;
31: 
32: struct CreateInfo;
33: struct BoundCreateTableInfo;
34: struct BoundCreateFunctionInfo;
35: struct CommonTableExpressionInfo;
36: 
37: enum class BindingMode : uint8_t { STANDARD_BINDING, EXTRACT_NAMES };
38: 
39: struct CorrelatedColumnInfo {
40: 	ColumnBinding binding;
41: 	LogicalType type;
42: 	string name;
43: 	idx_t depth;
44: 
45: 	explicit CorrelatedColumnInfo(BoundColumnRefExpression &expr)
46: 	    : binding(expr.binding), type(expr.return_type), name(expr.GetName()), depth(expr.depth) {
47: 	}
48: 
49: 	bool operator==(const CorrelatedColumnInfo &rhs) const {
50: 		return binding == rhs.binding;
51: 	}
52: };
53: 
54: //! Bind the parsed query tree to the actual columns present in the catalog.
55: /*!
56:   The binder is responsible for binding tables and columns to actual physical
57:   tables and columns in the catalog. In the process, it also resolves types of
58:   all expressions.
59: */
60: class Binder : public std::enable_shared_from_this<Binder> {
61: 	friend class ExpressionBinder;
62: 	friend class SelectBinder;
63: 	friend class RecursiveSubqueryPlanner;
64: 
65: public:
66: 	static shared_ptr<Binder> CreateBinder(ClientContext &context, Binder *parent = nullptr, bool inherit_ctes = true);
67: 
68: 	//! The client context
69: 	ClientContext &context;
70: 	//! A mapping of names to common table expressions
71: 	case_insensitive_map_t<CommonTableExpressionInfo *> CTE_bindings;
72: 	//! The CTEs that have already been bound
73: 	unordered_set<CommonTableExpressionInfo *> bound_ctes;
74: 	//! The bind context
75: 	BindContext bind_context;
76: 	//! The set of correlated columns bound by this binder (FIXME: this should probably be an unordered_set and not a
77: 	//! vector)
78: 	vector<CorrelatedColumnInfo> correlated_columns;
79: 	//! The set of parameter expressions bound by this binder
80: 	vector<BoundParameterExpression *> *parameters;
81: 	//! Whether or not the bound statement is read-only
82: 	bool read_only;
83: 	//! Whether or not the statement requires a valid transaction to run
84: 	bool requires_valid_transaction;
85: 	//! Whether or not the statement can be streamed to the client
86: 	bool allow_stream_result;
87: 	//! The alias for the currently processing subquery, if it exists
88: 	string alias;
89: 	//! Macro parameter bindings (if any)
90: 	MacroBinding *macro_binding = nullptr;
91: 
92: public:
93: 	BoundStatement Bind(SQLStatement &statement);
94: 	BoundStatement Bind(QueryNode &node);
95: 
96: 	unique_ptr<BoundCreateTableInfo> BindCreateTableInfo(unique_ptr<CreateInfo> info);
97: 	void BindCreateViewInfo(CreateViewInfo &base);
98: 	SchemaCatalogEntry *BindSchema(CreateInfo &info);
99: 	SchemaCatalogEntry *BindCreateFunctionInfo(CreateInfo &info);
100: 
101: 	//! Check usage, and cast named parameters to their types
102: 	static void BindNamedParameters(unordered_map<string, LogicalType> &types, unordered_map<string, Value> &values,
103: 	                                QueryErrorContext &error_context, string &func_name);
104: 
105: 	unique_ptr<BoundTableRef> Bind(TableRef &ref);
106: 	unique_ptr<LogicalOperator> CreatePlan(BoundTableRef &ref);
107: 
108: 	//! Generates an unused index for a table
109: 	idx_t GenerateTableIndex();
110: 
111: 	//! Add a common table expression to the binder
112: 	void AddCTE(const string &name, CommonTableExpressionInfo *cte);
113: 	//! Find a common table expression by name; returns nullptr if none exists
114: 	CommonTableExpressionInfo *FindCTE(const string &name, bool skip = false);
115: 
116: 	bool CTEIsAlreadyBound(CommonTableExpressionInfo *cte);
117: 
118: 	void PushExpressionBinder(ExpressionBinder *binder);
119: 	void PopExpressionBinder();
120: 	void SetActiveBinder(ExpressionBinder *binder);
121: 	ExpressionBinder *GetActiveBinder();
122: 	bool HasActiveBinder();
123: 
124: 	vector<ExpressionBinder *> &GetActiveBinders();
125: 
126: 	void MergeCorrelatedColumns(vector<CorrelatedColumnInfo> &other);
127: 	//! Add a correlated column to this binder (if it does not exist)
128: 	void AddCorrelatedColumn(const CorrelatedColumnInfo &info);
129: 
130: 	string FormatError(ParsedExpression &expr_context, const string &message);
131: 	string FormatError(TableRef &ref_context, const string &message);
132: 
133: 	string FormatErrorRecursive(idx_t query_location, const string &message, vector<ExceptionFormatValue> &values);
134: 	template <class T, typename... Args>
135: 	string FormatErrorRecursive(idx_t query_location, const string &msg, vector<ExceptionFormatValue> &values, T param,
136: 	                            Args... params) {
137: 		values.push_back(ExceptionFormatValue::CreateFormatValue<T>(param));
138: 		return FormatErrorRecursive(query_location, msg, values, params...);
139: 	}
140: 
141: 	template <typename... Args>
142: 	string FormatError(idx_t query_location, const string &msg, Args... params) {
143: 		vector<ExceptionFormatValue> values;
144: 		return FormatErrorRecursive(query_location, msg, values, params...);
145: 	}
146: 
147: 	static void BindLogicalType(ClientContext &context, LogicalType &type, const string &schema = "");
148: 
149: 	bool HasMatchingBinding(const string &table_name, const string &column_name, string &error_message);
150: 	bool HasMatchingBinding(const string &schema_name, const string &table_name, const string &column_name,
151: 	                        string &error_message);
152: 
153: 	void SetBindingMode(BindingMode mode);
154: 	BindingMode GetBindingMode();
155: 	void AddTableName(string table_name);
156: 	const unordered_set<string> &GetTableNames();
157: 
158: private:
159: 	//! The parent binder (if any)
160: 	shared_ptr<Binder> parent;
161: 	//! The vector of active binders
162: 	vector<ExpressionBinder *> active_binders;
163: 	//! The count of bound_tables
164: 	idx_t bound_tables;
165: 	//! Whether or not the binder has any unplanned subqueries that still need to be planned
166: 	bool has_unplanned_subqueries = false;
167: 	//! Whether or not subqueries should be planned already
168: 	bool plan_subquery = true;
169: 	//! Whether CTEs should reference the parent binder (if it exists)
170: 	bool inherit_ctes = true;
171: 	//! Whether or not the binder can contain NULLs as the root of expressions
172: 	bool can_contain_nulls = false;
173: 	//! The root statement of the query that is currently being parsed
174: 	SQLStatement *root_statement = nullptr;
175: 	//! Binding mode
176: 	BindingMode mode = BindingMode::STANDARD_BINDING;
177: 	//! Table names extracted for BindingMode::EXTRACT_NAMES
178: 	unordered_set<string> table_names;
179: 
180: private:
181: 	//! Bind the default values of the columns of a table
182: 	void BindDefaultValues(vector<ColumnDefinition> &columns, vector<unique_ptr<Expression>> &bound_defaults);
183: 	//! Bind a delimiter value (LIMIT or OFFSET)
184: 	unique_ptr<Expression> BindDelimiter(ClientContext &context, unique_ptr<ParsedExpression> delimiter,
185: 	                                     const LogicalType &type, Value &delimiter_value);
186: 
187: 	//! Move correlated expressions from the child binder to this binder
188: 	void MoveCorrelatedExpressions(Binder &other);
189: 
190: 	BoundStatement Bind(SelectStatement &stmt);
191: 	BoundStatement Bind(InsertStatement &stmt);
192: 	BoundStatement Bind(CopyStatement &stmt);
193: 	BoundStatement Bind(DeleteStatement &stmt);
194: 	BoundStatement Bind(UpdateStatement &stmt);
195: 	BoundStatement Bind(CreateStatement &stmt);
196: 	BoundStatement Bind(DropStatement &stmt);
197: 	BoundStatement Bind(AlterStatement &stmt);
198: 	BoundStatement Bind(TransactionStatement &stmt);
199: 	BoundStatement Bind(PragmaStatement &stmt);
200: 	BoundStatement Bind(ExplainStatement &stmt);
201: 	BoundStatement Bind(VacuumStatement &stmt);
202: 	BoundStatement Bind(RelationStatement &stmt);
203: 	BoundStatement Bind(ShowStatement &stmt);
204: 	BoundStatement Bind(CallStatement &stmt);
205: 	BoundStatement Bind(ExportStatement &stmt);
206: 	BoundStatement Bind(SetStatement &stmt);
207: 	BoundStatement Bind(LoadStatement &stmt);
208: 
209: 	unique_ptr<BoundQueryNode> BindNode(SelectNode &node);
210: 	unique_ptr<BoundQueryNode> BindNode(SetOperationNode &node);
211: 	unique_ptr<BoundQueryNode> BindNode(RecursiveCTENode &node);
212: 	unique_ptr<BoundQueryNode> BindNode(QueryNode &node);
213: 
214: 	unique_ptr<LogicalOperator> VisitQueryNode(BoundQueryNode &node, unique_ptr<LogicalOperator> root);
215: 	unique_ptr<LogicalOperator> CreatePlan(BoundRecursiveCTENode &node);
216: 	unique_ptr<LogicalOperator> CreatePlan(BoundSelectNode &statement);
217: 	unique_ptr<LogicalOperator> CreatePlan(BoundSetOperationNode &node);
218: 	unique_ptr<LogicalOperator> CreatePlan(BoundQueryNode &node);
219: 
220: 	unique_ptr<BoundTableRef> Bind(BaseTableRef &ref);
221: 	unique_ptr<BoundTableRef> Bind(CrossProductRef &ref);
222: 	unique_ptr<BoundTableRef> Bind(JoinRef &ref);
223: 	unique_ptr<BoundTableRef> Bind(SubqueryRef &ref, CommonTableExpressionInfo *cte = nullptr);
224: 	unique_ptr<BoundTableRef> Bind(TableFunctionRef &ref);
225: 	unique_ptr<BoundTableRef> Bind(EmptyTableRef &ref);
226: 	unique_ptr<BoundTableRef> Bind(ExpressionListRef &ref);
227: 
228: 	bool BindFunctionParameters(vector<unique_ptr<ParsedExpression>> &expressions, vector<LogicalType> &arguments,
229: 	                            vector<Value> &parameters, unordered_map<string, Value> &named_parameters,
230: 	                            unique_ptr<BoundSubqueryRef> &subquery, string &error);
231: 
232: 	unique_ptr<LogicalOperator> CreatePlan(BoundBaseTableRef &ref);
233: 	unique_ptr<LogicalOperator> CreatePlan(BoundCrossProductRef &ref);
234: 	unique_ptr<LogicalOperator> CreatePlan(BoundJoinRef &ref);
235: 	unique_ptr<LogicalOperator> CreatePlan(BoundSubqueryRef &ref);
236: 	unique_ptr<LogicalOperator> CreatePlan(BoundTableFunction &ref);
237: 	unique_ptr<LogicalOperator> CreatePlan(BoundEmptyTableRef &ref);
238: 	unique_ptr<LogicalOperator> CreatePlan(BoundExpressionListRef &ref);
239: 	unique_ptr<LogicalOperator> CreatePlan(BoundCTERef &ref);
240: 
241: 	unique_ptr<LogicalOperator> BindTable(TableCatalogEntry &table, BaseTableRef &ref);
242: 	unique_ptr<LogicalOperator> BindView(ViewCatalogEntry &view, BaseTableRef &ref);
243: 	unique_ptr<LogicalOperator> BindTableOrView(BaseTableRef &ref);
244: 
245: 	BoundStatement BindCopyTo(CopyStatement &stmt);
246: 	BoundStatement BindCopyFrom(CopyStatement &stmt);
247: 
248: 	void BindModifiers(OrderBinder &order_binder, QueryNode &statement, BoundQueryNode &result);
249: 	void BindModifierTypes(BoundQueryNode &result, const vector<LogicalType> &sql_types, idx_t projection_index);
250: 
251: 	BoundStatement BindSummarize(ShowStatement &stmt);
252: 	unique_ptr<BoundResultModifier> BindLimit(LimitModifier &limit_mod);
253: 	unique_ptr<BoundResultModifier> BindLimitPercent(LimitPercentModifier &limit_mod);
254: 	unique_ptr<Expression> BindOrderExpression(OrderBinder &order_binder, unique_ptr<ParsedExpression> expr);
255: 
256: 	unique_ptr<LogicalOperator> PlanFilter(unique_ptr<Expression> condition, unique_ptr<LogicalOperator> root);
257: 
258: 	void PlanSubqueries(unique_ptr<Expression> *expr, unique_ptr<LogicalOperator> *root);
259: 	unique_ptr<Expression> PlanSubquery(BoundSubqueryExpression &expr, unique_ptr<LogicalOperator> &root);
260: 
261: 	unique_ptr<LogicalOperator> CastLogicalOperatorToTypes(vector<LogicalType> &source_types,
262: 	                                                       vector<LogicalType> &target_types,
263: 	                                                       unique_ptr<LogicalOperator> op);
264: 
265: 	string FindBinding(const string &using_column, const string &join_side);
266: 	bool TryFindBinding(const string &using_column, const string &join_side, string &result);
267: 
268: 	void AddUsingBindingSet(unique_ptr<UsingColumnSet> set);
269: 	string RetrieveUsingBinding(Binder &current_binder, UsingColumnSet *current_set, const string &column_name,
270: 	                            const string &join_side, UsingColumnSet *new_set);
271: 
272: public:
273: 	// This should really be a private constructor, but make_shared does not allow it...
274: 	// If you are thinking about calling this, you should probably call Binder::CreateBinder
275: 	Binder(bool I_know_what_I_am_doing, ClientContext &context, shared_ptr<Binder> parent, bool inherit_ctes);
276: };
277: 
278: } // namespace duckdb
[end of src/include/duckdb/planner/binder.hpp]
[start of src/include/duckdb/planner/expression_binder/column_alias_binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression_binder/column_alias_binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_map.hpp"
12: #include "duckdb/planner/expression_binder.hpp"
13: 
14: namespace duckdb {
15: 
16: class BoundSelectNode;
17: class ColumnRefExpression;
18: 
19: //! A helper binder for WhereBinder and HavingBinder which support alias as a columnref.
20: class ColumnAliasBinder {
21: public:
22: 	ColumnAliasBinder(BoundSelectNode &node, const unordered_map<string, idx_t> &alias_map);
23: 
24: 	BindResult BindAlias(ExpressionBinder &enclosing_binder, ColumnRefExpression &expr, idx_t depth,
25: 	                     bool root_expression);
26: 
27: private:
28: 	BoundSelectNode &node;
29: 	const unordered_map<string, idx_t> &alias_map;
30: 	bool in_alias;
31: };
32: 
33: } // namespace duckdb
[end of src/include/duckdb/planner/expression_binder/column_alias_binder.hpp]
[start of src/include/duckdb/planner/expression_binder/group_binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression_binder/group_binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_map.hpp"
12: #include "duckdb/common/unordered_set.hpp"
13: #include "duckdb/planner/expression_binder.hpp"
14: 
15: namespace duckdb {
16: class ConstantExpression;
17: class ColumnRefExpression;
18: 
19: //! The GROUP binder is responsible for binding expressions in the GROUP BY clause
20: class GroupBinder : public ExpressionBinder {
21: public:
22: 	GroupBinder(Binder &binder, ClientContext &context, SelectNode &node, idx_t group_index,
23: 	            unordered_map<string, idx_t> &alias_map, unordered_map<string, idx_t> &group_alias_map);
24: 
25: 	//! The unbound root expression
26: 	unique_ptr<ParsedExpression> unbound_expression;
27: 	//! The group index currently being bound
28: 	idx_t bind_index;
29: 
30: protected:
31: 	BindResult BindExpression(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth, bool root_expression) override;
32: 
33: 	string UnsupportedAggregateMessage() override;
34: 
35: 	BindResult BindSelectRef(idx_t entry);
36: 	BindResult BindColumnRef(ColumnRefExpression &expr);
37: 	BindResult BindConstant(ConstantExpression &expr);
38: 
39: 	SelectNode &node;
40: 	unordered_map<string, idx_t> &alias_map;
41: 	unordered_map<string, idx_t> &group_alias_map;
42: 	unordered_set<idx_t> used_aliases;
43: 
44: 	idx_t group_index;
45: };
46: 
47: } // namespace duckdb
[end of src/include/duckdb/planner/expression_binder/group_binder.hpp]
[start of src/include/duckdb/planner/expression_binder/having_binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression_binder/having_binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/planner/expression_binder/select_binder.hpp"
12: #include "duckdb/planner/expression_binder/column_alias_binder.hpp"
13: 
14: namespace duckdb {
15: 
16: //! The HAVING binder is responsible for binding an expression within the HAVING clause of a SQL statement
17: class HavingBinder : public SelectBinder {
18: public:
19: 	HavingBinder(Binder &binder, ClientContext &context, BoundSelectNode &node, BoundGroupInformation &info,
20: 	             unordered_map<string, idx_t> &alias_map);
21: 
22: protected:
23: 	BindResult BindExpression(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth,
24: 	                          bool root_expression = false) override;
25: 
26: private:
27: 	BindResult BindColumnRef(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth, bool root_expression);
28: 
29: 	ColumnAliasBinder column_alias_binder;
30: };
31: 
32: } // namespace duckdb
[end of src/include/duckdb/planner/expression_binder/having_binder.hpp]
[start of src/include/duckdb/planner/expression_binder/order_binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression_binder/order_binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_map.hpp"
12: #include "duckdb/parser/expression_map.hpp"
13: #include "duckdb/parser/parsed_expression.hpp"
14: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
15: 
16: namespace duckdb {
17: class Binder;
18: class Expression;
19: class SelectNode;
20: 
21: //! The ORDER binder is responsible for binding an expression within the ORDER BY clause of a SQL statement
22: class OrderBinder {
23: public:
24: 	OrderBinder(vector<Binder *> binders, idx_t projection_index, unordered_map<string, idx_t> &alias_map,
25: 	            expression_map_t<idx_t> &projection_map, idx_t max_count);
26: 	OrderBinder(vector<Binder *> binders, idx_t projection_index, SelectNode &node,
27: 	            unordered_map<string, idx_t> &alias_map, expression_map_t<idx_t> &projection_map);
28: 
29: public:
30: 	unique_ptr<Expression> Bind(unique_ptr<ParsedExpression> expr);
31: 
32: 	idx_t MaxCount() {
33: 		return max_count;
34: 	}
35: 
36: private:
37: 	unique_ptr<Expression> CreateProjectionReference(ParsedExpression &expr, idx_t index);
38: 
39: private:
40: 	vector<Binder *> binders;
41: 	idx_t projection_index;
42: 	idx_t max_count;
43: 	vector<unique_ptr<ParsedExpression>> *extra_list;
44: 	unordered_map<string, idx_t> &alias_map;
45: 	expression_map_t<idx_t> &projection_map;
46: };
47: 
48: } // namespace duckdb
[end of src/include/duckdb/planner/expression_binder/order_binder.hpp]
[start of src/include/duckdb/planner/expression_binder/qualify_binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression_binder/qualify_binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/planner/expression_binder/select_binder.hpp"
12: #include "duckdb/planner/expression_binder/column_alias_binder.hpp"
13: 
14: namespace duckdb {
15: 
16: //! The QUALIFY binder is responsible for binding an expression within the QUALIFY clause of a SQL statement
17: class QualifyBinder : public SelectBinder {
18: public:
19: 	QualifyBinder(Binder &binder, ClientContext &context, BoundSelectNode &node, BoundGroupInformation &info,
20: 	              unordered_map<string, idx_t> &alias_map);
21: 
22: protected:
23: 	BindResult BindExpression(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth,
24: 	                          bool root_expression = false) override;
25: 
26: private:
27: 	BindResult BindColumnRef(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth, bool root_expression);
28: 
29: 	ColumnAliasBinder column_alias_binder;
30: };
31: 
32: } // namespace duckdb
[end of src/include/duckdb/planner/expression_binder/qualify_binder.hpp]
[start of src/include/duckdb/planner/expression_binder/select_binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression_binder/select_binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/unordered_map.hpp"
12: #include "duckdb/parser/expression_map.hpp"
13: #include "duckdb/planner/expression_binder.hpp"
14: 
15: namespace duckdb {
16: class BoundColumnRefExpression;
17: class WindowExpression;
18: 
19: class BoundSelectNode;
20: 
21: struct BoundGroupInformation {
22: 	expression_map_t<idx_t> map;
23: 	unordered_map<string, idx_t> alias_map;
24: };
25: 
26: //! The SELECT binder is responsible for binding an expression within the SELECT clause of a SQL statement
27: class SelectBinder : public ExpressionBinder {
28: public:
29: 	SelectBinder(Binder &binder, ClientContext &context, BoundSelectNode &node, BoundGroupInformation &info);
30: 
31: 	bool BoundAggregates() {
32: 		return bound_aggregate;
33: 	}
34: 	void ResetBindings() {
35: 		this->bound_aggregate = false;
36: 		this->bound_columns.clear();
37: 	}
38: 
39: protected:
40: 	BindResult BindExpression(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth,
41: 	                          bool root_expression = false) override;
42: 
43: 	BindResult BindAggregate(FunctionExpression &expr, AggregateFunctionCatalogEntry *function, idx_t depth) override;
44: 
45: 	BindResult BindUnnest(FunctionExpression &function, idx_t depth) override;
46: 
47: 	bool inside_window;
48: 	bool bound_aggregate = false;
49: 
50: 	BoundSelectNode &node;
51: 	BoundGroupInformation &info;
52: 
53: protected:
54: 	BindResult BindGroupingFunction(OperatorExpression &op, idx_t depth) override;
55: 	BindResult BindWindow(WindowExpression &expr, idx_t depth);
56: 
57: 	idx_t TryBindGroup(ParsedExpression &expr, idx_t depth);
58: 	BindResult BindGroup(ParsedExpression &expr, idx_t depth, idx_t group_index);
59: };
60: 
61: } // namespace duckdb
[end of src/include/duckdb/planner/expression_binder/select_binder.hpp]
[start of src/main/client_context.cpp]
1: #include "duckdb/main/client_context.hpp"
2: 
3: #include "duckdb/main/client_context_file_opener.hpp"
4: #include "duckdb/main/query_profiler.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
7: #include "duckdb/catalog/catalog_search_path.hpp"
8: #include "duckdb/common/serializer/buffered_deserializer.hpp"
9: #include "duckdb/common/serializer/buffered_serializer.hpp"
10: #include "duckdb/execution/physical_plan_generator.hpp"
11: #include "duckdb/main/database.hpp"
12: #include "duckdb/main/materialized_query_result.hpp"
13: #include "duckdb/main/query_result.hpp"
14: #include "duckdb/main/stream_query_result.hpp"
15: #include "duckdb/optimizer/optimizer.hpp"
16: #include "duckdb/parser/parser.hpp"
17: #include "duckdb/parser/expression/constant_expression.hpp"
18: #include "duckdb/parser/parsed_data/create_function_info.hpp"
19: #include "duckdb/parser/statement/drop_statement.hpp"
20: #include "duckdb/parser/statement/execute_statement.hpp"
21: #include "duckdb/parser/statement/explain_statement.hpp"
22: #include "duckdb/parser/statement/prepare_statement.hpp"
23: #include "duckdb/parser/statement/select_statement.hpp"
24: #include "duckdb/planner/operator/logical_execute.hpp"
25: #include "duckdb/planner/planner.hpp"
26: #include "duckdb/transaction/transaction_manager.hpp"
27: #include "duckdb/transaction/transaction.hpp"
28: #include "duckdb/storage/data_table.hpp"
29: #include "duckdb/main/appender.hpp"
30: #include "duckdb/main/relation.hpp"
31: #include "duckdb/parser/statement/relation_statement.hpp"
32: #include "duckdb/parallel/task_scheduler.hpp"
33: #include "duckdb/common/serializer/buffered_file_writer.hpp"
34: #include "duckdb/planner/pragma_handler.hpp"
35: #include "duckdb/common/to_string.hpp"
36: #include "duckdb/common/file_system.hpp"
37: #include "duckdb/execution/column_binding_resolver.hpp"
38: 
39: namespace duckdb {
40: 
41: struct ActiveQueryContext {
42: 	//! The query that is currently being executed
43: 	string query;
44: 	//! The currently open result
45: 	BaseQueryResult *open_result = nullptr;
46: 	//! Prepared statement data
47: 	shared_ptr<PreparedStatementData> prepared;
48: 	//! The query executor
49: 	unique_ptr<Executor> executor;
50: 	//! The progress bar
51: 	unique_ptr<ProgressBar> progress_bar;
52: };
53: 
54: ClientContext::ClientContext(shared_ptr<DatabaseInstance> database)
55:     : profiler(make_shared<QueryProfiler>(*this)), query_profiler_history(make_unique<QueryProfilerHistory>()),
56:       db(move(database)), transaction(db->GetTransactionManager(), *this), interrupted(false),
57:       temporary_objects(make_unique<SchemaCatalogEntry>(&db->GetCatalog(), TEMP_SCHEMA, true)),
58:       catalog_search_path(make_unique<CatalogSearchPath>(*this)),
59:       file_opener(make_unique<ClientContextFileOpener>(*this)) {
60: 	std::random_device rd;
61: 	random_engine.seed(rd());
62: }
63: 
64: ClientContext::~ClientContext() {
65: 	if (Exception::UncaughtException()) {
66: 		return;
67: 	}
68: 	// destroy the client context and rollback if there is an active transaction
69: 	// but only if we are not destroying this client context as part of an exception stack unwind
70: 	Destroy();
71: }
72: 
73: unique_ptr<ClientContextLock> ClientContext::LockContext() {
74: 	return make_unique<ClientContextLock>(context_lock);
75: }
76: 
77: void ClientContext::Destroy() {
78: 	auto lock = LockContext();
79: 	if (transaction.HasActiveTransaction()) {
80: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
81: 		if (!transaction.IsAutoCommit()) {
82: 			transaction.Rollback();
83: 		}
84: 	}
85: 	CleanupInternal(*lock);
86: }
87: 
88: unique_ptr<DataChunk> ClientContext::Fetch(ClientContextLock &lock, StreamQueryResult &result) {
89: 	D_ASSERT(IsActiveResult(lock, &result));
90: 	D_ASSERT(active_query->executor);
91: 	return FetchInternal(lock, *active_query->executor, result);
92: }
93: 
94: unique_ptr<DataChunk> ClientContext::FetchInternal(ClientContextLock &lock, Executor &executor,
95:                                                    BaseQueryResult &result) {
96: 	bool invalidate_query = true;
97: 	try {
98: 		// fetch the chunk and return it
99: 		auto chunk = executor.FetchChunk();
100: 		if (!chunk || chunk->size() == 0) {
101: 			CleanupInternal(lock, &result);
102: 		}
103: 		return chunk;
104: 	} catch (StandardException &ex) {
105: 		// standard exceptions do not invalidate the current transaction
106: 		result.error = ex.what();
107: 		invalidate_query = false;
108: 	} catch (std::exception &ex) {
109: 		result.error = ex.what();
110: 	} catch (...) { // LCOV_EXCL_START
111: 		result.error = "Unhandled exception in FetchInternal";
112: 	} // LCOV_EXCL_STOP
113: 	result.success = false;
114: 	CleanupInternal(lock, &result, invalidate_query);
115: 	return nullptr;
116: }
117: 
118: void ClientContext::BeginTransactionInternal(ClientContextLock &lock, bool requires_valid_transaction) {
119: 	// check if we are on AutoCommit. In this case we should start a transaction
120: 	D_ASSERT(!active_query);
121: 	if (requires_valid_transaction && transaction.HasActiveTransaction() &&
122: 	    transaction.ActiveTransaction().IsInvalidated()) {
123: 		throw Exception("Failed: transaction has been invalidated!");
124: 	}
125: 	active_query = make_unique<ActiveQueryContext>();
126: 	if (transaction.IsAutoCommit()) {
127: 		transaction.BeginTransaction();
128: 	}
129: }
130: 
131: void ClientContext::BeginQueryInternal(ClientContextLock &lock, const string &query) {
132: 	BeginTransactionInternal(lock, false);
133: 	LogQueryInternal(lock, query);
134: 	active_query->query = query;
135: 	query_progress = -1;
136: 	ActiveTransaction().active_query = db->GetTransactionManager().GetQueryNumber();
137: }
138: 
139: string ClientContext::EndQueryInternal(ClientContextLock &lock, bool success, bool invalidate_transaction) {
140: 	profiler->EndQuery();
141: 
142: 	D_ASSERT(active_query.get());
143: 	string error;
144: 	try {
145: 		if (transaction.HasActiveTransaction()) {
146: 			// Move the query profiler into the history
147: 			auto &prev_profilers = query_profiler_history->GetPrevProfilers();
148: 			prev_profilers.emplace_back(transaction.ActiveTransaction().active_query, move(profiler));
149: 			// Reinitialize the query profiler
150: 			profiler = make_shared<QueryProfiler>(*this);
151: 			// Propagate settings of the saved query into the new profiler.
152: 			profiler->Propagate(*prev_profilers.back().second);
153: 			if (prev_profilers.size() >= query_profiler_history->GetPrevProfilersSize()) {
154: 				prev_profilers.pop_front();
155: 			}
156: 
157: 			ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
158: 			if (transaction.IsAutoCommit()) {
159: 				if (success) {
160: 					transaction.Commit();
161: 				} else {
162: 					transaction.Rollback();
163: 				}
164: 			} else if (invalidate_transaction) {
165: 				D_ASSERT(!success);
166: 				ActiveTransaction().Invalidate();
167: 			}
168: 		}
169: 	} catch (std::exception &ex) {
170: 		error = ex.what();
171: 	} catch (...) { // LCOV_EXCL_START
172: 		error = "Unhandled exception!";
173: 	} // LCOV_EXCL_STOP
174: 	active_query.reset();
175: 	query_progress = -1;
176: 	return error;
177: }
178: 
179: void ClientContext::CleanupInternal(ClientContextLock &lock, BaseQueryResult *result, bool invalidate_transaction) {
180: 	if (!active_query) {
181: 		// no query currently active
182: 		return;
183: 	}
184: 	if (active_query->executor) {
185: 		active_query->executor->CancelTasks();
186: 	}
187: 	active_query->progress_bar.reset();
188: 
189: 	auto error = EndQueryInternal(lock, result ? result->success : false, invalidate_transaction);
190: 	if (result && result->success) {
191: 		// if an error occurred while committing report it in the result
192: 		result->error = error;
193: 		result->success = error.empty();
194: 	}
195: 	D_ASSERT(!active_query);
196: }
197: 
198: Executor &ClientContext::GetExecutor() {
199: 	D_ASSERT(active_query);
200: 	D_ASSERT(active_query->executor);
201: 	return *active_query->executor;
202: }
203: 
204: const string &ClientContext::GetCurrentQuery() {
205: 	D_ASSERT(active_query);
206: 	return active_query->query;
207: }
208: 
209: unique_ptr<QueryResult> ClientContext::FetchResultInternal(ClientContextLock &lock, PendingQueryResult &pending,
210:                                                            bool allow_stream_result) {
211: 	D_ASSERT(active_query);
212: 	D_ASSERT(active_query->open_result == &pending);
213: 	D_ASSERT(active_query->prepared);
214: 	auto &prepared = *active_query->prepared;
215: 	bool create_stream_result = prepared.allow_stream_result && allow_stream_result;
216: 	if (create_stream_result) {
217: 		active_query->progress_bar.reset();
218: 		query_progress = -1;
219: 
220: 		// successfully compiled SELECT clause and it is the last statement
221: 		// return a StreamQueryResult so the client can call Fetch() on it and stream the result
222: 		auto stream_result =
223: 		    make_unique<StreamQueryResult>(pending.statement_type, shared_from_this(), pending.types, pending.names);
224: 		active_query->open_result = stream_result.get();
225: 		return move(stream_result);
226: 	}
227: 	// create a materialized result by continuously fetching
228: 	auto result = make_unique<MaterializedQueryResult>(pending.statement_type, pending.types, pending.names);
229: 	while (true) {
230: 		auto chunk = FetchInternal(lock, GetExecutor(), *result);
231: 		if (!chunk || chunk->size() == 0) {
232: 			break;
233: 		}
234: #ifdef DEBUG
235: 		for (idx_t i = 0; i < chunk->ColumnCount(); i++) {
236: 			if (pending.types[i].id() == LogicalTypeId::VARCHAR) {
237: 				chunk->data[i].UTFVerify(chunk->size());
238: 			}
239: 		}
240: #endif
241: 		result->collection.Append(*chunk);
242: 	}
243: 	return move(result);
244: }
245: 
246: shared_ptr<PreparedStatementData> ClientContext::CreatePreparedStatement(ClientContextLock &lock, const string &query,
247:                                                                          unique_ptr<SQLStatement> statement) {
248: 	StatementType statement_type = statement->type;
249: 	auto result = make_shared<PreparedStatementData>(statement_type);
250: 
251: 	auto &profiler = QueryProfiler::Get(*this);
252: 	profiler.StartPhase("planner");
253: 	Planner planner(*this);
254: 	planner.CreatePlan(move(statement));
255: 	D_ASSERT(planner.plan);
256: 	profiler.EndPhase();
257: 
258: 	auto plan = move(planner.plan);
259: #ifdef DEBUG
260: 	plan->Verify();
261: #endif
262: 	// extract the result column names from the plan
263: 	result->read_only = planner.read_only;
264: 	result->requires_valid_transaction = planner.requires_valid_transaction;
265: 	result->allow_stream_result = planner.allow_stream_result;
266: 	result->names = planner.names;
267: 	result->types = planner.types;
268: 	result->value_map = move(planner.value_map);
269: 	result->catalog_version = Transaction::GetTransaction(*this).catalog_version;
270: 
271: 	if (config.enable_optimizer) {
272: 		profiler.StartPhase("optimizer");
273: 		Optimizer optimizer(*planner.binder, *this);
274: 		plan = optimizer.Optimize(move(plan));
275: 		D_ASSERT(plan);
276: 		profiler.EndPhase();
277: 
278: #ifdef DEBUG
279: 		plan->Verify();
280: #endif
281: 	}
282: 
283: 	profiler.StartPhase("physical_planner");
284: 	// now convert logical query plan into a physical query plan
285: 	PhysicalPlanGenerator physical_planner(*this);
286: 	auto physical_plan = physical_planner.CreatePlan(move(plan));
287: 	profiler.EndPhase();
288: 
289: #ifdef DEBUG
290: 	D_ASSERT(!physical_plan->ToString().empty());
291: #endif
292: 	result->plan = move(physical_plan);
293: 	return result;
294: }
295: 
296: double ClientContext::GetProgress() {
297: 	return query_progress.load();
298: }
299: 
300: unique_ptr<PendingQueryResult> ClientContext::PendingPreparedStatement(ClientContextLock &lock,
301:                                                                        shared_ptr<PreparedStatementData> statement_p,
302:                                                                        vector<Value> bound_values) {
303: 	D_ASSERT(active_query);
304: 	auto &statement = *statement_p;
305: 	if (ActiveTransaction().IsInvalidated() && statement.requires_valid_transaction) {
306: 		throw Exception("Current transaction is aborted (please ROLLBACK)");
307: 	}
308: 	auto &db_config = DBConfig::GetConfig(*this);
309: 	if (db_config.access_mode == AccessMode::READ_ONLY && !statement.read_only) {
310: 		throw Exception(StringUtil::Format("Cannot execute statement of type \"%s\" in read-only mode!",
311: 		                                   StatementTypeToString(statement.statement_type)));
312: 	}
313: 
314: 	// bind the bound values before execution
315: 	statement.Bind(move(bound_values));
316: 
317: 	active_query->executor = make_unique<Executor>(*this);
318: 	auto &executor = *active_query->executor;
319: 	if (config.enable_progress_bar) {
320: 		active_query->progress_bar = make_unique<ProgressBar>(executor, config.wait_time);
321: 		active_query->progress_bar->Start();
322: 		query_progress = 0;
323: 	}
324: 	executor.Initialize(statement.plan.get());
325: 	auto types = executor.GetTypes();
326: 	D_ASSERT(types == statement.types);
327: 	D_ASSERT(!active_query->open_result);
328: 
329: 	auto pending_result = make_unique<PendingQueryResult>(shared_from_this(), *statement_p, move(types));
330: 	active_query->prepared = move(statement_p);
331: 	active_query->open_result = pending_result.get();
332: 	return pending_result;
333: }
334: 
335: PendingExecutionResult ClientContext::ExecuteTaskInternal(ClientContextLock &lock, PendingQueryResult &result) {
336: 	D_ASSERT(active_query);
337: 	D_ASSERT(active_query->open_result == &result);
338: 	try {
339: 		auto result = active_query->executor->ExecuteTask();
340: 		if (active_query->progress_bar) {
341: 			active_query->progress_bar->Update(result == PendingExecutionResult::RESULT_READY);
342: 			query_progress = active_query->progress_bar->GetCurrentPercentage();
343: 		}
344: 		return result;
345: 	} catch (std::exception &ex) {
346: 		result.error = ex.what();
347: 	} catch (...) { // LCOV_EXCL_START
348: 		result.error = "Unhandled exception in ExecuteTaskInternal";
349: 	} // LCOV_EXCL_STOP
350: 	EndQueryInternal(lock, false, true);
351: 	result.success = false;
352: 	return PendingExecutionResult::EXECUTION_ERROR;
353: }
354: 
355: void ClientContext::InitialCleanup(ClientContextLock &lock) {
356: 	//! Cleanup any open results and reset the interrupted flag
357: 	CleanupInternal(lock);
358: 	interrupted = false;
359: }
360: 
361: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatements(const string &query) {
362: 	auto lock = LockContext();
363: 	return ParseStatementsInternal(*lock, query);
364: }
365: 
366: vector<unique_ptr<SQLStatement>> ClientContext::ParseStatementsInternal(ClientContextLock &lock, const string &query) {
367: 	Parser parser;
368: 	parser.ParseQuery(query);
369: 
370: 	PragmaHandler handler(*this);
371: 	handler.HandlePragmaStatements(lock, parser.statements);
372: 
373: 	return move(parser.statements);
374: }
375: 
376: void ClientContext::HandlePragmaStatements(vector<unique_ptr<SQLStatement>> &statements) {
377: 	auto lock = LockContext();
378: 
379: 	PragmaHandler handler(*this);
380: 	handler.HandlePragmaStatements(*lock, statements);
381: }
382: 
383: unique_ptr<LogicalOperator> ClientContext::ExtractPlan(const string &query) {
384: 	auto lock = LockContext();
385: 
386: 	auto statements = ParseStatementsInternal(*lock, query);
387: 	if (statements.size() != 1) {
388: 		throw Exception("ExtractPlan can only prepare a single statement");
389: 	}
390: 
391: 	unique_ptr<LogicalOperator> plan;
392: 	RunFunctionInTransactionInternal(*lock, [&]() {
393: 		Planner planner(*this);
394: 		planner.CreatePlan(move(statements[0]));
395: 		D_ASSERT(planner.plan);
396: 
397: 		plan = move(planner.plan);
398: 
399: 		if (config.enable_optimizer) {
400: 			Optimizer optimizer(*planner.binder, *this);
401: 			plan = optimizer.Optimize(move(plan));
402: 		}
403: 
404: 		ColumnBindingResolver resolver;
405: 		resolver.VisitOperator(*plan);
406: 
407: 		plan->ResolveOperatorTypes();
408: 	});
409: 	return plan;
410: }
411: 
412: unique_ptr<PreparedStatement> ClientContext::PrepareInternal(ClientContextLock &lock,
413:                                                              unique_ptr<SQLStatement> statement) {
414: 	auto n_param = statement->n_param;
415: 	auto statement_query = statement->query;
416: 	shared_ptr<PreparedStatementData> prepared_data;
417: 	auto unbound_statement = statement->Copy();
418: 	RunFunctionInTransactionInternal(
419: 	    lock, [&]() { prepared_data = CreatePreparedStatement(lock, statement_query, move(statement)); }, false);
420: 	prepared_data->unbound_statement = move(unbound_statement);
421: 	return make_unique<PreparedStatement>(shared_from_this(), move(prepared_data), move(statement_query), n_param);
422: }
423: 
424: unique_ptr<PreparedStatement> ClientContext::Prepare(unique_ptr<SQLStatement> statement) {
425: 	auto lock = LockContext();
426: 	// prepare the query
427: 	try {
428: 		InitialCleanup(*lock);
429: 		return PrepareInternal(*lock, move(statement));
430: 	} catch (std::exception &ex) {
431: 		return make_unique<PreparedStatement>(ex.what());
432: 	}
433: }
434: 
435: unique_ptr<PreparedStatement> ClientContext::Prepare(const string &query) {
436: 	auto lock = LockContext();
437: 	// prepare the query
438: 	try {
439: 		InitialCleanup(*lock);
440: 
441: 		// first parse the query
442: 		auto statements = ParseStatementsInternal(*lock, query);
443: 		if (statements.empty()) {
444: 			throw Exception("No statement to prepare!");
445: 		}
446: 		if (statements.size() > 1) {
447: 			throw Exception("Cannot prepare multiple statements at once!");
448: 		}
449: 		return PrepareInternal(*lock, move(statements[0]));
450: 	} catch (std::exception &ex) {
451: 		return make_unique<PreparedStatement>(ex.what());
452: 	}
453: }
454: 
455: unique_ptr<PendingQueryResult> ClientContext::PendingQueryPreparedInternal(ClientContextLock &lock, const string &query,
456:                                                                            shared_ptr<PreparedStatementData> &prepared,
457:                                                                            vector<Value> &values) {
458: 	try {
459: 		InitialCleanup(lock);
460: 	} catch (std::exception &ex) {
461: 		return make_unique<PendingQueryResult>(ex.what());
462: 	}
463: 	return PendingStatementOrPreparedStatementInternal(lock, query, nullptr, prepared, &values);
464: }
465: 
466: unique_ptr<PendingQueryResult>
467: ClientContext::PendingQuery(const string &query, shared_ptr<PreparedStatementData> &prepared, vector<Value> &values) {
468: 	auto lock = LockContext();
469: 	return PendingQueryPreparedInternal(*lock, query, prepared, values);
470: }
471: 
472: unique_ptr<QueryResult> ClientContext::Execute(const string &query, shared_ptr<PreparedStatementData> &prepared,
473:                                                vector<Value> &values, bool allow_stream_result) {
474: 	auto lock = LockContext();
475: 	auto pending = PendingQueryPreparedInternal(*lock, query, prepared, values);
476: 	if (!pending->success) {
477: 		return make_unique<MaterializedQueryResult>(pending->error);
478: 	}
479: 	return pending->ExecuteInternal(*lock, allow_stream_result);
480: }
481: 
482: unique_ptr<PendingQueryResult> ClientContext::PendingStatementInternal(ClientContextLock &lock, const string &query,
483:                                                                        unique_ptr<SQLStatement> statement) {
484: 	// prepare the query for execution
485: 	auto prepared = CreatePreparedStatement(lock, query, move(statement));
486: 	// by default, no values are bound
487: 	vector<Value> bound_values;
488: 	// execute the prepared statement
489: 	return PendingPreparedStatement(lock, move(prepared), move(bound_values));
490: }
491: 
492: unique_ptr<QueryResult> ClientContext::RunStatementInternal(ClientContextLock &lock, const string &query,
493:                                                             unique_ptr<SQLStatement> statement,
494:                                                             bool allow_stream_result, bool verify) {
495: 	auto pending = PendingQueryInternal(lock, move(statement), verify);
496: 	if (!pending->success) {
497: 		return make_unique<MaterializedQueryResult>(move(pending->error));
498: 	}
499: 	return ExecutePendingQueryInternal(lock, *pending, allow_stream_result);
500: }
501: 
502: bool ClientContext::IsActiveResult(ClientContextLock &lock, BaseQueryResult *result) {
503: 	if (!active_query) {
504: 		return false;
505: 	}
506: 	return active_query->open_result == result;
507: }
508: 
509: static bool IsExplainAnalyze(SQLStatement *statement) {
510: 	if (!statement) {
511: 		return false;
512: 	}
513: 	if (statement->type != StatementType::EXPLAIN_STATEMENT) {
514: 		return false;
515: 	}
516: 	auto &explain = (ExplainStatement &)*statement;
517: 	return explain.explain_type == ExplainType::EXPLAIN_ANALYZE;
518: }
519: 
520: unique_ptr<PendingQueryResult> ClientContext::PendingStatementOrPreparedStatementInternal(
521:     ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement,
522:     shared_ptr<PreparedStatementData> &prepared, vector<Value> *values) {
523: 	// check if we are on AutoCommit. In this case we should start a transaction.
524: 	if (statement && config.query_verification_enabled) {
525: 		// query verification is enabled
526: 		// create a copy of the statement, and use the copy
527: 		// this way we verify that the copy correctly copies all properties
528: 		auto copied_statement = statement->Copy();
529: 		if (statement->type == StatementType::SELECT_STATEMENT) {
530: 			// in case this is a select query, we verify the original statement
531: 			string error = VerifyQuery(lock, query, move(statement));
532: 			if (!error.empty()) {
533: 				// error in verifying query
534: 				return make_unique<PendingQueryResult>(error);
535: 			}
536: 		}
537: 		statement = move(copied_statement);
538: 	}
539: 	return PendingStatementOrPreparedStatement(lock, query, move(statement), prepared, values);
540: }
541: 
542: unique_ptr<PendingQueryResult>
543: ClientContext::PendingStatementOrPreparedStatement(ClientContextLock &lock, const string &query,
544:                                                    unique_ptr<SQLStatement> statement,
545:                                                    shared_ptr<PreparedStatementData> &prepared, vector<Value> *values) {
546: 	unique_ptr<PendingQueryResult> result;
547: 
548: 	BeginQueryInternal(lock, query);
549: 	// start the profiler
550: 	auto &profiler = QueryProfiler::Get(*this);
551: 	profiler.StartQuery(query, IsExplainAnalyze(statement ? statement.get() : prepared->unbound_statement.get()));
552: 	bool invalidate_query = true;
553: 	try {
554: 		if (statement) {
555: 			result = PendingStatementInternal(lock, query, move(statement));
556: 		} else {
557: 			auto &catalog = Catalog::GetCatalog(*this);
558: 			if (prepared->unbound_statement && catalog.GetCatalogVersion() != prepared->catalog_version) {
559: 				D_ASSERT(prepared->unbound_statement.get());
560: 				// catalog was modified: rebind the statement before execution
561: 				auto new_prepared = CreatePreparedStatement(lock, query, prepared->unbound_statement->Copy());
562: 				if (prepared->types != new_prepared->types) {
563: 					throw BinderException("Rebinding statement after catalog change resulted in change of types");
564: 				}
565: 				new_prepared->unbound_statement = move(prepared->unbound_statement);
566: 				prepared = move(new_prepared);
567: 			}
568: 			result = PendingPreparedStatement(lock, prepared, *values);
569: 		}
570: 	} catch (StandardException &ex) {
571: 		// standard exceptions do not invalidate the current transaction
572: 		result = make_unique<PendingQueryResult>(ex.what());
573: 		invalidate_query = false;
574: 	} catch (std::exception &ex) {
575: 		// other types of exceptions do invalidate the current transaction
576: 		result = make_unique<PendingQueryResult>(ex.what());
577: 	}
578: 	if (!result->success) {
579: 		// query failed: abort now
580: 		EndQueryInternal(lock, false, invalidate_query);
581: 		return result;
582: 	}
583: 	D_ASSERT(active_query->open_result == result.get());
584: 	return result;
585: }
586: 
587: void ClientContext::LogQueryInternal(ClientContextLock &, const string &query) {
588: 	if (!log_query_writer) {
589: #ifdef DUCKDB_FORCE_QUERY_LOG
590: 		try {
591: 			string log_path(DUCKDB_FORCE_QUERY_LOG);
592: 			log_query_writer = make_unique<BufferedFileWriter>(
593: 			    FileSystem::GetFileSystem(*this), log_path, BufferedFileWriter::DEFAULT_OPEN_FLAGS, file_opener.get());
594: 		} catch (...) {
595: 			return;
596: 		}
597: #else
598: 		return;
599: #endif
600: 	}
601: 	// log query path is set: log the query
602: 	log_query_writer->WriteData((const_data_ptr_t)query.c_str(), query.size());
603: 	log_query_writer->WriteData((const_data_ptr_t) "\n", 1);
604: 	log_query_writer->Flush();
605: 	log_query_writer->Sync();
606: }
607: 
608: unique_ptr<QueryResult> ClientContext::Query(unique_ptr<SQLStatement> statement, bool allow_stream_result) {
609: 	auto pending_query = PendingQuery(move(statement));
610: 	return pending_query->Execute(allow_stream_result);
611: }
612: 
613: unique_ptr<QueryResult> ClientContext::Query(const string &query, bool allow_stream_result) {
614: 	auto lock = LockContext();
615: 
616: 	string error;
617: 	vector<unique_ptr<SQLStatement>> statements;
618: 	if (!ParseStatements(*lock, query, statements, error)) {
619: 		return make_unique<MaterializedQueryResult>(move(error));
620: 	}
621: 	if (statements.empty()) {
622: 		// no statements, return empty successful result
623: 		return make_unique<MaterializedQueryResult>(StatementType::INVALID_STATEMENT);
624: 	}
625: 
626: 	unique_ptr<QueryResult> result;
627: 	QueryResult *last_result = nullptr;
628: 	for (idx_t i = 0; i < statements.size(); i++) {
629: 		auto &statement = statements[i];
630: 		bool is_last_statement = i + 1 == statements.size();
631: 		bool stream_result = allow_stream_result && is_last_statement;
632: 		auto pending_query = PendingQueryInternal(*lock, move(statement));
633: 		unique_ptr<QueryResult> current_result;
634: 		if (!pending_query->success) {
635: 			current_result = make_unique<MaterializedQueryResult>(pending_query->error);
636: 		} else {
637: 			current_result = ExecutePendingQueryInternal(*lock, *pending_query, stream_result);
638: 		}
639: 		// now append the result to the list of results
640: 		if (!last_result) {
641: 			// first result of the query
642: 			result = move(current_result);
643: 			last_result = result.get();
644: 		} else {
645: 			// later results; attach to the result chain
646: 			last_result->next = move(current_result);
647: 			last_result = last_result->next.get();
648: 		}
649: 	}
650: 	return result;
651: }
652: 
653: bool ClientContext::ParseStatements(ClientContextLock &lock, const string &query,
654:                                     vector<unique_ptr<SQLStatement>> &result, string &error) {
655: 	try {
656: 		InitialCleanup(lock);
657: 		// parse the query and transform it into a set of statements
658: 		result = ParseStatementsInternal(lock, query);
659: 		return true;
660: 	} catch (std::exception &ex) {
661: 		error = ex.what();
662: 		return false;
663: 	}
664: }
665: 
666: unique_ptr<PendingQueryResult> ClientContext::PendingQuery(const string &query) {
667: 	auto lock = LockContext();
668: 
669: 	string error;
670: 	vector<unique_ptr<SQLStatement>> statements;
671: 	if (!ParseStatements(*lock, query, statements, error)) {
672: 		return make_unique<PendingQueryResult>(move(error));
673: 	}
674: 	if (statements.size() != 1) {
675: 		return make_unique<PendingQueryResult>("PendingQuery can only take a single statement");
676: 	}
677: 	return PendingQueryInternal(*lock, move(statements[0]));
678: }
679: 
680: unique_ptr<PendingQueryResult> ClientContext::PendingQuery(unique_ptr<SQLStatement> statement) {
681: 	auto lock = LockContext();
682: 	return PendingQueryInternal(*lock, move(statement));
683: }
684: 
685: unique_ptr<PendingQueryResult> ClientContext::PendingQueryInternal(ClientContextLock &lock,
686:                                                                    unique_ptr<SQLStatement> statement, bool verify) {
687: 	auto query = statement->query;
688: 	shared_ptr<PreparedStatementData> prepared;
689: 	if (verify) {
690: 		return PendingStatementOrPreparedStatementInternal(lock, query, move(statement), prepared, nullptr);
691: 	} else {
692: 		return PendingStatementOrPreparedStatement(lock, query, move(statement), prepared, nullptr);
693: 	}
694: }
695: 
696: unique_ptr<QueryResult> ClientContext::ExecutePendingQueryInternal(ClientContextLock &lock, PendingQueryResult &query,
697:                                                                    bool allow_stream_result) {
698: 	return query.ExecuteInternal(lock, allow_stream_result);
699: }
700: 
701: void ClientContext::Interrupt() {
702: 	interrupted = true;
703: }
704: 
705: void ClientContext::EnableProfiling() {
706: 	auto lock = LockContext();
707: 	auto &config = ClientConfig::GetConfig(*this);
708: 	config.enable_profiler = true;
709: }
710: 
711: void ClientContext::DisableProfiling() {
712: 	auto lock = LockContext();
713: 	auto &config = ClientConfig::GetConfig(*this);
714: 	config.enable_profiler = false;
715: }
716: 
717: string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query, unique_ptr<SQLStatement> statement) {
718: 	D_ASSERT(statement->type == StatementType::SELECT_STATEMENT);
719: 	// aggressive query verification
720: 
721: 	// the purpose of this function is to test correctness of otherwise hard to test features:
722: 	// Copy() of statements and expressions
723: 	// Serialize()/Deserialize() of expressions
724: 	// Hash() of expressions
725: 	// Equality() of statements and expressions
726: 	// Correctness of plans both with and without optimizers
727: 	// Correctness of plans both with and without parallelism
728: 
729: 	// copy the statement
730: 	auto select_stmt = (SelectStatement *)statement.get();
731: 	auto copied_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
732: 	auto unoptimized_stmt = unique_ptr_cast<SQLStatement, SelectStatement>(select_stmt->Copy());
733: 
734: 	BufferedSerializer serializer;
735: 	select_stmt->Serialize(serializer);
736: 	BufferedDeserializer source(serializer);
737: 	auto deserialized_stmt = SelectStatement::Deserialize(source);
738: 	// all the statements should be equal
739: 	D_ASSERT(copied_stmt->Equals(statement.get()));
740: 	D_ASSERT(deserialized_stmt->Equals(statement.get()));
741: 	D_ASSERT(copied_stmt->Equals(deserialized_stmt.get()));
742: 
743: 	// now perform checking on the expressions
744: #ifdef DEBUG
745: 	auto &orig_expr_list = select_stmt->node->GetSelectList();
746: 	auto &de_expr_list = deserialized_stmt->node->GetSelectList();
747: 	auto &cp_expr_list = copied_stmt->node->GetSelectList();
748: 	D_ASSERT(orig_expr_list.size() == de_expr_list.size() && cp_expr_list.size() == de_expr_list.size());
749: 	for (idx_t i = 0; i < orig_expr_list.size(); i++) {
750: 		// run the ToString, to verify that it doesn't crash
751: 		orig_expr_list[i]->ToString();
752: 		// check that the expressions are equivalent
753: 		D_ASSERT(orig_expr_list[i]->Equals(de_expr_list[i].get()));
754: 		D_ASSERT(orig_expr_list[i]->Equals(cp_expr_list[i].get()));
755: 		D_ASSERT(de_expr_list[i]->Equals(cp_expr_list[i].get()));
756: 		// check that the hashes are equivalent too
757: 		D_ASSERT(orig_expr_list[i]->Hash() == de_expr_list[i]->Hash());
758: 		D_ASSERT(orig_expr_list[i]->Hash() == cp_expr_list[i]->Hash());
759: 
760: 		D_ASSERT(!orig_expr_list[i]->Equals(nullptr));
761: 	}
762: 	// now perform additional checking within the expressions
763: 	for (idx_t outer_idx = 0; outer_idx < orig_expr_list.size(); outer_idx++) {
764: 		auto hash = orig_expr_list[outer_idx]->Hash();
765: 		for (idx_t inner_idx = 0; inner_idx < orig_expr_list.size(); inner_idx++) {
766: 			auto hash2 = orig_expr_list[inner_idx]->Hash();
767: 			if (hash != hash2) {
768: 				// if the hashes are not equivalent, the expressions should not be equivalent
769: 				D_ASSERT(!orig_expr_list[outer_idx]->Equals(orig_expr_list[inner_idx].get()));
770: 			}
771: 		}
772: 	}
773: #endif
774: 
775: 	// disable profiling if it is enabled
776: 	auto &config = ClientConfig::GetConfig(*this);
777: 	bool profiling_is_enabled = config.enable_profiler;
778: 	if (profiling_is_enabled) {
779: 		config.enable_profiler = false;
780: 	}
781: 
782: 	// see below
783: 	auto statement_copy_for_explain = select_stmt->Copy();
784: 
785: 	unique_ptr<MaterializedQueryResult> original_result =
786: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
787: 	                                    copied_result =
788: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
789: 	                                    deserialized_result =
790: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT),
791: 	                                    unoptimized_result =
792: 	                                        make_unique<MaterializedQueryResult>(StatementType::SELECT_STATEMENT);
793: 
794: 	// execute the original statement
795: 	try {
796: 		auto result = RunStatementInternal(lock, query, move(statement), false, false);
797: 		original_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
798: 	} catch (std::exception &ex) {
799: 		original_result->error = ex.what();
800: 		original_result->success = false;
801: 	}
802: 	interrupted = false;
803: 
804: 	// check explain, only if q does not already contain EXPLAIN
805: 	if (original_result->success) {
806: 		auto explain_q = "EXPLAIN " + query;
807: 		auto explain_stmt = make_unique<ExplainStatement>(move(statement_copy_for_explain));
808: 		try {
809: 			RunStatementInternal(lock, explain_q, move(explain_stmt), false, false);
810: 		} catch (std::exception &ex) { // LCOV_EXCL_START
811: 			return "EXPLAIN failed but query did not (" + string(ex.what()) + ")";
812: 		} // LCOV_EXCL_STOP
813: 	}
814: 
815: 	// now execute the copied statement
816: 	try {
817: 		auto result = RunStatementInternal(lock, query, move(copied_stmt), false, false);
818: 		copied_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
819: 	} catch (std::exception &ex) {
820: 		copied_result->error = ex.what();
821: 		copied_result->success = false;
822: 	}
823: 	interrupted = false;
824: 	// now execute the deserialized statement
825: 	try {
826: 		auto result = RunStatementInternal(lock, query, move(deserialized_stmt), false, false);
827: 		deserialized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
828: 	} catch (std::exception &ex) {
829: 		deserialized_result->error = ex.what();
830: 		deserialized_result->success = false;
831: 	}
832: 	interrupted = false;
833: 	// now execute the unoptimized statement
834: 	config.enable_optimizer = false;
835: 	try {
836: 		auto result = RunStatementInternal(lock, query, move(unoptimized_stmt), false, false);
837: 		unoptimized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
838: 	} catch (std::exception &ex) {
839: 		unoptimized_result->error = ex.what();
840: 		unoptimized_result->success = false;
841: 	}
842: 	interrupted = false;
843: 	config.enable_optimizer = true;
844: 
845: 	if (profiling_is_enabled) {
846: 		config.enable_profiler = true;
847: 	}
848: 
849: 	// now compare the results
850: 	// the results of all runs should be identical
851: 	vector<unique_ptr<MaterializedQueryResult>> results;
852: 	results.push_back(move(copied_result));
853: 	results.push_back(move(deserialized_result));
854: 	results.push_back(move(unoptimized_result));
855: 	vector<string> names = {"Copied Result", "Deserialized Result", "Unoptimized Result"};
856: 	for (idx_t i = 0; i < results.size(); i++) {
857: 		if (original_result->success != results[i]->success) { // LCOV_EXCL_START
858: 			string result = names[i] + " differs from original result!\n";
859: 			result += "Original Result:\n" + original_result->ToString();
860: 			result += names[i] + ":\n" + results[i]->ToString();
861: 			return result;
862: 		}                                                                  // LCOV_EXCL_STOP
863: 		if (!original_result->collection.Equals(results[i]->collection)) { // LCOV_EXCL_START
864: 			string result = names[i] + " differs from original result!\n";
865: 			result += "Original Result:\n" + original_result->ToString();
866: 			result += names[i] + ":\n" + results[i]->ToString();
867: 			return result;
868: 		} // LCOV_EXCL_STOP
869: 	}
870: 
871: 	return "";
872: }
873: 
874: bool ClientContext::UpdateFunctionInfoFromEntry(ScalarFunctionCatalogEntry *existing_function,
875:                                                 CreateScalarFunctionInfo *new_info) {
876: 	if (new_info->functions.empty()) {
877: 		throw InternalException("Registering function without scalar function definitions!");
878: 	}
879: 	bool need_rewrite_entry = false;
880: 	idx_t size_new_func = new_info->functions.size();
881: 	for (idx_t exist_idx = 0; exist_idx < existing_function->functions.size(); ++exist_idx) {
882: 		bool can_add = true;
883: 		for (idx_t new_idx = 0; new_idx < size_new_func; ++new_idx) {
884: 			if (new_info->functions[new_idx].Equal(existing_function->functions[exist_idx])) {
885: 				can_add = false;
886: 				break;
887: 			}
888: 		}
889: 		if (can_add) {
890: 			new_info->functions.push_back(existing_function->functions[exist_idx]);
891: 			need_rewrite_entry = true;
892: 		}
893: 	}
894: 	return need_rewrite_entry;
895: }
896: 
897: void ClientContext::RegisterFunction(CreateFunctionInfo *info) {
898: 	RunFunctionInTransaction([&]() {
899: 		auto &catalog = Catalog::GetCatalog(*this);
900: 		auto existing_function = (ScalarFunctionCatalogEntry *)catalog.GetEntry(
901: 		    *this, CatalogType::SCALAR_FUNCTION_ENTRY, info->schema, info->name, true);
902: 		if (existing_function) {
903: 			if (UpdateFunctionInfoFromEntry(existing_function, (CreateScalarFunctionInfo *)info)) {
904: 				// function info was updated from catalog entry, rewrite is needed
905: 				info->on_conflict = OnCreateConflict::REPLACE_ON_CONFLICT;
906: 			}
907: 		}
908: 		// create function
909: 		catalog.CreateFunction(*this, info);
910: 	});
911: }
912: 
913: void ClientContext::RunFunctionInTransactionInternal(ClientContextLock &lock, const std::function<void(void)> &fun,
914:                                                      bool requires_valid_transaction) {
915: 	if (requires_valid_transaction && transaction.HasActiveTransaction() &&
916: 	    transaction.ActiveTransaction().IsInvalidated()) {
917: 		throw Exception("Failed: transaction has been invalidated!");
918: 	}
919: 	// check if we are on AutoCommit. In this case we should start a transaction
920: 	bool require_new_transaction = transaction.IsAutoCommit() && !transaction.HasActiveTransaction();
921: 	if (require_new_transaction) {
922: 		D_ASSERT(!active_query);
923: 		transaction.BeginTransaction();
924: 	}
925: 	try {
926: 		fun();
927: 	} catch (StandardException &ex) {
928: 		if (require_new_transaction) {
929: 			transaction.Rollback();
930: 		}
931: 		throw;
932: 	} catch (std::exception &ex) {
933: 		if (require_new_transaction) {
934: 			transaction.Rollback();
935: 		} else {
936: 			ActiveTransaction().Invalidate();
937: 		}
938: 		throw;
939: 	}
940: 	if (require_new_transaction) {
941: 		transaction.Commit();
942: 	}
943: }
944: 
945: void ClientContext::RunFunctionInTransaction(const std::function<void(void)> &fun, bool requires_valid_transaction) {
946: 	auto lock = LockContext();
947: 	RunFunctionInTransactionInternal(*lock, fun, requires_valid_transaction);
948: }
949: 
950: unique_ptr<TableDescription> ClientContext::TableInfo(const string &schema_name, const string &table_name) {
951: 	unique_ptr<TableDescription> result;
952: 	RunFunctionInTransaction([&]() {
953: 		// obtain the table info
954: 		auto &catalog = Catalog::GetCatalog(*this);
955: 		auto table = catalog.GetEntry<TableCatalogEntry>(*this, schema_name, table_name, true);
956: 		if (!table) {
957: 			return;
958: 		}
959: 		// write the table info to the result
960: 		result = make_unique<TableDescription>();
961: 		result->schema = schema_name;
962: 		result->table = table_name;
963: 		for (auto &column : table->columns) {
964: 			result->columns.emplace_back(column.name, column.type);
965: 		}
966: 	});
967: 	return result;
968: }
969: 
970: void ClientContext::Append(TableDescription &description, ChunkCollection &collection) {
971: 	RunFunctionInTransaction([&]() {
972: 		auto &catalog = Catalog::GetCatalog(*this);
973: 		auto table_entry = catalog.GetEntry<TableCatalogEntry>(*this, description.schema, description.table);
974: 		// verify that the table columns and types match up
975: 		if (description.columns.size() != table_entry->columns.size()) {
976: 			throw Exception("Failed to append: table entry has different number of columns!");
977: 		}
978: 		for (idx_t i = 0; i < description.columns.size(); i++) {
979: 			if (description.columns[i].type != table_entry->columns[i].type) {
980: 				throw Exception("Failed to append: table entry has different number of columns!");
981: 			}
982: 		}
983: 		for (auto &chunk : collection.Chunks()) {
984: 			table_entry->storage->Append(*table_entry, *this, *chunk);
985: 		}
986: 	});
987: }
988: 
989: void ClientContext::TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns) {
990: #ifdef DEBUG
991: 	D_ASSERT(!relation.GetAlias().empty());
992: 	D_ASSERT(!relation.ToString().empty());
993: #endif
994: 	RunFunctionInTransaction([&]() {
995: 		// bind the expressions
996: 		auto binder = Binder::CreateBinder(*this);
997: 		auto result = relation.Bind(*binder);
998: 		D_ASSERT(result.names.size() == result.types.size());
999: 		for (idx_t i = 0; i < result.names.size(); i++) {
1000: 			result_columns.emplace_back(result.names[i], result.types[i]);
1001: 		}
1002: 	});
1003: }
1004: 
1005: unordered_set<string> ClientContext::GetTableNames(const string &query) {
1006: 	auto lock = LockContext();
1007: 
1008: 	auto statements = ParseStatementsInternal(*lock, query);
1009: 	if (statements.size() != 1) {
1010: 		throw InvalidInputException("Expected a single statement");
1011: 	}
1012: 
1013: 	unordered_set<string> result;
1014: 	RunFunctionInTransactionInternal(*lock, [&]() {
1015: 		// bind the expressions
1016: 		auto binder = Binder::CreateBinder(*this);
1017: 		binder->SetBindingMode(BindingMode::EXTRACT_NAMES);
1018: 		binder->Bind(*statements[0]);
1019: 		result = binder->GetTableNames();
1020: 	});
1021: 	return result;
1022: }
1023: 
1024: unique_ptr<QueryResult> ClientContext::Execute(const shared_ptr<Relation> &relation) {
1025: 	auto lock = LockContext();
1026: 	InitialCleanup(*lock);
1027: 
1028: 	string query;
1029: 	if (config.query_verification_enabled) {
1030: 		// run the ToString method of any relation we run, mostly to ensure it doesn't crash
1031: 		relation->ToString();
1032: 		relation->GetAlias();
1033: 		if (relation->IsReadOnly()) {
1034: 			// verify read only statements by running a select statement
1035: 			auto select = make_unique<SelectStatement>();
1036: 			select->node = relation->GetQueryNode();
1037: 			RunStatementInternal(*lock, query, move(select), false);
1038: 		}
1039: 	}
1040: 	auto &expected_columns = relation->Columns();
1041: 	auto relation_stmt = make_unique<RelationStatement>(relation);
1042: 
1043: 	unique_ptr<QueryResult> result;
1044: 	result = RunStatementInternal(*lock, query, move(relation_stmt), false);
1045: 	if (!result->success) {
1046: 		return result;
1047: 	}
1048: 	// verify that the result types and result names of the query match the expected result types/names
1049: 	if (result->types.size() == expected_columns.size()) {
1050: 		bool mismatch = false;
1051: 		for (idx_t i = 0; i < result->types.size(); i++) {
1052: 			if (result->types[i] != expected_columns[i].type || result->names[i] != expected_columns[i].name) {
1053: 				mismatch = true;
1054: 				break;
1055: 			}
1056: 		}
1057: 		if (!mismatch) {
1058: 			// all is as expected: return the result
1059: 			return result;
1060: 		}
1061: 	}
1062: 	// result mismatch
1063: 	string err_str = "Result mismatch in query!\nExpected the following columns: [";
1064: 	for (idx_t i = 0; i < expected_columns.size(); i++) {
1065: 		if (i > 0) {
1066: 			err_str += ", ";
1067: 		}
1068: 		err_str += expected_columns[i].name + " " + expected_columns[i].type.ToString();
1069: 	}
1070: 	err_str += "]\nBut result contained the following: ";
1071: 	for (idx_t i = 0; i < result->types.size(); i++) {
1072: 		err_str += i == 0 ? "[" : ", ";
1073: 		err_str += result->names[i] + " " + result->types[i].ToString();
1074: 	}
1075: 	err_str += "]";
1076: 	return make_unique<MaterializedQueryResult>(err_str);
1077: }
1078: 
1079: bool ClientContext::TryGetCurrentSetting(const std::string &key, Value &result) {
1080: 	// first check the built-in settings
1081: 	auto &db_config = DBConfig::GetConfig(*this);
1082: 	auto option = db_config.GetOptionByName(key);
1083: 	if (option) {
1084: 		result = option->get_setting(*this);
1085: 		return true;
1086: 	}
1087: 
1088: 	// then check the session values
1089: 	const auto &session_config_map = config.set_variables;
1090: 	const auto &global_config_map = db_config.set_variables;
1091: 
1092: 	auto session_value = session_config_map.find(key);
1093: 	bool found_session_value = session_value != session_config_map.end();
1094: 	auto global_value = global_config_map.find(key);
1095: 	bool found_global_value = global_value != global_config_map.end();
1096: 	if (!found_session_value && !found_global_value) {
1097: 		return false;
1098: 	}
1099: 
1100: 	result = found_session_value ? session_value->second : global_value->second;
1101: 	return true;
1102: }
1103: 
1104: } // namespace duckdb
[end of src/main/client_context.cpp]
[start of src/main/config.cpp]
1: #include "duckdb/main/config.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/common/operator/cast_operators.hpp"
4: #include "duckdb/main/settings.hpp"
5: 
6: namespace duckdb {
7: 
8: #define DUCKDB_GLOBAL(_PARAM)                                                                                          \
9: 	{ _PARAM::Name, _PARAM::Description, _PARAM::InputType, _PARAM::SetGlobal, nullptr, _PARAM::GetSetting }
10: #define DUCKDB_GLOBAL_ALIAS(_ALIAS, _PARAM)                                                                            \
11: 	{ _ALIAS, _PARAM::Description, _PARAM::InputType, _PARAM::SetGlobal, nullptr, _PARAM::GetSetting }
12: 
13: #define DUCKDB_LOCAL(_PARAM)                                                                                           \
14: 	{ _PARAM::Name, _PARAM::Description, _PARAM::InputType, nullptr, _PARAM::SetLocal, _PARAM::GetSetting }
15: #define DUCKDB_LOCAL_ALIAS(_ALIAS, _PARAM)                                                                             \
16: 	{ _ALIAS, _PARAM::Description, _PARAM::InputType, nullptr, _PARAM::SetLocal, _PARAM::GetSetting }
17: 
18: #define DUCKDB_GLOBAL_LOCAL(_PARAM)                                                                                    \
19: 	{ _PARAM::Name, _PARAM::Description, _PARAM::InputType, _PARAM::SetGlobal, _PARAM::SetLocal, _PARAM::GetSetting }
20: #define DUCKDB_GLOBAL_LOCAL_ALIAS(_ALIAS, _PARAM)                                                                      \
21: 	{ _ALIAS, _PARAM::Description, _PARAM::InputType, _PARAM::SetGlobal, _PARAM::SetLocal, _PARAM::GetSetting }
22: #define FINAL_SETTING                                                                                                  \
23: 	{ nullptr, nullptr, LogicalTypeId::INVALID, nullptr, nullptr, nullptr }
24: 
25: static ConfigurationOption internal_options[] = {DUCKDB_GLOBAL(AccessModeSetting),
26:                                                  DUCKDB_GLOBAL(CheckpointThresholdSetting),
27:                                                  DUCKDB_GLOBAL(DebugCheckpointAbort),
28:                                                  DUCKDB_LOCAL(DebugForceExternal),
29:                                                  DUCKDB_GLOBAL(DebugManyFreeListBlocks),
30:                                                  DUCKDB_GLOBAL(DebugWindowMode),
31:                                                  DUCKDB_GLOBAL_LOCAL(DefaultCollationSetting),
32:                                                  DUCKDB_GLOBAL(DefaultOrderSetting),
33:                                                  DUCKDB_GLOBAL(DefaultNullOrderSetting),
34:                                                  DUCKDB_GLOBAL(DisabledOptimizersSetting),
35:                                                  DUCKDB_GLOBAL(EnableExternalAccessSetting),
36:                                                  DUCKDB_GLOBAL(EnableObjectCacheSetting),
37:                                                  DUCKDB_LOCAL(EnableProfilingSetting),
38:                                                  DUCKDB_LOCAL(EnableProgressBarSetting),
39:                                                  DUCKDB_LOCAL(ExplainOutputSetting),
40:                                                  DUCKDB_GLOBAL(ForceCompressionSetting),
41:                                                  DUCKDB_LOCAL(LogQueryPathSetting),
42:                                                  DUCKDB_GLOBAL(MaximumMemorySetting),
43:                                                  DUCKDB_GLOBAL_ALIAS("memory_limit", MaximumMemorySetting),
44:                                                  DUCKDB_GLOBAL_ALIAS("null_order", DefaultNullOrderSetting),
45:                                                  DUCKDB_LOCAL(PerfectHashThresholdSetting),
46:                                                  DUCKDB_LOCAL(ProfilerHistorySize),
47:                                                  DUCKDB_LOCAL(ProfileOutputSetting),
48:                                                  DUCKDB_LOCAL(ProfilingModeSetting),
49:                                                  DUCKDB_LOCAL_ALIAS("profiling_output", ProfileOutputSetting),
50:                                                  DUCKDB_LOCAL(ProgressBarTimeSetting),
51:                                                  DUCKDB_LOCAL(SchemaSetting),
52:                                                  DUCKDB_LOCAL(SearchPathSetting),
53:                                                  DUCKDB_GLOBAL(TempDirectorySetting),
54:                                                  DUCKDB_GLOBAL(ThreadsSetting),
55:                                                  DUCKDB_GLOBAL_ALIAS("wal_autocheckpoint", CheckpointThresholdSetting),
56:                                                  DUCKDB_GLOBAL_ALIAS("worker_threads", ThreadsSetting),
57:                                                  FINAL_SETTING};
58: 
59: vector<ConfigurationOption> DBConfig::GetOptions() {
60: 	vector<ConfigurationOption> options;
61: 	for (idx_t index = 0; internal_options[index].name; index++) {
62: 		options.push_back(internal_options[index]);
63: 	}
64: 	return options;
65: }
66: 
67: idx_t DBConfig::GetOptionCount() {
68: 	idx_t count = 0;
69: 	for (idx_t index = 0; internal_options[index].name; index++) {
70: 		count++;
71: 	}
72: 	return count;
73: }
74: 
75: ConfigurationOption *DBConfig::GetOptionByIndex(idx_t target_index) {
76: 	for (idx_t index = 0; internal_options[index].name; index++) {
77: 		if (index == target_index) {
78: 			return internal_options + index;
79: 		}
80: 	}
81: 	return nullptr;
82: }
83: 
84: ConfigurationOption *DBConfig::GetOptionByName(const string &name) {
85: 	for (idx_t index = 0; internal_options[index].name; index++) {
86: 		if (internal_options[index].name == name) {
87: 			return internal_options + index;
88: 		}
89: 	}
90: 	return nullptr;
91: }
92: 
93: void DBConfig::SetOption(const ConfigurationOption &option, const Value &value) {
94: 	if (!option.set_global) {
95: 		throw InternalException("Could not set option \"%s\" as a global option", option.name);
96: 	}
97: 	Value input = value.CastAs(option.parameter_type);
98: 	option.set_global(nullptr, *this, input);
99: }
100: 
101: void DBConfig::AddExtensionOption(string name, string description, LogicalType parameter,
102:                                   set_option_callback_t function) {
103: 	extension_parameters.insert(make_pair(move(name), ExtensionOption(move(description), move(parameter), function)));
104: }
105: 
106: idx_t DBConfig::ParseMemoryLimit(const string &arg) {
107: 	if (arg[0] == '-' || arg == "null" || arg == "none") {
108: 		return DConstants::INVALID_INDEX;
109: 	}
110: 	// split based on the number/non-number
111: 	idx_t idx = 0;
112: 	while (StringUtil::CharacterIsSpace(arg[idx])) {
113: 		idx++;
114: 	}
115: 	idx_t num_start = idx;
116: 	while ((arg[idx] >= '0' && arg[idx] <= '9') || arg[idx] == '.' || arg[idx] == 'e' || arg[idx] == 'E' ||
117: 	       arg[idx] == '-') {
118: 		idx++;
119: 	}
120: 	if (idx == num_start) {
121: 		throw ParserException("Memory limit must have a number (e.g. SET memory_limit=1GB");
122: 	}
123: 	string number = arg.substr(num_start, idx - num_start);
124: 
125: 	// try to parse the number
126: 	double limit = Cast::Operation<string_t, double>(string_t(number));
127: 
128: 	// now parse the memory limit unit (e.g. bytes, gb, etc)
129: 	while (StringUtil::CharacterIsSpace(arg[idx])) {
130: 		idx++;
131: 	}
132: 	idx_t start = idx;
133: 	while (idx < arg.size() && !StringUtil::CharacterIsSpace(arg[idx])) {
134: 		idx++;
135: 	}
136: 	if (limit < 0) {
137: 		// limit < 0, set limit to infinite
138: 		return (idx_t)-1;
139: 	}
140: 	string unit = StringUtil::Lower(arg.substr(start, idx - start));
141: 	idx_t multiplier;
142: 	if (unit == "byte" || unit == "bytes" || unit == "b") {
143: 		multiplier = 1;
144: 	} else if (unit == "kilobyte" || unit == "kilobytes" || unit == "kb" || unit == "k") {
145: 		multiplier = 1000LL;
146: 	} else if (unit == "megabyte" || unit == "megabytes" || unit == "mb" || unit == "m") {
147: 		multiplier = 1000LL * 1000LL;
148: 	} else if (unit == "gigabyte" || unit == "gigabytes" || unit == "gb" || unit == "g") {
149: 		multiplier = 1000LL * 1000LL * 1000LL;
150: 	} else if (unit == "terabyte" || unit == "terabytes" || unit == "tb" || unit == "t") {
151: 		multiplier = 1000LL * 1000LL * 1000LL * 1000LL;
152: 	} else {
153: 		throw ParserException("Unknown unit for memory_limit: %s (expected: b, mb, gb or tb)", unit);
154: 	}
155: 	return (idx_t)multiplier * limit;
156: }
157: 
158: } // namespace duckdb
[end of src/main/config.cpp]
[start of src/main/connection.cpp]
1: #include "duckdb/main/connection.hpp"
2: #include "duckdb/main/query_profiler.hpp"
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/main/database.hpp"
5: #include "duckdb/main/appender.hpp"
6: #include "duckdb/main/relation/query_relation.hpp"
7: #include "duckdb/main/relation/read_csv_relation.hpp"
8: #include "duckdb/main/relation/table_relation.hpp"
9: #include "duckdb/main/relation/table_function_relation.hpp"
10: #include "duckdb/main/relation/value_relation.hpp"
11: #include "duckdb/main/relation/view_relation.hpp"
12: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
13: #include "duckdb/parser/parser.hpp"
14: #include "duckdb/main/connection_manager.hpp"
15: #include "duckdb/planner/logical_operator.hpp"
16: 
17: namespace duckdb {
18: 
19: Connection::Connection(DatabaseInstance &database) : context(make_shared<ClientContext>(database.shared_from_this())) {
20: 	ConnectionManager::Get(database).AddConnection(*context);
21: #ifdef DEBUG
22: 	EnableProfiling();
23: #endif
24: }
25: 
26: Connection::Connection(DuckDB &database) : Connection(*database.instance) {
27: }
28: 
29: string Connection::GetProfilingInformation(ProfilerPrintFormat format) {
30: 	auto &profiler = QueryProfiler::Get(*context);
31: 	if (format == ProfilerPrintFormat::JSON) {
32: 		return profiler.ToJSON();
33: 	} else {
34: 		return profiler.ToString();
35: 	}
36: }
37: 
38: void Connection::Interrupt() {
39: 	context->Interrupt();
40: }
41: 
42: void Connection::EnableProfiling() {
43: 	context->EnableProfiling();
44: }
45: 
46: void Connection::DisableProfiling() {
47: 	context->DisableProfiling();
48: }
49: 
50: void Connection::EnableQueryVerification() {
51: 	ClientConfig::GetConfig(*context).query_verification_enabled = true;
52: }
53: 
54: void Connection::DisableQueryVerification() {
55: 	ClientConfig::GetConfig(*context).query_verification_enabled = false;
56: }
57: 
58: void Connection::ForceParallelism() {
59: 	ClientConfig::GetConfig(*context).verify_parallelism = true;
60: }
61: 
62: unique_ptr<QueryResult> Connection::SendQuery(const string &query) {
63: 	return context->Query(query, true);
64: }
65: 
66: unique_ptr<MaterializedQueryResult> Connection::Query(const string &query) {
67: 	auto result = context->Query(query, false);
68: 	D_ASSERT(result->type == QueryResultType::MATERIALIZED_RESULT);
69: 	return unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
70: }
71: 
72: unique_ptr<MaterializedQueryResult> Connection::Query(unique_ptr<SQLStatement> statement) {
73: 	auto result = context->Query(move(statement), false);
74: 	D_ASSERT(result->type == QueryResultType::MATERIALIZED_RESULT);
75: 	return unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
76: }
77: 
78: unique_ptr<PendingQueryResult> Connection::PendingQuery(const string &query) {
79: 	return context->PendingQuery(query);
80: }
81: 
82: unique_ptr<PendingQueryResult> Connection::PendingQuery(unique_ptr<SQLStatement> statement) {
83: 	return context->PendingQuery(move(statement));
84: }
85: 
86: unique_ptr<PreparedStatement> Connection::Prepare(const string &query) {
87: 	return context->Prepare(query);
88: }
89: 
90: unique_ptr<PreparedStatement> Connection::Prepare(unique_ptr<SQLStatement> statement) {
91: 	return context->Prepare(move(statement));
92: }
93: 
94: unique_ptr<QueryResult> Connection::QueryParamsRecursive(const string &query, vector<Value> &values) {
95: 	auto statement = Prepare(query);
96: 	if (!statement->success) {
97: 		return make_unique<MaterializedQueryResult>(statement->error);
98: 	}
99: 	return statement->Execute(values, false);
100: }
101: 
102: unique_ptr<TableDescription> Connection::TableInfo(const string &table_name) {
103: 	return TableInfo(DEFAULT_SCHEMA, table_name);
104: }
105: 
106: unique_ptr<TableDescription> Connection::TableInfo(const string &schema_name, const string &table_name) {
107: 	return context->TableInfo(schema_name, table_name);
108: }
109: 
110: vector<unique_ptr<SQLStatement>> Connection::ExtractStatements(const string &query) {
111: 	return context->ParseStatements(query);
112: }
113: 
114: unique_ptr<LogicalOperator> Connection::ExtractPlan(const string &query) {
115: 	return context->ExtractPlan(query);
116: }
117: 
118: void Connection::Append(TableDescription &description, DataChunk &chunk) {
119: 	ChunkCollection collection;
120: 	collection.Append(chunk);
121: 	Append(description, collection);
122: }
123: 
124: void Connection::Append(TableDescription &description, ChunkCollection &collection) {
125: 	context->Append(description, collection);
126: }
127: 
128: shared_ptr<Relation> Connection::Table(const string &table_name) {
129: 	return Table(DEFAULT_SCHEMA, table_name);
130: }
131: 
132: shared_ptr<Relation> Connection::Table(const string &schema_name, const string &table_name) {
133: 	auto table_info = TableInfo(schema_name, table_name);
134: 	if (!table_info) {
135: 		throw Exception("Table does not exist!");
136: 	}
137: 	return make_shared<TableRelation>(*context, move(table_info));
138: }
139: 
140: shared_ptr<Relation> Connection::View(const string &tname) {
141: 	return View(DEFAULT_SCHEMA, tname);
142: }
143: 
144: shared_ptr<Relation> Connection::View(const string &schema_name, const string &table_name) {
145: 	return make_shared<ViewRelation>(*context, schema_name, table_name);
146: }
147: 
148: shared_ptr<Relation> Connection::TableFunction(const string &fname) {
149: 	vector<Value> values;
150: 	unordered_map<string, Value> named_parameters;
151: 	return TableFunction(fname, values, named_parameters);
152: }
153: 
154: shared_ptr<Relation> Connection::TableFunction(const string &fname, const vector<Value> &values,
155:                                                const unordered_map<string, Value> &named_parameters) {
156: 	return make_shared<TableFunctionRelation>(*context, fname, values, named_parameters);
157: }
158: 
159: shared_ptr<Relation> Connection::TableFunction(const string &fname, const vector<Value> &values) {
160: 	return make_shared<TableFunctionRelation>(*context, fname, values);
161: }
162: 
163: shared_ptr<Relation> Connection::Values(const vector<vector<Value>> &values) {
164: 	vector<string> column_names;
165: 	return Values(values, column_names);
166: }
167: 
168: shared_ptr<Relation> Connection::Values(const vector<vector<Value>> &values, const vector<string> &column_names,
169:                                         const string &alias) {
170: 	return make_shared<ValueRelation>(*context, values, column_names, alias);
171: }
172: 
173: shared_ptr<Relation> Connection::Values(const string &values) {
174: 	vector<string> column_names;
175: 	return Values(values, column_names);
176: }
177: 
178: shared_ptr<Relation> Connection::Values(const string &values, const vector<string> &column_names, const string &alias) {
179: 	return make_shared<ValueRelation>(*context, values, column_names, alias);
180: }
181: 
182: shared_ptr<Relation> Connection::ReadCSV(const string &csv_file) {
183: 	BufferedCSVReaderOptions options;
184: 	options.file_path = csv_file;
185: 	options.auto_detect = true;
186: 	BufferedCSVReader reader(*context, options);
187: 	vector<ColumnDefinition> column_list;
188: 	for (idx_t i = 0; i < reader.sql_types.size(); i++) {
189: 		column_list.emplace_back(reader.col_names[i], reader.sql_types[i]);
190: 	}
191: 	return make_shared<ReadCSVRelation>(*context, csv_file, move(column_list), true);
192: }
193: 
194: shared_ptr<Relation> Connection::ReadCSV(const string &csv_file, const vector<string> &columns) {
195: 	// parse columns
196: 	vector<ColumnDefinition> column_list;
197: 	for (auto &column : columns) {
198: 		auto col_list = Parser::ParseColumnList(column);
199: 		if (col_list.size() != 1) {
200: 			throw ParserException("Expected a single column definition");
201: 		}
202: 		column_list.push_back(move(col_list[0]));
203: 	}
204: 	return make_shared<ReadCSVRelation>(*context, csv_file, move(column_list));
205: }
206: 
207: unordered_set<string> Connection::GetTableNames(const string &query) {
208: 	return context->GetTableNames(query);
209: }
210: 
211: shared_ptr<Relation> Connection::RelationFromQuery(const string &query, const string &alias) {
212: 	return make_shared<QueryRelation>(*context, query, alias);
213: }
214: 
215: void Connection::BeginTransaction() {
216: 	auto result = Query("BEGIN TRANSACTION");
217: 	if (!result->success) {
218: 		throw Exception(result->error);
219: 	}
220: }
221: 
222: void Connection::Commit() {
223: 	auto result = Query("COMMIT");
224: 	if (!result->success) {
225: 		throw Exception(result->error);
226: 	}
227: }
228: 
229: void Connection::Rollback() {
230: 	auto result = Query("ROLLBACK");
231: 	if (!result->success) {
232: 		throw Exception(result->error);
233: 	}
234: }
235: 
236: void Connection::SetAutoCommit(bool auto_commit) {
237: 	context->transaction.SetAutoCommit(auto_commit);
238: }
239: 
240: bool Connection::IsAutoCommit() {
241: 	return context->transaction.IsAutoCommit();
242: }
243: 
244: } // namespace duckdb
[end of src/main/connection.cpp]
[start of src/main/relation.cpp]
1: #include "duckdb/main/relation.hpp"
2: #include "duckdb/common/printer.hpp"
3: #include "duckdb/parser/parser.hpp"
4: #include "duckdb/main/relation/aggregate_relation.hpp"
5: #include "duckdb/main/relation/distinct_relation.hpp"
6: #include "duckdb/main/relation/explain_relation.hpp"
7: #include "duckdb/main/relation/filter_relation.hpp"
8: #include "duckdb/main/relation/insert_relation.hpp"
9: #include "duckdb/main/relation/limit_relation.hpp"
10: #include "duckdb/main/relation/order_relation.hpp"
11: #include "duckdb/main/relation/projection_relation.hpp"
12: #include "duckdb/main/relation/setop_relation.hpp"
13: #include "duckdb/main/relation/subquery_relation.hpp"
14: #include "duckdb/main/relation/table_function_relation.hpp"
15: #include "duckdb/main/relation/create_table_relation.hpp"
16: #include "duckdb/main/relation/create_view_relation.hpp"
17: #include "duckdb/main/relation/write_csv_relation.hpp"
18: #include "duckdb/main/client_context.hpp"
19: #include "duckdb/planner/binder.hpp"
20: #include "duckdb/parser/tableref/subqueryref.hpp"
21: #include "duckdb/parser/statement/select_statement.hpp"
22: #include "duckdb/parser/expression/conjunction_expression.hpp"
23: #include "duckdb/parser/expression/columnref_expression.hpp"
24: #include "duckdb/main/relation/join_relation.hpp"
25: #include "duckdb/main/relation/value_relation.hpp"
26: 
27: namespace duckdb {
28: 
29: shared_ptr<Relation> Relation::Project(const string &select_list) {
30: 	return Project(select_list, vector<string>());
31: }
32: 
33: shared_ptr<Relation> Relation::Project(const string &expression, const string &alias) {
34: 	return Project(expression, vector<string>({alias}));
35: }
36: 
37: shared_ptr<Relation> Relation::Project(const string &select_list, const vector<string> &aliases) {
38: 	auto expressions = Parser::ParseExpressionList(select_list);
39: 	return make_shared<ProjectionRelation>(shared_from_this(), move(expressions), aliases);
40: }
41: 
42: shared_ptr<Relation> Relation::Project(const vector<string> &expressions) {
43: 	vector<string> aliases;
44: 	return Project(expressions, aliases);
45: }
46: 
47: static vector<unique_ptr<ParsedExpression>> StringListToExpressionList(const vector<string> &expressions) {
48: 	if (expressions.empty()) {
49: 		throw ParserException("Zero expressions provided");
50: 	}
51: 	vector<unique_ptr<ParsedExpression>> result_list;
52: 	for (auto &expr : expressions) {
53: 		auto expression_list = Parser::ParseExpressionList(expr);
54: 		if (expression_list.size() != 1) {
55: 			throw ParserException("Expected a single expression in the expression list");
56: 		}
57: 		result_list.push_back(move(expression_list[0]));
58: 	}
59: 	return result_list;
60: }
61: 
62: shared_ptr<Relation> Relation::Project(const vector<string> &expressions, const vector<string> &aliases) {
63: 	auto result_list = StringListToExpressionList(expressions);
64: 	return make_shared<ProjectionRelation>(shared_from_this(), move(result_list), aliases);
65: }
66: 
67: shared_ptr<Relation> Relation::Filter(const string &expression) {
68: 	auto expression_list = Parser::ParseExpressionList(expression);
69: 	if (expression_list.size() != 1) {
70: 		throw ParserException("Expected a single expression as filter condition");
71: 	}
72: 	return make_shared<FilterRelation>(shared_from_this(), move(expression_list[0]));
73: }
74: 
75: shared_ptr<Relation> Relation::Filter(const vector<string> &expressions) {
76: 	// if there are multiple expressions, we AND them together
77: 	auto expression_list = StringListToExpressionList(expressions);
78: 	D_ASSERT(!expression_list.empty());
79: 
80: 	auto expr = move(expression_list[0]);
81: 	for (idx_t i = 1; i < expression_list.size(); i++) {
82: 		expr =
83: 		    make_unique<ConjunctionExpression>(ExpressionType::CONJUNCTION_AND, move(expr), move(expression_list[i]));
84: 	}
85: 	return make_shared<FilterRelation>(shared_from_this(), move(expr));
86: }
87: 
88: shared_ptr<Relation> Relation::Limit(int64_t limit, int64_t offset) {
89: 	return make_shared<LimitRelation>(shared_from_this(), limit, offset);
90: }
91: 
92: shared_ptr<Relation> Relation::Order(const string &expression) {
93: 	auto order_list = Parser::ParseOrderList(expression);
94: 	return make_shared<OrderRelation>(shared_from_this(), move(order_list));
95: }
96: 
97: shared_ptr<Relation> Relation::Order(const vector<string> &expressions) {
98: 	if (expressions.empty()) {
99: 		throw ParserException("Zero ORDER BY expressions provided");
100: 	}
101: 	vector<OrderByNode> order_list;
102: 	for (auto &expression : expressions) {
103: 		auto inner_list = Parser::ParseOrderList(expression);
104: 		if (inner_list.size() != 1) {
105: 			throw ParserException("Expected a single ORDER BY expression in the expression list");
106: 		}
107: 		order_list.push_back(move(inner_list[0]));
108: 	}
109: 	return make_shared<OrderRelation>(shared_from_this(), move(order_list));
110: }
111: 
112: shared_ptr<Relation> Relation::Join(const shared_ptr<Relation> &other, const string &condition, JoinType type) {
113: 	auto expression_list = Parser::ParseExpressionList(condition);
114: 	D_ASSERT(!expression_list.empty());
115: 
116: 	if (expression_list.size() > 1 || expression_list[0]->type == ExpressionType::COLUMN_REF) {
117: 		// multiple columns or single column ref: the condition is a USING list
118: 		vector<string> using_columns;
119: 		for (auto &expr : expression_list) {
120: 			if (expr->type != ExpressionType::COLUMN_REF) {
121: 				throw ParserException("Expected a single expression as join condition");
122: 			}
123: 			auto &colref = (ColumnRefExpression &)*expr;
124: 			if (colref.IsQualified()) {
125: 				throw ParserException("Expected unqualified column for column in USING clause");
126: 			}
127: 			using_columns.push_back(colref.column_names[0]);
128: 		}
129: 		return make_shared<JoinRelation>(shared_from_this(), other, move(using_columns), type);
130: 	} else {
131: 		// single expression that is not a column reference: use the expression as a join condition
132: 		return make_shared<JoinRelation>(shared_from_this(), other, move(expression_list[0]), type);
133: 	}
134: }
135: 
136: shared_ptr<Relation> Relation::Union(const shared_ptr<Relation> &other) {
137: 	return make_shared<SetOpRelation>(shared_from_this(), other, SetOperationType::UNION);
138: }
139: 
140: shared_ptr<Relation> Relation::Except(const shared_ptr<Relation> &other) {
141: 	return make_shared<SetOpRelation>(shared_from_this(), other, SetOperationType::EXCEPT);
142: }
143: 
144: shared_ptr<Relation> Relation::Intersect(const shared_ptr<Relation> &other) {
145: 	return make_shared<SetOpRelation>(shared_from_this(), other, SetOperationType::INTERSECT);
146: }
147: 
148: shared_ptr<Relation> Relation::Distinct() {
149: 	return make_shared<DistinctRelation>(shared_from_this());
150: }
151: 
152: shared_ptr<Relation> Relation::Alias(const string &alias) {
153: 	return make_shared<SubqueryRelation>(shared_from_this(), alias);
154: }
155: 
156: shared_ptr<Relation> Relation::Aggregate(const string &aggregate_list) {
157: 	auto expression_list = Parser::ParseExpressionList(aggregate_list);
158: 	return make_shared<AggregateRelation>(shared_from_this(), move(expression_list));
159: }
160: 
161: shared_ptr<Relation> Relation::Aggregate(const string &aggregate_list, const string &group_list) {
162: 	auto expression_list = Parser::ParseExpressionList(aggregate_list);
163: 	auto groups = Parser::ParseExpressionList(group_list);
164: 	return make_shared<AggregateRelation>(shared_from_this(), move(expression_list), move(groups));
165: }
166: 
167: shared_ptr<Relation> Relation::Aggregate(const vector<string> &aggregates) {
168: 	auto aggregate_list = StringListToExpressionList(aggregates);
169: 	return make_shared<AggregateRelation>(shared_from_this(), move(aggregate_list));
170: }
171: 
172: shared_ptr<Relation> Relation::Aggregate(const vector<string> &aggregates, const vector<string> &groups) {
173: 	auto aggregate_list = StringListToExpressionList(aggregates);
174: 	auto group_list = StringListToExpressionList(groups);
175: 	return make_shared<AggregateRelation>(shared_from_this(), move(aggregate_list), move(group_list));
176: }
177: 
178: string Relation::GetAlias() {
179: 	return "relation";
180: }
181: 
182: unique_ptr<TableRef> Relation::GetTableRef() {
183: 	auto select = make_unique<SelectStatement>();
184: 	select->node = GetQueryNode();
185: 	return make_unique<SubqueryRef>(move(select), GetAlias());
186: }
187: 
188: unique_ptr<QueryResult> Relation::Execute() {
189: 	return context.Execute(shared_from_this());
190: }
191: 
192: BoundStatement Relation::Bind(Binder &binder) {
193: 	SelectStatement stmt;
194: 	stmt.node = GetQueryNode();
195: 	return binder.Bind((SQLStatement &)stmt);
196: }
197: 
198: void Relation::Insert(const string &table_name) {
199: 	Insert(DEFAULT_SCHEMA, table_name);
200: }
201: 
202: void Relation::Insert(const string &schema_name, const string &table_name) {
203: 	auto insert = make_shared<InsertRelation>(shared_from_this(), schema_name, table_name);
204: 	auto res = insert->Execute();
205: 	if (!res->success) {
206: 		throw Exception("Failed to insert into table '" + table_name + "': " + res->error);
207: 	}
208: }
209: 
210: void Relation::Insert(const vector<vector<Value>> &values) {
211: 	vector<string> column_names;
212: 	auto rel = make_shared<ValueRelation>(context, values, move(column_names), "values");
213: 	rel->Insert(GetAlias());
214: }
215: 
216: void Relation::Create(const string &table_name) {
217: 	Create(DEFAULT_SCHEMA, table_name);
218: }
219: 
220: void Relation::Create(const string &schema_name, const string &table_name) {
221: 	auto create = make_shared<CreateTableRelation>(shared_from_this(), schema_name, table_name);
222: 	auto res = create->Execute();
223: 	if (!res->success) {
224: 		throw Exception("Failed to create table '" + table_name + "': " + res->error);
225: 	}
226: }
227: 
228: void Relation::WriteCSV(const string &csv_file) {
229: 	auto write_csv = make_shared<WriteCSVRelation>(shared_from_this(), csv_file);
230: 	auto res = write_csv->Execute();
231: 	if (!res->success) {
232: 		throw Exception("Failed to write '" + csv_file + "': " + res->error);
233: 	}
234: }
235: 
236: shared_ptr<Relation> Relation::CreateView(const string &name, bool replace, bool temporary) {
237: 	auto view = make_shared<CreateViewRelation>(shared_from_this(), name, replace, temporary);
238: 	auto res = view->Execute();
239: 	if (!res->success) {
240: 		throw Exception("Failed to create view '" + name + "': " + res->error);
241: 	}
242: 	return shared_from_this();
243: }
244: 
245: unique_ptr<QueryResult> Relation::Query(const string &sql) {
246: 	return context.Query(sql, false);
247: }
248: 
249: unique_ptr<QueryResult> Relation::Query(const string &name, const string &sql) {
250: 	CreateView(name);
251: 	return Query(sql);
252: }
253: 
254: unique_ptr<QueryResult> Relation::Explain() {
255: 	auto explain = make_shared<ExplainRelation>(shared_from_this());
256: 	return explain->Execute();
257: }
258: 
259: void Relation::Update(const string &update, const string &condition) {
260: 	throw Exception("UPDATE can only be used on base tables!");
261: }
262: 
263: void Relation::Delete(const string &condition) {
264: 	throw Exception("DELETE can only be used on base tables!");
265: }
266: 
267: shared_ptr<Relation> Relation::TableFunction(const std::string &fname, const vector<Value> &values,
268:                                              const unordered_map<string, Value> &named_parameters) {
269: 	return make_shared<TableFunctionRelation>(context, fname, values, named_parameters, shared_from_this());
270: }
271: 
272: shared_ptr<Relation> Relation::TableFunction(const std::string &fname, const vector<Value> &values) {
273: 	return make_shared<TableFunctionRelation>(context, fname, values, shared_from_this());
274: }
275: 
276: string Relation::ToString() {
277: 	string str;
278: 	str += "---------------------\n";
279: 	str += "-- Expression Tree --\n";
280: 	str += "---------------------\n";
281: 	str += ToString(0);
282: 	str += "\n\n";
283: 	str += "---------------------\n";
284: 	str += "-- Result Columns  --\n";
285: 	str += "---------------------\n";
286: 	auto &cols = Columns();
287: 	for (idx_t i = 0; i < cols.size(); i++) {
288: 		str += "- " + cols[i].name + " (" + cols[i].type.ToString() + ")\n";
289: 	}
290: 	return str;
291: }
292: 
293: // LCOV_EXCL_START
294: unique_ptr<QueryNode> Relation::GetQueryNode() {
295: 	throw InternalException("Cannot create a query node from this node type");
296: }
297: 
298: void Relation::Head(idx_t limit) {
299: 	auto limit_node = Limit(limit);
300: 	limit_node->Execute()->Print();
301: }
302: // LCOV_EXCL_STOP
303: 
304: void Relation::Print() {
305: 	Printer::Print(ToString());
306: }
307: 
308: string Relation::RenderWhitespace(idx_t depth) {
309: 	return string(depth * 2, ' ');
310: }
311: 
312: } // namespace duckdb
[end of src/main/relation.cpp]
[start of src/main/relation/query_relation.cpp]
1: #include "duckdb/main/relation/query_relation.hpp"
2: #include "duckdb/main/client_context.hpp"
3: #include "duckdb/parser/parser.hpp"
4: #include "duckdb/parser/statement/select_statement.hpp"
5: #include "duckdb/parser/tableref/subqueryref.hpp"
6: 
7: namespace duckdb {
8: 
9: QueryRelation::QueryRelation(ClientContext &context, string query, string alias)
10:     : Relation(context, RelationType::QUERY_RELATION), query(move(query)), alias(move(alias)) {
11: 	context.TryBindRelation(*this, this->columns);
12: }
13: 
14: unique_ptr<SelectStatement> QueryRelation::GetSelectStatement() {
15: 	Parser parser;
16: 	parser.ParseQuery(query);
17: 	if (parser.statements.size() != 1) {
18: 		throw ParserException("Expected a single SELECT statement");
19: 	}
20: 	if (parser.statements[0]->type != StatementType::SELECT_STATEMENT) {
21: 		throw ParserException("Expected a single SELECT statement");
22: 	}
23: 	return unique_ptr_cast<SQLStatement, SelectStatement>(move(parser.statements[0]));
24: }
25: 
26: unique_ptr<QueryNode> QueryRelation::GetQueryNode() {
27: 	auto select = GetSelectStatement();
28: 	return move(select->node);
29: }
30: 
31: unique_ptr<TableRef> QueryRelation::GetTableRef() {
32: 	auto subquery_ref = make_unique<SubqueryRef>(GetSelectStatement(), GetAlias());
33: 	return move(subquery_ref);
34: }
35: 
36: string QueryRelation::GetAlias() {
37: 	return alias;
38: }
39: 
40: const vector<ColumnDefinition> &QueryRelation::Columns() {
41: 	return columns;
42: }
43: 
44: string QueryRelation::ToString(idx_t depth) {
45: 	return RenderWhitespace(depth) + "Subquery [" + query + "]";
46: }
47: 
48: } // namespace duckdb
[end of src/main/relation/query_relation.cpp]
[start of src/main/relation/table_function_relation.cpp]
1: #include "duckdb/main/relation/table_function_relation.hpp"
2: #include "duckdb/parser/tableref/basetableref.hpp"
3: #include "duckdb/parser/query_node/select_node.hpp"
4: #include "duckdb/parser/expression/star_expression.hpp"
5: #include "duckdb/parser/tableref/table_function_ref.hpp"
6: #include "duckdb/parser/expression/constant_expression.hpp"
7: #include "duckdb/parser/expression/function_expression.hpp"
8: #include "duckdb/parser/expression/subquery_expression.hpp"
9: #include "duckdb/main/client_context.hpp"
10: #include "duckdb/parser/expression/comparison_expression.hpp"
11: #include "duckdb/parser/expression/columnref_expression.hpp"
12: 
13: namespace duckdb {
14: 
15: TableFunctionRelation::TableFunctionRelation(ClientContext &context, string name_p, vector<Value> parameters_p,
16:                                              unordered_map<string, Value> named_parameters,
17:                                              shared_ptr<Relation> input_relation_p)
18:     : Relation(context, RelationType::TABLE_FUNCTION_RELATION), name(move(name_p)), parameters(move(parameters_p)),
19:       named_parameters(move(named_parameters)), input_relation(move(input_relation_p)) {
20: 	context.TryBindRelation(*this, this->columns);
21: }
22: TableFunctionRelation::TableFunctionRelation(ClientContext &context, string name_p, vector<Value> parameters_p,
23: 
24:                                              shared_ptr<Relation> input_relation_p)
25:     : Relation(context, RelationType::TABLE_FUNCTION_RELATION), name(move(name_p)), parameters(move(parameters_p)),
26:       input_relation(move(input_relation_p)) {
27: 	context.TryBindRelation(*this, this->columns);
28: }
29: 
30: unique_ptr<QueryNode> TableFunctionRelation::GetQueryNode() {
31: 	auto result = make_unique<SelectNode>();
32: 	result->select_list.push_back(make_unique<StarExpression>());
33: 	result->from_table = GetTableRef();
34: 	return move(result);
35: }
36: 
37: unique_ptr<TableRef> TableFunctionRelation::GetTableRef() {
38: 	vector<unique_ptr<ParsedExpression>> children;
39: 	if (input_relation) { // input relation becomes first parameter if present, always
40: 		auto subquery = make_unique<SubqueryExpression>();
41: 		subquery->subquery = make_unique<SelectStatement>();
42: 		subquery->subquery->node = input_relation->GetQueryNode();
43: 		subquery->subquery_type = SubqueryType::SCALAR;
44: 		children.push_back(move(subquery));
45: 	}
46: 	for (auto &parameter : parameters) {
47: 		children.push_back(make_unique<ConstantExpression>(parameter));
48: 	}
49: 
50: 	for (auto &parameter : named_parameters) {
51: 		// Hackity-hack some comparisons with column refs
52: 		// This is all but pretty, basically the named parameter is the column, the table is empty because that's what
53: 		// the function binder likes
54: 		auto column_ref = make_unique<ColumnRefExpression>(parameter.first);
55: 		auto constant_value = make_unique<ConstantExpression>(parameter.second);
56: 		auto comparison =
57: 		    make_unique<ComparisonExpression>(ExpressionType::COMPARE_EQUAL, move(column_ref), move(constant_value));
58: 		children.push_back(move(comparison));
59: 	}
60: 
61: 	auto table_function = make_unique<TableFunctionRef>();
62: 	auto function = make_unique<FunctionExpression>(name, move(children));
63: 	table_function->function = move(function);
64: 	return move(table_function);
65: }
66: 
67: string TableFunctionRelation::GetAlias() {
68: 	return name;
69: }
70: 
71: const vector<ColumnDefinition> &TableFunctionRelation::Columns() {
72: 	return columns;
73: }
74: 
75: string TableFunctionRelation::ToString(idx_t depth) {
76: 	string function_call = name + "(";
77: 	for (idx_t i = 0; i < parameters.size(); i++) {
78: 		if (i > 0) {
79: 			function_call += ", ";
80: 		}
81: 		function_call += parameters[i].ToString();
82: 	}
83: 	function_call += ")";
84: 	return RenderWhitespace(depth) + function_call;
85: }
86: 
87: } // namespace duckdb
[end of src/main/relation/table_function_relation.cpp]
[start of src/main/relation/table_relation.cpp]
1: #include "duckdb/main/relation/table_relation.hpp"
2: #include "duckdb/parser/tableref/basetableref.hpp"
3: #include "duckdb/parser/query_node/select_node.hpp"
4: #include "duckdb/parser/expression/star_expression.hpp"
5: #include "duckdb/main/relation/delete_relation.hpp"
6: #include "duckdb/main/relation/update_relation.hpp"
7: #include "duckdb/parser/parser.hpp"
8: 
9: namespace duckdb {
10: 
11: TableRelation::TableRelation(ClientContext &context, unique_ptr<TableDescription> description)
12:     : Relation(context, RelationType::TABLE_RELATION), description(move(description)) {
13: }
14: 
15: unique_ptr<QueryNode> TableRelation::GetQueryNode() {
16: 	auto result = make_unique<SelectNode>();
17: 	result->select_list.push_back(make_unique<StarExpression>());
18: 	result->from_table = GetTableRef();
19: 	return move(result);
20: }
21: 
22: unique_ptr<TableRef> TableRelation::GetTableRef() {
23: 	auto table_ref = make_unique<BaseTableRef>();
24: 	table_ref->schema_name = description->schema;
25: 	table_ref->table_name = description->table;
26: 	return move(table_ref);
27: }
28: 
29: string TableRelation::GetAlias() {
30: 	return description->table;
31: }
32: 
33: const vector<ColumnDefinition> &TableRelation::Columns() {
34: 	return description->columns;
35: }
36: 
37: string TableRelation::ToString(idx_t depth) {
38: 	return RenderWhitespace(depth) + "Scan Table [" + description->table + "]";
39: }
40: 
41: static unique_ptr<ParsedExpression> ParseCondition(const string &condition) {
42: 	if (!condition.empty()) {
43: 		auto expression_list = Parser::ParseExpressionList(condition);
44: 		if (expression_list.size() != 1) {
45: 			throw ParserException("Expected a single expression as filter condition");
46: 		}
47: 		return move(expression_list[0]);
48: 	} else {
49: 		return nullptr;
50: 	}
51: }
52: 
53: void TableRelation::Update(const string &update_list, const string &condition) {
54: 	vector<string> update_columns;
55: 	vector<unique_ptr<ParsedExpression>> expressions;
56: 	auto cond = ParseCondition(condition);
57: 	Parser::ParseUpdateList(update_list, update_columns, expressions);
58: 	auto update = make_shared<UpdateRelation>(context, move(cond), description->schema, description->table,
59: 	                                          move(update_columns), move(expressions));
60: 	update->Execute();
61: }
62: 
63: void TableRelation::Delete(const string &condition) {
64: 	auto cond = ParseCondition(condition);
65: 	auto del = make_shared<DeleteRelation>(context, move(cond), description->schema, description->table);
66: 	del->Execute();
67: }
68: 
69: } // namespace duckdb
[end of src/main/relation/table_relation.cpp]
[start of src/main/relation/value_relation.cpp]
1: #include "duckdb/main/relation/value_relation.hpp"
2: #include "duckdb/parser/query_node/select_node.hpp"
3: #include "duckdb/parser/expression/star_expression.hpp"
4: #include "duckdb/parser/tableref/expressionlistref.hpp"
5: #include "duckdb/parser/expression/constant_expression.hpp"
6: #include "duckdb/main/client_context.hpp"
7: #include "duckdb/parser/parser.hpp"
8: 
9: namespace duckdb {
10: 
11: ValueRelation::ValueRelation(ClientContext &context, const vector<vector<Value>> &values, vector<string> names_p,
12:                              string alias_p)
13:     : Relation(context, RelationType::VALUE_LIST_RELATION), names(move(names_p)), alias(move(alias_p)) {
14: 	// create constant expressions for the values
15: 	for (idx_t row_idx = 0; row_idx < values.size(); row_idx++) {
16: 		auto &list = values[row_idx];
17: 		vector<unique_ptr<ParsedExpression>> expressions;
18: 		for (idx_t col_idx = 0; col_idx < list.size(); col_idx++) {
19: 			expressions.push_back(make_unique<ConstantExpression>(list[col_idx]));
20: 		}
21: 		this->expressions.push_back(move(expressions));
22: 	}
23: 	context.TryBindRelation(*this, this->columns);
24: }
25: 
26: ValueRelation::ValueRelation(ClientContext &context, const string &values_list, vector<string> names_p, string alias_p)
27:     : Relation(context, RelationType::VALUE_LIST_RELATION), names(move(names_p)), alias(move(alias_p)) {
28: 	this->expressions = Parser::ParseValuesList(values_list);
29: 	context.TryBindRelation(*this, this->columns);
30: }
31: 
32: unique_ptr<QueryNode> ValueRelation::GetQueryNode() {
33: 	auto result = make_unique<SelectNode>();
34: 	result->select_list.push_back(make_unique<StarExpression>());
35: 	result->from_table = GetTableRef();
36: 	return move(result);
37: }
38: 
39: unique_ptr<TableRef> ValueRelation::GetTableRef() {
40: 	auto table_ref = make_unique<ExpressionListRef>();
41: 	// set the expected types/names
42: 	if (columns.empty()) {
43: 		// no columns yet: only set up names
44: 		for (idx_t i = 0; i < names.size(); i++) {
45: 			table_ref->expected_names.push_back(names[i]);
46: 		}
47: 	} else {
48: 		for (idx_t i = 0; i < columns.size(); i++) {
49: 			table_ref->expected_names.push_back(columns[i].name);
50: 			table_ref->expected_types.push_back(columns[i].type);
51: 			D_ASSERT(names.size() == 0 || columns[i].name == names[i]);
52: 		}
53: 	}
54: 	// copy the expressions
55: 	for (auto &expr_list : expressions) {
56: 		vector<unique_ptr<ParsedExpression>> copied_list;
57: 		copied_list.reserve(expr_list.size());
58: 		for (auto &expr : expr_list) {
59: 			copied_list.push_back(expr->Copy());
60: 		}
61: 		table_ref->values.push_back(move(copied_list));
62: 	}
63: 	table_ref->alias = GetAlias();
64: 	return move(table_ref);
65: }
66: 
67: string ValueRelation::GetAlias() {
68: 	return alias;
69: }
70: 
71: const vector<ColumnDefinition> &ValueRelation::Columns() {
72: 	return columns;
73: }
74: 
75: string ValueRelation::ToString(idx_t depth) {
76: 	string str = RenderWhitespace(depth) + "Values ";
77: 	for (idx_t row_idx = 0; row_idx < expressions.size(); row_idx++) {
78: 		auto &list = expressions[row_idx];
79: 		str += row_idx > 0 ? ", (" : "(";
80: 		for (idx_t col_idx = 0; col_idx < list.size(); col_idx++) {
81: 			str += col_idx > 0 ? ", " : "";
82: 			str += list[col_idx]->ToString();
83: 		}
84: 		str += ")";
85: 	}
86: 	str += "\n";
87: 	return str;
88: }
89: 
90: } // namespace duckdb
[end of src/main/relation/value_relation.cpp]
[start of src/main/settings/settings.cpp]
1: #include "duckdb/main/settings.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/main/config.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/catalog/catalog_search_path.hpp"
6: #include "duckdb/storage/buffer_manager.hpp"
7: #include "duckdb/parallel/task_scheduler.hpp"
8: #include "duckdb/planner/expression_binder.hpp"
9: #include "duckdb/main/query_profiler.hpp"
10: #include "duckdb/storage/storage_manager.hpp"
11: 
12: namespace duckdb {
13: 
14: //===--------------------------------------------------------------------===//
15: // Access Mode
16: //===--------------------------------------------------------------------===//
17: void AccessModeSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
18: 	auto parameter = StringUtil::Lower(input.ToString());
19: 	if (parameter == "automatic") {
20: 		config.access_mode = AccessMode::AUTOMATIC;
21: 	} else if (parameter == "read_only") {
22: 		config.access_mode = AccessMode::READ_ONLY;
23: 	} else if (parameter == "read_write") {
24: 		config.access_mode = AccessMode::READ_WRITE;
25: 	} else {
26: 		throw InvalidInputException(
27: 		    "Unrecognized parameter for option ACCESS_MODE \"%s\". Expected READ_ONLY or READ_WRITE.", parameter);
28: 	}
29: }
30: 
31: Value AccessModeSetting::GetSetting(ClientContext &context) {
32: 	auto &config = DBConfig::GetConfig(context);
33: 	switch (config.access_mode) {
34: 	case AccessMode::AUTOMATIC:
35: 		return "automatic";
36: 	case AccessMode::READ_ONLY:
37: 		return "read_only";
38: 	case AccessMode::READ_WRITE:
39: 		return "read_write";
40: 	default:
41: 		throw InternalException("Unknown access mode setting");
42: 	}
43: }
44: 
45: //===--------------------------------------------------------------------===//
46: // Checkpoint Threshold
47: //===--------------------------------------------------------------------===//
48: void CheckpointThresholdSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
49: 	idx_t new_limit = DBConfig::ParseMemoryLimit(input.ToString());
50: 	config.checkpoint_wal_size = new_limit;
51: }
52: 
53: Value CheckpointThresholdSetting::GetSetting(ClientContext &context) {
54: 	auto &config = DBConfig::GetConfig(context);
55: 	return Value(StringUtil::BytesToHumanReadableString(config.checkpoint_wal_size));
56: }
57: 
58: //===--------------------------------------------------------------------===//
59: // Debug Checkpoint Abort
60: //===--------------------------------------------------------------------===//
61: void DebugCheckpointAbort::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
62: 	auto checkpoint_abort = StringUtil::Lower(input.ToString());
63: 	if (checkpoint_abort == "none") {
64: 		config.checkpoint_abort = CheckpointAbort::NO_ABORT;
65: 	} else if (checkpoint_abort == "before_truncate") {
66: 		config.checkpoint_abort = CheckpointAbort::DEBUG_ABORT_BEFORE_TRUNCATE;
67: 	} else if (checkpoint_abort == "before_header") {
68: 		config.checkpoint_abort = CheckpointAbort::DEBUG_ABORT_BEFORE_HEADER;
69: 	} else if (checkpoint_abort == "after_free_list_write") {
70: 		config.checkpoint_abort = CheckpointAbort::DEBUG_ABORT_AFTER_FREE_LIST_WRITE;
71: 	} else {
72: 		throw ParserException(
73: 		    "Unrecognized option for PRAGMA debug_checkpoint_abort, expected none, before_truncate or before_header");
74: 	}
75: }
76: 
77: Value DebugCheckpointAbort::GetSetting(ClientContext &context) {
78: 	return Value();
79: }
80: 
81: //===--------------------------------------------------------------------===//
82: // Debug Force External
83: //===--------------------------------------------------------------------===//
84: void DebugForceExternal::SetLocal(ClientContext &context, const Value &input) {
85: 	ClientConfig::GetConfig(context).force_external = input.GetValue<bool>();
86: }
87: 
88: Value DebugForceExternal::GetSetting(ClientContext &context) {
89: 	return Value::BOOLEAN(ClientConfig::GetConfig(context).force_external);
90: }
91: 
92: //===--------------------------------------------------------------------===//
93: // Debug Many Free List blocks
94: //===--------------------------------------------------------------------===//
95: void DebugManyFreeListBlocks::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
96: 	config.debug_many_free_list_blocks = input.GetValue<bool>();
97: }
98: 
99: Value DebugManyFreeListBlocks::GetSetting(ClientContext &context) {
100: 	auto &config = DBConfig::GetConfig(context);
101: 	return Value::BOOLEAN(config.debug_many_free_list_blocks);
102: }
103: 
104: //===--------------------------------------------------------------------===//
105: // Debug Window Mode
106: //===--------------------------------------------------------------------===//
107: void DebugWindowMode::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
108: 	auto param = StringUtil::Lower(input.ToString());
109: 	if (param == "window") {
110: 		config.window_mode = WindowAggregationMode::WINDOW;
111: 	} else if (param == "combine") {
112: 		config.window_mode = WindowAggregationMode::COMBINE;
113: 	} else if (param == "separate") {
114: 		config.window_mode = WindowAggregationMode::SEPARATE;
115: 	} else {
116: 		throw ParserException("Unrecognized option for PRAGMA debug_window_mode, expected window, combine or separate");
117: 	}
118: }
119: 
120: Value DebugWindowMode::GetSetting(ClientContext &context) {
121: 	return Value();
122: }
123: 
124: //===--------------------------------------------------------------------===//
125: // Default Collation
126: //===--------------------------------------------------------------------===//
127: void DefaultCollationSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
128: 	auto parameter = StringUtil::Lower(input.ToString());
129: 	config.collation = parameter;
130: }
131: 
132: void DefaultCollationSetting::SetLocal(ClientContext &context, const Value &input) {
133: 	auto parameter = input.ToString();
134: 	// bind the collation to verify that it exists
135: 	ExpressionBinder::TestCollation(context, parameter);
136: 	auto &config = DBConfig::GetConfig(context);
137: 	config.collation = parameter;
138: }
139: 
140: Value DefaultCollationSetting::GetSetting(ClientContext &context) {
141: 	auto &config = DBConfig::GetConfig(context);
142: 	return Value(config.collation);
143: }
144: 
145: //===--------------------------------------------------------------------===//
146: // Default Order
147: //===--------------------------------------------------------------------===//
148: void DefaultOrderSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
149: 	auto parameter = StringUtil::Lower(input.ToString());
150: 	if (parameter == "ascending" || parameter == "asc") {
151: 		config.default_order_type = OrderType::ASCENDING;
152: 	} else if (parameter == "descending" || parameter == "desc") {
153: 		config.default_order_type = OrderType::DESCENDING;
154: 	} else {
155: 		throw InvalidInputException("Unrecognized parameter for option DEFAULT_ORDER \"%s\". Expected ASC or DESC.",
156: 		                            parameter);
157: 	}
158: }
159: 
160: Value DefaultOrderSetting::GetSetting(ClientContext &context) {
161: 	auto &config = DBConfig::GetConfig(context);
162: 	switch (config.default_order_type) {
163: 	case OrderType::ASCENDING:
164: 		return "asc";
165: 	case OrderType::DESCENDING:
166: 		return "desc";
167: 	default:
168: 		throw InternalException("Unknown order type setting");
169: 	}
170: }
171: 
172: //===--------------------------------------------------------------------===//
173: // Default Null Order
174: //===--------------------------------------------------------------------===//
175: void DefaultNullOrderSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
176: 	auto parameter = StringUtil::Lower(input.ToString());
177: 
178: 	if (parameter == "nulls_first" || parameter == "nulls first" || parameter == "null first" || parameter == "first") {
179: 		config.default_null_order = OrderByNullType::NULLS_FIRST;
180: 	} else if (parameter == "nulls_last" || parameter == "nulls last" || parameter == "null last" ||
181: 	           parameter == "last") {
182: 		config.default_null_order = OrderByNullType::NULLS_LAST;
183: 	} else {
184: 		throw ParserException(
185: 		    "Unrecognized parameter for option NULL_ORDER \"%s\", expected either NULLS FIRST or NULLS LAST",
186: 		    parameter);
187: 	}
188: }
189: 
190: Value DefaultNullOrderSetting::GetSetting(ClientContext &context) {
191: 	auto &config = DBConfig::GetConfig(context);
192: 	switch (config.default_null_order) {
193: 	case OrderByNullType::NULLS_FIRST:
194: 		return "nulls_first";
195: 	case OrderByNullType::NULLS_LAST:
196: 		return "nulls_last";
197: 	default:
198: 		throw InternalException("Unknown null order setting");
199: 	}
200: }
201: 
202: //===--------------------------------------------------------------------===//
203: // Disabled Optimizer
204: //===--------------------------------------------------------------------===//
205: void DisabledOptimizersSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
206: 	auto list = StringUtil::Split(input.ToString(), ",");
207: 	set<OptimizerType> disabled_optimizers;
208: 	for (auto &entry : list) {
209: 		auto param = StringUtil::Lower(entry);
210: 		StringUtil::Trim(param);
211: 		if (param.empty()) {
212: 			continue;
213: 		}
214: 		disabled_optimizers.insert(OptimizerTypeFromString(param));
215: 	}
216: 	config.disabled_optimizers = move(disabled_optimizers);
217: }
218: 
219: Value DisabledOptimizersSetting::GetSetting(ClientContext &context) {
220: 	auto &config = DBConfig::GetConfig(context);
221: 	string result;
222: 	for (auto &optimizer : config.disabled_optimizers) {
223: 		if (!result.empty()) {
224: 			result += ",";
225: 		}
226: 		result += OptimizerTypeToString(optimizer);
227: 	}
228: 	return Value(result);
229: }
230: 
231: //===--------------------------------------------------------------------===//
232: // Enable External Access
233: //===--------------------------------------------------------------------===//
234: void EnableExternalAccessSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
235: 	auto new_value = input.GetValue<bool>();
236: 	if (db && new_value) {
237: 		throw InvalidInputException("Cannot change enable_external_access setting while database is running");
238: 	}
239: 	config.enable_external_access = new_value;
240: }
241: 
242: Value EnableExternalAccessSetting::GetSetting(ClientContext &context) {
243: 	auto &config = DBConfig::GetConfig(context);
244: 	return Value::BOOLEAN(config.enable_external_access);
245: }
246: 
247: //===--------------------------------------------------------------------===//
248: // Enable Object Cache
249: //===--------------------------------------------------------------------===//
250: void EnableObjectCacheSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
251: 	config.object_cache_enable = input.GetValue<bool>();
252: }
253: 
254: Value EnableObjectCacheSetting::GetSetting(ClientContext &context) {
255: 	auto &config = DBConfig::GetConfig(context);
256: 	return Value::BOOLEAN(config.object_cache_enable);
257: }
258: 
259: //===--------------------------------------------------------------------===//
260: // Enable Profiling
261: //===--------------------------------------------------------------------===//
262: void EnableProfilingSetting::SetLocal(ClientContext &context, const Value &input) {
263: 	auto parameter = StringUtil::Lower(input.ToString());
264: 
265: 	auto &config = ClientConfig::GetConfig(context);
266: 	if (parameter == "json") {
267: 		config.profiler_print_format = ProfilerPrintFormat::JSON;
268: 	} else if (parameter == "query_tree") {
269: 		config.profiler_print_format = ProfilerPrintFormat::QUERY_TREE;
270: 	} else if (parameter == "query_tree_optimizer") {
271: 		config.profiler_print_format = ProfilerPrintFormat::QUERY_TREE_OPTIMIZER;
272: 	} else {
273: 		throw ParserException(
274: 		    "Unrecognized print format %s, supported formats: [json, query_tree, query_tree_optimizer]", parameter);
275: 	}
276: 	config.enable_profiler = true;
277: }
278: 
279: Value EnableProfilingSetting::GetSetting(ClientContext &context) {
280: 	auto &config = ClientConfig::GetConfig(context);
281: 	if (!config.enable_profiler) {
282: 		return Value();
283: 	}
284: 	switch (config.profiler_print_format) {
285: 	case ProfilerPrintFormat::NONE:
286: 		return Value("none");
287: 	case ProfilerPrintFormat::JSON:
288: 		return Value("json");
289: 	case ProfilerPrintFormat::QUERY_TREE:
290: 		return Value("query_tree");
291: 	case ProfilerPrintFormat::QUERY_TREE_OPTIMIZER:
292: 		return Value("query_tree_optimizer");
293: 	default:
294: 		throw InternalException("Unsupported profiler print format");
295: 	}
296: }
297: 
298: //===--------------------------------------------------------------------===//
299: // Enable Progress Bar
300: //===--------------------------------------------------------------------===//
301: void EnableProgressBarSetting::SetLocal(ClientContext &context, const Value &input) {
302: 	ClientConfig::GetConfig(context).enable_progress_bar = input.GetValue<bool>();
303: }
304: 
305: Value EnableProgressBarSetting::GetSetting(ClientContext &context) {
306: 	return Value::BOOLEAN(ClientConfig::GetConfig(context).enable_progress_bar);
307: }
308: 
309: //===--------------------------------------------------------------------===//
310: // Explain Output
311: //===--------------------------------------------------------------------===//
312: void ExplainOutputSetting::SetLocal(ClientContext &context, const Value &input) {
313: 	auto parameter = StringUtil::Lower(input.ToString());
314: 	if (parameter == "all") {
315: 		ClientConfig::GetConfig(context).explain_output_type = ExplainOutputType::ALL;
316: 	} else if (parameter == "optimized_only") {
317: 		ClientConfig::GetConfig(context).explain_output_type = ExplainOutputType::OPTIMIZED_ONLY;
318: 	} else if (parameter == "physical_only") {
319: 		ClientConfig::GetConfig(context).explain_output_type = ExplainOutputType::PHYSICAL_ONLY;
320: 	} else {
321: 		throw ParserException("Unrecognized output type \"%s\", expected either ALL, OPTIMIZED_ONLY or PHYSICAL_ONLY",
322: 		                      parameter);
323: 	}
324: }
325: 
326: Value ExplainOutputSetting::GetSetting(ClientContext &context) {
327: 	switch (ClientConfig::GetConfig(context).explain_output_type) {
328: 	case ExplainOutputType::ALL:
329: 		return "all";
330: 	case ExplainOutputType::OPTIMIZED_ONLY:
331: 		return "optimized_only";
332: 	case ExplainOutputType::PHYSICAL_ONLY:
333: 		return "physical_only";
334: 	default:
335: 		throw InternalException("Unrecognized explain output type");
336: 	}
337: }
338: 
339: //===--------------------------------------------------------------------===//
340: // Force Compression
341: //===--------------------------------------------------------------------===//
342: void ForceCompressionSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
343: 	auto compression = StringUtil::Lower(input.ToString());
344: 	if (compression == "none") {
345: 		config.force_compression = CompressionType::COMPRESSION_AUTO;
346: 	} else {
347: 		auto compression_type = CompressionTypeFromString(compression);
348: 		if (compression_type == CompressionType::COMPRESSION_AUTO) {
349: 			throw ParserException("Unrecognized option for PRAGMA force_compression, expected none, uncompressed, rle, "
350: 			                      "dictionary, pfor, bitpacking or fsst");
351: 		}
352: 		config.force_compression = compression_type;
353: 	}
354: }
355: 
356: Value ForceCompressionSetting::GetSetting(ClientContext &context) {
357: 	return Value();
358: }
359: 
360: //===--------------------------------------------------------------------===//
361: // Log Query Path
362: //===--------------------------------------------------------------------===//
363: void LogQueryPathSetting::SetLocal(ClientContext &context, const Value &input) {
364: 	auto path = input.ToString();
365: 	if (path.empty()) {
366: 		// empty path: clean up query writer
367: 		context.log_query_writer = nullptr;
368: 	} else {
369: 		context.log_query_writer =
370: 		    make_unique<BufferedFileWriter>(FileSystem::GetFileSystem(context), path,
371: 		                                    BufferedFileWriter::DEFAULT_OPEN_FLAGS, context.file_opener.get());
372: 	}
373: }
374: 
375: Value LogQueryPathSetting::GetSetting(ClientContext &context) {
376: 	return context.log_query_writer ? Value(context.log_query_writer->path) : Value();
377: }
378: 
379: //===--------------------------------------------------------------------===//
380: // Maximum Memory
381: //===--------------------------------------------------------------------===//
382: void MaximumMemorySetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
383: 	config.maximum_memory = DBConfig::ParseMemoryLimit(input.ToString());
384: 	if (db) {
385: 		BufferManager::GetBufferManager(*db).SetLimit(config.maximum_memory);
386: 	}
387: }
388: 
389: Value MaximumMemorySetting::GetSetting(ClientContext &context) {
390: 	auto &config = DBConfig::GetConfig(context);
391: 	return Value(StringUtil::BytesToHumanReadableString(config.maximum_memory));
392: }
393: 
394: //===--------------------------------------------------------------------===//
395: // Perfect Hash Threshold
396: //===--------------------------------------------------------------------===//
397: void PerfectHashThresholdSetting::SetLocal(ClientContext &context, const Value &input) {
398: 	auto bits = input.GetValue<int32_t>();
399: 	if (bits < 0 || bits > 32) {
400: 		throw ParserException("Perfect HT threshold out of range: should be within range 0 - 32");
401: 	}
402: 	ClientConfig::GetConfig(context).perfect_ht_threshold = bits;
403: }
404: 
405: Value PerfectHashThresholdSetting::GetSetting(ClientContext &context) {
406: 	return Value::BIGINT(ClientConfig::GetConfig(context).perfect_ht_threshold);
407: }
408: 
409: //===--------------------------------------------------------------------===//
410: // Profiler History Size
411: //===--------------------------------------------------------------------===//
412: void ProfilerHistorySize::SetLocal(ClientContext &context, const Value &input) {
413: 	auto size = input.GetValue<int64_t>();
414: 	if (size <= 0) {
415: 		throw ParserException("Size should be >= 0");
416: 	}
417: 	context.query_profiler_history->SetProfilerHistorySize(size);
418: }
419: 
420: Value ProfilerHistorySize::GetSetting(ClientContext &context) {
421: 	return Value();
422: }
423: 
424: //===--------------------------------------------------------------------===//
425: // Profile Output
426: //===--------------------------------------------------------------------===//
427: void ProfileOutputSetting::SetLocal(ClientContext &context, const Value &input) {
428: 	auto &config = ClientConfig::GetConfig(context);
429: 	auto parameter = input.ToString();
430: 	config.profiler_save_location = parameter;
431: }
432: 
433: Value ProfileOutputSetting::GetSetting(ClientContext &context) {
434: 	auto &config = ClientConfig::GetConfig(context);
435: 	return Value(config.profiler_save_location);
436: }
437: 
438: //===--------------------------------------------------------------------===//
439: // Profiling Mode
440: //===--------------------------------------------------------------------===//
441: void ProfilingModeSetting::SetLocal(ClientContext &context, const Value &input) {
442: 	auto parameter = StringUtil::Lower(input.ToString());
443: 	auto &config = ClientConfig::GetConfig(context);
444: 	if (parameter == "standard") {
445: 		config.enable_profiler = true;
446: 		config.enable_detailed_profiling = false;
447: 	} else if (parameter == "detailed") {
448: 		config.enable_profiler = true;
449: 		config.enable_detailed_profiling = true;
450: 	} else {
451: 		throw ParserException("Unrecognized profiling mode \"%s\", supported formats: [standard, detailed]", parameter);
452: 	}
453: }
454: 
455: Value ProfilingModeSetting::GetSetting(ClientContext &context) {
456: 	auto &config = ClientConfig::GetConfig(context);
457: 	if (!config.enable_profiler) {
458: 		return Value();
459: 	}
460: 	return Value(config.enable_detailed_profiling ? "detailed" : "standard");
461: }
462: 
463: //===--------------------------------------------------------------------===//
464: // Progress Bar Time
465: //===--------------------------------------------------------------------===//
466: void ProgressBarTimeSetting::SetLocal(ClientContext &context, const Value &input) {
467: 	ClientConfig::GetConfig(context).wait_time = input.GetValue<int32_t>();
468: 	ClientConfig::GetConfig(context).enable_progress_bar = true;
469: }
470: 
471: Value ProgressBarTimeSetting::GetSetting(ClientContext &context) {
472: 	return Value::BIGINT(ClientConfig::GetConfig(context).wait_time);
473: }
474: 
475: //===--------------------------------------------------------------------===//
476: // Schema
477: //===--------------------------------------------------------------------===//
478: void SchemaSetting::SetLocal(ClientContext &context, const Value &input) {
479: 	auto parameter = input.ToString();
480: 	context.catalog_search_path->Set(parameter, true);
481: }
482: 
483: Value SchemaSetting::GetSetting(ClientContext &context) {
484: 	return SearchPathSetting::GetSetting(context);
485: }
486: 
487: //===--------------------------------------------------------------------===//
488: // Search Path
489: //===--------------------------------------------------------------------===//
490: void SearchPathSetting::SetLocal(ClientContext &context, const Value &input) {
491: 	auto parameter = input.ToString();
492: 	context.catalog_search_path->Set(parameter, false);
493: }
494: 
495: Value SearchPathSetting::GetSetting(ClientContext &context) {
496: 	return Value(StringUtil::Join(context.catalog_search_path->GetSetPaths(), ","));
497: }
498: 
499: //===--------------------------------------------------------------------===//
500: // Temp Directory
501: //===--------------------------------------------------------------------===//
502: void TempDirectorySetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
503: 	config.temporary_directory = input.ToString();
504: 	config.use_temporary_directory = !config.temporary_directory.empty();
505: 	if (db) {
506: 		auto &buffer_manager = BufferManager::GetBufferManager(*db);
507: 		buffer_manager.SetTemporaryDirectory(config.temporary_directory);
508: 	}
509: }
510: 
511: Value TempDirectorySetting::GetSetting(ClientContext &context) {
512: 	auto &buffer_manager = BufferManager::GetBufferManager(context);
513: 	return Value(buffer_manager.GetTemporaryDirectory());
514: }
515: 
516: //===--------------------------------------------------------------------===//
517: // Threads Setting
518: //===--------------------------------------------------------------------===//
519: void ThreadsSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
520: 	config.maximum_threads = input.GetValue<int64_t>();
521: 	if (db) {
522: 		TaskScheduler::GetScheduler(*db).SetThreads(config.maximum_threads);
523: 	}
524: }
525: 
526: Value ThreadsSetting::GetSetting(ClientContext &context) {
527: 	auto &config = DBConfig::GetConfig(context);
528: 	return Value::BIGINT(config.maximum_threads);
529: }
530: 
531: } // namespace duckdb
[end of src/main/settings/settings.cpp]
[start of src/parser/expression/columnref_expression.cpp]
1: #include "duckdb/parser/expression/columnref_expression.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/serializer.hpp"
5: #include "duckdb/common/types/hash.hpp"
6: 
7: namespace duckdb {
8: 
9: ColumnRefExpression::ColumnRefExpression(string column_name, string table_name)
10:     : ColumnRefExpression(table_name.empty() ? vector<string> {move(column_name)}
11:                                              : vector<string> {move(table_name), move(column_name)}) {
12: }
13: 
14: ColumnRefExpression::ColumnRefExpression(string column_name) : ColumnRefExpression(vector<string> {move(column_name)}) {
15: }
16: 
17: ColumnRefExpression::ColumnRefExpression(vector<string> column_names_p)
18:     : ParsedExpression(ExpressionType::COLUMN_REF, ExpressionClass::COLUMN_REF), column_names(move(column_names_p)) {
19: #ifdef DEBUG
20: 	for (auto &col_name : column_names) {
21: 		D_ASSERT(!col_name.empty());
22: 	}
23: #endif
24: }
25: 
26: bool ColumnRefExpression::IsQualified() const {
27: 	return column_names.size() > 1;
28: }
29: 
30: const string &ColumnRefExpression::GetColumnName() const {
31: 	D_ASSERT(column_names.size() <= 3);
32: 	return column_names.back();
33: }
34: 
35: const string &ColumnRefExpression::GetTableName() const {
36: 	D_ASSERT(column_names.size() >= 2 && column_names.size() <= 3);
37: 	return column_names.size() == 3 ? column_names[1] : column_names[0];
38: }
39: 
40: string ColumnRefExpression::GetName() const {
41: 	return !alias.empty() ? alias : column_names.back();
42: }
43: 
44: string ColumnRefExpression::ToString() const {
45: 	string result;
46: 	for (idx_t i = 0; i < column_names.size(); i++) {
47: 		if (i > 0) {
48: 			result += ".";
49: 		}
50: 		result += column_names[i];
51: 	}
52: 	return result;
53: }
54: 
55: bool ColumnRefExpression::Equals(const ColumnRefExpression *a, const ColumnRefExpression *b) {
56: 	return a->column_names == b->column_names;
57: }
58: 
59: hash_t ColumnRefExpression::Hash() const {
60: 	hash_t result = ParsedExpression::Hash();
61: 	for (auto &column_name : column_names) {
62: 		result = CombineHash(result, duckdb::Hash<const char *>(column_name.c_str()));
63: 	}
64: 	return result;
65: }
66: 
67: unique_ptr<ParsedExpression> ColumnRefExpression::Copy() const {
68: 	auto copy = make_unique<ColumnRefExpression>(column_names);
69: 	copy->CopyProperties(*this);
70: 	return move(copy);
71: }
72: 
73: void ColumnRefExpression::Serialize(Serializer &serializer) {
74: 	ParsedExpression::Serialize(serializer);
75: 	serializer.Write<idx_t>(column_names.size());
76: 	for (auto &column_name : column_names) {
77: 		serializer.WriteString(column_name);
78: 	}
79: }
80: 
81: unique_ptr<ParsedExpression> ColumnRefExpression::Deserialize(ExpressionType type, Deserializer &source) {
82: 	auto column_count = source.Read<idx_t>();
83: 	vector<string> column_names;
84: 	for (idx_t i = 0; i < column_count; i++) {
85: 		column_names.push_back(source.Read<string>());
86: 	}
87: 	auto expression = make_unique<ColumnRefExpression>(move(column_names));
88: 	return move(expression);
89: }
90: 
91: } // namespace duckdb
[end of src/parser/expression/columnref_expression.cpp]
[start of src/parser/parser.cpp]
1: #include "duckdb/parser/parser.hpp"
2: 
3: #include "duckdb/parser/transformer.hpp"
4: #include "duckdb/parser/parsed_data/create_table_info.hpp"
5: #include "duckdb/parser/statement/create_statement.hpp"
6: #include "duckdb/parser/statement/select_statement.hpp"
7: #include "duckdb/parser/statement/update_statement.hpp"
8: #include "duckdb/parser/query_node/select_node.hpp"
9: #include "duckdb/parser/tableref/expressionlistref.hpp"
10: #include "postgres_parser.hpp"
11: #include "duckdb/parser/query_error_context.hpp"
12: 
13: #include "parser/parser.hpp"
14: 
15: namespace duckdb {
16: 
17: Parser::Parser() {
18: }
19: 
20: void Parser::ParseQuery(const string &query) {
21: 	Transformer transformer;
22: 	{
23: 		PostgresParser parser;
24: 		parser.Parse(query);
25: 
26: 		if (!parser.success) {
27: 			throw ParserException(QueryErrorContext::Format(query, parser.error_message, parser.error_location - 1));
28: 		}
29: 
30: 		if (!parser.parse_tree) {
31: 			// empty statement
32: 			return;
33: 		}
34: 
35: 		// if it succeeded, we transform the Postgres parse tree into a list of
36: 		// SQLStatements
37: 		transformer.TransformParseTree(parser.parse_tree, statements);
38: 	}
39: 	if (!statements.empty()) {
40: 		auto &last_statement = statements.back();
41: 		last_statement->stmt_length = query.size() - last_statement->stmt_location;
42: 		for (auto &statement : statements) {
43: 			statement->query = query;
44: 			if (statement->type == StatementType::CREATE_STATEMENT) {
45: 				auto &create = (CreateStatement &)*statement;
46: 				create.info->sql = query.substr(statement->stmt_location, statement->stmt_length);
47: 			}
48: 		}
49: 	}
50: }
51: 
52: vector<SimplifiedToken> Parser::Tokenize(const string &query) {
53: 	auto pg_tokens = PostgresParser::Tokenize(query);
54: 	vector<SimplifiedToken> result;
55: 	result.reserve(pg_tokens.size());
56: 	for (auto &pg_token : pg_tokens) {
57: 		SimplifiedToken token;
58: 		switch (pg_token.type) {
59: 		case duckdb_libpgquery::PGSimplifiedTokenType::PG_SIMPLIFIED_TOKEN_IDENTIFIER:
60: 			token.type = SimplifiedTokenType::SIMPLIFIED_TOKEN_IDENTIFIER;
61: 			break;
62: 		case duckdb_libpgquery::PGSimplifiedTokenType::PG_SIMPLIFIED_TOKEN_NUMERIC_CONSTANT:
63: 			token.type = SimplifiedTokenType::SIMPLIFIED_TOKEN_NUMERIC_CONSTANT;
64: 			break;
65: 		case duckdb_libpgquery::PGSimplifiedTokenType::PG_SIMPLIFIED_TOKEN_STRING_CONSTANT:
66: 			token.type = SimplifiedTokenType::SIMPLIFIED_TOKEN_STRING_CONSTANT;
67: 			break;
68: 		case duckdb_libpgquery::PGSimplifiedTokenType::PG_SIMPLIFIED_TOKEN_OPERATOR:
69: 			token.type = SimplifiedTokenType::SIMPLIFIED_TOKEN_OPERATOR;
70: 			break;
71: 		case duckdb_libpgquery::PGSimplifiedTokenType::PG_SIMPLIFIED_TOKEN_KEYWORD:
72: 			token.type = SimplifiedTokenType::SIMPLIFIED_TOKEN_KEYWORD;
73: 			break;
74: 		// comments are not supported by our tokenizer right now
75: 		case duckdb_libpgquery::PGSimplifiedTokenType::PG_SIMPLIFIED_TOKEN_COMMENT: // LCOV_EXCL_START
76: 			token.type = SimplifiedTokenType::SIMPLIFIED_TOKEN_COMMENT;
77: 			break;
78: 		default:
79: 			throw InternalException("Unrecognized token category");
80: 		} // LCOV_EXCL_STOP
81: 		token.start = pg_token.start;
82: 		result.push_back(token);
83: 	}
84: 	return result;
85: }
86: 
87: bool Parser::IsKeyword(const string &text) {
88: 	return PostgresParser::IsKeyword(text);
89: }
90: 
91: vector<ParserKeyword> Parser::KeywordList() {
92: 	auto keywords = PostgresParser::KeywordList();
93: 	vector<ParserKeyword> result;
94: 	for (auto &kw : keywords) {
95: 		ParserKeyword res;
96: 		res.name = kw.text;
97: 		switch (kw.category) {
98: 		case duckdb_libpgquery::PGKeywordCategory::PG_KEYWORD_RESERVED:
99: 			res.category = KeywordCategory::KEYWORD_RESERVED;
100: 			break;
101: 		case duckdb_libpgquery::PGKeywordCategory::PG_KEYWORD_UNRESERVED:
102: 			res.category = KeywordCategory::KEYWORD_UNRESERVED;
103: 			break;
104: 		case duckdb_libpgquery::PGKeywordCategory::PG_KEYWORD_TYPE_FUNC:
105: 			res.category = KeywordCategory::KEYWORD_TYPE_FUNC;
106: 			break;
107: 		case duckdb_libpgquery::PGKeywordCategory::PG_KEYWORD_COL_NAME:
108: 			res.category = KeywordCategory::KEYWORD_COL_NAME;
109: 			break;
110: 		default:
111: 			throw InternalException("Unrecognized keyword category");
112: 		}
113: 		result.push_back(res);
114: 	}
115: 	return result;
116: }
117: 
118: vector<unique_ptr<ParsedExpression>> Parser::ParseExpressionList(const string &select_list) {
119: 	// construct a mock query prefixed with SELECT
120: 	string mock_query = "SELECT " + select_list;
121: 	// parse the query
122: 	Parser parser;
123: 	parser.ParseQuery(mock_query);
124: 	// check the statements
125: 	if (parser.statements.size() != 1 || parser.statements[0]->type != StatementType::SELECT_STATEMENT) {
126: 		throw ParserException("Expected a single SELECT statement");
127: 	}
128: 	auto &select = (SelectStatement &)*parser.statements[0];
129: 	if (select.node->type != QueryNodeType::SELECT_NODE) {
130: 		throw ParserException("Expected a single SELECT node");
131: 	}
132: 	auto &select_node = (SelectNode &)*select.node;
133: 	return move(select_node.select_list);
134: }
135: 
136: vector<OrderByNode> Parser::ParseOrderList(const string &select_list) {
137: 	// construct a mock query
138: 	string mock_query = "SELECT * FROM tbl ORDER BY " + select_list;
139: 	// parse the query
140: 	Parser parser;
141: 	parser.ParseQuery(mock_query);
142: 	// check the statements
143: 	if (parser.statements.size() != 1 || parser.statements[0]->type != StatementType::SELECT_STATEMENT) {
144: 		throw ParserException("Expected a single SELECT statement");
145: 	}
146: 	auto &select = (SelectStatement &)*parser.statements[0];
147: 	if (select.node->type != QueryNodeType::SELECT_NODE) {
148: 		throw InternalException("Expected a single SELECT node");
149: 	}
150: 	auto &select_node = (SelectNode &)*select.node;
151: 	if (select_node.modifiers.empty() || select_node.modifiers[0]->type != ResultModifierType::ORDER_MODIFIER ||
152: 	    select_node.modifiers.size() != 1) {
153: 		throw InternalException("Expected a single ORDER clause");
154: 	}
155: 	auto &order = (OrderModifier &)*select_node.modifiers[0];
156: 	return move(order.orders);
157: }
158: 
159: void Parser::ParseUpdateList(const string &update_list, vector<string> &update_columns,
160:                              vector<unique_ptr<ParsedExpression>> &expressions) {
161: 	// construct a mock query
162: 	string mock_query = "UPDATE tbl SET " + update_list;
163: 	// parse the query
164: 	Parser parser;
165: 	parser.ParseQuery(mock_query);
166: 	// check the statements
167: 	if (parser.statements.size() != 1 || parser.statements[0]->type != StatementType::UPDATE_STATEMENT) {
168: 		throw ParserException("Expected a single UPDATE statement");
169: 	}
170: 	auto &update = (UpdateStatement &)*parser.statements[0];
171: 	update_columns = move(update.columns);
172: 	expressions = move(update.expressions);
173: }
174: 
175: vector<vector<unique_ptr<ParsedExpression>>> Parser::ParseValuesList(const string &value_list) {
176: 	// construct a mock query
177: 	string mock_query = "VALUES " + value_list;
178: 	// parse the query
179: 	Parser parser;
180: 	parser.ParseQuery(mock_query);
181: 	// check the statements
182: 	if (parser.statements.size() != 1 || parser.statements[0]->type != StatementType::SELECT_STATEMENT) {
183: 		throw ParserException("Expected a single SELECT statement");
184: 	}
185: 	auto &select = (SelectStatement &)*parser.statements[0];
186: 	if (select.node->type != QueryNodeType::SELECT_NODE) {
187: 		throw ParserException("Expected a single SELECT node");
188: 	}
189: 	auto &select_node = (SelectNode &)*select.node;
190: 	if (!select_node.from_table || select_node.from_table->type != TableReferenceType::EXPRESSION_LIST) {
191: 		throw InternalException("Expected a single VALUES statement");
192: 	}
193: 	auto &values_list = (ExpressionListRef &)*select_node.from_table;
194: 	return move(values_list.values);
195: }
196: 
197: vector<ColumnDefinition> Parser::ParseColumnList(const string &column_list) {
198: 	string mock_query = "CREATE TABLE blabla (" + column_list + ")";
199: 	Parser parser;
200: 	parser.ParseQuery(mock_query);
201: 	if (parser.statements.size() != 1 || parser.statements[0]->type != StatementType::CREATE_STATEMENT) {
202: 		throw ParserException("Expected a single CREATE statement");
203: 	}
204: 	auto &create = (CreateStatement &)*parser.statements[0];
205: 	if (create.info->type != CatalogType::TABLE_ENTRY) {
206: 		throw InternalException("Expected a single CREATE TABLE statement");
207: 	}
208: 	auto &info = ((CreateTableInfo &)*create.info);
209: 	return move(info.columns);
210: }
211: 
212: } // namespace duckdb
[end of src/parser/parser.cpp]
[start of src/planner/binder/expression/bind_comparison_expression.cpp]
1: #include "duckdb/parser/expression/comparison_expression.hpp"
2: #include "duckdb/planner/expression/bound_cast_expression.hpp"
3: #include "duckdb/planner/expression/bound_constant_expression.hpp"
4: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
5: #include "duckdb/planner/expression/bound_function_expression.hpp"
6: #include "duckdb/planner/expression_binder.hpp"
7: #include "duckdb/catalog/catalog_entry/collate_catalog_entry.hpp"
8: #include "duckdb/common/string_util.hpp"
9: 
10: #include "duckdb/function/scalar/string_functions.hpp"
11: 
12: #include "duckdb/common/types/decimal.hpp"
13: 
14: #include "duckdb/main/config.hpp"
15: #include "duckdb/catalog/catalog.hpp"
16: 
17: namespace duckdb {
18: 
19: unique_ptr<Expression> ExpressionBinder::PushCollation(ClientContext &context, unique_ptr<Expression> source,
20:                                                        const string &collation_p, bool equality_only) {
21: 	// replace default collation with system collation
22: 	string collation;
23: 	if (collation_p.empty()) {
24: 		collation = DBConfig::GetConfig(context).collation;
25: 	} else {
26: 		collation = collation_p;
27: 	}
28: 	// bind the collation
29: 	if (collation.empty() || collation == "binary" || collation == "c" || collation == "posix") {
30: 		// binary collation: just skip
31: 		return source;
32: 	}
33: 	auto &catalog = Catalog::GetCatalog(context);
34: 	auto splits = StringUtil::Split(StringUtil::Lower(collation), ".");
35: 	vector<CollateCatalogEntry *> entries;
36: 	for (auto &collation_argument : splits) {
37: 		auto collation_entry = catalog.GetEntry<CollateCatalogEntry>(context, DEFAULT_SCHEMA, collation_argument);
38: 		if (collation_entry->combinable) {
39: 			entries.insert(entries.begin(), collation_entry);
40: 		} else {
41: 			if (!entries.empty() && !entries.back()->combinable) {
42: 				throw BinderException("Cannot combine collation types \"%s\" and \"%s\"", entries.back()->name,
43: 				                      collation_entry->name);
44: 			}
45: 			entries.push_back(collation_entry);
46: 		}
47: 	}
48: 	for (auto &collation_entry : entries) {
49: 		if (equality_only && collation_entry->not_required_for_equality) {
50: 			continue;
51: 		}
52: 		vector<unique_ptr<Expression>> children;
53: 		children.push_back(move(source));
54: 		auto function = ScalarFunction::BindScalarFunction(context, collation_entry->function, move(children));
55: 		source = move(function);
56: 	}
57: 	return source;
58: }
59: 
60: void ExpressionBinder::TestCollation(ClientContext &context, const string &collation) {
61: 	PushCollation(context, make_unique<BoundConstantExpression>(Value("")), collation);
62: }
63: 
64: LogicalType BoundComparisonExpression::BindComparison(LogicalType left_type, LogicalType right_type) {
65: 	auto result_type = LogicalType::MaxLogicalType(left_type, right_type);
66: 	switch (result_type.id()) {
67: 	case LogicalTypeId::DECIMAL: {
68: 		// result is a decimal: we need the maximum width and the maximum scale over width
69: 		vector<LogicalType> argument_types = {left_type, right_type};
70: 		uint8_t max_width = 0, max_scale = 0, max_width_over_scale = 0;
71: 		for (idx_t i = 0; i < argument_types.size(); i++) {
72: 			uint8_t width, scale;
73: 			auto can_convert = argument_types[i].GetDecimalProperties(width, scale);
74: 			if (!can_convert) {
75: 				return result_type;
76: 			}
77: 			max_width = MaxValue<uint8_t>(width, max_width);
78: 			max_scale = MaxValue<uint8_t>(scale, max_scale);
79: 			max_width_over_scale = MaxValue<uint8_t>(width - scale, max_width_over_scale);
80: 		}
81: 		max_width = MaxValue<uint8_t>(max_scale + max_width_over_scale, max_width);
82: 		if (max_width > Decimal::MAX_WIDTH_DECIMAL) {
83: 			// target width does not fit in decimal: truncate the scale (if possible) to try and make it fit
84: 			max_width = Decimal::MAX_WIDTH_DECIMAL;
85: 		}
86: 		return LogicalType::DECIMAL(max_width, max_scale);
87: 	}
88: 	case LogicalTypeId::VARCHAR:
89: 		// for comparison with strings, we prefer to bind to the numeric types
90: 		if (left_type.IsNumeric() || left_type.id() == LogicalTypeId::BOOLEAN) {
91: 			return left_type;
92: 		} else if (right_type.IsNumeric() || right_type.id() == LogicalTypeId::BOOLEAN) {
93: 			return right_type;
94: 		} else {
95: 			// else: check if collations are compatible
96: 			auto left_collation = StringType::GetCollation(left_type);
97: 			auto right_collation = StringType::GetCollation(right_type);
98: 			if (!left_collation.empty() && !right_collation.empty() && left_collation != right_collation) {
99: 				throw BinderException("Cannot combine types with different collation!");
100: 			}
101: 		}
102: 		return result_type;
103: 	case LogicalTypeId::UNKNOWN:
104: 		// comparing two prepared statement parameters (e.g. SELECT ?=?)
105: 		// default to VARCHAR
106: 		return LogicalType::VARCHAR;
107: 	default:
108: 		return result_type;
109: 	}
110: }
111: 
112: BindResult ExpressionBinder::BindExpression(ComparisonExpression &expr, idx_t depth) {
113: 	// first try to bind the children of the case expression
114: 	string error;
115: 	BindChild(expr.left, depth, error);
116: 	BindChild(expr.right, depth, error);
117: 	if (!error.empty()) {
118: 		return BindResult(error);
119: 	}
120: 	// the children have been successfully resolved
121: 	auto &left = (BoundExpression &)*expr.left;
122: 	auto &right = (BoundExpression &)*expr.right;
123: 	auto left_sql_type = left.expr->return_type;
124: 	auto right_sql_type = right.expr->return_type;
125: 	// cast the input types to the same type
126: 	// now obtain the result type of the input types
127: 	auto input_type = BoundComparisonExpression::BindComparison(left_sql_type, right_sql_type);
128: 	// add casts (if necessary)
129: 	left.expr = BoundCastExpression::AddCastToType(move(left.expr), input_type);
130: 	right.expr = BoundCastExpression::AddCastToType(move(right.expr), input_type);
131: 	if (input_type.id() == LogicalTypeId::VARCHAR) {
132: 		// handle collation
133: 		auto collation = StringType::GetCollation(input_type);
134: 		left.expr = PushCollation(context, move(left.expr), collation, expr.type == ExpressionType::COMPARE_EQUAL);
135: 		right.expr = PushCollation(context, move(right.expr), collation, expr.type == ExpressionType::COMPARE_EQUAL);
136: 	}
137: 	// now create the bound comparison expression
138: 	return BindResult(make_unique<BoundComparisonExpression>(expr.type, move(left.expr), move(right.expr)));
139: }
140: 
141: } // namespace duckdb
[end of src/planner/binder/expression/bind_comparison_expression.cpp]
[start of src/planner/binder/query_node/bind_select_node.cpp]
1: #include "duckdb/common/limits.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/execution/expression_executor.hpp"
4: #include "duckdb/main/config.hpp"
5: #include "duckdb/parser/expression/columnref_expression.hpp"
6: #include "duckdb/parser/expression/comparison_expression.hpp"
7: #include "duckdb/parser/expression/constant_expression.hpp"
8: #include "duckdb/parser/expression/subquery_expression.hpp"
9: #include "duckdb/parser/query_node/select_node.hpp"
10: #include "duckdb/parser/tableref/joinref.hpp"
11: #include "duckdb/planner/binder.hpp"
12: #include "duckdb/planner/expression_binder/column_alias_binder.hpp"
13: #include "duckdb/planner/expression_binder/constant_binder.hpp"
14: #include "duckdb/planner/expression_binder/group_binder.hpp"
15: #include "duckdb/planner/expression_binder/having_binder.hpp"
16: #include "duckdb/planner/expression_binder/qualify_binder.hpp"
17: #include "duckdb/planner/expression_binder/order_binder.hpp"
18: #include "duckdb/planner/expression_binder/select_binder.hpp"
19: #include "duckdb/planner/expression_binder/where_binder.hpp"
20: #include "duckdb/planner/query_node/bound_select_node.hpp"
21: #include "duckdb/planner/expression_binder/aggregate_binder.hpp"
22: 
23: namespace duckdb {
24: 
25: unique_ptr<Expression> Binder::BindOrderExpression(OrderBinder &order_binder, unique_ptr<ParsedExpression> expr) {
26: 	// we treat the Distinct list as a order by
27: 	auto bound_expr = order_binder.Bind(move(expr));
28: 	if (!bound_expr) {
29: 		// DISTINCT ON non-integer constant
30: 		// remove the expression from the DISTINCT ON list
31: 		return nullptr;
32: 	}
33: 	D_ASSERT(bound_expr->type == ExpressionType::BOUND_COLUMN_REF);
34: 	return bound_expr;
35: }
36: 
37: unique_ptr<Expression> Binder::BindDelimiter(ClientContext &context, unique_ptr<ParsedExpression> delimiter,
38:                                              const LogicalType &type, Value &delimiter_value) {
39: 	auto new_binder = Binder::CreateBinder(context, this, true);
40: 	ExpressionBinder expr_binder(*new_binder, context);
41: 	expr_binder.target_type = type;
42: 	auto expr = expr_binder.Bind(delimiter);
43: 	if (expr->IsFoldable()) {
44: 		//! this is a constant
45: 		delimiter_value = ExpressionExecutor::EvaluateScalar(*expr).CastAs(type);
46: 		return nullptr;
47: 	}
48: 	return expr;
49: }
50: 
51: unique_ptr<BoundResultModifier> Binder::BindLimit(LimitModifier &limit_mod) {
52: 	auto result = make_unique<BoundLimitModifier>();
53: 	if (limit_mod.limit) {
54: 		Value val;
55: 		result->limit = BindDelimiter(context, move(limit_mod.limit), LogicalType::BIGINT, val);
56: 		if (!result->limit) {
57: 			result->limit_val = val.GetValue<int64_t>();
58: 		}
59: 	}
60: 	if (limit_mod.offset) {
61: 		Value val;
62: 		result->offset = BindDelimiter(context, move(limit_mod.offset), LogicalType::BIGINT, val);
63: 		if (!result->offset) {
64: 			result->offset_val = val.GetValue<int64_t>();
65: 		}
66: 	}
67: 	return move(result);
68: }
69: 
70: unique_ptr<BoundResultModifier> Binder::BindLimitPercent(LimitPercentModifier &limit_mod) {
71: 	auto result = make_unique<BoundLimitPercentModifier>();
72: 	if (limit_mod.limit) {
73: 		Value val;
74: 		result->limit = BindDelimiter(context, move(limit_mod.limit), LogicalType::DOUBLE, val);
75: 		if (!result->limit) {
76: 			result->limit_percent = val.GetValue<double>();
77: 			if (result->limit_percent < 0.0) {
78: 				throw Exception("Limit percentage can't be negative value");
79: 			}
80: 		}
81: 	}
82: 	if (limit_mod.offset) {
83: 		Value val;
84: 		result->offset = BindDelimiter(context, move(limit_mod.offset), LogicalType::BIGINT, val);
85: 		if (!result->offset) {
86: 			result->offset_val = val.GetValue<int64_t>();
87: 		}
88: 	}
89: 	return move(result);
90: }
91: 
92: void Binder::BindModifiers(OrderBinder &order_binder, QueryNode &statement, BoundQueryNode &result) {
93: 	for (auto &mod : statement.modifiers) {
94: 		unique_ptr<BoundResultModifier> bound_modifier;
95: 		switch (mod->type) {
96: 		case ResultModifierType::DISTINCT_MODIFIER: {
97: 			auto &distinct = (DistinctModifier &)*mod;
98: 			auto bound_distinct = make_unique<BoundDistinctModifier>();
99: 			if (distinct.distinct_on_targets.empty()) {
100: 				for (idx_t i = 0; i < result.names.size(); i++) {
101: 					distinct.distinct_on_targets.push_back(make_unique<ConstantExpression>(Value::INTEGER(1 + i)));
102: 				}
103: 			}
104: 			for (auto &distinct_on_target : distinct.distinct_on_targets) {
105: 				auto expr = BindOrderExpression(order_binder, move(distinct_on_target));
106: 				if (!expr) {
107: 					continue;
108: 				}
109: 				bound_distinct->target_distincts.push_back(move(expr));
110: 			}
111: 			bound_modifier = move(bound_distinct);
112: 			break;
113: 		}
114: 		case ResultModifierType::ORDER_MODIFIER: {
115: 			auto &order = (OrderModifier &)*mod;
116: 			auto bound_order = make_unique<BoundOrderModifier>();
117: 			auto &config = DBConfig::GetConfig(context);
118: 			D_ASSERT(!order.orders.empty());
119: 			if (order.orders[0].expression->type == ExpressionType::STAR) {
120: 				// ORDER BY ALL
121: 				// replace the order list with the maximum order by count
122: 				D_ASSERT(order.orders.size() == 1);
123: 				auto order_type = order.orders[0].type;
124: 				auto null_order = order.orders[0].null_order;
125: 
126: 				vector<OrderByNode> new_orders;
127: 				for (idx_t i = 0; i < order_binder.MaxCount(); i++) {
128: 					new_orders.emplace_back(order_type, null_order,
129: 					                        make_unique<ConstantExpression>(Value::INTEGER(i + 1)));
130: 				}
131: 				order.orders = move(new_orders);
132: 			}
133: 			for (auto &order_node : order.orders) {
134: 				auto order_expression = BindOrderExpression(order_binder, move(order_node.expression));
135: 				if (!order_expression) {
136: 					continue;
137: 				}
138: 				auto type = order_node.type == OrderType::ORDER_DEFAULT ? config.default_order_type : order_node.type;
139: 				auto null_order = order_node.null_order == OrderByNullType::ORDER_DEFAULT ? config.default_null_order
140: 				                                                                          : order_node.null_order;
141: 				bound_order->orders.emplace_back(type, null_order, move(order_expression));
142: 			}
143: 			if (!bound_order->orders.empty()) {
144: 				bound_modifier = move(bound_order);
145: 			}
146: 			break;
147: 		}
148: 		case ResultModifierType::LIMIT_MODIFIER:
149: 			bound_modifier = BindLimit((LimitModifier &)*mod);
150: 			break;
151: 		case ResultModifierType::LIMIT_PERCENT_MODIFIER:
152: 			bound_modifier = BindLimitPercent((LimitPercentModifier &)*mod);
153: 			break;
154: 		default:
155: 			throw Exception("Unsupported result modifier");
156: 		}
157: 		if (bound_modifier) {
158: 			result.modifiers.push_back(move(bound_modifier));
159: 		}
160: 	}
161: }
162: 
163: void Binder::BindModifierTypes(BoundQueryNode &result, const vector<LogicalType> &sql_types, idx_t projection_index) {
164: 	for (auto &bound_mod : result.modifiers) {
165: 		switch (bound_mod->type) {
166: 		case ResultModifierType::DISTINCT_MODIFIER: {
167: 			auto &distinct = (BoundDistinctModifier &)*bound_mod;
168: 			if (distinct.target_distincts.empty()) {
169: 				// DISTINCT without a target: push references to the standard select list
170: 				for (idx_t i = 0; i < sql_types.size(); i++) {
171: 					distinct.target_distincts.push_back(
172: 					    make_unique<BoundColumnRefExpression>(sql_types[i], ColumnBinding(projection_index, i)));
173: 				}
174: 			} else {
175: 				// DISTINCT with target list: set types
176: 				for (auto &expr : distinct.target_distincts) {
177: 					D_ASSERT(expr->type == ExpressionType::BOUND_COLUMN_REF);
178: 					auto &bound_colref = (BoundColumnRefExpression &)*expr;
179: 					if (bound_colref.binding.column_index == DConstants::INVALID_INDEX) {
180: 						throw BinderException("Ambiguous name in DISTINCT ON!");
181: 					}
182: 					D_ASSERT(bound_colref.binding.column_index < sql_types.size());
183: 					bound_colref.return_type = sql_types[bound_colref.binding.column_index];
184: 				}
185: 			}
186: 			for (auto &target_distinct : distinct.target_distincts) {
187: 				auto &bound_colref = (BoundColumnRefExpression &)*target_distinct;
188: 				auto sql_type = sql_types[bound_colref.binding.column_index];
189: 				if (sql_type.id() == LogicalTypeId::VARCHAR) {
190: 					target_distinct = ExpressionBinder::PushCollation(context, move(target_distinct),
191: 					                                                  StringType::GetCollation(sql_type), true);
192: 				}
193: 			}
194: 			break;
195: 		}
196: 		case ResultModifierType::ORDER_MODIFIER: {
197: 			auto &order = (BoundOrderModifier &)*bound_mod;
198: 			for (auto &order_node : order.orders) {
199: 				auto &expr = order_node.expression;
200: 				D_ASSERT(expr->type == ExpressionType::BOUND_COLUMN_REF);
201: 				auto &bound_colref = (BoundColumnRefExpression &)*expr;
202: 				if (bound_colref.binding.column_index == DConstants::INVALID_INDEX) {
203: 					throw BinderException("Ambiguous name in ORDER BY!");
204: 				}
205: 				D_ASSERT(bound_colref.binding.column_index < sql_types.size());
206: 				auto sql_type = sql_types[bound_colref.binding.column_index];
207: 				bound_colref.return_type = sql_types[bound_colref.binding.column_index];
208: 				if (sql_type.id() == LogicalTypeId::VARCHAR) {
209: 					order_node.expression = ExpressionBinder::PushCollation(context, move(order_node.expression),
210: 					                                                        StringType::GetCollation(sql_type));
211: 				}
212: 			}
213: 			break;
214: 		}
215: 		default:
216: 			break;
217: 		}
218: 	}
219: }
220: 
221: unique_ptr<BoundQueryNode> Binder::BindNode(SelectNode &statement) {
222: 	auto result = make_unique<BoundSelectNode>();
223: 	result->projection_index = GenerateTableIndex();
224: 	result->group_index = GenerateTableIndex();
225: 	result->aggregate_index = GenerateTableIndex();
226: 	result->groupings_index = GenerateTableIndex();
227: 	result->window_index = GenerateTableIndex();
228: 	result->unnest_index = GenerateTableIndex();
229: 	result->prune_index = GenerateTableIndex();
230: 
231: 	// first bind the FROM table statement
232: 	result->from_table = Bind(*statement.from_table);
233: 
234: 	// bind the sample clause
235: 	if (statement.sample) {
236: 		result->sample_options = move(statement.sample);
237: 	}
238: 
239: 	// visit the select list and expand any "*" statements
240: 	vector<unique_ptr<ParsedExpression>> new_select_list;
241: 	for (auto &select_element : statement.select_list) {
242: 		if (select_element->GetExpressionType() == ExpressionType::STAR) {
243: 			// * statement, expand to all columns from the FROM clause
244: 			bind_context.GenerateAllColumnExpressions((StarExpression &)*select_element, new_select_list);
245: 		} else {
246: 			// regular statement, add it to the list
247: 			new_select_list.push_back(move(select_element));
248: 		}
249: 	}
250: 	if (new_select_list.empty()) {
251: 		throw BinderException("SELECT list is empty after resolving * expressions!");
252: 	}
253: 	statement.select_list = move(new_select_list);
254: 
255: 	// create a mapping of (alias -> index) and a mapping of (Expression -> index) for the SELECT list
256: 	unordered_map<string, idx_t> alias_map;
257: 	expression_map_t<idx_t> projection_map;
258: 	for (idx_t i = 0; i < statement.select_list.size(); i++) {
259: 		auto &expr = statement.select_list[i];
260: 		result->names.push_back(expr->GetName());
261: 		ExpressionBinder::QualifyColumnNames(*this, expr);
262: 		if (!expr->alias.empty()) {
263: 			alias_map[expr->alias] = i;
264: 			result->names[i] = expr->alias;
265: 		}
266: 		projection_map[expr.get()] = i;
267: 		result->original_expressions.push_back(expr->Copy());
268: 	}
269: 	result->column_count = statement.select_list.size();
270: 
271: 	// first visit the WHERE clause
272: 	// the WHERE clause happens before the GROUP BY, PROJECTION or HAVING clauses
273: 	if (statement.where_clause) {
274: 		ColumnAliasBinder alias_binder(*result, alias_map);
275: 		WhereBinder where_binder(*this, context, &alias_binder);
276: 		unique_ptr<ParsedExpression> condition = move(statement.where_clause);
277: 		result->where_clause = where_binder.Bind(condition);
278: 	}
279: 
280: 	// now bind all the result modifiers; including DISTINCT and ORDER BY targets
281: 	OrderBinder order_binder({this}, result->projection_index, statement, alias_map, projection_map);
282: 	BindModifiers(order_binder, statement, *result);
283: 
284: 	vector<unique_ptr<ParsedExpression>> unbound_groups;
285: 	BoundGroupInformation info;
286: 	auto &group_expressions = statement.groups.group_expressions;
287: 	if (!group_expressions.empty()) {
288: 		// the statement has a GROUP BY clause, bind it
289: 		unbound_groups.resize(group_expressions.size());
290: 		GroupBinder group_binder(*this, context, statement, result->group_index, alias_map, info.alias_map);
291: 		for (idx_t i = 0; i < group_expressions.size(); i++) {
292: 
293: 			// we keep a copy of the unbound expression;
294: 			// we keep the unbound copy around to check for group references in the SELECT and HAVING clause
295: 			// the reason we want the unbound copy is because we want to figure out whether an expression
296: 			// is a group reference BEFORE binding in the SELECT/HAVING binder
297: 			group_binder.unbound_expression = group_expressions[i]->Copy();
298: 			group_binder.bind_index = i;
299: 
300: 			// bind the groups
301: 			LogicalType group_type;
302: 			auto bound_expr = group_binder.Bind(group_expressions[i], &group_type);
303: 			D_ASSERT(bound_expr->return_type.id() != LogicalTypeId::INVALID);
304: 
305: 			// push a potential collation, if necessary
306: 			bound_expr =
307: 			    ExpressionBinder::PushCollation(context, move(bound_expr), StringType::GetCollation(group_type), true);
308: 			result->groups.group_expressions.push_back(move(bound_expr));
309: 
310: 			// in the unbound expression we DO bind the table names of any ColumnRefs
311: 			// we do this to make sure that "table.a" and "a" are treated the same
312: 			// if we wouldn't do this then (SELECT test.a FROM test GROUP BY a) would not work because "test.a" <> "a"
313: 			// hence we convert "a" -> "test.a" in the unbound expression
314: 			unbound_groups[i] = move(group_binder.unbound_expression);
315: 			ExpressionBinder::QualifyColumnNames(*this, unbound_groups[i]);
316: 			info.map[unbound_groups[i].get()] = i;
317: 		}
318: 	}
319: 	result->groups.grouping_sets = move(statement.groups.grouping_sets);
320: 
321: 	// bind the HAVING clause, if any
322: 	if (statement.having) {
323: 		HavingBinder having_binder(*this, context, *result, info, alias_map);
324: 		ExpressionBinder::QualifyColumnNames(*this, statement.having);
325: 		result->having = having_binder.Bind(statement.having);
326: 	}
327: 
328: 	// bind the QUALIFY clause, if any
329: 	if (statement.qualify) {
330: 		QualifyBinder qualify_binder(*this, context, *result, info, alias_map);
331: 		ExpressionBinder::QualifyColumnNames(*this, statement.qualify);
332: 		result->qualify = qualify_binder.Bind(statement.qualify);
333: 	}
334: 
335: 	// after that, we bind to the SELECT list
336: 	SelectBinder select_binder(*this, context, *result, info);
337: 	vector<LogicalType> internal_sql_types;
338: 	for (idx_t i = 0; i < statement.select_list.size(); i++) {
339: 		LogicalType result_type;
340: 		auto expr = select_binder.Bind(statement.select_list[i], &result_type);
341: 		if (statement.aggregate_handling == AggregateHandling::FORCE_AGGREGATES && select_binder.HasBoundColumns()) {
342: 			if (select_binder.BoundAggregates()) {
343: 				throw BinderException("Cannot mix aggregates with non-aggregated columns!");
344: 			}
345: 			// we are forcing aggregates, and the node has columns bound
346: 			// this entry becomes a group
347: 			auto group_ref = make_unique<BoundColumnRefExpression>(
348: 			    expr->return_type, ColumnBinding(result->group_index, result->groups.group_expressions.size()));
349: 			result->groups.group_expressions.push_back(move(expr));
350: 			expr = move(group_ref);
351: 		}
352: 		result->select_list.push_back(move(expr));
353: 		if (i < result->column_count) {
354: 			result->types.push_back(result_type);
355: 		}
356: 		internal_sql_types.push_back(result_type);
357: 		if (statement.aggregate_handling == AggregateHandling::FORCE_AGGREGATES) {
358: 			select_binder.ResetBindings();
359: 		}
360: 	}
361: 	result->need_prune = result->select_list.size() > result->column_count;
362: 
363: 	// in the normal select binder, we bind columns as if there is no aggregation
364: 	// i.e. in the query [SELECT i, SUM(i) FROM integers;] the "i" will be bound as a normal column
365: 	// since we have an aggregation, we need to either (1) throw an error, or (2) wrap the column in a FIRST() aggregate
366: 	// we choose the former one [CONTROVERSIAL: this is the PostgreSQL behavior]
367: 	if (!result->groups.group_expressions.empty() || !result->aggregates.empty() || statement.having ||
368: 	    !result->groups.grouping_sets.empty()) {
369: 		if (statement.aggregate_handling == AggregateHandling::NO_AGGREGATES_ALLOWED) {
370: 			throw BinderException("Aggregates cannot be present in a Project relation!");
371: 		} else if (statement.aggregate_handling == AggregateHandling::STANDARD_HANDLING) {
372: 			if (select_binder.HasBoundColumns()) {
373: 				auto &bound_columns = select_binder.GetBoundColumns();
374: 				throw BinderException(
375: 				    FormatError(bound_columns[0].query_location,
376: 				                "column \"%s\" must appear in the GROUP BY clause or be used in an aggregate function",
377: 				                bound_columns[0].name));
378: 			}
379: 		}
380: 	}
381: 
382: 	// QUALIFY clause requires at least one window function to be specified in at least one of the SELECT column list or
383: 	// the filter predicate of the QUALIFY clause
384: 	if (statement.qualify && result->windows.empty()) {
385: 		throw BinderException("at least one window function must appear in the SELECT column or QUALIFY clause");
386: 	}
387: 
388: 	// now that the SELECT list is bound, we set the types of DISTINCT/ORDER BY expressions
389: 	BindModifierTypes(*result, internal_sql_types, result->projection_index);
390: 	return move(result);
391: }
392: 
393: } // namespace duckdb
[end of src/planner/binder/query_node/bind_select_node.cpp]
[start of src/planner/binder/query_node/bind_setop_node.cpp]
1: #include "duckdb/parser/expression/columnref_expression.hpp"
2: #include "duckdb/parser/expression/constant_expression.hpp"
3: #include "duckdb/parser/expression_map.hpp"
4: #include "duckdb/parser/query_node/select_node.hpp"
5: #include "duckdb/parser/query_node/set_operation_node.hpp"
6: #include "duckdb/planner/binder.hpp"
7: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
8: #include "duckdb/planner/query_node/bound_set_operation_node.hpp"
9: #include "duckdb/planner/query_node/bound_select_node.hpp"
10: #include "duckdb/planner/expression_binder/order_binder.hpp"
11: 
12: namespace duckdb {
13: 
14: static void GatherAliases(BoundQueryNode &node, unordered_map<string, idx_t> &aliases,
15:                           expression_map_t<idx_t> &expressions) {
16: 	if (node.type == QueryNodeType::SET_OPERATION_NODE) {
17: 		// setop, recurse
18: 		auto &setop = (BoundSetOperationNode &)node;
19: 		GatherAliases(*setop.left, aliases, expressions);
20: 		GatherAliases(*setop.right, aliases, expressions);
21: 	} else {
22: 		// query node
23: 		D_ASSERT(node.type == QueryNodeType::SELECT_NODE);
24: 		auto &select = (BoundSelectNode &)node;
25: 		// fill the alias lists
26: 		for (idx_t i = 0; i < select.names.size(); i++) {
27: 			auto &name = select.names[i];
28: 			auto &expr = select.original_expressions[i];
29: 			// first check if the alias is already in there
30: 			auto entry = aliases.find(name);
31: 			if (entry != aliases.end()) {
32: 				// the alias already exists
33: 				// check if there is a conflict
34: 				if (entry->second != i) {
35: 					// there is a conflict
36: 					// we place "-1" in the aliases map at this location
37: 					// "-1" signifies that there is an ambiguous reference
38: 					aliases[name] = DConstants::INVALID_INDEX;
39: 				}
40: 			} else {
41: 				// the alias is not in there yet, just assign it
42: 				aliases[name] = i;
43: 			}
44: 			// now check if the node is already in the set of expressions
45: 			auto expr_entry = expressions.find(expr.get());
46: 			if (expr_entry != expressions.end()) {
47: 				// the node is in there
48: 				// repeat the same as with the alias: if there is an ambiguity we insert "-1"
49: 				if (expr_entry->second != i) {
50: 					expressions[expr.get()] = DConstants::INVALID_INDEX;
51: 				}
52: 			} else {
53: 				// not in there yet, just place it in there
54: 				expressions[expr.get()] = i;
55: 			}
56: 		}
57: 	}
58: }
59: 
60: unique_ptr<BoundQueryNode> Binder::BindNode(SetOperationNode &statement) {
61: 	auto result = make_unique<BoundSetOperationNode>();
62: 	result->setop_type = statement.setop_type;
63: 
64: 	// first recursively visit the set operations
65: 	// both the left and right sides have an independent BindContext and Binder
66: 	D_ASSERT(statement.left);
67: 	D_ASSERT(statement.right);
68: 
69: 	result->setop_index = GenerateTableIndex();
70: 
71: 	result->left_binder = Binder::CreateBinder(context, this);
72: 	result->left_binder->can_contain_nulls = true;
73: 	result->left = result->left_binder->BindNode(*statement.left);
74: 
75: 	result->right_binder = Binder::CreateBinder(context, this);
76: 	result->right_binder->can_contain_nulls = true;
77: 	result->right = result->right_binder->BindNode(*statement.right);
78: 
79: 	if (!statement.modifiers.empty()) {
80: 		// handle the ORDER BY/DISTINCT clauses
81: 
82: 		// we recursively visit the children of this node to extract aliases and expressions that can be referenced in
83: 		// the ORDER BY
84: 		unordered_map<string, idx_t> alias_map;
85: 		expression_map_t<idx_t> expression_map;
86: 		GatherAliases(*result, alias_map, expression_map);
87: 
88: 		// now we perform the actual resolution of the ORDER BY/DISTINCT expressions
89: 		OrderBinder order_binder({result->left_binder.get(), result->right_binder.get()}, result->setop_index,
90: 		                         alias_map, expression_map, statement.left->GetSelectList().size());
91: 		BindModifiers(order_binder, statement, *result);
92: 	}
93: 
94: 	result->names = result->left->names;
95: 
96: 	// move the correlated expressions from the child binders to this binder
97: 	MoveCorrelatedExpressions(*result->left_binder);
98: 	MoveCorrelatedExpressions(*result->right_binder);
99: 
100: 	// now both sides have been bound we can resolve types
101: 	if (result->left->types.size() != result->right->types.size()) {
102: 		throw BinderException("Set operations can only apply to expressions with the "
103: 		                      "same number of result columns");
104: 	}
105: 
106: 	// figure out the types of the setop result by picking the max of both
107: 	for (idx_t i = 0; i < result->left->types.size(); i++) {
108: 		auto result_type = LogicalType::MaxLogicalType(result->left->types[i], result->right->types[i]);
109: 		if (!can_contain_nulls) {
110: 			if (ExpressionBinder::ContainsNullType(result_type)) {
111: 				result_type = ExpressionBinder::ExchangeNullType(result_type);
112: 			}
113: 		}
114: 		result->types.push_back(result_type);
115: 	}
116: 
117: 	// finally bind the types of the ORDER/DISTINCT clause expressions
118: 	BindModifierTypes(*result, result->types, result->setop_index);
119: 	return move(result);
120: }
121: 
122: } // namespace duckdb
[end of src/planner/binder/query_node/bind_setop_node.cpp]
[start of src/planner/binder/statement/bind_insert.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/expression/constant_expression.hpp"
3: #include "duckdb/parser/statement/insert_statement.hpp"
4: #include "duckdb/parser/query_node/select_node.hpp"
5: #include "duckdb/parser/tableref/expressionlistref.hpp"
6: #include "duckdb/planner/binder.hpp"
7: #include "duckdb/planner/expression_binder/insert_binder.hpp"
8: #include "duckdb/planner/operator/logical_insert.hpp"
9: #include "duckdb/common/string_util.hpp"
10: 
11: namespace duckdb {
12: 
13: static void CheckInsertColumnCountMismatch(int64_t expected_columns, int64_t result_columns, bool columns_provided,
14:                                            const char *tname) {
15: 	if (result_columns != expected_columns) {
16: 		string msg = StringUtil::Format(!columns_provided ? "table %s has %lld columns but %lld values were supplied"
17: 		                                                  : "Column name/value mismatch for insert on %s: "
18: 		                                                    "expected %lld columns but %lld values were supplied",
19: 		                                tname, expected_columns, result_columns);
20: 		throw BinderException(msg);
21: 	}
22: }
23: 
24: BoundStatement Binder::Bind(InsertStatement &stmt) {
25: 	BoundStatement result;
26: 	result.names = {"Count"};
27: 	result.types = {LogicalType::BIGINT};
28: 
29: 	auto table = Catalog::GetCatalog(context).GetEntry<TableCatalogEntry>(context, stmt.schema, stmt.table);
30: 	D_ASSERT(table);
31: 	if (!table->temporary) {
32: 		// inserting into a non-temporary table: alters underlying database
33: 		this->read_only = false;
34: 	}
35: 
36: 	auto insert = make_unique<LogicalInsert>(table);
37: 
38: 	vector<idx_t> named_column_map;
39: 	if (!stmt.columns.empty()) {
40: 		// insertion statement specifies column list
41: 
42: 		// create a mapping of (list index) -> (column index)
43: 		unordered_map<string, idx_t> column_name_map;
44: 		for (idx_t i = 0; i < stmt.columns.size(); i++) {
45: 			column_name_map[stmt.columns[i]] = i;
46: 			auto entry = table->name_map.find(stmt.columns[i]);
47: 			if (entry == table->name_map.end()) {
48: 				throw BinderException("Column %s not found in table %s", stmt.columns[i], table->name);
49: 			}
50: 			if (entry->second == COLUMN_IDENTIFIER_ROW_ID) {
51: 				throw BinderException("Cannot explicitly insert values into rowid column");
52: 			}
53: 			insert->expected_types.push_back(table->columns[entry->second].type);
54: 			named_column_map.push_back(entry->second);
55: 		}
56: 		for (idx_t i = 0; i < table->columns.size(); i++) {
57: 			auto &col = table->columns[i];
58: 			auto entry = column_name_map.find(col.name);
59: 			if (entry == column_name_map.end()) {
60: 				// column not specified, set index to DConstants::INVALID_INDEX
61: 				insert->column_index_map.push_back(DConstants::INVALID_INDEX);
62: 			} else {
63: 				// column was specified, set to the index
64: 				insert->column_index_map.push_back(entry->second);
65: 			}
66: 		}
67: 	} else {
68: 		for (idx_t i = 0; i < table->columns.size(); i++) {
69: 			insert->expected_types.push_back(table->columns[i].type);
70: 		}
71: 	}
72: 
73: 	// bind the default values
74: 	BindDefaultValues(table->columns, insert->bound_defaults);
75: 	if (!stmt.select_statement) {
76: 		result.plan = move(insert);
77: 		return result;
78: 	}
79: 
80: 	idx_t expected_columns = stmt.columns.empty() ? table->columns.size() : stmt.columns.size();
81: 	// special case: check if we are inserting from a VALUES statement
82: 	if (stmt.select_statement->node->type == QueryNodeType::SELECT_NODE) {
83: 		auto &node = (SelectNode &)*stmt.select_statement->node;
84: 		if (node.from_table->type == TableReferenceType::EXPRESSION_LIST) {
85: 			auto &expr_list = (ExpressionListRef &)*node.from_table;
86: 			expr_list.expected_types.resize(expected_columns);
87: 			expr_list.expected_names.resize(expected_columns);
88: 
89: 			D_ASSERT(expr_list.values.size() > 0);
90: 			CheckInsertColumnCountMismatch(expected_columns, expr_list.values[0].size(), !stmt.columns.empty(),
91: 			                               table->name.c_str());
92: 
93: 			// VALUES list!
94: 			for (idx_t col_idx = 0; col_idx < expected_columns; col_idx++) {
95: 				idx_t table_col_idx = stmt.columns.empty() ? col_idx : named_column_map[col_idx];
96: 				D_ASSERT(table_col_idx < table->columns.size());
97: 
98: 				// set the expected types as the types for the INSERT statement
99: 				auto &column = table->columns[table_col_idx];
100: 				expr_list.expected_types[col_idx] = column.type;
101: 				expr_list.expected_names[col_idx] = column.name;
102: 
103: 				// now replace any DEFAULT values with the corresponding default expression
104: 				for (idx_t list_idx = 0; list_idx < expr_list.values.size(); list_idx++) {
105: 					if (expr_list.values[list_idx][col_idx]->type == ExpressionType::VALUE_DEFAULT) {
106: 						// DEFAULT value! replace the entry
107: 						if (column.default_value) {
108: 							expr_list.values[list_idx][col_idx] = column.default_value->Copy();
109: 						} else {
110: 							expr_list.values[list_idx][col_idx] = make_unique<ConstantExpression>(Value(column.type));
111: 						}
112: 					}
113: 				}
114: 			}
115: 		}
116: 	}
117: 
118: 	// insert from select statement
119: 	// parse select statement and add to logical plan
120: 	auto root_select = Bind(*stmt.select_statement);
121: 	CheckInsertColumnCountMismatch(expected_columns, root_select.types.size(), !stmt.columns.empty(),
122: 	                               table->name.c_str());
123: 
124: 	auto root = CastLogicalOperatorToTypes(root_select.types, insert->expected_types, move(root_select.plan));
125: 	insert->AddChild(move(root));
126: 
127: 	result.plan = move(insert);
128: 	this->allow_stream_result = false;
129: 	return result;
130: }
131: 
132: } // namespace duckdb
[end of src/planner/binder/statement/bind_insert.cpp]
[start of src/planner/binder/tableref/bind_named_parameters.cpp]
1: #include "duckdb/planner/binder.hpp"
2: 
3: namespace duckdb {
4: 
5: void Binder::BindNamedParameters(unordered_map<string, LogicalType> &types, unordered_map<string, Value> &values,
6:                                  QueryErrorContext &error_context, string &func_name) {
7: 	for (auto &kv : values) {
8: 		auto entry = types.find(kv.first);
9: 		if (entry == types.end()) {
10: 			// create a list of named parameters for the error
11: 			string named_params;
12: 			for (auto &kv : types) {
13: 				named_params += "    ";
14: 				named_params += kv.first;
15: 				named_params += " ";
16: 				named_params += kv.second.ToString();
17: 				named_params += "\n";
18: 			}
19: 			string error_msg;
20: 			if (named_params.empty()) {
21: 				error_msg = "Function does not accept any named parameters.";
22: 			} else {
23: 				error_msg = "Candidates: " + named_params;
24: 			}
25: 			throw BinderException(error_context.FormatError("Invalid named parameter \"%s\" for function %s\n%s",
26: 			                                                kv.first, func_name, error_msg));
27: 		}
28: 		if (entry->second.id() != LogicalTypeId::ANY) {
29: 			kv.second = kv.second.CastAs(entry->second);
30: 		}
31: 	}
32: }
33: 
34: } // namespace duckdb
[end of src/planner/binder/tableref/bind_named_parameters.cpp]
[start of src/planner/binder/tableref/bind_table_function.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/expression/function_expression.hpp"
3: #include "duckdb/parser/tableref/table_function_ref.hpp"
4: #include "duckdb/planner/binder.hpp"
5: #include "duckdb/parser/expression/columnref_expression.hpp"
6: #include "duckdb/parser/expression/comparison_expression.hpp"
7: #include "duckdb/planner/expression_binder/constant_binder.hpp"
8: #include "duckdb/planner/expression_binder/select_binder.hpp"
9: #include "duckdb/planner/operator/logical_get.hpp"
10: #include "duckdb/planner/tableref/bound_table_function.hpp"
11: #include "duckdb/planner/tableref/bound_subqueryref.hpp"
12: #include "duckdb/planner/query_node/bound_select_node.hpp"
13: #include "duckdb/execution/expression_executor.hpp"
14: #include "duckdb/common/algorithm.hpp"
15: #include "duckdb/parser/expression/subquery_expression.hpp"
16: 
17: namespace duckdb {
18: 
19: bool Binder::BindFunctionParameters(vector<unique_ptr<ParsedExpression>> &expressions, vector<LogicalType> &arguments,
20:                                     vector<Value> &parameters, unordered_map<string, Value> &named_parameters,
21:                                     unique_ptr<BoundSubqueryRef> &subquery, string &error) {
22: 	bool seen_subquery = false;
23: 	for (auto &child : expressions) {
24: 		string parameter_name;
25: 
26: 		// hack to make named parameters work
27: 		if (child->type == ExpressionType::COMPARE_EQUAL) {
28: 			// comparison, check if the LHS is a columnref
29: 			auto &comp = (ComparisonExpression &)*child;
30: 			if (comp.left->type == ExpressionType::COLUMN_REF) {
31: 				auto &colref = (ColumnRefExpression &)*comp.left;
32: 				if (!colref.IsQualified()) {
33: 					parameter_name = colref.GetColumnName();
34: 					child = move(comp.right);
35: 				}
36: 			}
37: 		}
38: 		if (child->type == ExpressionType::SUBQUERY) {
39: 			if (seen_subquery) {
40: 				error = "Table function can have at most one subquery parameter ";
41: 				return false;
42: 			}
43: 			auto binder = Binder::CreateBinder(this->context, this, true);
44: 			auto &se = (SubqueryExpression &)*child;
45: 			auto node = binder->BindNode(*se.subquery->node);
46: 			subquery = make_unique<BoundSubqueryRef>(move(binder), move(node));
47: 			seen_subquery = true;
48: 			arguments.emplace_back(LogicalTypeId::TABLE);
49: 			continue;
50: 		}
51: 		ConstantBinder binder(*this, context, "TABLE FUNCTION parameter");
52: 		LogicalType sql_type;
53: 		auto expr = binder.Bind(child, &sql_type);
54: 		if (!expr->IsFoldable()) {
55: 			error = "Table function requires a constant parameter";
56: 			return false;
57: 		}
58: 		auto constant = ExpressionExecutor::EvaluateScalar(*expr);
59: 		if (parameter_name.empty()) {
60: 			// unnamed parameter
61: 			if (!named_parameters.empty()) {
62: 				error = "Unnamed parameters cannot come after named parameters";
63: 				return false;
64: 			}
65: 			arguments.emplace_back(sql_type);
66: 			parameters.emplace_back(move(constant));
67: 		} else {
68: 			named_parameters[parameter_name] = move(constant);
69: 		}
70: 	}
71: 	return true;
72: }
73: 
74: unique_ptr<BoundTableRef> Binder::Bind(TableFunctionRef &ref) {
75: 	QueryErrorContext error_context(root_statement, ref.query_location);
76: 	auto bind_index = GenerateTableIndex();
77: 
78: 	D_ASSERT(ref.function->type == ExpressionType::FUNCTION);
79: 	auto fexpr = (FunctionExpression *)ref.function.get();
80: 
81: 	// evaluate the input parameters to the function
82: 	vector<LogicalType> arguments;
83: 	vector<Value> parameters;
84: 	unordered_map<string, Value> named_parameters;
85: 	unique_ptr<BoundSubqueryRef> subquery;
86: 	string error;
87: 	if (!BindFunctionParameters(fexpr->children, arguments, parameters, named_parameters, subquery, error)) {
88: 		throw BinderException(FormatError(ref, error));
89: 	}
90: 
91: 	// fetch the function from the catalog
92: 	auto &catalog = Catalog::GetCatalog(context);
93: 	auto function =
94: 	    catalog.GetEntry<TableFunctionCatalogEntry>(context, fexpr->schema, fexpr->function_name, false, error_context);
95: 
96: 	// select the function based on the input parameters
97: 	idx_t best_function_idx = Function::BindFunction(function->name, function->functions, arguments, error);
98: 	if (best_function_idx == DConstants::INVALID_INDEX) {
99: 		throw BinderException(FormatError(ref, error));
100: 	}
101: 	auto &table_function = function->functions[best_function_idx];
102: 
103: 	// now check the named parameters
104: 	BindNamedParameters(table_function.named_parameters, named_parameters, error_context, table_function.name);
105: 
106: 	// cast the parameters to the type of the function
107: 	for (idx_t i = 0; i < arguments.size(); i++) {
108: 		if (table_function.arguments[i] != LogicalType::ANY && table_function.arguments[i] != LogicalType::TABLE &&
109: 		    table_function.arguments[i] != LogicalType::POINTER &&
110: 		    table_function.arguments[i].id() != LogicalTypeId::LIST) {
111: 			parameters[i] = parameters[i].CastAs(table_function.arguments[i]);
112: 		}
113: 	}
114: 
115: 	vector<LogicalType> input_table_types;
116: 	vector<string> input_table_names;
117: 
118: 	if (subquery) {
119: 		input_table_types = subquery->subquery->types;
120: 		input_table_names = subquery->subquery->names;
121: 	}
122: 
123: 	// perform the binding
124: 	unique_ptr<FunctionData> bind_data;
125: 	vector<LogicalType> return_types;
126: 	vector<string> return_names;
127: 	if (table_function.bind) {
128: 		bind_data = table_function.bind(context, parameters, named_parameters, input_table_types, input_table_names,
129: 		                                return_types, return_names);
130: 	}
131: 	D_ASSERT(return_types.size() == return_names.size());
132: 	D_ASSERT(return_types.size() > 0);
133: 	// overwrite the names with any supplied aliases
134: 	for (idx_t i = 0; i < ref.column_name_alias.size() && i < return_names.size(); i++) {
135: 		return_names[i] = ref.column_name_alias[i];
136: 	}
137: 	for (idx_t i = 0; i < return_names.size(); i++) {
138: 		if (return_names[i].empty()) {
139: 			return_names[i] = "C" + to_string(i);
140: 		}
141: 	}
142: 	auto get = make_unique<LogicalGet>(bind_index, table_function, move(bind_data), return_types, return_names);
143: 	// now add the table function to the bind context so its columns can be bound
144: 	bind_context.AddTableFunction(bind_index, ref.alias.empty() ? fexpr->function_name : ref.alias, return_names,
145: 	                              return_types, *get);
146: 	if (subquery) {
147: 		get->children.push_back(Binder::CreatePlan(*subquery));
148: 	}
149: 
150: 	return make_unique_base<BoundTableRef, BoundTableFunction>(move(get));
151: }
152: 
153: } // namespace duckdb
[end of src/planner/binder/tableref/bind_table_function.cpp]
[start of src/planner/expression_binder/column_alias_binder.cpp]
1: #include "duckdb/planner/expression_binder/column_alias_binder.hpp"
2: 
3: #include "duckdb/parser/expression/columnref_expression.hpp"
4: #include "duckdb/planner/query_node/bound_select_node.hpp"
5: #include "duckdb/common/string_util.hpp"
6: 
7: namespace duckdb {
8: 
9: ColumnAliasBinder::ColumnAliasBinder(BoundSelectNode &node, const unordered_map<string, idx_t> &alias_map)
10:     : node(node), alias_map(alias_map), in_alias(false) {
11: }
12: 
13: BindResult ColumnAliasBinder::BindAlias(ExpressionBinder &enclosing_binder, ColumnRefExpression &expr, idx_t depth,
14:                                         bool root_expression) {
15: 	if (expr.IsQualified()) {
16: 		return BindResult(StringUtil::Format("Alias %s cannot be qualified.", expr.ToString()));
17: 	}
18: 
19: 	auto alias_entry = alias_map.find(expr.column_names[0]);
20: 	if (alias_entry == alias_map.end()) {
21: 		return BindResult(StringUtil::Format("Alias %s is not found.", expr.ToString()));
22: 	}
23: 
24: 	// found an alias: bind the alias expression
25: 	D_ASSERT(!in_alias);
26: 	auto expression = node.original_expressions[alias_entry->second]->Copy();
27: 	in_alias = true;
28: 	auto result = enclosing_binder.BindExpression(&expression, depth, root_expression);
29: 	in_alias = false;
30: 	return result;
31: }
32: 
33: } // namespace duckdb
[end of src/planner/expression_binder/column_alias_binder.cpp]
[start of src/planner/expression_binder/group_binder.cpp]
1: #include "duckdb/planner/expression_binder/group_binder.hpp"
2: 
3: #include "duckdb/parser/expression/columnref_expression.hpp"
4: #include "duckdb/parser/expression/constant_expression.hpp"
5: #include "duckdb/parser/query_node/select_node.hpp"
6: #include "duckdb/planner/expression/bound_constant_expression.hpp"
7: #include "duckdb/common/to_string.hpp"
8: 
9: namespace duckdb {
10: 
11: GroupBinder::GroupBinder(Binder &binder, ClientContext &context, SelectNode &node, idx_t group_index,
12:                          unordered_map<string, idx_t> &alias_map, unordered_map<string, idx_t> &group_alias_map)
13:     : ExpressionBinder(binder, context), node(node), alias_map(alias_map), group_alias_map(group_alias_map),
14:       group_index(group_index) {
15: }
16: 
17: BindResult GroupBinder::BindExpression(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth, bool root_expression) {
18: 	auto &expr = **expr_ptr;
19: 	if (root_expression && depth == 0) {
20: 		switch (expr.expression_class) {
21: 		case ExpressionClass::COLUMN_REF:
22: 			return BindColumnRef((ColumnRefExpression &)expr);
23: 		case ExpressionClass::CONSTANT:
24: 			return BindConstant((ConstantExpression &)expr);
25: 		default:
26: 			break;
27: 		}
28: 	}
29: 	switch (expr.expression_class) {
30: 	case ExpressionClass::DEFAULT:
31: 		return BindResult("GROUP BY clause cannot contain DEFAULT clause");
32: 	case ExpressionClass::WINDOW:
33: 		return BindResult("GROUP BY clause cannot contain window functions!");
34: 	default:
35: 		return ExpressionBinder::BindExpression(expr_ptr, depth);
36: 	}
37: }
38: 
39: string GroupBinder::UnsupportedAggregateMessage() {
40: 	return "GROUP BY clause cannot contain aggregates!";
41: }
42: 
43: BindResult GroupBinder::BindSelectRef(idx_t entry) {
44: 	if (used_aliases.find(entry) != used_aliases.end()) {
45: 		// the alias has already been bound to before!
46: 		// this happens if we group on the same alias twice
47: 		// e.g. GROUP BY k, k or GROUP BY 1, 1
48: 		// in this case, we can just replace the grouping with a constant since the second grouping has no effect
49: 		// (the constant grouping will be optimized out later)
50: 		return BindResult(make_unique<BoundConstantExpression>(Value::INTEGER(42)));
51: 	}
52: 	if (entry >= node.select_list.size()) {
53: 		throw BinderException("GROUP BY term out of range - should be between 1 and %d", (int)node.select_list.size());
54: 	}
55: 	// we replace the root expression, also replace the unbound expression
56: 	unbound_expression = node.select_list[entry]->Copy();
57: 	// move the expression that this refers to here and bind it
58: 	auto select_entry = move(node.select_list[entry]);
59: 	auto binding = Bind(select_entry, nullptr, false);
60: 	// now replace the original expression in the select list with a reference to this group
61: 	group_alias_map[to_string(entry)] = bind_index;
62: 	node.select_list[entry] = make_unique<ColumnRefExpression>(to_string(entry));
63: 	// insert into the set of used aliases
64: 	used_aliases.insert(entry);
65: 	return BindResult(move(binding));
66: }
67: 
68: BindResult GroupBinder::BindConstant(ConstantExpression &constant) {
69: 	// constant as root expression
70: 	if (!constant.value.type().IsIntegral()) {
71: 		// non-integral expression, we just leave the constant here.
72: 		return ExpressionBinder::BindExpression(constant, 0);
73: 	}
74: 	// INTEGER constant: we use the integer as an index into the select list (e.g. GROUP BY 1)
75: 	auto index = (idx_t)constant.value.GetValue<int64_t>();
76: 	return BindSelectRef(index - 1);
77: }
78: 
79: BindResult GroupBinder::BindColumnRef(ColumnRefExpression &colref) {
80: 	// columns in GROUP BY clauses:
81: 	// FIRST refer to the original tables, and
82: 	// THEN if no match is found refer to aliases in the SELECT list
83: 	// THEN if no match is found, refer to outer queries
84: 
85: 	// first try to bind to the base columns (original tables)
86: 	auto result = ExpressionBinder::BindExpression(colref, 0);
87: 	if (result.HasError()) {
88: 		if (colref.IsQualified()) {
89: 			// explicit table name: not an alias reference
90: 			return result;
91: 		}
92: 		// failed to bind the column and the node is the root expression with depth = 0
93: 		// check if refers to an alias in the select clause
94: 		auto alias_name = colref.column_names[0];
95: 		auto entry = alias_map.find(alias_name);
96: 		if (entry == alias_map.end()) {
97: 			// no matching alias found
98: 			return result;
99: 		}
100: 		result = BindResult(BindSelectRef(entry->second));
101: 		if (!result.HasError()) {
102: 			group_alias_map[alias_name] = bind_index;
103: 		}
104: 	}
105: 	return result;
106: }
107: 
108: } // namespace duckdb
[end of src/planner/expression_binder/group_binder.cpp]
[start of src/planner/expression_binder/having_binder.cpp]
1: #include "duckdb/planner/expression_binder/having_binder.hpp"
2: 
3: #include "duckdb/parser/expression/columnref_expression.hpp"
4: #include "duckdb/planner/binder.hpp"
5: #include "duckdb/planner/expression_binder/aggregate_binder.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/planner/query_node/bound_select_node.hpp"
8: 
9: namespace duckdb {
10: 
11: HavingBinder::HavingBinder(Binder &binder, ClientContext &context, BoundSelectNode &node, BoundGroupInformation &info,
12:                            unordered_map<string, idx_t> &alias_map)
13:     : SelectBinder(binder, context, node, info), column_alias_binder(node, alias_map) {
14: 	target_type = LogicalType(LogicalTypeId::BOOLEAN);
15: }
16: 
17: BindResult HavingBinder::BindColumnRef(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth, bool root_expression) {
18: 	auto &expr = (ColumnRefExpression &)**expr_ptr;
19: 	auto alias_result = column_alias_binder.BindAlias(*this, expr, depth, root_expression);
20: 	if (!alias_result.HasError()) {
21: 		return alias_result;
22: 	}
23: 
24: 	return BindResult(StringUtil::Format(
25: 	    "column %s must appear in the GROUP BY clause or be used in an aggregate function", expr.ToString()));
26: }
27: 
28: BindResult HavingBinder::BindExpression(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth, bool root_expression) {
29: 	auto &expr = **expr_ptr;
30: 	// check if the expression binds to one of the groups
31: 	auto group_index = TryBindGroup(expr, depth);
32: 	if (group_index != DConstants::INVALID_INDEX) {
33: 		return BindGroup(expr, depth, group_index);
34: 	}
35: 	switch (expr.expression_class) {
36: 	case ExpressionClass::WINDOW:
37: 		return BindResult("HAVING clause cannot contain window functions!");
38: 	case ExpressionClass::COLUMN_REF:
39: 		return BindColumnRef(expr_ptr, depth, root_expression);
40: 	default:
41: 		return duckdb::SelectBinder::BindExpression(expr_ptr, depth);
42: 	}
43: }
44: 
45: } // namespace duckdb
[end of src/planner/expression_binder/having_binder.cpp]
[start of src/planner/expression_binder/order_binder.cpp]
1: #include "duckdb/planner/expression_binder/order_binder.hpp"
2: 
3: #include "duckdb/parser/expression/columnref_expression.hpp"
4: #include "duckdb/parser/expression/positional_reference_expression.hpp"
5: #include "duckdb/parser/expression/constant_expression.hpp"
6: #include "duckdb/parser/expression/star_expression.hpp"
7: #include "duckdb/parser/query_node/select_node.hpp"
8: #include "duckdb/planner/expression_binder.hpp"
9: 
10: namespace duckdb {
11: 
12: OrderBinder::OrderBinder(vector<Binder *> binders, idx_t projection_index, unordered_map<string, idx_t> &alias_map,
13:                          expression_map_t<idx_t> &projection_map, idx_t max_count)
14:     : binders(move(binders)), projection_index(projection_index), max_count(max_count), extra_list(nullptr),
15:       alias_map(alias_map), projection_map(projection_map) {
16: }
17: OrderBinder::OrderBinder(vector<Binder *> binders, idx_t projection_index, SelectNode &node,
18:                          unordered_map<string, idx_t> &alias_map, expression_map_t<idx_t> &projection_map)
19:     : binders(move(binders)), projection_index(projection_index), alias_map(alias_map), projection_map(projection_map) {
20: 	this->max_count = node.select_list.size();
21: 	this->extra_list = &node.select_list;
22: }
23: 
24: unique_ptr<Expression> OrderBinder::CreateProjectionReference(ParsedExpression &expr, idx_t index) {
25: 	return make_unique<BoundColumnRefExpression>(expr.GetName(), LogicalType::INVALID,
26: 	                                             ColumnBinding(projection_index, index));
27: }
28: 
29: unique_ptr<Expression> OrderBinder::Bind(unique_ptr<ParsedExpression> expr) {
30: 	// in the ORDER BY clause we do not bind children
31: 	// we bind ONLY to the select list
32: 	// if there is no matching entry in the SELECT list already, we add the expression to the SELECT list and refer the
33: 	// new expression the new entry will then be bound later during the binding of the SELECT list we also don't do type
34: 	// resolution here: this only happens after the SELECT list has been bound
35: 	switch (expr->expression_class) {
36: 	case ExpressionClass::CONSTANT: {
37: 		// ORDER BY constant
38: 		// is the ORDER BY expression a constant integer? (e.g. ORDER BY 1)
39: 		auto &constant = (ConstantExpression &)*expr;
40: 		// ORDER BY a constant
41: 		if (!constant.value.type().IsIntegral()) {
42: 			// non-integral expression, we just leave the constant here.
43: 			// ORDER BY <constant> has no effect
44: 			// CONTROVERSIAL: maybe we should throw an error
45: 			return nullptr;
46: 		}
47: 		// INTEGER constant: we use the integer as an index into the select list (e.g. ORDER BY 1)
48: 		auto index = (idx_t)constant.value.GetValue<int64_t>();
49: 		if (index < 1 || index > max_count) {
50: 			throw BinderException("ORDER term out of range - should be between 1 and %lld", (idx_t)max_count);
51: 		}
52: 		return CreateProjectionReference(*expr, index - 1);
53: 	}
54: 	case ExpressionClass::COLUMN_REF: {
55: 		// COLUMN REF expression
56: 		// check if we can bind it to an alias in the select list
57: 		auto &colref = (ColumnRefExpression &)*expr;
58: 		// if there is an explicit table name we can't bind to an alias
59: 		if (colref.IsQualified()) {
60: 			break;
61: 		}
62: 		// check the alias list
63: 		auto entry = alias_map.find(colref.column_names[0]);
64: 		if (entry != alias_map.end()) {
65: 			// it does! point it to that entry
66: 			return CreateProjectionReference(*expr, entry->second);
67: 		}
68: 		break;
69: 	}
70: 	case ExpressionClass::POSITIONAL_REFERENCE: {
71: 		auto &posref = (PositionalReferenceExpression &)*expr;
72: 		return CreateProjectionReference(*expr, posref.index - 1);
73: 	}
74: 	default:
75: 		break;
76: 	}
77: 	// general case
78: 	// first bind the table names of this entry
79: 	for (auto &binder : binders) {
80: 		ExpressionBinder::QualifyColumnNames(*binder, expr);
81: 	}
82: 	// first check if the ORDER BY clause already points to an entry in the projection list
83: 	auto entry = projection_map.find(expr.get());
84: 	if (entry != projection_map.end()) {
85: 		if (entry->second == DConstants::INVALID_INDEX) {
86: 			throw BinderException("Ambiguous reference to column");
87: 		}
88: 		// there is a matching entry in the projection list
89: 		// just point to that entry
90: 		return CreateProjectionReference(*expr, entry->second);
91: 	}
92: 	if (!extra_list) {
93: 		// no extra list specified: we cannot push an extra ORDER BY clause
94: 		throw BinderException("Could not ORDER BY column \"%s\": add the expression/function to every SELECT, or move "
95: 		                      "the UNION into a FROM clause.",
96: 		                      expr->ToString());
97: 	}
98: 	// otherwise we need to push the ORDER BY entry into the select list
99: 	auto result = CreateProjectionReference(*expr, extra_list->size());
100: 	extra_list->push_back(move(expr));
101: 	return result;
102: }
103: 
104: } // namespace duckdb
[end of src/planner/expression_binder/order_binder.cpp]
[start of src/planner/expression_binder/qualify_binder.cpp]
1: #include "duckdb/planner/expression_binder/qualify_binder.hpp"
2: 
3: #include "duckdb/parser/expression/columnref_expression.hpp"
4: #include "duckdb/planner/binder.hpp"
5: #include "duckdb/planner/expression_binder/aggregate_binder.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/planner/query_node/bound_select_node.hpp"
8: 
9: namespace duckdb {
10: 
11: QualifyBinder::QualifyBinder(Binder &binder, ClientContext &context, BoundSelectNode &node, BoundGroupInformation &info,
12:                              unordered_map<string, idx_t> &alias_map)
13:     : SelectBinder(binder, context, node, info), column_alias_binder(node, alias_map) {
14: 	target_type = LogicalType(LogicalTypeId::BOOLEAN);
15: }
16: 
17: BindResult QualifyBinder::BindColumnRef(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth, bool root_expression) {
18: 	auto &expr = (ColumnRefExpression &)**expr_ptr;
19: 	auto result = duckdb::SelectBinder::BindExpression(expr_ptr, depth);
20: 	if (!result.HasError()) {
21: 		return result;
22: 	}
23: 
24: 	auto alias_result = column_alias_binder.BindAlias(*this, expr, depth, root_expression);
25: 	if (!alias_result.HasError()) {
26: 		return alias_result;
27: 	}
28: 
29: 	return BindResult(StringUtil::Format("Referenced column %s not found in FROM clause and can't find in alias map.",
30: 	                                     expr.ToString()));
31: }
32: 
33: BindResult QualifyBinder::BindExpression(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth, bool root_expression) {
34: 	auto &expr = **expr_ptr;
35: 	// check if the expression binds to one of the groups
36: 	auto group_index = TryBindGroup(expr, depth);
37: 	if (group_index != DConstants::INVALID_INDEX) {
38: 		return BindGroup(expr, depth, group_index);
39: 	}
40: 	switch (expr.expression_class) {
41: 	case ExpressionClass::WINDOW:
42: 		return BindWindow((WindowExpression &)expr, depth);
43: 	case ExpressionClass::COLUMN_REF:
44: 		return BindColumnRef(expr_ptr, depth, root_expression);
45: 	default:
46: 		return duckdb::SelectBinder::BindExpression(expr_ptr, depth);
47: 	}
48: }
49: 
50: } // namespace duckdb
[end of src/planner/expression_binder/qualify_binder.cpp]
[start of src/planner/pragma_handler.cpp]
1: #include "duckdb/planner/pragma_handler.hpp"
2: #include "duckdb/planner/binder.hpp"
3: #include "duckdb/parser/parser.hpp"
4: 
5: #include "duckdb/catalog/catalog.hpp"
6: #include "duckdb/catalog/catalog_entry/pragma_function_catalog_entry.hpp"
7: 
8: #include "duckdb/parser/parsed_data/pragma_info.hpp"
9: #include "duckdb/function/function.hpp"
10: 
11: #include "duckdb/main/client_context.hpp"
12: 
13: #include "duckdb/common/string_util.hpp"
14: #include "duckdb/common/file_system.hpp"
15: 
16: namespace duckdb {
17: 
18: PragmaHandler::PragmaHandler(ClientContext &context) : context(context) {
19: }
20: 
21: void PragmaHandler::HandlePragmaStatementsInternal(vector<unique_ptr<SQLStatement>> &statements) {
22: 	vector<unique_ptr<SQLStatement>> new_statements;
23: 	for (idx_t i = 0; i < statements.size(); i++) {
24: 		if (statements[i]->type == StatementType::PRAGMA_STATEMENT) {
25: 			// PRAGMA statement: check if we need to replace it by a new set of statements
26: 			PragmaHandler handler(context);
27: 			auto new_query = handler.HandlePragma(statements[i].get()); //*((PragmaStatement &)*statements[i]).info
28: 			if (!new_query.empty()) {
29: 				// this PRAGMA statement gets replaced by a new query string
30: 				// push the new query string through the parser again and add it to the transformer
31: 				Parser parser;
32: 				parser.ParseQuery(new_query);
33: 				// insert the new statements and remove the old statement
34: 				// FIXME: off by one here maybe?
35: 				for (idx_t j = 0; j < parser.statements.size(); j++) {
36: 					new_statements.push_back(move(parser.statements[j]));
37: 				}
38: 				continue;
39: 			}
40: 		}
41: 		new_statements.push_back(move(statements[i]));
42: 	}
43: 	statements = move(new_statements);
44: }
45: 
46: void PragmaHandler::HandlePragmaStatements(ClientContextLock &lock, vector<unique_ptr<SQLStatement>> &statements) {
47: 	// first check if there are any pragma statements
48: 	bool found_pragma = false;
49: 	for (idx_t i = 0; i < statements.size(); i++) {
50: 		if (statements[i]->type == StatementType::PRAGMA_STATEMENT) {
51: 			found_pragma = true;
52: 			break;
53: 		}
54: 	}
55: 	if (!found_pragma) {
56: 		// no pragmas: skip this step
57: 		return;
58: 	}
59: 	context.RunFunctionInTransactionInternal(lock, [&]() { HandlePragmaStatementsInternal(statements); });
60: }
61: 
62: string PragmaHandler::HandlePragma(SQLStatement *statement) { // PragmaInfo &info
63: 	auto info = *((PragmaStatement &)*statement).info;
64: 	auto entry =
65: 	    Catalog::GetCatalog(context).GetEntry<PragmaFunctionCatalogEntry>(context, DEFAULT_SCHEMA, info.name, false);
66: 	string error;
67: 	idx_t bound_idx = Function::BindFunction(entry->name, entry->functions, info, error);
68: 	if (bound_idx == DConstants::INVALID_INDEX) {
69: 		throw BinderException(error);
70: 	}
71: 	auto &bound_function = entry->functions[bound_idx];
72: 	if (bound_function.query) {
73: 		QueryErrorContext error_context(statement, statement->stmt_location);
74: 		Binder::BindNamedParameters(bound_function.named_parameters, info.named_parameters, error_context,
75: 		                            bound_function.name);
76: 		FunctionParameters parameters {info.parameters, info.named_parameters};
77: 		return bound_function.query(context, parameters);
78: 	}
79: 	return string();
80: }
81: 
82: } // namespace duckdb
[end of src/planner/pragma_handler.cpp]
[start of third_party/libpg_query/include/parser/scansup.hpp]
1: /*-------------------------------------------------------------------------
2:  *
3:  * scansup.h
4:  *	  scanner support routines.  used by both the bootstrap lexer
5:  * as well as the normal lexer
6:  *
7:  * Portions Copyright (c) 1996-2017, PostgreSQL Global Development PGGroup
8:  * Portions Copyright (c) 1994, Regents of the University of California
9:  *
10:  * src/include/parser/scansup.h
11:  *
12:  *-------------------------------------------------------------------------
13:  */
14: 
15: #pragma once
16: 
17: namespace duckdb_libpgquery {
18: 
19: char *scanstr(const char *s);
20: 
21: char *downcase_truncate_identifier(const char *ident, int len, bool warn);
22: 
23: char *downcase_identifier(const char *ident, int len, bool warn, bool truncate);
24: 
25: bool scanner_isspace(char ch);
26: 
27: }
[end of third_party/libpg_query/include/parser/scansup.hpp]
[start of third_party/libpg_query/include/postgres_parser.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // postgres_parser.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include <string>
12: #include <vector>
13: #include "nodes/pg_list.hpp"
14: #include "pg_simplified_token.hpp"
15: 
16: namespace duckdb {
17: class PostgresParser {
18: public:
19: 	PostgresParser();
20: 	~PostgresParser();
21: 
22: 	bool success;
23: 	duckdb_libpgquery::PGList *parse_tree;
24: 	std::string error_message;
25: 	int error_location;
26: public:
27: 	void Parse(const std::string &query);
28: 	static std::vector<duckdb_libpgquery::PGSimplifiedToken> Tokenize(const std::string &query);
29: 
30: 	static bool IsKeyword(const std::string &text);
31: 	static std::vector<duckdb_libpgquery::PGKeyword> KeywordList();
32: };
33: }
[end of third_party/libpg_query/include/postgres_parser.hpp]
[start of third_party/libpg_query/postgres_parser.cpp]
1: #include "postgres_parser.hpp"
2: 
3: #include "pg_functions.hpp"
4: #include "parser/parser.hpp"
5: #include "common/keywords.hpp"
6: 
7: using namespace std;
8: 
9: namespace duckdb {
10: 
11: PostgresParser::PostgresParser() : success(false), parse_tree(nullptr), error_message(""), error_location(0) {}
12: 
13: void PostgresParser::Parse(const string &query) {
14: 	duckdb_libpgquery::pg_parser_init();
15:     duckdb_libpgquery::parse_result res;
16: 	pg_parser_parse(query.c_str(), &res);
17: 	success = res.success;
18: 
19: 	if (success) {
20: 		parse_tree = res.parse_tree;
21: 	} else {
22: 		error_message = string(res.error_message);
23: 		error_location = res.error_location;
24: 	}
25: }
26: 
27: vector<duckdb_libpgquery::PGSimplifiedToken> PostgresParser::Tokenize(const std::string &query) {
28: 	duckdb_libpgquery::pg_parser_init();
29: 	auto tokens = duckdb_libpgquery::tokenize(query.c_str());
30: 	duckdb_libpgquery::pg_parser_cleanup();
31: 	return tokens;
32: }
33: 
34: PostgresParser::~PostgresParser()  {
35:     duckdb_libpgquery::pg_parser_cleanup();
36: }
37: 
38: bool PostgresParser::IsKeyword(const std::string &text) {
39: 	return duckdb_libpgquery::is_keyword(text.c_str());
40: }
41: 
42: vector<duckdb_libpgquery::PGKeyword> PostgresParser::KeywordList() {
43: 	return duckdb_libpgquery::keyword_list();
44: }
45: 
46: }
[end of third_party/libpg_query/postgres_parser.cpp]
[start of third_party/libpg_query/src_backend_parser_scansup.cpp]
1: /*--------------------------------------------------------------------
2:  * Symbols referenced in this file:
3:  * - truncate_identifier
4:  * - downcase_truncate_identifier
5:  * - downcase_identifier
6:  * - scanner_isspace
7:  *--------------------------------------------------------------------
8:  */
9: 
10: /*-------------------------------------------------------------------------
11:  *
12:  * scansup.c
13:  *	  support routines for the lex/flex scanner, used by both the normal
14:  * backend as well as the bootstrap backend
15:  *
16:  * Portions Copyright (c) 1996-2017, PostgreSQL Global Development PGGroup
17:  * Portions Copyright (c) 1994, Regents of the University of California
18:  *
19:  *
20:  * IDENTIFICATION
21:  *	  src/backend/parser/scansup.c
22:  *
23:  *-------------------------------------------------------------------------
24:  */
25: #include "pg_functions.hpp"
26: #include <string.h>
27: 
28: #include <ctype.h>
29: 
30: #include "parser/scansup.hpp"
31: #include "mb/pg_wchar.hpp"
32: 
33: namespace duckdb_libpgquery {
34: 
35: /* ----------------
36:  *		scanstr
37:  *
38:  * if the string passed in has escaped codes, map the escape codes to actual
39:  * chars
40:  *
41:  * the string returned is palloc'd and should eventually be pfree'd by the
42:  * caller!
43:  * ----------------
44:  */
45: 
46: /*
47:  * downcase_truncate_identifier() --- do appropriate downcasing and
48:  * truncation of an unquoted identifier.  Optionally warn of truncation.
49:  *
50:  * Returns a palloc'd string containing the adjusted identifier.
51:  *
52:  * Note: in some usages the passed string is not null-terminated.
53:  *
54:  * Note: the API of this function is designed to allow for downcasing
55:  * transformations that increase the string length, but we don't yet
56:  * support that.  If you want to implement it, you'll need to fix
57:  * SplitIdentifierString() in utils/adt/varlena.c.
58:  */
59: char *downcase_truncate_identifier(const char *ident, int len, bool warn) {
60: 	return downcase_identifier(ident, len, warn, true);
61: }
62: 
63: /*
64:  * a workhorse for downcase_truncate_identifier
65:  */
66: char *downcase_identifier(const char *ident, int len, bool warn, bool truncate) {
67: 	char *result;
68: 	int i;
69: 	bool enc_is_single_byte;
70: 
71: 	result = (char *)palloc(len + 1);
72: 	enc_is_single_byte = pg_database_encoding_max_length() == 1;
73: 
74: 	/*
75: 	 * SQL99 specifies Unicode-aware case normalization, which we don't yet
76: 	 * have the infrastructure for.  Instead we use tolower() to provide a
77: 	 * locale-aware translation.  However, there are some locales where this
78: 	 * is not right either (eg, Turkish may do strange things with 'i' and
79: 	 * 'I').  Our current compromise is to use tolower() for characters with
80: 	 * the high bit set, as long as they aren't part of a multi-byte
81: 	 * character, and use an ASCII-only downcasing for 7-bit characters.
82: 	 */
83: 	for (i = 0; i < len; i++) {
84: 		unsigned char ch = (unsigned char)ident[i];
85: 
86: 		if (ch >= 'A' && ch <= 'Z')
87: 			ch += 'a' - 'A';
88: 		else if (enc_is_single_byte && IS_HIGHBIT_SET(ch) && isupper(ch))
89: 			ch = tolower(ch);
90: 		result[i] = (char)ch;
91: 	}
92: 	result[i] = '\0';
93: 
94: 	return result;
95: }
96: 
97: /*
98:  * scanner_isspace() --- return true if flex scanner considers char whitespace
99:  *
100:  * This should be used instead of the potentially locale-dependent isspace()
101:  * function when it's important to match the lexer's behavior.
102:  *
103:  * In principle we might need similar functions for isalnum etc, but for the
104:  * moment only isspace seems needed.
105:  */
106: bool scanner_isspace(char ch) {
107: 	/* This must match scan.l's list of {space} characters */
108: 	if (ch == ' ' || ch == '\t' || ch == '\n' || ch == '\r' || ch == '\f')
109: 		return true;
110: 	return false;
111: }
112: }
[end of third_party/libpg_query/src_backend_parser_scansup.cpp]
[start of tools/pythonpkg/src/include/duckdb_python/map.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/pandas_scan.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #include "duckdb_python/pybind_wrapper.hpp"
13: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
14: 
15: namespace duckdb {
16: 
17: struct MapFunction : public TableFunction {
18: 
19: public:
20: 	MapFunction();
21: 
22: 	static unique_ptr<FunctionData> MapFunctionBind(ClientContext &context, vector<Value> &inputs,
23: 	                                                unordered_map<string, Value> &named_parameters,
24: 	                                                vector<LogicalType> &input_table_types,
25: 	                                                vector<string> &input_table_names,
26: 	                                                vector<LogicalType> &return_types, vector<string> &names);
27: 
28: 	static void MapFunctionExec(ClientContext &context, const FunctionData *bind_data,
29: 	                            FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output);
30: };
31: 
32: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/map.hpp]
[start of tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/pandas_scan.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
13: 
14: namespace duckdb {
15: 
16: struct PandasScanFunction : public TableFunction {
17: public:
18: 	static constexpr idx_t PANDAS_PARTITION_COUNT = 50 * STANDARD_VECTOR_SIZE;
19: 
20: public:
21: 	PandasScanFunction();
22: 
23: 	static unique_ptr<FunctionData> PandasScanBind(ClientContext &context, vector<Value> &inputs,
24: 	                                               unordered_map<string, Value> &named_parameters,
25: 	                                               vector<LogicalType> &input_table_types,
26: 	                                               vector<string> &input_table_names, vector<LogicalType> &return_types,
27: 	                                               vector<string> &names);
28: 
29: 	static unique_ptr<FunctionOperatorData> PandasScanInit(ClientContext &context, const FunctionData *bind_data_p,
30: 	                                                       const vector<column_t> &column_ids,
31: 	                                                       TableFilterCollection *filters);
32: 
33: 	static idx_t PandasScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p);
34: 
35: 	static unique_ptr<ParallelState> PandasScanInitParallelState(ClientContext &context,
36: 	                                                             const FunctionData *bind_data_p,
37: 	                                                             const vector<column_t> &column_ids,
38: 	                                                             TableFilterCollection *filters);
39: 
40: 	static unique_ptr<FunctionOperatorData>
41: 	PandasScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *state,
42: 	                       const vector<column_t> &column_ids, TableFilterCollection *filters);
43: 
44: 	static bool PandasScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
45: 	                                        FunctionOperatorData *operator_state, ParallelState *parallel_state_p);
46: 
47: 	static double PandasProgress(ClientContext &context, const FunctionData *bind_data_p);
48: 
49: 	//! The main pandas scan function: note that this can be called in parallel without the GIL
50: 	//! hence this needs to be GIL-safe, i.e. no methods that create Python objects are allowed
51: 	static void PandasScanFunc(ClientContext &context, const FunctionData *bind_data,
52: 	                           FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output);
53: 
54: 	static void PandasScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
55: 	                                   FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
56: 	                                   ParallelState *parallel_state_p);
57: 
58: 	static unique_ptr<NodeStatistics> PandasScanCardinality(ClientContext &context, const FunctionData *bind_data);
59: };
60: 
61: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp]
[start of tools/pythonpkg/src/map.cpp]
1: #include "duckdb_python/map.hpp"
2: #include "duckdb_python/vector_conversion.hpp"
3: #include "duckdb_python/array_wrapper.hpp"
4: #include "duckdb/common/string_util.hpp"
5: 
6: namespace duckdb {
7: 
8: MapFunction::MapFunction()
9:     : TableFunction("python_map_function", {LogicalType::TABLE, LogicalType::POINTER}, MapFunctionExec,
10:                     MapFunctionBind) {
11: }
12: 
13: struct MapFunctionData : public TableFunctionData {
14: 	MapFunctionData() : function(nullptr) {
15: 	}
16: 	PyObject *function;
17: 	vector<LogicalType> in_types, out_types;
18: 	vector<string> in_names, out_names;
19: };
20: 
21: static py::handle FunctionCall(NumpyResultConversion &conversion, vector<string> &names, PyObject *function) {
22: 	py::dict in_numpy_dict;
23: 	for (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {
24: 		in_numpy_dict[names[col_idx].c_str()] = conversion.ToArray(col_idx);
25: 	}
26: 	auto in_df = py::module::import("pandas").attr("DataFrame").attr("from_dict")(in_numpy_dict);
27: 	D_ASSERT(in_df.ptr());
28: 
29: 	D_ASSERT(function);
30: 	auto *df_obj = PyObject_CallObject(function, PyTuple_Pack(1, in_df.ptr()));
31: 	if (!df_obj) {
32: 		PyErr_PrintEx(1);
33: 		throw InvalidInputException("Python error. See above for a stack trace.");
34: 	}
35: 
36: 	py::handle df(df_obj);
37: 	if (df.is_none()) { // no return, probably modified in place
38: 		throw InvalidInputException("No return value from Python function");
39: 	}
40: 
41: 	return df;
42: }
43: 
44: // we call the passed function with a zero-row data frame to infer the output columns and their names.
45: // they better not change in the actual execution ^^
46: unique_ptr<FunctionData> MapFunction::MapFunctionBind(ClientContext &context, vector<Value> &inputs,
47:                                                       unordered_map<string, Value> &named_parameters,
48:                                                       vector<LogicalType> &input_table_types,
49:                                                       vector<string> &input_table_names,
50:                                                       vector<LogicalType> &return_types, vector<string> &names) {
51: 	py::gil_scoped_acquire acquire;
52: 
53: 	auto data_uptr = make_unique<MapFunctionData>();
54: 	auto &data = *data_uptr;
55: 	data.function = (PyObject *)inputs[0].GetPointer();
56: 	data.in_names = input_table_names;
57: 	data.in_types = input_table_types;
58: 
59: 	NumpyResultConversion conversion(data.in_types, 0);
60: 	auto df = FunctionCall(conversion, data.in_names, data.function);
61: 	vector<PandasColumnBindData> pandas_bind_data; // unused
62: 	VectorConversion::BindPandas(df, pandas_bind_data, return_types, names);
63: 
64: 	data.out_names = names;
65: 	data.out_types = return_types;
66: 	return move(data_uptr);
67: }
68: 
69: static string TypeVectorToString(vector<LogicalType> &types) {
70: 	return StringUtil::Join(types, types.size(), ", ", [](const LogicalType &argument) { return argument.ToString(); });
71: }
72: 
73: void MapFunction::MapFunctionExec(ClientContext &context, const FunctionData *bind_data,
74:                                   FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
75: 
76: 	py::gil_scoped_acquire acquire;
77: 
78: 	if (input->size() == 0) {
79: 		return;
80: 	}
81: 
82: 	auto &data = (MapFunctionData &)*bind_data;
83: 
84: 	D_ASSERT(input->GetTypes() == data.in_types);
85: 	NumpyResultConversion conversion(data.in_types, input->size());
86: 	conversion.Append(*input);
87: 
88: 	auto df = FunctionCall(conversion, data.in_names, data.function);
89: 
90: 	vector<PandasColumnBindData> pandas_bind_data;
91: 	vector<LogicalType> pandas_return_types;
92: 	vector<string> pandas_names;
93: 
94: 	VectorConversion::BindPandas(df, pandas_bind_data, pandas_return_types, pandas_names);
95: 	if (pandas_return_types.size() != output.ColumnCount()) {
96: 		throw InvalidInputException("Expected %llu columns from UDF, got %llu", output.ColumnCount(),
97: 		                            pandas_return_types.size());
98: 	}
99: 	D_ASSERT(output.GetTypes() == data.out_types);
100: 	if (pandas_return_types != output.GetTypes()) {
101: 		throw InvalidInputException("UDF column type mismatch, expected [%s], got [%s]",
102: 		                            TypeVectorToString(data.out_types), TypeVectorToString(pandas_return_types));
103: 	}
104: 	if (pandas_names != data.out_names) {
105: 		throw InvalidInputException("UDF column name mismatch, expected [%s], got [%s]",
106: 		                            StringUtil::Join(data.out_names, ", "), StringUtil::Join(pandas_names, ", "));
107: 	}
108: 
109: 	auto df_columns = py::list(df.attr("columns"));
110: 	auto get_fun = df.attr("__getitem__");
111: 
112: 	idx_t row_count = py::len(get_fun(df_columns[0]));
113: 	if (row_count > STANDARD_VECTOR_SIZE) {
114: 		throw InvalidInputException("UDF returned more than %llu rows, which is not allowed.", STANDARD_VECTOR_SIZE);
115: 	}
116: 
117: 	for (idx_t col_idx = 0; col_idx < output.ColumnCount(); col_idx++) {
118: 		VectorConversion::NumpyToDuckDB(pandas_bind_data[col_idx], pandas_bind_data[col_idx].numpy_col, row_count, 0,
119: 		                                output.data[col_idx]);
120: 	}
121: 	output.SetCardinality(row_count);
122: }
123: 
124: } // namespace duckdb
[end of tools/pythonpkg/src/map.cpp]
[start of tools/pythonpkg/src/pandas_scan.cpp]
1: #include "duckdb_python/pandas_scan.hpp"
2: #include "duckdb_python/array_wrapper.hpp"
3: #include "duckdb/parallel/parallel_state.hpp"
4: #include "utf8proc_wrapper.hpp"
5: #include "duckdb/common/types/timestamp.hpp"
6: #include "duckdb_python/vector_conversion.hpp"
7: #include "duckdb/main/client_context.hpp"
8: 
9: #include "duckdb/common/atomic.hpp"
10: 
11: namespace duckdb {
12: 
13: struct PandasScanFunctionData : public TableFunctionData {
14: 	PandasScanFunctionData(py::handle df, idx_t row_count, vector<PandasColumnBindData> pandas_bind_data,
15: 	                       vector<LogicalType> sql_types)
16: 	    : df(df), row_count(row_count), lines_read(0), pandas_bind_data(move(pandas_bind_data)),
17: 	      sql_types(move(sql_types)) {
18: 	}
19: 	py::handle df;
20: 	idx_t row_count;
21: 	atomic<idx_t> lines_read;
22: 	vector<PandasColumnBindData> pandas_bind_data;
23: 	vector<LogicalType> sql_types;
24: 
25: 	~PandasScanFunctionData() override {
26: 		py::gil_scoped_acquire acquire;
27: 		pandas_bind_data.clear();
28: 	}
29: };
30: 
31: struct PandasScanState : public FunctionOperatorData {
32: 	PandasScanState(idx_t start, idx_t end) : start(start), end(end) {
33: 	}
34: 
35: 	idx_t start;
36: 	idx_t end;
37: 	vector<column_t> column_ids;
38: };
39: 
40: struct ParallelPandasScanState : public ParallelState {
41: 	ParallelPandasScanState() : position(0) {
42: 	}
43: 
44: 	std::mutex lock;
45: 	idx_t position;
46: };
47: 
48: PandasScanFunction::PandasScanFunction()
49:     : TableFunction("pandas_scan", {LogicalType::POINTER}, PandasScanFunc, PandasScanBind, PandasScanInit, nullptr,
50:                     nullptr, nullptr, PandasScanCardinality, nullptr, nullptr, PandasScanMaxThreads,
51:                     PandasScanInitParallelState, PandasScanFuncParallel, PandasScanParallelInit,
52:                     PandasScanParallelStateNext, true, false, PandasProgress) {
53: }
54: 
55: unique_ptr<FunctionData> PandasScanFunction::PandasScanBind(ClientContext &context, vector<Value> &inputs,
56:                                                             unordered_map<string, Value> &named_parameters,
57:                                                             vector<LogicalType> &input_table_types,
58:                                                             vector<string> &input_table_names,
59:                                                             vector<LogicalType> &return_types, vector<string> &names) {
60: 	py::gil_scoped_acquire acquire;
61: 	py::handle df((PyObject *)(inputs[0].GetPointer()));
62: 
63: 	vector<PandasColumnBindData> pandas_bind_data;
64: 	VectorConversion::BindPandas(df, pandas_bind_data, return_types, names);
65: 
66: 	auto df_columns = py::list(df.attr("columns"));
67: 	auto get_fun = df.attr("__getitem__");
68: 
69: 	idx_t row_count = py::len(get_fun(df_columns[0]));
70: 	return make_unique<PandasScanFunctionData>(df, row_count, move(pandas_bind_data), return_types);
71: }
72: 
73: unique_ptr<FunctionOperatorData> PandasScanFunction::PandasScanInit(ClientContext &context,
74:                                                                     const FunctionData *bind_data_p,
75:                                                                     const vector<column_t> &column_ids,
76:                                                                     TableFilterCollection *filters) {
77: 	auto &bind_data = (const PandasScanFunctionData &)*bind_data_p;
78: 	auto result = make_unique<PandasScanState>(0, bind_data.row_count);
79: 	result->column_ids = column_ids;
80: 	return move(result);
81: }
82: 
83: idx_t PandasScanFunction::PandasScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p) {
84: 	if (ClientConfig::GetConfig(context).verify_parallelism) {
85: 		return context.db->NumberOfThreads();
86: 	}
87: 	auto &bind_data = (const PandasScanFunctionData &)*bind_data_p;
88: 	return bind_data.row_count / PANDAS_PARTITION_COUNT + 1;
89: }
90: 
91: unique_ptr<ParallelState> PandasScanFunction::PandasScanInitParallelState(ClientContext &context,
92:                                                                           const FunctionData *bind_data_p,
93:                                                                           const vector<column_t> &column_ids,
94:                                                                           TableFilterCollection *filters) {
95: 	return make_unique<ParallelPandasScanState>();
96: }
97: 
98: unique_ptr<FunctionOperatorData> PandasScanFunction::PandasScanParallelInit(ClientContext &context,
99:                                                                             const FunctionData *bind_data_p,
100:                                                                             ParallelState *state,
101:                                                                             const vector<column_t> &column_ids,
102:                                                                             TableFilterCollection *filters) {
103: 	auto result = make_unique<PandasScanState>(0, 0);
104: 	result->column_ids = column_ids;
105: 	if (!PandasScanParallelStateNext(context, bind_data_p, result.get(), state)) {
106: 		return nullptr;
107: 	}
108: 	return move(result);
109: }
110: 
111: bool PandasScanFunction::PandasScanParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
112:                                                      FunctionOperatorData *operator_state,
113:                                                      ParallelState *parallel_state_p) {
114: 	auto &bind_data = (const PandasScanFunctionData &)*bind_data_p;
115: 	auto &parallel_state = (ParallelPandasScanState &)*parallel_state_p;
116: 	auto &state = (PandasScanState &)*operator_state;
117: 
118: 	lock_guard<mutex> parallel_lock(parallel_state.lock);
119: 	if (parallel_state.position >= bind_data.row_count) {
120: 		return false;
121: 	}
122: 	state.start = parallel_state.position;
123: 	parallel_state.position += PANDAS_PARTITION_COUNT;
124: 	if (parallel_state.position > bind_data.row_count) {
125: 		parallel_state.position = bind_data.row_count;
126: 	}
127: 	state.end = parallel_state.position;
128: 	return true;
129: }
130: 
131: double PandasScanFunction::PandasProgress(ClientContext &context, const FunctionData *bind_data_p) {
132: 	auto &bind_data = (const PandasScanFunctionData &)*bind_data_p;
133: 	if (bind_data.row_count == 0) {
134: 		return 100;
135: 	}
136: 	auto percentage = (bind_data.lines_read * 100.0) / bind_data.row_count;
137: 	return percentage;
138: }
139: 
140: void PandasScanFunction::PandasScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
141:                                                 FunctionOperatorData *operator_state, DataChunk *input,
142:                                                 DataChunk &output, ParallelState *parallel_state_p) {
143: 	//! FIXME: Have specialized parallel function from pandas scan here
144: 	PandasScanFunc(context, bind_data, operator_state, input, output);
145: }
146: 
147: //! The main pandas scan function: note that this can be called in parallel without the GIL
148: //! hence this needs to be GIL-safe, i.e. no methods that create Python objects are allowed
149: void PandasScanFunction::PandasScanFunc(ClientContext &context, const FunctionData *bind_data,
150:                                         FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
151: 	if (!operator_state) {
152: 		return;
153: 	}
154: 	auto &data = (PandasScanFunctionData &)*bind_data;
155: 	auto &state = (PandasScanState &)*operator_state;
156: 
157: 	if (state.start >= state.end) {
158: 		return;
159: 	}
160: 	idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, state.end - state.start);
161: 	output.SetCardinality(this_count);
162: 	for (idx_t idx = 0; idx < state.column_ids.size(); idx++) {
163: 		auto col_idx = state.column_ids[idx];
164: 		if (col_idx == COLUMN_IDENTIFIER_ROW_ID) {
165: 			output.data[idx].Sequence(state.start, this_count);
166: 		} else {
167: 			VectorConversion::NumpyToDuckDB(data.pandas_bind_data[col_idx], data.pandas_bind_data[col_idx].numpy_col,
168: 			                                this_count, state.start, output.data[idx]);
169: 		}
170: 	}
171: 	state.start += this_count;
172: 	data.lines_read += this_count;
173: }
174: 
175: unique_ptr<NodeStatistics> PandasScanFunction::PandasScanCardinality(ClientContext &context,
176:                                                                      const FunctionData *bind_data) {
177: 	auto &data = (PandasScanFunctionData &)*bind_data;
178: 	return make_unique<NodeStatistics>(data.row_count, data.row_count);
179: }
180: 
181: } // namespace duckdb
[end of tools/pythonpkg/src/pandas_scan.cpp]
[start of tools/pythonpkg/src/pyconnection.cpp]
1: #include "duckdb_python/pyconnection.hpp"
2: #include "duckdb_python/pyresult.hpp"
3: #include "duckdb_python/pyrelation.hpp"
4: #include "duckdb_python/pandas_scan.hpp"
5: #include "duckdb_python/map.hpp"
6: 
7: #include "duckdb/common/arrow.hpp"
8: #include "duckdb_python/arrow_array_stream.hpp"
9: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
12: #include "duckdb/common/types/vector.hpp"
13: #include "duckdb/common/printer.hpp"
14: #include "duckdb/main/config.hpp"
15: #include "duckdb/parser/expression/constant_expression.hpp"
16: #include "duckdb/parser/expression/function_expression.hpp"
17: #include "duckdb/parser/tableref/table_function_ref.hpp"
18: 
19: #include "datetime.h" // from Python
20: 
21: #include <random>
22: 
23: namespace duckdb {
24: 
25: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::default_connection = nullptr;
26: 
27: void DuckDBPyConnection::Initialize(py::handle &m) {
28: 	py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection", py::module_local())
29: 	    .def("cursor", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
30: 	    .def("duplicate", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
31: 	    .def("execute", &DuckDBPyConnection::Execute,
32: 	         "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
33: 	         py::arg("parameters") = py::list(), py::arg("multiple_parameter_sets") = false)
34: 	    .def("executemany", &DuckDBPyConnection::ExecuteMany,
35: 	         "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
36: 	         py::arg("query"), py::arg("parameters") = py::list())
37: 	    .def("close", &DuckDBPyConnection::Close, "Close the connection")
38: 	    .def("fetchone", &DuckDBPyConnection::FetchOne, "Fetch a single row from a result following execute")
39: 	    .def("fetchall", &DuckDBPyConnection::FetchAll, "Fetch all rows from a result following execute")
40: 	    .def("fetchnumpy", &DuckDBPyConnection::FetchNumpy, "Fetch a result as list of NumPy arrays following execute")
41: 	    .def("fetchdf", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
42: 	    .def("fetch_df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
43: 	    .def("fetch_df_chunk", &DuckDBPyConnection::FetchDFChunk,
44: 	         "Fetch a chunk of the result as Data.Frame following execute()", py::arg("vectors_per_chunk") = 1)
45: 	    .def("df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
46: 	    .def("fetch_arrow_table", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()")
47: 	    .def("fetch_arrow_chunk", &DuckDBPyConnection::FetchArrowChunk,
48: 	         "Fetch a chunk of the result as an Arrow Table following execute()", py::arg("vectors_per_chunk") = 1,
49: 	         py::arg("return_table") = false)
50: 	    .def("fetch_record_batch", &DuckDBPyConnection::FetchRecordBatchReader,
51: 	         "Fetch an Arrow RecordBatchReader following execute()", py::arg("approx_batch_size") = 1)
52: 	    .def("arrow", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()")
53: 	    .def("begin", &DuckDBPyConnection::Begin, "Start a new transaction")
54: 	    .def("commit", &DuckDBPyConnection::Commit, "Commit changes performed within a transaction")
55: 	    .def("rollback", &DuckDBPyConnection::Rollback, "Roll back changes performed within a transaction")
56: 	    .def("append", &DuckDBPyConnection::Append, "Append the passed Data.Frame to the named table",
57: 	         py::arg("table_name"), py::arg("df"))
58: 	    .def("register", &DuckDBPyConnection::RegisterPythonObject,
59: 	         "Register the passed Python Object value for querying with a view", py::arg("view_name"),
60: 	         py::arg("python_object"), py::arg("rows_per_thread") = 1000000)
61: 	    .def("unregister", &DuckDBPyConnection::UnregisterPythonObject, "Unregister the view name",
62: 	         py::arg("view_name"))
63: 	    .def("table", &DuckDBPyConnection::Table, "Create a relation object for the name'd table",
64: 	         py::arg("table_name"))
65: 	    .def("view", &DuckDBPyConnection::View, "Create a relation object for the name'd view", py::arg("view_name"))
66: 	    .def("values", &DuckDBPyConnection::Values, "Create a relation object from the passed values",
67: 	         py::arg("values"))
68: 	    .def("table_function", &DuckDBPyConnection::TableFunction,
69: 	         "Create a relation object from the name'd table function with given parameters", py::arg("name"),
70: 	         py::arg("parameters") = py::list())
71: 	    .def("from_query", &DuckDBPyConnection::FromQuery, "Create a relation object from the given SQL query",
72: 	         py::arg("query"), py::arg("alias") = "query_relation")
73: 	    .def("query", &DuckDBPyConnection::FromQuery, "Create a relation object from the given SQL query",
74: 	         py::arg("query"), py::arg("alias") = "query_relation")
75: 	    .def("from_df", &DuckDBPyConnection::FromDF, "Create a relation object from the Data.Frame in df",
76: 	         py::arg("df") = py::none())
77: 	    .def("from_arrow_table", &DuckDBPyConnection::FromArrowTable, "Create a relation object from an Arrow table",
78: 	         py::arg("table"), py::arg("rows_per_thread") = 1000000)
79: 	    .def("df", &DuckDBPyConnection::FromDF, "Create a relation object from the Data.Frame in df (alias of from_df)",
80: 	         py::arg("df"))
81: 	    .def("from_csv_auto", &DuckDBPyConnection::FromCsvAuto,
82: 	         "Create a relation object from the CSV file in file_name", py::arg("file_name"))
83: 	    .def("from_parquet", &DuckDBPyConnection::FromParquet,
84: 	         "Create a relation object from the Parquet file in file_name", py::arg("file_name"),
85: 	         py::arg("binary_as_string") = false)
86: 	    .def_property_readonly("description", &DuckDBPyConnection::GetDescription,
87: 	                           "Get result set attributes, mainly column names");
88: 
89: 	PyDateTime_IMPORT;
90: }
91: 
92: DuckDBPyConnection *DuckDBPyConnection::ExecuteMany(const string &query, py::object params) {
93: 	Execute(query, std::move(params), true);
94: 	return this;
95: }
96: 
97: DuckDBPyConnection *DuckDBPyConnection::Execute(const string &query, py::object params, bool many) {
98: 	if (!connection) {
99: 		throw std::runtime_error("connection closed");
100: 	}
101: 	if (std::this_thread::get_id() != thread_id) {
102: 		throw std::runtime_error("DuckDB objects created in a thread can only be used in that same thread. The object "
103: 		                         "was created in thread id " +
104: 		                         to_string(std::hash<std::thread::id> {}(thread_id)) + " and this is thread id " +
105: 		                         to_string(std::hash<std::thread::id> {}(std::this_thread::get_id())));
106: 	}
107: 	result = nullptr;
108: 	unique_ptr<PreparedStatement> prep;
109: 	{
110: 		py::gil_scoped_release release;
111: 		auto statements = connection->ExtractStatements(query);
112: 		if (statements.empty()) {
113: 			// no statements to execute
114: 			return this;
115: 		}
116: 		// if there are multiple statements, we directly execute the statements besides the last one
117: 		// we only return the result of the last statement to the user, unless one of the previous statements fails
118: 		for (idx_t i = 0; i + 1 < statements.size(); i++) {
119: 			auto res = connection->Query(move(statements[i]));
120: 			if (!res->success) {
121: 				throw std::runtime_error(res->error);
122: 			}
123: 		}
124: 
125: 		prep = connection->Prepare(move(statements.back()));
126: 		if (!prep->success) {
127: 			throw std::runtime_error(prep->error);
128: 		}
129: 	}
130: 
131: 	// this is a list of a list of parameters in executemany
132: 	py::list params_set;
133: 	if (!many) {
134: 		params_set = py::list(1);
135: 		params_set[0] = params;
136: 	} else {
137: 		params_set = params;
138: 	}
139: 
140: 	for (pybind11::handle single_query_params : params_set) {
141: 		if (prep->n_param != py::len(single_query_params)) {
142: 			throw std::runtime_error("Prepared statement needs " + to_string(prep->n_param) + " parameters, " +
143: 			                         to_string(py::len(single_query_params)) + " given");
144: 		}
145: 		auto args = DuckDBPyConnection::TransformPythonParamList(single_query_params);
146: 		auto res = make_unique<DuckDBPyResult>();
147: 		{
148: 			py::gil_scoped_release release;
149: 			res->result = prep->Execute(args);
150: 			if (!res->result->success) {
151: 				throw std::runtime_error(res->result->error);
152: 			}
153: 		}
154: 
155: 		if (!many) {
156: 			result = move(res);
157: 		}
158: 	}
159: 	return this;
160: }
161: 
162: DuckDBPyConnection *DuckDBPyConnection::Append(const string &name, py::object value) {
163: 	RegisterPythonObject("__append_df", std::move(value));
164: 	return Execute("INSERT INTO \"" + name + "\" SELECT * FROM __append_df");
165: }
166: 
167: DuckDBPyConnection *DuckDBPyConnection::RegisterPythonObject(const string &name, py::object python_object,
168:                                                              const idx_t rows_per_tuple) {
169: 	if (!connection) {
170: 		throw std::runtime_error("connection closed");
171: 	}
172: 	auto py_object_type = string(py::str(python_object.get_type().attr("__name__")));
173: 
174: 	if (py_object_type == "DataFrame") {
175: 		{
176: 			py::gil_scoped_release release;
177: 			connection->TableFunction("pandas_scan", {Value::POINTER((uintptr_t)python_object.ptr())})
178: 			    ->CreateView(name, true, true);
179: 		}
180: 
181: 		// keep a reference
182: 		auto object = make_unique<RegisteredObject>(python_object);
183: 		registered_objects[name] = move(object);
184: 	} else if (py_object_type == "Table" || py_object_type == "FileSystemDataset" ||
185: 	           py_object_type == "InMemoryDataset") {
186: 		auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(python_object.ptr());
187: 
188: 		auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
189: 		{
190: 			py::gil_scoped_release release;
191: 			connection
192: 			    ->TableFunction("arrow_scan",
193: 			                    {Value::POINTER((uintptr_t)stream_factory.get()),
194: 			                     Value::POINTER((uintptr_t)stream_factory_produce), Value::UBIGINT(rows_per_tuple)})
195: 			    ->CreateView(name, true, true);
196: 		}
197: 		auto object = make_unique<RegisteredArrow>(move(stream_factory), move(python_object));
198: 		registered_objects[name] = move(object);
199: 	} else {
200: 		throw std::runtime_error("Python Object " + py_object_type + " not suitable to be registered as a view");
201: 	}
202: 	return this;
203: }
204: 
205: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromQuery(const string &query, const string &alias) {
206: 	if (!connection) {
207: 		throw std::runtime_error("connection closed");
208: 	}
209: 	return make_unique<DuckDBPyRelation>(connection->RelationFromQuery(query, alias));
210: }
211: 
212: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Table(const string &tname) {
213: 	if (!connection) {
214: 		throw std::runtime_error("connection closed");
215: 	}
216: 	return make_unique<DuckDBPyRelation>(connection->Table(tname));
217: }
218: 
219: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Values(py::object params) {
220: 	if (!connection) {
221: 		throw std::runtime_error("connection closed");
222: 	}
223: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(std::move(params))};
224: 	return make_unique<DuckDBPyRelation>(connection->Values(values));
225: }
226: 
227: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::View(const string &vname) {
228: 	if (!connection) {
229: 		throw std::runtime_error("connection closed");
230: 	}
231: 	return make_unique<DuckDBPyRelation>(connection->View(vname));
232: }
233: 
234: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fname, py::object params) {
235: 	if (!connection) {
236: 		throw std::runtime_error("connection closed");
237: 	}
238: 
239: 	return make_unique<DuckDBPyRelation>(
240: 	    connection->TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(std::move(params))));
241: }
242: 
243: static std::string GenerateRandomName() {
244: 	std::random_device rd;
245: 	std::mt19937 gen(rd());
246: 	std::uniform_int_distribution<> dis(0, 15);
247: 
248: 	std::stringstream ss;
249: 	int i;
250: 	ss << std::hex;
251: 	for (i = 0; i < 16; i++) {
252: 		ss << dis(gen);
253: 	}
254: 	return ss.str();
255: }
256: 
257: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(py::object value) {
258: 	if (!connection) {
259: 		throw std::runtime_error("connection closed");
260: 	}
261: 	string name = "df_" + GenerateRandomName();
262: 	registered_objects[name] = make_unique<RegisteredObject>(value);
263: 	vector<Value> params;
264: 	params.emplace_back(Value::POINTER((uintptr_t)value.ptr()));
265: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("pandas_scan", params)->Alias(name));
266: }
267: 
268: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromCsvAuto(const string &filename) {
269: 	if (!connection) {
270: 		throw std::runtime_error("connection closed");
271: 	}
272: 	vector<Value> params;
273: 	params.emplace_back(filename);
274: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("read_csv_auto", params)->Alias(filename));
275: }
276: 
277: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &filename, bool binary_as_string) {
278: 	if (!connection) {
279: 		throw std::runtime_error("connection closed");
280: 	}
281: 	vector<Value> params;
282: 	params.emplace_back(filename);
283: 	unordered_map<string, Value> named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)}});
284: 	return make_unique<DuckDBPyRelation>(
285: 	    connection->TableFunction("parquet_scan", params, named_parameters)->Alias(filename));
286: }
287: 
288: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrowTable(py::object &table, const idx_t rows_per_tuple) {
289: 	if (!connection) {
290: 		throw std::runtime_error("connection closed");
291: 	}
292: 	py::gil_scoped_acquire acquire;
293: 	string name = "arrow_table_" + GenerateRandomName();
294: 
295: 	auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(table.ptr());
296: 
297: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
298: 	auto rel = make_unique<DuckDBPyRelation>(
299: 	    connection
300: 	        ->TableFunction("arrow_scan",
301: 	                        {Value::POINTER((uintptr_t)stream_factory.get()),
302: 	                         Value::POINTER((uintptr_t)stream_factory_produce), Value::UBIGINT(rows_per_tuple)})
303: 	        ->Alias(name));
304: 	registered_objects[name] = make_unique<RegisteredArrow>(move(stream_factory), table);
305: 	return rel;
306: }
307: 
308: DuckDBPyConnection *DuckDBPyConnection::UnregisterPythonObject(const string &name) {
309: 	registered_objects.erase(name);
310: 	py::gil_scoped_release release;
311: 	if (connection) {
312: 		connection->Query("DROP VIEW \"" + name + "\"");
313: 	}
314: 	return this;
315: }
316: 
317: DuckDBPyConnection *DuckDBPyConnection::Begin() {
318: 	Execute("BEGIN TRANSACTION");
319: 	return this;
320: }
321: 
322: DuckDBPyConnection *DuckDBPyConnection::Commit() {
323: 	if (connection->context->transaction.IsAutoCommit()) {
324: 		return this;
325: 	}
326: 	Execute("COMMIT");
327: 	return this;
328: }
329: 
330: DuckDBPyConnection *DuckDBPyConnection::Rollback() {
331: 	Execute("ROLLBACK");
332: 	return this;
333: }
334: 
335: py::object DuckDBPyConnection::GetDescription() {
336: 	if (!result) {
337: 		return py::none();
338: 	}
339: 	return result->Description();
340: }
341: 
342: void DuckDBPyConnection::Close() {
343: 	result = nullptr;
344: 	connection = nullptr;
345: 	database = nullptr;
346: 	for (auto &cur : cursors) {
347: 		cur->Close();
348: 	}
349: 	cursors.clear();
350: }
351: 
352: // cursor() is stupid
353: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Cursor() {
354: 	auto res = make_shared<DuckDBPyConnection>();
355: 	res->database = database;
356: 	res->connection = make_unique<Connection>(*res->database);
357: 	cursors.push_back(res);
358: 	return res;
359: }
360: 
361: // these should be functions on the result but well
362: py::object DuckDBPyConnection::FetchOne() {
363: 	if (!result) {
364: 		throw std::runtime_error("no open result set");
365: 	}
366: 	return result->Fetchone();
367: }
368: 
369: py::list DuckDBPyConnection::FetchAll() {
370: 	if (!result) {
371: 		throw std::runtime_error("no open result set");
372: 	}
373: 	return result->Fetchall();
374: }
375: 
376: py::dict DuckDBPyConnection::FetchNumpy() {
377: 	if (!result) {
378: 		throw std::runtime_error("no open result set");
379: 	}
380: 	return result->FetchNumpyInternal();
381: }
382: py::object DuckDBPyConnection::FetchDF() {
383: 	if (!result) {
384: 		throw std::runtime_error("no open result set");
385: 	}
386: 	return result->FetchDF();
387: }
388: 
389: py::object DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk) const {
390: 	if (!result) {
391: 		throw std::runtime_error("no open result set");
392: 	}
393: 	return result->FetchDFChunk(vectors_per_chunk);
394: }
395: 
396: py::object DuckDBPyConnection::FetchArrow() {
397: 	if (!result) {
398: 		throw std::runtime_error("no open result set");
399: 	}
400: 	return result->FetchArrowTable();
401: }
402: 
403: py::object DuckDBPyConnection::FetchArrowChunk(const idx_t vectors_per_chunk, bool return_table) const {
404: 	if (!result) {
405: 		throw std::runtime_error("no open result set");
406: 	}
407: 	return result->FetchArrowTableChunk(vectors_per_chunk, return_table);
408: }
409: 
410: py::object DuckDBPyConnection::FetchRecordBatchReader(const idx_t approx_batch_size) const {
411: 	if (!result) {
412: 		throw std::runtime_error("no open result set");
413: 	}
414: 	return result->FetchRecordBatchReader(approx_batch_size);
415: }
416: static unique_ptr<TableFunctionRef>
417: TryReplacement(py::dict &dict, py::str &table_name,
418:                unordered_map<string, unique_ptr<RegisteredObject>> &registered_objects) {
419: 	if (!dict.contains(table_name)) {
420: 		// not present in the globals
421: 		return nullptr;
422: 	}
423: 	auto entry = dict[table_name];
424: 	auto py_object_type = string(py::str(entry.get_type().attr("__name__")));
425: 	auto table_function = make_unique<TableFunctionRef>();
426: 	vector<unique_ptr<ParsedExpression>> children;
427: 	if (py_object_type == "DataFrame") {
428: 		string name = "df_" + GenerateRandomName();
429: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)entry.ptr())));
430: 		table_function->function = make_unique<FunctionExpression>("pandas_scan", move(children));
431: 		// keep a reference
432: 		auto object = make_unique<RegisteredObject>(entry);
433: 		registered_objects[name] = move(object);
434: 	} else if (py_object_type == "Table" || py_object_type == "FileSystemDataset" ||
435: 	           py_object_type == "InMemoryDataset") {
436: 		string name = "arrow_" + GenerateRandomName();
437: 		auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(entry.ptr());
438: 		auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
439: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)stream_factory.get())));
440: 		children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)stream_factory_produce)));
441: 		children.push_back(make_unique<ConstantExpression>(Value::UBIGINT(1000000)));
442: 		table_function->function = make_unique<FunctionExpression>("arrow_scan", move(children));
443: 		registered_objects[name] = make_unique<RegisteredArrow>(move(stream_factory), entry);
444: 	} else {
445: 		throw std::runtime_error("Python Object " + py_object_type + " not suitable for replacement scans");
446: 	}
447: 	return table_function;
448: }
449: 
450: static unique_ptr<TableFunctionRef> ScanReplacement(const string &table_name, void *data) {
451: 	py::gil_scoped_acquire acquire;
452: 	auto registered_objects = (unordered_map<string, unique_ptr<RegisteredObject>> *)data;
453: 	// look in the locals first
454: 	PyObject *p = PyEval_GetLocals();
455: 	auto py_table_name = py::str(table_name);
456: 	if (p) {
457: 		auto local_dict = py::reinterpret_borrow<py::dict>(p);
458: 		auto result = TryReplacement(local_dict, py_table_name, *registered_objects);
459: 		if (result) {
460: 			return result;
461: 		}
462: 	}
463: 	// otherwise look in the globals
464: 	auto global_dict = py::globals();
465: 	return TryReplacement(global_dict, py_table_name, *registered_objects);
466: }
467: 
468: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Connect(const string &database, bool read_only,
469:                                                            const py::dict &config_dict) {
470: 	auto res = make_shared<DuckDBPyConnection>();
471: 	DBConfig config;
472: 	if (read_only) {
473: 		config.access_mode = AccessMode::READ_ONLY;
474: 	}
475: 	for (auto &kv : config_dict) {
476: 		string key = py::str(kv.first);
477: 		string val = py::str(kv.second);
478: 		auto config_property = DBConfig::GetOptionByName(key);
479: 		if (!config_property) {
480: 			throw InvalidInputException("Unrecognized configuration property \"%s\"", key);
481: 		}
482: 		config.SetOption(*config_property, Value(val));
483: 	}
484: 	if (config.enable_external_access) {
485: 		//		ReplacementScan replacement_scan(ScanReplacement, (void *) &res->registered_objects);
486: 		config.replacement_scans.emplace_back(ScanReplacement, (void *)&res->registered_objects);
487: 	}
488: 
489: 	res->database = make_unique<DuckDB>(database, &config);
490: 	res->connection = make_unique<Connection>(*res->database);
491: 
492: 	PandasScanFunction scan_fun;
493: 	CreateTableFunctionInfo scan_info(scan_fun);
494: 
495: 	MapFunction map_fun;
496: 	CreateTableFunctionInfo map_info(map_fun);
497: 
498: 	auto &context = *res->connection->context;
499: 	auto &catalog = Catalog::GetCatalog(context);
500: 	context.transaction.BeginTransaction();
501: 	catalog.CreateTableFunction(context, &scan_info);
502: 	catalog.CreateTableFunction(context, &map_info);
503: 
504: 	context.transaction.Commit();
505: 
506: 	return res;
507: }
508: 
509: vector<Value> DuckDBPyConnection::TransformPythonParamList(py::handle params) {
510: 	vector<Value> args;
511: 
512: 	auto datetime_mod = py::module::import("datetime");
513: 	auto datetime_date = datetime_mod.attr("date");
514: 	auto datetime_datetime = datetime_mod.attr("datetime");
515: 	auto datetime_time = datetime_mod.attr("time");
516: 	auto decimal_mod = py::module::import("decimal");
517: 	auto decimal_decimal = decimal_mod.attr("Decimal");
518: 
519: 	for (pybind11::handle ele : params) {
520: 		if (ele.is_none()) {
521: 			args.emplace_back();
522: 		} else if (py::isinstance<py::bool_>(ele)) {
523: 			args.push_back(Value::BOOLEAN(ele.cast<bool>()));
524: 		} else if (py::isinstance<py::int_>(ele)) {
525: 			args.push_back(Value::BIGINT(ele.cast<int64_t>()));
526: 		} else if (py::isinstance<py::float_>(ele)) {
527: 			args.push_back(Value::DOUBLE(ele.cast<double>()));
528: 		} else if (py::isinstance(ele, decimal_decimal)) {
529: 			args.emplace_back(py::str(ele).cast<string>());
530: 		} else if (py::isinstance(ele, datetime_datetime)) {
531: 			auto year = PyDateTime_GET_YEAR(ele.ptr());
532: 			auto month = PyDateTime_GET_MONTH(ele.ptr());
533: 			auto day = PyDateTime_GET_DAY(ele.ptr());
534: 			auto hour = PyDateTime_DATE_GET_HOUR(ele.ptr());
535: 			auto minute = PyDateTime_DATE_GET_MINUTE(ele.ptr());
536: 			auto second = PyDateTime_DATE_GET_SECOND(ele.ptr());
537: 			auto micros = PyDateTime_DATE_GET_MICROSECOND(ele.ptr());
538: 			args.push_back(Value::TIMESTAMP(year, month, day, hour, minute, second, micros));
539: 		} else if (py::isinstance(ele, datetime_time)) {
540: 			auto hour = PyDateTime_TIME_GET_HOUR(ele.ptr());
541: 			auto minute = PyDateTime_TIME_GET_MINUTE(ele.ptr());
542: 			auto second = PyDateTime_TIME_GET_SECOND(ele.ptr());
543: 			auto micros = PyDateTime_TIME_GET_MICROSECOND(ele.ptr());
544: 			args.push_back(Value::TIME(hour, minute, second, micros));
545: 		} else if (py::isinstance(ele, datetime_date)) {
546: 			auto year = PyDateTime_GET_YEAR(ele.ptr());
547: 			auto month = PyDateTime_GET_MONTH(ele.ptr());
548: 			auto day = PyDateTime_GET_DAY(ele.ptr());
549: 			args.push_back(Value::DATE(year, month, day));
550: 		} else if (py::isinstance<py::str>(ele)) {
551: 			args.emplace_back(ele.cast<string>());
552: 		} else if (py::isinstance<py::memoryview>(ele)) {
553: 			py::memoryview py_view = ele.cast<py::memoryview>();
554: 			PyObject *py_view_ptr = py_view.ptr();
555: 			Py_buffer *py_buf = PyMemoryView_GET_BUFFER(py_view_ptr);
556: 			args.emplace_back(Value::BLOB(const_data_ptr_t(py_buf->buf), idx_t(py_buf->len)));
557: 		} else if (py::isinstance<py::bytes>(ele)) {
558: 			const string &ele_string = ele.cast<string>();
559: 			args.emplace_back(Value::BLOB(const_data_ptr_t(ele_string.data()), ele_string.size()));
560: 		} else {
561: 			throw std::runtime_error("unknown param type " + py::str(ele.get_type()).cast<string>());
562: 		}
563: 	}
564: 	return args;
565: }
566: 
567: DuckDBPyConnection *DuckDBPyConnection::DefaultConnection() {
568: 	if (!default_connection) {
569: 		py::dict config_dict;
570: 		default_connection = DuckDBPyConnection::Connect(":memory:", false, config_dict);
571: 	}
572: 	return default_connection.get();
573: }
574: 
575: void DuckDBPyConnection::Cleanup() {
576: 	default_connection.reset();
577: }
578: 
579: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection.cpp]
[start of tools/rpkg/src/scan.cpp]
1: #include "rapi.hpp"
2: #include "typesr.hpp"
3: #include "altrepstring.hpp"
4: 
5: #include "duckdb/main/client_context.hpp"
6: 
7: using namespace duckdb;
8: 
9: template <class SRC, class DST, class RTYPE>
10: static void AppendColumnSegment(SRC *source_data, Vector &result, idx_t count) {
11: 	auto result_data = FlatVector::GetData<DST>(result);
12: 	auto &result_mask = FlatVector::Validity(result);
13: 	for (idx_t i = 0; i < count; i++) {
14: 		auto val = source_data[i];
15: 		if (RTYPE::IsNull(val)) {
16: 			result_mask.SetInvalid(i);
17: 		} else {
18: 			result_data[i] = RTYPE::Convert(val);
19: 		}
20: 	}
21: }
22: 
23: static void AppendStringSegment(SEXP coldata, Vector &result, idx_t row_idx, idx_t count) {
24: 	auto result_data = FlatVector::GetData<string_t>(result);
25: 	auto &result_mask = FlatVector::Validity(result);
26: 	for (idx_t i = 0; i < count; i++) {
27: 		SEXP val = STRING_ELT(coldata, row_idx + i);
28: 		if (val == NA_STRING) {
29: 			result_mask.SetInvalid(i);
30: 		} else {
31: 			result_data[i] = string_t((char *)CHAR(val));
32: 		}
33: 	}
34: }
35: 
36: struct DataFrameScanFunctionData : public TableFunctionData {
37: 	DataFrameScanFunctionData(SEXP df, idx_t row_count, vector<RType> rtypes)
38: 	    : df(df), row_count(row_count), rtypes(rtypes) {
39: 	}
40: 	SEXP df;
41: 	idx_t row_count;
42: 	vector<RType> rtypes;
43: };
44: 
45: struct DataFrameScanState : public FunctionOperatorData {
46: 	DataFrameScanState() : position(0) {
47: 	}
48: 
49: 	idx_t position;
50: };
51: 
52: static unique_ptr<FunctionData> dataframe_scan_bind(ClientContext &context, vector<Value> &inputs,
53:                                                     unordered_map<string, Value> &named_parameters,
54:                                                     vector<LogicalType> &input_table_types,
55:                                                     vector<string> &input_table_names,
56:                                                     vector<LogicalType> &return_types, vector<string> &names) {
57: 	RProtector r;
58: 	SEXP df((SEXP)inputs[0].GetPointer());
59: 
60: 	auto df_names = r.Protect(GET_NAMES(df));
61: 	vector<RType> rtypes;
62: 
63: 	for (idx_t col_idx = 0; col_idx < (idx_t)Rf_length(df); col_idx++) {
64: 		auto column_name = string(CHAR(STRING_ELT(df_names, col_idx)));
65: 		names.push_back(column_name);
66: 		SEXP coldata = VECTOR_ELT(df, col_idx);
67: 		rtypes.push_back(RApiTypes::DetectRType(coldata));
68: 		LogicalType duckdb_col_type;
69: 		switch (rtypes[col_idx]) {
70: 		case RType::LOGICAL:
71: 			duckdb_col_type = LogicalType::BOOLEAN;
72: 			break;
73: 		case RType::INTEGER:
74: 			duckdb_col_type = LogicalType::INTEGER;
75: 			break;
76: 		case RType::NUMERIC:
77: 			duckdb_col_type = LogicalType::DOUBLE;
78: 			break;
79: 		case RType::FACTOR: {
80: 			// TODO What about factors that use numeric?
81: 
82: 			auto levels = r.Protect(GET_LEVELS(coldata));
83: 			idx_t size = LENGTH(levels);
84: 			Vector duckdb_levels(LogicalType::VARCHAR, size);
85: 			for (idx_t level_idx = 0; level_idx < size; level_idx++) {
86: 				duckdb_levels.SetValue(level_idx, string(CHAR(STRING_ELT(levels, level_idx))));
87: 			}
88: 			duckdb_col_type = LogicalType::ENUM(column_name, duckdb_levels, size);
89: 			break;
90: 		}
91: 		case RType::STRING:
92: 			duckdb_col_type = LogicalType::VARCHAR;
93: 			break;
94: 		case RType::TIMESTAMP:
95: 			duckdb_col_type = LogicalType::TIMESTAMP;
96: 			break;
97: 		case RType::TIME_SECONDS:
98: 		case RType::TIME_MINUTES:
99: 		case RType::TIME_HOURS:
100: 		case RType::TIME_DAYS:
101: 		case RType::TIME_WEEKS:
102: 		case RType::TIME_SECONDS_INTEGER:
103: 		case RType::TIME_MINUTES_INTEGER:
104: 		case RType::TIME_HOURS_INTEGER:
105: 		case RType::TIME_DAYS_INTEGER:
106: 		case RType::TIME_WEEKS_INTEGER:
107: 			duckdb_col_type = LogicalType::TIME;
108: 			break;
109: 		case RType::DATE:
110: 		case RType::DATE_INTEGER:
111: 			duckdb_col_type = LogicalType::DATE;
112: 			break;
113: 		default:
114: 			Rf_error("Unsupported column type for scan");
115: 		}
116: 		return_types.push_back(duckdb_col_type);
117: 	}
118: 
119: 	auto row_count = Rf_length(VECTOR_ELT(df, 0));
120: 	return make_unique<DataFrameScanFunctionData>(df, row_count, rtypes);
121: }
122: 
123: static unique_ptr<FunctionOperatorData> dataframe_scan_init(ClientContext &context, const FunctionData *bind_data,
124:                                                             const vector<column_t> &column_ids,
125:                                                             TableFilterCollection *filters) {
126: 	return make_unique<DataFrameScanState>();
127: }
128: 
129: static void dataframe_scan_function(ClientContext &context, const FunctionData *bind_data,
130:                                     FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
131: 	auto &data = (DataFrameScanFunctionData &)*bind_data;
132: 	auto &state = (DataFrameScanState &)*operator_state;
133: 	if (state.position >= data.row_count) {
134: 		return;
135: 	}
136: 	idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, data.row_count - state.position);
137: 
138: 	output.SetCardinality(this_count);
139: 
140: 	for (idx_t col_idx = 0; col_idx < output.ColumnCount(); col_idx++) {
141: 		auto &v = output.data[col_idx];
142: 		SEXP coldata = VECTOR_ELT(data.df, col_idx);
143: 
144: 		switch (data.rtypes[col_idx]) {
145: 		case RType::LOGICAL: {
146: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
147: 			AppendColumnSegment<int, bool, RBooleanType>(data_ptr, v, this_count);
148: 			break;
149: 		}
150: 		case RType::INTEGER: {
151: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
152: 			AppendColumnSegment<int, int, RIntegerType>(data_ptr, v, this_count);
153: 			break;
154: 		}
155: 		case RType::NUMERIC: {
156: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
157: 			AppendColumnSegment<double, double, RDoubleType>(data_ptr, v, this_count);
158: 			break;
159: 		}
160: 		case RType::STRING: {
161: 			AppendStringSegment(coldata, v, state.position, this_count);
162: 			break;
163: 		}
164: 		case RType::FACTOR: {
165: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
166: 			switch (v.GetType().InternalType()) {
167: 			case PhysicalType::UINT8:
168: 				AppendColumnSegment<int, uint8_t, RFactorType>(data_ptr, v, this_count);
169: 				break;
170: 
171: 			case PhysicalType::UINT16:
172: 				AppendColumnSegment<int, uint16_t, RFactorType>(data_ptr, v, this_count);
173: 				break;
174: 
175: 			case PhysicalType::UINT32:
176: 				AppendColumnSegment<int, uint32_t, RFactorType>(data_ptr, v, this_count);
177: 				break;
178: 
179: 			default:
180: 				Rf_error("duckdb_execute_R: Unknown enum type for scan: %s",
181: 				         TypeIdToString(v.GetType().InternalType()).c_str());
182: 			}
183: 			break;
184: 		}
185: 		case RType::TIMESTAMP: {
186: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
187: 			AppendColumnSegment<double, timestamp_t, RTimestampType>(data_ptr, v, this_count);
188: 			break;
189: 		}
190: 		case RType::TIME_SECONDS: {
191: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
192: 			AppendColumnSegment<double, dtime_t, RTimeSecondsType>(data_ptr, v, this_count);
193: 			break;
194: 		}
195: 		case RType::TIME_MINUTES: {
196: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
197: 			AppendColumnSegment<double, dtime_t, RTimeMinutesType>(data_ptr, v, this_count);
198: 			break;
199: 		}
200: 		case RType::TIME_HOURS: {
201: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
202: 			AppendColumnSegment<double, dtime_t, RTimeHoursType>(data_ptr, v, this_count);
203: 			break;
204: 		}
205: 		case RType::TIME_DAYS: {
206: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
207: 			AppendColumnSegment<double, dtime_t, RTimeDaysType>(data_ptr, v, this_count);
208: 			break;
209: 		}
210: 		case RType::TIME_WEEKS: {
211: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
212: 			AppendColumnSegment<double, dtime_t, RTimeWeeksType>(data_ptr, v, this_count);
213: 			break;
214: 		}
215: 		case RType::TIME_SECONDS_INTEGER: {
216: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
217: 			AppendColumnSegment<int, dtime_t, RTimeSecondsType>(data_ptr, v, this_count);
218: 			break;
219: 		}
220: 		case RType::TIME_MINUTES_INTEGER: {
221: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
222: 			AppendColumnSegment<int, dtime_t, RTimeMinutesType>(data_ptr, v, this_count);
223: 			break;
224: 		}
225: 		case RType::TIME_HOURS_INTEGER: {
226: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
227: 			AppendColumnSegment<int, dtime_t, RTimeHoursType>(data_ptr, v, this_count);
228: 			break;
229: 		}
230: 		case RType::TIME_DAYS_INTEGER: {
231: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
232: 			AppendColumnSegment<int, dtime_t, RTimeDaysType>(data_ptr, v, this_count);
233: 			break;
234: 		}
235: 		case RType::TIME_WEEKS_INTEGER: {
236: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
237: 			AppendColumnSegment<int, dtime_t, RTimeWeeksType>(data_ptr, v, this_count);
238: 			break;
239: 		}
240: 		case RType::DATE: {
241: 			auto data_ptr = NUMERIC_POINTER(coldata) + state.position;
242: 			AppendColumnSegment<double, date_t, RDateType>(data_ptr, v, this_count);
243: 			break;
244: 		}
245: 		case RType::DATE_INTEGER: {
246: 			auto data_ptr = INTEGER_POINTER(coldata) + state.position;
247: 			AppendColumnSegment<int, date_t, RDateType>(data_ptr, v, this_count);
248: 			break;
249: 		}
250: 		default:
251: 			throw;
252: 		}
253: 	}
254: 
255: 	state.position += this_count;
256: }
257: 
258: static unique_ptr<NodeStatistics> dataframe_scan_cardinality(ClientContext &context, const FunctionData *bind_data) {
259: 	auto &data = (DataFrameScanFunctionData &)*bind_data;
260: 	return make_unique<NodeStatistics>(data.row_count, data.row_count);
261: }
262: 
263: DataFrameScanFunction::DataFrameScanFunction()
264:     : TableFunction("r_dataframe_scan", {LogicalType::POINTER}, dataframe_scan_function, dataframe_scan_bind,
265:                     dataframe_scan_init, nullptr, nullptr, nullptr, dataframe_scan_cardinality) {};
[end of tools/rpkg/src/scan.cpp]
[start of tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp]
1: #include "sqlite3.h"
2: #include "udf_struct_sqlite3.h"
3: #include "sqlite3_udf_wrapper.hpp"
4: 
5: #include "duckdb.hpp"
6: #include "duckdb/parser/parser.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/common/types.hpp"
9: #include "duckdb/common/operator/cast_operators.hpp"
10: 
11: #include "utf8proc_wrapper.hpp"
12: 
13: #include <ctype.h>
14: #include <stdio.h>
15: #include <stdlib.h>
16: #include <string.h>
17: #include <time.h>
18: #include <string>
19: #include <chrono>
20: #include <cassert>
21: #include <climits>
22: 
23: using namespace duckdb;
24: using namespace std;
25: 
26: static char *sqlite3_strdup(const char *str);
27: 
28: struct sqlite3_string_buffer {
29: 	//! String data
30: 	unique_ptr<char[]> data;
31: 	//! String length
32: 	int data_len;
33: };
34: 
35: struct sqlite3_stmt {
36: 	//! The DB object that this statement belongs to
37: 	sqlite3 *db;
38: 	//! The query string
39: 	string query_string;
40: 	//! The prepared statement object, if successfully prepared
41: 	unique_ptr<PreparedStatement> prepared;
42: 	//! The result object, if successfully executed
43: 	unique_ptr<QueryResult> result;
44: 	//! The current chunk that we are iterating over
45: 	unique_ptr<DataChunk> current_chunk;
46: 	//! The current row into the current chunk that we are iterating over
47: 	int64_t current_row;
48: 	//! Bound values, used for binding to the prepared statement
49: 	vector<Value> bound_values;
50: 	//! Names of the prepared parameters
51: 	vector<string> bound_names;
52: 	//! The current column values converted to string, used and filled by sqlite3_column_text
53: 	unique_ptr<sqlite3_string_buffer[]> current_text;
54: };
55: 
56: void sqlite3_randomness(int N, void *pBuf) {
57: 	static bool init = false;
58: 	if (!init) {
59: 		srand(time(NULL));
60: 		init = true;
61: 	}
62: 	unsigned char *zBuf = (unsigned char *)pBuf;
63: 	while (N--) {
64: 		unsigned char nextByte = rand() % 255;
65: 		zBuf[N] = nextByte;
66: 	}
67: }
68: 
69: int sqlite3_open(const char *filename, /* Database filename (UTF-8) */
70:                  sqlite3 **ppDb        /* OUT: SQLite db handle */
71: ) {
72: 	return sqlite3_open_v2(filename, ppDb, 0, NULL);
73: }
74: 
75: int sqlite3_open_v2(const char *filename, /* Database filename (UTF-8) */
76:                     sqlite3 **ppDb,       /* OUT: SQLite db handle */
77:                     int flags,            /* Flags */
78:                     const char *zVfs      /* Name of VFS module to use */
79: ) {
80: 	if (filename && strcmp(filename, ":memory:") == 0) {
81: 		filename = NULL;
82: 	}
83: 	*ppDb = nullptr;
84: 	if (zVfs) { /* unsupported so if set we complain */
85: 		return SQLITE_ERROR;
86: 	}
87: 	int rc = SQLITE_OK;
88: 	sqlite3 *pDb = nullptr;
89: 	try {
90: 		pDb = new sqlite3();
91: 		DBConfig config;
92: 		config.access_mode = AccessMode::AUTOMATIC;
93: 		if (flags & SQLITE_OPEN_READONLY) {
94: 			config.access_mode = AccessMode::READ_ONLY;
95: 		}
96: 		pDb->db = make_unique<DuckDB>(filename, &config);
97: 		pDb->con = make_unique<Connection>(*pDb->db);
98: 	} catch (std::exception &ex) {
99: 		if (pDb) {
100: 			pDb->last_error = ex.what();
101: 			pDb->errCode = SQLITE_ERROR;
102: 		}
103: 		rc = SQLITE_ERROR;
104: 	}
105: 	*ppDb = pDb;
106: 	return rc;
107: }
108: 
109: int sqlite3_close(sqlite3 *db) {
110: 	if (db) {
111: 		delete db;
112: 	}
113: 	return SQLITE_OK;
114: }
115: 
116: int sqlite3_shutdown(void) {
117: 	return SQLITE_OK;
118: }
119: 
120: /* In SQLite this function compiles the query into VDBE bytecode,
121:  * in the implementation it currently executes the query */
122: // TODO: prepare the statement instead of executing right away
123: int sqlite3_prepare_v2(sqlite3 *db,           /* Database handle */
124:                        const char *zSql,      /* SQL statement, UTF-8 encoded */
125:                        int nByte,             /* Maximum length of zSql in bytes. */
126:                        sqlite3_stmt **ppStmt, /* OUT: Statement handle */
127:                        const char **pzTail    /* OUT: Pointer to unused portion of zSql */
128: ) {
129: 	if (!db || !ppStmt || !zSql) {
130: 		return SQLITE_MISUSE;
131: 	}
132: 	*ppStmt = nullptr;
133: 	string query = nByte < 0 ? zSql : string(zSql, nByte);
134: 	if (pzTail) {
135: 		*pzTail = zSql + query.size();
136: 	}
137: 	try {
138: 		Parser parser;
139: 		parser.ParseQuery(query);
140: 		if (parser.statements.size() == 0) {
141: 			return SQLITE_OK;
142: 		}
143: 		// extract the remainder
144: 		idx_t next_location = parser.statements[0]->stmt_location + parser.statements[0]->stmt_length;
145: 		bool set_remainder = next_location < query.size();
146: 
147: 		// extract the first statement
148: 		vector<unique_ptr<SQLStatement>> statements;
149: 		statements.push_back(move(parser.statements[0]));
150: 
151: 		db->con->context->HandlePragmaStatements(statements);
152: 
153: 		// if there are multiple statements here, we are dealing with an import database statement
154: 		// we directly execute all statements besides the final one
155: 		for (idx_t i = 0; i + 1 < statements.size(); i++) {
156: 			auto res = db->con->Query(move(statements[i]));
157: 			if (!res->success) {
158: 				db->last_error = res->error;
159: 				return SQLITE_ERROR;
160: 			}
161: 		}
162: 
163: 		// now prepare the query
164: 		auto prepared = db->con->Prepare(move(statements.back()));
165: 		if (!prepared->success) {
166: 			// failed to prepare: set the error message
167: 			db->last_error = prepared->error;
168: 			return SQLITE_ERROR;
169: 		}
170: 
171: 		// create the statement entry
172: 		unique_ptr<sqlite3_stmt> stmt = make_unique<sqlite3_stmt>();
173: 		stmt->db = db;
174: 		stmt->query_string = query;
175: 		stmt->prepared = move(prepared);
176: 		stmt->current_row = -1;
177: 		for (idx_t i = 0; i < stmt->prepared->n_param; i++) {
178: 			stmt->bound_names.push_back("$" + to_string(i + 1));
179: 			stmt->bound_values.push_back(Value());
180: 		}
181: 
182: 		// extract the remainder of the query and assign it to the pzTail
183: 		if (pzTail && set_remainder) {
184: 			*pzTail = zSql + next_location + 1;
185: 		}
186: 
187: 		*ppStmt = stmt.release();
188: 		return SQLITE_OK;
189: 	} catch (std::exception &ex) {
190: 		db->last_error = ex.what();
191: 		return SQLITE_ERROR;
192: 	}
193: }
194: 
195: bool sqlite3_display_result(StatementType type) {
196: 	switch (type) {
197: 	case StatementType::EXECUTE_STATEMENT:
198: 	case StatementType::EXPLAIN_STATEMENT:
199: 	case StatementType::PRAGMA_STATEMENT:
200: 	case StatementType::SELECT_STATEMENT:
201: 	case StatementType::SHOW_STATEMENT:
202: 		return true;
203: 	default:
204: 		return false;
205: 	}
206: }
207: 
208: /* Prepare the next result to be retrieved */
209: int sqlite3_step(sqlite3_stmt *pStmt) {
210: 	if (!pStmt) {
211: 		return SQLITE_MISUSE;
212: 	}
213: 	if (!pStmt->prepared) {
214: 		pStmt->db->last_error = "Attempting sqlite3_step() on a non-successfully prepared statement";
215: 		return SQLITE_ERROR;
216: 	}
217: 	pStmt->current_text = nullptr;
218: 	if (!pStmt->result) {
219: 		// no result yet! call Execute()
220: 		pStmt->result = pStmt->prepared->Execute(pStmt->bound_values, true);
221: 		if (!pStmt->result->success) {
222: 			// error in execute: clear prepared statement
223: 			pStmt->db->last_error = pStmt->result->error;
224: 			pStmt->prepared = nullptr;
225: 			return SQLITE_ERROR;
226: 		}
227: 		// fetch a chunk
228: 		if (!pStmt->result->TryFetch(pStmt->current_chunk, pStmt->db->last_error)) {
229: 			pStmt->prepared = nullptr;
230: 			return SQLITE_ERROR;
231: 		}
232: 
233: 		pStmt->current_row = -1;
234: 
235: 		auto statement_type = pStmt->prepared->GetStatementType();
236: 		if (StatementTypeReturnChanges(statement_type) && pStmt->current_chunk->size() > 0) {
237: 			// update total changes
238: 			auto row_changes = pStmt->current_chunk->GetValue(0, 0);
239: 			if (!row_changes.IsNull() && row_changes.TryCastAs(LogicalType::BIGINT)) {
240: 				pStmt->db->last_changes = row_changes.GetValue<int64_t>();
241: 				pStmt->db->total_changes += row_changes.GetValue<int64_t>();
242: 			}
243: 		}
244: 		if (!sqlite3_display_result(statement_type)) {
245: 			// only SELECT statements return results
246: 			sqlite3_reset(pStmt);
247: 		}
248: 	}
249: 	if (!pStmt->current_chunk || pStmt->current_chunk->size() == 0) {
250: 		return SQLITE_DONE;
251: 	}
252: 	pStmt->current_row++;
253: 	if (pStmt->current_row >= (int32_t)pStmt->current_chunk->size()) {
254: 		// have to fetch again!
255: 		pStmt->current_row = 0;
256: 		if (!pStmt->result->TryFetch(pStmt->current_chunk, pStmt->db->last_error)) {
257: 			pStmt->prepared = nullptr;
258: 			return SQLITE_ERROR;
259: 		}
260: 		if (!pStmt->current_chunk || pStmt->current_chunk->size() == 0) {
261: 			sqlite3_reset(pStmt);
262: 			return SQLITE_DONE;
263: 		}
264: 	}
265: 	return SQLITE_ROW;
266: }
267: 
268: /* Execute multiple semicolon separated SQL statements
269:  * and execute the passed callback for each produced result,
270:  * largely copied from the original sqlite3 source */
271: int sqlite3_exec(sqlite3 *db,                /* The database on which the SQL executes */
272:                  const char *zSql,           /* The SQL to be executed */
273:                  sqlite3_callback xCallback, /* Invoke this callback routine */
274:                  void *pArg,                 /* First argument to xCallback() */
275:                  char **pzErrMsg             /* Write error messages here */
276: ) {
277: 	int rc = SQLITE_OK;            /* Return code */
278: 	const char *zLeftover;         /* Tail of unprocessed SQL */
279: 	sqlite3_stmt *pStmt = nullptr; /* The current SQL statement */
280: 	char **azCols = nullptr;       /* Names of result columns */
281: 	char **azVals = nullptr;       /* Result values */
282: 
283: 	if (zSql == nullptr) {
284: 		zSql = "";
285: 	}
286: 
287: 	while (rc == SQLITE_OK && zSql[0]) {
288: 		int nCol;
289: 
290: 		pStmt = nullptr;
291: 		rc = sqlite3_prepare_v2(db, zSql, -1, &pStmt, &zLeftover);
292: 		if (rc != SQLITE_OK) {
293: 			if (pzErrMsg) {
294: 				auto errmsg = sqlite3_errmsg(db);
295: 				*pzErrMsg = errmsg ? sqlite3_strdup(errmsg) : nullptr;
296: 			}
297: 			continue;
298: 		}
299: 		if (!pStmt) {
300: 			/* this happens for a comment or white-space */
301: 			zSql = zLeftover;
302: 			continue;
303: 		}
304: 
305: 		nCol = sqlite3_column_count(pStmt);
306: 		azCols = (char **)malloc(nCol * sizeof(const char *));
307: 		azVals = (char **)malloc(nCol * sizeof(const char *));
308: 		if (!azCols || !azVals) {
309: 			goto exec_out;
310: 		}
311: 		for (int i = 0; i < nCol; i++) {
312: 			azCols[i] = (char *)sqlite3_column_name(pStmt, i);
313: 		}
314: 
315: 		while (true) {
316: 			rc = sqlite3_step(pStmt);
317: 
318: 			/* Invoke the callback function if required */
319: 			if (xCallback && rc == SQLITE_ROW) {
320: 				for (int i = 0; i < nCol; i++) {
321: 					azVals[i] = (char *)sqlite3_column_text(pStmt, i);
322: 					if (!azVals[i] && sqlite3_column_type(pStmt, i) != SQLITE_NULL) {
323: 						fprintf(stderr, "sqlite3_exec: out of memory.\n");
324: 						goto exec_out;
325: 					}
326: 				}
327: 				if (xCallback(pArg, nCol, azVals, azCols)) {
328: 					/* EVIDENCE-OF: R-38229-40159 If the callback function to
329: 					** sqlite3_exec() returns non-zero, then sqlite3_exec() will
330: 					** return SQLITE_ABORT. */
331: 					rc = SQLITE_ABORT;
332: 					sqlite3_finalize(pStmt);
333: 					pStmt = 0;
334: 					fprintf(stderr, "sqlite3_exec: callback returned non-zero. "
335: 					                "Aborting.\n");
336: 					goto exec_out;
337: 				}
338: 			}
339: 			if (rc == SQLITE_DONE) {
340: 				rc = sqlite3_finalize(pStmt);
341: 				pStmt = nullptr;
342: 				zSql = zLeftover;
343: 				while (isspace(zSql[0]))
344: 					zSql++;
345: 				break;
346: 			} else if (rc != SQLITE_ROW) {
347: 				// error
348: 				if (pzErrMsg) {
349: 					auto errmsg = sqlite3_errmsg(db);
350: 					*pzErrMsg = errmsg ? sqlite3_strdup(errmsg) : nullptr;
351: 				}
352: 				goto exec_out;
353: 			}
354: 		}
355: 
356: 		sqlite3_free(azCols);
357: 		sqlite3_free(azVals);
358: 		azCols = nullptr;
359: 		azVals = nullptr;
360: 	}
361: 
362: exec_out:
363: 	if (pStmt) {
364: 		sqlite3_finalize(pStmt);
365: 	}
366: 	sqlite3_free(azCols);
367: 	sqlite3_free(azVals);
368: 	if (rc != SQLITE_OK && pzErrMsg && !*pzErrMsg) {
369: 		// error but no error message set
370: 		*pzErrMsg = sqlite3_strdup("Unknown error in DuckDB!");
371: 	}
372: 	return rc;
373: }
374: 
375: /* Return the text of the SQL that was used to prepare the statement */
376: const char *sqlite3_sql(sqlite3_stmt *pStmt) {
377: 	return pStmt->query_string.c_str();
378: }
379: 
380: int sqlite3_column_count(sqlite3_stmt *pStmt) {
381: 	if (!pStmt || !pStmt->prepared) {
382: 		return 0;
383: 	}
384: 	return (int)pStmt->prepared->ColumnCount();
385: }
386: 
387: ////////////////////////////
388: //     sqlite3_column     //
389: ////////////////////////////
390: int sqlite3_column_type(sqlite3_stmt *pStmt, int iCol) {
391: 	if (!pStmt || !pStmt->result || !pStmt->current_chunk) {
392: 		return 0;
393: 	}
394: 	if (FlatVector::IsNull(pStmt->current_chunk->data[iCol], pStmt->current_row)) {
395: 		return SQLITE_NULL;
396: 	}
397: 	auto column_type = pStmt->result->types[iCol];
398: 	switch (column_type.id()) {
399: 	case LogicalTypeId::BOOLEAN:
400: 	case LogicalTypeId::TINYINT:
401: 	case LogicalTypeId::SMALLINT:
402: 	case LogicalTypeId::INTEGER:
403: 	case LogicalTypeId::BIGINT: /* TODO: Maybe blob? */
404: 		return SQLITE_INTEGER;
405: 	case LogicalTypeId::FLOAT:
406: 	case LogicalTypeId::DOUBLE:
407: 	case LogicalTypeId::DECIMAL:
408: 		return SQLITE_FLOAT;
409: 	case LogicalTypeId::DATE:
410: 	case LogicalTypeId::TIME:
411: 	case LogicalTypeId::TIMESTAMP:
412: 	case LogicalTypeId::TIMESTAMP_SEC:
413: 	case LogicalTypeId::TIMESTAMP_MS:
414: 	case LogicalTypeId::TIMESTAMP_NS:
415: 	case LogicalTypeId::VARCHAR:
416: 	case LogicalTypeId::LIST:
417: 	case LogicalTypeId::STRUCT:
418: 	case LogicalTypeId::MAP:
419: 		return SQLITE_TEXT;
420: 	case LogicalTypeId::BLOB:
421: 		return SQLITE_BLOB;
422: 	default:
423: 		// TODO(wangfenjin): agg function don't have type?
424: 		return SQLITE_TEXT;
425: 	}
426: 	return 0;
427: }
428: 
429: const char *sqlite3_column_name(sqlite3_stmt *pStmt, int N) {
430: 	if (!pStmt || !pStmt->prepared) {
431: 		return nullptr;
432: 	}
433: 	return pStmt->prepared->GetNames()[N].c_str();
434: }
435: 
436: static bool sqlite3_column_has_value(sqlite3_stmt *pStmt, int iCol, LogicalType target_type, Value &val) {
437: 	if (!pStmt || !pStmt->result || !pStmt->current_chunk) {
438: 		return false;
439: 	}
440: 	if (iCol < 0 || iCol >= (int)pStmt->result->types.size()) {
441: 		return false;
442: 	}
443: 	if (FlatVector::IsNull(pStmt->current_chunk->data[iCol], pStmt->current_row)) {
444: 		return false;
445: 	}
446: 	try {
447: 		val = pStmt->current_chunk->data[iCol].GetValue(pStmt->current_row).CastAs(target_type);
448: 	} catch (...) {
449: 		return false;
450: 	}
451: 	return true;
452: }
453: 
454: double sqlite3_column_double(sqlite3_stmt *stmt, int iCol) {
455: 	Value val;
456: 	if (!sqlite3_column_has_value(stmt, iCol, LogicalType::DOUBLE, val)) {
457: 		return 0;
458: 	}
459: 	return DoubleValue::Get(val);
460: }
461: 
462: int sqlite3_column_int(sqlite3_stmt *stmt, int iCol) {
463: 	Value val;
464: 	if (!sqlite3_column_has_value(stmt, iCol, LogicalType::INTEGER, val)) {
465: 		return 0;
466: 	}
467: 	return IntegerValue::Get(val);
468: }
469: 
470: sqlite3_int64 sqlite3_column_int64(sqlite3_stmt *stmt, int iCol) {
471: 	Value val;
472: 	if (!sqlite3_column_has_value(stmt, iCol, LogicalType::BIGINT, val)) {
473: 		return 0;
474: 	}
475: 	return BigIntValue::Get(val);
476: }
477: 
478: const unsigned char *sqlite3_column_text(sqlite3_stmt *pStmt, int iCol) {
479: 	Value val;
480: 	if (!sqlite3_column_has_value(pStmt, iCol, LogicalType::VARCHAR, val)) {
481: 		return nullptr;
482: 	}
483: 	try {
484: 		if (!pStmt->current_text) {
485: 			pStmt->current_text =
486: 			    unique_ptr<sqlite3_string_buffer[]>(new sqlite3_string_buffer[pStmt->result->types.size()]);
487: 		}
488: 		auto &entry = pStmt->current_text[iCol];
489: 		if (!entry.data) {
490: 			// not initialized yet, convert the value and initialize it
491: 			auto &str_val = StringValue::Get(val);
492: 			entry.data = unique_ptr<char[]>(new char[str_val.size() + 1]);
493: 			memcpy(entry.data.get(), str_val.c_str(), str_val.size() + 1);
494: 			entry.data_len = str_val.length();
495: 		}
496: 		return (const unsigned char *)entry.data.get();
497: 	} catch (...) {
498: 		// memory error!
499: 		return nullptr;
500: 	}
501: }
502: 
503: const void *sqlite3_column_blob(sqlite3_stmt *pStmt, int iCol) {
504: 	Value val;
505: 	if (!sqlite3_column_has_value(pStmt, iCol, LogicalType::BLOB, val)) {
506: 		return nullptr;
507: 	}
508: 	try {
509: 		if (!pStmt->current_text) {
510: 			pStmt->current_text =
511: 			    unique_ptr<sqlite3_string_buffer[]>(new sqlite3_string_buffer[pStmt->result->types.size()]);
512: 		}
513: 		auto &entry = pStmt->current_text[iCol];
514: 		if (!entry.data) {
515: 			// not initialized yet, convert the value and initialize it
516: 			auto &str_val = StringValue::Get(val);
517: 			entry.data = unique_ptr<char[]>(new char[str_val.size() + 1]);
518: 			memcpy(entry.data.get(), str_val.c_str(), str_val.size() + 1);
519: 			entry.data_len = str_val.length();
520: 		}
521: 		return (const unsigned char *)entry.data.get();
522: 	} catch (...) {
523: 		// memory error!
524: 		return nullptr;
525: 	}
526: }
527: 
528: ////////////////////////////
529: //      sqlite3_bind      //
530: ////////////////////////////
531: int sqlite3_bind_parameter_count(sqlite3_stmt *stmt) {
532: 	if (!stmt) {
533: 		return 0;
534: 	}
535: 	return stmt->prepared->n_param;
536: }
537: 
538: const char *sqlite3_bind_parameter_name(sqlite3_stmt *stmt, int idx) {
539: 	if (!stmt) {
540: 		return nullptr;
541: 	}
542: 	if (idx < 1 || idx > (int)stmt->prepared->n_param) {
543: 		return nullptr;
544: 	}
545: 	return stmt->bound_names[idx - 1].c_str();
546: }
547: 
548: int sqlite3_bind_parameter_index(sqlite3_stmt *stmt, const char *zName) {
549: 	if (!stmt || !zName) {
550: 		return 0;
551: 	}
552: 	for (idx_t i = 0; i < stmt->bound_names.size(); i++) {
553: 		if (stmt->bound_names[i] == string(zName)) {
554: 			return i + 1;
555: 		}
556: 	}
557: 	return 0;
558: }
559: 
560: int sqlite3_internal_bind_value(sqlite3_stmt *stmt, int idx, Value value) {
561: 	if (!stmt || !stmt->prepared || stmt->result) {
562: 		return SQLITE_MISUSE;
563: 	}
564: 	if (idx < 1 || idx > (int)stmt->prepared->n_param) {
565: 		return SQLITE_RANGE;
566: 	}
567: 	stmt->bound_values[idx - 1] = value;
568: 	return SQLITE_OK;
569: }
570: 
571: int sqlite3_bind_int(sqlite3_stmt *stmt, int idx, int val) {
572: 	return sqlite3_internal_bind_value(stmt, idx, Value::INTEGER(val));
573: }
574: 
575: int sqlite3_bind_int64(sqlite3_stmt *stmt, int idx, sqlite3_int64 val) {
576: 	return sqlite3_internal_bind_value(stmt, idx, Value::BIGINT(val));
577: }
578: 
579: int sqlite3_bind_double(sqlite3_stmt *stmt, int idx, double val) {
580: 	return sqlite3_internal_bind_value(stmt, idx, Value::DOUBLE(val));
581: }
582: 
583: int sqlite3_bind_null(sqlite3_stmt *stmt, int idx) {
584: 	return sqlite3_internal_bind_value(stmt, idx, Value());
585: }
586: 
587: SQLITE_API int sqlite3_bind_value(sqlite3_stmt *, int, const sqlite3_value *) {
588: 	fprintf(stderr, "sqlite3_bind_value: unsupported.\n");
589: 	return SQLITE_ERROR;
590: }
591: 
592: int sqlite3_bind_text(sqlite3_stmt *stmt, int idx, const char *val, int length, void (*free_func)(void *)) {
593: 	if (!val) {
594: 		return SQLITE_MISUSE;
595: 	}
596: 	string value;
597: 	if (length < 0) {
598: 		value = string(val);
599: 	} else {
600: 		value = string(val, length);
601: 	}
602: 	if (free_func && ((ptrdiff_t)free_func) != -1) {
603: 		free_func((void *)val);
604: 		val = nullptr;
605: 	}
606: 	try {
607: 		return sqlite3_internal_bind_value(stmt, idx, Value(value));
608: 	} catch (std::exception &ex) {
609: 		return SQLITE_ERROR;
610: 	}
611: }
612: 
613: int sqlite3_bind_blob(sqlite3_stmt *stmt, int idx, const void *val, int length, void (*free_func)(void *)) {
614: 	if (!val) {
615: 		return SQLITE_MISUSE;
616: 	}
617: 	Value blob;
618: 	if (length < 0) {
619: 		blob = Value::BLOB(string((const char *)val));
620: 	} else {
621: 		blob = Value::BLOB((const_data_ptr_t)val, length);
622: 	}
623: 	if (free_func && ((ptrdiff_t)free_func) != -1) {
624: 		free_func((void *)val);
625: 		val = nullptr;
626: 	}
627: 	try {
628: 		return sqlite3_internal_bind_value(stmt, idx, blob);
629: 	} catch (std::exception &ex) {
630: 		return SQLITE_ERROR;
631: 	}
632: }
633: 
634: SQLITE_API int sqlite3_bind_zeroblob(sqlite3_stmt *stmt, int idx, int length) {
635: 	fprintf(stderr, "sqlite3_bind_zeroblob: unsupported.\n");
636: 	return SQLITE_ERROR;
637: }
638: 
639: int sqlite3_clear_bindings(sqlite3_stmt *stmt) {
640: 	if (!stmt) {
641: 		return SQLITE_MISUSE;
642: 	}
643: 	return SQLITE_OK;
644: }
645: 
646: int sqlite3_initialize(void) {
647: 	return SQLITE_OK;
648: }
649: 
650: int sqlite3_finalize(sqlite3_stmt *pStmt) {
651: 	if (pStmt) {
652: 		if (pStmt->result && !pStmt->result->success) {
653: 			pStmt->db->last_error = string(pStmt->result->error);
654: 			delete pStmt;
655: 			return SQLITE_ERROR;
656: 		}
657: 
658: 		delete pStmt;
659: 	}
660: 	return SQLITE_OK;
661: }
662: 
663: /*
664: ** Some systems have stricmp().  Others have strcasecmp().  Because
665: ** there is no consistency, we will define our own.
666: **
667: ** IMPLEMENTATION-OF: R-30243-02494 The sqlite3_stricmp() and
668: ** sqlite3_strnicmp() APIs allow applications and extensions to compare
669: ** the contents of two buffers containing UTF-8 strings in a
670: ** case-independent fashion, using the same definition of "case
671: ** independence" that SQLite uses internally when comparing identifiers.
672: */
673: 
674: const unsigned char sqlite3UpperToLower[] = {
675:     0,   1,   2,   3,   4,   5,   6,   7,   8,   9,   10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,
676:     22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,
677:     44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  97,
678:     98,  99,  100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,
679:     120, 121, 122, 91,  92,  93,  94,  95,  96,  97,  98,  99,  100, 101, 102, 103, 104, 105, 106, 107, 108, 109,
680:     110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,
681:     132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
682:     154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,
683:     176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,
684:     198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,
685:     220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,
686:     242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255};
687: 
688: int sqlite3StrICmp(const char *zLeft, const char *zRight) {
689: 	unsigned char *a, *b;
690: 	int c;
691: 	a = (unsigned char *)zLeft;
692: 	b = (unsigned char *)zRight;
693: 	for (;;) {
694: 		c = (int)sqlite3UpperToLower[*a] - (int)sqlite3UpperToLower[*b];
695: 		if (c || *a == 0)
696: 			break;
697: 		a++;
698: 		b++;
699: 	}
700: 	return c;
701: }
702: 
703: SQLITE_API int sqlite3_stricmp(const char *zLeft, const char *zRight) {
704: 	if (zLeft == 0) {
705: 		return zRight ? -1 : 0;
706: 	} else if (zRight == 0) {
707: 		return 1;
708: 	}
709: 	return sqlite3StrICmp(zLeft, zRight);
710: }
711: 
712: SQLITE_API int sqlite3_strnicmp(const char *zLeft, const char *zRight, int N) {
713: 	unsigned char *a, *b;
714: 	if (zLeft == 0) {
715: 		return zRight ? -1 : 0;
716: 	} else if (zRight == 0) {
717: 		return 1;
718: 	}
719: 	a = (unsigned char *)zLeft;
720: 	b = (unsigned char *)zRight;
721: 	while (N-- > 0 && *a != 0 && sqlite3UpperToLower[*a] == sqlite3UpperToLower[*b]) {
722: 		a++;
723: 		b++;
724: 	}
725: 	return N < 0 ? 0 : sqlite3UpperToLower[*a] - sqlite3UpperToLower[*b];
726: }
727: 
728: char *sqlite3_strdup(const char *str) {
729: 	char *result = (char *)sqlite3_malloc64(strlen(str) + 1);
730: 	strcpy(result, str);
731: 	return result;
732: }
733: 
734: void *sqlite3_malloc64(sqlite3_uint64 n) {
735: 	return malloc(n);
736: }
737: 
738: void sqlite3_free(void *pVoid) {
739: 	free(pVoid);
740: }
741: 
742: void *sqlite3_malloc(int n) {
743: 	return sqlite3_malloc64(n);
744: }
745: 
746: void *sqlite3_realloc(void *ptr, int n) {
747: 	return sqlite3_realloc64(ptr, n);
748: }
749: 
750: void *sqlite3_realloc64(void *ptr, sqlite3_uint64 n) {
751: 	return realloc(ptr, n);
752: }
753: 
754: // TODO: stub
755: int sqlite3_config(int i, ...) {
756: 	return SQLITE_OK;
757: }
758: 
759: int sqlite3_errcode(sqlite3 *db) {
760: 	if (!db) {
761: 		return SQLITE_NOMEM;
762: 	}
763: 	// return db->last_error.empty() ? SQLITE_OK : SQLITE_ERROR;
764: 	return db->errCode; //! We should return the exact error code
765: }
766: 
767: int sqlite3_extended_errcode(sqlite3 *db) {
768: 	return sqlite3_errcode(db);
769: }
770: 
771: const char *sqlite3_errmsg(sqlite3 *db) {
772: 	if (!db) {
773: 		return "";
774: 	}
775: 	return db->last_error.c_str();
776: }
777: 
778: void sqlite3_interrupt(sqlite3 *db) {
779: 	if (db) {
780: 		db->con->Interrupt();
781: 	}
782: }
783: 
784: const char *sqlite3_libversion(void) {
785: 	return DuckDB::LibraryVersion();
786: }
787: 
788: const char *sqlite3_sourceid(void) {
789: 	return DuckDB::SourceID();
790: }
791: 
792: int sqlite3_reset(sqlite3_stmt *stmt) {
793: 	if (stmt) {
794: 		stmt->result = nullptr;
795: 		stmt->current_chunk = nullptr;
796: 	}
797: 	return SQLITE_OK;
798: }
799: 
800: // support functions for shell.c
801: // most are dummies, we don't need them really
802: 
803: int sqlite3_db_status(sqlite3 *, int op, int *pCur, int *pHiwtr, int resetFlg) {
804: 	fprintf(stderr, "sqlite3_db_status: unsupported.\n");
805: 	return -1;
806: }
807: 
808: int sqlite3_changes(sqlite3 *db) {
809: 	return db->last_changes;
810: }
811: 
812: int sqlite3_total_changes(sqlite3 *db) {
813: 	return db->total_changes;
814: }
815: 
816: SQLITE_API sqlite3_int64 sqlite3_last_insert_rowid(sqlite3 *db) {
817: 	return SQLITE_ERROR;
818: }
819: 
820: // some code borrowed from sqlite
821: // its probably best to match its behavior
822: 
823: typedef uint8_t u8;
824: 
825: /*
826: ** Token types used by the sqlite3_complete() routine.  See the header
827: ** comments on that procedure for additional information.
828: */
829: #define tkSEMI  0
830: #define tkWS    1
831: #define tkOTHER 2
832: 
833: const unsigned char sqlite3CtypeMap[256] = {
834:     0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 00..07    ........ */
835:     0x00, 0x01, 0x01, 0x01, 0x01, 0x01, 0x00, 0x00, /* 08..0f    ........ */
836:     0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 10..17    ........ */
837:     0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 18..1f    ........ */
838:     0x01, 0x00, 0x80, 0x00, 0x40, 0x00, 0x00, 0x80, /* 20..27     !"#$%&' */
839:     0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 28..2f    ()*+,-./ */
840:     0x0c, 0x0c, 0x0c, 0x0c, 0x0c, 0x0c, 0x0c, 0x0c, /* 30..37    01234567 */
841:     0x0c, 0x0c, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 38..3f    89:;<=>? */
842: 
843:     0x00, 0x0a, 0x0a, 0x0a, 0x0a, 0x0a, 0x0a, 0x02, /* 40..47    @ABCDEFG */
844:     0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, /* 48..4f    HIJKLMNO */
845:     0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, /* 50..57    PQRSTUVW */
846:     0x02, 0x02, 0x02, 0x80, 0x00, 0x00, 0x00, 0x40, /* 58..5f    XYZ[\]^_ */
847:     0x80, 0x2a, 0x2a, 0x2a, 0x2a, 0x2a, 0x2a, 0x22, /* 60..67    `abcdefg */
848:     0x22, 0x22, 0x22, 0x22, 0x22, 0x22, 0x22, 0x22, /* 68..6f    hijklmno */
849:     0x22, 0x22, 0x22, 0x22, 0x22, 0x22, 0x22, 0x22, /* 70..77    pqrstuvw */
850:     0x22, 0x22, 0x22, 0x00, 0x00, 0x00, 0x00, 0x00, /* 78..7f    xyz{|}~. */
851: 
852:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* 80..87    ........ */
853:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* 88..8f    ........ */
854:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* 90..97    ........ */
855:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* 98..9f    ........ */
856:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* a0..a7    ........ */
857:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* a8..af    ........ */
858:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* b0..b7    ........ */
859:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* b8..bf    ........ */
860: 
861:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* c0..c7    ........ */
862:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* c8..cf    ........ */
863:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* d0..d7    ........ */
864:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* d8..df    ........ */
865:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* e0..e7    ........ */
866:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* e8..ef    ........ */
867:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* f0..f7    ........ */
868:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40  /* f8..ff    ........ */
869: };
870: 
871: // TODO this can probably be simplified
872: #define IdChar(C) ((sqlite3CtypeMap[(unsigned char)C] & 0x46) != 0)
873: 
874: int sqlite3_complete(const char *zSql) {
875: 	u8 state = 0; /* Current state, using numbers defined in header comment */
876: 	u8 token;     /* Value of the next token */
877: 
878: 	/* If triggers are not supported by this compile then the statement machine
879: 	 ** used to detect the end of a statement is much simpler
880: 	 */
881: 	static const u8 trans[3][3] = {
882: 	    /* Token:           */
883: 	    /* State:       **  SEMI  WS  OTHER */
884: 	    /* 0 INVALID: */ {
885: 	        1,
886: 	        0,
887: 	        2,
888: 	    },
889: 	    /* 1   START: */
890: 	    {
891: 	        1,
892: 	        1,
893: 	        2,
894: 	    },
895: 	    /* 2  NORMAL: */
896: 	    {
897: 	        1,
898: 	        2,
899: 	        2,
900: 	    },
901: 	};
902: 
903: 	while (*zSql) {
904: 		switch (*zSql) {
905: 		case ';': { /* A semicolon */
906: 			token = tkSEMI;
907: 			break;
908: 		}
909: 		case ' ':
910: 		case '\r':
911: 		case '\t':
912: 		case '\n':
913: 		case '\f': { /* White space is ignored */
914: 			token = tkWS;
915: 			break;
916: 		}
917: 		case '/': { /* C-style comments */
918: 			if (zSql[1] != '*') {
919: 				token = tkOTHER;
920: 				break;
921: 			}
922: 			zSql += 2;
923: 			while (zSql[0] && (zSql[0] != '*' || zSql[1] != '/')) {
924: 				zSql++;
925: 			}
926: 			if (zSql[0] == 0)
927: 				return 0;
928: 			zSql++;
929: 			token = tkWS;
930: 			break;
931: 		}
932: 		case '-': { /* SQL-style comments from "--" to end of line */
933: 			if (zSql[1] != '-') {
934: 				token = tkOTHER;
935: 				break;
936: 			}
937: 			while (*zSql && *zSql != '\n') {
938: 				zSql++;
939: 			}
940: 			if (*zSql == 0)
941: 				return state == 1;
942: 			token = tkWS;
943: 			break;
944: 		}
945: 		case '[': { /* Microsoft-style identifiers in [...] */
946: 			zSql++;
947: 			while (*zSql && *zSql != ']') {
948: 				zSql++;
949: 			}
950: 			if (*zSql == 0)
951: 				return 0;
952: 			token = tkOTHER;
953: 			break;
954: 		}
955: 		case '`': /* Grave-accent quoted symbols used by MySQL */
956: 		case '"': /* single- and double-quoted strings */
957: 		case '\'': {
958: 			int c = *zSql;
959: 			zSql++;
960: 			while (*zSql && *zSql != c) {
961: 				zSql++;
962: 			}
963: 			if (*zSql == 0)
964: 				return 0;
965: 			token = tkOTHER;
966: 			break;
967: 		}
968: 		default: {
969: 
970: 			if (IdChar((u8)*zSql)) {
971: 				/* Keywords and unquoted identifiers */
972: 				int nId;
973: 				for (nId = 1; IdChar(zSql[nId]); nId++) {
974: 				}
975: 				token = tkOTHER;
976: 
977: 				zSql += nId - 1;
978: 			} else {
979: 				/* Operators and special symbols */
980: 				token = tkOTHER;
981: 			}
982: 			break;
983: 		}
984: 		}
985: 		state = trans[state][token];
986: 		zSql++;
987: 	}
988: 	return state == 1;
989: }
990: 
991: // checks if input ends with ;
992: int sqlite3_complete_old(const char *sql) {
993: 	fprintf(stderr, "sqlite3_complete: unsupported. '%s'\n", sql);
994: 	return -1;
995: }
996: 
997: // length of varchar or blob value
998: int sqlite3_column_bytes(sqlite3_stmt *pStmt, int iCol) {
999: 	// fprintf(stderr, "sqlite3_column_bytes: unsupported.\n");
1000: 	return pStmt->current_text[iCol].data_len;
1001: 	// return -1;
1002: }
1003: 
1004: sqlite3_value *sqlite3_column_value(sqlite3_stmt *, int iCol) {
1005: 	fprintf(stderr, "sqlite3_column_value: unsupported.\n");
1006: 	return nullptr;
1007: }
1008: 
1009: int sqlite3_db_config(sqlite3 *, int op, ...) {
1010: 	fprintf(stderr, "sqlite3_db_config: unsupported.\n");
1011: 	return -1;
1012: }
1013: 
1014: int sqlite3_get_autocommit(sqlite3 *db) {
1015: 	return db->con->context->transaction.IsAutoCommit();
1016: }
1017: 
1018: int sqlite3_limit(sqlite3 *, int id, int newVal) {
1019: 	fprintf(stderr, "sqlite3_limit: unsupported.\n");
1020: 	return -1;
1021: }
1022: 
1023: int sqlite3_stmt_readonly(sqlite3_stmt *pStmt) {
1024: 	fprintf(stderr, "sqlite3_stmt_readonly: unsupported.\n");
1025: 	return -1;
1026: }
1027: 
1028: // TODO pretty easy schema lookup
1029: int sqlite3_table_column_metadata(sqlite3 *db,             /* Connection handle */
1030:                                   const char *zDbName,     /* Database name or NULL */
1031:                                   const char *zTableName,  /* Table name */
1032:                                   const char *zColumnName, /* Column name */
1033:                                   char const **pzDataType, /* OUTPUT: Declared data type */
1034:                                   char const **pzCollSeq,  /* OUTPUT: Collation sequence name */
1035:                                   int *pNotNull,           /* OUTPUT: True if NOT NULL constraint exists */
1036:                                   int *pPrimaryKey,        /* OUTPUT: True if column part of PK */
1037:                                   int *pAutoinc            /* OUTPUT: True if column is auto-increment */
1038: ) {
1039: 	fprintf(stderr, "sqlite3_table_column_metadata: unsupported.\n");
1040: 	return -1;
1041: }
1042: 
1043: const char *sqlite3_column_decltype(sqlite3_stmt *pStmt, int iCol) {
1044: 	if (!pStmt || !pStmt->prepared) {
1045: 		return NULL;
1046: 	}
1047: 	auto column_type = pStmt->prepared->GetTypes()[iCol];
1048: 	switch (column_type.id()) {
1049: 	case LogicalTypeId::BOOLEAN:
1050: 		return "BOOLEAN";
1051: 	case LogicalTypeId::TINYINT:
1052: 		return "TINYINT";
1053: 	case LogicalTypeId::SMALLINT:
1054: 		return "SMALLINT";
1055: 	case LogicalTypeId::INTEGER:
1056: 		return "INTEGER";
1057: 	case LogicalTypeId::BIGINT:
1058: 		return "BIGINT";
1059: 	case LogicalTypeId::FLOAT:
1060: 		return "FLOAT";
1061: 	case LogicalTypeId::DOUBLE:
1062: 		return "DOUBLE";
1063: 	case LogicalTypeId::DECIMAL:
1064: 		return "DECIMAL";
1065: 	case LogicalTypeId::DATE:
1066: 		return "DATE";
1067: 	case LogicalTypeId::TIME:
1068: 		return "TIME";
1069: 	case LogicalTypeId::TIMESTAMP:
1070: 	case LogicalTypeId::TIMESTAMP_NS:
1071: 	case LogicalTypeId::TIMESTAMP_MS:
1072: 	case LogicalTypeId::TIMESTAMP_SEC:
1073: 		return "TIMESTAMP";
1074: 	case LogicalTypeId::VARCHAR:
1075: 		return "VARCHAR";
1076: 	case LogicalTypeId::LIST:
1077: 		return "LIST";
1078: 	case LogicalTypeId::MAP:
1079: 		return "MAP";
1080: 	case LogicalTypeId::STRUCT:
1081: 		return "STRUCT";
1082: 	case LogicalTypeId::BLOB:
1083: 		return "BLOB";
1084: 	default:
1085: 		return NULL;
1086: 	}
1087: 	return NULL;
1088: }
1089: 
1090: int sqlite3_status64(int op, sqlite3_int64 *pCurrent, sqlite3_int64 *pHighwater, int resetFlag) {
1091: 	fprintf(stderr, "sqlite3_status64: unsupported.\n");
1092: 	return -1;
1093: }
1094: 
1095: int sqlite3_status64(sqlite3 *, int op, int *pCur, int *pHiwtr, int resetFlg) {
1096: 	fprintf(stderr, "sqlite3_status64: unsupported.\n");
1097: 	return -1;
1098: }
1099: 
1100: int sqlite3_stmt_status(sqlite3_stmt *, int op, int resetFlg) {
1101: 	fprintf(stderr, "sqlite3_stmt_status: unsupported.\n");
1102: 	return -1;
1103: }
1104: 
1105: int sqlite3_file_control(sqlite3 *, const char *zDbName, int op, void *) {
1106: 	fprintf(stderr, "sqlite3_file_control: unsupported.\n");
1107: 	return -1;
1108: }
1109: 
1110: int sqlite3_declare_vtab(sqlite3 *, const char *zSQL) {
1111: 	fprintf(stderr, "sqlite3_declare_vtab: unsupported.\n");
1112: 	return -1;
1113: }
1114: 
1115: const char *sqlite3_vtab_collation(sqlite3_index_info *, int) {
1116: 	fprintf(stderr, "sqlite3_vtab_collation: unsupported.\n");
1117: 	return nullptr;
1118: }
1119: 
1120: int sqlite3_sleep(int) {
1121: 	fprintf(stderr, "sqlite3_sleep: unsupported.\n");
1122: 	return -1;
1123: }
1124: 
1125: int sqlite3_busy_timeout(sqlite3 *, int ms) {
1126: 	fprintf(stderr, "sqlite3_busy_timeout: unsupported.\n");
1127: 	return -1;
1128: }
1129: 
1130: // unlikely to be supported
1131: 
1132: int sqlite3_trace_v2(sqlite3 *, unsigned uMask, int (*xCallback)(unsigned, void *, void *, void *), void *pCtx) {
1133: 	fprintf(stderr, "sqlite3_trace_v2: unsupported.\n");
1134: 	return -1;
1135: }
1136: 
1137: int sqlite3_test_control(int op, ...) {
1138: 	fprintf(stderr, "sqlite3_test_control: unsupported.\n");
1139: 	return -1;
1140: }
1141: 
1142: int sqlite3_enable_load_extension(sqlite3 *db, int onoff) {
1143: 	// fprintf(stderr, "sqlite3_enable_load_extension: unsupported.\n");
1144: 	return -1;
1145: }
1146: 
1147: int sqlite3_load_extension(sqlite3 *db,       /* Load the extension into this database connection */
1148:                            const char *zFile, /* Name of the shared library containing extension */
1149:                            const char *zProc, /* Entry point.  Derived from zFile if 0 */
1150:                            char **pzErrMsg    /* Put error message here if not 0 */
1151: ) {
1152: 	// fprintf(stderr, "sqlite3_load_extension: unsupported.\n");
1153: 	return -1;
1154: }
1155: 
1156: int sqlite3_create_module(sqlite3 *db,             /* SQLite connection to register module with */
1157:                           const char *zName,       /* Name of the module */
1158:                           const sqlite3_module *p, /* Methods for the module */
1159:                           void *pClientData        /* Client data for xCreate/xConnect */
1160: ) {
1161: 	// fprintf(stderr, "sqlite3_create_module: unsupported.\n");
1162: 	return -1;
1163: }
1164: 
1165: int sqlite3_create_function(sqlite3 *db, const char *zFunctionName, int nArg, int eTextRep, void *pApp,
1166:                             void (*xFunc)(sqlite3_context *, int, sqlite3_value **),
1167:                             void (*xStep)(sqlite3_context *, int, sqlite3_value **),
1168:                             void (*xFinal)(sqlite3_context *)) {
1169: 	if ((!xFunc && !xStep && !xFinal) || !zFunctionName || nArg < -1) {
1170: 		return SQLITE_MISUSE;
1171: 	}
1172: 	string fname = string(zFunctionName);
1173: 
1174: 	// Scalar function
1175: 	if (!xFunc) {
1176: 		return SQLITE_MISUSE;
1177: 	}
1178: 	auto udf_sqlite3 = SQLiteUDFWrapper::CreateSQLiteScalarFunction(xFunc, db, pApp);
1179: 	LogicalType varargs = LogicalType::INVALID;
1180: 	if (nArg == -1) {
1181: 		varargs = LogicalType::ANY;
1182: 		nArg = 0;
1183: 	}
1184: 
1185: 	vector<LogicalType> argv_types(nArg);
1186: 	for (idx_t i = 0; i < (idx_t)nArg; ++i) {
1187: 		argv_types[i] = LogicalType::ANY;
1188: 	}
1189: 
1190: 	UDFWrapper::RegisterFunction(fname, argv_types, LogicalType::VARCHAR, udf_sqlite3, *(db->con->context), varargs);
1191: 	return SQLITE_OK;
1192: }
1193: 
1194: int sqlite3_create_function_v2(sqlite3 *db, const char *zFunctionName, int nArg, int eTextRep, void *pApp,
1195:                                void (*xFunc)(sqlite3_context *, int, sqlite3_value **),
1196:                                void (*xStep)(sqlite3_context *, int, sqlite3_value **),
1197:                                void (*xFinal)(sqlite3_context *), void (*xDestroy)(void *)) {
1198: 	return -1;
1199: }
1200: 
1201: int sqlite3_set_authorizer(sqlite3 *, int (*xAuth)(void *, int, const char *, const char *, const char *, const char *),
1202:                            void *pUserData) {
1203: 	fprintf(stderr, "sqlite3_set_authorizer: unsupported.\n");
1204: 	return -1;
1205: }
1206: 
1207: // needed in shell timer
1208: static int unixCurrentTimeInt64(sqlite3_vfs *NotUsed, sqlite3_int64 *piNow) {
1209: 	using namespace std::chrono;
1210: 	*piNow = (sqlite3_int64)duration_cast<milliseconds>(system_clock::now().time_since_epoch()).count();
1211: 	return SQLITE_OK;
1212: }
1213: 
1214: static sqlite3_vfs static_sqlite3_virtual_file_systems[] = {{
1215:     3,                    // int iVersion;            /* Structure version number (currently 3) */
1216:     0,                    // int szOsFile;            /* Size of subclassed sqlite3_file */
1217:     0,                    // int mxPathname;          /* Maximum file pathname length */
1218:     nullptr,              // sqlite3_vfs *pNext;      /* Next registered VFS */
1219:     "dummy",              // const char *zName;       /* Name of this virtual file system */
1220:     nullptr,              // void *pAppData;          /* Pointer to application-specific data */
1221:     nullptr,              // int (*xOpen)(sqlite3_vfs*, const char *zName, sqlite3_file*, int flags, int *pOutFlags);
1222:     nullptr,              // int (*xDelete)(sqlite3_vfs*, const char *zName, int syncDir);
1223:     nullptr,              // int (*xAccess)(sqlite3_vfs*, const char *zName, int flags, int *pResOut);
1224:     nullptr,              // int (*xFullPathname)(sqlite3_vfs*, const char *zName, int nOut, char *zOut);
1225:     nullptr,              // void *(*xDlOpen)(sqlite3_vfs*, const char *zFilename);
1226:     nullptr,              // void (*xDlError)(sqlite3_vfs*, int nByte, char *zErrMsg);
1227:     nullptr,              // void (*(*xDlSym)(sqlite3_vfs*,void*, const char *zSymbol))(void);
1228:     nullptr,              // void (*xDlClose)(sqlite3_vfs*, void*);
1229:     nullptr,              // int (*xRandomness)(sqlite3_vfs*, int nByte, char *zOut);
1230:     nullptr,              // int (*xSleep)(sqlite3_vfs*, int microseconds);
1231:     nullptr,              // int (*xCurrentTime)(sqlite3_vfs*, double*);
1232:     nullptr,              // int (*xGetLastError)(sqlite3_vfs*, int, char *);
1233:     unixCurrentTimeInt64, // int (*xCurrentTimeInt64)(sqlite3_vfs*, sqlite3_int64*);
1234:     nullptr,              // int (*xSetSystemCall)(sqlite3_vfs*, const char *zName, sqlite3_syscall_ptr);
1235:     nullptr,              // sqlite3_syscall_ptr (*xGetSystemCall)(sqlite3_vfs*, const char *zName);
1236:     nullptr               // const char *(*xNextSystemCall)(sqlite3_vfs*, const char *zName);
1237: }};
1238: 
1239: // virtual file system, providing some dummies to avoid crashes
1240: sqlite3_vfs *sqlite3_vfs_find(const char *zVfsName) {
1241: 	// return a dummy because the shell does not check the return code.
1242: 	return static_sqlite3_virtual_file_systems;
1243: }
1244: 
1245: int sqlite3_vfs_register(sqlite3_vfs *, int makeDflt) {
1246: 	// fprintf(stderr, "sqlite3_vfs_register: unsupported.\n");
1247: 	return -1;
1248: }
1249: 
1250: // backups, unused
1251: 
1252: int sqlite3_backup_step(sqlite3_backup *p, int nPage) {
1253: 	fprintf(stderr, "sqlite3_backup_step: unsupported.\n");
1254: 	return -1;
1255: }
1256: 
1257: int sqlite3_backup_finish(sqlite3_backup *p) {
1258: 	fprintf(stderr, "sqlite3_backup_finish: unsupported.\n");
1259: 	return -1;
1260: }
1261: 
1262: sqlite3_backup *sqlite3_backup_init(sqlite3 *pDest,         /* Destination database handle */
1263:                                     const char *zDestName,  /* Destination database name */
1264:                                     sqlite3 *pSource,       /* Source database handle */
1265:                                     const char *zSourceName /* Source database name */
1266: ) {
1267: 	fprintf(stderr, "sqlite3_backup_init: unsupported.\n");
1268: 	return nullptr;
1269: }
1270: 
1271: // UDF support stuff, unused for now. These cannot be called as create_function above is disabled
1272: 
1273: SQLITE_API sqlite3 *sqlite3_context_db_handle(sqlite3_context *) {
1274: 	return nullptr;
1275: }
1276: 
1277: void *sqlite3_user_data(sqlite3_context *context) {
1278: 	assert(context);
1279: 	return context->pFunc.pUserData;
1280: }
1281: 
1282: #ifdef _WIN32
1283: #include <windows.h>
1284: 
1285: static void *sqlite3MallocZero(size_t n) {
1286: 	auto res = sqlite3_malloc(n);
1287: 	assert(res);
1288: 	memset(res, 0, n);
1289: 	return res;
1290: }
1291: 
1292: static LPWSTR winUtf8ToUnicode(const char *zText) {
1293: 	int nChar;
1294: 	LPWSTR zWideText;
1295: 
1296: 	nChar = MultiByteToWideChar(CP_UTF8, 0, zText, -1, NULL, 0);
1297: 	if (nChar == 0) {
1298: 		return 0;
1299: 	}
1300: 	zWideText = (LPWSTR)sqlite3MallocZero(nChar * sizeof(WCHAR));
1301: 	if (zWideText == 0) {
1302: 		return 0;
1303: 	}
1304: 	nChar = MultiByteToWideChar(CP_UTF8, 0, zText, -1, zWideText, nChar);
1305: 	if (nChar == 0) {
1306: 		sqlite3_free(zWideText);
1307: 		zWideText = 0;
1308: 	}
1309: 	return zWideText;
1310: }
1311: 
1312: static char *winUnicodeToMbcs(LPCWSTR zWideText, int useAnsi) {
1313: 	int nByte;
1314: 	char *zText;
1315: 	int codepage = useAnsi ? CP_ACP : CP_OEMCP;
1316: 
1317: 	nByte = WideCharToMultiByte(codepage, 0, zWideText, -1, 0, 0, 0, 0);
1318: 	if (nByte == 0) {
1319: 		return 0;
1320: 	}
1321: 	zText = (char *)sqlite3MallocZero(nByte);
1322: 	if (zText == 0) {
1323: 		return 0;
1324: 	}
1325: 	nByte = WideCharToMultiByte(codepage, 0, zWideText, -1, zText, nByte, 0, 0);
1326: 	if (nByte == 0) {
1327: 		sqlite3_free(zText);
1328: 		zText = 0;
1329: 	}
1330: 	return zText;
1331: }
1332: 
1333: static char *winUtf8ToMbcs(const char *zText, int useAnsi) {
1334: 	char *zTextMbcs;
1335: 	LPWSTR zTmpWide;
1336: 
1337: 	zTmpWide = winUtf8ToUnicode(zText);
1338: 	if (zTmpWide == 0) {
1339: 		return 0;
1340: 	}
1341: 	zTextMbcs = winUnicodeToMbcs(zTmpWide, useAnsi);
1342: 	sqlite3_free(zTmpWide);
1343: 	return zTextMbcs;
1344: }
1345: 
1346: SQLITE_API char *sqlite3_win32_utf8_to_mbcs_v2(const char *zText, int useAnsi) {
1347: 	return winUtf8ToMbcs(zText, useAnsi);
1348: }
1349: 
1350: LPWSTR sqlite3_win32_utf8_to_unicode(const char *zText) {
1351: 	return winUtf8ToUnicode(zText);
1352: }
1353: 
1354: static LPWSTR winMbcsToUnicode(const char *zText, int useAnsi) {
1355: 	int nByte;
1356: 	LPWSTR zMbcsText;
1357: 	int codepage = useAnsi ? CP_ACP : CP_OEMCP;
1358: 
1359: 	nByte = MultiByteToWideChar(codepage, 0, zText, -1, NULL, 0) * sizeof(WCHAR);
1360: 	if (nByte == 0) {
1361: 		return 0;
1362: 	}
1363: 	zMbcsText = (LPWSTR)sqlite3MallocZero(nByte * sizeof(WCHAR));
1364: 	if (zMbcsText == 0) {
1365: 		return 0;
1366: 	}
1367: 	nByte = MultiByteToWideChar(codepage, 0, zText, -1, zMbcsText, nByte);
1368: 	if (nByte == 0) {
1369: 		sqlite3_free(zMbcsText);
1370: 		zMbcsText = 0;
1371: 	}
1372: 	return zMbcsText;
1373: }
1374: 
1375: static char *winUnicodeToUtf8(LPCWSTR zWideText) {
1376: 	int nByte;
1377: 	char *zText;
1378: 
1379: 	nByte = WideCharToMultiByte(CP_UTF8, 0, zWideText, -1, 0, 0, 0, 0);
1380: 	if (nByte == 0) {
1381: 		return 0;
1382: 	}
1383: 	zText = (char *)sqlite3MallocZero(nByte);
1384: 	if (zText == 0) {
1385: 		return 0;
1386: 	}
1387: 	nByte = WideCharToMultiByte(CP_UTF8, 0, zWideText, -1, zText, nByte, 0, 0);
1388: 	if (nByte == 0) {
1389: 		sqlite3_free(zText);
1390: 		zText = 0;
1391: 	}
1392: 	return zText;
1393: }
1394: 
1395: static char *winMbcsToUtf8(const char *zText, int useAnsi) {
1396: 	char *zTextUtf8;
1397: 	LPWSTR zTmpWide;
1398: 
1399: 	zTmpWide = winMbcsToUnicode(zText, useAnsi);
1400: 	if (zTmpWide == 0) {
1401: 		return 0;
1402: 	}
1403: 	zTextUtf8 = winUnicodeToUtf8(zTmpWide);
1404: 	sqlite3_free(zTmpWide);
1405: 	return zTextUtf8;
1406: }
1407: 
1408: SQLITE_API char *sqlite3_win32_mbcs_to_utf8_v2(const char *zText, int useAnsi) {
1409: 	return winMbcsToUtf8(zText, useAnsi);
1410: }
1411: 
1412: SQLITE_API char *sqlite3_win32_unicode_to_utf8(LPCWSTR zWideText) {
1413: 	return winUnicodeToUtf8(zWideText);
1414: }
1415: 
1416: #endif
1417: 
1418: // TODO complain
1419: SQLITE_API void sqlite3_result_blob(sqlite3_context *context, const void *blob, int n_bytes, void (*)(void *)) {
1420: 	if (!blob) {
1421: 		context->isError = SQLITE_MISUSE;
1422: 		return;
1423: 	}
1424: 	context->result.type = SQLiteTypeValue::BLOB;
1425: 	context->result.n = n_bytes;
1426: 	string_t str = string_t((const char *)blob, n_bytes);
1427: 	context->result.str_t = str;
1428: }
1429: 
1430: SQLITE_API void sqlite3_result_blob64(sqlite3_context *, const void *, sqlite3_uint64, void (*)(void *)) {
1431: }
1432: 
1433: SQLITE_API void sqlite3_result_double(sqlite3_context *context, double val) {
1434: 	context->result.u.r = val;
1435: 	context->result.type = SQLiteTypeValue::FLOAT;
1436: }
1437: 
1438: SQLITE_API void sqlite3_result_error(sqlite3_context *context, const char *msg, int n_bytes) {
1439: 	context->isError = SQLITE_ERROR;
1440: 	sqlite3_result_text(context, msg, n_bytes, nullptr);
1441: }
1442: 
1443: SQLITE_API void sqlite3_result_error16(sqlite3_context *, const void *, int) {
1444: }
1445: 
1446: SQLITE_API void sqlite3_result_error_toobig(sqlite3_context *) {
1447: }
1448: 
1449: SQLITE_API void sqlite3_result_error_nomem(sqlite3_context *) {
1450: }
1451: 
1452: SQLITE_API void sqlite3_result_error_code(sqlite3_context *, int) {
1453: }
1454: 
1455: SQLITE_API void sqlite3_result_int(sqlite3_context *context, int val) {
1456: 	sqlite3_result_int64(context, val);
1457: }
1458: 
1459: SQLITE_API void sqlite3_result_int64(sqlite3_context *context, sqlite3_int64 val) {
1460: 	context->result.u.i = val;
1461: 	context->result.type = SQLiteTypeValue::INTEGER;
1462: }
1463: 
1464: SQLITE_API void sqlite3_result_null(sqlite3_context *context) {
1465: 	context->result.type = SQLiteTypeValue::NULL_VALUE;
1466: }
1467: 
1468: SQLITE_API void sqlite3_result_text(sqlite3_context *context, const char *str_c, int n_chars, void (*)(void *)) {
1469: 	if (!str_c) {
1470: 		context->isError = SQLITE_MISUSE;
1471: 		return;
1472: 	}
1473: 
1474: 	auto utf_type = Utf8Proc::Analyze(str_c, n_chars);
1475: 	if (utf_type == UnicodeType::INVALID) {
1476: 		context->isError = SQLITE_MISUSE;
1477: 		return;
1478: 	}
1479: 	context->result.type = SQLiteTypeValue::TEXT;
1480: 	context->result.n = n_chars;
1481: 	context->result.str_t = string_t(str_c, n_chars);
1482: }
1483: 
1484: SQLITE_API void sqlite3_result_text64(sqlite3_context *, const char *, sqlite3_uint64, void (*)(void *),
1485:                                       unsigned char encoding) {
1486: }
1487: 
1488: SQLITE_API void sqlite3_result_text16(sqlite3_context *, const void *, int, void (*)(void *)) {
1489: }
1490: 
1491: SQLITE_API void sqlite3_result_text16le(sqlite3_context *, const void *, int, void (*)(void *)) {
1492: }
1493: 
1494: SQLITE_API void sqlite3_result_text16be(sqlite3_context *, const void *, int, void (*)(void *)) {
1495: }
1496: 
1497: SQLITE_API void sqlite3_result_value(sqlite3_context *, sqlite3_value *) {
1498: }
1499: 
1500: SQLITE_API void sqlite3_result_pointer(sqlite3_context *, void *, const char *, void (*)(void *)) {
1501: }
1502: 
1503: SQLITE_API void sqlite3_result_zeroblob(sqlite3_context *, int n) {
1504: }
1505: 
1506: SQLITE_API int sqlite3_result_zeroblob64(sqlite3_context *, sqlite3_uint64 n) {
1507: 	return -1;
1508: }
1509: 
1510: // TODO complain
1511: const void *sqlite3_value_blob(sqlite3_value *pVal) {
1512: 	return sqlite3_value_text(pVal);
1513: }
1514: 
1515: double sqlite3_value_double(sqlite3_value *pVal) {
1516: 	if (!pVal) {
1517: 		pVal->db->errCode = SQLITE_MISUSE;
1518: 		return 0.0;
1519: 	}
1520: 	switch (pVal->type) {
1521: 	case SQLiteTypeValue::FLOAT:
1522: 		return pVal->u.r;
1523: 	case SQLiteTypeValue::INTEGER:
1524: 		return (double)pVal->u.i;
1525: 	case SQLiteTypeValue::TEXT:
1526: 	case SQLiteTypeValue::BLOB:
1527: 		double res;
1528: 		if (TryCast::Operation<string_t, double>(pVal->str_t, res)) {
1529: 			return res;
1530: 		}
1531: 		break;
1532: 	default:
1533: 		break;
1534: 	}
1535: 	pVal->db->errCode = SQLITE_MISMATCH;
1536: 	return 0.0;
1537: }
1538: 
1539: int sqlite3_value_int(sqlite3_value *pVal) {
1540: 	int64_t res = sqlite3_value_int64(pVal);
1541: 	if (res >= NumericLimits<int>::Minimum() && res <= NumericLimits<int>::Maximum()) {
1542: 		return res;
1543: 	}
1544: 	pVal->db->errCode = SQLITE_MISMATCH;
1545: 	return 0;
1546: }
1547: 
1548: sqlite3_int64 sqlite3_value_int64(sqlite3_value *pVal) {
1549: 	if (!pVal) {
1550: 		pVal->db->errCode = SQLITE_MISUSE;
1551: 		return 0;
1552: 	}
1553: 	int64_t res;
1554: 	switch (pVal->type) {
1555: 	case SQLiteTypeValue::INTEGER:
1556: 		return pVal->u.i;
1557: 	case SQLiteTypeValue::FLOAT:
1558: 		if (TryCast::Operation<double, int64_t>(pVal->u.r, res)) {
1559: 			return res;
1560: 		}
1561: 		break;
1562: 	case SQLiteTypeValue::TEXT:
1563: 	case SQLiteTypeValue::BLOB:
1564: 		if (TryCast::Operation<string_t, int64_t>(pVal->str_t, res)) {
1565: 			return res;
1566: 		}
1567: 		break;
1568: 	default:
1569: 		break;
1570: 	}
1571: 	pVal->db->errCode = SQLITE_MISMATCH;
1572: 	return 0;
1573: }
1574: 
1575: void *sqlite3_value_pointer(sqlite3_value *, const char *) {
1576: 	return nullptr;
1577: }
1578: 
1579: const unsigned char *sqlite3_value_text(sqlite3_value *pVal) {
1580: 	if (!pVal) {
1581: 		pVal->db->errCode = SQLITE_MISUSE;
1582: 		return nullptr;
1583: 	}
1584: 	// check if the string has already been allocated
1585: 	if (pVal->szMalloc > 0) {
1586: 		return (const unsigned char *)pVal->zMalloc;
1587: 	}
1588: 
1589: 	if (pVal->type == SQLiteTypeValue::TEXT || pVal->type == SQLiteTypeValue::BLOB) {
1590: 		auto length = pVal->str_t.GetSize();
1591: 		// new string including space for the null-terminated char ('\0')
1592: 		pVal->zMalloc = (char *)malloc(sizeof(char) * length + 1);
1593: 		if (!pVal->zMalloc) {
1594: 			pVal->db->errCode = SQLITE_NOMEM;
1595: 			return nullptr;
1596: 		}
1597: 		pVal->szMalloc = length + 1;
1598: 		memcpy(pVal->zMalloc, pVal->str_t.GetDataUnsafe(), length);
1599: 		pVal->zMalloc[length] = '\0';
1600: 		return (const unsigned char *)pVal->zMalloc;
1601: 	}
1602: 
1603: 	if (pVal->type == SQLiteTypeValue::INTEGER || pVal->type == SQLiteTypeValue::FLOAT) {
1604: 		Value value = (pVal->type == SQLiteTypeValue::INTEGER) ? Value::BIGINT(pVal->u.i) : Value::DOUBLE(pVal->u.r);
1605: 		if (value.TryCastAs(LogicalType::VARCHAR) == false) {
1606: 			pVal->db->errCode = SQLITE_NOMEM;
1607: 			return nullptr;
1608: 		}
1609: 		auto &str_val = StringValue::Get(value);
1610: 		size_t str_len = str_val.size();
1611: 		pVal->zMalloc = (char *)malloc(sizeof(char) * (str_len + 1));
1612: 		if (!pVal->zMalloc) {
1613: 			pVal->db->errCode = SQLITE_NOMEM;
1614: 			return nullptr;
1615: 		}
1616: 		pVal->szMalloc = str_len + 1; // +1 null-terminated char
1617: 		memcpy(pVal->zMalloc, str_val.c_str(), pVal->szMalloc);
1618: 
1619: 		pVal->str_t = string_t(pVal->zMalloc, pVal->szMalloc - 1); // -1 null-terminated char
1620: 		pVal->n = pVal->str_t.GetSize();
1621: 		pVal->type = SQLiteTypeValue::TEXT;
1622: 		return (const unsigned char *)pVal->zMalloc;
1623: 	}
1624: 	if (pVal->type == SQLiteTypeValue::NULL_VALUE) {
1625: 		return nullptr;
1626: 	}
1627: 	pVal->db->errCode = SQLITE_MISMATCH;
1628: 	return nullptr;
1629: }
1630: 
1631: SQLITE_API const void *sqlite3_value_text16(sqlite3_value *) {
1632: 	return nullptr;
1633: }
1634: 
1635: SQLITE_API const void *sqlite3_value_text16le(sqlite3_value *) {
1636: 	return nullptr;
1637: }
1638: 
1639: SQLITE_API const void *sqlite3_value_text16be(sqlite3_value *) {
1640: 	return nullptr;
1641: }
1642: 
1643: SQLITE_API int sqlite3_value_bytes(sqlite3_value *pVal) {
1644: 	if (pVal->type == SQLiteTypeValue::TEXT || pVal->type == SQLiteTypeValue::BLOB) {
1645: 		return pVal->n;
1646: 	}
1647: 	return 0;
1648: }
1649: 
1650: SQLITE_API int sqlite3_value_bytes16(sqlite3_value *) {
1651: 	return 0;
1652: }
1653: 
1654: SQLITE_API int sqlite3_value_type(sqlite3_value *pVal) {
1655: 	return (int)pVal->type;
1656: }
1657: 
1658: SQLITE_API int sqlite3_value_numeric_type(sqlite3_value *) {
1659: 	return 0;
1660: }
1661: 
1662: SQLITE_API int sqlite3_value_nochange(sqlite3_value *) {
1663: 	return 0;
1664: }
1665: 
1666: SQLITE_API void *sqlite3_aggregate_context(sqlite3_context *, int nBytes) {
1667: 	fprintf(stderr, "sqlite3_aggregate_context: unsupported.\n");
1668: 
1669: 	return nullptr;
1670: }
1671: 
1672: SQLITE_API int sqlite3_create_collation(sqlite3 *, const char *zName, int eTextRep, void *pArg,
1673:                                         int (*xCompare)(void *, int, const void *, int, const void *)) {
1674: 	return SQLITE_ERROR;
1675: }
1676: 
1677: SQLITE_API int sqlite3_create_window_function(sqlite3 *db, const char *zFunctionName, int nArg, int eTextRep,
1678:                                               void *pApp, void (*xStep)(sqlite3_context *, int, sqlite3_value **),
1679:                                               void (*xFinal)(sqlite3_context *), void (*xValue)(sqlite3_context *),
1680:                                               void (*xInverse)(sqlite3_context *, int, sqlite3_value **),
1681:                                               void (*xDestroy)(void *)) {
1682: 	// commented for now because such error message prevents the shell-test.py to pass
1683: 	//	fprintf(stderr, "sqlite3_create_window_function: unsupported.\n");
1684: 	return SQLITE_ERROR;
1685: }
1686: 
1687: SQLITE_API sqlite3 *sqlite3_db_handle(sqlite3_stmt *s) {
1688: 	return s->db;
1689: }
1690: 
1691: SQLITE_API char *sqlite3_expanded_sql(sqlite3_stmt *pStmt) {
1692: 	fprintf(stderr, "sqlite3_expanded_sql: unsupported.\n");
1693: 	return nullptr;
1694: }
1695: 
1696: SQLITE_API int sqlite3_keyword_check(const char *str, int len) {
1697: 	return Parser::IsKeyword(std::string(str, len));
1698: }
1699: 
1700: SQLITE_API int sqlite3_keyword_count(void) {
1701: 	fprintf(stderr, "sqlite3_keyword_count: unsupported.\n");
1702: 	return 0;
1703: }
1704: 
1705: SQLITE_API int sqlite3_keyword_name(int, const char **, int *) {
1706: 	fprintf(stderr, "sqlite3_keyword_name: unsupported.\n");
1707: 	return 0;
1708: }
1709: 
1710: SQLITE_API void sqlite3_progress_handler(sqlite3 *, int, int (*)(void *), void *) {
1711: 	fprintf(stderr, "sqlite3_progress_handler: unsupported.\n");
1712: }
1713: 
1714: SQLITE_API int sqlite3_stmt_isexplain(sqlite3_stmt *pStmt) {
1715: 	if (!pStmt || !pStmt->prepared) {
1716: 		return 0;
1717: 	}
1718: 	return pStmt->prepared->GetStatementType() == StatementType::EXPLAIN_STATEMENT;
1719: }
1720: 
1721: SQLITE_API int sqlite3_vtab_config(sqlite3 *, int op, ...) {
1722: 	fprintf(stderr, "sqlite3_vtab_config: unsupported.\n");
1723: 	return SQLITE_ERROR;
1724: }
1725: 
1726: SQLITE_API int sqlite3_busy_handler(sqlite3 *, int (*)(void *, int), void *) {
1727: 	return SQLITE_ERROR;
1728: }
1729: 
1730: SQLITE_API int sqlite3_get_table(sqlite3 *db,       /* An open database */
1731:                                  const char *zSql,  /* SQL to be evaluated */
1732:                                  char ***pazResult, /* Results of the query */
1733:                                  int *pnRow,        /* Number of result rows written here */
1734:                                  int *pnColumn,     /* Number of result columns written here */
1735:                                  char **pzErrmsg    /* Error msg written here */
1736: ) {
1737: 	fprintf(stderr, "sqlite3_get_table: unsupported.\n");
1738: 	return SQLITE_ERROR;
1739: }
1740: 
1741: SQLITE_API void sqlite3_free_table(char **result) {
1742: 	fprintf(stderr, "sqlite3_free_table: unsupported.\n");
1743: }
1744: 
1745: SQLITE_API int sqlite3_prepare(sqlite3 *db,           /* Database handle */
1746:                                const char *zSql,      /* SQL statement, UTF-8 encoded */
1747:                                int nByte,             /* Maximum length of zSql in bytes. */
1748:                                sqlite3_stmt **ppStmt, /* OUT: Statement handle */
1749:                                const char **pzTail    /* OUT: Pointer to unused portion of zSql */
1750: ) {
1751: 	return sqlite3_prepare_v2(db, zSql, nByte, ppStmt, pzTail);
1752: }
1753: 
1754: SQLITE_API void *sqlite3_trace(sqlite3 *, void (*xTrace)(void *, const char *), void *) {
1755: 	fprintf(stderr, "sqlite3_trace: unsupported.\n");
1756: 	return nullptr;
1757: }
1758: 
1759: SQLITE_API void *sqlite3_profile(sqlite3 *, void (*xProfile)(void *, const char *, sqlite3_uint64), void *) {
1760: 	fprintf(stderr, "sqlite3_profile: unsupported.\n");
1761: 	return nullptr;
1762: }
1763: 
1764: SQLITE_API int sqlite3_libversion_number(void) {
1765: 	return SQLITE_VERSION_NUMBER;
1766: }
1767: 
1768: SQLITE_API int sqlite3_threadsafe(void) {
1769: 	return SQLITE_OK;
1770: }
1771: 
1772: SQLITE_API sqlite3_mutex *sqlite3_mutex_alloc(int) {
1773: 	fprintf(stderr, "sqlite3_mutex_alloc: unsupported.\n");
1774: 	return nullptr;
1775: }
1776: 
1777: SQLITE_API void sqlite3_mutex_free(sqlite3_mutex *) {
1778: 	fprintf(stderr, "sqlite3_mutex_free: unsupported.\n");
1779: }
1780: 
1781: SQLITE_API int sqlite3_extended_result_codes(sqlite3 *db, int onoff) {
1782: 	fprintf(stderr, "sqlite3_extended_result_codes: unsupported.\n");
1783: 	return SQLITE_ERROR;
1784: }
1785: 
1786: SQLITE_API void *sqlite3_update_hook(sqlite3 *db, /* Attach the hook to this database */
1787:                                      void (*xCallback)(void *, int, char const *, char const *, sqlite_int64),
1788:                                      void *pArg /* Argument to the function */
1789: ) {
1790: 	fprintf(stderr, "sqlite3_update_hook: unsupported.\n");
1791: 	return nullptr;
1792: }
1793: 
1794: SQLITE_API void sqlite3_log(int iErrCode, const char *zFormat, ...) {
1795: 	fprintf(stderr, "sqlite3_log: unsupported.\n");
1796: }
1797: 
1798: SQLITE_API int sqlite3_unlock_notify(sqlite3 *db, void (*xNotify)(void **, int), void *pArg) {
1799: 	fprintf(stderr, "sqlite3_unlock_notify: unsupported.\n");
1800: 	return SQLITE_ERROR;
1801: }
1802: 
1803: SQLITE_API void *sqlite3_get_auxdata(sqlite3_context *pCtx, int iArg) {
1804: 	fprintf(stderr, "sqlite3_get_auxdata: unsupported.\n");
1805: 	return nullptr;
1806: }
1807: 
1808: SQLITE_API void *sqlite3_rollback_hook(sqlite3 *db,               /* Attach the hook to this database */
1809:                                        void (*xCallback)(void *), /* Callback function */
1810:                                        void *pArg                 /* Argument to the function */
1811: ) {
1812: 	fprintf(stderr, "sqlite3_rollback_hook: unsupported.\n");
1813: 	return nullptr;
1814: }
1815: 
1816: SQLITE_API void *sqlite3_commit_hook(sqlite3 *db,              /* Attach the hook to this database */
1817:                                      int (*xCallback)(void *), /* Function to invoke on each commit */
1818:                                      void *pArg                /* Argument to the function */
1819: ) {
1820: 	fprintf(stderr, "sqlite3_commit_hook: unsupported.\n");
1821: 	return nullptr;
1822: }
1823: 
1824: SQLITE_API int sqlite3_blob_open(sqlite3 *db,          /* The database connection */
1825:                                  const char *zDb,      /* The attached database containing the blob */
1826:                                  const char *zTable,   /* The table containing the blob */
1827:                                  const char *zColumn,  /* The column containing the blob */
1828:                                  sqlite_int64 iRow,    /* The row containing the glob */
1829:                                  int wrFlag,           /* True -> read/write access, false -> read-only */
1830:                                  sqlite3_blob **ppBlob /* Handle for accessing the blob returned here */
1831: ) {
1832: 	fprintf(stderr, "sqlite3_blob_open: unsupported.\n");
1833: 	return SQLITE_ERROR;
1834: }
1835: 
1836: SQLITE_API const char *sqlite3_db_filename(sqlite3 *db, const char *zDbName) {
1837: 	fprintf(stderr, "sqlite3_db_filename: unsupported.\n");
1838: 	return nullptr;
1839: }
1840: 
1841: SQLITE_API int sqlite3_stmt_busy(sqlite3_stmt *) {
1842: 	fprintf(stderr, "sqlite3_stmt_busy: unsupported.\n");
1843: 	return false;
1844: }
1845: 
1846: SQLITE_API int sqlite3_bind_pointer(sqlite3_stmt *pStmt, int i, void *pPtr, const char *zPTtype,
1847:                                     void (*xDestructor)(void *)) {
1848: 	fprintf(stderr, "sqlite3_bind_pointer: unsupported.\n");
1849: 	return SQLITE_ERROR;
1850: }
1851: 
1852: SQLITE_API int sqlite3_create_module_v2(sqlite3 *db,                   /* Database in which module is registered */
1853:                                         const char *zName,             /* Name assigned to this module */
1854:                                         const sqlite3_module *pModule, /* The definition of the module */
1855:                                         void *pAux,                    /* Context pointer for xCreate/xConnect */
1856:                                         void (*xDestroy)(void *)       /* Module destructor function */
1857: ) {
1858: 	fprintf(stderr, "sqlite3_create_module_v2: unsupported.\n");
1859: 	return SQLITE_ERROR;
1860: }
1861: 
1862: SQLITE_API int sqlite3_blob_write(sqlite3_blob *, const void *z, int n, int iOffset) {
1863: 	fprintf(stderr, "sqlite3_blob_write: unsupported.\n");
1864: 	return SQLITE_ERROR;
1865: }
1866: 
1867: SQLITE_API void sqlite3_set_auxdata(sqlite3_context *, int N, void *, void (*)(void *)) {
1868: 	fprintf(stderr, "sqlite3_set_auxdata: unsupported.\n");
1869: }
1870: 
1871: SQLITE_API sqlite3_stmt *sqlite3_next_stmt(sqlite3 *pDb, sqlite3_stmt *pStmt) {
1872: 	fprintf(stderr, "sqlite3_next_stmt: unsupported.\n");
1873: 	return nullptr;
1874: }
1875: 
1876: SQLITE_API int sqlite3_collation_needed(sqlite3 *, void *, void (*)(void *, sqlite3 *, int eTextRep, const char *)) {
1877: 	fprintf(stderr, "sqlite3_collation_needed: unsupported.\n");
1878: 	return SQLITE_ERROR;
1879: }
1880: 
1881: SQLITE_API int sqlite3_create_collation_v2(sqlite3 *, const char *zName, int eTextRep, void *pArg,
1882:                                            int (*xCompare)(void *, int, const void *, int, const void *),
1883:                                            void (*xDestroy)(void *)) {
1884: 	fprintf(stderr, "sqlite3_create_collation_v2: unsupported.\n");
1885: 	return SQLITE_ERROR;
1886: }
[end of tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: