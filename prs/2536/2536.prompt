You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
CSV to parquet on the fly?
Hello,

I am trying to convert a rather large CSV (hundreds of millions of rows, five columns only) to parquet. If I split the original file into 20M rows chunks I am able to sort each of the chunks (Linux sort), ingest it into DuckDB and write out a corresponding parquet file. Searching such parquet files with DuckDB:

```
SELECT * FROM 'dbsnp155_37_rsid_chrom_pos_ref_alt.??.parquet' WHERE rsidnum=927 LIMIT 1;
```
works OK, but it is a bit too slow for my use case. 

I have sort-merged aforementioned sorted CSV chunks into 2 files. To create parquet I tried:

```
 COPY (SELECT * FROM read_csv_auto('dbsnp155_37_rsid_chrom_pos_ref_alt.BZ.csv', 
 header=False, 
 columns={'rsID':'INT', 'CHR': 'VARCHAR', 'POS': 'INT','REFB': 'VARCHAR','ALTB':'ALTB':'VARCHAR'})) 
 TO 'dbsnp155_37_rsid_chrom_pos_ref_alt.BZ.parquet'  (FORMAT 'parquet');
 ```

 This works, but the resulting parquet file has no proper column names.

 How can I fix it?

DK

CSV to parquet on the fly?
Hello,

I am trying to convert a rather large CSV (hundreds of millions of rows, five columns only) to parquet. If I split the original file into 20M rows chunks I am able to sort each of the chunks (Linux sort), ingest it into DuckDB and write out a corresponding parquet file. Searching such parquet files with DuckDB:

```
SELECT * FROM 'dbsnp155_37_rsid_chrom_pos_ref_alt.??.parquet' WHERE rsidnum=927 LIMIT 1;
```
works OK, but it is a bit too slow for my use case. 

I have sort-merged aforementioned sorted CSV chunks into 2 files. To create parquet I tried:

```
 COPY (SELECT * FROM read_csv_auto('dbsnp155_37_rsid_chrom_pos_ref_alt.BZ.csv', 
 header=False, 
 columns={'rsID':'INT', 'CHR': 'VARCHAR', 'POS': 'INT','REFB': 'VARCHAR','ALTB':'ALTB':'VARCHAR'})) 
 TO 'dbsnp155_37_rsid_chrom_pos_ref_alt.BZ.parquet'  (FORMAT 'parquet');
 ```

 This works, but the resulting parquet file has no proper column names.

 How can I fix it?

DK


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of src/function/table/read_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/main/database.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/main/config.hpp"
8: #include "duckdb/parser/expression/constant_expression.hpp"
9: #include "duckdb/parser/expression/function_expression.hpp"
10: #include "duckdb/parser/tableref/table_function_ref.hpp"
11: #include "duckdb/common/windows_undefs.hpp"
12: 
13: #include <limits>
14: 
15: namespace duckdb {
16: 
17: static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, vector<Value> &inputs,
18:                                             unordered_map<string, Value> &named_parameters,
19:                                             vector<LogicalType> &input_table_types, vector<string> &input_table_names,
20:                                             vector<LogicalType> &return_types, vector<string> &names) {
21: 	auto result = make_unique<ReadCSVData>();
22: 	auto &options = result->options;
23: 
24: 	string file_pattern = inputs[0].str_value;
25: 
26: 	auto &fs = FileSystem::GetFileSystem(context);
27: 	result->files = fs.Glob(file_pattern);
28: 	if (result->files.empty()) {
29: 		throw IOException("No files found that match the pattern \"%s\"", file_pattern);
30: 	}
31: 
32: 	for (auto &kv : named_parameters) {
33: 		if (kv.first == "auto_detect") {
34: 			options.auto_detect = kv.second.value_.boolean;
35: 		} else if (kv.first == "sep" || kv.first == "delim") {
36: 			options.delimiter = kv.second.str_value;
37: 			options.has_delimiter = true;
38: 		} else if (kv.first == "header") {
39: 			options.header = kv.second.value_.boolean;
40: 			options.has_header = true;
41: 		} else if (kv.first == "quote") {
42: 			options.quote = kv.second.str_value;
43: 			options.has_quote = true;
44: 		} else if (kv.first == "escape") {
45: 			options.escape = kv.second.str_value;
46: 			options.has_escape = true;
47: 		} else if (kv.first == "nullstr") {
48: 			options.null_str = kv.second.str_value;
49: 		} else if (kv.first == "sample_size") {
50: 			int64_t sample_size = kv.second.GetValue<int64_t>();
51: 			if (sample_size < 1 && sample_size != -1) {
52: 				throw BinderException("Unsupported parameter for SAMPLE_SIZE: cannot be smaller than 1");
53: 			}
54: 			if (sample_size == -1) {
55: 				options.sample_chunks = std::numeric_limits<uint64_t>::max();
56: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
57: 			} else if (sample_size <= STANDARD_VECTOR_SIZE) {
58: 				options.sample_chunk_size = sample_size;
59: 				options.sample_chunks = 1;
60: 			} else {
61: 				options.sample_chunk_size = STANDARD_VECTOR_SIZE;
62: 				options.sample_chunks = sample_size / STANDARD_VECTOR_SIZE;
63: 			}
64: 		} else if (kv.first == "sample_chunk_size") {
65: 			options.sample_chunk_size = kv.second.GetValue<int64_t>();
66: 			if (options.sample_chunk_size > STANDARD_VECTOR_SIZE) {
67: 				throw BinderException(
68: 				    "Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be bigger than STANDARD_VECTOR_SIZE %d",
69: 				    STANDARD_VECTOR_SIZE);
70: 			} else if (options.sample_chunk_size < 1) {
71: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNK_SIZE: cannot be smaller than 1");
72: 			}
73: 		} else if (kv.first == "sample_chunks") {
74: 			options.sample_chunks = kv.second.GetValue<int64_t>();
75: 			if (options.sample_chunks < 1) {
76: 				throw BinderException("Unsupported parameter for SAMPLE_CHUNKS: cannot be smaller than 1");
77: 			}
78: 		} else if (kv.first == "all_varchar") {
79: 			options.all_varchar = kv.second.value_.boolean;
80: 		} else if (kv.first == "dateformat") {
81: 			options.has_format[LogicalTypeId::DATE] = true;
82: 			auto &date_format = options.date_format[LogicalTypeId::DATE];
83: 			date_format.format_specifier = kv.second.str_value;
84: 			string error = StrTimeFormat::ParseFormatSpecifier(date_format.format_specifier, date_format);
85: 			if (!error.empty()) {
86: 				throw InvalidInputException("Could not parse DATEFORMAT: %s", error.c_str());
87: 			}
88: 		} else if (kv.first == "timestampformat") {
89: 			options.has_format[LogicalTypeId::TIMESTAMP] = true;
90: 			auto &timestamp_format = options.date_format[LogicalTypeId::TIMESTAMP];
91: 			timestamp_format.format_specifier = kv.second.str_value;
92: 			string error = StrTimeFormat::ParseFormatSpecifier(timestamp_format.format_specifier, timestamp_format);
93: 			if (!error.empty()) {
94: 				throw InvalidInputException("Could not parse TIMESTAMPFORMAT: %s", error.c_str());
95: 			}
96: 		} else if (kv.first == "normalize_names") {
97: 			options.normalize_names = kv.second.value_.boolean;
98: 		} else if (kv.first == "columns") {
99: 			auto &child_type = kv.second.type();
100: 			if (child_type.id() != LogicalTypeId::STRUCT) {
101: 				throw BinderException("read_csv columns requires a a struct as input");
102: 			}
103: 			D_ASSERT(StructType::GetChildCount(child_type) == kv.second.struct_value.size());
104: 			for (idx_t i = 0; i < kv.second.struct_value.size(); i++) {
105: 				auto &name = StructType::GetChildName(child_type, i);
106: 				auto &val = kv.second.struct_value[i];
107: 				names.push_back(name);
108: 				if (val.type().id() != LogicalTypeId::VARCHAR) {
109: 					throw BinderException("read_csv requires a type specification as string");
110: 				}
111: 				return_types.emplace_back(TransformStringToLogicalType(val.str_value.c_str()));
112: 			}
113: 			if (names.empty()) {
114: 				throw BinderException("read_csv requires at least a single column as input!");
115: 			}
116: 		} else if (kv.first == "compression") {
117: 			options.compression = kv.second.str_value;
118: 		} else if (kv.first == "filename") {
119: 			result->include_file_name = kv.second.value_.boolean;
120: 		} else if (kv.first == "skip") {
121: 			options.skip_rows = kv.second.GetValue<int64_t>();
122: 		}
123: 	}
124: 	if (!options.auto_detect && return_types.empty()) {
125: 		throw BinderException("read_csv requires columns to be specified. Use read_csv_auto or set read_csv(..., "
126: 		                      "AUTO_DETECT=TRUE) to automatically guess columns.");
127: 	}
128: 	if (!(options.compression == "infer" || options.compression == "gzip" || options.compression == "none" ||
129: 	      options.compression.empty())) {
130: 		throw BinderException("read_csv currently only supports 'gzip' compression.");
131: 	}
132: 	if (options.auto_detect) {
133: 		options.file_path = result->files[0];
134: 		auto initial_reader = make_unique<BufferedCSVReader>(context, options);
135: 
136: 		return_types.assign(initial_reader->sql_types.begin(), initial_reader->sql_types.end());
137: 		names.assign(initial_reader->col_names.begin(), initial_reader->col_names.end());
138: 		result->initial_reader = move(initial_reader);
139: 	} else {
140: 		result->sql_types = return_types;
141: 		D_ASSERT(return_types.size() == names.size());
142: 	}
143: 	if (result->include_file_name) {
144: 		return_types.push_back(LogicalType::VARCHAR);
145: 		names.emplace_back("filename");
146: 	}
147: 	return move(result);
148: }
149: 
150: struct ReadCSVOperatorData : public FunctionOperatorData {
151: 	//! The CSV reader
152: 	unique_ptr<BufferedCSVReader> csv_reader;
153: 	//! The index of the next file to read (i.e. current file + 1)
154: 	idx_t file_index;
155: };
156: 
157: static unique_ptr<FunctionOperatorData> ReadCSVInit(ClientContext &context, const FunctionData *bind_data_p,
158:                                                     const vector<column_t> &column_ids,
159:                                                     TableFilterCollection *filters) {
160: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
161: 	auto result = make_unique<ReadCSVOperatorData>();
162: 	if (bind_data.initial_reader) {
163: 		result->csv_reader = move(bind_data.initial_reader);
164: 	} else {
165: 		bind_data.options.file_path = bind_data.files[0];
166: 		result->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);
167: 	}
168: 	bind_data.bytes_read = 0;
169: 	bind_data.file_size = result->csv_reader->file_size;
170: 	result->file_index = 1;
171: 	return move(result);
172: }
173: 
174: static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, vector<Value> &inputs,
175:                                                 unordered_map<string, Value> &named_parameters,
176:                                                 vector<LogicalType> &input_table_types,
177:                                                 vector<string> &input_table_names, vector<LogicalType> &return_types,
178:                                                 vector<string> &names) {
179: 	named_parameters["auto_detect"] = Value::BOOLEAN(true);
180: 	return ReadCSVBind(context, inputs, named_parameters, input_table_types, input_table_names, return_types, names);
181: }
182: 
183: static void ReadCSVFunction(ClientContext &context, const FunctionData *bind_data_p,
184:                             FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
185: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
186: 	auto &data = (ReadCSVOperatorData &)*operator_state;
187: 	do {
188: 		data.csv_reader->ParseCSV(output);
189: 		bind_data.bytes_read = data.csv_reader->bytes_in_chunk;
190: 		if (output.size() == 0 && data.file_index < bind_data.files.size()) {
191: 			// exhausted this file, but we have more files we can read
192: 			// open the next file and increment the counter
193: 			bind_data.options.file_path = bind_data.files[data.file_index];
194: 			data.csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, data.csv_reader->sql_types);
195: 			data.file_index++;
196: 		} else {
197: 			break;
198: 		}
199: 	} while (true);
200: 	if (bind_data.include_file_name) {
201: 		auto &col = output.data.back();
202: 		col.SetValue(0, Value(data.csv_reader->options.file_path));
203: 		col.SetVectorType(VectorType::CONSTANT_VECTOR);
204: 	}
205: }
206: 
207: static void ReadCSVAddNamedParameters(TableFunction &table_function) {
208: 	table_function.named_parameters["sep"] = LogicalType::VARCHAR;
209: 	table_function.named_parameters["delim"] = LogicalType::VARCHAR;
210: 	table_function.named_parameters["quote"] = LogicalType::VARCHAR;
211: 	table_function.named_parameters["escape"] = LogicalType::VARCHAR;
212: 	table_function.named_parameters["nullstr"] = LogicalType::VARCHAR;
213: 	table_function.named_parameters["columns"] = LogicalType::ANY;
214: 	table_function.named_parameters["header"] = LogicalType::BOOLEAN;
215: 	table_function.named_parameters["auto_detect"] = LogicalType::BOOLEAN;
216: 	table_function.named_parameters["sample_size"] = LogicalType::BIGINT;
217: 	table_function.named_parameters["sample_chunk_size"] = LogicalType::BIGINT;
218: 	table_function.named_parameters["sample_chunks"] = LogicalType::BIGINT;
219: 	table_function.named_parameters["all_varchar"] = LogicalType::BOOLEAN;
220: 	table_function.named_parameters["dateformat"] = LogicalType::VARCHAR;
221: 	table_function.named_parameters["timestampformat"] = LogicalType::VARCHAR;
222: 	table_function.named_parameters["normalize_names"] = LogicalType::BOOLEAN;
223: 	table_function.named_parameters["compression"] = LogicalType::VARCHAR;
224: 	table_function.named_parameters["filename"] = LogicalType::BOOLEAN;
225: 	table_function.named_parameters["skip"] = LogicalType::BIGINT;
226: }
227: 
228: int CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p) {
229: 	auto &bind_data = (ReadCSVData &)*bind_data_p;
230: 	if (bind_data.file_size == 0) {
231: 		return 100;
232: 	}
233: 	auto percentage = bind_data.bytes_read * 100 / bind_data.file_size;
234: 	return percentage;
235: }
236: 
237: TableFunction ReadCSVTableFunction::GetFunction() {
238: 	TableFunction read_csv("read_csv", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVBind, ReadCSVInit);
239: 	read_csv.table_scan_progress = CSVReaderProgress;
240: 	ReadCSVAddNamedParameters(read_csv);
241: 	return read_csv;
242: }
243: 
244: void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {
245: 	set.AddFunction(ReadCSVTableFunction::GetFunction());
246: 
247: 	TableFunction read_csv_auto("read_csv_auto", {LogicalType::VARCHAR}, ReadCSVFunction, ReadCSVAutoBind, ReadCSVInit);
248: 	read_csv_auto.table_scan_progress = CSVReaderProgress;
249: 	ReadCSVAddNamedParameters(read_csv_auto);
250: 	set.AddFunction(read_csv_auto);
251: }
252: 
253: unique_ptr<TableFunctionRef> ReadCSVReplacement(const string &table_name, void *data) {
254: 	if (!StringUtil::EndsWith(table_name, ".csv") && !StringUtil::EndsWith(table_name, ".tsv") &&
255: 	    !StringUtil::EndsWith(table_name, ".csv.gz")) {
256: 		return nullptr;
257: 	}
258: 	auto table_function = make_unique<TableFunctionRef>();
259: 	vector<unique_ptr<ParsedExpression>> children;
260: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
261: 	table_function->function = make_unique<FunctionExpression>("read_csv_auto", move(children));
262: 	return table_function;
263: }
264: 
265: void BuiltinFunctions::RegisterReadFunctions() {
266: 	CSVCopyFunction::RegisterFunction(*this);
267: 	ReadCSVTableFunction::RegisterFunction(*this);
268: 
269: 	auto &config = DBConfig::GetConfig(context);
270: 	config.replacement_scans.emplace_back(ReadCSVReplacement);
271: }
272: 
273: } // namespace duckdb
[end of src/function/table/read_csv.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: