{
  "repo": "duckdb/duckdb",
  "pull_number": 2536,
  "instance_id": "duckdb__duckdb-2536",
  "issue_numbers": [
    "2518",
    "2518"
  ],
  "base_commit": "5c86f109a0f1c489ee7144aa7522d3f1e58b9fb0",
  "patch": "diff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp\nindex b1abf084bda2..303f89aa1b35 100644\n--- a/src/function/table/read_csv.cpp\n+++ b/src/function/table/read_csv.cpp\n@@ -134,7 +134,11 @@ static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, vector<Value\n \t\tauto initial_reader = make_unique<BufferedCSVReader>(context, options);\n \n \t\treturn_types.assign(initial_reader->sql_types.begin(), initial_reader->sql_types.end());\n-\t\tnames.assign(initial_reader->col_names.begin(), initial_reader->col_names.end());\n+\t\tif (names.empty()) {\n+\t\t\tnames.assign(initial_reader->col_names.begin(), initial_reader->col_names.end());\n+\t\t} else {\n+\t\t\tD_ASSERT(return_types.size() == names.size());\n+\t\t}\n \t\tresult->initial_reader = move(initial_reader);\n \t} else {\n \t\tresult->sql_types = return_types;\n",
  "test_patch": "diff --git a/test/sql/copy/csv/column_names.test b/test/sql/copy/csv/column_names.test\nnew file mode 100644\nindex 000000000000..e8eb5b64f1f8\n--- /dev/null\n+++ b/test/sql/copy/csv/column_names.test\n@@ -0,0 +1,58 @@\n+# name: test/sql/copy/csv/column_names.test\n+# description: Test correct column name output in read_csv functions\n+# group: [csv]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE t1 AS SELECT * FROM read_csv_auto('test/sql/copy/csv/data/test/issue2518.csv', header=False, columns={'rsID':'INT', 'CHR': 'VARCHAR', 'POS': 'INT','REFB': 'VARCHAR','ALTB':'VARCHAR'})\n+\n+query IIIII\n+SELECT rsID, chr, pos, refb, altb FROM t1\n+----\n+4690\t1\t14673\tG\tA,C,T\n+5\t7\t91839110\tC\tT\n+6\t7\t91747131\tA\tG\n+7\t7\t91779557\tT\tA\n+8\t7\t92408329\tC\tT\n+9\t7\t92373453\tTG\tT\n+10\t7\t92383888\tA\tC,G,T\n+1090\t8\t402108\tC\tG,T\n+11\t7\t11364201\tC\tT\n+1184\t6\t187649\tT\tA,C,G\n+\n+statement ok\n+CREATE TABLE t2 AS SELECT * FROM read_csv_auto('test/sql/copy/csv/data/test/issue2518.csv', header=False, columns={'rsID':'INT', 'CHR': 'VARCHAR', 'POS': 'INT','REFB': 'VARCHAR','ALTB':'VARCHAR'}, AUTO_DETECT=0)\n+\n+query IIIII\n+SELECT rsID, chr, pos, refb, altb FROM t2\n+----\n+4690\t1\t14673\tG\tA,C,T\n+5\t7\t91839110\tC\tT\n+6\t7\t91747131\tA\tG\n+7\t7\t91779557\tT\tA\n+8\t7\t92408329\tC\tT\n+9\t7\t92373453\tTG\tT\n+10\t7\t92383888\tA\tC,G,T\n+1090\t8\t402108\tC\tG,T\n+11\t7\t11364201\tC\tT\n+1184\t6\t187649\tT\tA,C,G\n+\n+\n+statement ok\n+CREATE TABLE t3 AS SELECT * FROM read_csv_auto('test/sql/copy/csv/data/test/issue2518.csv', columns={'rsID':'INT', 'CHR': 'VARCHAR', 'POS': 'INT','REFB': 'VARCHAR','ALTB':'VARCHAR'})\n+\n+query IIIII\n+SELECT rsID, chr, pos, refb, altb FROM t3\n+----\n+4690\t1\t14673\tG\tA,C,T\n+5\t7\t91839110\tC\tT\n+6\t7\t91747131\tA\tG\n+7\t7\t91779557\tT\tA\n+8\t7\t92408329\tC\tT\n+9\t7\t92373453\tTG\tT\n+10\t7\t92383888\tA\tC,G,T\n+1090\t8\t402108\tC\tG,T\n+11\t7\t11364201\tC\tT\n+1184\t6\t187649\tT\tA,C,G\ndiff --git a/test/sql/copy/csv/data/test/issue2518.csv b/test/sql/copy/csv/data/test/issue2518.csv\nnew file mode 100644\nindex 000000000000..b1e560eff50f\n--- /dev/null\n+++ b/test/sql/copy/csv/data/test/issue2518.csv\n@@ -0,0 +1,10 @@\n+4690,1,14673,G,\"A,C,T\"\n+5,7,91839110,C,T\n+6,7,91747131,A,G\n+7,7,91779557,T,A\n+8,7,92408329,C,T\n+9,7,92373453,TG,T\n+10,7,92383888,A,\"C,G,T\"\n+1090,8,402108,C,\"G,T\"\n+11,7,11364201,C,T\n+1184,6,187649,T,\"A,C,G\"\n\\ No newline at end of file\n",
  "problem_statement": "CSV to parquet on the fly?\nHello,\r\n\r\nI am trying to convert a rather large CSV (hundreds of millions of rows, five columns only) to parquet. If I split the original file into 20M rows chunks I am able to sort each of the chunks (Linux sort), ingest it into DuckDB and write out a corresponding parquet file. Searching such parquet files with DuckDB:\r\n\r\n```\r\nSELECT * FROM 'dbsnp155_37_rsid_chrom_pos_ref_alt.??.parquet' WHERE rsidnum=927 LIMIT 1;\r\n```\r\nworks OK, but it is a bit too slow for my use case. \r\n\r\nI have sort-merged aforementioned sorted CSV chunks into 2 files. To create parquet I tried:\r\n\r\n```\r\n COPY (SELECT * FROM read_csv_auto('dbsnp155_37_rsid_chrom_pos_ref_alt.BZ.csv', \r\n header=False, \r\n columns={'rsID':'INT', 'CHR': 'VARCHAR', 'POS': 'INT','REFB': 'VARCHAR','ALTB':'ALTB':'VARCHAR'})) \r\n TO 'dbsnp155_37_rsid_chrom_pos_ref_alt.BZ.parquet'  (FORMAT 'parquet');\r\n ```\r\n\r\n This works, but the resulting parquet file has no proper column names.\r\n\r\n How can I fix it?\r\n\r\nDK\r\n\nCSV to parquet on the fly?\nHello,\r\n\r\nI am trying to convert a rather large CSV (hundreds of millions of rows, five columns only) to parquet. If I split the original file into 20M rows chunks I am able to sort each of the chunks (Linux sort), ingest it into DuckDB and write out a corresponding parquet file. Searching such parquet files with DuckDB:\r\n\r\n```\r\nSELECT * FROM 'dbsnp155_37_rsid_chrom_pos_ref_alt.??.parquet' WHERE rsidnum=927 LIMIT 1;\r\n```\r\nworks OK, but it is a bit too slow for my use case. \r\n\r\nI have sort-merged aforementioned sorted CSV chunks into 2 files. To create parquet I tried:\r\n\r\n```\r\n COPY (SELECT * FROM read_csv_auto('dbsnp155_37_rsid_chrom_pos_ref_alt.BZ.csv', \r\n header=False, \r\n columns={'rsID':'INT', 'CHR': 'VARCHAR', 'POS': 'INT','REFB': 'VARCHAR','ALTB':'ALTB':'VARCHAR'})) \r\n TO 'dbsnp155_37_rsid_chrom_pos_ref_alt.BZ.parquet'  (FORMAT 'parquet');\r\n ```\r\n\r\n This works, but the resulting parquet file has no proper column names.\r\n\r\n How can I fix it?\r\n\r\nDK\r\n\n",
  "hints_text": "This seems to work correctly for me on a small example I made myself, could you try to make a reproducible example?\n@Mytherin Thank you very much for looking into this.\r\n\r\nHere it goes:\r\n* test4duckdb.csv\r\n```CSV\r\n4690,1,14673,G,\"A,C,T\"\r\n5,7,91839110,C,T\r\n6,7,91747131,A,G\r\n7,7,91779557,T,A\r\n8,7,92408329,C,T\r\n9,7,92373453,TG,T\r\n10,7,92383888,A,\"C,G,T\"\r\n1090,8,402108,C,\"G,T\"\r\n11,7,11364201,C,T\r\n1184,6,187649,T,\"A,C,G\"\r\n```\r\n\r\n* code in duckdb CLI\r\n\r\n```SQL\r\nCOPY (SELECT * FROM read_csv_auto('test4duckdb.csv', header=False, columns={'rsID':'INT', 'CHR': 'VARCHAR', 'POS': 'INT','REFB': 'VARCHAR','ALTB':'VARCHAR'})) TO 'test4duckdb.parq\r\nCREATE TABLE test4duckdbtbl AS SELECT * FROM 'test4duckdb.parquet';\r\nSELECT * FROM test4duckdbtbl;\r\n```\r\n\r\nI got:\r\n\r\n```\r\n| column0 | column1 | column2  | column3 | column4 |\r\n<snip>\r\n```\r\n\r\n* settings\r\n\r\n```\r\n.show\r\n        echo: off\r\n         eqp: off\r\n     explain: auto\r\n     headers: on\r\n        mode: box\r\n   nullvalue: \"\"\r\n      output: stdout\r\ncolseparator: \"|\"\r\nrowseparator: \"\\n\"\r\n       stats: off\r\n       width: 0 0 0 0 0\r\n    filename: :memory:\r\nD .version\r\nSQLite v0.2.9 1776611ab\r\ngcc-5.5.0 20171010\r\n```\r\n\r\nI am using it from a Singularity container, at the moment on a head node of an HPC cluster. Went with 0.29.0 version because of the libssl version requirement. \r\n\r\nDK\r\n\r\n**edit**: SELECT uppercase\nThis seems to work correctly for me on a small example I made myself, could you try to make a reproducible example?\n@Mytherin Thank you very much for looking into this.\r\n\r\nHere it goes:\r\n* test4duckdb.csv\r\n```CSV\r\n4690,1,14673,G,\"A,C,T\"\r\n5,7,91839110,C,T\r\n6,7,91747131,A,G\r\n7,7,91779557,T,A\r\n8,7,92408329,C,T\r\n9,7,92373453,TG,T\r\n10,7,92383888,A,\"C,G,T\"\r\n1090,8,402108,C,\"G,T\"\r\n11,7,11364201,C,T\r\n1184,6,187649,T,\"A,C,G\"\r\n```\r\n\r\n* code in duckdb CLI\r\n\r\n```SQL\r\nCOPY (SELECT * FROM read_csv_auto('test4duckdb.csv', header=False, columns={'rsID':'INT', 'CHR': 'VARCHAR', 'POS': 'INT','REFB': 'VARCHAR','ALTB':'VARCHAR'})) TO 'test4duckdb.parq\r\nCREATE TABLE test4duckdbtbl AS SELECT * FROM 'test4duckdb.parquet';\r\nSELECT * FROM test4duckdbtbl;\r\n```\r\n\r\nI got:\r\n\r\n```\r\n| column0 | column1 | column2  | column3 | column4 |\r\n<snip>\r\n```\r\n\r\n* settings\r\n\r\n```\r\n.show\r\n        echo: off\r\n         eqp: off\r\n     explain: auto\r\n     headers: on\r\n        mode: box\r\n   nullvalue: \"\"\r\n      output: stdout\r\ncolseparator: \"|\"\r\nrowseparator: \"\\n\"\r\n       stats: off\r\n       width: 0 0 0 0 0\r\n    filename: :memory:\r\nD .version\r\nSQLite v0.2.9 1776611ab\r\ngcc-5.5.0 20171010\r\n```\r\n\r\nI am using it from a Singularity container, at the moment on a head node of an HPC cluster. Went with 0.29.0 version because of the libssl version requirement. \r\n\r\nDK\r\n\r\n**edit**: SELECT uppercase",
  "created_at": "2021-11-04T08:18:27Z"
}