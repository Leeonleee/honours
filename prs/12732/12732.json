{
  "repo": "duckdb/duckdb",
  "pull_number": 12732,
  "instance_id": "duckdb__duckdb-12732",
  "issue_numbers": [
    "12610"
  ],
  "base_commit": "c6aad82beccb08e8b5953fc34d00b0127f83c24a",
  "patch": "diff --git a/.github/workflows/CodeQuality.yml b/.github/workflows/CodeQuality.yml\nindex 37c0e33b0661..a5787139075d 100644\n--- a/.github/workflows/CodeQuality.yml\n+++ b/.github/workflows/CodeQuality.yml\n@@ -49,6 +49,10 @@ jobs:\n         shell: bash\n         run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build clang-format-11 && sudo pip3 install cmake-format black cxxheaderparser pcpp\n \n+      - name: List Installed Packages\n+        shell: bash\n+        run: pip3 freeze\n+\n       - name: Format Check\n         shell: bash\n         run: |\ndiff --git a/tools/pythonpkg/duckdb-stubs/__init__.pyi b/tools/pythonpkg/duckdb-stubs/__init__.pyi\nindex 7bcb25ec6f47..e469a70ef6d9 100644\n--- a/tools/pythonpkg/duckdb-stubs/__init__.pyi\n+++ b/tools/pythonpkg/duckdb-stubs/__init__.pyi\n@@ -315,7 +315,7 @@ class DuckDBPyConnection:\n     def view(self, view_name: str) -> DuckDBPyRelation: ...\n     def values(self, values: List[Any]) -> DuckDBPyRelation: ...\n     def table_function(self, name: str, parameters: object = None) -> DuckDBPyRelation: ...\n-    def read_json(self, name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None) -> DuckDBPyRelation: ...\n+    def read_json(self, name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, compression: Optional[str] = None, maximum_object_size: Optional[int] = None, ignore_errors: Optional[bool] = None, convert_strings_to_integers: Optional[bool] = None, field_appearance_threshold: Optional[float] = None, map_inference_threshold: Optional[int] = None, maximum_sample_files: Optional[int] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None) -> DuckDBPyRelation: ...\n     def extract_statements(self, query: str) -> List[Statement]: ...\n     def sql(self, query: str, *, alias: str = \"\", params: object = None) -> DuckDBPyRelation: ...\n     def query(self, query: str, *, alias: str = \"\", params: object = None) -> DuckDBPyRelation: ...\n@@ -640,7 +640,7 @@ def table(table_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyR\n def view(view_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n def values(values: List[Any], *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n def table_function(name: str, parameters: object = None, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n-def read_json(name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n+def read_json(name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, compression: Optional[str] = None, maximum_object_size: Optional[int] = None, ignore_errors: Optional[bool] = None, convert_strings_to_integers: Optional[bool] = None, field_appearance_threshold: Optional[float] = None, map_inference_threshold: Optional[int] = None, maximum_sample_files: Optional[int] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n def extract_statements(query: str, *, connection: DuckDBPyConnection = ...) -> List[Statement]: ...\n def sql(query: str, *, alias: str = \"\", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\n def query(query: str, *, alias: str = \"\", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...\ndiff --git a/tools/pythonpkg/duckdb_python.cpp b/tools/pythonpkg/duckdb_python.cpp\nindex bec9342d83c7..87ddabcaae13 100644\n--- a/tools/pythonpkg/duckdb_python.cpp\n+++ b/tools/pythonpkg/duckdb_python.cpp\n@@ -563,18 +563,42 @@ static void InitializeConnectionMethods(py::module_ &m) {\n \t    py::arg(\"parameters\") = py::none(), py::kw_only(), py::arg(\"connection\") = py::none());\n \tm.def(\n \t    \"read_json\",\n-\t    [](const string &filename, const Optional<py::object> &columns = py::none(),\n+\t    [](const string &name, const Optional<py::object> &columns = py::none(),\n \t       const Optional<py::object> &sample_size = py::none(), const Optional<py::object> &maximum_depth = py::none(),\n \t       const Optional<py::str> &records = py::none(), const Optional<py::str> &format = py::none(),\n+\t       const Optional<py::object> &date_format = py::none(),\n+\t       const Optional<py::object> &timestamp_format = py::none(),\n+\t       const Optional<py::object> &compression = py::none(),\n+\t       const Optional<py::object> &maximum_object_size = py::none(),\n+\t       const Optional<py::object> &ignore_errors = py::none(),\n+\t       const Optional<py::object> &convert_strings_to_integers = py::none(),\n+\t       const Optional<py::object> &field_appearance_threshold = py::none(),\n+\t       const Optional<py::object> &map_inference_threshold = py::none(),\n+\t       const Optional<py::object> &maximum_sample_files = py::none(),\n+\t       const Optional<py::object> &filename = py::none(),\n+\t       const Optional<py::object> &hive_partitioning = py::none(),\n+\t       const Optional<py::object> &union_by_name = py::none(), const Optional<py::object> &hive_types = py::none(),\n+\t       const Optional<py::object> &hive_types_autocast = py::none(),\n \t       shared_ptr<DuckDBPyConnection> conn = nullptr) {\n \t\t    if (!conn) {\n \t\t\t    conn = DuckDBPyConnection::DefaultConnection();\n \t\t    }\n-\t\t    return conn->ReadJSON(filename, columns, sample_size, maximum_depth, records, format);\n+\t\t    return conn->ReadJSON(name, columns, sample_size, maximum_depth, records, format, date_format,\n+\t\t                          timestamp_format, compression, maximum_object_size, ignore_errors,\n+\t\t                          convert_strings_to_integers, field_appearance_threshold, map_inference_threshold,\n+\t\t                          maximum_sample_files, filename, hive_partitioning, union_by_name, hive_types,\n+\t\t                          hive_types_autocast);\n \t    },\n \t    \"Create a relation object from the JSON file in 'name'\", py::arg(\"name\"), py::kw_only(),\n \t    py::arg(\"columns\") = py::none(), py::arg(\"sample_size\") = py::none(), py::arg(\"maximum_depth\") = py::none(),\n-\t    py::arg(\"records\") = py::none(), py::arg(\"format\") = py::none(), py::arg(\"connection\") = py::none());\n+\t    py::arg(\"records\") = py::none(), py::arg(\"format\") = py::none(), py::arg(\"date_format\") = py::none(),\n+\t    py::arg(\"timestamp_format\") = py::none(), py::arg(\"compression\") = py::none(),\n+\t    py::arg(\"maximum_object_size\") = py::none(), py::arg(\"ignore_errors\") = py::none(),\n+\t    py::arg(\"convert_strings_to_integers\") = py::none(), py::arg(\"field_appearance_threshold\") = py::none(),\n+\t    py::arg(\"map_inference_threshold\") = py::none(), py::arg(\"maximum_sample_files\") = py::none(),\n+\t    py::arg(\"filename\") = py::none(), py::arg(\"hive_partitioning\") = py::none(),\n+\t    py::arg(\"union_by_name\") = py::none(), py::arg(\"hive_types\") = py::none(),\n+\t    py::arg(\"hive_types_autocast\") = py::none(), py::arg(\"connection\") = py::none());\n \tm.def(\n \t    \"extract_statements\",\n \t    [](const string &query, shared_ptr<DuckDBPyConnection> conn = nullptr) {\ndiff --git a/tools/pythonpkg/scripts/connection_methods.json b/tools/pythonpkg/scripts/connection_methods.json\nindex abbf5b0299e5..6091a3fa8aa8 100644\n--- a/tools/pythonpkg/scripts/connection_methods.json\n+++ b/tools/pythonpkg/scripts/connection_methods.json\n@@ -571,6 +571,76 @@\n \t\t\t\t\"name\": \"format\",\n \t\t\t\t\"default\": \"None\",\n \t\t\t\t\"type\": \"Optional[str]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"date_format\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[str]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"timestamp_format\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[str]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"compression\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[str]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"maximum_object_size\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[int]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"ignore_errors\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[bool]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"convert_strings_to_integers\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[bool]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"field_appearance_threshold\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[float]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"map_inference_threshold\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[int]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"maximum_sample_files\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[int]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"filename\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[bool | str]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"hive_partitioning\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[bool]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"union_by_name\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[bool]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"hive_types\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[Dict[str, str]]\"\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\t\"name\": \"hive_types_autocast\",\n+\t\t\t\t\"default\": \"None\",\n+\t\t\t\t\"type\": \"Optional[bool]\"\n \t\t\t}\n \t\t],\n \t\t\"return\": \"DuckDBPyRelation\"\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\nindex ae6b5ed46fa5..d4f37b7ce2e7 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\n@@ -85,11 +85,21 @@ struct DuckDBPyConnection : public enable_shared_from_this<DuckDBPyConnection> {\n \n \tpy::list ExtractStatements(const string &query);\n \n-\tunique_ptr<DuckDBPyRelation> ReadJSON(const string &filename, const Optional<py::object> &columns = py::none(),\n-\t                                      const Optional<py::object> &sample_size = py::none(),\n-\t                                      const Optional<py::object> &maximum_depth = py::none(),\n-\t                                      const Optional<py::str> &records = py::none(),\n-\t                                      const Optional<py::str> &format = py::none());\n+\tunique_ptr<DuckDBPyRelation> ReadJSON(\n+\t    const string &name, const Optional<py::object> &columns = py::none(),\n+\t    const Optional<py::object> &sample_size = py::none(), const Optional<py::object> &maximum_depth = py::none(),\n+\t    const Optional<py::str> &records = py::none(), const Optional<py::str> &format = py::none(),\n+\t    const Optional<py::object> &date_format = py::none(), const Optional<py::object> &timestamp_format = py::none(),\n+\t    const Optional<py::object> &compression = py::none(),\n+\t    const Optional<py::object> &maximum_object_size = py::none(),\n+\t    const Optional<py::object> &ignore_errors = py::none(),\n+\t    const Optional<py::object> &convert_strings_to_integers = py::none(),\n+\t    const Optional<py::object> &field_appearance_threshold = py::none(),\n+\t    const Optional<py::object> &map_inference_threshold = py::none(),\n+\t    const Optional<py::object> &maximum_sample_files = py::none(),\n+\t    const Optional<py::object> &filename = py::none(), const Optional<py::object> &hive_partitioning = py::none(),\n+\t    const Optional<py::object> &union_by_name = py::none(), const Optional<py::object> &hive_types = py::none(),\n+\t    const Optional<py::object> &hive_types_autocast = py::none());\n \n \tshared_ptr<DuckDBPyType> MapType(const shared_ptr<DuckDBPyType> &key_type,\n \t                                 const shared_ptr<DuckDBPyType> &value_type);\ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 5702983377bf..2c18cacbdfbb 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -214,7 +214,14 @@ static void InitializeConnectionMethods(py::class_<DuckDBPyConnection, shared_pt\n \t      py::arg(\"parameters\") = py::none());\n \tm.def(\"read_json\", &DuckDBPyConnection::ReadJSON, \"Create a relation object from the JSON file in 'name'\",\n \t      py::arg(\"name\"), py::kw_only(), py::arg(\"columns\") = py::none(), py::arg(\"sample_size\") = py::none(),\n-\t      py::arg(\"maximum_depth\") = py::none(), py::arg(\"records\") = py::none(), py::arg(\"format\") = py::none());\n+\t      py::arg(\"maximum_depth\") = py::none(), py::arg(\"records\") = py::none(), py::arg(\"format\") = py::none(),\n+\t      py::arg(\"date_format\") = py::none(), py::arg(\"timestamp_format\") = py::none(),\n+\t      py::arg(\"compression\") = py::none(), py::arg(\"maximum_object_size\") = py::none(),\n+\t      py::arg(\"ignore_errors\") = py::none(), py::arg(\"convert_strings_to_integers\") = py::none(),\n+\t      py::arg(\"field_appearance_threshold\") = py::none(), py::arg(\"map_inference_threshold\") = py::none(),\n+\t      py::arg(\"maximum_sample_files\") = py::none(), py::arg(\"filename\") = py::none(),\n+\t      py::arg(\"hive_partitioning\") = py::none(), py::arg(\"union_by_name\") = py::none(),\n+\t      py::arg(\"hive_types\") = py::none(), py::arg(\"hive_types_autocast\") = py::none());\n \tm.def(\"extract_statements\", &DuckDBPyConnection::ExtractStatements,\n \t      \"Parse the query string and extract the Statement object(s) produced\", py::arg(\"query\"));\n \tm.def(\"sql\", &DuckDBPyConnection::RunQuery,\n@@ -737,17 +744,67 @@ shared_ptr<DuckDBPyConnection> DuckDBPyConnection::RegisterPythonObject(const st\n \treturn shared_from_this();\n }\n \n-unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(const string &name, const Optional<py::object> &columns,\n-                                                          const Optional<py::object> &sample_size,\n-                                                          const Optional<py::object> &maximum_depth,\n-                                                          const Optional<py::str> &records,\n-                                                          const Optional<py::str> &format) {\n+static void ParseMultiFileReaderOptions(named_parameter_map_t &options, const Optional<py::object> &filename,\n+                                        const Optional<py::object> &hive_partitioning,\n+                                        const Optional<py::object> &union_by_name,\n+                                        const Optional<py::object> &hive_types,\n+                                        const Optional<py::object> &hive_types_autocast) {\n+\tif (!py::none().is(filename)) {\n+\t\tauto val = TransformPythonValue(filename);\n+\t\toptions[\"filename\"] = val;\n+\t}\n+\n+\tif (!py::none().is(hive_types)) {\n+\t\tauto val = TransformPythonValue(hive_types);\n+\t\toptions[\"hive_types\"] = val;\n+\t}\n+\n+\tif (!py::none().is(hive_partitioning)) {\n+\t\tif (!py::isinstance<py::bool_>(hive_partitioning)) {\n+\t\t\tstring actual_type = py::str(hive_partitioning.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'hive_partitioning' as a boolean, not '%s'\", actual_type);\n+\t\t}\n+\t\tauto val = TransformPythonValue(hive_partitioning, LogicalTypeId::BOOLEAN);\n+\t\toptions[\"hive_partitioning\"] = val;\n+\t}\n+\n+\tif (!py::none().is(union_by_name)) {\n+\t\tif (!py::isinstance<py::bool_>(union_by_name)) {\n+\t\t\tstring actual_type = py::str(union_by_name.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'union_by_name' as a boolean, not '%s'\", actual_type);\n+\t\t}\n+\t\tauto val = TransformPythonValue(union_by_name, LogicalTypeId::BOOLEAN);\n+\t\toptions[\"union_by_name\"] = val;\n+\t}\n+\n+\tif (!py::none().is(hive_types_autocast)) {\n+\t\tif (!py::isinstance<py::bool_>(hive_types_autocast)) {\n+\t\t\tstring actual_type = py::str(hive_types_autocast.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'hive_types_autocast' as a boolean, not '%s'\", actual_type);\n+\t\t}\n+\t\tauto val = TransformPythonValue(hive_types_autocast, LogicalTypeId::BOOLEAN);\n+\t\toptions[\"hive_types_autocast\"] = val;\n+\t}\n+}\n+\n+unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(\n+    const string &name, const Optional<py::object> &columns, const Optional<py::object> &sample_size,\n+    const Optional<py::object> &maximum_depth, const Optional<py::str> &records, const Optional<py::str> &format,\n+    const Optional<py::object> &date_format, const Optional<py::object> &timestamp_format,\n+    const Optional<py::object> &compression, const Optional<py::object> &maximum_object_size,\n+    const Optional<py::object> &ignore_errors, const Optional<py::object> &convert_strings_to_integers,\n+    const Optional<py::object> &field_appearance_threshold, const Optional<py::object> &map_inference_threshold,\n+    const Optional<py::object> &maximum_sample_files, const Optional<py::object> &filename,\n+    const Optional<py::object> &hive_partitioning, const Optional<py::object> &union_by_name,\n+    const Optional<py::object> &hive_types, const Optional<py::object> &hive_types_autocast) {\n \tif (!connection) {\n \t\tthrow ConnectionException(\"Connection has already been closed\");\n \t}\n \n \tnamed_parameter_map_t options;\n \n+\tParseMultiFileReaderOptions(options, filename, hive_partitioning, union_by_name, hive_types, hive_types_autocast);\n+\n \tif (!py::none().is(columns)) {\n \t\tif (!py::is_dict_like(columns)) {\n \t\t\tthrow BinderException(\"read_json only accepts 'columns' as a dict[str, str]\");\n@@ -792,6 +849,36 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(const string &name, co\n \t\toptions[\"format\"] = Value(format_option);\n \t}\n \n+\tif (!py::none().is(date_format)) {\n+\t\tif (!py::isinstance<py::str>(date_format)) {\n+\t\t\tstring actual_type = py::str(date_format.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'date_format' as a string, not '%s'\", actual_type);\n+\t\t}\n+\t\tauto date_format_s = py::reinterpret_borrow<py::str>(date_format);\n+\t\tauto date_format_option = std::string(py::str(date_format_s));\n+\t\toptions[\"date_format\"] = Value(date_format_option);\n+\t}\n+\n+\tif (!py::none().is(timestamp_format)) {\n+\t\tif (!py::isinstance<py::str>(timestamp_format)) {\n+\t\t\tstring actual_type = py::str(timestamp_format.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'timestamp_format' as a string, not '%s'\", actual_type);\n+\t\t}\n+\t\tauto timestamp_format_s = py::reinterpret_borrow<py::str>(timestamp_format);\n+\t\tauto timestamp_format_option = std::string(py::str(timestamp_format_s));\n+\t\toptions[\"timestamp_format\"] = Value(timestamp_format_option);\n+\t}\n+\n+\tif (!py::none().is(compression)) {\n+\t\tif (!py::isinstance<py::str>(compression)) {\n+\t\t\tstring actual_type = py::str(compression.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'compression' as a string, not '%s'\", actual_type);\n+\t\t}\n+\t\tauto compression_s = py::reinterpret_borrow<py::str>(compression);\n+\t\tauto compression_option = std::string(py::str(compression_s));\n+\t\toptions[\"compression\"] = Value(compression_option);\n+\t}\n+\n \tif (!py::none().is(sample_size)) {\n \t\tif (!py::isinstance<py::int_>(sample_size)) {\n \t\t\tstring actual_type = py::str(sample_size.get_type());\n@@ -808,6 +895,64 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(const string &name, co\n \t\toptions[\"maximum_depth\"] = Value::INTEGER(py::int_(maximum_depth));\n \t}\n \n+\tif (!py::none().is(maximum_object_size)) {\n+\t\tif (!py::isinstance<py::int_>(maximum_object_size)) {\n+\t\t\tstring actual_type = py::str(maximum_object_size.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'maximum_object_size' as an unsigned integer, not '%s'\",\n+\t\t\t                      actual_type);\n+\t\t}\n+\t\tauto val = TransformPythonValue(maximum_object_size, LogicalTypeId::UINTEGER);\n+\t\toptions[\"maximum_object_size\"] = val;\n+\t}\n+\n+\tif (!py::none().is(ignore_errors)) {\n+\t\tif (!py::isinstance<py::bool_>(ignore_errors)) {\n+\t\t\tstring actual_type = py::str(ignore_errors.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'ignore_errors' as a boolean, not '%s'\", actual_type);\n+\t\t}\n+\t\tauto val = TransformPythonValue(ignore_errors, LogicalTypeId::BOOLEAN);\n+\t\toptions[\"ignore_errors\"] = val;\n+\t}\n+\n+\tif (!py::none().is(convert_strings_to_integers)) {\n+\t\tif (!py::isinstance<py::bool_>(convert_strings_to_integers)) {\n+\t\t\tstring actual_type = py::str(convert_strings_to_integers.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'convert_strings_to_integers' as a boolean, not '%s'\",\n+\t\t\t                      actual_type);\n+\t\t}\n+\t\tauto val = TransformPythonValue(convert_strings_to_integers, LogicalTypeId::BOOLEAN);\n+\t\toptions[\"convert_strings_to_integers\"] = val;\n+\t}\n+\n+\tif (!py::none().is(field_appearance_threshold)) {\n+\t\tif (!py::isinstance<py::float_>(field_appearance_threshold)) {\n+\t\t\tstring actual_type = py::str(field_appearance_threshold.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'field_appearance_threshold' as a float, not '%s'\",\n+\t\t\t                      actual_type);\n+\t\t}\n+\t\tauto val = TransformPythonValue(field_appearance_threshold, LogicalTypeId::DOUBLE);\n+\t\toptions[\"field_appearance_threshold\"] = val;\n+\t}\n+\n+\tif (!py::none().is(map_inference_threshold)) {\n+\t\tif (!py::isinstance<py::int_>(map_inference_threshold)) {\n+\t\t\tstring actual_type = py::str(map_inference_threshold.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'map_inference_threshold' as an integer, not '%s'\",\n+\t\t\t                      actual_type);\n+\t\t}\n+\t\tauto val = TransformPythonValue(map_inference_threshold, LogicalTypeId::BIGINT);\n+\t\toptions[\"map_inference_threshold\"] = val;\n+\t}\n+\n+\tif (!py::none().is(maximum_sample_files)) {\n+\t\tif (!py::isinstance<py::int_>(maximum_sample_files)) {\n+\t\t\tstring actual_type = py::str(maximum_sample_files.get_type());\n+\t\t\tthrow BinderException(\"read_json only accepts 'maximum_sample_files' as an integer, not '%s'\", actual_type);\n+\t\t}\n+\t\tauto val = TransformPythonValue(maximum_sample_files, LogicalTypeId::BIGINT);\n+\t\toptions[\"maximum_sample_files\"] = val;\n+\t}\n+\n \tbool auto_detect = false;\n \tif (!options.count(\"columns\")) {\n \t\toptions[\"auto_detect\"] = Value::BOOLEAN(true);\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/extensions/json/test_read_json.py b/tools/pythonpkg/tests/extensions/json/test_read_json.py\nindex ca46e07ab717..92ad3b66ae5d 100644\n--- a/tools/pythonpkg/tests/extensions/json/test_read_json.py\n+++ b/tools/pythonpkg/tests/extensions/json/test_read_json.py\n@@ -65,3 +65,47 @@ def test_read_json_records(self):\n         res = rel.fetchone()\n         print(res)\n         assert res == (1, 'O Brother, Where Art Thou?')\n+\n+    @pytest.mark.parametrize(\n+        'option',\n+        [\n+            ('filename', True),\n+            ('filename', 'test'),\n+            ('date_format', '%m-%d-%Y'),\n+            ('date_format', '%m-%d-%y'),\n+            ('date_format', '%d-%m-%Y'),\n+            ('date_format', '%d-%m-%y'),\n+            ('date_format', '%Y-%m-%d'),\n+            ('date_format', '%y-%m-%d'),\n+            ('timestamp_format', '%H:%M:%S%y-%m-%d'),\n+            ('compression', 'AUTO_DETECT'),\n+            ('compression', 'UNCOMPRESSED'),\n+            ('maximum_object_size', 5),\n+            ('ignore_errors', False),\n+            ('ignore_errors', True),\n+            ('convert_strings_to_integers', False),\n+            ('convert_strings_to_integers', True),\n+            ('field_appearance_threshold', 0.534),\n+            ('map_inference_threshold', 34234),\n+            ('maximum_sample_files', 5),\n+            ('hive_partitioning', True),\n+            ('hive_partitioning', False),\n+            ('union_by_name', True),\n+            ('union_by_name', False),\n+            ('hive_types_autocast', False),\n+            ('hive_types_autocast', True),\n+            ('hive_types', {'id': 'INTEGER', 'name': 'VARCHAR'}),\n+        ],\n+    )\n+    def test_read_json_options(self, duckdb_cursor, option):\n+        keyword_arguments = dict()\n+        option_name, option_value = option\n+        keyword_arguments[option_name] = option_value\n+        if option_name == 'hive_types':\n+            with pytest.raises(\n+                duckdb.InvalidInputException, match=r'Unknown hive_type: \"name\" does not appear to be a partition'\n+            ):\n+                rel = duckdb_cursor.read_json(TestFile('example.json'), **keyword_arguments)\n+        else:\n+            rel = duckdb_cursor.read_json(TestFile('example.json'), **keyword_arguments)\n+            res = rel.fetchall()\n",
  "problem_statement": "Python API suggests increasing maximum_object_size when read_json but it's unclear how to do that\n### What happens?\r\n\r\nWhen I try to read a large (93M) JSON file using read_json, it suggests increasing maximum_object_size but it's unclear how to accomplish this.\r\n\r\nCode snippet / output:\r\n```\r\nIn [3]: import duckdb\r\n\r\nIn [4]: db = duckdb.connect(\"foo.db\")\r\n\r\nIn [5]: db.read_json(\"oldbytes.space.user.feoh.json\")\r\n---------------------------------------------------------------------------\r\nInvalidInputException                     Traceback (most recent call last)\r\nCell In[5], line 1\r\n----> 1 db.read_json(\"oldbytes.space.user.feoh.json\")\r\n\r\nInvalidInputException: Invalid Input Error: \"maximum_object_size\" of 16777216 bytes exceeded while reading file \"oldbytes.space.user.feoh.json\" (>33554428 bytes).\r\n Try increasing \"maximum_object_size\".\r\n\r\nIn [6]:\r\n```\r\n\r\nTrying to pass that param as part of the reas_json call:\r\n```\r\nIn [7]: db.read_json(\"oldbytes.space.user.feoh.json\", maximum_object_size=6000000000)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[7], line 1\r\n----> 1 db.read_json(\"oldbytes.space.user.feoh.json\", maximum_object_size=6000000000)\r\n\r\nTypeError: read_json(): incompatible function arguments. The following argument types are supported:\r\n    1. (self: duckdb.duckdb.DuckDBPyConnection, name: str, *, columns: typing.Optional[object] = None, sample_size: typing.Optional[object] = None, maximum_depth: typing.Optional[object] = None, records: typing.Optional[str] = None, format: typing.Optional[str] = None) -> duckdb.duckdb.DuckDBPyRelation\r\n\r\nInvoked with: <duckdb.duckdb.DuckDBPyConnection object at 0x104e476b0>, 'oldbytes.space.user.feoh.json'; kwargs: maximum_object_size=6000000000\r\n```\r\n\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\nRun the following code with a very large JSON file:\r\nWhen I try to read a large (93M) JSON file using read_json, it suggests increasing maximum_object_size but it's unclear how to accomplish this.\r\n\r\nCode snippet / output:\r\n```\r\nIn [3]: import duckdb\r\n\r\nIn [4]: db = duckdb.connect(\"foo.db\")\r\n\r\nIn [5]: db.read_json(\"oldbytes.space.user.feoh.json\")\r\n---------------------------------------------------------------------------\r\nInvalidInputException                     Traceback (most recent call last)\r\nCell In[5], line 1\r\n----> 1 db.read_json(\"oldbytes.space.user.feoh.json\")\r\n\r\nInvalidInputException: Invalid Input Error: \"maximum_object_size\" of 16777216 bytes exceeded while reading file \"oldbytes.space.user.feoh.json\" (>33554428 bytes).\r\n Try increasing \"maximum_object_size\".\r\n\r\nIn [6]:\r\n```\r\n\r\n\r\n\r\n### OS:\r\n\r\nMacOS Sonoma 14.5, Ubuntu 24.03\r\n\r\n### DuckDB Version:\r\n\r\nv1.0.0 1f98600c2c\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nChristopher Patti\r\n\r\n### Affiliation:\r\n\r\nMIT Online Learning\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nNo - I cannot easily share my data sets due to their large size\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "",
  "created_at": "2024-06-27T11:57:21Z"
}