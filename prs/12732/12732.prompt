You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Python API suggests increasing maximum_object_size when read_json but it's unclear how to do that
### What happens?

When I try to read a large (93M) JSON file using read_json, it suggests increasing maximum_object_size but it's unclear how to accomplish this.

Code snippet / output:
```
In [3]: import duckdb

In [4]: db = duckdb.connect("foo.db")

In [5]: db.read_json("oldbytes.space.user.feoh.json")
---------------------------------------------------------------------------
InvalidInputException                     Traceback (most recent call last)
Cell In[5], line 1
----> 1 db.read_json("oldbytes.space.user.feoh.json")

InvalidInputException: Invalid Input Error: "maximum_object_size" of 16777216 bytes exceeded while reading file "oldbytes.space.user.feoh.json" (>33554428 bytes).
 Try increasing "maximum_object_size".

In [6]:
```

Trying to pass that param as part of the reas_json call:
```
In [7]: db.read_json("oldbytes.space.user.feoh.json", maximum_object_size=6000000000)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[7], line 1
----> 1 db.read_json("oldbytes.space.user.feoh.json", maximum_object_size=6000000000)

TypeError: read_json(): incompatible function arguments. The following argument types are supported:
    1. (self: duckdb.duckdb.DuckDBPyConnection, name: str, *, columns: typing.Optional[object] = None, sample_size: typing.Optional[object] = None, maximum_depth: typing.Optional[object] = None, records: typing.Optional[str] = None, format: typing.Optional[str] = None) -> duckdb.duckdb.DuckDBPyRelation

Invoked with: <duckdb.duckdb.DuckDBPyConnection object at 0x104e476b0>, 'oldbytes.space.user.feoh.json'; kwargs: maximum_object_size=6000000000
```




### To Reproduce

Run the following code with a very large JSON file:
When I try to read a large (93M) JSON file using read_json, it suggests increasing maximum_object_size but it's unclear how to accomplish this.

Code snippet / output:
```
In [3]: import duckdb

In [4]: db = duckdb.connect("foo.db")

In [5]: db.read_json("oldbytes.space.user.feoh.json")
---------------------------------------------------------------------------
InvalidInputException                     Traceback (most recent call last)
Cell In[5], line 1
----> 1 db.read_json("oldbytes.space.user.feoh.json")

InvalidInputException: Invalid Input Error: "maximum_object_size" of 16777216 bytes exceeded while reading file "oldbytes.space.user.feoh.json" (>33554428 bytes).
 Try increasing "maximum_object_size".

In [6]:
```



### OS:

MacOS Sonoma 14.5, Ubuntu 24.03

### DuckDB Version:

v1.0.0 1f98600c2c

### DuckDB Client:

Python

### Full Name:

Christopher Patti

### Affiliation:

MIT Online Learning

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot easily share my data sets due to their large size

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of .github/workflows/CodeQuality.yml]
1: name: CodeQuality
2: on:
3:   workflow_dispatch:
4:   repository_dispatch:
5:   push:
6:     branches:
7:       - '**'
8:       - '!main'
9:       - '!feature'
10:     paths-ignore:
11:       - '**.md'
12:       - '.github/patches/duckdb-wasm/**'
13:       - '.github/workflows/**'
14:       - '!.github/workflows/lcov_exclude'
15:       - '!.github/workflows/CodeQuality.yml'
16: 
17:   pull_request:
18:     types: [opened, reopened, ready_for_review]
19:     paths-ignore:
20:       - '**.md'
21:       - '.github/patches/duckdb-wasm/**'
22:       - '.github/workflows/**'
23:       - '!.github/workflows/lcov_exclude'
24:       - '!.github/workflows/CodeQuality.yml'
25: 
26: concurrency:
27:   group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/main' || github.sha }}
28:   cancel-in-progress: true
29: 
30: env:
31:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
32: 
33: jobs:
34:   format-check:
35:     name: Format Check
36:     runs-on: ubuntu-20.04
37: 
38:     env:
39:       CC: gcc-10
40:       CXX: g++-10
41:       GEN: ninja
42: 
43:     steps:
44:       - uses: actions/checkout@v3
45:         with:
46:           fetch-depth: 0
47: 
48:       - name: Install
49:         shell: bash
50:         run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build clang-format-11 && sudo pip3 install cmake-format black cxxheaderparser pcpp
51: 
52:       - name: Format Check
53:         shell: bash
54:         run: |
55:           clang-format --version
56:           clang-format --dump-config
57:           black --version
58:           make format-check-silent
59: 
60:       - name: Generated Check
61:         shell: bash
62:         run: |
63:           make generate-files
64:           git diff --exit-code
65: 
66:   enum-check:
67:     name: C Enum Integrity Check
68:     needs: format-check
69:     runs-on: ubuntu-20.04
70: 
71:     env:
72:       CC: gcc-10
73:       CXX: g++-10
74:       GEN: ninja
75: 
76:     steps:
77:     - uses: actions/checkout@v3
78:       with:
79:         fetch-depth: 0
80: 
81:     - name: Install python dependencies
82:       if: ${{ !startsWith(github.ref, 'refs/tags/v') }}
83:       shell: bash
84:       run: python -m pip install cxxheaderparser pcpp
85: 
86:     - name: Verify C enum integrity
87:       if: ${{ !startsWith(github.ref, 'refs/tags/v') }}
88:       shell: bash
89:       run: python scripts/verify_enum_integrity.py src/include/duckdb.h
90: 
91:   tidy-check:
92:     name: Tidy Check
93:     runs-on: ubuntu-22.04
94:     needs: format-check
95: 
96:     env:
97:       CC: gcc-10
98:       CXX: g++-10
99:       GEN: ninja
100:       TIDY_THREADS: 4
101: 
102:     steps:
103:       - uses: actions/checkout@v3
104:         with:
105:           fetch-depth: 0
106: 
107:       - name: Install
108:         shell: bash
109:         run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build clang-tidy && sudo pip3 install pybind11[global]
110: 
111:       - name: Setup Ccache
112:         uses: hendrikmuhs/ccache-action@main
113:         with:
114:           key: ${{ github.job }}
115:           save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}
116:       - name: Download clang-tidy-cache
117:         shell: bash
118:         run: |
119:           set -e
120:           curl -Lo /tmp/clang-tidy-cache https://github.com/ejfitzgerald/clang-tidy-cache/releases/download/v0.4.0/clang-tidy-cache-linux-amd64
121:           md5sum /tmp/clang-tidy-cache | grep 880b290d7bbe7c1fb2a4f591f9a86cc1
122:           chmod +x /tmp/clang-tidy-cache
123: 
124:       - name: Tidy Check
125:         shell: bash
126:         run: make tidy-check TIDY_BINARY=/tmp/clang-tidy-cache
[end of .github/workflows/CodeQuality.yml]
[start of tools/pythonpkg/duckdb-stubs/__init__.pyi]
1: # to regenerate this from scratch, run scripts/regenerate_python_stubs.sh .
2: # be warned - currently there are still tweaks needed after this file is
3: # generated. These should be annotated with a comment like
4: # # stubgen override
5: # to help the sanity of maintainers.
6: 
7: import duckdb.typing as typing
8: import duckdb.functional as functional
9: from duckdb.typing import DuckDBPyType
10: from duckdb.functional import FunctionNullHandling, PythonUDFType
11: from duckdb.value.constant import (
12:     Value,
13:     NullValue,
14:     BooleanValue,
15:     UnsignedBinaryValue,
16:     UnsignedShortValue,
17:     UnsignedIntegerValue,
18:     UnsignedLongValue,
19:     BinaryValue,
20:     ShortValue,
21:     IntegerValue,
22:     LongValue,
23:     HugeIntegerValue,
24:     FloatValue,
25:     DoubleValue,
26:     DecimalValue,
27:     StringValue,
28:     UUIDValue,
29:     BitValue,
30:     BlobValue,
31:     DateValue,
32:     IntervalValue,
33:     TimestampValue,
34:     TimestampSecondValue,
35:     TimestampMilisecondValue,
36:     TimestampNanosecondValue,
37:     TimestampTimeZoneValue,
38:     TimeValue,
39:     TimeTimeZoneValue,
40: )
41: 
42: # We also run this in python3.7, where this is needed
43: from typing_extensions import Literal
44: # stubgen override - missing import of Set
45: from typing import Any, ClassVar, Set, Optional, Callable
46: from io import StringIO, TextIOBase
47: 
48: from typing import overload, Dict, List, Union
49: import pandas
50: # stubgen override - unfortunately we need this for version checks
51: import sys
52: import fsspec
53: import pyarrow.lib
54: import polars
55: # stubgen override - This should probably not be exposed
56: apilevel: str
57: comment: token_type
58: default_connection: DuckDBPyConnection
59: identifier: token_type
60: keyword: token_type
61: numeric_const: token_type
62: operator: token_type
63: paramstyle: str
64: string_const: token_type
65: threadsafety: int
66: __standard_vector_size__: int
67: STANDARD: ExplainType
68: ANALYZE: ExplainType
69: DEFAULT: PythonExceptionHandling
70: RETURN_NULL: PythonExceptionHandling
71: ROWS: RenderMode
72: COLUMNS: RenderMode
73: 
74: __version__: str
75: 
76: __interactive__: bool
77: __jupyter__: bool
78: 
79: class BinderException(ProgrammingError): ...
80: 
81: class CatalogException(ProgrammingError): ...
82: 
83: class ConnectionException(OperationalError): ...
84: 
85: class ConstraintException(IntegrityError): ...
86: 
87: class ConversionException(DataError): ...
88: 
89: class DataError(Error): ...
90: 
91: class ExplainType:
92:     STANDARD: ExplainType
93:     ANALYZE: ExplainType
94:     def __int__(self) -> int: ...
95:     def __index__(self) -> int: ...
96:     @property
97:     def __members__(self) -> Dict[str, ExplainType]: ...
98:     @property
99:     def name(self) -> str: ...
100:     @property
101:     def value(self) -> int: ...
102: 
103: class RenderMode:
104:     ROWS: RenderMode
105:     COLUMNS: RenderMode
106:     def __int__(self) -> int: ...
107:     def __index__(self) -> int: ...
108:     @property
109:     def __members__(self) -> Dict[str, RenderMode]: ...
110:     @property
111:     def name(self) -> str: ...
112:     @property
113:     def value(self) -> int: ...
114: 
115: class PythonExceptionHandling:
116:     DEFAULT: PythonExceptionHandling
117:     RETURN_NULL: PythonExceptionHandling
118:     def __int__(self) -> int: ...
119:     def __index__(self) -> int: ...
120:     @property
121:     def __members__(self) -> Dict[str, PythonExceptionHandling]: ...
122:     @property
123:     def name(self) -> str: ...
124:     @property
125:     def value(self) -> int: ...
126: 
127: class ExpectedResultType:
128:     QUERY_RESULT: ExpectedResultType
129:     CHANGED_ROWS: ExpectedResultType
130:     NOTHING: ExpectedResultType
131:     def __int__(self) -> int: ...
132:     def __index__(self) -> int: ...
133:     @property
134:     def __members__(self) -> Dict[str, ExpectedResultType]: ...
135:     @property
136:     def name(self) -> str: ...
137:     @property
138:     def value(self) -> int: ...
139: 
140: class StatementType:
141:     INVALID: StatementType
142:     SELECT: StatementType
143:     INSERT: StatementType
144:     UPDATE: StatementType
145:     CREATE: StatementType
146:     DELETE: StatementType
147:     PREPARE: StatementType
148:     EXECUTE: StatementType
149:     ALTER: StatementType
150:     TRANSACTION: StatementType
151:     COPY: StatementType
152:     ANALYZE: StatementType
153:     VARIABLE_SET: StatementType
154:     CREATE_FUNC: StatementType
155:     EXPLAIN: StatementType
156:     DROP: StatementType
157:     EXPORT: StatementType
158:     PRAGMA: StatementType
159:     VACUUM: StatementType
160:     CALL: StatementType
161:     SET: StatementType
162:     LOAD: StatementType
163:     RELATION: StatementType
164:     EXTENSION: StatementType
165:     LOGICAL_PLAN: StatementType
166:     ATTACH: StatementType
167:     DETACH: StatementType
168:     MULTI: StatementType
169:     COPY_DATABASE: StatementType
170:     def __int__(self) -> int: ...
171:     def __index__(self) -> int: ...
172:     @property
173:     def __members__(self) -> Dict[str, StatementType]: ...
174:     @property
175:     def name(self) -> str: ...
176:     @property
177:     def value(self) -> int: ...
178: 
179: class Statement:
180:     def __init__(self, *args, **kwargs) -> None: ...
181:     @property
182:     def query(self) -> str: ...
183:     @property
184:     def named_parameters(self) -> Set[str]: ...
185:     @property
186:     def expected_result_type(self) -> List[ExpectedResultType]: ...
187:     @property
188:     def type(self) -> StatementType: ...
189: 
190: class Expression:
191:     def __init__(self, *args, **kwargs) -> None: ...
192:     def __neg__(self) -> "Expression": ...
193: 
194:     def __add__(self, expr: "Expression") -> "Expression": ...
195:     def __radd__(self, expr: "Expression") -> "Expression": ...
196: 
197:     def __sub__(self, expr: "Expression") -> "Expression": ...
198:     def __rsub__(self, expr: "Expression") -> "Expression": ...
199: 
200:     def __mul__(self, expr: "Expression") -> "Expression": ...
201:     def __rmul__(self, expr: "Expression") -> "Expression": ...
202: 
203:     def __div__(self, expr: "Expression") -> "Expression": ...
204:     def __rdiv__(self, expr: "Expression") -> "Expression": ...
205: 
206:     def __truediv__(self, expr: "Expression") -> "Expression": ...
207:     def __rtruediv__(self, expr: "Expression") -> "Expression": ...
208: 
209:     def __floordiv__(self, expr: "Expression") -> "Expression": ...
210:     def __rfloordiv__(self, expr: "Expression") -> "Expression": ...
211: 
212:     def __mod__(self, expr: "Expression") -> "Expression": ...
213:     def __rmod__(self, expr: "Expression") -> "Expression": ...
214: 
215:     def __pow__(self, expr: "Expression") -> "Expression": ...
216:     def __rpow__(self, expr: "Expression") -> "Expression": ...
217: 
218:     def __and__(self, expr: "Expression") -> "Expression": ...
219:     def __rand__(self, expr: "Expression") -> "Expression": ...
220:     def __or__(self, expr: "Expression") -> "Expression": ...
221:     def __ror__(self, expr: "Expression") -> "Expression": ...
222:     def __invert__(self) -> "Expression": ...
223: 
224:     def __eq__(# type: ignore[override]
225:         self, expr: "Expression") -> "Expression": ...
226:     def __ne__(# type: ignore[override]
227:         self, expr: "Expression") -> "Expression": ...
228:     def __gt__(self, expr: "Expression") -> "Expression": ...
229:     def __ge__(self, expr: "Expression") -> "Expression": ...
230:     def __lt__(self, expr: "Expression") -> "Expression": ...
231:     def __le__(self, expr: "Expression") -> "Expression": ...
232: 
233:     def show(self, max_width: Optional[int] = None, max_rows: Optional[int] = None, max_col_width: Optional[int] = None, null_value: Optional[str] = None, render_mode: Optional[RenderMode] = None) -> None: ...
234:     def __repr__(self) -> str: ...
235:     def alias(self, alias: str) -> "Expression": ...
236:     def when(self, condition: "Expression", value: "Expression") -> "Expression": ...
237:     def otherwise(self, value: "Expression") -> "Expression": ...
238:     def cast(self, type: DuckDBPyType) -> "Expression": ...
239:     def asc(self) -> "Expression": ...
240:     def desc(self) -> "Expression": ...
241:     def nulls_first(self) -> "Expression": ...
242:     def nulls_last(self) -> "Expression": ...
243:     def isnull(self) -> "Expression": ...
244:     def isnotnull(self) -> "Expression": ...
245:     def isin(self, *cols: "Expression") -> "Expression": ...
246:     def isnotin(self, *cols: "Expression") -> "Expression": ...
247: 
248: def StarExpression(exclude: Optional[List[str]]) -> Expression: ...
249: def ColumnExpression(column: str) -> Expression: ...
250: def ConstantExpression(val: Any) -> Expression: ...
251: def CaseExpression(condition: Expression, value: Expression) -> Expression: ...
252: def FunctionExpression(function: str, *cols: Expression) -> Expression: ...
253: def CoalesceOperator(*cols: Expression) -> Expression: ...
254: 
255: class DuckDBPyConnection:
256:     def __init__(self, *args, **kwargs) -> None: ...
257:     def __enter__(self) -> DuckDBPyConnection: ...
258:     def __exit__(self, exc_type: object, exc: object, traceback: object) -> None: ...
259:     def __del__(self) -> None: ...
260:     @property
261:     def description(self) -> Optional[List[Any]]: ...
262:     @property
263:     def rowcount(self) -> int: ...
264: 
265:     # NOTE: this section is generated by tools/pythonpkg/scripts/generate_connection_stubs.py.
266:     # Do not edit this section manually, your changes will be overwritten!
267: 
268:     # START OF CONNECTION METHODS
269:     def cursor(self) -> DuckDBPyConnection: ...
270:     def register_filesystem(self, filesystem: str) -> None: ...
271:     def unregister_filesystem(self, name: str) -> None: ...
272:     def list_filesystems(self) -> list: ...
273:     def filesystem_is_registered(self, name: str) -> bool: ...
274:     def create_function(self, name: str, function: function, parameters: Optional[List[DuckDBPyType]] = None, return_type: Optional[DuckDBPyType] = None, *, type: Optional[PythonUDFType] = PythonUDFType.NATIVE, null_handling: Optional[FunctionNullHandling] = FunctionNullHandling.DEFAULT, exception_handling: Optional[PythonExceptionHandling] = PythonExceptionHandling.DEFAULT, side_effects: bool = False) -> DuckDBPyConnection: ...
275:     def remove_function(self, name: str) -> DuckDBPyConnection: ...
276:     def sqltype(self, type_str: str) -> DuckDBPyType: ...
277:     def dtype(self, type_str: str) -> DuckDBPyType: ...
278:     def type(self, type_str: str) -> DuckDBPyType: ...
279:     def array_type(self, type: DuckDBPyType, size: int) -> DuckDBPyType: ...
280:     def list_type(self, type: DuckDBPyType) -> DuckDBPyType: ...
281:     def union_type(self, members: DuckDBPyType) -> DuckDBPyType: ...
282:     def string_type(self, collation: str = "") -> DuckDBPyType: ...
283:     def enum_type(self, name: str, type: DuckDBPyType, values: List[Any]) -> DuckDBPyType: ...
284:     def decimal_type(self, width: int, scale: int) -> DuckDBPyType: ...
285:     def struct_type(self, fields: Union[Dict[str, DuckDBPyType], List[str]]) -> DuckDBPyType: ...
286:     def row_type(self, fields: Union[Dict[str, DuckDBPyType], List[str]]) -> DuckDBPyType: ...
287:     def map_type(self, key: DuckDBPyType, value: DuckDBPyType) -> DuckDBPyType: ...
288:     def duplicate(self) -> DuckDBPyConnection: ...
289:     def execute(self, query: object, parameters: object = None) -> DuckDBPyConnection: ...
290:     def executemany(self, query: object, parameters: object = None) -> DuckDBPyConnection: ...
291:     def close(self) -> None: ...
292:     def interrupt(self) -> None: ...
293:     def fetchone(self) -> Optional[tuple]: ...
294:     def fetchmany(self, size: int = 1) -> List[Any]: ...
295:     def fetchall(self) -> List[Any]: ...
296:     def fetchnumpy(self) -> dict: ...
297:     def fetchdf(self, *, date_as_object: bool = False) -> pandas.DataFrame: ...
298:     def fetch_df(self, *, date_as_object: bool = False) -> pandas.DataFrame: ...
299:     def df(self, *, date_as_object: bool = False) -> pandas.DataFrame: ...
300:     def fetch_df_chunk(self, vectors_per_chunk: int = 1, *, date_as_object: bool = False) -> pandas.DataFrame: ...
301:     def pl(self, rows_per_batch: int = 1000000) -> polars.DataFrame: ...
302:     def fetch_arrow_table(self, rows_per_batch: int = 1000000) -> pyarrow.lib.Table: ...
303:     def arrow(self, rows_per_batch: int = 1000000) -> pyarrow.lib.Table: ...
304:     def fetch_record_batch(self, rows_per_batch: int = 1000000) -> pyarrow.lib.RecordBatchReader: ...
305:     def torch(self) -> dict: ...
306:     def tf(self) -> dict: ...
307:     def begin(self) -> DuckDBPyConnection: ...
308:     def commit(self) -> DuckDBPyConnection: ...
309:     def rollback(self) -> DuckDBPyConnection: ...
310:     def checkpoint(self) -> DuckDBPyConnection: ...
311:     def append(self, table_name: str, df: pandas.DataFrame, *, by_name: bool = False) -> DuckDBPyConnection: ...
312:     def register(self, view_name: str, python_object: object) -> DuckDBPyConnection: ...
313:     def unregister(self, view_name: str) -> DuckDBPyConnection: ...
314:     def table(self, table_name: str) -> DuckDBPyRelation: ...
315:     def view(self, view_name: str) -> DuckDBPyRelation: ...
316:     def values(self, values: List[Any]) -> DuckDBPyRelation: ...
317:     def table_function(self, name: str, parameters: object = None) -> DuckDBPyRelation: ...
318:     def read_json(self, name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None) -> DuckDBPyRelation: ...
319:     def extract_statements(self, query: str) -> List[Statement]: ...
320:     def sql(self, query: str, *, alias: str = "", params: object = None) -> DuckDBPyRelation: ...
321:     def query(self, query: str, *, alias: str = "", params: object = None) -> DuckDBPyRelation: ...
322:     def from_query(self, query: str, *, alias: str = "", params: object = None) -> DuckDBPyRelation: ...
323:     def read_csv(self, path_or_buffer: Union[str, StringIO, TextIOBase], *, header: Optional[bool | int] = None, compression: Optional[str] = None, sep: Optional[str] = None, delimiter: Optional[str] = None, dtype: Optional[Dict[str, str] | List[str]] = None, na_values: Optional[str| List[str]] = None, skiprows: Optional[int] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, encoding: Optional[str] = None, parallel: Optional[bool] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, sample_size: Optional[int] = None, all_varchar: Optional[bool] = None, normalize_names: Optional[bool] = None, filename: Optional[bool] = None, null_padding: Optional[bool] = None, names: Optional[List[str]] = None) -> DuckDBPyRelation: ...
324:     def from_csv_auto(self, path_or_buffer: Union[str, StringIO, TextIOBase], *, header: Optional[bool | int] = None, compression: Optional[str] = None, sep: Optional[str] = None, delimiter: Optional[str] = None, dtype: Optional[Dict[str, str] | List[str]] = None, na_values: Optional[str| List[str]] = None, skiprows: Optional[int] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, encoding: Optional[str] = None, parallel: Optional[bool] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, sample_size: Optional[int] = None, all_varchar: Optional[bool] = None, normalize_names: Optional[bool] = None, filename: Optional[bool] = None, null_padding: Optional[bool] = None, names: Optional[List[str]] = None) -> DuckDBPyRelation: ...
325:     def from_df(self, df: pandas.DataFrame) -> DuckDBPyRelation: ...
326:     def from_arrow(self, arrow_object: object) -> DuckDBPyRelation: ...
327:     def from_parquet(self, file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None) -> DuckDBPyRelation: ...
328:     def read_parquet(self, file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None) -> DuckDBPyRelation: ...
329:     def from_substrait(self, proto: str) -> DuckDBPyRelation: ...
330:     def get_substrait(self, query: str, *, enable_optimizer: bool = True) -> str: ...
331:     def get_substrait_json(self, query: str, *, enable_optimizer: bool = True) -> str: ...
332:     def from_substrait_json(self, json: str) -> DuckDBPyRelation: ...
333:     def get_table_names(self, query: str) -> List[str]: ...
334:     def install_extension(self, extension: str, *, force_install: bool = False) -> None: ...
335:     def load_extension(self, extension: str) -> None: ...
336:     # END OF CONNECTION METHODS
337: 
338: class DuckDBPyRelation:
339:     def close(self) -> None: ...
340:     def __getattr__(self, name: str) -> DuckDBPyRelation: ...
341:     def __getitem__(self, name: str) -> DuckDBPyRelation: ...
342:     def __init__(self, *args, **kwargs) -> None: ...
343:     def __contains__(self, name: str) -> bool: ...
344:     def aggregate(self, aggr_expr: str, group_expr: str = ...) -> DuckDBPyRelation: ...
345:     def apply(self, function_name: str, function_aggr: str, group_expr: str = ..., function_parameter: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
346: 
347:     def cume_dist(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
348:     def dense_rank(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
349:     def percent_rank(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
350:     def rank(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
351:     def rank_dense(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
352:     def row_number(self, window_spec: str, projected_columns: str = ...) -> DuckDBPyRelation: ...
353: 
354:     def lag(self, column: str, window_spec: str, offset: int, default_value: str, ignore_nulls: bool, projected_columns: str = ...) -> DuckDBPyRelation: ...
355:     def lead(self, column: str, window_spec: str, offset: int, default_value: str, ignore_nulls: bool, projected_columns: str = ...) -> DuckDBPyRelation: ...
356:     def nth_value(self, column: str, window_spec: str, offset: int, ignore_nulls: bool = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
357: 
358:     def value_counts(self, column: str, groups: str = ...) -> DuckDBPyRelation: ...
359:     def geomean(self, column: str, groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
360:     def first(self, column: str, groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
361:     def first_value(self, column: str, window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
362:     def last(self, column: str, groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
363:     def last_value(self, column: str, window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
364:     def mode(self, aggregation_columns: str, group_columns: str = ...) -> DuckDBPyRelation: ...
365:     def n_tile(self, window_spec: str, num_buckets: int, projected_columns: str = ...) -> DuckDBPyRelation: ...
366:     def quantile_cont(self, column: str, q: Union[float, List[float]] = ..., groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
367:     def quantile_disc(self, column: str, q: Union[float, List[float]] = ..., groups: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
368:     def sum(self, sum_aggr: str, group_expr: str = ...) -> DuckDBPyRelation: ...
369: 
370:     def any_value(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
371:     def arg_max(self, arg_column: str, value_column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
372:     def arg_min(self, arg_column: str, value_column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
373:     def avg(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
374:     def bit_and(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
375:     def bit_or(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
376:     def bit_xor(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
377:     def bitstring_agg(self, column: str, min: Optional[int], max: Optional[int], groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
378:     def bool_and(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
379:     def bool_or(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
380:     def count(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
381:     def favg(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
382:     def fsum(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
383:     def histogram(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
384:     def max(self, max_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
385:     def min(self, min_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
386:     def mean(self, mean_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
387:     def median(self, median_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
388:     def product(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
389:     def quantile(self, q: str, quantile_aggr: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
390:     def std(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
391:     def stddev(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
392:     def stddev_pop(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
393:     def stddev_samp(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
394:     def string_agg(self, column: str, sep: str = ..., groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
395:     def var(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
396:     def var_pop(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
397:     def var_samp(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
398:     def variance(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
399:     def list(self, column: str, groups: str = ..., window_spec: str = ..., projected_columns: str = ...) -> DuckDBPyRelation: ...
400: 
401:     def arrow(self, batch_size: int = ...) -> pyarrow.lib.Table: ...
402:     def create(self, table_name: str) -> None: ...
403:     def create_view(self, view_name: str, replace: bool = ...) -> DuckDBPyRelation: ...
404:     def describe(self) -> DuckDBPyRelation: ...
405:     def df(self, *args, **kwargs) -> pandas.DataFrame: ...
406:     def distinct(self) -> DuckDBPyRelation: ...
407:     def except_(self, other_rel: DuckDBPyRelation) -> DuckDBPyRelation: ...
408:     def execute(self, *args, **kwargs) -> DuckDBPyRelation: ...
409:     def explain(self, type: Optional[Literal['standard', 'analyze'] | int] = 'standard') -> str: ...
410:     def fetchall(self) -> List[Any]: ...
411:     def fetchmany(self, size: int = ...) -> List[Any]: ...
412:     def fetchnumpy(self) -> dict: ...
413:     def fetchone(self) -> Optional[tuple]: ...
414:     def fetchdf(self, *args, **kwargs) -> Any: ...
415:     def fetch_arrow_reader(self, batch_size: int = ...) -> pyarrow.lib.RecordBatchReader: ...
416:     def fetch_arrow_table(self, rows_per_batch: int = ...) -> pyarrow.lib.Table: ...
417:     def filter(self, filter_expr: Union[Expression, str]) -> DuckDBPyRelation: ...
418:     def insert(self, values: object) -> None: ...
419:     def insert_into(self, table_name: str) -> None: ...
420:     def intersect(self, other_rel: DuckDBPyRelation) -> DuckDBPyRelation: ...
421:     def join(self, other_rel: DuckDBPyRelation, condition: str, how: str = ...) -> DuckDBPyRelation: ...
422:     def limit(self, n: int, offset: int = ...) -> DuckDBPyRelation: ...
423:     def map(self, map_function: function, schema: Optional[Dict[str, DuckDBPyType]] = None) -> DuckDBPyRelation: ...
424:     def order(self, order_expr: str) -> DuckDBPyRelation: ...
425:     def sort(self, *cols: Expression) -> DuckDBPyRelation: ...
426:     def project(self, *cols: Union[str, Expression]) -> DuckDBPyRelation: ...
427:     def select(self, *cols: Union[str, Expression]) -> DuckDBPyRelation: ...
428:     def pl(self, rows_per_batch: int = ..., connection: DuckDBPyConnection = ...) -> polars.DataFrame: ...
429:     def query(self, virtual_table_name: str, sql_query: str) -> DuckDBPyRelation: ...
430:     def record_batch(self, batch_size: int = ...) -> pyarrow.lib.RecordBatchReader: ...
431:     def select_types(self, types: List[Union[str, DuckDBPyType]]) -> DuckDBPyRelation: ...
432:     def select_dtypes(self, types: List[Union[str, DuckDBPyType]]) -> DuckDBPyRelation: ...
433:     def set_alias(self, alias: str) -> DuckDBPyRelation: ...
434:     def show(self) -> None: ...
435:     def sql_query(self) -> str: ...
436:     def to_arrow_table(self, batch_size: int = ...) -> pyarrow.lib.Table: ...
437:     def to_csv(
438:             self,
439:             file_name: str,
440:             sep: Optional[str] = None,
441:             na_rep: Optional[str] = None,
442:             header: Optional[bool] = None,
443:             quotechar: Optional[str] = None,
444:             escapechar: Optional[str] = None,
445:             date_format: Optional[str] = None,
446:             timestamp_format: Optional[str] = None,
447:             quoting: Optional[str | int] = None,
448:             encoding: Optional[str] = None,
449:             compression: Optional[str] = None
450:     ) -> None: ...
451:     def to_df(self, *args, **kwargs) -> pandas.DataFrame: ...
452:     def to_parquet(
453:             self,
454:             file_name: str,
455:             compression: Optional[str] = None
456:     ) -> None: ...
457:     def fetch_df_chunk(self, vectors_per_chunk: int = 1, *, date_as_object: bool = False) -> pandas.DataFrame: ...
458:     def to_table(self, table_name: str) -> None: ...
459:     def to_view(self, view_name: str, replace: bool = ...) -> DuckDBPyRelation: ...
460:     def torch(self, connection: DuckDBPyConnection = ...) -> dict: ...
461:     def tf(self, connection: DuckDBPyConnection = ...) -> dict: ...
462:     def union(self, union_rel: DuckDBPyRelation) -> DuckDBPyRelation: ...
463:     def unique(self, unique_aggr: str) -> DuckDBPyRelation: ...
464:     def write_csv(
465:             self,
466:             file_name: str,
467:             sep: Optional[str] = None,
468:             na_rep: Optional[str] = None,
469:             header: Optional[bool] = None,
470:             quotechar: Optional[str] = None,
471:             escapechar: Optional[str] = None,
472:             date_format: Optional[str] = None,
473:             timestamp_format: Optional[str] = None,
474:             quoting: Optional[str | int] = None,
475:             encoding: Optional[str] = None,
476:             compression: Optional[str] = None,
477:             overwrite: Optional[bool] = None,
478:             per_thread_output: Optional[bool] = None,
479:             use_tmp_file: Optional[bool] = None,
480:             partition_by: Optional[List[str]] = None
481:     ) -> None: ...
482:     def write_parquet(
483:             self,
484:             file_name: str,
485:             compression: Optional[str] = None,
486:             field_ids: Optional[dict | str] = None,
487:             row_group_size_bytes: Optional[int | str] = None,
488:             row_group_size: Optional[int] = None
489:     ) -> None: ...
490:     def __len__(self) -> int: ...
491:     @property
492:     def alias(self) -> str: ...
493:     @property
494:     def columns(self) -> List[Any]: ...
495:     @property
496:     def dtypes(self) -> List[DuckDBPyType]: ...
497:     @property
498:     def description(self) -> List[Any]: ...
499:     @property
500:     def shape(self) -> tuple: ...
501:     @property
502:     def type(self) -> str: ...
503:     @property
504:     def types(self) -> List[DuckDBPyType]: ...
505: 
506: class Error(Exception): ...
507: 
508: class FatalException(Error): ...
509: 
510: class HTTPException(IOException):
511:     status_code: int
512:     body: str
513:     reason: str
514:     headers: Dict[str, str]
515: 
516: class IOException(OperationalError): ...
517: 
518: class IntegrityError(Error): ...
519: 
520: class InternalError(Error): ...
521: 
522: class InternalException(InternalError): ...
523: 
524: class InterruptException(Error): ...
525: 
526: class InvalidInputException(ProgrammingError): ...
527: 
528: class InvalidTypeException(ProgrammingError): ...
529: 
530: class NotImplementedException(NotSupportedError): ...
531: 
532: class NotSupportedError(Error): ...
533: 
534: class OperationalError(Error): ...
535: 
536: class OutOfMemoryException(OperationalError): ...
537: 
538: class OutOfRangeException(DataError): ...
539: 
540: class ParserException(ProgrammingError): ...
541: 
542: class PermissionException(Error): ...
543: 
544: class ProgrammingError(Error): ...
545: 
546: class SequenceException(Error): ...
547: 
548: class SerializationException(OperationalError): ...
549: 
550: class SyntaxException(ProgrammingError): ...
551: 
552: class TransactionException(OperationalError): ...
553: 
554: class TypeMismatchException(DataError): ...
555: 
556: class Warning(Exception): ...
557: 
558: class token_type:
559:     # stubgen override - these make mypy sad
560:     #__doc__: ClassVar[str] = ...  # read-only
561:     #__members__: ClassVar[dict] = ...  # read-only
562:     __entries: ClassVar[dict] = ...
563:     comment: ClassVar[token_type] = ...
564:     identifier: ClassVar[token_type] = ...
565:     keyword: ClassVar[token_type] = ...
566:     numeric_const: ClassVar[token_type] = ...
567:     operator: ClassVar[token_type] = ...
568:     string_const: ClassVar[token_type] = ...
569:     def __init__(self, value: int) -> None: ...
570:     def __eq__(self, other: object) -> bool: ...
571:     def __getstate__(self) -> int: ...
572:     def __hash__(self) -> int: ...
573:     # stubgen override - pybind only puts index in python >= 3.8: https://github.com/EricCousineau-TRI/pybind11/blob/54430436/include/pybind11/pybind11.h#L1789
574:     if sys.version_info >= (3, 7):
575:         def __index__(self) -> int: ...
576:     def __int__(self) -> int: ...
577:     def __ne__(self, other: object) -> bool: ...
578:     def __setstate__(self, state: int) -> None: ...
579:     @property
580:     def name(self) -> str: ...
581:     @property
582:     def value(self) -> int: ...
583:     @property
584:     # stubgen override - this gets removed by stubgen but it shouldn't
585:     def __members__(self) -> object: ...
586: 
587: def connect(database: str = ..., read_only: bool = ..., config: dict = ...) -> DuckDBPyConnection: ...
588: def tokenize(query: str) -> List[Any]: ...
589: 
590: # NOTE: this section is generated by tools/pythonpkg/scripts/generate_connection_wrapper_stubs.py.
591: # Do not edit this section manually, your changes will be overwritten!
592: 
593: # START OF CONNECTION WRAPPER
594: def cursor(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
595: def register_filesystem(filesystem: str, *, connection: DuckDBPyConnection = ...) -> None: ...
596: def unregister_filesystem(name: str, *, connection: DuckDBPyConnection = ...) -> None: ...
597: def list_filesystems(*, connection: DuckDBPyConnection = ...) -> list: ...
598: def filesystem_is_registered(name: str, *, connection: DuckDBPyConnection = ...) -> bool: ...
599: def create_function(name: str, function: function, parameters: Optional[List[DuckDBPyType]] = None, return_type: Optional[DuckDBPyType] = None, *, type: Optional[PythonUDFType] = PythonUDFType.NATIVE, null_handling: Optional[FunctionNullHandling] = FunctionNullHandling.DEFAULT, exception_handling: Optional[PythonExceptionHandling] = PythonExceptionHandling.DEFAULT, side_effects: bool = False, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
600: def remove_function(name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
601: def sqltype(type_str: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
602: def dtype(type_str: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
603: def type(type_str: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
604: def array_type(type: DuckDBPyType, size: int, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
605: def list_type(type: DuckDBPyType, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
606: def union_type(members: DuckDBPyType, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
607: def string_type(collation: str = "", *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
608: def enum_type(name: str, type: DuckDBPyType, values: List[Any], *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
609: def decimal_type(width: int, scale: int, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
610: def struct_type(fields: Union[Dict[str, DuckDBPyType], List[str]], *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
611: def row_type(fields: Union[Dict[str, DuckDBPyType], List[str]], *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
612: def map_type(key: DuckDBPyType, value: DuckDBPyType, *, connection: DuckDBPyConnection = ...) -> DuckDBPyType: ...
613: def duplicate(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
614: def execute(query: object, parameters: object = None, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
615: def executemany(query: object, parameters: object = None, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
616: def close(*, connection: DuckDBPyConnection = ...) -> None: ...
617: def interrupt(*, connection: DuckDBPyConnection = ...) -> None: ...
618: def fetchone(*, connection: DuckDBPyConnection = ...) -> Optional[tuple]: ...
619: def fetchmany(size: int = 1, *, connection: DuckDBPyConnection = ...) -> List[Any]: ...
620: def fetchall(*, connection: DuckDBPyConnection = ...) -> List[Any]: ...
621: def fetchnumpy(*, connection: DuckDBPyConnection = ...) -> dict: ...
622: def fetchdf(*, date_as_object: bool = False, connection: DuckDBPyConnection = ...) -> pandas.DataFrame: ...
623: def fetch_df(*, date_as_object: bool = False, connection: DuckDBPyConnection = ...) -> pandas.DataFrame: ...
624: def df(*, date_as_object: bool = False, connection: DuckDBPyConnection = ...) -> pandas.DataFrame: ...
625: def fetch_df_chunk(vectors_per_chunk: int = 1, *, date_as_object: bool = False, connection: DuckDBPyConnection = ...) -> pandas.DataFrame: ...
626: def pl(rows_per_batch: int = 1000000, *, connection: DuckDBPyConnection = ...) -> polars.DataFrame: ...
627: def fetch_arrow_table(rows_per_batch: int = 1000000, *, connection: DuckDBPyConnection = ...) -> pyarrow.lib.Table: ...
628: def arrow(rows_per_batch: int = 1000000, *, connection: DuckDBPyConnection = ...) -> pyarrow.lib.Table: ...
629: def fetch_record_batch(rows_per_batch: int = 1000000, *, connection: DuckDBPyConnection = ...) -> pyarrow.lib.RecordBatchReader: ...
630: def torch(*, connection: DuckDBPyConnection = ...) -> dict: ...
631: def tf(*, connection: DuckDBPyConnection = ...) -> dict: ...
632: def begin(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
633: def commit(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
634: def rollback(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
635: def checkpoint(*, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
636: def append(table_name: str, df: pandas.DataFrame, *, by_name: bool = False, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
637: def register(view_name: str, python_object: object, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
638: def unregister(view_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyConnection: ...
639: def table(table_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
640: def view(view_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
641: def values(values: List[Any], *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
642: def table_function(name: str, parameters: object = None, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
643: def read_json(name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
644: def extract_statements(query: str, *, connection: DuckDBPyConnection = ...) -> List[Statement]: ...
645: def sql(query: str, *, alias: str = "", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
646: def query(query: str, *, alias: str = "", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
647: def from_query(query: str, *, alias: str = "", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
648: def read_csv(path_or_buffer: Union[str, StringIO, TextIOBase], *, header: Optional[bool | int] = None, compression: Optional[str] = None, sep: Optional[str] = None, delimiter: Optional[str] = None, dtype: Optional[Dict[str, str] | List[str]] = None, na_values: Optional[str| List[str]] = None, skiprows: Optional[int] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, encoding: Optional[str] = None, parallel: Optional[bool] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, sample_size: Optional[int] = None, all_varchar: Optional[bool] = None, normalize_names: Optional[bool] = None, filename: Optional[bool] = None, null_padding: Optional[bool] = None, names: Optional[List[str]] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
649: def from_csv_auto(path_or_buffer: Union[str, StringIO, TextIOBase], *, header: Optional[bool | int] = None, compression: Optional[str] = None, sep: Optional[str] = None, delimiter: Optional[str] = None, dtype: Optional[Dict[str, str] | List[str]] = None, na_values: Optional[str| List[str]] = None, skiprows: Optional[int] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, encoding: Optional[str] = None, parallel: Optional[bool] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, sample_size: Optional[int] = None, all_varchar: Optional[bool] = None, normalize_names: Optional[bool] = None, filename: Optional[bool] = None, null_padding: Optional[bool] = None, names: Optional[List[str]] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
650: def from_df(df: pandas.DataFrame, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
651: def from_arrow(arrow_object: object, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
652: def from_parquet(file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
653: def read_parquet(file_glob: str, binary_as_string: bool = False, *, file_row_number: bool = False, filename: bool = False, hive_partitioning: bool = False, union_by_name: bool = False, compression: Optional[str] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
654: def from_substrait(proto: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
655: def get_substrait(query: str, *, enable_optimizer: bool = True, connection: DuckDBPyConnection = ...) -> str: ...
656: def get_substrait_json(query: str, *, enable_optimizer: bool = True, connection: DuckDBPyConnection = ...) -> str: ...
657: def from_substrait_json(json: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
658: def get_table_names(query: str, *, connection: DuckDBPyConnection = ...) -> List[str]: ...
659: def install_extension(extension: str, *, force_install: bool = False, connection: DuckDBPyConnection = ...) -> None: ...
660: def load_extension(extension: str, *, connection: DuckDBPyConnection = ...) -> None: ...
661: def project(df: pandas.DataFrame, *args: str, groups: str = "", connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
662: def distinct(df: pandas.DataFrame, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
663: def write_csv(df: pandas.DataFrame, filename: str, *, sep: Optional[str] = None, na_rep: Optional[str] = None, header: Optional[bool] = None, quotechar: Optional[str] = None, escapechar: Optional[str] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, quoting: Optional[str | int] = None, encoding: Optional[str] = None, compression: Optional[str] = None, overwrite: Optional[bool] = None, per_thread_output: Optional[bool] = None, use_tmp_file: Optional[bool] = None, partition_by: Optional[List[str]] = None, connection: DuckDBPyConnection = ...) -> None: ...
664: def aggregate(df: pandas.DataFrame, aggr_expr: str, group_expr: str = "", *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
665: def alias(df: pandas.DataFrame, alias: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
666: def filter(df: pandas.DataFrame, filter_expr: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
667: def limit(df: pandas.DataFrame, n: int, offset: int = 0, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
668: def order(df: pandas.DataFrame, order_expr: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
669: def query_df(df: pandas.DataFrame, virtual_table_name: str, sql_query: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
670: def description(*, connection: DuckDBPyConnection = ...) -> Optional[List[Any]]: ...
671: def rowcount(*, connection: DuckDBPyConnection = ...) -> int: ...
672: # END OF CONNECTION WRAPPER
[end of tools/pythonpkg/duckdb-stubs/__init__.pyi]
[start of tools/pythonpkg/duckdb_python.cpp]
1: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
2: 
3: #include "duckdb/common/atomic.hpp"
4: #include "duckdb/common/vector.hpp"
5: #include "duckdb/parser/parser.hpp"
6: 
7: #include "duckdb_python/python_objects.hpp"
8: #include "duckdb_python/pyconnection/pyconnection.hpp"
9: #include "duckdb_python/pystatement.hpp"
10: #include "duckdb_python/pyrelation.hpp"
11: #include "duckdb_python/expression/pyexpression.hpp"
12: #include "duckdb_python/pyresult.hpp"
13: #include "duckdb_python/pybind11/exceptions.hpp"
14: #include "duckdb_python/typing.hpp"
15: #include "duckdb_python/functional.hpp"
16: #include "duckdb_python/pybind11/conversions/pyconnection_default.hpp"
17: #include "duckdb/common/box_renderer.hpp"
18: #include "duckdb/function/function.hpp"
19: #include "duckdb_python/pybind11/conversions/exception_handling_enum.hpp"
20: #include "duckdb_python/pybind11/conversions/python_udf_type_enum.hpp"
21: #include "duckdb/common/enums/statement_type.hpp"
22: 
23: #include "duckdb.hpp"
24: 
25: #ifndef DUCKDB_PYTHON_LIB_NAME
26: #define DUCKDB_PYTHON_LIB_NAME duckdb
27: #endif
28: 
29: namespace py = pybind11;
30: 
31: namespace duckdb {
32: 
33: enum PySQLTokenType : uint8_t {
34: 	PY_SQL_TOKEN_IDENTIFIER = 0,
35: 	PY_SQL_TOKEN_NUMERIC_CONSTANT,
36: 	PY_SQL_TOKEN_STRING_CONSTANT,
37: 	PY_SQL_TOKEN_OPERATOR,
38: 	PY_SQL_TOKEN_KEYWORD,
39: 	PY_SQL_TOKEN_COMMENT
40: };
41: 
42: static py::list PyTokenize(const string &query) {
43: 	auto tokens = Parser::Tokenize(query);
44: 	py::list result;
45: 	for (auto &token : tokens) {
46: 		auto tuple = py::tuple(2);
47: 		tuple[0] = token.start;
48: 		switch (token.type) {
49: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_IDENTIFIER:
50: 			tuple[1] = PY_SQL_TOKEN_IDENTIFIER;
51: 			break;
52: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_NUMERIC_CONSTANT:
53: 			tuple[1] = PY_SQL_TOKEN_NUMERIC_CONSTANT;
54: 			break;
55: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_STRING_CONSTANT:
56: 			tuple[1] = PY_SQL_TOKEN_STRING_CONSTANT;
57: 			break;
58: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_OPERATOR:
59: 			tuple[1] = PY_SQL_TOKEN_OPERATOR;
60: 			break;
61: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_KEYWORD:
62: 			tuple[1] = PY_SQL_TOKEN_KEYWORD;
63: 			break;
64: 		case SimplifiedTokenType::SIMPLIFIED_TOKEN_COMMENT:
65: 			tuple[1] = PY_SQL_TOKEN_COMMENT;
66: 			break;
67: 		}
68: 		result.append(tuple);
69: 	}
70: 	return result;
71: }
72: 
73: static void InitializeConnectionMethods(py::module_ &m) {
74: 
75: 	// START_OF_CONNECTION_METHODS
76: 	m.def(
77: 	    "cursor",
78: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
79: 		    if (!conn) {
80: 			    conn = DuckDBPyConnection::DefaultConnection();
81: 		    }
82: 		    return conn->Cursor();
83: 	    },
84: 	    "Create a duplicate of the current connection", py::kw_only(), py::arg("connection") = py::none());
85: 	m.def(
86: 	    "register_filesystem",
87: 	    [](AbstractFileSystem filesystem, shared_ptr<DuckDBPyConnection> conn = nullptr) {
88: 		    if (!conn) {
89: 			    conn = DuckDBPyConnection::DefaultConnection();
90: 		    }
91: 		    conn->RegisterFilesystem(filesystem);
92: 	    },
93: 	    "Register a fsspec compliant filesystem", py::arg("filesystem"), py::kw_only(),
94: 	    py::arg("connection") = py::none());
95: 	m.def(
96: 	    "unregister_filesystem",
97: 	    [](const py::str &name, shared_ptr<DuckDBPyConnection> conn = nullptr) {
98: 		    if (!conn) {
99: 			    conn = DuckDBPyConnection::DefaultConnection();
100: 		    }
101: 		    conn->UnregisterFilesystem(name);
102: 	    },
103: 	    "Unregister a filesystem", py::arg("name"), py::kw_only(), py::arg("connection") = py::none());
104: 	m.def(
105: 	    "list_filesystems",
106: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
107: 		    if (!conn) {
108: 			    conn = DuckDBPyConnection::DefaultConnection();
109: 		    }
110: 		    return conn->ListFilesystems();
111: 	    },
112: 	    "List registered filesystems, including builtin ones", py::kw_only(), py::arg("connection") = py::none());
113: 	m.def(
114: 	    "filesystem_is_registered",
115: 	    [](const string &name, shared_ptr<DuckDBPyConnection> conn = nullptr) {
116: 		    if (!conn) {
117: 			    conn = DuckDBPyConnection::DefaultConnection();
118: 		    }
119: 		    return conn->FileSystemIsRegistered(name);
120: 	    },
121: 	    "Check if a filesystem with the provided name is currently registered", py::arg("name"), py::kw_only(),
122: 	    py::arg("connection") = py::none());
123: 	m.def(
124: 	    "create_function",
125: 	    [](const string &name, const py::function &udf, const py::object &arguments = py::none(),
126: 	       const shared_ptr<DuckDBPyType> &return_type = nullptr, PythonUDFType type = PythonUDFType::NATIVE,
127: 	       FunctionNullHandling null_handling = FunctionNullHandling::DEFAULT_NULL_HANDLING,
128: 	       PythonExceptionHandling exception_handling = PythonExceptionHandling::FORWARD_ERROR,
129: 	       bool side_effects = false, shared_ptr<DuckDBPyConnection> conn = nullptr) {
130: 		    if (!conn) {
131: 			    conn = DuckDBPyConnection::DefaultConnection();
132: 		    }
133: 		    return conn->RegisterScalarUDF(name, udf, arguments, return_type, type, null_handling, exception_handling,
134: 		                                   side_effects);
135: 	    },
136: 	    "Create a DuckDB function out of the passing in Python function so it can be used in queries", py::arg("name"),
137: 	    py::arg("function"), py::arg("parameters") = py::none(), py::arg("return_type") = py::none(), py::kw_only(),
138: 	    py::arg("type") = PythonUDFType::NATIVE, py::arg("null_handling") = FunctionNullHandling::DEFAULT_NULL_HANDLING,
139: 	    py::arg("exception_handling") = PythonExceptionHandling::FORWARD_ERROR, py::arg("side_effects") = false,
140: 	    py::arg("connection") = py::none());
141: 	m.def(
142: 	    "remove_function",
143: 	    [](const string &name, shared_ptr<DuckDBPyConnection> conn = nullptr) {
144: 		    if (!conn) {
145: 			    conn = DuckDBPyConnection::DefaultConnection();
146: 		    }
147: 		    return conn->UnregisterUDF(name);
148: 	    },
149: 	    "Remove a previously created function", py::arg("name"), py::kw_only(), py::arg("connection") = py::none());
150: 	m.def(
151: 	    "sqltype",
152: 	    [](const string &type_str, shared_ptr<DuckDBPyConnection> conn = nullptr) {
153: 		    if (!conn) {
154: 			    conn = DuckDBPyConnection::DefaultConnection();
155: 		    }
156: 		    return conn->Type(type_str);
157: 	    },
158: 	    "Create a type object by parsing the 'type_str' string", py::arg("type_str"), py::kw_only(),
159: 	    py::arg("connection") = py::none());
160: 	m.def(
161: 	    "dtype",
162: 	    [](const string &type_str, shared_ptr<DuckDBPyConnection> conn = nullptr) {
163: 		    if (!conn) {
164: 			    conn = DuckDBPyConnection::DefaultConnection();
165: 		    }
166: 		    return conn->Type(type_str);
167: 	    },
168: 	    "Create a type object by parsing the 'type_str' string", py::arg("type_str"), py::kw_only(),
169: 	    py::arg("connection") = py::none());
170: 	m.def(
171: 	    "type",
172: 	    [](const string &type_str, shared_ptr<DuckDBPyConnection> conn = nullptr) {
173: 		    if (!conn) {
174: 			    conn = DuckDBPyConnection::DefaultConnection();
175: 		    }
176: 		    return conn->Type(type_str);
177: 	    },
178: 	    "Create a type object by parsing the 'type_str' string", py::arg("type_str"), py::kw_only(),
179: 	    py::arg("connection") = py::none());
180: 	m.def(
181: 	    "array_type",
182: 	    [](const shared_ptr<DuckDBPyType> &type, idx_t size, shared_ptr<DuckDBPyConnection> conn = nullptr) {
183: 		    if (!conn) {
184: 			    conn = DuckDBPyConnection::DefaultConnection();
185: 		    }
186: 		    return conn->ArrayType(type, size);
187: 	    },
188: 	    "Create an array type object of 'type'", py::arg("type").none(false), py::arg("size"), py::kw_only(),
189: 	    py::arg("connection") = py::none());
190: 	m.def(
191: 	    "list_type",
192: 	    [](const shared_ptr<DuckDBPyType> &type, shared_ptr<DuckDBPyConnection> conn = nullptr) {
193: 		    if (!conn) {
194: 			    conn = DuckDBPyConnection::DefaultConnection();
195: 		    }
196: 		    return conn->ListType(type);
197: 	    },
198: 	    "Create a list type object of 'type'", py::arg("type").none(false), py::kw_only(),
199: 	    py::arg("connection") = py::none());
200: 	m.def(
201: 	    "union_type",
202: 	    [](const py::object &members, shared_ptr<DuckDBPyConnection> conn = nullptr) {
203: 		    if (!conn) {
204: 			    conn = DuckDBPyConnection::DefaultConnection();
205: 		    }
206: 		    return conn->UnionType(members);
207: 	    },
208: 	    "Create a union type object from 'members'", py::arg("members").none(false), py::kw_only(),
209: 	    py::arg("connection") = py::none());
210: 	m.def(
211: 	    "string_type",
212: 	    [](const string &collation = string(), shared_ptr<DuckDBPyConnection> conn = nullptr) {
213: 		    if (!conn) {
214: 			    conn = DuckDBPyConnection::DefaultConnection();
215: 		    }
216: 		    return conn->StringType(collation);
217: 	    },
218: 	    "Create a string type with an optional collation", py::arg("collation") = "", py::kw_only(),
219: 	    py::arg("connection") = py::none());
220: 	m.def(
221: 	    "enum_type",
222: 	    [](const string &name, const shared_ptr<DuckDBPyType> &type, const py::list &values_p,
223: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
224: 		    if (!conn) {
225: 			    conn = DuckDBPyConnection::DefaultConnection();
226: 		    }
227: 		    return conn->EnumType(name, type, values_p);
228: 	    },
229: 	    "Create an enum type of underlying 'type', consisting of the list of 'values'", py::arg("name"),
230: 	    py::arg("type"), py::arg("values"), py::kw_only(), py::arg("connection") = py::none());
231: 	m.def(
232: 	    "decimal_type",
233: 	    [](int width, int scale, shared_ptr<DuckDBPyConnection> conn = nullptr) {
234: 		    if (!conn) {
235: 			    conn = DuckDBPyConnection::DefaultConnection();
236: 		    }
237: 		    return conn->DecimalType(width, scale);
238: 	    },
239: 	    "Create a decimal type with 'width' and 'scale'", py::arg("width"), py::arg("scale"), py::kw_only(),
240: 	    py::arg("connection") = py::none());
241: 	m.def(
242: 	    "struct_type",
243: 	    [](const py::object &fields, shared_ptr<DuckDBPyConnection> conn = nullptr) {
244: 		    if (!conn) {
245: 			    conn = DuckDBPyConnection::DefaultConnection();
246: 		    }
247: 		    return conn->StructType(fields);
248: 	    },
249: 	    "Create a struct type object from 'fields'", py::arg("fields"), py::kw_only(),
250: 	    py::arg("connection") = py::none());
251: 	m.def(
252: 	    "row_type",
253: 	    [](const py::object &fields, shared_ptr<DuckDBPyConnection> conn = nullptr) {
254: 		    if (!conn) {
255: 			    conn = DuckDBPyConnection::DefaultConnection();
256: 		    }
257: 		    return conn->StructType(fields);
258: 	    },
259: 	    "Create a struct type object from 'fields'", py::arg("fields"), py::kw_only(),
260: 	    py::arg("connection") = py::none());
261: 	m.def(
262: 	    "map_type",
263: 	    [](const shared_ptr<DuckDBPyType> &key_type, const shared_ptr<DuckDBPyType> &value_type,
264: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
265: 		    if (!conn) {
266: 			    conn = DuckDBPyConnection::DefaultConnection();
267: 		    }
268: 		    return conn->MapType(key_type, value_type);
269: 	    },
270: 	    "Create a map type object from 'key_type' and 'value_type'", py::arg("key").none(false),
271: 	    py::arg("value").none(false), py::kw_only(), py::arg("connection") = py::none());
272: 	m.def(
273: 	    "duplicate",
274: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
275: 		    if (!conn) {
276: 			    conn = DuckDBPyConnection::DefaultConnection();
277: 		    }
278: 		    return conn->Cursor();
279: 	    },
280: 	    "Create a duplicate of the current connection", py::kw_only(), py::arg("connection") = py::none());
281: 	m.def(
282: 	    "execute",
283: 	    [](const py::object &query, py::object params = py::list(), shared_ptr<DuckDBPyConnection> conn = nullptr) {
284: 		    if (!conn) {
285: 			    conn = DuckDBPyConnection::DefaultConnection();
286: 		    }
287: 		    return conn->Execute(query, params);
288: 	    },
289: 	    "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
290: 	    py::arg("parameters") = py::none(), py::kw_only(), py::arg("connection") = py::none());
291: 	m.def(
292: 	    "executemany",
293: 	    [](const py::object &query, py::object params = py::list(), shared_ptr<DuckDBPyConnection> conn = nullptr) {
294: 		    if (!conn) {
295: 			    conn = DuckDBPyConnection::DefaultConnection();
296: 		    }
297: 		    return conn->ExecuteMany(query, params);
298: 	    },
299: 	    "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
300: 	    py::arg("query"), py::arg("parameters") = py::none(), py::kw_only(), py::arg("connection") = py::none());
301: 	m.def(
302: 	    "close",
303: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
304: 		    if (!conn) {
305: 			    conn = DuckDBPyConnection::DefaultConnection();
306: 		    }
307: 		    conn->Close();
308: 	    },
309: 	    "Close the connection", py::kw_only(), py::arg("connection") = py::none());
310: 	m.def(
311: 	    "interrupt",
312: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
313: 		    if (!conn) {
314: 			    conn = DuckDBPyConnection::DefaultConnection();
315: 		    }
316: 		    conn->Interrupt();
317: 	    },
318: 	    "Interrupt pending operations", py::kw_only(), py::arg("connection") = py::none());
319: 	m.def(
320: 	    "fetchone",
321: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
322: 		    if (!conn) {
323: 			    conn = DuckDBPyConnection::DefaultConnection();
324: 		    }
325: 		    return conn->FetchOne();
326: 	    },
327: 	    "Fetch a single row from a result following execute", py::kw_only(), py::arg("connection") = py::none());
328: 	m.def(
329: 	    "fetchmany",
330: 	    [](idx_t size, shared_ptr<DuckDBPyConnection> conn = nullptr) {
331: 		    if (!conn) {
332: 			    conn = DuckDBPyConnection::DefaultConnection();
333: 		    }
334: 		    return conn->FetchMany(size);
335: 	    },
336: 	    "Fetch the next set of rows from a result following execute", py::arg("size") = 1, py::kw_only(),
337: 	    py::arg("connection") = py::none());
338: 	m.def(
339: 	    "fetchall",
340: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
341: 		    if (!conn) {
342: 			    conn = DuckDBPyConnection::DefaultConnection();
343: 		    }
344: 		    return conn->FetchAll();
345: 	    },
346: 	    "Fetch all rows from a result following execute", py::kw_only(), py::arg("connection") = py::none());
347: 	m.def(
348: 	    "fetchnumpy",
349: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
350: 		    if (!conn) {
351: 			    conn = DuckDBPyConnection::DefaultConnection();
352: 		    }
353: 		    return conn->FetchNumpy();
354: 	    },
355: 	    "Fetch a result as list of NumPy arrays following execute", py::kw_only(), py::arg("connection") = py::none());
356: 	m.def(
357: 	    "fetchdf",
358: 	    [](bool date_as_object, shared_ptr<DuckDBPyConnection> conn = nullptr) {
359: 		    if (!conn) {
360: 			    conn = DuckDBPyConnection::DefaultConnection();
361: 		    }
362: 		    return conn->FetchDF(date_as_object);
363: 	    },
364: 	    "Fetch a result as DataFrame following execute()", py::kw_only(), py::arg("date_as_object") = false,
365: 	    py::arg("connection") = py::none());
366: 	m.def(
367: 	    "fetch_df",
368: 	    [](bool date_as_object, shared_ptr<DuckDBPyConnection> conn = nullptr) {
369: 		    if (!conn) {
370: 			    conn = DuckDBPyConnection::DefaultConnection();
371: 		    }
372: 		    return conn->FetchDF(date_as_object);
373: 	    },
374: 	    "Fetch a result as DataFrame following execute()", py::kw_only(), py::arg("date_as_object") = false,
375: 	    py::arg("connection") = py::none());
376: 	m.def(
377: 	    "df",
378: 	    [](bool date_as_object, shared_ptr<DuckDBPyConnection> conn = nullptr) {
379: 		    if (!conn) {
380: 			    conn = DuckDBPyConnection::DefaultConnection();
381: 		    }
382: 		    return conn->FetchDF(date_as_object);
383: 	    },
384: 	    "Fetch a result as DataFrame following execute()", py::kw_only(), py::arg("date_as_object") = false,
385: 	    py::arg("connection") = py::none());
386: 	m.def(
387: 	    "fetch_df_chunk",
388: 	    [](const idx_t vectors_per_chunk = 1, bool date_as_object = false,
389: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
390: 		    if (!conn) {
391: 			    conn = DuckDBPyConnection::DefaultConnection();
392: 		    }
393: 		    return conn->FetchDFChunk(vectors_per_chunk, date_as_object);
394: 	    },
395: 	    "Fetch a chunk of the result as DataFrame following execute()", py::arg("vectors_per_chunk") = 1, py::kw_only(),
396: 	    py::arg("date_as_object") = false, py::arg("connection") = py::none());
397: 	m.def(
398: 	    "pl",
399: 	    [](idx_t rows_per_batch, shared_ptr<DuckDBPyConnection> conn = nullptr) {
400: 		    if (!conn) {
401: 			    conn = DuckDBPyConnection::DefaultConnection();
402: 		    }
403: 		    return conn->FetchPolars(rows_per_batch);
404: 	    },
405: 	    "Fetch a result as Polars DataFrame following execute()", py::arg("rows_per_batch") = 1000000, py::kw_only(),
406: 	    py::arg("connection") = py::none());
407: 	m.def(
408: 	    "fetch_arrow_table",
409: 	    [](idx_t rows_per_batch, shared_ptr<DuckDBPyConnection> conn = nullptr) {
410: 		    if (!conn) {
411: 			    conn = DuckDBPyConnection::DefaultConnection();
412: 		    }
413: 		    return conn->FetchArrow(rows_per_batch);
414: 	    },
415: 	    "Fetch a result as Arrow table following execute()", py::arg("rows_per_batch") = 1000000, py::kw_only(),
416: 	    py::arg("connection") = py::none());
417: 	m.def(
418: 	    "arrow",
419: 	    [](idx_t rows_per_batch, shared_ptr<DuckDBPyConnection> conn = nullptr) {
420: 		    if (!conn) {
421: 			    conn = DuckDBPyConnection::DefaultConnection();
422: 		    }
423: 		    return conn->FetchArrow(rows_per_batch);
424: 	    },
425: 	    "Fetch a result as Arrow table following execute()", py::arg("rows_per_batch") = 1000000, py::kw_only(),
426: 	    py::arg("connection") = py::none());
427: 	m.def(
428: 	    "fetch_record_batch",
429: 	    [](const idx_t rows_per_batch, shared_ptr<DuckDBPyConnection> conn = nullptr) {
430: 		    if (!conn) {
431: 			    conn = DuckDBPyConnection::DefaultConnection();
432: 		    }
433: 		    return conn->FetchRecordBatchReader(rows_per_batch);
434: 	    },
435: 	    "Fetch an Arrow RecordBatchReader following execute()", py::arg("rows_per_batch") = 1000000, py::kw_only(),
436: 	    py::arg("connection") = py::none());
437: 	m.def(
438: 	    "torch",
439: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
440: 		    if (!conn) {
441: 			    conn = DuckDBPyConnection::DefaultConnection();
442: 		    }
443: 		    return conn->FetchPyTorch();
444: 	    },
445: 	    "Fetch a result as dict of PyTorch Tensors following execute()", py::kw_only(),
446: 	    py::arg("connection") = py::none());
447: 	m.def(
448: 	    "tf",
449: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
450: 		    if (!conn) {
451: 			    conn = DuckDBPyConnection::DefaultConnection();
452: 		    }
453: 		    return conn->FetchTF();
454: 	    },
455: 	    "Fetch a result as dict of TensorFlow Tensors following execute()", py::kw_only(),
456: 	    py::arg("connection") = py::none());
457: 	m.def(
458: 	    "begin",
459: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
460: 		    if (!conn) {
461: 			    conn = DuckDBPyConnection::DefaultConnection();
462: 		    }
463: 		    return conn->Begin();
464: 	    },
465: 	    "Start a new transaction", py::kw_only(), py::arg("connection") = py::none());
466: 	m.def(
467: 	    "commit",
468: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
469: 		    if (!conn) {
470: 			    conn = DuckDBPyConnection::DefaultConnection();
471: 		    }
472: 		    return conn->Commit();
473: 	    },
474: 	    "Commit changes performed within a transaction", py::kw_only(), py::arg("connection") = py::none());
475: 	m.def(
476: 	    "rollback",
477: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
478: 		    if (!conn) {
479: 			    conn = DuckDBPyConnection::DefaultConnection();
480: 		    }
481: 		    return conn->Rollback();
482: 	    },
483: 	    "Roll back changes performed within a transaction", py::kw_only(), py::arg("connection") = py::none());
484: 	m.def(
485: 	    "checkpoint",
486: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
487: 		    if (!conn) {
488: 			    conn = DuckDBPyConnection::DefaultConnection();
489: 		    }
490: 		    return conn->Checkpoint();
491: 	    },
492: 	    "Synchronizes data in the write-ahead log (WAL) to the database data file (no-op for in-memory connections)",
493: 	    py::kw_only(), py::arg("connection") = py::none());
494: 	m.def(
495: 	    "append",
496: 	    [](const string &name, const PandasDataFrame &value, bool by_name,
497: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
498: 		    if (!conn) {
499: 			    conn = DuckDBPyConnection::DefaultConnection();
500: 		    }
501: 		    return conn->Append(name, value, by_name);
502: 	    },
503: 	    "Append the passed DataFrame to the named table", py::arg("table_name"), py::arg("df"), py::kw_only(),
504: 	    py::arg("by_name") = false, py::arg("connection") = py::none());
505: 	m.def(
506: 	    "register",
507: 	    [](const string &name, const py::object &python_object, shared_ptr<DuckDBPyConnection> conn = nullptr) {
508: 		    if (!conn) {
509: 			    conn = DuckDBPyConnection::DefaultConnection();
510: 		    }
511: 		    return conn->RegisterPythonObject(name, python_object);
512: 	    },
513: 	    "Register the passed Python Object value for querying with a view", py::arg("view_name"),
514: 	    py::arg("python_object"), py::kw_only(), py::arg("connection") = py::none());
515: 	m.def(
516: 	    "unregister",
517: 	    [](const string &name, shared_ptr<DuckDBPyConnection> conn = nullptr) {
518: 		    if (!conn) {
519: 			    conn = DuckDBPyConnection::DefaultConnection();
520: 		    }
521: 		    return conn->UnregisterPythonObject(name);
522: 	    },
523: 	    "Unregister the view name", py::arg("view_name"), py::kw_only(), py::arg("connection") = py::none());
524: 	m.def(
525: 	    "table",
526: 	    [](const string &tname, shared_ptr<DuckDBPyConnection> conn = nullptr) {
527: 		    if (!conn) {
528: 			    conn = DuckDBPyConnection::DefaultConnection();
529: 		    }
530: 		    return conn->Table(tname);
531: 	    },
532: 	    "Create a relation object for the named table", py::arg("table_name"), py::kw_only(),
533: 	    py::arg("connection") = py::none());
534: 	m.def(
535: 	    "view",
536: 	    [](const string &vname, shared_ptr<DuckDBPyConnection> conn = nullptr) {
537: 		    if (!conn) {
538: 			    conn = DuckDBPyConnection::DefaultConnection();
539: 		    }
540: 		    return conn->View(vname);
541: 	    },
542: 	    "Create a relation object for the named view", py::arg("view_name"), py::kw_only(),
543: 	    py::arg("connection") = py::none());
544: 	m.def(
545: 	    "values",
546: 	    [](py::object params = py::none(), shared_ptr<DuckDBPyConnection> conn = nullptr) {
547: 		    if (!conn) {
548: 			    conn = DuckDBPyConnection::DefaultConnection();
549: 		    }
550: 		    return conn->Values(params);
551: 	    },
552: 	    "Create a relation object from the passed values", py::arg("values"), py::kw_only(),
553: 	    py::arg("connection") = py::none());
554: 	m.def(
555: 	    "table_function",
556: 	    [](const string &fname, py::object params = py::list(), shared_ptr<DuckDBPyConnection> conn = nullptr) {
557: 		    if (!conn) {
558: 			    conn = DuckDBPyConnection::DefaultConnection();
559: 		    }
560: 		    return conn->TableFunction(fname, params);
561: 	    },
562: 	    "Create a relation object from the named table function with given parameters", py::arg("name"),
563: 	    py::arg("parameters") = py::none(), py::kw_only(), py::arg("connection") = py::none());
564: 	m.def(
565: 	    "read_json",
566: 	    [](const string &filename, const Optional<py::object> &columns = py::none(),
567: 	       const Optional<py::object> &sample_size = py::none(), const Optional<py::object> &maximum_depth = py::none(),
568: 	       const Optional<py::str> &records = py::none(), const Optional<py::str> &format = py::none(),
569: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
570: 		    if (!conn) {
571: 			    conn = DuckDBPyConnection::DefaultConnection();
572: 		    }
573: 		    return conn->ReadJSON(filename, columns, sample_size, maximum_depth, records, format);
574: 	    },
575: 	    "Create a relation object from the JSON file in 'name'", py::arg("name"), py::kw_only(),
576: 	    py::arg("columns") = py::none(), py::arg("sample_size") = py::none(), py::arg("maximum_depth") = py::none(),
577: 	    py::arg("records") = py::none(), py::arg("format") = py::none(), py::arg("connection") = py::none());
578: 	m.def(
579: 	    "extract_statements",
580: 	    [](const string &query, shared_ptr<DuckDBPyConnection> conn = nullptr) {
581: 		    if (!conn) {
582: 			    conn = DuckDBPyConnection::DefaultConnection();
583: 		    }
584: 		    return conn->ExtractStatements(query);
585: 	    },
586: 	    "Parse the query string and extract the Statement object(s) produced", py::arg("query"), py::kw_only(),
587: 	    py::arg("connection") = py::none());
588: 	m.def(
589: 	    "sql",
590: 	    [](const py::object &query, string alias = "", py::object params = py::list(),
591: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
592: 		    if (!conn) {
593: 			    conn = DuckDBPyConnection::DefaultConnection();
594: 		    }
595: 		    return conn->RunQuery(query, alias, params);
596: 	    },
597: 	    "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
598: 	    "run the query as-is.",
599: 	    py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none(),
600: 	    py::arg("connection") = py::none());
601: 	m.def(
602: 	    "query",
603: 	    [](const py::object &query, string alias = "", py::object params = py::list(),
604: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
605: 		    if (!conn) {
606: 			    conn = DuckDBPyConnection::DefaultConnection();
607: 		    }
608: 		    return conn->RunQuery(query, alias, params);
609: 	    },
610: 	    "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
611: 	    "run the query as-is.",
612: 	    py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none(),
613: 	    py::arg("connection") = py::none());
614: 	m.def(
615: 	    "from_query",
616: 	    [](const py::object &query, string alias = "", py::object params = py::list(),
617: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
618: 		    if (!conn) {
619: 			    conn = DuckDBPyConnection::DefaultConnection();
620: 		    }
621: 		    return conn->RunQuery(query, alias, params);
622: 	    },
623: 	    "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
624: 	    "run the query as-is.",
625: 	    py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none(),
626: 	    py::arg("connection") = py::none());
627: 	m.def(
628: 	    "read_csv",
629: 	    [](const py::object &name, const py::object &header = py::none(), const py::object &compression = py::none(),
630: 	       const py::object &sep = py::none(), const py::object &delimiter = py::none(),
631: 	       const py::object &dtype = py::none(), const py::object &na_values = py::none(),
632: 	       const py::object &skiprows = py::none(), const py::object &quotechar = py::none(),
633: 	       const py::object &escapechar = py::none(), const py::object &encoding = py::none(),
634: 	       const py::object &parallel = py::none(), const py::object &date_format = py::none(),
635: 	       const py::object &timestamp_format = py::none(), const py::object &sample_size = py::none(),
636: 	       const py::object &all_varchar = py::none(), const py::object &normalize_names = py::none(),
637: 	       const py::object &filename = py::none(), const py::object &null_padding = py::none(),
638: 	       const py::object &names = py::none(), shared_ptr<DuckDBPyConnection> conn = nullptr) {
639: 		    if (!conn) {
640: 			    conn = DuckDBPyConnection::DefaultConnection();
641: 		    }
642: 		    return conn->ReadCSV(name, header, compression, sep, delimiter, dtype, na_values, skiprows, quotechar,
643: 		                         escapechar, encoding, parallel, date_format, timestamp_format, sample_size,
644: 		                         all_varchar, normalize_names, filename, null_padding, names);
645: 	    },
646: 	    "Create a relation object from the CSV file in 'name'", py::arg("path_or_buffer"), py::kw_only(),
647: 	    py::arg("header") = py::none(), py::arg("compression") = py::none(), py::arg("sep") = py::none(),
648: 	    py::arg("delimiter") = py::none(), py::arg("dtype") = py::none(), py::arg("na_values") = py::none(),
649: 	    py::arg("skiprows") = py::none(), py::arg("quotechar") = py::none(), py::arg("escapechar") = py::none(),
650: 	    py::arg("encoding") = py::none(), py::arg("parallel") = py::none(), py::arg("date_format") = py::none(),
651: 	    py::arg("timestamp_format") = py::none(), py::arg("sample_size") = py::none(),
652: 	    py::arg("all_varchar") = py::none(), py::arg("normalize_names") = py::none(), py::arg("filename") = py::none(),
653: 	    py::arg("null_padding") = py::none(), py::arg("names") = py::none(), py::arg("connection") = py::none());
654: 	m.def(
655: 	    "from_csv_auto",
656: 	    [](const py::object &name, const py::object &header = py::none(), const py::object &compression = py::none(),
657: 	       const py::object &sep = py::none(), const py::object &delimiter = py::none(),
658: 	       const py::object &dtype = py::none(), const py::object &na_values = py::none(),
659: 	       const py::object &skiprows = py::none(), const py::object &quotechar = py::none(),
660: 	       const py::object &escapechar = py::none(), const py::object &encoding = py::none(),
661: 	       const py::object &parallel = py::none(), const py::object &date_format = py::none(),
662: 	       const py::object &timestamp_format = py::none(), const py::object &sample_size = py::none(),
663: 	       const py::object &all_varchar = py::none(), const py::object &normalize_names = py::none(),
664: 	       const py::object &filename = py::none(), const py::object &null_padding = py::none(),
665: 	       const py::object &names = py::none(), shared_ptr<DuckDBPyConnection> conn = nullptr) {
666: 		    if (!conn) {
667: 			    conn = DuckDBPyConnection::DefaultConnection();
668: 		    }
669: 		    return conn->ReadCSV(name, header, compression, sep, delimiter, dtype, na_values, skiprows, quotechar,
670: 		                         escapechar, encoding, parallel, date_format, timestamp_format, sample_size,
671: 		                         all_varchar, normalize_names, filename, null_padding, names);
672: 	    },
673: 	    "Create a relation object from the CSV file in 'name'", py::arg("path_or_buffer"), py::kw_only(),
674: 	    py::arg("header") = py::none(), py::arg("compression") = py::none(), py::arg("sep") = py::none(),
675: 	    py::arg("delimiter") = py::none(), py::arg("dtype") = py::none(), py::arg("na_values") = py::none(),
676: 	    py::arg("skiprows") = py::none(), py::arg("quotechar") = py::none(), py::arg("escapechar") = py::none(),
677: 	    py::arg("encoding") = py::none(), py::arg("parallel") = py::none(), py::arg("date_format") = py::none(),
678: 	    py::arg("timestamp_format") = py::none(), py::arg("sample_size") = py::none(),
679: 	    py::arg("all_varchar") = py::none(), py::arg("normalize_names") = py::none(), py::arg("filename") = py::none(),
680: 	    py::arg("null_padding") = py::none(), py::arg("names") = py::none(), py::arg("connection") = py::none());
681: 	m.def(
682: 	    "from_df",
683: 	    [](const PandasDataFrame &value, shared_ptr<DuckDBPyConnection> conn = nullptr) {
684: 		    if (!conn) {
685: 			    conn = DuckDBPyConnection::DefaultConnection();
686: 		    }
687: 		    return conn->FromDF(value);
688: 	    },
689: 	    "Create a relation object from the DataFrame in df", py::arg("df"), py::kw_only(),
690: 	    py::arg("connection") = py::none());
691: 	m.def(
692: 	    "from_arrow",
693: 	    [](py::object &arrow_object, shared_ptr<DuckDBPyConnection> conn = nullptr) {
694: 		    if (!conn) {
695: 			    conn = DuckDBPyConnection::DefaultConnection();
696: 		    }
697: 		    return conn->FromArrow(arrow_object);
698: 	    },
699: 	    "Create a relation object from an Arrow object", py::arg("arrow_object"), py::kw_only(),
700: 	    py::arg("connection") = py::none());
701: 	m.def(
702: 	    "from_parquet",
703: 	    [](const string &file_glob, bool binary_as_string, bool file_row_number, bool filename, bool hive_partitioning,
704: 	       bool union_by_name, const py::object &compression = py::none(),
705: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
706: 		    if (!conn) {
707: 			    conn = DuckDBPyConnection::DefaultConnection();
708: 		    }
709: 		    return conn->FromParquet(file_glob, binary_as_string, file_row_number, filename, hive_partitioning,
710: 		                             union_by_name, compression);
711: 	    },
712: 	    "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
713: 	    py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
714: 	    py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
715: 	    py::arg("compression") = py::none(), py::arg("connection") = py::none());
716: 	m.def(
717: 	    "read_parquet",
718: 	    [](const string &file_glob, bool binary_as_string, bool file_row_number, bool filename, bool hive_partitioning,
719: 	       bool union_by_name, const py::object &compression = py::none(),
720: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
721: 		    if (!conn) {
722: 			    conn = DuckDBPyConnection::DefaultConnection();
723: 		    }
724: 		    return conn->FromParquet(file_glob, binary_as_string, file_row_number, filename, hive_partitioning,
725: 		                             union_by_name, compression);
726: 	    },
727: 	    "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
728: 	    py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
729: 	    py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
730: 	    py::arg("compression") = py::none(), py::arg("connection") = py::none());
731: 	m.def(
732: 	    "from_parquet",
733: 	    [](const vector<string> &file_globs, bool binary_as_string, bool file_row_number, bool filename,
734: 	       bool hive_partitioning, bool union_by_name, const py::object &compression = py::none(),
735: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
736: 		    if (!conn) {
737: 			    conn = DuckDBPyConnection::DefaultConnection();
738: 		    }
739: 		    return conn->FromParquets(file_globs, binary_as_string, file_row_number, filename, hive_partitioning,
740: 		                              union_by_name, compression);
741: 	    },
742: 	    "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
743: 	    py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
744: 	    py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
745: 	    py::arg("compression") = py::none(), py::arg("connection") = py::none());
746: 	m.def(
747: 	    "read_parquet",
748: 	    [](const vector<string> &file_globs, bool binary_as_string, bool file_row_number, bool filename,
749: 	       bool hive_partitioning, bool union_by_name, const py::object &compression = py::none(),
750: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
751: 		    if (!conn) {
752: 			    conn = DuckDBPyConnection::DefaultConnection();
753: 		    }
754: 		    return conn->FromParquets(file_globs, binary_as_string, file_row_number, filename, hive_partitioning,
755: 		                              union_by_name, compression);
756: 	    },
757: 	    "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
758: 	    py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
759: 	    py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
760: 	    py::arg("compression") = py::none(), py::arg("connection") = py::none());
761: 	m.def(
762: 	    "from_substrait",
763: 	    [](py::bytes &proto, shared_ptr<DuckDBPyConnection> conn = nullptr) {
764: 		    if (!conn) {
765: 			    conn = DuckDBPyConnection::DefaultConnection();
766: 		    }
767: 		    return conn->FromSubstrait(proto);
768: 	    },
769: 	    "Create a query object from protobuf plan", py::arg("proto"), py::kw_only(),
770: 	    py::arg("connection") = py::none());
771: 	m.def(
772: 	    "get_substrait",
773: 	    [](const string &query, bool enable_optimizer = true, shared_ptr<DuckDBPyConnection> conn = nullptr) {
774: 		    if (!conn) {
775: 			    conn = DuckDBPyConnection::DefaultConnection();
776: 		    }
777: 		    return conn->GetSubstrait(query, enable_optimizer);
778: 	    },
779: 	    "Serialize a query to protobuf", py::arg("query"), py::kw_only(), py::arg("enable_optimizer") = true,
780: 	    py::arg("connection") = py::none());
781: 	m.def(
782: 	    "get_substrait_json",
783: 	    [](const string &query, bool enable_optimizer = true, shared_ptr<DuckDBPyConnection> conn = nullptr) {
784: 		    if (!conn) {
785: 			    conn = DuckDBPyConnection::DefaultConnection();
786: 		    }
787: 		    return conn->GetSubstraitJSON(query, enable_optimizer);
788: 	    },
789: 	    "Serialize a query to protobuf on the JSON format", py::arg("query"), py::kw_only(),
790: 	    py::arg("enable_optimizer") = true, py::arg("connection") = py::none());
791: 	m.def(
792: 	    "from_substrait_json",
793: 	    [](const string &json, shared_ptr<DuckDBPyConnection> conn = nullptr) {
794: 		    if (!conn) {
795: 			    conn = DuckDBPyConnection::DefaultConnection();
796: 		    }
797: 		    return conn->FromSubstraitJSON(json);
798: 	    },
799: 	    "Create a query object from a JSON protobuf plan", py::arg("json"), py::kw_only(),
800: 	    py::arg("connection") = py::none());
801: 	m.def(
802: 	    "get_table_names",
803: 	    [](const string &query, shared_ptr<DuckDBPyConnection> conn = nullptr) {
804: 		    if (!conn) {
805: 			    conn = DuckDBPyConnection::DefaultConnection();
806: 		    }
807: 		    return conn->GetTableNames(query);
808: 	    },
809: 	    "Extract the required table names from a query", py::arg("query"), py::kw_only(),
810: 	    py::arg("connection") = py::none());
811: 	m.def(
812: 	    "install_extension",
813: 	    [](const string &extension, bool force_install = false, shared_ptr<DuckDBPyConnection> conn = nullptr) {
814: 		    if (!conn) {
815: 			    conn = DuckDBPyConnection::DefaultConnection();
816: 		    }
817: 		    conn->InstallExtension(extension, force_install);
818: 	    },
819: 	    "Install an extension by name", py::arg("extension"), py::kw_only(), py::arg("force_install") = false,
820: 	    py::arg("connection") = py::none());
821: 	m.def(
822: 	    "load_extension",
823: 	    [](const string &extension, shared_ptr<DuckDBPyConnection> conn = nullptr) {
824: 		    if (!conn) {
825: 			    conn = DuckDBPyConnection::DefaultConnection();
826: 		    }
827: 		    conn->LoadExtension(extension);
828: 	    },
829: 	    "Load an installed extension", py::arg("extension"), py::kw_only(), py::arg("connection") = py::none());
830: 	m.def(
831: 	    "project",
832: 	    [](const PandasDataFrame &df, const py::args &args, const string &groups = "",
833: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
834: 		    if (!conn) {
835: 			    conn = DuckDBPyConnection::DefaultConnection();
836: 		    }
837: 		    return conn->FromDF(df)->Project(args, groups);
838: 	    },
839: 	    "Project the relation object by the projection in project_expr", py::arg("df"), py::kw_only(),
840: 	    py::arg("groups") = "", py::arg("connection") = py::none());
841: 	m.def(
842: 	    "distinct",
843: 	    [](const PandasDataFrame &df, shared_ptr<DuckDBPyConnection> conn = nullptr) {
844: 		    if (!conn) {
845: 			    conn = DuckDBPyConnection::DefaultConnection();
846: 		    }
847: 		    return conn->FromDF(df)->Distinct();
848: 	    },
849: 	    "Retrieve distinct rows from this relation object", py::arg("df"), py::kw_only(),
850: 	    py::arg("connection") = py::none());
851: 	m.def(
852: 	    "write_csv",
853: 	    [](const PandasDataFrame &df, const string &filename, const py::object &sep = py::none(),
854: 	       const py::object &na_rep = py::none(), const py::object &header = py::none(),
855: 	       const py::object &quotechar = py::none(), const py::object &escapechar = py::none(),
856: 	       const py::object &date_format = py::none(), const py::object &timestamp_format = py::none(),
857: 	       const py::object &quoting = py::none(), const py::object &encoding = py::none(),
858: 	       const py::object &compression = py::none(), const py::object &overwrite = py::none(),
859: 	       const py::object &per_thread_output = py::none(), const py::object &use_tmp_file = py::none(),
860: 	       const py::object &partition_by = py::none(), shared_ptr<DuckDBPyConnection> conn = nullptr) {
861: 		    if (!conn) {
862: 			    conn = DuckDBPyConnection::DefaultConnection();
863: 		    }
864: 		    conn->FromDF(df)->ToCSV(filename, sep, na_rep, header, quotechar, escapechar, date_format, timestamp_format,
865: 		                            quoting, encoding, compression, overwrite, per_thread_output, use_tmp_file,
866: 		                            partition_by);
867: 	    },
868: 	    "Write the relation object to a CSV file in 'file_name'", py::arg("df"), py::arg("filename"), py::kw_only(),
869: 	    py::arg("sep") = py::none(), py::arg("na_rep") = py::none(), py::arg("header") = py::none(),
870: 	    py::arg("quotechar") = py::none(), py::arg("escapechar") = py::none(), py::arg("date_format") = py::none(),
871: 	    py::arg("timestamp_format") = py::none(), py::arg("quoting") = py::none(), py::arg("encoding") = py::none(),
872: 	    py::arg("compression") = py::none(), py::arg("overwrite") = py::none(),
873: 	    py::arg("per_thread_output") = py::none(), py::arg("use_tmp_file") = py::none(),
874: 	    py::arg("partition_by") = py::none(), py::arg("connection") = py::none());
875: 	m.def(
876: 	    "aggregate",
877: 	    [](const PandasDataFrame &df, const string &expr, const string &groups = "",
878: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
879: 		    if (!conn) {
880: 			    conn = DuckDBPyConnection::DefaultConnection();
881: 		    }
882: 		    return conn->FromDF(df)->Aggregate(expr, groups);
883: 	    },
884: 	    "Compute the aggregate aggr_expr by the optional groups group_expr on the relation", py::arg("df"),
885: 	    py::arg("aggr_expr"), py::arg("group_expr") = "", py::kw_only(), py::arg("connection") = py::none());
886: 	m.def(
887: 	    "alias",
888: 	    [](const PandasDataFrame &df, const string &expr, shared_ptr<DuckDBPyConnection> conn = nullptr) {
889: 		    if (!conn) {
890: 			    conn = DuckDBPyConnection::DefaultConnection();
891: 		    }
892: 		    return conn->FromDF(df)->SetAlias(expr);
893: 	    },
894: 	    "Rename the relation object to new alias", py::arg("df"), py::arg("alias"), py::kw_only(),
895: 	    py::arg("connection") = py::none());
896: 	m.def(
897: 	    "filter",
898: 	    [](const PandasDataFrame &df, const py::object &expr, shared_ptr<DuckDBPyConnection> conn = nullptr) {
899: 		    if (!conn) {
900: 			    conn = DuckDBPyConnection::DefaultConnection();
901: 		    }
902: 		    return conn->FromDF(df)->Filter(expr);
903: 	    },
904: 	    "Filter the relation object by the filter in filter_expr", py::arg("df"), py::arg("filter_expr"), py::kw_only(),
905: 	    py::arg("connection") = py::none());
906: 	m.def(
907: 	    "limit",
908: 	    [](const PandasDataFrame &df, int64_t n, int64_t offset = 0, shared_ptr<DuckDBPyConnection> conn = nullptr) {
909: 		    if (!conn) {
910: 			    conn = DuckDBPyConnection::DefaultConnection();
911: 		    }
912: 		    return conn->FromDF(df)->Limit(n, offset);
913: 	    },
914: 	    "Only retrieve the first n rows from this relation object, starting at offset", py::arg("df"), py::arg("n"),
915: 	    py::arg("offset") = 0, py::kw_only(), py::arg("connection") = py::none());
916: 	m.def(
917: 	    "order",
918: 	    [](const PandasDataFrame &df, const string &expr, shared_ptr<DuckDBPyConnection> conn = nullptr) {
919: 		    if (!conn) {
920: 			    conn = DuckDBPyConnection::DefaultConnection();
921: 		    }
922: 		    return conn->FromDF(df)->Order(expr);
923: 	    },
924: 	    "Reorder the relation object by order_expr", py::arg("df"), py::arg("order_expr"), py::kw_only(),
925: 	    py::arg("connection") = py::none());
926: 	m.def(
927: 	    "query_df",
928: 	    [](const PandasDataFrame &df, const string &view_name, const string &sql_query,
929: 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
930: 		    if (!conn) {
931: 			    conn = DuckDBPyConnection::DefaultConnection();
932: 		    }
933: 		    return conn->FromDF(df)->Query(view_name, sql_query);
934: 	    },
935: 	    "Run the given SQL query in sql_query on the view named virtual_table_name that refers to the relation object",
936: 	    py::arg("df"), py::arg("virtual_table_name"), py::arg("sql_query"), py::kw_only(),
937: 	    py::arg("connection") = py::none());
938: 	m.def(
939: 	    "description",
940: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
941: 		    if (!conn) {
942: 			    conn = DuckDBPyConnection::DefaultConnection();
943: 		    }
944: 		    return conn->GetDescription();
945: 	    },
946: 	    "Get result set attributes, mainly column names", py::kw_only(), py::arg("connection") = py::none());
947: 	m.def(
948: 	    "rowcount",
949: 	    [](shared_ptr<DuckDBPyConnection> conn = nullptr) {
950: 		    if (!conn) {
951: 			    conn = DuckDBPyConnection::DefaultConnection();
952: 		    }
953: 		    return conn->GetRowcount();
954: 	    },
955: 	    "Get result set row count", py::kw_only(), py::arg("connection") = py::none());
956: 	// END_OF_CONNECTION_METHODS
957: 
958: 	// We define these "wrapper" methods manually because they are overloaded
959: 	m.def(
960: 	    "arrow",
961: 	    [](idx_t rows_per_batch, shared_ptr<DuckDBPyConnection> conn) -> duckdb::pyarrow::Table {
962: 		    if (!conn) {
963: 			    conn = DuckDBPyConnection::DefaultConnection();
964: 		    }
965: 		    return conn->FetchArrow(rows_per_batch);
966: 	    },
967: 	    "Fetch a result as Arrow table following execute()", py::arg("rows_per_batch") = 1000000, py::kw_only(),
968: 	    py::arg("connection") = py::none());
969: 	m.def(
970: 	    "arrow",
971: 	    [](py::object &arrow_object, shared_ptr<DuckDBPyConnection> conn) -> unique_ptr<DuckDBPyRelation> {
972: 		    if (!conn) {
973: 			    conn = DuckDBPyConnection::DefaultConnection();
974: 		    }
975: 		    return conn->FromArrow(arrow_object);
976: 	    },
977: 	    "Create a relation object from an Arrow object", py::arg("arrow_object"), py::kw_only(),
978: 	    py::arg("connection") = py::none());
979: 	m.def(
980: 	    "df",
981: 	    [](bool date_as_object, shared_ptr<DuckDBPyConnection> conn) -> PandasDataFrame {
982: 		    if (!conn) {
983: 			    conn = DuckDBPyConnection::DefaultConnection();
984: 		    }
985: 		    return conn->FetchDF(date_as_object);
986: 	    },
987: 	    "Fetch a result as DataFrame following execute()", py::kw_only(), py::arg("date_as_object") = false,
988: 	    py::arg("connection") = py::none());
989: 	m.def(
990: 	    "df",
991: 	    [](const PandasDataFrame &value, shared_ptr<DuckDBPyConnection> conn) -> unique_ptr<DuckDBPyRelation> {
992: 		    if (!conn) {
993: 			    conn = DuckDBPyConnection::DefaultConnection();
994: 		    }
995: 		    return conn->FromDF(value);
996: 	    },
997: 	    "Create a relation object from the DataFrame df", py::arg("df"), py::kw_only(),
998: 	    py::arg("connection") = py::none());
999: }
1000: 
1001: static void RegisterStatementType(py::handle &m) {
1002: 	auto statement_type = py::enum_<duckdb::StatementType>(m, "StatementType");
1003: 	static const duckdb::StatementType TYPES[] = {
1004: 	    duckdb::StatementType::INVALID_STATEMENT,      duckdb::StatementType::SELECT_STATEMENT,
1005: 	    duckdb::StatementType::INSERT_STATEMENT,       duckdb::StatementType::UPDATE_STATEMENT,
1006: 	    duckdb::StatementType::CREATE_STATEMENT,       duckdb::StatementType::DELETE_STATEMENT,
1007: 	    duckdb::StatementType::PREPARE_STATEMENT,      duckdb::StatementType::EXECUTE_STATEMENT,
1008: 	    duckdb::StatementType::ALTER_STATEMENT,        duckdb::StatementType::TRANSACTION_STATEMENT,
1009: 	    duckdb::StatementType::COPY_STATEMENT,         duckdb::StatementType::ANALYZE_STATEMENT,
1010: 	    duckdb::StatementType::VARIABLE_SET_STATEMENT, duckdb::StatementType::CREATE_FUNC_STATEMENT,
1011: 	    duckdb::StatementType::EXPLAIN_STATEMENT,      duckdb::StatementType::DROP_STATEMENT,
1012: 	    duckdb::StatementType::EXPORT_STATEMENT,       duckdb::StatementType::PRAGMA_STATEMENT,
1013: 	    duckdb::StatementType::VACUUM_STATEMENT,       duckdb::StatementType::CALL_STATEMENT,
1014: 	    duckdb::StatementType::SET_STATEMENT,          duckdb::StatementType::LOAD_STATEMENT,
1015: 	    duckdb::StatementType::RELATION_STATEMENT,     duckdb::StatementType::EXTENSION_STATEMENT,
1016: 	    duckdb::StatementType::LOGICAL_PLAN_STATEMENT, duckdb::StatementType::ATTACH_STATEMENT,
1017: 	    duckdb::StatementType::DETACH_STATEMENT,       duckdb::StatementType::MULTI_STATEMENT,
1018: 	    duckdb::StatementType::COPY_DATABASE_STATEMENT};
1019: 	static const idx_t AMOUNT = sizeof(TYPES) / sizeof(duckdb::StatementType);
1020: 	for (idx_t i = 0; i < AMOUNT; i++) {
1021: 		auto &type = TYPES[i];
1022: 		statement_type.value(StatementTypeToString(type).c_str(), type);
1023: 	}
1024: 	statement_type.export_values();
1025: }
1026: 
1027: static void RegisterExpectedResultType(py::handle &m) {
1028: 	auto expected_return_type = py::enum_<duckdb::StatementReturnType>(m, "ExpectedResultType");
1029: 	static const duckdb::StatementReturnType TYPES[] = {duckdb::StatementReturnType::QUERY_RESULT,
1030: 	                                                    duckdb::StatementReturnType::CHANGED_ROWS,
1031: 	                                                    duckdb::StatementReturnType::NOTHING};
1032: 	static const idx_t AMOUNT = sizeof(TYPES) / sizeof(duckdb::StatementReturnType);
1033: 	for (idx_t i = 0; i < AMOUNT; i++) {
1034: 		auto &type = TYPES[i];
1035: 		expected_return_type.value(StatementReturnTypeToString(type).c_str(), type);
1036: 	}
1037: 	expected_return_type.export_values();
1038: }
1039: 
1040: PYBIND11_MODULE(DUCKDB_PYTHON_LIB_NAME, m) { // NOLINT
1041: 	py::enum_<duckdb::ExplainType>(m, "ExplainType")
1042: 	    .value("STANDARD", duckdb::ExplainType::EXPLAIN_STANDARD)
1043: 	    .value("ANALYZE", duckdb::ExplainType::EXPLAIN_ANALYZE)
1044: 	    .export_values();
1045: 
1046: 	RegisterStatementType(m);
1047: 
1048: 	RegisterExpectedResultType(m);
1049: 
1050: 	py::enum_<duckdb::PythonExceptionHandling>(m, "PythonExceptionHandling")
1051: 	    .value("DEFAULT", duckdb::PythonExceptionHandling::FORWARD_ERROR)
1052: 	    .value("RETURN_NULL", duckdb::PythonExceptionHandling::RETURN_NULL)
1053: 	    .export_values();
1054: 
1055: 	py::enum_<duckdb::RenderMode>(m, "RenderMode")
1056: 	    .value("ROWS", duckdb::RenderMode::ROWS)
1057: 	    .value("COLUMNS", duckdb::RenderMode::COLUMNS)
1058: 	    .export_values();
1059: 
1060: 	DuckDBPyTyping::Initialize(m);
1061: 	DuckDBPyFunctional::Initialize(m);
1062: 	DuckDBPyExpression::Initialize(m);
1063: 	DuckDBPyStatement::Initialize(m);
1064: 	DuckDBPyRelation::Initialize(m);
1065: 	DuckDBPyConnection::Initialize(m);
1066: 	PythonObject::Initialize();
1067: 
1068: 	py::options pybind_opts;
1069: 
1070: 	m.doc() = "DuckDB is an embeddable SQL OLAP Database Management System";
1071: 	m.attr("__package__") = "duckdb";
1072: 	m.attr("__version__") = std::string(DuckDB::LibraryVersion()).substr(1);
1073: 	m.attr("__standard_vector_size__") = DuckDB::StandardVectorSize();
1074: 	m.attr("__git_revision__") = DuckDB::SourceID();
1075: 	m.attr("__interactive__") = DuckDBPyConnection::DetectAndGetEnvironment();
1076: 	m.attr("__jupyter__") = DuckDBPyConnection::IsJupyter();
1077: 	m.attr("default_connection") = DuckDBPyConnection::DefaultConnection();
1078: 	m.attr("apilevel") = "2.0";
1079: 	m.attr("threadsafety") = 1;
1080: 	m.attr("paramstyle") = "qmark";
1081: 
1082: 	InitializeConnectionMethods(m);
1083: 
1084: 	RegisterExceptions(m);
1085: 
1086: 	m.def("connect", &DuckDBPyConnection::Connect,
1087: 	      "Create a DuckDB database instance. Can take a database file name to read/write persistent data and a "
1088: 	      "read_only flag if no changes are desired",
1089: 	      py::arg("database") = ":memory:", py::arg("read_only") = false, py::arg_v("config", py::dict(), "None"));
1090: 	m.def("tokenize", PyTokenize,
1091: 	      "Tokenizes a SQL string, returning a list of (position, type) tuples that can be "
1092: 	      "used for e.g. syntax highlighting",
1093: 	      py::arg("query"));
1094: 	py::enum_<PySQLTokenType>(m, "token_type", py::module_local())
1095: 	    .value("identifier", PySQLTokenType::PY_SQL_TOKEN_IDENTIFIER)
1096: 	    .value("numeric_const", PySQLTokenType::PY_SQL_TOKEN_NUMERIC_CONSTANT)
1097: 	    .value("string_const", PySQLTokenType::PY_SQL_TOKEN_STRING_CONSTANT)
1098: 	    .value("operator", PySQLTokenType::PY_SQL_TOKEN_OPERATOR)
1099: 	    .value("keyword", PySQLTokenType::PY_SQL_TOKEN_KEYWORD)
1100: 	    .value("comment", PySQLTokenType::PY_SQL_TOKEN_COMMENT)
1101: 	    .export_values();
1102: 
1103: 	// we need this because otherwise we try to remove registered_dfs on shutdown when python is already dead
1104: 	auto clean_default_connection = []() {
1105: 		DuckDBPyConnection::Cleanup();
1106: 	};
1107: 	m.add_object("_clean_default_connection", py::capsule(clean_default_connection));
1108: }
1109: 
1110: } // namespace duckdb
[end of tools/pythonpkg/duckdb_python.cpp]
[start of tools/pythonpkg/scripts/connection_methods.json]
1: [
2: 	{
3: 		"name": "cursor",
4: 		"function": "Cursor",
5: 		"docs": "Create a duplicate of the current connection",
6: 		"return": "DuckDBPyConnection"
7: 	},
8: 	{
9: 		"name": "register_filesystem",
10: 		"function": "RegisterFilesystem",
11: 		"docs": "Register a fsspec compliant filesystem",
12: 		"args": [
13: 			{
14: 				"name": "filesystem",
15: 				"type": "str"
16: 			}
17: 		],
18: 		"return": "None"
19: 	},
20: 	{
21: 		"name": "unregister_filesystem",
22: 		"function": "UnregisterFilesystem",
23: 		"docs": "Unregister a filesystem",
24: 		"args": [
25: 			{
26: 				"name": "name",
27: 				"type": "str"
28: 			}
29: 		],
30: 		"return": "None"
31: 	},
32: 	{
33: 		"name": "list_filesystems",
34: 		"function": "ListFilesystems",
35: 		"docs": "List registered filesystems, including builtin ones",
36: 		"return": "list"
37: 	},
38: 	{
39: 		"name": "filesystem_is_registered",
40: 		"function": "FileSystemIsRegistered",
41: 		"docs": "Check if a filesystem with the provided name is currently registered",
42: 		"args": [
43: 			{
44: 				"name": "name",
45: 				"type": "str"
46: 			}
47: 		],
48: 		"return": "bool"
49: 	},
50: 	{
51: 		"name": "create_function",
52: 		"function": "RegisterScalarUDF",
53: 		"docs": "Create a DuckDB function out of the passing in Python function so it can be used in queries",
54: 		"args": [
55: 			{
56: 				"name": "name",
57: 				"type": "str"
58: 			},
59: 			{
60: 				"name": "function",
61: 				"type": "function"
62: 			},
63: 			{
64: 				"name": "parameters",
65: 				"type": "Optional[List[DuckDBPyType]]",
66: 				"default": "None"
67: 			},
68: 			{
69: 				"name": "return_type",
70: 				"type": "Optional[DuckDBPyType]",
71: 				"default": "None"
72: 			}
73: 		],
74: 		"kwargs": [
75: 			{
76: 				"name": "type",
77: 				"type": "Optional[PythonUDFType]",
78: 				"default": "PythonUDFType.NATIVE"
79: 			},
80: 			{
81: 				"name": "null_handling",
82: 				"type": "Optional[FunctionNullHandling]",
83: 				"default": "FunctionNullHandling.DEFAULT"
84: 			},
85: 			{
86: 				"name": "exception_handling",
87: 				"type": "Optional[PythonExceptionHandling]",
88: 				"default": "PythonExceptionHandling.DEFAULT"
89: 			},
90: 			{
91: 				"name": "side_effects",
92: 				"type": "bool",
93: 				"default": "False"
94: 			}
95: 		],
96: 		"return": "DuckDBPyConnection"
97: 	},
98: 	{
99: 		"name": "remove_function",
100: 		"function": "UnregisterUDF",
101: 		"docs": "Remove a previously created function",
102: 		"args": [
103: 			{
104: 				"name": "name",
105: 				"type": "str"
106: 			}
107: 		],
108: 		"return": "DuckDBPyConnection"
109: 	},
110: 	{
111: 		"name": ["sqltype", "dtype", "type"],
112: 		"function": "Type",
113: 		"docs": "Create a type object by parsing the 'type_str' string",
114: 		"args": [
115: 			{
116: 				"name": "type_str",
117: 				"type": "str"
118: 			}
119: 		],
120: 		"return": "DuckDBPyType"
121: 	},
122: 	{
123: 		"name": "array_type",
124: 		"function": "ArrayType",
125: 		"docs": "Create an array type object of 'type'",
126: 		"args": [
127: 			{
128: 				"name": "type",
129: 				"type": "DuckDBPyType",
130: 				"allow_none": false
131: 			},
132: 			{
133: 				"name": "size",
134: 				"type": "int"
135: 			}
136: 		],
137: 		"return": "DuckDBPyType"
138: 	},
139: 	{
140: 		"name": "list_type",
141: 		"function": "ListType",
142: 		"docs": "Create a list type object of 'type'",
143: 		"args": [
144: 			{
145: 				"name": "type",
146: 				"type": "DuckDBPyType",
147: 				"allow_none": false
148: 			}
149: 		],
150: 		"return": "DuckDBPyType"
151: 	},
152: 	{
153: 		"name": "union_type",
154: 		"function": "UnionType",
155: 		"docs": "Create a union type object from 'members'",
156: 		"args": [
157: 			{
158: 				"name": "members",
159: 				"type": "DuckDBPyType",
160: 				"allow_none": false
161: 			}
162: 		],
163: 		"return": "DuckDBPyType"
164: 	},
165: 	{
166: 		"name": "string_type",
167: 		"function": "StringType",
168: 		"docs": "Create a string type with an optional collation",
169: 		"args": [
170: 			{
171: 				"name": "collation",
172: 				"type": "str",
173: 				"default": "\"\""
174: 			}
175: 		],
176: 		"return": "DuckDBPyType"
177: 	},
178: 	{
179: 		"name": "enum_type",
180: 		"function": "EnumType",
181: 		"docs": "Create an enum type of underlying 'type', consisting of the list of 'values'",
182: 		"args": [
183: 			{
184: 				"name": "name",
185: 				"type": "str"
186: 			},
187: 			{
188: 				"name": "type",
189: 				"type": "DuckDBPyType"
190: 			},
191: 			{
192: 				"name": "values",
193: 				"type": "List[Any]"
194: 			}
195: 		],
196: 		"return": "DuckDBPyType"
197: 	},
198: 	{
199: 		"name": "decimal_type",
200: 		"function": "DecimalType",
201: 		"docs": "Create a decimal type with 'width' and 'scale'",
202: 		"args": [
203: 			{
204: 				"name": "width",
205: 				"type": "int"
206: 			},
207: 			{
208: 				"name": "scale",
209: 				"type": "int"
210: 			}
211: 		],
212: 		"return": "DuckDBPyType"
213: 	},
214: 	{
215: 		"name": ["struct_type", "row_type"],
216: 		"function": "StructType",
217: 		"docs": "Create a struct type object from 'fields'",
218: 		"args": [
219: 			{
220: 				"name": "fields",
221: 				"type": "Union[Dict[str, DuckDBPyType], List[str]]"
222: 			}
223: 		],
224: 		"return": "DuckDBPyType"
225: 	},
226: 	{
227: 		"name": "map_type",
228: 		"function": "MapType",
229: 		"docs": "Create a map type object from 'key_type' and 'value_type'",
230: 		"args": [
231: 			{
232: 				"name": "key",
233: 				"allow_none": false,
234: 				"type": "DuckDBPyType"
235: 			},
236: 			{
237: 				"name": "value",
238: 				"allow_none": false,
239: 				"type": "DuckDBPyType"
240: 			}
241: 		],
242: 		"return": "DuckDBPyType"
243: 	},
244: 	{
245: 		"name": "duplicate",
246: 		"function": "Cursor",
247: 		"docs": "Create a duplicate of the current connection",
248: 		"return": "DuckDBPyConnection"
249: 	},
250: 	{
251: 		"name": "execute",
252: 		"function": "Execute",
253: 		"docs": "Execute the given SQL query, optionally using prepared statements with parameters set",
254: 		"args": [
255: 			{
256: 				"name": "query",
257: 				"type": "object"
258: 			},
259: 			{
260: 				"name": "parameters",
261: 				"default": "None",
262: 				"type": "object"
263: 			}
264: 		],
265: 		"return": "DuckDBPyConnection"
266: 	},
267: 	{
268: 		"name": "executemany",
269: 		"function": "ExecuteMany",
270: 		"docs": "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
271: 		"args": [
272: 			{
273: 				"name": "query",
274: 				"type": "object"
275: 			},
276: 			{
277: 				"name": "parameters",
278: 				"default": "None",
279: 				"type": "object"
280: 			}
281: 		],
282: 		"return": "DuckDBPyConnection"
283: 	},
284: 	{
285: 		"name": "close",
286: 		"function": "Close",
287: 		"docs": "Close the connection",
288: 		"return": "None"
289: 	},
290: 	{
291: 		"name": "interrupt",
292: 		"function": "Interrupt",
293: 		"docs": "Interrupt pending operations",
294: 		"return": "None"
295: 	},
296: 	{
297: 		"name": "fetchone",
298: 		"function": "FetchOne",
299: 		"docs": "Fetch a single row from a result following execute",
300: 		"return": "Optional[tuple]"
301: 	},
302: 	{
303: 		"name": "fetchmany",
304: 		"function": "FetchMany",
305: 		"docs": "Fetch the next set of rows from a result following execute",
306: 		"args": [
307: 			{
308: 				"name": "size",
309: 				"default": "1",
310: 				"type": "int"
311: 			}
312: 		],
313: 		"return": "List[Any]"
314: 	},
315: 	{
316: 		"name": "fetchall",
317: 		"function": "FetchAll",
318: 		"docs": "Fetch all rows from a result following execute",
319: 		"return": "List[Any]"
320: 	},
321: 	{
322: 		"name": "fetchnumpy",
323: 		"function": "FetchNumpy",
324: 		"docs": "Fetch a result as list of NumPy arrays following execute",
325: 		"return": "dict"
326: 	},
327: 	{
328: 		"name": ["fetchdf", "fetch_df", "df"],
329: 		"function": "FetchDF",
330: 		"docs": "Fetch a result as DataFrame following execute()",
331: 		"kwargs": [
332: 			{
333: 				"name": "date_as_object",
334: 				"default": "False",
335: 				"type": "bool"
336: 			}
337: 		],
338: 		"return": "pandas.DataFrame"
339: 	},
340: 	{
341: 		"name": "fetch_df_chunk",
342: 		"function": "FetchDFChunk",
343: 		"docs": "Fetch a chunk of the result as DataFrame following execute()",
344: 		"args": [
345: 			{
346: 				"name": "vectors_per_chunk",
347: 				"default": "1",
348: 				"type": "int"
349: 			}
350: 		],
351: 		"kwargs": [
352: 			{
353: 				"name": "date_as_object",
354: 				"default": "False",
355: 				"type": "bool"
356: 			}
357: 		],
358: 		"return": "pandas.DataFrame"
359: 	},
360: 	{
361: 		"name": "pl",
362: 		"function": "FetchPolars",
363: 		"docs": "Fetch a result as Polars DataFrame following execute()",
364: 		"args": [
365: 			{
366: 				"name": "rows_per_batch",
367: 				"default": "1000000",
368: 				"type": "int"
369: 			}
370: 		],
371: 		"return": "polars.DataFrame"
372: 	},
373: 	{
374: 		"name": ["fetch_arrow_table", "arrow"],
375: 		"function": "FetchArrow",
376: 		"docs": "Fetch a result as Arrow table following execute()",
377: 		"args": [
378: 			{
379: 				"name": "rows_per_batch",
380: 				"default": "1000000",
381: 				"type": "int"
382: 			}
383: 		],
384: 		"return": "pyarrow.lib.Table"
385: 	},
386: 	{
387: 		"name": "fetch_record_batch",
388: 		"function": "FetchRecordBatchReader",
389: 		"docs": "Fetch an Arrow RecordBatchReader following execute()",
390: 		"args": [
391: 			{
392: 				"name": "rows_per_batch",
393: 				"default": "1000000",
394: 				"type": "int"
395: 			}
396: 		],
397: 		"return": "pyarrow.lib.RecordBatchReader"
398: 	},
399: 	{
400: 		"name": "torch",
401: 		"function": "FetchPyTorch",
402: 		"docs": "Fetch a result as dict of PyTorch Tensors following execute()",
403: 		"return": "dict"
404: 	},
405: 	{
406: 		"name": "tf",
407: 		"function": "FetchTF",
408: 		"docs": "Fetch a result as dict of TensorFlow Tensors following execute()",
409: 		"return": "dict"
410: 	},
411: 	{
412: 		"name": "begin",
413: 		"function": "Begin",
414: 		"docs": "Start a new transaction",
415: 		"return": "DuckDBPyConnection"
416: 	},
417: 	{
418: 		"name": "commit",
419: 		"function": "Commit",
420: 		"docs": "Commit changes performed within a transaction",
421: 		"return": "DuckDBPyConnection"
422: 	},
423: 	{
424: 		"name": "rollback",
425: 		"function": "Rollback",
426: 		"docs": "Roll back changes performed within a transaction",
427: 		"return": "DuckDBPyConnection"
428: 	},
429: 	{
430: 		"name": "checkpoint",
431: 		"function": "Checkpoint",
432: 		"docs": "Synchronizes data in the write-ahead log (WAL) to the database data file (no-op for in-memory connections)",
433: 		"return": "DuckDBPyConnection"
434: 	},
435: 	{
436: 		"name": "append",
437: 		"function": "Append",
438: 		"docs": "Append the passed DataFrame to the named table",
439: 		"args": [
440: 			{
441: 				"name": "table_name",
442: 				"type": "str"
443: 			},
444: 			{
445: 				"name": "df",
446: 				"type": "pandas.DataFrame"
447: 			}
448: 		],
449: 		"kwargs": [
450: 			{
451: 				"name": "by_name",
452: 				"default": "False",
453: 				"type": "bool"
454: 			}
455: 		],
456: 		"return": "DuckDBPyConnection"
457: 	},
458: 	{
459: 		"name": "register",
460: 		"function": "RegisterPythonObject",
461: 		"docs": "Register the passed Python Object value for querying with a view",
462: 		"args": [
463: 			{
464: 				"name": "view_name",
465: 				"type": "str"
466: 			},
467: 			{
468: 				"name": "python_object",
469: 				"type": "object"
470: 			}
471: 		],
472: 		"return": "DuckDBPyConnection"
473: 	},
474: 	{
475: 		"name": "unregister",
476: 		"function": "UnregisterPythonObject",
477: 		"docs": "Unregister the view name",
478: 		"args": [
479: 			{
480: 				"name": "view_name",
481: 				"type": "str"
482: 			}
483: 		],
484: 		"return": "DuckDBPyConnection"
485: 	},
486: 	{
487: 		"name": "table",
488: 		"function": "Table",
489: 		"docs": "Create a relation object for the named table",
490: 		"args": [
491: 			{
492: 				"name": "table_name",
493: 				"type": "str"
494: 			}
495: 		],
496: 		"return": "DuckDBPyRelation"
497: 	},
498: 	{
499: 		"name": "view",
500: 		"function": "View",
501: 		"docs": "Create a relation object for the named view",
502: 		"args": [
503: 			{
504: 				"name": "view_name",
505: 				"type": "str"
506: 			}
507: 		],
508: 		"return": "DuckDBPyRelation"
509: 	},
510: 	{
511: 		"name": "values",
512: 		"function": "Values",
513: 		"docs": "Create a relation object from the passed values",
514: 		"args": [
515: 			{
516: 				"name": "values",
517: 				"type": "List[Any]"
518: 			}
519: 		],
520: 		"return": "DuckDBPyRelation"
521: 	},
522: 	{
523: 		"name": "table_function",
524: 		"function": "TableFunction",
525: 		"docs": "Create a relation object from the named table function with given parameters",
526: 		"args": [
527: 			{
528: 				"name": "name",
529: 				"type": "str"
530: 			},
531: 			{
532: 				"name": "parameters",
533: 				"default": "None",
534: 				"type": "object"
535: 			}
536: 		],
537: 		"return": "DuckDBPyRelation"
538: 	},
539: 	{
540: 		"name": "read_json",
541: 		"function": "ReadJSON",
542: 		"docs": "Create a relation object from the JSON file in 'name'",
543: 		"args": [
544: 			{
545: 				"name": "name",
546: 				"type": "str"
547: 			}
548: 		],
549: 		"kwargs": [
550: 			{
551: 				"name": "columns",
552: 				"default": "None",
553: 				"type": "Optional[Dict[str,str]]"
554: 			},
555: 			{
556: 				"name": "sample_size",
557: 				"default": "None",
558: 				"type": "Optional[int]"
559: 			},
560: 			{
561: 				"name": "maximum_depth",
562: 				"default": "None",
563: 				"type": "Optional[int]"
564: 			},
565: 			{
566: 				"name": "records",
567: 				"default": "None",
568: 				"type": "Optional[str]"
569: 			},
570: 			{
571: 				"name": "format",
572: 				"default": "None",
573: 				"type": "Optional[str]"
574: 			}
575: 		],
576: 		"return": "DuckDBPyRelation"
577: 	},
578: 	{
579: 		"name": "extract_statements",
580: 		"function": "ExtractStatements",
581: 		"docs": "Parse the query string and extract the Statement object(s) produced",
582: 		"args": [
583: 			{
584: 				"name": "query",
585: 				"type": "str"
586: 			}
587: 		],
588: 		"return": "List[Statement]"
589: 	},
590: 	{
591: 		"name": ["sql", "query", "from_query"],
592: 		"function": "RunQuery",
593: 		"docs": "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise run the query as-is.",
594: 		"args": [
595: 			{
596: 				"name": "query",
597: 				"type": "str"
598: 			}
599: 		],
600: 		"kwargs": [
601: 			{
602: 				"name": "alias",
603: 				"default": "\"\"",
604: 				"type": "str"
605: 			},
606: 			{
607: 				"name": "params",
608: 				"default": "None",
609: 				"type": "object"
610: 			}
611: 		],
612: 		"return": "DuckDBPyRelation"
613: 	},
614: 	{
615: 		"name": ["read_csv", "from_csv_auto"],
616: 		"function": "ReadCSV",
617: 		"docs": "Create a relation object from the CSV file in 'name'",
618: 		"args": [
619: 			{
620: 				"name": "path_or_buffer",
621: 				"type": "Union[str, StringIO, TextIOBase]"
622: 			}
623: 		],
624: 		"kwargs": [
625: 			{
626: 				"name": "header",
627: 				"default": "None",
628: 				"type": "Optional[bool | int]"
629: 			},
630: 			{
631: 				"name": "compression",
632: 				"default": "None",
633: 				"type": "Optional[str]"
634: 			},
635: 			{
636: 				"name": "sep",
637: 				"default": "None",
638: 				"type": "Optional[str]"
639: 			},
640: 			{
641: 				"name": "delimiter",
642: 				"default": "None",
643: 				"type": "Optional[str]"
644: 			},
645: 			{
646: 				"name": "dtype",
647: 				"default": "None",
648: 				"type": "Optional[Dict[str, str] | List[str]]"
649: 			},
650: 			{
651: 				"name": "na_values",
652: 				"default": "None",
653: 				"type": "Optional[str| List[str]]"
654: 			},
655: 			{
656: 				"name": "skiprows",
657: 				"default": "None",
658: 				"type": "Optional[int]"
659: 			},
660: 			{
661: 				"name": "quotechar",
662: 				"default": "None",
663: 				"type": "Optional[str]"
664: 			},
665: 			{
666: 				"name": "escapechar",
667: 				"default": "None",
668: 				"type": "Optional[str]"
669: 			},
670: 			{
671: 				"name": "encoding",
672: 				"default": "None",
673: 				"type": "Optional[str]"
674: 			},
675: 			{
676: 				"name": "parallel",
677: 				"default": "None",
678: 				"type": "Optional[bool]"
679: 			},
680: 			{
681: 				"name": "date_format",
682: 				"default": "None",
683: 				"type": "Optional[str]"
684: 			},
685: 			{
686: 				"name": "timestamp_format",
687: 				"default": "None",
688: 				"type": "Optional[str]"
689: 			},
690: 			{
691: 				"name": "sample_size",
692: 				"default": "None",
693: 				"type": "Optional[int]"
694: 			},
695: 			{
696: 				"name": "all_varchar",
697: 				"default": "None",
698: 				"type": "Optional[bool]"
699: 			},
700: 			{
701: 				"name": "normalize_names",
702: 				"default": "None",
703: 				"type": "Optional[bool]"
704: 			},
705: 			{
706: 				"name": "filename",
707: 				"default": "None",
708: 				"type": "Optional[bool]"
709: 			},
710: 			{
711: 				"name": "null_padding",
712: 				"default": "None",
713: 				"type": "Optional[bool]"
714: 			},
715: 			{
716: 				"name": "names",
717: 				"default": "None",
718: 				"type": "Optional[List[str]]"
719: 			}
720: 		],
721: 		"return": "DuckDBPyRelation"
722: 	},
723: 	{
724: 		"name": "from_df",
725: 		"function": "FromDF",
726: 		"docs": "Create a relation object from the DataFrame in df",
727: 		"args": [
728: 			{
729: 				"name": "df",
730: 				"type": "pandas.DataFrame"
731: 			}
732: 		],
733: 		"return": "DuckDBPyRelation"
734: 	},
735: 	{
736: 		"name": "from_arrow",
737: 		"function": "FromArrow",
738: 		"docs": "Create a relation object from an Arrow object",
739: 		"args": [
740: 			{
741: 				"name": "arrow_object",
742: 				"type": "object"
743: 			}
744: 		],
745: 		"return": "DuckDBPyRelation"
746: 	},
747: 	{
748: 		"name": ["from_parquet", "read_parquet"],
749: 		"function": "FromParquet",
750: 		"docs": "Create a relation object from the Parquet files in file_glob",
751: 		"args": [
752: 			{
753: 				"name": "file_glob",
754: 				"type": "str"
755: 			},
756: 			{
757: 				"name": "binary_as_string",
758: 				"default": "False",
759: 				"type": "bool"
760: 			}
761: 		],
762: 		"kwargs": [
763: 			{
764: 				"name": "file_row_number",
765: 				"default": "False",
766: 				"type": "bool"
767: 			},
768: 			{
769: 				"name": "filename",
770: 				"default": "False",
771: 				"type": "bool"
772: 			},
773: 			{
774: 				"name": "hive_partitioning",
775: 				"default": "False",
776: 				"type": "bool"
777: 			},
778: 			{
779: 				"name": "union_by_name",
780: 				"default": "False",
781: 				"type": "bool"
782: 			},
783: 			{
784: 				"name": "compression",
785: 				"default": "None",
786: 				"type": "Optional[str]"
787: 			}
788: 		],
789: 		"return": "DuckDBPyRelation"
790: 	},
791: 	{
792: 		"name": ["from_parquet", "read_parquet"],
793: 		"function": "FromParquets",
794: 		"docs": "Create a relation object from the Parquet files in file_globs",
795: 		"args": [
796: 			{
797: 				"name": "file_globs",
798: 				"type": "str"
799: 			},
800: 			{
801: 				"name": "binary_as_string",
802: 				"default": "False",
803: 				"type": "bool"
804: 			}
805: 		],
806: 		"kwargs": [
807: 			{
808: 				"name": "file_row_number",
809: 				"default": "False",
810: 				"type": "bool"
811: 			},
812: 			{
813: 				"name": "filename",
814: 				"default": "False",
815: 				"type": "bool"
816: 			},
817: 			{
818: 				"name": "hive_partitioning",
819: 				"default": "False",
820: 				"type": "bool"
821: 			},
822: 			{
823: 				"name": "union_by_name",
824: 				"default": "False",
825: 				"type": "bool"
826: 			},
827: 			{
828: 				"name": "compression",
829: 				"default": "None",
830: 				"type": "str"
831: 			}
832: 		],
833: 		"return": "DuckDBPyRelation"
834: 	},
835: 	{
836: 		"name": "from_substrait",
837: 		"function": "FromSubstrait",
838: 		"docs": "Create a query object from protobuf plan",
839: 		"args": [
840: 			{
841: 				"name": "proto",
842: 				"type": "str"
843: 			}
844: 		],
845: 		"return": "DuckDBPyRelation"
846: 	},
847: 	{
848: 		"name": "get_substrait",
849: 		"function": "GetSubstrait",
850: 		"docs": "Serialize a query to protobuf",
851: 		"args": [
852: 			{
853: 				"name": "query",
854: 				"type": "str"
855: 			}
856: 		],
857: 		"kwargs": [
858: 			{
859: 				"name": "enable_optimizer",
860: 				"default": "True",
861: 				"type": "bool"
862: 			}
863: 		],
864: 		"return": "str"
865: 	},
866: 	{
867: 		"name": "get_substrait_json",
868: 		"function": "GetSubstraitJSON",
869: 		"docs": "Serialize a query to protobuf on the JSON format",
870: 		"args": [
871: 			{
872: 				"name": "query",
873: 				"type": "str"
874: 			}
875: 		],
876: 		"kwargs": [
877: 			{
878: 				"name": "enable_optimizer",
879: 				"default": "True",
880: 				"type": "bool"
881: 			}
882: 		],
883: 		"return": "str"
884: 	},
885: 	{
886: 		"name": "from_substrait_json",
887: 		"function": "FromSubstraitJSON",
888: 		"docs": "Create a query object from a JSON protobuf plan",
889: 		"args": [
890: 			{
891: 				"name": "json",
892: 				"type": "str"
893: 			}
894: 		],
895: 		"return": "DuckDBPyRelation"
896: 	},
897: 	{
898: 		"name": "get_table_names",
899: 		"function": "GetTableNames",
900: 		"docs": "Extract the required table names from a query",
901: 		"args": [
902: 			{
903: 				"name": "query",
904: 				"type": "str"
905: 			}
906: 		],
907: 		"return": "List[str]"
908: 	},
909: 	{
910: 		"name": "install_extension",
911: 		"function": "InstallExtension",
912: 		"docs": "Install an extension by name",
913: 		"args": [
914: 			{
915: 				"name": "extension",
916: 				"type": "str"
917: 			}
918: 		],
919: 		"kwargs": [
920: 			{
921: 				"name": "force_install",
922: 				"default": "False",
923: 				"type": "bool"
924: 			}
925: 		],
926: 		"return": "None"
927: 	},
928: 	{
929: 		"name": "load_extension",
930: 		"function": "LoadExtension",
931: 		"docs": "Load an installed extension",
932: 		"args": [
933: 			{
934: 				"name": "extension",
935: 				"type": "str"
936: 			}
937: 		],
938: 		"return": "None"
939: 	}
940: ]
[end of tools/pythonpkg/scripts/connection_methods.json]
[start of tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/pyconnection/pyconnection.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: #include "duckdb_python/arrow/arrow_array_stream.hpp"
11: #include "duckdb.hpp"
12: #include "duckdb_python/pybind11/pybind_wrapper.hpp"
13: #include "duckdb/common/unordered_map.hpp"
14: #include "duckdb_python/import_cache/python_import_cache.hpp"
15: #include "duckdb_python/numpy/numpy_type.hpp"
16: #include "duckdb_python/pyrelation.hpp"
17: #include "duckdb_python/pytype.hpp"
18: #include "duckdb_python/path_like.hpp"
19: #include "duckdb/execution/operator/csv_scanner/csv_reader_options.hpp"
20: #include "duckdb_python/pyfilesystem.hpp"
21: #include "duckdb_python/pybind11/registered_py_object.hpp"
22: #include "duckdb_python/python_dependency.hpp"
23: #include "duckdb/function/scalar_function.hpp"
24: #include "duckdb_python/pybind11/conversions/exception_handling_enum.hpp"
25: #include "duckdb_python/pybind11/conversions/python_udf_type_enum.hpp"
26: #include "duckdb/common/shared_ptr.hpp"
27: 
28: namespace duckdb {
29: 
30: enum class PythonEnvironmentType { NORMAL, INTERACTIVE, JUPYTER };
31: 
32: struct DuckDBPyRelation;
33: 
34: class RegisteredArrow : public RegisteredObject {
35: 
36: public:
37: 	RegisteredArrow(unique_ptr<PythonTableArrowArrayStreamFactory> arrow_factory_p, py::object obj_p)
38: 	    : RegisteredObject(std::move(obj_p)), arrow_factory(std::move(arrow_factory_p)) {};
39: 	unique_ptr<PythonTableArrowArrayStreamFactory> arrow_factory;
40: };
41: 
42: struct DuckDBPyConnection : public enable_shared_from_this<DuckDBPyConnection> {
43: public:
44: 	shared_ptr<DuckDB> database;
45: 	unique_ptr<Connection> connection;
46: 	unique_ptr<DuckDBPyRelation> result;
47: 	vector<weak_ptr<DuckDBPyConnection>> cursors;
48: 	unordered_map<string, shared_ptr<Relation>> temporary_views;
49: 	std::mutex py_connection_lock;
50: 	//! MemoryFileSystem used to temporarily store file-like objects for reading
51: 	shared_ptr<ModifiedMemoryFileSystem> internal_object_filesystem;
52: 	case_insensitive_map_t<unique_ptr<ExternalDependency>> registered_functions;
53: 
54: public:
55: 	explicit DuckDBPyConnection() {
56: 	}
57: 	~DuckDBPyConnection();
58: 
59: public:
60: 	static void Initialize(py::handle &m);
61: 	static void Cleanup();
62: 
63: 	shared_ptr<DuckDBPyConnection> Enter();
64: 
65: 	static void Exit(DuckDBPyConnection &self, const py::object &exc_type, const py::object &exc,
66: 	                 const py::object &traceback);
67: 
68: 	static bool DetectAndGetEnvironment();
69: 	static bool IsJupyter();
70: 	static shared_ptr<DuckDBPyConnection> DefaultConnection();
71: 	static PythonImportCache *ImportCache();
72: 	static bool IsInteractive();
73: 
74: 	unique_ptr<DuckDBPyRelation>
75: 	ReadCSV(const py::object &name, const py::object &header = py::none(), const py::object &compression = py::none(),
76: 	        const py::object &sep = py::none(), const py::object &delimiter = py::none(),
77: 	        const py::object &dtype = py::none(), const py::object &na_values = py::none(),
78: 	        const py::object &skiprows = py::none(), const py::object &quotechar = py::none(),
79: 	        const py::object &escapechar = py::none(), const py::object &encoding = py::none(),
80: 	        const py::object &parallel = py::none(), const py::object &date_format = py::none(),
81: 	        const py::object &timestamp_format = py::none(), const py::object &sample_size = py::none(),
82: 	        const py::object &all_varchar = py::none(), const py::object &normalize_names = py::none(),
83: 	        const py::object &filename = py::none(), const py::object &null_padding = py::none(),
84: 	        const py::object &names = py::none());
85: 
86: 	py::list ExtractStatements(const string &query);
87: 
88: 	unique_ptr<DuckDBPyRelation> ReadJSON(const string &filename, const Optional<py::object> &columns = py::none(),
89: 	                                      const Optional<py::object> &sample_size = py::none(),
90: 	                                      const Optional<py::object> &maximum_depth = py::none(),
91: 	                                      const Optional<py::str> &records = py::none(),
92: 	                                      const Optional<py::str> &format = py::none());
93: 
94: 	shared_ptr<DuckDBPyType> MapType(const shared_ptr<DuckDBPyType> &key_type,
95: 	                                 const shared_ptr<DuckDBPyType> &value_type);
96: 	shared_ptr<DuckDBPyType> StructType(const py::object &fields);
97: 	shared_ptr<DuckDBPyType> ListType(const shared_ptr<DuckDBPyType> &type);
98: 	shared_ptr<DuckDBPyType> ArrayType(const shared_ptr<DuckDBPyType> &type, idx_t size);
99: 	shared_ptr<DuckDBPyType> UnionType(const py::object &members);
100: 	shared_ptr<DuckDBPyType> EnumType(const string &name, const shared_ptr<DuckDBPyType> &type,
101: 	                                  const py::list &values_p);
102: 	shared_ptr<DuckDBPyType> DecimalType(int width, int scale);
103: 	shared_ptr<DuckDBPyType> StringType(const string &collation = string());
104: 	shared_ptr<DuckDBPyType> Type(const string &type_str);
105: 
106: 	shared_ptr<DuckDBPyConnection>
107: 	RegisterScalarUDF(const string &name, const py::function &udf, const py::object &arguments = py::none(),
108: 	                  const shared_ptr<DuckDBPyType> &return_type = nullptr, PythonUDFType type = PythonUDFType::NATIVE,
109: 	                  FunctionNullHandling null_handling = FunctionNullHandling::DEFAULT_NULL_HANDLING,
110: 	                  PythonExceptionHandling exception_handling = PythonExceptionHandling::FORWARD_ERROR,
111: 	                  bool side_effects = false);
112: 
113: 	shared_ptr<DuckDBPyConnection> UnregisterUDF(const string &name);
114: 
115: 	shared_ptr<DuckDBPyConnection> ExecuteMany(const py::object &query, py::object params = py::list());
116: 
117: 	void ExecuteImmediately(vector<unique_ptr<SQLStatement>> statements);
118: 	unique_ptr<PreparedStatement> PrepareQuery(unique_ptr<SQLStatement> statement);
119: 	unique_ptr<QueryResult> ExecuteInternal(PreparedStatement &prep, py::object params = py::list());
120: 
121: 	shared_ptr<DuckDBPyConnection> Execute(const py::object &query, py::object params = py::list());
122: 	shared_ptr<DuckDBPyConnection> ExecuteFromString(const string &query);
123: 
124: 	shared_ptr<DuckDBPyConnection> Append(const string &name, const PandasDataFrame &value, bool by_name);
125: 
126: 	shared_ptr<DuckDBPyConnection> RegisterPythonObject(const string &name, const py::object &python_object);
127: 
128: 	void InstallExtension(const string &extension, bool force_install = false);
129: 
130: 	void LoadExtension(const string &extension);
131: 
132: 	unique_ptr<DuckDBPyRelation> RunQuery(const py::object &query, string alias = "", py::object params = py::list());
133: 
134: 	unique_ptr<DuckDBPyRelation> Table(const string &tname);
135: 
136: 	unique_ptr<DuckDBPyRelation> Values(py::object params = py::none());
137: 
138: 	unique_ptr<DuckDBPyRelation> View(const string &vname);
139: 
140: 	unique_ptr<DuckDBPyRelation> TableFunction(const string &fname, py::object params = py::list());
141: 
142: 	unique_ptr<DuckDBPyRelation> FromDF(const PandasDataFrame &value);
143: 
144: 	unique_ptr<DuckDBPyRelation> FromParquet(const string &file_glob, bool binary_as_string, bool file_row_number,
145: 	                                         bool filename, bool hive_partitioning, bool union_by_name,
146: 	                                         const py::object &compression = py::none());
147: 
148: 	unique_ptr<DuckDBPyRelation> FromParquets(const vector<string> &file_globs, bool binary_as_string,
149: 	                                          bool file_row_number, bool filename, bool hive_partitioning,
150: 	                                          bool union_by_name, const py::object &compression = py::none());
151: 
152: 	unique_ptr<DuckDBPyRelation> FromArrow(py::object &arrow_object);
153: 
154: 	unique_ptr<DuckDBPyRelation> FromSubstrait(py::bytes &proto);
155: 
156: 	unique_ptr<DuckDBPyRelation> GetSubstrait(const string &query, bool enable_optimizer = true);
157: 
158: 	unique_ptr<DuckDBPyRelation> GetSubstraitJSON(const string &query, bool enable_optimizer = true);
159: 
160: 	unique_ptr<DuckDBPyRelation> FromSubstraitJSON(const string &json);
161: 
162: 	unordered_set<string> GetTableNames(const string &query);
163: 
164: 	shared_ptr<DuckDBPyConnection> UnregisterPythonObject(const string &name);
165: 
166: 	shared_ptr<DuckDBPyConnection> Begin();
167: 
168: 	shared_ptr<DuckDBPyConnection> Commit();
169: 
170: 	shared_ptr<DuckDBPyConnection> Rollback();
171: 
172: 	shared_ptr<DuckDBPyConnection> Checkpoint();
173: 
174: 	void Close();
175: 
176: 	void Interrupt();
177: 
178: 	ModifiedMemoryFileSystem &GetObjectFileSystem();
179: 
180: 	// cursor() is stupid
181: 	shared_ptr<DuckDBPyConnection> Cursor();
182: 
183: 	Optional<py::list> GetDescription();
184: 
185: 	int GetRowcount();
186: 
187: 	// these should be functions on the result but well
188: 	Optional<py::tuple> FetchOne();
189: 
190: 	py::list FetchMany(idx_t size);
191: 
192: 	py::list FetchAll();
193: 
194: 	py::dict FetchNumpy();
195: 	PandasDataFrame FetchDF(bool date_as_object);
196: 	PandasDataFrame FetchDFChunk(const idx_t vectors_per_chunk = 1, bool date_as_object = false) const;
197: 
198: 	duckdb::pyarrow::Table FetchArrow(idx_t rows_per_batch);
199: 	PolarsDataFrame FetchPolars(idx_t rows_per_batch);
200: 
201: 	py::dict FetchPyTorch();
202: 
203: 	py::dict FetchTF();
204: 
205: 	duckdb::pyarrow::RecordBatchReader FetchRecordBatchReader(const idx_t rows_per_batch) const;
206: 
207: 	static shared_ptr<DuckDBPyConnection> Connect(const string &database, bool read_only, const py::dict &config);
208: 
209: 	static vector<Value> TransformPythonParamList(const py::handle &params);
210: 	static case_insensitive_map_t<Value> TransformPythonParamDict(const py::dict &params);
211: 
212: 	void RegisterFilesystem(AbstractFileSystem filesystem);
213: 	void UnregisterFilesystem(const py::str &name);
214: 	py::list ListFilesystems();
215: 	bool FileSystemIsRegistered(const string &name);
216: 
217: 	//! Default connection to an in-memory database
218: 	static shared_ptr<DuckDBPyConnection> default_connection;
219: 	//! Caches and provides an interface to get frequently used modules+subtypes
220: 	static shared_ptr<PythonImportCache> import_cache;
221: 
222: 	static bool IsPandasDataframe(const py::object &object);
223: 	static bool IsPolarsDataframe(const py::object &object);
224: 	static bool IsAcceptedArrowObject(const py::object &object);
225: 	static NumpyObjectType IsAcceptedNumpyObject(const py::object &object);
226: 
227: 	static unique_ptr<QueryResult> CompletePendingQuery(PendingQueryResult &pending_query);
228: 
229: private:
230: 	PathLike GetPathLike(const py::object &object);
231: 	unique_lock<std::mutex> AcquireConnectionLock();
232: 	ScalarFunction CreateScalarUDF(const string &name, const py::function &udf, const py::object &parameters,
233: 	                               const shared_ptr<DuckDBPyType> &return_type, bool vectorized,
234: 	                               FunctionNullHandling null_handling, PythonExceptionHandling exception_handling,
235: 	                               bool side_effects);
236: 	void RegisterArrowObject(const py::object &arrow_object, const string &name);
237: 	vector<unique_ptr<SQLStatement>> GetStatements(const py::object &query);
238: 
239: 	static PythonEnvironmentType environment;
240: 	static void DetectEnvironment();
241: };
242: 
243: template <typename T>
244: static bool ModuleIsLoaded() {
245: 	auto dict = pybind11::module_::import("sys").attr("modules");
246: 	return dict.contains(py::str(T::Name));
247: }
248: 
249: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp]
[start of tools/pythonpkg/src/pyconnection.cpp]
1: #include "duckdb_python/pyconnection/pyconnection.hpp"
2: 
3: #include "duckdb/catalog/default/default_types.hpp"
4: #include "duckdb/common/arrow/arrow.hpp"
5: #include "duckdb/common/enums/file_compression_type.hpp"
6: #include "duckdb/common/printer.hpp"
7: #include "duckdb/common/types.hpp"
8: #include "duckdb/common/types/vector.hpp"
9: #include "duckdb/function/table/read_csv.hpp"
10: #include "duckdb/main/client_config.hpp"
11: #include "duckdb/main/client_context.hpp"
12: #include "duckdb/main/config.hpp"
13: #include "duckdb/main/db_instance_cache.hpp"
14: #include "duckdb/main/extension_helper.hpp"
15: #include "duckdb/main/prepared_statement.hpp"
16: #include "duckdb/main/relation/read_csv_relation.hpp"
17: #include "duckdb/main/relation/read_json_relation.hpp"
18: #include "duckdb/main/relation/value_relation.hpp"
19: #include "duckdb/parser/expression/constant_expression.hpp"
20: #include "duckdb/parser/expression/function_expression.hpp"
21: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
22: #include "duckdb/parser/parser.hpp"
23: #include "duckdb/parser/statement/select_statement.hpp"
24: #include "duckdb/parser/tableref/subqueryref.hpp"
25: #include "duckdb/parser/tableref/table_function_ref.hpp"
26: #include "duckdb_python/arrow/arrow_array_stream.hpp"
27: #include "duckdb_python/map.hpp"
28: #include "duckdb_python/pandas/pandas_scan.hpp"
29: #include "duckdb_python/pyrelation.hpp"
30: #include "duckdb_python/pystatement.hpp"
31: #include "duckdb_python/pyresult.hpp"
32: #include "duckdb_python/python_conversion.hpp"
33: #include "duckdb_python/numpy/numpy_type.hpp"
34: #include "duckdb/main/prepared_statement.hpp"
35: #include "duckdb_python/jupyter_progress_bar_display.hpp"
36: #include "duckdb_python/pyfilesystem.hpp"
37: #include "duckdb/main/client_config.hpp"
38: #include "duckdb/function/table/read_csv.hpp"
39: #include "duckdb/common/enums/file_compression_type.hpp"
40: #include "duckdb/catalog/default/default_types.hpp"
41: #include "duckdb/main/relation/value_relation.hpp"
42: #include "duckdb_python/filesystem_object.hpp"
43: #include "duckdb/parser/parsed_data/create_scalar_function_info.hpp"
44: #include "duckdb/function/scalar_function.hpp"
45: #include "duckdb_python/pandas/pandas_scan.hpp"
46: #include "duckdb_python/python_objects.hpp"
47: #include "duckdb/function/function.hpp"
48: #include "duckdb_python/pybind11/conversions/exception_handling_enum.hpp"
49: #include "duckdb/parser/parsed_data/drop_info.hpp"
50: #include "duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp"
51: #include "duckdb/main/pending_query_result.hpp"
52: #include "duckdb/parser/keyword_helper.hpp"
53: #include "duckdb_python/python_replacement_scan.hpp"
54: #include "duckdb/common/shared_ptr.hpp"
55: #include "duckdb/main/materialized_query_result.hpp"
56: #include "duckdb/main/stream_query_result.hpp"
57: #include "duckdb/main/relation/materialized_relation.hpp"
58: #include "duckdb/main/relation/query_relation.hpp"
59: 
60: #include <random>
61: 
62: #include "duckdb/common/printer.hpp"
63: 
64: namespace duckdb {
65: 
66: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::default_connection = nullptr;       // NOLINT: allow global
67: DBInstanceCache instance_cache;                                                        // NOLINT: allow global
68: shared_ptr<PythonImportCache> DuckDBPyConnection::import_cache = nullptr;              // NOLINT: allow global
69: PythonEnvironmentType DuckDBPyConnection::environment = PythonEnvironmentType::NORMAL; // NOLINT: allow global
70: 
71: DuckDBPyConnection::~DuckDBPyConnection() {
72: 	try {
73: 		py::gil_scoped_release gil;
74: 		// Release any structures that do not need to hold the GIL here
75: 		database.reset();
76: 		connection.reset();
77: 		temporary_views.clear();
78: 	} catch (...) { // NOLINT
79: 	}
80: }
81: 
82: void DuckDBPyConnection::DetectEnvironment() {
83: 	// If __main__ does not have a __file__ attribute, we are in interactive mode
84: 	auto main_module = py::module_::import("__main__");
85: 	if (py::hasattr(main_module, "__file__")) {
86: 		return;
87: 	}
88: 	DuckDBPyConnection::environment = PythonEnvironmentType::INTERACTIVE;
89: 	if (!ModuleIsLoaded<IpythonCacheItem>()) {
90: 		return;
91: 	}
92: 
93: 	// Check to see if we are in a Jupyter Notebook
94: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
95: 	auto get_ipython = import_cache_py.IPython.get_ipython();
96: 	if (get_ipython.ptr() == nullptr) {
97: 		// Could either not load the IPython module, or it has no 'get_ipython' attribute
98: 		return;
99: 	}
100: 	auto ipython = get_ipython();
101: 	if (!py::hasattr(ipython, "config")) {
102: 		return;
103: 	}
104: 	py::dict ipython_config = ipython.attr("config");
105: 	if (ipython_config.contains("IPKernelApp")) {
106: 		DuckDBPyConnection::environment = PythonEnvironmentType::JUPYTER;
107: 	}
108: 	return;
109: }
110: 
111: bool DuckDBPyConnection::DetectAndGetEnvironment() {
112: 	DuckDBPyConnection::DetectEnvironment();
113: 	return DuckDBPyConnection::IsInteractive();
114: }
115: 
116: bool DuckDBPyConnection::IsJupyter() {
117: 	return DuckDBPyConnection::environment == PythonEnvironmentType::JUPYTER;
118: }
119: 
120: // NOTE: this function is generated by tools/pythonpkg/scripts/generate_connection_methods.py.
121: // Do not edit this function manually, your changes will be overwritten!
122: 
123: static void InitializeConnectionMethods(py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>> &m) {
124: 	m.def("cursor", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection");
125: 	m.def("register_filesystem", &DuckDBPyConnection::RegisterFilesystem, "Register a fsspec compliant filesystem",
126: 	      py::arg("filesystem"));
127: 	m.def("unregister_filesystem", &DuckDBPyConnection::UnregisterFilesystem, "Unregister a filesystem",
128: 	      py::arg("name"));
129: 	m.def("list_filesystems", &DuckDBPyConnection::ListFilesystems,
130: 	      "List registered filesystems, including builtin ones");
131: 	m.def("filesystem_is_registered", &DuckDBPyConnection::FileSystemIsRegistered,
132: 	      "Check if a filesystem with the provided name is currently registered", py::arg("name"));
133: 	m.def("create_function", &DuckDBPyConnection::RegisterScalarUDF,
134: 	      "Create a DuckDB function out of the passing in Python function so it can be used in queries",
135: 	      py::arg("name"), py::arg("function"), py::arg("parameters") = py::none(), py::arg("return_type") = py::none(),
136: 	      py::kw_only(), py::arg("type") = PythonUDFType::NATIVE,
137: 	      py::arg("null_handling") = FunctionNullHandling::DEFAULT_NULL_HANDLING,
138: 	      py::arg("exception_handling") = PythonExceptionHandling::FORWARD_ERROR, py::arg("side_effects") = false);
139: 	m.def("remove_function", &DuckDBPyConnection::UnregisterUDF, "Remove a previously created function",
140: 	      py::arg("name"));
141: 	m.def("sqltype", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
142: 	      py::arg("type_str"));
143: 	m.def("dtype", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
144: 	      py::arg("type_str"));
145: 	m.def("type", &DuckDBPyConnection::Type, "Create a type object by parsing the 'type_str' string",
146: 	      py::arg("type_str"));
147: 	m.def("array_type", &DuckDBPyConnection::ArrayType, "Create an array type object of 'type'",
148: 	      py::arg("type").none(false), py::arg("size"));
149: 	m.def("list_type", &DuckDBPyConnection::ListType, "Create a list type object of 'type'",
150: 	      py::arg("type").none(false));
151: 	m.def("union_type", &DuckDBPyConnection::UnionType, "Create a union type object from 'members'",
152: 	      py::arg("members").none(false));
153: 	m.def("string_type", &DuckDBPyConnection::StringType, "Create a string type with an optional collation",
154: 	      py::arg("collation") = "");
155: 	m.def("enum_type", &DuckDBPyConnection::EnumType,
156: 	      "Create an enum type of underlying 'type', consisting of the list of 'values'", py::arg("name"),
157: 	      py::arg("type"), py::arg("values"));
158: 	m.def("decimal_type", &DuckDBPyConnection::DecimalType, "Create a decimal type with 'width' and 'scale'",
159: 	      py::arg("width"), py::arg("scale"));
160: 	m.def("struct_type", &DuckDBPyConnection::StructType, "Create a struct type object from 'fields'",
161: 	      py::arg("fields"));
162: 	m.def("row_type", &DuckDBPyConnection::StructType, "Create a struct type object from 'fields'", py::arg("fields"));
163: 	m.def("map_type", &DuckDBPyConnection::MapType, "Create a map type object from 'key_type' and 'value_type'",
164: 	      py::arg("key").none(false), py::arg("value").none(false));
165: 	m.def("duplicate", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection");
166: 	m.def("execute", &DuckDBPyConnection::Execute,
167: 	      "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
168: 	      py::arg("parameters") = py::none());
169: 	m.def("executemany", &DuckDBPyConnection::ExecuteMany,
170: 	      "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
171: 	      py::arg("query"), py::arg("parameters") = py::none());
172: 	m.def("close", &DuckDBPyConnection::Close, "Close the connection");
173: 	m.def("interrupt", &DuckDBPyConnection::Interrupt, "Interrupt pending operations");
174: 	m.def("fetchone", &DuckDBPyConnection::FetchOne, "Fetch a single row from a result following execute");
175: 	m.def("fetchmany", &DuckDBPyConnection::FetchMany, "Fetch the next set of rows from a result following execute",
176: 	      py::arg("size") = 1);
177: 	m.def("fetchall", &DuckDBPyConnection::FetchAll, "Fetch all rows from a result following execute");
178: 	m.def("fetchnumpy", &DuckDBPyConnection::FetchNumpy, "Fetch a result as list of NumPy arrays following execute");
179: 	m.def("fetchdf", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
180: 	      py::arg("date_as_object") = false);
181: 	m.def("fetch_df", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
182: 	      py::arg("date_as_object") = false);
183: 	m.def("df", &DuckDBPyConnection::FetchDF, "Fetch a result as DataFrame following execute()", py::kw_only(),
184: 	      py::arg("date_as_object") = false);
185: 	m.def("fetch_df_chunk", &DuckDBPyConnection::FetchDFChunk,
186: 	      "Fetch a chunk of the result as DataFrame following execute()", py::arg("vectors_per_chunk") = 1,
187: 	      py::kw_only(), py::arg("date_as_object") = false);
188: 	m.def("pl", &DuckDBPyConnection::FetchPolars, "Fetch a result as Polars DataFrame following execute()",
189: 	      py::arg("rows_per_batch") = 1000000);
190: 	m.def("fetch_arrow_table", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
191: 	      py::arg("rows_per_batch") = 1000000);
192: 	m.def("arrow", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()",
193: 	      py::arg("rows_per_batch") = 1000000);
194: 	m.def("fetch_record_batch", &DuckDBPyConnection::FetchRecordBatchReader,
195: 	      "Fetch an Arrow RecordBatchReader following execute()", py::arg("rows_per_batch") = 1000000);
196: 	m.def("torch", &DuckDBPyConnection::FetchPyTorch, "Fetch a result as dict of PyTorch Tensors following execute()");
197: 	m.def("tf", &DuckDBPyConnection::FetchTF, "Fetch a result as dict of TensorFlow Tensors following execute()");
198: 	m.def("begin", &DuckDBPyConnection::Begin, "Start a new transaction");
199: 	m.def("commit", &DuckDBPyConnection::Commit, "Commit changes performed within a transaction");
200: 	m.def("rollback", &DuckDBPyConnection::Rollback, "Roll back changes performed within a transaction");
201: 	m.def("checkpoint", &DuckDBPyConnection::Checkpoint,
202: 	      "Synchronizes data in the write-ahead log (WAL) to the database data file (no-op for in-memory connections)");
203: 	m.def("append", &DuckDBPyConnection::Append, "Append the passed DataFrame to the named table",
204: 	      py::arg("table_name"), py::arg("df"), py::kw_only(), py::arg("by_name") = false);
205: 	m.def("register", &DuckDBPyConnection::RegisterPythonObject,
206: 	      "Register the passed Python Object value for querying with a view", py::arg("view_name"),
207: 	      py::arg("python_object"));
208: 	m.def("unregister", &DuckDBPyConnection::UnregisterPythonObject, "Unregister the view name", py::arg("view_name"));
209: 	m.def("table", &DuckDBPyConnection::Table, "Create a relation object for the named table", py::arg("table_name"));
210: 	m.def("view", &DuckDBPyConnection::View, "Create a relation object for the named view", py::arg("view_name"));
211: 	m.def("values", &DuckDBPyConnection::Values, "Create a relation object from the passed values", py::arg("values"));
212: 	m.def("table_function", &DuckDBPyConnection::TableFunction,
213: 	      "Create a relation object from the named table function with given parameters", py::arg("name"),
214: 	      py::arg("parameters") = py::none());
215: 	m.def("read_json", &DuckDBPyConnection::ReadJSON, "Create a relation object from the JSON file in 'name'",
216: 	      py::arg("name"), py::kw_only(), py::arg("columns") = py::none(), py::arg("sample_size") = py::none(),
217: 	      py::arg("maximum_depth") = py::none(), py::arg("records") = py::none(), py::arg("format") = py::none());
218: 	m.def("extract_statements", &DuckDBPyConnection::ExtractStatements,
219: 	      "Parse the query string and extract the Statement object(s) produced", py::arg("query"));
220: 	m.def("sql", &DuckDBPyConnection::RunQuery,
221: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
222: 	      "run the query as-is.",
223: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
224: 	m.def("query", &DuckDBPyConnection::RunQuery,
225: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
226: 	      "run the query as-is.",
227: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
228: 	m.def("from_query", &DuckDBPyConnection::RunQuery,
229: 	      "Run a SQL query. If it is a SELECT statement, create a relation object from the given SQL query, otherwise "
230: 	      "run the query as-is.",
231: 	      py::arg("query"), py::kw_only(), py::arg("alias") = "", py::arg("params") = py::none());
232: 	m.def("read_csv", &DuckDBPyConnection::ReadCSV, "Create a relation object from the CSV file in 'name'",
233: 	      py::arg("path_or_buffer"), py::kw_only(), py::arg("header") = py::none(), py::arg("compression") = py::none(),
234: 	      py::arg("sep") = py::none(), py::arg("delimiter") = py::none(), py::arg("dtype") = py::none(),
235: 	      py::arg("na_values") = py::none(), py::arg("skiprows") = py::none(), py::arg("quotechar") = py::none(),
236: 	      py::arg("escapechar") = py::none(), py::arg("encoding") = py::none(), py::arg("parallel") = py::none(),
237: 	      py::arg("date_format") = py::none(), py::arg("timestamp_format") = py::none(),
238: 	      py::arg("sample_size") = py::none(), py::arg("all_varchar") = py::none(),
239: 	      py::arg("normalize_names") = py::none(), py::arg("filename") = py::none(),
240: 	      py::arg("null_padding") = py::none(), py::arg("names") = py::none());
241: 	m.def("from_csv_auto", &DuckDBPyConnection::ReadCSV, "Create a relation object from the CSV file in 'name'",
242: 	      py::arg("path_or_buffer"), py::kw_only(), py::arg("header") = py::none(), py::arg("compression") = py::none(),
243: 	      py::arg("sep") = py::none(), py::arg("delimiter") = py::none(), py::arg("dtype") = py::none(),
244: 	      py::arg("na_values") = py::none(), py::arg("skiprows") = py::none(), py::arg("quotechar") = py::none(),
245: 	      py::arg("escapechar") = py::none(), py::arg("encoding") = py::none(), py::arg("parallel") = py::none(),
246: 	      py::arg("date_format") = py::none(), py::arg("timestamp_format") = py::none(),
247: 	      py::arg("sample_size") = py::none(), py::arg("all_varchar") = py::none(),
248: 	      py::arg("normalize_names") = py::none(), py::arg("filename") = py::none(),
249: 	      py::arg("null_padding") = py::none(), py::arg("names") = py::none());
250: 	m.def("from_df", &DuckDBPyConnection::FromDF, "Create a relation object from the DataFrame in df", py::arg("df"));
251: 	m.def("from_arrow", &DuckDBPyConnection::FromArrow, "Create a relation object from an Arrow object",
252: 	      py::arg("arrow_object"));
253: 	m.def("from_parquet", &DuckDBPyConnection::FromParquet,
254: 	      "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
255: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
256: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
257: 	      py::arg("compression") = py::none());
258: 	m.def("read_parquet", &DuckDBPyConnection::FromParquet,
259: 	      "Create a relation object from the Parquet files in file_glob", py::arg("file_glob"),
260: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
261: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
262: 	      py::arg("compression") = py::none());
263: 	m.def("from_parquet", &DuckDBPyConnection::FromParquets,
264: 	      "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
265: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
266: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
267: 	      py::arg("compression") = py::none());
268: 	m.def("read_parquet", &DuckDBPyConnection::FromParquets,
269: 	      "Create a relation object from the Parquet files in file_globs", py::arg("file_globs"),
270: 	      py::arg("binary_as_string") = false, py::kw_only(), py::arg("file_row_number") = false,
271: 	      py::arg("filename") = false, py::arg("hive_partitioning") = false, py::arg("union_by_name") = false,
272: 	      py::arg("compression") = py::none());
273: 	m.def("from_substrait", &DuckDBPyConnection::FromSubstrait, "Create a query object from protobuf plan",
274: 	      py::arg("proto"));
275: 	m.def("get_substrait", &DuckDBPyConnection::GetSubstrait, "Serialize a query to protobuf", py::arg("query"),
276: 	      py::kw_only(), py::arg("enable_optimizer") = true);
277: 	m.def("get_substrait_json", &DuckDBPyConnection::GetSubstraitJSON,
278: 	      "Serialize a query to protobuf on the JSON format", py::arg("query"), py::kw_only(),
279: 	      py::arg("enable_optimizer") = true);
280: 	m.def("from_substrait_json", &DuckDBPyConnection::FromSubstraitJSON,
281: 	      "Create a query object from a JSON protobuf plan", py::arg("json"));
282: 	m.def("get_table_names", &DuckDBPyConnection::GetTableNames, "Extract the required table names from a query",
283: 	      py::arg("query"));
284: 	m.def("install_extension", &DuckDBPyConnection::InstallExtension, "Install an extension by name",
285: 	      py::arg("extension"), py::kw_only(), py::arg("force_install") = false);
286: 	m.def("load_extension", &DuckDBPyConnection::LoadExtension, "Load an installed extension", py::arg("extension"));
287: } // END_OF_CONNECTION_METHODS
288: 
289: void DuckDBPyConnection::UnregisterFilesystem(const py::str &name) {
290: 	auto &fs = database->GetFileSystem();
291: 
292: 	fs.UnregisterSubSystem(name);
293: }
294: 
295: void DuckDBPyConnection::RegisterFilesystem(AbstractFileSystem filesystem) {
296: 	PythonGILWrapper gil_wrapper;
297: 
298: 	if (!py::isinstance<AbstractFileSystem>(filesystem)) {
299: 		throw InvalidInputException("Bad filesystem instance");
300: 	}
301: 
302: 	auto &fs = database->GetFileSystem();
303: 
304: 	auto protocol = filesystem.attr("protocol");
305: 	if (protocol.is_none() || py::str("abstract").equal(protocol)) {
306: 		throw InvalidInputException("Must provide concrete fsspec implementation");
307: 	}
308: 
309: 	vector<string> protocols;
310: 	if (py::isinstance<py::str>(protocol)) {
311: 		protocols.push_back(py::str(protocol));
312: 	} else {
313: 		for (const auto &sub_protocol : protocol) {
314: 			protocols.push_back(py::str(sub_protocol));
315: 		}
316: 	}
317: 
318: 	fs.RegisterSubSystem(make_uniq<PythonFilesystem>(std::move(protocols), std::move(filesystem)));
319: }
320: 
321: py::list DuckDBPyConnection::ListFilesystems() {
322: 	auto subsystems = database->GetFileSystem().ListSubSystems();
323: 	py::list names;
324: 	for (auto &name : subsystems) {
325: 		names.append(py::str(name));
326: 	}
327: 	return names;
328: }
329: 
330: py::list DuckDBPyConnection::ExtractStatements(const string &query) {
331: 	if (!connection) {
332: 		throw ConnectionException("Connection already closed!");
333: 	}
334: 	py::list result;
335: 	auto statements = connection->ExtractStatements(query);
336: 	for (auto &statement : statements) {
337: 		result.append(make_uniq<DuckDBPyStatement>(std::move(statement)));
338: 	}
339: 	return result;
340: }
341: 
342: bool DuckDBPyConnection::FileSystemIsRegistered(const string &name) {
343: 	auto subsystems = database->GetFileSystem().ListSubSystems();
344: 	return std::find(subsystems.begin(), subsystems.end(), name) != subsystems.end();
345: }
346: 
347: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::UnregisterUDF(const string &name) {
348: 	if (!connection) {
349: 		throw ConnectionException("Connection already closed!");
350: 	}
351: 	auto entry = registered_functions.find(name);
352: 	if (entry == registered_functions.end()) {
353: 		// Not registered or already unregistered
354: 		throw InvalidInputException("No function by the name of '%s' was found in the list of registered functions",
355: 		                            name);
356: 	}
357: 
358: 	auto &context = *connection->context;
359: 
360: 	context.RunFunctionInTransaction([&]() {
361: 		// create function
362: 		auto &catalog = Catalog::GetCatalog(context, SYSTEM_CATALOG);
363: 		DropInfo info;
364: 		info.type = CatalogType::SCALAR_FUNCTION_ENTRY;
365: 		info.name = name;
366: 		info.allow_drop_internal = true;
367: 		info.cascade = false;
368: 		info.if_not_found = OnEntryNotFound::THROW_EXCEPTION;
369: 		catalog.DropEntry(context, info);
370: 	});
371: 	registered_functions.erase(entry);
372: 
373: 	return shared_from_this();
374: }
375: 
376: shared_ptr<DuckDBPyConnection>
377: DuckDBPyConnection::RegisterScalarUDF(const string &name, const py::function &udf, const py::object &parameters_p,
378:                                       const shared_ptr<DuckDBPyType> &return_type_p, PythonUDFType type,
379:                                       FunctionNullHandling null_handling, PythonExceptionHandling exception_handling,
380:                                       bool side_effects) {
381: 	if (!connection) {
382: 		throw ConnectionException("Connection already closed!");
383: 	}
384: 	auto &context = *connection->context;
385: 
386: 	if (context.transaction.HasActiveTransaction()) {
387: 		throw InvalidInputException(
388: 		    "This function can not be called with an active transaction!, commit or abort the existing one first");
389: 	}
390: 	if (registered_functions.find(name) != registered_functions.end()) {
391: 		throw NotImplementedException("A function by the name of '%s' is already created, creating multiple "
392: 		                              "functions with the same name is not supported yet, please remove it first",
393: 		                              name);
394: 	}
395: 	auto scalar_function = CreateScalarUDF(name, udf, parameters_p, return_type_p, type == PythonUDFType::ARROW,
396: 	                                       null_handling, exception_handling, side_effects);
397: 	CreateScalarFunctionInfo info(scalar_function);
398: 
399: 	context.RegisterFunction(info);
400: 
401: 	auto dependency = make_uniq<ExternalDependency>();
402: 	dependency->AddDependency("function", PythonDependencyItem::Create(udf));
403: 	registered_functions[name] = std::move(dependency);
404: 
405: 	return shared_from_this();
406: }
407: 
408: void DuckDBPyConnection::Initialize(py::handle &m) {
409: 	auto connection_module =
410: 	    py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection", py::module_local());
411: 
412: 	connection_module.def("__enter__", &DuckDBPyConnection::Enter)
413: 	    .def("__exit__", &DuckDBPyConnection::Exit, py::arg("exc_type"), py::arg("exc"), py::arg("traceback"));
414: 	connection_module.def("__del__", &DuckDBPyConnection::Close);
415: 
416: 	InitializeConnectionMethods(connection_module);
417: 	connection_module.def_property_readonly("description", &DuckDBPyConnection::GetDescription,
418: 	                                        "Get result set attributes, mainly column names");
419: 	connection_module.def_property_readonly("rowcount", &DuckDBPyConnection::GetRowcount, "Get result set row count");
420: 	PyDateTime_IMPORT; // NOLINT
421: 	DuckDBPyConnection::ImportCache();
422: }
423: 
424: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::ExecuteMany(const py::object &query, py::object params_p) {
425: 	result.reset();
426: 	if (params_p.is_none()) {
427: 		params_p = py::list();
428: 	}
429: 
430: 	auto statements = GetStatements(query);
431: 	if (statements.empty()) {
432: 		// TODO: should we throw?
433: 		return nullptr;
434: 	}
435: 
436: 	auto last_statement = std::move(statements.back());
437: 	statements.pop_back();
438: 	// First immediately execute any preceding statements (if any)
439: 	// FIXME: DBAPI says to not accept an 'executemany' call with multiple statements
440: 	ExecuteImmediately(std::move(statements));
441: 
442: 	auto prep = PrepareQuery(std::move(last_statement));
443: 
444: 	if (!py::is_list_like(params_p)) {
445: 		throw InvalidInputException("executemany requires a list of parameter sets to be provided");
446: 	}
447: 	auto outer_list = py::list(params_p);
448: 	if (outer_list.empty()) {
449: 		throw InvalidInputException("executemany requires a non-empty list of parameter sets to be provided");
450: 	}
451: 
452: 	unique_ptr<QueryResult> query_result;
453: 	// Execute once for every set of parameters that are provided
454: 	for (auto &parameters : outer_list) {
455: 		auto params = py::reinterpret_borrow<py::object>(parameters);
456: 		query_result = ExecuteInternal(*prep, std::move(params));
457: 	}
458: 	// Set the internal 'result' object
459: 	if (query_result) {
460: 		auto py_result = make_uniq<DuckDBPyResult>(std::move(query_result));
461: 		result = make_uniq<DuckDBPyRelation>(std::move(py_result));
462: 	}
463: 
464: 	return shared_from_this();
465: }
466: 
467: static std::function<bool(PendingExecutionResult)> FinishedCondition(PendingQueryResult &pending_query) {
468: 	if (pending_query.AllowStreamResult()) {
469: 		return PendingQueryResult::IsFinishedOrBlocked;
470: 	}
471: 	return PendingQueryResult::IsFinished;
472: }
473: 
474: unique_ptr<QueryResult> DuckDBPyConnection::CompletePendingQuery(PendingQueryResult &pending_query) {
475: 	PendingExecutionResult execution_result;
476: 	auto is_finished = FinishedCondition(pending_query);
477: 	while (!is_finished(execution_result = pending_query.ExecuteTask())) {
478: 		{
479: 			py::gil_scoped_acquire gil;
480: 			if (PyErr_CheckSignals() != 0) {
481: 				throw std::runtime_error("Query interrupted");
482: 			}
483: 		}
484: 		if (execution_result == PendingExecutionResult::BLOCKED) {
485: 			pending_query.WaitForTask();
486: 		}
487: 	}
488: 	if (execution_result == PendingExecutionResult::EXECUTION_ERROR) {
489: 		pending_query.ThrowError();
490: 	}
491: 	return pending_query.Execute();
492: }
493: 
494: py::list TransformNamedParameters(const case_insensitive_map_t<idx_t> &named_param_map, const py::dict &params) {
495: 	py::list new_params(params.size());
496: 
497: 	for (auto &item : params) {
498: 		const std::string &item_name = item.first.cast<std::string>();
499: 		auto entry = named_param_map.find(item_name);
500: 		if (entry == named_param_map.end()) {
501: 			throw InvalidInputException(
502: 			    "Named parameters could not be transformed, because query string is missing named parameter '%s'",
503: 			    item_name);
504: 		}
505: 		auto param_idx = entry->second;
506: 		// Add the value of the named parameter to the list
507: 		new_params[param_idx - 1] = item.second;
508: 	}
509: 
510: 	if (named_param_map.size() != params.size()) {
511: 		// One or more named parameters were expected, but not found
512: 		vector<string> missing_params;
513: 		missing_params.reserve(named_param_map.size());
514: 		for (auto &entry : named_param_map) {
515: 			auto &name = entry.first;
516: 			if (!params.contains(name)) {
517: 				missing_params.push_back(name);
518: 			}
519: 		}
520: 		auto message = StringUtil::Join(missing_params, ", ");
521: 		throw InvalidInputException("Not all named parameters have been located, missing: %s", message);
522: 	}
523: 
524: 	return new_params;
525: }
526: 
527: case_insensitive_map_t<Value> TransformPreparedParameters(PreparedStatement &prep, const py::object &params) {
528: 	case_insensitive_map_t<Value> named_values;
529: 	if (py::is_list_like(params)) {
530: 		if (prep.n_param != py::len(params)) {
531: 			throw InvalidInputException("Prepared statement needs %d parameters, %d given", prep.n_param,
532: 			                            py::len(params));
533: 		}
534: 		auto unnamed_values = DuckDBPyConnection::TransformPythonParamList(params);
535: 		for (idx_t i = 0; i < unnamed_values.size(); i++) {
536: 			auto &value = unnamed_values[i];
537: 			auto identifier = std::to_string(i + 1);
538: 			named_values[identifier] = std::move(value);
539: 		}
540: 	} else if (py::is_dict_like(params)) {
541: 		auto dict = py::cast<py::dict>(params);
542: 		named_values = DuckDBPyConnection::TransformPythonParamDict(dict);
543: 	} else {
544: 		throw InvalidInputException("Prepared parameters can only be passed as a list or a dictionary");
545: 	}
546: 	return named_values;
547: }
548: 
549: unique_ptr<PreparedStatement> DuckDBPyConnection::PrepareQuery(unique_ptr<SQLStatement> statement) {
550: 	unique_ptr<PreparedStatement> prep;
551: 	{
552: 		py::gil_scoped_release release;
553: 		unique_lock<mutex> lock(py_connection_lock);
554: 
555: 		prep = connection->Prepare(std::move(statement));
556: 		if (prep->HasError()) {
557: 			prep->error.Throw();
558: 		}
559: 	}
560: 	return prep;
561: }
562: 
563: unique_ptr<QueryResult> DuckDBPyConnection::ExecuteInternal(PreparedStatement &prep, py::object params) {
564: 	if (!connection) {
565: 		throw ConnectionException("Connection has already been closed");
566: 	}
567: 	if (params.is_none()) {
568: 		params = py::list();
569: 	}
570: 
571: 	// Execute the prepared statement with the prepared parameters
572: 	auto named_values = TransformPreparedParameters(prep, params);
573: 	unique_ptr<QueryResult> res;
574: 	{
575: 		py::gil_scoped_release release;
576: 		unique_lock<std::mutex> lock(py_connection_lock);
577: 
578: 		auto pending_query = prep.PendingQuery(named_values);
579: 		res = CompletePendingQuery(*pending_query);
580: 
581: 		if (res->HasError()) {
582: 			res->ThrowError();
583: 		}
584: 	}
585: 	return res;
586: }
587: 
588: vector<unique_ptr<SQLStatement>> DuckDBPyConnection::GetStatements(const py::object &query) {
589: 	vector<unique_ptr<SQLStatement>> result;
590: 	if (!connection) {
591: 		throw ConnectionException("Connection has already been closed");
592: 	}
593: 
594: 	shared_ptr<DuckDBPyStatement> statement_obj;
595: 	if (py::try_cast(query, statement_obj)) {
596: 		result.push_back(statement_obj->GetStatement());
597: 		return result;
598: 	}
599: 	if (py::isinstance<py::str>(query)) {
600: 		auto sql_query = std::string(py::str(query));
601: 		return connection->ExtractStatements(sql_query);
602: 	}
603: 	throw InvalidInputException("Please provide either a DuckDBPyStatement or a string representing the query");
604: }
605: 
606: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::ExecuteFromString(const string &query) {
607: 	return Execute(py::str(query));
608: }
609: 
610: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Execute(const py::object &query, py::object params) {
611: 	result.reset();
612: 
613: 	auto statements = GetStatements(query);
614: 	if (statements.empty()) {
615: 		// TODO: should we throw?
616: 		return nullptr;
617: 	}
618: 
619: 	auto last_statement = std::move(statements.back());
620: 	statements.pop_back();
621: 	// First immediately execute any preceding statements (if any)
622: 	// FIXME: SQLites implementation says to not accept an 'execute' call with multiple statements
623: 	ExecuteImmediately(std::move(statements));
624: 
625: 	auto prep = PrepareQuery(std::move(last_statement));
626: 	auto res = ExecuteInternal(*prep, std::move(params));
627: 
628: 	// Set the internal 'result' object
629: 	if (res) {
630: 		auto py_result = make_uniq<DuckDBPyResult>(std::move(res));
631: 		result = make_uniq<DuckDBPyRelation>(std::move(py_result));
632: 	}
633: 	return shared_from_this();
634: }
635: 
636: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Append(const string &name, const PandasDataFrame &value,
637:                                                           bool by_name) {
638: 	RegisterPythonObject("__append_df", value);
639: 	string columns = "";
640: 	if (by_name) {
641: 		auto df_columns = value.attr("columns");
642: 		vector<string> column_names;
643: 		for (auto &column : df_columns) {
644: 			column_names.push_back(std::string(py::str(column)));
645: 		}
646: 		columns += "(";
647: 		for (idx_t i = 0; i < column_names.size(); i++) {
648: 			auto &column = column_names[i];
649: 			if (i != 0) {
650: 				columns += ", ";
651: 			}
652: 			columns += StringUtil::Format("%s", SQLIdentifier(column));
653: 		}
654: 		columns += ")";
655: 	}
656: 
657: 	auto sql_query = StringUtil::Format("INSERT INTO %s %s SELECT * FROM __append_df", SQLIdentifier(name), columns);
658: 	return Execute(py::str(sql_query));
659: }
660: 
661: void DuckDBPyConnection::RegisterArrowObject(const py::object &arrow_object, const string &name) {
662: 	auto stream_factory =
663: 	    make_uniq<PythonTableArrowArrayStreamFactory>(arrow_object.ptr(), connection->context->GetClientProperties());
664: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
665: 	auto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;
666: 	{
667: 		py::gil_scoped_release release;
668: 		temporary_views[name] =
669: 		    connection
670: 		        ->TableFunction("arrow_scan", {Value::POINTER(CastPointerToValue(stream_factory.get())),
671: 		                                       Value::POINTER(CastPointerToValue(stream_factory_produce)),
672: 		                                       Value::POINTER(CastPointerToValue(stream_factory_get_schema))})
673: 		        ->CreateView(name, true, true);
674: 	}
675: 	vector<shared_ptr<ExternalDependency>> dependencies;
676: 	auto dependency = make_shared_ptr<ExternalDependency>();
677: 	auto dependency_item =
678: 	    PythonDependencyItem::Create(make_uniq<RegisteredArrow>(std::move(stream_factory), arrow_object));
679: 	dependency->AddDependency("object", std::move(dependency_item));
680: 	dependencies.push_back(std::move(dependency));
681: 	connection->context->external_dependencies[name] = std::move(dependencies);
682: }
683: 
684: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::RegisterPythonObject(const string &name,
685:                                                                         const py::object &python_object) {
686: 	if (!connection) {
687: 		throw ConnectionException("Connection has already been closed");
688: 	}
689: 
690: 	if (DuckDBPyConnection::IsPandasDataframe(python_object)) {
691: 		if (PandasDataFrame::IsPyArrowBacked(python_object)) {
692: 			auto arrow_table = PandasDataFrame::ToArrowTable(python_object);
693: 			RegisterArrowObject(arrow_table, name);
694: 		} else {
695: 			auto new_df = PandasScanFunction::PandasReplaceCopiedNames(python_object);
696: 			{
697: 				py::gil_scoped_release release;
698: 				temporary_views[name] =
699: 				    connection->TableFunction("pandas_scan", {Value::POINTER(CastPointerToValue(new_df.ptr()))})
700: 				        ->CreateView(name, true, true);
701: 			}
702: 
703: 			auto dependency = make_shared_ptr<ExternalDependency>();
704: 			dependency->AddDependency("original", PythonDependencyItem::Create(python_object));
705: 			dependency->AddDependency("copy", PythonDependencyItem::Create(std::move(new_df)));
706: 
707: 			vector<shared_ptr<ExternalDependency>> dependencies;
708: 			dependencies.push_back(std::move(dependency));
709: 			connection->context->external_dependencies[name] = std::move(dependencies);
710: 		}
711: 	} else if (IsAcceptedArrowObject(python_object) || IsPolarsDataframe(python_object)) {
712: 		py::object arrow_object;
713: 		if (IsPolarsDataframe(python_object)) {
714: 			if (PolarsDataFrame::IsDataFrame(python_object)) {
715: 				arrow_object = python_object.attr("to_arrow")();
716: 			} else if (PolarsDataFrame::IsLazyFrame(python_object)) {
717: 				py::object materialized = python_object.attr("collect")();
718: 				arrow_object = materialized.attr("to_arrow")();
719: 			} else {
720: 				throw NotImplementedException("Unsupported Polars DF Type");
721: 			}
722: 		} else {
723: 			arrow_object = python_object;
724: 		}
725: 		RegisterArrowObject(arrow_object, name);
726: 	} else if (DuckDBPyRelation::IsRelation(python_object)) {
727: 		auto pyrel = py::cast<DuckDBPyRelation *>(python_object);
728: 		if (!pyrel->CanBeRegisteredBy(*connection)) {
729: 			throw InvalidInputException(
730: 			    "The relation you are attempting to register was not made from this connection");
731: 		}
732: 		pyrel->CreateView(name, true);
733: 	} else {
734: 		auto py_object_type = string(py::str(python_object.get_type().attr("__name__")));
735: 		throw InvalidInputException("Python Object %s not suitable to be registered as a view", py_object_type);
736: 	}
737: 	return shared_from_this();
738: }
739: 
740: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(const string &name, const Optional<py::object> &columns,
741:                                                           const Optional<py::object> &sample_size,
742:                                                           const Optional<py::object> &maximum_depth,
743:                                                           const Optional<py::str> &records,
744:                                                           const Optional<py::str> &format) {
745: 	if (!connection) {
746: 		throw ConnectionException("Connection has already been closed");
747: 	}
748: 
749: 	named_parameter_map_t options;
750: 
751: 	if (!py::none().is(columns)) {
752: 		if (!py::is_dict_like(columns)) {
753: 			throw BinderException("read_json only accepts 'columns' as a dict[str, str]");
754: 		}
755: 		py::dict columns_dict = columns;
756: 		child_list_t<Value> struct_fields;
757: 
758: 		for (auto &kv : columns_dict) {
759: 			auto &column_name = kv.first;
760: 			auto &type = kv.second;
761: 			if (!py::isinstance<py::str>(column_name)) {
762: 				string actual_type = py::str(column_name.get_type());
763: 				throw BinderException("The provided column name must be a str, not of type '%s'", actual_type);
764: 			}
765: 			if (!py::isinstance<py::str>(type)) {
766: 				string actual_type = py::str(column_name.get_type());
767: 				throw BinderException("The provided column type must be a str, not of type '%s'", actual_type);
768: 			}
769: 			struct_fields.emplace_back(py::str(column_name), Value(py::str(type)));
770: 		}
771: 		auto dtype_struct = Value::STRUCT(std::move(struct_fields));
772: 		options["columns"] = std::move(dtype_struct);
773: 	}
774: 
775: 	if (!py::none().is(records)) {
776: 		if (!py::isinstance<py::str>(records)) {
777: 			string actual_type = py::str(records.get_type());
778: 			throw BinderException("read_json only accepts 'records' as a string, not '%s'", actual_type);
779: 		}
780: 		auto records_s = py::reinterpret_borrow<py::str>(records);
781: 		auto records_option = std::string(py::str(records_s));
782: 		options["records"] = Value(records_option);
783: 	}
784: 
785: 	if (!py::none().is(format)) {
786: 		if (!py::isinstance<py::str>(format)) {
787: 			string actual_type = py::str(format.get_type());
788: 			throw BinderException("read_json only accepts 'format' as a string, not '%s'", actual_type);
789: 		}
790: 		auto format_s = py::reinterpret_borrow<py::str>(format);
791: 		auto format_option = std::string(py::str(format_s));
792: 		options["format"] = Value(format_option);
793: 	}
794: 
795: 	if (!py::none().is(sample_size)) {
796: 		if (!py::isinstance<py::int_>(sample_size)) {
797: 			string actual_type = py::str(sample_size.get_type());
798: 			throw BinderException("read_json only accepts 'sample_size' as an integer, not '%s'", actual_type);
799: 		}
800: 		options["sample_size"] = Value::INTEGER(py::int_(sample_size));
801: 	}
802: 
803: 	if (!py::none().is(maximum_depth)) {
804: 		if (!py::isinstance<py::int_>(maximum_depth)) {
805: 			string actual_type = py::str(maximum_depth.get_type());
806: 			throw BinderException("read_json only accepts 'maximum_depth' as an integer, not '%s'", actual_type);
807: 		}
808: 		options["maximum_depth"] = Value::INTEGER(py::int_(maximum_depth));
809: 	}
810: 
811: 	bool auto_detect = false;
812: 	if (!options.count("columns")) {
813: 		options["auto_detect"] = Value::BOOLEAN(true);
814: 		auto_detect = true;
815: 	}
816: 
817: 	auto read_json_relation =
818: 	    make_shared_ptr<ReadJSONRelation>(connection->context, name, std::move(options), auto_detect);
819: 	if (read_json_relation == nullptr) {
820: 		throw BinderException("read_json can only be used when the JSON extension is (statically) loaded");
821: 	}
822: 	return make_uniq<DuckDBPyRelation>(std::move(read_json_relation));
823: }
824: 
825: PathLike DuckDBPyConnection::GetPathLike(const py::object &object) {
826: 	return PathLike::Create(object, *this);
827: }
828: 
829: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadCSV(
830:     const py::object &name_p, const py::object &header, const py::object &compression, const py::object &sep,
831:     const py::object &delimiter, const py::object &dtype, const py::object &na_values, const py::object &skiprows,
832:     const py::object &quotechar, const py::object &escapechar, const py::object &encoding, const py::object &parallel,
833:     const py::object &date_format, const py::object &timestamp_format, const py::object &sample_size,
834:     const py::object &all_varchar, const py::object &normalize_names, const py::object &filename,
835:     const py::object &null_padding, const py::object &names_p) {
836: 	if (!connection) {
837: 		throw ConnectionException("Connection has already been closed");
838: 	}
839: 	CSVReaderOptions options;
840: 	auto path_like = GetPathLike(name_p);
841: 	auto &name = path_like.files;
842: 	auto file_like_object_wrapper = std::move(path_like.dependency);
843: 	named_parameter_map_t bind_parameters;
844: 
845: 	// First check if the header is explicitly set
846: 	// when false this affects the returned types, so it needs to be known at initialization of the relation
847: 	if (!py::none().is(header)) {
848: 
849: 		bool header_as_int = py::isinstance<py::int_>(header);
850: 		bool header_as_bool = py::isinstance<py::bool_>(header);
851: 
852: 		bool header_value;
853: 		if (header_as_bool) {
854: 			header_value = py::bool_(header);
855: 		} else if (header_as_int) {
856: 			if ((int)py::int_(header) != 0) {
857: 				throw InvalidInputException("read_csv only accepts 0 if 'header' is given as an integer");
858: 			}
859: 			header_value = true;
860: 		} else {
861: 			throw InvalidInputException("read_csv only accepts 'header' as an integer, or a boolean");
862: 		}
863: 		bind_parameters["header"] = Value::BOOLEAN(header_value);
864: 	}
865: 
866: 	if (!py::none().is(compression)) {
867: 		if (!py::isinstance<py::str>(compression)) {
868: 			throw InvalidInputException("read_csv only accepts 'compression' as a string");
869: 		}
870: 		bind_parameters["compression"] = Value(py::str(compression));
871: 	}
872: 
873: 	if (!py::none().is(dtype)) {
874: 		if (py::is_dict_like(dtype)) {
875: 			child_list_t<Value> struct_fields;
876: 			py::dict dtype_dict = dtype;
877: 			for (auto &kv : dtype_dict) {
878: 				shared_ptr<DuckDBPyType> sql_type;
879: 				if (!py::try_cast(kv.second, sql_type)) {
880: 					throw py::value_error("The types provided to 'dtype' have to be DuckDBPyType");
881: 				}
882: 				struct_fields.emplace_back(py::str(kv.first), Value(sql_type->ToString()));
883: 			}
884: 			auto dtype_struct = Value::STRUCT(std::move(struct_fields));
885: 			bind_parameters["dtypes"] = std::move(dtype_struct);
886: 		} else if (py::is_list_like(dtype)) {
887: 			vector<Value> list_values;
888: 			py::list dtype_list = dtype;
889: 			for (auto &child : dtype_list) {
890: 				shared_ptr<DuckDBPyType> sql_type;
891: 				if (!py::try_cast(child, sql_type)) {
892: 					throw py::value_error("The types provided to 'dtype' have to be DuckDBPyType");
893: 				}
894: 				list_values.push_back(sql_type->ToString());
895: 			}
896: 			bind_parameters["dtypes"] = Value::LIST(LogicalType::VARCHAR, std::move(list_values));
897: 		} else {
898: 			throw InvalidInputException("read_csv only accepts 'dtype' as a dictionary or a list of strings");
899: 		}
900: 	}
901: 
902: 	bool has_sep = !py::none().is(sep);
903: 	bool has_delimiter = !py::none().is(delimiter);
904: 	if (has_sep && has_delimiter) {
905: 		throw InvalidInputException("read_csv takes either 'delimiter' or 'sep', not both");
906: 	}
907: 	if (has_sep) {
908: 		bind_parameters["delim"] = Value(py::str(sep));
909: 	} else if (has_delimiter) {
910: 		bind_parameters["delim"] = Value(py::str(delimiter));
911: 	}
912: 
913: 	if (!py::none().is(names_p)) {
914: 		if (!py::is_list_like(names_p)) {
915: 			throw InvalidInputException("read_csv only accepts 'names' as a list of strings");
916: 		}
917: 		vector<Value> names;
918: 		py::list names_list = names_p;
919: 		for (auto &elem : names_list) {
920: 			if (!py::isinstance<py::str>(elem)) {
921: 				throw InvalidInputException("read_csv 'names' list has to consist of only strings");
922: 			}
923: 			names.push_back(Value(std::string(py::str(elem))));
924: 		}
925: 		bind_parameters["names"] = Value::LIST(LogicalType::VARCHAR, std::move(names));
926: 	}
927: 
928: 	if (!py::none().is(na_values)) {
929: 		vector<Value> null_values;
930: 		if (!py::isinstance<py::str>(na_values) && !py::is_list_like(na_values)) {
931: 			throw InvalidInputException("read_csv only accepts 'na_values' as a string or a list of strings");
932: 		} else if (py::isinstance<py::str>(na_values)) {
933: 			null_values.push_back(Value(py::str(na_values)));
934: 		} else {
935: 			py::list null_list = na_values;
936: 			for (auto &elem : null_list) {
937: 				if (!py::isinstance<py::str>(elem)) {
938: 					throw InvalidInputException("read_csv 'na_values' list has to consist of only strings");
939: 				}
940: 				null_values.push_back(Value(std::string(py::str(elem))));
941: 			}
942: 		}
943: 		bind_parameters["nullstr"] = Value::LIST(LogicalType::VARCHAR, std::move(null_values));
944: 	}
945: 
946: 	if (!py::none().is(skiprows)) {
947: 		if (!py::isinstance<py::int_>(skiprows)) {
948: 			throw InvalidInputException("read_csv only accepts 'skiprows' as an integer");
949: 		}
950: 		bind_parameters["skip"] = Value::INTEGER(py::int_(skiprows));
951: 	}
952: 
953: 	if (!py::none().is(parallel)) {
954: 		if (!py::isinstance<py::bool_>(parallel)) {
955: 			throw InvalidInputException("read_csv only accepts 'parallel' as a boolean");
956: 		}
957: 		bind_parameters["parallel"] = Value::BOOLEAN(py::bool_(parallel));
958: 	}
959: 
960: 	if (!py::none().is(quotechar)) {
961: 		if (!py::isinstance<py::str>(quotechar)) {
962: 			throw InvalidInputException("read_csv only accepts 'quotechar' as a string");
963: 		}
964: 		bind_parameters["quote"] = Value(py::str(quotechar));
965: 	}
966: 
967: 	if (!py::none().is(escapechar)) {
968: 		if (!py::isinstance<py::str>(escapechar)) {
969: 			throw InvalidInputException("read_csv only accepts 'escapechar' as a string");
970: 		}
971: 		bind_parameters["escape"] = Value(py::str(escapechar));
972: 	}
973: 
974: 	if (!py::none().is(encoding)) {
975: 		if (!py::isinstance<py::str>(encoding)) {
976: 			throw InvalidInputException("read_csv only accepts 'encoding' as a string");
977: 		}
978: 		string encoding_str = StringUtil::Lower(py::str(encoding));
979: 		if (encoding_str != "utf8" && encoding_str != "utf-8") {
980: 			throw BinderException("Copy is only supported for UTF-8 encoded files, ENCODING 'UTF-8'");
981: 		}
982: 	}
983: 
984: 	if (!py::none().is(date_format)) {
985: 		if (!py::isinstance<py::str>(date_format)) {
986: 			throw InvalidInputException("read_csv only accepts 'date_format' as a string");
987: 		}
988: 		bind_parameters["dateformat"] = Value(py::str(date_format));
989: 	}
990: 
991: 	if (!py::none().is(timestamp_format)) {
992: 		if (!py::isinstance<py::str>(timestamp_format)) {
993: 			throw InvalidInputException("read_csv only accepts 'timestamp_format' as a string");
994: 		}
995: 		bind_parameters["timestampformat"] = Value(py::str(timestamp_format));
996: 	}
997: 
998: 	if (!py::none().is(sample_size)) {
999: 		if (!py::isinstance<py::int_>(sample_size)) {
1000: 			throw InvalidInputException("read_csv only accepts 'sample_size' as an integer");
1001: 		}
1002: 		bind_parameters["sample_size"] = Value::INTEGER(py::int_(sample_size));
1003: 	}
1004: 
1005: 	if (!py::none().is(all_varchar)) {
1006: 		if (!py::isinstance<py::bool_>(all_varchar)) {
1007: 			throw InvalidInputException("read_csv only accepts 'all_varchar' as a boolean");
1008: 		}
1009: 		bind_parameters["all_varchar"] = Value::BOOLEAN(py::bool_(all_varchar));
1010: 	}
1011: 
1012: 	if (!py::none().is(normalize_names)) {
1013: 		if (!py::isinstance<py::bool_>(normalize_names)) {
1014: 			throw InvalidInputException("read_csv only accepts 'normalize_names' as a boolean");
1015: 		}
1016: 		bind_parameters["normalize_names"] = Value::BOOLEAN(py::bool_(normalize_names));
1017: 	}
1018: 
1019: 	if (!py::none().is(filename)) {
1020: 		if (!py::isinstance<py::bool_>(filename)) {
1021: 			throw InvalidInputException("read_csv only accepts 'filename' as a boolean");
1022: 		}
1023: 		bind_parameters["filename"] = Value::BOOLEAN(py::bool_(filename));
1024: 	}
1025: 
1026: 	if (!py::none().is(null_padding)) {
1027: 		if (!py::isinstance<py::bool_>(null_padding)) {
1028: 			throw InvalidInputException("read_csv only accepts 'null_padding' as a boolean");
1029: 		}
1030: 		bind_parameters["null_padding"] = Value::BOOLEAN(py::bool_(null_padding));
1031: 	}
1032: 
1033: 	// Create the ReadCSV Relation using the 'options'
1034: 
1035: 	auto read_csv_p = connection->ReadCSV(name, std::move(bind_parameters));
1036: 	auto &read_csv = read_csv_p->Cast<ReadCSVRelation>();
1037: 	if (file_like_object_wrapper) {
1038: 		read_csv.AddExternalDependency(std::move(file_like_object_wrapper));
1039: 	}
1040: 
1041: 	return make_uniq<DuckDBPyRelation>(read_csv_p->Alias(read_csv.alias));
1042: }
1043: 
1044: void DuckDBPyConnection::ExecuteImmediately(vector<unique_ptr<SQLStatement>> statements) {
1045: 	if (statements.empty()) {
1046: 		return;
1047: 	}
1048: 	for (auto &stmt : statements) {
1049: 		if (stmt->n_param != 0) {
1050: 			throw NotImplementedException(
1051: 			    "Prepared parameters are only supported for the last statement, please split your query up into "
1052: 			    "separate 'execute' calls if you want to use prepared parameters");
1053: 		}
1054: 		auto pending_query = connection->PendingQuery(std::move(stmt), false);
1055: 		auto res = CompletePendingQuery(*pending_query);
1056: 
1057: 		if (res->HasError()) {
1058: 			res->ThrowError();
1059: 		}
1060: 	}
1061: }
1062: 
1063: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::RunQuery(const py::object &query, string alias, py::object params) {
1064: 	if (!connection) {
1065: 		throw ConnectionException("Connection has already been closed");
1066: 	}
1067: 	if (alias.empty()) {
1068: 		alias = "unnamed_relation_" + StringUtil::GenerateRandomName(16);
1069: 	}
1070: 
1071: 	auto statements = GetStatements(query);
1072: 	if (statements.empty()) {
1073: 		// TODO: should we throw?
1074: 		return nullptr;
1075: 	}
1076: 
1077: 	auto last_statement = std::move(statements.back());
1078: 	statements.pop_back();
1079: 	// First immediately execute any preceding statements (if any)
1080: 	ExecuteImmediately(std::move(statements));
1081: 
1082: 	// Attempt to create a Relation for lazy execution if possible
1083: 	shared_ptr<Relation> relation;
1084: 	if (py::none().is(params)) {
1085: 		// FIXME: currently we can't create relations with prepared parameters
1086: 		auto statement_type = last_statement->type;
1087: 		switch (statement_type) {
1088: 		case StatementType::SELECT_STATEMENT: {
1089: 			auto select_statement = unique_ptr_cast<SQLStatement, SelectStatement>(std::move(last_statement));
1090: 			relation = connection->RelationFromQuery(std::move(select_statement), alias);
1091: 			break;
1092: 		}
1093: 		default:
1094: 			break;
1095: 		}
1096: 	}
1097: 
1098: 	if (!relation) {
1099: 		// Could not create a relation, resort to direct execution
1100: 		auto prep = PrepareQuery(std::move(last_statement));
1101: 		auto res = ExecuteInternal(*prep, std::move(params));
1102: 		if (!res) {
1103: 			return nullptr;
1104: 		}
1105: 		if (res->properties.return_type != StatementReturnType::QUERY_RESULT) {
1106: 			return nullptr;
1107: 		}
1108: 		if (res->type == QueryResultType::STREAM_RESULT) {
1109: 			auto &stream_result = res->Cast<StreamQueryResult>();
1110: 			res = stream_result.Materialize();
1111: 		}
1112: 		auto &materialized_result = res->Cast<MaterializedQueryResult>();
1113: 		relation = make_shared_ptr<MaterializedRelation>(connection->context, materialized_result.TakeCollection(),
1114: 		                                                 res->names, alias);
1115: 	}
1116: 	return make_uniq<DuckDBPyRelation>(std::move(relation));
1117: }
1118: 
1119: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Table(const string &tname) {
1120: 	if (!connection) {
1121: 		throw ConnectionException("Connection has already been closed");
1122: 	}
1123: 	auto qualified_name = QualifiedName::Parse(tname);
1124: 	if (qualified_name.schema.empty()) {
1125: 		qualified_name.schema = DEFAULT_SCHEMA;
1126: 	}
1127: 	try {
1128: 		return make_uniq<DuckDBPyRelation>(connection->Table(qualified_name.schema, qualified_name.name));
1129: 	} catch (const CatalogException &) {
1130: 		// CatalogException will be of the type '... is not a table'
1131: 		// Not a table in the database, make a query relation that can perform replacement scans
1132: 		auto sql_query = StringUtil::Format("from %s", KeywordHelper::WriteOptionallyQuoted(tname));
1133: 		return RunQuery(py::str(sql_query), tname);
1134: 	}
1135: }
1136: 
1137: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Values(py::object params) {
1138: 	if (!connection) {
1139: 		throw ConnectionException("Connection has already been closed");
1140: 	}
1141: 	if (params.is_none()) {
1142: 		params = py::list();
1143: 	}
1144: 	if (!py::hasattr(params, "__len__")) {
1145: 		throw InvalidInputException("Type of object passed to parameter 'values' must be iterable");
1146: 	}
1147: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(params)};
1148: 	return make_uniq<DuckDBPyRelation>(connection->Values(values));
1149: }
1150: 
1151: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::View(const string &vname) {
1152: 	if (!connection) {
1153: 		throw ConnectionException("Connection has already been closed");
1154: 	}
1155: 	// First check our temporary view
1156: 	if (temporary_views.find(vname) != temporary_views.end()) {
1157: 		return make_uniq<DuckDBPyRelation>(temporary_views[vname]);
1158: 	}
1159: 	return make_uniq<DuckDBPyRelation>(connection->View(vname));
1160: }
1161: 
1162: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fname, py::object params) {
1163: 	if (params.is_none()) {
1164: 		params = py::list();
1165: 	}
1166: 	if (!py::is_list_like(params)) {
1167: 		throw InvalidInputException("'params' has to be a list of parameters");
1168: 	}
1169: 	if (!connection) {
1170: 		throw ConnectionException("Connection has already been closed");
1171: 	}
1172: 
1173: 	return make_uniq<DuckDBPyRelation>(
1174: 	    connection->TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(params)));
1175: }
1176: 
1177: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(const PandasDataFrame &value) {
1178: 	if (!connection) {
1179: 		throw ConnectionException("Connection has already been closed");
1180: 	}
1181: 	string name = "df_" + StringUtil::GenerateRandomName();
1182: 	if (PandasDataFrame::IsPyArrowBacked(value)) {
1183: 		auto table = PandasDataFrame::ToArrowTable(value);
1184: 		return DuckDBPyConnection::FromArrow(table);
1185: 	}
1186: 	auto new_df = PandasScanFunction::PandasReplaceCopiedNames(value);
1187: 	vector<Value> params;
1188: 	params.emplace_back(Value::POINTER(CastPointerToValue(new_df.ptr())));
1189: 	auto rel = connection->TableFunction("pandas_scan", params)->Alias(name);
1190: 	auto dependency = make_shared_ptr<ExternalDependency>();
1191: 	dependency->AddDependency("original", PythonDependencyItem::Create(value));
1192: 	dependency->AddDependency("copy", PythonDependencyItem::Create(new_df));
1193: 	rel->AddExternalDependency(std::move(dependency));
1194: 	return make_uniq<DuckDBPyRelation>(std::move(rel));
1195: }
1196: 
1197: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_glob, bool binary_as_string,
1198:                                                              bool file_row_number, bool filename,
1199:                                                              bool hive_partitioning, bool union_by_name,
1200:                                                              const py::object &compression) {
1201: 	if (!connection) {
1202: 		throw ConnectionException("Connection has already been closed");
1203: 	}
1204: 	string name = "parquet_" + StringUtil::GenerateRandomName();
1205: 	vector<Value> params;
1206: 	params.emplace_back(file_glob);
1207: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)},
1208: 	                                        {"file_row_number", Value::BOOLEAN(file_row_number)},
1209: 	                                        {"filename", Value::BOOLEAN(filename)},
1210: 	                                        {"hive_partitioning", Value::BOOLEAN(hive_partitioning)},
1211: 	                                        {"union_by_name", Value::BOOLEAN(union_by_name)}});
1212: 
1213: 	if (!py::none().is(compression)) {
1214: 		if (!py::isinstance<py::str>(compression)) {
1215: 			throw InvalidInputException("from_parquet only accepts 'compression' as a string");
1216: 		}
1217: 		named_parameters["compression"] = Value(py::str(compression));
1218: 	}
1219: 	return make_uniq<DuckDBPyRelation>(
1220: 	    connection->TableFunction("parquet_scan", params, named_parameters)->Alias(name));
1221: }
1222: 
1223: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquets(const vector<string> &file_globs, bool binary_as_string,
1224:                                                               bool file_row_number, bool filename,
1225:                                                               bool hive_partitioning, bool union_by_name,
1226:                                                               const py::object &compression) {
1227: 	if (!connection) {
1228: 		throw ConnectionException("Connection has already been closed");
1229: 	}
1230: 	string name = "parquet_" + StringUtil::GenerateRandomName();
1231: 	vector<Value> params;
1232: 	auto file_globs_as_value = vector<Value>();
1233: 	for (const auto &file : file_globs) {
1234: 		file_globs_as_value.emplace_back(file);
1235: 	}
1236: 	params.emplace_back(Value::LIST(file_globs_as_value));
1237: 	named_parameter_map_t named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)},
1238: 	                                        {"file_row_number", Value::BOOLEAN(file_row_number)},
1239: 	                                        {"filename", Value::BOOLEAN(filename)},
1240: 	                                        {"hive_partitioning", Value::BOOLEAN(hive_partitioning)},
1241: 	                                        {"union_by_name", Value::BOOLEAN(union_by_name)}});
1242: 
1243: 	if (!py::none().is(compression)) {
1244: 		if (!py::isinstance<py::str>(compression)) {
1245: 			throw InvalidInputException("from_parquet only accepts 'compression' as a string");
1246: 		}
1247: 		named_parameters["compression"] = Value(py::str(compression));
1248: 	}
1249: 
1250: 	return make_uniq<DuckDBPyRelation>(
1251: 	    connection->TableFunction("parquet_scan", params, named_parameters)->Alias(name));
1252: }
1253: 
1254: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrow(py::object &arrow_object) {
1255: 	if (!connection) {
1256: 		throw ConnectionException("Connection has already been closed");
1257: 	}
1258: 	py::gil_scoped_acquire acquire;
1259: 	string name = "arrow_object_" + StringUtil::GenerateRandomName();
1260: 	if (!IsAcceptedArrowObject(arrow_object)) {
1261: 		auto py_object_type = string(py::str(arrow_object.get_type().attr("__name__")));
1262: 		throw InvalidInputException("Python Object Type %s is not an accepted Arrow Object.", py_object_type);
1263: 	}
1264: 	auto stream_factory =
1265: 	    make_uniq<PythonTableArrowArrayStreamFactory>(arrow_object.ptr(), connection->context->GetClientProperties());
1266: 
1267: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
1268: 	auto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;
1269: 
1270: 	auto rel = connection
1271: 	               ->TableFunction("arrow_scan", {Value::POINTER(CastPointerToValue(stream_factory.get())),
1272: 	                                              Value::POINTER(CastPointerToValue(stream_factory_produce)),
1273: 	                                              Value::POINTER(CastPointerToValue(stream_factory_get_schema))})
1274: 	               ->Alias(name);
1275: 	auto dependency = make_shared_ptr<ExternalDependency>();
1276: 	auto dependency_item =
1277: 	    PythonDependencyItem::Create(make_uniq<RegisteredArrow>(std::move(stream_factory), arrow_object));
1278: 	dependency->AddDependency("object", std::move(dependency_item));
1279: 	rel->AddExternalDependency(std::move(dependency));
1280: 	return make_uniq<DuckDBPyRelation>(std::move(rel));
1281: }
1282: 
1283: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstrait(py::bytes &proto) {
1284: 	if (!connection) {
1285: 		throw ConnectionException("Connection has already been closed");
1286: 	}
1287: 	string name = "substrait_" + StringUtil::GenerateRandomName();
1288: 	vector<Value> params;
1289: 	params.emplace_back(Value::BLOB_RAW(proto));
1290: 	return make_uniq<DuckDBPyRelation>(connection->TableFunction("from_substrait", params)->Alias(name));
1291: }
1292: 
1293: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstrait(const string &query, bool enable_optimizer) {
1294: 	if (!connection) {
1295: 		throw ConnectionException("Connection has already been closed");
1296: 	}
1297: 	vector<Value> params;
1298: 	params.emplace_back(query);
1299: 	named_parameter_map_t named_parameters({{"enable_optimizer", Value::BOOLEAN(enable_optimizer)}});
1300: 	return make_uniq<DuckDBPyRelation>(
1301: 	    connection->TableFunction("get_substrait", params, named_parameters)->Alias(query));
1302: }
1303: 
1304: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::GetSubstraitJSON(const string &query, bool enable_optimizer) {
1305: 	if (!connection) {
1306: 		throw ConnectionException("Connection has already been closed");
1307: 	}
1308: 	vector<Value> params;
1309: 	params.emplace_back(query);
1310: 	named_parameter_map_t named_parameters({{"enable_optimizer", Value::BOOLEAN(enable_optimizer)}});
1311: 	return make_uniq<DuckDBPyRelation>(
1312: 	    connection->TableFunction("get_substrait_json", params, named_parameters)->Alias(query));
1313: }
1314: 
1315: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromSubstraitJSON(const string &json) {
1316: 	if (!connection) {
1317: 		throw ConnectionException("Connection has already been closed");
1318: 	}
1319: 	string name = "from_substrait_" + StringUtil::GenerateRandomName();
1320: 	vector<Value> params;
1321: 	params.emplace_back(json);
1322: 	return make_uniq<DuckDBPyRelation>(connection->TableFunction("from_substrait_json", params)->Alias(name));
1323: }
1324: 
1325: unordered_set<string> DuckDBPyConnection::GetTableNames(const string &query) {
1326: 	if (!connection) {
1327: 		throw ConnectionException("Connection has already been closed");
1328: 	}
1329: 	return connection->GetTableNames(query);
1330: }
1331: 
1332: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::UnregisterPythonObject(const string &name) {
1333: 	connection->context->external_dependencies.erase(name);
1334: 	temporary_views.erase(name);
1335: 	py::gil_scoped_release release;
1336: 	if (connection) {
1337: 		connection->Query("DROP VIEW \"" + name + "\"");
1338: 	}
1339: 	return shared_from_this();
1340: }
1341: 
1342: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Begin() {
1343: 	ExecuteFromString("BEGIN TRANSACTION");
1344: 	return shared_from_this();
1345: }
1346: 
1347: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Commit() {
1348: 	if (connection->context->transaction.IsAutoCommit()) {
1349: 		return shared_from_this();
1350: 	}
1351: 	ExecuteFromString("COMMIT");
1352: 	return shared_from_this();
1353: }
1354: 
1355: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Rollback() {
1356: 	ExecuteFromString("ROLLBACK");
1357: 	return shared_from_this();
1358: }
1359: 
1360: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Checkpoint() {
1361: 	ExecuteFromString("CHECKPOINT");
1362: 	return shared_from_this();
1363: }
1364: 
1365: Optional<py::list> DuckDBPyConnection::GetDescription() {
1366: 	if (!result) {
1367: 		return py::none();
1368: 	}
1369: 	return result->Description();
1370: }
1371: 
1372: int DuckDBPyConnection::GetRowcount() {
1373: 	return -1;
1374: }
1375: 
1376: void DuckDBPyConnection::Close() {
1377: 	result = nullptr;
1378: 	connection = nullptr;
1379: 	database = nullptr;
1380: 	temporary_views.clear();
1381: 	// https://peps.python.org/pep-0249/#Connection.close
1382: 	for (auto &cur : cursors) {
1383: 		auto cursor = cur.lock();
1384: 		if (!cursor) {
1385: 			// The cursor has already been closed
1386: 			continue;
1387: 		}
1388: 		cursor->Close();
1389: 	}
1390: 	registered_functions.clear();
1391: 	cursors.clear();
1392: }
1393: 
1394: void DuckDBPyConnection::Interrupt() {
1395: 	if (!connection) {
1396: 		throw ConnectionException("Connection has already been closed");
1397: 	}
1398: 	connection->Interrupt();
1399: }
1400: 
1401: void DuckDBPyConnection::InstallExtension(const string &extension, bool force_install) {
1402: 	ExtensionHelper::InstallExtension(*connection->context, extension, force_install);
1403: }
1404: 
1405: void DuckDBPyConnection::LoadExtension(const string &extension) {
1406: 	ExtensionHelper::LoadExternalExtension(*connection->context, extension);
1407: }
1408: 
1409: // cursor() is stupid
1410: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Cursor() {
1411: 	if (!connection) {
1412: 		throw ConnectionException("Connection has already been closed");
1413: 	}
1414: 	auto res = make_shared_ptr<DuckDBPyConnection>();
1415: 	res->database = database;
1416: 	res->connection = make_uniq<Connection>(*res->database);
1417: 	cursors.push_back(res);
1418: 	return res;
1419: }
1420: 
1421: // these should be functions on the result but well
1422: Optional<py::tuple> DuckDBPyConnection::FetchOne() {
1423: 	if (!result) {
1424: 		throw InvalidInputException("No open result set");
1425: 	}
1426: 	return result->FetchOne();
1427: }
1428: 
1429: py::list DuckDBPyConnection::FetchMany(idx_t size) {
1430: 	if (!result) {
1431: 		throw InvalidInputException("No open result set");
1432: 	}
1433: 	return result->FetchMany(size);
1434: }
1435: 
1436: py::list DuckDBPyConnection::FetchAll() {
1437: 	if (!result) {
1438: 		throw InvalidInputException("No open result set");
1439: 	}
1440: 	return result->FetchAll();
1441: }
1442: 
1443: py::dict DuckDBPyConnection::FetchNumpy() {
1444: 	if (!result) {
1445: 		throw InvalidInputException("No open result set");
1446: 	}
1447: 	return result->FetchNumpyInternal();
1448: }
1449: 
1450: PandasDataFrame DuckDBPyConnection::FetchDF(bool date_as_object) {
1451: 	if (!result) {
1452: 		throw InvalidInputException("No open result set");
1453: 	}
1454: 	return result->FetchDF(date_as_object);
1455: }
1456: 
1457: PandasDataFrame DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk, bool date_as_object) const {
1458: 	if (!result) {
1459: 		throw InvalidInputException("No open result set");
1460: 	}
1461: 	return result->FetchDFChunk(vectors_per_chunk, date_as_object);
1462: }
1463: 
1464: duckdb::pyarrow::Table DuckDBPyConnection::FetchArrow(idx_t rows_per_batch) {
1465: 	if (!result) {
1466: 		throw InvalidInputException("No open result set");
1467: 	}
1468: 	return result->ToArrowTable(rows_per_batch);
1469: }
1470: 
1471: py::dict DuckDBPyConnection::FetchPyTorch() {
1472: 	if (!result) {
1473: 		throw InvalidInputException("No open result set");
1474: 	}
1475: 	return result->FetchPyTorch();
1476: }
1477: 
1478: py::dict DuckDBPyConnection::FetchTF() {
1479: 	if (!result) {
1480: 		throw InvalidInputException("No open result set");
1481: 	}
1482: 	return result->FetchTF();
1483: }
1484: 
1485: PolarsDataFrame DuckDBPyConnection::FetchPolars(idx_t rows_per_batch) {
1486: 	auto arrow = FetchArrow(rows_per_batch);
1487: 	return py::cast<PolarsDataFrame>(py::module::import("polars").attr("DataFrame")(arrow));
1488: }
1489: 
1490: duckdb::pyarrow::RecordBatchReader DuckDBPyConnection::FetchRecordBatchReader(const idx_t rows_per_batch) const {
1491: 	if (!result) {
1492: 		throw InvalidInputException("No open result set");
1493: 	}
1494: 	return result->FetchRecordBatchReader(rows_per_batch);
1495: }
1496: 
1497: case_insensitive_map_t<Value> TransformPyConfigDict(const py::dict &py_config_dict) {
1498: 	case_insensitive_map_t<Value> config_dict;
1499: 	for (auto &kv : py_config_dict) {
1500: 		auto key = py::str(kv.first);
1501: 		auto val = py::str(kv.second);
1502: 		config_dict[key] = Value(val);
1503: 	}
1504: 	return config_dict;
1505: }
1506: 
1507: void CreateNewInstance(DuckDBPyConnection &res, const string &database, DBConfig &config) {
1508: 	// We don't cache unnamed memory instances (i.e., :memory:)
1509: 	bool cache_instance = database != ":memory:" && !database.empty();
1510: 	config.replacement_scans.emplace_back(PythonReplacementScan::Replace);
1511: 	res.database = instance_cache.CreateInstance(database, config, cache_instance);
1512: 	res.connection = make_uniq<Connection>(*res.database);
1513: 	auto &context = *res.connection->context;
1514: 	PandasScanFunction scan_fun;
1515: 	CreateTableFunctionInfo scan_info(scan_fun);
1516: 	MapFunction map_fun;
1517: 	CreateTableFunctionInfo map_info(map_fun);
1518: 	auto &catalog = Catalog::GetSystemCatalog(context);
1519: 	context.transaction.BeginTransaction();
1520: 	catalog.CreateTableFunction(context, &scan_info);
1521: 	catalog.CreateTableFunction(context, &map_info);
1522: 	context.transaction.Commit();
1523: }
1524: 
1525: static bool HasJupyterProgressBarDependencies() {
1526: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1527: 	if (!import_cache.ipywidgets()) {
1528: 		// ipywidgets not installed, needed to support the progress bar
1529: 		return false;
1530: 	}
1531: 	return true;
1532: }
1533: 
1534: static void SetDefaultConfigArguments(ClientContext &context) {
1535: 	if (!DuckDBPyConnection::IsInteractive()) {
1536: 		// Don't need to set any special default arguments
1537: 		return;
1538: 	}
1539: 
1540: 	auto &config = ClientConfig::GetConfig(context);
1541: 	config.enable_progress_bar = true;
1542: 
1543: 	if (!DuckDBPyConnection::IsJupyter()) {
1544: 		return;
1545: 	}
1546: 	if (!HasJupyterProgressBarDependencies()) {
1547: 		// Disable progress bar altogether
1548: 		config.system_progress_bar_disable_reason =
1549: 		    "required package 'ipywidgets' is missing, which is needed to render progress bars in Jupyter";
1550: 		config.enable_progress_bar = false;
1551: 		return;
1552: 	}
1553: 
1554: 	// Set the function used to create the display for the progress bar
1555: 	context.config.display_create_func = JupyterProgressBarDisplay::Create;
1556: }
1557: 
1558: static shared_ptr<DuckDBPyConnection> FetchOrCreateInstance(const string &database, DBConfig &config) {
1559: 	auto res = make_shared_ptr<DuckDBPyConnection>();
1560: 	res->database = instance_cache.GetInstance(database, config);
1561: 	if (!res->database) {
1562: 		//! No cached database, we must create a new instance
1563: 		CreateNewInstance(*res, database, config);
1564: 		return res;
1565: 	}
1566: 	res->connection = make_uniq<Connection>(*res->database);
1567: 	return res;
1568: }
1569: 
1570: bool IsDefaultConnectionString(const string &database, bool read_only, case_insensitive_map_t<Value> &config) {
1571: 	bool is_default = StringUtil::CIEquals(database, ":default:");
1572: 	if (!is_default) {
1573: 		return false;
1574: 	}
1575: 	// Only allow fetching the default connection when no options are passed
1576: 	if (read_only == true || !config.empty()) {
1577: 		throw InvalidInputException("Default connection fetching is only allowed without additional options");
1578: 	}
1579: 	return true;
1580: }
1581: 
1582: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Connect(const string &database, bool read_only,
1583:                                                            const py::dict &config_options) {
1584: 	auto config_dict = TransformPyConfigDict(config_options);
1585: 	if (IsDefaultConnectionString(database, read_only, config_dict)) {
1586: 		return DuckDBPyConnection::DefaultConnection();
1587: 	}
1588: 
1589: 	DBConfig config(read_only);
1590: 	config.AddExtensionOption("pandas_analyze_sample",
1591: 	                          "The maximum number of rows to sample when analyzing a pandas object column.",
1592: 	                          LogicalType::UBIGINT, Value::UBIGINT(1000));
1593: 	config.AddExtensionOption("python_enable_replacements",
1594: 	                          "Whether variables visible to the current stack should be used for replacement scans.",
1595: 	                          LogicalType::BOOLEAN, Value::BOOLEAN(true));
1596: 	if (!DuckDBPyConnection::IsJupyter()) {
1597: 		config_dict["duckdb_api"] = Value("python");
1598: 	} else {
1599: 		config_dict["duckdb_api"] = Value("python jupyter");
1600: 	}
1601: 	config.SetOptionsByName(config_dict);
1602: 
1603: 	auto res = FetchOrCreateInstance(database, config);
1604: 	auto &client_context = *res->connection->context;
1605: 	SetDefaultConfigArguments(client_context);
1606: 	return res;
1607: }
1608: 
1609: vector<Value> DuckDBPyConnection::TransformPythonParamList(const py::handle &params) {
1610: 	vector<Value> args;
1611: 	args.reserve(py::len(params));
1612: 
1613: 	for (auto param : params) {
1614: 		args.emplace_back(TransformPythonValue(param, LogicalType::UNKNOWN, false));
1615: 	}
1616: 	return args;
1617: }
1618: 
1619: case_insensitive_map_t<Value> DuckDBPyConnection::TransformPythonParamDict(const py::dict &params) {
1620: 	case_insensitive_map_t<Value> args;
1621: 
1622: 	for (auto pair : params) {
1623: 		auto &key = pair.first;
1624: 		auto &value = pair.second;
1625: 		args[std::string(py::str(key))] = TransformPythonValue(value, LogicalType::UNKNOWN, false);
1626: 	}
1627: 	return args;
1628: }
1629: 
1630: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::DefaultConnection() {
1631: 	if (!default_connection) {
1632: 		py::dict config_dict;
1633: 		default_connection = DuckDBPyConnection::Connect(":memory:", false, config_dict);
1634: 	}
1635: 	return default_connection;
1636: }
1637: 
1638: PythonImportCache *DuckDBPyConnection::ImportCache() {
1639: 	if (!import_cache) {
1640: 		import_cache = make_shared_ptr<PythonImportCache>();
1641: 	}
1642: 	return import_cache.get();
1643: }
1644: 
1645: ModifiedMemoryFileSystem &DuckDBPyConnection::GetObjectFileSystem() {
1646: 	if (!internal_object_filesystem) {
1647: 		D_ASSERT(!FileSystemIsRegistered("DUCKDB_INTERNAL_OBJECTSTORE"));
1648: 		auto &import_cache_py = *ImportCache();
1649: 		auto modified_memory_fs = import_cache_py.duckdb.filesystem.ModifiedMemoryFileSystem();
1650: 		if (modified_memory_fs.ptr() == nullptr) {
1651: 			throw InvalidInputException(
1652: 			    "This operation could not be completed because required module 'fsspec' is not installed");
1653: 		}
1654: 		internal_object_filesystem = make_shared_ptr<ModifiedMemoryFileSystem>(modified_memory_fs());
1655: 		auto &abstract_fs = reinterpret_cast<AbstractFileSystem &>(*internal_object_filesystem);
1656: 		RegisterFilesystem(abstract_fs);
1657: 	}
1658: 	return *internal_object_filesystem;
1659: }
1660: 
1661: bool DuckDBPyConnection::IsInteractive() {
1662: 	return DuckDBPyConnection::environment != PythonEnvironmentType::NORMAL;
1663: }
1664: 
1665: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Enter() {
1666: 	return shared_from_this();
1667: }
1668: 
1669: void DuckDBPyConnection::Exit(DuckDBPyConnection &self, const py::object &exc_type, const py::object &exc,
1670:                               const py::object &traceback) {
1671: 	self.Close();
1672: 	if (exc_type.ptr() != Py_None) {
1673: 		// Propagate the exception if any occurred
1674: 		PyErr_SetObject(exc_type.ptr(), exc.ptr());
1675: 		throw py::error_already_set();
1676: 	}
1677: }
1678: 
1679: void DuckDBPyConnection::Cleanup() {
1680: 	default_connection.reset();
1681: 	import_cache.reset();
1682: }
1683: 
1684: bool DuckDBPyConnection::IsPandasDataframe(const py::object &object) {
1685: 	if (!ModuleIsLoaded<PandasCacheItem>()) {
1686: 		return false;
1687: 	}
1688: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
1689: 	return py::isinstance(object, import_cache_py.pandas.DataFrame());
1690: }
1691: 
1692: bool DuckDBPyConnection::IsPolarsDataframe(const py::object &object) {
1693: 	if (!ModuleIsLoaded<PolarsCacheItem>()) {
1694: 		return false;
1695: 	}
1696: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
1697: 	return py::isinstance(object, import_cache_py.polars.DataFrame()) ||
1698: 	       py::isinstance(object, import_cache_py.polars.LazyFrame());
1699: }
1700: 
1701: bool IsValidNumpyDimensions(const py::handle &object, int &dim) {
1702: 	// check the dimensions of numpy arrays
1703: 	// should only be called by IsAcceptedNumpyObject
1704: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1705: 	if (!py::isinstance(object, import_cache.numpy.ndarray())) {
1706: 		return false;
1707: 	}
1708: 	auto shape = (py::cast<py::array>(object)).attr("shape");
1709: 	if (py::len(shape) != 1) {
1710: 		return false;
1711: 	}
1712: 	int cur_dim = (shape.attr("__getitem__")(0)).cast<int>();
1713: 	dim = dim == -1 ? cur_dim : dim;
1714: 	return dim == cur_dim;
1715: }
1716: NumpyObjectType DuckDBPyConnection::IsAcceptedNumpyObject(const py::object &object) {
1717: 	if (!ModuleIsLoaded<NumpyCacheItem>()) {
1718: 		return NumpyObjectType::INVALID;
1719: 	}
1720: 	auto &import_cache = *DuckDBPyConnection::ImportCache();
1721: 	if (py::isinstance(object, import_cache.numpy.ndarray())) {
1722: 		auto len = py::len((py::cast<py::array>(object)).attr("shape"));
1723: 		switch (len) {
1724: 		case 1:
1725: 			return NumpyObjectType::NDARRAY1D;
1726: 		case 2:
1727: 			return NumpyObjectType::NDARRAY2D;
1728: 		default:
1729: 			return NumpyObjectType::INVALID;
1730: 		}
1731: 	} else if (py::is_dict_like(object)) {
1732: 		int dim = -1;
1733: 		for (auto item : py::cast<py::dict>(object)) {
1734: 			if (!IsValidNumpyDimensions(item.second, dim)) {
1735: 				return NumpyObjectType::INVALID;
1736: 			}
1737: 		}
1738: 		return NumpyObjectType::DICT;
1739: 	} else if (py::is_list_like(object)) {
1740: 		int dim = -1;
1741: 		for (auto item : py::cast<py::list>(object)) {
1742: 			if (!IsValidNumpyDimensions(item, dim)) {
1743: 				return NumpyObjectType::INVALID;
1744: 			}
1745: 		}
1746: 		return NumpyObjectType::LIST;
1747: 	}
1748: 	return NumpyObjectType::INVALID;
1749: }
1750: 
1751: bool DuckDBPyConnection::IsAcceptedArrowObject(const py::object &object) {
1752: 	if (!ModuleIsLoaded<PyarrowCacheItem>()) {
1753: 		return false;
1754: 	}
1755: 	auto &import_cache_py = *DuckDBPyConnection::ImportCache();
1756: 	if (py::isinstance(object, import_cache_py.pyarrow.Table()) ||
1757: 	    py::isinstance(object, import_cache_py.pyarrow.RecordBatchReader())) {
1758: 		return true;
1759: 	}
1760: 	if (!ModuleIsLoaded<PyarrowDatasetCacheItem>()) {
1761: 		return false;
1762: 	}
1763: 	return (py::isinstance(object, import_cache_py.pyarrow.dataset.Dataset()) ||
1764: 	        py::isinstance(object, import_cache_py.pyarrow.dataset.Scanner()));
1765: }
1766: 
1767: unique_lock<std::mutex> DuckDBPyConnection::AcquireConnectionLock() {
1768: 	// we first release the gil and then acquire the connection lock
1769: 	unique_lock<std::mutex> lock(py_connection_lock, std::defer_lock);
1770: 	{
1771: 		py::gil_scoped_release release;
1772: 		lock.lock();
1773: 	}
1774: 	return lock;
1775: }
1776: 
1777: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: