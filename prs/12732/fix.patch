diff --git a/.github/workflows/CodeQuality.yml b/.github/workflows/CodeQuality.yml
index 37c0e33b0661..a5787139075d 100644
--- a/.github/workflows/CodeQuality.yml
+++ b/.github/workflows/CodeQuality.yml
@@ -49,6 +49,10 @@ jobs:
         shell: bash
         run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build clang-format-11 && sudo pip3 install cmake-format black cxxheaderparser pcpp
 
+      - name: List Installed Packages
+        shell: bash
+        run: pip3 freeze
+
       - name: Format Check
         shell: bash
         run: |
diff --git a/tools/pythonpkg/duckdb-stubs/__init__.pyi b/tools/pythonpkg/duckdb-stubs/__init__.pyi
index 7bcb25ec6f47..e469a70ef6d9 100644
--- a/tools/pythonpkg/duckdb-stubs/__init__.pyi
+++ b/tools/pythonpkg/duckdb-stubs/__init__.pyi
@@ -315,7 +315,7 @@ class DuckDBPyConnection:
     def view(self, view_name: str) -> DuckDBPyRelation: ...
     def values(self, values: List[Any]) -> DuckDBPyRelation: ...
     def table_function(self, name: str, parameters: object = None) -> DuckDBPyRelation: ...
-    def read_json(self, name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None) -> DuckDBPyRelation: ...
+    def read_json(self, name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, compression: Optional[str] = None, maximum_object_size: Optional[int] = None, ignore_errors: Optional[bool] = None, convert_strings_to_integers: Optional[bool] = None, field_appearance_threshold: Optional[float] = None, map_inference_threshold: Optional[int] = None, maximum_sample_files: Optional[int] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None) -> DuckDBPyRelation: ...
     def extract_statements(self, query: str) -> List[Statement]: ...
     def sql(self, query: str, *, alias: str = "", params: object = None) -> DuckDBPyRelation: ...
     def query(self, query: str, *, alias: str = "", params: object = None) -> DuckDBPyRelation: ...
@@ -640,7 +640,7 @@ def table(table_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyR
 def view(view_name: str, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
 def values(values: List[Any], *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
 def table_function(name: str, parameters: object = None, *, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
-def read_json(name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
+def read_json(name: str, *, columns: Optional[Dict[str,str]] = None, sample_size: Optional[int] = None, maximum_depth: Optional[int] = None, records: Optional[str] = None, format: Optional[str] = None, date_format: Optional[str] = None, timestamp_format: Optional[str] = None, compression: Optional[str] = None, maximum_object_size: Optional[int] = None, ignore_errors: Optional[bool] = None, convert_strings_to_integers: Optional[bool] = None, field_appearance_threshold: Optional[float] = None, map_inference_threshold: Optional[int] = None, maximum_sample_files: Optional[int] = None, filename: Optional[bool | str] = None, hive_partitioning: Optional[bool] = None, union_by_name: Optional[bool] = None, hive_types: Optional[Dict[str, str]] = None, hive_types_autocast: Optional[bool] = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
 def extract_statements(query: str, *, connection: DuckDBPyConnection = ...) -> List[Statement]: ...
 def sql(query: str, *, alias: str = "", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
 def query(query: str, *, alias: str = "", params: object = None, connection: DuckDBPyConnection = ...) -> DuckDBPyRelation: ...
diff --git a/tools/pythonpkg/duckdb_python.cpp b/tools/pythonpkg/duckdb_python.cpp
index bec9342d83c7..87ddabcaae13 100644
--- a/tools/pythonpkg/duckdb_python.cpp
+++ b/tools/pythonpkg/duckdb_python.cpp
@@ -563,18 +563,42 @@ static void InitializeConnectionMethods(py::module_ &m) {
 	    py::arg("parameters") = py::none(), py::kw_only(), py::arg("connection") = py::none());
 	m.def(
 	    "read_json",
-	    [](const string &filename, const Optional<py::object> &columns = py::none(),
+	    [](const string &name, const Optional<py::object> &columns = py::none(),
 	       const Optional<py::object> &sample_size = py::none(), const Optional<py::object> &maximum_depth = py::none(),
 	       const Optional<py::str> &records = py::none(), const Optional<py::str> &format = py::none(),
+	       const Optional<py::object> &date_format = py::none(),
+	       const Optional<py::object> &timestamp_format = py::none(),
+	       const Optional<py::object> &compression = py::none(),
+	       const Optional<py::object> &maximum_object_size = py::none(),
+	       const Optional<py::object> &ignore_errors = py::none(),
+	       const Optional<py::object> &convert_strings_to_integers = py::none(),
+	       const Optional<py::object> &field_appearance_threshold = py::none(),
+	       const Optional<py::object> &map_inference_threshold = py::none(),
+	       const Optional<py::object> &maximum_sample_files = py::none(),
+	       const Optional<py::object> &filename = py::none(),
+	       const Optional<py::object> &hive_partitioning = py::none(),
+	       const Optional<py::object> &union_by_name = py::none(), const Optional<py::object> &hive_types = py::none(),
+	       const Optional<py::object> &hive_types_autocast = py::none(),
 	       shared_ptr<DuckDBPyConnection> conn = nullptr) {
 		    if (!conn) {
 			    conn = DuckDBPyConnection::DefaultConnection();
 		    }
-		    return conn->ReadJSON(filename, columns, sample_size, maximum_depth, records, format);
+		    return conn->ReadJSON(name, columns, sample_size, maximum_depth, records, format, date_format,
+		                          timestamp_format, compression, maximum_object_size, ignore_errors,
+		                          convert_strings_to_integers, field_appearance_threshold, map_inference_threshold,
+		                          maximum_sample_files, filename, hive_partitioning, union_by_name, hive_types,
+		                          hive_types_autocast);
 	    },
 	    "Create a relation object from the JSON file in 'name'", py::arg("name"), py::kw_only(),
 	    py::arg("columns") = py::none(), py::arg("sample_size") = py::none(), py::arg("maximum_depth") = py::none(),
-	    py::arg("records") = py::none(), py::arg("format") = py::none(), py::arg("connection") = py::none());
+	    py::arg("records") = py::none(), py::arg("format") = py::none(), py::arg("date_format") = py::none(),
+	    py::arg("timestamp_format") = py::none(), py::arg("compression") = py::none(),
+	    py::arg("maximum_object_size") = py::none(), py::arg("ignore_errors") = py::none(),
+	    py::arg("convert_strings_to_integers") = py::none(), py::arg("field_appearance_threshold") = py::none(),
+	    py::arg("map_inference_threshold") = py::none(), py::arg("maximum_sample_files") = py::none(),
+	    py::arg("filename") = py::none(), py::arg("hive_partitioning") = py::none(),
+	    py::arg("union_by_name") = py::none(), py::arg("hive_types") = py::none(),
+	    py::arg("hive_types_autocast") = py::none(), py::arg("connection") = py::none());
 	m.def(
 	    "extract_statements",
 	    [](const string &query, shared_ptr<DuckDBPyConnection> conn = nullptr) {
diff --git a/tools/pythonpkg/scripts/connection_methods.json b/tools/pythonpkg/scripts/connection_methods.json
index abbf5b0299e5..6091a3fa8aa8 100644
--- a/tools/pythonpkg/scripts/connection_methods.json
+++ b/tools/pythonpkg/scripts/connection_methods.json
@@ -571,6 +571,76 @@
 				"name": "format",
 				"default": "None",
 				"type": "Optional[str]"
+			},
+			{
+				"name": "date_format",
+				"default": "None",
+				"type": "Optional[str]"
+			},
+			{
+				"name": "timestamp_format",
+				"default": "None",
+				"type": "Optional[str]"
+			},
+			{
+				"name": "compression",
+				"default": "None",
+				"type": "Optional[str]"
+			},
+			{
+				"name": "maximum_object_size",
+				"default": "None",
+				"type": "Optional[int]"
+			},
+			{
+				"name": "ignore_errors",
+				"default": "None",
+				"type": "Optional[bool]"
+			},
+			{
+				"name": "convert_strings_to_integers",
+				"default": "None",
+				"type": "Optional[bool]"
+			},
+			{
+				"name": "field_appearance_threshold",
+				"default": "None",
+				"type": "Optional[float]"
+			},
+			{
+				"name": "map_inference_threshold",
+				"default": "None",
+				"type": "Optional[int]"
+			},
+			{
+				"name": "maximum_sample_files",
+				"default": "None",
+				"type": "Optional[int]"
+			},
+			{
+				"name": "filename",
+				"default": "None",
+				"type": "Optional[bool | str]"
+			},
+			{
+				"name": "hive_partitioning",
+				"default": "None",
+				"type": "Optional[bool]"
+			},
+			{
+				"name": "union_by_name",
+				"default": "None",
+				"type": "Optional[bool]"
+			},
+			{
+				"name": "hive_types",
+				"default": "None",
+				"type": "Optional[Dict[str, str]]"
+			},
+			{
+				"name": "hive_types_autocast",
+				"default": "None",
+				"type": "Optional[bool]"
 			}
 		],
 		"return": "DuckDBPyRelation"
diff --git a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp
index ae6b5ed46fa5..d4f37b7ce2e7 100644
--- a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp
+++ b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp
@@ -85,11 +85,21 @@ struct DuckDBPyConnection : public enable_shared_from_this<DuckDBPyConnection> {
 
 	py::list ExtractStatements(const string &query);
 
-	unique_ptr<DuckDBPyRelation> ReadJSON(const string &filename, const Optional<py::object> &columns = py::none(),
-	                                      const Optional<py::object> &sample_size = py::none(),
-	                                      const Optional<py::object> &maximum_depth = py::none(),
-	                                      const Optional<py::str> &records = py::none(),
-	                                      const Optional<py::str> &format = py::none());
+	unique_ptr<DuckDBPyRelation> ReadJSON(
+	    const string &name, const Optional<py::object> &columns = py::none(),
+	    const Optional<py::object> &sample_size = py::none(), const Optional<py::object> &maximum_depth = py::none(),
+	    const Optional<py::str> &records = py::none(), const Optional<py::str> &format = py::none(),
+	    const Optional<py::object> &date_format = py::none(), const Optional<py::object> &timestamp_format = py::none(),
+	    const Optional<py::object> &compression = py::none(),
+	    const Optional<py::object> &maximum_object_size = py::none(),
+	    const Optional<py::object> &ignore_errors = py::none(),
+	    const Optional<py::object> &convert_strings_to_integers = py::none(),
+	    const Optional<py::object> &field_appearance_threshold = py::none(),
+	    const Optional<py::object> &map_inference_threshold = py::none(),
+	    const Optional<py::object> &maximum_sample_files = py::none(),
+	    const Optional<py::object> &filename = py::none(), const Optional<py::object> &hive_partitioning = py::none(),
+	    const Optional<py::object> &union_by_name = py::none(), const Optional<py::object> &hive_types = py::none(),
+	    const Optional<py::object> &hive_types_autocast = py::none());
 
 	shared_ptr<DuckDBPyType> MapType(const shared_ptr<DuckDBPyType> &key_type,
 	                                 const shared_ptr<DuckDBPyType> &value_type);
diff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp
index 5702983377bf..2c18cacbdfbb 100644
--- a/tools/pythonpkg/src/pyconnection.cpp
+++ b/tools/pythonpkg/src/pyconnection.cpp
@@ -214,7 +214,14 @@ static void InitializeConnectionMethods(py::class_<DuckDBPyConnection, shared_pt
 	      py::arg("parameters") = py::none());
 	m.def("read_json", &DuckDBPyConnection::ReadJSON, "Create a relation object from the JSON file in 'name'",
 	      py::arg("name"), py::kw_only(), py::arg("columns") = py::none(), py::arg("sample_size") = py::none(),
-	      py::arg("maximum_depth") = py::none(), py::arg("records") = py::none(), py::arg("format") = py::none());
+	      py::arg("maximum_depth") = py::none(), py::arg("records") = py::none(), py::arg("format") = py::none(),
+	      py::arg("date_format") = py::none(), py::arg("timestamp_format") = py::none(),
+	      py::arg("compression") = py::none(), py::arg("maximum_object_size") = py::none(),
+	      py::arg("ignore_errors") = py::none(), py::arg("convert_strings_to_integers") = py::none(),
+	      py::arg("field_appearance_threshold") = py::none(), py::arg("map_inference_threshold") = py::none(),
+	      py::arg("maximum_sample_files") = py::none(), py::arg("filename") = py::none(),
+	      py::arg("hive_partitioning") = py::none(), py::arg("union_by_name") = py::none(),
+	      py::arg("hive_types") = py::none(), py::arg("hive_types_autocast") = py::none());
 	m.def("extract_statements", &DuckDBPyConnection::ExtractStatements,
 	      "Parse the query string and extract the Statement object(s) produced", py::arg("query"));
 	m.def("sql", &DuckDBPyConnection::RunQuery,
@@ -737,17 +744,67 @@ shared_ptr<DuckDBPyConnection> DuckDBPyConnection::RegisterPythonObject(const st
 	return shared_from_this();
 }
 
-unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(const string &name, const Optional<py::object> &columns,
-                                                          const Optional<py::object> &sample_size,
-                                                          const Optional<py::object> &maximum_depth,
-                                                          const Optional<py::str> &records,
-                                                          const Optional<py::str> &format) {
+static void ParseMultiFileReaderOptions(named_parameter_map_t &options, const Optional<py::object> &filename,
+                                        const Optional<py::object> &hive_partitioning,
+                                        const Optional<py::object> &union_by_name,
+                                        const Optional<py::object> &hive_types,
+                                        const Optional<py::object> &hive_types_autocast) {
+	if (!py::none().is(filename)) {
+		auto val = TransformPythonValue(filename);
+		options["filename"] = val;
+	}
+
+	if (!py::none().is(hive_types)) {
+		auto val = TransformPythonValue(hive_types);
+		options["hive_types"] = val;
+	}
+
+	if (!py::none().is(hive_partitioning)) {
+		if (!py::isinstance<py::bool_>(hive_partitioning)) {
+			string actual_type = py::str(hive_partitioning.get_type());
+			throw BinderException("read_json only accepts 'hive_partitioning' as a boolean, not '%s'", actual_type);
+		}
+		auto val = TransformPythonValue(hive_partitioning, LogicalTypeId::BOOLEAN);
+		options["hive_partitioning"] = val;
+	}
+
+	if (!py::none().is(union_by_name)) {
+		if (!py::isinstance<py::bool_>(union_by_name)) {
+			string actual_type = py::str(union_by_name.get_type());
+			throw BinderException("read_json only accepts 'union_by_name' as a boolean, not '%s'", actual_type);
+		}
+		auto val = TransformPythonValue(union_by_name, LogicalTypeId::BOOLEAN);
+		options["union_by_name"] = val;
+	}
+
+	if (!py::none().is(hive_types_autocast)) {
+		if (!py::isinstance<py::bool_>(hive_types_autocast)) {
+			string actual_type = py::str(hive_types_autocast.get_type());
+			throw BinderException("read_json only accepts 'hive_types_autocast' as a boolean, not '%s'", actual_type);
+		}
+		auto val = TransformPythonValue(hive_types_autocast, LogicalTypeId::BOOLEAN);
+		options["hive_types_autocast"] = val;
+	}
+}
+
+unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(
+    const string &name, const Optional<py::object> &columns, const Optional<py::object> &sample_size,
+    const Optional<py::object> &maximum_depth, const Optional<py::str> &records, const Optional<py::str> &format,
+    const Optional<py::object> &date_format, const Optional<py::object> &timestamp_format,
+    const Optional<py::object> &compression, const Optional<py::object> &maximum_object_size,
+    const Optional<py::object> &ignore_errors, const Optional<py::object> &convert_strings_to_integers,
+    const Optional<py::object> &field_appearance_threshold, const Optional<py::object> &map_inference_threshold,
+    const Optional<py::object> &maximum_sample_files, const Optional<py::object> &filename,
+    const Optional<py::object> &hive_partitioning, const Optional<py::object> &union_by_name,
+    const Optional<py::object> &hive_types, const Optional<py::object> &hive_types_autocast) {
 	if (!connection) {
 		throw ConnectionException("Connection has already been closed");
 	}
 
 	named_parameter_map_t options;
 
+	ParseMultiFileReaderOptions(options, filename, hive_partitioning, union_by_name, hive_types, hive_types_autocast);
+
 	if (!py::none().is(columns)) {
 		if (!py::is_dict_like(columns)) {
 			throw BinderException("read_json only accepts 'columns' as a dict[str, str]");
@@ -792,6 +849,36 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(const string &name, co
 		options["format"] = Value(format_option);
 	}
 
+	if (!py::none().is(date_format)) {
+		if (!py::isinstance<py::str>(date_format)) {
+			string actual_type = py::str(date_format.get_type());
+			throw BinderException("read_json only accepts 'date_format' as a string, not '%s'", actual_type);
+		}
+		auto date_format_s = py::reinterpret_borrow<py::str>(date_format);
+		auto date_format_option = std::string(py::str(date_format_s));
+		options["date_format"] = Value(date_format_option);
+	}
+
+	if (!py::none().is(timestamp_format)) {
+		if (!py::isinstance<py::str>(timestamp_format)) {
+			string actual_type = py::str(timestamp_format.get_type());
+			throw BinderException("read_json only accepts 'timestamp_format' as a string, not '%s'", actual_type);
+		}
+		auto timestamp_format_s = py::reinterpret_borrow<py::str>(timestamp_format);
+		auto timestamp_format_option = std::string(py::str(timestamp_format_s));
+		options["timestamp_format"] = Value(timestamp_format_option);
+	}
+
+	if (!py::none().is(compression)) {
+		if (!py::isinstance<py::str>(compression)) {
+			string actual_type = py::str(compression.get_type());
+			throw BinderException("read_json only accepts 'compression' as a string, not '%s'", actual_type);
+		}
+		auto compression_s = py::reinterpret_borrow<py::str>(compression);
+		auto compression_option = std::string(py::str(compression_s));
+		options["compression"] = Value(compression_option);
+	}
+
 	if (!py::none().is(sample_size)) {
 		if (!py::isinstance<py::int_>(sample_size)) {
 			string actual_type = py::str(sample_size.get_type());
@@ -808,6 +895,64 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::ReadJSON(const string &name, co
 		options["maximum_depth"] = Value::INTEGER(py::int_(maximum_depth));
 	}
 
+	if (!py::none().is(maximum_object_size)) {
+		if (!py::isinstance<py::int_>(maximum_object_size)) {
+			string actual_type = py::str(maximum_object_size.get_type());
+			throw BinderException("read_json only accepts 'maximum_object_size' as an unsigned integer, not '%s'",
+			                      actual_type);
+		}
+		auto val = TransformPythonValue(maximum_object_size, LogicalTypeId::UINTEGER);
+		options["maximum_object_size"] = val;
+	}
+
+	if (!py::none().is(ignore_errors)) {
+		if (!py::isinstance<py::bool_>(ignore_errors)) {
+			string actual_type = py::str(ignore_errors.get_type());
+			throw BinderException("read_json only accepts 'ignore_errors' as a boolean, not '%s'", actual_type);
+		}
+		auto val = TransformPythonValue(ignore_errors, LogicalTypeId::BOOLEAN);
+		options["ignore_errors"] = val;
+	}
+
+	if (!py::none().is(convert_strings_to_integers)) {
+		if (!py::isinstance<py::bool_>(convert_strings_to_integers)) {
+			string actual_type = py::str(convert_strings_to_integers.get_type());
+			throw BinderException("read_json only accepts 'convert_strings_to_integers' as a boolean, not '%s'",
+			                      actual_type);
+		}
+		auto val = TransformPythonValue(convert_strings_to_integers, LogicalTypeId::BOOLEAN);
+		options["convert_strings_to_integers"] = val;
+	}
+
+	if (!py::none().is(field_appearance_threshold)) {
+		if (!py::isinstance<py::float_>(field_appearance_threshold)) {
+			string actual_type = py::str(field_appearance_threshold.get_type());
+			throw BinderException("read_json only accepts 'field_appearance_threshold' as a float, not '%s'",
+			                      actual_type);
+		}
+		auto val = TransformPythonValue(field_appearance_threshold, LogicalTypeId::DOUBLE);
+		options["field_appearance_threshold"] = val;
+	}
+
+	if (!py::none().is(map_inference_threshold)) {
+		if (!py::isinstance<py::int_>(map_inference_threshold)) {
+			string actual_type = py::str(map_inference_threshold.get_type());
+			throw BinderException("read_json only accepts 'map_inference_threshold' as an integer, not '%s'",
+			                      actual_type);
+		}
+		auto val = TransformPythonValue(map_inference_threshold, LogicalTypeId::BIGINT);
+		options["map_inference_threshold"] = val;
+	}
+
+	if (!py::none().is(maximum_sample_files)) {
+		if (!py::isinstance<py::int_>(maximum_sample_files)) {
+			string actual_type = py::str(maximum_sample_files.get_type());
+			throw BinderException("read_json only accepts 'maximum_sample_files' as an integer, not '%s'", actual_type);
+		}
+		auto val = TransformPythonValue(maximum_sample_files, LogicalTypeId::BIGINT);
+		options["maximum_sample_files"] = val;
+	}
+
 	bool auto_detect = false;
 	if (!options.count("columns")) {
 		options["auto_detect"] = Value::BOOLEAN(true);
