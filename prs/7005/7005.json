{
  "repo": "duckdb/duckdb",
  "pull_number": 7005,
  "instance_id": "duckdb__duckdb-7005",
  "issue_numbers": [
    "6954"
  ],
  "base_commit": "1d7b9c315e89676eef4c7d4d4a3d045d9c3ff758",
  "patch": "diff --git a/.github/workflows/CodeQuality.yml b/.github/workflows/CodeQuality.yml\nindex 39d8b164a45c..aabb52a3095f 100644\n--- a/.github/workflows/CodeQuality.yml\n+++ b/.github/workflows/CodeQuality.yml\n@@ -124,7 +124,7 @@ jobs:\n       - name: Before Install\n         shell: bash\n         run: |\n-          pip install --prefer-binary \"pandas<=1.5.3\" \"requests>=2.26\" \"pyarrow==8.0\" \"psutil>=5.9.0\" pytest\n+          pip install --prefer-binary pandas \"requests>=2.26\" \"pyarrow==8.0\" \"psutil>=5.9.0\" pytest\n           sudo apt-get install g++\n \n       - name: Coverage Reset\ndiff --git a/.github/workflows/Regression.yml b/.github/workflows/Regression.yml\nindex 7d4d040818e7..e8e56b3cb4a9 100644\n--- a/.github/workflows/Regression.yml\n+++ b/.github/workflows/Regression.yml\n@@ -171,7 +171,7 @@ jobs:\n       shell: bash\n       run: |\n         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build\n-        pip install numpy pytest \"pandas<=1.5.3\" mypy psutil pyarrow\n+        pip install numpy pytest pandas mypy psutil pyarrow\n \n     - name: Setup Ccache\n       uses: hendrikmuhs/ccache-action@main\ndiff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp\nindex a6c1a43a66e8..ae6adb9b9b85 100644\n--- a/src/function/table/arrow.cpp\n+++ b/src/function/table/arrow.cpp\n@@ -19,6 +19,7 @@ LogicalType ArrowTableFunction::GetArrowLogicalType(\n \tif (arrow_convert_data.find(col_idx) == arrow_convert_data.end()) {\n \t\tarrow_convert_data[col_idx] = make_uniq<ArrowConvertData>();\n \t}\n+\tauto &convert_data = *arrow_convert_data[col_idx];\n \tif (format == \"n\") {\n \t\treturn LogicalType::SQLNULL;\n \t} else if (format == \"b\") {\n@@ -52,10 +53,10 @@ LogicalType ArrowTableFunction::GetArrowLogicalType(\n \t\t}\n \t\treturn LogicalType::DECIMAL(width, scale);\n \t} else if (format == \"u\") {\n-\t\tarrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);\n+\t\tconvert_data.variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);\n \t\treturn LogicalType::VARCHAR;\n \t} else if (format == \"U\") {\n-\t\tarrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);\n+\t\tconvert_data.variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);\n \t\treturn LogicalType::VARCHAR;\n \t} else if (format == \"tsn:\") {\n \t\treturn LogicalTypeId::TIMESTAMP_NS;\n@@ -66,56 +67,56 @@ LogicalType ArrowTableFunction::GetArrowLogicalType(\n \t} else if (format == \"tss:\") {\n \t\treturn LogicalTypeId::TIMESTAMP_SEC;\n \t} else if (format == \"tdD\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::DAYS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::DAYS);\n \t\treturn LogicalType::DATE;\n \t} else if (format == \"tdm\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);\n \t\treturn LogicalType::DATE;\n \t} else if (format == \"tts\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);\n \t\treturn LogicalType::TIME;\n \t} else if (format == \"ttm\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);\n \t\treturn LogicalType::TIME;\n \t} else if (format == \"ttu\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);\n \t\treturn LogicalType::TIME;\n \t} else if (format == \"ttn\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);\n \t\treturn LogicalType::TIME;\n \t} else if (format == \"tDs\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);\n \t\treturn LogicalType::INTERVAL;\n \t} else if (format == \"tDm\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);\n \t\treturn LogicalType::INTERVAL;\n \t} else if (format == \"tDu\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);\n \t\treturn LogicalType::INTERVAL;\n \t} else if (format == \"tDn\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);\n \t\treturn LogicalType::INTERVAL;\n \t} else if (format == \"tiD\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::DAYS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::DAYS);\n \t\treturn LogicalType::INTERVAL;\n \t} else if (format == \"tiM\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MONTHS);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::MONTHS);\n \t\treturn LogicalType::INTERVAL;\n \t} else if (format == \"tin\") {\n-\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MONTH_DAY_NANO);\n+\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::MONTH_DAY_NANO);\n \t\treturn LogicalType::INTERVAL;\n \t} else if (format == \"+l\") {\n-\t\tarrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);\n+\t\tconvert_data.variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);\n \t\tauto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);\n \t\treturn LogicalType::LIST(child_type);\n \t} else if (format == \"+L\") {\n-\t\tarrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);\n+\t\tconvert_data.variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);\n \t\tauto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);\n \t\treturn LogicalType::LIST(child_type);\n \t} else if (format[0] == '+' && format[1] == 'w') {\n \t\tstd::string parameters = format.substr(format.find(':') + 1);\n \t\tidx_t fixed_size = std::stoi(parameters);\n-\t\tarrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);\n+\t\tconvert_data.variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);\n \t\tauto child_type = GetArrowLogicalType(*schema.children[0], arrow_convert_data, col_idx);\n \t\treturn LogicalType::LIST(child_type);\n \t} else if (format == \"+s\") {\n@@ -127,7 +128,7 @@ LogicalType ArrowTableFunction::GetArrowLogicalType(\n \t\treturn LogicalType::STRUCT(child_types);\n \n \t} else if (format == \"+m\") {\n-\t\tarrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);\n+\t\tconvert_data.variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);\n \n \t\tauto &arrow_struct_type = *schema.children[0];\n \t\tD_ASSERT(arrow_struct_type.n_children == 2);\n@@ -135,26 +136,26 @@ LogicalType ArrowTableFunction::GetArrowLogicalType(\n \t\tauto value_type = GetArrowLogicalType(*arrow_struct_type.children[1], arrow_convert_data, col_idx);\n \t\treturn LogicalType::MAP(key_type, value_type);\n \t} else if (format == \"z\") {\n-\t\tarrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);\n+\t\tconvert_data.variable_sz_type.emplace_back(ArrowVariableSizeType::NORMAL, 0);\n \t\treturn LogicalType::BLOB;\n \t} else if (format == \"Z\") {\n-\t\tarrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);\n+\t\tconvert_data.variable_sz_type.emplace_back(ArrowVariableSizeType::SUPER_SIZE, 0);\n \t\treturn LogicalType::BLOB;\n \t} else if (format[0] == 'w') {\n \t\tstd::string parameters = format.substr(format.find(':') + 1);\n \t\tidx_t fixed_size = std::stoi(parameters);\n-\t\tarrow_convert_data[col_idx]->variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);\n+\t\tconvert_data.variable_sz_type.emplace_back(ArrowVariableSizeType::FIXED_SIZE, fixed_size);\n \t\treturn LogicalType::BLOB;\n \t} else if (format[0] == 't' && format[1] == 's') {\n \t\t// Timestamp with Timezone\n \t\tif (format[2] == 'n') {\n-\t\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);\n+\t\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::NANOSECONDS);\n \t\t} else if (format[2] == 'u') {\n-\t\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);\n+\t\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::MICROSECONDS);\n \t\t} else if (format[2] == 'm') {\n-\t\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);\n+\t\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::MILLISECONDS);\n \t\t} else if (format[2] == 's') {\n-\t\t\tarrow_convert_data[col_idx]->date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);\n+\t\t\tconvert_data.date_time_precision.emplace_back(ArrowDateTimeType::SECONDS);\n \t\t} else {\n \t\t\tthrow NotImplementedException(\" Timestamptz precision of not accepted\");\n \t\t}\ndiff --git a/src/function/table/arrow_conversion.cpp b/src/function/table/arrow_conversion.cpp\nindex aa123e80397d..6dc752c45ba4 100644\n--- a/src/function/table/arrow_conversion.cpp\n+++ b/src/function/table/arrow_conversion.cpp\n@@ -5,9 +5,19 @@\n #include \"duckdb/common/types/arrow_aux_data.hpp\"\n #include \"duckdb/function/scalar/nested_functions.hpp\"\n \n+namespace {\n+using duckdb::idx_t;\n+struct ArrowConvertDataIndices {\n+\t//! The index that refers to 'variable_sz_type' in ArrowConvertData\n+\tidx_t variable_sized_index;\n+\t//! The index that refers to 'date_time_precision' in ArrowConvertData\n+\tidx_t datetime_precision_index;\n+};\n+} // namespace\n+\n namespace duckdb {\n \n-void ShiftRight(unsigned char *ar, int size, int shift) {\n+static void ShiftRight(unsigned char *ar, int size, int shift) {\n \tint carry = 0;\n \twhile (shift--) {\n \t\tfor (int i = size - 1; i >= 0; --i) {\n@@ -18,8 +28,8 @@ void ShiftRight(unsigned char *ar, int size, int shift) {\n \t}\n }\n \n-void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                     int64_t nested_offset = -1, bool add_null = false) {\n+static void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                            int64_t nested_offset = -1, bool add_null = false) {\n \t// In certains we don't need to or cannot copy arrow's validity mask to duckdb.\n \t//\n \t// The conditions where we do want to copy arrow's mask to duckdb are:\n@@ -68,22 +78,23 @@ void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanLocalState\n \t}\n }\n \n-void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                     int64_t nested_offset, bool add_null = false) {\n+static void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                            int64_t nested_offset, bool add_null = false) {\n \tD_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);\n \tauto &mask = FlatVector::Validity(vector);\n \tGetValidityMask(mask, array, scan_state, size, nested_offset, add_null);\n }\n \n-void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                         std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n-                         std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset = -1,\n-                         ValidityMask *parent_mask = nullptr);\n+static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                                std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n+                                idx_t col_idx, ArrowConvertDataIndices &arrow_convert_idx, int64_t nested_offset = -1,\n+                                ValidityMask *parent_mask = nullptr);\n \n-void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                       std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n-                       std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {\n-\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];\n+static void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                              std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n+                              idx_t col_idx, ArrowConvertDataIndices &arrow_convert_idx, int64_t nested_offset,\n+                              ValidityMask *parent_mask) {\n+\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.variable_sized_index++];\n \tidx_t list_size = 0;\n \tSetValidityMask(vector, array, scan_state, size, nested_offset);\n \tidx_t start_offset = 0;\n@@ -157,10 +168,10 @@ void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowScanLocalState &s\n \t}\n }\n \n-void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                       std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n-                       std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset) {\n-\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];\n+static void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                              std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n+                              idx_t col_idx, ArrowConvertDataIndices &arrow_convert_idx, int64_t nested_offset) {\n+\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.variable_sized_index++];\n \tSetValidityMask(vector, array, scan_state, size, nested_offset);\n \tif (original_type.first == ArrowVariableSizeType::FIXED_SIZE) {\n \t\t//! Have to check validity mask before setting this up\n@@ -195,7 +206,7 @@ void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanLocalState &s\n \t} else {\n \t\t//! Check if last offset is higher than max uint32\n \t\tif (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START\n-\t\t\tthrow std::runtime_error(\"DuckDB does not support Blobs over 4GB\");\n+\t\t\tthrow ConversionException(\"DuckDB does not support Blobs over 4GB\");\n \t\t} // LCOV_EXCL_STOP\n \t\tauto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n \t\tif (nested_offset != -1) {\n@@ -213,7 +224,7 @@ void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanLocalState &s\n \t}\n }\n \n-void ArrowToDuckDBMapVerify(Vector &vector, idx_t count) {\n+static void ArrowToDuckDBMapVerify(Vector &vector, idx_t count) {\n \tauto valid_check = CheckMapValidity(vector, count);\n \tswitch (valid_check) {\n \tcase MapInvalidReason::VALID:\n@@ -246,7 +257,8 @@ static void SetVectorString(Vector &vector, idx_t size, char *cdata, T *offsets)\n \t}\n }\n \n-void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset) {\n+static void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n+                             int64_t nested_offset) {\n \tauto internal_type = GetTypeIdSize(vector.GetType().InternalType());\n \tauto data_ptr = (data_ptr_t)array.buffers[1] + internal_type * (scan_state.chunk_offset + array.offset);\n \tif (nested_offset != -1) {\n@@ -256,8 +268,8 @@ void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &sc\n }\n \n template <class T>\n-void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n-                    idx_t size, int64_t conversion) {\n+static void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n+                           idx_t size, int64_t conversion) {\n \tauto tgt_ptr = (dtime_t *)FlatVector::GetData(vector);\n \tauto &validity_mask = FlatVector::Validity(vector);\n \tauto src_ptr = (T *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n@@ -274,8 +286,8 @@ void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan\n \t}\n }\n \n-void TimestampTZConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n-                           idx_t size, int64_t conversion) {\n+static void TimestampTZConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n+                                  int64_t nested_offset, idx_t size, int64_t conversion) {\n \tauto tgt_ptr = (timestamp_t *)FlatVector::GetData(vector);\n \tauto &validity_mask = FlatVector::Validity(vector);\n \tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n@@ -292,8 +304,8 @@ void TimestampTZConversion(Vector &vector, ArrowArray &array, ArrowScanLocalStat\n \t}\n }\n \n-void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n-                          idx_t size, int64_t conversion) {\n+static void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n+                                 int64_t nested_offset, idx_t size, int64_t conversion) {\n \tauto tgt_ptr = (interval_t *)FlatVector::GetData(vector);\n \tauto src_ptr = (int64_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n \tif (nested_offset != -1) {\n@@ -308,8 +320,8 @@ void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanLocalState\n \t}\n }\n \n-void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n-                              idx_t size) {\n+static void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n+                                     int64_t nested_offset, idx_t size) {\n \tauto tgt_ptr = (interval_t *)FlatVector::GetData(vector);\n \tauto src_ptr = (int32_t *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n \tif (nested_offset != -1) {\n@@ -322,8 +334,8 @@ void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowScanLocalS\n \t}\n }\n \n-void IntervalConversionMonthDayNanos(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n-                                     int64_t nested_offset, idx_t size) {\n+static void IntervalConversionMonthDayNanos(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n+                                            int64_t nested_offset, idx_t size) {\n \tauto tgt_ptr = (interval_t *)FlatVector::GetData(vector);\n \tauto src_ptr = (ArrowInterval *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n \tif (nested_offset != -1) {\n@@ -336,9 +348,10 @@ void IntervalConversionMonthDayNanos(Vector &vector, ArrowArray &array, ArrowSca\n \t}\n }\n \n-void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                         std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data, idx_t col_idx,\n-                         std::pair<idx_t, idx_t> &arrow_convert_idx, int64_t nested_offset, ValidityMask *parent_mask) {\n+static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                                std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n+                                idx_t col_idx, ArrowConvertDataIndices &arrow_convert_idx, int64_t nested_offset,\n+                                ValidityMask *parent_mask) {\n \tswitch (vector.GetType().id()) {\n \tcase LogicalTypeId::SQLNULL:\n \t\tvector.Reference(Value());\n@@ -390,11 +403,11 @@ void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::VARCHAR: {\n-\t\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.first++];\n+\t\tauto original_type = arrow_convert_data[col_idx]->variable_sz_type[arrow_convert_idx.variable_sized_index++];\n \t\tauto cdata = (char *)array.buffers[2];\n \t\tif (original_type.first == ArrowVariableSizeType::SUPER_SIZE) {\n \t\t\tif (((uint64_t *)array.buffers[1])[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START\n-\t\t\t\tthrow std::runtime_error(\"DuckDB does not support Strings over 4GB\");\n+\t\t\t\tthrow ConversionException(\"DuckDB does not support Strings over 4GB\");\n \t\t\t} // LCOV_EXCL_STOP\n \t\t\tauto offsets = (uint64_t *)array.buffers[1] + array.offset + scan_state.chunk_offset;\n \t\t\tif (nested_offset != -1) {\n@@ -411,7 +424,7 @@ void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::DATE: {\n-\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n+\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.datetime_precision_index++];\n \t\tswitch (precision) {\n \t\tcase ArrowDateTimeType::DAYS: {\n \t\t\tDirectConversion(vector, array, scan_state, nested_offset);\n@@ -430,12 +443,12 @@ void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState\n \t\t\tbreak;\n \t\t}\n \t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported precision for Date Type \");\n+\t\t\tthrow NotImplementedException(\"Unsupported precision for Date Type \");\n \t\t}\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::TIME: {\n-\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n+\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.datetime_precision_index++];\n \t\tswitch (precision) {\n \t\tcase ArrowDateTimeType::SECONDS: {\n \t\t\tTimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000000);\n@@ -461,12 +474,12 @@ void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState\n \t\t\tbreak;\n \t\t}\n \t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported precision for Time Type \");\n+\t\t\tthrow NotImplementedException(\"Unsupported precision for Time Type \");\n \t\t}\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::TIMESTAMP_TZ: {\n-\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n+\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.datetime_precision_index++];\n \t\tswitch (precision) {\n \t\tcase ArrowDateTimeType::SECONDS: {\n \t\t\tTimestampTZConversion(vector, array, scan_state, nested_offset, size, 1000000);\n@@ -492,12 +505,12 @@ void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState\n \t\t\tbreak;\n \t\t}\n \t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported precision for TimestampTZ Type \");\n+\t\t\tthrow NotImplementedException(\"Unsupported precision for TimestampTZ Type \");\n \t\t}\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::INTERVAL: {\n-\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.second++];\n+\t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.datetime_precision_index++];\n \t\tswitch (precision) {\n \t\tcase ArrowDateTimeType::SECONDS: {\n \t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000000);\n@@ -534,7 +547,7 @@ void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState\n \t\t\tbreak;\n \t\t}\n \t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported precision for Interval/Duration Type \");\n+\t\t\tthrow NotImplementedException(\"Unsupported precision for Interval/Duration Type \");\n \t\t}\n \t\tbreak;\n \t}\n@@ -585,8 +598,8 @@ void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState\n \t\t\tbreak;\n \t\t}\n \t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported physical type for Decimal: \" +\n-\t\t\t                         TypeIdToString(vector.GetType().InternalType()));\n+\t\t\tthrow NotImplementedException(\"Unsupported physical type for Decimal: %s\",\n+\t\t\t                              TypeIdToString(vector.GetType().InternalType()));\n \t\t}\n \t\tbreak;\n \t}\n@@ -626,7 +639,7 @@ void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState\n \t\tbreak;\n \t}\n \tdefault:\n-\t\tthrow std::runtime_error(\"Unsupported type \" + vector.GetType().ToString());\n+\t\tthrow NotImplementedException(\"Unsupported type %s\", vector.GetType().ToString());\n \t}\n }\n \n@@ -644,7 +657,7 @@ static void SetSelectionVectorLoopWithChecks(SelectionVector &sel, data_ptr_t in\n \tauto indices = (T *)indices_p;\n \tfor (idx_t row = 0; row < size; row++) {\n \t\tif (indices[row] > NumericLimits<uint32_t>::Maximum()) {\n-\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n+\t\t\tthrow ConversionException(\"DuckDB only supports indices that fit on an uint32\");\n \t\t}\n \t\tsel.set_index(row, indices[row]);\n \t}\n@@ -664,8 +677,8 @@ static void SetMaskedSelectionVectorLoop(SelectionVector &sel, data_ptr_t indice\n \t}\n }\n \n-void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType &logical_type, idx_t size,\n-                        ValidityMask *mask = nullptr, idx_t last_element_pos = 0) {\n+static void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType &logical_type, idx_t size,\n+                               ValidityMask *mask = nullptr, idx_t last_element_pos = 0) {\n \tsel.Initialize(size);\n \n \tif (mask) {\n@@ -685,7 +698,7 @@ void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType\n \t\tcase LogicalTypeId::UINTEGER:\n \t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n \t\t\t\t//! Its guaranteed that our indices will point to the last element, so just throw an error\n-\t\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n+\t\t\t\tthrow ConversionException(\"DuckDB only supports indices that fit on an uint32\");\n \t\t\t}\n \t\t\tSetMaskedSelectionVectorLoop<uint32_t>(sel, indices_p, size, *mask, last_element_pos);\n \t\t\tbreak;\n@@ -695,20 +708,20 @@ void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType\n \t\tcase LogicalTypeId::UBIGINT:\n \t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n \t\t\t\t//! Its guaranteed that our indices will point to the last element, so just throw an error\n-\t\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n+\t\t\t\tthrow ConversionException(\"DuckDB only supports indices that fit on an uint32\");\n \t\t\t}\n \t\t\tSetMaskedSelectionVectorLoop<uint64_t>(sel, indices_p, size, *mask, last_element_pos);\n \t\t\tbreak;\n \t\tcase LogicalTypeId::BIGINT:\n \t\t\tif (last_element_pos > NumericLimits<uint32_t>::Maximum()) {\n \t\t\t\t//! Its guaranteed that our indices will point to the last element, so just throw an error\n-\t\t\t\tthrow std::runtime_error(\"DuckDB only supports indices that fit on an uint32\");\n+\t\t\t\tthrow ConversionException(\"DuckDB only supports indices that fit on an uint32\");\n \t\t\t}\n \t\t\tSetMaskedSelectionVectorLoop<int64_t>(sel, indices_p, size, *mask, last_element_pos);\n \t\t\tbreak;\n \n \t\tdefault:\n-\t\t\tthrow std::runtime_error(\"(Arrow) Unsupported type for selection vectors \" + logical_type.ToString());\n+\t\t\tthrow NotImplementedException(\"(Arrow) Unsupported type for selection vectors %s\", logical_type.ToString());\n \t\t}\n \n \t} else {\n@@ -748,17 +761,18 @@ void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, LogicalType\n \t\t\t}\n \t\t\tbreak;\n \t\tdefault:\n-\t\t\tthrow std::runtime_error(\"(Arrow) Unsupported type for selection vectors \" + logical_type.ToString());\n+\t\t\tthrow ConversionException(\"(Arrow) Unsupported type for selection vectors %s\", logical_type.ToString());\n \t\t}\n \t}\n }\n \n-void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                                   std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n-                                   idx_t col_idx, std::pair<idx_t, idx_t> &arrow_convert_idx) {\n+static void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n+                                          idx_t size,\n+                                          std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n+                                          idx_t col_idx, ArrowConvertDataIndices &arrow_convert_idx) {\n \tSelectionVector sel;\n \tauto &dict_vectors = scan_state.arrow_dictionary_vectors;\n-\tif (dict_vectors.find(col_idx) == dict_vectors.end()) {\n+\tif (!dict_vectors.count(col_idx)) {\n \t\t//! We need to set the dictionary data for this column\n \t\tauto base_vector = make_uniq<Vector>(vector.GetType(), array.dictionary->length);\n \t\tSetValidityMask(*base_vector, *array.dictionary, scan_state, array.dictionary->length, 0, array.null_count > 0);\n@@ -791,10 +805,11 @@ void ArrowTableFunction::ArrowToDuckDB(ArrowScanLocalState &scan_state,\n \t\tauto arrow_array_idx = arrow_scan_is_projected ? idx : col_idx;\n \n \t\tif (col_idx == COLUMN_IDENTIFIER_ROW_ID) {\n+\t\t\t// This column is skipped by the projection pushdown\n \t\t\tcontinue;\n \t\t}\n \n-\t\tstd::pair<idx_t, idx_t> arrow_convert_idx {0, 0};\n+\t\tArrowConvertDataIndices arrow_convert_idx {0, 0};\n \t\tauto &array = *scan_state.chunk->arrow_array.children[arrow_array_idx];\n \t\tif (!array.release) {\n \t\t\tthrow InvalidInputException(\"arrow_scan: released array passed\");\n@@ -802,6 +817,7 @@ void ArrowTableFunction::ArrowToDuckDB(ArrowScanLocalState &scan_state,\n \t\tif (array.length != scan_state.chunk->arrow_array.length) {\n \t\t\tthrow InvalidInputException(\"arrow_scan: array length mismatch\");\n \t\t}\n+\t\t// Make sure this Vector keeps the Arrow chunk alive in case we can zero-copy the data\n \t\toutput.data[idx].GetBuffer()->SetAuxiliaryData(make_uniq<ArrowAuxiliaryData>(scan_state.chunk));\n \t\tif (array.dictionary) {\n \t\t\tColumnArrowToDuckDBDictionary(output.data[idx], array, scan_state, output.size(), arrow_convert_data,\ndiff --git a/tools/pythonpkg/cibw.toml b/tools/pythonpkg/cibw.toml\nindex 6355b4cdc97b..0132dbb953c0 100644\n--- a/tools/pythonpkg/cibw.toml\n+++ b/tools/pythonpkg/cibw.toml\n@@ -4,7 +4,7 @@\n [tool.cibuildwheel]\n environment = \"PIP_CONSTRAINT='build-constraints.txt'\"\n before-build = 'pip install oldest-supported-numpy'\n-before-test = 'pip install --prefer-binary \"pandas<=1.5.3\" pytest-timeout mypy \"psutil>=5.9.0\" \"requests>=2.26\" fsspec   && (pip install --prefer-binary \"pyarrow>=8.0\" || true) && (pip install --prefer-binary \"torch\" || true) && (pip install --prefer-binary \"polars\" || true)&& (pip install --prefer-binary \"tensorflow\" || true)'\n+before-test = 'pip install --prefer-binary pandas pytest-timeout mypy \"psutil>=5.9.0\" \"requests>=2.26\" fsspec   && (pip install --prefer-binary \"pyarrow>=8.0\" || true) && (pip install --prefer-binary \"torch\" || true) && (pip install --prefer-binary \"polars\" || true)&& (pip install --prefer-binary \"tensorflow\" || true)'\n test-requires = 'pytest'\n test-command = 'DUCKDB_PYTHON_TEST_EXTENSION_PATH={project} DUCKDB_PYTHON_TEST_EXTENSION_REQUIRED=1 python -m pytest {project}/tests'\n \ndiff --git a/tools/pythonpkg/duckdb_python.cpp b/tools/pythonpkg/duckdb_python.cpp\nindex 557e3bde13cc..b26832a451ab 100644\n--- a/tools/pythonpkg/duckdb_python.cpp\n+++ b/tools/pythonpkg/duckdb_python.cpp\n@@ -1,17 +1,17 @@\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n #include \"duckdb/common/atomic.hpp\"\n #include \"duckdb/common/vector.hpp\"\n #include \"duckdb/parser/parser.hpp\"\n \n #include \"duckdb_python/python_objects.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pyrelation.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n+#include \"duckdb_python/pybind11/exceptions.hpp\"\n #include \"duckdb_python/typing.hpp\"\n-#include \"duckdb_python/exceptions.hpp\"\n #include \"duckdb_python/connection_wrapper.hpp\"\n-#include \"duckdb_python/conversions/pyconnection_default.hpp\"\n+#include \"duckdb_python/pybind11/conversions/pyconnection_default.hpp\"\n \n #include \"duckdb.hpp\"\n \ndiff --git a/tools/pythonpkg/requirements-dev.txt b/tools/pythonpkg/requirements-dev.txt\nindex 74ff0051876a..f824ec542217 100644\n--- a/tools/pythonpkg/requirements-dev.txt\n+++ b/tools/pythonpkg/requirements-dev.txt\n@@ -3,7 +3,7 @@ pybind11>=2.6.0\n setuptools_scm>=6.3\n setuptools>=58\n pytest\n-pandas<=1.5.3\n+pandas\n pyarrow\n mypy\n psutil>=5.9.0\ndiff --git a/tools/pythonpkg/setup.py b/tools/pythonpkg/setup.py\nindex 749ee1325837..a340ce10d22e 100755\n--- a/tools/pythonpkg/setup.py\n+++ b/tools/pythonpkg/setup.py\n@@ -286,7 +286,7 @@ def setup_data_files(data_files):\n     license='MIT',\n     data_files = data_files,\n     packages=[\n-\t\t'pyduckdb',\n+        'pyduckdb',\n         'duckdb-stubs'\n     ],\n     include_package_data=True,\ndiff --git a/tools/pythonpkg/src/CMakeLists.txt b/tools/pythonpkg/src/CMakeLists.txt\nindex 4981c6c1ed89..09b431b88640 100644\n--- a/tools/pythonpkg/src/CMakeLists.txt\n+++ b/tools/pythonpkg/src/CMakeLists.txt\n@@ -6,6 +6,10 @@ include_directories(${pybind11_INCLUDE_DIR})\n include_directories(${PYTHON_INCLUDE_DIRS})\n \n add_subdirectory(pyrelation)\n+add_subdirectory(pybind11)\n+add_subdirectory(numpy)\n+add_subdirectory(native)\n+add_subdirectory(jupyter)\n add_subdirectory(typing)\n add_subdirectory(pyconnection)\n add_subdirectory(common)\n@@ -13,19 +17,14 @@ add_subdirectory(pandas)\n \n add_library(\n   python_src OBJECT\n-  array_wrapper.cpp\n   path_like.cpp\n   pyconnection.cpp\n   python_import_cache.cpp\n   pyrelation.cpp\n   dataframe.cpp\n   pyresult.cpp\n-  map.cpp\n-  vector_conversion.cpp\n-  python_conversion.cpp\n-  python_objects.cpp\n-  pybind_wrapper.cpp\n-  arrow_array_stream.cpp)\n+  pyfilesystem.cpp\n+  map.cpp)\n \n set(ALL_OBJECT_FILES\n     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:python_src>\ndiff --git a/tools/pythonpkg/src/arrow/CMakeLists.txt b/tools/pythonpkg/src/arrow/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..e115f97019d1\n--- /dev/null\n+++ b/tools/pythonpkg/src/arrow/CMakeLists.txt\n@@ -0,0 +1,10 @@\n+# this is used for clang-tidy checks\n+include_directories(${pybind11_INCLUDE_DIR})\n+include_directories(${PYTHON_INCLUDE_DIRS})\n+find_package(pybind11 REQUIRED)\n+\n+add_library(python_arrow OBJECT arrow_array_stream.cpp)\n+\n+set(ALL_OBJECT_FILES\n+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:python_arrow>\n+    PARENT_SCOPE)\ndiff --git a/tools/pythonpkg/src/arrow_array_stream.cpp b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\nsimilarity index 97%\nrename from tools/pythonpkg/src/arrow_array_stream.cpp\nrename to tools/pythonpkg/src/arrow/arrow_array_stream.cpp\nindex 30526e5c9824..ce0ab987924b 100644\n--- a/tools/pythonpkg/src/arrow_array_stream.cpp\n+++ b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n@@ -1,4 +1,4 @@\n-#include \"include/duckdb_python/arrow_array_stream.hpp\"\n+#include \"duckdb_python/arrow/arrow_array_stream.hpp\"\n \n #include \"duckdb/common/assert.hpp\"\n #include \"duckdb/common/common.hpp\"\n@@ -8,7 +8,7 @@\n #include \"duckdb/planner/filter/constant_filter.hpp\"\n #include \"duckdb/planner/table_filter.hpp\"\n \n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pyrelation.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n \n@@ -44,7 +44,8 @@ PyArrowObjectType GetArrowType(const py::handle &obj) {\n }\n \n py::object PythonTableArrowArrayStreamFactory::ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,\n-                                                              ArrowStreamParameters &parameters, ClientConfig &config) {\n+                                                              ArrowStreamParameters &parameters,\n+                                                              const ClientConfig &config) {\n \tauto filters = parameters.filters;\n \tauto &column_list = parameters.projected_columns.columns;\n \tbool has_filter = filters && !filters->filters.empty();\n@@ -294,7 +295,7 @@ py::object TransformFilterRecursive(TableFilter *filter, const string &column_na\n \n py::object PythonTableArrowArrayStreamFactory::TransformFilter(TableFilterSet &filter_collection,\n                                                                std::unordered_map<idx_t, string> &columns,\n-                                                               ClientConfig &config) {\n+                                                               const ClientConfig &config) {\n \tauto filters_map = &filter_collection.filters;\n \tauto it = filters_map->begin();\n \tD_ASSERT(columns.find(it->first) != columns.end());\ndiff --git a/tools/pythonpkg/src/common/exceptions.cpp b/tools/pythonpkg/src/common/exceptions.cpp\nindex 16852800d33f..2ac907f79833 100644\n--- a/tools/pythonpkg/src/common/exceptions.cpp\n+++ b/tools/pythonpkg/src/common/exceptions.cpp\n@@ -1,7 +1,7 @@\n-#include \"duckdb_python/exceptions.hpp\"\n+#include \"duckdb_python/pybind11/exceptions.hpp\"\n \n #include \"duckdb/common/exception.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n namespace py = pybind11;\n \ndiff --git a/tools/pythonpkg/src/dataframe.cpp b/tools/pythonpkg/src/dataframe.cpp\nindex fd9c16518949..e45231083cb9 100644\n--- a/tools/pythonpkg/src/dataframe.cpp\n+++ b/tools/pythonpkg/src/dataframe.cpp\n@@ -1,5 +1,5 @@\n-#include \"duckdb_python/dataframe.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pybind11/dataframe.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n \n namespace duckdb {\n bool PolarsDataFrame::IsDataFrame(const py::handle &object) {\n@@ -7,7 +7,7 @@ bool PolarsDataFrame::IsDataFrame(const py::handle &object) {\n \t\treturn false;\n \t}\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\treturn import_cache.polars().DataFrame.IsInstance(object);\n+\treturn py::isinstance(object, import_cache.polars().DataFrame());\n }\n \n bool PolarsDataFrame::IsLazyFrame(const py::handle &object) {\n@@ -15,17 +15,40 @@ bool PolarsDataFrame::IsLazyFrame(const py::handle &object) {\n \t\treturn false;\n \t}\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\treturn import_cache.polars().LazyFrame.IsInstance(object);\n+\treturn py::isinstance(object, import_cache.polars().LazyFrame());\n }\n \n-bool DataFrame::check_(const py::handle &object) { // NOLINT\n+bool PandasDataFrame::check_(const py::handle &object) { // NOLINT\n+\tif (!ModuleIsLoaded<PandasCacheItem>()) {\n+\t\treturn false;\n+\t}\n+\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\treturn py::isinstance(object, import_cache.pandas().DataFrame());\n+}\n+\n+bool PandasDataFrame::IsPyArrowBacked(const py::handle &df) {\n+\tif (!PandasDataFrame::check_(df)) {\n+\t\treturn false;\n+\t}\n+\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\treturn import_cache.pandas().DataFrame.IsInstance(object);\n+\tpy::list dtypes = df.attr(\"dtypes\");\n+\tif (dtypes.empty()) {\n+\t\treturn false;\n+\t}\n+\n+\tauto arrow_dtype = import_cache.pandas().core.arrays.arrow.dtype.ArrowDtype();\n+\tfor (auto &dtype : dtypes) {\n+\t\tif (py::isinstance(dtype, arrow_dtype)) {\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n }\n \n bool PolarsDataFrame::check_(const py::handle &object) { // NOLINT\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\treturn import_cache.polars().DataFrame.IsInstance(object);\n+\treturn py::isinstance(object, import_cache.polars().DataFrame());\n }\n \n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp b/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp\nsimilarity index 92%\nrename from tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp\nindex c6c81dbcabc7..365bd58f6dc5 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/arrow/arrow_array_stream.hpp\n@@ -15,7 +15,7 @@\n #include \"duckdb/function/table/arrow.hpp\"\n #include \"duckdb/main/client_config.hpp\"\n #include \"duckdb/main/config.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n #include \"duckdb/common/string.hpp\"\n #include \"duckdb/common/vector.hpp\"\n@@ -54,7 +54,7 @@ PyArrowObjectType GetArrowType(const py::handle &obj);\n \n class PythonTableArrowArrayStreamFactory {\n public:\n-\texplicit PythonTableArrowArrayStreamFactory(PyObject *arrow_table, ClientConfig &config)\n+\texplicit PythonTableArrowArrayStreamFactory(PyObject *arrow_table, const ClientConfig &config)\n \t    : arrow_object(arrow_table), config(config) {};\n \n \t//! Produces an Arrow Scanner, should be only called once when initializing Scan States\n@@ -66,15 +66,15 @@ class PythonTableArrowArrayStreamFactory {\n \t//! Arrow Object (i.e., Scanner, Record Batch Reader, Table, Dataset)\n \tPyObject *arrow_object;\n \n-\tClientConfig &config;\n+\tconst ClientConfig &config;\n \n private:\n \t//! We transform a TableFilterSet to an Arrow Expression Object\n \tstatic py::object TransformFilter(TableFilterSet &filters, std::unordered_map<idx_t, string> &columns,\n-\t                                  ClientConfig &config);\n+\t                                  const ClientConfig &config);\n \n \tstatic py::object ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,\n-\t                                 ArrowStreamParameters &parameters, ClientConfig &config);\n+\t                                 ArrowStreamParameters &parameters, const ClientConfig &config);\n };\n } // namespace duckdb\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/connection_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/connection_wrapper.hpp\nindex fcd7d46831f5..0df307d0e55a 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/connection_wrapper.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/connection_wrapper.hpp\n@@ -1,6 +1,6 @@\n #pragma once\n \n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pyrelation.hpp\"\n #include \"duckdb_python/python_objects.hpp\"\n \n@@ -14,14 +14,17 @@ class PyConnectionWrapper {\n \tstatic shared_ptr<DuckDBPyConnection> ExecuteMany(const string &query, py::object params = py::list(),\n \t                                                  shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic unique_ptr<DuckDBPyRelation> DistinctDF(const DataFrame &df, shared_ptr<DuckDBPyConnection> conn = nullptr);\n+\tstatic unique_ptr<DuckDBPyRelation> DistinctDF(const PandasDataFrame &df,\n+\t                                               shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic unique_ptr<DuckDBPyRelation> QueryDF(const DataFrame &df, const string &view_name, const string &sql_query,\n-\t                                            shared_ptr<DuckDBPyConnection> conn = nullptr);\n+\tstatic unique_ptr<DuckDBPyRelation> QueryDF(const PandasDataFrame &df, const string &view_name,\n+\t                                            const string &sql_query, shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic void WriteCsvDF(const DataFrame &df, const string &file, shared_ptr<DuckDBPyConnection> conn = nullptr);\n+\tstatic void WriteCsvDF(const PandasDataFrame &df, const string &file,\n+\t                       shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic unique_ptr<DuckDBPyRelation> AggregateDF(const DataFrame &df, const string &expr, const string &groups = \"\",\n+\tstatic unique_ptr<DuckDBPyRelation> AggregateDF(const PandasDataFrame &df, const string &expr,\n+\t                                                const string &groups = \"\",\n \t                                                shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n \tstatic shared_ptr<DuckDBPyConnection> Execute(const string &query, py::object params = py::list(),\n@@ -41,7 +44,7 @@ class PyConnectionWrapper {\n \t                                           shared_ptr<DuckDBPyConnection> conn = nullptr);\n \tstatic shared_ptr<DuckDBPyType> Type(const string &type_str, shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic shared_ptr<DuckDBPyConnection> Append(const string &name, DataFrame value,\n+\tstatic shared_ptr<DuckDBPyConnection> Append(const string &name, PandasDataFrame value,\n \t                                             shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n \tstatic shared_ptr<DuckDBPyConnection> RegisterPythonObject(const string &name, py::object python_object,\n@@ -127,10 +130,10 @@ class PyConnectionWrapper {\n \n \tstatic py::dict FetchNumpy(shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic DataFrame FetchDF(bool date_as_object, shared_ptr<DuckDBPyConnection> conn = nullptr);\n+\tstatic PandasDataFrame FetchDF(bool date_as_object, shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic DataFrame FetchDFChunk(const idx_t vectors_per_chunk = 1, bool date_as_object = false,\n-\t                              shared_ptr<DuckDBPyConnection> conn = nullptr);\n+\tstatic PandasDataFrame FetchDFChunk(const idx_t vectors_per_chunk = 1, bool date_as_object = false,\n+\t                                    shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n \tstatic duckdb::pyarrow::Table FetchArrow(idx_t rows_per_batch, shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n@@ -148,7 +151,8 @@ class PyConnectionWrapper {\n \tstatic py::list ListFilesystems(shared_ptr<DuckDBPyConnection> conn);\n \tstatic bool FileSystemIsRegistered(const string &name, shared_ptr<DuckDBPyConnection> conn);\n \n-\tstatic unique_ptr<DuckDBPyRelation> FromDF(const DataFrame &df, shared_ptr<DuckDBPyConnection> conn = nullptr);\n+\tstatic unique_ptr<DuckDBPyRelation> FromDF(const PandasDataFrame &df,\n+\t                                           shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n \tstatic unique_ptr<DuckDBPyRelation> FromSubstrait(py::bytes &proto, shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n@@ -158,19 +162,19 @@ class PyConnectionWrapper {\n \tstatic unique_ptr<DuckDBPyRelation> FromParquetDefault(const string &filename,\n \t                                                       shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic unique_ptr<DuckDBPyRelation> ProjectDf(const DataFrame &df, const string &expr,\n+\tstatic unique_ptr<DuckDBPyRelation> ProjectDf(const PandasDataFrame &df, const string &expr,\n \t                                              shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic unique_ptr<DuckDBPyRelation> AliasDF(const DataFrame &df, const string &expr,\n+\tstatic unique_ptr<DuckDBPyRelation> AliasDF(const PandasDataFrame &df, const string &expr,\n \t                                            shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic unique_ptr<DuckDBPyRelation> FilterDf(const DataFrame &df, const string &expr,\n+\tstatic unique_ptr<DuckDBPyRelation> FilterDf(const PandasDataFrame &df, const string &expr,\n \t                                             shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic unique_ptr<DuckDBPyRelation> LimitDF(const DataFrame &df, int64_t n,\n+\tstatic unique_ptr<DuckDBPyRelation> LimitDF(const PandasDataFrame &df, int64_t n,\n \t                                            shared_ptr<DuckDBPyConnection> conn = nullptr);\n \n-\tstatic unique_ptr<DuckDBPyRelation> OrderDf(const DataFrame &df, const string &expr,\n+\tstatic unique_ptr<DuckDBPyRelation> OrderDf(const PandasDataFrame &df, const string &expr,\n \t                                            shared_ptr<DuckDBPyConnection> conn = nullptr);\n };\n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/filesystem_object.hpp b/tools/pythonpkg/src/include/duckdb_python/filesystem_object.hpp\nindex 211a3a7b26a4..76d4b51cf19e 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/filesystem_object.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/filesystem_object.hpp\n@@ -7,7 +7,7 @@\n //===----------------------------------------------------------------------===//\n \n #pragma once\n-#include \"duckdb_python/registered_py_object.hpp\"\n+#include \"duckdb_python/pybind11/registered_py_object.hpp\"\n #include \"duckdb_python/pyfilesystem.hpp\"\n \n namespace duckdb {\n@@ -21,7 +21,7 @@ class FileSystemObject : public RegisteredObject {\n \t\tpy::gil_scoped_acquire acquire;\n \t\t// Assert that the 'obj' is a filesystem\n \t\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\t\tD_ASSERT(import_cache.pyduckdb().filesystem.modified_memory_filesystem.IsInstance(obj));\n+\t\tD_ASSERT(py::isinstance(obj, import_cache.pyduckdb().filesystem.modified_memory_filesystem()));\n \t\tobj.attr(\"delete\")(filename);\n \t}\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp\nindex 2c32bf8695d3..d785787139ae 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/pandas_module.hpp\n@@ -12,6 +12,64 @@\n \n namespace duckdb {\n \n+// pandas.core.arrays.arrow.dtype\n+struct PandasCoreArraysArrowDtypeCacheItem : public PythonImportCacheItem {\n+public:\n+\t~PandasCoreArraysArrowDtypeCacheItem() override {\n+\t}\n+\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n+\t\tArrowDtype.LoadAttribute(\"ArrowDtype\", cache, *this);\n+\t}\n+\n+public:\n+\tPythonImportCacheItem ArrowDtype;\n+};\n+\n+// pandas.core.arrays.arrow\n+struct PandasCoreArraysArrowCacheItem : public PythonImportCacheItem {\n+public:\n+\t~PandasCoreArraysArrowCacheItem() override {\n+\t}\n+\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n+\t\tdtype.LoadModule(\"pandas.core.arrays.arrow.dtype\", cache);\n+\t}\n+\n+public:\n+\tPandasCoreArraysArrowDtypeCacheItem dtype;\n+\n+protected:\n+\tbool IsRequired() const override final {\n+\t\treturn false;\n+\t}\n+};\n+\n+// pandas.core.arrays\n+struct PandasCoreArraysCacheItem : public PythonImportCacheItem {\n+public:\n+\t~PandasCoreArraysCacheItem() override {\n+\t}\n+\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n+\t\tarrow.LoadModule(\"pandas.core.arrays.arrow\", cache);\n+\t}\n+\n+public:\n+\tPandasCoreArraysArrowCacheItem arrow;\n+};\n+\n+// pandas.core\n+struct PandasCoreCacheItem : public PythonImportCacheItem {\n+public:\n+\t~PandasCoreCacheItem() override {\n+\t}\n+\tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n+\t\tarrays.LoadModule(\"pandas.core.arrays\", cache);\n+\t}\n+\n+public:\n+\tPandasCoreArraysCacheItem arrays;\n+};\n+\n+// pandas.libs\n struct PandasLibsCacheItem : public PythonImportCacheItem {\n public:\n \t~PandasLibsCacheItem() override {\n@@ -39,6 +97,7 @@ struct PandasCacheItem : public PythonImportCacheItem {\n \tvirtual void LoadSubtypes(PythonImportCache &cache) override {\n \t\tDataFrame.LoadAttribute(\"DataFrame\", cache, *this);\n \t\tlibs.LoadModule(\"pandas._libs.missing\", cache);\n+\t\tcore.LoadModule(\"pandas.core\", cache);\n \t\tisnull.LoadAttribute(\"isnull\", cache, *this);\n \t}\n \n@@ -46,6 +105,7 @@ struct PandasCacheItem : public PythonImportCacheItem {\n \t//! pandas.DataFrame\n \tPythonImportCacheItem DataFrame;\n \tPandasLibsCacheItem libs;\n+\tPandasCoreCacheItem core;\n \tPythonImportCacheItem isnull;\n \n protected:\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache.hpp\nindex b0d09afef075..6e47b7d72b4b 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache.hpp\n@@ -8,10 +8,9 @@\n \n #pragma once\n \n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb.hpp\"\n #include \"duckdb/common/vector.hpp\"\n-#include \"duckdb_python/python_object_container.hpp\"\n #include \"duckdb_python/import_cache/python_import_cache_modules.hpp\"\n \n namespace duckdb {\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache_item.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache_item.hpp\nindex c45038803b85..f02e2b73f160 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache_item.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/python_import_cache_item.hpp\n@@ -8,10 +8,9 @@\n \n #pragma once\n \n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb.hpp\"\n #include \"duckdb/common/vector.hpp\"\n-#include \"duckdb_python/python_object_container.hpp\"\n \n namespace duckdb {\n \n@@ -29,7 +28,6 @@ struct PythonImportCacheItem {\n public:\n \tbool LoadSucceeded() const;\n \tbool IsLoaded() const;\n-\tbool IsInstance(py::handle object) const;\n \tpy::handle operator()(void) const;\n \tvoid LoadModule(const string &name, PythonImportCache &cache);\n \tvoid LoadAttribute(const string &name, PythonImportCache &cache, PythonImportCacheItem &source);\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/jupyter_progress_bar_display.hpp b/tools/pythonpkg/src/include/duckdb_python/jupyter_progress_bar_display.hpp\nindex e3c15a705af3..e85165dac3aa 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/jupyter_progress_bar_display.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/jupyter_progress_bar_display.hpp\n@@ -8,7 +8,7 @@\n \n #pragma once\n \n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb/common/progress_bar/progress_bar_display.hpp\"\n #include \"duckdb/common/helper.hpp\"\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/map.hpp b/tools/pythonpkg/src/include/duckdb_python/map.hpp\nindex fbdc42e53527..e078d9b2b456 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/map.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/map.hpp\n@@ -1,7 +1,7 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb_python/pandas_scan.hpp\n+// duckdb_python/pandas/pandas_scan.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n@@ -9,7 +9,7 @@\n #pragma once\n \n #include \"duckdb.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb/parser/parsed_data/create_table_function_info.hpp\"\n #include \"duckdb/execution/execution_context.hpp\"\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/array_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp\nsimilarity index 87%\nrename from tools/pythonpkg/src/include/duckdb_python/array_wrapper.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp\nindex aa2c4b3be3fa..966800bd4390 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/array_wrapper.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp\n@@ -8,10 +8,17 @@\n \n #pragma once\n \n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb.hpp\"\n \n namespace duckdb {\n+\n+struct RegisteredArray {\n+\texplicit RegisteredArray(py::array numpy_array) : numpy_array(std::move(numpy_array)) {\n+\t}\n+\tpy::array numpy_array;\n+};\n+\n struct RawArrayWrapper {\n \n \texplicit RawArrayWrapper(const LogicalType &type);\n@@ -28,6 +35,7 @@ struct RawArrayWrapper {\n \tvoid Resize(idx_t new_capacity);\n \tvoid Append(idx_t current_offset, Vector &input, idx_t count);\n };\n+\n struct ArrayWrapper {\n \texplicit ArrayWrapper(const LogicalType &type);\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_bind.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_bind.hpp\nnew file mode 100644\nindex 000000000000..aa79961e0774\n--- /dev/null\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_bind.hpp\n@@ -0,0 +1,16 @@\n+#pragma once\n+\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+#include \"duckdb/common/common.hpp\"\n+\n+namespace duckdb {\n+\n+struct PandasColumnBindData;\n+class ClientContext;\n+\n+struct NumpyBind {\n+\tstatic void Bind(const ClientContext &config, py::handle df, vector<PandasColumnBindData> &out,\n+\t                 vector<LogicalType> &return_types, vector<string> &names);\n+};\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_scan.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_scan.hpp\nnew file mode 100644\nindex 000000000000..4661bf7051bb\n--- /dev/null\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_scan.hpp\n@@ -0,0 +1,14 @@\n+#pragma once\n+\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+#include \"duckdb/common/common.hpp\"\n+\n+namespace duckdb {\n+\n+struct PandasColumnBindData;\n+\n+struct NumpyScan {\n+\tstatic void Scan(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out);\n+};\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pandas_type.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_type.hpp\nsimilarity index 75%\nrename from tools/pythonpkg/src/include/duckdb_python/pandas_type.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/numpy/numpy_type.hpp\nindex 70e0d1ea77f4..7c92d3949e7f 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pandas_type.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_type.hpp\n@@ -1,7 +1,7 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb_python/pandas_type.hpp\n+// duckdb_python/numpy/numpy_type.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n@@ -9,14 +9,13 @@\n #pragma once\n \n #include \"duckdb/common/types.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n-#include \"duckdb_python/dataframe.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n namespace duckdb {\n // Pandas has two different sets of types\n // NumPy dtypes (e.g., bool, int8,...)\n // Pandas Specific Types (e.g., categorical, datetime_tz,...)\n-enum class PandasType : uint8_t {\n+enum class NumpyNullableType : uint8_t {\n \t//! NumPy dtypes\n \tBOOL,      //! bool_, bool8\n \tINT_8,     //! byte, int8\n@@ -36,7 +35,7 @@ enum class PandasType : uint8_t {\n \tTIMEDELTA, //! timedelta64[D], timedelta64\n \n \t//! ------------------------------------------------------------\n-\t//! Pandas Specific Types\n+\t//! Extension Types\n \t//! ------------------------------------------------------------\n \tCATEGORY,    //! category\n \tDATETIME_TZ, //! datetime64[ns, TZ]\n@@ -52,16 +51,7 @@ enum class NumpyObjectType : uint8_t {\n \tDICT,      //! dict of numpy arrays of shape (n,)\n };\n \n-PandasType ConvertPandasType(const py::object &col_type);\n-LogicalType PandasToLogicalType(const PandasType &col_type);\n+NumpyNullableType ConvertNumpyType(const py::handle &col_type);\n+LogicalType NumpyToLogicalType(const NumpyNullableType &col_type);\n \n } // namespace duckdb\n-\n-namespace pybind11 {\n-namespace detail {\n-template <>\n-struct handle_type_name<duckdb::DataFrame> {\n-\tstatic constexpr auto name = _(\"pandas.DataFrame\");\n-};\n-} // namespace detail\n-} // namespace pybind11\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pandas/column/pandas_numpy_column.hpp b/tools/pythonpkg/src/include/duckdb_python/pandas/column/pandas_numpy_column.hpp\nnew file mode 100644\nindex 000000000000..9d8587eed936\n--- /dev/null\n+++ b/tools/pythonpkg/src/include/duckdb_python/pandas/column/pandas_numpy_column.hpp\n@@ -0,0 +1,20 @@\n+#pragma once\n+\n+#include \"duckdb_python/pandas/pandas_column.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+\n+namespace duckdb {\n+\n+class PandasNumpyColumn : public PandasColumn {\n+public:\n+\tPandasNumpyColumn(py::array array_p) : PandasColumn(PandasColumnBackend::NUMPY), array(std::move(array_p)) {\n+\t\tD_ASSERT(py::hasattr(array, \"strides\"));\n+\t\tstride = array.attr(\"strides\").attr(\"__getitem__\")(0).cast<idx_t>();\n+\t}\n+\n+public:\n+\tpy::array array;\n+\tidx_t stride;\n+};\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pandas_analyzer.hpp b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_analyzer.hpp\nsimilarity index 87%\nrename from tools/pythonpkg/src/include/duckdb_python/pandas_analyzer.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/pandas/pandas_analyzer.hpp\nindex e6800fb35339..ac98200cdc8e 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pandas_analyzer.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_analyzer.hpp\n@@ -1,7 +1,7 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb_python/pandas_analyzer.hpp\n+// duckdb_python/pandas/pandas_analyzer.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n@@ -10,9 +10,9 @@\n \n #include \"duckdb/common/types.hpp\"\n #include \"duckdb/main/config.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n-#include \"duckdb_python/python_object_container.hpp\"\n-#include \"duckdb_python/pandas_type.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/gil_wrapper.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n #include \"duckdb_python/python_conversion.hpp\"\n \n namespace duckdb {\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_bind.hpp b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_bind.hpp\nnew file mode 100644\nindex 000000000000..84c0a6d7ac7b\n--- /dev/null\n+++ b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_bind.hpp\n@@ -0,0 +1,29 @@\n+#pragma once\n+\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/python_object_container.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n+#include \"duckdb/common/helper.hpp\"\n+#include \"duckdb_python/pandas/pandas_column.hpp\"\n+\n+namespace duckdb {\n+\n+struct RegisteredArray;\n+class ClientContext;\n+\n+struct PandasColumnBindData {\n+\tNumpyNullableType numpy_type;\n+\tunique_ptr<PandasColumn> pandas_col;\n+\tunique_ptr<RegisteredArray> mask;\n+\t//! Only for categorical types\n+\tstring internal_categorical_type;\n+\t//! When object types are cast we must hold their data somewhere\n+\tPythonObjectContainer<py::str> object_str_val;\n+};\n+\n+struct Pandas {\n+\tstatic void Bind(const ClientContext &config, py::handle df, vector<PandasColumnBindData> &out,\n+\t                 vector<LogicalType> &return_types, vector<string> &names);\n+};\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_column.hpp b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_column.hpp\nnew file mode 100644\nindex 000000000000..e7f95a4595eb\n--- /dev/null\n+++ b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_column.hpp\n@@ -0,0 +1,23 @@\n+#pragma once\n+\n+namespace duckdb {\n+\n+enum class PandasColumnBackend { NUMPY };\n+\n+class PandasColumn {\n+public:\n+\tPandasColumn(PandasColumnBackend backend) : backend(backend) {\n+\t}\n+\tvirtual ~PandasColumn() {\n+\t}\n+\n+public:\n+\tPandasColumnBackend Backend() const {\n+\t\treturn backend;\n+\t}\n+\n+protected:\n+\tPandasColumnBackend backend;\n+};\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_scan.hpp\nsimilarity index 91%\nrename from tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/pandas/pandas_scan.hpp\nindex 03a73550ad93..d9ceefa1b250 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pandas_scan.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_scan.hpp\n@@ -10,8 +10,9 @@\n \n #include \"duckdb.hpp\"\n #include \"duckdb/parser/parsed_data/create_table_function_info.hpp\"\n+#include \"duckdb_python/pandas/pandas_bind.hpp\"\n \n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n namespace duckdb {\n \n@@ -49,6 +50,8 @@ struct PandasScanFunction : public TableFunction {\n \n \t// Helper function that transform pandas df names to make them work with our binder\n \tstatic py::object PandasReplaceCopiedNames(const py::object &original_df);\n+\n+\tstatic void PandasBackendScanSwitch(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out);\n };\n \n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/path_like.hpp b/tools/pythonpkg/src/include/duckdb_python/path_like.hpp\nindex 020e79d56e2a..b78eef480725 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/path_like.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/path_like.hpp\n@@ -1,7 +1,7 @@\n #pragma once\n \n #include \"duckdb/common/common.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb/main/external_dependencies.hpp\"\n \n namespace duckdb {\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/explain_enum.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/conversions/explain_enum.hpp\nsimilarity index 100%\nrename from tools/pythonpkg/src/include/duckdb_python/explain_enum.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/pybind11/conversions/explain_enum.hpp\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/conversions/pyconnection_default.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/conversions/pyconnection_default.hpp\nsimilarity index 95%\nrename from tools/pythonpkg/src/include/duckdb_python/conversions/pyconnection_default.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/pybind11/conversions/pyconnection_default.hpp\nindex bcfd07ee7d51..1c8908b98966 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/conversions/pyconnection_default.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pybind11/conversions/pyconnection_default.hpp\n@@ -1,6 +1,6 @@\n #pragma once\n \n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb/common/helper.hpp\"\n \n using duckdb::DuckDBPyConnection;\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/dataframe.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/dataframe.hpp\nsimilarity index 61%\nrename from tools/pythonpkg/src/include/duckdb_python/dataframe.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/pybind11/dataframe.hpp\nindex 3852830cb08f..df10d766a0e4 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/dataframe.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pybind11/dataframe.hpp\n@@ -1,7 +1,7 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb_python/pandas_dataframe.hpp\n+// duckdb_python/pybind11/dataframe.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n@@ -9,18 +9,19 @@\n #pragma once\n \n #include \"duckdb/common/types.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n namespace duckdb {\n \n-class DataFrame : public py::object {\n+class PandasDataFrame : public py::object {\n public:\n-\tDataFrame(const py::object &o) : py::object(o, borrowed_t {}) {\n+\tPandasDataFrame(const py::object &o) : py::object(o, borrowed_t {}) {\n \t}\n \tusing py::object::object;\n \n public:\n \tstatic bool check_(const py::handle &object); // NOLINT\n+\tstatic bool IsPyArrowBacked(const py::handle &df);\n };\n \n class PolarsDataFrame : public py::object {\n@@ -35,3 +36,12 @@ class PolarsDataFrame : public py::object {\n \tstatic bool check_(const py::handle &object); // NOLINT\n };\n } // namespace duckdb\n+\n+namespace pybind11 {\n+namespace detail {\n+template <>\n+struct handle_type_name<duckdb::PandasDataFrame> {\n+\tstatic constexpr auto name = _(\"pandas.DataFrame\");\n+};\n+} // namespace detail\n+} // namespace pybind11\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/exceptions.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/exceptions.hpp\nsimilarity index 68%\nrename from tools/pythonpkg/src/include/duckdb_python/exceptions.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/pybind11/exceptions.hpp\nindex f1625e55d9b5..f10253e63052 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/exceptions.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pybind11/exceptions.hpp\n@@ -1,4 +1,4 @@\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n namespace py = pybind11;\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pybind11/gil_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/gil_wrapper.hpp\nnew file mode 100644\nindex 000000000000..5a7c81aaf646\n--- /dev/null\n+++ b/tools/pythonpkg/src/include/duckdb_python/pybind11/gil_wrapper.hpp\n@@ -0,0 +1,11 @@\n+#pragma once\n+\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+\n+namespace duckdb {\n+\n+struct PythonGILWrapper {\n+\tpy::gil_scoped_acquire acquire;\n+};\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp\nnew file mode 100644\nindex 000000000000..27eeb6508fd8\n--- /dev/null\n+++ b/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp\n@@ -0,0 +1,82 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb_python/pybind11//pybind_wrapper.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include <pybind11/pybind11.h>\n+#include <pybind11/numpy.h>\n+#include <pybind11/stl.h>\n+#include \"duckdb/common/vector.hpp\"\n+#include \"duckdb/common/assert.hpp\"\n+#include \"duckdb/common/helper.hpp\"\n+\n+PYBIND11_DECLARE_HOLDER_TYPE(T, duckdb::unique_ptr<T>)\n+\n+namespace pybind11 {\n+\n+namespace detail {\n+template <typename Type, typename Alloc>\n+struct type_caster<duckdb::vector<Type, Alloc>> : list_caster<duckdb::vector<Type, Alloc>, Type> {};\n+} // namespace detail\n+\n+bool gil_check();\n+void gil_assert();\n+\n+} // namespace pybind11\n+\n+namespace duckdb {\n+#ifdef __GNUG__\n+#define PYBIND11_NAMESPACE pybind11 __attribute__((visibility(\"hidden\")))\n+#else\n+#define PYBIND11_NAMESPACE pybind11\n+#endif\n+namespace py {\n+\n+// We include everything from pybind11\n+using namespace pybind11;\n+\n+// But we have the option to override certain functions\n+template <typename T, detail::enable_if_t<std::is_base_of<object, T>::value, int> = 0>\n+bool isinstance(handle obj) {\n+\treturn T::check_(obj);\n+}\n+\n+template <typename T, detail::enable_if_t<!std::is_base_of<object, T>::value, int> = 0>\n+bool isinstance(handle obj) {\n+\treturn detail::isinstance_generic(obj, typeid(T));\n+}\n+\n+template <>\n+inline bool isinstance<handle>(handle) = delete;\n+template <>\n+inline bool isinstance<object>(handle obj) {\n+\treturn obj.ptr() != nullptr;\n+}\n+\n+inline bool isinstance(handle obj, handle type) {\n+\tif (type.ptr() == nullptr) {\n+\t\t// The type was not imported, just return false\n+\t\treturn false;\n+\t}\n+\tconst auto result = PyObject_IsInstance(obj.ptr(), type.ptr());\n+\tif (result == -1) {\n+\t\tthrow error_already_set();\n+\t}\n+\treturn result != 0;\n+}\n+\n+} // namespace py\n+\n+template <class T, typename... ARGS>\n+void DefineMethod(std::vector<const char *> aliases, T &mod, ARGS &&... args) {\n+\tfor (auto &alias : aliases) {\n+\t\tmod.def(alias, args...);\n+\t}\n+}\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/python_object_container.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/python_object_container.hpp\nsimilarity index 88%\nrename from tools/pythonpkg/src/include/duckdb_python/python_object_container.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/pybind11/python_object_container.hpp\nindex af020cecd99d..cf1f465e79fd 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/python_object_container.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pybind11/python_object_container.hpp\n@@ -1,14 +1,17 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb_python/python_object_container.hpp\n+// duckdb_python/pybind11/python_object_container.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n \n #pragma once\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb/common/vector.hpp\"\n+#include \"duckdb_python/pybind11/gil_wrapper.hpp\"\n+#include \"duckdb/common/helper.hpp\"\n \n namespace duckdb {\n \n@@ -17,10 +20,6 @@ struct PythonAssignmentFunction {\n \ttypedef void (*assign_t)(TGT_PY_TYPE &, SRC_PY_TYPE &);\n };\n \n-struct PythonGILWrapper {\n-\tpy::gil_scoped_acquire acquire;\n-};\n-\n //! Every Python Object Must be created through our container\n //! The Container ensures that the GIL is HOLD on Python Object Construction/Destruction/Modification\n template <class PY_TYPE>\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/registered_py_object.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/registered_py_object.hpp\nsimilarity index 82%\nrename from tools/pythonpkg/src/include/duckdb_python/registered_py_object.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/pybind11/registered_py_object.hpp\nindex 3e1fe807e0e2..a982cd877c76 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/registered_py_object.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pybind11/registered_py_object.hpp\n@@ -1,13 +1,13 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb_python/registered_py_object.hpp\n+// duckdb_python/pybind11/registered_py_object.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n \n #pragma once\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n namespace duckdb {\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pybind_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind_wrapper.hpp\ndeleted file mode 100644\nindex 544d139dbbb7..000000000000\n--- a/tools/pythonpkg/src/include/duckdb_python/pybind_wrapper.hpp\n+++ /dev/null\n@@ -1,46 +0,0 @@\n-//===----------------------------------------------------------------------===//\n-//                         DuckDB\n-//\n-// duckdb_python/pybind_wrapper.hpp\n-//\n-//\n-//===----------------------------------------------------------------------===//\n-\n-#pragma once\n-\n-#include <pybind11/pybind11.h>\n-#include <pybind11/numpy.h>\n-#include <pybind11/stl.h>\n-#include \"duckdb/common/assert.hpp\"\n-#include \"duckdb/common/vector.hpp\"\n-#include \"duckdb/common/helper.hpp\"\n-\n-PYBIND11_DECLARE_HOLDER_TYPE(T, duckdb::unique_ptr<T>)\n-\n-namespace pybind11 {\n-namespace detail {\n-template <typename Type, typename Alloc>\n-struct type_caster<duckdb::vector<Type, Alloc>> : list_caster<duckdb::vector<Type, Alloc>, Type> {};\n-} // namespace detail\n-\n-bool gil_check();\n-void gil_assert();\n-\n-} // namespace pybind11\n-\n-namespace duckdb {\n-#ifdef __GNUG__\n-#define PYBIND11_NAMESPACE pybind11 __attribute__((visibility(\"hidden\")))\n-#else\n-#define PYBIND11_NAMESPACE pybind11\n-#endif\n-namespace py = pybind11;\n-\n-template <class T, typename... ARGS>\n-void DefineMethod(vector<const char *> aliases, T &mod, ARGS &&... args) {\n-\tfor (auto &alias : aliases) {\n-\t\tmod.def(alias, args...);\n-\t}\n-}\n-\n-} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyconnection.hpp b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\nsimilarity index 93%\nrename from tools/pythonpkg/src/include/duckdb_python/pyconnection.hpp\nrename to tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\nindex 41ebc8a7cb47..aeef324baa32 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyconnection.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\n@@ -7,18 +7,18 @@\n //===----------------------------------------------------------------------===//\n \n #pragma once\n-#include \"arrow_array_stream.hpp\"\n+#include \"duckdb_python/arrow/arrow_array_stream.hpp\"\n #include \"duckdb.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb/common/unordered_map.hpp\"\n #include \"duckdb_python/import_cache/python_import_cache.hpp\"\n-#include \"duckdb_python/registered_py_object.hpp\"\n-#include \"duckdb_python/pandas_type.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n #include \"duckdb_python/pyrelation.hpp\"\n #include \"duckdb_python/pytype.hpp\"\n #include \"duckdb_python/path_like.hpp\"\n #include \"duckdb/execution/operator/persistent/csv_reader_options.hpp\"\n #include \"duckdb_python/pyfilesystem.hpp\"\n+#include \"duckdb_python/pybind11/registered_py_object.hpp\"\n \n namespace duckdb {\n \n@@ -96,7 +96,7 @@ struct DuckDBPyConnection : public std::enable_shared_from_this<DuckDBPyConnecti\n \n \tshared_ptr<DuckDBPyConnection> Execute(const string &query, py::object params = py::list(), bool many = false);\n \n-\tshared_ptr<DuckDBPyConnection> Append(const string &name, const DataFrame &value);\n+\tshared_ptr<DuckDBPyConnection> Append(const string &name, const PandasDataFrame &value);\n \n \tshared_ptr<DuckDBPyConnection> RegisterPythonObject(const string &name, const py::object &python_object);\n \n@@ -115,7 +115,7 @@ struct DuckDBPyConnection : public std::enable_shared_from_this<DuckDBPyConnecti\n \n \tunique_ptr<DuckDBPyRelation> TableFunction(const string &fname, py::object params = py::list());\n \n-\tunique_ptr<DuckDBPyRelation> FromDF(const DataFrame &value);\n+\tunique_ptr<DuckDBPyRelation> FromDF(const PandasDataFrame &value);\n \n \tunique_ptr<DuckDBPyRelation> FromParquet(const string &file_glob, bool binary_as_string, bool file_row_number,\n \t                                         bool filename, bool hive_partitioning, bool union_by_name,\n@@ -162,8 +162,8 @@ struct DuckDBPyConnection : public std::enable_shared_from_this<DuckDBPyConnecti\n \tpy::list FetchAll();\n \n \tpy::dict FetchNumpy();\n-\tDataFrame FetchDF(bool date_as_object);\n-\tDataFrame FetchDFChunk(const idx_t vectors_per_chunk = 1, bool date_as_object = false) const;\n+\tPandasDataFrame FetchDF(bool date_as_object);\n+\tPandasDataFrame FetchDFChunk(const idx_t vectors_per_chunk = 1, bool date_as_object = false) const;\n \n \tduckdb::pyarrow::Table FetchArrow(idx_t rows_per_batch);\n \tPolarsDataFrame FetchPolars(idx_t rows_per_batch);\n@@ -198,12 +198,13 @@ struct DuckDBPyConnection : public std::enable_shared_from_this<DuckDBPyConnecti\n private:\n \tPathLike GetPathLike(const py::object &object);\n \tunique_lock<std::mutex> AcquireConnectionLock();\n+\tvoid RegisterArrowObject(const py::object &arrow_object, const string &name);\n \n \tstatic PythonEnvironmentType environment;\n \tstatic void DetectEnvironment();\n };\n \n-template <class T>\n+template <typename T>\n static bool ModuleIsLoaded() {\n \tauto dict = pybind11::module_::import(\"sys\").attr(\"modules\");\n \treturn dict.contains(py::str(T::Name));\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyfilesystem.hpp b/tools/pythonpkg/src/include/duckdb_python/pyfilesystem.hpp\nindex 00486149ab33..31dfd4e4cffe 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyfilesystem.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyfilesystem.hpp\n@@ -2,9 +2,9 @@\n \n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/gil_wrapper.hpp\"\n #include \"duckdb/common/vector.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n-#include \"duckdb_python/python_object_container.hpp\"\n \n namespace duckdb {\n \n@@ -32,7 +32,7 @@ class AbstractFileSystem : public py::object {\n \n class PythonFileHandle : public FileHandle {\n public:\n-\tPythonFileHandle(FileSystem &file_system, const string &path, const py::object handle);\n+\tPythonFileHandle(FileSystem &file_system, const string &path, const py::object &handle);\n \t~PythonFileHandle() override;\n \tvoid Close() override {\n \t\tPythonGILWrapper gil;\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyrelation.hpp b/tools/pythonpkg/src/include/duckdb_python/pyrelation.hpp\nindex c7ddbbb3ab50..b967773cc2f6 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyrelation.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyrelation.hpp\n@@ -8,15 +8,16 @@\n \n #pragma once\n \n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb.hpp\"\n-#include \"arrow_array_stream.hpp\"\n+#include \"duckdb_python/arrow/arrow_array_stream.hpp\"\n #include \"duckdb/main/external_dependencies.hpp\"\n-#include \"duckdb_python/pandas_type.hpp\"\n-#include \"duckdb_python/registered_py_object.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n+#include \"duckdb_python/pybind11/registered_py_object.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n #include \"duckdb/parser/statement/explain_statement.hpp\"\n-#include \"duckdb_python/explain_enum.hpp\"\n+#include \"duckdb_python/pybind11/conversions/explain_enum.hpp\"\n+#include \"duckdb_python/pybind11/dataframe.hpp\"\n \n namespace duckdb {\n \n@@ -125,7 +126,7 @@ struct DuckDBPyRelation {\n \n \tunique_ptr<DuckDBPyRelation> Distinct();\n \n-\tDataFrame FetchDF(bool date_as_object);\n+\tPandasDataFrame FetchDF(bool date_as_object);\n \n \tOptional<py::tuple> FetchOne();\n \n@@ -141,7 +142,7 @@ struct DuckDBPyRelation {\n \n \tpy::dict FetchNumpyInternal(bool stream = false, idx_t vectors_per_chunk = 1);\n \n-\tDataFrame FetchDFChunk(idx_t vectors_per_chunk, bool date_as_object);\n+\tPandasDataFrame FetchDFChunk(idx_t vectors_per_chunk, bool date_as_object);\n \n \tduckdb::pyarrow::Table ToArrowTable(idx_t batch_size);\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyresult.hpp b/tools/pythonpkg/src/include/duckdb_python/pyresult.hpp\nindex e846f94696d0..75edcc0071dc 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyresult.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyresult.hpp\n@@ -8,10 +8,11 @@\n \n #pragma once\n \n-#include \"array_wrapper.hpp\"\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n #include \"duckdb.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb_python/python_objects.hpp\"\n+#include \"duckdb_python/pybind11/dataframe.hpp\"\n \n namespace duckdb {\n \n@@ -30,11 +31,11 @@ struct DuckDBPyResult {\n \n \tpy::dict FetchNumpyInternal(bool stream = false, idx_t vectors_per_chunk = 1);\n \n-\tDataFrame FetchDF(bool date_as_object);\n+\tPandasDataFrame FetchDF(bool date_as_object);\n \n \tduckdb::pyarrow::Table FetchArrowTable(idx_t rows_per_batch);\n \n-\tDataFrame FetchDFChunk(idx_t vectors_per_chunk, bool date_as_object);\n+\tPandasDataFrame FetchDFChunk(idx_t vectors_per_chunk, bool date_as_object);\n \n \tpy::dict FetchPyTorch();\n \n@@ -60,10 +61,10 @@ struct DuckDBPyResult {\n \n \tbool FetchArrowChunk(QueryResult *result, py::list &batches, idx_t rows_per_batch);\n \n-\tDataFrame FrameFromNumpy(bool date_as_object, const py::handle &o);\n+\tPandasDataFrame FrameFromNumpy(bool date_as_object, const py::handle &o);\n \n-\tvoid ChangeToTZType(DataFrame &df);\n-\tvoid ChangeDateToDatetime(DataFrame &df);\n+\tvoid ChangeToTZType(PandasDataFrame &df);\n+\tvoid ChangeDateToDatetime(PandasDataFrame &df);\n \tunique_ptr<DataChunk> FetchNext(QueryResult &result);\n \tunique_ptr<DataChunk> FetchNextRaw(QueryResult &result);\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp b/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp\nindex 89d41e0b3ec3..5c723cf01cc6 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp\n@@ -8,9 +8,9 @@\n \n #pragma once\n \n-#include \"array_wrapper.hpp\"\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n #include \"duckdb.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb_python/python_objects.hpp\"\n #include \"duckdb/common/types.hpp\"\n #include \"duckdb/common/types/hugeint.hpp\"\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/python_objects.hpp b/tools/pythonpkg/src/include/duckdb_python/python_objects.hpp\nindex 3b83a41ca404..6557375202a6 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/python_objects.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/python_objects.hpp\n@@ -1,6 +1,6 @@\n #pragma once\n \n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb/common/types/time.hpp\"\n #include \"duckdb/common/types/date.hpp\"\n #include \"duckdb/common/types/timestamp.hpp\"\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pytype.hpp b/tools/pythonpkg/src/include/duckdb_python/pytype.hpp\nindex 2f424a698aa7..649e12158036 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pytype.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pytype.hpp\n@@ -1,6 +1,6 @@\n #pragma once\n \n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb/common/types.hpp\"\n \n namespace duckdb {\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/typing.hpp b/tools/pythonpkg/src/include/duckdb_python/typing.hpp\nindex 50da5bc0eb63..4827b536c810 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/typing.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/typing.hpp\n@@ -1,8 +1,8 @@\n #pragma once\n \n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb_python/pytype.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n \n namespace duckdb {\n \ndiff --git a/tools/pythonpkg/src/include/duckdb_python/vector_conversion.hpp b/tools/pythonpkg/src/include/duckdb_python/vector_conversion.hpp\ndeleted file mode 100644\nindex ea4146c0d0f9..000000000000\n--- a/tools/pythonpkg/src/include/duckdb_python/vector_conversion.hpp\n+++ /dev/null\n@@ -1,50 +0,0 @@\n-//===----------------------------------------------------------------------===//\n-//                         DuckDB\n-//\n-// duckdb_python/vector_conversion.hpp\n-//\n-//\n-//===----------------------------------------------------------------------===//\n-\n-#pragma once\n-\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n-\n-#include \"duckdb.hpp\"\n-#include \"duckdb/main/config.hpp\"\n-#include \"duckdb_python/python_object_container.hpp\"\n-#include \"duckdb_python/pandas_analyzer.hpp\"\n-\n-namespace duckdb {\n-\n-struct NumPyArrayWrapper {\n-\texplicit NumPyArrayWrapper(py::array numpy_array) : numpy_array(std::move(numpy_array)) {\n-\t}\n-\n-\tpy::array numpy_array;\n-};\n-\n-struct PandasColumnBindData {\n-\tPandasType pandas_type;\n-\tpy::array numpy_col;\n-\tidx_t numpy_stride;\n-\tunique_ptr<NumPyArrayWrapper> mask;\n-\t// Only for categorical types\n-\tstring internal_categorical_type;\n-\t// When object types are cast we must hold their data somewhere\n-\tPythonObjectContainer<py::str> object_str_val;\n-};\n-\n-class VectorConversion {\n-public:\n-\tstatic void NumpyToDuckDB(PandasColumnBindData &bind_data, py::array &numpy_col, idx_t count, idx_t offset,\n-\t                          Vector &out);\n-\n-\tstatic void BindPandas(const DBConfig &config, py::handle df, vector<PandasColumnBindData> &out,\n-\t                       vector<LogicalType> &return_types, vector<string> &names);\n-\n-\tstatic void BindNumpy(const DBConfig &config, py::handle df, vector<PandasColumnBindData> &out,\n-\t                      vector<LogicalType> &return_types, vector<string> &names);\n-};\n-\n-} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/jupyter/CMakeLists.txt b/tools/pythonpkg/src/jupyter/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..60931e9ca193\n--- /dev/null\n+++ b/tools/pythonpkg/src/jupyter/CMakeLists.txt\n@@ -0,0 +1,10 @@\n+# this is used for clang-tidy checks\n+include_directories(${pybind11_INCLUDE_DIR})\n+include_directories(${PYTHON_INCLUDE_DIRS})\n+find_package(pybind11 REQUIRED)\n+\n+add_library(python_jupyter OBJECT jupyter_progress_bar_display.cpp)\n+\n+set(ALL_OBJECT_FILES\n+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:python_jupyter>\n+    PARENT_SCOPE)\ndiff --git a/tools/pythonpkg/src/jupyter/jupyter_progress_bar_display.cpp b/tools/pythonpkg/src/jupyter/jupyter_progress_bar_display.cpp\nindex 06862c5d537c..474e353ca3f2 100644\n--- a/tools/pythonpkg/src/jupyter/jupyter_progress_bar_display.cpp\n+++ b/tools/pythonpkg/src/jupyter/jupyter_progress_bar_display.cpp\n@@ -1,6 +1,6 @@\n #include \"duckdb_python/jupyter_progress_bar_display.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n namespace duckdb {\n \ndiff --git a/tools/pythonpkg/src/map.cpp b/tools/pythonpkg/src/map.cpp\nindex fdd93b296b64..33ec7cc9e452 100644\n--- a/tools/pythonpkg/src/map.cpp\n+++ b/tools/pythonpkg/src/map.cpp\n@@ -1,7 +1,11 @@\n #include \"duckdb_python/map.hpp\"\n-#include \"duckdb_python/vector_conversion.hpp\"\n-#include \"duckdb_python/array_wrapper.hpp\"\n+#include \"duckdb_python/numpy/numpy_scan.hpp\"\n+#include \"duckdb_python/pandas/pandas_bind.hpp\"\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n+#include \"duckdb_python/pandas/column/pandas_numpy_column.hpp\"\n+#include \"duckdb_python/pandas/pandas_scan.hpp\"\n+#include \"duckdb_python/pybind11/dataframe.hpp\"\n \n namespace duckdb {\n \n@@ -38,6 +42,11 @@ static py::handle FunctionCall(NumpyResultConversion &conversion, vector<string>\n \t\tthrow InvalidInputException(\"No return value from Python function\");\n \t}\n \n+\tif (PandasDataFrame::IsPyArrowBacked(df)) {\n+\t\tthrow InvalidInputException(\n+\t\t    \"Produced DataFrame has columns that are backed by PyArrow, which is not supported yet in 'map'\");\n+\t}\n+\n \treturn df;\n }\n \n@@ -92,7 +101,7 @@ unique_ptr<FunctionData> MapFunction::MapFunctionBind(ClientContext &context, Ta\n \tNumpyResultConversion conversion(data.in_types, 0);\n \tauto df = FunctionCall(conversion, data.in_names, data.function);\n \tvector<PandasColumnBindData> pandas_bind_data; // unused\n-\tVectorConversion::BindPandas(DBConfig::GetConfig(context), df, pandas_bind_data, return_types, names);\n+\tPandas::Bind(context, df, pandas_bind_data, return_types, names);\n \n \t// output types are potentially NULL, this happens for types that map to 'object' dtype\n \tOverrideNullType(return_types, names, data.in_types, data.in_names);\n@@ -126,8 +135,7 @@ OperatorResultType MapFunction::MapFunctionExec(ExecutionContext &context, Table\n \tvector<LogicalType> pandas_return_types;\n \tvector<string> pandas_names;\n \n-\tVectorConversion::BindPandas(DBConfig::GetConfig(context.client), df, pandas_bind_data, pandas_return_types,\n-\t                             pandas_names);\n+\tPandas::Bind(context.client, df, pandas_bind_data, pandas_return_types, pandas_names);\n \tif (pandas_return_types.size() != output.ColumnCount()) {\n \t\tthrow InvalidInputException(\"Expected %llu columns from UDF, got %llu\", output.ColumnCount(),\n \t\t                            pandas_return_types.size());\n@@ -151,8 +159,8 @@ OperatorResultType MapFunction::MapFunctionExec(ExecutionContext &context, Table\n \t}\n \n \tfor (idx_t col_idx = 0; col_idx < output.ColumnCount(); col_idx++) {\n-\t\tVectorConversion::NumpyToDuckDB(pandas_bind_data[col_idx], pandas_bind_data[col_idx].numpy_col, row_count, 0,\n-\t\t                                output.data[col_idx]);\n+\t\tauto &bind_data = pandas_bind_data[col_idx];\n+\t\tPandasScanFunction::PandasBackendScanSwitch(bind_data, row_count, 0, output.data[col_idx]);\n \t}\n \toutput.SetCardinality(row_count);\n \treturn OperatorResultType::NEED_MORE_INPUT;\ndiff --git a/tools/pythonpkg/src/native/CMakeLists.txt b/tools/pythonpkg/src/native/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..85d26aefe609\n--- /dev/null\n+++ b/tools/pythonpkg/src/native/CMakeLists.txt\n@@ -0,0 +1,10 @@\n+# this is used for clang-tidy checks\n+include_directories(${pybind11_INCLUDE_DIR})\n+include_directories(${PYTHON_INCLUDE_DIRS})\n+find_package(pybind11 REQUIRED)\n+\n+add_library(python_native OBJECT python_objects.cpp python_conversion.cpp)\n+\n+set(ALL_OBJECT_FILES\n+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:python_native>\n+    PARENT_SCOPE)\ndiff --git a/tools/pythonpkg/src/python_conversion.cpp b/tools/pythonpkg/src/native/python_conversion.cpp\nsimilarity index 98%\nrename from tools/pythonpkg/src/python_conversion.cpp\nrename to tools/pythonpkg/src/native/python_conversion.cpp\nindex 22128c9943e6..1033ed927d33 100644\n--- a/tools/pythonpkg/src/python_conversion.cpp\n+++ b/tools/pythonpkg/src/native/python_conversion.cpp\n@@ -1,8 +1,8 @@\n #include \"duckdb_python/python_conversion.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n \n #include \"duckdb_python/pyrelation.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n \n #include \"datetime.h\" //From Python\n@@ -252,7 +252,7 @@ PythonObjectType GetPythonObjectType(py::handle &ele) {\n \n \tif (ele.is_none()) {\n \t\treturn PythonObjectType::None;\n-\t} else if (import_cache.pandas().libs.NAType.IsInstance(ele)) {\n+\t} else if (py::isinstance(ele, import_cache.pandas().libs.NAType())) {\n \t\treturn PythonObjectType::None;\n \t} else if (py::isinstance<py::bool_>(ele)) {\n \t\treturn PythonObjectType::Bool;\ndiff --git a/tools/pythonpkg/src/python_objects.cpp b/tools/pythonpkg/src/native/python_objects.cpp\nsimilarity index 99%\nrename from tools/pythonpkg/src/python_objects.cpp\nrename to tools/pythonpkg/src/native/python_objects.cpp\nindex 54e90ad19f81..9441cc861a1c 100644\n--- a/tools/pythonpkg/src/python_objects.cpp\n+++ b/tools/pythonpkg/src/native/python_objects.cpp\n@@ -6,7 +6,7 @@\n #include \"duckdb/common/types/bit.hpp\"\n #include \"duckdb/common/types/cast_helpers.hpp\"\n #include \"duckdb/common/operator/cast_operators.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n \n #include \"datetime.h\" // Python datetime initialize #1\n \ndiff --git a/tools/pythonpkg/src/numpy/CMakeLists.txt b/tools/pythonpkg/src/numpy/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..77c1cf5331a8\n--- /dev/null\n+++ b/tools/pythonpkg/src/numpy/CMakeLists.txt\n@@ -0,0 +1,11 @@\n+# this is used for clang-tidy checks\n+include_directories(${pybind11_INCLUDE_DIR})\n+include_directories(${PYTHON_INCLUDE_DIRS})\n+find_package(pybind11 REQUIRED)\n+\n+add_library(python_numpy OBJECT type.cpp numpy_scan.cpp array_wrapper.cpp\n+                                numpy_bind.cpp)\n+\n+set(ALL_OBJECT_FILES\n+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:python_numpy>\n+    PARENT_SCOPE)\ndiff --git a/tools/pythonpkg/src/array_wrapper.cpp b/tools/pythonpkg/src/numpy/array_wrapper.cpp\nsimilarity index 99%\nrename from tools/pythonpkg/src/array_wrapper.cpp\nrename to tools/pythonpkg/src/numpy/array_wrapper.cpp\nindex 7872c0457efd..a0254d0b2b68 100644\n--- a/tools/pythonpkg/src/array_wrapper.cpp\n+++ b/tools/pythonpkg/src/numpy/array_wrapper.cpp\n@@ -1,4 +1,4 @@\n-#include \"duckdb_python/array_wrapper.hpp\"\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n #include \"duckdb/common/types/date.hpp\"\n #include \"duckdb/common/types/hugeint.hpp\"\n #include \"duckdb/common/types/time.hpp\"\n@@ -7,7 +7,7 @@\n #include \"duckdb/common/types/interval.hpp\"\n #include \"duckdb_python/pyrelation.hpp\"\n #include \"duckdb_python/python_objects.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n #include \"duckdb/common/types/uuid.hpp\"\n \ndiff --git a/tools/pythonpkg/src/numpy/numpy_bind.cpp b/tools/pythonpkg/src/numpy/numpy_bind.cpp\nnew file mode 100644\nindex 000000000000..937f7c458491\n--- /dev/null\n+++ b/tools/pythonpkg/src/numpy/numpy_bind.cpp\n@@ -0,0 +1,76 @@\n+#include \"duckdb_python/numpy/numpy_bind.hpp\"\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n+#include \"duckdb_python/pandas/pandas_analyzer.hpp\"\n+#include \"duckdb_python/pandas/column/pandas_numpy_column.hpp\"\n+#include \"duckdb_python/pandas/pandas_bind.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n+\n+namespace duckdb {\n+\n+void NumpyBind::Bind(const ClientContext &context, py::handle df, vector<PandasColumnBindData> &bind_columns,\n+                     vector<LogicalType> &return_types, vector<string> &names) {\n+\n+\tauto &config = DBConfig::GetConfig(context);\n+\tauto df_columns = py::list(df.attr(\"keys\")());\n+\tauto df_types = py::list();\n+\tfor (auto item : py::cast<py::dict>(df)) {\n+\t\tif (string(py::str(item.second.attr(\"dtype\").attr(\"char\"))) == \"U\") {\n+\t\t\tdf_types.attr(\"append\")(py::str(\"string\"));\n+\t\t\tcontinue;\n+\t\t}\n+\t\tdf_types.attr(\"append\")(py::str(item.second.attr(\"dtype\")));\n+\t}\n+\tauto get_fun = df.attr(\"__getitem__\");\n+\tif (py::len(df_columns) == 0 || py::len(df_types) == 0 || py::len(df_columns) != py::len(df_types)) {\n+\t\tthrow InvalidInputException(\"Need a DataFrame with at least one column\");\n+\t}\n+\tfor (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {\n+\t\tLogicalType duckdb_col_type;\n+\t\tPandasColumnBindData bind_data;\n+\n+\t\tnames.emplace_back(py::str(df_columns[col_idx]));\n+\t\tbind_data.numpy_type = ConvertNumpyType(df_types[col_idx]);\n+\n+\t\tauto column = get_fun(df_columns[col_idx]);\n+\n+\t\tif (bind_data.numpy_type == NumpyNullableType::FLOAT_16) {\n+\t\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(py::array(column.attr(\"astype\")(\"float32\")));\n+\t\t\tbind_data.numpy_type = NumpyNullableType::FLOAT_32;\n+\t\t\tduckdb_col_type = NumpyToLogicalType(bind_data.numpy_type);\n+\t\t} else if (bind_data.numpy_type == NumpyNullableType::OBJECT &&\n+\t\t           string(py::str(df_types[col_idx])) == \"string\") {\n+\t\t\tbind_data.numpy_type = NumpyNullableType::CATEGORY;\n+\t\t\tauto enum_name = string(py::str(df_columns[col_idx]));\n+\t\t\t// here we call numpy.unique\n+\t\t\t// this function call will return the unique values of a given array\n+\t\t\t// together with the indices to reconstruct the given array\n+\t\t\tauto uniq = py::cast<py::tuple>(py::module_::import(\"numpy\").attr(\"unique\")(column, false, true));\n+\t\t\tvector<string> enum_entries = py::cast<vector<string>>(uniq.attr(\"__getitem__\")(0));\n+\t\t\tidx_t size = enum_entries.size();\n+\t\t\tVector enum_entries_vec(LogicalType::VARCHAR, size);\n+\t\t\tauto enum_entries_ptr = FlatVector::GetData<string_t>(enum_entries_vec);\n+\t\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\t\tenum_entries_ptr[i] = StringVector::AddStringOrBlob(enum_entries_vec, enum_entries[i]);\n+\t\t\t}\n+\t\t\tduckdb_col_type = LogicalType::ENUM(enum_name, enum_entries_vec, size);\n+\t\t\tauto pandas_col = uniq.attr(\"__getitem__\")(1);\n+\t\t\tbind_data.internal_categorical_type = string(py::str(pandas_col.attr(\"dtype\")));\n+\t\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(pandas_col);\n+\t\t} else {\n+\t\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(column);\n+\t\t\tduckdb_col_type = NumpyToLogicalType(bind_data.numpy_type);\n+\t\t}\n+\n+\t\tif (bind_data.numpy_type == NumpyNullableType::OBJECT) {\n+\t\t\tPandasAnalyzer analyzer(config);\n+\t\t\tif (analyzer.Analyze(get_fun(df_columns[col_idx]))) {\n+\t\t\t\tduckdb_col_type = analyzer.AnalyzedType();\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn_types.push_back(duckdb_col_type);\n+\t\tbind_columns.push_back(std::move(bind_data));\n+\t}\n+}\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/numpy/numpy_scan.cpp b/tools/pythonpkg/src/numpy/numpy_scan.cpp\nnew file mode 100644\nindex 000000000000..8a9611cebdbc\n--- /dev/null\n+++ b/tools/pythonpkg/src/numpy/numpy_scan.cpp\n@@ -0,0 +1,393 @@\n+#include \"duckdb_python/pyrelation.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n+#include \"duckdb_python/pyresult.hpp\"\n+#include \"duckdb_python/python_conversion.hpp\"\n+#include \"duckdb/common/string_util.hpp\"\n+#include \"duckdb/common/types/timestamp.hpp\"\n+#include \"utf8proc_wrapper.hpp\"\n+#include \"duckdb/common/case_insensitive_map.hpp\"\n+#include \"duckdb_python/pandas/pandas_bind.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n+#include \"duckdb_python/pandas/pandas_analyzer.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n+#include \"duckdb/function/scalar/nested_functions.hpp\"\n+#include \"duckdb_python/numpy/numpy_scan.hpp\"\n+#include \"duckdb_python/pandas/column/pandas_numpy_column.hpp\"\n+\n+namespace duckdb {\n+\n+template <class T>\n+void ScanNumpyColumn(py::array &numpy_col, idx_t stride, idx_t offset, Vector &out, idx_t count) {\n+\tauto src_ptr = (T *)numpy_col.data();\n+\tif (stride == sizeof(T)) {\n+\t\tFlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));\n+\t} else {\n+\t\tauto tgt_ptr = (T *)FlatVector::GetData(out);\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\ttgt_ptr[i] = src_ptr[stride / sizeof(T) * (i + offset)];\n+\t\t}\n+\t}\n+}\n+\n+template <class T, class V>\n+void ScanNumpyCategoryTemplated(py::array &column, idx_t offset, Vector &out, idx_t count) {\n+\tauto src_ptr = (T *)column.data();\n+\tauto tgt_ptr = (V *)FlatVector::GetData(out);\n+\tauto &tgt_mask = FlatVector::Validity(out);\n+\tfor (idx_t i = 0; i < count; i++) {\n+\t\tif (src_ptr[i + offset] == -1) {\n+\t\t\t// Null value\n+\t\t\ttgt_mask.SetInvalid(i);\n+\t\t} else {\n+\t\t\ttgt_ptr[i] = src_ptr[i + offset];\n+\t\t}\n+\t}\n+}\n+\n+template <class T>\n+void ScanNumpyCategory(py::array &column, idx_t count, idx_t offset, Vector &out, string &src_type) {\n+\tif (src_type == \"int8\") {\n+\t\tScanNumpyCategoryTemplated<int8_t, T>(column, offset, out, count);\n+\t} else if (src_type == \"int16\") {\n+\t\tScanNumpyCategoryTemplated<int16_t, T>(column, offset, out, count);\n+\t} else if (src_type == \"int32\") {\n+\t\tScanNumpyCategoryTemplated<int32_t, T>(column, offset, out, count);\n+\t} else if (src_type == \"int64\") {\n+\t\tScanNumpyCategoryTemplated<int64_t, T>(column, offset, out, count);\n+\t} else {\n+\t\tthrow NotImplementedException(\"The Pandas type \" + src_type + \" for categorical types is not implemented yet\");\n+\t}\n+}\n+\n+template <class T>\n+void ScanNumpyMasked(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {\n+\tD_ASSERT(bind_data.pandas_col->Backend() == PandasColumnBackend::NUMPY);\n+\tauto &numpy_col = (PandasNumpyColumn &)*bind_data.pandas_col;\n+\tScanNumpyColumn<T>(numpy_col.array, numpy_col.stride, offset, out, count);\n+\tauto &result_mask = FlatVector::Validity(out);\n+\tif (bind_data.mask) {\n+\t\tauto mask = (bool *)bind_data.mask->numpy_array.data();\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tauto is_null = mask[offset + i];\n+\t\t\tif (is_null) {\n+\t\t\t\tresult_mask.SetInvalid(i);\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+template <class T>\n+void ScanNumpyFpColumn(T *src_ptr, idx_t stride, idx_t count, idx_t offset, Vector &out) {\n+\tauto &mask = FlatVector::Validity(out);\n+\tif (stride == sizeof(T)) {\n+\t\tFlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));\n+\t\t// Turn NaN values into NULL\n+\t\tauto tgt_ptr = FlatVector::GetData<T>(out);\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tif (Value::IsNan<T>(tgt_ptr[i])) {\n+\t\t\t\tmask.SetInvalid(i);\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\tauto tgt_ptr = FlatVector::GetData<T>(out);\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\ttgt_ptr[i] = src_ptr[stride / sizeof(T) * (i + offset)];\n+\t\t\tif (Value::IsNan<T>(tgt_ptr[i])) {\n+\t\t\t\tmask.SetInvalid(i);\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+template <class T>\n+static string_t DecodePythonUnicode(T *codepoints, idx_t codepoint_count, Vector &out) {\n+\t// first figure out how many bytes to allocate\n+\tidx_t utf8_length = 0;\n+\tfor (idx_t i = 0; i < codepoint_count; i++) {\n+\t\tint len = Utf8Proc::CodepointLength(int(codepoints[i]));\n+\t\tD_ASSERT(len >= 1);\n+\t\tutf8_length += len;\n+\t}\n+\tint sz;\n+\tauto result = StringVector::EmptyString(out, utf8_length);\n+\tauto target = result.GetDataWriteable();\n+\tfor (idx_t i = 0; i < codepoint_count; i++) {\n+\t\tUtf8Proc::CodepointToUtf8(int(codepoints[i]), sz, target);\n+\t\tD_ASSERT(sz >= 1);\n+\t\ttarget += sz;\n+\t}\n+\tresult.Finalize();\n+\treturn result;\n+}\n+\n+static void SetInvalidRecursive(Vector &out, idx_t index) {\n+\tauto &validity = FlatVector::Validity(out);\n+\tvalidity.SetInvalid(index);\n+\tif (out.GetType().InternalType() == PhysicalType::STRUCT) {\n+\t\tauto &children = StructVector::GetEntries(out);\n+\t\tfor (idx_t i = 0; i < children.size(); i++) {\n+\t\t\tSetInvalidRecursive(*children[i], index);\n+\t\t}\n+\t}\n+}\n+\n+//! 'count' is the amount of rows in the 'out' vector\n+//! 'offset' is the current row number within this vector\n+void ScanNumpyObject(PandasColumnBindData &bind_data, PyObject *object, idx_t offset, Vector &out) {\n+\n+\t// handle None\n+\tif (object == Py_None) {\n+\t\tSetInvalidRecursive(out, offset);\n+\t\treturn;\n+\t}\n+\n+\tauto val = TransformPythonValue(object, out.GetType());\n+\t// Check if the Value type is accepted for the LogicalType of Vector\n+\tout.SetValue(offset, val);\n+}\n+\n+static void VerifyMapConstraints(Vector &vec, idx_t count) {\n+\tauto invalid_reason = CheckMapValidity(vec, count);\n+\tswitch (invalid_reason) {\n+\tcase MapInvalidReason::VALID:\n+\t\treturn;\n+\tcase MapInvalidReason::DUPLICATE_KEY:\n+\t\tthrow InvalidInputException(\"Dict->Map conversion failed because 'key' list contains duplicates\");\n+\tcase MapInvalidReason::NULL_KEY_LIST:\n+\t\tthrow InvalidInputException(\"Dict->Map conversion failed because 'key' list is None\");\n+\tcase MapInvalidReason::NULL_KEY:\n+\t\tthrow InvalidInputException(\"Dict->Map conversion failed because 'key' list contains None\");\n+\tdefault:\n+\t\tthrow InvalidInputException(\"Option not implemented for MapInvalidReason\");\n+\t}\n+}\n+\n+void VerifyTypeConstraints(Vector &vec, idx_t count) {\n+\tswitch (vec.GetType().id()) {\n+\tcase LogicalTypeId::MAP: {\n+\t\tVerifyMapConstraints(vec, count);\n+\t\tbreak;\n+\t}\n+\tdefault:\n+\t\treturn;\n+\t}\n+}\n+\n+void ScanNumpyObjectColumn(PandasColumnBindData &bind_data, PyObject **col, idx_t count, idx_t offset, Vector &out) {\n+\t// numpy_col is a sequential list of objects, that make up one \"column\" (Vector)\n+\tout.SetVectorType(VectorType::FLAT_VECTOR);\n+\t{\n+\t\tPythonGILWrapper gil; // We're creating python objects here, so we need the GIL\n+\t\tfor (idx_t i = 0; i < count; i++) {\n+\t\t\tidx_t source_idx = offset + i;\n+\t\t\tScanNumpyObject(bind_data, col[source_idx], i, out);\n+\t\t}\n+\t}\n+\tVerifyTypeConstraints(out, count);\n+}\n+\n+//! 'offset' is the offset within the column\n+//! 'count' is the amount of values we will convert in this batch\n+void NumpyScan::Scan(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {\n+\tD_ASSERT(bind_data.pandas_col->Backend() == PandasColumnBackend::NUMPY);\n+\tauto &numpy_col = (PandasNumpyColumn &)*bind_data.pandas_col;\n+\tauto &array = numpy_col.array;\n+\n+\tswitch (bind_data.numpy_type) {\n+\tcase NumpyNullableType::BOOL:\n+\t\tScanNumpyMasked<bool>(bind_data, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::UINT_8:\n+\t\tScanNumpyMasked<uint8_t>(bind_data, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::UINT_16:\n+\t\tScanNumpyMasked<uint16_t>(bind_data, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::UINT_32:\n+\t\tScanNumpyMasked<uint32_t>(bind_data, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::UINT_64:\n+\t\tScanNumpyMasked<uint64_t>(bind_data, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::INT_8:\n+\t\tScanNumpyMasked<int8_t>(bind_data, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::INT_16:\n+\t\tScanNumpyMasked<int16_t>(bind_data, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::INT_32:\n+\t\tScanNumpyMasked<int32_t>(bind_data, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::INT_64:\n+\t\tScanNumpyMasked<int64_t>(bind_data, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::FLOAT_32:\n+\t\tScanNumpyFpColumn<float>((float *)array.data(), numpy_col.stride, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::FLOAT_64:\n+\t\tScanNumpyFpColumn<double>((double *)array.data(), numpy_col.stride, count, offset, out);\n+\t\tbreak;\n+\tcase NumpyNullableType::DATETIME:\n+\tcase NumpyNullableType::DATETIME_TZ: {\n+\t\tauto src_ptr = (int64_t *)array.data();\n+\t\tauto tgt_ptr = FlatVector::GetData<timestamp_t>(out);\n+\t\tauto &mask = FlatVector::Validity(out);\n+\n+\t\tfor (idx_t row = 0; row < count; row++) {\n+\t\t\tauto source_idx = offset + row;\n+\t\t\tif (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {\n+\t\t\t\t// pandas Not a Time (NaT)\n+\t\t\t\tmask.SetInvalid(row);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\ttgt_ptr[row] = Timestamp::FromEpochNanoSeconds(src_ptr[source_idx]);\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase NumpyNullableType::TIMEDELTA: {\n+\t\tauto src_ptr = (int64_t *)array.data();\n+\t\tauto tgt_ptr = FlatVector::GetData<interval_t>(out);\n+\t\tauto &mask = FlatVector::Validity(out);\n+\n+\t\tfor (idx_t row = 0; row < count; row++) {\n+\t\t\tauto source_idx = offset + row;\n+\t\t\tif (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {\n+\t\t\t\t// pandas Not a Time (NaT)\n+\t\t\t\tmask.SetInvalid(row);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tint64_t micro = src_ptr[source_idx] / 1000;\n+\t\t\tint64_t days = micro / Interval::MICROS_PER_DAY;\n+\t\t\tmicro = micro % Interval::MICROS_PER_DAY;\n+\t\t\tint64_t months = days / Interval::DAYS_PER_MONTH;\n+\t\t\tdays = days % Interval::DAYS_PER_MONTH;\n+\t\t\tinterval_t interval;\n+\t\t\tinterval.months = months;\n+\t\t\tinterval.days = days;\n+\t\t\tinterval.micros = micro;\n+\t\t\ttgt_ptr[row] = interval;\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase NumpyNullableType::OBJECT: {\n+\t\t//! We have determined the underlying logical type of this object column\n+\t\t// Get the source pointer of the numpy array\n+\t\tauto src_ptr = (PyObject **)array.data();\n+\t\tif (out.GetType().id() != LogicalTypeId::VARCHAR) {\n+\t\t\treturn ScanNumpyObjectColumn(bind_data, src_ptr, count, offset, out);\n+\t\t}\n+\n+\t\t// Get the data pointer and the validity mask of the result vector\n+\t\tauto tgt_ptr = FlatVector::GetData<string_t>(out);\n+\t\tauto &out_mask = FlatVector::Validity(out);\n+\t\tunique_ptr<PythonGILWrapper> gil;\n+\t\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\n+\t\t// Loop over every row of the arrays contents\n+\t\tauto stride = numpy_col.stride;\n+\t\tfor (idx_t row = 0; row < count; row++) {\n+\t\t\tauto source_idx = stride / sizeof(PyObject *) * (row + offset);\n+\n+\t\t\t// Get the pointer to the object\n+\t\t\tPyObject *val = src_ptr[source_idx];\n+\t\t\tif (bind_data.numpy_type == NumpyNullableType::OBJECT && !py::isinstance<py::str>(val)) {\n+\t\t\t\tif (val == Py_None) {\n+\t\t\t\t\tout_mask.SetInvalid(row);\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tif (import_cache.pandas().libs.NAType.IsLoaded()) {\n+\t\t\t\t\t// If pandas is imported, check if the type is NAType\n+\t\t\t\t\tauto val_type = Py_TYPE(val);\n+\t\t\t\t\tauto na_type = (PyTypeObject *)import_cache.pandas().libs.NAType().ptr();\n+\t\t\t\t\tif (val_type == na_type) {\n+\t\t\t\t\t\tout_mask.SetInvalid(row);\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tif (py::isinstance<py::float_>(val) && std::isnan(PyFloat_AsDouble(val))) {\n+\t\t\t\t\tout_mask.SetInvalid(row);\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tif (!py::isinstance<py::str>(val)) {\n+\t\t\t\t\tif (!gil) {\n+\t\t\t\t\t\tgil = bind_data.object_str_val.GetLock();\n+\t\t\t\t\t}\n+\t\t\t\t\tbind_data.object_str_val.AssignInternal<PyObject>(\n+\t\t\t\t\t    [](py::str &obj, PyObject &new_val) {\n+\t\t\t\t\t\t    py::handle object_handle = &new_val;\n+\t\t\t\t\t\t    obj = py::str(object_handle);\n+\t\t\t\t\t    },\n+\t\t\t\t\t    *val, *gil);\n+\t\t\t\t\tval = (PyObject *)bind_data.object_str_val.GetPointerTop()->ptr();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\t// Python 3 string representation:\n+\t\t\t// https://github.com/python/cpython/blob/3a8fdb28794b2f19f6c8464378fb8b46bce1f5f4/Include/cpython/unicodeobject.h#L79\n+\t\t\tif (!py::isinstance<py::str>(val)) {\n+\t\t\t\tout_mask.SetInvalid(row);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tif (PyUnicode_IS_COMPACT_ASCII(val)) {\n+\t\t\t\t// ascii string: we can zero copy\n+\t\t\t\ttgt_ptr[row] = string_t((const char *)PyUnicode_DATA(val), PyUnicode_GET_LENGTH(val));\n+\t\t\t} else {\n+\t\t\t\t// unicode gunk\n+\t\t\t\tauto ascii_obj = (PyASCIIObject *)val;\n+\t\t\t\tauto unicode_obj = (PyCompactUnicodeObject *)val;\n+\t\t\t\t// compact unicode string: is there utf8 data available?\n+\t\t\t\tif (unicode_obj->utf8) {\n+\t\t\t\t\t// there is! zero copy\n+\t\t\t\t\ttgt_ptr[row] = string_t((const char *)unicode_obj->utf8, unicode_obj->utf8_length);\n+\t\t\t\t} else if (PyUnicode_IS_COMPACT(unicode_obj) && !PyUnicode_IS_ASCII(unicode_obj)) {\n+\t\t\t\t\tauto kind = PyUnicode_KIND(val);\n+\t\t\t\t\tswitch (kind) {\n+\t\t\t\t\tcase PyUnicode_1BYTE_KIND:\n+\t\t\t\t\t\ttgt_ptr[row] =\n+\t\t\t\t\t\t    DecodePythonUnicode<Py_UCS1>(PyUnicode_1BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase PyUnicode_2BYTE_KIND:\n+\t\t\t\t\t\ttgt_ptr[row] =\n+\t\t\t\t\t\t    DecodePythonUnicode<Py_UCS2>(PyUnicode_2BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase PyUnicode_4BYTE_KIND:\n+\t\t\t\t\t\ttgt_ptr[row] =\n+\t\t\t\t\t\t    DecodePythonUnicode<Py_UCS4>(PyUnicode_4BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tdefault:\n+\t\t\t\t\t\tthrow NotImplementedException(\n+\t\t\t\t\t\t    \"Unsupported typekind constant %d for Python Unicode Compact decode\", kind);\n+\t\t\t\t\t}\n+\t\t\t\t} else if (ascii_obj->state.kind == PyUnicode_WCHAR_KIND) {\n+\t\t\t\t\tthrow InvalidInputException(\"Unsupported: decode not ready legacy string\");\n+\t\t\t\t} else if (!PyUnicode_IS_COMPACT(unicode_obj) && ascii_obj->state.kind != PyUnicode_WCHAR_KIND) {\n+\t\t\t\t\tthrow InvalidInputException(\"Unsupported: decode ready legacy string\");\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow InvalidInputException(\"Unsupported string type: no clue what this string is\");\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase NumpyNullableType::CATEGORY: {\n+\t\tswitch (out.GetType().InternalType()) {\n+\t\tcase PhysicalType::UINT8:\n+\t\t\tScanNumpyCategory<uint8_t>(array, count, offset, out, bind_data.internal_categorical_type);\n+\t\t\tbreak;\n+\t\tcase PhysicalType::UINT16:\n+\t\t\tScanNumpyCategory<uint16_t>(array, count, offset, out, bind_data.internal_categorical_type);\n+\t\t\tbreak;\n+\t\tcase PhysicalType::UINT32:\n+\t\t\tScanNumpyCategory<uint32_t>(array, count, offset, out, bind_data.internal_categorical_type);\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Invalid Physical Type for ENUMs\");\n+\t\t}\n+\t\tbreak;\n+\t}\n+\n+\tdefault:\n+\t\tthrow NotImplementedException(\"Unsupported pandas type\");\n+\t}\n+}\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/pandas/type.cpp b/tools/pythonpkg/src/numpy/type.cpp\nsimilarity index 56%\nrename from tools/pythonpkg/src/pandas/type.cpp\nrename to tools/pythonpkg/src/numpy/type.cpp\nindex fff059c1a003..31bb13104a94 100644\n--- a/tools/pythonpkg/src/pandas/type.cpp\n+++ b/tools/pythonpkg/src/numpy/type.cpp\n@@ -1,4 +1,4 @@\n-#include \"duckdb_python/pandas_type.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n #include \"duckdb/common/string.hpp\"\n #include \"duckdb/common/to_string.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n@@ -10,93 +10,102 @@ static bool IsDateTime(const string &col_type_str) {\n \tif (StringUtil::StartsWith(col_type_str, \"datetime64[ns\")) {\n \t\treturn true;\n \t}\n+\tif (StringUtil::StartsWith(col_type_str, \"datetime64[us\")) {\n+\t\treturn true;\n+\t}\n+\tif (StringUtil::StartsWith(col_type_str, \"datetime64[ms\")) {\n+\t\treturn true;\n+\t}\n+\tif (StringUtil::StartsWith(col_type_str, \"datetime64[s\")) {\n+\t\treturn true;\n+\t}\n \tif (col_type_str == \"<M8[ns]\") {\n \t\treturn true;\n \t}\n \treturn false;\n }\n \n-PandasType ConvertPandasType(const py::object &col_type) {\n+NumpyNullableType ConvertNumpyType(const py::handle &col_type) {\n \tauto col_type_str = string(py::str(col_type));\n \n \tif (col_type_str == \"bool\" || col_type_str == \"boolean\") {\n-\t\treturn PandasType::BOOL;\n+\t\treturn NumpyNullableType::BOOL;\n \t} else if (col_type_str == \"uint8\" || col_type_str == \"UInt8\") {\n-\t\treturn PandasType::UINT_8;\n+\t\treturn NumpyNullableType::UINT_8;\n \t} else if (col_type_str == \"uint16\" || col_type_str == \"UInt16\") {\n-\t\treturn PandasType::UINT_16;\n+\t\treturn NumpyNullableType::UINT_16;\n \t} else if (col_type_str == \"uint32\" || col_type_str == \"UInt32\") {\n-\t\treturn PandasType::UINT_32;\n+\t\treturn NumpyNullableType::UINT_32;\n \t} else if (col_type_str == \"uint64\" || col_type_str == \"UInt64\") {\n-\t\treturn PandasType::UINT_64;\n+\t\treturn NumpyNullableType::UINT_64;\n \t} else if (col_type_str == \"int8\" || col_type_str == \"Int8\") {\n-\t\treturn PandasType::INT_8;\n+\t\treturn NumpyNullableType::INT_8;\n \t} else if (col_type_str == \"int16\" || col_type_str == \"Int16\") {\n-\t\treturn PandasType::INT_16;\n+\t\treturn NumpyNullableType::INT_16;\n \t} else if (col_type_str == \"int32\" || col_type_str == \"Int32\") {\n-\t\treturn PandasType::INT_32;\n+\t\treturn NumpyNullableType::INT_32;\n \t} else if (col_type_str == \"int64\" || col_type_str == \"Int64\") {\n-\t\treturn PandasType::INT_64;\n+\t\treturn NumpyNullableType::INT_64;\n \t} else if (col_type_str == \"float16\" || col_type_str == \"Float16\") {\n-\t\treturn PandasType::FLOAT_16;\n+\t\treturn NumpyNullableType::FLOAT_16;\n \t} else if (col_type_str == \"float32\" || col_type_str == \"Float32\") {\n-\t\treturn PandasType::FLOAT_32;\n+\t\treturn NumpyNullableType::FLOAT_32;\n \t} else if (col_type_str == \"float64\" || col_type_str == \"Float64\") {\n-\t\treturn PandasType::FLOAT_64;\n+\t\treturn NumpyNullableType::FLOAT_64;\n \t} else if (col_type_str == \"object\" || col_type_str == \"string\") {\n \t\t//! this better be castable to strings\n-\t\treturn PandasType::OBJECT;\n+\t\treturn NumpyNullableType::OBJECT;\n \t} else if (col_type_str == \"timedelta64[ns]\") {\n-\t\treturn PandasType::TIMEDELTA;\n+\t\treturn NumpyNullableType::TIMEDELTA;\n \t} else if (IsDateTime(col_type_str)) {\n \t\tif (hasattr(col_type, \"tz\")) {\n \t\t\t// The datetime has timezone information.\n-\t\t\treturn PandasType::DATETIME_TZ;\n+\t\t\treturn NumpyNullableType::DATETIME_TZ;\n \t\t}\n-\t\treturn PandasType::DATETIME;\n+\t\treturn NumpyNullableType::DATETIME;\n \t} else if (col_type_str == \"category\") {\n-\t\treturn PandasType::CATEGORY;\n+\t\treturn NumpyNullableType::CATEGORY;\n \t} else {\n \t\tthrow NotImplementedException(\"Data type '%s' not recognized\", col_type_str);\n \t}\n }\n \n-LogicalType PandasToLogicalType(const PandasType &col_type) {\n+LogicalType NumpyToLogicalType(const NumpyNullableType &col_type) {\n \tswitch (col_type) {\n-\tcase PandasType::BOOL:\n+\tcase NumpyNullableType::BOOL:\n \t\treturn LogicalType::BOOLEAN;\n-\tcase PandasType::INT_8:\n+\tcase NumpyNullableType::INT_8:\n \t\treturn LogicalType::TINYINT;\n-\tcase PandasType::UINT_8:\n+\tcase NumpyNullableType::UINT_8:\n \t\treturn LogicalType::UTINYINT;\n-\tcase PandasType::INT_16:\n+\tcase NumpyNullableType::INT_16:\n \t\treturn LogicalType::SMALLINT;\n-\tcase PandasType::UINT_16:\n+\tcase NumpyNullableType::UINT_16:\n \t\treturn LogicalType::USMALLINT;\n-\tcase PandasType::INT_32:\n+\tcase NumpyNullableType::INT_32:\n \t\treturn LogicalType::INTEGER;\n-\tcase PandasType::UINT_32:\n+\tcase NumpyNullableType::UINT_32:\n \t\treturn LogicalType::UINTEGER;\n-\tcase PandasType::INT_64:\n+\tcase NumpyNullableType::INT_64:\n \t\treturn LogicalType::BIGINT;\n-\tcase PandasType::UINT_64:\n+\tcase NumpyNullableType::UINT_64:\n \t\treturn LogicalType::UBIGINT;\n-\tcase PandasType::FLOAT_16:\n+\tcase NumpyNullableType::FLOAT_16:\n \t\treturn LogicalType::FLOAT;\n-\tcase PandasType::FLOAT_32:\n+\tcase NumpyNullableType::FLOAT_32:\n \t\treturn LogicalType::FLOAT;\n-\tcase PandasType::FLOAT_64:\n+\tcase NumpyNullableType::FLOAT_64:\n \t\treturn LogicalType::DOUBLE;\n-\tcase PandasType::OBJECT:\n+\tcase NumpyNullableType::OBJECT:\n \t\treturn LogicalType::VARCHAR;\n-\tcase PandasType::TIMEDELTA:\n+\tcase NumpyNullableType::TIMEDELTA:\n \t\treturn LogicalType::INTERVAL;\n-\tcase PandasType::DATETIME:\n+\tcase NumpyNullableType::DATETIME:\n \t\treturn LogicalType::TIMESTAMP;\n-\tcase PandasType::DATETIME_TZ:\n+\tcase NumpyNullableType::DATETIME_TZ:\n \t\treturn LogicalType::TIMESTAMP_TZ;\n \tdefault:\n-\t\tthrow InternalException(\"No known conversion for PandasType '%d' to LogicalType\");\n+\t\tthrow InternalException(\"No known conversion for NumpyNullableType '%d' to LogicalType\");\n \t}\n }\n \ndiff --git a/tools/pythonpkg/src/pandas/CMakeLists.txt b/tools/pythonpkg/src/pandas/CMakeLists.txt\nindex 57df8da6a4ac..bd0a0733e5eb 100644\n--- a/tools/pythonpkg/src/pandas/CMakeLists.txt\n+++ b/tools/pythonpkg/src/pandas/CMakeLists.txt\n@@ -3,7 +3,7 @@ include_directories(${pybind11_INCLUDE_DIR})\n include_directories(${PYTHON_INCLUDE_DIRS})\n find_package(pybind11 REQUIRED)\n \n-add_library(python_pandas OBJECT type.cpp scan.cpp analyzer.cpp)\n+add_library(python_pandas OBJECT scan.cpp analyzer.cpp bind.cpp)\n \n set(ALL_OBJECT_FILES\n     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:python_pandas>\ndiff --git a/tools/pythonpkg/src/pandas/analyzer.cpp b/tools/pythonpkg/src/pandas/analyzer.cpp\nindex a4f5a7bc78e0..a951d2bff2e7 100644\n--- a/tools/pythonpkg/src/pandas/analyzer.cpp\n+++ b/tools/pythonpkg/src/pandas/analyzer.cpp\n@@ -1,7 +1,7 @@\n #include \"duckdb_python/pyrelation.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n-#include \"duckdb_python/pandas_analyzer.hpp\"\n+#include \"duckdb_python/pandas/pandas_analyzer.hpp\"\n #include \"duckdb_python/python_conversion.hpp\"\n #include \"duckdb/common/types/decimal.hpp\"\n \n@@ -306,10 +306,10 @@ LogicalType PandasAnalyzer::GetItemType(py::handle ele, bool &can_convert) {\n \t\treturn GetItemType(ele.attr(\"tolist\")(), can_convert);\n \t}\n \tcase PythonObjectType::NdArray: {\n-\t\tauto extended_type = ConvertPandasType(ele.attr(\"dtype\"));\n+\t\tauto extended_type = ConvertNumpyType(ele.attr(\"dtype\"));\n \t\tLogicalType ltype;\n-\t\tltype = PandasToLogicalType(extended_type);\n-\t\tif (extended_type == PandasType::OBJECT) {\n+\t\tltype = NumpyToLogicalType(extended_type);\n+\t\tif (extended_type == NumpyNullableType::OBJECT) {\n \t\t\tLogicalType converted_type = InnerAnalyze(ele, can_convert, false, 1);\n \t\t\tif (can_convert) {\n \t\t\t\tltype = converted_type;\n@@ -345,7 +345,9 @@ LogicalType PandasAnalyzer::InnerAnalyze(py::handle column, bool &can_convert, b\n \t// Keys are not guaranteed to start at 0 for Series, use the internal __array__ instead\n \tauto pandas_module = py::module::import(\"pandas\");\n \tauto pandas_series = pandas_module.attr(\"core\").attr(\"series\").attr(\"Series\");\n+\n \tif (py::isinstance(column, pandas_series)) {\n+\t\t// TODO: check if '_values' is more portable, and behaves the same as '__array__()'\n \t\tcolumn = column.attr(\"__array__\")();\n \t}\n \tauto row = column.attr(\"__getitem__\");\ndiff --git a/tools/pythonpkg/src/pandas/bind.cpp b/tools/pythonpkg/src/pandas/bind.cpp\nnew file mode 100644\nindex 000000000000..47c43a48bcee\n--- /dev/null\n+++ b/tools/pythonpkg/src/pandas/bind.cpp\n@@ -0,0 +1,142 @@\n+#include \"duckdb_python/pandas/pandas_bind.hpp\"\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n+#include \"duckdb_python/pandas/pandas_analyzer.hpp\"\n+#include \"duckdb_python/pandas/column/pandas_numpy_column.hpp\"\n+\n+namespace duckdb {\n+\n+namespace {\n+\n+struct PandasBindColumn {\n+public:\n+\tPandasBindColumn(py::handle name, py::handle type, py::handle column) : name(name), type(type), handle(column) {\n+\t}\n+\n+public:\n+\tpy::handle name;\n+\tpy::handle type;\n+\tpy::handle handle;\n+};\n+\n+struct PandasDataFrameBind {\n+public:\n+\texplicit PandasDataFrameBind(py::handle &df) {\n+\t\tnames = py::list(df.attr(\"columns\"));\n+\t\ttypes = py::list(df.attr(\"dtypes\"));\n+\t\tgetter = df.attr(\"__getitem__\");\n+\t}\n+\tPandasBindColumn operator[](idx_t index) const {\n+\t\tD_ASSERT(index < names.size());\n+\t\tauto column = getter(names[index]);\n+\t\tauto type = types[index];\n+\t\tauto name = names[index];\n+\t\treturn PandasBindColumn(name, type, column);\n+\t}\n+\n+public:\n+\tpy::list names;\n+\tpy::list types;\n+\n+private:\n+\tpy::object getter;\n+};\n+\n+}; // namespace\n+\n+static LogicalType BindColumn(PandasBindColumn column_p, PandasColumnBindData &bind_data,\n+                              const ClientContext &context) {\n+\tLogicalType column_type;\n+\tauto &column = column_p.handle;\n+\n+\tauto &config = DBConfig::GetConfig(context);\n+\tbind_data.numpy_type = ConvertNumpyType(column_p.type);\n+\tbool column_has_mask = py::hasattr(column.attr(\"array\"), \"_mask\");\n+\n+\tif (column_has_mask) {\n+\t\t// masked object, fetch the internal data and mask array\n+\t\tbind_data.mask = make_uniq<RegisteredArray>(column.attr(\"array\").attr(\"_mask\"));\n+\t}\n+\n+\tif (bind_data.numpy_type == NumpyNullableType::CATEGORY) {\n+\t\t// for category types, we create an ENUM type for string or use the converted numpy type for the rest\n+\t\tD_ASSERT(py::hasattr(column, \"cat\"));\n+\t\tD_ASSERT(py::hasattr(column.attr(\"cat\"), \"categories\"));\n+\t\tauto categories = py::array(column.attr(\"cat\").attr(\"categories\"));\n+\t\tauto categories_pd_type = ConvertNumpyType(categories.attr(\"dtype\"));\n+\t\tif (categories_pd_type == NumpyNullableType::OBJECT) {\n+\t\t\t// Let's hope the object type is a string.\n+\t\t\tbind_data.numpy_type = NumpyNullableType::CATEGORY;\n+\t\t\tauto enum_name = string(py::str(column_p.name));\n+\t\t\tvector<string> enum_entries = py::cast<vector<string>>(categories);\n+\t\t\tidx_t size = enum_entries.size();\n+\t\t\tVector enum_entries_vec(LogicalType::VARCHAR, size);\n+\t\t\tauto enum_entries_ptr = FlatVector::GetData<string_t>(enum_entries_vec);\n+\t\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\t\tenum_entries_ptr[i] = StringVector::AddStringOrBlob(enum_entries_vec, enum_entries[i]);\n+\t\t\t}\n+\t\t\tD_ASSERT(py::hasattr(column.attr(\"cat\"), \"codes\"));\n+\t\t\tcolumn_type = LogicalType::ENUM(enum_name, enum_entries_vec, size);\n+\t\t\tauto pandas_col = py::array(column.attr(\"cat\").attr(\"codes\"));\n+\t\t\tbind_data.internal_categorical_type = string(py::str(pandas_col.attr(\"dtype\")));\n+\t\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(pandas_col);\n+\t\t} else {\n+\t\t\tauto pandas_col = py::array(column.attr(\"to_numpy\")());\n+\t\t\tauto numpy_type = pandas_col.attr(\"dtype\");\n+\t\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(pandas_col);\n+\t\t\t// for category types (non-strings), we use the converted numpy type\n+\t\t\tbind_data.numpy_type = ConvertNumpyType(numpy_type);\n+\t\t\tcolumn_type = NumpyToLogicalType(bind_data.numpy_type);\n+\t\t}\n+\t} else if (bind_data.numpy_type == NumpyNullableType::FLOAT_16) {\n+\t\tauto pandas_array = column.attr(\"array\");\n+\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(py::array(column.attr(\"to_numpy\")(\"float32\")));\n+\t\tbind_data.numpy_type = NumpyNullableType::FLOAT_32;\n+\t\tcolumn_type = NumpyToLogicalType(bind_data.numpy_type);\n+\t} else {\n+\t\tauto pandas_array = column.attr(\"array\");\n+\t\tif (py::hasattr(pandas_array, \"_data\")) {\n+\t\t\t// This means we can access the numpy array directly\n+\t\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(column.attr(\"array\").attr(\"_data\"));\n+\t\t} else if (py::hasattr(pandas_array, \"asi8\")) {\n+\t\t\t// This is a datetime object, has the option to get the array as int64_t's\n+\t\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(py::array(pandas_array.attr(\"asi8\")));\n+\t\t} else {\n+\t\t\t// Otherwise we have to get it through 'to_numpy()'\n+\t\t\tbind_data.pandas_col = make_uniq<PandasNumpyColumn>(py::array(column.attr(\"to_numpy\")()));\n+\t\t}\n+\t\tcolumn_type = NumpyToLogicalType(bind_data.numpy_type);\n+\t}\n+\t// Analyze the inner data type of the 'object' column\n+\tif (bind_data.numpy_type == NumpyNullableType::OBJECT) {\n+\t\tPandasAnalyzer analyzer(config);\n+\t\tif (analyzer.Analyze(column)) {\n+\t\t\tcolumn_type = analyzer.AnalyzedType();\n+\t\t}\n+\t}\n+\treturn column_type;\n+}\n+\n+void Pandas::Bind(const ClientContext &context, py::handle df_p, vector<PandasColumnBindData> &bind_columns,\n+                  vector<LogicalType> &return_types, vector<string> &names) {\n+\n+\tPandasDataFrameBind df(df_p);\n+\tidx_t column_count = py::len(df.names);\n+\tif (column_count == 0 || py::len(df.types) == 0 || column_count != py::len(df.types)) {\n+\t\tthrow InvalidInputException(\"Need a DataFrame with at least one column\");\n+\t}\n+\n+\treturn_types.reserve(column_count);\n+\tnames.reserve(column_count);\n+\t// loop over every column\n+\tfor (idx_t col_idx = 0; col_idx < column_count; col_idx++) {\n+\t\tPandasColumnBindData bind_data;\n+\n+\t\tnames.emplace_back(py::str(df.names[col_idx]));\n+\t\tauto column_type = BindColumn(df[col_idx], bind_data, context);\n+\n+\t\treturn_types.push_back(column_type);\n+\t\tbind_columns.push_back(std::move(bind_data));\n+\t}\n+}\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/pandas/scan.cpp b/tools/pythonpkg/src/pandas/scan.cpp\nindex 0e081c7631fb..603dc9412101 100644\n--- a/tools/pythonpkg/src/pandas/scan.cpp\n+++ b/tools/pythonpkg/src/pandas/scan.cpp\n@@ -1,9 +1,12 @@\n-#include \"duckdb_python/pandas_scan.hpp\"\n-#include \"duckdb_python/array_wrapper.hpp\"\n+#include \"duckdb_python/pandas/pandas_scan.hpp\"\n+#include \"duckdb_python/pandas/pandas_bind.hpp\"\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n #include \"utf8proc_wrapper.hpp\"\n #include \"duckdb/common/types/timestamp.hpp\"\n-#include \"duckdb_python/vector_conversion.hpp\"\n+#include \"duckdb_python/numpy/numpy_scan.hpp\"\n+#include \"duckdb_python/numpy/numpy_bind.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n+#include \"duckdb_python/pandas/column/pandas_numpy_column.hpp\"\n \n #include \"duckdb/common/atomic.hpp\"\n \n@@ -73,13 +76,15 @@ unique_ptr<FunctionData> PandasScanFunction::PandasScanBind(ClientContext &conte\n \tpy::handle df((PyObject *)(input.inputs[0].GetPointer()));\n \n \tvector<PandasColumnBindData> pandas_bind_data;\n+\n \tauto is_py_dict = py::isinstance<py::dict>(df);\n \tif (is_py_dict) {\n-\t\tVectorConversion::BindNumpy(DBConfig::GetConfig(context), df, pandas_bind_data, return_types, names);\n+\t\tNumpyBind::Bind(context, df, pandas_bind_data, return_types, names);\n \t} else {\n-\t\tVectorConversion::BindPandas(DBConfig::GetConfig(context), df, pandas_bind_data, return_types, names);\n+\t\tPandas::Bind(context, df, pandas_bind_data, return_types, names);\n \t}\n \tauto df_columns = py::list(df.attr(\"keys\")());\n+\n \tauto get_fun = df.attr(\"__getitem__\");\n \tidx_t row_count = py::len(get_fun(df_columns[0]));\n \treturn make_uniq<PandasScanFunctionData>(df, row_count, std::move(pandas_bind_data), return_types);\n@@ -141,6 +146,20 @@ double PandasScanFunction::PandasProgress(ClientContext &context, const Function\n \treturn percentage;\n }\n \n+void PandasScanFunction::PandasBackendScanSwitch(PandasColumnBindData &bind_data, idx_t count, idx_t offset,\n+                                                 Vector &out) {\n+\tauto backend = bind_data.pandas_col->Backend();\n+\tswitch (backend) {\n+\tcase PandasColumnBackend::NUMPY: {\n+\t\tNumpyScan::Scan(bind_data, count, offset, out);\n+\t\tbreak;\n+\t}\n+\tdefault: {\n+\t\tthrow NotImplementedException(\"Type not implemented for PandasColumnBackend\");\n+\t}\n+\t}\n+}\n+\n //! The main pandas scan function: note that this can be called in parallel without the GIL\n //! hence this needs to be GIL-safe, i.e. no methods that create Python objects are allowed\n void PandasScanFunction::PandasScanFunc(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n@@ -159,8 +178,7 @@ void PandasScanFunction::PandasScanFunc(ClientContext &context, TableFunctionInp\n \t\tif (col_idx == COLUMN_IDENTIFIER_ROW_ID) {\n \t\t\toutput.data[idx].Sequence(state.start, 1, this_count);\n \t\t} else {\n-\t\t\tVectorConversion::NumpyToDuckDB(data.pandas_bind_data[col_idx], data.pandas_bind_data[col_idx].numpy_col,\n-\t\t\t                                this_count, state.start, output.data[idx]);\n+\t\t\tPandasBackendScanSwitch(data.pandas_bind_data[col_idx], this_count, state.start, output.data[idx]);\n \t\t}\n \t}\n \tstate.start += this_count;\ndiff --git a/tools/pythonpkg/src/path_like.cpp b/tools/pythonpkg/src/path_like.cpp\nindex 11fb848058fa..5e600b845e2a 100644\n--- a/tools/pythonpkg/src/path_like.cpp\n+++ b/tools/pythonpkg/src/path_like.cpp\n@@ -1,6 +1,6 @@\n #include \"duckdb_python/path_like.hpp\"\n #include \"duckdb_python/pyrelation.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb_python/pyfilesystem.hpp\"\n #include \"duckdb_python/filesystem_object.hpp\"\n@@ -14,7 +14,7 @@ PathLike PathLike::Create(const py::object &object, DuckDBPyConnection &connecti\n \t\tresult.str = py::str(object);\n \t\treturn result;\n \t}\n-\tif (import_cache.pathlib().Path.IsInstance(object)) {\n+\tif (py::isinstance(object, import_cache.pathlib().Path())) {\n \t\tresult.str = py::str(object);\n \t\treturn result;\n \t}\ndiff --git a/tools/pythonpkg/src/pybind11/CMakeLists.txt b/tools/pythonpkg/src/pybind11/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..ed902b3776b6\n--- /dev/null\n+++ b/tools/pythonpkg/src/pybind11/CMakeLists.txt\n@@ -0,0 +1,10 @@\n+# this is used for clang-tidy checks\n+include_directories(${pybind11_INCLUDE_DIR})\n+include_directories(${PYTHON_INCLUDE_DIRS})\n+find_package(pybind11 REQUIRED)\n+\n+add_library(python_pybind11 OBJECT pybind_wrapper.cpp)\n+\n+set(ALL_OBJECT_FILES\n+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:python_pybind11>\n+    PARENT_SCOPE)\ndiff --git a/tools/pythonpkg/src/pybind_wrapper.cpp b/tools/pythonpkg/src/pybind11/pybind_wrapper.cpp\nsimilarity index 87%\nrename from tools/pythonpkg/src/pybind_wrapper.cpp\nrename to tools/pythonpkg/src/pybind11/pybind_wrapper.cpp\nindex 11db54ddf4ff..5bbc05428897 100644\n--- a/tools/pythonpkg/src/pybind_wrapper.cpp\n+++ b/tools/pythonpkg/src/pybind11/pybind_wrapper.cpp\n@@ -1,4 +1,4 @@\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb/common/exception.hpp\"\n \n namespace pybind11 {\ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 6b03cde3c9f1..50fe826e7f51 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -1,4 +1,4 @@\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n \n #include \"duckdb/catalog/default/default_types.hpp\"\n #include \"duckdb/common/arrow/arrow.hpp\"\n@@ -23,13 +23,13 @@\n #include \"duckdb/parser/statement/select_statement.hpp\"\n #include \"duckdb/parser/tableref/subqueryref.hpp\"\n #include \"duckdb/parser/tableref/table_function_ref.hpp\"\n-#include \"duckdb_python/arrow_array_stream.hpp\"\n+#include \"duckdb_python/arrow/arrow_array_stream.hpp\"\n #include \"duckdb_python/map.hpp\"\n-#include \"duckdb_python/pandas_scan.hpp\"\n+#include \"duckdb_python/pandas/pandas_scan.hpp\"\n #include \"duckdb_python/pyrelation.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n #include \"duckdb_python/python_conversion.hpp\"\n-#include \"duckdb_python/pandas_type.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n #include \"duckdb/main/prepared_statement.hpp\"\n #include \"duckdb_python/jupyter_progress_bar_display.hpp\"\n #include \"duckdb_python/pyfilesystem.hpp\"\n@@ -89,6 +89,16 @@ bool DuckDBPyConnection::IsJupyter() {\n \treturn DuckDBPyConnection::environment == PythonEnvironmentType::JUPYTER;\n }\n \n+py::object ArrowTableFromDataframe(const py::object &df) {\n+\ttry {\n+\t\treturn py::module_::import(\"pyarrow\").attr(\"lib\").attr(\"Table\").attr(\"from_pandas\")(df);\n+\t} catch (py::error_already_set &e) {\n+\t\tthrow InvalidInputException(\n+\t\t    \"The dataframe could not be converted to a pyarrow.lib.Table, due to the following python exception: %s\",\n+\t\t    e.what());\n+\t}\n+}\n+\n static void InitializeConnectionMethods(py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>> &m) {\n \tm.def(\"cursor\", &DuckDBPyConnection::Cursor, \"Create a duplicate of the current connection\")\n \t    .def(\"register_filesystem\", &DuckDBPyConnection::RegisterFilesystem, \"Register a fsspec compliant filesystem\",\n@@ -429,11 +439,31 @@ shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Execute(const string &query,\n \treturn shared_from_this();\n }\n \n-shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Append(const string &name, const DataFrame &value) {\n+shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Append(const string &name, const PandasDataFrame &value) {\n \tRegisterPythonObject(\"__append_df\", value);\n \treturn Execute(\"INSERT INTO \\\"\" + name + \"\\\" SELECT * FROM __append_df\");\n }\n \n+void DuckDBPyConnection::RegisterArrowObject(const py::object &arrow_object, const string &name) {\n+\tauto stream_factory =\n+\t    make_uniq<PythonTableArrowArrayStreamFactory>(arrow_object.ptr(), connection->context->config);\n+\tauto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;\n+\tauto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;\n+\t{\n+\t\tpy::gil_scoped_release release;\n+\t\ttemporary_views[name] =\n+\t\t    connection\n+\t\t        ->TableFunction(\"arrow_scan\", {Value::POINTER((uintptr_t)stream_factory.get()),\n+\t\t                                       Value::POINTER((uintptr_t)stream_factory_produce),\n+\t\t                                       Value::POINTER((uintptr_t)stream_factory_get_schema)})\n+\t\t        ->CreateView(name, true, true);\n+\t}\n+\tvector<shared_ptr<ExternalDependency>> dependencies;\n+\tdependencies.push_back(\n+\t    make_shared<PythonDependencies>(make_uniq<RegisteredArrow>(std::move(stream_factory), arrow_object)));\n+\tconnection->context->external_dependencies[name] = std::move(dependencies);\n+}\n+\n shared_ptr<DuckDBPyConnection> DuckDBPyConnection::RegisterPythonObject(const string &name,\n                                                                         const py::object &python_object) {\n \tif (!connection) {\n@@ -441,18 +471,24 @@ shared_ptr<DuckDBPyConnection> DuckDBPyConnection::RegisterPythonObject(const st\n \t}\n \n \tif (DuckDBPyConnection::IsPandasDataframe(python_object)) {\n-\t\tauto new_df = PandasScanFunction::PandasReplaceCopiedNames(python_object);\n-\t\t{\n-\t\t\tpy::gil_scoped_release release;\n-\t\t\ttemporary_views[name] = connection->TableFunction(\"pandas_scan\", {Value::POINTER((uintptr_t)new_df.ptr())})\n-\t\t\t                            ->CreateView(name, true, true);\n-\t\t}\n+\t\tif (PandasDataFrame::IsPyArrowBacked(python_object)) {\n+\t\t\tauto arrow_table = ArrowTableFromDataframe(python_object);\n+\t\t\tRegisterArrowObject(arrow_table, name);\n+\t\t} else {\n+\t\t\tauto new_df = PandasScanFunction::PandasReplaceCopiedNames(python_object);\n+\t\t\t{\n+\t\t\t\tpy::gil_scoped_release release;\n+\t\t\t\ttemporary_views[name] =\n+\t\t\t\t    connection->TableFunction(\"pandas_scan\", {Value::POINTER((uintptr_t)new_df.ptr())})\n+\t\t\t\t        ->CreateView(name, true, true);\n+\t\t\t}\n \n-\t\t// keep a reference\n-\t\tvector<shared_ptr<ExternalDependency>> dependencies;\n-\t\tdependencies.push_back(make_shared<PythonDependencies>(make_uniq<RegisteredObject>(python_object),\n-\t\t                                                       make_uniq<RegisteredObject>(new_df)));\n-\t\tconnection->context->external_dependencies[name] = std::move(dependencies);\n+\t\t\t// keep a reference\n+\t\t\tvector<shared_ptr<ExternalDependency>> dependencies;\n+\t\t\tdependencies.push_back(make_shared<PythonDependencies>(make_uniq<RegisteredObject>(python_object),\n+\t\t\t                                                       make_uniq<RegisteredObject>(new_df)));\n+\t\t\tconnection->context->external_dependencies[name] = std::move(dependencies);\n+\t\t}\n \t} else if (IsAcceptedArrowObject(python_object) || IsPolarsDataframe(python_object)) {\n \t\tpy::object arrow_object;\n \t\tif (IsPolarsDataframe(python_object)) {\n@@ -467,23 +503,7 @@ shared_ptr<DuckDBPyConnection> DuckDBPyConnection::RegisterPythonObject(const st\n \t\t} else {\n \t\t\tarrow_object = python_object;\n \t\t}\n-\t\tauto stream_factory =\n-\t\t    make_uniq<PythonTableArrowArrayStreamFactory>(arrow_object.ptr(), connection->context->config);\n-\t\tauto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;\n-\t\tauto stream_factory_get_schema = PythonTableArrowArrayStreamFactory::GetSchema;\n-\t\t{\n-\t\t\tpy::gil_scoped_release release;\n-\t\t\ttemporary_views[name] =\n-\t\t\t    connection\n-\t\t\t        ->TableFunction(\"arrow_scan\", {Value::POINTER((uintptr_t)stream_factory.get()),\n-\t\t\t                                       Value::POINTER((uintptr_t)stream_factory_produce),\n-\t\t\t                                       Value::POINTER((uintptr_t)stream_factory_get_schema)})\n-\t\t\t        ->CreateView(name, true, true);\n-\t\t}\n-\t\tvector<shared_ptr<ExternalDependency>> dependencies;\n-\t\tdependencies.push_back(\n-\t\t    make_shared<PythonDependencies>(make_uniq<RegisteredArrow>(std::move(stream_factory), arrow_object)));\n-\t\tconnection->context->external_dependencies[name] = std::move(dependencies);\n+\t\tRegisterArrowObject(arrow_object, name);\n \t} else if (DuckDBPyRelation::IsRelation(python_object)) {\n \t\tauto pyrel = py::cast<DuckDBPyRelation *>(python_object);\n \t\tpyrel->CreateView(name, true);\n@@ -859,11 +879,15 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fna\n \t    connection->TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(params)));\n }\n \n-unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(const DataFrame &value) {\n+unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(const PandasDataFrame &value) {\n \tif (!connection) {\n \t\tthrow ConnectionException(\"Connection has already been closed\");\n \t}\n \tstring name = \"df_\" + StringUtil::GenerateRandomName();\n+\tif (PandasDataFrame::IsPyArrowBacked(value)) {\n+\t\tauto table = ArrowTableFromDataframe(value);\n+\t\treturn DuckDBPyConnection::FromArrow(table);\n+\t}\n \tauto new_df = PandasScanFunction::PandasReplaceCopiedNames(value);\n \tvector<Value> params;\n \tparams.emplace_back(Value::POINTER((uintptr_t)new_df.ptr()));\n@@ -1099,14 +1123,14 @@ py::dict DuckDBPyConnection::FetchNumpy() {\n \treturn result->FetchNumpyInternal();\n }\n \n-DataFrame DuckDBPyConnection::FetchDF(bool date_as_object) {\n+PandasDataFrame DuckDBPyConnection::FetchDF(bool date_as_object) {\n \tif (!result) {\n \t\tthrow InvalidInputException(\"No open result set\");\n \t}\n \treturn result->FetchDF(date_as_object);\n }\n \n-DataFrame DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk, bool date_as_object) const {\n+PandasDataFrame DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk, bool date_as_object) const {\n \tif (!result) {\n \t\tthrow InvalidInputException(\"No open result set\");\n \t}\n@@ -1173,12 +1197,18 @@ static unique_ptr<TableRef> TryReplacement(py::dict &dict, py::str &table_name,\n \tvector<unique_ptr<ParsedExpression>> children;\n \tNumpyObjectType numpytype; // Identify the type of accepted numpy objects.\n \tif (DuckDBPyConnection::IsPandasDataframe(entry)) {\n-\t\tstring name = \"df_\" + StringUtil::GenerateRandomName();\n-\t\tauto new_df = PandasScanFunction::PandasReplaceCopiedNames(entry);\n-\t\tchildren.push_back(make_uniq<ConstantExpression>(Value::POINTER((uintptr_t)new_df.ptr())));\n-\t\ttable_function->function = make_uniq<FunctionExpression>(\"pandas_scan\", std::move(children));\n-\t\ttable_function->external_dependency =\n-\t\t    make_uniq<PythonDependencies>(make_uniq<RegisteredObject>(entry), make_uniq<RegisteredObject>(new_df));\n+\t\tif (PandasDataFrame::IsPyArrowBacked(entry)) {\n+\t\t\tauto table = ArrowTableFromDataframe(entry);\n+\t\t\tCreateArrowScan(table, *table_function, children, config);\n+\t\t} else {\n+\t\t\tstring name = \"df_\" + StringUtil::GenerateRandomName();\n+\t\t\tauto new_df = PandasScanFunction::PandasReplaceCopiedNames(entry);\n+\t\t\tchildren.push_back(make_uniq<ConstantExpression>(Value::POINTER((uintptr_t)new_df.ptr())));\n+\t\t\ttable_function->function = make_uniq<FunctionExpression>(\"pandas_scan\", std::move(children));\n+\t\t\ttable_function->external_dependency =\n+\t\t\t    make_uniq<PythonDependencies>(make_uniq<RegisteredObject>(entry), make_uniq<RegisteredObject>(new_df));\n+\t\t}\n+\n \t} else if (DuckDBPyConnection::IsAcceptedArrowObject(entry)) {\n \t\tCreateArrowScan(entry, *table_function, children, config);\n \t} else if (DuckDBPyRelation::IsRelation(entry)) {\n@@ -1431,24 +1461,24 @@ bool DuckDBPyConnection::IsPandasDataframe(const py::object &object) {\n \tif (!ModuleIsLoaded<PandasCacheItem>()) {\n \t\treturn false;\n \t}\n-\tauto &py_import_cache = *DuckDBPyConnection::ImportCache();\n-\treturn py_import_cache.pandas().DataFrame.IsInstance(object);\n+\tauto &import_cache_py = *DuckDBPyConnection::ImportCache();\n+\treturn py::isinstance(object, import_cache_py.pandas().DataFrame());\n }\n \n bool DuckDBPyConnection::IsPolarsDataframe(const py::object &object) {\n \tif (!ModuleIsLoaded<PolarsCacheItem>()) {\n \t\treturn false;\n \t}\n-\tauto &py_import_cache = *DuckDBPyConnection::ImportCache();\n-\treturn py_import_cache.polars().DataFrame.IsInstance(object) ||\n-\t       py_import_cache.polars().LazyFrame.IsInstance(object);\n+\tauto &import_cache_py = *DuckDBPyConnection::ImportCache();\n+\treturn py::isinstance(object, import_cache_py.polars().DataFrame()) ||\n+\t       py::isinstance(object, import_cache_py.polars().LazyFrame());\n }\n \n bool IsValidNumpyDimensions(const py::handle &object, int &dim) {\n \t// check the dimensions of numpy arrays\n \t// should only be called by IsAcceptedNumpyObject\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\tif (!import_cache.numpy().ndarray.IsInstance(object)) {\n+\tif (!py::isinstance(object, import_cache.numpy().ndarray())) {\n \t\treturn false;\n \t}\n \tauto shape = (py::cast<py::array>(object)).attr(\"shape\");\n@@ -1464,7 +1494,7 @@ NumpyObjectType DuckDBPyConnection::IsAcceptedNumpyObject(const py::object &obje\n \t\treturn NumpyObjectType::INVALID;\n \t}\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\tif (import_cache.numpy().ndarray.IsInstance(object)) {\n+\tif (py::isinstance(object, import_cache.numpy().ndarray())) {\n \t\tauto len = py::len((py::cast<py::array>(object)).attr(\"shape\"));\n \t\tswitch (len) {\n \t\tcase 1:\n@@ -1498,16 +1528,16 @@ bool DuckDBPyConnection::IsAcceptedArrowObject(const py::object &object) {\n \tif (!ModuleIsLoaded<ArrowLibCacheItem>()) {\n \t\treturn false;\n \t}\n-\tauto &py_import_cache = *DuckDBPyConnection::ImportCache();\n-\tif (py_import_cache.arrow_lib().Table.IsInstance(object) ||\n-\t    py_import_cache.arrow_lib().RecordBatchReader.IsInstance(object)) {\n+\tauto &import_cache_py = *DuckDBPyConnection::ImportCache();\n+\tif (py::isinstance(object, import_cache_py.arrow_lib().Table()) ||\n+\t    py::isinstance(object, import_cache_py.arrow_lib().RecordBatchReader())) {\n \t\treturn true;\n \t}\n \tif (!ModuleIsLoaded<ArrowDatasetCacheItem>()) {\n \t\treturn false;\n \t}\n-\treturn py_import_cache.arrow_dataset().Dataset.IsInstance(object) ||\n-\t       py_import_cache.arrow_dataset().Scanner.IsInstance(object);\n+\treturn (py::isinstance(object, import_cache_py.arrow_dataset().Dataset()) ||\n+\t        py::isinstance(object, import_cache_py.arrow_dataset().Scanner()));\n }\n \n unique_lock<std::mutex> DuckDBPyConnection::AcquireConnectionLock() {\ndiff --git a/tools/pythonpkg/src/pyconnection/type_creation.cpp b/tools/pythonpkg/src/pyconnection/type_creation.cpp\nindex fc8026ef10b3..75a56c705f20 100644\n--- a/tools/pythonpkg/src/pyconnection/type_creation.cpp\n+++ b/tools/pythonpkg/src/pyconnection/type_creation.cpp\n@@ -1,4 +1,4 @@\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n \n namespace duckdb {\n \ndiff --git a/tools/pythonpkg/src/pyduckdb/connection_wrapper.cpp b/tools/pythonpkg/src/pyduckdb/connection_wrapper.cpp\nindex 0952402cad7a..e0bc051de833 100644\n--- a/tools/pythonpkg/src/pyduckdb/connection_wrapper.cpp\n+++ b/tools/pythonpkg/src/pyduckdb/connection_wrapper.cpp\n@@ -70,21 +70,23 @@ shared_ptr<DuckDBPyConnection> PyConnectionWrapper::ExecuteMany(const string &qu\n \treturn conn->ExecuteMany(query, params);\n }\n \n-unique_ptr<DuckDBPyRelation> PyConnectionWrapper::DistinctDF(const DataFrame &df, shared_ptr<DuckDBPyConnection> conn) {\n+unique_ptr<DuckDBPyRelation> PyConnectionWrapper::DistinctDF(const PandasDataFrame &df,\n+                                                             shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(df)->Distinct();\n }\n \n-void PyConnectionWrapper::WriteCsvDF(const DataFrame &df, const string &file, shared_ptr<DuckDBPyConnection> conn) {\n+void PyConnectionWrapper::WriteCsvDF(const PandasDataFrame &df, const string &file,\n+                                     shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(df)->ToCSV(file);\n }\n \n-unique_ptr<DuckDBPyRelation> PyConnectionWrapper::QueryDF(const DataFrame &df, const string &view_name,\n+unique_ptr<DuckDBPyRelation> PyConnectionWrapper::QueryDF(const PandasDataFrame &df, const string &view_name,\n                                                           const string &sql_query,\n                                                           shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(df)->Query(view_name, sql_query);\n }\n \n-unique_ptr<DuckDBPyRelation> PyConnectionWrapper::AggregateDF(const DataFrame &df, const string &expr,\n+unique_ptr<DuckDBPyRelation> PyConnectionWrapper::AggregateDF(const PandasDataFrame &df, const string &expr,\n                                                               const string &groups,\n                                                               shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(df)->Aggregate(expr, groups);\n@@ -95,7 +97,7 @@ shared_ptr<DuckDBPyConnection> PyConnectionWrapper::Execute(const string &query,\n \treturn conn->Execute(query, params, many);\n }\n \n-shared_ptr<DuckDBPyConnection> PyConnectionWrapper::Append(const string &name, DataFrame value,\n+shared_ptr<DuckDBPyConnection> PyConnectionWrapper::Append(const string &name, PandasDataFrame value,\n                                                            shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->Append(name, value);\n }\n@@ -127,7 +129,8 @@ unique_ptr<DuckDBPyRelation> PyConnectionWrapper::TableFunction(const string &fn\n \treturn conn->TableFunction(fname, params);\n }\n \n-unique_ptr<DuckDBPyRelation> PyConnectionWrapper::FromDF(const DataFrame &value, shared_ptr<DuckDBPyConnection> conn) {\n+unique_ptr<DuckDBPyRelation> PyConnectionWrapper::FromDF(const PandasDataFrame &value,\n+                                                         shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(value);\n }\n \n@@ -241,12 +244,12 @@ py::dict PyConnectionWrapper::FetchNumpy(shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FetchNumpy();\n }\n \n-DataFrame PyConnectionWrapper::FetchDF(bool date_as_object, shared_ptr<DuckDBPyConnection> conn) {\n+PandasDataFrame PyConnectionWrapper::FetchDF(bool date_as_object, shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FetchDF(date_as_object);\n }\n \n-DataFrame PyConnectionWrapper::FetchDFChunk(const idx_t vectors_per_chunk, bool date_as_object,\n-                                            shared_ptr<DuckDBPyConnection> conn) {\n+PandasDataFrame PyConnectionWrapper::FetchDFChunk(const idx_t vectors_per_chunk, bool date_as_object,\n+                                                  shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FetchDFChunk(vectors_per_chunk, date_as_object);\n }\n \n@@ -298,27 +301,27 @@ unique_ptr<DuckDBPyRelation> PyConnectionWrapper::RunQuery(const string &query,\n \treturn conn->RunQuery(query, alias);\n }\n \n-unique_ptr<DuckDBPyRelation> PyConnectionWrapper::ProjectDf(const DataFrame &df, const string &expr,\n+unique_ptr<DuckDBPyRelation> PyConnectionWrapper::ProjectDf(const PandasDataFrame &df, const string &expr,\n                                                             shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(df)->Project(expr);\n }\n \n-unique_ptr<DuckDBPyRelation> PyConnectionWrapper::AliasDF(const DataFrame &df, const string &expr,\n+unique_ptr<DuckDBPyRelation> PyConnectionWrapper::AliasDF(const PandasDataFrame &df, const string &expr,\n                                                           shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(df)->SetAlias(expr);\n }\n \n-unique_ptr<DuckDBPyRelation> PyConnectionWrapper::FilterDf(const DataFrame &df, const string &expr,\n+unique_ptr<DuckDBPyRelation> PyConnectionWrapper::FilterDf(const PandasDataFrame &df, const string &expr,\n                                                            shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(df)->Filter(expr);\n }\n \n-unique_ptr<DuckDBPyRelation> PyConnectionWrapper::LimitDF(const DataFrame &df, int64_t n,\n+unique_ptr<DuckDBPyRelation> PyConnectionWrapper::LimitDF(const PandasDataFrame &df, int64_t n,\n                                                           shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(df)->Limit(n);\n }\n \n-unique_ptr<DuckDBPyRelation> PyConnectionWrapper::OrderDf(const DataFrame &df, const string &expr,\n+unique_ptr<DuckDBPyRelation> PyConnectionWrapper::OrderDf(const PandasDataFrame &df, const string &expr,\n                                                           shared_ptr<DuckDBPyConnection> conn) {\n \treturn conn->FromDF(df)->Order(expr);\n }\ndiff --git a/tools/pythonpkg/src/pyfilesystem.cpp b/tools/pythonpkg/src/pyfilesystem.cpp\nindex ad6c5659b1af..f5d9a9d4caca 100644\n--- a/tools/pythonpkg/src/pyfilesystem.cpp\n+++ b/tools/pythonpkg/src/pyfilesystem.cpp\n@@ -1,12 +1,12 @@\n #include \"duckdb_python/pyfilesystem.hpp\"\n \n #include \"duckdb/common/string_util.hpp\"\n-#include \"duckdb_python/pybind_wrapper.hpp\"\n-#include \"duckdb_python/python_object_container.hpp\"\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+#include \"duckdb_python/pybind11/gil_wrapper.hpp\"\n \n namespace duckdb {\n \n-PythonFileHandle::PythonFileHandle(FileSystem &file_system, const string &path, const py::object handle)\n+PythonFileHandle::PythonFileHandle(FileSystem &file_system, const string &path, const py::object &handle)\n     : FileHandle(file_system, path), handle(handle) {\n }\n PythonFileHandle::~PythonFileHandle() {\n@@ -104,7 +104,7 @@ bool PythonFilesystem::Exists(const string &filename, const char *func_name) con\n vector<string> PythonFilesystem::Glob(const string &path, FileOpener *opener) {\n \tPythonGILWrapper gil;\n \n-\tif (!path.size()) {\n+\tif (path.empty()) {\n \t\treturn {path};\n \t}\n \tauto returner = py::list(filesystem.attr(\"glob\")(path));\ndiff --git a/tools/pythonpkg/src/pyrelation.cpp b/tools/pythonpkg/src/pyrelation.cpp\nindex 555ec6d80c88..d2523946e673 100644\n--- a/tools/pythonpkg/src/pyrelation.cpp\n+++ b/tools/pythonpkg/src/pyrelation.cpp\n@@ -1,11 +1,10 @@\n #include \"duckdb_python/pyrelation.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pytype.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n #include \"duckdb/parser/qualified_name.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n-#include \"duckdb_python/vector_conversion.hpp\"\n-#include \"duckdb_python/pandas_type.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n #include \"duckdb/main/relation/query_relation.hpp\"\n #include \"duckdb/parser/parser.hpp\"\n #include \"duckdb/main/relation/view_relation.hpp\"\n@@ -416,7 +415,7 @@ void DuckDBPyRelation::ExecuteOrThrow(bool stream_result) {\n \tresult = make_uniq<DuckDBPyResult>(std::move(query_result));\n }\n \n-DataFrame DuckDBPyRelation::FetchDF(bool date_as_object) {\n+PandasDataFrame DuckDBPyRelation::FetchDF(bool date_as_object) {\n \tif (!result) {\n \t\tif (!rel) {\n \t\t\treturn py::none();\n@@ -532,7 +531,7 @@ py::dict DuckDBPyRelation::FetchNumpyInternal(bool stream, idx_t vectors_per_chu\n }\n \n //! Should this also keep track of when the result is empty and set result->result_closed accordingly?\n-DataFrame DuckDBPyRelation::FetchDFChunk(idx_t vectors_per_chunk, bool date_as_object) {\n+PandasDataFrame DuckDBPyRelation::FetchDFChunk(idx_t vectors_per_chunk, bool date_as_object) {\n \tif (!result) {\n \t\tif (!rel) {\n \t\t\treturn py::none();\ndiff --git a/tools/pythonpkg/src/pyrelation/initialize.cpp b/tools/pythonpkg/src/pyrelation/initialize.cpp\nindex c0b50bafa180..cdf7d35687d5 100644\n--- a/tools/pythonpkg/src/pyrelation/initialize.cpp\n+++ b/tools/pythonpkg/src/pyrelation/initialize.cpp\n@@ -1,10 +1,9 @@\n #include \"duckdb_python/pyrelation.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n #include \"duckdb/parser/qualified_name.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n-#include \"duckdb_python/vector_conversion.hpp\"\n-#include \"duckdb_python/pandas_type.hpp\"\n+#include \"duckdb_python/numpy/numpy_type.hpp\"\n #include \"duckdb/main/relation/query_relation.hpp\"\n #include \"duckdb/parser/parser.hpp\"\n #include \"duckdb/main/relation/view_relation.hpp\"\ndiff --git a/tools/pythonpkg/src/pyresult.cpp b/tools/pythonpkg/src/pyresult.cpp\nindex a52fc3ba6824..2a3bb65291b7 100644\n--- a/tools/pythonpkg/src/pyresult.cpp\n+++ b/tools/pythonpkg/src/pyresult.cpp\n@@ -1,5 +1,5 @@\n #include \"duckdb_python/pyrelation.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb_python/pyresult.hpp\"\n #include \"duckdb_python/python_objects.hpp\"\n \n@@ -12,7 +12,7 @@\n #include \"duckdb/common/types/time.hpp\"\n #include \"duckdb/common/types/timestamp.hpp\"\n #include \"duckdb/common/types/uuid.hpp\"\n-#include \"duckdb_python/array_wrapper.hpp\"\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n #include \"duckdb/common/exception.hpp\"\n \n namespace duckdb {\n@@ -224,7 +224,7 @@ py::dict DuckDBPyResult::FetchNumpyInternal(bool stream, idx_t vectors_per_chunk\n }\n \n // TODO: unify these with an enum/flag to indicate which conversions to do\n-void DuckDBPyResult::ChangeToTZType(DataFrame &df) {\n+void DuckDBPyResult::ChangeToTZType(PandasDataFrame &df) {\n \tfor (idx_t i = 0; i < result->ColumnCount(); i++) {\n \t\tif (result->types[i] == LogicalType::TIMESTAMP_TZ) {\n \t\t\t// first localize to UTC then convert to timezone_config\n@@ -235,7 +235,7 @@ void DuckDBPyResult::ChangeToTZType(DataFrame &df) {\n }\n \n // TODO: unify these with an enum/flag to indicate which conversions to perform\n-void DuckDBPyResult::ChangeDateToDatetime(DataFrame &df) {\n+void DuckDBPyResult::ChangeDateToDatetime(PandasDataFrame &df) {\n \tfor (idx_t i = 0; i < result->ColumnCount(); i++) {\n \t\tif (result->types[i] == LogicalType::DATE) {\n \t\t\tdf[result->names[i].c_str()] = df[result->names[i].c_str()].attr(\"dt\").attr(\"date\");\n@@ -243,8 +243,8 @@ void DuckDBPyResult::ChangeDateToDatetime(DataFrame &df) {\n \t}\n }\n \n-DataFrame DuckDBPyResult::FrameFromNumpy(bool date_as_object, const py::handle &o) {\n-\tauto df = py::cast<DataFrame>(py::module::import(\"pandas\").attr(\"DataFrame\").attr(\"from_dict\")(o));\n+PandasDataFrame DuckDBPyResult::FrameFromNumpy(bool date_as_object, const py::handle &o) {\n+\tauto df = py::cast<PandasDataFrame>(py::module::import(\"pandas\").attr(\"DataFrame\").attr(\"from_dict\")(o));\n \t// Unfortunately we have to do a type change here for timezones since these types are not supported by numpy\n \tChangeToTZType(df);\n \tif (date_as_object) {\n@@ -253,12 +253,12 @@ DataFrame DuckDBPyResult::FrameFromNumpy(bool date_as_object, const py::handle &\n \treturn df;\n }\n \n-DataFrame DuckDBPyResult::FetchDF(bool date_as_object) {\n+PandasDataFrame DuckDBPyResult::FetchDF(bool date_as_object) {\n \ttimezone_config = QueryResult::GetConfigTimezone(*result);\n \treturn FrameFromNumpy(date_as_object, FetchNumpyInternal());\n }\n \n-DataFrame DuckDBPyResult::FetchDFChunk(idx_t num_of_vectors, bool date_as_object) {\n+PandasDataFrame DuckDBPyResult::FetchDFChunk(idx_t num_of_vectors, bool date_as_object) {\n \tif (timezone_config.empty()) {\n \t\ttimezone_config = QueryResult::GetConfigTimezone(*result);\n \t}\ndiff --git a/tools/pythonpkg/src/python_import_cache.cpp b/tools/pythonpkg/src/python_import_cache.cpp\nindex f2ef72040ed5..e245705bae3f 100644\n--- a/tools/pythonpkg/src/python_import_cache.cpp\n+++ b/tools/pythonpkg/src/python_import_cache.cpp\n@@ -20,15 +20,6 @@ bool PythonImportCacheItem::IsLoaded() const {\n \treturn type.ptr() != nullptr;\n }\n \n-bool PythonImportCacheItem::IsInstance(py::handle object) const {\n-\tauto type = (*this)();\n-\tif (!IsLoaded()) {\n-\t\t// Type was not imported\n-\t\treturn false;\n-\t}\n-\treturn py::isinstance(object, type);\n-}\n-\n PyObject *PythonImportCacheItem::AddCache(PythonImportCache &cache, py::object object) {\n \treturn cache.AddCache(std::move(object));\n }\n@@ -40,7 +31,6 @@ void PythonImportCacheItem::LoadModule(const string &name, PythonImportCache &ca\n \t\tload_succeeded = true;\n \t} catch (py::error_already_set &e) {\n \t\tif (IsRequired()) {\n-\t\t\tPyErr_PrintEx(1);\n \t\t\tthrow InvalidInputException(\n \t\t\t    \"Required module '%s' failed to import, due to the following Python exception:\\n%s\", name, e.what());\n \t\t}\n@@ -53,6 +43,9 @@ void PythonImportCacheItem::LoadAttribute(const string &name, PythonImportCache\n \tauto source_object = source();\n \tif (py::hasattr(source_object, name.c_str())) {\n \t\tobject = AddCache(cache, std::move(source_object.attr(name.c_str())));\n+\t} else {\n+\t\tobject = nullptr;\n+\t\treturn;\n \t}\n \tLoadSubtypes(cache);\n }\ndiff --git a/tools/pythonpkg/src/typing/pytype.cpp b/tools/pythonpkg/src/typing/pytype.cpp\nindex 6284444b47a9..fbab04d93059 100644\n--- a/tools/pythonpkg/src/typing/pytype.cpp\n+++ b/tools/pythonpkg/src/typing/pytype.cpp\n@@ -2,7 +2,7 @@\n #include \"duckdb/common/types.hpp\"\n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n #include \"duckdb/main/connection.hpp\"\n \n namespace duckdb {\n@@ -13,7 +13,7 @@ bool PyGenericAlias::check_(const py::handle &object) {\n \t\treturn false;\n \t}\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\treturn import_cache.types().GenericAlias.IsInstance(object);\n+\treturn py::isinstance(object, import_cache.types().GenericAlias());\n }\n \n // NOLINTNEXTLINE(readability-identifier-naming)\n@@ -26,10 +26,10 @@ bool PyUnionType::check_(const py::handle &object) {\n \t}\n \n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\tif (types_loaded && import_cache.types().UnionType.IsInstance(object)) {\n+\tif (types_loaded && py::isinstance(object, import_cache.types().UnionType())) {\n \t\treturn true;\n \t}\n-\tif (typing_loaded && import_cache.typing()._UnionGenericAlias.IsInstance(object)) {\n+\tif (typing_loaded && py::isinstance(object, import_cache.typing()._UnionGenericAlias())) {\n \t\treturn true;\n \t}\n \treturn false;\ndiff --git a/tools/pythonpkg/src/vector_conversion.cpp b/tools/pythonpkg/src/vector_conversion.cpp\ndeleted file mode 100644\nindex e144b22c4627..000000000000\n--- a/tools/pythonpkg/src/vector_conversion.cpp\n+++ /dev/null\n@@ -1,569 +0,0 @@\n-#include \"duckdb_python/pyrelation.hpp\"\n-#include \"duckdb_python/pyconnection.hpp\"\n-#include \"duckdb_python/pyresult.hpp\"\n-#include \"duckdb_python/vector_conversion.hpp\"\n-#include \"duckdb_python/python_conversion.hpp\"\n-#include \"duckdb/common/string_util.hpp\"\n-#include \"duckdb/common/types/timestamp.hpp\"\n-#include \"utf8proc_wrapper.hpp\"\n-#include \"duckdb/common/case_insensitive_map.hpp\"\n-#include \"duckdb_python/pandas_type.hpp\"\n-#include \"duckdb_python/pandas_analyzer.hpp\"\n-#include \"duckdb_python/pandas_type.hpp\"\n-#include \"duckdb/function/scalar/nested_functions.hpp\"\n-\n-namespace duckdb {\n-\n-template <class T>\n-void ScanPandasColumn(py::array &numpy_col, idx_t stride, idx_t offset, Vector &out, idx_t count) {\n-\tauto src_ptr = (T *)numpy_col.data();\n-\tif (stride == sizeof(T)) {\n-\t\tFlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));\n-\t} else {\n-\t\tauto tgt_ptr = (T *)FlatVector::GetData(out);\n-\t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\ttgt_ptr[i] = src_ptr[stride / sizeof(T) * (i + offset)];\n-\t\t}\n-\t}\n-}\n-\n-template <class T, class V>\n-void ScanPandasCategoryTemplated(py::array &column, idx_t offset, Vector &out, idx_t count) {\n-\tauto src_ptr = (T *)column.data();\n-\tauto tgt_ptr = (V *)FlatVector::GetData(out);\n-\tauto &tgt_mask = FlatVector::Validity(out);\n-\tfor (idx_t i = 0; i < count; i++) {\n-\t\tif (src_ptr[i + offset] == -1) {\n-\t\t\t// Null value\n-\t\t\ttgt_mask.SetInvalid(i);\n-\t\t} else {\n-\t\t\ttgt_ptr[i] = src_ptr[i + offset];\n-\t\t}\n-\t}\n-}\n-\n-template <class T>\n-void ScanPandasCategory(py::array &column, idx_t count, idx_t offset, Vector &out, string &src_type) {\n-\tif (src_type == \"int8\") {\n-\t\tScanPandasCategoryTemplated<int8_t, T>(column, offset, out, count);\n-\t} else if (src_type == \"int16\") {\n-\t\tScanPandasCategoryTemplated<int16_t, T>(column, offset, out, count);\n-\t} else if (src_type == \"int32\") {\n-\t\tScanPandasCategoryTemplated<int32_t, T>(column, offset, out, count);\n-\t} else if (src_type == \"int64\") {\n-\t\tScanPandasCategoryTemplated<int64_t, T>(column, offset, out, count);\n-\t} else {\n-\t\tthrow NotImplementedException(\"The Pandas type \" + src_type + \" for categorical types is not implemented yet\");\n-\t}\n-}\n-\n-template <class T>\n-void ScanPandasMasked(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {\n-\tScanPandasColumn<T>(bind_data.numpy_col, bind_data.numpy_stride, offset, out, count);\n-\tauto &result_mask = FlatVector::Validity(out);\n-\tif (bind_data.mask) {\n-\t\tauto mask = (bool *)bind_data.mask->numpy_array.data();\n-\t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tauto is_null = mask[offset + i];\n-\t\t\tif (is_null) {\n-\t\t\t\tresult_mask.SetInvalid(i);\n-\t\t\t}\n-\t\t}\n-\t}\n-}\n-\n-template <class T>\n-bool ValueIsNull(T value) {\n-\tthrow InvalidInputException(\"Unsupported type for ValueIsNull\");\n-}\n-\n-template <>\n-bool ValueIsNull(float value) {\n-\treturn !Value::FloatIsFinite(value);\n-}\n-\n-template <>\n-bool ValueIsNull(double value) {\n-\treturn !Value::DoubleIsFinite(value);\n-}\n-\n-template <class T>\n-void ScanPandasFpColumn(T *src_ptr, idx_t stride, idx_t count, idx_t offset, Vector &out) {\n-\tauto &mask = FlatVector::Validity(out);\n-\tif (stride == sizeof(T)) {\n-\t\tFlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));\n-\t\t// Turn NaN values into NULL\n-\t\tauto tgt_ptr = FlatVector::GetData<T>(out);\n-\t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tif (Value::IsNan<T>(tgt_ptr[i])) {\n-\t\t\t\tmask.SetInvalid(i);\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\tauto tgt_ptr = FlatVector::GetData<T>(out);\n-\t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\ttgt_ptr[i] = src_ptr[stride / sizeof(T) * (i + offset)];\n-\t\t\tif (Value::IsNan<T>(tgt_ptr[i])) {\n-\t\t\t\tmask.SetInvalid(i);\n-\t\t\t}\n-\t\t}\n-\t}\n-}\n-\n-template <class T>\n-static string_t DecodePythonUnicode(T *codepoints, idx_t codepoint_count, Vector &out) {\n-\t// first figure out how many bytes to allocate\n-\tidx_t utf8_length = 0;\n-\tfor (idx_t i = 0; i < codepoint_count; i++) {\n-\t\tint len = Utf8Proc::CodepointLength(int(codepoints[i]));\n-\t\tD_ASSERT(len >= 1);\n-\t\tutf8_length += len;\n-\t}\n-\tint sz;\n-\tauto result = StringVector::EmptyString(out, utf8_length);\n-\tauto target = result.GetDataWriteable();\n-\tfor (idx_t i = 0; i < codepoint_count; i++) {\n-\t\tUtf8Proc::CodepointToUtf8(int(codepoints[i]), sz, target);\n-\t\tD_ASSERT(sz >= 1);\n-\t\ttarget += sz;\n-\t}\n-\tresult.Finalize();\n-\treturn result;\n-}\n-\n-template <typename T>\n-bool TryCast(const py::object stuf, T &value) {\n-\ttry {\n-\t\tvalue = stuf.cast<T>();\n-\t\treturn true;\n-\t} catch (py::cast_error &e) {\n-\t\treturn false;\n-\t}\n-}\n-\n-template <typename T>\n-T Cast(const py::object obj) {\n-\treturn obj.cast<T>();\n-}\n-\n-static void SetInvalidRecursive(Vector &out, idx_t index) {\n-\tauto &validity = FlatVector::Validity(out);\n-\tvalidity.SetInvalid(index);\n-\tif (out.GetType().InternalType() == PhysicalType::STRUCT) {\n-\t\tauto &children = StructVector::GetEntries(out);\n-\t\tfor (idx_t i = 0; i < children.size(); i++) {\n-\t\t\tSetInvalidRecursive(*children[i], index);\n-\t\t}\n-\t}\n-}\n-\n-//! 'count' is the amount of rows in the 'out' vector\n-//! 'offset' is the current row number within this vector\n-void ScanPandasObject(PandasColumnBindData &bind_data, PyObject *object, idx_t offset, Vector &out) {\n-\n-\t// handle None\n-\tif (object == Py_None) {\n-\t\tSetInvalidRecursive(out, offset);\n-\t\treturn;\n-\t}\n-\n-\tauto val = TransformPythonValue(object, out.GetType());\n-\t// Check if the Value type is accepted for the LogicalType of Vector\n-\tout.SetValue(offset, val);\n-}\n-\n-static void VerifyMapConstraints(Vector &vec, idx_t count) {\n-\tauto invalid_reason = CheckMapValidity(vec, count);\n-\tswitch (invalid_reason) {\n-\tcase MapInvalidReason::VALID:\n-\t\treturn;\n-\tcase MapInvalidReason::DUPLICATE_KEY:\n-\t\tthrow InvalidInputException(\"Dict->Map conversion failed because 'key' list contains duplicates\");\n-\tcase MapInvalidReason::NULL_KEY_LIST:\n-\t\tthrow InvalidInputException(\"Dict->Map conversion failed because 'key' list is None\");\n-\tcase MapInvalidReason::NULL_KEY:\n-\t\tthrow InvalidInputException(\"Dict->Map conversion failed because 'key' list contains None\");\n-\tdefault:\n-\t\tthrow InvalidInputException(\"Option not implemented for MapInvalidReason\");\n-\t}\n-}\n-\n-void VerifyTypeConstraints(Vector &vec, idx_t count) {\n-\tswitch (vec.GetType().id()) {\n-\tcase LogicalTypeId::MAP: {\n-\t\tVerifyMapConstraints(vec, count);\n-\t\tbreak;\n-\t}\n-\tdefault:\n-\t\treturn;\n-\t}\n-}\n-\n-void ScanPandasObjectColumn(PandasColumnBindData &bind_data, PyObject **col, idx_t count, idx_t offset, Vector &out) {\n-\t// numpy_col is a sequential list of objects, that make up one \"column\" (Vector)\n-\tout.SetVectorType(VectorType::FLAT_VECTOR);\n-\t{\n-\t\tPythonGILWrapper gil; // We're creating python objects here, so we need the GIL\n-\t\tfor (idx_t i = 0; i < count; i++) {\n-\t\t\tidx_t source_idx = offset + i;\n-\t\t\tScanPandasObject(bind_data, col[source_idx], i, out);\n-\t\t}\n-\t}\n-\tVerifyTypeConstraints(out, count);\n-}\n-\n-//! 'offset' is the offset within the column\n-//! 'count' is the amount of values we will convert in this batch\n-void VectorConversion::NumpyToDuckDB(PandasColumnBindData &bind_data, py::array &numpy_col, idx_t count, idx_t offset,\n-                                     Vector &out) {\n-\tswitch (bind_data.pandas_type) {\n-\tcase PandasType::BOOL:\n-\t\tScanPandasMasked<bool>(bind_data, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::UINT_8:\n-\t\tScanPandasMasked<uint8_t>(bind_data, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::UINT_16:\n-\t\tScanPandasMasked<uint16_t>(bind_data, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::UINT_32:\n-\t\tScanPandasMasked<uint32_t>(bind_data, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::UINT_64:\n-\t\tScanPandasMasked<uint64_t>(bind_data, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::INT_8:\n-\t\tScanPandasMasked<int8_t>(bind_data, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::INT_16:\n-\t\tScanPandasMasked<int16_t>(bind_data, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::INT_32:\n-\t\tScanPandasMasked<int32_t>(bind_data, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::INT_64:\n-\t\tScanPandasMasked<int64_t>(bind_data, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::FLOAT_32:\n-\t\tScanPandasFpColumn<float>((float *)numpy_col.data(), bind_data.numpy_stride, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::FLOAT_64:\n-\t\tScanPandasFpColumn<double>((double *)numpy_col.data(), bind_data.numpy_stride, count, offset, out);\n-\t\tbreak;\n-\tcase PandasType::DATETIME:\n-\tcase PandasType::DATETIME_TZ: {\n-\t\tauto src_ptr = (int64_t *)numpy_col.data();\n-\t\tauto tgt_ptr = FlatVector::GetData<timestamp_t>(out);\n-\t\tauto &mask = FlatVector::Validity(out);\n-\n-\t\tfor (idx_t row = 0; row < count; row++) {\n-\t\t\tauto source_idx = offset + row;\n-\t\t\tif (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {\n-\t\t\t\t// pandas Not a Time (NaT)\n-\t\t\t\tmask.SetInvalid(row);\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\ttgt_ptr[row] = Timestamp::FromEpochNanoSeconds(src_ptr[source_idx]);\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase PandasType::TIMEDELTA: {\n-\t\tauto src_ptr = (int64_t *)numpy_col.data();\n-\t\tauto tgt_ptr = FlatVector::GetData<interval_t>(out);\n-\t\tauto &mask = FlatVector::Validity(out);\n-\n-\t\tfor (idx_t row = 0; row < count; row++) {\n-\t\t\tauto source_idx = offset + row;\n-\t\t\tif (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {\n-\t\t\t\t// pandas Not a Time (NaT)\n-\t\t\t\tmask.SetInvalid(row);\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tint64_t micro = src_ptr[source_idx] / 1000;\n-\t\t\tint64_t days = micro / Interval::MICROS_PER_DAY;\n-\t\t\tmicro = micro % Interval::MICROS_PER_DAY;\n-\t\t\tint64_t months = days / Interval::DAYS_PER_MONTH;\n-\t\t\tdays = days % Interval::DAYS_PER_MONTH;\n-\t\t\tinterval_t interval;\n-\t\t\tinterval.months = months;\n-\t\t\tinterval.days = days;\n-\t\t\tinterval.micros = micro;\n-\t\t\ttgt_ptr[row] = interval;\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase PandasType::OBJECT: {\n-\t\t//! We have determined the underlying logical type of this object column\n-\t\t// Get the source pointer of the numpy array\n-\t\tauto src_ptr = (PyObject **)numpy_col.data();\n-\t\tif (out.GetType().id() != LogicalTypeId::VARCHAR) {\n-\t\t\treturn ScanPandasObjectColumn(bind_data, src_ptr, count, offset, out);\n-\t\t}\n-\n-\t\t// Get the data pointer and the validity mask of the result vector\n-\t\tauto tgt_ptr = FlatVector::GetData<string_t>(out);\n-\t\tauto &out_mask = FlatVector::Validity(out);\n-\t\tunique_ptr<PythonGILWrapper> gil;\n-\t\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n-\n-\t\t// Loop over every row of the arrays contents\n-\t\tauto stride = bind_data.numpy_stride;\n-\t\tfor (idx_t row = 0; row < count; row++) {\n-\t\t\tauto source_idx = stride / sizeof(PyObject *) * (row + offset);\n-\n-\t\t\t// Get the pointer to the object\n-\t\t\tPyObject *val = src_ptr[source_idx];\n-\t\t\tif (bind_data.pandas_type == PandasType::OBJECT && !py::isinstance<py::str>(val)) {\n-\t\t\t\tif (val == Py_None) {\n-\t\t\t\t\tout_mask.SetInvalid(row);\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n-\t\t\t\tif (import_cache.pandas().libs.NAType.IsLoaded()) {\n-\t\t\t\t\t// If pandas is imported, check if the type is NAType\n-\t\t\t\t\tauto val_type = Py_TYPE(val);\n-\t\t\t\t\tauto na_type = (PyTypeObject *)import_cache.pandas().libs.NAType().ptr();\n-\t\t\t\t\tif (val_type == na_type) {\n-\t\t\t\t\t\tout_mask.SetInvalid(row);\n-\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif (py::isinstance<py::float_>(val) && std::isnan(PyFloat_AsDouble(val))) {\n-\t\t\t\t\tout_mask.SetInvalid(row);\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n-\t\t\t\tif (!py::isinstance<py::str>(val)) {\n-\t\t\t\t\tif (!gil) {\n-\t\t\t\t\t\tgil = bind_data.object_str_val.GetLock();\n-\t\t\t\t\t}\n-\t\t\t\t\tbind_data.object_str_val.AssignInternal<PyObject>(\n-\t\t\t\t\t    [](py::str &obj, PyObject &new_val) {\n-\t\t\t\t\t\t    py::handle object_handle = &new_val;\n-\t\t\t\t\t\t    obj = py::str(object_handle);\n-\t\t\t\t\t    },\n-\t\t\t\t\t    *val, *gil);\n-\t\t\t\t\tval = (PyObject *)bind_data.object_str_val.GetPointerTop()->ptr();\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\t// Python 3 string representation:\n-\t\t\t// https://github.com/python/cpython/blob/3a8fdb28794b2f19f6c8464378fb8b46bce1f5f4/Include/cpython/unicodeobject.h#L79\n-\t\t\tif (!py::isinstance<py::str>(val)) {\n-\t\t\t\tout_mask.SetInvalid(row);\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tif (PyUnicode_IS_COMPACT_ASCII(val)) {\n-\t\t\t\t// ascii string: we can zero copy\n-\t\t\t\ttgt_ptr[row] = string_t((const char *)PyUnicode_DATA(val), PyUnicode_GET_LENGTH(val));\n-\t\t\t} else {\n-\t\t\t\t// unicode gunk\n-\t\t\t\tauto ascii_obj = (PyASCIIObject *)val;\n-\t\t\t\tauto unicode_obj = (PyCompactUnicodeObject *)val;\n-\t\t\t\t// compact unicode string: is there utf8 data available?\n-\t\t\t\tif (unicode_obj->utf8) {\n-\t\t\t\t\t// there is! zero copy\n-\t\t\t\t\ttgt_ptr[row] = string_t((const char *)unicode_obj->utf8, unicode_obj->utf8_length);\n-\t\t\t\t} else if (PyUnicode_IS_COMPACT(unicode_obj) && !PyUnicode_IS_ASCII(unicode_obj)) {\n-\t\t\t\t\tauto kind = PyUnicode_KIND(val);\n-\t\t\t\t\tswitch (kind) {\n-\t\t\t\t\tcase PyUnicode_1BYTE_KIND:\n-\t\t\t\t\t\ttgt_ptr[row] =\n-\t\t\t\t\t\t    DecodePythonUnicode<Py_UCS1>(PyUnicode_1BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);\n-\t\t\t\t\t\tbreak;\n-\t\t\t\t\tcase PyUnicode_2BYTE_KIND:\n-\t\t\t\t\t\ttgt_ptr[row] =\n-\t\t\t\t\t\t    DecodePythonUnicode<Py_UCS2>(PyUnicode_2BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);\n-\t\t\t\t\t\tbreak;\n-\t\t\t\t\tcase PyUnicode_4BYTE_KIND:\n-\t\t\t\t\t\ttgt_ptr[row] =\n-\t\t\t\t\t\t    DecodePythonUnicode<Py_UCS4>(PyUnicode_4BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);\n-\t\t\t\t\t\tbreak;\n-\t\t\t\t\tdefault:\n-\t\t\t\t\t\tthrow NotImplementedException(\n-\t\t\t\t\t\t    \"Unsupported typekind constant %d for Python Unicode Compact decode\", kind);\n-\t\t\t\t\t}\n-\t\t\t\t} else if (ascii_obj->state.kind == PyUnicode_WCHAR_KIND) {\n-\t\t\t\t\tthrow InvalidInputException(\"Unsupported: decode not ready legacy string\");\n-\t\t\t\t} else if (!PyUnicode_IS_COMPACT(unicode_obj) && ascii_obj->state.kind != PyUnicode_WCHAR_KIND) {\n-\t\t\t\t\tthrow InvalidInputException(\"Unsupported: decode ready legacy string\");\n-\t\t\t\t} else {\n-\t\t\t\t\tthrow InvalidInputException(\"Unsupported string type: no clue what this string is\");\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase PandasType::CATEGORY: {\n-\t\tswitch (out.GetType().InternalType()) {\n-\t\tcase PhysicalType::UINT8:\n-\t\t\tScanPandasCategory<uint8_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);\n-\t\t\tbreak;\n-\t\tcase PhysicalType::UINT16:\n-\t\t\tScanPandasCategory<uint16_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);\n-\t\t\tbreak;\n-\t\tcase PhysicalType::UINT32:\n-\t\t\tScanPandasCategory<uint32_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);\n-\t\t\tbreak;\n-\t\tdefault:\n-\t\t\tthrow InternalException(\"Invalid Physical Type for ENUMs\");\n-\t\t}\n-\t\tbreak;\n-\t}\n-\n-\tdefault:\n-\t\tthrow NotImplementedException(\"Unsupported pandas type\");\n-\t}\n-}\n-\n-void VectorConversion::BindNumpy(const DBConfig &config, py::handle df, vector<PandasColumnBindData> &bind_columns,\n-                                 vector<LogicalType> &return_types, vector<string> &names) {\n-\tauto df_columns = py::list(df.attr(\"keys\")());\n-\tauto df_types = py::list();\n-\tfor (auto item : py::cast<py::dict>(df)) {\n-\t\tif (string(py::str(item.second.attr(\"dtype\").attr(\"char\"))) == \"U\") {\n-\t\t\tdf_types.attr(\"append\")(py::str(\"string\"));\n-\t\t\tcontinue;\n-\t\t}\n-\t\tdf_types.attr(\"append\")(py::str(item.second.attr(\"dtype\")));\n-\t}\n-\tauto get_fun = df.attr(\"__getitem__\");\n-\tif (py::len(df_columns) == 0 || py::len(df_types) == 0 || py::len(df_columns) != py::len(df_types)) {\n-\t\tthrow InvalidInputException(\"Need a DataFrame with at least one column\");\n-\t}\n-\tfor (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {\n-\t\tLogicalType duckdb_col_type;\n-\t\tPandasColumnBindData bind_data;\n-\n-\t\tnames.emplace_back(py::str(df_columns[col_idx]));\n-\t\tbind_data.pandas_type = ConvertPandasType(df_types[col_idx]);\n-\n-\t\tauto column = get_fun(df_columns[col_idx]);\n-\n-\t\tif (bind_data.pandas_type == PandasType::FLOAT_16) {\n-\t\t\tbind_data.numpy_col = py::array(column.attr(\"astype\")(\"float32\"));\n-\t\t\tbind_data.pandas_type = PandasType::FLOAT_32;\n-\t\t\tduckdb_col_type = PandasToLogicalType(bind_data.pandas_type);\n-\t\t} else if (bind_data.pandas_type == PandasType::OBJECT && string(py::str(df_types[col_idx])) == \"string\") {\n-\t\t\tbind_data.pandas_type = PandasType::CATEGORY;\n-\t\t\tauto enum_name = string(py::str(df_columns[col_idx]));\n-\t\t\t// here we call numpy.unique\n-\t\t\t// this function call will return the unique values of a given array\n-\t\t\t// together with the indices to reconstruct the given array\n-\t\t\tauto uniq = py::cast<py::tuple>(py::module_::import(\"numpy\").attr(\"unique\")(column, false, true));\n-\t\t\tvector<string> enum_entries = py::cast<vector<string>>(uniq.attr(\"__getitem__\")(0));\n-\t\t\tidx_t size = enum_entries.size();\n-\t\t\tVector enum_entries_vec(LogicalType::VARCHAR, size);\n-\t\t\tauto enum_entries_ptr = FlatVector::GetData<string_t>(enum_entries_vec);\n-\t\t\tfor (idx_t i = 0; i < size; i++) {\n-\t\t\t\tenum_entries_ptr[i] = StringVector::AddStringOrBlob(enum_entries_vec, enum_entries[i]);\n-\t\t\t}\n-\t\t\tduckdb_col_type = LogicalType::ENUM(enum_name, enum_entries_vec, size);\n-\t\t\tbind_data.numpy_col = uniq.attr(\"__getitem__\")(1);\n-\t\t\tD_ASSERT(py::hasattr(bind_data.numpy_col, \"dtype\"));\n-\t\t\tbind_data.internal_categorical_type = string(py::str(bind_data.numpy_col.attr(\"dtype\")));\n-\t\t} else {\n-\t\t\tbind_data.numpy_col = column;\n-\t\t\tduckdb_col_type = PandasToLogicalType(bind_data.pandas_type);\n-\t\t}\n-\n-\t\tif (bind_data.pandas_type == PandasType::OBJECT) {\n-\t\t\tPandasAnalyzer analyzer(config);\n-\t\t\tif (analyzer.Analyze(get_fun(df_columns[col_idx]))) {\n-\t\t\t\tduckdb_col_type = analyzer.AnalyzedType();\n-\t\t\t}\n-\t\t}\n-\n-\t\tD_ASSERT(py::hasattr(bind_data.numpy_col, \"strides\"));\n-\t\tbind_data.numpy_stride = bind_data.numpy_col.attr(\"strides\").attr(\"__getitem__\")(0).cast<idx_t>();\n-\t\treturn_types.push_back(duckdb_col_type);\n-\t\tbind_columns.push_back(std::move(bind_data));\n-\t}\n-}\n-\n-void VectorConversion::BindPandas(const DBConfig &config, py::handle df, vector<PandasColumnBindData> &bind_columns,\n-                                  vector<LogicalType> &return_types, vector<string> &names) {\n-\t// This performs a shallow copy that allows us to rename the dataframe\n-\tauto df_columns = py::list(df.attr(\"columns\"));\n-\tauto df_types = py::list(df.attr(\"dtypes\"));\n-\tauto get_fun = df.attr(\"__getitem__\");\n-\t// TODO support masked arrays as well\n-\t// TODO support dicts of numpy arrays as well\n-\tif (py::len(df_columns) == 0 || py::len(df_types) == 0 || py::len(df_columns) != py::len(df_types)) {\n-\t\tthrow InvalidInputException(\"Need a DataFrame with at least one column\");\n-\t}\n-\tpy::array column_attributes = df.attr(\"columns\").attr(\"values\");\n-\n-\t// loop over every column\n-\tfor (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {\n-\t\tLogicalType duckdb_col_type;\n-\t\tPandasColumnBindData bind_data;\n-\n-\t\tnames.emplace_back(py::str(df_columns[col_idx]));\n-\t\tbind_data.pandas_type = ConvertPandasType(df_types[col_idx]);\n-\t\tbool column_has_mask = py::hasattr(get_fun(df_columns[col_idx]).attr(\"array\"), \"_mask\");\n-\n-\t\tif (column_has_mask) {\n-\t\t\t// masked object, fetch the internal data and mask array\n-\t\t\tbind_data.mask = make_uniq<NumPyArrayWrapper>(get_fun(df_columns[col_idx]).attr(\"array\").attr(\"_mask\"));\n-\t\t}\n-\n-\t\tauto column = get_fun(df_columns[col_idx]);\n-\t\tif (bind_data.pandas_type == PandasType::CATEGORY) {\n-\t\t\t// for category types, we create an ENUM type for string or use the converted numpy type for the rest\n-\t\t\tD_ASSERT(py::hasattr(column, \"cat\"));\n-\t\t\tD_ASSERT(py::hasattr(column.attr(\"cat\"), \"categories\"));\n-\t\t\tauto categories = py::array(column.attr(\"cat\").attr(\"categories\"));\n-\t\t\tauto categories_pd_type = ConvertPandasType(categories.attr(\"dtype\"));\n-\t\t\tif (categories_pd_type == PandasType::OBJECT) {\n-\t\t\t\t// Let's hope the object type is a string.\n-\t\t\t\tbind_data.pandas_type = PandasType::CATEGORY;\n-\t\t\t\tauto enum_name = string(py::str(df_columns[col_idx]));\n-\t\t\t\tvector<string> enum_entries = py::cast<vector<string>>(categories);\n-\t\t\t\tidx_t size = enum_entries.size();\n-\t\t\t\tVector enum_entries_vec(LogicalType::VARCHAR, size);\n-\t\t\t\tauto enum_entries_ptr = FlatVector::GetData<string_t>(enum_entries_vec);\n-\t\t\t\tfor (idx_t i = 0; i < size; i++) {\n-\t\t\t\t\tenum_entries_ptr[i] = StringVector::AddStringOrBlob(enum_entries_vec, enum_entries[i]);\n-\t\t\t\t}\n-\t\t\t\tD_ASSERT(py::hasattr(column.attr(\"cat\"), \"codes\"));\n-\t\t\t\tduckdb_col_type = LogicalType::ENUM(enum_name, enum_entries_vec, size);\n-\t\t\t\tbind_data.numpy_col = py::array(column.attr(\"cat\").attr(\"codes\"));\n-\t\t\t\tD_ASSERT(py::hasattr(bind_data.numpy_col, \"dtype\"));\n-\t\t\t\tbind_data.internal_categorical_type = string(py::str(bind_data.numpy_col.attr(\"dtype\")));\n-\t\t\t} else {\n-\t\t\t\tbind_data.numpy_col = py::array(column.attr(\"to_numpy\")());\n-\t\t\t\tauto numpy_type = bind_data.numpy_col.attr(\"dtype\");\n-\t\t\t\t// for category types (non-strings), we use the converted numpy type\n-\t\t\t\tbind_data.pandas_type = ConvertPandasType(numpy_type);\n-\t\t\t\tduckdb_col_type = PandasToLogicalType(bind_data.pandas_type);\n-\t\t\t}\n-\t\t} else if (bind_data.pandas_type == PandasType::FLOAT_16) {\n-\t\t\tauto pandas_array = get_fun(df_columns[col_idx]).attr(\"array\");\n-\t\t\tbind_data.numpy_col = py::array(column.attr(\"to_numpy\")(\"float32\"));\n-\t\t\tbind_data.pandas_type = PandasType::FLOAT_32;\n-\t\t\tduckdb_col_type = PandasToLogicalType(bind_data.pandas_type);\n-\t\t} else {\n-\t\t\tauto pandas_array = get_fun(df_columns[col_idx]).attr(\"array\");\n-\t\t\tif (py::hasattr(pandas_array, \"_data\")) {\n-\t\t\t\t// This means we can access the numpy array directly\n-\t\t\t\tbind_data.numpy_col = get_fun(df_columns[col_idx]).attr(\"array\").attr(\"_data\");\n-\t\t\t} else {\n-\t\t\t\t// Otherwise we have to get it through 'to_numpy()'\n-\t\t\t\tbind_data.numpy_col = py::array(column.attr(\"to_numpy\")());\n-\t\t\t}\n-\t\t\tduckdb_col_type = PandasToLogicalType(bind_data.pandas_type);\n-\t\t}\n-\t\t// Analyze the inner data type of the 'object' column\n-\t\tif (bind_data.pandas_type == PandasType::OBJECT) {\n-\t\t\tPandasAnalyzer analyzer(config);\n-\t\t\tif (analyzer.Analyze(get_fun(df_columns[col_idx]))) {\n-\t\t\t\tduckdb_col_type = analyzer.AnalyzedType();\n-\t\t\t}\n-\t\t}\n-\n-\t\tD_ASSERT(py::hasattr(bind_data.numpy_col, \"strides\"));\n-\t\tbind_data.numpy_stride = bind_data.numpy_col.attr(\"strides\").attr(\"__getitem__\")(0).cast<idx_t>();\n-\t\treturn_types.push_back(duckdb_col_type);\n-\t\tbind_columns.push_back(std::move(bind_data));\n-\t}\n-}\n-\n-} // namespace duckdb\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/conftest.py b/tools/pythonpkg/tests/conftest.py\nindex 42767fb9b7a3..6d1d4acd9bd4 100644\n--- a/tools/pythonpkg/tests/conftest.py\n+++ b/tools/pythonpkg/tests/conftest.py\n@@ -4,6 +4,20 @@\n from os.path import abspath, join, dirname, normpath\n import glob\n import duckdb\n+from packaging.version import Version\n+\n+try:\n+    import pandas\n+    pyarrow_dtype = pandas.core.arrays.arrow.dtype.ArrowDtype\n+except:\n+    pyarrow_dtype = None\n+\n+# Check if pandas has arrow dtypes enabled\n+try:\n+    from pandas.compat import pa_version_under7p0\n+    pyarrow_dtypes_enabled = not pa_version_under7p0\n+except:\n+    pyarrow_dtypes_enabled = False\n \n # https://docs.pytest.org/en/latest/example/simple.html#control-skipping-of-tests-according-to-command-line-option\n # https://stackoverflow.com/a/47700320\n@@ -36,6 +50,78 @@ def duckdb_empty_cursor(request):\n     cursor = connection.cursor()\n     return cursor\n \n+def pandas_supports_arrow_backend():\n+    try:\n+        from pandas.compat import pa_version_under7p0\n+        if pa_version_under7p0 == True:\n+            return False\n+    except:\n+        return False\n+    import pandas as pd\n+    return Version(pd.__version__) >= Version('2.0.0')\n+\n+def numpy_pandas_df(*args, **kwargs):\n+    pandas = pytest.importorskip(\"pandas\")\n+    return pandas.DataFrame(*args, **kwargs)\n+\n+def arrow_pandas_df(*args, **kwargs):\n+    df = numpy_pandas_df(*args, **kwargs);\n+    return df.convert_dtypes(dtype_backend=\"pyarrow\")\n+\n+class NumpyPandas:\n+    def __init__(self):\n+        self.backend = 'numpy_nullable'\n+        self.DataFrame = numpy_pandas_df\n+        self.pandas = pytest.importorskip(\"pandas\")\n+    def __getattr__(self, __name: str):\n+        item = eval(f'self.pandas.{__name}')\n+        return item\n+\n+def convert_arrow_to_numpy_backend(df):\n+    pandas = pytest.importorskip(\"pandas\")\n+    names = df.columns\n+    df_content = {}\n+    for name in names:\n+        df_content[name] = df[name].array.__arrow_array__()\n+    # This should convert the pyarrow chunked arrays into numpy arrays\n+    return pandas.DataFrame(df_content)\n+\n+def convert_to_numpy(df):\n+    if pyarrow_dtypes_enabled and pyarrow_dtype != None and any([True for x in df.dtypes if isinstance(x, pyarrow_dtype)]):\n+        return convert_arrow_to_numpy_backend(df)\n+    return df\n+\n+def convert_and_equal(df1, df2, **kwargs):\n+    df1 = convert_to_numpy(df1)\n+    df2 = convert_to_numpy(df2)\n+    pytest.importorskip(\"pandas\").testing.assert_frame_equal(df1, df2, **kwargs)\n+\n+class ArrowMockTesting:\n+    def __init__(self):\n+        self.testing = pytest.importorskip(\"pandas\").testing\n+        self.assert_frame_equal = convert_and_equal\n+    def __getattr__(self, __name: str):\n+        item = eval(f'self.testing.{__name}')\n+        return item\n+\n+# This converts dataframes constructed with 'DataFrame(...)' to pyarrow backed dataframes\n+# Assert equal does the opposite, turning all pyarrow backed dataframes into numpy backed ones\n+# this is done because we don't produce pyarrow backed dataframes yet\n+class ArrowPandas:\n+    def __init__(self):\n+        self.pandas = pytest.importorskip(\"pandas\")\n+        if Version(self.pandas.__version__) >= Version('2.0.0') and pyarrow_dtypes_enabled:\n+            self.backend = 'pyarrow'\n+            self.DataFrame = arrow_pandas_df\n+        else:\n+            # For backwards compatible reasons, just mock regular pandas\n+            self.backend = 'numpy_nullable'\n+            self.DataFrame = self.pandas.DataFrame\n+        self.testing = ArrowMockTesting()\n+    def __getattr__(self, __name: str):\n+        item = eval(f'self.pandas.{__name}')\n+        return item\n+\n @pytest.fixture(scope=\"function\")\n def require():\n     def _require(extension_name, db_name=''):\ndiff --git a/tools/pythonpkg/tests/coverage/test_pandas_categorical_coverage.py b/tools/pythonpkg/tests/coverage/test_pandas_categorical_coverage.py\nindex 5289f1c8f79b..2bc71051f44b 100644\n--- a/tools/pythonpkg/tests/coverage/test_pandas_categorical_coverage.py\n+++ b/tools/pythonpkg/tests/coverage/test_pandas_categorical_coverage.py\n@@ -1,25 +1,26 @@\n import duckdb\n-import pandas as pd\n import numpy\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n def check_result_list(res):\n     for res_item in res:\n         assert res_item[0] == res_item[1]\n \n-def check_create_table(category):\n+def check_create_table(category, pandas):\n     conn = duckdb.connect()\n \n     conn.execute (\"PRAGMA enable_verification\")\n-    df_in = pd.DataFrame({\n-    'x': pd.Categorical(category, ordered=True),\n-    'y': pd.Categorical(category, ordered=True),\n+    df_in = pandas.DataFrame({\n+    'x': pandas.Categorical(category, ordered=True),\n+    'y': pandas.Categorical(category, ordered=True),\n     'z': category\n     })\n \n     category.append('bla')\n \n-    df_in_diff = pd.DataFrame({\n-    'k': pd.Categorical(category, ordered=True),\n+    df_in_diff = pandas.DataFrame({\n+    'k': pandas.Categorical(category, ordered=True),\n     })\n \n     df_out = duckdb.query_df(df_in, \"data\", \"SELECT * FROM data\")\n@@ -60,16 +61,19 @@ def check_create_table(category):\n     # We should be able to drop the table without any dependencies\n     conn.execute(\"DROP TABLE t1\")\n \n+# TODO: extend tests with ArrowPandas\n class TestCategory(object):\n \n-    def test_category_string_uint16(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_category_string_uint16(self, duckdb_cursor, pandas):\n         category = []\n         for i in range (300):\n             category.append(str(i))\n-        check_create_table(category)\n+        check_create_table(category, pandas)\n \n-    def test_category_string_uint32(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_category_string_uint32(self, duckdb_cursor, pandas):\n         category = []\n         for i in range (70000):\n             category.append(str(i))\n-        check_create_table(category)\n\\ No newline at end of file\n+        check_create_table(category, pandas)\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/tests/extensions/test_httpfs.py b/tools/pythonpkg/tests/extensions/test_httpfs.py\nindex e281d1108739..f78c4d6e1ef0 100644\n--- a/tools/pythonpkg/tests/extensions/test_httpfs.py\n+++ b/tools/pythonpkg/tests/extensions/test_httpfs.py\n@@ -1,8 +1,8 @@\n import duckdb\n import os\n-import pandas as pd\n from pytest import raises, mark\n-\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n # We only run this test if this env var is set\n pytestmark = mark.skipif(\n@@ -10,9 +10,9 @@\n     reason='httpfs extension not available'\n )\n \n-\n-def test_httpfs(require):\n-    connection = require('httpfs')     \n+@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+def test_httpfs(require, pandas):\n+    connection = require('httpfs')\n     try:\n         connection.execute(\"SELECT id, first_name, last_name FROM PARQUET_SCAN('https://raw.githubusercontent.com/cwida/duckdb/master/data/parquet-testing/userdata1.parquet') LIMIT 3;\")\n     except RuntimeError as e:\n@@ -25,14 +25,12 @@ def test_httpfs(require):\n             raise e\n \n     result_df = connection.fetchdf()\n-    exp_result = pd.DataFrame({\n-        'id': pd.Series([1, 2, 3], dtype=\"int32\"),\n+    exp_result = pandas.DataFrame({\n+        'id': pandas.Series([1, 2, 3], dtype=\"int32\"),\n         'first_name': ['Amanda', 'Albert', 'Evelyn'],\n         'last_name': ['Jordan', 'Freeman', 'Morgan']\n     })\n-    assert(result_df.equals(exp_result))\n-\n-\n+    pandas.testing.assert_frame_equal(result_df, exp_result)\n \n def test_http_exception(require):\n     connection = require('httpfs')\n@@ -45,4 +43,3 @@ def test_http_exception(require):\n     assert value.reason == 'Not Found'\n     assert value.body == ''\n     assert 'Content-Length' in value.headers\n-\ndiff --git a/tools/pythonpkg/tests/fast/api/test_3654.py b/tools/pythonpkg/tests/fast/api/test_3654.py\nindex 5ba7a24c3038..e3d817271b4e 100644\n--- a/tools/pythonpkg/tests/fast/api/test_3654.py\n+++ b/tools/pythonpkg/tests/fast/api/test_3654.py\n@@ -1,14 +1,17 @@\n-import pandas as pd\n import duckdb\n+import pytest\n try:\n     import pyarrow as pa\n     can_run = True\n except:\n     can_run = False\n+from conftest import NumpyPandas, ArrowPandas\n \n class Test3654(object):\n-    def test_3654_pandas(self, duckdb_cursor):\n-        df1 = pd.DataFrame({\n+    \n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_3654_pandas(self, duckdb_cursor, pandas):\n+        df1 = pandas.DataFrame({\n             'id': [1, 1, 2],\n         })\n         con = duckdb.connect()\n@@ -17,11 +20,12 @@ def test_3654_pandas(self, duckdb_cursor):\n         print(rel.execute().fetchall())\n         assert rel.execute().fetchall() == [(1,), (1,), (2,)]\n \n-    def test_3654_arrow(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_3654_arrow(self, duckdb_cursor, pandas):\n         if not can_run:\n             return\n \n-        df1 = pd.DataFrame({\n+        df1 = pandas.DataFrame({\n             'id': [1, 1, 2],\n         })\n         table = pa.Table.from_pandas(df1)\ndiff --git a/tools/pythonpkg/tests/fast/api/test_config.py b/tools/pythonpkg/tests/fast/api/test_config.py\nindex a80a57a98f42..a37bb796ec90 100644\n--- a/tools/pythonpkg/tests/fast/api/test_config.py\n+++ b/tools/pythonpkg/tests/fast/api/test_config.py\n@@ -2,28 +2,33 @@\n \n import duckdb\n import numpy\n-import pandas\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestDBConfig(object):\n-    def test_default_order(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_default_order(self, duckdb_cursor, pandas):\n         df = pandas.DataFrame({'a': [1,2,3]})\n         con = duckdb.connect(':memory:', config={'default_order': 'desc'})\n         result = con.execute('select * from df order by a').fetchall()\n         assert result == [(3,), (2,), (1,)]\n \n-    def test_null_order(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_null_order(self, duckdb_cursor, pandas):\n         df = pandas.DataFrame({'a': [1,2,3,None]})\n         con = duckdb.connect(':memory:', config={'default_null_order': 'nulls_last'})\n         result = con.execute('select * from df order by a').fetchall()\n         assert result == [(1,), (2,), (3,), (None,)]\n \n-    def test_multiple_options(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_multiple_options(self, duckdb_cursor, pandas):\n         df = pandas.DataFrame({'a': [1,2,3,None]})\n         con = duckdb.connect(':memory:', config={'default_null_order': 'nulls_last', 'default_order': 'desc'})\n         result = con.execute('select * from df order by a').fetchall()\n         assert result == [(3,), (2,), (1,), (None,)]\n \n-    def test_external_access(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_external_access(self, duckdb_cursor, pandas):\n         df = pandas.DataFrame({'a': [1,2,3]})\n         # this works (replacement scan)\n         con_regular = duckdb.connect(':memory:', config={})\n@@ -53,5 +58,3 @@ def test_incorrect_parameter(self, duckdb_cursor):\n         except:\n             success = False\n         assert success==False\n-\n-\ndiff --git a/tools/pythonpkg/tests/fast/api/test_dbapi00.py b/tools/pythonpkg/tests/fast/api/test_dbapi00.py\nindex 84d53a1e8e77..291a2e28d2e3 100644\n--- a/tools/pythonpkg/tests/fast/api/test_dbapi00.py\n+++ b/tools/pythonpkg/tests/fast/api/test_dbapi00.py\n@@ -1,10 +1,9 @@\n # simple DB API testcase\n \n import numpy\n-import pandas\n import pytest\n import duckdb\n-\n+from conftest import NumpyPandas, ArrowPandas\n \n def assert_result_equal(result):\n     assert result == [(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (9,), (None,)], \"Incorrect result returned\"\n@@ -86,13 +85,14 @@ def test_numpy_selection(self, duckdb_cursor):\n         arr.mask = [False, False, True]\n         numpy.testing.assert_array_equal(result['t'], arr, \"Incorrect result returned\")\n \n-    def test_pandas_selection(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_pandas_selection(self, duckdb_cursor, pandas):\n         duckdb_cursor.execute('SELECT * FROM integers')\n         result = duckdb_cursor.fetchdf()\n         arr = numpy.ma.masked_array(numpy.arange(11))\n         arr.mask = [False] * 10 + [True]\n         arr = {'i': arr}\n-        arr = pandas.DataFrame.from_dict(arr)\n+        arr = pandas.DataFrame(arr)\n         pandas.testing.assert_frame_equal(result, arr)\n \n         duckdb_cursor.execute('SELECT * FROM timestamps')\ndiff --git a/tools/pythonpkg/tests/fast/api/test_dbapi08.py b/tools/pythonpkg/tests/fast/api/test_dbapi08.py\nindex 4d9dc4eda74c..27fd03ad9dd9 100644\n--- a/tools/pythonpkg/tests/fast/api/test_dbapi08.py\n+++ b/tools/pythonpkg/tests/fast/api/test_dbapi08.py\n@@ -1,17 +1,21 @@\n # test fetchdf with various types\n-import pandas\n import numpy\n+import pytest\n+import duckdb\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestType(object):\n-    def test_fetchdf(self, duckdb_cursor):\n-        duckdb_cursor.execute(\"CREATE TABLE items(item VARCHAR)\")\n-        duckdb_cursor.execute(\"INSERT INTO items VALUES ('jeans'), (''), (NULL)\")\n-        res = duckdb_cursor.execute(\"SELECT item FROM items\").fetchdf()\n-        assert isinstance(res, pandas.DataFrame)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_fetchdf(self, pandas):\n+        con = duckdb.connect()\n+        con.execute(\"CREATE TABLE items(item VARCHAR)\")\n+        con.execute(\"INSERT INTO items VALUES ('jeans'), (''), (NULL)\")\n+        res = con.execute(\"SELECT item FROM items\").fetchdf()\n+        assert isinstance(res, pandas.core.frame.DataFrame)\n \n         arr = numpy.ma.masked_array(['jeans', '', None])\n         arr.mask = [False, False, True]\n         arr = {'item': arr}\n-        df = pandas.DataFrame.from_dict(arr)\n+        df = pandas.DataFrame(arr)\n \n         pandas.testing.assert_frame_equal(res, df)\ndiff --git a/tools/pythonpkg/tests/fast/api/test_duckdb_connection.py b/tools/pythonpkg/tests/fast/api/test_duckdb_connection.py\nindex 921e6332dce4..53330f7a0a02 100644\n--- a/tools/pythonpkg/tests/fast/api/test_duckdb_connection.py\n+++ b/tools/pythonpkg/tests/fast/api/test_duckdb_connection.py\n@@ -1,7 +1,6 @@\n import duckdb\n-from pandas import DataFrame\n-import pandas as pd\n import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n def is_dunder_method(method_name: str) -> bool:\n     if (len(method_name) < 4):\n@@ -11,9 +10,10 @@ def is_dunder_method(method_name: str) -> bool:\n # This file contains tests for DuckDBPyConnection methods,\n # wrapped by the 'duckdb' module, to execute with the 'default_connection'\n class TestDuckDBConnection(object):\n-    def test_append(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_append(self, pandas):\n         duckdb.execute(\"Create table integers (i integer)\")\n-        df_in = pd.DataFrame({'numbers': [1,2,3,4,5],})\n+        df_in = pandas.DataFrame({'numbers': [1,2,3,4,5],})\n         duckdb.append('integers',df_in)\n         assert duckdb.execute('select count(*) from integers').fetchone()[0] == 5\n         # cleanup\n@@ -167,9 +167,6 @@ def test_from_df(self):\n     def test_from_parquet(self):\n         assert None != duckdb.from_parquet\n \n-    def test_from_parquet(self):\n-        assert None != duckdb.from_parquet\n-\n     def test_from_query(self):\n         assert None != duckdb.from_query\n \n@@ -205,12 +202,13 @@ def test_register_relation(self):\n         assert duckdb.table('tbl').fetchall() == [([5, 4, 3],)]\n         duckdb.execute('drop table tbl')\n \n-    def test_relation_out_of_scope(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_relation_out_of_scope(self, pandas):\n         def temporary_scope():\n             # Create a connection, we will return this\n             con = duckdb.connect()\n             # Create a dataframe\n-            df = pd.DataFrame({'a': [1,2,3]})\n+            df = pandas.DataFrame({'a': [1,2,3]})\n             # The dataframe has to be registered as well\n             # making sure it does not go out of scope\n             con.register(\"df\", df)\ndiff --git a/tools/pythonpkg/tests/fast/api/test_to_csv.py b/tools/pythonpkg/tests/fast/api/test_to_csv.py\nindex 1af93d601e7b..025f13092a5a 100644\n--- a/tools/pythonpkg/tests/fast/api/test_to_csv.py\n+++ b/tools/pythonpkg/tests/fast/api/test_to_csv.py\n@@ -1,27 +1,28 @@\n import duckdb\n import tempfile\n import os\n-import pandas as pd\n import tempfile\n import pandas._testing as tm\n import datetime\n import csv\n import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestToCSV(object):\n-    def test_basic_to_csv(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_basic_to_csv(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame({'a': [5,3,23,2], 'b': [45,234,234,2]})\n+        df = pandas.DataFrame({'a': [5,3,23,2], 'b': [45,234,234,2]})\n         rel = duckdb.from_df(df)\n \n         rel.to_csv(temp_file_name)\n \n         csv_rel = duckdb.read_csv(temp_file_name)\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n-\n-    def test_to_csv_sep(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_sep(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame({'a': [5,3,23,2], 'b': [45,234,234,2]})\n+        df = pandas.DataFrame({'a': [5,3,23,2], 'b': [45,234,234,2]})\n         rel = duckdb.from_df(df)\n \n         rel.to_csv(temp_file_name, sep=',')\n@@ -29,9 +30,10 @@ def test_to_csv_sep(self):\n         csv_rel = duckdb.read_csv(temp_file_name, sep=',')\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_na_rep(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_na_rep(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame({'a': [5,None,23,2], 'b': [45,234,234,2]})\n+        df = pandas.DataFrame({'a': [5,None,23,2], 'b': [45,234,234,2]})\n         rel = duckdb.from_df(df)\n \n         rel.to_csv(temp_file_name, na_rep=\"test\")\n@@ -39,9 +41,10 @@ def test_to_csv_na_rep(self):\n         csv_rel = duckdb.read_csv(temp_file_name, na_values=\"test\")\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_header(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_header(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame({'a': [5,None,23,2], 'b': [45,234,234,2]})\n+        df = pandas.DataFrame({'a': [5,None,23,2], 'b': [45,234,234,2]})\n         rel = duckdb.from_df(df)\n \n         rel.to_csv(temp_file_name, header=True)\n@@ -49,9 +52,10 @@ def test_to_csv_header(self):\n         csv_rel = duckdb.read_csv(temp_file_name, header=True)\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_quotechar(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_quotechar(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame({'a': [\"\\'a,b,c\\'\",None,\"hello\",\"bye\"], 'b': [45,234,234,2]})\n+        df = pandas.DataFrame({'a': [\"\\'a,b,c\\'\",None,\"hello\",\"bye\"], 'b': [45,234,234,2]})\n         rel = duckdb.from_df(df)\n \n         rel.to_csv(temp_file_name, quotechar='\\'', sep=',')\n@@ -59,9 +63,10 @@ def test_to_csv_quotechar(self):\n         csv_rel = duckdb.read_csv(temp_file_name, sep=',', quotechar='\\'')\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_escapechar(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_escapechar(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame(\n+        df = pandas.DataFrame(\n             {\n                 \"c_bool\": [True, False],\n                 \"c_float\": [1.0, 3.2],\n@@ -74,11 +79,12 @@ def test_to_csv_escapechar(self):\n         csv_rel = duckdb.read_csv(temp_file_name, quotechar='\"', escapechar='!')\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_date_format(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_date_format(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame(tm.getTimeSeriesData())\n+        df = pandas.DataFrame(tm.getTimeSeriesData())\n         dt_index = df.index\n-        df = pd.DataFrame(\n+        df = pandas.DataFrame(\n             {\"A\": dt_index, \"B\": dt_index.shift(1)}, index=dt_index\n         )\n         rel = duckdb.from_df(df)\n@@ -88,11 +94,12 @@ def test_to_csv_date_format(self):\n \n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_timestamp_format(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_timestamp_format(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n         data = [datetime.time(hour=23, minute=1, second=34, microsecond=234345)]\n-        df = pd.DataFrame(\n-            {'0': pd.Series(data=data, dtype='object')}\n+        df = pandas.DataFrame(\n+            {'0': pandas.Series(data=data, dtype='object')}\n         )\n         rel = duckdb.from_df(df)\n         rel.to_csv(temp_file_name, timestamp_format='%m/%d/%Y')\n@@ -101,9 +108,10 @@ def test_to_csv_timestamp_format(self):\n \n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_quoting_off(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_quoting_off(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame(\n+        df = pandas.DataFrame(\n             {'a': ['string1', 'string2', 'string3']}\n         )\n         rel = duckdb.from_df(df)\n@@ -112,9 +120,10 @@ def test_to_csv_quoting_off(self):\n         csv_rel = duckdb.read_csv(temp_file_name)\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_quoting_on(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_quoting_on(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame(\n+        df = pandas.DataFrame(\n             {'a': ['string1', 'string2', 'string3']}\n         )\n         rel = duckdb.from_df(df)\n@@ -123,9 +132,10 @@ def test_to_csv_quoting_on(self):\n         csv_rel = duckdb.read_csv(temp_file_name)\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_quoting_quote_all(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_quoting_quote_all(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame(\n+        df = pandas.DataFrame(\n             {'a': ['string1', 'string2', 'string3']}\n         )\n         rel = duckdb.from_df(df)\n@@ -134,18 +144,20 @@ def test_to_csv_quoting_quote_all(self):\n         csv_rel = duckdb.read_csv(temp_file_name)\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_to_csv_encoding_incorrect(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_encoding_incorrect(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame(\n+        df = pandas.DataFrame(\n             {'a': ['string1', 'string2', 'string3']}\n         )\n         rel = duckdb.from_df(df)\n         with pytest.raises(duckdb.InvalidInputException, match=\"Invalid Input Error: The only supported encoding option is 'UTF8\"):\n             rel.to_csv(temp_file_name, encoding=\"nope\")\n \n-    def test_to_csv_encoding_correct(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_to_csv_encoding_correct(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame(\n+        df = pandas.DataFrame(\n             {'a': ['string1', 'string2', 'string3']}\n         )\n         rel = duckdb.from_df(df)\n@@ -153,9 +165,10 @@ def test_to_csv_encoding_correct(self):\n         csv_rel = duckdb.read_csv(temp_file_name)\n         assert rel.execute().fetchall() == csv_rel.execute().fetchall()\n \n-    def test_compression_gzip(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_compression_gzip(self, pandas):\n         temp_file_name = os.path.join(tempfile.mkdtemp(), next(tempfile._get_candidate_names()))\n-        df = pd.DataFrame(\n+        df = pandas.DataFrame(\n             {'a': ['string1', 'string2', 'string3']}\n         )\n         rel = duckdb.from_df(df)\ndiff --git a/tools/pythonpkg/tests/fast/arrow/parquet_write_roundtrip.py b/tools/pythonpkg/tests/fast/arrow/parquet_write_roundtrip.py\nindex 65cfc1d7dbaa..ca88abd39b0e 100644\n--- a/tools/pythonpkg/tests/fast/arrow/parquet_write_roundtrip.py\n+++ b/tools/pythonpkg/tests/fast/arrow/parquet_write_roundtrip.py\n@@ -4,11 +4,7 @@\n import numpy\n import pandas\n import datetime\n-try:\n-    import pyarrow as pa\n-    can_run = True\n-except:\n-    can_run = False\n+pa = pytest.importorskip(\"pyarrow\")\n \n def parquet_types_test(type_list):\n     temp = tempfile.NamedTemporaryFile()\n@@ -43,8 +39,6 @@ def parquet_types_test(type_list):\n \n class TestParquetRoundtrip(object):\n     def test_roundtrip_numeric(self, duckdb_cursor):\n-        if not can_run:\n-            return\n         type_list = [\n             ([-2**7, 0, 2**7-1], numpy.int8, 'TINYINT'),\n             ([-2**15, 0, 2**15-1], numpy.int16, 'SMALLINT'),\n@@ -60,8 +54,6 @@ def test_roundtrip_numeric(self, duckdb_cursor):\n         parquet_types_test(type_list)\n \n     def test_roundtrip_timestamp(self, duckdb_cursor):\n-        if not can_run:\n-            return\n         date_time_list = [\n             datetime.datetime(2018, 3, 10, 11, 17, 54),\n             datetime.datetime(1900, 12, 12, 23, 48, 42),\n@@ -78,8 +70,6 @@ def test_roundtrip_timestamp(self, duckdb_cursor):\n         parquet_types_test(type_list)\n \n     def test_roundtrip_varchar(self, duckdb_cursor):\n-        if not can_run:\n-            return\n         varchar_list = [\n             'hello',\n             'this is a very long string',\n@@ -90,4 +80,3 @@ def test_roundtrip_varchar(self, duckdb_cursor):\n             (varchar_list, object, 'VARCHAR')\n         ]\n         parquet_types_test(type_list)\n-\ndiff --git a/tools/pythonpkg/tests/fast/arrow/test_6796.py b/tools/pythonpkg/tests/fast/arrow/test_6796.py\nindex 2bd2f85c0307..85177a5db114 100644\n--- a/tools/pythonpkg/tests/fast/arrow/test_6796.py\n+++ b/tools/pythonpkg/tests/fast/arrow/test_6796.py\n@@ -1,10 +1,11 @@\n import duckdb\n-import pandas\n import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n pyarrow = pytest.importorskip('pyarrow')\n \n-def test_6796():\n+@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+def test_6796(pandas):\n \tconn = duckdb.connect()\n \tinput_df = pandas.DataFrame({ \"foo\": [\"bar\"] })\n \tconn.register(\"input_df\", input_df)\ndiff --git a/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py b/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\nindex 263ff19cafab..ddec7739c47c 100644\n--- a/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\n+++ b/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\n@@ -2,16 +2,12 @@\n import os\n import pytest\n import tempfile\n-try:\n-    import pyarrow as pa\n-    import pyarrow.parquet as pq\n-    import pyarrow.dataset as ds\n-    import numpy as np\n-    import pandas as pd\n-    import re\n-    can_run = True\n-except:\n-    can_run = False\n+pa = pytest.importorskip(\"pyarrow\")\n+pq = pytest.importorskip(\"pyarrow.parquet\")\n+ds = pytest.importorskip(\"pyarrow.dataset\")\n+np = pytest.importorskip(\"numpy\")\n+re = pytest.importorskip(\"re\")\n+from conftest import NumpyPandas, ArrowPandas\n \n ## DuckDB connection used in this test\n duckdb_conn = duckdb.connect()\n@@ -119,8 +115,6 @@ def string_check_or_pushdown(tbl_name):\n \n class TestArrowFilterPushdown(object):\n     def test_filter_pushdown_numeric(self,duckdb_cursor):\n-        if not can_run:\n-            return\n \n         numeric_types = ['TINYINT', 'SMALLINT', 'INTEGER', 'BIGINT', 'UTINYINT', 'USMALLINT', 'UINTEGER', 'UBIGINT',\n         'FLOAT', 'DOUBLE', 'HUGEINT']\n@@ -130,8 +124,6 @@ def test_filter_pushdown_numeric(self,duckdb_cursor):\n             numeric_check_or_pushdown(tbl_name)\n \n     def test_filter_pushdown_decimal(self,duckdb_cursor):\n-        if not can_run:\n-            return\n         numeric_types = {'DECIMAL(4,1)': 'test_decimal_4_1', 'DECIMAL(9,1)': 'test_decimal_9_1',\n                          'DECIMAL(18,4)': 'test_decimal_18_4','DECIMAL(30,12)': 'test_decimal_30_12'}\n         for data_type in numeric_types:\n@@ -140,8 +132,6 @@ def test_filter_pushdown_decimal(self,duckdb_cursor):\n             numeric_check_or_pushdown(tbl_name)\n \n     def test_filter_pushdown_varchar(self,duckdb_cursor):\n-        if not can_run:\n-            return\n         duckdb_conn.execute(\"CREATE TABLE test_varchar (a  VARCHAR, b VARCHAR, c VARCHAR)\")\n         duckdb_conn.execute(\"INSERT INTO  test_varchar VALUES ('1','1','1'),('10','10','10'),('100','10','100'),(NULL,NULL,NULL)\")\n         duck_tbl = duckdb_conn.table(\"test_varchar\")\n@@ -175,8 +165,6 @@ def test_filter_pushdown_varchar(self,duckdb_cursor):\n \n \n     def test_filter_pushdown_bool(self,duckdb_cursor):\n-        if not can_run:\n-            return\n         duckdb_conn.execute(\"CREATE TABLE test_bool (a  BOOL, b BOOL)\")\n         duckdb_conn.execute(\"INSERT INTO  test_bool VALUES (TRUE,TRUE),(TRUE,FALSE),(FALSE,TRUE),(NULL,NULL)\")\n         duck_tbl = duckdb_conn.table(\"test_bool\")\n@@ -197,8 +185,6 @@ def test_filter_pushdown_bool(self,duckdb_cursor):\n         assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = True or b =True\").fetchone()[0] == 3\n \n     def test_filter_pushdown_time(self,duckdb_cursor):\n-        if not can_run:\n-            return\n         duckdb_conn.execute(\"CREATE TABLE test_time (a  TIME, b TIME, c TIME)\")\n         duckdb_conn.execute(\"INSERT INTO  test_time VALUES ('00:01:00','00:01:00','00:01:00'),('00:10:00','00:10:00','00:10:00'),('01:00:00','00:10:00','01:00:00'),(NULL,NULL,NULL)\")\n         duck_tbl = duckdb_conn.table(\"test_time\")\n@@ -228,8 +214,6 @@ def test_filter_pushdown_time(self,duckdb_cursor):\n         assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = '01:00:00' or b ='00:01:00'\").fetchone()[0] == 2\n \n     def test_filter_pushdown_timestamp(self,duckdb_cursor):\n-        if not can_run:\n-            return\n         duckdb_conn.execute(\"CREATE TABLE test_timestamp (a  TIMESTAMP, b TIMESTAMP, c TIMESTAMP)\")\n         duckdb_conn.execute(\"INSERT INTO  test_timestamp VALUES ('2008-01-01 00:00:01','2008-01-01 00:00:01','2008-01-01 00:00:01'),('2010-01-01 10:00:01','2010-01-01 10:00:01','2010-01-01 10:00:01'),('2020-03-01 10:00:01','2010-01-01 10:00:01','2020-03-01 10:00:01'),(NULL,NULL,NULL)\")\n         duck_tbl = duckdb_conn.table(\"test_timestamp\")\n@@ -260,8 +244,6 @@ def test_filter_pushdown_timestamp(self,duckdb_cursor):\n         assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = '2020-03-01 10:00:01' or b ='2008-01-01 00:00:01'\").fetchone()[0] == 2\n \n     def test_filter_pushdown_timestamp_TZ(self,duckdb_cursor):\n-        if not can_run:\n-            return\n         duckdb_conn.execute(\"CREATE TABLE test_timestamptz (a  TIMESTAMPTZ, b TIMESTAMPTZ, c TIMESTAMPTZ)\")\n         duckdb_conn.execute(\"INSERT INTO  test_timestamptz VALUES ('2008-01-01 00:00:01','2008-01-01 00:00:01','2008-01-01 00:00:01'),('2010-01-01 10:00:01','2010-01-01 10:00:01','2010-01-01 10:00:01'),('2020-03-01 10:00:01','2010-01-01 10:00:01','2020-03-01 10:00:01'),(NULL,NULL,NULL)\")\n         duck_tbl = duckdb_conn.table(\"test_timestamptz\")\n@@ -293,8 +275,6 @@ def test_filter_pushdown_timestamp_TZ(self,duckdb_cursor):\n \n \n     def test_filter_pushdown_date(self,duckdb_cursor):\n-        if not can_run:\n-            return\n         duckdb_conn.execute(\"CREATE TABLE test_date (a  DATE, b DATE, c DATE)\")\n         duckdb_conn.execute(\"INSERT INTO  test_date VALUES ('2000-01-01','2000-01-01','2000-01-01'),('2000-10-01','2000-10-01','2000-10-01'),('2010-01-01','2000-10-01','2010-01-01'),(NULL,NULL,NULL)\")\n         duck_tbl = duckdb_conn.table(\"test_date\")\n@@ -323,10 +303,9 @@ def test_filter_pushdown_date(self,duckdb_cursor):\n         # Try Or\n         assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = '2010-01-01' or b ='2000-01-01'\").fetchone()[0] == 2\n \n-    def test_filter_pushdown_blob(self,duckdb_cursor):\n-        if not can_run:\n-            return\n-        df = pd.DataFrame({'a': [bytes([1]), bytes([2]), bytes([3]), None], 'b': [bytes([1]), bytes([2]), bytes([3]), None],'c': [bytes([1]), bytes([2]), bytes([3]), None]})\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_filter_pushdown_blob(self, pandas):\n+        df = pandas.DataFrame({'a': [bytes([1]), bytes([2]), bytes([3]), None], 'b': [bytes([1]), bytes([2]), bytes([3]), None],'c': [bytes([1]), bytes([2]), bytes([3]), None]})\n         arrow_table = pa.Table.from_pandas(df)\n \n         # Try ==\n@@ -353,8 +332,6 @@ def test_filter_pushdown_blob(self,duckdb_cursor):\n \n \n     def test_filter_pushdown_no_projection(self,duckdb_cursor):\n-        if not can_run:\n-            return\n         duckdb_conn.execute(\"CREATE TABLE test_int (a  INTEGER, b INTEGER, c INTEGER)\")\n         duckdb_conn.execute(\"INSERT INTO  test_int VALUES (1,1,1),(10,10,10),(100,10,100),(NULL,NULL,NULL)\")\n         duck_tbl = duckdb_conn.table(\"test_int\")\n@@ -365,16 +342,15 @@ def test_filter_pushdown_no_projection(self,duckdb_cursor):\n         duckdb_conn.register(\"testarrowdataset\",arrow_dataset)\n         assert duckdb_conn.execute(\"SELECT * FROM  testarrowdataset VALUES where a =1\").fetchall() == [(1, 1, 1)]\n \n-    def test_filter_pushdown_2145(self,duckdb_cursor):\n-        if not can_run:\n-            return\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_filter_pushdown_2145(self,duckdb_cursor, pandas):\n \n-        date1 = pd.date_range(\"2018-01-01\", \"2018-12-31\", freq=\"B\")\n-        df1 = pd.DataFrame(np.random.randn(date1.shape[0], 5), columns=list(\"ABCDE\"))\n+        date1 = pandas.date_range(\"2018-01-01\", \"2018-12-31\", freq=\"B\")\n+        df1 = pandas.DataFrame(np.random.randn(date1.shape[0], 5), columns=list(\"ABCDE\"))\n         df1[\"date\"] = date1\n \n-        date2 = pd.date_range(\"2019-01-01\", \"2019-12-31\", freq=\"B\")\n-        df2 = pd.DataFrame(np.random.randn(date2.shape[0], 5), columns=list(\"ABCDE\"))\n+        date2 = pandas.date_range(\"2019-01-01\", \"2019-12-31\", freq=\"B\")\n+        df2 = pandas.DataFrame(np.random.randn(date2.shape[0], 5), columns=list(\"ABCDE\"))\n         df2[\"date\"] = date2\n \n         pq.write_table(pa.table(df1), \"data1.parquet\")\n@@ -387,14 +363,12 @@ def test_filter_pushdown_2145(self,duckdb_cursor):\n \n         output_df = duckdb.arrow(table).filter(\"date > '2019-01-01'\").df()\n         expected_df = duckdb.from_parquet(\"data*.parquet\").filter(\"date > '2019-01-01'\").df()\n-        pd.testing.assert_frame_equal(expected_df, output_df)\n+        pandas.testing.assert_frame_equal(expected_df, output_df)\n \n         os.remove(\"data1.parquet\")\n         os.remove(\"data2.parquet\")\n \n     def test_filter_column_removal(self,duckdb_cursor):\n-        if not can_run:\n-            return\n         duckdb_conn.execute(\"CREATE TABLE test AS SELECT range i, range j FROM range(5)\")\n         duck_test_table = duckdb_conn.table(\"test\")\n         arrow_test_table = duck_test_table.arrow()\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_2304.py b/tools/pythonpkg/tests/fast/pandas/test_2304.py\nindex 6ccd27071f8b..c44fb07f7829 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_2304.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_2304.py\n@@ -1,17 +1,19 @@\n import duckdb\n-import pandas as pd\n import numpy as np\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestPandasMergeSameName(object):\n-    def test_2304(self, duckdb_cursor):\n-        df1 = pd.DataFrame({\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_2304(self, duckdb_cursor, pandas):\n+        df1 = pandas.DataFrame({\n             'id_1': [1, 1, 1, 2, 2],\n             'agedate': np.array(['2010-01-01','2010-02-01','2010-03-01','2020-02-01', '2020-03-01']).astype('datetime64[D]'),\n             'age': [1, 2, 3, 1, 2],\n             'v': [1.1, 1.2, 1.3, 2.1, 2.2]\n         })\n \n-        df2 = pd.DataFrame({\n+        df2 = pandas.DataFrame({\n             'id_1': [1, 1, 2],\n             'agedate': np.array(['2010-01-01','2010-02-01', '2020-03-01']).astype('datetime64[D]'),\n             'v2': [11.1, 11.2, 21.2]\n@@ -31,20 +33,21 @@ def test_2304(self, duckdb_cursor):\n \n         assert result == expected_result\n \n-    def test_pd_names(self, duckdb_cursor):\n-        df1 = pd.DataFrame({\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_pd_names(self, duckdb_cursor, pandas):\n+        df1 = pandas.DataFrame({\n             'id': [1, 1, 2],\n             'id_1': [1, 1, 2],\n             'id_3': [1, 1, 2],\n         })\n \n-        df2 = pd.DataFrame({\n+        df2 = pandas.DataFrame({\n             'id': [1, 1, 2],\n             'id_1': [1, 1, 2],\n             'id_2': [1, 1, 1]\n         })\n \n-        exp_result = pd.DataFrame({\n+        exp_result = pandas.DataFrame({\n             'id': [1, 1, 2, 1, 1],\n             'id_1': [1, 1, 2, 1, 1],\n             'id_3': [1, 1, 2, 1, 1],\n@@ -61,20 +64,21 @@ def test_pd_names(self, duckdb_cursor):\n         ON (df1.id_1=df2.id_1)\"\"\"\n \n         result_df = con.execute(query).fetchdf()\n-        assert(exp_result.equals(result_df))\n+        pandas.testing.assert_frame_equal(exp_result, result_df)\n \n-    def test_repeat_name(self, duckdb_cursor):\n-        df1 = pd.DataFrame({\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_repeat_name(self, duckdb_cursor, pandas):\n+        df1 = pandas.DataFrame({\n             'id': [1],\n             'id_1': [1],\n             'id_2': [1],\n         })\n \n-        df2 = pd.DataFrame({\n+        df2 = pandas.DataFrame({\n             'id': [1]\n         })\n \n-        exp_result = pd.DataFrame({\n+        exp_result = pandas.DataFrame({\n             'id': [1],\n             'id_1': [1],\n             'id_2': [1],\n@@ -84,10 +88,12 @@ def test_repeat_name(self, duckdb_cursor):\n         con = duckdb.connect()\n         con.register('df1', df1)\n         con.register('df2', df2)\n-        query = \"\"\"SELECT * from df1\n-        LEFT OUTER JOIN df2\n-        ON (df1.id=df2.id)\"\"\"\n \n-        result_df = con.execute(query).fetchdf()\n-        print(result_df)\n-        assert(exp_result.equals(result_df))\n\\ No newline at end of file\n+        result_df = con.execute(\n+            \"\"\"\n+                SELECT * from df1\n+                LEFT OUTER JOIN df2\n+                ON (df1.id=df2.id)\n+            \"\"\"\n+        ).fetchdf()\n+        pandas.testing.assert_frame_equal(exp_result, result_df)\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_append_df.py b/tools/pythonpkg/tests/fast/pandas/test_append_df.py\nindex 391c2a7fa991..b613969524d8 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_append_df.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_append_df.py\n@@ -1,11 +1,13 @@\n import duckdb\n-import pandas as pd\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestAppendDF(object):\n \n-    def test_df_to_table_append(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_df_to_table_append(self, duckdb_cursor, pandas):\n         conn = duckdb.connect()\n         conn.execute(\"Create table integers (i integer)\")\n-        df_in = pd.DataFrame({'numbers': [1,2,3,4,5],})\n+        df_in = pandas.DataFrame({'numbers': [1,2,3,4,5],})\n         conn.append('integers',df_in)\n         assert conn.execute('select count(*) from integers').fetchone()[0] == 5\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_bug5922.py b/tools/pythonpkg/tests/fast/pandas/test_bug5922.py\nindex 36a8ee1ae5b9..ab57263f6054 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_bug5922.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_bug5922.py\n@@ -1,11 +1,11 @@\n-import pandas as pd\n import duckdb\n-\n-\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestPandasAcceptFloat16(object):\n-    def test_pandas_accept_float16(self, duckdb_cursor):\n-        df = pd.DataFrame({'col': [1,2,3]})\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_pandas_accept_float16(self, duckdb_cursor, pandas):\n+        df = pandas.DataFrame({'col': [1,2,3]})\n         df16 = df.astype({'col':'float16'})\n         con = duckdb.connect()\n         con.execute('CREATE TABLE tbl AS SELECT * FROM df16')\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_create_table_from_pandas.py b/tools/pythonpkg/tests/fast/pandas/test_create_table_from_pandas.py\nindex 4bc2f67fd60c..0eab9e61d7b2 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_create_table_from_pandas.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_create_table_from_pandas.py\n@@ -1,21 +1,21 @@\n import pytest\n import duckdb\n-import pandas as pd\n import numpy as np\n import sys\n+from conftest import NumpyPandas, ArrowPandas\n \n-def assert_create(internal_data, expected_result, data_type):\n+def assert_create(internal_data, expected_result, data_type, pandas):\n     conn = duckdb.connect()\n-    df_in = pd.DataFrame(data=internal_data, dtype=data_type)\n+    df_in = pandas.DataFrame(data=internal_data, dtype=data_type)\n \n     conn.execute(\"CREATE TABLE t AS SELECT * FROM df_in\")\n \n     result = conn.execute(\"SELECT * FROM t\").fetchall()\n     assert result == expected_result\n \n-def assert_create_register(internal_data, expected_result, data_type):\n+def assert_create_register(internal_data, expected_result, data_type, pandas):\n     conn = duckdb.connect()\n-    df_in = pd.DataFrame(data=internal_data, dtype=data_type)\n+    df_in = pandas.DataFrame(data=internal_data, dtype=data_type)\n     conn.register(\"dataframe\", df_in)\n     conn.execute(\"CREATE TABLE t AS SELECT * FROM dataframe\")\n \n@@ -24,7 +24,8 @@ def assert_create_register(internal_data, expected_result, data_type):\n \n class TestCreateTableFromPandas(object):\n \n-    def test_integer_create_table(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_integer_create_table(self, duckdb_cursor, pandas):\n         if sys.version_info.major < 3:\n             return\n         #FIXME: This should work with other data types e.g., int8...\n@@ -33,7 +34,7 @@ def test_integer_create_table(self, duckdb_cursor):\n         expected_result = [(1,), (2,), (3,), (4,)]\n         for data_type in data_types:\n             print(data_type)\n-            assert_create_register(internal_data,expected_result,data_type)\n-            assert_create(internal_data,expected_result,data_type)\n+            assert_create_register(internal_data,expected_result,data_type, pandas)\n+            assert_create(internal_data,expected_result,data_type, pandas)\n \n     #FIXME: Also test other data types\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_datetime_time.py b/tools/pythonpkg/tests/fast/pandas/test_datetime_time.py\nindex 66b4dbcebcc4..dbcb9e8342c6 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_datetime_time.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_datetime_time.py\n@@ -1,66 +1,72 @@\n-import pandas as pd\n import duckdb\n import numpy as np\n import pytest\n+from conftest import NumpyPandas, ArrowPandas\n from datetime import datetime, timezone, time, timedelta\n \n class TestDateTimeTime(object):\n \n-    def test_time_high(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_time_high(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT make_time(23, 1, 34.234345) AS '0'\").df()\n         data = [time(hour=23, minute=1, second=34, microsecond=234345)]\n-        df_in = pd.DataFrame(\n-            {'0': pd.Series(data=data, dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {'0': pandas.Series(data=data, dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\n \n-    def test_time_low(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_time_low(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT make_time(00, 01, 1.000) AS '0'\").df()\n         data = [time(hour=0, minute=1, second=1)]\n-        df_in = pd.DataFrame(\n-            {'0': pd.Series(data=data, dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {'0': pandas.Series(data=data, dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\n \n-    def test_time_timezone_regular(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_time_timezone_regular(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT make_time(00, 01, 1.000) AS '0'\").df()\n         # time is 3 hours ahead of UTC\n         offset = timedelta(hours=3)\n         tz = timezone(offset)\n         data = [time(hour=3, minute=1, second=1, tzinfo=tz)]\n-        df_in = pd.DataFrame(\n-            {'0': pd.Series(data=data, dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {'0': pandas.Series(data=data, dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\n \n-    def test_time_timezone_negative_extreme(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_time_timezone_negative_extreme(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT make_time(12, 01, 1.000) AS '0'\").df()\n         # time is 14 hours behind UTC\n         offset = timedelta(hours=-14)\n         tz = timezone(offset)\n         data = [time(hour=22, minute=1, second=1, tzinfo=tz)]\n-        df_in = pd.DataFrame(\n-            {'0': pd.Series(data=data, dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {'0': pandas.Series(data=data, dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\n \n-    def test_time_timezone_positive_extreme(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_time_timezone_positive_extreme(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT make_time(12, 01, 1.000) AS '0'\").df()\n         # time is 20 hours ahead of UTC\n         offset = timedelta(hours=20)\n         tz = timezone(offset)\n         data = [time(hour=8, minute=1, second=1, tzinfo=tz)]\n-        df_in = pd.DataFrame(\n-            {'0': pd.Series(data=data, dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {'0': pandas.Series(data=data, dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\n \n-    def test_pandas_datetime_overflow(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_pandas_datetime_overflow(self, pandas):\n         duckdb_con = duckdb.connect()\n \n         duckdb_con.execute(\"create table test (date DATE)\")\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_datetime_timestamp.py b/tools/pythonpkg/tests/fast/pandas/test_datetime_timestamp.py\nindex 133c41c5ed3d..b619d9a732fd 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_datetime_timestamp.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_datetime_timestamp.py\n@@ -1,57 +1,61 @@\n-import pandas as pd\n import duckdb\n import datetime\n import numpy as np\n import pytest\n-\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestDateTimeTimeStamp(object):\n \n-    def test_timestamp_high(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_timestamp_high(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT '2260-01-01 23:59:00'::TIMESTAMP AS '0'\").df()\n-        df_in = pd.DataFrame(\n-            {0: pd.Series(data=[datetime.datetime(year=2260, month=1, day=1, hour=23, minute=59)], dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {0: pandas.Series(data=[datetime.datetime(year=2260, month=1, day=1, hour=23, minute=59)], dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\n \n-    def test_timestamp_low(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_timestamp_low(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT '1680-01-01 23:59:00'::TIMESTAMP AS '0'\").df()\n-        df_in = pd.DataFrame(\n-            {0: pd.Series(data=[datetime.datetime(year=1680, month=1, day=1, hour=23, minute=59)], dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {0: pandas.Series(data=[datetime.datetime(year=1680, month=1, day=1, hour=23, minute=59)], dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\n \n-    def test_timestamp_timezone_regular(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_timestamp_timezone_regular(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT '2022-01-01 12:00:00'::TIMESTAMP AS '0'\").df()\n         # time is 3 hours ahead of UTC\n         offset = datetime.timedelta(hours=3)\n         timezone = datetime.timezone(offset)\n-        df_in = pd.DataFrame(\n-            {0: pd.Series(data=[datetime.datetime(year=2022, month=1, day=1, hour=15, tzinfo=timezone)], dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {0: pandas.Series(data=[datetime.datetime(year=2022, month=1, day=1, hour=15, tzinfo=timezone)], dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\n \n-    def test_timestamp_timezone_negative_extreme(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_timestamp_timezone_negative_extreme(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT '2022-01-01 12:00:00'::TIMESTAMP AS '0'\").df()\n         # time is 14 hours behind UTC\n         offset = datetime.timedelta(hours=-14)\n         timezone = datetime.timezone(offset)\n-        df_in = pd.DataFrame(\n-            {0: pd.Series(data=[datetime.datetime(year=2021, month=12, day=31, hour=22, tzinfo=timezone)], dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {0: pandas.Series(data=[datetime.datetime(year=2021, month=12, day=31, hour=22, tzinfo=timezone)], dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\n \n-    def test_timestamp_timezone_positive_extreme(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_timestamp_timezone_positive_extreme(self, duckdb_cursor, pandas):\n         duckdb_time = duckdb.query(\"SELECT '2021-12-31 23:00:00'::TIMESTAMP AS '0'\").df()\n         # time is 20 hours ahead of UTC\n         offset = datetime.timedelta(hours=20)\n         timezone = datetime.timezone(offset)\n-        df_in = pd.DataFrame(\n-            {0: pd.Series(data=[datetime.datetime(year=2022, month=1, day=1, hour=19, tzinfo=timezone)], dtype='object')}\n+        df_in = pandas.DataFrame(\n+            {0: pandas.Series(data=[datetime.datetime(year=2022, month=1, day=1, hour=19, tzinfo=timezone)], dtype='object')}\n         )\n         df_out = duckdb.query_df(df_in, \"df\", \"select * from df\").df()\n-        pd.testing.assert_frame_equal(df_out, duckdb_time)\n+        pandas.testing.assert_frame_equal(df_out, duckdb_time)\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_df_analyze.py b/tools/pythonpkg/tests/fast/pandas/test_df_analyze.py\nindex 7c8a214f4abb..2275519cc763 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_df_analyze.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_df_analyze.py\n@@ -1,48 +1,56 @@\n-import pandas as pd\n import duckdb\n import datetime\n import numpy as np\n import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n-def create_generic_dataframe(data):\n-    return pd.DataFrame({'col0': pd.Series(data=data, dtype='object')})\n+def create_generic_dataframe(data, pandas):\n+\treturn pandas.DataFrame({'col0': pandas.Series(data=data, dtype='object')})\n \n class TestResolveObjectColumns(object):\n-\tdef test_sample_low_correct(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_sample_low_correct(self, duckdb_cursor, pandas):\n+\t\tprint(pandas.backend)\n \t\tduckdb_conn = duckdb.connect()\n \t\tduckdb_conn.execute(\"SET GLOBAL pandas_analyze_sample=3\")\n \t\tdata = [1000008, 6, 9, 4, 1, 6]\n-\t\tdf = create_generic_dataframe(data)\n+\t\tdf = create_generic_dataframe(data, pandas)\n \t\troundtripped_df = duckdb.query_df(df, \"x\", \"select * from x\", connection=duckdb_conn).df()\n \t\tduckdb_df = duckdb_conn.query(\"select * FROM (VALUES (1000008), (6), (9), (4), (1), (6)) as '0'\").df()\n-\t\tpd.testing.assert_frame_equal(duckdb_df, roundtripped_df)\n+\t\tpandas.testing.assert_frame_equal(duckdb_df, roundtripped_df, check_dtype=False)\n \n-\tdef test_sample_low_incorrect_detected(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_sample_low_incorrect_detected(self, duckdb_cursor, pandas):\n \t\tduckdb_conn = duckdb.connect()\n \t\tduckdb_conn.execute(\"SET GLOBAL pandas_analyze_sample=2\")\n \t\t# size of list (6) divided by 'pandas_analyze_sample' (2) is the increment used\n \t\t# in this case index 0 (1000008) and index 3 ([4]) are checked, which dont match\n \t\tdata = [1000008, 6, 9, [4], 1, 6]\n-\t\tdf = create_generic_dataframe(data)\n+\t\tdf = create_generic_dataframe(data, pandas)\n \t\troundtripped_df = duckdb.query_df(df, \"x\", \"select * from x\", connection=duckdb_conn).df()\n \t\t# Sample high enough to detect mismatch in types, fallback to VARCHAR\n \t\tassert(roundtripped_df['col0'].dtype == np.dtype('object'))\n \n-\tdef test_sample_zero(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_sample_zero(self, duckdb_cursor, pandas):\n \t\tduckdb_conn = duckdb.connect()\n \t\t# Disable dataframe analyze\n \t\tduckdb_conn.execute(\"SET GLOBAL pandas_analyze_sample=0\")\n \t\tdata = [1000008, 6, 9, 3, 1, 6]\n-\t\tdf = create_generic_dataframe(data)\n+\t\tdf = create_generic_dataframe(data, pandas)\n \t\troundtripped_df = duckdb.query_df(df, \"x\", \"select * from x\", connection=duckdb_conn).df()\n \t\t# Always converts to VARCHAR\n-\t\tassert(roundtripped_df['col0'].dtype == np.dtype('object'))\n+\t\tif (pandas.backend == 'pyarrow'):\n+\t\t\tassert(roundtripped_df['col0'].dtype == np.dtype('int64'))\n+\t\telse:\n+\t\t\tassert(roundtripped_df['col0'].dtype == np.dtype('object'))\n \n-\tdef test_sample_low_incorrect_undetected(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_sample_low_incorrect_undetected(self, duckdb_cursor, pandas):\n \t\tduckdb_conn = duckdb.connect()\n \t\tduckdb_conn.execute(\"SET GLOBAL pandas_analyze_sample=1\")\n \t\tdata = [1000008, 6, 9, [4], [1], 6]\n-\t\tdf = create_generic_dataframe(data)\n+\t\tdf = create_generic_dataframe(data, pandas)\n \t\t# Sample size is too low to detect the mismatch, exception is raised when trying to convert\n \t\twith pytest.raises(duckdb.InvalidInputException, match=\"Failed to cast value: Unimplemented type for cast\"):\n \t\t\troundtripped_df = duckdb.query_df(df, \"x\", \"select * from x\", connection=duckdb_conn).df()\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\nindex fc23fd4fba41..ac19221aa163 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n@@ -1,4 +1,3 @@\n-import pandas as pd\n import duckdb\n import datetime\n import numpy as np\n@@ -7,11 +6,12 @@\n import math\n from decimal import Decimal\n import re\n+from conftest import NumpyPandas, ArrowPandas\n \n standard_vector_size = duckdb.__standard_vector_size__\n \n-def create_generic_dataframe(data):\n-    return pd.DataFrame({'0': pd.Series(data=data, dtype='object')})\n+def create_generic_dataframe(data, pandas):\n+    return pandas.DataFrame({'0': pandas.Series(data=data, dtype='object')})\n \n class IntString:\n     def __init__(self, value: int):\n@@ -21,33 +21,37 @@ def __str__(self):\n \n # To avoid DECIMAL being upgraded to DOUBLE (because DOUBLE outranks DECIMAL as a LogicalType)\n # These floats had their precision preserved as string and are now cast to decimal.Decimal\n-def ConvertStringToDecimal(data: list):\n+def ConvertStringToDecimal(data: list, pandas):\n     for i in range(len(data)):\n         if isinstance(data[i], str):\n             data[i] = decimal.Decimal(data[i])\n-    data = pd.Series(data=data, dtype='object')\n+    data = pandas.Series(data=data, dtype='object')\n     return data\n \n class TestResolveObjectColumns(object):\n \n-    def test_integers(self):\n+    # TODO: add support for ArrowPandas\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_integers(self, pandas):\n         data = [5, 0, 3]\n-        df_in = create_generic_dataframe(data)\n+        df_in = create_generic_dataframe(data, pandas)\n         # These are float64 because pandas would force these to be float64 even if we set them to int8, int16, int32, int64 respectively\n-        df_expected_res = pd.DataFrame({'0': pd.Series(data=data, dtype='int8')})\n+        df_expected_res = pandas.DataFrame({'0': pandas.Series(data=data, dtype='int8')})\n         df_out = duckdb.query_df(df_in, \"data\", \"SELECT * FROM data\").df()\n         print(df_out)\n-        pd.testing.assert_frame_equal(df_expected_res, df_out)\n+        pandas.testing.assert_frame_equal(df_expected_res, df_out)\n \n-    def test_struct_correct(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_struct_correct(self, pandas):\n         data = [{'a': 1, 'b': 3, 'c': 3, 'd': 7}]\n-        df = pd.DataFrame({'0': pd.Series(data=data)})\n+        df = pandas.DataFrame({'0': pandas.Series(data=data)})\n         duckdb_col = duckdb.query(\"SELECT {a: 1, b: 3, c: 3, d: 7} as '0'\").df()\n         converted_col = duckdb.query_df(df, \"data\", \"SELECT * FROM data\").df()\n-        pd.testing.assert_frame_equal(duckdb_col, converted_col)\n+        pandas.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_map_fallback_different_keys(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_fallback_different_keys(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n@@ -58,7 +62,7 @@ def test_map_fallback_different_keys(self):\n         )\n \n         converted_df = duckdb.query_df(x, \"x\", \"SELECT * FROM x\").df()\n-        y = pd.DataFrame(\n+        y = pandas.DataFrame(\n             [\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 7]}],\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 7]}],\n@@ -68,10 +72,11 @@ def test_map_fallback_different_keys(self):\n             ]\n         )\n         equal_df = duckdb.query_df(y, \"y\", \"SELECT * FROM y\").df()\n-        pd.testing.assert_frame_equal(converted_df, equal_df)\n+        pandas.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_map_fallback_incorrect_amount_of_keys(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_fallback_incorrect_amount_of_keys(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n@@ -81,7 +86,7 @@ def test_map_fallback_incorrect_amount_of_keys(self):\n             ]\n         )\n         converted_df = duckdb.query_df(x, \"x\", \"SELECT * FROM x\").df()\n-        y = pd.DataFrame(\n+        y = pandas.DataFrame(\n             [\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 7]}],\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 7]}],\n@@ -91,10 +96,11 @@ def test_map_fallback_incorrect_amount_of_keys(self):\n             ]\n         )\n         equal_df = duckdb.query_df(y, \"y\", \"SELECT * FROM y\").df()\n-        pd.testing.assert_frame_equal(converted_df, equal_df)\n+        pandas.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_struct_value_upgrade(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_struct_value_upgrade(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 'string'}],\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n@@ -103,7 +109,7 @@ def test_struct_value_upgrade(self):\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}]\n             ]\n         )\n-        y = pd.DataFrame(\n+        y = pandas.DataFrame(\n             [\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 'string'}],\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': '7'}],\n@@ -114,10 +120,11 @@ def test_struct_value_upgrade(self):\n         )\n         converted_df = duckdb.query_df(x, \"x\", \"SELECT * FROM x\").df()\n         equal_df = duckdb.query_df(y, \"y\", \"SELECT * FROM y\").df()\n-        pd.testing.assert_frame_equal(converted_df, equal_df)\n+        pandas.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_struct_null(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_struct_null(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [None],\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n@@ -126,7 +133,7 @@ def test_struct_null(self):\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}]\n             ]\n         )\n-        y = pd.DataFrame(\n+        y = pandas.DataFrame(\n             [\n                 [None],\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n@@ -137,10 +144,11 @@ def test_struct_null(self):\n         )\n         converted_df = duckdb.query_df(x, \"x\", \"SELECT * FROM x\").df()\n         equal_df = duckdb.query_df(y, \"y\", \"SELECT * FROM y\").df()\n-        pd.testing.assert_frame_equal(converted_df, equal_df)\n+        pandas.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_map_fallback_value_upgrade(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_fallback_value_upgrade(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 'test'}],\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}],\n@@ -149,7 +157,7 @@ def test_map_fallback_value_upgrade(self):\n                 [{'a': 1, 'b': 3, 'c': 3, 'd': 7}]\n             ]\n         )\n-        y = pd.DataFrame(\n+        y = pandas.DataFrame(\n             [\n                 [{'a': '1', 'b': '3', 'c': '3', 'd': 'test'}],\n                 [{'a': '1', 'b': '3', 'c': '3', 'd': '7'}],\n@@ -160,10 +168,12 @@ def test_map_fallback_value_upgrade(self):\n         )\n         converted_df = duckdb.query_df(x, \"df\", \"SELECT * FROM df\").df()\n         equal_df = duckdb.query_df(y, \"df\", \"SELECT * FROM df\").df()\n-        pd.testing.assert_frame_equal(converted_df, equal_df)\n+        pandas.testing.assert_frame_equal(converted_df, equal_df)\n \n-    def test_map_correct(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_correct(self, pandas):\n+        con = duckdb.connect()\n+        x = pandas.DataFrame(\n             [\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 7]}],\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 7]}],\n@@ -173,23 +183,25 @@ def test_map_correct(self):\n             ]\n         )\n         x.rename(columns = {0 : 'a'}, inplace = True)\n-        converted_col = duckdb.query_df(x, \"x\", \"select * from x as 'a'\").df()\n-        duckdb.query(\"\"\"\n+        converted_col = duckdb.query_df(x, \"x\", \"select * from x as 'a'\", connection=con).df()\n+        con.query(\"\"\"\n             CREATE TABLE tmp(\n                 a MAP(VARCHAR, INTEGER)\n             );\n         \"\"\")\n         for _ in range(5):\n-            duckdb.query(\"\"\"\n+            con.query(\"\"\"\n                 INSERT INTO tmp VALUES (MAP(['a', 'b', 'c', 'd'], [1, 3, 3, 7]))\n             \"\"\")\n-        duckdb_col = duckdb.query(\"select a from tmp AS '0'\").df()\n+        duckdb_col = con.query(\"select a from tmp AS '0'\").df()\n         print(duckdb_col.columns)\n         print(converted_col.columns)\n-        pd.testing.assert_frame_equal(converted_col, duckdb_col)\n+        pandas.testing.assert_frame_equal(converted_col, duckdb_col)\n \n-    def test_map_value_upgrade(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_value_upgrade(self, pandas):\n+        con = duckdb.connect()\n+        x = pandas.DataFrame(\n             [\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 'test']}],\n                 [{'key': ['a', 'b', 'c', 'd'], 'value': [1, 3, 3, 7]}],\n@@ -199,26 +211,27 @@ def test_map_value_upgrade(self):\n             ]\n         )\n         x.rename(columns = {0 : 'a'}, inplace = True)\n-        converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n-        duckdb.query(\"\"\"\n+        converted_col = duckdb.query_df(x, \"x\", \"select * from x\", connection=con).df()\n+        con.query(\"\"\"\n             CREATE TABLE tmp2(\n                 a MAP(VARCHAR, VARCHAR)\n             );\n         \"\"\")\n-        duckdb.query(\"\"\"\n+        con.query(\"\"\"\n             INSERT INTO tmp2 VALUES (MAP(['a', 'b', 'c', 'd'], ['1', '3', '3', 'test']))\n         \"\"\")\n         for _ in range(4):\n-            duckdb.query(\"\"\"\n+            con.query(\"\"\"\n                 INSERT INTO tmp2 VALUES (MAP(['a', 'b', 'c', 'd'], ['1', '3', '3', '7']))\n             \"\"\")\n-        duckdb_col = duckdb.query(\"select a from tmp2 AS '0'\").df()\n+        duckdb_col = con.query(\"select a from tmp2 AS '0'\").df()\n         print(duckdb_col.columns)\n         print(converted_col.columns)\n-        pd.testing.assert_frame_equal(converted_col, duckdb_col)\n+        pandas.testing.assert_frame_equal(converted_col, duckdb_col)\n \n-    def test_map_duplicate(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_duplicate(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{'key': ['a', 'a', 'b'], 'value': [4, 0, 4]}]\n             ]\n@@ -226,8 +239,9 @@ def test_map_duplicate(self):\n         with pytest.raises(duckdb.InvalidInputException, match=\"Dict->Map conversion failed because 'key' list contains duplicates\"):\n             converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n \n-    def test_map_nullkey(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_nullkey(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{'key': [None, 'a', 'b'], 'value': [4, 0, 4]}]\n             ]\n@@ -235,8 +249,9 @@ def test_map_nullkey(self):\n         with pytest.raises(duckdb.InvalidInputException, match=\"Dict->Map conversion failed because 'key' list contains None\"):\n             converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n \n-    def test_map_nullkeylist(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_nullkeylist(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{'key': None, 'value': None}]\n             ]\n@@ -244,10 +259,11 @@ def test_map_nullkeylist(self):\n         # Isn't actually converted to MAP because isinstance(None, list) != True\n         converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n         duckdb_col = duckdb.query(\"SELECT {key: NULL, value: NULL} as '0'\").df()\n-        pd.testing.assert_frame_equal(duckdb_col, converted_col)\n+        pandas.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_map_fallback_nullkey(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_fallback_nullkey(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{'a': 4, None: 0, 'c': 4}],\n                 [{'a': 4, None: 0, 'd': 4}]\n@@ -256,8 +272,9 @@ def test_map_fallback_nullkey(self):\n         with pytest.raises(duckdb.InvalidInputException, match=\"Dict->Map conversion failed because 'key' list contains None\"):\n             converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n \n-    def test_map_fallback_nullkey_coverage(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_map_fallback_nullkey_coverage(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{'key': None, 'value': None}],\n                 [{'key': None, None: 5}],\n@@ -266,8 +283,9 @@ def test_map_fallback_nullkey_coverage(self):\n         with pytest.raises(duckdb.InvalidInputException, match=\"Dict->Map conversion failed because 'key' list contains None\"):\n             converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n \n-    def test_struct_key_conversion(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_struct_key_conversion(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [{\n                     IntString(5) :      1,\n@@ -280,10 +298,11 @@ def test_struct_key_conversion(self):\n         duckdb_col = duckdb.query(\"select {'5':1, '-25':3, '32':3, '32456':7} as '0'\").df()\n         converted_col = duckdb.query_df(x, \"tbl\", \"select * from tbl\").df()\n         duckdb.query(\"drop view if exists tbl\")\n-        pd.testing.assert_frame_equal(duckdb_col, converted_col)\n+        pandas.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_correct(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_list_correct(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 {'0': [[5], [34], [-245]]}\n             ]\n@@ -291,10 +310,11 @@ def test_list_correct(self):\n         duckdb_col = duckdb.query(\"select [[5], [34], [-245]] as '0'\").df()\n         converted_col = duckdb.query_df(x, \"tbl\", \"select * from tbl\").df()\n         duckdb.query(\"drop view if exists tbl\")\n-        pd.testing.assert_frame_equal(duckdb_col, converted_col)\n+        pandas.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_contains_null(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_list_contains_null(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 {'0': [[5], None, [-245]]}\n             ]\n@@ -302,10 +322,11 @@ def test_list_contains_null(self):\n         duckdb_col = duckdb.query(\"select [[5], NULL, [-245]] as '0'\").df()\n         converted_col = duckdb.query_df(x, \"tbl\", \"select * from tbl\").df()\n         duckdb.query(\"drop view if exists tbl\")\n-        pd.testing.assert_frame_equal(duckdb_col, converted_col)\n+        pandas.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_starts_with_null(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_list_starts_with_null(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 {'0': [None, [5], [-245]]}\n             ]\n@@ -313,10 +334,11 @@ def test_list_starts_with_null(self):\n         duckdb_col = duckdb.query(\"select [NULL, [5], [-245]] as '0'\").df()\n         converted_col = duckdb.query_df(x, \"tbl\", \"select * from tbl\").df()\n         duckdb.query(\"drop view if exists tbl\")\n-        pd.testing.assert_frame_equal(duckdb_col, converted_col)\n+        pandas.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_value_upgrade(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_list_value_upgrade(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 {'0': [['5'], [34], [-245]]}\n             ]\n@@ -324,10 +346,12 @@ def test_list_value_upgrade(self):\n         duckdb_col = duckdb.query(\"select [['5'], ['34'], ['-245']] as '0'\").df()\n         converted_col = duckdb.query_df(x, \"tbl\", \"select * from tbl\").df()\n         duckdb.query(\"drop view if exists tbl\")\n-        pd.testing.assert_frame_equal(duckdb_col, converted_col)\n+        pandas.testing.assert_frame_equal(duckdb_col, converted_col)\n \n-    def test_list_column_value_upgrade(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_list_column_value_upgrade(self, pandas):\n+        con = duckdb.connect()\n+        x = pandas.DataFrame(\n             [\n                 [ [1, 25, 300] ],\n                 [ [500, 345, 30] ],\n@@ -335,52 +359,60 @@ def test_list_column_value_upgrade(self):\n             ]\n         )\n         x.rename(columns = {0 : 'a'}, inplace = True)\n-        converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n-        duckdb.query(\"\"\"\n+        converted_col = duckdb.query_df(x, \"x\", \"select * from x\", connection=con).df()\n+        con.query(\"\"\"\n             CREATE TABLE tmp3(\n                 a VARCHAR[]\n             );\n         \"\"\")\n-        duckdb.query(\"\"\"\n+        con.query(\"\"\"\n             INSERT INTO tmp3 VALUES (['1', '25', '300'])\n         \"\"\")\n-        duckdb.query(\"\"\"\n+        con.query(\"\"\"\n             INSERT INTO tmp3 VALUES (['500', '345', '30'])\n         \"\"\")\n-        duckdb.query(\"\"\"\n+        con.query(\"\"\"\n             INSERT INTO tmp3 VALUES (['50', 'a', '67'])\n         \"\"\")\n-        duckdb_col = duckdb.query(\"select a from tmp3 AS '0'\").df()\n+        duckdb_col = con.query(\"select a from tmp3 AS '0'\").df()\n         print(duckdb_col.columns)\n         print(converted_col.columns)\n-        pd.testing.assert_frame_equal(converted_col, duckdb_col)\n+        pandas.testing.assert_frame_equal(converted_col, duckdb_col)\n \n-    def test_ubigint_object_conversion(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_ubigint_object_conversion(self, pandas):\n         # UBIGINT + TINYINT would result in HUGEINT, but conversion to HUGEINT is not supported yet from pandas->duckdb\n         # So this instead becomes a DOUBLE\n         data = [18446744073709551615, 0]\n-        x = pd.DataFrame({'0': pd.Series(data=data, dtype='object')})\n+        x = pandas.DataFrame({'0': pandas.Series(data=data, dtype='object')})\n         converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n-        float64 = np.dtype('float64')\n-        assert isinstance(converted_col['0'].dtype, float64.__class__) == True\n-\n-    def test_double_object_conversion(self):\n+        if pandas.backend == 'numpy_nullable':\n+            float64 = np.dtype('float64')\n+            assert isinstance(converted_col['0'].dtype, float64.__class__) == True\n+        else:\n+            uint64 = np.dtype('uint64')\n+            assert isinstance(converted_col['0'].dtype, uint64.__class__) == True\n+\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_double_object_conversion(self, pandas):\n         data = [18446744073709551616, 0]\n-        x = pd.DataFrame({'0': pd.Series(data=data, dtype='object')})\n+        x = pandas.DataFrame({'0': pandas.Series(data=data, dtype='object')})\n         converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n         double_dtype = np.dtype('float64')\n         assert isinstance(converted_col['0'].dtype, double_dtype.__class__) == True\n \n-    def test_numpy_stringliterals(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numpy_stringliterals(self, pandas):\n         con = duckdb.connect()\n-        df = pd.DataFrame({\"x\": list(map(np.str_, range(3)))})\n+        df = pandas.DataFrame({\"x\": list(map(np.str_, range(3)))})\n \n         res = con.execute(\"select * from df\").fetchall()\n         assert res == [('0',), ('1',), ('2',)]\n \n-    def test_integer_conversion_fail(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_integer_conversion_fail(self, pandas):\n         data = [2**10000, 0]\n-        x = pd.DataFrame({'0': pd.Series(data=data, dtype='object')})\n+        x = pandas.DataFrame({'0': pandas.Series(data=data, dtype='object')})\n         converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n         print(converted_col['0'])\n         double_dtype = np.dtype('object')\n@@ -388,8 +420,9 @@ def test_integer_conversion_fail(self):\n \n     # Most of the time numpy.datetime64 is just a wrapper around a datetime.datetime object\n     # But to support arbitrary precision, it can fall back to using an `int` internally\n-    # Which we don't support yet\n-    def test_numpy_datetime(self):\n+    \n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])# Which we don't support yet\n+    def test_numpy_datetime(self, pandas):\n         numpy = pytest.importorskip(\"numpy\")\n \n         data = []\n@@ -397,20 +430,22 @@ def test_numpy_datetime(self):\n         data += [numpy.datetime64('2022-02-21T06:59:23.324812')] * standard_vector_size\n         data += [numpy.datetime64('1974-06-05T13:12:01.000000')] * standard_vector_size\n         data += [numpy.datetime64('2049-01-13T00:24:31.999999')] * standard_vector_size\n-        x = pd.DataFrame({'dates': pd.Series(data=data, dtype='object')})\n+        x = pandas.DataFrame({'dates': pandas.Series(data=data, dtype='object')})\n         res = duckdb.query_df(x, \"x\", \"select distinct * from x\").df()\n         assert(len(res['dates'].__array__()) == 4)\n \n-    def test_numpy_datetime_int_internally(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_numpy_datetime_int_internally(self, pandas):\n         numpy = pytest.importorskip(\"numpy\")\n \n         data = [numpy.datetime64('2022-12-10T21:38:24.0000000000001')]\n-        x = pd.DataFrame({'dates': pd.Series(data=data, dtype='object')})\n+        x = pandas.DataFrame({'dates': pandas.Series(data=data, dtype='object')})\n         with pytest.raises(duckdb.ConversionException, match=re.escape(\"Conversion Error: Unimplemented type for cast (BIGINT -> TIMESTAMP)\")):\n             rel = duckdb.query_df(x, \"x\", \"create table dates as select dates::TIMESTAMP WITHOUT TIME ZONE from x\")\n \n-    def test_fallthrough_object_conversion(self):\n-        x = pd.DataFrame(\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_fallthrough_object_conversion(self, pandas):\n+        x = pandas.DataFrame(\n             [\n                 [IntString(4)],\n                 [IntString(2)],\n@@ -418,10 +453,11 @@ def test_fallthrough_object_conversion(self):\n             ]\n         )\n         duckdb_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n-        df_expected_res = pd.DataFrame({'0': pd.Series(['4','2','0'])})\n-        pd.testing.assert_frame_equal(duckdb_col, df_expected_res)\n+        df_expected_res = pandas.DataFrame({'0': pandas.Series(['4','2','0'])})\n+        pandas.testing.assert_frame_equal(duckdb_col, df_expected_res)\n \n-    def test_numeric_decimal(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numeric_decimal(self, pandas):\n         duckdb_conn = duckdb.connect()\n \n         # DuckDB uses DECIMAL where possible, so all the 'float' types here are actually DECIMAL\n@@ -438,20 +474,21 @@ def test_numeric_decimal(self):\n         \"\"\"\n         duckdb_conn.execute(reference_query)\n         # Because of this we need to wrap these native floats as DECIMAL for this test, to avoid these decimals being \"upgraded\" to DOUBLE\n-        x = pd.DataFrame({\n-            '0': ConvertStringToDecimal([5, '12.0', '-123.0', '-234234.0', None, '1.234']),\n-            '1': ConvertStringToDecimal([5002340, 13, '-12.0000000005', '7453324234.0', None, '-324234234']),\n-            '2': ConvertStringToDecimal(['-234234234234.0',  '324234234.00000005', -128, 345345, '1E5', '1324234359'])\n+        x = pandas.DataFrame({\n+            '0': ConvertStringToDecimal([5, '12.0', '-123.0', '-234234.0', None, '1.234'], pandas),\n+            '1': ConvertStringToDecimal([5002340, 13, '-12.0000000005', '7453324234.0', None, '-324234234'], pandas),\n+            '2': ConvertStringToDecimal(['-234234234234.0',  '324234234.00000005', -128, 345345, '1E5', '1324234359'], pandas)\n         })\n         reference = duckdb.query(\"select * from tbl\", connection=duckdb_conn).fetchall()\n         conversion = duckdb.query_df(x, \"x\", \"select * from x\").fetchall()\n \n         assert(conversion == reference)\n \n-    def test_numeric_decimal_coverage(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numeric_decimal_coverage(self, pandas):\n         duckdb_conn = duckdb.connect()\n \n-        x = pd.DataFrame({\n+        x = pandas.DataFrame({\n             '0': [Decimal(\"nan\"), Decimal(\"+nan\"), Decimal(\"-nan\"), Decimal(\"inf\"), Decimal(\"+inf\"), Decimal(\"-inf\")]\n         })\n         conversion = duckdb.query_df(x, \"x\", \"select * from x\").fetchall()\n@@ -467,24 +504,26 @@ def test_numeric_decimal_coverage(self):\n         assert(str(conversion) == '[(nan,), (nan,), (nan,), (inf,), (inf,), (inf,)]')\n \n     # Test that the column 'offset' is actually used when converting,\n-    # and that the same 2048 (STANDARD_VECTOR_SIZE) values are not being scanned over and over again\n-    def test_multiple_chunks(self):\n+    \n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])# and that the same 2048 (STANDARD_VECTOR_SIZE) values are not being scanned over and over again\n+    def test_multiple_chunks(self, pandas):\n         data = []\n         data += [datetime.date(2022, 9, 13) for x in range(standard_vector_size)]\n         data += [datetime.date(2022, 9, 14) for x in range(standard_vector_size)]\n         data += [datetime.date(2022, 9, 15) for x in range(standard_vector_size)]\n         data += [datetime.date(2022, 9, 16) for x in range(standard_vector_size)]\n-        x = pd.DataFrame({'dates': pd.Series(data=data, dtype='object')})\n+        x = pandas.DataFrame({'dates': pandas.Series(data=data, dtype='object')})\n         res = duckdb.query_df(x, \"x\", \"select distinct * from x\").df()\n         assert(len(res['dates'].__array__()) == 4)\n \n-    def test_multiple_chunks_aggregate(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_multiple_chunks_aggregate(self, pandas):\n         conn = duckdb.connect()\n         conn.execute(\"create table dates as select '2022-09-14'::DATE + INTERVAL (i::INTEGER) DAY as i from range(0, 4096) tbl(i);\")\n         res = duckdb.query(\"select * from dates\", connection=conn).df()\n         date_df = res.copy()\n         # Convert the values to `datetime.date` values, and the dtype of the column to 'object'\n-        date_df['i'] = pd.to_datetime(res['i']).dt.date\n+        date_df['i'] = pandas.to_datetime(res['i']).dt.date\n         assert(str(date_df['i'].dtype) == 'object')\n         expected_res = duckdb.query('select avg(epoch(i)), min(epoch(i)), max(epoch(i)) from dates;', connection=conn).fetchall()\n         actual_res = duckdb.query_df(date_df, 'x', 'select avg(epoch(i)), min(epoch(i)), max(epoch(i)) from x').fetchall()\n@@ -502,30 +541,33 @@ def test_multiple_chunks_aggregate(self):\n \n         date_df = res.copy()\n         # Convert the values to `datetime.date` values, and the dtype of the column to 'object'\n-        date_df['i'] = pd.to_datetime(res['i']).dt.date\n+        date_df['i'] = pandas.to_datetime(res['i']).dt.date\n         assert(str(date_df['i'].dtype) == 'object')\n         actual_res = duckdb.query_df(date_df, 'x', 'select avg(epoch(i)), min(epoch(i)), max(epoch(i)) from x').fetchall()\n         assert(expected_res == actual_res)\n \n-    def test_mixed_object_types(self):\n-        x = pd.DataFrame({\n-            'nested': pd.Series(data=[{'a': 1, 'b': 2}, [5, 4, 3], {'key': [1,2,3], 'value': ['a', 'b', 'c']}], dtype='object'),\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_mixed_object_types(self, pandas):\n+        x = pandas.DataFrame({\n+            'nested': pandas.Series(data=[{'a': 1, 'b': 2}, [5, 4, 3], {'key': [1,2,3], 'value': ['a', 'b', 'c']}], dtype='object'),\n         })\n         res = duckdb.query_df(x, \"x\", \"select * from x\").df()\n         assert(res['nested'].dtype == np.dtype('object'))\n \n \n-    def test_analyze_sample_too_small(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_analyze_sample_too_small(self, pandas):\n         data = [1 for _ in range(9)] + [[1,2,3]] + [1 for _ in range(9991)]\n-        x = pd.DataFrame({\n-            'a': pd.Series(data=data)\n+        x = pandas.DataFrame({\n+            'a': pandas.Series(data=data)\n         })\n         with pytest.raises(duckdb.InvalidInputException, match=\"Failed to cast value: Unimplemented type for cast\"):\n             res = duckdb.query_df(x, \"x\", \"select * from x\").df()\n \n-    def test_numeric_decimal_zero_fractional(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numeric_decimal_zero_fractional(self, pandas):\n         duckdb_conn = duckdb.connect()\n-        decimals = pd.DataFrame(\n+        decimals = pandas.DataFrame(\n             data={\n                 \"0\": [\n                     Decimal(\"0.00\"),\n@@ -556,7 +598,8 @@ def test_numeric_decimal_zero_fractional(self):\n \n         assert(conversion == reference)\n \n-    def test_numeric_decimal_incompatible(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numeric_decimal_incompatible(self, pandas):\n         duckdb_conn = duckdb.connect()\n         reference_query = \"\"\"\n             CREATE TABLE tbl AS SELECT * FROM (\n@@ -570,10 +613,10 @@ def test_numeric_decimal_incompatible(self):\n             ) tbl(a, b, c);\n         \"\"\"\n         duckdb_conn.execute(reference_query)\n-        x = pd.DataFrame({\n-            '0': ConvertStringToDecimal(['5', '12.0', '-123.0', '-234234.0', None, '1.234']),\n-            '1': ConvertStringToDecimal([5002340, 13, '-12.0000000005', 7453324234, None, '-324234234']),\n-            '2': ConvertStringToDecimal([-234234234234,  '324234234.00000005', -128, 345345, 0, '1324234359'])\n+        x = pandas.DataFrame({\n+            '0': ConvertStringToDecimal(['5', '12.0', '-123.0', '-234234.0', None, '1.234'], pandas),\n+            '1': ConvertStringToDecimal([5002340, 13, '-12.0000000005', 7453324234, None, '-324234234'], pandas),\n+            '2': ConvertStringToDecimal([-234234234234,  '324234234.00000005', -128, 345345, 0, '1324234359'], pandas)\n         })\n         reference = duckdb.query(\"select * from tbl\", connection=duckdb_conn).fetchall()\n         conversion = duckdb.query_df(x, \"x\", \"select * from x\").fetchall()\n@@ -582,10 +625,11 @@ def test_numeric_decimal_incompatible(self):\n         print(reference)\n         print(conversion)\n \n-    #result: [('1E-28',), ('10000000000000000000000000.0',)]\n-    def test_numeric_decimal_combined(self):\n+    \n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])#result: [('1E-28',), ('10000000000000000000000000.0',)]\n+    def test_numeric_decimal_combined(self, pandas):\n         duckdb_conn = duckdb.connect()\n-        decimals = pd.DataFrame(\n+        decimals = pandas.DataFrame(\n             data={\n                 \"0\": [\n                     Decimal(\"0.0000000000000000000000000001\"),\n@@ -608,9 +652,10 @@ def test_numeric_decimal_combined(self):\n         print(conversion)\n \n     #result: [('1234.0',), ('123456789.0',), ('1234567890123456789.0',), ('0.1234567890123456789',)]\n-    def test_numeric_decimal_varying_sizes(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numeric_decimal_varying_sizes(self, pandas):\n         duckdb_conn = duckdb.connect()\n-        decimals = pd.DataFrame(\n+        decimals = pandas.DataFrame(\n             data={\n                 \"0\": [\n                     Decimal(\"1234.0\"),\n@@ -636,11 +681,12 @@ def test_numeric_decimal_varying_sizes(self):\n         print(reference)\n         print(conversion)\n \n-    def test_numeric_decimal_fallback_to_double(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numeric_decimal_fallback_to_double(self, pandas):\n         duckdb_conn = duckdb.connect()\n         # The widths of these decimal values are bigger than the max supported width for DECIMAL\n         data = [Decimal(\"1.234567890123456789012345678901234567890123456789\"), Decimal(\"123456789012345678901234567890123456789012345678.0\")]\n-        decimals = pd.DataFrame(\n+        decimals = pandas.DataFrame(\n             data={\n                 \"0\": data\n             }\n@@ -658,7 +704,8 @@ def test_numeric_decimal_fallback_to_double(self):\n         assert(conversion == reference)\n         assert(isinstance(conversion[0][0], float))\n \n-    def test_numeric_decimal_double_mixed(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numeric_decimal_double_mixed(self, pandas):\n         duckdb_conn = duckdb.connect()\n         data = [\n             Decimal(\"1.234\"),\n@@ -670,7 +717,7 @@ def test_numeric_decimal_double_mixed(self):\n             Decimal(\"1232354.000000000000000000000000000035\"),\n             Decimal(\"123.5e300\")\n         ]\n-        decimals = pd.DataFrame(\n+        decimals = pandas.DataFrame(\n             data={\n                 \"0\": data\n             }\n@@ -694,10 +741,11 @@ def test_numeric_decimal_double_mixed(self):\n         assert(conversion == reference)\n         assert(isinstance(conversion[0][0], float))\n \n-    def test_numeric_decimal_out_of_range(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_numeric_decimal_out_of_range(self, pandas):\n         duckdb_conn = duckdb.connect()\n         data = [Decimal(\"1.234567890123456789012345678901234567\"), Decimal(\"123456789012345678901234567890123456.0\")]\n-        decimals = pd.DataFrame(\n+        decimals = pandas.DataFrame(\n             data={\n                 \"0\": data\n             }\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_df_recursive_nested.py b/tools/pythonpkg/tests/fast/pandas/test_df_recursive_nested.py\nindex 95e649b7d4ce..92c1a390a537 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_df_recursive_nested.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_df_recursive_nested.py\n@@ -1,9 +1,9 @@\n-import pandas as pd\n import duckdb\n import datetime\n import numpy as np\n import pytest\n import copy\n+from conftest import NumpyPandas, ArrowPandas\n \n NULL = None\n \n@@ -20,7 +20,8 @@ def create_reference_query(data):\n \treturn query\n \n class TestDFRecursiveNested(object):\n-\tdef test_list_of_structs(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_list_of_structs(self, duckdb_cursor, pandas):\n \t\tdata = [\n \t\t\t\t\t[\n \t\t\t\t\t\t{'a': 5},\n@@ -35,12 +36,13 @@ def test_list_of_structs(self, duckdb_cursor):\n \t\t\t\t\t]\n \t\t]\n \t\treference_query = create_reference_query(data)\n-\t\tdf = pd.DataFrame([\n+\t\tdf = pandas.DataFrame([\n \t\t\t{ 'a': data}\n \t\t])\n \t\tcheck_equal(df, reference_query)\n \n-\tdef test_list_of_map(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_list_of_map(self, duckdb_cursor, pandas):\n \t\t# LIST(MAP(VARCHAR, VARCHAR))\n \t\tdata = [\n \t\t\t\t\t[\n@@ -56,12 +58,13 @@ def test_list_of_map(self, duckdb_cursor):\n \t\t\t\t\t]\n \t\t]\n \t\treference_query = create_reference_query(data)\n-\t\tdf = pd.DataFrame([\n+\t\tdf = pandas.DataFrame([\n \t\t\t{ 'a': data}\n \t\t])\n \t\tcheck_equal(df, reference_query)\n \n-\tdef test_recursive_list(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_recursive_list(self, duckdb_cursor, pandas):\n \t\t# LIST(LIST(LIST(LIST(INTEGER))))\n \t\tdata = [\n \t\t\t\t\t[\n@@ -96,12 +99,13 @@ def test_recursive_list(self, duckdb_cursor):\n \t\t\t\t\t]\n \t\t]\n \t\treference_query = create_reference_query(data)\n-\t\tdf = pd.DataFrame([\n+\t\tdf = pandas.DataFrame([\n \t\t\t{ 'a': data}\n \t\t])\n \t\tcheck_equal(df, reference_query)\n \n-\tdef test_recursive_struct(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_recursive_struct(self, duckdb_cursor, pandas):\n \t\t#STRUCT(STRUCT(STRUCT(LIST)))\n \t\tdata = {\n \t\t\t'A': {\n@@ -128,12 +132,13 @@ def test_recursive_struct(self, duckdb_cursor):\n \t\t\t}\n \t\t}\n \t\treference_query = create_reference_query(data)\n-\t\tdf = pd.DataFrame([\n+\t\tdf = pandas.DataFrame([\n \t\t\t{ 'a': data}\n \t\t])\n \t\tcheck_equal(df, reference_query)\n \n-\tdef test_recursive_map(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_recursive_map(self, duckdb_cursor, pandas):\n \t\t#MAP(\n \t\t#\tMAP(\n \t\t#\t\tINTEGER,\n@@ -182,12 +187,13 @@ def test_recursive_map(self, duckdb_cursor):\n \t\t\t]\n \t\t}\n \t\treference_query = create_reference_query(data)\n-\t\tdf = pd.DataFrame([\n+\t\tdf = pandas.DataFrame([\n \t\t\t{ 'a': data}\n \t\t])\n \t\tcheck_equal(df, reference_query)\n \n-\tdef test_recursive_stresstest(self, duckdb_cursor):\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_recursive_stresstest(self, duckdb_cursor, pandas):\n \t\t#LIST(\n \t\t#\tSTRUCT(\n \t\t#\t\tMAP(\n@@ -240,7 +246,7 @@ def test_recursive_stresstest(self, duckdb_cursor):\n \t\t\t}\n \t\t]\n \t\treference_query = create_reference_query(data)\n-\t\tdf = pd.DataFrame([\n+\t\tdf = pandas.DataFrame([\n \t\t\t{ 'a': data}\n \t\t])\n \t\tcheck_equal(df, reference_query)\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_implicit_pandas_scan.py b/tools/pythonpkg/tests/fast/pandas/test_implicit_pandas_scan.py\nindex 6a21b1bd47ae..0b41f9a17bd1 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_implicit_pandas_scan.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_implicit_pandas_scan.py\n@@ -1,13 +1,28 @@\n # simple DB API testcase\n \n import duckdb\n-import numpy\n-import pandas\n+import pandas as pd\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n+from packaging.version import Version\n \n-global_df = pandas.DataFrame([{\"COL1\": \"val1\", \"CoL2\": 1.05},{\"COL1\": \"val4\", \"CoL2\": 17}])\n+numpy_nullable_df = pd.DataFrame([{\"COL1\": \"val1\", \"CoL2\": 1.05},{\"COL1\": \"val4\", \"CoL2\": 17}])\n+\n+try:\n+    from pandas.compat import pa_version_under7p0\n+    pyarrow_dtypes_enabled = not pa_version_under7p0\n+except:\n+    pyarrow_dtypes_enabled = False\n+\n+if Version(pd.__version__) >= Version('2.0.0') and pyarrow_dtypes_enabled:\n+    pyarrow_df = numpy_nullable_df.convert_dtypes(dtype_backend=\"pyarrow\")\n+else:\n+    # dtype_backend is not supported in pandas < 2.0.0\n+    pyarrow_df = numpy_nullable_df\n \n class TestImplicitPandasScan(object):\n-    def test_local_pandas_scan(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_local_pandas_scan(self, duckdb_cursor, pandas):\n         con = duckdb.connect()\n         df = pandas.DataFrame([{\"COL1\": \"val1\", \"CoL2\": 1.05},{\"COL1\": \"val3\", \"CoL2\": 17}])\n         r1 = con.execute('select * from df').fetchdf()\n@@ -16,11 +31,11 @@ def test_local_pandas_scan(self, duckdb_cursor):\n         assert r1[\"CoL2\"][0] == 1.05\n         assert r1[\"CoL2\"][1] == 17\n \n-    def test_global_pandas_scan(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_global_pandas_scan(self, duckdb_cursor, pandas):\n         con = duckdb.connect()\n-        r1 = con.execute('select * from global_df').fetchdf()\n+        r1 = con.execute(f'select * from {pandas.backend}_df').fetchdf()\n         assert r1[\"COL1\"][0] == \"val1\"\n         assert r1[\"COL1\"][1] == \"val4\"\n         assert r1[\"CoL2\"][0] == 1.05\n         assert r1[\"CoL2\"][1] == 17\n-\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_issue_1767.py b/tools/pythonpkg/tests/fast/pandas/test_issue_1767.py\nindex ca969b75505b..828510693299 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_issue_1767.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_issue_1767.py\n@@ -2,21 +2,22 @@\n # -*- coding: utf-8 -*-\n \n import duckdb\n-import pandas as pd\n import numpy\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n # Join from pandas not matching identical strings #1767\n class TestIssue1767(object):\n-    def test_unicode_join_pandas(self, duckdb_cursor):\n-        A = pd.DataFrame({\"key\": [\"a\", \"\u043f\"]})\n-        B = pd.DataFrame({\"key\": [\"a\", \"\u043f\"]})\n+    \n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_unicode_join_pandas(self, duckdb_cursor, pandas):\n+        A = pandas.DataFrame({\"key\": [\"a\", \"\u043f\"]})\n+        B = pandas.DataFrame({\"key\": [\"a\", \"\u043f\"]})\n         con = duckdb.connect(\":memory:\")\n         arrow = con.register(\"A\", A).register(\"B\", B)\n         q = arrow.query(\"\"\"SELECT key FROM \"A\" FULL JOIN \"B\" USING (\"key\") ORDER BY key\"\"\")\n         result = q.df()\n \n         d = {'key': [\"a\", \"\u043f\"]}\n-        df = pd.DataFrame(data=d)\n-        assert (result.equals(df))\n-\n-\n+        df = pandas.DataFrame(data=d)\n+        pandas.testing.assert_frame_equal(result, df)\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_limit.py b/tools/pythonpkg/tests/fast/pandas/test_limit.py\nindex ef4c79ee9587..aa7cd4da2314 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_limit.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_limit.py\n@@ -1,15 +1,17 @@\n import duckdb\n-import pandas as pd\n-\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestLimitPandas(object):\n \n-    def test_limit_df(self, duckdb_cursor):\n-        df_in = pd.DataFrame({'numbers': [1,2,3,4,5],})\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_limit_df(self, duckdb_cursor, pandas):\n+        df_in = pandas.DataFrame({'numbers': [1,2,3,4,5],})\n         limit_df = duckdb.limit(df_in,2)\n         assert len(limit_df.execute().fetchall()) == 2\n \n-    def test_aggregate_df(self, duckdb_cursor):\n-        df_in = pd.DataFrame({'numbers': [1,2,2,2],})\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_aggregate_df(self, duckdb_cursor, pandas):\n+        df_in = pandas.DataFrame({'numbers': [1,2,2,2],})\n         aggregate_df = duckdb.aggregate(df_in,'count(numbers)','numbers')\n         assert aggregate_df.execute().fetchall() == [(1,), (3,)]\n\\ No newline at end of file\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_pandas_arrow.py b/tools/pythonpkg/tests/fast/pandas/test_pandas_arrow.py\nnew file mode 100644\nindex 000000000000..e88f8d0d8500\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/pandas/test_pandas_arrow.py\n@@ -0,0 +1,146 @@\n+import duckdb\n+import pytest\n+import datetime\n+\n+from conftest import pandas_supports_arrow_backend\n+pd = pytest.importorskip(\"pandas\", '2.0.0')\n+import numpy as np\n+\n+@pytest.mark.skipif(not pandas_supports_arrow_backend(), reason=\"pandas does not support the 'pyarrow' backend\")\n+class TestPandasArrow(object):\n+    def test_pandas_arrow(self, duckdb_cursor):\n+        pd = pytest.importorskip(\"pandas\")\n+        df = pd.DataFrame({'a': pd.Series([5,4,3])}).convert_dtypes()\n+        con = duckdb.connect()\n+        res = con.sql(\"select * from df\").fetchall()\n+        assert res == [(5,),(4,),(3,)]\n+\n+    def test_mixed_columns(self):\n+        df = pd.DataFrame({\n+            'strings': pd.Series([\n+                'abc',\n+                'DuckDB',\n+                'quack',\n+                'quack'\n+            ]),\n+            'timestamps': pd.Series([\n+                datetime.datetime(1990, 10, 21),\n+                datetime.datetime(2023, 1, 11),\n+                datetime.datetime(2001, 2, 5),\n+                datetime.datetime(1990, 10, 21),\n+            ]),\n+            'objects': pd.Series([\n+                [5,4,3],\n+                'test',\n+                None,\n+                {'a': 42}\n+            ]),\n+            'integers': np.ndarray((4,), buffer=np.array([1,2,3,4,5]), offset=np.int_().itemsize, dtype=int)\n+        })\n+        pyarrow_df = df.convert_dtypes(dtype_backend='pyarrow')\n+        con = duckdb.connect()\n+        with pytest.raises(duckdb.InvalidInputException, match='Conversion failed for column objects with type object'):\n+            res = con.sql('select * from pyarrow_df').fetchall()\n+\n+        numpy_df = pd.DataFrame({'a': np.ndarray((2,), buffer=np.array([1,2,3]), offset=np.int_().itemsize, dtype=int)}).convert_dtypes(dtype_backend='numpy_nullable')\n+        arrow_df = pd.DataFrame({'a': pd.Series([\n+                datetime.datetime(1990, 10, 21),\n+                datetime.datetime(2023, 1, 11),\n+                datetime.datetime(2001, 2, 5),\n+                datetime.datetime(1990, 10, 21),\n+            ])}).convert_dtypes(dtype_backend='pyarrow')\n+        python_df = pd.DataFrame({'a': pd.Series(['test', [5,4,3], {'a': 42}])}).convert_dtypes()\n+\n+        df = pd.concat([numpy_df['a'], arrow_df['a'], python_df['a']], axis=1, keys=['numpy', 'arrow', 'python'])\n+        assert isinstance(df.dtypes[0], pd.core.arrays.integer.IntegerDtype)\n+        assert isinstance(df.dtypes[1], pd.core.arrays.arrow.dtype.ArrowDtype)\n+        assert isinstance(df.dtypes[2], np.dtype('O').__class__)\n+\n+        with pytest.raises(duckdb.InvalidInputException, match='Conversion failed for column python with type object'):\n+            res = con.sql('select * from df').fetchall()\n+\n+    def test_empty_df(self):\n+        df = pd.DataFrame({\n+            'string' : pd.Series(data=[], dtype='string'),\n+            'object' : pd.Series(data=[], dtype='object'),\n+            'Int64' : pd.Series(data=[], dtype='Int64'),\n+            'Float64' : pd.Series(data=[], dtype='Float64'),\n+            'bool' : pd.Series(data=[], dtype='bool'),\n+            'datetime64[ns]' : pd.Series(data=[], dtype='datetime64[ns]'),\n+            'datetime64[ms]' : pd.Series(data=[], dtype='datetime64[ms]'),\n+            'datetime64[us]' : pd.Series(data=[], dtype='datetime64[us]'),\n+            'datetime64[s]' : pd.Series(data=[], dtype='datetime64[s]'),\n+            'category' : pd.Series(data=[], dtype='category'),\n+            'timedelta64[ns]' : pd.Series(data=[], dtype='timedelta64[ns]'),\n+        })\n+        pyarrow_df = df.convert_dtypes(dtype_backend='pyarrow')\n+\n+        con = duckdb.connect()\n+        res = con.sql('select * from pyarrow_df').fetchall()\n+        assert res == []\n+    \n+    def test_completely_null_df(self):\n+        df = pd.DataFrame({\n+            'a' : pd.Series(data=[\n+                None,\n+                np.nan,\n+                pd.NA,\n+            ])\n+        })\n+        pyarrow_df = df.convert_dtypes(dtype_backend='pyarrow')\n+\n+        con = duckdb.connect()\n+        res = con.sql('select * from pyarrow_df').fetchall()\n+        assert res == [(None,), (None,), (None,)]\n+\n+    def test_mixed_nulls(self):\n+        df = pd.DataFrame({\n+            'float': pd.Series(data=[\n+                4.123123,\n+                None,\n+                7.23456\n+            ], dtype='Float64'),\n+            'int64': pd.Series(data=[\n+                -234234124,\n+                709329413,\n+                pd.NA\n+            ], dtype='Int64'),\n+            'bool': pd.Series(data=[\n+                np.nan,\n+                True,\n+                False\n+            ], dtype='boolean'),\n+            'string': pd.Series(data=[\n+                'NULL',\n+                None,\n+                'quack'\n+            ]),\n+            'list[str]': pd.Series(data=[\n+                [\n+                    'Huey',\n+                    'Dewey',\n+                    'Louie'\n+                ],\n+                [\n+                    None,\n+                    pd.NA,\n+                    np.nan,\n+                    'DuckDB'\n+                ],\n+                None\n+            ]),\n+            'datetime64' : pd.Series(data=[\n+                datetime.datetime(2011, 8, 16, 22, 7, 8),\n+                None,\n+                datetime.datetime(2010, 4, 26, 18, 14, 14)\n+            ]),\n+            'date' : pd.Series(data=[\n+                datetime.date(2008, 5, 28),\n+                datetime.date(2013, 7, 14),\n+                None\n+            ]),\n+        })\n+        pyarrow_df = df.convert_dtypes(dtype_backend='pyarrow')\n+        con = duckdb.connect()\n+        res = con.sql('select * from pyarrow_df').fetchone()\n+        assert res == (4.123123, -234234124, None, 'NULL', ['Huey', 'Dewey', 'Louie'], datetime.datetime(2011, 8, 16, 22, 7, 8), datetime.date(2008, 5, 28))\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_pandas_types.py b/tools/pythonpkg/tests/fast/pandas/test_pandas_types.py\nindex 631399b1d307..5b9819d3661c 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_pandas_types.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_pandas_types.py\n@@ -14,7 +14,7 @@ def round_trip(data,pandas_type):\n     print (df_in)\n     assert df_out.equals(df_in)\n \n-class TestPandasTypes(object):\n+class TestNumpyNullableTypes(object):\n     def test_pandas_numeric(self):\n         base_df = pd.DataFrame(\n             {'a':range(10)}\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_pandas_unregister.py b/tools/pythonpkg/tests/fast/pandas/test_pandas_unregister.py\nindex 239cb7366965..c0a9770351e4 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_pandas_unregister.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_pandas_unregister.py\n@@ -1,13 +1,15 @@\n import duckdb\n-import pandas as pd\n import pytest\n import tempfile\n import os\n import gc\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestPandasUnregister(object):\n-    def test_pandas_unregister1(self, duckdb_cursor):\n-        df = pd.DataFrame([[1, 2, 3], [4, 5, 6]])\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_pandas_unregister1(self, duckdb_cursor, pandas):\n+        df = pandas.DataFrame([[1, 2, 3], [4, 5, 6]])\n         connection = duckdb.connect(\":memory:\")\n         connection.register(\"dataframe\", df)\n \n@@ -19,14 +21,14 @@ def test_pandas_unregister1(self, duckdb_cursor):\n             connection.execute(\"DROP VIEW dataframe;\")\n         connection.execute(\"DROP VIEW IF EXISTS dataframe;\")\n \n-\n-    def test_pandas_unregister2(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_pandas_unregister2(self, duckdb_cursor, pandas):\n         fd, db = tempfile.mkstemp()\n         os.close(fd)\n         os.remove(db)\n \n         connection = duckdb.connect(db)\n-        df = pd.DataFrame([[1, 2, 3], [4, 5, 6]])\n+        df = pandas.DataFrame([[1, 2, 3], [4, 5, 6]])\n \n         connection.register(\"dataframe\", df)\n         connection.unregister(\"dataframe\")  # Attempting to unregister.\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_parallel_pandas_scan.py b/tools/pythonpkg/tests/fast/pandas/test_parallel_pandas_scan.py\nindex 26e4e918c3c6..3fa7fb954459 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_parallel_pandas_scan.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_parallel_pandas_scan.py\n@@ -1,11 +1,12 @@\n #!/usr/bin/env python\n # -*- coding: utf-8 -*-\n import duckdb\n-import pandas as pd\n import numpy\n import datetime\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n-def run_parallel_queries(main_table, left_join_table, expected_df, iteration_count = 5):\n+def run_parallel_queries(main_table, left_join_table, expected_df, pandas, iteration_count = 5):\n     for i in range(0, iteration_count):\n         output_df = None\n         sql = \"\"\"\n@@ -25,7 +26,7 @@ def run_parallel_queries(main_table, left_join_table, expected_df, iteration_cou\n             duckdb_conn.register('main_table', main_table)\n             duckdb_conn.register('left_join_table', left_join_table)\n             output_df = duckdb_conn.execute(sql).fetchdf()\n-            pd.testing.assert_frame_equal(expected_df, output_df)\n+            pandas.testing.assert_frame_equal(expected_df, output_df)\n             print(output_df)\n         except Exception as err:\n             print(err)\n@@ -33,45 +34,53 @@ def run_parallel_queries(main_table, left_join_table, expected_df, iteration_cou\n             duckdb_conn.close()\n \n class TestParallelPandasScan(object):\n-    def test_parallel_numeric_scan(self, duckdb_cursor):\n-        main_table = pd.DataFrame([{\"join_column\": 3}])\n-        left_join_table = pd.DataFrame([{\"join_column\": 3,\"other_column\": 4}])\n-        run_parallel_queries(main_table, left_join_table, left_join_table)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_parallel_numeric_scan(self, duckdb_cursor, pandas):\n+        main_table = pandas.DataFrame([{\"join_column\": 3}])\n+        left_join_table = pandas.DataFrame([{\"join_column\": 3,\"other_column\": 4}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table, pandas)\n \n-    def test_parallel_ascii_text(self, duckdb_cursor):\n-        main_table = pd.DataFrame([{\"join_column\":\"text\"}])\n-        left_join_table = pd.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\n-        run_parallel_queries(main_table, left_join_table, left_join_table)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_parallel_ascii_text(self, duckdb_cursor, pandas):\n+        main_table = pandas.DataFrame([{\"join_column\":\"text\"}])\n+        left_join_table = pandas.DataFrame([{\"join_column\":\"text\",\"other_column\":\"more text\"}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table, pandas)\n \n-    def test_parallel_unicode_text(self, duckdb_cursor):\n-        main_table = pd.DataFrame([{\"join_column\":u\"m\u00fchleisen\"}])\n-        left_join_table = pd.DataFrame([{\"join_column\": u\"m\u00fchleisen\",\"other_column\":u\"h\u00f6h\u00f6h\u00f6\"}])\n-        run_parallel_queries(main_table, left_join_table, left_join_table)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_parallel_unicode_text(self, duckdb_cursor, pandas):\n+        main_table = pandas.DataFrame([{\"join_column\":u\"m\u00fchleisen\"}])\n+        left_join_table = pandas.DataFrame([{\"join_column\": u\"m\u00fchleisen\",\"other_column\":u\"h\u00f6h\u00f6h\u00f6\"}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table, pandas)\n \n-    def test_parallel_complex_unicode_text(self, duckdb_cursor):\n-        main_table = pd.DataFrame([{\"join_column\":u\"\u9d28\"}])\n-        left_join_table = pd.DataFrame([{\"join_column\": u\"\u9d28\",\"other_column\":u\"\u6578\u64da\u5eab\"}])\n-        run_parallel_queries(main_table, left_join_table, left_join_table)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_parallel_complex_unicode_text(self, duckdb_cursor, pandas):\n+        main_table = pandas.DataFrame([{\"join_column\":u\"\u9d28\"}])\n+        left_join_table = pandas.DataFrame([{\"join_column\": u\"\u9d28\",\"other_column\":u\"\u6578\u64da\u5eab\"}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table, pandas)\n \n-    def test_parallel_emojis(self, duckdb_cursor):\n-        main_table = pd.DataFrame([{\"join_column\":u\"\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f L\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0fR \ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f\"}])\n-        left_join_table = pd.DataFrame([{\"join_column\": u\"\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f L\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0fR \ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f\",\"other_column\":u\"\ud83e\udd86\ud83c\udf5e\ud83e\udd86\"}])\n-        run_parallel_queries(main_table, left_join_table, left_join_table)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_parallel_emojis(self, duckdb_cursor, pandas):\n+        main_table = pandas.DataFrame([{\"join_column\":u\"\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f L\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0fR \ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f\"}])\n+        left_join_table = pandas.DataFrame([{\"join_column\": u\"\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f L\ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0fR \ud83e\udd26\ud83c\udffc\u200d\u2642\ufe0f\",\"other_column\":u\"\ud83e\udd86\ud83c\udf5e\ud83e\udd86\"}])\n+        run_parallel_queries(main_table, left_join_table, left_join_table, pandas)\n \n-    def test_parallel_numeric_object(self, duckdb_cursor):\n-        main_table = pd.DataFrame({ 'join_column': pd.Series([3], dtype=\"Int8\") })\n-        left_join_table = pd.DataFrame({ 'join_column': pd.Series([3], dtype=\"Int8\"), 'other_column': pd.Series([4], dtype=\"Int8\") })\n-        expected_df = pd.DataFrame({ \"join_column\": numpy.array([3], dtype=numpy.int8), \"other_column\": numpy.array([4], dtype=numpy.int8)})\n-        run_parallel_queries(main_table, left_join_table, expected_df)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_parallel_numeric_object(self, duckdb_cursor, pandas):\n+        main_table = pandas.DataFrame({ 'join_column': pandas.Series([3], dtype=\"Int8\") })\n+        left_join_table = pandas.DataFrame({ 'join_column': pandas.Series([3], dtype=\"Int8\"), 'other_column': pandas.Series([4], dtype=\"Int8\") })\n+        expected_df = pandas.DataFrame({ \"join_column\": numpy.array([3], dtype=numpy.int8), \"other_column\": numpy.array([4], dtype=numpy.int8)})\n+        run_parallel_queries(main_table, left_join_table, expected_df, pandas)\n \n-    def test_parallel_timestamp(self, duckdb_cursor):\n-        main_table = pd.DataFrame({ 'join_column': [pd.Timestamp('20180310T11:17:54Z')] })\n-        left_join_table = pd.DataFrame({ 'join_column': [pd.Timestamp('20180310T11:17:54Z')], 'other_column': [pd.Timestamp('20190310T11:17:54Z')] })\n-        expected_df = pd.DataFrame({ \"join_column\": numpy.array([datetime.datetime(2018, 3, 10, 11, 17, 54)], dtype='datetime64[ns]'), \"other_column\": numpy.array([datetime.datetime(2019, 3, 10, 11, 17, 54)], dtype='datetime64[ns]')})\n-        run_parallel_queries(main_table, left_join_table, expected_df)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_parallel_timestamp(self, duckdb_cursor, pandas):\n+        main_table = pandas.DataFrame({ 'join_column': [pandas.Timestamp('20180310T11:17:54Z')] })\n+        left_join_table = pandas.DataFrame({ 'join_column': [pandas.Timestamp('20180310T11:17:54Z')], 'other_column': [pandas.Timestamp('20190310T11:17:54Z')] })\n+        expected_df = pandas.DataFrame({ \"join_column\": numpy.array([datetime.datetime(2018, 3, 10, 11, 17, 54)], dtype='datetime64[ns]'), \"other_column\": numpy.array([datetime.datetime(2019, 3, 10, 11, 17, 54)], dtype='datetime64[ns]')})\n+        run_parallel_queries(main_table, left_join_table, expected_df, pandas)\n \n-    def test_parallel_empty(self,duckdb_cursor):\n-        df_empty = pd.DataFrame({'A' : []})\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_parallel_empty(self,duckdb_cursor, pandas):\n+        df_empty = pandas.DataFrame({'A' : []})\n         duckdb_conn = duckdb.connect()\n         duckdb_conn.execute(\"PRAGMA threads=4\")\n         duckdb_conn.execute(\"PRAGMA verify_parallelism\")\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_pyarrow_filter_pushdown.py b/tools/pythonpkg/tests/fast/pandas/test_pyarrow_filter_pushdown.py\nnew file mode 100644\nindex 000000000000..3e5aa0b47711\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/pandas/test_pyarrow_filter_pushdown.py\n@@ -0,0 +1,368 @@\n+import duckdb\n+import os\n+import pytest\n+import tempfile\n+\n+from conftest import pandas_supports_arrow_backend\n+pa = pytest.importorskip(\"pyarrow\")\n+pq = pytest.importorskip(\"pyarrow.parquet\")\n+ds = pytest.importorskip(\"pyarrow.dataset\")\n+np = pytest.importorskip(\"numpy\")\n+re = pytest.importorskip(\"re\")\n+_ = pytest.importorskip(\"pandas\", '2.0.0')\n+from conftest import ArrowPandas\n+\n+## DuckDB connection used in this test\n+duckdb_conn = duckdb.connect()\n+\n+def numeric_operators(data_type, tbl_name):\n+    duckdb_conn.execute(f\"CREATE TABLE {tbl_name} (a {data_type}, b {data_type}, c {data_type})\")\n+    duckdb_conn.execute(\"INSERT INTO  \" +tbl_name+ \" VALUES (1,1,1),(10,10,10),(100,10,100),(NULL,NULL,NULL)\")\n+    duck_tbl = duckdb_conn.table(tbl_name)\n+    arrow_df = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+    print (arrow_df)\n+\n+    duckdb_conn.register(\"testarrow\",arrow_df)\n+    # Try ==\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a =1\").fetchone()[0] == 1\n+    # Try >\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >1\").fetchone()[0] == 2\n+    # Try >=\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >=10\").fetchone()[0] == 2\n+    # Try <\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <10\").fetchone()[0] == 1\n+    # Try <=\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <=10\").fetchone()[0] == 2\n+\n+    # Try Is Null\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NULL\").fetchone()[0] == 1\n+    # Try Is Not Null\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NOT NULL\").fetchone()[0] == 3\n+\n+    # Try And\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a=10 and b =1\").fetchone()[0] == 0\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a =100 and b = 10 and c = 100\").fetchone()[0] == 1\n+\n+    # Try Or\n+    assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = 100 or b =1\").fetchone()[0] == 2\n+\n+    duckdb_conn.execute(\"EXPLAIN SELECT count(*) from testarrow where a = 100 or b =1\")\n+    print(duckdb_conn.fetchall())\n+\n+def numeric_check_or_pushdown(tbl_name):\n+    duck_tbl = duckdb_conn.table(tbl_name)\n+    arrow_df = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+\n+    arrow_tbl_name = \"testarrow_\" + tbl_name\n+    duckdb_conn.register(arrow_tbl_name ,arrow_df)\n+\n+    # Multiple column in the root OR node, don't push down\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE a=1 OR b=2 AND (a>3 OR b<5)\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters:.*\", query_res[0][1])\n+    assert not match\n+\n+    # Single column in the root OR node\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE a=1 OR a=10\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters: a=1 OR a=10.*|$\", query_res[0][1])\n+    assert match\n+\n+    # Single column + root OR node with AND\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE a=1 OR (a>3 AND a<5)\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters: a=1 OR a>3 AND a<5.*|$\", query_res[0][1])\n+    assert match\n+\n+    # Single column multiple ORs\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE a=1 OR a>3 OR a<5\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters: a=1 OR a>3 OR a<5.*|$\", query_res[0][1])\n+    assert match\n+\n+    # Testing not equal\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE a!=1 OR a>3 OR a<2\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters: a!=1 OR a>3 OR a<2.*|$\", query_res[0][1])\n+    assert match\n+\n+    # Multiple OR filters connected with ANDs\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE (a<2 OR a>3) AND (a=1 OR a=4) AND (b=1 OR b<5)\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters: a<2 OR a>3 AND a=1|\\n.*OR a=4.*\\n.*b=2 OR b<5.*|$\", query_res[0][1])\n+    assert match\n+\n+\n+def string_check_or_pushdown(tbl_name):\n+    duck_tbl = duckdb_conn.table(tbl_name)\n+    arrow_df = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+\n+    arrow_tbl_name = \"testarrow_varchar\"\n+    duckdb_conn.register(arrow_tbl_name ,arrow_df)\n+\n+    # Check string zonemap\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE a>='1' OR a<='10'\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters: a>=1 OR a<=10.*|$\", query_res[0][1])\n+    assert match\n+\n+    # No support for OR with is null\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE a IS NULL or a='1'\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters:.*\", query_res[0][1])\n+    assert not match\n+\n+    # No support for OR with is not null\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE a IS NOT NULL OR a='1'\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters:.*\", query_res[0][1])\n+    assert not match\n+\n+    # OR with the like operator\n+    query_res = duckdb_conn.execute(\"EXPLAIN SELECT * FROM \" +arrow_tbl_name+ \" WHERE a=1 OR a LIKE '10%'\").fetchall()\n+    match = re.search(\".*ARROW_SCAN.*Filters:.*\", query_res[0][1])\n+    assert not match\n+\n+\n+@pytest.mark.skipif(not pandas_supports_arrow_backend(), reason=\"pandas does not support the 'pyarrow' backend\")\n+class TestArrowDFFilterPushdown(object):\n+    def test_filter_pushdown_numeric(self,duckdb_cursor):\n+\n+        numeric_types = ['TINYINT', 'SMALLINT', 'INTEGER', 'BIGINT', 'UTINYINT', 'USMALLINT', 'UINTEGER', 'UBIGINT',\n+        'FLOAT', 'DOUBLE', 'HUGEINT']\n+        for data_type in numeric_types:\n+            tbl_name = \"test_\" + data_type\n+            numeric_operators(data_type, tbl_name)\n+            numeric_check_or_pushdown(tbl_name)\n+\n+    def test_filter_pushdown_decimal(self,duckdb_cursor):\n+        numeric_types = {'DECIMAL(4,1)': 'test_decimal_4_1', 'DECIMAL(9,1)': 'test_decimal_9_1',\n+                         'DECIMAL(18,4)': 'test_decimal_18_4','DECIMAL(30,12)': 'test_decimal_30_12'}\n+        for data_type in numeric_types:\n+            tbl_name = numeric_types[data_type]\n+            numeric_operators(data_type, tbl_name)\n+            numeric_check_or_pushdown(tbl_name)\n+\n+    def test_filter_pushdown_varchar(self,duckdb_cursor):\n+        duckdb_conn.execute(\"CREATE TABLE test_varchar (a  VARCHAR, b VARCHAR, c VARCHAR)\")\n+        duckdb_conn.execute(\"INSERT INTO  test_varchar VALUES ('1','1','1'),('10','10','10'),('100','10','100'),(NULL,NULL,NULL)\")\n+        duck_tbl = duckdb_conn.table(\"test_varchar\")\n+        arrow_table = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+\n+        duckdb_conn.register(\"testarrow\",arrow_table)\n+        # Try ==\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='1'\").fetchone()[0] == 1\n+        # Try >\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >'1'\").fetchone()[0] == 2\n+        # Try >=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >='10'\").fetchone()[0] == 2\n+        # Try <\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <'10'\").fetchone()[0] == 1\n+        # Try <=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <='10'\").fetchone()[0] == 2\n+\n+        # Try Is Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NULL\").fetchone()[0] == 1\n+        # Try Is Not Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NOT NULL\").fetchone()[0] == 3\n+\n+        # Try And\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a='10' and b ='1'\").fetchone()[0] == 0\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='100' and b = '10' and c = '100'\").fetchone()[0] == 1\n+        # Try Or\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = '100' or b ='1'\").fetchone()[0] == 2\n+\n+        # More complex tests for OR pushed down on string\n+        string_check_or_pushdown(\"test_varchar\")\n+\n+\n+    def test_filter_pushdown_bool(self,duckdb_cursor):\n+        duckdb_conn.execute(\"CREATE TABLE test_bool (a  BOOL, b BOOL)\")\n+        duckdb_conn.execute(\"INSERT INTO  test_bool VALUES (TRUE,TRUE),(TRUE,FALSE),(FALSE,TRUE),(NULL,NULL)\")\n+        duck_tbl = duckdb_conn.table(\"test_bool\")\n+        arrow_table = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+\n+        duckdb_conn.register(\"testarrow\",arrow_table)\n+        # Try ==\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a =True\").fetchone()[0] == 2\n+\n+        # Try Is Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NULL\").fetchone()[0] == 1\n+        # Try Is Not Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NOT NULL\").fetchone()[0] == 3\n+\n+        # Try And\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a=True and b =True\").fetchone()[0] == 1\n+        # Try Or\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = True or b =True\").fetchone()[0] == 3\n+\n+    def test_filter_pushdown_time(self,duckdb_cursor):\n+        duckdb_conn.execute(\"CREATE TABLE test_time (a  TIME, b TIME, c TIME)\")\n+        duckdb_conn.execute(\"INSERT INTO  test_time VALUES ('00:01:00','00:01:00','00:01:00'),('00:10:00','00:10:00','00:10:00'),('01:00:00','00:10:00','01:00:00'),(NULL,NULL,NULL)\")\n+        duck_tbl = duckdb_conn.table(\"test_time\")\n+        arrow_table = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+\n+        duckdb_conn.register(\"testarrow\",arrow_table)\n+        # Try ==\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='00:01:00'\").fetchone()[0] == 1\n+        # Try >\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >'00:01:00'\").fetchone()[0] == 2\n+        # Try >=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >='00:10:00'\").fetchone()[0] == 2\n+        # Try <\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <'00:10:00'\").fetchone()[0] == 1\n+        # Try <=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <='00:10:00'\").fetchone()[0] == 2\n+\n+        # Try Is Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NULL\").fetchone()[0] == 1\n+        # Try Is Not Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NOT NULL\").fetchone()[0] == 3\n+\n+        # Try And\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a='00:10:00' and b ='00:01:00'\").fetchone()[0] == 0\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='01:00:00' and b = '00:10:00' and c = '01:00:00'\").fetchone()[0] == 1\n+        # Try Or\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = '01:00:00' or b ='00:01:00'\").fetchone()[0] == 2\n+\n+    def test_filter_pushdown_timestamp(self,duckdb_cursor):\n+        duckdb_conn.execute(\"CREATE TABLE test_timestamp (a  TIMESTAMP, b TIMESTAMP, c TIMESTAMP)\")\n+        duckdb_conn.execute(\"INSERT INTO  test_timestamp VALUES ('2008-01-01 00:00:01','2008-01-01 00:00:01','2008-01-01 00:00:01'),('2010-01-01 10:00:01','2010-01-01 10:00:01','2010-01-01 10:00:01'),('2020-03-01 10:00:01','2010-01-01 10:00:01','2020-03-01 10:00:01'),(NULL,NULL,NULL)\")\n+        duck_tbl = duckdb_conn.table(\"test_timestamp\")\n+        arrow_table = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+        print (arrow_table)\n+\n+        duckdb_conn.register(\"testarrow\",arrow_table)\n+        # Try ==\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='2008-01-01 00:00:01'\").fetchone()[0] == 1\n+        # Try >\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >'2008-01-01 00:00:01'\").fetchone()[0] == 2\n+        # Try >=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >='2010-01-01 10:00:01'\").fetchone()[0] == 2\n+        # Try <\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <'2010-01-01 10:00:01'\").fetchone()[0] == 1\n+        # Try <=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <='2010-01-01 10:00:01'\").fetchone()[0] == 2\n+\n+        # Try Is Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NULL\").fetchone()[0] == 1\n+        # Try Is Not Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NOT NULL\").fetchone()[0] == 3\n+\n+        # Try And\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a='2010-01-01 10:00:01' and b ='2008-01-01 00:00:01'\").fetchone()[0] == 0\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='2020-03-01 10:00:01' and b = '2010-01-01 10:00:01' and c = '2020-03-01 10:00:01'\").fetchone()[0] == 1\n+        # Try Or\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = '2020-03-01 10:00:01' or b ='2008-01-01 00:00:01'\").fetchone()[0] == 2\n+\n+    def test_filter_pushdown_timestamp_TZ(self,duckdb_cursor):\n+        duckdb_conn.execute(\"\"\"\n+            CREATE TABLE test_timestamptz (\n+                a TIMESTAMPTZ,\n+                b TIMESTAMPTZ,\n+                c TIMESTAMPTZ\n+            )\n+        \"\"\")\n+        duckdb_conn.execute(\"\"\"\n+            INSERT INTO test_timestamptz VALUES\n+                ('2008-01-01 00:00:01','2008-01-01 00:00:01','2008-01-01 00:00:01'),\n+                ('2010-01-01 10:00:01','2010-01-01 10:00:01','2010-01-01 10:00:01'),\n+                ('2020-03-01 10:00:01','2010-01-01 10:00:01','2020-03-01 10:00:01'),\n+                (NULL,NULL,NULL)\n+        \"\"\")\n+        # Have to fetch as naive here, or the times will be converted into UTC and our predicates dont match\n+        duck_tbl = duckdb_conn.sql(\"\"\"\n+            select a::TIMESTAMP a, b::TIMESTAMP b, c::TIMESTAMP c from test_timestamptz\n+        \"\"\")\n+        arrow_table = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+        print (arrow_table)\n+\n+        duckdb_conn.register(\"testarrow\",arrow_table)\n+        # Try ==\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='2008-01-01 00:00:01'\").fetchone()[0] == 1\n+        # Try >\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >'2008-01-01 00:00:01'\").fetchone()[0] == 2\n+        # Try >=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >='2010-01-01 10:00:01'\").fetchone()[0] == 2\n+        # Try <\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <'2010-01-01 10:00:01'\").fetchone()[0] == 1\n+        # Try <=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <='2010-01-01 10:00:01'\").fetchone()[0] == 2\n+\n+        # Try Is Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NULL\").fetchone()[0] == 1\n+        # Try Is Not Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NOT NULL\").fetchone()[0] == 3\n+\n+        # Try And\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a='2010-01-01 10:00:01' and b ='2008-01-01 00:00:01'\").fetchone()[0] == 0\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='2020-03-01 10:00:01' and b = '2010-01-01 10:00:01' and c = '2020-03-01 10:00:01'\").fetchone()[0] == 1\n+        # Try Or\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = '2020-03-01 10:00:01' or b ='2008-01-01 00:00:01'\").fetchone()[0] == 2\n+\n+\n+    def test_filter_pushdown_date(self,duckdb_cursor):\n+        duckdb_conn.execute(\"CREATE TABLE test_date (a  DATE, b DATE, c DATE)\")\n+        duckdb_conn.execute(\"INSERT INTO  test_date VALUES ('2000-01-01','2000-01-01','2000-01-01'),('2000-10-01','2000-10-01','2000-10-01'),('2010-01-01','2000-10-01','2010-01-01'),(NULL,NULL,NULL)\")\n+        duck_tbl = duckdb_conn.table(\"test_date\")\n+        arrow_table = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+\n+        duckdb_conn.register(\"testarrow\",arrow_table)\n+        # Try ==\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='2000-01-01'\").fetchone()[0] == 1\n+        # Try >\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >'2000-01-01'\").fetchone()[0] == 2\n+        # Try >=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a >='2000-10-01'\").fetchone()[0] == 2\n+        # Try <\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <'2000-10-01'\").fetchone()[0] == 1\n+        # Try <=\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a <='2000-10-01'\").fetchone()[0] == 2\n+\n+        # Try Is Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NULL\").fetchone()[0] == 1\n+        # Try Is Not Null\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a IS NOT NULL\").fetchone()[0] == 3\n+\n+        # Try And\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a='2000-10-01' and b ='2000-01-01'\").fetchone()[0] == 0\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a ='2010-01-01' and b = '2000-10-01' and c = '2010-01-01'\").fetchone()[0] == 1\n+        # Try Or\n+        assert duckdb_conn.execute(\"SELECT count(*) from testarrow where a = '2010-01-01' or b ='2000-01-01'\").fetchone()[0] == 2\n+\n+\n+    def test_filter_pushdown_no_projection(self,duckdb_cursor):\n+        duckdb_conn.execute(\"CREATE TABLE test_int (a  INTEGER, b INTEGER, c INTEGER)\")\n+        duckdb_conn.execute(\"INSERT INTO  test_int VALUES (1,1,1),(10,10,10),(100,10,100),(NULL,NULL,NULL)\")\n+        duck_tbl = duckdb_conn.table(\"test_int\")\n+        arrow_table = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+        duckdb_conn.register(\"testarrowtable\",arrow_table)\n+        assert duckdb_conn.execute(\"SELECT * FROM  testarrowtable VALUES where a =1\").fetchall() == [(1, 1, 1)]\n+\n+    @pytest.mark.parametrize('pandas', [ArrowPandas()])\n+    def test_filter_pushdown_2145(self,duckdb_cursor, pandas):\n+\n+        date1 = pandas.date_range(\"2018-01-01\", \"2018-12-31\", freq=\"B\")\n+        df1 = pandas.DataFrame(np.random.randn(date1.shape[0], 5), columns=list(\"ABCDE\"))\n+        df1[\"date\"] = date1\n+\n+        date2 = pandas.date_range(\"2019-01-01\", \"2019-12-31\", freq=\"B\")\n+        df2 = pandas.DataFrame(np.random.randn(date2.shape[0], 5), columns=list(\"ABCDE\"))\n+        df2[\"date\"] = date2\n+\n+        pq.write_table(pa.table(df1), \"data1.parquet\")\n+        pq.write_table(pa.table(df2), \"data2.parquet\")\n+\n+        table = pq.ParquetDataset([\"data1.parquet\", \"data2.parquet\"]).read()\n+\n+        con = duckdb.connect()\n+        con.register(\"testarrow\",table)\n+\n+        output_df = duckdb.arrow(table).filter(\"date > '2019-01-01'\").df()\n+        expected_df = duckdb.from_parquet(\"data*.parquet\").filter(\"date > '2019-01-01'\").df()\n+        pandas.testing.assert_frame_equal(expected_df, output_df)\n+\n+        os.remove(\"data1.parquet\")\n+        os.remove(\"data2.parquet\")\n+\n+    def test_filter_column_removal(self,duckdb_cursor):\n+        duckdb_conn.execute(\"CREATE TABLE test AS SELECT range i, range j FROM range(5)\")\n+        duck_test_table = duckdb_conn.table(\"test\")\n+        arrow_test_table = duck_test_table.df().convert_dtypes(dtype_backend='pyarrow')\n+        duckdb_conn.register(\"arrow_test_table\",arrow_test_table)\n+\n+        # PR 4817 - remove filter columns that are unused in the remainder of the query plan from the table function\n+        query_res = duckdb_conn.execute(\"EXPLAIN SELECT count(*) from testarrow where a = 100 or b =1\").fetchall()\n+        match = re.search(\"\u2502 +j +\u2502\", query_res[0][1])\n+        assert not match\ndiff --git a/tools/pythonpkg/tests/fast/pandas/test_pyarrow_projection_pushdown.py b/tools/pythonpkg/tests/fast/pandas/test_pyarrow_projection_pushdown.py\nnew file mode 100644\nindex 000000000000..861939a29f34\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/pandas/test_pyarrow_projection_pushdown.py\n@@ -0,0 +1,19 @@\n+import duckdb\n+import os\n+import pytest\n+\n+from conftest import pandas_supports_arrow_backend\n+pa = pytest.importorskip(\"pyarrow\")\n+ds = pytest.importorskip(\"pyarrow.dataset\")\n+_ = pytest.importorskip(\"pandas\", '2.0.0')\n+\n+@pytest.mark.skipif(not pandas_supports_arrow_backend(), reason=\"pandas does not support the 'pyarrow' backend\")\n+class TestArrowDFProjectionPushdown(object):\n+    def test_projection_pushdown_no_filter(self,duckdb_cursor):\n+        duckdb_conn = duckdb.connect()\n+        duckdb_conn.execute(\"CREATE TABLE test (a  INTEGER, b INTEGER, c INTEGER)\")\n+        duckdb_conn.execute(\"INSERT INTO  test VALUES (1,1,1),(10,10,10),(100,10,100),(NULL,NULL,NULL)\")\n+        duck_tbl = duckdb_conn.table(\"test\")\n+        arrow_table = duck_tbl.df().convert_dtypes(dtype_backend='pyarrow')\n+        duckdb_conn.register(\"testarrowtable\",arrow_table)\n+        assert duckdb_conn.execute(\"SELECT sum(a) FROM  testarrowtable\").fetchall() == [(111,)]\ndiff --git a/tools/pythonpkg/tests/fast/test_case_alias.py b/tools/pythonpkg/tests/fast/test_case_alias.py\nindex 8ed4d8c7be22..461813c29cd1 100644\n--- a/tools/pythonpkg/tests/fast/test_case_alias.py\n+++ b/tools/pythonpkg/tests/fast/test_case_alias.py\n@@ -2,10 +2,12 @@\n import numpy as np\n import datetime\n import duckdb\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestCaseAlias(object):\n-    def test_case_alias(self, duckdb_cursor):\n-        import pandas\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_case_alias(self, duckdb_cursor, pandas):\n         import numpy as np\n         import datetime\n         import duckdb\ndiff --git a/tools/pythonpkg/tests/fast/test_insert.py b/tools/pythonpkg/tests/fast/test_insert.py\nindex 1571399ceced..e8fa8be4d0bf 100644\n--- a/tools/pythonpkg/tests/fast/test_insert.py\n+++ b/tools/pythonpkg/tests/fast/test_insert.py\n@@ -1,18 +1,22 @@\n import duckdb\n import tempfile\n import os\n-import pandas as pd\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestInsert(object):\n-    test_df = pd.DataFrame.from_dict({\"i\":[1, 2, 3], \"j\":[\"one\", \"two\", \"three\"]})\\\n-    # connect to an in-memory temporary database\n-    conn = duckdb.connect()\n-    # get a cursor\n-    cursor = conn.cursor()\n-    conn.execute(\"CREATE TABLE test (i INTEGER, j STRING)\")\n-    rel = conn.table(\"test\")\n-    rel.insert([1,'one'])\n-    rel.insert([2,'two'])\n-    rel.insert([3,'three'])\n-    rel_a3 = cursor.table('test').project('CAST(i as BIGINT)i, j').to_df()\n-    pd.testing.assert_frame_equal(rel_a3, test_df)\n+\t\n+\t@pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+\tdef test_insert(self, pandas):\n+\t\ttest_df = pandas.DataFrame({\"i\":[1, 2, 3], \"j\":[\"one\", \"two\", \"three\"]})\n+\t\t# connect to an in-memory temporary database\n+\t\tconn = duckdb.connect()\n+\t\t# get a cursor\n+\t\tcursor = conn.cursor()\n+\t\tconn.execute(\"CREATE TABLE test (i INTEGER, j STRING)\")\n+\t\trel = conn.table(\"test\")\n+\t\trel.insert([1,'one'])\n+\t\trel.insert([2,'two'])\n+\t\trel.insert([3,'three'])\n+\t\trel_a3 = cursor.table('test').project('CAST(i as BIGINT)i, j').to_df()\n+\t\tpandas.testing.assert_frame_equal(rel_a3, test_df)\ndiff --git a/tools/pythonpkg/tests/fast/test_map.py b/tools/pythonpkg/tests/fast/test_map.py\nindex 04866ef9160c..b75db20aed1c 100644\n--- a/tools/pythonpkg/tests/fast/test_map.py\n+++ b/tools/pythonpkg/tests/fast/test_map.py\n@@ -1,11 +1,12 @@\n import duckdb\n-import pandas as pd\n import numpy\n import pytest\n from datetime import date, timedelta\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestMap(object):\n-    def test_map(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas()])\n+    def test_map(self, duckdb_cursor, pandas):\n         testrel = duckdb.values([1, 2])\n         conn = duckdb.connect()\n         conn.execute('CREATE TABLE t (a integer)')\n@@ -43,16 +44,16 @@ def evil5(df):\n             this_makes_no_sense()\n \n         def return_dataframe(df):\n-            return pd.DataFrame({'A' : [1]})\n+            return pandas.DataFrame({'A' : [1]})\n \n         def return_big_dataframe(df):\n-            return pd.DataFrame({'A' : [1]*5000})\n+            return pandas.DataFrame({'A' : [1]*5000})\n \n         def return_none(df):\n             return None\n \n         def return_empty_df(df):\n-            return pd.DataFrame()\n+            return pandas.DataFrame()\n \n         with pytest.raises(duckdb.InvalidInputException, match='Expected 1 columns from UDF, got 2'):\n             print(testrel.map(evil1).df())\n@@ -77,12 +78,12 @@ def return_empty_df(df):\n         with pytest.raises(TypeError):\n             print(testrel.map().df())\n \n-        testrel.map(return_dataframe).df().equals(pd.DataFrame({'A' : [1]}))\n+        testrel.map(return_dataframe).df().equals(pandas.DataFrame({'A' : [1]}))\n         \n         with pytest.raises(duckdb.InvalidInputException, match='UDF returned more than 2048 rows, which is not allowed.'):\n             testrel.map(return_big_dataframe).df()\n \n-        empty_rel.map(return_dataframe).df().equals(pd.DataFrame({'A' : []}))\n+        empty_rel.map(return_dataframe).df().equals(pandas.DataFrame({'A' : []}))\n \n         with pytest.raises(duckdb.InvalidInputException, match='No return value from Python function'):\n             testrel.map(return_none).df()\n@@ -100,13 +101,14 @@ def return_with_no_modification(df):\n         # in this case we assume the returned type should be the same as the input type\n         duckdb_cursor.values([b'1234']).map(return_with_no_modification).fetchall()\n \n-    def test_isse_3237(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_isse_3237(self, duckdb_cursor, pandas):\n         def process(rel):\n             def mapper(x):\n                 dates = x['date'].to_numpy(\"datetime64[us]\")\n                 days = x['days_to_add'].to_numpy(\"int\")\n-                x[\"result1\"] = pd.Series([pd.to_datetime(y[0]).date() + timedelta(days=y[1].item()) for y in zip(dates,days)], dtype='datetime64[us]')\n-                x[\"result2\"] = pd.Series([pd.to_datetime(y[0]).date() + timedelta(days=-y[1].item()) for y in zip(dates,days)], dtype='datetime64[us]')\n+                x[\"result1\"] = pandas.Series([pandas.to_datetime(y[0]).date() + timedelta(days=y[1].item()) for y in zip(dates,days)], dtype='datetime64[us]')\n+                x[\"result2\"] = pandas.Series([pandas.to_datetime(y[0]).date() + timedelta(days=-y[1].item()) for y in zip(dates,days)], dtype='datetime64[us]')\n                 return x\n \n             rel = rel.map(mapper)\n@@ -115,8 +117,22 @@ def mapper(x):\n             rel = rel.project(\"*, IF(ABS(one) > ABS(two), one, two) as three\")            \n             return rel\n \n-        df = pd.DataFrame({'date': pd.Series([date(2000,1,1), date(2000,1,2)], dtype=\"datetime64[us]\"), 'days_to_add': [1,2]})\n+        df = pandas.DataFrame({'date': pandas.Series([date(2000,1,1), date(2000,1,2)], dtype=\"datetime64[us]\"), 'days_to_add': [1,2]})\n         rel = duckdb.from_df(df)\n         rel = process(rel)\n         x = rel.fetchdf()\n         assert x['days_to_add'].to_numpy()[0] == 1\n+\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_pyarrow_df(self, pandas):\n+        # PyArrow backed dataframes only exist on pandas >= 2.0.0\n+        _ = pytest.importorskip(\"pandas\", \"2.0.0\")\n+        \n+        def basic_function(df):\n+            # Create a pyarrow backed dataframe\n+            df = pandas.DataFrame({'a': [5,3,2,1,2]}).convert_dtypes(dtype_backend='pyarrow')\n+            return df\n+        \n+        con = duckdb.connect()\n+        with pytest.raises(duckdb.InvalidInputException):\n+            rel = con.sql('select 42').map(basic_function)\ndiff --git a/tools/pythonpkg/tests/fast/test_multithread.py b/tools/pythonpkg/tests/fast/test_multithread.py\nindex a7d976932b34..32a4da9a9fd0 100644\n--- a/tools/pythonpkg/tests/fast/test_multithread.py\n+++ b/tools/pythonpkg/tests/fast/test_multithread.py\n@@ -2,8 +2,8 @@\n import pytest\n import threading\n import queue as Queue\n-import pandas as pd\n import numpy as np\n+from conftest import NumpyPandas, ArrowPandas\n import os\n try:\n     import pyarrow as pa\n@@ -16,10 +16,11 @@ def connect_duck(duckdb_conn):\n     assert out == [(42,), (84,), (None,), (128,)]\n \n class DuckDBThreaded:\n-    def __init__(self,duckdb_insert_thread_count,thread_function):\n+    def __init__(self,duckdb_insert_thread_count,thread_function, pandas):\n         self.duckdb_insert_thread_count = duckdb_insert_thread_count\n         self.threads = []\n         self.thread_function = thread_function\n+        self.pandas = pandas\n         \n     def multithread_test(self,if_all_true=True):\n         duckdb_conn = duckdb.connect()\n@@ -27,7 +28,7 @@ def multithread_test(self,if_all_true=True):\n         return_value = False\n \n         for i in range(0,self.duckdb_insert_thread_count):\n-            self.threads.append(threading.Thread(target=self.thread_function, args=(duckdb_conn,queue),name='duckdb_thread_'+str(i)))\n+            self.threads.append(threading.Thread(target=self.thread_function, args=(duckdb_conn,queue, self.pandas),name='duckdb_thread_'+str(i)))\n \n         for i in range(0,len(self.threads)):\n             self.threads[i].start()\n@@ -46,7 +47,7 @@ def multithread_test(self,if_all_true=True):\n         assert (return_value)\n \n \n-def execute_query_same_connection(duckdb_conn, queue):\n+def execute_query_same_connection(duckdb_conn, queue, pandas):\n \n     try:\n         out = duckdb_conn.execute('select i from (values (42), (84), (NULL), (128)) tbl(i)')\n@@ -54,7 +55,7 @@ def execute_query_same_connection(duckdb_conn, queue):\n     except:\n         queue.put(True)\n \n-def execute_query(duckdb_conn, queue):\n+def execute_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -63,7 +64,7 @@ def execute_query(duckdb_conn, queue):\n     except:\n         queue.put(False)\n \n-def insert_runtime_error(duckdb_conn, queue):\n+def insert_runtime_error(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -72,7 +73,7 @@ def insert_runtime_error(duckdb_conn, queue):\n     except:\n         queue.put(True)  \n \n-def execute_many_query(duckdb_conn, queue):\n+def execute_many_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -89,7 +90,7 @@ def execute_many_query(duckdb_conn, queue):\n     except:\n         queue.put(False)  \n \n-def fetchone_query(duckdb_conn, queue):\n+def fetchone_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -98,7 +99,7 @@ def fetchone_query(duckdb_conn, queue):\n     except:\n         queue.put(False)  \n \n-def fetchall_query(duckdb_conn, queue):\n+def fetchall_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -107,7 +108,7 @@ def fetchall_query(duckdb_conn, queue):\n     except:\n         queue.put(False)  \n \n-def conn_close(duckdb_conn, queue):\n+def conn_close(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -116,7 +117,7 @@ def conn_close(duckdb_conn, queue):\n     except:\n         queue.put(False)  \n \n-def fetchnp_query(duckdb_conn, queue):\n+def fetchnp_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -125,7 +126,7 @@ def fetchnp_query(duckdb_conn, queue):\n     except:\n         queue.put(False) \n \n-def fetchdf_query(duckdb_conn, queue):\n+def fetchdf_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -134,7 +135,7 @@ def fetchdf_query(duckdb_conn, queue):\n     except:\n         queue.put(False)\n \n-def fetchdf_chunk_query(duckdb_conn, queue):\n+def fetchdf_chunk_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -143,7 +144,7 @@ def fetchdf_chunk_query(duckdb_conn, queue):\n     except:\n         queue.put(False) \n \n-def fetch_arrow_query(duckdb_conn, queue):\n+def fetch_arrow_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -153,7 +154,7 @@ def fetch_arrow_query(duckdb_conn, queue):\n         queue.put(False) \n \n \n-def fetch_record_batch_query(duckdb_conn, queue):\n+def fetch_record_batch_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -162,7 +163,7 @@ def fetch_record_batch_query(duckdb_conn, queue):\n     except:\n         queue.put(False) \n \n-def transaction_query(duckdb_conn, queue):\n+def transaction_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     duckdb_conn.execute(\"CREATE TABLE T ( i INTEGER)\")\n@@ -176,31 +177,31 @@ def transaction_query(duckdb_conn, queue):\n     except:\n         queue.put(False) \n \n-def df_append(duckdb_conn, queue):\n+def df_append(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     duckdb_conn.execute(\"CREATE TABLE T ( i INTEGER)\")\n-    df = pd.DataFrame(np.random.randint(0,100,size=15), columns=['A'])\n+    df = pandas.DataFrame(np.random.randint(0,100,size=15), columns=['A'])\n     try:\n         duckdb_conn.append('T',df)\n         queue.put(True)\n     except:\n         queue.put(False) \n \n-def df_register(duckdb_conn, queue):\n+def df_register(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n-    df = pd.DataFrame(np.random.randint(0,100,size=15), columns=['A'])\n+    df = pandas.DataFrame(np.random.randint(0,100,size=15), columns=['A'])\n     try:\n         duckdb_conn.register('T',df)\n         queue.put(True)\n     except:\n         queue.put(False) \n \n-def df_unregister(duckdb_conn, queue):\n+def df_unregister(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n-    df = pd.DataFrame(np.random.randint(0,100,size=15), columns=['A'])\n+    df = pandas.DataFrame(np.random.randint(0,100,size=15), columns=['A'])\n     try:\n         duckdb_conn.register('T',df)\n         duckdb_conn.unregister('T')\n@@ -208,7 +209,7 @@ def df_unregister(duckdb_conn, queue):\n     except:\n         queue.put(False) \n \n-def arrow_register_unregister(duckdb_conn, queue):\n+def arrow_register_unregister(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     arrow_tbl = pa.Table.from_pydict({'my_column':pa.array([1,2,3,4,5],type=pa.int64())})\n@@ -219,7 +220,7 @@ def arrow_register_unregister(duckdb_conn, queue):\n     except:\n         queue.put(False) \n \n-def table(duckdb_conn, queue):\n+def table(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     duckdb_conn.execute(\"CREATE TABLE T ( i INTEGER)\")\n@@ -229,7 +230,7 @@ def table(duckdb_conn, queue):\n     except:\n         queue.put(False) \n \n-def view(duckdb_conn, queue):\n+def view(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     duckdb_conn.execute(\"CREATE TABLE T ( i INTEGER)\")\n@@ -239,7 +240,7 @@ def view(duckdb_conn, queue):\n         queue.put(True)\n     except:\n         queue.put(False) \n-def values(duckdb_conn, queue):\n+def values(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -249,7 +250,7 @@ def values(duckdb_conn, queue):\n         queue.put(False) \n \n \n-def from_query(duckdb_conn, queue):\n+def from_query(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     try:\n@@ -258,17 +259,17 @@ def from_query(duckdb_conn, queue):\n     except:\n         queue.put(False) \n \n-def from_df(duckdb_conn, queue):\n+def from_df(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n-    df = pd.DataFrame(['bla', 'blabla']*10, columns=['A'])\n+    df = pandas.DataFrame(['bla', 'blabla']*10, columns=['A'])\n     try:\n         out = duckdb_conn.execute(\"select * from df\").fetchall()\n         queue.put(True)\n     except:\n         queue.put(False)\n \n-def from_arrow(duckdb_conn, queue):\n+def from_arrow(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     arrow_tbl = pa.Table.from_pydict({'my_column':pa.array([1,2,3,4,5],type=pa.int64())})\n@@ -278,7 +279,7 @@ def from_arrow(duckdb_conn, queue):\n     except:\n         queue.put(False)\n \n-def from_csv_auto(duckdb_conn, queue):\n+def from_csv_auto(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     filename = os.path.join(os.path.dirname(os.path.realpath(__file__)),'data','integers.csv')\n@@ -288,7 +289,7 @@ def from_csv_auto(duckdb_conn, queue):\n     except:\n         queue.put(False)     \n \n-def from_parquet(duckdb_conn, queue):\n+def from_parquet(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     filename = os.path.join(os.path.dirname(os.path.realpath(__file__)),'data','binary_string.parquet')\n@@ -298,7 +299,7 @@ def from_parquet(duckdb_conn, queue):\n     except:\n         queue.put(False)\n \n-def description(duckdb_conn, queue):\n+def description(duckdb_conn, queue, pandas):\n     # Get a new connection\n     duckdb_conn = duckdb.connect()\n     duckdb_conn.execute('CREATE TABLE test (i bool, j TIME, k VARCHAR)')\n@@ -311,7 +312,7 @@ def description(duckdb_conn, queue):\n     except:\n         queue.put(False)          \n \n-def cursor(duckdb_conn, queue):\n+def cursor(duckdb_conn, queue, pandas):\n     # Get a new connection\n     cx = duckdb_conn.cursor()  \n     try:\n@@ -322,112 +323,137 @@ def cursor(duckdb_conn, queue):\n \n class TestDuckMultithread(object):\n \n-    def test_execute(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,execute_query)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_execute(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,execute_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_execute_many(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,execute_many_query)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_execute_many(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,execute_many_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_fetchone(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,fetchone_query)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_fetchone(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,fetchone_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_fetchall(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,fetchall_query)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_fetchall(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,fetchall_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_close(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,conn_close)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_close(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,conn_close, pandas)\n         duck_threads.multithread_test()\n \n-    def test_fetchnp(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,fetchnp_query)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_fetchnp(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,fetchnp_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_fetchdf(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,fetchdf_query)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_fetchdf(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,fetchdf_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_fetchdfchunk(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,fetchdf_chunk_query)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_fetchdfchunk(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,fetchdf_chunk_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_fetcharrow(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_fetcharrow(self, duckdb_cursor, pandas):\n         if not can_run:\n             return\n-        duck_threads = DuckDBThreaded(10,fetch_arrow_query)\n+        duck_threads = DuckDBThreaded(10,fetch_arrow_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_fetch_record_batch(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_fetch_record_batch(self, duckdb_cursor, pandas):\n         if not can_run:\n             return\n-        duck_threads = DuckDBThreaded(10,fetch_record_batch_query)\n+        duck_threads = DuckDBThreaded(10,fetch_record_batch_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_transaction(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,transaction_query)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_transaction(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,transaction_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_df_append(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,df_append)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_df_append(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,df_append, pandas)\n         duck_threads.multithread_test()\n \n-    def test_df_register(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,df_register)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_df_register(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,df_register, pandas)\n         duck_threads.multithread_test()\n \n-    def test_df_unregister(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,df_unregister)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_df_unregister(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,df_unregister, pandas)\n         duck_threads.multithread_test()\n \n-    def test_arrow_register_unregister(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_arrow_register_unregister(self, duckdb_cursor, pandas):\n         if not can_run:\n             return\n-        duck_threads = DuckDBThreaded(10,arrow_register_unregister)\n+        duck_threads = DuckDBThreaded(10,arrow_register_unregister, pandas)\n         duck_threads.multithread_test()\n \n-    def test_table(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,table)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_table(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,table, pandas)\n         duck_threads.multithread_test()\n \n-    def test_view(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,view)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_view(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,view, pandas)\n         duck_threads.multithread_test()\n \n-    def test_values(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,values)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_values(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,values, pandas)\n         duck_threads.multithread_test()\n     \n-    def test_from_query(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,from_query)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_from_query(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,from_query, pandas)\n         duck_threads.multithread_test()\n \n-    def test_from_DF(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,from_df)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_from_DF(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,from_df, pandas)\n         duck_threads.multithread_test() \n \n-    def test_from_arrow(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_from_arrow(self, duckdb_cursor, pandas):\n         if not can_run:\n             return\n-        duck_threads = DuckDBThreaded(10,from_arrow)\n+        duck_threads = DuckDBThreaded(10,from_arrow, pandas)\n         duck_threads.multithread_test()\n  \n-    def test_from_csv_auto(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,from_csv_auto)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_from_csv_auto(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,from_csv_auto, pandas)\n         duck_threads.multithread_test()\n \n-    def test_from_parquet(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,from_parquet)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_from_parquet(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,from_parquet, pandas)\n         duck_threads.multithread_test()\n \n-    def test_description(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,description)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_description(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,description, pandas)\n         duck_threads.multithread_test()\n \n-    def test_cursor(self, duckdb_cursor):\n-        duck_threads = DuckDBThreaded(10,cursor)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_cursor(self, duckdb_cursor, pandas):\n+        duck_threads = DuckDBThreaded(10,cursor, pandas)\n         duck_threads.multithread_test(False)\n \n \ndiff --git a/tools/pythonpkg/tests/fast/test_parameter_list.py b/tools/pythonpkg/tests/fast/test_parameter_list.py\nindex 95f6c77462bc..f883c98d6fa7 100644\n--- a/tools/pythonpkg/tests/fast/test_parameter_list.py\n+++ b/tools/pythonpkg/tests/fast/test_parameter_list.py\n@@ -1,6 +1,6 @@\n import duckdb\n-import pandas as pd\n import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestParameterList(object): \n     def test_bool(self, duckdb_cursor):\n@@ -10,9 +10,10 @@ def test_bool(self, duckdb_cursor):\n         res = conn.execute(\"select count(*) from bool_table where a =?\",[True])\n         assert res.fetchone()[0] == 1\n \n-    def test_exception(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_exception(self, duckdb_cursor, pandas):\n         conn = duckdb.connect()\n-        df_in = pd.DataFrame({'numbers': [1,2,3,4,5],})\n+        df_in = pandas.DataFrame({'numbers': [1,2,3,4,5],})\n         conn.execute(\"create table bool_table (a bool)\")\n         conn.execute(\"insert into bool_table values (TRUE)\")\n         with pytest.raises(duckdb.NotImplementedException, match='Unable to transform'):\ndiff --git a/tools/pythonpkg/tests/fast/test_relation_dependency_leak.py b/tools/pythonpkg/tests/fast/test_relation_dependency_leak.py\nindex 24473460b61c..739b6618ae41 100644\n--- a/tools/pythonpkg/tests/fast/test_relation_dependency_leak.py\n+++ b/tools/pythonpkg/tests/fast/test_relation_dependency_leak.py\n@@ -1,61 +1,66 @@\n import duckdb\n-import pandas as pd\n import numpy as np\n import os, psutil\n+import pytest\n try:\n     import pyarrow as pa\n     can_run = True\n except:\n     can_run = False\n+from conftest import NumpyPandas, ArrowPandas\n \n-def check_memory(function_to_check):\n+def check_memory(function_to_check, pandas):\n     process = psutil.Process(os.getpid())\n     mem_usage = process.memory_info().rss/(10**9)\n     for __ in range(100):\n-        function_to_check()\n+        function_to_check(pandas)\n     cur_mem_usage = process.memory_info().rss/(10**9)\n     # This seems a good empirical value\n     assert cur_mem_usage/3 < mem_usage\n \n-def from_df():\n-    df = pd.DataFrame({\"x\": np.random.rand(1_000_000)})\n+def from_df(pandas):\n+    df = pandas.DataFrame({\"x\": np.random.rand(1_000_000)})\n     return duckdb.from_df(df)\n \n-\n-def from_arrow():\n+def from_arrow(pandas):\n     data = pa.array(np.random.rand(1_000_000), type=pa.float32())\n     arrow_table = pa.Table.from_arrays([data],['a'])\n     duckdb.from_arrow(arrow_table)\n \n-def arrow_replacement():\n+def arrow_replacement(pandas):\n     data = pa.array(np.random.rand(1_000_000), type=pa.float32())\n     arrow_table = pa.Table.from_arrays([data],['a'])\n     duckdb.query(\"select sum(a) from arrow_table\").fetchall()\n \n-def pandas_replacement():\n-    df = pd.DataFrame({\"x\": np.random.rand(1_000_000)})\n+def pandas_replacement(pandas):\n+    df = pandas.DataFrame({\"x\": np.random.rand(1_000_000)})\n     duckdb.query(\"select sum(x) from df\").fetchall()\n \n \n class TestRelationDependencyMemoryLeak(object):\n-    def test_from_arrow_leak(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_from_arrow_leak(self, duckdb_cursor, pandas):\n         if not can_run:\n             return\n-        check_memory(from_arrow)\n+        check_memory(from_arrow, pandas)\n \n-    def test_from_df_leak(self, duckdb_cursor):\n-        check_memory(from_df)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_from_df_leak(self, duckdb_cursor, pandas):\n+        check_memory(from_df, pandas)\n \n-    def test_arrow_replacement_scan_leak(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_arrow_replacement_scan_leak(self, duckdb_cursor, pandas):\n         if not can_run:\n             return\n-        check_memory(arrow_replacement)\n+        check_memory(arrow_replacement, pandas)\n \n-    def test_pandas_replacement_scan_leak(self, duckdb_cursor):\n-        check_memory(pandas_replacement)\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_pandas_replacement_scan_leak(self, duckdb_cursor, pandas):\n+        check_memory(pandas_replacement, pandas)\n \n-    def test_relation_view_leak(self, duckdb_cursor):\n-        rel = from_df()\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_relation_view_leak(self, duckdb_cursor, pandas):\n+        rel = from_df(pandas)\n         rel.create_view(\"bla\")\n         duckdb.default_connection.unregister(\"bla\")\n         assert rel.query(\"bla\", \"select count(*) from bla\").fetchone()[0] == 1_000_000\ndiff --git a/tools/pythonpkg/tests/fast/test_runtime_error.py b/tools/pythonpkg/tests/fast/test_runtime_error.py\nindex 2092bcba8c8b..390a35f550ea 100644\n--- a/tools/pythonpkg/tests/fast/test_runtime_error.py\n+++ b/tools/pythonpkg/tests/fast/test_runtime_error.py\n@@ -1,6 +1,6 @@\n import duckdb\n-import pandas as pd\n import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n closed = lambda: pytest.raises(duckdb.ConnectionException, match='Connection has already been closed')\n no_result_set = lambda: pytest.raises(duckdb.InvalidInputException, match='No open result set')\n@@ -54,36 +54,41 @@ def test_arrow_record_batch_reader_error(self):\n         with pytest.raises(duckdb.ProgrammingError, match='There is no query result'):\n             res.fetch_arrow_reader(1)\n \n-    def test_relation_fetchall_error(self):\n+\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_relation_fetchall_error(self, pandas):\n         conn = duckdb.connect()\n-        df_in = pd.DataFrame({'numbers': [1,2,3,4,5],})\n+        df_in = pandas.DataFrame({'numbers': [1,2,3,4,5],})\n         conn.execute(\"create view x as select * from df_in\")\n         rel = conn.query(\"select * from x\")\n         del df_in\n         with pytest.raises(duckdb.ProgrammingError, match='Table with name df_in does not exist'):\n             rel.fetchall()\n \n-    def test_relation_fetchall_execute(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_relation_fetchall_execute(self, pandas):\n         conn = duckdb.connect()\n-        df_in = pd.DataFrame({'numbers': [1,2,3,4,5],})\n+        df_in = pandas.DataFrame({'numbers': [1,2,3,4,5],})\n         conn.execute(\"create view x as select * from df_in\")\n         rel = conn.query(\"select * from x\")\n         del df_in\n         with pytest.raises(duckdb.ProgrammingError, match='Table with name df_in does not exist'):\n             rel.execute()\n \n-    def test_relation_query_error(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_relation_query_error(self, pandas):\n         conn = duckdb.connect()\n-        df_in = pd.DataFrame({'numbers': [1,2,3,4,5],})\n+        df_in = pandas.DataFrame({'numbers': [1,2,3,4,5],})\n         conn.execute(\"create view x as select * from df_in\")\n         rel = conn.query(\"select * from x\")\n         del df_in\n         with pytest.raises(duckdb.CatalogException, match='Table with name df_in does not exist'):\n             rel.query(\"bla\", \"select * from bla\")\n \n-    def test_conn_broken_statement_error(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_conn_broken_statement_error(self, pandas):\n         conn = duckdb.connect()\n-        df_in = pd.DataFrame({'numbers': [1,2,3,4,5],})\n+        df_in = pandas.DataFrame({'numbers': [1,2,3,4,5],})\n         conn.execute(\"create view x as select * from df_in\")\n         del df_in\n         with pytest.raises(duckdb.InvalidInputException):\n@@ -95,10 +100,11 @@ def test_conn_prepared_statement_error(self):\n         with pytest.raises(duckdb.InvalidInputException, match='Prepared statement needs 2 parameters, 1 given'):\n             conn.execute(\"select * from integers where a =? and b=?\",[1])\n \n-    def test_closed_conn_exceptions(self):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_closed_conn_exceptions(self, pandas):\n         conn = duckdb.connect()\n         conn.close()\n-        df_in = pd.DataFrame({'numbers': [1,2,3,4,5],})\n+        df_in = pandas.DataFrame({'numbers': [1,2,3,4,5],})\n \n         with closed():\n             conn.register(\"bla\",df_in)\ndiff --git a/tools/pythonpkg/tests/fast/types/test_nan.py b/tools/pythonpkg/tests/fast/types/test_nan.py\nindex dd75ababa0a2..3c0957adae43 100644\n--- a/tools/pythonpkg/tests/fast/types/test_nan.py\n+++ b/tools/pythonpkg/tests/fast/types/test_nan.py\n@@ -1,10 +1,12 @@\n-import pandas\n import numpy as np\n import datetime\n import duckdb\n+import pytest\n+from conftest import NumpyPandas, ArrowPandas\n \n class TestPandasNaN(object):\n-    def test_pandas_nan(self, duckdb_cursor):\n+    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n+    def test_pandas_nan(self, duckdb_cursor, pandas):\n         # create a DataFrame with some basic values\n         df = pandas.DataFrame([{\"col1\": \"val1\", \"col2\": 1.05},{\"col1\": \"val3\", \"col2\": np.NaN}])\n         # create a new column (newcol1) that includes either NaN or values from col1\n@@ -40,5 +42,3 @@ def test_pandas_nan(self, duckdb_cursor):\n         assert result_df['newcol1'][1] == df['newcol1'][1]\n         assert pandas.isnull(result_df['datetest'][0])\n         assert result_df['datetest'][1] == df['datetest'][1]\n-\n-        \n\\ No newline at end of file\n",
  "problem_statement": "[Python] Pandas 2.0.0 datetime with timezone consumption issue\n### What happens?\n\nPandas 2.0.0 has changed the way they represent datetime objects with timezone info.\r\nWe try to read these as epoch, and that turns into bogus data.\r\n\r\nThe fix is very easy and I've already added it on a branch, so this issue is just for people that run into it.\r\nWe are aware of the issue and it should be fixed soon \ud83d\udc4d \n\n### To Reproduce\n\n```py\r\nimport pandas as pd\r\nimport duckdb\r\n\r\nassert(pd.__version__ == 2.0.0)\r\npd_df = pd.DataFrame({'a': pd.Series([datetime.strptime('1990-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')]).dt.tz_localize('UTC')})\r\nres = duckdb.sql('select * from pd_df')\r\n```\n\n### OS:\n\nMacOS\n\n### DuckDB Version:\n\nmaster\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nThijs Bruineman\n\n### Affiliation:\n\nDuckDB Labs\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "",
  "created_at": "2023-04-08T08:29:35Z"
}