You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Compatibility issues with the DataFrame.groupBy method of the experimental Spark API
### What happens?

The DataFrame.groupBy method of the experimental Spark API does not behave the same way as pyspark. Some of the examples in the groupBy docstring do not work.

1. `groupBy` with no grouping columns fails. Because `DataFrame.agg` is not implemented, which I assume is intentional, this means that you cannot aggregate a column without specifying a grouping. I can `DataFrame.relation.aggregate()` as a workaround.
2. `groupBy` with a list of grouping columns fails. This may be expected. It can easily be fixed by handling that case. If you'd like me to open a PR with a fix, please let me know.
3. `groupBy().count()` produces a column named `"count_star()"` whereas pyspark produces `"count"`.

### To Reproduce

Global imports and variables:
```
from duckdb.experimental.spark.sql import DataFrame, SparkSession
import duckdb.experimental.spark.sql.functions as F
spark = SparkSession.builder.getOrCreate()
df = spark.createDataFrame([(2, "Alice"), (2, "Bob"), (2, "Bob"), (5, "Bob")], schema=["age", "name"])
```
1. `groupBy` with no grouping columns fails.
```
df.groupBy().avg().show()
```
```
ParserException                           Traceback (most recent call last)
Cell In[4], line 1
----> 1 df.groupBy().avg().show()

File ~/python-envs/duckdb/lib/python3.11/site-packages/duckdb/experimental/spark/sql/group.py:39, in df_varargs_api.<locals>._api(self, *cols)
     37 group_by = str(self._grouping)
     38 projections = self._grouping.get_columns()
---> 39 jdf = getattr(self._df.relation, "apply")(
     40     function_name=name,  # aggregate function
     41     function_aggr=expressions,  # inputs to aggregate
     42     group_expr=group_by,  # groups
     43     projected_columns=projections,  # projections
     44 )
     45 return DataFrame(jdf, self.session)

ParserException: Parser Error: SELECT clause without selection list
```
2. `groupBy` with a list of grouping columns fails. 
```
df.groupBy(["name", "age"]).count()
```
```
...
TypeError: ColumnExpression(): incompatible function arguments. The following argument types are supported:
    1. (name: str) -> duckdb.duckdb.Expression

Invoked with: ['name', 'age']
```
This can be fixed by replacing the end of the method with something like this:
```
        if len(cols) == 1 and isinstance(cols[0], list):
            columns = cols[0]
        else:
            columns = cols
        return GroupedData(Grouping(*columns), self)
```

3. `groupBy().count()` produces a column named `count_star()`
```
df.groupBy("name").count().columns
```
```
['name', 'count_star()']
```

### OS:

MacOS aarch64

### DuckDB Version:

1.0.0

### DuckDB Client:

Python

### Full Name:

Daniel Thom

### Affiliation:

National Renewable Energy Laboratory (NREL)

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Not applicable - the reproduction does not require a data set

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of tools/pythonpkg/duckdb/experimental/spark/sql/dataframe.py]
1: from functools import reduce
2: from typing import (
3:     TYPE_CHECKING,
4:     Any,
5:     Callable,
6:     List,
7:     Optional,
8:     Tuple,
9:     Union,
10:     cast,
11:     overload,
12: )
13: import uuid
14: 
15: import duckdb
16: from duckdb import ColumnExpression, Expression, StarExpression
17: 
18: from ._typing import ColumnOrName
19: from ..errors import PySparkTypeError
20: from ..exception import ContributionsAcceptedError
21: from .column import Column
22: from .readwriter import DataFrameWriter
23: from .type_utils import duckdb_to_spark_schema
24: from .types import Row, StructType
25: 
26: if TYPE_CHECKING:
27:     from pandas.core.frame import DataFrame as PandasDataFrame
28: 
29:     from .group import GroupedData, Grouping
30:     from .session import SparkSession
31: 
32: from ..errors import PySparkValueError
33: from .functions import _to_column
34: 
35: 
36: class DataFrame:
37:     def __init__(self, relation: duckdb.DuckDBPyRelation, session: "SparkSession"):
38:         self.relation = relation
39:         self.session = session
40:         self._schema = None
41:         if self.relation is not None:
42:             self._schema = duckdb_to_spark_schema(self.relation.columns, self.relation.types)
43: 
44:     def show(self, **kwargs) -> None:
45:         self.relation.show()
46: 
47:     def toPandas(self) -> "PandasDataFrame":
48:         return self.relation.df()
49: 
50:     def createOrReplaceTempView(self, name: str) -> None:
51:         """Creates or replaces a local temporary view with this :class:`DataFrame`.
52: 
53:         The lifetime of this temporary table is tied to the :class:`SparkSession`
54:         that was used to create this :class:`DataFrame`.
55: 
56:         Parameters
57:         ----------
58:         name : str
59:             Name of the view.
60: 
61:         Examples
62:         --------
63:         Create a local temporary view named 'people'.
64: 
65:         >>> df = spark.createDataFrame([(2, "Alice"), (5, "Bob")], schema=["age", "name"])
66:         >>> df.createOrReplaceTempView("people")
67: 
68:         Replace the local temporary view.
69: 
70:         >>> df2 = df.filter(df.age > 3)
71:         >>> df2.createOrReplaceTempView("people")
72:         >>> df3 = spark.sql("SELECT * FROM people")
73:         >>> sorted(df3.collect()) == sorted(df2.collect())
74:         True
75:         >>> spark.catalog.dropTempView("people")
76:         True
77: 
78:         """
79:         self.relation.create_view(name, True)
80: 
81:     def createGlobalTempView(self, name: str) -> None:
82:         raise NotImplementedError
83: 
84:     def withColumnRenamed(self, columnName: str, newName: str) -> "DataFrame":
85:         if columnName not in self.relation:
86:             raise ValueError(f"DataFrame does not contain a column named {columnName}")
87:         cols = []
88:         for x in self.relation.columns:
89:             col = ColumnExpression(x)
90:             if x.casefold() == columnName.casefold():
91:                 col = col.alias(newName)
92:             cols.append(col)
93:         rel = self.relation.select(*cols)
94:         return DataFrame(rel, self.session)
95: 
96:     def withColumn(self, columnName: str, col: Column) -> "DataFrame":
97:         if not isinstance(col, Column):
98:             raise PySparkTypeError(
99:                 error_class="NOT_COLUMN",
100:                 message_parameters={"arg_name": "col", "arg_type": type(col).__name__},
101:             )
102:         if columnName in self.relation:
103:             # We want to replace the existing column with this new expression
104:             cols = []
105:             for x in self.relation.columns:
106:                 if x.casefold() == columnName.casefold():
107:                     cols.append(col.expr.alias(columnName))
108:                 else:
109:                     cols.append(ColumnExpression(x))
110:         else:
111:             cols = [ColumnExpression(x) for x in self.relation.columns]
112:             cols.append(col.expr.alias(columnName))
113:         rel = self.relation.select(*cols)
114:         return DataFrame(rel, self.session)
115: 
116:     def transform(
117:         self, func: Callable[..., "DataFrame"], *args: Any, **kwargs: Any
118:     ) -> "DataFrame":
119:         """Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.
120: 
121:         .. versionadded:: 3.0.0
122: 
123:         .. versionchanged:: 3.4.0
124:             Supports Spark Connect.
125: 
126:         Parameters
127:         ----------
128:         func : function
129:             a function that takes and returns a :class:`DataFrame`.
130:         *args
131:             Positional arguments to pass to func.
132: 
133:             .. versionadded:: 3.3.0
134:         **kwargs
135:             Keyword arguments to pass to func.
136: 
137:             .. versionadded:: 3.3.0
138: 
139:         Returns
140:         -------
141:         :class:`DataFrame`
142:             Transformed DataFrame.
143: 
144:         Examples
145:         --------
146:         >>> from pyspark.sql.functions import col
147:         >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], ["int", "float"])
148:         >>> def cast_all_to_int(input_df):
149:         ...     return input_df.select([col(col_name).cast("int") for col_name in input_df.columns])
150:         ...
151:         >>> def sort_columns_asc(input_df):
152:         ...     return input_df.select(*sorted(input_df.columns))
153:         ...
154:         >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()
155:         +-----+---+
156:         |float|int|
157:         +-----+---+
158:         |    1|  1|
159:         |    2|  2|
160:         +-----+---+
161: 
162:         >>> def add_n(input_df, n):
163:         ...     return input_df.select([(col(col_name) + n).alias(col_name)
164:         ...                             for col_name in input_df.columns])
165:         >>> df.transform(add_n, 1).transform(add_n, n=10).show()
166:         +---+-----+
167:         |int|float|
168:         +---+-----+
169:         | 12| 12.0|
170:         | 13| 13.0|
171:         +---+-----+
172:         """
173:         result = func(self, *args, **kwargs)
174:         assert isinstance(result, DataFrame), (
175:             "Func returned an instance of type [%s], "
176:             "should have been DataFrame." % type(result)
177:         )
178:         return result
179: 
180:     def sort(
181:         self, *cols: Union[str, Column, List[Union[str, Column]]], **kwargs: Any
182:     ) -> "DataFrame":
183:         """Returns a new :class:`DataFrame` sorted by the specified column(s).
184: 
185:         Parameters
186:         ----------
187:         cols : str, list, or :class:`Column`, optional
188:              list of :class:`Column` or column names to sort by.
189: 
190:         Other Parameters
191:         ----------------
192:         ascending : bool or list, optional, default True
193:             boolean or list of boolean.
194:             Sort ascending vs. descending. Specify list for multiple sort orders.
195:             If a list is specified, the length of the list must equal the length of the `cols`.
196: 
197:         Returns
198:         -------
199:         :class:`DataFrame`
200:             Sorted DataFrame.
201: 
202:         Examples
203:         --------
204:         >>> from pyspark.sql.functions import desc, asc
205:         >>> df = spark.createDataFrame([
206:         ...     (2, "Alice"), (5, "Bob")], schema=["age", "name"])
207: 
208:         Sort the DataFrame in ascending order.
209: 
210:         >>> df.sort(asc("age")).show()
211:         +---+-----+
212:         |age| name|
213:         +---+-----+
214:         |  2|Alice|
215:         |  5|  Bob|
216:         +---+-----+
217: 
218:         Sort the DataFrame in descending order.
219: 
220:         >>> df.sort(df.age.desc()).show()
221:         +---+-----+
222:         |age| name|
223:         +---+-----+
224:         |  5|  Bob|
225:         |  2|Alice|
226:         +---+-----+
227:         >>> df.orderBy(df.age.desc()).show()
228:         +---+-----+
229:         |age| name|
230:         +---+-----+
231:         |  5|  Bob|
232:         |  2|Alice|
233:         +---+-----+
234:         >>> df.sort("age", ascending=False).show()
235:         +---+-----+
236:         |age| name|
237:         +---+-----+
238:         |  5|  Bob|
239:         |  2|Alice|
240:         +---+-----+
241: 
242:         Specify multiple columns
243: 
244:         >>> df = spark.createDataFrame([
245:         ...     (2, "Alice"), (2, "Bob"), (5, "Bob")], schema=["age", "name"])
246:         >>> df.orderBy(desc("age"), "name").show()
247:         +---+-----+
248:         |age| name|
249:         +---+-----+
250:         |  5|  Bob|
251:         |  2|Alice|
252:         |  2|  Bob|
253:         +---+-----+
254: 
255:         Specify multiple columns for sorting order at `ascending`.
256: 
257:         >>> df.orderBy(["age", "name"], ascending=[False, False]).show()
258:         +---+-----+
259:         |age| name|
260:         +---+-----+
261:         |  5|  Bob|
262:         |  2|  Bob|
263:         |  2|Alice|
264:         +---+-----+
265:         """
266:         if kwargs:
267:             raise ContributionsAcceptedError
268:         columns = []
269:         for col in cols:
270:             if isinstance(col, (str, Column)):
271:                 columns.append(_to_column(col))
272:             else:
273:                 raise ContributionsAcceptedError
274:         rel = self.relation.sort(*columns)
275:         return DataFrame(rel, self.session)
276: 
277:     orderBy = sort
278: 
279:     def filter(self, condition: "ColumnOrName") -> "DataFrame":
280:         """Filters rows using the given condition.
281: 
282:         :func:`where` is an alias for :func:`filter`.
283: 
284:         Parameters
285:         ----------
286:         condition : :class:`Column` or str
287:             a :class:`Column` of :class:`types.BooleanType`
288:             or a string of SQL expressions.
289: 
290:         Returns
291:         -------
292:         :class:`DataFrame`
293:             Filtered DataFrame.
294: 
295:         Examples
296:         --------
297:         >>> df = spark.createDataFrame([
298:         ...     (2, "Alice"), (5, "Bob")], schema=["age", "name"])
299: 
300:         Filter by :class:`Column` instances.
301: 
302:         >>> df.filter(df.age > 3).show()
303:         +---+----+
304:         |age|name|
305:         +---+----+
306:         |  5| Bob|
307:         +---+----+
308:         >>> df.where(df.age == 2).show()
309:         +---+-----+
310:         |age| name|
311:         +---+-----+
312:         |  2|Alice|
313:         +---+-----+
314: 
315:         Filter by SQL expression in a string.
316: 
317:         >>> df.filter("age > 3").show()
318:         +---+----+
319:         |age|name|
320:         +---+----+
321:         |  5| Bob|
322:         +---+----+
323:         >>> df.where("age = 2").show()
324:         +---+-----+
325:         |age| name|
326:         +---+-----+
327:         |  2|Alice|
328:         +---+-----+
329:         """
330:         cond = condition.expr if isinstance(condition, Column) else condition
331:         rel = self.relation.filter(cond)
332:         return DataFrame(rel, self.session)
333: 
334:     where = filter
335: 
336:     def select(self, *cols) -> "DataFrame":
337:         cols = list(cols)
338:         if len(cols) == 1:
339:             cols = cols[0]
340:         if isinstance(cols, list):
341:             projections = [
342:                 x.expr if isinstance(x, Column) else ColumnExpression(x) for x in cols
343:             ]
344:         else:
345:             projections = [
346:                 cols.expr if isinstance(cols, Column) else ColumnExpression(cols)
347:             ]
348:         rel = self.relation.select(*projections)
349:         return DataFrame(rel, self.session)
350: 
351:     @property
352:     def columns(self) -> List[str]:
353:         """Returns all column names as a list.
354: 
355:         Examples
356:         --------
357:         >>> df.columns
358:         ['age', 'name']
359:         """
360:         return [f.name for f in self.schema.fields]
361: 
362:     def join(
363:         self,
364:         other: "DataFrame",
365:         on: Optional[Union[str, List[str], Column, List[Column]]] = None,
366:         how: Optional[str] = None,
367:     ) -> "DataFrame":
368:         """Joins with another :class:`DataFrame`, using the given join expression.
369: 
370:         Parameters
371:         ----------
372:         other : :class:`DataFrame`
373:             Right side of the join
374:         on : str, list or :class:`Column`, optional
375:             a string for the join column name, a list of column names,
376:             a join expression (Column), or a list of Columns.
377:             If `on` is a string or a list of strings indicating the name of the join column(s),
378:             the column(s) must exist on both sides, and this performs an equi-join.
379:         how : str, optional
380:             default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,
381:             ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,
382:             ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,
383:             ``anti``, ``leftanti`` and ``left_anti``.
384: 
385:         Returns
386:         -------
387:         :class:`DataFrame`
388:             Joined DataFrame.
389: 
390:         Examples
391:         --------
392:         The following performs a full outer join between ``df1`` and ``df2``.
393: 
394:         >>> from pyspark.sql import Row
395:         >>> from pyspark.sql.functions import desc
396:         >>> df = spark.createDataFrame([(2, "Alice"), (5, "Bob")]).toDF("age", "name")
397:         >>> df2 = spark.createDataFrame([Row(height=80, name="Tom"), Row(height=85, name="Bob")])
398:         >>> df3 = spark.createDataFrame([Row(age=2, name="Alice"), Row(age=5, name="Bob")])
399:         >>> df4 = spark.createDataFrame([
400:         ...     Row(age=10, height=80, name="Alice"),
401:         ...     Row(age=5, height=None, name="Bob"),
402:         ...     Row(age=None, height=None, name="Tom"),
403:         ...     Row(age=None, height=None, name=None),
404:         ... ])
405: 
406:         Inner join on columns (default)
407: 
408:         >>> df.join(df2, 'name').select(df.name, df2.height).show()
409:         +----+------+
410:         |name|height|
411:         +----+------+
412:         | Bob|    85|
413:         +----+------+
414:         >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()
415:         +----+---+
416:         |name|age|
417:         +----+---+
418:         | Bob|  5|
419:         +----+---+
420: 
421:         Outer join for both DataFrames on the 'name' column.
422: 
423:         >>> df.join(df2, df.name == df2.name, 'outer').select(
424:         ...     df.name, df2.height).sort(desc("name")).show()
425:         +-----+------+
426:         | name|height|
427:         +-----+------+
428:         |  Bob|    85|
429:         |Alice|  NULL|
430:         | NULL|    80|
431:         +-----+------+
432:         >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc("name")).show()
433:         +-----+------+
434:         | name|height|
435:         +-----+------+
436:         |  Tom|    80|
437:         |  Bob|    85|
438:         |Alice|  NULL|
439:         +-----+------+
440: 
441:         Outer join for both DataFrams with multiple columns.
442: 
443:         >>> df.join(
444:         ...     df3,
445:         ...     [df.name == df3.name, df.age == df3.age],
446:         ...     'outer'
447:         ... ).select(df.name, df3.age).show()
448:         +-----+---+
449:         | name|age|
450:         +-----+---+
451:         |Alice|  2|
452:         |  Bob|  5|
453:         +-----+---+
454:         """
455: 
456:         if on is not None and not isinstance(on, list):
457:             on = [on]  # type: ignore[assignment]
458: 
459:         if on is not None:
460:             assert isinstance(on, list)
461:             # Get (or create) the Expressions from the list of Columns
462:             on = [_to_column(x) for x in on]
463: 
464:             # & all the Expressions together to form one Expression
465:             assert isinstance(
466:                 on[0], Expression
467:             ), "on should be Column or list of Column"
468:             on = reduce(lambda x, y: x.__and__(y), cast(List[Expression], on))
469: 
470:         if on is None and how is None:
471:             result = self.relation.join(other.relation)
472:         else:
473:             if how is None:
474:                 how = "inner"
475:             if on is None:
476:                 on = "true"
477:             else:
478:                 on = str(on)
479:             assert isinstance(how, str), "how should be a string"
480: 
481:             def map_to_recognized_jointype(how):
482:                 known_aliases = {
483:                     "inner": [],
484:                     "outer": ["full", "fullouter", "full_outer"],
485:                     "left": ["leftouter", "left_outer"],
486:                     "right": ["rightouter", "right_outer"],
487:                     "anti": ["leftanti", "left_anti"],
488:                     "semi": ["leftsemi", "left_semi"],
489:                 }
490:                 mapped_type = None
491:                 for type, aliases in known_aliases.items():
492:                     if how == type or how in aliases:
493:                         mapped_type = type
494:                         break
495: 
496:                 if not mapped_type:
497:                     mapped_type = how
498:                 return mapped_type
499: 
500:             how = map_to_recognized_jointype(how)
501:             result = self.relation.join(other.relation, on, how)
502:         return DataFrame(result, self.session)
503: 
504:     def alias(self, alias: str) -> "DataFrame":
505:         """Returns a new :class:`DataFrame` with an alias set.
506: 
507:         Parameters
508:         ----------
509:         alias : str
510:             an alias name to be set for the :class:`DataFrame`.
511: 
512:         Returns
513:         -------
514:         :class:`DataFrame`
515:             Aliased DataFrame.
516: 
517:         Examples
518:         --------
519:         >>> from pyspark.sql.functions import col, desc
520:         >>> df = spark.createDataFrame(
521:         ...     [(14, "Tom"), (23, "Alice"), (16, "Bob")], ["age", "name"])
522:         >>> df_as1 = df.alias("df_as1")
523:         >>> df_as2 = df.alias("df_as2")
524:         >>> joined_df = df_as1.join(df_as2, col("df_as1.name") == col("df_as2.name"), 'inner')
525:         >>> joined_df.select(
526:         ...     "df_as1.name", "df_as2.name", "df_as2.age").sort(desc("df_as1.name")).show()
527:         +-----+-----+---+
528:         | name| name|age|
529:         +-----+-----+---+
530:         |  Tom|  Tom| 14|
531:         |  Bob|  Bob| 16|
532:         |Alice|Alice| 23|
533:         +-----+-----+---+
534:         """
535:         assert isinstance(alias, str), "alias should be a string"
536:         return DataFrame(self.relation.set_alias(alias), self.session)
537: 
538:     def drop(self, *cols: "ColumnOrName") -> "DataFrame":  # type: ignore[misc]
539:         if len(cols) == 1:
540:             col = cols[0]
541:             if isinstance(col, str):
542:                 exclude = [col]
543:             elif isinstance(col, Column):
544:                 exclude = [col.expr]
545:             else:
546:                 raise TypeError("col should be a string or a Column")
547:         else:
548:             for col in cols:
549:                 if not isinstance(col, str):
550:                     raise TypeError("each col in the param list should be a string")
551:             exclude = list(cols)
552:         # Filter out the columns that don't exist in the relation
553:         exclude = [x for x in exclude if x in self.relation.columns]
554:         expr = StarExpression(exclude=exclude)
555:         return DataFrame(self.relation.select(expr), self.session)
556: 
557:     def __repr__(self) -> str:
558:         return str(self.relation)
559: 
560:     def limit(self, num: int) -> "DataFrame":
561:         """Limits the result count to the number specified.
562: 
563:         Parameters
564:         ----------
565:         num : int
566:             Number of records to return. Will return this number of records
567:             or all records if the DataFrame contains less than this number of records.
568: 
569:         Returns
570:         -------
571:         :class:`DataFrame`
572:             Subset of the records
573: 
574:         Examples
575:         --------
576:         >>> df = spark.createDataFrame(
577:         ...     [(14, "Tom"), (23, "Alice"), (16, "Bob")], ["age", "name"])
578:         >>> df.limit(1).show()
579:         +---+----+
580:         |age|name|
581:         +---+----+
582:         | 14| Tom|
583:         +---+----+
584:         >>> df.limit(0).show()
585:         +---+----+
586:         |age|name|
587:         +---+----+
588:         +---+----+
589:         """
590:         rel = self.relation.limit(num)
591:         return DataFrame(rel, self.session)
592: 
593:     def __contains__(self, item: str):
594:         """
595:         Check if the :class:`DataFrame` contains a column by the name of `item`
596:         """
597:         return item in self.relation
598: 
599:     @property
600:     def schema(self) -> StructType:
601:         """Returns the schema of this :class:`DataFrame` as a :class:`duckdb.experimental.spark.sql.types.StructType`.
602: 
603:         Examples
604:         --------
605:         >>> df.schema
606:         StructType([StructField('age', IntegerType(), True),
607:                     StructField('name', StringType(), True)])
608:         """
609:         return self._schema
610: 
611:     @overload
612:     def __getitem__(self, item: Union[int, str]) -> Column:
613:         ...
614: 
615:     @overload
616:     def __getitem__(self, item: Union[Column, List, Tuple]) -> "DataFrame":
617:         ...
618: 
619:     def __getitem__(
620:         self, item: Union[int, str, Column, List, Tuple]
621:     ) -> Union[Column, "DataFrame"]:
622:         """Returns the column as a :class:`Column`.
623: 
624:         Examples
625:         --------
626:         >>> df.select(df['age']).collect()
627:         [Row(age=2), Row(age=5)]
628:         >>> df[ ["name", "age"]].collect()
629:         [Row(name='Alice', age=2), Row(name='Bob', age=5)]
630:         >>> df[ df.age > 3 ].collect()
631:         [Row(age=5, name='Bob')]
632:         >>> df[df[0] > 3].collect()
633:         [Row(age=5, name='Bob')]
634:         """
635:         if isinstance(item, str):
636:             return self.item
637:         # elif isinstance(item, Column):
638:         #    return self.filter(item)
639:         # elif isinstance(item, (list, tuple)):
640:         #    return self.select(*item)
641:         # elif isinstance(item, int):
642:         #    jc = self._jdf.apply(self.columns[item])
643:         #    return Column(jc)
644:         else:
645:             raise TypeError("unexpected item type: %s" % type(item))
646: 
647:     def __getattr__(self, name: str) -> Column:
648:         """Returns the :class:`Column` denoted by ``name``.
649: 
650:         Examples
651:         --------
652:         >>> df.select(df.age).collect()
653:         [Row(age=2), Row(age=5)]
654:         """
655:         if name not in self.relation.columns:
656:             raise AttributeError(
657:                 "'%s' object has no attribute '%s'" % (self.__class__.__name__, name)
658:             )
659:         return Column(duckdb.ColumnExpression(name))
660: 
661:     @overload
662:     def groupBy(self, *cols: "ColumnOrName") -> "GroupedData":
663:         ...
664: 
665:     @overload
666:     def groupBy(self, __cols: Union[List[Column], List[str]]) -> "GroupedData":
667:         ...
668: 
669:     def groupBy(self, *cols: "ColumnOrName") -> "GroupedData":  # type: ignore[misc]
670:         """Groups the :class:`DataFrame` using the specified columns,
671:         so we can run aggregation on them. See :class:`GroupedData`
672:         for all the available aggregate functions.
673: 
674:         :func:`groupby` is an alias for :func:`groupBy`.
675: 
676:         Parameters
677:         ----------
678:         cols : list, str or :class:`Column`
679:             columns to group by.
680:             Each element should be a column name (string) or an expression (:class:`Column`)
681:             or list of them.
682: 
683:         Returns
684:         -------
685:         :class:`GroupedData`
686:             Grouped data by given columns.
687: 
688:         Examples
689:         --------
690:         >>> df = spark.createDataFrame([
691:         ...     (2, "Alice"), (2, "Bob"), (2, "Bob"), (5, "Bob")], schema=["age", "name"])
692: 
693:         Empty grouping columns triggers a global aggregation.
694: 
695:         >>> df.groupBy().avg().show()
696:         +--------+
697:         |avg(age)|
698:         +--------+
699:         |    2.75|
700:         +--------+
701: 
702:         Group-by 'name', and specify a dictionary to calculate the summation of 'age'.
703: 
704:         >>> df.groupBy("name").agg({"age": "sum"}).sort("name").show()
705:         +-----+--------+
706:         | name|sum(age)|
707:         +-----+--------+
708:         |Alice|       2|
709:         |  Bob|       9|
710:         +-----+--------+
711: 
712:         Group-by 'name', and calculate maximum values.
713: 
714:         >>> df.groupBy(df.name).max().sort("name").show()
715:         +-----+--------+
716:         | name|max(age)|
717:         +-----+--------+
718:         |Alice|       2|
719:         |  Bob|       5|
720:         +-----+--------+
721: 
722:         Group-by 'name' and 'age', and calculate the number of rows in each group.
723: 
724:         >>> df.groupBy(["name", df.age]).count().sort("name", "age").show()
725:         +-----+---+-----+
726:         | name|age|count|
727:         +-----+---+-----+
728:         |Alice|  2|    1|
729:         |  Bob|  2|    2|
730:         |  Bob|  5|    1|
731:         +-----+---+-----+
732:         """
733:         from .group import GroupedData, Grouping
734: 
735:         groups = Grouping(*cols)
736:         return GroupedData(groups, self)
737: 
738:     @property
739:     def write(self) -> DataFrameWriter:
740:         return DataFrameWriter(self)
741: 
742:     def printSchema(self):
743:         raise ContributionsAcceptedError
744: 
745:     def union(self, other: "DataFrame") -> "DataFrame":
746:         """Return a new :class:`DataFrame` containing union of rows in this and another
747:         :class:`DataFrame`.
748: 
749:         Parameters
750:         ----------
751:         other : :class:`DataFrame`
752:             Another :class:`DataFrame` that needs to be unioned
753: 
754:         Returns
755:         -------
756:         :class:`DataFrame`
757: 
758:         See Also
759:         --------
760:         DataFrame.unionAll
761: 
762:         Notes
763:         -----
764:         This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union
765:         (that does deduplication of elements), use this function followed by :func:`distinct`.
766: 
767:         Also as standard in SQL, this function resolves columns by position (not by name).
768: 
769:         Examples
770:         --------
771:         >>> df1 = spark.createDataFrame([[1, 2, 3]], ["col0", "col1", "col2"])
772:         >>> df2 = spark.createDataFrame([[4, 5, 6]], ["col1", "col2", "col0"])
773:         >>> df1.union(df2).show()
774:         +----+----+----+
775:         |col0|col1|col2|
776:         +----+----+----+
777:         |   1|   2|   3|
778:         |   4|   5|   6|
779:         +----+----+----+
780:         >>> df1.union(df1).show()
781:         +----+----+----+
782:         |col0|col1|col2|
783:         +----+----+----+
784:         |   1|   2|   3|
785:         |   1|   2|   3|
786:         +----+----+----+
787:         """
788:         return DataFrame(self.relation.union(other.relation), self.session)
789: 
790:     unionAll = union
791: 
792:     def unionByName(
793:         self, other: "DataFrame", allowMissingColumns: bool = False
794:     ) -> "DataFrame":
795:         """Returns a new :class:`DataFrame` containing union of rows in this and another
796:         :class:`DataFrame`.
797: 
798:         This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
799:         union (that does deduplication of elements), use this function followed by :func:`distinct`.
800: 
801:         .. versionadded:: 2.3.0
802: 
803:         .. versionchanged:: 3.4.0
804:             Supports Spark Connect.
805: 
806:         Parameters
807:         ----------
808:         other : :class:`DataFrame`
809:             Another :class:`DataFrame` that needs to be combined.
810:         allowMissingColumns : bool, optional, default False
811:            Specify whether to allow missing columns.
812: 
813:            .. versionadded:: 3.1.0
814: 
815:         Returns
816:         -------
817:         :class:`DataFrame`
818:             Combined DataFrame.
819: 
820:         Examples
821:         --------
822:         The difference between this function and :func:`union` is that this function
823:         resolves columns by name (not by position):
824: 
825:         >>> df1 = spark.createDataFrame([[1, 2, 3]], ["col0", "col1", "col2"])
826:         >>> df2 = spark.createDataFrame([[4, 5, 6]], ["col1", "col2", "col0"])
827:         >>> df1.unionByName(df2).show()
828:         +----+----+----+
829:         |col0|col1|col2|
830:         +----+----+----+
831:         |   1|   2|   3|
832:         |   6|   4|   5|
833:         +----+----+----+
834: 
835:         When the parameter `allowMissingColumns` is ``True``, the set of column names
836:         in this and other :class:`DataFrame` can differ; missing columns will be filled with null.
837:         Further, the missing columns of this :class:`DataFrame` will be added at the end
838:         in the schema of the union result:
839: 
840:         >>> df1 = spark.createDataFrame([[1, 2, 3]], ["col0", "col1", "col2"])
841:         >>> df2 = spark.createDataFrame([[4, 5, 6]], ["col1", "col2", "col3"])
842:         >>> df1.unionByName(df2, allowMissingColumns=True).show()
843:         +----+----+----+----+
844:         |col0|col1|col2|col3|
845:         +----+----+----+----+
846:         |   1|   2|   3|NULL|
847:         |NULL|   4|   5|   6|
848:         +----+----+----+----+
849:         """
850:         if not allowMissingColumns:
851:             raise ContributionsAcceptedError
852:         raise NotImplementedError
853:         # The relational API does not have support for 'union_by_name' yet
854:         # return DataFrame(self.relation.union_by_name(other.relation, allowMissingColumns), self.session)
855: 
856:     def dropDuplicates(self, subset: Optional[List[str]] = None) -> "DataFrame":
857:         """Return a new :class:`DataFrame` with duplicate rows removed,
858:         optionally only considering certain columns.
859: 
860:         For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming
861:         :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop
862:         duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can
863:         be and the system will accordingly limit the state. In addition, data older than
864:         watermark will be dropped to avoid any possibility of duplicates.
865: 
866:         :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.
867: 
868:         Parameters
869:         ----------
870:         subset : List of column names, optional
871:             List of columns to use for duplicate comparison (default All columns).
872: 
873:         Returns
874:         -------
875:         :class:`DataFrame`
876:             DataFrame without duplicates.
877: 
878:         Examples
879:         --------
880:         >>> from pyspark.sql import Row
881:         >>> df = spark.createDataFrame([
882:         ...     Row(name='Alice', age=5, height=80),
883:         ...     Row(name='Alice', age=5, height=80),
884:         ...     Row(name='Alice', age=10, height=80)
885:         ... ])
886: 
887:         Deduplicate the same rows.
888: 
889:         >>> df.dropDuplicates().show()
890:         +-----+---+------+
891:         | name|age|height|
892:         +-----+---+------+
893:         |Alice|  5|    80|
894:         |Alice| 10|    80|
895:         +-----+---+------+
896: 
897:         Deduplicate values on 'name' and 'height' columns.
898: 
899:         >>> df.dropDuplicates(['name', 'height']).show()
900:         +-----+---+------+
901:         | name|age|height|
902:         +-----+---+------+
903:         |Alice|  5|    80|
904:         +-----+---+------+
905:         """
906:         if subset:
907:             rn_col = f"tmp_col_{uuid.uuid1().hex}"
908:             subset_str = ', '.join([f'"{c}"' for c in subset])
909:             window_spec = f"OVER(PARTITION BY {subset_str}) AS {rn_col}"
910:             df = DataFrame(self.relation.row_number(window_spec, "*"), self.session)
911:             return df.filter(f"{rn_col} = 1").drop(rn_col)
912: 
913:         return self.distinct()
914: 
915: 
916:     def distinct(self) -> "DataFrame":
917:         """Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.
918: 
919:         Returns
920:         -------
921:         :class:`DataFrame`
922:             DataFrame with distinct records.
923: 
924:         Examples
925:         --------
926:         >>> df = spark.createDataFrame(
927:         ...     [(14, "Tom"), (23, "Alice"), (23, "Alice")], ["age", "name"])
928: 
929:         Return the number of distinct rows in the :class:`DataFrame`
930: 
931:         >>> df.distinct().count()
932:         2
933:         """
934:         distinct_rel = self.relation.distinct()
935:         return DataFrame(distinct_rel, self.session)
936: 
937:     def count(self) -> int:
938:         """Returns the number of rows in this :class:`DataFrame`.
939: 
940:         Returns
941:         -------
942:         int
943:             Number of rows.
944: 
945:         Examples
946:         --------
947:         >>> df = spark.createDataFrame(
948:         ...     [(14, "Tom"), (23, "Alice"), (16, "Bob")], ["age", "name"])
949: 
950:         Return the number of rows in the :class:`DataFrame`.
951: 
952:         >>> df.count()
953:         3
954:         """
955:         count_rel = self.relation.count("*")
956:         return int(count_rel.fetchone()[0])
957: 
958:     def _cast_types(self, *types) -> "DataFrame":
959:         existing_columns = self.relation.columns
960:         types_count = len(types)
961:         assert types_count == len(existing_columns)
962: 
963:         cast_expressions = [
964:             f"{existing}::{target_type} as {existing}"
965:             for existing, target_type in zip(existing_columns, types)
966:         ]
967:         cast_expressions = ", ".join(cast_expressions)
968:         new_rel = self.relation.project(cast_expressions)
969:         return DataFrame(new_rel, self.session)
970: 
971:     def toDF(self, *cols) -> "DataFrame":
972:         existing_columns = self.relation.columns
973:         column_count = len(cols)
974:         if column_count != len(existing_columns):
975:             raise PySparkValueError(
976:                 message="Provided column names and number of columns in the DataFrame don't match"
977:             )
978: 
979:         existing_columns = [ColumnExpression(x) for x in existing_columns]
980:         projections = [
981:             existing.alias(new) for existing, new in zip(existing_columns, cols)
982:         ]
983:         new_rel = self.relation.project(*projections)
984:         return DataFrame(new_rel, self.session)
985: 
986:     def collect(self) -> List[Row]:
987:         columns = self.relation.columns
988:         result = self.relation.fetchall()
989: 
990:         def construct_row(values, names) -> Row:
991:             row = tuple.__new__(Row, list(values))
992:             row.__fields__ = list(names)
993:             return row
994: 
995:         rows = [construct_row(x, columns) for x in result]
996:         return rows
997: 
998: 
999: __all__ = ["DataFrame"]
[end of tools/pythonpkg/duckdb/experimental/spark/sql/dataframe.py]
[start of tools/pythonpkg/duckdb/experimental/spark/sql/group.py]
1: #
2: # Licensed to the Apache Software Foundation (ASF) under one or more
3: # contributor license agreements.  See the NOTICE file distributed with
4: # this work for additional information regarding copyright ownership.
5: # The ASF licenses this file to You under the Apache License, Version 2.0
6: # (the "License"); you may not use this file except in compliance with
7: # the License.  You may obtain a copy of the License at
8: #
9: #    http://www.apache.org/licenses/LICENSE-2.0
10: #
11: # Unless required by applicable law or agreed to in writing, software
12: # distributed under the License is distributed on an "AS IS" BASIS,
13: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
14: # See the License for the specific language governing permissions and
15: # limitations under the License.
16: #
17: 
18: from ..exception import ContributionsAcceptedError
19: from typing import Callable, TYPE_CHECKING, overload, Dict, Union
20: 
21: from .column import Column
22: from .session import SparkSession
23: from .dataframe import DataFrame
24: from .functions import _to_column
25: from ._typing import ColumnOrName
26: 
27: if TYPE_CHECKING:
28:     from ._typing import LiteralType
29: 
30: __all__ = ["GroupedData", "Grouping"]
31: 
32: 
33: def df_varargs_api(f: Callable[..., DataFrame]) -> Callable[..., DataFrame]:
34:     def _api(self: "GroupedData", *cols: str) -> DataFrame:
35:         name = f.__name__
36:         expressions = ",".join(list(cols))
37:         group_by = str(self._grouping)
38:         projections = self._grouping.get_columns()
39:         jdf = getattr(self._df.relation, "apply")(
40:             function_name=name,  # aggregate function
41:             function_aggr=expressions,  # inputs to aggregate
42:             group_expr=group_by,  # groups
43:             projected_columns=projections,  # projections
44:         )
45:         return DataFrame(jdf, self.session)
46: 
47:     _api.__name__ = f.__name__
48:     _api.__doc__ = f.__doc__
49:     return _api
50: 
51: 
52: class Grouping:
53:     def __init__(self, *cols: "ColumnOrName", **kwargs):
54:         self._type = ""
55:         self._cols = [_to_column(x) for x in cols]
56:         if 'special' in kwargs:
57:             special = kwargs['special']
58:             accepted_special = ["cube", "rollup"]
59:             assert special in accepted_special
60:             self._type = special
61: 
62:     def get_columns(self) -> str:
63:         columns = ",".join([str(x) for x in self._cols])
64:         return columns
65: 
66:     def __str__(self):
67:         columns = self.get_columns()
68:         if self._type:
69:             return self._type + '(' + columns + ')'
70:         return columns
71: 
72: 
73: class GroupedData:
74:     """
75:     A set of methods for aggregations on a :class:`DataFrame`,
76:     created by :func:`DataFrame.groupBy`.
77: 
78:     """
79: 
80:     def __init__(self, grouping: Grouping, df: DataFrame):
81:         self._grouping = grouping
82:         self._df = df
83:         self.session: SparkSession = df.session
84: 
85:     def __repr__(self) -> str:
86:         return str(self._df)
87: 
88:     @df_varargs_api
89:     def count(self) -> DataFrame:
90:         """Counts the number of records for each group.
91: 
92:         Examples
93:         --------
94:         >>> df = spark.createDataFrame(
95:         ...      [(2, "Alice"), (3, "Alice"), (5, "Bob"), (10, "Bob")], ["age", "name"])
96:         >>> df.show()
97:         +---+-----+
98:         |age| name|
99:         +---+-----+
100:         |  2|Alice|
101:         |  3|Alice|
102:         |  5|  Bob|
103:         | 10|  Bob|
104:         +---+-----+
105: 
106:         Group-by name, and count each group.
107: 
108:         >>> df.groupBy(df.name).count().sort("name").show()
109:         +-----+-----+
110:         | name|count|
111:         +-----+-----+
112:         |Alice|    2|
113:         |  Bob|    2|
114:         +-----+-----+
115:         """
116: 
117:     @df_varargs_api
118:     def mean(self, *cols: str) -> DataFrame:
119:         """Computes average values for each numeric columns for each group.
120: 
121:         :func:`mean` is an alias for :func:`avg`.
122: 
123:         Parameters
124:         ----------
125:         cols : str
126:             column names. Non-numeric columns are ignored.
127:         """
128: 
129:     @df_varargs_api
130:     def avg(self, *cols: str) -> DataFrame:
131:         """Computes average values for each numeric columns for each group.
132: 
133:         :func:`mean` is an alias for :func:`avg`.
134: 
135:         Parameters
136:         ----------
137:         cols : str
138:             column names. Non-numeric columns are ignored.
139: 
140:         Examples
141:         --------
142:         >>> df = spark.createDataFrame([
143:         ...     (2, "Alice", 80), (3, "Alice", 100),
144:         ...     (5, "Bob", 120), (10, "Bob", 140)], ["age", "name", "height"])
145:         >>> df.show()
146:         +---+-----+------+
147:         |age| name|height|
148:         +---+-----+------+
149:         |  2|Alice|    80|
150:         |  3|Alice|   100|
151:         |  5|  Bob|   120|
152:         | 10|  Bob|   140|
153:         +---+-----+------+
154: 
155:         Group-by name, and calculate the mean of the age in each group.
156: 
157:         >>> df.groupBy("name").avg('age').sort("name").show()
158:         +-----+--------+
159:         | name|avg(age)|
160:         +-----+--------+
161:         |Alice|     2.5|
162:         |  Bob|     7.5|
163:         +-----+--------+
164: 
165:         Calculate the mean of the age and height in all data.
166: 
167:         >>> df.groupBy().avg('age', 'height').show()
168:         +--------+-----------+
169:         |avg(age)|avg(height)|
170:         +--------+-----------+
171:         |     5.0|      110.0|
172:         +--------+-----------+
173:         """
174: 
175:     @df_varargs_api
176:     def max(self, *cols: str) -> DataFrame:
177:         """Computes the max value for each numeric columns for each group.
178: 
179:         Examples
180:         --------
181:         >>> df = spark.createDataFrame([
182:         ...     (2, "Alice", 80), (3, "Alice", 100),
183:         ...     (5, "Bob", 120), (10, "Bob", 140)], ["age", "name", "height"])
184:         >>> df.show()
185:         +---+-----+------+
186:         |age| name|height|
187:         +---+-----+------+
188:         |  2|Alice|    80|
189:         |  3|Alice|   100|
190:         |  5|  Bob|   120|
191:         | 10|  Bob|   140|
192:         +---+-----+------+
193: 
194:         Group-by name, and calculate the max of the age in each group.
195: 
196:         >>> df.groupBy("name").max("age").sort("name").show()
197:         +-----+--------+
198:         | name|max(age)|
199:         +-----+--------+
200:         |Alice|       3|
201:         |  Bob|      10|
202:         +-----+--------+
203: 
204:         Calculate the max of the age and height in all data.
205: 
206:         >>> df.groupBy().max("age", "height").show()
207:         +--------+-----------+
208:         |max(age)|max(height)|
209:         +--------+-----------+
210:         |      10|        140|
211:         +--------+-----------+
212:         """
213: 
214:     @df_varargs_api
215:     def min(self, *cols: str) -> DataFrame:
216:         """Computes the min value for each numeric column for each group.
217: 
218:         Parameters
219:         ----------
220:         cols : str
221:             column names. Non-numeric columns are ignored.
222: 
223:         Examples
224:         --------
225:         >>> df = spark.createDataFrame([
226:         ...     (2, "Alice", 80), (3, "Alice", 100),
227:         ...     (5, "Bob", 120), (10, "Bob", 140)], ["age", "name", "height"])
228:         >>> df.show()
229:         +---+-----+------+
230:         |age| name|height|
231:         +---+-----+------+
232:         |  2|Alice|    80|
233:         |  3|Alice|   100|
234:         |  5|  Bob|   120|
235:         | 10|  Bob|   140|
236:         +---+-----+------+
237: 
238:         Group-by name, and calculate the min of the age in each group.
239: 
240:         >>> df.groupBy("name").min("age").sort("name").show()
241:         +-----+--------+
242:         | name|min(age)|
243:         +-----+--------+
244:         |Alice|       2|
245:         |  Bob|       5|
246:         +-----+--------+
247: 
248:         Calculate the min of the age and height in all data.
249: 
250:         >>> df.groupBy().min("age", "height").show()
251:         +--------+-----------+
252:         |min(age)|min(height)|
253:         +--------+-----------+
254:         |       2|         80|
255:         +--------+-----------+
256:         """
257: 
258:     @df_varargs_api
259:     def sum(self, *cols: str) -> DataFrame:
260:         """Computes the sum for each numeric columns for each group.
261: 
262:         Parameters
263:         ----------
264:         cols : str
265:             column names. Non-numeric columns are ignored.
266: 
267:         Examples
268:         --------
269:         >>> df = spark.createDataFrame([
270:         ...     (2, "Alice", 80), (3, "Alice", 100),
271:         ...     (5, "Bob", 120), (10, "Bob", 140)], ["age", "name", "height"])
272:         >>> df.show()
273:         +---+-----+------+
274:         |age| name|height|
275:         +---+-----+------+
276:         |  2|Alice|    80|
277:         |  3|Alice|   100|
278:         |  5|  Bob|   120|
279:         | 10|  Bob|   140|
280:         +---+-----+------+
281: 
282:         Group-by name, and calculate the sum of the age in each group.
283: 
284:         >>> df.groupBy("name").sum("age").sort("name").show()
285:         +-----+--------+
286:         | name|sum(age)|
287:         +-----+--------+
288:         |Alice|       5|
289:         |  Bob|      15|
290:         +-----+--------+
291: 
292:         Calculate the sum of the age and height in all data.
293: 
294:         >>> df.groupBy().sum("age", "height").show()
295:         +--------+-----------+
296:         |sum(age)|sum(height)|
297:         +--------+-----------+
298:         |      20|        440|
299:         +--------+-----------+
300:         """
301: 
302:     @overload
303:     def agg(self, *exprs: Column) -> DataFrame:
304:         ...
305: 
306:     @overload
307:     def agg(self, __exprs: Dict[str, str]) -> DataFrame:
308:         ...
309: 
310:     def agg(self, *exprs: Union[Column, Dict[str, str]]) -> DataFrame:
311:         """Compute aggregates and returns the result as a :class:`DataFrame`.
312: 
313:         The available aggregate functions can be:
314: 
315:         1. built-in aggregation functions, such as `avg`, `max`, `min`, `sum`, `count`
316: 
317:         2. group aggregate pandas UDFs, created with :func:`pyspark.sql.functions.pandas_udf`
318: 
319:            .. note:: There is no partial aggregation with group aggregate UDFs, i.e.,
320:                a full shuffle is required. Also, all the data of a group will be loaded into
321:                memory, so the user should be aware of the potential OOM risk if data is skewed
322:                and certain groups are too large to fit in memory.
323: 
324:            .. seealso:: :func:`pyspark.sql.functions.pandas_udf`
325: 
326:         If ``exprs`` is a single :class:`dict` mapping from string to string, then the key
327:         is the column to perform aggregation on, and the value is the aggregate function.
328: 
329:         Alternatively, ``exprs`` can also be a list of aggregate :class:`Column` expressions.
330: 
331:         .. versionadded:: 1.3.0
332: 
333:         .. versionchanged:: 3.4.0
334:             Supports Spark Connect.
335: 
336:         Parameters
337:         ----------
338:         exprs : dict
339:             a dict mapping from column name (string) to aggregate functions (string),
340:             or a list of :class:`Column`.
341: 
342:         Notes
343:         -----
344:         Built-in aggregation functions and group aggregate pandas UDFs cannot be mixed
345:         in a single call to this function.
346: 
347:         Examples
348:         --------
349:         >>> from pyspark.sql import functions as F
350:         >>> from pyspark.sql.functions import pandas_udf, PandasUDFType
351:         >>> df = spark.createDataFrame(
352:         ...      [(2, "Alice"), (3, "Alice"), (5, "Bob"), (10, "Bob")], ["age", "name"])
353:         >>> df.show()
354:         +---+-----+
355:         |age| name|
356:         +---+-----+
357:         |  2|Alice|
358:         |  3|Alice|
359:         |  5|  Bob|
360:         | 10|  Bob|
361:         +---+-----+
362: 
363:         Group-by name, and count each group.
364: 
365:         >>> df.groupBy(df.name)
366:         GroupedData[grouping...: [name...], value: [age: bigint, name: string], type: GroupBy]
367: 
368:         >>> df.groupBy(df.name).agg({"*": "count"}).sort("name").show()
369:         +-----+--------+
370:         | name|count(1)|
371:         +-----+--------+
372:         |Alice|       2|
373:         |  Bob|       2|
374:         +-----+--------+
375: 
376:         Group-by name, and calculate the minimum age.
377: 
378:         >>> df.groupBy(df.name).agg(F.min(df.age)).sort("name").show()
379:         +-----+--------+
380:         | name|min(age)|
381:         +-----+--------+
382:         |Alice|       2|
383:         |  Bob|       5|
384:         +-----+--------+
385: 
386:         Same as above but uses pandas UDF.
387: 
388:         >>> @pandas_udf('int', PandasUDFType.GROUPED_AGG)  # doctest: +SKIP
389:         ... def min_udf(v):
390:         ...     return v.min()
391:         ...
392:         >>> df.groupBy(df.name).agg(min_udf(df.age)).sort("name").show()  # doctest: +SKIP
393:         +-----+------------+
394:         | name|min_udf(age)|
395:         +-----+------------+
396:         |Alice|           2|
397:         |  Bob|           5|
398:         +-----+------------+
399:         """
400:         assert exprs, "exprs should not be empty"
401:         if len(exprs) == 1 and isinstance(exprs[0], dict):
402:             raise ContributionsAcceptedError
403:         else:
404:             # Columns
405:             assert all(isinstance(c, Column) for c in exprs), "all exprs should be Column"
406:             expressions = list(self._grouping._cols)
407:             expressions.extend([x.expr for x in exprs])
408:             group_by = str(self._grouping)
409:             rel = self._df.relation.select(*expressions, groups=group_by)
410:         return DataFrame(rel, self.session)
411: 
412:     # TODO: add 'pivot'
[end of tools/pythonpkg/duckdb/experimental/spark/sql/group.py]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: