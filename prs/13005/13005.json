{
  "repo": "duckdb/duckdb",
  "pull_number": 13005,
  "instance_id": "duckdb__duckdb-13005",
  "issue_numbers": [
    "12986"
  ],
  "base_commit": "87e070a4bf850f14bd36ebe2e3d521b3b816113b",
  "patch": "diff --git a/tools/pythonpkg/duckdb/experimental/spark/sql/dataframe.py b/tools/pythonpkg/duckdb/experimental/spark/sql/dataframe.py\nindex b1f1aff65981..30f4b1b8c721 100644\n--- a/tools/pythonpkg/duckdb/experimental/spark/sql/dataframe.py\n+++ b/tools/pythonpkg/duckdb/experimental/spark/sql/dataframe.py\n@@ -732,8 +732,11 @@ def groupBy(self, *cols: \"ColumnOrName\") -> \"GroupedData\":  # type: ignore[misc]\n         \"\"\"\n         from .group import GroupedData, Grouping\n \n-        groups = Grouping(*cols)\n-        return GroupedData(groups, self)\n+        if len(cols) == 1 and isinstance(cols[0], list):\n+            columns = cols[0]\n+        else:\n+            columns = cols\n+        return GroupedData(Grouping(*columns), self)\n \n     @property\n     def write(self) -> DataFrameWriter:\ndiff --git a/tools/pythonpkg/duckdb/experimental/spark/sql/group.py b/tools/pythonpkg/duckdb/experimental/spark/sql/group.py\nindex e6502a1a7527..0a2f49b7ae82 100644\n--- a/tools/pythonpkg/duckdb/experimental/spark/sql/group.py\n+++ b/tools/pythonpkg/duckdb/experimental/spark/sql/group.py\n@@ -16,33 +16,36 @@\n #\n \n from ..exception import ContributionsAcceptedError\n-from typing import Callable, TYPE_CHECKING, overload, Dict, Union\n+from typing import Callable, TYPE_CHECKING, overload, Dict, Union, List\n \n from .column import Column\n from .session import SparkSession\n from .dataframe import DataFrame\n from .functions import _to_column\n from ._typing import ColumnOrName\n+from .types import NumericType\n \n if TYPE_CHECKING:\n     from ._typing import LiteralType\n \n __all__ = [\"GroupedData\", \"Grouping\"]\n \n+def _api_internal(self: \"GroupedData\", name: str, *cols: str) -> DataFrame:\n+    expressions = \",\".join(list(cols))\n+    group_by = str(self._grouping) if self._grouping else \"\"\n+    projections = self._grouping.get_columns()\n+    jdf = getattr(self._df.relation, \"apply\")(\n+        function_name=name,  # aggregate function\n+        function_aggr=expressions,  # inputs to aggregate\n+        group_expr=group_by,  # groups\n+        projected_columns=projections,  # projections\n+    )\n+    return DataFrame(jdf, self.session)\n \n def df_varargs_api(f: Callable[..., DataFrame]) -> Callable[..., DataFrame]:\n     def _api(self: \"GroupedData\", *cols: str) -> DataFrame:\n         name = f.__name__\n-        expressions = \",\".join(list(cols))\n-        group_by = str(self._grouping)\n-        projections = self._grouping.get_columns()\n-        jdf = getattr(self._df.relation, \"apply\")(\n-            function_name=name,  # aggregate function\n-            function_aggr=expressions,  # inputs to aggregate\n-            group_expr=group_by,  # groups\n-            projected_columns=projections,  # projections\n-        )\n-        return DataFrame(jdf, self.session)\n+        return _api_internal(self, name, *cols)\n \n     _api.__name__ = f.__name__\n     _api.__doc__ = f.__doc__\n@@ -85,7 +88,6 @@ def __init__(self, grouping: Grouping, df: DataFrame):\n     def __repr__(self) -> str:\n         return str(self._df)\n \n-    @df_varargs_api\n     def count(self) -> DataFrame:\n         \"\"\"Counts the number of records for each group.\n \n@@ -113,6 +115,7 @@ def count(self) -> DataFrame:\n         |  Bob|    2|\n         +-----+-----+\n         \"\"\"\n+        return _api_internal(self, \"count\").withColumnRenamed('count_star()', 'count')\n \n     @df_varargs_api\n     def mean(self, *cols: str) -> DataFrame:\n@@ -126,7 +129,6 @@ def mean(self, *cols: str) -> DataFrame:\n             column names. Non-numeric columns are ignored.\n         \"\"\"\n \n-    @df_varargs_api\n     def avg(self, *cols: str) -> DataFrame:\n         \"\"\"Computes average values for each numeric columns for each group.\n \n@@ -171,6 +173,12 @@ def avg(self, *cols: str) -> DataFrame:\n         |     5.0|      110.0|\n         +--------+-----------+\n         \"\"\"\n+        columns = list(cols)\n+        if len(columns) == 0:\n+            schema = self._df.schema\n+            # Take only the numeric types of the relation\n+            columns: List[str] = [x.name for x in schema.fields if isinstance(x.dataType, NumericType)]\n+        return _api_internal(self, \"avg\", *columns)\n \n     @df_varargs_api\n     def max(self, *cols: str) -> DataFrame:\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/spark/test_spark_group_by.py b/tools/pythonpkg/tests/fast/spark/test_spark_group_by.py\nindex f9c8259b664a..5b0fe0305291 100644\n--- a/tools/pythonpkg/tests/fast/spark/test_spark_group_by.py\n+++ b/tools/pythonpkg/tests/fast/spark/test_spark_group_by.py\n@@ -44,7 +44,7 @@ def test_group_by(self, spark):\n         res = df2.collect()\n         assert (\n             str(res)\n-            == \"[Row(department='Finance', count_star()=4), Row(department='Marketing', count_star()=2), Row(department='Sales', count_star()=3)]\"\n+            == \"[Row(department='Finance', count=4), Row(department='Marketing', count=2), Row(department='Sales', count=3)]\"\n         )\n \n         df2 = df.groupBy(\"department\").min(\"salary\").sort(\"department\")\n@@ -115,3 +115,20 @@ def test_group_by(self, spark):\n             str(res)\n             == \"[Row(department='Finance', sum_salary=351000, avg_salary=87750.0, sum_bonus=81000, max_bonus=24000), Row(department='Sales', sum_salary=257000, avg_salary=85666.66666666667, sum_bonus=53000, max_bonus=23000)]\"\n         )\n+\n+    def test_group_by_empty(self, spark):\n+        df = spark.createDataFrame(\n+            [(2, 1.0, \"1\"), (2, 2.0, \"2\"), (2, 3.0, \"3\"), (5, 4.0, \"4\")], schema=[\"age\", \"extra\", \"name\"]\n+        )\n+\n+        res = df.groupBy().avg().collect()\n+        assert str(res) == \"[Row(avg(age)=2.75, avg(extra)=2.5)]\"\n+\n+        res = df.groupBy([\"name\", \"age\"]).count().sort(\"name\").collect()\n+        assert (\n+            str(res)\n+            == \"[Row(name='1', age=2, count=1), Row(name='2', age=2, count=1), Row(name='3', age=2, count=1), Row(name='4', age=5, count=1)]\"\n+        )\n+\n+        res = df.groupBy(\"name\").count().columns\n+        assert res == ['name', 'count']\n",
  "problem_statement": "Compatibility issues with the DataFrame.groupBy method of the experimental Spark API\n### What happens?\n\nThe DataFrame.groupBy method of the experimental Spark API does not behave the same way as pyspark. Some of the examples in the groupBy docstring do not work.\r\n\r\n1. `groupBy` with no grouping columns fails. Because `DataFrame.agg` is not implemented, which I assume is intentional, this means that you cannot aggregate a column without specifying a grouping. I can `DataFrame.relation.aggregate()` as a workaround.\r\n2. `groupBy` with a list of grouping columns fails. This may be expected. It can easily be fixed by handling that case. If you'd like me to open a PR with a fix, please let me know.\r\n3. `groupBy().count()` produces a column named `\"count_star()\"` whereas pyspark produces `\"count\"`.\n\n### To Reproduce\n\nGlobal imports and variables:\r\n```\r\nfrom duckdb.experimental.spark.sql import DataFrame, SparkSession\r\nimport duckdb.experimental.spark.sql.functions as F\r\nspark = SparkSession.builder.getOrCreate()\r\ndf = spark.createDataFrame([(2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\r\n```\r\n1. `groupBy` with no grouping columns fails.\r\n```\r\ndf.groupBy().avg().show()\r\n```\r\n```\r\nParserException                           Traceback (most recent call last)\r\nCell In[4], line 1\r\n----> 1 df.groupBy().avg().show()\r\n\r\nFile ~/python-envs/duckdb/lib/python3.11/site-packages/duckdb/experimental/spark/sql/group.py:39, in df_varargs_api.<locals>._api(self, *cols)\r\n     37 group_by = str(self._grouping)\r\n     38 projections = self._grouping.get_columns()\r\n---> 39 jdf = getattr(self._df.relation, \"apply\")(\r\n     40     function_name=name,  # aggregate function\r\n     41     function_aggr=expressions,  # inputs to aggregate\r\n     42     group_expr=group_by,  # groups\r\n     43     projected_columns=projections,  # projections\r\n     44 )\r\n     45 return DataFrame(jdf, self.session)\r\n\r\nParserException: Parser Error: SELECT clause without selection list\r\n```\r\n2. `groupBy` with a list of grouping columns fails. \r\n```\r\ndf.groupBy([\"name\", \"age\"]).count()\r\n```\r\n```\r\n...\r\nTypeError: ColumnExpression(): incompatible function arguments. The following argument types are supported:\r\n    1. (name: str) -> duckdb.duckdb.Expression\r\n\r\nInvoked with: ['name', 'age']\r\n```\r\nThis can be fixed by replacing the end of the method with something like this:\r\n```\r\n        if len(cols) == 1 and isinstance(cols[0], list):\r\n            columns = cols[0]\r\n        else:\r\n            columns = cols\r\n        return GroupedData(Grouping(*columns), self)\r\n```\r\n\r\n3. `groupBy().count()` produces a column named `count_star()`\r\n```\r\ndf.groupBy(\"name\").count().columns\r\n```\r\n```\r\n['name', 'count_star()']\r\n```\n\n### OS:\n\nMacOS aarch64\n\n### DuckDB Version:\n\n1.0.0\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nDaniel Thom\n\n### Affiliation:\n\nNational Renewable Energy Laboratory (NREL)\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNot applicable - the reproduction does not require a data set\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n",
  "hints_text": "",
  "created_at": "2024-07-16T09:48:01Z"
}