{
  "repo": "duckdb/duckdb",
  "pull_number": 16522,
  "instance_id": "duckdb__duckdb-16522",
  "issue_numbers": [
    "16501"
  ],
  "base_commit": "8e52ec43959ab363643d63cb78ee214577111da4",
  "patch": "diff --git a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\nindex 2d20e7344adb..f10a4dae05f9 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\n@@ -265,11 +265,14 @@ struct DuckDBPyConnection : public enable_shared_from_this<DuckDBPyConnection> {\n \tunique_ptr<DuckDBPyRelation> FromParquet(const string &file_glob, bool binary_as_string, bool file_row_number,\n \t                                         bool filename, bool hive_partitioning, bool union_by_name,\n \t                                         const py::object &compression = py::none());\n-\n \tunique_ptr<DuckDBPyRelation> FromParquets(const vector<string> &file_globs, bool binary_as_string,\n \t                                          bool file_row_number, bool filename, bool hive_partitioning,\n \t                                          bool union_by_name, const py::object &compression = py::none());\n \n+\tunique_ptr<DuckDBPyRelation> FromParquetInternal(Value &&file_param, bool binary_as_string, bool file_row_number,\n+\t                                                 bool filename, bool hive_partitioning, bool union_by_name,\n+\t                                                 const py::object &compression = py::none());\n+\n \tunique_ptr<DuckDBPyRelation> FromArrow(py::object &arrow_object);\n \n \tunordered_set<string> GetTableNames(const string &query);\ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 80fc21ba5ec3..ace9d1237335 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -1676,14 +1676,14 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(const PandasDataFrame &v\n \treturn make_uniq<DuckDBPyRelation>(std::move(rel));\n }\n \n-unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_glob, bool binary_as_string,\n-                                                             bool file_row_number, bool filename,\n-                                                             bool hive_partitioning, bool union_by_name,\n-                                                             const py::object &compression) {\n+unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquetInternal(Value &&file_param, bool binary_as_string,\n+                                                                     bool file_row_number, bool filename,\n+                                                                     bool hive_partitioning, bool union_by_name,\n+                                                                     const py::object &compression) {\n \tauto &connection = con.GetConnection();\n \tstring name = \"parquet_\" + StringUtil::GenerateRandomName();\n \tvector<Value> params;\n-\tparams.emplace_back(file_glob);\n+\tparams.emplace_back(std::move(file_param));\n \tnamed_parameter_map_t named_parameters({{\"binary_as_string\", Value::BOOLEAN(binary_as_string)},\n \t                                        {\"file_row_number\", Value::BOOLEAN(file_row_number)},\n \t                                        {\"filename\", Value::BOOLEAN(filename)},\n@@ -1701,32 +1701,27 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_\n \treturn make_uniq<DuckDBPyRelation>(connection.TableFunction(\"parquet_scan\", params, named_parameters)->Alias(name));\n }\n \n+unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_glob, bool binary_as_string,\n+                                                             bool file_row_number, bool filename,\n+                                                             bool hive_partitioning, bool union_by_name,\n+                                                             const py::object &compression) {\n+\tauto file_param = Value(file_glob);\n+\treturn FromParquetInternal(std::move(file_param), binary_as_string, file_row_number, filename, hive_partitioning,\n+\t                           union_by_name, compression);\n+}\n+\n unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquets(const vector<string> &file_globs, bool binary_as_string,\n                                                               bool file_row_number, bool filename,\n                                                               bool hive_partitioning, bool union_by_name,\n                                                               const py::object &compression) {\n-\tauto &connection = con.GetConnection();\n-\tstring name = \"parquet_\" + StringUtil::GenerateRandomName();\n \tvector<Value> params;\n \tauto file_globs_as_value = vector<Value>();\n \tfor (const auto &file : file_globs) {\n \t\tfile_globs_as_value.emplace_back(file);\n \t}\n-\tparams.emplace_back(Value::LIST(file_globs_as_value));\n-\tnamed_parameter_map_t named_parameters({{\"binary_as_string\", Value::BOOLEAN(binary_as_string)},\n-\t                                        {\"file_row_number\", Value::BOOLEAN(file_row_number)},\n-\t                                        {\"filename\", Value::BOOLEAN(filename)},\n-\t                                        {\"hive_partitioning\", Value::BOOLEAN(hive_partitioning)},\n-\t                                        {\"union_by_name\", Value::BOOLEAN(union_by_name)}});\n-\n-\tif (!py::none().is(compression)) {\n-\t\tif (!py::isinstance<py::str>(compression)) {\n-\t\t\tthrow InvalidInputException(\"from_parquet only accepts 'compression' as a string\");\n-\t\t}\n-\t\tnamed_parameters[\"compression\"] = Value(py::str(compression));\n-\t}\n-\n-\treturn make_uniq<DuckDBPyRelation>(connection.TableFunction(\"parquet_scan\", params, named_parameters)->Alias(name));\n+\tauto file_param = Value::LIST(file_globs_as_value);\n+\treturn FromParquetInternal(std::move(file_param), binary_as_string, file_row_number, filename, hive_partitioning,\n+\t                           union_by_name, compression);\n }\n \n unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrow(py::object &arrow_object) {\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/api/test_fsspec.py b/tools/pythonpkg/tests/fast/api/test_fsspec.py\nnew file mode 100644\nindex 000000000000..34a254e1caf1\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/api/test_fsspec.py\n@@ -0,0 +1,51 @@\n+import pytest\n+import duckdb\n+import io\n+\n+fsspec = pytest.importorskip(\"fsspec\")\n+\n+\n+class TestReadParquet(object):\n+    def test_fsspec_deadlock(self, duckdb_cursor, tmp_path):\n+        # Create test parquet data\n+        file_path = tmp_path / \"data.parquet\"\n+        duckdb_cursor.sql(\"COPY (FROM range(50_000)) TO '{}' (FORMAT parquet)\".format(str(file_path)))\n+        with open(file_path, \"rb\") as f:\n+            parquet_data = f.read()\n+\n+        class TestFileSystem(fsspec.AbstractFileSystem):\n+            protocol = \"deadlock\"\n+\n+            @property\n+            def fsid(self):\n+                return \"deadlock\"\n+\n+            def ls(self, path, detail=True, **kwargs):\n+                vals = [k for k in self._data.keys() if k.startswith(path)]\n+                if detail:\n+                    return [\n+                        {\n+                            \"name\": path,\n+                            \"size\": len(self._data[path]),\n+                            \"type\": \"file\",\n+                            \"created\": 0,\n+                            \"islink\": False,\n+                        }\n+                        for path in vals\n+                    ]\n+                else:\n+                    return vals\n+\n+            def _open(self, path, **kwargs):\n+                return io.BytesIO(self._data[path])\n+\n+            def __init__(self):\n+                super().__init__()\n+                self._data = {\"a\": parquet_data, \"b\": parquet_data}\n+\n+        fsspec.register_implementation(\"deadlock\", TestFileSystem, clobber=True)\n+        fs = fsspec.filesystem('deadlock')\n+        duckdb_cursor.register_filesystem(fs)\n+\n+        result = duckdb_cursor.read_parquet(file_globs=[\"deadlock://a\", \"deadlock://b\"], union_by_name=True)\n+        assert len(result.fetchall()) == 100_000\n",
  "problem_statement": "fsspec read deadlock\n### What happens?\n\nIt seems that from at least duckdb 1.2.0 there is a deadlock when using fsspec from read_parquet.\nNote I am using python 3.10\n\nFrom a quick gdb inspection it looks like read_parquet does not release the GIL, but a background thread tries to take it when calling back into fsspec -> deadlock.\n\nThe main thread then just spins: https://github.com/duckdb/duckdb/blob/ab451db4720e5d82e92503bcb28a527fb9348d4b/src/parallel/task_executor.cpp#L46 \n\n\n\n### To Reproduce\n\nyou should be able to stick this in a py file and run it\n\n```\nimport duckdb\nimport io\nimport threading\nimport tempfile\nimport fsspec\n\nwith tempfile.TemporaryFile() as fp1:\n    duckdb.sql(\"COPY (FROM generate_series(50_000)) TO '{}' (FORMAT parquet)\".format(fp1))\n    with open(str(fp1), \"rb\") as f:\n        _parquet_data = f.read()\n\nclass TestFileSystem(fsspec.AbstractFileSystem):\n    protocol = \"deadlock\"\n\n    @property\n    def fsid(self):\n        return \"deadlock\"\n\n    def ls(self, path, detail=True, **kwargs):\n        vals = [k for k in self._data.keys() if k.startswith(path)]\n        if detail:\n            return [\n                {\n                    \"name\": path,\n                    \"size\": len(self._data[path]),\n                    \"type\": \"file\",\n                    \"created\": 0,\n                    \"islink\": False,\n                } for path in vals\n            ]\n        else:\n            return vals\n\n    def _open(\n        self,\n        path,\n        **kwargs,\n    ):\n        return io.BytesIO(self._data[path])\n\n    def __init__(self):\n        super().__init__()\n        self._data = {\"a\": _parquet_data, \"b\": _parquet_data}\n\nfsspec.register_implementation(\"deadlock\", TestFileSystem, clobber=True)\nduckdb.register_filesystem(fsspec.filesystem('deadlock'))\nduckdb.read_parquet(file_globs=[\"deadlock://a\", \"deadlock://b\"], union_by_name=True)\n```\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\n1.2.0\n\n### DuckDB Client:\n\nPython\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nDylan Yudaken\n\n### Affiliation:\n\nQubos\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a nightly build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "Thanks we'll take a look!",
  "created_at": "2025-03-05T10:07:49Z"
}