{
  "repo": "duckdb/duckdb",
  "pull_number": 2844,
  "instance_id": "duckdb__duckdb-2844",
  "issue_numbers": [
    "1417"
  ],
  "base_commit": "6322c0deb9bc1aee3d49f08452d5e03a20395e6b",
  "patch": "diff --git a/src/common/pipe_file_system.cpp b/src/common/pipe_file_system.cpp\nindex 3c202dc6f7f9..f269990e6ea6 100644\n--- a/src/common/pipe_file_system.cpp\n+++ b/src/common/pipe_file_system.cpp\n@@ -27,6 +27,10 @@ int64_t PipeFile::WriteChunk(void *buffer, int64_t nr_bytes) {\n \treturn child_handle->Write(buffer, nr_bytes);\n }\n \n+void PipeFileSystem::Reset(FileHandle &handle) {\n+\tthrow InternalException(\"Cannot reset pipe file system\");\n+}\n+\n int64_t PipeFileSystem::Read(FileHandle &handle, void *buffer, int64_t nr_bytes) {\n \tauto &pipe = (PipeFile &)handle;\n \treturn pipe.ReadChunk(buffer, nr_bytes);\ndiff --git a/src/execution/operator/persistent/buffered_csv_reader.cpp b/src/execution/operator/persistent/buffered_csv_reader.cpp\nindex 2e0e24ab8f2c..67ccef36d7c4 100644\n--- a/src/execution/operator/persistent/buffered_csv_reader.cpp\n+++ b/src/execution/operator/persistent/buffered_csv_reader.cpp\n@@ -22,6 +22,125 @@\n \n namespace duckdb {\n \n+struct CSVFileHandle {\n+public:\n+\texplicit CSVFileHandle(unique_ptr<FileHandle> file_handle_p) : file_handle(move(file_handle_p)) {\n+\t\tcan_seek = file_handle->CanSeek();\n+\t\tplain_file_source = file_handle->OnDiskFile() && can_seek;\n+\t\tfile_size = file_handle->GetFileSize();\n+\t}\n+\n+\tbool CanSeek() {\n+\t\treturn can_seek;\n+\t}\n+\tvoid Seek(idx_t position) {\n+\t\tif (!can_seek) {\n+\t\t\tthrow InternalException(\"Cannot seek in this file\");\n+\t\t}\n+\t\tfile_handle->Seek(position);\n+\t}\n+\tidx_t SeekPosition() {\n+\t\tif (!can_seek) {\n+\t\t\tthrow InternalException(\"Cannot seek in this file\");\n+\t\t}\n+\t\treturn file_handle->SeekPosition();\n+\t}\n+\tvoid Reset() {\n+\t\tif (plain_file_source) {\n+\t\t\tfile_handle->Reset();\n+\t\t} else {\n+\t\t\tif (!reset_enabled) {\n+\t\t\t\tthrow InternalException(\"Reset called but reset is not enabled for this CSV Handle\");\n+\t\t\t}\n+\t\t\tread_position = 0;\n+\t\t}\n+\t}\n+\tbool PlainFileSource() {\n+\t\treturn plain_file_source;\n+\t}\n+\n+\tidx_t FileSize() {\n+\t\treturn file_size;\n+\t}\n+\n+\tidx_t Read(void *buffer, idx_t nr_bytes) {\n+\t\tif (!plain_file_source) {\n+\t\t\t// not a plain file source: we need to do some bookkeeping around the reset functionality\n+\t\t\tidx_t result_offset = 0;\n+\t\t\tif (read_position < buffer_size) {\n+\t\t\t\t// we need to read from our cached buffer\n+\t\t\t\tauto buffer_read_count = MinValue<idx_t>(nr_bytes, buffer_size - read_position);\n+\t\t\t\tmemcpy(buffer, cached_buffer.get() + read_position, buffer_read_count);\n+\t\t\t\tresult_offset += buffer_read_count;\n+\t\t\t\tread_position += buffer_read_count;\n+\t\t\t\tif (result_offset == nr_bytes) {\n+\t\t\t\t\treturn nr_bytes;\n+\t\t\t\t}\n+\t\t\t} else if (!reset_enabled && cached_buffer) {\n+\t\t\t\t// reset is disabled but we still have cached data\n+\t\t\t\t// we can remove any cached data\n+\t\t\t\tcached_buffer.reset();\n+\t\t\t\tbuffer_size = 0;\n+\t\t\t\tbuffer_capacity = 0;\n+\t\t\t\tread_position = 0;\n+\t\t\t}\n+\t\t\t// we have data left to read from the file\n+\t\t\t// read directly into the buffer\n+\t\t\tauto bytes_read = file_handle->Read((char *)buffer + result_offset, nr_bytes - result_offset);\n+\t\t\tread_position += bytes_read;\n+\t\t\tif (reset_enabled) {\n+\t\t\t\t// if reset caching is enabled, we need to cache the bytes that we have read\n+\t\t\t\tif (buffer_size + bytes_read >= buffer_capacity) {\n+\t\t\t\t\t// no space; first enlarge the buffer\n+\t\t\t\t\tbuffer_capacity = MaxValue<idx_t>(NextPowerOfTwo(buffer_size + bytes_read), buffer_capacity * 2);\n+\n+\t\t\t\t\tauto new_buffer = unique_ptr<data_t[]>(new data_t[buffer_capacity]);\n+\t\t\t\t\tif (buffer_size > 0) {\n+\t\t\t\t\t\tmemcpy(new_buffer.get(), cached_buffer.get(), buffer_size);\n+\t\t\t\t\t}\n+\t\t\t\t\tcached_buffer = move(new_buffer);\n+\t\t\t\t}\n+\t\t\t\tmemcpy(cached_buffer.get() + buffer_size, (char *)buffer + result_offset, bytes_read);\n+\t\t\t\tbuffer_size += bytes_read;\n+\t\t\t}\n+\n+\t\t\treturn result_offset + bytes_read;\n+\t\t} else {\n+\t\t\treturn file_handle->Read(buffer, nr_bytes);\n+\t\t}\n+\t}\n+\n+\tstring ReadLine() {\n+\t\tstring result;\n+\t\tchar buffer[1];\n+\t\twhile (true) {\n+\t\t\tidx_t tuples_read = Read(buffer, 1);\n+\t\t\tif (tuples_read == 0 || buffer[0] == '\\n') {\n+\t\t\t\treturn result;\n+\t\t\t}\n+\t\t\tif (buffer[0] != '\\r') {\n+\t\t\t\tresult += buffer[0];\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tvoid DisableReset() {\n+\t\tthis->reset_enabled = false;\n+\t}\n+\n+private:\n+\tunique_ptr<FileHandle> file_handle;\n+\tbool reset_enabled = true;\n+\tbool can_seek = false;\n+\tbool plain_file_source = false;\n+\tidx_t file_size = 0;\n+\t// reset support\n+\tunique_ptr<data_t[]> cached_buffer;\n+\tidx_t read_position = 0;\n+\tidx_t buffer_size = 0;\n+\tidx_t buffer_capacity = 0;\n+};\n+\n void BufferedCSVReaderOptions::SetDelimiter(const string &input) {\n \tthis->delimiter = StringUtil::Replace(input, \"\\\\t\", \"\\t\");\n \tthis->has_delimiter = true;\n@@ -154,6 +273,13 @@ BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOp\n                         requested_types) {\n }\n \n+BufferedCSVReader::~BufferedCSVReader() {\n+}\n+\n+idx_t BufferedCSVReader::GetFileSize() {\n+\treturn file_handle ? file_handle->FileSize() : 0;\n+}\n+\n void BufferedCSVReader::Initialize(const vector<LogicalType> &requested_types) {\n \tPrepareComplexParser();\n \tif (options.auto_detect) {\n@@ -170,6 +296,9 @@ void BufferedCSVReader::Initialize(const vector<LogicalType> &requested_types) {\n \t\tSkipRowsAndReadHeader(options.skip_rows, options.header);\n \t}\n \tInitParseChunk(sql_types.size());\n+\t// we only need reset support during the automatic CSV type detection\n+\t// since reset support might require caching (in the case of streams), we disable it for the remainder\n+\tfile_handle->DisableReset();\n }\n \n void BufferedCSVReader::PrepareComplexParser() {\n@@ -178,12 +307,10 @@ void BufferedCSVReader::PrepareComplexParser() {\n \tquote_search = TextSearchShiftArray(options.quote);\n }\n \n-unique_ptr<FileHandle> BufferedCSVReader::OpenCSV(const BufferedCSVReaderOptions &options) {\n-\tauto result = fs.OpenFile(options.file_path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK,\n-\t                          options.compression, this->opener);\n-\tplain_file_source = result->OnDiskFile() && result->CanSeek();\n-\tfile_size = result->GetFileSize();\n-\treturn result;\n+unique_ptr<CSVFileHandle> BufferedCSVReader::OpenCSV(const BufferedCSVReaderOptions &options) {\n+\tauto file_handle = fs.OpenFile(options.file_path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK,\n+\t                               options.compression, this->opener);\n+\treturn make_unique<CSVFileHandle>(move(file_handle));\n }\n \n // Helper function to generate column names\n@@ -349,7 +476,7 @@ bool BufferedCSVReader::JumpToNextSample() {\n \t// assess if it makes sense to jump, based on size of the first chunk relative to size of the entire file\n \tif (sample_chunk_idx == 0) {\n \t\tidx_t bytes_first_chunk = bytes_in_chunk;\n-\t\tdouble chunks_fit = (file_size / (double)bytes_first_chunk);\n+\t\tdouble chunks_fit = (file_handle->FileSize() / (double)bytes_first_chunk);\n \t\tjumping_samples = chunks_fit >= options.sample_chunks;\n \n \t\t// jump back to the beginning\n@@ -364,7 +491,7 @@ bool BufferedCSVReader::JumpToNextSample() {\n \n \t// if we deal with any other sources than plaintext files, jumping_samples can be tricky. In that case\n \t// we just read x continuous chunks from the stream TODO: make jumps possible for zipfiles.\n-\tif (!plain_file_source || !jumping_samples) {\n+\tif (!file_handle->PlainFileSource() || !jumping_samples) {\n \t\tsample_chunk_idx++;\n \t\treturn true;\n \t}\n@@ -374,13 +501,13 @@ bool BufferedCSVReader::JumpToNextSample() {\n \tbytes_per_line_avg = ((bytes_per_line_avg * (sample_chunk_idx)) + bytes_per_line) / (sample_chunk_idx + 1);\n \n \t// if none of the previous conditions were met, we can jump\n-\tidx_t partition_size = (idx_t)round(file_size / (double)options.sample_chunks);\n+\tidx_t partition_size = (idx_t)round(file_handle->FileSize() / (double)options.sample_chunks);\n \n \t// calculate offset to end of the current partition\n \tint64_t offset = partition_size - bytes_in_chunk - remaining_bytes_in_buffer;\n \tauto current_pos = file_handle->SeekPosition();\n \n-\tif (current_pos + offset < file_size) {\n+\tif (current_pos + offset < file_handle->FileSize()) {\n \t\t// set position in stream and clear failure bits\n \t\tfile_handle->Seek(current_pos + offset);\n \n@@ -391,10 +518,10 @@ bool BufferedCSVReader::JumpToNextSample() {\n \t\t// seek backwards from the end in last chunk and hope to catch the end of the file\n \t\t// TODO: actually it would be good to make sure that the end of file is being reached, because\n \t\t// messy end-lines are quite common. For this case, however, we first need a skip_end detection anyways.\n-\t\tfile_handle->Seek(file_size - bytes_in_chunk);\n+\t\tfile_handle->Seek(file_handle->FileSize() - bytes_in_chunk);\n \n \t\t// estimate linenr\n-\t\tlinenr = (idx_t)round((file_size - bytes_in_chunk) / bytes_per_line_avg);\n+\t\tlinenr = (idx_t)round((file_handle->FileSize() - bytes_in_chunk) / bytes_per_line_avg);\n \t\tlinenr_estimated = true;\n \t}\n \n@@ -548,6 +675,7 @@ void BufferedCSVReader::DetectDialect(const vector<LogicalType> &requested_types\n \n \t\t\t\t\tJumpToBeginning(original_options.skip_rows);\n \t\t\t\t\tsniffed_column_counts.clear();\n+\n \t\t\t\t\tif (!TryParseCSV(ParserMode::SNIFFING_DIALECT)) {\n \t\t\t\t\t\tcontinue;\n \t\t\t\t\t}\ndiff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp\nindex 2ea8828031cd..07e137a9a031 100644\n--- a/src/function/table/read_csv.cpp\n+++ b/src/function/table/read_csv.cpp\n@@ -169,7 +169,7 @@ static unique_ptr<FunctionOperatorData> ReadCSVInit(ClientContext &context, cons\n \t\tresult->csv_reader = make_unique<BufferedCSVReader>(context, bind_data.options, bind_data.sql_types);\n \t}\n \tbind_data.bytes_read = 0;\n-\tbind_data.file_size = result->csv_reader->file_size;\n+\tbind_data.file_size = result->csv_reader->GetFileSize();\n \tresult->file_index = 1;\n \treturn move(result);\n }\ndiff --git a/src/include/duckdb/common/pipe_file_system.hpp b/src/include/duckdb/common/pipe_file_system.hpp\nindex 4bf5adfb5e8c..e12f11d95da6 100644\n--- a/src/include/duckdb/common/pipe_file_system.hpp\n+++ b/src/include/duckdb/common/pipe_file_system.hpp\n@@ -21,6 +21,7 @@ class PipeFileSystem : public FileSystem {\n \n \tint64_t GetFileSize(FileHandle &handle) override;\n \n+\tvoid Reset(FileHandle &handle) override;\n \tbool OnDiskFile(FileHandle &handle) override {\n \t\treturn false;\n \t};\ndiff --git a/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp b/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp\nindex 121a673ee343..cbaa9f98e573 100644\n--- a/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp\n+++ b/src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp\n@@ -20,6 +20,7 @@\n \n namespace duckdb {\n struct CopyInfo;\n+struct CSVFileHandle;\n struct FileHandle;\n struct StrpTimeFormat;\n \n@@ -120,15 +121,14 @@ class BufferedCSVReader {\n \n \tBufferedCSVReader(FileSystem &fs, FileOpener *opener, BufferedCSVReaderOptions options,\n \t                  const vector<LogicalType> &requested_types = vector<LogicalType>());\n+\t~BufferedCSVReader();\n \n \tFileSystem &fs;\n \tFileOpener *opener;\n \tBufferedCSVReaderOptions options;\n \tvector<LogicalType> sql_types;\n \tvector<string> col_names;\n-\tunique_ptr<FileHandle> file_handle;\n-\tbool plain_file_source = false;\n-\tidx_t file_size = 0;\n+\tunique_ptr<CSVFileHandle> file_handle;\n \n \tunique_ptr<char[]> buffer;\n \tidx_t buffer_size;\n@@ -160,6 +160,8 @@ class BufferedCSVReader {\n \t//! Extract a single DataChunk from the CSV file and stores it in insert_chunk\n \tvoid ParseCSV(DataChunk &insert_chunk);\n \n+\tidx_t GetFileSize();\n+\n private:\n \t//! Initialize Parser\n \tvoid Initialize(const vector<LogicalType> &requested_types);\n@@ -206,7 +208,7 @@ class BufferedCSVReader {\n \t//! Reads a new buffer from the CSV file if the current one has been exhausted\n \tbool ReadBuffer(idx_t &start);\n \n-\tunique_ptr<FileHandle> OpenCSV(const BufferedCSVReaderOptions &options);\n+\tunique_ptr<CSVFileHandle> OpenCSV(const BufferedCSVReaderOptions &options);\n \n \t//! First phase of auto detection: detect CSV dialect (i.e. delimiter, quote rules, etc)\n \tvoid DetectDialect(const vector<LogicalType> &requested_types, BufferedCSVReaderOptions &original_options,\n",
  "test_patch": "diff --git a/tools/shell/shell-test.py b/tools/shell/shell-test.py\nindex eb201d1ca7d3..f046cc7d2a20 100644\n--- a/tools/shell/shell-test.py\n+++ b/tools/shell/shell-test.py\n@@ -540,19 +540,36 @@ def tf():\n \n if os.name != 'nt':\n      test('''\n-     create table mytable as select * from\n-     read_csv('/dev/stdin',\n-       columns=STRUCT_PACK(foo := 'INTEGER', bar := 'INTEGER', baz := 'VARCHAR'),\n-       AUTO_DETECT='false'\n-     );\n-     select * from mytable limit 1;\n-     ''',\n+create table mytable as select * from\n+read_csv('/dev/stdin',\n+  columns=STRUCT_PACK(foo := 'INTEGER', bar := 'INTEGER', baz := 'VARCHAR'),\n+  AUTO_DETECT='false'\n+);\n+select * from mytable limit 1;''',\n      extra_commands=['-csv', ':memory:'],\n      input_file='test/sql/copy/csv/data/test/test.csv',\n      out='''foo,bar,baz\n 0,0,\" test\"''')\n \n      test('''\n+create table mytable as select * from\n+read_csv_auto('/dev/stdin');\n+select * from mytable limit 1;\n+''',\n+          extra_commands=['-csv', ':memory:'],\n+          input_file='test/sql/copy/csv/data/test/test.csv',\n+          out='''column0,column1,column2\n+0,0,\" test\"''')\n+\n+     test('''create table mytable as select * from\n+read_csv_auto('/dev/stdin');\n+select channel,i_brand_id,sum_sales,number_sales from mytable;\n+          ''',\n+          extra_commands=['-csv', ':memory:'],\n+          input_file='data/csv/tpcds_14.csv',\n+          out='''web,8006004,844.21,21''')\n+\n+     test('''\n      COPY (SELECT 42) TO '/dev/stdout' WITH (FORMAT 'csv');\n      ''',\n      extra_commands=['-csv', ':memory:'],\n",
  "problem_statement": "Feature: support read_csv_auto reading from the '/dev/stdin' file\nThe duckdb CLI command can read from a file using read_csv_auto():\r\n\r\n```bash\r\nduckdb mydb \"create table mytable as select * from read_csv_auto('mydata.csv', HEADER=TRUE);\"\r\n```\r\n\r\nWhen I try to send the same data to the /dev/stdin file and have duckdb load it from there, it fails:\r\n\r\n```bash\r\ncat mydata.csv | duckdb mydb \"create table mytable as select * from read_csv_auto('/dev/stdin', HEADER=TRUE);\" \r\n```\r\n\r\nThe error is:\r\n\r\n```bash\r\nError: Invalid Input Error: Error in file \"/dev/stdin\": CSV options could not be auto-detected. Consider setting parser options manually.\r\n```\n",
  "hints_text": "Maybe @tdoehmen can  have a look?\nJust sharing some thoughts on the use cases. The duckdb CLI shell offers similar capabilities as sqlite3 for moving data in and out using both special \"dot commands\" and \"SQL\" command statements. Automating loading and export from the commandline, some of these so called dot-commands are supported: https://sqlite.org/cli.html... \r\n\r\nI think some common dot commands are `.import`, another is `.dump`, for reading and writing data in and out of the process. There is also `.output` (with different `.mode` settings). It seems those dot commands in sqlite3 have grown organically and there are now quite a few of these in sqlite3. In duckdb CLI shell, there seems to be support for `.import` and other dot commands but not a 1-to-1 match with all sqlite3 dot commands.\r\n\r\nIt seems there are faster and better duckdb-\"sql\"-statements for reading and writing data into and out from the duckdb process, such as maybe using:\r\n\r\n- \"create table if not exists stdin as select * from read_csv_auto('/dev/stdin')\" # for reading data \r\n- \"export database '/dev/stdout' (FORMAT PARQUET);\" # for writing data output (binary)\r\n- \"copy stdin to '/dev/stdout' (FORMAT CSV);\" # for writing data in a table called stdin to an output file (in text format)\r\n\r\nWhen reading and writing streams of data coming from stdin and going to stdout, it would be nice to have easy access to these files/streams, to use some of these faster duckdb specific import and export functions with less intermediate steps. It would be nice to not have to create various temp files and directories, unless required, for example due to larger data which cannot fit in RAM.\r\n\r\nThe dot commands can be piped to sqlite3, but not easily \"data\" so duckdb would have an edge if load and dump functions were to work with data from stdin, and results going to stdout. For sqlite3, to alleviate this, there are tools like https://tobimensch.github.io/termsql/ provided outside of sqlite3. Or shell wrappers can be used; here are some links that describe some usage/workarounds with sqlite3:\r\n  \r\n- https://unix.stackexchange.com/questions/288273/bash-redirection-doesnt-work-for-sqlite-command\r\n- https://stackoverflow.com/questions/41354486/sqlite-how-to-simultaneously-read-data-from-stdin-and-table-name-from-variable\r\n\r\nHere is a [rough attempt to do an R script](https://gist.github.com/mskyttner/46601e60138a886cdfbfc81067c71725) \"outside of duckdb shell/CLI\" which reads from stdin and writes to stdout. It is quite quick and dirty, just adding it to illustrate use cases that could be nice to support on the commandline, in my opinion, for example (after copying the script to ~/bin and doing chmod +x) ...\r\n\r\n```bash\r\ndf | tail -n +2 | duckstream.R --no-header -d \" \" -Q \"select X1,X2 from stdin;\" --dump myparquetoutput\r\n```\r\n\n@hannesmuehleisen I am not familiar with the cli integration of duckdb, so I only have a limited view on this. \r\nIn the csv reader, I could imagine that it's possible to treat `path=/dev/stdin` as magic value, which makes the parser read from stdin with something like `std::getline(std::cin,line)`. The parser would need to treat it similar to a .gz file (not the compression, but that data has to be read sequentially and that we can't jump to other positions in the input), which means that dialect and data type detection for CSV will only work to a limited extend. \r\n\r\nedit: one more thing - this would only work in conjunction with `create table` (not with views or anything else where the input could potentially be read more than once), unless the data from stdin gets buffered somewhere. Or am I missing something?\r\n\r\nedit 2: When it's only about conversion from CSV -> PARQUET/CSV, maybe it's worth offering this functionality through a dedicated API? \ud83e\udd14 \nBulk loading functionality through command line tools with input from stdin is supported in some databases, for example https://mariadb.com/kb/en/columnstore-bulk-data-loading/#bulking-loading-data-from-stdin. It would be pretty nice if that was built into the duckdb shell / CLI command instead of being provided \"on the outside\" by a separate utility. I guess that is what some of the dot commands in sqlite do, extend the shell with input/output features.\r\n\r\nThe use cases I had in mind when opening this feature wish was not only specifically about converting from CSV to PARQUET but also about being able to avoid reading data from or to disk altogether, like in the examples given here: https://github.com/tobimensch/termsql/#examples. But specifically for the example use case with exporting to parquet ... I admit that I use the CSV->duckdb->PARQUET a lot now, until the duckdb file format settles. \r\n\r\nWould it be possible to buffer the data from stdin in chunks into a \"buffer\" duckdb table (maybe single column, varchar), with chunkwise parse/move operations to final destination happening from there? First chunk would be a lookahead of n lines, used to guess data types, separators etc (if not specified), and all subsequent chunks of m lines each would be parsed with those settings?\r\n\r\nIf someone sends an endless stream (by mistake or as a Denial-of-Service attempt), it would be great to see some progress and finally warning / abort before system resources get depleted.\nThank you for the clarification and the information. Then, a more general solution would indeed make more sense.  \r\n\r\nMaybe buffering to a table could indeed save some headache around disk-spilling! It doesn't feel like a super clean solution (i'd prefer to buffer the raw data - if at all), but maybe it's a good starting point. One option to go without buffering would be to allow the use of stdin input only for specific queries which are guaranteed to need the input only once (e.g. COPY ... INTO). \r\n\r\nEndless input streams would, however, still remain a problem, thanks for the hint. I think that a line-break could generally be used to mark the end of the stream. Maybe an option like max_lines could be added to prevent endless reading from the stdin-stream if the line-break never comes. \r\n\r\nI can unfortunately not promise an immediate solution to this, as I am not familiar with the CLI/shell integration yet and also have to give the buffering aspect another thought.  But I will investigate and come back to you when I hopefully know more ; )\nI would highly value (low level) streaming in and out APIs for DuckDB. First, streaming in general speeds up data processing and would enable nice stream processing use cases, buffer as much as needed/wanted, e.g. for window functions, but generally streaming allows processing huge amounts of data with small amounts of memory.\r\n\r\nFurthermore, as a generalisation, DuckDB could read streams as any tables, and cache them into (compressed) memory if full \"scan\" is needed for column(s). CSV is anyway a (linear?) data source as well (varying row lengths). It would be badass cool to chain DuckDB to DuckDB.\r\n\r\nEdit: And not only DuckDB to DuckDB but from/to any data source, if the data transfer is standardised (like apache arrow flight).\nWe have streaming APIs both for reading table data and to retrieve results. Could you be more precise what you are missing? \n> We have streaming APIs both for reading table data and to retrieve results. Could you be more precise what you are missing?\r\n\r\nMy knowledge of the APIs is still limited, and more around node module we have. I'm thinking about query pushdown from DuckDB to DuckDB and being able to stream the response in a \"standard\" data format that is easily consumable (e.g. the apache arrow flight that was referred to in some other issue). Almost like read replication, but instead of full data replication, just stream based on a query and having the received data in a queryable format for in-memory DuckDB table for example.\nYou can use `Connection::SendQuery(const string &query);`, which will yield a generic `QueryResult`, which is usually going to be a streaming result. On that result you can then call `QueryResult::Fetch()` which will yield a `DataChunk` that holds a chunk of result rows. If you want a more generic representation of that chunk you can call `DataChunk::ToArrowArray(ArrowArray *out_array)` which will convert the chunk to an arrow-compatible Array. \r\n\n> We have streaming APIs both for reading table data and to retrieve results. Could you be more precise what you are missing?\r\n\r\n@hannesmuehleisen\r\n\r\nI can explain what's missing.\r\nDuckDB uses `seek` to check for csv type and auto detect stuff.\r\nBut that wouldn't work with streamable csv files, for example named pipes or http sources.\r\nFor the use case of: \"load csv into RAM from external source and then do some queries\" it becomes cumbersome pretty fast.\r\nIt expects fully functional POSIX file system with random access to files __just__ to load the file into RAM (`CanSeek()` always returns `true`, hardcoded), which is a pretty far reaching assumption.\r\nI can probably do a PR on how to fix that, but I'm not sure it's a wanted change, judging by no support for STDIN sources.\r\n\r\nContrived example of what I'm doing:\r\n```python\r\nimport os\r\nimport tempfile\r\nimport duckdb\r\nfrom multiprocessing import Process\r\n\r\ntmpdir = tempfile.mkdtemp()\r\ndata_file = os.path.join(tmpdir, 'data.csv')\r\nos.mkfifo(data_file)\r\n\r\ncon = duckdb.connect(database=':memory:', read_only=False)\r\n\r\ncon.execute(\"DROP TABLE if exists cities\")\r\ncon.execute(\"CREATE TABLE cities (latd integer, latm integer, lats integer, ns varchar, lond integer, lonm integer, lons integer, ew varchar, city varchar, state varchar)\")\r\n\r\n\r\ndef pipe_data(src, dst):\r\n    fifo = open(dst, 'wb')\r\n    with open(src, 'rb') as file:\r\n        while chunk := file.read(1024):\r\n            fifo.write(chunk)\r\n\r\n\r\ndef load_data(event, context):\r\n    path = 'cities.csv'\r\n    process = Process(target=pipe_data, args=(path, data_file,))\r\n    process.start()\r\n\r\n    con.execute('COPY cities FROM \\'{file_name}\\' ( HEADER )'.format(file_name=data_file))\r\n    process.join()\r\n```\r\n\r\nI'm open to hear what I can do better here.\n> I can explain what's missing.\r\n> DuckDB uses `seek` to check for csv type and auto detect stuff.\r\n> But that wouldn't work with streamable csv files, for example named pipes or http sources.\r\n> For the use case of: \"load csv into RAM from external source and then do some queries\" it becomes cumbersome pretty fast.\r\n> It expects fully functional POSIX file system with random access to files **just** to load the file into RAM (`CanSeek()` always returns `true`, hardcoded), which is a pretty far reaching assumption.\r\n\r\nIf `CanSeek()` returns true, the CSV reader uses seek to jump around in the file to look for samples for better auto detection. If `CanSeek()` is false it will instead only read from the beginning of the stream to do the type detection.\r\n\r\nIt is true that the regular file system (which is supposed to be a\"standard/POSIX file system) always returns true for CanSeek. This is not hard-coded, however. The method is virtual and can be overriden/replaced. For example, the [gzip file system](https://github.com/duckdb/duckdb/blob/30fda3efeb7ca07390a84e787c3110ede289153f/src/include/duckdb/common/gzip_file_system.hpp) returns false here (since random seeking inside gzip files is difficult/expensive). \r\n\r\nFor pipes I would recommend using a new file system as well, since pipes are (as you mentioned) not a regular file system and do not support the same set of operations that regular file systems do.\r\n\r\nI do still see one problem in supporting pipes there, however. The file system will call the `Reset` method on the file, which will still be problematic for pipes. Instead, ideally the CSV reader would need a minor redesign so it would re-use its own internal buffer instead of calling reset.\r\n\r\nAlso, minor note, CSV files [already work with HTTP sources](https://github.com/duckdb/duckdb/blob/30fda3efeb7ca07390a84e787c3110ede289153f/test/sql/copy/csv/test_csv_remote.test) ;)\r\n\r\n> I can probably do a PR on how to fix that, but I'm not sure it's a wanted change, judging by no support for STDIN sources.\r\n\r\nSupporting pipes for CSV file reading (and in general) seems like a good idea, no objections from my side. Happy to review a PR on this!\r\n\n@Mytherin \r\nWorking http sources is pretty nice.\r\nBut for `PipeFileSystem` I'm not sure how it should be chosen? In posix a pipe is just a file. It has no specific schema like http or extension like gzip.\r\nOr better, can you point me in the direction of the code that does the \"fle system specialization\"?\r\n\r\nP.S. and the `Reset()` comment is spot on. The buffered csv reader cannot reset stream at will, but it's quite happily doing it right now :)\n> Or better, can you point me in the direction of the code that does the \"fle system specialization\"?\r\n\r\nThe file system specialization happens [here](https://github.com/duckdb/duckdb/blob/14193fb87eb88566c44a417a73c3e50a4b0da8cc/src/common/file_system.cpp#L1014) for gzip files.\r\n\r\n> P.S. and the Reset() comment is spot on. The buffered csv reader cannot reset stream at will, but it's quite happily doing it right now :)\r\n\r\nThat is true and should probably be changed. However, do note that the auto-detect is optional and can be disabled. So even without any changes to the code streaming through a pipe might work already if auto-detect is disabled and the schema is manually specified. \n> The file system specialization happens here for gzip files.\r\n\r\nHmm, sorry to bother you so much, but where it decides if `http` should be used then?\r\n\r\n> That is true and should probably be changed. However, do note that the auto-detect is optional and can be disabled. So even without any changes to the code streaming through a pipe might work already if auto-detect is disabled and the schema is manually specified.\r\n\r\nTried defining the schema and it still tried to seek it.\r\nI think auto_detect is off by default?\r\nIt looks like it will `Reset()` anyway here: https://github.com/duckdb/duckdb/blob/45c9cb51852ecd14338f79854fe898d70054e8e7/src/execution/operator/persistent/buffered_csv_reader.cpp#L140\n> Hmm, sorry to bother you so much, but where it decides if http should be used then?\r\n\r\nThat happens in the `FindFileSystem(path)`, which uses the `CanHandleFile` to figure out which file system to use ([which is overloaded here for the httpfs](https://github.com/duckdb/duckdb/blob/30fda3efeb7ca07390a84e787c3110ede289153f/extension/httpfs/httpfs.cpp#L180)).\r\n\r\n> I think auto_detect is off by default?\r\n\r\nIt is turned on by default, but indeed it seems that the JumpToBeginning is still called in either case. That can be changed as a reset should not be required in that scenario.\n>It is turned on by default, but indeed it seems that the JumpToBeginning is still called in either case. That can be changed as a reset should not be required in that scenario.\r\n\r\nYup. \r\n\r\nIt also looks like the http loader is not in the latest (0.2.7) release yet?\r\n\r\n```python\r\nhttp_url = 'https://people.sc.fsu.edu/~jburkardt/data/csv/cities.csv'\r\ncon.execute('COPY cities FROM \\'{file_name}\\' ( HEADER )'.format(file_name=http_url))\r\n```\r\n\r\n```bash\r\nRuntimeError: IO Error: No files found that match the pattern \"https://people.sc.fsu.edu/~jburkardt/data/csv/cities.csv\"\r\n```\r\n\nIt is supported, but you need to enable the httpfs extension (`BUILD_HTTPFS=1 make` if you are building from source, or add it to the extensions list in `tools/pythonpkg/setup.py`). It is not enabled by default because of the dependency on OpenSSL.\r\n\r\nWe are still working on making it easier to have extensions loaded externally without needing to compile from source, so you could do e.g. `pip install duckdb-httpfs` and install the extension separately. That is not available yet unfortunately. \nRegarding the use case to \"load csv into RAM from external source and then do some queries\" using `read_csv_auto`, the automatic type guessing enabled by default is very useful and convenient. I can read a csv from a web url with the duckdb CLI and output results to stdout from a select query in one big chunk:\r\n\r\n```bash\r\ncurl -s https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv | sed 's/|/,/g' > /tmp/stdin \\\r\n&& cat /tmp/stdin | duckdb -csv :memory: \"create table mytable as select * from read_csv_auto('/tmp/stdin', HEADER=FALSE); select * from mytable;\"\r\n```\r\n\r\nWith duckdb CLI v0.2.7 this works so httpsfs is supported there:\r\n\r\n```bash\r\nduckdb -csv :memory: \"create table mytable as select * from read_csv_auto('https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv', HEADER=FALSE, SEP='|'); select * from mytable;\"\r\n```\r\n\r\nIf the duckdb CLI could read directly from the stdin stream, maybe even with a progress indicator for chunkwise processing steps, there would be no need to create extra temp files or write external utilities to load data incrementally in chunks (as in https://github.com/duckdb/duckdb/issues/1417#issuecomment-867506887 and https://gist.github.com/mskyttner/46601e60138a886cdfbfc81067c71725).\r\n\r\nHere is a duckdb CLI v 0.2.7 attempt to load data without the temp file:\r\n\r\n```bash\r\ncurl -s https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv | sed 's/|/,/g' \\\r\n| duckdb -csv :memory: \"create table mytable as select * from read_csv_auto('/dev/stdin', HEADER=TRUE); select * from mytable;\"\r\n```\r\nThe error reported is:\r\n\r\n`Error: IO Error: Could not seek to location 0 for file \"/dev/stdin\": Illegal seek`\r\n\r\nWould fixing the JumpToBeginning issue help here?\r\n\r\n\n> Would fixing the JumpToBeginning issue help here?\r\n\r\nYup, looks like it. Just need to detect that the stream is unseekable and then chose correct fs implementation. After getting rid of `JumpToBeginning`\n> It is supported, but you need to enable the httpfs extension (`BUILD_HTTPFS=1 make` if you are building from source, or add it to the extensions list in `tools/pythonpkg/setup.py`). It is not enabled by default because of the dependency on OpenSSL.\r\n\r\nHmm, I cannot build a python package.\r\n```bash\r\ncd tools/pythonpkg\r\npython setup.py sdist\r\n\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 99, in <module>\r\n    (source_list, include_list, original_sources) = package_build.build_package(os.path.join(script_path, 'duckdb'), extensions)\r\n  File \"/home/user/git/duckdb/tools/pythonpkg/../../scripts/package_build.py\", line 149, in build_package\r\n    include_package(ext, ext_path, include_files, include_list, source_list)\r\n  File \"/home/user/git/duckdb/tools/pythonpkg/../../scripts/package_build.py\", line 105, in include_package\r\n    ext_pkg = __import__(pkg_name + '_config')\r\nModuleNotFoundError: No module named 'httpfs_config'\r\n\r\n```\r\n\r\nThe build with `BUILD_HTTPFS=1 make` succeeds though\r\n\r\nAh, I see no `http_config.py` is there, need to define one.\n@mskyttner \r\n\r\nThe following will never work, because it needs to sample data which is impossible when streaming\r\n```\r\ncurl -s https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv | sed 's/|/,/g' \\\r\n| duckdb -csv :memory: \"create table mytable as select * from read_csv_auto('/dev/stdin', HEADER=TRUE); select * from mytable;\"\r\n```\r\nThis one will work soon:\r\n```\r\ncurl -s https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv | sed 's/|/,/g' \\\r\n| duckdb -csv :memory: \"create table mytable as select * from read_csv('/dev/stdin', columns=STRUCT_PACK(foo := 'INTEGER', bar := 'VARCHAR', baz := 'VARCHAR', bam := 'VARCHAR'), AUTO_DETECT='false'); select * from mytable;\"\r\n```\nNice, this should fix the `Error: IO Error: Could not seek to location 0 for file \"/dev/stdin\": Illegal seek` mentioned earlier! \r\n\r\nRegarding the \"impossible when streaming\" statement when using `read_csv_auto`, can you help me to understand better the challenge there? The read_csv_auto type guessing/detection by default uses random seek across the whole file to sample rows and base type guessing on this sample, right? For a chunked line buffered stream of CSV data this would not work since we only have individual chunks of data to work with? Is this why you say it would never work to use read_csv_auto on a buffered text stream to do the type guessing for the CSV columns? \r\n\r\nBut `read_csv_auto` allows for for type guessing using random sampling across the whole file to be switched off, since it supports [options SAMPLE_SIZE and ALL_VARCHAR](https://duckdb.org/docs/data/csv)? What about then (for a line buffered text stream) \"downgrading\" the type guessing strategy to either treat all columns as varchar (worst case) or to buffer the first thousand rows and base types on this sample rather than a random sample across all rows (if later chunks held lines that were \"non-conformant\" either the columns could be \"demoted\" to a less specific type such as varchar or replaced by NA while parsing issues could be logged (to stderr?) [in table format similar to how readr::read_csv does this](https://readr.tidyverse.org/reference/problems.html)? \r\n\r\nOr to add CHUNK_SIZE and use a combination of a lookahead and chunksize settings, such as in [this wrapper](https://gist.github.com/mskyttner/46601e60138a886cdfbfc81067c71725#file-duckstream-r-L37-L46)? This way column types and names (and number of columns) would not be need to be specified before reading the stream of CSV lines. Clean-up could later be done in separate steps, when data has landed in the database.\r\n\r\n\n> But read_csv_auto allows for for type guessing using random sampling across the whole file to be switched off, since it supports options SAMPLE_SIZE and ALL_VARCHAR? What about then (for a line buffered text stream) \"downgrading\" the type guessing strategy to either treat all columns as varchar (worst case) or to buffer the first thousand rows and base types on this sample rather than a random sample across all rows (if later chunks held lines that were \"non-conformant\" either the columns could be \"demoted\" to a less specific type such as varchar or replaced by NA while parsing issues could be logged (to stderr?) in table format similar to how readr::read_csv does this?\r\n\r\n> Or to add CHUNK_SIZE and use a combination of a lookahead and chunksize settings, such as in this wrapper? This way column types and names (and number of columns) would not be need to be specified before reading the stream of CSV lines. Clean-up could later be done in separate steps, when data has landed in the database.\r\n\r\nYep, both of these could be done, but it's much more involved and will need another feature (or two).\r\n\"Impossible\" in a strict streaming sense where you want to keep as low of a memory footprint as possible (I usually prefer <=64KB) and just pump data through as fast as possible.\r\n\nWith version 0.2.8, this duckdb CLI command works (which is great!):\r\n\r\n`curl -s https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv | sed 's/|/,/g' | duckdb -csv :memory: \"create table mytable as select * from read_csv('/dev/stdin', columns=STRUCT_PACK(foo := 'INTEGER', bar := 'VARCHAR', baz := 'VARCHAR', bam := 'VARCHAR', a := 'VARCHAR', b := 'VARCHAR', c := 'VARCHAR', d := 'VARCHAR', e := 'VARCHAR', f := 'VARCHAR', g := 'VARCHAR', h := 'VARCHAR', i := 'VARCHAR', j := 'VARCHAR'), AUTO_DETECT='false'); select * from mytable;\"\r\n`\r\n\r\nThe original issue was about using `read_csv_auto` rather than `read_csv`, since it would be quite nice to avoid specifying the column types explicitly. When swapping for this in the command above it results in `Error: Not implemented Error: Unsupported: Reset pipe/stream` which is a little unexpected perhaps, since it is an alias to `read_csv` according to the documentation? \r\n\r\nWould it be a good idea to open another issue to try to capture those features needed as mentioned by @pkit, in order to be able to do chunked line buffered reading from stdin using duckb CLI?\r\n\r\nIt is possible to do `duckdb -csv :memory: \"select * from read_csv_auto('https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv')\"` which is great but does not allow \"pre-processing\" of the content going in using a chain of various command line utilities. \r\n\r\nFor example, it would be nice to be able to send the output (below unchanged) further to another duckdb CLI call in another step:\r\n\r\n`duckdb -csv :memory: \"select * from read_csv_auto('https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv')\" | duckdb -csv :memory: \"select * from read_csv_auto('/dev/stdin');\"`\r\n\r\n\n@mskyttner I've opened your idea to enable `read_csv_auto` as a feature request in #2829 ",
  "created_at": "2021-12-28T17:48:04Z"
}