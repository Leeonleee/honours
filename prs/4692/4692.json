{
  "repo": "duckdb/duckdb",
  "pull_number": 4692,
  "instance_id": "duckdb__duckdb-4692",
  "issue_numbers": [
    "4689"
  ],
  "base_commit": "794992a3886b7773213603ac8e50bd3bd4b041ab",
  "patch": "diff --git a/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp b/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp\nindex b5b5d6433bed..23918a6075c4 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/python_conversion.hpp\n@@ -19,6 +19,27 @@\n \n namespace duckdb {\n \n+enum class PythonObjectType {\n+\tOther,\n+\tNone,\n+\tInteger,\n+\tFloat,\n+\tBool,\n+\tDecimal,\n+\tUuid,\n+\tDatetime,\n+\tDate,\n+\tTime,\n+\tTimedelta,\n+\tString,\n+\tByteArray,\n+\tMemoryView,\n+\tBytes,\n+\tList,\n+\tDict,\n+\tNdArray,\n+};\n+\n bool TryTransformPythonNumeric(Value &res, py::handle ele);\n bool DictionaryHasMapFormat(const PyDictionary &dict);\n Value TransformPythonValue(py::handle ele, const LogicalType &target_type = LogicalType::UNKNOWN,\ndiff --git a/tools/pythonpkg/src/python_conversion.cpp b/tools/pythonpkg/src/python_conversion.cpp\nindex 1d5d96f39201..aa5a2df87592 100644\n--- a/tools/pythonpkg/src/python_conversion.cpp\n+++ b/tools/pythonpkg/src/python_conversion.cpp\n@@ -201,59 +201,119 @@ bool TryTransformPythonNumeric(Value &res, py::handle ele) {\n \treturn true;\n }\n \n-Value TransformPythonValue(py::handle ele, const LogicalType &target_type, bool nan_as_null) {\n+PythonObjectType GetObjectType(py::handle &ele) {\n \tauto &import_cache = *DuckDBPyConnection::ImportCache();\n \n \tif (ele.is_none()) {\n-\t\treturn Value();\n+\t\treturn PythonObjectType::None;\n \t} else if (py::isinstance<py::bool_>(ele)) {\n-\t\treturn Value::BOOLEAN(ele.cast<bool>());\n+\t\treturn PythonObjectType::Bool;\n \t} else if (py::isinstance<py::int_>(ele)) {\n+\t\treturn PythonObjectType::Integer;\n+\t} else if (py::isinstance<py::float_>(ele)) {\n+\t\treturn PythonObjectType::Float;\n+\t} else if (py::isinstance(ele, import_cache.decimal.Decimal())) {\n+\t\treturn PythonObjectType::Decimal;\n+\t} else if (py::isinstance(ele, import_cache.uuid.UUID())) {\n+\t\treturn PythonObjectType::Uuid;\n+\t} else if (py::isinstance(ele, import_cache.datetime.datetime())) {\n+\t\treturn PythonObjectType::Datetime;\n+\t} else if (py::isinstance(ele, import_cache.datetime.time())) {\n+\t\treturn PythonObjectType::Time;\n+\t} else if (py::isinstance(ele, import_cache.datetime.date())) {\n+\t\treturn PythonObjectType::Date;\n+\t} else if (py::isinstance(ele, import_cache.datetime.timedelta())) {\n+\t\treturn PythonObjectType::Timedelta;\n+\t} else if (py::isinstance<py::str>(ele)) {\n+\t\treturn PythonObjectType::String;\n+\t} else if (py::isinstance<py::bytearray>(ele)) {\n+\t\treturn PythonObjectType::ByteArray;\n+\t} else if (py::isinstance<py::memoryview>(ele)) {\n+\t\treturn PythonObjectType::MemoryView;\n+\t} else if (py::isinstance<py::bytes>(ele)) {\n+\t\treturn PythonObjectType::Bytes;\n+\t} else if (py::isinstance<py::list>(ele)) {\n+\t\treturn PythonObjectType::List;\n+\t} else if (py::isinstance<py::dict>(ele)) {\n+\t\treturn PythonObjectType::Dict;\n+\t} else if (py::isinstance(ele, import_cache.numpy.ndarray())) {\n+\t\treturn PythonObjectType::NdArray;\n+\t} else {\n+\t\treturn PythonObjectType::Other;\n+\t}\n+}\n+\n+Value TransformPythonValue(py::handle ele, const LogicalType &target_type, bool nan_as_null) {\n+\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\n+\tauto object_type = GetObjectType(ele);\n+\n+\tswitch (object_type) {\n+\tcase PythonObjectType::None:\n+\t\treturn Value();\n+\tcase PythonObjectType::Bool:\n+\t\treturn Value::BOOLEAN(ele.cast<bool>());\n+\tcase PythonObjectType::Integer: {\n \t\tValue integer;\n \t\tif (!TryTransformPythonNumeric(integer, ele)) {\n \t\t\tthrow InvalidInputException(\"An error occurred attempting to convert a python integer\");\n \t\t}\n \t\treturn integer;\n-\t} else if (py::isinstance<py::float_>(ele)) {\n+\t}\n+\tcase PythonObjectType::Float:\n \t\tif (nan_as_null && std::isnan(PyFloat_AsDouble(ele.ptr()))) {\n \t\t\treturn Value();\n \t\t}\n \t\treturn Value::DOUBLE(ele.cast<double>());\n-\t} else if (py::isinstance(ele, import_cache.decimal.Decimal())) {\n+\tcase PythonObjectType::Decimal: {\n \t\tPyDecimal decimal(ele);\n \t\treturn decimal.ToDuckValue();\n-\t} else if (py::isinstance(ele, import_cache.uuid.UUID())) {\n+\t}\n+\tcase PythonObjectType::Uuid: {\n \t\tauto string_val = py::str(ele).cast<string>();\n \t\treturn Value::UUID(string_val);\n-\t} else if (py::isinstance(ele, import_cache.datetime.datetime())) {\n+\t}\n+\tcase PythonObjectType::Datetime: {\n+\t\tauto isnull_result = py::module::import(\"pandas\").attr(\"isnull\")(ele);\n+\t\tbool is_nat = string(py::str(isnull_result)) == \"True\";\n+\t\tif (is_nat) {\n+\t\t\treturn Value();\n+\t\t}\n \t\tauto datetime = PyDateTime(ele);\n \t\treturn datetime.ToDuckValue();\n-\t} else if (py::isinstance(ele, import_cache.datetime.time())) {\n+\t}\n+\tcase PythonObjectType::Time: {\n \t\tauto time = PyTime(ele);\n \t\treturn time.ToDuckValue();\n-\t} else if (py::isinstance(ele, import_cache.datetime.date())) {\n+\t}\n+\tcase PythonObjectType::Date: {\n \t\tauto date = PyDate(ele);\n \t\treturn date.ToDuckValue();\n-\t} else if (py::isinstance(ele, import_cache.datetime.timedelta())) {\n+\t}\n+\tcase PythonObjectType::Timedelta: {\n \t\tauto timedelta = PyTimeDelta(ele);\n \t\treturn Value::INTERVAL(timedelta.ToInterval());\n-\t} else if (py::isinstance<py::str>(ele)) {\n+\t}\n+\tcase PythonObjectType::String:\n \t\treturn ele.cast<string>();\n-\t} else if (py::isinstance<py::bytearray>(ele)) {\n+\tcase PythonObjectType::ByteArray: {\n \t\tauto byte_array = ele.ptr();\n \t\tauto bytes = PyByteArray_AsString(byte_array);\n \t\treturn Value::BLOB_RAW(bytes);\n-\t} else if (py::isinstance<py::memoryview>(ele)) {\n+\t}\n+\tcase PythonObjectType::MemoryView: {\n \t\tpy::memoryview py_view = ele.cast<py::memoryview>();\n \t\tPyObject *py_view_ptr = py_view.ptr();\n \t\tPy_buffer *py_buf = PyMemoryView_GET_BUFFER(py_view_ptr);\n \t\treturn Value::BLOB(const_data_ptr_t(py_buf->buf), idx_t(py_buf->len));\n-\t} else if (py::isinstance<py::bytes>(ele)) {\n+\t}\n+\tcase PythonObjectType::Bytes: {\n \t\tconst string &ele_string = ele.cast<string>();\n \t\treturn Value::BLOB(const_data_ptr_t(ele_string.data()), ele_string.size());\n-\t} else if (py::isinstance<py::list>(ele)) {\n+\t}\n+\tcase PythonObjectType::List:\n \t\treturn TransformListValue(ele);\n-\t} else if (py::isinstance<py::dict>(ele)) {\n+\tcase PythonObjectType::Dict: {\n \t\tPyDictionary dict = PyDictionary(py::reinterpret_borrow<py::object>(ele));\n \t\tswitch (target_type.id()) {\n \t\tcase LogicalTypeId::STRUCT:\n@@ -263,9 +323,10 @@ Value TransformPythonValue(py::handle ele, const LogicalType &target_type, bool\n \t\tdefault:\n \t\t\treturn TransformDictionary(dict);\n \t\t}\n-\t} else if (py::isinstance(ele, import_cache.numpy.ndarray())) {\n+\t}\n+\tcase PythonObjectType::NdArray:\n \t\treturn TransformPythonValue(ele.attr(\"tolist\")());\n-\t} else {\n+\tcase PythonObjectType::Other:\n \t\tthrow NotImplementedException(\"Unable to transform python value of type '%s' to DuckDB LogicalType\",\n \t\t                              py::str(ele.get_type()).cast<string>());\n \t}\ndiff --git a/tools/pythonpkg/src/vector_conversion.cpp b/tools/pythonpkg/src/vector_conversion.cpp\nindex 5ab06f5db14b..9a75f6d04224 100644\n--- a/tools/pythonpkg/src/vector_conversion.cpp\n+++ b/tools/pythonpkg/src/vector_conversion.cpp\n@@ -145,7 +145,7 @@ static void SetInvalidRecursive(Vector &out, idx_t index) {\n }\n \n //! 'count' is the amount of rows in the 'out' vector\n-//! offset is the current row number within this vector\n+//! 'offset' is the current row number within this vector\n void ScanPandasObject(PandasColumnBindData &bind_data, PyObject *object, idx_t offset, Vector &out) {\n \n \t// handle None\n@@ -192,12 +192,15 @@ void ScanPandasObjectColumn(PandasColumnBindData &bind_data, PyObject **col, idx\n \tauto gil = make_unique<PythonGILWrapper>(); // We're creating python objects here, so we need the GIL\n \n \tfor (idx_t i = 0; i < count; i++) {\n-\t\tScanPandasObject(bind_data, col[i], i, out);\n+\t\tidx_t source_idx = offset + i;\n+\t\tScanPandasObject(bind_data, col[source_idx], i, out);\n \t}\n \tgil.reset();\n \tVerifyTypeConstraints(out, count);\n }\n \n+//! 'offset' is the offset within the column\n+//! 'count' is the amount of values we will convert in this batch\n void VectorConversion::NumpyToDuckDB(PandasColumnBindData &bind_data, py::array &numpy_col, idx_t count, idx_t offset,\n                                      Vector &out) {\n \tswitch (bind_data.pandas_type) {\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\nindex ff1e7721b4d6..084e564b6462 100644\n--- a/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n+++ b/tools/pythonpkg/tests/fast/pandas/test_df_object_resolution.py\n@@ -346,8 +346,8 @@ def test_list_column_value_upgrade(self, duckdb_cursor):\n         pd.testing.assert_frame_equal(converted_col, duckdb_col)\n \n     def test_ubigint_object_conversion(self, duckdb_cursor):\n-\t\t# UBIGINT + TINYINT would result in HUGEINT, but conversion to HUGEINT is not supported yet from pandas->duckdb\n-\t\t# So this instead becomes a DOUBLE\n+        # UBIGINT + TINYINT would result in HUGEINT, but conversion to HUGEINT is not supported yet from pandas->duckdb\n+        # So this instead becomes a DOUBLE\n         data = [18446744073709551615, 0]\n         x = pd.DataFrame({'0': pd.Series(data=data, dtype='object')})\n         converted_col = duckdb.query_df(x, \"x\", \"select * from x\").df()\n@@ -408,6 +408,49 @@ def test_numeric_decimal(self):\n \n         assert(conversion == reference)\n \n+    # Test that the column 'offset' is actually used when converting,\n+    # and that the same 1024 (STANDARD_VECTOR_SIZE) values are not being scanned over and over again\n+    def test_multiple_chunks(self):\n+        standard_vector_size = 1024\n+\n+        data = []\n+        data += [datetime.date(2022, 9, 13) for x in range(standard_vector_size)]\n+        data += [datetime.date(2022, 9, 14) for x in range(standard_vector_size)]\n+        data += [datetime.date(2022, 9, 15) for x in range(standard_vector_size)]\n+        data += [datetime.date(2022, 9, 16) for x in range(standard_vector_size)]\n+        x = pd.DataFrame({'dates': pd.Series(data=data, dtype='object')})\n+        res = duckdb.query_df(x, \"x\", \"select distinct * from x\").df()\n+        assert(len(res['dates'].__array__()) == 4)\n+\n+    def test_multiple_chunks_aggregate(self):\n+        conn = duckdb.connect()\n+        conn.execute(\"create table dates as select '2022-09-14'::DATE + INTERVAL (i::INTEGER) DAY as i from range(0, 4096) tbl(i);\")\n+        res = duckdb.query(\"select * from dates\", connection=conn).df()\n+        date_df = res.copy()\n+        # Convert the values to `datetime.date` values, and the dtype of the column to 'object'\n+        date_df['i'] = pd.to_datetime(res['i']).dt.date\n+        assert(str(date_df['i'].dtype) == 'object')\n+        expected_res = duckdb.query('select avg(epoch(i)), min(epoch(i)), max(epoch(i)) from dates;', connection=conn).fetchall()\n+        actual_res = duckdb.query_df(date_df, 'x', 'select avg(epoch(i)), min(epoch(i)), max(epoch(i)) from x').fetchall()\n+        assert(expected_res == actual_res)\n+\n+        conn.execute('drop table dates')\n+        # Now with nulls interleaved\n+        for i in range(0, len(res['i']), 2):\n+            res['i'][i] = None\n+\n+\n+        date_view = conn.register(\"date_view\", res)\n+        date_view.execute('create table dates as select * from date_view')\n+        expected_res = duckdb.query(\"select avg(epoch(i)), min(epoch(i)), max(epoch(i)) from dates\", connection=conn).fetchall()\n+\n+        date_df = res.copy()\n+        # Convert the values to `datetime.date` values, and the dtype of the column to 'object'\n+        date_df['i'] = pd.to_datetime(res['i']).dt.date\n+        assert(str(date_df['i'].dtype) == 'object')\n+        actual_res = duckdb.query_df(date_df, 'x', 'select avg(epoch(i)), min(epoch(i)), max(epoch(i)) from x').fetchall()\n+        assert(expected_res == actual_res)\n+\n     def test_mixed_object_types(self):\n         x = pd.DataFrame({\n             'nested': pd.Series(data=[{'a': 1, 'b': 2}, [5, 4, 3], {'key': [1,2,3], 'value': ['a', 'b', 'c']}], dtype='object'),\n",
  "problem_statement": "Regression in distinct query over Python dates in 0.5.0\n### What happens?\n\nWe are encountering a regression in the behavior of a select distinct query over a dataframe with Python dates in it in 0.5.0\n\n### To Reproduce\n\nI do not have as minimal of a reproduction as with #4688, as this one seems like it may depend on the vast majority of values in the column being the same. But I do have a reproduction. I'm not sure the best way to share a small (under 1mb) parquet file like this these days. [Here's a link](https://www.dropbox.com/s/oijokech2hx1j2w/distinct_dates.parquet?dl=0) from my dropbox that hopefully works, but let me know if there's a better way to share this.\r\n\r\nThis parquet file has about 850k rows with a single `datetime64[ns]` column named `EVENT_TIME`. The vast majority of rows have the value `2021-01-27T00:00:00`, but not all as we'll see.\r\n\r\n```python\r\nimport duckdb\r\nimport pandas as pd\r\n\r\ninput_df = pd.read_parquet(\"distinct_dates.parquet\", engine=\"fastparquet\")\r\nconn = duckdb.connect(database= \":memory\", read_only=False)\r\nconn.register(\"input_df\", input_df)\r\nconn.execute(\"select distinct EVENT_TIME from input_df\")\r\ninitial_result = conn.df()\r\n```\r\n\r\nIf you inspect `initial_result` you will see that it has 159 rows, all values at midnight on different dates. Now we'll convert these to Python dates and run the same query:\r\n\r\n\r\n```python\r\ndate_df = input_df.copy()\r\ndate_df['EVENT_TIME'] = pd.to_datetime(date_df['EVENT_TIME']).dt.date\r\n\r\nconn.register(\"date_df\", date_df)\r\nconn.execute(\"select distinct EVENT_TIME from date_df\")\r\ndate_result = conn.df()\r\n```\r\n\r\nIf you inspect `date_result`, it only has a single row, with the value `datetime.date(2021, 1, 27)`. You can double check that `date_df` does contain other values. e.g. `print(date_df['EVENT_TIME'][4496])` yields `datetime.date(2021, 1, 28)`, but they are not showing up in the distinct query. Running the same query on 0.4.0 yields the correct 159 rows with the same input dataframe. I have checked with the latest master branch build and it also appears to be affected by this bug.\n\n### OS:\n\nmacOS, Linux\n\n### DuckDB Version:\n\n0.5.0\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nDylan Scott\n\n### Affiliation:\n\nHex Technologies\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "@Tishj seems related to distinct, can you please have a look?\nSure I can have a look, it looks related to object conversion, not really the distinct aspect\r\nBefore the conversion `to_datetime` the column is of:\r\ndtype = `datetime64[ns]`\r\nclass of items = `<class 'pandas._libs.tslibs.timestamps.Timestamp'>`\r\n\r\nAfter conversion the dtype changes to `object`\r\nclass of items = `<class 'datetime.date'>`\r\n\r\nSo it's likely an issue with conversion from datetime.date objects to DATE Values\nThat's what I suspected thanks\nFound the issue, I'm a dumbass ;) (completely disregarding the `offset` in the object conversion loop)\r\nShould be an easy fix!\r\n\r\nFixed:\r\n```\r\n    EVENT_TIME\r\n0   2020-09-26\r\n1   2020-09-27\r\n2   2020-09-28\r\n3   2020-09-29\r\n4   2020-09-30\r\n..         ...\r\n154 2021-01-22\r\n155 2021-01-23\r\n156 2021-01-24\r\n157 2021-01-28\r\n158 2021-01-18\r\n\r\n[159 rows x 1 columns]\r\n```",
  "created_at": "2022-09-13T08:22:31Z"
}