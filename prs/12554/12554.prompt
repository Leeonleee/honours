You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Insert on conflict always returning nextval of primary key sequence
### What happens?

I created a table that uses a sequence for its primary key. When I insert a row with a conflict, the returned value is the next value of the sequence, not the primary key of the row.

### To Reproduce

To reproduce, just paste this into the CLI:

```sql
CREATE SEQUENCE test_id_sequence START 1;

CREATE OR REPLACE TABLE test (
    id    INTEGER PRIMARY KEY DEFAULT nextval('test_id_sequence'),
    value VARCHAR NOT NULL UNIQUE
);

INSERT INTO test (value) values ('abc') RETURNING id;
INSERT INTO test (value) values ('abc') ON CONFLICT (value) DO NOTHING RETURNING id;
```

Actual output:
```
./duckdb test.db
v1.0.0 1f98600c2c
Enter ".help" for usage hints.
D CREATE SEQUENCE test_id_sequence START 1;
D 
D CREATE OR REPLACE TABLE test (
      id    INTEGER PRIMARY KEY DEFAULT nextval('test_id_sequence'),
      value VARCHAR NOT NULL UNIQUE
  );
D 
D INSERT INTO test (value) values ('abc') RETURNING id;
┌───────┐
│  id   │
│ int32 │
├───────┤
│     1 │
└───────┘
D 
D 
D INSERT INTO test (value) values ('abc') ON CONFLICT (value) DO NOTHING RETURNING id;
┌───────┐
│  id   │
│ int32 │
├───────┤
│     2 │
└───────┘
D 
```

I expect that last `2` to be `1` as the row has not changed.

Testing with the latest nightly python module produces the same result.

### OS:

Ubuntu 20.04

### DuckDB Version:

v1.0.0 1f98600c2c

### DuckDB Client:

Python and CLI

### Full Name:

Andy Murdoch

### Affiliation:

maxmine.com.au

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Not applicable - the reproduction does not require a data set

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/execution/operator/persistent/physical_insert.cpp]
1: #include "duckdb/execution/operator/persistent/physical_insert.hpp"
2: #include "duckdb/parallel/thread_context.hpp"
3: #include "duckdb/catalog/catalog_entry/duck_table_entry.hpp"
4: #include "duckdb/common/types/column/column_data_collection.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/execution/expression_executor.hpp"
7: #include "duckdb/storage/data_table.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/parser/parsed_data/create_table_info.hpp"
10: #include "duckdb/planner/expression/bound_constant_expression.hpp"
11: #include "duckdb/storage/table_io_manager.hpp"
12: #include "duckdb/transaction/local_storage.hpp"
13: #include "duckdb/parser/statement/insert_statement.hpp"
14: #include "duckdb/parser/statement/update_statement.hpp"
15: #include "duckdb/storage/table/scan_state.hpp"
16: #include "duckdb/common/types/conflict_manager.hpp"
17: #include "duckdb/execution/index/art/art.hpp"
18: #include "duckdb/transaction/duck_transaction.hpp"
19: #include "duckdb/storage/table/append_state.hpp"
20: #include "duckdb/storage/table/update_state.hpp"
21: 
22: namespace duckdb {
23: 
24: PhysicalInsert::PhysicalInsert(
25:     vector<LogicalType> types_p, TableCatalogEntry &table, physical_index_vector_t<idx_t> column_index_map,
26:     vector<unique_ptr<Expression>> bound_defaults, vector<unique_ptr<BoundConstraint>> bound_constraints_p,
27:     vector<unique_ptr<Expression>> set_expressions, vector<PhysicalIndex> set_columns, vector<LogicalType> set_types,
28:     idx_t estimated_cardinality, bool return_chunk, bool parallel, OnConflictAction action_type,
29:     unique_ptr<Expression> on_conflict_condition_p, unique_ptr<Expression> do_update_condition_p,
30:     unordered_set<column_t> conflict_target_p, vector<column_t> columns_to_fetch_p)
31:     : PhysicalOperator(PhysicalOperatorType::INSERT, std::move(types_p), estimated_cardinality),
32:       column_index_map(std::move(column_index_map)), insert_table(&table), insert_types(table.GetTypes()),
33:       bound_defaults(std::move(bound_defaults)), bound_constraints(std::move(bound_constraints_p)),
34:       return_chunk(return_chunk), parallel(parallel), action_type(action_type),
35:       set_expressions(std::move(set_expressions)), set_columns(std::move(set_columns)), set_types(std::move(set_types)),
36:       on_conflict_condition(std::move(on_conflict_condition_p)), do_update_condition(std::move(do_update_condition_p)),
37:       conflict_target(std::move(conflict_target_p)), columns_to_fetch(std::move(columns_to_fetch_p)) {
38: 
39: 	if (action_type == OnConflictAction::THROW) {
40: 		return;
41: 	}
42: 
43: 	D_ASSERT(this->set_expressions.size() == this->set_columns.size());
44: 
45: 	// One or more columns are referenced from the existing table,
46: 	// we use the 'insert_types' to figure out which types these columns have
47: 	types_to_fetch = vector<LogicalType>(columns_to_fetch.size(), LogicalType::SQLNULL);
48: 	for (idx_t i = 0; i < columns_to_fetch.size(); i++) {
49: 		auto &id = columns_to_fetch[i];
50: 		D_ASSERT(id < insert_types.size());
51: 		types_to_fetch[i] = insert_types[id];
52: 	}
53: }
54: 
55: PhysicalInsert::PhysicalInsert(LogicalOperator &op, SchemaCatalogEntry &schema, unique_ptr<BoundCreateTableInfo> info_p,
56:                                idx_t estimated_cardinality, bool parallel)
57:     : PhysicalOperator(PhysicalOperatorType::CREATE_TABLE_AS, op.types, estimated_cardinality), insert_table(nullptr),
58:       return_chunk(false), schema(&schema), info(std::move(info_p)), parallel(parallel),
59:       action_type(OnConflictAction::THROW) {
60: 	GetInsertInfo(*info, insert_types, bound_defaults);
61: }
62: 
63: void PhysicalInsert::GetInsertInfo(const BoundCreateTableInfo &info, vector<LogicalType> &insert_types,
64:                                    vector<unique_ptr<Expression>> &bound_defaults) {
65: 	auto &create_info = info.base->Cast<CreateTableInfo>();
66: 	for (auto &col : create_info.columns.Physical()) {
67: 		insert_types.push_back(col.GetType());
68: 		bound_defaults.push_back(make_uniq<BoundConstantExpression>(Value(col.GetType())));
69: 	}
70: }
71: 
72: //===--------------------------------------------------------------------===//
73: // Sink
74: //===--------------------------------------------------------------------===//
75: class InsertGlobalState : public GlobalSinkState {
76: public:
77: 	explicit InsertGlobalState(ClientContext &context, const vector<LogicalType> &return_types, DuckTableEntry &table)
78: 	    : table(table), insert_count(0), initialized(false), return_collection(context, return_types) {
79: 	}
80: 
81: 	mutex lock;
82: 	DuckTableEntry &table;
83: 	idx_t insert_count;
84: 	bool initialized;
85: 	LocalAppendState append_state;
86: 	ColumnDataCollection return_collection;
87: };
88: 
89: class InsertLocalState : public LocalSinkState {
90: public:
91: 	InsertLocalState(ClientContext &context, const vector<LogicalType> &types,
92: 	                 const vector<unique_ptr<Expression>> &bound_defaults,
93: 	                 const vector<unique_ptr<BoundConstraint>> &bound_constraints)
94: 	    : default_executor(context, bound_defaults), bound_constraints(bound_constraints) {
95: 		insert_chunk.Initialize(Allocator::Get(context), types);
96: 	}
97: 
98: 	DataChunk insert_chunk;
99: 	ExpressionExecutor default_executor;
100: 	TableAppendState local_append_state;
101: 	unique_ptr<RowGroupCollection> local_collection;
102: 	optional_ptr<OptimisticDataWriter> writer;
103: 	// Rows that have been updated by a DO UPDATE conflict
104: 	unordered_set<row_t> updated_global_rows;
105: 	// Rows in the transaction-local storage that have been updated by a DO UPDATE conflict
106: 	unordered_set<row_t> updated_local_rows;
107: 	idx_t update_count = 0;
108: 	unique_ptr<ConstraintState> constraint_state;
109: 	const vector<unique_ptr<BoundConstraint>> &bound_constraints;
110: 
111: 	ConstraintState &GetConstraintState(DataTable &table, TableCatalogEntry &tableref) {
112: 		if (!constraint_state) {
113: 			constraint_state = table.InitializeConstraintState(tableref, bound_constraints);
114: 		}
115: 		return *constraint_state;
116: 	}
117: };
118: 
119: unique_ptr<GlobalSinkState> PhysicalInsert::GetGlobalSinkState(ClientContext &context) const {
120: 	optional_ptr<TableCatalogEntry> table;
121: 	if (info) {
122: 		// CREATE TABLE AS
123: 		D_ASSERT(!insert_table);
124: 		auto &catalog = schema->catalog;
125: 		table = &catalog.CreateTable(catalog.GetCatalogTransaction(context), *schema.get_mutable(), *info)
126: 		             ->Cast<TableCatalogEntry>();
127: 	} else {
128: 		D_ASSERT(insert_table);
129: 		D_ASSERT(insert_table->IsDuckTable());
130: 		table = insert_table.get_mutable();
131: 	}
132: 	auto result = make_uniq<InsertGlobalState>(context, GetTypes(), table->Cast<DuckTableEntry>());
133: 	return std::move(result);
134: }
135: 
136: unique_ptr<LocalSinkState> PhysicalInsert::GetLocalSinkState(ExecutionContext &context) const {
137: 	return make_uniq<InsertLocalState>(context.client, insert_types, bound_defaults, bound_constraints);
138: }
139: 
140: void PhysicalInsert::ResolveDefaults(const TableCatalogEntry &table, DataChunk &chunk,
141:                                      const physical_index_vector_t<idx_t> &column_index_map,
142:                                      ExpressionExecutor &default_executor, DataChunk &result) {
143: 	chunk.Flatten();
144: 	default_executor.SetChunk(chunk);
145: 
146: 	result.Reset();
147: 	result.SetCardinality(chunk);
148: 
149: 	if (!column_index_map.empty()) {
150: 		// columns specified by the user, use column_index_map
151: 		for (auto &col : table.GetColumns().Physical()) {
152: 			auto storage_idx = col.StorageOid();
153: 			auto mapped_index = column_index_map[col.Physical()];
154: 			if (mapped_index == DConstants::INVALID_INDEX) {
155: 				// insert default value
156: 				default_executor.ExecuteExpression(storage_idx, result.data[storage_idx]);
157: 			} else {
158: 				// get value from child chunk
159: 				D_ASSERT((idx_t)mapped_index < chunk.ColumnCount());
160: 				D_ASSERT(result.data[storage_idx].GetType() == chunk.data[mapped_index].GetType());
161: 				result.data[storage_idx].Reference(chunk.data[mapped_index]);
162: 			}
163: 		}
164: 	} else {
165: 		// no columns specified, just append directly
166: 		for (idx_t i = 0; i < result.ColumnCount(); i++) {
167: 			D_ASSERT(result.data[i].GetType() == chunk.data[i].GetType());
168: 			result.data[i].Reference(chunk.data[i]);
169: 		}
170: 	}
171: }
172: 
173: bool AllConflictsMeetCondition(DataChunk &result) {
174: 	result.Flatten();
175: 	auto data = FlatVector::GetData<bool>(result.data[0]);
176: 	for (idx_t i = 0; i < result.size(); i++) {
177: 		if (!data[i]) {
178: 			return false;
179: 		}
180: 	}
181: 	return true;
182: }
183: 
184: void CheckOnConflictCondition(ExecutionContext &context, DataChunk &conflicts, const unique_ptr<Expression> &condition,
185:                               DataChunk &result) {
186: 	ExpressionExecutor executor(context.client, *condition);
187: 	result.Initialize(context.client, {LogicalType::BOOLEAN});
188: 	executor.Execute(conflicts, result);
189: 	result.SetCardinality(conflicts.size());
190: }
191: 
192: static void CombineExistingAndInsertTuples(DataChunk &result, DataChunk &scan_chunk, DataChunk &input_chunk,
193:                                            ClientContext &client, const PhysicalInsert &op) {
194: 	auto &types_to_fetch = op.types_to_fetch;
195: 	auto &insert_types = op.insert_types;
196: 
197: 	if (types_to_fetch.empty()) {
198: 		// We have not scanned the initial table, so we can just duplicate the initial chunk
199: 		result.Initialize(client, input_chunk.GetTypes());
200: 		result.Reference(input_chunk);
201: 		result.SetCardinality(input_chunk);
202: 		return;
203: 	}
204: 	vector<LogicalType> combined_types;
205: 	combined_types.reserve(insert_types.size() + types_to_fetch.size());
206: 	combined_types.insert(combined_types.end(), insert_types.begin(), insert_types.end());
207: 	combined_types.insert(combined_types.end(), types_to_fetch.begin(), types_to_fetch.end());
208: 
209: 	result.Initialize(client, combined_types);
210: 	result.Reset();
211: 	// Add the VALUES list
212: 	for (idx_t i = 0; i < insert_types.size(); i++) {
213: 		idx_t col_idx = i;
214: 		auto &other_col = input_chunk.data[i];
215: 		auto &this_col = result.data[col_idx];
216: 		D_ASSERT(other_col.GetType() == this_col.GetType());
217: 		this_col.Reference(other_col);
218: 	}
219: 	// Add the columns from the original conflicting tuples
220: 	for (idx_t i = 0; i < types_to_fetch.size(); i++) {
221: 		idx_t col_idx = i + insert_types.size();
222: 		auto &other_col = scan_chunk.data[i];
223: 		auto &this_col = result.data[col_idx];
224: 		D_ASSERT(other_col.GetType() == this_col.GetType());
225: 		this_col.Reference(other_col);
226: 	}
227: 	// This is guaranteed by the requirement of a conflict target to have a condition or set expressions
228: 	// Only when we have any sort of condition or SET expression that references the existing table is this possible
229: 	// to not be true.
230: 	// We can have a SET expression without a conflict target ONLY if there is only 1 Index on the table
231: 	// In which case this also can't cause a discrepancy between existing tuple count and insert tuple count
232: 	D_ASSERT(input_chunk.size() == scan_chunk.size());
233: 	result.SetCardinality(input_chunk.size());
234: }
235: 
236: static void CreateUpdateChunk(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table, Vector &row_ids,
237:                               DataChunk &update_chunk, const PhysicalInsert &op) {
238: 
239: 	auto &do_update_condition = op.do_update_condition;
240: 	auto &set_types = op.set_types;
241: 	auto &set_expressions = op.set_expressions;
242: 	// Check the optional condition for the DO UPDATE clause, to filter which rows will be updated
243: 	if (do_update_condition) {
244: 		DataChunk do_update_filter_result;
245: 		do_update_filter_result.Initialize(context.client, {LogicalType::BOOLEAN});
246: 		ExpressionExecutor where_executor(context.client, *do_update_condition);
247: 		where_executor.Execute(chunk, do_update_filter_result);
248: 		do_update_filter_result.SetCardinality(chunk.size());
249: 		do_update_filter_result.Flatten();
250: 
251: 		ManagedSelection selection(chunk.size());
252: 
253: 		auto where_data = FlatVector::GetData<bool>(do_update_filter_result.data[0]);
254: 		for (idx_t i = 0; i < chunk.size(); i++) {
255: 			if (where_data[i]) {
256: 				selection.Append(i);
257: 			}
258: 		}
259: 		if (selection.Count() != selection.Size()) {
260: 			// Not all conflicts met the condition, need to filter out the ones that don't
261: 			chunk.Slice(selection.Selection(), selection.Count());
262: 			chunk.SetCardinality(selection.Count());
263: 			// Also apply this Slice to the to-update row_ids
264: 			row_ids.Slice(selection.Selection(), selection.Count());
265: 		}
266: 	}
267: 
268: 	// Execute the SET expressions
269: 	update_chunk.Initialize(context.client, set_types);
270: 	ExpressionExecutor executor(context.client, set_expressions);
271: 	executor.Execute(chunk, update_chunk);
272: 	update_chunk.SetCardinality(chunk);
273: }
274: 
275: template <bool GLOBAL>
276: static idx_t PerformOnConflictAction(ExecutionContext &context, DataChunk &chunk, TableCatalogEntry &table,
277:                                      Vector &row_ids, const PhysicalInsert &op) {
278: 
279: 	if (op.action_type == OnConflictAction::NOTHING) {
280: 		return 0;
281: 	}
282: 	auto &set_columns = op.set_columns;
283: 
284: 	DataChunk update_chunk;
285: 	CreateUpdateChunk(context, chunk, table, row_ids, update_chunk, op);
286: 
287: 	auto &data_table = table.GetStorage();
288: 	// Perform the update, using the results of the SET expressions
289: 	if (GLOBAL) {
290: 		auto update_state = data_table.InitializeUpdate(table, context.client, op.bound_constraints);
291: 		data_table.Update(*update_state, context.client, row_ids, set_columns, update_chunk);
292: 	} else {
293: 		auto &local_storage = LocalStorage::Get(context.client, data_table.db);
294: 		// Perform the update, using the results of the SET expressions
295: 		local_storage.Update(data_table, row_ids, set_columns, update_chunk);
296: 	}
297: 	return update_chunk.size();
298: }
299: 
300: // TODO: should we use a hash table to keep track of this instead?
301: template <bool GLOBAL>
302: static void RegisterUpdatedRows(InsertLocalState &lstate, const Vector &row_ids, idx_t count) {
303: 	// Insert all rows, if any of the rows has already been updated before, we throw an error
304: 	auto data = FlatVector::GetData<row_t>(row_ids);
305: 
306: 	// The rowids in the transaction-local ART aren't final yet so we have to separately keep track of the two sets of
307: 	// rowids
308: 	unordered_set<row_t> &updated_rows = GLOBAL ? lstate.updated_global_rows : lstate.updated_local_rows;
309: 	for (idx_t i = 0; i < count; i++) {
310: 		auto result = updated_rows.insert(data[i]);
311: 		if (result.second == false) {
312: 			throw InvalidInputException(
313: 			    "ON CONFLICT DO UPDATE can not update the same row twice in the same command. Ensure that no rows "
314: 			    "proposed for insertion within the same command have duplicate constrained values");
315: 		}
316: 	}
317: }
318: 
319: template <bool GLOBAL>
320: static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &context, InsertLocalState &lstate,
321:                                    DataTable &data_table, const PhysicalInsert &op) {
322: 	auto &types_to_fetch = op.types_to_fetch;
323: 	auto &on_conflict_condition = op.on_conflict_condition;
324: 	auto &conflict_target = op.conflict_target;
325: 	auto &columns_to_fetch = op.columns_to_fetch;
326: 
327: 	auto &local_storage = LocalStorage::Get(context.client, data_table.db);
328: 
329: 	// We either want to do nothing, or perform an update when conflicts arise
330: 	ConflictInfo conflict_info(conflict_target);
331: 	ConflictManager conflict_manager(VerifyExistenceType::APPEND, lstate.insert_chunk.size(), &conflict_info);
332: 	if (GLOBAL) {
333: 		auto &constraint_state = lstate.GetConstraintState(data_table, table);
334: 		data_table.VerifyAppendConstraints(constraint_state, context.client, lstate.insert_chunk, &conflict_manager);
335: 	} else {
336: 		DataTable::VerifyUniqueIndexes(local_storage.GetIndexes(data_table), context.client, lstate.insert_chunk,
337: 		                               &conflict_manager);
338: 	}
339: 	conflict_manager.Finalize();
340: 	if (conflict_manager.ConflictCount() == 0) {
341: 		// No conflicts found, 0 updates performed
342: 		return 0;
343: 	}
344: 	auto &conflicts = conflict_manager.Conflicts();
345: 	auto &row_ids = conflict_manager.RowIds();
346: 
347: 	DataChunk conflict_chunk; // contains only the conflicting values
348: 	DataChunk scan_chunk;     // contains the original values, that caused the conflict
349: 	DataChunk combined_chunk; // contains conflict_chunk + scan_chunk (wide)
350: 
351: 	// Filter out everything but the conflicting rows
352: 	conflict_chunk.Initialize(context.client, lstate.insert_chunk.GetTypes());
353: 	conflict_chunk.Reference(lstate.insert_chunk);
354: 	conflict_chunk.Slice(conflicts.Selection(), conflicts.Count());
355: 	conflict_chunk.SetCardinality(conflicts.Count());
356: 
357: 	// Holds the pins for the fetched rows
358: 	unique_ptr<ColumnFetchState> fetch_state;
359: 	if (!types_to_fetch.empty()) {
360: 		D_ASSERT(scan_chunk.size() == 0);
361: 		// When these values are required for the conditions or the SET expressions,
362: 		// then we scan the existing table for the conflicting tuples, using the rowids
363: 		scan_chunk.Initialize(context.client, types_to_fetch);
364: 		fetch_state = make_uniq<ColumnFetchState>();
365: 		if (GLOBAL) {
366: 			auto &transaction = DuckTransaction::Get(context.client, table.catalog);
367: 			data_table.Fetch(transaction, scan_chunk, columns_to_fetch, row_ids, conflicts.Count(), *fetch_state);
368: 		} else {
369: 			local_storage.FetchChunk(data_table, row_ids, conflicts.Count(), columns_to_fetch, scan_chunk,
370: 			                         *fetch_state);
371: 		}
372: 	}
373: 
374: 	// Splice the Input chunk and the fetched chunk together
375: 	CombineExistingAndInsertTuples(combined_chunk, scan_chunk, conflict_chunk, context.client, op);
376: 
377: 	if (on_conflict_condition) {
378: 		DataChunk conflict_condition_result;
379: 		CheckOnConflictCondition(context, combined_chunk, on_conflict_condition, conflict_condition_result);
380: 		bool conditions_met = AllConflictsMeetCondition(conflict_condition_result);
381: 		if (!conditions_met) {
382: 			// Filter out the tuples that did pass the filter, then run the verify again
383: 			ManagedSelection sel(combined_chunk.size());
384: 			auto data = FlatVector::GetData<bool>(conflict_condition_result.data[0]);
385: 			for (idx_t i = 0; i < combined_chunk.size(); i++) {
386: 				if (!data[i]) {
387: 					// Only populate the selection vector with the tuples that did not meet the condition
388: 					sel.Append(i);
389: 				}
390: 			}
391: 			combined_chunk.Slice(sel.Selection(), sel.Count());
392: 			row_ids.Slice(sel.Selection(), sel.Count());
393: 			if (GLOBAL) {
394: 				auto &constraint_state = lstate.GetConstraintState(data_table, table);
395: 				data_table.VerifyAppendConstraints(constraint_state, context.client, combined_chunk, nullptr);
396: 			} else {
397: 				DataTable::VerifyUniqueIndexes(local_storage.GetIndexes(data_table), context.client,
398: 				                               lstate.insert_chunk, nullptr);
399: 			}
400: 			throw InternalException("The previous operation was expected to throw but didn't");
401: 		}
402: 	}
403: 
404: 	RegisterUpdatedRows<GLOBAL>(lstate, row_ids, combined_chunk.size());
405: 
406: 	idx_t updated_tuples = PerformOnConflictAction<GLOBAL>(context, combined_chunk, table, row_ids, op);
407: 
408: 	// Remove the conflicting tuples from the insert chunk
409: 	SelectionVector sel_vec(lstate.insert_chunk.size());
410: 	idx_t new_size =
411: 	    SelectionVector::Inverted(conflicts.Selection(), sel_vec, conflicts.Count(), lstate.insert_chunk.size());
412: 	lstate.insert_chunk.Slice(sel_vec, new_size);
413: 	lstate.insert_chunk.SetCardinality(new_size);
414: 	return updated_tuples;
415: }
416: 
417: idx_t PhysicalInsert::OnConflictHandling(TableCatalogEntry &table, ExecutionContext &context,
418:                                          InsertLocalState &lstate) const {
419: 	auto &data_table = table.GetStorage();
420: 	if (action_type == OnConflictAction::THROW) {
421: 		auto &constraint_state = lstate.GetConstraintState(data_table, table);
422: 		data_table.VerifyAppendConstraints(constraint_state, context.client, lstate.insert_chunk, nullptr);
423: 		return 0;
424: 	}
425: 	// Check whether any conflicts arise, and if they all meet the conflict_target + condition
426: 	// If that's not the case - We throw the first error
427: 	idx_t updated_tuples = 0;
428: 	updated_tuples += HandleInsertConflicts<true>(table, context, lstate, data_table, *this);
429: 	// Also check the transaction-local storage+ART so we can detect conflicts within this transaction
430: 	updated_tuples += HandleInsertConflicts<false>(table, context, lstate, data_table, *this);
431: 
432: 	return updated_tuples;
433: }
434: 
435: SinkResultType PhysicalInsert::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {
436: 	auto &gstate = input.global_state.Cast<InsertGlobalState>();
437: 	auto &lstate = input.local_state.Cast<InsertLocalState>();
438: 
439: 	auto &table = gstate.table;
440: 	auto &storage = table.GetStorage();
441: 	PhysicalInsert::ResolveDefaults(table, chunk, column_index_map, lstate.default_executor, lstate.insert_chunk);
442: 
443: 	if (!parallel) {
444: 		if (!gstate.initialized) {
445: 			storage.InitializeLocalAppend(gstate.append_state, table, context.client, bound_constraints);
446: 			gstate.initialized = true;
447: 		}
448: 
449: 		if (return_chunk) {
450: 			gstate.return_collection.Append(lstate.insert_chunk);
451: 		}
452: 		idx_t updated_tuples = OnConflictHandling(table, context, lstate);
453: 		gstate.insert_count += lstate.insert_chunk.size();
454: 		gstate.insert_count += updated_tuples;
455: 		storage.LocalAppend(gstate.append_state, table, context.client, lstate.insert_chunk, true);
456: 
457: 		// We finalize the local append to write the segment node count.
458: 		if (action_type != OnConflictAction::THROW) {
459: 			storage.FinalizeLocalAppend(gstate.append_state);
460: 			gstate.initialized = false;
461: 		}
462: 
463: 	} else {
464: 		D_ASSERT(!return_chunk);
465: 		// parallel append
466: 		if (!lstate.local_collection) {
467: 			lock_guard<mutex> l(gstate.lock);
468: 			auto table_info = storage.GetDataTableInfo();
469: 			auto &block_manager = TableIOManager::Get(storage).GetBlockManagerForRowData();
470: 			lstate.local_collection = make_uniq<RowGroupCollection>(std::move(table_info), block_manager, insert_types,
471: 			                                                        NumericCast<idx_t>(MAX_ROW_ID));
472: 			lstate.local_collection->InitializeEmpty();
473: 			lstate.local_collection->InitializeAppend(lstate.local_append_state);
474: 			lstate.writer = &gstate.table.GetStorage().CreateOptimisticWriter(context.client);
475: 		}
476: 		OnConflictHandling(table, context, lstate);
477: 
478: 		auto new_row_group = lstate.local_collection->Append(lstate.insert_chunk, lstate.local_append_state);
479: 		if (new_row_group) {
480: 			lstate.writer->WriteNewRowGroup(*lstate.local_collection);
481: 		}
482: 	}
483: 
484: 	return SinkResultType::NEED_MORE_INPUT;
485: }
486: 
487: SinkCombineResultType PhysicalInsert::Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const {
488: 	auto &gstate = input.global_state.Cast<InsertGlobalState>();
489: 	auto &lstate = input.local_state.Cast<InsertLocalState>();
490: 	auto &client_profiler = QueryProfiler::Get(context.client);
491: 	context.thread.profiler.Flush(*this, lstate.default_executor, "default_executor", 1);
492: 	client_profiler.Flush(context.thread.profiler);
493: 
494: 	if (!parallel || !lstate.local_collection) {
495: 		return SinkCombineResultType::FINISHED;
496: 	}
497: 
498: 	// parallel append: finalize the append
499: 	TransactionData tdata(0, 0);
500: 	lstate.local_collection->FinalizeAppend(tdata, lstate.local_append_state);
501: 
502: 	auto append_count = lstate.local_collection->GetTotalRows();
503: 
504: 	lock_guard<mutex> lock(gstate.lock);
505: 	gstate.insert_count += append_count;
506: 	if (append_count < Storage::ROW_GROUP_SIZE) {
507: 		// we have few rows - append to the local storage directly
508: 		auto &table = gstate.table;
509: 		auto &storage = table.GetStorage();
510: 		storage.InitializeLocalAppend(gstate.append_state, table, context.client, bound_constraints);
511: 		auto &transaction = DuckTransaction::Get(context.client, table.catalog);
512: 		lstate.local_collection->Scan(transaction, [&](DataChunk &insert_chunk) {
513: 			storage.LocalAppend(gstate.append_state, table, context.client, insert_chunk);
514: 			return true;
515: 		});
516: 		storage.FinalizeLocalAppend(gstate.append_state);
517: 	} else {
518: 		// we have written rows to disk optimistically - merge directly into the transaction-local storage
519: 		gstate.table.GetStorage().LocalMerge(context.client, *lstate.local_collection);
520: 		gstate.table.GetStorage().FinalizeOptimisticWriter(context.client, *lstate.writer);
521: 	}
522: 
523: 	return SinkCombineResultType::FINISHED;
524: }
525: 
526: SinkFinalizeType PhysicalInsert::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
527:                                           OperatorSinkFinalizeInput &input) const {
528: 	auto &gstate = input.global_state.Cast<InsertGlobalState>();
529: 	if (!parallel && gstate.initialized) {
530: 		auto &table = gstate.table;
531: 		auto &storage = table.GetStorage();
532: 		storage.FinalizeLocalAppend(gstate.append_state);
533: 	}
534: 	return SinkFinalizeType::READY;
535: }
536: 
537: //===--------------------------------------------------------------------===//
538: // Source
539: //===--------------------------------------------------------------------===//
540: class InsertSourceState : public GlobalSourceState {
541: public:
542: 	explicit InsertSourceState(const PhysicalInsert &op) {
543: 		if (op.return_chunk) {
544: 			D_ASSERT(op.sink_state);
545: 			auto &g = op.sink_state->Cast<InsertGlobalState>();
546: 			g.return_collection.InitializeScan(scan_state);
547: 		}
548: 	}
549: 
550: 	ColumnDataScanState scan_state;
551: };
552: 
553: unique_ptr<GlobalSourceState> PhysicalInsert::GetGlobalSourceState(ClientContext &context) const {
554: 	return make_uniq<InsertSourceState>(*this);
555: }
556: 
557: SourceResultType PhysicalInsert::GetData(ExecutionContext &context, DataChunk &chunk,
558:                                          OperatorSourceInput &input) const {
559: 	auto &state = input.global_state.Cast<InsertSourceState>();
560: 	auto &insert_gstate = sink_state->Cast<InsertGlobalState>();
561: 	if (!return_chunk) {
562: 		chunk.SetCardinality(1);
563: 		chunk.SetValue(0, 0, Value::BIGINT(NumericCast<int64_t>(insert_gstate.insert_count)));
564: 		return SourceResultType::FINISHED;
565: 	}
566: 
567: 	insert_gstate.return_collection.Scan(state.scan_state, chunk);
568: 	return chunk.size() == 0 ? SourceResultType::FINISHED : SourceResultType::HAVE_MORE_OUTPUT;
569: }
570: 
571: } // namespace duckdb
[end of src/execution/operator/persistent/physical_insert.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: