{
  "repo": "duckdb/duckdb",
  "pull_number": 11094,
  "instance_id": "duckdb__duckdb-11094",
  "issue_numbers": [
    "11084",
    "11084"
  ],
  "base_commit": "5a59c56acf2cf8eb1948e9045796019f781b317b",
  "patch": "diff --git a/extension/parquet/column_writer.cpp b/extension/parquet/column_writer.cpp\nindex 8eb0f68877a6..be0bcfe87499 100644\n--- a/extension/parquet/column_writer.cpp\n+++ b/extension/parquet/column_writer.cpp\n@@ -1750,6 +1750,40 @@ void ListColumnWriter::FinalizeAnalyze(ColumnWriterState &state_p) {\n \tchild_writer->FinalizeAnalyze(*state.child_state);\n }\n \n+idx_t GetConsecutiveChildList(Vector &list, Vector &result, idx_t offset, idx_t count) {\n+\t// returns a consecutive child list that fully flattens and repeats all required elements\n+\tauto &validity = FlatVector::Validity(list);\n+\tauto list_entries = FlatVector::GetData<list_entry_t>(list);\n+\tbool is_consecutive = true;\n+\tidx_t total_length = 0;\n+\tfor (idx_t c = offset; c < offset + count; c++) {\n+\t\tif (!validity.RowIsValid(c)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (list_entries[c].offset != total_length) {\n+\t\t\tis_consecutive = false;\n+\t\t}\n+\t\ttotal_length += list_entries[c].length;\n+\t}\n+\tif (is_consecutive) {\n+\t\t// already consecutive - leave it as-is\n+\t\treturn total_length;\n+\t}\n+\tSelectionVector sel(total_length);\n+\tidx_t index = 0;\n+\tfor (idx_t c = offset; c < offset + count; c++) {\n+\t\tif (!validity.RowIsValid(c)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tfor (idx_t k = 0; k < list_entries[c].length; k++) {\n+\t\t\tsel.set_index(index++, list_entries[c].offset + k);\n+\t\t}\n+\t}\n+\tresult.Slice(sel, total_length);\n+\tresult.Flatten(total_length);\n+\treturn total_length;\n+}\n+\n void ListColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {\n \tauto &state = state_p.Cast<ListColumnWriterState>();\n \n@@ -1803,7 +1837,7 @@ void ListColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *pa\n \n \tauto &list_child = ListVector::GetEntry(vector);\n \tVector child_list(list_child);\n-\tauto child_length = ListVector::GetConsecutiveChildList(vector, child_list, 0, count);\n+\tauto child_length = GetConsecutiveChildList(vector, child_list, 0, count);\n \tchild_writer->Prepare(*state.child_state, &state_p, child_list, child_length);\n }\n \n@@ -1817,7 +1851,7 @@ void ListColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t c\n \n \tauto &list_child = ListVector::GetEntry(vector);\n \tVector child_list(list_child);\n-\tauto child_length = ListVector::GetConsecutiveChildList(vector, child_list, 0, count);\n+\tauto child_length = GetConsecutiveChildList(vector, child_list, 0, count);\n \tchild_writer->Write(*state.child_state, child_list, child_length);\n }\n \n",
  "test_patch": "diff --git a/test/sql/copy/parquet/parquet_write_repeated_lists.test b/test/sql/copy/parquet/parquet_write_repeated_lists.test\nnew file mode 100644\nindex 000000000000..338741c871ef\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_write_repeated_lists.test\n@@ -0,0 +1,21 @@\n+# name: test/sql/copy/parquet/parquet_write_repeated_lists.test\n+# description: Write lists that have repeated list elements\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+COPY (\n+    SELECT *\n+            FROM (values(['asdf', 'fdsa']))\n+            JOIN (values(1),(2)) ON TRUE\n+) to '__TEST_DIR__/lists.parquet';\n+\n+query II\n+SELECT * FROM read_parquet('__TEST_DIR__/lists.parquet');\n+----\n+[asdf, fdsa]\t1\n+[asdf, fdsa]\t2\n",
  "problem_statement": "Out of buffer error when querying parquet file created from a JOIN query\n### What happens?\r\n\r\nI have some parquet files. \r\nI am facing `duckdb.duckdb.Error: Invalid Error: Out of buffer` when querying them.\r\n\r\nPoints worth to mention.\r\n- It only occurs when I create a parquet file from a select query using COPY statement and try to query that file.\r\nLike, this example works:\r\n```python\r\nimport duckdb\r\nduckdb.sql(\"\"\"\r\nCREATE VIEW assets AS SELECT * FROM read_parquet('assets.parquet');\r\nCREATE VIEW st AS SELECT * FROM read_parquet('st.parquet');\r\nCOPY (\r\n    SELECT \r\n               *\r\n            FROM st\r\n            JOIN assets a ON a.table_name LIKE CONCAT(st.table_name_pattern, '%')\r\n               AND a.qf LIKE st.qf_pattern\r\n               AND a.type_name <> 'Column'\r\n) to 'result.parquet';\r\n\r\nSELECT * FROM read_parquet('result.parquet')\"\"\").show()\r\n```\r\nbut as in reproducible example below, it does not.\r\n- If I explicitly specify columns in the SELECT query I see same issue but if I don't include `asset` column it works.\r\n\r\n### To Reproduce\r\n\r\n[Archive.zip](https://github.com/duckdb/duckdb/files/14558006/Archive.zip)\r\n1. Extract Archive.zip into '/tmp' folder\r\n2. Run the following query\r\n```\r\nduckdb.sql(\"\"\"\r\nCREATE VIEW assets AS SELECT * FROM read_parquet('/tmp/assets.parquet');\r\nCREATE VIEW st AS SELECT * FROM read_parquet('/tmp/st.parquet');\r\nCOPY (\r\n    SELECT \r\n               *\r\n            FROM st\r\n            JOIN assets a ON a.table_name LIKE CONCAT(st.table_name_pattern, '%')\r\n               AND a.qf LIKE st.qf_pattern\r\n               AND a.type_name <> 'Column'\r\n) to '/tmp/result.parquet';\r\n\r\nSELECT * FROM read_parquet('/tmp/result.parquet')\"\"\").show()\r\n```\r\n3. Notice that it throws out of buffer error.\r\n\r\n### OS:\r\n\r\nMac M1 (Sonoma)\r\n\r\n### DuckDB Version:\r\n\r\n0.10.0\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nMustafa Hasan Khan\r\n\r\n### Affiliation:\r\n\r\nAtlan\r\n\r\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\r\n\r\nI have tested with a nightly build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\nOut of buffer error when querying parquet file created from a JOIN query\n### What happens?\r\n\r\nI have some parquet files. \r\nI am facing `duckdb.duckdb.Error: Invalid Error: Out of buffer` when querying them.\r\n\r\nPoints worth to mention.\r\n- It only occurs when I create a parquet file from a select query using COPY statement and try to query that file.\r\nLike, this example works:\r\n```python\r\nimport duckdb\r\nduckdb.sql(\"\"\"\r\nCREATE VIEW assets AS SELECT * FROM read_parquet('assets.parquet');\r\nCREATE VIEW st AS SELECT * FROM read_parquet('st.parquet');\r\nCOPY (\r\n    SELECT \r\n               *\r\n            FROM st\r\n            JOIN assets a ON a.table_name LIKE CONCAT(st.table_name_pattern, '%')\r\n               AND a.qf LIKE st.qf_pattern\r\n               AND a.type_name <> 'Column'\r\n) to 'result.parquet';\r\n\r\nSELECT * FROM read_parquet('result.parquet')\"\"\").show()\r\n```\r\nbut as in reproducible example below, it does not.\r\n- If I explicitly specify columns in the SELECT query I see same issue but if I don't include `asset` column it works.\r\n\r\n### To Reproduce\r\n\r\n[Archive.zip](https://github.com/duckdb/duckdb/files/14558006/Archive.zip)\r\n1. Extract Archive.zip into '/tmp' folder\r\n2. Run the following query\r\n```\r\nduckdb.sql(\"\"\"\r\nCREATE VIEW assets AS SELECT * FROM read_parquet('/tmp/assets.parquet');\r\nCREATE VIEW st AS SELECT * FROM read_parquet('/tmp/st.parquet');\r\nCOPY (\r\n    SELECT \r\n               *\r\n            FROM st\r\n            JOIN assets a ON a.table_name LIKE CONCAT(st.table_name_pattern, '%')\r\n               AND a.qf LIKE st.qf_pattern\r\n               AND a.type_name <> 'Column'\r\n) to '/tmp/result.parquet';\r\n\r\nSELECT * FROM read_parquet('/tmp/result.parquet')\"\"\").show()\r\n```\r\n3. Notice that it throws out of buffer error.\r\n\r\n### OS:\r\n\r\nMac M1 (Sonoma)\r\n\r\n### DuckDB Version:\r\n\r\n0.10.0\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nMustafa Hasan Khan\r\n\r\n### Affiliation:\r\n\r\nAtlan\r\n\r\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\r\n\r\nI have tested with a nightly build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "Thanks, we can confirm that this is a bug.\nThanks, we can confirm that this is a bug.",
  "created_at": "2024-03-11T15:06:50Z"
}