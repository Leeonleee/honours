You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Incorrect result for MIN() on expression involving rowid
Consider the following statements:
```sql
CREATE TABLE t0(c0 INT, c1 INT);
INSERT INTO t0(c0) VALUES (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0),  (0), (0), (0), (0), (0), (0), (NULL), (NULL);
CREATE INDEX b ON t0(c1);
UPDATE t0 SET c1 = NULL;
SELECT MIN(100000000000000000<<t0.rowid) FROM t0; -- unexpected: {0}
```
Unexpectedly, the `SELECT` fetches 0, although there are values smaller than zero contained in the table. Interestingly, adding a `WHERE` clause results in a smaller minimum value:
```sql
SELECT MIN(100000000000000000<<t0.rowid) FROM t0 WHERE NOT c0; -- -8802109549835190272
```
I found this based on commit a382ba05a7d6c0abb0c8f8f50b2bdf3a4e480704.

</issue>
<code>
[start of README.md]
1: <img align="left" src="logo/duckdb-logo.png" height="120">
2: 
3: # DuckDB, the SQLite for Analytics
4: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
5: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
6: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
7: 
8: <br>
9: 
10: 
11: # Requirements
12: DuckDB requires [CMake](https://cmake.org) to be installed and a `C++11` compliant compiler. GCC 4.9 and newer, Clang 3.9 and newer and VisualStudio 2017 are tested on each revision.
13: 
14: ## Compiling
15: Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You may run `make unit` and `make allunit` to verify that your version works properly after making changes.
16: 
17: # Usage
18: A command line utility based on `sqlite3` can be found in either `build/release/duckdb_cli` (release, the default) or `build/debug/duckdb_cli` (debug).
19: 
20: # Embedding
21: As DuckDB is an embedded database, there is no database server to launch or client to connect to a running server. However, the database server can be embedded directly into an application using the C or C++ bindings. The main build process creates the shared library `build/release/src/libduckdb.[so|dylib|dll]` that can be linked against. A static library is built as well.
22: 
23: For examples on how to embed DuckDB into your application, see the [examples](https://github.com/cwida/duckdb/tree/master/examples) folder.
24: 
25: ## Benchmarks
26: After compiling, benchmarks can be executed from the root directory by executing `./build/release/benchmark/benchmark_runner`.
27: 
28: ## Standing on the Shoulders of Giants
29: DuckDB is implemented in C++ 11, should compile with GCC and clang, uses CMake to build and [Catch2](https://github.com/catchorg/Catch2) for testing. DuckDB uses some components from various Open-Source databases and draws inspiration from scientific publications. Here is an overview:
30: 
31: * Parser: We use the PostgreSQL parser that was [repackaged as a stand-alone library](https://github.com/lfittl/libpg_query). The translation to our own parse tree is inspired by [Peloton](https://pelotondb.io).
32: * Shell: We have adapted the [SQLite shell](https://sqlite.org/cli.html) to work with DuckDB.
33: * Tests: We use the [SQL Logic Tests from SQLite](https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki) to test DuckDB.
34: * Query fuzzing: We use [SQLsmith](https://github.com/anse1/sqlsmith) to generate random queries for additional testing.
35: * Date Math: We use the date math component from [MonetDB](https://www.monetdb.org).
36: * SQL Window Functions: DuckDB's window functions implementation uses Segment Tree Aggregation as described in the paper "Efficient Processing of Window Functions in Analytical SQL Queries" by Viktor Leis, Kan Kundhikanjana, Alfons Kemper and Thomas Neumann.
37: * Execution engine: The vectorized execution engine is inspired by the paper "MonetDB/X100: Hyper-Pipelining Query Execution" by Peter Boncz, Marcin Zukowski and Niels Nes.
38: * Optimizer: DuckDB's optimizer draws inspiration from the papers "Dynamic programming strikes back" by Guido Moerkotte and Thomas Neumman as well as "Unnesting Arbitrary Queries" by Thomas Neumann and Alfons Kemper.
39: * Concurrency control: Our MVCC implementation is inspired by the paper "Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems" by Thomas Neumann, Tobias Mühlbauer and Alfons Kemper.
40: * Regular Expression: DuckDB uses Google's [RE2](https://github.com/google/re2) regular expression engine.
41: 
42: ## Other pages
43: * [Continuous Benchmarking (CB™)](https://www.duckdb.org/benchmarks/index.html), runs TPC-H, TPC-DS and some microbenchmarks on every commit
[end of README.md]
[start of .gitignore]
1: #==============================================================================#
2: # This file specifies intentionally untracked files that git should ignore.
3: #==============================================================================#
4: 
5: #==============================================================================#
6: # File extensions to be ignored anywhere in the tree.
7: #==============================================================================#
8: # Temp files created by most text editors.
9: *~
10: # Merge files created by git.
11: *.orig
12: # Java bytecode
13: *.class
14: # Byte compiled python modules.
15: *.pyc
16: # vim swap files
17: .*.sw?
18: .sw?
19: #OS X specific files.
20: .DS_store
21: # Core files
22: #core
23: 
24: #==============================================================================#
25: # Explicit files to ignore (only matches one).
26: #==============================================================================#
27: # Various tag programs
28: /tags
29: /TAGS
30: /GPATH
31: /GRTAGS
32: /GSYMS
33: /GTAGS
34: .gitusers
35: autom4te.cache
36: cscope.files
37: cscope.out
38: autoconf/aclocal.m4
39: autoconf/autom4te.cache
40: /compile_commands.json
41: 
42: #==============================================================================#
43: # Directories to ignore (do not add trailing '/'s, they skip symlinks).
44: #==============================================================================#
45: # External projects that are tracked independently.
46: projects/*
47: !projects/*.*
48: !projects/Makefile
49: 
50: 
51: #==============================================================================#
52: # Autotools artifacts
53: #==============================================================================#
54: config/
55: configure
56: config-h.in
57: autom4te.cache
58: *Makefile.in
59: third_party/*/Makefile
60: libtool
61: aclocal.m4
62: config.log
63: config.status
64: stamp-h1
65: config.h
66: m4/libtool.m4
67: m4/ltoptions.m4
68: m4/ltsugar.m4
69: m4/ltversion.m4
70: m4/lt~obsolete.m4
71: 
72: #==============================================================================#
73: # Build artifacts
74: #==============================================================================#
75: #m4/
76: build/
77: #*.m4
78: *.o
79: *.lo
80: *.la
81: *~
82: *.pdf
83: *.swp
84: a.out
85: 
86: #==============================================================================#
87: # Kate Swap Files
88: #==============================================================================#
89: *.kate-swp
90: .#kate-*
91: 
92: #==============================================================================#
93: # Backup artifacts
94: #==============================================================================#
95: ~*
96: *~
97: tmp/
98: 
99: #==============================================================================#
100: # KDevelop files
101: #==============================================================================#
102: .kdev4
103: *.kdev4
104: .dirstamp
105: .deps
106: .libs
107: 
108: #==============================================================================#
109: # Eclipse files
110: #==============================================================================#
111: .wtpmodules
112: .classpath
113: .project
114: .cproject
115: .pydevproject
116: .settings
117: .autotools
118: .csettings
119: 
120: /Debug/
121: /misc/
122: 
123: #==============================================================================#
124: # Intellij files
125: #==============================================================================#
126: .idea
127: *.iml
128: 
129: #==============================================================================#
130: # Code Coverage files
131: #==============================================================================#
132: *.gcno
133: *.gcda
134: 
135: #==============================================================================#
136: # Scripts
137: #==============================================================================#
138: *.jar
139: scripts/PelotonTest/out
140: scripts/PelotonTest/lib
141: 
142: #==============================================================================#
143: # Protobuf
144: #==============================================================================#
145: *.pb-c.c
146: *.pb-c.h
147: *.pb.cc
148: *.pb.h
149: *.pb.go
150: 
151: #==============================================================================#
152: # Third party
153: #==============================================================================#
154: third_party/nanomsg/
155: third_party/nvml/
156: third_party/logcabin/
157: 
158: #==============================================================================#
159: # Eclipse
160: #==============================================================================#
161: 
162: .metadata
163: bin/
164: tmp/
165: *.tmp
166: *.bak
167: *.swp
168: *~.nib
169: local.properties
170: .settings/
171: .loadpath
172: .recommenders
173: 
174: # Eclipse Core
175: .project
176: 
177: # External tool builders
178: .externalToolBuilders/
179: 
180: # Locally stored "Eclipse launch configurations"
181: *.launch
182: 
183: # PyDev specific (Python IDE for Eclipse)
184: *.pydevproject
185: 
186: # CDT-specific (C/C++ Development Tooling)
187: .cproject
188: 
189: # JDT-specific (Eclipse Java Development Tools)
190: .classpath
191: 
192: # Java annotation processor (APT)
193: .factorypath
194: 
195: # PDT-specific (PHP Development Tools)
196: .buildpath
197: 
198: # sbteclipse plugin
199: .target
200: 
201: # Tern plugin
202: .tern-project
203: 
204: # TeXlipse plugin
205: .texlipse
206: 
207: # STS (Spring Tool Suite)
208: .springBeans
209: 
210: # Code Recommenders
211: .recommenders/
212: io_file
213: 
214: ## General
215: 
216: # Compiled Object files
217: *.slo
218: *.lo
219: *.o
220: *.cuo
221: 
222: # Compiled Dynamic libraries
223: *.so
224: *.dylib
225: 
226: # Compiled Static libraries
227: *.lai
228: *.la
229: *.a
230: 
231: # Compiled protocol buffers
232: *.pb.h
233: *.pb.cc
234: *_pb2.py
235: 
236: # Compiled python
237: *.pyc
238: 
239: # Compiled MATLAB
240: *.mex*
241: 
242: # IPython notebook checkpoints
243: .ipynb_checkpoints
244: 
245: # Editor temporaries
246: *.swp
247: *~
248: 
249: # Sublime Text settings
250: *.sublime-workspace
251: *.sublime-project
252: 
253: # Eclipse Project settings
254: *.*project
255: .settings
256: .csettings
257: 
258: # Visual Studio
259: .vs
260: settings.json
261: .vscode
262: 
263: # QtCreator files
264: *.user
265: 
266: # PyCharm files
267: .idea
268: 
269: # OSX dir files
270: .DS_Store
271: 
272: # User's build configuration
273: Makefile.config
274: 
275: # build, distribute, and bins (+ python proto bindings)
276: build
277: .build_debug/*
278: .build_release/*
279: distribute/*
280: *.testbin
281: *.bin
282: cmake_build
283: .cmake_build
284: cmake-build-debug
285: cmake-build-release
286: 
287: # Generated documentation
288: docs
289: 
290: # tests
291: test/test.sql
292: 
293: # SQLite logic tests
294: test/evidence/
295: third_party/sqllogictest
296: 
297: #imdb dataset
298: third_party/imdb/data
299: 
300: # Format timer
301: .last_format
302: # Benchmarks
303: .last_benchmarked_commit
304: benchmark_results/
305: duckdb_unittest_tempdir/
306: grammar.y.tmp
307: src/amalgamation/
[end of .gitignore]
[start of scripts/amalgamation.py]
1: # this script creates a single header + source file combination out of the DuckDB sources
2: import os, re, sys, shutil
3: amal_dir = os.path.join('src', 'amalgamation')
4: header_file = os.path.join(amal_dir, "duckdb.hpp")
5: source_file = os.path.join(amal_dir, "duckdb.cpp")
6: temp_header = 'duckdb.hpp.tmp'
7: temp_source = 'duckdb.cpp.tmp'
8: 
9: src_dir = 'src'
10: include_dir = os.path.join('src', 'include')
11: fmt_dir = os.path.join('third_party', 'fmt')
12: fmt_include_dir = os.path.join('third_party', 'fmt', 'include')
13: hll_dir = os.path.join('third_party', 'hyperloglog')
14: miniz_dir = os.path.join('third_party', 'miniz')
15: re2_dir = os.path.join('third_party', 're2')
16: pg_query_dir = os.path.join('third_party', 'libpg_query')
17: pg_query_include_dir = os.path.join('third_party', 'libpg_query', 'include')
18: 
19: utf8proc_dir = os.path.join('third_party', 'utf8proc')
20: utf8proc_include_dir = os.path.join('third_party', 'utf8proc', 'include')
21: 
22: # files included in the amalgamated "duckdb.hpp" file
23: main_header_files = [os.path.join(include_dir, 'duckdb.hpp'), os.path.join(include_dir, 'duckdb.h'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'date.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'timestamp.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'appender.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'client_context.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'function.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'table_function.hpp'), os.path.join(include_dir, 'duckdb', 'parser', 'parsed_data', 'create_table_function_info.hpp')]
24: 
25: # include paths for where to search for include files during amalgamation
26: include_paths = [include_dir, fmt_include_dir, hll_dir, re2_dir, miniz_dir, utf8proc_include_dir, utf8proc_dir, pg_query_include_dir, pg_query_dir]
27: # paths of where to look for files to compile and include to the final amalgamation
28: compile_directories = [src_dir, fmt_dir, hll_dir, miniz_dir, re2_dir, utf8proc_dir, pg_query_dir]
29: 
30: # files always excluded
31: always_excluded = ['src/amalgamation/duckdb.cpp', 'src/amalgamation/duckdb.hpp']
32: # files excluded from the amalgamation
33: excluded_files = ['grammar.cpp', 'grammar.hpp', 'symbols.cpp', 'file_system.cpp']
34: # files excluded from individual file compilation during test_compile
35: excluded_compilation_files = excluded_files + ['gram.hpp', 'kwlist.hpp', "duckdb-c.cpp"]
36: 
37: 
38: linenumbers = False
39: 
40: def get_includes(fpath, text):
41:     # find all the includes referred to in the directory
42:     include_statements = re.findall("(^[#]include[\t ]+[\"]([^\"]+)[\"])", text, flags=re.MULTILINE)
43:     include_files = []
44:     # figure out where they are located
45:     for included_file in [x[1] for x in include_statements]:
46:         included_file = os.sep.join(included_file.split('/'))
47:         found = False
48:         for include_path in include_paths:
49:             ipath = os.path.join(include_path, included_file)
50:             if os.path.isfile(ipath):
51:                 include_files.append(ipath)
52:                 found = True
53:                 break
54:         if not found:
55:             raise Exception('Could not find include file "' + included_file + '", included from file "' + fpath + '"')
56:     return ([x[0] for x in include_statements], include_files)
57: 
58: def cleanup_file(text):
59:     # remove all "#pragma once" notifications
60:     text = re.sub('#pragma once', '', text)
61:     return text
62: 
63: # recursively get all includes and write them
64: written_files = {}
65: 
66: def write_file(current_file, ignore_excluded = False):
67:     global linenumbers
68:     global written_files
69:     if current_file in always_excluded:
70:         return ""
71:     if current_file.split(os.sep)[-1] in excluded_files and not ignore_excluded:
72:         # file is in ignored files set
73:         return ""
74:     if current_file in written_files:
75:         # file is already written
76:         return ""
77:     written_files[current_file] = True
78: 
79:     # first read this file
80:     with open(current_file, 'r') as f:
81:         text = f.read()
82: 
83:     (statements, includes) = get_includes(current_file, text)
84:     # find the linenr of the final #include statement we parsed
85:     if len(statements) > 0:
86:         index = text.find(statements[-1])
87:         linenr = len(text[:index].split('\n'))
88: 
89:         # now write all the dependencies of this header first
90:         for i in range(len(includes)):
91:             include_text = write_file(includes[i])
92:             if linenumbers and i == len(includes) - 1:
93:                 # for the last include statement, we also include a #line directive
94:                 include_text += '\n#line %d "%s"\n' % (linenr, current_file)
95:             text = text.replace(statements[i], include_text)
96: 
97:     # add the initial line here
98:     if linenumbers:
99:         text = '\n#line 1 "%s"\n' % (current_file,) + text
100:     print(current_file)
101:     # now read the header and write it
102:     return cleanup_file(text)
103: 
104: def write_dir(dir, sfile):
105:     files = os.listdir(dir)
106:     files.sort()
107:     for fname in files:
108:         if fname in excluded_files:
109:             continue
110:         fpath = os.path.join(dir, fname)
111:         if os.path.isdir(fpath):
112:             write_dir(fpath, sfile)
113:         elif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):
114:             sfile.write(write_file(fpath))
115: 
116: def copy_if_different(src, dest):
117:     if os.path.isfile(dest):
118:         # dest exists, check if the files are different
119:         with open(src, 'r') as f:
120:             source_text = f.read()
121:         with open(dest, 'r') as f:
122:             dest_text = f.read()
123:         if source_text == dest_text:
124:             return
125:     shutil.copyfile(src, dest)
126: 
127: def generate_amalgamation(source_file, header_file):
128:     # now construct duckdb.hpp from these headers
129:     print("-----------------------")
130:     print("-- Writing " + header_file + " --")
131:     print("-----------------------")
132:     with open(temp_header, 'w+') as hfile:
133:         hfile.write("#pragma once\n")
134:         for fpath in main_header_files:
135:             hfile.write(write_file(fpath))
136: 
137: 
138:     # now construct duckdb.cpp
139:     print("------------------------")
140:     print("-- Writing " + source_file + " --")
141:     print("------------------------")
142: 
143:     # scan all the .cpp files
144:     with open(temp_source, 'w+') as sfile:
145:         header_file_name = header_file.split(os.sep)[-1]
146:         sfile.write('#include "' + header_file_name + '"\n\n')
147:         for compile_dir in compile_directories:
148:             write_dir(compile_dir, sfile)
149:         # for windows we write file_system.cpp last
150:         # this is because it includes windows.h which contains a lot of #define statements that mess up the other code
151:         sfile.write(write_file(os.path.join('src', 'common', 'file_system.cpp'), True))
152: 
153:     copy_if_different(temp_header, header_file)
154:     copy_if_different(temp_source, source_file)
155: 
156: 
157: 
158: if __name__ == "__main__":
159:     for arg in sys.argv:
160:         if arg == '--linenumbers':
161:             linenumbers = True
162:         elif arg == '--no-linenumbers':
163:             linenumbers = False
164:         elif arg.startswith('--header='):
165:             header_file = os.path.join(*arg.split('=', 1)[1].split('/'))
166:         elif arg.startswith('--source='):
167:             source_file = os.path.join(*arg.split('=', 1)[1].split('/'))
168:     if not os.path.exists(amal_dir):
169:         os.makedirs(amal_dir)
170: 
171:     generate_amalgamation(source_file, header_file)
[end of scripts/amalgamation.py]
[start of src/catalog/catalog_entry/CMakeLists.txt]
1: add_library_unity(duckdb_catalog_entries
2:                   OBJECT
3:                   schema_catalog_entry.cpp
4:                   sequence_catalog_entry.cpp
5:                   table_catalog_entry.cpp
6:                   table_function_catalog_entry.cpp
7:                   view_catalog_entry.cpp)
8: set(ALL_OBJECT_FILES ${ALL_OBJECT_FILES}
9:                      $<TARGET_OBJECTS:duckdb_catalog_entries> PARENT_SCOPE)
[end of src/catalog/catalog_entry/CMakeLists.txt]
[start of src/catalog/catalog_entry/table_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/serializer.hpp"
7: #include "duckdb/main/connection.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/parser/constraints/list.hpp"
10: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
11: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
12: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
13: #include "duckdb/planner/expression/bound_constant_expression.hpp"
14: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
15: #include "duckdb/storage/storage_manager.hpp"
16: #include "duckdb/planner/binder.hpp"
17: 
18: #include "duckdb/execution/index/art/art.hpp"
19: #include "duckdb/planner/expression/bound_reference_expression.hpp"
20: 
21: #include <algorithm>
22: 
23: using namespace duckdb;
24: using namespace std;
25: 
26: TableCatalogEntry::TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, BoundCreateTableInfo *info,
27:                                      std::shared_ptr<DataTable> inherited_storage)
28:     : StandardEntry(CatalogType::TABLE, schema, catalog, info->Base().table), storage(inherited_storage),
29:       columns(move(info->Base().columns)), constraints(move(info->Base().constraints)),
30:       bound_constraints(move(info->bound_constraints)), name_map(info->name_map) {
31: 	this->temporary = info->Base().temporary;
32: 	// add the "rowid" alias, if there is no rowid column specified in the table
33: 	if (name_map.find("rowid") == name_map.end()) {
34: 		name_map["rowid"] = COLUMN_IDENTIFIER_ROW_ID;
35: 	}
36: 	if (!storage) {
37: 		// create the physical storage
38: 		storage = make_shared<DataTable>(catalog->storage, schema->name, name, GetTypes(), move(info->data));
39: 
40: 		// create the unique indexes for the UNIQUE and PRIMARY KEY constraints
41: 		for (idx_t i = 0; i < bound_constraints.size(); i++) {
42: 			auto &constraint = bound_constraints[i];
43: 			if (constraint->type == ConstraintType::UNIQUE) {
44: 				// unique constraint: create a unique index
45: 				auto &unique = (BoundUniqueConstraint &)*constraint;
46: 				// fetch types and create expressions for the index from the columns
47: 				vector<column_t> column_ids;
48: 				vector<unique_ptr<Expression>> unbound_expressions;
49: 				vector<unique_ptr<Expression>> bound_expressions;
50: 				idx_t key_nr = 0;
51: 				for (auto &key : unique.keys) {
52: 					TypeId column_type = GetInternalType(columns[key].type);
53: 					assert(key < columns.size());
54: 
55: 					unbound_expressions.push_back(
56: 					    make_unique<BoundColumnRefExpression>(column_type, ColumnBinding(0, column_ids.size())));
57: 					bound_expressions.push_back(make_unique<BoundReferenceExpression>(column_type, key_nr++));
58: 					column_ids.push_back(key);
59: 				}
60: 				// create an adaptive radix tree around the expressions
61: 				auto art = make_unique<ART>(*storage, column_ids, move(unbound_expressions), true);
62: 
63: 				if (unique.is_primary_key) {
64: 					// if this is a primary key index, also create a NOT NULL constraint for each of the columns
65: 					for (auto &column_index : unique.keys) {
66: 						bound_constraints.push_back(make_unique<BoundNotNullConstraint>(column_index));
67: 					}
68: 				}
69: 				storage->AddIndex(move(art), bound_expressions);
70: 			}
71: 		}
72: 	}
73: }
74: 
75: bool TableCatalogEntry::ColumnExists(const string &name) {
76: 	return name_map.find(name) != name_map.end();
77: }
78: 
79: unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, AlterInfo *info) {
80: 	if (info->type != AlterType::ALTER_TABLE) {
81: 		throw CatalogException("Can only modify table with ALTER TABLE statement");
82: 	}
83: 	if (constraints.size() > 0) {
84: 		throw CatalogException("Cannot modify a table with constraints");
85: 	}
86: 	auto table_info = (AlterTableInfo *)info;
87: 	switch (table_info->alter_table_type) {
88: 	case AlterTableType::RENAME_COLUMN: {
89: 		auto rename_info = (RenameColumnInfo *)table_info;
90: 		auto create_info = make_unique<CreateTableInfo>(schema->name, name);
91: 		create_info->temporary = temporary;
92: 		bool found = false;
93: 		for (idx_t i = 0; i < columns.size(); i++) {
94: 			ColumnDefinition copy(columns[i].name, columns[i].type);
95: 			copy.oid = columns[i].oid;
96: 			copy.default_value = columns[i].default_value ? columns[i].default_value->Copy() : nullptr;
97: 
98: 			create_info->columns.push_back(move(copy));
99: 			if (rename_info->name == columns[i].name) {
100: 				assert(!found);
101: 				create_info->columns[i].name = rename_info->new_name;
102: 				found = true;
103: 			}
104: 		}
105: 		if (!found) {
106: 			throw CatalogException("Table does not have a column with name \"%s\"", rename_info->name.c_str());
107: 		}
108: 		assert(constraints.size() == 0);
109: 		// create_info->constraints.resize(constraints.size());
110: 		// for (idx_t i = 0; i < constraints.size(); i++) {
111: 		// 	create_info->constraints[i] = constraints[i]->Copy();
112: 		// }
113: 		Binder binder(context);
114: 		auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
115: 		return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
116: 		                                      storage);
117: 	}
118: 	case AlterTableType::RENAME_TABLE: {
119: 		auto rename_info = (RenameTableInfo *)table_info;
120: 		auto copied_table = Copy(context);
121: 		copied_table->name = rename_info->new_table_name;
122: 		return copied_table;
123: 	}
124: 	default:
125: 		throw CatalogException("Unrecognized alter table type!");
126: 	}
127: }
128: 
129: ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {
130: 	auto entry = name_map.find(name);
131: 	if (entry == name_map.end() || entry->second == COLUMN_IDENTIFIER_ROW_ID) {
132: 		throw CatalogException("Column with name %s does not exist!", name.c_str());
133: 	}
134: 	return columns[entry->second];
135: }
136: 
137: vector<TypeId> TableCatalogEntry::GetTypes() {
138: 	vector<TypeId> types;
139: 	for (auto &it : columns) {
140: 		types.push_back(GetInternalType(it.type));
141: 	}
142: 	return types;
143: }
144: 
145: vector<TypeId> TableCatalogEntry::GetTypes(const vector<column_t> &column_ids) {
146: 	vector<TypeId> result;
147: 	for (auto &index : column_ids) {
148: 		if (index == COLUMN_IDENTIFIER_ROW_ID) {
149: 			result.push_back(TypeId::INT64);
150: 		} else {
151: 			result.push_back(GetInternalType(columns[index].type));
152: 		}
153: 	}
154: 	return result;
155: }
156: 
157: void TableCatalogEntry::Serialize(Serializer &serializer) {
158: 	serializer.WriteString(schema->name);
159: 	serializer.WriteString(name);
160: 	assert(columns.size() <= std::numeric_limits<uint32_t>::max());
161: 	serializer.Write<uint32_t>((uint32_t)columns.size());
162: 	for (auto &column : columns) {
163: 		serializer.WriteString(column.name);
164: 		column.type.Serialize(serializer);
165: 		serializer.WriteOptional(column.default_value);
166: 	}
167: 	assert(constraints.size() <= std::numeric_limits<uint32_t>::max());
168: 	serializer.Write<uint32_t>((uint32_t)constraints.size());
169: 	for (auto &constraint : constraints) {
170: 		constraint->Serialize(serializer);
171: 	}
172: }
173: 
174: unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source) {
175: 	auto info = make_unique<CreateTableInfo>();
176: 
177: 	info->schema = source.Read<string>();
178: 	info->table = source.Read<string>();
179: 	auto column_count = source.Read<uint32_t>();
180: 
181: 	for (uint32_t i = 0; i < column_count; i++) {
182: 		auto column_name = source.Read<string>();
183: 		auto column_type = SQLType::Deserialize(source);
184: 		auto default_value = source.ReadOptional<ParsedExpression>();
185: 		info->columns.push_back(ColumnDefinition(column_name, column_type, move(default_value)));
186: 	}
187: 	auto constraint_count = source.Read<uint32_t>();
188: 
189: 	for (uint32_t i = 0; i < constraint_count; i++) {
190: 		auto constraint = Constraint::Deserialize(source);
191: 		info->constraints.push_back(move(constraint));
192: 	}
193: 	return info;
194: }
195: 
196: unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {
197: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
198: 	for (idx_t i = 0; i < columns.size(); i++) {
199: 		ColumnDefinition copy(columns[i].name, columns[i].type);
200: 		copy.oid = columns[i].oid;
201: 		copy.default_value = columns[i].default_value ? columns[i].default_value->Copy() : nullptr;
202: 		create_info->columns.push_back(move(copy));
203: 	}
204: 
205: 	for (idx_t i = 0; i < constraints.size(); i++) {
206: 		auto constraint = constraints[i]->Copy();
207: 		create_info->constraints.push_back(move(constraint));
208: 	}
209: 
210: 	Binder binder(context);
211: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
212: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
213: }
[end of src/catalog/catalog_entry/table_catalog_entry.cpp]
[start of src/catalog/catalog_set.cpp]
1: #include "duckdb/catalog/catalog_set.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/transaction/transaction_manager.hpp"
6: #include "duckdb/transaction/transaction.hpp"
7: #include "duckdb/common/serializer/buffered_serializer.hpp"
8: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
9: 
10: using namespace duckdb;
11: using namespace std;
12: 
13: CatalogSet::CatalogSet(Catalog &catalog) : catalog(catalog) {
14: }
15: 
16: bool CatalogSet::CreateEntry(Transaction &transaction, const string &name, unique_ptr<CatalogEntry> value,
17:                              unordered_set<CatalogEntry *> &dependencies) {
18: 	// lock the catalog for writing
19: 	lock_guard<mutex> write_lock(catalog.write_lock);
20: 	// lock this catalog set to disallow reading
21: 	lock_guard<mutex> read_lock(catalog_lock);
22: 
23: 	// first check if the entry exists in the unordered set
24: 	auto entry = data.find(name);
25: 	if (entry == data.end()) {
26: 		// if it does not: entry has never been created
27: 
28: 		// first create a dummy deleted entry for this entry
29: 		// so transactions started before the commit of this transaction don't
30: 		// see it yet
31: 		auto dummy_node = make_unique<CatalogEntry>(CatalogType::INVALID, value->catalog, name);
32: 		dummy_node->timestamp = 0;
33: 		dummy_node->deleted = true;
34: 		dummy_node->set = this;
35: 		data[name] = move(dummy_node);
36: 	} else {
37: 		// if it does, we have to check version numbers
38: 		CatalogEntry &current = *entry->second;
39: 		if (HasConflict(transaction, current)) {
40: 			// current version has been written to by a currently active
41: 			// transaction
42: 			throw TransactionException("Catalog write-write conflict on create with \"%s\"", current.name.c_str());
43: 		}
44: 		// there is a current version that has been committed
45: 		// if it has not been deleted there is a conflict
46: 		if (!current.deleted) {
47: 			return false;
48: 		}
49: 	}
50: 	// create a new entry and replace the currently stored one
51: 	// set the timestamp to the timestamp of the current transaction
52: 	// and point it at the dummy node
53: 	value->timestamp = transaction.transaction_id;
54: 	value->set = this;
55: 
56: 	// now add the dependency set of this object to the dependency manager
57: 	catalog.dependency_manager.AddObject(transaction, value.get(), dependencies);
58: 
59: 	value->child = move(data[name]);
60: 	value->child->parent = value.get();
61: 	// push the old entry in the undo buffer for this transaction
62: 	transaction.PushCatalogEntry(value->child.get());
63: 	data[name] = move(value);
64: 	return true;
65: }
66: 
67: bool CatalogSet::AlterEntry(ClientContext &context, const string &name, AlterInfo *alter_info) {
68: 	auto &transaction = Transaction::GetTransaction(context);
69: 	// lock the catalog for writing
70: 	lock_guard<mutex> write_lock(catalog.write_lock);
71: 
72: 	// first check if the entry exists in the unordered set
73: 	auto entry = data.find(name);
74: 	if (entry == data.end()) {
75: 		// if it does not: entry has never been created and cannot be altered
76: 		return false;
77: 	}
78: 	// if it does: we have to retrieve the entry and to check version numbers
79: 	CatalogEntry &current = *entry->second;
80: 	if (HasConflict(transaction, current)) {
81: 		// current version has been written to by a currently active
82: 		// transaction
83: 		throw TransactionException("Catalog write-write conflict on alter with \"%s\"", current.name.c_str());
84: 	}
85: 
86: 	// lock this catalog set to disallow reading
87: 	lock_guard<mutex> read_lock(catalog_lock);
88: 
89: 	// create a new entry and replace the currently stored one
90: 	// set the timestamp to the timestamp of the current transaction
91: 	// and point it to the updated table node
92: 	auto value = current.AlterEntry(context, alter_info);
93: 
94: 	// now transfer all dependencies from the old table to the new table
95: 	catalog.dependency_manager.AlterObject(transaction, data[name].get(), value.get());
96: 
97: 	value->timestamp = transaction.transaction_id;
98: 	value->child = move(data[name]);
99: 	value->child->parent = value.get();
100: 	value->set = this;
101: 
102: 	// serialize the AlterInfo into a temporary buffer
103: 	BufferedSerializer serializer;
104: 	alter_info->Serialize(serializer);
105: 	BinaryData serialized_alter = serializer.GetData();
106: 
107: 	// push the old entry in the undo buffer for this transaction
108: 	transaction.PushCatalogEntry(value->child.get(), serialized_alter.data.get(), serialized_alter.size);
109: 	data[name] = move(value);
110: 
111: 	return true;
112: }
113: 
114: bool CatalogSet::DropEntry(Transaction &transaction, const string &name, bool cascade) {
115: 	// lock the catalog for writing
116: 	lock_guard<mutex> write_lock(catalog.write_lock);
117: 	// we can only delete an entry that exists
118: 	auto entry = data.find(name);
119: 	if (entry == data.end()) {
120: 		return false;
121: 	}
122: 	if (HasConflict(transaction, *entry->second)) {
123: 		// current version has been written to by a currently active transaction
124: 		throw TransactionException("Catalog write-write conflict on drop with \"%s\"", name.c_str());
125: 	}
126: 	// there is a current version that has been committed by this transaction
127: 	if (entry->second->deleted) {
128: 		// if the entry was already deleted, it now does not exist anymore
129: 		// so we return that we could not find it
130: 		return false;
131: 	}
132: 
133: 	// lock this catalog for reading
134: 	// create the lock set for this delete operation
135: 	set_lock_map_t lock_set;
136: 	// now drop the entry
137: 	DropEntryInternal(transaction, *entry->second, cascade, lock_set);
138: 
139: 	return true;
140: }
141: 
142: void CatalogSet::DropEntryInternal(Transaction &transaction, CatalogEntry &current, bool cascade,
143:                                    set_lock_map_t &lock_set) {
144: 	assert(data.find(current.name) != data.end());
145: 	// first check any dependencies of this object
146: 	current.catalog->dependency_manager.DropObject(transaction, &current, cascade, lock_set);
147: 
148: 	// add this catalog to the lock set, if it is not there yet
149: 	if (lock_set.find(this) == lock_set.end()) {
150: 		lock_set.insert(make_pair(this, unique_lock<mutex>(catalog_lock)));
151: 	}
152: 
153: 	// create a new entry and replace the currently stored one
154: 	// set the timestamp to the timestamp of the current transaction
155: 	// and point it at the dummy node
156: 	auto value = make_unique<CatalogEntry>(CatalogType::DELETED_ENTRY, current.catalog, current.name);
157: 	value->timestamp = transaction.transaction_id;
158: 	value->child = move(data[current.name]);
159: 	value->child->parent = value.get();
160: 	value->set = this;
161: 	value->deleted = true;
162: 
163: 	// push the old entry in the undo buffer for this transaction
164: 	transaction.PushCatalogEntry(value->child.get());
165: 
166: 	data[current.name] = move(value);
167: }
168: 
169: bool CatalogSet::HasConflict(Transaction &transaction, CatalogEntry &current) {
170: 	return (current.timestamp >= TRANSACTION_ID_START && current.timestamp != transaction.transaction_id) ||
171: 	       (current.timestamp < TRANSACTION_ID_START && current.timestamp > transaction.start_time);
172: }
173: 
174: CatalogEntry *CatalogSet::GetEntryForTransaction(Transaction &transaction, CatalogEntry *current) {
175: 	while (current->child) {
176: 		if (current->timestamp == transaction.transaction_id) {
177: 			// we created this version
178: 			break;
179: 		}
180: 		if (current->timestamp < transaction.start_time) {
181: 			// this version was commited before we started the transaction
182: 			break;
183: 		}
184: 		current = current->child.get();
185: 		assert(current);
186: 	}
187: 	return current;
188: }
189: 
190: CatalogEntry *CatalogSet::GetEntry(Transaction &transaction, const string &name) {
191: 	lock_guard<mutex> lock(catalog_lock);
192: 
193: 	auto entry = data.find(name);
194: 	if (entry == data.end()) {
195: 		return nullptr;
196: 	}
197: 	// if it does, we have to check version numbers
198: 	CatalogEntry *current = GetEntryForTransaction(transaction, entry->second.get());
199: 	if (current->deleted) {
200: 		return nullptr;
201: 	}
202: 	return current;
203: }
204: 
205: CatalogEntry *CatalogSet::GetRootEntry(const string &name) {
206: 	lock_guard<mutex> lock(catalog_lock);
207: 	auto entry = data.find(name);
208: 	return entry == data.end() ? nullptr : entry->second.get();
209: }
210: 
211: void CatalogSet::Undo(CatalogEntry *entry) {
212: 	lock_guard<mutex> lock(catalog_lock);
213: 
214: 	// entry has to be restored
215: 	// and entry->parent has to be removed ("rolled back")
216: 
217: 	// i.e. we have to place (entry) as (entry->parent) again
218: 	auto &to_be_removed_node = entry->parent;
219: 	if (!to_be_removed_node->deleted) {
220: 		// delete the entry from the dependency manager as well
221: 		catalog.dependency_manager.EraseObject(to_be_removed_node);
222: 	}
223: 	if (to_be_removed_node->parent) {
224: 		// if the to be removed node has a parent, set the child pointer to the
225: 		// to be restored node
226: 		to_be_removed_node->parent->child = move(to_be_removed_node->child);
227: 		entry->parent = to_be_removed_node->parent;
228: 	} else {
229: 		// otherwise we need to update the base entry tables
230: 		auto &name = entry->name;
231: 		data[name] = move(to_be_removed_node->child);
232: 		entry->parent = nullptr;
233: 	}
234: }
[end of src/catalog/catalog_set.cpp]
[start of src/execution/operator/schema/physical_alter.cpp]
1: #include "duckdb/execution/operator/schema/physical_alter.hpp"
2: #include "duckdb/main/client_context.hpp"
3: 
4: using namespace duckdb;
5: using namespace std;
6: 
7: void PhysicalAlter::GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) {
8: 	context.catalog.AlterTable(context, (AlterTableInfo *)info.get());
9: 	state->finished = true;
10: }
[end of src/execution/operator/schema/physical_alter.cpp]
[start of src/execution/operator/schema/physical_create_index.cpp]
1: #include "duckdb/execution/operator/schema/physical_create_index.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: void PhysicalCreateIndex::CreateARTIndex() {
11: 	auto art = make_unique<ART>(*table.storage, column_ids, move(unbound_expressions), info->unique);
12: 
13: 	table.storage->AddIndex(move(art), expressions);
14: }
15: 
16: void PhysicalCreateIndex::GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) {
17: 	if (column_ids.size() == 0) {
18: 		throw NotImplementedException("CREATE INDEX does not refer to any columns in the base table!");
19: 	}
20: 
21: 	auto &schema = *table.schema;
22: 	if (!schema.CreateIndex(context, info.get())) {
23: 		// index already exists, but error ignored because of CREATE ... IF NOT
24: 		// EXISTS
25: 		return;
26: 	}
27: 
28: 	// create the chunk to hold intermediate expression results
29: 
30: 	switch (info->index_type) {
31: 	case IndexType::ART: {
32: 		CreateARTIndex();
33: 		break;
34: 	}
35: 	default:
36: 		assert(0);
37: 		throw NotImplementedException("Unimplemented index type");
38: 	}
39: 
40: 	chunk.SetCardinality(0);
41: 
42: 	state->finished = true;
43: }
[end of src/execution/operator/schema/physical_create_index.cpp]
[start of src/function/aggregate/distributive/minmax.cpp]
1: #include "duckdb/function/aggregate/distributive_functions.hpp"
2: #include "duckdb/common/exception.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/common/operator/comparison_operators.hpp"
5: #include "duckdb/common/vector_operations/aggregate_executor.hpp"
6: #include "duckdb/common/operator/aggregate_operators.hpp"
7: #include "duckdb/common/types/null_value.hpp"
8: 
9: using namespace std;
10: 
11: namespace duckdb {
12: 
13: struct MinMaxBase : public StandardDistributiveFunction {
14: 	template <class INPUT_TYPE, class STATE, class OP>
15: 	static void ConstantOperation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t count) {
16: 		assert(!nullmask[0]);
17: 		if (IsNullValue<INPUT_TYPE>(*state)) {
18: 			OP::template Assign<INPUT_TYPE, STATE>(state, input[0]);
19: 		} else {
20: 			OP::template Execute<INPUT_TYPE, STATE>(state, input[0]);
21: 		}
22: 	}
23: };
24: 
25: struct NumericMinMaxBase : public MinMaxBase {
26: 	template <class INPUT_TYPE, class STATE> static void Assign(STATE *state, INPUT_TYPE input) {
27: 		*state = input;
28: 	}
29: 
30: 	template <class T, class STATE>
31: 	static void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {
32: 		nullmask[idx] = IsNullValue<T>(*state);
33: 		target[idx] = *state;
34: 	}
35: };
36: 
37: struct MinOperation : public NumericMinMaxBase {
38: 	template <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {
39: 		if (LessThan::Operation<INPUT_TYPE>(input, *state)) {
40: 			*state = input;
41: 		}
42: 	}
43: };
44: 
45: struct MaxOperation : public NumericMinMaxBase {
46: 	template <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {
47: 		if (GreaterThan::Operation<INPUT_TYPE>(input, *state)) {
48: 			*state = input;
49: 		}
50: 	}
51: };
52: 
53: struct StringMinMaxBase : public MinMaxBase {
54: 	template <class STATE> static void Destroy(STATE *state) {
55: 		if (!state->IsInlined()) {
56: 			delete[] state->GetData();
57: 		}
58: 	}
59: 
60: 	template <class INPUT_TYPE, class STATE> static void Assign(STATE *state, INPUT_TYPE input) {
61: 		if (input.IsInlined()) {
62: 			*state = input;
63: 		} else {
64: 			// non-inlined string, need to allocate space for it
65: 			auto len = input.GetSize();
66: 			auto ptr = new char[len + 1];
67: 			memcpy(ptr, input.GetData(), len + 1);
68: 
69: 			*state = string_t(ptr, len);
70: 		}
71: 	}
72: 
73: 	template <class T, class STATE>
74: 	static void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {
75: 		if (IsNullValue<string_t>(*state)) {
76: 			nullmask[idx] = true;
77: 		} else {
78: 			target[idx] = StringVector::AddString(result, *state);
79: 		}
80: 	}
81: };
82: 
83: struct MinOperationString : public StringMinMaxBase {
84: 	template <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {
85: 		if (LessThan::Operation<INPUT_TYPE>(input, *state)) {
86: 			Assign(state, input);
87: 		}
88: 	}
89: };
90: 
91: struct MaxOperationString : public StringMinMaxBase {
92: 	template <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {
93: 		if (GreaterThan::Operation<INPUT_TYPE>(input, *state)) {
94: 			Assign(state, input);
95: 		}
96: 	}
97: };
98: 
99: template <class OP, class OP_STRING> static void AddMinMaxOperator(AggregateFunctionSet &set) {
100: 	for (auto type : SQLType::ALL_TYPES) {
101: 		if (type.id == SQLTypeId::VARCHAR) {
102: 			set.AddFunction(AggregateFunction::UnaryAggregateDestructor<string_t, string_t, string_t, OP_STRING>(
103: 			    SQLType::VARCHAR, SQLType::VARCHAR));
104: 		} else {
105: 			set.AddFunction(AggregateFunction::GetUnaryAggregate<OP>(type));
106: 		}
107: 	}
108: }
109: 
110: void MinFun::RegisterFunction(BuiltinFunctions &set) {
111: 	AggregateFunctionSet min("min");
112: 	AddMinMaxOperator<MinOperation, MinOperationString>(min);
113: 	set.AddFunction(min);
114: }
115: 
116: void MaxFun::RegisterFunction(BuiltinFunctions &set) {
117: 	AggregateFunctionSet max("max");
118: 	AddMinMaxOperator<MaxOperation, MaxOperationString>(max);
119: 	set.AddFunction(max);
120: }
121: 
122: } // namespace duckdb
[end of src/function/aggregate/distributive/minmax.cpp]
[start of src/include/duckdb/catalog/catalog_entry.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_entry.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/enums/catalog_type.hpp"
13: #include "duckdb/common/exception.hpp"
14: #include <memory>
15: 
16: namespace duckdb {
17: struct AlterInfo;
18: class Catalog;
19: class CatalogSet;
20: class ClientContext;
21: 
22: //! Abstract base class of an entry in the catalog
23: class CatalogEntry {
24: public:
25: 	CatalogEntry(CatalogType type, Catalog *catalog, string name)
26: 	    : type(type), catalog(catalog), set(nullptr), name(name), deleted(false), temporary(false), parent(nullptr) {
27: 	}
28: 	virtual ~CatalogEntry();
29: 
30: 	virtual unique_ptr<CatalogEntry> AlterEntry(ClientContext &context, AlterInfo *info) {
31: 		throw CatalogException("Unsupported alter type for catalog entry!");
32: 	}
33: 
34: 	virtual unique_ptr<CatalogEntry> Copy(ClientContext &context) {
35: 		throw CatalogException("Unsupported copy type for catalog entry!");
36: 	}
37: 	//! The type of this catalog entry
38: 	CatalogType type;
39: 	//! Reference to the catalog this entry belongs to
40: 	Catalog *catalog;
41: 	//! Reference to the catalog set this entry is stored in
42: 	CatalogSet *set;
43: 	//! The name of the entry
44: 	string name;
45: 	//! Whether or not the object is deleted
46: 	bool deleted;
47: 	//! Whether or not the object is temporary and should not be added to the WAL
48: 	bool temporary;
49: 	//! Timestamp at which the catalog entry was created
50: 	transaction_t timestamp;
51: 	//! Child entry
52: 	unique_ptr<CatalogEntry> child;
53: 	//! Parent entry (the node that owns this node)
54: 	CatalogEntry *parent;
55: };
56: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_entry.hpp]
[start of src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_entry/index_catalog_entry.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/standard_entry.hpp"
12: #include "duckdb/parser/parsed_data/create_index_info.hpp"
13: 
14: namespace duckdb {
15: 
16: //! An index catalog entry
17: class IndexCatalogEntry : public StandardEntry {
18: public:
19: 	//! Create a real TableCatalogEntry and initialize storage for it
20: 	IndexCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, CreateIndexInfo *info)
21: 	    : StandardEntry(CatalogType::INDEX, schema, catalog, info->index_name) {
22: 		// FIXME: add more information for drop index support
23: 	}
24: };
25: 
26: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp]
[start of src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_entry/table_catalog_entry.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/standard_entry.hpp"
12: #include "duckdb/common/unordered_map.hpp"
13: #include "duckdb/parser/column_definition.hpp"
14: #include "duckdb/parser/constraint.hpp"
15: #include "duckdb/planner/bound_constraint.hpp"
16: #include "duckdb/planner/expression.hpp"
17: 
18: namespace duckdb {
19: 
20: class ColumnStatistics;
21: class DataTable;
22: struct CreateTableInfo;
23: struct BoundCreateTableInfo;
24: 
25: //! A table catalog entry
26: class TableCatalogEntry : public StandardEntry {
27: public:
28: 	//! Create a real TableCatalogEntry and initialize storage for it
29: 	TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, BoundCreateTableInfo *info,
30: 	                  std::shared_ptr<DataTable> inherited_storage = nullptr);
31: 
32: 	//! A reference to the underlying storage unit used for this table
33: 	std::shared_ptr<DataTable> storage;
34: 	//! A list of columns that are part of this table
35: 	vector<ColumnDefinition> columns;
36: 	//! A list of constraints that are part of this table
37: 	vector<unique_ptr<Constraint>> constraints;
38: 	//! A list of constraints that are part of this table
39: 	vector<unique_ptr<BoundConstraint>> bound_constraints;
40: 	//! A map of column name to column index
41: 	unordered_map<string, column_t> name_map;
42: 
43: public:
44: 	unique_ptr<CatalogEntry> AlterEntry(ClientContext &context, AlterInfo *info) override;
45: 	//! Returns whether or not a column with the given name exists
46: 	bool ColumnExists(const string &name);
47: 	//! Returns a reference to the column of the specified name. Throws an
48: 	//! exception if the column does not exist.
49: 	ColumnDefinition &GetColumn(const string &name);
50: 	//! Returns a list of types of the table
51: 	vector<TypeId> GetTypes();
52: 	//! Returns a list of types of the specified columns of the table
53: 	vector<TypeId> GetTypes(const vector<column_t> &column_ids);
54: 
55: 	//! Serialize the meta information of the TableCatalogEntry a serializer
56: 	virtual void Serialize(Serializer &serializer);
57: 	//! Deserializes to a CreateTableInfo
58: 	static unique_ptr<CreateTableInfo> Deserialize(Deserializer &source);
59: 
60: 	unique_ptr<CatalogEntry> Copy(ClientContext &context) override;
61: };
62: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp]
[start of src/include/duckdb/execution/operator/schema/physical_create_index.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/schema/physical_create_index.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/execution/index/art/art.hpp"
13: #include "duckdb/parser/parsed_data/create_index_info.hpp"
14: 
15: #include "duckdb/storage/data_table.hpp"
16: 
17: #include <fstream>
18: 
19: namespace duckdb {
20: 
21: //! Physically CREATE INDEX statement
22: class PhysicalCreateIndex : public PhysicalOperator {
23: public:
24: 	PhysicalCreateIndex(LogicalOperator &op, TableCatalogEntry &table, vector<column_t> column_ids,
25: 	                    vector<unique_ptr<Expression>> expressions, unique_ptr<CreateIndexInfo> info,
26: 	                    vector<unique_ptr<Expression>> unbinded_expressions)
27: 	    : PhysicalOperator(PhysicalOperatorType::CREATE_INDEX, op.types), table(table), column_ids(column_ids),
28: 	      expressions(move(expressions)), info(std::move(info)), unbound_expressions(move(unbinded_expressions)) {
29: 	}
30: 
31: 	//! The table to create the index for
32: 	TableCatalogEntry &table;
33: 	//! The list of column IDs required for the index
34: 	vector<column_t> column_ids;
35: 	//! Set of expressions to index by
36: 	vector<unique_ptr<Expression>> expressions;
37: 	//! Info for index creation
38: 	unique_ptr<CreateIndexInfo> info;
39: 	//! Unbinded expressions to be used in the optimizer
40: 	vector<unique_ptr<Expression>> unbound_expressions;
41: 
42: public:
43: 	void GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) override;
44: 
45: private:
46: 	void CreateARTIndex();
47: };
48: } // namespace duckdb
[end of src/include/duckdb/execution/operator/schema/physical_create_index.hpp]
[start of src/include/duckdb/function/aggregate/distributive_functions.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/aggregate/distributive_functions.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/function/aggregate_function.hpp"
12: #include "duckdb/function/function_set.hpp"
13: #include "duckdb/common/types/null_value.hpp"
14: 
15: namespace duckdb {
16: 
17: struct StandardDistributiveFunction {
18: 	template <class STATE> static void Initialize(STATE *state) {
19: 		*state = NullValue<STATE>();
20: 	}
21: 
22: 	template <class INPUT_TYPE, class STATE, class OP>
23: 	static void Operation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t idx) {
24: 		if (IsNullValue<INPUT_TYPE>(*state)) {
25: 			OP::template Assign<INPUT_TYPE, STATE>(state, input[idx]);
26: 		} else {
27: 			OP::template Execute<INPUT_TYPE, STATE>(state, input[idx]);
28: 		}
29: 	}
30: 
31: 	template <class STATE, class OP> static void Combine(STATE source, STATE *target) {
32: 		if (IsNullValue<STATE>(source)) {
33: 			// source is NULL, nothing to do
34: 			return;
35: 		}
36: 		if (IsNullValue<STATE>(*target)) {
37: 			// target is NULL, use source value directly
38: 			*target = source;
39: 		} else {
40: 			// else perform the operation
41: 			OP::template Execute<STATE, STATE>(target, source);
42: 		}
43: 	}
44: 
45: 	static bool IgnoreNull() {
46: 		return true;
47: 	}
48: };
49: 
50: struct BitAndFun {
51: 	static void RegisterFunction(BuiltinFunctions &set);
52: };
53: 
54: struct BitOrFun {
55: 	static void RegisterFunction(BuiltinFunctions &set);
56: };
57: 
58: struct BitXorFun {
59: 	static void RegisterFunction(BuiltinFunctions &set);
60: };
61: 
62: struct CountStarFun {
63: 	static AggregateFunction GetFunction();
64: 
65: 	static void RegisterFunction(BuiltinFunctions &set);
66: };
67: 
68: struct CountFun {
69: 	static AggregateFunction GetFunction();
70: 
71: 	static void RegisterFunction(BuiltinFunctions &set);
72: };
73: 
74: struct FirstFun {
75: 	static AggregateFunction GetFunction(SQLType type);
76: 
77: 	static void RegisterFunction(BuiltinFunctions &set);
78: };
79: 
80: struct MaxFun {
81: 	static void RegisterFunction(BuiltinFunctions &set);
82: };
83: 
84: struct MinFun {
85: 	static void RegisterFunction(BuiltinFunctions &set);
86: };
87: 
88: struct SumFun {
89: 	static void RegisterFunction(BuiltinFunctions &set);
90: };
91: 
92: struct StringAggFun {
93: 	static void RegisterFunction(BuiltinFunctions &set);
94: };
95: 
96: } // namespace duckdb
[end of src/include/duckdb/function/aggregate/distributive_functions.hpp]
[start of src/include/duckdb/function/aggregate_function.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/aggregate_function.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/function/function.hpp"
12: #include "duckdb/common/vector_operations/aggregate_executor.hpp"
13: 
14: namespace duckdb {
15: 
16: class BoundAggregateExpression;
17: 
18: //! The type used for sizing hashed aggregate function states
19: typedef idx_t (*aggregate_size_t)();
20: //! The type used for initializing hashed aggregate function states
21: typedef void (*aggregate_initialize_t)(data_ptr_t state);
22: //! The type used for updating hashed aggregate functions
23: typedef void (*aggregate_update_t)(Vector inputs[], idx_t input_count, Vector &state, idx_t count);
24: //! The type used for combining hashed aggregate states (optional)
25: typedef void (*aggregate_combine_t)(Vector &state, Vector &combined, idx_t count);
26: //! The type used for finalizing hashed aggregate function payloads
27: typedef void (*aggregate_finalize_t)(Vector &state, Vector &result, idx_t count);
28: //! Binds the scalar function and creates the function data
29: typedef unique_ptr<FunctionData> (*bind_aggregate_function_t)(BoundAggregateExpression &expr, ClientContext &context,
30:                                                               SQLType &return_type);
31: //! The type used for the aggregate destructor method. NOTE: this method is used in destructors and MAY NOT throw.
32: typedef void (*aggregate_destructor_t)(Vector &state, idx_t count);
33: 
34: //! The type used for updating simple (non-grouped) aggregate functions
35: typedef void (*aggregate_simple_update_t)(Vector inputs[], idx_t input_count, data_ptr_t state, idx_t count);
36: 
37: class AggregateFunction : public SimpleFunction {
38: public:
39: 	AggregateFunction(string name, vector<SQLType> arguments, SQLType return_type, aggregate_size_t state_size,
40: 	                  aggregate_initialize_t initialize, aggregate_update_t update, aggregate_combine_t combine,
41: 	                  aggregate_finalize_t finalize, aggregate_simple_update_t simple_update = nullptr,
42: 	                  bind_aggregate_function_t bind = nullptr, aggregate_destructor_t destructor = nullptr)
43: 	    : SimpleFunction(name, arguments, return_type, false), state_size(state_size), initialize(initialize),
44: 	      update(update), combine(combine), finalize(finalize), simple_update(simple_update), bind(bind),
45: 	      destructor(destructor) {
46: 	}
47: 
48: 	AggregateFunction(vector<SQLType> arguments, SQLType return_type, aggregate_size_t state_size,
49: 	                  aggregate_initialize_t initialize, aggregate_update_t update, aggregate_combine_t combine,
50: 	                  aggregate_finalize_t finalize, aggregate_simple_update_t simple_update = nullptr,
51: 	                  bind_aggregate_function_t bind = nullptr, aggregate_destructor_t destructor = nullptr)
52: 	    : AggregateFunction(string(), arguments, return_type, state_size, initialize, update, combine, finalize,
53: 	                        simple_update, bind, destructor) {
54: 	}
55: 
56: 	//! The hashed aggregate state sizing function
57: 	aggregate_size_t state_size;
58: 	//! The hashed aggregate state initialization function
59: 	aggregate_initialize_t initialize;
60: 	//! The hashed aggregate update state function
61: 	aggregate_update_t update;
62: 	//! The hashed aggregate combine states function
63: 	aggregate_combine_t combine;
64: 	//! The hashed aggregate finalization function
65: 	aggregate_finalize_t finalize;
66: 	//! The simple aggregate update function (may be null)
67: 	aggregate_simple_update_t simple_update;
68: 
69: 	//! The bind function (may be null)
70: 	bind_aggregate_function_t bind;
71: 	//! The destructor method (may be null)
72: 	aggregate_destructor_t destructor;
73: 
74: 	bool operator==(const AggregateFunction &rhs) const {
75: 		return state_size == rhs.state_size && initialize == rhs.initialize && update == rhs.update &&
76: 		       combine == rhs.combine && finalize == rhs.finalize;
77: 	}
78: 	bool operator!=(const AggregateFunction &rhs) const {
79: 		return !(*this == rhs);
80: 	}
81: 
82: public:
83: 	template <class STATE, class INPUT_TYPE, class RESULT_TYPE, class OP>
84: 	static AggregateFunction UnaryAggregate(SQLType input_type, SQLType return_type) {
85: 		return AggregateFunction(
86: 		    {input_type}, return_type, AggregateFunction::StateSize<STATE>,
87: 		    AggregateFunction::StateInitialize<STATE, OP>, AggregateFunction::UnaryScatterUpdate<STATE, INPUT_TYPE, OP>,
88: 		    AggregateFunction::StateCombine<STATE, OP>, AggregateFunction::StateFinalize<STATE, RESULT_TYPE, OP>,
89: 		    AggregateFunction::UnaryUpdate<STATE, INPUT_TYPE, OP>);
90: 	};
91: 	template <class STATE, class INPUT_TYPE, class RESULT_TYPE, class OP>
92: 	static AggregateFunction UnaryAggregateDestructor(SQLType input_type, SQLType return_type) {
93: 		auto aggregate = UnaryAggregate<STATE, INPUT_TYPE, RESULT_TYPE, OP>(input_type, return_type);
94: 		aggregate.destructor = AggregateFunction::StateDestroy<STATE, OP>;
95: 		return aggregate;
96: 	};
97: 	template <class STATE, class A_TYPE, class B_TYPE, class RESULT_TYPE, class OP>
98: 	static AggregateFunction BinaryAggregate(SQLType a_type, SQLType b_type, SQLType return_type) {
99: 		return AggregateFunction({a_type, b_type}, return_type, AggregateFunction::StateSize<STATE>,
100: 		                         AggregateFunction::StateInitialize<STATE, OP>,
101: 		                         AggregateFunction::BinaryScatterUpdate<STATE, A_TYPE, B_TYPE, OP>,
102: 		                         AggregateFunction::StateCombine<STATE, OP>,
103: 		                         AggregateFunction::StateFinalize<STATE, RESULT_TYPE, OP>,
104: 		                         AggregateFunction::BinaryUpdate<STATE, A_TYPE, B_TYPE, OP>);
105: 	};
106: 	template <class STATE, class A_TYPE, class B_TYPE, class RESULT_TYPE, class OP>
107: 	static AggregateFunction BinaryAggregateDestructor(SQLType a_type, SQLType b_type, SQLType return_type) {
108: 		auto aggregate = BinaryAggregate<STATE, A_TYPE, B_TYPE, RESULT_TYPE, OP>(a_type, b_type, return_type);
109: 		aggregate.destructor = AggregateFunction::StateDestroy<STATE, OP>;
110: 		return aggregate;
111: 	};
112: 
113: public:
114: 	template <class OP> static AggregateFunction GetNumericUnaryAggregate(SQLType type) {
115: 		switch (type.id) {
116: 		case SQLTypeId::TINYINT:
117: 			return UnaryAggregate<int8_t, int8_t, int8_t, OP>(type, type);
118: 		case SQLTypeId::SMALLINT:
119: 			return UnaryAggregate<int16_t, int16_t, int16_t, OP>(type, type);
120: 		case SQLTypeId::INTEGER:
121: 			return UnaryAggregate<int32_t, int32_t, int32_t, OP>(type, type);
122: 		case SQLTypeId::BIGINT:
123: 			return UnaryAggregate<int64_t, int64_t, int64_t, OP>(type, type);
124: 		case SQLTypeId::FLOAT:
125: 			return UnaryAggregate<float, float, float, OP>(type, type);
126: 		case SQLTypeId::DOUBLE:
127: 			return UnaryAggregate<double, double, double, OP>(type, type);
128: 		case SQLTypeId::DECIMAL:
129: 			return UnaryAggregate<double, double, double, OP>(type, type);
130: 		default:
131: 			throw NotImplementedException("Unimplemented numeric type for unary aggregate");
132: 		}
133: 	}
134: 
135: 	template <class OP> static AggregateFunction GetUnaryAggregate(SQLType type) {
136: 		switch (type.id) {
137: 		case SQLTypeId::BOOLEAN:
138: 			return UnaryAggregate<int8_t, int8_t, int8_t, OP>(type, type);
139: 		case SQLTypeId::TINYINT:
140: 		case SQLTypeId::SMALLINT:
141: 		case SQLTypeId::INTEGER:
142: 		case SQLTypeId::BIGINT:
143: 		case SQLTypeId::FLOAT:
144: 		case SQLTypeId::DOUBLE:
145: 		case SQLTypeId::DECIMAL:
146: 			return GetNumericUnaryAggregate<OP>(type);
147: 		case SQLTypeId::DATE:
148: 			return UnaryAggregate<date_t, date_t, date_t, OP>(type, type);
149: 		case SQLTypeId::TIMESTAMP:
150: 			return UnaryAggregate<timestamp_t, timestamp_t, timestamp_t, OP>(type, type);
151: 		case SQLTypeId::VARCHAR:
152: 			return UnaryAggregate<string_t, string_t, string_t, OP>(type, type);
153: 		default:
154: 			throw NotImplementedException("Unimplemented type for unary aggregate");
155: 		}
156: 	}
157: 
158: public:
159: 	template <class STATE> static idx_t StateSize() {
160: 		return sizeof(STATE);
161: 	}
162: 
163: 	template <class STATE, class OP> static void StateInitialize(data_ptr_t state) {
164: 		OP::Initialize((STATE *)state);
165: 	}
166: 
167: 	template <class STATE, class T, class OP>
168: 	static void UnaryScatterUpdate(Vector inputs[], idx_t input_count, Vector &states, idx_t count) {
169: 		assert(input_count == 1);
170: 		AggregateExecutor::UnaryScatter<STATE, T, OP>(inputs[0], states, count);
171: 	}
172: 
173: 	template <class STATE, class INPUT_TYPE, class OP>
174: 	static void UnaryUpdate(Vector inputs[], idx_t input_count, data_ptr_t state, idx_t count) {
175: 		assert(input_count == 1);
176: 		AggregateExecutor::UnaryUpdate<STATE, INPUT_TYPE, OP>(inputs[0], state, count);
177: 	}
178: 
179: 	template <class STATE, class A_TYPE, class B_TYPE, class OP>
180: 	static void BinaryScatterUpdate(Vector inputs[], idx_t input_count, Vector &states, idx_t count) {
181: 		assert(input_count == 2);
182: 		AggregateExecutor::BinaryScatter<STATE, A_TYPE, B_TYPE, OP>(inputs[0], inputs[1], states, count);
183: 	}
184: 
185: 	template <class STATE, class A_TYPE, class B_TYPE, class OP>
186: 	static void BinaryUpdate(Vector inputs[], idx_t input_count, data_ptr_t state, idx_t count) {
187: 		assert(input_count == 2);
188: 		AggregateExecutor::BinaryUpdate<STATE, A_TYPE, B_TYPE, OP>(inputs[0], inputs[1], state, count);
189: 	}
190: 
191: 	template <class STATE, class OP> static void StateCombine(Vector &source, Vector &target, idx_t count) {
192: 		AggregateExecutor::Combine<STATE, OP>(source, target, count);
193: 	}
194: 
195: 	template <class STATE, class RESULT_TYPE, class OP>
196: 	static void StateFinalize(Vector &states, Vector &result, idx_t count) {
197: 		AggregateExecutor::Finalize<STATE, RESULT_TYPE, OP>(states, result, count);
198: 	}
199: 
200: 	template <class STATE, class OP> static void StateDestroy(Vector &states, idx_t count) {
201: 		AggregateExecutor::Destroy<STATE, OP>(states, count);
202: 	}
203: };
204: 
205: } // namespace duckdb
[end of src/include/duckdb/function/aggregate_function.hpp]
[start of src/include/duckdb/parser/column_definition.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/column_definition.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/types/value.hpp"
13: #include "duckdb/parser/parsed_expression.hpp"
14: 
15: namespace duckdb {
16: 
17: //! A column of a table.
18: class ColumnDefinition {
19: public:
20: 	ColumnDefinition(string name, SQLType type) : name(name), type(type) {
21: 	}
22: 	ColumnDefinition(string name, SQLType type, unique_ptr<ParsedExpression> default_value)
23: 	    : name(name), type(type), default_value(move(default_value)) {
24: 	}
25: 
26: 	//! The name of the entry
27: 	string name;
28: 	//! The index of the column in the table
29: 	idx_t oid;
30: 	//! The type of the column
31: 	SQLType type;
32: 	//! The default value of the column (if any)
33: 	unique_ptr<ParsedExpression> default_value;
34: };
35: } // namespace duckdb
[end of src/include/duckdb/parser/column_definition.hpp]
[start of src/include/duckdb/parser/parsed_data/alter_table_info.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/parsed_data/alter_table_info.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/parsed_data/parse_info.hpp"
12: 
13: namespace duckdb {
14: 
15: enum class AlterType : uint8_t { INVALID = 0, ALTER_TABLE = 1 };
16: 
17: struct AlterInfo : public ParseInfo {
18: 	AlterInfo(AlterType type) : type(type) {
19: 	}
20: 	virtual ~AlterInfo() {
21: 	}
22: 
23: 	AlterType type;
24: 
25: 	virtual void Serialize(Serializer &serializer);
26: 	static unique_ptr<AlterInfo> Deserialize(Deserializer &source);
27: };
28: 
29: enum class AlterTableType : uint8_t { INVALID = 0, RENAME_COLUMN = 1, RENAME_TABLE = 2 };
30: 
31: struct AlterTableInfo : public AlterInfo {
32: 	AlterTableInfo(AlterTableType type, string schema, string table)
33: 	    : AlterInfo(AlterType::ALTER_TABLE), alter_table_type(type), schema(schema), table(table) {
34: 	}
35: 	virtual ~AlterTableInfo() override {
36: 	}
37: 
38: 	AlterTableType alter_table_type;
39: 	//! Schema name to alter to
40: 	string schema;
41: 	//! Table name to alter to
42: 	string table;
43: 
44: 	virtual void Serialize(Serializer &serializer) override;
45: 	static unique_ptr<AlterInfo> Deserialize(Deserializer &source);
46: };
47: 
48: struct RenameColumnInfo : public AlterTableInfo {
49: 	RenameColumnInfo(string schema, string table, string name, string new_name)
50: 	    : AlterTableInfo(AlterTableType::RENAME_COLUMN, schema, table), name(name), new_name(new_name) {
51: 	}
52: 	~RenameColumnInfo() override {
53: 	}
54: 
55: 	//! Column old name
56: 	string name;
57: 	//! Column new name
58: 	string new_name;
59: 
60: 	void Serialize(Serializer &serializer) override;
61: 	static unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);
62: };
63: 
64: struct RenameTableInfo : public AlterTableInfo {
65: 	RenameTableInfo(string schema, string table, string new_name)
66: 	    : AlterTableInfo(AlterTableType::RENAME_TABLE, schema, table), new_table_name(new_name) {
67: 	}
68: 	~RenameTableInfo() override {
69: 	}
70: 
71: 	//! Table new name
72: 	string new_table_name;
73: 
74: 	void Serialize(Serializer &serializer) override;
75: 	static unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);
76: };
77: 
78: } // namespace duckdb
[end of src/include/duckdb/parser/parsed_data/alter_table_info.hpp]
[start of src/include/duckdb/parser/statement/alter_table_statement.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/statement/alter_table_statement.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/parser/column_definition.hpp"
12: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
13: #include "duckdb/parser/sql_statement.hpp"
14: #include "duckdb/parser/tableref.hpp"
15: 
16: namespace duckdb {
17: 
18: class AlterTableStatement : public SQLStatement {
19: public:
20: 	AlterTableStatement(unique_ptr<AlterTableInfo> info) : SQLStatement(StatementType::ALTER_STATEMENT), info(std::move(info)){};
21: 
22: 	unique_ptr<TableRef> table;
23: 	unique_ptr<AlterTableInfo> info;
24: };
25: 
26: } // namespace duckdb
[end of src/include/duckdb/parser/statement/alter_table_statement.hpp]
[start of src/include/duckdb/parser/transformer.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/parser/transformer.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include "duckdb/common/enums/expression_type.hpp"
13: #include "duckdb/common/types.hpp"
14: #include "duckdb/common/unordered_map.hpp"
15: #include "duckdb/parser/tokens.hpp"
16: 
17: #include "pg_definitions.hpp"
18: #include "nodes/parsenodes.hpp"
19: 
20: namespace duckdb {
21: 
22: class ColumnDefinition;
23: struct OrderByNode;
24: 
25: //! The transformer class is responsible for transforming the internal Postgres
26: //! parser representation into the DuckDB representation
27: class Transformer {
28: public:
29: 	Transformer(Transformer *parent = nullptr) : parent(parent) {
30: 	}
31: 
32: 	//! Transforms a Postgres parse tree into a set of SQL Statements
33: 	bool TransformParseTree(PGList *tree, vector<unique_ptr<SQLStatement>> &statements);
34: 	string NodetypeToString(PGNodeTag type);
35: 
36: 	idx_t ParamCount() {
37: 		return parent ? parent->ParamCount() : prepared_statement_parameter_index;
38: 	}
39: 
40: private:
41: 	Transformer *parent;
42: 	//! The current prepared statement parameter index
43: 	idx_t prepared_statement_parameter_index = 0;
44: 	//! Holds window expressions defined by name. We need those when transforming the expressions referring to them.
45: 	unordered_map<string, PGWindowDef *> window_clauses;
46: 
47: 	void SetParamCount(idx_t new_count) {
48: 		if (parent) {
49: 			parent->SetParamCount(new_count);
50: 		} else {
51: 			this->prepared_statement_parameter_index = new_count;
52: 		}
53: 	}
54: 
55: private:
56: 	//! Transforms a Postgres statement into a single SQL statement
57: 	unique_ptr<SQLStatement> TransformStatement(PGNode *stmt);
58: 	//===--------------------------------------------------------------------===//
59: 	// Statement transformation
60: 	//===--------------------------------------------------------------------===//
61: 	//! Transform a Postgres T_PGSelectStmt node into a SelectStatement
62: 	unique_ptr<SelectStatement> TransformSelect(PGNode *node);
63: 	//! Transform a Postgres T_AlterStmt node into a AlterTableStatement
64: 	unique_ptr<AlterTableStatement> TransformAlter(PGNode *node);
65: 	//! Transform a Postgres T_PGRenameStmt node into a RenameStatement
66: 	unique_ptr<AlterTableStatement> TransformRename(PGNode *node);
67: 	//! Transform a Postgres T_PGCreateStmt node into a CreateStatement
68: 	unique_ptr<CreateStatement> TransformCreateTable(PGNode *node);
69: 	//! Transform a Postgres T_PGCreateStmt node into a CreateStatement
70: 	unique_ptr<CreateStatement> TransformCreateTableAs(PGNode *node);
71: 	//! Transform a Postgres node into a CreateStatement
72: 	unique_ptr<CreateStatement> TransformCreateSchema(PGNode *node);
73: 	//! Transform a Postgres T_PGCreateSeqStmt node into a CreateStatement
74: 	unique_ptr<CreateStatement> TransformCreateSequence(PGNode *node);
75: 	//! Transform a Postgres T_PGViewStmt node into a CreateStatement
76: 	unique_ptr<CreateStatement> TransformCreateView(PGNode *node);
77: 	//! Transform a Postgres T_PGIndexStmt node into CreateStatement
78: 	unique_ptr<CreateStatement> TransformCreateIndex(PGNode *node);
79: 	//! Transform a Postgres T_PGDropStmt node into a Drop[Table,Schema]Statement
80: 	unique_ptr<SQLStatement> TransformDrop(PGNode *node);
81: 	//! Transform a Postgres T_PGInsertStmt node into a InsertStatement
82: 	unique_ptr<InsertStatement> TransformInsert(PGNode *node);
83: 	//! Transform a Postgres T_PGCopyStmt node into a CopyStatement
84: 	unique_ptr<CopyStatement> TransformCopy(PGNode *node);
85: 	//! Transform a Postgres T_PGTransactionStmt node into a TransactionStatement
86: 	unique_ptr<TransactionStatement> TransformTransaction(PGNode *node);
87: 	//! Transform a Postgres T_DeleteStatement node into a DeleteStatement
88: 	unique_ptr<DeleteStatement> TransformDelete(PGNode *node);
89: 	//! Transform a Postgres T_PGUpdateStmt node into a UpdateStatement
90: 	unique_ptr<UpdateStatement> TransformUpdate(PGNode *node);
91: 	//! Transform a Postgres T_PGPragmaStmt node into a PragmaStatement
92: 	unique_ptr<PragmaStatement> TransformPragma(PGNode *node);
93: 	unique_ptr<ExplainStatement> TransformExplain(PGNode *node);
94: 	unique_ptr<VacuumStatement> TransformVacuum(PGNode *node);
95: 	unique_ptr<PragmaStatement> TransformShow(PGNode *node);
96: 
97: 	unique_ptr<PrepareStatement> TransformPrepare(PGNode *node);
98: 	unique_ptr<ExecuteStatement> TransformExecute(PGNode *node);
99: 	unique_ptr<DropStatement> TransformDeallocate(PGNode *node);
100: 
101: 	//===--------------------------------------------------------------------===//
102: 	// Query Node Transform
103: 	//===--------------------------------------------------------------------===//
104: 	//! Transform a Postgres T_PGSelectStmt node into a QueryNode
105: 	unique_ptr<QueryNode> TransformSelectNode(PGSelectStmt *node);
106: 
107: 	//===--------------------------------------------------------------------===//
108: 	// Expression Transform
109: 	//===--------------------------------------------------------------------===//
110: 	//! Transform a Postgres boolean expression into an Expression
111: 	unique_ptr<ParsedExpression> TransformBoolExpr(PGBoolExpr *root);
112: 	//! Transform a Postgres case expression into an Expression
113: 	unique_ptr<ParsedExpression> TransformCase(PGCaseExpr *root);
114: 	//! Transform a Postgres type cast into an Expression
115: 	unique_ptr<ParsedExpression> TransformTypeCast(PGTypeCast *root);
116: 	//! Transform a Postgres coalesce into an Expression
117: 	unique_ptr<ParsedExpression> TransformCoalesce(PGAExpr *root);
118: 	//! Transform a Postgres column reference into an Expression
119: 	unique_ptr<ParsedExpression> TransformColumnRef(PGColumnRef *root);
120: 	//! Transform a Postgres constant value into an Expression
121: 	unique_ptr<ParsedExpression> TransformValue(PGValue val);
122: 	//! Transform a Postgres operator into an Expression
123: 	unique_ptr<ParsedExpression> TransformAExpr(PGAExpr *root);
124: 	//! Transform a Postgres abstract expression into an Expression
125: 	unique_ptr<ParsedExpression> TransformExpression(PGNode *node);
126: 	//! Transform a Postgres function call into an Expression
127: 	unique_ptr<ParsedExpression> TransformFuncCall(PGFuncCall *root);
128: 
129: 	//! Transform a Postgres constant value into an Expression
130: 	unique_ptr<ParsedExpression> TransformConstant(PGAConst *c);
131: 
132: 	unique_ptr<ParsedExpression> TransformResTarget(PGResTarget *root);
133: 	unique_ptr<ParsedExpression> TransformNullTest(PGNullTest *root);
134: 	unique_ptr<ParsedExpression> TransformParamRef(PGParamRef *node);
135: 	unique_ptr<ParsedExpression> TransformNamedArg(PGNamedArgExpr *root);
136: 
137: 	unique_ptr<ParsedExpression> TransformSQLValueFunction(PGSQLValueFunction *node);
138: 
139: 	unique_ptr<ParsedExpression> TransformSubquery(PGSubLink *root);
140: 	//===--------------------------------------------------------------------===//
141: 	// Constraints transform
142: 	//===--------------------------------------------------------------------===//
143: 	unique_ptr<Constraint> TransformConstraint(PGListCell *cell);
144: 
145: 	unique_ptr<Constraint> TransformConstraint(PGListCell *cell, ColumnDefinition &column, idx_t index);
146: 
147: 	//===--------------------------------------------------------------------===//
148: 	// Collation transform
149: 	//===--------------------------------------------------------------------===//
150: 	unique_ptr<ParsedExpression> TransformCollateExpr(PGCollateClause *collate);
151: 
152: 	string TransformCollation(PGCollateClause *collate);
153: 
154: 	//===--------------------------------------------------------------------===//
155: 	// Helpers
156: 	//===--------------------------------------------------------------------===//
157: 	string TransformAlias(PGAlias *root);
158: 	void TransformCTE(PGWithClause *de_with_clause, SelectStatement &select);
159: 	unique_ptr<QueryNode> TransformRecursiveCTE(PGCommonTableExpr *node);
160: 	// Operator String to ExpressionType (e.g. + => OPERATOR_ADD)
161: 	ExpressionType OperatorToExpressionType(string &op);
162: 
163: 	unique_ptr<ParsedExpression> TransformUnaryOperator(string op, unique_ptr<ParsedExpression> child);
164: 	unique_ptr<ParsedExpression> TransformBinaryOperator(string op, unique_ptr<ParsedExpression> left,
165: 	                                                     unique_ptr<ParsedExpression> right);
166: 	//===--------------------------------------------------------------------===//
167: 	// TableRef transform
168: 	//===--------------------------------------------------------------------===//
169: 	//! Transform a Postgres node into a TableRef
170: 	unique_ptr<TableRef> TransformTableRefNode(PGNode *node);
171: 	//! Transform a Postgres FROM clause into a TableRef
172: 	unique_ptr<TableRef> TransformFrom(PGList *root);
173: 	//! Transform a Postgres table reference into a TableRef
174: 	unique_ptr<TableRef> TransformRangeVar(PGRangeVar *root);
175: 	//! Transform a Postgres table-producing function into a TableRef
176: 	unique_ptr<TableRef> TransformRangeFunction(PGRangeFunction *root);
177: 	//! Transform a Postgres join node into a TableRef
178: 	unique_ptr<TableRef> TransformJoin(PGJoinExpr *root);
179: 	//! Transform a table producing subquery into a TableRef
180: 	unique_ptr<TableRef> TransformRangeSubselect(PGRangeSubselect *root);
181: 	//! Transform a VALUES list into a set of expressions
182: 	unique_ptr<TableRef> TransformValuesList(PGList *list);
183: 
184: 	//! Transform a Postgres TypeName string into a SQLType
185: 	SQLType TransformTypeName(PGTypeName *name);
186: 
187: 	//! Transform a Postgres GROUP BY expression into a list of Expression
188: 	bool TransformGroupBy(PGList *group, vector<unique_ptr<ParsedExpression>> &result);
189: 	//! Transform a Postgres ORDER BY expression into an OrderByDescription
190: 	bool TransformOrderBy(PGList *order, vector<OrderByNode> &result);
191: 
192: 	//! Transform a Postgres SELECT clause into a list of Expressions
193: 	bool TransformExpressionList(PGList *list, vector<unique_ptr<ParsedExpression>> &result);
194: 
195: 	void TransformWindowDef(PGWindowDef *window_spec, WindowExpression *expr);
196: };
197: 
198: } // namespace duckdb
[end of src/include/duckdb/parser/transformer.hpp]
[start of src/include/duckdb/storage/column_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/column_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/data_chunk.hpp"
12: #include "duckdb/storage/table/append_state.hpp"
13: #include "duckdb/storage/table/scan_state.hpp"
14: #include "duckdb/storage/table/persistent_segment.hpp"
15: 
16: namespace duckdb {
17: class DataTable;
18: class PersistentSegment;
19: class Transaction;
20: 
21: class ColumnData {
22: public:
23: 	ColumnData();
24: 	//! Set up the column data with the set of persistent segments, returns the amount of rows
25: 	void Initialize(vector<unique_ptr<PersistentSegment>> &segments);
26: 
27: 	//! The type of the column
28: 	TypeId type;
29: 	//! The table of the column
30: 	DataTable *table;
31: 	//! The column index of the column
32: 	idx_t column_idx;
33: 	//! The segments holding the data of the column
34: 	SegmentTree data;
35: 	//! The amount of persistent rows
36: 	idx_t persistent_rows;
37: 
38: public:
39: 	//! Initialize a scan of the column
40: 	void InitializeScan(ColumnScanState &state);
41: 	//! Scan the next vector from the column
42: 	void Scan(Transaction &transaction, ColumnScanState &state, Vector &result);
43: 	//! Scan the next vector from the column and apply a selection vector to filter the data
44: 	void FilterScan(Transaction &transaction, ColumnScanState &state, Vector &result, SelectionVector &sel,
45: 	                idx_t &approved_tuple_count);
46: 	//! Scan the next vector from the column, throwing an exception if there are any outstanding updates
47: 	void IndexScan(ColumnScanState &state, Vector &result);
48: 	//! Executes the filters directly in the table's data
49: 	void Select(Transaction &transaction, ColumnScanState &state, Vector &result, SelectionVector &sel,
50: 	            idx_t &approved_tuple_count, vector<TableFilter> &tableFilter);
51: 	//! Initialize an appending phase for this column
52: 	void InitializeAppend(ColumnAppendState &state);
53: 	//! Append a vector of type [type] to the end of the column
54: 	void Append(ColumnAppendState &state, Vector &vector, idx_t count);
55: 	//! Revert a set of appends to the ColumnData
56: 	void RevertAppend(row_t start_row);
57: 
58: 	//! Update the specified row identifiers
59: 	void Update(Transaction &transaction, Vector &updates, Vector &row_ids, idx_t count);
60: 
61: 	//! Fetch the vector from the column data that belongs to this specific row
62: 	void Fetch(ColumnScanState &state, row_t row_id, Vector &result);
63: 	//! Fetch a specific row id and append it to the vector
64: 	void FetchRow(ColumnFetchState &state, Transaction &transaction, row_t row_id, Vector &result, idx_t result_idx);
65: 
66: private:
67: 	//! Append a transient segment
68: 	void AppendTransientSegment(idx_t start_row);
69: };
70: 
71: } // namespace duckdb
[end of src/include/duckdb/storage/column_data.hpp]
[start of src/include/duckdb/storage/data_table.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/data_table.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/enums/index_type.hpp"
12: #include "duckdb/common/types/data_chunk.hpp"
13: #include "duckdb/storage/index.hpp"
14: #include "duckdb/storage/table_statistics.hpp"
15: #include "duckdb/storage/block.hpp"
16: #include "duckdb/storage/column_data.hpp"
17: #include "duckdb/storage/table/column_segment.hpp"
18: #include "duckdb/storage/table/persistent_segment.hpp"
19: #include "duckdb/storage/table/version_manager.hpp"
20: #include "duckdb/transaction/local_storage.hpp"
21: 
22: #include <atomic>
23: #include <mutex>
24: #include <vector>
25: 
26: namespace duckdb {
27: class ClientContext;
28: class ColumnDefinition;
29: class StorageManager;
30: class TableCatalogEntry;
31: class Transaction;
32: 
33: typedef unique_ptr<vector<unique_ptr<PersistentSegment>>[]> persistent_data_t;
34: //! TableFilter represents a filter pushed down into the table scan.
35: class TableFilter {
36: public:
37: 	TableFilter(Value constant, ExpressionType comparison_type, idx_t column_index)
38: 	    : constant(constant), comparison_type(comparison_type), column_index(column_index){};
39: 	Value constant;
40: 	ExpressionType comparison_type;
41: 	idx_t column_index;
42: };
43: 
44: //! DataTable represents a physical table on disk
45: class DataTable {
46: public:
47: 	DataTable(StorageManager &storage, string schema, string table, vector<TypeId> types, persistent_data_t data);
48: 
49: 	//! The amount of elements in the table. Note that this number signifies the amount of COMMITTED entries in the
50: 	//! table. It can be inaccurate inside of transactions. More work is needed to properly support that.
51: 	std::atomic<idx_t> cardinality;
52: 	// schema of the table
53: 	string schema;
54: 	// name of the table
55: 	string table;
56: 	//! Types managed by data table
57: 	vector<TypeId> types;
58: 	//! A reference to the base storage manager
59: 	StorageManager &storage;
60: 	//! Indexes
61: 	vector<unique_ptr<Index>> indexes;
62: 
63: public:
64: 	void InitializeScan(TableScanState &state, vector<column_t> column_ids,
65: 	                    unordered_map<idx_t, vector<TableFilter>> *table_filter = nullptr);
66: 	void InitializeScan(Transaction &transaction, TableScanState &state, vector<column_t> column_ids,
67: 	                    unordered_map<idx_t, vector<TableFilter>> *table_filters = nullptr);
68: 	//! Scans up to STANDARD_VECTOR_SIZE elements from the table starting
69: 	//! from offset and store them in result. Offset is incremented with how many
70: 	//! elements were returned.
71: 	//! Returns true if all pushed down filters were executed during data fetching
72: 	void Scan(Transaction &transaction, DataChunk &result, TableScanState &state,
73: 	          unordered_map<idx_t, vector<TableFilter>> &table_filters);
74: 
75: 	//! Initialize an index scan with a single predicate and a comparison type (= <= < > >=)
76: 	void InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value value,
77: 	                         ExpressionType expr_type, vector<column_t> column_ids);
78: 	//! Initialize an index scan with two predicates and two comparison types (> >= < <=)
79: 	void InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value low_value,
80: 	                         ExpressionType low_type, Value high_value, ExpressionType high_type,
81: 	                         vector<column_t> column_ids);
82: 	//! Scans up to STANDARD_VECTOR_SIZE elements from the table from the given index structure
83: 	void IndexScan(Transaction &transaction, DataChunk &result, TableIndexScanState &state);
84: 
85: 	//! Fetch data from the specific row identifiers from the base table
86: 	void Fetch(Transaction &transaction, DataChunk &result, vector<column_t> &column_ids, Vector &row_ids,
87: 	           idx_t fetch_count, TableIndexScanState &state);
88: 
89: 	//! Append a DataChunk to the table. Throws an exception if the columns don't match the tables' columns.
90: 	void Append(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk);
91: 	//! Delete the entries with the specified row identifier from the table
92: 	void Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_ids, idx_t count);
93: 	//! Update the entries with the specified row identifier from the table
94: 	void Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids, vector<column_t> &column_ids,
95: 	            DataChunk &data);
96: 
97: 	//! Add an index to the DataTable
98: 	void AddIndex(unique_ptr<Index> index, vector<unique_ptr<Expression>> &expressions);
99: 
100: 	//! Begin appending structs to this table, obtaining necessary locks, etc
101: 	void InitializeAppend(TableAppendState &state);
102: 	//! Append a chunk to the table using the AppendState obtained from BeginAppend
103: 	void Append(Transaction &transaction, transaction_t commit_id, DataChunk &chunk, TableAppendState &state);
104: 	//! Revert a set of appends made by the given AppendState, used to revert appends in the event of an error during
105: 	//! commit (e.g. because of an I/O exception)
106: 	void RevertAppend(TableAppendState &state);
107: 
108: 	//! Append a chunk with the row ids [row_start, ..., row_start + chunk.size()] to all indexes of the table, returns
109: 	//! whether or not the append succeeded
110: 	bool AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start);
111: 	//! Remove a chunk with the row ids [row_start, ..., row_start + chunk.size()] from all indexes of the table
112: 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start);
113: 	//! Remove the chunk with the specified set of row identifiers from all indexes of the table
114: 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers);
115: 	//! Remove the row identifiers from all the indexes of the table
116: 	void RemoveFromIndexes(Vector &row_identifiers, idx_t count);
117: 	//! Is this a temporary table?
118: 	bool IsTemporary();
119: 
120: private:
121: 	//! Verify constraints with a chunk from the Append containing all columns of the table
122: 	void VerifyAppendConstraints(TableCatalogEntry &table, DataChunk &chunk);
123: 	//! Verify constraints with a chunk from the Update containing only the specified column_ids
124: 	void VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chunk, vector<column_t> &column_ids);
125: 
126: 	void InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index,
127: 	                         vector<column_t> column_ids);
128: 
129: 	bool CheckZonemap(TableScanState &state, unordered_map<idx_t, vector<TableFilter>> &table_filters,
130: 	                  idx_t &current_row);
131: 	bool ScanBaseTable(Transaction &transaction, DataChunk &result, TableScanState &state, idx_t &current_row,
132: 	                   idx_t max_row, idx_t base_row, VersionManager &manager,
133: 	                   unordered_map<idx_t, vector<TableFilter>> &table_filters);
134: 	bool ScanCreateIndex(CreateIndexScanState &state, DataChunk &result, idx_t &current_row, idx_t max_row,
135: 	                     idx_t base_row);
136: 
137: 	//! Figure out which of the row ids to use for the given transaction by looking at inserted/deleted data. Returns
138: 	//! the amount of rows to use and places the row_ids in the result_rows array.
139: 	idx_t FetchRows(Transaction &transaction, Vector &row_identifiers, idx_t fetch_count, row_t result_rows[]);
140: 
141: 	//! The CreateIndexScan is a special scan that is used to create an index on the table, it keeps locks on the table
142: 	void InitializeCreateIndexScan(CreateIndexScanState &state, vector<column_t> column_ids);
143: 	void CreateIndexScan(CreateIndexScanState &structure, DataChunk &result);
144: 	//! Lock for appending entries to the table
145: 	std::mutex append_lock;
146: 	//! The version manager of the persistent segments of the tree
147: 	VersionManager persistent_manager;
148: 	//! The version manager of the transient segments of the tree
149: 	VersionManager transient_manager;
150: 	//! The physical columns of the table
151: 	unique_ptr<ColumnData[]> columns;
152: };
153: } // namespace duckdb
[end of src/include/duckdb/storage/data_table.hpp]
[start of src/include/duckdb/storage/table/version_manager.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/version_manager.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/unordered_map.hpp"
13: #include "duckdb/common/types/vector.hpp"
14: 
15: #include "duckdb/storage/table/chunk_info.hpp"
16: #include "duckdb/storage/storage_lock.hpp"
17: 
18: namespace duckdb {
19: class DataTable;
20: class Transaction;
21: class VersionManager;
22: 
23: class VersionManager {
24: public:
25: 	VersionManager(DataTable &table) : table(table), max_row(0), base_row(0) {
26: 	}
27: 
28: 	//! The DataTable
29: 	DataTable &table;
30: 	//! The read/write lock for the delete info and insert info
31: 	StorageLock lock;
32: 	//! The info for each of the chunks
33: 	unordered_map<idx_t, unique_ptr<ChunkInfo>> info;
34: 	//! The maximum amount of rows managed by the version manager
35: 	idx_t max_row;
36: 	//! The base row of the version manager, i.e. when passing row = base_row, it will be treated as row = 0
37: 	idx_t base_row;
38: 
39: public:
40: 	//! For a given chunk index, fills the selection vector with the relevant tuples for a given transaction. If count
41: 	//! == max_count, all tuples are relevant and the selection vector is not set
42: 	idx_t GetSelVector(Transaction &transaction, idx_t index, SelectionVector &sel_vector, idx_t max_count);
43: 
44: 	//! Fetch a specific row from the VersionManager, returns true if the row should be used for the transaction and
45: 	//! false otherwise.
46: 	bool Fetch(Transaction &transaction, idx_t row);
47: 
48: 	//! Delete the given set of rows in the version manager
49: 	void Delete(Transaction &transaction, Vector &row_ids, idx_t count);
50: 	//! Append a set of rows to the version manager, setting their inserted id to the given commit_id
51: 	void Append(Transaction &transaction, row_t row_start, idx_t count, transaction_t commit_id);
52: 	//! Revert a set of appends made to the version manager from the rows [row_start] until [row_end]
53: 	void RevertAppend(row_t row_start, row_t row_end);
54: 
55: private:
56: 	ChunkInsertInfo *GetInsertInfo(idx_t chunk_idx);
57: };
58: 
59: } // namespace duckdb
[end of src/include/duckdb/storage/table/version_manager.hpp]
[start of src/include/duckdb/transaction/commit_state.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/transaction/commit_state.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/transaction/undo_buffer.hpp"
12: 
13: namespace duckdb {
14: class CatalogEntry;
15: class DataChunk;
16: class DataTable;
17: class WriteAheadLog;
18: 
19: struct DeleteInfo;
20: struct UpdateInfo;
21: 
22: class CommitState {
23: public:
24: 	CommitState(transaction_t commit_id, WriteAheadLog *log = nullptr);
25: 
26: 	WriteAheadLog *log;
27: 	transaction_t commit_id;
28: 	UndoFlags current_op;
29: 
30: 	DataTable *current_table;
31: 	idx_t row_identifiers[STANDARD_VECTOR_SIZE];
32: 
33: 	unique_ptr<DataChunk> delete_chunk;
34: 	unique_ptr<DataChunk> update_chunk;
35: 
36: public:
37: 	template <bool HAS_LOG> void CommitEntry(UndoFlags type, data_ptr_t data);
38: 	void RevertCommit(UndoFlags type, data_ptr_t data);
39: 
40: private:
41: 	void SwitchTable(DataTable *table, UndoFlags new_op);
42: 
43: 	void WriteCatalogEntry(CatalogEntry *entry, data_ptr_t extra_data);
44: 	void WriteDelete(DeleteInfo *info);
45: 	void WriteUpdate(UpdateInfo *info);
46: 
47: 	void AppendRowId(row_t rowid);
48: };
49: 
50: } // namespace duckdb
[end of src/include/duckdb/transaction/commit_state.hpp]
[start of src/include/duckdb/transaction/delete_info.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/transaction/delete_info.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: 
13: namespace duckdb {
14: class ChunkInfo;
15: class DataTable;
16: 
17: struct DeleteInfo {
18: 	ChunkInfo *vinfo;
19: 	idx_t count;
20: 	idx_t base_row;
21: 	row_t rows[1];
22: 
23: 	DataTable &GetTable();
24: };
25: 
26: } // namespace duckdb
[end of src/include/duckdb/transaction/delete_info.hpp]
[start of src/include/duckdb/transaction/local_storage.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/transaction/local_storage.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/chunk_collection.hpp"
12: #include "duckdb/storage/table/scan_state.hpp"
13: #include "duckdb/storage/index.hpp"
14: 
15: namespace duckdb {
16: class DataTable;
17: class WriteAheadLog;
18: struct TableAppendState;
19: 
20: class LocalTableStorage {
21: public:
22: 	LocalTableStorage(DataTable &table);
23: 	~LocalTableStorage();
24: 
25: 	//! The main chunk collection holding the data
26: 	ChunkCollection collection;
27: 	//! The set of unique indexes
28: 	vector<unique_ptr<Index>> indexes;
29: 	//! The set of deleted entries
30: 	unordered_map<idx_t, unique_ptr<bool[]>> deleted_entries;
31: 	//! The max row
32: 	row_t max_row;
33: 
34: public:
35: 	void InitializeScan(LocalScanState &state);
36: 
37: 	void Clear();
38: };
39: 
40: //! The LocalStorage class holds appends that have not been committed yet
41: class LocalStorage {
42: public:
43: 	struct CommitState {
44: 		unordered_map<DataTable *, unique_ptr<TableAppendState>> append_states;
45: 	};
46: 
47: public:
48: 	//! Initialize a scan of the local storage
49: 	void InitializeScan(DataTable *table, LocalScanState &state);
50: 	//! Scan
51: 	void Scan(LocalScanState &state, const vector<column_t> &column_ids, DataChunk &result,
52: 	          unordered_map<idx_t, vector<TableFilter>> *table_filters = nullptr);
53: 
54: 	//! Append a chunk to the local storage
55: 	void Append(DataTable *table, DataChunk &chunk);
56: 	//! Delete a set of rows from the local storage
57: 	void Delete(DataTable *table, Vector &row_ids, idx_t count);
58: 	//! Update a set of rows in the local storage
59: 	void Update(DataTable *table, Vector &row_ids, vector<column_t> &column_ids, DataChunk &data);
60: 
61: 	//! Commits the local storage, writing it to the WAL and completing the commit
62: 	void Commit(LocalStorage::CommitState &commit_state, Transaction &transaction, WriteAheadLog *log,
63: 	            transaction_t commit_id);
64: 	//! Revert the commit made so far by the LocalStorage
65: 	void RevertCommit(LocalStorage::CommitState &commit_state);
66: 
67: 	bool ChangesMade() noexcept {
68: 		return table_storage.size() > 0;
69: 	}
70: 
71: private:
72: 	LocalTableStorage *GetStorage(DataTable *table);
73: 
74: 	template <class T> bool ScanTableStorage(DataTable *table, LocalTableStorage *storage, T &&fun);
75: 
76: private:
77: 	unordered_map<DataTable *, unique_ptr<LocalTableStorage>> table_storage;
78: };
79: 
80: } // namespace duckdb
[end of src/include/duckdb/transaction/local_storage.hpp]
[start of src/include/duckdb/transaction/transaction.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/transaction/transaction.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
12: #include "duckdb/common/types/data_chunk.hpp"
13: #include "duckdb/common/unordered_map.hpp"
14: #include "duckdb/transaction/undo_buffer.hpp"
15: #include "duckdb/transaction/local_storage.hpp"
16: 
17: namespace duckdb {
18: class SequenceCatalogEntry;
19: 
20: class ClientContext;
21: class CatalogEntry;
22: class DataTable;
23: class WriteAheadLog;
24: 
25: class ChunkInfo;
26: 
27: struct DeleteInfo;
28: struct UpdateInfo;
29: 
30: //! The transaction object holds information about a currently running or past
31: //! transaction
32: 
33: class Transaction {
34: public:
35: 	Transaction(transaction_t start_time, transaction_t transaction_id, timestamp_t start_timestamp)
36: 	    : start_time(start_time), transaction_id(transaction_id), commit_id(0), highest_active_query(0),
37: 	      active_query(MAXIMUM_QUERY_ID), start_timestamp(start_timestamp), is_invalidated(false) {
38: 	}
39: 
40: 	//! The start timestamp of this transaction
41: 	transaction_t start_time;
42: 	//! The transaction id of this transaction
43: 	transaction_t transaction_id;
44: 	//! The commit id of this transaction, if it has successfully been committed
45: 	transaction_t commit_id;
46: 	//! Highest active query when the transaction finished, used for cleaning up
47: 	transaction_t highest_active_query;
48: 	//! The current active query for the transaction. Set to MAXIMUM_QUERY_ID if
49: 	//! no query is active.
50: 	transaction_t active_query;
51: 	//! The timestamp when the transaction started
52: 	timestamp_t start_timestamp;
53: 	//! The set of uncommitted appends for the transaction
54: 	LocalStorage storage;
55: 	//! Map of all sequences that were used during the transaction and the value they had in this transaction
56: 	unordered_map<SequenceCatalogEntry *, SequenceValue> sequence_usage;
57: 	//! Whether or not the transaction has been invalidated
58: 	bool is_invalidated;
59: 
60: public:
61: 	static Transaction &GetTransaction(ClientContext &context);
62: 
63: 	void PushCatalogEntry(CatalogEntry *entry, data_ptr_t extra_data = nullptr, idx_t extra_data_size = 0);
64: 
65: 	//! Commit the current transaction with the given commit identifier. Returns an error message if the transaction
66: 	//! commit failed, or an empty string if the commit was sucessful
67: 	string Commit(WriteAheadLog *log, transaction_t commit_id) noexcept;
68: 	//! Rollback
69: 	void Rollback() noexcept {
70: 		undo_buffer.Rollback();
71: 	}
72: 	//! Cleanup the undo buffer
73: 	void Cleanup() {
74: 		undo_buffer.Cleanup();
75: 	}
76: 
77: 	timestamp_t GetCurrentTransactionStartTimestamp() {
78: 		return start_timestamp;
79: 	}
80: 
81: 	void PushDelete(ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row);
82: 
83: 	UpdateInfo *CreateUpdateInfo(idx_t type_size, idx_t entries);
84: 
85: private:
86: 	//! The undo buffer is used to store old versions of rows that are updated
87: 	//! or deleted
88: 	UndoBuffer undo_buffer;
89: 
90: 	Transaction(const Transaction &) = delete;
91: };
92: 
93: } // namespace duckdb
[end of src/include/duckdb/transaction/transaction.hpp]
[start of src/optimizer/index_scan.cpp]
1: #include "duckdb/optimizer/index_scan.hpp"
2: #include "duckdb/optimizer/matcher/expression_matcher.hpp"
3: 
4: #include "duckdb/parser/expression/comparison_expression.hpp"
5: 
6: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
7: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
8: #include "duckdb/planner/expression/bound_constant_expression.hpp"
9: #include "duckdb/planner/expression_iterator.hpp"
10: #include "duckdb/planner/operator/logical_filter.hpp"
11: #include "duckdb/planner/operator/logical_get.hpp"
12: #include "duckdb/planner/operator/logical_index_scan.hpp"
13: 
14: #include "duckdb/storage/data_table.hpp"
15: using namespace duckdb;
16: using namespace std;
17: 
18: unique_ptr<LogicalOperator> IndexScan::Optimize(unique_ptr<LogicalOperator> op) {
19: 	if (op->type == LogicalOperatorType::FILTER && op->children[0]->type == LogicalOperatorType::GET) {
20: 		return TransformFilterToIndexScan(move(op));
21: 	}
22: 	for (auto &child : op->children) {
23: 		child = Optimize(move(child));
24: 	}
25: 	return op;
26: }
27: 
28: static void RewriteIndexExpression(Index &index, LogicalGet &get, Expression &expr, bool &rewrite_possible) {
29: 	if (expr.type == ExpressionType::BOUND_COLUMN_REF) {
30: 		auto &bound_colref = (BoundColumnRefExpression &)expr;
31: 		// bound column ref: rewrite to fit in the current set of bound column ids
32: 		bound_colref.binding.table_index = get.table_index;
33: 		column_t referenced_column = index.column_ids[bound_colref.binding.column_index];
34: 		// search for the referenced column in the set of column_ids
35: 		for (idx_t i = 0; i < get.column_ids.size(); i++) {
36: 			if (get.column_ids[i] == referenced_column) {
37: 				bound_colref.binding.column_index = i;
38: 				return;
39: 			}
40: 		}
41: 		// column id not found in bound columns in the LogicalGet: rewrite not possible
42: 		rewrite_possible = false;
43: 	}
44: 	ExpressionIterator::EnumerateChildren(
45: 	    expr, [&](Expression &child) { RewriteIndexExpression(index, get, child, rewrite_possible); });
46: }
47: 
48: unique_ptr<LogicalOperator> IndexScan::TransformFilterToIndexScan(unique_ptr<LogicalOperator> op) {
49: 	assert(op->type == LogicalOperatorType::FILTER);
50: 	auto &filter = (LogicalFilter &)*op;
51: 	auto get = (LogicalGet *)op->children[0].get();
52: 
53: 	if (!get->table) {
54: 		return op;
55: 	}
56: 
57: 	auto &storage = *get->table->storage;
58: 
59: 	if (storage.indexes.size() == 0) {
60: 		// no indexes on the table, can't rewrite
61: 		return op;
62: 	}
63: 
64: 	// check all the indexes
65: 	for (size_t j = 0; j < storage.indexes.size(); j++) {
66: 		auto &index = storage.indexes[j];
67: 
68: 		//		assert(index->unbound_expressions.size() == 1);
69: 		// first rewrite the index expression so the ColumnBindings align with the column bindings of the current table
70: 		if (index->unbound_expressions.size() > 1)
71: 			continue;
72: 		auto index_expression = index->unbound_expressions[0]->Copy();
73: 		bool rewrite_possible = true;
74: 		RewriteIndexExpression(*index, *get, *index_expression, rewrite_possible);
75: 		if (!rewrite_possible) {
76: 			// could not rewrite!
77: 			continue;
78: 		}
79: 
80: 		Value low_value, high_value, equal_value;
81: 		// try to find a matching index for any of the filter expressions
82: 		auto expr = filter.expressions[0].get();
83: 		auto low_comparison_type = expr->type;
84: 		auto high_comparison_type = expr->type;
85: 		for (idx_t i = 0; i < filter.expressions.size(); i++) {
86: 			expr = filter.expressions[i].get();
87: 			// create a matcher for a comparison with a constant
88: 			ComparisonExpressionMatcher matcher;
89: 			// match on a comparison type
90: 			matcher.expr_type = make_unique<ComparisonExpressionTypeMatcher>();
91: 			// match on a constant comparison with the indexed expression
92: 			matcher.matchers.push_back(make_unique<ExpressionEqualityMatcher>(index_expression.get()));
93: 			matcher.matchers.push_back(make_unique<ConstantExpressionMatcher>());
94: 
95: 			matcher.policy = SetMatcher::Policy::UNORDERED;
96: 
97: 			vector<Expression *> bindings;
98: 			if (matcher.Match(expr, bindings)) {
99: 				// range or equality comparison with constant value
100: 				// we can use our index here
101: 				// bindings[0] = the expression
102: 				// bindings[1] = the index expression
103: 				// bindings[2] = the constant
104: 				auto comparison = (BoundComparisonExpression *)bindings[0];
105: 				assert(bindings[0]->GetExpressionClass() == ExpressionClass::BOUND_COMPARISON);
106: 				assert(bindings[2]->type == ExpressionType::VALUE_CONSTANT);
107: 
108: 				auto constant_value = ((BoundConstantExpression *)bindings[2])->value;
109: 				auto comparison_type = comparison->type;
110: 				if (comparison->left->type == ExpressionType::VALUE_CONSTANT) {
111: 					// the expression is on the right side, we flip them around
112: 					comparison_type = FlipComparisionExpression(comparison_type);
113: 				}
114: 				if (comparison_type == ExpressionType::COMPARE_EQUAL) {
115: 					// equality value
116: 					// equality overrides any other bounds so we just break here
117: 					equal_value = constant_value;
118: 					break;
119: 				} else if (comparison_type == ExpressionType::COMPARE_GREATERTHANOREQUALTO ||
120: 				           comparison_type == ExpressionType::COMPARE_GREATERTHAN) {
121: 					// greater than means this is a lower bound
122: 					low_value = constant_value;
123: 					low_comparison_type = comparison_type;
124: 				} else {
125: 					// smaller than means this is an upper bound
126: 					high_value = constant_value;
127: 					high_comparison_type = comparison_type;
128: 				}
129: 			}
130: 		}
131: 		if (!equal_value.is_null || !low_value.is_null || !high_value.is_null) {
132: 			auto logical_index_scan = make_unique<LogicalIndexScan>(*get->table, *get->table->storage, *index,
133: 			                                                        get->column_ids, get->table_index);
134: 			if (!equal_value.is_null) {
135: 				logical_index_scan->equal_value = equal_value;
136: 				logical_index_scan->equal_index = true;
137: 			}
138: 			if (!low_value.is_null) {
139: 				logical_index_scan->low_value = low_value;
140: 				logical_index_scan->low_index = true;
141: 				logical_index_scan->low_expression_type = low_comparison_type;
142: 			}
143: 			if (!high_value.is_null) {
144: 				logical_index_scan->high_value = high_value;
145: 				logical_index_scan->high_index = true;
146: 				logical_index_scan->high_expression_type = high_comparison_type;
147: 			}
148: 			op->children[0] = move(logical_index_scan);
149: 			break;
150: 		}
151: 	}
152: 	return op;
153: }
[end of src/optimizer/index_scan.cpp]
[start of src/optimizer/pushdown/pushdown_get.cpp]
1: #include "duckdb/optimizer/filter_pushdown.hpp"
2: #include "duckdb/planner/operator/logical_filter.hpp"
3: #include "duckdb/planner/operator/logical_get.hpp"
4: #include "duckdb/storage/data_table.hpp"
5: using namespace duckdb;
6: using namespace std;
7: 
8: unique_ptr<LogicalOperator> FilterPushdown::PushdownGet(unique_ptr<LogicalOperator> op) {
9: 	assert(op->type == LogicalOperatorType::GET);
10: 	auto &get = (LogicalGet &)*op;
11: 	if (!get.tableFilters.empty()) {
12: 		if (!filters.empty()) {
13: 			//! We didn't managed to push down all filters to table scan
14: 			auto logicalFilter = make_unique<LogicalFilter>();
15: 			for (auto &f : filters) {
16: 				logicalFilter->expressions.push_back(move(f->filter));
17: 			}
18: 			logicalFilter->children.push_back(move(op));
19: 			return move(logicalFilter);
20: 		} else {
21: 			return op;
22: 		}
23: 	}
24: 	//! FIXME: We only need to skip if the index is in the column being filtered
25: 	if (!get.table || !get.table->storage->indexes.empty()) {
26: 		//! now push any existing filters
27: 		if (filters.empty()) {
28: 			//! no filters to push
29: 			return op;
30: 		}
31: 		auto filter = make_unique<LogicalFilter>();
32: 		for (auto &f : filters) {
33: 			filter->expressions.push_back(move(f->filter));
34: 		}
35: 		filter->children.push_back(move(op));
36: 		return move(filter);
37: 	}
38: 	PushFilters();
39: 
40: 	vector<unique_ptr<Filter>> filtersToPushDown;
41: 	get.tableFilters = combiner.GenerateTableScanFilters(
42: 	    [&](unique_ptr<Expression> filter) {
43: 		    auto f = make_unique<Filter>();
44: 		    f->filter = move(filter);
45: 		    f->ExtractBindings();
46: 		    filtersToPushDown.push_back(move(f));
47: 	    },
48: 	    get.column_ids);
49: 	for (auto &f : get.tableFilters) {
50: 		f.column_index = get.column_ids[f.column_index];
51: 	}
52: 
53: 	GenerateFilters();
54: 	for (auto &f : filtersToPushDown) {
55: 		get.expressions.push_back(move(f->filter));
56: 	}
57: 
58: 	if (!filters.empty()) {
59: 		//! We didn't managed to push down all filters to table scan
60: 		auto logicalFilter = make_unique<LogicalFilter>();
61: 		for (auto &f : filters) {
62: 			logicalFilter->expressions.push_back(move(f->filter));
63: 		}
64: 		logicalFilter->children.push_back(move(op));
65: 		return move(logicalFilter);
66: 	}
67: 	return op;
68: }
[end of src/optimizer/pushdown/pushdown_get.cpp]
[start of src/parser/CMakeLists.txt]
1: include_directories(../../third_party/libpg_query/include)
2: include_directories(../../third_party/libpg_query)
3: 
4: add_subdirectory(constraints)
5: add_subdirectory(expression)
6: add_subdirectory(parsed_data)
7: add_subdirectory(query_node)
8: add_subdirectory(statement)
9: add_subdirectory(tableref)
10: add_subdirectory(transform)
11: 
12: add_library_unity(duckdb_parser
13:                   OBJECT
14:                   base_expression.cpp
15:                   constraint.cpp
16:                   expression_util.cpp
17:                   parsed_expression.cpp
18:                   parsed_expression_iterator.cpp
19:                   parser.cpp
20:                   query_node.cpp
21:                   result_modifier.cpp
22:                   tableref.cpp
23:                   transformer.cpp)
24: set(ALL_OBJECT_FILES
25:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_parser>
26:     PARENT_SCOPE)
[end of src/parser/CMakeLists.txt]
[start of src/parser/parsed_data/alter_table_info.cpp]
1: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
2: #include "duckdb/common/serializer.hpp"
3: 
4: using namespace duckdb;
5: using namespace std;
6: 
7: void AlterInfo::Serialize(Serializer &serializer) {
8: 	serializer.Write<AlterType>(type);
9: }
10: 
11: unique_ptr<AlterInfo> AlterInfo::Deserialize(Deserializer &source) {
12: 	auto type = source.Read<AlterType>();
13: 	switch (type) {
14: 	case AlterType::ALTER_TABLE:
15: 		return AlterTableInfo::Deserialize(source);
16: 	default:
17: 		throw SerializationException("Unknown alter type for deserialization!");
18: 	}
19: }
20: 
21: void AlterTableInfo::Serialize(Serializer &serializer) {
22: 	AlterInfo::Serialize(serializer);
23: 	serializer.Write<AlterTableType>(alter_table_type);
24: 	serializer.WriteString(schema);
25: 	serializer.WriteString(table);
26: }
27: 
28: unique_ptr<AlterInfo> AlterTableInfo::Deserialize(Deserializer &source) {
29: 	auto type = source.Read<AlterTableType>();
30: 	auto schema = source.Read<string>();
31: 	auto table = source.Read<string>();
32: 	unique_ptr<AlterTableInfo> info;
33: 	switch (type) {
34: 	case AlterTableType::RENAME_COLUMN:
35: 		return RenameColumnInfo::Deserialize(source, schema, table);
36: 	case AlterTableType::RENAME_TABLE:
37: 		return RenameTableInfo::Deserialize(source, schema, table);
38: 	default:
39: 		throw SerializationException("Unknown alter table type for deserialization!");
40: 	}
41: }
42: 
43: void RenameColumnInfo::Serialize(Serializer &serializer) {
44: 	AlterTableInfo::Serialize(serializer);
45: 	serializer.WriteString(name);
46: 	serializer.WriteString(new_name);
47: }
48: 
49: unique_ptr<AlterInfo> RenameColumnInfo::Deserialize(Deserializer &source, string schema, string table) {
50: 	auto name = source.Read<string>();
51: 	auto new_name = source.Read<string>();
52: 	return make_unique<RenameColumnInfo>(schema, table, name, new_name);
53: }
54: 
55: void RenameTableInfo::Serialize(Serializer &serializer) {
56: 	AlterTableInfo::Serialize(serializer);
57: 	serializer.WriteString(new_table_name);
58: }
59: 
60: unique_ptr<AlterInfo> RenameTableInfo::Deserialize(Deserializer &source, string schema, string table) {
61: 	auto new_name = source.Read<string>();
62: 	return make_unique<RenameTableInfo>(schema, table, new_name);
63: }
[end of src/parser/parsed_data/alter_table_info.cpp]
[start of src/parser/transform/statement/transform_alter_table.cpp]
1: #include "duckdb/parser/statement/alter_table_statement.hpp"
2: #include "duckdb/parser/transformer.hpp"
3: 
4: using namespace duckdb;
5: using namespace std;
6: 
7: unique_ptr<AlterTableStatement> Transformer::TransformAlter(PGNode *node) {
8: 	throw NotImplementedException("Alter table not supported yet!");
9: 	// auto stmt = reinterpret_cast<AlterTableStmt *>(node);
10: 	// assert(stmt);
11: 	// assert(stmt->relation);
12: 
13: 	// auto result = make_unique<AlterTableStatement>();
14: 	// auto &info = *result->info.get();
15: 	// auto new_alter_cmd = make_unique<AlterTableCmd>();
16: 	// result->table = TransformRangeVar(stmt->relation);
17: 
18: 	// info.table = stmt->relation->relname;
19: 
20: 	// // first we check the type of ALTER
21: 	// for (auto c = stmt->cmds->head; c != NULL; c = c->next) {
22: 	// 	auto command = reinterpret_cast<PGAlterTableCmd *>(lfirst(c));
23: 	// 	//TODO: Include more options for command->subtype
24: 	// 	switch (command->subtype) {
25: 	// 		case PG_AT_AddColumn: {
26: 	//                auto cdef = (ColumnDef *)command->def;
27: 	//                char *name = (reinterpret_cast<PGValue *>(
28: 	//                        cdef->typeName->names->tail->data.ptr_value)
29: 	//                        ->val.str);
30: 	//                auto centry =
31: 	//                        ColumnDefinition(cdef->colname,
32: 	//                        TransformStringToTypeId(name));
33: 	//                info.new_columns.push_back(centry);
34: 	//                break;
35: 	//            }
36: 	// 		case PG_AT_DropColumn:
37: 	// 		case PG_AT_AlterColumnType:
38: 	// 		default:
39: 	// 			throw NotImplementedException(
40: 	// 			    "ALTER TABLE option not supported yet!");
41: 	// 	}
42: 	// }
43: 
44: 	// return result;
45: }
[end of src/parser/transform/statement/transform_alter_table.cpp]
[start of src/parser/transform/statement/transform_create_table.cpp]
1: #include "duckdb/parser/statement/create_statement.hpp"
2: #include "duckdb/parser/parsed_data/create_table_info.hpp"
3: #include "duckdb/parser/transformer.hpp"
4: #include "duckdb/parser/constraint.hpp"
5: #include "duckdb/parser/expression/collate_expression.hpp"
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: string Transformer::TransformCollation(PGCollateClause *collate) {
11: 	if (!collate) {
12: 		return string();
13: 	}
14: 	string collation;
15: 	for (auto c = collate->collname->head; c != NULL; c = lnext(c)) {
16: 		auto pgvalue = (PGValue *)c->data.ptr_value;
17: 		if (pgvalue->type != T_PGString) {
18: 			throw ParserException("Expected a string as collation type!");
19: 		}
20: 		auto collation_argument = string(pgvalue->val.str);
21: 		if (collation.empty()) {
22: 			collation = collation_argument;
23: 		} else {
24: 			collation += "." + collation_argument;
25: 		}
26: 	}
27: 	return collation;
28: }
29: 
30: unique_ptr<ParsedExpression> Transformer::TransformCollateExpr(PGCollateClause *collate) {
31: 	auto child = TransformExpression(collate->arg);
32: 	auto collation = TransformCollation(collate);
33: 	return make_unique<CollateExpression>(collation, move(child));
34: }
35: 
36: unique_ptr<CreateStatement> Transformer::TransformCreateTable(PGNode *node) {
37: 	auto stmt = reinterpret_cast<PGCreateStmt *>(node);
38: 	assert(stmt);
39: 	auto result = make_unique<CreateStatement>();
40: 	auto info = make_unique<CreateTableInfo>();
41: 
42: 	if (stmt->inhRelations) {
43: 		throw NotImplementedException("inherited relations not implemented");
44: 	}
45: 	assert(stmt->relation);
46: 
47: 	info->schema = INVALID_SCHEMA;
48: 	if (stmt->relation->schemaname) {
49: 		info->schema = stmt->relation->schemaname;
50: 	}
51: 	info->table = stmt->relation->relname;
52: 	info->on_conflict = stmt->if_not_exists ? OnCreateConflict::IGNORE : OnCreateConflict::ERROR;
53: 	info->temporary = stmt->relation->relpersistence == PGPostgresRelPersistence::PG_RELPERSISTENCE_TEMP;
54: 
55: 	if (info->temporary && stmt->oncommit != PGOnCommitAction::PG_ONCOMMIT_PRESERVE_ROWS &&
56: 	    stmt->oncommit != PGOnCommitAction::PG_ONCOMMIT_NOOP) {
57: 		throw NotImplementedException("Only ON COMMIT PRESERVE ROWS is supported");
58: 	}
59: 	if (!stmt->tableElts) {
60: 		throw ParserException("Table must have at least one column!");
61: 	}
62: 
63: 	for (auto c = stmt->tableElts->head; c != NULL; c = lnext(c)) {
64: 		auto node = reinterpret_cast<PGNode *>(c->data.ptr_value);
65: 		switch (node->type) {
66: 		case T_PGColumnDef: {
67: 			auto cdef = (PGColumnDef *)c->data.ptr_value;
68: 			SQLType target_type = TransformTypeName(cdef->typeName);
69: 			target_type.collation = TransformCollation(cdef->collClause);
70: 
71: 			auto centry = ColumnDefinition(cdef->colname, target_type);
72: 
73: 			if (cdef->constraints) {
74: 				for (auto constr = cdef->constraints->head; constr != nullptr; constr = constr->next) {
75: 					auto constraint = TransformConstraint(constr, centry, info->columns.size());
76: 					if (constraint) {
77: 						info->constraints.push_back(move(constraint));
78: 					}
79: 				}
80: 			}
81: 			info->columns.push_back(move(centry));
82: 			break;
83: 		}
84: 		case T_PGConstraint: {
85: 			info->constraints.push_back(TransformConstraint(c));
86: 			break;
87: 		}
88: 		default:
89: 			throw NotImplementedException("ColumnDef type not handled yet");
90: 		}
91: 	}
92: 	result->info = move(info);
93: 	return result;
94: }
[end of src/parser/transform/statement/transform_create_table.cpp]
[start of src/planner/binder/statement/bind_create_table.cpp]
1: #include "duckdb/parser/constraints/list.hpp"
2: #include "duckdb/parser/expression/cast_expression.hpp"
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/planner/constraints/list.hpp"
5: #include "duckdb/planner/expression/bound_constant_expression.hpp"
6: #include "duckdb/planner/expression_binder/check_binder.hpp"
7: #include "duckdb/planner/expression_binder/constant_binder.hpp"
8: #include "duckdb/parser/parsed_data/create_table_info.hpp"
9: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
10: 
11: using namespace duckdb;
12: using namespace std;
13: 
14: static void CreateColumnMap(BoundCreateTableInfo &info) {
15: 	auto &base = (CreateTableInfo &)*info.base;
16: 
17: 	for (uint64_t oid = 0; oid < base.columns.size(); oid++) {
18: 		auto &col = base.columns[oid];
19: 		if (info.name_map.find(col.name) != info.name_map.end()) {
20: 			throw CatalogException("Column with name %s already exists!", col.name.c_str());
21: 		}
22: 
23: 		info.name_map[col.name] = oid;
24: 		col.oid = oid;
25: 	}
26: }
27: 
28: static void BindConstraints(Binder &binder, BoundCreateTableInfo &info) {
29: 	auto &base = (CreateTableInfo &)*info.base;
30: 
31: 	bool has_primary_key = false;
32: 	for (idx_t i = 0; i < base.constraints.size(); i++) {
33: 		auto &cond = base.constraints[i];
34: 		switch (cond->type) {
35: 		case ConstraintType::CHECK: {
36: 			auto bound_constraint = make_unique<BoundCheckConstraint>();
37: 			// check constraint: bind the expression
38: 			CheckBinder check_binder(binder, binder.context, base.table, base.columns, bound_constraint->bound_columns);
39: 			auto &check = (CheckConstraint &)*cond;
40: 			// create a copy of the unbound expression because the binding destroys the constraint
41: 			auto unbound_expression = check.expression->Copy();
42: 			// now bind the constraint and create a new BoundCheckConstraint
43: 			bound_constraint->expression = check_binder.Bind(check.expression);
44: 			info.bound_constraints.push_back(move(bound_constraint));
45: 			// move the unbound constraint back into the original check expression
46: 			check.expression = move(unbound_expression);
47: 			break;
48: 		}
49: 		case ConstraintType::NOT_NULL: {
50: 			auto &not_null = (NotNullConstraint &)*cond;
51: 			info.bound_constraints.push_back(make_unique<BoundNotNullConstraint>(not_null.index));
52: 			break;
53: 		}
54: 		case ConstraintType::UNIQUE: {
55: 			auto &unique = (UniqueConstraint &)*cond;
56: 			// have to resolve columns of the unique constraint
57: 			unordered_set<idx_t> keys;
58: 			if (unique.index != INVALID_INDEX) {
59: 				assert(unique.index < base.columns.size());
60: 				// unique constraint is given by single index
61: 				keys.insert(unique.index);
62: 			} else {
63: 				// unique constraint is given by list of names
64: 				// have to resolve names
65: 				assert(unique.columns.size() > 0);
66: 				for (auto &keyname : unique.columns) {
67: 					auto entry = info.name_map.find(keyname);
68: 					if (entry == info.name_map.end()) {
69: 						throw ParserException("column \"%s\" named in key does not exist", keyname.c_str());
70: 					}
71: 					if (find(keys.begin(), keys.end(), entry->second) != keys.end()) {
72: 						throw ParserException("column \"%s\" appears twice in "
73: 						                      "primary key constraint",
74: 						                      keyname.c_str());
75: 					}
76: 					keys.insert(entry->second);
77: 				}
78: 			}
79: 
80: 			if (unique.is_primary_key) {
81: 				// we can only have one primary key per table
82: 				if (has_primary_key) {
83: 					throw ParserException("table \"%s\" has more than one primary key", base.table.c_str());
84: 				}
85: 				has_primary_key = true;
86: 			}
87: 			info.bound_constraints.push_back(make_unique<BoundUniqueConstraint>(keys, unique.is_primary_key));
88: 			break;
89: 		}
90: 		default:
91: 			throw NotImplementedException("unrecognized constraint type in bind");
92: 		}
93: 	}
94: }
95: 
96: void Binder::BindDefaultValues(vector<ColumnDefinition> &columns, vector<unique_ptr<Expression>> &bound_defaults) {
97: 	for (idx_t i = 0; i < columns.size(); i++) {
98: 		unique_ptr<Expression> bound_default;
99: 		if (columns[i].default_value) {
100: 			// we bind a copy of the DEFAULT value because binding is destructive
101: 			// and we want to keep the original expression around for serialization
102: 			auto default_copy = columns[i].default_value->Copy();
103: 			ConstantBinder default_binder(*this, context, "DEFAULT value");
104: 			default_binder.target_type = columns[i].type;
105: 			bound_default = default_binder.Bind(default_copy);
106: 		} else {
107: 			// no default value specified: push a default value of constant null
108: 			bound_default = make_unique<BoundConstantExpression>(Value(GetInternalType(columns[i].type)));
109: 		}
110: 		bound_defaults.push_back(move(bound_default));
111: 	}
112: }
113: 
114: unique_ptr<BoundCreateTableInfo> Binder::BindCreateTableInfo(unique_ptr<CreateInfo> info) {
115: 	auto &base = (CreateTableInfo &)*info;
116: 
117: 	auto result = make_unique<BoundCreateTableInfo>(move(info));
118: 	result->schema = BindSchema(*result->base);
119: 	if (base.query) {
120: 		// construct the result object
121: 		auto query_obj = Bind(*base.query);
122: 		result->query = move(query_obj.plan);
123: 
124: 		// construct the set of columns based on the names and types of the query
125: 		auto &names = query_obj.names;
126: 		auto &sql_types = query_obj.types;
127: 		assert(names.size() == sql_types.size());
128: 		for (idx_t i = 0; i < names.size(); i++) {
129: 			base.columns.push_back(ColumnDefinition(names[i], sql_types[i]));
130: 		}
131: 		// create the name map for the statement
132: 		CreateColumnMap(*result);
133: 	} else {
134: 		// create the name map for the statement
135: 		CreateColumnMap(*result);
136: 		// bind any constraints
137: 		BindConstraints(*this, *result);
138: 		// bind the default values
139: 		BindDefaultValues(base.columns, result->bound_defaults);
140: 	}
141: 	// bind collations to detect any unsupported collation errors
142: 	for (auto &column : base.columns) {
143: 		ExpressionBinder::PushCollation(context, nullptr, column.type.collation);
144: 	}
145: 	return result;
146: }
[end of src/planner/binder/statement/bind_create_table.cpp]
[start of src/planner/binder/statement/bind_update.cpp]
1: #include "duckdb/parser/statement/update_statement.hpp"
2: #include "duckdb/planner/binder.hpp"
3: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
4: #include "duckdb/planner/expression/bound_default_expression.hpp"
5: #include "duckdb/planner/expression_binder/update_binder.hpp"
6: #include "duckdb/planner/expression_binder/where_binder.hpp"
7: #include "duckdb/planner/operator/logical_filter.hpp"
8: #include "duckdb/planner/operator/logical_get.hpp"
9: #include "duckdb/planner/operator/logical_projection.hpp"
10: #include "duckdb/planner/operator/logical_update.hpp"
11: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
12: #include "duckdb/parser/expression/columnref_expression.hpp"
13: #include "duckdb/storage/data_table.hpp"
14: #include "duckdb/planner/bound_tableref.hpp"
15: 
16: #include <algorithm>
17: 
18: using namespace std;
19: 
20: namespace duckdb {
21: 
22: static void BindExtraColumns(TableCatalogEntry &table, LogicalGet &get, LogicalProjection &proj, LogicalUpdate &update,
23:                              unordered_set<column_t> &bound_columns) {
24: 	if (bound_columns.size() <= 1) {
25: 		return;
26: 	}
27: 	idx_t found_column_count = 0;
28: 	unordered_set<idx_t> found_columns;
29: 	for (idx_t i = 0; i < update.columns.size(); i++) {
30: 		if (bound_columns.find(update.columns[i]) != bound_columns.end()) {
31: 			// this column is referenced in the CHECK constraint
32: 			found_column_count++;
33: 			found_columns.insert(update.columns[i]);
34: 		}
35: 	}
36: 	if (found_column_count > 0 && found_column_count != bound_columns.size()) {
37: 		// columns in this CHECK constraint were referenced, but not all were part of the UPDATE
38: 		// add them to the scan and update set
39: 		for (auto &check_column_id : bound_columns) {
40: 			if (found_columns.find(check_column_id) != found_columns.end()) {
41: 				// column is already projected
42: 				continue;
43: 			}
44: 			// column is not projected yet: project it by adding the clause "i=i" to the set of updated columns
45: 			auto &column = table.columns[check_column_id];
46: 			auto col_type = GetInternalType(column.type);
47: 			// first add
48: 			update.expressions.push_back(make_unique<BoundColumnRefExpression>(
49: 			    col_type, ColumnBinding(proj.table_index, proj.expressions.size())));
50: 			proj.expressions.push_back(
51: 			    make_unique<BoundColumnRefExpression>(col_type, ColumnBinding(get.table_index, get.column_ids.size())));
52: 			get.column_ids.push_back(check_column_id);
53: 			update.columns.push_back(check_column_id);
54: 		}
55: 	}
56: }
57: 
58: static void BindUpdateConstraints(TableCatalogEntry &table, LogicalGet &get, LogicalProjection &proj,
59:                                   LogicalUpdate &update) {
60: 	// check the constraints and indexes of the table to see if we need to project any additional columns
61: 	// we do this for indexes with multiple columns and CHECK constraints in the UPDATE clause
62: 	// suppose we have a constraint CHECK(i + j < 10); now we need both i and j to check the constraint
63: 	// if we are only updating one of the two columns we add the other one to the UPDATE set
64: 	// with a "useless" update (i.e. i=i) so we can verify that the CHECK constraint is not violated
65: 	for (auto &constraint : table.bound_constraints) {
66: 		if (constraint->type == ConstraintType::CHECK) {
67: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
68: 			// check constraint! check if we need to add any extra columns to the UPDATE clause
69: 			BindExtraColumns(table, get, proj, update, check.bound_columns);
70: 		}
71: 	}
72: 	// for index updates, we do the same, however, for index updates we always turn any update into an insert and a
73: 	// delete for the insert, we thus need all the columns to be available, hence we check if the update touches any
74: 	// index columns
75: 	update.is_index_update = false;
76: 	for (auto &index : table.storage->indexes) {
77: 		if (index->IndexIsUpdated(update.columns)) {
78: 			update.is_index_update = true;
79: 		}
80: 	}
81: 	if (update.is_index_update) {
82: 		// the update updates a column required by an index, push projections for all columns
83: 		unordered_set<column_t> all_columns;
84: 		for (idx_t i = 0; i < table.storage->types.size(); i++) {
85: 			all_columns.insert(i);
86: 		}
87: 		BindExtraColumns(table, get, proj, update, all_columns);
88: 	}
89: }
90: 
91: BoundStatement Binder::Bind(UpdateStatement &stmt) {
92: 	BoundStatement result;
93: 	// visit the table reference
94: 	auto bound_table = Bind(*stmt.table);
95: 	if (bound_table->type != TableReferenceType::BASE_TABLE) {
96: 		throw BinderException("Can only update base table!");
97: 	}
98: 	auto root = CreatePlan(*bound_table);
99: 	auto &get = (LogicalGet &)*root;
100: 	assert(root->type == LogicalOperatorType::GET && get.table);
101: 
102: 	auto &table = get.table;
103: 	if (!table->temporary) {
104: 		// update of persistent table: not read only!
105: 		this->read_only = false;
106: 	}
107: 	auto update = make_unique<LogicalUpdate>(table);
108: 	// bind the default values
109: 	BindDefaultValues(table->columns, update->bound_defaults);
110: 
111: 	// project any additional columns required for the condition/expressions
112: 	if (stmt.condition) {
113: 		WhereBinder binder(*this, context);
114: 		auto condition = binder.Bind(stmt.condition);
115: 
116: 		PlanSubqueries(&condition, &root);
117: 		auto filter = make_unique<LogicalFilter>(move(condition));
118: 		filter->AddChild(move(root));
119: 		root = move(filter);
120: 	}
121: 
122: 	assert(stmt.columns.size() == stmt.expressions.size());
123: 
124: 	auto proj_index = GenerateTableIndex();
125: 	vector<unique_ptr<Expression>> projection_expressions;
126: 	for (idx_t i = 0; i < stmt.columns.size(); i++) {
127: 		auto &colname = stmt.columns[i];
128: 		auto &expr = stmt.expressions[i];
129: 		if (!table->ColumnExists(colname)) {
130: 			throw BinderException("Referenced update column %s not found in table!", colname.c_str());
131: 		}
132: 		auto &column = table->GetColumn(colname);
133: 		if (std::find(update->columns.begin(), update->columns.end(), column.oid) != update->columns.end()) {
134: 			throw BinderException("Multiple assignments to same column \"%s\"", colname.c_str());
135: 		}
136: 		update->columns.push_back(column.oid);
137: 
138: 		if (expr->type == ExpressionType::VALUE_DEFAULT) {
139: 			update->expressions.push_back(
140: 			    make_unique<BoundDefaultExpression>(GetInternalType(column.type), column.type));
141: 		} else {
142: 			UpdateBinder binder(*this, context);
143: 			binder.target_type = column.type;
144: 			auto bound_expr = binder.Bind(expr);
145: 			PlanSubqueries(&bound_expr, &root);
146: 
147: 			update->expressions.push_back(make_unique<BoundColumnRefExpression>(
148: 			    bound_expr->return_type, ColumnBinding(proj_index, projection_expressions.size())));
149: 			projection_expressions.push_back(move(bound_expr));
150: 		}
151: 	}
152: 	// now create the projection
153: 	auto proj = make_unique<LogicalProjection>(proj_index, move(projection_expressions));
154: 	proj->AddChild(move(root));
155: 
156: 	// bind any extra columns necessary for CHECK constraints or indexes
157: 	BindUpdateConstraints(*table, get, *proj, *update);
158: 
159: 	// finally add the row id column to the projection list
160: 	proj->expressions.push_back(
161: 	    make_unique<BoundColumnRefExpression>(ROW_TYPE, ColumnBinding(get.table_index, get.column_ids.size())));
162: 	get.column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
163: 
164: 	// set the projection as child of the update node and finalize the result
165: 	update->AddChild(move(proj));
166: 
167: 	result.names = {"Count"};
168: 	result.types = {SQLType::BIGINT};
169: 	result.plan = move(update);
170: 	return result;
171: }
172: 
173: } // namespace duckdb
[end of src/planner/binder/statement/bind_update.cpp]
[start of src/planner/expression_binder/CMakeLists.txt]
1: add_library_unity(duckdb_expression_binders
2:                   OBJECT
3:                   aggregate_binder.cpp
4:                   check_binder.cpp
5:                   constant_binder.cpp
6:                   group_binder.cpp
7:                   having_binder.cpp
8:                   index_binder.cpp
9:                   insert_binder.cpp
10:                   order_binder.cpp
11:                   relation_binder.cpp
12:                   select_binder.cpp
13:                   update_binder.cpp
14:                   where_binder.cpp)
15: set(ALL_OBJECT_FILES ${ALL_OBJECT_FILES}
16:                      $<TARGET_OBJECTS:duckdb_expression_binders> PARENT_SCOPE)
[end of src/planner/expression_binder/CMakeLists.txt]
[start of src/planner/operator/logical_get.cpp]
1: #include "duckdb/planner/operator/logical_get.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/storage/data_table.hpp"
5: 
6: using namespace duckdb;
7: using namespace std;
8: 
9: LogicalGet::LogicalGet(idx_t table_index)
10:     : LogicalOperator(LogicalOperatorType::GET), table(nullptr), table_index(table_index) {
11: }
12: LogicalGet::LogicalGet(TableCatalogEntry *table, idx_t table_index)
13:     : LogicalOperator(LogicalOperatorType::GET), table(table), table_index(table_index) {
14: }
15: LogicalGet::LogicalGet(TableCatalogEntry *table, idx_t table_index, vector<column_t> column_ids)
16:     : LogicalOperator(LogicalOperatorType::GET), table(table), table_index(table_index), column_ids(column_ids) {
17: }
18: 
19: string LogicalGet::ParamsToString() const {
20: 	if (!table) {
21: 		return "";
22: 	}
23: 	return "(" + table->name + ")";
24: }
25: 
26: vector<ColumnBinding> LogicalGet::GetColumnBindings() {
27: 	if (!table) {
28: 		return {ColumnBinding(INVALID_INDEX, 0)};
29: 	}
30: 	if (column_ids.size() == 0) {
31: 		return {ColumnBinding(table_index, 0)};
32: 	}
33: 	vector<ColumnBinding> result;
34: 	for (idx_t i = 0; i < column_ids.size(); i++) {
35: 		result.push_back(ColumnBinding(table_index, i));
36: 	}
37: 	return result;
38: }
39: 
40: void LogicalGet::ResolveTypes() {
41: 	if (column_ids.size() == 0) {
42: 		column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
43: 	}
44: 	types = table->GetTypes(column_ids);
45: }
46: 
47: idx_t LogicalGet::EstimateCardinality() {
48: 	if (table) {
49: 		return table->storage->cardinality;
50: 	} else {
51: 		return 1;
52: 	}
53: }
[end of src/planner/operator/logical_get.cpp]
[start of src/storage/column_data.cpp]
1: #include "duckdb/storage/column_data.hpp"
2: #include "duckdb/storage/table/persistent_segment.hpp"
3: #include "duckdb/storage/table/transient_segment.hpp"
4: #include "duckdb/storage/data_table.hpp"
5: #include "duckdb/storage/storage_manager.hpp"
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: ColumnData::ColumnData() : persistent_rows(0) {
11: }
12: 
13: void ColumnData::Initialize(vector<unique_ptr<PersistentSegment>> &segments) {
14: 	for (auto &segment : segments) {
15: 		persistent_rows += segment->count;
16: 		data.AppendSegment(move(segment));
17: 	}
18: }
19: 
20: void ColumnData::InitializeScan(ColumnScanState &state) {
21: 	state.current = (ColumnSegment *)data.GetRootSegment();
22: 	state.vector_index = 0;
23: 	state.initialized = false;
24: }
25: 
26: void ColumnData::Scan(Transaction &transaction, ColumnScanState &state, Vector &result) {
27: 	if (!state.initialized) {
28: 		state.current->InitializeScan(state);
29: 		state.initialized = true;
30: 	}
31: 	// perform a scan of this segment
32: 	state.current->Scan(transaction, state, state.vector_index, result);
33: 	// move over to the next vector
34: 	state.Next();
35: }
36: 
37: void ColumnData::FilterScan(Transaction &transaction, ColumnScanState &state, Vector &result, SelectionVector &sel,
38:                             idx_t &approved_tuple_count) {
39: 	if (!state.initialized) {
40: 		state.current->InitializeScan(state);
41: 		state.initialized = true;
42: 	}
43: 	// perform a scan of this segment
44: 	state.current->FilterScan(transaction, state, result, sel, approved_tuple_count);
45: 	// move over to the next vector
46: 	state.Next();
47: }
48: 
49: void ColumnData::Select(Transaction &transaction, ColumnScanState &state, Vector &result, SelectionVector &sel,
50:                         idx_t &approved_tuple_count, vector<TableFilter> &tableFilter) {
51: 	if (!state.initialized) {
52: 		state.current->InitializeScan(state);
53: 		state.initialized = true;
54: 	}
55: 	// perform a scan of this segment
56: 	state.current->Select(transaction, state, result, sel, approved_tuple_count, tableFilter);
57: 	// move over to the next vector
58: 	state.Next();
59: }
60: 
61: void ColumnData::IndexScan(ColumnScanState &state, Vector &result) {
62: 	if (state.vector_index == 0) {
63: 		state.current->InitializeScan(state);
64: 	}
65: 	// perform a scan of this segment
66: 	state.current->IndexScan(state, result);
67: 	// move over to the next vector
68: 	state.Next();
69: }
70: 
71: void ColumnScanState::Next() {
72: 	//! There is no column segment
73: 	if (!current) {
74: 		return;
75: 	}
76: 	vector_index++;
77: 	if (vector_index * STANDARD_VECTOR_SIZE >= current->count) {
78: 		current = (ColumnSegment *)current->next.get();
79: 		vector_index = 0;
80: 		initialized = false;
81: 		segment_checked = false;
82: 	}
83: }
84: 
85: void TableScanState::NextVector() {
86: 	//! nothing to scan for this vector, skip the entire vector
87: 	for (idx_t j = 0; j < column_ids.size(); j++) {
88: 		auto column = column_ids[j];
89: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
90: 			column_scans[j].Next();
91: 		}
92: 	}
93: }
94: 
95: void ColumnData::InitializeAppend(ColumnAppendState &state) {
96: 	lock_guard<mutex> tree_lock(data.node_lock);
97: 	if (data.nodes.size() == 0) {
98: 		// no transient segments yet, append one
99: 		AppendTransientSegment(persistent_rows);
100: 	}
101: 	auto segment = (ColumnSegment *)data.GetLastSegment();
102: 	if (segment->segment_type == ColumnSegmentType::PERSISTENT) {
103: 		// cannot append to persistent segment, add a transient one
104: 		AppendTransientSegment(persistent_rows);
105: 		state.current = (TransientSegment *)data.GetLastSegment();
106: 	} else {
107: 		state.current = (TransientSegment *)segment;
108: 	}
109: 	assert(state.current->segment_type == ColumnSegmentType::TRANSIENT);
110: 	state.current->InitializeAppend(state);
111: }
112: 
113: void ColumnData::Append(ColumnAppendState &state, Vector &vector, idx_t count) {
114: 	idx_t offset = 0;
115: 	while (true) {
116: 		// append the data from the vector
117: 		idx_t copied_elements = state.current->Append(state, vector, offset, count);
118: 		if (copied_elements == count) {
119: 			// finished copying everything
120: 			break;
121: 		}
122: 
123: 		// we couldn't fit everything we wanted in the current column segment, create a new one
124: 		{
125: 			lock_guard<mutex> tree_lock(data.node_lock);
126: 			AppendTransientSegment(state.current->start + state.current->count);
127: 			state.current = (TransientSegment *)data.GetLastSegment();
128: 			state.current->InitializeAppend(state);
129: 		}
130: 		offset += copied_elements;
131: 		count -= copied_elements;
132: 	}
133: }
134: 
135: void ColumnData::RevertAppend(row_t start_row) {
136: 	lock_guard<mutex> tree_lock(data.node_lock);
137: 	// find the segment index that the current row belongs to
138: 	idx_t segment_index = data.GetSegmentIndex(start_row);
139: 	auto segment = data.nodes[segment_index].node;
140: 	auto &transient = (TransientSegment &)*segment;
141: 	assert(transient.segment_type == ColumnSegmentType::TRANSIENT);
142: 
143: 	// remove any segments AFTER this segment: they should be deleted entirely
144: 	if (segment_index < data.nodes.size() - 1) {
145: 		data.nodes.erase(data.nodes.begin() + segment_index + 1, data.nodes.end());
146: 	}
147: 	segment->next = nullptr;
148: 	transient.RevertAppend(start_row);
149: }
150: 
151: void ColumnData::Update(Transaction &transaction, Vector &updates, Vector &row_ids, idx_t count) {
152: 	// first find the segment that the update belongs to
153: 	idx_t first_id = FlatVector::GetValue<row_t>(row_ids, 0);
154: 	auto segment = (ColumnSegment *)data.GetSegment(first_id);
155: 	// now perform the update within the segment
156: 	segment->Update(*this, transaction, updates, FlatVector::GetData<row_t>(row_ids), count);
157: }
158: 
159: void ColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
160: 	// find the segment that the row belongs to
161: 	auto segment = (ColumnSegment *)data.GetSegment(row_id);
162: 	auto vector_index = (row_id - segment->start) / STANDARD_VECTOR_SIZE;
163: 	// now perform the fetch within the segment
164: 	segment->Fetch(state, vector_index, result);
165: }
166: 
167: void ColumnData::FetchRow(ColumnFetchState &state, Transaction &transaction, row_t row_id, Vector &result,
168:                           idx_t result_idx) {
169: 	// find the segment the row belongs to
170: 	auto segment = (TransientSegment *)data.GetSegment(row_id);
171: 	// now perform the fetch within the segment
172: 	segment->FetchRow(state, transaction, row_id, result, result_idx);
173: }
174: 
175: void ColumnData::AppendTransientSegment(idx_t start_row) {
176: 	auto new_segment = make_unique<TransientSegment>(*table->storage.buffer_manager, type, start_row);
177: 	data.AppendSegment(move(new_segment));
178: }
[end of src/storage/column_data.cpp]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/common/helper.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/planner/constraints/list.hpp"
9: #include "duckdb/transaction/transaction.hpp"
10: #include "duckdb/transaction/transaction_manager.hpp"
11: #include "duckdb/storage/table/transient_segment.hpp"
12: 
13: using namespace duckdb;
14: using namespace std;
15: using namespace chrono;
16: 
17: DataTable::DataTable(StorageManager &storage, string schema, string table, vector<TypeId> types_,
18:                      unique_ptr<vector<unique_ptr<PersistentSegment>>[]> data)
19:     : cardinality(0), schema(schema), table(table), types(types_), storage(storage), persistent_manager(*this),
20:       transient_manager(*this) {
21: 	// set up the segment trees for the column segments
22: 	columns = unique_ptr<ColumnData[]>(new ColumnData[types.size()]);
23: 	for (idx_t i = 0; i < types.size(); i++) {
24: 		columns[i].type = types[i];
25: 		columns[i].table = this;
26: 		columns[i].column_idx = i;
27: 	}
28: 
29: 	// initialize the table with the existing data from disk, if any
30: 	if (data && data[0].size() > 0) {
31: 		// first append all the segments to the set of column segments
32: 		for (idx_t i = 0; i < types.size(); i++) {
33: 			columns[i].Initialize(data[i]);
34: 			if (columns[i].persistent_rows != columns[0].persistent_rows) {
35: 				throw Exception("Column length mismatch in table load!");
36: 			}
37: 		}
38: 		persistent_manager.max_row = columns[0].persistent_rows;
39: 		transient_manager.base_row = persistent_manager.max_row;
40: 	}
41: }
42: 
43: //===--------------------------------------------------------------------===//
44: // Scan
45: //===--------------------------------------------------------------------===//
46: void DataTable::InitializeScan(TableScanState &state, vector<column_t> column_ids,
47:                                unordered_map<idx_t, vector<TableFilter>> *table_filters) {
48: 	// initialize a column scan state for each column
49: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
50: 	for (idx_t i = 0; i < column_ids.size(); i++) {
51: 		auto column = column_ids[i];
52: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
53: 			columns[column].InitializeScan(state.column_scans[i]);
54: 		}
55: 	}
56: 	state.column_ids = move(column_ids);
57: 	// initialize the chunk scan state
58: 	state.offset = 0;
59: 	state.current_persistent_row = 0;
60: 	state.max_persistent_row = persistent_manager.max_row;
61: 	state.current_transient_row = 0;
62: 	state.max_transient_row = transient_manager.max_row;
63: 	if (table_filters && table_filters->size() > 0) {
64: 		state.adaptive_filter = make_unique<AdaptiveFilter>(*table_filters);
65: 	}
66: }
67: 
68: void DataTable::InitializeScan(Transaction &transaction, TableScanState &state, vector<column_t> column_ids,
69:                                unordered_map<idx_t, vector<TableFilter>> *table_filters) {
70: 	InitializeScan(state, move(column_ids), table_filters);
71: 	transaction.storage.InitializeScan(this, state.local_state);
72: }
73: 
74: void DataTable::Scan(Transaction &transaction, DataChunk &result, TableScanState &state,
75:                      unordered_map<idx_t, vector<TableFilter>> &table_filters) {
76: 	// scan the persistent segments
77: 	while (ScanBaseTable(transaction, result, state, state.current_persistent_row, state.max_persistent_row, 0,
78: 	                     persistent_manager, table_filters)) {
79: 		if (result.size() > 0) {
80: 			return;
81: 		}
82: 	}
83: 	// scan the transient segments
84: 	while (ScanBaseTable(transaction, result, state, state.current_transient_row, state.max_transient_row,
85: 	                     persistent_manager.max_row, transient_manager, table_filters)) {
86: 		if (result.size() > 0) {
87: 			return;
88: 		}
89: 	}
90: 
91: 	// scan the transaction-local segments
92: 	transaction.storage.Scan(state.local_state, state.column_ids, result, &table_filters);
93: }
94: 
95: template <class T> bool checkZonemap(TableScanState &state, TableFilter &table_filter, T constant) {
96: 	T *min = (T *)state.column_scans[table_filter.column_index].current->stats.minimum.get();
97: 	T *max = (T *)state.column_scans[table_filter.column_index].current->stats.maximum.get();
98: 	switch (table_filter.comparison_type) {
99: 	case ExpressionType::COMPARE_EQUAL:
100: 		return constant >= *min && constant <= *max;
101: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
102: 		return constant <= *max;
103: 	case ExpressionType::COMPARE_GREATERTHAN:
104: 		return constant < *max;
105: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
106: 		return constant >= *min;
107: 	case ExpressionType::COMPARE_LESSTHAN:
108: 		return constant > *min;
109: 	default:
110: 		throw NotImplementedException("Operation not implemented");
111: 	}
112: }
113: 
114: bool checkZonemapString(TableScanState &state, TableFilter &table_filter, const char *constant) {
115: 	char *min = (char *)state.column_scans[table_filter.column_index].current->stats.minimum.get();
116: 	char *max = (char *)state.column_scans[table_filter.column_index].current->stats.maximum.get();
117: 	int min_comp = strcmp(min, constant);
118: 	int max_comp = strcmp(max, constant);
119: 	switch (table_filter.comparison_type) {
120: 	case ExpressionType::COMPARE_EQUAL:
121: 		return min_comp <= 0 && max_comp >= 0;
122: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
123: 	case ExpressionType::COMPARE_GREATERTHAN:
124: 		return max_comp >= 0;
125: 	case ExpressionType::COMPARE_LESSTHAN:
126: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
127: 		return min_comp <= 0;
128: 	default:
129: 		throw NotImplementedException("Operation not implemented");
130: 	}
131: }
132: 
133: bool DataTable::CheckZonemap(TableScanState &state, unordered_map<idx_t, vector<TableFilter>> &table_filters,
134:                              idx_t &current_row) {
135: 	bool readSegment = true;
136: 	for (auto &table_filter : table_filters) {
137: 		for (auto &predicate_constant : table_filter.second) {
138: 			if (!state.column_scans[predicate_constant.column_index].segment_checked) {
139: 				state.column_scans[predicate_constant.column_index].segment_checked = true;
140: 				if (!state.column_scans[predicate_constant.column_index].current) {
141: 					return true;
142: 				}
143: 				switch (state.column_scans[predicate_constant.column_index].current->type) {
144: 				case TypeId::INT8: {
145: 					int8_t constant = predicate_constant.constant.value_.tinyint;
146: 					readSegment &= checkZonemap<int8_t>(state, predicate_constant, constant);
147: 					break;
148: 				}
149: 				case TypeId::INT16: {
150: 					int16_t constant = predicate_constant.constant.value_.smallint;
151: 					readSegment &= checkZonemap<int16_t>(state, predicate_constant, constant);
152: 					break;
153: 				}
154: 				case TypeId::INT32: {
155: 					int32_t constant = predicate_constant.constant.value_.integer;
156: 					readSegment &= checkZonemap<int32_t>(state, predicate_constant, constant);
157: 					break;
158: 				}
159: 				case TypeId::INT64: {
160: 					int64_t constant = predicate_constant.constant.value_.bigint;
161: 					readSegment &= checkZonemap<int64_t>(state, predicate_constant, constant);
162: 					break;
163: 				}
164: 				case TypeId::FLOAT: {
165: 					float constant = predicate_constant.constant.value_.float_;
166: 					readSegment &= checkZonemap<float>(state, predicate_constant, constant);
167: 					break;
168: 				}
169: 				case TypeId::DOUBLE: {
170: 					double constant = predicate_constant.constant.value_.double_;
171: 					readSegment &= checkZonemap<double>(state, predicate_constant, constant);
172: 					break;
173: 				}
174: 				case TypeId::VARCHAR: {
175: 					//! we can only compare the first 7 bytes
176: 					size_t value_size = predicate_constant.constant.str_value.size() > 7
177: 					                        ? 7
178: 					                        : predicate_constant.constant.str_value.size();
179: 					string constant;
180: 					for (size_t i = 0; i < value_size; i++) {
181: 						constant += predicate_constant.constant.str_value[i];
182: 					}
183: 					readSegment &= checkZonemapString(state, predicate_constant, constant.c_str());
184: 					break;
185: 				}
186: 				default:
187: 					throw NotImplementedException("Unimplemented type for uncompressed segment");
188: 				}
189: 			}
190: 			if (!readSegment) {
191: 				//! We can skip this partition
192: 				idx_t vectorsToSkip =
193: 				    ceil((double)(state.column_scans[predicate_constant.column_index].current->count +
194: 				                  state.column_scans[predicate_constant.column_index].current->start - current_row) /
195: 				         STANDARD_VECTOR_SIZE);
196: 				for (idx_t i = 0; i < vectorsToSkip; ++i) {
197: 					state.NextVector();
198: 					current_row += STANDARD_VECTOR_SIZE;
199: 				}
200: 				return false;
201: 			}
202: 		}
203: 	}
204: 
205: 	return true;
206: }
207: 
208: bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, TableScanState &state, idx_t &current_row,
209:                               idx_t max_row, idx_t base_row, VersionManager &manager,
210:                               unordered_map<idx_t, vector<TableFilter>> &table_filters) {
211: 	if (current_row >= max_row) {
212: 		// exceeded the amount of rows to scan
213: 		return false;
214: 	}
215: 	idx_t max_count = std::min((idx_t)STANDARD_VECTOR_SIZE, max_row - current_row);
216: 	idx_t vector_offset = current_row / STANDARD_VECTOR_SIZE;
217: 	//! first check the zonemap if we have to scan this partition
218: 	if (!CheckZonemap(state, table_filters, current_row)) {
219: 		return true;
220: 	}
221: 	// second, scan the version chunk manager to figure out which tuples to load for this transaction
222: 	SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
223: 	idx_t count = manager.GetSelVector(transaction, vector_offset, valid_sel, max_count);
224: 	if (count == 0) {
225: 		// nothing to scan for this vector, skip the entire vector
226: 		state.NextVector();
227: 		current_row += STANDARD_VECTOR_SIZE;
228: 		return true;
229: 	}
230: 	idx_t approved_tuple_count = count;
231: 	if (count == max_count && table_filters.empty()) {
232: 		//! If we don't have any deleted tuples or filters we can just run a regular scan
233: 		for (idx_t i = 0; i < state.column_ids.size(); i++) {
234: 			auto column = state.column_ids[i];
235: 			if (column == COLUMN_IDENTIFIER_ROW_ID) {
236: 				// scan row id
237: 				assert(result.data[i].type == ROW_TYPE);
238: 				result.data[i].Sequence(base_row + current_row, 1);
239: 			} else {
240: 				columns[column].Scan(transaction, state.column_scans[i], result.data[i]);
241: 			}
242: 		}
243: 	} else {
244: 		SelectionVector sel;
245: 
246: 		if (count != max_count) {
247: 			sel.Initialize(valid_sel);
248: 		} else {
249: 			sel.Initialize(FlatVector::IncrementalSelectionVector);
250: 		}
251: 		//! First, we scan the columns with filters, fetch their data and generate a selection vector.
252: 		//! get runtime statistics
253: 		auto start_time = high_resolution_clock::now();
254: 		for (idx_t i = 0; i < table_filters.size(); i++) {
255: 			auto tf_idx = state.adaptive_filter->permutation[i];
256: 			columns[tf_idx].Select(transaction, state.column_scans[tf_idx], result.data[tf_idx], sel,
257: 			                       approved_tuple_count, table_filters[tf_idx]);
258: 		}
259: 		for (auto &table_filter : table_filters) {
260: 			result.data[table_filter.first].Slice(sel, approved_tuple_count);
261: 		}
262: 		//! Now we use the selection vector to fetch data for the other columns.
263: 		for (idx_t i = 0; i < state.column_ids.size(); i++) {
264: 			if (table_filters.find(i) == table_filters.end()) {
265: 				auto column = state.column_ids[i];
266: 				if (column == COLUMN_IDENTIFIER_ROW_ID) {
267: 					assert(result.data[i].type == TypeId::INT64);
268: 					result.data[i].vector_type = VectorType::FLAT_VECTOR;
269: 					auto result_data = (int64_t *)FlatVector::GetData(result.data[i]);
270: 					for (size_t sel_idx = 0; sel_idx < approved_tuple_count; sel_idx++) {
271: 						result_data[sel_idx] = base_row + current_row + sel.get_index(sel_idx);
272: 					}
273: 				} else {
274: 					columns[column].FilterScan(transaction, state.column_scans[i], result.data[i], sel,
275: 					                           approved_tuple_count);
276: 				}
277: 			}
278: 		}
279: 		auto end_time = high_resolution_clock::now();
280: 		if (state.adaptive_filter && table_filters.size() > 1) {
281: 			state.adaptive_filter->AdaptRuntimeStatistics(
282: 			    duration_cast<duration<double>>(end_time - start_time).count());
283: 		}
284: 	}
285: 
286: 	result.SetCardinality(approved_tuple_count);
287: 	current_row += STANDARD_VECTOR_SIZE;
288: 	return true;
289: }
290: 
291: //===--------------------------------------------------------------------===//
292: // Index Scan
293: //===--------------------------------------------------------------------===//
294: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index,
295:                                     vector<column_t> column_ids) {
296: 	state.index = &index;
297: 	state.column_ids = move(column_ids);
298: 	transaction.storage.InitializeScan(this, state.local_state);
299: }
300: 
301: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value value,
302:                                     ExpressionType expr_type, vector<column_t> column_ids) {
303: 	InitializeIndexScan(transaction, state, index, move(column_ids));
304: 	state.index_state = index.InitializeScanSinglePredicate(transaction, state.column_ids, value, expr_type);
305: }
306: 
307: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value low_value,
308:                                     ExpressionType low_type, Value high_value, ExpressionType high_type,
309:                                     vector<column_t> column_ids) {
310: 	InitializeIndexScan(transaction, state, index, move(column_ids));
311: 	state.index_state =
312: 	    index.InitializeScanTwoPredicates(transaction, state.column_ids, low_value, low_type, high_value, high_type);
313: }
314: 
315: void DataTable::IndexScan(Transaction &transaction, DataChunk &result, TableIndexScanState &state) {
316: 	// clear any previously pinned blocks
317: 	state.fetch_state.handles.clear();
318: 	// scan the index
319: 	state.index->Scan(transaction, state, result);
320: 	if (result.size() > 0) {
321: 		return;
322: 	}
323: 	// scan the local structure
324: 	transaction.storage.Scan(state.local_state, state.column_ids, result);
325: }
326: 
327: //===--------------------------------------------------------------------===//
328: // Fetch
329: //===--------------------------------------------------------------------===//
330: void DataTable::Fetch(Transaction &transaction, DataChunk &result, vector<column_t> &column_ids,
331:                       Vector &row_identifiers, idx_t fetch_count, TableIndexScanState &state) {
332: 	// first figure out which row identifiers we should use for this transaction by looking at the VersionManagers
333: 	row_t rows[STANDARD_VECTOR_SIZE];
334: 	idx_t count = FetchRows(transaction, row_identifiers, fetch_count, rows);
335: 
336: 	if (count == 0) {
337: 		// no rows to use
338: 		return;
339: 	}
340: 	// for each of the remaining rows, now fetch the data
341: 	result.SetCardinality(count);
342: 	for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
343: 		auto column = column_ids[col_idx];
344: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
345: 			// row id column: fill in the row ids
346: 			assert(result.data[col_idx].type == TypeId::INT64);
347: 			result.data[col_idx].vector_type = VectorType::FLAT_VECTOR;
348: 			auto data = FlatVector::GetData<row_t>(result.data[col_idx]);
349: 			for (idx_t i = 0; i < count; i++) {
350: 				data[i] = rows[i];
351: 			}
352: 		} else {
353: 			// regular column: fetch data from the base column
354: 			for (idx_t i = 0; i < count; i++) {
355: 				auto row_id = rows[i];
356: 				columns[column].FetchRow(state.fetch_state, transaction, row_id, result.data[col_idx], i);
357: 			}
358: 		}
359: 	}
360: }
361: 
362: idx_t DataTable::FetchRows(Transaction &transaction, Vector &row_identifiers, idx_t fetch_count, row_t result_rows[]) {
363: 	assert(row_identifiers.type == ROW_TYPE);
364: 
365: 	// obtain a read lock on the version managers
366: 	auto l1 = persistent_manager.lock.GetSharedLock();
367: 	auto l2 = transient_manager.lock.GetSharedLock();
368: 
369: 	// now iterate over the row ids and figure out which rows to use
370: 	idx_t count = 0;
371: 
372: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
373: 	for (idx_t i = 0; i < fetch_count; i++) {
374: 		auto row_id = row_ids[i];
375: 		bool use_row;
376: 		if ((idx_t)row_id < persistent_manager.max_row) {
377: 			// persistent row: use persistent manager
378: 			use_row = persistent_manager.Fetch(transaction, row_id);
379: 		} else {
380: 			// transient row: use transient manager
381: 			use_row = transient_manager.Fetch(transaction, row_id);
382: 		}
383: 		if (use_row) {
384: 			// row is not deleted; use the row
385: 			result_rows[count++] = row_id;
386: 		}
387: 	}
388: 	return count;
389: }
390: 
391: //===--------------------------------------------------------------------===//
392: // Append
393: //===--------------------------------------------------------------------===//
394: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, string &col_name) {
395: 	if (VectorOperations::HasNull(vector, count)) {
396: 		throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name.c_str(), col_name.c_str());
397: 	}
398: }
399: 
400: static void VerifyCheckConstraint(TableCatalogEntry &table, Expression &expr, DataChunk &chunk) {
401: 	ExpressionExecutor executor(expr);
402: 	Vector result(TypeId::INT32);
403: 	try {
404: 		executor.ExecuteExpression(chunk, result);
405: 	} catch (Exception &ex) {
406: 		throw ConstraintException("CHECK constraint failed: %s (Error: %s)", table.name.c_str(), ex.what());
407: 	} catch (...) {
408: 		throw ConstraintException("CHECK constraint failed: %s (Unknown Error)", table.name.c_str());
409: 	}
410: 	VectorData vdata;
411: 	result.Orrify(chunk.size(), vdata);
412: 
413: 	auto dataptr = (int32_t *)vdata.data;
414: 	for (idx_t i = 0; i < chunk.size(); i++) {
415: 		auto idx = vdata.sel->get_index(i);
416: 		if (!(*vdata.nullmask)[idx] && dataptr[idx] == 0) {
417: 			throw ConstraintException("CHECK constraint failed: %s", table.name.c_str());
418: 		}
419: 	}
420: }
421: 
422: void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, DataChunk &chunk) {
423: 	for (auto &constraint : table.bound_constraints) {
424: 		switch (constraint->type) {
425: 		case ConstraintType::NOT_NULL: {
426: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
427: 			VerifyNotNullConstraint(table, chunk.data[not_null.index], chunk.size(),
428: 			                        table.columns[not_null.index].name);
429: 			break;
430: 		}
431: 		case ConstraintType::CHECK: {
432: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
433: 			VerifyCheckConstraint(table, *check.expression, chunk);
434: 			break;
435: 		}
436: 		case ConstraintType::UNIQUE: {
437: 			//! check whether or not the chunk can be inserted into the indexes
438: 			for (auto &index : indexes) {
439: 				index->VerifyAppend(chunk);
440: 			}
441: 			break;
442: 		}
443: 		case ConstraintType::FOREIGN_KEY:
444: 		default:
445: 			throw NotImplementedException("Constraint type not implemented!");
446: 		}
447: 	}
448: }
449: 
450: void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
451: 	if (chunk.size() == 0) {
452: 		return;
453: 	}
454: 	if (chunk.column_count() != table.columns.size()) {
455: 		throw CatalogException("Mismatch in column count for append");
456: 	}
457: 
458: 	chunk.Verify();
459: 
460: 	// verify any constraints on the new chunk
461: 	VerifyAppendConstraints(table, chunk);
462: 
463: 	// append to the transaction local data
464: 	auto &transaction = Transaction::GetTransaction(context);
465: 	transaction.storage.Append(this, chunk);
466: }
467: 
468: void DataTable::InitializeAppend(TableAppendState &state) {
469: 	// obtain the append lock for this table
470: 	state.append_lock = unique_lock<mutex>(append_lock);
471: 	// obtain locks on all indexes for the table
472: 	state.index_locks = unique_ptr<IndexLock[]>(new IndexLock[indexes.size()]);
473: 	for (idx_t i = 0; i < indexes.size(); i++) {
474: 		indexes[i]->InitializeLock(state.index_locks[i]);
475: 	}
476: 	// for each column, initialize the append state
477: 	state.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[types.size()]);
478: 	for (idx_t i = 0; i < types.size(); i++) {
479: 		columns[i].InitializeAppend(state.states[i]);
480: 	}
481: 	state.row_start = transient_manager.max_row;
482: 	state.current_row = state.row_start;
483: }
484: 
485: void DataTable::Append(Transaction &transaction, transaction_t commit_id, DataChunk &chunk, TableAppendState &state) {
486: 	assert(chunk.column_count() == types.size());
487: 	chunk.Verify();
488: 
489: 	// set up the inserted info in the version manager
490: 	transient_manager.Append(transaction, state.current_row, chunk.size(), commit_id);
491: 
492: 	// append the physical data to each of the entries
493: 	for (idx_t i = 0; i < types.size(); i++) {
494: 		columns[i].Append(state.states[i], chunk.data[i], chunk.size());
495: 	}
496: 	cardinality += chunk.size();
497: 	state.current_row += chunk.size();
498: }
499: 
500: void DataTable::RevertAppend(TableAppendState &state) {
501: 	if (state.row_start == state.current_row) {
502: 		// nothing to revert!
503: 		return;
504: 	}
505: 	// revert changes in the base columns
506: 	for (idx_t i = 0; i < types.size(); i++) {
507: 		columns[i].RevertAppend(state.row_start);
508: 	}
509: 	// adjust the cardinality
510: 	cardinality -= state.current_row - state.row_start;
511: 	transient_manager.max_row = state.row_start;
512: 	// revert changes in the transient manager
513: 	transient_manager.RevertAppend(state.row_start, state.current_row);
514: }
515: 
516: //===--------------------------------------------------------------------===//
517: // Indexes
518: //===--------------------------------------------------------------------===//
519: bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
520: 	if (indexes.size() == 0) {
521: 		return true;
522: 	}
523: 	// first generate the vector of row identifiers
524: 	Vector row_identifiers(ROW_TYPE);
525: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
526: 
527: 	idx_t failed_index = INVALID_INDEX;
528: 	// now append the entries to the indices
529: 	for (idx_t i = 0; i < indexes.size(); i++) {
530: 		if (!indexes[i]->Append(state.index_locks[i], chunk, row_identifiers)) {
531: 			failed_index = i;
532: 			break;
533: 		}
534: 	}
535: 	if (failed_index != INVALID_INDEX) {
536: 		// constraint violation!
537: 		// remove any appended entries from previous indexes (if any)
538: 		for (idx_t i = 0; i < failed_index; i++) {
539: 			indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
540: 		}
541: 		return false;
542: 	}
543: 	return true;
544: }
545: 
546: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
547: 	if (indexes.size() == 0) {
548: 		return;
549: 	}
550: 	// first generate the vector of row identifiers
551: 	Vector row_identifiers(ROW_TYPE);
552: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
553: 
554: 	// now remove the entries from the indices
555: 	RemoveFromIndexes(state, chunk, row_identifiers);
556: }
557: 
558: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
559: 	for (idx_t i = 0; i < indexes.size(); i++) {
560: 		indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
561: 	}
562: }
563: 
564: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
565: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
566: 	// create a selection vector from the row_ids
567: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
568: 	for (idx_t i = 0; i < count; i++) {
569: 		sel.set_index(i, row_ids[i] % STANDARD_VECTOR_SIZE);
570: 	}
571: 
572: 	// fetch the data for these row identifiers
573: 	DataChunk result;
574: 	result.Initialize(types);
575: 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
576: 	auto states = unique_ptr<ColumnScanState[]>(new ColumnScanState[types.size()]);
577: 	for (idx_t i = 0; i < types.size(); i++) {
578: 		columns[i].Fetch(states[i], row_ids[0], result.data[i]);
579: 	}
580: 	result.Slice(sel, count);
581: 	for (idx_t i = 0; i < indexes.size(); i++) {
582: 		indexes[i]->Delete(result, row_identifiers);
583: 	}
584: }
585: 
586: //===--------------------------------------------------------------------===//
587: // Delete
588: //===--------------------------------------------------------------------===//
589: void DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_identifiers, idx_t count) {
590: 	assert(row_identifiers.type == ROW_TYPE);
591: 	if (count == 0) {
592: 		return;
593: 	}
594: 
595: 	auto &transaction = Transaction::GetTransaction(context);
596: 
597: 	row_identifiers.Normalify(count);
598: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
599: 	auto first_id = ids[0];
600: 
601: 	if (first_id >= MAX_ROW_ID) {
602: 		// deletion is in transaction-local storage: push delete into local chunk collection
603: 		transaction.storage.Delete(this, row_identifiers, count);
604: 	} else if ((idx_t)first_id < persistent_manager.max_row) {
605: 		// deletion is in persistent storage: delete in the persistent version manager
606: 		persistent_manager.Delete(transaction, row_identifiers, count);
607: 	} else {
608: 		// deletion is in transient storage: delete in the persistent version manager
609: 		transient_manager.Delete(transaction, row_identifiers, count);
610: 	}
611: }
612: 
613: //===--------------------------------------------------------------------===//
614: // Update
615: //===--------------------------------------------------------------------===//
616: static void CreateMockChunk(vector<TypeId> &types, vector<column_t> &column_ids, DataChunk &chunk,
617:                             DataChunk &mock_chunk) {
618: 	// construct a mock DataChunk
619: 	mock_chunk.InitializeEmpty(types);
620: 	for (column_t i = 0; i < column_ids.size(); i++) {
621: 		mock_chunk.data[column_ids[i]].Reference(chunk.data[i]);
622: 	}
623: 	mock_chunk.SetCardinality(chunk.size());
624: }
625: 
626: static bool CreateMockChunk(TableCatalogEntry &table, vector<column_t> &column_ids,
627:                             unordered_set<column_t> &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
628: 	idx_t found_columns = 0;
629: 	// check whether the desired columns are present in the UPDATE clause
630: 	for (column_t i = 0; i < column_ids.size(); i++) {
631: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
632: 			found_columns++;
633: 		}
634: 	}
635: 	if (found_columns == 0) {
636: 		// no columns were found: no need to check the constraint again
637: 		return false;
638: 	}
639: 	if (found_columns != desired_column_ids.size()) {
640: 		// FIXME: not all columns in UPDATE clause are present!
641: 		// this should not be triggered at all as the binder should add these columns
642: 		throw NotImplementedException(
643: 		    "Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
644: 	}
645: 	// construct a mock DataChunk
646: 	auto types = table.GetTypes();
647: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
648: 	return true;
649: }
650: 
651: void DataTable::VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chunk, vector<column_t> &column_ids) {
652: 	for (auto &constraint : table.bound_constraints) {
653: 		switch (constraint->type) {
654: 		case ConstraintType::NOT_NULL: {
655: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
656: 			// check if the constraint is in the list of column_ids
657: 			for (idx_t i = 0; i < column_ids.size(); i++) {
658: 				if (column_ids[i] == not_null.index) {
659: 					// found the column id: check the data in
660: 					VerifyNotNullConstraint(table, chunk.data[i], chunk.size(), table.columns[not_null.index].name);
661: 					break;
662: 				}
663: 			}
664: 			break;
665: 		}
666: 		case ConstraintType::CHECK: {
667: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
668: 
669: 			DataChunk mock_chunk;
670: 			if (CreateMockChunk(table, column_ids, check.bound_columns, chunk, mock_chunk)) {
671: 				VerifyCheckConstraint(table, *check.expression, mock_chunk);
672: 			}
673: 			break;
674: 		}
675: 		case ConstraintType::UNIQUE:
676: 		case ConstraintType::FOREIGN_KEY:
677: 			break;
678: 		default:
679: 			throw NotImplementedException("Constraint type not implemented!");
680: 		}
681: 	}
682: 	// update should not be called for indexed columns!
683: 	// instead update should have been rewritten to delete + update on higher layer
684: #ifdef DEBUG
685: 	for (idx_t i = 0; i < indexes.size(); i++) {
686: 		assert(!indexes[i]->IndexIsUpdated(column_ids));
687: 	}
688: #endif
689: }
690: 
691: void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids, vector<column_t> &column_ids,
692:                        DataChunk &updates) {
693: 	assert(row_ids.type == ROW_TYPE);
694: 
695: 	updates.Verify();
696: 	if (updates.size() == 0) {
697: 		return;
698: 	}
699: 
700: 	// first verify that no constraints are violated
701: 	VerifyUpdateConstraints(table, updates, column_ids);
702: 
703: 	// now perform the actual update
704: 	auto &transaction = Transaction::GetTransaction(context);
705: 
706: 	updates.Normalify();
707: 	row_ids.Normalify(updates.size());
708: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
709: 	if (first_id >= MAX_ROW_ID) {
710: 		// update is in transaction-local storage: push update into local storage
711: 		transaction.storage.Update(this, row_ids, column_ids, updates);
712: 		return;
713: 	}
714: 
715: 	for (idx_t i = 0; i < column_ids.size(); i++) {
716: 		auto column = column_ids[i];
717: 		assert(column != COLUMN_IDENTIFIER_ROW_ID);
718: 
719: 		columns[column].Update(transaction, updates.data[i], row_ids, updates.size());
720: 	}
721: }
722: 
723: //===--------------------------------------------------------------------===//
724: // Create Index Scan
725: //===--------------------------------------------------------------------===//
726: void DataTable::InitializeCreateIndexScan(CreateIndexScanState &state, vector<column_t> column_ids) {
727: 	// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan
728: 	state.append_lock = unique_lock<mutex>(append_lock);
729: 	// get a read lock on the VersionManagers to prevent any further deletions
730: 	state.locks.push_back(persistent_manager.lock.GetSharedLock());
731: 	state.locks.push_back(transient_manager.lock.GetSharedLock());
732: 
733: 	InitializeScan(state, column_ids);
734: }
735: 
736: void DataTable::CreateIndexScan(CreateIndexScanState &state, DataChunk &result) {
737: 	// scan the persistent segments
738: 	if (ScanCreateIndex(state, result, state.current_persistent_row, state.max_persistent_row, 0)) {
739: 		return;
740: 	}
741: 	// scan the transient segments
742: 	if (ScanCreateIndex(state, result, state.current_transient_row, state.max_transient_row,
743: 	                    state.max_persistent_row)) {
744: 		return;
745: 	}
746: }
747: 
748: bool DataTable::ScanCreateIndex(CreateIndexScanState &state, DataChunk &result, idx_t &current_row, idx_t max_row,
749:                                 idx_t base_row) {
750: 	if (current_row >= max_row) {
751: 		return false;
752: 	}
753: 	idx_t count = std::min((idx_t)STANDARD_VECTOR_SIZE, max_row - current_row);
754: 
755: 	// scan the base columns to fetch the actual data
756: 	// note that we insert all data into the index, even if it is marked as deleted
757: 	// FIXME: tuples that are already "cleaned up" do not need to be inserted into the index!
758: 	for (idx_t i = 0; i < state.column_ids.size(); i++) {
759: 		auto column = state.column_ids[i];
760: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
761: 			// scan row id
762: 			assert(result.data[i].type == ROW_TYPE);
763: 			result.data[i].Sequence(base_row + current_row, 1);
764: 		} else {
765: 			// scan actual base column
766: 			columns[column].IndexScan(state.column_scans[i], result.data[i]);
767: 		}
768: 	}
769: 	result.SetCardinality(count);
770: 
771: 	current_row += STANDARD_VECTOR_SIZE;
772: 	return count > 0;
773: }
774: 
775: void DataTable::AddIndex(unique_ptr<Index> index, vector<unique_ptr<Expression>> &expressions) {
776: 	DataChunk result;
777: 	result.Initialize(index->types);
778: 
779: 	DataChunk intermediate;
780: 	vector<TypeId> intermediate_types;
781: 	auto column_ids = index->column_ids;
782: 	column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
783: 	for (auto &id : index->column_ids) {
784: 		intermediate_types.push_back(types[id]);
785: 	}
786: 	intermediate_types.push_back(ROW_TYPE);
787: 	intermediate.Initialize(intermediate_types);
788: 
789: 	// initialize an index scan
790: 	CreateIndexScanState state;
791: 	InitializeCreateIndexScan(state, column_ids);
792: 
793: 	// now start incrementally building the index
794: 	IndexLock lock;
795: 	index->InitializeLock(lock);
796: 	ExpressionExecutor executor(expressions);
797: 	while (true) {
798: 		intermediate.Reset();
799: 		// scan a new chunk from the table to index
800: 		CreateIndexScan(state, intermediate);
801: 		if (intermediate.size() == 0) {
802: 			// finished scanning for index creation
803: 			// release all locks
804: 			break;
805: 		}
806: 		// resolve the expressions for this chunk
807: 		executor.Execute(intermediate, result);
808: 
809: 		// insert into the index
810: 		if (!index->Insert(lock, result, intermediate.data[intermediate.column_count() - 1])) {
811: 			throw ConstraintException("Cant create unique index, table contains duplicate data on indexed column(s)");
812: 		}
813: 	}
814: 	indexes.push_back(move(index));
815: }
816: 
817: bool DataTable::IsTemporary() {
818: 	return schema.compare(TEMP_SCHEMA) == 0;
819: }
[end of src/storage/data_table.cpp]
[start of src/storage/local_storage.cpp]
1: #include "duckdb/transaction/local_storage.hpp"
2: #include "duckdb/execution/index/art/art.hpp"
3: #include "duckdb/storage/table/append_state.hpp"
4: #include "duckdb/storage/write_ahead_log.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/storage/uncompressed_segment.hpp"
7: 
8: using namespace duckdb;
9: using namespace std;
10: 
11: LocalTableStorage::LocalTableStorage(DataTable &table) : max_row(0) {
12: 	for (auto &index : table.indexes) {
13: 		assert(index->type == IndexType::ART);
14: 		auto &art = (ART &)*index;
15: 		if (art.is_unique) {
16: 			// unique index: create a local ART index that maintains the same unique constraint
17: 			vector<unique_ptr<Expression>> unbound_expressions;
18: 			for (auto &expr : art.unbound_expressions) {
19: 				unbound_expressions.push_back(expr->Copy());
20: 			}
21: 			indexes.push_back(make_unique<ART>(table, art.column_ids, move(unbound_expressions), true));
22: 		}
23: 	}
24: }
25: 
26: LocalTableStorage::~LocalTableStorage() {
27: }
28: 
29: void LocalTableStorage::InitializeScan(LocalScanState &state) {
30: 	state.storage = this;
31: 
32: 	state.chunk_index = 0;
33: 	state.max_index = collection.chunks.size() - 1;
34: 	state.last_chunk_count = collection.chunks.back()->size();
35: }
36: 
37: void LocalTableStorage::Clear() {
38: 	collection.chunks.clear();
39: 	indexes.clear();
40: 	deleted_entries.clear();
41: }
42: 
43: void LocalStorage::InitializeScan(DataTable *table, LocalScanState &state) {
44: 	auto entry = table_storage.find(table);
45: 	if (entry == table_storage.end()) {
46: 		// no local storage for table: set scan to nullptr
47: 		state.storage = nullptr;
48: 		return;
49: 	}
50: 	state.storage = entry->second.get();
51: 	state.storage->InitializeScan(state);
52: }
53: 
54: void LocalStorage::Scan(LocalScanState &state, const vector<column_t> &column_ids, DataChunk &result,
55:                         unordered_map<idx_t, vector<TableFilter>> *table_filters) {
56: 	if (!state.storage || state.chunk_index > state.max_index) {
57: 		// nothing left to scan
58: 		result.Reset();
59: 		return;
60: 	}
61: 	auto &chunk = *state.storage->collection.chunks[state.chunk_index];
62: 	idx_t chunk_count = state.chunk_index == state.max_index ? state.last_chunk_count : chunk.size();
63: 	idx_t count = chunk_count;
64: 
65: 	// first create a selection vector from the deleted entries (if any)
66: 	SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
67: 	auto entry = state.storage->deleted_entries.find(state.chunk_index);
68: 	if (entry != state.storage->deleted_entries.end()) {
69: 		// deleted entries! create a selection vector
70: 		auto deleted = entry->second.get();
71: 		idx_t new_count = 0;
72: 		for (idx_t i = 0; i < count; i++) {
73: 			if (!deleted[i]) {
74: 				valid_sel.set_index(new_count++, i);
75: 			}
76: 		}
77: 		if (new_count == 0 && count > 0) {
78: 			// all entries in this chunk were deleted: continue to next chunk
79: 			state.chunk_index++;
80: 			Scan(state, column_ids, result, table_filters);
81: 			return;
82: 		}
83: 		count = new_count;
84: 	}
85: 
86: 	SelectionVector sel;
87: 	if (count != chunk_count) {
88: 		sel.Initialize(valid_sel);
89: 	} else {
90: 		sel.Initialize(FlatVector::IncrementalSelectionVector);
91: 	}
92: 	// now scan the vectors of the chunk
93: 	for (idx_t i = 0; i < column_ids.size(); i++) {
94: 		auto id = column_ids[i];
95: 		if (id == COLUMN_IDENTIFIER_ROW_ID) {
96: 			// row identifier: return a sequence of rowids starting from MAX_ROW_ID plus the row offset in the chunk
97: 			result.data[i].Sequence(MAX_ROW_ID + state.chunk_index * STANDARD_VECTOR_SIZE, 1);
98: 		} else {
99: 			result.data[i].Reference(chunk.data[id]);
100: 		}
101: 		idx_t approved_tuple_count = count;
102: 		if (table_filters) {
103: 			auto column_filters = table_filters->find(i);
104: 			if (column_filters != table_filters->end()) {
105: 				//! We have filters to apply here
106: 				for (auto &column_filter : column_filters->second) {
107: 					nullmask_t nullmask = FlatVector::Nullmask(result.data[i]);
108: 					UncompressedSegment::filterSelection(sel, result.data[i], column_filter, approved_tuple_count,
109: 					                                     nullmask);
110: 				}
111: 				count = approved_tuple_count;
112: 			}
113: 		}
114: 	}
115: 	if (count == 0){
116: 	    // all entries in this chunk were filtered:: Continue on next chunk
117: 			state.chunk_index++;
118: 			Scan(state, column_ids, result, table_filters);
119: 			return;
120: 	}
121: 	if (count == chunk_count) {
122: 		result.SetCardinality(count);
123: 	} else {
124: 		result.Slice(sel, count);
125: 	}
126: 	state.chunk_index++;
127: }
128: 
129: void LocalStorage::Append(DataTable *table, DataChunk &chunk) {
130: 	auto entry = table_storage.find(table);
131: 	LocalTableStorage *storage;
132: 	if (entry == table_storage.end()) {
133: 		auto new_storage = make_unique<LocalTableStorage>(*table);
134: 		storage = new_storage.get();
135: 		table_storage.insert(make_pair(table, move(new_storage)));
136: 	} else {
137: 		storage = entry->second.get();
138: 	}
139: 	// append to unique indices (if any)
140: 	if (storage->indexes.size() > 0) {
141: 		idx_t base_id = MAX_ROW_ID + storage->collection.count;
142: 
143: 		// first generate the vector of row identifiers
144: 		Vector row_ids(ROW_TYPE);
145: 		VectorOperations::GenerateSequence(row_ids, chunk.size(), base_id, 1);
146: 
147: 		// now append the entries to the indices
148: 		for (auto &index : storage->indexes) {
149: 			if (!index->Append(chunk, row_ids)) {
150: 				throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
151: 			}
152: 		}
153: 	}
154: 
155: 	//! Append to the chunk
156: 	storage->collection.Append(chunk);
157: }
158: 
159: LocalTableStorage *LocalStorage::GetStorage(DataTable *table) {
160: 	auto entry = table_storage.find(table);
161: 	assert(entry != table_storage.end());
162: 	return entry->second.get();
163: }
164: 
165: static idx_t GetChunk(Vector &row_ids) {
166: 	auto ids = FlatVector::GetData<row_t>(row_ids);
167: 	auto first_id = ids[0] - MAX_ROW_ID;
168: 
169: 	return first_id / STANDARD_VECTOR_SIZE;
170: }
171: 
172: void LocalStorage::Delete(DataTable *table, Vector &row_ids, idx_t count) {
173: 	auto storage = GetStorage(table);
174: 	// figure out the chunk from which these row ids came
175: 	idx_t chunk_idx = GetChunk(row_ids);
176: 	assert(chunk_idx < storage->collection.chunks.size());
177: 
178: 	// get a pointer to the deleted entries for this chunk
179: 	bool *deleted;
180: 	auto entry = storage->deleted_entries.find(chunk_idx);
181: 	if (entry == storage->deleted_entries.end()) {
182: 		// nothing deleted yet, add the deleted entries
183: 		auto del_entries = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);
184: 		memset(del_entries.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
185: 		deleted = del_entries.get();
186: 		storage->deleted_entries.insert(make_pair(chunk_idx, move(del_entries)));
187: 	} else {
188: 		deleted = entry->second.get();
189: 	}
190: 
191: 	// now actually mark the entries as deleted in the deleted vector
192: 	idx_t base_index = MAX_ROW_ID + chunk_idx * STANDARD_VECTOR_SIZE;
193: 
194: 	auto ids = FlatVector::GetData<row_t>(row_ids);
195: 	for (idx_t i = 0; i < count; i++) {
196: 		auto id = ids[i] - base_index;
197: 		deleted[id] = true;
198: 	}
199: }
200: 
201: template <class T>
202: static void update_data(Vector &data_vector, Vector &update_vector, Vector &row_ids, idx_t count, idx_t base_index) {
203: 	VectorData udata;
204: 	update_vector.Orrify(count, udata);
205: 
206: 	auto target = FlatVector::GetData<T>(data_vector);
207: 	auto &nullmask = FlatVector::Nullmask(data_vector);
208: 	auto ids = FlatVector::GetData<row_t>(row_ids);
209: 	auto updates = (T *)udata.data;
210: 
211: 	for (idx_t i = 0; i < count; i++) {
212: 		auto uidx = udata.sel->get_index(i);
213: 
214: 		auto id = ids[i] - base_index;
215: 		target[id] = updates[uidx];
216: 		nullmask[id] = (*udata.nullmask)[uidx];
217: 	}
218: }
219: 
220: static void update_chunk(Vector &data, Vector &updates, Vector &row_ids, idx_t count, idx_t base_index) {
221: 	assert(data.type == updates.type);
222: 	assert(row_ids.type == ROW_TYPE);
223: 
224: 	switch (data.type) {
225: 	case TypeId::INT8:
226: 		update_data<int8_t>(data, updates, row_ids, count, base_index);
227: 		break;
228: 	case TypeId::INT16:
229: 		update_data<int16_t>(data, updates, row_ids, count, base_index);
230: 		break;
231: 	case TypeId::INT32:
232: 		update_data<int32_t>(data, updates, row_ids, count, base_index);
233: 		break;
234: 	case TypeId::INT64:
235: 		update_data<int64_t>(data, updates, row_ids, count, base_index);
236: 		break;
237: 	case TypeId::FLOAT:
238: 		update_data<float>(data, updates, row_ids, count, base_index);
239: 		break;
240: 	case TypeId::DOUBLE:
241: 		update_data<double>(data, updates, row_ids, count, base_index);
242: 		break;
243: 	default:
244: 		throw Exception("Unsupported type for in-place update");
245: 	}
246: }
247: 
248: void LocalStorage::Update(DataTable *table, Vector &row_ids, vector<column_t> &column_ids, DataChunk &data) {
249: 	auto storage = GetStorage(table);
250: 	// figure out the chunk from which these row ids came
251: 	idx_t chunk_idx = GetChunk(row_ids);
252: 	assert(chunk_idx < storage->collection.chunks.size());
253: 
254: 	idx_t base_index = MAX_ROW_ID + chunk_idx * STANDARD_VECTOR_SIZE;
255: 
256: 	// now perform the actual update
257: 	auto &chunk = *storage->collection.chunks[chunk_idx];
258: 	for (idx_t i = 0; i < column_ids.size(); i++) {
259: 		auto col_idx = column_ids[i];
260: 		update_chunk(chunk.data[col_idx], data.data[i], row_ids, data.size(), base_index);
261: 	}
262: }
263: 
264: template <class T> bool LocalStorage::ScanTableStorage(DataTable *table, LocalTableStorage *storage, T &&fun) {
265: 	vector<column_t> column_ids;
266: 	for (idx_t i = 0; i < table->types.size(); i++) {
267: 		column_ids.push_back(i);
268: 	}
269: 
270: 	DataChunk chunk;
271: 	chunk.Initialize(table->types);
272: 
273: 	// initialize the scan
274: 	LocalScanState state;
275: 	storage->InitializeScan(state);
276: 
277: 	while (true) {
278: 		Scan(state, column_ids, chunk);
279: 		if (chunk.size() == 0) {
280: 			return true;
281: 		}
282: 		if (!fun(chunk)) {
283: 			return false;
284: 		}
285: 	}
286: }
287: 
288: void LocalStorage::Commit(LocalStorage::CommitState &commit_state, Transaction &transaction, WriteAheadLog *log,
289:                           transaction_t commit_id) {
290: 	// commit local storage, iterate over all entries in the table storage map
291: 	for (auto &entry : table_storage) {
292: 		auto table = entry.first;
293: 		auto storage = entry.second.get();
294: 
295: 		// initialize the append state
296: 		auto append_state_ptr = make_unique<TableAppendState>();
297: 		auto &append_state = *append_state_ptr;
298: 		// add it to the set of append states
299: 		commit_state.append_states[table] = move(append_state_ptr);
300: 		table->InitializeAppend(append_state);
301: 
302: 		if (log && !table->IsTemporary()) {
303: 			log->WriteSetTable(table->schema, table->table);
304: 		}
305: 
306: 		// scan all chunks in this storage
307: 		ScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {
308: 			// append this chunk to the indexes of the table
309: 			if (!table->AppendToIndexes(append_state, chunk, append_state.current_row)) {
310: 				throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
311: 			}
312: 
313: 			// append to base table
314: 			table->Append(transaction, commit_id, chunk, append_state);
315: 			// if there is a WAL, write the chunk to there as well
316: 			if (log && !table->IsTemporary()) {
317: 				log->WriteInsert(chunk);
318: 			}
319: 			return true;
320: 		});
321: 	}
322: 	// finished commit: clear local storage
323: 	for (auto &entry : table_storage) {
324: 		entry.second->Clear();
325: 	}
326: 	table_storage.clear();
327: }
328: 
329: void LocalStorage::RevertCommit(LocalStorage::CommitState &commit_state) {
330: 	for (auto &entry : commit_state.append_states) {
331: 		auto table = entry.first;
332: 		auto storage = table_storage[table].get();
333: 		auto &append_state = *entry.second;
334: 		if (table->indexes.size() > 0 && !(table->schema == "temp")) {
335: 			row_t current_row = append_state.row_start;
336: 			// remove the data from the indexes, if there are any indexes
337: 			ScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {
338: 				// append this chunk to the indexes of the table
339: 				table->RemoveFromIndexes(append_state, chunk, current_row);
340: 
341: 				current_row += chunk.size();
342: 				if (current_row >= append_state.current_row) {
343: 					// finished deleting all rows from the index: abort now
344: 					return false;
345: 				}
346: 				return true;
347: 			});
348: 		}
349: 
350: 		table->RevertAppend(*entry.second);
351: 	}
352: }
[end of src/storage/local_storage.cpp]
[start of src/storage/table/version_manager.cpp]
1: #include "duckdb/storage/table/version_manager.hpp"
2: #include "duckdb/transaction/transaction.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: 
5: using namespace duckdb;
6: using namespace std;
7: 
8: idx_t VersionManager::GetSelVector(Transaction &transaction, idx_t index, SelectionVector &sel_vector,
9:                                    idx_t max_count) {
10: 	// obtain a read lock
11: 	auto read_lock = lock.GetSharedLock();
12: 
13: 	auto entry = info.find(index);
14: 	if (entry == info.end()) {
15: 		// no info, use everything
16: 		return max_count;
17: 	} else {
18: 		// get the selection vector from the chunk info
19: 		return entry->second->GetSelVector(transaction, sel_vector, max_count);
20: 	}
21: }
22: 
23: bool VersionManager::Fetch(Transaction &transaction, idx_t row) {
24: 	row -= base_row;
25: 	idx_t vector_index = row / STANDARD_VECTOR_SIZE;
26: 
27: 	auto entry = info.find(vector_index);
28: 	if (entry == info.end()) {
29: 		// no info, use the row
30: 		return true;
31: 	} else {
32: 		// there is an info: need to figure out if we want to use the row or not
33: 		return entry->second->Fetch(transaction, row - vector_index * STANDARD_VECTOR_SIZE);
34: 	}
35: }
36: 
37: class VersionDeleteState {
38: public:
39: 	VersionDeleteState(VersionManager &manager, Transaction &transaction, idx_t base_row)
40: 	    : manager(manager), transaction(transaction), current_info(nullptr), current_chunk((idx_t)-1), count(0),
41: 	      base_row(base_row) {
42: 	}
43: 
44: 	VersionManager &manager;
45: 	Transaction &transaction;
46: 	ChunkInfo *current_info;
47: 	idx_t current_chunk;
48: 	row_t rows[STANDARD_VECTOR_SIZE];
49: 	idx_t count;
50: 	idx_t base_row;
51: 	idx_t chunk_row;
52: 
53: public:
54: 	void Delete(row_t row_id);
55: 	void Flush();
56: };
57: 
58: void VersionManager::Delete(Transaction &transaction, Vector &row_ids, idx_t count) {
59: 	VersionDeleteState del_state(*this, transaction, base_row);
60: 
61: 	VectorData rdata;
62: 	row_ids.Orrify(count, rdata);
63: 	// obtain a write lock
64: 	auto write_lock = lock.GetExclusiveLock();
65: 	auto ids = (row_t *)rdata.data;
66: 	for (idx_t i = 0; i < count; i++) {
67: 		auto ridx = rdata.sel->get_index(i);
68: 		del_state.Delete(ids[ridx] - base_row);
69: 	}
70: 	del_state.Flush();
71: }
72: 
73: void VersionDeleteState::Delete(row_t row_id) {
74: 	idx_t chunk_idx = row_id / STANDARD_VECTOR_SIZE;
75: 	idx_t idx_in_chunk = row_id - chunk_idx * STANDARD_VECTOR_SIZE;
76: 
77: 	// check if we are targetting a different chunk than the current chunk
78: 	if (chunk_idx != current_chunk) {
79: 		// if we are, first flush the previous chunk
80: 		Flush();
81: 
82: 		// then look up if the chunk already exists
83: 		auto entry = manager.info.find(chunk_idx);
84: 		if (entry == manager.info.end()) {
85: 			// no version info yet: have to create one
86: 			auto new_info = make_unique<ChunkDeleteInfo>(manager, chunk_idx * STANDARD_VECTOR_SIZE);
87: 			current_info = new_info.get();
88: 			manager.info[chunk_idx] = move(new_info);
89: 		} else {
90: 			// version info already exists: alter existing version info
91: 			current_info = entry->second.get();
92: 		}
93: 		current_chunk = chunk_idx;
94: 		chunk_row = chunk_idx * STANDARD_VECTOR_SIZE;
95: 	}
96: 
97: 	// now add the row to the set of to-be-deleted rows
98: 	rows[count++] = idx_in_chunk;
99: }
100: 
101: void VersionDeleteState::Flush() {
102: 	if (count == 0) {
103: 		return;
104: 	}
105: 	// delete in the current info
106: 	current_info->Delete(transaction, rows, count);
107: 	// now push the delete into the undo buffer
108: 	transaction.PushDelete(current_info, rows, count, base_row + chunk_row);
109: 	count = 0;
110: }
111: 
112: void VersionManager::Append(Transaction &transaction, row_t row_start, idx_t count, transaction_t commit_id) {
113: 	idx_t chunk_idx = row_start / STANDARD_VECTOR_SIZE;
114: 	idx_t idx_in_chunk = row_start - chunk_idx * STANDARD_VECTOR_SIZE;
115: 
116: 	// obtain a write lock
117: 	auto write_lock = lock.GetExclusiveLock();
118: 	auto current_info = GetInsertInfo(chunk_idx);
119: 	for (idx_t i = 0; i < count; i++) {
120: 		current_info->inserted[idx_in_chunk] = commit_id;
121: 		idx_in_chunk++;
122: 		if (idx_in_chunk == STANDARD_VECTOR_SIZE) {
123: 			chunk_idx++;
124: 			idx_in_chunk = 0;
125: 			current_info = GetInsertInfo(chunk_idx);
126: 		}
127: 	}
128: 	max_row += count;
129: }
130: 
131: ChunkInsertInfo *VersionManager::GetInsertInfo(idx_t chunk_idx) {
132: 	auto entry = info.find(chunk_idx);
133: 	if (entry == info.end()) {
134: 		// no version info yet: have to create one
135: 		auto new_info = make_unique<ChunkInsertInfo>(*this, chunk_idx * STANDARD_VECTOR_SIZE);
136: 		auto result = new_info.get();
137: 		info[chunk_idx] = move(new_info);
138: 		return result;
139: 	} else {
140: 		// version info already exists: check if it is insert or delete info
141: 		auto current_info = entry->second.get();
142: 		if (current_info->type == ChunkInfoType::INSERT_INFO) {
143: 			return (ChunkInsertInfo *)current_info;
144: 		} else {
145: 			assert(current_info->type == ChunkInfoType::DELETE_INFO);
146: 			// delete info, change to insert info
147: 			auto new_info = make_unique<ChunkInsertInfo>((ChunkDeleteInfo &)*current_info);
148: 			auto result = new_info.get();
149: 			info[chunk_idx] = move(new_info);
150: 			return result;
151: 		}
152: 	}
153: }
154: 
155: void VersionManager::RevertAppend(row_t row_start, row_t row_end) {
156: 	auto write_lock = lock.GetExclusiveLock();
157: 
158: 	idx_t chunk_start = row_start / STANDARD_VECTOR_SIZE + (row_start % STANDARD_VECTOR_SIZE == 0 ? 0 : 1);
159: 	idx_t chunk_end = row_end / STANDARD_VECTOR_SIZE;
160: 	for (; chunk_start <= chunk_end; chunk_start++) {
161: 		info.erase(chunk_start);
162: 	}
163: }
[end of src/storage/table/version_manager.cpp]
[start of src/transaction/CMakeLists.txt]
1: add_library_unity(duckdb_transaction
2:                   OBJECT
3:                   undo_buffer.cpp
4:                   delete_info.cpp
5:                   transaction_context.cpp
6:                   transaction.cpp
7:                   transaction_manager.cpp
8:                   commit_state.cpp
9:                   rollback_state.cpp
10:                   cleanup_state.cpp)
11: set(ALL_OBJECT_FILES
12:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_transaction>
13:     PARENT_SCOPE)
[end of src/transaction/CMakeLists.txt]
[start of src/transaction/cleanup_state.cpp]
1: #include "duckdb/transaction/cleanup_state.hpp"
2: #include "duckdb/transaction/delete_info.hpp"
3: #include "duckdb/transaction/update_info.hpp"
4: 
5: #include "duckdb/storage/data_table.hpp"
6: #include "duckdb/storage/uncompressed_segment.hpp"
7: 
8: #include "duckdb/catalog/catalog.hpp"
9: #include "duckdb/catalog/dependency_manager.hpp"
10: 
11: using namespace duckdb;
12: using namespace std;
13: 
14: CleanupState::CleanupState() : current_table(nullptr), count(0) {
15: }
16: 
17: CleanupState::~CleanupState() {
18: 	Flush();
19: }
20: 
21: void CleanupState::CleanupEntry(UndoFlags type, data_ptr_t data) {
22: 	switch (type) {
23: 	case UndoFlags::CATALOG_ENTRY: {
24: 		CatalogEntry *catalog_entry = *((CatalogEntry **)data);
25: 		// destroy the backed up entry: it is no longer required
26: 		assert(catalog_entry->parent);
27: 		if (catalog_entry->parent->type != CatalogType::UPDATED_ENTRY) {
28: 			if (!catalog_entry->parent->child->deleted) {
29: 				// delete the entry from the dependency manager, if it is not deleted yet
30: 				catalog_entry->catalog->dependency_manager.EraseObject(catalog_entry->parent->child.get());
31: 			}
32: 			catalog_entry->parent->child = move(catalog_entry->child);
33: 		}
34: 		break;
35: 	}
36: 	case UndoFlags::DELETE_TUPLE: {
37: 		auto info = (DeleteInfo *)data;
38: 		CleanupDelete(info);
39: 		break;
40: 	}
41: 	case UndoFlags::UPDATE_TUPLE: {
42: 		auto info = (UpdateInfo *)data;
43: 		CleanupUpdate(info);
44: 		break;
45: 	}
46: 	default:
47: 		break;
48: 	}
49: }
50: 
51: void CleanupState::CleanupUpdate(UpdateInfo *info) {
52: 	// remove the update info from the update chain
53: 	// first obtain an exclusive lock on the segment
54: 	auto lock = info->segment->lock.GetExclusiveLock();
55: 	info->segment->CleanupUpdate(info);
56: }
57: 
58: void CleanupState::CleanupDelete(DeleteInfo *info) {
59: 	auto version_table = &info->GetTable();
60: 	if (version_table->indexes.size() == 0) {
61: 		// this table has no indexes: no cleanup to be done
62: 		return;
63: 	}
64: 	if (current_table != version_table) {
65: 		// table for this entry differs from previous table: flush and switch to the new table
66: 		Flush();
67: 		current_table = version_table;
68: 	}
69: 	for (idx_t i = 0; i < info->count; i++) {
70: 		if (count == STANDARD_VECTOR_SIZE) {
71: 			Flush();
72: 		}
73: 		row_numbers[count++] = info->vinfo->start + info->rows[i];
74: 	}
75: }
76: 
77: void CleanupState::Flush() {
78: 	if (count == 0) {
79: 		return;
80: 	}
81: 
82: 	// set up the row identifiers vector
83: 	Vector row_identifiers(ROW_TYPE, (data_ptr_t)row_numbers);
84: 
85: 	// delete the tuples from all the indexes
86: 	current_table->RemoveFromIndexes(row_identifiers, count);
87: 
88: 	count = 0;
89: }
[end of src/transaction/cleanup_state.cpp]
[start of src/transaction/commit_state.cpp]
1: #include "duckdb/transaction/commit_state.hpp"
2: #include "duckdb/transaction/delete_info.hpp"
3: #include "duckdb/transaction/update_info.hpp"
4: 
5: #include "duckdb/storage/data_table.hpp"
6: #include "duckdb/storage/write_ahead_log.hpp"
7: #include "duckdb/storage/uncompressed_segment.hpp"
8: #include "duckdb/common/serializer/buffered_deserializer.hpp"
9: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
10: 
11: using namespace duckdb;
12: using namespace std;
13: 
14: CommitState::CommitState(transaction_t commit_id, WriteAheadLog *log)
15:     : log(log), commit_id(commit_id), current_table(nullptr) {
16: }
17: 
18: void CommitState::SwitchTable(DataTable *table, UndoFlags new_op) {
19: 	if (current_table != table) {
20: 		// write the current table to the log
21: 		log->WriteSetTable(table->schema, table->table);
22: 		current_table = table;
23: 	}
24: }
25: 
26: void CommitState::WriteCatalogEntry(CatalogEntry *entry, data_ptr_t dataptr) {
27: 	assert(log);
28: 	// look at the type of the parent entry
29: 	auto parent = entry->parent;
30: 	switch (parent->type) {
31: 	case CatalogType::TABLE:
32: 		if (parent->temporary) {
33: 			return;
34: 		}
35: 		if (entry->type == CatalogType::TABLE) {
36: 			// ALTER TABLE statement, read the extra data after the entry
37: 			auto extra_data_size = *((idx_t *)dataptr);
38: 			auto extra_data = (data_ptr_t)(dataptr + sizeof(idx_t));
39: 			// deserialize it
40: 			BufferedDeserializer source(extra_data, extra_data_size);
41: 			auto info = AlterInfo::Deserialize(source);
42: 			// write the alter table in the log
43: 			log->WriteAlter(*info);
44: 		} else {
45: 			// CREATE TABLE statement
46: 			log->WriteCreateTable((TableCatalogEntry *)parent);
47: 		}
48: 		break;
49: 	case CatalogType::SCHEMA:
50: 		if (entry->type == CatalogType::SCHEMA) {
51: 			// ALTER TABLE statement, skip it
52: 			return;
53: 		}
54: 		log->WriteCreateSchema((SchemaCatalogEntry *)parent);
55: 		break;
56: 	case CatalogType::VIEW:
57: 		log->WriteCreateView((ViewCatalogEntry *)parent);
58: 		break;
59: 	case CatalogType::SEQUENCE:
60: 		log->WriteCreateSequence((SequenceCatalogEntry *)parent);
61: 		break;
62: 	case CatalogType::DELETED_ENTRY:
63: 		if (entry->type == CatalogType::TABLE) {
64: 			log->WriteDropTable((TableCatalogEntry *)entry);
65: 		} else if (entry->type == CatalogType::SCHEMA) {
66: 			log->WriteDropSchema((SchemaCatalogEntry *)entry);
67: 		} else if (entry->type == CatalogType::VIEW) {
68: 			log->WriteDropView((ViewCatalogEntry *)entry);
69: 		} else if (entry->type == CatalogType::SEQUENCE) {
70: 			log->WriteDropSequence((SequenceCatalogEntry *)entry);
71: 		} else if (entry->type == CatalogType::PREPARED_STATEMENT) {
72: 			// do nothing, we log the query to drop this
73: 		} else {
74: 			throw NotImplementedException("Don't know how to drop this type!");
75: 		}
76: 		break;
77: 
78: 	case CatalogType::INDEX:
79: 	case CatalogType::PREPARED_STATEMENT:
80: 	case CatalogType::AGGREGATE_FUNCTION:
81: 	case CatalogType::SCALAR_FUNCTION:
82: 	case CatalogType::TABLE_FUNCTION:
83: 
84: 		// do nothing, we log the query to recreate this
85: 		break;
86: 	default:
87: 		throw NotImplementedException("UndoBuffer - don't know how to write this entry to the WAL");
88: 	}
89: }
90: 
91: void CommitState::WriteDelete(DeleteInfo *info) {
92: 	assert(log);
93: 	// switch to the current table, if necessary
94: 	SwitchTable(&info->GetTable(), UndoFlags::DELETE_TUPLE);
95: 
96: 	if (!delete_chunk) {
97: 		delete_chunk = make_unique<DataChunk>();
98: 		vector<TypeId> delete_types = {ROW_TYPE};
99: 		delete_chunk->Initialize(delete_types);
100: 	}
101: 	auto rows = FlatVector::GetData<row_t>(delete_chunk->data[0]);
102: 	for (idx_t i = 0; i < info->count; i++) {
103: 		rows[i] = info->base_row + info->rows[i];
104: 	}
105: 	delete_chunk->SetCardinality(info->count);
106: 	log->WriteDelete(*delete_chunk);
107: }
108: 
109: void CommitState::WriteUpdate(UpdateInfo *info) {
110: 	assert(log);
111: 	// switch to the current table, if necessary
112: 	SwitchTable(info->column_data->table, UndoFlags::UPDATE_TUPLE);
113: 
114: 	update_chunk = make_unique<DataChunk>();
115: 	vector<TypeId> update_types = {info->column_data->type, ROW_TYPE};
116: 	update_chunk->Initialize(update_types);
117: 
118: 	// fetch the updated values from the base table
119: 	ColumnScanState state;
120: 	info->segment->InitializeScan(state);
121: 	info->segment->Fetch(state, info->vector_index, update_chunk->data[0]);
122: 
123: 	// write the row ids into the chunk
124: 	auto row_ids = FlatVector::GetData<row_t>(update_chunk->data[1]);
125: 	idx_t start = info->segment->row_start + info->vector_index * STANDARD_VECTOR_SIZE;
126: 	for (idx_t i = 0; i < info->N; i++) {
127: 		row_ids[info->tuples[i]] = start + info->tuples[i];
128: 	}
129: 	SelectionVector sel(info->tuples);
130: 	update_chunk->Slice(sel, info->N);
131: 
132: 	log->WriteUpdate(*update_chunk, info->column_data->column_idx);
133: }
134: 
135: template <bool HAS_LOG> void CommitState::CommitEntry(UndoFlags type, data_ptr_t data) {
136: 	switch (type) {
137: 	case UndoFlags::CATALOG_ENTRY: {
138: 		// set the commit timestamp of the catalog entry to the given id
139: 		CatalogEntry *catalog_entry = *((CatalogEntry **)data);
140: 		assert(catalog_entry->parent);
141: 		catalog_entry->parent->timestamp = commit_id;
142: 
143: 		if (HAS_LOG) {
144: 			// push the catalog update to the WAL
145: 			WriteCatalogEntry(catalog_entry, data + sizeof(CatalogEntry *));
146: 		}
147: 		break;
148: 	}
149: 	case UndoFlags::DELETE_TUPLE: {
150: 		// deletion:
151: 		auto info = (DeleteInfo *)data;
152: 		info->GetTable().cardinality -= info->count;
153: 		if (HAS_LOG && !info->GetTable().IsTemporary()) {
154: 			WriteDelete(info);
155: 		}
156: 		// mark the tuples as committed
157: 		info->vinfo->CommitDelete(commit_id, info->rows, info->count);
158: 		break;
159: 	}
160: 	case UndoFlags::UPDATE_TUPLE: {
161: 		// update:
162: 		auto info = (UpdateInfo *)data;
163: 		if (HAS_LOG && !info->column_data->table->IsTemporary()) {
164: 			WriteUpdate(info);
165: 		}
166: 		info->version_number = commit_id;
167: 		break;
168: 	}
169: 	default:
170: 		throw NotImplementedException("UndoBuffer - don't know how to commit this type!");
171: 	}
172: }
173: 
174: void CommitState::RevertCommit(UndoFlags type, data_ptr_t data) {
175: 	transaction_t transaction_id = commit_id;
176: 	switch (type) {
177: 	case UndoFlags::CATALOG_ENTRY: {
178: 		// set the commit timestamp of the catalog entry to the given id
179: 		CatalogEntry *catalog_entry = *((CatalogEntry **)data);
180: 		assert(catalog_entry->parent);
181: 		catalog_entry->parent->timestamp = transaction_id;
182: 		break;
183: 	}
184: 	case UndoFlags::DELETE_TUPLE: {
185: 		// deletion:
186: 		auto info = (DeleteInfo *)data;
187: 		info->GetTable().cardinality += info->count;
188: 		// revert the commit by writing the (uncommitted) transaction_id back into the version info
189: 		info->vinfo->CommitDelete(transaction_id, info->rows, info->count);
190: 		break;
191: 	}
192: 	case UndoFlags::UPDATE_TUPLE: {
193: 		// update:
194: 		auto info = (UpdateInfo *)data;
195: 		info->version_number = transaction_id;
196: 		break;
197: 	}
198: 	default:
199: 		throw NotImplementedException("UndoBuffer - don't know how to revert commit of this type!");
200: 	}
201: }
202: 
203: template void CommitState::CommitEntry<true>(UndoFlags type, data_ptr_t data);
204: template void CommitState::CommitEntry<false>(UndoFlags type, data_ptr_t data);
[end of src/transaction/commit_state.cpp]
[start of src/transaction/delete_info.cpp]
1: #include "duckdb/transaction/delete_info.hpp"
2: #include "duckdb/storage/table/chunk_info.hpp"
3: #include "duckdb/storage/table/version_manager.hpp"
4: 
5: using namespace duckdb;
6: using namespace std;
7: 
8: DataTable &DeleteInfo::GetTable() {
9: 	return vinfo->manager.table;
10: }
[end of src/transaction/delete_info.cpp]
[start of src/transaction/transaction.cpp]
1: #include "duckdb/transaction/transaction.hpp"
2: 
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/parser/column_definition.hpp"
7: #include "duckdb/storage/data_table.hpp"
8: #include "duckdb/storage/write_ahead_log.hpp"
9: 
10: #include "duckdb/transaction/delete_info.hpp"
11: #include "duckdb/transaction/update_info.hpp"
12: 
13: #include <cstring>
14: 
15: using namespace duckdb;
16: using namespace std;
17: 
18: Transaction &Transaction::GetTransaction(ClientContext &context) {
19: 	return context.ActiveTransaction();
20: }
21: 
22: void Transaction::PushCatalogEntry(CatalogEntry *entry, data_ptr_t extra_data, idx_t extra_data_size) {
23: 	idx_t alloc_size = sizeof(CatalogEntry *);
24: 	if (extra_data_size > 0) {
25: 		alloc_size += extra_data_size + sizeof(idx_t);
26: 	}
27: 	auto baseptr = undo_buffer.CreateEntry(UndoFlags::CATALOG_ENTRY, alloc_size);
28: 	// store the pointer to the catalog entry
29: 	*((CatalogEntry **)baseptr) = entry;
30: 	if (extra_data_size > 0) {
31: 		// copy the extra data behind the catalog entry pointer (if any)
32: 		baseptr += sizeof(CatalogEntry *);
33: 		// first store the extra data size
34: 		*((idx_t *)baseptr) = extra_data_size;
35: 		baseptr += sizeof(idx_t);
36: 		// then copy over the actual data
37: 		memcpy(baseptr, extra_data, extra_data_size);
38: 	}
39: }
40: 
41: void Transaction::PushDelete(ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row) {
42: 	auto delete_info =
43: 	    (DeleteInfo *)undo_buffer.CreateEntry(UndoFlags::DELETE_TUPLE, sizeof(DeleteInfo) + sizeof(row_t) * count);
44: 	delete_info->vinfo = vinfo;
45: 	delete_info->count = count;
46: 	delete_info->base_row = base_row;
47: 	memcpy(delete_info->rows, rows, sizeof(row_t) * count);
48: }
49: 
50: UpdateInfo *Transaction::CreateUpdateInfo(idx_t type_size, idx_t entries) {
51: 	auto update_info = (UpdateInfo *)undo_buffer.CreateEntry(
52: 	    UndoFlags::UPDATE_TUPLE, sizeof(UpdateInfo) + (sizeof(sel_t) + type_size) * entries);
53: 	update_info->max = entries;
54: 	update_info->tuples = (sel_t *)(((data_ptr_t)update_info) + sizeof(UpdateInfo));
55: 	update_info->tuple_data = ((data_ptr_t)update_info) + sizeof(UpdateInfo) + sizeof(sel_t) * entries;
56: 	update_info->version_number = transaction_id;
57: 	update_info->nullmask.reset();
58: 	return update_info;
59: }
60: 
61: string Transaction::Commit(WriteAheadLog *log, transaction_t commit_id) noexcept {
62: 	this->commit_id = commit_id;
63: 
64: 	UndoBuffer::IteratorState iterator_state;
65: 	LocalStorage::CommitState commit_state;
66: 	int64_t initial_wal_size;
67: 	if (log) {
68: 		initial_wal_size = log->GetWALSize();
69: 	}
70: 	bool changes_made = undo_buffer.ChangesMade() || storage.ChangesMade() || sequence_usage.size() > 0;
71: 	try {
72: 		// commit the undo buffer
73: 		undo_buffer.Commit(iterator_state, log, commit_id);
74: 		storage.Commit(commit_state, *this, log, commit_id);
75: 		if (log) {
76: 			// commit any sequences that were used to the WAL
77: 			for (auto &entry : sequence_usage) {
78: 				log->WriteSequenceValue(entry.first, entry.second);
79: 			}
80: 			// flush the WAL
81: 			if (changes_made) {
82: 				log->Flush();
83: 			}
84: 		}
85: 		return string();
86: 	} catch (std::exception &ex) {
87: 		undo_buffer.RevertCommit(iterator_state, transaction_id);
88: 		storage.RevertCommit(commit_state);
89: 		if (log && changes_made) {
90: 			// remove any entries written into the WAL by truncating it
91: 			log->Truncate(initial_wal_size);
92: 		}
93: 		return ex.what();
94: 	}
95: }
[end of src/transaction/transaction.cpp]
[start of tools/pythonpkg/duckdb_python.cpp]
1: #include <pybind11/pybind11.h>
2: #include <pybind11/numpy.h>
3: 
4: #include <unordered_map>
5: #include <vector>
6: 
7: #include "datetime.h" // from Python
8: 
9: #include "duckdb.hpp"
10: 
11: namespace py = pybind11;
12: 
13: using namespace duckdb;
14: using namespace std;
15: 
16: namespace duckdb_py_convert {
17: 
18: struct RegularConvert {
19: 	template <class DUCKDB_T, class NUMPY_T> static NUMPY_T convert_value(DUCKDB_T val) {
20: 		return (NUMPY_T)val;
21: 	}
22: };
23: 
24: struct TimestampConvert {
25: 	template <class DUCKDB_T, class NUMPY_T> static int64_t convert_value(timestamp_t val) {
26: 		return Date::Epoch(Timestamp::GetDate(val)) * 1000 + (int64_t)(Timestamp::GetTime(val));
27: 	}
28: };
29: 
30: struct DateConvert {
31: 	template <class DUCKDB_T, class NUMPY_T> static int64_t convert_value(date_t val) {
32: 		return Date::Epoch(val);
33: 	}
34: };
35: 
36: struct StringConvert {
37: 	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(string_t val) {
38: 		return py::str(val.GetData());
39: 	}
40: };
41: 
42: template <class DUCKDB_T, class NUMPY_T, class CONVERT>
43: static py::array fetch_column(string numpy_type, ChunkCollection &collection, idx_t column) {
44: 	auto out = py::array(py::dtype(numpy_type), collection.count);
45: 	auto out_ptr = (NUMPY_T *)out.mutable_data();
46: 
47: 	idx_t out_offset = 0;
48: 	for (auto &data_chunk : collection.chunks) {
49: 		auto &src = data_chunk->data[column];
50: 		auto src_ptr = FlatVector::GetData<DUCKDB_T>(src);
51: 		auto &nullmask = FlatVector::Nullmask(src);
52: 		for (idx_t i = 0; i < data_chunk->size(); i++) {
53: 			if (nullmask[i]) {
54: 				continue;
55: 			}
56: 			out_ptr[i + out_offset] = CONVERT::template convert_value<DUCKDB_T, NUMPY_T>(src_ptr[i]);
57: 		}
58: 		out_offset += data_chunk->size();
59: 	}
60: 	return out;
61: }
62: 
63: template <class T> static py::array fetch_column_regular(string numpy_type, ChunkCollection &collection, idx_t column) {
64: 	return fetch_column<T, T, RegularConvert>(numpy_type, collection, column);
65: }
66: 
67: }; // namespace duckdb_py_convert
68: // namespace duckdb_py_convert
69: 
70: namespace random_string {
71: static std::random_device rd;
72: static std::mt19937 gen(rd());
73: static std::uniform_int_distribution<> dis(0, 15);
74: 
75: std::string generate() {
76: 	std::stringstream ss;
77: 	int i;
78: 	ss << std::hex;
79: 	for (i = 0; i < 16; i++) {
80: 		ss << dis(gen);
81: 	}
82: 	return ss.str();
83: }
84: } // namespace random_string
85: 
86: struct PandasScanFunctionData : public TableFunctionData {
87: 	PandasScanFunctionData(py::handle df, idx_t row_count, vector<SQLType> sql_types)
88: 	    : df(df), row_count(row_count), sql_types(sql_types), position(0) {
89: 	}
90: 	py::handle df;
91: 	idx_t row_count;
92: 	vector<SQLType> sql_types;
93: 	idx_t position;
94: };
95: 
96: struct PandasScanFunction : public TableFunction {
97: 	PandasScanFunction()
98: 	    : TableFunction("pandas_scan", {SQLType::VARCHAR}, pandas_scan_bind, pandas_scan_function, nullptr){};
99: 
100: 	static unique_ptr<FunctionData> pandas_scan_bind(ClientContext &context, vector<Value> inputs,
101: 	                                                 vector<SQLType> &return_types, vector<string> &names) {
102: 		// Hey, it works (TM)
103: 		py::handle df((PyObject *)std::stoull(inputs[0].GetValue<string>(), nullptr, 16));
104: 
105: 		/* TODO this fails on Python2 for some reason
106: 		auto pandas_mod = py::module::import("pandas.core.frame");
107: 		auto df_class = pandas_mod.attr("DataFrame");
108: 
109: 		if (!df.get_type().is(df_class)) {
110: 		    throw Exception("parameter is not a DataFrame");
111: 		} */
112: 
113: 		auto df_names = py::list(df.attr("columns"));
114: 		auto df_types = py::list(df.attr("dtypes"));
115: 		// TODO support masked arrays as well
116: 		// TODO support dicts of numpy arrays as well
117: 		if (py::len(df_names) == 0 || py::len(df_types) == 0 || py::len(df_names) != py::len(df_types)) {
118: 			throw runtime_error("Need a DataFrame with at least one column");
119: 		}
120: 		for (idx_t col_idx = 0; col_idx < py::len(df_names); col_idx++) {
121: 			auto col_type = string(py::str(df_types[col_idx]));
122: 			names.push_back(string(py::str(df_names[col_idx])));
123: 			SQLType duckdb_col_type;
124: 			if (col_type == "bool") {
125: 				duckdb_col_type = SQLType::BOOLEAN;
126: 			} else if (col_type == "int8") {
127: 				duckdb_col_type = SQLType::TINYINT;
128: 			} else if (col_type == "int16") {
129: 				duckdb_col_type = SQLType::SMALLINT;
130: 			} else if (col_type == "int32") {
131: 				duckdb_col_type = SQLType::INTEGER;
132: 			} else if (col_type == "int64") {
133: 				duckdb_col_type = SQLType::BIGINT;
134: 			} else if (col_type == "float32") {
135: 				duckdb_col_type = SQLType::FLOAT;
136: 			} else if (col_type == "float64") {
137: 				duckdb_col_type = SQLType::DOUBLE;
138: 			} else if (col_type == "datetime64[ns]") {
139: 				duckdb_col_type = SQLType::TIMESTAMP;
140: 			} else if (col_type == "object") {
141: 				// this better be strings
142: 				duckdb_col_type = SQLType::VARCHAR;
143: 			} else {
144: 				throw runtime_error("unsupported python type " + col_type);
145: 			}
146: 			return_types.push_back(duckdb_col_type);
147: 		}
148: 		idx_t row_count = py::len(df.attr("__getitem__")(df_names[0]));
149: 		return make_unique<PandasScanFunctionData>(df, row_count, return_types);
150: 	}
151: 
152: 	template <class T> static void scan_pandas_column(py::array numpy_col, idx_t count, idx_t offset, Vector &out) {
153: 
154: 		auto src_ptr = (T *)numpy_col.data();
155: 		auto tgt_ptr = FlatVector::GetData<T>(out);
156: 
157: 		for (idx_t row = 0; row < count; row++) {
158: 			tgt_ptr[row] = src_ptr[row + offset];
159: 		}
160: 	}
161: 
162: 	static void pandas_scan_function(ClientContext &context, vector<Value> &input, DataChunk &output,
163: 	                                 FunctionData *dataptr) {
164: 		auto &data = *((PandasScanFunctionData *)dataptr);
165: 
166: 		if (data.position >= data.row_count) {
167: 			return;
168: 		}
169: 		idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, data.row_count - data.position);
170: 
171: 		auto df_names = py::list(data.df.attr("columns"));
172: 		auto get_fun = data.df.attr("__getitem__");
173: 
174: 		output.SetCardinality(this_count);
175: 		for (idx_t col_idx = 0; col_idx < output.column_count(); col_idx++) {
176: 			auto numpy_col = py::array(get_fun(df_names[col_idx]).attr("to_numpy")());
177: 
178: 			switch (data.sql_types[col_idx].id) {
179: 			case SQLTypeId::BOOLEAN:
180: 				scan_pandas_column<bool>(numpy_col, this_count, data.position, output.data[col_idx]);
181: 				break;
182: 			case SQLTypeId::TINYINT:
183: 				scan_pandas_column<int8_t>(numpy_col, this_count, data.position, output.data[col_idx]);
184: 				break;
185: 			case SQLTypeId::SMALLINT:
186: 				scan_pandas_column<int16_t>(numpy_col, this_count, data.position, output.data[col_idx]);
187: 				break;
188: 			case SQLTypeId::INTEGER:
189: 				scan_pandas_column<int32_t>(numpy_col, this_count, data.position, output.data[col_idx]);
190: 				break;
191: 			case SQLTypeId::BIGINT:
192: 				scan_pandas_column<int64_t>(numpy_col, this_count, data.position, output.data[col_idx]);
193: 				break;
194: 			case SQLTypeId::FLOAT:
195: 				scan_pandas_column<float>(numpy_col, this_count, data.position, output.data[col_idx]);
196: 				break;
197: 			case SQLTypeId::DOUBLE:
198: 				scan_pandas_column<double>(numpy_col, this_count, data.position, output.data[col_idx]);
199: 				break;
200: 			case SQLTypeId::TIMESTAMP: {
201: 				auto src_ptr = (int64_t *)numpy_col.data();
202: 				auto tgt_ptr = (timestamp_t *)FlatVector::GetData(output.data[col_idx]);
203: 
204: 				for (idx_t row = 0; row < this_count; row++) {
205: 					auto ms = src_ptr[row] / 1000000; // nanoseconds
206: 					auto ms_per_day = (int64_t)60 * 60 * 24 * 1000;
207: 					date_t date = Date::EpochToDate(ms / 1000);
208: 					dtime_t time = (dtime_t)(ms % ms_per_day);
209: 					tgt_ptr[row] = Timestamp::FromDatetime(date, time);
210: 				}
211: 				break;
212: 			} break;
213: 			case SQLTypeId::VARCHAR: {
214: 				auto src_ptr = (py::object *)numpy_col.data();
215: 				auto tgt_ptr = (string_t *)FlatVector::GetData(output.data[col_idx]);
216: 
217: 				for (idx_t row = 0; row < this_count; row++) {
218: 					auto val = src_ptr[row + data.position];
219: 
220: 					if (!py::isinstance<py::str>(val)) {
221: 						FlatVector::SetNull(output.data[col_idx], row, true);
222: 						continue;
223: 					}
224: 					tgt_ptr[row] = StringVector::AddString(output.data[col_idx], val.cast<string>());
225: 				}
226: 				break;
227: 			}
228: 			default:
229: 				throw runtime_error("Unsupported type " + SQLTypeToString(data.sql_types[col_idx]));
230: 			}
231: 		}
232: 		data.position += this_count;
233: 	}
234: };
235: 
236: struct DuckDBPyResult {
237: 
238: 	template <class SRC> static SRC fetch_scalar(Vector &src_vec, idx_t offset) {
239: 		auto src_ptr = FlatVector::GetData<SRC>(src_vec);
240: 		return src_ptr[offset];
241: 	}
242: 
243: 	py::object fetchone() {
244: 		if (!result) {
245: 			throw runtime_error("result closed");
246: 		}
247: 		if (!current_chunk || chunk_offset >= current_chunk->size()) {
248: 			current_chunk = result->Fetch();
249: 			chunk_offset = 0;
250: 		}
251: 		if (current_chunk->size() == 0) {
252: 			return py::none();
253: 		}
254: 		py::tuple res(result->types.size());
255: 
256: 		for (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {
257: 			auto &nullmask = FlatVector::Nullmask(current_chunk->data[col_idx]);
258: 			if (nullmask[chunk_offset]) {
259: 				res[col_idx] = py::none();
260: 				continue;
261: 			}
262: 			auto val = current_chunk->data[col_idx].GetValue(chunk_offset);
263: 			switch (result->sql_types[col_idx].id) {
264: 			case SQLTypeId::BOOLEAN:
265: 				res[col_idx] = val.GetValue<bool>();
266: 				break;
267: 			case SQLTypeId::TINYINT:
268: 				res[col_idx] = val.GetValue<int8_t>();
269: 				break;
270: 			case SQLTypeId::SMALLINT:
271: 				res[col_idx] = val.GetValue<int16_t>();
272: 				break;
273: 			case SQLTypeId::INTEGER:
274: 				res[col_idx] = val.GetValue<int32_t>();
275: 				break;
276: 			case SQLTypeId::BIGINT:
277: 				res[col_idx] = val.GetValue<int64_t>();
278: 				break;
279: 			case SQLTypeId::FLOAT:
280: 				res[col_idx] = val.GetValue<float>();
281: 				break;
282: 			case SQLTypeId::DOUBLE:
283: 				res[col_idx] = val.GetValue<double>();
284: 				break;
285: 			case SQLTypeId::VARCHAR:
286: 				res[col_idx] = val.GetValue<string>();
287: 				break;
288: 
289: 			case SQLTypeId::TIMESTAMP: {
290: 				if (result->types[col_idx] != TypeId::INT64) {
291: 					throw runtime_error("expected int64 for timestamp");
292: 				}
293: 				auto timestamp = val.GetValue<int64_t>();
294: 				auto date = Timestamp::GetDate(timestamp);
295: 				res[col_idx] = PyDateTime_FromDateAndTime(
296: 				    Date::ExtractYear(date), Date::ExtractMonth(date), Date::ExtractDay(date),
297: 				    Timestamp::GetHours(timestamp), Timestamp::GetMinutes(timestamp), Timestamp::GetSeconds(timestamp),
298: 				    Timestamp::GetMilliseconds(timestamp) * 1000 - Timestamp::GetSeconds(timestamp) * 1000000);
299: 
300: 				break;
301: 			}
302: 
303: 			case SQLTypeId::DATE: {
304: 				if (result->types[col_idx] != TypeId::INT32) {
305: 					throw runtime_error("expected int32 for date");
306: 				}
307: 				auto date = val.GetValue<int32_t>();
308: 				res[col_idx] = PyDate_FromDate(duckdb::Date::ExtractYear(date), duckdb::Date::ExtractMonth(date),
309: 				                               duckdb::Date::ExtractDay(date));
310: 				break;
311: 			}
312: 
313: 			default:
314: 				throw runtime_error("unsupported type: " + SQLTypeToString(result->sql_types[col_idx]));
315: 			}
316: 		}
317: 		chunk_offset++;
318: 		return move(res);
319: 	}
320: 
321: 	py::list fetchall() {
322: 		py::list res;
323: 		while (true) {
324: 			auto fres = fetchone();
325: 			if (fres.is_none()) {
326: 				break;
327: 			}
328: 			res.append(fres);
329: 		}
330: 		return res;
331: 	}
332: 
333: 	py::dict fetchnumpy() {
334: 		if (!result) {
335: 			throw runtime_error("result closed");
336: 		}
337: 		// need to materialize the result if it was streamed because we need the count :/
338: 		MaterializedQueryResult *mres = nullptr;
339: 		unique_ptr<QueryResult> mat_res_holder;
340: 		if (result->type == QueryResultType::STREAM_RESULT) {
341: 			mat_res_holder = ((StreamQueryResult *)result.get())->Materialize();
342: 			mres = (MaterializedQueryResult *)mat_res_holder.get();
343: 		} else {
344: 			mres = (MaterializedQueryResult *)result.get();
345: 		}
346: 		assert(mres);
347: 
348: 		py::dict res;
349: 		for (idx_t col_idx = 0; col_idx < mres->types.size(); col_idx++) {
350: 			// convert the actual payload
351: 			py::array col_res;
352: 			switch (mres->sql_types[col_idx].id) {
353: 			case SQLTypeId::BOOLEAN:
354: 				col_res = duckdb_py_convert::fetch_column_regular<bool>("bool", mres->collection, col_idx);
355: 				break;
356: 			case SQLTypeId::TINYINT:
357: 				col_res = duckdb_py_convert::fetch_column_regular<int8_t>("int8", mres->collection, col_idx);
358: 				break;
359: 			case SQLTypeId::SMALLINT:
360: 				col_res = duckdb_py_convert::fetch_column_regular<int16_t>("int16", mres->collection, col_idx);
361: 				break;
362: 			case SQLTypeId::INTEGER:
363: 				col_res = duckdb_py_convert::fetch_column_regular<int32_t>("int32", mres->collection, col_idx);
364: 				break;
365: 			case SQLTypeId::BIGINT:
366: 				col_res = duckdb_py_convert::fetch_column_regular<int64_t>("int64", mres->collection, col_idx);
367: 				break;
368: 			case SQLTypeId::FLOAT:
369: 				col_res = duckdb_py_convert::fetch_column_regular<float>("float32", mres->collection, col_idx);
370: 				break;
371: 			case SQLTypeId::DOUBLE:
372: 				col_res = duckdb_py_convert::fetch_column_regular<double>("float64", mres->collection, col_idx);
373: 				break;
374: 			case SQLTypeId::TIMESTAMP:
375: 				col_res = duckdb_py_convert::fetch_column<timestamp_t, int64_t, duckdb_py_convert::TimestampConvert>(
376: 				    "datetime64[ms]", mres->collection, col_idx);
377: 				break;
378: 			case SQLTypeId::DATE:
379: 				col_res = duckdb_py_convert::fetch_column<date_t, int64_t, duckdb_py_convert::DateConvert>(
380: 				    "datetime64[s]", mres->collection, col_idx);
381: 				break;
382: 
383: 			case SQLTypeId::VARCHAR:
384: 				col_res = duckdb_py_convert::fetch_column<string_t, py::str, duckdb_py_convert::StringConvert>(
385: 				    "object", mres->collection, col_idx);
386: 				break;
387: 			default:
388: 				throw runtime_error("unsupported type " + SQLTypeToString(mres->sql_types[col_idx]));
389: 			}
390: 
391: 			// convert the nullmask
392: 			py::array_t<bool> nullmask;
393: 			nullmask.resize({mres->collection.count});
394: 			bool *nullmask_ptr = nullmask.mutable_data();
395: 
396: 			idx_t out_offset = 0;
397: 			for (auto &data_chunk : mres->collection.chunks) {
398: 				auto &src_nm = FlatVector::Nullmask(data_chunk->data[col_idx]);
399: 				for (idx_t i = 0; i < data_chunk->size(); i++) {
400: 					nullmask_ptr[i + out_offset] = src_nm[i];
401: 				}
402: 				out_offset += data_chunk->size();
403: 			}
404: 
405: 			// create masked array and assign to output
406: 			auto masked_array = py::module::import("numpy.ma").attr("masked_array")(col_res, nullmask);
407: 			res[mres->names[col_idx].c_str()] = masked_array;
408: 		}
409: 		return res;
410: 	}
411: 
412: 	py::object fetchdf() {
413: 		return py::module::import("pandas").attr("DataFrame").attr("from_dict")(fetchnumpy());
414: 	}
415: 
416: 	py::list description() {
417: 		py::list desc(result->names.size());
418: 		for (idx_t col_idx = 0; col_idx < result->names.size(); col_idx++) {
419: 			py::tuple col_desc(7);
420: 			col_desc[0] = py::str(result->names[col_idx]);
421: 			col_desc[1] = py::none();
422: 			col_desc[2] = py::none();
423: 			col_desc[3] = py::none();
424: 			col_desc[4] = py::none();
425: 			col_desc[5] = py::none();
426: 			col_desc[6] = py::none();
427: 			desc[col_idx] = col_desc;
428: 		}
429: 		return desc;
430: 	}
431: 
432: 	void close() {
433: 		result = nullptr;
434: 	}
435: 	idx_t chunk_offset = 0;
436: 
437: 	unique_ptr<QueryResult> result;
438: 	unique_ptr<DataChunk> current_chunk;
439: };
440: 
441: struct DuckDBPyRelation;
442: 
443: struct DuckDBPyConnection {
444: 
445: 	DuckDBPyConnection *executemany(string query, py::object params = py::list()) {
446: 		execute(query, params, true);
447: 		return this;
448: 	}
449: 
450: 	~DuckDBPyConnection() {
451: 		for (auto &element : registered_dfs) {
452: 			unregister_df(element.first);
453: 		}
454: 	}
455: 
456: 	DuckDBPyConnection *execute(string query, py::object params = py::list(), bool many = false) {
457: 		if (!connection) {
458: 			throw runtime_error("connection closed");
459: 		}
460: 		result = nullptr;
461: 
462: 		auto prep = connection->Prepare(query);
463: 		if (!prep->success) {
464: 			throw runtime_error(prep->error);
465: 		}
466: 
467: 		// this is a list of a list of parameters in executemany
468: 		py::list params_set;
469: 		if (!many) {
470: 			params_set = py::list(1);
471: 			params_set[0] = params;
472: 		} else {
473: 			params_set = params;
474: 		}
475: 
476: 		for (auto &single_query_params : params_set) {
477: 			if (prep->n_param != py::len(single_query_params)) {
478: 				throw runtime_error("Prepared statments needs " + to_string(prep->n_param) + " parameters, " +
479: 				                    to_string(py::len(single_query_params)) + " given");
480: 			}
481: 			auto args = DuckDBPyConnection::transform_python_param_list(single_query_params);
482: 			auto res = make_unique<DuckDBPyResult>();
483: 			res->result = prep->Execute(args);
484: 			if (!res->result->success) {
485: 				throw runtime_error(res->result->error);
486: 			}
487: 			if (!many) {
488: 				result = move(res);
489: 			}
490: 		}
491: 		return this;
492: 	}
493: 
494: 	DuckDBPyConnection *append(string name, py::object value) {
495: 		register_df("__append_df", value);
496: 		return execute("INSERT INTO \"" + name + "\" SELECT * FROM __append_df");
497: 	}
498: 
499: 	static string ptr_to_string(void const *ptr) {
500: 		std::ostringstream address;
501: 		address << ptr;
502: 		return address.str();
503: 	}
504: 
505: 	DuckDBPyConnection *register_df(string name, py::object value) {
506: 		// hack alert: put the pointer address into the function call as a string
507: 		execute("CREATE OR REPLACE VIEW \"" + name + "\" AS SELECT * FROM pandas_scan('" + ptr_to_string(value.ptr()) +
508: 		        "')");
509: 
510: 		// try to bind
511: 		execute("SELECT * FROM \"" + name + "\" WHERE FALSE");
512: 
513: 		// keep a reference
514: 		registered_dfs[name] = value;
515: 		return this;
516: 	}
517: 
518: 	unique_ptr<DuckDBPyRelation> table(string tname) {
519: 		if (!connection) {
520: 			throw runtime_error("connection closed");
521: 		}
522: 		return make_unique<DuckDBPyRelation>(connection->Table(tname));
523: 	}
524: 
525: 	unique_ptr<DuckDBPyRelation> view(string vname) {
526: 		if (!connection) {
527: 			throw runtime_error("connection closed");
528: 		}
529: 		return make_unique<DuckDBPyRelation>(connection->View(vname));
530: 	}
531: 
532: 	unique_ptr<DuckDBPyRelation> table_function(string fname, py::object params = py::list()) {
533: 		if (!connection) {
534: 			throw runtime_error("connection closed");
535: 		}
536: 
537: 		return make_unique<DuckDBPyRelation>(
538: 		    connection->TableFunction(fname, DuckDBPyConnection::transform_python_param_list(params)));
539: 	}
540: 
541: 	unique_ptr<DuckDBPyRelation> from_df(py::object value) {
542: 		if (!connection) {
543: 			throw runtime_error("connection closed");
544: 		};
545: 		string name = "df_" + random_string::generate();
546: 		registered_dfs[name] = value;
547: 		vector<Value> params;
548: 		params.push_back(Value(ptr_to_string(value.ptr())));
549: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("pandas_scan", params)->Alias(name));
550: 	}
551: 
552: 	unique_ptr<DuckDBPyRelation> from_csv_auto(string filename) {
553: 		if (!connection) {
554: 			throw runtime_error("connection closed");
555: 		};
556: 		vector<Value> params;
557: 		params.push_back(Value(filename));
558: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("read_csv_auto", params)->Alias(filename));
559: 	}
560: 
561: 	DuckDBPyConnection *unregister_df(string name) {
562: 		registered_dfs[name] = py::none();
563: 		return this;
564: 	}
565: 
566: 	DuckDBPyConnection *begin() {
567: 		execute("BEGIN TRANSACTION");
568: 		return this;
569: 	}
570: 
571: 	DuckDBPyConnection *commit() {
572: 		if (connection->context->transaction.IsAutoCommit()) {
573: 			return this;
574: 		}
575: 		execute("COMMIT");
576: 		return this;
577: 	}
578: 
579: 	DuckDBPyConnection *rollback() {
580: 		execute("ROLLBACK");
581: 		return this;
582: 	}
583: 
584: 	py::object getattr(py::str key) {
585: 		if (key.cast<string>() == "description") {
586: 			if (!result) {
587: 				throw runtime_error("no open result set");
588: 			}
589: 			return result->description();
590: 		}
591: 		return py::none();
592: 	}
593: 
594: 	void close() {
595: 		connection = nullptr;
596: 		database = nullptr;
597: 	}
598: 
599: 	// cursor() is stupid
600: 	unique_ptr<DuckDBPyConnection> cursor() {
601: 		auto res = make_unique<DuckDBPyConnection>();
602: 		res->database = database;
603: 		res->connection = make_unique<Connection>(*res->database);
604: 		return res;
605: 	}
606: 
607: 	// these should be functions on the result but well
608: 	py::tuple fetchone() {
609: 		if (!result) {
610: 			throw runtime_error("no open result set");
611: 		}
612: 		return result->fetchone();
613: 	}
614: 
615: 	py::list fetchall() {
616: 		if (!result) {
617: 			throw runtime_error("no open result set");
618: 		}
619: 		return result->fetchall();
620: 	}
621: 
622: 	py::dict fetchnumpy() {
623: 		if (!result) {
624: 			throw runtime_error("no open result set");
625: 		}
626: 		return result->fetchnumpy();
627: 	}
628: 	py::object fetchdf() {
629: 		if (!result) {
630: 			throw runtime_error("no open result set");
631: 		}
632: 		return result->fetchdf();
633: 	}
634: 
635: 	static unique_ptr<DuckDBPyConnection> connect(string database, bool read_only) {
636: 		auto res = make_unique<DuckDBPyConnection>();
637: 		DBConfig config;
638: 		if (read_only)
639: 			config.access_mode = AccessMode::READ_ONLY;
640: 		res->database = make_unique<DuckDB>(database, &config);
641: 		res->connection = make_unique<Connection>(*res->database);
642: 
643: 		PandasScanFunction scan_fun;
644: 		CreateTableFunctionInfo info(scan_fun);
645: 
646: 		auto &context = *res->connection->context;
647: 		context.transaction.BeginTransaction();
648: 		context.catalog.CreateTableFunction(context, &info);
649: 		context.transaction.Commit();
650: 
651: 		if (!read_only) {
652: 			res->connection->Query("CREATE OR REPLACE VIEW sqlite_master AS SELECT * FROM sqlite_master()");
653: 		}
654: 
655: 		return res;
656: 	}
657: 
658: 	shared_ptr<DuckDB> database;
659: 	unique_ptr<Connection> connection;
660: 	unordered_map<string, py::object> registered_dfs;
661: 	unique_ptr<DuckDBPyResult> result;
662: 
663: 	static vector<Value> transform_python_param_list(py::handle params) {
664: 		vector<Value> args;
665: 
666: 		auto datetime_mod = py::module::import("datetime");
667: 		auto datetime_date = datetime_mod.attr("datetime");
668: 		auto datetime_datetime = datetime_mod.attr("date");
669: 
670: 		for (auto &ele : params) {
671: 			if (ele.is_none()) {
672: 				args.push_back(Value());
673: 			} else if (py::isinstance<py::bool_>(ele)) {
674: 				args.push_back(Value::BOOLEAN(ele.cast<bool>()));
675: 			} else if (py::isinstance<py::int_>(ele)) {
676: 				args.push_back(Value::BIGINT(ele.cast<int64_t>()));
677: 			} else if (py::isinstance<py::float_>(ele)) {
678: 				args.push_back(Value::DOUBLE(ele.cast<double>()));
679: 			} else if (py::isinstance<py::str>(ele)) {
680: 				args.push_back(Value(ele.cast<string>()));
681: 			} else if (ele.get_type().is(datetime_date)) {
682: 				throw runtime_error("date parameters not supported yet :/");
683: 				// args.push_back(Value::DATE(1984, 4, 24));
684: 			} else if (ele.get_type().is(datetime_datetime)) {
685: 				throw runtime_error("datetime parameters not supported yet :/");
686: 				// args.push_back(Value::TIMESTAMP(1984, 4, 24, 14, 42, 0, 0));
687: 			} else {
688: 				throw runtime_error("unknown param type " + py::str(ele.get_type()).cast<string>());
689: 			}
690: 		}
691: 		return args;
692: 	}
693: };
694: 
695: static unique_ptr<DuckDBPyConnection> default_connection_ = nullptr;
696: 
697: static DuckDBPyConnection *default_connection() {
698: 	if (!default_connection_) {
699: 		default_connection_ = DuckDBPyConnection::connect(":memory:", false);
700: 	}
701: 	return default_connection_.get();
702: }
703: 
704: struct DuckDBPyRelation {
705: 
706: 	DuckDBPyRelation(shared_ptr<Relation> rel) : rel(rel) {
707: 	}
708: 
709: 	static unique_ptr<DuckDBPyRelation> from_df(py::object df) {
710: 		return default_connection()->from_df(df);
711: 	}
712: 
713: 	static unique_ptr<DuckDBPyRelation> from_csv_auto(string filename) {
714: 		return default_connection()->from_csv_auto(filename);
715: 	}
716: 
717: 	unique_ptr<DuckDBPyRelation> project(string expr) {
718: 		return make_unique<DuckDBPyRelation>(rel->Project(expr));
719: 	}
720: 
721: 	static unique_ptr<DuckDBPyRelation> project_df(py::object df, string expr) {
722: 		return default_connection()->from_df(df)->project(expr);
723: 	}
724: 
725: 	unique_ptr<DuckDBPyRelation> alias(string expr) {
726: 		return make_unique<DuckDBPyRelation>(rel->Alias(expr));
727: 	}
728: 
729: 	static unique_ptr<DuckDBPyRelation> alias_df(py::object df, string expr) {
730: 		return default_connection()->from_df(df)->alias(expr);
731: 	}
732: 
733: 	unique_ptr<DuckDBPyRelation> filter(string expr) {
734: 		return make_unique<DuckDBPyRelation>(rel->Filter(expr));
735: 	}
736: 
737: 	static unique_ptr<DuckDBPyRelation> filter_df(py::object df, string expr) {
738: 		return default_connection()->from_df(df)->filter(expr);
739: 	}
740: 
741: 	unique_ptr<DuckDBPyRelation> limit(int64_t n) {
742: 		return make_unique<DuckDBPyRelation>(rel->Limit(n));
743: 	}
744: 
745: 	static unique_ptr<DuckDBPyRelation> limit_df(py::object df, int64_t n) {
746: 		return default_connection()->from_df(df)->limit(n);
747: 	}
748: 
749: 	unique_ptr<DuckDBPyRelation> order(string expr) {
750: 		return make_unique<DuckDBPyRelation>(rel->Order(expr));
751: 	}
752: 
753: 	static unique_ptr<DuckDBPyRelation> order_df(py::object df, string expr) {
754: 		return default_connection()->from_df(df)->order(expr);
755: 	}
756: 
757: 	unique_ptr<DuckDBPyRelation> aggregate(string expr, string groups = "") {
758: 		if (groups.size() > 0) {
759: 			return make_unique<DuckDBPyRelation>(rel->Aggregate(expr, groups));
760: 		}
761: 		return make_unique<DuckDBPyRelation>(rel->Aggregate(expr));
762: 	}
763: 
764: 	static unique_ptr<DuckDBPyRelation> aggregate_df(py::object df, string expr, string groups = "") {
765: 		return default_connection()->from_df(df)->aggregate(expr, groups);
766: 	}
767: 
768: 	unique_ptr<DuckDBPyRelation> distinct() {
769: 		return make_unique<DuckDBPyRelation>(rel->Distinct());
770: 	}
771: 
772: 	static unique_ptr<DuckDBPyRelation> distinct_df(py::object df) {
773: 		return default_connection()->from_df(df)->distinct();
774: 	}
775: 
776: 	py::object to_df() {
777: 		auto res = make_unique<DuckDBPyResult>();
778: 		res->result = rel->Execute();
779: 		if (!res->result->success) {
780: 			throw runtime_error(res->result->error);
781: 		}
782: 		return res->fetchdf();
783: 	}
784: 
785: 	unique_ptr<DuckDBPyRelation> union_(DuckDBPyRelation *other) {
786: 		return make_unique<DuckDBPyRelation>(rel->Union(other->rel));
787: 	}
788: 
789: 	unique_ptr<DuckDBPyRelation> except(DuckDBPyRelation *other) {
790: 		return make_unique<DuckDBPyRelation>(rel->Except(other->rel));
791: 	}
792: 
793: 	unique_ptr<DuckDBPyRelation> intersect(DuckDBPyRelation *other) {
794: 		return make_unique<DuckDBPyRelation>(rel->Intersect(other->rel));
795: 	}
796: 
797: 	unique_ptr<DuckDBPyRelation> join(DuckDBPyRelation *other, string condition) {
798: 		return make_unique<DuckDBPyRelation>(rel->Join(other->rel, condition));
799: 	}
800: 
801: 	void write_csv(string file) {
802: 		rel->WriteCSV(file);
803: 	}
804: 
805: 	static void write_csv_df(py::object df, string file) {
806: 		return default_connection()->from_df(df)->write_csv(file);
807: 	}
808: 
809: 	// should this return a rel with the new view?
810: 	unique_ptr<DuckDBPyRelation> create_view(string view_name, bool replace = true) {
811: 		rel->CreateView(view_name, replace);
812: 		return make_unique<DuckDBPyRelation>(rel);
813: 	}
814: 
815: 	static unique_ptr<DuckDBPyRelation> create_view_df(py::object df, string view_name, bool replace = true) {
816: 		return default_connection()->from_df(df)->create_view(view_name, replace);
817: 	}
818: 
819: 	unique_ptr<DuckDBPyResult> query(string view_name, string sql_query) {
820: 		auto res = make_unique<DuckDBPyResult>();
821: 		res->result = rel->Query(view_name, sql_query);
822: 		if (!res->result->success) {
823: 			throw runtime_error(res->result->error);
824: 		}
825: 		return res;
826: 	}
827: 
828: 	unique_ptr<DuckDBPyResult> execute() {
829: 		auto res = make_unique<DuckDBPyResult>();
830: 		res->result = rel->Execute();
831: 		if (!res->result->success) {
832: 			throw runtime_error(res->result->error);
833: 		}
834: 		return res;
835: 	}
836: 
837: 	static unique_ptr<DuckDBPyResult> query_df(py::object df, string view_name, string sql_query) {
838: 		return default_connection()->from_df(df)->query(view_name, sql_query);
839: 	}
840: 
841: 	void insert(string table) {
842: 		rel->Insert(table);
843: 	}
844: 
845: 	void create(string table) {
846: 		rel->Create(table);
847: 	}
848: 
849: 	string print() {
850: 		rel->Print();
851: 		rel->Limit(10)->Execute()->Print();
852: 		return "";
853: 	}
854: 
855: 	py::object getattr(py::str key) {
856: 		auto key_s = key.cast<string>();
857: 		if (key_s == "alias") {
858: 			return py::str(string(rel->GetAlias()));
859: 		} else if (key_s == "type") {
860: 			return py::str(RelationTypeToString(rel->type));
861: 		} else if (key_s == "columns") {
862: 			py::list res;
863: 			for (auto &col : rel->Columns()) {
864: 				res.append(col.name);
865: 			}
866: 			return move(res);
867: 		} else if (key_s == "types" || key_s == "dtypes") {
868: 			py::list res;
869: 			for (auto &col : rel->Columns()) {
870: 				res.append(SQLTypeToString(col.type));
871: 			}
872: 			return move(res);
873: 		}
874: 		return py::none();
875: 	}
876: 
877: 	shared_ptr<Relation> rel;
878: };
879: 
880: PYBIND11_MODULE(duckdb, m) {
881: 	m.def("connect", &DuckDBPyConnection::connect, "some doc string",
882: 	      py::arg("database") = ":memory:", py::arg("read_only") = false);
883: 
884: 	auto conn_class =
885: 	    py::class_<DuckDBPyConnection>(m, "DuckDBPyConnection")
886: 	        .def("cursor", &DuckDBPyConnection::cursor)
887: 	        .def("begin", &DuckDBPyConnection::begin)
888: 	        .def("table", &DuckDBPyConnection::table, "some doc string for table", py::arg("name"))
889: 	        .def("view", &DuckDBPyConnection::view, "some doc string for view", py::arg("name"))
890: 	        .def("table_function", &DuckDBPyConnection::table_function, "some doc string for table_function",
891: 	             py::arg("name"), py::arg("parameters") = py::list())
892: 	        .def("from_df", &DuckDBPyConnection::from_df, "some doc string for from_df", py::arg("value"))
893: 	        .def("from_csv_auto", &DuckDBPyConnection::from_csv_auto, "some doc string for from_csv_auto",
894: 	             py::arg("filename"))
895: 	        .def("df", &DuckDBPyConnection::from_df, "some doc string for df", py::arg("value"))
896: 	        .def("commit", &DuckDBPyConnection::commit)
897: 	        .def("rollback", &DuckDBPyConnection::rollback)
898: 	        .def("execute", &DuckDBPyConnection::execute, "some doc string for execute", py::arg("query"),
899: 	             py::arg("parameters") = py::list(), py::arg("multiple_parameter_sets") = false)
900: 	        .def("executemany", &DuckDBPyConnection::executemany, "some doc string for executemany", py::arg("query"),
901: 	             py::arg("parameters") = py::list())
902: 	        .def("append", &DuckDBPyConnection::append, py::arg("table"), py::arg("value"))
903: 	        .def("register", &DuckDBPyConnection::register_df, py::arg("name"), py::arg("value"))
904: 	        .def("unregister", &DuckDBPyConnection::unregister_df, py::arg("name"))
905: 	        .def("close", &DuckDBPyConnection::close)
906: 	        .def("fetchone", &DuckDBPyConnection::fetchone)
907: 	        .def("fetchall", &DuckDBPyConnection::fetchall)
908: 	        .def("fetchnumpy", &DuckDBPyConnection::fetchnumpy)
909: 	        .def("fetchdf", &DuckDBPyConnection::fetchdf)
910: 	        .def("__getattr__", &DuckDBPyConnection::getattr);
911: 
912: 	py::class_<DuckDBPyResult>(m, "DuckDBPyResult")
913: 	    .def("close", &DuckDBPyResult::close)
914: 	    .def("fetchone", &DuckDBPyResult::fetchone)
915: 	    .def("fetchall", &DuckDBPyResult::fetchall)
916: 	    .def("fetchnumpy", &DuckDBPyResult::fetchnumpy)
917: 	    .def("fetchdf", &DuckDBPyResult::fetchdf)
918: 	    .def("fetch_df", &DuckDBPyResult::fetchdf)
919: 	    .def("df", &DuckDBPyResult::fetchdf);
920: 
921: 	py::class_<DuckDBPyRelation>(m, "DuckDBPyRelation")
922: 	    .def("filter", &DuckDBPyRelation::filter, "some doc string for filter", py::arg("filter_expr"))
923: 	    .def("project", &DuckDBPyRelation::project, "some doc string for project", py::arg("project_expr"))
924: 	    .def("set_alias", &DuckDBPyRelation::alias, "some doc string for alias", py::arg("alias"))
925: 	    .def("order", &DuckDBPyRelation::order, "some doc string for order", py::arg("order_expr"))
926: 	    .def("aggregate", &DuckDBPyRelation::aggregate, "some doc string for aggregate", py::arg("aggr_expr"),
927: 	         py::arg("group_expr") = "")
928: 	    .def("union", &DuckDBPyRelation::union_, "some doc string for union")
929: 	    .def("except_", &DuckDBPyRelation::except, "some doc string for except", py::arg("other_rel"))
930: 	    .def("intersect", &DuckDBPyRelation::intersect, "some doc string for intersect", py::arg("other_rel"))
931: 	    .def("join", &DuckDBPyRelation::join, "some doc string for join", py::arg("other_rel"),
932: 	         py::arg("join_condition"))
933: 	    .def("distinct", &DuckDBPyRelation::distinct, "some doc string for distinct")
934: 	    .def("limit", &DuckDBPyRelation::limit, "some doc string for limit", py::arg("n"))
935: 	    .def("query", &DuckDBPyRelation::query, "some doc string for query", py::arg("virtual_table_name"),
936: 	         py::arg("sql_query"))
937: 	    .def("execute", &DuckDBPyRelation::execute, "some doc string for execute")
938: 	    .def("write_csv", &DuckDBPyRelation::write_csv, "some doc string for write_csv", py::arg("file_name"))
939: 	    .def("insert", &DuckDBPyRelation::insert, "some doc string for insert", py::arg("table_name"))
940: 	    .def("create", &DuckDBPyRelation::create, "some doc string for create", py::arg("table_name"))
941: 	    .def("to_df", &DuckDBPyRelation::to_df, "some doc string for to_df")
942: 	    .def("create_view", &DuckDBPyRelation::create_view, "some doc string for create_view", py::arg("view_name"),
943: 	         py::arg("replace") = true)
944: 	    .def("df", &DuckDBPyRelation::to_df, "some doc string for df")
945: 	    .def("__str__", &DuckDBPyRelation::print, "some doc string for print")
946: 	    .def("__repr__", &DuckDBPyRelation::print, "some doc string for repr")
947: 	    .def("__getattr__", &DuckDBPyRelation::getattr);
948: 
949: 	m.def("from_df", &DuckDBPyRelation::from_df, "some doc string for filter", py::arg("df"));
950: 	m.def("from_csv_auto", &DuckDBPyRelation::from_csv_auto, "some doc string for from_csv_auto", py::arg("filename"));
951: 	m.def("df", &DuckDBPyRelation::from_df, "some doc string for filter", py::arg("df"));
952: 	m.def("filter", &DuckDBPyRelation::filter_df, "some doc string for filter", py::arg("df"), py::arg("filter_expr"));
953: 	m.def("project", &DuckDBPyRelation::project_df, "some doc string for project", py::arg("df"),
954: 	      py::arg("project_expr"));
955: 	m.def("alias", &DuckDBPyRelation::alias_df, "some doc string for alias", py::arg("df"), py::arg("alias"));
956: 	m.def("order", &DuckDBPyRelation::order_df, "some doc string for order", py::arg("df"), py::arg("order_expr"));
957: 	m.def("aggregate", &DuckDBPyRelation::aggregate_df, "some doc string for aggregate", py::arg("df"),
958: 	      py::arg("aggr_expr"), py::arg("group_expr") = "");
959: 	m.def("distinct", &DuckDBPyRelation::distinct_df, "some doc string for distinct", py::arg("df"));
960: 	m.def("limit", &DuckDBPyRelation::limit_df, "some doc string for limit", py::arg("df"), py::arg("n"));
961: 	m.def("query", &DuckDBPyRelation::query_df, "some doc string for query", py::arg("df"),
962: 	      py::arg("virtual_table_name"), py::arg("sql_query"));
963: 	m.def("write_csv", &DuckDBPyRelation::write_csv_df, "some doc string for write_csv", py::arg("df"),
964: 	      py::arg("file_name"));
965: 
966: 	// we need this because otherwise we try to remove registered_dfs on shutdown when python is already dead
967: 	auto clean_default_connection = []() { default_connection_ = nullptr; };
968: 	m.add_object("_clean_default_connection", py::capsule(clean_default_connection));
969: 	PyDateTime_IMPORT;
970: }
[end of tools/pythonpkg/duckdb_python.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: