{
  "repo": "duckdb/duckdb",
  "pull_number": 612,
  "instance_id": "duckdb__duckdb-612",
  "issue_numbers": [
    "609"
  ],
  "base_commit": "1a7e48d7dab3b2cd7f4fb947fe1a74bf419c2d63",
  "patch": "diff --git a/.gitignore b/.gitignore\nindex a2d3308f4ad8..1ef6d63a493e 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -304,4 +304,8 @@ third_party/imdb/data\n benchmark_results/\n duckdb_unittest_tempdir/\n grammar.y.tmp\n-src/amalgamation/\n\\ No newline at end of file\n+src/amalgamation/\n+# single file compile\n+amalgamation.cache\n+dependencies.d\n+deps.s\ndiff --git a/scripts/amalgamation.py b/scripts/amalgamation.py\nindex 46f6282e3ecd..93ba1ca56064 100644\n--- a/scripts/amalgamation.py\n+++ b/scripts/amalgamation.py\n@@ -20,7 +20,7 @@\n utf8proc_include_dir = os.path.join('third_party', 'utf8proc', 'include')\n \n # files included in the amalgamated \"duckdb.hpp\" file\n-main_header_files = [os.path.join(include_dir, 'duckdb.hpp'), os.path.join(include_dir, 'duckdb.h'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'date.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'timestamp.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'appender.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'client_context.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'function.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'table_function.hpp'), os.path.join(include_dir, 'duckdb', 'parser', 'parsed_data', 'create_table_function_info.hpp')]\n+main_header_files = [os.path.join(include_dir, 'duckdb.hpp'), os.path.join(include_dir, 'duckdb.h'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'date.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'timestamp.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'time.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'appender.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'client_context.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'function.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'table_function.hpp'), os.path.join(include_dir, 'duckdb', 'parser', 'parsed_data', 'create_table_function_info.hpp')]\n \n # include paths for where to search for include files during amalgamation\n include_paths = [include_dir, fmt_include_dir, hll_dir, re2_dir, miniz_dir, utf8proc_include_dir, utf8proc_dir, pg_query_include_dir, pg_query_dir]\ndiff --git a/src/catalog/catalog_entry/CMakeLists.txt b/src/catalog/catalog_entry/CMakeLists.txt\nindex a80e8aab9063..eff60dc8e986 100644\n--- a/src/catalog/catalog_entry/CMakeLists.txt\n+++ b/src/catalog/catalog_entry/CMakeLists.txt\n@@ -1,5 +1,6 @@\n add_library_unity(duckdb_catalog_entries\n                   OBJECT\n+                  index_catalog_entry.cpp\n                   schema_catalog_entry.cpp\n                   sequence_catalog_entry.cpp\n                   table_catalog_entry.cpp\ndiff --git a/src/catalog/catalog_entry/index_catalog_entry.cpp b/src/catalog/catalog_entry/index_catalog_entry.cpp\nnew file mode 100644\nindex 000000000000..0c58f1b1af53\n--- /dev/null\n+++ b/src/catalog/catalog_entry/index_catalog_entry.cpp\n@@ -0,0 +1,19 @@\n+#include \"duckdb/catalog/catalog_entry/index_catalog_entry.hpp\"\n+#include \"duckdb/storage/data_table.hpp\"\n+\n+namespace duckdb {\n+\n+IndexCatalogEntry::~IndexCatalogEntry() {\n+\t// remove the associated index from the info\n+\tif (!info || !index) {\n+\t\treturn;\n+\t}\n+\tfor (idx_t i = 0; i < info->indexes.size(); i++) {\n+\t\tif (info->indexes[i].get() == index) {\n+\t\t\tinfo->indexes.erase(info->indexes.begin() + i);\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/catalog/catalog_entry/table_catalog_entry.cpp b/src/catalog/catalog_entry/table_catalog_entry.cpp\nindex 097b50a8c439..323189376e0f 100644\n--- a/src/catalog/catalog_entry/table_catalog_entry.cpp\n+++ b/src/catalog/catalog_entry/table_catalog_entry.cpp\n@@ -10,13 +10,17 @@\n #include \"duckdb/parser/parsed_data/alter_table_info.hpp\"\n #include \"duckdb/planner/constraints/bound_not_null_constraint.hpp\"\n #include \"duckdb/planner/constraints/bound_unique_constraint.hpp\"\n+#include \"duckdb/planner/constraints/bound_check_constraint.hpp\"\n #include \"duckdb/planner/expression/bound_constant_expression.hpp\"\n #include \"duckdb/planner/parsed_data/bound_create_table_info.hpp\"\n #include \"duckdb/storage/storage_manager.hpp\"\n #include \"duckdb/planner/binder.hpp\"\n \n #include \"duckdb/execution/index/art/art.hpp\"\n+#include \"duckdb/parser/expression/columnref_expression.hpp\"\n #include \"duckdb/planner/expression/bound_reference_expression.hpp\"\n+#include \"duckdb/parser/parsed_expression_iterator.hpp\"\n+#include \"duckdb/planner/expression_binder/alter_binder.hpp\"\n \n #include <algorithm>\n \n@@ -59,13 +63,6 @@ TableCatalogEntry::TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schem\n \t\t\t\t}\n \t\t\t\t// create an adaptive radix tree around the expressions\n \t\t\t\tauto art = make_unique<ART>(*storage, column_ids, move(unbound_expressions), true);\n-\n-\t\t\t\tif (unique.is_primary_key) {\n-\t\t\t\t\t// if this is a primary key index, also create a NOT NULL constraint for each of the columns\n-\t\t\t\t\tfor (auto &column_index : unique.keys) {\n-\t\t\t\t\t\tbound_constraints.push_back(make_unique<BoundNotNullConstraint>(column_index));\n-\t\t\t\t\t}\n-\t\t\t\t}\n \t\t\t\tstorage->AddIndex(move(art), bound_expressions);\n \t\t\t}\n \t\t}\n@@ -80,40 +77,11 @@ unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, A\n \tif (info->type != AlterType::ALTER_TABLE) {\n \t\tthrow CatalogException(\"Can only modify table with ALTER TABLE statement\");\n \t}\n-\tif (constraints.size() > 0) {\n-\t\tthrow CatalogException(\"Cannot modify a table with constraints\");\n-\t}\n \tauto table_info = (AlterTableInfo *)info;\n \tswitch (table_info->alter_table_type) {\n \tcase AlterTableType::RENAME_COLUMN: {\n \t\tauto rename_info = (RenameColumnInfo *)table_info;\n-\t\tauto create_info = make_unique<CreateTableInfo>(schema->name, name);\n-\t\tcreate_info->temporary = temporary;\n-\t\tbool found = false;\n-\t\tfor (idx_t i = 0; i < columns.size(); i++) {\n-\t\t\tColumnDefinition copy(columns[i].name, columns[i].type);\n-\t\t\tcopy.oid = columns[i].oid;\n-\t\t\tcopy.default_value = columns[i].default_value ? columns[i].default_value->Copy() : nullptr;\n-\n-\t\t\tcreate_info->columns.push_back(move(copy));\n-\t\t\tif (rename_info->name == columns[i].name) {\n-\t\t\t\tassert(!found);\n-\t\t\t\tcreate_info->columns[i].name = rename_info->new_name;\n-\t\t\t\tfound = true;\n-\t\t\t}\n-\t\t}\n-\t\tif (!found) {\n-\t\t\tthrow CatalogException(\"Table does not have a column with name \\\"%s\\\"\", rename_info->name.c_str());\n-\t\t}\n-\t\tassert(constraints.size() == 0);\n-\t\t// create_info->constraints.resize(constraints.size());\n-\t\t// for (idx_t i = 0; i < constraints.size(); i++) {\n-\t\t// \tcreate_info->constraints[i] = constraints[i]->Copy();\n-\t\t// }\n-\t\tBinder binder(context);\n-\t\tauto bound_create_info = binder.BindCreateTableInfo(move(create_info));\n-\t\treturn make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),\n-\t\t                                      storage);\n+\t\treturn RenameColumn(context, *rename_info);\n \t}\n \tcase AlterTableType::RENAME_TABLE: {\n \t\tauto rename_info = (RenameTableInfo *)table_info;\n@@ -121,9 +89,260 @@ unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, A\n \t\tcopied_table->name = rename_info->new_table_name;\n \t\treturn copied_table;\n \t}\n+\tcase AlterTableType::ADD_COLUMN: {\n+\t\tauto add_info = (AddColumnInfo *)table_info;\n+\t\treturn AddColumn(context, *add_info);\n+\t}\n+\tcase AlterTableType::REMOVE_COLUMN: {\n+\t\tauto remove_info = (RemoveColumnInfo *)table_info;\n+\t\treturn RemoveColumn(context, *remove_info);\n+\t}\n+\tcase AlterTableType::SET_DEFAULT: {\n+\t\tauto set_default_info = (SetDefaultInfo *)table_info;\n+\t\treturn SetDefault(context, *set_default_info);\n+\t}\n+\tcase AlterTableType::ALTER_COLUMN_TYPE: {\n+\t\tauto change_type_info = (ChangeColumnTypeInfo *)table_info;\n+\t\treturn ChangeColumnType(context, *change_type_info);\n+\t}\n \tdefault:\n-\t\tthrow CatalogException(\"Unrecognized alter table type!\");\n+\t\tthrow InternalException(\"Unrecognized alter table type!\");\n+\t}\n+}\n+\n+static void RenameExpression(ParsedExpression &expr, RenameColumnInfo &info) {\n+\tif (expr.type == ExpressionType::COLUMN_REF) {\n+\t\tauto &colref = (ColumnRefExpression &)expr;\n+\t\tif (colref.column_name == info.name) {\n+\t\t\tcolref.column_name = info.new_name;\n+\t\t}\n+\t}\n+\tParsedExpressionIterator::EnumerateChildren(\n+\t    expr, [&](const ParsedExpression &child) { RenameExpression((ParsedExpression &)child, info); });\n+}\n+\n+unique_ptr<CatalogEntry> TableCatalogEntry::RenameColumn(ClientContext &context, RenameColumnInfo &info) {\n+\tauto create_info = make_unique<CreateTableInfo>(schema->name, name);\n+\tcreate_info->temporary = temporary;\n+\tbool found = false;\n+\tfor (idx_t i = 0; i < columns.size(); i++) {\n+\t\tColumnDefinition copy = columns[i].Copy();\n+\n+\t\tcreate_info->columns.push_back(move(copy));\n+\t\tif (info.name == columns[i].name) {\n+\t\t\tassert(!found);\n+\t\t\tcreate_info->columns[i].name = info.new_name;\n+\t\t\tfound = true;\n+\t\t}\n+\t}\n+\tif (!found) {\n+\t\tthrow CatalogException(\"Table does not have a column with name \\\"%s\\\"\", info.name.c_str());\n+\t}\n+\tfor (idx_t c_idx = 0; c_idx < constraints.size(); c_idx++) {\n+\t\tauto copy = constraints[c_idx]->Copy();\n+\t\tswitch (copy->type) {\n+\t\tcase ConstraintType::NOT_NULL:\n+\t\t\t// NOT NULL constraint: no adjustments necessary\n+\t\t\tbreak;\n+\t\tcase ConstraintType::CHECK: {\n+\t\t\t// CHECK constraint: need to rename column references that refer to the renamed column\n+\t\t\tauto &check = (CheckConstraint &)*copy;\n+\t\t\tRenameExpression(*check.expression, info);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ConstraintType::UNIQUE: {\n+\t\t\t// UNIQUE constraint: possibly need to rename columns\n+\t\t\tauto &unique = (UniqueConstraint &)*copy;\n+\t\t\tfor (idx_t i = 0; i < unique.columns.size(); i++) {\n+\t\t\t\tif (unique.columns[i] == info.name) {\n+\t\t\t\t\tunique.columns[i] = info.new_name;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\tthrow CatalogException(\"Unsupported constraint for entry!\");\n+\t\t}\n+\t\tcreate_info->constraints.push_back(move(copy));\n+\t}\n+\tBinder binder(context);\n+\tauto bound_create_info = binder.BindCreateTableInfo(move(create_info));\n+\treturn make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);\n+}\n+\n+unique_ptr<CatalogEntry> TableCatalogEntry::AddColumn(ClientContext &context, AddColumnInfo &info) {\n+\tauto create_info = make_unique<CreateTableInfo>(schema->name, name);\n+\tcreate_info->temporary = temporary;\n+\tfor (idx_t i = 0; i < columns.size(); i++) {\n+\t\tcreate_info->columns.push_back(columns[i].Copy());\n+\t}\n+\tinfo.new_column.oid = columns.size();\n+\tcreate_info->columns.push_back(info.new_column.Copy());\n+\n+\tBinder binder(context);\n+\tauto bound_create_info = binder.BindCreateTableInfo(move(create_info));\n+\tauto new_storage =\n+\t    make_shared<DataTable>(context, *storage, info.new_column, bound_create_info->bound_defaults.back().get());\n+\treturn make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),\n+\t                                      new_storage);\n+}\n+\n+unique_ptr<CatalogEntry> TableCatalogEntry::RemoveColumn(ClientContext &context, RemoveColumnInfo &info) {\n+\tidx_t removed_index = INVALID_INDEX;\n+\tauto create_info = make_unique<CreateTableInfo>(schema->name, name);\n+\tcreate_info->temporary = temporary;\n+\tfor (idx_t i = 0; i < columns.size(); i++) {\n+\t\tif (columns[i].name == info.removed_column) {\n+\t\t\tassert(removed_index == INVALID_INDEX);\n+\t\t\tremoved_index = i;\n+\t\t\tcontinue;\n+\t\t}\n+\t\tcreate_info->columns.push_back(columns[i].Copy());\n+\t}\n+\tif (removed_index == INVALID_INDEX) {\n+\t\tif (!info.if_exists) {\n+\t\t\tthrow CatalogException(\"Table does not have a column with name \\\"%s\\\"\", info.removed_column.c_str());\n+\t\t}\n+\t\treturn nullptr;\n+\t}\n+\tif (create_info->columns.size() == 0) {\n+\t\tthrow CatalogException(\"Cannot drop column: table only has one column remaining!\");\n+\t}\n+\t// handle constraints for the new table\n+\tassert(constraints.size() == bound_constraints.size());\n+\tfor (idx_t constr_idx = 0; constr_idx < constraints.size(); constr_idx++) {\n+\t\tauto &constraint = constraints[constr_idx];\n+\t\tauto &bound_constraint = bound_constraints[constr_idx];\n+\t\tswitch (bound_constraint->type) {\n+\t\tcase ConstraintType::NOT_NULL: {\n+\t\t\tauto &not_null_constraint = (BoundNotNullConstraint &)*bound_constraint;\n+\t\t\tif (not_null_constraint.index != removed_index) {\n+\t\t\t\t// the constraint is not about this column: we need to copy it\n+\t\t\t\t// we might need to shift the index back by one though, to account for the removed column\n+\t\t\t\tidx_t new_index = not_null_constraint.index;\n+\t\t\t\tif (not_null_constraint.index > removed_index) {\n+\t\t\t\t\tnew_index -= 1;\n+\t\t\t\t}\n+\t\t\t\tcreate_info->constraints.push_back(make_unique<NotNullConstraint>(new_index));\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ConstraintType::CHECK: {\n+\t\t\t// CHECK constraint\n+\t\t\tauto &bound_check = (BoundCheckConstraint &)*bound_constraint;\n+\t\t\t// check if the removed column is part of the check constraint\n+\t\t\tif (bound_check.bound_columns.find(removed_index) != bound_check.bound_columns.end()) {\n+\t\t\t\tif (bound_check.bound_columns.size() > 1) {\n+\t\t\t\t\t// CHECK constraint that concerns mult\n+\t\t\t\t\tthrow CatalogException(\n+\t\t\t\t\t    \"Cannot drop column \\\"%s\\\" because there is a CHECK constraint that depends on it\",\n+\t\t\t\t\t    info.removed_column.c_str());\n+\t\t\t\t} else {\n+\t\t\t\t\t// CHECK constraint that ONLY concerns this column, strip the constraint\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\t// check constraint does not concern the removed column: simply re-add it\n+\t\t\t\tcreate_info->constraints.push_back(constraint->Copy());\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ConstraintType::UNIQUE:\n+\t\t\tcreate_info->constraints.push_back(constraint->Copy());\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unsupported constraint for entry!\");\n+\t\t}\n+\t}\n+\n+\tBinder binder(context);\n+\tauto bound_create_info = binder.BindCreateTableInfo(move(create_info));\n+\tauto new_storage = make_shared<DataTable>(context, *storage, removed_index);\n+\treturn make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),\n+\t                                      new_storage);\n+}\n+\n+unique_ptr<CatalogEntry> TableCatalogEntry::SetDefault(ClientContext &context, SetDefaultInfo &info) {\n+\tauto create_info = make_unique<CreateTableInfo>(schema->name, name);\n+\tbool found = false;\n+\tfor (idx_t i = 0; i < columns.size(); i++) {\n+\t\tauto copy = columns[i].Copy();\n+\t\tif (info.column_name == copy.name) {\n+\t\t\t// set the default value of this column\n+\t\t\tcopy.default_value = info.expression ? info.expression->Copy() : nullptr;\n+\t\t\tfound = true;\n+\t\t}\n+\t\tcreate_info->columns.push_back(move(copy));\n+\t}\n+\tif (!found) {\n+\t\tthrow BinderException(\"Table \\\"%s\\\" does not have a column with name \\\"%s\\\"\", info.table.c_str(),\n+\t\t                      info.column_name.c_str());\n \t}\n+\n+\tfor (idx_t i = 0; i < constraints.size(); i++) {\n+\t\tauto constraint = constraints[i]->Copy();\n+\t\tcreate_info->constraints.push_back(move(constraint));\n+\t}\n+\n+\tBinder binder(context);\n+\tauto bound_create_info = binder.BindCreateTableInfo(move(create_info));\n+\treturn make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);\n+}\n+\n+unique_ptr<CatalogEntry> TableCatalogEntry::ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info) {\n+\tauto create_info = make_unique<CreateTableInfo>(schema->name, name);\n+\tidx_t change_idx = INVALID_INDEX;\n+\tfor (idx_t i = 0; i < columns.size(); i++) {\n+\t\tauto copy = columns[i].Copy();\n+\t\tif (info.column_name == copy.name) {\n+\t\t\t// set the default value of this column\n+\t\t\tchange_idx = i;\n+\t\t\tcopy.type = info.target_type;\n+\t\t}\n+\t\tcreate_info->columns.push_back(move(copy));\n+\t}\n+\tif (change_idx == INVALID_INDEX) {\n+\t\tthrow BinderException(\"Table \\\"%s\\\" does not have a column with name \\\"%s\\\"\", info.table.c_str(),\n+\t\t                      info.column_name.c_str());\n+\t}\n+\n+\tfor (idx_t i = 0; i < constraints.size(); i++) {\n+\t\tauto constraint = constraints[i]->Copy();\n+\t\tswitch (constraint->type) {\n+\t\tcase ConstraintType::CHECK: {\n+\t\t\tauto &bound_check = (BoundCheckConstraint &)*bound_constraints[i];\n+\t\t\tif (bound_check.bound_columns.find(change_idx) != bound_check.bound_columns.end()) {\n+\t\t\t\tthrow BinderException(\"Cannot change the type of a column that has a CHECK constraint specified\");\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase ConstraintType::NOT_NULL:\n+\t\t\tbreak;\n+\t\tcase ConstraintType::UNIQUE: {\n+\t\t\tauto &bound_unique = (BoundUniqueConstraint &)*bound_constraints[i];\n+\t\t\tif (bound_unique.keys.find(change_idx) != bound_unique.keys.end()) {\n+\t\t\t\tthrow BinderException(\n+\t\t\t\t    \"Cannot change the type of a column that has a UNIQUE or PRIMARY KEY constraint specified\");\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unsupported constraint for entry!\");\n+\t\t}\n+\t\tcreate_info->constraints.push_back(move(constraint));\n+\t}\n+\n+\tBinder binder(context);\n+\t// bind the specified expression\n+\tvector<column_t> bound_columns;\n+\tAlterBinder expr_binder(binder, context, name, columns, bound_columns, info.target_type);\n+\tauto expression = info.expression->Copy();\n+\tauto bound_expression = expr_binder.Bind(expression);\n+\tauto new_storage =\n+\t    make_shared<DataTable>(context, *storage, change_idx, info.target_type, move(bound_columns), *bound_expression);\n+\n+\tauto bound_create_info = binder.BindCreateTableInfo(move(create_info));\n+\treturn make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),\n+\t                                      new_storage);\n }\n \n ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {\n@@ -160,9 +379,7 @@ void TableCatalogEntry::Serialize(Serializer &serializer) {\n \tassert(columns.size() <= std::numeric_limits<uint32_t>::max());\n \tserializer.Write<uint32_t>((uint32_t)columns.size());\n \tfor (auto &column : columns) {\n-\t\tserializer.WriteString(column.name);\n-\t\tcolumn.type.Serialize(serializer);\n-\t\tserializer.WriteOptional(column.default_value);\n+\t\tcolumn.Serialize(serializer);\n \t}\n \tassert(constraints.size() <= std::numeric_limits<uint32_t>::max());\n \tserializer.Write<uint32_t>((uint32_t)constraints.size());\n@@ -179,10 +396,8 @@ unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source)\n \tauto column_count = source.Read<uint32_t>();\n \n \tfor (uint32_t i = 0; i < column_count; i++) {\n-\t\tauto column_name = source.Read<string>();\n-\t\tauto column_type = SQLType::Deserialize(source);\n-\t\tauto default_value = source.ReadOptional<ParsedExpression>();\n-\t\tinfo->columns.push_back(ColumnDefinition(column_name, column_type, move(default_value)));\n+\t\tauto column = ColumnDefinition::Deserialize(source);\n+\t\tinfo->columns.push_back(move(column));\n \t}\n \tauto constraint_count = source.Read<uint32_t>();\n \n@@ -196,10 +411,7 @@ unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source)\n unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {\n \tauto create_info = make_unique<CreateTableInfo>(schema->name, name);\n \tfor (idx_t i = 0; i < columns.size(); i++) {\n-\t\tColumnDefinition copy(columns[i].name, columns[i].type);\n-\t\tcopy.oid = columns[i].oid;\n-\t\tcopy.default_value = columns[i].default_value ? columns[i].default_value->Copy() : nullptr;\n-\t\tcreate_info->columns.push_back(move(copy));\n+\t\tcreate_info->columns.push_back(columns[i].Copy());\n \t}\n \n \tfor (idx_t i = 0; i < constraints.size(); i++) {\n@@ -211,3 +423,7 @@ unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {\n \tauto bound_create_info = binder.BindCreateTableInfo(move(create_info));\n \treturn make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);\n }\n+\n+void TableCatalogEntry::SetAsRoot() {\n+\tstorage->SetAsRoot();\n+}\ndiff --git a/src/catalog/catalog_set.cpp b/src/catalog/catalog_set.cpp\nindex 419e68d3388e..d6a6b5dd5d20 100644\n--- a/src/catalog/catalog_set.cpp\n+++ b/src/catalog/catalog_set.cpp\n@@ -90,6 +90,10 @@ bool CatalogSet::AlterEntry(ClientContext &context, const string &name, AlterInf\n \t// set the timestamp to the timestamp of the current transaction\n \t// and point it to the updated table node\n \tauto value = current.AlterEntry(context, alter_info);\n+\tif (!value) {\n+\t\t// alter failed, but did not result in an error\n+\t\treturn true;\n+\t}\n \n \t// now transfer all dependencies from the old table to the new table\n \tcatalog.dependency_manager.AlterObject(transaction, data[name].get(), value.get());\n@@ -228,6 +232,7 @@ void CatalogSet::Undo(CatalogEntry *entry) {\n \t} else {\n \t\t// otherwise we need to update the base entry tables\n \t\tauto &name = entry->name;\n+\t\tto_be_removed_node->child->SetAsRoot();\n \t\tdata[name] = move(to_be_removed_node->child);\n \t\tentry->parent = nullptr;\n \t}\ndiff --git a/src/execution/operator/schema/physical_alter.cpp b/src/execution/operator/schema/physical_alter.cpp\nindex bb88c38586ed..ae52d698c36f 100644\n--- a/src/execution/operator/schema/physical_alter.cpp\n+++ b/src/execution/operator/schema/physical_alter.cpp\n@@ -1,10 +1,15 @@\n #include \"duckdb/execution/operator/schema/physical_alter.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n+#include \"duckdb/parser/parsed_data/alter_table_info.hpp\"\n \n-using namespace duckdb;\n using namespace std;\n \n+namespace duckdb {\n+\n void PhysicalAlter::GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) {\n-\tcontext.catalog.AlterTable(context, (AlterTableInfo *)info.get());\n+\tauto table_info = (AlterTableInfo *)info.get();\n+\tcontext.catalog.AlterTable(context, table_info);\n \tstate->finished = true;\n }\n+\n+} // namespace duckdb\ndiff --git a/src/execution/operator/schema/physical_create_index.cpp b/src/execution/operator/schema/physical_create_index.cpp\nindex b442472b9076..bed54e94b995 100644\n--- a/src/execution/operator/schema/physical_create_index.cpp\n+++ b/src/execution/operator/schema/physical_create_index.cpp\n@@ -1,17 +1,13 @@\n #include \"duckdb/execution/operator/schema/physical_create_index.hpp\"\n \n+#include \"duckdb/catalog/catalog_entry/index_catalog_entry.hpp\"\n #include \"duckdb/catalog/catalog_entry/schema_catalog_entry.hpp\"\n #include \"duckdb/catalog/catalog_entry/table_catalog_entry.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n \n-using namespace duckdb;\n using namespace std;\n \n-void PhysicalCreateIndex::CreateARTIndex() {\n-\tauto art = make_unique<ART>(*table.storage, column_ids, move(unbound_expressions), info->unique);\n-\n-\ttable.storage->AddIndex(move(art), expressions);\n-}\n+namespace duckdb {\n \n void PhysicalCreateIndex::GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) {\n \tif (column_ids.size() == 0) {\n@@ -19,25 +15,28 @@ void PhysicalCreateIndex::GetChunkInternal(ClientContext &context, DataChunk &ch\n \t}\n \n \tauto &schema = *table.schema;\n-\tif (!schema.CreateIndex(context, info.get())) {\n-\t\t// index already exists, but error ignored because of CREATE ... IF NOT\n-\t\t// EXISTS\n+\tauto index_entry = (IndexCatalogEntry *)schema.CreateIndex(context, info.get());\n+\tif (!index_entry) {\n+\t\t// index already exists, but error ignored because of IF NOT EXISTS\n \t\treturn;\n \t}\n \n-\t// create the chunk to hold intermediate expression results\n-\n+\tunique_ptr<Index> index;\n \tswitch (info->index_type) {\n \tcase IndexType::ART: {\n-\t\tCreateARTIndex();\n+\t\tindex = make_unique<ART>(*table.storage, column_ids, move(unbound_expressions), info->unique);\n \t\tbreak;\n \t}\n \tdefault:\n \t\tassert(0);\n \t\tthrow NotImplementedException(\"Unimplemented index type\");\n \t}\n+\tindex_entry->index = index.get();\n+\tindex_entry->info = table.storage->info;\n+\ttable.storage->AddIndex(move(index), expressions);\n \n \tchunk.SetCardinality(0);\n-\n \tstate->finished = true;\n }\n+\n+} // namespace duckdb\ndiff --git a/src/function/aggregate/distributive/minmax.cpp b/src/function/aggregate/distributive/minmax.cpp\nindex 9b70853f84de..529393556bd3 100644\n--- a/src/function/aggregate/distributive/minmax.cpp\n+++ b/src/function/aggregate/distributive/minmax.cpp\n@@ -10,79 +10,170 @@ using namespace std;\n \n namespace duckdb {\n \n-struct MinMaxBase : public StandardDistributiveFunction {\n+template <class T> struct min_max_state_t {\n+\tT value;\n+\tbool isset;\n+};\n+\n+template <class OP> static AggregateFunction GetUnaryAggregate(SQLType type) {\n+\tswitch (type.id) {\n+\tcase SQLTypeId::BOOLEAN:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<int8_t>, int8_t, int8_t, OP>(type, type);\n+\tcase SQLTypeId::TINYINT:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<int8_t>, int8_t, int8_t, OP>(type, type);\n+\tcase SQLTypeId::SMALLINT:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<int16_t>, int16_t, int16_t, OP>(type, type);\n+\tcase SQLTypeId::INTEGER:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<int32_t>, int32_t, int32_t, OP>(type, type);\n+\tcase SQLTypeId::BIGINT:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<int64_t>, int64_t, int64_t, OP>(type, type);\n+\tcase SQLTypeId::FLOAT:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<float>, float, float, OP>(type, type);\n+\tcase SQLTypeId::DOUBLE:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<double>, double, double, OP>(type, type);\n+\tcase SQLTypeId::DECIMAL:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<double>, double, double, OP>(type, type);\n+\tcase SQLTypeId::DATE:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<date_t>, date_t, date_t, OP>(type, type);\n+\tcase SQLTypeId::TIMESTAMP:\n+\t\treturn AggregateFunction::UnaryAggregate<min_max_state_t<timestamp_t>, timestamp_t, timestamp_t, OP>(type,\n+\t\t                                                                                                     type);\n+\tdefault:\n+\t\tthrow NotImplementedException(\"Unimplemented type for unary aggregate\");\n+\t}\n+}\n+\n+struct MinMaxBase {\n+\ttemplate <class STATE> static void Initialize(STATE *state) {\n+\t\tstate->isset = false;\n+\t}\n+\n \ttemplate <class INPUT_TYPE, class STATE, class OP>\n \tstatic void ConstantOperation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t count) {\n \t\tassert(!nullmask[0]);\n-\t\tif (IsNullValue<INPUT_TYPE>(*state)) {\n+\t\tif (!state->isset) {\n+\t\t\tstate->isset = true;\n \t\t\tOP::template Assign<INPUT_TYPE, STATE>(state, input[0]);\n \t\t} else {\n \t\t\tOP::template Execute<INPUT_TYPE, STATE>(state, input[0]);\n \t\t}\n \t}\n+\n+\ttemplate <class INPUT_TYPE, class STATE, class OP>\n+\tstatic void Operation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t idx) {\n+\t\tif (!state->isset) {\n+\t\t\tstate->isset = true;\n+\t\t\tOP::template Assign<INPUT_TYPE, STATE>(state, input[idx]);\n+\t\t} else {\n+\t\t\tOP::template Execute<INPUT_TYPE, STATE>(state, input[idx]);\n+\t\t}\n+\t}\n+\n+\tstatic bool IgnoreNull() {\n+\t\treturn true;\n+\t}\n };\n \n struct NumericMinMaxBase : public MinMaxBase {\n \ttemplate <class INPUT_TYPE, class STATE> static void Assign(STATE *state, INPUT_TYPE input) {\n-\t\t*state = input;\n+\t\tstate->value = input;\n \t}\n \n \ttemplate <class T, class STATE>\n \tstatic void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {\n-\t\tnullmask[idx] = IsNullValue<T>(*state);\n-\t\ttarget[idx] = *state;\n+\t\tnullmask[idx] = !state->isset;\n+\t\ttarget[idx] = state->value;\n \t}\n };\n \n struct MinOperation : public NumericMinMaxBase {\n \ttemplate <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {\n-\t\tif (LessThan::Operation<INPUT_TYPE>(input, *state)) {\n-\t\t\t*state = input;\n+\t\tif (LessThan::Operation<INPUT_TYPE>(input, state->value)) {\n+\t\t\tstate->value = input;\n+\t\t}\n+\t}\n+\n+\ttemplate <class STATE, class OP> static void Combine(STATE source, STATE *target) {\n+\t\tif (!source.isset) {\n+\t\t\t// source is NULL, nothing to do\n+\t\t\treturn;\n+\t\t}\n+\t\tif (!target->isset) {\n+\t\t\t// target is NULL, use source value directly\n+\t\t\t*target = source;\n+\t\t} else if (target->value > source.value) {\n+\t\t\ttarget->value = source.value;\n \t\t}\n \t}\n };\n \n struct MaxOperation : public NumericMinMaxBase {\n \ttemplate <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {\n-\t\tif (GreaterThan::Operation<INPUT_TYPE>(input, *state)) {\n-\t\t\t*state = input;\n+\t\tif (GreaterThan::Operation<INPUT_TYPE>(input, state->value)) {\n+\t\t\tstate->value = input;\n+\t\t}\n+\t}\n+\n+\ttemplate <class STATE, class OP> static void Combine(STATE source, STATE *target) {\n+\t\tif (!source.isset) {\n+\t\t\t// source is NULL, nothing to do\n+\t\t\treturn;\n+\t\t}\n+\t\tif (!target->isset) {\n+\t\t\t// target is NULL, use source value directly\n+\t\t\t*target = source;\n+\t\t} else if (target->value < source.value) {\n+\t\t\ttarget->value = source.value;\n \t\t}\n \t}\n };\n \n struct StringMinMaxBase : public MinMaxBase {\n \ttemplate <class STATE> static void Destroy(STATE *state) {\n-\t\tif (!state->IsInlined()) {\n-\t\t\tdelete[] state->GetData();\n+\t\tif (state->isset && !state->value.IsInlined()) {\n+\t\t\tdelete[] state->value.GetData();\n \t\t}\n \t}\n \n \ttemplate <class INPUT_TYPE, class STATE> static void Assign(STATE *state, INPUT_TYPE input) {\n \t\tif (input.IsInlined()) {\n-\t\t\t*state = input;\n+\t\t\tstate->value = input;\n \t\t} else {\n \t\t\t// non-inlined string, need to allocate space for it\n \t\t\tauto len = input.GetSize();\n \t\t\tauto ptr = new char[len + 1];\n \t\t\tmemcpy(ptr, input.GetData(), len + 1);\n \n-\t\t\t*state = string_t(ptr, len);\n+\t\t\tstate->value = string_t(ptr, len);\n \t\t}\n \t}\n \n \ttemplate <class T, class STATE>\n \tstatic void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {\n-\t\tif (IsNullValue<string_t>(*state)) {\n+\t\tif (!state->isset) {\n \t\t\tnullmask[idx] = true;\n \t\t} else {\n-\t\t\ttarget[idx] = StringVector::AddString(result, *state);\n+\t\t\ttarget[idx] = StringVector::AddString(result, state->value);\n+\t\t}\n+\t}\n+\n+\ttemplate <class STATE, class OP> static void Combine(STATE source, STATE *target) {\n+\t\tif (!source.isset) {\n+\t\t\t// source is NULL, nothing to do\n+\t\t\treturn;\n+\t\t}\n+\t\tif (!target->isset) {\n+\t\t\t// target is NULL, use source value directly\n+\t\t\t*target = source;\n+\t\t} else {\n+\t\t\tOP::template Execute<string_t, STATE>(target, source.value);\n \t\t}\n \t}\n };\n \n struct MinOperationString : public StringMinMaxBase {\n \ttemplate <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {\n-\t\tif (LessThan::Operation<INPUT_TYPE>(input, *state)) {\n+\t\tif (LessThan::Operation<INPUT_TYPE>(input, state->value)) {\n \t\t\tAssign(state, input);\n \t\t}\n \t}\n@@ -90,7 +181,7 @@ struct MinOperationString : public StringMinMaxBase {\n \n struct MaxOperationString : public StringMinMaxBase {\n \ttemplate <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {\n-\t\tif (GreaterThan::Operation<INPUT_TYPE>(input, *state)) {\n+\t\tif (GreaterThan::Operation<INPUT_TYPE>(input, state->value)) {\n \t\t\tAssign(state, input);\n \t\t}\n \t}\n@@ -99,10 +190,11 @@ struct MaxOperationString : public StringMinMaxBase {\n template <class OP, class OP_STRING> static void AddMinMaxOperator(AggregateFunctionSet &set) {\n \tfor (auto type : SQLType::ALL_TYPES) {\n \t\tif (type.id == SQLTypeId::VARCHAR) {\n-\t\t\tset.AddFunction(AggregateFunction::UnaryAggregateDestructor<string_t, string_t, string_t, OP_STRING>(\n-\t\t\t    SQLType::VARCHAR, SQLType::VARCHAR));\n+\t\t\tset.AddFunction(\n+\t\t\t    AggregateFunction::UnaryAggregateDestructor<min_max_state_t<string_t>, string_t, string_t, OP_STRING>(\n+\t\t\t        SQLType::VARCHAR, SQLType::VARCHAR));\n \t\t} else {\n-\t\t\tset.AddFunction(AggregateFunction::GetUnaryAggregate<OP>(type));\n+\t\t\tset.AddFunction(GetUnaryAggregate<OP>(type));\n \t\t}\n \t}\n }\ndiff --git a/src/include/duckdb/catalog/catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry.hpp\nindex 357a8444e40c..33252bc9a2de 100644\n--- a/src/include/duckdb/catalog/catalog_entry.hpp\n+++ b/src/include/duckdb/catalog/catalog_entry.hpp\n@@ -34,6 +34,11 @@ class CatalogEntry {\n \tvirtual unique_ptr<CatalogEntry> Copy(ClientContext &context) {\n \t\tthrow CatalogException(\"Unsupported copy type for catalog entry!\");\n \t}\n+\t//! Sets the CatalogEntry as the new root entry (i.e. the newest entry) - this is called on a rollback to an\n+\t//! AlterEntry\n+\tvirtual void SetAsRoot() {\n+\t}\n+\n \t//! The type of this catalog entry\n \tCatalogType type;\n \t//! Reference to the catalog this entry belongs to\ndiff --git a/src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp\nindex 760f5612adc9..ffc264d0ef2f 100644\n--- a/src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp\n+++ b/src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp\n@@ -13,14 +13,20 @@\n \n namespace duckdb {\n \n+struct DataTableInfo;\n+class Index;\n+\n //! An index catalog entry\n class IndexCatalogEntry : public StandardEntry {\n public:\n \t//! Create a real TableCatalogEntry and initialize storage for it\n \tIndexCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, CreateIndexInfo *info)\n-\t    : StandardEntry(CatalogType::INDEX, schema, catalog, info->index_name) {\n-\t\t// FIXME: add more information for drop index support\n+\t    : StandardEntry(CatalogType::INDEX, schema, catalog, info->index_name), index(nullptr) {\n \t}\n+\t~IndexCatalogEntry();\n+\n+\tIndex *index;\n+\tshared_ptr<DataTableInfo> info;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\nindex 2af53968f1c2..1a418b6625aa 100644\n--- a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\n+++ b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp\n@@ -22,6 +22,12 @@ class DataTable;\n struct CreateTableInfo;\n struct BoundCreateTableInfo;\n \n+struct RenameColumnInfo;\n+struct AddColumnInfo;\n+struct RemoveColumnInfo;\n+struct SetDefaultInfo;\n+struct ChangeColumnTypeInfo;\n+\n //! A table catalog entry\n class TableCatalogEntry : public StandardEntry {\n public:\n@@ -58,5 +64,14 @@ class TableCatalogEntry : public StandardEntry {\n \tstatic unique_ptr<CreateTableInfo> Deserialize(Deserializer &source);\n \n \tunique_ptr<CatalogEntry> Copy(ClientContext &context) override;\n+\n+\tvoid SetAsRoot() override;\n+\n+private:\n+\tunique_ptr<CatalogEntry> RenameColumn(ClientContext &context, RenameColumnInfo &info);\n+\tunique_ptr<CatalogEntry> AddColumn(ClientContext &context, AddColumnInfo &info);\n+\tunique_ptr<CatalogEntry> RemoveColumn(ClientContext &context, RemoveColumnInfo &info);\n+\tunique_ptr<CatalogEntry> SetDefault(ClientContext &context, SetDefaultInfo &info);\n+\tunique_ptr<CatalogEntry> ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info);\n };\n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/schema/physical_create_index.hpp b/src/include/duckdb/execution/operator/schema/physical_create_index.hpp\nindex 2bef7a2bfee6..1e9ddaf820a6 100644\n--- a/src/include/duckdb/execution/operator/schema/physical_create_index.hpp\n+++ b/src/include/duckdb/execution/operator/schema/physical_create_index.hpp\n@@ -41,8 +41,5 @@ class PhysicalCreateIndex : public PhysicalOperator {\n \n public:\n \tvoid GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) override;\n-\n-private:\n-\tvoid CreateARTIndex();\n };\n } // namespace duckdb\ndiff --git a/src/include/duckdb/function/aggregate/distributive_functions.hpp b/src/include/duckdb/function/aggregate/distributive_functions.hpp\nindex 89a4797e399d..646c4225664a 100644\n--- a/src/include/duckdb/function/aggregate/distributive_functions.hpp\n+++ b/src/include/duckdb/function/aggregate/distributive_functions.hpp\n@@ -14,39 +14,6 @@\n \n namespace duckdb {\n \n-struct StandardDistributiveFunction {\n-\ttemplate <class STATE> static void Initialize(STATE *state) {\n-\t\t*state = NullValue<STATE>();\n-\t}\n-\n-\ttemplate <class INPUT_TYPE, class STATE, class OP>\n-\tstatic void Operation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t idx) {\n-\t\tif (IsNullValue<INPUT_TYPE>(*state)) {\n-\t\t\tOP::template Assign<INPUT_TYPE, STATE>(state, input[idx]);\n-\t\t} else {\n-\t\t\tOP::template Execute<INPUT_TYPE, STATE>(state, input[idx]);\n-\t\t}\n-\t}\n-\n-\ttemplate <class STATE, class OP> static void Combine(STATE source, STATE *target) {\n-\t\tif (IsNullValue<STATE>(source)) {\n-\t\t\t// source is NULL, nothing to do\n-\t\t\treturn;\n-\t\t}\n-\t\tif (IsNullValue<STATE>(*target)) {\n-\t\t\t// target is NULL, use source value directly\n-\t\t\t*target = source;\n-\t\t} else {\n-\t\t\t// else perform the operation\n-\t\t\tOP::template Execute<STATE, STATE>(target, source);\n-\t\t}\n-\t}\n-\n-\tstatic bool IgnoreNull() {\n-\t\treturn true;\n-\t}\n-};\n-\n struct BitAndFun {\n \tstatic void RegisterFunction(BuiltinFunctions &set);\n };\ndiff --git a/src/include/duckdb/function/aggregate_function.hpp b/src/include/duckdb/function/aggregate_function.hpp\nindex e580ea358e91..9041b34d4095 100644\n--- a/src/include/duckdb/function/aggregate_function.hpp\n+++ b/src/include/duckdb/function/aggregate_function.hpp\n@@ -110,51 +110,6 @@ class AggregateFunction : public SimpleFunction {\n \t\treturn aggregate;\n \t};\n \n-public:\n-\ttemplate <class OP> static AggregateFunction GetNumericUnaryAggregate(SQLType type) {\n-\t\tswitch (type.id) {\n-\t\tcase SQLTypeId::TINYINT:\n-\t\t\treturn UnaryAggregate<int8_t, int8_t, int8_t, OP>(type, type);\n-\t\tcase SQLTypeId::SMALLINT:\n-\t\t\treturn UnaryAggregate<int16_t, int16_t, int16_t, OP>(type, type);\n-\t\tcase SQLTypeId::INTEGER:\n-\t\t\treturn UnaryAggregate<int32_t, int32_t, int32_t, OP>(type, type);\n-\t\tcase SQLTypeId::BIGINT:\n-\t\t\treturn UnaryAggregate<int64_t, int64_t, int64_t, OP>(type, type);\n-\t\tcase SQLTypeId::FLOAT:\n-\t\t\treturn UnaryAggregate<float, float, float, OP>(type, type);\n-\t\tcase SQLTypeId::DOUBLE:\n-\t\t\treturn UnaryAggregate<double, double, double, OP>(type, type);\n-\t\tcase SQLTypeId::DECIMAL:\n-\t\t\treturn UnaryAggregate<double, double, double, OP>(type, type);\n-\t\tdefault:\n-\t\t\tthrow NotImplementedException(\"Unimplemented numeric type for unary aggregate\");\n-\t\t}\n-\t}\n-\n-\ttemplate <class OP> static AggregateFunction GetUnaryAggregate(SQLType type) {\n-\t\tswitch (type.id) {\n-\t\tcase SQLTypeId::BOOLEAN:\n-\t\t\treturn UnaryAggregate<int8_t, int8_t, int8_t, OP>(type, type);\n-\t\tcase SQLTypeId::TINYINT:\n-\t\tcase SQLTypeId::SMALLINT:\n-\t\tcase SQLTypeId::INTEGER:\n-\t\tcase SQLTypeId::BIGINT:\n-\t\tcase SQLTypeId::FLOAT:\n-\t\tcase SQLTypeId::DOUBLE:\n-\t\tcase SQLTypeId::DECIMAL:\n-\t\t\treturn GetNumericUnaryAggregate<OP>(type);\n-\t\tcase SQLTypeId::DATE:\n-\t\t\treturn UnaryAggregate<date_t, date_t, date_t, OP>(type, type);\n-\t\tcase SQLTypeId::TIMESTAMP:\n-\t\t\treturn UnaryAggregate<timestamp_t, timestamp_t, timestamp_t, OP>(type, type);\n-\t\tcase SQLTypeId::VARCHAR:\n-\t\t\treturn UnaryAggregate<string_t, string_t, string_t, OP>(type, type);\n-\t\tdefault:\n-\t\t\tthrow NotImplementedException(\"Unimplemented type for unary aggregate\");\n-\t\t}\n-\t}\n-\n public:\n \ttemplate <class STATE> static idx_t StateSize() {\n \t\treturn sizeof(STATE);\ndiff --git a/src/include/duckdb/parser/column_definition.hpp b/src/include/duckdb/parser/column_definition.hpp\nindex 07de0cdae0b5..287cae8a582d 100644\n--- a/src/include/duckdb/parser/column_definition.hpp\n+++ b/src/include/duckdb/parser/column_definition.hpp\n@@ -31,5 +31,12 @@ class ColumnDefinition {\n \tSQLType type;\n \t//! The default value of the column (if any)\n \tunique_ptr<ParsedExpression> default_value;\n+\n+public:\n+\tColumnDefinition Copy();\n+\n+\tvoid Serialize(Serializer &serializer);\n+\tstatic ColumnDefinition Deserialize(Deserializer &source);\n };\n+\n } // namespace duckdb\ndiff --git a/src/include/duckdb/parser/parsed_data/alter_table_info.hpp b/src/include/duckdb/parser/parsed_data/alter_table_info.hpp\nindex 7c1de035b644..798c71818c04 100644\n--- a/src/include/duckdb/parser/parsed_data/alter_table_info.hpp\n+++ b/src/include/duckdb/parser/parsed_data/alter_table_info.hpp\n@@ -9,6 +9,7 @@\n #pragma once\n \n #include \"duckdb/parser/parsed_data/parse_info.hpp\"\n+#include \"duckdb/parser/column_definition.hpp\"\n \n namespace duckdb {\n \n@@ -26,7 +27,15 @@ struct AlterInfo : public ParseInfo {\n \tstatic unique_ptr<AlterInfo> Deserialize(Deserializer &source);\n };\n \n-enum class AlterTableType : uint8_t { INVALID = 0, RENAME_COLUMN = 1, RENAME_TABLE = 2 };\n+enum class AlterTableType : uint8_t {\n+\tINVALID = 0,\n+\tRENAME_COLUMN = 1,\n+\tRENAME_TABLE = 2,\n+\tADD_COLUMN = 3,\n+\tREMOVE_COLUMN = 4,\n+\tALTER_COLUMN_TYPE = 5,\n+\tSET_DEFAULT = 6\n+};\n \n struct AlterTableInfo : public AlterInfo {\n \tAlterTableInfo(AlterTableType type, string schema, string table)\n@@ -41,10 +50,14 @@ struct AlterTableInfo : public AlterInfo {\n \t//! Table name to alter to\n \tstring table;\n \n+public:\n \tvirtual void Serialize(Serializer &serializer) override;\n \tstatic unique_ptr<AlterInfo> Deserialize(Deserializer &source);\n };\n \n+//===--------------------------------------------------------------------===//\n+// RenameColumnInfo\n+//===--------------------------------------------------------------------===//\n struct RenameColumnInfo : public AlterTableInfo {\n \tRenameColumnInfo(string schema, string table, string name, string new_name)\n \t    : AlterTableInfo(AlterTableType::RENAME_COLUMN, schema, table), name(name), new_name(new_name) {\n@@ -57,10 +70,14 @@ struct RenameColumnInfo : public AlterTableInfo {\n \t//! Column new name\n \tstring new_name;\n \n+public:\n \tvoid Serialize(Serializer &serializer) override;\n \tstatic unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);\n };\n \n+//===--------------------------------------------------------------------===//\n+// RenameTableInfo\n+//===--------------------------------------------------------------------===//\n struct RenameTableInfo : public AlterTableInfo {\n \tRenameTableInfo(string schema, string table, string new_name)\n \t    : AlterTableInfo(AlterTableType::RENAME_TABLE, schema, table), new_table_name(new_name) {\n@@ -71,6 +88,91 @@ struct RenameTableInfo : public AlterTableInfo {\n \t//! Table new name\n \tstring new_table_name;\n \n+public:\n+\tvoid Serialize(Serializer &serializer) override;\n+\tstatic unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// AddColumnInfo\n+//===--------------------------------------------------------------------===//\n+struct AddColumnInfo : public AlterTableInfo {\n+\tAddColumnInfo(string schema, string table, ColumnDefinition new_column)\n+\t    : AlterTableInfo(AlterTableType::ADD_COLUMN, schema, table), new_column(move(new_column)) {\n+\t}\n+\t~AddColumnInfo() override {\n+\t}\n+\n+\t//! New column\n+\tColumnDefinition new_column;\n+\n+public:\n+\tvoid Serialize(Serializer &serializer) override;\n+\tstatic unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// RemoveColumnInfo\n+//===--------------------------------------------------------------------===//\n+struct RemoveColumnInfo : public AlterTableInfo {\n+\tRemoveColumnInfo(string schema, string table, string removed_column, bool if_exists)\n+\t    : AlterTableInfo(AlterTableType::REMOVE_COLUMN, schema, table), removed_column(move(removed_column)),\n+\t      if_exists(if_exists) {\n+\t}\n+\t~RemoveColumnInfo() override {\n+\t}\n+\n+\t//! The column to remove\n+\tstring removed_column;\n+\t//! Whether or not an error should be thrown if the column does not exist\n+\tbool if_exists;\n+\n+public:\n+\tvoid Serialize(Serializer &serializer) override;\n+\tstatic unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// ChangeColumnTypeInfo\n+//===--------------------------------------------------------------------===//\n+struct ChangeColumnTypeInfo : public AlterTableInfo {\n+\tChangeColumnTypeInfo(string schema, string table, string column_name, SQLType target_type,\n+\t                     unique_ptr<ParsedExpression> expression)\n+\t    : AlterTableInfo(AlterTableType::ALTER_COLUMN_TYPE, schema, table), column_name(move(column_name)),\n+\t      target_type(move(target_type)), expression(move(expression)) {\n+\t}\n+\t~ChangeColumnTypeInfo() override {\n+\t}\n+\n+\t//! The column name to alter\n+\tstring column_name;\n+\t//! The target type of the column\n+\tSQLType target_type;\n+\t//! The expression used for data conversion\n+\tunique_ptr<ParsedExpression> expression;\n+\n+public:\n+\tvoid Serialize(Serializer &serializer) override;\n+\tstatic unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// SetDefaultInfo\n+//===--------------------------------------------------------------------===//\n+struct SetDefaultInfo : public AlterTableInfo {\n+\tSetDefaultInfo(string schema, string table, string column_name, unique_ptr<ParsedExpression> new_default)\n+\t    : AlterTableInfo(AlterTableType::SET_DEFAULT, schema, table), column_name(move(column_name)),\n+\t      expression(move(new_default)) {\n+\t}\n+\t~SetDefaultInfo() override {\n+\t}\n+\n+\t//! The column name to alter\n+\tstring column_name;\n+\t//! The expression used for data conversion\n+\tunique_ptr<ParsedExpression> expression;\n+\n+public:\n \tvoid Serialize(Serializer &serializer) override;\n \tstatic unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);\n };\ndiff --git a/src/include/duckdb/parser/statement/alter_table_statement.hpp b/src/include/duckdb/parser/statement/alter_table_statement.hpp\nindex 790114971efd..977a4124d5b0 100644\n--- a/src/include/duckdb/parser/statement/alter_table_statement.hpp\n+++ b/src/include/duckdb/parser/statement/alter_table_statement.hpp\n@@ -11,15 +11,17 @@\n #include \"duckdb/parser/column_definition.hpp\"\n #include \"duckdb/parser/parsed_data/alter_table_info.hpp\"\n #include \"duckdb/parser/sql_statement.hpp\"\n-#include \"duckdb/parser/tableref.hpp\"\n \n namespace duckdb {\n \n class AlterTableStatement : public SQLStatement {\n public:\n-\tAlterTableStatement(unique_ptr<AlterTableInfo> info) : SQLStatement(StatementType::ALTER_STATEMENT), info(std::move(info)){};\n+\tAlterTableStatement() : SQLStatement(StatementType::ALTER_STATEMENT) {\n+\t}\n+\tAlterTableStatement(unique_ptr<AlterTableInfo> info)\n+\t    : SQLStatement(StatementType::ALTER_STATEMENT), info(std::move(info)) {\n+\t}\n \n-\tunique_ptr<TableRef> table;\n \tunique_ptr<AlterTableInfo> info;\n };\n \ndiff --git a/src/include/duckdb/parser/transformer.hpp b/src/include/duckdb/parser/transformer.hpp\nindex e7ea14b09443..c7e2c1ff0d0d 100644\n--- a/src/include/duckdb/parser/transformer.hpp\n+++ b/src/include/duckdb/parser/transformer.hpp\n@@ -151,6 +151,7 @@ class Transformer {\n \n \tstring TransformCollation(PGCollateClause *collate);\n \n+\tColumnDefinition TransformColumnDefinition(PGColumnDef *cdef);\n \t//===--------------------------------------------------------------------===//\n \t// Helpers\n \t//===--------------------------------------------------------------------===//\ndiff --git a/src/include/duckdb/planner/expression_binder/alter_binder.hpp b/src/include/duckdb/planner/expression_binder/alter_binder.hpp\nnew file mode 100644\nindex 000000000000..fe608b04b876\n--- /dev/null\n+++ b/src/include/duckdb/planner/expression_binder/alter_binder.hpp\n@@ -0,0 +1,33 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/planner/expression_binder/alter_binder.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/parser/column_definition.hpp\"\n+#include \"duckdb/planner/expression_binder.hpp\"\n+\n+namespace duckdb {\n+//! The ALTER binder is responsible for binding an expression within alter statements\n+class AlterBinder : public ExpressionBinder {\n+public:\n+\tAlterBinder(Binder &binder, ClientContext &context, string table, vector<ColumnDefinition> &columns,\n+\t            vector<column_t> &bound_columns, SQLType target_type);\n+\n+\tstring table;\n+\tvector<ColumnDefinition> &columns;\n+\tvector<column_t> &bound_columns;\n+\n+protected:\n+\tBindResult BindExpression(ParsedExpression &expr, idx_t depth, bool root_expression = false) override;\n+\n+\tBindResult BindColumn(ColumnRefExpression &expr);\n+\n+\tstring UnsupportedAggregateMessage() override;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/storage/column_data.hpp b/src/include/duckdb/storage/column_data.hpp\nindex 92d184ffaddd..3672800639d7 100644\n--- a/src/include/duckdb/storage/column_data.hpp\n+++ b/src/include/duckdb/storage/column_data.hpp\n@@ -14,20 +14,22 @@\n #include \"duckdb/storage/table/persistent_segment.hpp\"\n \n namespace duckdb {\n-class DataTable;\n class PersistentSegment;\n class Transaction;\n \n+struct DataTableInfo;\n+\n class ColumnData {\n public:\n-\tColumnData();\n+\tColumnData(BufferManager &manager, DataTableInfo &table_info);\n \t//! Set up the column data with the set of persistent segments, returns the amount of rows\n \tvoid Initialize(vector<unique_ptr<PersistentSegment>> &segments);\n \n+\tDataTableInfo &table_info;\n \t//! The type of the column\n \tTypeId type;\n-\t//! The table of the column\n-\tDataTable *table;\n+\t//! The buffer manager\n+\tBufferManager &manager;\n \t//! The column index of the column\n \tidx_t column_idx;\n \t//! The segments holding the data of the column\ndiff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp\nindex 3d8145b07174..e18ab68b83ce 100644\n--- a/src/include/duckdb/storage/data_table.hpp\n+++ b/src/include/duckdb/storage/data_table.hpp\n@@ -26,11 +26,13 @@\n namespace duckdb {\n class ClientContext;\n class ColumnDefinition;\n+class DataTable;\n class StorageManager;\n class TableCatalogEntry;\n class Transaction;\n \n typedef unique_ptr<vector<unique_ptr<PersistentSegment>>[]> persistent_data_t;\n+\n //! TableFilter represents a filter pushed down into the table scan.\n class TableFilter {\n public:\n@@ -41,10 +43,9 @@ class TableFilter {\n \tidx_t column_index;\n };\n \n-//! DataTable represents a physical table on disk\n-class DataTable {\n-public:\n-\tDataTable(StorageManager &storage, string schema, string table, vector<TypeId> types, persistent_data_t data);\n+struct DataTableInfo {\n+\tDataTableInfo(string schema, string table) : cardinality(0), schema(move(schema)), table(move(table)) {\n+\t}\n \n \t//! The amount of elements in the table. Note that this number signifies the amount of COMMITTED entries in the\n \t//! table. It can be inaccurate inside of transactions. More work is needed to properly support that.\n@@ -53,12 +54,32 @@ class DataTable {\n \tstring schema;\n \t// name of the table\n \tstring table;\n+\t//! Indexes associated with the current table\n+\tvector<unique_ptr<Index>> indexes;\n+\n+\tbool IsTemporary() {\n+\t\treturn schema == TEMP_SCHEMA;\n+\t}\n+};\n+\n+//! DataTable represents a physical table on disk\n+class DataTable {\n+public:\n+\t//! Constructs a new data table from an (optional) set of persistent segments\n+\tDataTable(StorageManager &storage, string schema, string table, vector<TypeId> types, persistent_data_t data);\n+\t//! Constructs a DataTable as a delta on an existing data table with a newly added column\n+\tDataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value);\n+\t//! Constructs a DataTable as a delta on an existing data table but with one column removed\n+\tDataTable(ClientContext &context, DataTable &parent, idx_t removed_column);\n+\t//! Constructs a DataTable as a delta on an existing data table but with one column changed type\n+\tDataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, SQLType target_type,\n+\t          vector<column_t> bound_columns, Expression &cast_expr);\n+\n+\tshared_ptr<DataTableInfo> info;\n \t//! Types managed by data table\n \tvector<TypeId> types;\n \t//! A reference to the base storage manager\n \tStorageManager &storage;\n-\t//! Indexes\n-\tvector<unique_ptr<Index>> indexes;\n \n public:\n \tvoid InitializeScan(TableScanState &state, vector<column_t> column_ids,\n@@ -114,8 +135,10 @@ class DataTable {\n \tvoid RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers);\n \t//! Remove the row identifiers from all the indexes of the table\n \tvoid RemoveFromIndexes(Vector &row_identifiers, idx_t count);\n-\t//! Is this a temporary table?\n-\tbool IsTemporary();\n+\n+\tvoid SetAsRoot() {\n+\t\tthis->is_root = true;\n+\t}\n \n private:\n \t//! Verify constraints with a chunk from the Append containing all columns of the table\n@@ -141,13 +164,18 @@ class DataTable {\n \t//! The CreateIndexScan is a special scan that is used to create an index on the table, it keeps locks on the table\n \tvoid InitializeCreateIndexScan(CreateIndexScanState &state, vector<column_t> column_ids);\n \tvoid CreateIndexScan(CreateIndexScanState &structure, DataChunk &result);\n+\n+private:\n \t//! Lock for appending entries to the table\n \tstd::mutex append_lock;\n \t//! The version manager of the persistent segments of the tree\n-\tVersionManager persistent_manager;\n+\tshared_ptr<VersionManager> persistent_manager;\n \t//! The version manager of the transient segments of the tree\n-\tVersionManager transient_manager;\n+\tshared_ptr<VersionManager> transient_manager;\n \t//! The physical columns of the table\n-\tunique_ptr<ColumnData[]> columns;\n+\tvector<shared_ptr<ColumnData>> columns;\n+\t//! Whether or not the data table is the root DataTable for this table; the root DataTable is the newest version\n+\t//! that can be appended to\n+\tbool is_root;\n };\n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/table/version_manager.hpp b/src/include/duckdb/storage/table/version_manager.hpp\nindex f6c9fcf58334..8c9640b46846 100644\n--- a/src/include/duckdb/storage/table/version_manager.hpp\n+++ b/src/include/duckdb/storage/table/version_manager.hpp\n@@ -20,13 +20,15 @@ class DataTable;\n class Transaction;\n class VersionManager;\n \n+struct DataTableInfo;\n+\n class VersionManager {\n public:\n-\tVersionManager(DataTable &table) : table(table), max_row(0), base_row(0) {\n+\tVersionManager(DataTableInfo &table_info) : table_info(table_info), max_row(0), base_row(0) {\n \t}\n \n-\t//! The DataTable\n-\tDataTable &table;\n+\t//! The DataTableInfo\n+\tDataTableInfo &table_info;\n \t//! The read/write lock for the delete info and insert info\n \tStorageLock lock;\n \t//! The info for each of the chunks\n@@ -46,7 +48,7 @@ class VersionManager {\n \tbool Fetch(Transaction &transaction, idx_t row);\n \n \t//! Delete the given set of rows in the version manager\n-\tvoid Delete(Transaction &transaction, Vector &row_ids, idx_t count);\n+\tvoid Delete(Transaction &transaction, DataTable *table, Vector &row_ids, idx_t count);\n \t//! Append a set of rows to the version manager, setting their inserted id to the given commit_id\n \tvoid Append(Transaction &transaction, row_t row_start, idx_t count, transaction_t commit_id);\n \t//! Revert a set of appends made to the version manager from the rows [row_start] until [row_end]\ndiff --git a/src/include/duckdb/transaction/commit_state.hpp b/src/include/duckdb/transaction/commit_state.hpp\nindex 790c5e9d0d1b..21badd123a81 100644\n--- a/src/include/duckdb/transaction/commit_state.hpp\n+++ b/src/include/duckdb/transaction/commit_state.hpp\n@@ -13,9 +13,9 @@\n namespace duckdb {\n class CatalogEntry;\n class DataChunk;\n-class DataTable;\n class WriteAheadLog;\n \n+struct DataTableInfo;\n struct DeleteInfo;\n struct UpdateInfo;\n \n@@ -27,7 +27,7 @@ class CommitState {\n \ttransaction_t commit_id;\n \tUndoFlags current_op;\n \n-\tDataTable *current_table;\n+\tDataTableInfo *current_table_info;\n \tidx_t row_identifiers[STANDARD_VECTOR_SIZE];\n \n \tunique_ptr<DataChunk> delete_chunk;\n@@ -38,7 +38,7 @@ class CommitState {\n \tvoid RevertCommit(UndoFlags type, data_ptr_t data);\n \n private:\n-\tvoid SwitchTable(DataTable *table, UndoFlags new_op);\n+\tvoid SwitchTable(DataTableInfo *table, UndoFlags new_op);\n \n \tvoid WriteCatalogEntry(CatalogEntry *entry, data_ptr_t extra_data);\n \tvoid WriteDelete(DeleteInfo *info);\ndiff --git a/src/include/duckdb/transaction/delete_info.hpp b/src/include/duckdb/transaction/delete_info.hpp\nindex afd7bcef0d8d..89f57260a845 100644\n--- a/src/include/duckdb/transaction/delete_info.hpp\n+++ b/src/include/duckdb/transaction/delete_info.hpp\n@@ -15,12 +15,11 @@ class ChunkInfo;\n class DataTable;\n \n struct DeleteInfo {\n+\tDataTable *table;\n \tChunkInfo *vinfo;\n \tidx_t count;\n \tidx_t base_row;\n \trow_t rows[1];\n-\n-\tDataTable &GetTable();\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/transaction/local_storage.hpp b/src/include/duckdb/transaction/local_storage.hpp\nindex 9c3081378421..704bcb01e519 100644\n--- a/src/include/duckdb/transaction/local_storage.hpp\n+++ b/src/include/duckdb/transaction/local_storage.hpp\n@@ -68,6 +68,10 @@ class LocalStorage {\n \t\treturn table_storage.size() > 0;\n \t}\n \n+\tvoid AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column, Expression *default_value);\n+\tvoid ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, SQLType target_type,\n+\t                vector<column_t> bound_columns, Expression &cast_expr);\n+\n private:\n \tLocalTableStorage *GetStorage(DataTable *table);\n \ndiff --git a/src/include/duckdb/transaction/transaction.hpp b/src/include/duckdb/transaction/transaction.hpp\nindex 697da7f05ae2..ef4682cee779 100644\n--- a/src/include/duckdb/transaction/transaction.hpp\n+++ b/src/include/duckdb/transaction/transaction.hpp\n@@ -78,7 +78,7 @@ class Transaction {\n \t\treturn start_timestamp;\n \t}\n \n-\tvoid PushDelete(ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row);\n+\tvoid PushDelete(DataTable *table, ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row);\n \n \tUpdateInfo *CreateUpdateInfo(idx_t type_size, idx_t entries);\n \ndiff --git a/src/optimizer/index_scan.cpp b/src/optimizer/index_scan.cpp\nindex e110f9c27463..f013b57b97da 100644\n--- a/src/optimizer/index_scan.cpp\n+++ b/src/optimizer/index_scan.cpp\n@@ -56,14 +56,14 @@ unique_ptr<LogicalOperator> IndexScan::TransformFilterToIndexScan(unique_ptr<Log\n \n \tauto &storage = *get->table->storage;\n \n-\tif (storage.indexes.size() == 0) {\n+\tif (storage.info->indexes.size() == 0) {\n \t\t// no indexes on the table, can't rewrite\n \t\treturn op;\n \t}\n \n \t// check all the indexes\n-\tfor (size_t j = 0; j < storage.indexes.size(); j++) {\n-\t\tauto &index = storage.indexes[j];\n+\tfor (size_t j = 0; j < storage.info->indexes.size(); j++) {\n+\t\tauto &index = storage.info->indexes[j];\n \n \t\t//\t\tassert(index->unbound_expressions.size() == 1);\n \t\t// first rewrite the index expression so the ColumnBindings align with the column bindings of the current table\ndiff --git a/src/optimizer/pushdown/pushdown_get.cpp b/src/optimizer/pushdown/pushdown_get.cpp\nindex 6943322f508f..9f217b34b85c 100644\n--- a/src/optimizer/pushdown/pushdown_get.cpp\n+++ b/src/optimizer/pushdown/pushdown_get.cpp\n@@ -22,7 +22,7 @@ unique_ptr<LogicalOperator> FilterPushdown::PushdownGet(unique_ptr<LogicalOperat\n \t\t}\n \t}\n \t//! FIXME: We only need to skip if the index is in the column being filtered\n-\tif (!get.table || !get.table->storage->indexes.empty()) {\n+\tif (!get.table || !get.table->storage->info->indexes.empty()) {\n \t\t//! now push any existing filters\n \t\tif (filters.empty()) {\n \t\t\t//! no filters to push\ndiff --git a/src/parser/CMakeLists.txt b/src/parser/CMakeLists.txt\nindex 5b5c1d93cb4d..e39b8ba82b81 100644\n--- a/src/parser/CMakeLists.txt\n+++ b/src/parser/CMakeLists.txt\n@@ -12,6 +12,7 @@ add_subdirectory(transform)\n add_library_unity(duckdb_parser\n                   OBJECT\n                   base_expression.cpp\n+                  column_definition.cpp\n                   constraint.cpp\n                   expression_util.cpp\n                   parsed_expression.cpp\ndiff --git a/src/parser/column_definition.cpp b/src/parser/column_definition.cpp\nnew file mode 100644\nindex 000000000000..319fc82292fa\n--- /dev/null\n+++ b/src/parser/column_definition.cpp\n@@ -0,0 +1,26 @@\n+#include \"duckdb/parser/column_definition.hpp\"\n+#include \"duckdb/common/serializer.hpp\"\n+\n+namespace duckdb {\n+\n+ColumnDefinition ColumnDefinition::Copy() {\n+\tColumnDefinition copy(name, type);\n+\tcopy.oid = oid;\n+\tcopy.default_value = default_value ? default_value->Copy() : nullptr;\n+\treturn copy;\n+}\n+\n+void ColumnDefinition::Serialize(Serializer &serializer) {\n+\tserializer.WriteString(name);\n+\ttype.Serialize(serializer);\n+\tserializer.WriteOptional(default_value);\n+}\n+\n+ColumnDefinition ColumnDefinition::Deserialize(Deserializer &source) {\n+\tauto column_name = source.Read<string>();\n+\tauto column_type = SQLType::Deserialize(source);\n+\tauto default_value = source.ReadOptional<ParsedExpression>();\n+\treturn ColumnDefinition(column_name, column_type, move(default_value));\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/parser/parsed_data/alter_table_info.cpp b/src/parser/parsed_data/alter_table_info.cpp\nindex f54e4ff39ba8..b23e5176c0c2 100644\n--- a/src/parser/parsed_data/alter_table_info.cpp\n+++ b/src/parser/parsed_data/alter_table_info.cpp\n@@ -35,11 +35,22 @@ unique_ptr<AlterInfo> AlterTableInfo::Deserialize(Deserializer &source) {\n \t\treturn RenameColumnInfo::Deserialize(source, schema, table);\n \tcase AlterTableType::RENAME_TABLE:\n \t\treturn RenameTableInfo::Deserialize(source, schema, table);\n+\tcase AlterTableType::ADD_COLUMN:\n+\t\treturn AddColumnInfo::Deserialize(source, schema, table);\n+\tcase AlterTableType::REMOVE_COLUMN:\n+\t\treturn RemoveColumnInfo::Deserialize(source, schema, table);\n+\tcase AlterTableType::ALTER_COLUMN_TYPE:\n+\t\treturn ChangeColumnTypeInfo::Deserialize(source, schema, table);\n+\tcase AlterTableType::SET_DEFAULT:\n+\t\treturn SetDefaultInfo::Deserialize(source, schema, table);\n \tdefault:\n \t\tthrow SerializationException(\"Unknown alter table type for deserialization!\");\n \t}\n }\n \n+//===--------------------------------------------------------------------===//\n+// RenameColumnInfo\n+//===--------------------------------------------------------------------===//\n void RenameColumnInfo::Serialize(Serializer &serializer) {\n \tAlterTableInfo::Serialize(serializer);\n \tserializer.WriteString(name);\n@@ -52,6 +63,9 @@ unique_ptr<AlterInfo> RenameColumnInfo::Deserialize(Deserializer &source, string\n \treturn make_unique<RenameColumnInfo>(schema, table, name, new_name);\n }\n \n+//===--------------------------------------------------------------------===//\n+// RenameTableInfo\n+//===--------------------------------------------------------------------===//\n void RenameTableInfo::Serialize(Serializer &serializer) {\n \tAlterTableInfo::Serialize(serializer);\n \tserializer.WriteString(new_table_name);\n@@ -61,3 +75,63 @@ unique_ptr<AlterInfo> RenameTableInfo::Deserialize(Deserializer &source, string\n \tauto new_name = source.Read<string>();\n \treturn make_unique<RenameTableInfo>(schema, table, new_name);\n }\n+\n+//===--------------------------------------------------------------------===//\n+// AddColumnInfo\n+//===--------------------------------------------------------------------===//\n+void AddColumnInfo::Serialize(Serializer &serializer) {\n+\tAlterTableInfo::Serialize(serializer);\n+\tnew_column.Serialize(serializer);\n+}\n+\n+unique_ptr<AlterInfo> AddColumnInfo::Deserialize(Deserializer &source, string schema, string table) {\n+\tauto new_column = ColumnDefinition::Deserialize(source);\n+\treturn make_unique<AddColumnInfo>(schema, table, move(new_column));\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// RemoveColumnInfo\n+//===--------------------------------------------------------------------===//\n+void RemoveColumnInfo::Serialize(Serializer &serializer) {\n+\tAlterTableInfo::Serialize(serializer);\n+\tserializer.WriteString(removed_column);\n+\tserializer.Write<bool>(if_exists);\n+}\n+\n+unique_ptr<AlterInfo> RemoveColumnInfo::Deserialize(Deserializer &source, string schema, string table) {\n+\tauto new_name = source.Read<string>();\n+\tauto if_exists = source.Read<bool>();\n+\treturn make_unique<RemoveColumnInfo>(schema, table, new_name, if_exists);\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// ChangeColumnTypeInfo\n+//===--------------------------------------------------------------------===//\n+void ChangeColumnTypeInfo::Serialize(Serializer &serializer) {\n+\tAlterTableInfo::Serialize(serializer);\n+\tserializer.WriteString(column_name);\n+\ttarget_type.Serialize(serializer);\n+\tserializer.WriteOptional(expression);\n+}\n+\n+unique_ptr<AlterInfo> ChangeColumnTypeInfo::Deserialize(Deserializer &source, string schema, string table) {\n+\tauto column_name = source.Read<string>();\n+\tauto target_type = SQLType::Deserialize(source);\n+\tauto expression = source.ReadOptional<ParsedExpression>();\n+\treturn make_unique<ChangeColumnTypeInfo>(schema, table, move(column_name), move(target_type), move(expression));\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// SetDefaultInfo\n+//===--------------------------------------------------------------------===//\n+void SetDefaultInfo::Serialize(Serializer &serializer) {\n+\tAlterTableInfo::Serialize(serializer);\n+\tserializer.WriteString(column_name);\n+\tserializer.WriteOptional(expression);\n+}\n+\n+unique_ptr<AlterInfo> SetDefaultInfo::Deserialize(Deserializer &source, string schema, string table) {\n+\tauto column_name = source.Read<string>();\n+\tauto new_default = source.ReadOptional<ParsedExpression>();\n+\treturn make_unique<SetDefaultInfo>(schema, table, move(column_name), move(new_default));\n+}\ndiff --git a/src/parser/transform/statement/transform_alter_table.cpp b/src/parser/transform/statement/transform_alter_table.cpp\nindex 3d46344ce7e1..b5b59e33ad8c 100644\n--- a/src/parser/transform/statement/transform_alter_table.cpp\n+++ b/src/parser/transform/statement/transform_alter_table.cpp\n@@ -1,45 +1,79 @@\n #include \"duckdb/parser/statement/alter_table_statement.hpp\"\n #include \"duckdb/parser/transformer.hpp\"\n+#include \"duckdb/parser/tableref/basetableref.hpp\"\n+#include \"duckdb/parser/expression/cast_expression.hpp\"\n+#include \"duckdb/parser/expression/columnref_expression.hpp\"\n+#include \"duckdb/parser/constraint.hpp\"\n \n-using namespace duckdb;\n using namespace std;\n \n+namespace duckdb {\n+\n unique_ptr<AlterTableStatement> Transformer::TransformAlter(PGNode *node) {\n-\tthrow NotImplementedException(\"Alter table not supported yet!\");\n-\t// auto stmt = reinterpret_cast<AlterTableStmt *>(node);\n-\t// assert(stmt);\n-\t// assert(stmt->relation);\n-\n-\t// auto result = make_unique<AlterTableStatement>();\n-\t// auto &info = *result->info.get();\n-\t// auto new_alter_cmd = make_unique<AlterTableCmd>();\n-\t// result->table = TransformRangeVar(stmt->relation);\n-\n-\t// info.table = stmt->relation->relname;\n-\n-\t// // first we check the type of ALTER\n-\t// for (auto c = stmt->cmds->head; c != NULL; c = c->next) {\n-\t// \tauto command = reinterpret_cast<PGAlterTableCmd *>(lfirst(c));\n-\t// \t//TODO: Include more options for command->subtype\n-\t// \tswitch (command->subtype) {\n-\t// \t\tcase PG_AT_AddColumn: {\n-\t//                auto cdef = (ColumnDef *)command->def;\n-\t//                char *name = (reinterpret_cast<PGValue *>(\n-\t//                        cdef->typeName->names->tail->data.ptr_value)\n-\t//                        ->val.str);\n-\t//                auto centry =\n-\t//                        ColumnDefinition(cdef->colname,\n-\t//                        TransformStringToTypeId(name));\n-\t//                info.new_columns.push_back(centry);\n-\t//                break;\n-\t//            }\n-\t// \t\tcase PG_AT_DropColumn:\n-\t// \t\tcase PG_AT_AlterColumnType:\n-\t// \t\tdefault:\n-\t// \t\t\tthrow NotImplementedException(\n-\t// \t\t\t    \"ALTER TABLE option not supported yet!\");\n-\t// \t}\n-\t// }\n-\n-\t// return result;\n+\tauto stmt = reinterpret_cast<PGAlterTableStmt *>(node);\n+\tassert(stmt);\n+\tassert(stmt->relation);\n+\n+\tauto result = make_unique<AlterTableStatement>();\n+\n+\tauto table = TransformRangeVar(stmt->relation);\n+\tassert(table->type == TableReferenceType::BASE_TABLE);\n+\n+\tauto &basetable = (BaseTableRef &)*table;\n+\t// first we check the type of ALTER\n+\tfor (auto c = stmt->cmds->head; c != NULL; c = c->next) {\n+\t\tauto command = reinterpret_cast<PGAlterTableCmd *>(lfirst(c));\n+\t\t// TODO: Include more options for command->subtype\n+\t\tswitch (command->subtype) {\n+\t\tcase PG_AT_AddColumn: {\n+\t\t\tauto cdef = (PGColumnDef *)command->def;\n+\t\t\tauto centry = TransformColumnDefinition(cdef);\n+\t\t\tif (cdef->constraints) {\n+\t\t\t\tfor (auto constr = cdef->constraints->head; constr != nullptr; constr = constr->next) {\n+\t\t\t\t\tauto constraint = TransformConstraint(constr, centry, 0);\n+\t\t\t\t\tif (constraint) {\n+\t\t\t\t\t\tthrow ParserException(\"Adding columns with constraints not yet supported\");\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tresult->info = make_unique<AddColumnInfo>(basetable.schema_name, basetable.table_name, move(centry));\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase PG_AT_DropColumn: {\n+\t\t\tresult->info = make_unique<RemoveColumnInfo>(basetable.schema_name, basetable.table_name, command->name,\n+\t\t\t                                             command->missing_ok);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase PG_AT_ColumnDefault: {\n+\t\t\tauto expr = TransformExpression(command->def);\n+\t\t\tresult->info =\n+\t\t\t    make_unique<SetDefaultInfo>(basetable.schema_name, basetable.table_name, command->name, move(expr));\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase PG_AT_AlterColumnType: {\n+\t\t\tauto cdef = (PGColumnDef *)command->def;\n+\t\t\tSQLType target_type = TransformTypeName(cdef->typeName);\n+\t\t\ttarget_type.collation = TransformCollation(cdef->collClause);\n+\n+\t\t\tunique_ptr<ParsedExpression> expr;\n+\t\t\tif (cdef->raw_default) {\n+\t\t\t\texpr = TransformExpression(cdef->raw_default);\n+\t\t\t} else {\n+\t\t\t\tauto colref = make_unique<ColumnRefExpression>(command->name);\n+\t\t\t\texpr = make_unique<CastExpression>(target_type, move(colref));\n+\t\t\t}\n+\t\t\tresult->info = make_unique<ChangeColumnTypeInfo>(basetable.schema_name, basetable.table_name, command->name,\n+\t\t\t                                                 target_type, move(expr));\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase PG_AT_DropConstraint:\n+\t\tcase PG_AT_DropNotNull:\n+\t\tdefault:\n+\t\t\tthrow NotImplementedException(\"ALTER TABLE option not supported yet!\");\n+\t\t}\n+\t}\n+\n+\treturn result;\n }\n+\n+} // namespace duckdb\ndiff --git a/src/parser/transform/statement/transform_create_table.cpp b/src/parser/transform/statement/transform_create_table.cpp\nindex 9614f9baf533..932422709527 100644\n--- a/src/parser/transform/statement/transform_create_table.cpp\n+++ b/src/parser/transform/statement/transform_create_table.cpp\n@@ -33,6 +33,13 @@ unique_ptr<ParsedExpression> Transformer::TransformCollateExpr(PGCollateClause *\n \treturn make_unique<CollateExpression>(collation, move(child));\n }\n \n+ColumnDefinition Transformer::TransformColumnDefinition(PGColumnDef *cdef) {\n+\tSQLType target_type = TransformTypeName(cdef->typeName);\n+\ttarget_type.collation = TransformCollation(cdef->collClause);\n+\n+\treturn ColumnDefinition(cdef->colname, target_type);\n+}\n+\n unique_ptr<CreateStatement> Transformer::TransformCreateTable(PGNode *node) {\n \tauto stmt = reinterpret_cast<PGCreateStmt *>(node);\n \tassert(stmt);\n@@ -65,11 +72,7 @@ unique_ptr<CreateStatement> Transformer::TransformCreateTable(PGNode *node) {\n \t\tswitch (node->type) {\n \t\tcase T_PGColumnDef: {\n \t\t\tauto cdef = (PGColumnDef *)c->data.ptr_value;\n-\t\t\tSQLType target_type = TransformTypeName(cdef->typeName);\n-\t\t\ttarget_type.collation = TransformCollation(cdef->collClause);\n-\n-\t\t\tauto centry = ColumnDefinition(cdef->colname, target_type);\n-\n+\t\t\tauto centry = TransformColumnDefinition(cdef);\n \t\t\tif (cdef->constraints) {\n \t\t\t\tfor (auto constr = cdef->constraints->head; constr != nullptr; constr = constr->next) {\n \t\t\t\t\tauto constraint = TransformConstraint(constr, centry, info->columns.size());\ndiff --git a/src/planner/binder/statement/bind_create_table.cpp b/src/planner/binder/statement/bind_create_table.cpp\nindex f1e995a6ac12..13bde6ad6126 100644\n--- a/src/planner/binder/statement/bind_create_table.cpp\n+++ b/src/planner/binder/statement/bind_create_table.cpp\n@@ -29,6 +29,7 @@ static void BindConstraints(Binder &binder, BoundCreateTableInfo &info) {\n \tauto &base = (CreateTableInfo &)*info.base;\n \n \tbool has_primary_key = false;\n+\tunordered_set<idx_t> primary_keys;\n \tfor (idx_t i = 0; i < base.constraints.size(); i++) {\n \t\tauto &cond = base.constraints[i];\n \t\tswitch (cond->type) {\n@@ -83,6 +84,7 @@ static void BindConstraints(Binder &binder, BoundCreateTableInfo &info) {\n \t\t\t\t\tthrow ParserException(\"table \\\"%s\\\" has more than one primary key\", base.table.c_str());\n \t\t\t\t}\n \t\t\t\thas_primary_key = true;\n+\t\t\t\tprimary_keys = keys;\n \t\t\t}\n \t\t\tinfo.bound_constraints.push_back(make_unique<BoundUniqueConstraint>(keys, unique.is_primary_key));\n \t\t\tbreak;\n@@ -91,6 +93,13 @@ static void BindConstraints(Binder &binder, BoundCreateTableInfo &info) {\n \t\t\tthrow NotImplementedException(\"unrecognized constraint type in bind\");\n \t\t}\n \t}\n+\tif (has_primary_key) {\n+\t\t// if there is a primary key index, also create a NOT NULL constraint for each of the columns\n+\t\tfor (auto &column_index : primary_keys) {\n+\t\t\tbase.constraints.push_back(make_unique<NotNullConstraint>(column_index));\n+\t\t\tinfo.bound_constraints.push_back(make_unique<BoundNotNullConstraint>(column_index));\n+\t\t}\n+\t}\n }\n \n void Binder::BindDefaultValues(vector<ColumnDefinition> &columns, vector<unique_ptr<Expression>> &bound_defaults) {\ndiff --git a/src/planner/binder/statement/bind_update.cpp b/src/planner/binder/statement/bind_update.cpp\nindex d373192187b6..9dd44a36eff9 100644\n--- a/src/planner/binder/statement/bind_update.cpp\n+++ b/src/planner/binder/statement/bind_update.cpp\n@@ -73,7 +73,7 @@ static void BindUpdateConstraints(TableCatalogEntry &table, LogicalGet &get, Log\n \t// delete for the insert, we thus need all the columns to be available, hence we check if the update touches any\n \t// index columns\n \tupdate.is_index_update = false;\n-\tfor (auto &index : table.storage->indexes) {\n+\tfor (auto &index : table.storage->info->indexes) {\n \t\tif (index->IndexIsUpdated(update.columns)) {\n \t\t\tupdate.is_index_update = true;\n \t\t}\ndiff --git a/src/planner/expression_binder/CMakeLists.txt b/src/planner/expression_binder/CMakeLists.txt\nindex b1bf26e2291e..2ea615951374 100644\n--- a/src/planner/expression_binder/CMakeLists.txt\n+++ b/src/planner/expression_binder/CMakeLists.txt\n@@ -1,6 +1,7 @@\n add_library_unity(duckdb_expression_binders\n                   OBJECT\n                   aggregate_binder.cpp\n+                  alter_binder.cpp\n                   check_binder.cpp\n                   constant_binder.cpp\n                   group_binder.cpp\ndiff --git a/src/planner/expression_binder/alter_binder.cpp b/src/planner/expression_binder/alter_binder.cpp\nnew file mode 100644\nindex 000000000000..a62b72197e02\n--- /dev/null\n+++ b/src/planner/expression_binder/alter_binder.cpp\n@@ -0,0 +1,50 @@\n+#include \"duckdb/planner/expression_binder/alter_binder.hpp\"\n+\n+#include \"duckdb/parser/expression/columnref_expression.hpp\"\n+#include \"duckdb/planner/expression/bound_reference_expression.hpp\"\n+\n+using namespace std;\n+\n+namespace duckdb {\n+\n+AlterBinder::AlterBinder(Binder &binder, ClientContext &context, string table, vector<ColumnDefinition> &columns,\n+                         vector<column_t> &bound_columns, SQLType target_type)\n+    : ExpressionBinder(binder, context), table(table), columns(columns), bound_columns(bound_columns) {\n+\tthis->target_type = target_type;\n+}\n+\n+BindResult AlterBinder::BindExpression(ParsedExpression &expr, idx_t depth, bool root_expression) {\n+\tswitch (expr.GetExpressionClass()) {\n+\tcase ExpressionClass::WINDOW:\n+\t\treturn BindResult(\"window functions are not allowed in alter statement\");\n+\tcase ExpressionClass::SUBQUERY:\n+\t\treturn BindResult(\"cannot use subquery in alter statement\");\n+\tcase ExpressionClass::COLUMN_REF:\n+\t\treturn BindColumn((ColumnRefExpression &)expr);\n+\tdefault:\n+\t\treturn ExpressionBinder::BindExpression(expr, depth);\n+\t}\n+}\n+\n+string AlterBinder::UnsupportedAggregateMessage() {\n+\treturn \"aggregate functions are not allowed in alter statement\";\n+}\n+\n+BindResult AlterBinder::BindColumn(ColumnRefExpression &colref) {\n+\tif (!colref.table_name.empty() && colref.table_name != table) {\n+\t\tthrow BinderException(\"Cannot reference table %s from within alter statement for table %s!\",\n+\t\t                      colref.table_name.c_str(), table.c_str());\n+\t}\n+\tfor (idx_t i = 0; i < columns.size(); i++) {\n+\t\tif (colref.column_name == columns[i].name) {\n+\t\t\tbound_columns.push_back(i);\n+\t\t\treturn BindResult(\n+\t\t\t    make_unique<BoundReferenceExpression>(GetInternalType(columns[i].type), bound_columns.size() - 1),\n+\t\t\t    columns[i].type);\n+\t\t}\n+\t}\n+\tthrow BinderException(\"Table does not contain column %s referenced in alter statement!\",\n+\t                      colref.column_name.c_str());\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/planner/operator/logical_get.cpp b/src/planner/operator/logical_get.cpp\nindex 91d3663943a5..0beb5cefc1a5 100644\n--- a/src/planner/operator/logical_get.cpp\n+++ b/src/planner/operator/logical_get.cpp\n@@ -46,7 +46,7 @@ void LogicalGet::ResolveTypes() {\n \n idx_t LogicalGet::EstimateCardinality() {\n \tif (table) {\n-\t\treturn table->storage->cardinality;\n+\t\treturn table->storage->info->cardinality;\n \t} else {\n \t\treturn 1;\n \t}\ndiff --git a/src/storage/column_data.cpp b/src/storage/column_data.cpp\nindex 2cfb5a82d5f3..cf804b8a98fd 100644\n--- a/src/storage/column_data.cpp\n+++ b/src/storage/column_data.cpp\n@@ -7,7 +7,8 @@\n using namespace duckdb;\n using namespace std;\n \n-ColumnData::ColumnData() : persistent_rows(0) {\n+ColumnData::ColumnData(BufferManager &manager, DataTableInfo &table_info)\n+    : table_info(table_info), manager(manager), persistent_rows(0) {\n }\n \n void ColumnData::Initialize(vector<unique_ptr<PersistentSegment>> &segments) {\n@@ -173,6 +174,6 @@ void ColumnData::FetchRow(ColumnFetchState &state, Transaction &transaction, row\n }\n \n void ColumnData::AppendTransientSegment(idx_t start_row) {\n-\tauto new_segment = make_unique<TransientSegment>(*table->storage.buffer_manager, type, start_row);\n+\tauto new_segment = make_unique<TransientSegment>(manager, type, start_row);\n \tdata.AppendSegment(move(new_segment));\n }\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex 0b064349fd81..908e9cf9c813 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -9,6 +9,7 @@\n #include \"duckdb/transaction/transaction.hpp\"\n #include \"duckdb/transaction/transaction_manager.hpp\"\n #include \"duckdb/storage/table/transient_segment.hpp\"\n+#include \"duckdb/storage/storage_manager.hpp\"\n \n using namespace duckdb;\n using namespace std;\n@@ -16,28 +17,159 @@ using namespace chrono;\n \n DataTable::DataTable(StorageManager &storage, string schema, string table, vector<TypeId> types_,\n                      unique_ptr<vector<unique_ptr<PersistentSegment>>[]> data)\n-    : cardinality(0), schema(schema), table(table), types(types_), storage(storage), persistent_manager(*this),\n-      transient_manager(*this) {\n+    : info(make_shared<DataTableInfo>(schema, table)), types(types_), storage(storage),\n+      persistent_manager(make_shared<VersionManager>(*info)), transient_manager(make_shared<VersionManager>(*info)),\n+      is_root(true) {\n \t// set up the segment trees for the column segments\n-\tcolumns = unique_ptr<ColumnData[]>(new ColumnData[types.size()]);\n \tfor (idx_t i = 0; i < types.size(); i++) {\n-\t\tcolumns[i].type = types[i];\n-\t\tcolumns[i].table = this;\n-\t\tcolumns[i].column_idx = i;\n+\t\tauto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);\n+\t\tcolumn_data->type = types[i];\n+\t\tcolumn_data->column_idx = i;\n+\t\tcolumns.push_back(move(column_data));\n \t}\n \n \t// initialize the table with the existing data from disk, if any\n \tif (data && data[0].size() > 0) {\n \t\t// first append all the segments to the set of column segments\n \t\tfor (idx_t i = 0; i < types.size(); i++) {\n-\t\t\tcolumns[i].Initialize(data[i]);\n-\t\t\tif (columns[i].persistent_rows != columns[0].persistent_rows) {\n+\t\t\tcolumns[i]->Initialize(data[i]);\n+\t\t\tif (columns[i]->persistent_rows != columns[0]->persistent_rows) {\n \t\t\t\tthrow Exception(\"Column length mismatch in table load!\");\n \t\t\t}\n \t\t}\n-\t\tpersistent_manager.max_row = columns[0].persistent_rows;\n-\t\ttransient_manager.base_row = persistent_manager.max_row;\n+\t\tpersistent_manager->max_row = columns[0]->persistent_rows;\n+\t\ttransient_manager->base_row = persistent_manager->max_row;\n+\t}\n+}\n+\n+DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value)\n+    : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),\n+      transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {\n+\t// prevent any new tuples from being added to the parent\n+\tlock_guard<mutex> parent_lock(parent.append_lock);\n+\t// this table replaces the previous table, hence the parent is no longer the root DataTable\n+\tparent.is_root = false;\n+\t// add the new column to this DataTable\n+\tauto new_column_type = GetInternalType(new_column.type);\n+\tidx_t new_column_idx = columns.size();\n+\n+\ttypes.push_back(new_column_type);\n+\tauto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);\n+\tcolumn_data->type = new_column_type;\n+\tcolumn_data->column_idx = new_column_idx;\n+\tcolumns.push_back(move(column_data));\n+\n+\t// fill the column with its DEFAULT value, or NULL if none is specified\n+\tidx_t rows_to_write = persistent_manager->max_row + transient_manager->max_row;\n+\tif (rows_to_write > 0) {\n+\t\tExpressionExecutor executor;\n+\t\tDataChunk dummy_chunk;\n+\t\tVector result(new_column_type);\n+\t\tif (!default_value) {\n+\t\t\tFlatVector::Nullmask(result).set();\n+\t\t} else {\n+\t\t\texecutor.AddExpression(*default_value);\n+\t\t}\n+\n+\t\tColumnAppendState state;\n+\t\tcolumns[new_column_idx]->InitializeAppend(state);\n+\t\tfor (idx_t i = 0; i < rows_to_write; i += STANDARD_VECTOR_SIZE) {\n+\t\t\tidx_t rows_in_this_vector = std::min(rows_to_write - i, (idx_t)STANDARD_VECTOR_SIZE);\n+\t\t\tif (default_value) {\n+\t\t\t\tdummy_chunk.SetCardinality(rows_in_this_vector);\n+\t\t\t\texecutor.ExecuteExpression(dummy_chunk, result);\n+\t\t\t}\n+\t\t\tcolumns[new_column_idx]->Append(state, result, rows_in_this_vector);\n+\t\t}\n+\t}\n+\t// also add this column to client local storage\n+\tTransaction::GetTransaction(context).storage.AddColumn(&parent, this, new_column, default_value);\n+}\n+\n+DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)\n+    : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),\n+      transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {\n+\t// prevent any new tuples from being added to the parent\n+\tlock_guard<mutex> parent_lock(parent.append_lock);\n+\t// first check if there are any indexes that exist that point to the removed column\n+\tfor (auto &index : info->indexes) {\n+\t\tfor (auto &column_id : index->column_ids) {\n+\t\t\tif (column_id == removed_column) {\n+\t\t\t\tthrow CatalogException(\"Cannot drop this column: an index depends on it!\");\n+\t\t\t} else if (column_id > removed_column) {\n+\t\t\t\tthrow CatalogException(\"Cannot drop this column: an index depends on a column after it!\");\n+\t\t\t}\n+\t\t}\n \t}\n+\t// this table replaces the previous table, hence the parent is no longer the root DataTable\n+\tparent.is_root = false;\n+\t// erase the column from this DataTable\n+\tassert(removed_column < types.size());\n+\ttypes.erase(types.begin() + removed_column);\n+\tcolumns.erase(columns.begin() + removed_column);\n+}\n+\n+DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, SQLType target_type,\n+                     vector<column_t> bound_columns, Expression &cast_expr)\n+    : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),\n+      transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {\n+\n+\t// prevent any new tuples from being added to the parent\n+\tlock_guard<mutex> parent_lock(parent.append_lock);\n+\t// this table replaces the previous table, hence the parent is no longer the root DataTable\n+\tparent.is_root = false;\n+\t// first check if there are any indexes that exist that point to the changed column\n+\tfor (auto &index : info->indexes) {\n+\t\tfor (auto &column_id : index->column_ids) {\n+\t\t\tif (column_id == changed_idx) {\n+\t\t\t\tthrow CatalogException(\"Cannot change the type of this column: an index depends on it!\");\n+\t\t\t}\n+\t\t}\n+\t}\n+\t// change the type in this DataTable\n+\tauto new_type = GetInternalType(target_type);\n+\ttypes[changed_idx] = new_type;\n+\n+\t// construct a new column data for this type\n+\tauto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);\n+\tcolumn_data->type = new_type;\n+\tcolumn_data->column_idx = changed_idx;\n+\n+\tColumnAppendState append_state;\n+\tcolumn_data->InitializeAppend(append_state);\n+\n+\t// scan the original table, and fill the new column with the transformed value\n+\tauto &transaction = Transaction::GetTransaction(context);\n+\tTableScanState scan_state;\n+\n+\tvector<TypeId> types;\n+\tfor (idx_t i = 0; i < bound_columns.size(); i++) {\n+\t\ttypes.push_back(parent.types[i]);\n+\t}\n+\tparent.InitializeScan(transaction, scan_state, bound_columns, nullptr);\n+\n+\tDataChunk scan_chunk;\n+\tscan_chunk.Initialize(types);\n+\tunordered_map<idx_t, vector<TableFilter>> dummy_filters;\n+\n+\tExpressionExecutor executor;\n+\texecutor.AddExpression(cast_expr);\n+\n+\tVector append_vector(new_type);\n+\twhile (true) {\n+\t\t// scan the table\n+\t\tparent.Scan(transaction, scan_chunk, scan_state, dummy_filters);\n+\t\tif (scan_chunk.size() == 0) {\n+\t\t\tbreak;\n+\t\t}\n+\t\t// execute the expression\n+\t\texecutor.ExecuteExpression(scan_chunk, append_vector);\n+\t\tcolumn_data->Append(append_state, append_vector, scan_chunk.size());\n+\t}\n+\t// also add this column to client local storage\n+\ttransaction.storage.ChangeType(&parent, this, changed_idx, target_type, bound_columns, cast_expr);\n+\n+\tcolumns[changed_idx] = move(column_data);\n }\n \n //===--------------------------------------------------------------------===//\n@@ -50,16 +182,16 @@ void DataTable::InitializeScan(TableScanState &state, vector<column_t> column_id\n \tfor (idx_t i = 0; i < column_ids.size(); i++) {\n \t\tauto column = column_ids[i];\n \t\tif (column != COLUMN_IDENTIFIER_ROW_ID) {\n-\t\t\tcolumns[column].InitializeScan(state.column_scans[i]);\n+\t\t\tcolumns[column]->InitializeScan(state.column_scans[i]);\n \t\t}\n \t}\n \tstate.column_ids = move(column_ids);\n \t// initialize the chunk scan state\n \tstate.offset = 0;\n \tstate.current_persistent_row = 0;\n-\tstate.max_persistent_row = persistent_manager.max_row;\n+\tstate.max_persistent_row = persistent_manager->max_row;\n \tstate.current_transient_row = 0;\n-\tstate.max_transient_row = transient_manager.max_row;\n+\tstate.max_transient_row = transient_manager->max_row;\n \tif (table_filters && table_filters->size() > 0) {\n \t\tstate.adaptive_filter = make_unique<AdaptiveFilter>(*table_filters);\n \t}\n@@ -75,14 +207,14 @@ void DataTable::Scan(Transaction &transaction, DataChunk &result, TableScanState\n                      unordered_map<idx_t, vector<TableFilter>> &table_filters) {\n \t// scan the persistent segments\n \twhile (ScanBaseTable(transaction, result, state, state.current_persistent_row, state.max_persistent_row, 0,\n-\t                     persistent_manager, table_filters)) {\n+\t                     *persistent_manager, table_filters)) {\n \t\tif (result.size() > 0) {\n \t\t\treturn;\n \t\t}\n \t}\n \t// scan the transient segments\n \twhile (ScanBaseTable(transaction, result, state, state.current_transient_row, state.max_transient_row,\n-\t                     persistent_manager.max_row, transient_manager, table_filters)) {\n+\t                     persistent_manager->max_row, *transient_manager, table_filters)) {\n \t\tif (result.size() > 0) {\n \t\t\treturn;\n \t\t}\n@@ -237,7 +369,7 @@ bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, Table\n \t\t\t\tassert(result.data[i].type == ROW_TYPE);\n \t\t\t\tresult.data[i].Sequence(base_row + current_row, 1);\n \t\t\t} else {\n-\t\t\t\tcolumns[column].Scan(transaction, state.column_scans[i], result.data[i]);\n+\t\t\t\tcolumns[column]->Scan(transaction, state.column_scans[i], result.data[i]);\n \t\t\t}\n \t\t}\n \t} else {\n@@ -253,8 +385,8 @@ bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, Table\n \t\tauto start_time = high_resolution_clock::now();\n \t\tfor (idx_t i = 0; i < table_filters.size(); i++) {\n \t\t\tauto tf_idx = state.adaptive_filter->permutation[i];\n-\t\t\tcolumns[tf_idx].Select(transaction, state.column_scans[tf_idx], result.data[tf_idx], sel,\n-\t\t\t                       approved_tuple_count, table_filters[tf_idx]);\n+\t\t\tcolumns[tf_idx]->Select(transaction, state.column_scans[tf_idx], result.data[tf_idx], sel,\n+\t\t\t                        approved_tuple_count, table_filters[tf_idx]);\n \t\t}\n \t\tfor (auto &table_filter : table_filters) {\n \t\t\tresult.data[table_filter.first].Slice(sel, approved_tuple_count);\n@@ -271,8 +403,8 @@ bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, Table\n \t\t\t\t\t\tresult_data[sel_idx] = base_row + current_row + sel.get_index(sel_idx);\n \t\t\t\t\t}\n \t\t\t\t} else {\n-\t\t\t\t\tcolumns[column].FilterScan(transaction, state.column_scans[i], result.data[i], sel,\n-\t\t\t\t\t                           approved_tuple_count);\n+\t\t\t\t\tcolumns[column]->FilterScan(transaction, state.column_scans[i], result.data[i], sel,\n+\t\t\t\t\t                            approved_tuple_count);\n \t\t\t\t}\n \t\t\t}\n \t\t}\n@@ -353,7 +485,7 @@ void DataTable::Fetch(Transaction &transaction, DataChunk &result, vector<column\n \t\t\t// regular column: fetch data from the base column\n \t\t\tfor (idx_t i = 0; i < count; i++) {\n \t\t\t\tauto row_id = rows[i];\n-\t\t\t\tcolumns[column].FetchRow(state.fetch_state, transaction, row_id, result.data[col_idx], i);\n+\t\t\t\tcolumns[column]->FetchRow(state.fetch_state, transaction, row_id, result.data[col_idx], i);\n \t\t\t}\n \t\t}\n \t}\n@@ -363,8 +495,8 @@ idx_t DataTable::FetchRows(Transaction &transaction, Vector &row_identifiers, id\n \tassert(row_identifiers.type == ROW_TYPE);\n \n \t// obtain a read lock on the version managers\n-\tauto l1 = persistent_manager.lock.GetSharedLock();\n-\tauto l2 = transient_manager.lock.GetSharedLock();\n+\tauto l1 = persistent_manager->lock.GetSharedLock();\n+\tauto l2 = transient_manager->lock.GetSharedLock();\n \n \t// now iterate over the row ids and figure out which rows to use\n \tidx_t count = 0;\n@@ -373,12 +505,12 @@ idx_t DataTable::FetchRows(Transaction &transaction, Vector &row_identifiers, id\n \tfor (idx_t i = 0; i < fetch_count; i++) {\n \t\tauto row_id = row_ids[i];\n \t\tbool use_row;\n-\t\tif ((idx_t)row_id < persistent_manager.max_row) {\n+\t\tif ((idx_t)row_id < persistent_manager->max_row) {\n \t\t\t// persistent row: use persistent manager\n-\t\t\tuse_row = persistent_manager.Fetch(transaction, row_id);\n+\t\t\tuse_row = persistent_manager->Fetch(transaction, row_id);\n \t\t} else {\n \t\t\t// transient row: use transient manager\n-\t\t\tuse_row = transient_manager.Fetch(transaction, row_id);\n+\t\t\tuse_row = transient_manager->Fetch(transaction, row_id);\n \t\t}\n \t\tif (use_row) {\n \t\t\t// row is not deleted; use the row\n@@ -435,7 +567,7 @@ void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, DataChunk &chu\n \t\t}\n \t\tcase ConstraintType::UNIQUE: {\n \t\t\t//! check whether or not the chunk can be inserted into the indexes\n-\t\t\tfor (auto &index : indexes) {\n+\t\t\tfor (auto &index : info->indexes) {\n \t\t\t\tindex->VerifyAppend(chunk);\n \t\t\t}\n \t\t\tbreak;\n@@ -454,6 +586,9 @@ void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChu\n \tif (chunk.column_count() != table.columns.size()) {\n \t\tthrow CatalogException(\"Mismatch in column count for append\");\n \t}\n+\tif (!is_root) {\n+\t\tthrow TransactionException(\"Transaction conflict: adding entries to a table that has been altered!\");\n+\t}\n \n \tchunk.Verify();\n \n@@ -468,32 +603,36 @@ void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChu\n void DataTable::InitializeAppend(TableAppendState &state) {\n \t// obtain the append lock for this table\n \tstate.append_lock = unique_lock<mutex>(append_lock);\n+\tif (!is_root) {\n+\t\tthrow TransactionException(\"Transaction conflict: adding entries to a table that has been altered!\");\n+\t}\n \t// obtain locks on all indexes for the table\n-\tstate.index_locks = unique_ptr<IndexLock[]>(new IndexLock[indexes.size()]);\n-\tfor (idx_t i = 0; i < indexes.size(); i++) {\n-\t\tindexes[i]->InitializeLock(state.index_locks[i]);\n+\tstate.index_locks = unique_ptr<IndexLock[]>(new IndexLock[info->indexes.size()]);\n+\tfor (idx_t i = 0; i < info->indexes.size(); i++) {\n+\t\tinfo->indexes[i]->InitializeLock(state.index_locks[i]);\n \t}\n \t// for each column, initialize the append state\n \tstate.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[types.size()]);\n \tfor (idx_t i = 0; i < types.size(); i++) {\n-\t\tcolumns[i].InitializeAppend(state.states[i]);\n+\t\tcolumns[i]->InitializeAppend(state.states[i]);\n \t}\n-\tstate.row_start = transient_manager.max_row;\n+\tstate.row_start = transient_manager->max_row;\n \tstate.current_row = state.row_start;\n }\n \n void DataTable::Append(Transaction &transaction, transaction_t commit_id, DataChunk &chunk, TableAppendState &state) {\n+\tassert(is_root);\n \tassert(chunk.column_count() == types.size());\n \tchunk.Verify();\n \n \t// set up the inserted info in the version manager\n-\ttransient_manager.Append(transaction, state.current_row, chunk.size(), commit_id);\n+\ttransient_manager->Append(transaction, state.current_row, chunk.size(), commit_id);\n \n \t// append the physical data to each of the entries\n \tfor (idx_t i = 0; i < types.size(); i++) {\n-\t\tcolumns[i].Append(state.states[i], chunk.data[i], chunk.size());\n+\t\tcolumns[i]->Append(state.states[i], chunk.data[i], chunk.size());\n \t}\n-\tcardinality += chunk.size();\n+\tinfo->cardinality += chunk.size();\n \tstate.current_row += chunk.size();\n }\n \n@@ -502,22 +641,24 @@ void DataTable::RevertAppend(TableAppendState &state) {\n \t\t// nothing to revert!\n \t\treturn;\n \t}\n+\tassert(is_root);\n \t// revert changes in the base columns\n \tfor (idx_t i = 0; i < types.size(); i++) {\n-\t\tcolumns[i].RevertAppend(state.row_start);\n+\t\tcolumns[i]->RevertAppend(state.row_start);\n \t}\n \t// adjust the cardinality\n-\tcardinality -= state.current_row - state.row_start;\n-\ttransient_manager.max_row = state.row_start;\n+\tinfo->cardinality -= state.current_row - state.row_start;\n+\ttransient_manager->max_row = state.row_start;\n \t// revert changes in the transient manager\n-\ttransient_manager.RevertAppend(state.row_start, state.current_row);\n+\ttransient_manager->RevertAppend(state.row_start, state.current_row);\n }\n \n //===--------------------------------------------------------------------===//\n // Indexes\n //===--------------------------------------------------------------------===//\n bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {\n-\tif (indexes.size() == 0) {\n+\tassert(is_root);\n+\tif (info->indexes.size() == 0) {\n \t\treturn true;\n \t}\n \t// first generate the vector of row identifiers\n@@ -526,8 +667,8 @@ bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t\n \n \tidx_t failed_index = INVALID_INDEX;\n \t// now append the entries to the indices\n-\tfor (idx_t i = 0; i < indexes.size(); i++) {\n-\t\tif (!indexes[i]->Append(state.index_locks[i], chunk, row_identifiers)) {\n+\tfor (idx_t i = 0; i < info->indexes.size(); i++) {\n+\t\tif (!info->indexes[i]->Append(state.index_locks[i], chunk, row_identifiers)) {\n \t\t\tfailed_index = i;\n \t\t\tbreak;\n \t\t}\n@@ -536,7 +677,7 @@ bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t\n \t\t// constraint violation!\n \t\t// remove any appended entries from previous indexes (if any)\n \t\tfor (idx_t i = 0; i < failed_index; i++) {\n-\t\t\tindexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);\n+\t\t\tinfo->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);\n \t\t}\n \t\treturn false;\n \t}\n@@ -544,7 +685,8 @@ bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t\n }\n \n void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {\n-\tif (indexes.size() == 0) {\n+\tassert(is_root);\n+\tif (info->indexes.size() == 0) {\n \t\treturn;\n \t}\n \t// first generate the vector of row identifiers\n@@ -556,12 +698,14 @@ void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row\n }\n \n void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {\n-\tfor (idx_t i = 0; i < indexes.size(); i++) {\n-\t\tindexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);\n+\tassert(is_root);\n+\tfor (idx_t i = 0; i < info->indexes.size(); i++) {\n+\t\tinfo->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);\n \t}\n }\n \n void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {\n+\tassert(is_root);\n \tauto row_ids = FlatVector::GetData<row_t>(row_identifiers);\n \t// create a selection vector from the row_ids\n \tSelectionVector sel(STANDARD_VECTOR_SIZE);\n@@ -575,11 +719,11 @@ void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {\n \t// FIXME: we do not need to fetch all columns, only the columns required by the indices!\n \tauto states = unique_ptr<ColumnScanState[]>(new ColumnScanState[types.size()]);\n \tfor (idx_t i = 0; i < types.size(); i++) {\n-\t\tcolumns[i].Fetch(states[i], row_ids[0], result.data[i]);\n+\t\tcolumns[i]->Fetch(states[i], row_ids[0], result.data[i]);\n \t}\n \tresult.Slice(sel, count);\n-\tfor (idx_t i = 0; i < indexes.size(); i++) {\n-\t\tindexes[i]->Delete(result, row_identifiers);\n+\tfor (idx_t i = 0; i < info->indexes.size(); i++) {\n+\t\tinfo->indexes[i]->Delete(result, row_identifiers);\n \t}\n }\n \n@@ -601,12 +745,12 @@ void DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector\n \tif (first_id >= MAX_ROW_ID) {\n \t\t// deletion is in transaction-local storage: push delete into local chunk collection\n \t\ttransaction.storage.Delete(this, row_identifiers, count);\n-\t} else if ((idx_t)first_id < persistent_manager.max_row) {\n+\t} else if ((idx_t)first_id < persistent_manager->max_row) {\n \t\t// deletion is in persistent storage: delete in the persistent version manager\n-\t\tpersistent_manager.Delete(transaction, row_identifiers, count);\n+\t\tpersistent_manager->Delete(transaction, this, row_identifiers, count);\n \t} else {\n \t\t// deletion is in transient storage: delete in the persistent version manager\n-\t\ttransient_manager.Delete(transaction, row_identifiers, count);\n+\t\ttransient_manager->Delete(transaction, this, row_identifiers, count);\n \t}\n }\n \n@@ -682,8 +826,8 @@ void DataTable::VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chu\n \t// update should not be called for indexed columns!\n \t// instead update should have been rewritten to delete + update on higher layer\n #ifdef DEBUG\n-\tfor (idx_t i = 0; i < indexes.size(); i++) {\n-\t\tassert(!indexes[i]->IndexIsUpdated(column_ids));\n+\tfor (idx_t i = 0; i < info->indexes.size(); i++) {\n+\t\tassert(!info->indexes[i]->IndexIsUpdated(column_ids));\n \t}\n #endif\n }\n@@ -716,7 +860,7 @@ void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector\n \t\tauto column = column_ids[i];\n \t\tassert(column != COLUMN_IDENTIFIER_ROW_ID);\n \n-\t\tcolumns[column].Update(transaction, updates.data[i], row_ids, updates.size());\n+\t\tcolumns[column]->Update(transaction, updates.data[i], row_ids, updates.size());\n \t}\n }\n \n@@ -727,8 +871,8 @@ void DataTable::InitializeCreateIndexScan(CreateIndexScanState &state, vector<co\n \t// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan\n \tstate.append_lock = unique_lock<mutex>(append_lock);\n \t// get a read lock on the VersionManagers to prevent any further deletions\n-\tstate.locks.push_back(persistent_manager.lock.GetSharedLock());\n-\tstate.locks.push_back(transient_manager.lock.GetSharedLock());\n+\tstate.locks.push_back(persistent_manager->lock.GetSharedLock());\n+\tstate.locks.push_back(transient_manager->lock.GetSharedLock());\n \n \tInitializeScan(state, column_ids);\n }\n@@ -763,7 +907,7 @@ bool DataTable::ScanCreateIndex(CreateIndexScanState &state, DataChunk &result,\n \t\t\tresult.data[i].Sequence(base_row + current_row, 1);\n \t\t} else {\n \t\t\t// scan actual base column\n-\t\t\tcolumns[column].IndexScan(state.column_scans[i], result.data[i]);\n+\t\t\tcolumns[column]->IndexScan(state.column_scans[i], result.data[i]);\n \t\t}\n \t}\n \tresult.SetCardinality(count);\n@@ -790,6 +934,10 @@ void DataTable::AddIndex(unique_ptr<Index> index, vector<unique_ptr<Expression>>\n \tCreateIndexScanState state;\n \tInitializeCreateIndexScan(state, column_ids);\n \n+\tif (!is_root) {\n+\t\tthrow TransactionException(\"Transaction conflict: cannot add an index to a table that has been altered!\");\n+\t}\n+\n \t// now start incrementally building the index\n \tIndexLock lock;\n \tindex->InitializeLock(lock);\n@@ -811,9 +959,5 @@ void DataTable::AddIndex(unique_ptr<Index> index, vector<unique_ptr<Expression>>\n \t\t\tthrow ConstraintException(\"Cant create unique index, table contains duplicate data on indexed column(s)\");\n \t\t}\n \t}\n-\tindexes.push_back(move(index));\n-}\n-\n-bool DataTable::IsTemporary() {\n-\treturn schema.compare(TEMP_SCHEMA) == 0;\n+\tinfo->indexes.push_back(move(index));\n }\ndiff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp\nindex 2f931eb2576a..9f9b4a1dc4bc 100644\n--- a/src/storage/local_storage.cpp\n+++ b/src/storage/local_storage.cpp\n@@ -9,7 +9,7 @@ using namespace duckdb;\n using namespace std;\n \n LocalTableStorage::LocalTableStorage(DataTable &table) : max_row(0) {\n-\tfor (auto &index : table.indexes) {\n+\tfor (auto &index : table.info->indexes) {\n \t\tassert(index->type == IndexType::ART);\n \t\tauto &art = (ART &)*index;\n \t\tif (art.is_unique) {\n@@ -112,11 +112,11 @@ void LocalStorage::Scan(LocalScanState &state, const vector<column_t> &column_id\n \t\t\t}\n \t\t}\n \t}\n-\tif (count == 0){\n-\t    // all entries in this chunk were filtered:: Continue on next chunk\n-\t\t\tstate.chunk_index++;\n-\t\t\tScan(state, column_ids, result, table_filters);\n-\t\t\treturn;\n+\tif (count == 0) {\n+\t\t// all entries in this chunk were filtered:: Continue on next chunk\n+\t\tstate.chunk_index++;\n+\t\tScan(state, column_ids, result, table_filters);\n+\t\treturn;\n \t}\n \tif (count == chunk_count) {\n \t\tresult.SetCardinality(count);\n@@ -299,8 +299,8 @@ void LocalStorage::Commit(LocalStorage::CommitState &commit_state, Transaction &\n \t\tcommit_state.append_states[table] = move(append_state_ptr);\n \t\ttable->InitializeAppend(append_state);\n \n-\t\tif (log && !table->IsTemporary()) {\n-\t\t\tlog->WriteSetTable(table->schema, table->table);\n+\t\tif (log && !table->info->IsTemporary()) {\n+\t\t\tlog->WriteSetTable(table->info->schema, table->info->table);\n \t\t}\n \n \t\t// scan all chunks in this storage\n@@ -313,7 +313,7 @@ void LocalStorage::Commit(LocalStorage::CommitState &commit_state, Transaction &\n \t\t\t// append to base table\n \t\t\ttable->Append(transaction, commit_id, chunk, append_state);\n \t\t\t// if there is a WAL, write the chunk to there as well\n-\t\t\tif (log && !table->IsTemporary()) {\n+\t\t\tif (log && !table->info->IsTemporary()) {\n \t\t\t\tlog->WriteInsert(chunk);\n \t\t\t}\n \t\t\treturn true;\n@@ -331,7 +331,7 @@ void LocalStorage::RevertCommit(LocalStorage::CommitState &commit_state) {\n \t\tauto table = entry.first;\n \t\tauto storage = table_storage[table].get();\n \t\tauto &append_state = *entry.second;\n-\t\tif (table->indexes.size() > 0 && !(table->schema == \"temp\")) {\n+\t\tif (table->info->indexes.size() > 0 && !table->info->IsTemporary()) {\n \t\t\trow_t current_row = append_state.row_start;\n \t\t\t// remove the data from the indexes, if there are any indexes\n \t\t\tScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {\n@@ -350,3 +350,48 @@ void LocalStorage::RevertCommit(LocalStorage::CommitState &commit_state) {\n \t\ttable->RevertAppend(*entry.second);\n \t}\n }\n+\n+void LocalStorage::AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column,\n+                             Expression *default_value) {\n+\t// check if there are any pending appends for the old version of the table\n+\tauto entry = table_storage.find(old_dt);\n+\tif (entry == table_storage.end()) {\n+\t\treturn;\n+\t}\n+\t// take over the storage from the old entry\n+\tauto new_storage = move(entry->second);\n+\n+\t// now add the new column filled with the default value to all chunks\n+\tauto new_column_type = GetInternalType(new_column.type);\n+\tExpressionExecutor executor;\n+\tDataChunk dummy_chunk;\n+\tif (default_value) {\n+\t\texecutor.AddExpression(*default_value);\n+\t}\n+\n+\tnew_storage->collection.types.push_back(new_column_type);\n+\tfor (idx_t chunk_idx = 0; chunk_idx < new_storage->collection.chunks.size(); chunk_idx++) {\n+\t\tauto &chunk = new_storage->collection.chunks[chunk_idx];\n+\t\tVector result(new_column_type);\n+\t\tif (default_value) {\n+\t\t\tdummy_chunk.SetCardinality(chunk->size());\n+\t\t\texecutor.ExecuteExpression(dummy_chunk, result);\n+\t\t} else {\n+\t\t\tFlatVector::Nullmask(result).set();\n+\t\t}\n+\t\tchunk->data.push_back(move(result));\n+\t}\n+\n+\ttable_storage.erase(entry);\n+\ttable_storage[new_dt] = move(new_storage);\n+}\n+\n+void LocalStorage::ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, SQLType target_type,\n+                              vector<column_t> bound_columns, Expression &cast_expr) {\n+\t// check if there are any pending appends for the old version of the table\n+\tauto entry = table_storage.find(old_dt);\n+\tif (entry == table_storage.end()) {\n+\t\treturn;\n+\t}\n+\tthrow NotImplementedException(\"FIXME: ALTER TYPE with transaction local data not currently supported\");\n+}\ndiff --git a/src/storage/table/version_manager.cpp b/src/storage/table/version_manager.cpp\nindex 2c3cfc2a6e75..a8f8d070e08d 100644\n--- a/src/storage/table/version_manager.cpp\n+++ b/src/storage/table/version_manager.cpp\n@@ -36,13 +36,14 @@ bool VersionManager::Fetch(Transaction &transaction, idx_t row) {\n \n class VersionDeleteState {\n public:\n-\tVersionDeleteState(VersionManager &manager, Transaction &transaction, idx_t base_row)\n-\t    : manager(manager), transaction(transaction), current_info(nullptr), current_chunk((idx_t)-1), count(0),\n-\t      base_row(base_row) {\n+\tVersionDeleteState(VersionManager &manager, Transaction &transaction, DataTable *table, idx_t base_row)\n+\t    : manager(manager), transaction(transaction), table(table), current_info(nullptr), current_chunk((idx_t)-1),\n+\t      count(0), base_row(base_row) {\n \t}\n \n \tVersionManager &manager;\n \tTransaction &transaction;\n+\tDataTable *table;\n \tChunkInfo *current_info;\n \tidx_t current_chunk;\n \trow_t rows[STANDARD_VECTOR_SIZE];\n@@ -55,8 +56,8 @@ class VersionDeleteState {\n \tvoid Flush();\n };\n \n-void VersionManager::Delete(Transaction &transaction, Vector &row_ids, idx_t count) {\n-\tVersionDeleteState del_state(*this, transaction, base_row);\n+void VersionManager::Delete(Transaction &transaction, DataTable *table, Vector &row_ids, idx_t count) {\n+\tVersionDeleteState del_state(*this, transaction, table, base_row);\n \n \tVectorData rdata;\n \trow_ids.Orrify(count, rdata);\n@@ -105,7 +106,7 @@ void VersionDeleteState::Flush() {\n \t// delete in the current info\n \tcurrent_info->Delete(transaction, rows, count);\n \t// now push the delete into the undo buffer\n-\ttransaction.PushDelete(current_info, rows, count, base_row + chunk_row);\n+\ttransaction.PushDelete(table, current_info, rows, count, base_row + chunk_row);\n \tcount = 0;\n }\n \ndiff --git a/src/transaction/CMakeLists.txt b/src/transaction/CMakeLists.txt\nindex e9039f8b852c..7e72327bd68e 100644\n--- a/src/transaction/CMakeLists.txt\n+++ b/src/transaction/CMakeLists.txt\n@@ -1,7 +1,6 @@\n add_library_unity(duckdb_transaction\n                   OBJECT\n                   undo_buffer.cpp\n-                  delete_info.cpp\n                   transaction_context.cpp\n                   transaction.cpp\n                   transaction_manager.cpp\ndiff --git a/src/transaction/cleanup_state.cpp b/src/transaction/cleanup_state.cpp\nindex a991ab28ce53..42ae200023e7 100644\n--- a/src/transaction/cleanup_state.cpp\n+++ b/src/transaction/cleanup_state.cpp\n@@ -56,8 +56,8 @@ void CleanupState::CleanupUpdate(UpdateInfo *info) {\n }\n \n void CleanupState::CleanupDelete(DeleteInfo *info) {\n-\tauto version_table = &info->GetTable();\n-\tif (version_table->indexes.size() == 0) {\n+\tauto version_table = info->table;\n+\tif (version_table->info->indexes.size() == 0) {\n \t\t// this table has no indexes: no cleanup to be done\n \t\treturn;\n \t}\ndiff --git a/src/transaction/commit_state.cpp b/src/transaction/commit_state.cpp\nindex b3b5317938c5..3fe5ac85f640 100644\n--- a/src/transaction/commit_state.cpp\n+++ b/src/transaction/commit_state.cpp\n@@ -12,14 +12,14 @@ using namespace duckdb;\n using namespace std;\n \n CommitState::CommitState(transaction_t commit_id, WriteAheadLog *log)\n-    : log(log), commit_id(commit_id), current_table(nullptr) {\n+    : log(log), commit_id(commit_id), current_table_info(nullptr) {\n }\n \n-void CommitState::SwitchTable(DataTable *table, UndoFlags new_op) {\n-\tif (current_table != table) {\n+void CommitState::SwitchTable(DataTableInfo *table_info, UndoFlags new_op) {\n+\tif (current_table_info != table_info) {\n \t\t// write the current table to the log\n-\t\tlog->WriteSetTable(table->schema, table->table);\n-\t\tcurrent_table = table;\n+\t\tlog->WriteSetTable(table_info->schema, table_info->table);\n+\t\tcurrent_table_info = table_info;\n \t}\n }\n \n@@ -91,7 +91,7 @@ void CommitState::WriteCatalogEntry(CatalogEntry *entry, data_ptr_t dataptr) {\n void CommitState::WriteDelete(DeleteInfo *info) {\n \tassert(log);\n \t// switch to the current table, if necessary\n-\tSwitchTable(&info->GetTable(), UndoFlags::DELETE_TUPLE);\n+\tSwitchTable(info->table->info.get(), UndoFlags::DELETE_TUPLE);\n \n \tif (!delete_chunk) {\n \t\tdelete_chunk = make_unique<DataChunk>();\n@@ -109,7 +109,7 @@ void CommitState::WriteDelete(DeleteInfo *info) {\n void CommitState::WriteUpdate(UpdateInfo *info) {\n \tassert(log);\n \t// switch to the current table, if necessary\n-\tSwitchTable(info->column_data->table, UndoFlags::UPDATE_TUPLE);\n+\tSwitchTable(&info->column_data->table_info, UndoFlags::UPDATE_TUPLE);\n \n \tupdate_chunk = make_unique<DataChunk>();\n \tvector<TypeId> update_types = {info->column_data->type, ROW_TYPE};\n@@ -149,8 +149,8 @@ template <bool HAS_LOG> void CommitState::CommitEntry(UndoFlags type, data_ptr_t\n \tcase UndoFlags::DELETE_TUPLE: {\n \t\t// deletion:\n \t\tauto info = (DeleteInfo *)data;\n-\t\tinfo->GetTable().cardinality -= info->count;\n-\t\tif (HAS_LOG && !info->GetTable().IsTemporary()) {\n+\t\tinfo->table->info->cardinality -= info->count;\n+\t\tif (HAS_LOG && !info->table->info->IsTemporary()) {\n \t\t\tWriteDelete(info);\n \t\t}\n \t\t// mark the tuples as committed\n@@ -160,7 +160,7 @@ template <bool HAS_LOG> void CommitState::CommitEntry(UndoFlags type, data_ptr_t\n \tcase UndoFlags::UPDATE_TUPLE: {\n \t\t// update:\n \t\tauto info = (UpdateInfo *)data;\n-\t\tif (HAS_LOG && !info->column_data->table->IsTemporary()) {\n+\t\tif (HAS_LOG && !info->column_data->table_info.IsTemporary()) {\n \t\t\tWriteUpdate(info);\n \t\t}\n \t\tinfo->version_number = commit_id;\n@@ -184,7 +184,7 @@ void CommitState::RevertCommit(UndoFlags type, data_ptr_t data) {\n \tcase UndoFlags::DELETE_TUPLE: {\n \t\t// deletion:\n \t\tauto info = (DeleteInfo *)data;\n-\t\tinfo->GetTable().cardinality += info->count;\n+\t\tinfo->table->info->cardinality += info->count;\n \t\t// revert the commit by writing the (uncommitted) transaction_id back into the version info\n \t\tinfo->vinfo->CommitDelete(transaction_id, info->rows, info->count);\n \t\tbreak;\ndiff --git a/src/transaction/delete_info.cpp b/src/transaction/delete_info.cpp\ndeleted file mode 100644\nindex 3ba3a013221f..000000000000\n--- a/src/transaction/delete_info.cpp\n+++ /dev/null\n@@ -1,10 +0,0 @@\n-#include \"duckdb/transaction/delete_info.hpp\"\n-#include \"duckdb/storage/table/chunk_info.hpp\"\n-#include \"duckdb/storage/table/version_manager.hpp\"\n-\n-using namespace duckdb;\n-using namespace std;\n-\n-DataTable &DeleteInfo::GetTable() {\n-\treturn vinfo->manager.table;\n-}\ndiff --git a/src/transaction/transaction.cpp b/src/transaction/transaction.cpp\nindex 072c431ce711..1e3451494cf0 100644\n--- a/src/transaction/transaction.cpp\n+++ b/src/transaction/transaction.cpp\n@@ -38,10 +38,11 @@ void Transaction::PushCatalogEntry(CatalogEntry *entry, data_ptr_t extra_data, i\n \t}\n }\n \n-void Transaction::PushDelete(ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row) {\n+void Transaction::PushDelete(DataTable *table, ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row) {\n \tauto delete_info =\n \t    (DeleteInfo *)undo_buffer.CreateEntry(UndoFlags::DELETE_TUPLE, sizeof(DeleteInfo) + sizeof(row_t) * count);\n \tdelete_info->vinfo = vinfo;\n+\tdelete_info->table = table;\n \tdelete_info->count = count;\n \tdelete_info->base_row = base_row;\n \tmemcpy(delete_info->rows, rows, sizeof(row_t) * count);\ndiff --git a/tools/pythonpkg/duckdb_python.cpp b/tools/pythonpkg/duckdb_python.cpp\nindex c89cb0f471b4..859d0143339f 100644\n--- a/tools/pythonpkg/duckdb_python.cpp\n+++ b/tools/pythonpkg/duckdb_python.cpp\n@@ -33,6 +33,12 @@ struct DateConvert {\n \t}\n };\n \n+struct TimeConvert {\n+\ttemplate <class DUCKDB_T, class NUMPY_T> static py::str convert_value(time_t val) {\n+\t\treturn py::str(duckdb::Time::ToString(val).c_str());\n+\t}\n+};\n+\n struct StringConvert {\n \ttemplate <class DUCKDB_T, class NUMPY_T> static py::str convert_value(string_t val) {\n \t\treturn py::str(val.GetData());\n@@ -299,7 +305,16 @@ struct DuckDBPyResult {\n \n \t\t\t\tbreak;\n \t\t\t}\n-\n+\t\t\tcase SQLTypeId::TIME: {\n+\t\t\t\tif (result->types[col_idx] != TypeId::INT32) {\n+\t\t\t\t\tthrow runtime_error(\"expected int32 for time\");\n+\t\t\t\t}\n+\t\t\t\tint32_t hour, min, sec, msec;\n+\t\t\t\tauto time = val.GetValue<int32_t>();\n+\t\t\t\tduckdb::Time::Convert(time, hour, min, sec, msec);\n+\t\t\t\tres[col_idx] = PyTime_FromTime(hour, min, sec, msec * 1000);\n+\t\t\t\tbreak;\n+\t\t\t}\n \t\t\tcase SQLTypeId::DATE: {\n \t\t\t\tif (result->types[col_idx] != TypeId::INT32) {\n \t\t\t\t\tthrow runtime_error(\"expected int32 for date\");\n@@ -379,7 +394,10 @@ struct DuckDBPyResult {\n \t\t\t\tcol_res = duckdb_py_convert::fetch_column<date_t, int64_t, duckdb_py_convert::DateConvert>(\n \t\t\t\t    \"datetime64[s]\", mres->collection, col_idx);\n \t\t\t\tbreak;\n-\n+\t\t\tcase SQLTypeId::TIME:\n+\t\t\t\tcol_res = duckdb_py_convert::fetch_column<time_t, py::str, duckdb_py_convert::TimeConvert>(\n+\t\t\t\t    \"object\", mres->collection, col_idx);\n+\t\t\t\tbreak;\n \t\t\tcase SQLTypeId::VARCHAR:\n \t\t\t\tcol_res = duckdb_py_convert::fetch_column<string_t, py::str, duckdb_py_convert::StringConvert>(\n \t\t\t\t    \"object\", mres->collection, col_idx);\n",
  "test_patch": "diff --git a/scripts/test_compile.py b/scripts/test_compile.py\nindex 786c4361eb57..51827c6ef391 100644\n--- a/scripts/test_compile.py\n+++ b/scripts/test_compile.py\n@@ -1,20 +1,51 @@\n-import os, sys, amalgamation, pickle\n+import os\n+import sys\n+import amalgamation\n+import pickle\n+import subprocess\n \n-# where to cache which files have already been compiled, only used for --compile --resume\n+# where to cache which files have already been compiled\n cache_file = 'amalgamation.cache'\n+ignored_files = ['utf8proc_data.cpp']\n \n-resume = False\n+RESUME_AUTO = 0\n+RESUME_ALWAYS = 1\n+RESUME_NEVER = 2\n \n+# resume behavior\n+# by default, we resume if the previous test_compile was run on the same commit hash as this one\n+resume = RESUME_AUTO\n for arg in sys.argv:\n \tif arg == '--resume':\n-\t\tresume = True\n+\t\tresume = RESUME_ALWAYS\n+\telif arg == '--restart':\n+\t\tcache = RESUME_NEVER\n \n-if not resume:\n+if resume == RESUME_NEVER:\n \ttry:\n \t\tos.remove(cache_file)\n \texcept:\n \t\tpass\n \n+def get_git_hash():\n+\tproc = subprocess.Popen(['git', 'rev-parse', 'HEAD'], stdout=subprocess.PIPE)\n+\treturn proc.stdout.read().strip()\n+\n+current_hash = get_git_hash()\n+\n+# load the cache, and check the commit hash\n+try:\n+\twith open(cache_file, 'rb') as cf:\n+\t\tcache = pickle.load(cf)\n+\tif resume == RESUME_AUTO:\n+\t\t# auto resume, check\n+\t\tif cache['commit_hash'] != current_hash:\n+\t\t\tcache = {}\n+except:\n+\tcache = {}\n+\n+cache['commit_hash'] = current_hash\n+\n def try_compilation(fpath, cache):\n \tif fpath in cache:\n \t\treturn\n@@ -32,7 +63,7 @@ def compile_dir(dir, cache):\n \tfiles = os.listdir(dir)\n \tfiles.sort()\n \tfor fname in files:\n-\t\tif fname in amalgamation.excluded_compilation_files:\n+\t\tif fname in amalgamation.excluded_compilation_files or fname in ignored_files:\n \t\t\tcontinue\n \t\tfpath = os.path.join(dir, fname)\n \t\tif os.path.isdir(fpath):\n@@ -40,14 +71,7 @@ def compile_dir(dir, cache):\n \t\telif fname.endswith('.cpp') or fname.endswith('.hpp') or fname.endswith('.c') or fname.endswith('.cc'):\n \t\t\ttry_compilation(fpath, cache)\n \n-# compilation pass only\n # compile all files in the src directory (including headers!) individually\n-try:\n-\twith open(cache_file, 'rb') as cf:\n-\t\tcache = pickle.load(cf)\n-except:\n-\tcache = {}\n-\n for cdir in amalgamation.compile_directories:\n \tcompile_dir(cdir, cache)\n \ndiff --git a/test/rigger/test_rigger.cpp b/test/rigger/test_rigger.cpp\nindex 4dd1be33e206..37644302d706 100644\n--- a/test/rigger/test_rigger.cpp\n+++ b/test/rigger/test_rigger.cpp\n@@ -630,9 +630,9 @@ TEST_CASE(\"Tests found by Rigger\", \"[rigger]\") {\n \t\tresult = con.Query(\"SELECT (- 41756167 + '1969-12-11 032657' ::DATE)::VARCHAR;\");\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {\"112356-06-10 (BC)\"}));\n \t}\n-\tSECTION(\"592\"){\n-\t    // Expression with LIKE and comparison causes an assertion failure\n-\t    REQUIRE_NO_FAIL(con.Query(\"CREATE TABLE t0(c0 VARCHAR);\"));\n+\tSECTION(\"592\") {\n+\t\t// Expression with LIKE and comparison causes an assertion failure\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE t0(c0 VARCHAR);\"));\n \t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO t0 VALUES (0);\"));\n \t\tresult = con.Query(\"SELECT * FROM t0 WHERE c0 LIKE '' AND c0 < true;\");\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {}));\n@@ -666,4 +666,16 @@ TEST_CASE(\"Tests found by Rigger\", \"[rigger]\") {\n \t\tresult = con.Query(\"SELECT * FROM t0 WHERE 'a' BETWEEN c0 AND c1 COLLATE NOACCENT.NOCASE;\");\n \t\tREQUIRE(CHECK_COLUMN(result, 0, {}));\n \t}\n+\tSECTION(\"609\") {\n+\t\t// Incorrect result for MIN() on expression involving rowid\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE t0(c0 INT, c1 INT);\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO t0(c0) VALUES (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), \"\n+\t\t                          \"(0), (0), (0), (0), (0),  (0), (0), (0), (0), (0), (0), (NULL), (NULL);\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE INDEX b ON t0(c1);\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"UPDATE t0 SET c1 = NULL;\"));\n+\t\tresult = con.Query(\"SELECT MIN(100000000000000000<<t0.rowid) FROM t0;\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {Value::BIGINT(-9223372036854775807LL - 1)}));\n+\t\tresult = con.Query(\"SELECT MIN(100000000000000000<<t0.rowid) FROM t0 WHERE NOT c0;\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {Value::BIGINT(-8802109549835190272LL)}));\n+\t}\n }\ndiff --git a/test/sql/aggregate/test_aggregate.cpp b/test/sql/aggregate/test_aggregate.cpp\nindex 702d8a9e7f62..6d5d7429f40d 100644\n--- a/test/sql/aggregate/test_aggregate.cpp\n+++ b/test/sql/aggregate/test_aggregate.cpp\n@@ -1,5 +1,6 @@\n #include \"catch.hpp\"\n #include \"test_helpers.hpp\"\n+#include \"duckdb/main/appender.hpp\"\n \n using namespace duckdb;\n using namespace std;\n@@ -329,12 +330,14 @@ TEST_CASE(\"Test STRING_AGG operator with many groups\", \"[aggregate][.]\") {\n \tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE strings(g INTEGER, x VARCHAR);\"));\n \tvector<Value> expected_g, expected_h;\n \tstring expected_large_value;\n+\tAppender appender(con, \"strings\");\n \tfor (idx_t i = 0; i < 10000; i++) {\n-\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO strings VALUES (?, ?);\", (int)i, \"hello\"));\n+\t\tappender.AppendRow((int)i, \"hello\");\n \t\texpected_g.push_back(Value::INTEGER(i));\n \t\texpected_h.push_back(Value(\"hello\"));\n \t\texpected_large_value += (i > 0 ? \",\" : \"\") + string(\"hello\");\n \t}\n+\tappender.Close();\n \tREQUIRE_NO_FAIL(con.Query(\"COMMIT;\"));\n \n \t// many small groups\n@@ -511,9 +514,12 @@ TEST_CASE(\"Test GROUP BY with many groups\", \"[aggregate][.]\") {\n \tConnection con(db);\n \n \tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE integers(i INTEGER, j INTEGER);\"));\n+\tAppender appender(con, \"integers\");\n \tfor (idx_t i = 0; i < 10000; i++) {\n-\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO integers VALUES (\" + to_string(i) + \", 1), (\" + to_string(i) + \", 2);\"));\n+\t\tappender.AppendRow((int)i, (int)1);\n+\t\tappender.AppendRow((int)i, (int)2);\n \t}\n+\tappender.Close();\n \tresult = con.Query(\"SELECT SUM(i), SUM(sums) FROM (SELECT i, SUM(j) AS sums FROM integers GROUP BY i) tbl1\");\n \tREQUIRE(CHECK_COLUMN(result, 0, {49995000}));\n \tREQUIRE(CHECK_COLUMN(result, 1, {30000}));\ndiff --git a/test/sql/alter/test_alter.cpp b/test/sql/alter/test_alter.cpp\nindex 66eab0a64cf4..68e0a96cc929 100644\n--- a/test/sql/alter/test_alter.cpp\n+++ b/test/sql/alter/test_alter.cpp\n@@ -131,15 +131,643 @@ TEST_CASE(\"Test ALTER TABLE RENAME COLUMN on a table with constraints\", \"[alter]\n \tDuckDB db(nullptr);\n \tConnection con(db);\n \n-\t// create a table with a check constraint referencing the to-be-renamed column\n-\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER CHECK(i < 10), j INTEGER)\"));\n-\t// insert some elements\n-\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (i, j) VALUES (1, 2), (2, 3)\"));\n-\tREQUIRE_FAIL(con.Query(\"INSERT INTO test (i, j) VALUES (100, 2)\"));\n-\t// now alter the column name\n-\t// currently, we don't support altering tables with constraints\n-\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test RENAME COLUMN i TO k\"));\n-\t// the check should still work after the alter table\n-\t// REQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (k, j) VALUES (1, 2), (2, 3)\"));\n-\t// REQUIRE_FAIL(con.Query(\"INSERT INTO test (k, j) VALUES (100, 2)\"));\n+\tSECTION(\"CHECK constraint\") {\n+\t\t// create a table with a check constraint referencing the to-be-renamed column\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER CHECK(i < 10), j INTEGER)\"));\n+\t\t// insert some elements\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (i, j) VALUES (1, 2), (2, 3)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test (i, j) VALUES (100, 2)\"));\n+\t\t// now alter the column name\n+\t\t// currently, we don't support altering tables with constraints\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test RENAME COLUMN i TO k\"));\n+\t\t// the check should still work after the alter table\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (k, j) VALUES (1, 2), (2, 3)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test (k, j) VALUES (100, 2)\"));\n+\t}\n+\tSECTION(\"NOT NULL constraint\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER NOT NULL, j INTEGER)\"));\n+\t\t// insert some elements\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (i, j) VALUES (1, 2), (2, 3)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test (i, j) VALUES (NULL, 2)\"));\n+\t\t// now alter the column name\n+\t\t// currently, we don't support altering tables with constraints\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test RENAME COLUMN i TO k\"));\n+\t\t// the check should still work after the alter table\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (k, j) VALUES (1, 2), (2, 3)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test (k, j) VALUES (NULL, 2)\"));\n+\t}\n+\tSECTION(\"UNIQUE constraint\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER, j INTEGER, PRIMARY KEY(i, j))\"));\n+\t\t// insert some elements\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (i, j) VALUES (1, 1), (2, 2)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test (i, j) VALUES (1, 1)\"));\n+\t\t// now alter the column name\n+\t\t// currently, we don't support altering tables with constraints\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test RENAME COLUMN i TO k\"));\n+\t\t// the check should still work after the alter table\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (k, j) VALUES (3, 3), (4, 4)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test (k, j) VALUES (1, 1)\"));\n+\t}\n+}\n+\n+TEST_CASE(\"Test ALTER TABLE ADD COLUMN\", \"[alter]\") {\n+\tunique_ptr<QueryResult> result;\n+\tDuckDB db(nullptr);\n+\tConnection con(db);\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER, j INTEGER)\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (1, 1), (2, 2)\"));\n+\n+\tSECTION(\"Standard ADD COLUMN\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {Value(), Value()}));\n+\t}\n+\tSECTION(\"ADD COLUMN with default value\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN l INTEGER DEFAULT 3\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {3, 3}));\n+\t}\n+\tSECTION(\"ADD COLUMN with sequence as default value\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE SEQUENCE seq\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN m INTEGER DEFAULT nextval('seq')\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {1, 2}));\n+\t}\n+\tSECTION(\"ADD COLUMN with data inside local storage\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (3, 3)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2, 3}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {Value(), Value(), Value()}));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ROLLBACK\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(result->names.size() == 2);\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (3, 3)\"));\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2, 3}));\n+\t}\n+\tSECTION(\"multiple ADD COLUMN in the same transaction\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (3, 3)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN l INTEGER\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN m INTEGER DEFAULT 3\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2, 3}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {Value(), Value(), Value()}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 3, {Value(), Value(), Value()}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 4, {3, 3, 3}));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ROLLBACK\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(result->names.size() == 2);\n+\t}\n+\tSECTION(\"ADD COLUMN with index\") {\n+\t\t// what if we create an index on the new column, then rollback\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER DEFAULT 2\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE INDEX i_index ON test(k)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"COMMIT\"));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (3, 3, 3)\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test WHERE k=2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {2, 2}));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test WHERE k=3\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {3}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {3}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {3}));\n+\t}\n+\tSECTION(\"ADD COLUMN rollback with index\") {\n+\t\t// what if we create an index on the new column, then rollback\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE INDEX i_index ON test(k)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ROLLBACK\"));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (3, 3)\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2, 3}));\n+\t}\n+\tSECTION(\"Incorrect usage\") {\n+\t\t// cannot add a column that already exists!\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN i INTEGER\"));\n+\t}\n+}\n+\n+TEST_CASE(\"Test ALTER TABLE ADD COLUMN with multiple transactions\", \"[alter]\") {\n+\tunique_ptr<QueryResult> result;\n+\tDuckDB db(nullptr);\n+\tConnection con(db), con2(db);\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER, j INTEGER)\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (1, 1), (2, 2)\"));\n+\n+\tSECTION(\"Only one pending table alter can be active at a time\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\t// con adds a column to test\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER\"));\n+\t\t// con2 cannot add a new column now!\n+\t\tREQUIRE_FAIL(con2.Query(\"ALTER TABLE test ADD COLUMN l INTEGER\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"COMMIT\"));\n+\t\t// after a commit, con2 can add a new column again\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"ALTER TABLE test ADD COLUMN l INTEGER\"));\n+\t}\n+\tSECTION(\"Can only append to newest table\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\t// con adds a column to test\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER\"));\n+\n+\t\t// con2 cannot append now!\n+\t\tREQUIRE_FAIL(con2.Query(\"INSERT INTO test (i, j) VALUES (3, 3)\"));\n+\t\t// but we can delete rows!\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"DELETE FROM test WHERE i=1\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {Value(), Value()}));\n+\n+\t\tresult = con2.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {2}));\n+\n+\t\t// we can also update rows\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"UPDATE test SET j=100\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {Value(), Value()}));\n+\n+\t\tresult = con2.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {100}));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"COMMIT\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {100}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {Value()}));\n+\t}\n+\tSECTION(\"Alter table while other transaction still has pending appends\") {\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"INSERT INTO test VALUES (3, 3)\"));\n+\n+\t\t// now con adds a column\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER\"));\n+\n+\t\t// cannot commit con2! conflict on append\n+\t\tREQUIRE_FAIL(con2.Query(\"COMMIT\"));\n+\t}\n+}\n+\n+TEST_CASE(\"Test ALTER TABLE DROP COLUMN\", \"[alter]\") {\n+\tunique_ptr<QueryResult> result;\n+\tDuckDB db(nullptr);\n+\tConnection con(db);\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER, j INTEGER)\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (1, 1), (2, 2)\"));\n+\n+\tSECTION(\"Standard DROP COLUMN\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN j\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(result->names.size() == 1);\n+\t}\n+\tSECTION(\"Rollback of DROP COLUMN\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN j\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(result->names.size() == 1);\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ROLLBACK\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(result->names.size() == 2);\n+\t}\n+\tSECTION(\"Cannot DROP COLUMN which has an index built on it\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE INDEX i_index ON test(j)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN j\"));\n+\n+\t\t// we can remove the column after dropping the index\n+\t\tREQUIRE_NO_FAIL(con.Query(\"DROP INDEX i_index\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN j\"));\n+\t}\n+\tSECTION(\"DROP COLUMN with check constraint on single column\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test2(i INTEGER, j INTEGER CHECK(j < 10))\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (1, 1), (2, 2)\"));\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\n+\t\t// we can drop a column that has a single check constraint on it\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test2 DROP COLUMN j\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (3)\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\t\tREQUIRE(result->names.size() == 1);\n+\t}\n+\tSECTION(\"DROP COLUMN with check constraint on multiple columns\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test2(i INTEGER, j INTEGER CHECK(i+j < 10))\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (1, 1), (2, 2)\"));\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\n+\t\t// we CANNOT drop one of the columns, because the CHECK constraint depends on both\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test2 DROP COLUMN j\"));\n+\t}\n+\tSECTION(\"DROP COLUMN with NOT NULL constraint\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test2(i INTEGER, j INTEGER NOT NULL)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (1, 1), (2, 2)\"));\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test2 DROP COLUMN j\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (3)\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\t\tREQUIRE(result->names.size() == 1);\n+\t}\n+\tSECTION(\"DROP COLUMN with check constraint on subsequent column\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test2(i INTEGER, j INTEGER CHECK(j < 10))\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (1, 1), (2, 2)\"));\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\n+\t\t// we can drop a column that has a single check constraint on it\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test2 DROP COLUMN i\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test2 VALUES (20)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (3)\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\t\tREQUIRE(result->names.size() == 1);\n+\t}\n+\tSECTION(\"DROP COLUMN with NOT NULL constraint on subsequent column\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test2(i INTEGER, j INTEGER, k INTEGER NOT NULL)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (1, 1, 11), (2, 2, 12)\"));\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 2, {11, 12}));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test2 DROP COLUMN j\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test2 VALUES (3, NULL)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (3, 13)\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {11, 12, 13}));\n+\t\tREQUIRE(result->names.size() == 2);\n+\t}\n+\tSECTION(\"DROP COLUMN with index built on subsequent column\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE INDEX i_index ON test(j)\"));\n+\n+\t\t// cannot drop indexed column\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN j\"));\n+\t\t// we also cannot drop the column i (for now) because an index depends on a subsequent column\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN i\"));\n+\t}\n+\tSECTION(\"DROP COLUMN from table with primary key constraint\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test2(i INTEGER PRIMARY KEY, j INTEGER)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (1, 1), (2, 2)\"));\n+\n+\t\t// cannot drop primary key column\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test2 DROP COLUMN i\"));\n+\t\t// but we can drop column \"i\"\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test2 DROP COLUMN j\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (3)\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\t\tREQUIRE(result->names.size() == 1);\n+\t}\n+\tSECTION(\"DROP COLUMN errors\") {\n+\t\t// cannot drop column which does not exist\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN blabla\"));\n+\t\t// unless IF EXISTS is specified\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN IF EXISTS blabla\"));\n+\n+\t\t// cannot drop ALL columns of a table\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN i\"));\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN j\"));\n+\t}\n+}\n+\n+TEST_CASE(\"Test ALTER TABLE DROP COLUMN with multiple transactions\", \"[alter]\") {\n+\tunique_ptr<QueryResult> result;\n+\tDuckDB db(nullptr);\n+\tConnection con(db), con2(db);\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER, j INTEGER)\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (1, 1), (2, 2)\"));\n+\n+\tSECTION(\"Only one pending table alter can be active at a time\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\t// con removes a column to test\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN j\"));\n+\t\t// con2 cannot add a new column now!\n+\t\tREQUIRE_FAIL(con2.Query(\"ALTER TABLE test ADD COLUMN k INTEGER\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"COMMIT\"));\n+\t\t// we can add the column after the commit\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"ALTER TABLE test ADD COLUMN k INTEGER\"));\n+\t}\n+\tSECTION(\"Can only append to newest table\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\t// con removes a column from test\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN i\"));\n+\n+\t\t// con2 cannot append now!\n+\t\tREQUIRE_FAIL(con2.Query(\"INSERT INTO test (i, j) VALUES (3, 3)\"));\n+\t\t// but we can delete rows!\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"DELETE FROM test WHERE i=1\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\n+\t\tresult = con2.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {2}));\n+\n+\t\t// we can also update rows\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"UPDATE test SET j=100\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\n+\t\tresult = con2.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {100}));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"COMMIT\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {100}));\n+\t}\n+\tSECTION(\"Alter table while other transaction still has pending appends\") {\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"INSERT INTO test VALUES (3, 3)\"));\n+\n+\t\t// now con adds a column\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN i\"));\n+\n+\t\t// cannot commit con2! conflict on append\n+\t\tREQUIRE_FAIL(con2.Query(\"COMMIT\"));\n+\t}\n+\tSECTION(\"Create index on column that has been removed by other transaction\") {\n+\t\t// con2 removes a column\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"ALTER TABLE test DROP COLUMN j\"));\n+\n+\t\t// now con tries to add an index to that column: this should fail\n+\t\tREQUIRE_FAIL(con.Query(\"CREATE INDEX i_index ON test(j\"));\n+\t}\n+}\n+\n+TEST_CASE(\"Test ALTER TABLE SET DEFAULT\", \"[alter]\") {\n+\tunique_ptr<QueryResult> result;\n+\tDuckDB db(nullptr);\n+\tConnection con(db);\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER, j INTEGER)\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (1, 1), (2, 2)\"));\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER j SET DEFAULT 3\"));\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (i) VALUES (3)\"));\n+\tresult = con.Query(\"SELECT * FROM test\");\n+\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));\n+\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2, 3}));\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER COLUMN j DROP DEFAULT\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (i) VALUES (4)\"));\n+\tresult = con.Query(\"SELECT * FROM test\");\n+\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3, 4}));\n+\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2, 3, Value()}));\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"CREATE SEQUENCE seq\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER j SET DEFAULT nextval('seq')\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test (i) VALUES (5), (6)\"));\n+\tresult = con.Query(\"SELECT * FROM test\");\n+\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3, 4, 5, 6}));\n+\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2, 3, Value(), 1, 2}));\n+\n+\t// fail when column does not exist\n+\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test ALTER blabla SET DEFAULT 3\"));\n+\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test ALTER blabla DROP DEFAULT\"));\n+}\n+\n+TEST_CASE(\"Test ALTER TABLE ALTER TYPE\", \"[alter]\") {\n+\tunique_ptr<QueryResult> result;\n+\tDuckDB db(nullptr);\n+\tConnection con(db);\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER, j INTEGER)\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (1, 1), (2, 2)\"));\n+\n+\tSECTION(\"Standard ALTER TYPE\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER i SET DATA TYPE VARCHAR\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {\"1\", \"2\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t}\n+\tSECTION(\"ALTER TYPE with expression\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER i TYPE BIGINT USING i+100\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {101, 102}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t}\n+\tSECTION(\"Rollback ALTER TYPE\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER i SET DATA TYPE VARCHAR\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"UPDATE test SET i='hello'\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {\"hello\", \"hello\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ROLLBACK\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t}\n+\tSECTION(\"ALTER TYPE with transaction local data\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (3, 3)\"));\n+\t\t// not currently supported\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test ALTER i SET DATA TYPE BIGINT\"));\n+\t}\n+\tSECTION(\"ALTER TYPE with expression using multiple columns\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER i TYPE INTEGER USING 2*(i+j)\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {4, 8}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\t}\n+\tSECTION(\"ALTER TYPE with NOT NULL constraint\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test2(i INTEGER NOT NULL, j INTEGER)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (1, 1), (2, 2)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test2 VALUES (NULL, 4)\"));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test2 ALTER i SET DATA TYPE VARCHAR\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES ('hello', 3)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"INSERT INTO test2 VALUES (NULL, 4)\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {\"1\", \"2\", \"hello\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2, 3}));\n+\t}\n+\tSECTION(\"ALTER TYPE with CHECK constraint\") {\n+\t\t// we disallow ALTER TYPE on a column with a CHECK constraint\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test2(i INTEGER CHECK(i < 10), j INTEGER)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (1, 1), (2, 2)\"));\n+\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test2 ALTER i SET DATA TYPE VARCHAR\"));\n+\t}\n+\tSECTION(\"ALTER TYPE with UNIQUE constraint\") {\n+\t\t// we disallow ALTER TYPE on a column with a UNIQUE constraint\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test2(i INTEGER UNIQUE, j INTEGER)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test2 VALUES (1, 1), (2, 2)\"));\n+\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test2 ALTER i SET DATA TYPE VARCHAR\"));\n+\t\t// but we CAN change the other column\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test2 ALTER j SET DATA TYPE VARCHAR\"));\n+\t\tresult = con.Query(\"SELECT * FROM test2\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1, 2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {\"1\", \"2\"}));\n+\t}\n+\tSECTION(\"ALTER TYPE with INDEX\") {\n+\t\t// we disallow ALTER TYPE on a column with an index on it\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE INDEX i_index ON test(i)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test ALTER i SET DATA TYPE VARCHAR\"));\n+\n+\t\t// we can alter the table after the index is dropped, however\n+\t\tREQUIRE_NO_FAIL(con.Query(\"DROP INDEX i_index\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER i SET DATA TYPE VARCHAR\"));\n+\t}\n+\tSECTION(\"ALTER TYPE with unknown columns\") {\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test ALTER blabla SET TYPE VARCHAR\"));\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test ALTER i SET TYPE VARCHAR USING blabla\"));\n+\t\t// cannot use aggregates or window functions\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test ALTER i SET TYPE VARCHAR USING SUM(i)\"));\n+\t\tREQUIRE_FAIL(con.Query(\"ALTER TABLE test ALTER i SET TYPE VARCHAR USING row_id() OVER ()\"));\n+\t}\n+}\n+\n+TEST_CASE(\"Test ALTER TABLE ALTER TYPE with multiple transactions\", \"[alter]\") {\n+\tunique_ptr<QueryResult> result;\n+\tDuckDB db(nullptr);\n+\tConnection con(db), con2(db);\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test(i INTEGER, j INTEGER)\"));\n+\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (1, 1), (2, 2)\"));\n+\n+\tSECTION(\"Only one pending table alter can be active at a time\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\t// con alters a column to test\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER j TYPE VARCHAR\"));\n+\t\t// con2 cannot alter another column now!\n+\t\tREQUIRE_FAIL(con2.Query(\"ALTER TABLE test ALTER i TYPE VARCHAR\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"COMMIT\"));\n+\t\t// we can alter the column after the commit\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"ALTER TABLE test ALTER i TYPE VARCHAR\"));\n+\t}\n+\tSECTION(\"Can only append to newest table\") {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n+\t\t// con removes a column from test\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER i TYPE VARCHAR\"));\n+\n+\t\t// con2 cannot append now!\n+\t\tREQUIRE_FAIL(con2.Query(\"INSERT INTO test (i, j) VALUES (3, 3)\"));\n+\t\t// but we can delete rows!\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"DELETE FROM test WHERE i=1\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {\"1\", \"2\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\n+\t\tresult = con2.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {2}));\n+\n+\t\t// we can also update rows, but updates to i will not be seen...\n+\t\t// should we check this somehow?\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"UPDATE test SET i=1000\"));\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"UPDATE test SET j=100\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {\"1\", \"2\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {1, 2}));\n+\n+\t\tresult = con2.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {1000}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {100}));\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"COMMIT\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {\"2\"}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {100}));\n+\t}\n+\tSECTION(\"Alter table while other transaction still has pending appends\") {\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"INSERT INTO test VALUES (3, 3)\"));\n+\n+\t\t// now con adds a column\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER i TYPE VARCHAR\"));\n+\n+\t\t// cannot commit con2! conflict on append\n+\t\tREQUIRE_FAIL(con2.Query(\"COMMIT\"));\n+\t}\n+\tSECTION(\"Create index on column that has been altered by other transaction\") {\n+\t\t// con2 removes a column\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"BEGIN TRANSACTION\"));\n+\t\tREQUIRE_NO_FAIL(con2.Query(\"ALTER TABLE test ALTER j TYPE VARCHAR\"));\n+\n+\t\t// now con tries to add an index to that column: this should fail\n+\t\tREQUIRE_FAIL(con.Query(\"CREATE INDEX i_index ON test(j\"));\n+\t}\n }\ndiff --git a/test/sql/storage/test_store_alter.cpp b/test/sql/storage/test_store_alter.cpp\nindex 334f0868400a..1e36a816487b 100644\n--- a/test/sql/storage/test_store_alter.cpp\n+++ b/test/sql/storage/test_store_alter.cpp\n@@ -5,7 +5,7 @@\n using namespace duckdb;\n using namespace std;\n \n-TEST_CASE(\"Test storage of alter table\", \"[storage]\") {\n+TEST_CASE(\"Test storage of alter table rename column\", \"[storage]\") {\n \tunique_ptr<QueryResult> result;\n \tauto storage_database = TestCreatePath(\"storage_test\");\n \tauto config = GetTestConfig();\n@@ -40,3 +40,142 @@ TEST_CASE(\"Test storage of alter table\", \"[storage]\") {\n \t}\n \tDeleteDatabase(storage_database);\n }\n+\n+TEST_CASE(\"Test storage of alter table add column\", \"[storage]\") {\n+\tunique_ptr<QueryResult> result;\n+\tauto storage_database = TestCreatePath(\"storage_test\");\n+\tauto config = GetTestConfig();\n+\n+\t// make sure the database does not exist\n+\tDeleteDatabase(storage_database);\n+\t{\n+\t\t// create a database and insert values\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test (a INTEGER, b INTEGER);\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (11, 22), (13, 22), (12, 21)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER DEFAULT 2\"));\n+\n+\t\tresult = con.Query(\"SELECT k FROM test ORDER BY k\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2, 2, 2}));\n+\t}\n+\t// reload the database from disk\n+\tfor (idx_t i = 0; i < 2; i++) {\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\t\tresult = con.Query(\"SELECT k FROM test ORDER BY k\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2, 2, 2}));\n+\t}\n+\tDeleteDatabase(storage_database);\n+}\n+\n+TEST_CASE(\"Add column to persistent table\", \"[storage]\") {\n+\tunique_ptr<QueryResult> result;\n+\tauto storage_database = TestCreatePath(\"storage_test\");\n+\tauto config = GetTestConfig();\n+\n+\t// make sure the database does not exist\n+\tDeleteDatabase(storage_database);\n+\t{\n+\t\t// create a database and insert values\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test (a INTEGER, b INTEGER);\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (11, 22), (13, 22), (12, 21)\"));\n+\t}\n+\t// reload and alter\n+\t{\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ADD COLUMN k INTEGER DEFAULT 2\"));\n+\n+\t\tresult = con.Query(\"SELECT k FROM test ORDER BY k\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2, 2, 2}));\n+\t}\n+\t// now reload\n+\tfor (idx_t i = 0; i < 2; i++) {\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\n+\t\tresult = con.Query(\"SELECT k FROM test ORDER BY k\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {2, 2, 2}));\n+\t}\n+\tDeleteDatabase(storage_database);\n+}\n+\n+TEST_CASE(\"Remove column from persistent table\", \"[storage]\") {\n+\tunique_ptr<QueryResult> result;\n+\tauto storage_database = TestCreatePath(\"storage_test\");\n+\tauto config = GetTestConfig();\n+\n+\t// make sure the database does not exist\n+\tDeleteDatabase(storage_database);\n+\t{\n+\t\t// create a database and insert values\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test (a INTEGER, b INTEGER);\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (11, 22), (13, 22), (12, 21)\"));\n+\t}\n+\t// reload and alter\n+\t{\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test DROP COLUMN b\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test ORDER BY 1\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {11, 12, 13}));\n+\t\tREQUIRE(result->names.size() == 1);\n+\t}\n+\t// now reload\n+\tfor (idx_t i = 0; i < 2; i++) {\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\n+\t\tresult = con.Query(\"SELECT * FROM test ORDER BY 1\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {11, 12, 13}));\n+\t\tREQUIRE(result->names.size() == 1);\n+\t}\n+\tDeleteDatabase(storage_database);\n+}\n+\n+TEST_CASE(\"Alter column type of persistent table\", \"[storage]\") {\n+\tunique_ptr<QueryResult> result;\n+\tauto storage_database = TestCreatePath(\"storage_test\");\n+\tauto config = GetTestConfig();\n+\n+\t// make sure the database does not exist\n+\tDeleteDatabase(storage_database);\n+\t{\n+\t\t// create a database and insert values\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE test (a INTEGER, b INTEGER);\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO test VALUES (11, 22), (13, 22), (12, 21)\"));\n+\t}\n+\t// reload and alter\n+\t{\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ALTER TABLE test ALTER b TYPE VARCHAR\"));\n+\n+\t\tresult = con.Query(\"SELECT * FROM test ORDER BY 1\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {11, 12, 13}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {\"22\", \"21\", \"22\"}));\n+\t\tREQUIRE(result->names.size() == 2);\n+\t}\n+\t// now reload\n+\tfor (idx_t i = 0; i < 2; i++) {\n+\t\tDuckDB db(storage_database, config.get());\n+\t\tConnection con(db);\n+\n+\t\tresult = con.Query(\"SELECT * FROM test ORDER BY 1\");\n+\t\tREQUIRE(CHECK_COLUMN(result, 0, {11, 12, 13}));\n+\t\tREQUIRE(CHECK_COLUMN(result, 1, {\"22\", \"21\", \"22\"}));\n+\t\tREQUIRE(result->names.size() == 2);\n+\t}\n+\tDeleteDatabase(storage_database);\n+}\ndiff --git a/tools/pythonpkg/tests/test_dbapi09.py b/tools/pythonpkg/tests/test_dbapi09.py\nindex 82b665da0dc8..a58997e7607a 100644\n--- a/tools/pythonpkg/tests/test_dbapi09.py\n+++ b/tools/pythonpkg/tests/test_dbapi09.py\n@@ -1,6 +1,6 @@\n # date type\n \n-import numpy \n+import numpy\n import datetime\n import pandas\n \ndiff --git a/tools/pythonpkg/tests/test_dbapi13.py b/tools/pythonpkg/tests/test_dbapi13.py\nnew file mode 100644\nindex 000000000000..39733e2c4f23\n--- /dev/null\n+++ b/tools/pythonpkg/tests/test_dbapi13.py\n@@ -0,0 +1,21 @@\n+# time type\n+\n+import numpy\n+import datetime\n+import pandas\n+\n+class TestNumpyTime(object):\n+    def test_fetchall_date(self, duckdb_cursor):\n+        res = duckdb_cursor.execute(\"SELECT TIME '13:06:40' as test_time\").fetchall()\n+        assert res == [(datetime.time(13, 6, 40),)]\n+\n+    def test_fetchnumpy_date(self, duckdb_cursor):\n+        res = duckdb_cursor.execute(\"SELECT TIME '13:06:40' as test_time\").fetchnumpy()\n+        arr = numpy.array(['13:06:40'], dtype=\"object\")\n+        arr = numpy.ma.masked_array(arr)\n+        numpy.testing.assert_array_equal(res['test_time'], arr)\n+\n+    def test_fetchdf_date(self, duckdb_cursor):\n+        res = duckdb_cursor.execute(\"SELECT TIME '13:06:40' as test_time\").fetchdf()\n+        ser = pandas.Series(numpy.array(['13:06:40'], dtype=\"object\"), name=\"test_time\")\n+        pandas.testing.assert_series_equal(res['test_time'], ser)\n",
  "problem_statement": "Incorrect result for MIN() on expression involving rowid\nConsider the following statements:\r\n```sql\r\nCREATE TABLE t0(c0 INT, c1 INT);\r\nINSERT INTO t0(c0) VALUES (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0), (0),  (0), (0), (0), (0), (0), (0), (NULL), (NULL);\r\nCREATE INDEX b ON t0(c1);\r\nUPDATE t0 SET c1 = NULL;\r\nSELECT MIN(100000000000000000<<t0.rowid) FROM t0; -- unexpected: {0}\r\n```\r\nUnexpectedly, the `SELECT` fetches 0, although there are values smaller than zero contained in the table. Interestingly, adding a `WHERE` clause results in a smaller minimum value:\r\n```sql\r\nSELECT MIN(100000000000000000<<t0.rowid) FROM t0 WHERE NOT c0; -- -8802109549835190272\r\n```\r\nI found this based on commit a382ba05a7d6c0abb0c8f8f50b2bdf3a4e480704.\n",
  "hints_text": "",
  "created_at": "2020-05-02T21:15:22Z"
}