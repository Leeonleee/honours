diff --git a/.gitignore b/.gitignore
index a2d3308f4ad8..1ef6d63a493e 100644
--- a/.gitignore
+++ b/.gitignore
@@ -304,4 +304,8 @@ third_party/imdb/data
 benchmark_results/
 duckdb_unittest_tempdir/
 grammar.y.tmp
-src/amalgamation/
\ No newline at end of file
+src/amalgamation/
+# single file compile
+amalgamation.cache
+dependencies.d
+deps.s
diff --git a/scripts/amalgamation.py b/scripts/amalgamation.py
index 46f6282e3ecd..93ba1ca56064 100644
--- a/scripts/amalgamation.py
+++ b/scripts/amalgamation.py
@@ -20,7 +20,7 @@
 utf8proc_include_dir = os.path.join('third_party', 'utf8proc', 'include')
 
 # files included in the amalgamated "duckdb.hpp" file
-main_header_files = [os.path.join(include_dir, 'duckdb.hpp'), os.path.join(include_dir, 'duckdb.h'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'date.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'timestamp.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'appender.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'client_context.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'function.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'table_function.hpp'), os.path.join(include_dir, 'duckdb', 'parser', 'parsed_data', 'create_table_function_info.hpp')]
+main_header_files = [os.path.join(include_dir, 'duckdb.hpp'), os.path.join(include_dir, 'duckdb.h'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'date.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'timestamp.hpp'), os.path.join(include_dir, 'duckdb', 'common', 'types', 'time.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'appender.hpp'), os.path.join(include_dir, 'duckdb', 'main', 'client_context.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'function.hpp'), os.path.join(include_dir, 'duckdb', 'function', 'table_function.hpp'), os.path.join(include_dir, 'duckdb', 'parser', 'parsed_data', 'create_table_function_info.hpp')]
 
 # include paths for where to search for include files during amalgamation
 include_paths = [include_dir, fmt_include_dir, hll_dir, re2_dir, miniz_dir, utf8proc_include_dir, utf8proc_dir, pg_query_include_dir, pg_query_dir]
diff --git a/src/catalog/catalog_entry/CMakeLists.txt b/src/catalog/catalog_entry/CMakeLists.txt
index a80e8aab9063..eff60dc8e986 100644
--- a/src/catalog/catalog_entry/CMakeLists.txt
+++ b/src/catalog/catalog_entry/CMakeLists.txt
@@ -1,5 +1,6 @@
 add_library_unity(duckdb_catalog_entries
                   OBJECT
+                  index_catalog_entry.cpp
                   schema_catalog_entry.cpp
                   sequence_catalog_entry.cpp
                   table_catalog_entry.cpp
diff --git a/src/catalog/catalog_entry/index_catalog_entry.cpp b/src/catalog/catalog_entry/index_catalog_entry.cpp
new file mode 100644
index 000000000000..0c58f1b1af53
--- /dev/null
+++ b/src/catalog/catalog_entry/index_catalog_entry.cpp
@@ -0,0 +1,19 @@
+#include "duckdb/catalog/catalog_entry/index_catalog_entry.hpp"
+#include "duckdb/storage/data_table.hpp"
+
+namespace duckdb {
+
+IndexCatalogEntry::~IndexCatalogEntry() {
+	// remove the associated index from the info
+	if (!info || !index) {
+		return;
+	}
+	for (idx_t i = 0; i < info->indexes.size(); i++) {
+		if (info->indexes[i].get() == index) {
+			info->indexes.erase(info->indexes.begin() + i);
+			break;
+		}
+	}
+}
+
+} // namespace duckdb
diff --git a/src/catalog/catalog_entry/table_catalog_entry.cpp b/src/catalog/catalog_entry/table_catalog_entry.cpp
index 097b50a8c439..323189376e0f 100644
--- a/src/catalog/catalog_entry/table_catalog_entry.cpp
+++ b/src/catalog/catalog_entry/table_catalog_entry.cpp
@@ -10,13 +10,17 @@
 #include "duckdb/parser/parsed_data/alter_table_info.hpp"
 #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
 #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
+#include "duckdb/planner/constraints/bound_check_constraint.hpp"
 #include "duckdb/planner/expression/bound_constant_expression.hpp"
 #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
 #include "duckdb/storage/storage_manager.hpp"
 #include "duckdb/planner/binder.hpp"
 
 #include "duckdb/execution/index/art/art.hpp"
+#include "duckdb/parser/expression/columnref_expression.hpp"
 #include "duckdb/planner/expression/bound_reference_expression.hpp"
+#include "duckdb/parser/parsed_expression_iterator.hpp"
+#include "duckdb/planner/expression_binder/alter_binder.hpp"
 
 #include <algorithm>
 
@@ -59,13 +63,6 @@ TableCatalogEntry::TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schem
 				}
 				// create an adaptive radix tree around the expressions
 				auto art = make_unique<ART>(*storage, column_ids, move(unbound_expressions), true);
-
-				if (unique.is_primary_key) {
-					// if this is a primary key index, also create a NOT NULL constraint for each of the columns
-					for (auto &column_index : unique.keys) {
-						bound_constraints.push_back(make_unique<BoundNotNullConstraint>(column_index));
-					}
-				}
 				storage->AddIndex(move(art), bound_expressions);
 			}
 		}
@@ -80,40 +77,11 @@ unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, A
 	if (info->type != AlterType::ALTER_TABLE) {
 		throw CatalogException("Can only modify table with ALTER TABLE statement");
 	}
-	if (constraints.size() > 0) {
-		throw CatalogException("Cannot modify a table with constraints");
-	}
 	auto table_info = (AlterTableInfo *)info;
 	switch (table_info->alter_table_type) {
 	case AlterTableType::RENAME_COLUMN: {
 		auto rename_info = (RenameColumnInfo *)table_info;
-		auto create_info = make_unique<CreateTableInfo>(schema->name, name);
-		create_info->temporary = temporary;
-		bool found = false;
-		for (idx_t i = 0; i < columns.size(); i++) {
-			ColumnDefinition copy(columns[i].name, columns[i].type);
-			copy.oid = columns[i].oid;
-			copy.default_value = columns[i].default_value ? columns[i].default_value->Copy() : nullptr;
-
-			create_info->columns.push_back(move(copy));
-			if (rename_info->name == columns[i].name) {
-				assert(!found);
-				create_info->columns[i].name = rename_info->new_name;
-				found = true;
-			}
-		}
-		if (!found) {
-			throw CatalogException("Table does not have a column with name \"%s\"", rename_info->name.c_str());
-		}
-		assert(constraints.size() == 0);
-		// create_info->constraints.resize(constraints.size());
-		// for (idx_t i = 0; i < constraints.size(); i++) {
-		// 	create_info->constraints[i] = constraints[i]->Copy();
-		// }
-		Binder binder(context);
-		auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
-		return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
-		                                      storage);
+		return RenameColumn(context, *rename_info);
 	}
 	case AlterTableType::RENAME_TABLE: {
 		auto rename_info = (RenameTableInfo *)table_info;
@@ -121,9 +89,260 @@ unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, A
 		copied_table->name = rename_info->new_table_name;
 		return copied_table;
 	}
+	case AlterTableType::ADD_COLUMN: {
+		auto add_info = (AddColumnInfo *)table_info;
+		return AddColumn(context, *add_info);
+	}
+	case AlterTableType::REMOVE_COLUMN: {
+		auto remove_info = (RemoveColumnInfo *)table_info;
+		return RemoveColumn(context, *remove_info);
+	}
+	case AlterTableType::SET_DEFAULT: {
+		auto set_default_info = (SetDefaultInfo *)table_info;
+		return SetDefault(context, *set_default_info);
+	}
+	case AlterTableType::ALTER_COLUMN_TYPE: {
+		auto change_type_info = (ChangeColumnTypeInfo *)table_info;
+		return ChangeColumnType(context, *change_type_info);
+	}
 	default:
-		throw CatalogException("Unrecognized alter table type!");
+		throw InternalException("Unrecognized alter table type!");
+	}
+}
+
+static void RenameExpression(ParsedExpression &expr, RenameColumnInfo &info) {
+	if (expr.type == ExpressionType::COLUMN_REF) {
+		auto &colref = (ColumnRefExpression &)expr;
+		if (colref.column_name == info.name) {
+			colref.column_name = info.new_name;
+		}
+	}
+	ParsedExpressionIterator::EnumerateChildren(
+	    expr, [&](const ParsedExpression &child) { RenameExpression((ParsedExpression &)child, info); });
+}
+
+unique_ptr<CatalogEntry> TableCatalogEntry::RenameColumn(ClientContext &context, RenameColumnInfo &info) {
+	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
+	create_info->temporary = temporary;
+	bool found = false;
+	for (idx_t i = 0; i < columns.size(); i++) {
+		ColumnDefinition copy = columns[i].Copy();
+
+		create_info->columns.push_back(move(copy));
+		if (info.name == columns[i].name) {
+			assert(!found);
+			create_info->columns[i].name = info.new_name;
+			found = true;
+		}
+	}
+	if (!found) {
+		throw CatalogException("Table does not have a column with name \"%s\"", info.name.c_str());
+	}
+	for (idx_t c_idx = 0; c_idx < constraints.size(); c_idx++) {
+		auto copy = constraints[c_idx]->Copy();
+		switch (copy->type) {
+		case ConstraintType::NOT_NULL:
+			// NOT NULL constraint: no adjustments necessary
+			break;
+		case ConstraintType::CHECK: {
+			// CHECK constraint: need to rename column references that refer to the renamed column
+			auto &check = (CheckConstraint &)*copy;
+			RenameExpression(*check.expression, info);
+			break;
+		}
+		case ConstraintType::UNIQUE: {
+			// UNIQUE constraint: possibly need to rename columns
+			auto &unique = (UniqueConstraint &)*copy;
+			for (idx_t i = 0; i < unique.columns.size(); i++) {
+				if (unique.columns[i] == info.name) {
+					unique.columns[i] = info.new_name;
+				}
+			}
+			break;
+		}
+		default:
+			throw CatalogException("Unsupported constraint for entry!");
+		}
+		create_info->constraints.push_back(move(copy));
+	}
+	Binder binder(context);
+	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
+	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
+}
+
+unique_ptr<CatalogEntry> TableCatalogEntry::AddColumn(ClientContext &context, AddColumnInfo &info) {
+	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
+	create_info->temporary = temporary;
+	for (idx_t i = 0; i < columns.size(); i++) {
+		create_info->columns.push_back(columns[i].Copy());
+	}
+	info.new_column.oid = columns.size();
+	create_info->columns.push_back(info.new_column.Copy());
+
+	Binder binder(context);
+	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
+	auto new_storage =
+	    make_shared<DataTable>(context, *storage, info.new_column, bound_create_info->bound_defaults.back().get());
+	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
+	                                      new_storage);
+}
+
+unique_ptr<CatalogEntry> TableCatalogEntry::RemoveColumn(ClientContext &context, RemoveColumnInfo &info) {
+	idx_t removed_index = INVALID_INDEX;
+	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
+	create_info->temporary = temporary;
+	for (idx_t i = 0; i < columns.size(); i++) {
+		if (columns[i].name == info.removed_column) {
+			assert(removed_index == INVALID_INDEX);
+			removed_index = i;
+			continue;
+		}
+		create_info->columns.push_back(columns[i].Copy());
+	}
+	if (removed_index == INVALID_INDEX) {
+		if (!info.if_exists) {
+			throw CatalogException("Table does not have a column with name \"%s\"", info.removed_column.c_str());
+		}
+		return nullptr;
+	}
+	if (create_info->columns.size() == 0) {
+		throw CatalogException("Cannot drop column: table only has one column remaining!");
+	}
+	// handle constraints for the new table
+	assert(constraints.size() == bound_constraints.size());
+	for (idx_t constr_idx = 0; constr_idx < constraints.size(); constr_idx++) {
+		auto &constraint = constraints[constr_idx];
+		auto &bound_constraint = bound_constraints[constr_idx];
+		switch (bound_constraint->type) {
+		case ConstraintType::NOT_NULL: {
+			auto &not_null_constraint = (BoundNotNullConstraint &)*bound_constraint;
+			if (not_null_constraint.index != removed_index) {
+				// the constraint is not about this column: we need to copy it
+				// we might need to shift the index back by one though, to account for the removed column
+				idx_t new_index = not_null_constraint.index;
+				if (not_null_constraint.index > removed_index) {
+					new_index -= 1;
+				}
+				create_info->constraints.push_back(make_unique<NotNullConstraint>(new_index));
+			}
+			break;
+		}
+		case ConstraintType::CHECK: {
+			// CHECK constraint
+			auto &bound_check = (BoundCheckConstraint &)*bound_constraint;
+			// check if the removed column is part of the check constraint
+			if (bound_check.bound_columns.find(removed_index) != bound_check.bound_columns.end()) {
+				if (bound_check.bound_columns.size() > 1) {
+					// CHECK constraint that concerns mult
+					throw CatalogException(
+					    "Cannot drop column \"%s\" because there is a CHECK constraint that depends on it",
+					    info.removed_column.c_str());
+				} else {
+					// CHECK constraint that ONLY concerns this column, strip the constraint
+				}
+			} else {
+				// check constraint does not concern the removed column: simply re-add it
+				create_info->constraints.push_back(constraint->Copy());
+			}
+			break;
+		}
+		case ConstraintType::UNIQUE:
+			create_info->constraints.push_back(constraint->Copy());
+			break;
+		default:
+			throw InternalException("Unsupported constraint for entry!");
+		}
+	}
+
+	Binder binder(context);
+	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
+	auto new_storage = make_shared<DataTable>(context, *storage, removed_index);
+	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
+	                                      new_storage);
+}
+
+unique_ptr<CatalogEntry> TableCatalogEntry::SetDefault(ClientContext &context, SetDefaultInfo &info) {
+	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
+	bool found = false;
+	for (idx_t i = 0; i < columns.size(); i++) {
+		auto copy = columns[i].Copy();
+		if (info.column_name == copy.name) {
+			// set the default value of this column
+			copy.default_value = info.expression ? info.expression->Copy() : nullptr;
+			found = true;
+		}
+		create_info->columns.push_back(move(copy));
+	}
+	if (!found) {
+		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.table.c_str(),
+		                      info.column_name.c_str());
 	}
+
+	for (idx_t i = 0; i < constraints.size(); i++) {
+		auto constraint = constraints[i]->Copy();
+		create_info->constraints.push_back(move(constraint));
+	}
+
+	Binder binder(context);
+	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
+	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
+}
+
+unique_ptr<CatalogEntry> TableCatalogEntry::ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info) {
+	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
+	idx_t change_idx = INVALID_INDEX;
+	for (idx_t i = 0; i < columns.size(); i++) {
+		auto copy = columns[i].Copy();
+		if (info.column_name == copy.name) {
+			// set the default value of this column
+			change_idx = i;
+			copy.type = info.target_type;
+		}
+		create_info->columns.push_back(move(copy));
+	}
+	if (change_idx == INVALID_INDEX) {
+		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.table.c_str(),
+		                      info.column_name.c_str());
+	}
+
+	for (idx_t i = 0; i < constraints.size(); i++) {
+		auto constraint = constraints[i]->Copy();
+		switch (constraint->type) {
+		case ConstraintType::CHECK: {
+			auto &bound_check = (BoundCheckConstraint &)*bound_constraints[i];
+			if (bound_check.bound_columns.find(change_idx) != bound_check.bound_columns.end()) {
+				throw BinderException("Cannot change the type of a column that has a CHECK constraint specified");
+			}
+			break;
+		}
+		case ConstraintType::NOT_NULL:
+			break;
+		case ConstraintType::UNIQUE: {
+			auto &bound_unique = (BoundUniqueConstraint &)*bound_constraints[i];
+			if (bound_unique.keys.find(change_idx) != bound_unique.keys.end()) {
+				throw BinderException(
+				    "Cannot change the type of a column that has a UNIQUE or PRIMARY KEY constraint specified");
+			}
+			break;
+		}
+		default:
+			throw InternalException("Unsupported constraint for entry!");
+		}
+		create_info->constraints.push_back(move(constraint));
+	}
+
+	Binder binder(context);
+	// bind the specified expression
+	vector<column_t> bound_columns;
+	AlterBinder expr_binder(binder, context, name, columns, bound_columns, info.target_type);
+	auto expression = info.expression->Copy();
+	auto bound_expression = expr_binder.Bind(expression);
+	auto new_storage =
+	    make_shared<DataTable>(context, *storage, change_idx, info.target_type, move(bound_columns), *bound_expression);
+
+	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
+	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
+	                                      new_storage);
 }
 
 ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {
@@ -160,9 +379,7 @@ void TableCatalogEntry::Serialize(Serializer &serializer) {
 	assert(columns.size() <= std::numeric_limits<uint32_t>::max());
 	serializer.Write<uint32_t>((uint32_t)columns.size());
 	for (auto &column : columns) {
-		serializer.WriteString(column.name);
-		column.type.Serialize(serializer);
-		serializer.WriteOptional(column.default_value);
+		column.Serialize(serializer);
 	}
 	assert(constraints.size() <= std::numeric_limits<uint32_t>::max());
 	serializer.Write<uint32_t>((uint32_t)constraints.size());
@@ -179,10 +396,8 @@ unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source)
 	auto column_count = source.Read<uint32_t>();
 
 	for (uint32_t i = 0; i < column_count; i++) {
-		auto column_name = source.Read<string>();
-		auto column_type = SQLType::Deserialize(source);
-		auto default_value = source.ReadOptional<ParsedExpression>();
-		info->columns.push_back(ColumnDefinition(column_name, column_type, move(default_value)));
+		auto column = ColumnDefinition::Deserialize(source);
+		info->columns.push_back(move(column));
 	}
 	auto constraint_count = source.Read<uint32_t>();
 
@@ -196,10 +411,7 @@ unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source)
 unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {
 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
 	for (idx_t i = 0; i < columns.size(); i++) {
-		ColumnDefinition copy(columns[i].name, columns[i].type);
-		copy.oid = columns[i].oid;
-		copy.default_value = columns[i].default_value ? columns[i].default_value->Copy() : nullptr;
-		create_info->columns.push_back(move(copy));
+		create_info->columns.push_back(columns[i].Copy());
 	}
 
 	for (idx_t i = 0; i < constraints.size(); i++) {
@@ -211,3 +423,7 @@ unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {
 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
 }
+
+void TableCatalogEntry::SetAsRoot() {
+	storage->SetAsRoot();
+}
diff --git a/src/catalog/catalog_set.cpp b/src/catalog/catalog_set.cpp
index 419e68d3388e..d6a6b5dd5d20 100644
--- a/src/catalog/catalog_set.cpp
+++ b/src/catalog/catalog_set.cpp
@@ -90,6 +90,10 @@ bool CatalogSet::AlterEntry(ClientContext &context, const string &name, AlterInf
 	// set the timestamp to the timestamp of the current transaction
 	// and point it to the updated table node
 	auto value = current.AlterEntry(context, alter_info);
+	if (!value) {
+		// alter failed, but did not result in an error
+		return true;
+	}
 
 	// now transfer all dependencies from the old table to the new table
 	catalog.dependency_manager.AlterObject(transaction, data[name].get(), value.get());
@@ -228,6 +232,7 @@ void CatalogSet::Undo(CatalogEntry *entry) {
 	} else {
 		// otherwise we need to update the base entry tables
 		auto &name = entry->name;
+		to_be_removed_node->child->SetAsRoot();
 		data[name] = move(to_be_removed_node->child);
 		entry->parent = nullptr;
 	}
diff --git a/src/execution/operator/schema/physical_alter.cpp b/src/execution/operator/schema/physical_alter.cpp
index bb88c38586ed..ae52d698c36f 100644
--- a/src/execution/operator/schema/physical_alter.cpp
+++ b/src/execution/operator/schema/physical_alter.cpp
@@ -1,10 +1,15 @@
 #include "duckdb/execution/operator/schema/physical_alter.hpp"
 #include "duckdb/main/client_context.hpp"
+#include "duckdb/parser/parsed_data/alter_table_info.hpp"
 
-using namespace duckdb;
 using namespace std;
 
+namespace duckdb {
+
 void PhysicalAlter::GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) {
-	context.catalog.AlterTable(context, (AlterTableInfo *)info.get());
+	auto table_info = (AlterTableInfo *)info.get();
+	context.catalog.AlterTable(context, table_info);
 	state->finished = true;
 }
+
+} // namespace duckdb
diff --git a/src/execution/operator/schema/physical_create_index.cpp b/src/execution/operator/schema/physical_create_index.cpp
index b442472b9076..bed54e94b995 100644
--- a/src/execution/operator/schema/physical_create_index.cpp
+++ b/src/execution/operator/schema/physical_create_index.cpp
@@ -1,17 +1,13 @@
 #include "duckdb/execution/operator/schema/physical_create_index.hpp"
 
+#include "duckdb/catalog/catalog_entry/index_catalog_entry.hpp"
 #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
 #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
 #include "duckdb/execution/expression_executor.hpp"
 
-using namespace duckdb;
 using namespace std;
 
-void PhysicalCreateIndex::CreateARTIndex() {
-	auto art = make_unique<ART>(*table.storage, column_ids, move(unbound_expressions), info->unique);
-
-	table.storage->AddIndex(move(art), expressions);
-}
+namespace duckdb {
 
 void PhysicalCreateIndex::GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) {
 	if (column_ids.size() == 0) {
@@ -19,25 +15,28 @@ void PhysicalCreateIndex::GetChunkInternal(ClientContext &context, DataChunk &ch
 	}
 
 	auto &schema = *table.schema;
-	if (!schema.CreateIndex(context, info.get())) {
-		// index already exists, but error ignored because of CREATE ... IF NOT
-		// EXISTS
+	auto index_entry = (IndexCatalogEntry *)schema.CreateIndex(context, info.get());
+	if (!index_entry) {
+		// index already exists, but error ignored because of IF NOT EXISTS
 		return;
 	}
 
-	// create the chunk to hold intermediate expression results
-
+	unique_ptr<Index> index;
 	switch (info->index_type) {
 	case IndexType::ART: {
-		CreateARTIndex();
+		index = make_unique<ART>(*table.storage, column_ids, move(unbound_expressions), info->unique);
 		break;
 	}
 	default:
 		assert(0);
 		throw NotImplementedException("Unimplemented index type");
 	}
+	index_entry->index = index.get();
+	index_entry->info = table.storage->info;
+	table.storage->AddIndex(move(index), expressions);
 
 	chunk.SetCardinality(0);
-
 	state->finished = true;
 }
+
+} // namespace duckdb
diff --git a/src/function/aggregate/distributive/minmax.cpp b/src/function/aggregate/distributive/minmax.cpp
index 9b70853f84de..529393556bd3 100644
--- a/src/function/aggregate/distributive/minmax.cpp
+++ b/src/function/aggregate/distributive/minmax.cpp
@@ -10,79 +10,170 @@ using namespace std;
 
 namespace duckdb {
 
-struct MinMaxBase : public StandardDistributiveFunction {
+template <class T> struct min_max_state_t {
+	T value;
+	bool isset;
+};
+
+template <class OP> static AggregateFunction GetUnaryAggregate(SQLType type) {
+	switch (type.id) {
+	case SQLTypeId::BOOLEAN:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<int8_t>, int8_t, int8_t, OP>(type, type);
+	case SQLTypeId::TINYINT:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<int8_t>, int8_t, int8_t, OP>(type, type);
+	case SQLTypeId::SMALLINT:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<int16_t>, int16_t, int16_t, OP>(type, type);
+	case SQLTypeId::INTEGER:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<int32_t>, int32_t, int32_t, OP>(type, type);
+	case SQLTypeId::BIGINT:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<int64_t>, int64_t, int64_t, OP>(type, type);
+	case SQLTypeId::FLOAT:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<float>, float, float, OP>(type, type);
+	case SQLTypeId::DOUBLE:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<double>, double, double, OP>(type, type);
+	case SQLTypeId::DECIMAL:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<double>, double, double, OP>(type, type);
+	case SQLTypeId::DATE:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<date_t>, date_t, date_t, OP>(type, type);
+	case SQLTypeId::TIMESTAMP:
+		return AggregateFunction::UnaryAggregate<min_max_state_t<timestamp_t>, timestamp_t, timestamp_t, OP>(type,
+		                                                                                                     type);
+	default:
+		throw NotImplementedException("Unimplemented type for unary aggregate");
+	}
+}
+
+struct MinMaxBase {
+	template <class STATE> static void Initialize(STATE *state) {
+		state->isset = false;
+	}
+
 	template <class INPUT_TYPE, class STATE, class OP>
 	static void ConstantOperation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t count) {
 		assert(!nullmask[0]);
-		if (IsNullValue<INPUT_TYPE>(*state)) {
+		if (!state->isset) {
+			state->isset = true;
 			OP::template Assign<INPUT_TYPE, STATE>(state, input[0]);
 		} else {
 			OP::template Execute<INPUT_TYPE, STATE>(state, input[0]);
 		}
 	}
+
+	template <class INPUT_TYPE, class STATE, class OP>
+	static void Operation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t idx) {
+		if (!state->isset) {
+			state->isset = true;
+			OP::template Assign<INPUT_TYPE, STATE>(state, input[idx]);
+		} else {
+			OP::template Execute<INPUT_TYPE, STATE>(state, input[idx]);
+		}
+	}
+
+	static bool IgnoreNull() {
+		return true;
+	}
 };
 
 struct NumericMinMaxBase : public MinMaxBase {
 	template <class INPUT_TYPE, class STATE> static void Assign(STATE *state, INPUT_TYPE input) {
-		*state = input;
+		state->value = input;
 	}
 
 	template <class T, class STATE>
 	static void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {
-		nullmask[idx] = IsNullValue<T>(*state);
-		target[idx] = *state;
+		nullmask[idx] = !state->isset;
+		target[idx] = state->value;
 	}
 };
 
 struct MinOperation : public NumericMinMaxBase {
 	template <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {
-		if (LessThan::Operation<INPUT_TYPE>(input, *state)) {
-			*state = input;
+		if (LessThan::Operation<INPUT_TYPE>(input, state->value)) {
+			state->value = input;
+		}
+	}
+
+	template <class STATE, class OP> static void Combine(STATE source, STATE *target) {
+		if (!source.isset) {
+			// source is NULL, nothing to do
+			return;
+		}
+		if (!target->isset) {
+			// target is NULL, use source value directly
+			*target = source;
+		} else if (target->value > source.value) {
+			target->value = source.value;
 		}
 	}
 };
 
 struct MaxOperation : public NumericMinMaxBase {
 	template <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {
-		if (GreaterThan::Operation<INPUT_TYPE>(input, *state)) {
-			*state = input;
+		if (GreaterThan::Operation<INPUT_TYPE>(input, state->value)) {
+			state->value = input;
+		}
+	}
+
+	template <class STATE, class OP> static void Combine(STATE source, STATE *target) {
+		if (!source.isset) {
+			// source is NULL, nothing to do
+			return;
+		}
+		if (!target->isset) {
+			// target is NULL, use source value directly
+			*target = source;
+		} else if (target->value < source.value) {
+			target->value = source.value;
 		}
 	}
 };
 
 struct StringMinMaxBase : public MinMaxBase {
 	template <class STATE> static void Destroy(STATE *state) {
-		if (!state->IsInlined()) {
-			delete[] state->GetData();
+		if (state->isset && !state->value.IsInlined()) {
+			delete[] state->value.GetData();
 		}
 	}
 
 	template <class INPUT_TYPE, class STATE> static void Assign(STATE *state, INPUT_TYPE input) {
 		if (input.IsInlined()) {
-			*state = input;
+			state->value = input;
 		} else {
 			// non-inlined string, need to allocate space for it
 			auto len = input.GetSize();
 			auto ptr = new char[len + 1];
 			memcpy(ptr, input.GetData(), len + 1);
 
-			*state = string_t(ptr, len);
+			state->value = string_t(ptr, len);
 		}
 	}
 
 	template <class T, class STATE>
 	static void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {
-		if (IsNullValue<string_t>(*state)) {
+		if (!state->isset) {
 			nullmask[idx] = true;
 		} else {
-			target[idx] = StringVector::AddString(result, *state);
+			target[idx] = StringVector::AddString(result, state->value);
+		}
+	}
+
+	template <class STATE, class OP> static void Combine(STATE source, STATE *target) {
+		if (!source.isset) {
+			// source is NULL, nothing to do
+			return;
+		}
+		if (!target->isset) {
+			// target is NULL, use source value directly
+			*target = source;
+		} else {
+			OP::template Execute<string_t, STATE>(target, source.value);
 		}
 	}
 };
 
 struct MinOperationString : public StringMinMaxBase {
 	template <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {
-		if (LessThan::Operation<INPUT_TYPE>(input, *state)) {
+		if (LessThan::Operation<INPUT_TYPE>(input, state->value)) {
 			Assign(state, input);
 		}
 	}
@@ -90,7 +181,7 @@ struct MinOperationString : public StringMinMaxBase {
 
 struct MaxOperationString : public StringMinMaxBase {
 	template <class INPUT_TYPE, class STATE> static void Execute(STATE *state, INPUT_TYPE input) {
-		if (GreaterThan::Operation<INPUT_TYPE>(input, *state)) {
+		if (GreaterThan::Operation<INPUT_TYPE>(input, state->value)) {
 			Assign(state, input);
 		}
 	}
@@ -99,10 +190,11 @@ struct MaxOperationString : public StringMinMaxBase {
 template <class OP, class OP_STRING> static void AddMinMaxOperator(AggregateFunctionSet &set) {
 	for (auto type : SQLType::ALL_TYPES) {
 		if (type.id == SQLTypeId::VARCHAR) {
-			set.AddFunction(AggregateFunction::UnaryAggregateDestructor<string_t, string_t, string_t, OP_STRING>(
-			    SQLType::VARCHAR, SQLType::VARCHAR));
+			set.AddFunction(
+			    AggregateFunction::UnaryAggregateDestructor<min_max_state_t<string_t>, string_t, string_t, OP_STRING>(
+			        SQLType::VARCHAR, SQLType::VARCHAR));
 		} else {
-			set.AddFunction(AggregateFunction::GetUnaryAggregate<OP>(type));
+			set.AddFunction(GetUnaryAggregate<OP>(type));
 		}
 	}
 }
diff --git a/src/include/duckdb/catalog/catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry.hpp
index 357a8444e40c..33252bc9a2de 100644
--- a/src/include/duckdb/catalog/catalog_entry.hpp
+++ b/src/include/duckdb/catalog/catalog_entry.hpp
@@ -34,6 +34,11 @@ class CatalogEntry {
 	virtual unique_ptr<CatalogEntry> Copy(ClientContext &context) {
 		throw CatalogException("Unsupported copy type for catalog entry!");
 	}
+	//! Sets the CatalogEntry as the new root entry (i.e. the newest entry) - this is called on a rollback to an
+	//! AlterEntry
+	virtual void SetAsRoot() {
+	}
+
 	//! The type of this catalog entry
 	CatalogType type;
 	//! Reference to the catalog this entry belongs to
diff --git a/src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp
index 760f5612adc9..ffc264d0ef2f 100644
--- a/src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp
+++ b/src/include/duckdb/catalog/catalog_entry/index_catalog_entry.hpp
@@ -13,14 +13,20 @@
 
 namespace duckdb {
 
+struct DataTableInfo;
+class Index;
+
 //! An index catalog entry
 class IndexCatalogEntry : public StandardEntry {
 public:
 	//! Create a real TableCatalogEntry and initialize storage for it
 	IndexCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, CreateIndexInfo *info)
-	    : StandardEntry(CatalogType::INDEX, schema, catalog, info->index_name) {
-		// FIXME: add more information for drop index support
+	    : StandardEntry(CatalogType::INDEX, schema, catalog, info->index_name), index(nullptr) {
 	}
+	~IndexCatalogEntry();
+
+	Index *index;
+	shared_ptr<DataTableInfo> info;
 };
 
 } // namespace duckdb
diff --git a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp
index 2af53968f1c2..1a418b6625aa 100644
--- a/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp
+++ b/src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp
@@ -22,6 +22,12 @@ class DataTable;
 struct CreateTableInfo;
 struct BoundCreateTableInfo;
 
+struct RenameColumnInfo;
+struct AddColumnInfo;
+struct RemoveColumnInfo;
+struct SetDefaultInfo;
+struct ChangeColumnTypeInfo;
+
 //! A table catalog entry
 class TableCatalogEntry : public StandardEntry {
 public:
@@ -58,5 +64,14 @@ class TableCatalogEntry : public StandardEntry {
 	static unique_ptr<CreateTableInfo> Deserialize(Deserializer &source);
 
 	unique_ptr<CatalogEntry> Copy(ClientContext &context) override;
+
+	void SetAsRoot() override;
+
+private:
+	unique_ptr<CatalogEntry> RenameColumn(ClientContext &context, RenameColumnInfo &info);
+	unique_ptr<CatalogEntry> AddColumn(ClientContext &context, AddColumnInfo &info);
+	unique_ptr<CatalogEntry> RemoveColumn(ClientContext &context, RemoveColumnInfo &info);
+	unique_ptr<CatalogEntry> SetDefault(ClientContext &context, SetDefaultInfo &info);
+	unique_ptr<CatalogEntry> ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info);
 };
 } // namespace duckdb
diff --git a/src/include/duckdb/execution/operator/schema/physical_create_index.hpp b/src/include/duckdb/execution/operator/schema/physical_create_index.hpp
index 2bef7a2bfee6..1e9ddaf820a6 100644
--- a/src/include/duckdb/execution/operator/schema/physical_create_index.hpp
+++ b/src/include/duckdb/execution/operator/schema/physical_create_index.hpp
@@ -41,8 +41,5 @@ class PhysicalCreateIndex : public PhysicalOperator {
 
 public:
 	void GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) override;
-
-private:
-	void CreateARTIndex();
 };
 } // namespace duckdb
diff --git a/src/include/duckdb/function/aggregate/distributive_functions.hpp b/src/include/duckdb/function/aggregate/distributive_functions.hpp
index 89a4797e399d..646c4225664a 100644
--- a/src/include/duckdb/function/aggregate/distributive_functions.hpp
+++ b/src/include/duckdb/function/aggregate/distributive_functions.hpp
@@ -14,39 +14,6 @@
 
 namespace duckdb {
 
-struct StandardDistributiveFunction {
-	template <class STATE> static void Initialize(STATE *state) {
-		*state = NullValue<STATE>();
-	}
-
-	template <class INPUT_TYPE, class STATE, class OP>
-	static void Operation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t idx) {
-		if (IsNullValue<INPUT_TYPE>(*state)) {
-			OP::template Assign<INPUT_TYPE, STATE>(state, input[idx]);
-		} else {
-			OP::template Execute<INPUT_TYPE, STATE>(state, input[idx]);
-		}
-	}
-
-	template <class STATE, class OP> static void Combine(STATE source, STATE *target) {
-		if (IsNullValue<STATE>(source)) {
-			// source is NULL, nothing to do
-			return;
-		}
-		if (IsNullValue<STATE>(*target)) {
-			// target is NULL, use source value directly
-			*target = source;
-		} else {
-			// else perform the operation
-			OP::template Execute<STATE, STATE>(target, source);
-		}
-	}
-
-	static bool IgnoreNull() {
-		return true;
-	}
-};
-
 struct BitAndFun {
 	static void RegisterFunction(BuiltinFunctions &set);
 };
diff --git a/src/include/duckdb/function/aggregate_function.hpp b/src/include/duckdb/function/aggregate_function.hpp
index e580ea358e91..9041b34d4095 100644
--- a/src/include/duckdb/function/aggregate_function.hpp
+++ b/src/include/duckdb/function/aggregate_function.hpp
@@ -110,51 +110,6 @@ class AggregateFunction : public SimpleFunction {
 		return aggregate;
 	};
 
-public:
-	template <class OP> static AggregateFunction GetNumericUnaryAggregate(SQLType type) {
-		switch (type.id) {
-		case SQLTypeId::TINYINT:
-			return UnaryAggregate<int8_t, int8_t, int8_t, OP>(type, type);
-		case SQLTypeId::SMALLINT:
-			return UnaryAggregate<int16_t, int16_t, int16_t, OP>(type, type);
-		case SQLTypeId::INTEGER:
-			return UnaryAggregate<int32_t, int32_t, int32_t, OP>(type, type);
-		case SQLTypeId::BIGINT:
-			return UnaryAggregate<int64_t, int64_t, int64_t, OP>(type, type);
-		case SQLTypeId::FLOAT:
-			return UnaryAggregate<float, float, float, OP>(type, type);
-		case SQLTypeId::DOUBLE:
-			return UnaryAggregate<double, double, double, OP>(type, type);
-		case SQLTypeId::DECIMAL:
-			return UnaryAggregate<double, double, double, OP>(type, type);
-		default:
-			throw NotImplementedException("Unimplemented numeric type for unary aggregate");
-		}
-	}
-
-	template <class OP> static AggregateFunction GetUnaryAggregate(SQLType type) {
-		switch (type.id) {
-		case SQLTypeId::BOOLEAN:
-			return UnaryAggregate<int8_t, int8_t, int8_t, OP>(type, type);
-		case SQLTypeId::TINYINT:
-		case SQLTypeId::SMALLINT:
-		case SQLTypeId::INTEGER:
-		case SQLTypeId::BIGINT:
-		case SQLTypeId::FLOAT:
-		case SQLTypeId::DOUBLE:
-		case SQLTypeId::DECIMAL:
-			return GetNumericUnaryAggregate<OP>(type);
-		case SQLTypeId::DATE:
-			return UnaryAggregate<date_t, date_t, date_t, OP>(type, type);
-		case SQLTypeId::TIMESTAMP:
-			return UnaryAggregate<timestamp_t, timestamp_t, timestamp_t, OP>(type, type);
-		case SQLTypeId::VARCHAR:
-			return UnaryAggregate<string_t, string_t, string_t, OP>(type, type);
-		default:
-			throw NotImplementedException("Unimplemented type for unary aggregate");
-		}
-	}
-
 public:
 	template <class STATE> static idx_t StateSize() {
 		return sizeof(STATE);
diff --git a/src/include/duckdb/parser/column_definition.hpp b/src/include/duckdb/parser/column_definition.hpp
index 07de0cdae0b5..287cae8a582d 100644
--- a/src/include/duckdb/parser/column_definition.hpp
+++ b/src/include/duckdb/parser/column_definition.hpp
@@ -31,5 +31,12 @@ class ColumnDefinition {
 	SQLType type;
 	//! The default value of the column (if any)
 	unique_ptr<ParsedExpression> default_value;
+
+public:
+	ColumnDefinition Copy();
+
+	void Serialize(Serializer &serializer);
+	static ColumnDefinition Deserialize(Deserializer &source);
 };
+
 } // namespace duckdb
diff --git a/src/include/duckdb/parser/parsed_data/alter_table_info.hpp b/src/include/duckdb/parser/parsed_data/alter_table_info.hpp
index 7c1de035b644..798c71818c04 100644
--- a/src/include/duckdb/parser/parsed_data/alter_table_info.hpp
+++ b/src/include/duckdb/parser/parsed_data/alter_table_info.hpp
@@ -9,6 +9,7 @@
 #pragma once
 
 #include "duckdb/parser/parsed_data/parse_info.hpp"
+#include "duckdb/parser/column_definition.hpp"
 
 namespace duckdb {
 
@@ -26,7 +27,15 @@ struct AlterInfo : public ParseInfo {
 	static unique_ptr<AlterInfo> Deserialize(Deserializer &source);
 };
 
-enum class AlterTableType : uint8_t { INVALID = 0, RENAME_COLUMN = 1, RENAME_TABLE = 2 };
+enum class AlterTableType : uint8_t {
+	INVALID = 0,
+	RENAME_COLUMN = 1,
+	RENAME_TABLE = 2,
+	ADD_COLUMN = 3,
+	REMOVE_COLUMN = 4,
+	ALTER_COLUMN_TYPE = 5,
+	SET_DEFAULT = 6
+};
 
 struct AlterTableInfo : public AlterInfo {
 	AlterTableInfo(AlterTableType type, string schema, string table)
@@ -41,10 +50,14 @@ struct AlterTableInfo : public AlterInfo {
 	//! Table name to alter to
 	string table;
 
+public:
 	virtual void Serialize(Serializer &serializer) override;
 	static unique_ptr<AlterInfo> Deserialize(Deserializer &source);
 };
 
+//===--------------------------------------------------------------------===//
+// RenameColumnInfo
+//===--------------------------------------------------------------------===//
 struct RenameColumnInfo : public AlterTableInfo {
 	RenameColumnInfo(string schema, string table, string name, string new_name)
 	    : AlterTableInfo(AlterTableType::RENAME_COLUMN, schema, table), name(name), new_name(new_name) {
@@ -57,10 +70,14 @@ struct RenameColumnInfo : public AlterTableInfo {
 	//! Column new name
 	string new_name;
 
+public:
 	void Serialize(Serializer &serializer) override;
 	static unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);
 };
 
+//===--------------------------------------------------------------------===//
+// RenameTableInfo
+//===--------------------------------------------------------------------===//
 struct RenameTableInfo : public AlterTableInfo {
 	RenameTableInfo(string schema, string table, string new_name)
 	    : AlterTableInfo(AlterTableType::RENAME_TABLE, schema, table), new_table_name(new_name) {
@@ -71,6 +88,91 @@ struct RenameTableInfo : public AlterTableInfo {
 	//! Table new name
 	string new_table_name;
 
+public:
+	void Serialize(Serializer &serializer) override;
+	static unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);
+};
+
+//===--------------------------------------------------------------------===//
+// AddColumnInfo
+//===--------------------------------------------------------------------===//
+struct AddColumnInfo : public AlterTableInfo {
+	AddColumnInfo(string schema, string table, ColumnDefinition new_column)
+	    : AlterTableInfo(AlterTableType::ADD_COLUMN, schema, table), new_column(move(new_column)) {
+	}
+	~AddColumnInfo() override {
+	}
+
+	//! New column
+	ColumnDefinition new_column;
+
+public:
+	void Serialize(Serializer &serializer) override;
+	static unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);
+};
+
+//===--------------------------------------------------------------------===//
+// RemoveColumnInfo
+//===--------------------------------------------------------------------===//
+struct RemoveColumnInfo : public AlterTableInfo {
+	RemoveColumnInfo(string schema, string table, string removed_column, bool if_exists)
+	    : AlterTableInfo(AlterTableType::REMOVE_COLUMN, schema, table), removed_column(move(removed_column)),
+	      if_exists(if_exists) {
+	}
+	~RemoveColumnInfo() override {
+	}
+
+	//! The column to remove
+	string removed_column;
+	//! Whether or not an error should be thrown if the column does not exist
+	bool if_exists;
+
+public:
+	void Serialize(Serializer &serializer) override;
+	static unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);
+};
+
+//===--------------------------------------------------------------------===//
+// ChangeColumnTypeInfo
+//===--------------------------------------------------------------------===//
+struct ChangeColumnTypeInfo : public AlterTableInfo {
+	ChangeColumnTypeInfo(string schema, string table, string column_name, SQLType target_type,
+	                     unique_ptr<ParsedExpression> expression)
+	    : AlterTableInfo(AlterTableType::ALTER_COLUMN_TYPE, schema, table), column_name(move(column_name)),
+	      target_type(move(target_type)), expression(move(expression)) {
+	}
+	~ChangeColumnTypeInfo() override {
+	}
+
+	//! The column name to alter
+	string column_name;
+	//! The target type of the column
+	SQLType target_type;
+	//! The expression used for data conversion
+	unique_ptr<ParsedExpression> expression;
+
+public:
+	void Serialize(Serializer &serializer) override;
+	static unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);
+};
+
+//===--------------------------------------------------------------------===//
+// SetDefaultInfo
+//===--------------------------------------------------------------------===//
+struct SetDefaultInfo : public AlterTableInfo {
+	SetDefaultInfo(string schema, string table, string column_name, unique_ptr<ParsedExpression> new_default)
+	    : AlterTableInfo(AlterTableType::SET_DEFAULT, schema, table), column_name(move(column_name)),
+	      expression(move(new_default)) {
+	}
+	~SetDefaultInfo() override {
+	}
+
+	//! The column name to alter
+	string column_name;
+	//! The expression used for data conversion
+	unique_ptr<ParsedExpression> expression;
+
+public:
 	void Serialize(Serializer &serializer) override;
 	static unique_ptr<AlterInfo> Deserialize(Deserializer &source, string schema, string table);
 };
diff --git a/src/include/duckdb/parser/statement/alter_table_statement.hpp b/src/include/duckdb/parser/statement/alter_table_statement.hpp
index 790114971efd..977a4124d5b0 100644
--- a/src/include/duckdb/parser/statement/alter_table_statement.hpp
+++ b/src/include/duckdb/parser/statement/alter_table_statement.hpp
@@ -11,15 +11,17 @@
 #include "duckdb/parser/column_definition.hpp"
 #include "duckdb/parser/parsed_data/alter_table_info.hpp"
 #include "duckdb/parser/sql_statement.hpp"
-#include "duckdb/parser/tableref.hpp"
 
 namespace duckdb {
 
 class AlterTableStatement : public SQLStatement {
 public:
-	AlterTableStatement(unique_ptr<AlterTableInfo> info) : SQLStatement(StatementType::ALTER_STATEMENT), info(std::move(info)){};
+	AlterTableStatement() : SQLStatement(StatementType::ALTER_STATEMENT) {
+	}
+	AlterTableStatement(unique_ptr<AlterTableInfo> info)
+	    : SQLStatement(StatementType::ALTER_STATEMENT), info(std::move(info)) {
+	}
 
-	unique_ptr<TableRef> table;
 	unique_ptr<AlterTableInfo> info;
 };
 
diff --git a/src/include/duckdb/parser/transformer.hpp b/src/include/duckdb/parser/transformer.hpp
index e7ea14b09443..c7e2c1ff0d0d 100644
--- a/src/include/duckdb/parser/transformer.hpp
+++ b/src/include/duckdb/parser/transformer.hpp
@@ -151,6 +151,7 @@ class Transformer {
 
 	string TransformCollation(PGCollateClause *collate);
 
+	ColumnDefinition TransformColumnDefinition(PGColumnDef *cdef);
 	//===--------------------------------------------------------------------===//
 	// Helpers
 	//===--------------------------------------------------------------------===//
diff --git a/src/include/duckdb/planner/expression_binder/alter_binder.hpp b/src/include/duckdb/planner/expression_binder/alter_binder.hpp
new file mode 100644
index 000000000000..fe608b04b876
--- /dev/null
+++ b/src/include/duckdb/planner/expression_binder/alter_binder.hpp
@@ -0,0 +1,33 @@
+//===----------------------------------------------------------------------===//
+//                         DuckDB
+//
+// duckdb/planner/expression_binder/alter_binder.hpp
+//
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include "duckdb/parser/column_definition.hpp"
+#include "duckdb/planner/expression_binder.hpp"
+
+namespace duckdb {
+//! The ALTER binder is responsible for binding an expression within alter statements
+class AlterBinder : public ExpressionBinder {
+public:
+	AlterBinder(Binder &binder, ClientContext &context, string table, vector<ColumnDefinition> &columns,
+	            vector<column_t> &bound_columns, SQLType target_type);
+
+	string table;
+	vector<ColumnDefinition> &columns;
+	vector<column_t> &bound_columns;
+
+protected:
+	BindResult BindExpression(ParsedExpression &expr, idx_t depth, bool root_expression = false) override;
+
+	BindResult BindColumn(ColumnRefExpression &expr);
+
+	string UnsupportedAggregateMessage() override;
+};
+
+} // namespace duckdb
diff --git a/src/include/duckdb/storage/column_data.hpp b/src/include/duckdb/storage/column_data.hpp
index 92d184ffaddd..3672800639d7 100644
--- a/src/include/duckdb/storage/column_data.hpp
+++ b/src/include/duckdb/storage/column_data.hpp
@@ -14,20 +14,22 @@
 #include "duckdb/storage/table/persistent_segment.hpp"
 
 namespace duckdb {
-class DataTable;
 class PersistentSegment;
 class Transaction;
 
+struct DataTableInfo;
+
 class ColumnData {
 public:
-	ColumnData();
+	ColumnData(BufferManager &manager, DataTableInfo &table_info);
 	//! Set up the column data with the set of persistent segments, returns the amount of rows
 	void Initialize(vector<unique_ptr<PersistentSegment>> &segments);
 
+	DataTableInfo &table_info;
 	//! The type of the column
 	TypeId type;
-	//! The table of the column
-	DataTable *table;
+	//! The buffer manager
+	BufferManager &manager;
 	//! The column index of the column
 	idx_t column_idx;
 	//! The segments holding the data of the column
diff --git a/src/include/duckdb/storage/data_table.hpp b/src/include/duckdb/storage/data_table.hpp
index 3d8145b07174..e18ab68b83ce 100644
--- a/src/include/duckdb/storage/data_table.hpp
+++ b/src/include/duckdb/storage/data_table.hpp
@@ -26,11 +26,13 @@
 namespace duckdb {
 class ClientContext;
 class ColumnDefinition;
+class DataTable;
 class StorageManager;
 class TableCatalogEntry;
 class Transaction;
 
 typedef unique_ptr<vector<unique_ptr<PersistentSegment>>[]> persistent_data_t;
+
 //! TableFilter represents a filter pushed down into the table scan.
 class TableFilter {
 public:
@@ -41,10 +43,9 @@ class TableFilter {
 	idx_t column_index;
 };
 
-//! DataTable represents a physical table on disk
-class DataTable {
-public:
-	DataTable(StorageManager &storage, string schema, string table, vector<TypeId> types, persistent_data_t data);
+struct DataTableInfo {
+	DataTableInfo(string schema, string table) : cardinality(0), schema(move(schema)), table(move(table)) {
+	}
 
 	//! The amount of elements in the table. Note that this number signifies the amount of COMMITTED entries in the
 	//! table. It can be inaccurate inside of transactions. More work is needed to properly support that.
@@ -53,12 +54,32 @@ class DataTable {
 	string schema;
 	// name of the table
 	string table;
+	//! Indexes associated with the current table
+	vector<unique_ptr<Index>> indexes;
+
+	bool IsTemporary() {
+		return schema == TEMP_SCHEMA;
+	}
+};
+
+//! DataTable represents a physical table on disk
+class DataTable {
+public:
+	//! Constructs a new data table from an (optional) set of persistent segments
+	DataTable(StorageManager &storage, string schema, string table, vector<TypeId> types, persistent_data_t data);
+	//! Constructs a DataTable as a delta on an existing data table with a newly added column
+	DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value);
+	//! Constructs a DataTable as a delta on an existing data table but with one column removed
+	DataTable(ClientContext &context, DataTable &parent, idx_t removed_column);
+	//! Constructs a DataTable as a delta on an existing data table but with one column changed type
+	DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, SQLType target_type,
+	          vector<column_t> bound_columns, Expression &cast_expr);
+
+	shared_ptr<DataTableInfo> info;
 	//! Types managed by data table
 	vector<TypeId> types;
 	//! A reference to the base storage manager
 	StorageManager &storage;
-	//! Indexes
-	vector<unique_ptr<Index>> indexes;
 
 public:
 	void InitializeScan(TableScanState &state, vector<column_t> column_ids,
@@ -114,8 +135,10 @@ class DataTable {
 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers);
 	//! Remove the row identifiers from all the indexes of the table
 	void RemoveFromIndexes(Vector &row_identifiers, idx_t count);
-	//! Is this a temporary table?
-	bool IsTemporary();
+
+	void SetAsRoot() {
+		this->is_root = true;
+	}
 
 private:
 	//! Verify constraints with a chunk from the Append containing all columns of the table
@@ -141,13 +164,18 @@ class DataTable {
 	//! The CreateIndexScan is a special scan that is used to create an index on the table, it keeps locks on the table
 	void InitializeCreateIndexScan(CreateIndexScanState &state, vector<column_t> column_ids);
 	void CreateIndexScan(CreateIndexScanState &structure, DataChunk &result);
+
+private:
 	//! Lock for appending entries to the table
 	std::mutex append_lock;
 	//! The version manager of the persistent segments of the tree
-	VersionManager persistent_manager;
+	shared_ptr<VersionManager> persistent_manager;
 	//! The version manager of the transient segments of the tree
-	VersionManager transient_manager;
+	shared_ptr<VersionManager> transient_manager;
 	//! The physical columns of the table
-	unique_ptr<ColumnData[]> columns;
+	vector<shared_ptr<ColumnData>> columns;
+	//! Whether or not the data table is the root DataTable for this table; the root DataTable is the newest version
+	//! that can be appended to
+	bool is_root;
 };
 } // namespace duckdb
diff --git a/src/include/duckdb/storage/table/version_manager.hpp b/src/include/duckdb/storage/table/version_manager.hpp
index f6c9fcf58334..8c9640b46846 100644
--- a/src/include/duckdb/storage/table/version_manager.hpp
+++ b/src/include/duckdb/storage/table/version_manager.hpp
@@ -20,13 +20,15 @@ class DataTable;
 class Transaction;
 class VersionManager;
 
+struct DataTableInfo;
+
 class VersionManager {
 public:
-	VersionManager(DataTable &table) : table(table), max_row(0), base_row(0) {
+	VersionManager(DataTableInfo &table_info) : table_info(table_info), max_row(0), base_row(0) {
 	}
 
-	//! The DataTable
-	DataTable &table;
+	//! The DataTableInfo
+	DataTableInfo &table_info;
 	//! The read/write lock for the delete info and insert info
 	StorageLock lock;
 	//! The info for each of the chunks
@@ -46,7 +48,7 @@ class VersionManager {
 	bool Fetch(Transaction &transaction, idx_t row);
 
 	//! Delete the given set of rows in the version manager
-	void Delete(Transaction &transaction, Vector &row_ids, idx_t count);
+	void Delete(Transaction &transaction, DataTable *table, Vector &row_ids, idx_t count);
 	//! Append a set of rows to the version manager, setting their inserted id to the given commit_id
 	void Append(Transaction &transaction, row_t row_start, idx_t count, transaction_t commit_id);
 	//! Revert a set of appends made to the version manager from the rows [row_start] until [row_end]
diff --git a/src/include/duckdb/transaction/commit_state.hpp b/src/include/duckdb/transaction/commit_state.hpp
index 790c5e9d0d1b..21badd123a81 100644
--- a/src/include/duckdb/transaction/commit_state.hpp
+++ b/src/include/duckdb/transaction/commit_state.hpp
@@ -13,9 +13,9 @@
 namespace duckdb {
 class CatalogEntry;
 class DataChunk;
-class DataTable;
 class WriteAheadLog;
 
+struct DataTableInfo;
 struct DeleteInfo;
 struct UpdateInfo;
 
@@ -27,7 +27,7 @@ class CommitState {
 	transaction_t commit_id;
 	UndoFlags current_op;
 
-	DataTable *current_table;
+	DataTableInfo *current_table_info;
 	idx_t row_identifiers[STANDARD_VECTOR_SIZE];
 
 	unique_ptr<DataChunk> delete_chunk;
@@ -38,7 +38,7 @@ class CommitState {
 	void RevertCommit(UndoFlags type, data_ptr_t data);
 
 private:
-	void SwitchTable(DataTable *table, UndoFlags new_op);
+	void SwitchTable(DataTableInfo *table, UndoFlags new_op);
 
 	void WriteCatalogEntry(CatalogEntry *entry, data_ptr_t extra_data);
 	void WriteDelete(DeleteInfo *info);
diff --git a/src/include/duckdb/transaction/delete_info.hpp b/src/include/duckdb/transaction/delete_info.hpp
index afd7bcef0d8d..89f57260a845 100644
--- a/src/include/duckdb/transaction/delete_info.hpp
+++ b/src/include/duckdb/transaction/delete_info.hpp
@@ -15,12 +15,11 @@ class ChunkInfo;
 class DataTable;
 
 struct DeleteInfo {
+	DataTable *table;
 	ChunkInfo *vinfo;
 	idx_t count;
 	idx_t base_row;
 	row_t rows[1];
-
-	DataTable &GetTable();
 };
 
 } // namespace duckdb
diff --git a/src/include/duckdb/transaction/local_storage.hpp b/src/include/duckdb/transaction/local_storage.hpp
index 9c3081378421..704bcb01e519 100644
--- a/src/include/duckdb/transaction/local_storage.hpp
+++ b/src/include/duckdb/transaction/local_storage.hpp
@@ -68,6 +68,10 @@ class LocalStorage {
 		return table_storage.size() > 0;
 	}
 
+	void AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column, Expression *default_value);
+	void ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, SQLType target_type,
+	                vector<column_t> bound_columns, Expression &cast_expr);
+
 private:
 	LocalTableStorage *GetStorage(DataTable *table);
 
diff --git a/src/include/duckdb/transaction/transaction.hpp b/src/include/duckdb/transaction/transaction.hpp
index 697da7f05ae2..ef4682cee779 100644
--- a/src/include/duckdb/transaction/transaction.hpp
+++ b/src/include/duckdb/transaction/transaction.hpp
@@ -78,7 +78,7 @@ class Transaction {
 		return start_timestamp;
 	}
 
-	void PushDelete(ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row);
+	void PushDelete(DataTable *table, ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row);
 
 	UpdateInfo *CreateUpdateInfo(idx_t type_size, idx_t entries);
 
diff --git a/src/optimizer/index_scan.cpp b/src/optimizer/index_scan.cpp
index e110f9c27463..f013b57b97da 100644
--- a/src/optimizer/index_scan.cpp
+++ b/src/optimizer/index_scan.cpp
@@ -56,14 +56,14 @@ unique_ptr<LogicalOperator> IndexScan::TransformFilterToIndexScan(unique_ptr<Log
 
 	auto &storage = *get->table->storage;
 
-	if (storage.indexes.size() == 0) {
+	if (storage.info->indexes.size() == 0) {
 		// no indexes on the table, can't rewrite
 		return op;
 	}
 
 	// check all the indexes
-	for (size_t j = 0; j < storage.indexes.size(); j++) {
-		auto &index = storage.indexes[j];
+	for (size_t j = 0; j < storage.info->indexes.size(); j++) {
+		auto &index = storage.info->indexes[j];
 
 		//		assert(index->unbound_expressions.size() == 1);
 		// first rewrite the index expression so the ColumnBindings align with the column bindings of the current table
diff --git a/src/optimizer/pushdown/pushdown_get.cpp b/src/optimizer/pushdown/pushdown_get.cpp
index 6943322f508f..9f217b34b85c 100644
--- a/src/optimizer/pushdown/pushdown_get.cpp
+++ b/src/optimizer/pushdown/pushdown_get.cpp
@@ -22,7 +22,7 @@ unique_ptr<LogicalOperator> FilterPushdown::PushdownGet(unique_ptr<LogicalOperat
 		}
 	}
 	//! FIXME: We only need to skip if the index is in the column being filtered
-	if (!get.table || !get.table->storage->indexes.empty()) {
+	if (!get.table || !get.table->storage->info->indexes.empty()) {
 		//! now push any existing filters
 		if (filters.empty()) {
 			//! no filters to push
diff --git a/src/parser/CMakeLists.txt b/src/parser/CMakeLists.txt
index 5b5c1d93cb4d..e39b8ba82b81 100644
--- a/src/parser/CMakeLists.txt
+++ b/src/parser/CMakeLists.txt
@@ -12,6 +12,7 @@ add_subdirectory(transform)
 add_library_unity(duckdb_parser
                   OBJECT
                   base_expression.cpp
+                  column_definition.cpp
                   constraint.cpp
                   expression_util.cpp
                   parsed_expression.cpp
diff --git a/src/parser/column_definition.cpp b/src/parser/column_definition.cpp
new file mode 100644
index 000000000000..319fc82292fa
--- /dev/null
+++ b/src/parser/column_definition.cpp
@@ -0,0 +1,26 @@
+#include "duckdb/parser/column_definition.hpp"
+#include "duckdb/common/serializer.hpp"
+
+namespace duckdb {
+
+ColumnDefinition ColumnDefinition::Copy() {
+	ColumnDefinition copy(name, type);
+	copy.oid = oid;
+	copy.default_value = default_value ? default_value->Copy() : nullptr;
+	return copy;
+}
+
+void ColumnDefinition::Serialize(Serializer &serializer) {
+	serializer.WriteString(name);
+	type.Serialize(serializer);
+	serializer.WriteOptional(default_value);
+}
+
+ColumnDefinition ColumnDefinition::Deserialize(Deserializer &source) {
+	auto column_name = source.Read<string>();
+	auto column_type = SQLType::Deserialize(source);
+	auto default_value = source.ReadOptional<ParsedExpression>();
+	return ColumnDefinition(column_name, column_type, move(default_value));
+}
+
+} // namespace duckdb
diff --git a/src/parser/parsed_data/alter_table_info.cpp b/src/parser/parsed_data/alter_table_info.cpp
index f54e4ff39ba8..b23e5176c0c2 100644
--- a/src/parser/parsed_data/alter_table_info.cpp
+++ b/src/parser/parsed_data/alter_table_info.cpp
@@ -35,11 +35,22 @@ unique_ptr<AlterInfo> AlterTableInfo::Deserialize(Deserializer &source) {
 		return RenameColumnInfo::Deserialize(source, schema, table);
 	case AlterTableType::RENAME_TABLE:
 		return RenameTableInfo::Deserialize(source, schema, table);
+	case AlterTableType::ADD_COLUMN:
+		return AddColumnInfo::Deserialize(source, schema, table);
+	case AlterTableType::REMOVE_COLUMN:
+		return RemoveColumnInfo::Deserialize(source, schema, table);
+	case AlterTableType::ALTER_COLUMN_TYPE:
+		return ChangeColumnTypeInfo::Deserialize(source, schema, table);
+	case AlterTableType::SET_DEFAULT:
+		return SetDefaultInfo::Deserialize(source, schema, table);
 	default:
 		throw SerializationException("Unknown alter table type for deserialization!");
 	}
 }
 
+//===--------------------------------------------------------------------===//
+// RenameColumnInfo
+//===--------------------------------------------------------------------===//
 void RenameColumnInfo::Serialize(Serializer &serializer) {
 	AlterTableInfo::Serialize(serializer);
 	serializer.WriteString(name);
@@ -52,6 +63,9 @@ unique_ptr<AlterInfo> RenameColumnInfo::Deserialize(Deserializer &source, string
 	return make_unique<RenameColumnInfo>(schema, table, name, new_name);
 }
 
+//===--------------------------------------------------------------------===//
+// RenameTableInfo
+//===--------------------------------------------------------------------===//
 void RenameTableInfo::Serialize(Serializer &serializer) {
 	AlterTableInfo::Serialize(serializer);
 	serializer.WriteString(new_table_name);
@@ -61,3 +75,63 @@ unique_ptr<AlterInfo> RenameTableInfo::Deserialize(Deserializer &source, string
 	auto new_name = source.Read<string>();
 	return make_unique<RenameTableInfo>(schema, table, new_name);
 }
+
+//===--------------------------------------------------------------------===//
+// AddColumnInfo
+//===--------------------------------------------------------------------===//
+void AddColumnInfo::Serialize(Serializer &serializer) {
+	AlterTableInfo::Serialize(serializer);
+	new_column.Serialize(serializer);
+}
+
+unique_ptr<AlterInfo> AddColumnInfo::Deserialize(Deserializer &source, string schema, string table) {
+	auto new_column = ColumnDefinition::Deserialize(source);
+	return make_unique<AddColumnInfo>(schema, table, move(new_column));
+}
+
+//===--------------------------------------------------------------------===//
+// RemoveColumnInfo
+//===--------------------------------------------------------------------===//
+void RemoveColumnInfo::Serialize(Serializer &serializer) {
+	AlterTableInfo::Serialize(serializer);
+	serializer.WriteString(removed_column);
+	serializer.Write<bool>(if_exists);
+}
+
+unique_ptr<AlterInfo> RemoveColumnInfo::Deserialize(Deserializer &source, string schema, string table) {
+	auto new_name = source.Read<string>();
+	auto if_exists = source.Read<bool>();
+	return make_unique<RemoveColumnInfo>(schema, table, new_name, if_exists);
+}
+
+//===--------------------------------------------------------------------===//
+// ChangeColumnTypeInfo
+//===--------------------------------------------------------------------===//
+void ChangeColumnTypeInfo::Serialize(Serializer &serializer) {
+	AlterTableInfo::Serialize(serializer);
+	serializer.WriteString(column_name);
+	target_type.Serialize(serializer);
+	serializer.WriteOptional(expression);
+}
+
+unique_ptr<AlterInfo> ChangeColumnTypeInfo::Deserialize(Deserializer &source, string schema, string table) {
+	auto column_name = source.Read<string>();
+	auto target_type = SQLType::Deserialize(source);
+	auto expression = source.ReadOptional<ParsedExpression>();
+	return make_unique<ChangeColumnTypeInfo>(schema, table, move(column_name), move(target_type), move(expression));
+}
+
+//===--------------------------------------------------------------------===//
+// SetDefaultInfo
+//===--------------------------------------------------------------------===//
+void SetDefaultInfo::Serialize(Serializer &serializer) {
+	AlterTableInfo::Serialize(serializer);
+	serializer.WriteString(column_name);
+	serializer.WriteOptional(expression);
+}
+
+unique_ptr<AlterInfo> SetDefaultInfo::Deserialize(Deserializer &source, string schema, string table) {
+	auto column_name = source.Read<string>();
+	auto new_default = source.ReadOptional<ParsedExpression>();
+	return make_unique<SetDefaultInfo>(schema, table, move(column_name), move(new_default));
+}
diff --git a/src/parser/transform/statement/transform_alter_table.cpp b/src/parser/transform/statement/transform_alter_table.cpp
index 3d46344ce7e1..b5b59e33ad8c 100644
--- a/src/parser/transform/statement/transform_alter_table.cpp
+++ b/src/parser/transform/statement/transform_alter_table.cpp
@@ -1,45 +1,79 @@
 #include "duckdb/parser/statement/alter_table_statement.hpp"
 #include "duckdb/parser/transformer.hpp"
+#include "duckdb/parser/tableref/basetableref.hpp"
+#include "duckdb/parser/expression/cast_expression.hpp"
+#include "duckdb/parser/expression/columnref_expression.hpp"
+#include "duckdb/parser/constraint.hpp"
 
-using namespace duckdb;
 using namespace std;
 
+namespace duckdb {
+
 unique_ptr<AlterTableStatement> Transformer::TransformAlter(PGNode *node) {
-	throw NotImplementedException("Alter table not supported yet!");
-	// auto stmt = reinterpret_cast<AlterTableStmt *>(node);
-	// assert(stmt);
-	// assert(stmt->relation);
-
-	// auto result = make_unique<AlterTableStatement>();
-	// auto &info = *result->info.get();
-	// auto new_alter_cmd = make_unique<AlterTableCmd>();
-	// result->table = TransformRangeVar(stmt->relation);
-
-	// info.table = stmt->relation->relname;
-
-	// // first we check the type of ALTER
-	// for (auto c = stmt->cmds->head; c != NULL; c = c->next) {
-	// 	auto command = reinterpret_cast<PGAlterTableCmd *>(lfirst(c));
-	// 	//TODO: Include more options for command->subtype
-	// 	switch (command->subtype) {
-	// 		case PG_AT_AddColumn: {
-	//                auto cdef = (ColumnDef *)command->def;
-	//                char *name = (reinterpret_cast<PGValue *>(
-	//                        cdef->typeName->names->tail->data.ptr_value)
-	//                        ->val.str);
-	//                auto centry =
-	//                        ColumnDefinition(cdef->colname,
-	//                        TransformStringToTypeId(name));
-	//                info.new_columns.push_back(centry);
-	//                break;
-	//            }
-	// 		case PG_AT_DropColumn:
-	// 		case PG_AT_AlterColumnType:
-	// 		default:
-	// 			throw NotImplementedException(
-	// 			    "ALTER TABLE option not supported yet!");
-	// 	}
-	// }
-
-	// return result;
+	auto stmt = reinterpret_cast<PGAlterTableStmt *>(node);
+	assert(stmt);
+	assert(stmt->relation);
+
+	auto result = make_unique<AlterTableStatement>();
+
+	auto table = TransformRangeVar(stmt->relation);
+	assert(table->type == TableReferenceType::BASE_TABLE);
+
+	auto &basetable = (BaseTableRef &)*table;
+	// first we check the type of ALTER
+	for (auto c = stmt->cmds->head; c != NULL; c = c->next) {
+		auto command = reinterpret_cast<PGAlterTableCmd *>(lfirst(c));
+		// TODO: Include more options for command->subtype
+		switch (command->subtype) {
+		case PG_AT_AddColumn: {
+			auto cdef = (PGColumnDef *)command->def;
+			auto centry = TransformColumnDefinition(cdef);
+			if (cdef->constraints) {
+				for (auto constr = cdef->constraints->head; constr != nullptr; constr = constr->next) {
+					auto constraint = TransformConstraint(constr, centry, 0);
+					if (constraint) {
+						throw ParserException("Adding columns with constraints not yet supported");
+					}
+				}
+			}
+			result->info = make_unique<AddColumnInfo>(basetable.schema_name, basetable.table_name, move(centry));
+			break;
+		}
+		case PG_AT_DropColumn: {
+			result->info = make_unique<RemoveColumnInfo>(basetable.schema_name, basetable.table_name, command->name,
+			                                             command->missing_ok);
+			break;
+		}
+		case PG_AT_ColumnDefault: {
+			auto expr = TransformExpression(command->def);
+			result->info =
+			    make_unique<SetDefaultInfo>(basetable.schema_name, basetable.table_name, command->name, move(expr));
+			break;
+		}
+		case PG_AT_AlterColumnType: {
+			auto cdef = (PGColumnDef *)command->def;
+			SQLType target_type = TransformTypeName(cdef->typeName);
+			target_type.collation = TransformCollation(cdef->collClause);
+
+			unique_ptr<ParsedExpression> expr;
+			if (cdef->raw_default) {
+				expr = TransformExpression(cdef->raw_default);
+			} else {
+				auto colref = make_unique<ColumnRefExpression>(command->name);
+				expr = make_unique<CastExpression>(target_type, move(colref));
+			}
+			result->info = make_unique<ChangeColumnTypeInfo>(basetable.schema_name, basetable.table_name, command->name,
+			                                                 target_type, move(expr));
+			break;
+		}
+		case PG_AT_DropConstraint:
+		case PG_AT_DropNotNull:
+		default:
+			throw NotImplementedException("ALTER TABLE option not supported yet!");
+		}
+	}
+
+	return result;
 }
+
+} // namespace duckdb
diff --git a/src/parser/transform/statement/transform_create_table.cpp b/src/parser/transform/statement/transform_create_table.cpp
index 9614f9baf533..932422709527 100644
--- a/src/parser/transform/statement/transform_create_table.cpp
+++ b/src/parser/transform/statement/transform_create_table.cpp
@@ -33,6 +33,13 @@ unique_ptr<ParsedExpression> Transformer::TransformCollateExpr(PGCollateClause *
 	return make_unique<CollateExpression>(collation, move(child));
 }
 
+ColumnDefinition Transformer::TransformColumnDefinition(PGColumnDef *cdef) {
+	SQLType target_type = TransformTypeName(cdef->typeName);
+	target_type.collation = TransformCollation(cdef->collClause);
+
+	return ColumnDefinition(cdef->colname, target_type);
+}
+
 unique_ptr<CreateStatement> Transformer::TransformCreateTable(PGNode *node) {
 	auto stmt = reinterpret_cast<PGCreateStmt *>(node);
 	assert(stmt);
@@ -65,11 +72,7 @@ unique_ptr<CreateStatement> Transformer::TransformCreateTable(PGNode *node) {
 		switch (node->type) {
 		case T_PGColumnDef: {
 			auto cdef = (PGColumnDef *)c->data.ptr_value;
-			SQLType target_type = TransformTypeName(cdef->typeName);
-			target_type.collation = TransformCollation(cdef->collClause);
-
-			auto centry = ColumnDefinition(cdef->colname, target_type);
-
+			auto centry = TransformColumnDefinition(cdef);
 			if (cdef->constraints) {
 				for (auto constr = cdef->constraints->head; constr != nullptr; constr = constr->next) {
 					auto constraint = TransformConstraint(constr, centry, info->columns.size());
diff --git a/src/planner/binder/statement/bind_create_table.cpp b/src/planner/binder/statement/bind_create_table.cpp
index f1e995a6ac12..13bde6ad6126 100644
--- a/src/planner/binder/statement/bind_create_table.cpp
+++ b/src/planner/binder/statement/bind_create_table.cpp
@@ -29,6 +29,7 @@ static void BindConstraints(Binder &binder, BoundCreateTableInfo &info) {
 	auto &base = (CreateTableInfo &)*info.base;
 
 	bool has_primary_key = false;
+	unordered_set<idx_t> primary_keys;
 	for (idx_t i = 0; i < base.constraints.size(); i++) {
 		auto &cond = base.constraints[i];
 		switch (cond->type) {
@@ -83,6 +84,7 @@ static void BindConstraints(Binder &binder, BoundCreateTableInfo &info) {
 					throw ParserException("table \"%s\" has more than one primary key", base.table.c_str());
 				}
 				has_primary_key = true;
+				primary_keys = keys;
 			}
 			info.bound_constraints.push_back(make_unique<BoundUniqueConstraint>(keys, unique.is_primary_key));
 			break;
@@ -91,6 +93,13 @@ static void BindConstraints(Binder &binder, BoundCreateTableInfo &info) {
 			throw NotImplementedException("unrecognized constraint type in bind");
 		}
 	}
+	if (has_primary_key) {
+		// if there is a primary key index, also create a NOT NULL constraint for each of the columns
+		for (auto &column_index : primary_keys) {
+			base.constraints.push_back(make_unique<NotNullConstraint>(column_index));
+			info.bound_constraints.push_back(make_unique<BoundNotNullConstraint>(column_index));
+		}
+	}
 }
 
 void Binder::BindDefaultValues(vector<ColumnDefinition> &columns, vector<unique_ptr<Expression>> &bound_defaults) {
diff --git a/src/planner/binder/statement/bind_update.cpp b/src/planner/binder/statement/bind_update.cpp
index d373192187b6..9dd44a36eff9 100644
--- a/src/planner/binder/statement/bind_update.cpp
+++ b/src/planner/binder/statement/bind_update.cpp
@@ -73,7 +73,7 @@ static void BindUpdateConstraints(TableCatalogEntry &table, LogicalGet &get, Log
 	// delete for the insert, we thus need all the columns to be available, hence we check if the update touches any
 	// index columns
 	update.is_index_update = false;
-	for (auto &index : table.storage->indexes) {
+	for (auto &index : table.storage->info->indexes) {
 		if (index->IndexIsUpdated(update.columns)) {
 			update.is_index_update = true;
 		}
diff --git a/src/planner/expression_binder/CMakeLists.txt b/src/planner/expression_binder/CMakeLists.txt
index b1bf26e2291e..2ea615951374 100644
--- a/src/planner/expression_binder/CMakeLists.txt
+++ b/src/planner/expression_binder/CMakeLists.txt
@@ -1,6 +1,7 @@
 add_library_unity(duckdb_expression_binders
                   OBJECT
                   aggregate_binder.cpp
+                  alter_binder.cpp
                   check_binder.cpp
                   constant_binder.cpp
                   group_binder.cpp
diff --git a/src/planner/expression_binder/alter_binder.cpp b/src/planner/expression_binder/alter_binder.cpp
new file mode 100644
index 000000000000..a62b72197e02
--- /dev/null
+++ b/src/planner/expression_binder/alter_binder.cpp
@@ -0,0 +1,50 @@
+#include "duckdb/planner/expression_binder/alter_binder.hpp"
+
+#include "duckdb/parser/expression/columnref_expression.hpp"
+#include "duckdb/planner/expression/bound_reference_expression.hpp"
+
+using namespace std;
+
+namespace duckdb {
+
+AlterBinder::AlterBinder(Binder &binder, ClientContext &context, string table, vector<ColumnDefinition> &columns,
+                         vector<column_t> &bound_columns, SQLType target_type)
+    : ExpressionBinder(binder, context), table(table), columns(columns), bound_columns(bound_columns) {
+	this->target_type = target_type;
+}
+
+BindResult AlterBinder::BindExpression(ParsedExpression &expr, idx_t depth, bool root_expression) {
+	switch (expr.GetExpressionClass()) {
+	case ExpressionClass::WINDOW:
+		return BindResult("window functions are not allowed in alter statement");
+	case ExpressionClass::SUBQUERY:
+		return BindResult("cannot use subquery in alter statement");
+	case ExpressionClass::COLUMN_REF:
+		return BindColumn((ColumnRefExpression &)expr);
+	default:
+		return ExpressionBinder::BindExpression(expr, depth);
+	}
+}
+
+string AlterBinder::UnsupportedAggregateMessage() {
+	return "aggregate functions are not allowed in alter statement";
+}
+
+BindResult AlterBinder::BindColumn(ColumnRefExpression &colref) {
+	if (!colref.table_name.empty() && colref.table_name != table) {
+		throw BinderException("Cannot reference table %s from within alter statement for table %s!",
+		                      colref.table_name.c_str(), table.c_str());
+	}
+	for (idx_t i = 0; i < columns.size(); i++) {
+		if (colref.column_name == columns[i].name) {
+			bound_columns.push_back(i);
+			return BindResult(
+			    make_unique<BoundReferenceExpression>(GetInternalType(columns[i].type), bound_columns.size() - 1),
+			    columns[i].type);
+		}
+	}
+	throw BinderException("Table does not contain column %s referenced in alter statement!",
+	                      colref.column_name.c_str());
+}
+
+} // namespace duckdb
diff --git a/src/planner/operator/logical_get.cpp b/src/planner/operator/logical_get.cpp
index 91d3663943a5..0beb5cefc1a5 100644
--- a/src/planner/operator/logical_get.cpp
+++ b/src/planner/operator/logical_get.cpp
@@ -46,7 +46,7 @@ void LogicalGet::ResolveTypes() {
 
 idx_t LogicalGet::EstimateCardinality() {
 	if (table) {
-		return table->storage->cardinality;
+		return table->storage->info->cardinality;
 	} else {
 		return 1;
 	}
diff --git a/src/storage/column_data.cpp b/src/storage/column_data.cpp
index 2cfb5a82d5f3..cf804b8a98fd 100644
--- a/src/storage/column_data.cpp
+++ b/src/storage/column_data.cpp
@@ -7,7 +7,8 @@
 using namespace duckdb;
 using namespace std;
 
-ColumnData::ColumnData() : persistent_rows(0) {
+ColumnData::ColumnData(BufferManager &manager, DataTableInfo &table_info)
+    : table_info(table_info), manager(manager), persistent_rows(0) {
 }
 
 void ColumnData::Initialize(vector<unique_ptr<PersistentSegment>> &segments) {
@@ -173,6 +174,6 @@ void ColumnData::FetchRow(ColumnFetchState &state, Transaction &transaction, row
 }
 
 void ColumnData::AppendTransientSegment(idx_t start_row) {
-	auto new_segment = make_unique<TransientSegment>(*table->storage.buffer_manager, type, start_row);
+	auto new_segment = make_unique<TransientSegment>(manager, type, start_row);
 	data.AppendSegment(move(new_segment));
 }
diff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp
index 0b064349fd81..908e9cf9c813 100644
--- a/src/storage/data_table.cpp
+++ b/src/storage/data_table.cpp
@@ -9,6 +9,7 @@
 #include "duckdb/transaction/transaction.hpp"
 #include "duckdb/transaction/transaction_manager.hpp"
 #include "duckdb/storage/table/transient_segment.hpp"
+#include "duckdb/storage/storage_manager.hpp"
 
 using namespace duckdb;
 using namespace std;
@@ -16,28 +17,159 @@ using namespace chrono;
 
 DataTable::DataTable(StorageManager &storage, string schema, string table, vector<TypeId> types_,
                      unique_ptr<vector<unique_ptr<PersistentSegment>>[]> data)
-    : cardinality(0), schema(schema), table(table), types(types_), storage(storage), persistent_manager(*this),
-      transient_manager(*this) {
+    : info(make_shared<DataTableInfo>(schema, table)), types(types_), storage(storage),
+      persistent_manager(make_shared<VersionManager>(*info)), transient_manager(make_shared<VersionManager>(*info)),
+      is_root(true) {
 	// set up the segment trees for the column segments
-	columns = unique_ptr<ColumnData[]>(new ColumnData[types.size()]);
 	for (idx_t i = 0; i < types.size(); i++) {
-		columns[i].type = types[i];
-		columns[i].table = this;
-		columns[i].column_idx = i;
+		auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
+		column_data->type = types[i];
+		column_data->column_idx = i;
+		columns.push_back(move(column_data));
 	}
 
 	// initialize the table with the existing data from disk, if any
 	if (data && data[0].size() > 0) {
 		// first append all the segments to the set of column segments
 		for (idx_t i = 0; i < types.size(); i++) {
-			columns[i].Initialize(data[i]);
-			if (columns[i].persistent_rows != columns[0].persistent_rows) {
+			columns[i]->Initialize(data[i]);
+			if (columns[i]->persistent_rows != columns[0]->persistent_rows) {
 				throw Exception("Column length mismatch in table load!");
 			}
 		}
-		persistent_manager.max_row = columns[0].persistent_rows;
-		transient_manager.base_row = persistent_manager.max_row;
+		persistent_manager->max_row = columns[0]->persistent_rows;
+		transient_manager->base_row = persistent_manager->max_row;
+	}
+}
+
+DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value)
+    : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
+      transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
+	// prevent any new tuples from being added to the parent
+	lock_guard<mutex> parent_lock(parent.append_lock);
+	// this table replaces the previous table, hence the parent is no longer the root DataTable
+	parent.is_root = false;
+	// add the new column to this DataTable
+	auto new_column_type = GetInternalType(new_column.type);
+	idx_t new_column_idx = columns.size();
+
+	types.push_back(new_column_type);
+	auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
+	column_data->type = new_column_type;
+	column_data->column_idx = new_column_idx;
+	columns.push_back(move(column_data));
+
+	// fill the column with its DEFAULT value, or NULL if none is specified
+	idx_t rows_to_write = persistent_manager->max_row + transient_manager->max_row;
+	if (rows_to_write > 0) {
+		ExpressionExecutor executor;
+		DataChunk dummy_chunk;
+		Vector result(new_column_type);
+		if (!default_value) {
+			FlatVector::Nullmask(result).set();
+		} else {
+			executor.AddExpression(*default_value);
+		}
+
+		ColumnAppendState state;
+		columns[new_column_idx]->InitializeAppend(state);
+		for (idx_t i = 0; i < rows_to_write; i += STANDARD_VECTOR_SIZE) {
+			idx_t rows_in_this_vector = std::min(rows_to_write - i, (idx_t)STANDARD_VECTOR_SIZE);
+			if (default_value) {
+				dummy_chunk.SetCardinality(rows_in_this_vector);
+				executor.ExecuteExpression(dummy_chunk, result);
+			}
+			columns[new_column_idx]->Append(state, result, rows_in_this_vector);
+		}
+	}
+	// also add this column to client local storage
+	Transaction::GetTransaction(context).storage.AddColumn(&parent, this, new_column, default_value);
+}
+
+DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
+    : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
+      transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
+	// prevent any new tuples from being added to the parent
+	lock_guard<mutex> parent_lock(parent.append_lock);
+	// first check if there are any indexes that exist that point to the removed column
+	for (auto &index : info->indexes) {
+		for (auto &column_id : index->column_ids) {
+			if (column_id == removed_column) {
+				throw CatalogException("Cannot drop this column: an index depends on it!");
+			} else if (column_id > removed_column) {
+				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
+			}
+		}
 	}
+	// this table replaces the previous table, hence the parent is no longer the root DataTable
+	parent.is_root = false;
+	// erase the column from this DataTable
+	assert(removed_column < types.size());
+	types.erase(types.begin() + removed_column);
+	columns.erase(columns.begin() + removed_column);
+}
+
+DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, SQLType target_type,
+                     vector<column_t> bound_columns, Expression &cast_expr)
+    : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
+      transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
+
+	// prevent any new tuples from being added to the parent
+	lock_guard<mutex> parent_lock(parent.append_lock);
+	// this table replaces the previous table, hence the parent is no longer the root DataTable
+	parent.is_root = false;
+	// first check if there are any indexes that exist that point to the changed column
+	for (auto &index : info->indexes) {
+		for (auto &column_id : index->column_ids) {
+			if (column_id == changed_idx) {
+				throw CatalogException("Cannot change the type of this column: an index depends on it!");
+			}
+		}
+	}
+	// change the type in this DataTable
+	auto new_type = GetInternalType(target_type);
+	types[changed_idx] = new_type;
+
+	// construct a new column data for this type
+	auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
+	column_data->type = new_type;
+	column_data->column_idx = changed_idx;
+
+	ColumnAppendState append_state;
+	column_data->InitializeAppend(append_state);
+
+	// scan the original table, and fill the new column with the transformed value
+	auto &transaction = Transaction::GetTransaction(context);
+	TableScanState scan_state;
+
+	vector<TypeId> types;
+	for (idx_t i = 0; i < bound_columns.size(); i++) {
+		types.push_back(parent.types[i]);
+	}
+	parent.InitializeScan(transaction, scan_state, bound_columns, nullptr);
+
+	DataChunk scan_chunk;
+	scan_chunk.Initialize(types);
+	unordered_map<idx_t, vector<TableFilter>> dummy_filters;
+
+	ExpressionExecutor executor;
+	executor.AddExpression(cast_expr);
+
+	Vector append_vector(new_type);
+	while (true) {
+		// scan the table
+		parent.Scan(transaction, scan_chunk, scan_state, dummy_filters);
+		if (scan_chunk.size() == 0) {
+			break;
+		}
+		// execute the expression
+		executor.ExecuteExpression(scan_chunk, append_vector);
+		column_data->Append(append_state, append_vector, scan_chunk.size());
+	}
+	// also add this column to client local storage
+	transaction.storage.ChangeType(&parent, this, changed_idx, target_type, bound_columns, cast_expr);
+
+	columns[changed_idx] = move(column_data);
 }
 
 //===--------------------------------------------------------------------===//
@@ -50,16 +182,16 @@ void DataTable::InitializeScan(TableScanState &state, vector<column_t> column_id
 	for (idx_t i = 0; i < column_ids.size(); i++) {
 		auto column = column_ids[i];
 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
-			columns[column].InitializeScan(state.column_scans[i]);
+			columns[column]->InitializeScan(state.column_scans[i]);
 		}
 	}
 	state.column_ids = move(column_ids);
 	// initialize the chunk scan state
 	state.offset = 0;
 	state.current_persistent_row = 0;
-	state.max_persistent_row = persistent_manager.max_row;
+	state.max_persistent_row = persistent_manager->max_row;
 	state.current_transient_row = 0;
-	state.max_transient_row = transient_manager.max_row;
+	state.max_transient_row = transient_manager->max_row;
 	if (table_filters && table_filters->size() > 0) {
 		state.adaptive_filter = make_unique<AdaptiveFilter>(*table_filters);
 	}
@@ -75,14 +207,14 @@ void DataTable::Scan(Transaction &transaction, DataChunk &result, TableScanState
                      unordered_map<idx_t, vector<TableFilter>> &table_filters) {
 	// scan the persistent segments
 	while (ScanBaseTable(transaction, result, state, state.current_persistent_row, state.max_persistent_row, 0,
-	                     persistent_manager, table_filters)) {
+	                     *persistent_manager, table_filters)) {
 		if (result.size() > 0) {
 			return;
 		}
 	}
 	// scan the transient segments
 	while (ScanBaseTable(transaction, result, state, state.current_transient_row, state.max_transient_row,
-	                     persistent_manager.max_row, transient_manager, table_filters)) {
+	                     persistent_manager->max_row, *transient_manager, table_filters)) {
 		if (result.size() > 0) {
 			return;
 		}
@@ -237,7 +369,7 @@ bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, Table
 				assert(result.data[i].type == ROW_TYPE);
 				result.data[i].Sequence(base_row + current_row, 1);
 			} else {
-				columns[column].Scan(transaction, state.column_scans[i], result.data[i]);
+				columns[column]->Scan(transaction, state.column_scans[i], result.data[i]);
 			}
 		}
 	} else {
@@ -253,8 +385,8 @@ bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, Table
 		auto start_time = high_resolution_clock::now();
 		for (idx_t i = 0; i < table_filters.size(); i++) {
 			auto tf_idx = state.adaptive_filter->permutation[i];
-			columns[tf_idx].Select(transaction, state.column_scans[tf_idx], result.data[tf_idx], sel,
-			                       approved_tuple_count, table_filters[tf_idx]);
+			columns[tf_idx]->Select(transaction, state.column_scans[tf_idx], result.data[tf_idx], sel,
+			                        approved_tuple_count, table_filters[tf_idx]);
 		}
 		for (auto &table_filter : table_filters) {
 			result.data[table_filter.first].Slice(sel, approved_tuple_count);
@@ -271,8 +403,8 @@ bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, Table
 						result_data[sel_idx] = base_row + current_row + sel.get_index(sel_idx);
 					}
 				} else {
-					columns[column].FilterScan(transaction, state.column_scans[i], result.data[i], sel,
-					                           approved_tuple_count);
+					columns[column]->FilterScan(transaction, state.column_scans[i], result.data[i], sel,
+					                            approved_tuple_count);
 				}
 			}
 		}
@@ -353,7 +485,7 @@ void DataTable::Fetch(Transaction &transaction, DataChunk &result, vector<column
 			// regular column: fetch data from the base column
 			for (idx_t i = 0; i < count; i++) {
 				auto row_id = rows[i];
-				columns[column].FetchRow(state.fetch_state, transaction, row_id, result.data[col_idx], i);
+				columns[column]->FetchRow(state.fetch_state, transaction, row_id, result.data[col_idx], i);
 			}
 		}
 	}
@@ -363,8 +495,8 @@ idx_t DataTable::FetchRows(Transaction &transaction, Vector &row_identifiers, id
 	assert(row_identifiers.type == ROW_TYPE);
 
 	// obtain a read lock on the version managers
-	auto l1 = persistent_manager.lock.GetSharedLock();
-	auto l2 = transient_manager.lock.GetSharedLock();
+	auto l1 = persistent_manager->lock.GetSharedLock();
+	auto l2 = transient_manager->lock.GetSharedLock();
 
 	// now iterate over the row ids and figure out which rows to use
 	idx_t count = 0;
@@ -373,12 +505,12 @@ idx_t DataTable::FetchRows(Transaction &transaction, Vector &row_identifiers, id
 	for (idx_t i = 0; i < fetch_count; i++) {
 		auto row_id = row_ids[i];
 		bool use_row;
-		if ((idx_t)row_id < persistent_manager.max_row) {
+		if ((idx_t)row_id < persistent_manager->max_row) {
 			// persistent row: use persistent manager
-			use_row = persistent_manager.Fetch(transaction, row_id);
+			use_row = persistent_manager->Fetch(transaction, row_id);
 		} else {
 			// transient row: use transient manager
-			use_row = transient_manager.Fetch(transaction, row_id);
+			use_row = transient_manager->Fetch(transaction, row_id);
 		}
 		if (use_row) {
 			// row is not deleted; use the row
@@ -435,7 +567,7 @@ void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, DataChunk &chu
 		}
 		case ConstraintType::UNIQUE: {
 			//! check whether or not the chunk can be inserted into the indexes
-			for (auto &index : indexes) {
+			for (auto &index : info->indexes) {
 				index->VerifyAppend(chunk);
 			}
 			break;
@@ -454,6 +586,9 @@ void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChu
 	if (chunk.column_count() != table.columns.size()) {
 		throw CatalogException("Mismatch in column count for append");
 	}
+	if (!is_root) {
+		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
+	}
 
 	chunk.Verify();
 
@@ -468,32 +603,36 @@ void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChu
 void DataTable::InitializeAppend(TableAppendState &state) {
 	// obtain the append lock for this table
 	state.append_lock = unique_lock<mutex>(append_lock);
+	if (!is_root) {
+		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
+	}
 	// obtain locks on all indexes for the table
-	state.index_locks = unique_ptr<IndexLock[]>(new IndexLock[indexes.size()]);
-	for (idx_t i = 0; i < indexes.size(); i++) {
-		indexes[i]->InitializeLock(state.index_locks[i]);
+	state.index_locks = unique_ptr<IndexLock[]>(new IndexLock[info->indexes.size()]);
+	for (idx_t i = 0; i < info->indexes.size(); i++) {
+		info->indexes[i]->InitializeLock(state.index_locks[i]);
 	}
 	// for each column, initialize the append state
 	state.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[types.size()]);
 	for (idx_t i = 0; i < types.size(); i++) {
-		columns[i].InitializeAppend(state.states[i]);
+		columns[i]->InitializeAppend(state.states[i]);
 	}
-	state.row_start = transient_manager.max_row;
+	state.row_start = transient_manager->max_row;
 	state.current_row = state.row_start;
 }
 
 void DataTable::Append(Transaction &transaction, transaction_t commit_id, DataChunk &chunk, TableAppendState &state) {
+	assert(is_root);
 	assert(chunk.column_count() == types.size());
 	chunk.Verify();
 
 	// set up the inserted info in the version manager
-	transient_manager.Append(transaction, state.current_row, chunk.size(), commit_id);
+	transient_manager->Append(transaction, state.current_row, chunk.size(), commit_id);
 
 	// append the physical data to each of the entries
 	for (idx_t i = 0; i < types.size(); i++) {
-		columns[i].Append(state.states[i], chunk.data[i], chunk.size());
+		columns[i]->Append(state.states[i], chunk.data[i], chunk.size());
 	}
-	cardinality += chunk.size();
+	info->cardinality += chunk.size();
 	state.current_row += chunk.size();
 }
 
@@ -502,22 +641,24 @@ void DataTable::RevertAppend(TableAppendState &state) {
 		// nothing to revert!
 		return;
 	}
+	assert(is_root);
 	// revert changes in the base columns
 	for (idx_t i = 0; i < types.size(); i++) {
-		columns[i].RevertAppend(state.row_start);
+		columns[i]->RevertAppend(state.row_start);
 	}
 	// adjust the cardinality
-	cardinality -= state.current_row - state.row_start;
-	transient_manager.max_row = state.row_start;
+	info->cardinality -= state.current_row - state.row_start;
+	transient_manager->max_row = state.row_start;
 	// revert changes in the transient manager
-	transient_manager.RevertAppend(state.row_start, state.current_row);
+	transient_manager->RevertAppend(state.row_start, state.current_row);
 }
 
 //===--------------------------------------------------------------------===//
 // Indexes
 //===--------------------------------------------------------------------===//
 bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
-	if (indexes.size() == 0) {
+	assert(is_root);
+	if (info->indexes.size() == 0) {
 		return true;
 	}
 	// first generate the vector of row identifiers
@@ -526,8 +667,8 @@ bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t
 
 	idx_t failed_index = INVALID_INDEX;
 	// now append the entries to the indices
-	for (idx_t i = 0; i < indexes.size(); i++) {
-		if (!indexes[i]->Append(state.index_locks[i], chunk, row_identifiers)) {
+	for (idx_t i = 0; i < info->indexes.size(); i++) {
+		if (!info->indexes[i]->Append(state.index_locks[i], chunk, row_identifiers)) {
 			failed_index = i;
 			break;
 		}
@@ -536,7 +677,7 @@ bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t
 		// constraint violation!
 		// remove any appended entries from previous indexes (if any)
 		for (idx_t i = 0; i < failed_index; i++) {
-			indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
+			info->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
 		}
 		return false;
 	}
@@ -544,7 +685,8 @@ bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t
 }
 
 void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
-	if (indexes.size() == 0) {
+	assert(is_root);
+	if (info->indexes.size() == 0) {
 		return;
 	}
 	// first generate the vector of row identifiers
@@ -556,12 +698,14 @@ void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row
 }
 
 void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
-	for (idx_t i = 0; i < indexes.size(); i++) {
-		indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
+	assert(is_root);
+	for (idx_t i = 0; i < info->indexes.size(); i++) {
+		info->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
 	}
 }
 
 void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
+	assert(is_root);
 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
 	// create a selection vector from the row_ids
 	SelectionVector sel(STANDARD_VECTOR_SIZE);
@@ -575,11 +719,11 @@ void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
 	auto states = unique_ptr<ColumnScanState[]>(new ColumnScanState[types.size()]);
 	for (idx_t i = 0; i < types.size(); i++) {
-		columns[i].Fetch(states[i], row_ids[0], result.data[i]);
+		columns[i]->Fetch(states[i], row_ids[0], result.data[i]);
 	}
 	result.Slice(sel, count);
-	for (idx_t i = 0; i < indexes.size(); i++) {
-		indexes[i]->Delete(result, row_identifiers);
+	for (idx_t i = 0; i < info->indexes.size(); i++) {
+		info->indexes[i]->Delete(result, row_identifiers);
 	}
 }
 
@@ -601,12 +745,12 @@ void DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector
 	if (first_id >= MAX_ROW_ID) {
 		// deletion is in transaction-local storage: push delete into local chunk collection
 		transaction.storage.Delete(this, row_identifiers, count);
-	} else if ((idx_t)first_id < persistent_manager.max_row) {
+	} else if ((idx_t)first_id < persistent_manager->max_row) {
 		// deletion is in persistent storage: delete in the persistent version manager
-		persistent_manager.Delete(transaction, row_identifiers, count);
+		persistent_manager->Delete(transaction, this, row_identifiers, count);
 	} else {
 		// deletion is in transient storage: delete in the persistent version manager
-		transient_manager.Delete(transaction, row_identifiers, count);
+		transient_manager->Delete(transaction, this, row_identifiers, count);
 	}
 }
 
@@ -682,8 +826,8 @@ void DataTable::VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chu
 	// update should not be called for indexed columns!
 	// instead update should have been rewritten to delete + update on higher layer
 #ifdef DEBUG
-	for (idx_t i = 0; i < indexes.size(); i++) {
-		assert(!indexes[i]->IndexIsUpdated(column_ids));
+	for (idx_t i = 0; i < info->indexes.size(); i++) {
+		assert(!info->indexes[i]->IndexIsUpdated(column_ids));
 	}
 #endif
 }
@@ -716,7 +860,7 @@ void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector
 		auto column = column_ids[i];
 		assert(column != COLUMN_IDENTIFIER_ROW_ID);
 
-		columns[column].Update(transaction, updates.data[i], row_ids, updates.size());
+		columns[column]->Update(transaction, updates.data[i], row_ids, updates.size());
 	}
 }
 
@@ -727,8 +871,8 @@ void DataTable::InitializeCreateIndexScan(CreateIndexScanState &state, vector<co
 	// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan
 	state.append_lock = unique_lock<mutex>(append_lock);
 	// get a read lock on the VersionManagers to prevent any further deletions
-	state.locks.push_back(persistent_manager.lock.GetSharedLock());
-	state.locks.push_back(transient_manager.lock.GetSharedLock());
+	state.locks.push_back(persistent_manager->lock.GetSharedLock());
+	state.locks.push_back(transient_manager->lock.GetSharedLock());
 
 	InitializeScan(state, column_ids);
 }
@@ -763,7 +907,7 @@ bool DataTable::ScanCreateIndex(CreateIndexScanState &state, DataChunk &result,
 			result.data[i].Sequence(base_row + current_row, 1);
 		} else {
 			// scan actual base column
-			columns[column].IndexScan(state.column_scans[i], result.data[i]);
+			columns[column]->IndexScan(state.column_scans[i], result.data[i]);
 		}
 	}
 	result.SetCardinality(count);
@@ -790,6 +934,10 @@ void DataTable::AddIndex(unique_ptr<Index> index, vector<unique_ptr<Expression>>
 	CreateIndexScanState state;
 	InitializeCreateIndexScan(state, column_ids);
 
+	if (!is_root) {
+		throw TransactionException("Transaction conflict: cannot add an index to a table that has been altered!");
+	}
+
 	// now start incrementally building the index
 	IndexLock lock;
 	index->InitializeLock(lock);
@@ -811,9 +959,5 @@ void DataTable::AddIndex(unique_ptr<Index> index, vector<unique_ptr<Expression>>
 			throw ConstraintException("Cant create unique index, table contains duplicate data on indexed column(s)");
 		}
 	}
-	indexes.push_back(move(index));
-}
-
-bool DataTable::IsTemporary() {
-	return schema.compare(TEMP_SCHEMA) == 0;
+	info->indexes.push_back(move(index));
 }
diff --git a/src/storage/local_storage.cpp b/src/storage/local_storage.cpp
index 2f931eb2576a..9f9b4a1dc4bc 100644
--- a/src/storage/local_storage.cpp
+++ b/src/storage/local_storage.cpp
@@ -9,7 +9,7 @@ using namespace duckdb;
 using namespace std;
 
 LocalTableStorage::LocalTableStorage(DataTable &table) : max_row(0) {
-	for (auto &index : table.indexes) {
+	for (auto &index : table.info->indexes) {
 		assert(index->type == IndexType::ART);
 		auto &art = (ART &)*index;
 		if (art.is_unique) {
@@ -112,11 +112,11 @@ void LocalStorage::Scan(LocalScanState &state, const vector<column_t> &column_id
 			}
 		}
 	}
-	if (count == 0){
-	    // all entries in this chunk were filtered:: Continue on next chunk
-			state.chunk_index++;
-			Scan(state, column_ids, result, table_filters);
-			return;
+	if (count == 0) {
+		// all entries in this chunk were filtered:: Continue on next chunk
+		state.chunk_index++;
+		Scan(state, column_ids, result, table_filters);
+		return;
 	}
 	if (count == chunk_count) {
 		result.SetCardinality(count);
@@ -299,8 +299,8 @@ void LocalStorage::Commit(LocalStorage::CommitState &commit_state, Transaction &
 		commit_state.append_states[table] = move(append_state_ptr);
 		table->InitializeAppend(append_state);
 
-		if (log && !table->IsTemporary()) {
-			log->WriteSetTable(table->schema, table->table);
+		if (log && !table->info->IsTemporary()) {
+			log->WriteSetTable(table->info->schema, table->info->table);
 		}
 
 		// scan all chunks in this storage
@@ -313,7 +313,7 @@ void LocalStorage::Commit(LocalStorage::CommitState &commit_state, Transaction &
 			// append to base table
 			table->Append(transaction, commit_id, chunk, append_state);
 			// if there is a WAL, write the chunk to there as well
-			if (log && !table->IsTemporary()) {
+			if (log && !table->info->IsTemporary()) {
 				log->WriteInsert(chunk);
 			}
 			return true;
@@ -331,7 +331,7 @@ void LocalStorage::RevertCommit(LocalStorage::CommitState &commit_state) {
 		auto table = entry.first;
 		auto storage = table_storage[table].get();
 		auto &append_state = *entry.second;
-		if (table->indexes.size() > 0 && !(table->schema == "temp")) {
+		if (table->info->indexes.size() > 0 && !table->info->IsTemporary()) {
 			row_t current_row = append_state.row_start;
 			// remove the data from the indexes, if there are any indexes
 			ScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {
@@ -350,3 +350,48 @@ void LocalStorage::RevertCommit(LocalStorage::CommitState &commit_state) {
 		table->RevertAppend(*entry.second);
 	}
 }
+
+void LocalStorage::AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column,
+                             Expression *default_value) {
+	// check if there are any pending appends for the old version of the table
+	auto entry = table_storage.find(old_dt);
+	if (entry == table_storage.end()) {
+		return;
+	}
+	// take over the storage from the old entry
+	auto new_storage = move(entry->second);
+
+	// now add the new column filled with the default value to all chunks
+	auto new_column_type = GetInternalType(new_column.type);
+	ExpressionExecutor executor;
+	DataChunk dummy_chunk;
+	if (default_value) {
+		executor.AddExpression(*default_value);
+	}
+
+	new_storage->collection.types.push_back(new_column_type);
+	for (idx_t chunk_idx = 0; chunk_idx < new_storage->collection.chunks.size(); chunk_idx++) {
+		auto &chunk = new_storage->collection.chunks[chunk_idx];
+		Vector result(new_column_type);
+		if (default_value) {
+			dummy_chunk.SetCardinality(chunk->size());
+			executor.ExecuteExpression(dummy_chunk, result);
+		} else {
+			FlatVector::Nullmask(result).set();
+		}
+		chunk->data.push_back(move(result));
+	}
+
+	table_storage.erase(entry);
+	table_storage[new_dt] = move(new_storage);
+}
+
+void LocalStorage::ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, SQLType target_type,
+                              vector<column_t> bound_columns, Expression &cast_expr) {
+	// check if there are any pending appends for the old version of the table
+	auto entry = table_storage.find(old_dt);
+	if (entry == table_storage.end()) {
+		return;
+	}
+	throw NotImplementedException("FIXME: ALTER TYPE with transaction local data not currently supported");
+}
diff --git a/src/storage/table/version_manager.cpp b/src/storage/table/version_manager.cpp
index 2c3cfc2a6e75..a8f8d070e08d 100644
--- a/src/storage/table/version_manager.cpp
+++ b/src/storage/table/version_manager.cpp
@@ -36,13 +36,14 @@ bool VersionManager::Fetch(Transaction &transaction, idx_t row) {
 
 class VersionDeleteState {
 public:
-	VersionDeleteState(VersionManager &manager, Transaction &transaction, idx_t base_row)
-	    : manager(manager), transaction(transaction), current_info(nullptr), current_chunk((idx_t)-1), count(0),
-	      base_row(base_row) {
+	VersionDeleteState(VersionManager &manager, Transaction &transaction, DataTable *table, idx_t base_row)
+	    : manager(manager), transaction(transaction), table(table), current_info(nullptr), current_chunk((idx_t)-1),
+	      count(0), base_row(base_row) {
 	}
 
 	VersionManager &manager;
 	Transaction &transaction;
+	DataTable *table;
 	ChunkInfo *current_info;
 	idx_t current_chunk;
 	row_t rows[STANDARD_VECTOR_SIZE];
@@ -55,8 +56,8 @@ class VersionDeleteState {
 	void Flush();
 };
 
-void VersionManager::Delete(Transaction &transaction, Vector &row_ids, idx_t count) {
-	VersionDeleteState del_state(*this, transaction, base_row);
+void VersionManager::Delete(Transaction &transaction, DataTable *table, Vector &row_ids, idx_t count) {
+	VersionDeleteState del_state(*this, transaction, table, base_row);
 
 	VectorData rdata;
 	row_ids.Orrify(count, rdata);
@@ -105,7 +106,7 @@ void VersionDeleteState::Flush() {
 	// delete in the current info
 	current_info->Delete(transaction, rows, count);
 	// now push the delete into the undo buffer
-	transaction.PushDelete(current_info, rows, count, base_row + chunk_row);
+	transaction.PushDelete(table, current_info, rows, count, base_row + chunk_row);
 	count = 0;
 }
 
diff --git a/src/transaction/CMakeLists.txt b/src/transaction/CMakeLists.txt
index e9039f8b852c..7e72327bd68e 100644
--- a/src/transaction/CMakeLists.txt
+++ b/src/transaction/CMakeLists.txt
@@ -1,7 +1,6 @@
 add_library_unity(duckdb_transaction
                   OBJECT
                   undo_buffer.cpp
-                  delete_info.cpp
                   transaction_context.cpp
                   transaction.cpp
                   transaction_manager.cpp
diff --git a/src/transaction/cleanup_state.cpp b/src/transaction/cleanup_state.cpp
index a991ab28ce53..42ae200023e7 100644
--- a/src/transaction/cleanup_state.cpp
+++ b/src/transaction/cleanup_state.cpp
@@ -56,8 +56,8 @@ void CleanupState::CleanupUpdate(UpdateInfo *info) {
 }
 
 void CleanupState::CleanupDelete(DeleteInfo *info) {
-	auto version_table = &info->GetTable();
-	if (version_table->indexes.size() == 0) {
+	auto version_table = info->table;
+	if (version_table->info->indexes.size() == 0) {
 		// this table has no indexes: no cleanup to be done
 		return;
 	}
diff --git a/src/transaction/commit_state.cpp b/src/transaction/commit_state.cpp
index b3b5317938c5..3fe5ac85f640 100644
--- a/src/transaction/commit_state.cpp
+++ b/src/transaction/commit_state.cpp
@@ -12,14 +12,14 @@ using namespace duckdb;
 using namespace std;
 
 CommitState::CommitState(transaction_t commit_id, WriteAheadLog *log)
-    : log(log), commit_id(commit_id), current_table(nullptr) {
+    : log(log), commit_id(commit_id), current_table_info(nullptr) {
 }
 
-void CommitState::SwitchTable(DataTable *table, UndoFlags new_op) {
-	if (current_table != table) {
+void CommitState::SwitchTable(DataTableInfo *table_info, UndoFlags new_op) {
+	if (current_table_info != table_info) {
 		// write the current table to the log
-		log->WriteSetTable(table->schema, table->table);
-		current_table = table;
+		log->WriteSetTable(table_info->schema, table_info->table);
+		current_table_info = table_info;
 	}
 }
 
@@ -91,7 +91,7 @@ void CommitState::WriteCatalogEntry(CatalogEntry *entry, data_ptr_t dataptr) {
 void CommitState::WriteDelete(DeleteInfo *info) {
 	assert(log);
 	// switch to the current table, if necessary
-	SwitchTable(&info->GetTable(), UndoFlags::DELETE_TUPLE);
+	SwitchTable(info->table->info.get(), UndoFlags::DELETE_TUPLE);
 
 	if (!delete_chunk) {
 		delete_chunk = make_unique<DataChunk>();
@@ -109,7 +109,7 @@ void CommitState::WriteDelete(DeleteInfo *info) {
 void CommitState::WriteUpdate(UpdateInfo *info) {
 	assert(log);
 	// switch to the current table, if necessary
-	SwitchTable(info->column_data->table, UndoFlags::UPDATE_TUPLE);
+	SwitchTable(&info->column_data->table_info, UndoFlags::UPDATE_TUPLE);
 
 	update_chunk = make_unique<DataChunk>();
 	vector<TypeId> update_types = {info->column_data->type, ROW_TYPE};
@@ -149,8 +149,8 @@ template <bool HAS_LOG> void CommitState::CommitEntry(UndoFlags type, data_ptr_t
 	case UndoFlags::DELETE_TUPLE: {
 		// deletion:
 		auto info = (DeleteInfo *)data;
-		info->GetTable().cardinality -= info->count;
-		if (HAS_LOG && !info->GetTable().IsTemporary()) {
+		info->table->info->cardinality -= info->count;
+		if (HAS_LOG && !info->table->info->IsTemporary()) {
 			WriteDelete(info);
 		}
 		// mark the tuples as committed
@@ -160,7 +160,7 @@ template <bool HAS_LOG> void CommitState::CommitEntry(UndoFlags type, data_ptr_t
 	case UndoFlags::UPDATE_TUPLE: {
 		// update:
 		auto info = (UpdateInfo *)data;
-		if (HAS_LOG && !info->column_data->table->IsTemporary()) {
+		if (HAS_LOG && !info->column_data->table_info.IsTemporary()) {
 			WriteUpdate(info);
 		}
 		info->version_number = commit_id;
@@ -184,7 +184,7 @@ void CommitState::RevertCommit(UndoFlags type, data_ptr_t data) {
 	case UndoFlags::DELETE_TUPLE: {
 		// deletion:
 		auto info = (DeleteInfo *)data;
-		info->GetTable().cardinality += info->count;
+		info->table->info->cardinality += info->count;
 		// revert the commit by writing the (uncommitted) transaction_id back into the version info
 		info->vinfo->CommitDelete(transaction_id, info->rows, info->count);
 		break;
diff --git a/src/transaction/delete_info.cpp b/src/transaction/delete_info.cpp
deleted file mode 100644
index 3ba3a013221f..000000000000
--- a/src/transaction/delete_info.cpp
+++ /dev/null
@@ -1,10 +0,0 @@
-#include "duckdb/transaction/delete_info.hpp"
-#include "duckdb/storage/table/chunk_info.hpp"
-#include "duckdb/storage/table/version_manager.hpp"
-
-using namespace duckdb;
-using namespace std;
-
-DataTable &DeleteInfo::GetTable() {
-	return vinfo->manager.table;
-}
diff --git a/src/transaction/transaction.cpp b/src/transaction/transaction.cpp
index 072c431ce711..1e3451494cf0 100644
--- a/src/transaction/transaction.cpp
+++ b/src/transaction/transaction.cpp
@@ -38,10 +38,11 @@ void Transaction::PushCatalogEntry(CatalogEntry *entry, data_ptr_t extra_data, i
 	}
 }
 
-void Transaction::PushDelete(ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row) {
+void Transaction::PushDelete(DataTable *table, ChunkInfo *vinfo, row_t rows[], idx_t count, idx_t base_row) {
 	auto delete_info =
 	    (DeleteInfo *)undo_buffer.CreateEntry(UndoFlags::DELETE_TUPLE, sizeof(DeleteInfo) + sizeof(row_t) * count);
 	delete_info->vinfo = vinfo;
+	delete_info->table = table;
 	delete_info->count = count;
 	delete_info->base_row = base_row;
 	memcpy(delete_info->rows, rows, sizeof(row_t) * count);
diff --git a/tools/pythonpkg/duckdb_python.cpp b/tools/pythonpkg/duckdb_python.cpp
index c89cb0f471b4..859d0143339f 100644
--- a/tools/pythonpkg/duckdb_python.cpp
+++ b/tools/pythonpkg/duckdb_python.cpp
@@ -33,6 +33,12 @@ struct DateConvert {
 	}
 };
 
+struct TimeConvert {
+	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(time_t val) {
+		return py::str(duckdb::Time::ToString(val).c_str());
+	}
+};
+
 struct StringConvert {
 	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(string_t val) {
 		return py::str(val.GetData());
@@ -299,7 +305,16 @@ struct DuckDBPyResult {
 
 				break;
 			}
-
+			case SQLTypeId::TIME: {
+				if (result->types[col_idx] != TypeId::INT32) {
+					throw runtime_error("expected int32 for time");
+				}
+				int32_t hour, min, sec, msec;
+				auto time = val.GetValue<int32_t>();
+				duckdb::Time::Convert(time, hour, min, sec, msec);
+				res[col_idx] = PyTime_FromTime(hour, min, sec, msec * 1000);
+				break;
+			}
 			case SQLTypeId::DATE: {
 				if (result->types[col_idx] != TypeId::INT32) {
 					throw runtime_error("expected int32 for date");
@@ -379,7 +394,10 @@ struct DuckDBPyResult {
 				col_res = duckdb_py_convert::fetch_column<date_t, int64_t, duckdb_py_convert::DateConvert>(
 				    "datetime64[s]", mres->collection, col_idx);
 				break;
-
+			case SQLTypeId::TIME:
+				col_res = duckdb_py_convert::fetch_column<time_t, py::str, duckdb_py_convert::TimeConvert>(
+				    "object", mres->collection, col_idx);
+				break;
 			case SQLTypeId::VARCHAR:
 				col_res = duckdb_py_convert::fetch_column<string_t, py::str, duckdb_py_convert::StringConvert>(
 				    "object", mres->collection, col_idx);
