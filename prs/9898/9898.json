{
  "repo": "duckdb/duckdb",
  "pull_number": 9898,
  "instance_id": "duckdb__duckdb-9898",
  "issue_numbers": [
    "9341"
  ],
  "base_commit": "fc961bb635f262c99e53eca13f891189695c588b",
  "patch": "diff --git a/src/include/duckdb/main/client_context.hpp b/src/include/duckdb/main/client_context.hpp\nindex c97043f40757..e33f3eb62bdc 100644\n--- a/src/include/duckdb/main/client_context.hpp\n+++ b/src/include/duckdb/main/client_context.hpp\n@@ -65,7 +65,7 @@ class ClientContextState {\n class ClientContext : public std::enable_shared_from_this<ClientContext> {\n \tfriend class PendingQueryResult;\n \tfriend class StreamQueryResult;\n-\tfriend class DuckTransactionManager;\n+\tfriend class ConnectionManager;\n \n public:\n \tDUCKDB_API explicit ClientContext(shared_ptr<DatabaseInstance> db);\ndiff --git a/src/include/duckdb/main/connection_manager.hpp b/src/include/duckdb/main/connection_manager.hpp\nindex 11495742de50..b59ce503db26 100644\n--- a/src/include/duckdb/main/connection_manager.hpp\n+++ b/src/include/duckdb/main/connection_manager.hpp\n@@ -17,37 +17,25 @@ namespace duckdb {\n class ClientContext;\n class DatabaseInstance;\n \n-class ConnectionManager {\n-public:\n-\tConnectionManager() {\n+struct ClientLockWrapper {\n+\tClientLockWrapper(mutex &client_lock, shared_ptr<ClientContext> connection)\n+\t    : connection(std::move(connection)), connection_lock(make_uniq<lock_guard<mutex>>(client_lock)) {\n \t}\n \n-\tvoid AddConnection(ClientContext &context) {\n-\t\tlock_guard<mutex> lock(connections_lock);\n-\t\tconnections.insert(make_pair(&context, weak_ptr<ClientContext>(context.shared_from_this())));\n-\t}\n+\tshared_ptr<ClientContext> connection;\n+\tunique_ptr<lock_guard<mutex>> connection_lock;\n+};\n \n-\tvoid RemoveConnection(ClientContext &context) {\n-\t\tlock_guard<mutex> lock(connections_lock);\n-\t\tconnections.erase(&context);\n-\t}\n+class ConnectionManager {\n+public:\n+\tConnectionManager();\n \n-\tvector<shared_ptr<ClientContext>> GetConnectionList() {\n-\t\tvector<shared_ptr<ClientContext>> result;\n-\t\tfor (auto &it : connections) {\n-\t\t\tauto connection = it.second.lock();\n-\t\t\tif (!connection) {\n-\t\t\t\tconnections.erase(it.first);\n-\t\t\t\tcontinue;\n-\t\t\t} else {\n-\t\t\t\tresult.push_back(std::move(connection));\n-\t\t\t}\n-\t\t}\n-\n-\t\treturn result;\n-\t}\n+\tvoid AddConnection(ClientContext &context);\n+\tvoid RemoveConnection(ClientContext &context);\n \n-\tClientContext *GetConnection(DatabaseInstance *db);\n+\tvector<shared_ptr<ClientContext>> GetConnectionList();\n+\n+\tvoid LockClients(vector<ClientLockWrapper> &client_locks, ClientContext &context);\n \n \tstatic ConnectionManager &Get(DatabaseInstance &db);\n \tstatic ConnectionManager &Get(ClientContext &context);\n@@ -55,6 +43,9 @@ class ConnectionManager {\n public:\n \tmutex connections_lock;\n \tunordered_map<ClientContext *, weak_ptr<ClientContext>> connections;\n+\n+\tmutex lock_clients_lock;\n+\tbool is_locking;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/transaction/duck_transaction_manager.hpp b/src/include/duckdb/transaction/duck_transaction_manager.hpp\nindex 8cd995772605..269167f524fb 100644\n--- a/src/include/duckdb/transaction/duck_transaction_manager.hpp\n+++ b/src/include/duckdb/transaction/duck_transaction_manager.hpp\n@@ -49,7 +49,6 @@ class DuckTransactionManager : public TransactionManager {\n \tbool CanCheckpoint(optional_ptr<DuckTransaction> current = nullptr);\n \t//! Remove the given transaction from the list of active transactions\n \tvoid RemoveTransaction(DuckTransaction &transaction) noexcept;\n-\tvoid LockClients(vector<ClientLockWrapper> &client_locks, ClientContext &context);\n \n private:\n \t//! The current start timestamp used by transactions\ndiff --git a/src/main/CMakeLists.txt b/src/main/CMakeLists.txt\nindex 0df3eaf3ca20..ebc417cb8c77 100644\n--- a/src/main/CMakeLists.txt\n+++ b/src/main/CMakeLists.txt\n@@ -19,6 +19,7 @@ add_library_unity(\n   client_context.cpp\n   client_data.cpp\n   client_verify.cpp\n+  connection_manager.cpp\n   chunk_scan_state.cpp\n   config.cpp\n   connection.cpp\ndiff --git a/src/main/connection_manager.cpp b/src/main/connection_manager.cpp\nnew file mode 100644\nindex 000000000000..c8ca304ab1a3\n--- /dev/null\n+++ b/src/main/connection_manager.cpp\n@@ -0,0 +1,53 @@\n+#include \"duckdb/main/connection_manager.hpp\"\n+\n+namespace duckdb {\n+\n+ConnectionManager::ConnectionManager() : is_locking(false) {\n+}\n+\n+void ConnectionManager::AddConnection(ClientContext &context) {\n+\tlock_guard<mutex> lock(connections_lock);\n+\tconnections.insert(make_pair(&context, weak_ptr<ClientContext>(context.shared_from_this())));\n+}\n+\n+void ConnectionManager::RemoveConnection(ClientContext &context) {\n+\tlock_guard<mutex> lock(connections_lock);\n+\tconnections.erase(&context);\n+}\n+\n+vector<shared_ptr<ClientContext>> ConnectionManager::GetConnectionList() {\n+\tvector<shared_ptr<ClientContext>> result;\n+\tfor (auto &it : connections) {\n+\t\tauto connection = it.second.lock();\n+\t\tif (!connection) {\n+\t\t\tconnections.erase(it.first);\n+\t\t\tcontinue;\n+\t\t} else {\n+\t\t\tresult.push_back(std::move(connection));\n+\t\t}\n+\t}\n+\n+\treturn result;\n+}\n+\n+void ConnectionManager::LockClients(vector<ClientLockWrapper> &client_locks, ClientContext &context) {\n+\t{\n+\t\tlock_guard<mutex> l(lock_clients_lock);\n+\t\tif (is_locking) {\n+\t\t\tthrow TransactionException(\"Failed to lock clients - another thread is running FORCE CHECKPOINT\");\n+\t\t}\n+\t\tis_locking = true;\n+\t}\n+\tclient_locks.emplace_back(connections_lock, nullptr);\n+\tauto connection_list = GetConnectionList();\n+\tfor (auto &con : connection_list) {\n+\t\tif (con.get() == &context) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tauto &context_lock = con->context_lock;\n+\t\tclient_locks.emplace_back(context_lock, std::move(con));\n+\t}\n+\tis_locking = false;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/main/database.cpp b/src/main/database.cpp\nindex e8066fdc6fd0..de2e1854f382 100644\n--- a/src/main/database.cpp\n+++ b/src/main/database.cpp\n@@ -119,15 +119,6 @@ ConnectionManager &ConnectionManager::Get(DatabaseInstance &db) {\n \treturn db.GetConnectionManager();\n }\n \n-ClientContext *ConnectionManager::GetConnection(DatabaseInstance *db) {\n-\tfor (auto &conn : connections) {\n-\t\tif (conn.first->db.get() == db) {\n-\t\t\treturn conn.first;\n-\t\t}\n-\t}\n-\treturn nullptr;\n-}\n-\n ConnectionManager &ConnectionManager::Get(ClientContext &context) {\n \treturn ConnectionManager::Get(DatabaseInstance::GetDatabase(context));\n }\ndiff --git a/src/transaction/duck_transaction_manager.cpp b/src/transaction/duck_transaction_manager.cpp\nindex 26c11d8f439e..b16be4c98916 100644\n--- a/src/transaction/duck_transaction_manager.cpp\n+++ b/src/transaction/duck_transaction_manager.cpp\n@@ -90,28 +90,6 @@ Transaction &DuckTransactionManager::StartTransaction(ClientContext &context) {\n \treturn transaction_ref;\n }\n \n-struct ClientLockWrapper {\n-\tClientLockWrapper(mutex &client_lock, shared_ptr<ClientContext> connection)\n-\t    : connection(std::move(connection)), connection_lock(make_uniq<lock_guard<mutex>>(client_lock)) {\n-\t}\n-\n-\tshared_ptr<ClientContext> connection;\n-\tunique_ptr<lock_guard<mutex>> connection_lock;\n-};\n-\n-void DuckTransactionManager::LockClients(vector<ClientLockWrapper> &client_locks, ClientContext &context) {\n-\tauto &connection_manager = ConnectionManager::Get(context);\n-\tclient_locks.emplace_back(connection_manager.connections_lock, nullptr);\n-\tauto connection_list = connection_manager.GetConnectionList();\n-\tfor (auto &con : connection_list) {\n-\t\tif (con.get() == &context) {\n-\t\t\tcontinue;\n-\t\t}\n-\t\tauto &context_lock = con->context_lock;\n-\t\tclient_locks.emplace_back(context_lock, std::move(con));\n-\t}\n-}\n-\n void DuckTransactionManager::Checkpoint(ClientContext &context, bool force) {\n \tauto &storage_manager = db.GetStorageManager();\n \tif (storage_manager.InMemory()) {\n@@ -119,22 +97,13 @@ void DuckTransactionManager::Checkpoint(ClientContext &context, bool force) {\n \t}\n \n \t// first check if no other thread is checkpointing right now\n+\tauto current = &DuckTransaction::Get(context, db);\n \tauto lock = unique_lock<mutex>(transaction_lock);\n \tif (thread_is_checkpointing) {\n \t\tthrow TransactionException(\"Cannot CHECKPOINT: another thread is checkpointing right now\");\n \t}\n \tCheckpointLock checkpoint_lock(*this);\n \tcheckpoint_lock.Lock();\n-\tlock.unlock();\n-\n-\t// lock all the clients AND the connection manager now\n-\t// this ensures no new queries can be started, and no new connections to the database can be made\n-\t// to avoid deadlock we release the transaction lock while locking the clients\n-\tvector<ClientLockWrapper> client_locks;\n-\tLockClients(client_locks, context);\n-\n-\tauto current = &DuckTransaction::Get(context, db);\n-\tlock.lock();\n \tif (current->ChangesMade()) {\n \t\tthrow TransactionException(\"Cannot CHECKPOINT: the current transaction has transaction local changes\");\n \t}\n@@ -144,6 +113,16 @@ void DuckTransactionManager::Checkpoint(ClientContext &context, bool force) {\n \t\t\t                           \"the other transactions and force a checkpoint\");\n \t\t}\n \t} else {\n+\t\tlock.unlock();\n+\n+\t\t// lock all the clients AND the connection manager now\n+\t\t// this ensures no new queries can be started, and no new connections to the database can be made\n+\t\t// to avoid deadlock we release the transaction lock while locking the clients\n+\t\tauto &connection_manager = ConnectionManager::Get(context);\n+\t\tvector<ClientLockWrapper> client_locks;\n+\t\tconnection_manager.LockClients(client_locks, context);\n+\n+\t\tlock.lock();\n \t\tif (!CanCheckpoint(current)) {\n \t\t\tfor (size_t i = 0; i < active_transactions.size(); i++) {\n \t\t\t\tauto &transaction = active_transactions[i];\n@@ -197,18 +176,6 @@ string DuckTransactionManager::CommitTransaction(ClientContext &context, Transac\n \tif (checkpoint) {\n \t\tif (transaction.AutomaticCheckpoint(db)) {\n \t\t\tcheckpoint_lock.Lock();\n-\t\t\t// we might be able to checkpoint: lock all clients\n-\t\t\t// to avoid deadlock we release the transaction lock while locking the clients\n-\t\t\tlock.reset();\n-\n-\t\t\tLockClients(client_locks, context);\n-\n-\t\t\tlock = make_uniq<lock_guard<mutex>>(transaction_lock);\n-\t\t\tcheckpoint = CanCheckpoint(&transaction);\n-\t\t\tif (!checkpoint) {\n-\t\t\t\tcheckpoint_lock.Unlock();\n-\t\t\t\tclient_locks.clear();\n-\t\t\t}\n \t\t} else {\n \t\t\tcheckpoint = false;\n \t\t}\n",
  "test_patch": "diff --git a/test/sql/attach/attach_checkpoint_deadlock.test_slow b/test/sql/attach/attach_checkpoint_deadlock.test_slow\nnew file mode 100644\nindex 000000000000..9858b3414e97\n--- /dev/null\n+++ b/test/sql/attach/attach_checkpoint_deadlock.test_slow\n@@ -0,0 +1,40 @@\n+# name: test/sql/attach/attach_checkpoint_deadlock.test_slow\n+# description: Deadlock when checkpointing multiple databases\n+# group: [attach]\n+\n+require noforcestorage\n+\n+require skip_reload\n+\n+concurrentforeach dbname foo bar i1 i2 i3 i4 i5 i6 i7 i8 i9\n+\n+statement ok\n+attach '__TEST_DIR__/${dbname}.duckdb' as ${dbname}\n+\n+statement ok\n+create table ${dbname}.${dbname}(foo bigint)\n+\n+statement ok\n+insert into ${dbname}.${dbname} select sum(i) from range(1000000) t(i)\n+\n+statement maybe\n+checkpoint ${dbname}\n+----\n+there are other transactions\n+\n+statement ok\n+select\n+    coalesce(t.table_catalog, current_database()) as \"database\",\n+    t.table_schema as \"schema\",\n+    t.table_name as \"name\",\n+    t.table_type as \"type\",\n+    array_agg(c.column_name order by c.ordinal_position) as \"column_names\",\n+    array_agg(c.data_type order by c.ordinal_position) as \"column_types\",\n+    array_agg(c.is_nullable = 'YES' order by c.ordinal_position) as \"column_nullable\"\n+from information_schema.tables t\n+join information_schema.columns c on t.table_schema = c.table_schema and t.table_name = c.table_name\n+where t.table_schema = 'main'\n+group by 1, 2, 3, 4\n+order by 1, 2, 3, 4\n+\n+endloop\ndiff --git a/test/sql/attach/attach_concurrent_checkpoint.test_slow b/test/sql/attach/attach_concurrent_checkpoint.test_slow\nnew file mode 100644\nindex 000000000000..761f30f05e43\n--- /dev/null\n+++ b/test/sql/attach/attach_concurrent_checkpoint.test_slow\n@@ -0,0 +1,30 @@\n+# name: test/sql/attach/attach_concurrent_checkpoint.test_slow\n+# description: Concurrently checkpoint the same database\n+# group: [attach]\n+\n+require noforcestorage\n+\n+require skip_reload\n+\n+statement ok\n+ATTACH '__TEST_DIR__/concurrent_checkpoint.db' AS db\n+\n+statement ok\n+CREATE TABLE db.integers(i INTEGER);\n+\n+concurrentloop i 0 10\n+\n+statement ok\n+INSERT INTO db.integers FROM range(1000000);\n+\n+statement maybe\n+CHECKPOINT db\n+----\n+there are other transactions\n+\n+endloop\n+\n+query II\n+SELECT COUNT(*), SUM(i) FROM db.integers\n+----\n+10000000\t4999995000000\ndiff --git a/test/sql/attach/attach_force_checkpoint_deadlock.test_slow b/test/sql/attach/attach_force_checkpoint_deadlock.test_slow\nnew file mode 100644\nindex 000000000000..ada569c575d3\n--- /dev/null\n+++ b/test/sql/attach/attach_force_checkpoint_deadlock.test_slow\n@@ -0,0 +1,40 @@\n+# name: test/sql/attach/attach_force_checkpoint_deadlock.test_slow\n+# description: Deadlock when force checkpointing multiple databases\n+# group: [attach]\n+\n+require noforcestorage\n+\n+require skip_reload\n+\n+concurrentforeach dbname foo bar i1 i2 i3 i4 i5 i6 i7 i8 i9\n+\n+statement ok\n+attach '__TEST_DIR__/${dbname}.duckdb' as ${dbname}\n+\n+statement ok\n+create table ${dbname}.${dbname}(foo bigint)\n+\n+statement ok\n+insert into ${dbname}.${dbname} select sum(i) from range(1000000) t(i)\n+\n+statement maybe\n+force checkpoint ${dbname}\n+----\n+another thread is running FORCE CHECKPOINT\n+\n+statement ok\n+select\n+    coalesce(t.table_catalog, current_database()) as \"database\",\n+    t.table_schema as \"schema\",\n+    t.table_name as \"name\",\n+    t.table_type as \"type\",\n+    array_agg(c.column_name order by c.ordinal_position) as \"column_names\",\n+    array_agg(c.data_type order by c.ordinal_position) as \"column_types\",\n+    array_agg(c.is_nullable = 'YES' order by c.ordinal_position) as \"column_nullable\"\n+from information_schema.tables t\n+join information_schema.columns c on t.table_schema = c.table_schema and t.table_name = c.table_name\n+where t.table_schema = 'main'\n+group by 1, 2, 3, 4\n+order by 1, 2, 3, 4\n+\n+endloop\n",
  "problem_statement": "Parallel checkpoint of two attached databases deadlocks.\n### What happens?\n\nDoing checkpoints of different attached databases deadlocks. From the documentation I got the sense that `checkpoint` would be blocking, but in the sense that when checkpointing, all other clients are locked and wait until a given checkpoint finishes.\n\n### To Reproduce\n\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom pathlib import Path\r\nfrom random import random\r\n\r\nimport duckdb\r\n\r\n\r\ndef run_thread(cursor, name):\r\n    cursor.execute(f\"insert into {name}.{name} select sum(i) from range({int(random()*1000000)}) r(i)\")\r\n    cursor.execute(f\"checkpoint {name}\")\r\n\r\n\r\nPath(\"foo.duckdb\").unlink(missing_ok=True)\r\nPath(\"bar.duckdb\").unlink(missing_ok=True)\r\nconn = duckdb.connect()\r\ncursor_foo = conn.cursor()\r\ncursor_bar = conn.cursor()\r\ncursor_foo.execute(\"attach 'foo.duckdb' as foo\")\r\ncursor_foo.execute(\"create table foo.foo(foo bigint)\")\r\ncursor_bar.execute(\"attach 'bar.duckdb' as bar\")\r\ncursor_bar.execute(\"create table bar.bar(bar bigint)\")\r\n\r\nfor i in range(1000):\r\n    print(i)\r\n    with ThreadPoolExecutor(max_workers=2) as executor:\r\n        footure = executor.submit(run_thread, cursor_foo, \"foo\")\r\n        barture = executor.submit(run_thread, cursor_bar, \"bar\")\r\n        footure.result()\r\n        barture.result()\r\n```\r\n\r\nand wait for a bit.\n\n### OS:\n\nUbuntu x64 in WSL on Windows 11\n\n### DuckDB Version:\n\n0.9.2.dev14+g0ef2a6faa2\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nM\u00edma Hlav\u00e1\u010dek\n\n### Affiliation:\n\nBlindspot.ai\n\n### Have you tried this on the latest `main` branch?\n\nI have tested with a main build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "",
  "created_at": "2023-12-05T13:46:15Z"
}