{
  "repo": "duckdb/duckdb",
  "pull_number": 6104,
  "instance_id": "duckdb__duckdb-6104",
  "issue_numbers": [
    "6067",
    "6067"
  ],
  "base_commit": "dba3b77df7a240a386f3ccd2da8249c13834576b",
  "patch": "diff --git a/extension/parquet/parquet_writer.cpp b/extension/parquet/parquet_writer.cpp\nindex 18de9a2bafc8..548f63d176b7 100644\n--- a/extension/parquet/parquet_writer.cpp\n+++ b/extension/parquet/parquet_writer.cpp\n@@ -211,6 +211,19 @@ void ParquetWriter::SetSchemaProperties(const LogicalType &duckdb_type,\n \t}\n }\n \n+void VerifyUniqueNames(const vector<string> &names) {\n+#ifdef DEBUG\n+\tunordered_set<string> name_set;\n+\tname_set.reserve(names.size());\n+\tfor (auto &column : names) {\n+\t\tauto res = name_set.insert(column);\n+\t\tD_ASSERT(res.second == true);\n+\t}\n+\t// If there would be duplicates, these sizes would differ\n+\tD_ASSERT(name_set.size() == names.size());\n+#endif\n+}\n+\n ParquetWriter::ParquetWriter(FileSystem &fs, string file_name_p, FileOpener *file_opener_p, vector<LogicalType> types_p,\n                              vector<string> names_p, CompressionCodec::type codec)\n     : file_name(std::move(file_name_p)), sql_types(std::move(types_p)), column_names(std::move(names_p)), codec(codec) {\n@@ -237,10 +250,13 @@ ParquetWriter::ParquetWriter(FileSystem &fs, string file_name_p, FileOpener *fil\n \tfile_meta_data.schema[0].repetition_type = duckdb_parquet::format::FieldRepetitionType::REQUIRED;\n \tfile_meta_data.schema[0].__isset.repetition_type = true;\n \n+\tauto &unique_names = column_names;\n+\tVerifyUniqueNames(unique_names);\n+\n \tvector<string> schema_path;\n \tfor (idx_t i = 0; i < sql_types.size(); i++) {\n \t\tcolumn_writers.push_back(ColumnWriter::CreateWriterRecursive(file_meta_data.schema, *this, sql_types[i],\n-\t\t                                                             column_names[i], schema_path));\n+\t\t                                                             unique_names[i], schema_path));\n \t}\n }\n \ndiff --git a/src/planner/binder/statement/bind_copy.cpp b/src/planner/binder/statement/bind_copy.cpp\nindex 0e94e4b03ca9..296949c0c929 100644\n--- a/src/planner/binder/statement/bind_copy.cpp\n+++ b/src/planner/binder/statement/bind_copy.cpp\n@@ -33,6 +33,34 @@ static vector<idx_t> ColumnListToIndices(const vector<bool> &vec) {\n \treturn ret;\n }\n \n+vector<string> GetUniqueNames(const vector<string> &original_names) {\n+\tunordered_set<string> name_set;\n+\tvector<string> unique_names;\n+\tunique_names.reserve(original_names.size());\n+\n+\tfor (auto &name : original_names) {\n+\t\tauto insert_result = name_set.insert(name);\n+\t\tif (insert_result.second == false) {\n+\t\t\t// Could not be inserted, name already exists\n+\t\t\tidx_t index = 1;\n+\t\t\tstring postfixed_name;\n+\t\t\twhile (true) {\n+\t\t\t\tpostfixed_name = StringUtil::Format(\"%s:%d\", name, index);\n+\t\t\t\tauto res = name_set.insert(postfixed_name);\n+\t\t\t\tif (!res.second) {\n+\t\t\t\t\tindex++;\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tunique_names.push_back(postfixed_name);\n+\t\t} else {\n+\t\t\tunique_names.push_back(name);\n+\t\t}\n+\t}\n+\treturn unique_names;\n+}\n+\n BoundStatement Binder::BindCopyTo(CopyStatement &stmt) {\n \t// COPY TO a file\n \tauto &config = DBConfig::GetConfig(context);\n@@ -99,8 +127,10 @@ BoundStatement Binder::BindCopyTo(CopyStatement &stmt) {\n \t\tuse_tmp_file = is_file_and_exists && !per_thread_output && partition_cols.empty() && !is_stdout;\n \t}\n \n+\tauto unique_column_names = GetUniqueNames(select_node.names);\n+\n \tauto function_data =\n-\t    copy_function->function.copy_to_bind(context, *stmt.info, select_node.names, select_node.types);\n+\t    copy_function->function.copy_to_bind(context, *stmt.info, unique_column_names, select_node.types);\n \t// now create the copy information\n \tauto copy = make_unique<LogicalCopyToFile>(copy_function->function, std::move(function_data));\n \tcopy->file_path = stmt.info->file_path;\n@@ -110,7 +140,8 @@ BoundStatement Binder::BindCopyTo(CopyStatement &stmt) {\n \tcopy->per_thread_output = per_thread_output;\n \tcopy->partition_output = !partition_cols.empty();\n \tcopy->partition_columns = std::move(partition_cols);\n-\tcopy->names = select_node.names;\n+\n+\tcopy->names = unique_column_names;\n \tcopy->expected_types = select_node.types;\n \n \tcopy->AddChild(std::move(select_node.plan));\n",
  "test_patch": "diff --git a/test/sql/copy/csv/test_csv_duplicate_columns.test b/test/sql/copy/csv/test_csv_duplicate_columns.test\nnew file mode 100644\nindex 000000000000..d7df991db3d5\n--- /dev/null\n+++ b/test/sql/copy/csv/test_csv_duplicate_columns.test\n@@ -0,0 +1,32 @@\n+# name: test/sql/copy/csv/test_csv_duplicate_columns.test\n+# group: [csv]\n+\n+require parquet\n+\n+statement ok\n+COPY (SELECT 1 as a, 2 as a, 3 as a) TO '__TEST_DIR__/dupe_cols.csv' (FORMAT CSV, HEADER);\n+\n+# The duplicate column(s) get `:{index}` appended to it\n+query III\n+SELECT a, \"a:1\", \"a:2\" FROM '__TEST_DIR__/dupe_cols.csv';\n+----\n+1\t2\t3\n+\n+# Original names are not preserved if a dupe appears first\n+statement ok\n+COPY (SELECT 1 as a, 2 as a, 3 as \"a:1\") TO '__TEST_DIR__/dupe_cols.csv' (FORMAT CSV, HEADER);\n+\n+# original 'a:1' is renamed to 'a:2'\n+query III\n+SELECT a, \"a:1\", \"a:1:1\" FROM '__TEST_DIR__/dupe_cols.csv';\n+----\n+1\t2\t3\n+\n+statement ok\n+COPY (SELECT 1 as a, 3 as \"a:1\", 2 as a) TO '__TEST_DIR__/dupe_cols.csv' (FORMAT CSV, HEADER);\n+\n+# Here the name is preserved, because it appears before the dupe\n+query III\n+SELECT a, \"a:1\", \"a:2\" FROM '__TEST_DIR__/dupe_cols.csv';\n+----\n+1\t3\t2\ndiff --git a/test/sql/copy/parquet/test_parquet_duplicate_columns.test b/test/sql/copy/parquet/test_parquet_duplicate_columns.test\nnew file mode 100644\nindex 000000000000..edd60f7c306c\n--- /dev/null\n+++ b/test/sql/copy/parquet/test_parquet_duplicate_columns.test\n@@ -0,0 +1,32 @@\n+# name: test/sql/copy/parquet/test_parquet_duplicate_columns.test\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+COPY (SELECT 1 as a, 2 as a, 3 as a) TO '__TEST_DIR__/dupe_cols.parquet';\n+\n+# The duplicate column(s) get `:{index}` appended to it\n+query III\n+SELECT a, \"a:1\", \"a:2\" FROM '__TEST_DIR__/dupe_cols.parquet';\n+----\n+1\t2\t3\n+\n+# Original names are not preserved if a dupe appears first\n+statement ok\n+COPY (SELECT 1 as a, 2 as a, 3 as \"a:1\") TO '__TEST_DIR__/dupe_cols.parquet';\n+\n+# original 'a:1' is renamed to 'a:2'\n+query III\n+SELECT a, \"a:1\", \"a:1:1\" FROM '__TEST_DIR__/dupe_cols.parquet';\n+----\n+1\t2\t3\n+\n+statement ok\n+COPY (SELECT 1 as a, 3 as \"a:1\", 2 as a) TO '__TEST_DIR__/dupe_cols.parquet';\n+\n+# Here the name is preserved, because it appears before the dupe\n+query III\n+SELECT a, \"a:1\", \"a:2\" FROM '__TEST_DIR__/dupe_cols.parquet';\n+----\n+1\t3\t2\n",
  "problem_statement": "Parquet writer allows duplicate columns\n### What happens?\n\nWhen writing to Parquet there is no error if there are duplicate column names.\r\n\r\nThis causes exception when Parquet file is read. It would be better if exception was thrown before writing the file instead of producing broken file.\n\n### To Reproduce\n\n```sql\r\nCOPY (SELECT 1 as a, 2 as a) TO '/tmp/duckdb-test/bad.parquet';\r\nSELECT * FROM '/tmp/duckdb-test/bad.parquet';\r\n```\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\n0.6.2-dev1770\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nTomasz Tara\u015b\n\n### Affiliation:\n\nDXC\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nParquet writer allows duplicate columns\n### What happens?\n\nWhen writing to Parquet there is no error if there are duplicate column names.\r\n\r\nThis causes exception when Parquet file is read. It would be better if exception was thrown before writing the file instead of producing broken file.\n\n### To Reproduce\n\n```sql\r\nCOPY (SELECT 1 as a, 2 as a) TO '/tmp/duckdb-test/bad.parquet';\r\nSELECT * FROM '/tmp/duckdb-test/bad.parquet';\r\n```\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\n0.6.2-dev1770\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nTomasz Tara\u015b\n\n### Affiliation:\n\nDXC\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "I think in the CSV case we allow this, but add postfixes to the duplicates, maybe that's also what we want to do here, instead of throwing on write\r\n\r\nActually.. in the CSV case we don't do anything to prevent duplicate names in headers\r\n```sql\r\nCOPY (SELECT 1 as a, 2 as a) TO 'tmp/dupe_cols.csv' (FORMAT csv, HEADER);\r\n```\r\n```csv\r\na,a\r\n1,2\r\n```\r\n\r\nYou're right though, pandas `read_parquet` also thinks the produced parquet file is broken\r\n```\r\n>>> df = pandas.read_parquet('tmp/dupe_cols.parquet')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/homebrew/lib/python3.10/site-packages/pandas/io/parquet.py\", line 503, in read_parquet\r\n    return impl.read(\r\n  File \"/opt/homebrew/lib/python3.10/site-packages/pandas/io/parquet.py\", line 251, in read\r\n    result = self.api.parquet.read_table(\r\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyarrow/parquet/__init__.py\", line 2827, in read_table\r\n    return dataset.read(columns=columns, use_threads=use_threads,\r\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyarrow/parquet/__init__.py\", line 2473, in read\r\n    table = self._dataset.to_table(\r\n  File \"pyarrow/_dataset.pyx\", line 331, in pyarrow._dataset.Dataset.to_table\r\n  File \"pyarrow/_dataset.pyx\", line 298, in pyarrow._dataset.Dataset.scanner\r\n  File \"pyarrow/_dataset.pyx\", line 2361, in pyarrow._dataset.Scanner.from_dataset\r\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Multiple matches for FieldRef.Name(a) in a: int32\r\na: int32\r\n__fragment_index: int32\r\n__batch_index: int32\r\n__last_in_fragment: bool\r\n__filename: string\r\n```\nI have written a fix for this, adding a postfix behind the duplicate column names:\r\n```sql\r\nrequire parquet\r\n\r\nstatement ok\r\nCOPY (SELECT 1 as a, 2 as a) TO '__TEST_DIR__/dupe_cols.parquet';\r\n\r\nquery II\r\nSELECT a, a_1 FROM '__TEST_DIR__/dupe_cols.parquet';\r\n----\r\n1\t2\r\n```\nI think in the CSV case we allow this, but add postfixes to the duplicates, maybe that's also what we want to do here, instead of throwing on write\r\n\r\nActually.. in the CSV case we don't do anything to prevent duplicate names in headers\r\n```sql\r\nCOPY (SELECT 1 as a, 2 as a) TO 'tmp/dupe_cols.csv' (FORMAT csv, HEADER);\r\n```\r\n```csv\r\na,a\r\n1,2\r\n```\r\n\r\nYou're right though, pandas `read_parquet` also thinks the produced parquet file is broken\r\n```\r\n>>> df = pandas.read_parquet('tmp/dupe_cols.parquet')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/homebrew/lib/python3.10/site-packages/pandas/io/parquet.py\", line 503, in read_parquet\r\n    return impl.read(\r\n  File \"/opt/homebrew/lib/python3.10/site-packages/pandas/io/parquet.py\", line 251, in read\r\n    result = self.api.parquet.read_table(\r\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyarrow/parquet/__init__.py\", line 2827, in read_table\r\n    return dataset.read(columns=columns, use_threads=use_threads,\r\n  File \"/opt/homebrew/lib/python3.10/site-packages/pyarrow/parquet/__init__.py\", line 2473, in read\r\n    table = self._dataset.to_table(\r\n  File \"pyarrow/_dataset.pyx\", line 331, in pyarrow._dataset.Dataset.to_table\r\n  File \"pyarrow/_dataset.pyx\", line 298, in pyarrow._dataset.Dataset.scanner\r\n  File \"pyarrow/_dataset.pyx\", line 2361, in pyarrow._dataset.Scanner.from_dataset\r\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Multiple matches for FieldRef.Name(a) in a: int32\r\na: int32\r\n__fragment_index: int32\r\n__batch_index: int32\r\n__last_in_fragment: bool\r\n__filename: string\r\n```\nI have written a fix for this, adding a postfix behind the duplicate column names:\r\n```sql\r\nrequire parquet\r\n\r\nstatement ok\r\nCOPY (SELECT 1 as a, 2 as a) TO '__TEST_DIR__/dupe_cols.parquet';\r\n\r\nquery II\r\nSELECT a, a_1 FROM '__TEST_DIR__/dupe_cols.parquet';\r\n----\r\n1\t2\r\n```",
  "created_at": "2023-02-06T10:09:29Z"
}