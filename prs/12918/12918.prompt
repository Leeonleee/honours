You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
COPY FROM not allowing union_by_name=True and not checking the schema from table
### What happens?

I am trying to run a COPY FROM command to pull data from Parquet files in S3 and load it into a PostgreSQL database. The S3 directory contains multiple files.

I am encountering the following error:
```
failed to cast column <column_name> from type INTEGER to VARCHAR[]: Unimplemented type for cast (INTEGER -> VARCHAR[])
```

Additionally
```
This can happen when reading multiple Parquet files. The schema information is taken from the first Parquet file by default. Possible solutions:
* Enable the union_by_name=True option to combine the schema of all Parquet files (duckdb.org/docs/data/multiple_files/combining_schemas)
* Use a COPY statement to automatically derive types from an existing table.
```

### To Reproduce

1. Create a set of Parquet files using a CREATE TABLE command in Athena.
2. Attach a PostgreSQL instance to DuckDB.
3. Run a COPY FROM command similar to the following:
```
COPY <db>.<schema>.<table_name>
FROM '{s3_location}' (FORMAT PARQUET);
```


Issue : 
There is no way to add union_by_name=True to the COPY FROM command, and this is already a COPY command.

### OS:

linux

### DuckDB Version:

1.0.0

### DuckDB Client:

python

### Full Name:

Arpit Aggarwal

### Affiliation:

Candor Health

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

No - I cannot share the data sets because they are confidential

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: 
3: #include "boolean_column_reader.hpp"
4: #include "brotli/decode.h"
5: #include "callback_column_reader.hpp"
6: #include "cast_column_reader.hpp"
7: #include "duckdb.hpp"
8: #include "expression_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "lz4.hpp"
11: #include "miniz_wrapper.hpp"
12: #include "null_column_reader.hpp"
13: #include "parquet_decimal_utils.hpp"
14: #include "parquet_reader.hpp"
15: #include "parquet_timestamp.hpp"
16: #include "row_number_column_reader.hpp"
17: #include "snappy.h"
18: #include "string_column_reader.hpp"
19: #include "struct_column_reader.hpp"
20: #include "templated_column_reader.hpp"
21: #include "utf8proc_wrapper.hpp"
22: #include "zstd.h"
23: 
24: #ifndef DUCKDB_AMALGAMATION
25: #include "duckdb/common/helper.hpp"
26: #include "duckdb/common/types/bit.hpp"
27: #include "duckdb/common/types/blob.hpp"
28: #endif
29: 
30: namespace duckdb {
31: 
32: using duckdb_parquet::format::CompressionCodec;
33: using duckdb_parquet::format::ConvertedType;
34: using duckdb_parquet::format::Encoding;
35: using duckdb_parquet::format::PageType;
36: using duckdb_parquet::format::Type;
37: 
38: const uint64_t ParquetDecodeUtils::BITPACK_MASKS[] = {0,
39:                                                       1,
40:                                                       3,
41:                                                       7,
42:                                                       15,
43:                                                       31,
44:                                                       63,
45:                                                       127,
46:                                                       255,
47:                                                       511,
48:                                                       1023,
49:                                                       2047,
50:                                                       4095,
51:                                                       8191,
52:                                                       16383,
53:                                                       32767,
54:                                                       65535,
55:                                                       131071,
56:                                                       262143,
57:                                                       524287,
58:                                                       1048575,
59:                                                       2097151,
60:                                                       4194303,
61:                                                       8388607,
62:                                                       16777215,
63:                                                       33554431,
64:                                                       67108863,
65:                                                       134217727,
66:                                                       268435455,
67:                                                       536870911,
68:                                                       1073741823,
69:                                                       2147483647,
70:                                                       4294967295,
71:                                                       8589934591,
72:                                                       17179869183,
73:                                                       34359738367,
74:                                                       68719476735,
75:                                                       137438953471,
76:                                                       274877906943,
77:                                                       549755813887,
78:                                                       1099511627775,
79:                                                       2199023255551,
80:                                                       4398046511103,
81:                                                       8796093022207,
82:                                                       17592186044415,
83:                                                       35184372088831,
84:                                                       70368744177663,
85:                                                       140737488355327,
86:                                                       281474976710655,
87:                                                       562949953421311,
88:                                                       1125899906842623,
89:                                                       2251799813685247,
90:                                                       4503599627370495,
91:                                                       9007199254740991,
92:                                                       18014398509481983,
93:                                                       36028797018963967,
94:                                                       72057594037927935,
95:                                                       144115188075855871,
96:                                                       288230376151711743,
97:                                                       576460752303423487,
98:                                                       1152921504606846975,
99:                                                       2305843009213693951,
100:                                                       4611686018427387903,
101:                                                       9223372036854775807,
102:                                                       18446744073709551615ULL};
103: 
104: const uint64_t ParquetDecodeUtils::BITPACK_MASKS_SIZE = sizeof(ParquetDecodeUtils::BITPACK_MASKS) / sizeof(uint64_t);
105: 
106: const uint8_t ParquetDecodeUtils::BITPACK_DLEN = 8;
107: 
108: ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
109:                            idx_t max_define_p, idx_t max_repeat_p)
110:     : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), reader(reader),
111:       type(std::move(type_p)), page_rows_available(0) {
112: 
113: 	// dummies for Skip()
114: 	dummy_define.resize(reader.allocator, STANDARD_VECTOR_SIZE);
115: 	dummy_repeat.resize(reader.allocator, STANDARD_VECTOR_SIZE);
116: }
117: 
118: ColumnReader::~ColumnReader() {
119: }
120: 
121: Allocator &ColumnReader::GetAllocator() {
122: 	return reader.allocator;
123: }
124: 
125: ParquetReader &ColumnReader::Reader() {
126: 	return reader;
127: }
128: 
129: const LogicalType &ColumnReader::Type() const {
130: 	return type;
131: }
132: 
133: const SchemaElement &ColumnReader::Schema() const {
134: 	return schema;
135: }
136: 
137: idx_t ColumnReader::FileIdx() const {
138: 	return file_idx;
139: }
140: 
141: idx_t ColumnReader::MaxDefine() const {
142: 	return max_define;
143: }
144: 
145: idx_t ColumnReader::MaxRepeat() const {
146: 	return max_repeat;
147: }
148: 
149: void ColumnReader::RegisterPrefetch(ThriftFileTransport &transport, bool allow_merge) {
150: 	if (chunk) {
151: 		uint64_t size = chunk->meta_data.total_compressed_size;
152: 		transport.RegisterPrefetch(FileOffset(), size, allow_merge);
153: 	}
154: }
155: 
156: uint64_t ColumnReader::TotalCompressedSize() {
157: 	if (!chunk) {
158: 		return 0;
159: 	}
160: 
161: 	return chunk->meta_data.total_compressed_size;
162: }
163: 
164: // Note: It's not trivial to determine where all Column data is stored. Chunk->file_offset
165: // apparently is not the first page of the data. Therefore we determine the address of the first page by taking the
166: // minimum of all page offsets.
167: idx_t ColumnReader::FileOffset() const {
168: 	if (!chunk) {
169: 		throw std::runtime_error("FileOffset called on ColumnReader with no chunk");
170: 	}
171: 	auto min_offset = NumericLimits<idx_t>::Maximum();
172: 	if (chunk->meta_data.__isset.dictionary_page_offset) {
173: 		min_offset = MinValue<idx_t>(min_offset, chunk->meta_data.dictionary_page_offset);
174: 	}
175: 	if (chunk->meta_data.__isset.index_page_offset) {
176: 		min_offset = MinValue<idx_t>(min_offset, chunk->meta_data.index_page_offset);
177: 	}
178: 	min_offset = MinValue<idx_t>(min_offset, chunk->meta_data.data_page_offset);
179: 
180: 	return min_offset;
181: }
182: 
183: idx_t ColumnReader::GroupRowsAvailable() {
184: 	return group_rows_available;
185: }
186: 
187: unique_ptr<BaseStatistics> ColumnReader::Stats(idx_t row_group_idx_p, const vector<ColumnChunk> &columns) {
188: 	return ParquetStatisticsUtils::TransformColumnStatistics(*this, columns);
189: }
190: 
191: void ColumnReader::Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, idx_t num_values, // NOLINT
192:                          parquet_filter_t &filter, idx_t result_offset, Vector &result) {
193: 	throw NotImplementedException("Plain");
194: }
195: 
196: void ColumnReader::Dictionary(shared_ptr<ResizeableBuffer> dictionary_data, idx_t num_entries) { // NOLINT
197: 	throw NotImplementedException("Dictionary");
198: }
199: 
200: void ColumnReader::Offsets(uint32_t *offsets, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
201:                            idx_t result_offset, Vector &result) {
202: 	throw NotImplementedException("Offsets");
203: }
204: 
205: void ColumnReader::PrepareDeltaLengthByteArray(ResizeableBuffer &buffer) {
206: 	throw std::runtime_error("DELTA_LENGTH_BYTE_ARRAY encoding is only supported for text or binary data");
207: }
208: 
209: void ColumnReader::PrepareDeltaByteArray(ResizeableBuffer &buffer) {
210: 	throw std::runtime_error("DELTA_BYTE_ARRAY encoding is only supported for text or binary data");
211: }
212: 
213: void ColumnReader::DeltaByteArray(uint8_t *defines, idx_t num_values, // NOLINT
214:                                   parquet_filter_t &filter, idx_t result_offset, Vector &result) {
215: 	throw NotImplementedException("DeltaByteArray");
216: }
217: 
218: void ColumnReader::DictReference(Vector &result) {
219: }
220: void ColumnReader::PlainReference(shared_ptr<ByteBuffer>, Vector &result) { // NOLINT
221: }
222: 
223: void ColumnReader::InitializeRead(idx_t row_group_idx_p, const vector<ColumnChunk> &columns, TProtocol &protocol_p) {
224: 	D_ASSERT(file_idx < columns.size());
225: 	chunk = &columns[file_idx];
226: 	protocol = &protocol_p;
227: 	D_ASSERT(chunk);
228: 	D_ASSERT(chunk->__isset.meta_data);
229: 
230: 	if (chunk->__isset.file_path) {
231: 		throw std::runtime_error("Only inlined data files are supported (no references)");
232: 	}
233: 
234: 	// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.
235: 	chunk_read_offset = chunk->meta_data.data_page_offset;
236: 	if (chunk->meta_data.__isset.dictionary_page_offset && chunk->meta_data.dictionary_page_offset >= 4) {
237: 		// this assumes the data pages follow the dict pages directly.
238: 		chunk_read_offset = chunk->meta_data.dictionary_page_offset;
239: 	}
240: 	group_rows_available = chunk->meta_data.num_values;
241: }
242: 
243: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
244: 	dict_decoder.reset();
245: 	defined_decoder.reset();
246: 	bss_decoder.reset();
247: 	block.reset();
248: 	PageHeader page_hdr;
249: 	reader.Read(page_hdr, *protocol);
250: 
251: 	switch (page_hdr.type) {
252: 	case PageType::DATA_PAGE_V2:
253: 		PreparePageV2(page_hdr);
254: 		PrepareDataPage(page_hdr);
255: 		break;
256: 	case PageType::DATA_PAGE:
257: 		PreparePage(page_hdr);
258: 		PrepareDataPage(page_hdr);
259: 		break;
260: 	case PageType::DICTIONARY_PAGE:
261: 		PreparePage(page_hdr);
262: 		Dictionary(std::move(block), page_hdr.dictionary_page_header.num_values);
263: 		break;
264: 	default:
265: 		break; // ignore INDEX page type and any other custom extensions
266: 	}
267: 	ResetPage();
268: }
269: 
270: void ColumnReader::ResetPage() {
271: }
272: 
273: void ColumnReader::PreparePageV2(PageHeader &page_hdr) {
274: 	D_ASSERT(page_hdr.type == PageType::DATA_PAGE_V2);
275: 
276: 	auto &trans = reinterpret_cast<ThriftFileTransport &>(*protocol->getTransport());
277: 
278: 	AllocateBlock(page_hdr.uncompressed_page_size + 1);
279: 	bool uncompressed = false;
280: 	if (page_hdr.data_page_header_v2.__isset.is_compressed && !page_hdr.data_page_header_v2.is_compressed) {
281: 		uncompressed = true;
282: 	}
283: 	if (chunk->meta_data.codec == CompressionCodec::UNCOMPRESSED) {
284: 		if (page_hdr.compressed_page_size != page_hdr.uncompressed_page_size) {
285: 			throw std::runtime_error("Page size mismatch");
286: 		}
287: 		uncompressed = true;
288: 	}
289: 	if (uncompressed) {
290: 		reader.ReadData(*protocol, block->ptr, page_hdr.compressed_page_size);
291: 		return;
292: 	}
293: 
294: 	// copy repeats & defines as-is because FOR SOME REASON they are uncompressed
295: 	auto uncompressed_bytes = page_hdr.data_page_header_v2.repetition_levels_byte_length +
296: 	                          page_hdr.data_page_header_v2.definition_levels_byte_length;
297: 	trans.read(block->ptr, uncompressed_bytes);
298: 
299: 	auto compressed_bytes = page_hdr.compressed_page_size - uncompressed_bytes;
300: 
301: 	AllocateCompressed(compressed_bytes);
302: 	reader.ReadData(*protocol, compressed_buffer.ptr, compressed_bytes);
303: 
304: 	DecompressInternal(chunk->meta_data.codec, compressed_buffer.ptr, compressed_bytes, block->ptr + uncompressed_bytes,
305: 	                   page_hdr.uncompressed_page_size - uncompressed_bytes);
306: }
307: 
308: void ColumnReader::AllocateBlock(idx_t size) {
309: 	if (!block) {
310: 		block = make_shared_ptr<ResizeableBuffer>(GetAllocator(), size);
311: 	} else {
312: 		block->resize(GetAllocator(), size);
313: 	}
314: }
315: 
316: void ColumnReader::AllocateCompressed(idx_t size) {
317: 	compressed_buffer.resize(GetAllocator(), size);
318: }
319: 
320: void ColumnReader::PreparePage(PageHeader &page_hdr) {
321: 	AllocateBlock(page_hdr.uncompressed_page_size + 1);
322: 	if (chunk->meta_data.codec == CompressionCodec::UNCOMPRESSED) {
323: 		if (page_hdr.compressed_page_size != page_hdr.uncompressed_page_size) {
324: 			throw std::runtime_error("Page size mismatch");
325: 		}
326: 		reader.ReadData(*protocol, block->ptr, page_hdr.compressed_page_size);
327: 		return;
328: 	}
329: 
330: 	AllocateCompressed(page_hdr.compressed_page_size + 1);
331: 	reader.ReadData(*protocol, compressed_buffer.ptr, page_hdr.compressed_page_size);
332: 
333: 	DecompressInternal(chunk->meta_data.codec, compressed_buffer.ptr, page_hdr.compressed_page_size, block->ptr,
334: 	                   page_hdr.uncompressed_page_size);
335: }
336: 
337: void ColumnReader::DecompressInternal(CompressionCodec::type codec, const_data_ptr_t src, idx_t src_size,
338:                                       data_ptr_t dst, idx_t dst_size) {
339: 	switch (codec) {
340: 	case CompressionCodec::UNCOMPRESSED:
341: 		throw InternalException("Parquet data unexpectedly uncompressed");
342: 	case CompressionCodec::GZIP: {
343: 		MiniZStream s;
344: 		s.Decompress(const_char_ptr_cast(src), src_size, char_ptr_cast(dst), dst_size);
345: 		break;
346: 	}
347: 	case CompressionCodec::LZ4_RAW: {
348: 		auto res = duckdb_lz4::LZ4_decompress_safe(const_char_ptr_cast(src), char_ptr_cast(dst), src_size, dst_size);
349: 		if (res != NumericCast<int>(dst_size)) {
350: 			throw std::runtime_error("LZ4 decompression failure");
351: 		}
352: 		break;
353: 	}
354: 	case CompressionCodec::SNAPPY: {
355: 		{
356: 			size_t uncompressed_size = 0;
357: 			auto res = duckdb_snappy::GetUncompressedLength(const_char_ptr_cast(src), src_size, &uncompressed_size);
358: 			if (!res) {
359: 				throw std::runtime_error("Snappy decompression failure");
360: 			}
361: 			if (uncompressed_size != dst_size) {
362: 				throw std::runtime_error("Snappy decompression failure: Uncompressed data size mismatch");
363: 			}
364: 		}
365: 		auto res = duckdb_snappy::RawUncompress(const_char_ptr_cast(src), src_size, char_ptr_cast(dst));
366: 		if (!res) {
367: 			throw std::runtime_error("Snappy decompression failure");
368: 		}
369: 		break;
370: 	}
371: 	case CompressionCodec::ZSTD: {
372: 		auto res = duckdb_zstd::ZSTD_decompress(dst, dst_size, src, src_size);
373: 		if (duckdb_zstd::ZSTD_isError(res) || res != dst_size) {
374: 			throw std::runtime_error("ZSTD Decompression failure");
375: 		}
376: 		break;
377: 	}
378: 	case CompressionCodec::BROTLI: {
379: 		auto state = duckdb_brotli::BrotliDecoderCreateInstance(nullptr, nullptr, nullptr);
380: 		size_t total_out = 0;
381: 		auto src_size_size_t = NumericCast<size_t>(src_size);
382: 		auto dst_size_size_t = NumericCast<size_t>(dst_size);
383: 
384: 		auto res = duckdb_brotli::BrotliDecoderDecompressStream(state, &src_size_size_t, &src, &dst_size_size_t, &dst,
385: 		                                                        &total_out);
386: 		if (res != duckdb_brotli::BROTLI_DECODER_RESULT_SUCCESS) {
387: 			throw std::runtime_error("Brotli Decompression failure");
388: 		}
389: 		duckdb_brotli::BrotliDecoderDestroyInstance(state);
390: 		break;
391: 	}
392: 
393: 	default: {
394: 		std::stringstream codec_name;
395: 		codec_name << codec;
396: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
397: 		                         "\". Supported options are uncompressed, brotli, gzip, lz4_raw, snappy or zstd");
398: 	}
399: 	}
400: }
401: 
402: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
403: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
404: 		throw std::runtime_error("Missing data page header from data page");
405: 	}
406: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
407: 		throw std::runtime_error("Missing data page header from data page v2");
408: 	}
409: 
410: 	bool is_v1 = page_hdr.type == PageType::DATA_PAGE;
411: 	bool is_v2 = page_hdr.type == PageType::DATA_PAGE_V2;
412: 	auto &v1_header = page_hdr.data_page_header;
413: 	auto &v2_header = page_hdr.data_page_header_v2;
414: 
415: 	page_rows_available = is_v1 ? v1_header.num_values : v2_header.num_values;
416: 	auto page_encoding = is_v1 ? v1_header.encoding : v2_header.encoding;
417: 
418: 	if (HasRepeats()) {
419: 		uint32_t rep_length = is_v1 ? block->read<uint32_t>() : v2_header.repetition_levels_byte_length;
420: 		block->available(rep_length);
421: 		repeated_decoder = make_uniq<RleBpDecoder>(block->ptr, rep_length, RleBpDecoder::ComputeBitWidth(max_repeat));
422: 		block->inc(rep_length);
423: 	} else if (is_v2 && v2_header.repetition_levels_byte_length > 0) {
424: 		block->inc(v2_header.repetition_levels_byte_length);
425: 	}
426: 
427: 	if (HasDefines()) {
428: 		uint32_t def_length = is_v1 ? block->read<uint32_t>() : v2_header.definition_levels_byte_length;
429: 		block->available(def_length);
430: 		defined_decoder = make_uniq<RleBpDecoder>(block->ptr, def_length, RleBpDecoder::ComputeBitWidth(max_define));
431: 		block->inc(def_length);
432: 	} else if (is_v2 && v2_header.definition_levels_byte_length > 0) {
433: 		block->inc(v2_header.definition_levels_byte_length);
434: 	}
435: 
436: 	switch (page_encoding) {
437: 	case Encoding::RLE_DICTIONARY:
438: 	case Encoding::PLAIN_DICTIONARY: {
439: 		// where is it otherwise??
440: 		auto dict_width = block->read<uint8_t>();
441: 		// TODO somehow dict_width can be 0 ?
442: 		dict_decoder = make_uniq<RleBpDecoder>(block->ptr, block->len, dict_width);
443: 		block->inc(block->len);
444: 		break;
445: 	}
446: 	case Encoding::RLE: {
447: 		if (type.id() != LogicalTypeId::BOOLEAN) {
448: 			throw std::runtime_error("RLE encoding is only supported for boolean data");
449: 		}
450: 		block->inc(sizeof(uint32_t));
451: 		rle_decoder = make_uniq<RleBpDecoder>(block->ptr, block->len, 1);
452: 		break;
453: 	}
454: 	case Encoding::DELTA_BINARY_PACKED: {
455: 		dbp_decoder = make_uniq<DbpDecoder>(block->ptr, block->len);
456: 		block->inc(block->len);
457: 		break;
458: 	}
459: 	case Encoding::DELTA_LENGTH_BYTE_ARRAY: {
460: 		PrepareDeltaLengthByteArray(*block);
461: 		break;
462: 	}
463: 	case Encoding::DELTA_BYTE_ARRAY: {
464: 		PrepareDeltaByteArray(*block);
465: 		break;
466: 	}
467: 	case Encoding::BYTE_STREAM_SPLIT: {
468: 		// Subtract 1 from length as the block is allocated with 1 extra byte,
469: 		// but the byte stream split encoder needs to know the correct data size.
470: 		bss_decoder = make_uniq<BssDecoder>(block->ptr, block->len - 1);
471: 		block->inc(block->len);
472: 		break;
473: 	}
474: 	case Encoding::PLAIN:
475: 		// nothing to do here, will be read directly below
476: 		break;
477: 
478: 	default:
479: 		throw std::runtime_error("Unsupported page encoding");
480: 	}
481: }
482: 
483: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, data_ptr_t define_out, data_ptr_t repeat_out,
484:                          Vector &result) {
485: 	// we need to reset the location because multiple column readers share the same protocol
486: 	auto &trans = reinterpret_cast<ThriftFileTransport &>(*protocol->getTransport());
487: 	trans.SetLocation(chunk_read_offset);
488: 
489: 	// Perform any skips that were not applied yet.
490: 	if (pending_skips > 0) {
491: 		ApplyPendingSkips(pending_skips);
492: 	}
493: 
494: 	idx_t result_offset = 0;
495: 	auto to_read = num_values;
496: 
497: 	while (to_read > 0) {
498: 		while (page_rows_available == 0) {
499: 			PrepareRead(filter);
500: 		}
501: 
502: 		D_ASSERT(block);
503: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
504: 
505: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
506: 
507: 		if (HasRepeats()) {
508: 			D_ASSERT(repeated_decoder);
509: 			repeated_decoder->GetBatch<uint8_t>(repeat_out + result_offset, read_now);
510: 		}
511: 
512: 		if (HasDefines()) {
513: 			D_ASSERT(defined_decoder);
514: 			defined_decoder->GetBatch<uint8_t>(define_out + result_offset, read_now);
515: 		}
516: 
517: 		idx_t null_count = 0;
518: 
519: 		if ((dict_decoder || dbp_decoder || rle_decoder || bss_decoder) && HasDefines()) {
520: 			// we need the null count because the dictionary offsets have no entries for nulls
521: 			for (idx_t i = 0; i < read_now; i++) {
522: 				if (define_out[i + result_offset] != max_define) {
523: 					null_count++;
524: 				}
525: 			}
526: 		}
527: 
528: 		if (dict_decoder) {
529: 			offset_buffer.resize(reader.allocator, sizeof(uint32_t) * (read_now - null_count));
530: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
531: 			DictReference(result);
532: 			Offsets(reinterpret_cast<uint32_t *>(offset_buffer.ptr), define_out, read_now, filter, result_offset,
533: 			        result);
534: 		} else if (dbp_decoder) {
535: 			// TODO keep this in the state
536: 			auto read_buf = make_shared_ptr<ResizeableBuffer>();
537: 
538: 			switch (schema.type) {
539: 			case duckdb_parquet::format::Type::INT32:
540: 				read_buf->resize(reader.allocator, sizeof(int32_t) * (read_now - null_count));
541: 				dbp_decoder->GetBatch<int32_t>(read_buf->ptr, read_now - null_count);
542: 
543: 				break;
544: 			case duckdb_parquet::format::Type::INT64:
545: 				read_buf->resize(reader.allocator, sizeof(int64_t) * (read_now - null_count));
546: 				dbp_decoder->GetBatch<int64_t>(read_buf->ptr, read_now - null_count);
547: 				break;
548: 
549: 			default:
550: 				throw std::runtime_error("DELTA_BINARY_PACKED should only be INT32 or INT64");
551: 			}
552: 			// Plain() will put NULLs in the right place
553: 			Plain(read_buf, define_out, read_now, filter, result_offset, result);
554: 		} else if (rle_decoder) {
555: 			// RLE encoding for boolean
556: 			D_ASSERT(type.id() == LogicalTypeId::BOOLEAN);
557: 			auto read_buf = make_shared_ptr<ResizeableBuffer>();
558: 			read_buf->resize(reader.allocator, sizeof(bool) * (read_now - null_count));
559: 			rle_decoder->GetBatch<uint8_t>(read_buf->ptr, read_now - null_count);
560: 			PlainTemplated<bool, TemplatedParquetValueConversion<bool>>(read_buf, define_out, read_now, filter,
561: 			                                                            result_offset, result);
562: 		} else if (byte_array_data) {
563: 			// DELTA_BYTE_ARRAY or DELTA_LENGTH_BYTE_ARRAY
564: 			DeltaByteArray(define_out, read_now, filter, result_offset, result);
565: 		} else if (bss_decoder) {
566: 			auto read_buf = make_shared_ptr<ResizeableBuffer>();
567: 
568: 			switch (schema.type) {
569: 			case duckdb_parquet::format::Type::FLOAT:
570: 				read_buf->resize(reader.allocator, sizeof(float) * (read_now - null_count));
571: 				bss_decoder->GetBatch<float>(read_buf->ptr, read_now - null_count);
572: 				break;
573: 			case duckdb_parquet::format::Type::DOUBLE:
574: 				read_buf->resize(reader.allocator, sizeof(double) * (read_now - null_count));
575: 				bss_decoder->GetBatch<double>(read_buf->ptr, read_now - null_count);
576: 				break;
577: 			default:
578: 				throw std::runtime_error("BYTE_STREAM_SPLIT encoding is only supported for FLOAT or DOUBLE data");
579: 			}
580: 
581: 			Plain(read_buf, define_out, read_now, filter, result_offset, result);
582: 		} else {
583: 			PlainReference(block, result);
584: 			Plain(block, define_out, read_now, filter, result_offset, result);
585: 		}
586: 
587: 		result_offset += read_now;
588: 		page_rows_available -= read_now;
589: 		to_read -= read_now;
590: 	}
591: 	group_rows_available -= num_values;
592: 	chunk_read_offset = trans.GetLocation();
593: 
594: 	return num_values;
595: }
596: 
597: void ColumnReader::Skip(idx_t num_values) {
598: 	pending_skips += num_values;
599: }
600: 
601: void ColumnReader::ApplyPendingSkips(idx_t num_values) {
602: 	pending_skips -= num_values;
603: 
604: 	dummy_define.zero();
605: 	dummy_repeat.zero();
606: 
607: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
608: 	Vector dummy_result(type, nullptr);
609: 
610: 	idx_t remaining = num_values;
611: 	idx_t read = 0;
612: 
613: 	while (remaining) {
614: 		idx_t to_read = MinValue<idx_t>(remaining, STANDARD_VECTOR_SIZE);
615: 		read += Read(to_read, none_filter, dummy_define.ptr, dummy_repeat.ptr, dummy_result);
616: 		remaining -= to_read;
617: 	}
618: 
619: 	if (read != num_values) {
620: 		throw std::runtime_error("Row count mismatch when skipping rows");
621: 	}
622: }
623: 
624: //===--------------------------------------------------------------------===//
625: // String Column Reader
626: //===--------------------------------------------------------------------===//
627: StringColumnReader::StringColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
628:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p)
629:     : TemplatedColumnReader<string_t, StringParquetValueConversion>(reader, std::move(type_p), schema_p, schema_idx_p,
630:                                                                     max_define_p, max_repeat_p) {
631: 	fixed_width_string_length = 0;
632: 	if (schema_p.type == Type::FIXED_LEN_BYTE_ARRAY) {
633: 		D_ASSERT(schema_p.__isset.type_length);
634: 		fixed_width_string_length = schema_p.type_length;
635: 	}
636: }
637: 
638: uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len, const bool is_varchar) {
639: 	if (!is_varchar) {
640: 		return str_len;
641: 	}
642: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
643: 	// technically Parquet should guarantee this, but reality is often disappointing
644: 	UnicodeInvalidReason reason;
645: 	size_t pos;
646: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len, &reason, &pos);
647: 	if (utf_type == UnicodeType::INVALID) {
648: 		throw InvalidInputException("Invalid string encoding found in Parquet file: value \"" +
649: 		                            Blob::ToString(string_t(str_data, str_len)) + "\" is not valid UTF8!");
650: 	}
651: 	return str_len;
652: }
653: 
654: uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {
655: 	return VerifyString(str_data, str_len, Type() == LogicalTypeId::VARCHAR);
656: }
657: 
658: void StringColumnReader::Dictionary(shared_ptr<ResizeableBuffer> data, idx_t num_entries) {
659: 	dict = std::move(data);
660: 	dict_strings = unsafe_unique_ptr<string_t[]>(new string_t[num_entries]);
661: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
662: 		uint32_t str_len;
663: 		if (fixed_width_string_length == 0) {
664: 			// variable length string: read from dictionary
665: 			str_len = dict->read<uint32_t>();
666: 		} else {
667: 			// fixed length string
668: 			str_len = fixed_width_string_length;
669: 		}
670: 		dict->available(str_len);
671: 
672: 		auto dict_str = reinterpret_cast<const char *>(dict->ptr);
673: 		auto actual_str_len = VerifyString(dict_str, str_len);
674: 		dict_strings[dict_idx] = string_t(dict_str, actual_str_len);
675: 		dict->inc(str_len);
676: 	}
677: }
678: 
679: static shared_ptr<ResizeableBuffer> ReadDbpData(Allocator &allocator, ResizeableBuffer &buffer, idx_t &value_count) {
680: 	auto decoder = make_uniq<DbpDecoder>(buffer.ptr, buffer.len);
681: 	value_count = decoder->TotalValues();
682: 	auto result = make_shared_ptr<ResizeableBuffer>();
683: 	result->resize(allocator, sizeof(uint32_t) * value_count);
684: 	decoder->GetBatch<uint32_t>(result->ptr, value_count);
685: 	decoder->Finalize();
686: 	buffer.inc(buffer.len - decoder->BufferPtr().len);
687: 	return result;
688: }
689: 
690: void StringColumnReader::PrepareDeltaLengthByteArray(ResizeableBuffer &buffer) {
691: 	idx_t value_count;
692: 	auto length_buffer = ReadDbpData(reader.allocator, buffer, value_count);
693: 	if (value_count == 0) {
694: 		// no values
695: 		byte_array_data = make_uniq<Vector>(LogicalType::VARCHAR, nullptr);
696: 		return;
697: 	}
698: 	auto length_data = reinterpret_cast<uint32_t *>(length_buffer->ptr);
699: 	byte_array_data = make_uniq<Vector>(LogicalType::VARCHAR, value_count);
700: 	byte_array_count = value_count;
701: 	delta_offset = 0;
702: 	auto string_data = FlatVector::GetData<string_t>(*byte_array_data);
703: 	for (idx_t i = 0; i < value_count; i++) {
704: 		auto str_len = length_data[i];
705: 		string_data[i] = StringVector::EmptyString(*byte_array_data, str_len);
706: 		auto result_data = string_data[i].GetDataWriteable();
707: 		memcpy(result_data, buffer.ptr, length_data[i]);
708: 		buffer.inc(length_data[i]);
709: 		string_data[i].Finalize();
710: 	}
711: }
712: 
713: void StringColumnReader::PrepareDeltaByteArray(ResizeableBuffer &buffer) {
714: 	idx_t prefix_count, suffix_count;
715: 	auto prefix_buffer = ReadDbpData(reader.allocator, buffer, prefix_count);
716: 	auto suffix_buffer = ReadDbpData(reader.allocator, buffer, suffix_count);
717: 	if (prefix_count != suffix_count) {
718: 		throw std::runtime_error("DELTA_BYTE_ARRAY - prefix and suffix counts are different - corrupt file?");
719: 	}
720: 	if (prefix_count == 0) {
721: 		// no values
722: 		byte_array_data = make_uniq<Vector>(LogicalType::VARCHAR, nullptr);
723: 		return;
724: 	}
725: 	auto prefix_data = reinterpret_cast<uint32_t *>(prefix_buffer->ptr);
726: 	auto suffix_data = reinterpret_cast<uint32_t *>(suffix_buffer->ptr);
727: 	byte_array_data = make_uniq<Vector>(LogicalType::VARCHAR, prefix_count);
728: 	byte_array_count = prefix_count;
729: 	delta_offset = 0;
730: 	auto string_data = FlatVector::GetData<string_t>(*byte_array_data);
731: 	for (idx_t i = 0; i < prefix_count; i++) {
732: 		auto str_len = prefix_data[i] + suffix_data[i];
733: 		string_data[i] = StringVector::EmptyString(*byte_array_data, str_len);
734: 		auto result_data = string_data[i].GetDataWriteable();
735: 		if (prefix_data[i] > 0) {
736: 			if (i == 0 || prefix_data[i] > string_data[i - 1].GetSize()) {
737: 				throw std::runtime_error("DELTA_BYTE_ARRAY - prefix is out of range - corrupt file?");
738: 			}
739: 			memcpy(result_data, string_data[i - 1].GetData(), prefix_data[i]);
740: 		}
741: 		memcpy(result_data + prefix_data[i], buffer.ptr, suffix_data[i]);
742: 		buffer.inc(suffix_data[i]);
743: 		string_data[i].Finalize();
744: 	}
745: }
746: 
747: void StringColumnReader::DeltaByteArray(uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
748:                                         idx_t result_offset, Vector &result) {
749: 	if (!byte_array_data) {
750: 		throw std::runtime_error("Internal error - DeltaByteArray called but there was no byte_array_data set");
751: 	}
752: 	auto result_ptr = FlatVector::GetData<string_t>(result);
753: 	auto &result_mask = FlatVector::Validity(result);
754: 	auto string_data = FlatVector::GetData<string_t>(*byte_array_data);
755: 	for (idx_t row_idx = 0; row_idx < num_values; row_idx++) {
756: 		if (HasDefines() && defines[row_idx + result_offset] != max_define) {
757: 			result_mask.SetInvalid(row_idx + result_offset);
758: 			continue;
759: 		}
760: 		if (filter.test(row_idx + result_offset)) {
761: 			if (delta_offset >= byte_array_count) {
762: 				throw IOException("DELTA_BYTE_ARRAY - length mismatch between values and byte array lengths (attempted "
763: 				                  "read of %d from %d entries) - corrupt file?",
764: 				                  delta_offset + 1, byte_array_count);
765: 			}
766: 			result_ptr[row_idx + result_offset] = string_data[delta_offset++];
767: 		} else {
768: 			delta_offset++;
769: 		}
770: 	}
771: 	StringVector::AddHeapReference(result, *byte_array_data);
772: }
773: 
774: class ParquetStringVectorBuffer : public VectorBuffer {
775: public:
776: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
777: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(std::move(buffer_p)) {
778: 	}
779: 
780: private:
781: 	shared_ptr<ByteBuffer> buffer;
782: };
783: 
784: void StringColumnReader::DictReference(Vector &result) {
785: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
786: }
787: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
788: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(std::move(plain_data)));
789: }
790: 
791: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
792: 	return reader.Cast<StringColumnReader>().dict_strings[offset];
793: }
794: 
795: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
796: 	auto &scr = reader.Cast<StringColumnReader>();
797: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
798: 	plain_data.available(str_len);
799: 	auto plain_str = char_ptr_cast(plain_data.ptr);
800: 	auto actual_str_len = reader.Cast<StringColumnReader>().VerifyString(plain_str, str_len);
801: 	auto ret_str = string_t(plain_str, actual_str_len);
802: 	plain_data.inc(str_len);
803: 	return ret_str;
804: }
805: 
806: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
807: 	auto &scr = reader.Cast<StringColumnReader>();
808: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
809: 	plain_data.inc(str_len);
810: }
811: 
812: bool StringParquetValueConversion::PlainAvailable(const ByteBuffer &plain_data, const idx_t count) {
813: 	return true;
814: }
815: 
816: string_t StringParquetValueConversion::UnsafePlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
817: 	return PlainRead(plain_data, reader);
818: }
819: 
820: void StringParquetValueConversion::UnsafePlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
821: 	PlainSkip(plain_data, reader);
822: }
823: 
824: //===--------------------------------------------------------------------===//
825: // List Column Reader
826: //===--------------------------------------------------------------------===//
827: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, data_ptr_t define_out,
828:                              data_ptr_t repeat_out, Vector &result_out) {
829: 	idx_t result_offset = 0;
830: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
831: 	auto &result_mask = FlatVector::Validity(result_out);
832: 
833: 	if (pending_skips > 0) {
834: 		ApplyPendingSkips(pending_skips);
835: 	}
836: 
837: 	D_ASSERT(ListVector::GetListSize(result_out) == 0);
838: 	// if an individual list is longer than STANDARD_VECTOR_SIZE we actually have to loop the child read to fill it
839: 	bool finished = false;
840: 	while (!finished) {
841: 		idx_t child_actual_num_values = 0;
842: 
843: 		// check if we have any overflow from a previous read
844: 		if (overflow_child_count == 0) {
845: 			// we don't: read elements from the child reader
846: 			child_defines.zero();
847: 			child_repeats.zero();
848: 			// we don't know in advance how many values to read because of the beautiful repetition/definition setup
849: 			// we just read (up to) a vector from the child column, and see if we have read enough
850: 			// if we have not read enough, we read another vector
851: 			// if we have read enough, we leave any unhandled elements in the overflow vector for a subsequent read
852: 			auto child_req_num_values =
853: 			    MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
854: 			read_vector.ResetFromCache(read_cache);
855: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
856: 			                                                    child_repeats_ptr, read_vector);
857: 		} else {
858: 			// we do: use the overflow values
859: 			child_actual_num_values = overflow_child_count;
860: 			overflow_child_count = 0;
861: 		}
862: 
863: 		if (child_actual_num_values == 0) {
864: 			// no more elements available: we are done
865: 			break;
866: 		}
867: 		read_vector.Verify(child_actual_num_values);
868: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
869: 
870: 		// hard-won piece of code this, modify at your own risk
871: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
872: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
873: 		idx_t child_idx;
874: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
875: 			if (child_repeats_ptr[child_idx] == max_repeat) {
876: 				// value repeats on this level, append
877: 				D_ASSERT(result_offset > 0);
878: 				result_ptr[result_offset - 1].length++;
879: 				continue;
880: 			}
881: 
882: 			if (result_offset >= num_values) {
883: 				// we ran out of output space
884: 				finished = true;
885: 				break;
886: 			}
887: 			if (child_defines_ptr[child_idx] >= max_define) {
888: 				// value has been defined down the stack, hence its NOT NULL
889: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
890: 				result_ptr[result_offset].length = 1;
891: 			} else if (child_defines_ptr[child_idx] == max_define - 1) {
892: 				// empty list
893: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
894: 				result_ptr[result_offset].length = 0;
895: 			} else {
896: 				// value is NULL somewhere up the stack
897: 				result_mask.SetInvalid(result_offset);
898: 				result_ptr[result_offset].offset = 0;
899: 				result_ptr[result_offset].length = 0;
900: 			}
901: 
902: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
903: 			define_out[result_offset] = child_defines_ptr[child_idx];
904: 
905: 			result_offset++;
906: 		}
907: 		// actually append the required elements to the child list
908: 		ListVector::Append(result_out, read_vector, child_idx);
909: 
910: 		// we have read more values from the child reader than we can fit into the result for this read
911: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
912: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
913: 			read_vector.Slice(read_vector, child_idx, child_actual_num_values);
914: 			overflow_child_count = child_actual_num_values - child_idx;
915: 			read_vector.Verify(overflow_child_count);
916: 
917: 			// move values in the child repeats and defines *backward* by child_idx
918: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
919: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
920: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
921: 			}
922: 		}
923: 	}
924: 	result_out.Verify(result_offset);
925: 	return result_offset;
926: }
927: 
928: ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
929:                                    idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
930:                                    unique_ptr<ColumnReader> child_column_reader_p)
931:     : ColumnReader(reader, std::move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
932:       child_column_reader(std::move(child_column_reader_p)),
933:       read_cache(reader.allocator, ListType::GetChildType(Type())), read_vector(read_cache), overflow_child_count(0) {
934: 
935: 	child_defines.resize(reader.allocator, STANDARD_VECTOR_SIZE);
936: 	child_repeats.resize(reader.allocator, STANDARD_VECTOR_SIZE);
937: 	child_defines_ptr = (uint8_t *)child_defines.ptr;
938: 	child_repeats_ptr = (uint8_t *)child_repeats.ptr;
939: 
940: 	child_filter.set();
941: }
942: 
943: void ListColumnReader::ApplyPendingSkips(idx_t num_values) {
944: 	pending_skips -= num_values;
945: 
946: 	auto define_out = unique_ptr<uint8_t[]>(new uint8_t[num_values]);
947: 	auto repeat_out = unique_ptr<uint8_t[]>(new uint8_t[num_values]);
948: 
949: 	idx_t remaining = num_values;
950: 	idx_t read = 0;
951: 
952: 	while (remaining) {
953: 		Vector result_out(Type());
954: 		parquet_filter_t filter;
955: 		idx_t to_read = MinValue<idx_t>(remaining, STANDARD_VECTOR_SIZE);
956: 		read += Read(to_read, filter, define_out.get(), repeat_out.get(), result_out);
957: 		remaining -= to_read;
958: 	}
959: 
960: 	if (read != num_values) {
961: 		throw InternalException("Not all skips done!");
962: 	}
963: }
964: 
965: //===--------------------------------------------------------------------===//
966: // Row NumberColumn Reader
967: //===--------------------------------------------------------------------===//
968: RowNumberColumnReader::RowNumberColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
969:                                              idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p)
970:     : ColumnReader(reader, std::move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p) {
971: }
972: 
973: unique_ptr<BaseStatistics> RowNumberColumnReader::Stats(idx_t row_group_idx_p, const vector<ColumnChunk> &columns) {
974: 	auto stats = NumericStats::CreateUnknown(type);
975: 	auto &row_groups = reader.GetFileMetadata()->row_groups;
976: 	D_ASSERT(row_group_idx_p < row_groups.size());
977: 	idx_t row_group_offset_min = 0;
978: 	for (idx_t i = 0; i < row_group_idx_p; i++) {
979: 		row_group_offset_min += row_groups[i].num_rows;
980: 	}
981: 
982: 	NumericStats::SetMin(stats, Value::BIGINT(row_group_offset_min));
983: 	NumericStats::SetMax(stats, Value::BIGINT(row_group_offset_min + row_groups[row_group_idx_p].num_rows));
984: 	stats.Set(StatsInfo::CANNOT_HAVE_NULL_VALUES);
985: 	return stats.ToUnique();
986: }
987: 
988: void RowNumberColumnReader::InitializeRead(idx_t row_group_idx_p, const vector<ColumnChunk> &columns,
989:                                            TProtocol &protocol_p) {
990: 	row_group_offset = 0;
991: 	auto &row_groups = reader.GetFileMetadata()->row_groups;
992: 	for (idx_t i = 0; i < row_group_idx_p; i++) {
993: 		row_group_offset += row_groups[i].num_rows;
994: 	}
995: }
996: 
997: idx_t RowNumberColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, data_ptr_t define_out,
998:                                   data_ptr_t repeat_out, Vector &result) {
999: 
1000: 	auto data_ptr = FlatVector::GetData<int64_t>(result);
1001: 	for (idx_t i = 0; i < num_values; i++) {
1002: 		data_ptr[i] = row_group_offset++;
1003: 	}
1004: 	return num_values;
1005: }
1006: 
1007: //===--------------------------------------------------------------------===//
1008: // Cast Column Reader
1009: //===--------------------------------------------------------------------===//
1010: CastColumnReader::CastColumnReader(unique_ptr<ColumnReader> child_reader_p, LogicalType target_type_p)
1011:     : ColumnReader(child_reader_p->Reader(), std::move(target_type_p), child_reader_p->Schema(),
1012:                    child_reader_p->FileIdx(), child_reader_p->MaxDefine(), child_reader_p->MaxRepeat()),
1013:       child_reader(std::move(child_reader_p)) {
1014: 	vector<LogicalType> intermediate_types {child_reader->Type()};
1015: 	intermediate_chunk.Initialize(reader.allocator, intermediate_types);
1016: }
1017: 
1018: unique_ptr<BaseStatistics> CastColumnReader::Stats(idx_t row_group_idx_p, const vector<ColumnChunk> &columns) {
1019: 	// casting stats is not supported (yet)
1020: 	return nullptr;
1021: }
1022: 
1023: void CastColumnReader::InitializeRead(idx_t row_group_idx_p, const vector<ColumnChunk> &columns,
1024:                                       TProtocol &protocol_p) {
1025: 	child_reader->InitializeRead(row_group_idx_p, columns, protocol_p);
1026: }
1027: 
1028: idx_t CastColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, data_ptr_t define_out,
1029:                              data_ptr_t repeat_out, Vector &result) {
1030: 	intermediate_chunk.Reset();
1031: 	auto &intermediate_vector = intermediate_chunk.data[0];
1032: 
1033: 	auto amount = child_reader->Read(num_values, filter, define_out, repeat_out, intermediate_vector);
1034: 	if (!filter.all()) {
1035: 		// work-around for filters: set all values that are filtered to NULL to prevent the cast from failing on
1036: 		// uninitialized data
1037: 		intermediate_vector.Flatten(amount);
1038: 		auto &validity = FlatVector::Validity(intermediate_vector);
1039: 		for (idx_t i = 0; i < amount; i++) {
1040: 			if (!filter.test(i)) {
1041: 				validity.SetInvalid(i);
1042: 			}
1043: 		}
1044: 	}
1045: 	string error_message;
1046: 	bool all_succeeded = VectorOperations::DefaultTryCast(intermediate_vector, result, amount, &error_message);
1047: 	if (!all_succeeded) {
1048: 		string extended_error;
1049: 		extended_error =
1050: 		    StringUtil::Format("In file \"%s\" the column \"%s\" has type %s, but we are trying to read it as type %s.",
1051: 		                       reader.file_name, schema.name, intermediate_vector.GetType(), result.GetType());
1052: 		extended_error += "\nThis can happen when reading multiple Parquet files. The schema information is taken from "
1053: 		                  "the first Parquet file by default. Possible solutions:\n";
1054: 		extended_error += "* Enable the union_by_name=True option to combine the schema of all Parquet files "
1055: 		                  "(duckdb.org/docs/data/multiple_files/combining_schemas)\n";
1056: 		extended_error += "* Use a COPY statement to automatically derive types from an existing table.";
1057: 		throw ConversionException(
1058: 		    "In Parquet reader of file \"%s\": failed to cast column \"%s\" from type %s to %s: %s\n\n%s",
1059: 		    reader.file_name, schema.name, intermediate_vector.GetType(), result.GetType(), error_message,
1060: 		    extended_error);
1061: 	}
1062: 	return amount;
1063: }
1064: 
1065: void CastColumnReader::Skip(idx_t num_values) {
1066: 	child_reader->Skip(num_values);
1067: }
1068: 
1069: idx_t CastColumnReader::GroupRowsAvailable() {
1070: 	return child_reader->GroupRowsAvailable();
1071: }
1072: 
1073: //===--------------------------------------------------------------------===//
1074: // Expression Column Reader
1075: //===--------------------------------------------------------------------===//
1076: ExpressionColumnReader::ExpressionColumnReader(ClientContext &context, unique_ptr<ColumnReader> child_reader_p,
1077:                                                unique_ptr<Expression> expr_p)
1078:     : ColumnReader(child_reader_p->Reader(), expr_p->return_type, child_reader_p->Schema(), child_reader_p->FileIdx(),
1079:                    child_reader_p->MaxDefine(), child_reader_p->MaxRepeat()),
1080:       child_reader(std::move(child_reader_p)), expr(std::move(expr_p)), executor(context, expr.get()) {
1081: 	vector<LogicalType> intermediate_types {child_reader->Type()};
1082: 	intermediate_chunk.Initialize(reader.allocator, intermediate_types);
1083: }
1084: 
1085: unique_ptr<BaseStatistics> ExpressionColumnReader::Stats(idx_t row_group_idx_p, const vector<ColumnChunk> &columns) {
1086: 	// expression stats is not supported (yet)
1087: 	return nullptr;
1088: }
1089: 
1090: void ExpressionColumnReader::InitializeRead(idx_t row_group_idx_p, const vector<ColumnChunk> &columns,
1091:                                             TProtocol &protocol_p) {
1092: 	child_reader->InitializeRead(row_group_idx_p, columns, protocol_p);
1093: }
1094: 
1095: idx_t ExpressionColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, data_ptr_t define_out,
1096:                                    data_ptr_t repeat_out, Vector &result) {
1097: 	intermediate_chunk.Reset();
1098: 	auto &intermediate_vector = intermediate_chunk.data[0];
1099: 
1100: 	auto amount = child_reader->Read(num_values, filter, define_out, repeat_out, intermediate_vector);
1101: 	if (!filter.all()) {
1102: 		// work-around for filters: set all values that are filtered to NULL to prevent the cast from failing on
1103: 		// uninitialized data
1104: 		intermediate_vector.Flatten(amount);
1105: 		auto &validity = FlatVector::Validity(intermediate_vector);
1106: 		for (idx_t i = 0; i < amount; i++) {
1107: 			if (!filter[i]) {
1108: 				validity.SetInvalid(i);
1109: 			}
1110: 		}
1111: 	}
1112: 	// Execute the expression
1113: 	intermediate_chunk.SetCardinality(amount);
1114: 	executor.ExecuteExpression(intermediate_chunk, result);
1115: 	return amount;
1116: }
1117: 
1118: void ExpressionColumnReader::Skip(idx_t num_values) {
1119: 	child_reader->Skip(num_values);
1120: }
1121: 
1122: idx_t ExpressionColumnReader::GroupRowsAvailable() {
1123: 	return child_reader->GroupRowsAvailable();
1124: }
1125: 
1126: //===--------------------------------------------------------------------===//
1127: // Struct Column Reader
1128: //===--------------------------------------------------------------------===//
1129: StructColumnReader::StructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
1130:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
1131:                                        vector<unique_ptr<ColumnReader>> child_readers_p)
1132:     : ColumnReader(reader, std::move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
1133:       child_readers(std::move(child_readers_p)) {
1134: 	D_ASSERT(type.InternalType() == PhysicalType::STRUCT);
1135: }
1136: 
1137: ColumnReader *StructColumnReader::GetChildReader(idx_t child_idx) {
1138: 	D_ASSERT(child_idx < child_readers.size());
1139: 	return child_readers[child_idx].get();
1140: }
1141: 
1142: void StructColumnReader::InitializeRead(idx_t row_group_idx_p, const vector<ColumnChunk> &columns,
1143:                                         TProtocol &protocol_p) {
1144: 	for (auto &child : child_readers) {
1145: 		child->InitializeRead(row_group_idx_p, columns, protocol_p);
1146: 	}
1147: }
1148: 
1149: idx_t StructColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, data_ptr_t define_out,
1150:                                data_ptr_t repeat_out, Vector &result) {
1151: 	auto &struct_entries = StructVector::GetEntries(result);
1152: 	D_ASSERT(StructType::GetChildTypes(Type()).size() == struct_entries.size());
1153: 
1154: 	if (pending_skips > 0) {
1155: 		ApplyPendingSkips(pending_skips);
1156: 	}
1157: 
1158: 	idx_t read_count = num_values;
1159: 	for (idx_t i = 0; i < struct_entries.size(); i++) {
1160: 		auto child_num_values = child_readers[i]->Read(num_values, filter, define_out, repeat_out, *struct_entries[i]);
1161: 		if (i == 0) {
1162: 			read_count = child_num_values;
1163: 		} else if (read_count != child_num_values) {
1164: 			throw std::runtime_error("Struct child row count mismatch");
1165: 		}
1166: 	}
1167: 	// set the validity mask for this level
1168: 	auto &validity = FlatVector::Validity(result);
1169: 	for (idx_t i = 0; i < read_count; i++) {
1170: 		if (define_out[i] < max_define) {
1171: 			validity.SetInvalid(i);
1172: 		}
1173: 	}
1174: 
1175: 	return read_count;
1176: }
1177: 
1178: void StructColumnReader::Skip(idx_t num_values) {
1179: 	for (auto &child_reader : child_readers) {
1180: 		child_reader->Skip(num_values);
1181: 	}
1182: }
1183: 
1184: void StructColumnReader::RegisterPrefetch(ThriftFileTransport &transport, bool allow_merge) {
1185: 	for (auto &child : child_readers) {
1186: 		child->RegisterPrefetch(transport, allow_merge);
1187: 	}
1188: }
1189: 
1190: uint64_t StructColumnReader::TotalCompressedSize() {
1191: 	uint64_t size = 0;
1192: 	for (auto &child : child_readers) {
1193: 		size += child->TotalCompressedSize();
1194: 	}
1195: 	return size;
1196: }
1197: 
1198: static bool TypeHasExactRowCount(const LogicalType &type) {
1199: 	switch (type.id()) {
1200: 	case LogicalTypeId::LIST:
1201: 	case LogicalTypeId::MAP:
1202: 		return false;
1203: 	case LogicalTypeId::STRUCT:
1204: 		for (auto &kv : StructType::GetChildTypes(type)) {
1205: 			if (TypeHasExactRowCount(kv.second)) {
1206: 				return true;
1207: 			}
1208: 		}
1209: 		return false;
1210: 	default:
1211: 		return true;
1212: 	}
1213: }
1214: 
1215: idx_t StructColumnReader::GroupRowsAvailable() {
1216: 	for (idx_t i = 0; i < child_readers.size(); i++) {
1217: 		if (TypeHasExactRowCount(child_readers[i]->Type())) {
1218: 			return child_readers[i]->GroupRowsAvailable();
1219: 		}
1220: 	}
1221: 	return child_readers[0]->GroupRowsAvailable();
1222: }
1223: 
1224: //===--------------------------------------------------------------------===//
1225: // Decimal Column Reader
1226: //===--------------------------------------------------------------------===//
1227: template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>
1228: struct DecimalParquetValueConversion {
1229: 	static DUCKDB_PHYSICAL_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
1230: 		return reinterpret_cast<DUCKDB_PHYSICAL_TYPE *>(dict.ptr)[offset];
1231: 	}
1232: 
1233: 	static DUCKDB_PHYSICAL_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
1234: 		idx_t byte_len;
1235: 		if (FIXED_LENGTH) {
1236: 			byte_len = (idx_t)reader.Schema().type_length; /* sure, type length needs to be a signed int */
1237: 		} else {
1238: 			byte_len = plain_data.read<uint32_t>();
1239: 		}
1240: 		plain_data.available(byte_len);
1241: 		auto res = ParquetDecimalUtils::ReadDecimalValue<DUCKDB_PHYSICAL_TYPE>(const_data_ptr_cast(plain_data.ptr),
1242: 		                                                                       byte_len, reader.Schema());
1243: 
1244: 		plain_data.inc(byte_len);
1245: 		return res;
1246: 	}
1247: 
1248: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
1249: 		uint32_t decimal_len = FIXED_LENGTH ? reader.Schema().type_length : plain_data.read<uint32_t>();
1250: 		plain_data.inc(decimal_len);
1251: 	}
1252: 
1253: 	static bool PlainAvailable(const ByteBuffer &plain_data, const idx_t count) {
1254: 		return true;
1255: 	}
1256: 
1257: 	static DUCKDB_PHYSICAL_TYPE UnsafePlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
1258: 		return PlainRead(plain_data, reader);
1259: 	}
1260: 
1261: 	static void UnsafePlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
1262: 		PlainSkip(plain_data, reader);
1263: 	}
1264: };
1265: 
1266: template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>
1267: class DecimalColumnReader
1268:     : public TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
1269:                                    DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>> {
1270: 	using BaseType =
1271: 	    TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE, DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>>;
1272: 
1273: public:
1274: 	DecimalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, // NOLINT
1275: 	                    idx_t file_idx_p, idx_t max_define_p, idx_t max_repeat_p)
1276: 	    : TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
1277: 	                            DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>>(
1278: 	          reader, std::move(type_p), schema_p, file_idx_p, max_define_p, max_repeat_p) {};
1279: 
1280: protected:
1281: 	void Dictionary(shared_ptr<ResizeableBuffer> dictionary_data, idx_t num_entries) { // NOLINT
1282: 		BaseType::AllocateDict(num_entries * sizeof(DUCKDB_PHYSICAL_TYPE));
1283: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)this->dict->ptr;
1284: 		for (idx_t i = 0; i < num_entries; i++) {
1285: 			dict_ptr[i] =
1286: 			    DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>::PlainRead(*dictionary_data, *this);
1287: 		}
1288: 	}
1289: };
1290: 
1291: template <bool FIXED_LENGTH>
1292: static unique_ptr<ColumnReader> CreateDecimalReaderInternal(ParquetReader &reader, const LogicalType &type_p,
1293:                                                             const SchemaElement &schema_p, idx_t file_idx_p,
1294:                                                             idx_t max_define, idx_t max_repeat) {
1295: 	switch (type_p.InternalType()) {
1296: 	case PhysicalType::INT16:
1297: 		return make_uniq<DecimalColumnReader<int16_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
1298: 		                                                             max_repeat);
1299: 	case PhysicalType::INT32:
1300: 		return make_uniq<DecimalColumnReader<int32_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
1301: 		                                                             max_repeat);
1302: 	case PhysicalType::INT64:
1303: 		return make_uniq<DecimalColumnReader<int64_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
1304: 		                                                             max_repeat);
1305: 	case PhysicalType::INT128:
1306: 		return make_uniq<DecimalColumnReader<hugeint_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
1307: 		                                                               max_repeat);
1308: 	case PhysicalType::DOUBLE:
1309: 		return make_uniq<DecimalColumnReader<double, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
1310: 		                                                            max_repeat);
1311: 	default:
1312: 		throw InternalException("Unrecognized type for Decimal");
1313: 	}
1314: }
1315: 
1316: template <>
1317: double ParquetDecimalUtils::ReadDecimalValue(const_data_ptr_t pointer, idx_t size,
1318:                                              const duckdb_parquet::format::SchemaElement &schema_ele) {
1319: 	double res = 0;
1320: 	bool positive = (*pointer & 0x80) == 0;
1321: 	for (idx_t i = 0; i < size; i += 8) {
1322: 		auto byte_size = MinValue<idx_t>(sizeof(uint64_t), size - i);
1323: 		uint64_t input = 0;
1324: 		auto res_ptr = reinterpret_cast<uint8_t *>(&input);
1325: 		for (idx_t k = 0; k < byte_size; k++) {
1326: 			auto byte = pointer[i + k];
1327: 			res_ptr[sizeof(uint64_t) - k - 1] = positive ? byte : byte ^ 0xFF;
1328: 		}
1329: 		res *= double(NumericLimits<uint64_t>::Maximum()) + 1;
1330: 		res += input;
1331: 	}
1332: 	if (!positive) {
1333: 		res += 1;
1334: 		res /= pow(10, schema_ele.scale);
1335: 		return -res;
1336: 	}
1337: 	res /= pow(10, schema_ele.scale);
1338: 	return res;
1339: }
1340: 
1341: unique_ptr<ColumnReader> ParquetDecimalUtils::CreateReader(ParquetReader &reader, const LogicalType &type_p,
1342:                                                            const SchemaElement &schema_p, idx_t file_idx_p,
1343:                                                            idx_t max_define, idx_t max_repeat) {
1344: 	if (schema_p.__isset.type_length) {
1345: 		return CreateDecimalReaderInternal<true>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1346: 	} else {
1347: 		return CreateDecimalReaderInternal<false>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1348: 	}
1349: }
1350: 
1351: //===--------------------------------------------------------------------===//
1352: // UUID Column Reader
1353: //===--------------------------------------------------------------------===//
1354: struct UUIDValueConversion {
1355: 	static hugeint_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
1356: 		return reinterpret_cast<hugeint_t *>(dict.ptr)[offset];
1357: 	}
1358: 
1359: 	static hugeint_t ReadParquetUUID(const_data_ptr_t input) {
1360: 		hugeint_t result;
1361: 		result.lower = 0;
1362: 		uint64_t unsigned_upper = 0;
1363: 		for (idx_t i = 0; i < sizeof(uint64_t); i++) {
1364: 			unsigned_upper <<= 8;
1365: 			unsigned_upper += input[i];
1366: 		}
1367: 		for (idx_t i = sizeof(uint64_t); i < sizeof(hugeint_t); i++) {
1368: 			result.lower <<= 8;
1369: 			result.lower += input[i];
1370: 		}
1371: 		result.upper = unsigned_upper;
1372: 		result.upper ^= (int64_t(1) << 63);
1373: 		return result;
1374: 	}
1375: 
1376: 	static hugeint_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
1377: 		plain_data.available(sizeof(hugeint_t));
1378: 		return UnsafePlainRead(plain_data, reader);
1379: 	}
1380: 
1381: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
1382: 		plain_data.inc(sizeof(hugeint_t));
1383: 	}
1384: 
1385: 	static bool PlainAvailable(const ByteBuffer &plain_data, const idx_t count) {
1386: 		return plain_data.check_available(count * sizeof(hugeint_t));
1387: 	}
1388: 
1389: 	static hugeint_t UnsafePlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
1390: 		auto res = ReadParquetUUID(const_data_ptr_cast(plain_data.ptr));
1391: 		plain_data.unsafe_inc(sizeof(hugeint_t));
1392: 		return res;
1393: 	}
1394: 
1395: 	static void UnsafePlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
1396: 		plain_data.unsafe_inc(sizeof(hugeint_t));
1397: 	}
1398: };
1399: 
1400: class UUIDColumnReader : public TemplatedColumnReader<hugeint_t, UUIDValueConversion> {
1401: 
1402: public:
1403: 	UUIDColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
1404: 	                 idx_t max_define_p, idx_t max_repeat_p)
1405: 	    : TemplatedColumnReader<hugeint_t, UUIDValueConversion>(reader, std::move(type_p), schema_p, file_idx_p,
1406: 	                                                            max_define_p, max_repeat_p) {};
1407: 
1408: protected:
1409: 	void Dictionary(shared_ptr<ResizeableBuffer> dictionary_data, idx_t num_entries) { // NOLINT
1410: 		AllocateDict(num_entries * sizeof(hugeint_t));
1411: 		auto dict_ptr = reinterpret_cast<hugeint_t *>(this->dict->ptr);
1412: 		for (idx_t i = 0; i < num_entries; i++) {
1413: 			dict_ptr[i] = UUIDValueConversion::PlainRead(*dictionary_data, *this);
1414: 		}
1415: 	}
1416: };
1417: 
1418: //===--------------------------------------------------------------------===//
1419: // Interval Column Reader
1420: //===--------------------------------------------------------------------===//
1421: struct IntervalValueConversion {
1422: 	static constexpr const idx_t PARQUET_INTERVAL_SIZE = 12;
1423: 
1424: 	static interval_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
1425: 		return reinterpret_cast<interval_t *>(dict.ptr)[offset];
1426: 	}
1427: 
1428: 	static interval_t ReadParquetInterval(const_data_ptr_t input) {
1429: 		interval_t result;
1430: 		result.months = Load<uint32_t>(input);
1431: 		result.days = Load<uint32_t>(input + sizeof(uint32_t));
1432: 		result.micros = int64_t(Load<uint32_t>(input + sizeof(uint32_t) * 2)) * 1000;
1433: 		return result;
1434: 	}
1435: 
1436: 	static interval_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
1437: 		plain_data.available(PARQUET_INTERVAL_SIZE);
1438: 		return UnsafePlainRead(plain_data, reader);
1439: 	}
1440: 
1441: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
1442: 		plain_data.inc(PARQUET_INTERVAL_SIZE);
1443: 	}
1444: 
1445: 	static bool PlainAvailable(const ByteBuffer &plain_data, const idx_t count) {
1446: 		return plain_data.check_available(count * PARQUET_INTERVAL_SIZE);
1447: 	}
1448: 
1449: 	static interval_t UnsafePlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
1450: 		auto res = ReadParquetInterval(const_data_ptr_cast(plain_data.ptr));
1451: 		plain_data.unsafe_inc(PARQUET_INTERVAL_SIZE);
1452: 		return res;
1453: 	}
1454: 
1455: 	static void UnsafePlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
1456: 		plain_data.unsafe_inc(PARQUET_INTERVAL_SIZE);
1457: 	}
1458: };
1459: 
1460: class IntervalColumnReader : public TemplatedColumnReader<interval_t, IntervalValueConversion> {
1461: 
1462: public:
1463: 	IntervalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
1464: 	                     idx_t max_define_p, idx_t max_repeat_p)
1465: 	    : TemplatedColumnReader<interval_t, IntervalValueConversion>(reader, std::move(type_p), schema_p, file_idx_p,
1466: 	                                                                 max_define_p, max_repeat_p) {};
1467: 
1468: protected:
1469: 	void Dictionary(shared_ptr<ResizeableBuffer> dictionary_data, idx_t num_entries) override { // NOLINT
1470: 		AllocateDict(num_entries * sizeof(interval_t));
1471: 		auto dict_ptr = reinterpret_cast<interval_t *>(this->dict->ptr);
1472: 		for (idx_t i = 0; i < num_entries; i++) {
1473: 			dict_ptr[i] = IntervalValueConversion::PlainRead(*dictionary_data, *this);
1474: 		}
1475: 	}
1476: };
1477: 
1478: //===--------------------------------------------------------------------===//
1479: // Create Column Reader
1480: //===--------------------------------------------------------------------===//
1481: template <class T>
1482: unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,
1483:                                              const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
1484:                                              idx_t max_repeat) {
1485: 	switch (type_p.InternalType()) {
1486: 	case PhysicalType::INT16:
1487: 		return make_uniq<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(
1488: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1489: 	case PhysicalType::INT32:
1490: 		return make_uniq<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(
1491: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1492: 	case PhysicalType::INT64:
1493: 		return make_uniq<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(
1494: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1495: 	default:
1496: 		throw NotImplementedException("Unimplemented internal type for CreateDecimalReader");
1497: 	}
1498: }
1499: 
1500: unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,
1501:                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
1502:                                                     idx_t max_repeat) {
1503: 	switch (type_p.id()) {
1504: 	case LogicalTypeId::BOOLEAN:
1505: 		return make_uniq<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1506: 	case LogicalTypeId::UTINYINT:
1507: 		return make_uniq<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
1508: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1509: 	case LogicalTypeId::USMALLINT:
1510: 		return make_uniq<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
1511: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1512: 	case LogicalTypeId::UINTEGER:
1513: 		return make_uniq<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
1514: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1515: 	case LogicalTypeId::UBIGINT:
1516: 		return make_uniq<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
1517: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1518: 	case LogicalTypeId::TINYINT:
1519: 		return make_uniq<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(
1520: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1521: 	case LogicalTypeId::SMALLINT:
1522: 		return make_uniq<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(
1523: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1524: 	case LogicalTypeId::INTEGER:
1525: 		return make_uniq<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
1526: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1527: 	case LogicalTypeId::BIGINT:
1528: 		return make_uniq<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
1529: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1530: 	case LogicalTypeId::FLOAT:
1531: 		return make_uniq<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
1532: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1533: 	case LogicalTypeId::DOUBLE:
1534: 		switch (schema_p.type) {
1535: 		case Type::BYTE_ARRAY:
1536: 		case Type::FIXED_LEN_BYTE_ARRAY:
1537: 			return ParquetDecimalUtils::CreateReader(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1538: 		default:
1539: 			return make_uniq<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
1540: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1541: 		}
1542: 	case LogicalTypeId::TIMESTAMP:
1543: 	case LogicalTypeId::TIMESTAMP_TZ:
1544: 		switch (schema_p.type) {
1545: 		case Type::INT96:
1546: 			return make_uniq<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
1547: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1548: 		case Type::INT64:
1549: 			if (schema_p.__isset.logicalType && schema_p.logicalType.__isset.TIMESTAMP) {
1550: 				if (schema_p.logicalType.TIMESTAMP.unit.__isset.MILLIS) {
1551: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
1552: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1553: 				} else if (schema_p.logicalType.TIMESTAMP.unit.__isset.MICROS) {
1554: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
1555: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1556: 				} else if (schema_p.logicalType.TIMESTAMP.unit.__isset.NANOS) {
1557: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampNsToTimestamp>>(
1558: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1559: 				}
1560: 			} else if (schema_p.__isset.converted_type) {
1561: 				switch (schema_p.converted_type) {
1562: 				case ConvertedType::TIMESTAMP_MICROS:
1563: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
1564: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1565: 				case ConvertedType::TIMESTAMP_MILLIS:
1566: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
1567: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1568: 				default:
1569: 					break;
1570: 				}
1571: 			}
1572: 		default:
1573: 			break;
1574: 		}
1575: 		break;
1576: 	case LogicalTypeId::TIMESTAMP_NS:
1577: 		switch (schema_p.type) {
1578: 		case Type::INT96:
1579: 			return make_uniq<CallbackColumnReader<Int96, timestamp_ns_t, ImpalaTimestampToTimestampNS>>(
1580: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1581: 		case Type::INT64:
1582: 			if (schema_p.__isset.logicalType && schema_p.logicalType.__isset.TIMESTAMP) {
1583: 				if (schema_p.logicalType.TIMESTAMP.unit.__isset.MILLIS) {
1584: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_ns_t, ParquetTimestampMsToTimestampNs>>(
1585: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1586: 				} else if (schema_p.logicalType.TIMESTAMP.unit.__isset.MICROS) {
1587: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_ns_t, ParquetTimestampUsToTimestampNs>>(
1588: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1589: 				} else if (schema_p.logicalType.TIMESTAMP.unit.__isset.NANOS) {
1590: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_ns_t, ParquetTimestampNsToTimestampNs>>(
1591: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1592: 				}
1593: 			} else if (schema_p.__isset.converted_type) {
1594: 				switch (schema_p.converted_type) {
1595: 				case ConvertedType::TIMESTAMP_MICROS:
1596: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_ns_t, ParquetTimestampUsToTimestampNs>>(
1597: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1598: 				case ConvertedType::TIMESTAMP_MILLIS:
1599: 					return make_uniq<CallbackColumnReader<int64_t, timestamp_ns_t, ParquetTimestampMsToTimestampNs>>(
1600: 					    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1601: 				default:
1602: 					break;
1603: 				}
1604: 			}
1605: 		default:
1606: 			break;
1607: 		}
1608: 		break;
1609: 	case LogicalTypeId::DATE:
1610: 		return make_uniq<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p, file_idx_p,
1611: 		                                                                          max_define, max_repeat);
1612: 	case LogicalTypeId::TIME:
1613: 		if (schema_p.__isset.logicalType && schema_p.logicalType.__isset.TIME) {
1614: 			if (schema_p.logicalType.TIME.unit.__isset.MILLIS) {
1615: 				return make_uniq<CallbackColumnReader<int32_t, dtime_t, ParquetIntToTimeMs>>(
1616: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1617: 			} else if (schema_p.logicalType.TIME.unit.__isset.MICROS) {
1618: 				return make_uniq<CallbackColumnReader<int64_t, dtime_t, ParquetIntToTime>>(
1619: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1620: 			} else if (schema_p.logicalType.TIME.unit.__isset.NANOS) {
1621: 				return make_uniq<CallbackColumnReader<int64_t, dtime_t, ParquetIntToTimeNs>>(
1622: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1623: 			}
1624: 		} else if (schema_p.__isset.converted_type) {
1625: 			switch (schema_p.converted_type) {
1626: 			case ConvertedType::TIME_MICROS:
1627: 				return make_uniq<CallbackColumnReader<int64_t, dtime_t, ParquetIntToTime>>(
1628: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1629: 			case ConvertedType::TIME_MILLIS:
1630: 				return make_uniq<CallbackColumnReader<int32_t, dtime_t, ParquetIntToTimeMs>>(
1631: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1632: 			default:
1633: 				break;
1634: 			}
1635: 		}
1636: 		throw NotImplementedException("Unsupported time encoding in Parquet file");
1637: 	case LogicalTypeId::TIME_TZ:
1638: 		if (schema_p.__isset.logicalType && schema_p.logicalType.__isset.TIME) {
1639: 			if (schema_p.logicalType.TIME.unit.__isset.MILLIS) {
1640: 				return make_uniq<CallbackColumnReader<int32_t, dtime_tz_t, ParquetIntToTimeMsTZ>>(
1641: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1642: 			} else if (schema_p.logicalType.TIME.unit.__isset.MICROS) {
1643: 				return make_uniq<CallbackColumnReader<int64_t, dtime_tz_t, ParquetIntToTimeTZ>>(
1644: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1645: 			} else if (schema_p.logicalType.TIME.unit.__isset.NANOS) {
1646: 				return make_uniq<CallbackColumnReader<int64_t, dtime_tz_t, ParquetIntToTimeNsTZ>>(
1647: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1648: 			}
1649: 		} else if (schema_p.__isset.converted_type) {
1650: 			switch (schema_p.converted_type) {
1651: 			case ConvertedType::TIME_MICROS:
1652: 				return make_uniq<CallbackColumnReader<int64_t, dtime_tz_t, ParquetIntToTimeTZ>>(
1653: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1654: 			default:
1655: 				break;
1656: 			}
1657: 		}
1658: 		throw NotImplementedException("Unsupported time encoding in Parquet file");
1659: 	case LogicalTypeId::BLOB:
1660: 	case LogicalTypeId::VARCHAR:
1661: 		return make_uniq<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1662: 	case LogicalTypeId::DECIMAL:
1663: 		// we have to figure out what kind of int we need
1664: 		switch (schema_p.type) {
1665: 		case Type::INT32:
1666: 			return CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1667: 		case Type::INT64:
1668: 			return CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1669: 		case Type::BYTE_ARRAY:
1670: 		case Type::FIXED_LEN_BYTE_ARRAY:
1671: 			return ParquetDecimalUtils::CreateReader(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1672: 		default:
1673: 			throw NotImplementedException("Unrecognized Parquet type for Decimal");
1674: 		}
1675: 		break;
1676: 	case LogicalTypeId::UUID:
1677: 		return make_uniq<UUIDColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1678: 	case LogicalTypeId::INTERVAL:
1679: 		return make_uniq<IntervalColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1680: 	case LogicalTypeId::SQLNULL:
1681: 		return make_uniq<NullColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
1682: 	default:
1683: 		break;
1684: 	}
1685: 	throw NotImplementedException(type_p.ToString());
1686: }
1687: 
1688: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of extension/parquet/include/parquet_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parquet_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb/common/common.hpp"
14: #include "duckdb/common/exception.hpp"
15: #include "duckdb/common/multi_file_reader.hpp"
16: #include "duckdb/common/multi_file_reader_options.hpp"
17: #include "duckdb/common/string_util.hpp"
18: #include "duckdb/common/types/data_chunk.hpp"
19: #include "duckdb/planner/filter/conjunction_filter.hpp"
20: #include "duckdb/planner/filter/constant_filter.hpp"
21: #include "duckdb/planner/filter/null_filter.hpp"
22: #include "duckdb/planner/table_filter.hpp"
23: #endif
24: #include "column_reader.hpp"
25: #include "parquet_file_metadata_cache.hpp"
26: #include "parquet_rle_bp_decoder.hpp"
27: #include "parquet_types.h"
28: #include "resizable_buffer.hpp"
29: 
30: #include <exception>
31: 
32: namespace duckdb_parquet {
33: namespace format {
34: class FileMetaData;
35: }
36: } // namespace duckdb_parquet
37: 
38: namespace duckdb {
39: class Allocator;
40: class ClientContext;
41: class BaseStatistics;
42: class TableFilterSet;
43: class ParquetEncryptionConfig;
44: 
45: struct ParquetReaderPrefetchConfig {
46: 	// Percentage of data in a row group span that should be scanned for enabling whole group prefetch
47: 	static constexpr double WHOLE_GROUP_PREFETCH_MINIMUM_SCAN = 0.95;
48: };
49: 
50: struct ParquetReaderScanState {
51: 	vector<idx_t> group_idx_list;
52: 	int64_t current_group;
53: 	idx_t group_offset;
54: 	unique_ptr<FileHandle> file_handle;
55: 	unique_ptr<ColumnReader> root_reader;
56: 	std::unique_ptr<duckdb_apache::thrift::protocol::TProtocol> thrift_file_proto;
57: 
58: 	bool finished;
59: 	SelectionVector sel;
60: 
61: 	ResizeableBuffer define_buf;
62: 	ResizeableBuffer repeat_buf;
63: 
64: 	bool prefetch_mode = false;
65: 	bool current_group_prefetched = false;
66: };
67: 
68: struct ParquetColumnDefinition {
69: public:
70: 	static ParquetColumnDefinition FromSchemaValue(ClientContext &context, const Value &column_value);
71: 
72: public:
73: 	int32_t field_id;
74: 	string name;
75: 	LogicalType type;
76: 	Value default_value;
77: 
78: public:
79: 	void Serialize(Serializer &serializer) const;
80: 	static ParquetColumnDefinition Deserialize(Deserializer &deserializer);
81: };
82: 
83: struct ParquetOptions {
84: 	explicit ParquetOptions() {
85: 	}
86: 	explicit ParquetOptions(ClientContext &context);
87: 
88: 	bool binary_as_string = false;
89: 	bool file_row_number = false;
90: 	shared_ptr<ParquetEncryptionConfig> encryption_config;
91: 
92: 	MultiFileReaderOptions file_options;
93: 	vector<ParquetColumnDefinition> schema;
94: 
95: public:
96: 	void Serialize(Serializer &serializer) const;
97: 	static ParquetOptions Deserialize(Deserializer &deserializer);
98: };
99: 
100: struct ParquetUnionData {
101: 	~ParquetUnionData();
102: 
103: 	string file_name;
104: 	vector<string> names;
105: 	vector<LogicalType> types;
106: 	ParquetOptions options;
107: 	shared_ptr<ParquetFileMetadataCache> metadata;
108: 	unique_ptr<ParquetReader> reader;
109: 
110: 	const string &GetFileName() {
111: 		return file_name;
112: 	}
113: };
114: 
115: class ParquetReader {
116: public:
117: 	using UNION_READER_DATA = unique_ptr<ParquetUnionData>;
118: 
119: public:
120: 	ParquetReader(ClientContext &context, string file_name, ParquetOptions parquet_options,
121: 	              shared_ptr<ParquetFileMetadataCache> metadata = nullptr);
122: 	~ParquetReader();
123: 
124: 	FileSystem &fs;
125: 	Allocator &allocator;
126: 	string file_name;
127: 	vector<LogicalType> return_types;
128: 	vector<string> names;
129: 	shared_ptr<ParquetFileMetadataCache> metadata;
130: 	ParquetOptions parquet_options;
131: 	MultiFileReaderData reader_data;
132: 	unique_ptr<ColumnReader> root_reader;
133: 
134: 	//! Index of the file_row_number column
135: 	idx_t file_row_number_idx = DConstants::INVALID_INDEX;
136: 	//! Parquet schema for the generated columns
137: 	vector<duckdb_parquet::format::SchemaElement> generated_column_schema;
138: 
139: public:
140: 	void InitializeScan(ClientContext &context, ParquetReaderScanState &state, vector<idx_t> groups_to_read);
141: 	void Scan(ParquetReaderScanState &state, DataChunk &output);
142: 
143: 	static unique_ptr<ParquetUnionData> StoreUnionReader(unique_ptr<ParquetReader> reader_p, idx_t file_idx) {
144: 		auto result = make_uniq<ParquetUnionData>();
145: 		result->file_name = reader_p->file_name;
146: 		if (file_idx == 0) {
147: 			result->names = reader_p->names;
148: 			result->types = reader_p->return_types;
149: 			result->options = reader_p->parquet_options;
150: 			result->metadata = reader_p->metadata;
151: 			result->reader = std::move(reader_p);
152: 		} else {
153: 			result->names = std::move(reader_p->names);
154: 			result->types = std::move(reader_p->return_types);
155: 			result->options = std::move(reader_p->parquet_options);
156: 			result->metadata = std::move(reader_p->metadata);
157: 		}
158: 		return result;
159: 	}
160: 
161: 	idx_t NumRows();
162: 	idx_t NumRowGroups();
163: 
164: 	const duckdb_parquet::format::FileMetaData *GetFileMetadata();
165: 
166: 	uint32_t Read(duckdb_apache::thrift::TBase &object, TProtocol &iprot);
167: 	uint32_t ReadData(duckdb_apache::thrift::protocol::TProtocol &iprot, const data_ptr_t buffer,
168: 	                  const uint32_t buffer_size);
169: 
170: 	unique_ptr<BaseStatistics> ReadStatistics(const string &name);
171: 	static LogicalType DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string);
172: 
173: 	FileHandle &GetHandle() {
174: 		return *file_handle;
175: 	}
176: 
177: 	const string &GetFileName() {
178: 		return file_name;
179: 	}
180: 	const vector<string> &GetNames() {
181: 		return names;
182: 	}
183: 	const vector<LogicalType> &GetTypes() {
184: 		return return_types;
185: 	}
186: 
187: 	static unique_ptr<BaseStatistics> ReadStatistics(ClientContext &context, ParquetOptions parquet_options,
188: 	                                                 shared_ptr<ParquetFileMetadataCache> metadata, const string &name);
189: 
190: private:
191: 	//! Construct a parquet reader but **do not** open a file, used in ReadStatistics only
192: 	ParquetReader(ClientContext &context, ParquetOptions parquet_options,
193: 	              shared_ptr<ParquetFileMetadataCache> metadata);
194: 
195: 	void InitializeSchema(ClientContext &context);
196: 	bool ScanInternal(ParquetReaderScanState &state, DataChunk &output);
197: 	unique_ptr<ColumnReader> CreateReader(ClientContext &context);
198: 
199: 	unique_ptr<ColumnReader> CreateReaderRecursive(ClientContext &context, idx_t depth, idx_t max_define,
200: 	                                               idx_t max_repeat, idx_t &next_schema_idx, idx_t &next_file_idx);
201: 	const duckdb_parquet::format::RowGroup &GetGroup(ParquetReaderScanState &state);
202: 	uint64_t GetGroupCompressedSize(ParquetReaderScanState &state);
203: 	idx_t GetGroupOffset(ParquetReaderScanState &state);
204: 	// Group span is the distance between the min page offset and the max page offset plus the max page compressed size
205: 	uint64_t GetGroupSpan(ParquetReaderScanState &state);
206: 	void PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx);
207: 	LogicalType DeriveLogicalType(const SchemaElement &s_ele);
208: 
209: 	template <typename... Args>
210: 	std::runtime_error FormatException(const string fmt_str, Args... params) {
211: 		return std::runtime_error("Failed to read Parquet file \"" + file_name +
212: 		                          "\": " + StringUtil::Format(fmt_str, params...));
213: 	}
214: 
215: private:
216: 	unique_ptr<FileHandle> file_handle;
217: };
218: 
219: } // namespace duckdb
[end of extension/parquet/include/parquet_reader.hpp]
[start of extension/parquet/parquet_extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include "parquet_extension.hpp"
4: 
5: #include "cast_column_reader.hpp"
6: #include "duckdb.hpp"
7: #include "duckdb/parser/expression/positional_reference_expression.hpp"
8: #include "duckdb/parser/query_node/select_node.hpp"
9: #include "duckdb/parser/tableref/subqueryref.hpp"
10: #include "duckdb/planner/operator/logical_projection.hpp"
11: #include "duckdb/planner/query_node/bound_select_node.hpp"
12: #include "geo_parquet.hpp"
13: #include "parquet_crypto.hpp"
14: #include "parquet_metadata.hpp"
15: #include "parquet_reader.hpp"
16: #include "parquet_writer.hpp"
17: #include "struct_column_reader.hpp"
18: #include "zstd_file_system.hpp"
19: 
20: #include <fstream>
21: #include <iostream>
22: #include <numeric>
23: #include <string>
24: #include <vector>
25: #ifndef DUCKDB_AMALGAMATION
26: #include "duckdb/catalog/catalog.hpp"
27: #include "duckdb/catalog/catalog_entry/table_function_catalog_entry.hpp"
28: #include "duckdb/common/constants.hpp"
29: #include "duckdb/common/enums/file_compression_type.hpp"
30: #include "duckdb/common/file_system.hpp"
31: #include "duckdb/common/helper.hpp"
32: #include "duckdb/common/multi_file_reader.hpp"
33: #include "duckdb/common/serializer/deserializer.hpp"
34: #include "duckdb/common/serializer/serializer.hpp"
35: #include "duckdb/common/type_visitor.hpp"
36: #include "duckdb/function/copy_function.hpp"
37: #include "duckdb/function/pragma_function.hpp"
38: #include "duckdb/function/table_function.hpp"
39: #include "duckdb/main/client_context.hpp"
40: #include "duckdb/main/config.hpp"
41: #include "duckdb/main/extension_util.hpp"
42: #include "duckdb/parser/expression/constant_expression.hpp"
43: #include "duckdb/parser/expression/function_expression.hpp"
44: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
45: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
46: #include "duckdb/parser/tableref/table_function_ref.hpp"
47: #include "duckdb/planner/expression/bound_cast_expression.hpp"
48: #include "duckdb/planner/operator/logical_get.hpp"
49: #include "duckdb/storage/statistics/base_statistics.hpp"
50: #include "duckdb/storage/table/row_group.hpp"
51: #endif
52: 
53: namespace duckdb {
54: 
55: struct ParquetReadBindData : public TableFunctionData {
56: 	shared_ptr<MultiFileList> file_list;
57: 	unique_ptr<MultiFileReader> multi_file_reader;
58: 
59: 	shared_ptr<ParquetReader> initial_reader;
60: 	atomic<idx_t> chunk_count;
61: 	vector<string> names;
62: 	vector<LogicalType> types;
63: 
64: 	// The union readers are created (when parquet union_by_name option is on) during binding
65: 	// Those readers can be re-used during ParquetParallelStateNext
66: 	vector<unique_ptr<ParquetUnionData>> union_readers;
67: 
68: 	// These come from the initial_reader, but need to be stored in case the initial_reader is removed by a filter
69: 	idx_t initial_file_cardinality;
70: 	idx_t initial_file_row_groups;
71: 	ParquetOptions parquet_options;
72: 
73: 	MultiFileReaderBindData reader_bind;
74: 
75: 	void Initialize(shared_ptr<ParquetReader> reader) {
76: 		initial_reader = std::move(reader);
77: 		initial_file_cardinality = initial_reader->NumRows();
78: 		initial_file_row_groups = initial_reader->NumRowGroups();
79: 		parquet_options = initial_reader->parquet_options;
80: 	}
81: 	void Initialize(ClientContext &, unique_ptr<ParquetUnionData> &union_data) {
82: 		Initialize(std::move(union_data->reader));
83: 	}
84: };
85: 
86: struct ParquetReadLocalState : public LocalTableFunctionState {
87: 	shared_ptr<ParquetReader> reader;
88: 	ParquetReaderScanState scan_state;
89: 	bool is_parallel;
90: 	idx_t batch_index;
91: 	idx_t file_index;
92: 	//! The DataChunk containing all read columns (even columns that are immediately removed)
93: 	DataChunk all_columns;
94: };
95: 
96: enum class ParquetFileState : uint8_t { UNOPENED, OPENING, OPEN, CLOSED };
97: 
98: struct ParquetFileReaderData {
99: 	// Create data for an unopened file
100: 	explicit ParquetFileReaderData(const string &file_to_be_opened)
101: 	    : reader(nullptr), file_state(ParquetFileState::UNOPENED), file_mutex(make_uniq<mutex>()),
102: 	      file_to_be_opened(file_to_be_opened) {
103: 	}
104: 	// Create data for an existing reader
105: 	explicit ParquetFileReaderData(shared_ptr<ParquetReader> reader_p)
106: 	    : reader(std::move(reader_p)), file_state(ParquetFileState::OPEN), file_mutex(make_uniq<mutex>()) {
107: 	}
108: 	// Create data for an existing reader
109: 	explicit ParquetFileReaderData(unique_ptr<ParquetUnionData> union_data_p) : file_mutex(make_uniq<mutex>()) {
110: 		if (union_data_p->reader) {
111: 			reader = std::move(union_data_p->reader);
112: 			file_state = ParquetFileState::OPEN;
113: 		} else {
114: 			union_data = std::move(union_data_p);
115: 			file_state = ParquetFileState::UNOPENED;
116: 		}
117: 	}
118: 
119: 	//! Currently opened reader for the file
120: 	shared_ptr<ParquetReader> reader;
121: 	//! Flag to indicate the file is being opened
122: 	ParquetFileState file_state;
123: 	//! Mutexes to wait for the file when it is being opened
124: 	unique_ptr<mutex> file_mutex;
125: 	//! Parquet options for opening the file
126: 	unique_ptr<ParquetUnionData> union_data;
127: 
128: 	//! (only set when file_state is UNOPENED) the file to be opened
129: 	string file_to_be_opened;
130: };
131: 
132: struct ParquetReadGlobalState : public GlobalTableFunctionState {
133: 	//! The scan over the file_list
134: 	MultiFileListScanData file_list_scan;
135: 
136: 	unique_ptr<MultiFileReaderGlobalState> multi_file_reader_state;
137: 
138: 	mutex lock;
139: 
140: 	//! The current set of parquet readers
141: 	vector<unique_ptr<ParquetFileReaderData>> readers;
142: 
143: 	//! Signal to other threads that a file failed to open, letting every thread abort.
144: 	bool error_opening_file = false;
145: 
146: 	//! Index of file currently up for scanning
147: 	atomic<idx_t> file_index;
148: 	//! Index of row group within file currently up for scanning
149: 	idx_t row_group_index;
150: 	//! Batch index of the next row group to be scanned
151: 	idx_t batch_index;
152: 
153: 	idx_t max_threads;
154: 	vector<idx_t> projection_ids;
155: 	vector<LogicalType> scanned_types;
156: 	vector<column_t> column_ids;
157: 	TableFilterSet *filters;
158: 
159: 	idx_t MaxThreads() const override {
160: 		return max_threads;
161: 	}
162: 
163: 	bool CanRemoveColumns() const {
164: 		return !projection_ids.empty();
165: 	}
166: };
167: 
168: struct ParquetWriteBindData : public TableFunctionData {
169: 	vector<LogicalType> sql_types;
170: 	vector<string> column_names;
171: 	duckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
172: 	vector<pair<string, string>> kv_metadata;
173: 	idx_t row_group_size = Storage::ROW_GROUP_SIZE;
174: 
175: 	//! If row_group_size_bytes is not set, we default to row_group_size * BYTES_PER_ROW
176: 	static constexpr const idx_t BYTES_PER_ROW = 1024;
177: 	idx_t row_group_size_bytes;
178: 
179: 	//! How/Whether to encrypt the data
180: 	shared_ptr<ParquetEncryptionConfig> encryption_config;
181: 
182: 	//! Dictionary compression is applied only if the compression ratio exceeds this threshold
183: 	double dictionary_compression_ratio_threshold = 1.0;
184: 
185: 	//! After how many row groups to rotate to a new file
186: 	optional_idx row_groups_per_file;
187: 
188: 	ChildFieldIDs field_ids;
189: 	//! The compression level, higher value is more
190: 	optional_idx compression_level;
191: };
192: 
193: struct ParquetWriteGlobalState : public GlobalFunctionData {
194: 	unique_ptr<ParquetWriter> writer;
195: };
196: 
197: struct ParquetWriteLocalState : public LocalFunctionData {
198: 	explicit ParquetWriteLocalState(ClientContext &context, const vector<LogicalType> &types)
199: 	    : buffer(context, types, ColumnDataAllocatorType::HYBRID) {
200: 		buffer.InitializeAppend(append_state);
201: 	}
202: 
203: 	ColumnDataCollection buffer;
204: 	ColumnDataAppendState append_state;
205: };
206: 
207: BindInfo ParquetGetBindInfo(const optional_ptr<FunctionData> bind_data) {
208: 	auto bind_info = BindInfo(ScanType::PARQUET);
209: 	auto &parquet_bind = bind_data->Cast<ParquetReadBindData>();
210: 
211: 	vector<Value> file_path;
212: 	for (const auto &file : parquet_bind.file_list->Files()) {
213: 		file_path.emplace_back(file);
214: 	}
215: 
216: 	// LCOV_EXCL_START
217: 	bind_info.InsertOption("file_path", Value::LIST(LogicalType::VARCHAR, file_path));
218: 	bind_info.InsertOption("binary_as_string", Value::BOOLEAN(parquet_bind.parquet_options.binary_as_string));
219: 	bind_info.InsertOption("file_row_number", Value::BOOLEAN(parquet_bind.parquet_options.file_row_number));
220: 	parquet_bind.parquet_options.file_options.AddBatchInfo(bind_info);
221: 	// LCOV_EXCL_STOP
222: 	return bind_info;
223: }
224: 
225: static void ParseFileRowNumberOption(MultiFileReaderBindData &bind_data, ParquetOptions &options,
226:                                      vector<LogicalType> &return_types, vector<string> &names) {
227: 	if (options.file_row_number) {
228: 		if (StringUtil::CIFind(names, "file_row_number") != DConstants::INVALID_INDEX) {
229: 			throw BinderException(
230: 			    "Using file_row_number option on file with column named file_row_number is not supported");
231: 		}
232: 
233: 		bind_data.file_row_number_idx = names.size();
234: 		return_types.emplace_back(LogicalType::BIGINT);
235: 		names.emplace_back("file_row_number");
236: 	}
237: }
238: 
239: static MultiFileReaderBindData BindSchema(ClientContext &context, vector<LogicalType> &return_types,
240:                                           vector<string> &names, ParquetReadBindData &result, ParquetOptions &options) {
241: 	D_ASSERT(!options.schema.empty());
242: 
243: 	options.file_options.AutoDetectHivePartitioning(*result.file_list, context);
244: 
245: 	auto &file_options = options.file_options;
246: 	if (file_options.union_by_name || file_options.hive_partitioning) {
247: 		throw BinderException("Parquet schema cannot be combined with union_by_name=true or hive_partitioning=true");
248: 	}
249: 
250: 	vector<string> schema_col_names;
251: 	vector<LogicalType> schema_col_types;
252: 	schema_col_names.reserve(options.schema.size());
253: 	schema_col_types.reserve(options.schema.size());
254: 	for (const auto &column : options.schema) {
255: 		schema_col_names.push_back(column.name);
256: 		schema_col_types.push_back(column.type);
257: 	}
258: 
259: 	// perform the binding on the obtained set of names + types
260: 	MultiFileReaderBindData bind_data;
261: 	result.multi_file_reader->BindOptions(options.file_options, *result.file_list, schema_col_types, schema_col_names,
262: 	                                      bind_data);
263: 
264: 	names = schema_col_names;
265: 	return_types = schema_col_types;
266: 	D_ASSERT(names.size() == return_types.size());
267: 
268: 	ParseFileRowNumberOption(bind_data, options, return_types, names);
269: 
270: 	return bind_data;
271: }
272: 
273: static void InitializeParquetReader(ParquetReader &reader, const ParquetReadBindData &bind_data,
274:                                     const vector<column_t> &global_column_ids,
275:                                     optional_ptr<TableFilterSet> table_filters, ClientContext &context,
276:                                     optional_idx file_idx, optional_ptr<MultiFileReaderGlobalState> reader_state) {
277: 	auto &parquet_options = bind_data.parquet_options;
278: 	auto &reader_data = reader.reader_data;
279: 
280: 	// Mark the file in the file list we are scanning here
281: 	reader_data.file_list_idx = file_idx;
282: 
283: 	if (bind_data.parquet_options.schema.empty()) {
284: 		bind_data.multi_file_reader->InitializeReader(
285: 		    reader, parquet_options.file_options, bind_data.reader_bind, bind_data.types, bind_data.names,
286: 		    global_column_ids, table_filters, bind_data.file_list->GetFirstFile(), context, reader_state);
287: 		return;
288: 	}
289: 
290: 	// a fixed schema was supplied, initialize the MultiFileReader settings here so we can read using the schema
291: 
292: 	// this deals with hive partitioning and filename=true
293: 	bind_data.multi_file_reader->FinalizeBind(parquet_options.file_options, bind_data.reader_bind, reader.GetFileName(),
294: 	                                          reader.GetNames(), bind_data.types, bind_data.names, global_column_ids,
295: 	                                          reader_data, context, reader_state);
296: 
297: 	// create a mapping from field id to column index in file
298: 	unordered_map<uint32_t, idx_t> field_id_to_column_index;
299: 	auto &column_readers = reader.root_reader->Cast<StructColumnReader>().child_readers;
300: 	for (idx_t column_index = 0; column_index < column_readers.size(); column_index++) {
301: 		auto &column_schema = column_readers[column_index]->Schema();
302: 		if (column_schema.__isset.field_id) {
303: 			field_id_to_column_index[column_schema.field_id] = column_index;
304: 		}
305: 	}
306: 
307: 	// loop through the schema definition
308: 	for (idx_t i = 0; i < global_column_ids.size(); i++) {
309: 		auto global_column_index = global_column_ids[i];
310: 
311: 		// check if this is a constant column
312: 		bool constant = false;
313: 		for (auto &entry : reader_data.constant_map) {
314: 			if (entry.column_id == i) {
315: 				constant = true;
316: 				break;
317: 			}
318: 		}
319: 		if (constant) {
320: 			// this column is constant for this file
321: 			continue;
322: 		}
323: 
324: 		// Handle any generate columns that are not in the schema (currently only file_row_number)
325: 		if (global_column_index >= parquet_options.schema.size()) {
326: 			if (bind_data.reader_bind.file_row_number_idx == global_column_index) {
327: 				reader_data.column_mapping.push_back(i);
328: 				reader_data.column_ids.push_back(reader.file_row_number_idx);
329: 			}
330: 			continue;
331: 		}
332: 
333: 		const auto &column_definition = parquet_options.schema[global_column_index];
334: 		auto it = field_id_to_column_index.find(column_definition.field_id);
335: 		if (it == field_id_to_column_index.end()) {
336: 			// field id not present in file, use default value
337: 			reader_data.constant_map.emplace_back(i, column_definition.default_value);
338: 			continue;
339: 		}
340: 
341: 		const auto &local_column_index = it->second;
342: 		auto &column_reader = column_readers[local_column_index];
343: 		if (column_reader->Type() != column_definition.type) {
344: 			// differing types, wrap in a cast column reader
345: 			reader_data.cast_map[local_column_index] = column_definition.type;
346: 		}
347: 
348: 		reader_data.column_mapping.push_back(i);
349: 		reader_data.column_ids.push_back(local_column_index);
350: 	}
351: 	reader_data.empty_columns = reader_data.column_ids.empty();
352: 
353: 	// Finally, initialize the filters
354: 	bind_data.multi_file_reader->CreateFilterMap(bind_data.types, table_filters, reader_data, reader_state);
355: 	reader_data.filters = table_filters;
356: }
357: 
358: static bool GetBooleanArgument(const pair<string, vector<Value>> &option) {
359: 	if (option.second.empty()) {
360: 		return true;
361: 	}
362: 	Value boolean_value;
363: 	string error_message;
364: 	if (!option.second[0].DefaultTryCastAs(LogicalType::BOOLEAN, boolean_value, &error_message)) {
365: 		throw InvalidInputException("Unable to cast \"%s\" to BOOLEAN for Parquet option \"%s\"",
366: 		                            option.second[0].ToString(), option.first);
367: 	}
368: 	return BooleanValue::Get(boolean_value);
369: }
370: 
371: class ParquetScanFunction {
372: public:
373: 	static TableFunctionSet GetFunctionSet() {
374: 		TableFunction table_function("parquet_scan", {LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind,
375: 		                             ParquetScanInitGlobal, ParquetScanInitLocal);
376: 		table_function.statistics = ParquetScanStats;
377: 		table_function.cardinality = ParquetCardinality;
378: 		table_function.table_scan_progress = ParquetProgress;
379: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
380: 		table_function.named_parameters["file_row_number"] = LogicalType::BOOLEAN;
381: 		table_function.named_parameters["compression"] = LogicalType::VARCHAR;
382: 		table_function.named_parameters["schema"] =
383: 		    LogicalType::MAP(LogicalType::INTEGER, LogicalType::STRUCT({{{"name", LogicalType::VARCHAR},
384: 		                                                                 {"type", LogicalType::VARCHAR},
385: 		                                                                 {"default_value", LogicalType::VARCHAR}}}));
386: 		table_function.named_parameters["encryption_config"] = LogicalTypeId::ANY;
387: 		table_function.get_batch_index = ParquetScanGetBatchIndex;
388: 		table_function.serialize = ParquetScanSerialize;
389: 		table_function.deserialize = ParquetScanDeserialize;
390: 		table_function.get_bind_info = ParquetGetBindInfo;
391: 		table_function.projection_pushdown = true;
392: 		table_function.filter_pushdown = true;
393: 		table_function.filter_prune = true;
394: 		table_function.pushdown_complex_filter = ParquetComplexFilterPushdown;
395: 
396: 		MultiFileReader::AddParameters(table_function);
397: 
398: 		return MultiFileReader::CreateFunctionSet(table_function);
399: 	}
400: 
401: 	static unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,
402: 	                                                vector<string> &expected_names,
403: 	                                                vector<LogicalType> &expected_types) {
404: 		D_ASSERT(expected_names.size() == expected_types.size());
405: 		ParquetOptions parquet_options(context);
406: 
407: 		for (auto &option : info.options) {
408: 			auto loption = StringUtil::Lower(option.first);
409: 			if (loption == "compression" || loption == "codec" || loption == "row_group_size") {
410: 				// CODEC/COMPRESSION and ROW_GROUP_SIZE options have no effect on parquet read.
411: 				// These options are determined from the file.
412: 				continue;
413: 			} else if (loption == "binary_as_string") {
414: 				parquet_options.binary_as_string = GetBooleanArgument(option);
415: 			} else if (loption == "file_row_number") {
416: 				parquet_options.file_row_number = GetBooleanArgument(option);
417: 			} else if (loption == "encryption_config") {
418: 				if (option.second.size() != 1) {
419: 					throw BinderException("Parquet encryption_config cannot be empty!");
420: 				}
421: 				parquet_options.encryption_config = ParquetEncryptionConfig::Create(context, option.second[0]);
422: 			} else {
423: 				throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
424: 			}
425: 		}
426: 
427: 		// TODO: Allow overriding the MultiFileReader for COPY FROM?
428: 		auto multi_file_reader = MultiFileReader::CreateDefault("ParquetCopy");
429: 		vector<string> paths = {info.file_path};
430: 		auto file_list = multi_file_reader->CreateFileList(context, paths);
431: 
432: 		return ParquetScanBindInternal(context, std::move(multi_file_reader), std::move(file_list), expected_types,
433: 		                               expected_names, parquet_options);
434: 	}
435: 
436: 	static unique_ptr<BaseStatistics> ParquetScanStats(ClientContext &context, const FunctionData *bind_data_p,
437: 	                                                   column_t column_index) {
438: 		auto &bind_data = bind_data_p->Cast<ParquetReadBindData>();
439: 
440: 		if (IsRowIdColumnId(column_index)) {
441: 			return nullptr;
442: 		}
443: 
444: 		// NOTE: we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
445: 
446: 		auto &config = DBConfig::GetConfig(context);
447: 
448: 		if (bind_data.file_list->GetExpandResult() != FileExpandResult::MULTIPLE_FILES) {
449: 			if (bind_data.initial_reader) {
450: 				// most common path, scanning single parquet file
451: 				return bind_data.initial_reader->ReadStatistics(bind_data.names[column_index]);
452: 			} else if (!config.options.object_cache_enable) {
453: 				// our initial reader was reset
454: 				return nullptr;
455: 			}
456: 		} else if (config.options.object_cache_enable) {
457: 			// multiple files, object cache enabled: merge statistics
458: 			unique_ptr<BaseStatistics> overall_stats;
459: 
460: 			auto &cache = ObjectCache::GetObjectCache(context);
461: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
462: 			// enabled at all)
463: 			FileSystem &fs = FileSystem::GetFileSystem(context);
464: 
465: 			for (const auto &file_name : bind_data.file_list->Files()) {
466: 				auto metadata = cache.Get<ParquetFileMetadataCache>(file_name);
467: 				if (!metadata) {
468: 					// missing metadata entry in cache, no usable stats
469: 					return nullptr;
470: 				}
471: 				if (!fs.IsRemoteFile(file_name)) {
472: 					auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
473: 					// we need to check if the metadata cache entries are current
474: 					if (fs.GetLastModifiedTime(*handle) >= metadata->read_time) {
475: 						// missing or invalid metadata entry in cache, no usable stats overall
476: 						return nullptr;
477: 					}
478: 				} else {
479: 					// for remote files we just avoid reading stats entirely
480: 					return nullptr;
481: 				}
482: 				// get and merge stats for file
483: 				auto file_stats = ParquetReader::ReadStatistics(context, bind_data.parquet_options, metadata,
484: 				                                                bind_data.names[column_index]);
485: 				if (!file_stats) {
486: 					return nullptr;
487: 				}
488: 				if (overall_stats) {
489: 					overall_stats->Merge(*file_stats);
490: 				} else {
491: 					overall_stats = std::move(file_stats);
492: 				}
493: 			}
494: 			// success!
495: 			return overall_stats;
496: 		}
497: 
498: 		// multiple files and no object cache, no luck!
499: 		return nullptr;
500: 	}
501: 
502: 	static unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context,
503: 	                                                        unique_ptr<MultiFileReader> multi_file_reader,
504: 	                                                        unique_ptr<MultiFileList> file_list,
505: 	                                                        vector<LogicalType> &return_types, vector<string> &names,
506: 	                                                        ParquetOptions parquet_options) {
507: 		auto result = make_uniq<ParquetReadBindData>();
508: 		result->multi_file_reader = std::move(multi_file_reader);
509: 		result->file_list = std::move(file_list);
510: 
511: 		bool bound_on_first_file = true;
512: 		if (result->multi_file_reader->Bind(parquet_options.file_options, *result->file_list, result->types,
513: 		                                    result->names, result->reader_bind)) {
514: 			result->multi_file_reader->BindOptions(parquet_options.file_options, *result->file_list, result->types,
515: 			                                       result->names, result->reader_bind);
516: 			// Enable the parquet file_row_number on the parquet options if the file_row_number_idx was set
517: 			if (result->reader_bind.file_row_number_idx != DConstants::INVALID_INDEX) {
518: 				parquet_options.file_row_number = true;
519: 			}
520: 			bound_on_first_file = false;
521: 		} else if (!parquet_options.schema.empty()) {
522: 			// A schema was supplied: use the schema for binding
523: 			result->reader_bind = BindSchema(context, result->types, result->names, *result, parquet_options);
524: 		} else {
525: 			parquet_options.file_options.AutoDetectHivePartitioning(*result->file_list, context);
526: 			// Default bind
527: 			result->reader_bind = result->multi_file_reader->BindReader<ParquetReader>(
528: 			    context, result->types, result->names, *result->file_list, *result, parquet_options);
529: 		}
530: 
531: 		if (return_types.empty()) {
532: 			// no expected types - just copy the types
533: 			return_types = result->types;
534: 			names = result->names;
535: 		} else {
536: 			if (return_types.size() != result->types.size()) {
537: 				auto file_string = bound_on_first_file ? result->file_list->GetFirstFile()
538: 				                                       : StringUtil::Join(result->file_list->GetPaths(), ",");
539: 				throw std::runtime_error(StringUtil::Format(
540: 				    "Failed to read file(s) \"%s\" - column count mismatch: expected %d columns but found %d",
541: 				    file_string, return_types.size(), result->types.size()));
542: 			}
543: 			// expected types - overwrite the types we want to read instead
544: 			result->types = return_types;
545: 		}
546: 		result->parquet_options = parquet_options;
547: 		return std::move(result);
548: 	}
549: 
550: 	static unique_ptr<FunctionData> ParquetScanBind(ClientContext &context, TableFunctionBindInput &input,
551: 	                                                vector<LogicalType> &return_types, vector<string> &names) {
552: 		auto multi_file_reader = MultiFileReader::Create(input.table_function);
553: 
554: 		ParquetOptions parquet_options(context);
555: 		for (auto &kv : input.named_parameters) {
556: 			auto loption = StringUtil::Lower(kv.first);
557: 			if (multi_file_reader->ParseOption(kv.first, kv.second, parquet_options.file_options, context)) {
558: 				continue;
559: 			}
560: 			if (loption == "binary_as_string") {
561: 				parquet_options.binary_as_string = BooleanValue::Get(kv.second);
562: 			} else if (loption == "file_row_number") {
563: 				parquet_options.file_row_number = BooleanValue::Get(kv.second);
564: 			} else if (loption == "schema") {
565: 				// Argument is a map that defines the schema
566: 				const auto &schema_value = kv.second;
567: 				const auto column_values = ListValue::GetChildren(schema_value);
568: 				if (column_values.empty()) {
569: 					throw BinderException("Parquet schema cannot be empty");
570: 				}
571: 				parquet_options.schema.reserve(column_values.size());
572: 				for (idx_t i = 0; i < column_values.size(); i++) {
573: 					parquet_options.schema.emplace_back(
574: 					    ParquetColumnDefinition::FromSchemaValue(context, column_values[i]));
575: 				}
576: 
577: 				// cannot be combined with hive_partitioning=true, so we disable auto-detection
578: 				parquet_options.file_options.auto_detect_hive_partitioning = false;
579: 			} else if (loption == "encryption_config") {
580: 				parquet_options.encryption_config = ParquetEncryptionConfig::Create(context, kv.second);
581: 			}
582: 		}
583: 
584: 		auto file_list = multi_file_reader->CreateFileList(context, input.inputs[0]);
585: 		return ParquetScanBindInternal(context, std::move(multi_file_reader), std::move(file_list), return_types, names,
586: 		                               parquet_options);
587: 	}
588: 
589: 	static double ParquetProgress(ClientContext &context, const FunctionData *bind_data_p,
590: 	                              const GlobalTableFunctionState *global_state) {
591: 		auto &bind_data = bind_data_p->Cast<ParquetReadBindData>();
592: 		auto &gstate = global_state->Cast<ParquetReadGlobalState>();
593: 
594: 		auto total_count = bind_data.file_list->GetTotalFileCount();
595: 		if (total_count == 0) {
596: 			return 100.0;
597: 		}
598: 		if (bind_data.initial_file_cardinality == 0) {
599: 			return (100.0 * (gstate.file_index + 1)) / total_count;
600: 		}
601: 		auto percentage = MinValue<double>(
602: 		    100.0, (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100.0 / bind_data.initial_file_cardinality));
603: 		return (percentage + 100.0 * gstate.file_index) / total_count;
604: 	}
605: 
606: 	static unique_ptr<LocalTableFunctionState>
607: 	ParquetScanInitLocal(ExecutionContext &context, TableFunctionInitInput &input, GlobalTableFunctionState *gstate_p) {
608: 		auto &bind_data = input.bind_data->Cast<ParquetReadBindData>();
609: 		auto &gstate = gstate_p->Cast<ParquetReadGlobalState>();
610: 
611: 		auto result = make_uniq<ParquetReadLocalState>();
612: 		result->is_parallel = true;
613: 		result->batch_index = 0;
614: 
615: 		if (gstate.CanRemoveColumns()) {
616: 			result->all_columns.Initialize(context.client, gstate.scanned_types);
617: 		}
618: 		if (!ParquetParallelStateNext(context.client, bind_data, *result, gstate)) {
619: 			return nullptr;
620: 		}
621: 		return std::move(result);
622: 	}
623: 
624: 	static unique_ptr<GlobalTableFunctionState> ParquetScanInitGlobal(ClientContext &context,
625: 	                                                                  TableFunctionInitInput &input) {
626: 		auto &bind_data = input.bind_data->CastNoConst<ParquetReadBindData>();
627: 		auto result = make_uniq<ParquetReadGlobalState>();
628: 		bind_data.file_list->InitializeScan(result->file_list_scan);
629: 
630: 		result->multi_file_reader_state = bind_data.multi_file_reader->InitializeGlobalState(
631: 		    context, bind_data.parquet_options.file_options, bind_data.reader_bind, *bind_data.file_list,
632: 		    bind_data.types, bind_data.names, input.column_ids);
633: 		if (bind_data.file_list->IsEmpty()) {
634: 			result->readers = {};
635: 		} else if (!bind_data.union_readers.empty()) {
636: 			// TODO: confirm we are not changing behaviour by modifying the order here?
637: 			for (auto &reader : bind_data.union_readers) {
638: 				if (!reader) {
639: 					break;
640: 				}
641: 				result->readers.push_back(make_uniq<ParquetFileReaderData>(std::move(reader)));
642: 			}
643: 			if (result->readers.size() != bind_data.file_list->GetTotalFileCount()) {
644: 				// This case happens with recursive CTEs: the first execution the readers have already
645: 				// been moved out of the bind data.
646: 				// FIXME: clean up this process and make it more explicit
647: 				result->readers = {};
648: 			}
649: 		} else if (bind_data.initial_reader) {
650: 			// Ensure the initial reader was actually constructed from the first file
651: 			if (bind_data.initial_reader->file_name != bind_data.file_list->GetFirstFile()) {
652: 				throw InternalException("First file from list ('%s') does not match first reader ('%s')",
653: 				                        bind_data.initial_reader->file_name, bind_data.file_list->GetFirstFile());
654: 			}
655: 			result->readers.push_back(make_uniq<ParquetFileReaderData>(std::move(bind_data.initial_reader)));
656: 		}
657: 
658: 		// Ensure all readers are initialized and FileListScan is sync with readers list
659: 		for (auto &reader_data : result->readers) {
660: 			string file_name;
661: 			idx_t file_idx = result->file_list_scan.current_file_idx;
662: 			bind_data.file_list->Scan(result->file_list_scan, file_name);
663: 			if (reader_data->union_data) {
664: 				if (file_name != reader_data->union_data->GetFileName()) {
665: 					throw InternalException("Mismatch in filename order and union reader order in parquet scan");
666: 				}
667: 			} else {
668: 				D_ASSERT(reader_data->reader);
669: 				if (file_name != reader_data->reader->file_name) {
670: 					throw InternalException("Mismatch in filename order and reader order in parquet scan");
671: 				}
672: 				InitializeParquetReader(*reader_data->reader, bind_data, input.column_ids, input.filters, context,
673: 				                        file_idx, result->multi_file_reader_state);
674: 			}
675: 		}
676: 
677: 		result->column_ids = input.column_ids;
678: 		result->filters = input.filters.get();
679: 		result->row_group_index = 0;
680: 		result->file_index = 0;
681: 		result->batch_index = 0;
682: 		result->max_threads = ParquetScanMaxThreads(context, input.bind_data.get());
683: 
684: 		bool require_extra_columns =
685: 		    result->multi_file_reader_state && result->multi_file_reader_state->RequiresExtraColumns();
686: 		if (input.CanRemoveFilterColumns() || require_extra_columns) {
687: 			if (!input.projection_ids.empty()) {
688: 				result->projection_ids = input.projection_ids;
689: 			} else {
690: 				result->projection_ids.resize(input.column_ids.size());
691: 				iota(begin(result->projection_ids), end(result->projection_ids), 0);
692: 			}
693: 
694: 			const auto table_types = bind_data.types;
695: 			for (const auto &col_idx : input.column_ids) {
696: 				if (IsRowIdColumnId(col_idx)) {
697: 					result->scanned_types.emplace_back(LogicalType::ROW_TYPE);
698: 				} else {
699: 					result->scanned_types.push_back(table_types[col_idx]);
700: 				}
701: 			}
702: 		}
703: 
704: 		if (require_extra_columns) {
705: 			for (const auto &column_type : result->multi_file_reader_state->extra_columns) {
706: 				result->scanned_types.push_back(column_type);
707: 			}
708: 		}
709: 
710: 		return std::move(result);
711: 	}
712: 
713: 	static idx_t ParquetScanGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,
714: 	                                      LocalTableFunctionState *local_state,
715: 	                                      GlobalTableFunctionState *global_state) {
716: 		auto &data = local_state->Cast<ParquetReadLocalState>();
717: 		return data.batch_index;
718: 	}
719: 
720: 	static void ParquetScanSerialize(Serializer &serializer, const optional_ptr<FunctionData> bind_data_p,
721: 	                                 const TableFunction &function) {
722: 		auto &bind_data = bind_data_p->Cast<ParquetReadBindData>();
723: 
724: 		serializer.WriteProperty(100, "files", bind_data.file_list->GetAllFiles());
725: 		serializer.WriteProperty(101, "types", bind_data.types);
726: 		serializer.WriteProperty(102, "names", bind_data.names);
727: 		serializer.WriteProperty(103, "parquet_options", bind_data.parquet_options);
728: 	}
729: 
730: 	static unique_ptr<FunctionData> ParquetScanDeserialize(Deserializer &deserializer, TableFunction &function) {
731: 		auto &context = deserializer.Get<ClientContext &>();
732: 		auto files = deserializer.ReadProperty<vector<string>>(100, "files");
733: 		auto types = deserializer.ReadProperty<vector<LogicalType>>(101, "types");
734: 		auto names = deserializer.ReadProperty<vector<string>>(102, "names");
735: 		auto parquet_options = deserializer.ReadProperty<ParquetOptions>(103, "parquet_options");
736: 
737: 		vector<Value> file_path;
738: 		for (auto &path : files) {
739: 			file_path.emplace_back(path);
740: 		}
741: 
742: 		auto multi_file_reader = MultiFileReader::Create(function);
743: 		auto file_list = multi_file_reader->CreateFileList(context, Value::LIST(LogicalType::VARCHAR, file_path),
744: 		                                                   FileGlobOptions::DISALLOW_EMPTY);
745: 		return ParquetScanBindInternal(context, std::move(multi_file_reader), std::move(file_list), types, names,
746: 		                               parquet_options);
747: 	}
748: 
749: 	static void ParquetScanImplementation(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {
750: 		if (!data_p.local_state) {
751: 			return;
752: 		}
753: 		auto &data = data_p.local_state->Cast<ParquetReadLocalState>();
754: 		auto &gstate = data_p.global_state->Cast<ParquetReadGlobalState>();
755: 		auto &bind_data = data_p.bind_data->CastNoConst<ParquetReadBindData>();
756: 
757: 		do {
758: 			if (gstate.CanRemoveColumns()) {
759: 				data.all_columns.Reset();
760: 				data.reader->Scan(data.scan_state, data.all_columns);
761: 				bind_data.multi_file_reader->FinalizeChunk(context, bind_data.reader_bind, data.reader->reader_data,
762: 				                                           data.all_columns, gstate.multi_file_reader_state);
763: 				output.ReferenceColumns(data.all_columns, gstate.projection_ids);
764: 			} else {
765: 				data.reader->Scan(data.scan_state, output);
766: 				bind_data.multi_file_reader->FinalizeChunk(context, bind_data.reader_bind, data.reader->reader_data,
767: 				                                           output, gstate.multi_file_reader_state);
768: 			}
769: 
770: 			bind_data.chunk_count++;
771: 			if (output.size() > 0) {
772: 				return;
773: 			}
774: 			if (!ParquetParallelStateNext(context, bind_data, data, gstate)) {
775: 				return;
776: 			}
777: 		} while (true);
778: 	}
779: 
780: 	static unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {
781: 		auto &data = bind_data->Cast<ParquetReadBindData>();
782: 
783: 		auto file_list_cardinality_estimate = data.file_list->GetCardinality(context);
784: 		if (file_list_cardinality_estimate) {
785: 			return file_list_cardinality_estimate;
786: 		}
787: 
788: 		return make_uniq<NodeStatistics>(data.initial_file_cardinality * data.file_list->GetTotalFileCount());
789: 	}
790: 
791: 	static idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {
792: 		auto &data = bind_data->Cast<ParquetReadBindData>();
793: 
794: 		if (data.file_list->GetExpandResult() == FileExpandResult::MULTIPLE_FILES) {
795: 			return TaskScheduler::GetScheduler(context).NumberOfThreads();
796: 		}
797: 
798: 		return MaxValue(data.initial_file_row_groups, (idx_t)1);
799: 	}
800: 
801: 	// Queries the metadataprovider for another file to scan, updating the files/reader lists in the process.
802: 	// Returns true if resized
803: 	static bool ResizeFiles(const ParquetReadBindData &bind_data, ParquetReadGlobalState &parallel_state) {
804: 		string scanned_file;
805: 		if (!bind_data.file_list->Scan(parallel_state.file_list_scan, scanned_file)) {
806: 			return false;
807: 		}
808: 
809: 		// Push the file in the reader data, to be opened later
810: 		parallel_state.readers.push_back(make_uniq<ParquetFileReaderData>(scanned_file));
811: 
812: 		return true;
813: 	}
814: 
815: 	// This function looks for the next available row group. If not available, it will open files from bind_data.files
816: 	// until there is a row group available for scanning or the files runs out
817: 	static bool ParquetParallelStateNext(ClientContext &context, const ParquetReadBindData &bind_data,
818: 	                                     ParquetReadLocalState &scan_data, ParquetReadGlobalState &parallel_state) {
819: 		unique_lock<mutex> parallel_lock(parallel_state.lock);
820: 
821: 		while (true) {
822: 			if (parallel_state.error_opening_file) {
823: 				return false;
824: 			}
825: 
826: 			if (parallel_state.file_index >= parallel_state.readers.size() && !ResizeFiles(bind_data, parallel_state)) {
827: 				return false;
828: 			}
829: 
830: 			auto &current_reader_data = *parallel_state.readers[parallel_state.file_index];
831: 			if (current_reader_data.file_state == ParquetFileState::OPEN) {
832: 				if (parallel_state.row_group_index < current_reader_data.reader->NumRowGroups()) {
833: 					// The current reader has rowgroups left to be scanned
834: 					scan_data.reader = current_reader_data.reader;
835: 					vector<idx_t> group_indexes {parallel_state.row_group_index};
836: 					scan_data.reader->InitializeScan(context, scan_data.scan_state, group_indexes);
837: 					scan_data.batch_index = parallel_state.batch_index++;
838: 					scan_data.file_index = parallel_state.file_index;
839: 					parallel_state.row_group_index++;
840: 					return true;
841: 				} else {
842: 					// Close current file
843: 					current_reader_data.file_state = ParquetFileState::CLOSED;
844: 					current_reader_data.reader = nullptr;
845: 
846: 					// Set state to the next file
847: 					parallel_state.file_index++;
848: 					parallel_state.row_group_index = 0;
849: 
850: 					continue;
851: 				}
852: 			}
853: 
854: 			if (TryOpenNextFile(context, bind_data, scan_data, parallel_state, parallel_lock)) {
855: 				continue;
856: 			}
857: 
858: 			// Check if the current file is being opened, in that case we need to wait for it.
859: 			if (current_reader_data.file_state == ParquetFileState::OPENING) {
860: 				WaitForFile(parallel_state.file_index, parallel_state, parallel_lock);
861: 			}
862: 		}
863: 	}
864: 
865: 	static void ParquetComplexFilterPushdown(ClientContext &context, LogicalGet &get, FunctionData *bind_data_p,
866: 	                                         vector<unique_ptr<Expression>> &filters) {
867: 		auto &data = bind_data_p->Cast<ParquetReadBindData>();
868: 
869: 		auto new_list = data.multi_file_reader->ComplexFilterPushdown(context, *data.file_list,
870: 		                                                              data.parquet_options.file_options, get, filters);
871: 
872: 		if (new_list) {
873: 			data.file_list = std::move(new_list);
874: 			MultiFileReader::PruneReaders(data, *data.file_list);
875: 		}
876: 	}
877: 
878: 	//! Wait for a file to become available. Parallel lock should be locked when calling.
879: 	static void WaitForFile(idx_t file_index, ParquetReadGlobalState &parallel_state,
880: 	                        unique_lock<mutex> &parallel_lock) {
881: 		while (true) {
882: 			// Get pointer to file mutex before unlocking
883: 			auto &file_mutex = *parallel_state.readers[file_index]->file_mutex;
884: 
885: 			// To get the file lock, we first need to release the parallel_lock to prevent deadlocking. Note that this
886: 			// requires getting the ref to the file mutex pointer with the lock stil held: readers get be resized
887: 			parallel_lock.unlock();
888: 			unique_lock<mutex> current_file_lock(file_mutex);
889: 			parallel_lock.lock();
890: 
891: 			// Here we have both locks which means we can stop waiting if:
892: 			// - the thread opening the file is done and the file is available
893: 			// - the thread opening the file has failed
894: 			// - the file was somehow scanned till the end while we were waiting
895: 			if (parallel_state.file_index >= parallel_state.readers.size() ||
896: 			    parallel_state.readers[parallel_state.file_index]->file_state != ParquetFileState::OPENING ||
897: 			    parallel_state.error_opening_file) {
898: 				return;
899: 			}
900: 		}
901: 	}
902: 
903: 	//! Helper function that try to start opening a next file. Parallel lock should be locked when calling.
904: 	static bool TryOpenNextFile(ClientContext &context, const ParquetReadBindData &bind_data,
905: 	                            ParquetReadLocalState &scan_data, ParquetReadGlobalState &parallel_state,
906: 	                            unique_lock<mutex> &parallel_lock) {
907: 		const auto file_index_limit =
908: 		    parallel_state.file_index + TaskScheduler::GetScheduler(context).NumberOfThreads();
909: 
910: 		for (idx_t i = parallel_state.file_index; i < file_index_limit; i++) {
911: 			// We check if we can resize files in this loop too otherwise we will only ever open 1 file ahead
912: 			if (i >= parallel_state.readers.size() && !ResizeFiles(bind_data, parallel_state)) {
913: 				return false;
914: 			}
915: 
916: 			auto &current_reader_data = *parallel_state.readers[i];
917: 			if (current_reader_data.file_state == ParquetFileState::UNOPENED) {
918: 				current_reader_data.file_state = ParquetFileState::OPENING;
919: 				auto pq_options = bind_data.parquet_options;
920: 
921: 				// Get pointer to file mutex before unlocking
922: 				auto &current_file_lock = *current_reader_data.file_mutex;
923: 
924: 				// Now we switch which lock we are holding, instead of locking the global state, we grab the lock on
925: 				// the file we are opening. This file lock allows threads to wait for a file to be opened.
926: 				parallel_lock.unlock();
927: 				unique_lock<mutex> file_lock(current_file_lock);
928: 
929: 				shared_ptr<ParquetReader> reader;
930: 				try {
931: 					if (current_reader_data.union_data) {
932: 						auto &union_data = *current_reader_data.union_data;
933: 						reader = make_shared_ptr<ParquetReader>(context, union_data.file_name, union_data.options,
934: 						                                        union_data.metadata);
935: 					} else {
936: 						reader =
937: 						    make_shared_ptr<ParquetReader>(context, current_reader_data.file_to_be_opened, pq_options);
938: 					}
939: 					InitializeParquetReader(*reader, bind_data, parallel_state.column_ids, parallel_state.filters,
940: 					                        context, i, parallel_state.multi_file_reader_state);
941: 				} catch (...) {
942: 					parallel_lock.lock();
943: 					parallel_state.error_opening_file = true;
944: 					throw;
945: 				}
946: 
947: 				// Now re-lock the state and add the reader
948: 				parallel_lock.lock();
949: 				current_reader_data.reader = std::move(reader);
950: 				current_reader_data.file_state = ParquetFileState::OPEN;
951: 
952: 				return true;
953: 			}
954: 		}
955: 
956: 		return false;
957: 	}
958: };
959: 
960: static case_insensitive_map_t<LogicalType> GetChildNameToTypeMap(const LogicalType &type) {
961: 	case_insensitive_map_t<LogicalType> name_to_type_map;
962: 	switch (type.id()) {
963: 	case LogicalTypeId::LIST:
964: 		name_to_type_map.emplace("element", ListType::GetChildType(type));
965: 		break;
966: 	case LogicalTypeId::MAP:
967: 		name_to_type_map.emplace("key", MapType::KeyType(type));
968: 		name_to_type_map.emplace("value", MapType::ValueType(type));
969: 		break;
970: 	case LogicalTypeId::STRUCT:
971: 		for (auto &child_type : StructType::GetChildTypes(type)) {
972: 			if (child_type.first == FieldID::DUCKDB_FIELD_ID) {
973: 				throw BinderException("Cannot have column named \"%s\" with FIELD_IDS", FieldID::DUCKDB_FIELD_ID);
974: 			}
975: 			name_to_type_map.emplace(child_type);
976: 		}
977: 		break;
978: 	default: // LCOV_EXCL_START
979: 		throw InternalException("Unexpected type in GetChildNameToTypeMap");
980: 	} // LCOV_EXCL_STOP
981: 	return name_to_type_map;
982: }
983: 
984: static void GetChildNamesAndTypes(const LogicalType &type, vector<string> &child_names,
985:                                   vector<LogicalType> &child_types) {
986: 	switch (type.id()) {
987: 	case LogicalTypeId::LIST:
988: 		child_names.emplace_back("element");
989: 		child_types.emplace_back(ListType::GetChildType(type));
990: 		break;
991: 	case LogicalTypeId::MAP:
992: 		child_names.emplace_back("key");
993: 		child_names.emplace_back("value");
994: 		child_types.emplace_back(MapType::KeyType(type));
995: 		child_types.emplace_back(MapType::ValueType(type));
996: 		break;
997: 	case LogicalTypeId::STRUCT:
998: 		for (auto &child_type : StructType::GetChildTypes(type)) {
999: 			child_names.emplace_back(child_type.first);
1000: 			child_types.emplace_back(child_type.second);
1001: 		}
1002: 		break;
1003: 	default: // LCOV_EXCL_START
1004: 		throw InternalException("Unexpected type in GetChildNamesAndTypes");
1005: 	} // LCOV_EXCL_STOP
1006: }
1007: 
1008: static void GenerateFieldIDs(ChildFieldIDs &field_ids, idx_t &field_id, const vector<string> &names,
1009:                              const vector<LogicalType> &sql_types) {
1010: 	D_ASSERT(names.size() == sql_types.size());
1011: 	for (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {
1012: 		const auto &col_name = names[col_idx];
1013: 		auto inserted = field_ids.ids->insert(make_pair(col_name, FieldID(field_id++)));
1014: 		D_ASSERT(inserted.second);
1015: 
1016: 		const auto &col_type = sql_types[col_idx];
1017: 		if (col_type.id() != LogicalTypeId::LIST && col_type.id() != LogicalTypeId::MAP &&
1018: 		    col_type.id() != LogicalTypeId::STRUCT) {
1019: 			continue;
1020: 		}
1021: 
1022: 		// Cannot use GetChildNameToTypeMap here because we lose order, and we want to generate depth-first
1023: 		vector<string> child_names;
1024: 		vector<LogicalType> child_types;
1025: 		GetChildNamesAndTypes(col_type, child_names, child_types);
1026: 
1027: 		GenerateFieldIDs(inserted.first->second.child_field_ids, field_id, child_names, child_types);
1028: 	}
1029: }
1030: 
1031: static void GetFieldIDs(const Value &field_ids_value, ChildFieldIDs &field_ids,
1032:                         unordered_set<uint32_t> &unique_field_ids,
1033:                         const case_insensitive_map_t<LogicalType> &name_to_type_map) {
1034: 	const auto &struct_type = field_ids_value.type();
1035: 	if (struct_type.id() != LogicalTypeId::STRUCT) {
1036: 		throw BinderException(
1037: 		    "Expected FIELD_IDS to be a STRUCT, e.g., {col1: 42, col2: {%s: 43, nested_col: 44}, col3: 44}",
1038: 		    FieldID::DUCKDB_FIELD_ID);
1039: 	}
1040: 	const auto &struct_children = StructValue::GetChildren(field_ids_value);
1041: 	D_ASSERT(StructType::GetChildTypes(struct_type).size() == struct_children.size());
1042: 	for (idx_t i = 0; i < struct_children.size(); i++) {
1043: 		const auto &col_name = StringUtil::Lower(StructType::GetChildName(struct_type, i));
1044: 		if (col_name == FieldID::DUCKDB_FIELD_ID) {
1045: 			continue;
1046: 		}
1047: 
1048: 		auto it = name_to_type_map.find(col_name);
1049: 		if (it == name_to_type_map.end()) {
1050: 			string names;
1051: 			for (const auto &name : name_to_type_map) {
1052: 				if (!names.empty()) {
1053: 					names += ", ";
1054: 				}
1055: 				names += name.first;
1056: 			}
1057: 			throw BinderException("Column name \"%s\" specified in FIELD_IDS not found. Available column names: [%s]",
1058: 			                      col_name, names);
1059: 		}
1060: 		D_ASSERT(field_ids.ids->find(col_name) == field_ids.ids->end()); // Caught by STRUCT - deduplicates keys
1061: 
1062: 		const auto &child_value = struct_children[i];
1063: 		const auto &child_type = child_value.type();
1064: 		optional_ptr<const Value> field_id_value;
1065: 		optional_ptr<const Value> child_field_ids_value;
1066: 
1067: 		if (child_type.id() == LogicalTypeId::STRUCT) {
1068: 			const auto &nested_children = StructValue::GetChildren(child_value);
1069: 			D_ASSERT(StructType::GetChildTypes(child_type).size() == nested_children.size());
1070: 			for (idx_t nested_i = 0; nested_i < nested_children.size(); nested_i++) {
1071: 				const auto &field_id_or_nested_col = StructType::GetChildName(child_type, nested_i);
1072: 				if (field_id_or_nested_col == FieldID::DUCKDB_FIELD_ID) {
1073: 					field_id_value = &nested_children[nested_i];
1074: 				} else {
1075: 					child_field_ids_value = &child_value;
1076: 				}
1077: 			}
1078: 		} else {
1079: 			field_id_value = &child_value;
1080: 		}
1081: 
1082: 		FieldID field_id;
1083: 		if (field_id_value) {
1084: 			Value field_id_integer_value = field_id_value->DefaultCastAs(LogicalType::INTEGER);
1085: 			const uint32_t field_id_int = IntegerValue::Get(field_id_integer_value);
1086: 			if (!unique_field_ids.insert(field_id_int).second) {
1087: 				throw BinderException("Duplicate field_id %s found in FIELD_IDS", field_id_integer_value.ToString());
1088: 			}
1089: 			field_id = FieldID(field_id_int);
1090: 		}
1091: 		auto inserted = field_ids.ids->insert(make_pair(col_name, std::move(field_id)));
1092: 		D_ASSERT(inserted.second);
1093: 
1094: 		if (child_field_ids_value) {
1095: 			const auto &col_type = it->second;
1096: 			if (col_type.id() != LogicalTypeId::LIST && col_type.id() != LogicalTypeId::MAP &&
1097: 			    col_type.id() != LogicalTypeId::STRUCT) {
1098: 				throw BinderException("Column \"%s\" with type \"%s\" cannot have a nested FIELD_IDS specification",
1099: 				                      col_name, LogicalTypeIdToString(col_type.id()));
1100: 			}
1101: 
1102: 			GetFieldIDs(*child_field_ids_value, inserted.first->second.child_field_ids, unique_field_ids,
1103: 			            GetChildNameToTypeMap(col_type));
1104: 		}
1105: 	}
1106: }
1107: 
1108: unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyFunctionBindInput &input,
1109:                                           const vector<string> &names, const vector<LogicalType> &sql_types) {
1110: 	D_ASSERT(names.size() == sql_types.size());
1111: 	bool row_group_size_bytes_set = false;
1112: 	auto bind_data = make_uniq<ParquetWriteBindData>();
1113: 	for (auto &option : input.info.options) {
1114: 		const auto loption = StringUtil::Lower(option.first);
1115: 		if (option.second.size() != 1) {
1116: 			// All parquet write options require exactly one argument
1117: 			throw BinderException("%s requires exactly one argument", StringUtil::Upper(loption));
1118: 		}
1119: 		if (loption == "row_group_size" || loption == "chunk_size") {
1120: 			bind_data->row_group_size = option.second[0].GetValue<uint64_t>();
1121: 		} else if (loption == "row_group_size_bytes") {
1122: 			auto roption = option.second[0];
1123: 			if (roption.GetTypeMutable().id() == LogicalTypeId::VARCHAR) {
1124: 				bind_data->row_group_size_bytes = DBConfig::ParseMemoryLimit(roption.ToString());
1125: 			} else {
1126: 				bind_data->row_group_size_bytes = option.second[0].GetValue<uint64_t>();
1127: 			}
1128: 			row_group_size_bytes_set = true;
1129: 		} else if (loption == "row_groups_per_file") {
1130: 			bind_data->row_groups_per_file = option.second[0].GetValue<uint64_t>();
1131: 		} else if (loption == "compression" || loption == "codec") {
1132: 			const auto roption = StringUtil::Lower(option.second[0].ToString());
1133: 			if (roption == "uncompressed") {
1134: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::UNCOMPRESSED;
1135: 			} else if (roption == "snappy") {
1136: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
1137: 			} else if (roption == "gzip") {
1138: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::GZIP;
1139: 			} else if (roption == "zstd") {
1140: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::ZSTD;
1141: 			} else if (roption == "brotli") {
1142: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::BROTLI;
1143: 			} else if (roption == "lz4" || roption == "lz4_raw") {
1144: 				/* LZ4 is technically another compression scheme, but deprecated and arrow also uses them
1145: 				 * interchangeably */
1146: 				bind_data->codec = duckdb_parquet::format::CompressionCodec::LZ4_RAW;
1147: 			} else {
1148: 				throw BinderException("Expected %s argument to be either [uncompressed, brotli, gzip, snappy, or zstd]",
1149: 				                      loption);
1150: 			}
1151: 		} else if (loption == "field_ids") {
1152: 			if (option.second[0].type().id() == LogicalTypeId::VARCHAR &&
1153: 			    StringUtil::Lower(StringValue::Get(option.second[0])) == "auto") {
1154: 				idx_t field_id = 0;
1155: 				GenerateFieldIDs(bind_data->field_ids, field_id, names, sql_types);
1156: 			} else {
1157: 				unordered_set<uint32_t> unique_field_ids;
1158: 				case_insensitive_map_t<LogicalType> name_to_type_map;
1159: 				for (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {
1160: 					if (names[col_idx] == FieldID::DUCKDB_FIELD_ID) {
1161: 						throw BinderException("Cannot have a column named \"%s\" when writing FIELD_IDS",
1162: 						                      FieldID::DUCKDB_FIELD_ID);
1163: 					}
1164: 					name_to_type_map.emplace(names[col_idx], sql_types[col_idx]);
1165: 				}
1166: 				GetFieldIDs(option.second[0], bind_data->field_ids, unique_field_ids, name_to_type_map);
1167: 			}
1168: 		} else if (loption == "kv_metadata") {
1169: 			auto &kv_struct = option.second[0];
1170: 			auto &kv_struct_type = kv_struct.type();
1171: 			if (kv_struct_type.id() != LogicalTypeId::STRUCT) {
1172: 				throw BinderException("Expected kv_metadata argument to be a STRUCT");
1173: 			}
1174: 			auto values = StructValue::GetChildren(kv_struct);
1175: 			for (idx_t i = 0; i < values.size(); i++) {
1176: 				auto value = values[i];
1177: 				auto key = StructType::GetChildName(kv_struct_type, i);
1178: 				// If the value is a blob, write the raw blob bytes
1179: 				// otherwise, cast to string
1180: 				if (value.type().id() == LogicalTypeId::BLOB) {
1181: 					bind_data->kv_metadata.emplace_back(key, StringValue::Get(value));
1182: 				} else {
1183: 					bind_data->kv_metadata.emplace_back(key, value.ToString());
1184: 				}
1185: 			}
1186: 		} else if (loption == "encryption_config") {
1187: 			bind_data->encryption_config = ParquetEncryptionConfig::Create(context, option.second[0]);
1188: 		} else if (loption == "dictionary_compression_ratio_threshold") {
1189: 			auto val = option.second[0].GetValue<double>();
1190: 			if (val == -1) {
1191: 				val = NumericLimits<double>::Maximum();
1192: 			} else if (val < 0) {
1193: 				throw BinderException("dictionary_compression_ratio_threshold must be greater than 0, or -1 to disable "
1194: 				                      "dictionary compression");
1195: 			}
1196: 			bind_data->dictionary_compression_ratio_threshold = val;
1197: 		} else if (loption == "compression_level") {
1198: 			bind_data->compression_level = option.second[0].GetValue<uint64_t>();
1199: 		} else {
1200: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
1201: 		}
1202: 	}
1203: 	if (row_group_size_bytes_set) {
1204: 		if (DBConfig::GetConfig(context).options.preserve_insertion_order) {
1205: 			throw BinderException("ROW_GROUP_SIZE_BYTES does not work while preserving insertion order. Use \"SET "
1206: 			                      "preserve_insertion_order=false;\" to disable preserving insertion order.");
1207: 		}
1208: 	} else {
1209: 		// We always set a max row group size bytes so we don't use too much memory
1210: 		bind_data->row_group_size_bytes = bind_data->row_group_size * ParquetWriteBindData::BYTES_PER_ROW;
1211: 	}
1212: 
1213: 	bind_data->sql_types = sql_types;
1214: 	bind_data->column_names = names;
1215: 	return std::move(bind_data);
1216: }
1217: 
1218: unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &context, FunctionData &bind_data,
1219:                                                             const string &file_path) {
1220: 	auto global_state = make_uniq<ParquetWriteGlobalState>();
1221: 	auto &parquet_bind = bind_data.Cast<ParquetWriteBindData>();
1222: 
1223: 	auto &fs = FileSystem::GetFileSystem(context);
1224: 	global_state->writer = make_uniq<ParquetWriter>(
1225: 	    context, fs, file_path, parquet_bind.sql_types, parquet_bind.column_names, parquet_bind.codec,
1226: 	    parquet_bind.field_ids.Copy(), parquet_bind.kv_metadata, parquet_bind.encryption_config,
1227: 	    parquet_bind.dictionary_compression_ratio_threshold, parquet_bind.compression_level);
1228: 	return std::move(global_state);
1229: }
1230: 
1231: void ParquetWriteSink(ExecutionContext &context, FunctionData &bind_data_p, GlobalFunctionData &gstate,
1232:                       LocalFunctionData &lstate, DataChunk &input) {
1233: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
1234: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
1235: 	auto &local_state = lstate.Cast<ParquetWriteLocalState>();
1236: 
1237: 	// append data to the local (buffered) chunk collection
1238: 	local_state.buffer.Append(local_state.append_state, input);
1239: 
1240: 	if (local_state.buffer.Count() >= bind_data.row_group_size ||
1241: 	    local_state.buffer.SizeInBytes() >= bind_data.row_group_size_bytes) {
1242: 		// if the chunk collection exceeds a certain size (rows/bytes) we flush it to the parquet file
1243: 		local_state.append_state.current_chunk_state.handles.clear();
1244: 		global_state.writer->Flush(local_state.buffer);
1245: 		local_state.buffer.InitializeAppend(local_state.append_state);
1246: 	}
1247: }
1248: 
1249: void ParquetWriteCombine(ExecutionContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
1250:                          LocalFunctionData &lstate) {
1251: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
1252: 	auto &local_state = lstate.Cast<ParquetWriteLocalState>();
1253: 	// flush any data left in the local state to the file
1254: 	global_state.writer->Flush(local_state.buffer);
1255: }
1256: 
1257: void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
1258: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
1259: 	// finalize: write any additional metadata to the file here
1260: 	global_state.writer->Finalize();
1261: }
1262: 
1263: unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ExecutionContext &context, FunctionData &bind_data_p) {
1264: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
1265: 	return make_uniq<ParquetWriteLocalState>(context.client, bind_data.sql_types);
1266: }
1267: 
1268: // LCOV_EXCL_START
1269: 
1270: // FIXME: Have these be generated instead
1271: template <>
1272: const char *EnumUtil::ToChars<duckdb_parquet::format::CompressionCodec::type>(
1273:     duckdb_parquet::format::CompressionCodec::type value) {
1274: 	switch (value) {
1275: 	case CompressionCodec::UNCOMPRESSED:
1276: 		return "UNCOMPRESSED";
1277: 		break;
1278: 	case CompressionCodec::SNAPPY:
1279: 		return "SNAPPY";
1280: 		break;
1281: 	case CompressionCodec::GZIP:
1282: 		return "GZIP";
1283: 		break;
1284: 	case CompressionCodec::LZO:
1285: 		return "LZO";
1286: 		break;
1287: 	case CompressionCodec::BROTLI:
1288: 		return "BROTLI";
1289: 		break;
1290: 	case CompressionCodec::LZ4:
1291: 		return "LZ4";
1292: 		break;
1293: 	case CompressionCodec::LZ4_RAW:
1294: 		return "LZ4_RAW";
1295: 		break;
1296: 	case CompressionCodec::ZSTD:
1297: 		return "ZSTD";
1298: 		break;
1299: 	default:
1300: 		throw NotImplementedException(StringUtil::Format("Enum value: '%s' not implemented", value));
1301: 	}
1302: }
1303: 
1304: template <>
1305: duckdb_parquet::format::CompressionCodec::type
1306: EnumUtil::FromString<duckdb_parquet::format::CompressionCodec::type>(const char *value) {
1307: 	if (StringUtil::Equals(value, "UNCOMPRESSED")) {
1308: 		return CompressionCodec::UNCOMPRESSED;
1309: 	}
1310: 	if (StringUtil::Equals(value, "SNAPPY")) {
1311: 		return CompressionCodec::SNAPPY;
1312: 	}
1313: 	if (StringUtil::Equals(value, "GZIP")) {
1314: 		return CompressionCodec::GZIP;
1315: 	}
1316: 	if (StringUtil::Equals(value, "LZO")) {
1317: 		return CompressionCodec::LZO;
1318: 	}
1319: 	if (StringUtil::Equals(value, "BROTLI")) {
1320: 		return CompressionCodec::BROTLI;
1321: 	}
1322: 	if (StringUtil::Equals(value, "LZ4")) {
1323: 		return CompressionCodec::LZ4;
1324: 	}
1325: 	if (StringUtil::Equals(value, "LZ4_RAW")) {
1326: 		return CompressionCodec::LZ4_RAW;
1327: 	}
1328: 	if (StringUtil::Equals(value, "ZSTD")) {
1329: 		return CompressionCodec::ZSTD;
1330: 	}
1331: 	throw NotImplementedException(StringUtil::Format("Enum value: '%s' not implemented", value));
1332: }
1333: 
1334: static void ParquetCopySerialize(Serializer &serializer, const FunctionData &bind_data_p,
1335:                                  const CopyFunction &function) {
1336: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
1337: 	serializer.WriteProperty(100, "sql_types", bind_data.sql_types);
1338: 	serializer.WriteProperty(101, "column_names", bind_data.column_names);
1339: 	serializer.WriteProperty(102, "codec", bind_data.codec);
1340: 	serializer.WriteProperty(103, "row_group_size", bind_data.row_group_size);
1341: 	serializer.WriteProperty(104, "row_group_size_bytes", bind_data.row_group_size_bytes);
1342: 	serializer.WriteProperty(105, "kv_metadata", bind_data.kv_metadata);
1343: 	serializer.WriteProperty(106, "field_ids", bind_data.field_ids);
1344: 	serializer.WritePropertyWithDefault<shared_ptr<ParquetEncryptionConfig>>(107, "encryption_config",
1345: 	                                                                         bind_data.encryption_config, nullptr);
1346: 	serializer.WriteProperty(108, "dictionary_compression_ratio_threshold",
1347: 	                         bind_data.dictionary_compression_ratio_threshold);
1348: 	serializer.WritePropertyWithDefault<optional_idx>(109, "compression_level", bind_data.compression_level);
1349: 	serializer.WriteProperty(110, "row_groups_per_file", bind_data.row_groups_per_file);
1350: }
1351: 
1352: static unique_ptr<FunctionData> ParquetCopyDeserialize(Deserializer &deserializer, CopyFunction &function) {
1353: 	auto data = make_uniq<ParquetWriteBindData>();
1354: 	data->sql_types = deserializer.ReadProperty<vector<LogicalType>>(100, "sql_types");
1355: 	data->column_names = deserializer.ReadProperty<vector<string>>(101, "column_names");
1356: 	data->codec = deserializer.ReadProperty<duckdb_parquet::format::CompressionCodec::type>(102, "codec");
1357: 	data->row_group_size = deserializer.ReadProperty<idx_t>(103, "row_group_size");
1358: 	data->row_group_size_bytes = deserializer.ReadProperty<idx_t>(104, "row_group_size_bytes");
1359: 	data->kv_metadata = deserializer.ReadProperty<vector<pair<string, string>>>(105, "kv_metadata");
1360: 	data->field_ids = deserializer.ReadProperty<ChildFieldIDs>(106, "field_ids");
1361: 	deserializer.ReadPropertyWithDefault<shared_ptr<ParquetEncryptionConfig>>(107, "encryption_config",
1362: 	                                                                          data->encryption_config, nullptr);
1363: 	deserializer.ReadPropertyWithDefault<double>(108, "dictionary_compression_ratio_threshold",
1364: 	                                             data->dictionary_compression_ratio_threshold, 1.0);
1365: 	deserializer.ReadPropertyWithDefault<optional_idx>(109, "compression_level", data->compression_level);
1366: 	data->row_groups_per_file =
1367: 	    deserializer.ReadPropertyWithDefault<optional_idx>(110, "row_groups_per_file", optional_idx::Invalid());
1368: 	return std::move(data);
1369: }
1370: // LCOV_EXCL_STOP
1371: 
1372: //===--------------------------------------------------------------------===//
1373: // Execution Mode
1374: //===--------------------------------------------------------------------===//
1375: CopyFunctionExecutionMode ParquetWriteExecutionMode(bool preserve_insertion_order, bool supports_batch_index) {
1376: 	if (!preserve_insertion_order) {
1377: 		return CopyFunctionExecutionMode::PARALLEL_COPY_TO_FILE;
1378: 	}
1379: 	if (supports_batch_index) {
1380: 		return CopyFunctionExecutionMode::BATCH_COPY_TO_FILE;
1381: 	}
1382: 	return CopyFunctionExecutionMode::REGULAR_COPY_TO_FILE;
1383: }
1384: //===--------------------------------------------------------------------===//
1385: // Prepare Batch
1386: //===--------------------------------------------------------------------===//
1387: struct ParquetWriteBatchData : public PreparedBatchData {
1388: 	PreparedRowGroup prepared_row_group;
1389: };
1390: 
1391: unique_ptr<PreparedBatchData> ParquetWritePrepareBatch(ClientContext &context, FunctionData &bind_data,
1392:                                                        GlobalFunctionData &gstate,
1393:                                                        unique_ptr<ColumnDataCollection> collection) {
1394: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
1395: 	auto result = make_uniq<ParquetWriteBatchData>();
1396: 	global_state.writer->PrepareRowGroup(*collection, result->prepared_row_group);
1397: 	return std::move(result);
1398: }
1399: 
1400: //===--------------------------------------------------------------------===//
1401: // Flush Batch
1402: //===--------------------------------------------------------------------===//
1403: void ParquetWriteFlushBatch(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
1404:                             PreparedBatchData &batch_p) {
1405: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
1406: 	auto &batch = batch_p.Cast<ParquetWriteBatchData>();
1407: 	global_state.writer->FlushRowGroup(batch.prepared_row_group);
1408: }
1409: 
1410: //===--------------------------------------------------------------------===//
1411: // Desired Batch Size
1412: //===--------------------------------------------------------------------===//
1413: idx_t ParquetWriteDesiredBatchSize(ClientContext &context, FunctionData &bind_data_p) {
1414: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
1415: 	return bind_data.row_group_size;
1416: }
1417: 
1418: //===--------------------------------------------------------------------===//
1419: // File rotation
1420: //===--------------------------------------------------------------------===//
1421: bool ParquetWriteRotateFiles(FunctionData &bind_data_p, const optional_idx &file_size_bytes) {
1422: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
1423: 	return file_size_bytes.IsValid() || bind_data.row_groups_per_file.IsValid();
1424: }
1425: 
1426: bool ParquetWriteRotateNextFile(GlobalFunctionData &gstate, FunctionData &bind_data_p,
1427:                                 const optional_idx &file_size_bytes) {
1428: 	auto &global_state = gstate.Cast<ParquetWriteGlobalState>();
1429: 	auto &bind_data = bind_data_p.Cast<ParquetWriteBindData>();
1430: 	if (file_size_bytes.IsValid() && global_state.writer->FileSize() > file_size_bytes.GetIndex()) {
1431: 		return true;
1432: 	}
1433: 	if (bind_data.row_groups_per_file.IsValid() &&
1434: 	    global_state.writer->NumberOfRowGroups() >= bind_data.row_groups_per_file.GetIndex()) {
1435: 		return true;
1436: 	}
1437: 	return false;
1438: }
1439: 
1440: //===--------------------------------------------------------------------===//
1441: // Scan Replacement
1442: //===--------------------------------------------------------------------===//
1443: unique_ptr<TableRef> ParquetScanReplacement(ClientContext &context, ReplacementScanInput &input,
1444:                                             optional_ptr<ReplacementScanData> data) {
1445: 	auto &table_name = input.table_name;
1446: 	if (!ReplacementScan::CanReplace(table_name, {"parquet"})) {
1447: 		return nullptr;
1448: 	}
1449: 	auto table_function = make_uniq<TableFunctionRef>();
1450: 	vector<unique_ptr<ParsedExpression>> children;
1451: 	children.push_back(make_uniq<ConstantExpression>(Value(table_name)));
1452: 	table_function->function = make_uniq<FunctionExpression>("parquet_scan", std::move(children));
1453: 
1454: 	if (!FileSystem::HasGlob(table_name)) {
1455: 		auto &fs = FileSystem::GetFileSystem(context);
1456: 		table_function->alias = fs.ExtractBaseName(table_name);
1457: 	}
1458: 
1459: 	return std::move(table_function);
1460: }
1461: 
1462: //===--------------------------------------------------------------------===//
1463: // Select
1464: //===--------------------------------------------------------------------===//
1465: // Helper predicates for ParquetWriteSelect
1466: static bool IsTypeNotSupported(const LogicalType &type) {
1467: 	if (type.IsNested()) {
1468: 		return false;
1469: 	}
1470: 	return !ParquetWriter::TryGetParquetType(type);
1471: }
1472: 
1473: static bool IsTypeLossy(const LogicalType &type) {
1474: 	return type.id() == LogicalTypeId::HUGEINT || type.id() == LogicalTypeId::UHUGEINT;
1475: }
1476: 
1477: static vector<unique_ptr<Expression>> ParquetWriteSelect(CopyToSelectInput &input) {
1478: 
1479: 	auto &context = input.context;
1480: 
1481: 	vector<unique_ptr<Expression>> result;
1482: 
1483: 	bool any_change = false;
1484: 
1485: 	for (auto &expr : input.select_list) {
1486: 
1487: 		const auto &type = expr->return_type;
1488: 		const auto &name = expr->alias;
1489: 
1490: 		// Spatial types need to be encoded into WKB when writing GeoParquet.
1491: 		// But dont perform this conversion if this is a EXPORT DATABASE statement
1492: 		if (input.copy_to_type == CopyToType::COPY_TO_FILE && type.id() == LogicalTypeId::BLOB && type.HasAlias() &&
1493: 		    type.GetAlias() == "GEOMETRY") {
1494: 
1495: 			LogicalType wkb_blob_type(LogicalTypeId::BLOB);
1496: 			wkb_blob_type.SetAlias("WKB_BLOB");
1497: 
1498: 			auto cast_expr = BoundCastExpression::AddCastToType(context, std::move(expr), wkb_blob_type, false);
1499: 			cast_expr->alias = name;
1500: 			result.push_back(std::move(cast_expr));
1501: 			any_change = true;
1502: 		}
1503: 		// If this is an EXPORT DATABASE statement, we dont want to write "lossy" types, instead cast them to VARCHAR
1504: 		else if (input.copy_to_type == CopyToType::EXPORT_DATABASE && TypeVisitor::Contains(type, IsTypeLossy)) {
1505: 			// Replace all lossy types with VARCHAR
1506: 			auto new_type = TypeVisitor::VisitReplace(
1507: 			    type, [](const LogicalType &ty) -> LogicalType { return IsTypeLossy(ty) ? LogicalType::VARCHAR : ty; });
1508: 
1509: 			// Cast the column to the new type
1510: 			auto cast_expr = BoundCastExpression::AddCastToType(context, std::move(expr), new_type, false);
1511: 			cast_expr->alias = name;
1512: 			result.push_back(std::move(cast_expr));
1513: 			any_change = true;
1514: 		}
1515: 		// Else look if there is any unsupported type
1516: 		else if (TypeVisitor::Contains(type, IsTypeNotSupported)) {
1517: 			// If there is at least one unsupported type, replace all unsupported types with varchar
1518: 			// and perform a CAST
1519: 			auto new_type = TypeVisitor::VisitReplace(type, [](const LogicalType &ty) -> LogicalType {
1520: 				return IsTypeNotSupported(ty) ? LogicalType::VARCHAR : ty;
1521: 			});
1522: 
1523: 			auto cast_expr = BoundCastExpression::AddCastToType(context, std::move(expr), new_type, false);
1524: 			cast_expr->alias = name;
1525: 			result.push_back(std::move(cast_expr));
1526: 			any_change = true;
1527: 		}
1528: 		// Otherwise, just reference the input column
1529: 		else {
1530: 			result.push_back(std::move(expr));
1531: 		}
1532: 	}
1533: 
1534: 	// If any change was made, return the new expressions
1535: 	// otherwise, return an empty vector to indicate no change and avoid pushing another projection on to the plan
1536: 	if (any_change) {
1537: 		return result;
1538: 	}
1539: 	return {};
1540: }
1541: 
1542: void ParquetExtension::Load(DuckDB &db) {
1543: 	auto &db_instance = *db.instance;
1544: 	auto &fs = db.GetFileSystem();
1545: 	fs.RegisterSubSystem(FileCompressionType::ZSTD, make_uniq<ZStdFileSystem>());
1546: 
1547: 	auto scan_fun = ParquetScanFunction::GetFunctionSet();
1548: 	scan_fun.name = "read_parquet";
1549: 	ExtensionUtil::RegisterFunction(db_instance, scan_fun);
1550: 	scan_fun.name = "parquet_scan";
1551: 	ExtensionUtil::RegisterFunction(db_instance, scan_fun);
1552: 
1553: 	// parquet_metadata
1554: 	ParquetMetaDataFunction meta_fun;
1555: 	ExtensionUtil::RegisterFunction(db_instance, MultiFileReader::CreateFunctionSet(meta_fun));
1556: 
1557: 	// parquet_schema
1558: 	ParquetSchemaFunction schema_fun;
1559: 	ExtensionUtil::RegisterFunction(db_instance, MultiFileReader::CreateFunctionSet(schema_fun));
1560: 
1561: 	// parquet_key_value_metadata
1562: 	ParquetKeyValueMetadataFunction kv_meta_fun;
1563: 	ExtensionUtil::RegisterFunction(db_instance, MultiFileReader::CreateFunctionSet(kv_meta_fun));
1564: 
1565: 	// parquet_file_metadata
1566: 	ParquetFileMetadataFunction file_meta_fun;
1567: 	ExtensionUtil::RegisterFunction(db_instance, MultiFileReader::CreateFunctionSet(file_meta_fun));
1568: 
1569: 	CopyFunction function("parquet");
1570: 	function.copy_to_select = ParquetWriteSelect;
1571: 	function.copy_to_bind = ParquetWriteBind;
1572: 	function.copy_to_initialize_global = ParquetWriteInitializeGlobal;
1573: 	function.copy_to_initialize_local = ParquetWriteInitializeLocal;
1574: 	function.copy_to_sink = ParquetWriteSink;
1575: 	function.copy_to_combine = ParquetWriteCombine;
1576: 	function.copy_to_finalize = ParquetWriteFinalize;
1577: 	function.execution_mode = ParquetWriteExecutionMode;
1578: 	function.copy_from_bind = ParquetScanFunction::ParquetReadBind;
1579: 	function.copy_from_function = scan_fun.functions[0];
1580: 	function.prepare_batch = ParquetWritePrepareBatch;
1581: 	function.flush_batch = ParquetWriteFlushBatch;
1582: 	function.desired_batch_size = ParquetWriteDesiredBatchSize;
1583: 	function.rotate_files = ParquetWriteRotateFiles;
1584: 	function.rotate_next_file = ParquetWriteRotateNextFile;
1585: 	function.serialize = ParquetCopySerialize;
1586: 	function.deserialize = ParquetCopyDeserialize;
1587: 
1588: 	function.extension = "parquet";
1589: 	ExtensionUtil::RegisterFunction(db_instance, function);
1590: 
1591: 	// parquet_key
1592: 	auto parquet_key_fun = PragmaFunction::PragmaCall("add_parquet_key", ParquetCrypto::AddKey,
1593: 	                                                  {LogicalType::VARCHAR, LogicalType::VARCHAR});
1594: 	ExtensionUtil::RegisterFunction(db_instance, parquet_key_fun);
1595: 
1596: 	auto &config = DBConfig::GetConfig(*db.instance);
1597: 	config.replacement_scans.emplace_back(ParquetScanReplacement);
1598: 	config.AddExtensionOption("binary_as_string", "In Parquet files, interpret binary data as a string.",
1599: 	                          LogicalType::BOOLEAN);
1600: }
1601: 
1602: std::string ParquetExtension::Name() {
1603: 	return "parquet";
1604: }
1605: 
1606: std::string ParquetExtension::Version() const {
1607: #ifdef EXT_VERSION_PARQUET
1608: 	return EXT_VERSION_PARQUET;
1609: #else
1610: 	return "";
1611: #endif
1612: }
1613: 
1614: } // namespace duckdb
1615: 
1616: #ifdef DUCKDB_BUILD_LOADABLE_EXTENSION
1617: extern "C" {
1618: 
1619: DUCKDB_EXTENSION_API void parquet_init(duckdb::DatabaseInstance &db) { // NOLINT
1620: 	duckdb::DuckDB db_wrapper(db);
1621: 	db_wrapper.LoadExtension<duckdb::ParquetExtension>();
1622: }
1623: 
1624: DUCKDB_EXTENSION_API const char *parquet_version() { // NOLINT
1625: 	return duckdb::DuckDB::LibraryVersion();
1626: }
1627: }
1628: #endif
1629: 
1630: #ifndef DUCKDB_EXTENSION_MAIN
1631: #error DUCKDB_EXTENSION_MAIN not defined
1632: #endif
[end of extension/parquet/parquet_extension.cpp]
[start of src/storage/storage_info.cpp]
1: #include "duckdb/storage/storage_info.hpp"
2: 
3: #include "duckdb/common/numeric_utils.hpp"
4: #include "duckdb/common/optional_idx.hpp"
5: 
6: namespace duckdb {
7: 
8: const uint64_t VERSION_NUMBER = 64;
9: 
10: struct StorageVersionInfo {
11: 	const char *version_name;
12: 	idx_t storage_version;
13: };
14: 
15: struct SerializationVersionInfo {
16: 	const char *version_name;
17: 	idx_t serialization_version;
18: };
19: 
20: // These sections are automatically generated by scripts/generate_storage_info.py
21: // Do not edit them manually, your changes will be overwritten
22: 
23: // START OF STORAGE VERSION INFO
24: static const StorageVersionInfo storage_version_info[] = {
25:     {"v0.0.4", 1},  {"v0.1.0", 1},  {"v0.1.1", 1},  {"v0.1.2", 1},  {"v0.1.3", 1},   {"v0.1.4", 1},   {"v0.1.5", 1},
26:     {"v0.1.6", 1},  {"v0.1.7", 1},  {"v0.1.8", 1},  {"v0.1.9", 1},  {"v0.2.0", 1},   {"v0.2.1", 1},   {"v0.2.2", 4},
27:     {"v0.2.3", 6},  {"v0.2.4", 11}, {"v0.2.5", 13}, {"v0.2.6", 15}, {"v0.2.7", 17},  {"v0.2.8", 18},  {"v0.2.9", 21},
28:     {"v0.3.0", 25}, {"v0.3.1", 27}, {"v0.3.2", 31}, {"v0.3.3", 33}, {"v0.3.4", 33},  {"v0.3.5", 33},  {"v0.4.0", 33},
29:     {"v0.5.0", 38}, {"v0.5.1", 38}, {"v0.6.0", 39}, {"v0.6.1", 39}, {"v0.7.0", 43},  {"v0.7.1", 43},  {"v0.8.0", 51},
30:     {"v0.8.1", 51}, {"v0.9.0", 64}, {"v0.9.1", 64}, {"v0.9.2", 64}, {"v0.10.0", 64}, {"v0.10.1", 64}, {"v0.10.2", 64},
31:     {nullptr, 0}};
32: // END OF STORAGE VERSION INFO
33: 
34: // START OF SERIALIZATION VERSION INFO
35: static const SerializationVersionInfo serialization_version_info[] = {
36:     {"v0.10.0", 1}, {"v0.10.1", 1}, {"v0.10.2", 1}, {"latest", 2}, {nullptr, 0}};
37: // END OF SERIALIZATION VERSION INFO
38: 
39: optional_idx GetStorageVersion(const char *version_string) {
40: 	for (idx_t i = 0; storage_version_info[i].version_name; i++) {
41: 		if (!strcmp(storage_version_info[i].version_name, version_string)) {
42: 			return storage_version_info[i].storage_version;
43: 		}
44: 	}
45: 	return optional_idx();
46: }
47: 
48: optional_idx GetSerializationVersion(const char *version_string) {
49: 	for (idx_t i = 0; serialization_version_info[i].version_name; i++) {
50: 		if (!strcmp(serialization_version_info[i].version_name, version_string)) {
51: 			return serialization_version_info[i].serialization_version;
52: 		}
53: 	}
54: 	return optional_idx();
55: }
56: 
57: vector<string> GetSerializationCandidates() {
58: 	vector<string> candidates;
59: 	for (idx_t i = 0; serialization_version_info[i].version_name; i++) {
60: 		candidates.push_back(serialization_version_info[i].version_name);
61: 	}
62: 	return candidates;
63: }
64: 
65: string GetDuckDBVersion(idx_t version_number) {
66: 	vector<string> versions;
67: 	for (idx_t i = 0; storage_version_info[i].version_name; i++) {
68: 		if (version_number == storage_version_info[i].storage_version) {
69: 			versions.push_back(string(storage_version_info[i].version_name));
70: 		}
71: 	}
72: 	if (versions.empty()) {
73: 		return string();
74: 	}
75: 	string result;
76: 	for (idx_t i = 0; i < versions.size(); i++) {
77: 		string sep = "";
78: 		if (i) {
79: 			sep = i + 1 == versions.size() ? " or " : ", ";
80: 		}
81: 		result += sep;
82: 		result += versions[i];
83: 	}
84: 	return result;
85: }
86: 
87: void Storage::VerifyBlockAllocSize(const idx_t block_alloc_size) {
88: 	if (!IsPowerOfTwo(block_alloc_size)) {
89: 		throw InvalidInputException("the block size must be a power of two, got %llu", block_alloc_size);
90: 	}
91: 	if (block_alloc_size < MIN_BLOCK_ALLOC_SIZE) {
92: 		throw InvalidInputException(
93: 		    "the block size must be greater or equal than the minimum block size of %llu, got %llu",
94: 		    MIN_BLOCK_ALLOC_SIZE, block_alloc_size);
95: 	}
96: 	if (block_alloc_size > MAX_BLOCK_ALLOC_SIZE) {
97: 		throw InvalidInputException(
98: 		    "the block size must be lesser or equal than the maximum block size of %llu, got %llu",
99: 		    MAX_BLOCK_ALLOC_SIZE, block_alloc_size);
100: 	}
101: 	auto max_value = NumericCast<idx_t>(NumericLimits<int32_t>().Maximum());
102: 	if (block_alloc_size > max_value) {
103: 		throw InvalidInputException(
104: 		    "the block size must not be greater than the maximum 32-bit signed integer value of %llu, got %llu",
105: 		    max_value, block_alloc_size);
106: 	}
107: }
108: 
109: } // namespace duckdb
[end of src/storage/storage_info.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: