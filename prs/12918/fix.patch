diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp
index 65d5e01c0d4a..c9f481479385 100644
--- a/extension/parquet/column_reader.cpp
+++ b/extension/parquet/column_reader.cpp
@@ -1046,14 +1046,33 @@ idx_t CastColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, data
 	bool all_succeeded = VectorOperations::DefaultTryCast(intermediate_vector, result, amount, &error_message);
 	if (!all_succeeded) {
 		string extended_error;
-		extended_error =
-		    StringUtil::Format("In file \"%s\" the column \"%s\" has type %s, but we are trying to read it as type %s.",
-		                       reader.file_name, schema.name, intermediate_vector.GetType(), result.GetType());
-		extended_error += "
This can happen when reading multiple Parquet files. The schema information is taken from "
-		                  "the first Parquet file by default. Possible solutions:
";
-		extended_error += "* Enable the union_by_name=True option to combine the schema of all Parquet files "
-		                  "(duckdb.org/docs/data/multiple_files/combining_schemas)
";
-		extended_error += "* Use a COPY statement to automatically derive types from an existing table.";
+		if (!reader.table_columns.empty()) {
+			// COPY .. FROM
+			extended_error = StringUtil::Format(
+			    "In file \"%s\" the column \"%s\" has type %s, but we are trying to load it into column ",
+			    reader.file_name, schema.name, intermediate_vector.GetType());
+			if (FileIdx() < reader.table_columns.size()) {
+				extended_error += "\"" + reader.table_columns[FileIdx()] + "\" ";
+			}
+			extended_error += StringUtil::Format("with type %s.", result.GetType());
+			extended_error += "
This means the Parquet schema does not match the schema of the table.";
+			extended_error += "
Possible solutions:";
+			extended_error += "
* Insert by name instead of by position using \"INSERT INTO tbl BY NAME SELECT * FROM "
+			                  "read_parquet(...)\"";
+			extended_error += "
* Manually specify which columns to insert using \"INSERT INTO tbl SELECT ... FROM "
+			                  "read_parquet(...)\"";
+		} else {
+			// read_parquet() with multiple files
+			extended_error = StringUtil::Format(
+			    "In file \"%s\" the column \"%s\" has type %s, but we are trying to read it as type %s.",
+			    reader.file_name, schema.name, intermediate_vector.GetType(), result.GetType());
+			extended_error +=
+			    "
This can happen when reading multiple Parquet files. The schema information is taken from "
+			    "the first Parquet file by default. Possible solutions:
";
+			extended_error += "* Enable the union_by_name=True option to combine the schema of all Parquet files "
+			                  "(duckdb.org/docs/data/multiple_files/combining_schemas)
";
+			extended_error += "* Use a COPY statement to automatically derive types from an existing table.";
+		}
 		throw ConversionException(
 		    "In Parquet reader of file \"%s\": failed to cast column \"%s\" from type %s to %s: %s

%s",
 		    reader.file_name, schema.name, intermediate_vector.GetType(), result.GetType(), error_message,
diff --git a/extension/parquet/include/parquet_reader.hpp b/extension/parquet/include/parquet_reader.hpp
index 3e8f70cc06ae..326efa48d515 100644
--- a/extension/parquet/include/parquet_reader.hpp
+++ b/extension/parquet/include/parquet_reader.hpp
@@ -135,6 +135,8 @@ class ParquetReader {
 	idx_t file_row_number_idx = DConstants::INVALID_INDEX;
 	//! Parquet schema for the generated columns
 	vector<duckdb_parquet::format::SchemaElement> generated_column_schema;
+	//! Table column names - set when using COPY tbl FROM file.parquet
+	vector<string> table_columns;
 
 public:
 	void InitializeScan(ClientContext &context, ParquetReaderScanState &state, vector<idx_t> groups_to_read);
diff --git a/extension/parquet/parquet_extension.cpp b/extension/parquet/parquet_extension.cpp
index 7b4e19a6bb0e..5fb3fbb0d108 100644
--- a/extension/parquet/parquet_extension.cpp
+++ b/extension/parquet/parquet_extension.cpp
@@ -60,6 +60,8 @@ struct ParquetReadBindData : public TableFunctionData {
 	atomic<idx_t> chunk_count;
 	vector<string> names;
 	vector<LogicalType> types;
+	//! Table column names - set when using COPY tbl FROM file.parquet
+	vector<string> table_columns;
 
 	// The union readers are created (when parquet union_by_name option is on) during binding
 	// Those readers can be re-used during ParquetParallelStateNext
@@ -277,6 +279,7 @@ static void InitializeParquetReader(ParquetReader &reader, const ParquetReadBind
 	auto &parquet_options = bind_data.parquet_options;
 	auto &reader_data = reader.reader_data;
 
+	reader.table_columns = bind_data.table_columns;
 	// Mark the file in the file list we are scanning here
 	reader_data.file_list_idx = file_idx;
 
@@ -536,12 +539,31 @@ class ParquetScanFunction {
 			if (return_types.size() != result->types.size()) {
 				auto file_string = bound_on_first_file ? result->file_list->GetFirstFile()
 				                                       : StringUtil::Join(result->file_list->GetPaths(), ",");
-				throw std::runtime_error(StringUtil::Format(
-				    "Failed to read file(s) \"%s\" - column count mismatch: expected %d columns but found %d",
-				    file_string, return_types.size(), result->types.size()));
+				string extended_error;
+				extended_error = "Table schema: ";
+				for (idx_t col_idx = 0; col_idx < return_types.size(); col_idx++) {
+					if (col_idx > 0) {
+						extended_error += ", ";
+					}
+					extended_error += names[col_idx] + " " + return_types[col_idx].ToString();
+				}
+				extended_error += "
Parquet schema: ";
+				for (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {
+					if (col_idx > 0) {
+						extended_error += ", ";
+					}
+					extended_error += result->names[col_idx] + " " + result->types[col_idx].ToString();
+				}
+				extended_error += "

Possible solutions:";
+				extended_error += "
* Manually specify which columns to insert using \"INSERT INTO tbl SELECT ... "
+				                  "FROM read_parquet(...)\"";
+				throw ConversionException(
+				    "Failed to read file(s) \"%s\" - column count mismatch: expected %d columns but found %d
%s",
+				    file_string, return_types.size(), result->types.size(), extended_error);
 			}
 			// expected types - overwrite the types we want to read instead
 			result->types = return_types;
+			result->table_columns = names;
 		}
 		result->parquet_options = parquet_options;
 		return std::move(result);
@@ -725,6 +747,9 @@ class ParquetScanFunction {
 		serializer.WriteProperty(101, "types", bind_data.types);
 		serializer.WriteProperty(102, "names", bind_data.names);
 		serializer.WriteProperty(103, "parquet_options", bind_data.parquet_options);
+		if (serializer.ShouldSerialize(3)) {
+			serializer.WriteProperty(104, "table_columns", bind_data.table_columns);
+		}
 	}
 
 	static unique_ptr<FunctionData> ParquetScanDeserialize(Deserializer &deserializer, TableFunction &function) {
@@ -733,6 +758,8 @@ class ParquetScanFunction {
 		auto types = deserializer.ReadProperty<vector<LogicalType>>(101, "types");
 		auto names = deserializer.ReadProperty<vector<string>>(102, "names");
 		auto parquet_options = deserializer.ReadProperty<ParquetOptions>(103, "parquet_options");
+		auto table_columns =
+		    deserializer.ReadPropertyWithDefault<vector<string>>(104, "table_columns", vector<string> {});
 
 		vector<Value> file_path;
 		for (auto &path : files) {
@@ -742,8 +769,10 @@ class ParquetScanFunction {
 		auto multi_file_reader = MultiFileReader::Create(function);
 		auto file_list = multi_file_reader->CreateFileList(context, Value::LIST(LogicalType::VARCHAR, file_path),
 		                                                   FileGlobOptions::DISALLOW_EMPTY);
-		return ParquetScanBindInternal(context, std::move(multi_file_reader), std::move(file_list), types, names,
-		                               parquet_options);
+		auto bind_data = ParquetScanBindInternal(context, std::move(multi_file_reader), std::move(file_list), types,
+		                                         names, parquet_options);
+		bind_data->Cast<ParquetReadBindData>().table_columns = std::move(table_columns);
+		return bind_data;
 	}
 
 	static void ParquetScanImplementation(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {
diff --git a/src/storage/storage_info.cpp b/src/storage/storage_info.cpp
index ab61e0fd42dc..83382128df4b 100644
--- a/src/storage/storage_info.cpp
+++ b/src/storage/storage_info.cpp
@@ -32,8 +32,9 @@ static const StorageVersionInfo storage_version_info[] = {
 // END OF STORAGE VERSION INFO
 
 // START OF SERIALIZATION VERSION INFO
-static const SerializationVersionInfo serialization_version_info[] = {
-    {"v0.10.0", 1}, {"v0.10.1", 1}, {"v0.10.2", 1}, {"latest", 2}, {nullptr, 0}};
+static const SerializationVersionInfo serialization_version_info[] = {{"v0.10.0", 1}, {"v0.10.1", 1}, {"v0.10.2", 1},
+                                                                      {"v0.10.3", 2}, {"v1.0.0", 2},  {"v1.1.0", 3},
+                                                                      {"latest", 3},  {nullptr, 0}};
 // END OF SERIALIZATION VERSION INFO
 
 optional_idx GetStorageVersion(const char *version_string) {
