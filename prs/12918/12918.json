{
  "repo": "duckdb/duckdb",
  "pull_number": 12918,
  "instance_id": "duckdb__duckdb-12918",
  "issue_numbers": [
    "12870"
  ],
  "base_commit": "884755b73ee99fc23252015ba5f49f60510a7149",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex 65d5e01c0d4a..c9f481479385 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -1046,14 +1046,33 @@ idx_t CastColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, data\n \tbool all_succeeded = VectorOperations::DefaultTryCast(intermediate_vector, result, amount, &error_message);\n \tif (!all_succeeded) {\n \t\tstring extended_error;\n-\t\textended_error =\n-\t\t    StringUtil::Format(\"In file \\\"%s\\\" the column \\\"%s\\\" has type %s, but we are trying to read it as type %s.\",\n-\t\t                       reader.file_name, schema.name, intermediate_vector.GetType(), result.GetType());\n-\t\textended_error += \"\\nThis can happen when reading multiple Parquet files. The schema information is taken from \"\n-\t\t                  \"the first Parquet file by default. Possible solutions:\\n\";\n-\t\textended_error += \"* Enable the union_by_name=True option to combine the schema of all Parquet files \"\n-\t\t                  \"(duckdb.org/docs/data/multiple_files/combining_schemas)\\n\";\n-\t\textended_error += \"* Use a COPY statement to automatically derive types from an existing table.\";\n+\t\tif (!reader.table_columns.empty()) {\n+\t\t\t// COPY .. FROM\n+\t\t\textended_error = StringUtil::Format(\n+\t\t\t    \"In file \\\"%s\\\" the column \\\"%s\\\" has type %s, but we are trying to load it into column \",\n+\t\t\t    reader.file_name, schema.name, intermediate_vector.GetType());\n+\t\t\tif (FileIdx() < reader.table_columns.size()) {\n+\t\t\t\textended_error += \"\\\"\" + reader.table_columns[FileIdx()] + \"\\\" \";\n+\t\t\t}\n+\t\t\textended_error += StringUtil::Format(\"with type %s.\", result.GetType());\n+\t\t\textended_error += \"\\nThis means the Parquet schema does not match the schema of the table.\";\n+\t\t\textended_error += \"\\nPossible solutions:\";\n+\t\t\textended_error += \"\\n* Insert by name instead of by position using \\\"INSERT INTO tbl BY NAME SELECT * FROM \"\n+\t\t\t                  \"read_parquet(...)\\\"\";\n+\t\t\textended_error += \"\\n* Manually specify which columns to insert using \\\"INSERT INTO tbl SELECT ... FROM \"\n+\t\t\t                  \"read_parquet(...)\\\"\";\n+\t\t} else {\n+\t\t\t// read_parquet() with multiple files\n+\t\t\textended_error = StringUtil::Format(\n+\t\t\t    \"In file \\\"%s\\\" the column \\\"%s\\\" has type %s, but we are trying to read it as type %s.\",\n+\t\t\t    reader.file_name, schema.name, intermediate_vector.GetType(), result.GetType());\n+\t\t\textended_error +=\n+\t\t\t    \"\\nThis can happen when reading multiple Parquet files. The schema information is taken from \"\n+\t\t\t    \"the first Parquet file by default. Possible solutions:\\n\";\n+\t\t\textended_error += \"* Enable the union_by_name=True option to combine the schema of all Parquet files \"\n+\t\t\t                  \"(duckdb.org/docs/data/multiple_files/combining_schemas)\\n\";\n+\t\t\textended_error += \"* Use a COPY statement to automatically derive types from an existing table.\";\n+\t\t}\n \t\tthrow ConversionException(\n \t\t    \"In Parquet reader of file \\\"%s\\\": failed to cast column \\\"%s\\\" from type %s to %s: %s\\n\\n%s\",\n \t\t    reader.file_name, schema.name, intermediate_vector.GetType(), result.GetType(), error_message,\ndiff --git a/extension/parquet/include/parquet_reader.hpp b/extension/parquet/include/parquet_reader.hpp\nindex 3e8f70cc06ae..326efa48d515 100644\n--- a/extension/parquet/include/parquet_reader.hpp\n+++ b/extension/parquet/include/parquet_reader.hpp\n@@ -135,6 +135,8 @@ class ParquetReader {\n \tidx_t file_row_number_idx = DConstants::INVALID_INDEX;\n \t//! Parquet schema for the generated columns\n \tvector<duckdb_parquet::format::SchemaElement> generated_column_schema;\n+\t//! Table column names - set when using COPY tbl FROM file.parquet\n+\tvector<string> table_columns;\n \n public:\n \tvoid InitializeScan(ClientContext &context, ParquetReaderScanState &state, vector<idx_t> groups_to_read);\ndiff --git a/extension/parquet/parquet_extension.cpp b/extension/parquet/parquet_extension.cpp\nindex 7b4e19a6bb0e..5fb3fbb0d108 100644\n--- a/extension/parquet/parquet_extension.cpp\n+++ b/extension/parquet/parquet_extension.cpp\n@@ -60,6 +60,8 @@ struct ParquetReadBindData : public TableFunctionData {\n \tatomic<idx_t> chunk_count;\n \tvector<string> names;\n \tvector<LogicalType> types;\n+\t//! Table column names - set when using COPY tbl FROM file.parquet\n+\tvector<string> table_columns;\n \n \t// The union readers are created (when parquet union_by_name option is on) during binding\n \t// Those readers can be re-used during ParquetParallelStateNext\n@@ -277,6 +279,7 @@ static void InitializeParquetReader(ParquetReader &reader, const ParquetReadBind\n \tauto &parquet_options = bind_data.parquet_options;\n \tauto &reader_data = reader.reader_data;\n \n+\treader.table_columns = bind_data.table_columns;\n \t// Mark the file in the file list we are scanning here\n \treader_data.file_list_idx = file_idx;\n \n@@ -536,12 +539,31 @@ class ParquetScanFunction {\n \t\t\tif (return_types.size() != result->types.size()) {\n \t\t\t\tauto file_string = bound_on_first_file ? result->file_list->GetFirstFile()\n \t\t\t\t                                       : StringUtil::Join(result->file_list->GetPaths(), \",\");\n-\t\t\t\tthrow std::runtime_error(StringUtil::Format(\n-\t\t\t\t    \"Failed to read file(s) \\\"%s\\\" - column count mismatch: expected %d columns but found %d\",\n-\t\t\t\t    file_string, return_types.size(), result->types.size()));\n+\t\t\t\tstring extended_error;\n+\t\t\t\textended_error = \"Table schema: \";\n+\t\t\t\tfor (idx_t col_idx = 0; col_idx < return_types.size(); col_idx++) {\n+\t\t\t\t\tif (col_idx > 0) {\n+\t\t\t\t\t\textended_error += \", \";\n+\t\t\t\t\t}\n+\t\t\t\t\textended_error += names[col_idx] + \" \" + return_types[col_idx].ToString();\n+\t\t\t\t}\n+\t\t\t\textended_error += \"\\nParquet schema: \";\n+\t\t\t\tfor (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {\n+\t\t\t\t\tif (col_idx > 0) {\n+\t\t\t\t\t\textended_error += \", \";\n+\t\t\t\t\t}\n+\t\t\t\t\textended_error += result->names[col_idx] + \" \" + result->types[col_idx].ToString();\n+\t\t\t\t}\n+\t\t\t\textended_error += \"\\n\\nPossible solutions:\";\n+\t\t\t\textended_error += \"\\n* Manually specify which columns to insert using \\\"INSERT INTO tbl SELECT ... \"\n+\t\t\t\t                  \"FROM read_parquet(...)\\\"\";\n+\t\t\t\tthrow ConversionException(\n+\t\t\t\t    \"Failed to read file(s) \\\"%s\\\" - column count mismatch: expected %d columns but found %d\\n%s\",\n+\t\t\t\t    file_string, return_types.size(), result->types.size(), extended_error);\n \t\t\t}\n \t\t\t// expected types - overwrite the types we want to read instead\n \t\t\tresult->types = return_types;\n+\t\t\tresult->table_columns = names;\n \t\t}\n \t\tresult->parquet_options = parquet_options;\n \t\treturn std::move(result);\n@@ -725,6 +747,9 @@ class ParquetScanFunction {\n \t\tserializer.WriteProperty(101, \"types\", bind_data.types);\n \t\tserializer.WriteProperty(102, \"names\", bind_data.names);\n \t\tserializer.WriteProperty(103, \"parquet_options\", bind_data.parquet_options);\n+\t\tif (serializer.ShouldSerialize(3)) {\n+\t\t\tserializer.WriteProperty(104, \"table_columns\", bind_data.table_columns);\n+\t\t}\n \t}\n \n \tstatic unique_ptr<FunctionData> ParquetScanDeserialize(Deserializer &deserializer, TableFunction &function) {\n@@ -733,6 +758,8 @@ class ParquetScanFunction {\n \t\tauto types = deserializer.ReadProperty<vector<LogicalType>>(101, \"types\");\n \t\tauto names = deserializer.ReadProperty<vector<string>>(102, \"names\");\n \t\tauto parquet_options = deserializer.ReadProperty<ParquetOptions>(103, \"parquet_options\");\n+\t\tauto table_columns =\n+\t\t    deserializer.ReadPropertyWithDefault<vector<string>>(104, \"table_columns\", vector<string> {});\n \n \t\tvector<Value> file_path;\n \t\tfor (auto &path : files) {\n@@ -742,8 +769,10 @@ class ParquetScanFunction {\n \t\tauto multi_file_reader = MultiFileReader::Create(function);\n \t\tauto file_list = multi_file_reader->CreateFileList(context, Value::LIST(LogicalType::VARCHAR, file_path),\n \t\t                                                   FileGlobOptions::DISALLOW_EMPTY);\n-\t\treturn ParquetScanBindInternal(context, std::move(multi_file_reader), std::move(file_list), types, names,\n-\t\t                               parquet_options);\n+\t\tauto bind_data = ParquetScanBindInternal(context, std::move(multi_file_reader), std::move(file_list), types,\n+\t\t                                         names, parquet_options);\n+\t\tbind_data->Cast<ParquetReadBindData>().table_columns = std::move(table_columns);\n+\t\treturn bind_data;\n \t}\n \n \tstatic void ParquetScanImplementation(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\ndiff --git a/src/storage/storage_info.cpp b/src/storage/storage_info.cpp\nindex ab61e0fd42dc..83382128df4b 100644\n--- a/src/storage/storage_info.cpp\n+++ b/src/storage/storage_info.cpp\n@@ -32,8 +32,9 @@ static const StorageVersionInfo storage_version_info[] = {\n // END OF STORAGE VERSION INFO\n \n // START OF SERIALIZATION VERSION INFO\n-static const SerializationVersionInfo serialization_version_info[] = {\n-    {\"v0.10.0\", 1}, {\"v0.10.1\", 1}, {\"v0.10.2\", 1}, {\"latest\", 2}, {nullptr, 0}};\n+static const SerializationVersionInfo serialization_version_info[] = {{\"v0.10.0\", 1}, {\"v0.10.1\", 1}, {\"v0.10.2\", 1},\n+                                                                      {\"v0.10.3\", 2}, {\"v1.0.0\", 2},  {\"v1.1.0\", 3},\n+                                                                      {\"latest\", 3},  {nullptr, 0}};\n // END OF SERIALIZATION VERSION INFO\n \n optional_idx GetStorageVersion(const char *version_string) {\n",
  "test_patch": "diff --git a/test/sql/copy/parquet/parquet_copy_type_mismatch.test b/test/sql/copy/parquet/parquet_copy_type_mismatch.test\nnew file mode 100644\nindex 000000000000..e1e68a83f644\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_copy_type_mismatch.test\n@@ -0,0 +1,43 @@\n+# name: test/sql/copy/parquet/parquet_copy_type_mismatch.test\n+# description: Test error message when COPY FROM finds a type mismatch\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+SET storage_compatibility_version='latest'\n+\n+statement ok\n+CREATE TABLE integers(i INTEGER);\n+\n+statement ok\n+COPY (SELECT DATE '1992-01-01' d) TO '__TEST_DIR__/single_date.parquet' (FORMAT parquet);\n+\n+statement error\n+COPY integers FROM '__TEST_DIR__/single_date.parquet'\n+----\n+the column \"d\" has type DATE, but we are trying to load it into column \"i\" with type INTEGER\n+\n+statement ok\n+COPY (SELECT DATE '1992-01-01' d, 42 k) TO '__TEST_DIR__/too_many_columns.parquet' (FORMAT parquet);\n+\n+statement error\n+COPY integers FROM '__TEST_DIR__/too_many_columns.parquet'\n+----\n+Table schema: i INTEGER\n+\n+# multiple files with different schema\n+statement ok\n+COPY (SELECT 42 i) TO '__TEST_DIR__/f2.parquet' (FORMAT parquet);\n+\n+statement ok\n+COPY (SELECT date '1992-01-01' d, 84 i) TO '__TEST_DIR__/f1.parquet' (FORMAT parquet);\n+\n+# result here depends on globbing order\n+statement maybe\n+COPY integers FROM '__TEST_DIR__/f*.parquet' (FORMAT parquet);\n+----\n+column count mismatch: expected 1 columns but found 2\n",
  "problem_statement": "COPY FROM not allowing union_by_name=True and not checking the schema from table\n### What happens?\n\nI am trying to run a COPY FROM command to pull data from Parquet files in S3 and load it into a PostgreSQL database. The S3 directory contains multiple files.\r\n\r\nI am encountering the following error:\r\n```\r\nfailed to cast column <column_name> from type INTEGER to VARCHAR[]: Unimplemented type for cast (INTEGER -> VARCHAR[])\r\n```\r\n\r\nAdditionally\r\n```\r\nThis can happen when reading multiple Parquet files. The schema information is taken from the first Parquet file by default. Possible solutions:\r\n* Enable the union_by_name=True option to combine the schema of all Parquet files (duckdb.org/docs/data/multiple_files/combining_schemas)\r\n* Use a COPY statement to automatically derive types from an existing table.\r\n```\n\n### To Reproduce\n\n1. Create a set of Parquet files using a CREATE TABLE command in Athena.\r\n2. Attach a PostgreSQL instance to DuckDB.\r\n3. Run a COPY FROM command similar to the following:\r\n```\r\nCOPY <db>.<schema>.<table_name>\r\nFROM '{s3_location}' (FORMAT PARQUET);\r\n```\r\n\r\n\r\nIssue : \r\nThere is no way to add union_by_name=True to the COPY FROM command, and this is already a COPY command.\n\n### OS:\n\nlinux\n\n### DuckDB Version:\n\n1.0.0\n\n### DuckDB Client:\n\npython\n\n### Full Name:\n\nArpit Aggarwal\n\n### Affiliation:\n\nCandor Health\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNo - I cannot share the data sets because they are confidential\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n",
  "hints_text": "Hi @arpit94, thanks for opening this issue. There are many moving parts here:\r\n* AWS Athena\r\n* Postgres connection\r\n* Parquet file\r\n\r\nIt seems to me that the Athena and Postgres are not required to reproduce the issue. Instead, the issue is that the following instruction works:\r\n\r\n```sql\r\nCREATE OR REPLACE TABLE tbl AS FROM read_parquet('https://duckdb.org/data/prices.parquet', union_by_name = true);\r\n```\r\n\r\nBut this one doesn't:\r\n```sql\r\nCOPY tbl FROM 'prices.parquet' (FORMAT PARQUET, UNION_BY_NAME true);\r\n```\r\n```console\r\nNot implemented Error: Unsupported option for COPY FROM parquet: UNION_BY_NAME\r\n```\r\n\r\nYou can work around your issue by first using a `CREATE [OR REPLACE] TABLE` command to create an interim table, then manually copy the content of that table to your target Postgres table.",
  "created_at": "2024-07-09T14:34:14Z"
}