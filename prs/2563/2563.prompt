You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
I can't read from a parquet file with C API
I have csv-file and parquet-file in a same folder and I can SELECT count(*) FROM both with DuckDB client.
```
D select count(*) from 'file.parquet';
┌──────────────┐
│ count_star() │
├──────────────┤
│ 21754        │
└──────────────┘
D select count(*) from 'salary.csv';
┌──────────────┐
│ count_star() │
├──────────────┤
│ 65           │
└──────────────┘
```
But if I try to same selects via C API the csv-file works nicely but the parquet-file gives the following error message:
`Catalog Error: Table with name file.parquet does not exist!`

I'm using libduckdb which is built with the following commands from `https://github.com/duckdb/duckdb/archive/refs/tags/v0.3.0.tar.gz`
```
cd build
cmake -DCMAKE_INSTALL_PREFIX=$prefix -DCMAKE_TOOLCHAIN_FILE=${CMAKE_TARGET_TOOLCHAIN} -DCMAKE_BUILD_TYPE=Release -DDISABLE_UNITY=TRUE -DENABLE_SANITIZER=FALSE -DBUILD_UNITTESTS=FALSE ..
make -j${nproc}
make install
```

What should I do to get a support for parquet-files for C API?

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16: </p>
17: 
18: ## DuckDB
19: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/docs/why_duckdb.html).
20: 
21: ## Installation
22: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
23: 
24: ## Data Import
25: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
26: 
27: ```sql
28: SELECT * FROM 'myfile.csv';
29: SELECT * FROM 'myfile.parquet';
30: ```
31: 
32: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
33: 
34: ## SQL Reference
35: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
36: 
37: ## Development
38: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
39: 
40: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
41: 
42: 
[end of README.md]
[start of .github/workflows/LinuxRelease.yml]
1: name: LinuxRelease
2: on: [push, pull_request]
3: 
4: defaults:
5:   run:
6:     shell: bash
7: 
8: env:
9:   GH_TOKEN: ${{ secrets.GH_TOKEN }}
10:   TWINE_PASSWORD: ${{ secrets.TWINE_PASSWORD }}
11:   AWS_ACCESS_KEY_ID: AKIAVBLKPL2ZW2T7TYFQ
12:   AWS_SECRET_ACCESS_KEY: ${{ secrets.NODE_PRE_GYP_SECRETACCESSKEY }}
13:   NODE_AUTH_TOKEN: ${{secrets.NODE_AUTH_TOKEN}}
14: 
15: jobs:
16:  linux-release-64:
17:     name: Linux (64 Bit)
18:     runs-on: ubuntu-latest
19:     container: ubuntu:16.04
20:     env:
21:       GEN: ninja
22:       BUILD_VISUALIZER: 1
23:       BUILD_BENCHMARK: 1
24:       BUILD_ICU: 1
25:       BUILD_TPCH: 1
26:       BUILD_TPCDS: 1
27:       BUILD_FTS: 1
28:       BUILD_REST: 1
29:       BUILD_JDBC: 1
30:       BUILD_HTTPFS: 1
31:       TREAT_WARNINGS_AS_ERRORS: 1
32:       FORCE_WARN_UNUSED: 1
33: 
34:     steps:
35:     - name: Install
36:       run: |
37:         apt-get update -y -qq
38:         apt-get install -y -qq software-properties-common
39:         add-apt-repository ppa:deadsnakes/ppa
40:         add-apt-repository ppa:git-core/ppa
41:         apt-get update -y -qq
42:         apt-get install -y -qq git ninja-build make gcc-multilib g++-multilib libssl-dev wget openjdk-8-jdk python3.7 zip python3-pip maven
43:         python3.7 -m pip install pip
44:         python3.7 -m pip install requests
45: 
46:     - uses: actions/checkout@v2
47:       with:
48:         fetch-depth: 0
49: 
50:     - name: Version Check
51:       run: |
52:         ldd --version ldd
53:         python3.7 --version
54:         git --version
55: 
56:     - name: Install CMake
57:       run: |
58:         wget https://github.com/Kitware/CMake/releases/download/v3.21.3/cmake-3.21.3-linux-x86_64.sh
59:         chmod +x cmake-3.21.3-linux-x86_64.sh
60:         ./cmake-3.21.3-linux-x86_64.sh --skip-license --prefix=/usr/local
61: 
62:     - name: Build
63:       run: STATIC_LIBCPP=1 make
64: 
65:     - name: Test
66:       run: make allunit
67: 
68:     - name: Symbol Leakage Test
69:       run: python3.7 scripts/exported_symbols_check.py build/release/src/libduckdb*.so
70: 
71:     - name: Tools Tests
72:       run: |
73:         python3.7 tools/shell/shell-test.py build/release/duckdb
74:         python3.7 tools/rest/test_the_rest.py build/release/tools/rest
75:         java -cp build/release/tools/jdbc/duckdb_jdbc.jar org.duckdb.test.TestDuckDBJDBC
76: 
77:     - name: Examples
78:       run: |
79:         (cd examples/embedded-c; make)
80:         (cd examples/embedded-c++; make)
81:         (cd examples/jdbc; make; make maven)
82:         build/release/benchmark/benchmark_runner benchmark/tpch/sf1/q01.benchmark
83: 
84:     - name: Deploy
85:       run: |
86:         python3.7 scripts/amalgamation.py
87:         zip -j duckdb_cli-linux-amd64.zip build/release/duckdb
88:         zip -j libduckdb-linux-amd64.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
89:         zip -j libduckdb-src.zip src/amalgamation/duckdb.hpp src/amalgamation/duckdb.cpp src/include/duckdb.h
90:         zip -j duckdb_rest-linux-amd64.zip build/release/tools/rest/duckdb_rest_server
91:         python3.7 scripts/asset-upload-gha.py libduckdb-src.zip libduckdb-linux-amd64.zip duckdb_cli-linux-amd64.zip duckdb_rest-linux-amd64.zip duckdb_jdbc-linux-amd64.jar=build/release/tools/jdbc/duckdb_jdbc.jar
92: 
93:     - uses: actions/upload-artifact@v2
94:       with:
95:         name: duckdb-binaries-linux
96:         path: |
97:           libduckdb-linux-amd64.zip
98:           duckdb_cli-linux-amd64.zip
99:           build/release/tools/jdbc/duckdb_jdbc.jar
100: 
101: 
102:  linux-release-32:
103:     name: Linux (32 Bit)
104:     runs-on: ubuntu-latest
105:     container: ubuntu:16.04
106:     needs: linux-release-64
107:     env:
108:       GEN: ninja
109: 
110:     steps:
111:     - name: Install
112:       run: |
113:         apt-get update -y -qq
114:         apt-get install -y -qq software-properties-common
115:         add-apt-repository ppa:deadsnakes/ppa
116:         add-apt-repository ppa:git-core/ppa
117:         apt-get update -y -qq
118:         apt-get install -y -qq git make ninja-build libc6-dev-i386 gcc-multilib g++-multilib lib32readline6-dev libssl-dev wget openjdk-8-jdk python3.7 zip
119: 
120:     - uses: actions/checkout@v2
121:       with:
122:         fetch-depth: 0
123: 
124:     - name: Version Check
125:       run: |
126:         ldd --version ldd
127:         python3.7 --version
128:         git --version
129: 
130:     - name: Install CMake
131:       run: |
132:         wget https://github.com/Kitware/CMake/releases/download/v3.21.3/cmake-3.21.3-linux-x86_64.sh
133:         chmod +x cmake-3.21.3-linux-x86_64.sh
134:         ./cmake-3.21.3-linux-x86_64.sh --skip-license --prefix=/usr/local
135: 
136:     - name: Build
137:       run: |
138:         mkdir -p build/release
139:         (cd build/release && cmake -DSTATIC_LIBCPP=1 -DJDBC_DRIVER=1 -DBUILD_ICU_EXTENSION=1 -DBUILD_PARQUET_EXTENSION=1 -DBUILD_FTS_EXTENSION=1 -DFORCE_32_BIT=1 -DCMAKE_BUILD_TYPE=Release ../.. && cmake --build .)
140: 
141:     - name: Test
142:       run: build/release/test/unittest "*"
143: 
144:     - name: Deploy
145:       run: |
146:         python3.7 scripts/amalgamation.py
147:         zip -j duckdb_cli-linux-i386.zip build/release/duckdb
148:         zip -j libduckdb-linux-i386.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
149:         python3.7 scripts/asset-upload-gha.py libduckdb-linux-i386.zip duckdb_cli-linux-i386.zip duckdb_jdbc-linux-i386.jar=build/release/tools/jdbc/duckdb_jdbc.jar
150: 
151:     - uses: actions/upload-artifact@v2
152:       with:
153:         name: duckdb-binaries-linux
154:         path: |
155:           libduckdb-linux-i386.zip
156:           duckdb_cli-linux-i386.zip
157:           build/release/tools/jdbc/duckdb_jdbc.jar
158: 
159: 
160:  linux-rpi:
161:     name: Linux (Raspberry Pi)
162:     runs-on: ubuntu-20.04
163:     needs: linux-release-64
164:     steps:
165:     - uses: actions/checkout@v2
166:       with:
167:         fetch-depth: 0
168: 
169:     - uses: actions/setup-python@v2
170:       with:
171:         python-version: '3.7'
172: 
173:     - name: Install
174:       run: |
175:         sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
176:         git clone https://github.com/raspberrypi/tools --depth=1 rpi-tools
177: 
178:     - name: Build
179:       run: |
180:         export TOOLCHAIN=`pwd`/rpi-tools
181:         mkdir -p build/release
182:         cd build/release
183:         cmake -G Ninja -DBUILD_TPCH_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1 -DDUCKDB_RPI_TOOLCHAIN_PREFIX=$TOOLCHAIN -DBUILD_UNITTESTS=0 -DCMAKE_TOOLCHAIN_FILE=../../scripts/raspberry-pi-cmake-toolchain.cmake ../../
184:         cmake --build .
185:         file duckdb
186: 
187:     - name: Deploy
188:       run: |
189:         python scripts/amalgamation.py
190:         zip -j duckdb_cli-linux-rpi.zip build/release/duckdb
191:         zip -j libduckdb-linux-rpi.zip build/release/src/libduckdb*.so src/amalgamation/duckdb.hpp src/include/duckdb.h
192:         python scripts/asset-upload-gha.py libduckdb-linux-rpi.zip duckdb_cli-linux-rpi.zip
193: 
194:     - uses: actions/upload-artifact@v2
195:       with:
196:         name: duckdb-binaries-rpi
197:         path: |
198:           libduckdb-linux-rpi.zip
199:           duckdb_cli-linux-rpi.zip
200: 
201: 
202: 
203: 
204:  old-gcc:
205:     name: GCC 4.8
206:     runs-on: ubuntu-18.04
207:     needs: linux-release-64
208: 
209:     env:
210:       CC: gcc-4.8
211:       CXX: g++-4.8
212: 
213:     steps:
214:     - uses: actions/checkout@v2
215:       with:
216:         fetch-depth: 0
217: 
218:     - uses: actions/setup-python@v2
219:       with:
220:         python-version: '3.7'
221: 
222:     - name: Install
223:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq g++-4.8 binutils
224: 
225:     - name: Build
226:       run: make release
227: 
228:     - name: Test
229:       run: make allunit
230: 
231:  centos:
232:     name: CentOS 7
233:     runs-on: ubuntu-latest
234:     container: centos:7
235:     needs: linux-release-64
236:     steps:
237:     - uses: actions/checkout@v2
238:       with:
239:         fetch-depth: 0
240: 
241:     - name: Install
242:       run: yum install -y gcc gcc-c++ git cmake make
243: 
244:     - name: Build
245:       run: make release
246: 
247:     - name: Test
248:       run: ./build/release/test/unittest
249: 
250:  release-assert:
251:     name: Release Assertions
252:     runs-on: ubuntu-20.04
253:     needs: linux-release-64
254:     env:
255:       CC: gcc-10
256:       CXX: g++-10
257:       GEN: ninja
258:       BUILD_ICU: 1
259:       BUILD_TPCH: 1
260:       BUILD_TPCDS: 1
261:       BUILD_FTS: 1
262:       BUILD_VISUALIZER: 1
263:       DISABLE_SANITIZER: 1
264: 
265:     steps:
266:     - uses: actions/checkout@v2
267:       with:
268:         fetch-depth: 0
269: 
270:     - name: Install
271:       run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
272: 
273:     - name: Build
274:       run: make relassert
275: 
276:     - name: Test
277:       run: |
278:           python3 scripts/run_tests_one_by_one.py build/relassert/test/unittest "*"
279: 
280: 
281: 
282: 
283:  vector-sizes:
284:     name: Vector Sizes
285:     runs-on: ubuntu-20.04
286:     needs: linux-release-64
287:     env:
288:       CC: gcc-10
289:       CXX: g++-10
290: 
291:     steps:
292:     - uses: actions/checkout@v2
293:       with:
294:         fetch-depth: 0
295: 
296:     - uses: actions/setup-python@v2
297:       with:
298:         python-version: '3.7'
299: 
300:     - name: Test
301:       run: python scripts/test_vector_sizes.py
302: 
303:  linux-wasm-release:
304:     name: WebAssembly Release
305:     runs-on: ubuntu-20.04
306:     needs: linux-release-64
307:     steps:
308:     - uses: actions/checkout@v2
309:       with:
310:         fetch-depth: 0
311: 
312:     - name: Build Amalgamation
313:       run: python scripts/amalgamation.py
314: 
315:     - name: Setup
316:       run: ./scripts/wasm_configure.sh
317: 
318:     - name: Build Library Module
319:       run: ./scripts/wasm_build_lib.sh Release
320: 
321:     - name: Build Test Module
322:       run: ./scripts/wasm_build_test.sh Release
323: 
324:     - name: Test WASM Module
325:       run: node ./test/wasm/hello_wasm_test.js
326: 
327:     - name: Package
328:       run: |
329:         zip -j duckdb-wasm32-nothreads.zip ./.wasm/build/duckdb.wasm
330:         python scripts/asset-upload-gha.py duckdb-wasm32-nothreads.zip
331: 
332:     - uses: actions/upload-artifact@v2
333:       with:
334:         name: duckdb-wasm32-nothreads
335:         path: |
336:           duckdb-wasm32-nothreads.zip
337: 
[end of .github/workflows/LinuxRelease.yml]
[start of .github/workflows/cifuzz.yml]
1: name: CIFuzz
2: on: [pull_request]
3: jobs:
4:   Fuzzing:
5:     runs-on: ubuntu-latest
6:     steps:
7:     - name: Build Fuzzers
8:       id: build
9:       uses: google/oss-fuzz/infra/cifuzz/actions/build_fuzzers@master
10:       with:
11:         oss-fuzz-project-name: 'duckdb'
12:         dry-run: false
13:     - name: Run Fuzzers
14:       uses: google/oss-fuzz/infra/cifuzz/actions/run_fuzzers@master
15:       with:
16:         oss-fuzz-project-name: 'duckdb'
17:         fuzz-seconds: 600
18:         dry-run: false
19:     - name: Upload Crash
20:       uses: actions/upload-artifact@v1
21:       if: failure() && steps.build.outcome == 'success'
22:       with:
23:         name: artifacts
24:         path: ./out/artifacts
[end of .github/workflows/cifuzz.yml]
[start of .github/workflows/lcov_exclude]
1: /usr*
2: */cl.hpp
3: */tools/jdbc/*
4: */tools/nodejs/*
5: */tools/odbc/*
6: */tools/rest/*
7: */tools/rpkg/*
8: */tools/shell/*
9: */tools/sqlite3_api_wrapper/*
10: */benchmark/*
11: */examples/*
12: */third_party/*
13: */test/*
14: */extension/fts/*
15: */extension/icu/*
16: */extension/tpcds/*
17: */extension/tpch/*
18: */extension/visualizer/*
19: */extension/extension_helper.hpp
[end of .github/workflows/lcov_exclude]
[start of Makefile]
1: .PHONY: all opt unit clean debug release release_expanded test unittest allunit docs doxygen format sqlite imdb
2: 
3: all: release
4: opt: release
5: unit: unittest
6: imdb: third_party/imdb/data
7: 
8: GENERATOR=
9: FORCE_COLOR=
10: WARNINGS_AS_ERRORS=
11: FORCE_WARN_UNUSED_FLAG=
12: DISABLE_UNITY_FLAG=
13: DISABLE_SANITIZER_FLAG=
14: ifeq ($(GEN),ninja)
15: 	GENERATOR=-G "Ninja"
16: 	FORCE_COLOR=-DFORCE_COLORED_OUTPUT=1
17: endif
18: ifeq (${TREAT_WARNINGS_AS_ERRORS}, 1)
19: 	WARNINGS_AS_ERRORS=-DTREAT_WARNINGS_AS_ERRORS=1
20: endif
21: ifeq (${FORCE_WARN_UNUSED}, 1)
22: 	FORCE_WARN_UNUSED_FLAG=-DFORCE_WARN_UNUSED=1
23: endif
24: ifeq (${DISABLE_UNITY}, 1)
25: 	DISABLE_UNITY_FLAG=-DDISABLE_UNITY=1
26: endif
27: ifeq (${DISABLE_SANITIZER}, 1)
28: 	DISABLE_SANITIZER_FLAG=-DENABLE_SANITIZER=FALSE -DENABLE_UBSAN=0
29: endif
30: ifeq (${DISABLE_UBSAN}, 1)
31: 	DISABLE_SANITIZER_FLAG=-DENABLE_UBSAN=0
32: endif
33: ifeq (${DISABLE_VPTR_SANITIZER}, 1)
34: 	DISABLE_SANITIZER_FLAG:=${DISABLE_SANITIZER_FLAG} -DDISABLE_VPTR_SANITIZER=1
35: endif
36: ifeq (${FORCE_SANITIZER}, 1)
37: 	DISABLE_SANITIZER_FLAG:=${DISABLE_SANITIZER_FLAG} -DFORCE_SANITIZER=1
38: endif
39: ifeq (${THREADSAN}, 1)
40: 	DISABLE_SANITIZER_FLAG:=${DISABLE_SANITIZER_FLAG} -DENABLE_THREAD_SANITIZER=1
41: endif
42: ifeq (${STATIC_LIBCPP}, 1)
43: 	STATIC_LIBCPP=-DSTATIC_LIBCPP=TRUE
44: endif
45: EXTENSIONS=-DBUILD_PARQUET_EXTENSION=TRUE
46: ifeq (${BUILD_BENCHMARK}, 1)
47: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_BENCHMARKS=1
48: endif
49: ifeq (${BUILD_ICU}, 1)
50: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_ICU_EXTENSION=1
51: endif
52: ifeq (${BUILD_TPCH}, 1)
53: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_TPCH_EXTENSION=1
54: endif
55: ifeq (${BUILD_TPCDS}, 1)
56: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_TPCDS_EXTENSION=1
57: endif
58: ifeq (${BUILD_FTS}, 1)
59: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_FTS_EXTENSION=1
60: endif
61: ifeq (${BUILD_VISUALIZER}, 1)
62: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_VISUALIZER_EXTENSION=1
63: endif
64: ifeq (${BUILD_HTTPFS}, 1)
65: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_HTTPFS_EXTENSION=1
66: endif
67: ifeq (${BUILD_SQLSMITH}, 1)
68: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_SQLSMITH=1
69: endif
70: ifeq (${BUILD_TPCE}, 1)
71: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_TPCE=1
72: endif
73: ifeq (${BUILD_JDBC}, 1)
74: 	EXTENSIONS:=${EXTENSIONS} -DJDBC_DRIVER=1
75: endif
76: ifeq (${BUILD_ODBC}, 1)
77: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_ODBC_DRIVER=1
78: endif
79: ifeq (${BUILD_PYTHON}, 1)
80: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_PYTHON=1 -DBUILD_FTS_EXTENSION=1 -DBUILD_TPCH_EXTENSION=1 -DBUILD_VISUALIZER_EXTENSION=1 -DBUILD_TPCDS_EXTENSION=1
81: endif
82: ifeq (${BUILD_R}, 1)
83: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_R=1
84: endif
85: ifeq (${CONFIGURE_R}, 1)
86: 	EXTENSIONS:=${EXTENSIONS} -DCONFIGURE_R=1
87: endif
88: ifeq (${BUILD_REST}, 1)
89: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_REST=1
90: endif
91: ifneq ($(TIDY_THREADS),)
92: 	TIDY_THREAD_PARAMETER := -j ${TIDY_THREADS}
93: endif
94: ifeq ($(BUILD_ARROW_ABI_TEST), 1)
95: 	EXTENSIONS:=${EXTENSIONS} -DBUILD_ARROW_ABI_TEST=1
96: endif
97: ifneq ("${FORCE_QUERY_LOG}a", "a")
98: 	EXTENSIONS:=${EXTENSIONS} -DFORCE_QUERY_LOG=${FORCE_QUERY_LOG}
99: endif
100: 
101: clean:
102: 	rm -rf build
103: 
104: debug:
105: 	mkdir -p build/debug && \
106: 	cd build/debug && \
107: 	cmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${DISABLE_UNITY_FLAG} ${DISABLE_SANITIZER_FLAG} ${STATIC_LIBCPP} ${EXTENSIONS} -DCMAKE_BUILD_TYPE=Debug ../.. && \
108: 	cmake --build .
109: 
110: release_expanded:
111: 	mkdir -p build/release_expanded && \
112: 	cd build/release_expanded && \
113: 	cmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${FORCE_WARN_UNUSED_FLAG} ${DISABLE_UNITY_FLAG} ${DISABLE_SANITIZER_FLAG} ${STATIC_LIBCPP} ${EXTENSIONS} -DCMAKE_BUILD_TYPE=Release ../.. && \
114: 	cmake --build .
115: 
116: cldebug:
117: 	mkdir -p build/cldebug && \
118: 	cd build/cldebug && \
119: 	cmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${DISABLE_UNITY_FLAG} ${EXTENSIONS} -DBUILD_PYTHON=1 -DBUILD_R=1 -DENABLE_SANITIZER=0 -DENABLE_UBSAN=0 -DCMAKE_BUILD_TYPE=Debug ../.. && \
120: 	cmake --build .
121: 
122: clreldebug:
123: 	mkdir -p build/clreldebug && \
124: 	cd build/clreldebug && \
125: 	cmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${DISABLE_UNITY_FLAG} ${STATIC_LIBCPP} ${EXTENSIONS} -DBUILD_PYTHON=1 -DBUILD_R=1 -DBUILD_FTS_EXTENSION=1 -DENABLE_SANITIZER=0 -DENABLE_UBSAN=0 -DCMAKE_BUILD_TYPE=RelWithDebInfo ../.. && \
126: 	cmake --build .
127: 
128: unittest: debug
129: 	build/debug/test/unittest
130: 	build/debug/tools/sqlite3_api_wrapper/test_sqlite3_api_wrapper
131: 
132: unittestci:
133: 	python3 scripts/run_tests_one_by_one.py build/debug/test/unittest
134: 	build/debug/tools/sqlite3_api_wrapper/test_sqlite3_api_wrapper
135: 
136: unittestarrow:
137: 	build/debug/test/unittest "[arrow]"
138: 
139: 
140: allunit: release_expanded # uses release build because otherwise allunit takes forever
141: 	build/release_expanded/test/unittest "*"
142: 
143: docs:
144: 	mkdir -p build/docs && \
145: 	doxygen Doxyfile
146: 
147: doxygen: docs
148: 	open build/docs/html/index.html
149: 
150: release:
151: 	mkdir -p build/release && \
152: 	cd build/release && \
153: 	cmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${FORCE_WARN_UNUSED_FLAG} ${DISABLE_UNITY_FLAG} ${DISABLE_SANITIZER_FLAG} ${STATIC_LIBCPP} ${EXTENSIONS} -DCMAKE_BUILD_TYPE=Release ../.. && \
154: 	cmake --build .
155: 
156: reldebug:
157: 	mkdir -p build/reldebug && \
158: 	cd build/reldebug && \
159: 	cmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${DISABLE_UNITY_FLAG} ${DISABLE_SANITIZER_FLAG} ${STATIC_LIBCPP} ${EXTENSIONS} -DCMAKE_BUILD_TYPE=RelWithDebInfo ../.. && \
160: 	cmake --build .
161: 
162: relassert:
163: 	mkdir -p build/relassert && \
164: 	cd build/relassert && \
165: 	cmake $(GENERATOR) $(FORCE_COLOR) ${WARNINGS_AS_ERRORS} ${DISABLE_UNITY_FLAG} ${DISABLE_SANITIZER_FLAG} ${STATIC_LIBCPP} ${EXTENSIONS} -DFORCE_ASSERT=1 -DCMAKE_BUILD_TYPE=RelWithDebInfo ../.. && \
166: 	cmake --build .
167: 
168: amaldebug:
169: 	mkdir -p build/amaldebug && \
170: 	python scripts/amalgamation.py && \
171: 	cd build/amaldebug && \
172: 	cmake $(GENERATOR) $(FORCE_COLOR) ${STATIC_LIBCPP} ${EXTENSIONS} -DAMALGAMATION_BUILD=1 -DCMAKE_BUILD_TYPE=Debug ../.. && \
173: 	cmake --build .
174: 
175: tidy-check:
176: 	mkdir -p build/tidy && \
177: 	cd build/tidy && \
178: 	cmake -DCLANG_TIDY=1 -DDISABLE_UNITY=1 -DBUILD_ODBC_DRIVER=TRUE -DBUILD_PARQUET_EXTENSION=TRUE -DBUILD_PYTHON_PKG=TRUE -DBUILD_SHELL=0 -DCMAKE_EXPORT_COMPILE_COMMANDS=ON ../.. && \
179: 	python3 ../../scripts/run-clang-tidy.py -quiet ${TIDY_THREAD_PARAMETER}
180: 
181: tidy-fix:
182: 	mkdir -p build/tidy && \
183: 	cd build/tidy && \
184: 	cmake -DCLANG_TIDY=1 -DDISABLE_UNITY=1 -DBUILD_PARQUET_EXTENSION=TRUE -DBUILD_SHELL=0 -DCMAKE_EXPORT_COMPILE_COMMANDS=ON ../.. && \
185: 	python3 ../../scripts/run-clang-tidy.py -fix
186: 
187: test_compile: # test compilation of individual cpp files
188: 	python scripts/amalgamation.py --compile
189: 
190: format-check:
191: 	python3 scripts/format.py --all --check
192: 
193: format-check-silent:
194: 	python3 scripts/format.py --all --check --silent
195: 
196: format-fix:
197: 	python3 scripts/format.py --all --fix --noconfirm
198: 
199: format-head:
200: 	python3 scripts/format.py HEAD --fix --noconfirm
201: 
202: format-changes:
203: 	python3 scripts/format.py HEAD --fix --noconfirm
204: 
205: format-master:
206: 	python3 scripts/format.py master --fix --noconfirm
207: 
208: third_party/sqllogictest:
209: 	git clone --depth=1 https://github.com/cwida/sqllogictest.git third_party/sqllogictest
210: 
211: third_party/imdb/data:
212: 	wget -i "http://download.duckdb.org/imdb/list.txt" -P third_party/imdb/data
213: 
214: sqlite: release_expanded | third_party/sqllogictest
215: 	git --git-dir third_party/sqllogictest/.git pull
216: 	./build/release_expanded/test/unittest "[sqlitelogic]"
217: 
218: sqlsmith: debug
219: 	./build/debug/third_party/sqlsmith/sqlsmith --duckdb=:memory:
220: 
221: clangd:
222: 	mkdir -p ./build/clangd && \
223: 	cd ./build/clangd && \
224: 	cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=1 ../.. && \
225: 	cd ../.. && \
226: 	ln -sf ./build/clangd/compile_commands.json ./compile_commands.json
[end of Makefile]
[start of benchmark/CMakeLists.txt]
1: include_directories(../third_party/catch)
2: include_directories(../third_party/tpce-tool/include)
3: include_directories(../third_party/sqlite/include)
4: include_directories(../test/include)
5: include_directories(include)
6: 
7: add_subdirectory(micro)
8: if(${BUILD_TPCH_EXTENSION})
9:   add_subdirectory(tpch)
10: endif()
11: add_subdirectory(imdb)
12: 
13: add_extension_definitions()
14: add_definitions(-DDUCKDB_ROOT_DIRECTORY="${PROJECT_SOURCE_DIR}")
15: 
16: add_executable(benchmark_runner benchmark_runner.cpp interpreted_benchmark.cpp
17:                                 ${BENCHMARK_OBJECT_FILES})
18: 
19: target_link_libraries(benchmark_runner duckdb imdb test_helpers)
20: 
21: link_extension_libraries(benchmark_runner)
22: 
23: if(${BUILD_TPCE})
24:   target_link_libraries(benchmark_runner tpce)
25: endif()
[end of benchmark/CMakeLists.txt]
[start of benchmark/interpreted_benchmark.cpp]
1: #include "interpreted_benchmark.hpp"
2: #include "benchmark_runner.hpp"
3: #include "duckdb.hpp"
4: 
5: #include <fstream>
6: #include <sstream>
7: #include "duckdb/main/query_profiler.hpp"
8: #include "duckdb/common/string_util.hpp"
9: #include "duckdb/main/client_context.hpp"
10: #include "extension_helper.hpp"
11: 
12: namespace duckdb {
13: 
14: static string ParseGroupFromPath(string file) {
15: 	string extension = "";
16: 	// move backwards to the last slash
17: 	int group_begin = -1, group_end = -1;
18: 	for (size_t i = file.size(); i > 0; i--) {
19: 		if (file[i - 1] == '/' || file[i - 1] == '\\') {
20: 			if (group_end == -1) {
21: 				group_end = i - 1;
22: 			} else {
23: 				group_begin = i;
24: 				return "[" + file.substr(group_begin, group_end - group_begin) + "]" + extension;
25: 			}
26: 		}
27: 	}
28: 	if (group_end == -1) {
29: 		return "[" + file + "]" + extension;
30: 	}
31: 	return "[" + file.substr(0, group_end) + "]" + extension;
32: }
33: 
34: struct InterpretedBenchmarkState : public BenchmarkState {
35: 	DuckDB db;
36: 	Connection con;
37: 	unique_ptr<MaterializedQueryResult> result;
38: 	InterpretedBenchmarkState() : db(nullptr), con(db) {
39: 		con.EnableProfiling();
40: 		auto &instance = BenchmarkRunner::GetInstance();
41: 		auto res = con.Query("PRAGMA threads=" + to_string(instance.threads));
42: 		D_ASSERT(res->success);
43: 	}
44: };
45: 
46: struct BenchmarkFileReader {
47: 	BenchmarkFileReader(string path_, unordered_map<std::string, std::string> replacement_map)
48: 	    : path(path_), infile(path), linenr(0), replacements(replacement_map) {
49: 	}
50: 
51: public:
52: 	bool ReadLine(std::string &line) {
53: 		if (!std::getline(infile, line)) {
54: 			return false;
55: 		}
56: 		linenr++;
57: 		for (auto &replacement : replacements) {
58: 			line = StringUtil::Replace(line, "${" + replacement.first + "}", replacement.second);
59: 		}
60: 		StringUtil::Trim(line);
61: 		return true;
62: 	}
63: 
64: 	int LineNumber() {
65: 		return linenr;
66: 	}
67: 
68: 	std::string FormatException(string exception_msg) {
69: 		return path + ":" + std::to_string(linenr) + " - " + exception_msg;
70: 	}
71: 
72: private:
73: 	std::string path;
74: 	std::ifstream infile;
75: 	int linenr;
76: 	unordered_map<std::string, std::string> replacements;
77: };
78: 
79: InterpretedBenchmark::InterpretedBenchmark(string full_path)
80:     : Benchmark(true, full_path, ParseGroupFromPath(full_path)), benchmark_path(full_path) {
81: 	replacement_mapping["BENCHMARK_DIR"] = BenchmarkRunner::DUCKDB_BENCHMARK_DIRECTORY;
82: }
83: 
84: void InterpretedBenchmark::LoadBenchmark() {
85: 	if (is_loaded) {
86: 		return;
87: 	}
88: 	BenchmarkFileReader reader(benchmark_path, replacement_mapping);
89: 	string line;
90: 	while (reader.ReadLine(line)) {
91: 		// skip blank lines and comments
92: 		if (line.empty() || line[0] == '#') {
93: 			continue;
94: 		}
95: 		// look for a command in this line
96: 		auto splits = StringUtil::Split(StringUtil::Lower(line), ' ');
97: 		if (splits[0] == "load" || splits[0] == "run" || splits[0] == "init" || splits[0] == "cleanup") {
98: 			if (queries.find(splits[0]) != queries.end()) {
99: 				throw std::runtime_error("Multiple calls to " + splits[0] + " in the same benchmark file");
100: 			}
101: 			// load command: keep reading until we find a blank line or EOF
102: 			string query;
103: 			while (reader.ReadLine(line)) {
104: 				if (line.empty()) {
105: 					break;
106: 				} else {
107: 					query += line + " ";
108: 				}
109: 			}
110: 			if (splits.size() > 1 && !splits[1].empty()) {
111: 				// read entire file into query
112: 				std::ifstream file(splits[1], std::ios::ate);
113: 				std::streamsize size = file.tellg();
114: 				file.seekg(0, std::ios::beg);
115: 				if (size < 0) {
116: 					throw std::runtime_error("Failed to read " + splits[0] + " from file " + splits[1]);
117: 				}
118: 
119: 				auto buffer = unique_ptr<char[]>(new char[size]);
120: 				if (!file.read(buffer.get(), size)) {
121: 					throw std::runtime_error("Failed to read " + splits[0] + " from file " + splits[1]);
122: 				}
123: 				query = string(buffer.get(), size);
124: 			}
125: 			StringUtil::Trim(query);
126: 			if (query.empty()) {
127: 				throw std::runtime_error("Encountered an empty " + splits[0] + " node!");
128: 			}
129: 			queries[splits[0]] = query;
130: 		} else if (splits[0] == "require") {
131: 			if (splits.size() != 2) {
132: 				throw std::runtime_error(reader.FormatException("require requires a single parameter"));
133: 			}
134: 			extensions.insert(splits[1]);
135: 		} else if (splits[0] == "cache") {
136: 			if (splits.size() != 2) {
137: 				throw std::runtime_error(reader.FormatException("cache requires a single parameter"));
138: 			}
139: 			data_cache = splits[1];
140: 		} else if (splits[0] == "name" || splits[0] == "group" || splits[0] == "subgroup") {
141: 			if (splits.size() == 1) {
142: 				throw std::runtime_error(reader.FormatException(splits[0] + " requires a parameter"));
143: 			}
144: 			string result = line.substr(splits[0].size() + 1, line.size() - 1);
145: 			StringUtil::Trim(result);
146: 			if (splits[0] == "name") {
147: 				display_name = result;
148: 			} else if (splits[0] == "group") {
149: 				display_group = result;
150: 			} else {
151: 				subgroup = result;
152: 			}
153: 		} else if (splits[0] == "result") {
154: 			if (result_column_count > 0) {
155: 				throw std::runtime_error(reader.FormatException("multiple results found"));
156: 			}
157: 			// count the amount of columns
158: 			if (splits.size() <= 1 || splits[1].size() == 0) {
159: 				throw std::runtime_error(
160: 				    reader.FormatException("result must be followed by a column count (e.g. result III) or a file "
161: 				                           "(e.g. result /path/to/file.csv)"));
162: 			}
163: 			bool is_file = false;
164: 			for (idx_t i = 0; i < splits[1].size(); i++) {
165: 				if (splits[1][i] != 'i') {
166: 					is_file = true;
167: 					break;
168: 				}
169: 			}
170: 			if (is_file) {
171: 				// read the results from the file
172: 				result_column_count = -1;
173: 				std::ifstream csv_infile(splits[1]);
174: 				bool skipped_header = false;
175: 				idx_t line_number = 0;
176: 				while (std::getline(csv_infile, line)) {
177: 					line_number++;
178: 					if (line.empty()) {
179: 						break;
180: 					}
181: 					if (!skipped_header) {
182: 						skipped_header = true;
183: 						continue;
184: 					}
185: 					auto result_splits = StringUtil::Split(line, "|");
186: 					if (result_column_count < 0) {
187: 						result_column_count = result_splits.size();
188: 					} else if (idx_t(result_column_count) != result_splits.size()) {
189: 						throw std::runtime_error("error in file " + splits[1] +
190: 						                         ", inconsistent amount of rows in CSV on line " +
191: 						                         to_string(line_number));
192: 					}
193: 					result_values.push_back(move(result_splits));
194: 				}
195: 
196: 				// read the main file until we encounter an empty line
197: 				while (reader.ReadLine(line)) {
198: 					if (line.empty()) {
199: 						break;
200: 					}
201: 				}
202: 			} else {
203: 				result_column_count = splits[1].size();
204: 				// keep reading results until eof
205: 				while (reader.ReadLine(line)) {
206: 					if (line.empty()) {
207: 						break;
208: 					}
209: 					auto result_splits = StringUtil::Split(line, "\t");
210: 					if ((int64_t)result_splits.size() != result_column_count) {
211: 						throw std::runtime_error(
212: 						    reader.FormatException("expected " + std::to_string(result_splits.size()) +
213: 						                           " values but got " + std::to_string(result_column_count)));
214: 					}
215: 					result_values.push_back(move(result_splits));
216: 				}
217: 			}
218: 		} else if (splits[0] == "template") {
219: 			// template: update the path to read
220: 			benchmark_path = splits[1];
221: 			// now read parameters
222: 			while (reader.ReadLine(line)) {
223: 				if (line.empty()) {
224: 					break;
225: 				}
226: 				auto parameters = StringUtil::Split(line, '=');
227: 				if (parameters.size() != 2) {
228: 					throw std::runtime_error(
229: 					    reader.FormatException("Expected a template parameter in the form of X=Y"));
230: 				}
231: 				replacement_mapping[parameters[0]] = parameters[1];
232: 			}
233: 			// restart the load from the template file
234: 			LoadBenchmark();
235: 			return;
236: 		} else {
237: 			throw std::runtime_error(reader.FormatException("unrecognized command " + splits[0]));
238: 		}
239: 	}
240: 	// set up the queries
241: 	if (queries.find("run") == queries.end()) {
242: 		throw Exception("Invalid benchmark file: no \"run\" query specified");
243: 	}
244: 	run_query = queries["run"];
245: 	is_loaded = true;
246: }
247: 
248: unique_ptr<BenchmarkState> InterpretedBenchmark::Initialize(BenchmarkConfiguration &config) {
249: 	unique_ptr<QueryResult> result;
250: 	LoadBenchmark();
251: 	auto state = make_unique<InterpretedBenchmarkState>();
252: 	for (auto &extension : extensions) {
253: 		auto result = ExtensionHelper::LoadExtension(state->db, extension);
254: 		if (result == ExtensionLoadResult::EXTENSION_UNKNOWN) {
255: 			throw std::runtime_error("Unknown extension " + extension);
256: 		} else if (result == ExtensionLoadResult::NOT_LOADED) {
257: 			throw std::runtime_error("Extension " + extension +
258: 			                         " is not available/was not compiled. Cannot run this benchmark.");
259: 		}
260: 	}
261: 
262: 	if (queries.find("init") != queries.end()) {
263: 		string init_query = queries["init"];
264: 		result = state->con.Query(init_query);
265: 		while (result) {
266: 			if (!result->success) {
267: 				throw Exception(result->error);
268: 			}
269: 			result = move(result->next);
270: 		}
271: 	}
272: 
273: 	string load_query;
274: 	if (queries.find("load") != queries.end()) {
275: 		load_query = queries["load"];
276: 	}
277: 
278: 	if (data_cache.empty()) {
279: 		// no cache specified: just run the initialization code
280: 		result = state->con.Query(load_query);
281: 	} else {
282: 		// cache specified: try to load the cache
283: 		if (!BenchmarkRunner::TryLoadDatabase(state->db, data_cache)) {
284: 			// failed to load: write the cache
285: 			result = state->con.Query(load_query);
286: 			BenchmarkRunner::SaveDatabase(state->db, data_cache);
287: 		}
288: 	}
289: 	while (result) {
290: 		if (!result->success) {
291: 			throw Exception(result->error);
292: 		}
293: 		result = move(result->next);
294: 	}
295: 	if (config.profile_info == BenchmarkProfileInfo::NORMAL) {
296: 		state->con.Query("PRAGMA enable_profiling");
297: 	} else if (config.profile_info == BenchmarkProfileInfo::DETAILED) {
298: 		state->con.Query("PRAGMA enable_profiling");
299: 		state->con.Query("PRAGMA profiling_mode='detailed'");
300: 	}
301: 	return state;
302: }
303: 
304: string InterpretedBenchmark::GetQuery() {
305: 	LoadBenchmark();
306: 	return run_query;
307: }
308: 
309: void InterpretedBenchmark::Run(BenchmarkState *state_p) {
310: 	auto &state = (InterpretedBenchmarkState &)*state_p;
311: 	state.result = state.con.Query(run_query);
312: }
313: 
314: void InterpretedBenchmark::Cleanup(BenchmarkState *state_p) {
315: 	auto &state = (InterpretedBenchmarkState &)*state_p;
316: 	if (queries.find("cleanup") != queries.end()) {
317: 		unique_ptr<QueryResult> result;
318: 		string cleanup_query = queries["cleanup"];
319: 		result = state.con.Query(cleanup_query);
320: 		while (result) {
321: 			if (!result->success) {
322: 				throw Exception(result->error);
323: 			}
324: 			result = move(result->next);
325: 		}
326: 	}
327: }
328: 
329: string InterpretedBenchmark::Verify(BenchmarkState *state_p) {
330: 	auto &state = (InterpretedBenchmarkState &)*state_p;
331: 	if (!state.result->success) {
332: 		return state.result->error;
333: 	}
334: 	if (result_column_count == 0) {
335: 		// no result specified
336: 		return string();
337: 	}
338: 	// compare the column count
339: 	if (result_column_count >= 0 && (int64_t)state.result->ColumnCount() != result_column_count) {
340: 		return StringUtil::Format("Error in result: expected %lld columns but got %lld\nObtained result: %s",
341: 		                          (int64_t)result_column_count, (int64_t)state.result->ColumnCount(),
342: 		                          state.result->ToString());
343: 	}
344: 	// compare row count
345: 	if (state.result->collection.Count() != result_values.size()) {
346: 		return StringUtil::Format("Error in result: expected %lld rows but got %lld\nObtained result: %s",
347: 		                          (int64_t)result_values.size(), (int64_t)state.result->collection.Count(),
348: 		                          state.result->ToString());
349: 	}
350: 	// compare values
351: 	for (int64_t r = 0; r < (int64_t)result_values.size(); r++) {
352: 		for (int64_t c = 0; c < result_column_count; c++) {
353: 			auto value = state.result->collection.GetValue(c, r);
354: 			if (result_values[r][c] == "NULL" && value.is_null) {
355: 				continue;
356: 			}
357: 
358: 			Value verify_val(result_values[r][c]);
359: 			try {
360: 				if (result_values[r][c] == value.ToString()) {
361: 					continue;
362: 				}
363: 				verify_val = verify_val.CastAs(state.result->types[c]);
364: 				if (result_values[r][c] == "(empty)" && (verify_val.ToString() == "" || value.is_null)) {
365: 					continue;
366: 				}
367: 			} catch (...) {
368: 			}
369: 			if (!Value::ValuesAreEqual(value, verify_val)) {
370: 				return StringUtil::Format(
371: 				    "Error in result on row %lld column %lld: expected value \"%s\" but got value \"%s\"", r + 1, c + 1,
372: 				    verify_val.ToString().c_str(), value.ToString().c_str());
373: 			}
374: 		}
375: 	}
376: 	return string();
377: }
378: 
379: void InterpretedBenchmark::Interrupt(BenchmarkState *state_p) {
380: 	auto &state = (InterpretedBenchmarkState &)*state_p;
381: 	state.con.Interrupt();
382: }
383: 
384: string InterpretedBenchmark::BenchmarkInfo() {
385: 	return string();
386: }
387: 
388: string InterpretedBenchmark::GetLogOutput(BenchmarkState *state_p) {
389: 	auto &state = (InterpretedBenchmarkState &)*state_p;
390: 	return state.con.context->profiler->ToJSON();
391: }
392: 
393: string InterpretedBenchmark::DisplayName() {
394: 	LoadBenchmark();
395: 	return display_name.empty() ? name : display_name;
396: }
397: 
398: string InterpretedBenchmark::Group() {
399: 	LoadBenchmark();
400: 	return display_group.empty() ? group : display_group;
401: }
402: 
403: string InterpretedBenchmark::Subgroup() {
404: 	LoadBenchmark();
405: 	return subgroup;
406: }
407: 
408: } // namespace duckdb
[end of benchmark/interpreted_benchmark.cpp]
[start of benchmark/tpch/startup.cpp]
1: #include "benchmark_runner.hpp"
2: #include "compare_result.hpp"
3: #include "tpch-extension.hpp"
4: #include "duckdb_benchmark_macro.hpp"
5: 
6: using namespace duckdb;
7: 
8: #define SF 1
9: 
10: #define TPCHStartup(QUERY)                                                                                             \
11: 	string db_path = "duckdb_benchmark_db.db";                                                                         \
12: 	void Load(DuckDBBenchmarkState *state) override {                                                                  \
13: 		DeleteDatabase(db_path);                                                                                       \
14: 		{                                                                                                              \
15: 			DuckDB db(db_path);                                                                                        \
16: 			db.LoadExtension<TPCHExtension>();                                                                         \
17: 			Connection con(db);                                                                                        \
18: 			con.Query("CALL dbgen(sf=" + std::to_string(SF) + ")");                                                    \
19: 		}                                                                                                              \
20: 		{                                                                                                              \
21: 			auto config = GetConfig();                                                                                 \
22: 			config->checkpoint_wal_size = 0;                                                                           \
23: 			DuckDB db(db_path, config.get());                                                                          \
24: 		}                                                                                                              \
25: 	}                                                                                                                  \
26: 	void RunBenchmark(DuckDBBenchmarkState *state) override {                                                          \
27: 		auto config = GetConfig();                                                                                     \
28: 		DuckDB db(db_path, config.get());                                                                              \
29: 		db.LoadExtension<TPCHExtension>();                                                                             \
30: 		Connection con(db);                                                                                            \
31: 		state->result = con.Query(QUERY);                                                                              \
32: 	}                                                                                                                  \
33: 	string BenchmarkInfo() override {                                                                                  \
34: 		return string("Start a TPC-H SF1 database and run ") + QUERY + string(" in the database");                     \
35: 	}
36: 
37: #define NormalConfig()                                                                                                 \
38: 	unique_ptr<DBConfig> GetConfig() {                                                                                 \
39: 		return make_unique<DBConfig>();                                                                                \
40: 	}
41: 
42: DUCKDB_BENCHMARK(TPCHEmptyStartup, "[startup]")
43: TPCHStartup("SELECT * FROM lineitem WHERE 1=0") NormalConfig() string VerifyResult(QueryResult *result) override {
44: 	if (!result->success) {
45: 		return result->error;
46: 	}
47: 	return string();
48: }
49: FINISH_BENCHMARK(TPCHEmptyStartup)
50: 
51: DUCKDB_BENCHMARK(TPCHCount, "[startup]")
52: TPCHStartup("SELECT COUNT(*) FROM lineitem") NormalConfig() string VerifyResult(QueryResult *result) override {
53: 	if (!result->success) {
54: 		return result->error;
55: 	}
56: 	return string();
57: }
58: FINISH_BENCHMARK(TPCHCount)
59: 
60: DUCKDB_BENCHMARK(TPCHSimpleAggr, "[startup]")
61: TPCHStartup("SELECT SUM(l_extendedprice) FROM lineitem") NormalConfig() string
62:     VerifyResult(QueryResult *result) override {
63: 	if (!result->success) {
64: 		return result->error;
65: 	}
66: 	return string();
67: }
68: FINISH_BENCHMARK(TPCHSimpleAggr)
69: 
70: DUCKDB_BENCHMARK(TPCHQ1, "[startup]")
71: TPCHStartup("PRAGMA tpch(1)") NormalConfig() string VerifyResult(QueryResult *result) override {
72: 	if (!result->success) {
73: 		return result->error;
74: 	}
75: 	return compare_csv(*result, TPCHExtension::GetAnswer(SF, 1), true);
76: }
77: FINISH_BENCHMARK(TPCHQ1)
[end of benchmark/tpch/startup.cpp]
[start of extension/extension_helper.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // extension_helper.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include <string>
12: 
13: #ifdef BUILD_ICU_EXTENSION
14: #include "icu-extension.hpp"
15: #endif
16: 
17: #ifdef BUILD_PARQUET_EXTENSION
18: #include "parquet-extension.hpp"
19: #endif
20: 
21: #ifdef BUILD_TPCH_EXTENSION
22: #include "tpch-extension.hpp"
23: #endif
24: 
25: #ifdef BUILD_TPCDS_EXTENSION
26: #include "tpcds-extension.hpp"
27: #endif
28: 
29: #ifdef BUILD_FTS_EXTENSION
30: #include "fts-extension.hpp"
31: #endif
32: 
33: #ifdef BUILD_HTTPFS_EXTENSION
34: #include "httpfs-extension.hpp"
35: #endif
36: 
37: #ifdef BUILD_VISUALIZER_EXTENSION
38: #include "visualizer-extension.hpp"
39: #endif
40: 
41: namespace duckdb {
42: class DuckDB;
43: 
44: enum class ExtensionLoadResult : uint8_t { LOADED_EXTENSION = 0, EXTENSION_UNKNOWN = 1, NOT_LOADED = 2 };
45: 
46: class ExtensionHelper {
47: public:
48: 	static void LoadAllExtensions(DuckDB &db) {
49: #ifdef BUILD_ICU_EXTENSION
50: 		db.LoadExtension<ICUExtension>();
51: #endif
52: #ifdef BUILD_PARQUET_EXTENSION
53: 		db.LoadExtension<ParquetExtension>();
54: #endif
55: #ifdef BUILD_TPCH_EXTENSION
56: 		db.LoadExtension<TPCHExtension>();
57: #endif
58: #ifdef BUILD_TPCDS_EXTENSION
59: 		db.LoadExtension<TPCDSExtension>();
60: #endif
61: #ifdef BUILD_FTS_EXTENSION
62: 		db.LoadExtension<FTSExtension>();
63: #endif
64: #ifdef BUILD_HTTPFS_EXTENSION
65: 		db.LoadExtension<HTTPFsExtension>();
66: #endif
67: #ifdef BUILD_VISUALIZER_EXTENSION
68: 		db.LoadExtension<VisualizerExtension>();
69: #endif
70: 	}
71: 
72: 	static ExtensionLoadResult LoadExtension(DuckDB &db, std::string extension) {
73: 		if (extension == "parquet") {
74: #ifdef BUILD_PARQUET_EXTENSION
75: 			db.LoadExtension<ParquetExtension>();
76: #else
77: 			// parquet extension required but not build: skip this test
78: 			return ExtensionLoadResult::NOT_LOADED;
79: #endif
80: 		} else if (extension == "icu") {
81: #ifdef BUILD_ICU_EXTENSION
82: 			db.LoadExtension<ICUExtension>();
83: #else
84: 			// icu extension required but not build: skip this test
85: 			return ExtensionLoadResult::NOT_LOADED;
86: #endif
87: 		} else if (extension == "tpch") {
88: #ifdef BUILD_TPCH_EXTENSION
89: 			db.LoadExtension<TPCHExtension>();
90: #else
91: 			// icu extension required but not build: skip this test
92: 			return ExtensionLoadResult::NOT_LOADED;
93: #endif
94: 		} else if (extension == "tpcds") {
95: #ifdef BUILD_TPCDS_EXTENSION
96: 			db.LoadExtension<TPCDSExtension>();
97: #else
98: 			// icu extension required but not build: skip this test
99: 			return ExtensionLoadResult::NOT_LOADED;
100: #endif
101: 		} else if (extension == "fts") {
102: #ifdef BUILD_FTS_EXTENSION
103: 			db.LoadExtension<FTSExtension>();
104: #else
105: 			// fts extension required but not build: skip this test
106: 			return ExtensionLoadResult::NOT_LOADED;
107: #endif
108: 		} else if (extension == "httpfs") {
109: #ifdef BUILD_HTTPFS_EXTENSION
110: 			db.LoadExtension<HTTPFsExtension>();
111: #else
112: 			return ExtensionLoadResult::NOT_LOADED;
113: #endif
114: 		} else if (extension == "visualizer") {
115: #ifdef BUILD_VISUALIZER_EXTENSION
116: 			db.LoadExtension<VisualizerExtension>();
117: #else
118: 			// visualizer extension required but not build: skip this test
119: 			return ExtensionLoadResult::NOT_LOADED;
120: #endif
121: 		} else {
122: 			// unknown extension
123: 			return ExtensionLoadResult::EXTENSION_UNKNOWN;
124: 		}
125: 		return ExtensionLoadResult::LOADED_EXTENSION;
126: 	}
127: };
128: 
129: } // namespace duckdb
[end of extension/extension_helper.hpp]
[start of extension/fts/CMakeLists.txt]
1: cmake_minimum_required(VERSION 2.8.12)
2: 
3: project(FTSExtension)
4: 
5: include_directories(include ../../third_party/snowball/libstemmer)
6: 
7: add_library(
8:   fts_extension STATIC
9:   fts-extension.cpp
10:   fts_indexing.cpp
11:   ../../third_party/snowball/libstemmer/libstemmer.cpp
12:   ../../third_party/snowball/runtime/utilities.cpp
13:   ../../third_party/snowball/runtime/api.cpp
14:   ../../third_party/snowball/src_c/stem_UTF_8_arabic.cpp
15:   ../../third_party/snowball/src_c/stem_UTF_8_basque.cpp
16:   ../../third_party/snowball/src_c/stem_UTF_8_catalan.cpp
17:   ../../third_party/snowball/src_c/stem_UTF_8_danish.cpp
18:   ../../third_party/snowball/src_c/stem_UTF_8_dutch.cpp
19:   ../../third_party/snowball/src_c/stem_UTF_8_english.cpp
20:   ../../third_party/snowball/src_c/stem_UTF_8_finnish.cpp
21:   ../../third_party/snowball/src_c/stem_UTF_8_french.cpp
22:   ../../third_party/snowball/src_c/stem_UTF_8_german.cpp
23:   ../../third_party/snowball/src_c/stem_UTF_8_german2.cpp
24:   ../../third_party/snowball/src_c/stem_UTF_8_greek.cpp
25:   ../../third_party/snowball/src_c/stem_UTF_8_hindi.cpp
26:   ../../third_party/snowball/src_c/stem_UTF_8_hungarian.cpp
27:   ../../third_party/snowball/src_c/stem_UTF_8_indonesian.cpp
28:   ../../third_party/snowball/src_c/stem_UTF_8_irish.cpp
29:   ../../third_party/snowball/src_c/stem_UTF_8_italian.cpp
30:   ../../third_party/snowball/src_c/stem_UTF_8_kraaij_pohlmann.cpp
31:   ../../third_party/snowball/src_c/stem_UTF_8_lithuanian.cpp
32:   ../../third_party/snowball/src_c/stem_UTF_8_lovins.cpp
33:   ../../third_party/snowball/src_c/stem_UTF_8_nepali.cpp
34:   ../../third_party/snowball/src_c/stem_UTF_8_norwegian.cpp
35:   ../../third_party/snowball/src_c/stem_UTF_8_porter.cpp
36:   ../../third_party/snowball/src_c/stem_UTF_8_portuguese.cpp
37:   ../../third_party/snowball/src_c/stem_UTF_8_romanian.cpp
38:   ../../third_party/snowball/src_c/stem_UTF_8_russian.cpp
39:   ../../third_party/snowball/src_c/stem_UTF_8_serbian.cpp
40:   ../../third_party/snowball/src_c/stem_UTF_8_spanish.cpp
41:   ../../third_party/snowball/src_c/stem_UTF_8_swedish.cpp
42:   ../../third_party/snowball/src_c/stem_UTF_8_tamil.cpp
43:   ../../third_party/snowball/src_c/stem_UTF_8_turkish.cpp)
[end of extension/fts/CMakeLists.txt]
[start of extension/httpfs/CMakeLists.txt]
1: cmake_minimum_required(VERSION 2.8.12)
2: 
3: project(HTTPFsExtension)
4: 
5: add_extension_definitions()
6: 
7: include_directories(include ../.. ../../third_party/httplib
8:                     ../../third_party/picohash ../parquet/include)
9: 
10: add_library(httpfs_extension STATIC s3fs.cpp httpfs.cpp crypto.cpp
11:                                     httpfs-extension.cpp)
[end of extension/httpfs/CMakeLists.txt]
[start of extension/icu/CMakeLists.txt]
1: cmake_minimum_required(VERSION 2.8.12)
2: 
3: project(ICUExtension)
4: 
5: include_directories(include)
6: 
7: add_library(icu_extension STATIC icu-collate.cpp icu-extension.cpp)
8: link_threads(icu_extension)
9: disable_target_warnings(icu_extension)
[end of extension/icu/CMakeLists.txt]
[start of extension/parquet/CMakeLists.txt]
1: cmake_minimum_required(VERSION 2.8.12)
2: 
3: project(ParquetExtension)
4: 
5: include_directories(
6:   include ../../third_party/parquet ../../third_party/snappy
7:   ../../third_party/miniz ../../third_party/thrift
8:   ../../third_party/zstd/include)
9: 
10: set(PARQUET_EXTENSION_FILES
11:     parquet-extension.cpp
12:     parquet_metadata.cpp
13:     parquet_reader.cpp
14:     parquet_timestamp.cpp
15:     parquet_writer.cpp
16:     parquet_statistics.cpp
17:     column_reader.cpp)
18: 
19: if(NOT CLANG_TIDY)
20:   set(PARQUET_EXTENSION_FILES
21:       ${PARQUET_EXTENSION_FILES}
22:       ../../third_party/parquet/parquet_constants.cpp
23:       ../../third_party/parquet/parquet_types.cpp
24:       ../../third_party/thrift/thrift/protocol/TProtocol.cpp
25:       ../../third_party/thrift/thrift/transport/TTransportException.cpp
26:       ../../third_party/thrift/thrift/transport/TBufferTransports.cpp
27:       ../../third_party/snappy/snappy.cc
28:       ../../third_party/snappy/snappy-sinksource.cc
29:       ../../third_party/zstd/decompress/zstd_ddict.cpp
30:       ../../third_party/zstd/decompress/huf_decompress.cpp
31:       ../../third_party/zstd/decompress/zstd_decompress.cpp
32:       ../../third_party/zstd/decompress/zstd_decompress_block.cpp
33:       ../../third_party/zstd/common/entropy_common.cpp
34:       ../../third_party/zstd/common/fse_decompress.cpp
35:       ../../third_party/zstd/common/zstd_common.cpp
36:       ../../third_party/zstd/common/error_private.cpp
37:       ../../third_party/zstd/common/xxhash.cpp
38:       ../../third_party/zstd/compress/fse_compress.cpp
39:       ../../third_party/zstd/compress/hist.cpp
40:       ../../third_party/zstd/compress/huf_compress.cpp
41:       ../../third_party/zstd/compress/zstd_compress.cpp
42:       ../../third_party/zstd/compress/zstd_compress_literals.cpp
43:       ../../third_party/zstd/compress/zstd_compress_sequences.cpp
44:       ../../third_party/zstd/compress/zstd_compress_superblock.cpp
45:       ../../third_party/zstd/compress/zstd_double_fast.cpp
46:       ../../third_party/zstd/compress/zstd_fast.cpp
47:       ../../third_party/zstd/compress/zstd_lazy.cpp
48:       ../../third_party/zstd/compress/zstd_ldm.cpp
49:       ../../third_party/zstd/compress/zstd_opt.cpp)
50: endif()
51: 
52: add_library(parquet_extension STATIC ${PARQUET_EXTENSION_FILES})
53: 
54: if(NOT CLANG_TIDY)
55:   add_executable(parquetcli parquetcli.cpp)
56:   target_link_libraries(parquetcli parquet_extension)
57:   target_link_libraries(parquetcli duckdb_static)
58: endif()
[end of extension/parquet/CMakeLists.txt]
[start of extension/tpcds/CMakeLists.txt]
1: cmake_minimum_required(VERSION 2.8.12)
2: 
3: project(TPCDSExtension)
4: 
5: include_directories(include)
6: include_directories(dsdgen/include)
7: add_subdirectory(dsdgen)
8: 
9: add_library(tpcds_extension STATIC tpcds-extension.cpp ${DSDGEN_OBJECT_FILES})
10: # target_link_libraries(tpcds_extension dsdgen)
[end of extension/tpcds/CMakeLists.txt]
[start of extension/tpch/CMakeLists.txt]
1: cmake_minimum_required(VERSION 2.8.12)
2: 
3: project(TPCHExtension)
4: 
5: include_directories(dbgen/include)
6: include_directories(include)
7: add_subdirectory(dbgen)
8: 
9: add_library(tpch_extension STATIC tpch-extension.cpp ${DBGEN_OBJECT_FILES})
10: # target_link_libraries(tpch_extension dbgen)
[end of extension/tpch/CMakeLists.txt]
[start of extension/visualizer/CMakeLists.txt]
1: cmake_minimum_required(VERSION 2.8.12)
2: 
3: project(VisualizerExtension)
4: 
5: include_directories(include)
6: 
7: add_library(visualizer_extension STATIC visualizer-extension.cpp)
[end of extension/visualizer/CMakeLists.txt]
[start of scripts/amalgamation.py]
1: # this script creates a single header + source file combination out of the DuckDB sources
2: import os
3: import re
4: import sys
5: import shutil
6: import subprocess
7: from python_helpers import open_utf8, normalize_path
8: 
9: amal_dir = os.path.join('src', 'amalgamation')
10: header_file = os.path.join(amal_dir, "duckdb.hpp")
11: source_file = os.path.join(amal_dir, "duckdb.cpp")
12: temp_header = 'duckdb.hpp.tmp'
13: temp_source = 'duckdb.cpp.tmp'
14: 
15: skip_duckdb_includes = False
16: 
17: src_dir = 'src'
18: include_dir = os.path.join('src', 'include')
19: fmt_dir = os.path.join('third_party', 'fmt')
20: fmt_include_dir = os.path.join('third_party', 'fmt', 'include')
21: miniz_dir = os.path.join('third_party', 'miniz')
22: re2_dir = os.path.join('third_party', 're2')
23: pg_query_dir = os.path.join('third_party', 'libpg_query')
24: pg_query_include_dir = os.path.join('third_party', 'libpg_query', 'include')
25: hll_dir = os.path.join('third_party', 'hyperloglog')
26: tdigest_dir = os.path.join('third_party', 'tdigest')
27: utf8proc_dir = os.path.join('third_party', 'utf8proc')
28: utf8proc_include_dir = os.path.join('third_party', 'utf8proc', 'include')
29: 
30: moodycamel_include_dir = os.path.join('third_party', 'concurrentqueue')
31: pcg_include_dir = os.path.join('third_party', 'pcg')
32: 
33: # files included in the amalgamated "duckdb.hpp" file
34: main_header_files = [os.path.join(include_dir, 'duckdb.hpp'),
35:     os.path.join(include_dir, 'duckdb.h'),
36:     os.path.join(include_dir, 'duckdb', 'common', 'types', 'date.hpp'),
37:     os.path.join(include_dir, 'duckdb', 'common', 'arrow.hpp'),
38:     os.path.join(include_dir, 'duckdb', 'common', 'types', 'blob.hpp'),
39:     os.path.join(include_dir, 'duckdb', 'common', 'types', 'decimal.hpp'),
40:     os.path.join(include_dir, 'duckdb', 'common', 'types', 'hugeint.hpp'),
41:     os.path.join(include_dir, 'duckdb', 'common', 'types', 'uuid.hpp'),
42:     os.path.join(include_dir, 'duckdb', 'common', 'types', 'interval.hpp'),
43:     os.path.join(include_dir, 'duckdb', 'common', 'types', 'timestamp.hpp'),
44:     os.path.join(include_dir, 'duckdb', 'common', 'types', 'time.hpp'),
45:     os.path.join(include_dir, 'duckdb', 'common', 'serializer', 'buffered_file_writer.hpp'),
46:     os.path.join(include_dir, 'duckdb', 'common', 'serializer', 'buffered_serializer.hpp'),
47:     os.path.join(include_dir, 'duckdb', 'main', 'appender.hpp'),
48:     os.path.join(include_dir, 'duckdb', 'main', 'client_context.hpp'),
49:     os.path.join(include_dir, 'duckdb', 'function', 'function.hpp'),
50:     os.path.join(include_dir, 'duckdb', 'function', 'table_function.hpp'),
51:     os.path.join(include_dir, 'duckdb', 'parser', 'parsed_data', 'create_table_function_info.hpp'),
52:     os.path.join(include_dir, 'duckdb', 'parser', 'parsed_data', 'create_copy_function_info.hpp')]
53: extended_amalgamation = False
54: if '--extended' in sys.argv:
55:     def add_include_dir(dirpath):
56:         return [os.path.join(dirpath, x) for x in os.listdir(dirpath)]
57: 
58:     extended_amalgamation = True
59:     main_header_files += [os.path.join(include_dir, x) for x in [
60:         'duckdb/planner/expression/bound_constant_expression.hpp',
61:         'duckdb/planner/expression/bound_function_expression.hpp',
62:         'duckdb/catalog/catalog_entry/scalar_function_catalog_entry.hpp',
63:         'duckdb/parser/parsed_data/create_table_info.hpp',
64:         'duckdb/planner/parsed_data/bound_create_table_info.hpp',
65:         'duckdb/parser/constraints/not_null_constraint.hpp',
66:         'duckdb/storage/data_table.hpp',
67:         'duckdb/function/pragma_function.hpp',
68:         'duckdb/parser/qualified_name.hpp',
69:         'duckdb/parser/parser.hpp',
70:         'duckdb/planner/binder.hpp',
71:         'duckdb/storage/object_cache.hpp',
72:         'duckdb/planner/table_filter.hpp',
73:         "duckdb/storage/statistics/string_statistics.hpp",
74:         "duckdb/storage/statistics/numeric_statistics.hpp",
75:         "duckdb/planner/filter/conjunction_filter.hpp",
76:         "duckdb/planner/filter/constant_filter.hpp",
77:         "duckdb/execution/operator/persistent/buffered_csv_reader.hpp",
78:         "duckdb/common/types/vector_cache.hpp",
79:         "duckdb/planner/filter/null_filter.hpp",
80:         "duckdb/common/arrow_wrapper.hpp"]]
81:     main_header_files += add_include_dir(os.path.join(include_dir, 'duckdb/parser/expression'))
82:     main_header_files += add_include_dir(os.path.join(include_dir, 'duckdb/parser/parsed_data'))
83:     main_header_files += add_include_dir(os.path.join(include_dir, 'duckdb/parser/tableref'))
84:     main_header_files = normalize_path(main_header_files)
85: 
86: # include paths for where to search for include files during amalgamation
87: include_paths = [include_dir, fmt_include_dir, re2_dir, miniz_dir, utf8proc_include_dir, hll_dir, tdigest_dir, utf8proc_dir, pg_query_include_dir, pg_query_dir, moodycamel_include_dir,pcg_include_dir]
88: # paths of where to look for files to compile and include to the final amalgamation
89: compile_directories = [src_dir, fmt_dir, miniz_dir, re2_dir, hll_dir, utf8proc_dir, pg_query_dir]
90: 
91: # files always excluded
92: always_excluded = normalize_path(['src/amalgamation/duckdb.cpp', 'src/amalgamation/duckdb.hpp', 'src/amalgamation/parquet-amalgamation.cpp', 'src/amalgamation/parquet-amalgamation.hpp'])
93: # files excluded from the amalgamation
94: excluded_files = ['grammar.cpp', 'grammar.hpp', 'symbols.cpp']
95: # files excluded from individual file compilation during test_compile
96: excluded_compilation_files = excluded_files + ['gram.hpp', 'kwlist.hpp', "duckdb-c.cpp"]
97: 
98: linenumbers = False
99: 
100: def get_includes(fpath, text):
101:     # find all the includes referred to in the directory
102:     regex_include_statements = re.findall("(^[\t ]*[#][\t ]*include[\t ]+[\"]([^\"]+)[\"])", text, flags=re.MULTILINE)
103:     include_statements = []
104:     include_files = []
105:     # figure out where they are located
106:     for x in regex_include_statements:
107:         included_file = x[1]
108:         if skip_duckdb_includes and 'duckdb' in included_file:
109:             continue
110:         include_statements.append(x[0])
111:         included_file = os.sep.join(included_file.split('/'))
112:         found = False
113:         for include_path in include_paths:
114:             ipath = os.path.join(include_path, included_file)
115:             if os.path.isfile(ipath):
116:                 include_files.append(ipath)
117:                 found = True
118:                 break
119:         if not found:
120:             raise Exception('Could not find include file "' + included_file + '", included from file "' + fpath + '"')
121:     return (include_statements, include_files)
122: 
123: def cleanup_file(text):
124:     # remove all "#pragma once" notifications
125:     text = re.sub('#pragma once', '', text)
126:     return text
127: 
128: # recursively get all includes and write them
129: written_files = {}
130: 
131: #licenses
132: licenses = []
133: 
134: def need_to_write_file(current_file, ignore_excluded = False):
135:     if amal_dir in current_file:
136:         return False
137:     if current_file in always_excluded:
138:         return False
139:     if current_file.split(os.sep)[-1] in excluded_files and not ignore_excluded:
140:         # file is in ignored files set
141:         return False
142:     if current_file in written_files:
143:         # file is already written
144:         return False
145:     return True
146: 
147: def find_license(original_file):
148:     global licenses
149:     file = original_file
150:     license = ""
151:     while True:
152:         (file, end) = os.path.split(file)
153:         if file == "":
154:             break
155:         potential_license = os.path.join(file, "LICENSE")
156:         if os.path.exists(potential_license):
157:             license = potential_license
158:     if license == "":
159:         raise "Could not find license for %s" % original_file
160: 
161:     if license not in licenses:
162:         licenses += [license]
163: 
164:     return licenses.index(license)
165: 
166: 
167: def write_file(current_file, ignore_excluded = False):
168:     global linenumbers
169:     global written_files
170:     if not need_to_write_file(current_file, ignore_excluded):
171:         return ""
172:     written_files[current_file] = True
173: 
174:     # first read this file
175:     with open_utf8(current_file, 'r') as f:
176:         text = f.read()
177: 
178:     if current_file.startswith("third_party") and not current_file.endswith("LICENSE"):
179:         lic_idx = find_license(current_file)
180:         text = "\n\n// LICENSE_CHANGE_BEGIN\n// The following code up to LICENSE_CHANGE_END is subject to THIRD PARTY LICENSE #%s\n// See the end of this file for a list\n\n" % str(lic_idx + 1) + text + "\n\n// LICENSE_CHANGE_END\n"
181: 
182:     (statements, includes) = get_includes(current_file, text)
183:     # find the linenr of the final #include statement we parsed
184:     if len(statements) > 0:
185:         index = text.find(statements[-1])
186:         linenr = len(text[:index].split('\n'))
187: 
188:         # now write all the dependencies of this header first
189:         for i in range(len(includes)):
190:             include_text = write_file(includes[i])
191:             if linenumbers and i == len(includes) - 1:
192:                 # for the last include statement, we also include a #line directive
193:                 include_text += '\n#line %d "%s"\n' % (linenr, current_file)
194:             text = text.replace(statements[i], include_text)
195: 
196:     # add the initial line here
197:     if linenumbers:
198:         text = '\n#line 1 "%s"\n' % (current_file,) + text
199:     # print(current_file)
200:     # now read the header and write it
201:     return cleanup_file(text)
202: 
203: def write_dir(dir):
204:     files = os.listdir(dir)
205:     files.sort()
206:     text = ""
207:     for fname in files:
208:         if fname in excluded_files:
209:             continue
210:         # print(fname)
211:         fpath = os.path.join(dir, fname)
212:         if os.path.isdir(fpath):
213:             text += write_dir(fpath)
214:         elif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):
215:             text += write_file(fpath)
216:     return text
217: 
218: def copy_if_different(src, dest):
219:     if os.path.isfile(dest):
220:         # dest exists, check if the files are different
221:         with open_utf8(src, 'r') as f:
222:             source_text = f.read()
223:         with open_utf8(dest, 'r') as f:
224:             dest_text = f.read()
225:         if source_text == dest_text:
226:             # print("Skipping copy of " + src + ", identical copy already exists at " + dest)
227:             return
228:     # print("Copying " + src + " to " + dest)
229:     shutil.copyfile(src, dest)
230: 
231: def git_commit_hash():
232:     return subprocess.check_output(['git','log','-1','--format=%h']).strip().decode('utf8')
233: 
234: def git_dev_version():
235:     version = subprocess.check_output(['git','describe','--tags','--abbrev=0']).strip().decode('utf8')
236:     long_version = subprocess.check_output(['git','describe','--tags','--long']).strip().decode('utf8')
237:     version_splits = version.lstrip('v').split('.')
238:     dev_version = long_version.split('-')[1]
239:     if int(dev_version) == 0:
240:         # directly on a tag: emit the regular version
241:         return '.'.join(version_splits)
242:     else:
243:         # not on a tag: increment the version by one and add a -devX suffix
244:         version_splits[2] = str(int(version_splits[2]) + 1)
245:         return '.'.join(version_splits) + "-dev" + dev_version
246: 
247: def generate_duckdb_hpp(header_file):
248:     print("-----------------------")
249:     print("-- Writing " + header_file + " --")
250:     print("-----------------------")
251:     with open_utf8(temp_header, 'w+') as hfile:
252:         hfile.write("/*\n")
253:         hfile.write(write_file("LICENSE"))
254:         hfile.write("*/\n\n")
255: 
256:         hfile.write("#pragma once\n")
257:         hfile.write("#define DUCKDB_AMALGAMATION 1\n")
258:         if extended_amalgamation:
259:             hfile.write("#define DUCKDB_AMALGAMATION_EXTENDED 1\n")
260:         hfile.write("#define DUCKDB_SOURCE_ID \"%s\"\n" % git_commit_hash())
261:         hfile.write("#define DUCKDB_VERSION \"%s\"\n" % git_dev_version())
262:         for fpath in main_header_files:
263:             hfile.write(write_file(fpath))
264: 
265: def generate_amalgamation(source_file, header_file):
266:     # construct duckdb.hpp from these headers
267:     generate_duckdb_hpp(header_file)
268: 
269:     # now construct duckdb.cpp
270:     print("------------------------")
271:     print("-- Writing " + source_file + " --")
272:     print("------------------------")
273: 
274:     # scan all the .cpp files
275:     with open_utf8(temp_source, 'w+') as sfile:
276:         header_file_name = header_file.split(os.sep)[-1]
277:         sfile.write('#include "' + header_file_name + '"\n\n')
278:         sfile.write("#ifndef DUCKDB_AMALGAMATION\n#error header mismatch\n#endif\n\n")
279:         sfile.write("#if (!defined(DEBUG) && !defined NDEBUG)\n#define NDEBUG\n#endif\n\n")
280:         for compile_dir in compile_directories:
281:             sfile.write(write_dir(compile_dir))
282: 
283:         sfile.write('\n\n/*\n')
284:         license_idx = 0
285:         for license in licenses:
286:             sfile.write("\n\n\n### THIRD PARTY LICENSE #%s ###\n\n" % str(license_idx + 1))
287:             sfile.write(write_file(license))
288:             license_idx+=1
289:         sfile.write('\n\n*/\n')
290: 
291: 
292:     copy_if_different(temp_header, header_file)
293:     copy_if_different(temp_source, source_file)
294:     try:
295:         os.remove(temp_header)
296:         os.remove(temp_source)
297:     except:
298:         pass
299: 
300: def list_files(dname, file_list):
301:     files = os.listdir(dname)
302:     files.sort()
303:     for fname in files:
304:         if fname in excluded_files:
305:             continue
306:         fpath = os.path.join(dname, fname)
307:         if os.path.isdir(fpath):
308:             list_files(fpath, file_list)
309:         elif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):
310:             if need_to_write_file(fpath):
311:                 file_list.append(fpath)
312: 
313: def list_sources():
314:     file_list = []
315:     for compile_dir in compile_directories:
316:         list_files(compile_dir, file_list)
317:     return file_list
318: 
319: def list_include_files_recursive(dname, file_list):
320:     files = os.listdir(dname)
321:     files.sort()
322:     for fname in files:
323:         if fname in excluded_files:
324:             continue
325:         fpath = os.path.join(dname, fname)
326:         if os.path.isdir(fpath):
327:             list_include_files_recursive(fpath, file_list)
328:         elif fname.endswith('.hpp') or fname.endswith('.h') or fname.endswith('.hh') or fname.endswith('.tcc'):
329:             file_list.append(fpath)
330: 
331: def list_includes_files(include_dirs):
332:     file_list = []
333:     for include_dir in include_dirs:
334:         list_include_files_recursive(include_dir, file_list)
335:     return file_list
336: 
337: def list_includes():
338:     return list_includes_files(include_paths)
339: 
340: def gather_file(current_file, source_files, header_files):
341:     global linenumbers
342:     global written_files
343:     if not need_to_write_file(current_file, False):
344:         return ""
345:     written_files[current_file] = True
346: 
347:     # first read this file
348:     with open_utf8(current_file, 'r') as f:
349:         text = f.read()
350: 
351:     (statements, includes) = get_includes(current_file, text)
352:     # find the linenr of the final #include statement we parsed
353:     if len(statements) > 0:
354:         index = text.find(statements[-1])
355:         linenr = len(text[:index].split('\n'))
356: 
357:         # now write all the dependencies of this header first
358:         for i in range(len(includes)):
359:             # source file inclusions are inlined into the main text
360:             include_text = write_file(includes[i])
361:             if linenumbers and i == len(includes) - 1:
362:                 # for the last include statement, we also include a #line directive
363:                 include_text += '\n#line %d "%s"\n' % (linenr, current_file)
364:             if includes[i].endswith('.cpp') or includes[i].endswith('.cc') or includes[i].endswith('.c'):
365:                 # source file inclusions are inlined into the main text
366:                 text = text.replace(statements[i], include_text)
367:             else:
368:                 text = text.replace(statements[i], '')
369:                 header_files.append(include_text)
370: 
371:     # add the initial line here
372:     if linenumbers:
373:         text = '\n#line 1 "%s"\n' % (current_file,) + text
374:     source_files.append(cleanup_file(text))
375: 
376: def gather_files(dir, source_files, header_files):
377:     files = os.listdir(dir)
378:     files.sort()
379:     for fname in files:
380:         if fname in excluded_files:
381:             continue
382:         fpath = os.path.join(dir, fname)
383:         if os.path.isdir(fpath):
384:             gather_files(fpath, source_files, header_files)
385:         elif fname.endswith('.cpp') or fname.endswith('.c') or fname.endswith('.cc'):
386:             gather_file(fpath, source_files, header_files)
387: 
388: 
389: def generate_amalgamation_splits(source_file, header_file, nsplits):
390:     # construct duckdb.hpp from these headers
391:     generate_duckdb_hpp(header_file)
392: 
393:     # gather all files to read and write
394:     source_files = []
395:     header_files = []
396:     for compile_dir in compile_directories:
397:         if compile_dir != src_dir:
398:             continue
399:         gather_files(compile_dir, source_files, header_files)
400: 
401:     # write duckdb-internal.hpp
402:     if '.hpp' in header_file:
403:         internal_header_file = header_file.replace('.hpp', '-internal.hpp')
404:     elif '.h' in header_file:
405:         internal_header_file = header_file.replace('.h', '-internal.h')
406:     else:
407:         raise "Unknown extension of header file"
408: 
409:     temp_internal_header = internal_header_file + '.tmp'
410: 
411:     with open_utf8(temp_internal_header, 'w+') as f:
412:         write_license(f)
413:         for hfile in header_files:
414:             f.write(hfile)
415: 
416:     # count the total amount of bytes in the source files
417:     total_bytes = 0
418:     for sfile in source_files:
419:         total_bytes += len(sfile)
420: 
421:     # now write the individual splits
422:     # we approximate the splitting up by making every file have roughly the same amount of bytes
423:     split_bytes = total_bytes / nsplits
424:     current_bytes = 0
425:     partitions = []
426:     partition_names = []
427:     current_partition = []
428:     current_partition_idx = 1
429:     for sfile in source_files:
430:         current_partition.append(sfile)
431:         current_bytes += len(sfile)
432:         if current_bytes >= split_bytes:
433:             partition_names.append(str(current_partition_idx))
434:             partitions.append(current_partition)
435:             current_partition = []
436:             current_bytes = 0
437:             current_partition_idx += 1
438:     if len(current_partition) > 0:
439:         partition_names.append(str(current_partition_idx))
440:         partitions.append(current_partition)
441:         current_partition = []
442:         current_bytes = 0
443:     # generate partitions from the third party libraries
444:     for compile_dir in compile_directories:
445:         if compile_dir != src_dir:
446:             partition_names.append(compile_dir.split(os.sep)[-1])
447:             partitions.append(write_dir(compile_dir))
448: 
449:     header_file_name = header_file.split(os.sep)[-1]
450:     internal_header_file_name = internal_header_file.split(os.sep)[-1]
451: 
452:     partition_fnames = []
453:     current_partition = 0
454:     for partition in partitions:
455:         partition_name = source_file.replace('.cpp', '-%s.cpp' % (partition_names[current_partition],))
456:         temp_partition_name = partition_name + '.tmp'
457:         partition_fnames.append([partition_name, temp_partition_name])
458:         with open_utf8(temp_partition_name, 'w+') as f:
459:             write_license(f)
460:             f.write('#include "%s"\n#include "%s"' % (header_file_name, internal_header_file_name))
461:             f.write('''
462: #ifndef DUCKDB_AMALGAMATION
463: #error header mismatch
464: #endif
465: ''')
466:             for sfile in partition:
467:                 f.write(sfile)
468:         current_partition += 1
469: 
470:     copy_if_different(temp_header, header_file)
471:     copy_if_different(temp_internal_header, internal_header_file)
472:     try:
473:         os.remove(temp_header)
474:         os.remove(temp_internal_header)
475:     except:
476:         pass
477:     for p in partition_fnames:
478:         copy_if_different(p[1], p[0])
479:         try:
480:             os.remove(p[1])
481:         except:
482:             pass
483: def list_include_dirs():
484:     return include_paths
485: 
486: if __name__ == "__main__":
487:     nsplits = 1
488:     for arg in sys.argv:
489:         if arg == '--linenumbers':
490:             linenumbers = True
491:         elif arg == '--no-linenumbers':
492:             linenumbers = False
493:         elif arg.startswith('--header='):
494:             header_file = os.path.join(*arg.split('=', 1)[1].split('/'))
495:         elif arg.startswith('--source='):
496:             source_file = os.path.join(*arg.split('=', 1)[1].split('/'))
497:         elif arg.startswith('--splits='):
498:             nsplits = int(arg.split('=', 1)[1])
499:         elif arg.startswith('--list-sources'):
500:             file_list = list_sources()
501:             print('\n'.join(file_list))
502:             exit(1)
503:         elif arg.startswith('--list-objects'):
504:             file_list = list_sources()
505:             print(' '.join([x.rsplit('.', 1)[0] + '.o' for x in file_list]))
506:             exit(1)
507:         elif arg.startswith('--includes'):
508:             include_dirs = list_include_dirs()
509:             print(' '.join(['-I' + x for x in include_dirs]))
510:             exit(1)
511:         elif arg.startswith('--include-directories'):
512:             include_dirs = list_include_dirs()
513:             print('\n'.join(include_dirs))
514:             exit(1)
515:     if not os.path.exists(amal_dir):
516:         os.makedirs(amal_dir)
517: 
518:     if nsplits > 1:
519:         generate_amalgamation_splits(source_file, header_file, nsplits)
520:     else:
521:         generate_amalgamation(source_file, header_file)
522: 
[end of scripts/amalgamation.py]
[start of scripts/package_build.py]
1: import os
2: import sys
3: import shutil
4: import subprocess
5: from python_helpers import open_utf8
6: 
7: excluded_objects = ['utf8proc_data.cpp']
8: 
9: def get_libraries(binary_dir, libraries, extensions):
10:     result_libs = []
11:     def find_library_recursive(search_dir, potential_libnames):
12:         flist = os.listdir(search_dir)
13:         for fname in flist:
14:             fpath = os.path.join(search_dir, fname)
15:             if os.path.isdir(fpath):
16:                 entry = find_library_recursive(fpath, potential_libnames)
17:                 if entry != None:
18:                     return entry
19:             elif os.path.isfile(fpath) and fname in potential_libnames:
20:                 return search_dir
21:         return None
22: 
23:     def find_library(search_dir, libname, result_libs):
24:         if libname == 'Threads::Threads':
25:             result_libs += [(None, 'pthread')]
26:             return
27:         libextensions = ['.a', '.lib']
28:         libprefixes = ['', 'lib']
29:         potential_libnames = []
30:         for ext in libextensions:
31:             for prefix in libprefixes:
32:                 potential_libnames.append(prefix + libname + ext)
33:         libdir = find_library_recursive(binary_dir, potential_libnames)
34: 
35:         result_libs += [(libdir, libname)]
36: 
37:     result_libs += [(os.path.join(binary_dir, 'src'), 'duckdb_static')]
38:     for ext in extensions:
39:         result_libs += [(os.path.join(binary_dir, 'extension', ext), ext + '_extension')]
40: 
41:     for libname in libraries:
42:         find_library(binary_dir, libname, result_libs)
43: 
44:     return result_libs
45: 
46: def includes(extensions):
47:     scripts_dir = os.path.dirname(os.path.abspath(__file__))
48:     # add includes for duckdb and extensions
49:     includes = []
50:     includes.append(os.path.join(scripts_dir, '..', 'src', 'include'))
51:     includes.append(os.path.join(scripts_dir, '..'))
52:     includes.append(os.path.join(scripts_dir, '..', 'third_party', 'utf8proc', 'include'))
53:     for ext in extensions:
54:         includes.append(os.path.join(scripts_dir, '..', 'extension', ext, 'include'))
55:     return includes
56: 
57: def include_flags(extensions):
58:     return ' ' + ' '.join(['-I' + x for x in includes(extensions)])
59: 
60: def convert_backslashes(x):
61:     return '/'.join(x.split(os.path.sep))
62: 
63: def get_relative_path(source_dir, target_file):
64:     source_dir = convert_backslashes(source_dir)
65:     target_file = convert_backslashes(target_file)
66: 
67:     # absolute path: try to convert
68:     if source_dir in target_file:
69:         target_file = target_file.replace(source_dir, "").lstrip('/')
70:     return target_file
71: 
72: def git_commit_hash():
73:     try:
74:         return subprocess.check_output(['git','log','-1','--format=%h']).strip().decode('utf8')
75:     except:
76:         if 'SETUPTOOLS_SCM_PRETEND_HASH' in os.environ:
77:             return os.environ['SETUPTOOLS_SCM_PRETEND_HASH']
78:         else:
79:             return "deadbeeff"
80: 
81: def git_dev_version():
82:     try:
83:         version = subprocess.check_output(['git','describe','--tags','--abbrev=0']).strip().decode('utf8')
84:         long_version = subprocess.check_output(['git','describe','--tags','--long']).strip().decode('utf8')
85:         version_splits = version.lstrip('v').split('.')
86:         dev_version = long_version.split('-')[1]
87:         if int(dev_version) == 0:
88:             # directly on a tag: emit the regular version
89:             return '.'.join(version_splits)
90:         else:
91:             # not on a tag: increment the version by one and add a -devX suffix
92:             version_splits[2] = str(int(version_splits[2]) + 1)
93:             return '.'.join(version_splits) + "-dev" + dev_version
94:     except:
95:         if 'SETUPTOOLS_SCM_PRETEND_VERSION' in os.environ:
96:             return os.environ['SETUPTOOLS_SCM_PRETEND_VERSION']
97:         else:
98:             return "0.0.0"
99: 
100: def include_package(pkg_name, pkg_dir, include_files, include_list, source_list):
101:     import amalgamation
102:     original_path = sys.path
103:     # append the directory
104:     sys.path.append(pkg_dir)
105:     ext_pkg = __import__(pkg_name + '_config')
106: 
107:     ext_include_dirs = ext_pkg.include_directories
108:     ext_source_files = ext_pkg.source_files
109: 
110:     include_files += amalgamation.list_includes_files(ext_include_dirs)
111:     include_list += ext_include_dirs
112:     source_list += ext_source_files
113: 
114:     sys.path = original_path
115: 
116: def build_package(target_dir, extensions, linenumbers = False):
117:     if not os.path.isdir(target_dir):
118:         os.mkdir(target_dir)
119: 
120:     scripts_dir = os.path.dirname(os.path.abspath(__file__))
121:     sys.path.append(scripts_dir)
122:     import amalgamation
123: 
124:     prev_wd = os.getcwd()
125:     os.chdir(os.path.join(scripts_dir, '..'))
126: 
127:     # obtain the list of source files from the amalgamation
128:     source_list = amalgamation.list_sources()
129:     include_list = amalgamation.list_include_dirs()
130:     include_files = amalgamation.list_includes()
131: 
132:     def copy_file(src, target_dir):
133:         # get the path
134:         full_path = src.split(os.path.sep)
135:         current_path = target_dir
136:         for i in range(len(full_path) - 1):
137:             current_path = os.path.join(current_path, full_path[i])
138:             if not os.path.isdir(current_path):
139:                 os.mkdir(current_path)
140:         target_name = full_path[-1]
141:         target_file = os.path.join(current_path, target_name)
142:         amalgamation.copy_if_different(src, target_file)
143: 
144:     # include the main extension helper
145:     include_files += [os.path.join('extension', 'extension_helper.hpp')]
146:     # include the separate extensions
147:     for ext in extensions:
148:         ext_path = os.path.join(scripts_dir, '..', 'extension', ext)
149:         include_package(ext, ext_path, include_files, include_list, source_list)
150: 
151:     for src in source_list:
152:         copy_file(src, target_dir)
153: 
154:     for inc in include_files:
155:         copy_file(inc, target_dir)
156: 
157:     # handle pragma_version.cpp: paste #define DUCKDB_SOURCE_ID and DUCKDB_VERSION there
158:     curdir = os.getcwd()
159:     os.chdir(os.path.join(scripts_dir, '..'))
160:     githash = git_commit_hash()
161:     dev_version = git_dev_version()
162:     os.chdir(curdir)
163:     # open the file and read the current contents
164:     fpath = os.path.join(target_dir, 'src', 'function', 'table', 'version', 'pragma_version.cpp')
165:     with open_utf8(fpath, 'r') as f:
166:         text = f.read()
167:     # now add the DUCKDB_SOURCE_ID define, if it is not there already
168:     found_hash = False
169:     found_dev = False
170:     lines = text.split('\n')
171:     for i in range(len(lines)):
172:         if '#define DUCKDB_SOURCE_ID ' in lines[i]:
173:             lines[i] = '#define DUCKDB_SOURCE_ID "{}"'.format(githash)
174:             found_hash = True
175:             break
176:         if '#define DUCKDB_VERSION ' in lines[i]:
177:             lines[i] = '#define DUCKDB_VERSION "{}"'.format(dev_version)
178:             found_dev = True
179:             break
180:     if not found_hash:
181:         lines = ['#ifndef DUCKDB_SOURCE_ID', '#define DUCKDB_SOURCE_ID "{}"'.format(githash), '#endif'] + lines
182:     if not found_dev:
183:         lines = ['#ifndef DUCKDB_VERSION', '#define DUCKDB_VERSION "{}"'.format(dev_version), '#endif'] + lines
184:     text = '\n'.join(lines)
185:     with open_utf8(fpath, 'w+') as f:
186:         f.write(text)
187: 
188:     def file_is_excluded(fname):
189:         for entry in excluded_objects:
190:             if entry in fname:
191:                 return True
192:         return False
193: 
194:     def generate_unity_build(entries, idx, linenumbers):
195:         ub_file = os.path.join(target_dir, 'amalgamation-{}.cpp'.format(str(idx)))
196:         with open_utf8(ub_file, 'w+') as f:
197:             for entry in entries:
198:                 if linenumbers:
199:                     f.write('#line 0 "{}"\n'.format(convert_backslashes(entry)))
200:                 f.write('#include "{}"\n\n'.format(convert_backslashes(entry)))
201:         return ub_file
202: 
203:     def generate_unity_builds(source_list, nsplits, linenumbers):
204:         source_list.sort()
205: 
206:         files_per_split = len(source_list) / nsplits
207:         new_source_files = []
208:         current_files = []
209:         idx = 1
210:         for entry in source_list:
211:             if not entry.startswith('src'):
212:                 new_source_files.append(os.path.join('duckdb', entry))
213:                 continue
214: 
215:             current_files.append(entry)
216:             if len(current_files) > files_per_split:
217:                 new_source_files.append(generate_unity_build(current_files, idx, linenumbers))
218:                 current_files = []
219:                 idx += 1
220:         if len(current_files) > 0:
221:             new_source_files.append(generate_unity_build(current_files, idx, linenumbers))
222:             current_files = []
223:             idx += 1
224: 
225:         return new_source_files
226: 
227:     original_sources = source_list
228:     source_list = generate_unity_builds(source_list, 8, linenumbers)
229: 
230:     os.chdir(prev_wd)
231:     return ([convert_backslashes(x) for x in source_list if not file_is_excluded(x)],
232:             [convert_backslashes(x) for x in include_list],
233:             [convert_backslashes(x) for x in original_sources])
[end of scripts/package_build.py]
[start of src/CMakeLists.txt]
1: add_definitions(-DDUCKDB)
2: 
3: if(${DISABLE_THREADS})
4:   add_definitions(-DDUCKDB_NO_THREADS)
5: endif()
6: 
7: if(NOT MSVC)
8:   set(CMAKE_CXX_FLAGS_DEBUG
9:       "${CMAKE_CXX_FLAGS_DEBUG} -Wextra -Wno-unused-parameter -Wno-redundant-move"
10:   )
11: endif()
12: 
13: # clang-tidy config from presto-cpp set(CMAKE_CXX_CLANG_TIDY clang-tidy
14: # --checks=*,-abseil-*,-android-*,-cert-err58-cpp,-clang-analyzer-
15: # osx-*,-cppcoreguidelines-avoid-c-arrays,-cppcoreguidelines-avoid-magic-
16: # numbers,-cppcoreguidelines-pro-bounds-array-to-pointer-
17: # decay,-cppcoreguidelines-pro-bounds-pointer-arithmetic,-cppcoreguidelines-pro-
18: # type-reinterpret-cast,-cppcoreguidelines-pro-type-
19: # vararg,-fuchsia-*,-google-*,-hicpp-avoid-c-arrays,-hicpp-deprecated-
20: # headers,-hicpp-no-array-decay,-hicpp-use-equals-default,-hicpp-
21: # vararg,-llvmlibc-*,-llvm-header-guard,-llvm-include-order,-mpi-*,-misc-non-
22: # private-member-variables-in-classes,-misc-no-recursion,-misc-unused-
23: # parameters,-modernize-avoid-c-arrays,-modernize-deprecated-headers,-modernize-
24: # use-nodiscard,-modernize-use-trailing-return-
25: # type,-objc-*,-openmp-*,-readability-avoid-const-params-in-decls,-readability-
26: # convert-member-functions-to-static,-readability-magic-numbers,-zircon-*)
27: 
28: if(AMALGAMATION_BUILD)
29: 
30:   if(WIN32)
31:     add_definitions(/bigobj)
32:   endif()
33: 
34:   add_library(duckdb SHARED "${PROJECT_SOURCE_DIR}/src/amalgamation/duckdb.cpp")
35:   target_link_libraries(duckdb ${CMAKE_DL_LIBS})
36:   link_threads(duckdb)
37: 
38:   add_library(duckdb_static STATIC
39:               "${PROJECT_SOURCE_DIR}/src/amalgamation/duckdb.cpp")
40:   target_link_libraries(duckdb_static ${CMAKE_DL_LIBS})
41:   link_threads(duckdb_static)
42: 
43:   install(FILES "${PROJECT_SOURCE_DIR}/src/amalgamation/duckdb.hpp"
44:                 "${PROJECT_SOURCE_DIR}/src/include/duckdb.h"
45:           DESTINATION "${INSTALL_INCLUDE_DIR}")
46:   install(FILES "${PROJECT_SOURCE_DIR}/src/include/duckdb/common/winapi.hpp"
47:           DESTINATION "${INSTALL_INCLUDE_DIR}/duckdb/common")
48: 
49: else()
50: 
51:   add_definitions(-DDUCKDB_MAIN_LIBRARY)
52: 
53:   add_subdirectory(optimizer)
54:   add_subdirectory(planner)
55:   add_subdirectory(parser)
56:   add_subdirectory(function)
57:   add_subdirectory(catalog)
58:   add_subdirectory(common)
59:   add_subdirectory(execution)
60:   add_subdirectory(main)
61:   add_subdirectory(parallel)
62:   add_subdirectory(storage)
63:   add_subdirectory(transaction)
64: 
65:   set(DUCKDB_LINK_LIBS
66:       ${CMAKE_DL_LIBS}
67:       fmt
68:       pg_query
69:       duckdb_re2
70:       miniz
71:       utf8proc
72:       hyperloglog)
73: 
74:   add_library(duckdb SHARED ${ALL_OBJECT_FILES})
75:   target_link_libraries(duckdb ${DUCKDB_LINK_LIBS})
76:   link_threads(duckdb)
77: 
78:   add_library(duckdb_static STATIC ${ALL_OBJECT_FILES})
79:   target_link_libraries(duckdb_static ${DUCKDB_LINK_LIBS})
80:   link_threads(duckdb_static)
81: 
82:   target_include_directories(
83:     duckdb PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>
84:                   $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>)
85: 
86:   target_include_directories(
87:     duckdb_static PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>
88:                          $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>)
89: 
90:   install(
91:     DIRECTORY "${PROJECT_SOURCE_DIR}/src/include/duckdb"
92:     DESTINATION "${INSTALL_INCLUDE_DIR}"
93:     FILES_MATCHING
94:     PATTERN "*.hpp")
95:   install(FILES "${PROJECT_SOURCE_DIR}/src/include/duckdb.hpp"
96:                 "${PROJECT_SOURCE_DIR}/src/include/duckdb.h"
97:           DESTINATION "${INSTALL_INCLUDE_DIR}")
98: 
99: endif()
100: 
101: if(BUILD_PYTHON
102:    OR BUILD_R
103:    OR CONFIGURE_R)
104:   if(CMAKE_BUILD_TYPE STREQUAL "Debug")
105:     set(ALL_COMPILE_FLAGS "${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_DEBUG}")
106:   elseif(CMAKE_BUILD_TYPE STREQUAL "Release")
107:     set(ALL_COMPILE_FLAGS "${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_RELEASE}")
108:   elseif(CMAKE_BUILD_TYPE STREQUAL "RelWithDebInfo")
109:     set(ALL_COMPILE_FLAGS
110:         "${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_RELWITHDEBINFO}")
111:   else()
112:     set(ALL_COMPILE_FLAGS "${CMAKE_CXX_FLAGS}")
113:   endif()
114: 
115:   get_target_property(duckdb_libs duckdb LINK_LIBRARIES)
116: 
117:   if(BUILD_PYTHON)
118:     if(USER_SPACE)
119:       add_custom_target(
120:         duckdb_python ALL
121:         COMMAND python3 setup.py install --user --binary-dir=${CMAKE_BINARY_DIR}
122:                 --compile-flags=${ALL_COMPILE_FLAGS} --libs="${duckdb_libs}"
123:         DEPENDS duckdb
124:         WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/tools/pythonpkg
125:         COMMENT Build
126:         Python package)
127:     else()
128:       add_custom_target(
129:         duckdb_python ALL
130:         COMMAND python3 setup.py install --binary-dir=${CMAKE_BINARY_DIR}
131:                 --compile-flags=${ALL_COMPILE_FLAGS} --libs="${duckdb_libs}"
132:         DEPENDS duckdb
133:         WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/tools/pythonpkg
134:         COMMENT Build
135:         Python package)
136:     endif()
137:     add_extension_dependencies(duckdb_python)
138:   endif()
139:   if(CONFIGURE_R)
140:     add_custom_target(
141:       duckdb_configure_r ALL
142:       COMMAND
143:         DUCKDB_R_BINDIR=${CMAKE_BINARY_DIR}
144:         DUCKDB_R_CFLAGS=\"${ALL_COMPILE_FLAGS}\"
145:         DUCKDB_R_LIBS=\"${duckdb_libs}\" python rconfigure.py
146:       DEPENDS duckdb
147:       WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/tools/rpkg
148:       COMMENT Configure
149:       R package)
150:     add_extension_dependencies(duckdb_configure_r)
151:   endif()
152:   if(BUILD_R)
153:     add_custom_target(
154:       duckdb_r ALL
155:       COMMAND
156:         DUCKDB_R_BINDIR=${CMAKE_BINARY_DIR}
157:         DUCKDB_R_CFLAGS=\"${ALL_COMPILE_FLAGS}\"
158:         DUCKDB_R_LIBS=\"${duckdb_libs}\" DUCKDB_R_DEBUG= R CMD INSTALL .
159:       DEPENDS duckdb
160:       WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/tools/rpkg
161:       COMMENT Build
162:       R package)
163:     add_extension_dependencies(duckdb_r)
164:   endif()
165: endif()
166: 
167: install(
168:   TARGETS duckdb duckdb_static
169:   EXPORT "${DUCKDB_EXPORT_SET}"
170:   LIBRARY DESTINATION "${INSTALL_LIB_DIR}"
171:   ARCHIVE DESTINATION "${INSTALL_LIB_DIR}")
[end of src/CMakeLists.txt]
[start of src/include/duckdb/main/config.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/main/config.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/allocator.hpp"
12: #include "duckdb/common/case_insensitive_map.hpp"
13: #include "duckdb/common/common.hpp"
14: #include "duckdb/common/enums/order_type.hpp"
15: #include "duckdb/common/file_system.hpp"
16: #include "duckdb/common/winapi.hpp"
17: #include "duckdb/common/types/value.hpp"
18: #include "duckdb/common/vector.hpp"
19: #include "duckdb/function/replacement_scan.hpp"
20: #include "duckdb/common/set.hpp"
21: #include "duckdb/common/enums/compression_type.hpp"
22: #include "duckdb/common/enums/optimizer_type.hpp"
23: #include "duckdb/common/enums/window_aggregation_mode.hpp"
24: 
25: namespace duckdb {
26: class ClientContext;
27: class TableFunctionRef;
28: class CompressionFunction;
29: 
30: struct CompressionFunctionSet;
31: 
32: enum class AccessMode : uint8_t { UNDEFINED = 0, AUTOMATIC = 1, READ_ONLY = 2, READ_WRITE = 3 };
33: 
34: enum class CheckpointAbort : uint8_t {
35: 	NO_ABORT = 0,
36: 	DEBUG_ABORT_BEFORE_TRUNCATE = 1,
37: 	DEBUG_ABORT_BEFORE_HEADER = 2,
38: 	DEBUG_ABORT_AFTER_FREE_LIST_WRITE = 3
39: };
40: 
41: enum class ConfigurationOptionType : uint32_t {
42: 	INVALID = 0,
43: 	ACCESS_MODE,
44: 	DEFAULT_ORDER_TYPE,
45: 	DEFAULT_NULL_ORDER,
46: 	ENABLE_EXTERNAL_ACCESS,
47: 	ENABLE_OBJECT_CACHE,
48: 	MAXIMUM_MEMORY,
49: 	THREADS
50: };
51: 
52: struct ConfigurationOption {
53: 	ConfigurationOptionType type;
54: 	const char *name;
55: 	const char *description;
56: 	LogicalTypeId parameter_type;
57: };
58: 
59: // this is optional and only used in tests at the moment
60: struct DBConfig {
61: 	friend class DatabaseInstance;
62: 	friend class StorageManager;
63: 
64: public:
65: 	DUCKDB_API DBConfig();
66: 	DUCKDB_API ~DBConfig();
67: 
68: 	//! Access mode of the database (AUTOMATIC, READ_ONLY or READ_WRITE)
69: 	AccessMode access_mode = AccessMode::AUTOMATIC;
70: 	//! The allocator used by the system
71: 	Allocator allocator;
72: 	// Checkpoint when WAL reaches this size (default: 16MB)
73: 	idx_t checkpoint_wal_size = 1 << 24;
74: 	//! Whether or not to use Direct IO, bypassing operating system buffers
75: 	bool use_direct_io = false;
76: 	//! The FileSystem to use, can be overwritten to allow for injecting custom file systems for testing purposes (e.g.
77: 	//! RamFS or something similar)
78: 	unique_ptr<FileSystem> file_system;
79: 	//! The maximum memory used by the database system (in bytes). Default: 80% of System available memory
80: 	idx_t maximum_memory = (idx_t)-1;
81: 	//! The maximum amount of CPU threads used by the database system. Default: all available.
82: 	idx_t maximum_threads = (idx_t)-1;
83: 	//! Whether or not to create and use a temporary directory to store intermediates that do not fit in memory
84: 	bool use_temporary_directory = true;
85: 	//! Directory to store temporary structures that do not fit in memory
86: 	string temporary_directory;
87: 	//! The collation type of the database
88: 	string collation = string();
89: 	//! The order type used when none is specified (default: ASC)
90: 	OrderType default_order_type = OrderType::ASCENDING;
91: 	//! Null ordering used when none is specified (default: NULLS FIRST)
92: 	OrderByNullType default_null_order = OrderByNullType::NULLS_FIRST;
93: 	//! enable COPY and related commands
94: 	bool enable_external_access = true;
95: 	//! Whether or not object cache is used
96: 	bool object_cache_enable = false;
97: 	//! Database configuration variables as controlled by SET
98: 	case_insensitive_map_t<Value> set_variables;
99: 	//! Force checkpoint when CHECKPOINT is called or on shutdown, even if no changes have been made
100: 	bool force_checkpoint = false;
101: 	//! Run a checkpoint on successful shutdown and delete the WAL, to leave only a single database file behind
102: 	bool checkpoint_on_shutdown = true;
103: 	//! Debug flag that decides when a checkpoing should be aborted. Only used for testing purposes.
104: 	CheckpointAbort checkpoint_abort = CheckpointAbort::NO_ABORT;
105: 	//! Replacement table scans are automatically attempted when a table name cannot be found in the schema
106: 	vector<ReplacementScan> replacement_scans;
107: 	//! Initialize the database with the standard set of DuckDB functions
108: 	//! You should probably not touch this unless you know what you are doing
109: 	bool initialize_default_database = true;
110: 	//! The set of disabled optimizers (default empty)
111: 	set<OptimizerType> disabled_optimizers;
112: 	//! Force a specific compression method to be used when checkpointing (if available)
113: 	CompressionType force_compression = CompressionType::COMPRESSION_AUTO;
114: 	//! Debug flag that adds additional (unnecessary) free_list blocks to the storage
115: 	bool debug_many_free_list_blocks = false;
116: 	//! Debug setting for window aggregation mode: (window, combine, separate)
117: 	WindowAggregationMode window_mode = WindowAggregationMode::WINDOW;
118: 
119: public:
120: 	DUCKDB_API static DBConfig &GetConfig(ClientContext &context);
121: 	DUCKDB_API static DBConfig &GetConfig(DatabaseInstance &db);
122: 	DUCKDB_API static vector<ConfigurationOption> GetOptions();
123: 	DUCKDB_API static idx_t GetOptionCount();
124: 
125: 	//! Fetch an option by index. Returns a pointer to the option, or nullptr if out of range
126: 	DUCKDB_API static ConfigurationOption *GetOptionByIndex(idx_t index);
127: 	//! Fetch an option by name. Returns a pointer to the option, or nullptr if none exists.
128: 	DUCKDB_API static ConfigurationOption *GetOptionByName(const string &name);
129: 
130: 	DUCKDB_API void SetOption(const ConfigurationOption &option, const Value &value);
131: 
132: 	DUCKDB_API static idx_t ParseMemoryLimit(const string &arg);
133: 
134: 	//! Return the list of possible compression functions for the specific physical type
135: 	DUCKDB_API vector<CompressionFunction *> GetCompressionFunctions(PhysicalType data_type);
136: 	//! Return the compression function for the specified compression type/physical type combo
137: 	DUCKDB_API CompressionFunction *GetCompressionFunction(CompressionType type, PhysicalType data_type);
138: 
139: private:
140: 	unique_ptr<CompressionFunctionSet> compression_functions;
141: };
142: 
143: } // namespace duckdb
[end of src/include/duckdb/main/config.hpp]
[start of src/main/CMakeLists.txt]
1: if(NOT CLANG_TIDY)
2:   add_subdirectory(capi)
3: endif()
4: add_subdirectory(relation)
5: 
6: if(FORCE_QUERY_LOG)
7:   add_definitions(-DDUCKDB_FORCE_QUERY_LOG="\""${FORCE_QUERY_LOG}"\"")
8: endif()
9: 
10: add_library_unity(
11:   duckdb_main
12:   OBJECT
13:   appender.cpp
14:   client_context_file_opener.cpp
15:   client_context.cpp
16:   config.cpp
17:   connection.cpp
18:   database.cpp
19:   materialized_query_result.cpp
20:   prepared_statement.cpp
21:   prepared_statement_data.cpp
22:   relation.cpp
23:   query_profiler.cpp
24:   query_result.cpp
25:   stream_query_result.cpp)
26: set(ALL_OBJECT_FILES
27:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_main>
28:     PARENT_SCOPE)
[end of src/main/CMakeLists.txt]
[start of src/main/database.cpp]
1: #include "duckdb/main/database.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/common/virtual_file_system.hpp"
5: #include "duckdb/main/client_context.hpp"
6: #include "duckdb/parallel/task_scheduler.hpp"
7: #include "duckdb/storage/storage_manager.hpp"
8: #include "duckdb/storage/object_cache.hpp"
9: #include "duckdb/transaction/transaction_manager.hpp"
10: #include "duckdb/main/connection_manager.hpp"
11: #include "duckdb/function/compression_function.hpp"
12: 
13: #ifndef DUCKDB_NO_THREADS
14: #include "duckdb/common/thread.hpp"
15: #endif
16: 
17: namespace duckdb {
18: 
19: DBConfig::DBConfig() {
20: 	compression_functions = make_unique<CompressionFunctionSet>();
21: }
22: 
23: DBConfig::~DBConfig() {
24: }
25: 
26: DatabaseInstance::DatabaseInstance() {
27: }
28: 
29: DatabaseInstance::~DatabaseInstance() {
30: 	if (std::uncaught_exception()) {
31: 		return;
32: 	}
33: 
34: 	// shutting down: attempt to checkpoint the database
35: 	// but only if we are not cleaning up as part of an exception unwind
36: 	try {
37: 		auto &storage = StorageManager::GetStorageManager(*this);
38: 		if (!storage.InMemory()) {
39: 			auto &config = storage.db.config;
40: 			if (!config.checkpoint_on_shutdown) {
41: 				return;
42: 			}
43: 			storage.CreateCheckpoint(true);
44: 		}
45: 	} catch (...) {
46: 	}
47: }
48: 
49: BufferManager &BufferManager::GetBufferManager(DatabaseInstance &db) {
50: 	return *db.GetStorageManager().buffer_manager;
51: }
52: 
53: BlockManager &BlockManager::GetBlockManager(DatabaseInstance &db) {
54: 	return *db.GetStorageManager().block_manager;
55: }
56: 
57: BlockManager &BlockManager::GetBlockManager(ClientContext &context) {
58: 	return BlockManager::GetBlockManager(DatabaseInstance::GetDatabase(context));
59: }
60: 
61: DatabaseInstance &DatabaseInstance::GetDatabase(ClientContext &context) {
62: 	return *context.db;
63: }
64: 
65: StorageManager &StorageManager::GetStorageManager(DatabaseInstance &db) {
66: 	return db.GetStorageManager();
67: }
68: 
69: Catalog &Catalog::GetCatalog(DatabaseInstance &db) {
70: 	return db.GetCatalog();
71: }
72: 
73: FileSystem &FileSystem::GetFileSystem(DatabaseInstance &db) {
74: 	return db.GetFileSystem();
75: }
76: 
77: DBConfig &DBConfig::GetConfig(DatabaseInstance &db) {
78: 	return db.config;
79: }
80: 
81: TransactionManager &TransactionManager::Get(ClientContext &context) {
82: 	return TransactionManager::Get(DatabaseInstance::GetDatabase(context));
83: }
84: 
85: TransactionManager &TransactionManager::Get(DatabaseInstance &db) {
86: 	return db.GetTransactionManager();
87: }
88: 
89: ConnectionManager &ConnectionManager::Get(DatabaseInstance &db) {
90: 	return db.GetConnectionManager();
91: }
92: 
93: ConnectionManager &ConnectionManager::Get(ClientContext &context) {
94: 	return ConnectionManager::Get(DatabaseInstance::GetDatabase(context));
95: }
96: 
97: void DatabaseInstance::Initialize(const char *path, DBConfig *new_config) {
98: 	if (new_config) {
99: 		// user-supplied configuration
100: 		Configure(*new_config);
101: 	} else {
102: 		// default configuration
103: 		DBConfig config;
104: 		Configure(config);
105: 	}
106: 	if (config.temporary_directory.empty() && path) {
107: 		// no directory specified: use default temp path
108: 		config.temporary_directory = string(path) + ".tmp";
109: 
110: 		// special treatment for in-memory mode
111: 		if (strcmp(path, ":memory:") == 0) {
112: 			config.temporary_directory = ".tmp";
113: 		}
114: 	}
115: 	if (new_config && !new_config->use_temporary_directory) {
116: 		// temporary directories explicitly disabled
117: 		config.temporary_directory = string();
118: 	}
119: 
120: 	storage =
121: 	    make_unique<StorageManager>(*this, path ? string(path) : string(), config.access_mode == AccessMode::READ_ONLY);
122: 	catalog = make_unique<Catalog>(*this);
123: 	transaction_manager = make_unique<TransactionManager>(*this);
124: 	scheduler = make_unique<TaskScheduler>();
125: 	object_cache = make_unique<ObjectCache>();
126: 	connection_manager = make_unique<ConnectionManager>();
127: 
128: 	// initialize the database
129: 	storage->Initialize();
130: 
131: 	// only increase thread count after storage init because we get races on catalog otherwise
132: 	scheduler->SetThreads(config.maximum_threads);
133: }
134: 
135: DuckDB::DuckDB(const char *path, DBConfig *new_config) : instance(make_shared<DatabaseInstance>()) {
136: 	instance->Initialize(path, new_config);
137: }
138: 
139: DuckDB::DuckDB(const string &path, DBConfig *config) : DuckDB(path.c_str(), config) {
140: }
141: 
142: DuckDB::~DuckDB() {
143: }
144: 
145: StorageManager &DatabaseInstance::GetStorageManager() {
146: 	return *storage;
147: }
148: 
149: Catalog &DatabaseInstance::GetCatalog() {
150: 	return *catalog;
151: }
152: 
153: TransactionManager &DatabaseInstance::GetTransactionManager() {
154: 	return *transaction_manager;
155: }
156: 
157: TaskScheduler &DatabaseInstance::GetScheduler() {
158: 	return *scheduler;
159: }
160: 
161: ObjectCache &DatabaseInstance::GetObjectCache() {
162: 	return *object_cache;
163: }
164: 
165: FileSystem &DatabaseInstance::GetFileSystem() {
166: 	return *config.file_system;
167: }
168: 
169: ConnectionManager &DatabaseInstance::GetConnectionManager() {
170: 	return *connection_manager;
171: }
172: 
173: FileSystem &DuckDB::GetFileSystem() {
174: 	return instance->GetFileSystem();
175: }
176: 
177: Allocator &Allocator::Get(ClientContext &context) {
178: 	return Allocator::Get(*context.db);
179: }
180: 
181: Allocator &Allocator::Get(DatabaseInstance &db) {
182: 	return db.config.allocator;
183: }
184: 
185: void DatabaseInstance::Configure(DBConfig &new_config) {
186: 	config.access_mode = AccessMode::READ_WRITE;
187: 	if (new_config.access_mode != AccessMode::UNDEFINED) {
188: 		config.access_mode = new_config.access_mode;
189: 	}
190: 	if (new_config.file_system) {
191: 		config.file_system = move(new_config.file_system);
192: 	} else {
193: 		config.file_system = make_unique<VirtualFileSystem>();
194: 	}
195: 	config.maximum_memory = new_config.maximum_memory;
196: 	if (config.maximum_memory == (idx_t)-1) {
197: 		config.maximum_memory = FileSystem::GetAvailableMemory() * 8 / 10;
198: 	}
199: 	if (new_config.maximum_threads == (idx_t)-1) {
200: #ifndef DUCKDB_NO_THREADS
201: 		config.maximum_threads = std::thread::hardware_concurrency();
202: #else
203: 		config.maximum_threads = 1;
204: #endif
205: 	} else {
206: 		config.maximum_threads = new_config.maximum_threads;
207: 	}
208: 	config.force_compression = new_config.force_compression;
209: 	config.allocator = move(new_config.allocator);
210: 	config.checkpoint_wal_size = new_config.checkpoint_wal_size;
211: 	config.use_direct_io = new_config.use_direct_io;
212: 	config.temporary_directory = new_config.temporary_directory;
213: 	config.collation = new_config.collation;
214: 	config.default_order_type = new_config.default_order_type;
215: 	config.default_null_order = new_config.default_null_order;
216: 	config.enable_external_access = new_config.enable_external_access;
217: 	config.replacement_scans = move(new_config.replacement_scans);
218: 	config.initialize_default_database = new_config.initialize_default_database;
219: 	config.disabled_optimizers = move(new_config.disabled_optimizers);
220: }
221: 
222: DBConfig &DBConfig::GetConfig(ClientContext &context) {
223: 	return context.db->config;
224: }
225: 
226: idx_t DatabaseInstance::NumberOfThreads() {
227: 	return scheduler->NumberOfThreads();
228: }
229: 
230: idx_t DuckDB::NumberOfThreads() {
231: 	return instance->NumberOfThreads();
232: }
233: 
234: } // namespace duckdb
[end of src/main/database.cpp]
[start of third_party/sqlsmith/duckdb.cc]
1: #include "duckdb.hh"
2: #include "tpch-extension.hpp"
3: 
4: #include <cassert>
5: #include <cstring>
6: #include <iostream>
7: #include <stdexcept>
8: #include <thread>
9: #include <chrono>
10: 
11: #include <regex>
12: 
13: using namespace duckdb;
14: using namespace std;
15: 
16: static regex e_syntax("syntax error at or near .*");
17: 
18: duckdb_connection::duckdb_connection(string &conninfo) {
19: 	// in-memory database
20: 	database = make_unique<DuckDB>(nullptr);
21: 	database->LoadExtension<TPCHExtension>();
22: 	connection = make_unique<Connection>(*database);
23: }
24: 
25: void duckdb_connection::q(const char *query) {
26: 	auto result = connection->Query(query);
27: 	if (!result->success) {
28: 		throw runtime_error(result->error);
29: 	}
30: }
31: 
32: schema_duckdb::schema_duckdb(std::string &conninfo, bool no_catalog) : duckdb_connection(conninfo) {
33: 	// generate empty TPC-H schema
34: 	connection->Query("CALL dbgen(sf=0)");
35: 
36: 	cerr << "Loading tables...";
37: 	auto result = connection->Query("SELECT * FROM sqlite_master WHERE type IN ('table', 'view')");
38: 	if (!result->success) {
39: 		throw runtime_error(result->error);
40: 	}
41: 	for (size_t i = 0; i < result->collection.Count(); i++) {
42: 		auto type = result->collection.GetValue(0, i).str_value;
43: 		auto name = result->collection.GetValue(2, i).str_value;
44: 		bool view = type == "view";
45: 		table tab(name, "main", !view, !view);
46: 		tables.push_back(tab);
47: 	}
48: 	cerr << "done." << endl;
49: 
50: 	if (tables.size() == 0) {
51: 		throw std::runtime_error("No tables available in catalog!");
52: 	}
53: 
54: 	cerr << "Loading columns and constraints...";
55: 
56: 	for (auto t = tables.begin(); t != tables.end(); ++t) {
57: 		result = connection->Query("PRAGMA table_info('" + t->name + "')");
58: 		if (!result->success) {
59: 			throw runtime_error(result->error);
60: 		}
61: 		for (size_t i = 0; i < result->collection.Count(); i++) {
62: 			auto name = result->collection.GetValue(1, i).str_value;
63: 			auto type = result->collection.GetValue(2, i).str_value;
64: 			column c(name, sqltype::get(type));
65: 			t->columns().push_back(c);
66: 		}
67: 	}
68: 
69: 	cerr << "done." << endl;
70: 
71: #define BINOP(n, t)                                                                                                    \
72: 	do {                                                                                                               \
73: 		op o(#n, sqltype::get(#t), sqltype::get(#t), sqltype::get(#t));                                                \
74: 		register_operator(o);                                                                                          \
75: 	} while (0)
76: 
77: 	BINOP(||, TEXT);
78: 	BINOP(*, INTEGER);
79: 	BINOP(/, INTEGER);
80: 
81: 	BINOP(+, INTEGER);
82: 	BINOP(-, INTEGER);
83: 
84: 	BINOP(>>, INTEGER);
85: 	BINOP(<<, INTEGER);
86: 
87: 	BINOP(&, INTEGER);
88: 	BINOP(|, INTEGER);
89: 
90: 	BINOP(<, INTEGER);
91: 	BINOP(<=, INTEGER);
92: 	BINOP(>, INTEGER);
93: 	BINOP(>=, INTEGER);
94: 
95: 	BINOP(=, INTEGER);
96: 	BINOP(<>, INTEGER);
97: 	BINOP(IS, INTEGER);
98: 	BINOP(IS NOT, INTEGER);
99: 
100: 	BINOP(AND, INTEGER);
101: 	BINOP(OR, INTEGER);
102: 
103: #define FUNC(n, r)                                                                                                     \
104: 	do {                                                                                                               \
105: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
106: 		register_routine(proc);                                                                                        \
107: 	} while (0)
108: 
109: #define FUNC1(n, r, a)                                                                                                 \
110: 	do {                                                                                                               \
111: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
112: 		proc.argtypes.push_back(sqltype::get(#a));                                                                     \
113: 		register_routine(proc);                                                                                        \
114: 	} while (0)
115: 
116: #define FUNC2(n, r, a, b)                                                                                              \
117: 	do {                                                                                                               \
118: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
119: 		proc.argtypes.push_back(sqltype::get(#a));                                                                     \
120: 		proc.argtypes.push_back(sqltype::get(#b));                                                                     \
121: 		register_routine(proc);                                                                                        \
122: 	} while (0)
123: 
124: #define FUNC3(n, r, a, b, c)                                                                                           \
125: 	do {                                                                                                               \
126: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
127: 		proc.argtypes.push_back(sqltype::get(#a));                                                                     \
128: 		proc.argtypes.push_back(sqltype::get(#b));                                                                     \
129: 		proc.argtypes.push_back(sqltype::get(#c));                                                                     \
130: 		register_routine(proc);                                                                                        \
131: 	} while (0)
132: 
133: 	// FUNC(last_insert_rowid, INTEGER);
134: 	// FUNC(random, INTEGER);
135: 	// FUNC(sqlite_source_id, TEXT);
136: 	// FUNC(sqlite_version, TEXT);
137: 	// FUNC(total_changes, INTEGER);
138: 
139: 	FUNC1(abs, INTEGER, REAL);
140: 	// FUNC1(hex, TEXT, TEXT);
141: 	FUNC1(length, INTEGER, TEXT);
142: 	FUNC1(lower, TEXT, TEXT);
143: 	// FUNC1(ltrim, TEXT, TEXT);
144: 	// FUNC1(quote, TEXT, TEXT);
145: 	// FUNC1(randomblob, TEXT, INTEGER);
146: 	FUNC1(round, INTEGER, REAL);
147: 	// FUNC1(rtrim, TEXT, TEXT);
148: 	// FUNC1(soundex, TEXT, TEXT);
149: 	// FUNC1(sqlite_compileoption_get, TEXT, INTEGER);
150: 	// FUNC1(sqlite_compileoption_used, INTEGER, TEXT);
151: 	// FUNC1(trim, TEXT, TEXT);
152: 	// FUNC1(typeof, TEXT, INTEGER);
153: 	// FUNC1(typeof, TEXT, NUMERIC);
154: 	// FUNC1(typeof, TEXT, REAL);
155: 	// FUNC1(typeof, TEXT, TEXT);
156: 	// FUNC1(unicode, INTEGER, TEXT);
157: 	FUNC1(upper, TEXT, TEXT);
158: 	// FUNC1(zeroblob, TEXT, INTEGER);
159: 
160: 	// FUNC2(glob, INTEGER, TEXT, TEXT);
161: 	// FUNC2(instr, INTEGER, TEXT, TEXT);
162: 	// FUNC2(like, INTEGER, TEXT, TEXT);
163: 	// FUNC2(ltrim, TEXT, TEXT, TEXT);
164: 	// FUNC2(rtrim, TEXT, TEXT, TEXT);
165: 	// FUNC2(trim, TEXT, TEXT, TEXT);
166: 	// FUNC2(round, INTEGER, REAL, INTEGER);
167: 	FUNC2(substr, TEXT, TEXT, INTEGER);
168: 
169: 	FUNC3(substr, TEXT, TEXT, INTEGER, INTEGER);
170: 	FUNC3(replace, TEXT, TEXT, TEXT, TEXT);
171: 
172: #define AGG(n, r, a)                                                                                                   \
173: 	do {                                                                                                               \
174: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
175: 		proc.argtypes.push_back(sqltype::get(#a));                                                                     \
176: 		register_aggregate(proc);                                                                                      \
177: 	} while (0)
178: 
179: 	AGG(avg, INTEGER, INTEGER);
180: 	AGG(avg, REAL, REAL);
181: 	AGG(count, INTEGER, REAL);
182: 	AGG(count, INTEGER, TEXT);
183: 	AGG(count, INTEGER, INTEGER);
184: 	AGG(string_agg, TEXT, TEXT);
185: 	AGG(max, REAL, REAL);
186: 	AGG(max, INTEGER, INTEGER);
187: 	AGG(min, REAL, REAL);
188: 	AGG(min, INTEGER, INTEGER);
189: 	AGG(sum, REAL, REAL);
190: 	AGG(sum, INTEGER, INTEGER);
191: 	// AGG(total, REAL, INTEGER);
192: 	// AGG(total, REAL, REAL);
193: 
194: 	booltype = sqltype::get("INTEGER");
195: 	inttype = sqltype::get("INTEGER");
196: 
197: 	internaltype = sqltype::get("internal");
198: 	arraytype = sqltype::get("ARRAY");
199: 
200: 	true_literal = "1";
201: 	false_literal = "0";
202: 
203: 	generate_indexes();
204: }
205: 
206: dut_duckdb::dut_duckdb(std::string &conninfo) : duckdb_connection(conninfo) {
207: 	cerr << "Generating TPC-H...";
208: 	connection->Query("CALL dbgen(sf=0.01)");
209: 	cerr << "done." << endl;
210: 	// q("PRAGMA main.auto_vacuum = 2");
211: }
212: 
213: volatile bool is_active = false;
214: // timeout is 10ms * TIMEOUT_TICKS
215: #define TIMEOUT_TICKS 50
216: 
217: void sleep_thread(Connection *connection) {
218: 	for (size_t i = 0; i < TIMEOUT_TICKS && is_active; i++) {
219: 		std::this_thread::sleep_for(std::chrono::milliseconds(10));
220: 	}
221: 	if (is_active) {
222: 		connection->Interrupt();
223: 	}
224: }
225: 
226: void dut_duckdb::test(const std::string &stmt) {
227: 	is_active = true;
228: 	thread interrupt_thread(sleep_thread, connection.get());
229: 	auto result = connection->Query(stmt);
230: 	is_active = false;
231: 	interrupt_thread.join();
232: 
233: 	if (!result->success) {
234: 		auto error = result->error.c_str();
235: 		try {
236: 			if (regex_match(error, e_syntax))
237: 				throw dut::syntax(error);
238: 			else
239: 				throw dut::failure(error);
240: 		} catch (dut::failure &e) {
241: 			throw;
242: 		}
243: 	}
244: }
[end of third_party/sqlsmith/duckdb.cc]
[start of tools/jdbc/src/jni/duckdb_java.cpp]
1: #include "org_duckdb_DuckDBNative.h"
2: #include "duckdb.hpp"
3: #include "duckdb/main/client_context.hpp"
4: #include "duckdb/main/appender.hpp"
5: #include "parquet-extension.hpp"
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: static string byte_array_to_string(JNIEnv *env, jbyteArray ba_j) {
11: 	idx_t len = env->GetArrayLength(ba_j);
12: 	string ret;
13: 	ret.resize(len);
14: 
15: 	jbyte *bytes = (jbyte *)env->GetByteArrayElements(ba_j, NULL);
16: 
17: 	for (idx_t i = 0; i < len; i++) {
18: 		ret[i] = bytes[i];
19: 	}
20: 	env->ReleaseByteArrayElements(ba_j, bytes, 0);
21: 
22: 	return ret;
23: }
24: 
25: static jobject decode_charbuffer_to_jstring(JNIEnv *env, const char *d_str, idx_t d_str_len) {
26: 	// TODO cache this somewhere, probably slow to look this stuff for every string
27: 	jclass charset_class = env->FindClass("java/nio/charset/Charset");
28: 	jclass charbuffer_class = env->FindClass("java/nio/CharBuffer");
29: 	jmethodID for_name =
30: 	    env->GetStaticMethodID(charset_class, "forName", "(Ljava/lang/String;)Ljava/nio/charset/Charset;");
31: 	jobject charset = env->CallStaticObjectMethod(charset_class, for_name, env->NewStringUTF("UTF-8"));
32: 	jmethodID charset_decode =
33: 	    env->GetMethodID(charset_class, "decode", "(Ljava/nio/ByteBuffer;)Ljava/nio/CharBuffer;");
34: 	jmethodID charbuffer_to_string = env->GetMethodID(charbuffer_class, "toString", "()Ljava/lang/String;");
35: 
36: 	auto bb = env->NewDirectByteBuffer((void *)d_str, d_str_len);
37: 	auto j_cb = env->CallObjectMethod(charset, charset_decode, bb);
38: 	auto j_str = env->CallObjectMethod(j_cb, charbuffer_to_string);
39: 	return j_str;
40: }
41: 
42: JNIEXPORT jobject JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1startup(JNIEnv *env, jclass, jbyteArray database_j,
43:                                                                              jboolean read_only) {
44: 	auto database = byte_array_to_string(env, database_j);
45: 	DBConfig config;
46: 	if (read_only) {
47: 		config.access_mode = AccessMode::READ_ONLY;
48: 	}
49: 	try {
50: 		auto db = new DuckDB(database, &config);
51: 		db->LoadExtension<ParquetExtension>();
52: 		return env->NewDirectByteBuffer(db, 0);
53: 	} catch (exception &e) {
54: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
55: 	}
56: 	return nullptr;
57: }
58: 
59: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1shutdown(JNIEnv *env, jclass, jobject db_ref_buf) {
60: 	auto db_ref = (DuckDB *)env->GetDirectBufferAddress(db_ref_buf);
61: 	if (db_ref) {
62: 		delete db_ref;
63: 	}
64: }
65: 
66: JNIEXPORT jobject JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1connect(JNIEnv *env, jclass, jobject db_ref_buf) {
67: 	auto db_ref = (DuckDB *)env->GetDirectBufferAddress(db_ref_buf);
68: 	try {
69: 		auto conn = new Connection(*db_ref);
70: 		return env->NewDirectByteBuffer(conn, 0);
71: 	} catch (exception &e) {
72: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
73: 	}
74: 	return nullptr;
75: }
76: 
77: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1set_1auto_1commit(JNIEnv *env, jclass,
78:                                                                                     jobject conn_ref_buf,
79:                                                                                     jboolean auto_commit) {
80: 	auto conn_ref = (Connection *)env->GetDirectBufferAddress(conn_ref_buf);
81: 	if (!conn_ref || !conn_ref->context) {
82: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), "Invalid connection");
83: 	}
84: 	conn_ref->context->RunFunctionInTransaction([&]() { conn_ref->SetAutoCommit(auto_commit); });
85: }
86: 
87: JNIEXPORT jboolean JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1get_1auto_1commit(JNIEnv *env, jclass,
88:                                                                                         jobject conn_ref_buf) {
89: 	auto conn_ref = (Connection *)env->GetDirectBufferAddress(conn_ref_buf);
90: 	if (!conn_ref) {
91: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), "Invalid connection");
92: 	}
93: 	return conn_ref->IsAutoCommit();
94: }
95: 
96: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1disconnect(JNIEnv *env, jclass,
97:                                                                              jobject conn_ref_buf) {
98: 	auto conn_ref = (Connection *)env->GetDirectBufferAddress(conn_ref_buf);
99: 	if (conn_ref) {
100: 		delete conn_ref;
101: 	}
102: }
103: 
104: struct StatementHolder {
105: 	unique_ptr<PreparedStatement> stmt;
106: };
107: 
108: #include "utf8proc_wrapper.hpp"
109: 
110: JNIEXPORT jobject JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1prepare(JNIEnv *env, jclass, jobject conn_ref_buf,
111:                                                                              jbyteArray query_j) {
112: 	auto conn_ref = (Connection *)env->GetDirectBufferAddress(conn_ref_buf);
113: 	if (!conn_ref) {
114: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), "Invalid connection");
115: 	}
116: 
117: 	auto query = byte_array_to_string(env, query_j);
118: 
119: 	auto stmt_ref = new StatementHolder();
120: 	stmt_ref->stmt = conn_ref->Prepare(query);
121: 	if (!stmt_ref->stmt->success) {
122: 		string error_msg = string(stmt_ref->stmt->error);
123: 		stmt_ref->stmt = nullptr;
124: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), error_msg.c_str());
125: 	}
126: 	return env->NewDirectByteBuffer(stmt_ref, 0);
127: }
128: 
129: struct ResultHolder {
130: 	unique_ptr<QueryResult> res;
131: 	unique_ptr<DataChunk> chunk;
132: };
133: 
134: JNIEXPORT jobject JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1execute(JNIEnv *env, jclass, jobject stmt_ref_buf,
135:                                                                              jobjectArray params) {
136: 	auto stmt_ref = (StatementHolder *)env->GetDirectBufferAddress(stmt_ref_buf);
137: 	if (!stmt_ref) {
138: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), "Invalid statement");
139: 	}
140: 	auto res_ref = new ResultHolder();
141: 	vector<Value> duckdb_params;
142: 
143: 	idx_t param_len = env->GetArrayLength(params);
144: 	if (param_len != stmt_ref->stmt->n_param) {
145: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), "Parameter count mismatch");
146: 	}
147: 
148: 	if (param_len > 0) {
149: 		auto bool_class = env->FindClass("java/lang/Boolean");
150: 		auto byte_class = env->FindClass("java/lang/Byte");
151: 		auto short_class = env->FindClass("java/lang/Short");
152: 		auto integer_class = env->FindClass("java/lang/Integer");
153: 		auto long_class = env->FindClass("java/lang/Long");
154: 		auto float_class = env->FindClass("java/lang/Float");
155: 		auto double_class = env->FindClass("java/lang/Double");
156: 		auto string_class = env->FindClass("java/lang/String");
157: 
158: 		for (idx_t i = 0; i < param_len; i++) {
159: 			auto param = env->GetObjectArrayElement(params, i);
160: 			if (param == nullptr) {
161: 				duckdb_params.push_back(Value());
162: 				continue;
163: 			} else if (env->IsInstanceOf(param, bool_class)) {
164: 				duckdb_params.push_back(
165: 				    Value::BOOLEAN(env->CallBooleanMethod(param, env->GetMethodID(bool_class, "booleanValue", "()Z"))));
166: 				continue;
167: 			} else if (env->IsInstanceOf(param, byte_class)) {
168: 				duckdb_params.push_back(
169: 				    Value::TINYINT(env->CallByteMethod(param, env->GetMethodID(byte_class, "byteValue", "()B"))));
170: 				continue;
171: 			} else if (env->IsInstanceOf(param, short_class)) {
172: 				duckdb_params.push_back(
173: 				    Value::SMALLINT(env->CallShortMethod(param, env->GetMethodID(short_class, "shortValue", "()S"))));
174: 				continue;
175: 			} else if (env->IsInstanceOf(param, integer_class)) {
176: 				duckdb_params.push_back(
177: 				    Value::INTEGER(env->CallIntMethod(param, env->GetMethodID(integer_class, "intValue", "()I"))));
178: 				continue;
179: 			} else if (env->IsInstanceOf(param, long_class)) {
180: 				duckdb_params.push_back(
181: 				    Value::BIGINT(env->CallLongMethod(param, env->GetMethodID(long_class, "longValue", "()J"))));
182: 				continue;
183: 			} else if (env->IsInstanceOf(param, float_class)) {
184: 				duckdb_params.push_back(
185: 				    Value::FLOAT(env->CallFloatMethod(param, env->GetMethodID(float_class, "floatValue", "()F"))));
186: 				continue;
187: 			} else if (env->IsInstanceOf(param, double_class)) {
188: 				duckdb_params.push_back(
189: 				    Value::DOUBLE(env->CallDoubleMethod(param, env->GetMethodID(double_class, "doubleValue", "()D"))));
190: 				continue;
191: 			} else if (env->IsInstanceOf(param, string_class)) {
192: 				auto *param_string = env->GetStringUTFChars((jstring)param, 0);
193: 				duckdb_params.push_back(Value(param_string));
194: 				env->ReleaseStringUTFChars((jstring)param, param_string);
195: 				continue;
196: 			} else {
197: 				env->ThrowNew(env->FindClass("java/sql/SQLException"), "Unsupported parameter type");
198: 			}
199: 		}
200: 	}
201: 
202: 	res_ref->res = stmt_ref->stmt->Execute(duckdb_params, false);
203: 	if (!res_ref->res->success) {
204: 		string error_msg = string(res_ref->res->error);
205: 		res_ref->res = nullptr;
206: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), error_msg.c_str());
207: 	}
208: 	return env->NewDirectByteBuffer(res_ref, 0);
209: }
210: 
211: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1release(JNIEnv *env, jclass, jobject stmt_ref_buf) {
212: 	auto stmt_ref = (StatementHolder *)env->GetDirectBufferAddress(stmt_ref_buf);
213: 	if (stmt_ref) {
214: 		delete stmt_ref;
215: 	}
216: }
217: 
218: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1free_1result(JNIEnv *env, jclass,
219:                                                                                jobject res_ref_buf) {
220: 	auto res_ref = (ResultHolder *)env->GetDirectBufferAddress(res_ref_buf);
221: 	if (res_ref) {
222: 		delete res_ref;
223: 	}
224: }
225: 
226: JNIEXPORT jobject JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1meta(JNIEnv *env, jclass, jobject stmt_ref_buf) {
227: 
228: 	auto stmt_ref = (StatementHolder *)env->GetDirectBufferAddress(stmt_ref_buf);
229: 	if (!stmt_ref || !stmt_ref->stmt || !stmt_ref->stmt->success) {
230: 		jclass Exception = env->FindClass("java/sql/SQLException");
231: 		env->ThrowNew(Exception, "Invalid statement");
232: 	}
233: 
234: 	jclass meta = env->FindClass("org/duckdb/DuckDBResultSetMetaData");
235: 	jmethodID meta_construct = env->GetMethodID(meta, "<init>", "(II[Ljava/lang/String;[Ljava/lang/String;)V");
236: 
237: 	auto column_count = stmt_ref->stmt->ColumnCount();
238: 	auto &names = stmt_ref->stmt->GetNames();
239: 	auto &types = stmt_ref->stmt->GetTypes();
240: 
241: 	auto name_array = env->NewObjectArray(column_count, env->FindClass("java/lang/String"), nullptr);
242: 	auto type_array = env->NewObjectArray(column_count, env->FindClass("java/lang/String"), nullptr);
243: 
244: 	for (idx_t col_idx = 0; col_idx < column_count; col_idx++) {
245: 		env->SetObjectArrayElement(name_array, col_idx,
246: 		                           decode_charbuffer_to_jstring(env, names[col_idx].c_str(), names[col_idx].length()));
247: 		env->SetObjectArrayElement(type_array, col_idx, env->NewStringUTF(types[col_idx].ToString().c_str()));
248: 	}
249: 
250: 	return env->NewObject(meta, meta_construct, stmt_ref->stmt->n_param, column_count, name_array, type_array);
251: }
252: 
253: JNIEXPORT jobjectArray JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1fetch(JNIEnv *env, jclass,
254:                                                                                 jobject res_ref_buf) {
255: 	auto res_ref = (ResultHolder *)env->GetDirectBufferAddress(res_ref_buf);
256: 	if (!res_ref || !res_ref->res || !res_ref->res->success) {
257: 		jclass Exception = env->FindClass("java/sql/SQLException");
258: 		env->ThrowNew(Exception, "Invalid result set");
259: 	}
260: 
261: 	res_ref->chunk = res_ref->res->Fetch();
262: 	if (!res_ref->chunk) {
263: 		res_ref->chunk = make_unique<DataChunk>();
264: 	}
265: 	auto row_count = res_ref->chunk->size();
266: 
267: 	auto vec_array = (jobjectArray)env->NewObjectArray(res_ref->chunk->ColumnCount(),
268: 	                                                   env->FindClass("org/duckdb/DuckDBVector"), nullptr);
269: 	for (idx_t col_idx = 0; col_idx < res_ref->chunk->ColumnCount(); col_idx++) {
270: 		auto &vec = res_ref->chunk->data[col_idx];
271: 		auto type_str = env->NewStringUTF(vec.GetType().ToString().c_str());
272: 		// construct nullmask
273: 		auto null_array = env->NewBooleanArray(row_count);
274: 		jboolean *null_array_ptr = env->GetBooleanArrayElements(null_array, nullptr);
275: 		for (idx_t row_idx = 0; row_idx < row_count; row_idx++) {
276: 			null_array_ptr[row_idx] = FlatVector::IsNull(vec, row_idx);
277: 		}
278: 		env->ReleaseBooleanArrayElements(null_array, null_array_ptr, 0);
279: 
280: 		jclass vec_class = env->FindClass("org/duckdb/DuckDBVector");
281: 		jmethodID vec_construct = env->GetMethodID(vec_class, "<init>", "(Ljava/lang/String;I[Z)V");
282: 		auto jvec = env->NewObject(vec_class, vec_construct, type_str, (int)row_count, null_array);
283: 
284: 		jobject constlen_data = nullptr;
285: 		jobjectArray varlen_data = nullptr;
286: 
287: 		switch (vec.GetType().id()) {
288: 		case LogicalTypeId::BOOLEAN:
289: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(bool));
290: 			break;
291: 		case LogicalTypeId::TINYINT:
292: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(int8_t));
293: 			break;
294: 		case LogicalTypeId::SMALLINT:
295: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(int16_t));
296: 			break;
297: 		case LogicalTypeId::INTEGER:
298: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(int32_t));
299: 			break;
300: 		case LogicalTypeId::BIGINT:
301: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(int64_t));
302: 			break;
303: 		case LogicalTypeId::UTINYINT:
304: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(uint8_t));
305: 			break;
306: 		case LogicalTypeId::USMALLINT:
307: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(uint16_t));
308: 			break;
309: 		case LogicalTypeId::UINTEGER:
310: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(uint32_t));
311: 			break;
312: 		case LogicalTypeId::UBIGINT:
313: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(uint64_t));
314: 			break;
315: 		case LogicalTypeId::HUGEINT:
316: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(hugeint_t));
317: 			break;
318: 		case LogicalTypeId::FLOAT:
319: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(float));
320: 			break;
321: 		case LogicalTypeId::DECIMAL: {
322: 			Vector double_vec(LogicalType::DOUBLE);
323: 			VectorOperations::Cast(vec, double_vec, row_count);
324: 			vec.ReferenceAndSetType(double_vec);
325: 			// fall through on purpose
326: 		}
327: 		case LogicalTypeId::DOUBLE:
328: 			constlen_data = env->NewDirectByteBuffer(FlatVector::GetData(vec), row_count * sizeof(double));
329: 			break;
330: 		case LogicalTypeId::TIME:
331: 		case LogicalTypeId::DATE:
332: 		case LogicalTypeId::TIMESTAMP:
333: 		case LogicalTypeId::INTERVAL: {
334: 			Vector string_vec(LogicalType::VARCHAR);
335: 			VectorOperations::Cast(vec, string_vec, row_count);
336: 			vec.ReferenceAndSetType(string_vec);
337: 			// fall through on purpose
338: 		}
339: 		case LogicalTypeId::VARCHAR:
340: 			varlen_data = env->NewObjectArray(row_count, env->FindClass("java/lang/String"), nullptr);
341: 			for (idx_t row_idx = 0; row_idx < row_count; row_idx++) {
342: 				if (FlatVector::IsNull(vec, row_idx)) {
343: 					continue;
344: 				}
345: 				auto d_str = ((string_t *)FlatVector::GetData(vec))[row_idx];
346: 				auto j_str = decode_charbuffer_to_jstring(env, d_str.GetDataUnsafe(), d_str.GetSize());
347: 				env->SetObjectArrayElement(varlen_data, row_idx, j_str);
348: 			}
349: 			break;
350: 		case LogicalTypeId::BLOB:
351: 			varlen_data = env->NewObjectArray(row_count, env->FindClass("java/nio/ByteBuffer"), nullptr);
352: 
353: 			for (idx_t row_idx = 0; row_idx < row_count; row_idx++) {
354: 				if (FlatVector::IsNull(vec, row_idx)) {
355: 					continue;
356: 				}
357: 				auto &d_str = ((string_t *)FlatVector::GetData(vec))[row_idx];
358: 				auto j_obj = env->NewDirectByteBuffer((void *)d_str.GetDataUnsafe(), d_str.GetSize());
359: 				env->SetObjectArrayElement(varlen_data, row_idx, j_obj);
360: 			}
361: 			break;
362: 		default:
363: 			jclass Exception = env->FindClass("java/sql/SQLException");
364: 			env->ThrowNew(Exception, ("Unsupported result column type " + vec.GetType().ToString()).c_str());
365: 		}
366: 
367: 		jfieldID constlen_data_field = env->GetFieldID(vec_class, "constlen_data", "Ljava/nio/ByteBuffer;");
368: 		jfieldID varlen_data_field = env->GetFieldID(vec_class, "varlen_data", "[Ljava/lang/Object;");
369: 
370: 		env->SetObjectField(jvec, constlen_data_field, constlen_data);
371: 		env->SetObjectField(jvec, varlen_data_field, varlen_data);
372: 
373: 		env->SetObjectArrayElement(vec_array, col_idx, jvec);
374: 	}
375: 
376: 	return vec_array;
377: }
378: 
379: JNIEXPORT jint JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1fetch_1size(JNIEnv *, jclass) {
380: 	return STANDARD_VECTOR_SIZE;
381: }
382: 
383: JNIEXPORT jstring JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1prepare_1type(JNIEnv *env, jclass,
384:                                                                                    jobject stmt_ref_buf) {
385: 
386: 	auto stmt_ref = (StatementHolder *)env->GetDirectBufferAddress(stmt_ref_buf);
387: 	if (!stmt_ref || !stmt_ref->stmt || !stmt_ref->stmt->success) {
388: 		jclass Exception = env->FindClass("java/sql/SQLException");
389: 		env->ThrowNew(Exception, "Invalid statement");
390: 	}
391: 	return env->NewStringUTF(StatementTypeToString(stmt_ref->stmt->GetStatementType()).c_str());
392: }
393: 
394: JNIEXPORT jobject JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1create_1appender(JNIEnv *env, jclass,
395:                                                                                       jobject conn_ref_buf,
396:                                                                                       jbyteArray schema_name_j,
397:                                                                                       jbyteArray table_name_j) {
398: 
399: 	auto conn_ref = (Connection *)env->GetDirectBufferAddress(conn_ref_buf);
400: 	if (!conn_ref || !conn_ref->context) {
401: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), "Invalid connection");
402: 	}
403: 	auto schema_name = byte_array_to_string(env, schema_name_j);
404: 	auto table_name = byte_array_to_string(env, table_name_j);
405: 	try {
406: 		auto appender = new Appender(*conn_ref, schema_name, table_name);
407: 		return env->NewDirectByteBuffer(appender, 0);
408: 	} catch (exception &e) {
409: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
410: 	}
411: 	return nullptr;
412: }
413: 
414: static Appender *get_appender(JNIEnv *env, jobject appender_ref_buf) {
415: 	auto appender_ref = (Appender *)env->GetDirectBufferAddress(appender_ref_buf);
416: 	if (!appender_ref) {
417: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), "Invalid appender");
418: 	}
419: 	return appender_ref;
420: }
421: 
422: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1begin_1row(JNIEnv *env, jclass,
423:                                                                                        jobject appender_ref_buf) {
424: 	try {
425: 		get_appender(env, appender_ref_buf)->BeginRow();
426: 	} catch (exception &e) {
427: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
428: 	}
429: }
430: 
431: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1end_1row(JNIEnv *env, jclass,
432:                                                                                      jobject appender_ref_buf) {
433: 	try {
434: 		get_appender(env, appender_ref_buf)->EndRow();
435: 	} catch (exception &e) {
436: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
437: 	}
438: }
439: 
440: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1flush(JNIEnv *env, jclass,
441:                                                                                   jobject appender_ref_buf) {
442: 	try {
443: 		get_appender(env, appender_ref_buf)->Flush();
444: 	} catch (exception &e) {
445: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
446: 	}
447: }
448: 
449: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1close(JNIEnv *env, jclass,
450:                                                                                   jobject appender_ref_buf) {
451: 	try {
452: 		auto appender = get_appender(env, appender_ref_buf);
453: 		appender->Close();
454: 		delete appender;
455: 	} catch (exception &e) {
456: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
457: 	}
458: }
459: 
460: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1append_1boolean(JNIEnv *env, jclass,
461:                                                                                             jobject appender_ref_buf,
462:                                                                                             jboolean value) {
463: 	try {
464: 		get_appender(env, appender_ref_buf)->Append((bool)value);
465: 	} catch (exception &e) {
466: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
467: 	}
468: }
469: 
470: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1append_1byte(JNIEnv *env, jclass,
471:                                                                                          jobject appender_ref_buf,
472:                                                                                          jbyte value) {
473: 	try {
474: 		get_appender(env, appender_ref_buf)->Append((int8_t)value);
475: 	} catch (exception &e) {
476: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
477: 	}
478: }
479: 
480: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1append_1short(JNIEnv *env, jclass,
481:                                                                                           jobject appender_ref_buf,
482:                                                                                           jshort value) {
483: 	try {
484: 		get_appender(env, appender_ref_buf)->Append((int16_t)value);
485: 	} catch (exception &e) {
486: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
487: 	}
488: }
489: 
490: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1append_1int(JNIEnv *env, jclass,
491:                                                                                         jobject appender_ref_buf,
492:                                                                                         jint value) {
493: 	try {
494: 		get_appender(env, appender_ref_buf)->Append((int32_t)value);
495: 	} catch (exception &e) {
496: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
497: 	}
498: }
499: 
500: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1append_1long(JNIEnv *env, jclass,
501:                                                                                          jobject appender_ref_buf,
502:                                                                                          jlong value) {
503: 	try {
504: 		get_appender(env, appender_ref_buf)->Append((int64_t)value);
505: 	} catch (exception &e) {
506: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
507: 	}
508: }
509: 
510: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1append_1float(JNIEnv *env, jclass,
511:                                                                                           jobject appender_ref_buf,
512:                                                                                           jfloat value) {
513: 	try {
514: 		get_appender(env, appender_ref_buf)->Append((float)value);
515: 	} catch (exception &e) {
516: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
517: 	}
518: }
519: 
520: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1append_1double(JNIEnv *env, jclass,
521:                                                                                            jobject appender_ref_buf,
522:                                                                                            jdouble value) {
523: 	try {
524: 		get_appender(env, appender_ref_buf)->Append((double)value);
525: 	} catch (exception &e) {
526: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
527: 	}
528: }
529: 
530: JNIEXPORT void JNICALL Java_org_duckdb_DuckDBNative_duckdb_1jdbc_1appender_1append_1string(JNIEnv *env, jclass,
531:                                                                                            jobject appender_ref_buf,
532:                                                                                            jbyteArray value) {
533: 	try {
534: 		auto string_value = byte_array_to_string(env, value);
535: 		get_appender(env, appender_ref_buf)->Append(string_value.c_str());
536: 	} catch (exception &e) {
537: 		env->ThrowNew(env->FindClass("java/sql/SQLException"), e.what());
538: 	}
539: }
[end of tools/jdbc/src/jni/duckdb_java.cpp]
[start of tools/pythonpkg/src/pyconnection.cpp]
1: #include "duckdb_python/pyconnection.hpp"
2: #include "duckdb_python/pyresult.hpp"
3: #include "duckdb_python/pyrelation.hpp"
4: #include "duckdb_python/pandas_scan.hpp"
5: #include "duckdb_python/map.hpp"
6: 
7: #include "duckdb/common/arrow.hpp"
8: #include "duckdb_python/arrow_array_stream.hpp"
9: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
10: #include "duckdb/main/client_context.hpp"
11: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
12: #include "duckdb/common/types/vector.hpp"
13: #include "duckdb/common/printer.hpp"
14: #include "duckdb/main/config.hpp"
15: #include "duckdb/parser/expression/constant_expression.hpp"
16: #include "duckdb/parser/expression/function_expression.hpp"
17: #include "duckdb/parser/tableref/table_function_ref.hpp"
18: 
19: #include "extension/extension_helper.hpp"
20: 
21: #include "datetime.h" // from Python
22: 
23: #include <random>
24: 
25: namespace duckdb {
26: 
27: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::default_connection = nullptr;
28: 
29: void DuckDBPyConnection::Initialize(py::handle &m) {
30: 	py::class_<DuckDBPyConnection, shared_ptr<DuckDBPyConnection>>(m, "DuckDBPyConnection", py::module_local())
31: 	    .def("cursor", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
32: 	    .def("duplicate", &DuckDBPyConnection::Cursor, "Create a duplicate of the current connection")
33: 	    .def("execute", &DuckDBPyConnection::Execute,
34: 	         "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
35: 	         py::arg("parameters") = py::list(), py::arg("multiple_parameter_sets") = false)
36: 	    .def("executemany", &DuckDBPyConnection::ExecuteMany,
37: 	         "Execute the given prepared statement multiple times using the list of parameter sets in parameters",
38: 	         py::arg("query"), py::arg("parameters") = py::list())
39: 	    .def("close", &DuckDBPyConnection::Close, "Close the connection")
40: 	    .def("fetchone", &DuckDBPyConnection::FetchOne, "Fetch a single row from a result following execute")
41: 	    .def("fetchall", &DuckDBPyConnection::FetchAll, "Fetch all rows from a result following execute")
42: 	    .def("fetchnumpy", &DuckDBPyConnection::FetchNumpy, "Fetch a result as list of NumPy arrays following execute")
43: 	    .def("fetchdf", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
44: 	    .def("fetch_df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
45: 	    .def("fetch_df_chunk", &DuckDBPyConnection::FetchDFChunk,
46: 	         "Fetch a chunk of the result as Data.Frame following execute()", py::arg("vectors_per_chunk") = 1)
47: 	    .def("df", &DuckDBPyConnection::FetchDF, "Fetch a result as Data.Frame following execute()")
48: 	    .def("fetch_arrow_table", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()")
49: 	    .def("fetch_arrow_chunk", &DuckDBPyConnection::FetchArrowChunk,
50: 	         "Fetch a chunk of the result as an Arrow Table following execute()", py::arg("vectors_per_chunk") = 1,
51: 	         py::arg("return_table") = false)
52: 	    .def("fetch_record_batch", &DuckDBPyConnection::FetchRecordBatchReader,
53: 	         "Fetch an Arrow RecordBatchReader following execute()", py::arg("approx_batch_size") = 1)
54: 	    .def("arrow", &DuckDBPyConnection::FetchArrow, "Fetch a result as Arrow table following execute()")
55: 	    .def("begin", &DuckDBPyConnection::Begin, "Start a new transaction")
56: 	    .def("commit", &DuckDBPyConnection::Commit, "Commit changes performed within a transaction")
57: 	    .def("rollback", &DuckDBPyConnection::Rollback, "Roll back changes performed within a transaction")
58: 	    .def("append", &DuckDBPyConnection::Append, "Append the passed Data.Frame to the named table",
59: 	         py::arg("table_name"), py::arg("df"))
60: 	    .def("register", &DuckDBPyConnection::RegisterDF,
61: 	         "Register the passed Data.Frame value for querying with a view", py::arg("view_name"), py::arg("df"))
62: 	    .def("unregister", &DuckDBPyConnection::UnregisterPythonObject, "Unregister the view name",
63: 	         py::arg("view_name"))
64: 	    .def("register_arrow", &DuckDBPyConnection::RegisterArrow,
65: 	         "Register the passed Arrow Table for querying with a view", py::arg("view_name"), py::arg("arrow_object"),
66: 	         py::arg("rows_per_thread") = 1000000)
67: 	    .def("table", &DuckDBPyConnection::Table, "Create a relation object for the name'd table",
68: 	         py::arg("table_name"))
69: 	    .def("view", &DuckDBPyConnection::View, "Create a relation object for the name'd view", py::arg("view_name"))
70: 	    .def("values", &DuckDBPyConnection::Values, "Create a relation object from the passed values",
71: 	         py::arg("values"))
72: 	    .def("table_function", &DuckDBPyConnection::TableFunction,
73: 	         "Create a relation object from the name'd table function with given parameters", py::arg("name"),
74: 	         py::arg("parameters") = py::list())
75: 	    .def("from_query", &DuckDBPyConnection::FromQuery, "Create a relation object from the given SQL query",
76: 	         py::arg("query"), py::arg("alias") = "query_relation")
77: 	    .def("query", &DuckDBPyConnection::FromQuery, "Create a relation object from the given SQL query",
78: 	         py::arg("query"), py::arg("alias") = "query_relation")
79: 	    .def("from_df", &DuckDBPyConnection::FromDF, "Create a relation object from the Data.Frame in df",
80: 	         py::arg("df") = py::none())
81: 	    .def("from_arrow_table", &DuckDBPyConnection::FromArrowTable, "Create a relation object from an Arrow table",
82: 	         py::arg("table"), py::arg("rows_per_thread") = 1000000)
83: 	    .def("df", &DuckDBPyConnection::FromDF, "Create a relation object from the Data.Frame in df (alias of from_df)",
84: 	         py::arg("df"))
85: 	    .def("from_csv_auto", &DuckDBPyConnection::FromCsvAuto,
86: 	         "Create a relation object from the CSV file in file_name", py::arg("file_name"))
87: 	    .def("from_parquet", &DuckDBPyConnection::FromParquet,
88: 	         "Create a relation object from the Parquet file in file_name", py::arg("file_name"),
89: 	         py::arg("binary_as_string") = false)
90: 	    .def_property_readonly("description", &DuckDBPyConnection::GetDescription,
91: 	                           "Get result set attributes, mainly column names");
92: 
93: 	PyDateTime_IMPORT;
94: }
95: 
96: DuckDBPyConnection *DuckDBPyConnection::ExecuteMany(const string &query, py::object params) {
97: 	Execute(query, std::move(params), true);
98: 	return this;
99: }
100: 
101: DuckDBPyConnection *DuckDBPyConnection::Execute(const string &query, py::object params, bool many) {
102: 	if (!connection) {
103: 		throw std::runtime_error("connection closed");
104: 	}
105: 	result = nullptr;
106: 
107: 	auto statements = connection->ExtractStatements(query);
108: 	if (statements.empty()) {
109: 		// no statements to execute
110: 		return this;
111: 	}
112: 	// if there are multiple statements, we directly execute the statements besides the last one
113: 	// we only return the result of the last statement to the user, unless one of the previous statements fails
114: 	for (idx_t i = 0; i + 1 < statements.size(); i++) {
115: 		auto res = connection->Query(move(statements[i]));
116: 		if (!res->success) {
117: 			throw std::runtime_error(res->error);
118: 		}
119: 	}
120: 
121: 	auto prep = connection->Prepare(move(statements.back()));
122: 	if (!prep->success) {
123: 		throw std::runtime_error(prep->error);
124: 	}
125: 
126: 	// this is a list of a list of parameters in executemany
127: 	py::list params_set;
128: 	if (!many) {
129: 		params_set = py::list(1);
130: 		params_set[0] = params;
131: 	} else {
132: 		params_set = params;
133: 	}
134: 
135: 	for (pybind11::handle single_query_params : params_set) {
136: 		if (prep->n_param != py::len(single_query_params)) {
137: 			throw std::runtime_error("Prepared statement needs " + to_string(prep->n_param) + " parameters, " +
138: 			                         to_string(py::len(single_query_params)) + " given");
139: 		}
140: 		auto args = DuckDBPyConnection::TransformPythonParamList(single_query_params);
141: 		auto res = make_unique<DuckDBPyResult>();
142: 		{
143: 			py::gil_scoped_release release;
144: 			res->result = prep->Execute(args);
145: 		}
146: 		if (!res->result->success) {
147: 			throw std::runtime_error(res->result->error);
148: 		}
149: 		if (!many) {
150: 			result = move(res);
151: 		}
152: 	}
153: 	return this;
154: }
155: 
156: DuckDBPyConnection *DuckDBPyConnection::Append(const string &name, py::object value) {
157: 	RegisterDF("__append_df", std::move(value));
158: 	return Execute("INSERT INTO \"" + name + "\" SELECT * FROM __append_df");
159: }
160: 
161: DuckDBPyConnection *DuckDBPyConnection::RegisterDF(const string &name, py::object value) {
162: 	if (!connection) {
163: 		throw std::runtime_error("connection closed");
164: 	}
165: 	connection->TableFunction("pandas_scan", {Value::POINTER((uintptr_t)value.ptr())})->CreateView(name, true, true);
166: 	// keep a reference
167: 	auto object = make_unique<RegisteredObject>(value);
168: 	registered_objects[name] = move(object);
169: 	return this;
170: }
171: 
172: DuckDBPyConnection *DuckDBPyConnection::RegisterArrow(const string &name, py::object &table,
173:                                                       const idx_t rows_per_tuple) {
174: 	if (!connection) {
175: 		throw std::runtime_error("connection closed");
176: 	}
177: 	auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(table.ptr());
178: 
179: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
180: 	connection
181: 	    ->TableFunction("arrow_scan",
182: 	                    {Value::POINTER((uintptr_t)stream_factory.get()),
183: 	                     Value::POINTER((uintptr_t)stream_factory_produce), Value::UBIGINT(rows_per_tuple)})
184: 	    ->CreateView(name, true, true);
185: 	auto object = make_unique<RegisteredArrow>(move(stream_factory), move(table));
186: 	registered_objects[name] = move(object);
187: 	return this;
188: }
189: 
190: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromQuery(const string &query, const string &alias) {
191: 	if (!connection) {
192: 		throw std::runtime_error("connection closed");
193: 	}
194: 	return make_unique<DuckDBPyRelation>(connection->RelationFromQuery(query, alias));
195: }
196: 
197: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Table(const string &tname) {
198: 	if (!connection) {
199: 		throw std::runtime_error("connection closed");
200: 	}
201: 	return make_unique<DuckDBPyRelation>(connection->Table(tname));
202: }
203: 
204: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::Values(py::object params) {
205: 	if (!connection) {
206: 		throw std::runtime_error("connection closed");
207: 	}
208: 	vector<vector<Value>> values {DuckDBPyConnection::TransformPythonParamList(std::move(params))};
209: 	return make_unique<DuckDBPyRelation>(connection->Values(values));
210: }
211: 
212: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::View(const string &vname) {
213: 	if (!connection) {
214: 		throw std::runtime_error("connection closed");
215: 	}
216: 	return make_unique<DuckDBPyRelation>(connection->View(vname));
217: }
218: 
219: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::TableFunction(const string &fname, py::object params) {
220: 	if (!connection) {
221: 		throw std::runtime_error("connection closed");
222: 	}
223: 
224: 	return make_unique<DuckDBPyRelation>(
225: 	    connection->TableFunction(fname, DuckDBPyConnection::TransformPythonParamList(std::move(params))));
226: }
227: 
228: static std::string GenerateRandomName() {
229: 	std::random_device rd;
230: 	std::mt19937 gen(rd());
231: 	std::uniform_int_distribution<> dis(0, 15);
232: 
233: 	std::stringstream ss;
234: 	int i;
235: 	ss << std::hex;
236: 	for (i = 0; i < 16; i++) {
237: 		ss << dis(gen);
238: 	}
239: 	return ss.str();
240: }
241: 
242: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(py::object value) {
243: 	if (!connection) {
244: 		throw std::runtime_error("connection closed");
245: 	}
246: 	string name = "df_" + GenerateRandomName();
247: 	registered_objects[name] = make_unique<RegisteredObject>(value);
248: 	vector<Value> params;
249: 	params.emplace_back(Value::POINTER((uintptr_t)value.ptr()));
250: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("pandas_scan", params)->Alias(name));
251: }
252: 
253: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromCsvAuto(const string &filename) {
254: 	if (!connection) {
255: 		throw std::runtime_error("connection closed");
256: 	}
257: 	vector<Value> params;
258: 	params.emplace_back(filename);
259: 	return make_unique<DuckDBPyRelation>(connection->TableFunction("read_csv_auto", params)->Alias(filename));
260: }
261: 
262: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &filename, bool binary_as_string) {
263: 	if (!connection) {
264: 		throw std::runtime_error("connection closed");
265: 	}
266: 	vector<Value> params;
267: 	params.emplace_back(filename);
268: 	unordered_map<string, Value> named_parameters({{"binary_as_string", Value::BOOLEAN(binary_as_string)}});
269: 	return make_unique<DuckDBPyRelation>(
270: 	    connection->TableFunction("parquet_scan", params, named_parameters)->Alias(filename));
271: }
272: 
273: unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrowTable(py::object &table, const idx_t rows_per_tuple) {
274: 	if (!connection) {
275: 		throw std::runtime_error("connection closed");
276: 	}
277: 	py::gil_scoped_acquire acquire;
278: 	string name = "arrow_table_" + GenerateRandomName();
279: 
280: 	auto stream_factory = make_unique<PythonTableArrowArrayStreamFactory>(table.ptr());
281: 
282: 	auto stream_factory_produce = PythonTableArrowArrayStreamFactory::Produce;
283: 	auto rel = make_unique<DuckDBPyRelation>(
284: 	    connection
285: 	        ->TableFunction("arrow_scan",
286: 	                        {Value::POINTER((uintptr_t)stream_factory.get()),
287: 	                         Value::POINTER((uintptr_t)stream_factory_produce), Value::UBIGINT(rows_per_tuple)})
288: 	        ->Alias(name));
289: 	registered_objects[name] = make_unique<RegisteredArrow>(move(stream_factory), table);
290: 	return rel;
291: }
292: 
293: DuckDBPyConnection *DuckDBPyConnection::UnregisterPythonObject(const string &name) {
294: 	registered_objects.erase(name);
295: 
296: 	if (connection) {
297: 		connection->Query("DROP VIEW \"" + name + "\"");
298: 	}
299: 	return this;
300: }
301: 
302: DuckDBPyConnection *DuckDBPyConnection::Begin() {
303: 	Execute("BEGIN TRANSACTION");
304: 	return this;
305: }
306: 
307: DuckDBPyConnection *DuckDBPyConnection::Commit() {
308: 	if (connection->context->transaction.IsAutoCommit()) {
309: 		return this;
310: 	}
311: 	Execute("COMMIT");
312: 	return this;
313: }
314: 
315: DuckDBPyConnection *DuckDBPyConnection::Rollback() {
316: 	Execute("ROLLBACK");
317: 	return this;
318: }
319: 
320: py::object DuckDBPyConnection::GetDescription() {
321: 	if (!result) {
322: 		return py::none();
323: 	}
324: 	return result->Description();
325: }
326: 
327: void DuckDBPyConnection::Close() {
328: 	result = nullptr;
329: 	connection = nullptr;
330: 	database = nullptr;
331: 	for (auto &cur : cursors) {
332: 		cur->Close();
333: 	}
334: 	cursors.clear();
335: }
336: 
337: // cursor() is stupid
338: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Cursor() {
339: 	auto res = make_shared<DuckDBPyConnection>();
340: 	res->database = database;
341: 	res->connection = make_unique<Connection>(*res->database);
342: 	cursors.push_back(res);
343: 	return res;
344: }
345: 
346: // these should be functions on the result but well
347: py::object DuckDBPyConnection::FetchOne() {
348: 	if (!result) {
349: 		throw std::runtime_error("no open result set");
350: 	}
351: 	return result->Fetchone();
352: }
353: 
354: py::list DuckDBPyConnection::FetchAll() {
355: 	if (!result) {
356: 		throw std::runtime_error("no open result set");
357: 	}
358: 	return result->Fetchall();
359: }
360: 
361: py::dict DuckDBPyConnection::FetchNumpy() {
362: 	if (!result) {
363: 		throw std::runtime_error("no open result set");
364: 	}
365: 	return result->FetchNumpyInternal();
366: }
367: py::object DuckDBPyConnection::FetchDF() {
368: 	if (!result) {
369: 		throw std::runtime_error("no open result set");
370: 	}
371: 	return result->FetchDF();
372: }
373: 
374: py::object DuckDBPyConnection::FetchDFChunk(const idx_t vectors_per_chunk) const {
375: 	if (!result) {
376: 		throw std::runtime_error("no open result set");
377: 	}
378: 	return result->FetchDFChunk(vectors_per_chunk);
379: }
380: 
381: py::object DuckDBPyConnection::FetchArrow() {
382: 	if (!result) {
383: 		throw std::runtime_error("no open result set");
384: 	}
385: 	return result->FetchArrowTable();
386: }
387: 
388: py::object DuckDBPyConnection::FetchArrowChunk(const idx_t vectors_per_chunk, bool return_table) const {
389: 	if (!result) {
390: 		throw std::runtime_error("no open result set");
391: 	}
392: 	return result->FetchArrowTableChunk(vectors_per_chunk, return_table);
393: }
394: 
395: py::object DuckDBPyConnection::FetchRecordBatchReader(const idx_t approx_batch_size) const {
396: 	if (!result) {
397: 		throw std::runtime_error("no open result set");
398: 	}
399: 	return result->FetchRecordBatchReader(approx_batch_size);
400: }
401: 
402: static unique_ptr<TableFunctionRef> TryPandasReplacement(py::dict &dict, py::str &table_name) {
403: 	if (!dict.contains(table_name)) {
404: 		// not present in the globals
405: 		return nullptr;
406: 	}
407: 	auto entry = dict[table_name];
408: 
409: 	// check if there is a local or global variable
410: 	auto table_function = make_unique<TableFunctionRef>();
411: 	vector<unique_ptr<ParsedExpression>> children;
412: 	children.push_back(make_unique<ConstantExpression>(Value::POINTER((uintptr_t)entry.ptr())));
413: 	table_function->function = make_unique<FunctionExpression>("pandas_scan", move(children));
414: 	return table_function;
415: }
416: 
417: static unique_ptr<TableFunctionRef> PandasScanReplacement(const string &table_name, void *data) {
418: 	py::gil_scoped_acquire acquire;
419: 	// look in the locals first
420: 	PyObject *p = PyEval_GetLocals();
421: 	auto py_table_name = py::str(table_name);
422: 	if (p) {
423: 		auto local_dict = py::reinterpret_borrow<py::dict>(p);
424: 		auto result = TryPandasReplacement(local_dict, py_table_name);
425: 		if (result) {
426: 			return result;
427: 		}
428: 	}
429: 	// otherwise look in the globals
430: 	auto global_dict = py::globals();
431: 	return TryPandasReplacement(global_dict, py_table_name);
432: }
433: 
434: shared_ptr<DuckDBPyConnection> DuckDBPyConnection::Connect(const string &database, bool read_only,
435:                                                            const py::dict &config_dict) {
436: 	auto res = make_shared<DuckDBPyConnection>();
437: 	DBConfig config;
438: 	if (read_only) {
439: 		config.access_mode = AccessMode::READ_ONLY;
440: 	}
441: 	for (auto &kv : config_dict) {
442: 		string key = py::str(kv.first);
443: 		string val = py::str(kv.second);
444: 		auto config_property = DBConfig::GetOptionByName(key);
445: 		if (!config_property) {
446: 			throw InvalidInputException("Unrecognized configuration property \"%s\"", key);
447: 		}
448: 		config.SetOption(*config_property, Value(val));
449: 	}
450: 	if (config.enable_external_access) {
451: 		config.replacement_scans.emplace_back(PandasScanReplacement);
452: 	}
453: 
454: 	res->database = make_unique<DuckDB>(database, &config);
455: 	ExtensionHelper::LoadAllExtensions(*res->database);
456: 	res->connection = make_unique<Connection>(*res->database);
457: 
458: 	PandasScanFunction scan_fun;
459: 	CreateTableFunctionInfo scan_info(scan_fun);
460: 
461: 	MapFunction map_fun;
462: 	CreateTableFunctionInfo map_info(map_fun);
463: 
464: 	auto &context = *res->connection->context;
465: 	auto &catalog = Catalog::GetCatalog(context);
466: 	context.transaction.BeginTransaction();
467: 	catalog.CreateTableFunction(context, &scan_info);
468: 	catalog.CreateTableFunction(context, &map_info);
469: 
470: 	context.transaction.Commit();
471: 
472: 	return res;
473: }
474: 
475: vector<Value> DuckDBPyConnection::TransformPythonParamList(py::handle params) {
476: 	vector<Value> args;
477: 
478: 	auto datetime_mod = py::module::import("datetime");
479: 	auto datetime_date = datetime_mod.attr("date");
480: 	auto datetime_datetime = datetime_mod.attr("datetime");
481: 	auto datetime_time = datetime_mod.attr("time");
482: 	auto decimal_mod = py::module::import("decimal");
483: 	auto decimal_decimal = decimal_mod.attr("Decimal");
484: 
485: 	for (pybind11::handle ele : params) {
486: 		if (ele.is_none()) {
487: 			args.emplace_back();
488: 		} else if (py::isinstance<py::bool_>(ele)) {
489: 			args.push_back(Value::BOOLEAN(ele.cast<bool>()));
490: 		} else if (py::isinstance<py::int_>(ele)) {
491: 			args.push_back(Value::BIGINT(ele.cast<int64_t>()));
492: 		} else if (py::isinstance<py::float_>(ele)) {
493: 			args.push_back(Value::DOUBLE(ele.cast<double>()));
494: 		} else if (py::isinstance(ele, decimal_decimal)) {
495: 			args.emplace_back(py::str(ele).cast<string>());
496: 		} else if (py::isinstance(ele, datetime_datetime)) {
497: 			auto year = PyDateTime_GET_YEAR(ele.ptr());
498: 			auto month = PyDateTime_GET_MONTH(ele.ptr());
499: 			auto day = PyDateTime_GET_DAY(ele.ptr());
500: 			auto hour = PyDateTime_DATE_GET_HOUR(ele.ptr());
501: 			auto minute = PyDateTime_DATE_GET_MINUTE(ele.ptr());
502: 			auto second = PyDateTime_DATE_GET_SECOND(ele.ptr());
503: 			auto micros = PyDateTime_DATE_GET_MICROSECOND(ele.ptr());
504: 			args.push_back(Value::TIMESTAMP(year, month, day, hour, minute, second, micros));
505: 		} else if (py::isinstance(ele, datetime_time)) {
506: 			auto hour = PyDateTime_TIME_GET_HOUR(ele.ptr());
507: 			auto minute = PyDateTime_TIME_GET_MINUTE(ele.ptr());
508: 			auto second = PyDateTime_TIME_GET_SECOND(ele.ptr());
509: 			auto micros = PyDateTime_TIME_GET_MICROSECOND(ele.ptr());
510: 			args.push_back(Value::TIME(hour, minute, second, micros));
511: 		} else if (py::isinstance(ele, datetime_date)) {
512: 			auto year = PyDateTime_GET_YEAR(ele.ptr());
513: 			auto month = PyDateTime_GET_MONTH(ele.ptr());
514: 			auto day = PyDateTime_GET_DAY(ele.ptr());
515: 			args.push_back(Value::DATE(year, month, day));
516: 		} else if (py::isinstance<py::str>(ele)) {
517: 			args.emplace_back(ele.cast<string>());
518: 		} else if (py::isinstance<py::memoryview>(ele)) {
519: 			py::memoryview py_view = ele.cast<py::memoryview>();
520: 			PyObject *py_view_ptr = py_view.ptr();
521: 			Py_buffer *py_buf = PyMemoryView_GET_BUFFER(py_view_ptr);
522: 			args.emplace_back(Value::BLOB(const_data_ptr_t(py_buf->buf), idx_t(py_buf->len)));
523: 		} else if (py::isinstance<py::bytes>(ele)) {
524: 			const string &ele_string = ele.cast<string>();
525: 			args.emplace_back(Value::BLOB(const_data_ptr_t(ele_string.data()), ele_string.size()));
526: 		} else {
527: 			throw std::runtime_error("unknown param type " + py::str(ele.get_type()).cast<string>());
528: 		}
529: 	}
530: 	return args;
531: }
532: 
533: DuckDBPyConnection *DuckDBPyConnection::DefaultConnection() {
534: 	if (!default_connection) {
535: 		py::dict config_dict;
536: 		default_connection = DuckDBPyConnection::Connect(":memory:", false, config_dict);
537: 	}
538: 	return default_connection.get();
539: }
540: 
541: void DuckDBPyConnection::Cleanup() {
542: 	default_connection.reset();
543: }
544: 
545: } // namespace duckdb
[end of tools/pythonpkg/src/pyconnection.cpp]
[start of tools/rest/CMakeLists.txt]
1: include_directories(.)
2: 
3: include_directories(../../third_party/httplib)
4: 
5: add_executable(duckdb_rest_server server.cpp)
6: 
7: if(${BUILD_SUN})
8:   set(LINK_EXTRA -lsocket)
9: endif()
10: 
11: add_extension_definitions()
12: 
13: target_link_libraries(duckdb_rest_server duckdb_static ${LINK_EXTRA})
14: link_threads(duckdb_rest_server)
15: 
16: link_extension_libraries(duckdb_rest_server)
[end of tools/rest/CMakeLists.txt]
[start of tools/rest/server.cpp]
1: #include <chrono>
2: #include <cstdio>
3: #include <thread>
4: #include <iostream>
5: 
6: #include "duckdb.hpp"
7: #include "duckdb/common/types/data_chunk.hpp"
8: #include "duckdb/common/vector_operations/vector_operations.hpp"
9: #include "duckdb/common/string_util.hpp"
10: #include "duckdb/main/client_context.hpp"
11: 
12: #include "extension_helper.hpp"
13: 
14: // you can set this to enable compression. You will need to link zlib as well.
15: // #define CPPHTTPLIB_ZLIB_SUPPORT 1
16: #define CPPHTTPLIB_KEEPALIVE_TIMEOUT_USECOND 10000
17: #define CPPHTTPLIB_KEEPALIVE_TIMEOUT_SECOND  0
18: #define CPPHTTPLIB_THREAD_POOL_COUNT         16
19: 
20: #include "httplib.hpp"
21: #include "json.hpp"
22: 
23: #include <unordered_map>
24: 
25: using namespace httplib;
26: using namespace duckdb;
27: using namespace nlohmann;
28: 
29: void print_help() {
30: 	fprintf(stderr, "🦆 Usage: duckdb_rest_server\n");
31: 	fprintf(stderr, "          --listen=[address]    listening address\n");
32: 	fprintf(stderr, "          --port=[no]           listening port\n");
33: 	fprintf(stderr, "          --database=[file]     use given database file\n");
34: 	fprintf(stderr, "          --read_only           open database in read-only mode\n");
35: 	fprintf(stderr, "          --disable_copy        disallow file import/export, e.g. in COPY\n");
36: 	fprintf(stderr, "          --query_timeout=[sec] query timeout in seconds\n");
37: 	fprintf(stderr, "          --fetch_timeout=[sec] result set timeout in seconds\n");
38: 	fprintf(stderr, "          --static=[folder]     static resource folder to serve\n");
39: 	fprintf(stderr, "          --log=[file]          log queries to file\n\n");
40: 	fprintf(stderr, "Version: %s\n", DuckDB::SourceID());
41: }
42: 
43: // https://stackoverflow.com/a/12468109/2652376
44: std::string random_string(size_t length) {
45: 	auto randchar = []() -> char {
46: 		const char charset[] = "0123456789"
47: 		                       "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
48: 		                       "abcdefghijklmnopqrstuvwxyz";
49: 		const size_t max_index = (sizeof(charset) - 1);
50: 		return charset[rand() % max_index];
51: 	};
52: 	std::string str(length, 0);
53: 	std::generate_n(str.begin(), length, randchar);
54: 	return str;
55: }
56: 
57: struct RestClientState {
58: 	unique_ptr<duckdb::QueryResult> res;
59: 	unique_ptr<duckdb::Connection> con;
60: 	time_t touched;
61: };
62: 
63: enum ReturnContentType { JSON, BSON, CBOR, MESSAGE_PACK, UBJSON };
64: 
65: template <class T, class TARGET>
66: static void assign_json_loop(Vector &v, idx_t col_idx, idx_t count, json &j) {
67: 	v.Normalify(count);
68: 	auto data_ptr = FlatVector::GetData<T>(v);
69: 	auto &mask = FlatVector::Validity(v);
70: 	for (idx_t i = 0; i < count; i++) {
71: 		if (mask.RowIsValid(i)) {
72: 			j["data"][col_idx] += (TARGET)data_ptr[i];
73: 		} else {
74: 			j["data"][col_idx] += nullptr;
75: 		}
76: 	}
77: }
78: 
79: static void assign_json_string_loop(Vector &v, idx_t col_idx, idx_t count, json &j) {
80: 	Vector cast_vector(LogicalType::VARCHAR);
81: 	Vector *result_vector;
82: 	if (v.GetType().id() != LogicalTypeId::VARCHAR) {
83: 		VectorOperations::Cast(v, cast_vector, count);
84: 		result_vector = &cast_vector;
85: 	} else {
86: 		result_vector = &v;
87: 	}
88: 	result_vector->Normalify(count);
89: 	auto data_ptr = FlatVector::GetData<string_t>(*result_vector);
90: 	auto &mask = FlatVector::Validity(*result_vector);
91: 	for (idx_t i = 0; i < count; i++) {
92: 		if (mask.RowIsValid(i)) {
93: 			j["data"][col_idx] += data_ptr[i].GetString();
94: 
95: 		} else {
96: 			j["data"][col_idx] += nullptr;
97: 		}
98: 	}
99: }
100: 
101: void serialize_chunk(QueryResult *res, DataChunk *chunk, json &j) {
102: 	D_ASSERT(res);
103: 	for (size_t col_idx = 0; col_idx < chunk->ColumnCount(); col_idx++) {
104: 		Vector &v = chunk->data[col_idx];
105: 		switch (v.GetType().id()) {
106: 		case LogicalTypeId::BOOLEAN:
107: 			assign_json_loop<bool, int64_t>(v, col_idx, chunk->size(), j);
108: 			break;
109: 		case LogicalTypeId::TINYINT:
110: 			assign_json_loop<int8_t, int64_t>(v, col_idx, chunk->size(), j);
111: 			break;
112: 		case LogicalTypeId::SMALLINT:
113: 			assign_json_loop<int16_t, int64_t>(v, col_idx, chunk->size(), j);
114: 			break;
115: 		case LogicalTypeId::INTEGER:
116: 			assign_json_loop<int32_t, int64_t>(v, col_idx, chunk->size(), j);
117: 			break;
118: 		case LogicalTypeId::BIGINT:
119: 			assign_json_loop<int64_t, int64_t>(v, col_idx, chunk->size(), j);
120: 			break;
121: 		case LogicalTypeId::FLOAT:
122: 			assign_json_loop<float, double>(v, col_idx, chunk->size(), j);
123: 			break;
124: 		case LogicalTypeId::DOUBLE:
125: 			assign_json_loop<double, double>(v, col_idx, chunk->size(), j);
126: 			break;
127: 		case LogicalTypeId::DATE:
128: 		case LogicalTypeId::TIME:
129: 		case LogicalTypeId::TIMESTAMP:
130: 		case LogicalTypeId::DECIMAL:
131: 		case LogicalTypeId::INTERVAL:
132: 		case LogicalTypeId::HUGEINT:
133: 		case LogicalTypeId::BLOB:
134: 		case LogicalTypeId::VARCHAR:
135: 		default:
136: 			assign_json_string_loop(v, col_idx, chunk->size(), j);
137: 			break;
138: 		}
139: 	}
140: }
141: 
142: void serialize_json(const Request &req, Response &resp, json &j) {
143: 	auto return_type = ReturnContentType::JSON;
144: 	j["duckdb_version"] = DuckDB::SourceID();
145: 
146: 	if (req.has_header("Accept")) {
147: 		auto accept = req.get_header_value("Accept");
148: 		if (accept.rfind("application/bson", 0) == 0 || accept.rfind("application/x-bson", 0) == 0) {
149: 			return_type = ReturnContentType::BSON;
150: 		} else if (accept.rfind("application/cbor", 0) == 0) {
151: 			return_type = ReturnContentType::CBOR;
152: 		} else if (accept.rfind("application/msgpack", 0) == 0 || accept.rfind("application/x-msgpack", 0) == 0 ||
153: 		           accept.rfind("application/vnd.msgpack", 0) == 0) {
154: 			return_type = ReturnContentType::MESSAGE_PACK;
155: 		} else if (accept.rfind("application/ubjson", 0) == 0) {
156: 			return_type = ReturnContentType::UBJSON;
157: 		}
158: 	}
159: 
160: 	switch (return_type) {
161: 	case ReturnContentType::JSON: {
162: 		if (req.has_param("callback")) {
163: 			auto jsonp_callback = req.get_param_value("callback");
164: 			resp.set_content(jsonp_callback + "(" + j.dump() + ");", "application/javascript");
165: 
166: 		} else {
167: 			resp.set_content(j.dump(), "application/json");
168: 		}
169: 		break;
170: 	}
171: 	case ReturnContentType::BSON: {
172: 		auto bson = json::to_bson(j);
173: 		resp.set_content((const char *)bson.data(), bson.size(), "application/bson");
174: 		break;
175: 	}
176: 	case ReturnContentType::CBOR: {
177: 		auto cbor = json::to_cbor(j);
178: 		resp.set_content((const char *)cbor.data(), cbor.size(), "application/cbor");
179: 		break;
180: 	}
181: 	case ReturnContentType::MESSAGE_PACK: {
182: 		auto msgpack = json::to_msgpack(j);
183: 		resp.set_content((const char *)msgpack.data(), msgpack.size(), "application/msgpack");
184: 		break;
185: 	}
186: 	case ReturnContentType::UBJSON: {
187: 		auto ubjson = json::to_ubjson(j);
188: 		resp.set_content((const char *)ubjson.data(), ubjson.size(), "application/ubjson");
189: 		break;
190: 	}
191: 	}
192: }
193: 
194: void sleep_thread(duckdb::Connection *conn, bool *is_active, int timeout_duration) {
195: 	// timeout is given in seconds
196: 	// we wait 10ms per iteration, so timeout * 100 gives us the amount of
197: 	// iterations
198: 	D_ASSERT(conn);
199: 	D_ASSERT(is_active);
200: 
201: 	if (timeout_duration < 0) {
202: 		return;
203: 	}
204: 	for (size_t i = 0; i < (size_t)(timeout_duration * 100) && *is_active; i++) {
205: 		std::this_thread::sleep_for(std::chrono::milliseconds(10));
206: 	}
207: 	if (*is_active) {
208: 		conn->Interrupt();
209: 	}
210: }
211: 
212: void client_state_cleanup(unordered_map<string, RestClientState> *map, std::mutex *mutex, int timeout_duration) {
213: 	// timeout is given in seconds
214: 	while (true) {
215: 		// sleep for half the timeout duration
216: 		std::this_thread::sleep_for(std::chrono::milliseconds((timeout_duration * 1000) / 2));
217: 		{
218: 			std::lock_guard<std::mutex> guard(*mutex);
219: 			auto now = std::time(nullptr);
220: 			for (auto it = map->cbegin(); it != map->cend();) {
221: 				if (now - it->second.touched > timeout_duration) {
222: 					it = map->erase(it);
223: 				} else {
224: 					++it;
225: 				}
226: 			}
227: 		}
228: 	}
229: }
230: 
231: int main(int argc, char **argv) {
232: 	Server svr;
233: 	if (!svr.is_valid()) {
234: 		printf("server has an error...\n");
235: 		return -1;
236: 	}
237: 
238: 	std::mutex out_mutex;
239: 	srand(time(nullptr));
240: 
241: 	DBConfig config;
242: 	string dbfile = "";
243: 	string logfile_name;
244: 
245: 	string listen = "localhost";
246: 	string static_files;
247: 	int port = 1294;
248: 	std::ofstream logfile;
249: 
250: 	int query_timeout = 60;
251: 	int fetch_timeout = 60 * 5;
252: 
253: 	// parse config
254: 	for (int arg_index = 1; arg_index < argc; ++arg_index) {
255: 		string arg = argv[arg_index];
256: 		if (arg == "--help") {
257: 			print_help();
258: 			exit(0);
259: 		} else if (arg == "--read_only") {
260: 			config.access_mode = AccessMode::READ_ONLY;
261: 		} else if (arg == "--disable_copy") {
262: 			config.enable_external_access = false;
263: 		} else if (StringUtil::StartsWith(arg, "--database=")) {
264: 			auto splits = StringUtil::Split(arg, '=');
265: 			if (splits.size() != 2) {
266: 				print_help();
267: 				exit(1);
268: 			}
269: 			dbfile = string(splits[1]);
270: 		} else if (StringUtil::StartsWith(arg, "--log=")) {
271: 			auto splits = StringUtil::Split(arg, '=');
272: 			if (splits.size() != 2) {
273: 				print_help();
274: 				exit(1);
275: 			}
276: 			logfile_name = string(splits[1]);
277: 		} else if (StringUtil::StartsWith(arg, "--static=")) {
278: 			auto splits = StringUtil::Split(arg, '=');
279: 			if (splits.size() != 2) {
280: 				print_help();
281: 				exit(1);
282: 			}
283: 			static_files = string(splits[1]);
284: 		} else if (StringUtil::StartsWith(arg, "--listen=")) {
285: 			auto splits = StringUtil::Split(arg, '=');
286: 			if (splits.size() != 2) {
287: 				print_help();
288: 				exit(1);
289: 			}
290: 			listen = string(splits[1]);
291: 		} else if (StringUtil::StartsWith(arg, "--port=")) {
292: 			auto splits = StringUtil::Split(arg, '=');
293: 			if (splits.size() != 2) {
294: 				print_help();
295: 				exit(1);
296: 			}
297: 			port = std::stoi(splits[1]);
298: 
299: 		} else if (StringUtil::StartsWith(arg, "--query_timeout=")) {
300: 			auto splits = StringUtil::Split(arg, '=');
301: 			if (splits.size() != 2) {
302: 				print_help();
303: 				exit(1);
304: 			}
305: 			query_timeout = std::stoi(splits[1]);
306: 
307: 		} else if (StringUtil::StartsWith(arg, "--fetch_timeout=")) {
308: 			auto splits = StringUtil::Split(arg, '=');
309: 			if (splits.size() != 2) {
310: 				print_help();
311: 				exit(1);
312: 			}
313: 			fetch_timeout = std::stoi(splits[1]);
314: 
315: 		} else {
316: 			fprintf(stderr, "Error: unknown argument %s\n", arg.c_str());
317: 			print_help();
318: 			exit(1);
319: 		}
320: 	}
321: 
322: 	unordered_map<string, RestClientState> client_state_map;
323: 	std::mutex client_state_map_mutex;
324: 	std::thread client_state_cleanup_thread(client_state_cleanup, &client_state_map, &client_state_map_mutex,
325: 	                                        fetch_timeout);
326: 
327: 	if (!logfile_name.empty()) {
328: 		logfile.open(logfile_name, std::ios_base::app);
329: 	}
330: 
331: 	config.maximum_memory = 10737418240;
332: 
333: 	DuckDB duckdb(dbfile.empty() ? nullptr : dbfile.c_str(), &config);
334: 	ExtensionHelper::LoadAllExtensions(duckdb);
335: 
336: 	svr.Get("/query", [&](const Request &req, Response &resp) {
337: 		auto q = req.get_param_value("q");
338: 		{
339: 			std::lock_guard<std::mutex> guard(out_mutex);
340: 			logfile << q << " ; -- DFgoEnx9UIRgHFsVYW8K" << std::endl
341: 			        << std::flush; // using a terminator that will **never** occur in queries
342: 		}
343: 
344: 		json j;
345: 
346: 		RestClientState state;
347: 		state.con = make_unique<duckdb::Connection>(duckdb);
348: 		state.con->EnableProfiling();
349: 		state.touched = std::time(nullptr);
350: 		bool is_active = true;
351: 
352: 		std::thread interrupt_thread(sleep_thread, state.con.get(), &is_active, query_timeout);
353: 		auto res = state.con->context->Query(q, true);
354: 
355: 		is_active = false;
356: 		interrupt_thread.join();
357: 
358: 		state.res = move(res);
359: 
360: 		if (state.res->success) {
361: 			j = {{"query", q},
362: 			     {"success", state.res->success},
363: 			     {"column_count", state.res->types.size()},
364: 
365: 			     {"statement_type", StatementTypeToString(state.res->statement_type)},
366: 			     {"names", json(state.res->names)},
367: 			     {"name_index_map", json::object()},
368: 			     {"types", json::array()},
369: 			     {"sql_types", json::array()},
370: 			     {"data", json::array()}};
371: 
372: 			for (auto &sql_type : state.res->types) {
373: 				j["sql_types"] += sql_type.ToString();
374: 			}
375: 			for (auto &type : state.res->types) {
376: 				j["types"] += TypeIdToString(type.InternalType());
377: 			}
378: 
379: 			// make it easier to get col data by name
380: 			size_t col_idx = 0;
381: 			for (auto &name : state.res->names) {
382: 				j["name_index_map"][name] = col_idx;
383: 				col_idx++;
384: 			}
385: 
386: 			// only do this if query was successful
387: 			string query_ref = random_string(10);
388: 			j["ref"] = query_ref;
389: 			auto chunk = state.res->Fetch();
390: 			if (chunk != nullptr) {
391: 				serialize_chunk(state.res.get(), chunk.get(), j);
392: 			}
393: 			{
394: 				std::lock_guard<std::mutex> guard(client_state_map_mutex);
395: 				client_state_map[query_ref] = move(state);
396: 			}
397: 
398: 		} else {
399: 			j = {{"query", q}, {"success", state.res->success}, {"error", state.res->error}};
400: 		}
401: 
402: 		serialize_json(req, resp, j);
403: 	});
404: 
405: 	svr.Get("/fetch", [&](const Request &req, Response &resp) {
406: 		auto ref = req.get_param_value("ref");
407: 		json j;
408: 		RestClientState state;
409: 		bool found_state = false;
410: 		{
411: 			std::lock_guard<std::mutex> guard(client_state_map_mutex);
412: 			auto it = client_state_map.find(ref);
413: 			if (it != client_state_map.end()) {
414: 				state = move(it->second);
415: 				client_state_map.erase(it);
416: 				found_state = true;
417: 			}
418: 		}
419: 
420: 		if (found_state) {
421: 			bool is_active = true;
422: 			std::thread interrupt_thread(sleep_thread, state.con.get(), &is_active, query_timeout);
423: 			auto chunk = state.res->Fetch();
424: 			is_active = false;
425: 			interrupt_thread.join();
426: 
427: 			j = {{"success", true}, {"ref", ref}, {"count", chunk->size()}, {"data", json::array()}};
428: 			serialize_chunk(state.res.get(), chunk.get(), j);
429: 			if (chunk->size() != 0) {
430: 				std::lock_guard<std::mutex> guard(client_state_map_mutex);
431: 				state.touched = std::time(nullptr);
432: 				client_state_map[ref] = move(state);
433: 			}
434: 		} else {
435: 			j = {{"success", false}, {"error", "Unable to find ref."}};
436: 		}
437: 
438: 		serialize_json(req, resp, j);
439: 	});
440: 
441: 	svr.Get("/close", [&](const Request &req, Response &resp) {
442: 		auto ref = req.get_param_value("ref");
443: 		duckdb::Connection conn(duckdb);
444: 		json j;
445: 		std::lock_guard<std::mutex> guard(client_state_map_mutex);
446: 		if (client_state_map.find(ref) != client_state_map.end()) {
447: 			client_state_map.erase(client_state_map.find(ref));
448: 			j = {{"success", true}, {"ref", ref}};
449: 		} else {
450: 			j = {{"success", false}, {"error", "Unable to find ref."}};
451: 		}
452: 
453: 		serialize_json(req, resp, j);
454: 	});
455: 
456: 	svr.Get("/", [&](const Request &req, Response &resp) {
457: 		resp.status = 302;
458: 
459: 		resp.set_header("Location", "/select.html");
460: 		resp.set_content("<a href='/select.html'>select.html</a>", "text/html");
461: 	});
462: 
463: 	if (!static_files.empty()) {
464: 		svr.set_base_dir(static_files.c_str());
465: 	}
466: 
467: 	std::cout << "🦆 serving " + dbfile + " on http://" + listen + ":" + std::to_string(port) + "\n";
468: 
469: 	svr.listen(listen.c_str(), port);
470: 	return 0;
471: }
[end of tools/rest/server.cpp]
[start of tools/rpkg/src/database.cpp]
1: #include "rapi.hpp"
2: #include "duckdb/main/client_context.hpp"
3: #include "extension/extension_helper.hpp"
4: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
5: 
6: using namespace duckdb;
7: 
8: static SEXP duckdb_finalize_database_R(SEXP dbsexp) {
9: 	if (TYPEOF(dbsexp) != EXTPTRSXP) {
10: 		Rf_error("duckdb_finalize_connection_R: Need external pointer parameter");
11: 	}
12: 	auto db_wrapper = (DBWrapper *)R_ExternalPtrAddr(dbsexp);
13: 	if (db_wrapper) {
14: 		Rf_warning("duckdb_finalize_database_R: Database is garbage-collected, use dbDisconnect(con, shutdown=TRUE) or "
15: 		           "duckdb::duckdb_shutdown(drv) to avoid this.");
16: 		R_ClearExternalPtr(dbsexp);
17: 		delete db_wrapper;
18: 	}
19: 	return R_NilValue;
20: }
21: 
22: SEXP RApi::Startup(SEXP dbdirsexp, SEXP readonlysexp, SEXP configsexp) {
23: 	if (TYPEOF(dbdirsexp) != STRSXP || Rf_length(dbdirsexp) != 1) {
24: 		Rf_error("duckdb_startup_R: Need string parameter for dbdir");
25: 	}
26: 	char *dbdir = (char *)CHAR(STRING_ELT(dbdirsexp, 0));
27: 
28: 	if (TYPEOF(readonlysexp) != LGLSXP || Rf_length(readonlysexp) != 1) {
29: 		Rf_error("duckdb_startup_R: Need string parameter for read_only");
30: 	}
31: 	bool read_only = (bool)LOGICAL_ELT(readonlysexp, 0);
32: 
33: 	if (strlen(dbdir) == 0 || strcmp(dbdir, ":memory:") == 0) {
34: 		dbdir = NULL;
35: 	}
36: 
37: 	DBConfig config;
38: 	if (read_only) {
39: 		config.access_mode = AccessMode::READ_ONLY;
40: 	}
41: 
42: 	RProtector r;
43: 	auto confignamessexp = r.Protect(GET_NAMES(configsexp));
44: 
45: 	for (idx_t i = 0; i < (idx_t)Rf_length(configsexp); i++) {
46: 		string key = string(CHAR(STRING_ELT(confignamessexp, i)));
47: 		string val = string(CHAR(STRING_ELT(VECTOR_ELT(configsexp, i), 0)));
48: 		auto config_property = DBConfig::GetOptionByName(key);
49: 		if (!config_property) {
50: 			Rf_error("Unrecognized configuration property '%s'", key.c_str());
51: 		}
52: 		try {
53: 			config.SetOption(*config_property, Value(val));
54: 		} catch (std::exception &e) {
55: 			Rf_error("duckdb_startup_R: Failed to set configuration option: %s", e.what());
56: 		}
57: 	}
58: 
59: 	DBWrapper *wrapper;
60: 
61: 	try {
62: 		wrapper = new DBWrapper();
63: 		config.replacement_scans.emplace_back(ArrowScanReplacement, wrapper);
64: 		wrapper->db = make_unique<DuckDB>(dbdir, &config);
65: 	} catch (std::exception &e) {
66: 		Rf_error("duckdb_startup_R: Failed to open database: %s", e.what());
67: 	}
68: 	D_ASSERT(wrapper->db);
69: 	ExtensionHelper::LoadAllExtensions(*wrapper->db);
70: 
71: 	DataFrameScanFunction scan_fun;
72: 	CreateTableFunctionInfo info(scan_fun);
73: 	Connection conn(*wrapper->db);
74: 	auto &context = *conn.context;
75: 	auto &catalog = Catalog::GetCatalog(context);
76: 	context.transaction.BeginTransaction();
77: 	catalog.CreateTableFunction(context, &info);
78: 	context.transaction.Commit();
79: 
80: 	SEXP dbsexp = r.Protect(R_MakeExternalPtr(wrapper, R_NilValue, R_NilValue));
81: 	R_RegisterCFinalizer(dbsexp, (void (*)(SEXP))duckdb_finalize_database_R);
82: 	return dbsexp;
83: }
84: 
85: SEXP RApi::Shutdown(SEXP dbsexp) {
86: 	if (TYPEOF(dbsexp) != EXTPTRSXP) {
87: 		Rf_error("duckdb_finalize_connection_R: Need external pointer parameter");
88: 	}
89: 	auto db_wrapper = (DBWrapper *)R_ExternalPtrAddr(dbsexp);
90: 	if (db_wrapper) {
91: 		R_ClearExternalPtr(dbsexp);
92: 		delete db_wrapper;
93: 	}
94: 
95: 	return R_NilValue;
96: }
[end of tools/rpkg/src/database.cpp]
[start of tools/sqlite3_api_wrapper/CMakeLists.txt]
1: include_directories(include)
2: add_subdirectory(sqlite3)
3: 
4: include_directories(sqlite3_udf_api/include)
5: add_subdirectory(sqlite3_udf_api)
6: 
7: add_extension_definitions()
8: add_definitions(-DSQLITE_SHELL_IS_UTF8)
9: 
10: include_directories(../../third_party/utf8proc/include)
11: 
12: add_library(sqlite3_api_wrapper_static STATIC sqlite3_api_wrapper.cpp
13:                                               ${ALL_OBJECT_FILES})
14: target_link_libraries(sqlite3_api_wrapper_static duckdb_static utf8proc)
15: link_threads(sqlite3_api_wrapper_static)
16: 
17: link_extension_libraries(sqlite3_api_wrapper_static)
18: 
19: if(NOT WIN32)
20:   add_library(sqlite3_api_wrapper SHARED sqlite3_api_wrapper.cpp
21:                                          ${ALL_OBJECT_FILES})
22:   target_link_libraries(sqlite3_api_wrapper duckdb ${DUCKDB_EXTRA_LINK_FLAGS})
23:   link_threads(sqlite3_api_wrapper)
24: 
25:   link_extension_libraries(sqlite3_api_wrapper)
26: 
27:   include_directories(../../third_party/catch)
28: 
29:   include_directories(test/include)
30:   add_subdirectory(test)
31: 
32:   add_executable(test_sqlite3_api_wrapper ${SQLITE_TEST_FILES})
33:   target_link_libraries(test_sqlite3_api_wrapper sqlite3_api_wrapper)
34: endif()
[end of tools/sqlite3_api_wrapper/CMakeLists.txt]
[start of tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp]
1: #include "sqlite3.h"
2: #include "udf_struct_sqlite3.h"
3: #include "sqlite3_udf_wrapper.hpp"
4: 
5: #include "duckdb.hpp"
6: #include "duckdb/parser/parser.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/common/types.hpp"
9: #include "duckdb/common/operator/cast_operators.hpp"
10: 
11: #include "utf8proc_wrapper.hpp"
12: 
13: #include <ctype.h>
14: #include <stdio.h>
15: #include <stdlib.h>
16: #include <string.h>
17: #include <time.h>
18: #include <string>
19: #include <chrono>
20: #include <cassert>
21: #include <climits>
22: 
23: #include "extension_helper.hpp"
24: 
25: using namespace duckdb;
26: using namespace std;
27: 
28: static char *sqlite3_strdup(const char *str);
29: 
30: struct sqlite3_string_buffer {
31: 	//! String data
32: 	unique_ptr<char[]> data;
33: 	//! String length
34: 	int data_len;
35: };
36: 
37: struct sqlite3_stmt {
38: 	//! The DB object that this statement belongs to
39: 	sqlite3 *db;
40: 	//! The query string
41: 	string query_string;
42: 	//! The prepared statement object, if successfully prepared
43: 	unique_ptr<PreparedStatement> prepared;
44: 	//! The result object, if successfully executed
45: 	unique_ptr<QueryResult> result;
46: 	//! The current chunk that we are iterating over
47: 	unique_ptr<DataChunk> current_chunk;
48: 	//! The current row into the current chunk that we are iterating over
49: 	int64_t current_row;
50: 	//! Bound values, used for binding to the prepared statement
51: 	vector<Value> bound_values;
52: 	//! Names of the prepared parameters
53: 	vector<string> bound_names;
54: 	//! The current column values converted to string, used and filled by sqlite3_column_text
55: 	unique_ptr<sqlite3_string_buffer[]> current_text;
56: };
57: 
58: void sqlite3_randomness(int N, void *pBuf) {
59: 	static bool init = false;
60: 	if (!init) {
61: 		srand(time(NULL));
62: 		init = true;
63: 	}
64: 	unsigned char *zBuf = (unsigned char *)pBuf;
65: 	while (N--) {
66: 		unsigned char nextByte = rand() % 255;
67: 		zBuf[N] = nextByte;
68: 	}
69: }
70: 
71: int sqlite3_open(const char *filename, /* Database filename (UTF-8) */
72:                  sqlite3 **ppDb        /* OUT: SQLite db handle */
73: ) {
74: 	return sqlite3_open_v2(filename, ppDb, 0, NULL);
75: }
76: 
77: int sqlite3_open_v2(const char *filename, /* Database filename (UTF-8) */
78:                     sqlite3 **ppDb,       /* OUT: SQLite db handle */
79:                     int flags,            /* Flags */
80:                     const char *zVfs      /* Name of VFS module to use */
81: ) {
82: 	if (filename && strcmp(filename, ":memory:") == 0) {
83: 		filename = NULL;
84: 	}
85: 	*ppDb = nullptr;
86: 	if (zVfs) { /* unsupported so if set we complain */
87: 		return SQLITE_ERROR;
88: 	}
89: 	sqlite3 *pDb = nullptr;
90: 	try {
91: 		pDb = new sqlite3();
92: 		DBConfig config;
93: 		config.access_mode = AccessMode::AUTOMATIC;
94: 		if (flags & SQLITE_OPEN_READONLY) {
95: 			config.access_mode = AccessMode::READ_ONLY;
96: 		}
97: 		pDb->db = make_unique<DuckDB>(filename, &config);
98: 		pDb->con = make_unique<Connection>(*pDb->db);
99: 
100: 		ExtensionHelper::LoadAllExtensions(*pDb->db);
101: 	} catch (std::exception &ex) {
102: 		if (pDb) {
103: 			pDb->last_error = ex.what();
104: 			pDb->errCode = SQLITE_ERROR;
105: 		}
106: 		return SQLITE_ERROR;
107: 	}
108: 	*ppDb = pDb;
109: 	return SQLITE_OK;
110: }
111: 
112: int sqlite3_close(sqlite3 *db) {
113: 	if (db) {
114: 		delete db;
115: 	}
116: 	return SQLITE_OK;
117: }
118: 
119: int sqlite3_shutdown(void) {
120: 	return SQLITE_OK;
121: }
122: 
123: /* In SQLite this function compiles the query into VDBE bytecode,
124:  * in the implementation it currently executes the query */
125: // TODO: prepare the statement instead of executing right away
126: int sqlite3_prepare_v2(sqlite3 *db,           /* Database handle */
127:                        const char *zSql,      /* SQL statement, UTF-8 encoded */
128:                        int nByte,             /* Maximum length of zSql in bytes. */
129:                        sqlite3_stmt **ppStmt, /* OUT: Statement handle */
130:                        const char **pzTail    /* OUT: Pointer to unused portion of zSql */
131: ) {
132: 	if (!db || !ppStmt || !zSql) {
133: 		return SQLITE_MISUSE;
134: 	}
135: 	*ppStmt = nullptr;
136: 	string query = nByte < 0 ? zSql : string(zSql, nByte);
137: 	if (pzTail) {
138: 		*pzTail = zSql + query.size();
139: 	}
140: 	try {
141: 		Parser parser;
142: 		parser.ParseQuery(query);
143: 		if (parser.statements.size() == 0) {
144: 			return SQLITE_OK;
145: 		}
146: 		// extract the remainder
147: 		idx_t next_location = parser.statements[0]->stmt_location + parser.statements[0]->stmt_length;
148: 		bool set_remainder = next_location < query.size();
149: 
150: 		// extract the first statement
151: 		vector<unique_ptr<SQLStatement>> statements;
152: 		statements.push_back(move(parser.statements[0]));
153: 
154: 		db->con->context->HandlePragmaStatements(statements);
155: 
156: 		// if there are multiple statements here, we are dealing with an import database statement
157: 		// we directly execute all statements besides the final one
158: 		for (idx_t i = 0; i + 1 < statements.size(); i++) {
159: 			auto res = db->con->Query(move(statements[i]));
160: 			if (!res->success) {
161: 				db->last_error = res->error;
162: 				return SQLITE_ERROR;
163: 			}
164: 		}
165: 
166: 		// now prepare the query
167: 		auto prepared = db->con->Prepare(move(statements.back()));
168: 		if (!prepared->success) {
169: 			// failed to prepare: set the error message
170: 			db->last_error = prepared->error;
171: 			return SQLITE_ERROR;
172: 		}
173: 
174: 		// create the statement entry
175: 		unique_ptr<sqlite3_stmt> stmt = make_unique<sqlite3_stmt>();
176: 		stmt->db = db;
177: 		stmt->query_string = query;
178: 		stmt->prepared = move(prepared);
179: 		stmt->current_row = -1;
180: 		for (idx_t i = 0; i < stmt->prepared->n_param; i++) {
181: 			stmt->bound_names.push_back("$" + to_string(i + 1));
182: 			stmt->bound_values.push_back(Value());
183: 		}
184: 
185: 		// extract the remainder of the query and assign it to the pzTail
186: 		if (pzTail && set_remainder) {
187: 			*pzTail = zSql + next_location + 1;
188: 		}
189: 
190: 		*ppStmt = stmt.release();
191: 		return SQLITE_OK;
192: 	} catch (std::exception &ex) {
193: 		db->last_error = ex.what();
194: 		return SQLITE_ERROR;
195: 	}
196: }
197: 
198: bool sqlite3_display_result(StatementType type) {
199: 	switch (type) {
200: 	case StatementType::EXECUTE_STATEMENT:
201: 	case StatementType::EXPLAIN_STATEMENT:
202: 	case StatementType::PRAGMA_STATEMENT:
203: 	case StatementType::SELECT_STATEMENT:
204: 	case StatementType::SHOW_STATEMENT:
205: 		return true;
206: 	default:
207: 		return false;
208: 	}
209: }
210: 
211: /* Prepare the next result to be retrieved */
212: int sqlite3_step(sqlite3_stmt *pStmt) {
213: 	if (!pStmt) {
214: 		return SQLITE_MISUSE;
215: 	}
216: 	if (!pStmt->prepared) {
217: 		pStmt->db->last_error = "Attempting sqlite3_step() on a non-successfully prepared statement";
218: 		return SQLITE_ERROR;
219: 	}
220: 	pStmt->current_text = nullptr;
221: 	if (!pStmt->result) {
222: 		// no result yet! call Execute()
223: 		pStmt->result = pStmt->prepared->Execute(pStmt->bound_values, true);
224: 		if (!pStmt->result->success) {
225: 			// error in execute: clear prepared statement
226: 			pStmt->db->last_error = pStmt->result->error;
227: 			pStmt->prepared = nullptr;
228: 			return SQLITE_ERROR;
229: 		}
230: 		// fetch a chunk
231: 		if (!pStmt->result->TryFetch(pStmt->current_chunk, pStmt->db->last_error)) {
232: 			pStmt->prepared = nullptr;
233: 			return SQLITE_ERROR;
234: 		}
235: 
236: 		pStmt->current_row = -1;
237: 
238: 		auto statement_type = pStmt->prepared->GetStatementType();
239: 		if (StatementTypeReturnChanges(statement_type) && pStmt->current_chunk->size() > 0) {
240: 			// update total changes
241: 			auto row_changes = pStmt->current_chunk->GetValue(0, 0);
242: 			if (!row_changes.is_null && row_changes.TryCastAs(LogicalType::BIGINT)) {
243: 				pStmt->db->last_changes = row_changes.GetValue<int64_t>();
244: 				pStmt->db->total_changes += row_changes.GetValue<int64_t>();
245: 			}
246: 		}
247: 		if (!sqlite3_display_result(statement_type)) {
248: 			// only SELECT statements return results
249: 			sqlite3_reset(pStmt);
250: 		}
251: 	}
252: 	if (!pStmt->current_chunk || pStmt->current_chunk->size() == 0) {
253: 		return SQLITE_DONE;
254: 	}
255: 	pStmt->current_row++;
256: 	if (pStmt->current_row >= (int32_t)pStmt->current_chunk->size()) {
257: 		// have to fetch again!
258: 		pStmt->current_row = 0;
259: 		if (!pStmt->result->TryFetch(pStmt->current_chunk, pStmt->db->last_error)) {
260: 			pStmt->prepared = nullptr;
261: 			return SQLITE_ERROR;
262: 		}
263: 		if (!pStmt->current_chunk || pStmt->current_chunk->size() == 0) {
264: 			sqlite3_reset(pStmt);
265: 			return SQLITE_DONE;
266: 		}
267: 	}
268: 	return SQLITE_ROW;
269: }
270: 
271: /* Execute multiple semicolon separated SQL statements
272:  * and execute the passed callback for each produced result,
273:  * largely copied from the original sqlite3 source */
274: int sqlite3_exec(sqlite3 *db,                /* The database on which the SQL executes */
275:                  const char *zSql,           /* The SQL to be executed */
276:                  sqlite3_callback xCallback, /* Invoke this callback routine */
277:                  void *pArg,                 /* First argument to xCallback() */
278:                  char **pzErrMsg             /* Write error messages here */
279: ) {
280: 	int rc = SQLITE_OK;            /* Return code */
281: 	const char *zLeftover;         /* Tail of unprocessed SQL */
282: 	sqlite3_stmt *pStmt = nullptr; /* The current SQL statement */
283: 	char **azCols = nullptr;       /* Names of result columns */
284: 	char **azVals = nullptr;       /* Result values */
285: 
286: 	if (zSql == nullptr) {
287: 		zSql = "";
288: 	}
289: 
290: 	while (rc == SQLITE_OK && zSql[0]) {
291: 		int nCol;
292: 
293: 		pStmt = nullptr;
294: 		rc = sqlite3_prepare_v2(db, zSql, -1, &pStmt, &zLeftover);
295: 		if (rc != SQLITE_OK) {
296: 			if (pzErrMsg) {
297: 				auto errmsg = sqlite3_errmsg(db);
298: 				*pzErrMsg = errmsg ? sqlite3_strdup(errmsg) : nullptr;
299: 			}
300: 			continue;
301: 		}
302: 		if (!pStmt) {
303: 			/* this happens for a comment or white-space */
304: 			zSql = zLeftover;
305: 			continue;
306: 		}
307: 
308: 		nCol = sqlite3_column_count(pStmt);
309: 		azCols = (char **)malloc(nCol * sizeof(const char *));
310: 		azVals = (char **)malloc(nCol * sizeof(const char *));
311: 		if (!azCols || !azVals) {
312: 			goto exec_out;
313: 		}
314: 		for (int i = 0; i < nCol; i++) {
315: 			azCols[i] = (char *)sqlite3_column_name(pStmt, i);
316: 		}
317: 
318: 		while (true) {
319: 			rc = sqlite3_step(pStmt);
320: 
321: 			/* Invoke the callback function if required */
322: 			if (xCallback && rc == SQLITE_ROW) {
323: 				for (int i = 0; i < nCol; i++) {
324: 					azVals[i] = (char *)sqlite3_column_text(pStmt, i);
325: 					if (!azVals[i] && sqlite3_column_type(pStmt, i) != SQLITE_NULL) {
326: 						fprintf(stderr, "sqlite3_exec: out of memory.\n");
327: 						goto exec_out;
328: 					}
329: 				}
330: 				if (xCallback(pArg, nCol, azVals, azCols)) {
331: 					/* EVIDENCE-OF: R-38229-40159 If the callback function to
332: 					** sqlite3_exec() returns non-zero, then sqlite3_exec() will
333: 					** return SQLITE_ABORT. */
334: 					rc = SQLITE_ABORT;
335: 					sqlite3_finalize(pStmt);
336: 					pStmt = 0;
337: 					fprintf(stderr, "sqlite3_exec: callback returned non-zero. "
338: 					                "Aborting.\n");
339: 					goto exec_out;
340: 				}
341: 			}
342: 			if (rc == SQLITE_DONE) {
343: 				rc = sqlite3_finalize(pStmt);
344: 				pStmt = nullptr;
345: 				zSql = zLeftover;
346: 				while (isspace(zSql[0]))
347: 					zSql++;
348: 				break;
349: 			} else if (rc != SQLITE_ROW) {
350: 				// error
351: 				if (pzErrMsg) {
352: 					auto errmsg = sqlite3_errmsg(db);
353: 					*pzErrMsg = errmsg ? sqlite3_strdup(errmsg) : nullptr;
354: 				}
355: 				goto exec_out;
356: 			}
357: 		}
358: 
359: 		sqlite3_free(azCols);
360: 		sqlite3_free(azVals);
361: 		azCols = nullptr;
362: 		azVals = nullptr;
363: 	}
364: 
365: exec_out:
366: 	if (pStmt) {
367: 		sqlite3_finalize(pStmt);
368: 	}
369: 	sqlite3_free(azCols);
370: 	sqlite3_free(azVals);
371: 	if (rc != SQLITE_OK && pzErrMsg && !*pzErrMsg) {
372: 		// error but no error message set
373: 		*pzErrMsg = sqlite3_strdup("Unknown error in DuckDB!");
374: 	}
375: 	return rc;
376: }
377: 
378: /* Return the text of the SQL that was used to prepare the statement */
379: const char *sqlite3_sql(sqlite3_stmt *pStmt) {
380: 	return pStmt->query_string.c_str();
381: }
382: 
383: int sqlite3_column_count(sqlite3_stmt *pStmt) {
384: 	if (!pStmt || !pStmt->prepared) {
385: 		return 0;
386: 	}
387: 	return (int)pStmt->prepared->ColumnCount();
388: }
389: 
390: ////////////////////////////
391: //     sqlite3_column     //
392: ////////////////////////////
393: int sqlite3_column_type(sqlite3_stmt *pStmt, int iCol) {
394: 	if (!pStmt || !pStmt->result || !pStmt->current_chunk) {
395: 		return 0;
396: 	}
397: 	if (FlatVector::IsNull(pStmt->current_chunk->data[iCol], pStmt->current_row)) {
398: 		return SQLITE_NULL;
399: 	}
400: 	auto column_type = pStmt->result->types[iCol];
401: 	switch (column_type.id()) {
402: 	case LogicalTypeId::BOOLEAN:
403: 	case LogicalTypeId::TINYINT:
404: 	case LogicalTypeId::SMALLINT:
405: 	case LogicalTypeId::INTEGER:
406: 	case LogicalTypeId::BIGINT: /* TODO: Maybe blob? */
407: 		return SQLITE_INTEGER;
408: 	case LogicalTypeId::FLOAT:
409: 	case LogicalTypeId::DOUBLE:
410: 	case LogicalTypeId::DECIMAL:
411: 		return SQLITE_FLOAT;
412: 	case LogicalTypeId::DATE:
413: 	case LogicalTypeId::TIME:
414: 	case LogicalTypeId::TIMESTAMP:
415: 	case LogicalTypeId::TIMESTAMP_SEC:
416: 	case LogicalTypeId::TIMESTAMP_MS:
417: 	case LogicalTypeId::TIMESTAMP_NS:
418: 	case LogicalTypeId::VARCHAR:
419: 	case LogicalTypeId::LIST:
420: 	case LogicalTypeId::STRUCT:
421: 	case LogicalTypeId::MAP:
422: 		return SQLITE_TEXT;
423: 	case LogicalTypeId::BLOB:
424: 		return SQLITE_BLOB;
425: 	default:
426: 		// TODO(wangfenjin): agg function don't have type?
427: 		return SQLITE_TEXT;
428: 	}
429: 	return 0;
430: }
431: 
432: const char *sqlite3_column_name(sqlite3_stmt *pStmt, int N) {
433: 	if (!pStmt || !pStmt->prepared) {
434: 		return nullptr;
435: 	}
436: 	return pStmt->prepared->GetNames()[N].c_str();
437: }
438: 
439: static bool sqlite3_column_has_value(sqlite3_stmt *pStmt, int iCol, LogicalType target_type, Value &val) {
440: 	if (!pStmt || !pStmt->result || !pStmt->current_chunk) {
441: 		return false;
442: 	}
443: 	if (iCol < 0 || iCol >= (int)pStmt->result->types.size()) {
444: 		return false;
445: 	}
446: 	if (FlatVector::IsNull(pStmt->current_chunk->data[iCol], pStmt->current_row)) {
447: 		return false;
448: 	}
449: 	try {
450: 		val = pStmt->current_chunk->data[iCol].GetValue(pStmt->current_row).CastAs(target_type);
451: 	} catch (...) {
452: 		return false;
453: 	}
454: 	return true;
455: }
456: 
457: double sqlite3_column_double(sqlite3_stmt *stmt, int iCol) {
458: 	Value val;
459: 	if (!sqlite3_column_has_value(stmt, iCol, LogicalType::DOUBLE, val)) {
460: 		return 0;
461: 	}
462: 	return val.value_.double_;
463: }
464: 
465: int sqlite3_column_int(sqlite3_stmt *stmt, int iCol) {
466: 	Value val;
467: 	if (!sqlite3_column_has_value(stmt, iCol, LogicalType::INTEGER, val)) {
468: 		return 0;
469: 	}
470: 	return val.value_.integer;
471: }
472: 
473: sqlite3_int64 sqlite3_column_int64(sqlite3_stmt *stmt, int iCol) {
474: 	Value val;
475: 	if (!sqlite3_column_has_value(stmt, iCol, LogicalType::BIGINT, val)) {
476: 		return 0;
477: 	}
478: 	return val.value_.bigint;
479: }
480: 
481: const unsigned char *sqlite3_column_text(sqlite3_stmt *pStmt, int iCol) {
482: 	Value val;
483: 	if (!sqlite3_column_has_value(pStmt, iCol, LogicalType::VARCHAR, val)) {
484: 		return nullptr;
485: 	}
486: 	try {
487: 		if (!pStmt->current_text) {
488: 			pStmt->current_text =
489: 			    unique_ptr<sqlite3_string_buffer[]>(new sqlite3_string_buffer[pStmt->result->types.size()]);
490: 		}
491: 		auto &entry = pStmt->current_text[iCol];
492: 		if (!entry.data) {
493: 			// not initialized yet, convert the value and initialize it
494: 			entry.data = unique_ptr<char[]>(new char[val.str_value.size() + 1]);
495: 			memcpy(entry.data.get(), val.str_value.c_str(), val.str_value.size() + 1);
496: 			entry.data_len = val.str_value.length();
497: 		}
498: 		return (const unsigned char *)entry.data.get();
499: 	} catch (...) {
500: 		// memory error!
501: 		return nullptr;
502: 	}
503: }
504: 
505: const void *sqlite3_column_blob(sqlite3_stmt *pStmt, int iCol) {
506: 	Value val;
507: 	if (!sqlite3_column_has_value(pStmt, iCol, LogicalType::BLOB, val)) {
508: 		return nullptr;
509: 	}
510: 	try {
511: 		if (!pStmt->current_text) {
512: 			pStmt->current_text =
513: 			    unique_ptr<sqlite3_string_buffer[]>(new sqlite3_string_buffer[pStmt->result->types.size()]);
514: 		}
515: 		auto &entry = pStmt->current_text[iCol];
516: 		if (!entry.data) {
517: 			// not initialized yet, convert the value and initialize it
518: 			entry.data = unique_ptr<char[]>(new char[val.str_value.size() + 1]);
519: 			memcpy(entry.data.get(), val.str_value.c_str(), val.str_value.size() + 1);
520: 			entry.data_len = val.str_value.length();
521: 		}
522: 		return (const unsigned char *)entry.data.get();
523: 	} catch (...) {
524: 		// memory error!
525: 		return nullptr;
526: 	}
527: }
528: 
529: ////////////////////////////
530: //      sqlite3_bind      //
531: ////////////////////////////
532: int sqlite3_bind_parameter_count(sqlite3_stmt *stmt) {
533: 	if (!stmt) {
534: 		return 0;
535: 	}
536: 	return stmt->prepared->n_param;
537: }
538: 
539: const char *sqlite3_bind_parameter_name(sqlite3_stmt *stmt, int idx) {
540: 	if (!stmt) {
541: 		return nullptr;
542: 	}
543: 	if (idx < 1 || idx > (int)stmt->prepared->n_param) {
544: 		return nullptr;
545: 	}
546: 	return stmt->bound_names[idx - 1].c_str();
547: }
548: 
549: int sqlite3_bind_parameter_index(sqlite3_stmt *stmt, const char *zName) {
550: 	if (!stmt || !zName) {
551: 		return 0;
552: 	}
553: 	for (idx_t i = 0; i < stmt->bound_names.size(); i++) {
554: 		if (stmt->bound_names[i] == string(zName)) {
555: 			return i + 1;
556: 		}
557: 	}
558: 	return 0;
559: }
560: 
561: int sqlite3_internal_bind_value(sqlite3_stmt *stmt, int idx, Value value) {
562: 	if (!stmt || !stmt->prepared || stmt->result) {
563: 		return SQLITE_MISUSE;
564: 	}
565: 	if (idx < 1 || idx > (int)stmt->prepared->n_param) {
566: 		return SQLITE_RANGE;
567: 	}
568: 	stmt->bound_values[idx - 1] = value;
569: 	return SQLITE_OK;
570: }
571: 
572: int sqlite3_bind_int(sqlite3_stmt *stmt, int idx, int val) {
573: 	return sqlite3_internal_bind_value(stmt, idx, Value::INTEGER(val));
574: }
575: 
576: int sqlite3_bind_int64(sqlite3_stmt *stmt, int idx, sqlite3_int64 val) {
577: 	return sqlite3_internal_bind_value(stmt, idx, Value::BIGINT(val));
578: }
579: 
580: int sqlite3_bind_double(sqlite3_stmt *stmt, int idx, double val) {
581: 	return sqlite3_internal_bind_value(stmt, idx, Value::DOUBLE(val));
582: }
583: 
584: int sqlite3_bind_null(sqlite3_stmt *stmt, int idx) {
585: 	return sqlite3_internal_bind_value(stmt, idx, Value());
586: }
587: 
588: SQLITE_API int sqlite3_bind_value(sqlite3_stmt *, int, const sqlite3_value *) {
589: 	fprintf(stderr, "sqlite3_bind_value: unsupported.\n");
590: 	return SQLITE_ERROR;
591: }
592: 
593: int sqlite3_bind_text(sqlite3_stmt *stmt, int idx, const char *val, int length, void (*free_func)(void *)) {
594: 	if (!val) {
595: 		return SQLITE_MISUSE;
596: 	}
597: 	string value;
598: 	if (length < 0) {
599: 		value = string(val);
600: 	} else {
601: 		value = string(val, val + length);
602: 	}
603: 	if (free_func && ((ptrdiff_t)free_func) != -1) {
604: 		free_func((void *)val);
605: 		val = nullptr;
606: 	}
607: 	try {
608: 		return sqlite3_internal_bind_value(stmt, idx, Value(value));
609: 	} catch (std::exception &ex) {
610: 		return SQLITE_ERROR;
611: 	}
612: }
613: 
614: int sqlite3_bind_blob(sqlite3_stmt *stmt, int idx, const void *val, int length, void (*free_func)(void *)) {
615: 	if (!val) {
616: 		return SQLITE_MISUSE;
617: 	}
618: 	Value blob;
619: 	if (length < 0) {
620: 		blob = Value::BLOB(string((const char *)val));
621: 	} else {
622: 		blob = Value::BLOB((const_data_ptr_t)val, length);
623: 	}
624: 	if (free_func && ((ptrdiff_t)free_func) != -1) {
625: 		free_func((void *)val);
626: 		val = nullptr;
627: 	}
628: 	try {
629: 		return sqlite3_internal_bind_value(stmt, idx, blob);
630: 	} catch (std::exception &ex) {
631: 		return SQLITE_ERROR;
632: 	}
633: }
634: 
635: SQLITE_API int sqlite3_bind_zeroblob(sqlite3_stmt *stmt, int idx, int length) {
636: 	fprintf(stderr, "sqlite3_bind_zeroblob: unsupported.\n");
637: 	return SQLITE_ERROR;
638: }
639: 
640: int sqlite3_clear_bindings(sqlite3_stmt *stmt) {
641: 	if (!stmt) {
642: 		return SQLITE_MISUSE;
643: 	}
644: 	return SQLITE_OK;
645: }
646: 
647: int sqlite3_initialize(void) {
648: 	return SQLITE_OK;
649: }
650: 
651: int sqlite3_finalize(sqlite3_stmt *pStmt) {
652: 	if (pStmt) {
653: 		if (pStmt->result && !pStmt->result->success) {
654: 			pStmt->db->last_error = string(pStmt->result->error);
655: 			delete pStmt;
656: 			return SQLITE_ERROR;
657: 		}
658: 
659: 		delete pStmt;
660: 	}
661: 	return SQLITE_OK;
662: }
663: 
664: /*
665: ** Some systems have stricmp().  Others have strcasecmp().  Because
666: ** there is no consistency, we will define our own.
667: **
668: ** IMPLEMENTATION-OF: R-30243-02494 The sqlite3_stricmp() and
669: ** sqlite3_strnicmp() APIs allow applications and extensions to compare
670: ** the contents of two buffers containing UTF-8 strings in a
671: ** case-independent fashion, using the same definition of "case
672: ** independence" that SQLite uses internally when comparing identifiers.
673: */
674: 
675: const unsigned char sqlite3UpperToLower[] = {
676:     0,   1,   2,   3,   4,   5,   6,   7,   8,   9,   10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,
677:     22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,
678:     44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  97,
679:     98,  99,  100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,
680:     120, 121, 122, 91,  92,  93,  94,  95,  96,  97,  98,  99,  100, 101, 102, 103, 104, 105, 106, 107, 108, 109,
681:     110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,
682:     132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
683:     154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,
684:     176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,
685:     198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,
686:     220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,
687:     242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255};
688: 
689: int sqlite3StrICmp(const char *zLeft, const char *zRight) {
690: 	unsigned char *a, *b;
691: 	int c;
692: 	a = (unsigned char *)zLeft;
693: 	b = (unsigned char *)zRight;
694: 	for (;;) {
695: 		c = (int)sqlite3UpperToLower[*a] - (int)sqlite3UpperToLower[*b];
696: 		if (c || *a == 0)
697: 			break;
698: 		a++;
699: 		b++;
700: 	}
701: 	return c;
702: }
703: 
704: SQLITE_API int sqlite3_stricmp(const char *zLeft, const char *zRight) {
705: 	if (zLeft == 0) {
706: 		return zRight ? -1 : 0;
707: 	} else if (zRight == 0) {
708: 		return 1;
709: 	}
710: 	return sqlite3StrICmp(zLeft, zRight);
711: }
712: 
713: SQLITE_API int sqlite3_strnicmp(const char *zLeft, const char *zRight, int N) {
714: 	unsigned char *a, *b;
715: 	if (zLeft == 0) {
716: 		return zRight ? -1 : 0;
717: 	} else if (zRight == 0) {
718: 		return 1;
719: 	}
720: 	a = (unsigned char *)zLeft;
721: 	b = (unsigned char *)zRight;
722: 	while (N-- > 0 && *a != 0 && sqlite3UpperToLower[*a] == sqlite3UpperToLower[*b]) {
723: 		a++;
724: 		b++;
725: 	}
726: 	return N < 0 ? 0 : sqlite3UpperToLower[*a] - sqlite3UpperToLower[*b];
727: }
728: 
729: char *sqlite3_strdup(const char *str) {
730: 	char *result = (char *)sqlite3_malloc64(strlen(str) + 1);
731: 	strcpy(result, str);
732: 	return result;
733: }
734: 
735: void *sqlite3_malloc64(sqlite3_uint64 n) {
736: 	return malloc(n);
737: }
738: 
739: void sqlite3_free(void *pVoid) {
740: 	free(pVoid);
741: }
742: 
743: void *sqlite3_malloc(int n) {
744: 	return sqlite3_malloc64(n);
745: }
746: 
747: void *sqlite3_realloc(void *ptr, int n) {
748: 	return sqlite3_realloc64(ptr, n);
749: }
750: 
751: void *sqlite3_realloc64(void *ptr, sqlite3_uint64 n) {
752: 	return realloc(ptr, n);
753: }
754: 
755: // TODO: stub
756: int sqlite3_config(int i, ...) {
757: 	return SQLITE_OK;
758: }
759: 
760: int sqlite3_errcode(sqlite3 *db) {
761: 	if (!db) {
762: 		return SQLITE_NOMEM;
763: 	}
764: 	// return db->last_error.empty() ? SQLITE_OK : SQLITE_ERROR;
765: 	return db->errCode; //! We should return the exact error code
766: }
767: 
768: int sqlite3_extended_errcode(sqlite3 *db) {
769: 	return sqlite3_errcode(db);
770: }
771: 
772: const char *sqlite3_errmsg(sqlite3 *db) {
773: 	if (!db) {
774: 		return "";
775: 	}
776: 	return db->last_error.c_str();
777: }
778: 
779: void sqlite3_interrupt(sqlite3 *db) {
780: 	if (db) {
781: 		db->con->Interrupt();
782: 	}
783: }
784: 
785: const char *sqlite3_libversion(void) {
786: 	return DuckDB::LibraryVersion();
787: }
788: 
789: const char *sqlite3_sourceid(void) {
790: 	return DuckDB::SourceID();
791: }
792: 
793: int sqlite3_reset(sqlite3_stmt *stmt) {
794: 	if (stmt) {
795: 		stmt->result = nullptr;
796: 		stmt->current_chunk = nullptr;
797: 	}
798: 	return SQLITE_OK;
799: }
800: 
801: // support functions for shell.c
802: // most are dummies, we don't need them really
803: 
804: int sqlite3_db_status(sqlite3 *, int op, int *pCur, int *pHiwtr, int resetFlg) {
805: 	fprintf(stderr, "sqlite3_db_status: unsupported.\n");
806: 	return -1;
807: }
808: 
809: int sqlite3_changes(sqlite3 *db) {
810: 	return db->last_changes;
811: }
812: 
813: int sqlite3_total_changes(sqlite3 *db) {
814: 	return db->total_changes;
815: }
816: 
817: SQLITE_API sqlite3_int64 sqlite3_last_insert_rowid(sqlite3 *db) {
818: 	return SQLITE_ERROR;
819: }
820: 
821: // some code borrowed from sqlite
822: // its probably best to match its behavior
823: 
824: typedef uint8_t u8;
825: 
826: /*
827: ** Token types used by the sqlite3_complete() routine.  See the header
828: ** comments on that procedure for additional information.
829: */
830: #define tkSEMI  0
831: #define tkWS    1
832: #define tkOTHER 2
833: 
834: const unsigned char sqlite3CtypeMap[256] = {
835:     0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 00..07    ........ */
836:     0x00, 0x01, 0x01, 0x01, 0x01, 0x01, 0x00, 0x00, /* 08..0f    ........ */
837:     0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 10..17    ........ */
838:     0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 18..1f    ........ */
839:     0x01, 0x00, 0x80, 0x00, 0x40, 0x00, 0x00, 0x80, /* 20..27     !"#$%&' */
840:     0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 28..2f    ()*+,-./ */
841:     0x0c, 0x0c, 0x0c, 0x0c, 0x0c, 0x0c, 0x0c, 0x0c, /* 30..37    01234567 */
842:     0x0c, 0x0c, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /* 38..3f    89:;<=>? */
843: 
844:     0x00, 0x0a, 0x0a, 0x0a, 0x0a, 0x0a, 0x0a, 0x02, /* 40..47    @ABCDEFG */
845:     0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, /* 48..4f    HIJKLMNO */
846:     0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, 0x02, /* 50..57    PQRSTUVW */
847:     0x02, 0x02, 0x02, 0x80, 0x00, 0x00, 0x00, 0x40, /* 58..5f    XYZ[\]^_ */
848:     0x80, 0x2a, 0x2a, 0x2a, 0x2a, 0x2a, 0x2a, 0x22, /* 60..67    `abcdefg */
849:     0x22, 0x22, 0x22, 0x22, 0x22, 0x22, 0x22, 0x22, /* 68..6f    hijklmno */
850:     0x22, 0x22, 0x22, 0x22, 0x22, 0x22, 0x22, 0x22, /* 70..77    pqrstuvw */
851:     0x22, 0x22, 0x22, 0x00, 0x00, 0x00, 0x00, 0x00, /* 78..7f    xyz{|}~. */
852: 
853:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* 80..87    ........ */
854:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* 88..8f    ........ */
855:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* 90..97    ........ */
856:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* 98..9f    ........ */
857:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* a0..a7    ........ */
858:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* a8..af    ........ */
859:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* b0..b7    ........ */
860:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* b8..bf    ........ */
861: 
862:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* c0..c7    ........ */
863:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* c8..cf    ........ */
864:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* d0..d7    ........ */
865:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* d8..df    ........ */
866:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* e0..e7    ........ */
867:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* e8..ef    ........ */
868:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, /* f0..f7    ........ */
869:     0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40, 0x40  /* f8..ff    ........ */
870: };
871: 
872: // TODO this can probably be simplified
873: #define IdChar(C) ((sqlite3CtypeMap[(unsigned char)C] & 0x46) != 0)
874: 
875: int sqlite3_complete(const char *zSql) {
876: 	u8 state = 0; /* Current state, using numbers defined in header comment */
877: 	u8 token;     /* Value of the next token */
878: 
879: 	/* If triggers are not supported by this compile then the statement machine
880: 	 ** used to detect the end of a statement is much simpler
881: 	 */
882: 	static const u8 trans[3][3] = {
883: 	    /* Token:           */
884: 	    /* State:       **  SEMI  WS  OTHER */
885: 	    /* 0 INVALID: */ {
886: 	        1,
887: 	        0,
888: 	        2,
889: 	    },
890: 	    /* 1   START: */
891: 	    {
892: 	        1,
893: 	        1,
894: 	        2,
895: 	    },
896: 	    /* 2  NORMAL: */
897: 	    {
898: 	        1,
899: 	        2,
900: 	        2,
901: 	    },
902: 	};
903: 
904: 	while (*zSql) {
905: 		switch (*zSql) {
906: 		case ';': { /* A semicolon */
907: 			token = tkSEMI;
908: 			break;
909: 		}
910: 		case ' ':
911: 		case '\r':
912: 		case '\t':
913: 		case '\n':
914: 		case '\f': { /* White space is ignored */
915: 			token = tkWS;
916: 			break;
917: 		}
918: 		case '/': { /* C-style comments */
919: 			if (zSql[1] != '*') {
920: 				token = tkOTHER;
921: 				break;
922: 			}
923: 			zSql += 2;
924: 			while (zSql[0] && (zSql[0] != '*' || zSql[1] != '/')) {
925: 				zSql++;
926: 			}
927: 			if (zSql[0] == 0)
928: 				return 0;
929: 			zSql++;
930: 			token = tkWS;
931: 			break;
932: 		}
933: 		case '-': { /* SQL-style comments from "--" to end of line */
934: 			if (zSql[1] != '-') {
935: 				token = tkOTHER;
936: 				break;
937: 			}
938: 			while (*zSql && *zSql != '\n') {
939: 				zSql++;
940: 			}
941: 			if (*zSql == 0)
942: 				return state == 1;
943: 			token = tkWS;
944: 			break;
945: 		}
946: 		case '[': { /* Microsoft-style identifiers in [...] */
947: 			zSql++;
948: 			while (*zSql && *zSql != ']') {
949: 				zSql++;
950: 			}
951: 			if (*zSql == 0)
952: 				return 0;
953: 			token = tkOTHER;
954: 			break;
955: 		}
956: 		case '`': /* Grave-accent quoted symbols used by MySQL */
957: 		case '"': /* single- and double-quoted strings */
958: 		case '\'': {
959: 			int c = *zSql;
960: 			zSql++;
961: 			while (*zSql && *zSql != c) {
962: 				zSql++;
963: 			}
964: 			if (*zSql == 0)
965: 				return 0;
966: 			token = tkOTHER;
967: 			break;
968: 		}
969: 		default: {
970: 
971: 			if (IdChar((u8)*zSql)) {
972: 				/* Keywords and unquoted identifiers */
973: 				int nId;
974: 				for (nId = 1; IdChar(zSql[nId]); nId++) {
975: 				}
976: 				token = tkOTHER;
977: 
978: 				zSql += nId - 1;
979: 			} else {
980: 				/* Operators and special symbols */
981: 				token = tkOTHER;
982: 			}
983: 			break;
984: 		}
985: 		}
986: 		state = trans[state][token];
987: 		zSql++;
988: 	}
989: 	return state == 1;
990: }
991: 
992: // checks if input ends with ;
993: int sqlite3_complete_old(const char *sql) {
994: 	fprintf(stderr, "sqlite3_complete: unsupported. '%s'\n", sql);
995: 	return -1;
996: }
997: 
998: // length of varchar or blob value
999: int sqlite3_column_bytes(sqlite3_stmt *pStmt, int iCol) {
1000: 	// fprintf(stderr, "sqlite3_column_bytes: unsupported.\n");
1001: 	return pStmt->current_text[iCol].data_len;
1002: 	// return -1;
1003: }
1004: 
1005: sqlite3_value *sqlite3_column_value(sqlite3_stmt *, int iCol) {
1006: 	fprintf(stderr, "sqlite3_column_value: unsupported.\n");
1007: 	return nullptr;
1008: }
1009: 
1010: int sqlite3_db_config(sqlite3 *, int op, ...) {
1011: 	fprintf(stderr, "sqlite3_db_config: unsupported.\n");
1012: 	return -1;
1013: }
1014: 
1015: int sqlite3_get_autocommit(sqlite3 *db) {
1016: 	return db->con->context->transaction.IsAutoCommit();
1017: }
1018: 
1019: int sqlite3_limit(sqlite3 *, int id, int newVal) {
1020: 	fprintf(stderr, "sqlite3_limit: unsupported.\n");
1021: 	return -1;
1022: }
1023: 
1024: int sqlite3_stmt_readonly(sqlite3_stmt *pStmt) {
1025: 	fprintf(stderr, "sqlite3_stmt_readonly: unsupported.\n");
1026: 	return -1;
1027: }
1028: 
1029: // TODO pretty easy schema lookup
1030: int sqlite3_table_column_metadata(sqlite3 *db,             /* Connection handle */
1031:                                   const char *zDbName,     /* Database name or NULL */
1032:                                   const char *zTableName,  /* Table name */
1033:                                   const char *zColumnName, /* Column name */
1034:                                   char const **pzDataType, /* OUTPUT: Declared data type */
1035:                                   char const **pzCollSeq,  /* OUTPUT: Collation sequence name */
1036:                                   int *pNotNull,           /* OUTPUT: True if NOT NULL constraint exists */
1037:                                   int *pPrimaryKey,        /* OUTPUT: True if column part of PK */
1038:                                   int *pAutoinc            /* OUTPUT: True if column is auto-increment */
1039: ) {
1040: 	fprintf(stderr, "sqlite3_table_column_metadata: unsupported.\n");
1041: 	return -1;
1042: }
1043: 
1044: const char *sqlite3_column_decltype(sqlite3_stmt *pStmt, int iCol) {
1045: 	if (!pStmt || !pStmt->prepared) {
1046: 		return NULL;
1047: 	}
1048: 	auto column_type = pStmt->prepared->GetTypes()[iCol];
1049: 	switch (column_type.id()) {
1050: 	case LogicalTypeId::BOOLEAN:
1051: 		return "BOOLEAN";
1052: 	case LogicalTypeId::TINYINT:
1053: 		return "TINYINT";
1054: 	case LogicalTypeId::SMALLINT:
1055: 		return "SMALLINT";
1056: 	case LogicalTypeId::INTEGER:
1057: 		return "INTEGER";
1058: 	case LogicalTypeId::BIGINT:
1059: 		return "BIGINT";
1060: 	case LogicalTypeId::FLOAT:
1061: 		return "FLOAT";
1062: 	case LogicalTypeId::DOUBLE:
1063: 		return "DOUBLE";
1064: 	case LogicalTypeId::DECIMAL:
1065: 		return "DECIMAL";
1066: 	case LogicalTypeId::DATE:
1067: 		return "DATE";
1068: 	case LogicalTypeId::TIME:
1069: 		return "TIME";
1070: 	case LogicalTypeId::TIMESTAMP:
1071: 	case LogicalTypeId::TIMESTAMP_NS:
1072: 	case LogicalTypeId::TIMESTAMP_MS:
1073: 	case LogicalTypeId::TIMESTAMP_SEC:
1074: 		return "TIMESTAMP";
1075: 	case LogicalTypeId::VARCHAR:
1076: 		return "VARCHAR";
1077: 	case LogicalTypeId::LIST:
1078: 		return "LIST";
1079: 	case LogicalTypeId::MAP:
1080: 		return "MAP";
1081: 	case LogicalTypeId::STRUCT:
1082: 		return "STRUCT";
1083: 	case LogicalTypeId::BLOB:
1084: 		return "BLOB";
1085: 	default:
1086: 		return NULL;
1087: 	}
1088: 	return NULL;
1089: }
1090: 
1091: int sqlite3_status64(int op, sqlite3_int64 *pCurrent, sqlite3_int64 *pHighwater, int resetFlag) {
1092: 	fprintf(stderr, "sqlite3_status64: unsupported.\n");
1093: 	return -1;
1094: }
1095: 
1096: int sqlite3_status64(sqlite3 *, int op, int *pCur, int *pHiwtr, int resetFlg) {
1097: 	fprintf(stderr, "sqlite3_status64: unsupported.\n");
1098: 	return -1;
1099: }
1100: 
1101: int sqlite3_stmt_status(sqlite3_stmt *, int op, int resetFlg) {
1102: 	fprintf(stderr, "sqlite3_stmt_status: unsupported.\n");
1103: 	return -1;
1104: }
1105: 
1106: int sqlite3_file_control(sqlite3 *, const char *zDbName, int op, void *) {
1107: 	fprintf(stderr, "sqlite3_file_control: unsupported.\n");
1108: 	return -1;
1109: }
1110: 
1111: int sqlite3_declare_vtab(sqlite3 *, const char *zSQL) {
1112: 	fprintf(stderr, "sqlite3_declare_vtab: unsupported.\n");
1113: 	return -1;
1114: }
1115: 
1116: const char *sqlite3_vtab_collation(sqlite3_index_info *, int) {
1117: 	fprintf(stderr, "sqlite3_vtab_collation: unsupported.\n");
1118: 	return nullptr;
1119: }
1120: 
1121: int sqlite3_sleep(int) {
1122: 	fprintf(stderr, "sqlite3_sleep: unsupported.\n");
1123: 	return -1;
1124: }
1125: 
1126: int sqlite3_busy_timeout(sqlite3 *, int ms) {
1127: 	fprintf(stderr, "sqlite3_busy_timeout: unsupported.\n");
1128: 	return -1;
1129: }
1130: 
1131: // unlikely to be supported
1132: 
1133: int sqlite3_trace_v2(sqlite3 *, unsigned uMask, int (*xCallback)(unsigned, void *, void *, void *), void *pCtx) {
1134: 	fprintf(stderr, "sqlite3_trace_v2: unsupported.\n");
1135: 	return -1;
1136: }
1137: 
1138: int sqlite3_test_control(int op, ...) {
1139: 	fprintf(stderr, "sqlite3_test_control: unsupported.\n");
1140: 	return -1;
1141: }
1142: 
1143: int sqlite3_enable_load_extension(sqlite3 *db, int onoff) {
1144: 	// fprintf(stderr, "sqlite3_enable_load_extension: unsupported.\n");
1145: 	return -1;
1146: }
1147: 
1148: int sqlite3_load_extension(sqlite3 *db,       /* Load the extension into this database connection */
1149:                            const char *zFile, /* Name of the shared library containing extension */
1150:                            const char *zProc, /* Entry point.  Derived from zFile if 0 */
1151:                            char **pzErrMsg    /* Put error message here if not 0 */
1152: ) {
1153: 	// fprintf(stderr, "sqlite3_load_extension: unsupported.\n");
1154: 	return -1;
1155: }
1156: 
1157: int sqlite3_create_module(sqlite3 *db,             /* SQLite connection to register module with */
1158:                           const char *zName,       /* Name of the module */
1159:                           const sqlite3_module *p, /* Methods for the module */
1160:                           void *pClientData        /* Client data for xCreate/xConnect */
1161: ) {
1162: 	// fprintf(stderr, "sqlite3_create_module: unsupported.\n");
1163: 	return -1;
1164: }
1165: 
1166: int sqlite3_create_function(sqlite3 *db, const char *zFunctionName, int nArg, int eTextRep, void *pApp,
1167:                             void (*xFunc)(sqlite3_context *, int, sqlite3_value **),
1168:                             void (*xStep)(sqlite3_context *, int, sqlite3_value **),
1169:                             void (*xFinal)(sqlite3_context *)) {
1170: 	if ((!xFunc && !xStep && !xFinal) || !zFunctionName || nArg < -1) {
1171: 		return SQLITE_MISUSE;
1172: 	}
1173: 	string fname = string(zFunctionName);
1174: 
1175: 	// Scalar function
1176: 	if (xFunc) {
1177: 		auto udf_sqlite3 = SQLiteUDFWrapper::CreateSQLiteScalarFunction(xFunc, db, pApp);
1178: 		LogicalType varargs = LogicalType::INVALID;
1179: 		if (nArg == -1) {
1180: 			varargs = LogicalType::ANY;
1181: 			nArg = 0;
1182: 		}
1183: 
1184: 		vector<LogicalType> argv_types(nArg);
1185: 		for (idx_t i = 0; i < (idx_t)nArg; ++i) {
1186: 			argv_types[i] = LogicalType::ANY;
1187: 		}
1188: 
1189: 		UDFWrapper::RegisterFunction(fname, argv_types, LogicalType::VARCHAR, udf_sqlite3, *(db->con->context),
1190: 		                             varargs);
1191: 
1192: 		return SQLITE_OK;
1193: 	}
1194: 
1195: 	return SQLITE_MISUSE;
1196: }
1197: 
1198: int sqlite3_create_function_v2(sqlite3 *db, const char *zFunctionName, int nArg, int eTextRep, void *pApp,
1199:                                void (*xFunc)(sqlite3_context *, int, sqlite3_value **),
1200:                                void (*xStep)(sqlite3_context *, int, sqlite3_value **),
1201:                                void (*xFinal)(sqlite3_context *), void (*xDestroy)(void *)) {
1202: 	return -1;
1203: }
1204: 
1205: int sqlite3_set_authorizer(sqlite3 *, int (*xAuth)(void *, int, const char *, const char *, const char *, const char *),
1206:                            void *pUserData) {
1207: 	fprintf(stderr, "sqlite3_set_authorizer: unsupported.\n");
1208: 	return -1;
1209: }
1210: 
1211: // needed in shell timer
1212: static int unixCurrentTimeInt64(sqlite3_vfs *NotUsed, sqlite3_int64 *piNow) {
1213: 	using namespace std::chrono;
1214: 	*piNow = (sqlite3_int64)duration_cast<milliseconds>(system_clock::now().time_since_epoch()).count();
1215: 	return SQLITE_OK;
1216: }
1217: 
1218: static sqlite3_vfs static_sqlite3_virtual_file_systems[] = {{
1219:     3,                    // int iVersion;            /* Structure version number (currently 3) */
1220:     0,                    // int szOsFile;            /* Size of subclassed sqlite3_file */
1221:     0,                    // int mxPathname;          /* Maximum file pathname length */
1222:     nullptr,              // sqlite3_vfs *pNext;      /* Next registered VFS */
1223:     "dummy",              // const char *zName;       /* Name of this virtual file system */
1224:     nullptr,              // void *pAppData;          /* Pointer to application-specific data */
1225:     nullptr,              // int (*xOpen)(sqlite3_vfs*, const char *zName, sqlite3_file*, int flags, int *pOutFlags);
1226:     nullptr,              // int (*xDelete)(sqlite3_vfs*, const char *zName, int syncDir);
1227:     nullptr,              // int (*xAccess)(sqlite3_vfs*, const char *zName, int flags, int *pResOut);
1228:     nullptr,              // int (*xFullPathname)(sqlite3_vfs*, const char *zName, int nOut, char *zOut);
1229:     nullptr,              // void *(*xDlOpen)(sqlite3_vfs*, const char *zFilename);
1230:     nullptr,              // void (*xDlError)(sqlite3_vfs*, int nByte, char *zErrMsg);
1231:     nullptr,              // void (*(*xDlSym)(sqlite3_vfs*,void*, const char *zSymbol))(void);
1232:     nullptr,              // void (*xDlClose)(sqlite3_vfs*, void*);
1233:     nullptr,              // int (*xRandomness)(sqlite3_vfs*, int nByte, char *zOut);
1234:     nullptr,              // int (*xSleep)(sqlite3_vfs*, int microseconds);
1235:     nullptr,              // int (*xCurrentTime)(sqlite3_vfs*, double*);
1236:     nullptr,              // int (*xGetLastError)(sqlite3_vfs*, int, char *);
1237:     unixCurrentTimeInt64, // int (*xCurrentTimeInt64)(sqlite3_vfs*, sqlite3_int64*);
1238:     nullptr,              // int (*xSetSystemCall)(sqlite3_vfs*, const char *zName, sqlite3_syscall_ptr);
1239:     nullptr,              // sqlite3_syscall_ptr (*xGetSystemCall)(sqlite3_vfs*, const char *zName);
1240:     nullptr               // const char *(*xNextSystemCall)(sqlite3_vfs*, const char *zName);
1241: }};
1242: 
1243: // virtual file system, providing some dummies to avoid crashes
1244: sqlite3_vfs *sqlite3_vfs_find(const char *zVfsName) {
1245: 	// return a dummy because the shell does not check the return code.
1246: 	return static_sqlite3_virtual_file_systems;
1247: }
1248: 
1249: int sqlite3_vfs_register(sqlite3_vfs *, int makeDflt) {
1250: 	// fprintf(stderr, "sqlite3_vfs_register: unsupported.\n");
1251: 	return -1;
1252: }
1253: 
1254: // backups, unused
1255: 
1256: int sqlite3_backup_step(sqlite3_backup *p, int nPage) {
1257: 	fprintf(stderr, "sqlite3_backup_step: unsupported.\n");
1258: 	return -1;
1259: }
1260: 
1261: int sqlite3_backup_finish(sqlite3_backup *p) {
1262: 	fprintf(stderr, "sqlite3_backup_finish: unsupported.\n");
1263: 	return -1;
1264: }
1265: 
1266: sqlite3_backup *sqlite3_backup_init(sqlite3 *pDest,         /* Destination database handle */
1267:                                     const char *zDestName,  /* Destination database name */
1268:                                     sqlite3 *pSource,       /* Source database handle */
1269:                                     const char *zSourceName /* Source database name */
1270: ) {
1271: 	fprintf(stderr, "sqlite3_backup_init: unsupported.\n");
1272: 	return nullptr;
1273: }
1274: 
1275: // UDF support stuff, unused for now. These cannot be called as create_function above is disabled
1276: 
1277: SQLITE_API sqlite3 *sqlite3_context_db_handle(sqlite3_context *) {
1278: 	return nullptr;
1279: }
1280: 
1281: void *sqlite3_user_data(sqlite3_context *context) {
1282: 	assert(context);
1283: 	return context->pFunc.pUserData;
1284: }
1285: 
1286: #ifdef _WIN32
1287: #include <windows.h>
1288: 
1289: static void *sqlite3MallocZero(size_t n) {
1290: 	auto res = sqlite3_malloc(n);
1291: 	assert(res);
1292: 	memset(res, 0, n);
1293: 	return res;
1294: }
1295: 
1296: static LPWSTR winUtf8ToUnicode(const char *zText) {
1297: 	int nChar;
1298: 	LPWSTR zWideText;
1299: 
1300: 	nChar = MultiByteToWideChar(CP_UTF8, 0, zText, -1, NULL, 0);
1301: 	if (nChar == 0) {
1302: 		return 0;
1303: 	}
1304: 	zWideText = (LPWSTR)sqlite3MallocZero(nChar * sizeof(WCHAR));
1305: 	if (zWideText == 0) {
1306: 		return 0;
1307: 	}
1308: 	nChar = MultiByteToWideChar(CP_UTF8, 0, zText, -1, zWideText, nChar);
1309: 	if (nChar == 0) {
1310: 		sqlite3_free(zWideText);
1311: 		zWideText = 0;
1312: 	}
1313: 	return zWideText;
1314: }
1315: 
1316: static char *winUnicodeToMbcs(LPCWSTR zWideText, int useAnsi) {
1317: 	int nByte;
1318: 	char *zText;
1319: 	int codepage = useAnsi ? CP_ACP : CP_OEMCP;
1320: 
1321: 	nByte = WideCharToMultiByte(codepage, 0, zWideText, -1, 0, 0, 0, 0);
1322: 	if (nByte == 0) {
1323: 		return 0;
1324: 	}
1325: 	zText = (char *)sqlite3MallocZero(nByte);
1326: 	if (zText == 0) {
1327: 		return 0;
1328: 	}
1329: 	nByte = WideCharToMultiByte(codepage, 0, zWideText, -1, zText, nByte, 0, 0);
1330: 	if (nByte == 0) {
1331: 		sqlite3_free(zText);
1332: 		zText = 0;
1333: 	}
1334: 	return zText;
1335: }
1336: 
1337: static char *winUtf8ToMbcs(const char *zText, int useAnsi) {
1338: 	char *zTextMbcs;
1339: 	LPWSTR zTmpWide;
1340: 
1341: 	zTmpWide = winUtf8ToUnicode(zText);
1342: 	if (zTmpWide == 0) {
1343: 		return 0;
1344: 	}
1345: 	zTextMbcs = winUnicodeToMbcs(zTmpWide, useAnsi);
1346: 	sqlite3_free(zTmpWide);
1347: 	return zTextMbcs;
1348: }
1349: 
1350: SQLITE_API char *sqlite3_win32_utf8_to_mbcs_v2(const char *zText, int useAnsi) {
1351: 	return winUtf8ToMbcs(zText, useAnsi);
1352: }
1353: 
1354: LPWSTR sqlite3_win32_utf8_to_unicode(const char *zText) {
1355: 	return winUtf8ToUnicode(zText);
1356: }
1357: 
1358: static LPWSTR winMbcsToUnicode(const char *zText, int useAnsi) {
1359: 	int nByte;
1360: 	LPWSTR zMbcsText;
1361: 	int codepage = useAnsi ? CP_ACP : CP_OEMCP;
1362: 
1363: 	nByte = MultiByteToWideChar(codepage, 0, zText, -1, NULL, 0) * sizeof(WCHAR);
1364: 	if (nByte == 0) {
1365: 		return 0;
1366: 	}
1367: 	zMbcsText = (LPWSTR)sqlite3MallocZero(nByte * sizeof(WCHAR));
1368: 	if (zMbcsText == 0) {
1369: 		return 0;
1370: 	}
1371: 	nByte = MultiByteToWideChar(codepage, 0, zText, -1, zMbcsText, nByte);
1372: 	if (nByte == 0) {
1373: 		sqlite3_free(zMbcsText);
1374: 		zMbcsText = 0;
1375: 	}
1376: 	return zMbcsText;
1377: }
1378: 
1379: static char *winUnicodeToUtf8(LPCWSTR zWideText) {
1380: 	int nByte;
1381: 	char *zText;
1382: 
1383: 	nByte = WideCharToMultiByte(CP_UTF8, 0, zWideText, -1, 0, 0, 0, 0);
1384: 	if (nByte == 0) {
1385: 		return 0;
1386: 	}
1387: 	zText = (char *)sqlite3MallocZero(nByte);
1388: 	if (zText == 0) {
1389: 		return 0;
1390: 	}
1391: 	nByte = WideCharToMultiByte(CP_UTF8, 0, zWideText, -1, zText, nByte, 0, 0);
1392: 	if (nByte == 0) {
1393: 		sqlite3_free(zText);
1394: 		zText = 0;
1395: 	}
1396: 	return zText;
1397: }
1398: 
1399: static char *winMbcsToUtf8(const char *zText, int useAnsi) {
1400: 	char *zTextUtf8;
1401: 	LPWSTR zTmpWide;
1402: 
1403: 	zTmpWide = winMbcsToUnicode(zText, useAnsi);
1404: 	if (zTmpWide == 0) {
1405: 		return 0;
1406: 	}
1407: 	zTextUtf8 = winUnicodeToUtf8(zTmpWide);
1408: 	sqlite3_free(zTmpWide);
1409: 	return zTextUtf8;
1410: }
1411: 
1412: SQLITE_API char *sqlite3_win32_mbcs_to_utf8_v2(const char *zText, int useAnsi) {
1413: 	return winMbcsToUtf8(zText, useAnsi);
1414: }
1415: 
1416: SQLITE_API char *sqlite3_win32_unicode_to_utf8(LPCWSTR zWideText) {
1417: 	return winUnicodeToUtf8(zWideText);
1418: }
1419: 
1420: #endif
1421: 
1422: // TODO complain
1423: SQLITE_API void sqlite3_result_blob(sqlite3_context *context, const void *blob, int n_bytes, void (*)(void *)) {
1424: 	if (!blob) {
1425: 		context->isError = SQLITE_MISUSE;
1426: 		return;
1427: 	}
1428: 	context->result.type = SQLiteTypeValue::BLOB;
1429: 	context->result.n = n_bytes;
1430: 	string_t str = string_t((const char *)blob, n_bytes);
1431: 	context->result.str_t = str;
1432: }
1433: 
1434: SQLITE_API void sqlite3_result_blob64(sqlite3_context *, const void *, sqlite3_uint64, void (*)(void *)) {
1435: }
1436: 
1437: SQLITE_API void sqlite3_result_double(sqlite3_context *context, double val) {
1438: 	context->result.u.r = val;
1439: 	context->result.type = SQLiteTypeValue::FLOAT;
1440: }
1441: 
1442: SQLITE_API void sqlite3_result_error(sqlite3_context *context, const char *msg, int n_bytes) {
1443: 	context->isError = SQLITE_ERROR;
1444: 	sqlite3_result_text(context, msg, n_bytes, nullptr);
1445: }
1446: 
1447: SQLITE_API void sqlite3_result_error16(sqlite3_context *, const void *, int) {
1448: }
1449: 
1450: SQLITE_API void sqlite3_result_error_toobig(sqlite3_context *) {
1451: }
1452: 
1453: SQLITE_API void sqlite3_result_error_nomem(sqlite3_context *) {
1454: }
1455: 
1456: SQLITE_API void sqlite3_result_error_code(sqlite3_context *, int) {
1457: }
1458: 
1459: SQLITE_API void sqlite3_result_int(sqlite3_context *context, int val) {
1460: 	sqlite3_result_int64(context, val);
1461: }
1462: 
1463: SQLITE_API void sqlite3_result_int64(sqlite3_context *context, sqlite3_int64 val) {
1464: 	context->result.u.i = val;
1465: 	context->result.type = SQLiteTypeValue::INTEGER;
1466: }
1467: 
1468: SQLITE_API void sqlite3_result_null(sqlite3_context *context) {
1469: 	context->result.type = SQLiteTypeValue::NULL_VALUE;
1470: }
1471: 
1472: SQLITE_API void sqlite3_result_text(sqlite3_context *context, const char *str_c, int n_chars, void (*)(void *)) {
1473: 	if (!str_c) {
1474: 		context->isError = SQLITE_MISUSE;
1475: 		return;
1476: 	}
1477: 
1478: 	auto utf_type = Utf8Proc::Analyze(str_c, n_chars);
1479: 	if (utf_type == UnicodeType::INVALID) {
1480: 		context->isError = SQLITE_MISUSE;
1481: 		return;
1482: 	}
1483: 	context->result.type = SQLiteTypeValue::TEXT;
1484: 	context->result.n = n_chars;
1485: 	context->result.str_t = string_t(str_c, n_chars);
1486: }
1487: 
1488: SQLITE_API void sqlite3_result_text64(sqlite3_context *, const char *, sqlite3_uint64, void (*)(void *),
1489:                                       unsigned char encoding) {
1490: }
1491: 
1492: SQLITE_API void sqlite3_result_text16(sqlite3_context *, const void *, int, void (*)(void *)) {
1493: }
1494: 
1495: SQLITE_API void sqlite3_result_text16le(sqlite3_context *, const void *, int, void (*)(void *)) {
1496: }
1497: 
1498: SQLITE_API void sqlite3_result_text16be(sqlite3_context *, const void *, int, void (*)(void *)) {
1499: }
1500: 
1501: SQLITE_API void sqlite3_result_value(sqlite3_context *, sqlite3_value *) {
1502: }
1503: 
1504: SQLITE_API void sqlite3_result_pointer(sqlite3_context *, void *, const char *, void (*)(void *)) {
1505: }
1506: 
1507: SQLITE_API void sqlite3_result_zeroblob(sqlite3_context *, int n) {
1508: }
1509: 
1510: SQLITE_API int sqlite3_result_zeroblob64(sqlite3_context *, sqlite3_uint64 n) {
1511: 	return -1;
1512: }
1513: 
1514: // TODO complain
1515: const void *sqlite3_value_blob(sqlite3_value *pVal) {
1516: 	return sqlite3_value_text(pVal);
1517: }
1518: 
1519: double sqlite3_value_double(sqlite3_value *pVal) {
1520: 	if (!pVal) {
1521: 		pVal->db->errCode = SQLITE_MISUSE;
1522: 		return 0.0;
1523: 	}
1524: 	switch (pVal->type) {
1525: 	case SQLiteTypeValue::FLOAT:
1526: 		return pVal->u.r;
1527: 	case SQLiteTypeValue::INTEGER:
1528: 		return (double)pVal->u.i;
1529: 	case SQLiteTypeValue::TEXT:
1530: 	case SQLiteTypeValue::BLOB:
1531: 		double res;
1532: 		if (TryCast::Operation<string_t, double>(pVal->str_t, res)) {
1533: 			return res;
1534: 		}
1535: 		break;
1536: 	default:
1537: 		break;
1538: 	}
1539: 	pVal->db->errCode = SQLITE_MISMATCH;
1540: 	return 0.0;
1541: }
1542: 
1543: int sqlite3_value_int(sqlite3_value *pVal) {
1544: 	int64_t res = sqlite3_value_int64(pVal);
1545: 	if (res >= NumericLimits<int>::Minimum() && res <= NumericLimits<int>::Maximum()) {
1546: 		return res;
1547: 	}
1548: 	pVal->db->errCode = SQLITE_MISMATCH;
1549: 	return 0;
1550: }
1551: 
1552: sqlite3_int64 sqlite3_value_int64(sqlite3_value *pVal) {
1553: 	if (!pVal) {
1554: 		pVal->db->errCode = SQLITE_MISUSE;
1555: 		return 0;
1556: 	}
1557: 	int64_t res;
1558: 	switch (pVal->type) {
1559: 	case SQLiteTypeValue::INTEGER:
1560: 		return pVal->u.i;
1561: 	case SQLiteTypeValue::FLOAT:
1562: 		if (TryCast::Operation<double, int64_t>(pVal->u.r, res)) {
1563: 			return res;
1564: 		}
1565: 		break;
1566: 	case SQLiteTypeValue::TEXT:
1567: 	case SQLiteTypeValue::BLOB:
1568: 		if (TryCast::Operation<string_t, int64_t>(pVal->str_t, res)) {
1569: 			return res;
1570: 		}
1571: 		break;
1572: 	default:
1573: 		break;
1574: 	}
1575: 	pVal->db->errCode = SQLITE_MISMATCH;
1576: 	return 0;
1577: }
1578: 
1579: void *sqlite3_value_pointer(sqlite3_value *, const char *) {
1580: 	return nullptr;
1581: }
1582: 
1583: const unsigned char *sqlite3_value_text(sqlite3_value *pVal) {
1584: 	if (!pVal) {
1585: 		pVal->db->errCode = SQLITE_MISUSE;
1586: 		return nullptr;
1587: 	}
1588: 	// check if the string has already been allocated
1589: 	if (pVal->szMalloc > 0) {
1590: 		return (const unsigned char *)pVal->zMalloc;
1591: 	}
1592: 
1593: 	if (pVal->type == SQLiteTypeValue::TEXT || pVal->type == SQLiteTypeValue::BLOB) {
1594: 		auto length = pVal->str_t.GetSize();
1595: 		// new string including space for the null-terminated char ('\0')
1596: 		pVal->zMalloc = (char *)malloc(sizeof(char) * length + 1);
1597: 		if (!pVal->zMalloc) {
1598: 			pVal->db->errCode = SQLITE_NOMEM;
1599: 			return nullptr;
1600: 		}
1601: 		pVal->szMalloc = length + 1;
1602: 		memcpy(pVal->zMalloc, pVal->str_t.GetDataUnsafe(), length);
1603: 		pVal->zMalloc[length] = '\0';
1604: 		return (const unsigned char *)pVal->zMalloc;
1605: 	}
1606: 
1607: 	if (pVal->type == SQLiteTypeValue::INTEGER || pVal->type == SQLiteTypeValue::FLOAT) {
1608: 		Value value = (pVal->type == SQLiteTypeValue::INTEGER) ? Value::BIGINT(pVal->u.i) : Value::DOUBLE(pVal->u.r);
1609: 		if (value.TryCastAs(LogicalType::VARCHAR) == false) {
1610: 			pVal->db->errCode = SQLITE_NOMEM;
1611: 			return nullptr;
1612: 		}
1613: 		size_t str_len = value.str_value.size();
1614: 		pVal->zMalloc = (char *)malloc(sizeof(char) * (str_len + 1));
1615: 		if (!pVal->zMalloc) {
1616: 			pVal->db->errCode = SQLITE_NOMEM;
1617: 			return nullptr;
1618: 		}
1619: 		pVal->szMalloc = str_len + 1; // +1 null-terminated char
1620: 		memcpy(pVal->zMalloc, value.str_value.c_str(), pVal->szMalloc);
1621: 
1622: 		pVal->str_t = string_t(pVal->zMalloc, pVal->szMalloc - 1); // -1 null-terminated char
1623: 		pVal->n = pVal->str_t.GetSize();
1624: 		pVal->type = SQLiteTypeValue::TEXT;
1625: 		return (const unsigned char *)pVal->zMalloc;
1626: 	}
1627: 	if (pVal->type == SQLiteTypeValue::NULL_VALUE) {
1628: 		return nullptr;
1629: 	}
1630: 	pVal->db->errCode = SQLITE_MISMATCH;
1631: 	return nullptr;
1632: }
1633: 
1634: SQLITE_API const void *sqlite3_value_text16(sqlite3_value *) {
1635: 	return nullptr;
1636: }
1637: 
1638: SQLITE_API const void *sqlite3_value_text16le(sqlite3_value *) {
1639: 	return nullptr;
1640: }
1641: 
1642: SQLITE_API const void *sqlite3_value_text16be(sqlite3_value *) {
1643: 	return nullptr;
1644: }
1645: 
1646: SQLITE_API int sqlite3_value_bytes(sqlite3_value *pVal) {
1647: 	if (pVal->type == SQLiteTypeValue::TEXT || pVal->type == SQLiteTypeValue::BLOB) {
1648: 		return pVal->n;
1649: 	}
1650: 	return 0;
1651: }
1652: 
1653: SQLITE_API int sqlite3_value_bytes16(sqlite3_value *) {
1654: 	return 0;
1655: }
1656: 
1657: SQLITE_API int sqlite3_value_type(sqlite3_value *pVal) {
1658: 	return (int)pVal->type;
1659: }
1660: 
1661: SQLITE_API int sqlite3_value_numeric_type(sqlite3_value *) {
1662: 	return 0;
1663: }
1664: 
1665: SQLITE_API int sqlite3_value_nochange(sqlite3_value *) {
1666: 	return 0;
1667: }
1668: 
1669: SQLITE_API void *sqlite3_aggregate_context(sqlite3_context *, int nBytes) {
1670: 	fprintf(stderr, "sqlite3_aggregate_context: unsupported.\n");
1671: 
1672: 	return nullptr;
1673: }
1674: 
1675: SQLITE_API int sqlite3_create_collation(sqlite3 *, const char *zName, int eTextRep, void *pArg,
1676:                                         int (*xCompare)(void *, int, const void *, int, const void *)) {
1677: 	return SQLITE_ERROR;
1678: }
1679: 
1680: SQLITE_API int sqlite3_create_window_function(sqlite3 *db, const char *zFunctionName, int nArg, int eTextRep,
1681:                                               void *pApp, void (*xStep)(sqlite3_context *, int, sqlite3_value **),
1682:                                               void (*xFinal)(sqlite3_context *), void (*xValue)(sqlite3_context *),
1683:                                               void (*xInverse)(sqlite3_context *, int, sqlite3_value **),
1684:                                               void (*xDestroy)(void *)) {
1685: 	// commented for now because such error message prevents the shell-test.py to pass
1686: 	//	fprintf(stderr, "sqlite3_create_window_function: unsupported.\n");
1687: 	return SQLITE_ERROR;
1688: }
1689: 
1690: SQLITE_API sqlite3 *sqlite3_db_handle(sqlite3_stmt *s) {
1691: 	return s->db;
1692: }
1693: 
1694: SQLITE_API char *sqlite3_expanded_sql(sqlite3_stmt *pStmt) {
1695: 	fprintf(stderr, "sqlite3_expanded_sql: unsupported.\n");
1696: 	return nullptr;
1697: }
1698: 
1699: SQLITE_API int sqlite3_keyword_check(const char *str, int len) {
1700: 	return Parser::IsKeyword(std::string(str, len));
1701: }
1702: 
1703: SQLITE_API int sqlite3_keyword_count(void) {
1704: 	fprintf(stderr, "sqlite3_keyword_count: unsupported.\n");
1705: 	return 0;
1706: }
1707: 
1708: SQLITE_API int sqlite3_keyword_name(int, const char **, int *) {
1709: 	fprintf(stderr, "sqlite3_keyword_name: unsupported.\n");
1710: 	return 0;
1711: }
1712: 
1713: SQLITE_API void sqlite3_progress_handler(sqlite3 *, int, int (*)(void *), void *) {
1714: 	fprintf(stderr, "sqlite3_progress_handler: unsupported.\n");
1715: }
1716: 
1717: SQLITE_API int sqlite3_stmt_isexplain(sqlite3_stmt *pStmt) {
1718: 	if (!pStmt || !pStmt->prepared) {
1719: 		return 0;
1720: 	}
1721: 	return pStmt->prepared->GetStatementType() == StatementType::EXPLAIN_STATEMENT;
1722: }
1723: 
1724: SQLITE_API int sqlite3_vtab_config(sqlite3 *, int op, ...) {
1725: 	fprintf(stderr, "sqlite3_vtab_config: unsupported.\n");
1726: 	return SQLITE_ERROR;
1727: }
1728: 
1729: SQLITE_API int sqlite3_busy_handler(sqlite3 *, int (*)(void *, int), void *) {
1730: 	return SQLITE_ERROR;
1731: }
1732: 
1733: SQLITE_API int sqlite3_get_table(sqlite3 *db,       /* An open database */
1734:                                  const char *zSql,  /* SQL to be evaluated */
1735:                                  char ***pazResult, /* Results of the query */
1736:                                  int *pnRow,        /* Number of result rows written here */
1737:                                  int *pnColumn,     /* Number of result columns written here */
1738:                                  char **pzErrmsg    /* Error msg written here */
1739: ) {
1740: 	fprintf(stderr, "sqlite3_get_table: unsupported.\n");
1741: 	return SQLITE_ERROR;
1742: }
1743: 
1744: SQLITE_API void sqlite3_free_table(char **result) {
1745: 	fprintf(stderr, "sqlite3_free_table: unsupported.\n");
1746: }
1747: 
1748: SQLITE_API int sqlite3_prepare(sqlite3 *db,           /* Database handle */
1749:                                const char *zSql,      /* SQL statement, UTF-8 encoded */
1750:                                int nByte,             /* Maximum length of zSql in bytes. */
1751:                                sqlite3_stmt **ppStmt, /* OUT: Statement handle */
1752:                                const char **pzTail    /* OUT: Pointer to unused portion of zSql */
1753: ) {
1754: 	return sqlite3_prepare_v2(db, zSql, nByte, ppStmt, pzTail);
1755: }
1756: 
1757: SQLITE_API void *sqlite3_trace(sqlite3 *, void (*xTrace)(void *, const char *), void *) {
1758: 	fprintf(stderr, "sqlite3_trace: unsupported.\n");
1759: 	return nullptr;
1760: }
1761: 
1762: SQLITE_API void *sqlite3_profile(sqlite3 *, void (*xProfile)(void *, const char *, sqlite3_uint64), void *) {
1763: 	fprintf(stderr, "sqlite3_profile: unsupported.\n");
1764: 	return nullptr;
1765: }
1766: 
1767: SQLITE_API int sqlite3_libversion_number(void) {
1768: 	return SQLITE_VERSION_NUMBER;
1769: }
1770: 
1771: SQLITE_API int sqlite3_threadsafe(void) {
1772: 	return SQLITE_OK;
1773: }
1774: 
1775: SQLITE_API sqlite3_mutex *sqlite3_mutex_alloc(int) {
1776: 	fprintf(stderr, "sqlite3_mutex_alloc: unsupported.\n");
1777: 	return nullptr;
1778: }
1779: 
1780: SQLITE_API void sqlite3_mutex_free(sqlite3_mutex *) {
1781: 	fprintf(stderr, "sqlite3_mutex_free: unsupported.\n");
1782: }
1783: 
1784: SQLITE_API int sqlite3_extended_result_codes(sqlite3 *db, int onoff) {
1785: 	fprintf(stderr, "sqlite3_extended_result_codes: unsupported.\n");
1786: 	return SQLITE_ERROR;
1787: }
1788: 
1789: SQLITE_API void *sqlite3_update_hook(sqlite3 *db, /* Attach the hook to this database */
1790:                                      void (*xCallback)(void *, int, char const *, char const *, sqlite_int64),
1791:                                      void *pArg /* Argument to the function */
1792: ) {
1793: 	fprintf(stderr, "sqlite3_update_hook: unsupported.\n");
1794: 	return nullptr;
1795: }
1796: 
1797: SQLITE_API void sqlite3_log(int iErrCode, const char *zFormat, ...) {
1798: 	fprintf(stderr, "sqlite3_log: unsupported.\n");
1799: }
1800: 
1801: SQLITE_API int sqlite3_unlock_notify(sqlite3 *db, void (*xNotify)(void **, int), void *pArg) {
1802: 	fprintf(stderr, "sqlite3_unlock_notify: unsupported.\n");
1803: 	return SQLITE_ERROR;
1804: }
1805: 
1806: SQLITE_API void *sqlite3_get_auxdata(sqlite3_context *pCtx, int iArg) {
1807: 	fprintf(stderr, "sqlite3_get_auxdata: unsupported.\n");
1808: 	return nullptr;
1809: }
1810: 
1811: SQLITE_API void *sqlite3_rollback_hook(sqlite3 *db,               /* Attach the hook to this database */
1812:                                        void (*xCallback)(void *), /* Callback function */
1813:                                        void *pArg                 /* Argument to the function */
1814: ) {
1815: 	fprintf(stderr, "sqlite3_rollback_hook: unsupported.\n");
1816: 	return nullptr;
1817: }
1818: 
1819: SQLITE_API void *sqlite3_commit_hook(sqlite3 *db,              /* Attach the hook to this database */
1820:                                      int (*xCallback)(void *), /* Function to invoke on each commit */
1821:                                      void *pArg                /* Argument to the function */
1822: ) {
1823: 	fprintf(stderr, "sqlite3_commit_hook: unsupported.\n");
1824: 	return nullptr;
1825: }
1826: 
1827: SQLITE_API int sqlite3_blob_open(sqlite3 *db,          /* The database connection */
1828:                                  const char *zDb,      /* The attached database containing the blob */
1829:                                  const char *zTable,   /* The table containing the blob */
1830:                                  const char *zColumn,  /* The column containing the blob */
1831:                                  sqlite_int64 iRow,    /* The row containing the glob */
1832:                                  int wrFlag,           /* True -> read/write access, false -> read-only */
1833:                                  sqlite3_blob **ppBlob /* Handle for accessing the blob returned here */
1834: ) {
1835: 	fprintf(stderr, "sqlite3_blob_open: unsupported.\n");
1836: 	return SQLITE_ERROR;
1837: }
1838: 
1839: SQLITE_API const char *sqlite3_db_filename(sqlite3 *db, const char *zDbName) {
1840: 	fprintf(stderr, "sqlite3_db_filename: unsupported.\n");
1841: 	return nullptr;
1842: }
1843: 
1844: SQLITE_API int sqlite3_stmt_busy(sqlite3_stmt *) {
1845: 	fprintf(stderr, "sqlite3_stmt_busy: unsupported.\n");
1846: 	return false;
1847: }
1848: 
1849: SQLITE_API int sqlite3_bind_pointer(sqlite3_stmt *pStmt, int i, void *pPtr, const char *zPTtype,
1850:                                     void (*xDestructor)(void *)) {
1851: 	fprintf(stderr, "sqlite3_bind_pointer: unsupported.\n");
1852: 	return SQLITE_ERROR;
1853: }
1854: 
1855: SQLITE_API int sqlite3_create_module_v2(sqlite3 *db,                   /* Database in which module is registered */
1856:                                         const char *zName,             /* Name assigned to this module */
1857:                                         const sqlite3_module *pModule, /* The definition of the module */
1858:                                         void *pAux,                    /* Context pointer for xCreate/xConnect */
1859:                                         void (*xDestroy)(void *)       /* Module destructor function */
1860: ) {
1861: 	fprintf(stderr, "sqlite3_create_module_v2: unsupported.\n");
1862: 	return SQLITE_ERROR;
1863: }
1864: 
1865: SQLITE_API int sqlite3_blob_write(sqlite3_blob *, const void *z, int n, int iOffset) {
1866: 	fprintf(stderr, "sqlite3_blob_write: unsupported.\n");
1867: 	return SQLITE_ERROR;
1868: }
1869: 
1870: SQLITE_API void sqlite3_set_auxdata(sqlite3_context *, int N, void *, void (*)(void *)) {
1871: 	fprintf(stderr, "sqlite3_set_auxdata: unsupported.\n");
1872: }
1873: 
1874: SQLITE_API sqlite3_stmt *sqlite3_next_stmt(sqlite3 *pDb, sqlite3_stmt *pStmt) {
1875: 	fprintf(stderr, "sqlite3_next_stmt: unsupported.\n");
1876: 	return nullptr;
1877: }
1878: 
1879: SQLITE_API int sqlite3_collation_needed(sqlite3 *, void *, void (*)(void *, sqlite3 *, int eTextRep, const char *)) {
1880: 	fprintf(stderr, "sqlite3_collation_needed: unsupported.\n");
1881: 	return SQLITE_ERROR;
1882: }
1883: 
1884: SQLITE_API int sqlite3_create_collation_v2(sqlite3 *, const char *zName, int eTextRep, void *pArg,
1885:                                            int (*xCompare)(void *, int, const void *, int, const void *),
1886:                                            void (*xDestroy)(void *)) {
1887: 	fprintf(stderr, "sqlite3_create_collation_v2: unsupported.\n");
1888: 	return SQLITE_ERROR;
1889: }
[end of tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: