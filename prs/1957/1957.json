{
  "repo": "duckdb/duckdb",
  "pull_number": 1957,
  "instance_id": "duckdb__duckdb-1957",
  "issue_numbers": [
    "1417"
  ],
  "base_commit": "7697a08531afa375086827c72ea55673438faaa7",
  "patch": "diff --git a/src/common/CMakeLists.txt b/src/common/CMakeLists.txt\nindex bb0ecad28593..3157188e6ec0 100644\n--- a/src/common/CMakeLists.txt\n+++ b/src/common/CMakeLists.txt\n@@ -21,6 +21,7 @@ add_library_unity(\n   file_buffer.cpp\n   file_system.cpp\n   gzip_file_system.cpp\n+  pipe_file_system.cpp\n   limits.cpp\n   printer.cpp\n   progress_bar.cpp\ndiff --git a/src/common/file_system.cpp b/src/common/file_system.cpp\nindex 52b61dd16fda..628318748afc 100644\n--- a/src/common/file_system.cpp\n+++ b/src/common/file_system.cpp\n@@ -3,6 +3,7 @@\n #include \"duckdb/common/checksum.hpp\"\n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/gzip_file_system.hpp\"\n+#include \"duckdb/common/pipe_file_system.hpp\"\n #include \"duckdb/common/helper.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/function/scalar/string_functions.hpp\"\n@@ -227,6 +228,32 @@ time_t FileSystem::GetLastModifiedTime(FileHandle &handle) {\n \treturn s.st_mtime;\n }\n \n+FileType FileSystem::GetFileType(FileHandle &handle) {\n+\tint fd = ((UnixFileHandle &)handle).fd;\n+\tstruct stat s;\n+\tif (fstat(fd, &s) == -1) {\n+\t\treturn FileType::FILE_TYPE_INVALID;\n+\t}\n+\tswitch (s.st_mode & S_IFMT) {\n+\tcase S_IFBLK:\n+\t\treturn FileType::FILE_TYPE_BLOCKDEV;\n+\tcase S_IFCHR:\n+\t\treturn FileType::FILE_TYPE_CHARDEV;\n+\tcase S_IFIFO:\n+\t\treturn FileType::FILE_TYPE_FIFO;\n+\tcase S_IFDIR:\n+\t\treturn FileType::FILE_TYPE_DIR;\n+\tcase S_IFLNK:\n+\t\treturn FileType::FILE_TYPE_LINK;\n+\tcase S_IFREG:\n+\t\treturn FileType::FILE_TYPE_REGULAR;\n+\tcase S_IFSOCK:\n+\t\treturn FileType::FILE_TYPE_SOCKET;\n+\tdefault:\n+\t\treturn FileType::FILE_TYPE_INVALID;\n+\t}\n+}\n+\n void FileSystem::Truncate(FileHandle &handle, int64_t new_size) {\n \tint fd = ((UnixFileHandle &)handle).fd;\n \tif (ftruncate(fd, new_size) != 0) {\n@@ -402,6 +429,8 @@ string FileSystem::GetWorkingDirectory() {\n }\n #else\n \n+constexpr char PIPE_PREFIX[] = \"\\\\\\\\.\\\\pipe\\\\\";\n+\n // Returns the last Win32 error, in string format. Returns an empty string if there is no error.\n std::string GetLastErrorAsString() {\n \t// Get the error message, if any.\n@@ -765,6 +794,23 @@ string FileSystem::GetWorkingDirectory() {\n \t}\n \treturn string(buffer.get(), ret);\n }\n+\n+FileType FileSystem::GetFileType(FileHandle &handle) {\n+\tauto path = ((WindowsFileHandle &)handle).path;\n+\t// pipes in windows are just files in '\\\\.\\pipe\\' folder\n+\tif (strncmp(path.c_str(), PIPE_PREFIX, strlen(PIPE_PREFIX)) == 0) {\n+\t\treturn FileType::FILE_TYPE_FIFO;\n+\t}\n+\tDWORD attrs = GetFileAttributesA(path.c_str());\n+\tif (attrs != INVALID_FILE_ATTRIBUTES) {\n+\t\tif (attrs & FILE_ATTRIBUTE_DIRECTORY) {\n+\t\t\treturn FileType::FILE_TYPE_DIR;\n+\t\t} else {\n+\t\t\treturn FileType::FILE_TYPE_REGULAR;\n+\t\t}\n+\t}\n+\treturn FileType::FILE_TYPE_INVALID;\n+}\n #endif\n \n string FileSystem::GetHomeDirectory() {\n@@ -891,6 +937,10 @@ void FileHandle::Truncate(int64_t new_size) {\n \tfile_system.Truncate(*this, new_size);\n }\n \n+FileType FileHandle::GetType() {\n+\treturn file_system.GetFileType(*this);\n+}\n+\n static bool HasGlob(const string &str) {\n \tfor (idx_t i = 0; i < str.size(); i++) {\n \t\tswitch (str[i]) {\n@@ -1022,7 +1072,9 @@ unique_ptr<FileHandle> VirtualFileSystem::OpenFile(const string &path, uint8_t f\n \t}\n \t// open the base file handle\n \tauto file_handle = FindFileSystem(path)->OpenFile(path, flags, lock, FileCompressionType::UNCOMPRESSED);\n-\tif (compression != FileCompressionType::UNCOMPRESSED) {\n+\tif (file_handle->GetType() == FileType::FILE_TYPE_FIFO) {\n+\t\tfile_handle = PipeFileSystem::OpenPipe(move(file_handle));\n+\t} else if (compression != FileCompressionType::UNCOMPRESSED) {\n \t\tswitch (compression) {\n \t\tcase FileCompressionType::GZIP:\n \t\t\tfile_handle = GZipFileSystem::OpenCompressedFile(move(file_handle));\ndiff --git a/src/common/pipe_file_system.cpp b/src/common/pipe_file_system.cpp\nnew file mode 100644\nindex 000000000000..5e837c66c112\n--- /dev/null\n+++ b/src/common/pipe_file_system.cpp\n@@ -0,0 +1,73 @@\n+#include \"duckdb/common/pipe_file_system.hpp\"\n+#include \"duckdb/common/exception.hpp\"\n+#include \"duckdb/common/file_system.hpp\"\n+\n+namespace duckdb {\n+class PipeFile : public FileHandle {\n+public:\n+\tPipeFile(unique_ptr<FileHandle> child_handle_p, const string &path)\n+\t    : FileHandle(pipe_fs, path), child_handle(move(child_handle_p)) {\n+\t}\n+\n+\tint64_t ReadChunk(void *buffer, int64_t nr_bytes);\n+\tint64_t WriteChunk(void *buffer, int64_t nr_bytes);\n+\n+\tPipeFileSystem pipe_fs;\n+\tunique_ptr<FileHandle> child_handle;\n+\n+protected:\n+\tvoid Close() override {\n+\t}\n+};\n+\n+int64_t PipeFile::ReadChunk(void *buffer, int64_t nr_bytes) {\n+\treturn child_handle->Read(buffer, nr_bytes);\n+}\n+int64_t PipeFile::WriteChunk(void *buffer, int64_t nr_bytes) {\n+\treturn child_handle->Write(buffer, nr_bytes);\n+}\n+\n+void PipeFileSystem::Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) {\n+\tthrow NotImplementedException(\"Unsupported: Random read from pipe/stream\");\n+}\n+\n+int64_t PipeFileSystem::Read(FileHandle &handle, void *buffer, int64_t nr_bytes) {\n+\tauto &pipe = (PipeFile &)handle;\n+\treturn pipe.ReadChunk(buffer, nr_bytes);\n+}\n+\n+void PipeFileSystem::Write(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) {\n+\tthrow NotImplementedException(\"Unsupported: Random write to pipe/stream\");\n+}\n+\n+int64_t PipeFileSystem::Write(FileHandle &handle, void *buffer, int64_t nr_bytes) {\n+\tauto &pipe = (PipeFile &)handle;\n+\treturn pipe.WriteChunk(buffer, nr_bytes);\n+}\n+\n+void PipeFileSystem::Truncate(FileHandle &handle, int64_t new_size) {\n+\tthrow NotImplementedException(\"Unsupported: Truncate pipe/stream\");\n+}\n+\n+void PipeFileSystem::FileSync(FileHandle &handle) {\n+\tthrow NotImplementedException(\"Unsupported: Sync pipe/stream\");\n+}\n+\n+void PipeFileSystem::Seek(FileHandle &handle, idx_t location) {\n+\tthrow NotImplementedException(\"Unsupported: Seek within pipe/stream\");\n+}\n+\n+void PipeFileSystem::Reset(FileHandle &handle) {\n+\tthrow NotImplementedException(\"Unsupported: Reset pipe/stream\");\n+}\n+\n+int64_t PipeFileSystem::GetFileSize(FileHandle &handle) {\n+\treturn 0;\n+}\n+\n+unique_ptr<FileHandle> PipeFileSystem::OpenPipe(unique_ptr<FileHandle> handle) {\n+\tauto path = handle->path;\n+\treturn make_unique<PipeFile>(move(handle), path);\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/file_system.hpp b/src/include/duckdb/common/file_system.hpp\nindex 42b56a3b3d0b..5aaa179bd5e0 100644\n--- a/src/include/duckdb/common/file_system.hpp\n+++ b/src/include/duckdb/common/file_system.hpp\n@@ -26,6 +26,25 @@ class ClientContext;\n class DatabaseInstance;\n class FileSystem;\n \n+enum class FileType {\n+\t//! Regular file\n+\tFILE_TYPE_REGULAR,\n+\t//! Directory\n+\tFILE_TYPE_DIR,\n+\t//! FIFO named pipe\n+\tFILE_TYPE_FIFO,\n+\t//! Socket\n+\tFILE_TYPE_SOCKET,\n+\t//! Symbolic link\n+\tFILE_TYPE_LINK,\n+\t//! Block device\n+\tFILE_TYPE_BLOCKDEV,\n+\t//! Character device\n+\tFILE_TYPE_CHARDEV,\n+\t//! Unknown or invalid file handle\n+\tFILE_TYPE_INVALID,\n+};\n+\n struct FileHandle {\n public:\n \tFileHandle(FileSystem &file_system, string path) : file_system(file_system), path(path) {\n@@ -48,6 +67,7 @@ struct FileHandle {\n \tbool CanSeek();\n \tbool OnDiskFile();\n \tidx_t GetFileSize();\n+\tFileType GetType();\n \n protected:\n \tvirtual void Close() = 0;\n@@ -104,6 +124,8 @@ class FileSystem {\n \tvirtual int64_t GetFileSize(FileHandle &handle);\n \t//! Returns the file last modified time of a file handle, returns timespec with zero on all attributes on error\n \tvirtual time_t GetLastModifiedTime(FileHandle &handle);\n+\t//! Returns the file last modified time of a file handle, returns timespec with zero on all attributes on error\n+\tvirtual FileType GetFileType(FileHandle &handle);\n \t//! Truncate a file to a maximum size of new_size, new_size should be smaller than or equal to the current size of\n \t//! the file\n \tvirtual void Truncate(FileHandle &handle, int64_t new_size);\n@@ -203,6 +225,9 @@ class VirtualFileSystem : public FileSystem {\n \ttime_t GetLastModifiedTime(FileHandle &handle) override {\n \t\treturn handle.file_system.GetLastModifiedTime(handle);\n \t}\n+\tFileType GetFileType(FileHandle &handle) override {\n+\t\treturn handle.file_system.GetFileType(handle);\n+\t}\n \n \tvoid Truncate(FileHandle &handle, int64_t new_size) override {\n \t\thandle.file_system.Truncate(handle, new_size);\ndiff --git a/src/include/duckdb/common/pipe_file_system.hpp b/src/include/duckdb/common/pipe_file_system.hpp\nnew file mode 100644\nindex 000000000000..f0161b08f844\n--- /dev/null\n+++ b/src/include/duckdb/common/pipe_file_system.hpp\n@@ -0,0 +1,40 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/pipe_file_system.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/file_system.hpp\"\n+\n+namespace duckdb {\n+\n+class PipeFileSystem : public FileSystem {\n+public:\n+\tstatic unique_ptr<FileHandle> OpenPipe(unique_ptr<FileHandle> handle);\n+\n+\tvoid Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n+\tint64_t Read(FileHandle &handle, void *buffer, int64_t nr_bytes) override;\n+\tvoid Write(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n+\tint64_t Write(FileHandle &handle, void *buffer, int64_t nr_bytes) override;\n+\n+\t// unsupported operations\n+\tvoid Truncate(FileHandle &handle, int64_t new_size) override;\n+\tvoid FileSync(FileHandle &handle) override;\n+\tvoid Seek(FileHandle &handle, idx_t location) override;\n+\tvoid Reset(FileHandle &handle) override;\n+\n+\tint64_t GetFileSize(FileHandle &handle) override;\n+\n+\tbool OnDiskFile(FileHandle &handle) override {\n+\t\treturn false;\n+\t};\n+\tbool CanSeek() override {\n+\t\treturn false;\n+\t}\n+};\n+\n+} // namespace duckdb\n",
  "test_patch": "diff --git a/tools/shell/shell-test.py b/tools/shell/shell-test.py\nindex 0995c9c4ba57..972f07435649 100644\n--- a/tools/shell/shell-test.py\n+++ b/tools/shell/shell-test.py\n@@ -18,11 +18,15 @@ def test_exception(command, input, stdout, stderr, errmsg):\n      print(stderr)\n      raise Exception(errmsg)\n \n-def test(cmd, out=None, err=None, extra_commands=None):\n+def test(cmd, out=None, err=None, extra_commands=None, input_file=None):\n      command = [sys.argv[1], '--batch', '-init', '/dev/null']\n      if extra_commands:\n           command += extra_commands\n-     res = subprocess.run(command, capture_output=True, input=bytearray(cmd, 'utf8'))\n+     if input_file:\n+          command += [cmd]\n+          res = subprocess.run(command, capture_output=True, input=open(input_file, 'rb').read())\n+     else:\n+          res = subprocess.run(command, capture_output=True, input=bytearray(cmd, 'utf8'))\n      stdout = res.stdout.decode('utf8').strip()\n      stderr = res.stderr.decode('utf8').strip()\n \n@@ -517,3 +521,17 @@ def tf():\n ''', out='42')\n \n test('/* ;;;;;; */ select 42;', out='42')\n+\n+if os.name != 'nt':\n+     test('''\n+     create table mytable as select * from\n+     read_csv('/dev/stdin',\n+       columns=STRUCT_PACK(foo := 'INTEGER', bar := 'INTEGER', baz := 'VARCHAR'),\n+       AUTO_DETECT='false'\n+     );\n+     select * from mytable limit 1;\n+     ''',\n+     extra_commands=['-csv', ':memory:'],\n+     input_file='test/sql/copy/csv/data/test/test.csv',\n+     out='''foo,bar,baz\n+0,0,\" test\"''')\n",
  "problem_statement": "Feature: support read_csv_auto reading from the '/dev/stdin' file\nThe duckdb CLI command can read from a file using read_csv_auto():\r\n\r\n```bash\r\nduckdb mydb \"create table mytable as select * from read_csv_auto('mydata.csv', HEADER=TRUE);\"\r\n```\r\n\r\nWhen I try to send the same data to the /dev/stdin file and have duckdb load it from there, it fails:\r\n\r\n```bash\r\ncat mydata.csv | duckdb mydb \"create table mytable as select * from read_csv_auto('/dev/stdin', HEADER=TRUE);\" \r\n```\r\n\r\nThe error is:\r\n\r\n```bash\r\nError: Invalid Input Error: Error in file \"/dev/stdin\": CSV options could not be auto-detected. Consider setting parser options manually.\r\n```\n",
  "hints_text": "Maybe @tdoehmen can  have a look?\nJust sharing some thoughts on the use cases. The duckdb CLI shell offers similar capabilities as sqlite3 for moving data in and out using both special \"dot commands\" and \"SQL\" command statements. Automating loading and export from the commandline, some of these so called dot-commands are supported: https://sqlite.org/cli.html... \r\n\r\nI think some common dot commands are `.import`, another is `.dump`, for reading and writing data in and out of the process. There is also `.output` (with different `.mode` settings). It seems those dot commands in sqlite3 have grown organically and there are now quite a few of these in sqlite3. In duckdb CLI shell, there seems to be support for `.import` and other dot commands but not a 1-to-1 match with all sqlite3 dot commands.\r\n\r\nIt seems there are faster and better duckdb-\"sql\"-statements for reading and writing data into and out from the duckdb process, such as maybe using:\r\n\r\n- \"create table if not exists stdin as select * from read_csv_auto('/dev/stdin')\" # for reading data \r\n- \"export database '/dev/stdout' (FORMAT PARQUET);\" # for writing data output (binary)\r\n- \"copy stdin to '/dev/stdout' (FORMAT CSV);\" # for writing data in a table called stdin to an output file (in text format)\r\n\r\nWhen reading and writing streams of data coming from stdin and going to stdout, it would be nice to have easy access to these files/streams, to use some of these faster duckdb specific import and export functions with less intermediate steps. It would be nice to not have to create various temp files and directories, unless required, for example due to larger data which cannot fit in RAM.\r\n\r\nThe dot commands can be piped to sqlite3, but not easily \"data\" so duckdb would have an edge if load and dump functions were to work with data from stdin, and results going to stdout. For sqlite3, to alleviate this, there are tools like https://tobimensch.github.io/termsql/ provided outside of sqlite3. Or shell wrappers can be used; here are some links that describe some usage/workarounds with sqlite3:\r\n  \r\n- https://unix.stackexchange.com/questions/288273/bash-redirection-doesnt-work-for-sqlite-command\r\n- https://stackoverflow.com/questions/41354486/sqlite-how-to-simultaneously-read-data-from-stdin-and-table-name-from-variable\r\n\r\nHere is a [rough attempt to do an R script](https://gist.github.com/mskyttner/46601e60138a886cdfbfc81067c71725) \"outside of duckdb shell/CLI\" which reads from stdin and writes to stdout. It is quite quick and dirty, just adding it to illustrate use cases that could be nice to support on the commandline, in my opinion, for example (after copying the script to ~/bin and doing chmod +x) ...\r\n\r\n```bash\r\ndf | tail -n +2 | duckstream.R --no-header -d \" \" -Q \"select X1,X2 from stdin;\" --dump myparquetoutput\r\n```\r\n\n@hannesmuehleisen I am not familiar with the cli integration of duckdb, so I only have a limited view on this. \r\nIn the csv reader, I could imagine that it's possible to treat `path=/dev/stdin` as magic value, which makes the parser read from stdin with something like `std::getline(std::cin,line)`. The parser would need to treat it similar to a .gz file (not the compression, but that data has to be read sequentially and that we can't jump to other positions in the input), which means that dialect and data type detection for CSV will only work to a limited extend. \r\n\r\nedit: one more thing - this would only work in conjunction with `create table` (not with views or anything else where the input could potentially be read more than once), unless the data from stdin gets buffered somewhere. Or am I missing something?\r\n\r\nedit 2: When it's only about conversion from CSV -> PARQUET/CSV, maybe it's worth offering this functionality through a dedicated API? \ud83e\udd14 \nBulk loading functionality through command line tools with input from stdin is supported in some databases, for example https://mariadb.com/kb/en/columnstore-bulk-data-loading/#bulking-loading-data-from-stdin. It would be pretty nice if that was built into the duckdb shell / CLI command instead of being provided \"on the outside\" by a separate utility. I guess that is what some of the dot commands in sqlite do, extend the shell with input/output features.\r\n\r\nThe use cases I had in mind when opening this feature wish was not only specifically about converting from CSV to PARQUET but also about being able to avoid reading data from or to disk altogether, like in the examples given here: https://github.com/tobimensch/termsql/#examples. But specifically for the example use case with exporting to parquet ... I admit that I use the CSV->duckdb->PARQUET a lot now, until the duckdb file format settles. \r\n\r\nWould it be possible to buffer the data from stdin in chunks into a \"buffer\" duckdb table (maybe single column, varchar), with chunkwise parse/move operations to final destination happening from there? First chunk would be a lookahead of n lines, used to guess data types, separators etc (if not specified), and all subsequent chunks of m lines each would be parsed with those settings?\r\n\r\nIf someone sends an endless stream (by mistake or as a Denial-of-Service attempt), it would be great to see some progress and finally warning / abort before system resources get depleted.\nThank you for the clarification and the information. Then, a more general solution would indeed make more sense.  \r\n\r\nMaybe buffering to a table could indeed save some headache around disk-spilling! It doesn't feel like a super clean solution (i'd prefer to buffer the raw data - if at all), but maybe it's a good starting point. One option to go without buffering would be to allow the use of stdin input only for specific queries which are guaranteed to need the input only once (e.g. COPY ... INTO). \r\n\r\nEndless input streams would, however, still remain a problem, thanks for the hint. I think that a line-break could generally be used to mark the end of the stream. Maybe an option like max_lines could be added to prevent endless reading from the stdin-stream if the line-break never comes. \r\n\r\nI can unfortunately not promise an immediate solution to this, as I am not familiar with the CLI/shell integration yet and also have to give the buffering aspect another thought.  But I will investigate and come back to you when I hopefully know more ; )\nI would highly value (low level) streaming in and out APIs for DuckDB. First, streaming in general speeds up data processing and would enable nice stream processing use cases, buffer as much as needed/wanted, e.g. for window functions, but generally streaming allows processing huge amounts of data with small amounts of memory.\r\n\r\nFurthermore, as a generalisation, DuckDB could read streams as any tables, and cache them into (compressed) memory if full \"scan\" is needed for column(s). CSV is anyway a (linear?) data source as well (varying row lengths). It would be badass cool to chain DuckDB to DuckDB.\r\n\r\nEdit: And not only DuckDB to DuckDB but from/to any data source, if the data transfer is standardised (like apache arrow flight).\nWe have streaming APIs both for reading table data and to retrieve results. Could you be more precise what you are missing? \n> We have streaming APIs both for reading table data and to retrieve results. Could you be more precise what you are missing?\r\n\r\nMy knowledge of the APIs is still limited, and more around node module we have. I'm thinking about query pushdown from DuckDB to DuckDB and being able to stream the response in a \"standard\" data format that is easily consumable (e.g. the apache arrow flight that was referred to in some other issue). Almost like read replication, but instead of full data replication, just stream based on a query and having the received data in a queryable format for in-memory DuckDB table for example.\nYou can use `Connection::SendQuery(const string &query);`, which will yield a generic `QueryResult`, which is usually going to be a streaming result. On that result you can then call `QueryResult::Fetch()` which will yield a `DataChunk` that holds a chunk of result rows. If you want a more generic representation of that chunk you can call `DataChunk::ToArrowArray(ArrowArray *out_array)` which will convert the chunk to an arrow-compatible Array. \r\n\n> We have streaming APIs both for reading table data and to retrieve results. Could you be more precise what you are missing?\r\n\r\n@hannesmuehleisen\r\n\r\nI can explain what's missing.\r\nDuckDB uses `seek` to check for csv type and auto detect stuff.\r\nBut that wouldn't work with streamable csv files, for example named pipes or http sources.\r\nFor the use case of: \"load csv into RAM from external source and then do some queries\" it becomes cumbersome pretty fast.\r\nIt expects fully functional POSIX file system with random access to files __just__ to load the file into RAM (`CanSeek()` always returns `true`, hardcoded), which is a pretty far reaching assumption.\r\nI can probably do a PR on how to fix that, but I'm not sure it's a wanted change, judging by no support for STDIN sources.\r\n\r\nContrived example of what I'm doing:\r\n```python\r\nimport os\r\nimport tempfile\r\nimport duckdb\r\nfrom multiprocessing import Process\r\n\r\ntmpdir = tempfile.mkdtemp()\r\ndata_file = os.path.join(tmpdir, 'data.csv')\r\nos.mkfifo(data_file)\r\n\r\ncon = duckdb.connect(database=':memory:', read_only=False)\r\n\r\ncon.execute(\"DROP TABLE if exists cities\")\r\ncon.execute(\"CREATE TABLE cities (latd integer, latm integer, lats integer, ns varchar, lond integer, lonm integer, lons integer, ew varchar, city varchar, state varchar)\")\r\n\r\n\r\ndef pipe_data(src, dst):\r\n    fifo = open(dst, 'wb')\r\n    with open(src, 'rb') as file:\r\n        while chunk := file.read(1024):\r\n            fifo.write(chunk)\r\n\r\n\r\ndef load_data(event, context):\r\n    path = 'cities.csv'\r\n    process = Process(target=pipe_data, args=(path, data_file,))\r\n    process.start()\r\n\r\n    con.execute('COPY cities FROM \\'{file_name}\\' ( HEADER )'.format(file_name=data_file))\r\n    process.join()\r\n```\r\n\r\nI'm open to hear what I can do better here.\n> I can explain what's missing.\r\n> DuckDB uses `seek` to check for csv type and auto detect stuff.\r\n> But that wouldn't work with streamable csv files, for example named pipes or http sources.\r\n> For the use case of: \"load csv into RAM from external source and then do some queries\" it becomes cumbersome pretty fast.\r\n> It expects fully functional POSIX file system with random access to files **just** to load the file into RAM (`CanSeek()` always returns `true`, hardcoded), which is a pretty far reaching assumption.\r\n\r\nIf `CanSeek()` returns true, the CSV reader uses seek to jump around in the file to look for samples for better auto detection. If `CanSeek()` is false it will instead only read from the beginning of the stream to do the type detection.\r\n\r\nIt is true that the regular file system (which is supposed to be a\"standard/POSIX file system) always returns true for CanSeek. This is not hard-coded, however. The method is virtual and can be overriden/replaced. For example, the [gzip file system](https://github.com/duckdb/duckdb/blob/30fda3efeb7ca07390a84e787c3110ede289153f/src/include/duckdb/common/gzip_file_system.hpp) returns false here (since random seeking inside gzip files is difficult/expensive). \r\n\r\nFor pipes I would recommend using a new file system as well, since pipes are (as you mentioned) not a regular file system and do not support the same set of operations that regular file systems do.\r\n\r\nI do still see one problem in supporting pipes there, however. The file system will call the `Reset` method on the file, which will still be problematic for pipes. Instead, ideally the CSV reader would need a minor redesign so it would re-use its own internal buffer instead of calling reset.\r\n\r\nAlso, minor note, CSV files [already work with HTTP sources](https://github.com/duckdb/duckdb/blob/30fda3efeb7ca07390a84e787c3110ede289153f/test/sql/copy/csv/test_csv_remote.test) ;)\r\n\r\n> I can probably do a PR on how to fix that, but I'm not sure it's a wanted change, judging by no support for STDIN sources.\r\n\r\nSupporting pipes for CSV file reading (and in general) seems like a good idea, no objections from my side. Happy to review a PR on this!\r\n\n@Mytherin \r\nWorking http sources is pretty nice.\r\nBut for `PipeFileSystem` I'm not sure how it should be chosen? In posix a pipe is just a file. It has no specific schema like http or extension like gzip.\r\nOr better, can you point me in the direction of the code that does the \"fle system specialization\"?\r\n\r\nP.S. and the `Reset()` comment is spot on. The buffered csv reader cannot reset stream at will, but it's quite happily doing it right now :)\n> Or better, can you point me in the direction of the code that does the \"fle system specialization\"?\r\n\r\nThe file system specialization happens [here](https://github.com/duckdb/duckdb/blob/14193fb87eb88566c44a417a73c3e50a4b0da8cc/src/common/file_system.cpp#L1014) for gzip files.\r\n\r\n> P.S. and the Reset() comment is spot on. The buffered csv reader cannot reset stream at will, but it's quite happily doing it right now :)\r\n\r\nThat is true and should probably be changed. However, do note that the auto-detect is optional and can be disabled. So even without any changes to the code streaming through a pipe might work already if auto-detect is disabled and the schema is manually specified. \n> The file system specialization happens here for gzip files.\r\n\r\nHmm, sorry to bother you so much, but where it decides if `http` should be used then?\r\n\r\n> That is true and should probably be changed. However, do note that the auto-detect is optional and can be disabled. So even without any changes to the code streaming through a pipe might work already if auto-detect is disabled and the schema is manually specified.\r\n\r\nTried defining the schema and it still tried to seek it.\r\nI think auto_detect is off by default?\r\nIt looks like it will `Reset()` anyway here: https://github.com/duckdb/duckdb/blob/45c9cb51852ecd14338f79854fe898d70054e8e7/src/execution/operator/persistent/buffered_csv_reader.cpp#L140\n> Hmm, sorry to bother you so much, but where it decides if http should be used then?\r\n\r\nThat happens in the `FindFileSystem(path)`, which uses the `CanHandleFile` to figure out which file system to use ([which is overloaded here for the httpfs](https://github.com/duckdb/duckdb/blob/30fda3efeb7ca07390a84e787c3110ede289153f/extension/httpfs/httpfs.cpp#L180)).\r\n\r\n> I think auto_detect is off by default?\r\n\r\nIt is turned on by default, but indeed it seems that the JumpToBeginning is still called in either case. That can be changed as a reset should not be required in that scenario.\n>It is turned on by default, but indeed it seems that the JumpToBeginning is still called in either case. That can be changed as a reset should not be required in that scenario.\r\n\r\nYup. \r\n\r\nIt also looks like the http loader is not in the latest (0.2.7) release yet?\r\n\r\n```python\r\nhttp_url = 'https://people.sc.fsu.edu/~jburkardt/data/csv/cities.csv'\r\ncon.execute('COPY cities FROM \\'{file_name}\\' ( HEADER )'.format(file_name=http_url))\r\n```\r\n\r\n```bash\r\nRuntimeError: IO Error: No files found that match the pattern \"https://people.sc.fsu.edu/~jburkardt/data/csv/cities.csv\"\r\n```\r\n\nIt is supported, but you need to enable the httpfs extension (`BUILD_HTTPFS=1 make` if you are building from source, or add it to the extensions list in `tools/pythonpkg/setup.py`). It is not enabled by default because of the dependency on OpenSSL.\r\n\r\nWe are still working on making it easier to have extensions loaded externally without needing to compile from source, so you could do e.g. `pip install duckdb-httpfs` and install the extension separately. That is not available yet unfortunately. \nRegarding the use case to \"load csv into RAM from external source and then do some queries\" using `read_csv_auto`, the automatic type guessing enabled by default is very useful and convenient. I can read a csv from a web url with the duckdb CLI and output results to stdout from a select query in one big chunk:\r\n\r\n```bash\r\ncurl -s https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv | sed 's/|/,/g' > /tmp/stdin \\\r\n&& cat /tmp/stdin | duckdb -csv :memory: \"create table mytable as select * from read_csv_auto('/tmp/stdin', HEADER=FALSE); select * from mytable;\"\r\n```\r\n\r\nWith duckdb CLI v0.2.7 this works so httpsfs is supported there:\r\n\r\n```bash\r\nduckdb -csv :memory: \"create table mytable as select * from read_csv_auto('https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv', HEADER=FALSE, SEP='|'); select * from mytable;\"\r\n```\r\n\r\nIf the duckdb CLI could read directly from the stdin stream, maybe even with a progress indicator for chunkwise processing steps, there would be no need to create extra temp files or write external utilities to load data incrementally in chunks (as in https://github.com/duckdb/duckdb/issues/1417#issuecomment-867506887 and https://gist.github.com/mskyttner/46601e60138a886cdfbfc81067c71725).\r\n\r\nHere is a duckdb CLI v 0.2.7 attempt to load data without the temp file:\r\n\r\n```bash\r\ncurl -s https://raw.githubusercontent.com/duckdb/duckdb/master/test/sql/copy/csv/data/real/web_page.csv | sed 's/|/,/g' \\\r\n| duckdb -csv :memory: \"create table mytable as select * from read_csv_auto('/dev/stdin', HEADER=TRUE); select * from mytable;\"\r\n```\r\nThe error reported is:\r\n\r\n`Error: IO Error: Could not seek to location 0 for file \"/dev/stdin\": Illegal seek`\r\n\r\nWould fixing the JumpToBeginning issue help here?\r\n\r\n\n> Would fixing the JumpToBeginning issue help here?\r\n\r\nYup, looks like it. Just need to detect that the stream is unseekable and then chose correct fs implementation. After getting rid of `JumpToBeginning`\n> It is supported, but you need to enable the httpfs extension (`BUILD_HTTPFS=1 make` if you are building from source, or add it to the extensions list in `tools/pythonpkg/setup.py`). It is not enabled by default because of the dependency on OpenSSL.\r\n\r\nHmm, I cannot build a python package.\r\n```bash\r\ncd tools/pythonpkg\r\npython setup.py sdist\r\n\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 99, in <module>\r\n    (source_list, include_list, original_sources) = package_build.build_package(os.path.join(script_path, 'duckdb'), extensions)\r\n  File \"/home/user/git/duckdb/tools/pythonpkg/../../scripts/package_build.py\", line 149, in build_package\r\n    include_package(ext, ext_path, include_files, include_list, source_list)\r\n  File \"/home/user/git/duckdb/tools/pythonpkg/../../scripts/package_build.py\", line 105, in include_package\r\n    ext_pkg = __import__(pkg_name + '_config')\r\nModuleNotFoundError: No module named 'httpfs_config'\r\n\r\n```\r\n\r\nThe build with `BUILD_HTTPFS=1 make` succeeds though\r\n\r\nAh, I see no `http_config.py` is there, need to define one.",
  "created_at": "2021-07-04T11:55:39Z"
}