{
  "repo": "duckdb/duckdb",
  "pull_number": 8745,
  "instance_id": "duckdb__duckdb-8745",
  "issue_numbers": [
    "8652"
  ],
  "base_commit": "44fec4a8121bdfaacc9b59255e6543c157171d6e",
  "patch": "diff --git a/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp\nindex f6a1ef91d693..8c262671bd7d 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/array_wrapper.hpp\n@@ -9,40 +9,41 @@\n #pragma once\n \n #include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+#include \"duckdb_python/numpy/raw_array_wrapper.hpp\"\n #include \"duckdb.hpp\"\n+#include \"duckdb/common/types.hpp\"\n \n namespace duckdb {\n \n-struct RegisteredArray {\n-\texplicit RegisteredArray(py::array numpy_array) : numpy_array(std::move(numpy_array)) {\n-\t}\n-\tpy::array numpy_array;\n-};\n+struct ClientProperties;\n \n-struct RawArrayWrapper {\n+struct NumpyAppendData {\n+public:\n+\tNumpyAppendData(UnifiedVectorFormat &idata, const ClientProperties &client_properties, Vector &input)\n+\t    : idata(idata), client_properties(client_properties), input(input) {\n+\t}\n \n-\texplicit RawArrayWrapper(const LogicalType &type);\n+public:\n+\tUnifiedVectorFormat &idata;\n+\tconst ClientProperties &client_properties;\n+\tVector &input;\n \n-\tpy::array array;\n-\tdata_ptr_t data;\n-\tLogicalType type;\n-\tidx_t type_width;\n+\tidx_t target_offset;\n+\tdata_ptr_t target_data;\n+\tbool *target_mask;\n \tidx_t count;\n-\n-public:\n-\tstatic string DuckDBToNumpyDtype(const LogicalType &type);\n-\tvoid Initialize(idx_t capacity);\n-\tvoid Resize(idx_t new_capacity);\n-\tvoid Append(idx_t current_offset, Vector &input, idx_t count);\n+\tPhysicalType physical_type = PhysicalType::INVALID;\n+\tbool pandas = false;\n };\n \n struct ArrayWrapper {\n-\texplicit ArrayWrapper(const LogicalType &type, const ClientProperties &client_properties);\n+\texplicit ArrayWrapper(const LogicalType &type, const ClientProperties &client_properties, bool pandas = false);\n \n \tunique_ptr<RawArrayWrapper> data;\n \tunique_ptr<RawArrayWrapper> mask;\n \tbool requires_mask;\n \tconst ClientProperties client_properties;\n+\tbool pandas;\n \n public:\n \tvoid Initialize(idx_t capacity);\n@@ -51,24 +52,4 @@ struct ArrayWrapper {\n \tpy::object ToArray(idx_t count) const;\n };\n \n-class NumpyResultConversion {\n-public:\n-\tNumpyResultConversion(const vector<LogicalType> &types, idx_t initial_capacity,\n-\t                      const ClientProperties &client_properties);\n-\n-\tvoid Append(DataChunk &chunk);\n-\n-\tpy::object ToArray(idx_t col_idx) {\n-\t\treturn owned_data[col_idx].ToArray(count);\n-\t}\n-\n-private:\n-\tvoid Resize(idx_t new_capacity);\n-\n-private:\n-\tvector<ArrayWrapper> owned_data;\n-\tidx_t count;\n-\tidx_t capacity;\n-};\n-\n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_result_conversion.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_result_conversion.hpp\nnew file mode 100644\nindex 000000000000..e53b6add5729\n--- /dev/null\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/numpy_result_conversion.hpp\n@@ -0,0 +1,37 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb_python/numpy/numpy_result_conversion.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n+#include \"duckdb.hpp\"\n+\n+namespace duckdb {\n+\n+class NumpyResultConversion {\n+public:\n+\tNumpyResultConversion(const vector<LogicalType> &types, idx_t initial_capacity,\n+\t                      const ClientProperties &client_properties, bool pandas = false);\n+\n+\tvoid Append(DataChunk &chunk);\n+\n+\tpy::object ToArray(idx_t col_idx) {\n+\t\treturn owned_data[col_idx].ToArray(count);\n+\t}\n+\n+private:\n+\tvoid Resize(idx_t new_capacity);\n+\n+private:\n+\tvector<ArrayWrapper> owned_data;\n+\tidx_t count;\n+\tidx_t capacity;\n+};\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/numpy/raw_array_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/numpy/raw_array_wrapper.hpp\nnew file mode 100644\nindex 000000000000..124f2112f89b\n--- /dev/null\n+++ b/tools/pythonpkg/src/include/duckdb_python/numpy/raw_array_wrapper.hpp\n@@ -0,0 +1,33 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb_python/array_wrapper.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n+#include \"duckdb.hpp\"\n+\n+namespace duckdb {\n+\n+struct RawArrayWrapper {\n+\n+\texplicit RawArrayWrapper(const LogicalType &type);\n+\n+\tpy::array array;\n+\tdata_ptr_t data;\n+\tLogicalType type;\n+\tidx_t type_width;\n+\tidx_t count;\n+\n+public:\n+\tstatic string DuckDBToNumpyDtype(const LogicalType &type);\n+\tvoid Initialize(idx_t capacity);\n+\tvoid Resize(idx_t new_capacity);\n+\tvoid Append(idx_t current_offset, Vector &input, idx_t count);\n+};\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_bind.hpp b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_bind.hpp\nindex d85f18815164..5b58de592e83 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_bind.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pandas/pandas_bind.hpp\n@@ -8,9 +8,14 @@\n \n namespace duckdb {\n \n-struct RegisteredArray;\n class ClientContext;\n \n+struct RegisteredArray {\n+\texplicit RegisteredArray(py::array numpy_array) : numpy_array(std::move(numpy_array)) {\n+\t}\n+\tpy::array numpy_array;\n+};\n+\n struct PandasColumnBindData {\n \tNumpyType numpy_type;\n \tunique_ptr<PandasColumn> pandas_col;\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyresult.hpp b/tools/pythonpkg/src/include/duckdb_python/pyresult.hpp\nindex 8dfce815bf20..46e844eb3af2 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyresult.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyresult.hpp\n@@ -8,7 +8,7 @@\n \n #pragma once\n \n-#include \"duckdb_python/numpy/array_wrapper.hpp\"\n+#include \"duckdb_python/numpy/numpy_result_conversion.hpp\"\n #include \"duckdb.hpp\"\n #include \"duckdb_python/pybind11/pybind_wrapper.hpp\"\n #include \"duckdb_python/python_objects.hpp\"\n@@ -29,7 +29,8 @@ struct DuckDBPyResult {\n \n \tpy::dict FetchNumpy();\n \n-\tpy::dict FetchNumpyInternal(bool stream = false, idx_t vectors_per_chunk = 1);\n+\tpy::dict FetchNumpyInternal(bool stream = false, idx_t vectors_per_chunk = 1,\n+\t                            unique_ptr<NumpyResultConversion> conversion = nullptr);\n \n \tPandasDataFrame FetchDF(bool date_as_object);\n \n@@ -67,6 +68,7 @@ struct DuckDBPyResult {\n \tvoid ChangeDateToDatetime(PandasDataFrame &df);\n \tunique_ptr<DataChunk> FetchNext(QueryResult &result);\n \tunique_ptr<DataChunk> FetchNextRaw(QueryResult &result);\n+\tunique_ptr<NumpyResultConversion> InitializeNumpyConversion(bool pandas = false);\n \n private:\n \tidx_t chunk_offset = 0;\ndiff --git a/tools/pythonpkg/src/map.cpp b/tools/pythonpkg/src/map.cpp\nindex a76198edc72a..cd724779a3d7 100644\n--- a/tools/pythonpkg/src/map.cpp\n+++ b/tools/pythonpkg/src/map.cpp\n@@ -1,7 +1,7 @@\n #include \"duckdb_python/map.hpp\"\n #include \"duckdb_python/numpy/numpy_scan.hpp\"\n #include \"duckdb_python/pandas/pandas_bind.hpp\"\n-#include \"duckdb_python/numpy/array_wrapper.hpp\"\n+#include \"duckdb_python/numpy/numpy_result_conversion.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb_python/pandas/column/pandas_numpy_column.hpp\"\n #include \"duckdb_python/pandas/pandas_scan.hpp\"\ndiff --git a/tools/pythonpkg/src/numpy/CMakeLists.txt b/tools/pythonpkg/src/numpy/CMakeLists.txt\nindex 77c1cf5331a8..d7b818156e2d 100644\n--- a/tools/pythonpkg/src/numpy/CMakeLists.txt\n+++ b/tools/pythonpkg/src/numpy/CMakeLists.txt\n@@ -3,8 +3,10 @@ include_directories(${pybind11_INCLUDE_DIR})\n include_directories(${PYTHON_INCLUDE_DIRS})\n find_package(pybind11 REQUIRED)\n \n-add_library(python_numpy OBJECT type.cpp numpy_scan.cpp array_wrapper.cpp\n-                                numpy_bind.cpp)\n+add_library(\n+  python_numpy OBJECT\n+  type.cpp numpy_scan.cpp array_wrapper.cpp raw_array_wrapper.cpp\n+  numpy_bind.cpp numpy_result_conversion.cpp)\n \n set(ALL_OBJECT_FILES\n     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:python_numpy>\ndiff --git a/tools/pythonpkg/src/numpy/array_wrapper.cpp b/tools/pythonpkg/src/numpy/array_wrapper.cpp\nindex 6f95280ad593..f0f011b88237 100644\n--- a/tools/pythonpkg/src/numpy/array_wrapper.cpp\n+++ b/tools/pythonpkg/src/numpy/array_wrapper.cpp\n@@ -21,8 +21,9 @@ struct RegularConvert {\n \t\treturn (NUMPY_T)val;\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn 0;\n \t}\n };\n@@ -33,8 +34,9 @@ struct TimestampConvert {\n \t\treturn Timestamp::GetEpochNanoSeconds(val);\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn 0;\n \t}\n };\n@@ -45,8 +47,9 @@ struct TimestampConvertSec {\n \t\treturn Timestamp::GetEpochNanoSeconds(Timestamp::FromEpochSeconds(val.value));\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn 0;\n \t}\n };\n@@ -57,8 +60,9 @@ struct TimestampConvertMilli {\n \t\treturn Timestamp::GetEpochNanoSeconds(Timestamp::FromEpochMs(val.value));\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn 0;\n \t}\n };\n@@ -69,8 +73,9 @@ struct TimestampConvertNano {\n \t\treturn val.value;\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn 0;\n \t}\n };\n@@ -81,8 +86,9 @@ struct DateConvert {\n \t\treturn Date::EpochNanoseconds(val);\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn 0;\n \t}\n };\n@@ -93,8 +99,9 @@ struct IntervalConvert {\n \t\treturn Interval::GetNanoseconds(val);\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn 0;\n \t}\n };\n@@ -106,8 +113,9 @@ struct TimeConvert {\n \t\treturn PyUnicode_FromStringAndSize(str.c_str(), str.size());\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn nullptr;\n \t}\n };\n@@ -204,8 +212,13 @@ struct StringConvert {\n \t\tmemcpy(target_data, data, len);\n \t\treturn result;\n \t}\n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tif (PANDAS) {\n+\t\t\tset_mask = false;\n+\t\t\treturn Py_None;\n+\t\t}\n+\t\tset_mask = true;\n \t\treturn nullptr;\n \t}\n };\n@@ -216,8 +229,9 @@ struct BlobConvert {\n \t\treturn PyByteArray_FromStringAndSize(val.GetData(), val.GetSize());\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn nullptr;\n \t}\n };\n@@ -228,8 +242,9 @@ struct BitConvert {\n \t\treturn PyBytes_FromStringAndSize(val.GetData(), val.GetSize());\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn nullptr;\n \t}\n };\n@@ -242,8 +257,9 @@ struct UUIDConvert {\n \t\treturn h.ptr();\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn nullptr;\n \t}\n };\n@@ -315,8 +331,9 @@ struct IntegralConvert {\n \t\treturn NUMPY_T(val);\n \t}\n \n-\ttemplate <class NUMPY_T>\n-\tstatic NUMPY_T NullValue() {\n+\ttemplate <class NUMPY_T, bool PANDAS>\n+\tstatic NUMPY_T NullValue(bool &set_mask) {\n+\t\tset_mask = true;\n \t\treturn 0;\n \t}\n };\n@@ -331,23 +348,33 @@ double IntegralConvert::ConvertValue(hugeint_t val) {\n } // namespace duckdb_py_convert\n \n template <class DUCKDB_T, class NUMPY_T, class CONVERT>\n-static bool ConvertColumn(idx_t target_offset, data_ptr_t target_data, bool *target_mask, UnifiedVectorFormat &idata,\n-                          idx_t count) {\n+static bool ConvertColumn(NumpyAppendData &append_data) {\n+\tauto target_offset = append_data.target_offset;\n+\tauto target_data = append_data.target_data;\n+\tauto target_mask = append_data.target_mask;\n+\tauto &idata = append_data.idata;\n+\tauto count = append_data.count;\n+\n \tauto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);\n \tauto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);\n \tif (!idata.validity.AllValid()) {\n+\t\tbool mask_is_set = false;\n \t\tfor (idx_t i = 0; i < count; i++) {\n \t\t\tidx_t src_idx = idata.sel->get_index(i);\n \t\t\tidx_t offset = target_offset + i;\n \t\t\tif (!idata.validity.RowIsValidUnsafe(src_idx)) {\n-\t\t\t\ttarget_mask[offset] = true;\n-\t\t\t\tout_ptr[offset] = CONVERT::template NullValue<NUMPY_T>();\n+\t\t\t\tif (append_data.pandas) {\n+\t\t\t\t\tout_ptr[offset] = CONVERT::template NullValue<NUMPY_T, true>(target_mask[offset]);\n+\t\t\t\t} else {\n+\t\t\t\t\tout_ptr[offset] = CONVERT::template NullValue<NUMPY_T, false>(target_mask[offset]);\n+\t\t\t\t}\n+\t\t\t\tmask_is_set = mask_is_set || target_mask[offset];\n \t\t\t} else {\n \t\t\t\tout_ptr[offset] = CONVERT::template ConvertValue<DUCKDB_T, NUMPY_T>(src_ptr[src_idx]);\n \t\t\t\ttarget_mask[offset] = false;\n \t\t\t}\n \t\t}\n-\t\treturn true;\n+\t\treturn mask_is_set;\n \t} else {\n \t\tfor (idx_t i = 0; i < count; i++) {\n \t\t\tidx_t src_idx = idata.sel->get_index(i);\n@@ -360,8 +387,12 @@ static bool ConvertColumn(idx_t target_offset, data_ptr_t target_data, bool *tar\n }\n \n template <class DUCKDB_T, class NUMPY_T>\n-static bool ConvertColumnCategoricalTemplate(idx_t target_offset, data_ptr_t target_data, UnifiedVectorFormat &idata,\n-                                             idx_t count) {\n+static bool ConvertColumnCategoricalTemplate(NumpyAppendData &append_data) {\n+\tauto target_offset = append_data.target_offset;\n+\tauto target_data = append_data.target_data;\n+\tauto &idata = append_data.idata;\n+\tauto count = append_data.count;\n+\n \tauto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);\n \tauto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);\n \tif (!idata.validity.AllValid()) {\n@@ -388,8 +419,15 @@ static bool ConvertColumnCategoricalTemplate(idx_t target_offset, data_ptr_t tar\n }\n \n template <class NUMPY_T, class CONVERT>\n-static bool ConvertNested(idx_t target_offset, data_ptr_t target_data, bool *target_mask, Vector &input,\n-                          UnifiedVectorFormat &idata, idx_t count, const ClientProperties &client_properties) {\n+static bool ConvertNested(NumpyAppendData &append_data) {\n+\tauto target_offset = append_data.target_offset;\n+\tauto target_data = append_data.target_data;\n+\tauto target_mask = append_data.target_mask;\n+\tauto &input = append_data.input;\n+\tauto &idata = append_data.idata;\n+\tauto &client_properties = append_data.client_properties;\n+\tauto count = append_data.count;\n+\n \tauto out_ptr = reinterpret_cast<NUMPY_T *>(target_data);\n \tif (!idata.validity.AllValid()) {\n \t\tfor (idx_t i = 0; i < count; i++) {\n@@ -414,30 +452,33 @@ static bool ConvertNested(idx_t target_offset, data_ptr_t target_data, bool *tar\n }\n \n template <class NUMPY_T>\n-static bool ConvertColumnCategorical(idx_t target_offset, data_ptr_t target_data, UnifiedVectorFormat &idata,\n-                                     idx_t count, PhysicalType physical_type) {\n+static bool ConvertColumnCategorical(NumpyAppendData &append_data) {\n+\tauto physical_type = append_data.physical_type;\n \tswitch (physical_type) {\n \tcase PhysicalType::UINT8:\n-\t\treturn ConvertColumnCategoricalTemplate<uint8_t, NUMPY_T>(target_offset, target_data, idata, count);\n+\t\treturn ConvertColumnCategoricalTemplate<uint8_t, NUMPY_T>(append_data);\n \tcase PhysicalType::UINT16:\n-\t\treturn ConvertColumnCategoricalTemplate<uint16_t, NUMPY_T>(target_offset, target_data, idata, count);\n+\t\treturn ConvertColumnCategoricalTemplate<uint16_t, NUMPY_T>(append_data);\n \tcase PhysicalType::UINT32:\n-\t\treturn ConvertColumnCategoricalTemplate<uint32_t, NUMPY_T>(target_offset, target_data, idata, count);\n+\t\treturn ConvertColumnCategoricalTemplate<uint32_t, NUMPY_T>(append_data);\n \tdefault:\n \t\tthrow InternalException(\"Enum Physical Type not Allowed\");\n \t}\n }\n \n template <class T>\n-static bool ConvertColumnRegular(idx_t target_offset, data_ptr_t target_data, bool *target_mask,\n-                                 UnifiedVectorFormat &idata, idx_t count) {\n-\treturn ConvertColumn<T, T, duckdb_py_convert::RegularConvert>(target_offset, target_data, target_mask, idata,\n-\t                                                              count);\n+static bool ConvertColumnRegular(NumpyAppendData &append_data) {\n+\treturn ConvertColumn<T, T, duckdb_py_convert::RegularConvert>(append_data);\n }\n \n template <class DUCKDB_T>\n-static bool ConvertDecimalInternal(idx_t target_offset, data_ptr_t target_data, bool *target_mask,\n-                                   UnifiedVectorFormat &idata, idx_t count, double division) {\n+static bool ConvertDecimalInternal(NumpyAppendData &append_data, double division) {\n+\tauto target_offset = append_data.target_offset;\n+\tauto target_data = append_data.target_data;\n+\tauto target_mask = append_data.target_mask;\n+\tauto &idata = append_data.idata;\n+\tauto count = append_data.count;\n+\n \tauto src_ptr = UnifiedVectorFormat::GetData<DUCKDB_T>(idata);\n \tauto out_ptr = reinterpret_cast<double *>(target_data);\n \tif (!idata.validity.AllValid()) {\n@@ -465,172 +506,26 @@ static bool ConvertDecimalInternal(idx_t target_offset, data_ptr_t target_data,\n \t}\n }\n \n-static bool ConvertDecimal(const LogicalType &decimal_type, idx_t target_offset, data_ptr_t target_data,\n-                           bool *target_mask, UnifiedVectorFormat &idata, idx_t count) {\n+static bool ConvertDecimal(NumpyAppendData &append_data) {\n+\tauto &decimal_type = append_data.input.GetType();\n \tauto dec_scale = DecimalType::GetScale(decimal_type);\n \tdouble division = pow(10, dec_scale);\n \tswitch (decimal_type.InternalType()) {\n \tcase PhysicalType::INT16:\n-\t\treturn ConvertDecimalInternal<int16_t>(target_offset, target_data, target_mask, idata, count, division);\n+\t\treturn ConvertDecimalInternal<int16_t>(append_data, division);\n \tcase PhysicalType::INT32:\n-\t\treturn ConvertDecimalInternal<int32_t>(target_offset, target_data, target_mask, idata, count, division);\n+\t\treturn ConvertDecimalInternal<int32_t>(append_data, division);\n \tcase PhysicalType::INT64:\n-\t\treturn ConvertDecimalInternal<int64_t>(target_offset, target_data, target_mask, idata, count, division);\n+\t\treturn ConvertDecimalInternal<int64_t>(append_data, division);\n \tcase PhysicalType::INT128:\n-\t\treturn ConvertDecimalInternal<hugeint_t>(target_offset, target_data, target_mask, idata, count, division);\n+\t\treturn ConvertDecimalInternal<hugeint_t>(append_data, division);\n \tdefault:\n \t\tthrow NotImplementedException(\"Unimplemented internal type for DECIMAL\");\n \t}\n }\n \n-RawArrayWrapper::RawArrayWrapper(const LogicalType &type) : data(nullptr), type(type), count(0) {\n-\tswitch (type.id()) {\n-\tcase LogicalTypeId::BOOLEAN:\n-\t\ttype_width = sizeof(bool);\n-\t\tbreak;\n-\tcase LogicalTypeId::UTINYINT:\n-\t\ttype_width = sizeof(uint8_t);\n-\t\tbreak;\n-\tcase LogicalTypeId::USMALLINT:\n-\t\ttype_width = sizeof(uint16_t);\n-\t\tbreak;\n-\tcase LogicalTypeId::UINTEGER:\n-\t\ttype_width = sizeof(uint32_t);\n-\t\tbreak;\n-\tcase LogicalTypeId::UBIGINT:\n-\t\ttype_width = sizeof(uint64_t);\n-\t\tbreak;\n-\tcase LogicalTypeId::TINYINT:\n-\t\ttype_width = sizeof(int8_t);\n-\t\tbreak;\n-\tcase LogicalTypeId::SMALLINT:\n-\t\ttype_width = sizeof(int16_t);\n-\t\tbreak;\n-\tcase LogicalTypeId::INTEGER:\n-\t\ttype_width = sizeof(int32_t);\n-\t\tbreak;\n-\tcase LogicalTypeId::BIGINT:\n-\t\ttype_width = sizeof(int64_t);\n-\t\tbreak;\n-\tcase LogicalTypeId::FLOAT:\n-\t\ttype_width = sizeof(float);\n-\t\tbreak;\n-\tcase LogicalTypeId::HUGEINT:\n-\tcase LogicalTypeId::DOUBLE:\n-\tcase LogicalTypeId::DECIMAL:\n-\t\ttype_width = sizeof(double);\n-\t\tbreak;\n-\tcase LogicalTypeId::TIMESTAMP:\n-\tcase LogicalTypeId::TIMESTAMP_SEC:\n-\tcase LogicalTypeId::TIMESTAMP_MS:\n-\tcase LogicalTypeId::TIMESTAMP_NS:\n-\tcase LogicalTypeId::DATE:\n-\tcase LogicalTypeId::INTERVAL:\n-\tcase LogicalTypeId::TIMESTAMP_TZ:\n-\t\ttype_width = sizeof(int64_t);\n-\t\tbreak;\n-\tcase LogicalTypeId::TIME:\n-\tcase LogicalTypeId::TIME_TZ:\n-\tcase LogicalTypeId::VARCHAR:\n-\tcase LogicalTypeId::BIT:\n-\tcase LogicalTypeId::BLOB:\n-\tcase LogicalTypeId::ENUM:\n-\tcase LogicalTypeId::LIST:\n-\tcase LogicalTypeId::MAP:\n-\tcase LogicalTypeId::STRUCT:\n-\tcase LogicalTypeId::UNION:\n-\tcase LogicalTypeId::UUID:\n-\t\ttype_width = sizeof(PyObject *);\n-\t\tbreak;\n-\tdefault:\n-\t\tthrow NotImplementedException(\"Unsupported type \\\"%s\\\" for DuckDB -> NumPy conversion\", type.ToString());\n-\t}\n-}\n-\n-string RawArrayWrapper::DuckDBToNumpyDtype(const LogicalType &type) {\n-\tswitch (type.id()) {\n-\tcase LogicalTypeId::BOOLEAN:\n-\t\treturn \"bool\";\n-\tcase LogicalTypeId::TINYINT:\n-\t\treturn \"int8\";\n-\tcase LogicalTypeId::SMALLINT:\n-\t\treturn \"int16\";\n-\tcase LogicalTypeId::INTEGER:\n-\t\treturn \"int32\";\n-\tcase LogicalTypeId::BIGINT:\n-\t\treturn \"int64\";\n-\tcase LogicalTypeId::UTINYINT:\n-\t\treturn \"uint8\";\n-\tcase LogicalTypeId::USMALLINT:\n-\t\treturn \"uint16\";\n-\tcase LogicalTypeId::UINTEGER:\n-\t\treturn \"uint32\";\n-\tcase LogicalTypeId::UBIGINT:\n-\t\treturn \"uint64\";\n-\tcase LogicalTypeId::FLOAT:\n-\t\treturn \"float32\";\n-\tcase LogicalTypeId::HUGEINT:\n-\tcase LogicalTypeId::DOUBLE:\n-\tcase LogicalTypeId::DECIMAL:\n-\t\treturn \"float64\";\n-\tcase LogicalTypeId::TIMESTAMP:\n-\t\treturn \"datetime64[us]\";\n-\tcase LogicalTypeId::TIMESTAMP_TZ:\n-\t\treturn \"datetime64[us]\";\n-\tcase LogicalTypeId::TIMESTAMP_NS:\n-\t\treturn \"datetime64[ns]\";\n-\tcase LogicalTypeId::TIMESTAMP_MS:\n-\t\treturn \"datetime64[ms]\";\n-\tcase LogicalTypeId::TIMESTAMP_SEC:\n-\t\treturn \"datetime64[s]\";\n-\tcase LogicalTypeId::DATE:\n-\t\t// FIXME: should this not be 'date64[ns]' ?\n-\t\treturn \"datetime64[ns]\";\n-\tcase LogicalTypeId::INTERVAL:\n-\t\treturn \"timedelta64[ns]\";\n-\tcase LogicalTypeId::TIME:\n-\tcase LogicalTypeId::TIME_TZ:\n-\tcase LogicalTypeId::VARCHAR:\n-\tcase LogicalTypeId::BIT:\n-\tcase LogicalTypeId::BLOB:\n-\tcase LogicalTypeId::LIST:\n-\tcase LogicalTypeId::MAP:\n-\tcase LogicalTypeId::STRUCT:\n-\tcase LogicalTypeId::UNION:\n-\tcase LogicalTypeId::UUID:\n-\t\treturn \"object\";\n-\tcase LogicalTypeId::ENUM: {\n-\t\tauto size = EnumType::GetSize(type);\n-\t\tif (size <= (idx_t)NumericLimits<int8_t>::Maximum()) {\n-\t\t\treturn \"int8\";\n-\t\t} else if (size <= (idx_t)NumericLimits<int16_t>::Maximum()) {\n-\t\t\treturn \"int16\";\n-\t\t} else if (size <= (idx_t)NumericLimits<int32_t>::Maximum()) {\n-\t\t\treturn \"int32\";\n-\t\t} else {\n-\t\t\tthrow InternalException(\"Size not supported on ENUM types\");\n-\t\t}\n-\t}\n-\tdefault:\n-\t\tthrow NotImplementedException(\"Unsupported type \\\"%s\\\"\", type.ToString());\n-\t}\n-}\n-\n-void RawArrayWrapper::Initialize(idx_t capacity) {\n-\tstring dtype = DuckDBToNumpyDtype(type);\n-\n-\tarray = py::array(py::dtype(dtype), capacity);\n-\tdata = data_ptr_cast(array.mutable_data());\n-}\n-\n-void RawArrayWrapper::Resize(idx_t new_capacity) {\n-\tvector<py::ssize_t> new_shape {py::ssize_t(new_capacity)};\n-\tarray.resize(new_shape, false);\n-\tdata = data_ptr_cast(array.mutable_data());\n-}\n-\n-ArrayWrapper::ArrayWrapper(const LogicalType &type, const ClientProperties &client_properties_p)\n-    : requires_mask(false), client_properties(client_properties_p) {\n+ArrayWrapper::ArrayWrapper(const LogicalType &type, const ClientProperties &client_properties_p, bool pandas)\n+    : requires_mask(false), client_properties(client_properties_p), pandas(pandas) {\n \tdata = make_uniq<RawArrayWrapper>(type);\n \tmask = make_uniq<RawArrayWrapper>(LogicalType::BOOLEAN);\n }\n@@ -655,111 +550,106 @@ void ArrayWrapper::Append(idx_t current_offset, Vector &input, idx_t count) {\n \n \tUnifiedVectorFormat idata;\n \tinput.ToUnifiedFormat(count, idata);\n+\n+\tNumpyAppendData append_data(idata, client_properties, input);\n+\tappend_data.target_offset = current_offset;\n+\tappend_data.target_data = dataptr;\n+\tappend_data.count = count;\n+\tappend_data.target_mask = maskptr;\n+\tappend_data.pandas = pandas;\n+\n \tswitch (input.GetType().id()) {\n \tcase LogicalTypeId::ENUM: {\n \t\tauto size = EnumType::GetSize(input.GetType());\n-\t\tauto physical_type = input.GetType().InternalType();\n+\t\tappend_data.physical_type = input.GetType().InternalType();\n \t\tif (size <= (idx_t)NumericLimits<int8_t>::Maximum()) {\n-\t\t\tmay_have_null = ConvertColumnCategorical<int8_t>(current_offset, dataptr, idata, count, physical_type);\n+\t\t\tmay_have_null = ConvertColumnCategorical<int8_t>(append_data);\n \t\t} else if (size <= (idx_t)NumericLimits<int16_t>::Maximum()) {\n-\t\t\tmay_have_null = ConvertColumnCategorical<int16_t>(current_offset, dataptr, idata, count, physical_type);\n+\t\t\tmay_have_null = ConvertColumnCategorical<int16_t>(append_data);\n \t\t} else if (size <= (idx_t)NumericLimits<int32_t>::Maximum()) {\n-\t\t\tmay_have_null = ConvertColumnCategorical<int32_t>(current_offset, dataptr, idata, count, physical_type);\n+\t\t\tmay_have_null = ConvertColumnCategorical<int32_t>(append_data);\n \t\t} else {\n \t\t\tthrow InternalException(\"Size not supported on ENUM types\");\n \t\t}\n \t} break;\n \tcase LogicalTypeId::BOOLEAN:\n-\t\tmay_have_null = ConvertColumnRegular<bool>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<bool>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::TINYINT:\n-\t\tmay_have_null = ConvertColumnRegular<int8_t>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<int8_t>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::SMALLINT:\n-\t\tmay_have_null = ConvertColumnRegular<int16_t>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<int16_t>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::INTEGER:\n-\t\tmay_have_null = ConvertColumnRegular<int32_t>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<int32_t>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::BIGINT:\n-\t\tmay_have_null = ConvertColumnRegular<int64_t>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<int64_t>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::UTINYINT:\n-\t\tmay_have_null = ConvertColumnRegular<uint8_t>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<uint8_t>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::USMALLINT:\n-\t\tmay_have_null = ConvertColumnRegular<uint16_t>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<uint16_t>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::UINTEGER:\n-\t\tmay_have_null = ConvertColumnRegular<uint32_t>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<uint32_t>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::UBIGINT:\n-\t\tmay_have_null = ConvertColumnRegular<uint64_t>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<uint64_t>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::HUGEINT:\n-\t\tmay_have_null = ConvertColumn<hugeint_t, double, duckdb_py_convert::IntegralConvert>(current_offset, dataptr,\n-\t\t                                                                                     maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumn<hugeint_t, double, duckdb_py_convert::IntegralConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::FLOAT:\n-\t\tmay_have_null = ConvertColumnRegular<float>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<float>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::DOUBLE:\n-\t\tmay_have_null = ConvertColumnRegular<double>(current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumnRegular<double>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::DECIMAL:\n-\t\tmay_have_null = ConvertDecimal(input.GetType(), current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertDecimal(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::TIMESTAMP:\n \tcase LogicalTypeId::TIMESTAMP_TZ:\n \tcase LogicalTypeId::TIMESTAMP_SEC:\n \tcase LogicalTypeId::TIMESTAMP_MS:\n \tcase LogicalTypeId::TIMESTAMP_NS:\n-\t\tmay_have_null = ConvertColumn<timestamp_t, int64_t, duckdb_py_convert::TimestampConvertNano>(\n-\t\t    current_offset, dataptr, maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumn<timestamp_t, int64_t, duckdb_py_convert::TimestampConvertNano>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::DATE:\n-\t\tmay_have_null = ConvertColumn<date_t, int64_t, duckdb_py_convert::DateConvert>(current_offset, dataptr, maskptr,\n-\t\t                                                                               idata, count);\n+\t\tmay_have_null = ConvertColumn<date_t, int64_t, duckdb_py_convert::DateConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::TIME:\n-\t\tmay_have_null = ConvertColumn<dtime_t, PyObject *, duckdb_py_convert::TimeConvert>(current_offset, dataptr,\n-\t\t                                                                                   maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumn<dtime_t, PyObject *, duckdb_py_convert::TimeConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::INTERVAL:\n-\t\tmay_have_null = ConvertColumn<interval_t, int64_t, duckdb_py_convert::IntervalConvert>(current_offset, dataptr,\n-\t\t                                                                                       maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumn<interval_t, int64_t, duckdb_py_convert::IntervalConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::VARCHAR:\n-\t\tmay_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::StringConvert>(current_offset, dataptr,\n-\t\t                                                                                      maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::StringConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::BLOB:\n-\t\tmay_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::BlobConvert>(current_offset, dataptr,\n-\t\t                                                                                    maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::BlobConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::BIT:\n-\t\tmay_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::BitConvert>(current_offset, dataptr,\n-\t\t                                                                                   maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumn<string_t, PyObject *, duckdb_py_convert::BitConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::LIST:\n-\t\tmay_have_null = ConvertNested<py::list, duckdb_py_convert::ListConvert>(current_offset, dataptr, maskptr, input,\n-\t\t                                                                        idata, count, client_properties);\n+\t\tmay_have_null = ConvertNested<py::list, duckdb_py_convert::ListConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::MAP:\n-\t\tmay_have_null = ConvertNested<py::dict, duckdb_py_convert::MapConvert>(current_offset, dataptr, maskptr, input,\n-\t\t                                                                       idata, count, client_properties);\n+\t\tmay_have_null = ConvertNested<py::dict, duckdb_py_convert::MapConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::UNION:\n-\t\tmay_have_null = ConvertNested<py::object, duckdb_py_convert::UnionConvert>(\n-\t\t    current_offset, dataptr, maskptr, input, idata, count, client_properties);\n+\t\tmay_have_null = ConvertNested<py::object, duckdb_py_convert::UnionConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::STRUCT:\n-\t\tmay_have_null = ConvertNested<py::dict, duckdb_py_convert::StructConvert>(\n-\t\t    current_offset, dataptr, maskptr, input, idata, count, client_properties);\n+\t\tmay_have_null = ConvertNested<py::dict, duckdb_py_convert::StructConvert>(append_data);\n \t\tbreak;\n \tcase LogicalTypeId::UUID:\n-\t\tmay_have_null = ConvertColumn<hugeint_t, PyObject *, duckdb_py_convert::UUIDConvert>(current_offset, dataptr,\n-\t\t                                                                                     maskptr, idata, count);\n+\t\tmay_have_null = ConvertColumn<hugeint_t, PyObject *, duckdb_py_convert::UUIDConvert>(append_data);\n \t\tbreak;\n \n \tdefault:\n@@ -788,44 +678,4 @@ py::object ArrayWrapper::ToArray(idx_t count) const {\n \treturn masked_array;\n }\n \n-NumpyResultConversion::NumpyResultConversion(const vector<LogicalType> &types, idx_t initial_capacity,\n-                                             const ClientProperties &client_properties)\n-    : count(0), capacity(0) {\n-\towned_data.reserve(types.size());\n-\tfor (auto &type : types) {\n-\t\towned_data.emplace_back(type, client_properties);\n-\t}\n-\tResize(initial_capacity);\n-}\n-\n-void NumpyResultConversion::Resize(idx_t new_capacity) {\n-\tif (capacity == 0) {\n-\t\tfor (auto &data : owned_data) {\n-\t\t\tdata.Initialize(new_capacity);\n-\t\t}\n-\t} else {\n-\t\tfor (auto &data : owned_data) {\n-\t\t\tdata.Resize(new_capacity);\n-\t\t}\n-\t}\n-\tcapacity = new_capacity;\n-}\n-\n-void NumpyResultConversion::Append(DataChunk &chunk) {\n-\tif (count + chunk.size() > capacity) {\n-\t\tResize(capacity * 2);\n-\t}\n-\tauto chunk_types = chunk.GetTypes();\n-\tfor (idx_t col_idx = 0; col_idx < owned_data.size(); col_idx++) {\n-\t\towned_data[col_idx].Append(count, chunk.data[col_idx], chunk.size());\n-\t}\n-\tcount += chunk.size();\n-#ifdef DEBUG\n-\tfor (auto &data : owned_data) {\n-\t\tD_ASSERT(data.data->count == count);\n-\t\tD_ASSERT(data.mask->count == count);\n-\t}\n-#endif\n-}\n-\n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/numpy/numpy_result_conversion.cpp b/tools/pythonpkg/src/numpy/numpy_result_conversion.cpp\nnew file mode 100644\nindex 000000000000..b8d832e0ea53\n--- /dev/null\n+++ b/tools/pythonpkg/src/numpy/numpy_result_conversion.cpp\n@@ -0,0 +1,46 @@\n+#include \"duckdb_python/numpy/array_wrapper.hpp\"\n+#include \"duckdb_python/numpy/numpy_result_conversion.hpp\"\n+\n+namespace duckdb {\n+\n+NumpyResultConversion::NumpyResultConversion(const vector<LogicalType> &types, idx_t initial_capacity,\n+                                             const ClientProperties &client_properties, bool pandas)\n+    : count(0), capacity(0) {\n+\towned_data.reserve(types.size());\n+\tfor (auto &type : types) {\n+\t\towned_data.emplace_back(type, client_properties, pandas);\n+\t}\n+\tResize(initial_capacity);\n+}\n+\n+void NumpyResultConversion::Resize(idx_t new_capacity) {\n+\tif (capacity == 0) {\n+\t\tfor (auto &data : owned_data) {\n+\t\t\tdata.Initialize(new_capacity);\n+\t\t}\n+\t} else {\n+\t\tfor (auto &data : owned_data) {\n+\t\t\tdata.Resize(new_capacity);\n+\t\t}\n+\t}\n+\tcapacity = new_capacity;\n+}\n+\n+void NumpyResultConversion::Append(DataChunk &chunk) {\n+\tif (count + chunk.size() > capacity) {\n+\t\tResize(capacity * 2);\n+\t}\n+\tauto chunk_types = chunk.GetTypes();\n+\tfor (idx_t col_idx = 0; col_idx < owned_data.size(); col_idx++) {\n+\t\towned_data[col_idx].Append(count, chunk.data[col_idx], chunk.size());\n+\t}\n+\tcount += chunk.size();\n+#ifdef DEBUG\n+\tfor (auto &data : owned_data) {\n+\t\tD_ASSERT(data.data->count == count);\n+\t\tD_ASSERT(data.mask->count == count);\n+\t}\n+#endif\n+}\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/numpy/raw_array_wrapper.cpp b/tools/pythonpkg/src/numpy/raw_array_wrapper.cpp\nnew file mode 100644\nindex 000000000000..f3166acdcd2d\n--- /dev/null\n+++ b/tools/pythonpkg/src/numpy/raw_array_wrapper.cpp\n@@ -0,0 +1,162 @@\n+#include \"duckdb_python/numpy/raw_array_wrapper.hpp\"\n+#include \"duckdb/common/types/date.hpp\"\n+#include \"duckdb/common/types/hugeint.hpp\"\n+#include \"duckdb/common/types/time.hpp\"\n+#include \"duckdb/common/types/timestamp.hpp\"\n+#include \"utf8proc_wrapper.hpp\"\n+#include \"duckdb/common/types/interval.hpp\"\n+#include \"duckdb_python/pyrelation.hpp\"\n+#include \"duckdb_python/python_objects.hpp\"\n+#include \"duckdb_python/pyconnection/pyconnection.hpp\"\n+#include \"duckdb_python/pyresult.hpp\"\n+#include \"duckdb/common/types/uuid.hpp\"\n+\n+namespace duckdb {\n+\n+RawArrayWrapper::RawArrayWrapper(const LogicalType &type) : data(nullptr), type(type), count(0) {\n+\tswitch (type.id()) {\n+\tcase LogicalTypeId::BOOLEAN:\n+\t\ttype_width = sizeof(bool);\n+\t\tbreak;\n+\tcase LogicalTypeId::UTINYINT:\n+\t\ttype_width = sizeof(uint8_t);\n+\t\tbreak;\n+\tcase LogicalTypeId::USMALLINT:\n+\t\ttype_width = sizeof(uint16_t);\n+\t\tbreak;\n+\tcase LogicalTypeId::UINTEGER:\n+\t\ttype_width = sizeof(uint32_t);\n+\t\tbreak;\n+\tcase LogicalTypeId::UBIGINT:\n+\t\ttype_width = sizeof(uint64_t);\n+\t\tbreak;\n+\tcase LogicalTypeId::TINYINT:\n+\t\ttype_width = sizeof(int8_t);\n+\t\tbreak;\n+\tcase LogicalTypeId::SMALLINT:\n+\t\ttype_width = sizeof(int16_t);\n+\t\tbreak;\n+\tcase LogicalTypeId::INTEGER:\n+\t\ttype_width = sizeof(int32_t);\n+\t\tbreak;\n+\tcase LogicalTypeId::BIGINT:\n+\t\ttype_width = sizeof(int64_t);\n+\t\tbreak;\n+\tcase LogicalTypeId::FLOAT:\n+\t\ttype_width = sizeof(float);\n+\t\tbreak;\n+\tcase LogicalTypeId::HUGEINT:\n+\tcase LogicalTypeId::DOUBLE:\n+\tcase LogicalTypeId::DECIMAL:\n+\t\ttype_width = sizeof(double);\n+\t\tbreak;\n+\tcase LogicalTypeId::TIMESTAMP:\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\tcase LogicalTypeId::TIMESTAMP_NS:\n+\tcase LogicalTypeId::DATE:\n+\tcase LogicalTypeId::INTERVAL:\n+\tcase LogicalTypeId::TIMESTAMP_TZ:\n+\t\ttype_width = sizeof(int64_t);\n+\t\tbreak;\n+\tcase LogicalTypeId::TIME:\n+\tcase LogicalTypeId::TIME_TZ:\n+\tcase LogicalTypeId::VARCHAR:\n+\tcase LogicalTypeId::BIT:\n+\tcase LogicalTypeId::BLOB:\n+\tcase LogicalTypeId::ENUM:\n+\tcase LogicalTypeId::LIST:\n+\tcase LogicalTypeId::MAP:\n+\tcase LogicalTypeId::STRUCT:\n+\tcase LogicalTypeId::UNION:\n+\tcase LogicalTypeId::UUID:\n+\t\ttype_width = sizeof(PyObject *);\n+\t\tbreak;\n+\tdefault:\n+\t\tthrow NotImplementedException(\"Unsupported type \\\"%s\\\" for DuckDB -> NumPy conversion\", type.ToString());\n+\t}\n+}\n+\n+string RawArrayWrapper::DuckDBToNumpyDtype(const LogicalType &type) {\n+\tswitch (type.id()) {\n+\tcase LogicalTypeId::BOOLEAN:\n+\t\treturn \"bool\";\n+\tcase LogicalTypeId::TINYINT:\n+\t\treturn \"int8\";\n+\tcase LogicalTypeId::SMALLINT:\n+\t\treturn \"int16\";\n+\tcase LogicalTypeId::INTEGER:\n+\t\treturn \"int32\";\n+\tcase LogicalTypeId::BIGINT:\n+\t\treturn \"int64\";\n+\tcase LogicalTypeId::UTINYINT:\n+\t\treturn \"uint8\";\n+\tcase LogicalTypeId::USMALLINT:\n+\t\treturn \"uint16\";\n+\tcase LogicalTypeId::UINTEGER:\n+\t\treturn \"uint32\";\n+\tcase LogicalTypeId::UBIGINT:\n+\t\treturn \"uint64\";\n+\tcase LogicalTypeId::FLOAT:\n+\t\treturn \"float32\";\n+\tcase LogicalTypeId::HUGEINT:\n+\tcase LogicalTypeId::DOUBLE:\n+\tcase LogicalTypeId::DECIMAL:\n+\t\treturn \"float64\";\n+\tcase LogicalTypeId::TIMESTAMP:\n+\t\treturn \"datetime64[us]\";\n+\tcase LogicalTypeId::TIMESTAMP_TZ:\n+\t\treturn \"datetime64[us]\";\n+\tcase LogicalTypeId::TIMESTAMP_NS:\n+\t\treturn \"datetime64[ns]\";\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\t\treturn \"datetime64[ms]\";\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\t\treturn \"datetime64[s]\";\n+\tcase LogicalTypeId::DATE:\n+\t\t// FIXME: should this not be 'date64[ns]' ?\n+\t\treturn \"datetime64[ns]\";\n+\tcase LogicalTypeId::INTERVAL:\n+\t\treturn \"timedelta64[ns]\";\n+\tcase LogicalTypeId::TIME:\n+\tcase LogicalTypeId::TIME_TZ:\n+\tcase LogicalTypeId::VARCHAR:\n+\tcase LogicalTypeId::BIT:\n+\tcase LogicalTypeId::BLOB:\n+\tcase LogicalTypeId::LIST:\n+\tcase LogicalTypeId::MAP:\n+\tcase LogicalTypeId::STRUCT:\n+\tcase LogicalTypeId::UNION:\n+\tcase LogicalTypeId::UUID:\n+\t\treturn \"object\";\n+\tcase LogicalTypeId::ENUM: {\n+\t\tauto size = EnumType::GetSize(type);\n+\t\tif (size <= (idx_t)NumericLimits<int8_t>::Maximum()) {\n+\t\t\treturn \"int8\";\n+\t\t} else if (size <= (idx_t)NumericLimits<int16_t>::Maximum()) {\n+\t\t\treturn \"int16\";\n+\t\t} else if (size <= (idx_t)NumericLimits<int32_t>::Maximum()) {\n+\t\t\treturn \"int32\";\n+\t\t} else {\n+\t\t\tthrow InternalException(\"Size not supported on ENUM types\");\n+\t\t}\n+\t}\n+\tdefault:\n+\t\tthrow NotImplementedException(\"Unsupported type \\\"%s\\\"\", type.ToString());\n+\t}\n+}\n+\n+void RawArrayWrapper::Initialize(idx_t capacity) {\n+\tstring dtype = DuckDBToNumpyDtype(type);\n+\n+\tarray = py::array(py::dtype(dtype), capacity);\n+\tdata = data_ptr_cast(array.mutable_data());\n+}\n+\n+void RawArrayWrapper::Resize(idx_t new_capacity) {\n+\tvector<py::ssize_t> new_shape {py::ssize_t(new_capacity)};\n+\tarray.resize(new_shape, false);\n+\tdata = data_ptr_cast(array.mutable_data());\n+}\n+\n+} // namespace duckdb\ndiff --git a/tools/pythonpkg/src/pandas/bind.cpp b/tools/pythonpkg/src/pandas/bind.cpp\nindex c4204ae2264e..32de79df2222 100644\n--- a/tools/pythonpkg/src/pandas/bind.cpp\n+++ b/tools/pythonpkg/src/pandas/bind.cpp\n@@ -1,5 +1,4 @@\n #include \"duckdb_python/pandas/pandas_bind.hpp\"\n-#include \"duckdb_python/numpy/array_wrapper.hpp\"\n #include \"duckdb_python/pandas/pandas_analyzer.hpp\"\n #include \"duckdb_python/pandas/column/pandas_numpy_column.hpp\"\n \ndiff --git a/tools/pythonpkg/src/pyresult.cpp b/tools/pythonpkg/src/pyresult.cpp\nindex e5e7b944f0c9..aa4c316a1761 100644\n--- a/tools/pythonpkg/src/pyresult.cpp\n+++ b/tools/pythonpkg/src/pyresult.cpp\n@@ -163,12 +163,11 @@ void InsertCategory(QueryResult &result, unordered_map<idx_t, py::list> &categor\n \t}\n }\n \n-py::dict DuckDBPyResult::FetchNumpyInternal(bool stream, idx_t vectors_per_chunk) {\n+unique_ptr<NumpyResultConversion> DuckDBPyResult::InitializeNumpyConversion(bool pandas) {\n \tif (!result) {\n \t\tthrow InvalidInputException(\"result closed\");\n \t}\n \n-\t// iterate over the result to materialize the data needed for the NumPy arrays\n \tidx_t initial_capacity = STANDARD_VECTOR_SIZE * 2ULL;\n \tif (result->type == QueryResultType::MATERIALIZED_RESULT) {\n \t\t// materialized query result: we know exactly how much space we need\n@@ -176,7 +175,21 @@ py::dict DuckDBPyResult::FetchNumpyInternal(bool stream, idx_t vectors_per_chunk\n \t\tinitial_capacity = materialized.RowCount();\n \t}\n \n-\tNumpyResultConversion conversion(result->types, initial_capacity, result->client_properties);\n+\tauto conversion =\n+\t    make_uniq<NumpyResultConversion>(result->types, initial_capacity, result->client_properties, pandas);\n+\treturn std::move(conversion);\n+}\n+\n+py::dict DuckDBPyResult::FetchNumpyInternal(bool stream, idx_t vectors_per_chunk,\n+                                            unique_ptr<NumpyResultConversion> conversion_p) {\n+\tif (!result) {\n+\t\tthrow InvalidInputException(\"result closed\");\n+\t}\n+\tif (!conversion_p) {\n+\t\tconversion_p = InitializeNumpyConversion();\n+\t}\n+\tauto &conversion = *conversion_p;\n+\n \tif (result->type == QueryResultType::MATERIALIZED_RESULT) {\n \t\tauto &materialized = result->Cast<MaterializedQueryResult>();\n \t\tfor (auto &chunk : materialized.Collection().Chunks()) {\n@@ -259,11 +272,13 @@ PandasDataFrame DuckDBPyResult::FrameFromNumpy(bool date_as_object, const py::ha\n }\n \n PandasDataFrame DuckDBPyResult::FetchDF(bool date_as_object) {\n-\treturn FrameFromNumpy(date_as_object, FetchNumpyInternal());\n+\tauto conversion = InitializeNumpyConversion(true);\n+\treturn FrameFromNumpy(date_as_object, FetchNumpyInternal(false, 1, std::move(conversion)));\n }\n \n PandasDataFrame DuckDBPyResult::FetchDFChunk(idx_t num_of_vectors, bool date_as_object) {\n-\treturn FrameFromNumpy(date_as_object, FetchNumpyInternal(true, num_of_vectors));\n+\tauto conversion = InitializeNumpyConversion(true);\n+\treturn FrameFromNumpy(date_as_object, FetchNumpyInternal(true, num_of_vectors, std::move(conversion)));\n }\n \n py::dict DuckDBPyResult::FetchPyTorch() {\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/api/test_dbapi08.py b/tools/pythonpkg/tests/fast/api/test_dbapi08.py\nindex 70272b76b769..a81acfd1d54d 100644\n--- a/tools/pythonpkg/tests/fast/api/test_dbapi08.py\n+++ b/tools/pythonpkg/tests/fast/api/test_dbapi08.py\n@@ -2,7 +2,7 @@\n import numpy\n import pytest\n import duckdb\n-from conftest import NumpyPandas, ArrowPandas\n+from conftest import NumpyPandas\n \n \n class TestType(object):\n@@ -14,9 +14,8 @@ def test_fetchdf(self, pandas):\n         res = con.execute(\"SELECT item FROM items\").fetchdf()\n         assert isinstance(res, pandas.core.frame.DataFrame)\n \n-        arr = numpy.ma.masked_array(['jeans', '', None])\n-        arr.mask = [False, False, True]\n-        arr = {'item': arr}\n-        df = pandas.DataFrame(arr)\n+        df = pandas.DataFrame({'item': ['jeans', '', None]})\n \n+        print(res)\n+        print(df)\n         pandas.testing.assert_frame_equal(res, df)\ndiff --git a/tools/pythonpkg/tests/fast/types/test_nan.py b/tools/pythonpkg/tests/fast/types/test_nan.py\nindex ab244f7b2187..4aa8544382b2 100644\n--- a/tools/pythonpkg/tests/fast/types/test_nan.py\n+++ b/tools/pythonpkg/tests/fast/types/test_nan.py\n@@ -2,12 +2,12 @@\n import datetime\n import duckdb\n import pytest\n-from conftest import NumpyPandas, ArrowPandas\n+\n+pandas = pytest.importorskip(\"pandas\")\n \n \n class TestPandasNaN(object):\n-    @pytest.mark.parametrize('pandas', [NumpyPandas(), ArrowPandas()])\n-    def test_pandas_nan(self, duckdb_cursor, pandas):\n+    def test_pandas_nan(self, duckdb_cursor):\n         # create a DataFrame with some basic values\n         df = pandas.DataFrame([{\"col1\": \"val1\", \"col2\": 1.05}, {\"col1\": \"val3\", \"col2\": np.NaN}])\n         # create a new column (newcol1) that includes either NaN or values from col1\n@@ -33,13 +33,16 @@ def test_pandas_nan(self, duckdb_cursor, pandas):\n         assert results[1][2] == 'val3'\n         assert results[1][3] == current_time\n \n-        # now fetch the results as a df:\n+        # now fetch the results as numpy:\n+        result_np = conn.execute('select * from testing_null_values').fetchnumpy()\n+        assert result_np['col1'][0] == df['col1'][0]\n+        assert result_np['col1'][1] == df['col1'][1]\n+        assert result_np['col2'][0] == df['col2'][0]\n+\n+        assert result_np['col2'].mask[1]\n+        assert result_np['newcol1'].mask[0]\n+        assert result_np['newcol1'][1] == df['newcol1'][1]\n+\n         result_df = conn.execute('select * from testing_null_values').fetchdf()\n-        assert result_df['col1'][0] == df['col1'][0]\n-        assert result_df['col1'][1] == df['col1'][1]\n-        assert result_df['col2'][0] == df['col2'][0]\n-        assert np.isnan(result_df['col2'][1])\n-        assert np.isnan(result_df['newcol1'][0])\n-        assert result_df['newcol1'][1] == df['newcol1'][1]\n         assert pandas.isnull(result_df['datetest'][0])\n         assert result_df['datetest'][1] == df['datetest'][1]\n",
  "problem_statement": "Exporting string columns to pandas gives NaNs for NULL values\n### What happens?\n\nNull values for `VARCHAR` columns in duckdb become `NaN` in pandas. They should become `None`, which is the default missing value for string data. e.g. \r\n\r\n```python\r\nimport pandas as pd\r\n\r\npd.DataFrame([['Amsterdam', 1], [None, None]])\r\n```\n\n### To Reproduce\n\n\r\n```python\r\nimport duckdb\r\n\r\nduckdb.sql(\"SELECT * FROM (VALUES ('Amsterdam', 1), (NULL, NULL))\").df()\r\n```\n\n### OS:\n\nmacOS Ventura 13.4 \n\n### DuckDB Version:\n\n0.8.1\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nMahesh Vashishtha\n\n### Affiliation:\n\nPonder Data\n\n### Have you tried this on the latest `main` branch?\n\nI have tested with a main build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "Our \"NullValue\" for VARCHAR is `nullptr`, we create a masked numpy array and that is later converted to Pandas.\r\nBecause of how pandas processes masked arrays *they* are the ones that convert this to NaN instead of None, regardless of what we set as the value\r\n\r\nFeel free to experiment:\r\n`tools/pythonpkg/src/numpy/array_wrapper.cpp` line 209\r\n\nIndeed this is expected behaviour and not something we can do a lot about. Pandas' NULL handling is quite problematic.\n> Our \"NullValue\" for VARCHAR is nullptr, we create a masked numpy array and that is later converted to Pandas.\r\n\r\n@Tishj thank you for the explanation!\r\n\r\n> Indeed this is expected behaviour\r\n\r\n@hannes I disagree, and I suggest that we reopen this issue. I am a duckdb user and I did not expect this. I have used the pandas integrations of a few different database connectors in python and have so far only had this problem with duckdb. None of sqlite, snowflake-connector-python, pyscopg3, the google bigquery connector, the pandas API on spark, or the redshift python connector, for example, have had this bug.\r\n\r\nThe behavior might be expected if you know that duckdb is trying to create pandas dataframes out of masked numpy arrays, and that pandas turns missing values in masked numpy arrays into `NaN`, but that doesn't make it desirable.\r\n\r\nTo start with addressing this issue, I suggest:\r\n\r\n1) Open an issue with pandas about the behavior that we think may be a bug. I have opened for that. I have filed for https://github.com/pandas-dev/pandas/issues/54706\r\n2) Meanwhile, see how much work it would take to work around 1), which may turn out to be expected behavior in pandas",
  "created_at": "2023-08-31T10:29:44Z"
}