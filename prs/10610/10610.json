{
  "repo": "duckdb/duckdb",
  "pull_number": 10610,
  "instance_id": "duckdb__duckdb-10610",
  "issue_numbers": [
    "10608"
  ],
  "base_commit": "d745e29fd1aeecdbdb48d9539067938903e56436",
  "patch": "diff --git a/src/function/table/arrow/arrow_array_scan_state.cpp b/src/function/table/arrow/arrow_array_scan_state.cpp\nindex 88aa49bfa1db..0e60b470f352 100644\n--- a/src/function/table/arrow/arrow_array_scan_state.cpp\n+++ b/src/function/table/arrow/arrow_array_scan_state.cpp\n@@ -1,8 +1,11 @@\n #include \"duckdb/function/table/arrow.hpp\"\n+#include \"duckdb/common/printer.hpp\"\n+#include \"duckdb/common/types/arrow_aux_data.hpp\"\n \n namespace duckdb {\n \n ArrowArrayScanState::ArrowArrayScanState(ArrowScanLocalState &state) : state(state) {\n+\tarrow_dictionary = nullptr;\n }\n \n ArrowArrayScanState &ArrowArrayScanState::GetChild(idx_t child_idx) {\n@@ -10,20 +13,43 @@ ArrowArrayScanState &ArrowArrayScanState::GetChild(idx_t child_idx) {\n \tif (it == children.end()) {\n \t\tauto child_p = make_uniq<ArrowArrayScanState>(state);\n \t\tauto &child = *child_p;\n+\t\tchild.owned_data = owned_data;\n \t\tchildren.emplace(std::make_pair(child_idx, std::move(child_p)));\n \t\treturn child;\n \t}\n+\tif (!it->second->owned_data) {\n+\t\t// Propagate down the ownership, for dictionaries in children\n+\t\tD_ASSERT(owned_data);\n+\t\tit->second->owned_data = owned_data;\n+\t}\n \treturn *it->second;\n }\n \n-void ArrowArrayScanState::AddDictionary(unique_ptr<Vector> dictionary_p) {\n-\tthis->dictionary = std::move(dictionary_p);\n+void ArrowArrayScanState::AddDictionary(unique_ptr<Vector> dictionary_p, ArrowArray *arrow_dict) {\n+\tdictionary = std::move(dictionary_p);\n+\tD_ASSERT(owned_data);\n+\tD_ASSERT(arrow_dict);\n+\tarrow_dictionary = arrow_dict;\n+\t// Make sure the data referenced by the dictionary stays alive\n+\tdictionary->GetBuffer()->SetAuxiliaryData(make_uniq<ArrowAuxiliaryData>(owned_data));\n }\n \n bool ArrowArrayScanState::HasDictionary() const {\n \treturn dictionary != nullptr;\n }\n \n+bool ArrowArrayScanState::CacheOutdated(ArrowArray *dictionary) const {\n+\tif (!dictionary) {\n+\t\t// Not cached\n+\t\treturn true;\n+\t}\n+\tif (dictionary == arrow_dictionary.get()) {\n+\t\t// Already cached, not outdated\n+\t\treturn false;\n+\t}\n+\treturn true;\n+}\n+\n Vector &ArrowArrayScanState::GetDictionary() {\n \tD_ASSERT(HasDictionary());\n \treturn *dictionary;\ndiff --git a/src/function/table/arrow_conversion.cpp b/src/function/table/arrow_conversion.cpp\nindex 40915d5dd8a0..d3289f475574 100644\n--- a/src/function/table/arrow_conversion.cpp\n+++ b/src/function/table/arrow_conversion.cpp\n@@ -588,9 +588,6 @@ static void ColumnArrowToDuckDBRunEndEncoded(Vector &vector, ArrowArray &array,\n static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArrayScanState &array_state, idx_t size,\n                                 const ArrowType &arrow_type, int64_t nested_offset, ValidityMask *parent_mask,\n                                 uint64_t parent_offset) {\n-\tif (parent_offset != 0) {\n-\t\t(void)array_state;\n-\t}\n \tauto &scan_state = array_state.state;\n \tD_ASSERT(!array.dictionary);\n \n@@ -1070,10 +1067,10 @@ static bool CanContainNull(ArrowArray &array, ValidityMask *parent_mask) {\n static void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, ArrowArrayScanState &array_state,\n                                           idx_t size, const ArrowType &arrow_type, int64_t nested_offset,\n                                           ValidityMask *parent_mask, uint64_t parent_offset) {\n+\tD_ASSERT(arrow_type.HasDictionary());\n \tauto &scan_state = array_state.state;\n-\n \tconst bool has_nulls = CanContainNull(array, parent_mask);\n-\tif (!array_state.HasDictionary()) {\n+\tif (array_state.CacheOutdated(array.dictionary)) {\n \t\t//! We need to set the dictionary data for this column\n \t\tauto base_vector = make_uniq<Vector>(vector.GetType(), array.dictionary->length);\n \t\tSetValidityMask(*base_vector, *array.dictionary, scan_state, array.dictionary->length, 0, 0, has_nulls);\n@@ -1095,7 +1092,7 @@ static void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, Arr\n \t\tdefault:\n \t\t\tthrow NotImplementedException(\"ArrowArrayPhysicalType not recognized\");\n \t\t};\n-\t\tarray_state.AddDictionary(std::move(base_vector));\n+\t\tarray_state.AddDictionary(std::move(base_vector), array.dictionary);\n \t}\n \tauto offset_type = arrow_type.GetDuckType();\n \t//! Get Pointer to Indices of Dictionary\n@@ -1120,6 +1117,7 @@ static void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, Arr\n \t\tSetSelectionVector(sel, indices, offset_type, size);\n \t}\n \tvector.Slice(array_state.GetDictionary(), sel, size);\n+\tvector.Verify(size);\n }\n \n void ArrowTableFunction::ArrowToDuckDB(ArrowScanLocalState &scan_state, const arrow_column_map_t &arrow_convert_data,\n@@ -1144,20 +1142,17 @@ void ArrowTableFunction::ArrowToDuckDB(ArrowScanLocalState &scan_state, const ar\n \t\tif (array.length != scan_state.chunk->arrow_array.length) {\n \t\t\tthrow InvalidInputException(\"arrow_scan: array length mismatch\");\n \t\t}\n-\t\t// Make sure this Vector keeps the Arrow chunk alive in case we can zero-copy the data\n-\t\tif (scan_state.arrow_owned_data.find(idx) == scan_state.arrow_owned_data.end()) {\n-\t\t\tauto arrow_data = make_shared<ArrowArrayWrapper>();\n-\t\t\tarrow_data->arrow_array = scan_state.chunk->arrow_array;\n-\t\t\tscan_state.chunk->arrow_array.release = nullptr;\n-\t\t\tscan_state.arrow_owned_data[idx] = arrow_data;\n-\t\t}\n-\n-\t\toutput.data[idx].GetBuffer()->SetAuxiliaryData(make_uniq<ArrowAuxiliaryData>(scan_state.arrow_owned_data[idx]));\n \n \t\tD_ASSERT(arrow_convert_data.find(col_idx) != arrow_convert_data.end());\n \t\tauto &arrow_type = *arrow_convert_data.at(col_idx);\n \t\tauto &array_state = scan_state.GetState(col_idx);\n \n+\t\t// Make sure this Vector keeps the Arrow chunk alive in case we can zero-copy the data\n+\t\tif (!array_state.owned_data) {\n+\t\t\tarray_state.owned_data = scan_state.chunk;\n+\t\t}\n+\t\toutput.data[idx].GetBuffer()->SetAuxiliaryData(make_uniq<ArrowAuxiliaryData>(array_state.owned_data));\n+\n \t\tauto array_physical_type = GetArrowArrayPhysicalType(arrow_type);\n \n \t\tswitch (array_physical_type) {\ndiff --git a/src/include/duckdb/function/table/arrow.hpp b/src/include/duckdb/function/table/arrow.hpp\nindex ce923906e80c..d74e4f4d1b5e 100644\n--- a/src/include/duckdb/function/table/arrow.hpp\n+++ b/src/include/duckdb/function/table/arrow.hpp\n@@ -86,7 +86,11 @@ struct ArrowArrayScanState {\n \n public:\n \tArrowScanLocalState &state;\n+\t// Hold ownership over the Arrow Arrays owned by DuckDB to allow for zero-copy\n+\tshared_ptr<ArrowArrayWrapper> owned_data;\n \tunordered_map<idx_t, unique_ptr<ArrowArrayScanState>> children;\n+\t// Optionally holds the pointer that was used to create the cached dictionary\n+\toptional_ptr<ArrowArray> arrow_dictionary = nullptr;\n \t// Cache the (optional) dictionary of this array\n \tunique_ptr<Vector> dictionary;\n \t//! Run-end-encoding state\n@@ -94,8 +98,9 @@ struct ArrowArrayScanState {\n \n public:\n \tArrowArrayScanState &GetChild(idx_t child_idx);\n-\tvoid AddDictionary(unique_ptr<Vector> dictionary_p);\n+\tvoid AddDictionary(unique_ptr<Vector> dictionary_p, ArrowArray *arrow_dict);\n \tbool HasDictionary() const;\n+\tbool CacheOutdated(ArrowArray *dictionary) const;\n \tVector &GetDictionary();\n \tArrowRunEndEncodingState &RunEndEncoding() {\n \t\treturn run_end_encoding;\n@@ -106,6 +111,10 @@ struct ArrowArrayScanState {\n \t\t// Note: dictionary is not reset\n \t\t// the dictionary should be the same for every array scanned of this column\n \t\trun_end_encoding.Reset();\n+\t\tfor (auto &child : children) {\n+\t\t\tchild.second->Reset();\n+\t\t}\n+\t\towned_data.reset();\n \t}\n };\n \n@@ -117,9 +126,6 @@ struct ArrowScanLocalState : public LocalTableFunctionState {\n public:\n \tunique_ptr<ArrowArrayStreamWrapper> stream;\n \tshared_ptr<ArrowArrayWrapper> chunk;\n-\t// This vector hold the Arrow Vectors owned by DuckDB to allow for zero-copy\n-\t// Note that only DuckDB can release these vectors\n-\tunordered_map<idx_t, shared_ptr<ArrowArrayWrapper>> arrow_owned_data;\n \tidx_t chunk_offset = 0;\n \tidx_t batch_index = 0;\n \tvector<column_t> column_ids;\n",
  "test_patch": "diff --git a/scripts/regression_test_python.py b/scripts/regression_test_python.py\nindex 2bef5c9a2e0e..9acfa9337dc6 100644\n--- a/scripts/regression_test_python.py\n+++ b/scripts/regression_test_python.py\n@@ -90,7 +90,7 @@ def load_lineitem(self, collector) -> List[BenchmarkResult]:\n         return results\n \n \n-class TPCHTester:\n+class TPCHBenchmarker:\n     def __init__(self, name: str):\n         self.initialize_connection()\n         self.name = name\n@@ -131,7 +131,7 @@ def run_tpch(self, collector) -> List[BenchmarkResult]:\n         return results\n \n \n-def main():\n+def test_tpch():\n     print_msg(f\"Generating TPCH (sf={scale_factor})\")\n     tpch = TPCHData(scale_factor)\n \n@@ -171,7 +171,7 @@ def convert_arrow(conn: duckdb.DuckDBPyConnection, table_name: str):\n     # Convert TPCH data to the right format, then run TPCH queries on that data\n     for convertor in CONVERTORS:\n         tables = tpch.get_tables(CONVERTORS[convertor])\n-        tester = TPCHTester(convertor)\n+        tester = TPCHBenchmarker(convertor)\n         tester.register_tables(tables)\n         collector = COLLECTORS[convertor]\n         results: List[BenchmarkResult] = tester.run_tpch(collector)\n@@ -180,6 +180,89 @@ def convert_arrow(conn: duckdb.DuckDBPyConnection, table_name: str):\n             run_number = res.run_number\n             duration = res.duration\n             write_result(benchmark_name, run_number, duration)\n+\n+\n+def generate_string(seed: int):\n+    output = ''\n+    for _ in range(10):\n+        output += chr(ord('A') + int(seed % 26))\n+        seed /= 26\n+    return output\n+\n+\n+class ArrowDictionary:\n+    def __init__(self, unique_values):\n+        self.size = unique_values\n+        self.dict = [generate_string(x) for x in range(unique_values)]\n+\n+\n+class ArrowDictionaryBenchmark:\n+    def __init__(self, unique_values, values, arrow_dict: ArrowDictionary):\n+        assert unique_values <= arrow_dict.size\n+        self.initialize_connection()\n+        self.generate(unique_values, values, arrow_dict)\n+\n+    def initialize_connection(self):\n+        self.con = duckdb.connect()\n+        if not threads:\n+            return\n+        print_msg(f'Limiting threads to {threads}')\n+        self.con.execute(f\"SET threads={threads}\")\n+\n+    def generate(self, unique_values, values, arrow_dict: ArrowDictionary):\n+        self.input = []\n+        self.expected = []\n+        for x in range(values):\n+            value = arrow_dict.dict[x % unique_values]\n+            self.input.append(value)\n+            self.expected.append((value,))\n+\n+        array = pa.array(\n+            self.input,\n+            type=pa.dictionary(pa.int64(), pa.string()),\n+        )\n+        self.table = pa.table([array], names=[\"x\"])\n+\n+    def benchmark(self) -> List[BenchmarkResult]:\n+        self.con.register('arrow_table', self.table)\n+        results = []\n+        for nrun in range(nruns):\n+            duration = 0.0\n+            start = time.time()\n+            res = self.con.execute(\n+                \"\"\"\n+                select * from arrow_table\n+            \"\"\"\n+            ).fetchall()\n+            end = time.time()\n+            duration = float(end - start)\n+            assert self.expected == res\n+            del res\n+            padding = \" \" * len(str(nruns))\n+            print_msg(f\"T{padding}: {duration}s\")\n+            results.append(BenchmarkResult(duration, nrun))\n+        return results\n+\n+\n+def test_arrow_dictionaries_scan():\n+    DICT_SIZE = 26 * 1000\n+    print_msg(f\"Generating a unique dictionary of size {DICT_SIZE}\")\n+    arrow_dict = ArrowDictionary(DICT_SIZE)\n+    DATASET_SIZE = 10000000\n+    for unique_values in [2, 1000, DICT_SIZE]:\n+        test = ArrowDictionaryBenchmark(unique_values, DATASET_SIZE, arrow_dict)\n+        results = test.benchmark()\n+        benchmark_name = f\"arrow_dict_unique_{unique_values}_total_{DATASET_SIZE}\"\n+        for res in results:\n+            run_number = res.run_number\n+            duration = res.duration\n+            write_result(benchmark_name, run_number, duration)\n+\n+\n+def main():\n+    test_tpch()\n+    test_arrow_dictionaries_scan()\n+\n     close_result()\n \n \ndiff --git a/tools/pythonpkg/tests/fast/arrow/test_dictionary_arrow.py b/tools/pythonpkg/tests/fast/arrow/test_dictionary_arrow.py\nindex 8b51daca339b..0b10a9ba8af9 100644\n--- a/tools/pythonpkg/tests/fast/arrow/test_dictionary_arrow.py\n+++ b/tools/pythonpkg/tests/fast/arrow/test_dictionary_arrow.py\n@@ -4,6 +4,7 @@\n \n pa = pytest.importorskip(\"pyarrow\")\n pq = pytest.importorskip(\"pyarrow.parquet\")\n+ds = pytest.importorskip(\"pyarrow.dataset\")\n np = pytest.importorskip(\"numpy\")\n pd = pytest.importorskip(\"pandas\")\n import datetime\n@@ -154,6 +155,29 @@ def test_dictionary_batches(self, duckdb_cursor):\n         result = [(None, None), (100, 1), (None, None), (100, 1), (100, 2), (100, 1), (10, 0)] * 10000\n         assert rel.execute().fetchall() == result\n \n+    def test_dictionary_lifetime(self, duckdb_cursor):\n+        tables = []\n+        expected = ''\n+        for i in range(100):\n+            if i % 3 == 0:\n+                input = 'ABCD' * 17000\n+            elif i % 3 == 1:\n+                input = 'FOOO' * 17000\n+            else:\n+                input = 'BARR' * 17000\n+            expected += input\n+            array = pa.array(\n+                input,\n+                type=pa.dictionary(pa.int16(), pa.string()),\n+            )\n+            tables.append(pa.table([array], names=[\"x\"]))\n+        # All of the tables with different dictionaries are getting merged into one dataset\n+        # This is testing that our cache is being evicted correctly\n+        x = ds.dataset(tables)\n+        res = duckdb_cursor.sql(\"select * from x\").fetchall()\n+        expected = [(x,) for x in expected]\n+        assert res == expected\n+\n     def test_dictionary_batches_parallel(self, duckdb_cursor):\n         duckdb_cursor.execute(\"PRAGMA threads=4\")\n         duckdb_cursor.execute(\"PRAGMA verify_parallelism\")\n",
  "problem_statement": "Crash while scanning PyArrow Dataset\n### What happens?\n\nWhen trying to scan a large number of Feather files via `pyarrow.dataset` I get a segfault in `duckdb::StringHeap::AddBlob`:\r\n\r\n```\r\n#0  0x00007ffff6f27e85 in __memmove_avx_unaligned_erms () from /usr/lib64/libc.so.6\r\n#1  0x00007fffd6401c26 in duckdb::StringHeap::AddBlob(char const*, unsigned long) ()\r\n   from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#2  0x00007fffd644b997 in void duckdb::ColumnDataCopy<duckdb::string_t>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long, unsigned long)\r\n    () from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#3  0x00007fffd644df4a in duckdb::ColumnDataCollection::Append(duckdb::ColumnDataAppendState&, duckdb::DataChunk&) ()\r\n   from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#4  0x00007fffd697d415 in duckdb::PhysicalBatchCollector::Sink(duckdb::ExecutionContext&, duckdb::DataChunk&, duckdb::OperatorSinkInput&) const ()\r\n   from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#5  0x00007fffd6d7ef6a in duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long) ()\r\n   from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#6  0x00007fffd6d834e5 in duckdb::PipelineExecutor::Execute(unsigned long) ()\r\n   from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#7  0x00007fffd6d88529 in duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) ()\r\n   from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#8  0x00007fffd6d794f3 in duckdb::ExecutorTask::Execute(duckdb::TaskExecutionMode) ()\r\n   from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#9  0x00007fffd6d7ae85 in duckdb::TaskScheduler::ExecuteForever(std::atomic<bool>*) ()\r\n   from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#10 0x00007fffd70b29e0 in execute_native_thread_routine () from /home/maubury/.conda/envs/juggernaut/lib/python3.11/site-packages/duckdb/duckdb.cpython-311-x86_64-linux-gnu.so\r\n#11 0x00007ffff7bb71cf in start_thread () from /usr/lib64/libpthread.so.0\r\n#12 0x00007ffff6e91dd3 in clone () from /usr/lib64/libc.so.6\r\n```\n\n### To Reproduce\n\nThe code I'm executing is as follows:\r\n```\r\nimport duckdb\r\nimport pyarrow.dataset as ds\r\n\r\nfiles = [...]\r\nduckdb.arrow(ds.dataset(files, format=\"feather\")).arrow()\r\n```\r\nUnfortunately I'm unable to provide the files which trigger the crash :-(\r\n\r\nI can confirm that all files have the same schema, and going direct to a `pa.Table` does not crash.\r\n\n\n### OS:\n\nx64\n\n### DuckDB Version:\n\n0.9.2\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nMatt Aubury\n\n### Affiliation:\n\n.\n\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\n\nI have tested with a nightly build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "That sounds like a data ownership issue, heap-use-after-free if I'd have to guess\r\n\r\nI'd be curious what data you're consuming\r\n\r\nI also don't think the result collection method is required to reproduce this, can you try `.fetchall()` or fetchmany ?\nIt crashes with `.fetchall()`, and with `.fetchmany()` if I pass a large enough number.\r\n\r\nThe data is not very unusual, it's 5.6 million rows of `int64`, `string`, `float64`, `date32` columns, over 278 files.\r\n\r\nMy guess was that duckdb was interning the strings but somehow overflowed the reserved space?\nwhat do you mean by interning?\r\n\r\nThe part it crashes at is constructing the materialized result, which performs a copy\r\n\r\nBut until then it assumes it has (read only) ownership of the arrow data\nInternal dictionary encoding (see https://en.wikipedia.org/wiki/String_interning), but that was just a hypothesis, I don't know duckdb internals at all.\r\n\r\n\n@mattaubury that sounds like our dictionary vectortype but I don't think we produce that from an arrow scan\nOkay, I found a specific column that seemed to be causing issues, and managed to create a reproducer (at least on my machine...):\r\n```\r\nimport duckdb\r\nimport pyarrow.feather\r\nimport pyarrow.dataset as ds\r\nimport pyarrow as pa\r\nimport random\r\n\r\ntables = []\r\nfor i in range(100):\r\n    array = pa.array(\r\n        random.choice([\"ABCD\", \"FOOO\", \"BARR\"]) * 17000,\r\n        type=pa.dictionary(pa.int16(), pa.string()),\r\n    )\r\n    tables.append(pa.table([array], names=[\"x\"]))\r\n\r\nduckdb.arrow(ds.dataset(tables)).fetchall()\r\n```\r\nThis crashes with a segfault as before.\r\n\r\n[edit: skip creating feather, scanning tables in-memory shows the same issue]",
  "created_at": "2024-02-12T22:35:16Z"
}