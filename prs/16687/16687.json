{
  "repo": "duckdb/duckdb",
  "pull_number": 16687,
  "instance_id": "duckdb__duckdb-16687",
  "issue_numbers": [
    "16551"
  ],
  "base_commit": "005bdde63c008ab518b0030b35cc29986d1d6827",
  "patch": "diff --git a/.github/actions/build_extensions/action.yml b/.github/actions/build_extensions/action.yml\nindex db05fd4c9dfb..4b581851954b 100644\n--- a/.github/actions/build_extensions/action.yml\n+++ b/.github/actions/build_extensions/action.yml\n@@ -11,7 +11,7 @@ inputs:\n \n   # Deploy config\n   deploy_as:\n-    description: 'Binary architecture name for deploy step'\n+    description: 'Binary architecture name for deploy step - DEPRECATED'\n     default: ''\n   deploy_version:\n     description: 'Version tag or commit short hash for deploy step'\n@@ -217,35 +217,5 @@ runs:\n     - name: Deploy\n       if: ${{ inputs.deploy_as != '' }}\n       shell: bash\n-      env:\n-        AWS_ACCESS_KEY_ID: ${{ inputs.s3_id }}\n-        AWS_SECRET_ACCESS_KEY: ${{ inputs.s3_key }}\n-        DUCKDB_EXTENSION_SIGNING_PK: ${{ inputs.signing_pk }}\n-        AWS_DEFAULT_REGION: us-east-1\n-        DUCKDB_DEPLOY_SCRIPT_MODE: for_real\n       run: |\n-        cd  ${{ inputs.build_dir}}\n-        if [[ \"$GITHUB_REPOSITORY\" = \"duckdb/duckdb\" ]] ; then\n-          if [[ ! -z \"${{ inputs.deploy_version }}\" ]] ; then\n-            ./scripts/extension-upload-all.sh ${{ inputs.deploy_as }} ${{ inputs.deploy_version }}\n-          elif [[ \"$GITHUB_REF\" =~ ^(refs/tags/v.+)$ ]] ; then\n-            ./scripts/extension-upload-all.sh ${{ inputs.deploy_as }} ${{ github.ref_name }}\n-          elif [[ \"$GITHUB_REF\" =~ ^(refs/heads/main)$ ]] ; then\n-            ./scripts/extension-upload-all.sh ${{ inputs.deploy_as }} `git log -1 --format=%h`\n-          fi\n-        fi\n-\n-    # Run the unittests (excluding the out-of-tree tests) with the extensions that we deployed to S3\n-    - name: Test deployed extensions\n-      if: ${{ inputs.deploy_as != '' && inputs.run_tests == 1 }}\n-      shell: bash\n-      env:\n-        AWS_ACCESS_KEY_ID: ${{ inputs.s3_id }}\n-        AWS_SECRET_ACCESS_KEY: ${{ inputs.s3_key }}\n-        AWS_DEFAULT_REGION: us-east-1\n-      run: |\n-        rm -rf ~/.duckdb\n-        cd  ${{ inputs.build_dir}}\n-        if [[ \"$GITHUB_REF\" =~ ^(refs/heads/main|refs/tags/v.+)$ && \"$GITHUB_REPOSITORY\" = \"duckdb/duckdb\" ]] ; then\n-          ./scripts/extension-upload-test.sh\n-        fi\n+        exit 1\ndiff --git a/.github/actions/build_extensions_dockerized/action.yml b/.github/actions/build_extensions_dockerized/action.yml\nindex 651c0b6d72b1..0e0eb7fd54e4 100644\n--- a/.github/actions/build_extensions_dockerized/action.yml\n+++ b/.github/actions/build_extensions_dockerized/action.yml\n@@ -11,6 +11,9 @@ inputs:\n   vcpkg_target_triplet:\n     description: 'Target triplet for installing vcpkg dependencies'\n     default: ''\n+  override_git_describe:\n+    description: 'Override git describe'\n+    default: ''\n \n runs:\n   using: \"composite\"\n@@ -56,7 +59,7 @@ runs:\n           echo \"OPENSSL_DIR=/duckdb_build_dir/build/release/vcpkg_installed/${{ inputs.vcpkg_target_triplet }}\" >> docker_env.txt\n           echo \"OPENSSL_USE_STATIC_LIBS=true\" >> docker_env.txt\n           echo \"DUCKDB_PLATFORM=${{ inputs.duckdb_arch }}\" >> docker_env.txt\n-          echo \"DUCKDB_GIT_VERSION=${{ inputs.override_git_describe }}\" >> docker_env.txt\n+          echo \"OVERRIDE_GIT_DESCRIBE=${{ inputs.override_git_describe }}\" >> docker_env.txt\n           echo \"LINUX_CI_IN_DOCKER=1\" >> docker_env.txt\n           echo \"TOOLCHAIN_FLAGS=${{ inputs.duckdb_arch == 'linux_arm64' && '-DCMAKE_C_COMPILER=aarch64-linux-gnu-gcc -DCMAKE_CXX_COMPILER=aarch64-linux-gnu-g++ -DCMAKE_Fortran_COMPILER=aarch64-linux-gnu-gfortran' || '' }}\" >> docker_env.txt\n           echo \"CC=${{ inputs.duckdb_arch == 'linux_arm64' && 'aarch64-linux-gnu-gcc' || '' }}\" >> docker_env.txt\ndiff --git a/.github/config/out_of_tree_extensions.cmake b/.github/config/out_of_tree_extensions.cmake\nindex 0fb1762f3fa0..3f1a426101de 100644\n--- a/.github/config/out_of_tree_extensions.cmake\n+++ b/.github/config/out_of_tree_extensions.cmake\n@@ -111,7 +111,7 @@ if (NOT MINGW)\n duckdb_extension_load(spatial\n     DONT_LINK LOAD_TESTS\n     GIT_URL https://github.com/duckdb/duckdb-spatial\n-    GIT_TAG 919c69fe47443b4eafbd883e2fcac0b2ec448725\n+    GIT_TAG 2905968a85703e5ca3698976daafd759554e1744\n     INCLUDE_DIR spatial/include\n     TEST_DIR test/sql\n     APPLY_PATCHES\ndiff --git a/.github/workflows/InvokeCI.yml b/.github/workflows/InvokeCI.yml\nindex 8bbba55f2677..dd22bee5f529 100644\n--- a/.github/workflows/InvokeCI.yml\n+++ b/.github/workflows/InvokeCI.yml\n@@ -26,6 +26,7 @@ jobs:\n       override_git_describe: ${{ inputs.override_git_describe }}\n       git_ref: ${{ inputs.git_ref }}\n       skip_tests: ${{ inputs.skip_tests }}\n+      run_all: ${{ inputs.run_all }}\n \n   linux-release:\n     uses: ./.github/workflows/LinuxRelease.yml\n@@ -42,6 +43,7 @@ jobs:\n       override_git_describe: ${{ inputs.override_git_describe }}\n       git_ref: ${{ inputs.git_ref }}\n       skip_tests: ${{ inputs.skip_tests }}\n+      run_all: ${{ inputs.run_all }}\n \n   python:\n     uses: ./.github/workflows/Python.yml\ndiff --git a/.github/workflows/LinuxRelease.yml b/.github/workflows/LinuxRelease.yml\nindex 15d0cfd1970b..b11dbe916d0d 100644\n--- a/.github/workflows/LinuxRelease.yml\n+++ b/.github/workflows/LinuxRelease.yml\n@@ -150,6 +150,7 @@ jobs:\n           vcpkg_target_triplet: ${{ matrix.vcpkg_triplet }}\n           duckdb_arch: ${{ matrix.duckdb_arch }}\n           run_tests: ${{ inputs.skip_tests != 'true' }}\n+          override_git_describe: ${{ inputs.override_git_describe }}\n \n       - uses: actions/upload-artifact@v4\n         with:\n@@ -192,6 +193,7 @@ jobs:\n           vcpkg_target_triplet: ${{ matrix.vcpkg_triplet }}\n           duckdb_arch: ${{ matrix.duckdb_arch }}\n           run_tests: ${{ inputs.skip_tests != 'true' }}\n+          override_git_describe: ${{ inputs.override_git_describe }}\n \n       - uses: actions/upload-artifact@v4\n         with:\n@@ -233,6 +235,7 @@ jobs:\n           vcpkg_target_triplet: ${{ matrix.vcpkg_triplet }}\n           duckdb_arch: ${{ matrix.duckdb_arch }}\n           run_tests: ${{ inputs.skip_tests != 'true' }}\n+          override_git_describe: ${{ inputs.override_git_describe }}\n \n       - uses: actions/upload-artifact@v4\n         with:\ndiff --git a/.github/workflows/OSX.yml b/.github/workflows/OSX.yml\nindex 498d9c04f049..6d2dc09a7ee5 100644\n--- a/.github/workflows/OSX.yml\n+++ b/.github/workflows/OSX.yml\n@@ -8,6 +8,8 @@ on:\n         type: string\n       skip_tests:\n         type: string\n+      run_all:\n+        type: string\n   workflow_dispatch:\n     inputs:\n       override_git_describe:\n@@ -16,6 +18,9 @@ on:\n         type: string\n       skip_tests:\n         type: string\n+      run_all:\n+        type: string\n+  repository_dispatch:\n   push:\n     branches-ignore:\n       - 'main'\n@@ -311,12 +316,11 @@ jobs:\n         python-version: '3.12'\n \n     - name: Execute test\n+      if: (( inputs.skip_tests != 'true' )) && (( startsWith(github.ref, 'refs/tags/v') || github.ref == 'refs/heads/main' || inputs.run_all == 'true' )) && (( github.repository == 'duckdb/duckdb' ))\n       shell: bash\n       env:\n           AWS_ACCESS_KEY_ID: ${{secrets.S3_ID}}\n           AWS_SECRET_ACCESS_KEY: ${{secrets.S3_KEY}}\n           AWS_DEFAULT_REGION: us-east-1\n       run: |\n-          if [[ \"$GITHUB_REF\" =~ ^(refs/heads/main|refs/tags/v.+)$ && \"$GITHUB_REPOSITORY\" = \"duckdb/duckdb\" ]] ; then\n-            ./scripts/extension-upload-test.sh\n-          fi\n+          ./scripts/extension-upload-test.sh\ndiff --git a/.github/workflows/Python.yml b/.github/workflows/Python.yml\nindex b3608dc25498..05eaacd082c6 100644\n--- a/.github/workflows/Python.yml\n+++ b/.github/workflows/Python.yml\n@@ -150,6 +150,7 @@ jobs:\n           vcpkg_target_triplet: x64-linux\n           duckdb_arch: linux_amd64_gcc4\n           run_tests: ${{ inputs.skip_tests != 'true' }}\n+          override_git_describe: ${{ inputs.override_git_describe }}\n \n       - uses: actions/upload-artifact@v4\n         with:\n@@ -158,7 +159,6 @@ jobs:\n             build/release/extension/*/*.duckdb_extension\n \n   upload-linux-extensions-gcc4:\n-    ## TODO: Add a if: github.ref == main, for now this is explicitly missing to be able to test on PR, expected is this should fail due to missing secrets\n     name: Upload Linux Extensions (gcc4)\n     needs: manylinux-extensions-x64\n     strategy:\ndiff --git a/.github/workflows/Windows.yml b/.github/workflows/Windows.yml\nindex 7d71dedafa03..3584c880f90a 100644\n--- a/.github/workflows/Windows.yml\n+++ b/.github/workflows/Windows.yml\n@@ -8,6 +8,8 @@ on:\n         type: string\n       skip_tests:\n         type: string\n+      run_all:\n+        type: string\n   workflow_dispatch:\n     inputs:\n       override_git_describe:\n@@ -16,6 +18,9 @@ on:\n         type: string\n       skip_tests:\n         type: string\n+      run_all:\n+        type: string\n+  repository_dispatch:\n   push:\n     branches-ignore:\n       - 'main'\n@@ -144,7 +149,7 @@ jobs:\n \n  win-release-32:\n     name: Windows (32 Bit)\n-    if: github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb'\n+    if: github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' || inputs.run_all == 'true'\n     runs-on: windows-2019\n     needs: win-release-64\n \n@@ -184,7 +189,7 @@ jobs:\n \n  win-release-arm64:\n    name: Windows (ARM64)\n-   if: github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb'\n+   if: github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' || inputs.run_all == 'true'\n    runs-on: windows-2019\n    needs: win-release-64\n \ndiff --git a/benchmark/benchmark_runner.cpp b/benchmark/benchmark_runner.cpp\nindex 7ba981285517..d30b3e5a4a93 100644\n--- a/benchmark/benchmark_runner.cpp\n+++ b/benchmark/benchmark_runner.cpp\n@@ -123,6 +123,7 @@ void BenchmarkRunner::RunBenchmark(Benchmark *benchmark) {\n \tduckdb::unique_ptr<BenchmarkState> state;\n \ttry {\n \t\tstate = benchmark->Initialize(configuration);\n+\t\tbenchmark->Assert(state.get());\n \t} catch (std::exception &ex) {\n \t\tLog(StringUtil::Format(\"%s\\t1\\t\", benchmark->name));\n \t\tLogResult(\"ERROR\");\ndiff --git a/benchmark/include/benchmark.hpp b/benchmark/include/benchmark.hpp\nindex ece2dda3d3de..1fe4b7df79f6 100644\n--- a/benchmark/include/benchmark.hpp\n+++ b/benchmark/include/benchmark.hpp\n@@ -43,6 +43,8 @@ class Benchmark {\n \tvirtual duckdb::unique_ptr<BenchmarkState> Initialize(BenchmarkConfiguration &config) {\n \t\treturn nullptr;\n \t}\n+\t//! Assert correctness after load, before run\n+\tvirtual void Assert(BenchmarkState *state) {};\n \t//! Run the benchmark\n \tvirtual void Run(BenchmarkState *state) = 0;\n \t//! Cleanup the benchmark, called after each Run\ndiff --git a/benchmark/include/interpreted_benchmark.hpp b/benchmark/include/interpreted_benchmark.hpp\nindex ecb8a47b833e..9cd4c19db80d 100644\n--- a/benchmark/include/interpreted_benchmark.hpp\n+++ b/benchmark/include/interpreted_benchmark.hpp\n@@ -19,6 +19,17 @@ struct InterpretedBenchmarkState;\n \n const string DEFAULT_DB_PATH = \"duckdb_benchmark_db.db\";\n \n+struct BenchmarkQuery {\n+public:\n+\tBenchmarkQuery() {\n+\t}\n+\n+public:\n+\tstring query;\n+\tidx_t column_count = 0;\n+\tvector<vector<string>> expected_result;\n+};\n+\n //! Interpreted benchmarks read the benchmark from a file\n class InterpretedBenchmark : public Benchmark {\n public:\n@@ -27,6 +38,8 @@ class InterpretedBenchmark : public Benchmark {\n \tvoid LoadBenchmark();\n \t//! Initialize the benchmark state\n \tduckdb::unique_ptr<BenchmarkState> Initialize(BenchmarkConfiguration &config) override;\n+\t//! Assert correct/expected state of the db, before Run\n+\tvoid Assert(BenchmarkState *state) override;\n \t//! Run the benchmark\n \tvoid Run(BenchmarkState *state) override;\n \t//! Cleanup the benchmark, called after each Run\n@@ -63,10 +76,10 @@ class InterpretedBenchmark : public Benchmark {\n \t}\n \n private:\n-\tstring VerifyInternal(BenchmarkState *state_p, MaterializedQueryResult &result);\n+\tstring VerifyInternal(BenchmarkState *state_p, const BenchmarkQuery &query, MaterializedQueryResult &result);\n \n-\tvoid ReadResultFromFile(BenchmarkFileReader &reader, const string &file);\n-\tvoid ReadResultFromReader(BenchmarkFileReader &reader, const string &file);\n+\tBenchmarkQuery ReadQueryFromFile(BenchmarkFileReader &reader, const string &file);\n+\tBenchmarkQuery ReadQueryFromReader(BenchmarkFileReader &reader, const string &sql, const string &header);\n \n \tunique_ptr<QueryResult> RunLoadQuery(InterpretedBenchmarkState &state, const string &load_query);\n \n@@ -84,9 +97,10 @@ class InterpretedBenchmark : public Benchmark {\n \t// can be used to test accessing data from a different db in a non-persistent connection\n \tbool cache_no_connect = false;\n \tstd::unordered_set<string> extensions;\n-\tint64_t result_column_count = 0;\n-\tvector<vector<string>> result_values;\n-\tstring result_query;\n+\n+\t//! Queries used to assert a given state of the data\n+\tvector<BenchmarkQuery> assert_queries;\n+\tvector<BenchmarkQuery> result_queries;\n \t//! How many times to retry the load, if any\n \tidx_t retry_load = 0;\n \n@@ -95,6 +109,7 @@ class InterpretedBenchmark : public Benchmark {\n \tstring subgroup;\n \n \tbool in_memory = true;\n+\tstring storage_version;\n \tQueryResultType result_type = QueryResultType::MATERIALIZED_RESULT;\n \tidx_t arrow_batch_size = STANDARD_VECTOR_SIZE;\n \tbool require_reinit = false;\ndiff --git a/benchmark/interpreted_benchmark.cpp b/benchmark/interpreted_benchmark.cpp\nindex 7225100b7da6..e91d65dabcda 100644\n--- a/benchmark/interpreted_benchmark.cpp\n+++ b/benchmark/interpreted_benchmark.cpp\n@@ -43,16 +43,19 @@ struct InterpretedBenchmarkState : public BenchmarkState {\n \tConnection con;\n \tduckdb::unique_ptr<MaterializedQueryResult> result;\n \n-\texplicit InterpretedBenchmarkState(string path)\n-\t    : benchmark_config(GetBenchmarkConfig()), db(path.empty() ? nullptr : path.c_str(), benchmark_config.get()),\n-\t      con(db) {\n+\texplicit InterpretedBenchmarkState(string path, const string &version)\n+\t    : benchmark_config(GetBenchmarkConfig(version)),\n+\t      db(path.empty() ? nullptr : path.c_str(), benchmark_config.get()), con(db) {\n \t\tauto &instance = BenchmarkRunner::GetInstance();\n \t\tauto res = con.Query(\"PRAGMA threads=\" + to_string(instance.threads));\n \t\tD_ASSERT(!res->HasError());\n \t}\n \n-\tduckdb::unique_ptr<DBConfig> GetBenchmarkConfig() {\n+\tduckdb::unique_ptr<DBConfig> GetBenchmarkConfig(const string &version = \"\") {\n \t\tauto result = make_uniq<DBConfig>();\n+\t\tif (!version.empty()) {\n+\t\t\tresult->options.serialization_compatibility = SerializationCompatibility::FromString(version);\n+\t\t}\n \t\tresult->options.load_extensions = false;\n \t\treturn result;\n \t}\n@@ -96,24 +99,31 @@ InterpretedBenchmark::InterpretedBenchmark(string full_path)\n \treplacement_mapping[\"BENCHMARK_DIR\"] = BenchmarkRunner::DUCKDB_BENCHMARK_DIRECTORY;\n }\n \n-void InterpretedBenchmark::ReadResultFromFile(BenchmarkFileReader &reader, const string &file) {\n+BenchmarkQuery InterpretedBenchmark::ReadQueryFromFile(BenchmarkFileReader &reader, const string &file) {\n \t// read the results from the file\n+\tBenchmarkQuery query;\n+\tquery.query = \"\";\n+\n \tDuckDB db;\n \tConnection con(db);\n \tauto result = con.Query(\"FROM read_csv('\" + file +\n \t                        \"', delim='|', header=1, nullstr='NULL', all_varchar=1, quote ='\\\"', escape ='\\\"')\");\n-\tresult_column_count = result->ColumnCount();\n+\tquery.column_count = result->ColumnCount();\n \tfor (auto &row : *result) {\n \t\tvector<string> row_values;\n \t\tfor (idx_t col_idx = 0; col_idx < result->ColumnCount(); col_idx++) {\n \t\t\trow_values.push_back(row.GetValue<string>(col_idx));\n \t\t}\n-\t\tresult_values.push_back(std::move(row_values));\n+\t\tquery.expected_result.push_back(std::move(row_values));\n \t}\n+\treturn query;\n }\n \n-void InterpretedBenchmark::ReadResultFromReader(BenchmarkFileReader &reader, const string &header) {\n-\tresult_column_count = header.size();\n+BenchmarkQuery InterpretedBenchmark::ReadQueryFromReader(BenchmarkFileReader &reader, const string &sql,\n+                                                         const string &header) {\n+\tBenchmarkQuery query;\n+\tquery.query = sql;\n+\tquery.column_count = header.size();\n \t// keep reading results until eof\n \tstring line;\n \twhile (reader.ReadLine(line)) {\n@@ -121,12 +131,13 @@ void InterpretedBenchmark::ReadResultFromReader(BenchmarkFileReader &reader, con\n \t\t\tbreak;\n \t\t}\n \t\tauto result_splits = StringUtil::Split(line, \"\\t\");\n-\t\tif ((int64_t)result_splits.size() != result_column_count) {\n+\t\tif ((int64_t)result_splits.size() != query.column_count) {\n \t\t\tthrow std::runtime_error(reader.FormatException(\"expected \" + std::to_string(result_splits.size()) +\n-\t\t\t                                                \" values but got \" + std::to_string(result_column_count)));\n+\t\t\t                                                \" values but got \" + std::to_string(query.column_count)));\n \t\t}\n-\t\tresult_values.push_back(std::move(result_splits));\n+\t\tquery.expected_result.push_back(std::move(result_splits));\n \t}\n+\treturn query;\n }\n \n static void ThrowResultModeError(BenchmarkFileReader &reader) {\n@@ -153,6 +164,7 @@ void InterpretedBenchmark::LoadBenchmark() {\n \t\t\tif (queries.find(splits[0]) != queries.end()) {\n \t\t\t\tthrow std::runtime_error(\"Multiple calls to \" + splits[0] + \" in the same benchmark file\");\n \t\t\t}\n+\n \t\t\t// load command: keep reading until we find a blank line or EOF\n \t\t\tstring query;\n \t\t\twhile (reader.ReadLine(line)) {\n@@ -233,8 +245,8 @@ void InterpretedBenchmark::LoadBenchmark() {\n \t\t\t\tcache_db = string();\n \t\t\t}\n \t\t} else if (splits[0] == \"storage\") {\n-\t\t\tif (splits.size() != 2) {\n-\t\t\t\tthrow std::runtime_error(reader.FormatException(\"storage requires a single parameter\"));\n+\t\t\tif (splits.size() < 2) {\n+\t\t\t\tthrow std::runtime_error(reader.FormatException(\"storage requires at least one parameter\"));\n \t\t\t}\n \t\t\tif (splits[1] == \"transient\") {\n \t\t\t\tin_memory = true;\n@@ -243,6 +255,10 @@ void InterpretedBenchmark::LoadBenchmark() {\n \t\t\t} else {\n \t\t\t\tthrow std::runtime_error(reader.FormatException(\"Invalid argument for storage\"));\n \t\t\t}\n+\n+\t\t\tif (splits.size() == 3) {\n+\t\t\t\tstorage_version = splits[2];\n+\t\t\t}\n \t\t} else if (splits[0] == \"require_reinit\") {\n \t\t\tif (splits.size() != 1) {\n \t\t\t\tthrow std::runtime_error(reader.FormatException(\"require_reinit does not take any parameters\"));\n@@ -261,25 +277,13 @@ void InterpretedBenchmark::LoadBenchmark() {\n \t\t\t} else {\n \t\t\t\tsubgroup = result;\n \t\t\t}\n-\t\t} else if (splits[0] == \"result_query\") {\n-\t\t\tif (result_column_count > 0) {\n-\t\t\t\tthrow std::runtime_error(reader.FormatException(\"multiple results found\"));\n-\t\t\t}\n+\t\t} else if (splits[0] == \"assert\") {\n \t\t\t// count the amount of columns\n \t\t\tif (splits.size() <= 1 || splits[1].size() == 0) {\n \t\t\t\tthrow std::runtime_error(\n-\t\t\t\t    reader.FormatException(\"result_query must be followed by a column count (e.g. result III)\"));\n-\t\t\t}\n-\t\t\tbool is_file = false;\n-\t\t\tfor (idx_t i = 0; i < splits[1].size(); i++) {\n-\t\t\t\tif (splits[1][i] != 'i') {\n-\t\t\t\t\tis_file = true;\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif (is_file) {\n-\t\t\t\tReadResultFromFile(reader, splits[1]);\n+\t\t\t\t    reader.FormatException(\"assert must be followed by a column count (e.g. result III)\"));\n \t\t\t}\n+\n \t\t\t// read the actual query\n \t\t\tbool found_end = false;\n \t\t\tstring sql;\n@@ -290,23 +294,20 @@ void InterpretedBenchmark::LoadBenchmark() {\n \t\t\t\t}\n \t\t\t\tsql += \"\\n\" + line;\n \t\t\t}\n-\t\t\tresult_query = sql;\n \t\t\tif (!found_end) {\n \t\t\t\tthrow std::runtime_error(reader.FormatException(\n \t\t\t\t    \"result_query must be followed by a query and a result (separated by ----)\"));\n \t\t\t}\n-\t\t\tif (!is_file) {\n-\t\t\t\tReadResultFromReader(reader, splits[1]);\n-\t\t\t}\n-\t\t} else if (splits[0] == \"result\") {\n-\t\t\tif (result_column_count > 0) {\n+\n+\t\t\tassert_queries.push_back(ReadQueryFromReader(reader, sql, splits[1]));\n+\t\t} else if (splits[0] == \"result_query\" || splits[0] == \"result\") {\n+\t\t\tif (!result_queries.empty()) {\n \t\t\t\tthrow std::runtime_error(reader.FormatException(\"multiple results found\"));\n \t\t\t}\n \t\t\t// count the amount of columns\n \t\t\tif (splits.size() <= 1 || splits[1].size() == 0) {\n \t\t\t\tthrow std::runtime_error(\n-\t\t\t\t    reader.FormatException(\"result must be followed by a column count (e.g. result III) or a file \"\n-\t\t\t\t                           \"(e.g. result /path/to/file.csv)\"));\n+\t\t\t\t    reader.FormatException(\"result_query must be followed by a column count (e.g. result III)\"));\n \t\t\t}\n \t\t\tbool is_file = false;\n \t\t\tfor (idx_t i = 0; i < splits[1].size(); i++) {\n@@ -316,17 +317,30 @@ void InterpretedBenchmark::LoadBenchmark() {\n \t\t\t\t}\n \t\t\t}\n \t\t\tif (is_file) {\n-\t\t\t\tReadResultFromFile(reader, splits[1]);\n-\n-\t\t\t\t// read the main file until we encounter an empty line\n-\t\t\t\tstring line;\n-\t\t\t\twhile (reader.ReadLine(line)) {\n-\t\t\t\t\tif (line.empty()) {\n-\t\t\t\t\t\tbreak;\n+\t\t\t\tresult_queries.push_back(ReadQueryFromFile(reader, splits[1]));\n+\t\t\t} else {\n+\t\t\t\tstring result_query;\n+\t\t\t\tif (splits[0] == \"result_query\") {\n+\t\t\t\t\t// read the actual query\n+\t\t\t\t\tbool found_end = false;\n+\t\t\t\t\tstring sql;\n+\t\t\t\t\twhile (reader.ReadLine(line)) {\n+\t\t\t\t\t\tif (line == \"----\") {\n+\t\t\t\t\t\t\tfound_end = true;\n+\t\t\t\t\t\t\tbreak;\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tsql += \"\\n\" + line;\n \t\t\t\t\t}\n+\t\t\t\t\tif (!found_end) {\n+\t\t\t\t\t\tthrow std::runtime_error(reader.FormatException(\n+\t\t\t\t\t\t    \"result_query must be followed by a query and a result (separated by ----)\"));\n+\t\t\t\t\t}\n+\t\t\t\t\tresult_query = sql;\n+\t\t\t\t} else {\n+\t\t\t\t\t//! Read directly from the answer\n+\t\t\t\t\tresult_query = \"select * from __answer\";\n \t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tReadResultFromReader(reader, splits[1]);\n+\t\t\t\tresult_queries.push_back(ReadQueryFromReader(reader, result_query, splits[1]));\n \t\t\t}\n \t\t} else if (splits[0] == \"retry\") {\n \t\t\tif (splits.size() != 3) {\n@@ -383,12 +397,12 @@ unique_ptr<BenchmarkState> InterpretedBenchmark::Initialize(BenchmarkConfigurati\n \tduckdb::unique_ptr<InterpretedBenchmarkState> state;\n \tauto full_db_path = GetDatabasePath();\n \ttry {\n-\t\tstate = make_uniq<InterpretedBenchmarkState>(full_db_path);\n+\t\tstate = make_uniq<InterpretedBenchmarkState>(full_db_path, storage_version);\n \t} catch (Exception &e) {\n \t\t// if the connection throws an error, chances are it's a storage format error.\n \t\t// In this case delete the file and connect again.\n \t\tDeleteDatabase(full_db_path);\n-\t\tstate = make_uniq<InterpretedBenchmarkState>(full_db_path);\n+\t\tstate = make_uniq<InterpretedBenchmarkState>(full_db_path, storage_version);\n \t}\n \textensions.insert(\"core_functions\");\n \textensions.insert(\"parquet\");\n@@ -494,6 +508,22 @@ ScopedConfigSetting PrepareResultCollector(ClientConfig &config, InterpretedBenc\n \treturn ScopedConfigSetting(config);\n }\n \n+void InterpretedBenchmark::Assert(BenchmarkState *state_p) {\n+\tauto &state = (InterpretedBenchmarkState &)*state_p;\n+\n+\tfor (auto &assert_query : assert_queries) {\n+\t\tauto &query = assert_query.query;\n+\t\tauto result = state.con.Query(query);\n+\t\tif (result->HasError()) {\n+\t\t\tresult->ThrowError();\n+\t\t}\n+\t\tauto verify_result = VerifyInternal(state_p, assert_query, *result);\n+\t\tif (!verify_result.empty()) {\n+\t\t\tthrow InvalidInputException(\"Assertion query failed:\\n%s\", verify_result);\n+\t\t}\n+\t}\n+}\n+\n void InterpretedBenchmark::Run(BenchmarkState *state_p) {\n \tauto &state = (InterpretedBenchmarkState &)*state_p;\n \tauto &context = state.con.context;\n@@ -545,21 +575,25 @@ string InterpretedBenchmark::GetDatabasePath() {\n \treturn db_path;\n }\n \n-string InterpretedBenchmark::VerifyInternal(BenchmarkState *state_p, MaterializedQueryResult &result) {\n+string InterpretedBenchmark::VerifyInternal(BenchmarkState *state_p, const BenchmarkQuery &query,\n+                                            MaterializedQueryResult &result) {\n \tauto &state = (InterpretedBenchmarkState &)*state_p;\n-\t// compare the column count\n-\tif (result_column_count >= 0 && (int64_t)result.ColumnCount() != result_column_count) {\n+\n+\tauto &result_values = query.expected_result;\n+\tD_ASSERT(query.column_count >= 1);\n+\tif (query.column_count != (int64_t)result.ColumnCount()) {\n \t\treturn StringUtil::Format(\"Error in result: expected %lld columns but got %lld\\nObtained result: %s\",\n-\t\t                          (int64_t)result_column_count, (int64_t)result.ColumnCount(), result.ToString());\n+\t\t                          (int64_t)query.column_count, (int64_t)result.ColumnCount(), result.ToString());\n \t}\n+\n \t// compare row count\n-\tif (result.RowCount() != result_values.size()) {\n+\tif (result.RowCount() != query.expected_result.size()) {\n \t\treturn StringUtil::Format(\"Error in result: expected %lld rows but got %lld\\nObtained result: %s\",\n \t\t                          (int64_t)result_values.size(), (int64_t)result.RowCount(), result.ToString());\n \t}\n \t// compare values\n \tfor (int64_t r = 0; r < (int64_t)result_values.size(); r++) {\n-\t\tfor (int64_t c = 0; c < result_column_count; c++) {\n+\t\tfor (int64_t c = 0; c < query.column_count; c++) {\n \t\t\tauto value = result.GetValue(c, r);\n \t\t\tif (result_values[r][c] == \"NULL\" && value.IsNull()) {\n \t\t\t\tcontinue;\n@@ -597,47 +631,49 @@ string InterpretedBenchmark::Verify(BenchmarkState *state_p) {\n \tif (state.result->HasError()) {\n \t\treturn state.result->GetError();\n \t}\n-\tif (result_column_count == 0) {\n+\tif (result_queries.empty()) {\n \t\t// no result specified\n \t\treturn string();\n \t}\n-\tif (!result_query.empty()) {\n-\t\t// we are running a result query\n-\t\t// store the current result in a table called \"__answer\"\n-\t\tauto &collection = state.result->Collection();\n-\t\tauto &names = state.result->names;\n-\t\tauto &types = state.result->types;\n-\t\t// first create the (empty) table\n-\t\tstring create_tbl = \"CREATE OR REPLACE TEMP TABLE __answer(\";\n-\t\tfor (idx_t i = 0; i < names.size(); i++) {\n-\t\t\tif (i > 0) {\n-\t\t\t\tcreate_tbl += \", \";\n-\t\t\t}\n-\t\t\tcreate_tbl += KeywordHelper::WriteOptionallyQuoted(names[i]);\n-\t\t\tcreate_tbl += \" \";\n-\t\t\tcreate_tbl += types[i].ToString();\n-\t\t}\n-\t\tcreate_tbl += \")\";\n-\t\tauto new_result = state.con.Query(create_tbl);\n-\t\tif (new_result->HasError()) {\n-\t\t\treturn new_result->GetError();\n-\t\t}\n-\t\t// now append the result to the answer table\n-\t\tauto table_info = state.con.TableInfo(\"__answer\");\n-\t\tif (table_info == nullptr) {\n-\t\t\tthrow std::runtime_error(\"Received a nullptr when querying table info of __answer\");\n-\t\t}\n-\t\tstate.con.Append(*table_info, collection);\n+\tD_ASSERT(result_queries.size() == 1);\n+\tauto &query = result_queries[0];\n+\tif (query.query.empty()) {\n+\t\treturn string();\n+\t}\n \n-\t\t// finally run the result query and verify the result of that query\n-\t\tnew_result = state.con.Query(result_query);\n-\t\tif (new_result->HasError()) {\n-\t\t\treturn new_result->GetError();\n+\t// we are running a result query\n+\t// store the current result in a table called \"__answer\"\n+\tauto &collection = state.result->Collection();\n+\tauto &names = state.result->names;\n+\tauto &types = state.result->types;\n+\t// first create the (empty) table\n+\tstring create_tbl = \"CREATE OR REPLACE TEMP TABLE __answer(\";\n+\tfor (idx_t i = 0; i < names.size(); i++) {\n+\t\tif (i > 0) {\n+\t\t\tcreate_tbl += \", \";\n \t\t}\n-\t\treturn VerifyInternal(state_p, *new_result);\n-\t} else {\n-\t\treturn VerifyInternal(state_p, *state.result);\n+\t\tcreate_tbl += KeywordHelper::WriteOptionallyQuoted(names[i]);\n+\t\tcreate_tbl += \" \";\n+\t\tcreate_tbl += types[i].ToString();\n+\t}\n+\tcreate_tbl += \")\";\n+\tauto new_result = state.con.Query(create_tbl);\n+\tif (new_result->HasError()) {\n+\t\treturn new_result->GetError();\n+\t}\n+\t// now append the result to the answer table\n+\tauto table_info = state.con.TableInfo(\"__answer\");\n+\tif (table_info == nullptr) {\n+\t\tthrow std::runtime_error(\"Received a nullptr when querying table info of __answer\");\n+\t}\n+\tstate.con.Append(*table_info, collection);\n+\n+\t// finally run the result query and verify the result of that query\n+\tnew_result = state.con.Query(query.query);\n+\tif (new_result->HasError()) {\n+\t\treturn new_result->GetError();\n \t}\n+\treturn VerifyInternal(state_p, query, *new_result);\n }\n \n void InterpretedBenchmark::Interrupt(BenchmarkState *state_p) {\ndiff --git a/benchmark/micro/compression/roaring/roaring_array_read.benchmark b/benchmark/micro/compression/roaring/roaring_array_read.benchmark\nindex fb954f6d0025..64f9a6cc6fc4 100644\n--- a/benchmark/micro/compression/roaring/roaring_array_read.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_array_read.benchmark\n@@ -4,7 +4,7 @@\n \n name Roaring Scan Array Container\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n load\n DROP TABLE IF EXISTS tbl;\n@@ -12,6 +12,11 @@ PRAGMA force_compression='Roaring';\n CREATE TABLE tbl AS SELECT case when i%25=0 then 1337 else null end as a FROM range(0, 250_000_000) tbl(i);\n checkpoint;\n \n+assert I\n+select DISTINCT compression from pragma_storage_info('tbl') where segment_type in ('VALIDITY')\n+----\n+Roaring\n+\n run\n select count(*) from tbl WHERE a IS NOT NULL;\n \ndiff --git a/benchmark/micro/compression/roaring/roaring_array_store.benchmark b/benchmark/micro/compression/roaring/roaring_array_store.benchmark\nindex 8d331e49cab7..dd53dd1d2ab5 100644\n--- a/benchmark/micro/compression/roaring/roaring_array_store.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_array_store.benchmark\n@@ -4,12 +4,19 @@\n \n name Roaring Write Array Container\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n load\n+CREATE TABLE data_source AS SELECT case when i%25=0 then 1337 else null end as a FROM range(0, 250_000_000) tbl(i);\n PRAGMA force_compression='Roaring';\n SET checkpoint_threshold = '10.0 GB';\n-CREATE TABLE data_source AS SELECT case when i%25=0 then 1337 else null end as a FROM range(0, 250_000_000) tbl(i);\n+CREATE TABLE test_compression as FROM data_source;\n+checkpoint;\n+\n+assert I\n+select DISTINCT compression from pragma_storage_info('test_compression') where segment_type in ('VALIDITY')\n+----\n+Roaring\n \n run\n CREATE TABLE tbl AS FROM data_source;\ndiff --git a/benchmark/micro/compression/roaring/roaring_bitset_read.benchmark b/benchmark/micro/compression/roaring/roaring_bitset_read.benchmark\nindex 93951f911d3f..af135b0a4ff3 100644\n--- a/benchmark/micro/compression/roaring/roaring_bitset_read.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_bitset_read.benchmark\n@@ -4,7 +4,7 @@\n \n name Roaring Scan Run Container Inverted\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n load\n DROP TABLE IF EXISTS tbl;\n@@ -12,6 +12,11 @@ PRAGMA force_compression='Roaring';\n CREATE TABLE tbl AS SELECT case when i%3=0 then 1337 else null end as a FROM range(0, 250_000_000) tbl(i);\n checkpoint;\n \n+assert I\n+select DISTINCT compression from pragma_storage_info('tbl') where segment_type in ('VALIDITY')\n+----\n+Roaring\n+\n run\n select count(*) from tbl WHERE a IS NOT NULL;\n \ndiff --git a/benchmark/micro/compression/roaring/roaring_bitset_store.benchmark b/benchmark/micro/compression/roaring/roaring_bitset_store.benchmark\nindex ea9ce54d5c09..e8f7671f50be 100644\n--- a/benchmark/micro/compression/roaring/roaring_bitset_store.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_bitset_store.benchmark\n@@ -4,12 +4,19 @@\n \n name Roaring Write Run Container Inverted\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n load\n+CREATE TABLE data_source AS SELECT case when i%3=0 then 1337 else null end as a FROM range(0, 250_000_000) tbl(i);\n PRAGMA force_compression='roaring';\n SET checkpoint_threshold = '10.0 GB';\n-CREATE TABLE data_source AS SELECT case when i%3=0 then 1337 else null end as a FROM range(0, 250_000_000) tbl(i);\n+CREATE TABLE test_compression as FROM data_source;\n+checkpoint;\n+\n+assert I\n+select DISTINCT compression from pragma_storage_info('test_compression') where segment_type in ('VALIDITY')\n+----\n+Roaring\n \n run\n CREATE TABLE tbl AS FROM data_source;\ndiff --git a/benchmark/micro/compression/roaring/roaring_inverted_array_read.benchmark b/benchmark/micro/compression/roaring/roaring_inverted_array_read.benchmark\nindex 0fb691fa5125..fc9e93eced1e 100644\n--- a/benchmark/micro/compression/roaring/roaring_inverted_array_read.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_inverted_array_read.benchmark\n@@ -4,7 +4,7 @@\n \n name Roaring Scan Array Container Inverted\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n load\n DROP TABLE IF EXISTS tbl;\n@@ -12,6 +12,11 @@ PRAGMA force_compression='Roaring';\n CREATE TABLE tbl AS SELECT case when i%25=0 then null else 1337 end as a FROM range(0, 250_000_000) tbl(i);\n checkpoint;\n \n+assert I\n+select DISTINCT compression from pragma_storage_info('tbl') where segment_type in ('VALIDITY')\n+----\n+Roaring\n+\n run\n select count(*) from tbl WHERE a IS NOT NULL;\n \ndiff --git a/benchmark/micro/compression/roaring/roaring_inverted_array_store.benchmark b/benchmark/micro/compression/roaring/roaring_inverted_array_store.benchmark\nindex ac4868b2ab68..41673dddb5dd 100644\n--- a/benchmark/micro/compression/roaring/roaring_inverted_array_store.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_inverted_array_store.benchmark\n@@ -4,12 +4,19 @@\n \n name Roaring Scan Array Container Inverted\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n load\n+CREATE TABLE data_source AS SELECT case when i%25=0 then null else 1337 end as a FROM range(0, 250_000_000) tbl(i);\n PRAGMA force_compression='roaring';\n SET checkpoint_threshold = '10.0 GB';\n-CREATE TABLE data_source AS SELECT case when i%25=0 then null else 1337 end as a FROM range(0, 250_000_000) tbl(i);\n+CREATE TABLE test_compression as FROM data_source;\n+checkpoint;\n+\n+assert I\n+select DISTINCT compression from pragma_storage_info('test_compression') where segment_type in ('VALIDITY')\n+----\n+Roaring\n \n run\n CREATE TABLE tbl AS FROM data_source;\ndiff --git a/benchmark/micro/compression/roaring/roaring_inverted_run_read.benchmark b/benchmark/micro/compression/roaring/roaring_inverted_run_read.benchmark\nindex c7244d679fb3..5c6e9ab865e7 100644\n--- a/benchmark/micro/compression/roaring/roaring_inverted_run_read.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_inverted_run_read.benchmark\n@@ -4,7 +4,7 @@\n \n name Roaring Scan Run Container Inverted\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n load\n DROP TABLE IF EXISTS tbl;\n@@ -12,6 +12,11 @@ PRAGMA force_compression='Roaring';\n CREATE TABLE tbl AS SELECT case when i = 0 or (i % 512 != 0 and (i % 512) < 350 or (i % 512) > 450) then 1337 else null end as a FROM range(0, 250_000_000) tbl(i);\n checkpoint;\n \n+assert I\n+select DISTINCT compression from pragma_storage_info('tbl') where segment_type in ('VALIDITY')\n+----\n+Roaring\n+\n run\n select count(*) from tbl WHERE a IS NOT NULL;\n \ndiff --git a/benchmark/micro/compression/roaring/roaring_inverted_run_store.benchmark b/benchmark/micro/compression/roaring/roaring_inverted_run_store.benchmark\nindex e876c838f6c9..e1c27414f2ed 100644\n--- a/benchmark/micro/compression/roaring/roaring_inverted_run_store.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_inverted_run_store.benchmark\n@@ -4,12 +4,19 @@\n \n name Roaring Write Run Container Inverted\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n load\n+CREATE TABLE data_source AS SELECT case when i = 0 or (i % 512 != 0 and (i % 512) < 350 or (i % 512) > 450) then 1337 else null end as a FROM range(0, 250_000_000) tbl(i);\n PRAGMA force_compression='Roaring';\n SET checkpoint_threshold = '10.0 GB';\n-CREATE TABLE data_source AS SELECT case when i = 0 or (i % 512 != 0 and (i % 512) < 350 or (i % 512) > 450) then 1337 else null end as a FROM range(0, 250_000_000) tbl(i);\n+CREATE TABLE test_compression as FROM data_source;\n+checkpoint;\n+\n+assert I\n+select DISTINCT compression from pragma_storage_info('test_compression') where segment_type in ('VALIDITY')\n+----\n+Roaring\n \n run\n CREATE TABLE tbl AS FROM data_source;\ndiff --git a/benchmark/micro/compression/roaring/roaring_run_read.benchmark b/benchmark/micro/compression/roaring/roaring_run_read.benchmark\nindex 8164940f17bf..a14684354181 100644\n--- a/benchmark/micro/compression/roaring/roaring_run_read.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_run_read.benchmark\n@@ -4,7 +4,7 @@\n \n name Roaring Scan Run Container Inverted\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n load\n DROP TABLE IF EXISTS tbl;\n@@ -12,6 +12,11 @@ PRAGMA force_compression='Roaring';\n CREATE TABLE tbl AS SELECT case when i = 0 or (i % 512 != 0 and (i % 512) < 350 or (i % 512) > 450) then null else 1337 end as a FROM range(0, 250_000_000) tbl(i);\n checkpoint;\n \n+assert I\n+select DISTINCT compression from pragma_storage_info('tbl') where segment_type in ('VALIDITY')\n+----\n+Roaring\n+\n run\n select count(*) from tbl WHERE a IS NOT NULL;\n \ndiff --git a/benchmark/micro/compression/roaring/roaring_run_store.benchmark b/benchmark/micro/compression/roaring/roaring_run_store.benchmark\nindex 8a13c8404d9f..d5af79ec4a44 100644\n--- a/benchmark/micro/compression/roaring/roaring_run_store.benchmark\n+++ b/benchmark/micro/compression/roaring/roaring_run_store.benchmark\n@@ -4,13 +4,20 @@\n \n name Roaring Write Run Container\n group roaring\n-storage persistent\n+storage persistent v1.2.0\n \n # Roughly 8 runs per Vector\n load\n+CREATE TABLE data_source AS SELECT case when i = 0 or (i % 512 != 0 and (i % 512) < 350 or (i % 512) > 450) then null else 1337 end as a FROM range(0, 250_000_000) tbl(i);\n PRAGMA force_compression='Roaring';\n SET checkpoint_threshold = '10.0 GB';\n-CREATE TABLE data_source AS SELECT case when i = 0 or (i % 512 != 0 and (i % 512) < 350 or (i % 512) > 450) then null else 1337 end as a FROM range(0, 250_000_000) tbl(i);\n+CREATE TABLE test_compression as FROM data_source;\n+checkpoint;\n+\n+assert I\n+select DISTINCT compression from pragma_storage_info('test_compression') where segment_type in ('VALIDITY')\n+----\n+Roaring\n \n run\n CREATE TABLE tbl AS FROM data_source;\ndiff --git a/benchmark/micro/compression/zstd/zstd_read.benchmark b/benchmark/micro/compression/zstd/zstd_read.benchmark\nindex 4684249e67fe..4472b1a80666 100644\n--- a/benchmark/micro/compression/zstd/zstd_read.benchmark\n+++ b/benchmark/micro/compression/zstd/zstd_read.benchmark\n@@ -4,7 +4,7 @@\n \n name ZSTD Scan\n group zstd\n-storage persistent\n+storage persistent v1.2.0\n \n load\n DROP TABLE IF EXISTS zstd_strings;\n@@ -13,5 +13,10 @@ set variable my_string = (list_reduce([chr(((i % 26) + ord('a'))::INTEGER) for i\n create table zstd_strings as select getvariable('my_string') as data from range(2_500_000) tbl(i);\n checkpoint;\n \n+assert I\n+select DISTINCT compression from pragma_storage_info('zstd_strings') where segment_type in ('VARCHAR')\n+----\n+ZSTD\n+\n run\n select avg(strlen(data)) from zstd_strings;\ndiff --git a/benchmark/micro/compression/zstd/zstd_store.benchmark b/benchmark/micro/compression/zstd/zstd_store.benchmark\nindex bab765e2f89a..b8511a974be3 100644\n--- a/benchmark/micro/compression/zstd/zstd_store.benchmark\n+++ b/benchmark/micro/compression/zstd/zstd_store.benchmark\n@@ -4,13 +4,20 @@\n \n name ZSTD Compression Write\n group zstd\n-storage persistent\n+storage persistent v1.2.0\n require_reinit\n \n load\n DROP TABLE IF EXISTS zstd_strings;\n PRAGMA force_compression='zstd';\n set variable my_string = (list_reduce([chr(((i % 26) + ord('a'))::INTEGER) for i in range(4096)], (x, y) -> concat(x, y)));\n+create table test_compression as select getvariable('my_string') as data from range(2_500_000) tbl(i);\n+checkpoint;\n+\n+assert I\n+select DISTINCT compression from pragma_storage_info('test_compression') where segment_type in ('VARCHAR')\n+----\n+ZSTD\n \n run\n create table zstd_strings as select getvariable('my_string') as data from range(2_500_000) tbl(i);\ndiff --git a/data/storage/artupdates.db.gz b/data/storage/artupdates.db.gz\nnew file mode 100644\nindex 000000000000..de8c920afcac\nBinary files /dev/null and b/data/storage/artupdates.db.gz differ\ndiff --git a/src/execution/index/art/art.cpp b/src/execution/index/art/art.cpp\nindex c47f7740b976..68aad532ce7a 100644\n--- a/src/execution/index/art/art.cpp\n+++ b/src/execution/index/art/art.cpp\n@@ -45,7 +45,7 @@ ART::ART(const string &name, const IndexConstraintType index_constraint_type, co\n          const shared_ptr<array<unsafe_unique_ptr<FixedSizeAllocator>, ALLOCATOR_COUNT>> &allocators_ptr,\n          const IndexStorageInfo &info)\n     : BoundIndex(name, ART::TYPE_NAME, index_constraint_type, column_ids, table_io_manager, unbound_expressions, db),\n-      allocators(allocators_ptr), owns_data(false) {\n+      allocators(allocators_ptr), owns_data(false), verify_max_key_len(false) {\n \n \t// FIXME: Use the new byte representation function to support nested types.\n \tfor (idx_t i = 0; i < types.size(); i++) {\n@@ -70,6 +70,12 @@ ART::ART(const string &name, const IndexConstraintType index_constraint_type, co\n \t\t}\n \t}\n \n+\tif (types.size() > 1) {\n+\t\tverify_max_key_len = true;\n+\t} else if (types[0] == PhysicalType::VARCHAR) {\n+\t\tverify_max_key_len = true;\n+\t}\n+\n \t// Initialize the allocators.\n \tSetPrefixCount(info);\n \tif (!allocators) {\n@@ -380,11 +386,25 @@ void GenerateKeysInternal(ArenaAllocator &allocator, DataChunk &input, unsafe_ve\n template <>\n void ART::GenerateKeys<>(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys) {\n \tGenerateKeysInternal<false>(allocator, input, keys);\n+\tif (!verify_max_key_len) {\n+\t\treturn;\n+\t}\n+\tauto max_len = MAX_KEY_LEN * idx_t(prefix_count);\n+\tfor (idx_t i = 0; i < input.size(); i++) {\n+\t\tkeys[i].VerifyKeyLength(max_len);\n+\t}\n }\n \n template <>\n void ART::GenerateKeys<true>(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys) {\n \tGenerateKeysInternal<true>(allocator, input, keys);\n+\tif (!verify_max_key_len) {\n+\t\treturn;\n+\t}\n+\tauto max_len = MAX_KEY_LEN * idx_t(prefix_count);\n+\tfor (idx_t i = 0; i < input.size(); i++) {\n+\t\tkeys[i].VerifyKeyLength(max_len);\n+\t}\n }\n \n void ART::GenerateKeyVectors(ArenaAllocator &allocator, DataChunk &input, Vector &row_ids, unsafe_vector<ARTKey> &keys,\n@@ -976,6 +996,8 @@ bool ART::Scan(IndexScanState &state, const idx_t max_count, unsafe_vector<row_t\n \tD_ASSERT(scan_state.values[0].type().InternalType() == types[0]);\n \tArenaAllocator arena_allocator(Allocator::Get(db));\n \tauto key = ARTKey::CreateKey(arena_allocator, types[0], scan_state.values[0]);\n+\tauto max_len = MAX_KEY_LEN * prefix_count;\n+\tkey.VerifyKeyLength(max_len);\n \n \tif (scan_state.values[1].IsNull()) {\n \t\t// Single predicate.\n@@ -1000,6 +1022,8 @@ bool ART::Scan(IndexScanState &state, const idx_t max_count, unsafe_vector<row_t\n \tlock_guard<mutex> l(lock);\n \tD_ASSERT(scan_state.values[1].type().InternalType() == types[0]);\n \tauto upper_bound = ARTKey::CreateKey(arena_allocator, types[0], scan_state.values[1]);\n+\tupper_bound.VerifyKeyLength(max_len);\n+\n \tbool left_equal = scan_state.expressions[0] == ExpressionType ::COMPARE_GREATERTHANOREQUALTO;\n \tbool right_equal = scan_state.expressions[1] == ExpressionType ::COMPARE_LESSTHANOREQUALTO;\n \treturn SearchCloseRange(key, upper_bound, left_equal, right_equal, max_count, row_ids);\n@@ -1305,11 +1329,6 @@ void ART::SetPrefixCount(const IndexStorageInfo &info) {\n \t\treturn;\n \t}\n \n-\tif (!IsUnique()) {\n-\t\tprefix_count = Prefix::ROW_ID_COUNT;\n-\t\treturn;\n-\t}\n-\n \tidx_t compound_size = 0;\n \tfor (const auto &type : types) {\n \t\tcompound_size += GetTypeIdSize(type);\ndiff --git a/src/execution/index/art/art_key.cpp b/src/execution/index/art/art_key.cpp\nindex d5769f0f5810..bcc949434956 100644\n--- a/src/execution/index/art/art_key.cpp\n+++ b/src/execution/index/art/art_key.cpp\n@@ -16,6 +16,13 @@ ARTKey::ARTKey(ArenaAllocator &allocator, idx_t len) : len(len) {\n \tdata = allocator.Allocate(len);\n }\n \n+void ARTKey::VerifyKeyLength(const idx_t max_len) const {\n+\tif (len > max_len) {\n+\t\tthrow InvalidInputException(\"key size of %d bytes exceeds the maximum size of %d bytes for this ART\", len,\n+\t\t                            max_len);\n+\t}\n+}\n+\n template <>\n ARTKey ARTKey::CreateARTKey(ArenaAllocator &allocator, string_t value) {\n \tauto string_data = const_data_ptr_cast(value.GetData());\n@@ -29,22 +36,22 @@ ARTKey ARTKey::CreateARTKey(ArenaAllocator &allocator, string_t value) {\n \t\t}\n \t}\n \n-\tidx_t len = string_len + escape_count + 1;\n-\tauto data = allocator.Allocate(len);\n+\tidx_t key_len = string_len + escape_count + 1;\n+\tauto key_data = allocator.Allocate(key_len);\n \n \t// Copy over the data and add escapes.\n \tidx_t pos = 0;\n \tfor (idx_t i = 0; i < string_len; i++) {\n \t\tif (string_data[i] <= 1) {\n \t\t\t// Add escape.\n-\t\t\tdata[pos++] = '\\01';\n+\t\t\tkey_data[pos++] = '\\01';\n \t\t}\n-\t\tdata[pos++] = string_data[i];\n+\t\tkey_data[pos++] = string_data[i];\n \t}\n \n \t// End with a null-terminator.\n-\tdata[pos] = '\\0';\n-\treturn ARTKey(data, len);\n+\tkey_data[pos] = '\\0';\n+\treturn ARTKey(key_data, key_len);\n }\n \n template <>\ndiff --git a/src/execution/operator/csv_scanner/sniffer/dialect_detection.cpp b/src/execution/operator/csv_scanner/sniffer/dialect_detection.cpp\nindex 74c928b332c6..ef7c3c101248 100644\n--- a/src/execution/operator/csv_scanner/sniffer/dialect_detection.cpp\n+++ b/src/execution/operator/csv_scanner/sniffer/dialect_detection.cpp\n@@ -525,6 +525,15 @@ void CSVSniffer::RefineCandidates() {\n \t\t\tunique_ptr<ColumnCountScanner> cc_best_candidate = std::move(successful_candidates[i]);\n \t\t\tif (cc_best_candidate->state_machine->state_machine_options.quote != '\\0' &&\n \t\t\t    cc_best_candidate->ever_quoted) {\n+\t\t\t\t// If we have multiple candidates with the same quote, but different escapes\n+\t\t\t\tfor (idx_t j = i + 1; j < successful_candidates.size(); j++) {\n+\t\t\t\t\t// we give preference if it has the same character between escape and quote\n+\t\t\t\t\tif (successful_candidates[j]->state_machine->state_machine_options.escape ==\n+\t\t\t\t\t    successful_candidates[j]->state_machine->state_machine_options.quote) {\n+\t\t\t\t\t\tcc_best_candidate = std::move(successful_candidates[j]);\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t}\n+\t\t\t\t}\n \t\t\t\tcandidates.clear();\n \t\t\t\tcandidates.push_back(std::move(cc_best_candidate));\n \t\t\t\treturn;\ndiff --git a/src/execution/operator/persistent/physical_batch_insert.cpp b/src/execution/operator/persistent/physical_batch_insert.cpp\nindex 3caaff78914d..5090588848fe 100644\n--- a/src/execution/operator/persistent/physical_batch_insert.cpp\n+++ b/src/execution/operator/persistent/physical_batch_insert.cpp\n@@ -542,8 +542,12 @@ SinkResultType PhysicalBatchInsert::Sink(ExecutionContext &context, DataChunk &c\n \tif (!lstate.constraint_state) {\n \t\tlstate.constraint_state = table.GetStorage().InitializeConstraintState(table, bound_constraints);\n \t}\n+\n \tauto &storage = table.GetStorage();\n-\tstorage.VerifyAppendConstraints(*lstate.constraint_state, context.client, lstate.insert_chunk, nullptr, nullptr);\n+\tauto &local_storage = LocalStorage::Get(context.client, storage.db);\n+\tauto local_table_storage = local_storage.GetStorage(table.GetStorage());\n+\tstorage.VerifyAppendConstraints(*lstate.constraint_state, context.client, lstate.insert_chunk, local_table_storage,\n+\t                                nullptr);\n \n \tauto &collection = table.GetStorage().GetOptimisticCollection(context.client, lstate.collection_index);\n \tauto new_row_group = collection.Append(lstate.insert_chunk, lstate.current_append_state);\ndiff --git a/src/execution/operator/persistent/physical_insert.cpp b/src/execution/operator/persistent/physical_insert.cpp\nindex 8d5572be200c..ca99242717dd 100644\n--- a/src/execution/operator/persistent/physical_insert.cpp\n+++ b/src/execution/operator/persistent/physical_insert.cpp\n@@ -479,13 +479,12 @@ static idx_t HandleInsertConflicts(TableCatalogEntry &table, ExecutionContext &c\n \n \tConflictInfo conflict_info(conflict_target);\n \tConflictManager conflict_manager(VerifyExistenceType::APPEND, tuples.size(), &conflict_info);\n+\tauto storage = local_storage.GetStorage(data_table);\n \tif (GLOBAL) {\n \t\tauto &constraint_state = lstate.GetConstraintState(data_table, table);\n-\t\tauto storage = local_storage.GetStorage(data_table);\n \t\tdata_table.VerifyAppendConstraints(constraint_state, context.client, tuples, storage, &conflict_manager);\n \t} else {\n \t\tauto &indexes = local_storage.GetIndexes(data_table);\n-\t\tauto storage = local_storage.GetStorage(data_table);\n \t\tDataTable::VerifyUniqueIndexes(indexes, storage, tuples, &conflict_manager);\n \t}\n \ndiff --git a/src/execution/operator/schema/physical_create_art_index.cpp b/src/execution/operator/schema/physical_create_art_index.cpp\nindex 11d05fd8cce0..c5f51fb12ef5 100644\n--- a/src/execution/operator/schema/physical_create_art_index.cpp\n+++ b/src/execution/operator/schema/physical_create_art_index.cpp\n@@ -142,8 +142,8 @@ SinkResultType PhysicalCreateARTIndex::Sink(ExecutionContext &context, DataChunk\n \t\t}\n \t}\n \n-\tART::GenerateKeyVectors(l_state.arena_allocator, l_state.key_chunk, chunk.data[chunk.ColumnCount() - 1],\n-\t                        l_state.keys, l_state.row_ids);\n+\tl_state.local_index->Cast<ART>().GenerateKeyVectors(\n+\t    l_state.arena_allocator, l_state.key_chunk, chunk.data[chunk.ColumnCount() - 1], l_state.keys, l_state.row_ids);\n \n \tif (sorted) {\n \t\treturn SinkSorted(input);\ndiff --git a/src/function/table/table_scan.cpp b/src/function/table/table_scan.cpp\nindex 361bc718f112..58cb9274ee14 100644\n--- a/src/function/table/table_scan.cpp\n+++ b/src/function/table/table_scan.cpp\n@@ -42,8 +42,12 @@ struct IndexScanLocalState : public LocalTableFunctionState {\n \t//! The DataChunk containing all read columns.\n \t//! This includes filter columns, which are immediately removed.\n \tDataChunk all_columns;\n-\t//! Fetch state\n+\t//! The row fetch state.\n \tColumnFetchState fetch_state;\n+\t//! The current position in the local storage scan.\n+\tTableScanState scan_state;\n+\t//! The column IDs of the local storage scan.\n+\tvector<StorageIndex> column_ids;\n };\n \n static StorageIndex TransformStorageIndex(const ColumnIndex &column_id) {\n@@ -118,8 +122,6 @@ class DuckIndexScanState : public TableScanGlobalState {\n \t//! Synchronize changes to the global index scan state.\n \tmutex index_scan_lock;\n \n-\tTableScanState table_scan_state;\n-\n public:\n \tunique_ptr<LocalTableFunctionState> InitLocalState(ExecutionContext &context,\n \t                                                   TableFunctionInitInput &input) override {\n@@ -127,6 +129,19 @@ class DuckIndexScanState : public TableScanGlobalState {\n \t\tif (input.CanRemoveFilterColumns()) {\n \t\t\tl_state->all_columns.Initialize(context.client, scanned_types);\n \t\t}\n+\t\tl_state->scan_state.options.force_fetch_row = ClientConfig::GetConfig(context.client).force_fetch_row;\n+\n+\t\t// Initialize the local storage scan.\n+\t\tauto &bind_data = input.bind_data->Cast<TableScanBindData>();\n+\t\tauto &duck_table = bind_data.table.Cast<DuckTableEntry>();\n+\t\tauto &storage = duck_table.GetStorage();\n+\t\tauto &local_storage = LocalStorage::Get(context.client, duck_table.catalog);\n+\n+\t\tfor (const auto &col_idx : input.column_indexes) {\n+\t\t\tl_state->column_ids.push_back(GetStorageIndex(bind_data.table, col_idx));\n+\t\t}\n+\t\tl_state->scan_state.Initialize(l_state->column_ids, context.client, input.filters.get());\n+\t\tlocal_storage.InitializeScan(storage, l_state->scan_state.local_state, input.filters);\n \t\treturn std::move(l_state);\n \t}\n \n@@ -172,10 +187,10 @@ class DuckIndexScanState : public TableScanGlobalState {\n \t\t\tauto &local_storage = LocalStorage::Get(tx);\n \t\t\tif (CanRemoveFilterColumns()) {\n \t\t\t\tl_state.all_columns.Reset();\n-\t\t\t\tlocal_storage.Scan(table_scan_state.local_state, column_ids, l_state.all_columns);\n+\t\t\t\tlocal_storage.Scan(l_state.scan_state.local_state, column_ids, l_state.all_columns);\n \t\t\t\toutput.ReferenceColumns(l_state.all_columns, projection_ids);\n \t\t\t} else {\n-\t\t\t\tlocal_storage.Scan(table_scan_state.local_state, column_ids, output);\n+\t\t\t\tlocal_storage.Scan(l_state.scan_state.local_state, column_ids, output);\n \t\t\t}\n \t\t}\n \t}\n@@ -325,7 +340,7 @@ unique_ptr<GlobalTableFunctionState> DuckTableScanInitGlobal(ClientContext &cont\n }\n \n unique_ptr<GlobalTableFunctionState> DuckIndexScanInitGlobal(ClientContext &context, TableFunctionInitInput &input,\n-                                                             DataTable &storage, const TableScanBindData &bind_data,\n+                                                             const TableScanBindData &bind_data,\n                                                              unsafe_vector<row_t> &row_ids) {\n \tauto g_state = make_uniq<DuckIndexScanState>(context, input.bind_data.get());\n \tif (!row_ids.empty()) {\n@@ -335,9 +350,6 @@ unique_ptr<GlobalTableFunctionState> DuckIndexScanInitGlobal(ClientContext &cont\n \tg_state->finished = g_state->row_ids.empty() ? true : false;\n \n \tauto &duck_table = bind_data.table.Cast<DuckTableEntry>();\n-\tauto &local_storage = LocalStorage::Get(context, duck_table.catalog);\n-\tg_state->table_scan_state.options.force_fetch_row = ClientConfig::GetConfig(context).force_fetch_row;\n-\n \tif (input.CanRemoveFilterColumns()) {\n \t\tg_state->projection_ids = input.projection_ids;\n \t}\n@@ -352,9 +364,6 @@ unique_ptr<GlobalTableFunctionState> DuckIndexScanInitGlobal(ClientContext &cont\n \t\tg_state->scanned_types.push_back(columns.GetColumn(col_idx.ToLogical()).Type());\n \t}\n \n-\tg_state->table_scan_state.Initialize(g_state->column_ids, context, input.filters);\n-\tlocal_storage.InitializeScan(storage, g_state->table_scan_state.local_state, input.filters);\n-\n \t// Const-cast to indicate an index scan.\n \t// We need this information in the bind data so that we can access it during ANALYZE.\n \tauto &no_const_bind_data = bind_data.CastNoConst<TableScanBindData>();\n@@ -604,7 +613,7 @@ unique_ptr<GlobalTableFunctionState> TableScanInitGlobal(ClientContext &context,\n \tif (!index_scan) {\n \t\treturn DuckTableScanInitGlobal(context, input, storage, bind_data);\n \t}\n-\treturn DuckIndexScanInitGlobal(context, input, storage, bind_data, row_ids);\n+\treturn DuckIndexScanInitGlobal(context, input, bind_data, row_ids);\n }\n \n static unique_ptr<BaseStatistics> TableScanStatistics(ClientContext &context, const FunctionData *bind_data_p,\ndiff --git a/src/function/window/window_constant_aggregator.cpp b/src/function/window/window_constant_aggregator.cpp\nindex 312161223903..7ae1784b133e 100644\n--- a/src/function/window/window_constant_aggregator.cpp\n+++ b/src/function/window/window_constant_aggregator.cpp\n@@ -308,7 +308,9 @@ void WindowConstantAggregator::Finalize(WindowAggregatorState &gstate, WindowAgg\n \tlastate.statef.Combine(gastate.statef);\n \tlastate.statef.Destroy();\n \n-\tgastate.statef.Finalize(*gastate.results);\n+\tif (!--gastate.locals) {\n+\t\tgastate.statef.Finalize(*gastate.results);\n+\t}\n }\n \n unique_ptr<WindowAggregatorState> WindowConstantAggregator::GetLocalState(const WindowAggregatorState &gstate) const {\ndiff --git a/src/include/duckdb/execution/index/art/art.hpp b/src/include/duckdb/execution/index/art/art.hpp\nindex d6f41c1c4d92..ea638abf6bd2 100644\n--- a/src/include/duckdb/execution/index/art/art.hpp\n+++ b/src/include/duckdb/execution/index/art/art.hpp\n@@ -35,6 +35,8 @@ class ART : public BoundIndex {\n \tstatic constexpr uint8_t ALLOCATOR_COUNT = 9;\n \t//! FixedSizeAllocator count of deprecated ARTs.\n \tstatic constexpr uint8_t DEPRECATED_ALLOCATOR_COUNT = ALLOCATOR_COUNT - 3;\n+\t//! Keys must not exceed MAX_KEY_LEN * prefix_count.\n+\tstatic constexpr idx_t MAX_KEY_LEN = 8192;\n \n public:\n \tART(const string &name, const IndexConstraintType index_constraint_type, const vector<column_t> &column_ids,\n@@ -59,6 +61,8 @@ class ART : public BoundIndex {\n \tshared_ptr<array<unsafe_unique_ptr<FixedSizeAllocator>, ALLOCATOR_COUNT>> allocators;\n \t//! True, if the ART owns its data.\n \tbool owns_data;\n+\t//! True, if keys need a key length verification pass.\n+\tbool verify_max_key_len;\n \t//! The number of bytes fitting in the prefix.\n \tuint8_t prefix_count;\n \n@@ -106,9 +110,9 @@ class ART : public BoundIndex {\n \n \t//! ART key generation.\n \ttemplate <bool IS_NOT_NULL = false>\n-\tstatic void GenerateKeys(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys);\n-\tstatic void GenerateKeyVectors(ArenaAllocator &allocator, DataChunk &input, Vector &row_ids,\n-\t                               unsafe_vector<ARTKey> &keys, unsafe_vector<ARTKey> &row_id_keys);\n+\tvoid GenerateKeys(ArenaAllocator &allocator, DataChunk &input, unsafe_vector<ARTKey> &keys);\n+\tvoid GenerateKeyVectors(ArenaAllocator &allocator, DataChunk &input, Vector &row_ids, unsafe_vector<ARTKey> &keys,\n+\t                        unsafe_vector<ARTKey> &row_id_keys);\n \n \t//! Verifies the nodes and optionally returns a string of the ART.\n \tstring VerifyAndToString(IndexLock &state, const bool only_verify) override;\ndiff --git a/src/include/duckdb/execution/index/art/art_key.hpp b/src/include/duckdb/execution/index/art/art_key.hpp\nindex e19e3cbb9d6f..dabb414ad798 100644\n--- a/src/include/duckdb/execution/index/art/art_key.hpp\n+++ b/src/include/duckdb/execution/index/art/art_key.hpp\n@@ -73,6 +73,7 @@ class ARTKey {\n \tvoid Concat(ArenaAllocator &allocator, const ARTKey &other);\n \trow_t GetRowId() const;\n \tidx_t GetMismatchPos(const ARTKey &other, const idx_t start) const;\n+\tvoid VerifyKeyLength(const idx_t max_len) const;\n \n private:\n \ttemplate <class T>\ndiff --git a/src/main/extension/extension_helper.cpp b/src/main/extension/extension_helper.cpp\nindex 1088f0f135dd..cbf2cae8b160 100644\n--- a/src/main/extension/extension_helper.cpp\n+++ b/src/main/extension/extension_helper.cpp\n@@ -224,6 +224,14 @@ bool ExtensionHelper::TryAutoLoadExtension(ClientContext &context, const string\n \t}\n }\n \n+static string GetAutoInstallExtensionsRepository(const DBConfigOptions &options) {\n+\tstring repository_url = options.autoinstall_extension_repo;\n+\tif (repository_url.empty()) {\n+\t\trepository_url = options.custom_extension_repo;\n+\t}\n+\treturn repository_url;\n+}\n+\n bool ExtensionHelper::TryAutoLoadExtension(DatabaseInstance &instance, const string &extension_name) noexcept {\n \tif (instance.ExtensionIsLoaded(extension_name)) {\n \t\treturn true;\n@@ -232,8 +240,8 @@ bool ExtensionHelper::TryAutoLoadExtension(DatabaseInstance &instance, const str\n \ttry {\n \t\tauto &fs = FileSystem::GetFileSystem(instance);\n \t\tif (dbconfig.options.autoinstall_known_extensions) {\n-\t\t\tauto autoinstall_repo =\n-\t\t\t    ExtensionRepository::GetRepositoryByUrl(dbconfig.options.autoinstall_extension_repo);\n+\t\t\tauto repository_url = GetAutoInstallExtensionsRepository(dbconfig.options);\n+\t\t\tauto autoinstall_repo = ExtensionRepository::GetRepositoryByUrl(repository_url);\n \t\t\tExtensionInstallOptions options;\n \t\t\toptions.repository = autoinstall_repo;\n \t\t\tExtensionHelper::InstallExtension(instance, fs, extension_name, options);\n@@ -382,10 +390,10 @@ void ExtensionHelper::AutoLoadExtension(DatabaseInstance &db, const string &exte\n \t\tauto fs = FileSystem::CreateLocal();\n #ifndef DUCKDB_WASM\n \t\tif (dbconfig.options.autoinstall_known_extensions) {\n-\t\t\t//! Get the autoloading repository\n-\t\t\tauto repository = ExtensionRepository::GetRepositoryByUrl(dbconfig.options.autoinstall_extension_repo);\n+\t\t\tauto repository_url = GetAutoInstallExtensionsRepository(dbconfig.options);\n+\t\t\tauto autoinstall_repo = ExtensionRepository::GetRepositoryByUrl(repository_url);\n \t\t\tExtensionInstallOptions options;\n-\t\t\toptions.repository = repository;\n+\t\t\toptions.repository = autoinstall_repo;\n \t\t\tExtensionHelper::InstallExtension(db, *fs, extension_name, options);\n \t\t}\n #endif\ndiff --git a/src/main/extension/extension_install.cpp b/src/main/extension/extension_install.cpp\nindex 6cb8ad24e185..711bbca19cf9 100644\n--- a/src/main/extension/extension_install.cpp\n+++ b/src/main/extension/extension_install.cpp\n@@ -220,11 +220,10 @@ string ExtensionHelper::ExtensionUrlTemplate(optional_ptr<const DatabaseInstance\n \t} else {\n \t\tversioned_path = \"/${REVISION}/${PLATFORM}/${NAME}.duckdb_extension\";\n \t}\n+\tstring default_endpoint = ExtensionRepository::DEFAULT_REPOSITORY_URL;\n #ifdef WASM_LOADABLE_EXTENSIONS\n-\tstring default_endpoint = DEFAULT_REPOSITORY;\n \tversioned_path = versioned_path + \".wasm\";\n #else\n-\tstring default_endpoint = ExtensionRepository::DEFAULT_REPOSITORY_URL;\n \tversioned_path = versioned_path + CompressionExtensionFromType(FileCompressionType::GZIP);\n #endif\n \tstring url_template = repository.path + versioned_path;\ndiff --git a/src/optimizer/column_lifetime_analyzer.cpp b/src/optimizer/column_lifetime_analyzer.cpp\nindex 90d0d60237c1..cc34d2d53378 100644\n--- a/src/optimizer/column_lifetime_analyzer.cpp\n+++ b/src/optimizer/column_lifetime_analyzer.cpp\n@@ -223,6 +223,9 @@ void ColumnLifetimeAnalyzer::AddVerificationProjection(unique_ptr<LogicalOperato\n \n \t// Create a projection and swap the operators accordingly\n \tauto projection = make_uniq<LogicalProjection>(table_index, std::move(expressions));\n+\tif (child->has_estimated_cardinality) {\n+\t\tprojection->SetEstimatedCardinality(child->estimated_cardinality);\n+\t}\n \tprojection->children.emplace_back(std::move(child));\n \tchild = std::move(projection);\n \ndiff --git a/src/optimizer/late_materialization.cpp b/src/optimizer/late_materialization.cpp\nindex 5c2f19231a93..acbcd5bbcf85 100644\n--- a/src/optimizer/late_materialization.cpp\n+++ b/src/optimizer/late_materialization.cpp\n@@ -331,12 +331,18 @@ bool LateMaterialization::TryLateMaterialization(unique_ptr<LogicalOperator> &op\n \tif (root_type == LogicalOperatorType::LOGICAL_TOP_N) {\n \t\t// for top-n we need to order on expressions, so we need to order AFTER the final projection\n \t\tauto proj = make_uniq<LogicalProjection>(proj_index, std::move(final_proj_list));\n+\t\tif (join->has_estimated_cardinality) {\n+\t\t\tproj->SetEstimatedCardinality(join->estimated_cardinality);\n+\t\t}\n \t\tproj->children.push_back(std::move(join));\n \n \t\tfor (auto &order : final_orders) {\n \t\t\tReplaceTableReferences(*order.expression, proj_index);\n \t\t}\n \t\tauto order = make_uniq<LogicalOrder>(std::move(final_orders));\n+\t\tif (proj->has_estimated_cardinality) {\n+\t\t\torder->SetEstimatedCardinality(proj->estimated_cardinality);\n+\t\t}\n \t\torder->children.push_back(std::move(proj));\n \n \t\top = std::move(order);\n@@ -344,9 +350,15 @@ bool LateMaterialization::TryLateMaterialization(unique_ptr<LogicalOperator> &op\n \t\t// for limit/sample we order on row-id, so we need to order BEFORE the final projection\n \t\t// because the final projection removes row-ids\n \t\tauto order = make_uniq<LogicalOrder>(std::move(final_orders));\n+\t\tif (join->has_estimated_cardinality) {\n+\t\t\torder->SetEstimatedCardinality(join->estimated_cardinality);\n+\t\t}\n \t\torder->children.push_back(std::move(join));\n \n \t\tauto proj = make_uniq<LogicalProjection>(proj_index, std::move(final_proj_list));\n+\t\tif (order->has_estimated_cardinality) {\n+\t\t\tproj->SetEstimatedCardinality(order->estimated_cardinality);\n+\t\t}\n \t\tproj->children.push_back(std::move(order));\n \n \t\top = std::move(proj);\ndiff --git a/src/optimizer/sum_rewriter.cpp b/src/optimizer/sum_rewriter.cpp\nindex 587144110e6b..a08d9ef4c12c 100644\n--- a/src/optimizer/sum_rewriter.cpp\n+++ b/src/optimizer/sum_rewriter.cpp\n@@ -167,6 +167,9 @@ void SumRewriterOptimizer::RewriteSums(unique_ptr<LogicalOperator> &op) {\n \n \t// push the projection to replace the aggregate\n \tauto proj = make_uniq<LogicalProjection>(proj_index, std::move(projection_expressions));\n+\tif (op->has_estimated_cardinality) {\n+\t\tproj->SetEstimatedCardinality(op->estimated_cardinality);\n+\t}\n \tproj->children.push_back(std::move(op));\n \top = std::move(proj);\n }\ndiff --git a/tools/pythonpkg/README.md b/tools/pythonpkg/README.md\nindex 722a4df6695e..0cb65c7e413b 100644\n--- a/tools/pythonpkg/README.md\n+++ b/tools/pythonpkg/README.md\n@@ -77,50 +77,18 @@ pip install --prefix $DUCKDB_PREFIX -e $DUCKDB_PREFIX/src/duckdb-pythonpkg/duckd\n \n ## Development and Stubs\n \n-`*.pyi` stubs are generated with [Mypy's `stubgen`](https://mypy.readthedocs.io/en/stable/stubgen.html) and tweaked. These are important for autocomplete in many IDEs, as static-analysis based language servers can't introspect `duckdb`'s binary module.\n-\n-The stubs from stubgen are pretty good, but not perfect. In some cases, you can help stubgen out: for example, function annotation types that it can't figure out should be specified in the cpp where necessary, as in the example.\n-\n-```cpp\n-// without this change, the generated stub is\n-// def query_df(self, df: object, virtual_table_name: str, sql_query: str) -> DuckDBPyRelation: ...\n-pybind_opts.disable_function_signatures();\n-m.def(\"query_df\", &DuckDBPyRelation::QueryDF,\n-      \"query_df(self, df: pandas.DataFrame, virtual_table_name: str, sql_query: str) -> DuckDBPyRelation \\n\"\n-      \"Run the given SQL query in sql_query on the view named virtual_table_name that contains the content of \"\n-      \"Data.Frame df\",\n-      py::arg(\"df\"), py::arg(\"virtual_table_name\"), py::arg(\"sql_query\"));\n-pybind_opts.enable_function_signatures();\n-// now the generated stub is\n-// def query_df(self, df: pandas.DataFrame, virtual_table_name: str, sql_query: str) -> DuckDBPyRelation: ...\n-```\n-\n-If you want to regenerate the stubs, there is a bit of a chicken and egg situation - the stubs should go in the package, but\n-`stubgen` needs to look at the package to generate the stubs!\n-\n-There is a test that you can run to check the stubs match the real duckdb package - this runs in CI (sorry in advance...). If you add a method to the duckdb py library and forget to add it to the stub, this test will helpfully fail. The test is a run of [mypy.stubtest](https://github.com/python/mypy/issues/5028#issuecomment-740101546).\n+`*.pyi` stubs in `duckdb-stubs` are manually maintained. The connection-related stubs are generated using dedicated scripts in `tools/pythonpkg/scripts/`:\n+- `generate_connection_stubs.py`\n+- `generate_connection_wrapper_stubs.py`\n \n-The workflow for getting the stubs right will look something like\n+These stubs are important for autocomplete in many IDEs, as static-analysis based language servers can't introspect `duckdb`'s binary module.\n \n+To verify the stubs match the actual implementation:\n ```bash\n-# Edit python package...\n-vim tools/pythonpkg/duckdb_python.cpp # or whatever\n-\n-# Install duckdb python package to\n-#  - compile it so stubgen can read it\n-#  - put the stubs in editable mode so you can tweak them easily\n-(cd tools/pythonpkg; pip install -e .)\n-# regerate stub once your changes have been installed.\n-scripts/regenerate_python_stubs.sh\n-# (re-apply our fixes on top of generate stubs,\n-# hint: git add -p; git checkout HEAD tools/pythonpkg/duckdb-stubs)\n-\n-# check tests\n-pytest tests/stubs\n-# edit and re-test stubs until you're happy\n+python3 -m pytest tests/stubs\n ```\n \n-All the above should be done in a virtualenv.\n+If you add new methods to the DuckDB Python API, you'll need to manually add corresponding type hints to the stub files.\n \n ## Frequently encountered issue with extensions\n \ndiff --git a/tools/pythonpkg/duckdb-stubs/__init__.pyi b/tools/pythonpkg/duckdb-stubs/__init__.pyi\nindex 507af1141a71..5aecae1f6eef 100644\n--- a/tools/pythonpkg/duckdb-stubs/__init__.pyi\n+++ b/tools/pythonpkg/duckdb-stubs/__init__.pyi\n@@ -243,7 +243,7 @@ class Expression:\n     def __lt__(self, expr: \"Expression\") -> \"Expression\": ...\n     def __le__(self, expr: \"Expression\") -> \"Expression\": ...\n \n-    def show(self, max_width: Optional[int] = None, max_rows: Optional[int] = None, max_col_width: Optional[int] = None, null_value: Optional[str] = None, render_mode: Optional[RenderMode] = None) -> None: ...\n+    def show(self) -> None: ...\n     def __repr__(self) -> str: ...\n     def get_name(self) -> str: ...\n     def alias(self, alias: str) -> \"Expression\": ...\n@@ -268,7 +268,7 @@ def ConstantExpression(val: Any) -> Expression: ...\n def CaseExpression(condition: Expression, value: Expression) -> Expression: ...\n def FunctionExpression(function: str, *cols: Expression) -> Expression: ...\n def CoalesceOperator(*cols: Expression) -> Expression: ...\n-def LambdaExpression(lhs: Union[Tuple[\"Expression\", ...], str]) -> Expression: ...\n+def LambdaExpression(lhs: Union[Tuple[\"Expression\", ...], str], rhs: Expression) -> Expression: ...\n def SQLExpression(expr: str) -> Expression: ...\n \n class DuckDBPyConnection:\n@@ -449,7 +449,7 @@ class DuckDBPyRelation:\n     def select_types(self, types: List[Union[str, DuckDBPyType]]) -> DuckDBPyRelation: ...\n     def select_dtypes(self, types: List[Union[str, DuckDBPyType]]) -> DuckDBPyRelation: ...\n     def set_alias(self, alias: str) -> DuckDBPyRelation: ...\n-    def show(self) -> None: ...\n+    def show(self, max_width: Optional[int] = None, max_rows: Optional[int] = None, max_col_width: Optional[int] = None, null_value: Optional[str] = None, render_mode: Optional[RenderMode] = None) -> None: ...\n     def sql_query(self) -> str: ...\n     def to_arrow_table(self, batch_size: int = ...) -> pyarrow.lib.Table: ...\n     def to_csv(\ndiff --git a/tools/pythonpkg/scripts/cache_data.json b/tools/pythonpkg/scripts/cache_data.json\nindex 685499fbf636..6de027f9a464 100644\n--- a/tools/pythonpkg/scripts/cache_data.json\n+++ b/tools/pythonpkg/scripts/cache_data.json\n@@ -315,7 +315,6 @@\n         \"full_path\": \"numpy\",\n         \"name\": \"numpy\",\n         \"children\": [\n-            \"numpy.core\",\n             \"numpy.ma\",\n             \"numpy.ndarray\",\n             \"numpy.datetime64\",\n@@ -342,20 +341,6 @@\n         ],\n         \"required\": false\n     },\n-    \"numpy.core\": {\n-        \"type\": \"attribute\",\n-        \"full_path\": \"numpy.core\",\n-        \"name\": \"core\",\n-        \"children\": [\n-            \"numpy.core.multiarray\"\n-        ]\n-    },\n-    \"numpy.core.multiarray\": {\n-        \"type\": \"attribute\",\n-        \"full_path\": \"numpy.core.multiarray\",\n-        \"name\": \"multiarray\",\n-        \"children\": []\n-    },\n     \"numpy.ma\": {\n         \"type\": \"attribute\",\n         \"full_path\": \"numpy.ma\",\ndiff --git a/tools/pythonpkg/scripts/imports.py b/tools/pythonpkg/scripts/imports.py\nindex d23200daf7b2..b25f37290729 100644\n--- a/tools/pythonpkg/scripts/imports.py\n+++ b/tools/pythonpkg/scripts/imports.py\n@@ -58,7 +58,6 @@\n \n import numpy\n \n-numpy.core.multiarray\n numpy.ma.masked\n numpy.ma.masked_array\n numpy.ndarray\ndiff --git a/tools/pythonpkg/src/arrow/arrow_array_stream.cpp b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\nindex 13140d85c043..69cc0a3e87b1 100644\n--- a/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n+++ b/tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n@@ -52,23 +52,21 @@ py::object PythonTableArrowArrayStreamFactory::ProduceScanner(DBConfig &config,\n \tpy::list projection_list = py::cast(column_list);\n \n \tbool has_filter = filters && !filters->filters.empty();\n+\tpy::dict kwargs;\n+\tif (!column_list.empty()) {\n+\t\tkwargs[\"columns\"] = projection_list;\n+\t}\n \n \tif (has_filter) {\n \t\tauto filter = TransformFilter(*filters, parameters.projected_columns.projection_map, filter_to_col,\n \t\t                              client_properties, arrow_table);\n-\t\tif (column_list.empty()) {\n-\t\t\treturn arrow_scanner(arrow_obj_handle, py::arg(\"filter\") = filter);\n-\t\t} else {\n-\t\t\treturn arrow_scanner(arrow_obj_handle, py::arg(\"columns\") = projection_list, py::arg(\"filter\") = filter);\n-\t\t}\n-\t} else {\n-\t\tif (column_list.empty()) {\n-\t\t\treturn arrow_scanner(arrow_obj_handle);\n-\t\t} else {\n-\t\t\treturn arrow_scanner(arrow_obj_handle, py::arg(\"columns\") = projection_list);\n+\t\tif (!filter.is(py::none())) {\n+\t\t\tkwargs[\"filter\"] = filter;\n \t\t}\n \t}\n+\treturn arrow_scanner(arrow_obj_handle, **kwargs);\n }\n+\n unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(uintptr_t factory_ptr,\n                                                                                 ArrowStreamParameters &parameters) {\n \tpy::gil_scoped_acquire acquire;\n@@ -342,6 +340,9 @@ py::object TransformFilterRecursive(TableFilter &filter, vector<string> column_r\n \t\tfor (idx_t i = 0; i < or_filter.child_filters.size(); i++) {\n \t\t\tauto &child_filter = *or_filter.child_filters[i];\n \t\t\tpy::object child_expression = TransformFilterRecursive(child_filter, column_ref, timezone_config, type);\n+\t\t\tif (child_expression.is(py::none())) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n \t\t\tif (expression.is(py::none())) {\n \t\t\t\texpression = std::move(child_expression);\n \t\t\t} else {\n@@ -356,6 +357,9 @@ py::object TransformFilterRecursive(TableFilter &filter, vector<string> column_r\n \t\tfor (idx_t i = 0; i < and_filter.child_filters.size(); i++) {\n \t\t\tauto &child_filter = *and_filter.child_filters[i];\n \t\t\tpy::object child_expression = TransformFilterRecursive(child_filter, column_ref, timezone_config, type);\n+\t\t\tif (child_expression.is(py::none())) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n \t\t\tif (expression.is(py::none())) {\n \t\t\t\texpression = std::move(child_expression);\n \t\t\t} else {\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp\nindex 7ec78a883842..d6abf1cb3b38 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/import_cache/modules/numpy_module.hpp\n@@ -26,18 +26,6 @@ struct NumpyMaCacheItem : public PythonImportCacheItem {\n \tPythonImportCacheItem masked_array;\n };\n \n-struct NumpyCoreCacheItem : public PythonImportCacheItem {\n-\n-public:\n-\tNumpyCoreCacheItem(optional_ptr<PythonImportCacheItem> parent)\n-\t    : PythonImportCacheItem(\"core\", parent), multiarray(\"multiarray\", this) {\n-\t}\n-\t~NumpyCoreCacheItem() override {\n-\t}\n-\n-\tPythonImportCacheItem multiarray;\n-};\n-\n struct NumpyCacheItem : public PythonImportCacheItem {\n \n public:\n@@ -45,9 +33,9 @@ struct NumpyCacheItem : public PythonImportCacheItem {\n \n public:\n \tNumpyCacheItem()\n-\t    : PythonImportCacheItem(\"numpy\"), core(this), ma(this), ndarray(\"ndarray\", this),\n-\t      datetime64(\"datetime64\", this), generic(\"generic\", this), int64(\"int64\", this), bool_(\"bool_\", this),\n-\t      byte(\"byte\", this), ubyte(\"ubyte\", this), short_(\"short\", this), ushort_(\"ushort\", this), intc(\"intc\", this),\n+\t    : PythonImportCacheItem(\"numpy\"), ma(this), ndarray(\"ndarray\", this), datetime64(\"datetime64\", this),\n+\t      generic(\"generic\", this), int64(\"int64\", this), bool_(\"bool_\", this), byte(\"byte\", this),\n+\t      ubyte(\"ubyte\", this), short_(\"short\", this), ushort_(\"ushort\", this), intc(\"intc\", this),\n \t      uintc(\"uintc\", this), int_(\"int_\", this), uint(\"uint\", this), longlong(\"longlong\", this),\n \t      ulonglong(\"ulonglong\", this), half(\"half\", this), float16(\"float16\", this), single(\"single\", this),\n \t      longdouble(\"longdouble\", this), csingle(\"csingle\", this), cdouble(\"cdouble\", this),\n@@ -56,7 +44,6 @@ struct NumpyCacheItem : public PythonImportCacheItem {\n \t~NumpyCacheItem() override {\n \t}\n \n-\tNumpyCoreCacheItem core;\n \tNumpyMaCacheItem ma;\n \tPythonImportCacheItem ndarray;\n \tPythonImportCacheItem datetime64;\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp b/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp\nindex 6717648d0b03..a492db9aaa6b 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp\n@@ -32,6 +32,8 @@ void gil_assert();\n bool is_list_like(handle obj);\n bool is_dict_like(handle obj);\n \n+std::string to_string(const object &obj);\n+\n } // namespace pybind11\n \n namespace duckdb {\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\nindex 36d1b29cdf93..f9fdc18eac37 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/pyconnection/pyconnection.hpp\n@@ -270,11 +270,14 @@ struct DuckDBPyConnection : public enable_shared_from_this<DuckDBPyConnection> {\n \tunique_ptr<DuckDBPyRelation> FromParquet(const string &file_glob, bool binary_as_string, bool file_row_number,\n \t                                         bool filename, bool hive_partitioning, bool union_by_name,\n \t                                         const py::object &compression = py::none());\n-\n \tunique_ptr<DuckDBPyRelation> FromParquets(const vector<string> &file_globs, bool binary_as_string,\n \t                                          bool file_row_number, bool filename, bool hive_partitioning,\n \t                                          bool union_by_name, const py::object &compression = py::none());\n \n+\tunique_ptr<DuckDBPyRelation> FromParquetInternal(Value &&file_param, bool binary_as_string, bool file_row_number,\n+\t                                                 bool filename, bool hive_partitioning, bool union_by_name,\n+\t                                                 const py::object &compression = py::none());\n+\n \tunique_ptr<DuckDBPyRelation> FromArrow(py::object &arrow_object);\n \n \tunordered_set<string> GetTableNames(const string &query);\ndiff --git a/tools/pythonpkg/src/pybind11/pybind_wrapper.cpp b/tools/pythonpkg/src/pybind11/pybind_wrapper.cpp\nindex 23bf80354a16..ce3122a0fc9c 100644\n--- a/tools/pythonpkg/src/pybind11/pybind_wrapper.cpp\n+++ b/tools/pythonpkg/src/pybind11/pybind_wrapper.cpp\n@@ -36,4 +36,9 @@ bool is_dict_like(handle obj) {\n \treturn isinstance(obj, mapping);\n }\n \n+// NOLINTNEXTLINE(readability-identifier-naming)\n+std::string to_string(const object &obj) {\n+\treturn std::string(py::str(obj));\n+}\n+\n } // namespace pybind11\ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 9163e2747289..8fb32c9872ae 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -1679,14 +1679,14 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromDF(const PandasDataFrame &v\n \treturn make_uniq<DuckDBPyRelation>(std::move(rel));\n }\n \n-unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_glob, bool binary_as_string,\n-                                                             bool file_row_number, bool filename,\n-                                                             bool hive_partitioning, bool union_by_name,\n-                                                             const py::object &compression) {\n+unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquetInternal(Value &&file_param, bool binary_as_string,\n+                                                                     bool file_row_number, bool filename,\n+                                                                     bool hive_partitioning, bool union_by_name,\n+                                                                     const py::object &compression) {\n \tauto &connection = con.GetConnection();\n \tstring name = \"parquet_\" + StringUtil::GenerateRandomName();\n \tvector<Value> params;\n-\tparams.emplace_back(file_glob);\n+\tparams.emplace_back(std::move(file_param));\n \tnamed_parameter_map_t named_parameters({{\"binary_as_string\", Value::BOOLEAN(binary_as_string)},\n \t                                        {\"file_row_number\", Value::BOOLEAN(file_row_number)},\n \t                                        {\"filename\", Value::BOOLEAN(filename)},\n@@ -1704,32 +1704,27 @@ unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_\n \treturn make_uniq<DuckDBPyRelation>(connection.TableFunction(\"parquet_scan\", params, named_parameters)->Alias(name));\n }\n \n+unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquet(const string &file_glob, bool binary_as_string,\n+                                                             bool file_row_number, bool filename,\n+                                                             bool hive_partitioning, bool union_by_name,\n+                                                             const py::object &compression) {\n+\tauto file_param = Value(file_glob);\n+\treturn FromParquetInternal(std::move(file_param), binary_as_string, file_row_number, filename, hive_partitioning,\n+\t                           union_by_name, compression);\n+}\n+\n unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromParquets(const vector<string> &file_globs, bool binary_as_string,\n                                                               bool file_row_number, bool filename,\n                                                               bool hive_partitioning, bool union_by_name,\n                                                               const py::object &compression) {\n-\tauto &connection = con.GetConnection();\n-\tstring name = \"parquet_\" + StringUtil::GenerateRandomName();\n \tvector<Value> params;\n \tauto file_globs_as_value = vector<Value>();\n \tfor (const auto &file : file_globs) {\n \t\tfile_globs_as_value.emplace_back(file);\n \t}\n-\tparams.emplace_back(Value::LIST(file_globs_as_value));\n-\tnamed_parameter_map_t named_parameters({{\"binary_as_string\", Value::BOOLEAN(binary_as_string)},\n-\t                                        {\"file_row_number\", Value::BOOLEAN(file_row_number)},\n-\t                                        {\"filename\", Value::BOOLEAN(filename)},\n-\t                                        {\"hive_partitioning\", Value::BOOLEAN(hive_partitioning)},\n-\t                                        {\"union_by_name\", Value::BOOLEAN(union_by_name)}});\n-\n-\tif (!py::none().is(compression)) {\n-\t\tif (!py::isinstance<py::str>(compression)) {\n-\t\t\tthrow InvalidInputException(\"from_parquet only accepts 'compression' as a string\");\n-\t\t}\n-\t\tnamed_parameters[\"compression\"] = Value(py::str(compression));\n-\t}\n-\n-\treturn make_uniq<DuckDBPyRelation>(connection.TableFunction(\"parquet_scan\", params, named_parameters)->Alias(name));\n+\tauto file_param = Value::LIST(file_globs_as_value);\n+\treturn FromParquetInternal(std::move(file_param), binary_as_string, file_row_number, filename, hive_partitioning,\n+\t                           union_by_name, compression);\n }\n \n unique_ptr<DuckDBPyRelation> DuckDBPyConnection::FromArrow(py::object &arrow_object) {\ndiff --git a/tools/pythonpkg/src/python_udf.cpp b/tools/pythonpkg/src/python_udf.cpp\nindex b615f64cc936..41daf4e2461c 100644\n--- a/tools/pythonpkg/src/python_udf.cpp\n+++ b/tools/pythonpkg/src/python_udf.cpp\n@@ -373,6 +373,17 @@ struct ParameterKind {\n \t}\n };\n \n+static bool NumpyDeprecatesAccessToCore(const py::tuple &numpy_version) {\n+\tif (numpy_version.empty()) {\n+\t\treturn false;\n+\t}\n+\tif (string(py::str(numpy_version[0])) == string(\"2\")) {\n+\t\t//! Starting with numpy version 2.0.0 the use of 'core' is deprecated.\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n struct PythonUDFData {\n public:\n \tPythonUDFData(const string &name, bool vectorized, FunctionNullHandling null_handling)\n@@ -475,9 +486,21 @@ struct PythonUDFData {\n \tScalarFunction GetFunction(const py::function &udf, PythonExceptionHandling exception_handling, bool side_effects,\n \t                           const ClientProperties &client_properties) {\n \n-\t\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n \t\t// Import this module, because importing this from a non-main thread causes a segfault\n-\t\t(void)import_cache.numpy.core.multiarray();\n+\n+\t\tauto &import_cache = *DuckDBPyConnection::ImportCache();\n+\t\tpy::handle core;\n+\t\tauto numpy = import_cache.numpy();\n+\t\tif (!numpy) {\n+\t\t\tthrow InvalidInputException(\"'numpy' is required for this operation, but it wasn't installed\");\n+\t\t}\n+\t\tauto numpy_version = py::cast<py::tuple>(numpy.attr(\"__version__\"));\n+\t\tif (NumpyDeprecatesAccessToCore(numpy_version)) {\n+\t\t\tcore = numpy.attr(\"_core\");\n+\t\t} else {\n+\t\t\tcore = numpy.attr(\"core\");\n+\t\t}\n+\t\t(void)core.attr(\"multiarray\");\n \n \t\tscalar_function_t func;\n \t\tif (vectorized) {\n",
  "test_patch": "diff --git a/test/extension/duckdb_extension_settings.test b/test/extension/duckdb_extension_settings.test\nnew file mode 100644\nindex 000000000000..34ca19445e02\n--- /dev/null\n+++ b/test/extension/duckdb_extension_settings.test\n@@ -0,0 +1,28 @@\n+# name: test/extension/duckdb_extension_settings.test\n+# description: settings for extensions\n+# group: [extension]\n+\n+statement ok\n+SET autoinstall_known_extensions = true;\n+\n+statement ok\n+SET autoload_known_extensions = true;\n+\n+statement ok\n+SET extension_directory = '__TEST_DIR__/custom_extension_directory';\n+\n+statement ok\n+SET custom_extension_repository = '__TEST_DIR__/not_existing_folder'\n+\n+statement error\n+FROM read_csv('https://some.org/file.csv');\n+----\n+not_existing_folder\n+\n+statement ok\n+SET autoinstall_extension_repository = '__TEST_DIR__/other_folder';\n+\n+statement error\n+FROM read_csv('https://some.org/file.csv');\n+----\n+other_folder\ndiff --git a/test/sql/copy/csv/test_quoted_later_escaped.test b/test/sql/copy/csv/test_quoted_later_escaped.test\nnew file mode 100644\nindex 000000000000..d723b663890f\n--- /dev/null\n+++ b/test/sql/copy/csv/test_quoted_later_escaped.test\n@@ -0,0 +1,35 @@\n+# name: test/sql/copy/csv/test_quoted_later_escaped.test\n+# description: Test quoted file, with escapes only happening much later.\n+# group: [csv]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE T as select '1, \"Oogie Boogie\"' from range (100000)\n+\n+statement ok\n+insert into T values ('2, \"\"\"sir\"\" Oogie Boogie\"')\n+\n+statement ok\n+COPY T to '__TEST_DIR__/out.csv' (FORMAT CSV, HEADER 0, QUOTE '');\n+\n+query II\n+SELECT quote, escape FROM sniff_csv('__TEST_DIR__/out.csv');\n+----\n+\"\t\"\n+\n+# Test we always give preference to escaped options\n+statement ok\n+CREATE TABLE T_2 as select '1, \"Oogie Boogie\"' from range (5000)\n+\n+statement ok\n+insert into T_2 values ('2, \"\\\"sir\\\" Oogie Boogie\"')\n+\n+statement ok\n+COPY T_2 to '__TEST_DIR__/out_2.csv' (FORMAT CSV, HEADER 0, QUOTE '');\n+\n+query II\n+SELECT quote, escape FROM sniff_csv('__TEST_DIR__/out_2.csv');\n+----\n+\"\t\\\n\\ No newline at end of file\ndiff --git a/test/sql/index/art/create_drop/test_art_long_keys.test_slow b/test/sql/index/art/create_drop/test_art_long_keys.test_slow\nnew file mode 100644\nindex 000000000000..09ce66b816b9\n--- /dev/null\n+++ b/test/sql/index/art/create_drop/test_art_long_keys.test_slow\n@@ -0,0 +1,49 @@\n+# name: test/sql/index/art/create_drop/test_art_long_keys.test_slow\n+# description: Test ART creation very long BLOBs.\n+# group: [create_drop]\n+\n+# Try creating an index with identical long keys.\n+statement ok\n+CREATE TABLE long_strings (id BLOB);\n+\n+statement ok\n+INSERT INTO long_strings SELECT repeat('k', 1000000);\n+\n+statement ok\n+INSERT INTO long_strings SELECT range::VARCHAR::BLOB FROM range(100000);\n+\n+statement ok\n+INSERT INTO long_strings SELECT repeat('k', 1000000);\n+\n+statement ok\n+INSERT INTO long_strings SELECT range::VARCHAR::BLOB || 'other' FROM range(100000);\n+\n+statement ok\n+INSERT INTO long_strings SELECT repeat('k', 1000000);\n+\n+statement error\n+CREATE INDEX idx ON long_strings(id);\n+----\n+<REGEX>:Invalid Input Error.*exceeds the maximum size.*\n+\n+# Now we try medium-sized keys.\n+statement ok\n+CREATE TABLE medium_strings (id BLOB);\n+\n+statement ok\n+INSERT INTO medium_strings SELECT repeat('k', 122879);\n+\n+statement ok\n+INSERT INTO medium_strings SELECT range::VARCHAR::BLOB FROM range(100000);\n+\n+statement ok\n+INSERT INTO medium_strings SELECT repeat('k', 122879);\n+\n+statement ok\n+INSERT INTO medium_strings SELECT range::VARCHAR::BLOB || 'other' FROM range(100000);\n+\n+statement ok\n+INSERT INTO medium_strings SELECT repeat('k', 122879);\n+\n+statement ok\n+CREATE INDEX idx ON medium_strings(id);\n\\ No newline at end of file\ndiff --git a/test/sql/index/art/insert_update_delete/test_art_parallel_updates.test_slow b/test/sql/index/art/insert_update_delete/test_art_parallel_updates.test_slow\nnew file mode 100644\nindex 000000000000..494b7634f5cc\n--- /dev/null\n+++ b/test/sql/index/art/insert_update_delete/test_art_parallel_updates.test_slow\n@@ -0,0 +1,36 @@\n+# name: test/sql/index/art/insert_update_delete/test_art_parallel_updates.test_slow\n+# description: Test concurrent updates causing index scans.\n+# group: [insert_update_delete]\n+\n+unzip data/storage/artupdates.db.gz __TEST_DIR__/artupdates.db\n+\n+statement ok\n+ATTACH '__TEST_DIR__/artupdates.db' AS db;\n+\n+statement ok\n+USE db;\n+\n+statement ok\n+BEGIN TRANSACTION;\n+\n+loop i 0 100\n+\n+statement ok\n+UPDATE test SET importId = 725 WHERE id = 34165;\n+\n+statement ok\n+UPDATE test SET importId = 663 WHERE id = 42638;\n+\n+statement ok\n+UPDATE test SET importId = 210 WHERE id = 11288;\n+\n+statement ok\n+UPDATE test SET importId = 805 WHERE id = 764;\n+\n+statement ok\n+UPDATE test SET importId = 782 WHERE id = 10151;\n+\n+statement ok\n+UPDATE test SET importId = 53 WHERE id = 3229;\n+\n+endloop\n\\ No newline at end of file\ndiff --git a/test/sql/index/test_art_keys.cpp b/test/sql/index/test_art_keys.cpp\nindex dd92377feab8..07ccc8c9685c 100644\n--- a/test/sql/index/test_art_keys.cpp\n+++ b/test/sql/index/test_art_keys.cpp\n@@ -45,7 +45,7 @@ static void TestKeys(duckdb::vector<ARTKey> &keys) {\n \t}\n }\n \n-static ARTKey CreateCompoundKey(ArenaAllocator &arena_allocator, string str_val, int32_t int_val) {\n+static ARTKey CreateCompoundKey(ArenaAllocator &arena_allocator, const string &str_val, int32_t int_val) {\n \n \tauto key_left = ARTKey::CreateARTKey<string_t>(arena_allocator, string_t(str_val.c_str(), str_val.size()));\n \tauto key_right = ARTKey::CreateARTKey<int32_t>(arena_allocator, int_val);\ndiff --git a/test/sql/window/test_window_constant_aggregate.test b/test/sql/window/test_window_constant_aggregate.test\nindex 89525d65e8a5..6aafb23d0742 100644\n--- a/test/sql/window/test_window_constant_aggregate.test\n+++ b/test/sql/window/test_window_constant_aggregate.test\n@@ -1,5 +1,5 @@\n # name: test/sql/window/test_window_constant_aggregate.test\n-# description: Most basic window function\n+# description: Test \"constant\" aggregation (single result for each partition)\n # group: [window]\n \n statement ok\n@@ -200,3 +200,88 @@ ORDER BY ALL\n 1\t17\t19,18,17,16,15,14,13,12,11,10\n 1\t18\t19,18,17,16,15,14,13,12,11,10\n 1\t19\t19,18,17,16,15,14,13,12,11,10\n+\n+# Test hash group with two partitions and blocks\n+statement ok\n+pragma threads=2\n+\n+loop i 0 100\n+\n+query III\n+with table_1 AS (\n+    SELECT\n+        'fb30cf47-6f6b-42ef-dec2-3f984479a2aa'::uuid    AS id,\n+        unnest(generate_series(\n+            '2024-04-01'::date,\n+            '2025-03-01'::date,\n+            interval '1 month'\n+        ))                                              AS date\n+    UNION ALL BY NAME\n+    SELECT\n+        '7d1cc557-2d45-6900-a1ed-b2c64f5d9200'::uuid    AS id,\n+        unnest(generate_series(\n+            '2024-02-01'::date,\n+            '2025-01-01'::date,\n+            interval '1 month'\n+        ))                                              AS date\n+), table_2 AS (\n+    SELECT\n+        'fb30cf47-6f6b-42ef-dec2-3f984479a2aa'::uuid    AS id,\n+        unnest(generate_series(\n+            '2024-04-01'::date,\n+            '2025-03-01'::date,\n+            interval '1 month'\n+        ))                                              AS date,\n+        1                                               AS value\n+    UNION ALL BY NAME\n+    SELECT\n+        '7d1cc557-2d45-6900-a1ed-b2c64f5d9200'::uuid    AS id,\n+        unnest(generate_series(\n+            '2022-12-01'::date,\n+            '2023-12-01'::date,\n+            interval '1 month'\n+        ))                                              AS date,\n+        1                                               AS value\n+), output AS (\n+    SELECT\n+        table_1.id,\n+        table_1.date,\n+        sum(table_2.value) over (\n+            PARTITION BY table_1.id\n+            ORDER BY table_1.date ASC\n+            ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n+        ) AS test_sum,\n+    FROM table_1\n+    LEFT JOIN table_2\n+        ON table_1.id = table_2.id\n+        AND table_1.date = table_2.date\n+)\n+SELECT * FROM output\n+ORDER BY id DESC, date DESC;\n+----\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2025-03-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2025-02-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2025-01-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-12-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-11-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-10-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-09-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-08-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-07-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-06-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-05-01 00:00:00\t12\n+fb30cf47-6f6b-42ef-dec2-3f984479a2aa\t2024-04-01 00:00:00\t12\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2025-01-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-12-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-11-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-10-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-09-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-08-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-07-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-06-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-05-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-04-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-03-01 00:00:00\tNULL\n+7d1cc557-2d45-6900-a1ed-b2c64f5d9200\t2024-02-01 00:00:00\tNULL\n+\n+endloop\ndiff --git a/tools/pythonpkg/tests/fast/api/test_fsspec.py b/tools/pythonpkg/tests/fast/api/test_fsspec.py\nnew file mode 100644\nindex 000000000000..34a254e1caf1\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/api/test_fsspec.py\n@@ -0,0 +1,51 @@\n+import pytest\n+import duckdb\n+import io\n+\n+fsspec = pytest.importorskip(\"fsspec\")\n+\n+\n+class TestReadParquet(object):\n+    def test_fsspec_deadlock(self, duckdb_cursor, tmp_path):\n+        # Create test parquet data\n+        file_path = tmp_path / \"data.parquet\"\n+        duckdb_cursor.sql(\"COPY (FROM range(50_000)) TO '{}' (FORMAT parquet)\".format(str(file_path)))\n+        with open(file_path, \"rb\") as f:\n+            parquet_data = f.read()\n+\n+        class TestFileSystem(fsspec.AbstractFileSystem):\n+            protocol = \"deadlock\"\n+\n+            @property\n+            def fsid(self):\n+                return \"deadlock\"\n+\n+            def ls(self, path, detail=True, **kwargs):\n+                vals = [k for k in self._data.keys() if k.startswith(path)]\n+                if detail:\n+                    return [\n+                        {\n+                            \"name\": path,\n+                            \"size\": len(self._data[path]),\n+                            \"type\": \"file\",\n+                            \"created\": 0,\n+                            \"islink\": False,\n+                        }\n+                        for path in vals\n+                    ]\n+                else:\n+                    return vals\n+\n+            def _open(self, path, **kwargs):\n+                return io.BytesIO(self._data[path])\n+\n+            def __init__(self):\n+                super().__init__()\n+                self._data = {\"a\": parquet_data, \"b\": parquet_data}\n+\n+        fsspec.register_implementation(\"deadlock\", TestFileSystem, clobber=True)\n+        fs = fsspec.filesystem('deadlock')\n+        duckdb_cursor.register_filesystem(fs)\n+\n+        result = duckdb_cursor.read_parquet(file_globs=[\"deadlock://a\", \"deadlock://b\"], union_by_name=True)\n+        assert len(result.fetchall()) == 100_000\ndiff --git a/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py b/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\nindex aa8da22eb1f4..9a8cc743f96d 100644\n--- a/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\n+++ b/tools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\n@@ -946,5 +946,43 @@ def test_in_filter_pushdown(self, duckdb_cursor):\n         duck_probe = duckdb_conn.table(\"probe\")\n         duck_probe_arrow = duck_probe.arrow()\n         duckdb_conn.register(\"duck_probe_arrow\", duck_probe_arrow)\n-        assert duckdb_conn.execute(\"SELECT * from duck_probe_arrow where a in (1, 999)\").fetchall() == [(1,), (999,)]\n         assert duckdb_conn.execute(\"SELECT * from duck_probe_arrow where a = any([1,999])\").fetchall() == [(1,), (999,)]\n+\n+    def test_pushdown_of_optional_filter(self, duckdb_cursor):\n+        cardinality_table = pa.Table.from_pydict(\n+            {\n+                'column_name': [\n+                    'id',\n+                    'product_code',\n+                    'price',\n+                    'quantity',\n+                    'category',\n+                    'is_available',\n+                    'rating',\n+                    'discount',\n+                    'color',\n+                ],\n+                'cardinality': [100, 100, 100, 45, 5, 3, 6, 39, 5],\n+            }\n+        )\n+\n+        result = duckdb.query(\n+            \"\"\"\n+            SELECT *\n+            FROM cardinality_table\n+            WHERE cardinality > 1\n+            ORDER BY cardinality ASC\n+        \"\"\"\n+        )\n+        res = result.fetchall()\n+        assert res == [\n+            ('is_available', 3),\n+            ('category', 5),\n+            ('color', 5),\n+            ('rating', 6),\n+            ('discount', 39),\n+            ('quantity', 45),\n+            ('id', 100),\n+            ('product_code', 100),\n+            ('price', 100),\n+        ]\n",
  "problem_statement": "ArrowNotImplementedError: 'and_kleene' error when querying Polars DataFrame in DuckDB\n### What happens?\n\nDuckDB query fails with \"ArrowNotImplementedError: Function 'and_kleene' has no kernel matching input types (bool, null)\" when querying a Polars DataFrame containing nullable boolean values.\n\n### To Reproduce\n\n## Code\n```python\nimport numpy as np\nimport random\nimport polars as pl\nimport duckdb\n\nnp.random.seed(42)\nrandom.seed(42)\nn_rows = 100\n\ndata = {\n    \"id\": list(range(1, n_rows + 1)),\n    \"product_code\": [f\"P{i:04d}\" for i in range(n_rows)],\n    \"price\": np.random.uniform(10, 1000, n_rows).tolist(),\n    \"quantity\": [random.randint(1, 50) if random.random() > 0.1 else None for _ in range(n_rows)],\n    \"category\": np.random.choice([\"A\", \"B\", \"C\", \"D\", None], n_rows, p=[0.3, 0.3, 0.2, 0.1, 0.1]).tolist(),\n    \"is_available\": np.random.choice([True, False, None], n_rows, p=[0.6, 0.3, 0.1]).tolist(),\n    \"rating\": [random.randint(1, 5) if random.random() > 0.2 else None for _ in range(n_rows)],\n    \"discount\": [round(random.uniform(0, 0.5), 2) if random.random() > 0.3 else None for _ in range(n_rows)],\n    \"color\": np.random.choice([\"red\", \"blue\", \"green\", \"black\", None], n_rows).tolist()\n}\n\ndf = pl.DataFrame(data)\ncardinality_df = pl.DataFrame({\n    \"column_name\": df.columns,\n    \"cardinality\": [df[col].n_unique() for col in df.columns]\n})\n\n# This successfully creates the cardinality dataframe\nprint(cardinality_df)\n\n# This query fails\nresult = duckdb.query(\"\"\"\nSELECT *\nFROM cardinality_df\nWHERE cardinality > 1\nORDER BY cardinality ASC\n\"\"\")\n\nprint(result)\n```\n\n## Output\nFirst, the Polars DataFrame is successfully created and displays correctly:\n```\nshape: (9, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name  \u2506 cardinality \u2502\n\u2502 ---          \u2506 ---         \u2502\n\u2502 str          \u2506 i64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 id           \u2506 100         \u2502\n\u2502 product_code \u2506 100         \u2502\n\u2502 price        \u2506 100         \u2502\n\u2502 quantity     \u2506 45          \u2502\n\u2502 category     \u2506 5           \u2502\n\u2502 is_available \u2506 3           \u2502\n\u2502 rating       \u2506 6           \u2502\n\u2502 discount     \u2506 39          \u2502\n\u2502 color        \u2506 5           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThen, when trying to run the DuckDB query, it fails with this error:\n```\nArrowNotImplementedError: Function 'and_kleene' has no kernel matching input types (bool, null)\nAt:\n  pyarrow/error.pxi(92): pyarrow.lib.check_status\n```\n\n## Expected behavior\nThe DuckDB query should successfully filter and sort the Polars DataFrame based on the cardinality column.\n\n## Additional info\nConverting the polars dataframe to pandas via `cardinality_df = cardinality_df.to_pandas()` avoids the error.\n\n### OS:\n\nLinux\n\n### DuckDB Version:\n\n1.2.0\n\n### DuckDB Client:\n\nPython\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nPavel Khokhlov\n\n### Affiliation:\n\npersonal\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "",
  "created_at": "2025-03-17T11:51:46Z"
}