You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Support all PostgreSQL CSV import options
https://www.postgresql.org/docs/9.2/sql-copy.html

Except the following which don't make much sense for us:

STDIN, STDOUT, OIDS

For now we should enforce that ENCODING is UTF-8

Our current parser also does not support for multi-character delimiters and newlines in quotes. Add more tests with e.g. unicode delimiter, newlines in quotes, and broken CSV files.



</issue>
<code>
[start of README.md]
1: <img align="left" src="logo/duckdb-logo.png" height="120">
2: 
3: # DuckDB, the SQLite for Analytics
4: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
5: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
6: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
7: 
8: <br>
9: 
10: 
11: # Requirements
12: DuckDB requires [CMake](https://cmake.org) to be installed and a `C++11` compliant compiler. GCC 4.9 and newer, Clang 3.9 and newer and VisualStudio 2017 are tested on each revision.
13: 
14: ## Compiling
15: Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug versoin. You may run `make unit` and `make allunit` to verify that your version works properly after making changes.
16: 
17: # Usage
18: A command line utility based on `sqlite3` can be found in either `build/release/tools/shell/shell` (release, the default) or `build/debug/tools/shell/shell` (debug).
19: 
20: # Embedding
21: As DuckDB is an embedded database, there is no database server to launch or client to connect to a running server. However, the database server can be embedded directly into an application using the C or C++ bindings. The main build process creates the shared library `build/release/src/libduckdb.[so|dylib|dll]` that can be linked against. A static library is built as well.
22: 
23: For examples on how to embed DuckDB into your application, see the [examples](https://github.com/cwida/duckdb/tree/master/examples) folder.
24: 
25: ## Benchchmarks
26: After compiling, benchmarks can be executed from the root directory by executing `./build/release/benchmark/benchmark_runner`.
27: 
28: ## Standing on the Shoulders of Giants
29: DuckDB is implemented in C++ 11, should compile with GCC and clang, uses CMake to build and [Catch2](https://github.com/catchorg/Catch2) for testing. DuckDB uses some components from various Open-Source databases and draws inspiration from scientific publications. Here is an overview:
30: 
31: * Parser: We use the PostgreSQL parser that was [repackaged as a stand-alone library](https://github.com/lfittl/libpg_query). The translation to our own parse tree is inspired by [Peloton](https://pelotondb.io).
32: * Shell: We have adapted the [SQLite shell](https://sqlite.org/cli.html) to work with DuckDB.
33: * Tests: We use the [SQL Logic Tests from SQLite](https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki) to test DuckDB.
34: * Query fuzzing: We use [SQLsmith](https://github.com/anse1/sqlsmith) to generate random queries for additional testing.
35: * Date Math: We use the date math component from [MonetDB](https://www.monetdb.org).
36: * SQL Window Functions: DuckDB's window functions implementation uses Segment Tree Aggregation as described in the paper "Efficient Processing of Window Functions in Analytical SQL Queries" by Viktor Leis, Kan Kundhikanjana, Alfons Kemper and Thomas Neumann.
37: * Execution engine: The vectorized execution engine is inspired by the paper "MonetDB/X100: Hyper-Pipelining Query Execution" by Peter Boncz, Marcin Zukowski and Niels Nes.
38: * Optimizer: DuckDB's optimizer draws inspiration from the papers "Dynamic programming strikes back" by Guido Moerkotte and Thomas Neumman as well as "Unnesting Arbitrary Queries" by Thomas Neumann and Alfons Kemper.
39: * Concurrency control: Our MVCC implementation is inspired by the paper "Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems" by Thomas Neumann, Tobias Mühlbauer and Alfons Kemper.
40: * Regular Expression: DuckDB uses Google's [RE2](https://github.com/google/re2) regular expression engine.
41: 
42: ## Other pages
43: * [Continuous Benchmarking (CB™)](https://www.duckdb.org/benchmarks/index.html), runs TPC-H, TPC-DS and some microbenchmarks on every commit
[end of README.md]
[start of src/execution/operator/persistent/buffered_csv_reader.cpp]
1: #include "execution/operator/persistent/buffered_csv_reader.hpp"
2: #include "execution/operator/persistent/physical_copy_from_file.hpp"
3: 
4: #include "catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "common/vector_operations/vector_operations.hpp"
6: #include "main/database.hpp"
7: #include "storage/data_table.hpp"
8: #include "parser/column_definition.hpp"
9: 
10: #include <algorithm>
11: #include <fstream>
12: 
13: using namespace duckdb;
14: using namespace std;
15: 
16: static char is_newline(char c) {
17: 	return c == '\n' || c == '\r';
18: }
19: 
20: BufferedCSVReader::BufferedCSVReader(CopyInfo &info, vector<SQLType> sql_types, istream &source)
21:     : info(info), sql_types(sql_types), source(source), buffer_size(0), position(0), start(0) {
22: 	// initialize the parse_chunk with a set of VARCHAR types
23: 	vector<TypeId> varchar_types;
24: 	for (index_t i = 0; i < sql_types.size(); i++) {
25: 		varchar_types.push_back(TypeId::VARCHAR);
26: 	}
27: 	parse_chunk.Initialize(varchar_types);
28: 
29: 	if (info.header) {
30: 		// ignore the first line as a header line
31: 		string read_line;
32: 		getline(source, read_line);
33: 		linenr++;
34: 	}
35: }
36: 
37: void BufferedCSVReader::ParseCSV(DataChunk &insert_chunk) {
38: 	cached_buffers.clear();
39: 
40: 	index_t column = 0;
41: 	index_t offset = 0;
42: 	bool in_quotes = false;
43: 	bool finished_chunk = false;
44: 	bool seen_escape = true;
45: 
46: 	if (position >= buffer_size) {
47: 		if (!ReadBuffer(start)) {
48: 			return;
49: 		}
50: 	}
51: 
52: 	// read until we exhaust the stream
53: 	while (true) {
54: 		if (finished_chunk) {
55: 			return;
56: 		}
57: 		if (in_quotes) {
58: 			if (buffer[position] == info.escape) {
59: 				seen_escape = true;
60: 				// FIXME this is only part of the deal, we also need to zap the escapes below
61: 			}
62: 			else if (!seen_escape) {
63: 				if (buffer[position] == info.quote) {
64: 					// end quote
65: 					offset = 1;
66: 					in_quotes = false;
67: 				}
68: 			} else {
69: 				seen_escape = false;
70: 			}
71: 
72: 		} else {
73: 			if (buffer[position] == info.quote) {
74: 				// start quotes can only occur at the start of a field
75: 				if (position == start) {
76: 					// increment start by 1
77: 					start++;
78: 					// read until we encounter a quote again
79: 					in_quotes = true;
80: 				}
81: 			} else if (buffer[position] == info.delimiter) {
82: 				// encountered delimiter
83: 				AddValue(buffer.get() + start, position - start - offset, column);
84: 				start = position + 1;
85: 				offset = 0;
86: 			}
87: 			if (is_newline(buffer[position]) || (source.eof() && position + 1 == buffer_size)) {
88: 				char newline = buffer[position];
89: 				// encountered a newline, add the current value and push the row
90: 				AddValue(buffer.get() + start, position - start - offset, column);
91: 				finished_chunk = AddRow(insert_chunk, column);
92: 
93: 				// move to the next character
94: 				start = position + 1;
95: 				offset = 0;
96: 				if (newline == '\r') {
97: 					// \r, skip subsequent \n
98: 					if (position + 1 >= buffer_size) {
99: 						if (!ReadBuffer(start)) {
100: 							break;
101: 						}
102: 						if (buffer[position] == '\n') {
103: 							start++;
104: 							position++;
105: 						}
106: 						continue;
107: 					}
108: 					if (buffer[position + 1] == '\n') {
109: 						start++;
110: 						position++;
111: 					}
112: 				}
113: 			}
114: 			if (offset != 0) {
115: 				in_quotes = true;
116: 			}
117: 		}
118: 
119: 		position++;
120: 		if (position >= buffer_size) {
121: 			// exhausted the buffer
122: 			if (!ReadBuffer(start)) {
123: 				break;
124: 			}
125: 		}
126: 	}
127: 	if (in_quotes) {
128: 		throw ParserException("Error on line %lld: unterminated quotes", linenr);
129: 	}
130: 	Flush(insert_chunk);
131: }
132: 
133: bool BufferedCSVReader::ReadBuffer(index_t &start) {
134: 	auto old_buffer = move(buffer);
135: 
136: 	// the remaining part of the last buffer
137: 	index_t remaining = buffer_size - start;
138: 	index_t buffer_read_size = INITIAL_BUFFER_SIZE;
139: 	while (remaining > buffer_read_size) {
140: 		buffer_read_size *= 2;
141: 	}
142: 	if (remaining + buffer_read_size > MAXIMUM_CSV_LINE_SIZE) {
143: 		throw ParserException("Maximum line size of %llu bytes exceeded!", MAXIMUM_CSV_LINE_SIZE);
144: 	}
145: 	buffer = unique_ptr<char[]>(new char[buffer_read_size + remaining + 1]);
146: 	buffer_size = remaining + buffer_read_size;
147: 	if (remaining > 0) {
148: 		// remaining from last buffer: copy it here
149: 		memcpy(buffer.get(), old_buffer.get() + start, remaining);
150: 	}
151: 	source.read(buffer.get() + remaining, buffer_read_size);
152: 	index_t read_count = source.eof() ? source.gcount() : buffer_read_size;
153: 	buffer_size = remaining + read_count;
154: 	buffer[buffer_size] = '\0';
155: 	if (old_buffer && start != 0) {
156: 		cached_buffers.push_back(move(old_buffer));
157: 	}
158: 	start = 0;
159: 	position = remaining;
160: 
161: 	return read_count > 0;
162: }
163: 
164: void BufferedCSVReader::AddValue(char *str_val, index_t length, index_t &column) {
165: 	if (column == sql_types.size() && length == 0) {
166: 		// skip a single trailing delimiter
167: 		column++;
168: 		return;
169: 	}
170: 	if (column >= sql_types.size()) {
171: 		throw ParserException("Error on line %lld: expected %lld values but got %d", linenr, sql_types.size(),
172: 		                      column + 1);
173: 	}
174: 	// insert the line number into the chunk
175: 	index_t row_entry = parse_chunk.data[column].count++;
176: 	if (length == 0) {
177: 		parse_chunk.data[column].nullmask[row_entry] = true;
178: 	} else {
179: 		auto data = (const char **)parse_chunk.data[column].data;
180: 		data[row_entry] = str_val;
181: 		str_val[length] = '\0';
182: 		if (!Value::IsUTF8String(str_val)) {
183: 			throw ParserException("Error on line %lld: file is not valid UTF8", linenr);
184: 		}
185: 	}
186: 	// move to the next column
187: 	column++;
188: }
189: 
190: bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, index_t &column) {
191: 	if (column < sql_types.size()) {
192: 		throw ParserException("Error on line %lld: expected %lld values but got %d", linenr, sql_types.size(), column);
193: 	}
194: 	nr_elements++;
195: 	if (nr_elements == STANDARD_VECTOR_SIZE) {
196: 		Flush(insert_chunk);
197: 		return true;
198: 	}
199: 	column = 0;
200: 	linenr++;
201: 	return false;
202: }
203: 
204: void BufferedCSVReader::Flush(DataChunk &insert_chunk) {
205: 	if (nr_elements == 0) {
206: 		return;
207: 	}
208: 	// convert the columns in the parsed chunk to the types of the table
209: 	for (index_t col_idx = 0; col_idx < sql_types.size(); col_idx++) {
210: 		if (sql_types[col_idx].id == SQLTypeId::VARCHAR) {
211: 			// target type is varchar: just move the parsed chunk
212: 			parse_chunk.data[col_idx].Move(insert_chunk.data[col_idx]);
213: 		} else {
214: 			// target type is not varchar: perform a cast
215: 			VectorOperations::Cast(parse_chunk.data[col_idx], insert_chunk.data[col_idx], SQLType::VARCHAR,
216: 			                       sql_types[col_idx]);
217: 		}
218: 	}
219: 	parse_chunk.Reset();
220: 
221: 	nr_elements = 0;
222: }
[end of src/execution/operator/persistent/buffered_csv_reader.cpp]
[start of src/execution/operator/persistent/physical_copy_to_file.cpp]
1: #include "execution/operator/persistent/physical_copy_to_file.hpp"
2: #include "common/vector_operations/vector_operations.hpp"
3: 
4: #include <algorithm>
5: #include <fstream>
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: class BufferedWriter {
11: 	constexpr static index_t BUFFER_SIZE = 4096 * 4;
12: 
13: public:
14: 	BufferedWriter(string &path) : pos(0) {
15: 		to_csv.open(path);
16: 		if (to_csv.fail()) {
17: 			throw IOException("Could not open CSV file");
18: 		}
19: 	}
20: 
21: 	void Flush() {
22: 		if (pos > 0) {
23: 			to_csv.write(buffer, pos);
24: 			pos = 0;
25: 		}
26: 	}
27: 
28: 	void Close() {
29: 		Flush();
30: 		to_csv.close();
31: 	}
32: 
33: 	void Write(const char *buf, index_t len) {
34: 		if (len >= BUFFER_SIZE) {
35: 			Flush();
36: 			to_csv.write(buf, len);
37: 			return;
38: 		}
39: 		if (pos + len > BUFFER_SIZE) {
40: 			Flush();
41: 		}
42: 		memcpy(buffer + pos, buf, len);
43: 		pos += len;
44: 	}
45: 
46: 	void Write(string &value) {
47: 		Write(value.c_str(), value.size());
48: 	}
49: 
50: private:
51: 	char buffer[BUFFER_SIZE];
52: 	index_t pos = 0;
53: 
54: 	ofstream to_csv;
55: };
56: 
57: static void WriteQuotedString(BufferedWriter &writer, const char *str_value, char delimiter, char quote) {
58: 	// scan the string for the delimiter
59: 	bool write_quoted = false;
60: 	index_t len = 0;
61: 	for (const char *val = str_value; *val; val++) {
62: 		len++;
63: 		if (*val == delimiter || *val == '\n' || *val == '\r') {
64: 			// delimiter or newline, write a quoted string
65: 			write_quoted = true;
66: 		}
67: 	}
68: 	if (!write_quoted) {
69: 		writer.Write(str_value, len);
70: 	} else {
71: 		writer.Write(&quote, 1);
72: 		writer.Write(str_value, len);
73: 		writer.Write(&quote, 1);
74: 	}
75: }
76: 
77: void PhysicalCopyToFile::GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state) {
78: 	auto &info = *this->info;
79: 	index_t total = 0;
80: 
81: 	string newline = "\n";
82: 	BufferedWriter writer(info.file_path);
83: 	if (info.header) {
84: 		// write the header line
85: 		for (index_t i = 0; i < names.size(); i++) {
86: 			if (i != 0) {
87: 				writer.Write(&info.delimiter, 1);
88: 			}
89: 			WriteQuotedString(writer, names[i].c_str(), info.delimiter, info.quote);
90: 		}
91: 		writer.Write(newline);
92: 	}
93: 	// cerate a chunk with VARCHAR columns
94: 	vector<TypeId> types;
95: 	for (index_t col_idx = 0; col_idx < state->child_chunk.column_count; col_idx++) {
96: 		types.push_back(TypeId::VARCHAR);
97: 	}
98: 	DataChunk cast_chunk;
99: 	cast_chunk.Initialize(types);
100: 
101: 	while (true) {
102: 		children[0]->GetChunk(context, state->child_chunk, state->child_state.get());
103: 		if (state->child_chunk.size() == 0) {
104: 			break;
105: 		}
106: 		// cast the columns of the chunk to varchar
107: 		for (index_t col_idx = 0; col_idx < state->child_chunk.column_count; col_idx++) {
108: 			if (sql_types[col_idx].id == SQLTypeId::VARCHAR) {
109: 				// VARCHAR, just create a reference
110: 				cast_chunk.data[col_idx].Reference(state->child_chunk.data[col_idx]);
111: 			} else {
112: 				// non varchar column, perform the cast
113: 				VectorOperations::Cast(state->child_chunk.data[col_idx], cast_chunk.data[col_idx], sql_types[col_idx],
114: 				                       SQLType::VARCHAR);
115: 			}
116: 		}
117: 		// now loop over the vectors and output the values
118: 		VectorOperations::Exec(cast_chunk.data[0], [&](index_t i, index_t k) {
119: 			for (index_t col_idx = 0; col_idx < state->child_chunk.column_count; col_idx++) {
120: 				if (col_idx != 0) {
121: 					writer.Write(&info.delimiter, 1);
122: 				}
123: 				if (cast_chunk.data[col_idx].nullmask[i]) {
124: 					continue;
125: 				}
126: 				// non-null value, fetch the string value from the cast chunk
127: 				auto str_value = ((const char **)cast_chunk.data[col_idx].data)[i];
128: 				WriteQuotedString(writer, str_value, info.delimiter, info.quote);
129: 			}
130: 			writer.Write(newline);
131: 		});
132: 		total += cast_chunk.size();
133: 	}
134: 	writer.Close();
135: 
136: 	chunk.data[0].count = 1;
137: 	chunk.data[0].SetValue(0, Value::BIGINT(total));
138: 
139: 	state->finished = true;
140: }
[end of src/execution/operator/persistent/physical_copy_to_file.cpp]
[start of src/include/execution/operator/persistent/buffered_csv_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // execution/operator/persistent/buffered_csv_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "execution/physical_operator.hpp"
12: #include "parser/parsed_data/copy_info.hpp"
13: 
14: namespace duckdb {
15: struct CopyInfo;
16: 
17: //! Buffered CSV reader is a class that reads values from a stream and parses them as a CSV file
18: class BufferedCSVReader {
19: 	static constexpr index_t INITIAL_BUFFER_SIZE = 16384;
20: 	static constexpr index_t MAXIMUM_CSV_LINE_SIZE = 1048576;
21: 
22: public:
23: 	BufferedCSVReader(CopyInfo &info, vector<SQLType> sql_types, std::istream &source);
24: 
25: 	CopyInfo &info;
26: 	vector<SQLType> sql_types;
27: 	std::istream &source;
28: 
29: 	unique_ptr<char[]> buffer;
30: 	index_t buffer_size;
31: 	index_t position;
32: 	index_t start = 0;
33: 
34: 	index_t linenr = 0;
35: 	index_t nr_elements = 0;
36: 
37: 	vector<unique_ptr<char[]>> cached_buffers;
38: 
39: 	DataChunk parse_chunk;
40: 
41: public:
42: 	//! Extract a single DataChunk from the CSV file and stores it in insert_chunk
43: 	void ParseCSV(DataChunk &insert_chunk);
44: 
45: private:
46: 	//! Adds a value to the current row
47: 	void AddValue(char *str_val, index_t length, index_t &column);
48: 	//! Adds a row to the insert_chunk, returns true if the chunk is filled as a result of this row being added
49: 	bool AddRow(DataChunk &insert_chunk, index_t &column);
50: 	//! Finalizes a chunk, parsing all values that have been added so far and adding them to the insert_chunk
51: 	void Flush(DataChunk &insert_chunk);
52: 	//! Reads a new buffer from the CSV file if the current one has been exhausted
53: 	bool ReadBuffer(index_t &start);
54: };
55: 
56: } // namespace duckdb
[end of src/include/execution/operator/persistent/buffered_csv_reader.hpp]
[start of src/include/parser/parsed_data/copy_info.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parser/parsed_data/copy_info.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "common/common.hpp"
12: 
13: namespace duckdb {
14: 
15: //===--------------------------------------------------------------------===//
16: // External File Format Types
17: //===--------------------------------------------------------------------===//
18: enum class ExternalFileFormat : uint8_t { INVALID, CSV };
19: 
20: struct CopyInfo {
21: 	//! The schema name to copy to/from
22: 	string schema;
23: 	//! The table name to copy to/from
24: 	string table;
25: 	//! The file path to copy to or copy from
26: 	string file_path;
27: 	//! Whether or not this is a copy to file or copy from a file
28: 	bool is_from;
29: 	//! Delimiter to parse
30: 	char delimiter;
31: 	//! Quote to use
32: 	char quote;
33: 	//! Escape character to use
34: 	char escape;
35: 	//! Whether or not the file has a header line
36: 	bool header;
37: 	//! The file format of the external file
38: 	ExternalFileFormat format;
39: 	// List of Columns that will be copied from/to.
40: 	vector<string> select_list;
41: 
42: 	CopyInfo()
43: 	    : schema(DEFAULT_SCHEMA), is_from(false), delimiter(','), quote('"'), escape('\0'), header(false),
44: 	      format(ExternalFileFormat::CSV) {
45: 	}
46: };
47: 
48: } // namespace duckdb
[end of src/include/parser/parsed_data/copy_info.hpp]
[start of src/include/planner/statement/bound_copy_statement.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // planner/statement/bound_copy_statement.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "planner/bound_query_node.hpp"
12: #include "planner/bound_sql_statement.hpp"
13: #include "planner/statement/bound_insert_statement.hpp"
14: 
15: namespace duckdb {
16: class TableCatalogEntry;
17: 
18: //! Bound equivalent to CopyStatement
19: class BoundCopyStatement : public BoundSQLStatement {
20: public:
21: 	BoundCopyStatement() : BoundSQLStatement(StatementType::COPY) {
22: 	}
23: 
24: 	//! The CopyInfo
25: 	unique_ptr<CopyInfo> info;
26: 	//! The bound insert statement (only for COPY from file -> database)
27: 	unique_ptr<BoundSQLStatement> bound_insert;
28: 	// The bound SQL statement (only for COPY from database -> file)
29: 	unique_ptr<BoundQueryNode> select_statement;
30: 
31: 	vector<string> names;
32: 	vector<SQLType> sql_types;
33: 
34: public:
35: 	vector<string> GetNames() override {
36: 		return {"Count"};
37: 	}
38: 	vector<SQLType> GetTypes() override {
39: 		return {SQLType::BIGINT};
40: 	}
41: };
42: } // namespace duckdb
[end of src/include/planner/statement/bound_copy_statement.hpp]
[start of src/parser/transform/statement/transform_copy.cpp]
1: #include "parser/expression/columnref_expression.hpp"
2: #include "parser/expression/star_expression.hpp"
3: #include "parser/statement/copy_statement.hpp"
4: #include "parser/statement/select_statement.hpp"
5: #include "parser/tableref/basetableref.hpp"
6: #include "parser/transformer.hpp"
7: #include "common/string_util.hpp"
8: 
9: #include <cstring>
10: 
11: using namespace duckdb;
12: using namespace postgres;
13: using namespace std;
14: 
15: static ExternalFileFormat StringToExternalFileFormat(const string &str) {
16: 	auto upper = StringUtil::Upper(str);
17: 	if (upper == "CSV") {
18: 		return ExternalFileFormat::CSV;
19: 	}
20: 	throw ConversionException("No ExternalFileFormat for input '%s'", upper.c_str());
21: }
22: 
23: unique_ptr<CopyStatement> Transformer::TransformCopy(Node *node) {
24: 	const string kDelimiterTok = "delimiter";
25: 	const string kFormatTok = "format";
26: 	const string kQuoteTok = "quote";
27: 	const string kEscapeTok = "escape";
28: 	const string kHeaderTok = "header";
29: 
30: 	CopyStmt *stmt = reinterpret_cast<CopyStmt *>(node);
31: 	assert(stmt);
32: 	auto result = make_unique<CopyStatement>();
33: 	auto &info = *result->info;
34: 	info.file_path = stmt->filename;
35: 	info.is_from = stmt->is_from;
36: 
37: 	if (stmt->attlist) {
38: 		for (auto n = stmt->attlist->head; n != nullptr; n = n->next) {
39: 			auto target = reinterpret_cast<ResTarget *>(n->data.ptr_value);
40: 			if (target->name) {
41: 				info.select_list.push_back(string(target->name));
42: 			}
43: 		}
44: 	}
45: 
46: 	if (stmt->relation) {
47: 		auto ref = TransformRangeVar(stmt->relation);
48: 		if (info.is_from) {
49: 			// copy file into table
50: 			auto &table = *reinterpret_cast<BaseTableRef *>(ref.get());
51: 			info.table = table.table_name;
52: 			info.schema = table.schema_name;
53: 		} else {
54: 			// copy table into file, generate SELECT * FROM table;
55: 			auto statement = make_unique<SelectNode>();
56: 			statement->from_table = move(ref);
57: 			if (stmt->attlist) {
58: 				for (index_t i = 0; i < info.select_list.size(); i++)
59: 					statement->select_list.push_back(make_unique<ColumnRefExpression>(info.select_list[i]));
60: 			} else {
61: 				statement->select_list.push_back(make_unique<StarExpression>());
62: 			}
63: 			result->select_statement = move(statement);
64: 		}
65: 	} else {
66: 		result->select_statement = TransformSelectNode((SelectStmt *)stmt->query);
67: 	}
68: 
69: 	// Handle options
70: 	if (stmt->options) {
71: 		ListCell *cell = nullptr;
72: 		for_each_cell(cell, stmt->options->head) {
73: 			auto *def_elem = reinterpret_cast<DefElem *>(cell->data.ptr_value);
74: 
75: 			if (StringUtil::StartsWith(def_elem->defname, "delim") ||
76: 			    StringUtil::StartsWith(def_elem->defname, "sep")) {
77: 				// delimiter
78: 				auto *delimiter_val = reinterpret_cast<postgres::Value *>(def_elem->arg);
79: 				if (!delimiter_val || delimiter_val->type != T_String) {
80: 					throw ParserException("Unsupported parameter type for DELIMITER: expected e.g. DELIMITER ','");
81: 				}
82: 				index_t delim_len = strlen(delimiter_val->val.str);
83: 				info.delimiter = '\0';
84: 				char *delim_cstr = delimiter_val->val.str;
85: 				if (delim_len == 1) {
86: 					info.delimiter = delim_cstr[0];
87: 				}
88: 				if (delim_len == 2 && delim_cstr[0] == '\\' && delim_cstr[1] == 't') {
89: 					info.delimiter = '\t';
90: 				}
91: 				if (info.delimiter == '\0') {
92: 					throw Exception("Could not interpret DELIMITER option");
93: 				}
94: 			} else if (def_elem->defname == kFormatTok) {
95: 				// format
96: 				auto *format_val = reinterpret_cast<postgres::Value *>(def_elem->arg);
97: 				if (!format_val || format_val->type != T_String) {
98: 					throw ParserException("Unsupported parameter type for FORMAT: expected e.g. FORMAT 'csv'");
99: 				}
100: 				info.format = StringToExternalFileFormat(format_val->val.str);
101: 			} else if (def_elem->defname == kQuoteTok) {
102: 				// quote
103: 				auto *quote_val = reinterpret_cast<postgres::Value *>(def_elem->arg);
104: 				if (!quote_val || quote_val->type != T_String) {
105: 					throw ParserException("Unsupported parameter type for QUOTE: expected e.g. QUOTE '\"'");
106: 				}
107: 				info.quote = *quote_val->val.str;
108: 			} else if (def_elem->defname == kEscapeTok) {
109: 				// escape
110: 				auto *escape_val = reinterpret_cast<postgres::Value *>(def_elem->arg);
111: 				if (!escape_val || escape_val->type != T_String) {
112: 					throw ParserException("Unsupported parameter type for ESCAPE: expected e.g. ESCAPE '\\'");
113: 				}
114: 				info.escape = *escape_val->val.str;
115: 			} else if (def_elem->defname == kHeaderTok) {
116: 				auto *header_val = reinterpret_cast<postgres::Value *>(def_elem->arg);
117: 				if (!header_val) {
118: 					info.header = true;
119: 					continue;
120: 				}
121: 				switch (header_val->type) {
122: 				case T_Integer:
123: 					info.header = header_val->val.ival == 1 ? true : false;
124: 					break;
125: 				case T_String: {
126: 					auto val = duckdb::Value(string(header_val->val.str));
127: 					info.header = val.CastAs(TypeId::BOOLEAN).value_.boolean;
128: 					break;
129: 				}
130: 				default:
131: 					throw ParserException("Unsupported parameter type for HEADER");
132: 				}
133: 			} else {
134: 				throw ParserException("Unsupported COPY option: %s", def_elem->defname);
135: 			}
136: 		}
137: 	}
138: 
139: 	return result;
140: }
[end of src/parser/transform/statement/transform_copy.cpp]
[start of src/planner/binder/statement/bind_copy.cpp]
1: #include "main/client_context.hpp"
2: #include "parser/statement/copy_statement.hpp"
3: #include "planner/binder.hpp"
4: #include "parser/statement/insert_statement.hpp"
5: #include "planner/statement/bound_copy_statement.hpp"
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: unique_ptr<BoundSQLStatement> Binder::Bind(CopyStatement &stmt) {
11: 	auto result = make_unique<BoundCopyStatement>();
12: 	if (stmt.select_statement) {
13: 		// COPY from a query
14: 		result->select_statement = Bind(*stmt.select_statement);
15: 		result->names = {"Count"};
16: 		result->sql_types = {SQLType::BIGINT};
17: 	} else {
18: 		assert(!stmt.info->table.empty());
19: 		// COPY to a table
20: 		// generate an insert statement for the the to-be-inserted table
21: 		InsertStatement insert;
22: 		insert.table = stmt.info->table;
23: 		insert.schema = stmt.info->schema;
24: 		insert.columns = stmt.info->select_list;
25: 
26: 		// bind the insert statement to the base table
27: 		result->bound_insert = Bind(insert);
28: 		auto &bound_insert = (BoundInsertStatement &)*result->bound_insert;
29: 		// get the set of expected columns from the insert statement; these types will be parsed from the CSV
30: 		result->sql_types = bound_insert.expected_types;
31: 	}
32: 	result->info = move(stmt.info);
33: 	return move(result);
34: }
[end of src/planner/binder/statement/bind_copy.cpp]
[start of src/planner/logical_plan/statement/plan_copy.cpp]
1: #include "planner/logical_plan_generator.hpp"
2: #include "planner/operator/logical_copy_from_file.hpp"
3: #include "planner/operator/logical_copy_to_file.hpp"
4: #include "planner/statement/bound_copy_statement.hpp"
5: 
6: using namespace duckdb;
7: using namespace std;
8: 
9: unique_ptr<LogicalOperator> LogicalPlanGenerator::CreatePlan(BoundCopyStatement &stmt) {
10: 	if (stmt.select_statement) {
11: 		// COPY from a query
12: 		auto names = stmt.select_statement->names;
13: 		auto types = stmt.select_statement->types;
14: 
15: 		// first plan the query
16: 		auto root = CreatePlan(*stmt.select_statement);
17: 		// now create the copy information
18: 		auto copy = make_unique<LogicalCopyToFile>(move(stmt.info));
19: 		copy->AddChild(move(root));
20: 		copy->names = names;
21: 		copy->sql_types = types;
22: 
23: 		return move(copy);
24: 	} else {
25: 		// COPY to a table
26: 		assert(!stmt.info->table.empty());
27: 		// first create a plan for the insert statement
28: 		auto insert = CreatePlan(*stmt.bound_insert);
29: 		// now create the copy statement and set it as a child of the insert statement
30: 		auto copy = make_unique<LogicalCopyFromFile>(move(stmt.info), stmt.sql_types);
31: 		insert->children.push_back(move(copy));
32: 		return insert;
33: 	}
34: }
[end of src/planner/logical_plan/statement/plan_copy.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: